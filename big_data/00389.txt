Multi-Information Source Optimization

with General Model Discrepancies∗
Matthias Poloczek, Jialei Wang, and Peter I. Frazier

School of Operations Research and Information Engineering

Cornell University

{poloczek,jw865,pf98}@cornell.edu

In the multi-information source optimization problem our goal is to optimize
a complex design. However, we only have indirect access to the objective value
of any design via information sources that are subject to model discrepancy,
i.e. whose internal model inherently deviates from reality. We present a novel
algorithm that is based on a rigorous mathematical treatment of the uncertainties
arising from the model discrepancies. Its optimization decisions rely on a stringent
value of information analysis that trades oﬀ the predicted beneﬁt and its cost.
We conduct an experimental evaluation that demonstrates that the method
consistently outperforms other state-of-the-art techniques: it ﬁnds designs of
considerably higher objective value and additionally inﬂicts less cost in the
exploration process.

6
1
0
2

 
r
a

M
1

 

 
 
]
L
M

.
t
a
t
s
[
 
 

1
v
9
8
3
0
0

.

3
0
6
1
:
v
i
X
r
a

∗The authors were partially supported by NSF CAREER CMMI-1254298, NSF CMMI-1536895, NSF

IIS-1247696, AFOSR FA9550-12-1-0200, AFOSR FA9550-15-1-0038, and AFOSR FA9550-16-1-0046.

1

1 Introduction

In the multi-information source optimization problem (MISO) we study complex optimization
tasks arising for instance in engineering or the natural sciences, where our goal is to optimize
a design speciﬁed by multiple parameters. In order to assess the true value of some design,
we have access to a variety of information sources. In many cases these are numerical
simulations that employ models of the true objective function of varying complexity. A
simple example from aerospace design is given in [LK10], where one may choose between
high ﬁdelity Euler equations or two cheaper-to-calculate approximations to evaluate the
quality of a transonic airfoil design.

In either case, the outputs of information sources diﬀer from the true value of the design,
since the simulations only apply an incomplete description of the laws of physics. Thus,
inevitably we face a model discrepancy which denotes an inherent inability to describe the
reality accurately.

We stress that this notion goes far beyond typical “noise” such as measurement errors
or numerical inaccuracies. The latter grasps uncertainty that arises when sampling from
an information source, and is the type of inaccuracy that most of the previous work on
multiﬁdelity optimization deals with. In particular, such an understanding of noise assumes
implicitly that the internal value (or state) of the information source itself is an accurate
description of the truth. Our more general notion of model discrepancy captures the
uncertainty about the truth that originates from the inherent deviation of the internal model
from reality. This has severe implications for the machinery that we can apply to tackle the
optimization problem: if information sources are inherently biased and unable to return
the true objective value of a design, then techniques that seek to reduce the experimental
cost by building cheaper statistical approximations and calibrate them by queries to the
real function become inapplicable. In this article we describe a novel approach based on a
rigorous mathematical treatment of the arising uncertainties and a decision-theoretically
founded optimization policy.

Related Work. The task of optimizing a single, expensive-to-evaluate black-box function
has received a lot of attention. A successful approach to this end is to apply Bayesian
optimization, a prominent representative being the eﬃcient global optimization (EGO) method
by Jones, Schonlau, and Welch [JSW98]. Also known as “sequential kriging optimization”,
this method builds on earlier works by [Kus64, Moc89, SWMW89]. The EGO method assumes
that there is only a single information source that returns the true value; the goal is to
ﬁnd a global maximum of the objective while minimizing the query cost. It consists of
the following two steps that have served as a blueprint for many subsequent Bayesian
optimization techniques. First a stochastic, in their case Gaussian, process is formulated
and ﬁt to a set of initial data points. Then they search for a global optimum by iteratively
sampling a point of highest predicted score according to some “acquisition criterion”: Jones
et al. employ expected improvement (EI) as acquisition criterion, that samples the point
next that oﬀers a largest expected increase in objective value with respect to the best

2

current design. Subsequent articles extended the expected improvement method to deal
with observational noise, e.g., see [HANZ06, FSK07, SFP11].

To the best of our knowledge, the fundamental paper of Kennedy and O’Hagan [KO00]
in this article the
introduced Gaussian process regression to multiﬁdelity optimization:
objective is to optimize a design given several computer codes that vary in accuracy and
computational complexity. This approach of building cheap-to-evaluate, approximate models
for the real function, that oﬀer diﬀerent ﬁdelity-cost trade-oﬀs, is also known as “surrogate
modeling” and has gained a lot of popularity in engineering sciences (e.g., see the survey
article of Queipo et al. [QHS+05]). However, previous works impose several restrictions on
the nature of the model discrepancy. A common constraint in multiﬁdelity optimization
(e.g., see [KO00, BV04, EGC04, RHK08, MW12]) is that information sources are required
to form a hierarchy, thereby limiting possible correlations among their outputs: in particular,
once one has queried a high ﬁdelity source for some point x, then no further knowledge
on g(x) can be gained by querying any other information source of lower ﬁdelity (at any
point). A second frequent assumption is that information sources are unbiased, admitting
only (typically normally distributed) noise that further must be independent across diﬀerent
sources.

A crucial limitation of many models, e.g., [EGC04, HANM06, FSK07, LK10], is the
dependence on access to the true objective function, i.e. that there is an unbiased, arbitrarily
accurate information source. Note that this prerequisite is a powerful assumption, as it
provides in particular a leverage to assess the bias and variance of all information sources,
which is used to build and verify the surrogates. The seminal paper of Lam, Allaire,
and Willcox [LAW15] addresses several of these shortcomings by a novel surrogate-based
approach that requires the information sources to be neither hierarchical nor unbiased,
and allows a more general notion of model discrepancy building on theoretical foundations
given by Allaire and Willcox [AW14]. Their model has a separate Gaussian process for
each information source, that gives for each design x ∈ D a distribution capturing the
uncertainty of that particular information source on x. Next Lam et al. construct a single
surrogate for each x by fusing all these distributions into a single one using the method
of Winkler [Win81]. Then they apply the EI acquisition function on these surrogates to
ﬁrst decide what design x∗ should be evaluated next and afterwards select the respective
information source to query x∗; the latter decision is based on a heuristic that aims to
balance information gain and query cost.

Our Contributions. We present an approach to multi-information source optimization
that allows to handle model discrepancy arising in this setting in a more general and
stringent way. We build on the common technique to capture the model discrepancy of each
information source by a Gaussian process. However, we break through the separation and
build a single statistical model that allows a uniform Bayesian treatment of the (black box
or even inaccessible) objective function and the information sources. Therefore, we are able
to perform a value of information based analysis to select in a single, coordinated step the

3

design and source we should query next in order to optimize the objective. Our method
preserves the features introduced in previous works: in particular, we allow non-hierarchical
information sources, whose model discrepancies and query cost may vary over the domain,
and we do not require direct access to true value of the design according to the true objective
function, also sometimes referred to as the real system.

Furthermore, our statistical model improves on previous works, as it allows to rigorously
exploit correlations across diﬀerent information sources and hence are able to reduce our
uncertainty about all information sources when receiving one new data point, even if it
originates from a source with lower ﬁdelity. Thus, we obtain a more accurate estimate of
the true objective function from each sample.

The second key contribution is a method that decides the design x to evaluate next and
the information source (cid:96) that will be queried in a coordinated manner. Our approach, based
on a mathematically stringent value of information analysis, quantiﬁes the uncertainty
about the real system and information sources, in particular, the model discrepancies and
noise. To this end, we utilize an (arbitrarily accurate) approximation to the knowledge
gradient proposed in Frazier, Powell, and Dayanik [FPD09] that selects a pair ((cid:96), x) such
that the information gain relative to the speciﬁc query cost c(cid:96)(x) is maximized, where the
query cost can be incorporated as part of the objective in a natural way: our policy picks a
pair that oﬀers an optimal trade-oﬀ between predicted beneﬁt and cost.

Note that the computational cost of ﬁnding such an approximately optimal pair may
grow quadratically with the resolution of our representation of the domain; we address this
issue by a new two-level parallelization of the knowledge gradient algorithm that provides
an essentially linear speedup.

We also demonstrate that our model is capable of handling information sources whose
model discrepancies are interrelated in a more sophisticated way: in particular, we address
the scenario of groups of information sources whose models deviate from the truth in a
correlated fashion. For instance, in silico simulations of physical or chemical processes
might employ similar approximations whose deviations from the physical laws are thus
correlated, e.g., ﬁnite element analyses with diﬀerent mesh ﬁneness in Huang, Allen, Notz,
and Miller [HANM06] or calculations based on shared data sets. Additionally, experiments
conducted in the same location are exposed to the same environmental conditions or singular
events, thus the outputs of these experiments might deviate from the truth by more than
independent measurement errors. Another important factor is humans involved in the lab
work, as typically workers have received the comparable training and may have made similar
experiences during previous joint projects, which inﬂuences their actions and decisions.

2 The Model
Each design x is speciﬁed by d parameters. Given some compact set D ⊂ Rd of feasible
designs, our goal is to ﬁnd a best design under some objective function g : D → R, i.e. we
want to ﬁnd a design in argmaxx∈D g(x). Restrictions on D such as box constraints can be

4

easily incorporated in our model.
We have access to M possibly biased and/or noisy information sources IS 1,IS 2, . . . ,IS M
that give us information about g. We suppose that repeatedly observing IS (cid:96)(x) for some (cid:96)
and x provides independent and normally distributed observations with mean value f ((cid:96), x)
and variance λ(cid:96)(x). These sources are thought of as approximating g, with variable model
discrepancy or bias δ(cid:96)(x) = g(x) − f ((cid:96), x). If we wish to suppose that g can be observed
directly without bias (but possibly with noise), we can model this by supposing that
f ((cid:96), x) = g(x) for some (cid:96) > 0 and all x, but we do not need to make this assumption.
Each IS (cid:96) is also associated with a query cost function c(cid:96)(x) : D → R+. We assume that
the cost function c(cid:96)(x) and the variance function λ(cid:96)(x) are both known and continuously
diﬀerentiable. In practice, these functions may either be provided by domain experts or may
be estimated along with other model parameters from data.

For convenience of notation, we deﬁne an additional information source equal to our true

Our motivation in having the cost and noise vary over the space of designs is that
physical experiments may become diﬃcult to conduct and/or expensive when environmental
parameters are extreme. Moreover, the simulations may be limited to certain speciﬁed
parameter settings and their accuracy diminish quickly.
objective, IS 0 = g, which will not be available for observation.
We now place a single Gaussian process prior on f (i.e., on g and the mean response from
the M information sources). Let µ : [M ] × D (cid:55)→ R be the mean function of this Gaussian
process, and Σ : ([M ] × D)2 (cid:55)→ R be the covariance kernel. (Here, for any a ∈ Z+ we use [a]
as a shorthand for the set {1, 2, . . . , a}, and further deﬁne [a]0 = {0, 1, 2, . . . , a}.) Recall
that we did not impose further conditions on the nature of the unknown objective function g:
even if g is not continuous, the Gaussian process regression can be applied.

While our method can be used with an arbitrary mean function and positive semi-deﬁnite
covariance kernel, we now provide two concrete parameterized classes of mean functions and
covariance kernels that are useful for multi-information source optimization.

2.1 Independent Model Discrepancy
We ﬁrst propose a parameterized class of mean functions µ and covariance functions Σ
derived by supposing that model discrepancies are chosen independently across information
sources. This ﬁrst approach is appropriate when information sources are diﬀerent in kind
from each other and share no relationship except the fact that they are modeling a common
objective. Below we will propose a more general parameterized class that models correlation
between model discrepancies, as is typical when information sources can be partitioned into
groups, such that information sources within a group tend to agree more amongst themselves
than they do with information sources in other groups.
We suppose here that δ(cid:96) for each (cid:96) > 0 was drawn from a separate independent Gaussian
process, δ(cid:96) ∼ GP (µ(cid:96), Σ(cid:96)). We also suppose that δ0 is identically 0, and that f (0, x) ∼
GP (µ0, Σ0), for some given µ0 and Σ0. We then deﬁne f ((cid:96), x) = f (0, x) + δ(cid:96)(x) for each
(cid:96) ∈ [M ].

5

Typically, one would not have a strong prior belief as to the direction of the bias inherent
in an information source, and so we set µ(cid:96)(x) = 0. (If one does have a strong prior opinion
that an information source is biased in one direction, then one may instead set µ(cid:96) to a
constant to be estimated using maximum a posteriori estimation.)
With this modeling choice, we see that f ∼ GP (µ, Σ) with a mean function µ and

covariance kernel Σ that can be computed as follows:

µ((cid:96), x) = E [f ((cid:96), x)] = E [f (0, x)] + E [δ(cid:96)(x)] = µ0(x)

for each (cid:96) ∈ [M ]0, since E [δ(cid:96)(x)] = 0 holds. Additionally, for (cid:96), m ∈ [M ]0 and x, x(cid:48) ∈ D,

Σ(cid:0)((cid:96), x), (m, x(cid:48))(cid:1)

= Cov(f (0, x) + δ(cid:96)(x), f (0, x(cid:48)) + δm(x(cid:48)))
= Cov(f (0, x), f (0, x(cid:48))) + Cov(δ(cid:96)(x), δm(x(cid:48)))
= Σ0(x, x(cid:48)) + 1(cid:96),m · Σ(cid:96)(x, x(cid:48)),

where 1(cid:96),m denotes Kronecker’s delta, and where we have used independence of δ(cid:96), δm, and
f (0,·).
One would then suppose that the functions µ0(·) and Σ(cid:96)(·,·) with (cid:96) ∈ [M ]0 belong to
some parameterized class, and similarly for the observation noises λ(cid:96)(·), and the parameters
that best ﬁt the data may be chosen based on maximum a posteriori estimation from data,
as is commonly done in more traditional applications of Gaussian process regression [RW06].
For example, one might set µ0(·) and each λ(cid:96)(·) to a constant, and suppose that Σ(cid:96) each
belong to the class of Matèrn covariance kernels.

2.2 Correlated Model Discrepancies
Next we demonstrate that our approach is ﬂexible and can easily be extended to scenarios
where some of the information sources have correlated model discrepancies. This arises if
some sources share a common modeling approach, as for example, if one set of sources for
an airfoil modeling problem correspond to diﬀerent discretizations of a PDE that models
wing ﬂutter, while another set of sources correspond to diﬀerent discretizations of a diﬀerent
PDE that models airﬂow. Two information sources that solve the same PDE will be more
correlated than two that solve diﬀerent PDEs. This also arises if data was collected in
batches and thus is correlated over time. (See Sect. 1 for a discussion).
Formally, let P = {P1, . . . , PQ} denote a partition of [M ] and deﬁne the function k : [M ] →
[Q] that gives for each IS its corresponding partition in P . Let there be an independent
Gaussian process ε(k((cid:96)), x) ∼ GP (µk((cid:96)), Σk((cid:96))) for each partition. Again our approach is to
incorporate all Gaussian processes into a single one with prior distribution f ∼ GP (µ, Σ):1
therefore, for all (cid:96) ∈ [M ]0 and x ∈ D we deﬁne f ((cid:96), x) = f (0, x) + ε(k((cid:96)), x) + δ(cid:96)(x),

1For simplicity we reuse the notation from the above model to denote their pendants in this model.

6

where f (0, x) = g(x) is the objective function that we want to optimize. Due to linearity of
expectation, we have

µ((cid:96), x) = E [f (0, x) + ε(k((cid:96)), x) + δ(cid:96)(x)]

= E [f (0, x)] + E [ε(k((cid:96)), x)] + E [δ(cid:96)(x)]
= µ0(x),

since E [ε(k((cid:96)), x)] = E [δ(cid:96)(x)] = 0. Recall that the indicator variable 1(cid:96),m denotes Kro-
necker’s delta. Let (cid:96), m ∈ [M ]0 and x, x(cid:48) ∈ D, then we deﬁne the following composite
covariance function Σ

Σ(cid:0)((cid:96), x), (m, x(cid:48))(cid:1) =Cov(cid:0)f ((cid:96), x), f (m, x(cid:48))(cid:1)

=Cov(cid:0)f (0, x) + ε(k((cid:96)), x) + δ(cid:96)(x), f (0, x(cid:48)) + ε(k(m), x(cid:48)) + δm(x(cid:48))(cid:1)

=Σ0(x, x(cid:48)) + 1k((cid:96)),k(m) · Σk((cid:96))(x, x(cid:48)) + 1(cid:96),m · Σ(cid:96)(x, x(cid:48)).

3 The Value of Information Analysis
Our optimization algorithm proceeds in rounds, where in each round it selects a design x ∈ D
and an information source IS (cid:96) with (cid:96) ∈ [M ]. The goal is to ﬁnd an x that maximizes g(x)
over D.

Let us assume for the moment that the query cost is uniform over the whole domain
and all information sources; we will remove this assumption later. Further, assume that
we have already sampled n points X and made the observations Y . If we were to pick
an x ∈ D now irrevocably, then we would select an x of maximum expectation as follows: let
us denote by En [f ((cid:96), x)] the expected value according to the posterior distribution given X
and Y ; since that distribution is normal, the best expected objective value of any design, as
estimated by our statistical model, is maxx∈D En [f (0, x)] = maxx∈D µ(n) (0, x). Thus, we
choose the next design x(n+1) and information source (cid:96)(n+1) that we will sample in order to
maximize En
expected gain

(cid:2)maxx(cid:48)∈D µ(n+1)(0, x(cid:48))(cid:12)(cid:12) (cid:96)(n+1) = (cid:96), x(n+1) = x(cid:3), or equivalently maximize the
(cid:20)
x(cid:48)∈D µ(n+1)(0, x(cid:48))

− max
x(cid:48)∈D µ(n)(0, x(cid:48)).

(cid:12)(cid:12)(cid:12)(cid:12) (cid:96)(n+1) = (cid:96), x(n+1) = x

(cid:21)

(1)

En

max

Eq. (1) gives the value of information of each pair ((cid:96), x). Frazier et al. [FPD09] refer to
the stationary policy that selects (cid:96) and x to maximize that expected gain as the knowledge
gradient (KG). It follows immediately from the deﬁnition that the KG policy is optimal
under this Bayesian perspective, if one is allowed only one more sample. Further, they argue
that one may expect the KG policy to have a reasonable, albeit not necessarily optimal
performance, when used as heuristic to select multiple samples in a one-by-one fashion.

7

3.1 An Approximation of the Knowledge Gradient
Frazier et al. [FPD09] show how to compute the knowledge gradient exactly when there is
only a ﬁnite number of alternatives to choose from in each round. To apply their method, we
discretize the compact set D of designs into A, where we have some ﬂexibility in choosing |A|
elements of D, e.g., emphasizing certain promising designs based on experts’ knowledge at the
expense of other, rather exotic choices. For simplicity and to avoid computational overhead,
we assume that A is chosen initially and kept throughout the computation; however, we
point out that we still may adapt A later if we realize that a certain region of D turns out
to be of particular interest, due to the ﬂexibility of our statistical model and the algorithms
we use. Moreover, note that we have control over the approximation error introduced by
the discretization, as it is upper-bounded by the maximum distance of two discrete points
and the smoothness of f, which in turn is determined by our choice of the kernel. Summing
up, an approximation to the value of information in Eq. (1) is given by

(cid:20)

(cid:12)(cid:12)(cid:12)(cid:12) (cid:96)(n+1) = (cid:96), x(n+1) = x

(cid:21)

En

x(cid:48)∈A µ(n+1)(0, x(cid:48))

max

− max
x(cid:48)∈A µ(n)(0, x(cid:48)).

(2)

Next we show how to obtain the ﬁrst term, the conditional expectation for ﬁxed (cid:96) and x,
from the posterior distribution: recall that µ(n) is its mean vector and let Σ(n) denote its
covariance matrix. We deﬁne ˜σx(cid:48)((cid:96), x) =
. Observe that if we condition
on X and Y , then the random variables µ(n+1)(0, x(cid:48)) and µ(n)(0, x(cid:48)) + ˜σx(cid:48)((cid:96), x) · Z have the
same conditional distributions, where Z is a standard normal random variable: that is,

((cid:96),x),((cid:96),x)+λ(cid:96)(x)

(0,x(cid:48)),((cid:96),x)

(cid:113)

Σ(n)

Σ(n)

µ(n+1)(0, x(cid:48)) d= µ(n)(0, x(cid:48)) + ˜σx(cid:48)((cid:96), x) · Z

holds for every x(cid:48) ∈ A.

3.2 Computing the Knowledge Gradient in Parallel
We may apply Algorithm 2 of [FPD09] in our setting to calculate the approximation of the
knowledge gradient given in Eq. (2), which yields an exact maximizer to Eq. (2). However,

the running time of this algorithm is O(cid:0)M · |A|2(cid:1), and therefore may become a bottleneck in
diﬀerent choices of the next sample decision(cid:0)(cid:96)(n+1), x(n+1)(cid:1) are independent and thus can

probing the domain with high accuracy. Therefore, we present two modiﬁcations to address
the scalability of Algorithm 2: the ﬁrst stems from the observation that the computations for

be done in parallel; the only required communication is to scatter the data and afterwards
determine the best sample, hence the speedup is essentially linear. Note that instead
of enumerating all the (discrete) options in A for the next design x(n+1), we can also
use a gradient-based method to ﬁnd an optimal x(n+1) ∈ D in the continuous domain;
see [SFP11, TPF] for other usages of gradient-ascent techniques to compute (approximations
of) the knowledge gradient.

8

value of information of each(cid:0)(cid:96)(n+1), x(n+1)(cid:1). Thus, we oﬀer two levels of parallelization that

The second optimization is more intricate: we also parallelize the computation of the

can be used separately or combined to better utilize several multi-core CPUs of a cluster.
In particular, the ﬁrst modiﬁcation corresponds to a parallel execution of the outer loop
of Algorithm 2, whereas the second aims at parallelizing each iteration of the loop itself.
Due to space constraints we only provide a sketch of the parallel algorithm here and refer
the reader to [FPD09] for a detailed presentation of their algorithms that we will build on.
Let ρ : [M ] × A → [M · |A|] be a bijection and deﬁne the (M · |A|)-dimensional vectors ¯µn
ρ((cid:96)(cid:48),x(cid:48)) = µ(n)(0, x(cid:48)) and ¯σ((cid:96), x)ρ((cid:96)(cid:48),x(cid:48)) = ˜σx(cid:48)((cid:96), x) respectively. Then we can
and ¯σ((cid:96), x) as ¯µn
deﬁne in analogy to [FPD09] with Z ∼ N (0, 1)

(cid:20)

h((cid:126)a,(cid:126)b) = E

max

i

(cid:21)

and thus may restate our approximation of the knowledge gradient given in Eq. (2) as

(cid:20)

argmax
(cid:96)∈[M ],x∈A
= argmax
(cid:96)∈[M ],x∈A

En

x(cid:48)∈A µ(n+1)(0, x(cid:48))

max

h (¯µn, ¯σ((cid:96), x)) .

ai + bi · Z

− max

ai,

i

(cid:12)(cid:12)(cid:12)(cid:12) (cid:96)(n+1)=(cid:96), x(n+1)=x

(cid:21)

− max
x(cid:48)∈A µ(n)(0, x(cid:48))

We begin its computation by sorting the entries of ¯µn and ¯σ((cid:96), x) in parallel by ascend-
ing ¯σ((cid:96), x)-value; if multiple entries have the same ¯σ((cid:96), x), we only keep the entry with largest
value in ¯µn and discard all others, since they are dominated [FPD09]. W.l.o.g. we assume in
the sequel that both vectors do not contain dominated entries and that ¯σ((cid:96), x)i < ¯σ((cid:96), x)j
whenever i < j for i, j ∈ [M ·|A|]. However, there is another type of domination that is even
i + ¯σ((cid:96), x)i·z,
more important: for each z ∈ R it is suﬃcient to ﬁnd the g(z) := max argmaxi ¯µn
i + ¯σ((cid:96), x)i · z for any z: let n(cid:48)
which is equivalent to removing those i that never maximize ¯µn
be the number of sample points that remain after this step, and consider the sequence (c)
for i ∈ [n(cid:48) − 1], c0 = −∞ and cn(cid:48) = ∞; observe that the intervals
with ci =
between these points uniquely determine the respective g(z) for all z in that interval.

¯σ((cid:96),x)i+1−¯σ((cid:96),x)i

i −¯µn
¯µn

i+1

In order to parallelize Algorithm 1 of [FPD09] that determines undominated samples
(also called alternatives), we divide the previously sorted sequence (c) into p subsequences,
where p is the number of cores we wish to use. After running the linear scan algorithm
of [FPD09] on each subsequence separately and in parallel, we “merge” adjacent subsequences
pairwise in parallel: when merging two sequences, say L and R where L contains the smaller
elements, it suﬃces to search for the rightmost element in L not dominated by the leftmost
one in R (or vice versa): the reason is that we have ensured previously that no element is
dominated by another within each subsequence.
After at most (cid:100)log2 p(cid:101) merging rounds, each core has determined which of the elements in
its respective subsequence of (c) are not dominated as required by Algorithm 2. Note that
the “merging” procedure does not require actual transfers of the elements among cores. Hence
the ﬁnal step, the summation in Eq. (14) on p. 605 in [FPD09] that calculates h (¯µn, ¯σ((cid:96), x)),
is trivial to parallelize.

9

3.3 A Generalization to Variable Cost Functions
Finally, we show how the assumption made at the beginning of this section, that query cost are
uniform across the domain and for all information sources, can be removed. To this end, we
associate a query cost function c(cid:96)(x) : D → R+ with each information source IS (cid:96) for (cid:96) ∈ [M ].
Then our goal becomes to ﬁnd a sample ((cid:96)(n+1), x(n+1)) whose value of information divided
by its respective query cost is maximum. The gist is that conditioned on any ﬁxed sample the
expected gain of all x ∈ A (or even all x ∈ D) is scaled by the same value c(cid:96)(n+1)(x(n+1))−1.
Thus, we can use the same algorithm to determine the value of information of ((cid:96)(n+1), x(n+1))
as in the case of uniform cost. Then the knowledge gradient policy picks a sample ((cid:96), x) that
maximizes the expectation En
conditioned on (cid:96)(n+1)=(cid:96)
and x(n+1)=x, which is equivalent to maximizing the conditional expectation of h(¯µn,¯σ((cid:96),x))
,
where h is the function introduced in [FPD09] that computes value of the respective pair
(see also Sect. 3.2).

(cid:104) maxx(cid:48)∈A µ(n+1)(0,x(cid:48))−maxx(cid:48)∈A µ(n)(0,x(cid:48))

c(cid:96)(x)

(cid:105)

c(cid:96)(x)

4 Some Numerical Experiments

Before we demonstrate the usefulness of the new approach described above on some bench-
mark problems, we highlight some features of its implementation. The statistical model
and the value of information analysis were implemented in Python 2.7 and C++ using the
functionality provided by Metrics Optimization Engine [MOE], in particular for Gaus-
sian processes and ﬁtting hyper-parameters. We demonstrate the performance of the novel
multi-information source optimization algorithm, subsequently referred to as misoKG, by
comparing it with two other Bayesian global optimization algorithms. The ﬁrst algorithm
called misoEI is a state-of-the-art method developed by Lam et al. [LAW15] to solve MISO
problems, and therefore is a good competing method to compare with. The second technique
is the well-known EGO [JSW98] algorithm that was also used in [LAW15] as baseline method:
following Lam et al. [LAW15], EGO only chooses the design to sample next and then queries
every information source at this input. Thus, EGO gains signiﬁcantly more information that
its competitors in each step, and if the other methods beat EGO then this indicates a superior
value of information analysis. Note that the method clearly accumulates higher query cost,
thus we are less interested in this measure for EGO. The algorithm has been implemented as
part of [MOE], and we use this implementation for benchmark experiments.

We conducted numerical experiments on two test problems: the ﬁrst is a modiﬁed 2-d
Rosenbrock function tweaked into the MISO setting by Lam et al. [LAW15]. The second
problem takes a simulator of an assemble-to-order system [ATO, HN06]: here the objective
is to optimize an 8-dimensional conﬁguration vector in order to maximize the expected daily
proﬁt of a company, for which an estimated is provided as an output by the simulator. In
each scenario all three methods were given the same initial data. We report the “gain” over
the best initial solution, that is the value of the respective design a method would return
at each iteration minus the best value in the initial data set. Here we estimate the value

10

of a design as the output of the information source of highest ﬁdelity if the truth is not
known, as for the assemble-to-order problem. The axes of the plots are scaled by the natural
logarithm.

4.1 The Rosenbrock Benchmarks
We consider the design space D = [−2, 2]2, and M = 2 information sources. IS 1 is the
Rosenbrock function with an added Gaussian noise, and IS 2 is the Rosenbrock function
with an additional oscillatory component:

f (x) = (1 − x1)2 + 100 · (x2 − x2
1)2
y1(x) = f (x) + u · ε
y2(x) = f (x) + v · sin(10x1 + 5x2)

(3)

where x = (x1, x2)T ∈ D, ε is noise drawn from the standard normal distribution, and
u and v are conﬁguration constants that can vary when running experiments in diﬀerent
settings. We also assumed the noise and cost are known for each IS and constant over D.
We ran the benchmark experiments on two diﬀerent conﬁgurations to gain a better insight
of characteristics of the algorithms.

Since Lam et al. [LAW15] reported a good performance of their method on (3), we
replicated their experiment using the same parameters to compare the performance of the
three methods: that is, we set u = 0, v = 0.1, and the noise variance is 10−3 for IS 1 and
10−2 for IS 2. Furthermore, we assume a cost of 1000 for each query to IS 1 and of 1 for IS 2.
Since all three methods converged to an almost optimal solution quickly, we investigate the
ratio of improvement over cost: Fig. 1 (l) displays the gain of each method over the best
initial solution as a function of the total cost inﬂicted by querying information sources. We
see that the new method misoKG oﬀers a signiﬁcantly better ratio of gain to cost. A closer
analysis shows that misoKG converges to the overall optimum within 14 samples: the ﬁrst
seven queries are to the cheap information source and explore the domain; in total misoKG
queries the expensive IS 1 three times. On the other hand, misoEI requires 32 queries, six
of them to IS 1, before it gains over the best initial design for the ﬁrst time. A second
improvement is achieved after 92 samples, but it does not obtain the value of misoKG before
being stopped after a hundred samples.
For the second setup, we set u = 1, v = 2, a variance of 1 for IS 1 and of 5 for IS 2; further,
the cost for an invocation of IS 1 is 50 and 1 for IS 2. Note that now the diﬀerence of the
costs of both sources is much smaller, while their variances are considerably bigger. The
results are displayed in Fig. 1 (r): as for the ﬁrst conﬁguration, misoKG outperforms the other
methods. Interestingly, EGO seems to cope much better with these settings than misoEI,
whose performance is drastically decreased compared to the ﬁrst choice of parameters.
Looking closer, we see that misoKG initially queries only the cheap information source IS 2
until it comes close to an optimal value, and then starts to query IS 1 occasionally: misoKG
is very close to that value after ﬁve samples. misoEI queries only the expensive information

11

Figure 1: (l) The Rosenbrock benchmark with the parameter setting of [LAW15]: misoKG
substantially outperforms EGO and misoEI. (r) The Rosenbrock benchmark with
an alternative setup.

source IS 1 in this setting, and requires 95 samples until it obtains a solution of comparable,
albeit still worse, true objective value as misoKG. Note that the graphs oscillated slightly;
this phenomenon occurs if the respective method favored a design whose unknown true
value is in fact smaller than that of previous solutions.

4.2 The Assemble-To-Order Benchmark
In the assemble-to-order (ATO) benchmark problem we are managing the inventory of a
company that manufactures a variety of m products. Each of these products is made of a
selection from n items, where we distinguish for each product between key items and non-key
items: if the company runs out of key items, then it cannot sell the respective products until
it has restocked its inventory; non-key items are optional and used if available. When the
company sends a replenishment order, the required item is delivered after a random period
whose distribution is known. Since items in the inventory inﬂict holding cost, our goal is to
ﬁnd an optimal target inventory level vector b that determines the amount of each item we
want to stock, such that we maximize the expected proﬁt per day (see [HN06] for details).
[ATO] proposes a speciﬁc scenario with m = 5 diﬀerent products that depend on a subset
of n = 8 items, thus our task is to optimize the 8-dimensional target vector b ∈ [0, 20]8. For
each such vector the simulator of [ATO] provides an estimate of the expected daily proﬁt for
any b by running the simulation for a ﬁxed number of replications. Increasing this number
yields a more accurate estimate but also has higher computational cost. For this scenario
we set up four information sources and estimated their variance and cost, for the sake of
simplicity assuming that both functions are constant over the domain. The parameters are
summarized in Table 1. Fig. 2 (l) presents the gain versus the accumulated query cost of the
three methods: misoKG achieves a signiﬁcantly better ratio than its competitors, ﬁnding a

12

0246810log(total cost)0.00.51.01.52.02.53.03.54.0log(gain)misoKGmisoEIEGO012345678log(total cost)0.00.51.01.52.02.53.03.54.0log(gain)misoKGmisoEIEGOTable 1: The parameters for the assemble-to-order problem.

IS 1
IS 2
IS 3
IS 4

# Replications Variance
2.485
0.332
0.056
0.027

10
100
500
1000

Cost
8.887
11.422
24.633
41.327

Figure 2: (l) The gain-vs-cost graph for the assemble-to-order benchmark. (r) Information

sources invoked by misoKG for the assemble-to-order benchmark.

good solution quickly and at low query cost. In particular, the design found by misoKG has
a much higher objective value than misoEI and EGO. After 15 samples misoKG has obtained
a design of approximately highest objective value; during this time, three invocations are
to the cheapest source and the others to the second cheapest information source IS 2 (cp.
Fig. 2 (r)). misoEI on the other hand only queries the most expensive information source.

5 Conclusions

We have presented a novel algorithm for multi-information source optimization that is based
on a rigorous treatment of the uncertainties arising from the inherent model discrepancy of
information sources, i.e. their inability to model the reality accurately. The optimization
decision relies on a stringent value of information analysis that trades oﬀ the predicted
beneﬁt and its cost naturally.

The experimental evaluation demonstrates that our method consistently outperforms
other state-of-the-art techniques: our variant of the knowledge gradient ﬁnds designs of
considerably higher objective value and additionally inﬂicts less cost in the exploration
process.

13

02468log(total cost)0.00.51.01.52.02.53.03.54.0log(gain)misoKGmisoEIEGO020406080100Iteration012345log(gain)IS1IS2IS3IS4References

[ATO]

[AW14]

[BV04]

ATOSim. Assemble to order simulator. http://simopt.org/wiki/index.php?
title=Assemble_to_Order&oldid=447. Last accessed on 02/05/2016.

Douglas Allaire and Karen Willcox. A mathematical and computational frame-
work for multiﬁdelity design and analysis with computer models. International
Journal for Uncertainty Quantiﬁcation, 4(1), 2014.

Vladmir Balabanov and Gerhard Venter. Multi-ﬁdelity optimization with
high-ﬁdelity analysis and low-ﬁdelity gradients. In 10th AIAA/ISSMO Multi-
disciplinary Analysis and Optimization Conference, volume 4459, 2004.

[EGC04] Michael S Eldred, Anthony A Giunta, and Samuel S Collis. Second-order
corrections for surrogate-based optimization with model hierarchies. In Pro-
ceedings of the 10th AIAA/ISSMO Multidisciplinary Analysis and Optimization
Conference, pages 2013–2014, 2004.

[FPD09]

[FSK07]

Peter I Frazier, Warren B Powell, and Savas Dayanik. The Knowledge Gradi-
ent Policy for Correlated Normal Beliefs. INFORMS Journal on Computing,
21(4):599–613, 2009.

Alexander IJ Forrester, András Sóbester, and Andy J Keane. Multi-ﬁdelity
optimization via surrogate modelling. Proceedings of the Royal Society of
London A: Mathematical, Physical and Engineering Sciences, 463(2088):3251–
3269, 2007.

[HANM06] Deng Huang, TT Allen, WI Notz, and RA Miller. Sequential kriging opti-
mization using multiple-ﬁdelity evaluations. Structural and Multidisciplinary
Optimization, 32(5):369–382, 2006.

[HANZ06] Deng Huang, Theodore T Allen, William I Notz, and Ning Zeng. Global
Optimization of Stochastic Black-Box Systems via Sequential Kriging Meta-
Models. Journal of Global Optimization, 34(3):441–466, 2006.

[HN06]

[JSW98]

[KO00]

L Jeﬀ Hong and Barry L Nelson. Discrete optimization via simulation using
compass. Operations Research, 54(1):115–129, 2006.

Donald R Jones, Matthias Schonlau, and William J Welch. Eﬃcient Global
Optimization of Expensive Black-Box Functions. Journal of Global Optimization,
13(4):455–492, 1998.

Marc C Kennedy and Anthony O’Hagan. Predicting the output from a complex
computer code when fast approximations are available. Biometrika, 87(1):1–13,
2000.

14

[Kus64]

[LAW15]

[LK10]

[Moc89]

[MOE]

[MW12]

Harold J Kushner. A new method of locating the maximum point of an arbitrary
multipeak curve in the presence of noise. Journal of Fluids Engineering, 86(1):97–
106, 1964.

Remi Lam, Douglas Allaire, and Karen Willcox. Multiﬁdelity optimization
using statistical surrogate modeling for non-hierarchical information sources. In
56th AIAA/ASCE/AHS/ASC Structures, Structural Dynamics, and Materials
Conference, 2015.

Leifur Leifsson and Slawomir Koziel. Multi-ﬁdelity design optimization of
transonic airfoils using physics-based surrogate modeling and shape-preserving
response prediction. Journal of Computational Science, 1(2):98–106, 2010.

Jonas Mockus. Bayesian approach to global optimization: theory and applica-
tions. Kluwer Academic, Dordrecht, 1989.

MOE. Metrics optimization engine. http://yelp.github.io/MOE/. Last
accessed on 2016-01-21.

Andrew March and Karen Willcox. Provably convergent multiﬁdelity op-
timization algorithm not requiring high-ﬁdelity derivatives. AIAA journal,
50(5):1079–1089, 2012.

[QHS+05] Nestor V Queipo, Raphael T Haftka, Wei Shyy, Tushar Goel, Rajkumar
Vaidyanathan, and P Kevin Tucker. Surrogate-based analysis and optimization.
Progress in aerospace sciences, 41(1):1–28, 2005.

[RHK08]

[RW06]

[SFP11]

Dev Rajnarayan, Alex Haas, and Ilan Kroo. A multiﬁdelity gradient-free
optimization method and application to aerodynamic design. In Proceedings of
the 12th AIAA/ISSMO Multidisciplinary Analysis and Optimization Conference,
number 6020, 2008.
Carl Edward Rasmussen and Christopher K.˜I. Williams. Gaussian Processes
for Machine Learning. MIT Press, 2006.

Warren R. Scott, Peter I. Frazier, and Warren B. Powell. The correlated
knowledge gradient for simulation optimization of continuous parameters using
gaussian process regression. SIAM Journal on Optimization, 21(3):996–1026,
2011.

[SWMW89] Jerome Sacks, William J Welch, Toby J Mitchell, and Henry P Wynn. Design
and analysis of computer experiments. Statistical science, pages 409–423, 1989.

[TPF]

Saul Toscano-Palmerin and Peter Frazier. Stratiﬁed bayesian optimization. In
Submission.

15

[Win81]

Robert L Winkler. Combining probability distributions from dependent infor-
mation sources. Management Science, 27(4):479–488, 1981.

16

