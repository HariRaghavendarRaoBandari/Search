An Algorithm for Solving Quadratic Optimization Problems with

Nonlinear Equality Constraints

Tuan T. Nguyen, Mircea Lazar and Hans Butler

6
1
0
2

 
r
a

 

M
6
1

 
 
]

.

C
O
h
t
a
m

[
 
 

1
v
7
1
1
5
0

.

3
0
6
1
:
v
i
X
r
a

Abstract— The classical method to solve a quadratic op-
timization problem with nonlinear equality constraints is to
solve the Karush-Kuhn-Tucker (KKT) optimality conditions
using Newton’s method. This approach however is usually
computationally demanding, especially for large-scale problems.
This paper presents a new computationally efﬁcient algorithm
for solving quadratic optimization problems with nonlinear
equality constraints. It is proven that the proposed algorithm
converges locally to a solution of the KKT optimality conditions.
Two relevant application problems, ﬁtting of ellipses and state
reference generation for electrical machines, are presented to
demonstrate the effectiveness of the proposed algorithm.

I. INTRODUCTION

Quadratic optimization problems with nonlinear equality
constraints arise very frequently in science and engineering.
They have a wide range of applications in computer vi-
sion [1]–[4], mechatronics [5]–[7], system identiﬁcation [8],
[9], etc. They are also the heart of advanced optimization
solvers such as trust region method [10], [11] and sequential
quadratic programming [12], [13].

The classical method to solve a quadratic optimization
problem with nonlinear equality constraints is to solve the
Karush-Kuhn-Tucker (KKT) optimality conditions [14] using
the iterative Newton’s method [15]. The disadvantage of
this approach is that it introduces additional optimization
variables, i.e. Lagrange multipliers, which increases the com-
plexity of the problem. In addition, this approach requires
evaluation and storing of the Hessian matrix.

To reduce the computational load, this paper aims for
an optimization algorithm which bypasses the KKT opti-
mality conditions and implements a direct iterative search
for the optimum. The quadratic optimization problem with
nonlinear equality constraints is ﬁrst transformed into a least
2-norm problem of an underdetermined nonlinear system
of equations. A well known method to ﬁnd a solution of
an underdetermined system of equations is the generalized
Newton’s method [16]. However, this method only searches
for a feasible solution of the system of equations.

In this paper we propose an alternative algorithm which
searches for the minimum 2-norm solution of an underde-
termined system of equations. The update of the algorithm
at every iteration is deﬁned as an interpolation between
the generalized Newton’s algorithm update and a proposed
update that aims at minimizing the 2-norm of the solution. It
is proven that there exists a suitable interpolation coefﬁcient
such that the developed algorithm converges locally to a local

The authors are with the Department of Electrical Engineering, Eindhoven
University of Technology, P.O. Box 513, 5600 MB Eindhoven, The Nether-
lands. E-mails: {t.t.nguyen,m.lazar,h.butler}@tue.nl

minimum of the 2-norm solution of the underdetermined
system of equations.

Compared to the classical approach which solves the KKT
conditions using Newton’s method, the proposed algorithm
does not introduce additional optimization variables and does
not require evaluation of the Hessian matrix. Therefore,
the proposed algorithm requires less computational cost and
less amount of memory. This feature will be beneﬁcial for
large-scale problems and real-time applications where the
time for computation is limited. Two benchmark application
problems, ﬁtting of ellipses and state reference generation
for electrical machines, are presented to demonstrate the
effectiveness of the proposed algorithm.

The remainder of this paper is organized as follows.
Section II introduces the notation used in the paper. The
optimization problem is formulated in Section III. Section IV
presents the proposed numerical optimization algorithm and
proves the local convergence property of the algorithm.
Examples are shown in Section V to demonstrate the ef-
fectiveness of the algorithm. Section VI summarizes the
conclusions.

II. NOTATION

Let N denote the set of natural numbers, R denote the set
of real numbers, R>0 denote the set of positive real numbers.
The notation R[c1,c2) denotes the set {c ∈ R : c1 ≤ c < c2}.
Let Rn denote the set of real column vectors of dimension
n. Let Rn×m denote the set of real n × m matrices. For
a vector x ∈ Rn, x[i] denotes the ith element of x. The
notation 0n×m denotes the n×m zero matrix and In denotes
the n × n identity matrix. Let (cid:107) · (cid:107)2 denote the 2-norm. The
Nabla symbol ∇ denotes the gradient operator. For a vector
x ∈ Rn and a mapping Φ : Rn → R

(cid:104) ∂Φ(x)

(cid:105)

.

∇xΦ(x) =

(1)
Let B(x0, r) denote the open ball {x ∈ Rn : (cid:107)x−x0(cid:107)2 < r}.

∂x[1]

. . .

∂Φ(x)
∂x[2]

∂Φ(x)
∂x[n]

III. PROBLEM FORMULATION

Consider a quadratic optimization problem with nonlinear

equality constraints:

Problem III.1

subject to

zT P z + 2qT z

min

z

G(z) = 0m×1,

(cid:20)xk+1
(cid:21)
(cid:20)xk
(cid:21)

λk+1

=

λk

−(cid:16)∇2

The system of equations (7) is solved iteratively using
Newton’s method. Let xk and λk be the estimated values
of x and λ at any iteration k ∈ N. Then the new estimated
values of x and λ are computed as follows:

[xk,λk]T L(xk, λk)

[xk,λk]T L(xk, λk)

where ∇2
L(xk, λk):
∇2
[x,λ]T L(x, λ) =

(cid:20)∇2

is

(cid:17)−1 ∇[xk,λk]T L(xk, λk)T ,

(8)
the Hessian matrix of

(cid:21)

xL(x, λ) ∇xF (x)T
∇xF (x)
0m×m

.

(9)

The iterative process stops when a predeﬁned accuracy ε

is reached, i.e.

(cid:107)F (xk)(cid:107)2 ≤ ε,

(10)

where ε is a small positive value.

This algorithm converges locally to a local minimum of
Problem III.2 and the convergence rate is quadratic. The
proof of convergence of the Newton’s method can be found
in [17], [18]. However, this method introduces additional
variables,
i.e. Lagrange multipliers, which increases the
complexity of the problem. In addition, the need to evaluate
and store the Hessian matrix increases the computational
time and the amount of memory needed.
B. Proposed algorithm

In this section, we propose a new algorithm which does
not
introduce additional optimization variables and does
not require computation of the Hessian matrix. Instead of
formulating the optimality conditions (7), we directly solve
the system of equations

F (x) = 0m×1.

(11)

The proposed method to solve (11) is inspired by the
classical Newton’s method. The original idea of the iterative
Newton’s algorithm is to linearize F (x) around the current
guess xk at iteration k:

F (x) ≈ ˜F (x) := F (xk) + ∇F (xk)(x − xk).

where z ∈ Rn, q ∈ Rn, P ∈ Rn×n is a positive deﬁnite
matrix and G is a nonlinear mapping from Rn to Rm. Here, n
is the number of optimization variables and m is the number
of constraints. In this paper, we are only interested in the case
when the constraint set has an inﬁnite number of points, i.e.
m < n, since the other cases are trivial.

Let us deﬁne

x = P 1/2z + P −1/2q,

where x ∈ Rn. It follows that

z = P −1/2(x − P −1/2q).

(2)

(3)

The constraint can be rewritten in term of x as follows

G(z) = G(P −1/2(x − P −1/2q)) =: F (x).

(4)
It is obvious that F is also a mapping from Rn to Rm. The
cost function is also rewritten as a function of x as follows

zT P z + 2qT z = xT x − qT P −1q.

(5)

Since the optimization problem is not affected by the afﬁne
term qT P −1q, Problem III.1 reduces to

Problem III.2

xT x

min

x

subject to

F (x) = 0m×1.

The above derivation shows that any quadratic optimiza-
tion problems in the form of Problem III.1 can be trans-
formed into the form of Problem III.2. Therefore, instead
of solving Problem III.1, we can solve Problem III.2 to get
the solution in x. The solution in z can then be deduced
from (3).

IV. PROPOSED ALGORITHM

This section ﬁrst reviews the standard approach to solve
a quadratic optimization problem with nonlinear equality
constraints using Lagrange multipliers and Newton’s method.
This method will be referred to as the Lagrange-Newton
method for short. Then, the proposed algorithm for solving
Problem III.2 is presented.

A. Lagrange-Newton method

Then the new guess xk+1 of the solution has to satisfy

A common approach to solve the optimization prob-
lem III.2 is to solve the set of optimality conditions, which
are known as the KKT conditions, using the Newton’s
method.

First, deﬁne the Lagrange function

L(x, λ) = xT x + λT F (x),

(6)
where λ ∈ Rm is the vector of Lagrange multipliers. The
optimality conditions of Problem III.2 are:

∇[x,λ]T L(x, λ)T =(cid:2)∇xL(x, λ) ∇λL(x, λ)(cid:3)T

(cid:20)2x + ∇xF (x)T λ
(cid:21)

= 0(n+m)×1.

(7)

=

F (x)

˜F (xk+1) = 0m×1,

which is equivalent to

∇F (xk)xk+1 = ∇F (xk)xk − F (xk).

Since the system of equations (13) is underdetermined,
i.e. m < n, it has an inﬁnite number of solutions. A feasible
solution of (13) is

xk+1 = xk − ∇F (xk)−RF (xk), k = 0, 1, . . . ,

(15)
where ∇F (xk)−R is the minimum 2-norm generalized in-
verse, or the right inverse of ∇F (xk) [19]:

∇F (xk)−R = ∇F (xk)T (∇F (xk)∇F (xk)T )−1.

(16)

(12)

(13)

(14)

The update (15) is known as the generalized Newton’s
method for underdetermined systems of equations, which
provides a feasible solution of (13). However, instead of
just computing a feasible solution, we can compute the
minimum 2-norm solution of (13), which is the solution that
has smallest value of xT

k+1xk+1, i.e.

xk+1 = ∇F (xk)−R(∇F (xk)xk − F (xk)),

(17)

(18)

or, equivalently
xk+1 = ∇F (xk)−R∇F (xk)xk − ∇F (xk)−RF (xk).
The generalized Newton’s method (15) only provides a
feasible solution of the system of equations (11), but its
local convergence has been proven in [16], [20]. On the
other hand, the new proposed iteration (18) provides the min-
imum 2-norm solution of (11), but its convergence has not
been proven yet. From the authors’ experience, sometimes
iteration (18) does not converge even if the initial guess is
very close to the minimum 2-norm solution of (11). To solve
this issue of (18), we propose a new iteration, which is the
interpolation between iteration (15) and iteration (18), i.e.

xk+1 = αxk + (1 − α)∇F (xk)−R∇F (xk)xk
−∇F (xk)−RF (xk),

(19)

where 0 < α < 1. The proposed iteration (19) inherits the
local convergence property of (15), while it converges to the
minimum 2-norm ﬁxed point of (18), as it will be outlined
next.

In what follows we will prove that iteration (19) converges
locally to a ﬁxed point that satisﬁes the KKT optimality
conditions (7).

First, we introduce simpliﬁed notation for brevity. Let us

denote

as the Jacobian matrix of F (xk), and
Tk := T (xk) = ∇F (xk)−R = J T

(21)
as the right inverse of Jk. Here, Jk ∈ Rm×n and Tk ∈
Rn×m. It follows that:

k (JkJ T

k )−1

Jk := J(xk) = ∇F (xk)

(20)

matrix:

Then
(cid:107)F (x)−F (y)−J(y)(x−y)(cid:107)2 ≤ γ
2

(cid:107)x−y(cid:107)2

2, for all x, y ∈ D.
(28)

Proof: This result can be proved using the mean value

theorem:

(cid:107)F (x) − F (y) − J(y)(x − y)(cid:107)2

= (cid:107)

(J(y + φ(x − y)) − J(y)) (x − y)dφ(cid:107)2

(cid:107) (J(y + φ(x − y)) − J(y)) (x − y)(cid:107)2dφ

(cid:107) (J(y + φ(x − y)) − J(y))(cid:107)2(cid:107)(x − y)(cid:107)2dφ

0

(cid:90) 1
(cid:90) 1
(cid:90) 1
(cid:90) 1

0

0

≤

≤

≤

=

γφ(cid:107)(x − y)(cid:107)2
(cid:107)x − y(cid:107)2
2.

2dφ

0
γ
2

(29)

Lemma IV.2 Let D be a subset of Rn where J(x) and T (x)
are well deﬁned. For any x ∈ D, it holds that
(cid:107)In − T (x)J(x)(cid:107)2 = 1.

(30)

Proof:

First we will prove that (T (x)J(x)) is a

Hermitian matrix. We have:

(T (x)J(x))T = (J(x)T (J(x)J(x)T )−1J(x))T

= J(x)T (J(x)J(x)T )−1J(x)
= T (x)J(x).

(31)
Next we will prove that (In−T (x)J(x)) is an idempotent

(In − T (x)J(x))(In − T (x)J(x))

= In − 2T (x)J(x) + T (x)J(x)T (x)J(x)
= In − T (x)J(x).

(32)
Denote A := In − T (x)J(x). Then A is a Hermitian

idempotent matrix, i.e.

Assume that x∗ is a ﬁxed point of (19), let us denote

JkTk = Im.

F ∗ := F (x∗),
J∗ := J(x∗),
T ∗ := T (x∗).

(22)

(23)
(24)
(25)

With the simpliﬁed notation, the proposed iteration (19) can
be rewritten as:

xk+1 = αxk + (1 − α)TkJkxk − TkFk.

(26)
Lemma IV.1 Let D be a convex subset of Rn in which F :
D → Rm is differentiable and J(x) is Lipschitz continuous
for all x ∈ D, i.e. there exists a γ > 0 such that

(cid:107)J(x) − J(y)(cid:107)2 ≤ γ(cid:107)x − y(cid:107)2, for all x, y ∈ D.

(27)

AT = A,
AA = A.

Let ρi be an eigenvalue of AT A, then ρi satisﬁes

ρix = AT Ax

= AT A(AT Ax)
= AT A(ρix)
= ρi(AT Ax)
= ρ2

i x, ∀x ∈ D.

ρi = ρ2
i ,

This results in

(33)
(34)

(35)

(36)

which means that ρi is equal to either 0 or 1. If AT A is a
nonzero matrix then at least one eigenvalue is 1. The 2-norm

of A is deﬁned as the maximum singular value of A. This
is equal to the square root of the maximum eigenvalue of
AT A, which is equal to 1.

Proof: Left-multiplying (26) with Jk yields that

Jk(xk+1 − xk) = −Fk.

(47)

Theorem IV.3 If algorithm (26) converges to a ﬁxed point
x∗, then x∗ satisﬁes the KKT optimality conditions (7).
Proof: Left-multiplying (26) with TkJk, we have
TkJkxk+1 = αTkJkxk + (1 − α)TkJkxk − TkFk.

(37)

Subtracting (37) from (26) results in

xk+1 = αxk − αTkJkxk + TkJkxk+1.

(38)

Consequently, if algorithm (26) converges to a ﬁxed point
x∗ then

x∗ = αx∗ − αT ∗J∗x∗ + T ∗J∗x∗

= αx∗ + (1 − α)T ∗J∗x∗.

Besides, due to (26), x∗ also satisﬁes

x∗ = αx∗ + (1 − α)T ∗J∗x∗ − T ∗F ∗.

(39)

(40)

Subtracting (40) from (39) results in
T ∗F ∗ = 0n×1.

(41)
Here, T ∗ is a n × m matrix where m < n. This means that
T ∗ has an empty null space. Therefore, (41) is equivalent to
(42)

F ∗ = 0m×1.

From (39) and the fact that α < 1, it follows that

x∗ = T ∗J∗x∗,

(43)

which is equivalent to

(In − T ∗J∗)x∗ = 0n×1.

(44)
This means that x∗ is in the null space of (In − T ∗J∗),
which is the range of J∗T [21]. Therefore, there exists a
vector λ ∈ Rm such that

x∗ = − 1
2

J∗T λ,

(45)

or equivalently

(46)
From (42) and (46), we conclude that x∗ satisﬁes the KKT

2x∗ + J∗T λ = 0n×1.

optimality conditions (7).
Theorem IV.4 Let D ⊆ Rn be an open convex invariant set
for (26) in which the following conditions hold

(i) F (x) is Lipschitz continuous,
(ii) J(x) is well deﬁned and Lipschitz continuous,
(iii) T (x) is well deﬁned and bounded,
(iv) T (x)J(x) and T (x)J(x)x are Lipschitz continuous,
(v) there exists a solution x∗ of the KKT optimality condi-
Then there exist a α ∈ R(0,1) and a r ∈ R>0 such that
B(x∗, r) ⊆ D and iteration (26) converges to a ﬁxed point
that satisﬁes (7) for any initial condition x0 ∈ B(x∗, r).

tions (7) in D.

We have

xk+1 − xk

= α(xk − xk−1) + (1 − α)(TkJkxk − Tk−1Jk−1xk−1)

−(TkFk − Tk−1Fk−1)

= α(xk − xk−1) + (1 − α)(TkJkxk − Tk−1Jk−1xk−1)

−TkFk − Tk−1Jk−1(xk − xk−1)
= α(In − Tk−1Jk−1)(xk − xk−1)

+(1 − α)(TkJk − Tk−1Jk−1)xk − TkFk.

(48)
Due to the condition that T (x) is bounded for all x ∈ D

and Lemma IV.1, we have:
(cid:107)TkFk(cid:107)2 ≤(cid:107)Tk(cid:107)2(cid:107)Fk(cid:107)2

=(cid:107)Tk(cid:107)2(cid:107)Fk − Fk−1 − Jk−1(xk − xk−1)(cid:107)2
≤L(cid:107)xk − xk−1(cid:107)2
2,

(49)

where L > 0.
ous for all x ∈ D, it follows that:

Due to the condition that T (x)J(x) is Lipschitz continu-

(cid:107)(TkJk − Tk−1Jk−1)xk(cid:107)2 ≤ N(cid:107)xk − xk−1(cid:107)2,

(50)

Lemma IV.2 states that (cid:107)In − Tk−1Jk−1(cid:107)2 = 1. Conse-

where N > 0.

quently:

(cid:107)(In − Tk−1Jk−1)(xk − xk−1)(cid:107)2

≤ (cid:107)(In − Tk−1Jk−1)(cid:107)2(cid:107)(xk − xk−1)(cid:107)2
= (cid:107)xk − xk−1(cid:107)2.

(51)
The equality happens if and only if (xk − xk−1) is in
the range of (In − Tk−1Jk−1), which is the null space of
Jk−1 [21]. This is equivalent to:

Jk−1(xk − xk−1) = −Fk−1 = 0m×1.

(52)

This shows that the equality happens if and only if xk−1 is
an exact solution of (11). Let us assume that Fk−1 (cid:54)= 0m×1
for any k < ∞. Then there exists a constant M ∈ R(0,1)
such that
(cid:107)(In − Tk−1Jk−1)(xk − xk−1)(cid:107)2 ≤ M(cid:107)xk − xk−1(cid:107)2. (53)
From (48), (49), (50) and (53), it follows that
(cid:107)xk+1 − xk(cid:107)2 ≤ K(cid:107)xk − xk−1(cid:107)2 + L(cid:107)xk − xk−1(cid:107)2

2

= (K + L(cid:107)xk − xk−1(cid:107)2)(cid:107)xk − xk−1(cid:107)2,

(54)
where K = αM + (1 − α)N. If N ≤ 1 then K < 1 for all
α ∈ R(0,1). If N > 1 then K < 1 if and only if

N − 1
N − M

(55)
Therefore, there always exists an α ∈ R(0,1) such that K <
1.

< α < 1

From (26) and the conditions that F (x) and T (x)J(x)x

are Lipschitz continuous, we have
(cid:107)x1 − x0(cid:107)2 = (cid:107)(1 − α)(T0J0x0 − x0) − T0F0(cid:107)2

= (cid:107)(1 − α)(T0J0x0 − x0 − T ∗J∗x∗ + x∗)

−(T0F0 − T0F ∗)(cid:107)2

= (cid:107)(1 − α)(T0J0x0 − T ∗J∗x∗)

−(1 − α)(x0 − x∗) − T0(F0 − F ∗)(cid:107)2

= (cid:107)(1 − α)(T0J0x0 − T ∗J∗x∗)(cid:107)2

+(cid:107)(1 − α)(x0 − x∗)(cid:107)2 + (cid:107)T0(F0 − F ∗)(cid:107)2
(56)

≤ Q(cid:107)x0 − x∗(cid:107)2,

where Q > 0. If x0 is close enough to x∗ such that

(cid:107)x0 − x∗(cid:107)2 <

1 − K
QL

,

then it follows that

K + L(cid:107)x1 − x0(cid:107)2 < 1.

Next, we will prove that if

then

K + L(cid:107)xk+1 − xk(cid:107)2 < 1,

K + L(cid:107)xk+2 − xk+1(cid:107)2 < 1.

Indeed, if (59) holds then due to (54) we have
(cid:107)xk+2 − xk+1(cid:107)2 < (cid:107)xk+1 − xk(cid:107)2.

(57)

(58)

(59)

(60)

(61)

This leads to
K + L(cid:107)xk+2 − xk+1(cid:107)2 < K + L(cid:107)xk+1 − xk(cid:107)2 < 1. (62)
We have proved that if (59) holds then (60) holds. Since (58)
also holds for any x0 which satisﬁes (57), it follows by
induction that

K + L(cid:107)xk+1 − xk(cid:107)2 < 1, ∀k = 0, 1, . . . .

(63)

Therefore, it follows from (54) that

(cid:107)xk+2 − xk+1(cid:107)2 < (cid:107)xk+1 − xk(cid:107)2, ∀k = 0, 1, . . . .

(64)

and

Therefore, algorithm (26) converges, and by Theorem IV.3,
it converges to a solution of (7) for any x0 ∈ B(x∗, r), where

r =

,

(65)

In the case when B(cid:16)

and α satisﬁes (55).

1 − K
QL

(cid:17)

x∗, 1−K

inside D, then r can be chosen as the solution of

QL

does not lie completely

max

r

r

subject to

B(x∗, r) ⊆ D.

It is worth mentioning that the assumptions on Lipschitz
continuity of F (x), J(x) and boundedness of T (x) are
commonly used in proving convergence of Newton-based
algorithms. Different authors also use different additional

where

assumptions such that the convergence holds, see for exam-
ple [16], [20], [22]–[24].

Remark IV.5 In the case when the equality constraint (11)
is nonconvex, the ﬁxed point of both the Lagrange-Newton
method and the proposed method can be either a local
minimum or a local maximum. To determined whether it
is a local minimum or a local maximum, it is necessary to
check the second-order conditions or to check the value of
the cost function in the vicinity of the ﬁxed point. How to
guarantee convergence of the proposed algorithm to a global
minimum will be the subject of future research.

In summary, similar to the classical Lagrange-Newton al-
gorithm, the proposed algorithm converges locally to a KKT
point of Problem III.2. However, the proposed algorithm
requires less computational cost and less amount of memory
than the Lagrange-Newton algorithm. This is beneﬁcial for
large-scale problems and real-time applications.

V. EXAMPLES

This section presents two examples to verify the perfor-

mance of the proposed algorithm.

A. Least square ﬁtting of ellipses

Fitting of ellipses to data points is a fundamental task in
pattern recognition and computer vision. This problem has
been extensively studied and widely applied. In this example,
we follow the direct least square ﬁtting method proposed
in [3].

An ellipse in 2D (x, y)-coordinate can be represented by

a second order polynomial:

Γ(θ, η) = θT η = ax2 + bxy + cy2 + dx + ey + f = 0, (66)

where θ is the coefﬁcients vector:

θ =(cid:2)a b
η =(cid:2)x2 xy

f(cid:3)T
1(cid:3)T

,

c d e

y2 x y

(67)

(68)

.

The polynomial Γ(θ, η) is called the algebraic distance of a
point (x, y) to the ellipse Γ(θ, η) = 0. An approach to ﬁt
an ellipse to Np data points (xi, yi), i = 1, . . . , Np, is by
minimizing the sum of squared algebraic distances:

Np(cid:88)
D =(cid:2)η1

i=1

Γ(θ, ηi) = θT DT Dθ,

(69)

(70)

.

(cid:3)T

···

η2

ηNp

In addition, an ellipse has to satisfy the constraint that the
discriminant b2−ac is negative. Since we have the freedom to
arbitrarily scale the parameters, the constraint can be written
as

θT Sθ = −1,

(71)

where



0
0
−2
0
0
0

0 −2
0
1
0
0
0
0
0
0
0
0

S =

 .

0 0 0
0
0
0
0
0
0
0
0
0
0

0
0
0
0
0

(72)

From (69) and (71), the ellipse ﬁtting problem can be

formulated as

Problem V.1

θT Rθ

min

θ

subject to

θT Sθ + 1 = 0,

where R = DT D is positive deﬁnite. Following the trans-
formation derived in Section III, Problem V.1 can be trans-
formed into

Problem V.2

ϕT ϕ

min

ϕ

subject to

ϕT Hϕ + 1 = 0,

where

ϕ = R1/2θ,

H = R−1/2SR−1/2.

(73)

In this example, 11 data points are generated from the
ellipse shown in Fig. 1. The data points are then corrupted by
adding zero-mean Gaussian noise of standard deviation σ =
1.

The ﬁtting problem is solved using both the Lagrange-
Newton algorithm and the proposed algorithm. Since the
proposed algorithm can only solve problems in the form
of Problem V.2, it has to spend time on transforming the
original Problem V.1 into Problem V.2. For a fair com-
parison, the proposed algorithm solves Problems V.2, while
the Lagrange-Newton algorithm solves directly the original
Problem V.1, in order to avoid wasting computation time on
the transformation (73).
The algorithms are implemented on a 2.4GHz computer.
The predeﬁned accuracy is ε = 10−4. The initial point
for the Lagrange-Newton algorithm is chosen as θ0 =

(cid:2)1 1 1 1 1 1(cid:3)T , and the initial point for the proposed

algorithm is chosen as ϕ0 = R1/2θ0. For the proposed
algorithm, α is chosen equal to 0.2.

Both algorithms converge to the same solution as shown
in Fig. 1. The Lagrange-Newton algorithm converges after
4 iterations, while the proposed algorithm converges after
5 iterations. However, despite the fact that the proposed
algorithm needs more iterations to converges and also has to
calculate the transformation (73), its total computation time
is 0.43ms, while the total computation time of the Lagrange-
Newton algorithm is 0.62ms. If α is chosen equal to 0.1 then
the proposed algorithm also converges after 4 iterations like

Fig. 1. The original ellipse and the returned ellipses.

the Lagrange-Newton algorithm, and the total computation
time reduces further to 0.39ms. This demonstrates the ad-
vantage of the proposed algorithm in computation time.

B. State reference generation for torque control in externally
excited synchronous machines

State reference generation for torque control is a chal-
lenging problem for externally excited synchronous machines
(EESM). It requires solving a quadratic optimization problem
with quadratic equality constraints in real-time [7]. This
section solves the EESM state reference problem using the
proposed algorithm.

The electromagnetic torque of a EESM is calculated as

follows

Te =

3Pp
2

(Mdiqsie + (Ld − Lq)idsiqs) ,

(74)

where ids and iqs are the d- and q-axis stator currents, ie is
the excitation current, Pp is the number of pole pairs, Md
is the mutual inductance between stator and rotor windings,
Ld and Lq are the d- and q-axis inductances, respectively.

The copper losses is calculated as

PCu =

3
2

Rs(i2

ds + i2

qs) + Rei2
e,

(75)

where Rs is the stator resistance and Re is the excitation
resistance.

In torque control of EESM, the state variables can be

(cid:113) 3Rs

2

iqs

(cid:105)T

√

ie

Re

,

(76)

(77)

(78)

chosen as

(cid:104)

(cid:113) 3Rs

2

x =

ids

and the output is the electromagnetic torque

The torque equation (74) can be rewritten as

y = Te.

y = xT Cx,

−200−1000100200300400−200−1000100200300400xyEllipse fittingoriginal ellipsegenerated data pointsLagrange−Newton algorithmproposed algorithmwhere

C =



0

Pp(Ld−Lq)

2Rs
0

Pp(Ld−Lq)

2Rs
0

(cid:113) 2

3PpMd

4

3RsRe

The copper losses can be rewritten as

 .

(cid:113) 2

0

3RsRe

0

(79)

3PpMd

4

PCu = xT x.

(80)
For torque control, let yref be the reference torque, we
have to ﬁnd a value for the state variable x such that the
torque y is equal to the reference yref . There is an inﬁnite
number of solutions for this problem. When there is an
inﬁnite number of solutions, we have the freedom to choose
the solution which is beneﬁcial for the application. An
attractive solution is the one that minimizes the copper losses.
The state reference generation problem can be formulated as

Problem V.3

xT x

min

x

subject to

xT Cx − yref = 0.

In this example, let us consider an EESM with the param-
eters given in Table I. The reference torque is yref = 10Nm.

TABLE I

EESM PARAMETERS

Parameter
Number of pole pairs
Stator phase resistance
Excitation winding resistance
Mutual inductance
d-axis inductance
q-axis inductance

Symbol

Pp
Rs
Re
M d
Ld
Lq

Value

8

0.00775

7.4

9.069
0.1488
0.2264

Unit

Ω
Ω
mH
mH
mH

Problem V.3 is solved using both the Lagrange-Newton
algorithm and the proposed algorithm. The algorithms are
implemented on a 2.4GHz computer. The predeﬁned accu-
racy is ε = 10−7. The initial point for both algorithms is

1(cid:3)T . For the proposed algorithm,

chosen as x0 =(cid:2)−1 1

α is chosen equal to 0.3.

Both algorithms converge after 7 iterations. The resulting

solutions are the same for both algorithms:

x∗ =(cid:2)−1.083 5.133 5.017(cid:3)T

The resulting currents are

ids = −10.046(A),
ids = 47.604(A),
ie = 1.844(A).

.

(81)

(82)
(83)
(84)

Although both algorithms converge to the same solutions, the
total computational time of the Lagrange-Newton algorithm

is 0.6ms, while the total computational time of the proposed
algorithm is 0.3ms. This demonstrates the improvement in
computational time that the proposed algorithm can bring.
The algorithm is thus beneﬁcial for real-time machines with
high sampling frequency where the time for computation
is limited and implementation of embedded optimization
solvers is impractical.

VI. CONCLUSIONS

This paper proposed a new algorithm for solving quadratic
optimization problems with nonlinear equality constraints. It
was proven that the proposed algorithm converges locally to
a solution of the KKT optimality conditions. The algorithm
is computationally efﬁcient since it does not introduces addi-
tional optimization variables and does not require evaluation
of the Hessian matrix. The effectiveness of the proposed
algorithm was demonstrated in two examples.

For future research, how to guarantee convergence of
the proposed algorithm to a global minimum is of interest.
Another interesting topic is how to compute the interpolation
coefﬁcient such that
the fastest speed of convergence is
achieved.

ACKNOWLEDGMENT

The authors are grateful to Dr. Sabin - Constantin Carpiuc

for his help with the EESM example.

REFERENCES

[1] W. Gander, G. H. Golub, and R. Strebel, “Least-squares ﬁtting of
circles and ellipses,” BIT Numerical Mathematics, vol. 34, no. 4, pp.
558–578.

[2] R. Halif and J. Flusser, “Numerically stable direct least squares ﬁtting
of ellipses,” in Proceedings of the Sixth International Conference on
Computer Graphics and Visualization, vol. 1, 1998, pp. 125–132.

[3] A. Fitzgibbon, M. Pilu, and R. B. Fisher, “Direct least square ﬁtting
of ellipses,” IEEE Transactions on Pattern Analysis and Machine
Intelligence, vol. 21, no. 5, pp. 476–480, May 1999.

[4] S. X. Yu and J. Shi, “Segmentation given partial grouping constraints,”
Pattern Analysis and Machine Intelligence, IEEE Transaction on,
vol. 26, no. 2, pp. 173–183, Jan. 2004.

[5] D. C. Meeker, “Optimal solutions to the inverse problem in quadratic
magnetic actuators,” Ph.D. dissertation, University of Virginia, 1996.
[6] S. D. Ruben and T.-C. Tsao, “Real-time optimal commutation for min-
imizing thermally induced inaccuracy in multi-motor driven stages,”
Automatica, vol. 48, no. 8, pp. 1566 – 1574, 2012.

[7] S. C. Carpiuc and M. Lazar, “Efﬁcient state reference generation for
torque control in externally excited synchronous machines,” ASME
Journal of Dynamic Systems, Measurement, and Control, vol. 137,
no. 5, May 2015.

[8] T. H. V. Pelt and D. S. Bernstein, “Quadratically constrained least
squares identiﬁcation,” in American Control Conference, 2001. Pro-
ceedings of the 2001, vol. 5, 2001, pp. 3684–3689 vol.5.

[9] A. Yeredor and B. D. Moor, “On homogeneous least-squares problems
and the inconsistency introduced by mis-constraining,” Computational
Statistics and Data Analysis, vol. 47, no. 3, pp. 455 – 465, 2004.

[10] J. J. More, “Generalizations of the trust region problem,” Optimization

Methods and Software, vol. 2, no. 3-4, pp. 189–209, 1993.

[11] F. Rendl and H. Wolkowicz, “A semideﬁnite framework for trust
region subproblems with applications to large scale minimization,”
Mathematical Programming, vol. 77, no. 1, pp. 273–299, 1997.

[12] Z. Zhu, “A sequential equality constrained quadratic programming
algorithm for inequality constrained optimization,” Journal of Com-
putational and Applied Mathematics, vol. 212, no. 1, pp. 112 – 125,
2008.

[13] J. L. Morales, J. Nocedal, and Y. Wu, “A sequential quadratic
programming algorithm with an additional equality constrained phase,”
IMA Journal of Numerical Analysis, 2011.

[16] Y. Levin and A. Ben-Israel, “A Newton method for systems of m
equations in n variables,” Nonlinear Analysis: Theory, Methods and
Applications, vol. 47, no. 3, pp. 1961 – 1971, 2001, proceedings of
the Third World Congress of Nonlinear Analysts.

[17] L. V. Kantorovich and G. P. Akilov, Functional Analysis in Normed

Spaces. New York: Pergamon Press, 1964.

[18] L. B. Rall, Computational Solution of Nonlinear Operator Equations.

New York: Wiley, 1969.

[14] H. W. Kuhn and A. W. Tucker, “Nonlinear programming,” in Proceed-
ings of the Second Berkeley Symposium on Mathematical Statistics and
Probability. Berkeley, Calif.: University of California Press, 1951,
pp. 481–492.

[15] J. Nocedal and S. Wright, Numerical Optimization, 2nd ed. New

York, NY, USA: Springer-Verlag, 2006.

[19] C. R. Rao and S. K. Mitra, “Generalized inverse of a matrix and
its applications,” in Proceedings of the Sixth Berkeley Symposium
on Mathematical Statistics and Probability, Volume 1: Theory of
Statistics. Berkeley, Calif.: University of California Press, 1972, pp.
601–620.

[20] A. Ben-Israel, “A Newton-Raphson method for the solution of systems
of equations,” Journal of Mathematical Analysis and Applications,
vol. 15, no. 2, pp. 243 – 252, 1966.

[21] D. S. Bernstein, Matrix Mathematics: Theory, Facts, and Formulas
(Second Edition), ser. Princeton reference. Princeton University Press,
2009.

[22] W. M. H¨außler, “A Kantorovich-type convergence analysis for the
Gauss-Newton-method,” Numerische Mathematik, vol. 48, no. 1, pp.
119–125, 1986.

[23] M. Z. Nashed and X. Chen, “Convergence of Newton-like methods
for singular operator equations using outer inverses,” Numerische
Mathematik, vol. 66, pp. 235–257, 1993.

[24] X. Chen, Z. Nashed, and L. Qi, “Convergence of Newton’s method
for singular smooth and nonsmooth equations using adaptive outer
inverses,” SIAM Journal on Optimization, vol. 7, no. 2, pp. 445–462,
1997.

