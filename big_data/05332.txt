6
1
0
2

 
r
a

M
 
7
1

 
 
]

.

A
N
h
t
a
m

[
 
 

1
v
2
3
3
5
0

.

3
0
6
1
:
v
i
X
r
a

REGULARIZATION BASED ON ALL-AT-ONCE FORMULATIONS

FOR INVERSE PROBLEMS∗

BARBARA KALTENBACHER†

Abstract. Parameter identiﬁcation problems typically consist of a model equation, e.g. a (sys-
tem of) ordinary or partial diﬀerential equation(s), and the observation equation. In the conventional
reduced setting, the model equation is eliminated via the parameter-to-state map. Alternatively, one
might consider both sets of equations (model and observations) as one large system, to which some
regularization method is applied. The choice of the formulation (reduced or all-at-once) can make a
large diﬀerence computationally, depending on which regularization method is used: Whereas almost
the same optimality system arises for the reduced and the all-at-once Tikhonov method, the situation
is diﬀerent for iterative methods, especially in the context of nonlinear models. In this paper we will
exemplarily provide some convergence results for all-at-once versions of variational, Newton type and
gradient based regularization methods. Moreover we will compare the implementation requirements
for the respective all-at-one and reduced versions and provide some numerical comparison.

Key words. inverse problems, regularization, all-at-once formulations

AMS subject classiﬁcations. 65M32, 65J22, 35R30

1. Introduction. In their original formulation, inverse problems often consist
of a model and additional observations. Consider, e.g., an equation (PDE, ODE,
integral equation) model for the state u

(1)

A(x, u) = 0

containing a parameter x (or a set of parameters) that is to be determined from
additional observations of the state

C(u) = y

(2)
Here A : D(A)(⊆ X × V ) → W ∗ and C : D(C)(⊆ V ) → Y are operators acting
between Banach spaces X, V , W ∗, Y (the star indicates that in variational formu-
lations of models W ∗ will typically be the dual of some Banach space). The setting
could be extended in several directions, e.g., the observation can as well depend on
some unknown parameters that have to be identiﬁed or the model could consist of a
variational inequality instead of an equation. Still (1), (2) is suﬃciently general to
comprise a wide range of applications, e.g., the following examples.

1.1. Examples.
Example 1. Consider a boundary value problem for a linear elliptic PDE on a

smooth bounded domain Ω ⊆ Rd, d ∈ {1, 2, 3}

−∇(a∇u) + cu = b in Ω ,

∂u
∂n

= g on ∂Ω

with a given boundary excitation g ∈ H−1/2(∂Ω), and possibly spatially varying co-
eﬃcients a, b, c and the inverse problem of identifying these coeﬃcients a, b, c (or
part of them) from additional measurements C(u) of the PDE solution u on (part
of ) the domain or on its boundary. This ﬁts into the above framework with, e.g.,

∗This work was supported by the Austrian Science Fund FWF under grant I2271.
†Alpen-Adria-Universit¨at Klagenfurt, Austria (barbara.kaltenbacher@aau.at, http://wwwu.

uni-klu.ac.at/bkaltenb/).

1

2
X = W 1,p(Ω) × L∞(Ω) × Lp(Ω), p ∈ [1,∞], V = H 1(Ω) = W , (or, if Ω, g are
suﬃciently smooth, V = W 2,p(Ω), W ∗ = Lp(Ω), p ∈ [1,∞],)

B. KALTENBACHER

(cid:90)

(cid:16)

(cid:17)

(cid:90)

(cid:104)A(a, b, c, u), w(cid:105)W ∗,W =

a∇u · ∇w + cuw − bw

dx −

gw ds

Ω

∂Ω

and Y = Lq(Σ), p ∈ [1,∞], C(u) = u|Σ, where Σ ⊆ Ω is an open subdomain of Ω or a
regular curve/surface contained in its boundary or in its interior, so that an embedding
or a trace theorem yields continuity of the observation map C : H 1(Ω) → Lp(Σ).

Example 2. Using similar measurements but a nonlinear model, we consider
identiﬁcation of the nonlinearity, i.e., the function q in the elliptic boundary value
problem

−∆u + q(u) = 0 in Ω ,

∂u
∂n

= g on ∂Ω

with given g. Here we use a space X that is continuously embedded in C[u, u] for
an interval [u, u] containing all possibly appearing values of u (which can, e.g., be
estimated by using maximum principles in case the PDE is elliptic, depending on the
monotonicity of q), V = H 1(Ω) = W ,

(cid:90)

(cid:16)∇u · ∇w + q(u)w

(cid:17)

(cid:90)

dx −

gw ds

(cid:104)A(q, u), w(cid:105)W ∗,W =

Ω

∂Ω

and Y , C as in Example 1 above.

Example 3. Alternatively, one often encounters inverse source problems for non-

linear PDEs such as the simple model example of identifying b in

−∆u + ζu3 = b in Ω ,

∂u
∂n

= g on ∂Ω

where g and ζ are given. Here we have X = Lp(Ω), p ∈ [1,∞], V = H 1(Ω) = W ,

(cid:90)

(cid:16)∇u · ∇w + ζu3w − bw

(cid:17)

(cid:90)

dx −

gw ds ,

(cid:104)A(b, u), w(cid:105)W ∗,W =

Ω

∂Ω

and again Y , C as in Example 1 above.

Example 4. Consider identiﬁcation of the (possibly inﬁnite dimensional) param-

eter ϑ in the state space system consisting of an ODE model and observations

˙u(t) = f (t, u(t), ϑ) t ∈ (0, T ) ,
y = C(u) ,

u(0) = u0

where the dot denotes time diﬀerentiation, f : (0, T ) × Rn × X → Rn is a given func-
tion and u0 ∈ Rn is a given initial value. Using semigroup theory, this could as well
be extended to time dependent PDEs. An example of an inﬁnite dimensional station-
ary parameter ϑ to be identiﬁed in a system of ODEs is the Preisach weight function
in some hysteretic evolutionary model. The observations C(u) are, e.g. discrete or
continuous in time yi = gi(u(ti)), i ∈ {1, . . . , m} (including the case of ﬁnal measure-
ments ti = T ) or y(t) = g(t, u(t)), t ∈ (0, T ) with given functions gi : Rn → RmM or
g : (0, T ) × Rn → L2(0, T ; RM ).

REGULARIZATION BASED ON ALL-AT-ONCE FORMULATIONS

3

1.2. Motivation of the all-at-once-approach. Deﬁnition and analysis of so-
lution methods for such inverse problems is often based on a reduced formulation that
is obtained by the use of a parameter-to-state map i.e., a mapping S : D(S)(⊆ X) →
V , that resolves (1) with respect to x

∀x ∈ D(S) : A(x, S(x)) = 0 and

∀u ∈ V : ((x, u) ∈ D(A) and A(x, u) = 0) ⇒ u = S(x).

Existence of such a mapping is guaranteed by the Implicit Function Theorem if for
an open set B with D(S) ⊂ B ⊆ X, A is continuously Fr´echet diﬀerentiable on B × V
and its derivative Au with respect to the state is boundedly invertible with uniform
bound:Assumption 1.

∃CA ∀(x, u) ∈ (B × V ) ∩ D(A) : (cid:13)(cid:13)Au(x, u)−1(cid:13)(cid:13) ≤ CA .

In order to satisfy this assumption, usually the domain of A has to be restricted, e.g.,
to

(3)

D(A) ⊆ {(x, u) = (a, b, c, u) : a ≤ a ≤ a a.e. on Ω, c ≥ c a.e. on Ω}

with positive constants 0 < a < a, 0 < c in example 1 or to

(cid:40)
(x, u) = (q, u) : q ≤ q(˜λ) − q(λ)

˜λ − λ

(cid:41)

∀˜λ (cid:54)= λ ∈ R

(4)

D(A) ⊆

with some constant q > 0 in example 2.
Under such conditions the forward operator F : D(F )(⊆ X) → Y , F = C ◦ S
is well-deﬁned on D(F ) = D(S) and the inverse problem (1), (2) can be equivalently
written as an operator equation

(5)

F (x) = y

Such problems are typically ill-posedness in the sense that F is not continuously
invertible and instead of y usually only a noisy version yδ is available, which we here
assume to obey the deterministic and known noise bound δ

(cid:13)(cid:13)y − yδ(cid:13)(cid:13) ≤ δ ,

(6)

thus regularization (see, e.g., [2, 6, 15, 17, 20, 21, 24] and the references therein) has
to be employed.

In this paper we will return to the original formulation as an all-at-once system

of model and observation equation (1), (2),

(7)

F(x) = F(x, u) =

=

= y

(cid:18) A(x, u)

(cid:19)

C(u)

(cid:19)

(cid:18) 0

y

and investigate the behaviour of some well-known regularization paradigms when
applied to F instead of F . We will see that this enables to avoid restrictions like (3), (4)
and, moreover, can make a considerable diﬀerence when it comes to implementation.
We will also provide a convergence analysis that goes beyond the mere application of
known results to the operator F in the sense that regularization might not just be

4

B. KALTENBACHER

applied to the whole of x = (x, u) but - as natural - only to the x part, a case which
requires some extra considerations in the convergence analysis.

All-at-once approaches to inverse problems have already been considered previ-
ously, see, e.g., [3, 4, 8, 14, 23]. While these papers concentrate on computational
aspects and convergence analysis of particular methods, our aim is here to provide a
comparative overview on several regularization paradigms.

In the remainder of this paper we will assume that a solution

(x†, u†) ∈ D(F) ⊆ D(A) ∩ (X × D(C))

(8)
to (7) exists and that D(F) is convex. Note that D(F) need not necessarily be the
maximal domain of F, so restriction to a convex set can be done without loss of
generality here.

While data misﬁt and regularization terms will be deﬁned by norms for simplicity
of exposition, most of the results are extendable to more general discrepancy and
regularization functionals as considered, e.g. in [7, 19, 25].

We treat methods and convergence conditions only exemplarily to highlight analo-
gies and diﬀerences between reduced and all-at-once formulations, so our aim is not
to provide a complete convergence analysis (a priori and a posteriori choice of regu-
larization parameters, general rates, etc.) for each of the discussed methods.

The remainder of this paper is organized as follows. In Sections 2, 3, 4, we consider
Tikhonov, Newton type, and gradient type regularization, respectively. For each of
these three paradigms we provide some convergence results with a priori and a posteri-
ori regularization parameter choice strategies and under diﬀerent assumptions on the
forward operator (This analysis part is restricted to just quotation of a convergence
result in the gradient method case.) Moreover we compare the key implementation
requirements. Section 5 contains a veriﬁcation of the convergence conditions for the
iteratively regularized Gauss-Newton method from Section 3 for Example 3, whose
all-at-once and reduced versions are then compared numerically. Some preliminary
considerations on comparison of convergence conditions for all-at-once and reduced
versions are provided in Section 6, and we give a short summary and an outlook in
Section 7.

Notation. For some r ∈ [1,∞) we denote by r∗ = r
X denotes the duality mapping with gauge function 1

r =
r (cid:107)·(cid:107)r
∂ 1
r tr, which is in general
set valued. For smooth spaces, i.e., spaces with Gˆateaux diﬀerentiable norm on the
unit sphere, J X
r (x) will be single valued; otherwise, by a slight abuse of notation, we
denote by J X

r (x) a single valued selection from this set.

r−1 the dual index. J X

2. Tikhonov regularization. For any ρ, α > 0, m, o, r ∈ [1,∞), x0 ∈ B, u0 ∈

V , we deﬁne the pair (xδ

α, uδ

α) as a minimizer of

(9)

with

(10)

and

(11)

min

(x,u)∈D(F)

S(F(x, u), (0, yδ)) + αR(x, u)

S((w∗, y), (ymod, yobs)) =

(cid:13)(cid:13)(cid:13)w∗ − ymod(cid:13)(cid:13)(cid:13)m

W ∗ +

(cid:13)(cid:13)(cid:13)y − yobs(cid:13)(cid:13)(cid:13)o

Y

1
o

ρ
m

R(x, u) = R1(x) =

(cid:107)x − x0(cid:107)r

X

1
r

REGULARIZATION BASED ON ALL-AT-ONCE FORMULATIONS

5

or

(12)

R(x, u) = R2(x, u) =

(cid:107)x − x0(cid:107)r

V +

1
r

(cid:107)u − u0(cid:107)q

V ,

1
q

Note that in case of (11) regularization is only applied to x. In both versions, ρ will
remain ﬁxed, whereas α will be chosen in dependence of δ - typically in such a way
that it tends to zero as δ → 0.

2.1. Well-deﬁnedness and convergence. The analysis will be based on the

following assumptions

Assumption 2. Bounded sets in X, V are weakly (or weakly*) compact.
Assumption 3. F is weakly (or weakly*) sequentially closed, i.e.,
∀((xk, uk))k∈N ⊆ D(F) :

(cid:17)

xk (cid:42) x , uk (cid:42) u , A(xk, uk) (cid:42) f , C(uk) (cid:42) y
(x, u) ∈ D(F) and A(x, u) = f , C(u) = y

(cid:16)
=⇒ (cid:16)

(cid:17)

,

where (cid:42) denotes weak or weak* convergence.

For only proving convergence provided minimizers are already well-deﬁned the
following somewhat weaker assumption (note that we have strong convergence of the
images in the premiss) suﬃces in place of Assumption 3.

Assumption 4.
∀((xk, uk))k∈N ⊆ D(F) :

(cid:16)
=⇒ (cid:16)

xk (cid:42) x , uk (cid:42) u , A(xk, uk) → f , C(uk) → y
(x, u) ∈ D(F) and A(x, u) = f , C(u) = y

(cid:17)

(cid:17)

Assumption 2 is satisﬁed if X, V are reﬂexive or duals of separable normed spaces.
(The star in weak* will be skipped in the following.) Suﬃcient for Assumption 3 is
weak continuity of A and C and weak closedness of D(F).

To prove well-deﬁnedness of a minimizer and convergence in case of regularization
with respect to x only, (11), we additionally impose Fr´echet diﬀerentiability of A and
C, Assumption 1 and a growth condition on the derivative Ax of the model operator
with respect to the parameter:
ing) function ψ : R+ → R+ such that

Assumption 5. There exists a (without loss of generality monotonically increas-

(a) ∀(x, v) ∈ (B × V ) ∩ D(A) : (cid:107)Ax(x, u)(cid:107) ≤ ψ ((cid:107)x(cid:107)X ) (1 + (cid:107)u(cid:107)V )

and ψ(cid:0)(cid:13)(cid:13)x†(cid:13)(cid:13)X + λ(cid:1) λ → 0 as λ → 0 and (cid:13)(cid:13)x† − x0

(cid:13)(cid:13)X is suﬃciently small

tions 1, 5 be satisﬁed.

or
(b) ∀(x, v) ∈ (B × V ) ∩ D(A) : (cid:107)Ax(x, u)(cid:107) ≤ ψ ((cid:107)x(cid:107)X )
Proposition 1. Let Assumptions 2 3, and in case of (11) additionally Assump-
• Then there exists δ > 0 such that for all δ ∈ (0, δ), α > 0 a minimizer of (9)
• These minimizers are stable with respect to the data yδ in the sense that for
any sequence (yk)k∈N converging to yδ in the Y norm and any sequence of
corresponding minimizers ((xk, uk))k∈N there exists a weakly convergent sub-
sequence and the limit of every weakly convergent subsequence is a minimizer
of (9).

with (10) and (11) or (12) exists.

6

B. KALTENBACHER

• If α = α(δ) is chosen a priori according to

(13)

α → 0 and

δo
α

→ 0 as δ → 0

or a posteriori according to the generalized discrepancy principle

δo
o

≤ S(F(xδ

α, uδ

α), (0, yδ)) ≤ τ δo
o

(14)
for some ﬁxed τ > 0 independent of δ ∈ (0, δ), then there exists C > 0
independent of δ ∈ (0, δ) such that for any solution (x†, u†) of (1), (2) we
have boundedness
∀δ ∈ (0, δ) :

oα(δ) with (13)

(cid:40) 1

≤

α(δ) − x0

X + δo
X with (14)

(15)

(cid:13)(cid:13)(cid:13)r

X

1
r

(cid:13)(cid:13)(cid:13)xδ
(cid:13)(cid:13)(cid:13)uδ

and

α(δ)

(cid:13)(cid:13)(cid:13)V

≤ C

in case of minimizers of (9) with (10) and (11) and

r
1
r

(cid:13)(cid:13)r
(cid:13)(cid:13)r

(cid:13)(cid:13)x† − x0
(cid:13)(cid:13)x† − x0
(cid:40)R2(x†, u†) + δo

(16)

∀δ ∈ (0, δ) : R2(xδ

α(δ), uδ

α(δ)) ≤

α(δ) with (13)

R2(x†, u†) with (14)

in case of minimizers of (9) with (10) and (12).

• In both cases (15), (16), the regularized approximations (xδ

α(δ), uδ

α(δ)) converge

weakly subsequentially to a solution of (1), (2).

Proof. The assertion follows from the results in Section 3 of [10] with u, F (u)
there deﬁned by (x, u), F(x, u) here. Indeed almost all items of [10, Assumption 2.1]
easily follow from Assumption 2 3 (note that it actually suﬃces to assume weak
sequential closedness of the operator in place of [10, Assumption 2.1 (3), (5)]). The
only exception arises in case of regularization with respect to x only (11), where
boundedness, i.e., weak compactness, of level sets

(cid:26)

(cid:27)

Mα(M ) =

(x, u) ∈ D(F) :

1
α

S(F(x, u), (0, yδ)) + R(x, u) ≤ M

[10, Assumption 2.1 (6)], which the proofs there actually only require to hold for
suﬃciently small M , has to be shown separately. For obtaining this boundedness, we
will make use of Assumptions 1, 5 in that case. Since we have also stated convergence
with the discrepancy principle, which is not treated in [10] and for completeness of
exposition we provide the details of the convergence part of the proof for the case of
(11).
The standard argument of minimality together with (6) for any solution (x†, u†)

of (1), (2) yields the estimate

(17)

S(F(xδ

α, uδ

α), (0, yδ)) +

α
r

(cid:13)(cid:13)xδ

α − x0

(cid:13)(cid:13)r ≤ 1

δo +

α
r

o

(cid:13)(cid:13)x† − x0

(cid:13)(cid:13)r

thus, upon division by α and setting α = α(δ) (in case of (14) using the lower estimate
there) we get the estimate on xδ
α as well, we
use the identity

α in (15). To obtain boundedness of uδ

(18) A(xδ

α, uδ

Ax(xθ, uθ)(xδ

α − x†) + Au(xθ, uθ)(uδ

(cid:17)
α − u†)

dθ

(cid:90) 1

(cid:16)

+

0

(cid:125)
(cid:123)(cid:122)
α) = A(x†, u†)

(cid:124)

=0

REGULARIZATION BASED ON ALL-AT-ONCE FORMULATIONS

7
α − u†) and Assumptions 1, 5, as well as

where xθ = x† + θ(xδ
(17) or the upper bound in (14)

α − x†), uθ = u† + θ(uδ

m
ρ

α, uδ

(cid:40) 1
(cid:13)(cid:13)x0 − x†(cid:13)(cid:13)r
(cid:13)(cid:13)A(xδ
α)(cid:13)(cid:13)m
W ∗ ≤ C δ =
(cid:16)
α − u†(cid:13)(cid:13)V ≤CA
(cid:13)(cid:13)uδ
α − x†(cid:13)(cid:13)X
δ + ψ(cid:0)(cid:13)(cid:13)x†(cid:13)(cid:13)X +(cid:13)(cid:13)xδ
(cid:40)
(1 +(cid:13)(cid:13)u†(cid:13)(cid:13)V +(cid:13)(cid:13)uδ

o δo + α(δ)
o δo in case of (14)

1/m

C

r

τ

×

to arrive at the estimate

(19)

1 in case (b)

X in case of (13)

.

(cid:17)
α − x†(cid:13)(cid:13)X
(cid:1)(cid:13)(cid:13)xδ
α − u†(cid:13)(cid:13)V ) in case (a)
(cid:13)(cid:13)(cid:13)uδ
α(δ) − u†(cid:13)(cid:13)(cid:13)V
(cid:1)(cid:17) ≤ c < 1

.

(cid:40)
(cid:13)(cid:13)(cid:13)uδ
α(δ) − u†(cid:13)(cid:13)(cid:13)V

in case (b). In case (a) we additionally use the fact that the smallness assumption on

By the ﬁrst (already proven) part of (15), this directly yields a bound on

(cid:13)(cid:13)x0 − x†(cid:13)(cid:13)X and the growth condition on ψ allows us to achieve
(cid:1)(cid:1) ϕ(cid:0)(cid:13)(cid:13)x0 − x†(cid:13)(cid:13)X

δ + ψ(cid:0)(cid:13)(cid:13)x†(cid:13)(cid:13)X + ϕ(cid:0)(cid:13)(cid:13)x0 − x†(cid:13)(cid:13)X

(cid:16)

CA

1/m

C

for some constant c independent of δ, where

ϕ(s) =

s + (sr + rδo
2s in case of (14)

oα(δ) )1/r in case of (13)

.

Rearranging terms in (19) (case (a)) we therefore get

(cid:0)1 +(cid:13)(cid:13)u†(cid:13)(cid:13)V

(cid:1) .

≤ c
1 − c

The rest follows by standard arguments from the assumed continuity assumptions on
A and C.

Remark 1. Note that in case of regularization with respect to both x and u, (12),
we do not need Assumptions 1, 5 and can therefore also deal with situations in which
a parameter-to state map not necessarily exists.
Well-deﬁnedness of α according to the discrepancy principle (14) follows from [16,
Lemma 1] provided X × V is reﬂexive and strictly convex and either (i) F is weakly
closed (i.e., Assumption 3 is satisﬁed) and Y is reﬂexive or (ii) F is weak-to-weak
continuous and D(F) is weakly closed (which by the assumed convexity of this set is
satisﬁed, e.g., if it is closed wrt. norm convergence).
If X × V satisﬁes the Kadets-Klee property, the results of Theorem 1 imply strong
convergence and if the solution is unique then by a subsequence-subsequence argument
the whole sequence converges.

2.2. Convergence rates. As usual we can conclude convergence rates with
respect to the Bregman distance under source conditions. Just exemplarily we will
state a rates result for the all-at-once Tikhonov method with regularization with
respect to both x and u under a benchmark source condition. To this end, we use an
element of the subgradient

(20)

ξ ∈ ∂R(x†, u†) where R(x, u) = R2(x, u) =

(cid:107)x − x0(cid:107)r

X +

1
r

(cid:107)u − u0(cid:107)q

V

1
q

8

B. KALTENBACHER

to deﬁne the Bregman distance

(21)

D(x0,u0)

ξ

((x, u), (x†, u†)) = R(x, u) − R(x†, u†) − (cid:104)ξ, (x − x†, x − u†)(cid:105)

and impose a local smoothness condition on F.

Assumption 6.

(cid:13)(cid:13)Ax(x†, u†)(x − x†) + Au(x†, u†)(u − u†)(cid:13)(cid:13) +(cid:13)(cid:13)C(cid:48)(u†)(u − u†)(cid:13)(cid:13)
(cid:13)(cid:13)C(u) − C(u†)(cid:13)(cid:13)o(cid:17)1/t

(cid:13)(cid:13)A(x, u) − A(x†, u†)(cid:13)(cid:13)m

(cid:16) ρ

≤ C L

+

1
o

m

+ LD(x0,u0)

ξ

((x, u), (x†, u†)) ,

for some ξ according to (20), t > 0, and all (x, u) ∈ D(F).
In case m = o = t, Assumption 6 follows from the inverse triangle inequality and the
Taylor remainder estimate

(cid:13)(cid:13)A(x†, u†) + Ax(x†, u†)(x − x†) + Au(x†, u†)(u − u†) − A(x, u)(cid:13)(cid:13)
+(cid:13)(cid:13)C(u†) + C(cid:48)(u†)(u − u†) − C(u)(cid:13)(cid:13) ≤ LD(x0,u0)

((x, u), (x†, u†)) ,

ξ

(cid:13)(cid:13)u − u†(cid:13)(cid:13)2

which in the quadratic Hilbert space case D(x0,u0)
1
2

dition on F(cid:48).

corresponds to the usual estimate obtained under a local Lipschitz con-
Proposition 2. Let F(cid:48) satisfy Assumption 6 and let, for some (vmod, vobs) ∈
< 1 the source condition

(cid:13)(cid:13)(cid:13)(vmod, vobs)

(cid:16)(cid:13)(cid:13)(cid:13)vmod(cid:13)(cid:13)(cid:13)W

(cid:13)(cid:13)(cid:13)vobs(cid:13)(cid:13)(cid:13)V ∗

(cid:13)(cid:13)(cid:13) := L

(cid:17)

W × Y ∗ with L

((x, u), (x†, u†)) = 1

2

+

+

ξ

(22)
hold for some ξ ∈ ∂R2(x†, u†).

ξ = F(cid:48)(x†, u†)∗(vmod, vobs)

Then for the Tikhonov minimizers according to (9), (10), (12) with the apriori

choice

(cid:13)(cid:13)x − x†(cid:13)(cid:13)2

(23)
(with t∗ = t

t−1 ) or the a posteriori choice (14) we have

α(δ) ∼ δ

o

t∗

(24)

D(x0,u0)

ξ

((xδ

α(δ), uδ

α(δ)), (x†, u†)) = O(δo/t)

Proof. The proof follows the lines of the classical rates proof from [5].
By (16) and (22) we have in case of (14)

α(δ) − u†)(cid:105)

α(δ)) − A(x†, u†)

(cid:13)(cid:13)(cid:13)m

ξ

C L

((xδ

α(δ), uδ

α(δ) − x†, uδ
α(δ), uδ

α(δ)), (x†, u†))
D(x0,u0)
≤ −(cid:104)(vmod, vobs), F(cid:48)(x†, u†)(xδ

≤(cid:13)(cid:13)(cid:13)(vmod, vobs)
(cid:13)(cid:13)(cid:13)(cid:16)
(cid:13)(cid:13)(cid:13)A(xδ
(cid:16) ρ
(cid:13)(cid:13)(cid:13)C(uδ
(cid:13)(cid:13)(cid:13)o(cid:17)1/t
m
α(δ)) − C(u†)
(cid:13)(cid:13)(cid:13)(vmod, vobs)
(cid:13)(cid:13)(cid:13)(cid:17)
α(δ)), (x†, u†)) ≤(cid:13)(cid:13)(cid:13)(vmod, vobs)

+ LD(x0,u0)

α(δ), uδ

cD(x0,u0)

((xδ

> 0

1
o

+

ξ

ξ

((xδ

α(δ), uδ

(cid:17)
α(δ)), (x†, u†))
(cid:17)1/t

(cid:13)(cid:13)(cid:13)(cid:16) C L(τ + 1)δo

o

.

hence with c = 1 − L

REGULARIZATION BASED ON ALL-AT-ONCE FORMULATIONS

9

In case of (23) we get from minimality (cf. (17))
α), (0, yδ)) + αR2(xδ

S(F(xδ

α, uδ

α) ≤ 1
o

δo + αR2(x†) ,

hence after division by α and by deﬁnition (21) of the Bregman distance, as well as
the source condition (22)
S(F(xδ

α, uδ
α), (0, yδ))
α

((xδ
− (cid:104)(vmod, vobs), F(cid:48)(x†, u†)(xδ

+ D(x0,u0)

ξ

(cid:13)(cid:13)(cid:13)(cid:16)
(cid:13)(cid:13)(cid:13)(vmod, vobs)

≤ δo
α
≤ δo
α
+ LD(x0,u0)

((xδ

+

α, uδ

ξ

(cid:16)S(F(xδ
(cid:17)

C L

α), (x†, u†))

,

α, uδ
α − x†, uδ

α), (x†, u†))
(cid:17)1/t
α − u†)(cid:105)

α, uδ

α), (0, yδ))

hence with c as above and using Young’s inequality

S(F(xδ

≤ δo
α
≤ δo
α

+

+

α, uδ

ξ

((xδ

+ cD(x0,u0)

α), (0, yδ))
α, uδ
α

(cid:13)(cid:13)(cid:13) C L
(cid:13)(cid:13)(cid:13)(vmod, vobs)
(cid:16)S(F(xδ
(cid:13)(cid:13)(cid:13) C L
(cid:16)(cid:16)(cid:13)(cid:13)(cid:13)(vmod, vobs)
(cid:17) 1
(cid:17)t

α, uδ

t−1

(cid:17)1/t
α), (x†, u†))

α), (0, yδ))
S(F(xδ

2α

+

t

α), (0, yδ))
α, uδ
2α

,

which by the choice (23) yields (24).

Remark 2. Note that in case of regularization with respect to x only, i.e., (11),
we do get boundedness also of the u part via Assumptions 1, 5. However, the sharp
estimate (16), that was crucially used in the rates proof above, fails to hold in general.
Therefore we do not expect to get (optimal) convergence rates in that case.

2.3. Comparison of implementation. To compare the all-at-once Tikhonov
method (9), with (10) and (11) or (12) with Tikhonov regularization for the reduced
formulation

(25)

min
x∈D(F )

1
o

Y +

(cid:107)x − x0(cid:107)r

α
r

(cid:13)(cid:13)F (x) − yδ(cid:13)(cid:13)o

we write the latter as a PDE constrained minimization problem

(26)

min

(x,u)∈D(F)

1
o

Y +

(cid:107)x − x0(cid:107)r

α
r

s.t. A(x, u) = 0 ,

(cid:13)(cid:13)C(u) − yδ(cid:13)(cid:13)o

and denote the Tikhonov minimizer as well as its corresponding state by a bar, i.e.,
xδ
α minimizes (26) and A(xδ
α) = 0. For convenience of exposition we will assume
W to be reﬂexive and identify it with its bidual.

α, uδ

We ﬁrst of all show that reduced Tikhonov regularization (26), (25) is equivalent
to all-at-once Tikhonov regularization (9), with (10) and (11) in case m = 1 with ρ
suﬃciently large. This is due to exact penalization, cf., e.g., [18, Theorem 17.3], whose
proof remains valid in the Banach space setting. More precisely, for this purpose ρ
has to be larger than the norm of the adjoint state, i.e., the solution wδ

α ∈ W to

Au(xδ

α, uδ

α)∗w = C(cid:48)(uδ

α)∗ J Y

o (C(uδ

α) − yδ) in V ∗

10

B. KALTENBACHER

α, uδ

for (xδ
entiability of C together with Assumption 1, which also implies that

α) solving (26). Existence of this adjoint state is ensured by Fr´echet diﬀer-

(cid:13)(cid:13)C(cid:48)(uδ

α)(cid:13)(cid:13)(cid:13)(cid:13)C(uδ

α) − yδ(cid:13)(cid:13)o−1

Y

.

(27)

Moreover, by the same arguments as in the proof of Theorem 1, uniform (wrt. δ > 0)
boundedness of (xδ
α(δ)) can be concluded from its deﬁnition as a minimizer of
(25)

α(δ), uδ

α

(cid:13)(cid:13)W ≤ CA
(cid:13)(cid:13)wδ
(cid:13)(cid:13)(cid:13)F (xδ
α(δ)) − yδ(cid:13)(cid:13)(cid:13)o

1
o

α(δ)

with α(δ) chosen a priori according to (13) or a posteriori according to

Y

+

1
r

α(δ) − x0

(cid:13)(cid:13)(cid:13)r ≤ 1
(cid:13)(cid:13)(cid:13)xδ
α(δ)) − yδ(cid:13)(cid:13)(cid:13)o ≤ τ δo ,
δo ≤(cid:13)(cid:13)(cid:13)F (xδ

o

δo
α(δ)

(cid:13)(cid:13)x† − xo

(cid:13)(cid:13)r

+

1
r

(which due to exact penalization will ﬁnally coincide with (14),) together with the
identity (cf. (18))

0 = A(xδ

(cid:90) 1

0

=

α, uδ
α)

α(δ), uδ

Ax(xθ, uθ)(xδ

α(δ)) = A(xδ

(cid:16)
α(δ) − x†), uθ = u† + θ(uδ
α) replaced by (xδ

α(δ) − x†) + Au(xθ, uθ)(uδ

(cid:17)
α(δ) − u†)

dθ

α

α, uδ

with xθ = x† + θ(xδ
(19) with C δ = 0 and (xδ

uniform bound on(cid:13)(cid:13)wδ

(cid:13)(cid:13)W and can conclude the following equivalence.

α(δ) − u†), which yields an estimate like
α(δ), uδ
α(δ)). Thus, from (27) we get a

Proposition 3. Let Assumptions 1, 5 be satisﬁed and let C be Fr´echet diﬀeren-
tiable with C(cid:48) mapping bounded sets to bounded sets. There exist ρ > 0 suﬃciently
large and δ > 0 suﬃciently small such that for all δ ∈ (0, δ) and α(δ) chosen according
to (13) or (14), the minimizers of the all-at-once Tikhonov functional according to
(9) with (10), (11) with m = 1 coincide with those of the reduced Tikhonov functional
(25).
We now consider ﬁrst order optimality conditions for the reduced and the all-
at-once formulations (26), (9) with general m ≥ 1, so that they are not necessarily
equivalent, and for this purpose assume A, C, and the occurring norms to be contin-
uously Fr´echet diﬀerentiable (i.e., the corresponding spaces to be uniformly smooth).
In case D(F) = X × V , we get the ﬁrst order necessary conditions

(28)

A(xδ
α J X
Au(xδ

α, uδ
α) = 0
α − x0) = −Ax(xδ
r (xδ
α = −C(cid:48)(uδ
α)∗wδ
α, uδ

α, uδ
α)∗ J Y

α)∗wδ
o (C(uδ

α

α) − yδ)

for a minimizer (xδ

α, uδ

α) of the reduced Tikhonov functional (26) and

r (xδ
α J X
ρAu(xδ

α − x0) = −ρAx(xδ
α, uδ
α, uδ

α)∗ J W ∗

m (A(xδ

α)∗ J W ∗

α, uδ
α)) = −C(cid:48)(uδ

m (A(xδ

α, uδ
α)∗ J Y

α))

o (C(uδ

α) − yδ)(cid:2)−αJ V

r (uδ

α − u0)(cid:3)

for a minimizer (xδ
or (12), where the term in brackets is skipped in case of (11). Upon deﬁning wδ

α) of the all-at-once Tikhonov functional (9) with (10) and (11)
α as the

α, uδ

REGULARIZATION BASED ON ALL-AT-ONCE FORMULATIONS

α) − yδ)(cid:2)−αJ V

r (uδ

α − u0)(cid:3), as justiﬁed

11

solution to Au(xδ
o (C(uδ
by Assumption 1, the latter can be rewritten as

α, uδ

α)∗w = −C(cid:48)(uδ

α)∗ J Y

(29)

ρ J W ∗
α J X
Au(xδ

α, uδ

α)) = wδ
m (A(xδ
α
α − x0) = −Ax(xδ
r (xδ
α = −C(cid:48)(uδ
α)∗wδ
α, uδ

α, uδ
α)∗ J Y

α)∗wδ
o (C(uδ

α

α) − yδ)(cid:2)−αJ V

r (uδ

α − u0)(cid:3)

so very similar to (28). These ﬁrst order necessary optimality conditions can be
justiﬁed, e.g., by a Zowe Kurcyuscz constraint qualiﬁcation [22, Section 6.1], which
in the all-at-once case is an empty condition and in the reduced case amounts to
surjectivity of A(cid:48)(xδ
α) and is thus obviously satisﬁed, e.g., under Assumption 1.
In case of additional convex constrains on the parameters D(F) = C × V with
some convex set C, e.g., deﬁned by the pointwise bounds a, a, c, q in (3), (4), the
second line in (28), (29) changes to

α, uδ

(30)

(cid:104)α J X

r (˜x − x0) + Ax(˜x, ˜u)∗ ˜w, x − ˜x(cid:105)X∗,X ≥ 0 for all x ∈ C
α, uδ

α), (˜x, ˜u, ˜w) = (xδ

α, wδ

for (˜x, ˜u, ˜w) = (xδ
α), respectively, and the Zowe Kur-
cyuscz constraint qualiﬁcation is again always satisﬁed in the all-at-once case and
amounts to

α, wδ

α, uδ

Ax(xδ

α) + Au(xδ

α)C(xδ

α, uδ

α) =(cid:8)γ(x − xδ

α)V = W ∗

α, uδ

α) : γ ≥ 0, x ∈ C(cid:9)

C(xδ

with

(32)

with

(33)

or

(34)

in the reduced case, which again obviously holds under Assumption 1.

3. The iteratively regularized Gauss-Newton method. Throughout this
section we assume A and C to be continuously Fr´echet diﬀerentiable on D(F) and
abbreviate, analogously to (10)

(31)

=

ρ
m

S(F(˜x, ˜u) + F(cid:48)(˜x, ˜u)(x − ˜x, u − ˜u), (ymod, yobs))

(cid:13)(cid:13)(cid:13)A(˜x, ˜u) + Ax(˜x, ˜u)(x − ˜x) + Au(˜x, ˜u)(u − ˜u) − ymod(cid:13)(cid:13)(cid:13)m
(cid:13)(cid:13)(cid:13)C(˜u) + C(cid:48)(˜u)(u − ˜u) − yobs(cid:13)(cid:13)(cid:13)o

+

.

W ∗

Y

1
o

Given some iterate (xδ
iteratively regularized Gauss Newton IRGNM, cf., e.g.
iterate (xδ

k) ∈ D(F), we deﬁne the next Newton (more, precisely,
[1, 2, 15, 13, 16, 11, 26])

k, uδ

k+1(α), uδ

k+1(α)) as a minimizer of
S(F(xδ

k) + F(cid:48)(xδ

k, uδ

k, uδ

min

(x,u)∈D(F)

k)(x − xδ

k, u − uδ

k), (0, yδ)) + αR(x, u)

R(x, u) = R1(x) =

(cid:107)x − x0(cid:107)r

X

1
r

R(x, u) = R2(x, u) =

(cid:107)x − x0(cid:107)r

V +

1
r

(cid:107)u − u0(cid:107)q

V ,

1
q

like in the previous section, cf. (11), (12).

12

B. KALTENBACHER

3.1. Well-deﬁnedness and convergence. As compared to Tikhonov regular-
ization, iterative regularization methods for nonlinear problems can only be proven
to converge under certain structural assumptions restricting the nonlinearity of the
forward operator. We here ﬁrst of all concentrate on convergence under the simple
tangential cone condition.

Assumption 7.

S(F(˜x, ˜u) + F(cid:48)(˜x, ˜u)(x − ˜x, u − ˜u), F(x)) ≤ ctcS(F(x, u), F(˜x, ˜u))

for some 0 < ctc < 1 and all (x, u), (˜x, ˜u) ∈ D(F)∩B(x0, u0) with B(x0, u0) a closed
ball with suﬃciently small radius  > 0 around (x0, u0).
Correspondingly, we consider a posteriori choice of the stopping index k∗ = k∗(δ) by
the discrepancy principle

k∗ = min{k ∈ N : S(F(xδ

(35)
and of αk for k ≤ k∗ by the inexact Newton strategy
(36)

σ ≤ σk(xδ

k+1(αk), uδ

k, uδ

k), (0, yδ)) ≤ τ δo
o

}

where

σk(x, u) =

S(F(xδ

k, uδ

k) + F(cid:48)(xδ
k, uδ
S(F(xδ

k, u − uδ

k), (0, yδ))

k+1(αk)) ≤ σ
k)(x − xδ
k, uδ
k), (0, yδ))

is the ratio between the predicted and the old data misﬁt. Note that by deﬁnition (35)
of k∗, the denominator in σk(xδ
Proposition 4. Let X×V be reﬂexive and uniformly convex and let Assumptions

k+1(αk)) is nonzero for k < k∗ and δ > 0.

k+1(αk), uδ

2, 4, 7 be satisﬁed with ctc suﬃciently small

ctc < σ < σ < 1

and either (i) F(cid:48)(x, u) be weakly closed for all (x, u) ∈ D(F) and Y be reﬂexive or
(ii) D(F) be weakly closed. In case of regularization with respect to x only (33), we
additionally impose Assumptions 1, 5.

Moreover, let τ be chosen suﬃciently large so that
) ≤ σ and ctc <

1 + CSctc

CS(ctc +

τ

1 − σ

2

,

(37)

for

let (x0, u0) be close enough to (x†, u†) with respect to the Bregman distance, and let
the signal-to-noise ratio condition

CS = max{ ρ
m

2m−1,

2o−1} ,

1
o

oS(F(x0, u0), (0, yδ))

δo

τ <

hold. • Then for all k ≤ k∗(δ) − 1 with k∗(δ) according to (35), the iterates

(xδ

k+1, uδ

k+1) :=

(x0, u0)

k+1(αk), uδ

k+1(αk)), αk as in (36)

if σk(x0, u0) ≥ σ
else

(cid:26) (xδ

and the stopping index k∗(δ) are well-deﬁned by (32) with (31) and (33) or
(34).

REGULARIZATION BASED ON ALL-AT-ONCE FORMULATIONS

13

• There exists C > 0 independent of δ ∈ (0, δ) such that

(cid:13)(cid:13)(cid:13)xδ

(cid:13)(cid:13)(cid:13)r

X

≤(cid:13)(cid:13)x† − x0

(cid:13)(cid:13)r

X and

(cid:13)(cid:13)(cid:13)uδ

k∗(δ)

(cid:13)(cid:13)(cid:13)V

≤ C

(38)

∀δ ∈ (0, δ) :

k∗(δ) − x0

in case of minimizers of (32) with (31) and (33) and

(39)

∀δ ∈ (0, δ) : R2(xδ

k∗(δ), uδ

k∗(δ)) ≤ R2(x†, u†)

• In both cases (38), (39), the regularized approximations (xδ

in case of minimizers of (32) with (31) and (34).
verges weakly subsequentially to a solution of (1), (2) as δ → 0.

k∗(δ), uδ

k∗(δ)) con-

(cid:16)

(cid:17)S(F(xδ

)

k, uδ

k), (0, yδ)) +

k+1 − x0

(cid:13)(cid:13)r

V

(cid:13)(cid:13)x† − x0
(cid:13)(cid:13)xδ
(cid:13)(cid:13)V

αk
r

k+1 − x0

Proof. See [16, Theorem 3].
It only remains to prove boundedness of the u part in case (33). Again we
concentrate on the convergence part of the proof and use minimality and the tangential
cone condition Assumption 7 to obtain
k+1 − uδ
k+1 − xδ
S(F(xδ
k, uδ
k, uδ
≤ S(F(xδ
k, u† − uδ
k)(x† − xδ
≤ CS(1 + CSctc)δ + CSctcS(F(xδ
k), (0, yδ)) +
k, uδ
which for k ≤ k∗ − 1 by (35), (36), yields

(cid:13)(cid:13)xδ
(cid:13)(cid:13)x† − x0
(cid:13)(cid:13)r

k), (0, yδ)) +
αk
r

k), (0, yδ)) +

k) + F(cid:48)(xδ
k, uδ

k, uδ
k) + F(cid:48)(xδ

k+1 − x0

(cid:13)(cid:13)r

(cid:13)(cid:13)r

k)(xδ

k, uδ

αk
r

αk
r

X ,

X

X

1 + CSctc

(cid:13)(cid:13)r

τ
V ,

σ − CS(ctc +
≤ αk
r

(cid:13)(cid:13)x† − x0
(cid:16)

(cid:17)

which upon division by αk gives a uniform bound on(cid:13)(cid:13)xδ

(namely the one
stated in (38); obviously, (39) can be obtained in the same manner). Thus up to the

σ − CS(ctc + 1+CS ctc

we are in the same situation as in (17) and
positive factor
thus can use the same arguments to prove uniform boundedness of uδ
k∗ . As a matter of
fact, since we only consider the a posteriori regularization parameter choice now, the
situation is even simpler, since we can use the upper estimate in (35) in the identity
(18) (with (xδ

α) replaced by (xδ

α, uδ

k, uδ

k)).

)

τ

A suﬃcient condition for Assumption 7 to hold is, like in the reduced case, the

adjoint range invariance condition
F(cid:48)(˜x, ˜u) = R(˜x,˜u)

(x,u)F(cid:48)(x, u) with (cid:107)R(˜x,˜u)

(x,u) − I(cid:107) ≤ cR < 1

(40)
for some 0 < ctc < 1 and all (x, u), (˜x, ˜u) ∈ D(F) ∩ B(x0, u0), as well as linear
operators R(˜x,˜u)

(x,u) : W ∗ × Y → W ∗ × Y . More explicitly, with R(˜x,˜u)

(cid:18) R11 R12

(x,u) =

(cid:19)

R21 R22

the identity between the derivatives of F can be rewritten as

(41)

Ax(˜x, ˜u) = R11Ax(x, u)
Au(˜x, ˜u) = R11Au(x, u) + R12C

0 = R21Ax(x, u)

C(cid:48)(˜u) = R21Au(x, u) + R22C(cid:48)(u)

Actually, this is the condition that we will check for some of our examples.

Closely related is the following range invariance condition

14

B. KALTENBACHER

Assumption 8.

F(cid:48)(˜x, ˜u) = F(cid:48)(x, u)R(˜x,˜u)

(x,u) with (cid:107)R(˜x,˜u)

(x,u) − I(cid:107) ≤ cR < 1

(42)
for some 0 < ctc < 1 and all (x, u), (˜x, ˜u) ∈ D(F) ∩ B(x0, u0), as well as linear
operators R(˜x,˜u)
Again we can write the identity between the derivatives of F more explicitly by using
the block decomposition R(˜x,˜u)

(cid:18) R11 R12

(x,u) : X × V → X × V .

(cid:19)

:

(x,u) =

R21 R22

(43)

Ax(˜x, ˜u) = Ax(x, u)R11 + Au(x, u)R21
Au(˜x, ˜u) = Ax(x, u)R12 + Au(x, u)R22

0 = C(cid:48)(u)R21
C(cid:48)(˜u) = C(cid:48)(u)R22

Under this alternative condition we get convergence with an a priori choice of αk
and k∗, however only in Hilbert space. Note that convergence under such a range
invariance condition in the general Banach space setting is still an open problem also
for the conventional reduced formulation. We quote a convergence result for the case
of (34).
Proposition 5. Let o = m = r = 2 and X × V and W ∗ × V be Hilbert spaces
and let Assumptions 2, 4, 8 be satisﬁed with ctc suﬃciently small. Moreover, let τ be
chosen suﬃciently large, and let (x0, u0) be close enough to (x†, u†).
k+1(αk), uδ

Then for all k ∈ N, the iterates (xδ
apriori chosen sequence (αk)k∈N satisfying

k+1(αk)) with an

k+1) = (xδ

k+1, uδ

(44)

αk → 0 as k → ∞ and 1 ≤ αk
αk+1

≤ Cα

for some Cα ≥ 1 are well-deﬁned by (32) with (31) and (34), and with a choice of
k∗(δ) such that

(45)

k∗(δ) → ∞ and

δ√
αk∗(δ)

→ 0 as δ → 0

(xδ

k∗(δ), uδ

k∗(δ)) converges strongly to a solution of (1), (2) as δ → 0.

Proof. See [12, Theorem 2.7].

3.2. Convergence rates. Exactly as in the proof of the ﬁrst part (case of a
posteriori regularization parameter choice) of Proposition 2, using (39) we get rates
under a source condition (cf. [13, 26] for the reduced case).
Proposition 6. Let F(cid:48) satisfy Assumption 6 and let, for and some (vmod, vobs) ∈

(cid:13)(cid:13)(cid:13) < 1 the source condition (22) hold.

(cid:13)(cid:13)(cid:13)(vmod, vobs)

W × Y ∗ with L

Then for the Newton iterates according to (34) with the a posteriori choice (35),

(36) or the a priori choice (44), αk∗(δ) ∼ δ o

D(x0,u0)

ξ

(cf. (24)).

((xδ

k∗(δ), uδ

t∗ (cf. (23)), we have
k∗(δ)), (x†, u†)) = O(δo/t)

REGULARIZATION BASED ON ALL-AT-ONCE FORMULATIONS

15

3.3. Comparison of implementation. We compare one step of the the all-at-

once IRGNM to one step of the reduced IRGNM

(cid:13)(cid:13)F (xδ
k, ˜u) ∈ D(F), x ∈ X , u ∈ V(cid:9), and denote the iterates re-
where ˜D = (cid:8)(x, u, ˜u) : (xδ

k) − yδ(cid:13)(cid:13)o

k) + F (cid:48)(xδ

k)(x − xδ

(cid:107)x − x0(cid:107)r

min
x∈D(F )

Y +

(46)

V ,

α
r

1
o

sulting from the reduced version by a bar. This can be rewritten as a PDE con-
strained minimization problem (setting ˜u = S(xδ
k) =
˜u − Au(xδ

k), u = ˜u + S(cid:48)(xδ

k)(x − xδ

k, ˜u)−1Ax(xδ

(47)

min

(x,u,˜u)∈ ˜D

s.t.

k, ˜u)(x − xδ
k)

(cid:13)(cid:13)C(˜u) + C(cid:48)(˜u)(u − ˜u) − yδ(cid:13)(cid:13)o
(cid:40)

1
o

A(xδ
A(xδ

k, ˜u) = 0 ,
k, ˜u) + Ax(xδ

Y +

(cid:107)x − x0(cid:107)r

α
r

k, ˜u)(x − xδ

k) + Au(xδ

k, ˜u)(u − ˜u) = 0 ,

in particular it contains the nonlinear model A(xδ
one as constraints.

k, ˜u) = 0 as well as the linearized

Again we assume W to be reﬂexive and start with the exact penalization case
m = 1 in (32). However, the PDE constrained minimization problem to which (32)
with m = 1 and ρ suﬃciently large is equivalent (analogously to Proposition 3) is the
following

(cid:13)(cid:13)C(uδ

k) − yδ(cid:13)(cid:13)o

min

(x,u,˜u)∈ ˜D

1
o

(48)

k) + C(cid:48)(uδ

k)(u − uδ

s.t. A(xδ

k, uδ

k) + Ax(xδ

k, uδ

k)(x − xδ

Y +

α
r
k, uδ
k) + Au(xδ

(cid:107)x − x0(cid:107)r
k)(u − uδ

k) = 0

so only the linearized model appears as a constraint here, and the problem is obviously
not the same as the reduced one (47), diﬀerently from Section 2. For the sake of
completeness, we point out that exact penalization indeed holds for suﬃciently large
but ﬁnite ρ since the adjoint state wk for (48) by Assumption 1 and Proposition 4 is
well deﬁned by

Au(xδ

k, uδ

k)∗wk = −C(cid:48)(uδ

k)∗J Y

o (C(uδ

k) + C(cid:48)(uδ

k)(uδ

k+1 − uδ

k) − yδ) in V ∗

and uniformly (wrt δ ∈ (0, δ) and k ≤ k∗(δ)) bounded, thus it is possible to choose
such a uniform penalty parameter ρ ≥ supδ∈(0,δ) maxk∈{1,...,k∗(δ)} (cid:107)wk(cid:107)W .
lations (47), (32), (48) in case D(F) = X × V read as follows:
(49)

The ﬁrst order optimality conditions for the reduced and the all-at-once formu-

A(xδ
A(xδ
α J X
Au(xδ

k, uδ
k) = 0
k) + Ax(xδ
k, uδ
k)(xδ
k+1 − x0) = −Ax(xδ
r (xδ
k = −(C(cid:48)(cid:48)(uδ
k)∗zδ
k, uδ

k, uδ

k+1 − xδ
k)∗wδ
k, uδ
k+1 − uδ
k)(uδ
− (Aux(xδ
k)(xδ
k, uδ
k)∗ J Y
o (C(uδ

k+1

k, uδ

Au(xδ

k)∗wδ

k+1 = −C(cid:48)(uδ
k, uδ
for a minimizer (xδ
for the Lagrange multiplier zδ

k+1, uδ

k) + Au(xδ

k, uδ

k)(uδ

k+1 − uδ

k) = 0

o (C(uδ

k))∗ J Y
k+1 − xδ
k) + C(cid:48)(uδ

k) + C(cid:48)(uδ
k, uδ
k+1 − uδ

k)(uδ
k)(uδ
k) − yδ)

k) + Auu(xδ

k)(uδ

k+1 − uδ
k+1 − uδ

k) − yδ)
k))∗wδ

k+1

k+1) of the reduced version (47), (where the equation
k corresponding to the second PDE constraint may be

16

skipped)

B. KALTENBACHER

(50)

k, uδ

ρ J W ∗
α J X
Au(xδ

k) + Ax(xδ

k, uδ
m (A(xδ
k+1 − x0) = −Ax(xδ
r (xδ
k+1 = −C(cid:48)(uδ
k)∗wδ
k, uδ
k+1, uδ
for a minimizer (xδ

k)(xδ
k, uδ
k)∗ J Y

k+1 − xδ
k)∗wδ
k+1
o (C(uδ

(51)

A(xδ
α J X
Au(xδ

k, uδ

k) + Ax(xδ
k, uδ
k)(xδ
k+1 − x0) = −Ax(xδ
r (xδ
k+1 = −C(cid:48)(uδ
k)∗wδ
k, uδ
k+1, uδ

k+1 − xδ
k, uδ
k)∗ J Y

k)∗wδ
k+1
o (C(uδ

for a minimizer (xδ
suﬃciently large, i.e., of (48).

k) + Au(xδ

k, uδ

k)(uδ

k+1 − uδ

k)) = wδ

k+1

k) + C(cid:48)(uδ

k)(uδ

k+1 − uδ

k) − yδ)

k+1) of the all-at-once version (32), and

k) + Au(xδ

k, uδ

k)(uδ

k+1 − uδ

k)) = 0

k) + C(cid:48)(uδ

k)(uδ

k+1 − uδ

k) − yδ)

k+1) of the all-at-once version (32) in case m = 1 and ρ

(cid:19)

(cid:18)

Justiﬁcation of these ﬁrst order optimality conditions by Fr´echet diﬀerentiability
of A and C and a Zowe Kurcyuscz constraint qualiﬁcation is again trivial in case of
(32) with m > 1. In case of (47), we need surjectivity of

G(cid:48)(x, u, ˜u) =

0
0
k, ˜u) Axu(xδ
k, ˜u) Au(xδ
Ax(xδ

k, ˜u)(x − xδ

Au(xδ

k, ˜u)

k) + Axu(xδ

k, ˜u)(u − ˜u)

at (x, u, ˜u) = (xδ

k+1, uδ

k+1, uδ

k) and in case of (48) surjectivity of

G(cid:48)(x, u) =(cid:0)Ax(xδ

k)(cid:1)

k, uδ

k) Au(xδ

k, uδ

at (x, u) = (xδ
k+1, uδ
k+1), so that in both cases the constraint qualiﬁcations are satisﬁed
under Assumption 1.
In case of additional convex constrains on the parameters D(F) = C × V with
some convex set C, the second lines in (49), (50), (49) again change to variational
inequalities of the form (30).

4. Landweber iteration. Gradient methods applied to the unregularized least
squares problem (i.e., (25) or (9) with α = 0) lead to the Landweber iteration in a
reduced

(52)

x∗δ
k+1 = x∗δ
k+1 = J X∗
xδ

k − µkF (cid:48)(xδ
r∗ (x∗δ

k+1)

k)∗ J Y

o (F (xδ

k) − yδ)

and an all-at-once setting

(53)

(x∗δ
(xδ

k+1, u∗δ
k+1, uδ

k+1) = (x∗δ
k+1) = (J X∗

k , u∗δ
r∗ (x∗δ

k ) − µkF(cid:48)(xδ
q∗ (u∗δ
k+1), J V ∗

k, uδ
k+1))

k)∗ (J W ∗

m , J Y

o )(F(xδ

k, uδ

k) − (0, yδ))

with appropriately chosen stepsizes µk, cf., e.g., [16]. We here assume that X and V
are smooth and s-convex so that the duality mappings J X∗
q∗ are single valued
and in fact inverses of J X
q , respectively. Moreover, we postulate continuous
Fr´echet diﬀerentiability of A and C, and, for (52), also Assumption 1 to guarantee
well-deﬁnedness of F .

r∗ , J V ∗

r , J V

Under a tangential cone condition

(cid:107)F (˜x) + F (cid:48)(˜x)(x − ˜x) − F (x)(cid:107)Y ≤ ctc (cid:107)F (˜x) − F (x)(cid:107)Y

REGULARIZATION BASED ON ALL-AT-ONCE FORMULATIONS

17
for some 0 < ctc < 1 and all x ∈ B(x0) in the reduced case (52) or Assumption 7
in the all-at-once case (53), we have convergence of the iterates x0
k) to a
solution of (1), (2) as k → ∞ in the exact case δ = 0. Under the same conditions,
with additionally Y or W ∗ × Y being uniformly smooth, an a posteriori stopping rule
according to

k or (x0

k, u0

k∗ = min{k ∈ N : (cid:13)(cid:13)F (xδ

k), yδ)(cid:13)(cid:13)Y ≤ τ δ} ,

for (52) or (35) for (53) yields convergence of xδ
k∗(δ)) to a solution
of (5) or (1), (2) as δ → 0 holds. This is an immediate consequence of Theorems 1
and 2 in [16]. Note that even if the all-at-once tangential cone condition Assumption
7 might be hard to verify for nonlinear problems, Landweber still makes sense for
linear problems where this condition is trivially satisﬁed. And it actually makes a big
diﬀerence to the reduced version also in the linear case as we will see in the following.

k∗(δ) or (xδ

k∗(δ), uδ

4.1. Comparison of implementation. Although we do not deal with mini-
mization problems now, each reduced Landweber step can be split into a system that
resembles (28), but is has triangular (so not fully coupled) structure. Namely, using
the deﬁnition of F = C ◦ S, the chain rule, and the Implicit Function Theorem for
diﬀerentiating S

A(x, S(x)) = 0 , Ax(x, S(x)) + Au(x, S(x))S(cid:48)(x) = 0 ,

i.e.,
F (cid:48)(x)∗ J Y

o (F (x) − yδ) = −Ax(x, S(x))∗(Au(x, S(x))∗)−1C(cid:48)(S(x))∗ J Y

and setting uδ
rewrite one reduced Landweber step as

k = S(xδ

k), wδ

k = −(Au(xδ

k, uδ

k)∗)−1C(cid:48)(uδ

k)∗ J Y

o (C(uδ

o (C(S(x)) − yδ)
k) − yδ) we can

k, uδ
A(xδ
Au(xδ
k, uδ
k+1 = x∗δ
x∗δ
k+1 = J X∗
xδ

k) = 0
k)∗wδ
k − µkAx(xδ
r∗ (x∗δ

k = −C(cid:48)(uδ
k, uδ

k+1)

k)∗ J Y
k)∗wδ

k

o (C(uδ

k) − yδ)

which involves solution of a nonlinear and a linearized (adjoint) model, whereas the
all-at-once version using

(cid:18)Ax(x, u) Au(x, u)

(cid:19)∗(cid:18) J W ∗

0

C(cid:48)(u)

(cid:19)
m (A(x, u))
o (C(u) − yδ))
J Y
(cid:17)

o (C(u) − yδ)

F(cid:48)(x, u)∗(J W ∗

m , J Y

o )(F(x, u) − (0, yδ)) =

reads

x∗δ
k+1 = x∗δ
u∗δ
k+1 = u∗δ
k+1, uδ
(xδ

(cid:16)
k − µkAx(x∗δ
k − µk
Au(x∗δ
r∗ (x∗δ
k+1) = (J X∗

k , u∗δ
k , u∗δ
k+1), J V ∗

k )∗J W ∗
k )∗J W ∗
q∗ (u∗δ

m (A(x∗δ
m (A(x∗δ
k+1)) ,

k , u∗δ
k ))
k , u∗δ
k )) + C(cid:48)(u)∗J Y

(where again we have assumed W to be reﬂexive). So as opposed to all other schemes
considered here and to the reduced Landweber setting, in an all-at-once Landweber
step no linear or nonlinear model is solved.

18

B. KALTENBACHER

5. Application and Numerical Tests. Since the main diﬀerence in implemen-
tation seems to arise in the context of nonlinear PDEs (at least as far as the IRGNM
is concerned) we consider an inverse source problem for a nonlinear PDE, namely,
identiﬁcation of b in

−∆u + ξu3 = b in Ω ,

u = 0 on ∂Ω

(54)
from measurements of u in Ω ⊆ Rd, d ∈ {1, 2, 3}. Here ξ ∈ R is a known parameter,
which we use in order to study diﬀerent strengths of nonlinearity of the PDE and
also the non-elliptic case arising when ξ is smaller than the negative of the embedding
0 (Ω) → L4(Ω), so that the parameter-to-state map fails to be well-deﬁned
constant H 1
and therefore the reduced approach is not applicable. This example has already been
used in [14] to compare reduced Tikhonov regularization with an all-at-once version
of the IRGNM, both with adaptive discretization.

5.1. Veriﬁcation of convergence conditions. Indeed, even a more general
version of this example satisﬁes the structural conditions on the forward operator
(41), (43) for the all-at-once IRGNM, and also their reduced versions.
Lemma 7. Let A, C in (1), (2) have the form A(x, u) = ˆA(u) + Lx, C(u) = Cu
with L : X → W ∗, C : V → Y linear operators, ˆA : V → W ∗ possibly nonlinear and
Fr´echet diﬀerentiable, and let C†, L† denote the Moore Penrose generalized inverse
of C and L, respectively.

(a) If N ( ˆA(cid:48)(˜u) − ˆA(cid:48)(u))⊥ ⊆ N (C)⊥, then (41) is satisﬁed with

(cid:107)R(˜x,˜u)

(x,u) − I(cid:107) = (cid:107)( ˆA(cid:48)(˜u) − ˆA(cid:48)(u))C†(cid:107)
(b) If R( ˆA(cid:48)(˜u) − ˆA(cid:48)(u)) ⊆ R(L), then (43) is satisﬁed with
(x,u) − I(cid:107) = (cid:107)L†( ˆA(cid:48)(˜u) − ˆA(cid:48)(u))(cid:107)

(cid:107)R(˜x,˜u)

(c) If ˆA(cid:48)(u) is boundedly invertible, ˆA(cid:48) is continuous, and N (C) = {0}, then
for all ˜x in a suﬃciently small neighborhood of x, the reduced adjoint range
invariance condition

(55)

F (cid:48)(˜x) = R˜x

xF (cid:48)(x)

is satisﬁed with

(cid:107)R˜x

x − I(cid:107) =

(cid:113)(cid:107)C( ˆA(cid:48)(S(x)) − ˆA(cid:48)(S(˜x))) ˆA(cid:48)(S(˜x))−1C†(cid:107)2 + (cid:107)ProjR(C)⊥(cid:107)2

(d) If ˆA(cid:48)(u) is boundedly invertible, ˆA(cid:48) is continuous, and R(L) = W ∗, then the

reduced range invariance condition

(56)

F (cid:48)(˜x) = F (cid:48)(x)R˜x

x

is satisﬁed with

(cid:107)R˜x

x − I(cid:107) =

(cid:113)(cid:107)L†( ˆA(cid:48)(S(x)) − ˆA(cid:48)(S(˜x))) ˆA(cid:48)(S(˜x))−1L(cid:107)2 + (cid:107)ProjN (L)(cid:107)2

REGULARIZATION BASED ON ALL-AT-ONCE FORMULATIONS

19

ξ
0
10
100
1000
-0.5
-1.
-10
-100
-1000

τ 2
4
20
20
20
4
4
4
4
4

itaao
34
43
55
68
33
35
44
77
70

itred
32
43
56
68
32
-
-
-
-

cpuaao
0.14
0.20
0.28
0.42
0.13
0.23
0.23
0.59
0.49

cpured
0.10
0.55
0.82
1.07
0.35
-
-
-
-
Table 1

(cid:107)bδ

k∗ (δ),aao−b†(cid:107)X

(cid:107)b†(cid:107)X

(cid:107)bδ

k∗ (δ),red
(cid:107)b†(cid:107)X

−b†(cid:107)X

0.0149
0.0996
0.0721
0.0543
0.1174
0.2023
0.0768
0.2246
0.0321

0.0151
0.1505
0.0770
0.0588
0.2165
-
-
-
-

Comparison of reduced and all-at-once IRGNM

Proof. Since the derivatives here simplify to Ax(x, u) = L, Au(x, u) = ˆA(cid:48)(u),

C(cid:48)(u) = C, the assertions can be readily checked by setting

(a) R11 = I , R12 = ( ˆA(cid:48)(˜u) − ˆA(cid:48)(u))C† , R21 = 0 , R22 = I
(b) R11 = I , R12 = L†( ˆA(cid:48)(˜u) − ˆA(cid:48)(u)) , R21 = 0 , R22 = I
(d) R˜x
(c) R˜x

x = C ˆA(cid:48)(S(x)) ˆA(cid:48)(S(˜x))−1C†
x = L† ˆA(cid:48)(S(x)) ˆA(cid:48)(S(˜x))−1L

The above example (54) ﬁts into this framework with

X = L2(Ω) , V = H 2(Ω) ∩ H 1
ˆA(u) = −∆u + ζu3 , L = −id

0 (Ω) , W = L2(Ω) , Y = L2(Ω)

and therefore satisﬁes (43), and in the elliptic case ξ > ξ = −(cid:107)idH 1
0 (Ω)→L4(Ω)(cid:107) also
(56), provided C is linear. In case of full measurements relative to the model A, i.e.,
if N (C) ⊆ N ( ˆA(cid:48)(u)) for all (x, u) ∈ D(F) ∩ B(x0, u0), also (41) holds and if ξ > ξ
and even N (C) = {0}, R(C) = Y hold, then (55) is satisﬁed.

5.2. Numerical experiments. In the following, we show numerical results for
this example in the quadratic (i.e., m = o = r = 2) Hilbert space setting in the
one-dimensional situation Ω = (0, 1) with a simple ﬁnite diﬀerence discretization
on an equidistant grid of size 0.01. Computational results in 2-d with an adaptive
discretization can be found in [14].

Table 1 shows a comparison of the reduced and the all-at-once versions of the
IRGNM for diﬀerent values of the nonlinearity parameter ξ with one per cent noise
in the data. Here itaao, itred, cpuaao, cpured denote the number of iterations and
CPU times (in seconds) for the all-at-once and the reduced version, respectively. The
corresponding reconstructions are displayed in Figure 1. In both cases the sequence of
regularization parameters was chosen as αk = 10∗0.7k and the constant functions with
value zero were used as starting values for b (and u). As expected, for larger values of
ξ, i.e., stronger nonlinearity, the all-at-once version performs better. Also for negative
0 (Ω) →
values of ξ with modulus larger than the reciprocal norm of the embedding H 1
L4(Ω), for which the parameter-to state-map does not exist (and the reduced IRGNM
actually fails) we still get a reasonable behavior of the all-at-once IRGNM.

20

ξ = 1000

B. KALTENBACHER

ξ = −0.5

ξ = −1000

Fig. 1. Comparison of reduced (bottom row) and all-at-once (top row) IRGNM

ξ
0.5
5
10
-0.5
-1

τ 2
4
4
4
4
4

itaao
5178
2000000
2000000
10895
18954

itred
2697
48510
100000
2016
-

cpuaao
2.97
1293.60
1257.50
8.85
11.42

cpured
18.07
482.19
639.87
14.55
-

Table 2

(cid:107)bδ

k∗ (δ),aao−b†(cid:107)X

(cid:107)b†(cid:107)X

(cid:107)bδ

k∗ (δ),red
(cid:107)b†(cid:107)X

−b†(cid:107)X

0.0724
0.7837
0.9621
0.1406
0.2313

0.1047
0.1633
0.1632
0.2295
-

Comparison of reduced and all-at-once Landweber

A similar comparison is done for the Landweber iteration in Table 2 and Figure 2,
however, with less choices of ξ since here already for a relatively moderate (positive)
nonlinearity the all-at-once Landweber iteration fails, which is no surprise, since it
does not contain solution of models at all. For negative values of ξ, like in the IRGNM
tests, the reduced version only works with suﬃciently small |ξ|, whereas the all-at-once
version is also able to cope with absolutely slightly larger negative values of ξ.

6. Further remarks on the comparison between all-at-once and reduced

version.

6.1. Comparison of conditions on forward operator. Assumptions 2, 3,
and 6 cannot be directly compared with their respective reduced versions in the sense
that one of them would imply the other. They have to be checked on a case by case
basis, which is beyond the scope of this paper but will be subject of future research,
e.g., for the examples from the introduction.

As far as the more structural conditions on nonlinearity of the forward operator
(tangential cone condition as well as range invariance and adjoint range invariance)
are concerned, they seem to be quite diﬀerent in the reduced and in the all-at-once
setting. For instance, both conditions have been proven to hold for certain special
cases of Example 1 in the reduced setting see, e.g., [5, 9, 15] but they do not seem to
be rigorously veriﬁable for these cases in the all-at-once setting. To demonstrate this
for the adjoint range invariance condition (41) we consider the two special cases

REGULARIZATION BASED ON ALL-AT-ONCE FORMULATIONS

21

ξ = 5

ξ = 0.5

ξ = −0.5

Fig. 2. Comparison of reduced (bottom row) and all-at-once (top row) Landweber

{1, 2, 3};

(i) identify c, while a ≡ 1, b ∈ H−1(Ω) given, (the so-called c problem), d ∈
(ii) identify a, while b ∈ H−1(Ω), c ≡ 0 given (the so-called a problem), d = 1;
with full observations, i.e., C the embedding operator V → Y . Note that these are
exactly the cases in which the analogous adjoint range invariance condition can be
veriﬁed for the reduced formulation.

For simplicity of exposition we switch to inhomogeneous Dirichlet instead of Neu-

mann boundary conditions, i.e.,

−∇(a∇ˆu) + cˆu = b in Ω ,

ˆu = gΓ on ∂Ω ,

which with the harmonic extension g ∈ H 1(Ω) of the inhomogeneous Dirichlet bound-
ary data gΓ to the interior and ˆu = u + g, can be formulated variationally as

u ∈ H 1

0 (Ω) and for all v ∈ H 1

0 (Ω) :

a∇(u + g)∇v + c(u + g)v

= 0

(cid:90)

(cid:16)

Ω

(cid:17)

where we assume gΓ to be chosen such that

|u + g| ≥ ˆc > 0 in Ω in case (i) and |(u + g)(cid:48)| ≥ ˆc > 0 in Ω in case (ii)

for all (x, u) ∈ D(F) ∩ B(x0, u0), a condition which can be incorporated into the
deﬁnition of D(F).

For the c problem (i) with x = c we make use of higher elliptic regularity and

consider the function space setting

X = L∞(Ω) , V = H 2(Ω) ∩ H 1

0 (Ω) , W = L2(Ω) , Y = Lp(Ω)

with p ∈ [1,∞]. We have Ax(x, u)h = h(u + g), Au(x, u)v = −∆v + xv and set

R11w∗ =

˜u + g
u + g

w∗

R21 = 0

R12y =

R22 = I

u − ˜u
u + g

(−∆y) +

˜x(u + g) − x(˜u + g)

u + g

y

22
for w∗ ∈ L2(Ω), y ∈ Lp(Ω). to formally obtain (41). It remains to bound the diﬀerence
(cid:107)R(˜x,˜u)

(x,u) − I(cid:107) =(cid:112)(cid:107)R11 − I(cid:107)2

W ∗→W ∗ + (cid:107)R12(cid:107)Y →W ∗ , which we do by estimating

B. KALTENBACHER

(cid:107)R11w∗ − w∗(cid:107)W ∗ =

(cid:13)(cid:13)(cid:13)(cid:13) ˜u − u

u + g

w∗(cid:13)(cid:13)(cid:13)(cid:13)L2(Ω)

(cid:13)(cid:13)(cid:13)(cid:13) ˜u − u

≤

u + g
(cid:107)˜u − u(cid:107)V (cid:107)w∗(cid:107)W ∗

≤ CH 2→L∞

ˆc

(cid:107)w∗(cid:107)L2(Ω)

(cid:13)(cid:13)(cid:13)(cid:13)L∞(Ω)
(cid:13)(cid:13)(cid:13)(cid:13)L2(Ω)

y

for any w∗ ∈ L2(Ω) and

(cid:107)R12y(cid:107)W ∗ =

(cid:13)(cid:13)(cid:13)(cid:13) u − ˜u

u + g

(−∆y) +

˜x(u + g) − x(˜u + g)

u + g

for any y ∈ Lp(Ω), which we are supposed to estimate by a small multiple of (cid:107)y(cid:107)Lp(Ω).
However, the appearance of −∆y under the L2(Ω) norm indicates that although the
identity (41) formally holds, the required estimate on (cid:107)R − I(cid:107) in (40) does not seem
to be obtainable.

For the 1-d a problem (ii) with the somewhat more convenient boundary condi-

tions

−(aˆu(cid:48))(cid:48) = 0 on (0, 1) ,

ˆu(cid:48)(0) = g1 ,

ˆu(1) = g1 ,

and under the assumption that a takes the known value a0 at the left hand boundary
point, we use the extension g(s) = g1(s− 1) + g1, s ∈ (0, 1) and the Ansatz a = a0 + x,
ˆu = u + g. With
X = {x ∈ W 1,q(0, 1) : x(0) = 0} ,
W ∗ = Lq(0, 1) ,
where the prescribed boundary values are to be understood in a trace sense and p ∈
[1,∞], q ∈ (1,∞), A(x, u) = −(a0 +x)u(cid:48)(cid:48)−x(cid:48)(g1 +u(cid:48)), Ax(x, u)h = −hu(cid:48)(cid:48)−h(cid:48)(g1 +u(cid:48)),
Au(x, u)v = −(a0 + x)v(cid:48)(cid:48) − x(cid:48)v(cid:48) and setting

V = {x ∈ W 2,q(0, 1) ∩ H 1(0, 1) : v(cid:48)(0) = 0} ,

Y = Lp(0, 1) ,

(cid:18) (˜u + g)(cid:48)

(u + g)(cid:48)

(cid:90) ·

0

(cid:19)(cid:48)

R11w∗ =

w∗ ds

R12y = −

(cid:18) (a0 + ˜x)(u + g)(cid:48) − (a0 + x)(˜u + g)(cid:48)

(cid:19)(cid:48)

y(cid:48))

(u + g)(cid:48)

R21 = 0
we formally satisfy (41). Also here the estimate of the R11 − I term of R − I works
out

R22 = I

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
(cid:18) ˜u(cid:48)(cid:48)(g1 + u(cid:48)) − u(cid:48)(cid:48)(g1 + ˜u(cid:48))

(g1 + u(cid:48))2

(cid:90) ·

0

w∗ ds

(cid:19)(cid:48)
(cid:17)(cid:107)w∗(cid:107)L1(0,1)

+

˜u(cid:48) − u(cid:48)
g1 + u(cid:48) w∗

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)Lq(0,1)

(cid:107)R11w∗ − w∗(cid:107)W ∗ =

≤(cid:16) 1

(cid:107)˜u(cid:48)(cid:48) − u(cid:48)(cid:48)(cid:107)Lq(0,1) +
ˆc
(cid:107)˜u(cid:48) − u(cid:48)(cid:107)L∞(0,1)(cid:107)w∗(cid:107)Lq(0,1) ,
1
ˆc

+

1

ˆc2(cid:107)u(cid:48)(cid:48)(cid:107)Lq(0,1)(cid:107)˜u(cid:48) − u(cid:48)(cid:107)L∞(0,1)

which by embeddings can be estimated by some constant times (cid:107)˜u − u(cid:107)V (cid:107)w∗(cid:107)W ∗ ,
whereas R12y contains derivatives of y that prevent an estimate of its Lq(0, 1) norm
by a small multiple of (cid:107)y(cid:107)Lp(0,1).

REGULARIZATION BASED ON ALL-AT-ONCE FORMULATIONS

23

6.2. Comparison of source conditions in reduced and all-at-once set-
ting. As a consequence of Proposition (3) we expect the source condition (22) to be
equivalent for the reduced and the all-at-once version of Tikhonov if m = 1 (and ρ
is large enough). However, for m > 1, at a ﬁrst glance there might be a diﬀerence,
so we now consider the special case of m = o = r = 2 in a Hilbert space setting and
√
compare the reduced and the all-at-once benchmark source condition, i.e., the one
δ) convergence of the error norm in both versions. The reduced source
yielding O(
condition

x† − x0 = RX F (cid:48)(x†)∗v

for some v ∈ Y ∗ with the abbreviations L = Ax(x†, u†), K = Au(x†, u†) and the Riesz
isomorphism RX : X∗ → X is equivalent to

(57)

x† − x0 = −RX (K−1L)∗C(cid:48)(u†)∗v

whereas the all-at-once version (22)

(cid:19)

(cid:18)x† − x0

u† − u0

= RX×V F(cid:48)(x†, u†)∗(cid:18)vmod

(cid:19)

(cid:18)

=

vobs

RV (K∗vmod + C(cid:48)(u†)∗vobs)

RX L∗vmod

(cid:19)

for some (vmod, vobs) ∈ W × Y ∗ after elimination of vmod (relying on Assumption 1)
and using u† = S(x†), −K−1L = S(cid:48)(x†) can be rewritten as

vmod = (K∗)−1(cid:16)

V (u† − u0) − C(cid:48)(u†)∗vobs(cid:17)

R−1

x† − x0 + RX ((K−1L)∗R−1

V (K−1Lx† + u0) = −RX (K−1L)∗C(cid:48)(u†)∗vobs .

In the linear case Ax ≡ L, Au ≡ K, with S = −K−1L, and the positive deﬁnite
operator T = I + RX S∗R−1

V S the latter is equivalent to
T (x† − x0) = −RX (K−1L)∗(C(cid:48)(u†)∗vobs + R−1

V (K−1Lx0 + u0)) .

(58)

(59)

(60)

Thus, replacing the linear model Ku + Lx = 0 by its transformed version Ku +
LT −1 ˜x = 0, we see that both source conditions (57) and (60) are equivalent in the
Hilbert space case (even with possibly nonlinear observations) provided the initial
point satisﬁes the model. This is actually not surprising in view of well-known converse
√
results for linear inverse problems cf., [6, Section 4.2] and the fact that, e.g., the
δ) convergence rate
respective version of Tikhonov regularization yields the same O(
under both conditions.

However, there might still be a considerable diﬀerence in the nonlinear and/or
Banach space case setting with variational source conditions. This conjecture is sup-
ported by the observation that the nonconvergence at the right endpoint of the interval
for the respective reduced version in case ξ = −0.5 of Figure 1 and cases ξ = ±0.5 of
Figure 2 (as expected since the diﬀerence between x† and x0 in this point is known
to lead to violation of the reduced benchmark source condition) seems to be relaxed
in the all-at-once versions.

7. Conclusions and Outlook. All-at- once versions of regularization methods
can oﬀer advantages over their classical reduced counterparts when it comes to avoid-
ing explicit use of parameter-to-state maps, i.e., of exactly solving possibly nonlinear
models in each step of iterative methods. More precisely, while there is no signiﬁcant
diﬀerence in the implementation of all-at-once and reduced Tikhonov regularization,

24

B. KALTENBACHER

in reduced Newton iterations one has to solve nonlinear and linear models in each
step, while an all-at-once Newton step only requires to solve linearized models. Still
going further, as opposed to reduced Landweber, which amounts to solving a nonlin-
ear and an adjoint linear model in each step, there is no model solved at all in an
all-at-once Landweber step.

It remains to more thoroughly compare source conditions and restrictions on
the nonlinearity like tangential cone and (adjoint) range invariance conditions for
some relevant model problems and for real applications. Moreover we will investigate
more general data misﬁt and regularization functionals, as well as other regularization
paradigms.

REFERENCES

[1] A. Bakushinskii, The problem of the convergence of the iteratively regularized Gauß-Newton

method, Comput. Maths. Math. Phys., 32 (1992), pp. 1353–1359.

[2] A. Bakushinsky and M. Kokurin, Iterative Methods for Approximate Solution of Inverse

Problems, vol. 577 of Mathematics and Its Applications, Springer, Dordrecht, 2004.

[3] M. Burger and W. M¨uhlhuber, Iterative regularization of parameter identiﬁcation problems
by sequential quadratic programming methods, Inverse Problems, 18 (2002), pp. 943–969.
[4] M. Burger and W. M¨uhlhuber, Numerical approximation of an SQP-type method for pa-

rameter identiﬁcation, SIAM J. Numer. Anal., 40 (2002), pp. 1775–1797.

[5] H. Engl, K. Kunisch, and A. Neubauer, Convergence rates for Tikhonov regularisation of

non-linear ill-posed problems, Inverse Problems, 5 (1989), pp. 523–540.

[6] H. W. Engl, M. Hanke, and A. Neubauer, Regularization of Inverse Problems, Kluwer

Academic Publishers, Dordrecht, 1996.

[7] J. Flemming, Generized Tikhonov Regularization: Basic Theory and Comprehensive Results
on Convergence Rates, Shaker Verlag, Aachen, 2012. PhD thesis, TU Chemnitz 2011,
http://nbn-resolving.de/urn:nbn:de:bsz:ch1-qucosa-78152.

[8] E. Haber and U. M. Ascher, Preconditioned all-at-once methods for large, sparse parameter

estimation problems, Inverse Problems, 17 (2001), p. 1847.

[9] M. Hanke, A. Neubauer, and O. Scherzer, A convergence analysis of the Landweber itera-

tion for nonlinear ill-posed problems, Numer. Math., 72 (1995), pp. 21–37.

[10] B. Hofmann, B. Kaltenbacher, C. P¨oschl, and O. Scherzer, A convergence rates result for
Tikhonov regularization in Banach spaces with non-smooth operators, Inverse Problems,
23 (2007), pp. 987–1010.

[11] T. Hohage and F. Werner, Iteratively regularized Newton methods with general data mis-
ﬁt functionals and applications to Poisson data, Numerische Mathematik, 123 (2013),
pp. 745–779.

[12] B. Kaltenbacher, Some Newton type methods for the regularization of nonlinear ill–posed

problems, Inverse Problems, 13 (1997), pp. 729–753.

[13] B. Kaltenbacher and B. Hofmann, Convergence rates for the iteratively regularized Gauss-

Newton method in Banach spaces, Inverse Problems, 26 (2010), p. 035007.

[14] B. Kaltenbacher, A. Kirchner, and B. Vexler, Goal oriented adaptivity in the IRGNM for
parameter identiﬁcation in PDEs II: all-at once formulations, Inverse Problems, (2014),
p. 045002.

[15] B. Kaltenbacher, A. Neubauer, and O. Scherzer, Iterative regularization methods for
nonlinear ill-posed problems, vol. 6 of Radon Series on Computational and Applied Math-
ematics, Walter de Gruyter GmbH & Co. KG, Berlin, 2008.

[16] B. Kaltenbacher, F. Sch¨opfer, and T. Schuster, Convergence of some iterative methods
for the regularization of nonlinear ill-posed problems in Banach spaces, Inverse Problems,
25 (2009). 065003 doi: 10.1088/0266-5611/25/6/065003.

[17] A. Kirsch, An Introduction to the Mathematical Theory of Inverse Problems, Springer, New

York, 1996.

[18] J. Nocedal and S. Wright, Numerical Optimization, Springer Series in Operations Research
and Financial Engineering, Springer New York, 2006, https://books.google.at/books?id=
VbHYoSyelFcC.

[19] C. P¨oschl, Tikhonov regularization with general residual term, PhD thesis, University of Inns-

bruck, 2008.

[20] O. Scherzer, M. Grasmair, H. Grossauer, M. Haltmeier, and F. Lenzen, Variational

REGULARIZATION BASED ON ALL-AT-ONCE FORMULATIONS

25

Methods in Imaging, Springer, New York, 2008.

[21] T. Schuster, B. Kaltenbacher, B. Hofmann, and K. Kazimierski, Regularization Methods
in Banach Spaces, de Gruyter, Berlin, New York, 2012. Radon Series on Computational
and Applied Mathematics.

[22] F. Tr¨oltzsch, Optimal control of partial diﬀerential equations, vol. 112, American Mathemat-
ical Society, Providence, RI, 2010. Theory, methods and applications, Translated from the
2005 German original by J¨urgen Sprekels.

[23] T. van Leeuwen and F. J. Herrmann, A penalty method for pde-constrained optimization in
inverse problems, Inverse Problems, 32 (2016), p. 015007, http://stacks.iop.org/0266-5611/
32/i=1/a=015007.

[24] C. R. Vogel, Computational Methods for Inverse Problems, SIAM, Providence, 2002.
[25] F. Werner, Inverse problems with Poisson data: Tikhonov-type regularization and iteratively
regularized Newton methods, Der andere Verlag, Uelvesb¨ull, 2012. PhD thesis, University
of G¨ottingen 2012, http://num.math.uni-goettingen.de/∼f.werner/ﬁles/diss frank werner.
pdf.

[26] F. Werner, On convergence rates for iteratively regularized newton-type methods under a

lipschitz-type nonlinearity condition, Journal of Inverse and Ill-Posed Problems, (2015).

