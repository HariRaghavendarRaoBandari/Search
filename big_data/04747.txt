6
1
0
2

 
r
a

 

M
5
1

 
 
]
L
C
.
s
c
[
 
 

1
v
7
4
7
4
0

.

3
0
6
1
:
v
i
X
r
a

Topic Modeling Using Distributed Word Embeddings

Ramandeep S. Randhawa∗

University of Southern California

Parag Jain†
TopicIQ, Inc.

Gagan Madan ‡

IIT Delhi

February 12, 2016

Abstract

We propose a new algorithm for topic modeling, Vec2Topic, that identiﬁes the main topics
in a corpus using semantic information captured via high-dimensional distributed word em-
beddings. Our technique is unsupervised and generates a list of topics ranked with respect to
importance. We ﬁnd that it works better than existing topic modeling techniques such as Latent
Dirichlet Allocation for identifying key topics in user-generated content, such as emails, chats,
etc., where topics are diﬀused across the corpus. We also ﬁnd that Vec2Topic works equally
well for non-user generated content, such as papers, reports, etc., and for small corpora such as
a single-document.

1

Introduction

Understanding an individual’s key topics of interest to meet their customized needs is an important
challenge for personalization and information ﬁltering applications, such as recommender systems.
With the proliferation of diverse types of applications and smart devices, users are generating a large
amount of text through their emails, chats, and on social media, such as Twitter and Facebook.
This data typically contains many words that reﬂect their interests: for example, if we consider the
work emails of someone who works on datacenter servers, they will likely have several semantically-
similar words such as processors, cloud computing, virtualization etc. Individual emails, however,
tend to be more about the context in which these words are used, for instance, meetings, status
updates, follow-ups, and so on. The topics themselves are diﬀused across all the emails — our goal
in this paper is to build an unsupervised algorithm that automatically infers these key topics of
interest.

State-of-the-art topic modeling algorithms, such as Latent Dirichlet Allocation (LDA, [6, 2]),
have been successfully applied to discover the main topics across a large collection of documents
for some time now, and are a natural candidate for solving the problem at hand. The typical goal
of these methods has been to organize this collection according to the discovered topics — they
work well for news articles, papers in a journal, etc., where the document is about the key topics,
and the same words show up several times. User-generated content, however, is usually about the
context: actual keywords show up infrequently, and are surrounded by several contextual words
such as meetings, status, email, thanks, etc. Because of this diﬀerence in structure of corpus, LDA

∗Marshall School of Business, e-mail: ramandeep.randhawa@marshall.usc.edu
†e-mail: paragjain78@gmail.com
‡e-mail: gagan.madan1@gmail.com

1

Table 1: Former Enron CEO Kenneth Lay’s emails: Top-10 words from the Top-3 scoring topics
in Vec2Topic compared with three LDA-based topics

Topic 1
business

development

market

management

gas
oil

technology

policy

industry

future

Vec2Topic
Topic 2
power
supply

cost
risk

Topic 3
strategy
product
internet
solutions
service

tax

demand
california
electricity
company
emission
software
reduction marketing

system

budget

Topic A
meeting

email
time

information

business
company
thanks

conference

LDA
Topic B
senate

tax
sept
gift

speaker
truth

computer

arrival

market

year

congressman

result

Topic C
conﬂict

libya

developer

end

context
delegate

conf
guide
gap
click

tends to cluster topics with similar contexts, and for each topic tends to capture the most frequently
used words. It, however, fails to capture the key topics across the entire corpus.

In this paper, we propose a new technique of topic modeling, Vec2Topic, that is aimed toward
capturing the key topics across user-generated content. Our algorithm consists of two main ideas.
The ﬁrst idea is that a key topic in a corpus should contain a large number of semantically-similar
words that relate to that topic. We capture this notion using a depth measure of a word, that
helps identify clusters with the highest density of semantically-similar words. We leverage word
semantics as captured by high-dimensional distributed word embeddings. Speciﬁcally, we perform
agglomerative clustering on these distributed word vectors over the vocabulary, and we use the
resulting dendrogram to compute the depth score for a word as the number of links between the
word and the root node. Clusters that contain words with the highest depth scores reﬂect the user’s
key topics of interest. Our second idea is aimed at deriving good labels that best describe these key
topics — the idea is that such keywords not only have high depth, but also show up in the context
of a large number of diﬀerent words. We capture this using a degree measure of a word, which we
deﬁne as the count of the number of unique words that co-occur with the word within a context
window in the corpus. We then combine the depth and degree measures into a single word score.
We use this score to rank all the topics in the corpus.

To illustrate this concept, we ran Vec2Topic on Enron’s publicly available email corpus. Table 1
shows the results on former Enron CEO Kenneth Lay’s emails. Topics 1-3 are the top-3 topics
identiﬁed by Vec2Topic in decreasing order of importance; each topic is listed with the ten highest
scoring constituent words. We can see that the most important topic is about the corporate
functions of a CEO (business, development, market, policy, etc.), the next topic reﬂects that Enron
is an energy company (power, supply, demand, electricity, etc.), and the third topic is more about
its operating aspects (product, strategy, marketing, budget, etc.). We then contrast it with LDA1,
which tends to cluster emails with similar context into topics. In Table 1, we notice that LDA’s
Topic A captures words like meeting, email, time, thanks, etc., that show up frequently in the
context of his emails; Topic C seems to capture a speciﬁc theme on the Libya conﬂict. While these
are themes in Mr. Lay’s emails, they do not capture the overall essence of who Mr. Lay is, what

1We ran LDA with K = 10, 25, and 50 and picked the topics that seemed most relevant.

2

Table 2: Former Enron Managing Director of Research V. Kaminski’s emails: Top-10 words from
the Top-3 scoring topics in Vec2Topic compared with three LDA-based topics

Vec2Topic
Topic 2

investment

market
equity
asset

insurance
banking

bank
capital
markets

Topic 1
analysis
model

approach
probability
modeling
calculation

method

estimation
volatility
pricing

technology

ferc

Topic 3

Topic A

government
commission

debt
tax

authority

policy

enron
thanks
email
market

time
price

payment

company

requirement
regulation

need
work
year

LDA
Topic B

conjecture

idg
eia

involvement

arkansas
luncheon

piano
ﬁre

protection

answer

Topic C
interest

increment

optimisation

garp
klaus
freebie
plateau

uhc

virtue

regression

he works on, and what his key interests are across all his communication. Vec2Topic does a much
better job in capturing this insight, and in a sense is answering a diﬀerent question than LDA does.
Table 2 shows similar results on Enron’s Former Managing Director of Research, V. Kaminski’s
emails — again, we see that it captures key topics such as modeling and analysis, investment, and
government-related topics, which are fairly reﬂective of his work portfolio.

To summarize, we propose a new architecture for topic modeling that is geared for extracting
key topics of interest from user generated text content. We show that it works better than existing
topic modeling techniques such as LDA. We also study its performance on non-user generated
content, and ﬁnd that it works equally well on such datasets. In particular, we show results on the
set of all the accepted papers from NIPS 2015 conference. We also demonstrate its eﬃcacy when
run on a single-document by running it on the annual ﬁnancial report of Apple.

2 Background

In 2003, LDA was introduced as a generative probabilistic topic modeling model to manage large
document archives, [6]. The goal was to discover the main themes across a large collection of
documents, and organize the collection according to the discovered themes. The intuition behind
LDA is that documents have a latent topic structure; each document is a probability distribution
over topics, and each topic in turn is a distribution over a ﬁxed vocabulary. Words in the vocabulary
are represented by 1-of-V encoding, where V is the size of the vocabulary — there is no notion of
semantic similarity between words. The central problem is to use the observed documents to infer
the hidden topic structure.

Since then, a huge body of work has been done to relax and extend the statistical assumptions
made in LDA to uncover more sophisticated structure in the text: Topic models that assume that
topics generate words conditional on the previous word [25]; dynamic topic models that respect
the ordering of documents [4]; Bayesian non-parametric topic models that generate a hierarchy of
topics from the data [3, 22]; Correlated topic models that allow topics to exhibit correlation [5];
Relational topic models that capture both the topic model and a network model for the documents
[7].

3

Our algorithm is inspired by the recent work in learning word vector representations using
neural networks [1, 8, 15, 23, 9]. In this formulation, each word is represented by a vector that is
concatenated or averaged with other word vectors in a context, and the resulting vector is used
to predict other words in the context. The outcome is that after the model is trained, the word
vectors are mapped into a vector space such that semantically-similar words have similar word
representations (e.g. “strong” is close to “powerful”). [1] used a feedforward neural network with a
linear projection layer and a non-linear hidden layer to jointly learn the word vector representations
and a statistical language model.
[8] and [9] leveraged distributed word vectors to show that
neural network based models match or outperform feature-engineered systems for standard Natural
Language Processing (NLP) tasks that include part-of-speech tagging, chunking, named entity
recognition, and semantic role labeling. [10] introduced a technique to learn better word embeddings
by incorporating both local and global document context, and account for homonymy and polysemy
by learning multiple embeddings per word.

[12, 13, 14] introduced Word2Vec and the Skip-gram model, a very simple method for learn-
ing word vectors from large amounts of unstructured text data. The model avoids non-linear
transformations and therefore makes training extremely eﬃcient. This enables learning of high-
dimensional word vectors from huge datasets with billions of words, and millions of words in the vo-
cabulary. High-dimensional word vectors can capture subtle semantic relationships between words.
For example, word vectors can be used to answer analogy questions using simple vector algebra:
vKing − vman + vwoman = vQueen, where vx denotes the vector for the word x. [17] later introduced
GloVe, Global Vectors for Word Representation, which combines word-word global co-occurrence
statistics from a corpus, and context based learning similar to Word2Vec to deliver an improved
word vector representation. Word vectors are an attractive building block and are being used as
input for many neural net based natural language tasks such as sentiment analysis [18, 19, 20, 21],
question and answer systems, [11, 26], and others.

3 Methodology

In this section, we describe our algorithm for identifying the key topics underlying a corpus. Our
approach is three-fold. First, we build distributed word embeddings for the vocabulary of the
corpus. Second, we cluster the word embeddings using K-means to yield K clusters of semantically-
related words. We implement K-means using the standard euclidean distance metric, but with all
word vectors normalized so that their norm is unity. We identify each of the K clusters obtained
through K-means as a topic. Third, we score the importance of each topic and label the keywords
that best describe the topic.

The core of Vec2Topic lies in the third step, i.e., scoring the importance of topics. So, we will
focus on describing this next in Section 3.1. For ease of exposition, we proceed by assuming that
the ﬁrst and second steps have been completed. That is, we are in possession of good distributed
word embeddings for all nouns and nouns phrases in the corpus, and that these have been clustered
using K-means. We formally discuss how to build the word embeddings in Section 3.2. Algorithm 1
describes all steps of Vec2Topic.

4

3.1 Scoring importance of topics

We put forth the basic idea that: a core topic in a corpus should contain a large number of words
that relate to that topic, and further, these words, though distinct, should have similar meanings.
For instance, we expect the email corpus of Mr. Kaminski (who was Enron’s Managing Director
of Research) to contain many modeling-related words such as “probability”, “standard deviation,”
“covariance”, etc. Further, when considered in the vector space of the word embeddings, these words
should form a tight cluster with a large number of words that are near each other. We do expect
additional clusters to form, for instance, time-related words such as days of the week and names
of months would have vector representations that would be close to each other. However, such
a time-related cluster would be much less dense than the modeling-related one because modeling
would consist of a much larger number of words that are closer in the high-dimensional space. In
this fashion, identifying the densest clusters can help us identify the core topics.

To formally capture this notion of cluster density, we use the notion of depth of a word. In par-
ticular, we take all the words in our vocabulary and perform hierarchical (agglomerative) clustering
on the corresponding word vectors. This is an iterative method that starts by placing all words in
their own cluster, and then the proceeds by merging the two closest clusters in each step, until a
single cluster that includes all words is obtained. We use the typical cosine distance measure for
this clustering approach, i.e., for any two word vectors wx and wy, we deﬁne the distance

d(wx, wy) = 1 −

wx

k wx k2

·

wy

k wy k2

,

(1)

where k · k2 denotes the Euclidean norm. As an output of this clustering method, we obtain a
dendrogram, which is a tree with each word represented by a leaf node and every non-leaf node
representing a cluster that contains the words corresponding to its leaf nodes. The root node of
the dendrogram reﬂects the cluster of all words. We modify this dendrogram by normalizing the
length of all links between parent and children nodes to unity. We then deﬁne the depth of a word
as the (minimum) distance of the (leaf) node that corresponds to the word from the root node on
this modiﬁed tree. Thus, the depth of a word can also be understood as the minimum number of
links that need to be traversed to travel from the word node to the root node. That is,

depth(w) = No. of links between root node and w.

(2)

Table 3 lists the ten deepest words for Mr. Kaminski’s email corpora.

We next explain this measure in more detail by focusing on Mr. Kaminski’s email corpus.
Figure 1 (left) displays the modiﬁed dendrogram corresponding to the agglomerative clustering over
the vocabulary. This dendrogram preserves the clusters of the agglomerative clustering approach,
but creates links with unit length at each level. At the top of the dendrogram is the root node;
words that cluster close to the root do not have many semantically-similar words close to them in
vector space, and hence have low depth. The words with the highest depth are the ones that have
many semantically-similar words that are close to them, and therefore, are farthest away from the
root. The topic cluster that the deepest words belong to reﬂects the core topic for the corpus.

Figure 1 (right) displays our word embeddings in two-dimensional space. We clearly see that
words with the highest depth are very close to each other in vector space, and the topic they belong
to is the core topic. Our word embeddings are 325-dimensional vectors (which will be described in
more detail in Section 3.2) so visualizing them in two-dimensions perfectly is not possible. However,

5

Table 3: Mr. Kaminiski’s email corpus: Top-10 words based on Depth and Score

Depth

approximation

probability
covariance
estimation
variance

calculation
convolution

Score

analysis
model

approach
probability
modeling
calculation
investment

parameter estimation

standard deviation

method
market

coeﬃcient

estimation

Depth

Core Topic

Deepest Words
approximation
probability
covariance
estimation
variance

Core Topic

Deepest Words

Figure 1: Mr. Kaminski’s vocabulary: (Left) Depth view of hierarchical clustering and (Right)
Word vectors plotted using t-SNE. The blue box encloses the deepest words, and the red box (Left)
and red colored points (Right) indicate the core topic that contains the deepest words.

6

we use the t-SNE technique [24], which does a remarkable job of dimension-reduction by trying to
ensure that the distances in two dimensions are reﬂective of those in the higher-dimensional space.
We notice that the depth measure helps identify the core topics, however, the deepest words
are by themselves not suﬃcient to understand or label the overall topic to which they pertain. For
instance, Mr. Kaminski’s deepest words are “approximation, probability, covariance,” which are
quite specialized and do not explicitly reveal their overall topic.

To identify words that provide good labels for the key topics, we next put forth our second
idea: topical words that are indicative of core topics would be used in the context of a large number
of diﬀerent words. For instance, a modeling-related topic in Mr. Kaminski’s email corpus should
contain “model” or a similar word as a topical word, and this word should be used in the context
of many words, which would indicate that Mr. Kaminski is communicating about diﬀerent types
of models, with diﬀerent people, etc. We use the degree of a word as another independent measure
that quantiﬁes this notion of “topicality.” We compute the degree of the words as follows: we build
a graph of the corpus by dividing the corpus into individual sentences; we create a link between
any two words of the vocabulary V that co-occur in the same sentence; then, we deﬁne the number
of neighbors of each word in this graph as the degree of that word. That is,

degree(w) = No. of unique words co-occurring with w.

(3)

We next combine the depth and degree measures in (2) and (3) to create an overall score for each
word. We do so by normalizing depth and degree measures by their maximum values across the
vocabulary. The degree measure tends to be quite skewed, so we take a logarithmic transformation
before scaling by the maximum. Formally, the score is deﬁned as: for v ∈ V ,

Score(v) =(cid:18)
×(cid:18)

depth(v)

maxu∈V depth(u)(cid:19)α
log(1 + maxu∈V degree(u))(cid:19)β

log(1 + degree(v))

,

(4)

where α and β are normalization parameters; in all our experiments we set α = β = 1 and we
discuss a generalization of this in Section 4.4. Intuitively, the deepest words pertain to core topics
but can be too specialized to indicate the topic. So, to identify the topics, we need to identify
words that are similar in meaning to the deep words but are also used in various contexts. That is,
these words should have both high depth and high degree. The score in (4) reﬂects this intuition
by formally multiplying the two measures, after scaling them appropriately. Note that because we
take the logarithmic transformation of degree, we consider (1 + degree(v)) to avoid the argument
of the logarithm from taking the value zero.

Table 3 displays the top 10 words based on this score for Mr. Kaminski’s corpus. We believe
that this list of words seems quite consistent with what one would expect to be his important words
and provides labels for the core topics appropriately. For instance, “analysis” and “model” are the
top words, which represent the core topic of modeling quite well.

Once the words have been scored, we compute a score for each topic by averaging the score of

the words that comprise the corresponding cluster (based on K-means clustering). That is,

Score of T opici = Pv∈T opici

|T opici|

Score(v)

.

(5)

7

Topic 2
investment
equity

market

government

commission 

debt

approach
analysismodel

Topic 3

Topic 1

Figure 2: Visualizing the topics captured from Mr. Kaminski’s corpus using t-SNE; Top-3 words
of the top-3 topics are marked.

Thus, we obtain a sorted list of topics, with higher scoring topics identiﬁed as being more important
than lower scoring ones. Tables 1 and 2 depict the topics with the highest three scores from
Mr. Lay’s and Mr. Kaminski’s corpora, respectively. In both cases, the topics are numbered in
decreasing order of the score; Topic 1 has the highest score. Notice that within each topic, the
constituent words are also ranked based on the word score. In this manner, each topic is represented
by the words most relevant to that topic. Figure 2 visualizes K = 10 topics using the vocabulary
of Mr. Kaminski’s emails. The ten clusters are labeled in diﬀerent colors; the top-3 topics are
numbered as in Table 2 and their top-3 words are also displayed.

3.2 Building distributed word embeddings

A natural way to build distributed word embeddings is to apply a standard technique such as
skip-gram [12] on the given corpus. This approach however assumes that the training corpus is
large enough, and words show up in several diﬀerent contexts to develop semantically accurate
word representations; trained models have been built on corpuses that have billions of tokens and
millions of words in the vocabulary [12, 13]. User generated data, however, tends to have much
smaller corpus and vocabulary size, which does not allow understanding the relative meaning of
words suﬃciently; nevertheless, training this model on the data does capture how these words are
used in the user’s context.

To overcome this limitation, we take a two step approach: we learn “global” word embeddings
using the skip-gram technique on a knowledge-base — these word embeddings tend to capture
the generic meaning of words in widely used contexts. We then augment these with “local” word
embeddings learned from the user’s data to capture their context to generate our desired word
vectors. Denoting the knowledge-base vectors by k : V → Rκ, where V is the knowledge-base
vocabulary, and our local word vectors by ℓ : V → Rλ, we use the concatenated word vectors

8

Algorithm 1 Vec2Topic
Input:

Text corpus
Knowledge-base k of word embeddings
Number of topics to extract, K

Build word embeddings:

1: Extract vocabulary of nouns and noun phrases V from corpus
2: Learn distributed word representations on corpus, ℓv for v ∈ V
3: Compute word vectors wv = [kv; ℓv] for v ∈ V

Identify topics:

4: Perform K-Means clustering to obtain T opici for i = 1, . . . , K

Score topics:

5: Perform agglomerative clustering on {wv : v ∈ V }
6: Compute depth(v) for v ∈ V using (2)
7: Compute the degree of each word, degree(v) for v ∈ V , using the sentence-level co-occurrence

graph as in (3).

8: For v ∈ V , compute:

Score(v) = (cid:18)

depth(v)

maxu∈V depth(u)(cid:19)α(cid:18)

log(1 + degree(v))

log(1 + maxu∈V degree(u))(cid:19)β

,

where α, β are normalization parameters; all our experiments use α = β = 1.

9: Compute average score of each topic as the average score of constituent words using (5)

Output:

Score(v) for v ∈ V : the score for each word
T opic[i] for i = 1, . . . , K: the ranked list of topics

9

Table 4: Mr. Kaminski’s email: Comparing results of local word vectors with knowledge-base word
vectors

Knowledge-base word vectors

Topic 3
analysis
process

rate

method

calculation
modeling
function
estimation

server
user

desktop

email

interface

unix
data

browser methodology
freebsd

curve

Topic 4
morgan
harvey
campbell
richardson

henry
collins
morris
freeman

ﬁsher
massey

equity
pricing
valuation
company

asset

payment

transaction

investor

Topic 1

Topic 2
investment microsoft
volatility

Topic 1

state

government

dpc

dabhol
demand

commission

mseb

maharashtra

authority

supply

Local word vectors
Topic 3
Topic 2
model
microsoft
package

volatility
market
option

user

vulnerability

approach
pricing

server

machine

spot
var
curve

valuation

code
source
world
ﬂaw

Topic 4

bank

banking
oﬃcer
equity

investment

division

bond

morgan
brokerage

march

w = [k(v); ℓ(v)] for v ∈ V , with w(v) ∈ Rd, and d := κ + λ. For our experiments, we ﬁrst use the
skip-gram technique on the Wikipedia corpus that contains about three million unique words, and
about six billion total tokens. We set κ = 300 and λ = 25 to obtain 325-dimensional word vectors
after combining the knowledge-base and local word vectors.

It is educational to consider the cases in which the augmentation is not performed and the
word vectors are derived either solely based on the given corpus, or solely based on the knowledge-
base. Table 4 presents the top four topics obtained for both of these cases on Mr. Kaminski’s
email corpus. Comparing the two cases, we see that when using the local word vectors alone, word
semantics are not captured that well, and thus the topics obtained are more diﬀuse. When using
knowledge-base word vectors, the semantics dominate the topic deﬁnition with similar meaning
words pulled into topics from all over the corpus, without consideration for the context. When
one augments both these word vectors, we obtain a good balance that yields the sharper results of
Table 2.

4 Discussion

In this section, we discuss various aspects of Vec2Topic. First, we discuss its speed in Section 4.1,
then, in Section 4.2, we discuss its robustness to the number of topics to extract. In Section 4.3
we demonstrate its performance when implemented on: a non-user generated dataset (NIPS 2015
papers) and a small dataset (a single-document, Apple’s 10-K ﬁnancial report) . Finally, in Sec-
tion 4.4, we discuss some additional considerations in implementing the algorithm.

4.1 Algorithm complexity

To understand the complexity of the algorithm, we consider each of its key components. We use ¯V
to denote the entire vocabulary of the corpus (recall that |V | is the vocabulary of nouns and noun
phrases).

1. Hierarchical (agglomerative) clustering: we use the fastcluster method, [16], which has

a complexity of Θ(|V |2).

10

Table 5: Details of all corpora studied

Corpus

Documents

Apple 10-K

Lay

Kaminski
NIPS 2015

1
540
8,644
403

Tokens
58,795
104,194
1,171,632
1,943,649

Runtime2

|V |
∼4 sec
632
∼7 sec
1,301
5,739 ∼70 sec
4,754 ∼ 100 sec

2. Building word vectors: we use the skip-gram model [12], which has a running complexity
of E × T × Q, where E is the number of iterations, which is typically 5 − 50, T is the total
number of words or tokens in the corpus, and Q = c × (x + x log2(| ¯V |)), where c is the context
size (in all our experiments we set c = 5) and x ∈ {κ, λ} denotes the dimensionality of the
word vectors used in training. The case x = κ represents the training over the knowledge-
base, which needs to only be done once or a limited number of times, and x = λ represents
the training over the local corpus.

3. K-means clustering: this has a complexity of O(|V | × K × d × i), where i is the number of

iterations.

4. Computing Score: The score has two components: depth and degree. Computing depth is
linear in the size of the vocabulary of interest |V | once the hierarchical clustering has been
done. Computing degree involves building a co-occurrence matrix for the entire vocabulary
¯V over each sentence, and this has a worst case complexity of O(| ¯V |2 × S), where S is the
number of sentences in the corpus.

In all our experiments, the algorithm does not take more than a few minutes to run. Table 5
provides details on the sizes of all the datasets considered in this paper and the runtime of the
algorithm. The slowest component of Vec2Topic is building word vectors, which took majority of
the run time. However, this method is quite scalable and has been used to build word vectors on a
Google news dataset of 100 billion tokens in a couple of days. Indeed, we trained our knowledge-
base vectors on Wikipedia corpus, which contains about 6 billion tokens and about 2.8 million
unique words in about 6 hours.

The K-means clustering has an eﬃcient implementation as well, and scales with sizes well.
Further, computing the degree measure can also be done very eﬃciently as it involves hashing
as well as sparse matrix operations. So, as the scale of the corpus grows, we believe the critical
component of the algorithm from a run time perspective is hierarchical clustering. For |V | = 5, 000
with d = 325, this component completes in about 2.5 seconds. This scales up quadratically so that
|V | = 10, 000 takes 10 seconds, and |V | = 50, 000 takes slightly more than four hours. Given that
V only comprises nouns and noun phrases, we believe that its size will be an order smaller than
the total unique words in the document, and thus overall, the algorithm has the ability to scale
well with data size.

2Based on a 4-core 4GHz Intel processor with 32GB RAM. Python 2.7 was used on a Ubuntu machine with the
libraries Numpy, Scipy, Scikit-learn (for K-means clustering), Fastcluster (for agglomerative clustering), and Gensim
(for extracting bigrams and running Skip-gram); many components were not parallelized. The runtime is the total
time after text corpus is presented as input and includes time for any pre-processing. Our pre-processing included
running a lemmatizer on the corpus, and in the case of an email corpus, removing any names that were included in
to, from, cc ﬁelds. Note: runtime excludes the time taken to load the knowledge-base vectors into memory, which
was about 40 seconds.

11

Table 6: Mr. Kaminski’s email: Top-4 topics for diﬀerent K values

K = 5

Topic 2

investment

Topic 3
server

market
equity
demand

asset
growth
prices

insurance
banking

application
microsoft
technology

desktop
internet
enterprise
browser

Topic 4
thing

something
everyone
anything
someone
anyone

fact
lot

Topic 1
analysis
model

approach
probability
modeling
calculation

method

estimation
volatility
pricing

K = 50

Topic 2

investment

Topic 3
market

equity
asset

insurance
banking

bank
capital
markets
company
investor

technology
development

customer
industry
internet
enterprise
ecommerce
companies

solution

Topic 4
process
testing

implementation

assessment
evaluation

concept

requirement
framework
reliability
monitoring

user

everything

bank

ecommerce

nothing

Topic 1
analysis
model

approach
probability
modeling
calculation

method

estimation
volatility
pricing

4.2 Eﬀect of changing K

Similar to other topic modeling methods, our algorithm takes as input the number of topics to be
extracted, K. For the results described so far, we ﬁxed K = 10. Table 6 displays the top-four
topics for Mr. Kaminski when the number of topics K is set to 5 and 50. Notice that in both cases,
the overall word score remains unaﬀected by the change in K, the only change is the clustering
of topics. Observe that the top topic is identical for these two cases, and for the case K = 10
of Table 2. Even the second-ranked topic is quite similar. This illustrates the robustness of our
approach in capturing the key topics from a corpus.

Notice that the case K = 5 is quite crude in the sense that all words are clubbed into 5 clusters
and hence we observe a diﬀuse topic such as Topic 4. On the other hand K = 50 is much more
pointed and picks up a large number of small clusters, which leads to identifying many specialized
topics. In our extensive experiments, we found that moderate K values, such as K = 10 or 20,
work well in identifying key topics.

4.3 Other datasets

NIPS 2015 Dataset We next consider a dataset consisting of the full-text of all papers accepted
at the NIPS 2015 conference.3 Table 7 lists the top-3 topics obtain from applying Vec2Topic
compared with three relevant LDA-based topics (we ran LDA with K = 10 to be consistent with
our approach). We note that in this case LDA does a very good job of capturing topics. In fact,
the table provides a good insight into how Vec2Topic diﬀers from LDA. Our algorithm focuses
on concepts across the documents in the dataset, whereas LDA appears to focus on concepts
within each document in the dataset. Consider Topic 1 and Topic A, which contain two common
words (“graph” and “matrix”). Topic 1 from Vec2Topic places these words along with others
that tend to be used in similar themes, such as graph theory. Although Topic A also appears to
relate to the topic of graph theory, it is more focused on the context of each paper, and contains
broader words such as “data” and “problem.” Again Topic 2 and Topic B appear to be about
inference, but Topic 2 contains terms that relate to inference such as “algorithm”, “MCMC”,
whereas Topic B seems to be more about the context in which inference is done in the papers with

3We used Andrej Karpathy’s scripts available at https://github.com/karpathy/nipspreview.git to scrape this
information. As part of pre-processing we removed the References section of each paper; we discuss the value of doing
so in Section 4.4.

12

Table 7: NIPS 2015 Dataset: Top-3 scoring topics in Vec2Topic compared with three LDA-based
topics

Topic 1
graph

approximation

gaussian
subspace
nonzero

eigenvalue
equation
manifold
covariance

matrix

Vec2Topic
Topic 2
algorithm
bayesian
inference
regression

optimization
computation

classiﬁer

mcmc

bayesian inference

speedup

Topic 3
problem

idea

complexity

notion

eﬃciency

interpretation

proof

knowledge

concept

sense

Topic A
algorithm

sparse

set

graph
matrix
data

random
problem
theorem
number

LDA

Topic B

set

algorithm
submodular

problem
function

time

number
inference
greedy
node

Topic C
algorithm

regret
bound

loss

learning
online
bounds
problem
function
convex

words: “submodular”, “function”, “greedy”, etc. Finally, Topic 3 is about general concepts such
as “problem”, “complexity”, “eﬃciency”, etc., that is once again a common topic across all the
papers, whereas Topic C appears to be about learning algorithms.

Apple’s 2015 10-K Financial Report We next apply Vec2Topic to a single document. We
consider Apple’s ﬁnancial report, the 10-K document for 2015. We apply Vec2Topic with K = 10.
Table 8 displays the top-4 topics from the report along with the top-10 highest scoring words.
We notice that Topic 1 corresponds to the ﬁnancial accounting terminology that one expects to
be central to the ﬁnancial report (asset, tax, income, etc.). Topic 2 is about the nature of the
company (produce, software, hardware, etc.); Topic 4 provides speciﬁcs on these (iOS, iPhone,
iPad, etc.). Topic 3 pertains to ﬁnancial considerations of the company (cost, risk, volatility, etc.).
Notice that the list of top words goes across the diﬀerent topics and contains both ﬁnancial and
product-related words (asset, product, software, income, etc.). We would like to point out that
LDA does not directly apply to a single document so we do not provide the comparison.

Table 8: Apple 2015 10-K: Top-10 scoring overall words and Top-4 scoring topics in Vec2Topic

Top words

Topic 1

asset

product

tax

asset
tax

income

software

investment

cost

income

risk

hardware
content

equity
liability
dividend
pricing

transaction

Topic 2
product
software
hardware
content

application
customer

intellectual property

service
solution

application

payment

operating system

13

Topic 3

cost
risk

Topic 4

ios

iphone

ipad
mac

expense
exposure
ﬂuctuation iphone ipad
ipod touch

eﬀect
change
increase
volatility
amount

device
ipod
os x
user

4.4 Other implementation considerations

Normalization parameters, α and β The word score computed in (4) uses the normalization
parameters α and β, which were ﬁxed as α = β = 1 in the experiments. We brieﬂy discuss how these
may be modiﬁed to improve the performance of the algorithm in some cases. The score combines
two diﬀerent types of measures: depth and degree, which are distributed quite diﬀerently over the
vocabulary. We need to ensure that the role both these measures play in identifying the core topics
is “balanced.” In most documents, setting α = β = 1 works because degree is normalized by the
logarithmic transformation. However, we ﬁnd that a more robust score can be obtained by choosing
α and β so that after this power transformation the depth and degree measures both have median
values equal to 1/2. Formally, we choose α so that we have

Medianv∈V (cid:18)

depth(v)

maxu∈V depth(u)(cid:19)α

=

1
2

.

(6)

The value for β can be computed in an analogous fashion. For datasets in this paper, the values
of α and β so computed are quite close to one; incorporating this into the algorithm leaves all the
results in the paper qualitatively unchanged. However, in our extensive experiments we did ﬁnd
some datasets for which using this normalization method improved performance.

Performance In this paper, we have demonstrated that Vec2Topic works well both for user-
generated and non-user generated datasets and even for single documents. The ability of the
algorithm to capture core topics depends on how well-deﬁned these topics are. For documents with
a clear core topic, the algorithm can extract it even when the size of the document is quite small.
In our extensive experiments, we found it to perform well even for documents with a couple of
hundred tokens in total, such as a news article. However, in that case, we did not build local word
embeddings and instead used the knowledge-base word vectors alone. The reason this works is
because a document such as a news article is written around a central theme or topic, and hence
the context of usage is not that consequential for identifying the topic.

We would like to point out that though Vec2Topic does not explicitly use a probabilistic ap-
proach, its output is not deterministic because it depends on building local word embeddings. In
our experiments, we used the skip-gram approach, which results in slightly diﬀerent word embed-
dings each time it is run. This eﬀect is more pronounced with smaller datasets, so while the degree
score remains constant across runs, the depth score may vary slightly. Nevertheless, the overall
results and the top core topics are qualitatively quite robust to this eﬀect.

The performance of the algorithm is also limited by the quality of the knowledge-base vectors.
In our experiments, we used the English Wikipedia for our knowledge-base. One of the limitations
therein was with respect to foreign words. This knowledge-base contains many foreign words (for
instance, movie names) without suﬃcient context. This implies that the learned word representa-
tions of these words are not semantically accurate. To alleviate this issue, we removed such words
(this issue came up with Mr. Kaminski’s emails). Another related issue is that of people names.
The word vectors for these tend to be quite close to each other, and if there are a large number of
names, then this aﬀects the depth measure. Clearly, people names would not make for key topics.
So, to resolve this issue, in the email corpora, we remove names of people listed in the address
ﬁelds. In the NIPS dataset, we remove the References section of the paper. Another way to resolve
this issue is to run the dataset through a Named Entity Recognition algorithm.

14

5 Conclusions

In this paper we propose a novel technique for topic modeling that leverages understanding of word
semantics using high-dimensional word vectors. We showed that Vec2Topic works well to extract
a user’s key topics of interest across their own generated content — it also ranks these topics, and
identiﬁes keywords that best describe it. We contrasted it with the state-of-the-art topic modeling
algorithm LDA, and observed that it works much better when the topic keywords are spread across
the various documents, and are surrounded by several contextual words that are generic in nature.
Further we observe that the technique is not limited to user generated content; it works equally
well on more structured documents such as scientiﬁc papers, news articles, blogs, web pages, etc.
It is also fairly robust to the corpus size — it can scale from a single document to a large collection.
One of our ongoing eﬀorts is focused on extending the algorithm to identify phrases — sequences
of keywords that together capture the user’s key interests. For example, turning to the example
of a professional who works on datacenter servers, the phrase “cloud server virtualization” conveys
a lot more topical context than cloud, server and virtualization individually. Each of these words
may show up in several contexts in the user’s data — we therefore need a way to rank such phrases.
Also, these phrases diﬀer from bigram/trigram phrases in the sense that their words may not show
up consecutively in the user’s data. We believe extending topic modeling in this direction is an
interesting avenue for future study.

6 Acknowledgments

The authors would like to thank Achal Bassamboo for many useful discussions.

References

[1] Y. Bengio, R. Ducharme, P. Vincent, and C. Janvin. A neural probabilistic language model.

J. Mach. Learn. Res., 3:1137–1155, Mar. 2003.

[2] D. M. Blei. Probabilistic topic models. Commun. ACM, 55(4):77–84, Apr. 2012.

[3] D. M. Blei, T. L. Griﬃths, and M. I. Jordan. The nested Chinese restaurant process and
bayesian nonparametric inference of topic hierarchies. Journal of the ACM (JACM), 57(2):7,
2010.

[4] D. M. Blei and J. D. Laﬀerty. Dynamic topic models. In Proceedings of the 23rd international

conference on Machine learning, pages 113–120. ACM, 2006.

[5] D. M. Blei and J. D. Laﬀerty. A correlated topic model of science. The Annals of Applied

Statistics, pages 17–35, 2007.

[6] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent dirichlet allocation. J. Mach. Learn. Res.,

3:993–1022, Mar. 2003.

[7] J. Chang and D. M. Blei. Hierarchical relational models for document networks. The Annals

of Applied Statistics, pages 124–150, 2010.

15

[8] R. Collobert and J. Weston. A uniﬁed architecture for natural language processing: Deep
neural networks with multitask learning. In Proceedings of the 25th International Conference
on Machine Learning, ICML ’08, pages 160–167, New York, NY, USA, 2008. ACM.

[9] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P. Kuksa. Natural

language processing (almost) from scratch. J. Mach. Learn. Res., 12:2493–2537, Nov. 2011.

[10] E. H. Huang, R. Socher, C. D. Manning, and A. Y. Ng. Improving word representations via
global context and multiple word prototypes. In Proceedings of the 50th Annual Meeting of the
Association for Computational Linguistics: Long Papers - Volume 1, ACL ’12, pages 873–882,
Stroudsburg, PA, USA, 2012. Association for Computational Linguistics.

[11] A. Kumar, O. Irsoy, J. Su, J. Bradbury, R. English, B. Pierce, P. Ondruska, I. Gulrajani, and
R. Socher. Ask me anything: Dynamic memory networks for natural language processing.
arXiv preprint arXiv:1506.07285, 2015.

[12] T. Mikolov, K. Chen, G. Corrado, and J. Dean. Eﬃcient estimation of word representations

in vector space. CoRR, abs/1301.3781, 2013.

[13] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed representations of
words and phrases and their compositionality. In C. Burges, L. Bottou, M. Welling, Z. Ghahra-
mani, and K. Weinberger, editors, Advances in Neural Information Processing Systems 26,
pages 3111–3119. Curran Associates, Inc., 2013.

[14] T. Mikolov, W.-T. Yih, and G. Zweig. Linguistic regularities in continuous space word repre-

sentations. In HLT-NAACL, pages 746–751, 2013.

[15] A. Mnih and G. E. Hinton. A scalable hierarchical distributed language model. In Advances

in neural information processing systems, pages 1081–1088, 2009.

[16] D. M¨ullner. fastcluster: Fast hierarchical, agglomerative clustering routines for r and python.

Journal of Statistical Software, 53(9):1–18, 2013.

[17] J. Pennington, R. Socher, and C. D. Manning. Glove: Global vectors for word representation.

In EMNLP, volume 14, pages 1532–1543, 2014.

[18] R. Socher, E. H. Huang, J. Pennin, C. D. Manning, and A. Y. Ng. Dynamic pooling and
unfolding recursive auto-encoders for paraphrase detection. In Advances in Neural Information
Processing Systems, pages 801–809, 2011.

[19] R. Socher, C. C. Lin, C. Manning, and A. Y. Ng. Parsing natural scenes and natural language
with recursive neural networks. In Proceedings of the 28th international conference on machine
learning (ICML-11), pages 129–136, 2011.

[20] R. Socher, J. Pennington, E. H. Huang, A. Y. Ng, and C. D. Manning. Semi-supervised
recursive auto-encoders for predicting sentiment distributions. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language Processing, pages 151–161. Association for
Computational Linguistics, 2011.

16

[21] R. Socher, A. Perelygin, J. Y. Wu, J. Chuang, C. D. Manning, A. Y. Ng, and C. Potts.
Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings
of the conference on empirical methods in natural language processing (EMNLP), volume 1631,
page 1642. Citeseer, 2013.

[22] Y. W. Teh, M. I. Jordan, M. J. Beal, D. M. Blei, et al. Hierarchical dirichlet processes. Journal

of the American Statistical Association, 101:1566–1581, 2006.

[23] J. Turian, L. Ratinov, and Y. Bengio. Word representations: a simple and general method
for semi-supervised learning. In Proceedings of the 48th annual meeting of the association for
computational linguistics, pages 384–394. Association for Computational Linguistics, 2010.

[24] L. Van der Maaten and G. Hinton. Visualizing data using t-SNE. Journal of Machine Learning

Research, 9(2579-2605):85, 2008.

[25] H. M. Wallach. Topic modeling: beyond bag-of-words. In Proceedings of the 23rd international

conference on Machine learning, pages 977–984. ACM, 2006.

[26] J. Weston, A. Bordes, S. Chopra, and T. Mikolov. Towards ai-complete question answering:

A set of prerequisite toy tasks. arXiv preprint arXiv:1502.05698, 2015.

17

