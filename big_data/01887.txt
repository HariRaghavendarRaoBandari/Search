Concentrated Diﬀerential Privacy

Cynthia Dwork

Guy N. Rothblum

6
1
0
2

 
r
a

 

M
6
1

 
 
]
S
D
.
s
c
[
 
 

2
v
7
8
8
1
0

.

3
0
6
1
:
v
i
X
r
a

We introduce Concentrated Diﬀerential Privacy, a relaxation of Diﬀerential Privacy enjoying
better accuracy than both pure diﬀerential privacy and its popular “(ε, δ)” relaxation without
compromising on cumulative privacy loss over multiple computations.

Abstract

1

Introduction

The Fundamental Law of Information Recovery states, informally, that “overly accurate” estimates
of “too many” statistics completely destroys privacy ([DN03] et sequelae). Diﬀerential privacy is
a mathematically rigorous deﬁnition of privacy tailored to analysis of large datasets and equipped
with a formal measure of privacy loss [DMNS06, Dwo06]. Moreover, diﬀerentially private algorithms
take as input a parameter, typically called ε, that caps the permitted privacy loss in any execution
of the algorithm and oﬀers a concrete privacy/utility tradeoﬀ. One of the strengths of diﬀerential
privacy is the ability to reason about cumulative privacy loss over multiple analyses, given the values
of ε used in each individual analysis. By appropriate choice of ε it is possible to stay within the
bounds of the Fundamental Law while releasing any given number of estimated statistics; however,
before this work the bounds were not tight.

Roughly speaking, diﬀerential privacy ensures that the outcome of any anlysis on a database
x is distributed very similarly to the outcome on any neighboring database y that diﬀers from x
in just one row (Deﬁnition 2.3). That is, diﬀerentially private algorithms are randomized, and in
particular the max divergence between these two distributions (a sort maximum log odds ratio for
any event; see Deﬁnition 2.2 below) is bounded by the privacy parameter ε. This absolute guarantee
on the maximum privacy loss is now sometimes referred to as “pure” diﬀerential privacy. A popular
relaxation, (ε, δ)-diﬀerential privacy (Deﬁnition 2.4)[DKM+06], guarantees that with probability at
most 1− δ the privacy loss does not exceed ε.1 Typically δ is taken to be “cryptographically” small,
that is, smaller than the inverse of any polynomial in the size of the dataset, and pure diﬀerential
privacy is simply the special case in which δ = 0. The relaxation frequently permits asymptotically
better accuracy than pure diﬀerential privacy for the same value of ε, even when δ is very small.

What happens in the case of multiple analyses? While the composition of k (ε, 0)-diﬀerentially

privacy algorithms is at worst (kε, 0)-diﬀerentially private, it is also simultaneously (p2k ln(1/δ)ε+
kε(eε − 1), δ)-diﬀerentially private for every δ [DRV10]. Let us deconstruct the statement of this
result. First, privacy loss is a random variable that captures diﬀerences in the probability distri-
butions obtained when a randomized algorithm M is run on x as opposed to neighboring database
y (see Section 2.1). In general, if the max divergence between two distributions is bounded by ε

1Note that the probability is over the coins of the algorithm performing the analysis. The above formulation is

not immediate from the deﬁnition of (ε, δ)-diﬀerential privacy, this was proved in the full version of [DRV10].

1

then their KL-divergence (Deﬁnition 2.1) is bounded by ε(eε − 1). This means that the expected
privacy loss for a single (ε, 0)-diﬀerentially private computation is bounded by ε(eε − 1). By lin-
earity of expectation, the expected loss over k (ε, 0)-diﬀerentially private algorithms is bounded by
kε(eε − 1). The statement therefore says that the cumulative privacy loss random variable over k
computations is tightly concentrated about its mean: the probability of privacy loss exceeding its
expectation by t√kε falls exponentially in t2/2 for all t ≥ 0. We will return to this formulation

presently.

More generally, we prove the following Advanced Composition theorem, which improves on the
composition theorem of Dwork, Rothblum, and Vadhan [DRV10] by exactly halving the bound on
expected privacy loss of (ε, 0)-diﬀerentially privacy mechanisms (the proof is otherwise identical).
Details for the sharper bound appear in Section 3.2.
Theorem 1.1. For all ε, δ, δ′ ≥ 0, the class of (ε, δ′)-diﬀerentially private mechanisms satisﬁes
(p2k ln(1/δ)ε + kε(eε − 1)/2, kδ′ + δ)-diﬀerential privacy under k-fold adaptive composition.

As the theorem shows (recall that δ′ is usually taken to be “sub-polynomially small”), the
pure and relaxed forms of diﬀerential privacy behave quite similarly under composition. For the
all-important class of counting queries (“How many people in the dataset satisfy property P ?”)
Theorem 1.1 leads to accuracy bounds that diﬀer from the bounds imposed by (one instantiation

of) the Fundamental Law by a factor of roughly p2 log(1/δ).2 Recently, tight bounds on the

composition of (ε, δ)-diﬀerentially private algorithms have been given in [KOV15, MV16](see below).

In this work we introduce a diﬀerent relaxation, Concentrated Diﬀerential
A New Relaxation.
Privacy (CDP), incomparable to (ε, δ)-diﬀerential privacy but again having the same behavior
under composition. Concentrated diﬀerential privacy is closer to the “every δ” property in the
statement of Theorem 1.1: An algorithm oﬀers (µ, τ )-concentrated diﬀerential privacy if the privacy
loss random variable has mean µ and if, after subtracting oﬀ this mean, the resulting (centered)
random variable, ξ, is subgaussian with standard τ .3 In consequence (see, e.g., [BK00] Lemma 1.3),
∀x > 0:

Pr[ξ ≥ x] ≤ exp(cid:18)−

x2

2τ 2(cid:19) and Pr[ξ ≤ −x] ≤ exp(cid:18)−

x2

2τ 2(cid:19)

Thus, concentrated diﬀerential privacy ensures that the expected privacy loss is µ and the proba-
bility of loss exceeding its mean by x = tτ is bounded by e−t2/2, echoing the form of the guarantee
oﬀered by the Advanced Composition Theorem (Theorem 1.1).

Consider the case in which τ = ε. On the one hand, (µ, ε)-concentrated diﬀerential privacy is
clearly weaker than (ε, δ)-diﬀerential privacy, because even if the expected loss µ is very small, the
probability of privacy loss exceeding ε in the former can be constant in the former but is only δ,
which is tiny, in the latter. On the other hand, in (ε, δ)-diﬀerential privacy there is no bound on
the expected privacy loss, since with probability δ all bets are oﬀ and the loss can be inﬁnite.

Concentrated diﬀerential privacy enjoys two advantages over (ε, δ)-diﬀerential privacy.
2For example, for O(n) counting queries errors on the order of o(√n) on all but a 0.239 fraction of queries leads to
blatant non-privacy [DMT07], while noise drawn from a Laplace distribution with standard deviation O(pn ln(1/δ)/ε)
yields (ε, δ)-diﬀerential privacy [DMNS06, DRV10].

3A random variable X is subgaussian with standard τ for a constant τ > 0 if ∀λ ∈ R : E[eλ·X] ≤ e

Section 2.2.

2

λ

2

·τ
2

. See

2

• Improved Accuracy. Concentrated diﬀerential privacy is tailored to the (realistic!) case of
large numbers of computations. Traditionally, to ensure small cumulative loss with high prob-
ability, the permitted loss for each individual query is set very low, say ε′ = εp(log 1/δ)/k,
even though a privacy loss of, say, ε/10 or even ε itself may not be of great concern for
any single query. This is precisely the ﬂexibility we give in concentrated diﬀerential pri-
vacy: much less concern about single-query loss, but high probability bounds for cumulative
loss. The composition of k (µ, τ )-concentrated diﬀerential privacy mechanisms is (kµ,√kτ )-
concentrated diﬀerential privacy (Theorem 3.4). Setting τ = ε we get an expected privacy loss
of kµ and, for all t simultaneously, the probability of privacy loss exceeding its epectation by
t√kε falls exponentially in t2/2, just as we obtained in the composition for the other variants
of diﬀerential privacy in Theorem 1.1 above. However, we get better accuracy. For example,
to handle n counting queries using the Gaussian mechanism, we can add independent random
noise drawn from N (0, n/ε2) to each query, achieving (ε(eε− 1)/2, ε)-concentrated diﬀerential
privacy (Theorem 3.2)4. When ε = Θ(1) the noise is scaled to O(√n); the Fundamental Law
says noise o(√n) is disastrous.

• Group Privacy Group privacy bounds the privacy loss even for pairs of databases that
diﬀer in the data of a small group of individuals; for example, in a health survey one may
wish not only to know that one’s own health information remains private but also that the
information of one’s family as a whole is strongly protected. Any (ε, 0)-diﬀerentially private
algorithm automatically ensures (sε, 0)-diﬀerential privacy for all groups of size s [DMNS06],
with the expected privacy loss growing by a factor of about s2. The situation for (ε, δ)-
diﬀerential privacy is not quite so elegant: the literature shows (sε, ses−1δ)-diﬀerential privacy
for groups of size s, a troubling exponential increase in the failure probability (the ses−1δ
term). The situation for concentrated diﬀerential privacy is much better: for all known natural
mechanisms with concentrated diﬀerential privacy we get tight bounds. For (hypothetical)
arbitrary algorithms oﬀering subgaussian privacy loss, Theorem 4.1 shows bounds that are
asymptotically nearly-tight (tight up to low-order terms). We suspect that the tight bounds
should hold for arbitrary mechanisms, and it would be interesting to close the gap.

2. Every ( τ 2

1. Under certain conditions, satisﬁed by all pure diﬀerential privacy mechanisms5, and
the addition of Gaussian noise, any (µ, τ )-concentrated diﬀerential privacy mechanism
satisﬁes (s2·µ, s·τ )-concentrated diﬀerential privacy for groups of size s, which is optimal.
2 · (1 + o(1)), s ·
τ · (1 + o(1))-concentrated diﬀerential privacy for groups of size s. The bound holds
so long as s · τ is small enough (roughly, (1/τ ) should remain quasi-linear in s). See
Theorem 4.1 for the formal statement. We also assume here that µ ≤ τ 2
2 .6

2 , τ )-concentrated diﬀerential privacy mechanism satisﬁes (s2 · τ 2

Remark 1.2. Consider any mechanism built by composing a collection of “good” mechanisms, each
satisfying the conditions in Item 1 above. To prove that the composed mechanism has the good group
privacy bounds we can ﬁrst apply the group privacy bounds for the underlying “good” mechanisms,
and then apply composition. It is interesting that these two operations commute.

by a factor of pln(1/δ).

4To achieve (ε, δ)-diﬀerential privacy one adds noise drawn from N (0, 2 ln(1/δ)), increasing the typical distortion
5Every (ε, 0)-diﬀerentially private mechanism yields (ε(eε−1)/2, ε)-concentrated diﬀerential privacy (Theorem 3.5).
6Up to low-order terms, this relationship holds for all mechanisms we know. For other mechanisms, it is possible

to derive a less elegant general bound, or τ can be “artiﬁcially inﬂated” until this condition holds.

3

Tight Bounds on Expected Loss. As noted above, we improve by a factor of two the known
upper bound on expected privacy loss of any (ε, 0)-diﬀerentially private mechanism, closing a gap
open since 2010 [DRV10]. This immediately translates to an improvement by a factor of √2
on the utility/privacy tradeoﬀ in any application of the Advanced Composition Theorem. The
new bound, which is tight, is obtained by ﬁrst proving the result for special pairs of probablity
distributions that we call antipodal (privacy loss for any outcome is in {−ε, 0, ε}), and then showing
a reduction, in which an arbitrary pair of distributions with max divergence bounded by ε – such as
the distributions on outcomes of a diﬀerentially private mechanism when run on databases x and y
diﬀering in a single row – can be “replaced” by a antipodal pair with no change in max divergence
and no decrease in KL-divergence.

Remark 1.3. If all (ε, 0)-diﬀerentially private algorithms enjoy concentrated diﬀerential privacy,
as well as the Gaussian mechanism, which (ε, δ)-diﬀerentially private algorithms are ruled out? All
(ε, δ)-diﬀerentially private algorithms in which there is some probability δ′ ≤ δ of inﬁnite privacy
loss. This includes many (but not all!) algorithms in the “Propose-Test-Release” framework [DL09],
in which a diﬀerentially private test is ﬁrst performed to check that the dataset satisﬁes some
“safety” conditions and, if so, an operation is carried out that only ensures privacy if the conditions
are met. There could be a small probability of failure in the ﬁrst step, meaning that the test
reports that the safety conditions are met, but in fact they are not, in which case privacy could be
compromised in the second step.

1.1 Recent Developments

Tightness. Optimality in privacy loss is a suprisingly subtle and diﬃcult question under com-
position. Results of Kairouz, Oh and Viswanath [KOV15] obtain tight bounds under composition
of arbitrary (ǫ, δ)-mechanisms, when all mechanisms share the same values of ǫ and δ. That is,
they ﬁnd the optimal ǫ′, δ′ such that the composition of k mechanisms, each of which is (ǫ0, δ0)-
diﬀerentially private, is (ǫ′, δ′)-diﬀerentially private. The nonhomogeneous case, in which the ith
mechanism is (ǫi, δi)-diﬀerentially private, has been analyzed by Murtagh and Vadhan [MV16],
where it is shown that determining the bounds of the optimal composition is hard for #P. Both
these papers use an analysis similar to that found in our Lemma 3.8, which we obtained prior to the
publication of those works. Optimal bounds on the composition of arbitrary mechanisms are very
interesting, but we are also interested in bounds on the speciﬁc mechanisms that we have in hand
and wish to use and analyze. To this end, we obtain a complete characterization of the privacy loss
of the Gaussian mechanism. We show that the privacy loss of the composition of multiple applica-
tion of the Gaussian mechanism, possibly with diﬀerent individual parameters, is itself a Gaussian
random variable, and we give exact bounds for its mean and variance. This characterization is
not possible using the framework of (ǫ, δ)-diﬀerential privacy, the previous prevailing view of the
Gaussian mechanism.

Subsequent Work. Motivated by our work, Bun and Steinke [BS15] suggest a relaxation of
concentrated diﬀerential privacy.
Instead of framing the privacy loss as a subgaussian random
variable as we do here, they instead frame the question in terms of Renyi entropy, obtaining a
relaxation of concentrated diﬀerential privacy that also supports a similar composition theorem.
Their notion also provides privacy guarantees for groups. The bounds we get using concentrated

4

diﬀerential privacy (Theorem 4.1) are tighter; we do not know whether this is inherent in the
deﬁnitions.

2 Preliminaries

Divergence. We will need several diﬀerent notions of divergence between distributions. We will
also introduce a new notion, subgaussian divergence in Section 3.

Deﬁnition 2.1 (KL-Divergence). The KL-Divergence, or Relative entropy, between two random
variables Y and Z is deﬁned as:

DKL(Y ||Z) = Ey∼Y (cid:20)ln

Pr[Y = y]

Pr[Z = y](cid:21) ,

where if the support of Y is not equal to the support of Z, then DKL(Y ||Z) is not deﬁned.
Deﬁnition 2.2 (Max Divergence). The Max Divergence between two random variables Y and Z
is deﬁned to be:

D∞(Y ||Z) = max

S⊆Supp(Y )(cid:20)ln

Pr[Y ∈ S]
Pr[Z ∈ S](cid:21) ,

where if the support of Y is not equal to the support of Z, then D∞(Y ||Z) is not deﬁned.

The δ-approximate divergence between Y and Z is deﬁned to be:

Dδ
∞(Y ||Z) =

max

S⊆Supp(Y ):Pr[Y ∈S]≥δ(cid:20)ln

Pr[Z ∈ S](cid:21) ,
Pr[Y ∈ S]
∞(Y ||Z) is not deﬁned.

where if Pr[Y ∈ Supp(Y ) \ Supp(Z)] > δ, then Dδ
2.1 Diﬀerential Privacy

For a given database d, a (randomized) non-interactive database access mechanism M computes
an output M(x) that can later be used to reconstruct information about d. We will be concerned
with mechanisms M that are private according to various privacy notions described below.
We think of a database x as a multiset of rows, each from a data universe U . Intuitively, each
row contains the data of a single individual. We will often view a database of size n as a tuple
x ∈ U n for some n ∈ N (the number of individuals whose data is in the database). We treat n as
public information throughout.
We say databases x, y are adjacent if they diﬀer only in one row, meaning that we can obtain
one from the other by deleting one row and adding another. I.e. databases are adjacent if they
are of the same size and their edit distance is 1. To handle worst case pairs of databases, our
probabilities will be over the random choices made by the privacy mechanism.
Deﬁnition 2.3 ((ε, 0)-Diﬀerential Privacy ((ε, 0)-DP) [DMNS06]). A randomized algorithm M is
ε-diﬀerentially private if for all pairs of adjacent databases x, y, and for all sets S ⊆ Range(M(x))∪
Range(M(y))

Pr[M(x) ∈ S] ≤ eε · P r[M(y) ∈ S],

where the probabilities are over algorithm M’s coins. Or alternatively:
D∞(M(x)||M(y)), D∞(M(y)||M(x)) ≤ ε

5

Deﬁnition 2.4 ((ε, δ)-Diﬀerential Privacy ((ε, δ)-DP) [DKM+06]). A randomized algorithm M
gives (ε, δ)-diﬀerential privacy if for all pairs of adjacent databases x and y and all S ⊆ Range(M)

Pr[M(x) ∈ S] ≤ eε · Pr[M(y) ∈ S] + δ,

where the probabilities are over the coin ﬂips of the algorithm M. Or alternatively:

Dδ
∞(M(x)||M(y)), Dδ

∞(M(y)||M(x)) ≤ ε

Privacy Loss as a Random Variable. Consider running an algorithm M on a pair of databases
x, y. For an outcome o, the privacy loss on o is the log-ratio of its probability when M is run on
each database:

L(o)
(M(x)||M(y)) = ln

Pr[M(x) = o]
Pr[M(y) = o]

.

Concentrated diﬀerential privacy delves more deeply into the privacy loss random variable:
this real-valued random variable measures the privacy loss ensuing when algorithm M is run on
(M(x)||M(y)). This
x (as opposed to y).
random variable can take positive or negative values. For (ε, 0)-diﬀerentially private algorithms,
its magnitude is always bounded by ε. For (ε, δ)-diﬀerentially private algorithms, with all but δ
probability the magnitude is bounded by ε.

It is sampled by taking y ∼ M(x) and outputting L(o)

2.2 Subgassian Random Variables

Subgaussian random variables were introduced by Kahane [Kah60]. A subgaussian random variable
is one for which there is a positive real number τ > 0 s.t.
the moment generating function is
always smaller than the moment generating function of a Gaussian with standard deviation τ and
expectation 0. In this section we brieﬂy review the deﬁnition and basic lemmata from the literature.

Deﬁnition 2.5 (Subgaussian Random Variable [Kah60]). A random variable X is τ -subgaussian
for a constant τ > 0 if:

∀λ ∈ R : E[eλ·X ] ≤ e

λ2 ·τ 2

2

We say that X is subgaussian if there exists τ ≥ 0 s.t. X is τ -subgaussian. For a subgaussian
random variable X, the subgaussian standard of X is:

τ (X) = inf {τ ≥ 0 : X is τ -subgaussian}

Remarks. An immediate consequence of Deﬁnition 2.5 is that an τ -subgaussian random variable
has expectation 0, and variance bounded by τ 2 (see Fact 2.1). Note also that the gaussian distribu-
tion with expectation 0 and standard deviation σ is σ-subgaussian. There are also known bounds
on the higher moments of subgaussian random variables (see Fact 2.2). See [BK00] and [Riv12] for
further discussion.

Lemma 2.1 (Subgaussian Concentration). If X is τ -subgaussian for τ > 0, then:

Pr[X ≥ t · τ ] ≤ e−t2/2,

Pr[X ≤ −t · τ ] ≤ e−t2/2

6

Proof. For every λ > 0 and t > 0:

Pr[X ≥ t · τ ] = Pr[eλ·X ≥ eλ·t·τ ]
≤ e−λ·t·τ · E[eλ·X]
≤ e

2 −λ·t·τ

λ2 ·τ 2

where the ﬁrst equality is obtained by raking an exponential of all arguments, the second (in)equality
is by Markov, and the this is by the properties of the subgaussian random variable X. The right-
hand side is minimized when λ = t/τ , and thus we get that:

Pr[X ≥ t · τ ] ≤ e−t2/2

The proof that Pr[X ≤ −t · τ ] ≤ e−t2/2 is similar.
Fact 2.1 (Subgaussian Variance). The variance of any τ -subgaussian random variable Y is bounded
by Var (Y ) ≤ τ 2.
Fact 2.2 (Subgaussian Moments). For any τ -subgaussian random variable Y , and integer k, the
k-th moment is bounded by:

E[Y k] ≤(cid:16)(⌈k/2⌉!) · 2⌈k/2⌉+1 · τ k(cid:17)

Lemma 2.2 (Sum of Subgaussians). Let X1, . . . , Xk be (jointly distributed) real-valued random
variables such that for every i ∈ [k], and for every (x1, . . . , xi−1) ∈ Supp(X1, . . . , Xk−1), it holds
that the random variable (Xi|X1 = x1, . . . , Xi−1 = xi−1) is τi-subgaussian. Then the random
variable Pi∈[k] Xi is τ -subgaussian, where τ =qPi∈[k] τ 2
Proof. The proof is by induction over k. The base case k = 1 is immediate. For k > 1, for any
λ ∈ R, we have:
E[eλ·Pi∈[k] Xi] = E(x1,...,xk−1)∼(X1,...,Xk−1))hEXk [eλ·Pi∈[k] Xi|X1 = x1, . . . , Xk−1 = xk−1]i

i .

= E(x1,...,xk−1)∼(X1,...,Xk−1))heλ·Pi∈[k−1] xi · EXk [eλ·Xk|X1 = x1, . . . , Xk−1 = xk−1]i
≤ E(x1,...,xk−1)∼(X1,...,Xk−1))(cid:20)eλ·Pi∈[k−1] xi · e

2 (cid:21)

λ2 ·τ 2
k

λ2 ·τ 2
k

2

λ2 ·τ 2
k

2

= e

≤ e
= e

· E[eλ·Pi∈[k−1] Xi]
· e

λ2 ·Pi∈[k−1] τ 2
i

2

λ2 ·Pi∈[k] τ 2
i

2

where the last inequality is by the induction hypothesis.

The following technical Lemma about the products of (jointly distributed) random variables,

one of which is exponential in a subgaussian, will be used extensively in proving group privacy:

7

Lemma 2.3 (Expected Product with Exponential in Subgaussian). Let X and Y be jointly dis-
tributed random variables, where Y is τ -Subgaussian for τ ≤ 1/3. Then:

E[X · eY ] ≤ E[X] +pE [X 2] · (τ + 3τ 2)

≤ E[X] + (pVar (X) + E[X]) · (τ + 3τ 2)

Proof. Taking the Taylor expansion of eY we have:

∞

Y k

E[X · eY ] = E"X · 1 + Y +

k! !#
= E[X] + E[X · Y ] + E"X ·  ∞
Xk=2

Xk=2

Y k

k! !#

By the Cauchy-Schwartz inequality:

E[X · Y ] ≤pE[X 2] ·pE[Y 2]

≤pE[X 2] · τ

(1)

(2)

(3)

where the last inequality uses the fact that for a τ -subgaussian RV Y , Var (Y ) ≤ τ 2 (Fact 2.1), and
that √a + b ≤ √a + √b (for any a, b ≥ 0). To bound the last summand in Inequality 2, we use
linearity of the expectation and Cauchy-Schwartz:

E"X ·  ∞
Xk=2

∞

Y k

k! !# =

Y k

∞

≤

k! (cid:21)(cid:19)

Xk=2(cid:18)E(cid:20)X ·
Xk=2pE [X 2] ·sE(cid:20) Y 2k
(k!)2(cid:21)
Xk=2sE(cid:20) Y 2k
(k!)2(cid:21)
=pE [X 2] ·

∞

Using the fact that for any τ -subgaussian distribution Y , the 2k-th moment E[Y 2k] is bounded by

8

Y k

∞

∞

∞

∞

(k!)2

(k!)2

E"X ·  ∞
Xk=2

(cid:0)(k!) · 2k+1 · τ 2k(cid:1) (see Fact 2.2), we conclude from the above that for τ < 1:
Xk=2s (k!) · 2k+1 · τ 2k
k! !# ≤pE [X 2] ·
Xk=2s (k!) · 2k+1 · τ 2k
=pE [X 2] ·
Xk=2s 2k+1 · τ 2k
=pE [X 2] ·
√4τ 2k
Xk=2
≤pE [X 2] ·
Xk=0
=pE [X 2] · 2τ 2 ·
2τ 2
=pE [X 2] ·
1 − τ

Putting together Inequalities (2), (3), (4), we conclude that for τ ≤ 1/2:
2τ 2

(k!)

∞

τ k

E[X · eY ] ≤ E[X] +(cid:16)pE [X 2] · τ(cid:17) +(cid:18)pE [X 2] ·

1 − τ(cid:19)

2τ 2

= E[X] +pE [X 2] ·(cid:18)τ +
≤ E[X] +pE [X 2] · (τ + 3τ 2)
≤ E[X] + (pVar (X) + E[X]) · (τ + 3τ 2)
pE[X 2] ≤pVar (X) + E[X] (because √a + b ≤ √a + √b for any a, b > 0).

1 − τ(cid:19)

(4)

where the next-to-last inequality holds whenever τ ≤ 1/3, and the last inequality is because

3 Concentrated Diﬀerential Privacy: Deﬁnition and Properties

Deﬁnition 3.1 (Privacy Loss Random Variable L(Y ||Z)). For two discrete random variables Y and
Z, the privacy loss random variable L(Y ||Z), whose range is R, is distributed by drawing y ∼ Y , and
outputting ln(Pr[Y = y]/Pr[Z = y]). In particular, the expectation of LY ||Z is equal to DKL(Y ||Z).
If the supports of Y and Z aren’t equal, then the privacy loss random variable is not deﬁned.

We study the privacy loss random variable, focusing on the case where this random variable is
tightly concentrated around its expectation. In particular, we will be interested in the case where
the privacy loss (shifted by its expectation) is subgaussian.

Deﬁnition 3.2 (Subgaussian Divergence and Indistinguishability). For two random variables Y
and Z, we say that DsubG (Y ||Z) (cid:22) (µ, τ ) if and only if:

1. E[L(Y ||Z)] ≤ µ

9

2. The (centered) distribution (L(Y ||Z)− E[L(Y ||Z)]) is deﬁned and subgaussian, and its subgaus-

sian parameter is at most τ .

If we have both DsubG (Y ||Z) (cid:22) (µ, τ ) and DsubG (Z||Y ) (cid:22) (µ, τ ), then we say that the pair of

random variables X and Y are (µ, τ )-subgaussian-indistinguishable.

Deﬁnition 3.3 ((µ, τ )-Concentrated Diﬀerential Privacy ((µ, τ )-CDP)). A randomized algorithm
M is (µ, τ )-concentrated diﬀerentially private if for all pairs of adjacent databases x, y, we have
DsubG (M(x)||M(y)) (cid:22) (µ, τ ).
Corollary 3.1 (Concentrated Privacy Loss). For every (µ, τ )-CDP algorithm M, for all pairs of
adjacent databases x, y, taking Y to be the distribution of M(x), and Z to be the distribution of
M(y):

Pr[L(Y ||Z) ≥ µ + (t · τ )] ≤ exp(−

t2
2

)

Proof. Follows from Deﬁnition 3.3 and the concentration properties of subgaussian random variables
(Lemma 2.1).

Guassian Mechanism Revisited. We revisit the Gaussian noise mechanism of [DMNS06],
giving a tight characterization of the privacy loss random variable.

Theorem 3.2 (Gaussian is CDP). Let f : x → R be any real-valued function with sensitivity ∆(f ).
Then the Gaussian mechanism with noise magnitude σ is (τ 2/2, τ )-CDP, where τ = ∆(f )/σ.

Later, we will prove (Theorem 3.5) that every pure diﬀerentially private mechanism also enjoys
concentrated diﬀerential privacy. The Gaussian mechanism is diﬀerent, as it only ensures (ε, δ)-
diﬀerential privacy for δ > 0.
of Theorem 3.2. Let M be the Gaussian mechanism with noise magnitude σ. Let d, d′ be adjacent
databases, and suppose w.l.o.g that f (d) = f (d′) + ∆f . We examine the privacy loss random
variable obtained by drawing a noise magnitude x ∼ N (0, σ2) and outputting:

ln

Pr[M(d) = (f (d) + x)]
Pr[M(d′) = (f (d) + x)]

= ln

e(−1/2σ2)·x2

e(−1/2σ2)·(x+∆f )2

1

1

= ln e(−1/2σ2)·[x2−(x+∆f )2]
2σ2 ·(cid:2)x2 − (x2 + 2x · ∆f + (∆f )2)(cid:3)
= −
2σ2 ·(cid:2)−2x · ∆f − (∆f )2(cid:3)
= −
= (cid:18) ∆f
σ ·

σ (cid:19)2
2(cid:18) ∆f

1

x

σ(cid:19) +

Since x ∼ N (0, σ2), we conclude that the distribution of the privacy loss random variable L(U||V )
is also Gaussian, with expectation (∆f /σ)2 /2, and standard deviation ∆f /σ. Taking τ = ∆f /σ,
we get that DsubG (M(d)||M(d′)) (cid:22) (τ 2/2, τ ).

10

As noted in the Introduction, it is a consequence of Theorem 3.2 that we can achieve (ε(eε −
1)/2, ε)-concentrated diﬀerential privacy by adding independent random noise drawn from N (0, n/ε2)
If we further relax to (ε,√ε)-cdp, we can add even smaller noise, of magnitude
to each query.
(p1/ε). This would make sense in settings where we expect further composition, and so we can

focus on the expected privacy loss (bounding it by ε) and allow more slackness in the standard devi-
ation. For small ε, this gives another order of magnitude improvement in the amount of distortion
introduced to protect privacy.

Finally, we observe that the bounds for group concentrated diﬀerential privacy of the Gaussian
mechanism follow immediately from from Theorem 3.2, noting that for a group of size s the group
sensitivity of a function f is at most s · ∆f .
Corollary 3.3 (Group CDP for the Gaussian Mechanism). The Gaussian mechanism with noise
magnitude σ satisﬁes ((s∆f /σ)2/2, s∆f )-concentrated diﬀerential privacy.

3.1 Composition

Concentrated Diﬀerential Privacy composes “as well as” standard diﬀerential privacy. Indeed, a
primary advantage of CDP is that it permits greater accuracy and smaller noise, with essentially
no loss in privacy under composition. In this section we prove these composition properties. We
follow the formalization in [DRV10]
in modeling composition. Composition covers both repeated
use of (various) CDP algorithms on the same database, which allows modular construction of CDP
algorithms, and repeated use of (various) CDP algorithms on diﬀerent databases that might contain
information pertaining to the same individual. In both of these scenarios, the improved accuracy
of CDP algorithms can provide greater utility for the same “privacy budget”.

Composition of k CDP mechanisms (over the same database, or diﬀerent databases) is formal-
ized by a sequence of pairs of random variables (U, V ) = ((U (1), V (1)), . . . , (U (k), V (k))). The random
variables are the outcomes of adversarially and adaptively chosen CDP mechanisms M1, . . . ,Mk.
In the U sequence (reality), the random variable U (i) is sampled by running mechanism Mi on a
database (of the adversary’s choice) containing an individual’s, say Bob’s, data. In the V sequence
(alternative reality), the random variable V (i) is sampled by running mechanism Mi on the same
database, but where Bob’s data are replaced by (adversarially chosen) data belonging to a diﬀerent
individual, Alice. The requirement is that even for adaptively and adversarially chosen mechanisms
and database-pairs, the outcome of U (Bob-reality) and V (Alice-reality) are “very close”, and in
particular the privacy loss L(U||V ) is subgaussian.

In more detail, and following [DRV10], we deﬁne a game in which a dealer ﬂips a fair coin
to choose between symbols U and V , and an adversary adaptively chooses a sequence of pairs of
adjacent databases (xU
i ) and a mechanism Mi enjoying (µi, τi)-CDP and that will operate on
either the left element (if the dealer chose U ) or the right element (if the dealer chose V ) of the
pair, and return the output, for 1 ≤ i ≤ k. The adversary’s choices are completely adaptive and
hence may depend not only on arbitrary external knowledge but also on what has been observed
in steps 1, . . . , i − 1. The goal of the adversary is to maximize privacy loss. It is framed as a game
because large privacy loss is associated with an increased ability to determine which of (U, V ) was
selected by the dealer, and we imagine this to be the motivation of the adversary.

i , xV

Theorem 3.4 (Composition of CDP). For every integer k ∈ N, every µ1, . . . , µk, τ1, . . . , τk ≥ 0,
and

(U, V ) = ((U (1), V (1)), . . . , (U (k), V (k)))

11

i )1/2).
Proof. Consider the random variables U and V deﬁned above, and the privacy loss random variable

i=i µi, (Pk
constructed as in the game described above, we have that DsubG (U||V ) (cid:22) (Pk
L(U||V ). This random variable is obtained by picking y ∼ U and outputting ln Pr[U =y]
Pr[V =y] .

The mechanism and datasets chosen by the adversary at step i depend on the adversary’s view
at that time. The adversary’s view comprises its randomness and the outcomes it has observed
thus far. Letting RU and RV denote the randomness in the U -world and V -world respectively, we
have, for any y = (y1, . . . , yk) ∈ Supp(U ) and random string r

i=1 τ 2

ln

Pr[U = y]
Pr[V = y]

Pr[RV = r] · Qi∈[k] Pr[U (i) = yi|U (i−1) = yi−1, . . . , U (1) = y1]
Qi∈[k] Pr[V (i) = yi|V (i−1) = yi−1, . . . , V (1) = y1]!
Pr[U (i) = yi|U (i−1) = yi−1, . . . , U (1) = y1]
Pr[V (i) = yi|V (i−1) = yi−1, . . . , V (1) = y1]

= ln  Pr[RU = r]
= Xi∈[k]
, Xi∈[k]

ln

ci(r, y1, . . . , yi) .

Now for every preﬁx (r, y1, . . . , yi−1) we condition on RU = r, U1 = y1, . . . , Ui−1 = yi−1, and
analyze the the random variable ci(RU , U1, . . . , Ui) = ci(r, y1, . . . , yi−1, U (i). Once the preﬁx is
ﬁxed, the next pair of databases xU
i and the mechanism Mi output by the advesary are
also determined. Thus Ui is distributed according to Mi(xU
ci(r, y1, . . . , yi−1, yi) = ln(cid:18) Pr[Mi(xU
Pr[Mi(xV

i ) and for any value yi, we have

i ) = yi](cid:19)

i and xV

i ) = yi]

which is simply the privacy loss when Mi(xU
is (µi, τi) subgaussian.

i ) = yi. By the CDP properties of Mi, L(Mi(xU

i )||Mi(xV

i ))

By the subgaussian properties of the random variables Ci = ci(r, U (1), . . . , U (i)), we have that

L(U||V ) =Pi∈[k] Ci, i.e. the privacy loss random variable equals the sum of the Ci random variables.

By linearity of expectation, we conclude that:

E[L(U||V )] = E[Xi∈[k]

Ci] = Xi∈[k]

E[Ci] = Xiin[k]

µi

and by Lemma 2.2, we have that the random variable:

(L(U||V ) − E[L(U||V )]) = Xi∈[k]

(Ci − E[Ci])

-subgaussian.

is (cid:16)Pi∈[k] τ 2

i(cid:17)1/2

3.2 Relationship to DP

In this section, we explore the relationship between diﬀerential privacy and concentrated diﬀerential.
We show that any diﬀerentially private algorithm is also concentrated diﬀerentially private. Our
main contribution here is a reﬁned upper bound on the expected privacy loss of diﬀerentially private
algorithms: we show that if M is ε-DP, then its expected privacy loss is only (roughly) ε2/2 (for
small enough ε). We also show that the privacy loss random variable for any ε-DP algorithm is
subgaussian, with parameter τ = O(ε):

12

Theorem 3.5 (DP ⇒ CDP). Let M be any ε-DP algorithm. Then M is (ε · (eε − 1)/2, ε)-CDP.
Proof. Since M is (ε, 0)-diﬀerentially private, we know that the privacy loss random variable is
always bounded in magnitude by ε. The random variable obtained by subtracting oﬀ the expected
privacy loss, call it µ, therefore has mean zero and lies in the interval [−ε − µ, ε − µ]. It follows
from Hoeﬀding’s Lemma, stated next, that such a bounded, centered, random variable is (ε − µ −
(−ε − µ))/2 = ε-subgaussian.
Lemma 3.6 (Hoeﬀding’s Lemma). Let X be a zero-mean random variable such that Pr[X ∈
[a, b]] = 1. Then E[eλX ] ≤ e(1/8)λ2(b−a)2

The main challenge is therefore to bound the expectation, namely the quantity DKL(D||D′),
where D is the distribution of M(x) and D′ is the distribution of M(y), and x, y are adjacent
databases. In [DRV10] it was shown that:
Lemma 3.7 ([DRV10]). For any two distributions D and D′ such that D∞(D||D′), D∞(D′||D) ≤ ε,

.

DKL(D||D′) ≤ DKL(D||D′) + DKL(D′||D) ≤ ε · (eε − 1)

We improve this bound, obtaining the following reﬁnement:

Lemma 3.8. For any two distributions D and D′ such that ∆∞(D, D′) = ε,

DKL(D||D′) ≤ ε · (eε − 1)/2

The proof of Theorem 3.5 follows from Lemma 3.8. To prove Lemma 3.8, we introduce the

notion of antipodal distributions:
Deﬁnition 3.4 (Antipodal Distributions). Let D and D′ be two distributions with support X ,
such that ∆∞(D, D′) ≤ ε for some ε > 0. We say that D and D′ are antipodal if ∀x ∈ X , ln D[x]
D′[x] ∈
{−ε, 0, ε}.

We then use the following two lemmas about maximally divergent distributions (the proofs

follow below) to prove Lemma 3.8:
Lemma 3.9. For any two distributions D and D′, there exist antipodal distributions M and M′
such that ∆∞(M, M′) = ∆∞(D, D′) and DKL(D, D′) ≤ DKL(M, M′). Note that the support of
D, D′ may diﬀer from the support of M, M′.
Lemma 3.10. For any antipodal distributions M and M′, as in Deﬁnition 3.4, DKL(M, M′) =
DKL(M′, M ).
Proof of Lemma 3.8. By Lemma 3.9 there exist antipodal distributions M and M′ s.t. ∆∞(M, M′) ≤
ε and DKL(D, D′) ≤ DKL(M, M′). By lemma 3.7, DKL(M||M′) + DKL(M′||M ) ≤ ε · (eε − 1). By
Lemma 3.10, DKL(M||M′) = DKL(M′||M ), and so DKL(M, M′) ≤ ε · (eε − 1)/2. Putting these
together:

DKL(D, D′) ≤ DKL(M, M′) ≤ ε · (eε − 1)/2

13

Proof of Lemma 3.9. Let ε = ∆∞(D, D′). We construct M and M′ iteratively from D and D′ by
enumerating over each x ∈ X . For each such x, we add a new element sx to the support. The idea
is that the mass of x in D and D′ will be “split” between x and sx in M and M′ (respectively).
This split will ensure that the probabilities of sx in M and in M′ are identical, and the probabilities
of x in M and M′ are “maximally divergent”. We will show that the “contribution” of x and sx to
the KL divergence from M to M′ is at least as large as the contribution of x to the KL divergence
from D to D′. The lemma then follows.

We proceed with the full speciﬁcation of this “split” and then formalize the above intuition. For
x ∈ X , take px = D′[x]. Since ∆∞(D, D′) = eε, there must exist α ∈ [−1, 1] s.t. D[x] = eα·ε · px.
We introduce a new item sx into the support, and set the mass of M and M′ on x and sx as follows:

eα·ε − 1
M′[x] = px ·
esign(α)·ε − 1
M [x] = esign(α)·ε · M [x]
= esign(α)·ε · px ·
M′[sx] = D′[x] − M′[x]

= px · (1 −

eα·ε − 1

esign(α)·ε − 1

)

M [sx] = D[x] − M [x]

eα·ε − 1

esign(α)·ε − 1

eα·ε − 1

esign(α)·ε − 1

)
eα·ε − 1

)

esign(α)·ε − 1
eα·ε − 1
esign(α)·ε − 1 − ·
eα·ε − 1

)

esign(α)·ε − 1

= px · (eα·ε − esign(α)·ε ·
= px · (eα·ε − (esign(α)·ε − 1 + 1) ·
= px · (eα·ε − (esign(α)·ε − 1) ·
= px · (eα·ε − (eα·ε − 1) −
= px · (1 −
)
= M′[sx]

esign(α)·ε − 1

eα·ε − 1

eα·ε − 1

esign(α)·ε − 1

)

Observe that all probabilities are at least zero and sum to 1 (for M and M′ respectively). Moreover,
M and M′ are antipodal, ∆∞(M, M′) = ε = ∆∞(D, D′). Thus, the distributions M and M′ satisfy
the conditions of the lemma. Finally, we emphasize that by the above we have ∀x ∈ X, M [sx] =
M′[sx].

We now compare the KL divergence from D to D′ with the divergence from M to M′.

DKL(M, M′) − DKL(D, D′) = Xx∈X(cid:2)(cid:0)M [x] · ln
= Xx∈X(cid:0)M [x] · ln

M [x]
M′[x] − D[x] · ln
M [x]
M′[x] − D[x] · ln

D[x]

D′[x](cid:1) +(cid:0)M [sx] · ln
D′[x](cid:1)

D[x]

M [sx]

M′[sx](cid:1)(cid:3)

Proposition 3.1 below, shows that for every x ∈ X , the summand of x in the above sum is

non-negative. The lemma follows.

14

Proposition 3.1. For every x ∈ X :
(cid:0)M [x] · ln

M [x]
M′[x] − D[x] · ln

D[x]

D′[x](cid:1) ≥ 0

Proof. We use the following equality, and the complete the proof using a case analysis, depending
on whether α ≥ 0 or α < 0.
M [x]
M′[x] − D[x] · ln

M [x] · ln

eα·ε − 1
= (px · esign(α)·ε ·
esign(α)·ε − 1 · sign(α) · ε) − (px · eα·ε · (α · ε))
= px · ε ·(cid:0)sign(α) · esign(α)·ε ·

eα·ε − 1
esign(α)·ε − 1 − eα·ε · α(cid:1)

D[x]
D′[x]

=

=

We proceed with a case analysis:

=

px · ε
esign(α)·ε − 1 ·(cid:0)(sign(α) · esign(α)·ε · (eα·ε − 1) − ((esign(α)·ε − 1) · eα·ε · α)(cid:1)
px · ε · esign(α)·ε
esign(α)·ε − 1 ·(cid:0)sign(α) · (eα·ε − 1) − (eα·ε · α) + (e−sign(α)·ε · eα·ε · α)(cid:1)
px · ε · esign(α)·ε
esign(α)·ε − 1 ·(cid:0)eα·ε · (sign(α) + α · (−1 + e−sign(α)·ε)) − sign(α)(cid:1)

Case I: α ≥ 0. Here sign(α) = 1. We use the following inequality:
Claim 3.11. For ε ≥ 0, α ∈ [0, 1]:

α · (−1 + e−ε) ≥ e−α·ε − 1

Proof. Observe that, for any ﬁxed α ∈ [0, 1], we have equality when ε = 0 (both the left-hand and
right-hand sides of the above inequality equal 0). Taking derivatives (by ε) for both sides, on the
left-hand side the derivative is −α· e−ε, whereas on the right-hand side it is −α· e−α·ε. We conclude
that ∀ε > 0, α ∈ [0, 1], the derivative left-hand side is at least the derivative on the right-hand side
(because in this range −α · e−ε ≥ −α · e−α·ε).

Now, by the above we have:

M [x] · ln

M [x]
M′[x] − D[x] · ln

D[x]
D′[x]

=

px · ε · eε
eε − 1 ·(cid:0)eα·ε · (1 + α · (−1 + e−ε)) − 1(cid:1)
px · ε · eε
eε − 1 ·(cid:0)eα·ε · (1 + (e−α·ε − 1)) − 1(cid:1)

≥
= 0

Case II: α < 0. Here sign(α) = −1. We use the following inequality:
Claim 3.12. For ε ≥ 0, α ∈ [−1, 0):

α · (−1 + eε) ≤ −e−α·ε + 1

15

Proof. Observe that, for any ﬁxed α ∈ [−1, 0), we have equality when ε = 0 (both the left-hand
and right-hand sides of the above inequality equal 0). Taking derivatives (by ε) for both sides, on
the left-hand side the derivative is α· eε, whereas on the right-hand side it is α· e−α·ε. We conclude
that ∀ε ≥ 0, α ∈ [−1, 0), the derivative on the left-hand side is always at most the derivative on the
right-hand side (because in this range α · eε ≤ α · e−α·ε).

Recall from above that:

M [x] · ln

M [x]
M′[x] − D[x] · ln

D[x]
D′[x]

=

px · ε · e−ε
e−ε − 1 ·(cid:0)eα·ε · (−1 + α · (−1 + eε)) + 1(cid:1)

The right-hand side of this equation is a product of two terms. The ﬁrst is:

px · ε · e−ε
e−ε − 1 ≤ 0

(because the numerator is positive, and the denominator is negative).

For the second term in the product, we use Claim 3.12, and get:

(eα·ε · (−1 + α · (−1 + eε)) + 1) ≤ (eα·ε · (−1 + −e−α·ε + 1) + 1)

= 0

We conclude that in this case (α < 0), the diﬀerence in x’s contribution to the KL divergences
equals the product of two non-positive terms, and so it must be non-negative.

Proof of Lemma 3.10. Let ε = ∆∞(M, M′). Since M and M′ are antipodal, w.l.o.g we can study
their KL divergence for the special case where the support is over 3 items: {x, y, s}, and where

M [x] = p, M [y] = q, M [s] = r

M′[x] = p′ = p · e−ε, M′[y] = q′ = q · eε, D′[s] = r′ = r

We analyze the expected privacy losses, or KL divergences, from M to M′ and from M′ to M .

First, observe that 1 = (p + q + r) = (p′ + q′ + r′). We conclude that:
p + q = p′ + q′ ⇒ p + q = p · e−ε + q · eε

⇒ p · (1 − e−ε) = q · (eε − 1)
⇒ p = q ·

eε − 1
1 − e−ε

Examining the KL divergences, we have:

DKL(M||M′) = px · ln(p/p′) + q · ln(q/q′) + s · ln(s/s′)

DKL(M′||M ) = p′ · ln(p′/p) + q′ · ln(q′/q) + s′ · ln(s′/s)

= px · ε − q · ε
= ε · (px − q)

= −px · e−ε · ε + q · eε · ε
= ε · (q · eε − px · e−ε)

16

We bound the diﬀerence as follows:

DKL(M||M′) − DKL(M′||M ) = ε · (p − q) − ε · (q · eε − p · e−ε)
= ε ·(cid:0)p · (e−ε + 1) − q · (eε + 1)(cid:1)
eε − 1
1 − e−ε · (e−ε + 1) − (eε + 1)(cid:1)
= ε · q ·(cid:0)
eε − 1
1 − e−ε · (e−ε − 1 + 2) − (eε + 1)(cid:1)
= ε · q ·(cid:0)
eε − 1
(eε − 1) · (e−ε − 1)
1 − e−ε − (eε + 1)(cid:1)
= ε · q ·(cid:0)
1 − e−ε
= ε · q ·(cid:0) − (eε − 1) + 2 ·
eε − 1
1 − e−ε − eε(cid:1)
= 2 · ε · q ·(cid:0)
(eε − 1) − eε · (1 − e−ε)
= 2 · ε · q ·(cid:0)
1 − e−ε
(eε − 1) − eε + 1
= 2 · ε · q ·(cid:0)

+ 2 ·
eε − 1
1 − e−ε − (eε + 1))

1 − e−ε

= 0

(cid:1)

(cid:1)

4 Group Privacy

We show that arbitrary mechanisms that guarantee concentrated diﬀerential privacy also provide
concentrated diﬀerential privacy for groups. This is stated in Theorem 4.1 below. The bounds
are asymptotically nearly-tight, up to low-order terms. It would be interesting to tighten these
bounds to match the tight group privacy guarantees of (all) known concentrated diﬀerential privacy
mechanisms, such as the Gaussian Mechanism or pure-ε Diﬀerentially private mechanisms. See the
discussion in the introduction.

Theorem 4.1 (Group CDP). Let M be a (µ, τ )-concentrated diﬀerentially private mechanism.
Let x, y be a pair of databases that diﬀer on exactly s rows. Suppose that (τ · s · log3 s) is bounded
from above by a suﬃciently small constant and µ ≤ τ 2/2. Then:

DsubG (M(x)||M(y)) (cid:22)(cid:18) (s · τ )2

2

+ ˜O((s · τ )2.5), (s · τ ) + ˜O((s · τ )1.5)(cid:19)

(Note that since (s · τ ) < 1, this implies that the privacy loss random variable has expectation
roughly (s·τ )2

, and the subgaussian standard is roughly (s · τ ), all up to the low-order terms).

2

Proof. We assume for convenience that s is a power of 2. The proof will be by induction over the
value of log s. For s = 1 the claim follows immediately. For the induction step, suppose that the
claim is true for databases diﬀering on 2m rows. We will show that it is true for x, y that diﬀer
on 2m+1 rows. We take µm and τm to be bounds on the expectation and the standard of the

17

(centered) privacy loss distribution for databases that diﬀer on at most 2m rows, so µ0 = µ and
τ0 = τ . We maintain the invariant that µm ≤ τ 2
m/2 (for the base case m = 0 this holds by the
lemma conditions).

For the induction step, let z be an “midpoint” database that diﬀers from both x and y on exactly
2m rows. Deﬁne the mechanism’s output distributions on these databases by D = M(x), D′ =
M(z), D′′ = M(y). By the induction hypothesis, we conclude that:

DsubG (D||D′), DsubG (D′||D) (cid:22) (µm, τm)
DsubG (D′′||D′), DsubG (D′||D′′) (cid:22) (µm, τm)

We use Lemmas 4.2 and 4.3, stated and proved in Sections 4.1 and 4.2 below, to bound µm+1 and
τm+1.

Bounding τm+1. By Lemma 4.3, we have that

We prove that this recursive relation implies that:

τm+1 ≤ 2τm + 34τ 1.5

m

τm+1 ≤ (2m+1 · τ ) + α · (2m+1 · (m + 1)3 · τ )1.5

(5)

(6)

where α > 0 is a suﬃciently large universal constant speciﬁed below. This implies the claimed
bound on τlog s.

To prove the bound in Inequality (6), consider τm in light of the recurrence relation of Equation

(5). We bound τm by proving a bound of the following form:

m

τm ≤ 2m · τ +

m

cm,i · τ 1.5i
Xi=1
Xi=1
= cm,0 · τ +
cm,i · τ 1.5i
Xi=0

cm,i · τ 1.5i

m

=

(7)

Where the term cm,0 is (by deﬁnition) equal to 2m, and we bound the terms {cm,i}m∈[1,log s],i∈[1,m]
as follows. For m ∈ [1, log s], i ∈ [1, m], we show that:

cm,i ≤ 2 · (2m)1.5i

· 342·(1.5i+1−1.5) · m2·(1.5i+1−1.5)

(8)

We conclude that:

cm,i < (2m)1.5i
= 2m·1.5i
= (2m · 343 · m3)1.5i

· 342·(1.5i+1) · m2·(1.5i+1)
· 343·1.5i

· m3·1.5i

18

These bounds are shown below. Plugging the bounds into equation (7) we get:

m+1

m+1

Xi=1
Xi=1

τm+1 ≤ (2m+1 · τ ) +

(2m+1 · 343 · (m + 1)3)1.5i

· τ 1.5i

(2m+1 · 343 · (m + 1)3 · τ )1.5i

= (2m+1 · τ ) +
≤ (2m+1 · τ ) + 2(2m+1 · 343 · (m + 1)3 · τ )1.5
= (2m+1 · τ ) + α · (2m+1 · (m + 1)3 · τ )1.5

where the next-to-last inequality assumes that (τ · s · log3 s · 343) ≤ 1/2, and the last inequality
holds for a suﬃciently large universal constant α = 2 · 344.5. This proves the bound claimed in
Equation (6).
We prove Inequalities (7) and (8) by induction on m. The base case is for m = 0, where c0,0 = 1
and for i ≥ 1 we have c0,i = 0. Assuming the bounds are true for m, and using Inequality (5)
(derived from Lemma 4.3), we have:

τm+1 ≤ 2τm + 34τ 1.5

m

cm,i · τ 1.5i!1.5

≤ 2  m
Xi=0
≤ 2  m
Xi=0
= 2cm,0 · τ +

cm,i · τ 1.5i! + 34 ·  m
Xi=0
cm,i · τ 1.5i! + 34 · √m + 1 ·  m
Xi=0
Xi=1(cid:0)2cm,i + (34√m + 1 · c1.5
m,i−1)(cid:1) · τ 1.5
where Inequality (9) uses the fact that for positive terms ai we have (Pm
(Pm

i=0 a1.5
Using Inequality (10), we can deﬁne the terms cm+1,i as follows:

i ), which follows from Jensen’s Inequality (since x1.5 is a convex function).

m

c1.5

m,i · τ 1.5i+1!

(9)

(10)

i=0 ai)1.5 ≤ √m + 1 ·

cm+1,0 = 2cm,0 = 2m+1 · τ

∀i ∈ [1, m], cm+1,i = 2cm,i + 34 · √m + 1 · c1.5

m,i−1

It remains to prove Inequality (8) for i ∈ [1, m]. The proof is by induction. The base case for m = 0
follows by deﬁnition. Assume for a ﬁxed value m, the inequality holds ∀i ∈ [1, m]. I.e.:

cm,i ≤ (2m)1.5i

· 342·(1.5i+1−1.5) · m2·(1.5i+1−1.5)

19

Then we get that ∀i ∈ [1, m + 1]:
cm+1,i = 2cm,i + 34 · √m + 1 · c1.5

m,i−1

· 342·(1.5i+1−1.5) · m2·(1.5i+1−1.5)(cid:17) + 34√m + 1 ·(cid:16)(2m)1.5i−1
· 342·(1.5i+1−1.5) · m2·(1.5i+1−1.5)(cid:17) + 34√m + 1 ·(cid:16)2m·1.5i
· 342·(1.5i+1−1.5) · m2·(1.5i+1−1.5)(cid:17) +(cid:16)2m·1.5i
· 342·(1.5i+1−1.5) · m2·(1.5i+1−1.5)(cid:17) +(cid:16)2m·1.5i
· 342·(1.5i+1−1.5) · m2·(1.5i+1−1.5)(cid:17) + (cid:16)2m·1.5i · 342·(1.5i+1−1.5) · (m + 1)2·(1.5i+1−1.5)(cid:17)
· 342·(1.5i+1−1) · (m + 1)2·(1.5i+1−1/2)

· 342·(1.5i−1.5) · m2·(1.5i−1.5)(cid:17)1.5
· 342·(1.5i+1−1.52) · m2·(1.5i+1−1.52)(cid:17)
· 342·(1.5i+1−1.52)+1 · (m + 1)2·(1.5i+1−1.52)+1/2(cid:17)
· 342·(1.5i+1−1.5)−0.5 · (m + 1)2·(1.5i+1−1.5)−1(cid:17)

340.5 · (m + 1)

≤ 2(cid:16)(2m)1.5i
= 2(cid:16)2m·1.5i
< 2(cid:16)2m·1.5i
= 2(cid:16)2m·1.5i
= 2(cid:16)2m·1.5i
< 2.5 · 2m·1.5i
= 2.5 · 2((m+1)·1.5i)−1.5i

· 342·(1.5i+1−1.5) · (m + 1)2·(1.5i+1−1.5)

=

2.5

21.5i · 2(m+1)·1.5i

· 342·(1.5i+1−1.5) · (m + 1)2·(1.5i+1−1.5)

< 2(m+1)·1.5i

· 342·(1.5i+1−1.5) · (m + 1)2·(1.5i+1−1.5)

Bounding µm+1. We prove that:

µm+1 ≤

(2m+1 · τ )2

2

+ α · (2m+1 · τ )2.5 · m4.5.

(11)

(where α is the universal constant speciﬁed above). The proof will be by induction over m. For
the base case m = 0, we know that µ ≤ τ 2/2 by the lemma conditions. For the induction step, by
Lemma 4.2 and using µm ≤ τ 2

m + 3.5τ 3

m + 1.5τ 4
m.

(12)

m/2:
µm+1 ≤ 2µm + τ 2

Using the induction hypothesis and the bound on the subgaussian standard τm shown above (In-
equality (6)), we conclude (so long as τm is a suﬃciently small constant) that:

µm+1 ≤ 2µm +(cid:0)τ 2

m + 3.5τ 3

m + 1.5τ 4

m(cid:1)

≤ 2µm +(cid:0)(2m · τ )2 + 3α · ((2m · τ )2.5 · m4.5)(cid:1)
≤(cid:0)(2m · τ )2 + 2α · (2m · τ )2.5 · m4.5(cid:1) +(cid:0)(2m · τ )2 + 3α · ((2m · τ )2.5 · m4.5)(cid:1)
= 2(2m · τ )2 + 5α · ((2m · τ )2.5 · m4.5)
(2m+1 · τ )2
+ α · (2m+1 · τ )2.5 · m4.5.
<

2

This implies the claimed bound on µlog s (since (s · polylogs · τ ) < 1).

20

Relationship between µm+1 and τm+1. Finally, we show that the above bounds maintain that
µm+1 ≤ τ 2

m+1/2. To see this:

τ 2

m+1 =(cid:0)(2m+1 · τ ) + α · (2m+1 · (m + 1)3 · τ )1.5(cid:1)

≥ (2m+1 · τ )2 + 2α · (2m+1 · τ ) · (2m+1 · (m + 1)3 · τ )1.5
= (2m+1 · τ )2 + 2α · (2m+1 · τ )2.5 · m4.5
≥ 2µm+1

4.1 Group Privacy: Bounding the Expected Privacy Loss

In this section we bound the expected privacy loss for groups, using the following Lemma:
Lemma 4.2. Let D, D′, D′′ be distributions over domain X , such that DsubG (D||D′), DsubG (D′||D) (cid:22)
(µ1, τ1) and that DsubG (D′||D′′), DsubG (D′′||D′) (cid:22) (µ2, τ2). Suppose moreover that τ1 ≤ 1/3. Then
it is also the case that:

DKL(D||D′′) ≤ µ1 + µ2 + τ1 · τ2 +(cid:0)(2τ 2

1 · τ2) + ((τ1 + 3τ 2
Proof. For x ∈ X , we deﬁne S(x) to be the centered value S(x) = ln D′[x]
D[x] − DKL(D′||D), and S′′(x)
to be the centered value S′′(x) = ln D′[x]
D′′[x] − DKL(D′||D′′). When x is drawn by D′[x], both S[x]
and S′′[x] are (centered) subgaussian random variables. We use Var (S), Var (S′′) to denote the
variances of these random variables (which are bounded by τ 2

1 ) · µ2)(cid:1)

2 respectively). We have that:

1 , τ 2

D[x]

D′′[x](cid:19)

DKL(D||D′′) = Xx∈X(cid:18)D[x] · ln
= Xx∈X(cid:18)D[x] ·(cid:0) ln
= Xx∈X(cid:18)D[x] · ln

D′[x]

+ ln

D[x]
D′[x]

D′′[x](cid:1)(cid:19)
D′[x](cid:19) + Xx∈X(cid:18)D[x] · ln

D[x]

D′[x]

D′′[x](cid:19)

(13)

21

The ﬁrst of these summands is DKL(D||D′) = µ1. We bound the second summand:

Xx∈X(cid:18)D[x] · ln

D′[x]

D′′[x](cid:19) = Xx∈X(cid:18)D′[x] ·

D′[x]

D′[x]

D′′[x](cid:19)
D′′[x](cid:19)

D[x]
D′[x] · ln
= Xx∈X(cid:18)D′[x] · e−ln D′[x]
D[x] · ln
= Xx∈X(cid:18)D′[x] · e−(ln D′[x]
= e−DKL(D′||D) · Xx∈X(cid:18)D′[x] · e−S(x) · ln
≤ Xx∈X(cid:18)D′[x] · e−S(x) · ln
D′′[x] · e−S(x)(cid:21)
x∼D′(cid:20)ln

D′′[x](cid:19)

D′[x]

D′[x]

= E

D[x] −DKL(D′||D)+DKL(D′||D)) · ln

D′[x]

D′′[x](cid:19)

D′[x]

D′′[x](cid:19)

where the next-to-last inequality is by non-negativity of KL-divergence. Recall that S(x) denotes
the centered log-ratio of probabilities by D′ and by D (and is τ1-Subgaussian). By Lemma 2.3 we
conclude that:

Xx∈X(cid:18)D[x] · ln

D′[x]

D′′[x](cid:19) ≤ E

D′[x]

D′′[x] · e−S(x)(cid:21)

x∼D′(cid:20)ln
≤ DKL(D′||D′′) +(cid:16)pVar (S′′) + DKL(D′||D′′)(cid:17) · (τ1 + 3τ 2
≤ µ2 +(cid:18)qτ 2
= µ2 + τ1 · τ2 +(cid:0)(3τ 2

2 + µ2(cid:19) · (τ1 + 3τ 2

1 · τ2) + ((τ1 + 3τ 2

1 )

1 )

(14)

1 ) · µ2)(cid:1)
1 · τ2) + ((τ1 + 3τ 2

1 ) · µ2)(cid:1)

Putting together Equations (13) and (14) we get that:

DKL(D||D′′) ≤ µ1 + µ2 + τ1 · τ2 +(cid:0)(3τ 2

4.2 Group Privacy: Bounding the Subgaussian Standard
Lemma 4.3. Let D, D′, D′′ be distributions over domain X , such that DsubG (D||D′), DsubG (D′||D) (cid:22)
(µ1, τ1) and that DsubG (D′||D′′), DsubG (D′′||D′) (cid:22) (µ2, τ2). Suppose moreover that τ1, τ2 ≤ τ ≤ 1/4
and that µ1, µ2 ≤ τ 2/2. Then for any real λ:

E

x∼D(cid:20)e

λ·(ln D[x]

D′′[x]−DKL(D||D′′))(cid:21) ≤ e

λ2
2 ·(2τ +34τ 1.5)2

(15)

i.e. the (centered) privacy-loss random variable from D to D′′ is subgaussian, and its standard is
bounded by 2τ + O(τ 1.5).

22

Proof. We assume without loss of generality that:

λ · DKL(D′||D) ≥ λ · DKL(D′||D′′)
⇒ (λ + 1) · DKL(D′||D) ≥ λ · DKL(D′||D′′)

(16)
(otherwise we ﬂip the roles of D and D′′). As in the proof of Lemma 4.2, for x ∈ X , we deﬁne
S to be the centered value S(x) = ln D′[x]
D[x] − DKL(D′||D), and S′′(x) to be the centered value
S′′(x) = ln D′[x]
D′′[x] − DKL(D′||D′′). Recall that when x is drawn by D′[x], both S[x] and S′′[x] are
(centered) subgaussian random variables.
We want to show that Inequality (15) holds for any real λ. We proceed with a case analysis for

the value of λ as a function of τ .

. Observe that:

Case I: |λ| ≥ 1
8√τ
λ·ln D[x]

E

x∼D(cid:20)e

D′[x]

ln D[x]

+ln D′[x]
D′′[x]

λ·ln D[x]

λ·(ln D[x]

)(cid:19)
λ·ln D′[x]
D′′[x](cid:19)

D′′[x](cid:21) = Xx∈X(cid:18)D[x] · e
D′′[x](cid:19)
= Xx∈X(cid:18)D′[x] · e
D′[x] · e
= Xx∈X(cid:18)D′[x] · e−(λ+1)·ln D′[x]
D[x] · e
= Xx∈X(cid:16)D′[x] · e−(λ+1)·(S(x)+DKL(D′||D)) · eλ·(S ′′(x)+DKL(D′||D′′))(cid:17)
=(cid:16)e(−(λ+1)·DKL(D′||D))+(λ·DKL(D′||D′′))(cid:17) · Xx∈X(cid:16)D′[x] · e−(λ+1)·S(x) · eλ·S ′′(x)(cid:17)
≤ Xx∈X(cid:16)D′[x] · e−(λ+1)·S(x) · eλ·S ′′(x)(cid:17)

where the last inequality is by Equation (16). Recall that S(x) and S′′(x) are both subgaussian
(when x is drawn by D′[x]), with standards τ1, τ2. Applying the Cauchy-Schwartz inequality to the
above, we conclude:

E

x∼D(cid:20)e

λ·ln D[x]

D′′[x](cid:21) ≤sXx∈X

D′[x] · e−2(λ+1)·S(x) ·sXx∈X

≤pe(2λ2+4λ+2)·τ 2

= e(λ+1)2·τ 2

1 +λ2·τ 2

1 ·pe2λ2·τ 2

2

2

D′[x] · e−2λ·S ′′(x)

(17)

Since |λ| ≥ 1

8√τ , we have that:

(λ + 1)2 = λ2 · (1 +

1
λ

)2

≤ λ2 · (1 + 8√τ )2
= λ2 · (1 + 16√τ + 64τ )
≤ λ2 · (1 + 48√τ )

23

(recall also that τ ≤ 1/4, so τ ≤ √τ /2). Plugging this into Equation (17), we get that:

λ·ln D[x]

E

x∼D(cid:20)e

1 +λ2·τ 2

2

1 ·(1+48√τ ))+(λ2·τ 2
2 )
1 +τ 2

2 +48τ 2.5)

D′′[x](cid:21) ≤ e(λ+1)2·τ 2
≤ e(λ2·τ 2
≤ eλ2·(τ 2
≤ e

λ2
2 ·(2τ +12τ 1.5)2

.

We conclude that for λ ≥ 1

8√τ , the “standard” is bounded by (2τ + 12τ 1.5).

8√τ Taking a Taylor expansion, we get that:

Case II: |λ| < 1
x∼D(cid:20)e
λ·ln D[x]

E

D′′[x](cid:21) = 1 + λ · DKL(D||D′′) +

λ2
2 · E

x∼D(cid:20)ln2 D[x]

D′′[x](cid:21) +

∞

Xk=3

λk
k! · E

x∼D(cid:20)lnk D[x]
D′′[x](cid:21)

(18)

(where we observe that the linear summand in the Taylor expansion is the expected log ratio or
“privacy loss” from D to D′′). In the following two claims, we bound the higher moments in the
Taylor expansion:

Claim 4.4.

Claim 4.5.

E

x∼D(cid:20)ln2 D[x]

D′′[x](cid:21) ≤ 2(τ 2

2 ) + 2µ2

1 + 2µ2

2 + 50τ1 · τ 2

2

1 + τ 2
≤ 4τ 2 + 51τ 3
≤ 4τ 2 + 26τ 2.5

∞

Xk=3

λk
k! · E

x∼D(cid:20)lnk D[x]

D′′[x](cid:21) ≤ (33 · τ 2.5
≤ 55 · τ 2.5 · λ2

1

· λ2) + (32 · τ 2.5

2

(19)

· λ2)

The proofs of Claims 4.4 and 4.5 follow below. Before presenting these proofs, we complete the
proof of the bound for case II. Plugging the bounds from the Claims into Inequality (18) we get:

λ·ln D[x]

E

x∼D(cid:20)e

D′′[x](cid:21) = 1 + λ · DKL(D||D′′) +
= 1 + λ · DKL(D||D′′) +
≤ eλ·DKL(D||D′′)+ λ2
≤ eλ·DKL(D||D′′)+ λ2

2 ·((2τ )2+136τ 2.5)
2 ·(2τ +34τ 1.5)2

λ2 · (4τ 2 + 26τ 2.5)

2
+ 68τ 2.5 · λ2

4λ2 · τ 2

2

+ 55τ 2.5 · λ2

Thus, for the centered privacy-loss random variable we have:

E

x∼D(cid:20)e

λ·(ln D[x]

D′′[x]−DKL(D||D′′))(cid:21) ≤ e

λ2
2 ·(2τ +34τ 1.5)2

24

We conclude that for λ < 1

8√τ , the “standard” is bounded by (2τ + 34τ 1.5).

Before proceeding to prove the claims, we state and prove the following useful fact:

Fact 4.1. For a real value k ≥ 1 and for any two reals a, b:

Proof. Since the function xk is convex, by Jensen’s inequality we have:

(a + b)k ≤ 2k−1 · (ak + bk)
2 (cid:19)k

(2a)k + (2b)k

≤

2b

2

+

(a + b)k =(cid:18) 2a

2

= 2k−1 · (ak + bk)

Proof of Claim 4.4. We observe that, since (a + b)2 ≤ 2a2 + 2b2 (for all a, b, see Fact 4.1):

E

x∼D(cid:20)ln2 D[x]

D[x]
D′[x]

x∼D(cid:20)(ln
D′′[x](cid:21) = E
x∼D(cid:20)ln2 D[x]
≤ 2 E
≤ 2(τ 2

1 + µ2

+ ln

D′[x]
D′′[x]

)2(cid:21)

D′[x](cid:21) + 2 E

x∼D(cid:20)ln2 D′[x]
D′′[x](cid:21)
x∼D(cid:20)ln2 D′[x]
D′′[x](cid:21)

1) + 2 E

(20)

where the last inequality uses Fact 2.1 (and the fact that E[X 2] = Var (X) + (E[X])2 for any RV
X). To bound the second summand in Inequality (20), we use Lemma 2.3 and conclude that:

E

x∼D(cid:20)ln2 D′[x]

x∼D′(cid:20)ln2 D′[x]
D′′[x](cid:21) = E
x∼D′(cid:20)ln2 D′[x]
≤ E
x∼D′(cid:20)ln2 D′[x]
x∼D′(cid:20)ln2 D′[x]
≤ E

D′′[x] · e− ln D′[x]
D[x](cid:21)
D′′[x](cid:21) +s E
x∼D′(cid:20)ln4 D′[x]
D′′[x](cid:21) +q E
D′′[x](cid:21) +q8 · ( E

x∼D′

x∼D′

= E

D′′[x](cid:21) · (τ1 + 3τ 2

1 )

[(S′′(x) + DKL(D′||D′′))4] · (τ1 + 3τ 2
1 )

[S′′(x)4] + DKL(D′||D′′)4) · (τ1 + 3τ 2
1 )

where the last inequality uses Fact 4.1 (with k = 4). By the bound on the 4-th moment of the
subgaussian S′′(x) (Fact 2.2), we conclude that:

E

x∼D(cid:20)ln2 D′[x]

D′′[x](cid:21) ≤ (τ 2
≤ (τ 2
≤ (τ 2
≤ (τ 2

2 + µ2

2 + µ2
2 + µ2
2 + µ2

2) +q8 · (16τ 4
2) +q8 · (16τ 4
2) + 24τ1 · τ 2
2) + 25τ1 · τ 2

2

2 + µ4

2 + µ4

2) · (τ1 + 3τ 2
1 )
2) · 2τ1

2 + 6τ1 · µ2

2

(21)

Claim 4.4 follows from inequalities (20) and (21):

E

x∼D(cid:20)ln2 D[x]

D′′[x](cid:21) ≤ 2(τ 2
≤ 2(τ 2

1 + µ2

1) + 2(τ 2

2 + µ2

1 + τ 2

2 ) + 2µ2

1 + 2µ2

2 + 25τ1 · τ 2
2 )
2 + 50τ1 · τ 2

2

25

Proof of Claim 4.5. Observe that, by Fact 4.1:

E

x∼D(cid:20)lnk D[x]

D′[x]
D′′[x]

)k(cid:21)

+ ln

D[x]
D′[x]

D′′[x](cid:21) = E
x∼D(cid:20)(ln
x∼D(cid:20)2k−1 · (lnk D[x]
≤ E
= 2k−1 ·(cid:18) E
x∼D(cid:20)lnk D[x]

D′[x]

+ lnk D′[x]
D′′[x]

)(cid:21)
x∼D(cid:20)lnk D′[x]
D′′[x](cid:21)(cid:19)

D′[x](cid:21) + E

By Fact 2.2, and using also Fact 4.1, the ﬁrst term is bounded by:

E

x∼D(cid:20)lnk D[x]

D′[x](cid:21) ≤ 2k−1 ·(cid:18) E

x∼DhS(x)k + DKL(D||D′)ki(cid:19)
≤ (2k−1 · 2⌈k/2⌉+1 · (⌈k/2⌉)! · τ k
= (2τ1)k · 2⌈k/2⌉ · (⌈k/2⌉)! + (2µ1)k/2
√k!(cid:17) + (2µ1)k/2
≤(cid:16)(2τ1)k · 2k/2+1 ·

1 ) + (2µ1)k/2

We bound the second term using Lemma 2.3 and Fact 4.1:

(22)

(23)

E

x∼D(cid:20)lnk D′[x]

1 )

D′′[x](cid:21) · (τ1 + 3τ 2

x∼D′(cid:20)lnk D′[x]
D′′[x](cid:21) = E
x∼D′(cid:20)lnk D′[x]
≤ E
≤ 2k−1 · E
≤ 2k−1(cid:18) E
≤ 2k−1(cid:18) E
≤ 2k−1 ·(cid:18) E
≤ 2k−1 · E

D′[x](cid:21)
D′′[x] · e− ln D[x]
D′′[x](cid:21) +s E
x∼D′(cid:20)ln2k D′[x]
x∼D′hS′′(x)k + DKL(D′||D′′)ki +s E
x∼D′(cid:20)ln2k D′[x]
x∼D′hS′′(x)ki + µk
x∼D′hS′′(x)ki + µk
x∼D′hS′′(x)ki + µk
x∼D′hS′′(x)ki +q22k−1 · E

2(cid:19) +s E
2(cid:19) +q22k−1 · E
2(cid:19) +(cid:18)q22k−1 · E

x∼D′

x∼D′

1 )

1 )

D′′[x](cid:21) · (τ1 + 3τ 2

x∼D′(cid:20)ln2k D′[x]
D′′[x](cid:21) · (τ1 + 3τ 2
[S′′(x)2k + DKL(D′||D′′)2k] · (τ1 + 3τ 2
1 )
2 (cid:19) · (τ1 + 3τ 2
[S′′(x)2k] +q22k−1 · µ2k

x∼D′

1 )

[S′′(x)2k] · (τ1 + 3τ 2

1 ) + (2µ2)k,

where the last inequality follows because τ1 + 3τ 2
the above that:

1 ≤ 7/16 < 1/2. By Fact 2.2, we conclude from
1 )(cid:19) + (2µ2)k

2

E

x∼D(cid:20)lnk D′[x]

D′′[x](cid:21) ≤(cid:16)(2τ2)k · 2⌈k/2⌉ · (⌈k/2⌉)!(cid:17) +(cid:18)q22k−1 · 2k+1 · (k!) · τ 2k
√k! · (τ1 + 3τ 2

=(cid:16)(2τ2)k · 2⌈k/2⌉ · (⌈k/2⌉)!(cid:17) +(cid:16)(2τ2)k · 2k/2 ·
≤(cid:16)(2τ2)k · 2k/2+1 ·

√k!(cid:17) + (2µ2)k

· (τ1 + 3τ 2
1 )(cid:17) + (2µ2)k

(24)

26

Plugging the bounds from Equations (23) and (24) into Equation (22), we conclude that:

E

x∼D(cid:20)lnk D[x]

D′′[x](cid:21) ≤(cid:16)(4τ1)k + (4τ2)k(cid:17) ·(cid:16)2k/2 ·
≤(cid:16)(4τ1)k + (4τ2)k(cid:17) ·(cid:16)2k/2 ·

√k!(cid:17) + (2µ2)k + (2µ2)k
√k!(cid:17) + 2 · (2µ1)k

(25)

√k!
k!

< 2, we get that:

Using the fact that for k ≥ 3 we have 2k/2·
∞
Xk=3

x∼D(cid:20)lnk D[x]

D′′[x](cid:21) ≤

λk
k! · E

∞

Xk=3
= 2 · (4τ1 · λ)3 ·

2(cid:16)(4τ1 · λ)k + (4τ2 · λ)k(cid:17) + 2 ·

(2µ1 · λ)k
(4τ1 · λ)k! + 2 · (4τ2 · λ)3 ·
Xk=0

(4τ1 · λ)k! + (µ1 · λ)2
where the last inequality uses the fact that for k ≥ 3, we have (2µ1·λ)k
k! < (µ1 · λ)2 · (1/4)k (because
we assume here that λ < 1/8√τ , so µ1 · λ ≤ τ 1.5/16 < 1/128). Moreover, since 4τ1 · λ, 4τ2 · λ ≤ 1/2,

the geometric sums above converge to a value smaller than 2, and we get:

Xk=0

∞

∞

k!

∞

Xk=3

λk
k! · E

x∼D(cid:20)lnk D[x]

D′′[x](cid:21) ≤ 44 · (τ1 · λ)3 + 44 · (τ2 · λ)3 + (µ2 · λ)2

Finally, since √τ1 · λ,√τ2 · λ ≤ 1/8, and µ2 ≤ τ 2
D′′[x](cid:21) ≤ (32 · τ 2.5
≤ (33 · τ 2.5

x∼D(cid:20)lnk D[x]

λk
k! · E

Xk=3

∞

1

1

1 /2, we get that:

2

· λ2) + (32 · τ 2.5
· λ2) + (32 · τ 2.5

2

1 · λ2)

· λ2) + (τ 4
· λ2)

References

[BK00]

V.V. Buldygin and I.U.V. Kozachenko. Metric Characterization of Random Variables
and Random Processes. Cross Cultural Communication. American Mathematical Soc.,
2000.

[BS15]

Mark Bun and Thomas Steinke. Concentrated diﬀerential privacy revisited. Manuscript,
2015.

[DKM+06] Cynthia Dwork, Krishnaram Kenthapadi, Frank McSherry, Ilya Mironov, and Moni
Naor. Our data, ourselves: Privacy via distributed noise generation. In Advances in
Cryptology - EUROCRYPT 2006, 25th Annual International Conference on the Theory
and Applications of Cryptographic Techniques, St. Petersburg, Russia, May 28 - June
1, 2006, Proceedings, pages 486–503, 2006.

27

[DL09]

C. Dwork and J. Lei. Diﬀerential privacy and robust statistics. In STOC, 2009.

[DMNS06] C. Dwork, F. McSherry, K. Nissim, and A. Smith. Calibrating noise to sensitivity in

private data analysis. In TCC, pages 265–284, 2006.

[DMT07] C. Dwork, F. McSherry, and K. Talwar. The price of privacy and the limits of lp

decoding. In STOC, pages pp. 85–94, 2007.

[DN03]

Irit Dinur and Kobbi Nissim. Revealing information while preserving privacy. In PODS,
pages 202–210, 2003.

[DRV10]

Cynthia Dwork, Guy N. Rothblum, and Salil P. Vadhan. Boosting and diﬀerential
privacy. In FOCS, pages 51–60, 2010.

[Dwo06]

C. Dwork. Diﬀerential privacy. In ICALP, pages 1–12, 2006.

[Kah60]

J. Kahane. Proprits locales des fonctions sries de fourier alatoires. Studia Mathematica,
19(1):1–25, 1960.

[KOV15]

Peter Kairouz, Sewoong Oh, and Pramod Viswanath. The composition theorem for
diﬀerential privacy. In ICML, pages 1376–1385, 2015.

[MV16]

[Riv12]

Jack Murtagh and Salil P. Vadhan. The complexity of computing the optimal compo-
sition of diﬀerential privacy. In TCC(A), pages 157–175, 2016.

Omar Rivasplata. Subgaussian random variables: An expository note. Unpublished
note, 2012.

28

