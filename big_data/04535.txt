SUBMITTED TO IEEE TRANSACTIONS ON CYBERNETICS

1

Domain Adaptation via

Maximum Independence of Domain Features

Ke Yan, Lu Kou, and David Zhang, Fellow, IEEE

6
1
0
2

 
r
a

 

M
5
1

 
 
]

V
C
.
s
c
[
 
 

1
v
5
3
5
4
0

.

3
0
6
1
:
v
i
X
r
a

Abstract—When the distributions of the source and the target
domains are different, domain adaptation techniques are needed.
For example, in the ﬁeld of sensors and measurement, discrete
and continuous distributional change often exist in data because
of instrumental variation and time-varying sensor drift. In this
paper, we propose maximum independence domain adaptation
(MIDA) to address this problem. Domain features are ﬁrst deﬁned
to describe the background information of a sample, such as the
device label and acquisition time. Then, MIDA learns features
which have maximal independence with the domain features,
so as to reduce the inter-domain discrepancy in distributions. A
feature augmentation strategy is designed so that the learned pro-
jection is background-speciﬁc. Semi-supervised MIDA (SMIDA)
extends MIDA by exploiting the label information. The proposed
methods can handle not only discrete domains in traditional
domain adaptation problems but also continuous distributional
change such as the time-varying drift. In addition, they are
naturally applicable in supervised/semi-supervised/unsupervised
classiﬁcation or regression problems with multiple domains. This
ﬂexibility brings potential for a wide range of applications.
The effectiveness of our approaches is veriﬁed by experiments
on synthetic datasets and four real-world ones on sensors,
measurement, and computer vision.

Index Terms—Dimensionality reduction, domain adaptation,
drift correction, Hilbert-Schmidt independence criterion, ma-
chine olfaction, transfer learning

I. INTRODUCTION

D OMAIN adaptation techniques are useful when the la-

beled training data are from a source domain and the
test ones are from a target domain. Samples of the two
domains are collected under different conditions, thus have
different distributions. Labeling samples in the target domain
to develop new prediction models is often labor-intensive
and time-consuming. Therefore, domain adaptation or transfer
learning is needed to improve the performance in the target
domain by leveraging unlabeled (and maybe a few labeled)
target samples [1]. This topic is receiving increasing attention
in recent years due to its broad applications such as computer
vision [2], [3], [4] and text classiﬁcation [5], [6]. It is also

The work is partially supported by the GRF fund from the HKSAR Gov-
ernment, the central fund from Hong Kong Polytechnic University, the NSFC
fund (61332011, 61272292, 61271344), Shenzhen Fundamental Research fund
(JCYJ20150403161923528, JCYJ20140508160910917), and Key Laboratory
of Network Oriented Intelligent Computation, Shenzhen, China.

K. Yan is with the Department of Electronic Engineering, Graduate
School at Shenzhen, Tsinghua University, Shenzhen 518055, China (e-mail:
yank10@mails.tsinghua.edu.cn).

L. Kou is with the Department of Computing, The Hong Kong Polytechnic

University, Kowloon, Hong Kong (e-mail: cslkou@comp.polyu.edu.hk).

D. Zhang is with the Shenzhen Graduate School, Harbin Institute of
Technology, Shenzhen 518055, China, and also with the Department of Com-
puting, Biometrics Research Centre, The Hong Kong Polytechnic University,
Kowloon, Hong Kong (e-mail: csdzhang@comp.polyu.edu.hk).

important in the ﬁeld of sensors and measurement. Because
of the variations in the fabrication of sensors and devices, the
responses to the same signal source may not be identical for
different instruments. Furthermore, the sensing characteristics
of the sensors, the operating condition, or even the signal
source itself, can change over time. As a result, the prediction
model trained with the samples from the initial device in an
earlier time period (source domain) is not suitable for new
devices or in a latter time (target domains).

A typical application plagued by this problem is machine
olfaction, which uses electronic noses (e-noses) and pattern
recognition algorithms to predict the type and concentration of
odors [7]. The applications of machine olfaction range from
agriculture and food to environmental monitoring, robotics,
biometrics, and disease analysis [8], [9], [10], [11]. However,
owing to the nature of chemical sensors, many e-noses are
prone to instrumental variation and time-varying drift men-
tioned above [12], [13], which greatly hamper their usage
in real-world applications. Traditional methods for calibra-
tion transfer (compensating instrumental variation) and drift
correction (compensating time-varying drift) require a set of
predeﬁned gas samples as transfer samples. They should be
collected with each device and in each time period so as to
provide mapping information between the source and the target
domains [12], [10], [14], [15]. Then, a widely-used method
is to map the features in the target domain to the source
domain with regression algorithms [10], [14]. Nevertheless,
collecting transfer samples is demanding for non-professional
e-nose users because standard gases need to be prepared and
much effort has to be made.

In such cases, domain adaptation techniques with unlabeled
target samples are desirable. An intuitive idea is to reduce the
difference of distributions among domains in the feature level,
i.e. to learn domain-invariant feature representation [5], [16],
[17], [3], [18], [19]. For example, Pan et al. [5] proposed trans-
fer component analysis (TCA), which ﬁnds a latent feature
space that minimizes the difference of distributions between
two domains in the sense of maximum mean discrepancy.
More related methods will be introduced in Section II. When
applied to calibration transfer and drift correction, however,
existing domain adaptation algorithms are faced with two
difﬁculties. First, they are designed to handle discrete source
and target domains. In time-varying drift, samples come in a
stream, so the change in data distribution is often continuous.
One solution is to split data into several batches, but
it
will
the temporal order information. Second, because
of the variation in the sensitivity of chemical sensors, the
in different conditions may indicate different
same signal

lost

SUBMITTED TO IEEE TRANSACTIONS ON CYBERNETICS

2

concepts. In other words, the conditional probability P (Y |X)
may change for samples with different backgrounds, where
“background” means when and with which device a sample
was collected. Methods like TCA project all samples to a
common subspace, hence the samples with similar appearance
but different concepts cannot be distinguished.

In this paper, we present a simple yet effective algorithm
called maximum independence domain adaptation (MIDA).
The algorithm ﬁrst deﬁnes “domain features” for each sample
to describe its background. Then, it ﬁnds a latent feature space
in which the samples and their domain features are maximally
independent in the sense of Hilbert-Schmidt independence cri-
terion (HSIC) [20]. Thus, the discrete and continuous change
in distribution can be handled uniformly. In order to project
samples according to their backgrounds, feature augmentation
is performed by concatenating the original feature vector with
the domain features. We also propose semi-supervised MIDA
(SMIDA) to exploit the label information with HSIC. MIDA
and SMIDA are both very ﬂexible. (1) They can be applied in
situations with single or multiple source or target domains
thanks to the use of domain features. In fact,
the notion
“domain” has been extended to “background” which can carry
more information about the background of a sample. (2) Al-
though they are designed for unsupervised domain adaptation
problems (no labeled sample in target domains), the proposed
methods naturally allow both unlabeled and labeled samples
in any domains, thus can be applied in semi-supervised (both
unlabeled and labeled samples in target domains) and super-
vised (only labeled samples in target domains) problems as
well. (3) The label information can be either discrete (binary-
or multi-class classiﬁcation) or continuous (regression). This
advantage is inherited from HSIC.

To illustrate the effect of our algorithms, we ﬁrst evaluate
them on several synthetic datasets. Then, calibration transfer
and drift correction experiments are performed on two e-nose
datasets and one spectroscopy dataset. Note that spectrometers
suffer the same instrumental variation problem as e-noses
[21]. Finally, a domain adaptation experiment is conducted
on a well-known object recognition benchmark [22]. Results
conﬁrm the effectiveness of the proposed algorithms. The rest
of the paper is organized as follows. Related work on unsu-
pervised domain adaptation and HSIC is brieﬂy reviewed in
Section II. Section III describes domain features, MIDA, and
SMIDA in detail. The experimental conﬁgurations and results
are presented in Section IV, along with some discussions.
Section V concludes the paper.

II. RELATED WORK

A. Unsupervised domain adaptation

Two good surveys on domain adaptation can be found in
[1] and [2]. In this section, we focus on typical methods
that extract domain-invariant features. Compared to model-
level adaptation methods, feature-level ones are easier to
use because the extracted features can be applied to various
prediction models. In order to reduce the difference of distri-
butions among domains while preserving useful information,
researchers have developed many strategies. Some algorithms

project all samples to a common latent space [5], [16], [19].
Transfer component analysis (TCA) [5] tries to learn transfer
components across domains in a reproducing kernel Hilbert
space (RKHS) using maximum mean discrepancy. It is further
extended to semi-supervised TCA (SSTCA) to encode label
information and preserve local geometry of the manifold.
Shi et al. [16] measured domain difference by the mutual
information between all samples and their binary domain
labels, which can be viewed as a primitive version of the
domain features used in this paper. They also minimized the
negated mutual information between the target samples and
their cluster labels to reduce the expected classiﬁcation error.
The low-rank transfer subspace learning (LTSL) algorithm
presented in [19] is a reconstruction guided knowledge transfer
method. It aligns source and target data by representing each
target sample with some local combination of source samples
in the projected subspace. The label and geometry information
can be retained by embedding different subspace learning
methods into LTSL.

Another class of methods ﬁrst project the source and the
target data into separate subspaces, and then build connections
between them [17], [22], [23], [3]. Fernando et al. [17] utilized
a transformation matrix to map the source subspace to the
target one, where a subspace was represented by eigenvectors
of PCA. The geodesic ﬂow kernel (GFK) method [22] mea-
sures the geometric distance between two different domains
in a Grassmann manifold by constructing a geodesic ﬂow. An
inﬁnite number of subspaces are combined along the ﬂow in
order to model a smooth change from the source to the target
domain. Liu et al. [23] adapted GFK for drift correction of e-
noses. A sample stream is ﬁrst split into batches according to
the acquisition time. The ﬁrst and the last batches (domains)
are then connected through every intermediate batch using
GFK. Another improvement of GFK is domain adaptation
by shifting covariance (DASC) [3]. Observing that modeling
one domain as a subspace is not sufﬁcient to represent the
difference of distributions, DASC characterizes domains as
covariance matrices and interpolates them along the geodesic
to bridge the domains.

B. Hilbert-Schmidt independence criterion (HSIC)

HSIC is used as a convenient method to measure the
dependence between two sample sets X and Y . Let kx and
ky be two kernel functions associated with RKHSs F and G,
respectively. pxy is the joint distribution. HSIC is deﬁned as
the square of the Hilbert-Schmidt norm of the cross-covariance
operator Cxy [20]:
HSIC(pxy,F,G) = (cid:107)Cxy(cid:107)2
=Exx(cid:48)yy(cid:48)[kx(x, x(cid:48))ky(y, y(cid:48))] + Exx(cid:48)[kx(x, x(cid:48))]Eyy(cid:48)[ky(y, y(cid:48))]

HS

− 2Exy[Ex(cid:48)[kx(x, x(cid:48))]Ey(cid:48)[ky(y, y(cid:48))]].

Here Exx(cid:48)yy(cid:48) is the expectation over independent pairs (x, y)
and (x(cid:48), y(cid:48)) drawn from pxy. It can be proved that with
characteristic kernels kx and ky, HSIC(pxy,F,G) is zero if
and only if x and y are independent [24]. A large HSIC
suggests strong dependence with respect
to the choice of
kernels. HSIC has a biased empirical estimate. Suppose Z =

SUBMITTED TO IEEE TRANSACTIONS ON CYBERNETICS

3

where H = I − n−11n1T

X × Y = {(x1, y1), . . . , (xn, yn)}, Kx, Ky ∈ Rn×n are the
kernel matrices of X and Y , respectively, then [20]:
HSIC(Z,F,G) = (n − 1)−2tr(KxHKyH),

(1)
n ∈ Rn×n is the centering matrix.
Due to its simplicity and power, HSIC has been adopted
for feature extraction [25], [5], [26] and feature selection
[24]. Researchers typically use it to maximize the dependence
between the extracted/selected features and the label. However,
to our knowledge, it has not been utilized in domain adaptation
to reduce the dependence between the extracted features and
the domain features.

III. PROPOSED METHOD

A. Domain Feature

We aim to reduce the dependence between the extracted
features and the background information. A sample’s back-
ground information should (1) naturally exists, thus can be
easily obtained; (2) has different distributions in training and
test samples; (3) correlates with the distribution of the original
features. The domain label (which domain a sample belongs)
in common domain adaptation problems is an example of such
information. According to these characteristics, the informa-
tion clearly interferes the testing performance of a prediction
model. Thus, minimizing the aforementioned dependence is
desirable. First, a group of new features need to be designed
to describe the background information. The features are called
“domain features”. From the perspective of calibration transfer
and drift correction, there are two main types of background
information: the device label (with which device the sample
was collected) and the acquisition time (when the sample was
collected). We can actually encode more information such as
the place of collection, the operation condition, and so on,
which will be useful in other domain adaptation problems.

Formally, if we only consider the instrumental variation, an
one-hot coding scheme can be used. Suppose there are ndev
devices, which result in ndev different but related domains.
The domain feature vector is thus d ∈ Rndev, where dp = 1
if the sample is from the pth device and 0 otherwise. This
scheme also applies to traditional domain adaptation problems
with several discrete domains. If the time-varying drift is also
considered, the acquisition time can be further added. If a
sample was collected from the pth device at time t, then d ∈
R2ndev, where

1,

q = 2p − 1,
q = 2p,

t,
0, otherwise.

dq =

(2)

According to (1),

the kernel matrix Kd of the domain
features needs to be computed for HSIC. We apply the linear
kernel. Suppose D = [d1, . . . , dn] ∈ Rmd×n, md is the
dimension of a domain feature vector. Then

Kd = DTD.

(3)
(Kd)ij is 0 if samples i and j are from different devices;
otherwise, it is 1 when time-varying drift is not considered, or
1 + titj when it is considered.

B. Feature Augmentation

Feature augmentation is used in this paper

to learn
background-speciﬁc subspaces. In [27], the author proposed
a feature augmentation strategy for domain adaptation: if a
sample x ∈ Rm is from the source domain, then its augmented

 x
 ∈ R3m; If it is from the target
 x
 ∈ R3m. The augmented labeled

x
0m

feature vector is ˆx =

domain, then ˆx =

source and target samples are then used jointly to train one
prediction model. In this way, the learned model can be viewed
as two different models for the two domains. Meanwhile, the
two models share a common component [27]. However, this
strategy requires that data lie in discrete domains and cannot
deal with time-varying drift. We propose a more general
and efﬁcient feature augmentation strategy: concatenating the
original features and the domain features, i.e.

0m
x

(cid:20)x

(cid:21)

d

ˆx =

∈ Rm+md .

(4)

Wd

(cid:21)

x x + W T

(cid:20)Wx

The role of this strategy can be demonstrated through a
linear dimensionality reduction example. Suppose a projection
matrix W ∈ R(m+md)×h has been learned for the augmented
feature vector. h is the dimension of the subspace. W has two
, Wx ∈ Rm×h, Wd ∈ Rmd×h. The embed-
parts: W =
d d ∈ Rh,
ding of ˆx can be expressed as W T ˆx = W T
which means that a background-speciﬁc bias (W T
d d)i has been
added to each dimension i of the embedding. One may argue
that a bias may not always be enough for alignment. It may be
better to have domain-speciﬁc afﬁne transformation matrices.
However, with the absence of target labels, learning such
matrices can be prone to overﬁtting. From another perspective,
the feature augmentation strategy maps the samples to an
augmented space with higher dimension before projecting
them to a subspace. It will be easier to ﬁnd a projection
direction in the augmented space to align the samples well
in the subspace. The effect of feature augmentation will be
illustrated on several synthetic datasets in Section IV-A.
Take machine olfaction for example, there are situations
when the conditional probability P (Y |X) changes along with
the background. For instance,
the sensitivity of chemical
sensors often decays over time. A signal that indicates low
concentration in an earlier time actually suggests high con-
centration in a later time. In such cases, feature augmentation
is important, because it allows samples with similar appear-
ance but different concepts to be treated differently by the
background-speciﬁc bias. The strategy also helps to align the
domains better in each projected dimension.

C. Maximum Independence Domain Adaptation (MIDA)

In this section, we introduce the formulation of MIDA in
detail. Suppose X ∈ Rm×n is the matrix of n samples.
The training and the test samples are pooled together. More
importantly, we do not have to explicitly differentiate which
domain a sample is from. The feature vectors has been

SUBMITTED TO IEEE TRANSACTIONS ON CYBERNETICS

4

augmented, but we use the notations X and m instead of
ˆX and m + md for brevity. A linear or nonlinear mapping
function Φ can be used to map X to a new space. Based on
the kernel trick, we need not know the exact form of Φ, but
the inner product of Φ(X) can be represented by the kernel
matrix Kx = Φ(X)TΦ(X). Then, a projection matrix ˜W is
applied to project Φ(X) to a subspace with dimension h,
leading to the projected samples Z = ˜W TΦ(X) ∈ Rh×n.
Similar to other kernel dimensionality reduction algorithms
[28], [29], the key idea is to express each projection direction
as a linear combination of all samples in the space, namely
˜W = Φ(X)W . W ∈ Rn×h is the projection matrix to be
actually learned. Thus, the projected samples are

Z = W TΦ(X)TΦ(X) = W TKx

with the kernel matrix

Kz = KxW W TKx.

(5)

(6)

In domain adaptation,

Intuitively, if the projected features are independent of the
domain features, then we cannot distinguish the background
of a sample by its projected features, suggesting that
the
difference of distributions among domains is diminished in
the subspace. Therefore, by substituting (6) into the empirical
HSIC (1) and omit the scaling factor, we get the expression
to be minimized: tr(KzHKdH) = tr(KxW W TKxHKdH).
is not only minimizing
the difference of distributions, but also preserving important
properties of data, such as the variance [5]. It can be achieved
by maximizing the trace of the covariance matrix of the project
samples. The covariance matrix is
cov(Z) = cov(W TKx)
(W TKx − 1
n
= W TKxHKxW,

n )(W TKx − 1
n

W TKx1n1T

the goal

1
n

=

where H = I − n−11n1T
n is the same as that in (1). An
orthonormal constraint is further added on W . The learning
problem then becomes

max

− tr(W TKxHKdHKxW ) + µ tr(W TKxHKxW ),

W
s.t. W TW = I,

(8)
where µ > 0 is a trade-off hyper-parameter. To solve (8), we
can use its Lagrangian:
tr(W TKx(−HKdH + µH)KxW ) − tr((W TW − I)Λ), (9)
where Λ is a matrix containing the Lagrange multipliers.
Setting the derivative of (9) with respect to W to zero, we
get

Kx(−HKdH + µH)KxW = W Λ.

(10)
Consequently, W is the eigenvectors of Kx(−HKdH +
µH)Kx corresponding to the h largest eigenvalues. Note that
a conventional constraint is requiring ˜W to be orthonormal as
in [26], which will lead to a generalized eigenvector problem.
However, we ﬁnd that this strategy is inferior to the proposed

one in both adaptation accuracy and training speed in practice,
so it is not used.

2σ2

(cid:107)x−y(cid:107)2

When computing Kx, a proper kernel function needs to be
selected. Common kernel functions include linear (k(x, y) =
xyT), polynomial (k(x, y) = (σxyT + 1)d), Gaussian radial
basis function (RBF, k(x, y) = exp(
)), and so on.
Different kernels indicate different assumptions on the type
of dependence in using HSIC [24]. According to [24], the
polynomial and RBF kernels map the original features to a
higher or inﬁnite dimensional space, thus are able to detect
more types of dependence. However, choosing a suitable
kernel width parameter (σ) is also important for these more
powerful kernels [24]. When looking for a linear projection
matrix to align the domains, ﬁrst mapping the original features
to a higher dimensional space may also be helpful, see the
experiment on a synthetic dataset (Section IV-A, Fig. 4) for
an example.

The maximum mean discrepancy (MMD) criterion is used
in TCA [5] to measure the difference between two dis-
tributions. Song et al. [24] showed that when HSIC and
MMD are both applied to measure the dependence of features
and labels in a binary-class classiﬁcation problem, they are
identical up to a constant factor if the label kernel matrix in
HSIC is properly designed. However, TCA is feasible only
when there are two discrete domains. On the other hand,
MIDA can deal with a variety of situations including multiple
domains and continuous distributional change. The stationary
subspace analysis (SSA) algorithm [30] is able to identify
temporally stationary components in multivariate time series.
However, SSA only ensures that the mean and covariance
of the components are stationary, while they may not be
suitable for preserving important properties in data. Concept
drift adaptation algorithms [31] are able to correct continuous
time-varying drift. However, most of them rely on newly
arrived labeled data to update the prediction models, while
MIDA works unsupervisedly.

D. Semi-supervised MIDA (SMIDA)

MIDA aligns samples with different backgrounds without
considering the label information. However, if the labels of
some samples are known,
they can be incorporated into
the subspace learning process, which may be beneﬁcial for
prediction. Therefore, we extend MIDA to semi-supervised
MIDA (SMIDA). Since we do not explicitly differentiate the
domain labels of the samples, both unlabeled and labeled
samples can exist in any domain. Similar to [25], [5], [26],
[24], HSIC is adopted to maximize the dependence between
the projected features and the labels. The biggest advantage of
this strategy is that all types of labels can be exploited, such
as the discrete labels in classiﬁcation and the continuous ones
in regression.

The label matrix Y is deﬁned as follows. For c-class
classiﬁcation problems, the one-hot coding scheme can be
used, i.e. Y ∈ Rn×c, yi,j = 1 if xi is labeled and belongs
to the jth class; 0 otherwise. For regression problems, the
target values can be centered ﬁrst. Then, Y ∈ Rn, yi equals

W TKx1n1T

n )T

(7)

SUBMITTED TO IEEE TRANSACTIONS ON CYBERNETICS

5

to the target value of xi if it is labeled; 0 otherwise. The linear
kernel function is chosen for the label kernel matrix, i.e.

Ky = Y Y T.

The objective of SMIDA is

max

tr(W TKx(−HKdH + µH + γHKyH)KxW ),

W
s.t. W TW = I,

(11)

(12)

where γ > 0 is a trade-off hyper-parameter. Its solution
is the eigenvectors of Kx(−HKdH + µH + γHKyH)Kx
corresponding to the h largest eigenvalues. The outline of
MIDA and SMIDA is summarized in Algorithm III.1. The
statements in brackets correspond to those specialized for
SMIDA.

Algorithm III.1 MIDA [or SMIDA]
Input: The matrix of all samples X and their background
the kernel

information; [the labels of some samples];
function for X; h, µ, [and γ].

Output: The projected samples Z.
1: Construct

the domain features according to the back-

ground information, e.g. Section III-A.

2: Augment the original features with the domain features

(4).

3: Compute the kernel matrices Kx, Kd (3), [and Ky (11)].
4: Obtain W , namely the eigenvectors of Kx(−HKdH +
µH)Kx [or Kx(−HKdH + µH + γHKyH)Kx] corre-
sponding to the h largest eigenvalues.

5: Z = W TKx.

Besides variance and label dependence, another useful
property of data is the geometry structure, which can be
preserved by manifold regularization (MR) [32]. The man-
ifold structure is modeled by a data adjacency graph. MR
can be conveniently incorporated into SMIDA by adding a
regularizer −λ tr(W TKxLKxW ) into (12), where L is the
graph Laplacian matrix [32], λ > 0 is a trade-off hyper-
parameter. In our experiments, adding MR generally increases
the accuracy slightly. However, It also brings three more hyper-
parameters, including λ, the number of nearest neighbors, and
the kernel width when computing the data adjacency graph.
Consequently, the experimental results in the next section were
obtained without MR. It can still be an option in applications
where geometry structure is important.

IV. EXPERIMENTS

In this section, we ﬁrst conduct experiments on some
synthetic datasets to verify the effect of the proposed methods.
Then, calibration transfer and drift correction experiments
are performed on two e-nose datasets and a spectroscopy
dataset. To show the universality of the proposed methods, we
further evaluate them on a visual object recognition dataset.
Comparison is made between them and recent unsupervised
feature-level domain adaptation algorithms.

A. Synthetic Dataset

In Fig. 1, TCA [5] and MIDA are compared on a 2D dataset
with two discrete domains. For both methods, the linear kernel
was used on the original features and the hyper-parameter µ
was set to 1. In order to quantitatively assessing the effect
of domain adaptation, logistic regression models were trained
on the labeled source data and tested on the target data. The
accuracies are displayed in the caption, showing that the order
of performance is MIDA > TCA > original feature. TCA
aligns the two domains only on the ﬁrst projected dimension.
However, we can ﬁnd that the two classes have large overlap
on that dimension. This is because the direction for alignment
is different from that for discrimination. Incorporating the label
information of the source domain (SSTCA) did no help. On
the contrary, MIDA can align the two domains well in both
projected dimensions, in which the domain-speciﬁc bias on
the second dimension brought by feature augmentation played
a key role. Thus, good accuracy can be obtained by using the
two dimensions for classiﬁcation.

In Fig. 2, SSA [30] and MIDA are compared on a 2D
dataset with continuous distributional change, which resembles
time-varying drift in machine olfaction. The chronological
order of a sample is indicated by color ranging from blue
to red. Samples in both classes drift to the upper right. The
parameter setting of MIDA was the same with those in Fig. 1,
whereas the number of stationary components in SSA was set
to 1. The classiﬁcation accuracies were obtained by training
a logistic regression model on the ﬁrst halves of the data
in both classes, and testing them on the last halves. SSA
succeeds in ﬁnding a direction (z1) that is free from time-
varying drift (z2). However, the two classes cannot be well
separated in that direction. In plot (c), the randomly scattered
colors suggest that the time-varying drift is totally removed in
the subspace. MIDA ﬁrst mapped the 2D data into a 3D space
with the third dimension being time. Then, the augmented
data were projected to a 2D plane that is orthogonal to the
direction of drift in the 3D space. The projection direction
was decided so that the independence of the projected data and
time is maximized, meanwhile class separation was achieved
by properly exploiting the background information.

No label information was used in the last two experiments.
If keeping the label dependence in the subspace is a priority,
SMIDA can be adopted instead of MIDA. In the 3D synthetic
dataset in Fig. 3, the best direction to align the two domains
(x3) also mixes the two classes, which results in the output
of MIDA in plot (b). For SMIDA, the weights for variance
(µ) and label dependence (γ) were both set to 1. The labels
in the source domain were used when learning the subspace.
From plot (c), we can observe that the classes are separated. In
fact, class separation can still be found in the third dimension
of the space learned by MIDA. However, for the purpose
of dimensionality reduction, we generally hope to keep the
important information in the ﬁrst few dimensions.

Nonlinear kernels are often applied in machine learning
algorithms when data is not linearly separable. Besides, they
are also useful in domain adaptation when domains are not
linearly “alignable”, as shown in Fig. 4. As can be found in

SUBMITTED TO IEEE TRANSACTIONS ON CYBERNETICS

6

Fig. 1. Comparison of TCA and MIDA in a 2D synthetic dataset. Plots (a)-(c) show data in the original space and projected spaces of TCA and MIDA,
respectively. The classiﬁcation accuracies are 53%, 70% (only using the ﬁrst projected dimension z1), and 88%.

Fig. 2. Comparison of SSA and MIDA in a 2D synthetic dataset. Plots (a)-(c) show data in the original space, projected spaces of SSA and MIDA, respectively.
The chronological order of a sample is indicated by color. The classiﬁcation accuracies are 55%, 74% (only using the ﬁrst projected dimension z1), and 90%.

Fig. 3. Comparison of MIDA and SMIDA in a 3D synthetic dataset. Plots (a)-(c) show data in the original space and projected spaces of MIDA and SMIDA,
respectively. The classiﬁcation accuracies are 50%, 55%, and 82%.

plot (a), the inter-domain changes in distributions are different
for the two classes. Hence, it is difﬁcult to ﬁnd a linear
projection direction to align the two domains, even with the
domain-speciﬁc biases of MIDA. Actually, domain-speciﬁc
rotation matrices are needed. Since the target labels are not
available, the rotation matrices cannot be obtained accurately.
However, a nonlinear kernel can be used to map the original
features to a space with higher dimensions,
in which the
domains may be linearly alignable. We applied an RBF kernel
with width σ = 10. Although the domains are not perfectly
aligned in plot (c), the classiﬁcation model trained in the
source domain can be better adapted to the target domain.

B. Gas Sensor Array Drift Dataset

The gas sensor array drift dataset1 collected by Vergara et
al. [33] is dedicated to research in drift correction. A total of
13910 samples were collected by an e-nose with 16 gas sensors
over a course of 36 months. There are six different kinds of
gases (ammonia, acetaldehyde, acetone, ethylene, ethanol, and
toluene) at different concentrations. They were split into 10
batches by the authors according to their acquisition time.
Table I shows the period of collection and the number of
samples of different type of gases in each batch. We aim to
classify the type of gases, despite their concentrations.

1 http://archive.ics.uci.edu/ml/datasets/Gas+Sensor+Array+Drift+Dataset+at+Different+Concentrations

−2−1012−2−1.5−1−0.500.511.52x1x2(a)  Pos. source dataNeg. source dataPos. target dataNeg. target data−10−50510−20−15−10−5051015z1z2(b)−10−50510−2−1.5−1−0.500.511.52z1z2(c)−20246−20246810121416x1x2(a)  Pos. old dataNeg. old dataPos. new dataNeg. new data−4−2024−20246810z1z2(b)−80−60−40−2002040−20−15−10−505101520z1z2(c)−1−0.500.511.5−202−2−10123 x1(a) x2x3Pos. source dataNeg. source dataPos. target dataNeg. target data−10−505−3−2−10123z1z2(b)−4−20246−3−2−1012345z1z2(c)SUBMITTED TO IEEE TRANSACTIONS ON CYBERNETICS

7

Fig. 4. Comparison of different kernels in a 2D synthetic dataset. Plots (a)-(c) show data in the original space and projected spaces of MIDA with linear and
RBF kernels, respectively. The classiﬁcation accuracies are 50%, 57%, and 87%.

PERIOD OF COLLECTION AND NUMBER OF SAMPLES IN THE GAS SENSOR ARRAY DRIFT DATASET [33].

TABLE I

Batch ID Month
1
2
3
4
5
6
7
8
9
10

1, 2
3, 4, 8-10
11-13
14,15
16
17-20
21
22, 23
24, 30
36

Ammonia
83
100
216
12
20
110
360
40
100
600

Acetaldehyde
30
109
240
30
46
29
744
33
75
600

Acetone
70
532
275
12
63
606
630
143
78
600

Ethylene
98
334
490
43
40
574
662
30
55
600

Ethanol
90
164
365
64
28
514
649
30
61
600

Toluene
74
5
0
0
0
467
568
18
101
600

# Total
445
1244
1586
161
197
2300
3613
294
470
3600

A strategy used in previous literatures [33], [23] was also
adopted in this paper so that the performance of the domain
adaptation algorithms can be evaluated. We took the samples
in batch 1 as labeled training samples, whereas those in
batches 2–10 are unlabeled test ones. This evaluation strategy
also resembles the situation in real-world applications. In the
dataset, each sample is represented by 128 features extracted
from the sensors’ response curves [33]. The original features
have quite different dynamic ranges, which will interfere the
learning process. Therefore, each feature was ﬁrst normalized
to have zero mean and unit variance within each batch. The
time-varying drift of the preprocessed features across batches
can be visually inspected in Fig. 5. It is obvious that samples in
different batches have different distributions. Next, the labeled
samples in batch 1 were adopted as the source domain and
the unlabeled ones in batch b (b = 2, . . . , 10) as the target
domain. The proposed algorithms together with several recent
ones were used to learn domain-invariant features based on
these samples. Then, a logistic regression model was trained
on the source domain and tested on each target one. For multi-
class classiﬁcation, the one-vs-all strategy was utilized.

As displayed in Table II, the compared methods include
kernel PCA (KPCA),
transfer component analysis (TCA),
semi-supervised TCA (SSTCA) [5], subspace alignment (SA)
[17], geodesic ﬂow kernel (GFK) [22], manifold regularization
with combination GFK (ML-comGFK) [23], and information-
theoretical learning (ITL) [16]. For all methods, the hyper-
parameters were tuned for the best accuracy. In KPCA, TCA,
SSTCA, and the proposed MIDA and SMIDA, the polynomial
kernel with degree 2 was used. KPCA learned a subspace

Fig. 5. Scatter of ethanol (dots) and acetone (plus signs) samples in batches
1,3,5,7,9 in the gas sensor array drift dataset. Samples are projected to a 2D
subspace using PCA. Different colors indicate different batches.

based on the union of source and target data. In TCA, SSTCA,
MIDA, and SMIDA, eigenvalue decomposition needs to be
done on kernel matrices. If the number of samples is too
large, this step can be time-consuming. In order to reduce the
computational burden, we randomly chose at most nt samples
in each target domain when using these methods, with nt being
twice the number of the samples in the source domain. GFK

−4−20246−3−2−101234x1x2(a)  Pos. source dataNeg. source dataPos. target dataNeg. target data−60−40−200204060−4−3−2−1012345z1z2(b)99.51010.511.11.21.31.41.51.61.71.81.9z1z2(c)−30−20−1001020−15−10−505PC1PC2  Batch 1Batch 3Batch 5Batch 7Batch 9SUBMITTED TO IEEE TRANSACTIONS ON CYBERNETICS

8

used PCA to generate the subspaces in both source and target
domains. The subspace dimension of GFK was determined
according to the subspace disagreement measure in [22]. The
results of ML-comGFK are copied from [23].

We also compared several variants of our methods. In Table
II, the notation “(discrete)” means that two discrete domains
(source and target) were used in MIDA and SMIDA, which is
similar to other compared methods. The domain feature vector
of a sample was thus [1, 0]T if it is from the source domain and
[0, 1]T if it is from the target. However, this strategy cannot
make use of the samples in intermediate batches. An intuitive
assumption is that the distributions of adjacent batches should
be similar. When adapting the information from batch 1 to
b, taking samples from batches 2 to b − 1 into consideration
may improve the generalization ability of the learned subspace.
Concretely, nt samples were randomly selected from batches
2 to b instead of batch b alone. For each sample, the domain
feature was deﬁned as its batch index, which can be viewed
as a proxy of its acquisition time. MIDA and SMIDA then
maximized the independence between the learned subspace
and the batch indices. The results are labeled as “(continuous)”
in Table II. Besides, the accuracies of continuous SMIDA
without feature augmentation (no aug.) are also shown.

From Table II, we can ﬁnd that as the batch index in-
creases, the accuracies of all methods generally degrade, which
conﬁrms the inﬂuence of the time-varying drift. Continuous
SMIDA achieves the best average domain adaptation accuracy.
The continuous versions of MIDA and SMIDA outperform the
discrete versions, proving that the proposed methods can ef-
fectively exploit the chronological information of the samples.
They also surpass ML-comGFK which uses the samples in
intermediate batches to build connections between the source
and the target batches. Feature augmentation is important in
this dataset, since removing it in continuous SMIDA causes
a drop of four percentage points in average accuracy. In Fig.
6, the average classiﬁcation accuracies with varying subspace
dimension are shown. MIDA and SMIDA are better than other
methods when more than 30 features are extracted.

C. Breath Analysis Dataset

As a noninvasive approach, disease screening and moni-
toring with e-noses is attracting more and more attention [8],
[11]. The concentration of some biomarkers in breath has been
proved to be related with certain diseases, which makes it
possible to analyze a person’s health state with an e-nose
conveniently. For example, the concentration of acetone in
diabetics’ breath is often higher than that in healthy people
[11]. However, the instrumental variation and time-varying
drift of e-noses hinder the popularization of this technology
in real-world applications. Most traditional calibration transfer
and drift correction algorithms require transfer samples to be
collected in each new device and time period, which is a
demanding job, especially for non-professional e-nose users.
When the devices are used for breath sample collection in
communities for disease screening, breath samples are easy to
acquire, but not their labels. Therefore, unsupervised domain
adaptation algorithms are necessary.

Fig. 6. Performance comparison on the gas sensor array drift dataset with
respect to the subspace dimension h.

We have collected a breath analysis dataset in years 2014–
2015 using two e-noses of the same model [11]. In this
paper, samples of ﬁve diseases were selected for experiments,
including diabetes, chronical kidney disease (CKD), cardiopa-
thy, lung cancer, and breast cancer. They have been proved
to be related with certain breath biomarkers. We performed
ﬁve binary-class classiﬁcation tasks to distinguish samples
with one disease from the healthy samples. Each sample was
represented by the steady state responses of nine gas sensors in
the e-nose. When a gas sensor is used to sense a gas sample, its
response will reach a steady state in a few minutes. The steady
state response has a close relationship with the concentration
of the measured gas. Therefore, the 9D feature vector contains
most information needed for disease screening.

To show the instrumental variation and time-varying drift in
the dataset, we draw the steady state responses of two sensors
of the CKD samples in Fig. 7. Each data point indicates a
breath sample. In plot (a), the sensitivity of the sensor in
both devices gradually decayed as time elapsed. In plot (b),
the aging effect was so signiﬁcant that we had to replace the
sensors in the two devices with new ones on about day 200. In
this case, a signal at 0.3 V will suggest low concentration on
day 0 and 250 but high concentration in day 150 and 450. In
addition, the responses in different devices are different (e.g.
plot (b), after day 200).

The numbers of samples in the six classes (healthy and the
ﬁve diseases mentioned above) are 125, 431, 340, 97, 156,
and 215, respectively. We chose the ﬁrst 50 samples collected
with device 1 in each class as labeled training samples. Among
the other samples, 10 samples were randomly selected in each
class for validation, the rest for testing. The hyper-parameters
were tuned in the validation sets. Logistic regression was
adopted as the classiﬁer. Because of the difference in the
number of samples in each class, the F-score was used as
the accuracy criterion. Results are compared in Table III.

020406080100505560657075# Projected dimensions (h)Average classification accuracy (%)  KPCATCASSTCASAITLMIDA (continuous)SMIDA (continuous)SUBMITTED TO IEEE TRANSACTIONS ON CYBERNETICS

9

CLASSIFICATION ACCURACY (%) ON THE GAS SENSOR ARRAY DRIFT DATASET. BOLD VALUES INDICATE THE BEST RESULTS.

TABLE II

Original feature
KPCA
TCA [5]
SSTCA [5]
SA [17]
GFK [22]
ML-comGFK [23]
ITL [16]
MIDA (discrete)
SMIDA (discrete)
MIDA (continuous)
SMIDA (no aug.)
SMIDA (continuous)

Batch 2
80.47
75.88
82.96
84.57
80.79
77.41
80.25
76.85
81.03
80.47
84.32
82.23
83.68

3
79.26
69.04
81.97
80.90
80.01
80.26
74.99
79.45
85.62
87.07
81.59
83.17
82.28

4
69.57
49.07
65.22
80.12
71.43
71.43
78.79
59.63
60.25
65.22
68.32
67.70
73.91

5
77.16
57.87
76.14
75.63
75.63
76.14
67.41
96.45
75.63
75.63
75.63
75.13
75.63

6
77.39
62.65
89.09
87.26
78.35
77.65
77.82
78.00
87.61
90.04
91.74
85.22
93.00

7
64.21
52.26
58.98
66.37
64.68
64.99
71.68
60.95
62.44
59.20
63.13
61.67
63.49

8
52.04
37.07
49.32
54.76
52.04
36.39
49.96
49.32
48.30
50.00
78.91
51.02
79.25

9
47.87
47.66
66.17
61.28
48.51
47.45
50.79
77.02
67.87
62.77
62.34
61.49
62.34

10
48.78
49.97
49.50
54.44
49.58
48.72
53.79
48.58
48.36
44.81
45.14
54.61
45.50

Average
66.30
55.72
68.82
71.70
66.78
64.49
67.28
69.58
68.57
68.36
72.35
69.14
73.23

CLASSIFICATION ACCURACY (%) ON THE BREATH ANALYSIS DATASET.

BOLD VALUES INDICATE THE BEST RESULTS.

TABLE III

Original feature
KPCA
TCA [5]
SSTCA [5]
SA [17]
GFK [22]
ITL [16]
SSA [30]
MIDA (discrete)
SMIDA (discrete)
MIDA (continuous)
SMIDA (no aug.)
SMIDA (continuous)

Task 1
34.34
58.05
67.19
67.01
29.95
41.49
68.59
49.77
62.17
80.16
68.30
82.80
85.29

2
63.67
72.58
68.31
68.06
72.42
68.50
66.53
72.10
71.74
84.18
67.54
72.57
80.18

3
73.71
84.78
59.93
74.14
72.74
58.96
74.75
33.49
84.21
88.47
74.01
72.61
91.67

4
43.17
44.95
67.08
68.31
42.19
75.63
66.67
52.64
67.05
68.45
73.04
80.33
74.28

5
42.93
42.60
68.17
67.36
44.54
70.16
68.03
55.38
67.06
52.41
69.63
70.05
66.55

Average
51.57
60.59
66.14
68.97
52.37
62.95
68.91
52.68
70.45
74.73
70.50
75.67
79.59

Fig. 7. Illustration of the instrumental variation and time-varying drift in the
breath analysis dataset. Plots (a) and (b) show the steady state responses of
the CKD samples of sensors 2 and 7, respectively.

In KPCA, TCA, SSTCA, MIDA, and SMIDA, the RBF ker-
nel was used. Because methods other than stationary subspace
analysis (SSA) [30], MIDA, and SMIDA are not capable of
handling the chronological information, we simply regarded
each device as a discrete domain and learned device-invariant
features with them. The same strategy was used in discrete
MIDA and SMIDA. In continuous MIDA and SMIDA, the
domain features were deﬁned according to (4), where t was
the exact acquisition time converted to years and the number of
devices ndev = 2. SSA naturally considers the chronological
information by treating the sample stream as a multivariate
time series and identifying temporally stationary components.
However, SSA cannot deal with time series with multiple
sources, such as the multi-device case in this dataset. Thus,

the samples were arranged in chronological order despite their
device labels.

From Table III, we can ﬁnd that the performance of the
original features is not promising, which is caused by the
instrumental variation and time-varying drift in the dataset.
The domain adaptation algorithms can improve the accuracy.
The improvement made by SSA is little, possibly because the
stationary criterion is not suitable for preserving important
properties in data. For example, the noise in data can also be
stationary [5]. MIDA and SMIDA achieved obviously better
results than other methods. They can address both instrumental
variation and time-varying drift. With the background-speciﬁc
bias brought by feature augmentation, they can compensate for
the change in conditional probability in this dataset. Similar
to the gas sensor array drift dataset, it can be seen that the
continuous MIDA and SMIDA that utilize the time informa-
tion are better than the discrete ones. Feature augmentation can

05010015020025030035040045000.20.40.60.81Acquisition time (day)Sensor response (Volt)(a)05010015020025030035040045000.511.5Acquisition time (day)Sensor response (Volt)(b)  Device 1Device 2SUBMITTED TO IEEE TRANSACTIONS ON CYBERNETICS

10

Fig. 8. Scatter of the samples measured with the three devices in the corn
dataset.

improve continuous SMIDA by six percentage points. SMIDA
is better than MIDA because the label information of the ﬁrst
50 samples in each class was better kept.

D. Corn Dataset

Similar to e-noses, data collected with spectrometers are
one-dimensional signals indicating the concentration of the
analytes. Instrumental variation is also a problem for them
[21]. In this section, we test out methods on the corn dataset
2. It
is a spectroscopy dataset collected with three near-
infrared spectrometers designated as m5, mp5, and mp6. The
moisture, oil, protein, and starch contents of 80 corn samples
were measured by each device, with ranges of the measured
values as [9.377, 10.993], [3.088, 3.832], [7.654, 9.711], and
[62.826, 66.472], respectively. Each sample is represented by
a spectrum with 700 features. One can observe the discrepancy
among the three devices from Fig. 8. This dataset resembles
traditional domain adaptation datasets because there is no
time-varying drift. Three discrete domains can be deﬁned
based on the three devices. We adopt m5 as the source domain,
mp5 and mp6 as the target ones. In each domain, samples
4, 8, . . . , 76, 80 were assigned as the test set, the rest as the
training set. For hyper-parameter tuning, we applied a three-
fold cross-validation on the training sets of the three domains.
After the best hyper-parameters were determined for each
algorithm, a regression model was trained on the training set
from the source domain and applied on the test set from the
target domains. The regression algorithm was ridge regression
with the L2 regularization parameter λ = 1.

Table IV displays the root mean square error (RMSE) of
the four prediction tasks and their average on the two target
domains. We also plot the overall average RMSE of the two
domains with respect to the subspace dimension h in Fig.
9. ITL was not investigated because it is only applicable in
classiﬁcation problems. In KPCA, TCA, SSTCA, MIDA, and
SMIDA, the RBF kernel was used. For the semi-supervised
methods SSTCA and SMIDA, the target values were normal-
ized to zero mean and unit variance before subspace learning.

2http://www.eigenvector.com/data/Corn/

Fig. 9. Performance comparison on the corn dataset with respect to the
subspace dimension h.

We can ﬁnd that when no domain adaptation was done, the
prediction error is large. All domain adaptation algorithms
managed to signiﬁcantly reduce the error. KPCA also has
good performance, which is probably because the source and
the target domains have similar principal directions, which
also contain the most discriminative information. Therefore,
source regression models can ﬁt the target samples well. In
this dataset, different domains have identical data composition.
As a result, corresponding data can be aligned by subspaces
alignment, which explains the small error of SA. However,
this condition may not hold in other datasets.

MIDA and SMIDA obtained the lowest average errors
in both target domains. Aiming at exploring the prediction
accuracy when there is no instrument variation, we further
trained regression models on the training set of the two target
domains and tested on the same domain. The results are listed
as “train on target” in Table IV. It can be found that SMIDA
outperforms these results. This could be attributed to three
reasons: (1) The discrepancy in distributions in this dataset is
relatively easy to correct; (2) The use of RBF kernel in SMIDA
improves the accuracy; (3) SMIDA learned the subspace on
the basis of both training and test samples. Although the test
samples were unlabeled, they can provide some information
about the distribution of the samples to make the learned
subspace generalize better, which can be viewed as the merit
of semi-supervised learning. To testify this assumption, we
conducted another experiment with multiple target domains.
The training samples from the source domain and the test
ones from both target domains were leveraged together for
subspace learning in MIDA and SMIDA. The average RMSE
for the two target domains are 0.209 and 0.217 for MIDA,
and 0.208 and 0.218 for SMIDA. Compared with the results
in Table IV with single target domain, the results have been
further improved, showing that incorporating more unlabeled
samples from target domains can be beneﬁcial.

−2−101234−2.5−2−1.5−1−0.500.511.522.5PC1PC2  Device m5Device mp5Device mp601020304050600.20.250.30.350.40.450.5# Projected dimensions (h)Average regression RMSE  KPCATCASSTCASAMIDASMIDASUBMITTED TO IEEE TRANSACTIONS ON CYBERNETICS

11

REGRESSION RMSE ON THE CORN DATASET. BOLD VALUES INDICATE THE BEST CALIBRATION TRANSFER RESULTS.

TABLE IV

Mp5 as target domain
Moisture Oil

Mp6 as target domain

Protein Starch Average Moisture Oil

Protein Starch Average

Original feature 1.327
KPCA
0.477
0.539
TCA [5]
0.343
SSTCA [5]
0.302
SA [17]
0.267
GFK [22]
0.317
MIDA
0.287
SMIDA
Train on target
0.176

0.107 1.155
0.165 0.215
0.322 0.217
0.093 0.140
0.094 0.186
0.197 0.342
0.078 0.141
0.072 0.143
0.094 0.201

2.651
0.315
0.402
0.366
0.351
0.621
0.378
0.339
0.388

1.310
0.293
0.370
0.235
0.233
0.357
0.228
0.210
0.215

1.433
0.396
0.398
0.367
0.324
0.263
0.317
0.316
0.182

0.101 1.413
0.164 0.238
0.145 0.259
0.088 0.186
0.079 0.158
0.189 0.264
0.084 0.158
0.073 0.152
0.108 0.206

2.776
0.290
0.572
0.318
0.390
0.485
0.352
0.325
0.414

1.431
0.272
0.343
0.240
0.238
0.301
0.228
0.217
0.228

E. Visual Object Recognition Dataset

In [22], Gong et al. evaluated domain adaptation algorithms
on four visual object recognition datasets, namely Amazon
(A), Caltech-256 (C), DSLR (D), and Webcam (W). Ten
common classes were selected from them, with 8 to 151
samples per class per domain, and 2533 images in total. Each
image was encoded with an 800-bin histogram using SURF
features. The normalized histograms were z-scored to have
zero mean and unit variance in each dimension. Following
the experimental setting provided in the sample code from the
authors of [22], experiments were conducted in 20 random
trials for each pair of domains. For each unsupervised trail,
20 (for A, C, W) or 8 (for D) labeled samples per class
were randomly chosen from the source domain as the training
set (other samples were used unsupervisedly for domain
adaptation), while all unlabeled samples in the target domain
made up the test set. In semi-supervised trails, three labeled
samples per class in the target domain were also assumed to
be labeled. Averaged accuracies on each pair of domains as
well as standard errors are listed in Tables V and VI.

For GFK, low-rank transfer subspace learning (LTSL), and
domain adaptation by shifting covariance (DASC), we copied
the best results reported in the original papers [18], [19],
[3]. For other methods tested,
the hyper-parameters were
tuned for the best accuracy. Logistic regression was adopted
as the classiﬁer. The polynomial kernel with degree 2 was
used in KPCA, TCA, SSTCA, MIDA, and SMIDA. MIDA
and SMIDA achieve the best average accuracies in both
unsupervised and semi-supervised visual object recognition
experiments. We observe that TCA and SSTCA have com-
parable performance with MIDA and SMIDA, which may be
explained by the fact that the HSIC criterion used in MIDA
and MMD used in TCA are identical under certain conditions
when there are one source and one target domain [24]. Besides,
the feature augmentation strategy in MIDA is not crucial in this
dataset because there is no change in conditional probability.
On the other hand, TCA and SSTCA can only handle one
source and one target domains. SSTCA uses the manifold
regularization strategy to preserve local geometry information,
hence introduces three more hyper-parameters than SMIDA.
Moreover, computing the data adjacency graph in SSTCA and

the matrix inversion operation in TCA and SSTCA make them
slower than MIDA and SMIDA. We compared their speed on
the domain adaptation experiment C→ A. They were run on a
server with Intel Xeon 2.00 GHz CPU and 128 GB RAM. No
parallel computing was used. The codes of the algorithms were
written in Matlab R2014a. On average, the running times of
each trial of MIDA, SMIDA, TCA, and SSTCA were 2.4 s, 2.5
s, 3.0 s, and 10.2 s, respectively. Therefore, MIDA and SMIDA
are more practical to use than TCA and SSTCA. Besides,
they were initially designed for calibration transfer and drift
correction. This dataset is used to show their universality.

V. CONCLUSION

In this paper, we introduced maximum independence do-
main adaptation (MIDA) to learn domain-invariant features.
The main idea of MIDA is to reduce the discrepancy among
domains by maximizing the independence between the learned
features and the domain features of the samples. The domain
features describe the background information of each sample,
such as the domain label in traditional domain adaptation
problems. In the ﬁeld of sensors and measurement, the device
label and acquisition time of the each collected sample can
be expressed by the domain features, so that unsupervised
calibration transfer and drift correction can be achieved by
using MIDA. The feature augmentation strategy proposed in
this paper adds domain-speciﬁc biases to the learned features,
which helps MIDA to align domains. It is also useful when
there is a change in conditional probability. Finally, to incor-
porate the label information, semi-supervised MIDA (SMIDA)
is further presented.

MIDA and SMIDA are ﬂexible algorithms. With the design
of the domain features and the use of the HSIC criterion,
they can be applied in all kinds of domain adaptation prob-
lems, including discrete or continuous distributional change,
supervised/semi-supervised/unsupervised, multiple domains,
classiﬁcation or regression, etc. Thus, they have a wide range
of potential applications. They are also easy to implement and
fast, requiring to solve only one eigenvalue decomposition
problem. Experimental results on various types of datasets
proved their effectiveness. Although initially designed for
calibration transfer and drift correction in machine olfaction,

SUBMITTED TO IEEE TRANSACTIONS ON CYBERNETICS

12

UNSUPERVISED DOMAIN ADAPTATION ACCURACY (%) ON THE VISUAL OBJECT RECOGNITION DATASET. BOLD VALUES INDICATE THE BEST RESULTS.

X → Y MEANS THAT X IS THE SOURCE DOMAIN AND Y IS THE TARGET ONE.

TABLE V

A → C

A → D

A → W C → W D → W Average

D → C W → C

D → A W → A

C → D W → D

C → A
43.2±2.2 34.9±1.1 36.8±0.6 38.5±1.6 31.7±1.2 32.7±0.9 37.3±3.1 40.5±3.6 80.6±2.0 37.5±2.9 37.1±3.6 76.7±2.0 43.97
Ori. ft.
27.4±2.0 27.0±1.6 27.6±1.2 25.8±1.3 22.0±2.4 23.7±1.6 29.2±2.9 27.6±2.7 55.1±1.8 29.0±2.7 27.1±3.2 50.3±2.6 30.97
KPCA
49.8±2.7 38.6±1.4 39.2±1.0 42.8±2.1 35.3±1.5 35.7±0.8 42.8±3.2 45.9±3.8 83.9±1.7 41.7±3.3 42.8±5.4 81.5±2.1 48.35
TCA [5]
SSTCA [5] 50.5±2.8 39.3±1.6 40.5±0.7 42.4±1.8 36.1±1.5 35.9±0.8 42.8±3.1 46.6±3.5 80.6±2.3 42.5±2.7 42.8±4.7 81.8±1.9 48.48
41.2±3.0 35.7±2.0 38.4±1.1 34.8±1.8 28.7±1.5 31.4±1.6 34.5±3.0 32.3±3.8 67.0±2.7 31.8±3.6 36.4±4.2 71.1±3.2 40.27
ITL [16]
48.4±2.9 36.3±2.6 37.3±1.7 38.5±2.1 33.4±1.9 35.1±0.9 35.0±3.6 39.7±5.0 63.2±2.8 36.7±4.4 41.3±5.5 70.3±2.5 42.93
SA [17]
40.4±0.7 36.2±0.4 35.5±0.7 37.9±0.4 32.7±0.4 29.3±0.4 35.1±0.8 41.1±1.3 71.2±0.9 35.7±0.9 35.8±1.0 79.1±0.7 42.50
GFK [18]
50.4±0.4 40.2±0.6 44.1±0.3 38.6±0.3 35.3±0.3 37.4±0.2 38.3±1.1 53.7±0.9 79.8±0.4 38.8±1.3 47.0±1.0 72.8±0.7 48.03
LTSL [19]
39.1±0.3 39.3±0.8 37.7±0.7 49.8±0.4 48.5±0.8 45.4±0.9 36.5±0.3 35.6±0.3 88.3±0.4 36.3±0.4 33.3±0.3 79.8±0.9 47.47
DASC [3]
50.3±2.5 39.2±1.9 39.8±1.0 42.7±1.8 35.5±1.1 35.7±0.7 42.3±2.8 45.7±3.6 82.2±2.0 42.8±2.8 43.6±5.0 82.4±2.0 48.49
MIDA
50.5±2.4 39.1±1.8 39.8±1.1 42.7±2.0 35.5±1.2 35.4±0.8 42.4±2.6 45.8±3.3 82.5±2.1 42.9±2.8 43.4±5.1 81.9±2.0 48.51
SMIDA

SEMI-SUPERVISED DOMAIN ADAPTATION ACCURACY (%) ON THE VISUAL OBJECT RECOGNITION DATASET. BOLD VALUES INDICATE THE BEST RESULTS.

TABLE VI

A → C

A → D

A → W C → W D → W Average

D → C W → C

D → A W → A

C → D W → D

C → A
48.8±1.8 44.5±1.6 43.5±1.4 41.6±1.9 36.7±2.1 37.3±1.4 48.1±4.2 49.3±3.2 81.7±2.4 51.0±3.4 50.9±4.4 80.5±2.2 51.17
Ori. ft.
53.3±2.4 46.2±1.7 43.2±1.1 44.1±1.4 39.1±1.8 37.8±1.1 47.3±3.3 53.9±3.4 81.5±2.9 49.7±2.7 54.0±4.1 81.1±2.2 52.60
KPCA
55.3±2.2 48.6±1.8 45.7±1.4 46.1±2.0 40.3±2.1 39.7±1.4 52.1±3.0 56.3±4.5 83.7±2.9 55.4±3.5 58.3±4.5 84.2±1.9 55.46
TCA [5]
SSTCA [5] 55.3±2.2 48.6±1.8 45.6±1.4 46.0±2.0 40.3±2.1 39.7±1.3 52.1±3.1 56.3±4.6 83.7±2.9 55.4±3.5 58.4±4.5 84.2±1.9 55.47
51.5±3.1 47.7±2.5 44.1±2.1 40.0±2.2 36.8±3.2 36.6±2.2 44.4±4.1 48.2±4.0 59.7±2.6 51.5±4.5 54.9±3.8 68.5±3.3 48.65
ITL [16]
51.8±2.3 47.6±2.8 44.7±1.7 42.6±1.7 36.9±2.9 36.8±2.0 45.3±4.2 49.0±3.9 71.0±2.9 47.2±2.9 49.0±3.6 76.1±2.5 49.82
SA [17]
46.1±0.6 46.2±0.6 46.2±0.7 39.6±0.4 33.9±0.6 32.3±0.6 50.9±0.9 55.0±0.9 74.1±0.9 56.9±1.0 57.0±0.9 80.2±0.4 51.53
GFK [18]
50.4±0.5 47.4±0.5 47.8±0.4 39.8±0.4 36.7±0.4 38.5±0.3 59.1±0.7 59.6±0.6 82.6±0.5 59.5±1.1 59.5±0.8 78.3±0.4 54.93
LTSL [19]
55.2±2.2 48.6±1.8 45.6±1.4 46.1±2.0 40.4±2.2 39.7±1.4 52.1±3.0 56.4±4.6 83.7±2.8 55.3±3.4 58.5±4.6 84.2±1.8 55.48
MIDA
55.2±2.2 48.6±1.8 45.7±1.4 46.1±2.0 40.3±2.1 39.7±1.4 52.2±3.1 56.3±4.6 83.7±2.9 55.4±3.5 58.5±4.5 84.3±1.9 55.49
SMIDA

they also performed well in spectroscopy and vision datasets.
Future directions may include further extending the deﬁnition
of the domain features for other applications. Besides, this
paper mainly focus on reducing the difference of the marginal
distribution P (X) among domains, while it may be better if
the conditional distribution P (Y |X) among domains can also
be approximately aligned [16].

ACKNOWLEDGMENT

The authors would like to thank the providers of the datasets

used in this paper.

REFERENCES

[1] S. J. Pan and Q. Yang, “A survey on transfer learning,” IEEE Trans.

Knowl. Data Eng., vol. 22, no. 10, pp. 1345–1359, 2010.

[2] V. M. Patel, R. Gopalan, R. Li, and R. Chellappa, “Visual domain
adaptation: A survey of recent advances,” Signal Processing Magazine,
IEEE, vol. 32, no. 3, pp. 53–69, 2015.

[3] Z. Cui, W. Li, D. Xu, S. Shan, X. Chen, and X. Li, “Flowing on
riemannian manifold: Domain adaptation by shifting covariance,” IEEE
Trans. Cybern., vol. 44, no. 12, pp. 2264–2273, 2014.

[4] W. Bian, D. Tao, and Y. Rui, “Cross-domain human action recognition,”
Systems, Man, and Cybernetics, Part B: Cybernetics, IEEE Transactions
on, vol. 42, no. 2, pp. 298–307, 2012.

[5] S. J. Pan, I. W. Tsang, J. T. Kwok, and Q. Yang, “Domain adaptation
via transfer component analysis,” Neural Networks, IEEE Transactions
on, vol. 22, no. 2, pp. 199–210, 2011.

[6] C.-W. Seah, Y.-S. Ong, and I. W. Tsang, “Combating negative transfer
from predictive distribution differences,” IEEE Trans. Cybern., vol. 43,
no. 4, pp. 1153–1165, 2013.

[7] J. W. Gardner and P. N. Bartlett, “A brief history of electronic noses,”

Sens. Actuators B: Chem., vol. 18, no. 1, pp. 210–211, 1994.

[8] F. R¨ock, N. Barsan, and U. Weimar, “Electronic nose: current status and

future trends,” Chem. Rev., vol. 108, no. 2, pp. 705–725, 2008.

[9] A. Loutﬁ, S. Coradeschi, A. J. Lilienthal, and J. Gonzalez, “Gas
distribution mapping of multiple odour sources using a mobile robot,”
Robotica, vol. 27, no. 02, pp. 311–319, 2009.

[10] L. Zhang, F. Tian, C. Kadri, B. Xiao, H. Li, L. Pan, and H. Zhou,
“On-line sensor calibration transfer among electronic nose instruments
for monitoring volatile organic chemicals in indoor air quality,” Sens.
Actuators: B. Chem., vol. 160, no. 1, pp. 899–909, 2011.

[11] K. Yan, D. Zhang, D. Wu, H. Wei, and G. Lu, “Design of a breath anal-
ysis system for diabetes screening and blood glucose level prediction,”
IEEE Trans. Biomed. Eng., vol. 61, no. 11, pp. 2787–2795, 2014.

[12] S. Marco and A. Guti´errez-G´alvez, “Signal and data processing for
machine olfaction and chemical sensing: a review,” IEEE Sens. J.,
vol. 12, no. 11, pp. 3189–3214, 2012.

[13] S. Di Carlo and M. Falasconi, Drift correction methods for gas chem-
ical sensors in artiﬁcial olfaction systems: techniques and challenges.
InTech, 2012, ch. 14, pp. 305–326.

[14] K. Yan and D. Zhang, “Improving the transfer ability of prediction
models for electronic noses,” Sens. Actuators B: Chem., vol. 220, pp.
115–124, 2015.

[15] ——, “Calibration transfer and drift compensation of e-noses via cou-
pled task learning,” Sens. Actuators B: Chem., vol. 225, pp. 288–297,
2016.

[16] Y. Shi and F. Sha, “Information-theoretical learning of discriminative
clusters for unsupervised domain adaptation,” in Proceedings of the Intl.
Conf. on Machine Learning (ICML), 2012.

[17] B. Fernando, A. Habrard, M. Sebban, and T. Tuytelaars, “Unsupervised
visual domain adaptation using subspace alignment,” in Proceedings of
the IEEE International Conference on Computer Vision, 2013, pp. 2960–
2967.

[18] B. Gong, K. Grauman, and F. Sha, “Learning kernels for unsupervised

SUBMITTED TO IEEE TRANSACTIONS ON CYBERNETICS

13

domain adaptation with applications to visual object recognition,” In-
ternational Journal of Computer Vision, vol. 109, no. 1-2, pp. 3–27,
2014.

[19] M. Shao, D. Kit, and Y. Fu, “Generalized transfer subspace learning
through low-rank constraint,” International Journal of Computer Vision,
vol. 109, no. 1-2, pp. 74–93, 2014.

[20] A. Gretton, O. Bousquet, A. Smola, and B. Schlkopf, “Measuring statis-
tical dependence with hilbert-schmidt norms,” in Algorithmic learning
theory. Springer, 2005, pp. 63–77.

[21] R. N. Feudale, N. A. Woody, H. Tan, A. J. Myles, S. D. Brown,
and J. Ferr´e, “Transfer of multivariate calibration models: a review,”
Chemometr. Intell. Lab., vol. 64, no. 2, pp. 181–192, 2002.

[22] B. Gong, Y. Shi, F. Sha, and K. Grauman, “Geodesic ﬂow kernel
for unsupervised domain adaptation,” in Computer Vision and Pattern
Recognition (CVPR), 2012 IEEE Conference on. IEEE, 2012, pp. 2066–
2073.

[23] Q. Liu, X. Li, M. Ye, S. S. Ge, and X. Du, “Drift compensation for
electronic nose by semi-supervised domain adaption,” IEEE Sens. J.,
vol. 14, no. 3, pp. 657–665, 2014.

[24] L. Song, A. Smola, A. Gretton, J. Bedo, and K. Borgwardt, “Feature
selection via dependence maximization,” J. Mach. Learn. Res., vol. 13,
no. 1, pp. 1393–1434, 2012.

[25] L. Song, A. Gretton, K. M. Borgwardt, and A. J. Smola, “Colored maxi-
mum variance unfolding,” in Advances in neural information processing
systems, 2007, pp. 1385–1392.

[26] E. Barshan, A. Ghodsi, Z. Azimifar, and M. Z. Jahromi, “Supervised
principal component analysis: Visualization, classiﬁcation and regression
on subspaces and submanifolds,” Pattern Recogn., vol. 44, no. 7, pp.
1357–1371, 2011.

[27] H. Daum III, “Frustratingly easy domain adaptation,” in Proc. 45th Ann.

Meeting of the Assoc. for Computational Linguistics, 2007.

[28] B. Schlkopf, A. Smola, and K.-R. Mller, “Nonlinear component analysis
as a kernel eigenvalue problem,” Neural computation, vol. 10, no. 5, pp.
1299–1319, 1998.

[29] B. Scholkopft and K.-R. Mullert, “Fisher discriminant analysis with
kernels,” Neural networks for signal processing, vol. IX, pp. 41–48,
1999.

[30] P. Von B¨unau, F. C. Meinecke, F. C. Kir´aly, and K.-R. M¨uller, “Finding
stationary subspaces in multivariate time series,” Physical review letters,
vol. 103, no. 21, p. 214101, 2009.

[31] J. a. Gama, I. ˇZliobait˙e, A. Bifet, M. Pechenizkiy, and A. Bouchachia, “A
survey on concept drift adaptation,” ACM Computing Surveys (CSUR),
vol. 46, no. 4, p. 44, 2014.

[32] M. Belkin, P. Niyogi, and V. Sindhwani, “Manifold regularization: A
geometric framework for learning from labeled and unlabeled examples,”
J. Mach. Learn. Res., vol. 7, pp. 2399–2434, 2006.

[33] A. Vergara, S. Vembu, T. Ayhan, M. A. Ryan, M. L. Homer, and
R. Huerta, “Chemical gas sensor drift compensation using classiﬁer
ensembles,” Sens. Actuators B: Chem., vol. 166, pp. 320–329, 2012.

