6
1
0
2

 
r
a

 

M
1
1

 
 
]
E
M

.
t
a
t
s
[
 
 

1
v
0
4
6
3
0

.

3
0
6
1
:
v
i
X
r
a

Sequential Clustering for Functional Data

Ana Justel∗ and Marcela Svarc∗∗1

∗Departamento de Matem´aticas, Universidad Aut´onoma de Madrid, Spain
∗∗Departamento de Matem´atica y Ciencias, Universidad de San Andr´es and

CONICET, Argentina

Abstract

This paper presents SeqClusFD, a sequential top-down clustering method for
functional data. The clustering algorithm extracts the partitioning information
either from trajectories, ﬁrst or second derivatives. The splitting stages include lo-
cal and global search for the structure of clusters. Initial partition is based on gap
statistics that provides local information to identify the instant with more clus-
tering evidence in trajectories or derivatives. Sample is tentatively classiﬁed using
one-dimensional k-means. Then functional boxplots allow reconsidering overall
allocation and each observation is ﬁnally assigned to the cluster where it spends
most of the time within whiskers. In this way we seek to balance local and global
eﬀects. The procedure is applied recursively until no evidence of clustering at any
time on trajectories or ﬁrst and second derivatives. SeqClusFD simultaneously
estimates the number of groups and provides data allocation.
It also provides
valuable information to explain which are the most important features that de-
termine the structure of the clusters. Computational aspects have been analyzed
and the new method is tested on synthetic and real data sets.

Keywords: Hierarchical Clustering, Functional Boxplot, Gap Statistics.

1

Introduction

Functional data analysis (FDA) is a very active area of research nowadays, mainly
since it has become very easy to collect and store data in “continuous time” (see [11]).
Although generally each data is recorded only in a ﬁnite number of moments, it is more
common to analyze the data as functions rather than as vectors. The observations in this
context are each of the curves instead of points and is necessary to develop new statistical
techniques for analyzing such data. Most existing procedures for functional data require
a large amount of consecutive observations on which smoothing techniques are applied,

1Corresponding author: Marcela Svarc, Departamento de Matem´atica y Ciencias, Universidad de

San Andr´es.... Email: msvarc@udesa.edu.ar

1

an overview can be found at [25]. The classical assumption is that functions belong
to a Hilbert space (for example, square integrable functions on a ﬁnite real interval
[a, b]) and can be represented with a convenient functional basis. Depending on the
characteristics of the functions, the most common basis are Fourier, Haar or wavelets.
A ﬁnite truncation of the series facilitates the analysis with the conventional techniques
of multivariate statistics applied to the ﬁnite set of retained coeﬃcients.

The aim of cluster analysis, or unsupervised classiﬁcation, is to assign observations
into subsets, such that similar objects are in the same group and dissimilar ones are in
diﬀerent groups. This is a complex problem since usually there is no previous information
about the data structure. There is no “one size ﬁts all” method for analyzing any data set
and the nature of the data should determine the procedure to be used. In fact, there are
clustering methods that have outstanding performance in certain conﬁgurations of data
and a terrible behavior under diﬀerent conditions. For instance, it is well known that the
popular k-means is suitable in Rd for round, Gaussian and well separated clusters but
it is not able to ﬁnd cluster structures with nested groups. Some multivariate clustering
techniques have been successfully adapted to functional data after considering speciﬁc
geometrical notions and can be classiﬁed in one of the next three categories.

Centroid based clustering methods use optimization algorithms for centroid ﬁnding
of each group (as k-means). Several authors studied and adapted these methods to
functional data, see for instance [1], [30], [9], [10], [33], [13], [26], [27] and [15].

Model based clustering methods assume diﬀerent population model for each group,
typically all from the same distribution family (as Gaussian mixture models). In FDA
there are several procedures following this strategy, such as those proposed in [18],[5]
and [16].

Hierarchical clustering methods are based on the idea of divisive and/or agglom-
erative sequential grouping to provide the best partition on each possible number of
groups. Divisive methods begin allocating all observations into one group and sequen-
tially separate into diﬀerent groups the observations that are more distant from the rest.
This idea is repeated until there are as many groups as observations. Agglomerative
methods initially assign each observation to a diﬀerent group and then sequentially join
the closest observations until all observations are in a single group. For functional data
Giraldo et al.
[14] developed an algorithm for spatially correlated data and Boull`e et
al. [6] introduced a bayesian proposal of nonparametric hierarchical clustering for func-
tional data. At the best of our knowledge, the particular case of hierarchical clustering
based on decision trees with sequential binary partitions, have never been extended to
FDA from multivariate data approaches where several extensions can be found see for
instance [19],[3] and [12].

2

In general, in centroid or model based methods the number of clusters is known
while in hierarchical clustering is unknown. Recently, Jacques and Preda [17] described
thoroughly the state of the art of this ﬁeld.

The aim of this paper is to introduce a new hierarchical clustering method for func-
tional data with the particularity that, in any division, two local and global steps allow
data reallocation between clusters.We call SeqClusFD to the new cluster method. The
idea is to develop a divisive top-down clustering algorithm designed only for FDA since is
based on sequential exploration of the functions and their derivatives. This means that
we consider local level and global shape properties that are available in functions, and
not in ﬁnite dimensional data. Output from the algorithm includes number of groups,
group assignment and some guidelines for the interpretation on group structure.

As it is well known, not all relevant information on a function is visible in the
trajectory. Derivatives are usually very helpful to highlight diﬀerences. Although all
clustering methods for functional data mentioned above can be applied to the set of
trajectories, or the set of ﬁrst derivatives, second derivatives, and so on, it is interesting
to have a statistical procedure able to extract relevant information from any or all of
these sets. In supervised classiﬁcation of functional data, Alonso et al. [2] considered a
distance base on using derivatives. Sequential exploration of functions and derivatives
is a key point in the new clustering method since group structure must be associated
to the trajectories for some groups and to derivatives in some others. To illustrate this
idea, lets analyze a very simple example with the three groups of 25 functions shown
in Figure 1. Functions in groups 1 and 2 are constant but with diﬀerent levels, while
functions in group 3 have constant positive slope and levels are similar to functions in
group 1. Considering original functions, only two groups are identiﬁed by looking in any
instant, a cluster that contains lines of groups 1 and 3 and the other with lines from group
2. Switching to apply clustering methods with ﬁrst derivative functions, we note that all
lines in groups 1 and 2 have zero constant derivative function, diﬀerent than derivatives
from group 3 that are also constant but nonzero. Any clustering method applied to
derivatives also identify a maximum of two clusters, one with the lines of groups 1 and
2 and the other with lines from group 3. In summary, by applying clustering methods
to trajectories, functions are separated by level and when applied to derivatives are
separated by shape. We will only be able to succeed in identifying the three groups if
we run a sequential clustering scheme taking into account trajectories and derivatives.

The remainder of this paper is organized as follows. In Section 2, SeqClusFD method
is presented and some details on practical implementation are analyzed. In Section 3, a
simulation study with synthetic data is performed, for each case the correct classiﬁcation
rate (CCR) is reported. In Section 4, several sets of well known real data are analyzed
and a further study is conducted including interpretation of the results. The conclusions

3

Figure 1: Light solid lines are in group 1; dark solid lines are in group 2; and dashed
lines are in group 3.

are given in the last Section.

2 Divisive top-down SeqClusFD method

We observe X1(t), . . . , Xn(t) functions grouped into an unknown number k of popu-
lations, C1, . . . , Ck. We assume all the functions are smooth and deﬁned in the same
compact real interval [a, b]. Each data X(t) is a realization of a stochastic process deﬁned
on the probability space (Ω, F, P ), hence X(t) is a random variable for each t ∈ [a, b].

Two facts are motivating the need of a new sequential procedure for cluster identi-
ﬁcation that takes into account the shape of the curves. The ﬁrst is that functions that
belong to diﬀerent groups must diﬀer by at least one of the following characteristics:
trajectories, velocities or accelerations. The second is that it may happen that some
groups diﬀer by the shapes of the trajectories and some others by the shapes on any
of their derivatives. We use the notation X l
n(t), where l = 0, 1, 2 means the
order of derivation. For the sake of simplicity, if l = 0 we omit the superscript.

1(t), . . . , X l

The top-down splitting stages include local and global sequential search for the
structure of clusters.
Initial partition is based on gap statistics, [32], that provides
local information to identify the instant with more clustering evidence in trajectories
or derivatives. Sample is tentatively classiﬁed using one-dimensional k-means. Then
functional boxplots, [29], allow reconsider overall allocation using global information
and each observation is ﬁnally assigned to the cluster where it spends most of the time
within whiskers.

Hence, our idea is to start with a single group and, on each splitting stage, sequen-
tially apply the following local and global clustering steps. We stop the divisions when
there is no evidence of new cluster structure inside any group.

4

00.51012Local clustering step: Find the instant t ∈ T in which the cluster structure is more
evident considering X(t), X 1(t) or X 2(t). Local clustering evidence is assessed with the
gap statistics, that can be used to determine the number of clusters with any clustering
method. Once the instant, the feature (trajectory, velocity or acceleration) and the
number of groups have been selected, the sample is tentatively classiﬁed using the one-
dimensional k-means allocation on t.

This initial classiﬁcation is based on a local criteria, then we need a global criteria

to correct possible local eﬀects, as warping eﬀects.

Global revision clustering step: Considering the same feature of the initial clustering
allocation we built up, for each group, the functional boxplot. Reallocation of mislead-
ing data is done by computing the proportion of time that each function is inside the
functional boxplot. In the ﬁnal cluster division, each observation belongs to the group
where it spends most of the time inside whiskers.

Before providing the detailed SeqClusFD algorithm, we brieﬂy recall the two main

tools that we utilize, gap statistics and functional boxplot.

2.1 Gap statistics and functional boxplot

Gap statistic (GS) was introduced by Tibshirani et al. [32] to estimate the number of
clusters in a data set. The idea behind the proposal is to compare the change in the
within cluster dispersion when one additional group is considered with the expected
change under an appropriate reference distribution. The main property of GS is that
can be used in combination with any cluster method.

In the simplest case in which X1, . . . , Xn is a sample of one dimensional observations

from groups C1, . . . , Ck, the within cluster dispersions are estimated by

(cid:88)

i,i(cid:48)∈Cr

Dr =

dii(cid:48),

where dii(cid:48) is the euclidean distance between two observations of the same cluster. The
gain of considering k + 1 groups instead of k is

where Wk =(cid:80)k

Lk,k+1 = ln(Wk) − ln(Wk+1),

(2.1)

r=1 Dr/2nr is the total sum of pairwise distances within C1, . . . , Ck and

nr is the cardinal of Cr.

As a large Lk,k+1 indicates evidence of k + 1 groups, this value is compared with that
obtained from a sample generated from an appropriate reference distribution. Tibshirani
et al. [32] proved that, for the one-dimensional case, the reference distribution should

5

be uniform. Hence we generate b = 1, . . . , B random samples of size n from the uniform
distribution on the interval [min Xi, max Xi]. For each sample we compute ln(Wk,b) and
ln(Wk+1,b). We denote the empirical version of (2.1) by L∗(k, k + 1) = E∗(ln(Wk,b)) −
E∗(ln(Wk+1,b)), where E∗(ln(Wkb)) and E∗(ln(Wk+1,b)) are empirical means. Empirical
standard deviation of ln(Wk,b) is denoted by s∗(k).

The gap statistic is the minimum(cid:98)k that satisﬁes

(2.2)

(cid:112)1 + 1/Bs∗(k + 1) ≥ L(k, k + 1),

L∗(k, k + 1) + nsd

where nsd is the number of standard deviations to be ﬁxed. In [32], the authors suggest
nsd = 1, but stronger rules could be considered.

Functional boxplot (FB) was introduced by Sun and Genton [29]. Construction is
based on ordering functions from the center outward using band-depth deﬁnition of
L´opez-Pintado and Romo [20]. The appearance is determined by deepest curve (me-
dian), envelope of 50% central functions (box borders) and maximum non-outlying en-
velope (whiskers). In analogy with classical boxplots, 1.5 times the 50% central region
is typically considered for the non-outlying envelope. In [20] the authors suggest that
this value can be adjusted on practice.

2.2 SeqClusFD algorithm

STEP 0.

Deﬁne a grid on the interval [a, b]: a = t0 < ··· < tN = b.
Compute X l

i(tj) for i = 1, . . . , n, l = 0, 1, 2 and j = 0, . . . , N − l.

Start with a single group and sequentially apply the following two steps:

STEP 1. Local clustering.

1(tj), . . . , X l

- For all data sets Aj,l = {X l

estimate the gap statistic(cid:101)k(Aj,l) in combination with k-means clustering.
n(tj)}, where l = 0, 1, 2 and j = 0, . . . , N − l,
- Estimate the number of groups by ˆk = maxj,l(cid:101)k(Aj,l) and call C1, . . . , Cˆk to the
ˆAj,l = arg maxj,l(cid:101)k(Aj,l). In case of ties, choose ˆAj,l that provides more evidence

sets that contains the complete functions associated to the k-means clusters of

of ˆk clusters, this means that has larger value of Lˆk,ˆk+1(j, l).

STEP 2. Global cluster revision.

6

- Compute ˆk functional boxplots with the curves in each of groups C1, . . . , Cˆk, see
[29]. Consider 3 times the point wise interquartil range for maximum length of
whiskers, which is the usual rule in one dimensional boxplot to identify extreme
outliers. For r = 1, . . . , ˆk, call (LBr(t), U Br(t)) to the lower and upper extreme
outlier whisker bands of functional boxplots.

- For each cluster r = 1, . . . , ˆk,

identify potential outliers as functions outside

(LBr(t), U Br(t)), at least for one instant.

- Reallocate each potential outliers into the cluster C1, . . . , Cˆk where spends more

time inside (LBr(t), U Br(t)), for r = 1, . . . , ˆk.

REPEAT.

If one group is divided, repeat step 1 and 2 for this group to ﬁnd possible partitions.
Stop division when gap statistic in step 1 determines that there is only one group for
every instant t0 < ··· < tN and feature of the data set (trajectories or derivatives).

2.3 Practical implementation

Considering again the example shown in Figure 1, we ﬁnd that SeqClusFD separates
in the ﬁrst step the third group using derivative functions. In second step, SeqClusFD
separates the ﬁrst two groups considering the trajectories.
In addition, SeqClusFD
determines that there are no further groups. Then, three groups are found and there
are not misclassiﬁed observations. Ieva et al [15], perform a k means clustering procedure
for functional data, but instead of using the classical L2 norm they work in the Sobolev
H 1 space with the ordinary norm, this means that they consider simultaneously the
information of the function and the derivative. If we use that approach in this example,
and estimate the number of clusters either by the Gap Statistics or by Calinski-Harabasz
approach, which are among the best well known procedures to estimate the number of
clusters, neither of them detect the correct number of clusters.

In more realistic problems computation of the ﬁrst two derivatives is not a simple
task. Typically, functional data are characterized by a high amount of consecutive
observations deﬁned in an interval of ﬁnite length, but frequency may defer from one
individual to another and observations are not necessarily equidistant. When measures
do not have sampling errors, function values can be interpolated and then derivatives
are computed by diﬀerentiating. Otherwise it is necessary to use smoothing techniques
in order to transform observations into functions that can be evaluated for any time t.
Diﬀerent smoothing techniques should be considered since each data set has diﬀerent
properties, as for instance functions could be monotone, or periodic or non-negative.

7

Ramsay et al., [24] discuss the main options that are consider in FDA, most of them
implemented in Matlab and R routines that can be downloaded from http://www.
functionaldata.org.

For every tj and feature of the trajectories, uniform distribution boundaries for the
bootstrap samples, that are necessary for gap statistic computations, will probably be
diﬀerent. Three bootstrap procedures should be executed for each tj. Then the thinner
the grid t0 < ··· < tN deﬁned in step 0 of SeqClusFD algorithm more computationally
expensive is step 1. To speed up SeqClusFD we suggest pointwise rescaling to the
n(tj)} (l = 0, 1, 2 and
interval [0, 1] for observations in subsets Aj,l = {X l
j = 0, . . . , N − l) as follows,

1(tj), . . . , X l

Y l
i (tj) =

i(tj) − min{Aj,l}
X l
max{Aj,l} − min{Aj,l}, for i = 1, . . . , n.

Now all bootstrap samples should be generated according to a uniform distribution on
the interval [0, 1]. This means that bootstrap procedure can be done only once for each
visit of the algorithm to step 1. However, when step 1 is repeated it is because a group is
divided and then sample size is diﬀerent. We can not avoid bootstrap should be redone
on diﬀerent visits.

3 Simulation Study

To show the performance of SeqClusFD we conducted a simulation study using diﬀerent
artiﬁcial data sets that have been previously proposed in the literature for clustering of
functional data. In all cases we report the number of times that SeqClusFD selects the
correct number of groups. For these successful examples, the correct classiﬁcation rate
(CCR) is estimated.

As SeqClusFD is based on gap statistics and functional boxplot, before executing
the algorithm it is necessary to ﬁx the value of some parameters related to these tools.
For gap statistic we follow suggestions in the original paper, except for nsd = 3 on
equation (2.2) since we are applying a stricter rule for identifying clusters with strong
evidence at a certain time period. More that 500 bootstrap samples are not necessary.
For functional boxplot we follow the classical rule to identify severe outliers, considering
the maximum length of the whiskers as three times the interquantil range.

To reduce computational eﬀort in the local clustering step we limit the maximum
number of clusters to ﬁve. It is important to remark that, even though this parameter
ﬁxes an upper bound for the number of clusters on each partitioning step, there is not an
upper bound for the general procedure. In the global cluster revision step, A minimum

8

Figure 2: Examples of data set of functions simulated with models A, B and C.

of 10 functions are required to compute a boxplot. The minimum cluster size to continue
with splitting is also ﬁxed on 10 functions

We propose the same values for all the simulations and real data examples.

3.1 Sampling errors associated with the entire curve

Sangalli et al. [26] introduced three models for curve generation. We follow their models
for data generation with the idea of exploring the SeqClusFD performance in clusters
structures with diﬀerent amplitudes and curve registration problems.

Model A: Two clusters with n/2 functions generated as follows,

(cid:33)

Xi(t) = (1 + 1i) sin (3i + 4it) +
(3i + 4it)2

(1 + 2i) sin

for i = 1, . . . , n/2,

Xi(t) = (1 + 1i) sin (3i + 4it) −
(3i + 4it)2

(1 + 2i) sin

(cid:32)

(cid:32)

(cid:33)

(3.3)

(3.4)

2π

2π

for i = n/2 + 1, . . . , n.

Model B: Two clusters with n/2 functions generated in the ﬁrst group following (3.3)
and in the second group as follows,

Xi(t) = (1 + 1i) sin

(1 + 2i) sin

(cid:18)
(cid:18)
(cid:32)(cid:0)3i + 4i
(cid:0)− 1

3i + 4i

+

−1
3
4
3
3 + 3

(cid:19)(cid:19)
4t(cid:1)(cid:1)2

t

2π

−

(cid:33)

(3.5)

for i = n/2 + 1, . . . , n.

Model C: Three clusters with n/3 functions generated in the ﬁrst group following (3.3),
in the second group following (3.4) and in the third group following (3.5).

9

0246-2-1012Case C0246-2-1012Case B0246-2-1012Case ATable 1: Distribution of the Number of Groups found by SeqClusFD and Mean CCR.

Number of clusters Model A Model B Model C

2

3

4

5

88

12

0

0

99.5

0.05

0

0

0

75.5

23.5

1

Mean CCR

99.67

99.96

99.45

We simulate 200 data sets of size n = 90 for each model. All errors 1i, . . . , 4i are
independent and normally distributed with mean 0 and standard deviation 0.05. Figure
2 shows three examples of 90 curves generated according with models A, B and C.

Table 1 reports for each model, percentage of data sets in which SeqClusFD ﬁnds
the number of clusters indicated on ﬁrst column. In bottom line are displayed the mean
of correct classiﬁcation rates (CCR) calculated only with data in which the number of
clusters is correctly identiﬁed. In almost all cases it ﬁnd the correct number of clusters.
The result is poorer in the model C, which is also the most challenging problem, with
a 75.5% of successful identiﬁcations. When the number of clusters is not identiﬁed
correctly, generally it is determined that there is an additional cluster. We observed
that in these cases one of the original cluster is divided into two and the other clusters
are identiﬁed without error. Table 1 also shows the mean of correct classiﬁcation rates
(CCR), calculated only with data sets in which the number of clusters is correctly
identiﬁed. It is clear that SeqClusFD has an outstanding performance in these cases.

3.2 Sampling errors associated to each instant

We simulate 200 data sets with four clusters that are generated with a similar model
to that introduced by Serban and Wasserman [28]. Each cluster contains 150 functions
that are generated as follows

Xij(t) = fj(t) + i(t),

for t ∈ [0, 1], i = 1, . . . , 150 and j = 1, . . . , 4,

(cid:18)2 − 5t

(cid:18)2 − 5t

,

2

(cid:18)5πt

(cid:19)(cid:19)(cid:19)

,

2

sin

2

2

where

f1(t) = min

f2(t) = −f1(t), f3(t) = cos(2πt) and f4(t) = −f4(t).

10

Figure 3: (a) Simulated data set with sampling errors associated to each instant; (b)
smoothed data set; (c) ﬁrst derivatives and (d) second derivatives.

In [28] the authors consider independent errors, while we consider correlated errors
from a Gaussian process. For all the functions (t) has normal distribution with mean
0.4, standard deviation 0.9 and covariance structure given by,

(cid:18)
−(s − t)2

(cid:19)

for s, t ∈ [0, 1] .

,

ρ (s, t) = 0.3 exp

0.3

Figures 3(a) and 3(b) show one of the generated data set and the corresponding
smoothed data set with B-splines, respectively. Each color represents one theoretical
cluster. In Figures 3(c) and 3(d) are displayed the ﬁrsts two derivatives. To prevent
boundary eﬀects we reﬂect one third of the observations at the beginning and at the
end of each measurement. In all the 200 replicates SeqClusFD identiﬁes four groups and
mean correct classiﬁcation rate is 99.85%.

4 Real Data Examples

4.1 Berkeley Growth Study data

The Berkeley Growth Study is one of the best known long-term development research
ever conducted, and the height growth data set introduced by Tuddenham and Snyder
[34] is a reference to illustrate diﬀerent methods for FDA. In particular, the heights of
54 girls and 39 boys measured between 1 and 18 years in 31 unequally spaced moments
(see Figure 4a) are considered one of the most challenging data sets for clustering pur-
poses. More measurements were taken when growth was more rapid, as childhood and
adolescence, and least in the early years, when growth was more stable. As a conse-
quence of that, for the computation of the ﬁrst two derivatives, we need to transform
observations into functions that can be evaluated for any time. We consider a monotonic
cubic regression spline smoothing as suggested by Ramsay et al. [24].

Our ﬁrst objective is to test the eﬀectiveness of SeqClusFD in determining the num-
ber of groups in this data set without regard the gender information. Second, we use

11

00.51(b) 00.51(c) 00.51(d)010.5(a) (a) Growth trajectories

(b) Growth derivatives

Figure 4: Heights of 54 girls (green curves) and 39 boys (orange curves) measured
between 1 and 18 years. Misclassiﬁed curves by SeqClusFD are highlighted with thicker
lines.

the output of SeqClusFD for classifying children. The information from step 1 gives us
the key for understanding how the groups diﬀer.

Using the same parameters as in simulation study for step 1 and 2 of SeqClusFD,
ﬁnal output identiﬁes two clusters. The algorithm ﬁnds in the local step the highest
evidence of clustering in the ﬁrst derivative (growth speed) at the age of 14, which is
coincident with the end of puberty in girls but not in boys yet. In the ﬁnal classiﬁcation
of the data, one cluster is coincident with boys and the other with girls. Only 10
curves are misclassiﬁed and are highlighted with thicker lines in Figures 4a and 4b.
Misclassiﬁcations correspond to a boy with early puberty and 9 girls with late maturity.
The correct classiﬁcation rate is CCR = 89.25%.

This data set has been used recently by Jacques and Preda [16] to compare several
clustering methods for functional data. All analyzed methods make use of the informa-
tion that the number of clusters is two, which could suggest that a priori SeqClusFD
is less competitive. However the CCR for alternative methods are not always superior:
funclust-CCR = 69.89% (Jacques and Preda, [16]); FunHDDC-CCR = 96.77% (Bou-
veyon and Jacques, [7]); fclust-CCR = 69.89% (James and Sugar, [18]); and kCFC-CCR
= 93.55% (Chiou and Li, [9]). SeqClusFD is closer to the most successful (FunHDDC
and KCFC) than to funclust and fclust. In addition, the output of SeqClusFD also in-
cludes an estimate of the number of groups and provides assistance for the interpretation
of the clusters.

Sangalli et al.

[27] also analyzed this data set. Although they ﬁnd more evidence
for the existence of a single cluster, they analyze the case of two clusters and the best
CCR obtained is 88.17%.

12

24681012141618Age24681012141618Age4.2 EGC200 data

The 200 electrocardiograms of ECG200 data set can be found in the UCR Time Series
Classiﬁcation and Clustering website [8]. Data set has two groups, with 133 and 67
electrocardiograms each one, all of them recorded at 96 equally spaced instants. Left side
of Figure 5 shows electrocardiograms (f ) and their ﬁrst (f(cid:48)) and second (f(cid:48)(cid:48)) derivatives
for the two clusters (orange and blue curves). These data has been analyzed by Jacques
and Preda [16], among others, using the same clustering procedures than in the previous
example, except kCFC. The results are: funclust-CCR = 84%; FunHDDC-CCR = 75%;
and fclust-CCR = 74.5%.

Tree structure in central part of Figure 5 shows the results on the three iterations
needed by SeqClusFD to complete the clustering procedure. We use the same parameters
as in simulation study for step 1 and 2.
In the ﬁrst iteration, SeqClusFD algorithm
detects two groups using the information provided by the electrocardiograms at t = 43.
When the algorithm visits again step 1 for the ﬁrst group, gap statistic determines that
there is only one cluster and SeqClusFD stops division. The majority of the curves in
this cluster are blue, 35, and 19 curves are orange. In the second iteration, the other
group is divided in two clusters at t = 41 with the information of the ﬁrst derivatives.
One cluster is a terminal node with 99 orange curves and only 7 blue that could be
considered as misclassiﬁcations. In the third iteration, the remaining observations are
separated in two groups at t = 24 considering the information of the second derivatives.
The green curves are a terminal cluster with 21 blue curves and 15 oranges. Finally, last
four curves can not be separated because of the small size of the group (under 10). When
exploring which are these curves we ﬁnd that they can be considered as outliers. Some
authors, see for instance [16], eliminate these electrocardiograms from the beginning of
the analysis.

Right side of Figure 5 shows the ﬁnal three cluster allocation in f , f(cid:48) and f(cid:48)(cid:48). The
four outliers appear in dark green. Even though this data set only contains two clusters
and we have found three, seen in the results that the group found in second iteration
can be considered the same as the original cluster of orange curves, with 7 curves that
are misclassiﬁed. Assuming that the original cluster of blue electrocardiograms is the
union of the other two (blue and green), the CCR is 79.08%. For this calculation we
have excluded the four outliers in order to compare SeqClusFD results with those of
Jacques and Preda [16].

The result of SeqClusFD classiﬁcation is very good in comparison with the other
methods. Although SeqClusFD–CCR is not the highest CCR, note that the other
methods start from an advantageous position by assuming the number of clusters is
known. SeqClusFD is only overcome by funclust.

13

Figure 5: On the left side, electrocardiograms (f ), ﬁrst (f(cid:48)) and second (f(cid:48)(cid:48)) derivatives
for the two clusters (blue and orange) of ECG200 data set. On the central part, results
of the three iterations executed by SeqClusFD. On the right side, ﬁnal classiﬁcation
with SeqClusFD.

Figure 6: Power demand curves along a day in Italy: (a) 513 curves in cluster 1; and
(b) 516 curves in cluster 2.

4.3

Italy Power Demand data

Italy Power Demand data set can also be found in the UCR Time Series Classiﬁcation
and Clustering website [8]. Data set contains two clusters: cluster 1 with 513 curves
of power demand (see Figure 6(a)); and cluster 2 with 516 curves (see Figure 6(b)).
Note that inside each cluster there are two diﬀerent pattern of consumers. All curves
were measured at 24 equally spaced moments along a day. To increase the grid size on
step 0 of SeqClusFD algorithm, data are smoothed with six equally spaced knot splines
(choosing more knots lead to similar results). To prevent boundary eﬀects we reﬂect one
third of the records at the beginning and end of each record. SeqClusFD is executed
with the same parameters as in previous examples.

SeqclusFD ﬁnds 4 clusters in the two iterations shown in the tree of Figure 7, identi-
fying the original clusters and the two diﬀerent pattern of consumers within them. The
partitions are always based on information provided by ﬁrst derivatives, at t = 7 in the
ﬁrst iteration and t = 23 and t = 9 in the second. The ﬁnal classiﬁcation is shown on
the right side of Figure 7. Orange curves are cluster 1, while green curves are cluster 2.
The darker green and orange curves are misclassiﬁed power demand curves. Collapsing
the four clusters in only two we can calculate CCR, which is 93.49%.

14

UCR Time Series Classification and Clustering website: http://www.cs.ucr.edu/~eamonn/time_series_data/ 200 electrocardiograms with two groups:  133 (orange) and 67 (blue) curves. t = 43 t = 41 t = 24 19 orange 7 blue 15 orange 2610141822-2032610141822-203(a)(b)Figure 7: Results of the two iterations executed by SeqClusFD to classify curves of
power demand in Italy. First derivatives are shown in all iterations.

5 Conclusion

We introduce SeqClusFD, a hierarchical clustering algorithm for functional data that
simultaneously determines the number of groups and the clustering conformation. As
we are interested in cluster structures that takes into account the shape of the curves,
we also consider the speed and acceleration functions linked to the original curves. It is
a sequential procedure, that successively runs two steps, it searches the instant of time,
along the functions and its derivatives, with most clustering ability, according to the
gap statistics criterion and one dimensional k-means algorithm. Once this initial groups
are conformed it builds up a functional boxplot for each group, with the objective
of reallocating possibly misclassiﬁed data. Moreover, it provides helpful information
towards the grouping structure.

Although the SeqClusFD is based on the one dimensional gap statistic, alternative
methods to estimate the number of groups could be considered. Similarly, the functional
boxplot could be adapted to consider diﬀerent depth deﬁnitions for functional data.
Several proposals can be found in [23].

The algorithm could be easily extended to the multivariate case, where each data is a
vector of functions (see [4]). In the local step, any clustering method could be considered.
On the global revision step, the boxplot could be designed using band depth proposed by
Lopez-Pintado et al. [21] for multivariate functional data. The eﬃciency of the method
should decrease with the dimensionality.

Finally, the output of the algorithm exhibits number of groups and clustering alloca-
tion. In addition, it gives information about the key moments t ∈ [a, b] and features, i.e.
X(t), X 1(t) or X 2(t). This yields to a better understanding of the clustering structure.
This topic is closely related to the variable selection problem that has been widely stud-
ied recently in the supervised classiﬁcation framework, see for instance Tian and James

15

  [31] and Martin-Barragan et al. [22]. However, this problem has not been analyzed for
unsupervised classiﬁcation problems, at the best of our knowledge.

References

[1] C. Abraham, P.A. Cornillon, E. Matzner-Løber and N. Molinari, “Unsupervised
curves clustering using B-Splines”, Scandinavian Journal of Statistics, 30, 581–595,
1993.

[2] A.M. Alonso, D. Casado and J. Romo, “Supervised classiﬁcation for functional data:
a weighted distance aprproach”, Computational Statistics and Data Analysis, 56,
2334–2346, 2012.

[3] J. Basak and R. Krishnapuram, “Interpretable hierarchical clustering by construc-
tion an unsupervised decision tree”. IEEE Transactions on Knowledge and Data
Engineering, 17, 121-132, 2005.

[4] J.R. Berrendero, A. Justel, and M. Svarc, “Principal components for multivariate
functional data”. Computational Statistics and Data Analysis, 55, 2619–2634, 2011.

[5] J.G. Booth, G. Casella and J.P. Hobert, “Clustering using objective functions and

stochastic search”, Journal of the Royal Statistical Society, B, 70, 119–139, 2008.

[6] M. Boull´e, R. Guigour`es, and F. Rossi, “Non parametric hierarchical clustering of
functional data”. Advance in Knowledge Discovery and Management, Studies in
Computational Intelligence, 5, 15–35, 2014.

[7] C. Bouveyron and J. Jacques, “Model-based clustering of time series in group-speciﬁc
functional subspaces”. Advances in Data Analysis and Classiﬁcation, 5, 281-300,
2011.

[8] Y. Chen, E. Keogh, B. Hu, N. Begum, A. Bagnall, A. Mueen and G. Batista. The
UCR Time Series Classiﬁcation Archive, 2015. (http://www.cs.ucr.edu/~eamonn/
time_series_data/)

[9] J.M. Chiou, and P.L. Li, “Functional clustering and identifying substructures of

longitudinal data”. Journal of the Royal Statistical Society, B, 69, 679-699, 2011.

[10] J.A. Cuesta Albertos and R. Fraiman, “Impartial Trimmed k-means for Functional

Data”. Computational Statistics and Data Analysis, 51, 4864–4877, 2007.

[11] A. Cuevas, “A partial overview of the theory of statistics with functional data”.

Journal of Statistical Planning and Inference, 147, 1-23, 2014..

16

[12] R. Fraiman, B. Ghattas and M. Svarc, “Interpretable clustering using unsupervised

binary trees”. Advances in Data Analysis and Classiﬁcation, 7, 125–145, 2013.

[13] C. Genolini and B. Falisard, “KmL: k-means for longitudinal data”. Computational

Statistics, 25, 317–328, 2010.

[14] R. Giraldo, P. Delicado and J. Mateu, “Hierarchical clustering of spatially correlated

functional data”. Statistica Neerlandica, 66, 403–421, 2012.

[15] F. Ieva, A. M. Paganoni, D. Pigoli and V. Vitelli, “Multivariate functional clustering
for the morphological analysis of electrocardiograph curves”. Journal of the Royal
Statistical Society: Series C (Applied Statistics), 62, 401-418, 2013.

[16] J. Jacques and C. Preda, “Model-based clustering of functional data.”, Computa-

tional Statistics and Data Analysis, 71, 92–106, 2014.

[17] J. Jacques and C. Preda, “Functional Data Clustering: A Survey.”, Advances in

Data Analysis and Classiﬁcation, 8 (3), 231–255, 2014.

[18] G. James and C. Sugar, “Clustering for sparsely sampled functional data”. Journal

of the American Statistical Association, 98, 397-408, 2003.

[19] B Liu, Y. Xia and P.S. Yu, “Clustering through decision tree construction”. CIKM
00. In: Proceedings of the ninth international conference on information and knowl-
edge management. ACM, New York, NY, USA, pp 20-29, 2000.

[20] S. L´opez-Pintado and J. Romo, “On the concept of depth for functional data”.

Journal of the American Statistical Association, 104, 718-734, 2009.

[21] S. L´opez-Pintado, Y. Sun, J.K. Lin and M.G. Genton, “Simplicial band depth
for multivariate functional data”. Advances in Data Analysis and Classiﬁcation, 8,
321–338, 2014.

[22] B. Martin-Barragan, R. Lillo and J. Romo, “Interpretable support vector machines
for functional data”. European Journal of Operational Research, 232, 146.–155, 2014.

[23] K. Mosler, “Depth Statistics”. Robustness and Complex Data Structures. Editors:

Becker, C., Fried, R. and Kuhnt, S. Springer Berlin Heidelberg. 17–34, 2013.

[24] J. Ramsay, G. Hooker and S. Graves, Functional Data Analysis with R and Matlab.

Springer, New York, 2009.

[25] J. Ramsay and B.W. Silverman, Functional Data Analysis (2nd ed). Springer, New

York, 2005.

17

[26] L.M. Sangalli, P. Secchi, S. Vantini and V. Vitelli, “k-means alignment for curve

clustering”. Computational Statistics and Data Analysis, 54, 1219–1233, 2010.

[27] L.M. Sangalli, P. Secchi, S. Vantini and V. Vitelli, “Functional clustering and align-
ment methods with applications”. Communications in Applied and Industrial Math-
ematics, 1, 205–224, 2010.

[28] N. Serban and L. Wasserman, “CATS: Cluster Analysis by Transformation and

Smoothing”. Journal of the American Statistical Association, 100, 990–999, 2005.

[29] Y. Sun and M.G. Genton, “Functional boxplots”. Journal of Computational and

Graphical Statistics, 20, 316–334, 2011.

[30] T. Tarpey and K.K.J. Kinateder, “Clustering functional data”. Journal of classiﬁ-

cation, 20, 93–114, 2003.

[31] T.S. Tian and G.M. James, “Interpretable dimension reduction for classifying func-

tional data”. Computational Statistics and Data Analysis, 57, 282–296, 2013.

[32] R. Tibshirani, G. Walther and T. Hastie, “Estimating the number of data clusters
via the gap statistic”. Journal of the Royal Statistical Society, B, 63, 411-423, 2001.

[33] S. Tokushige, H. Yadohisa and K. Inada, “Crisp and Fuzzy k-means clustering al-
gorithms for multivariate functional data”. Computational Statistics, 21, 1–16, 2008.

[34] R.D. Tuddenham and M.M. Snyder, M.M., Physical growth of California boys and
girls from birth to eighteen years, Tech. Rep. 1, University of CaliforniaPublications
in Child Development., 1954.

Acknowledgments

Ana Justel is supported by MINECO (Spain), grants CTM2011-28736/ANT and CTM2013-
47381-P

18

