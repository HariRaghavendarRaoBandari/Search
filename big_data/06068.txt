6
1
0
2

 
r
a

 

M
9
1

 
 
]

B
D
.
s
c
[
 
 

1
v
8
6
0
6
0

.

3
0
6
1
:
v
i
X
r
a

Measuring the Accuracy of Linked Data Indices

Thomas Gottron

Institute for Web Science and Technologies

University of Koblenz-Landau

Universitätsstraße 1

56070 Koblenz

gottron@uni-koblenz.de

Abstract: Being based on Web technologies, Linked Data is distributed and decen-
tralised in its nature. Hence, for the purpose of ﬁnding relevant Linked Data on the
Web, search indices play an important role. Also for avoiding network communication
overhead and latency, applications rely on indices or caches over Linked Data. These
indices and caches are based on local copies of the original data and, thereby, intro-
duce redundancy. Furthermore, as changes at the original Linked Data sources are
not automatically propagated to the local copies, there is a risk of having inaccurate
indices and caches due to outdated information. In this paper I discuss and compare
methods for measuring the accuracy of indices. I will present different measures which
have been used in related work and evaluate their advantages and disadvantages from
a theoretic point of view as well as from a practical point of view by analysing their
behaviour on real world data in an empirical experiment.

1

Introduction

The Linked Open Data (LOD) movement implements Tim Berner-Lee’s vision of a Web of
Data. The principles underlying Linked Data enable data providers to model, interlink and
publish their data on the Web in a distributed and decentralised way. On the consumer’s
side the Web-oriented technological basis (i.e. using RDF and HTTP) permits to easily
make use of the data and integrate it into various applications. These beneﬁts and advan-
tages of the Linked Data idea lead to, on the one hand, more and more data providers to
contribute to the Web of Data and, on the other hand, more and more developers to spawn
new applications making use of this data. However, the growth and conceptual nature of
Linked Data poses some technological challenges. The distributed nature of Linked Data
calls for search engines and indices to provide data catalogues of what kind of data is
available and where it can be found. Furthermore, some applications use data caches to
avoid communication overhead when accessing data on the Web.
Such indices and caches correspond to local views on the data. These local views might
be inaccurate in the sense that they do not reﬂect the state of the data at the original data
sources. There are two main reasons for indices constituting an inaccurate view on the
data. The ﬁrst reason is rooted in the decentralised and dynamic nature of Linked Data.

1 INTRODUCTION

2

Figure 1: Inaccurate indices I can occur when computed over outdated data R or when computed
in an approximative way over current data RGS. A measure of accuracy needs to compare such an
index I to a perfect, gold standard index IGS computed over RGS in a lossless way.

Changes at the original data sources are not automatically propagated to the applications
and their indices and data caches. Accordingly a task which needs to be addressed in
the context of Linked Data applications is the active maintenance of indices. This means,
that applications have to synchronise their local view on the data with the data at the
origin [UHH+10, DKG14]. A second reason are applications which build their indices
only in an approximate way [KGSS12]. This happens, for instance, to achieve scalability
and to be able to efﬁciently process large volumes of Linked Data. Figure 1 illustrates
these two paths which can lead from a Linked Data set RGS and an older version of this
data set R to an index I which is inaccurate compared to a perfect index IGS.
In both cases, there is a certain tradeoff between index accuracy and the commitment
of limited resources. When updating and maintaining an index, the limited resource is
typically network bandwidth. Spending more effort on updating an index requires more
bandwidth but will yield indices of higher accuracy. Likewise, the approximate computa-
tion of indices can usually be inﬂuenced by a parameter which trades index accuracy for
required computational resources. Hence, given the limitations of available resources the
providers of LOD indices will need to strike a balance between the effort for building and
maintaining an index and the quality of service, i.e. the accuracy of their index.
While measuring the costs for computation and network bandwidth has been addressed in
many other ﬁelds, measures for the accuracy of a Linked Data index are less established.
In related work, different methods for measuring index accuracy have been described and
applied. But, rarely more than one approach is used. This makes it difﬁcult to compare
results. Thus, the idea of this paper is to compare different measures for index accuracy

RGSIGSk1	  k2	  k3	  ...	  kn	  d1,1	  d1,2	  d1,3	  ...	  d2,1	  d2,2	  	  d3,1	  d3,2	  d3,3	  ...	  dn,1	  dn,2	  dn,3	  ...	  RIk1	  k2	  k3	  ...	  kn	  d1,1	  d1,2	  d1,3	  ...	  d2,1	  d2,2	  	  d3,1	  d3,2	  d3,3	  ...	  dn,1	  dn,2	  dn,3	  ...	  compare	  compute	  compute	  approximate	  Accuracy	  Measure	  Index	  Data	  2 RELATED WORK

3

on a theoretical and practical level. This should help in judging and comparing existing
results as well as to support the choice of measures to use in future research work in this
ﬁeld. In more detail, this paper will make three contributions:

Survey of Measures: Following a review of related work, this paper provides an exten-
sive overview of approaches and methods used to measure the accuracy of index
structures over Linked Data. All methods are presented in a uniﬁed and formal way
for ease of comparison.

Theoretical Comparison: Considering the general task of RDF indices and the speciﬁc
Web based setting of Linked Data, this paper compares approaches and methods to
measure the accuracy of LOD indices w.r.t their theoretical limitations and advan-
tages.

Practical Comparison: By using an established data set of evolving Linked Data the
paper analyses the approaches and methods to measure the accuracy of LOD indices
w.r.t their behaviour in practice. In particular it presents an analysis of the correlation
of the measures to understand how far similar or different notions of index accuracy
are captured by the discussed measures.

As a next step we look at related work in Section 2 to get an overview of the context in
which LOD indices are used and evaluated for their accuracy. This section will also pro-
vide a ﬁrst and brief review of the methods used to measure index accuracy. Subsequently,
Section 3 will present an abstract and formal representation for Linked Data indices. This
formalisation allows to abstract from concrete implementations and serves as basis for the
uniﬁed deﬁnition of index accuracy measures in Section 4. The theoretic analysis and
comparison of the measures is presented in Section 5 and the practical and empirical com-
parison in Section 6. The ﬁndings are discussed in 7 and the paper is concluded with a
summary and an outlook at future work.

2 Related Work

In recent years, various index models over LOD have been proposed. Many of them focus
on speciﬁc aspects of the data or are dedicated to support application speciﬁc tasks. When
looking at the RDF basis of LOD, one has to consider also the works on indices for RDF
triple stores, such as Hexastore [WKB08] or RDF3X [NW10]. These indices are intended
for optimising access to a single, centrally managed data storage solution. In this case,
accuracy of the index is not an issue as all changes in the data are under control of the
storage solution and are reﬂected in the index immediately.
More speciﬁc to LOD are indices for optimising federated queries [NW09], on-demand
queries on the Web [HHK+10] or looking up data sources relevant to particular schema
patterns [KGSS12]. However, most of these approaches do not deal with index accuracy
either. Their focus is more on how to implement or make use of the index in speciﬁc
scenarios.

3 ABSTRACT INDEX MODELS FOR LINKED DATA

4

So far, only few publications address the issue of Linked Data index accuracy. This
only happens in a context where the dynamics of data is explicitly addressed [KAU+13,
DSGG13, DGSG14], where implementations trade a loss of index accuracy for an efﬁ-
cient and scalable index computation [KGSS12, GP13] or where the accuracy of indices
is under investigation itself [GG14, Got14, GP13]. These methods used will be reviewed
and explained in more detail in Section 4.
In the context of the “classical” Web of documents, there is some work on index accuracy
of search engines [LG99]. Such analysis is performed by comparing the coverage of rele-
vant web documents for speciﬁc queries. The results of several individual search engines
can be compared to the union of their results as well as to a separately obtained collection
of relevant documents which serves as ground truth. Other works attempt to measure age
and freshness of an index [CGM00]. Incorporating time information into a change predic-
tion allows for more efﬁcient and effective synchronisation plans between Web based data
sources. Yet another direction of research addressed the question of guarantees of fresh-
ness of cached copies for web documents [BC00]. However, measuring the freshness and
accuracy of search indices for Web documents is different from the accuracy of Linked
Data indices insofar as the indices are all of the same type, namely mapping keywords to
documents. In the context of Linked Data indices can be of different types and address
speciﬁc information needs which are directly encoded in the index structure [GG14].

3 Abstract Index Models for Linked Data

As indicated in the previous section, there is a wide range of different index models over
LOD. In this paper we will not look at speciﬁc implementations of indices, but rather
operate on an abstract level. Therefore, we now brieﬂy recall a formalisation of abstract
index models over Linked Data [GG14]. This formalisation will serve as basis for a generic
and uniﬁed deﬁnition of accuracy measures in Section 4.
On the LOD cloud we can assume data items to be in the form of NQuads [Car14]. In an
NQuad (s, p, o, c) the entries s, p, and o correspond to the subject, predicate and object of
the RDF triple statement. The entry c provides the context, i.e. the data source on the Web
where this information has been published.
Thus, we deﬁne an index model for LOD over a data set R of (s, p, o, c) NQuads. Depend-
ing on the application scenario an index will typically not serve to store all information
contained in the NQuads. Rather it will deﬁne a derived set D of managed data items
which are of interest in the scenario and typically constitute a restriction of the quads to
smaller tuples. Such restrictions can be, for instance, the RDF triples or even single en-
tries, e.g. the subject or the context URIs. Furthermore, an index model has to deﬁne a set
K of key elements which are used to lookup and retrieve data items. These key elements
are used as domain for a selection function σ : K → P(D) to select a subset of the data
items in the index.
Eventually, an abstract index model is deﬁned as a tuple (D,K, σ) of the stored data
items D, the key elements K used for the lookup index and the selection function σ to

4 MEASURING ACCURACY

5

Figure 2: A data set R of NQuads and the elements of an index model over this data.

retrieve data from the index. Figure 2 illustrates the elements of an abstract index model I
computed over a data set R.

4 Measuring Accuracy

The formal deﬁnition of abstract index models in Section 3 enables us to now formalise
measures for index accuracy in a uniﬁed way. As already indicated in Figure 1, measuring
the accuracy of an index will be based on a perfect index which is entirely accurate. In
the following, this gold standard index will be referred to as IGS = (DGS,KGS, σGS)
which is built over a data set RGS. The potentially inaccurate index for which we want to
determine its accuracy will be referred to as I = (D,K, σ). Note, that it is not necessary
to distinguish between I being inaccurate because it was built over an outdated data set R
or because it was computed in an approximative way over the gold standard data set RGS.
The presented measures are divided into four families, based on their methods and un-
derlying ideas for measuring accuracy: (1) index agnostic measures, (2) measures based
on the overlap of key elements, (3) distribution based measures and (4) retrieval based
measures.

4.1

Index Agnostic Measures

A direct way to measure the potential impact of data changes on indices is to simply
compare the differences in the data itself. As indicated in Figure 3 such an accuracy
measure would accordingly ignore the index and operate only on the original input data.

Index	  k1	  k2	  k3	  ...	  kn	  d1,1	  d1,2	  d1,3	  ...	  d2,1	  d2,2	  	  d3,1	  d3,2	  d3,3	  ...	  dn,1	  dn,2	  dn,3	  ...	  Kσ(cid:1)D()Data	  items	  /	  Payload	  	  Keys	  Data	  s	  o	  p	  c	  s	  o	  p	  c	  s	  o	  p	  c	  s	  o	  p	  c	  ...	  RNQuads	  4 MEASURING ACCURACY

6

Figure 3: Index agnostic accuracy measures consider only the original data set.

One established metric for comparing the data sets is the Jaccard similarity. Applying it
to RGS and R, it is deﬁned as:

Jaccard(RGS, R) =

|RGS ∩ R|
|RGS ∪ R|

(1)

A higher similarity value indicates a larger overlap between the data sets while a low value
indicates a stronger deviation, i.e. change in the data.
Such an index agnostic measures can always be computed over two versions of a data
set at different points in time. Therefore, it is independent of the index model applied.
While this might be suitable to measure the evolution of the data in general, it does not
make any statement about the impact on speciﬁc index models used in speciﬁc use case
scenarios. Furthermore, it requires the availability of the full raw data sets and is by design
not applicable in settings were indices are computed in an approximative way over the gold
standard data set RGS.

4.2 Overlap of Key Elements

A relatively simple approach for comparing indices themselves is to look at the key ele-
ments (cf. Figure 4). These are the elements used to retrieve information from the index
and, thus, provide the primary access point to the contained data. Hence, the degree of how
far an index reﬂects the right key elements can already give some insights into its accuracy.
Note, that this type of measure will not consider at all the data elements. In particular, it
might assert an index a perfect accuracy even if the index provides wrong results.

Data	  s	  o	  p	  c	  s	  o	  p	  c	  s	  o	  p	  c	  s	  o	  p	  c	  ...	  NQuads	  Index	  k1	  k2	  k3	  ...	  kn	  d1,1	  d1,2	  d1,3	  ...	  d2,1	  d2,2	  	  d3,1	  d3,2	  d3,3	  ...	  dn,1	  dn,2	  dn,3	  ...	  Data	  items	  /	  Payload	  	  Keys	  4 MEASURING ACCURACY

7

Figure 4: Accuracy measures based on the key elements do not make use of any information about
the data items.

Technically, this approach comes down to comparing two sets: KGS and K. There are
various measures for comparing sets. In the context of measuring the accuracy of LOD
indices, also here the Jaccard similarity has been used [GG14]:

Jaccard(KGS,K) =

|KGS ∩ K|
|KGS ∪ K|

(2)

Again, a higher similarity value indicates a larger overlap between the sets of key elements
while a low value indicates a stronger deviation. In this way we can get an impression of
how stable is the set of elements used for indexing in the different indexing approaches.
A further measures to operate on the set of key elements is the asymmetric recall, which
measures which fraction of the gold standard key set is covered in an inaccurate index.

r(KGS,K) =

|KGS ∩ K|
|KGS|

(3)

The rational for using this variation is that key elements in an inaccurate index which
are not available in the gold standard might not be used to retrieve information as well.
However, this rational is debatable as a query with one of this key elements missing in the
gold standard will lead to retrieving false positive information from the inaccurate index,
a fact which is neglected by recall.

4.3 Distribution Based Measures

One option to include some more information in the evaluation of index accuracy is pur-
sued by distribution based approaches [GG14, Got14]. The advantage is that these ap-
proaches take into consideration the volume of data items retrieved for a given key ele-
ment. This idea is visualised in Figure 5 by omitting the concrete data items assigned to a

Index	  k1	  k2	  k3	  ...	  kn	  Keys	  Data	  s	  o	  p	  c	  s	  o	  p	  c	  s	  o	  p	  c	  s	  o	  p	  c	  ...	  NQuads	  d1,1	  d1,2	  d1,3	  ...	  d2,1	  d2,2	  	  d3,1	  d3,2	  d3,3	  ...	  dn,1	  dn,2	  dn,3	  ...	  Data	  items	  /	  Payload	  	  4 MEASURING ACCURACY

8

Figure 5: Distribution based measures compare distributions of the data over key elements.

key element. The volume of the data is used to model probabilistic distributions for which
there are several well established and well understood metrics for comparison. However,
they do not distinguish the data items but merely use their count information. Hence, also
here there is a certain risk of asserting a perfect index accuracy while the index actually is
not accurate.
Estimating a distribution over an index is based on determining how probable it is for
an element to belong to one speciﬁc index key k and—conversely—the amount of data
obtained when querying the index for this key element k. If we consider the distribution
over an index I = (D,K, σ), this effectively corresponds to modelling a random variable
X taking values of the key elements K. The estimated density gives the distribution of
this random variable X. This means we determine the probability P (X = k) for each
entry k ∈ K to be associated with a data item. To estimate the densities we can use the
count information of data elements associated with the key elements in an index. This
corresponds to using a maximum likelihood estimation to derive the probability, i.e.

P (X = k) =

(cid:80)
|σ(k)|
k(cid:48)∈K |σ(k(cid:48))|

(4)

where σ(k) indicates the result set obtained from an index when querying for a speciﬁc
key element k.
As we consider inaccurate or outdated indices it is likely that the set of key elements does
not match (see also Section 4.2). Thus, it can happen that certain key elements might
be available in the perfect gold standard index, but not in an outdated or approximated
index. When comparing densities over indices we need to consider the effect of such zero-
size entries. Using a maximum likelihood estimation for the densities would lead to zero
probabilities for certain events which renders comparison of densities impractical. We
apply smoothing to overcoming zero probabilities. We make use of Lidstone smoothing
which adds a small constant value of λ to all counts obtained for the number of results

Index	  k1	  k2	  k3	  ...	  kn	  Keys	  Data	  s	  o	  p	  c	  s	  o	  p	  c	  s	  o	  p	  c	  s	  o	  p	  c	  ...	  NQuads	  Data	  items	  /	  Payload	  	  4 MEASURING ACCURACY

9

|σ(k)|. The parameter λ is set to 0.5 in our experiments which has shown to provide good
results in prior work [Got14].
The main idea of measuring index accuracy on the basis of data distributions is to compare
their density function. Common metrics to compare density functions are cross entropy,
Kullback-Leibler divergence and perplexity. Let us brieﬂy review the deﬁnitions of these
metrics and explain their interpretation.
Assuming two probability distributions P (X) and PGS(X) for the inaccurate index I and
the gold standard index IGS. Then cross entropy H(PGS, P ) is deﬁned as:

H(PGS, P ) = − (cid:88)

k∈K∪KGS

PGS(X = k) log(P (X = k))

(5)

In the context of compression theory, cross entropy can be interpreted as the average num-
ber of bits needed to encode events following the distribution PGS based on an optimal
encoding scheme derived from P . If the two distributions are equivalent, then cross en-
tropy corresponds to the normal entropy H(PGS). The entropy of PGS also provides a
lower bound for cross entropy. Based on this interpretation, the Kullback-Leibler diver-
gence gives the deviation in entropy (or overhead in encoding) relative to the entropy for
PGS and is deﬁned as:

DKL(PGS, P ) = H(PGS, P ) − H(PGS)

(6)

Therefore, if two distributions are equivalent, they have a Kullback-Leibler divergence of
zero.
Perplexity, instead, provides an evaluation of a distribution by giving the number of events
(in our case key elements) which under a uniform distribution would yield the same en-
tropy value. As such it is considered to be more easily interpretable by humans than the
somewhat abstract entropy values. Perplexity itself is deﬁned over entropy values, though.
Here we formulate it directly on the basis of cross entropy:

PP(PGS, P ) = 2H(PGS ,P )

(7)

Perplexity is a standard metric for evaluating probabilistic models. The lower the perplex-
ity is, the better a model explains observed data and the more truthful are its estimates of
the probabilities. Furthermore, the interpretation of perplexity relative to the event space
of key elements allows for a normalisation. The normalised perplexity PPnorm is deﬁned
as:

PPnorm(PGS, P ) =

PP(PGS, P )

|KGS|

(8)

4 MEASURING ACCURACY

10

Figure 6: Retrieval based measures include the entire index in the evaluation.

4.4 Retrieval Based Measures

The third type of measures are directly evaluating the results of queries posed towards an
index. Precision and recall are two such measures which are typically employed in Infor-
mation Retrieval. While in general, this approach can be based on a ﬁxed and predeﬁned
set of queries, such a set of queries might not be available in most cases. Furthermore, a
ﬁxed set of queries will always address only a certain fraction of the index. Thus, an alter-
native approach which has been followed in related work is to pose virtually all possible
queries which could be asked over the gold standard as well as the inaccurate index. In this
way, the measures make use of all information stored in the index as indicated in Figure 6.
For one speciﬁc query, i.e. a key item k, precision and recall are deﬁned as follows:

p(k) =

r(k) =

|σ(k) ∩ σGS(k)|

|σ(k)|

|σ(k) ∩ σGS(k)|

|σGS(k)|

(9)

(10)

There is, however a distinction to be made with respect to the way of aggregating results
over the set of all queries. Namely, there are micro- and macro-averages which can be
computed.
Macro-average builds an average over all precision or recall values for all of the queries.
Formally this gives (here for the example of precision):
|σ(k) ∩ σGS(k)|

(cid:88)

(11)

pmacro =

1

|K ∪ KGS|

k∈K∪KGS

|σ(k)|

Index	  k1	  k2	  k3	  ...	  kn	  d1,1	  d1,2	  d1,3	  ...	  d2,1	  d2,2	  	  d3,1	  d3,2	  d3,3	  ...	  dn,1	  dn,2	  dn,3	  ...	  Data	  items	  /	  Payload	  	  Keys	  Data	  s	  o	  p	  c	  s	  o	  p	  c	  s	  o	  p	  c	  s	  o	  p	  c	  ...	  NQuads	  5 THEORETIC ANALYSIS OF MEASURES

11

Micro-average, instead, aggregates the set sizes of the observed result sets and their inter-
section. Precision and recall are then computed on these aggregated count information.
Formally, it is deﬁned as:

(cid:80)

(cid:80)

k∈K∪KGS

pmicro =

|σ(k) ∩ σGS(k)|

k∈K∪KGS

|σ(k)|

(12)

The difference of the two ways for computing averages can be seen as follows. Macro-
average gives equal weight to each query. Hence a lot of good or bad performing queries
which provide small result sets will strongly affect the overall score. Computing a micro-
average, instead, considers also the size of the results set in the overall aggregated score.
Larger result sets and their performance have a stronger impact on the overall result.

5 Theoretic Analysis of Measures

Let us consider the measures introduced above in Section 4 and compare them on a the-
oretic level. To this end we will consider the following four criteria and argue for and
against the individual approches, independent of a concrete scenario, index model or data
set.

Sensitivity to Data Changes Not all measures perfectly reﬂect the accuracy of an index.
In some cases a change in the data and a loss of accuracy of an outdated index will
not be captured by an accuracy measure. The higher the sensitivity of a measure
the more precise and reliable it is in evaluating an index’s ability to provide truthful
results. In the context of this paper the theoretic sensitivity is judged in a relative
and categorial way, by assigning the measures a low (–), medium (o) or perfect (+)
sensitivity. A perfect sensitivity implies that each error in an inaccurate index will
be detected.

Data Volume When computing the measures, one question is how much data needs to
be obtained from a perfect index at the original data source. A smaller volume of
data which needs to be transfered from separate indices makes it easier to com-
pute the measures remotely and only request the needed information from the actual
computing nodes running the index. Furthermore, the computational complexity of
the presented measures is linear in the amount of data items to consider. Thus, the
data volume indicates how long the computation of the measures will take. For this
reason, the data volume will be given in big O notation based on the elements of
abstract index models.

Normalised Value Range A normalised value range is favourable in an index accuracy
measure. If a measure provides a normalised value range it is easier to compare its
values across experiments. This can affect both: the comparison of accuracy of the
same index over different data sets as well as the comparison of different indices

6 EMPIRICAL COMPARISON OF MEASURES

12

√
over the same data set. Having a normalised value range is a binary feature of an
) or not (×).
accuracy measure, i.e. a measure can either have a normalised range (
Index Agnostic If a measure is index agnostic it can be computed independently of a
concrete index model or implementation. The advantage is, that the computation of
one single value over the data is sufﬁcient to judge a (potential) impact on different
types of indices. A conceptual disadvantage is, that index agnostic measures cannot
be used to evaluate approaches performing an approximate index computation. Also
being index agnostic is a binary feature, i.e. the measure is index agnostic (
) or
not (×).

√

Table 1 summarises the theoretic evaluation of the measures from Section 4 w.r.t the above
mentioned criteria. Within each family of measures, the characteristics are mostly the
same. Typically, the measures have the same sensitivity, require the same volume of data
and are all either index speciﬁc or index agnostic.
Judging from this theoretic point of view, the retrieval based measures seem favourable.
After all they directly address the purpose of an index and measure the accuracy using
the core functionality. Furthermore, they provide normalised values which renders them
suitable for comparisons across data sets and types of indices. However, they require a
large volume of data. Basically, all information of the entire index has to be considered for
a comparison, which—depending on the type of index—might be as costly as comparing
the entire underlying RDF data set. Under this aspect, the measures using the overlap
of key elements and the distribution based measures need less information. They merely
operate on the set of key elements and—in the case of the distribution based measures—
a size estimation of how many elements are associated with a key element. Such size
estimations can be realised using count operators over the existing indices, causing only
a slight computational overhead. Their drawback is that they might not always capture
perfectly the accuracy of an index.

6 Empirical Comparison of Measures

Following the theoretical evaluation of the measures we now consider how they perform in
practice. The question we want to answer is which measures are closely correlated to each
other in practice. The aim is to ﬁnd out which measures seem to be redundant and do not
need to be compared. This might help in reducing overhead in practical evaluation and in
identifying efﬁcient measures to approximate or even substitute more complex measures.
In particular, a correlation analysis can also address the question how large is the effect of
data changes in practice on the less sensitive key element and distribution based measures,
i.e. how far they still agree with the measures having a perfect sensitivity.
To perform such an evaluation we need a set of indices (for the index speciﬁc measures)
and a data set which undergoes realistic changes. As data set we can employ the weekly
snapshots of a well deﬁned part of the Linked Data cloud provided by the Dynamic Linked
Data Observatory (DyLDO) [KAU+13]. The range of index models is wide and it is

6 EMPIRICAL COMPARISON OF MEASURES

13

Table 1: Theoretic analysis of measures for index accuracy

Measure
Index agnostic measures
Jaccard (RDF triples)
Overlap of key elements
Jaccard (Key elements)
Recall (Key elements)
Distribution based measures
Cross-Entropy
KL-Divergence
Perplexity
Normalised Perplexity
Retrieval based measures
Recall (macro-avg)
Precision (macro-avg)
Recall (micro-avg)
Precision (micro-avg)

Sensitivity Normalised range

Volume

Index agnostic

+

–
–

o
o
o
o

+
+
+
+

√

√
√

×
×
×
√

√
√
√
√

O(|R| + |RGS|)

O(|K| + |KGS|)
O(|K| + |KGS|)

O(|K| + |KGS|)
O(|K| + |KGS|)
O(|K| + |KGS|)
O(|K| + |KGS|)

O(|D| + |DGS|)
O(|D| + |DGS|)
O(|D| + |DGS|)
O(|D| + |DGS|)

√

×
×

×
×
×
×

×
×
×
×

beyond the scope of this paper to evaluate all types of index models. Thus, let us chose a
few selected index models which cover a good range of index granularities and scope.
The indices are taken vom related work and operate on different types of data they return:
(1) full triple statements (Dtriple), (2) single URIs (DURI) or (3) solely the source where
relevant information can be found on the Linked Data cloud (Dcontext). In the following,
the index models are described brieﬂy along with a formal deﬁnition for clarity. More
details can be found in original publications.
Subject Index: This type of index simply uses URIs as key elements and its selection
function returns all triple statement containing a given URI in the subject position. Using
the abstract notation introduced in Section 3, such an index can be formalised as IS :=
(Dtriple,KS, σS) where:

• Key elements: KS := {s ∈ U | ∃p, o, c : (s, p, o, c) ∈ R}
• Selection function: σS(k) := {(s, p, o) | s = k}

RDF Type Index: Making use of the speciﬁc semantics behind rdf:type statements, this
index is used to look up all entities (i.e. URIs) which are of a particular RDF class
type [GG14]. Formally it can be deﬁned as IT := (DURI,KT, σT), with:

• Key elements: KT := {o | ∃s, c : (s, rdf:type, o, c) ∈ R} ∪ {s | ∃c : (s, rdf:type,

rdfs:Class, c) ∈ R}

• Selection function: σT(k) := {s | ∃c : (s, rdf:type, k, c) ∈ R}

RDF Type Set (TS) Index: Extending the RDF type class index, this index gives all
entities for a speciﬁc set of class types, i.e. all entities which satisfy being of exactly all
types given in the set (and of no other type) [KGSS12]. The formal deﬁnition is given by
ITS := (DURI,KTS, σTS), where:

6 EMPIRICAL COMPARISON OF MEASURES

14

• Key elements: KTS := P(KT)
• Selection function: σTS(k) := {s | (∀t ∈ k : (∃c : (s, rdf:type, t, c) ∈ R)) ∧

∀(s, rdf:type, o, c) ∈ R : (o ∈ k)}

Property Set (PS) Index: Using sets of predicates—also known as characteristics sets—
this index retrieves entities based on the properties with which they have been described
in RDF [NW09]. Formally it can be deﬁned as IPS := (DURI,KPS, σPS), with:

• Key elements: KPS := P(Kp)
• Selection function: σPS(k) := {s | (∀p ∈ k : (∃o, c : (s, p, o, c) ∈ R)) ∧

∀(s, p, o, c) ∈ R : (p ∈ k)}

Extended Characteristic Set (ECS) Index: This index essentially combines the TS
and PS index and characterises Linked Data entities by both:
their types and proper-
ties [DSGG13]. Formally it is deﬁned by IECS := (DURI,KECS, σECS), where:

• Key elements: KECS := P(KP ∪ KT)
• Selection function: σECS(k) := {s | (∀p ∈ k∩KP : (s ∈ σPS(p)))∧ (∀t ∈ k∩KT :

(s ∈ σTS(t)))}

SchemEX Index over data sources: The SchemEX index combines induced schematic
information about the types of subject URIs, their properties and the types of object URIs
they are linked to [KGSS12]. To this end it re-uses the concepts of type clusters for the do-
main and range of observed property sets. Those concepts are combined using a restricted
bi-simulation over property sets and a stratiﬁcation of the entities in equivalence classes
of type sets. Its formal deﬁnition is given by ISchemEX := (Dcontext,KSchemEX, σSchemEX),
where:

• Key elements: KSchemEX := P(KTS × P(KPS × KTS))
• Selection function: σSchemEX(k = (ts, E)) := {c | ∃s ∈ σTS(ts) ∧ ∀(ps, ts2) ∈ E :

(s ∈ σPS(ps) ∧ ∃o ∈ σTS(ts2) : (∀p ∈ ps : ((s, p, o, c) ∈ R))}

For the empirical evaluation we computed these indices over 77 snapshots provided by
the DyLDO data set. Assuming an index has been computed over the initial snapshot, we
evaluate it accuracy compared to the later on versions of the data sets. This corresponds
to ﬁxing an index over the initial data set and comparing it to gold standard indices over
later data sets. All measures have been computed for all of the evaluated index models.
This gives us for each update of the data set one observation of the accuracy measures.
The results of a spearman rank correlation analysis between all pairs of measures over this
data is visualised in Figure 7.
There are several interesting observations to be made. First of all we observe two in-
dex models where the correlations are much stronger in general: the subject index and
SchemEX. All other analysed index models exhibit lower correlation values. The expla-
nation for this behaviour is the granularity of the indices. Both, the subject index and

6 EMPIRICAL COMPARISON OF MEASURES

15

(a) Subject index

(b) RDF type index

(c) Type set index

(d) Property set index

(e) Extended characteristic set index

(f) SchemEX index

Figure 7: Correlation of accuracy measures on different indices over an evolving data set.

−1−0.8−0.6−0.4−0.200.20.40.60.81Jaccard (RDF triples)Jaccard (Key elements)Recall (Key elements)Cross EntropyKL−DivergencePerplexityNormalised PerplexityMacro−Avg RecallMacro−Avg PrecisionMicro−Avg RecallMicro−Avg PrecisionJaccard (RDF triples)Jaccard (Key elements)Recall (Key elements)Cross EntropyKL−DivergencePerplexityNormalised PerplexityMacro−Avg RecallMacro−Avg PrecisionMicro−Avg RecallMicro−Avg PrecisionCorrelation of Measures for a  Subject  Index100931009189100−92−81−89100−96−95−8486100−92−81−8910086100−93−92−79869886100908897−88−82−88−78100464835−33−44−33−4845100958593−97−88−97−869440100878181−77−80−77−77826482100−1−0.8−0.6−0.4−0.200.20.40.60.81Jaccard (RDF triples)Jaccard (Key elements)Recall (Key elements)Cross EntropyKL−DivergencePerplexityNormalised PerplexityMacro−Avg RecallMacro−Avg PrecisionMicro−Avg RecallMicro−Avg PrecisionJaccard (RDF triples)Jaccard (Key elements)Recall (Key elements)Cross EntropyKL−DivergencePerplexityNormalised PerplexityMacro−Avg RecallMacro−Avg PrecisionMicro−Avg RecallMicro−Avg PrecisionCorrelation of Measures for a  Type  Index100861008977100−11−7−22100−9−22457100−11−7−2210057100−27−32−28916691100786695−375−37−38100666779−39−4−39−4787100665869−33−32−33−346968100938082−15−9−15−28715959100−1−0.8−0.6−0.4−0.200.20.40.60.81Jaccard (RDF triples)Jaccard (Key elements)Recall (Key elements)Cross EntropyKL−DivergencePerplexityNormalised PerplexityMacro−Avg RecallMacro−Avg PrecisionMicro−Avg RecallMicro−Avg PrecisionJaccard (RDF triples)Jaccard (Key elements)Recall (Key elements)Cross EntropyKL−DivergencePerplexityNormalised PerplexityMacro−Avg RecallMacro−Avg PrecisionMicro−Avg RecallMicro−Avg PrecisionCorrelation of Measures for a  TS  Index100681006382100242131000−32−1556100242131005610018−51593569310067829815−91516100655661−612−6−76910093686527−32719696210091665711−12115605884100−1−0.8−0.6−0.4−0.200.20.40.60.81Jaccard (RDF triples)Jaccard (Key elements)Recall (Key elements)Cross EntropyKL−DivergencePerplexityNormalised PerplexityMacro−Avg RecallMacro−Avg PrecisionMicro−Avg RecallMicro−Avg PrecisionJaccard (RDF triples)Jaccard (Key elements)Recall (Key elements)Cross EntropyKL−DivergencePerplexityNormalised PerplexityMacro−Avg RecallMacro−Avg PrecisionMicro−Avg RecallMicro−Avg PrecisionCorrelation of Measures for a  PS  Index100861006765100−35−18−53100−59−58−3666100−35−18−5310066100−53−44−30809080100726998−49−35−49−31100433729−32−26−32−4339100938674−39−63−39−527635100917754−48−78−48−73583888100−1−0.8−0.6−0.4−0.200.20.40.60.81Jaccard (RDF triples)Jaccard (Key elements)Recall (Key elements)Cross EntropyKL−DivergencePerplexityNormalised PerplexityMacro−Avg RecallMacro−Avg PrecisionMicro−Avg RecallMicro−Avg PrecisionJaccard (RDF triples)Jaccard (Key elements)Recall (Key elements)Cross EntropyKL−DivergencePerplexityNormalised PerplexityMacro−Avg RecallMacro−Avg PrecisionMicro−Avg RecallMicro−Avg PrecisionCorrelation of Measures for a  ECS  Index100861007174100−40−45−34100−38−53−2382100−40−45−3410082100−40−48−11888788100757799−31−21−31−13100423129494−437100938479−52−51−52−468132100948364−49−47−49−51673791100−1−0.8−0.6−0.4−0.200.20.40.60.81Jaccard (RDF triples)Jaccard (Key elements)Recall (Key elements)Cross EntropyKL−DivergencePerplexityNormalised PerplexityMacro−Avg RecallMacro−Avg PrecisionMicro−Avg RecallMicro−Avg PrecisionJaccard (RDF triples)Jaccard (Key elements)Recall (Key elements)Cross EntropyKL−DivergencePerplexityNormalised PerplexityMacro−Avg RecallMacro−Avg PrecisionMicro−Avg RecallMicro−Avg PrecisionCorrelation of Measures for a  SchemEX  Index100931007883100−81−74−87100−95−94−7479100−81−74−8710079100−95−92−778697861007984100−86−76−86−78100777952−49−79−49−7355100938988−92−92−92−938969100959179−85−94−85−968178951007 DISCUSSION

16

SchemEX, generate a relatively large number of key elements over the analysed data set.
Furthermore, they have many key elements in the index which provide only one element
entry upon request. This means on the one hand, that measuring the accuracy of a speciﬁc
query is often reduced to a binary task: either the index provides the correct element or not.
Therefore, the retrieval based and key element based measures are strongly correlated in a
positive way. On the other hand, as the change of a data element being assigned to a dif-
ferent key element is typically caused by a single triple, also the Jaccard index over triples
correlates strongly with the measures. Also the changes in the distribution are affected
by this. Changes for key elements with one entry typically leads to a strong shift in the
overall distribution, reducing a MLE probability to zero1. This shift in probability mass is
recognised by the distribution based measures. The observation that the correlation here is
highly negative comes from the fact that for those measures a high value indicates a strong
change, while the other measures use high values to indicate no or only weak changes.
For all other analysed index models, we have a different overall behaviour. Typically, the
measures within each family are correlated (though to a lower extent than for the subject
index or SchemEX). Furthermore, the key element based measures correlate strongly with
the retrieval based measures and the Jaccard index over the sets of RDF triples. The distri-
bution based measures, instead seem to measure something else. Beyond a few exception,
no strong correlation can be observed here.
Looking at individual measures we can also observe some interesting behaviour: Across
all types of index models macro-average precision seems to be not or only weakly cor-
related to most other measures. Hence, the different way of aggregating precision seems
to have a strong impact. The micro-average show a better sensitivity of the accuracy of
queries with large results sets. This seems to make a distinction with other evaluation mea-
sures over most analysed index models. The perfect Spearman correlation between Cross-
Entropy and Perplexity, instead, is conceptual as Perplexity is only a rank-preserving non-
linear scaling of Cross-Entropy.

7 Discussion

Given the analysis in the previous two section, we can observe that a look at the key
elements might already give a good indication for the accuracy of an index. While from
a theoretic point of view they might not be very sensitive to changes they correlated well
with other, more sensitive measures in the empiric evaluation. In particular they reﬂect
quite well the retrieval based accuracy measures. This observations holds for all analysed
index models.
Additionally, in many cases the distribution based measures provide further insights. Given
that the volume of data needed to compute these measures is of the same order as for the
key elements, such an analysis causes little overhead.

1Effectively the probability has a small value, slightly higher than zero due to the use of Lidstone smoothing

as explained in Section 4.3.

8 SUMMARY AND CONCLUSIONS

17

In general, the empirical observations in the previous section were made only over a single
data set. However, given that this data set has been carefully designed to give a represen-
tative excerpt of the LOD cloud, we might want to conjecture that density based measures
as well as the measures based on key elements sufﬁce to judge the quality of Linked
Data indices. As information on the distributions over key elements in Linked Data index
structures is sufﬁcient to compute both types of measures, they are also attractive from a
computational point of view.

8 Summary and Conclusions

In this paper I investigated the question of how to measure the accuracy of index structures
and data caches which are built in an approximative way or over evolving Linked Data
sets. I gave an overview of different approaches for such measures, compared them on a
theoretical and empirical level. One observation is that information about the data distri-
bution over key elements seems to provide good insights into index accuracy without the
need to access all data in an index.
This observation also motivates a roadmap for future work. Operating on samples of
Linked Data can provide good estimates of these distributions [Got14]. Accordingly an
interesting question is how suitable are sampling based strategies for evaluating index
accuracy. This means, that instead of asking all possible queries, we consider only a
random sample of queries to obtain distributions. This might speed up computation or can
at least provide certain conﬁdence intervals about what the true accuracy of an index might
be.

References

[BC00]

[Car14]

[CGM00]

Brian E Brewington and George Cybenko. How dynamic is the Web? Computer
Networks, 33(1):257–276, 2000.

Gavin Carothers. RDF 1.1 N-Quads. W3C Recommendation, Feb. 2014. (accessed
14 March 2014).

Junghoo Cho and Hector Garcia-Molina. Synchronizing a Database to Improve Fresh-
ness. In Proceedings of the 2000 ACM SIGMOD International Conference on Manage-
ment of Data, SIGMOD ’00, pages 117–128, New York, NY, USA, 2000. ACM.

[DGSG14] Renata Dividino, Thomas Gottron, Ansgar Scherp, and Gerd Gröner. From Changes
In PROFILES’14:
to Dynamics: Dynamics Analysis of Linked Open Data Sources.
Proceedings of the Workshop on Dataset ProﬁIling and Federated Search for Linked
Data, 2014. (to appear).

[DKG14] Renata Dividino, Andre Kramer, and Thomas Gottron. An Investigation of HTTP
Header Information for Detecting Changes of Linked Open Data S ources. In ESWC’14:
Proceedings of the Extended Semantic Web Conference, 2014. (to appear).

8 SUMMARY AND CONCLUSIONS

18

[DSGG13] Renata Dividino, Ansgar Scherp, Gerd Gröner, and Thomas Gottron. Change-a-LOD:
Does the Schema on the Linked Data Cloud Change or Not? In COLD’13: International
Workshop on Consuming Linked Data, 2013.

[GG14]

[Got14]

[GP13]

Thomas Gottron and Christian Gottron. Perplexity of Index Models over Evolving
Linked Data. In ESWC’14: Proceedings of the Extended Semantic Web Conference,
pages 161–175, 2014.

Thomas Gottron. Of Sampling and Smoothing: Approximating Distributions over
Linked Open Data. In PROFILES’14: Proceedings of the Workshop on Dataset ProﬁIl-
ing and Federated Search for Linked Data, 2014. (to appear).

Thomas Gottron and Rene Pickhardt. A Detailed Analysis of the Quality of Stream-
Based Schema Construction on Linked Open Data. In Juanzi Li, Guilin Qi, Dongyan
Zhao, Wolfgang Nejdl, and Hai-Tao Zheng, editors, Semantic Web and Web Science,
Springer Proceedings in Complexity, pages 89–102. Springer New York, 2013.

[HHK+10] Andreas Harth, Katja Hose, Marcel Karnstedt, Axel Polleres, Kai-Uwe Sattler, and
Jürgen Umbrich. Data summaries for on-demand queries over linked data. In Int. Conf.
on World wide web, pages 411–420. ACM, 2010.

[KAU+13] Tobias Käfer, Ahmed Abdelrahman, Jürgen Umbrich, Patrick O’Byrne, and Aidan
Hogan. Observing Linked Data Dynamics. In Philipp Cimiano, Oscar Corcho, Valentina
Presutti, Laura Hollink, and Sebastian Rudolph, editors, The Semantic Web: Semantics
and Big Data, volume 7882 of Lecture Notes in Computer Science, pages 213–227.
Springer Berlin Heidelberg, 2013.

[KGSS12] Mathias Konrath, Thomas Gottron, Steffen Staab, and Ansgar Scherp. SchemEX—
Efﬁcient Construction of a Data Catalogue by Stream-based Indexing of Linked Data.
Web Semantics: Science, Services and Agents on the World Wide Web, 16(5):52 – 58,
2012. The Semantic Web Challenge 2011.

[LG99]

[NW09]

[NW10]

Steve Lawrence and C Lee Giles. Accessibility of information on the web. Nature,
400(6740):107–107, 1999.

Thomas Neumann and Gerhard Weikum. Scalable join processing on very large RDF
graphs. In Int. Conf. on Management of data, pages 627–640. ACM, 2009.

Thomas Neumann and Gerhard Weikum. The RDF-3X engine for scalable management
of RDF data. The VLDB Journal, 19(1):91–113, 2010.

[UHH+10] Jürgen Umbrich, Michael Hausenblas, Aidan Hogan, Axel Polleres, and Stefan Decker.
In

Towards Dataset Dynamics: Change Frequency of Linked Open Data Sources.
LDOW, 2010.

[WKB08] Cathrin Weiss, Panagiotis Karras, and Abraham Bernstein. Hexastore: sextuple index-
ing for semantic web data management. Proc. VLDB Endow., 1(1):1008–1019, August
2008.

