6
1
0
2

 
r
a

M
8

 

 
 
]

.

C
O
h
t
a
m

[
 
 

1
v
0
0
4
2
0

.

3
0
6
1
:
v
i
X
r
a

ZERO-SUM RISK-SENSITIVE STOCHASTIC GAMES FOR

CONTINUOUS TIME MARKOV CHAINS

MRINAL K. GHOSH, K. SURESH KUMAR AND CHANDAN PAL

Abstract. We study inﬁnite horizon discounted-cost and ergodic-cost
risk-sensitive zero-sum stochastic games for controlled continuous time
Markov chains on a countable state space. For the discounted-cost game
we prove the existence of value and saddle-point equilibrium in the class
of Markov strategies under nominal conditions. For the ergodic-cost
game we prove the existence of values and saddle point equilibrium
by studying the corresponding Hamilton-Jacobi-Isaacs equation under
a certain Lyapunov condition.

Key words: Risk-sensitive cost, inﬁnite horizon discounted cost, inﬁnite hori-
zon ergodic cost, HJI equation, value, saddle point equilibrium.

2000 Mathematics Subject Classiﬁcation. Primary 93E20, Secondary 60J75.

1. Introduction

This paper is a sequel to [8] where the risk sensitive continuous time
Markov decision process is studied on a countable state space. In this pa-
per we extend the result of [8] to risk-sensitive zero sum stochastic games
for continuous time control Markov chains on a countable state space. A
zero-sum risk-sensitive diﬀerential game has been studied in [1] and the cor-
responding discrete time problem studied in [2]. As noted in [1] and [2], the
zero-sum risk-sensitive stochastic dynamic game is relevant in worst-case
scenarios, for example, in ﬁnancial applications when a risk-averse investor
is trying to minimize his long-term portfolio loss against the market which,
by default, is antagonistic and hence the maximizer. As a result the mini-
mizer chooses the risk-aversion parameter θ > 0 and tries to minimizes his
expected risk-sensitive costs. Thus the risk-sensitive parameter is positive.
If θ < 0 then minimizer would be risk-seeking. The maximizer is not risk-
seeking but simply antagonistic to the minimizer. Under certain conditions
we establish value and saddle point strategies for both players.

The rest of the paper is structured as follows. Section 2 deals with the
description of the problem. In Section 3, we prove the existence of value and
saddle-point equilibrium in the class of Markov strategies for the discounted-
cost risk-sensitive zero-sum game. The analysis of ergodic-cost risk-sensitive

1

2

MRINAL K. GHOSH, K. SURESH KUMAR AND CHANDAN PAL

zero-sum game is carried out in Section 4. The paper is concluded in Section
5 with some concluding remarks.

2. Problem Description

Let Ui, i = 1, 2, be compact metric space and Vi = P(Ui), space of prob-

ability measure on Ui with Prohorov topology. Let

U := U1 × U2 and V := V1 × V2.

Let ¯πij : U → [0, ∞) for i 6= j and ¯πii : U → R for i ∈ S. Deﬁne πij : V → R
as follows: for v := (v1, v2) ∈ V ,

πij(v1, v2) =ZU2ZU1

¯πij(u1, u2)v1(du1)v2(du2) :=ZU

¯πij(u)v(du),

where u := (u1, u2) ∈ U .
We consider a continuous time controlled Markov chain Y (·) with state space
S = {1, 2, · · · } and controlled rate matrix Πv1,v2 = (πij(v1, v2)), given by the
stochastic integral

(2.1)

h(Y (t−), v1(t), v2(t), z)℘(dzdt).

dY (t) = ZR

Here ℘(dzdt) is a Poisson random measure with intensity m(dz)dt, where
m(dz) denote the Lebesque measure on R. The control process v(·) :=
(v1(·), v2(·)) takes values in V , and h : S × V × R → R is deﬁned as follows:

(2.2)

h(i, v, z) = (cid:26) j − i

0

if

z ∈ ∆ij(v)
otherwise,

where v := (v1, v2) and {∆ij(v) : i 6= j, i, j ∈ S} denote intervals of the form
[a, b) with length of ∆ij(v) = πij(v) which are pairwise disjoint for each
ﬁxed v ∈ V .
If vi(t) = ¯vi(t, Y (t−)) for some measurable map ¯vi : [0, ∞) × S → Vi, then
vi(·) is called a Markov strategy for the ith player. With an abuse of notation
the map ¯vi itself is called a Markov strategy of player i. A Markov strategy
¯vi(·) is called a stationary strategy if the map ¯vi does not depend explicitly
on time. We denote the set of all Markov strategies by Mi and set of all
stationary strategies by Si for the ith player.
Throughout this paper we assume that:

• ¯πij(u) ≥ 0 for all i 6= j, u ∈ U and the (inﬁnite) matrix (¯πij(u)) is

conservative, i.e.,

¯πij(u) = 0 for i ∈ S and u ∈ U .

Xj∈S

• The function ¯πij are continuous and

sup

i∈S,u∈U

[−¯πii(u)] := M < ∞ .

RISK-SENSITIVE GAMES FOR CONTINUOUS TIME MARKOV CHAINS

3

The existence of a unique weak solution to the equation (2.1) for a pair of
Markov strategies (v1, v2) for a given initial distribution µ ∈ P(S) follows
using the above assumption, see [[6], Theorem 2.3, Theorem 2.5, pp.14-15].
Let ¯r : S × U1 × U2 → [0, ∞) be the running cost function. Throughout
this paper, we assume that the function ¯r(·) is nonnegative, bounded and
continuous.

We list the commonly used notations below.

• Cb[a, b] denotes the set of all bounded and continuous functions on

[a, b].

• B(S) denotes the set of all bounded functions on S.
• C 1(a, b) denotes the set of all continuously diﬀerentiable functions

on (a, b).

• C ∞

c (a, b) denotes the set of all inﬁnitely diﬀerentiable functions on

(a, b) with compact support.

• Cb([a, b] × S) denotes the set of all functions f : [a, b] × S −→ R such

that f (t, i) ∈ Cb[a, b], for each i ∈ S.

• C 1((a, b) × S) denotes the set of all functions f : (a, b) × S −→ R

such that f (t, i) ∈ C 1(a, b), for each i ∈ S.

Set

BW (S) = {h : S → R| sup
i∈S

|h(i)|
W (i)

< ∞},

where W is the Lyapunov function as in (A1) (to be described in Section
4). Deﬁne for h ∈ BW (S),

khkW = sup
i∈S

|h(i)|
W (i)

.

Then BW (S) is a Banach space with the norm k · kW .

2.1. Discounted Cost Criterion. For a pair of Markov strategies (v1, v2),
deﬁne α-discounted risk-sensitive cost by

(2.3)

βv1,v2
α

(θ, i) =

1
θ

ln Ev1,v2

i

heθ R ∞

0 e−αtr(Y (t),v1(t,Y (t−)),v2(t,Y (t−)))dti

for some θ ∈ (0, Θ), and a ﬁxed Θ > 0, α > 0 is the discount factor, Y (·) is
the Markov chain corresponding to (v1, v2) ∈ M1 × M2 with Y (0) = i, and
r : S × V → R+ is given by

r(i, v1, v2) =ZU2ZU1

¯r(i, u1, u2)v1(du1)v2(du2) :=ZU

¯r(i, u)v(du),

where u := (u1, u2) and v := (v1, v2).
Let θ ∈ (0, Θ) be the “risk-sensitive parameter” chosen by the minimizer.
When the state of the system is i and players 1,2, choose strategies v1 ∈ M1,
v2 ∈ M2 respectively, the minimizer (player 1) tries to minimize his inﬁnite-
horizon discounted risk-sensitive cost βv1,v2
(θ, i) over his strategies whereas

α

4

MRINAL K. GHOSH, K. SURESH KUMAR AND CHANDAN PAL

the maximizer (player 2) tries to maximize the same over his strategies.
A strategy v∗

1 ∈ M1 is called optimal for player 1 for (θ, i) ∈ (0, Θ) × S, if

v∗
1 ,˜v2
α

β

(θ, i) ≤ sup
v2∈M2

inf

v1∈M1

βv1,v2
α

(θ, i) := β(α, θ, i) (lower value)

for any ˜v2 ∈ M2. Similarly a strategy v∗
2 for (θ, i) ∈ (0, Θ) × S, if

2 ∈ M2 is called optimal for player

˜v1,v∗
2
α

β

(θ, i) ≥ inf

v1∈M1

sup
v2∈M2

βv1,v2
α

(θ, i) := β(α, θ, i) (upper value)

for any ˜v1 ∈ M1. The game has value if
(2.4)
A pair of strategies (v∗
point equilibrium, and then v∗
player 2.

1, v∗

β(α, θ, i) = β(α, θ, i) = β(α, θ, i) ∀ i ∈ S, ∀ θ ∈ (0, Θ).

2) at which this value is attained is called a saddle-
2 is optimal for

1 is optimal for player 1, and v∗

2.2. Ergodic Cost Criterion. For a pair of Markov strategies (v1, v2), the
risk-sensitive ergodic cost is given by

(2.5) ρv1,v2(θ, i) = lim sup
T →∞

1
θT

ln Ev1,v2

i

heθ R T

0 r(Y (t),v1(t,Y (t−)),v2(t,Y (t−)))dti ,

for some θ ∈ (0, Θ), and a ﬁxed Θ > 0, Y (·) is the Markov chain corre-
sponding to (v1, v2) ∈ M1 × M2 with Y (0) = i.

Optimal strategies, saddle point equilibrium, etc.

for this criterion are

deﬁned analogously. The ergodic cost ρv1,v2 may depend on (θ, i).

3. Analysis of Discounted Cost Criterion

We carry out our analysis of the discounted cost criterion via the criterion

(3.1)

ξv1,v2
α

(θ, i) = Ev1,v2

i

heθ R ∞

0 e−αtr(Y (t),v1(t,Y (t−)),v2(t,Y (t−)))dti .

Since logarithmic is an increasing function, therefore the optimal strategies
for the criterion (2.3) are optimal strategies for the above criterion.
Corresponding to the cost criterion (3.1), the value function is deﬁned as

and

ψα(θ, i) = inf

v1∈M1

sup
v2∈M2

ξv1,v2
α

(θ, i)

ψ

α

(θ, i) = sup
v2∈M2

inf

v1∈M1

ξv1,v2
α

(θ, i).

Using dynamic programming heuristics, the Hamilton-Jacobi-Isaacs (HJI)
equations for discounted cost criterion are given by

αθ

dψα
dθ

(θ, i) = inf
v1∈V1

= sup
v2∈V2

(3.2)

ψα(0, i) = 1,

sup

v2∈V2hΠv1,v2 ψα(θ, i) + θr(i, v1, v2)ψα(θ, i)i
v1∈V1hΠv1,v2 ψα(θ, i) + θr(i, v1, v2)ψα(θ, i)i

inf

RISK-SENSITIVE GAMES FOR CONTINUOUS TIME MARKOV CHAINS

5

where Πv1,v2f (i) :=Xj∈S

πij(v1, v2)f (j), for any function f (i).

Next we prove that the equations (3.2) have a smooth, bounded solution.
Fix ǫ > 0 and consider the ordinary diﬀerential equation (ODE)

αθ

dψǫ
α
dθ

(θ, i) = inf
v1∈V1

(3.3)

= sup
v2∈V2

sup

v2∈V2hΠv1,v2 ψǫ
v1∈V1hΠv1,v2 ψǫ

inf

α(θ, i) + θr(i, v1, v2)ψǫ

α(θ, i) + θr(i, v1, v2)ψǫ

α(θ, i)i
α(θ, i)i

ψǫ

α(ǫ, i) = e

ǫ

α krk∞ := hǫ,

where k · k∞ denotes the supnorm. Note that the second equality follows
from Fan’s minimax theorem, see [[4], Theorem 3].
Let δ > 0. Deﬁne the nonlinear operator T : Cb([ǫ, ǫ + δ] × S) → Cb([ǫ, ǫ +
δ] × S) by

T f (η, i) := e

ǫ

α krk∞ +

1

αZ η

ǫ

inf
v1∈V1

sup

v2∈V2h 1

θ

Πv1,v2f (θ, i) + r(i, v1, v2)f (θ, i)idθ.

By using the fact

sup

i∈S,u∈U

[−¯πii(u)] = M < ∞ and r is bounded, we have

kT f1 − T f2k∞ ≤

1

αhkrk∞δ + 2M ln(cid:16)1 +

ǫ(cid:17)ikf1 − f2k∞.

δ

Choose δ such that 1
operator. Therefore by Banach’s ﬁxed point theorem there exists a function
ψǫ

ǫ(cid:17)i < 1. Then T is a contraction

αhkrk∞δ + 2M ln(cid:16)1 + δ

α ∈ Cb([ǫ, ǫ + δ] × S) such that

ψǫ

α(η, i) = e

ǫ

α krk∞ +

1

αZ η

ǫ

inf
v1∈V1

sup

v2∈V2h 1

θ

Πv1,v2 ψǫ

α(θ, i)+r(i, v1, v2)ψǫ

α(θ, i)idθ.

Note that the bracketed term in the above integrand is bounded and jointly
continuous in (θ, v1, v2). Since V1 and V2 are compact metric spaces, it
follows that the integrand above is bounded and continuous in θ ∈ [ǫ, ǫ + δ].
Thus it follows that ψǫ
α is in C 1((ǫ, ǫ + δ] × S) ∩ Cb([ǫ, ǫ + δ] × S). Proceeding
in this way we get a C 1((ǫ, Θ) × S) ∩ Cb([ǫ, Θ) × S) solution for the ODE
(3.3). Let

¯vi : (0, Θ) × S → Vi,

i = 1, 2,

be measurable functions such that

(3.4)

α(θ, i) + θr(i, ¯v1(θ, i), v2)ψǫ

inf
v1∈V1

sup

v2∈V2hΠv1,v2 ψǫ

= sup

v2∈V2hΠ¯v1,v2 ψǫ

α(θ, i) + θr(i, v1, v2)ψǫ

α(θ, i)i
α(θ, i)i

6

and

(3.5)

MRINAL K. GHOSH, K. SURESH KUMAR AND CHANDAN PAL

sup
v2∈V2

inf

v1∈V1hΠv1,v2 ψǫ

= inf

v1∈V1hΠv1,¯v2 ψǫ

α(θ, i) + θr(i, v1, v2)ψǫ

α(θ, i) + θr(i, , v1, ¯v2(θ, i))ψǫ

α(θ, i)i
α(θ, i)i.

The existence of such measurable maps are ensured by Beneˇs measurable
selection theorem, see [3]. Let

be deﬁned by

v∗
i : R+ × S → Vi,

i = 1, 2,

v∗
i (t, i) = ¯vi(θe−αt, i),

i = 1, 2.

Set θ(t) = θe−αt and deﬁne Tǫ by

Tǫ = inf{t ≥ 0 : θ(t) = ǫ}.

For (v∗
218-219) to the function

1 , v2) ∈ M1 × M2, applying Itˆo formula (see [6], Appendix C, pp.

eR t

0 θ(s)r(Y (s),v∗

1 (s,Y (s−)),v2(s,Y (s−)))dsψǫ

α(θ(t), Y (t)),

we obtain
Ev1,v2

i

[eR Tǫ

0 θ(s)r(Y (s),v∗

1 (s,Y (s−)),v2(s,Y (s−)))dsψǫ

= Ev1,v2

i

eR t

0 θ(s)r(Y (s),v∗

hZ Tǫ

0

α(θ(Tǫ), Y (Tǫ))] − ψǫ
dψǫ
α
dθ

1 (s,Y (s−)),v2(s,Y (s−)))dsn − αθ(t)

α(θ, i)

(θ(t), Y (t))

+Πv1,v2 ψǫ

α(θ(t), Y (t)) + θ(t)r(Y (t), v∗

1(t, Y (t−)), v2(t, Y (t−)))ψǫ

α(θ(t), Y (t))odti.

Since ψǫ

α satisﬁes (3.4), we obtain

v∗
1 ,v2
E
i

[eR Tǫ

0 θ(s)r(Y (s),v∗

1 (s,Y (s−)),v2(s,Y (s−)))dshǫ] − ψǫ

α(θ, i) ≤ 0,

where hǫ is as in (3.3). Since v2 is arbitrary, we get

(3.6)

ψǫ

α(θ, i) ≥ sup
v2∈M2

v∗
1 ,v2
E
i

Using analogous arguments, we can show that

(3.7)

ψǫ

α(θ, i) ≤

inf

v1∈M1

v1,v∗
E
2
i

Therefore, from (3.6) and (3.7), we obtain

0 θ(s)r(Y (s),v∗

0 θ(s)r(Y (s),v1(s,Y (s−)),v∗

hhǫeR Tǫ
hhǫeR Tǫ
hhǫeR Tǫ
hhǫeR Tǫ

1 (s,Y (s−)),v2(s,Y (s−)))dsi .
2 (s,Y (s−)))dsi .
0 θ(s)r(Y (s),v1(s,Y (s−)),v2(s,Y (s−)))dsi
0 θ(s)r(Y (s),v1(s,Y (s−)),v2(s,Y (s−)))dsi .

ψǫ

α(θ, i) = sup
v2∈M2

inf

v1∈M1

Ev1,v2

i

(3.8)

= inf

v1∈M1

sup
v2∈M2

Ev1,v2

i

Next we take limit of ψǫ
(3.2), i.e., we prove the following theorem.

α as ǫ → 0 and prove that the limit function satisﬁes

RISK-SENSITIVE GAMES FOR CONTINUOUS TIME MARKOV CHAINS

7

Theorem 3.1. There exists a unique solution ψα in the class Cb((0, Θ) ×
S)∩C 1((0, Θ)×S) to (3.2). The solution admits the following representation

ψα(θ, i) = sup
v2∈M2

inf

v1∈M1

=

inf

v1∈M1

sup
v2∈M2

Ev1,v2

i

Ev1,v2

i

heθ R ∞
heθ R ∞

0 e−αsr(Y (s),v1(s,Y (s−)),v2(s,Y (s−)))dsi
0 e−αsr(Y (s),v1(s,Y (s−)),v2(s,Y (s−)))dsi .

Furthermore ψα is the value function for the discounted cost criterion (3.1).
Moreover, a saddle point equilibrium exists in M1 × M2.
Proof. First recall the stochastic representation of ψǫ

α from (3.8),

ψǫ

α(θ, i) = sup
v2∈M2

inf

v1∈M1

Ev1,v2

i

=

inf

v1∈M1

sup
v2∈M2

Ev1,v2

i

hhǫeR Tǫ
hhǫeR Tǫ

0 θ(s)r(Y (s),v1(s,Y (s−)),v2(s,Y (s−)))dsi
0 θ(s)r(Y (s),v1(s,Y (s−)),v2(s,Y (s−)))dsi .

From the representation of ψǫ

α, we have

1 ≤ ψǫ

α(θ, i) ≤ hǫe

θ

α krk∞(1−e−αTǫ ) = e

θ

α krk∞

for every ǫ > 0, and all (θ, i).
By closely mimicking the arguments in the proof of [[5], Theorem 3.4], it fol-
lows that the HJB equation (3.2) has a solution in Cb((0, Θ)×S)∩C 1((0, Θ)×
S). Let

be measurable selectors such that

¯vi : (0, Θ) × S → Vi,

i = 1, 2,

= sup

sup

inf
v1∈V1

v2∈V2hΠv1,v2 ψα(θ, i) + θr(i, v1, v2)ψα(θ, i)i
v2∈V2hΠ¯v1,v2 ψα(θ, i) + θr(i, ¯v1(θ, i), v2)ψα(θ, i)i
v1∈V1hΠv1,v2 ψα(θ, i) + θr(i, v1, v2)ψα(θ, i)i
v1∈V1hΠv1,¯v2 ψα(θ, i) + θr(i, , v1, ¯v2(θ, i))ψα(θ, i)i.

sup
v2∈V2

inf

= inf

(3.9)

and

(3.10)

Let

be deﬁned by

(3.11)
For (v∗

v∗
i : R+ × S → Vi,

i = 1, 2,

v∗
i (t, i) = ¯vi(θe−αt, i),

i = 1, 2.

1, v2) ∈ M1 × M2, applying Itˆo formula to the function
1 (s,Y (s−)),v2(s,Y (s−)))dsψα(θ(t), Y (t))

0 θ(s)r(Y (s),v∗

eR t

and using (3.9), we get

v∗
1 ,v2
E
i

[eR T

0 θ(s)r(Y (s),v∗

1 (s,Y (s−)),v2(s,Y (s−)))dsψα(θ(T ), Y (T ))] − ψα(θ, i) ≤ 0.

8

MRINAL K. GHOSH, K. SURESH KUMAR AND CHANDAN PAL

Since v2 is arbitrary, we get

ψα(θ, i) ≥ sup
v2∈M2

v∗
1 ,v2
E
i

hψα(θ(T ), Y (T ))eR T

0 θ(s)r(Y (s),v∗

1 (s,Y (s−)),v2(s,Y (s−)))dsi .

Since 1 ≤ ¯ψǫ

α ≤ e

θ

α krk∞ for all ǫ > 0, we get

ψα(θ, i) ≥ sup
v2∈M2

v∗
1 ,v2
E
i

0 θ(s)r(Y (s),v∗

1 (s,Y (s−)),v2(s,Y (s−)))dsi .

By using monotone convergence theorem for letting T → ∞ in the above we
obtain

(3.12)

ψα(θ, i) ≥ sup
v2∈M2

v∗
1 ,v2
E
i

0 θ(s)r(Y (s),v∗

1 (s,Y (s−)),v2(s,Y (s−)))dsi .

Using analogous arguments we can show that

heR T

heR ∞

heR ∞

(3.13)

ψα(θ, i) ≤

inf

v1∈M1

v1,v∗
E
2
i

0 θ(s)r(Y (s),v1(s,Y (s−)),v∗

2 (s,Y (s−)))dsi .

Therefore, from (3.12) and (3.13), we obtain

ψα(θ, i) = sup
v2∈M2

inf

v1∈M1

=

inf

v1∈M1

sup
v2∈M2

Ev1,v2

i

Ev1,v2

i

0 θ(s)r(Y (s),v1(s,Y (s−)),v2(s,Y (s−)))dsi
heR ∞
heR ∞
0 θ(s)r(Y (s),v1(s,Y (s−)),v2(s,Y (s−)))dsi .

It is easy to check that ψα is the value function for the discounted cost
criterion (3.1). Moreover, the pair of Markov strategies given by (3.11)
forms a saddle point equilibrium. This completes the proof.
(cid:3)

4. Analysis of Ergodic Cost Criterion

In this section we prove the existence of value and stationary Markov
saddle point strategies for the ergodic cost criterion under the following
assumption:
(A1)(Lyapunov condition) There exist constants b > 0, δ > 0, a ﬁnite set
C and a map W : S → [1, ∞) with W (i) → ∞ as i → ∞, such that

ΠvW (i) ≤ −2δW (i) + bIC(i), i ∈ S, v ∈ V.

Throughout this section, we assume that for every pair of stationary

Markov strategies (v1, v2) the corresponding Markov chain is irreducible.

We carry out our analysis of the ergodic cost criterion as a limit of the

corresponding ﬁnite horizon cost criterion given by

(4.1)

IT (i, v1, v2) := Ev1,v2

i

heR T
0 r(Y (s),v1(s,Y (s−)),v2(s,Y (s−)))dsi

where Y (·) is the Markov chain corresponding to (v1, v2) ∈ M1 × M2 with
initial condition i ∈ S. Using the dynamic programming heuristics, the HJI

RISK-SENSITIVE GAMES FOR CONTINUOUS TIME MARKOV CHAINS

9

equations for the above cost criterion, are given by

−

dφ
dt

(t, i) = inf
v1∈V1

= sup
v2∈V2

sup

v2∈V2hΠv1,v2φ(t, i) + r(i, v1, v2)φ(t, i)i
v1∈V1hΠv1,v2φ(t, i) + r(i, v1, v2)φ(t, i)i

inf

φ(T, i) = 1.

(4.2)
As before, we can show the existence of a C 1((0, T ) × S) ∩ Cb([0, T ] × S)
solution for the ODE (4.2). Using a standard application of Itˆo’s formula
we get

φ(t, i) = sup
v2∈M2

inf

v1∈M1

=

inf

v1∈M1

sup
v2∈M2

Ev1,v2

i

Ev1,v2

i

heR T
t r(Y (s),v1(s,Y (s−)),v2(s,Y (s−)))dsi
heR T
t r(Y (s),v1(s,Y (s−)),v2(s,Y (s−)))dsi .

Set ψ(t, i) = φ(T − t, i). Then ψ is the unique C 1((0, T ) × S) ∩ Cb([0, T ] × S)
solution to

dψ
dt

(t, i) = inf
v1∈V1

= sup
v2∈V2

ψ(0, i) = 1.

sup

v2∈V2hΠv1,v2 ψ(t, i) + r(i, v1, v2)ψ(t, i)i
v1∈V1hΠv1,v2 ψ(t, i) + r(i, v1, v2)ψ(t, i)i

inf

Using Itˆo’s formula, we obtain

ψ(t, i) = sup
v2∈M2

inf

v1∈M1

=

inf

v1∈M1

sup
v2∈M2

Ev1,v2

i

Ev1,v2

i

heR t
0 r(Y (s),v1(s,Y (s−)),v2(s,Y (s−)))dsi
heR t
0 r(Y (s),v1(s,Y (s−)),v2(s,Y (s−)))dsi .

Formally, using separation of variables, we write

ψ(t, i) = eρt ˆψ(i).

This yields

ρ ˆψ(i) = inf
v1∈V1

(4.3)

= sup
v2∈V2

sup

v2∈V2hΠv1,v2
v1∈V1hΠv1,v2

inf

ˆψ(i) + r(i, v1, v2) ˆψ(i)i
ˆψ(i) + r(i, v1, v2) ˆψ(i)i.

The above equation is the HJI equation for the ergodic cost (2.5).

We now proceed to make a rigorous analysis of the above. First we trun-
cate our cost function which plays a crucial role to derive the HJI equations
and ﬁnd the value of the game. Let rn : S × V → [0, ∞) be given by

(4.4)

rn :=(cid:26) r

if i ∈ {1, 2, · · · , n}

0 otherwise

MRINAL K. GHOSH, K. SURESH KUMAR AND CHANDAN PAL

10

and

ψn(t, i) = sup
v2∈M2

inf

v1∈M1

Ev1,v2

i

=

inf

v1∈M1

sup
v2∈M2

Ev1,v2

i

heR t
0 rn(Y (s),v1(s,Y (s−)),v2(s,Y (s−)))dsi
heR t
0 rn(Y (s),v1(s,Y (s−)),v2(s,Y (s−)))dsi .

Then, as above, we can show that ψn is the unique solution in C 1((0, T ) ×
S) ∩ Cb([0, T ] × S) to

dψn
dt

(t, i) = inf
v1∈V1

= sup
v2∈V2

ψn(0, i) = 1.

sup

v2∈V2hΠv1,v2 ψn(t, i) + rn(i, v1, v2)ψn(t, i)i
v1∈V1hΠv1,v2 ψn(t, i) + rn(i, v1, v2)ψn(t, i)i

inf

Now onward, we ﬁx a reference state i0 ∈ S such that W (i0) ≥ 1 + b
set

δ and

(4.5)

¯ψn(t, i) =

ψn(t, i)
ψn(t, i0)

.

Then it is easy to see that ¯ψn is the unique solution in C 1((0, T ) × S) ∩
Cb([0, T ] × S) to
d ¯ψn
dt

(t, i0) = inf
v1∈V1

¯ψn(t, i)
ψn(t, i0)

dψn
dt

(t, i) +

(4.6)

¯ψn(0, i) = 1.

= sup
v2∈V2

sup

v2∈V2hΠv1,v2
v1∈V1hΠv1,v2

inf

¯ψn(t, i) + rn(i, v1, v2) ¯ψn(t, i)i
¯ψn(t, i) + rn(i, v1, v2) ¯ψn(t, i)i

Next we take limit as t → ∞ in (4.6), to derive the existence of a solution
for ergodic HJI equation with cost function rn. For this we want to show
that ¯ψn(t, i) is uniformly bounded (for each ﬁxed n ). To this end ﬁx a
strategy of player 2 and consider the corresponding optimal control problem
for player 1 .
Let v∗

2n : R+ × S → V2 be a measurable map such that

inf
v1∈V1

sup

v2∈V2hΠv1,v2 ψn(t, i) + rn(i, v1, v2)ψn(t, i)i

2n(t,i)ψn(t, i) + rn(i, v1, v∗

= inf

v1∈V1hΠv1,v∗

2n(t, i))ψn(t, i)i.

2 instead.

We suppress the dependence of n on v∗

2n and write v∗

For the ﬁxed Markov strategy v∗

2 ∈ M2 consider the pure jump processes

given by

(4.7)

dYv∗

2 (t) = ZR

h(Yv∗

2 (t−), v1(t), v∗

2(t, Yv∗

2 (t−)), z)℘(dzdt),

where h is as in (2.2).
Now we consider a new auxiliary continuous time Markov decision problem

RISK-SENSITIVE GAMES FOR CONTINUOUS TIME MARKOV CHAINS

11

(CTMDP) corresponding to the process (4.7), i.e., player 2 ﬁxes the strategy
v∗
2 and player 1 treats it as a CTMDP. First we deﬁne the set of all admissible
controls denoted by A.

A V1-valued process v1(·) is said to be admissible if it is predictable and

the equation

(4.8)

dYv∗

2 (t) = ZR

h(Yv∗

2 (t−), v1(t), v∗

2(t, Yv∗

2 (t−)), z)℘(dzdt)

has a unique weak solution for each initial Y0 independent of ℘(dzdt).

For an admissible control v1(·) ∈ A, the risk-sensitive cost for the ﬁnite

horizon [0,T] is deﬁned by

I n
v∗
2

v1,v∗
(i, v1(·)) = E
2
i

where Yv∗
condition i ∈ S.
Consider the ODE

2 (·) is the pure jump process (4.8) corresponding to v1(·) and initial

R T
0 rn(Yv∗

(t),v1(t),v∗

2 (t,Yv∗
2

2

he

(t−)))dti ,

−

(4.9)

dΨn
v∗
2

dt
Ψn
v∗
2

(t, i) = inf

v1∈V1hΠv1,v∗

2 (t,i)Ψn
v∗
2

(t, i) + rn(i, v1, v∗

2(t, i))Ψn
v∗
2

(t, i)i

(T, i) = 1.

Deﬁne the nonlinear operator T : Cb([0, T ] × S) → Cb([0, T ] × S) by

T f (t, i) := 1 +Z T

t

inf

v1∈V1hΠv1,v∗

2 (t,i)f (s, i) + rn(i, v1, v∗

2(t, i))f (s, i)ids.

As before, we get the existence of a solution to (4.9) in Cb([0, T ] × S). Using
a standard application of Itˆo’s formula, we obtain

Ψn
v∗
2

(t, i) =

inf

v1(·)∈A

v1,v∗
E
2
i

Set ψn
v∗
2
to

(t, i) = Ψn
v∗
2

(T −t, i). Then ψn
v∗
2

R T
t rn(Yv∗
2

(cid:20)e

(s),v1(s,Yv∗
2

(s−)),v∗

2 (s,Yv∗
2

(s−)))ds(cid:21) .

is the unique solution in Cb([0, T ]×S)

dψn
v∗
2
dt
ψv∗

(t, i) = inf

v1∈V1hΠv1,v∗

2 (t,i)ψn
v∗
2

(t, i) + rn(i, v1, v∗

2(t, i))ψn
v∗
2

(t, i)i

It is easy to see that any minimizing selector in (4.10) corresponding to ψn
v∗
2
is optimal for the ﬁnite horizon CTMDP for player 1. Since any minimizing
selector corresponds to a Markov control, we have

ψn
v∗
2

(t, i) = inf

v1∈M1

v1,v∗
E
2
i

R t
0 rn(Yv∗
2

(cid:20)e

(s),v1(s,Yv∗
2

(s−)),v∗

2 (s,Yv∗
2

(s−)))ds(cid:21) .

(4.10)

2 (0, i) = 1.

Using Itˆo’s formula, we obtain

ψn
v∗
2

(t, i) =

inf

v1(·)∈A

v1,v∗
E
2
i

(cid:20)e

R t
0 rn(Yv∗
2

(s),v1(s),v∗

2 (s,Yv∗
2

(s−)))ds(cid:21) .

12

MRINAL K. GHOSH, K. SURESH KUMAR AND CHANDAN PAL

For the reference state i0 ∈ S, we deﬁne

¯ψn
v∗
2

(t, i) =

ψn
v∗
2
ψn
v∗
2

(t, i)

(t, i0)

.

A simple calculation shows that ¯ψn
v∗
2

= ¯ψn, where ¯ψn is as in (4.5).

By closely mimicking the arguments in [[7], Theorem 3.1], one can easily

get the following multiplicative DPP; we omit the details.

Theorem 4.1. For any set ˜S ⊆ S,

ψn
v∗
2

(t, i) = inf

v1(·)∈A

v1,v∗
E
2
i

R t∧τ

0

he

rn(Yv∗
2

(s),v1(s),v∗

2 (s,Yv∗
2

(s−)))ds

ψn
v∗
2

(t−(t∧τ ), Yv∗

2 (t∧τ ))i, t ≥ 0 ,

where τ is the hitting time of the process Yv∗

2 (·) to the set ˜S.

Using the similar arguments as in the proofs of [8], we can prove the

following results; we omit the details.

Lemma 4.1. Assume (A1). Let Y (·) be a process (2.1) corresponding to
(v1, v∗

2) ∈ M1 × M2. Then
v1,v∗
E
2
i

where τi0 = inf{t ≥ 0 : Y (t) = i0}.

heδτi0i ≤ W (i), i ∈ S,

Lemma 4.2. Assume (A1) and krk∞ ≤ δ, where δ > 0 is given in (A1).
Then

| ¯ψn
v∗
2

(t, i)| ≤ W (i), t ≥ 0, i ∈ S.

Lemma 4.3. Assume (A1) and krk∞ ≤ δ, where δ > 0 is given in (A1).
Then

sup

t>0,i∈S

k ¯ψn
v∗
2

(t, i)k∞ < ∞.

Proof. Let i ≥ n + 1 and Yv∗
with initial condition i. Then from Theorem 4.1, we have

2 (·) be the solution corresponding to ˆv(·) ∈ M1

(t − (t ∧ τ ), Yv∗

2 (t ∧ τ ))i ,

ψn
v∗
2

ˆv,v∗
(t, i) ≤ E
2
i
ˆv,v∗
= E
2
i

v∗
2

v∗
2

R t∧τ

he
hψn
hψn
hψn
hψn
hψn

v∗
2

v∗
2

ˆv,v∗
+E
2
i
ˆv,v∗
= E
2
i

ˆv,v∗
+E
2
i

ˆv,v∗
≤ 1+E
2
i

0

rn(Yv∗
2

(s),ˆv(s,Yv∗
2

(s)),v∗

2 (s,Yv∗
2

(s)))ds

ψn
v∗
2

(t − (t ∧ τ ), Yv∗

(t − (t ∧ τ ), Yv∗

2 (t ∧ τ ))I{t ≤ τ }i
2 (t ∧ τ ))I{t > τ }i

(0, Yv∗

(t − τ, Yv∗

2 (t))I{t ≤ τ }i
2 (τ ))I{t > τ }i
2 (τ ))I{t > τ }i ,

(t, Yv∗

v∗
2

τ = inf{t ≥ 0 : Yv∗

2 (t) ∈ {1, 2, · · · , n}}.

where

RISK-SENSITIVE GAMES FOR CONTINUOUS TIME MARKOV CHAINS

13

In the last inequality we used the fact that ψn
v∗
2

for each ﬁxed i. Hence

(·, i) is nondecreasing in t

¯ψn
v∗
2

(t, i) ≤ 1 + max

j=1,...,n

¯ψn
v∗
2

(t, j) ≤ 1 + max

j=1,...,n

W (j)

since ψn
v∗
2
for each n ≥ 1, ¯ψn
v∗
2

(t, i0) ≥ 1 and last inequality follows from Lemma 4.2. Therefore
(cid:3)

is bounded.

Remark 4.1. From Lemma 4.3, it follows that for each n, ¯ψn
is uniformly
v∗
2
bounded (in t and i) and that bound is independent of the v∗
2. Therefore, we
conclude that for each n, ¯ψn is also uniformly bounded (in t and i), since
¯ψn
v∗
2

= ¯ψn.

Lemma 4.4. Assume (A1) and krk∞ ≤ δ, where δ > 0 is given in (A1).
Then

1

ψn(t, i0)

dψn
dt

sup

t≥0(cid:13)(cid:13)(cid:13)

< ∞.

(t, ·)(cid:13)(cid:13)(cid:13)W

(t, i) =

(t, i) +

d ¯ψn
dt

¯ψn(t, i)
ψn(t, i0)

dψn
dt

(t, i0)

Proof. Note that

1

ψn(t, i0)

dψn
dt

= inf
v1∈V1

= sup
v2∈V2

sup

v2∈V2hΠv1,v2
v1∈V1hΠv1,v2

inf

¯ψn(t, i) + rn(i, v1, v2) ¯ψn(t, i)i
¯ψn(t, i) + rn(i, v1, v2) ¯ψn(t, i)i.

The second equality follows from (4.6). Now the result follows from the fact

sup

i∈S,u∈U

[−¯πii(u)] = M < ∞

and Remark 4.1.

(cid:3)

Now we prove the existence of a solution to the HJI equation for the cost

function rn.

Theorem 4.2. Assume (A1) and krk∞ ≤ δ, where δ > 0 is given in (A1).
Then the equation

ρn ˆψn(i) = inf
v1∈V1

(4.11)

= sup
v2∈V2

sup

v2∈V2hΠv1,v2
v1∈V1hΠv1,v2

inf

ˆψn(i) + rn(i, v1, v2) ˆψn(i)i
ˆψn(i) + rn(i, v1, v2) ˆψn(i)i

has a solution (ρn, ˆψn(i)) satisfying ˆψn(i0) = 1. Also
(4.12)

ρn ≤ sup
v2∈M2

inf

v1∈M1

lim sup
T →∞

1
T

ln Ev1,v2

i

heR T
0 r(Y (s),v1(s,Y (s−)),v2(s,Y (s−)))dsi

and

(4.13)

0 < ˆψn(i) ≤ W (i), n ≥ 1, i ∈ S.

14

MRINAL K. GHOSH, K. SURESH KUMAR AND CHANDAN PAL

Proof. Using mean value theorem and Remark 4.1, there exists s(t, i) ∈
[t, 2t], t > 0 such that

By Lemma 4.2, we have

d ¯ψn
dt

lim
t→∞

(s(t, i), i) = 0 .

| ¯ψn(s(t, i), i)| ≤ W (i).

sup
t≥0

Using a diagonalization argument, along a subsequence, ¯ψn(s(t, i), i) →
ˆψn(i), for each i ∈ S for some ˆψn ∈ BW (S).
By Lemma 4.4, we have

1

ψn(s(t, i), i0)

sup
|
t≥0

dψn
dt

(s(t, i), i0)| < ∞.

Therefore, along a further subsequence denoted by the same notation by an
abuse of notation, we have

1

ψn(s(t, i), i0)

dψn
dt

(s(t, i), i0) → ρn, for some ρn ∈ R.

Hence, along a suitable subsequence, we have

d ¯ψn
dt

(s(t, i), i) +

¯ψn(s(t, i), i)
ψn(s(t, i), i0)

dψn
dt

(s(t, i), i0) → ˆψn(i)ρn.

By letting t → ∞ in (4.6) at t = s(t, i) along a suitable subsequence, and
using (A1) it follows that (ρn, ˆψn(i)) is a solution to the equation (4.11)
satisfying ˆψn(i0) = 1.

Let v∗

n = (v∗

1n, v∗

2n) : S → V be a min-max selector such that

sup

v2∈V2hΠv∗

1n(i),v2

= inf
v1∈V1

= sup
v2∈V2

sup

v2∈V2hΠv1,v2
v1∈V1hΠv1,v2

inf

= inf

v1∈V1hΠv1,v∗

2n(i)

ˆψn(i) + rn(i, v∗

1n(i), v2) ˆψn(i)i
ˆψn(i) + rn(i, v1, v2) ˆψn(i)i
ˆψn(i) + rn(i, v1, v2) ˆψn(i)i
2n(i)) ˆψn(i)i

ˆψn(i) + rn(i, v1, v∗

(4.14)

For vn := (v1, v∗
2n) ∈ M1 × M2, let Y (·) be the process (4.8) corresponding
to vn with initial condition i ∈ S. Then using Itˆo-Dynkin’s formula and
(4.14), we get

0 (rn(Y (s),v1(s,Y (s−)),v∗

v1,v∗
2n
E
i

heR T

2n(s,Y (s−)))−ρn)ds ˆψn(Y (T ))i − ˆψn(i) ≥ 0.

From Remark 4.1 it follows that for each n, ˆψn is bounded. Therefore we
have

v1,v∗
ˆψn(i) ≤ K(n)E
2n
i

0 (rn(Y (s),v1(s,Y (s−)),v∗

heR T

2n(s,Y (s−)))−ρn)dsi.

RISK-SENSITIVE GAMES FOR CONTINUOUS TIME MARKOV CHAINS

15

Taking logarithm, dividing by T and by letting T → ∞, we get

ρn ≤ lim sup
T →∞

1
T

v1,v∗
ln E
2n
i

heR T

Since v1 ∈ M1 is arbitrary, it follows that

0 rn(Y (s),v1(s,Y (s−)),v∗

2n(s,Y (s−)))dsi.

ρn ≤ inf

v1∈M1

lim sup
T →∞

1
T

v1,v∗
ln E
2n
i

Therefore we have

ρn ≤ sup
v2∈M2

inf

v1∈M1

lim sup
T →∞

Since rn ≤ r, we have

ρn ≤ sup
v2∈M2

inf

v1∈M1

lim sup
T →∞

1
T

1
T

0 rn(Y (s),v1(s,Y (s−)),v∗

heR T
2n(s,Y (s−)))dsi .
heR T
0 rn(Y (s),v1(s,Y (s−)),v2(s,Y (s−)))dsi .

ln Ev1,v2

i

ln Ev1,v2

i

heR T
0 r(Y (s),v1(s,Y (s−)),v2(s,Y (s−)))dsi .

(cid:3)

Now by using Theorem 4.2, one can closely mimic the arguments in the

proof of [[8], Theorem 3.3] to prove the following.

Theorem 4.3. Assume (A1) and krk∞ ≤ δ, where δ > 0 is given in (A1).
Then the equation (4.3) has a solution (ρ, ˆψ(i)) satisfying ˆψ(i0) = 1. Also
(4.15)

ρ ≤ sup
v2∈M2

inf

v1∈M1

lim sup
T →∞

1
T

ln Ev1,v2

i

heR T
0 r(Y (s),v1(s,Y (s−)),v2(s,Y (s−)))dsi .

To prove that the ρ in Theorem 4.3 is indeed the value of the game. We
used the atomic structure of the state dynamics, as in [8]. Let v∗
1 ∈ S1 be
the outer minimizing selector in (4.3) for player 1, v2 ∈ M2 any strategy for
player 2 and let Y (·) be a continuous time Markov chain corresponding to
(v∗

1, v2) ∈ S1 × M2.
Deﬁne the twisted kernel ˜P (j, i) associated with Y (·) and r as follows.

(4.16)

h(j) ˜P (j, i) =

Xj∈S

Set

v∗
1 ,v2
E
i

[eR 1
v∗
1 ,v2
E
i

0 r(Y (s),v∗
[eR 1

1 (Y (s−)),v2(s,Y (s−)))dsh(Y (1))]

0 r(Y (s),v∗

1 (Y (s−)),v2(s,Y (s−)))ds]

, i ∈ S, h ∈ B(S).

v∗
1 ,v2
eˆr(i) = E
i

[eR 1

0 r(Y (s),v∗

1 (Y (s−)),v2(s,Y (s−)))ds].

Let { ˜Yn} be a Markov chain on some probability space ( ˜Ω, ˜F , ˜P ) with tran-
sition kernel ˜P (·, ·) and initial condition i ∈ S. We will denote the corre-
sponding expectation by ˜Ei[·].
Fix i ∈ S. Let { ˜Yn} be the Markov chain given by (4.16) with ˜Y0 = i. Set
˜τ = inf{n ≥ 1| ˜Yn = i} := ˜τ1 and ˜τn+1 = inf{n ≥ ˜τn + 1| ˜Yn = i}. Deﬁne

D(ρ) = ˜Ei[eP ˜τ

n=1(ˆr( ˜Yn)−ρ)].

16

MRINAL K. GHOSH, K. SURESH KUMAR AND CHANDAN PAL

We state the following lemmas which play a crucial role in this section. Since
the proofs of these results closely mimic the corresponding proofs in [8], we
omit the details.

Lemma 4.5. Assume (A1) and krk∞ < δ. Then

(4.17)

D(ρ) ≤ 1.

Lemma 4.6. Assume (A1). Then for each i ∈ S such that W (i) ≥ 1+ be
e

δ
2 −1

3δ
2

,

˜Ei[eδ˜τ /2] ≤ e−δ/2(W (i) + be3δ/2),

where δ > 0 is given in (A1).

Deﬁne C0 = {i ∈ S : W (i) ≥ 1 + be
e

δ
2 −1

3δ
2

main theorem.

}. Now we state and prove the

Theorem 4.4. Assume (A1) and krk∞ < δ
Let (ρ, ˆψ(i)) be the solution obtained in Theorem 4.3. Then

2 , where δ > 0 is given in (A1).

ρ =

inf

v1(·)∈M1

sup

v2(·)∈M2

lim sup
T →∞

=

sup

v2(·)∈M2

inf

v1(·)∈M1

lim sup
T →∞

1
T

1
T

ln Ev1,v2

i

ln Ev1,v2

i

heR T
0 r(Y (s),v1(s,Y (s−)),v2(s,Y (s−)))dsi
heR T
0 r(Y (s),v1(s,Y (s−)),v2(s,Y (s−)))dsi ,

i.e., ρ is the value of the risk-sensitive ergodic game. Furthermore there
exists a pair of saddle point stationary Markov strategies (v∗
2) such that
v∗
1 is the outer minimizing selector in (4.3), and v∗
2 is the outer maximizing
selector in (4.3).

1, v∗

Proof. In view of Theorem 4.3, it remains to show that

ρ ≥ inf

v1(·)∈M1

sup

v2(·)∈M2

lim sup
T →∞

1
T

ln Ev1,v2

i

heR T
0 r(Y (s),v1(s,Y (s−)),v2(s,Y (s−)))dsi ,

1, v∗

2) such that v∗

and the existence of a pair of saddle point stationary Markov strategies
(v∗
2 is the
outer maximizing selector in (4.3).
Fix an i ∈ C0 and N ∈ N, deﬁne

1 is the outer minimizing selector in (4.3), and v∗

eBN (i) = ˜Ei"exp{

N ∧˜τ

Xk=1

(ˆr( ˜Yk) − ρ)}# for N ∈ N.

Arguing as in the proof of [[8], Theorem 3.5], it follows that

eBN +k(i) ≥ ˜Eih exp{

k

Xm=1

(ˆr( ˜Ym) − ρ) − N (kˆrk∞ + ρ)}i.

RISK-SENSITIVE GAMES FOR CONTINUOUS TIME MARKOV CHAINS

17

From Lemma 4.6, it follows that for each i ∈ C0, eBN (i) ≤ e−δ/2(W (i) +
be3δ/2). Therefore taking logarithm in both sides and letting k → ∞ we get

lim sup

k

1
k

ln ˜Eih exp{

k−1

Xm=0

ˆr( ˜Ym)}i ≤ ρ.

By using mathematical induction, we show that ∀k ∈ N

v∗
1 ,v2
E
i

h exp{Z k

0

r(Y (s), v∗

1(Y (s−)), v2(s, Y (s−)))ds}i = ˜Eih exp{
Xi=0

ˆr( ˜Yi)}i.

That is true for k = 1, by deﬁnition. Let this be true for k = n. Then for
k = n + 1

k−1

n

˜Eih exp{
Xi=0

ˆr( ˜Yi)}i
= ˜Eih exp{ˆr(i)} ˜E ˜Y1
Xi=0
= ˜Eih exp{ˆr(i)}E ˜Y1h exp{Z n

[exp{

n−1

0

ˆr( ˜Yi)}]i

r(Y (s), v∗

1(Y (s−)), v2(s, Y (s−)))ds}ii

v∗
1 ,v2
= E
i

E

0

v∗
1 ,v2

h exp{Z 1
Y (1)h exp{Z n
h exp{Z n+1

0

0

v∗
1 ,v2
= E
i

Hence we get

r(Y (s), v∗

1(Y (s−)), v2(s, Y (s−)))ds}

r(Y (s), v∗

r(Y (s), v∗

1(Y (s−)), v2(s, Y (s−))))ds}ii
1(Y (s−)), v2(s, Y (s−)))ds}i.

ρ ≥ lim sup
T →∞

1
T

v∗
1 ,v2
ln E
i

0 r(Y (s),v∗

Since v2 ∈ M2 is arbitrary, it follows that

heR T
heR T

1 (Y (s−)),v2(s,Y (s−)))dsi , i ∈ C0.
1 (Y (s−)),v2(s,Y (s−)))dsi , i ∈ C0.

v∗
1 ,v2
ln E
i

0 r(Y (s),v∗

ρ ≥ sup
v2∈M2

lim sup
T →∞
Therefore we have

1
T

ρ ≥ inf

v1∈M1

sup
v2∈M2

lim sup
T →∞

1
T

v∗
1 ,v2
ln E
i

0 r(Y (s),v∗

heR T

Thus

ρ = lim sup
T →∞

1
T

v∗
1 ,v∗
ln E
2
i

0 r(Y (s),v∗

1 (Y (s−)),v∗

1 (Y (s−)),v2(s,Y (s−)))dsi , i ∈ C0.
2 (Y (s−)))dsi , i ∈ C0.

Arguing as in the proof of [[8], Theorem 3.5], it follows that the above
equation holds for all i ∈ S, i.e.,

ρ = lim sup
T →∞

1
T

1 ,v∗
v∗
ln E
2
i

0 r(Y (s),v∗

1 (Y (s−)),v∗

2 (Y (s−)))dsi , i ∈ S.

heR T

heR T

18

MRINAL K. GHOSH, K. SURESH KUMAR AND CHANDAN PAL

It is easy to check that ρ is the value of the risk-sensitive ergodic game.
Moreover, the pair of stationary Markov strategies (v∗
1 is the
outer minimizing selector in (4.3), v∗
2 is the outer maximizing selector in
(4.3) forms a saddle point equilibrium. This completes the proof.
(cid:3)

2) where v∗

1, v∗

5. Conclusions

We have studied zero-sum risk-sensitive stochastic games for continuous
time Markov chains on a countable state space. For the ergodic case we
have taken the risk sensitive parameter θ = 1 for the sake of simplicity. If
we choose any other θ ∈ (0, Θ), then the assumption in Theorem 4.4 has to

be modiﬁed to θkrk∞ <
standard in the literature [1], [2], [5], [8]. The corresponding non-zero sum
case is currently under investigation.

. This is the so called small cost criterion which is

δ
2

Acknowledgments:
The work of the ﬁrst named author is supported in part by UGC Centre
for Advanced Study. The work of the second named author is supported
in part by the DST, India project no. SR/S4/MS:751/12. The work of the
third named author is supported in part by Dr. D. S. Kothari postdoctoral
fellowship of UGC.

References

[1] Basu, A. and Ghosh, M. K. 2012. Risk-sensitive stochastic diﬀerential games, Math.

Oper. Res. 37(3), 437- 449.

[2] Basu, A. and Ghosh, M. K. 2014. Zero-sum risk-sensitive stochastic games on a

countable state space, Stochastic Process. Appl. 124(1), 961-983.

[3] Beneˇs, V. E. 1970. Existence of optimal strategies based on speciﬁed information, for

a class of stochastic decision problems. SIAM J. Control. 8, 179-188.

[4] Fan, K. 1952. Fixed-point and minimax theorems in locally convex topological linear

spaces. Proc. Nat. Acad. Sc. 38, 121-126.

[5] Ghosh, M. K. and Saha, S. 2014. Risk-sensitive control of continuous time Markov

chain, Stochastics, 86, 655-675.

[6] Guo, X. P. and Herna´andez-Lerma O. 2009. Continuous time Markov decision pro-

cesses, Stochastic modelling and applied probability 62, Springer, Berlin 2009.

[7] Kumar, K. S. and Pal, C. 2013. Risk-Sensitive control of pure jump process on count-

able space with near monotone cost. Appl. Math. Optim. 68(3), 311-331.

[8] Kumar, K. S. and Pal, C. 2015. Risk-Sensitive ergodic control of continuous time
Markov processes with denumerable state space, Stoch. Anal. Appl., 33(5), 863-881.

Department of Mathematics, Indian Institute of Science, Bangalore -560012,

India.

Department of Mathematics, Indian Institute of Technology Bombay, Mum-

bai - 400076, India.

Department of Mathematics, Indian Institute of Science, Bangalore -560012,

India.

E-mail address: mkg@math.iisc.ernet.in, suresh@math.iitb.ac.in, chandan14@math.iisc.ernet.in

