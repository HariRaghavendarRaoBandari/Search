6
1
0
2

 
r
a

 

M
4
1

 
 
]

.

R
P
h
t
a
m

[
 
 

1
v
8
4
3
4
0

.

3
0
6
1
:
v
i
X
r
a

Maximum likelihood eigenfunctions of the Fokker Planck

equation and Hellinger projection

Damiano Brigo

Dept. of Mathematics

Imperial College London

180 Queen’s Gate

damiano.brigo@imperial.ac.uk

Giovanni Pistone

de Castro Statistics

Collegio Carlo Alberto
Via Real Collegio 30
10024 Moncalieri, IT

March 15, 2016

Abstract

We apply the L2 based Fisher-Rao vector-ﬁeld projection by Brigo, Hanzon and LeG-
land (1999) to ﬁnite dimensional approximations of the Fokker Planck equation on expo-
nential families. We show that if the suﬃcient statistics are chosen among the diﬀusion
eigenfunctions the ﬁnite dimensional projection or the equivalent assumed density approx-
imation provide the exact maximum likelihood density. The same result had been derived
earlier by Brigo and Pistone (2016) in the inﬁnite-dimensional Orlicz based geometry of
Pistone and co-authors as opposed to the L2 structure used here.

keywords Finite Dimensional Families of Probability Distributions, Exponential Families,
Mixture Families, Hellinger distance, Fisher information metric, Direct L2 metric, Kullback
Leibler information, Eigenfunctions, Fokker Plack equation, Forward Kolmogorov Equation,
Maximum Likelihood Estimator.

1

Introduction

We apply the L2 based Hellinger distance and the metric it induces on ﬁnite dimensional fam-
ilies, namely the Fisher-Rao metric, to the vector-ﬁeld projection of measure-valued evolution
equations, and of the Fokker Planck equation in particular, onto ﬁnite dimensional manifolds.
This follows the work initially sketched in Hanzon (1987) [9] and fully developed in Brigo et
al (1999)[7] in the context of stochastic ﬁltering and provides a general method to derive ﬁnite
dimensional approximations of the Fokker Planck equation on exponential families. Using an
alternative metric we also derive a second type of vector ﬁeld projection that works better
with mixture families. In the exponential family/Fisher Rao case, we show that if the suﬃ-
cient statistics of the exponntial family on which we project are chosen among the diﬀusion
eigenfunctions then the ﬁnite dimensional projection or the equivalent assumed density approx-
imation provide the exact maximum likelihood density. For the assumed density approximation
see for example [10] for Gaussian families and [7] for exponential families. For references and
a detailed literature review see [8], where this same result is presented in the conclusions of a
much longer work and in the context of the richer statistical manifolds geometry of G. Pistone
and co-authors [12], based on Orlicz spaces, rather than on the minimal L2 structure we use
here.

D. Brigo, G. Pistone. ML of Fokker Planck eq. on exponential families via eigenfunctions

2

2 Statistical manifolds

For a full summary see [7]. We consider parametric families of probability densities, {p(·, θ), θ ∈
Θ} with Θ convex open set in Rn. The set of square roots of such densities is a subset of L2
that we may view as a ﬁnite dimensional manifold. In general the L2 distance between square
roots of densities leads to the Hellinger distance. A curve in such a manifold is given by

t 7→pp(·, θ(t)). Diﬀerentiating with respect to t we obtain that all tangent vectors at θ are in
the space span(cid:26) ∂√p(·,θ)
, i = 1, . . . , n(cid:27). We can use the L2 inner product h·,·i to introduce an
∂√p(·,θ)
inner product on the tangent space and a metric. Deﬁne gi,j(θ)/4 =(cid:28) ∂√p(·,θ)
∂θj (cid:29). This

is, up to the factor 4, the familiar Fisher-Rao information matrix. If we have a L2 vector v, we
can project it via the orthogonal projection

∂θi

∂θi

,

Πg

θ[v] =

m

m

Xi=1

[

Xj=1

4gij(θ) hv,

∂p(·, θ)
∂θj

i]

1

2pp(·, θ)

∂p(·, θ)

∂θi

(1)

1

2pp(·, θ)

where upper indices denote the inverse matrix. A diﬀerent possibility is a geometry that
does not use the square root. This would be done most generally using a duality argument
involving L1 and L∞, but here we will assume that all densities are square integrable, so
that all p’s we deal with are in L2. Then we can mimick the above structure but without

∂θi

m

m

square roots. The curve is t 7→ p(·, θ(t)), the tangent space is spann ∂p(·,θ)
Deﬁne γi,j(θ) = D ∂p(·,θ)
γij(θ) hv,

, i = 1, . . . , no.
∂θj E. This leads to what we called direct metric projection in [3],
∂p(·, θ)
∂θj

. There is another way of measuring how close

∂p(·, θ)

, p(·,θ)

Πγ

θ [v] =

i]
densities p and q: K(p, q) := R log p(x)

two densities are. Consider the Kullback–Leibler information or relative entropy between two
q(x) p(x) dx. This is not a metric, since it is not symmetric
and it does not satisfy the triangular inequality. It is a classic result that the Fisher metric
and the Kullback–Leibler information coincide inﬁnitesimally. Indeed, by Taylor expansion it
is easy to show that

∂θi

Xi=1

∂θi

[

Xj=1

K(p(·, θ), p(·, θ + dθ)) =

m

Xi,j=1

gij(θ) dθi dθj + O(|dθ|3) .

(2)

3 Vector ﬁeld projection of Fokker Planck

We ﬁrst summarize the key results for projection in Hellinger distance g on exponential families
and for projection in direct metric γ on mixture families. For a full account see [7] for the
Hellinger case and [3] for the direct metric case. Consider a stochastic diﬀerential equation on
a probability space (Ω,F , (Ft)t, P) taking values in RN ,

dXt = f (Xt, t)dt + σ(Xt, t)dWt, X0, a(x, t) = σ(x, t)σ′(x, t)

where the prime index denotes transposition. Let us start from the Fokker Planck equation
for the probability density pt(x) = p(x, t) of the solution Xt of our SDE. Examples of possible
applications of approximating the Fokker Planck equation and its stochastic PDE extensions are
given in [8], and include signal processing, stochastic- (local-) volatility modeling in quantitative

D. Brigo, G. Pistone. ML of Fokker Planck eq. on exponential families via eigenfunctions

3

ﬁnance, the anisotropic heat equation in physics, and quantum theory evolution equations. For
our SDE the Fokker Planck equation reads

∂p(x, t)

∂t

= L∗t p(x, t), L∗t p = −

∂
∂xi

n

Xi=1

[fi(·, t) p] +

1
2

n

Xi,j=1

∂2

∂xi∂xj

[aij(·, t) p].

In many cases the solution of this equation is inﬁnite dimensional. We may need ﬁnite
dimensional approximations. We can project this parabolic PDE according to either the L2
direct metric (inducing the metric γ(θ)) or, by deriving the analogous equation for √pt,
∂√p
∂t = 1

√pL∗t p, according to the Hellinger metric (inducing g(θ)). The respective projections

d
dt

p(·, θ(t)) = Πγ

θ(t) [L∗t p(·, θ)] ,

d

dtpp(·, θ(t)) = Πg

θ(t)"

1

pp(·, θ)L∗t p(·, θ)#

(3)

transform the PDE into a ﬁnite dimensional ODE for θ via the chain rule:

d
dt

ρ(p(·, θt)) =

∂ρ(p(·, θ))

∂θj

m

Xj=1

˙θj(t), ρ(p) = p or ρ(p) = √p.

For brevity we write, for suitable functions ϕ, Eθ[ϕ] := R ϕ(x)p(x, θ)dx. The projections in
direct and Hellinger metric respectively yield, after an integration by parts using the fact that
L∗ is the formal adjoint of L:

˙θi
t =

m

Xj=1

γij(θt) Eθt(cid:20)L(cid:18)∂p(·, θt)

∂θj (cid:19)(cid:21) ,

˙θi
t =

m

Xj=1

gij(θt) Eθt(cid:20)L(cid:18)∂ log p(·, θt)

∂θj

(cid:19)(cid:21) ,

θi
0 .

(4)

We now choose speciﬁc families to carry out the projection. In particular, we project on the
exponential family using the Hellinger distance and on the mixture family using the direct
distance. The exponential families we consider are EF(c), whose generic density is deﬁned
as p(x, θ) := exp[θ′c(x) − ψ(θ)]. The functions c are the suﬃcient statistics of the family,
the parameters θ ∈ Rn are the canonical parameters of the family. This works well with the
Hellinger/Fisher Rao choice because the tangent space has a simple structure: square roots do
not complicate issues thanks to the exponential structure. Moreover, the Fisher matrix has a
simple structure: ∂2
θi,θj ψ(θ) = gij(θ). The structure of the projection Πg is simple for exponential
families. Finally, alternative coordinates, expectation parameters, deﬁned via η(θ) = Eθ[c] =
∂θψ(θ) are available, with the two coordinate systems η and θ being bi-orthogonal.

For the direct metric, we will project on mixture families. We deﬁne a simple mixture family
as follows. Given m + 1 ﬁxed squared integrable probability densities q = [q1, q2, . . . , qm+1]′,
deﬁne ˆθ(θ) := [θ1, θ2, . . . , θm, 1 − θ1 − θ2 − . . . − θm]′ for all θ ∈ Rm. The simple mixture family
(on the simplex) is deﬁned as p(·, θ) = ˆθ(θ)′q. If we consider the L2 / γ(θ) distance, the metric
γ(θ) itself and the related projection become very simple and do not depend on the point θ.
For example, ∂p(·,θ)
= qi − qm+1. Accordingly, the tangent space at p(·, θ) does not depend on
∂θi
θ and is given by span{q1 − qm+1, q2 − qm+1,· · · , qm − qm+1}. Also the L2 projection becomes
particularly simple and is equivalent to a classic Galerkin method. One has that the ﬁrst Eq.
(4) specializes to

˙θi
t =

γij

m

Xj=1

m+1

Xk=1

ˆθk(t)Z (L (qj(x) − qm+1(x))) (qk(x) − qm+1(x))dx, θi

0 .

(5)

D. Brigo, G. Pistone. ML of Fokker Planck eq. on exponential families via eigenfunctions

4

which is a linear equation. See [3] for more details, we do not pursue mixtures further here and
we focus on Hellinger distance and exponential families.

The projection of the Fokker Planck equation in Fisher metric has been introduced in [4].
First of all we refer to the the Hellinger projection above, the second Eq.
in (4), as to the
“vector ﬁeld projection”. This is because we project the L2 vector ﬁeld of the Fokker Planck
equation for √p instant by instant onto the tangent space of EF(c), thus obtaining a vector
ﬁeld in EF(c). For the second Eq. in (3) to hold we need to ensure that (L∗t p(·, θ))/pp(·, θ) is

indeed an L2 vector for all t and θ. This holds in turn if the condition

sup
t≥0

Eθ"(cid:12)(cid:12)(cid:12)(cid:12)

L∗t p(·, θ)

p(·, θ) (cid:12)(cid:12)(cid:12)(cid:12)

2# < ∞ for all θ ∈ Θ

holds, see [7], where it is shown that suﬃcient conditions guaranteeing this in EF(c) are the
following: ft and its ﬁrst derivatives, at and its ﬁrst and second derivatives, and c with its
ﬁrst and second derivatives have at most polynomial growth, and densities in EF(c) integrate
any polynomial. The paper [7] also discusses conditions under which all vector ﬁelds are well
deﬁned and the projection is well deﬁned, see Theorem 5.4 for the special case h = 0.

Again for brevity, we write Eη[ϕ] := R ϕ(x)p(x; η(θ))dx. When projecting on EF(c) the

second Eq. (4) specializes into

˙θi
t =

m

Xj=1

gij(θt) Eθt [Lci] , θi
0 ,

or

t = Eηt [Lci] , ηi
˙ηi

0

(6)

where the second equation has been obtained from the ﬁrst by recalling that dη(θ) = g(θ)dθ.

A natural question when projecting is how good the projection is locally? Also, one would
like to have a measure for how far the projected evolution is, locally, from the original one. We
now deﬁne a local projection residual as the L2 norm of the Fokker Planck inﬁnite dimensional
vector ﬁeld minus its ﬁnite-dimensional orthogonal projection. Deﬁne the vector ﬁeld minus its
projection and its norm as

εt(θ) := L∗t p(·, θ)

2pp(·, θ) − Πg

2pp(·, θ)# , R2
θ" L∗t p(·, θ)

t := kεt(θ)k2

2

The projection residual Rt can be computed jointly with the projected equation evolution (6)
to have a local measure of the goodness of the approximation involved in the projection.

Monitoring the projection residual and its peaks can be helpful in tracking the projection
method performance, see also [7] for examples of L2-based projection residuals in the more
complex case of the Kushner-Stratonovich equations of nonlinear ﬁltering. However, the pro-
jection residual only allows for a local approximation error numerical analysis. To illustrate
this, assume for a moment that time is discrete 1, 2, 3, . . .. To make the point, we are artiﬁcially
separating projection and propagation and the local and global errors. This is not completely
precise but allows us to make an important point on our method. If we start from the manifold
with a p0 = p(·, θ0) ∈ EF(c) (omitting square roots in the notation and with upper indices
denoting time), the Fokker Planck vector ﬁeld driven by L∗ will move us out of the manifold
as the L2 vector related to L∗p0 will not be tangent to EF(c). We then project this L2 vector
on the tangent space of EF(c) and follow it, obtaining a new p(·, θ1) in the manifold. Now we
continue, again the Fokker Planck vector ﬁeld driven by L∗p(·, θ1) would bring us out of EF(c),
and to avoid this we project it onto the tangent space and follow the projected vector, obtaining
p(·, θ2). The crucial point here is that this second step was done starting from an approximate

D. Brigo, G. Pistone. ML of Fokker Planck eq. on exponential families via eigenfunctions

5

point p(·, θ1) rather than from the true Fokker Planck density p1. This means that, besides the
local projection error measured by Rt, we have a second error coming from the fact that we
start the projection from the wrong point. If we leave the global approximation error analysis
aside for a minute, the big advantage of the above method is that it does not require us to
know the true solution of the Fokker Planck equation to be implemented. Indeed, Equation (6)
works perfectly well without knowing the true solution pt.

4 Entropy point projection & eigenfunctions based ML

Now, to study the global error, we introduce a second projection method. We call this method
“point projection”, since here we will project directly the L2 densities onto EF(c) rather than
their evolutions, and in particular we will make no use of tangent spaces. Point projection
will require us to know the true solution, so as an approximation method it will be pointless.
However,
it will help us with the global error analysis, and a modiﬁcation of the method
based on the assumed density approximation will allow us to ﬁnd an algorithm that does not
require the true solution. The point projection method works as follows. Starting again from
the manifold with a p0 = p(·, θ0) ∈ EF(c), the Fokker Planck vector ﬁeld driven by L∗ will
move us out of the manifold; we follow this vector ﬁeld and reach p1. To go back to EF(c),
we project p1 onto the exponential family by minimizing the relative entropy, or Kullback
Leibler information of p1 with respect to EF(c), ﬁnding the orthogonal projection of p1 on
EF(c).
It is well known that the orthogonal projection in relative entropy is obtained by
matching the suﬃcient statistics expectations of the true density. Namely, the projection is
the particular exponential density of EF(c) with c-expectations η1 = Ep1[c]. See for example
[5] for a quick proof and an application to ﬁltering in discrete time. We know that EF(c),
besides θ, admits another important coordinate system, the expectation parameters η. If one
deﬁnes η(θ) = Ep(θ)[c] as above, then dη(θ) = g(θ)dθ where g is the Fisher metric. Thus, we
can take the η1 above coming from the true density p1 and look for the exponential density
p(·; η1) sharing these c-expectations. This will be the closest in relative entropy to the true p1
in EF(c). We then continue moving forward in time, iterating this algorithm.
The advantage of this method compared to the previous vector ﬁeld based one is that we
ﬁnd at every time the best possible approximation (“maximum likelihood”) of the true solution
in EF(c). The disadvantage is that in order to compute the projection at every time, such as for
example η1 = Ep1[c], we need to know the true solution p1 at that time. However it turns out
that we can somewhat combine the two ideas and analyze the error if we invoke the assumed
density approximation. This works as follows. Diﬀerentiate both sides of ηt = Ept[c] to obtain

˙ηt =

d

dtZ c(x)pt(x)dx =Z c(x)

∂pt(x)

∂t

dx =Z c(x)L∗t pt(x)dx = Ept[Lc]

so that ˙ηt = Ept[Lc]. This last equation is not a closed equation, since pt in the right hand side
is not characterized by η. Thus, to be solved this equation should be coupled with the original
Fokker Planck for pt and we would still be in inﬁnite dimension. However, at this point we
can close the equation by invoking the assumed density approximation: we replace pt with the
exponential density p(·, ηt). We obtain

d
dt

˜ηt = E˜ηt[Lc].

This is now a ﬁnite dimensional ODE for the expectation parameters. There is more: this
last equation is the same as our earlier vector ﬁeld based projected equation (6). This result

D. Brigo, G. Pistone. ML of Fokker Planck eq. on exponential families via eigenfunctions

6

had been proven for nonlinear ﬁltering in [7], although here we gave a more direct derivation.
Intuitively, the result is related to the fact that the Fisher-Rao metric and relative entropy are
inﬁnitesimally equivalent, see Eq (2).

Theorem 4.1 Closing the evolution equation for the relative entropy point projection of the
Fokker Planck solution onto EF(c) by forcing an exponential density on the right hand side is
equivalent to the approximation based on the vector ﬁeld projection in Fisher metric.

We can now attempt an analysis of the error between the best possible projection ηt and the
vector ﬁeld based (or equivalently assumed density approximation based) projection ˜η. To do
this, write ǫt := ηt − ˜ηt, expressing the diﬀerence between the best possible approximation and
the vector ﬁeld projection / assumed density one, in expectation coordinates. Diﬀerentiating
we see easily that ˙ǫt = (Ept[Lc] − Ep(˜ηt)[Lc]). Now suppose that the c statistics in EF(c) are
chosen among the eigenfunctions of the operator L, so that Lc = −Λc, where Λ is a n × n
diagonal matrix with the eigenvalues corresponding to the chosen eigenfunctions. Substituing,
we obtain

˙ǫt = −Λ(Ept[c] − Ep(˜ηt)[c])

or

˙ǫt = −Λǫt ⇒ ǫt = exp(−Λt)ǫ0

so that if we start from the manifold (ǫ0 = 0) the error is always zero, meaning that the vector
ﬁeld projection gives us the best possible Maximum Likelihood (ML) approximation.
If we
don’t start– from the manifold, ie if p0 is outside EF(c), then the diﬀerence between the vector
ﬁeld approach and the best possible approximation dies out exponentially fast in time provided
we have negative eigenvalues for the chosen eigenfunctions. This leads to the main result in
this paper:

Theorem 4.2 (ML for the Fokker Planck Equation & Fisher-Rao projection.) The vec-
tor ﬁeld projection approach leading to (6) provides also the best possible approximation of the
Fokker Planck equation solution in relative entropy in the family EF(c), provided that the suﬃ-
cient statistics c are chosen among the eigenfunctions of the adjoint operator L of the original
Fokker Planck equation, and provided that EF(c) is an exponential family when using such eigen-
functions. In other words, under such conditions the Fisher Rao vector ﬁeld projected equation
(6) provides the exact maximum likelihood density for the solution of the Fokker Planck equation
in the related exponential family.

The choice or availability of suitable eigenfunctions is not always straightforward, except in
a few simple cases. See [11] for a discussion on eigenfunctions for the Fokker Planck equation.
For example, in the one dimensional case N = 1 where the diﬀusion is on a bounded domain
[ℓ, r] with reﬂecting boundaries and strictly positive diﬀusion coeﬃcient σ then the spectrum
of the operator L is discrete, there is a stationary density and eigenfunctions can be expressed
with respect to this stationary density, that could be taken as background measure, see [8]. For
the case N > 1 special types of SDEs allow for a speciﬁc eigenfunctions/eigenvalue analysis,
see again [11]. Further research is needed to explore the eigenfunctions approach in connection
with maximum likelihood.

References

[1] J. Aggrawal: Sur l’information de Fisher. In: Th´eories de l’Information (J. Kampe de

Feriet, ed.), Springer-Verlag, Berlin–New York 1974, pp. 111-117.

[2] Amari, S. Diﬀerential-geometrical methods in statistics, Lecture notes in statistics,

Springer-Verlag, Berlin, 1985

D. Brigo, G. Pistone. ML of Fokker Planck eq. on exponential families via eigenfunctions

7

[3] John Armstrong and Damiano Brigo. Nonlinear ﬁltering via stochastic PDE projection
on mixture manifolds in L2 direct metric. Mathematics of Control, Signals and Systems,
28(1):1–33, 2016.

[4] Brigo, D.: On nonlinear SDEs whose densities evolve in a ﬁnite–dimensional family. In:
Stochastic Diﬀerential and Diﬀerence Equations, Progress in Systems and Control Theory,
vol. 23, pp. 11–19. Birkh¨auser Boston (1997)

[5] Brigo, D.: On some ﬁltering problems arising in mathematical ﬁnance. Insurance: Math-

ematics and Economics 22(1), 53–64 (1998)

[6] Brigo, D, Hanzon, B, LeGland, F, A diﬀerential geometric approach to nonlinear ﬁltering:

The projection ﬁlter, IEEE T AUTOMAT CONTR, 1998, Vol: 43, Pages: 247 – 252

[7] Brigo, D, Hanzon, B, Le Gland, F, Approximate nonlinear ﬁltering by projection on ex-

ponential manifolds of densities, BERNOULLI, 1999, Vol: 5, Pages: 495 – 534

[8] Brigo, D., and Pistone, G. (2016). Dimensionality reduction for measure-valued evo-
lution equations in statistical manifolds. To appear in: Nielsen, F., Critchley, F., &
Dodson, K. (Eds), Proceedings of the conference on Computational Information Ge-
ometry for Image and Signal Processing, Springer Verlag, 2016. Preprint available at
http://arxiv.org/abs/1601.04189

[9] Hanzon, B. A diﬀerential-geometric approach to approximate nonlinear ﬁltering. In C.T.J.
Dodson, Geometrization of Statistical Theory, pages 219 – 223,ULMD Publications, Uni-
versity of Lancaster, 1987.

[10] Maybeck, P. S. (1982). Stochastic models, estimation and control. Academic Press.

[11] G. A. Pavliotis. Stochastic Processes and Applications: Diﬀusion Processes, the Fokker-

Planck and Langevin Equations. Springer, Heidelberg, 2014.

[12] Pistone, G., and Sempi, C. (1995). An Inﬁnite Dimensional Geometric Structure On the
space of All the Probability Measures Equivalent to a Given one. The Annals of Statistics
23(5), 1995

