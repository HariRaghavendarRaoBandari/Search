A Discontinuous Neural Network

for Non-Negative Sparse Approximation

Martijn Arts, Marius Cordts, Monika Gorin, Marc Spehr, and Rudolf Mathar

1

Abstract—This paper investigates a discontinuous neural net-
work which is used as a model of the mammalian olfactory system
and can more generally be applied to solve non-negative sparse
approximation problems. By inherently limiting the systems
integrators to having non-negative outputs, the system function
becomes discontinuous since the integrators switch between being
inactive and being active. It is shown that the presented network
converges to equilibrium points which are solutions to general
non-negative least squares optimization problems. We specify a
Caratheodory solution and prove that the network is stable,
provided that the system matrix has full column-rank. Under
a mild condition on the equilibrium point, we show that the
network converges to its equilibrium within a ﬁnite number of
switches. Two applications of the neural network are shown.
Firstly, we apply the network as a model of the olfactory system
and show that in principle it may be capable of performing
complex sparse signal recovery tasks. Secondly, we generalize
the application to include non-negative sparse approximation
problems and compare the recovery performance to a classical
non-negative basis pursuit denoising algorithm. We conclude
that the recovery performance differs only marginally from the
classical algorithm, while the neural network has the advantage
that no performance critical regularization parameter has to be
chosen prior to recovery.

Index Terms—Discontinuity, optimization, NNLS, non-negative

sparse approximation, olfactory system.

I. INTRODUCTION

S PARSITY is a concept

that has recently generated a
considerable amount of interest in mathematics, computer
science and electrical engineering. In essence, it states that
many signals may be represented by only a small number
of non-zero coefﬁcients using a suitable basis or dictionary.
The ﬁeld of compressed sensing exploits knowledge about the
sparsity which allows to recover signals from a smaller number
of measurements compared to classical methods [1], [2]. This
is done by solving optimization problems that penalize non-
sparse solution candidates.

In this work, we study a sparse signal recovery problem
faced by the mammalian main olfactory system, namely the
decomposition of a mixture of chemical stimuli from noisy
measurements. The result of our modeling is a discontinuous

6
1
0
2

 
r
a

 

M
1
2

 
 
]
E
N
.
s
c
[
 
 

1
v
3
5
3
6
0

.

3
0
6
1
:
v
i
X
r
a

M. Arts and R. Mathar are with the Institute for Theoretical Information

Technology, RWTH Aachen University, Aachen, Germany.

M. Cordts is now with the Environment Perception Dept. at Daimler R&D,

Sindelﬁngen, Germany.

M. Gorin and M. Spehr are with the Department of Chemosensation,

Institute for Biology II, RWTH Aachen University, Aachen, Germany.

This work was partly supported by the Deutsche Forschungsgemeinschaft
(DFG) SPP 1395: Informations- und Kommunikationstheorie in der Moleku-
larbiologie (InKoMBio), Grants: MA 1184/20-2 & SP 724/8-1.

Please direct your e-mail correspondence to: arts@ti.rwth-aachen.de
This work has been submitted to the IEEE for possible publication.
Copyright may be transferred without notice, after which this version may
no longer be accessible.

neural network that solves a quadratic program (QP) in order
to recover the signal of interest from the measurements. We
later show that this network can be applied more generally to
solve non-negative sparse approximation problems, which do
not have to be related to the biological application.

The connection between neural networks and optimization
problems has been investigated for several decades now. A par-
ticularly interesting class, called (continuous) Hopﬁeld Neural
Networks (HNNs), was proposed by Hopﬁeld in the 1980s
[3], [4]. These networks consider either weighted excitatory or
inhibitory feedback between several “neurons”. The neurons
themselves are modeled as ampliﬁers with a potentially arbi-
trary input-output function. A sufﬁcient condition for stability
of such a network is that the synaptic weights are symmetric
and feedback loops, i.e., the output of a neuron is connected
to its own input, are not present. Such networks can be
used to solve a variety of optimization problems, see [5]
for a review. In the context of QPs, known networks used
to solve (bounded) QPs such as [6], [7] are functionally
related to the network presented in this work. These net-
works, however, differ fundamentally in structure and use other
methods to investigate the convergence behavior compared to
our approach. Speciﬁcally for sparse approximation scenarios,
Locally Competitive Algorithms (LCAs) [8] were proposed.
They solve optimization problems with a quadratic objective
function, regularized by a cost function that is applied to the
output of the integrators. Depending on the choice of the cost
function, these neural networks solve the well known basis
pursuit denoising problem (see also (3)) or an approximation
of the original non-convex compressed sensing problem, where
each non-zero entry is penalized independent of its actual
amplitude (cf. (1)). The convergence behavior of the LCAs
was studied in detail in [9].

The contributions of this paper are summarized in the
following. We study a neural network in the form of a dynamic
system where the integrators of the system are limited to
having non-negative outputs. This results in discontinuities of
the system function, since the integrators switch between being
active and inactive depending on the value of their output.
First, we show that the neural networks equilibrium points
are the minimizers of a general non-negative least squares
optimization problem, then we specify a Caratheodory solution
for the systems discontinuous differential equations. Having
deﬁned the Caratheodory solution, we use Lyapunov’s direct
method to show that the neural network is asymptotically
stable, provided that
the system matrix has full column-
rank. Under a mild condition on the equilibrium point, we
show that the network converges to this point within a ﬁnite
number of switches. Subsequently, we discuss that the non-
negative integrators can be generalized to arbitrary lower and

upper limits, so that the equilibrium point corresponds to the
solution of a bounded least squares problem. Two different
applications of the neural network are presented. The ﬁrst
is the original application, which served as the foundation
for the presented neural network. It is derived as a model
of the mammalian olfactory system and we investigate the
performance of a stimulus mixture decomposition scenario.
Based on the network performance, we conclude that
the
olfactory system may indeed recover the composition of
chemical stimuli from a noisy mixture of neural responses. The
second application deals with the more general class of non-
negative sparse approximation problems. Here, we compare
the recovery performance of the neural network with a non-
negative basis pursuit denoising method and show that for
this type of application the stability of the network remains
unaffected even if the system matrix does not have full
column-rank. From this investigation, we conclude that the
neural network performance differs only marginally compared
to the “classical” recovery method. However, the network has
the additional advantage that no performance critical regular-
ization parameter has to be determined prior to recovery.

An overview of the structure of this paper is given in
the following. In Section II-A, we give the necessary back-
ground about compressed sensing / sparse approximation and
summarize relevant results on the when dealing with non-
negative sparse recovery problems. A concise overview of
the stability theory of (discontinuous) dynamic systems can
be found in Section II-B. The discontinuous neural network
which is considered in this work is presented in Section III
and relevant notation is deﬁned. A formal investigation of
the convergence properties and stability analysis of the neural
network is given in Section IV. We present two applications of
the presented neural network. Firstly, it is used as a model of
the mammalian olfactory system, where it is shown that such
a network would be able to decompose a mixture of chemical
stimuli (Section V). Secondly, in Section VI, we show that the
network is more generally able to solve non-negative sparse
approximation problems. Section VII concludes the paper.

II. BACKGROUND

A. Compressed Sensing / Sparse Approximation

Compressed sensing is a relatively new research ﬁeld that
deals with recovering signals from a lower number of linear
measurements compared to classical methods of signal recov-
ery [1], [2]. The ﬁeld relies on a concept called sparsity, i.e.,
the assumption that the signal can be represented by only a
few nonzero coefﬁcients in a suitable basis. A signal x ∈ RN
is called s-sparse if at most s coefﬁcients of the vector x are
nonzero, that is, if (cid:107)x(cid:107)0 ≤ s. Here, (cid:107)x(cid:107)0 := #{j : xj (cid:54)= 0}
denotes the (cid:96)0 pseudo-norm. Note that
this ”norm” does
not fulﬁll the homogeneity property and therefore does not
represent a proper norm.
The task in compressed sensing is to recover an s-sparse
signal x from a measured signal b ∈ RM , obtained by a
linear measurement process described by b = Ax with A ∈
RM×N . Here, the number of measurements is small compared
to the dimension of the original signal, i.e., M (cid:28) N. Ideally,

one would like to recover x using the following optimization
problem:

minimize

x

(cid:107)x(cid:107)0

2

(1)

subject to b = Ax.

However, solving problem (1) is NP-hard and thus intractable
[2]. If the matrix A fulﬁlls certain conditions however, e.g.,
the so-called null space property or the restricted isometry
property, the minimizer of the (cid:96)1-minimization problem (also
called basis pursuit)

minimize

x

(cid:107)x(cid:107)1

subject to b = Ax,

(2)

is also the minimizer of problem (1) and hence may recover
the desired signal vector exactly [1], [2]. In problem (2), the
(cid:96)1-norm, which is the convex relaxation of the (cid:96)0-“norm”, was
used as the objective. Note, that we conﬁned x, A and b to
the real numbers but the results summarized above also hold
in the complex setting.

In practical scenarios, virtually all measurements are per-
turbed by some form of noise. Thus, in practice the mea-
surement process may be modeled as b = Ax + ε where
ε ∈ RM denotes the noise component. In this case, taking
M = N measurements will not guarantee exact recovery of
the desired signal vector anymore. Hence, one would typically
take more measurements (M > N) and try to recover a
good approximation of the signal vector through least-squares
minimization. Likewise, in compressed sensing, one cannot
hope to recover the exact signal vector when measurements are
noisy. Consequently, the linear constraint b = Ax in problem
(2) is too strict since it does not allow for deviations caused
by noise. To cope with this, one may replace the constraint
in problem (2) by a quadratic constraint (cid:107)Ax − b(cid:107)2
2 ≤ cε
which allows for deviations within a sphere with constant
radius. To choose cε meaningfully, it is necessary to have
prior knowledge about the noise power. Another method is
regularizing the objective function, as done in basis pursuit
denoising (BPDN):

minimize

x

(cid:107)Ax − b(cid:107)2

2 + α(cid:107)x(cid:107)1 .

1
2

(3)

Here, α is the regularization parameter which allows to tune
the inﬂuence of the two terms in the objective. Naturally,
exact recovery of the signal vector cannot be guaranteed in
the presence of noise, however, under certain conditions the
recovery error can be bounded [10] w.r.t. cε.

The discovery of recovery guarantees and error bounds re-
sulted in compressed sensing becoming a very active research
ﬁeld. Many applications have been investigated, among them
magnetic resonance imaging (MRI), data acquisition and a
multitude of signal processing tasks [11]. A related research
ﬁeld is sparse approximation. While in compressed sensing
it is typically assumed that the matrix A may be modiﬁed,
in sparse approximation the matrix is ﬁxed and given by the
application. Since the recovery algorithms are essentially the
same, we do not make a special distinction between these two
ﬁelds here.

Quite surprising results have been reported when the recov-
ery of non-negative signal vectors, i.e., x ∈ RN≥0 is desired.
Such non-negativity constraints appear naturally in, e.g., image
processing, text & data mining and environmetrics (see also
[12]). In the noiseless case, the non-negative signal recovery
problem can be formalized as:

minimize

x

(cid:107)x(cid:107)0

subject to b = Ax
x ≥ 0.

Interestingly, the solution space {x | Ax = b, x ≥ 0} is a
singleton, provided that certain conditions on x and the matrix
A are fulﬁlled [13], [14]. Firstly, x must be sufﬁciently sparse,
where a threshold on the sparsity is deﬁned by the one-sided
coherence of the matrix A. The condition on the matrix states
that the row-span of A must intersect the positive orthant, i.e.,
a linear combination h must exist, such that hT A > 0.

Let us consider an optimization problem with a general
objective function f as well as constraints Ax = b and
x ≥ 0. Furthermore, let the conditions discussed above be
fulﬁlled. Then, since the solution to the linear equation system
with non-negativity constraint is unique, for a broad class
of objective functions f, including the sparsity inducing (cid:96)0-
“norm” and (cid:96)1-norm, it is the solution to the optimization
problem as well [13]. This means, that problem (4) can be
solved efﬁciently by a linear program, a result which is similar
to the classical compressed sensing scenario.

Turning to the noisy case again, where the measurements
are obtained by b = Ax + ε, perfect recovery is generally
not possible. Similar to the classical compressed sensing
case, one could add the non-negativity constraint to problem
(3), resulting in the non-negative basis pursuit denoising
(NNBPDN) problem. Inspired by the noiseless results above,
one could argue that the sparsity inducing objective function
may be redundant, so that solving a non-negative least squares
(NNLS) problem (see problem (8)) would sufﬁce. This idea
was investigated in [15], where subsequent thresholding was
suggested to gain truly sparse solutions. In the same work,
error bounds were given under the condition that the matrix
M AT A)h ≥
A fulﬁlls the self-regularizing property: hT ( 1
0(1T h)2 ∀h ≥ 0, where c0 ≥ 0 is a universal constant.
c2
One advantage of the NNLS approach to solving non-negative
sparse approximation problems is that the method is indepen-
dent of any regularization parameters.

3

In many cases, it is desirable to analyze dynamical sys-
tems for stability. Stable systems ensure a certain level of
predictability, thus this concept is very important in control
theory. We brieﬂy revise the deﬁnitions of stability, which
are relevant
to this work, following [16]. A point xe of
a dynamical system with static input u(t) = cu is called
equilibrium point if

(4)

˙x(t) = fx(xe, u(t) = cu) = 0.

if x0 ∈ Uδ(0),

We assume xe = 0 and cu = 0 without loss of generality in
the following since every system can be transformed to this
case by a simple linear transformation. An equilibrium point
xe is called (locally) attractive if all starting points within a
neighborhood around the origin x0 ∈ U (0) cause a trajectory
limt→∞ x(t) = xe = 0 for the static input u(t) = 0. If this
neighborhood is U (0) = RR, xe is called globally attractive.
Additionally, we call an equilibrium point Lyapunov stable if
for every ε-neighborhood Uε(0) = {γ ∈ RR | (cid:107)γ(cid:107)2 < ε},
there is a δ-neighborhood Uδ(0) = {γ ∈ RR | (cid:107)γ(cid:107)2 < δ}
then x(t) ∈ Uε(0) for every
such that
t > 0 and δ = δ(ε). Finally, if the equilibrium is locally
(globally) attractive and Lyapunov stable, it is called locally
(globally) asymptotically stable. In summary, if asymptotic
stability holds, the dynamical system with constant input and
a starting point x0 close enough to the equilibrium point will
generate a trajectory converging to xe while the trajectory is
guaranteed to stay within a certain range of xe in the process.
For classical dynamical systems, when analyzing stability
there is the restriction that the system function fx must be
continuously differentiable [17] or more generally Lipschitz
continuous [18], [19]. This guarantees that a solution x(t)
to the differential vector equation (5) exists and that it is
unique for a starting point x0. When studying discontinuous
dynamical systems,
this restriction is dropped so that fx
can be discontinuous. Stability analysis of such systems is
more complicated, since existence and uniqueness of solutions
is not guaranteed anymore. Also, there are different kinds
of generalized solutions which have been studied, such as
Caratheodory or Filippov solutions [20]. In this work, we
make use of a Caratheodory solution. In simple terms, such
a solution generally satisﬁes the system equations, except for
a set of time instances of measure zero (e.g., a countable,
not necessarily ﬁnite, set). Formally, a Caratheodory solution
can be deﬁned using the Lebesgue integral as an absolutely
continuous function x(t), which satisﬁes [20]

(cid:90) t

B. (Discontinuous) Dynamical Systems

x(t) = x0 +

fx(x(t(cid:48)), u(t(cid:48))) dt(cid:48).

Dynamical systems with multiple-inputs and multiple-
outputs are commonly described in state space. Deﬁning the
input vector u(t) ∈ RM , the output vector y(t) ∈ RN and the
state vector x(t) ∈ RR, the dynamical system can be described
as

˙x(t) = fx(x(t), u(t)),
y(t) = fy(x(t), u(t)).

(5)
Here, fx : RR × RM → RR is called the system function and
fy : RR × RM → RN is called the output function. The term
˙x(t) is the time derivative of the states, i.e., ˙x(t) = dx(t)/ dt.

0

III. SYSTEM MODEL

We consider a continuously valued neural network with
input vector u(t) ∈ RM and non-negative output vector
y(t) ∈ RN≥0. For the upcoming analysis, we represent the
network as a discontinuous system in state space with state
vector x(t) (see also Figure 1). From this, it follows that the

matrix A ∈ RM×N . The integrator denoted by(cid:82) + is a non-

linear integrator as deﬁned in the following. If all states are
positive, this block behaves like a standard integrator, however,

u(t)

AT

˜x(t)

x(t)

y(t)

(cid:82) +

AT A

Fig. 1.
integrator.

Block diagram of the discontinuous system with a non-linear

if a state reaches zero, integration of this state is halted. Thus,
said state remains at value zero until it is subjected to a
positive gradient again. Deﬁning the non-linear integrator as
0 ˙x(τ ) dτ, it follows that the derivative of the state

x(t) =(cid:82) t

vector ˙x(t) must be discontinuous:

˜xi(t),

[˜xi(t)]+,
c,

˙xi(t) =

if xi(t) > 0
if xi(t) = 0
if xi(t) < 0.

i = 1, . . . , N.

(6)

Here, ˜xi(t) is the input of the i-th integrator and the pos-
itive part of the i-th vector element is denoted as [˜xi]+ =
max{0, ˜xi}. The inclusion of the constant c > 0 is a techni-
cality to ensure recovery from non-negative states due to faulty
initial conditions or disturbances. See Figure 2 for an example
of the behavior of the integrator described in (6).

l
a
n
g
i
s

t
u
p
t
u
o

150
100
50
0
−50
−100
−150

1
0.5
0
−0.5
−1

l
a
n
g
i
s

t
u
p
n
i

0

200

400
time

600

800

Fig. 2. Comparison between the output signals of a standard linear integrator
(dashed blue line) and a non-linear integrator as described above (solid blue
line). The input signal (dotted green line) is plotted according to the second
ordinate on the right.

Using (6) and

˜x(t) = AT u(t) − AT Ax(t),

the system equations can be formalized for the discontinuous
system depicted in Figure 1.

˙x(t) = fx(x(t), u(t)), with ˙xi(t) = fxi(x(t), u(t)),

[˜x(t)]i ,

[˜x(t)]+
i ,

˙xi(t) =

if xi(t) > 0
if xi(t) = 0
if xi(t) < 0
y(t) = fy(x(t), u(t)) = x(t).

c,

i = 1, . . . , N

(7)

The system deﬁned by (7) has a very interesting property:
it is able to solve non-negative least squares (NNLS) opti-
mization problems. To further clarify this feature, consider
the following optimization problem with optimal point x(cid:63) and
b ∈ RM :

(cid:107)Ax − b(cid:107)2

2

1
2

minimize
subject to x ≥ 0.

x

4

(8)

This is a convex optimization problem, which does not possess
an analytical solution, in contrast to an unconstrained least-
squares problem [21].

If the discontinuous system of Figure 1 receives the constant
input u(t) = b, it will converge to its equilibrium point xe =
x(cid:63), which is also the optimal point of the NNLS optimization
problem (8). We formally prove this fact in Section IV-A.

IV. CONVERGENCE ANALYSIS

In this section, we take a closer look at the convergence
behavior of the discontinuous dynamic system shown in Fig-
ure 1. First, we show that the equilibrium points of the system
are solutions of NNLS optimization problems in Section IV-A.
Second, we characterize the Caratheodory solution of the
differential equation in Section IV-B, which is a prerequisite
for the stability analysis. Then, we turn to the actual stability
analysis in Section IV-C, which is based on Lyapunov’s
direct method. Later, we additionally prove in Section IV-D
that
the number of switches performed by the non-linear
integrator during convergence to the equilibrium point is ﬁnite
under a mild condition on the solution. Finally, we brieﬂy
discuss in Section IV-E how the convergence analysis can be
generalized to limited integrators with box constraints of the
form l ≤ x(t) ≤ h.

Before we begin the convergence analysis, we introduce
some notation which will be helpful in subsequent sections
of this text. First, we denote the set of all states’ indices as
I = {1, . . . , N}. Then, we divide this set into three time-
varying sets I +(t), I 0(t) and I−(t). These can informally be
described as the indices of the system states that are positive,
zero and negative at time t, respectively. A formal deﬁnition
of the sets can be given as follows:

(cid:111)

(cid:110)
i ∈ I(cid:12)(cid:12) xi(t) > 0
∪(cid:110)
i ∈ I(cid:12)(cid:12) xi(t) = 0, ˜xi(t) ≥ 0
(cid:111)
(cid:110)
i ∈ I(cid:12)(cid:12) xi(t) = 0, ˜xi(t) < 0
(cid:110)
(cid:111)
i ∈ I(cid:12)(cid:12) xi(t) < 0

.

,

(cid:111)

,

I +(t) =

I 0(t) =
I−(t) =

(9)

Note, that the set I +(t) also contains indices of states which
are zero at time t if its corresponding integrator input is
positive. Also, no state index can be present in two or all
of the sets I +(t), I 0(t) and I−(t) at any given time t. Keep
in mind, that states may only appear in I−(t) if faulty initial
conditions are present or disturbances cause a state to become
negative. Otherwise, it is impossible for a state to become
negative.

We use the presented notation to create vectors from a
selection of entries from another vector and to create matrices,
which are a selection of a set of rows or columns from
another matrix. For example, a vector containing all states
whose indices’ are given in the set I + is denoted as [x(t)]I+.
Similarly, a matrix containing only the columns speciﬁed by
the indices in the set I + is written as [A]:I+.

A. Equilibrium Point

B. Caratheodory Solution

First, we prove that the equilibrium point of the dynamic
system depicted in Figure 1 is a solution of the NNLS
optimization problem (8). To that end, we ﬁrst review the
KKT conditions of the NNLS problem. Let f0(x) refer to
its objective function and fi(x) = −xi ≤ 0 for i = 1, . . . , N
to its inequality constraints in standard form. Their gradients
are ∇f0(x) = AT Ax − AT b and ∇fi(x) = −ei for
i = 1, . . . , N, respectively. Here, ei
is the i-th standard
basis vector. From this, we can derive the KKT conditions
of problem (8) as:

x(cid:63) ≥ 0
λ(cid:63) ≥ 0
λ(cid:63)
i x(cid:63)
i = 0,
AT Ax(cid:63) − AT b − λ(cid:63) = 0.

i = 1, . . . , N

(10)

Obviously, there exists a strictly feasible point, i.e., x > 0.
Thus, Slater’s condition holds for problem (8). As a conse-
quence, strong duality holds and the KKT conditions presented
in (10) are necessary and sufﬁcient conditions for optimality
[21]. Therefore, the points x(cid:63) and λ(cid:63) are primal and dual
optimal. If the matrix A has full column-rank, the solution of
problem (8) is unique [22].

For an equilibrium point xe of a dynamic system with static

input u(t) = ue

˙x(t) = fx(xe, ue) = 0.

(11)

must hold [16]. In other words, the system states stay the same
for constant input ue, since the time derivatives of the system
states vanish at the equilibrium point xe.

Let the input of the system described by (7) be constant

u(t) = b and deﬁne

λe := AT Axe − AT b.

(12)

As stated by (11), for an equilibrium point xe it must hold
for i = 1, . . . , N:

[−λe]i ,

[−λe]+
i ,

c,

if [xe]i > 0
if [xe]i = 0
if [xe]i < 0

.

(13)

0 = ˙xi(t) = fxi(xe, b) =

It can be easily seen that no [xe]i can be negative since c > 0.
Thus, xe ≥ 0 must hold. For the ﬁrst case of (13), i.e., [xe]i >
0, it follows that [λe]i = 0. Similarly, for the second case of
(13), i.e., [xe]i = 0, we must have [λe]i ≥ 0. In summary, we
can write these conditions as:

xe ≥ 0
λe ≥ 0
[λe]i [xe]i = 0,
AT Axe − AT b − λe = 0.

i = 1, . . . , N

(14)

Comparing (10) and (14), we see that (xe, λe) satisfy the KKT
conditions of the NNLS optimization problem and therefore
are a primal and dual optimal solution. This means that given
the constant system input u(t) = b, the discontinuous system
depicted in Figure 1 has equilibrium points that are in fact the
solution of the NNLS problem as deﬁned in (8).

5

(15)

First, we reformulate system (7) into the following equiva-

lent formulation:

˙xi(t) =

˜xi(t),

0,
c,

(cid:26) if xi(t) > 0

if xi(t) = 0, ˜xi(t) ≥ 0
if xi(t) = 0, ˜xi(t) < 0
if xi(t) < 0.

the positive part present

Here,
in the second case in (7)
was manually split in (15). Note also that the cases of (15)
correspond to the deﬁnitions of the index sets in (9).
Let us investigate a time span, in which the index sets from
(9) are constant, i.e., time instances τ in an interval τ ∈
[tj, tj+1) with I +(τ ) = I +
j and I−(τ ) = I−
j .
For such a time span, the system (15) can be split into several
linear subsystems. States in I 0
j are not inﬂuenced by
other states, hence forming independent linear subsystems. A
new linear subsystem can be formed for the states in I +
j , which
is independent of the states in I 0
j and the inﬂuence from the
states in I−
j can be counted as additional input, since the states
in I−

j are independent from others. Deﬁning

j ,I 0(τ ) = I 0

j or I−

x+

,

j (τ ) = [x(τ )]I+
j
A+
,
j (τ ) = u(τ ) − [A]:I−
u+

j = [A]:I+

j

j

(16)

[x(τ )]I−

j

,

the system differential equation for this subsystem can be
given as

j (τ ) = A+
˙x+

j u+

j (τ ) − A+

j

TA+

j x+

j (τ ).

(17)

j

j

j

TA+
j

j

• I +

i→ I 0

j has full column-rank,

Thus, provided the matrix A+
this
subsystem is globally asymptotically stable. This follows since
the subsystem is linear and its system matrix −A+
is
negative deﬁnite, i.e., all of its eigenvalues are negative [23].
Next, we inspect the transitions of states between the index
sets (9). The index j, already used in the preceding para-
graph, identiﬁes these transitions and can be used to specify
time intervals [tj, tj+1) with constant
index sets. Assume
τ ∈ [tj, tj+1) in the following:
j+1: When a state i ∈ I +
in the subsystem (17)
reaches zero, it transits from I +
to I 0
j+1. When reaching
zero, the states’ inﬂuence vanishes. Consequently, this
transition does not cause discontinuities in any other
states’ trajectories.
j+1: If the input u(τ ) or changes in the values
of states in I +
j cause the term ˜xi(τ ) to become
greater or equal to zero, the state i transits from I 0
j to
I +
j+1. However, since its immediate inﬂuence on the other
states is zero, this transition is continuous.
• I−

j+1: At time tj, the state xi(tj)
j
must be negative, thus at constant rate of integration it
will become zero at tj+1 = tj − xi(tj )
. Depending on
˜xi(tj+1) it will transit to either I 0
j+1. Note, that
this transition is also continuous, since the immediate
inﬂuence of the state i is zero.
i→ I−

j+1: These transitions are

j+1 or I 0

j+1 or I−

j+1 or I +

j or I−

i→ I−

j
impossible with the system deﬁnition (15).

i→ I +

i→ I +

i→ I 0

• I 0

j

• I +

c

j

j

In summary, all transitions are continuous. Provided that the
system matrix A has full column-rank, the behavior of the
subsystem (17) is unique at every point in time. Also, the
behavior of the subsystems formed by I 0
is unique.
Finally, the transitions are obviously unique as well and thus
it follows the solution x(t) must be unique. Let T (t) be
the set containing all transition times up until time t, i.e.,
T (t) = {tj | tj ≤ t}. Note, that T (t) is a countable set
with measure zero. Deﬁning t0 := 0 and t|T (t)|+1 := t, the
unique Caratheodory solution of the system follows as:

j and I−

j

x(t) = x0 +

fx(x(t(cid:48)), u(t(cid:48))) dt(cid:48).

(18)

The individual integrals in (18) can be given using the notation
of the subsystems in (16) for j = 1, . . . ,|T (t)| as:

. . . dt(cid:48)

=

e−A+

j

TA+

j (tj+1−t(cid:48))A+

j

(cid:90) tj+1

|T (t)|(cid:88)

j=0

tj

(cid:90) tj+1

tj

= 0,

= c (tj+1 − tj) 1.

(cid:35)
(cid:35)
(cid:35)

I+

j

I0

j

(cid:34)(cid:90) tj+1
(cid:34)(cid:90) tj+1
(cid:34)(cid:90) tj+1

tj

tj

. . . dt(cid:48)

. . . dt(cid:48)

tj

I−

j

T u+

j (t(cid:48)) dt(cid:48),

(19)

6

with Caratheodory solutions. However, if a smooth Lyapunov
function is applicable, the theory reduces to classical stability
analysis [20]. Indeed, a smooth Lyapunov function sufﬁces for
this system and it is chosen as:

V (z) =

zT z,

1
2

N(cid:88)

with gradient ∇V (z) = z. The time derivative of V (z) yields

˙V (z) = [∇V (z)]T ˙z = zT ˙z =

zi ˙zi.

Three conditions must be fulﬁlled such that the chosen func-
tion is a Lyapunov function for the system and asymptotic
stability holds:

i=1

V (0) = 0 ,
1
2

(cid:107)z(cid:107)2

2 > 0 ∀z ∈ RN \ {0} ,

V (z) =
˙V (z) < 0 ∀z ∈ RN \ {0} .

(21)

Obviously, the ﬁrst two conditions are fulﬁlled. With (20), we
can write



zi [−AT Az]i ,
zi [−AT Az − λe]i ,
zi [−AT Az]+
i ,
zi [−AT Az − λe]+
i ,
zi c,

[xe]i > 0

(cid:26) zi + [xe]i > 0,
(cid:26) zi > 0,
(cid:26) zi + [xe]i = 0,
(cid:26) zi = 0,

[xe]i > 0

[xe]i = 0

[xe]i = 0

if

if

if

if

(22a)

(22b)

(22c)

(22d)

(22e)
Note that it was shown in Section IV-A that λe ≥ 0. For
z ∈ Uε(0), it follows for the subcases:
[λe]i

≤ −zi [AT Az]i

if zi + [xe]i < 0.

(cid:124)(cid:123)(cid:122)(cid:125)≥0

(22b) = −zi [AT Az]i − zi(cid:124)(cid:123)(cid:122)(cid:125)≥0
(cid:125)
(cid:123)(cid:122)

(22c) =

(cid:124)

[AT Az]+
i
≥[AT Az]i
[−AT Az − λe]+

zi(cid:124)(cid:123)(cid:122)(cid:125)
(22d) = zi(cid:124)(cid:123)(cid:122)(cid:125)
(22e) = zi(cid:124)(cid:123)(cid:122)(cid:125)≤0

=0

=−[xe]i≤0

≥−[AT Az]i

≤ −zi [AT Az]i
i = 0 = − zi(cid:124)(cid:123)(cid:122)(cid:125)
≤ −zi [AT Az]i .

=0

[AT Az]i

Thus, for z ∈ RN \ {0} and A having full column-rank, it
holds (almost everywhere):

˙V (z) =

−zi [AT Az]i
= −zT AT Az < 0.

i=1

(23)
In the last inequality, we used the fact that AT A is a positive
deﬁnite matrix if A has full column-rank. The function V (z)
satisﬁes the conditions (21) and hence the equilibrium point
z = 0 is asymptotically stable in Uε(0). The parameter ε
present in the neighborhood Uε(0) can be chosen arbitrarily,
therefore semi-global stability holds for the discontinuous
system (7).

c(cid:124)(cid:123)(cid:122)(cid:125)
zi ˙zi ≤ N(cid:88)

N(cid:88)

i=1

Thus, the Caratheodory solution is sufﬁciently deﬁned. Finding
a closed form of (18), however, requires expressions for the
transition times and a closed form of the ﬁrst equation in (19),
which can be very challenging.

zi ˙zi =

C. Stability

In order to show asymptotic stability of the equilibrium
point xe for the system in (7), we apply a linear transformation
as discussed in Section II-B. We substitute z(t) = x(t) − xe
and ˆu(t) = u(t) − b, so that the equilibrium point of the
transformed system is ze = 0 for the constant input ˆu(t) = 0.
The transformed system function can be found using:

˙zi(t) = fz i(z(t), ˆu(t)) = fxi(z(t) + xe, ˆu(t) + b).

Note that we omit the time index t in the following derivations
to increase readability. Using (12), splitting the cases and
simplifying, we arrive at:

[−AT Az − AT Axe + AT b]+
i ,

[−AT Az − AT Axe + AT b]i ,


[−AT Az]i ,
[−AT Az − λe]i ,
[−AT Az]+
i ,
[−AT Az − λe]+
i ,

c,

c,

˙zi =

=

if zi + [xe]i > 0, [xe]i > 0
if zi > 0, [xe]i = 0
if zi + [xe]i = 0, [xe]i > 0
if zi = 0, [xe]i = 0
if zi + [xe]i < 0

.

if zi + [xe]i > 0
if zi + [xe]i = 0
if zi + [xe]i < 0

(20)
In the following, we study stability using Lyapunov’s direct
method. For discontinuous dynamical systems, there exists a
lot of dedicated work on stability theory, e.g., for systems

D. Finite Number of Switches

In Section IV-B, we already established that the set of
transition times T (t) is countable. Hence, if there is a time
after which no more switches (transitions) occur, then the
number of switches is ﬁnite. We show that under a mild
condition on the solution, this is indeed the case. For the
following analysis, we deﬁne the time-dependent vector

λ(t) = −˜x(t) = AT Ax(t) − AT b,

which is the negative integrator input. Above, it was already
shown that the equilibrium point xe is unique if A has full
column-rank. Similar to (9), we can deﬁne unique index sets
for the equilibrium point as:

(cid:111)

I +
e =

(cid:110)
i ∈ I(cid:12)(cid:12) [xe]i > 0
∪(cid:110)
(cid:111)
i ∈ I(cid:12)(cid:12) [xe]i = 0, − [λe]i ≥ 0
(cid:110)
(cid:111)
i ∈ I(cid:12)(cid:12) [xe]i = 0, − [λe]i < 0
I 0
e =
I−
e = ∅.

,

,

To prevent inﬁnite oscillations at the transition boundaries, we
assume there exists εx, ελ > 0, such that
[xe]i > εx
[λe]i > 2ελ.

∀i ∈ I +
e :
∀i ∈ I 0
e :

Essentially,
this means the equilibrium point xe is good
mannered in the sense that it is not directly on a transition
boundary (see also [9]). Note that under this assumption also

∀i ∈ I +
e :
∀i ∈ I 0
e :

[λe]i = 0
[xe]i = 0

holds.

The system states x(t) converge to the equilibrium point xe
and likewise λ(t) converges to λe. Additionally, due to the
behavior of the bounded integrator (6), after a limited time all
states must be non-negative. Thus, there exists a time T > 0
so that ∀t ≥ T :

(cid:107)x(t) − xe(cid:107)2 < εx
(cid:107)λ(t) − λe(cid:107)2 < ελ
x(t) ≥ 0.

Similar to the approach in [9], we can say that for ∀t ≥ T
and ∀i ∈ I +
e :

εx > |xi − [xe]i| ≥ |[xe]i| − |xi| > εx − |xi(t)|

⇒ |xi(t)| > 0
⇒ xi(t) > 0.

As can be seen, for all times t ≥ T the states in I +
stay
strictly positive and hence do not switch anymore. In other
words, all states in I +
e have a certain gap to zero, that is
e stay closer
[xe]I+
to the equilibrium value than this gap.

> εx and after the time T all states in I +

e

e

Likewise, for t ≥ T and ∀i ∈ I 0

e it follows:

ελ > |λi − [λe]i| ≥ |[λe]i| − |λi| > 2ελ − |λi(t)|

⇒ |λi(t)| > ελ > 0
⇒ λi(t) > ελ > 0.

7

So, for all times t ≥ T , the integrator inputs ˜xi(t) = −λi(t)
of the states i ∈ I 0
e stay strictly negative. At time T , however,
some states in I 0 may still be positive and some may be zero
already. We investigate both cases separately:
e ∩ I 0(T ): As explained above,
the integrator input stays negative. Thus, no transitions
occur after time T .
e ∩I +(T ): For t ≥ T , it holds

• non-zero states, i.e., ∀i ∈ I 0

• zero states, i.e., ∀i ∈ I 0

xi(t) = xi(T ) +

< xi(T ) +

−λi(t(cid:48)) dt(cid:48)

(24)

−ελ dt(cid:48) = xi(T ) − ελ(t − T ).

(cid:90) t
(cid:90) t

T

T

Thus, a state i reaches zero at time xi(T )
+ T at the latest
ελ
and stays zero since the integrator input remains negative.

Hence, after time

i∈I0

max
e∩I+(T )

xi(T )

ελ

+ T ,

no more switches occur. Since this point in time is ﬁnite, the
number of switches is ﬁnite as well.

E. Generalization to Box Constraints

In this section, we brieﬂy discuss an extension to system
(7), so that it is able to solve optimization problems of the
form

xT P x − qT x

minimize
subject to l ≤ x ≤ h.

x

1
2

(25)

If the matrix P is positive deﬁnite, which we assume in the
following, this is a convex quadratic program (QP) with lower
and upper bounds, see e.g. [21]. By deﬁning P = AT A and
q = AT b this problem is identical to a bounded least squares
problem.

The integrator used in system (7) is replaced by an integrator
with general lower and upper bounds deﬁned by l and h,
respectively. Also, the system matrix is replaced by the matrix
P . Deﬁning the new integrator input as ˆx(t) = u(t)− P x(t),
we can give the system equations of the modiﬁed system as:



−c,
−
[ˆx(t)]
i ,
[ˆx(t)]i ,
[ˆx(t)]+
i ,

c,

˙xi(t)=

if xi(t) > hi
if xi(t) = hi
if li < xi(t) < hi
if xi(t) = li
if xi(t) < li

i = 1, . . . , N

(26)

y(t) = x(t).

If system (26) receives the constant input u(t) = q, it will
converge to its equilibrium point, which is the solution of the
QP (25). This can be shown by using the deﬁnition of an
equilibrium point and comparing it to the KKT conditions of
problem (25). To show that a Caratheodory solution exists,
one can reformulate the system equations similar to (15).
Corresponding to the cases present in the system function,
ﬁve index sets (compare (9)) can be deﬁned: one for states

which are strictly inside the bounds forming an independent
linear system, two for states at the bounds and ﬁnally two
for states which exceed the bounds due to initial conditions
or disturbances. Transitions can be shown to be smooth and
the Caratheodory solution can be characterized. Asymptotic
stability holds as well. After linearly transforming the system,
the same Lyapunov function as above may be used. Similarly
to (22), 11 cases may be worked out and the Lyapunov
function can be shown to be upper bounded by −zT P z < 0,
as we assumed P to be positive deﬁnite.

V. APPLICATION: OLFACTORY MIXTURE DECOMPOSITION
The ﬁrst application of the dynamic system (7) we present is
the decomposition of a mixture of sensory stimuli as a model
for a biological neural network. It is based on the structure of
the olfactory system in mammals or more precisely the main
olfactory bulb (MOB). We ﬁrst present the model and discuss
numerical results thereafter.

A. Model Description

The olfactory system is considered a ﬂat system since it
consists of few processing stages before olfactory information
is transmitted to higher emotional or cognitive brain regions.
Olfactory sensory neurons (OSNs) are the chemical sensors of
the system. By expressing different olfactory receptor genes,
there are approximately 1000 “types” of OSNs. There are
around 1000 to 10000 OSNs of each type in mice / rodents,
resulting in a total number of OSNs between 106 and 107,
see [24]–[27]. In the main olfactory bulb, all OSNs of the
same type converge into two spherical neuropil structures of
high connectivity, called glomeruli. In turn, connected to one
glomerulus are approximately 25 mitral cells (MCs) [24],
which ﬁnally project to higher brain areas. These parallel
processing paths are inﬂuenced by the periglomerular cells
(PGCs) and interact with each other via short axon cells
(SACs) on the glomeruli level and through granule cells (GCs)
on the mitral cell level, compare also Figure 3. It is believed,
these so called lateral interactions play a key role in the signal
processing within the MOB, see e.g. [25], [27], [28]. Despite
the MOB’s biological signiﬁcance, however, the principles
behind its information processing still remain unclear.
The MC → GC → MC interconnection circuit seems to be
unique in the body and has a peculiar behavior. An excited MC
that is connected to a GC will be inhibited via the connection
(self-inhibition). Another MC, which is also connected to
the same GC, will be inhibited as well. So in total
the
GC processing is inhibitory and thus lowers the activity of
connected MCs.

Nerve cells are modeled by a dimensionless positive activity
level which expresses their level of excitation. This way,
we can abstract from the electro-chemical
level of action
potentials and do not need to make additional assumptions
about
the existence of rate- or timing codes, while still
retaining a proportional relationship. Furthermore, the GCs
are modeled as passive components and the MCs are the only
active components in the system. While the GCs are able to

PGC

SAC

“Type A”

“Type B”

8

higher
brain

GC

higher
brain

OSN

GL

MC

Fig. 3. Highly abstracted block diagram of the main olfactory bulb. Two
parallel pathways, associated with two types (olfactory receptor genes) of
OSNs, are shown. The sources of potential lateral interactions (SACs and
GCs) are depicted within the gray shaded area. Also, the number of cells
involved for each cell type is qualitatively reﬂected.

generate action potentials, they seem to do so only rarely and
may thus be covered in a passive manner by the model.

Let all the GCs and MCs in the system be identiﬁable by
indices. Then we can model the activity level of a GC with
index j at a given time t as follows:

lj(t) =

wjixi(t) ,

(27)

(cid:88)

i

where xi is the output of the MC with index i and wji ≥ 0
is a weight describing the synaptic connection. We assume
for this model that multiple GCs connected to one MC are
only present to ease synaptic routing in the body. Thus in our
model, through one GC a MC can be connected to all other
MCs with the corresponding weights. Likewise, the i-th MC
can be modeled as:

(cid:88)
(cid:88)

j

˜xi(t) = gi(t) +

= gi(t) +

(−wji)lj(t)

(cid:88)

(−wji)

wjkxk(t) ,

(28)

j

k

where ˜xi is the input of the i-th MC and gi is the output of
the corresponding glomerulus. Here, we assume the excitatory
connections and the self-inhibitory feedback connections to
have the same weights. The active MC is assumed to behave
like the integrator described in (6). Both (27) and (28) can be
written in matrix form, using the vectors g, l, ˜x and x which
contain the output of the glomeruli, the activity levels of the
GCs and the inputs and outputs of the MCs, respectively. Thus,
the model of the GCs becomes l(t) = W x(t) and the model
for the MCs with interconnected GCs follows as:

˜x(t) = g(t) − W T W x(t) .

One potential task of the MOB is to decompose a mixture
of chemical stimuli
into its individual parts. Let o be a
vector of chemical stimuli of arbitrary dimension. The sensors
of the olfactory system,
to the chemical
components of this mixture and produce an output vector u
of dimension N. Here, we do not consider individual OSNs.
Each entry of the vector can be thought of as the average
output of the OSNs of one “type” since the purpose of the high

the OSNs, react

number of individual sensors is most likely noise reduction
and redundancy. Thus, the dimension N corresponds directly
to the number of glomeruli in the system. Say the olfactory
system has an internal model of odor patterns and the problem
is to ﬁnd the most plausible mixture that causes the current
(noisy) OSN output vector u(t). A smart way of solving this
task would be to minimize the quadratic distance between
u(t) and a vector created according to the internal model.
Assuming linear mixing of the chemical stimuli, the “known”
stimuli patterns can be collected as the columns of a matrix
and we assume them to be reﬂected in the weight matrix W .
If the processing of the SACs (and PGCs) on the glomerulus
level can be described by g(t) = W T u(t), then we can
see that the total system behaves just like the system deﬁned
by (7), explained above. Hence, under these assumptions if
u(t) is almost constant for a long enough time to let the
dynamic system converge and the synaptic weights contain
the “known” or “learned” stimuli patterns, the system solves
a NNLS optimization problem to ﬁnd the most likely mixture
of chemical stimuli encountered. More precisely, for constant
u(t) = u as t → ∞, x(t) converges to the minimizer of the
NNLS optimization problem (8), with the change of variable
A = W and b = u. Likewise, the system equations follow
by a change of variable A = W as (7).

We can make some observations stemming from the mod-
eling. The weights in the model are non-negative, so that
the weight matrix W ≥ 0, or equivalently W ∈ RN×N≥0
.
Assuming W is full rank, it is a basis for RN . It cannot,
however, generate all points in RN≥0 using only positive linear
combinations, i.e., θ = W γ with γ ∈ RN≥0. As said above,
the vector u is the noisy output of the OSNs: a noisy mea-
surement. Say u = W x0 + η, where x0 is the actual mixture
and η is an additive noise vector. Then, although the solution
x(cid:63) of the NNLS problem is unique and η must be linearly
2 ≥ 0. In
dependent on W , the optimal value 1
contrast, the solution of a regular least-squares optimization
problem without the non-negativity constraint would always
yield an optimal value of zero in this case.

2 (cid:107)W x(cid:63) − u(cid:107)2

In [29], a similar modeling of the olfactory system is
presented. However, it differs fundamentally from ours since
there the GCs are modeled as active components whereas they
are modeled statically here. Furthermore, a sparsity inducing
cost function is assumed in [29], which is obsolete in our
model due to the self-regularizing behavior induced by the
non-negativity constraint.

B. Numerical Evaluation

To evaluate the capabilities of the dynamic olfactory system
model we performed Monte Carlo simulations. Two non-
negative data models were used in these simulations, one
based on Gaussian random variables and the other based on
rectangular random variables. Evidently, the performance of
the dynamic olfactory system model must be evaluated for
noisy data. In the following, the data models used in the Monte
Carlo simulations are explained in detail. Note that we use the
same data model in Section VI and so the data models are
introduced for non-square matrices W ∈ RM×N , although
M = N in the olfactory system model.

9

Let x0 be the actual mixture, having exactly s nonzero
entries at the indices contained in the sparse support set S.
Conversely, the indices where x0 is zero are in the comple-
mentary sparse support set, denoted as Sc. The actual (noise
free) input is u0 = W x0. The dynamic system, however, is
presented with a noisy input u = W x0 + η, where η is an
additive noise vector.

For the numerical evaluation, x0, W and η are drawn ran-
domly. Here, the random matrix W is entry-wise i.i.d. with ex-
pectation µw and variance σ2
w. The matrix W is subsequently
normalized, so that the euclidean column-norm is equal to
one. Thus, each entry has expectation E [wij] =

w+σ2
w)
and variance Var [wij] =
w) after normalization. Each
nonzero entry of the i.i.d. random vector x0 has expectation
x. Similarly, for the i.i.d. noise vector we
µx and variance σ2
have E [ηi] = 0 and Var [ηi] = σ2
η. Additionally, W , x0 and
η are entry-wise mutually independent.

σ2
w
M (µ2
w+σ2

µw√

M (µ2

We deﬁne the input signal-to-noise ratio (SNR) as E[uT

0 u0]
E[ηT η] .
Using the deﬁnitions from above, the input SNR can be given
as (a detailed derivation can be found in the Appendix):

(cid:18)

(cid:19)

E [uT
0 u0]
E [ηT η]

=

s

M σ2
η

σ2
x +

µ2
x (sµ2

w + σ2
w)

µ2
w + σ2
w

.

(29)

√

√

3.

√

√

3 ση.

12], so that σ2

x = 1 and µx =

Irrespective of the data model, the nonzero entries in x0 are
always drawn from a rectangular distribution with support
[0,

√
For the rectangular (rect) data model, the entries in W and
the entries in η are drawn from rectangular distributions. This
corresponds to a situation where the support of input and noise
values is naturally limited by the application, for example
through saturation effects. The matrix entries are drawn with
support [0,
3. Since the
noise entries are zero mean, the support interval [−γ, γ] can
directly be determined for the chosen input SNR by solving
(29) for σ2

w = 1 and µw =

η and observing that γ =

12], so that σ2

In the Gaussian model the entries are drawn from Gaussian
distributions for W and η. Since the matrix W is assumed
to be entry-wise non-negative, mean and variance are set to
µw = 5 and σ2
w = 1, respectively. Thus, the entries of W are
non-negative with very high probability. For η, the parameter
η is found analogously to the rectangular case.
σ2
For the numerical evaluation, 5000 Monte Carlo instances
are generated. Note that in each instance the sparse support
set S, x0, W and η are drawn randomly as described above.
In Figure 4, on the left the mean relative error on the sparse
support set S is plotted for different input SNRs as well as
different sparsities s for both non-negative data models. As can
be seen, higher sparsities correspond to higher errors, which
is to be expected. Also, the Gaussian data model generally
performs worse than the rectangular one. Though relative
errors of less than 10% can be reached, when the input SNR
is high enough.

To show the denoising capabilities of this system, we

evaluate the output SNR, which is deﬁned as:

(cid:20) [x]TS [x]S

(cid:21)

[x]TSc [x]Sc

E

.

(30)

rect
gaussian

s = 1
s = 5

rect
gaussian

SNR: 6 dB
SNR: 18 dB
SNR: 12 dB SNR: 24 dB

10

r
o
r
r
e

e
v
i
t
a
l
e
r

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

6

48

42

36

30

24

18

12

6

]

B
d
[

R
N
S

t
u
p
t
u
o

n
a
e
m

12

18

24

6

12

18

0
24

input SNR [dB]

]

B
d
[

R
N
S

t
u
p
t
u
o

n
a
e
m

48
42
36
30
24
18
12
6
0
−6
−12

0

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9

pruning ratio

left: Mean relative error on the sparse support set S for different

Fig. 4.
input SNRs. right: Mean output SNR (see (30)) for different input SNRs.

There, [x]S is a vector comprised only of the entries of x
whose indices are contained in the set S. Hence, the output
SNR is the ratio between the power in the sparse support S
(desired signal) and the power in the complementary sparse
support Sc (noise) after recovery. The mean output SNR versus
the input SNR is shown in Figure 4 on the right side. For
both data models under low sparsities s, the output SNR is
signiﬁcantly higher than the input SNR. This suggests that the
system possesses denoising capabilities.

Biological systems exhibit high fault tolerance and tend
to be power efﬁcient. Since the matrix W models synaptic
connections, we can test two effects by setting the smallest
entries in the matrix to zero. On the one hand, we may simulate
synaptic failures this way and thereby test the fault tolerance of
the system. On the other hand, we may also test robustness and
power efﬁcient operation at the same time since maintaining a
multitude of synaptic connections comes at a high cost for the
body. In Figure 5, we show the mean output SNR evaluated
for pruning ratios between 0 and 0.9. Here, a pruning ratio of
0.1 means that the smallest 10% of the matrix entries of W
are set to zero. Especially for the rectangular data model, we
can see that even moderate pruning ratios (≈ 0.5 − 0.7) are
acceptable since the output SNR stays higher than the input
SNR.

In summary, these results indicate that a topologically sim-
ple neural network like the olfactory system could in principle
tackle complicated signal recovery tasks, e.g., non-negative
sparse approximation. It could greatly beneﬁt from the inherent
denoising capability and the fault tolerance operation of the
presented network.

VI. APPLICATION: NON-NEGATIVE SPARSE

APPROXIMATION

The second application we present is solving non-negative
sparse approximation problems. Here, we discuss a more
general model compared to the previous section including
the very important class of underdetermined problems. We
ﬁrst introduce the model and present numerical evaluations
afterwards.

Fig. 5. Mean output SNR for different pruning ratios of the matrix W .

A. Model description

We consider non-negative sparse approximation problems
as an application of the network presented in Section III. As
introduced similarly in Section II-A, the task is to recover an
s-sparse signal x0 from linear measurements. The non-zero
entries of x0 are contained in the sparse support set S and it is
assumed that these entries are non-negative, i.e., [x0]S ∈ R|S|
>0,
where |S| is the cardinality of the set S. The measured signal
b is obtained from a noisy linear measurement process, which
is described by b = Ax0 + ε with A ∈ RM×N
and ε ∈ RM .
Typically, the number of measurements is assumed to be small
so that M (cid:28) N. Note that we assume for simplicity that all
entries of the matrix A are strictly greater than zero. This
implies that the matrix fulﬁlls the self-regularizing property
[15] (see also Section II-A). Furthermore, we assume that the
measured signal is element-wise non-negative, i.e., b ≥ 0, with
very high probability. This holds if the noise ε is zero mean,
the distribution of ε has ﬁnite support or tails with negligible
contributions and the SNR is high enough.

>0

To solve non-negative sparse approximation problems with
the discontinuous dynamic system from Section III, the system
is presented with the measurement at its input u(t) = b and
the system matrix equals the linear system model. The systems
output y(t) will hold the recovered signal vector x, once it
has converged.

In the convergence analysis in Section IV, in some places
it was required that the system matrix A has full column-
rank. Obviously, if M < N, as is typical in non-negative
sparse approximation problems, the matrix cannot have full
column-rank by deﬁnition. We discuss the consequences in
the following. We see, that it does not affect the practical
applicability of the network for the relevant problem class.

The solution of the NNLS problem is not unique if the ma-
trix A does not have full column-rank (compare Section IV-A).
However, the KKT conditions of the discontinuous dynamic
system and the NNLS problem are still equal. Moreover, since
we assume that the matrix A is entry-wise non-negative, the
row-span of the matrix intersects the positive orthant and the
self-regularizing property is fulﬁlled [15]. Thus, if no noise
is present the solution is unique nevertheless and if noise is

T A+

present, the recovery error is bounded. So even if the solution
of the NNLS problem would differ from the equilibrium point
of the dynamic system in the noisy case, because the KKT
conditions are equal, the error is bounded for both solutions.
For a given starting point x(0) and a given constant input
u(t) = b, we can still split the system into linear subsystems
(compare Section IV-B). The “active” subsystems correspond-
ing to the index sets I +
j are (locally) asymptotically stable
since −zT A+
j z < 0 for all z (cid:54)= 0, due to the fact
that A+
j > 0. Hence, all state transitions remain unique and
continuous. Also, the Caratheodory solution can be deﬁned
in the same way as before. The stability analysis can be
performed using a given starting point x(0), a given constant
input u(t) = b and a corresponding equilibrium point xe
(which depends on x(0) and b). Since A > 0, (23) still holds
although AT A is only positive semi-deﬁnite. For a given tuple
(x(0), u(t) = b, xe) and the condition on the solution the
proof of the ﬁnite number of switches during convergence
remains the same as well (c.f. Section IV-D).

j

Summarizing, even for the case that M < N which prevents
the matrix A from having full column-rank, the system is
(locally) asymptotically stable if A > 0 and can thus be used
to solve non-negative sparse approximation problems.

B. Numerical Evaluation

For the evaluation, we use the same non-negative data
models introduced in Section V-B. Here, our main focus of
the evaluation is to compare the recovery performance of
the dynamic system (NNLS) with non-negative basis pursuit
denoising (NNBPDN):

minimize
subject to x ≥ 0 .

x

1
2

(cid:107)Ax − b(cid:107)2

2 + α(cid:107)x(cid:107)1

(31)

Note that in contrast to the NNLS problem, the NNBPDN
problem has a regularization parameter α which must be deter-
mined in advance. For the evaluation, for each Monte Carlo in-
stance, we solve the NNBPDN problem for 50 logarithmically
spaced regularization parameters and pick the solution which
has the smallest mean squared error (MSE) with respect to
the actual signal vector x0. This gives the NNBPDN problem
an advantage since in practice one obviously cannot use this
procedure and α must be ﬁxed to some empirically determined
value beforehand.
In Figure 6, the MSE of the recovered signal x with respect
to the actual signal vector x0 on the indices contained in S,
i.e., the MSE on the sparse support, is shown for different input
SNRs. As can be seen, both methods differ only marginally
with NNLS featuring a slightly lower error for a medium to
high number of measurements M. Only for a low number of
measurements and higher SNRs, NNBPDN has a smaller error
on average.

Next, we evaluate the sparse support set recovery perfor-
mance in Figure 7. Here, we check whether there exists a
threshold with which the sparse support S can be separated
from the complementary sparse support Sc,
i.e., whether
max([x]Sc ) < min([x]S ). We can see, that NNBPDN seems

11

NNBPDN SNR: 10 dB SNR: 30 dB
NNLS
SNR: 20 dB SNR: 40 dB

100

75
150
# measurements (M)

125

175

200

t
r
o
p
p
u
s

n
o

E
S
M

101

100
10−1
10−2
10−3
10−4

25

50

Fig. 6.
Comparison of the MSE with respect to the actual signal vector
x0 on the sparse support set S for different input SNRs, N = 200, s = 5
and using the uniform (rect) data model. Note the logarithmic scaling of the
ordinate.

to be slightly superior as far as recovery of the support is
concerned.

NNBPDN SNR: 10 dB SNR: 30 dB
SNR: 20 dB SNR: 40 dB
NNLS

1

0.8

0.6

0.4

0.2

y
r
e
v
o
c
e
r

t
r
o
p
p
u
s

0
25

50

100

75
150
# measurements (M)

125

175

200

Fig. 7.
Fraction of monte carlo instances in which clear recovery of the
sparse support set S is possible through thresholding for different input SNRs,
N = 200, s = 5 and using the uniform (rect) data model.

To analyze the inﬂuence of the sparsity on the recovery per-
formance, Figure 8 depicts the MSE on the support on the left
and the support recovery fraction on the right both for different
values of the sparsity s. As expected, the higher the sparsity
the higher the error and hence also the lower the fraction of
recoverable supports. Although NNBPDN performs slightly
superior for most choices of the parameters, it is interesting
to note that for low SNR situations the discontinuous network
yields solutions that have a lower MSE on the support on
average.

So far, all results were presented for the uniform (rect) data
model. A comparison between the recovery performance of the
uniform and the Gaussian data model is shown in Figure 9.
Similar to the results in Section V-B, the Gaussian data model
generally performs worse. The general effects discussed above
are similarly present with the Gaussian data model, however.
the recovery performance of

Summarizing, we see that

t
r
o
p
p
u
s

n
o

E
S
M

4

3

2

1

0

5

NNBPDN SNR: 10 dB SNR: 30 dB
NNLS
SNR: 20 dB SNR: 40 dB

1

0.8

0.6

0.4

0.2

y
r
e
v
o
c
e
r

t
r
o
p
p
u
s

10

15

20

5

10

15

0
20

sparsity (s)

Fig. 8.
Inﬂuence of the sparsity s for different input SNRs, M = 50,
N = 200 for the uniform (rect) data model. left: MSE on sparse support S.
right: sparse support recovery by thresholding.

NNBPDN rect
NNLS rect

NNBPDN gaussian
NNLS gaussian

100

10−1

10−2

10−3

t
r
o
p
p
u
s

n
o

E
S
M

10−4

25 50

1

0.8

0.6

0.4

y
r
e
v
o
c
e
r

t
r
o
p
p
u
s

150

0.2

200

100

150

200

100
# measurements (M)

25 50

Fig. 9. Comparison of the two non-negative data models for an input SNR
of 40 dB, s = 5, M = 50 and N = 200. left: MSE on sparse support S.
right: sparse support recovery by thresholding. Note the logarithmic scaling
of the left ordinate.

the discontinuous network is very similar to the performance
obtained by solving a NNBPDN optimization problem. The
NNBPDN approach tends to recover the sparse support slightly
better. For most choices of the parameters,
the dynamic
system features a marginally lower MSE on the support, this
difference becomes more profound in low SNR situations.
Nevertheless, we expect these differences to be smaller in
practice since the regularization parameter α of the NNBPDN
was chosen as the best from a set of 50 values for each
Monte Carlo instance, which cannot be done in practice.
Thus,
the discontinuous network shows a very promising
recovery performance for non-negative sparse approximation
applications.

C. Relation to existing work

As brieﬂy summarized in Section I, several networks exist
which are used in similar applications. Firstly,
the most
profound novelty of our presented system (7) is that
the
integrators are discontinuous and have a lower limit equal to

12

zero (or are individually upper- and lower-bounded, compare
Section IV-E). In contrast to this, in related systems (cf. [6]–
[8]), the output of the integrators is not constrained. Although
for some of these systems, the output is directly processed by
a nonlinear thresholding function before being connected to
neighboring nodes, all of these systems still feature unbounded
internal system states. This also inﬂuences the convergence
behavior of said systems. The limited integrator in our system
captures inherent limits, e.g., on neuronal ﬁring rates, directly.
However, careful stability analysis is required due to the
discontinuous nature of the system. Secondly, our original
application for which our neural network was investigated (see
Section V-A) features self-inhibition that violates the necessary
conditions on stability in classical continuous HNNs.

VII. CONCLUSION

In this work, we investigated a discontinuous neural network
that is applied as a model of the mammalian olfactory system
and more generally as a method for solving non-negative
sparse approximation problems. The discontinuities in the
system function of the neural network arise from the fact
that
the integrators of the system are bounded to having
non-negative outputs. Hence, the integrators “switch” between
being active if the output
is greater than zero and being
inactive if the output reaches zero. First of all, we showed
that the presented neural networks features equilibrium points
that correspond to the solutions of a general non-negative
least squares problem. We then speciﬁed a Caratheodory
solution to the system’s discontinuous differential equations
and proceeded to prove that the network is stable, provided
that the system matrix has full column-rank, using Lyapunov’s
direct method. Moreover, we were able to show, under a mild
condition on the equilibrium point, that the system performs
only a ﬁnite amount of switches while converging. We then
generalized the obtained results to the case where the inte-
grators of the system have arbitrary lower- and upper limits.
In this case, the neural network converges to the solution of a
bounded least squares optimization problem. Two applications
for the presented neural network were shown. Firstly, our
original application was shown, where the neural network was
derived from modeling of the mammalian olfactory system.
We used this model to investigate a potential application of
the olfactory system, which is the decomposition of a mixture
of chemical stimuli from noisy neural responses. Based on
our ﬁndings, we argued that the olfactory system may be
capable of performing such a complex sparse recovery task.
Secondly, we generalized the application to embrace non-
negative sparse approximation problems. We showed that the
stability of the neural networks is unaffected under this model,
even if the system matrix does not possess full column-rank.
Then, we compared the recovery performance of the network
with a classical non-negative basis pursuit denoising algorithm.
Since the recovery performance differs only marginally, the
presented network can be successfully applied. Compared to
the classical algorithm, the network is independent from the
choice of a regularization parameter, which strongly affects
the recovery performance of the classical non-negative basis
pursuit denoising algorithm.

13

[9] A. Balavoine, J. Romberg, and C. Rozell, “Convergence and Rate Anal-
ysis of Neural Networks for Sparse Approximation,” IEEE Transactions
on Neural Networks and Learning Systems, vol. 23, no. 9, pp. 1377–
1389, 2012.

[10] E. J. Candes, J. K. Romberg, and T. Tao, “Stable signal recovery from
incomplete and inaccurate measurements,” Communications on Pure and
Applied Mathematics, vol. 59, no. 8, pp. 1207–1223, 2006.

[11] E. Candes and M. Wakin, “An Introduction To Compressive Sampling,”

IEEE Signal Processing Magazine, vol. 25, no. 2, pp. 21–30, 2008.

[12] D. Chen and R. J. Plemmons, “Nonnegativity constraints in numerical
analysis,” in Symposium on the Birth of Numerical Analysis, 2009, pp.
109–140.

[13] A. Bruckstein, M. Elad, and M. Zibulevsky, “On the Uniqueness of Non-
negative Sparse Solutions to Underdetermined Systems of Equations,”
IEEE Transactions on Information Theory, vol. 54, no. 11, pp. 4813–
4820, 2008.

[14] M. Wang and A. Tang, “Conditions for a unique non-negative solution
to an underdetermined system,” in 47th Annual Allerton Conference on
Communication, Control, and Computing, 2009., 2009, pp. 301–307.

[15] M. Slawski and M. Hein, “Non-negative least squares for high-
dimensional linear models: Consistency and sparse recovery without
regularization,” Electronic Journal of Statistics, vol. 7, pp. 3004–3056,
2013.

[16] J. Adamy, “Grundlagen nichtlinearer Systeme,” in Nichtlineare Regelun-
gen, ser. Springer-Lehrbuch. Berlin Heidelberg, DE: Springer, 2009,
pp. 1–43.

[17] J. J. E. Slotine and W. Li, Applied nonlinear control. Englewood Cliffs,

[18] H. Khalil, Nonlinear Systems, 3rd ed. Upper Saddle River, NJ, US:

NJ, US: Prentice Hall, 1991.

Prentice Hall, 2002.

[19] S. Sastry, Nonlinear Systems: Analysis, Stability and Control, ser.
Interdisciplinary applied mathematics. New York, NY, US: Springer,
1999, vol. 10.

[20] J. Cortes, “Discontinuous dynamical systems,” Control Systems, IEEE,

vol. 28, no. 3, pp. 36–73, 2008.

[21] S. Boyd and L. Vandenberghe, Convex Optimization, 7th ed. New York,

NY, US: Cambridge University Press, 2004.

[22] A. Bj¨orck, “Constrained Least Squares Problems,” in Numerical Meth-
ods for Least Squares Problems, ser. Other Titles in Applied Math-
ematics.
Philadelphia, PA, US: Society for Industrial and Applied
Mathematics, 1996, vol. 51, pp. 187–213.

[23] J. Lunze, “Beschreibung und Verhalten von Mehrgr¨oßensystemen,” in
Regelungstechnik 2, 7th ed., ser. Springer-Lehrbuch. Berlin Heidelberg,
DE: Springer, 2013, vol. 2, pp. 15–62.

[24] L. B. Buck, “Information Coding in the Vertebrate Olfactory System,”

Annual Review of Neuroscience, vol. 19, no. 1, pp. 517–544, 1996.

[25] G. Laurent, “A Systems Perspective on Early Olfactory Coding,” Sci-

ence, vol. 286, no. 5440, pp. 723–728, 1999.

[26] P. Mombaerts, “Axonal Wiring in the Mouse Olfactory System,” Annual
Review of Cell and Developmental Biology, vol. 22, no. 1, pp. 713–737,
2006.

[27] R. I. Wilson and Z. F. Mainen, “Early events in olfactory processing,”

Annual Review of Neuroscience, vol. 29, no. 1, pp. 163–201, 2006.

[28] V. N. Murthy, “Olfactory Maps in the Brain,” Annual Review of

Neuroscience, vol. 34, no. 1, pp. 233–258, 2011.

[29] A. Koulakov and D. Rinberg, “Sparse Incomplete Representations: A
Potential Role of Olfactory Granule Cells,” Neuron, vol. 72, no. 1, pp.
124–136, 2011.

APPENDIX

INPUT SNR DERIVATION

Let IS denote a sparse identity matrix, i.e., a diagonal
matrix with ones only at positions {(i, i) | i ∈ S} and zeros
otherwise. Likewise, let 1S denote a sparse one-vector con-
taining ones solely at indices {i | i ∈ S} and zeros otherwise.
For the derivation of the input signal power E [uT
0 u0], we
need E [W T W ], Tr [E [W T W ] IS ] and 1TS E [W T W ] 1S.
Considering the independencies between the entries we get:

(cid:34) M(cid:88)

(cid:35)

E [W T W ]ij = E

wki wkj

= M E [w1i w1j]

k=1

⇒ E [W T W ] =

µ2
w
µ2
w + σ2
w
Based on this result it follows that

11T +

σ2
w

µ2
w + σ2
w

I .

Tr [E [W T W ] IS ] = s ,

and

1TS E [W T W ] 1S =

=

1TS 11T 1S +

µ2
w
µ2
w + σ2
w
w + s σ2
s2µ2
w
w + σ2
µ2
w

.

σ2
w

µ2
w + σ2
w

1TS 1S

Finally, we can calculate the input signal power E [uT
using the results obtained above:

0 u0]

E [uT

0 u0] = E [xT

0 ]]

= Tr [E [W T W ] E [x0xT

0 W T W x0] = E [Tr [W T W x0xT

= Tr(cid:2)E [W T W ] (Cov [x0] + E [x0] E [x0]T )(cid:3)
= Tr(cid:2)E [W T W ] (σ2

x 1S 1TS )(cid:3)

x IS + µ2
x Tr [E [W T W ] IS ] + µ2

x 1TS E [W T W ] 1S

= σ2

0 ]]

= σ2

xs +

x(s2µ2
µ2

w + s σ2
w)

w + σ2
µ2
w

.

The noise power is simply E [ηT η] = M σ2
SNR follows as (29).

η. Thus, the input

REFERENCES

[1] D. L. Donoho, “Compressed sensing,” Information Theory, IEEE Trans-

actions on, vol. 52, no. 4, pp. 1289–1306, 2006.

[2] E. Candes, J. Romberg, and T. Tao, “Robust uncertainty principles: exact
signal reconstruction from highly incomplete frequency information,”
IEEE Transactions on Information Theory, vol. 52, no. 2, pp. 489–509,
2006.

[3] J. J. Hopﬁeld, “Neural networks and physical systems with emergent
collective computational abilities,” Proceedings of the National Academy
of Sciences, vol. 79, no. 8, pp. 2554–2558, 1982.

[4] J. J. Hopﬁeld and D. W. Tank, “”Neural” computation of decisions in
optimization problems,” Biological Cybernetics, vol. 52, no. 3, pp. 141–
152, 1985.

[5] U.-P. Wen, K.-M. Lan, and H.-S. Shih, “A review of Hopﬁeld neural
networks for solving mathematical programming problems,” European
Journal of Operational Research, vol. 198, no. 3, pp. 675–687, 2009.
[6] A. Bouzerdoum and T. Pattison, “Neural network for quadratic optimiza-
tion with bound constraints,” IEEE Transactions on Neural Networks,
vol. 4, no. 2, pp. 293–304, 1993.

[7] Y. Xia, “A new neural network for solving linear and quadratic pro-
gramming problems,” IEEE Transactions on Neural Networks, vol. 7,
no. 6, pp. 1544–1548, 1996.

[8] C. J. Rozell, D. H. Johnson, R. G. Baraniuk, and B. A. Olshausen,
“Sparse Coding via Thresholding and Local Competition in Neural
Circuits,” Neural Computation, vol. 20, no. 10, pp. 2526–2563, 2008.

