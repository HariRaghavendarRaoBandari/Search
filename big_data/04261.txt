6
1
0
2

 
r
a

 

M
4
1

 
 
]
T
S
h
t
a
m

.

[
 
 

1
v
1
6
2
4
0

.

3
0
6
1
:
v
i
X
r
a

Impact of subsampling and pruning on random

forests.

Roxane Duroux
Sorbonne Universit´es, UPMC Univ Paris 06, F-75005, Paris, France
roxane.duroux@upmc.fr

Erwan Scornet
Sorbonne Universit´es, UPMC Univ Paris 06, F-75005, Paris, France
erwan.scornet@upmc.fr

Abstract

Random forests are ensemble learning methods introduced by Breiman
(2001) that operate by averaging several decision trees built on a ran-
domly selected subspace of the data set. Despite their widespread use
in practice, the respective roles of the diﬀerent mechanisms at work in
Breiman’s forests are not yet fully understood, neither is the tuning of the
corresponding parameters. In this paper, we study the inﬂuence of two pa-
rameters, namely the subsampling rate and the tree depth, on Breiman’s
forests performance. More precisely, we show that fully developed sub-
sampled forests and pruned (without subsampling) forests have similar
performances, as long as respective parameters are well chosen. More-
over, experiments show that a proper tuning of subsampling or pruning
lead in most cases to an improvement of Breiman’s original forests errors.

Index Terms — Random forests, randomization, parameter tuning, sub-
sampling, tree depth.

1

Introduction

Random forests are a class of learning algorithms used to solve pattern recog-
nition problems. As ensemble methods, they grow many base learners (i.e.,
decision trees) and aggregate them to predict. Building several diﬀerent trees
from a single data set requires to randomize the tree building process by, for
example, sampling the data set. Thus, there exists a large variety of random
forests, depending on how trees are designed and how the randomization is
introduced in the whole procedure.

One of the most popular random forests is that of Breiman (2001) which grows
trees based on CART procedure (Classiﬁcation And Regression Trees, Breiman
et al., 1984) and randomizes both the training set and the splitting directions.
Breiman’s (2001) random forests have been under active investigation during the
last decade mainly because of their good practical performance and their ability
to handle high-dimensional data sets. They are acknowledged to be state-of-
the-art methods in ﬁelds such as genomics (Qi, 2012) and pattern recognition
(Rogez et al., 2008), just to name a few.

The ease of the implementation of random forests algorithms is one of their
key strengths and has greatly contributed to their widespread use. A proper

1

tuning of the diﬀerent parameters of the algorithm is not mandatory to obtain
a plausible prediction, making random forests a turn-key solution to deal with
large, heterogeneous data sets.

Several authors studied the inﬂuence of the parameters on random forests ac-
curacy. For example, the number M of trees in the forests has been thoroughly
investigated by D´ıaz-Uriarte and de Andr´es (2006) and Genuer et al. (2010).
It is easy to see that the computational cost for inducing a forest increases
linearly with M , so a good choice for M results from a trade-oﬀ between com-
putational complexity and accuracy (M must be large enough for predictions
to be stable). D´ıaz-Uriarte and de Andr´es (2006) argued that in micro-array
classiﬁcation problems, the particular value of M is irrelevant, assuming that
M is large enough (typically over 500). Several recent studies provided theoret-
ical guarantees for choosing M . Scornet (2014) proposed a way to tune M so
that the error of the forest is minimal. Mentch and Hooker (2014) and Wager
(2014) gave a more in-depth analysis by establishing a central limit theorem for
random forests prediction and providing a method to estimate their variance.
All in all, the role of M on the forest prediction is broadly understood.

Besides the number of trees, forests depend on three parameters: the number an
of data points selected to build each tree, the number mtry of preselected vari-
ables along which the best split is chosen, and the minimum number nodesize
of data points in each cell of each tree. The eﬀect of mtry was thoroughly in-
vestigated in D´ıaz-Uriarte and de Andr´es (2006) and Genuer et al. (2010) who
claimed that the default value is either optimal or too small, therefore leading
to no global understanding of this parameter. The story is roughly the same
regarding the parameter nodesize for which the default value has been reported
as a good choice by D´ıaz-Uriarte and de Andr´es (2006). Furthermore, there is
no theoretical guarantee to support the default values of parameters or any of
the data-driven tuning proposed in the literature.

Our objective in this paper is two-folds: (i) to provide a theoretical framework
to analyse the inﬂuence of the number of points an used to build each tree,
and the tree depth (corresponding to parameters nodesize or maxnode) on the
random forest performances; (ii) to implement several experiments to test our
theoretical ﬁndings. The paper is organized as follows. Section 2 is devoted
to notations and presents Breiman’s random forests algorithm. To carry out
a theoretical analysis of the subsample size and the tree depth, we study in
Section 3 a particular random forest called median forest. We establish an upper
bound for the risk of median forests and by doing so, we highlight the fact that
subsampling and pruning have similar inﬂuence on median forest predictions.
The numerous experiments on Breiman forests are presented in Section 4. Proofs
are postponed to Section 6.

2 First deﬁnitions

2.1 General framework
In this paper, we consider a training sample Dn = {(X1, Y1), . . . , (Xn, Yn)}
of [0, 1]d× R-valued independent and identically distributed observations of a

2

random pair (X, Y ), where E[Y 2] < ∞. The variable X denotes the predictor
variable and Y the response variable. We wish to estimate the regression func-
tion m(x) = E [Y |X = x]. In this context, we use random forests to build an
estimate mn : [0, 1]d → R of m, based on the data set Dn.
Random forests are classiﬁcation and regression methods based on a collection of
M randomized trees. We denote by mn(x, Θj,Dn) the predicted value at point
x given by the j-th tree, where Θ1, . . . , ΘM are independent random variables,
distributed as a generic random variable Θ, independent of the sample Dn.
In practice, the variable Θ can be used to sample the data set or to select
the candidate directions or positions for splitting. The predictions of the M
randomized trees are then averaged to obtain the random forest prediction

M(cid:88)

m=1

mM,n(x, Θ1, . . . , ΘM ,Dn) =

1
M

mn(x, Θm,Dn).

(1)

By the law of large numbers, for any ﬁxed x, conditionally on Dn, the ﬁnite
forest estimate tends to the inﬁnite forest estimate

m∞,n(x,Dn) = EΘ [mn(x, Θ)] .

For the sake of simplicity, we denote m∞,n(x,Dn) by m∞,n(x). Since we carry
out our analysis within the L2 regression estimation framework, we say that
m∞,n is L2 consistent if its risk, E[m∞,n(X)− m(X)]2, tends to zero, as n goes
to inﬁnity.

2.2 Breiman’s forests

Breiman’s (2001) forest is one of the most used random forest algorithms.
In Breiman’s forests, each node of a single tree is associated with a hyper-
rectangular cell included in [0, 1]d. The root of the tree is [0, 1]d itself and, at
each step of the tree construction, a node (or equivalently its corresponding
cell) is split in two parts. The terminal nodes (or leaves), taken together, form
a partition of [0, 1]d. In details, the algorithm works as follows:

1. Grow M trees as follows:

(a) Prior to the j-th tree construction, select uniformly with replacement,
an data points among Dn. Only these an observations are used in
the tree construction.

(b) Consider the cell [0, 1]d.
(c) Select uniformly without replacement mtry coordinates among {1,

. . . , d}.

(d) Select the split minimizing the CART-split criterion (see Breiman

et al., 1984, for details) along the pre-selected mtry directions.

(e) Cut the cell at the selected split.
(f) Repeat (c) − (e) for the two resulting cells until each cell of the tree

contains less than nodesize observations.

3

(g) For a query point x, the j-th tree outputs the average of the Yi falling

into the same cell as x.

2. For a query point x, Breiman’s forest outputs the average of the predic-

tions given by the M trees.

The whole procedure depends on four parameters: the number M of trees, the
number an of sampled data points in each tree, the number mtry of pre-selected
directions for splitting, and the maximum number nodesize of observations in
each leaf. By default in the R package randomForest, M is set to 500, an = n
(bootstrap samples are used to build each tree), mtry = d/3 and nodesize= 5.

Note that selecting the split that minimizes the CART-split criterion is equiva-
lent to select the split such that the two resulting cells have a minimal (empirical)
variance (regarding the Yi falling into each of the two cells).

3 Theoretical results

The numerous mechanisms at work in Breiman’s forests, such as the subsam-
pling step, the CART-criterion and the trees aggregation, make the whole proce-
dure diﬃcult to theoretically analyse. Most attempts to understand the random
forest algorithms (see e.g., Biau et al., 2008; Ishwaran and Kogalur, 2010; Denil
et al., 2013) have focused on simpliﬁed procedures, ignoring the subsampling
step and/or replacing the CART-split criterion by a data independent procedure
more amenable to analysis. On the other hand, recent studies try to dissect the
original Breiman’s algorithm in order to prove its asymptotic normality (Mentch
and Hooker, 2014; Wager, 2014) or its consistency (Scornet et al., 2015). When
studying the original algorithm, one faces the complexity of the algorithm thus
requiring high-level mathematics to prove insightful—but rough— results.

In order to provide theoretical guarantees on the parameters default values in
random forests, we focus in this section on a simpliﬁed random forest called
median forest (see, for example, Biau and Devroye, 2014, for details on median
tree).

3.1 Median Forests

To take one step further into the understanding of Breiman’s (2001) forest
behavior, we study the median random forest, which satisﬁes the X-property
(Devroye et al., 1996). Indeed, its construction depends only on the Xi’s which
is a good trade oﬀ between the complexity of Breiman’s (2001) forests and the
simplicity of totally non adaptive forests, whose construction is independent of
the data set. Besides, median forests can be tuned such that each leaf of each
tree contains exactly one point. In this way, they are closer to Breiman’s forests
than totally non adaptive forests (whose cells cannot contain a pre-speciﬁed
number of points) and thus provide a good understanding on Breiman’s forests
performance even when there is exactly one data point in each leaf.

We now describe the construction of median forest. In the spirit of Breiman’s
(2001) algorithm, before growing each tree, data are subsampled, that is an

4

points (an < n) are selected, without replacement. Then, each split is performed
on an empirical median along a coordinate, chosen uniformly at random among
the d coordinates. Recall that the median of n real valued random variables
X1, . . . , Xn is deﬁned as the only X((cid:96)) satisfying Fn(X((cid:96)−1)) ≤ 1/2 < Fn(X((cid:96))),
where the X(i)’s are ordered increasingly and Fn is the empirical distribution
function of X. Note that data points on which splits are performed are not sent
down to the resulting cells. This is done to ensure that data points are uniformly
distributed on the resulting cells (otherwise, there would be at least one data
point on the edge of a resulting cell, and thus the data points distribution would
not be uniform on this cell). Finally, the algorithm stops when each cell has
been cut exactly kn times, i.e., nodesize= an2−kn . The parameter kn, also
known as the level of the tree, is assumed to verify an2−kn ≥ 4. The overall
construction process is recalled below.

1. Grow M trees as follows:

(a) Prior to the j-th tree construction, select uniformly without replace-
ment, an data points among Dn. Only these an observations are used
in the tree construction.
(b) Consider the cell [0, 1]d.
(c) Select uniformly one coordinate among {1, . . . , d} without replace-

ment.

(d) Cut the cell at the empirical median of the Xi falling into the cell

along the preselected direction.

(e) Repeat (c) − (d) for the two resulting cells until each cell has been

cut exactly kn times.

(f) For a query point x, the j-th tree outputs the average of the Yi falling

into the same cell as x.

2. For a query point x, median forest outputs the average of the predictions

given by the M trees.

3.2 Main theorem

Let us ﬁrst make some regularity assumptions on the regression model.

(H) One has

Y = m(X) + ε,

where ε is a centred noise such that V[ε|X = x] ≤ σ2, where σ2 < ∞ is a
constant. Moreover, X is uniformly distributed on [0, 1]d and m is L-Lipschitz
continuous.

Theorem 3.1 presents an upper bound for the L2 risk of m∞,n.
Theorem 3.1. Assume that (H) is satisﬁed. Then, for all n, for all x ∈ [0, 1]d,

E(cid:2)m∞,n(x) − m(x)(cid:3)2 ≤ 2σ2 2k

(cid:19)k

(cid:18)

1 − 3
4d

+ dL2C1

n

.

(2)

5

In addition, let β = 1 − 3/4d. The right-hand side is minimal for

(cid:20)

(cid:21)

kn =

1

ln 2 − ln β

ln(n) + C3

,

(3)

ln 2

ln 2−ln β . For these choices of kn and an, we

under the condition that an ≥ C4n
have

E(cid:2)m∞,n(x) − m(x)(cid:3)2 ≤ Cn

ln β

ln 2−ln β .

(4)

Equation (2) stems from a standard decomposition of the estimation/appro-
ximation error of median forests.
Indeed, the ﬁrst term in equation (2) cor-
responds to the estimation error of the forest as in Biau (2012) or Arlot and
Genuer (2014) whereas the second term is the approximation error of the forest,
which decreases exponentially in k. Note that this decomposition is consistent
with the existing literature on random forests. Two common assumptions to
prove consistency of simpliﬁed random forests are n/2k → ∞ and k → ∞, which
respectively controls the estimation and approximation of the forest. According
to Theorem 3.1, making these assumptions for median forests results in proving
their consistency.

Note that the estimation error of a single tree grown with an observations is of
order 2k/an. Thus, because of the subsampling step (i.e., since an < n), the
estimation error of median forests 2k/n is smaller than that of a single tree. The
variance reduction of random forests is a well-known property, already noticed
by Genuer (2012) for a totally non adaptive forest, and by Scornet (2014) in the
case of median forests. In our case, we exhibit an explicit bound on the forest
variance, which allows us to precisely compare it to the individual tree variance
therefore highlighting a ﬁrst beneﬁt of forests over singular trees.

As for the ﬁrst term in inequality (2), the second term could be expected.
Indeed, in the levels close to the root, a split is very close to the center of a side
of a cell (since X is uniformly distributed over [0, 1]d). Thus, for all k small
enough, the approximation error of median forests should be close to that of
centred forests studied by Biau (2012). Surprisingly, the rate of consistency of
median forests is faster than that of centred forest established in Biau (2012),
which is equal to

E(cid:2)mcc∞,n(X) − m(X)(cid:3)2 ≤ Cn

−3

4d ln 2+3 ,

(5)

where mcc∞,n stands for the centred forest estimate. A close inspection of the
proof of Proposition 2.2 in Biau (2012) shows that it can be easily adapted to
match the (more optimal) upper bound in Theorem 3.1.

Noteworthy, the fact that the upper bound (4) is sharper than (5) appears to
be important in the case where d = 1. In that case, according to Theorem 3.1,
for all n, for all x ∈ [0, 1]d,

E(cid:2)m∞,n(x) − m(x)(cid:3)2 ≤ Cn−2/3,

which is the minimax rate over the class of Lipschitz functions (see, e.g., Stone,
1980, 1982). This was to be expected since, in dimension one, median random

6

forests are simply a median tree which is known to reach minimax rate (Devroye
et al., 1996). Unfortunately, for d = 1, the centred forest bound (5) turns out
to be suboptimal since it results in

E(cid:2)mcc∞,n(X) − m(X)(cid:3)2 ≤ Cn

−3

4 ln 2+3 .

(6)

Theorem 3.1 allows us to derive rates of consistency for two particular forests:
the pruned median forest, where no subsampling is performed prior to build
each tree, and the fully developed median forest, where each leaf contains a
small number of points. Corollary 1 deals with the pruned forests.
Corollary 1 (Pruned median forests). Let β = 1 − 3/4d. Assume that (H) is
satisﬁed. Consider a median forest without subsampling (i.e., an = n) and such
that the parameter kn satisﬁes (3). Then, for all n, for all x ∈ [0, 1]d,

E(cid:2)m∞,n(x) − m(x)(cid:3)2 ≤ Cn

ln β

ln 2−ln β .

Up to an approximation, Corollary 1 is the counterpart of Theorem 2.2 in Biau
(2012) but tailored for median forests. Indeed, up to a small modiﬁcation of
the proof of Theorem 2.2, the rate of consistency provided in Theorem 2.2 for
centred forests and that of Corollary 1 for median forests are identical. Note
that, for both forests, the optimal depth kn of each tree is the same.

Corollary 2 handles the case of fully grown median forests, that is forests which
contain a small number of points in each leaf.
Indeed, note that since kn =
log2(an) − 2, the number of observations in each leaf varies between 4 and 8.
Corollary 2 (Fully grown median forest). Let β = 1− 3/4d. Assume that (H)
is satisﬁed. Consider a fully grown median forest whose parameters kn and an
satisfy kn = log2(an) − 2. The optimal choice for an (that minimizes the L2
error in (2)) is then given by (3), that is

an = C4n
In that case, for all n, for all x ∈ [0, 1]d,

ln 2

ln 2−ln β .

E(cid:2)m∞,n(x) − m(x)(cid:3)2 ≤ Cn

ln β

ln 2−ln β .

Whereas each individual tree in the fully developed median forest is inconsistent
(since each leaf contains a small number of points), the whole forest is consistent
and its rate of consistency is provided by Corollary 2. Besides, Corollary 2
provides us with the optimal subsampling size for fully developed median forests.

Provided a proper parameter tuning, pruned median forests without subsam-
pling and fully grown median forests (with subsampling) have similar perfor-
mance. A close look at Theorem 3.1 shows that the subsampling size has no
eﬀect on the performance, provided it is large enough. The parameter of real im-
portance is the tree depth kn. Thus, ﬁxing kn as in equation (3), and by varying
the subsampling rate an/n one can obtain random forests that are more-or-less
pruned, all satisfying the optimal bound in Theorem 3.1. In that way, Corollary
1 and 2 are simply two particular examples of such forests.

7

Although our analysis sheds some light on the role of subsampling and tree
depth, the statistical performances of median forests does not allow us to choose
between pruned and subsampled forests.
Interestingly, note that these two
types of random forests can be used in two diﬀerent contexts. If one wants to
obtain fast predictions, then subsampled forests, as described in Corollary 2,
are to be preferred since their computational time is lower than pruned random
forests (described in Corollary 1). However, if one wants to build more accurate
predictions, pruned random forests have to be chosen since the recursive random
forest procedure allows to build several forests of diﬀerent tree depths in one
run, therefore allowing to select the best model among these forests.

4 Experiments

In the light of Section 3, we carry out some simulations to investigate (i) how
pruned and subsampled forests compare with Breiman’s forests and (ii) the
inﬂuence of subsampling size and tree depth on Breiman’s procedure. To do
so, we start by deﬁning various regression models on which the several experi-
ments are based. Throughout this section, we assess the forest performances by
computing their empirical L2 error.

Model 1: n = 800, d = 50, Y = ˜X 2

1 + exp(− ˜X 2
2 )

3 − ˜X4 ˜X7 + ˜X8 ˜X10 − ˜X 2

6 +N (0, 0.5)
Model 2: n = 600, d = 100, Y = ˜X1 ˜X2 + ˜X 2
Model 3: n = 600, d = 100, Y = − sin(2 ˜X1) + ˜X 2
2 + ˜X3 − exp(− ˜X4) +N (0, 0.5)
Model 4: n = 600, d = 100, Y = ˜X1 +(2 ˜X2−1)2 +sin(2π ˜X3)/(2−sin(2π ˜X3))+
sin(2π ˜X4) + 2 cos(2π ˜X4) + 3 sin2(2π ˜X4) + 4 cos2(2π ˜X4) + N (0, 0.5)
Model 5: n = 700, d = 20, Y = 1 ˜X1>0 + ˜X 3
exp(− ˜X 2

2 + 1 ˜X4+ ˜X6− ˜X8− ˜X9>1+ ˜X10

+

2 ) + N (0, 0.5)

Model 6: n = 500, d = 30, Y =(cid:80)10

1 ˜X 3

k=1

Model 7: n = 600, d = 300, Y = ˜X 2

1 + ˜X 2

2

Model 8: n = 500, d = 1000, Y = ˜X1 + 3 ˜X 2

k<0 − 1N (0,1)>1.25
˜X3 exp(−| ˜X4|) + ˜X6 − ˜X8 +N (0, 0.5)
3 − 2 exp(− ˜X5) + ˜X6

For all regression frameworks, we consider covariates X = (X1, . . . , Xd) that are
uniformly distributed over [0, 1]d. We also let ˜Xi = 2(Xi − 0.5) for 1 ≤ i ≤ d.
Some of these models are toy models (Model 1, 5-8). Model 2 can be found
in van der Laan et al. (2007) and Models 3-4 are presented in Meier et al.
(2009). All numerical implementations have been performed using the free R
software. For each experiment, the data set is divided into a training set (80%
of the data set) and a test set (the remaining 20%). Then, the empirical risk
(L2 error) is evaluated on the test set.

8

4.1 Pruning

We start by studying Breiman’s original forests and pruned Breiman’s forests.
Breiman’s forests are the standard procedure implemented in the R package
randomForest, with the parameters default values, as described in Section 2.
Pruned Breiman’s forests are similar to Breiman’s forests except that the tree
depth is controlled via the parameter maxnodes (which corresponds to the num-
ber of leaves in each tree) and that the whole sample Dn is used to build each
tree.

In Figure 1, we present, for the Models 1-8 introduced previously, the evo-
lution of the empirical risk of pruned forests for diﬀerent numbers of terminal
nodes. We add the representation of the empirical risk of Breiman’s original
forest in order to compare all forests errors at a glance. Every sub-ﬁgure of
Figure 1 presents forests built with 500 trees. The printed errors are obtained
by averaging the risks of 50 forests. Because of the estimation/approximation
compromise, we expect the empirical risk of pruned forests to be decreasing and
then increasing, as the number of leaves grows. In most of the models, it seems
that the estimation error is too low to be detected, this is why several risks in
Figure 1 are only decreasing.

For every model, we can notice that pruned forests performance is comparable
with the one of standard Breiman’s forest, as long as the pruning parameter
(the number of leaves) is well chosen. For example, for the Model 1, a pruned
forest with approximately 110 leaves for each tree has the same empirical risk
as the standard Breiman’s forest. In the original algorithm of Breiman’s forest,
the construction of each tree uses a bootstrap sample of the data. For the
pruned forests, the whole data set is used for each tree, and then the randomness
comes only from the pre-selected directions for splitting. The performances of
bootstrapped and pruned forests are very alike. Thus, bootstrap seems not to be
the cornerstone of the Breiman’s forest practical superiority to other regression
algorithms. As it is shown in Corollary 1 and the simulations, pruning and
sampling of the data set (here bootstrap) are equivalent.

In order to study the optimal pruning value (maxnodes parameter in the R
algorithm), we draw the same curves as in Figure 1, for diﬀerent learning data
set sizes (n = 100, 200, 300 and 400). We also copy in an other graph the
optimal values that we found for each size of the learning set. The optimal
pruning value m(cid:63) is deﬁned as

m(cid:63) = min{m : | ˆLm − min

r

ˆLr| < 0.05 × (max

r

ˆLr − min

r

ˆLr)}

where ˆLr is the risk of the forest built with the parameter maxnodes= r. The
results can be seen in Figure 2. According to the last sub-ﬁgure in Figure 2, the
optimal pruning value seems to be proportional to the sample size. For Model
1, the optimal value m(cid:63) seems to verify 0.25n < m(cid:63) < 0.3n. The other models
show a similar behaviour, as it can be seen in Figure 3.
We also present the L2 errors of pruned Breiman’s forests for diﬀerent pruning
percentages (10%, 30%, 63%, 80% and 100%), when the sample size is ﬁxed, for
Models 1-8. The results can be found in Figure 4 in the form of box-plots. We
can notice that the forests with a 30% pruning (i.e., such that maxnodes= 0.3n)

9

give similar (Model 5) or best (Model 6) performances than the standard
Breiman’s forest.

4.2 Subsampling

In this section, we study the inﬂuence of subsampling on Breiman’s forests by
comparing the original Breiman’s procedure with subsampled Breiman’s forests.
Subsampled Breiman’s forests are nothing but Breiman’s forests where the sub-
sampling step consists in choosing an observations without replacement (instead
of choosing n observations among n with replacement), where an is the subsam-
ple size. Comparison of Breiman’s forests and subsampled Breiman’s forests is
presented in Figure 5 for the Models 1-8 introduced previously. More pre-
cisely, we can see the evolution of the empirical risk of subsampled forests with
diﬀerent subsampling values, and the empirical risk of the Breiman’s forest as
a reference. Every sub-ﬁgure of Figure 5 presents forests built with 500 trees.
The printed errors are obtained by averaging the risks of 50 forests.

For every model, we can notice that subsampled forests performance is com-
parable with the one of standard Breiman’s forest, as long as the subsampling
parameter is well chosen. For example, a forest with a subsampling rate of 50%
has the same empirical risk as the standard Breiman’s forest, for Model 2. Once
again, the similarity between bootstrapped and subsampled Breiman’s forests
moves aside bootstrap as a performance criteria. As it is shown in Corollary 2
and the simulations, subsampling and bootstrap of the data set are equivalent.

We want of course to study the optimal subsampling size (samplesize param-
eter in the R algorithm). For this, we draw the curves of Figure 5 for diﬀerent
learning data set sizes, the same as in Figure 2. We also copy in an other graph
the optimal subsample size a(cid:63)
n that we found for each size of the learning set.
The optimal subsampling size a(cid:63)
n = min{a : | ˆLa − min
a(cid:63)

ˆLs| < 0.05 × (max

n is deﬁned as

ˆLs − min

ˆLs)}

s

s

s

where ˆLs is the risk of the forest with parameter sampsize = s. The results
can be seen in Figure 6. The optimal subsampling size seems, once again, to be
proportional to the sample size, as illustrated in the last sub-ﬁgure of Figure 6.
For Model 1, the optimal value a(cid:63)
n seems to be close to 0.8n. The other models
show a similar behaviour, as it can be seen in Figure 7.
Then we present, in Figure 8, the L2 errors of subsampled Breiman’s forests for
diﬀerent subsampling sizes (0.4n, 0.5n, 0.63n and 0.9n), when the sample size
is ﬁxed, for Models 1-8. We can notice that the forests with a subsampling
size of 0.63n give similar performances than the standard Breiman’s forests.
This is not surprising.
Indeed, a bootstrap sample contains around 63% of
distinct observations. Moreover the high subsampling sizes, around 0.9n, lead
to small L2 errors. It may arise from the probably high signal/noise rate. In
each model, when the noise is increasing, the results, exempliﬁed in Figure 9,
are less obvious. That is why we can lawfully use the subsampling size as an
optimization parameter for the Breiman’s forest performance.

10

5 Discussion

In this paper, we studied the role of subsampling step and tree depth in Breiman’s
forest procedure. By analysing a simple version of random forests, we show that
the performance of fully grown subsampled forests and that of pruned forests
with no subsampling step are similar, provided a proper tuning of the parameter
of interest (subsample size and tree depth respectively).

The extended experiments have shown similar results: Breiman’s forests can
be outperformed by either subsampled or pruned Breiman’s forests by properly
tuning parameters. Noteworthy, tuning tree depth can be done at almost no
additional cost while running Breiman’s forests (due to the intrinsic recursive
nature of forests). However if one is interested in a faster procedure, subsampled
Breiman’s forests are to be preferred to pruned forests.

As a by-product, our analysis also shows that there is no particular interest
in bootstrapping data instead of subsampling: in our experiments, bootstrap is
comparable (or worse) than subsampling. This sheds some light on several previ-
ous theoretical analysis where the bootstrap step was replaced by subsampling,
which is more amenable to analyse. Similarly, proving theoretical results on fully
grown Breiman’s forests turned out to be extremely diﬃcult. Our analysis shows
that there is no theoretical background for considering Breiman’s forests with
default parameters values instead of pruned or subsampled Breiman’s forests,
which reveal themselves to be easier to examine.

11

Figure 1: Comparison of standard Breiman’s forests (B. RF) against pruned
Breiman’s forests in terms of L2 error.

12

llllllllllllllllllll01002003004000.020.040.060.080.100.12Model 1Number of terminal nodesL2 errorlPruned B. RFClassic B. RFllllllllllllllllllll01002003004000.440.460.480.500.520.540.56Model 2Number of terminal nodesL2 errorlPruned B. RFClassic B. RFllllllllllllllllllll01002003004000.40.60.81.0Model 3Number of terminal nodesL2 errorlPruned B. RFClassic B. RFllllllllllllllllllll0100200300400345678Model 4Number of terminal nodesL2 errorlPruned B. RFClassic B. RFllllllllllllllllllll01002003004000.200.250.300.350.400.45Model 5Number of terminal nodesL2 errorlPruned B. RFClassic B. RFllllllllllllllllllll01002003004000.40.60.81.01.2Model 6Number of terminal nodesL2 errorlPruned B. RFClassic B. RFllllllllllllllllllll01002003004000.20.30.40.50.6Model 7Number of terminal nodesL2 errorlPruned B. RFClassic B. RFllllllllllllllllllll01002003004000.120.140.160.180.200.22Model 8Number of terminal nodesL2 errorlPruned B. RFClassic B. RFFigure 2: Tuning of pruning parameter (Model 1).

13

llllllllllllllllllll0204060801000.060.080.100.12Model 1Number of terminal nodesL2 errorlPruned B. RFClassic B. RFllllllllllllllllllll0501001502000.040.060.080.100.12Model 1Number of terminal nodesL2 errorlPruned B. RFClassic B. RFllllllllllllllllllll0501001502002503000.040.060.080.100.12Model 1Number of terminal nodesL2 errorlPruned B. RFClassic B. RFllllllllllllllllllll01002003004000.020.040.060.080.100.12Model 1Number of terminal nodesL2 errorlPruned B. RFClassic B. RFllll100150200250300350400406080100120140160Model 1Sample sizeNumber of terminal nodesloptimal rateFigure 3: Optimal values of pruning parameter for Models 1-8.

14

llll100150200250300350400406080100120140160Model 1Sample sizeNumber of terminal nodesloptimal ratellll10015020025030035040051015Model 2Sample sizeNumber of terminal nodesloptimal ratellll10015020025030035040020253035404550Model 3Sample sizeNumber of terminal nodesloptimal ratellll10015020025030035040020406080Model 4Sample sizeNumber of terminal nodesloptimal ratellll1001502002503003504001520253035404550Model 5Sample sizeNumber of terminal nodesloptimal ratellll10015020025030035040020253035404550Model 6Sample sizeNumber of terminal nodesloptimal ratellll1001502002503003504001020304050Model 7Sample sizeNumber of terminal nodesloptimal ratellll10015020025030035040015202530Model 8Sample sizeNumber of terminal nodesloptimal rateFigure 4: Comparison of standard Breiman’s forests against several pruned
Breiman forests in terms of L2 error.

15

10%30%B. RF63%80%100%0.0150.0200.0250.030Model 1lllllll10%30%B. RF63%80%100%0.300.350.400.450.500.55Model 2llllllllllllllllllllll10%30%B. RF63%80%100%0.200.250.300.35Model 310%30%B. RF63%80%100%1.61.82.02.22.42.62.8Model 410%30%B. RF63%80%100%0.120.140.160.18Model 5lllll10%30%B. RF63%80%100%0.150.200.250.300.350.40Model 610%30%B. RF63%80%100%0.160.180.200.220.240.260.28Model 7lllllllllllllllll10%30%B. RF63%80%100%0.090.100.110.120.130.140.15Model 8Figure 5: Standard Breiman Forests versus Subsampled Breiman Forests.

16

llllllllll501001502002503003504000.030.040.050.060.070.080.09Model 1Sample sizeL2 errorlSubsampled B. RFClassic B. RFllllllllll501001502002503003504000.450.500.550.60Model 2Sample sizeL2 errorlSubsampled B. RFClassic B. RFllllllllll501001502002503003504000.30.40.50.60.70.80.9Model 3Sample sizeL2 errorlSubsampled B. RFClassic B. RFllllllllll501001502002503003504002.53.03.54.04.55.05.56.0Model 4Sample sizeL2 errorlSubsampled B. RFClassic B. RFllllllllll501001502002503003504000.200.250.300.35Model 5Sample sizeL2 errorlSubsampled B. RFClassic B. RFllllllllll501001502002503003504000.40.60.81.0Model 6Sample sizeL2 errorlSubsampled B. RFClassic B. RFllllllllll501001502002503003504000.30.40.50.6Model 7Sample sizeL2 errorlSubsampled B. RFClassic B. RFllllllllll501001502002503003504000.120.140.160.18Model 8Sample sizeL2 errorlSubsampled B. RFClassic B. RFFigure 6: Tuning of subsampling rate (model 1).

17

llllllllll204060801000.060.080.100.120.14Model 1Sample sizeL2 errorlSubsampled B. RFClassic B. RFllllllllll501001502000.040.060.080.100.12Model 1Sample sizeL2 errorlSubsampled B. RFClassic B. RFllllllllll501001502002503000.040.060.080.10Model 1Sample sizeL2 errorlSubsampled B. RFClassic B. RFllllllllll501001502002503003504000.030.040.050.060.070.080.09Model 1Sample sizeL2 errorlSubsampled B. RFClassic B. RFllll100150200250300350400100150200250300350Model 1Sample sizeSubsample sizeloptimal rateFigure 7: Optimal values of pruning parameter.

18

llll100150200250300350400100150200250300350Model 1Sample sizeSubsample sizeloptimal ratellll100150200250300350400050100150200250300Model 2Sample sizeSubsample sizeloptimal ratellll100150200250300350400100150200250300Model 3Sample sizeSubsample sizeloptimal ratellll100150200250300350400100150200250300350Model 4Sample sizeSubsample sizeloptimal ratellll100150200250300350400100150200Model 5Sample sizeSubsample sizeloptimal ratellll100150200250300350400100150200250300350400Model 6Sample sizeSubsample sizeloptimal ratellll100150200250300350400100150200250300Model 7Sample sizeSubsample sizeloptimal ratellll100150200250300350400100150200250300Model 8Sample sizeSubsample sizeloptimal rateFigure 8: Standard Breiman forests versus several pruned Breiman forests.

19

SS=0.4nSS=0.5nB. RFSS=0.63nSS=0.9n0.0150.0200.0250.030Model 1lSS=0.4nSS=0.5nB. RFSS=0.63nSS=0.9n0.300.350.400.450.500.55Model 2llllllllllSS=0.4nSS=0.5nB. RFSS=0.63nSS=0.9n0.200.250.300.35Model 3lllllllllSS=0.4nSS=0.5nB. RFSS=0.63nSS=0.9n1.52.02.53.03.5Model 4llllSS=0.4nSS=0.5nB. RFSS=0.63nSS=0.9n0.120.140.160.180.20Model 5lllSS=0.4nSS=0.5nB. RFSS=0.63nSS=0.9n0.150.200.250.300.350.400.45Model 6SS=0.4nSS=0.5nB. RFSS=0.63nSS=0.9n0.150.200.25Model 7lSS=0.4nSS=0.5nB. RFSS=0.63nSS=0.9n0.090.100.110.120.130.14Model 8Figure 9: Standard Breiman forests versus several pruned Breiman forests (noisy
models).

20

lllllSS=0.4nSS=0.5nB. RFSS=0.63nSS=0.9nSS=n0.100.150.20Model 1llSS=0.4nSS=0.5nB. RFSS=0.63nSS=0.9nSS=n0.40.50.60.70.8Model 2llllllSS=0.4nSS=0.5nB. RFSS=0.63nSS=0.9nSS=n0.30.40.50.6Model 3llllllSS=0.4nSS=0.5nB. RFSS=0.63nSS=0.9nSS=n2.02.53.03.54.0Model 4SS=0.4nSS=0.5nB. RFSS=0.63nSS=0.9nSS=n0.150.200.250.300.35Model 5llllSS=0.4nSS=0.5nB. RFSS=0.63nSS=0.9nSS=n0.30.40.50.60.7Model 6llllSS=0.4nSS=0.5nB. RFSS=0.63nSS=0.9nSS=n0.200.250.300.350.40Model 7lllllllllSS=0.4nSS=0.5nB. RFSS=0.63nSS=0.9nSS=n0.140.150.160.170.18Model 86 Proofs

Proof of Theorem 3.1. Let us start by recalling that the random forest estimate
mn can be written as a local averaging estimate

n(cid:88)

(cid:20) n(cid:88)
(cid:20) n(cid:88)

i=1

m∞,n(x) =

Wni(x)Yi,

where

i=1

Wni(x) =

1

Xi

Θ↔x
Nn(x, Θ)

.

Xi

Θ↔x

The quantity 1
indicates whether the observation Xi is in the cell of the
tree which contains x or not, and Nn(x, Θ) denotes the number of data points
falling in the same cell as x. The L2-error of the forest estimate takes then the
form

E(cid:2)m∞,n(x) − m(x)(cid:3)2 ≤ 2E

Wni(x)(Yi − m(Xi))

(cid:21)2

(cid:21)2

+ 2E

Wni(x)(m(Xi) − m(x))

i=1
= 2In + 2Jn.

We can identify the term In as the estimation error and Jn as the approximation
error, and then work on each term In and Jn separately.

Approximation error. Let An(x, Θ) be the cell containing x in the tree
built with the random parameter Θ. Regarding Jn, by the Cauchy Schwartz
inequality,

(cid:112)Wni(x)(cid:112)Wni(x)|m(Xi) − m(x)|

(cid:21)2

Jn ≤ E

≤ E

≤ E

i=1

(cid:20) n(cid:88)
(cid:20) n(cid:88)
 n(cid:88)
(cid:34)
(cid:20)

i=1

i=1

≤ L2E

≤ L2E

(cid:21)

(cid:21)

Wni(x)(m(Xi) − m(x))2

1

Xi

Θ↔x
Nn(x, Θ)

1

Nn(x, Θ)

sup
x,z,

|x−z|≤diam(An(x))

n(cid:88)

i=1

1

Xi

Θ↔x

(cid:21)

(diam(An(x, Θ)))2

,



|m(x) − m(z)|2

(cid:35)

(diam(An(x, Θ)))2

Jn ≤ L2

E

Vl(x, Θ)2

.

(cid:20)

d(cid:88)

l=1

21

where the fourth inequality is due to the L-Lipschitz continuity of m. Let
V(cid:96)(x, Θ) be the length of the cell containing x along the (cid:96)-th side. Then,

According to Lemma 1 speciﬁed further, we have

(cid:20)

(cid:21)

(cid:18)

(cid:19)k

,

1 − 3
4d
with C = exp(12/(4d − 3)). Thus, for all k, we have

Vl(x, Θ)2

≤ C

E

(cid:18)

(cid:19)k

.

Jn ≤ dL2C

1 − 3
4d

Estimation error. Let us now focusing on the term In, we have

E

Wni(x)Wnj(x)(Yi − m(Xi))(Yj − m(Xj))

(cid:21)

(cid:21)2

(cid:21)

In = E

Wni(x)(Yi − m(Xi))

(cid:20)

i=1

(cid:20) n(cid:88)
n(cid:88)
n(cid:88)
(cid:20) n(cid:88)
(cid:20)

i=1

i=1

i=1

=

= E

≤ σ2E

(cid:20)

ni(x)(Yi − m(Xi))2
W 2

max
1≤i≤n

Wni(x)

,

(cid:21)

(cid:20)

since, by (H), the variance of εi is bounded above by σ2. Recalling that an is
the number of subsampled observations used to build the tree, we can note that

E

max
1≤i≤n

Wni(x)

= E

(cid:21)

≤

(cid:20) 1

(cid:21)(cid:21)

EΘ

(cid:20)

max
1≤i≤n
1

x Θ↔Xi
Nn(x, Θ)

(cid:20)

Observe that in the subsampling step, there are exactly(cid:0)an−1

2k − 2

max
1≤i≤n

PΘ

E

an

n−1

x Θ↔ Xi

a ﬁxed observation Xi. Since x and Xi belong to the same cell only if Xi is
selected in the subsampling step, we see that

(cid:21)(cid:21)
(cid:1) choices to pick

.

(cid:104)

x Θ↔ Xi

(cid:105) ≤

(cid:0)an−1
(cid:1)
(cid:1) =
(cid:0)an

n−1

n

an
n

.

PΘ

So,

an
2k − 2
n
since an/2k ≥ 4. Consequently, we obtain

In ≤ σ2

an

1

E(cid:2)m∞,n(x) − m(x)(cid:3)2 ≤ In + Jn ≤ 2σ2 2k

≤ 2σ2 2k
n

,

(cid:18)

1 − 3
4d

(cid:19)k

.

+ dL2C

n

22

We set up now Lemma 1 about the length of a cell that we used to bound the
approximation error.
Lemma 1. For all (cid:96) ∈ {1, . . . , d} and k ∈ N∗, we have

(cid:20)

(cid:21)

(cid:18)

E

Vl(x, Θ)2

≤ C

1 − 3
4d

(cid:19)k

,

with C = exp(12/(4d − 3)).
Proof of Lemma 1. Let us ﬁx x ∈ [0, 1]d and denote by n0, n1, . . . , nk the num-
ber of points in the successive cells containing x (for example, n0 is the number
of points in the root of the tree, that is n0 = an). Note that n0, n1, . . . , nk
depends on Dn and Θ, but to lighten notations, we omit these dependencies.
Recalling that V(cid:96)(x, Θ) is the length of the (cid:96)-th side of the cell containing x,
this quantity can be written as a product of independent beta distributions:

V(cid:96)(x, Θ)

D
=

(cid:2)B(nj + 1, nj−1 − nj)(cid:3)δ(cid:96),j (x,Θ)

,

k(cid:89)

j=1

where B(α, β) denotes the beta distribution of parameters α and β, and the in-
dicator δ(cid:96),j(x, Θ) equals to 1 if the j-th split of the cell containing x is performed
along the (cid:96)-th dimension (and 0 otherwise). Consequently,

E(cid:2)V(cid:96)(x, Θ)2(cid:3) =

j=1

j=1

j=1

=

=

k(cid:89)
k(cid:89)
k(cid:89)
k(cid:89)
k(cid:89)
≤ k(cid:89)
≤ k(cid:89)

=

=

j=1

j=1

j=1

j=1

E

E

E

E

(cid:20)(cid:2)B(nj + 1, nj−1 − nj)(cid:3)2δ(cid:96),j (x,Θ)(cid:21)
(cid:21)(cid:21)
(cid:20)(cid:2)B(nj + 1, nj−1 − nj)(cid:3)2δ(cid:96),j (x,Θ)(cid:12)(cid:12)δ(cid:96),j(x, Θ)
(cid:20)
(cid:20)
1δ(cid:96),j (x,Θ)=0 + E(cid:2)B(nj + 1, nj−1 − nj)(cid:3)21δ(cid:96),j (x,Θ)=1
(cid:18) d − 1
(cid:18) d − 1
(cid:18) d − 1
(cid:18)

E(cid:2)B(nj + 1, nj−1 − nj)(cid:3)2(cid:19)
(cid:19)
(cid:19)

(nj−1 + 2)(nj−1 + 4)
(nj−1 + 1)(nj−1 + 2)

(nj−1 + 1)(nj−1 + 2)

(nj + 1)(nj + 2)

(cid:19)

1
4d

1
d

1
d

+

+

+

d

d

d

(cid:21)

1 − 1
d

+

1
4d

nj−1 + 4
nj−1 + 1

,

(7)

where the ﬁrst inequality stems from the relation nj ≤ nj−1/2 for all j ∈

23

{1, . . . , k}. We have the following inequalities.

nj−1 + 4
nj−1 + 1

≤ an + 2j+1

an − 2j−1 =

≤

since

Going back to inequality (7), we ﬁnd

(cid:20)

(cid:21)

E

Vl(x, Θ)2

(cid:19)

(cid:19)2(cid:21)

an + 2j+1
an(1 − 2j−1
1

an

)

2j−1
an

1 +

1 − 2j−1

an

(cid:18)
(cid:19)2

,

≤ 1
2

.

+

1
4d

1 +

2j+1
an

(cid:18)

(cid:21)

2j−1
an

2j−k

2k
an

(cid:21)

2−j−1

.

(cid:21)

+

+

+

3
d

3
d

3
d

≤ an + 2j+1

an

(cid:18)

2j+1
an

1 +

2j−1
an

1 − 3
4d

j=1

j=1

an

1 − 1
d

≤ 2k−1
(cid:20)
(cid:20)
(cid:20)
(cid:20)

≤ k(cid:89)
≤ k(cid:89)
≤ k(cid:89)
≤ k−1(cid:89)
(cid:21) = k ln

j=0

j=1

1 − 3
4d

1 − 3
4d

(cid:18)
(cid:18)

≤ k ln

Moreover, we can notice that

(cid:20)

k−1(cid:89)

j=0

ln

1 − 3
4d

+

3
d

2−j−1

This yields to the desired upper bound

(cid:20)

(cid:21)

E

Vl(x, Θ)2

≤ C

(cid:19)
(cid:19)

(cid:18)

ln

1 + 6

(cid:19)

2−j
4d − 3

k−1(cid:88)

j=0

12

4d − 3

.

,

+

+

(cid:19)k

1 − 3
4d

1 − 3
4d

(cid:18)

1 − 3
4d

with C = exp(12/(4d − 3)).

We now put our interest in the proofs of the two Corollaries presented in Section
3.

Proof of Corollary 1. Regarding Theorem 3.1, we want to ﬁnd the optimal value
of pruning, in order to obtain the best rate of convergence for the forest estimate.

24

(cid:16)

(cid:17)

Let C1 = 2σ2

n and C2 = d 3

2 L2C and β =

E(cid:2)m∞,n(x) − m(x)(cid:3)2 ≤ C12k + C2βk.

. Then,

4d

1 − 3

Let f : x (cid:55)→ C1ex ln 2 + C2ex ln(β). Thus,

f(cid:48)(x) = C1 ln 2ex ln 2 + C2 ln(β)ex ln(β)

= C1 ln 2ex ln 2

1 +

C2 ln(β)
C1 ln 2

ex(ln(β)−ln 2)

(cid:18)

(cid:19)

.

Since β ≤ 1, f(cid:48)(x) ≤ 0 for all x ≤ x(cid:63) and f(cid:48)(x) ≥ 0 for all x ≥ x(cid:63), where x(cid:63)
satisﬁes

f(cid:48)(x(cid:63)) = 0

⇐⇒ x(cid:63) =

1

ln 2 − ln(β)

ln

(cid:20)

1

ln 2 − ln(β)

ln

⇐⇒ x(cid:63) =

⇐⇒ x(cid:63) =

⇐⇒ x(cid:63) =

(cid:16)

1

(cid:20)

ln 2 − ln

1 − 3

4d

1

(cid:16)

ln 2 − ln β

− dL2C ln

2σ2 ln 2

1− 3

4d

(cid:17)

where C3 = ln

Consequently,

E(cid:2)m∞,n(x) − m(x)(cid:3)2 ≤ C1 exp(x(cid:63) ln 2) + C2 exp(x(cid:63) ln β)
(cid:21)

(cid:20)

≤ C1 exp

ln(n) + C3

ln 2

+ C2 exp

ln(n) + C3

ln β

ln(n) + C3

,

C1

+ ln

− C2 ln(β)
C1 ln 2

(cid:18)
(cid:19)
(cid:18) 1
(cid:19)
(cid:18)
ln(n) + ln
(cid:17)
(cid:21)
.
(cid:18)
(cid:18)
(cid:19)
(cid:18) C3 ln 2
(cid:18) C3 ln β
(cid:16)

ln 2 − ln β
ln 2−ln β −1 + C6n

ln 2 − ln β

ln 2 − ln β

ln 2 − ln β

ln 2

1

1

(cid:16)

ln

1− 3
4d

(cid:19)

≤ C1 exp

+ C2 exp

≤ C5n

(cid:18)
(cid:19)

and C6 = C2 exp

25

− C2 ln(β)

(cid:19)(cid:21)
(cid:16)
− dL2C ln

ln 2

1 − 3

4d

2σ2 ln 2

(cid:17)



(cid:19)

(cid:19)

(cid:21)

(cid:19)

(cid:19)

ln 2

(cid:18)

ln 2 − ln β
ln β

ln(n)

exp

ln 2 − ln β

ln(n)

(cid:20)

(cid:18)

exp

(cid:19)
(cid:17)

(cid:17)

ln β

ln 2−ln β

(cid:18)

(cid:19)

.

C3 ln β
ln 2−ln β

≤

C5 + C6

ln 2−ln

n

1− 3
4d

,

(cid:18)

where C5 = 2σ2 exp

C3 ln 2
ln 2−ln β

Proof of Corollary 2. In this Corollary, we focus on the optimal value of sub-
sampling, always for the speed of convergence for the forest estimate. Since
kn = log2(an) − 2 and kn satisﬁes equation (3), we have

an = 4.2

C3

ln 2−ln β .n

ln 2

ln 2−ln β ,

where, simple calculations show that

(cid:18) 3L2e12/(4d−3)

(cid:19) ln 2

ln 2−ln β

.

C3

ln 2−ln β =

2

8σ2 ln 2

This concludes the proof, according to Theorem 3.1.

References

S. Arlot and R. Genuer. Analysis of purely random forests bias. arXiv:1407.3939,

2014.

G. Biau. Analysis of a random forests model. Journal of Machine Learning

Research, 13:1063–1095, 2012.

G. Biau and L. Devroye. Cellular tree classiﬁers.

In Algorithmic Learning

Theory, pages 8–17. Springer, 2014.

G. Biau, L. Devroye, and G. Lugosi. Consistency of random forests and other
averaging classiﬁers. Journal of Machine Learning Research, 9:2015–2033,
2008.

L. Breiman. Random forests. Machine Learning, 45:5–32, 2001.

L. Breiman, J.H. Friedman, R.A. Olshen, and C.J. Stone. Classiﬁcation and

Regression Trees. Chapman & Hall/CRC, Boca Raton, 1984.

M. Denil, D. Matheson, and N. de Freitas. Consistency of online random forests,

2013. arXiv:1302.4853.

L. Devroye, L. Gy¨orﬁ, and G. Lugosi. A Probabilistic Theory of Pattern Recog-

nition. Springer, New York, 1996.

R. D´ıaz-Uriarte and S. Alvarez de Andr´es. Gene selection and classiﬁcation of

microarray data using random forest. BMC Bioinformatics, 7:1–13, 2006.

R. Genuer. Variance reduction in purely random forests. Journal of Nonpara-

metric Statistics, 24:543–562, 2012.

R. Genuer, J. Poggi, and C. Tuleau-Malot. Variable selection using random

forests. Pattern Recognition Letters, 31:2225-2236, 2010.

H. Ishwaran and U.B. Kogalur. Consistency of random survival forests. Statistics

& Probability Letters, 80:1056–1064, 2010.

L. Meier, S. Van de Geer, and P. B¨uhlmann. High-dimensional additive model-

ing. The Annals of Statistics, 37:3779–3821, 2009.

26

L. Mentch and G. Hooker. Ensemble trees and clts: Statistical inference for

supervised learning. arXiv:1404.6473, 2014.

Y. Qi. Ensemble Machine Learning, chapter Random forest for bioinformatics,

pages 307–323. Springer, 2012.

G. Rogez, J. Rihan, S. Ramalingam, C. Orrite, and P. H. Torr. Randomized
In IEEE Conference on Computer Vision

trees for human pose detection.
and Pattern Recognition, pages 1–8, 2008.

E. Scornet. On the asymptotics of random forests. arXiv:1409.2090, 2014.

E. Scornet, G. Biau, and J.-P. Vert. Consistency of random forests. The Annals

of Statistics, 43:1716–1741, 2015.

C.J. Stone. Optimal rates of convergence for nonparametric estimators. The

Annals of Statistics, 8:1348–1360, 1980.

C.J. Stone. Optimal global rates of convergence for nonparametric regression.

The Annals of Statistics, 10:1040–1053, 1982.

M. van der Laan, E.C. Polley, and A.E. Hubbard. Super learner. Statistical

Applications in Genetics and Molecular Biology, 6, 2007.

S. Wager. Asymptotic theory for random forests. arXiv:1405.0352, 2014.

27

