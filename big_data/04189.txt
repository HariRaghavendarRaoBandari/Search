6
1
0
2

 
r
a

 

M
4
1

 
 
]
P
A

.
t
a
t
s
[
 
 

1
v
9
8
1
4
0

.

3
0
6
1
:
v
i
X
r
a

Cohort eﬀect in survival analysis: a change-point

perspective.

Olivier Bouaziz

MAP5, Universit´e Paris Descartes

45 rue des Saints P`eres, 75270 Paris Cedex 06

Gr´egory Nuel

LPMA, CNRS 7599

4 place Jussieu, F-75005 Paris

March 15, 2016

Abstract

In a survival analysis context we suggest a new method to treat the so-called
cohort eﬀet as a breakpoint problem where each stratum has its own baseline haz-
ard function and proportional hazard parameter. Our approach combines various
baseline hazard estimators (exponential, Weibull, piecewise constant, and non para-
metric) with the proportional hazard model into a EM-based segmentation approach
using constrained HMM. The method provides an automatic procedure to estimate
the hazard function in each stratum as well as the full posterior distribution of
breakpoint locations. The new method is applied both to simulated data and to a
diabetes cohort where it proves its consistence and usefulness.

Keywords: Cohort eﬀect; Constrained HMM; Cox model; EM; Survival analysis

1

Introduction

In epidemiology, it is well known that survival data are subject to the so-called cohort
eﬀect (see Yang and Land, 2013, for the deﬁnition of cohort eﬀect for instance). In a
cohort study, individuals recruited or born at diﬀerent dates might have heterogeneous
hazard rates. While this phenomenon can be continuous in nature with a slow change of
hazard rates over time, it is often due to a radical change in the treatment or prevention
strategy. For instance, if one is interested in the time until ﬁrst bacterial infection, the
discover of penicillin might represent a breakpoint in the survival study. Patients born at
an early date such that they could not beneﬁt from the penicillin treatment would have
a diﬀerent survival distribution compared to other patients who might have had access
to penicillin treatment. Other examples include tritherapy in HIV patients, national
screening policy for patients with cancer such as breast cancer for instance. This cohort

1

eﬀect naturally lead us to consider breakpoint models that take into account the survival
heterogeneity between patients.

In the literature, one classical method to avoid bias caused by a possible heterogeneity
in the survival distribution consists in adjusting with respect to the variable year of birth.
For example, in Andersen et al (1993), the authors studied a dataset on nephropathy
for diabetics (introduced in Example I.3.11 of their book) using a multi-state model,
where each transition intensity models was adjusted with respect to the calendar time
of disease onset (see Table VII.2.1 page 520 of their book). The authors concluded that
“it is seen that all intensities decrease with t0 (the calendar year of onset of diabetes),
indicating a general medical improvement over time”. More recently, adjusting with
respect to the year of birth is also one of the recommendation of Kratz (2011) where
the authors mention that “Survival analysis tracks length of time without reference to
calendar time. This is the reason that many analysis adjust for year of diagnosis or birth
cohort (i.e., year or period of years of birth)”.

A more sophisticated method to take into account a cohort eﬀect is the Age-Period-
Cohort approach which consists in adjusting a proportional hazard model on various
cohort-orientated covariates (age, date of birth or recruitment, location, etc.). We refer
the reader to Yang and Land (2013) for a thorough review of all recent works for Age-
Period-Cohort analysis.

An other alternative consists in dividing the ordered dataset in arbitrary segments
(typically, every decade) and estimate some survival quantities on each segment. This is
what is done in Bergh et al (1999) for example, where the authors stratiﬁed their study
on each year of birth and computed odds ratio on each stratum. A reﬁnement of this
stratiﬁcation method could be to merge any adjacent segments whose survival distribu-
tions are similar. To our knowledge, there exists no methods that perform automatic
location in the change of survival distribution and at the same time allows estimation
on each stratum.

From a statistical point of view we consider this situation as a change-point model
where abrupt changes occur either in terms of baseline hazard rates or in terms of
proportional factors. In such a model, we aim at two objectives: ﬁrst we want to estimate
the hazard rates and the proportional factors in each homogenous region through a Cox
model (see Cox et al, 1972). Secondly, we want to accurately provide the number and
location of the breakpoints. Recently a constrained Hidden Markov Model (HMM)
method was suggested in the context of breakpoint analysis (see Luong et al, 2013).
This method allows to perform a full change-point analysis in a segment-based model
(one parameter by segment) providing linear EM estimates of the parameter and a full
speciﬁcation of the posterior distribution of change points.
In this paper we adapt
this method to the context of survival analysis with hazard rate estimates, where the
estimation is performed through the EM algorithm (see Dempster et al, 1977) to provide
update of the estimates and the posterior distribution at each iteration step.

In the classical Cox model, the baseline is usually left unspeciﬁed. This allows great
ﬂexibility in the model while the Cox’s partial likelihood provides eﬃcient estimation of
the regression parameters. Estimation of the cumulative baseline is performed through

2

the Breslow estimator (see Breslow, 1972). However, in our context classical estimation
methods will not lead to consistent estimators due to numerical instabilities. In order
to provide estimation in a Cox model with nonparametric baseline, a smooth estimator
of the baseline is required. Therefore, diﬀerent strategies are proposed throughout this
article to model the baseline in the Cox model. Three parametric baseline models are
studied, the exponential baseline, the Weibull baseline and the piecewise constant haz-
ard baseline, and the nonparametric baseline model is implemented with a kernel type
estimator for the baseline.

In Section 2.1, the stratiﬁed Cox model is presented along with some discussions of
basic assumptions on the model. In Section 2.2, the likelihood of the model is presented.
It can be seen as a weighted likelihood where the weights correspond to the posterior
probability of each individual to be in each segment given the data and the previous
update of the model parameter. The EM algorithm is then introduced as an iterated
method to perform estimation in this context. In Section 3, computation of the weights is
derived. In Section 4, maximisation of the log-likelihood is developed for a ﬁxed weight.
All three parametric baseline hazards and the nonparametric baseline are considered in
this section. Section 5 gives a summary of the implementation of the proposed algorithm
along with some discussions on the calibration of the algorithm parameters. A simulation
study is presented in Section 6 and a real data analysis on survival of diabetic patients is
studied in Section 7. Finally, Section 8 concludes this article with some general comments
on the proposed methods.

2 Model and estimation procedure

2.1 The stratiﬁed hazard rate model
Let T ∗ represent the survival time of interest associated with its counting process N∗(t) =
I(T ∗ ≤ t) and its at risk process Y ∗(t) = I(T ∗ ≥ t) for t ≥ 0. Let X represent a p-
dimensional covariate row vector. In practice, T ∗ might be censored by a random variable
C so that we observe (T = T ∗∧ C, ∆ = I(T ∗ ≤ C), X). Introduce the observed counting
and at risk processes denoted respectively by N (t) = I(T ≤ t, ∆ = 1) and Y (t) = I(T ≥
t) and let τ be the endpoint of the study. The data consist of n independent replications
(Ti, ∆i, X i)i=1,...,n associated with their counting process Ni(t) and at risk process Yi(t),
for t ∈ [0, τ ].

The cohort eﬀect is modelized through the random variables R and its n i.i.d. repli-
cations R1, R2, . . . , Rn which represent a segment index associated to each individual.
We suppose that the population is composed of K segments such that for i = 1, . . . , n,
Ri ∈ {1, 2, . . . , K}. Without loss of generality, we also assume that the Ris are ordered.
For example, if the population is a mixture of three subpopulations such that we have
n = 10 and two breakpoints occurring after positions 3 and 7 then R1:10 = 1112222333.
The goal of this paper is to study a hazard Cox model stratiﬁed with respect to the

3

segment index. This model is deﬁned in the following way:

K(cid:88)

E[dN∗(t)|Y ∗(t), X, R] = Y ∗(t)

λk(t) exp(Xβk)I(R = k)dt,

(1)

k=1

gression parameters associated to each segment index. Let Λk(t) = (cid:82) t

where the λk represent unknown baseline hazard functions and the βk unknown re-
0 λk(s)ds repre-
sents the cumulative baseline hazard function of the kth segment index. We denote by
θ = (Λ1, . . . , ΛK, β1, . . . , βK) the model parameter we aim to estimate. Note that if
the Ris were observed and if β1 = ··· = βK, this model would reduce to the classical
stratiﬁed Cox model (see for instance Martinussen and Scheike, 2006, page 190).

In order to make inference on the model parameter we will assume that the endpoint
τ is deﬁned such that, for all t in [0, τ ], P[T > t] > 0. This assumption is common in
survival analysis settings to prevent us from classical estimation problems that occur in
the right tail of the distribution of T , see for instance Andersen et al (1993). We will
also suppose the following independent censoring assumption. For all t in [0, τ ], assume
that

E[dN∗(t)|Y ∗(t), X, R] = E[dN∗(t)|Y (t), X, R].

(2)

This is the classical independent right censoring assumption (see for instance Def-
inition III.2.1. of Andersen et al, 1993 or page 53 of Martinussen and Scheike, 2006)
adapted to our stratiﬁed model. This assumption means that the censoring variable does
not carry any extra information about the probability of observing a new event given the
covariate vector and the segment index. It will be trivially fulﬁlled if one would suppose
the censoring variable to be independent of the event time conditionally on X and R.
Note however that Equation (2) is slightly more general. In the absence of covariates,
examples of data where C and T ∗ are dependent but Equation (2) is still valid can be
found for instance in Exercise 1.8 in Fleming and Harrington (1991).
The main interest in Equation (2) lies in the fact that under independent censoring
our model deﬁned by Equation (1) is still veriﬁed if we replace the processes N∗(t) and
Y ∗(t) by their observed counterpart, namely N (t) and Y (t). This is a standard result
in survival analysis. In the absence of covariates and of the segment index, the proof of
this result can be found for example in Fleming and Harrington (1991), pages 27-29.

Therefore, an estimation procedure on the model parameter can be carried out using

the observed data. Let

ei(k; θ) = P(Ti, ∆i, X i|Ri = k; θ)

represents the contribution of the ith individual to the likelihood conditionally to its
segment index being equal to k. From standard arguments on likelihood constructions
in the context of survival analysis (see for instance Andersen et al, 1993), we have under

4

independent and non informative censoring:

(cid:90) τ

0

(cid:8)log(cid:0)λk(t)(cid:1) + X iβk

(cid:9) dNi(t)

log ei(k; θ) =

(cid:90) τ

0

−

Yi(t)λk(t) exp(X iβk)dt,

(3)

where the equality holds true up to a constant that does not depend on the model pa-
rameter θ. Since the segment indexes are not observed, the conditional likelihood of our
model with respect to the segment indexes cannot be directly computed. To overcome
this problem, an Expectation-Maximization (EM) algorithm procedure is developed in
the next section.

2.2 The EM algorithm

By considering the segmentation R1:n = R1, R2, . . . , Rn as a latent variable, the EM-
algorithm (see Dempster et al, 1977) consists in performing alternatively until conver-
gence the following two-steps.

(cid:90)

Expectation Step: compute the conditional expected log-likelihood

Q(θ|θold) =

P(R1:n|data; θold) log P(R1:n, data; θ)dR1:n

R1:n

where θold denote the previous value of the parameter and data = (T1:n, ∆1:n, X 1:n).

Maximization Step: update parameter with

ˆθ = argmax

θ

Q(θ|θold).

(4)

Assuming that the prior segmentation distribution P(R1:n; θ) does not depend on θ,

we easily get:

Q(θ|θold) =

n(cid:88)

K(cid:88)

wi(k; θold) log ei(k; θ)

(5)

where for any i ∈ {1, . . . , n}, k ∈ {1, . . . , K} and θ we deﬁne:

i=1

k=1

wi(k; θ) = P(Ri = k|data; θ).

Our EM algorithm hence alternates two steps. First, the E-Step which consists in
computing the weights wi(k; θold). This is done in Section 3 using a constrained Hidden
Markov Model (HMM). Then for the M-Step, Equation (4) needs to be solved. This is
done in Section 4 using the weighted log-likelihood expression given by Equation (5).

5

3 Computation of the posterior segment distribution

As suggested in Luong et al (2013), the posterior segmentation distribution can be
obtained using the constrained HMM. For completeness, we give all the necessary in-
formation to implement this constrained HMM. The basic idea consists in modeling the
segmentation variable R1:n using a Markov chain over {1, . . . , K, K + 1} where K + 1
is an absorbing (technical junk) state. The segmentation always start with R1 = 1 and
its transition matrix P(Ri|Ri−1) is given by the following matrix (in the particular case

where K = 4): 

1 − ηi(1)

0
0
0
0

ηi(1)
1 − ηi(2)

0
0
0

0

ηi(2)
1 − ηi(3)

0
0

0
0

0
0
ηi(3)
0
1 − ηi(4) ηi(4)
1

0



where ηi(k) = P(Ri = k + 1|Ri−1 = k) is a prior distribution. In order to obtain a valid
segmentation of n points into K segments, one must add the constraint that {Rn = K},
this is why the model can be seen as a constrained HMM. A very natural choice for
the prior distribution is to use ηi(k) = constant ∈ [0, 1] which leads to a uniform prior
distribution over the space of segmentations. But more sophisticated prior might be
use: priors forbidding change-points at certain locations (this might for example be
useful for dealing with ties in data ordering), priors incorporating knowledge on most
likely breakpoint locations, or even using posterior segmentation distribution from a
previous study as a prior.
For any given parameter θ, we then introduce the following forward and backward
quantities: Fi(k; θ) = P(data1:i, Ri = k; θ) and Bi(k; θ) = P(datai+1:n, Rn = K|Ri =
k; θ) for all i ∈ {1, . . . , n} and k ∈ {1, . . . , K}. These quantities can be computed
recursively using the following recursions:

Fi(k; θ) = Fi−1(k − 1; θ)ηi(k − 1)ei(k; θ) + Fi−1(k; θ)(1 − ηi(k))ei(k; θ)
Bi−1(k; θ) = (1 − ηi(k))ei(k; θ)Bi(k; θ) + ηi(k)ei+1(k + 1; θ)Bi(k + 1; θ)

and we can derive from them posterior distributions of interest:

P(Ri = k|data; θ) = wi(k; θ) ∝ Fi(k; θ)Bi(k; θ)

P(BPk = i|data; θ) ∝ Fi(k; θ)ηi+1(k)ei+1(k + 1; θ)Bi+1(k + 1; θ)

where {BPk = i} = {Ri = k, Ri+1 = k + 1}.
It is hence clear that Equation (8)
allows to compute the marginal weights used in the EM algorithm (Section 2.2) while
Equation (9) gives the marginal distribution of the kth breakpoint. Note that the full
posterior segmentation distribution can be proved to be an heterogeneous Markov chain
which transition can be derived immediatelty from Equations (8) and (9) (see Luong
et al, 2013, for more details).

(6)

(7)

(8)

(9)

6

Let us ﬁnally point out that the likelihood also can be derived from the forward-

backward quantities and for any i ∈ {1, . . . , n} as:

P(data|θ) =

P(data, R1:n, Rn = K|θ)

P(R1:n, Rn = K|θ)

=

R1:n

k=1 Fi(k; θ)Bi(k; θ)

k=1 F 0

i (k)B0

i (k)

(10)

(cid:80)

(cid:80)

R1:n

(cid:80)K
(cid:80)K

where F 0 and B0 are obtained through recursions (6) and (7) by replacing all ei(k; θ)
by 1:

i−1(k − 1)ηi(k − 1) + F 0

i−1(k)(1 − ηi(k))

i (k) = F 0
F 0
i−1(k) = (1 − ηi(k))B0
B0

i (k) + ηi(k)B0

i (k + 1).

These quantities depend only on η, n and K, thus they do not need to be updated during
the EM algorithm.

4 Log-likelihood maximization with known weights

Suppose you have at hand some preliminary estimator θold. In Section 3, we showed how
to use this quantity to estimate the marginal posterior probability wi(k; θold) of position
i to be in the kth segment given the data and under θold. From the expression of the
ei(k, θ) derived in (3), Equation (4) can be solved by maximizing a simple weighted
log-likelihood. When the weights are all equal to 1, statistical inference has already
been studied, either in a fully parametric case if one assumes a parametric form for the
baseline hazard rate (see for instance Kalbﬂeisch and Prentice, 2002) or in a semipara-
metric way if the baseline hazard rate is left unspeciﬁed which corresponds to the well
known Cox model.
In the latter case, a weighted log-likelihood has also been brieﬂy
studied in Therneau and Grambsch (2000), pages 161-168. But in both parametric and
semiparametric cases, our weighted log-likelihood estimation procedure is very similar
to the standard estimation techniques used in the absence of weights.

From Equation (1) applied to the observed counting and at-risk processes, one gets

the following decomposition:

Ni(t)I(Ri = k) −

Yi(s) exp(X iβk)I(Ri = k)dΛk(s) = I(Ri = k)Mi(t),

(cid:90) t

0

where Mi(t) is a martingale with respect to the ﬁltration σ(Ni(s), Yi(s), X i, I(Ri = k) :
0 ≤ s ≤ t, k = 1, . . . , K). Taking the expectation conditionally on {N1:n(t), Y1:n(t), X 1:n :
0 ≤ t ≤ τ}, summing over the n individuals and taking the diﬀerential of this expression
gives:

{dNi(t)wi(k; θold) − Yi(t) exp(X iβk)wi(k; θold)dΛk(t)} =

wi(k; θold)dMik(t).

(11)
This is the weighted version of the standard martingale decomposition. In Section 4.3 a
weighted Nelson-Aalen estimator will be derived from this equality.

n(cid:88)

i=1

n(cid:88)

i=1

7

In the next sections, we introduce diﬀerent estimators obtained from diﬀerent choices
for the baseline hazard rate in a Cox model. We propose three parametric families for
the baseline hazard: the exponential, the Weibull and the piecewise constant hazard
cases. We also study the nonparametric case that arises when the baseline hazard is
left unspeciﬁed. This case is the most ﬂexible since it does not require any particular
form for the baseline hazard but, as shown in Section 5.2, this model involves a smooth-
ing parameter in the estimation procedure in order to consistently estimate the model
parameter and the posterior segment distribution.

4.1 The exponential and Weibull baseline hazards

In this model, we assume that the baseline hazard in the kth segment index belongs to
the Weibull family with shape parameter λk and scale parameter pk. That is, λk(t) =
pk(t/λk)pk−1/λk, Λk(t) = (t/λk)pk and Sk(t) = exp(−(t/λk)pk ).

Equation (3) can then be written in the following way:
log (ei(k; θ)) = ∆i (log(pk) − pk log(λk) + (pk − 1) log(Ti) + X iβk)

−(cid:16) Ti

λk

(cid:17)pk

exp(X iβk).

The exponential family is derived as a special case of the Weibull case by setting

pk = 1 for all k = 1, . . . , K. In that case, Equation (3) reduces to:

log (ei(k; θ)) = ∆i (− log(λk) + X iβk) −(cid:16) Ti

(cid:17)

exp(X iβk).

λk

Computation of the estimates through Equation (4) is done via the survreg function
in the survival R package. The gradient vector and Hessian matrix can directly be
derived from the expression of the log-likelihood and the estimates can then be computed
using the Newton-Raphson algorithm. A weight option is also available in the survreg
function which allows to compute estimates that precisely maximize the log-likelihood
Q(θ|θold) presented in Equation (4).

The models obtained under these families of baseline hazard functions have the nice
property that they both belong to the class of parametric Cox models and of parametric
Accelerated Failure Time models (Kalbﬂeisch and Prentice, 2002). Moreover, the two
parameters of the Weibull family make the baseline hazard quite ﬂexible. As a matter of
fact, the Weibull model will provide a fairly good ﬁt to any true baseline hazard that is
monotone with time. However, these families of model will not properly ﬁt a model with
true baseline hazard having a bathtub shape (i.e a ∪ shape) or an upside down bathtub
shape (i.e. a ∩ shape) which are common types of baseline that can occur in practice.
The model introduced in the next section does not assume any speciﬁc shape for
the baseline hazard and consequently will be able to ﬁt any class of baseline hazard
functions. However, this model requires to specify in advance a number of cutpoints and
makes the approximation that the hazard is constant between each cutpoint.

8

4.2 The piecewise constant baseline hazard

In this model, the baseline hazard on each segment index is assumed to be piecewise
constant on L cuts represented by c0, c1, . . . , cL, with the convention that c0 = 0 and
cL = +∞. Let Il(t) = I(cl−1 < t ≤ cl). We suppose that

L(cid:88)

l=1

λk(t) =

Il(t)αk
l ,

L(cid:88)
L(cid:88)

l=2

l=2

Λk(t) = αk

1tI1(t) +

(αk

1c1 + ··· + αk

l−1(cl−1 − cl−2) + αk

l (t − cl−1))Il(t),

Sk(t) = exp(αk

1t)I1(t) +

exp(αk

1c1 + ··· + αk

l (t − cl−1))Il(t).

Equation (3) can then be written in the following form:

l−1(cl−1 − cl−2) + αk
(cid:90) τ

as:

(cid:111)

log (ei(k; θ)) = ∆i (log(λk(Ti)) + X iβk) −

Yi(t)λk(t)dt exp(X iβk).

For computational purpose, it is interesting to note that the log-likelihood can be written

in a Poisson regression form. Introduce Ri,l =(cid:82) τ
time individual i is at risk in the lth interval and Oi,l =(cid:82) τ
(cid:80)

0 Yi(t)Il(t)dt = cl ∧ Ti − cl−1, the total
0 Il(t)dNi(t) = Il(Ti)∆i, the
number of events for individual i in the lth subinterval. Then, we have ∆i log(λk) =
l Ri,l and the log-likelihood can be written again

l Oi,l log(αk

l αk

0

l ),(cid:82) +∞
Yi(t)λk(t)dt =(cid:80)
(cid:110)
K(cid:88)
n(cid:88)
L(cid:88)

0

Q(θ|θold) =

wi(k; θold)

Oi,l(log(αk

l ) + X iβk) − αk

l Ri,l exp(X iβk)

.

i=1

k=1

l=1

This log-likelihood is proportional to the log-likelihood one would obtain in a Poisson
regression, where the Oi,l are the response variables and are assumed to follow, condi-
tionally on the X i, a Poisson distribution with parameter equal to αk
l Ri,l exp(X iβk).
Therefore, the estimates can easily be computed using the glm function in the R soft-
ware and specifying log(Ri,l) as “oﬀsets” in the model. See for instance Aalen et al
(2008) p.223-225 for more details on the connection between piecewise-constant hazard
model and Poisson regression. A weight option is also available in the glm function.

Finally, note that the exponential case could also be derived as a special case of the

piecewise constant hazard family with L = 1.

As mentioned earlier, the piecewise constant hazard model is very useful when one
does not know the shape of the baseline hazard a priori. However one must specify in
advance the value of L in the model. A low value of L will make the estimator imprecise
between two cut points, since the model makes the assumption that the hazard is con-
stant on these intervals. On the other hand, as the number of cut points increases the
speciﬁcation of the baseline hazard becomes more ﬂexible. But increasing the value of

9

L will also automatically increase the variance of the baseline hazard estimator. There-
fore, a too large value of L should be avoided since it might lead to overﬁtting. So a
balance should be kept between bias and variance by choosing an adequate number of
cut points. The choice of L is discussed in more details in Section 5.3.
In the next
section, the nonparametric setting where the baseline hazard function is left completely
unspeciﬁed is presented.

4.3 The nonparametric baseline hazard

In the absence of weights, this model has been widely used because of its great ﬂexibility,
the baseline hazard being estimated without making any assumption on its shape, and
because it can easily be implemented in a straightforward manner. First, the regression
parameter is estimated by maximizing the Cox partial likelihood which contains terms
involving only the regression parameter (and not the baseline hazard). Secondly, the
baseline hazard estimator is deduced by the martingale decomposition of the observed
counting process. More details on the standard estimation procedure in the Cox model
can be found for instance in Andersen et al (1993) or Fleming and Harrington (1991).
The martingale decomposition in Equation (11) suggests, for a ﬁxed βk, the following

weighted Nelson-Aalen type estimator for Λk:

(cid:90) t

n(cid:88)

i=1

0

(cid:80)

˜Λk(t, βk) =

wi(k; θold)dNi(s)

j Yj(s) exp(X jβk)wj(k; θold)

·

Now, plugging-in this quantity into Q(θ|θold) gives the following weighted Cox partial
likelihood:

QPL(β1, . . . , βK|θold) =

(cid:90) τ
n(cid:88)
K(cid:88)
X iβk + log(wi(k; θold)) − log
 n(cid:88)

k=1

i=1

0

j=1

 wi(k; θold)dNi(t).

Yj(t) exp(X jβk)wj(k; θold)

k (t, β; θold) =(cid:80)

j exp(X jβ)wj(k; θold)
k (t, β; θold). Then, on each stratum k, deﬁne the

j Yj(t)X⊗l

Introduce for k = 1, . . . , K, l = 0, 1, 2, S(l)
and Ek(t, β; θold) = S(1)
k (t, β; θold)/S(0)
score function

(cid:90) τ

n(cid:88)

such that (cid:98)βk veriﬁes the equality Uk((cid:98)βk|θold) = 0.

i=1

0

Introduce Vk(t, β; θold) = S(2)

k (t, β; θold)/S(0)

Uk(β|θold) =

{X i − Ek(t, β; θold)} wi(k; θold)dNi(t),

k (t, β; θold) − Ek(t, β; θold)⊗2 and let

Ik(t, β|θold) =

Vk(t, β; θold)wi(k; θold)dNi(t),

(cid:90) τ

n(cid:88)

i=1

0

10

The mth iteration step writes as follows:

(cid:98)β

k =(cid:98)β

(m)

(m−1)
k

+ Ik(t,(cid:98)β

(m−1)
k

|θold)−1Uk((cid:98)β

represents minus the derivative of the score function with respect to β. Then, computa-

tion of the estimator(cid:98)θ can be performed using the iterative Newton-Raphson algorithm.

(m−1)
k

|θold).

library. The weights option can be directly speciﬁed in this function.

At convergence, we get the estimator (cid:101)θ = (˜Λ1, . . . , ˜ΛK,(cid:98)β1, . . . ,(cid:98)βK) where ˜Λk(t) =
˜Λk(t,(cid:98)βk) are plug-in Nelson-Aalen estimators of the cumulative hazard functions. Note
that the (cid:101)θ estimator can be computed with the coxph function in the R survival
again, i.e. to replace θ by(cid:101)θ in the expression of the ei(k; θ). However, although this is
in ei(k;(cid:101)θ)), at a given time point is limited. To stabilize the solution, smoothing is

a relevant strategy for the parametric models it will not lead to a consistent estimator
for the Cox model. Because of the shape of the Nelson-Aalen estimators, which are
stepwise functions, the information in the estimated partial likelihood (or equivalently

Finally, as for the parametric models, computation of the new weights is done through
the EM algorithm (see Section 3). Then, a simple idea could be to use plug-in estimators

needed. In Section 5.2, new kernel type estimators of the Λks and λks are derived and
are used as plug-in estimates in order to compute the weights.

5 Practical implementation

5.1 Parametric baseline hazards

The parametric case is straightforward: the ﬁnal estimators are obtained by alternating
computation of the estimates through Equation (4) and computation of the weights
through the posterior segment distribution calculated in Section 3.

The algorithm of our estimation procedure is as follows. First suppose you have at

your disposal an initial weight function wi(k; θold).

Step 1. Compute(cid:98)θ = argmaxθ Q(θ|θold) from Equation (4). In the exponential or Weibull

models, this can be done via the survreg function in R (see Section 4.1) and in
the piecewise constant hazard model, this can be done via the glm function in R
(see Section 4.2)

Step 2. Compute the new weights wi(k;(cid:98)θ) using Equation (8) in Section 3.
Step 3. Let θold =(cid:98)θ and return to Step 1.

5.2 Nonparametric baseline hazard

The nonparametric case requires one supplementary step. After the ﬁrst step, smoothed
versions of the baseline hazard and cumulative baseline hazard estimators need to be
derived. The weighted log-likelihood and the weights are then computed using these
smoothed estimators. We propose in this work to use kernel type estimators but our

11

method could be extended to any type of smoothing estimators such as wavelets, splines,
k-nearest neighbor estimators, projection estimators etc.

Let K be a kernel such that(cid:82) K(u)du = 1, (cid:82) uK(u)du = 0, (cid:82) u2K(u)du < ∞ and
(cid:82) K2(u)du < ∞. Let h be a bandwidth satisfying h → 0 and nh → ∞ as n tends to

inﬁnity. We now introduce the smoothed versions of ˜λk and ˜Λk:

(cid:90)

n(cid:88)

(cid:18) u − t

(cid:19)

(cid:90) t

ˆλk(t) =

1
h

K

d˜Λk(u) and ˆΛk(t) =

ˆλk(s)ds,

(12)

and we note(cid:98)θ = (ˆΛ1, . . . , ˆΛK,(cid:98)β1, . . . ,(cid:98)βK). This new estimator is now used to estimate

i=1

0

h

ei(k; θ) and then to obtain estimators of the weights. From Equation (3) we have:

(cid:16)

ei(k;(cid:98)θ)

(cid:17)

(cid:16)

log(cid:0)ˆλk(Ti)(cid:1) + X i(cid:98)βk

(cid:17) − ˆΛk(Ti) exp(X i(cid:98)βk).

log

Note that the weighted likelihood Q((cid:98)θ|θold) obtained from these ei(k;(cid:98)θ) does not reduce

= ∆i

(13)

to a partial likelihood like in Section 4.3 due to the use of smoothed hazard and cumu-
lative hazard estimators. However this is not an important matter since our algorithm
does not require the maximization of this likelihood: Equation (13) is only needed for
the computation of the new weights from Equation (8) in Section 3 while the optimiza-
tion step only involves the Cox partial likelihood and is easily performed through the
Newton-Raphson algorithm.

The ﬁnal algorithm of our estimation procedure is as follows. First suppose you have

at your disposal an initial weight function wi(k; θold).

Step 1. Compute (cid:101)θ using the Newton-Raphson algorithm described in Section 4.3. This
Step 2. Smooth the ˜λk and ˜Λk using Equation (12). This gives(cid:98)θ.

can be done via the coxph function in R.

as in Equation (13) and get the new weights wi(k;(cid:98)θ) from

Step 3. Compute log

(cid:16)

(cid:17)
ei(k;(cid:98)θ)

Equation (8) in Section 3.

Step 4. Let θold =(cid:98)θ and return to Step 1.

5.3 Choice of the parameters and stopping rule to ﬁnd the correct

model

These algorithms need to be initialized by either choosing initial model parameters or
by directly choosing initial weight functions. We propose the following ad-hoc method
to initialize the weights for a sample of size n and K segments. First divide the sample
in K segments and for any individual i in segment k, choose wi(k, θold) = p with p a
high number between 0 and 1 (for instance, take p = 0.7). For any individual j that is
not in segment k, choose wj(k, θold) = 1 − p.

12

for (cid:98)β

In all models, the Newton-Raphson algorithm is initialized by taking the null vector
(0)
k . Step 2 in the parametric models and step 3 in the Cox model are performed

using the R package postCP developed by Luong et al (2013).

The exponential and Weibull baseline hazard models only require the initialization
of either the model parameters or the weights. On the opposite, the piecewise constant
baseline hazard model and the nonparametric baseline model require an extra parameter
to be chosen. In both models, the estimation procedure is not very sensitive to the choice
of this parameter, especially in terms of breakpoints detection. In particular, the number
of cut points in the piecewise constant hazard is set by default to 3 and as shown in the
simulation section, this leads to very performant breakpoints selection. Increasing the
number of cut points does usually not make the breakpoints detection more accurate.
These 3 breakpoints can be chosen for instance from the data as the quantiles of the
event times of order 0.25, 0.5 and 0.75 respectively. The same phenomena happens for
the choice of the bandwidth in the nonparametric model: detecting the correct number of
breakpoints is not much aﬀected by the choice of the bandwidth. However, it might still
be of interest to ﬁnd an optimal bandwidth if one wants to give a precise estimation of
the baseline hazard. This problem is classical for density estimation and has been studied
for nonparametric estimation of baseline hazards by Andersen et al (1993). Equations
(4.2.25) and (4.2.26) of their book suggest that a bandwidth of order n−1/5 would give the
best compromise between bias and variance trade-oﬀ in the estimation of the baseline
In particular asymptotic normality of order (nh)1/2 would be achieved with
hazard.
such a bandwidth as expressed by their theorem IV.2.4. More discussions about how
to choose the bandwidth from the data can be found in Andersen et al (1993), see in
particular their Examples IV.2.3, IV.2.4 and IV.2.5. Since the interest in the choice of
the bandwidth is limited in our context we will not pursue this discussion here but as a
rule of thumb we recommend the user to choose h = n−1/5 in real data situations.

One other important issue is to ﬁnd the correct number of breakpoints in the dataset.
A simple solution consists to start with a model with one breakpoint and increment the
number of breakpoints one by one. As presented in the real data analysis for example
(see Section 7) a visual inspection of the plots of the maximum a posteriori of the
breakpoints can help to ﬁnd the right model. However, the conclusion from theses plots
can be subjective and it is therefore important to propose a numerical indicator that
helps discriminating between diﬀerent models. We propose the following BIC criterion
designed to make a tradeoﬀ between information provided by the data on a model and
the complexity of the model:

BIC(d) = −2 log P(data| ˆθ) + d log(n)

where the likelihood P(data| ˆθ) can be computed using Equation (10), and d corresponds
to the dimension of the model. The value of d is diﬀerent for every model, it corresponds
to the total number of parameters that need to be estimated. For the exponential base-
line, d = (p + 1)K, for the Weibull baseline, d = (p + 2)K and for the piecewise constant
hazard baseline, d = (p + L)K. No such indicator can be derived for a nonparametric
baseline hazard since in that case the number of parameters to be estimated equals inﬁn-

13

ity. This BIC criterion is used in Section 7 for the exponential baseline to discriminate
between diﬀerent models and ﬁnd the correct number of breakpoints.

6 Simulated data

In this section we evaluate the performance of our estimation technique through numer-
ical experiments. We consider a Cox model as deﬁned by Equation (1), with K = 3
segments and a binary covariate X distributed as a Bernoulli variable with parameter
equal to 0.5. We consider diﬀerent scenarios corresponding to diﬀerent baseline hazards
and diﬀerent regression parameters:

Scenario 1. Exponential baselines, with λ1(t) = 1, λ2(t) = 0.5, λ3(t) = 0.7 and β1 = 1.5,

β2 = −0.5, β3 = −0.5.

Scenario 2. Weibull baselines, λ1(t) = 5t4, λ2(t) = 2t, λ3(t) = 2t and β1 = 1.5, β2 = −1,

β3 = −5.

Scenario 3. Piecewise constant baselines,

λ1(t) = 0.8 I(0 < t ≤ 1) + 1.2 I(1 < t ≤ 3) + 1.6 I(3 < t),
λ2(t) = 1.2 I(0 < t ≤ 4) + 1.6 I(4 < t ≤ 6) + 2 I(6 < t),
λ3(t) = 1.6 I(0 < t ≤ 5) + 2 I(5 < t ≤ 7) + 2.4 I(7 < t),

and β1 = 1.5, β2 = −0.5, β3 = −1.5.

Scenario 4. Gompertz baselines, λ1(t) = e5t, λ2(t) = e2t, λ3(t) = e2t and β1 = 1.5, β2 = −0.5,

β3 = −1.5.

In all three scenarios, the sample size n equals 3 000, and the data were simulated
such that R1 = ··· = R1000 = 1, R1001 = ··· = R2000 = 2 and R2001 = ··· = R3000 = 3.
Each scenario was calibrated such that the change in the hazard distribution between
segments 1 and 2 was more important than the diﬀerence in the hazard distribution
between segments 2 and 3. This is illustrated by Figure 1 which provides the plots of
the conditional hazard rates in each scenario. The censoring variable was chosen as a
uniform distribution such that approximately 50% of the observations were censored in
each scenario. In Scenario 1, the censoring was distributed as a uniform distribution
with parameters 0 and 2.4, such that 24%, 65% and 60% of individuals were respectively
censored in segments 1, 2 and 3. In Scenario 2, the censoring was distributed as a uniform
distribution with parameters 0 and 1.8, such that 33%, 47% and 67% of individuals
were respectively censored in segments 1, 2 and 3.
In Scenario 3, the censoring was
distributed as a uniform distribution with parameters 0 and 1.5, such that 38%, 54%
and 58% of individuals were respectively censored in segments 1, 2 and 3. In Scenario 4,
the censoring was distributed as a uniform distribution with parameters 0 and 0.9, such
that 23%, 58% and 67% of individuals were respectively censored in segments 1, 2 and
3. For the piecewise constant hazard model estimator, as recommended in Section 5.3,

14

the cuts positions were chosen from the quartile of the data. This lead us to take the
value 0.2, 0.5 and 1.1 for Scenario 1, 0.4, 0.7, 1 for Scenario 2, 0.15, 0.35 and 0.5 for
Scenario 3 and 0.1, 0.2 and 0.4 for Scenario 4. For the nonparametric baseline hazard
model estimator, as recommended in Section 5.3, the bandwidth was chosen equals to
3000−1/5 ≈ 0.2 in all scenarios. Finally we ran 1 000 replications of each of these scenarios
and the results were reported in Tables 1.

In all scenarios, detection of the ﬁrst breakpoint is usually very accurate where in
many cases the average breakpoint location is exactly equal to the true breakpoint lo-
cation, 1 000. The second breakpoint is more diﬃcult to detect as shown by wider
conﬁdence intervals even though the average breakpoint location is usually close to the
true breakpoint location, 2 000. The average value of the marginal probability of break-
point detections also illustrate the uncertainty about the second breakpoint location:
the probability for the ﬁrst breakpoint location is in all cases much higher than for the
second breakpoint location.

The most problematic breakpoint to ﬁnd corresponds to the breakpoint from segment
2 to 3 under Scenario 1 and as a matter of fact none of the proposed methods manage
to provide an accurate 95% conﬁdence interval. In this scenario, for every estimation
methods there was a probability of approximately 1 over 1 000 that the algorithm fails
to ﬁnd the second breakpoints leading to an error in the program.

It is interesting to notice that on the overall the true hazard distribution of the data
does not seem to play any role in the detection power of our estimation methods as long
as the change in the hazard distribution in two segments is big enough. For instance,
in Scenario 4, which involves a simulation setup that does not correspond to any of
the parametric baseline distributions proposed in the diﬀerent estimation methods, all
estimators ﬁnd very accurate breakpoint locations with very narrow conﬁdence intervals.
The estimation performance of the regression parameter does not seem to be much
aﬀected by the data simulation setup neither, since the Weibull, piecewise constant and
nonparametric baseline estimators show little diﬀerence in their estimation performance
from one scenario to another. One exception is the exponential baseline estimator which
seems to behave poorly in Scenarios 2 and 4 when looking at the regression parameter
estimates and the conﬁdence intervals for the second breakpoint compared to the other
estimators.

Globally, all estimators are performant both in breakpoint detections and parameters
estimation as long as the change in the hazard distribution is big enough from one
segment to another. In that case, the nonparametric baseline estimator seems to give
the biggest value of the probability of the breakpoint distribution. When only a slight
change occurs between the hazard distribution of two segments, all the proposed methods
are less precise and the exponential baseline estimator seems to be the less performant
of all baseline estimators.

More simulation studies which are not reported here have been carried out. When the
change in distribution between two segments increases, the probability of the marginal
breakpoint distribution increases accordingly and can be almost equal to 1 in some sit-
uations. For instance the marginal probabilities of the breakpoints found in Section 7

15

seem to indicate a much drastic change in the survival distributions than the simulation
setting presented here. Finally scenarios which mix diﬀerent parametric survival distri-
butions in each segment have also been investigated. These simulations lead to similar
behaviour of our estimators and are therefore omitted.

7 Survival analysis of diabetic patients at the Steno memo-

rial hospital

In this section we illustrate our method on a dataset on survival of diabetics patients
at the Steno memorial hospital. The data are described in great details in Example
I.3.11 in Andersen et al (1993) and were originally studied through a illness-death model
where the illness state corresponded to the diabetic nephropathy status of the patients.
Here, we will only focus our interest on the survival of the patients, that is the variable
of interest is the time from diagnostic of diabetes of a patient until death. The data
were collected between 1933 and 1981 and patients were included in the study if the
diagnosis of diabetes mellitus was established before age 31 years and between 1933 and
1972. A total of 2 709 patients were followed from the ﬁrst contact with the hospital
until death, emigration or the 31st of December 1984. On these 2 709 patients 707 (26%)
deaths were observed and the other 2 002 (74%) patients were considered right censored.
Since most of the patients did not contact the hospital directly after the diagnostic
of diabetes, patients in this dataset are also left truncated. This needs to be taken
into account because it means that individuals have a delayed entry into the study and
will be observed only if they did not die before attending the Steno hospital. Without
appropriate methods to deal with left truncation our estimation techniques will tend to
overestimate the survival of diabetics patients. Gender (coded as 0 for women and 1 for
men) and the year of birth was recorded for every patients. The dataset is composed
of approximately 56% of male and 44% of female. The years of birth range from 1903
to 1971. Our aim was to determine if there was any change in the hazard distribution
according to the date of birth when adjusting by gender. The marginal survival curves
and parameter estimates in a Cox model with exponential baseline hazard were also
computed.
To accommodate our method for left truncation the individual at risk process Yi(t)
needs to be replaced by Yi(t) = I(Li ≤ t ≤ Ti) where Li represents the left truncation
variable for individual i. This will aﬀect the value of the emission probability ei(k; θ) (see
Equation (3)) which in turn will aﬀect the value of the a posteriori segment distribution
wi(k; θ) and the value of the weighted log likelihood Q(θ|θold). The parameters are
estimated by maximizing the log likelihood in Equation (4) as before. For example, in
the exponential model, the logarithm of the emission probability is equal to:

log (ei(k; θ)) = ∆i (− log(λk) + X iβk) −(cid:16) Ti − Li

(cid:17)

exp(X iβk).

λk

Since only the year of birth is known (and not the exact date of birth) it means that
a breakpoint can only occur when changing from one year of birth to another. To take

16

17

Figure 1: Conditional hazard rates in simulated data for Scenarios 1 to 4 from top to
bottom. Solid line: hazard in segment 1. Dash line: hazard rate in segment 2. Dot line:
hazard rate in segment 3.

0.00.51.01.5012345TimeHazard rate conditionally to X=00.00.51.01.5012345TimeHazard rate conditionally to X=10.00.51.01.50246810TimeHazard rate conditionally to X=00.00.51.01.50246810TimeHazard rate conditionally to X=10246801234567TimeHazard rate conditionally to X=00246801234567TimeHazard rate conditionally to X=10.00.51.01.5051015202530TimeHazard rate conditionally to X=00.00.51.01.5051015202530TimeHazard rate conditionally to X=1Table 1: Bias, variance, MSE of ˆβ1, ˆβ2, ˆβ3 and estimations of the maximum probability of breakpoints, average
breakpoint locations along with their 95% conﬁdence intervals from the exponential baseline model, the Weibull

baseline model, the piecewise constant baseline model and the nonparametric baseline model.

Scenario 1

Bias of Variance of MSE of Max proba Mean
bp12

bp12

ˆβ

ˆβ

ˆβ

95% CI Max proba Mean
bp23

bp12

bp23

95% CI

bp23

Exponential

Weibull

Piecewise

Nonparametric

0.002
-0.002
-0.052
0.002
-0.002
-0.007
0.003
0.000
-0.066
0.002
-0.069
-0.017

0.006
0.015
0.706
0.007
0.011
0.407
0.007
0.009
0.574
0.007
0.820
2.597

0.006
0.015
0.709
0.007
0.011
0.407
0.007
0.009
0.578
0.007
0.825
2.598

0.411

1000

994-1006

0.032

2120

1662-2974

0.408

1000

994-1006

0.043

2216

1740-2981

0.402

1000

994-1006

0.069

2479

1800-2987

0.429

1001

996-1007

0.054

1954

1013-2995

Scenario 2

Bias of Variance of MSE of Max proba Mean
bp12

bp12

ˆβ

ˆβ

ˆβ

95% CI Max proba Mean
bp23

bp12

bp23

95% CI

bp23

Exponential

Weibull

Piecewise

Nonparametric

-1.207
0.512
2.737
-0.010
-0.009
-0.043
-0.187
0.031
0.007
0.000
-0.006
-0.122

0.000
0.003
0.168
0.008
0.008
0.255
0.007
0.007
0.304
0.010
0.009
0.708

1.458
0.266
7.661
0.008
0.008
0.257
0.042
0.008
0.304
0.010
0.009
0.723

0.054

998

973-1016

0.092

1943

1407-2002

0.309

1002

996-1020

0.154

1997

1978-2009

0.323

1001

995-1008

0.192

1998

1983-2011

0.332

1000

992-1008

0.195

1998

1983-2012

Scenario 3

Bias of Variance of MSE of Max proba Mean
bp12

bp12

ˆβ

ˆβ

ˆβ

95% CI Max proba Mean
bp23

bp12

bp23

95% CI

bp23

Exponential

Weibull

Piecewise

Nonparametric

-0.033
0.002
-0.007
-0.013
0.003
-0.007
-0.007
0.006
-0.005
0.002
-0.001
-0.006

0.008
0.010
0.016
0.007
0.010
0.015
0.008
0.011
0.016
0.008
0.010
0.015

0.009
0.010
0.016
0.008
0.010
0.015
0.008
0.011
0.016
0.008
0.010
0.015

0.214

1001

986-1014

0.043

1997

1854-2119

0.216

1001

986-1014

0.044

1994

1847-2111

0.217

1001

986-1014

0.046

1990

1844-2116

0.220

1002

991-1021

0.042

1997

1847-2131

Scenario 4

Bias of Variance of MSE of Max proba Mean
bp12

bp12

ˆβ

ˆβ

ˆβ

95% CI Max proba Mean
bp23

bp12

bp23

95% CI

bp23

Exponential

Weibull

Piecewise

Nonparametric

-0.639
0.196
0.575
-0.212
0.022
0.044
-0.076
0.013
0.028
0.006
-0.004
-0.023

0.002
0.020
0.035
0.005
0.010
0.017
0.007
0.010
0.019
0.008
0.011
0.165

0.410
0.058
0.366
0.050
0.010
0.019
0.013
0.011
0.020
0.008
0.011
0.165

0.238

18
0.352

1000

992-1006

0.027

1641

1015-2016

1000

994-1006

0.049

1994

1899-2079

0.378

1000

994-1006

0.051

1989

1862-2080

0.420

1000

991-1006

0.049

2009

1928-2137

this into account we ﬁrst ordered all individuals with respect to their date of birth and
the computation of the posterior distribution was constrained through the priors ηi(k),
deﬁned in Section 3, such that ηi(k) = 0 for any k if individuals i and i + 1 were born
the same year. Other priors were set to 0.5. Since 0 is an absorbing state this ensured
us to have change-points only for a new birth cohort.

The exponential model was ﬁrst computed for one breakpoint, then we increased
the numbers of breakpoints. The BIC criterion introduced in Section 5.3 was used as a
stopping rule to ﬁnd the correct number of breakpoints.

The maximum a posteriori of the breakpoints have been computed in Figure 2. For
example, from the model with only one breakpoint it seems that the survival of diabetics
patients was diﬀerent for individuals born (strictly) before 1936 than for individuals born
after 1936 (including the year 1936) with a probability of having a breakpoint equal to
88%. In each ﬁgure the peaks of the distribution of the breakpoint is high except for the
model with four breakpoints where the probability of having a breakpoint in 1921 is equal
to 27% while, in the same model, the probability of having a breakpoint in 1930, 1936
and 1947 is equal respectively to 63%, 61% and 67%. The same last three breakpoints
were found in the model with three breakpoints with respective probabilities equal this
time to 79%, 65% and 67%. All these results seem to indicate that the correct model
is the three breakpoints model. This is conﬁrmed by the value of the BIC criterion in
each model in Table 2 where the lowest BIC value is obtained for the three breakpoints
model.

In Table 2, parameter estimates for the Cox model with exponential value have been
computed with gender as a covariate. The baseline values are slightly decreasing with
respect to the birth cohort in the sense that men and women born at a latter birth period
have a smaller hazard of death than individuals born at a earlier birth period. For the
three breakpoints model, the eﬀect of gender is positively associated to the hazard before
1930 (the regression parameter is equal to 0.345 and the hazard rate equals 1.41) and
between 1936 and 1947 (the regression parameter is equal to 0.356 and the hazard rate
equals 1.43) while the eﬀect is nearly null for other periods (the hazard rate is equal to
0.95 for both other periods).

Non parametric survival estimates have been computed using a weighted Kaplan-
Meier estimator in Figure 3. The curves show a clear increase in the survival of patients
according to the year of birth. Patients born at a latter year have a greater survival
than patients born at a earlier period at any age. For example, in the three breakpoints
model, the survival 30 years after diagnoses of diabetes is equal to 53.2%, 62.4%, 72.5%
and 85.1% for the respective birth cohorts 1903 − 1929, 1930 − 1935, 1936 − 1946 and
1947 − 1971.

The dataset has also been studied for the exponential model without adjusting by
gender. The same breakpoints were found using the BIC criterion and the hazard and
survival estimates were nearly identical.

19

Table 2: λ’s and β’s estimates in the Cox model adjusted by gender with exponen-
tial baseline for the models with one, two, three and four breakpoints along with BIC
criterion for each model.

One bp

1936

Two bp
1934, 47

Three bp

Four bp

1930, 36, 47

1921, 30, 36, 47

0.020
0.006

ˆλ1
ˆλ2
ˆλ3
ˆλ4
ˆλ5
ˆβ1
ˆβ2
ˆβ3
ˆβ4
ˆβ5
BIC 7342.539

0.213
0.289

0.020
0.009
0.004

0.260
0.286
-0.034

0.021
0.017
0.008
0.004

0.345
-0.053
0.356
-0.056

0.022
0.020
0.017
0.008
0.004
0.426
0.240
-0.050
0.359
-0.057

7307.425

7306.889

7314.102

8 Discussion

In this article we proposed a new way to deal with cohort studies through a change-
point model. In this model we suppose that abrupt changes can occur in the survival
distribution of the event time. More speciﬁcally after specifying the number of segments,
either the baseline hazard rates or the regression parameters are allowed to change in
the diﬀerent segments. Estimation in such a model is performed by an EM algorithm
with use of constrained Hidden Markov Model (HMM) method as recently suggested
by Luong et al (2013). The method proposes diﬀerent speciﬁcations of the baseline and
as shown by the simulation study, all diﬀerent models provide both accurate estimates
and accurate breakpoint locations. The method was also shown to adapt to more realistic
problematics such as left truncation on the Steno memorial hospital dataset. Clearly,
the methods developed here could be extended to a more complex setting. In particular,
handling time dependent covariates or applying the method to recurrent events could
be straightforward extensions. Also, the methodology should be directly applicable to
other survival models such as the Accelerated Failure Time Model (see Kalbﬂeisch and
Prentice, 2002; Wei, 1992) or the Aalen model (see Aalen, 1980; Scheike, 2002).

Parametric baseline models suﬀer from less ﬂexibility compared to the nonparametric
but as shown in the simulation study, they still detect the location breakpoints very
accurately and provide very eﬃcient estimates of the regression parameters even when
the true dataset does not belong to the model estimator. These estimators also share
the advantage to be parametric and consequently a BIC criterion could be derived. This
is a very useful tool in order to ﬁnd the correct number of breakpoints. This feature of
parametric models was illustrated on the Steno memorial hospital dataset where it was

20

Figure 2: Marginal distributions of the breakpoints in the models with one, two, three
and four breakpoints. The maximum a posteriori for the breakpoints are respectively:
top left 1936, top right 1934 and 1947, bottom left 1930, 1936 and 1947, bottom right
1921, 1930, 1936 and 1947.

found that the three breakpoints model seems to be the correct model. On the other
hand, no BIC criterion could be derived for the nonparametric model but as mentioned
in the simulation study, this model gives the biggest value of the probability of the
breakpoint distribution and consequently seems to be the most performant model to
detect a change-point in the hazard distribution.
Acknowledgements The authors are very grateful to Professor Per Kragh Andersen
for his valuable comments and for sharing with us the Steno memorial hospital dataset.
This work is part of the DECURION project which was funded both by the IRESP and
the french “Ligue nationale contre le Cancer.

21

102030405060700.00.20.40.60.8Year of birth (20th century)Probability of breakpoint102030405060700.00.20.40.60.8Year of birth (20th century)Probability of breakpoint102030405060700.00.20.40.60.8Year of birth (20th century)Probability of breakpoint102030405060700.00.20.40.60.8Year of birth (20th century)Probability of breakpointFigure 3: Weighted Kaplan-Meier estimators in the models with one, two and three
breakpoints.

References

Aalen O (1980) A model for nonparametric regression analysis of counting processes.
In: Lecture Notes in Statistics-2: Mathematical Statistics and Probability Theory,
Springer-Verlag, New York, pp 1–25

Aalen OO, Borgan Ø, Gjessing HK (2008) Survival and Event History Analysis. Statistics

for Biology and Health, Springer Science

Andersen PK, Borgan Ø, Gill RD, Keiding N (1993) Statistical models based on counting

processes. Springer Series in Statistics, Springer-Verlag, New York

Bergh T, Ericson A, Hillensj¨o T, Nygren KG, Wennerholm UB (1999) Deliveries and
children born after in-vitro fertilisation in sweden 1982-95: a retrospective cohort
study. Lancet 354(9190):1579–85

Breslow NE (1972) Discussion of the paper by D. R. Cox. Journal of the Royal Statistical

Society Series B 34(2):187–220

22

010203040500.00.20.40.60.81.0Time since diagnosis of diabetesSurvival probability1903−19351936−1971010203040500.00.20.40.60.81.0Time since diagnosis of diabetesSurvival probability1903−19331934−19461947−1971010203040500.00.20.40.60.81.0Time since diagnosis of diabetesSurvival probability1903−19291930−19351936−19461947−1971010203040500.00.20.40.60.81.0Time since diagnosis of diabetesSurvival probability1903−19201921−19291930−19351936−19461947−1971Cox DR, Society S, Methodological SB (1972) Regression Models and Life-Tables. Jour-

nal of the Royal Statistical Society Series B 34(2):187–220

Dempster AP, Laird NM, Rubin DB (1977) Maximum likelihood from incomplete data

via the EM algorithm. Journal of the Royal Statistical Society Series B 39(1):1–38

Fleming TR, Harrington DP (1991) Counting processes and survival analysis. Wiley
Series in Probability and Mathematical Statistics: Applied Probability and Statistics,
John Wiley & Sons Inc., New York

Kalbﬂeisch JD, Prentice RL (2002) The statistical analysis of failure time data, 2nd edn.
Wiley Series in Probability and Statistics, Wiley-Interscience (John Wiley & Sons),
Hoboken, NJ

Kratz MH (2011) Multivariable Analysis: A Practical Guide for Clinicians and Public

Health Researchers, 3rd edn. Cambridge University Press

Luong TM, Rozenholc Y, Nuel G (2013) Fast estimation of posterior probabilities in
change-point analysis through a constrained hidden markov model. Computational
Statistics and Data Analysis 68:129–140

Martinussen T, Scheike TH (2006) Dynamic regression models for survival data. Statis-

tics for Biology and Health, Springer, New York

Scheike TH (2002) The additive nonparametric and semiparametric Aalen model as the

rate function for a counting process. Lifetime Data Analysis 8(3):247–262

Therneau TM, Grambsch PM (2000) Modeling survival data: extending the Cox model.

Statistics for Biology and Health, Springer-Verlag, New York

Wei LJ (1992) The accelerated failure time model : a useful alternative to the cox

regression. Statistics in medicine 11:1871–1879

Yang Y, Land KC (2013) Age-Period-Cohort Analysis. Interdisciplinary Statistics, Chap-

man et Hall

23

