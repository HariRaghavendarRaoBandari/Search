CoinCalc – A new R package for quantifying simultaneities of event series

Jonatan F. Siegmunda,b, Nicole Siegmundb,c,d, Reik V. Donnera

aResearch Domain IV – Transdisciplinary Concepts and Methods, Potsdam Institute for Climate Impact Research, Telegrafenberg A31,

bInstitute of Earth and Environmental Science, University of Potsdam, Karl-Liebknecht-Straße 24-25, 14476 Potsdam-Golm, Germany

cLeibniz Centre for Agricultural Landscape Research, Department for Soil Landscape Reseach, Eberswalder Straße 84, 15374 M¨uncheberg,

14473 Potsdam, Germany

dInstitute of Meteorology and Climate Research, Atmospheric Environmental Research (IMK-IFU), Karlsruhe Institute of Technology,

Kreuzeckbahnstraße 19, 82467 Garmisch-Partenkirchen, Germany

Germany

6
1
0
2

 
r
a

 

M
6
1

 
 
]

O
C

.
t
a
t
s
[
 
 

1
v
8
3
0
5
0

.

3
0
6
1
:
v
i
X
r
a

Abstract

We present the new R package CoinCalc for performing event coincidence analysis (ECA), a novel statistical method to
quantify the simultaneity of events contained in two series of observations, either as simultaneous or lagged coincidences
within a user-speciﬁc temporal tolerance window. The package also provides diﬀerent analytical as well as surrogate-
based signiﬁcance tests (valid under diﬀerent assumptions about the nature of the observed event series) as well as
an intuitive visualization of the identiﬁed coincidences. We demonstrate the usage of CoinCalc based on two typical
geoscientiﬁc example problems addressing the relationship between meteorological extremes and plant phenology as well
as that between soil properties and land cover.

Keywords: event coincidence analysis, R, point processes, extreme events, time series analysis

1. Introduction

In many areas of geosciences, but also other scientiﬁc
disciplines like neurosciences, there has been a rising in-
terest in inferring information on dynamical interdepen-
dencies between diﬀerent observational series that are not
given in the form of continuous or discrete-valued time se-
ries, but as sequences of events (e.g., unmarked or marked
point processes). Traditional statistical tools like classical
(Pearson) correlation analysis are often not directly appli-
cable to such series or of limited explanatory value. While
in neurosciences, many methodological developments have
been introduced and subsequently applied for studying the
statistical interrelationships between event series (e.g., de-
scribing sequences of neuronal spiking activity [4, 13]),
there have been relatively few attempts to transfer cor-
responding approaches to geoscientiﬁc problems [2, 14].

Event coincidence analysis (ECA) is a recently devel-
oped method for studying the statistical interdependency
between two event series, which has been originally intro-
duced and applied in a geoscientiﬁc context [7, 8, 16, 18].
Unlike correlation analysis, this method exclusively takes
the timings of certain well-deﬁned events in two series into
account and ignores potentially available other informa-
tion (e.g., underlying explicit time series values) on the
gradual variability of related observables. Therefore, it
provides a complementary view on data that are either by

Email address: jonatan.siegmund@pik-potsdam.de (Jonatan

F. Siegmund)

deﬁnition of binary structure (event/no event) or where
only certain values (e.g., extreme events) are expected to
result in a speciﬁc response of interest. Examples include
the timings of natural disasters like earthquakes or ﬂoods
[8] or cases where strong deviations from “normal” behav-
ior can result in qualitatively diﬀerent interdependencies
between the variables of interest (e.g., ecosystem responses
to extreme environmental conditions like droughts, cold
spells or volcanic eruptions) [17, 22].

So far, ECA has been successfully applied to study-
ing problems in biogeoscientiﬁc [16, 18], socio-ecological
[8] and paleoclimatic contexts [7]. The diversity of re-
search questions discussed in the aforementioned publica-
tions suggests a wide range of possible future applications.
While Rammig et al. [16] and Siegmund et al. [18] used the
approach to derive complementary information (beyond
classical correlation analysis) by looking at the timing of
extreme events, the analyses of Donges et al. [7, 8] could
not have been conducted using standard tools of classical
statistics since they addressed series of explicit events.

This paper introduces CoinCalc, an easy-to-handle im-
plementation of ECA in the open statistical software R,
which is available via the Comprehensive R Archive Net-
work (CRAN, www.r-project.org). We emphasize that
the CRAN repository already contains the package CNA for
performing an entirely diﬀerent type of analysis referred to
as coincidence analysis [1], and that the same term is also
used in particle physics [21] in yet another diﬀerent con-
text. Within the framework of CoinCalc, we exclusively
refer to the deﬁnition of event coincidence analysis as com-

Preprint submitted to Computers and Geosciences

March 17, 2016

prehensively described by Donges et al. [8].

The remainder of this paper is organized as follows: In
Sect. 2, the methodological background of ECA is pro-
vided, followed by a detailed description of the functions
provided by CoinCalc and their options in Sect. 3. Fi-
nally, two exemplary applications of the package to diﬀer-
ent geoscientiﬁc data sets are discussed in Sect. 4. The
paper concludes with a short summary in Sect. 5.

such a presumed directional inﬂuence cannot be postu-
lated in advance – in this case, ECA can be utilized as
an explanatory rather than conﬁrmatory statistical tool
to test for the existence, direction and signiﬁcance of such
relationships.

Following this conceptual idea, ECA distinguishes be-

tween the precursor coincidence rate

2. Methodological Background

rp(∆T, τ ) =

1[0,∆T ]((tA

i − τ ) − tB
j )

NA(cid:88)

i=1

Θ

1
NA

 NB(cid:88)
(cid:32) NA(cid:88)

j=1

NB(cid:88)

Θ

j=1

i=1

 (1)
(cid:33)

,

2.1. Event series with continuous and discrete event times

Let us consider two sequences of events of distinct types
A and B that occur at times tA
j with i = 1, . . . , NA
and j = 1, . . . , NB, where NA and NB are the number of
events of type A and B, respectively. Here, we exclusively
focus on the timing of events and disregard any possibly
available information on the magnitudes of these events.

i and tB

Depending on the speciﬁc question under study, event
series can be given in terms of two generic data types corre-
sponding to either continuous or discrete timings of events.
On the one hand, we may have just a list of event times
(e.g., in [7]) with no continuously observed data between
these events. We will refer to this type of data as event se-
quences in the following. On the other hand, we may have
the situation of a time series containing time-discrete ob-
servations of a possibly continuously-valued variable upon
which events are deﬁned. The resulting event time series is
conveniently represented by a binary sequence of a length
T equal to the number of underlying observations, where
entries 1 (0) correspond to time steps with (without) an
event.

2.2. Counting Coincidences

ECA essentially counts how often events occur in both
series simultaneously (referred to as coincidences). The
notion of simultaneity can be further speciﬁed by consid-
ering two parameters: a user-deﬁned time lag τ and a cer-
tain tolerance window ∆T . The consideration of τ (cid:54)= 0 can
be important in order to study lagged responses of events
of type A to such of type B (or vice versa) as known for,
e.g., the energy exchange between hydrosphere and atmo-
sphere [9, 11, 19] or various ecological systems [3, 5, 12]. In
turn, ∆T allows addressing uncertain timings of events in
examples like climate reconstructions [10, 20], archaeolog-
ical, paleontological or paleoanthropological records [7], or
events with an extended duration like climate regime shifts
and ecological or social responses to natural disasters [7, 8].
By deﬁnition, the notion of event coincidence is not sym-
metric, i.e., always takes one of the two event series as a
reference to which the second is compared. Commonly, in
this context events of type B are considered as possibly
inﬂuencing the timings of events of type A, and not vice
versa (of course, the roles of both variables might be in-
terchanged). However, there might be applications where

and the trigger coincidence rate

rt(∆T, τ ) =

1
NB

1[0,∆T ]((tA

i − τ ) − tB
j )

(2)
where Θ(·) is the Heaviside function and 1[0,∆T ] is the in-
dicator function of the interval [0, ∆T ]. For ∆T = 0, the
i − τ, tB
term in the inner sum will just collapse to δ(tA
j ),
where δ(·,·) is the Kronecker delta, providing a value of 1
if and only if both arguments are equal, and zero other-
wise. In this context, rp(∆T, τ ) denotes “the fraction of
A-type events that are preceded by at least one B-type
event”, while rt(∆T, τ ) measures “the fraction of B-type
events that are followed by at least one A-type event” [8].
By deﬁnition, both coincidence rates can only take values
between 0 (complete absence of coincidences) and 1 (all
events coincide with events in the reference series).

2.3. Signiﬁcance Tests

Beyond the sole calculation of coincidence rates,
CoinCalc currently provides three signiﬁcance tests that
can be selected in order to comply with the speciﬁc prop-
erties of the event series under study.

2.3.1. Analytical Test: Poissonian approximation

Under the assumption that the A and B-type events
are randomly distributed and mutually independent (i.e.,
follow two independent Poisson processes) and suﬃciently
rare, the probability of observing a given number of pre-
cursor coincidences Kp = NA · rp can be approximated by
a binomial distribution as

P (Kp) =

(cid:18)

(cid:18) NA

Kp

(cid:19)(cid:32)
(cid:32)(cid:18)

×

(cid:19)NB(cid:33)Kp
(cid:19)NB(cid:33)NA−Kp

1 −

1 − T OL
T − τ

1 − T OL
T − τ

×

.

(3)

Here, all time values are given either in absolute time units
(for event sequences) or as discrete numbers of time steps
(for event time series). Accordingly, we have T OL = ∆T
for event sequences and T OL = ∆T + 1 for event time

2

series. In a similar way, T denotes either the total time
span of observations (for event sequences) or the number of
observations (for event time series). Note that while T OL
is a non-negative parameter that can be selected according
to the speciﬁc problem under study, T is itself part of the
necessary information on the event series under study that
needs to be known in order to perform ECA.

The p-value of the corresponding analytical signiﬁcance
test provided by CoinCalc corresponds to the probability
that Kp or more coincidences occur due to chance accord-
ing to Eq. (3),

(cid:88)

p≥Kp
K(cid:48)

pKp =

P (K(cid:48)
p),

(4)

where Kp is the number of precursor coincidences obtained
when comparing the empirically found event sequences A
and B. The p-value for the corresponding signiﬁcance test
of the trigger coincidence rate rt = Kt/NB is obtained in
the same way by interchanging NA and NB in Eq. (3) and
replacing Kp by Kt [8]. In both cases, the null hypothesis
of the test is that the observed number of coincidences
can be explained by two independent series of randomly
distributed events. If the given p-value is smaller than a
user-deﬁned conﬁdence level α, this null hypothesis can be
rejected.

2.3.2. Surrogates with random event times: Shuﬄe test

A rejection resulting from the analytical signiﬁcance test
described above can have two possible implications: either
the two event series are not independent of each other (in
most cases the desired type of information), or the ana-
lytical approximation does not hold. The latter problem
appears, for example, if the number of events is too large
in comparison to the full window of observations and the
associated sampling interval, i.e., if the events cannot be
considered rare [8]. This is the case if the implicit condi-
tions NA∆T, NB∆T (cid:28) T – under which the deﬁnition of
events is meaningful – are violated.

In order to cope with data sets of the latter type,
CoinCalc provides a second signiﬁcance test based on the
generation of an ensemble of surrogate event series where
only the numbers of events in both series (NA and NB)
are prescribed and the actual event times are selected uni-
formly at random from the time interval of observations.
By construction, in the limit T → ∞, the waiting time
distribution of such surrogate event series would be ex-
ponential corresponding again to a Poisson process. The
empirical distribution ˆPsurr(Kp) of the resulting surrogate
precursor coincidence rates from all pairs of these “shuf-
ﬂed” surrogate event series approximates the distribution
of coincidence rates that would result from two event series
with the same length and number of events as the original
data sets where the individual events are completely inde-
pendent of each other. Hence, the p-value for the precursor
coincidence rate can be approximated as

3

pKp = 1 − ˆPsurr(Kp).

(5)

For trigger coincidences, pKt follows in full analogy.

2.3.3. Surrogates with prescribed waiting time distribu-

tions

Besides the too large number of events in a series, an-
other possible reason for the analytical signiﬁcance test to
provide incorrect results is that the empirical distribution
of waiting times between subsequent events can show devi-
ations from the exponential behavior expected for Poisson
processes. For such cases, CoinCalc provides a third sig-
niﬁcance test based on another type of surrogate event
series that resemble the original data sets in pertaining
their series length and waiting time distributions. Speciﬁ-
cally, each surrogate event series is produced by iteratively
selecting the waiting time until the next event from the
empirical distribution of waiting times of the original data
sets uniformly at random. The calculation of p-values then
follows the same strategy as described above for the shuﬄe
surrogates.

Unlike analytical and shuﬄe tests, the latter surrogate-
based test does not make any assumption about the Pois-
sonian nature of the event series and is insofar more gen-
erally applicable and, hence, less restrictive. Moreover, as
the shuﬄe test, it allows to perform ECA for event series
that do not match the condition of rare events. However,
it still provides only approximate results for cases in which
there is evidence for correlations between the events of one
of the series. This situation requires the utilization of nu-
merical approximations of the distribution of the consid-
ered test statistics making use of even more sophisticated
resampling approaches [8]. A corresponding extension of
the currently implemented signiﬁcance tests is scheduled
for future versions of CoinCalc.

2.4. Symmetric Tolerance Windows

Extending upon the methodological setting used by
Donges et al. [8], CoinCalc also provides the possibility
to deﬁne a symmetric tolerance window. The deﬁnition
of such a window is of speciﬁc interest for analyses in
which there is evidence for uncertainties in the timing of
the events in one or both data sets. In Eqs. (1) and (2) dis-
cussed above, ∆T is supposed to deﬁne a non-symmetric
window, thus either preceding or following the time step of
i −τ ]
i −τ−∆T, tA
interest, resulting in tolerance windows [tA
for a precursor coincidence and [tB
j + τ, tB
j + τ + ∆T ] for a
trigger coincidence.

In turn, for τ = 0, a symmetric tolerance window corre-
sponds to counting coincidences of events in series B falling
into a time window including a time interval ∆T both be-
fore and after an event in series A. Here, the deﬁnition of
the time window using this approach is [tA
i + ∆T ]
(i.e., the symmetric tolerance windows are twice as large
as their directional counterparts discussed above). For in-
tervals centered around tB
j and for delayed coincidences

i − ∆T, tA

with τ (cid:54)= 0, the corresponding modiﬁcations are straight-
forward. Hence, Eq. (1) (and similarly Eq. (2)) can be
rewritten as

 .

1[−∆T,∆T ](tA

i − tB
j )

(6)

 NB(cid:88)

NA(cid:88)

Θ

rp(∆T, τ ) =

1
NA

i=1

j=1

As a consequence, for the calculation of P (Kp) in Eq. (3)
with a symmetric tolerance window, we have T OL = 2∆T
for event sequences and T OL = 2∆T + 1 for event time
series.

3. Description of the package

The R package CoinCalc provides all necessary func-
tionality to perform the calculation of coincidence rates
and the associated signiﬁcance tests according to the user’s
speciﬁc requirements. Speciﬁcally, it is possible to perform
ECA for both event sequences (es format) and event time
series (ts format).

To illustrate these two diﬀerent data types,

let us
suppose a set of 15 equidistant observations, where
the 4th, 6th, 7th and 11th values correspond to
events. The corresponding event time series would read
(0,0,0,1,0,1,1,0,0,0,1,0,0,0,0), whereas the asso-
ciated event sequence object would be {(4,6,7,11),
span(1,15)}, i.e., an object containing a list of event times
as well as a vector of length 2 with the start and end point
of observations. Note that the latter vector is essential for
performing the signiﬁcance tests and, hence, meaningfully
interpreting the obtained coincidence rates.

Notably, the consideration of the es format is particu-
larly useful for large data sets, since the resulting compu-
tational demands are considerably lower than for ts data.
In turn, the disadvantage of the es format is that in the
current implementation of CoinCalc, the data set must
be based on continuous observations with no missing peri-
ods of recording. In the case of missing observations, the
ts format should be used instead. In general, performing
ECA between two data sets requires the corresponding
data being given in the same format. For this purpose,
CoinCalc provides functions for transforming each of the
two formats into the other.

In the following, we give a brief overview on the
functions currently provided by the package as well as
their usage and possible options:

CC.binarize(): This function binarizes a numerical vec-
tor (i.e., a time series of an arbitrarily distributed variable)
using a given threshold. This threshold can either be a per-
centile of the variable’s empirical distribution or a speciﬁc
prescribed value. The output object has ts format (i.e., is
a binary vector of the same length T as the original data).
The following arguments and options need to be provided:
• data: Numerical vector (time series) to be binarized.

• ev.def:
String specifying the event deﬁnition
method.
If ev.def="percentile" (default), events
are deﬁned using the value of thres (see below) as
percentile threshold. If ev.def="absolute", events
are deﬁned according to an absolute threshold value
thres.
• thres:

If
thres must be a real
ev.def="percentile",
number within [0,1]. For ev.def="absolute", it can
take any real number within the range of data.

Binarization

threshold.

• event: String specifying whether values "higher"
(default) or "lower" than thres are to be considered
as events.

CC.ts2es(): This function converts an event time series
(ts format) into an event sequence (es format).
It has
only a single argument:

• data: Binary event time series to be transformed into

an event sequence.

CC.es2ts(): This function converts an event sequence (es
format) object into a binary event time series (ts format).
It requires the following arguments:

• data: Event sequence comprising event positions to

be transformed into an event time series.

• span: Numerical vector with two elements (span[1]:
starting point of the data set, span[2]: end point of
data set).

• es.round: Number of digits for rounding the given
values in data. es.round additionally deﬁnes the
temporal resolution (e.g., es.round=3 leads to a sam-
pling interval of 0.001 time units).

CC.eca.es(): This function performs the actual ECA us-
ing two event sequences (es format). The arguments and
options are listed below:

• seriesA: Numerical vector specifying the timings of

events of type A.

• seriesB: Numerical vector specifying the timings of

events of type B.

• spanA: Numerical vector of length 2 specifying the
start and end points of the data set given in seriesA.
• spanB: Numerical vector of length 2 specifying the
start and end points of the data set given in seriesB.
• delT: Non-negative real number for deﬁning the tol-
erance window ∆T . If delT=0 (default), only simul-
taneous coincidences are counted.

• sym: Boolean variable (default: FALSE) specifying if
the temporal tolerance window should be taken sym-
metrically or not.

4

• tau: Non-negative real number (default: 0) specifying

the time lag τ .

• sigtest: String specifying the type of signiﬁcance
test.
If sigtest="poisson" (default), the analyti-
cal signiﬁcance test based on the assumption of inde-
pendent and sparse Poisson processes is performed.
If sigtest="shuffle", the test statistics for ran-
domly located events is numerically approximated. If
sigtest="surrogate", the numerical approximation
of the test statistics based on coincidence rates for an
ensemble of surrogate event sequences with the same
waiting time distributions as the original data is uti-
lized.

• reps: Positive integer (default: 1000) specifying the
surrogate ensemble size for the numerical signiﬁcance
test.

• alpha: Desired conﬁdence level (default: α=0.05) for

the signiﬁcance test speciﬁed by sigtest.

CC.eca.ts(): This function performs ECA using two
event time series (ts format). Missing values are allowed.
If NAs are found in one of the input time series, the cor-
responding time steps (as well as their respective counter-
parts in the second series) are ignored in the performed
analysis. The following arguments and options are re-
quired:

• seriesA: Binary vector specifying steps with and

without events in series A.

• seriesB: Binary vector specifying steps with and

without events in series B.

• delT: Non-negative integer (default: 0) for deﬁning
the tolerance window ∆T (number of time steps ac-
cepted as time diﬀerence). For delT=0, only simulta-
neous events are counted as coincidences.

• sym: see CC.eca.es()
• tau: Non-negative integer (default: 0) specifying the

time lag τ .

• sigtest: see CC.eca.es()
• reps: see CC.eca.es()
• alpha: see CC.eca.es()

CC.plot(): This function creates a visualization of the
event series and the results of ECA. The output graph-
ics displays which individual events correspond to coin-
cidences. CC.plot() is currently only available for data
given as event time series (ts format); an extension to
general event sequences is planned for a future version of
CoinCalc. The arguments and options are listed below:

• seriesA: see CC.eca.ts()

5

• seriesB: see CC.eca.ts()
• delT: see CC.eca.ts()
• sym: see CC.eca.ts()
• tau: see CC.eca.ts()
• dates: Vector of length T , containing characters or
numerical values (default = NA) providing date infor-
mation for the two series. If speciﬁed, event dates are
added to the plot.

One example of an illustrative visualization produced by
CC.plot() can be found in Sect. 4

4. Examples

4.1. Plant phenology and meteorological extremes

As a ﬁrst example for the application of the CoinCalc
package, let us reconsider the problem studied by Sieg-
mund et al. [18], where ECA was used to identify time win-
dows during the year within which unusually warm (cold)
weather conditions can result in (i.e., coincide with) very
early (late) ﬂowering of central European shrub species in
the same year. The information on ﬂowering dates was
provided by the German Weather Service (DWD) [6], and
the temperature data were obtained by an area-weighted
interpolation of temperature data from DWD-operated
meteorological stations [15].

Here, we illustrate the utilization of CoinCalc taking
just one time series of annual Lilac ﬂowering dates and
one April mean temperature time series from 1950 to
2010 as an example. Speciﬁcally, we use the data from
a phenological station located in Niederrimbach, Germany
(49.4833◦N, 10.000◦E). Events in the two considered time
series correspond to “very early ﬂowering” and “very warm
conditions”, respectively. While the former is deﬁned as
a ﬂowering occurring earlier in the year than the empiri-
cal 10th percentile of all historical ﬂowering dates in this
record, a mean April temperature is considered to be very
warm if it exceeds the 90th percentile of all observed April
values. Figure 1 shows the two time series as well as the
two thresholds for the deﬁnition of events.

In order to conduct ECA for these two time series using
CoinCalc, let the ﬂowering data be stored in the vector
Fl.60y and the temperature data in the vector TT.60y.
Since a lagged eﬀect of spring temperatures on ﬂowering
time in a yearly resolved data set is not expected, τ and
∆T take their default values of 0. Thus, executing

Fl.60y.bin <- CC.binarize(data=Fl.60y, ...
... ev.def="percentile", ...
... thres=0.10, event="lower")
TT.60y.bin <- CC.binarize(data=TT.60y, ...
... ev.def="percentile", ...
... thres=0.90, event="higher")
ca.out <- CC.eca.ts(Fl.60y.bin, TT.60y.bin,...
...sigtest="poisson")

where FL.dates is a vector containing a sequence from
1951 to 2010. The light blue and light red bars indi-
cate events in the phenological (A) and temperature (B)
time series, respectively, where dark colors highlight those
events that correspond to coincidences. It can be seen that
in the present example, three of the six early ﬂowering
events (series A) coincide with warm April temperatures
(series B) (and vice versa), yielding precursor and trigger
coincidence rates of 0.5 as already given above.

4.2. Soil organic carbon content and land-use

Our second example illustrates the application of
CoinCalc to a quite diﬀerent problem. Here, ECA is not
used for studying two time series, but a set of soil samples
providing information about the soil organic carbon (SOC)
contents of 218 smallhold farmer sites in the Nyando dis-
trict, Western Kenya. The samples were collected, pro-
cessed and analyzed in 2013/2014 in the course of the
project SAMPLES (http://samples.ccafs.cgiar.org).
In addition to the carbon data set, information on the crop
types planted on the sampled plot was collected. During
ﬁeld work, two hypotheses were drawn: (i) The plantation
of certain crop types (e.g., sorghum) generally leads to
very low SOC contents. (ii) Intensive intercropping (i.e.,
the simultaneous plantation of diﬀerent crop types) gen-
erally leads to very high SOC rates in the top soil layer
(0–20 cm).

To test hypothesis (i), carbon contents below the 5th
percentile of all samples were deﬁned as events in the SOC
data set (event time series carb), and the cultivation of
sorghum was deﬁned as an event in the crop cover data
set (event time series crop.sorghum).
In this example,
both τ and ∆T are again 0, i.e., we are only interested in
“simultaneous” events. Although the given data sets are
no time series, the appropriate data format to choose here
is that of event time series (ts) where individual samples
take the role of (temporal) observation points. Executing

ca.out <- CC.eca.ts(carb,crop.sorghum,...
... sigtest="poisson")

yields the following results contained in the list ca.out:

NH precursor: FALSE
NH trigger: FALSE
p-value precursor: 0.03824319
p-value trigger: 0.04147892
precursor coincidence rate: 0.2727273
trigger coincidence rate: 0.1875

Hence, at the α = 0.05 signiﬁcance level, both null hy-
potheses can be rejected and, thus, a non-random statisti-
cal relationship between sorghum plantation and very low
SOC values can be deduced. Note that in contrast to the
previous example, precursor and trigger coincidence rates
are not equal due to the diﬀerent numbers of events in both
data sets. Here, the precursor coindicence rate corresponds
to the fraction of plots with low SOC contents on which

6

Figure 1: Time series of Lilac ﬂowering (Julian Day (JD) of the
year, upper panel) and April mean temperature (lower panel) in
Niederrimbach from 1951 to 2010. The red dashed lines mark the
thresholds at the empirical 10th and 90th percentiles, respectively.
Events in both time series are deﬁned as those values, that are lower
(higher) than the respective threshold.

results in the list ca.out containing the following informa-
tion:

NH precursor: FALSE
NH trigger: FALSE
p-value precursor: 0.01777557
p-value trigger: 0.01777557
precursor coincidence rate: 0.5
trigger coincidence rate: 0.5

In this example, the null hypotheses of independent ran-
dom event series can be rejected for both precursor and
trigger coincidence rates at a conﬁdence level of α = 0.05.
In the speciﬁc setting considered here, the two coincidence
rates and their resulting p-values are identical, since both
time series have the same length, no tolerance window
is considered (∆T = 0) and the same number of events
NA = NB is present in both series due to their deﬁnition
using the empirical 10th and 90th percentile, respectively.
It may be worth noting that in the present example, the
correlation coeﬃcient between both original time series is
already −0.83, suggesting the existence of a strong cor-
relation between the two considered variables. However,
such a strong correlation does not necessarily imply the
co-occurrence of extreme values in both records, since cor-
relations take all parts of the distribution into account.
Hence, even with a generally strong correlation, certain
parts of the distribution of the two observables can still
completely mismatch in terms of their appearance in the
series. Thus, ECA is a prospective complementary tool
providing information to understand the relationship be-
tween distinct parts of the distribution of two time series.
Figure 2 shows the plot generated by the function call

CC.plot(Fl_60y.bin,TT.60y.bin,dates=FL.dates)

1001101201301401501955196019651970197519801985199019952000200520105678910111213195519601965197019751980198519901995200020052010Time [AD] Flowering Date [JD]April Temperature [°C]Figure 2: Graphical output of the CC.plot function for the ﬂowering (red) and temperature time series (blue). Blue and red bars mark time
steps with events. For the series to be compared with the respective reference sequence, light colors show events without coincidence and
dark colors coincidences. For the reference series, no corresponding visual distinction is made. Note that in the present example, ∆T = 0 and
τ = 0, i.e., only simultaneous events (referring here to the same year) are considered as coincident.

sorghum was cultivated, whereas the trigger coincidence
rate gives the fraction of plots with sorghum plantation on
which low SOC contents were observed.

Next, in order to investigate whether intercropping sys-
tematically co-occurs with very high top-layer SOC con-
tents (hypothesis (ii)), we deﬁne crop covers with at least
four diﬀerent crop types (crop.inter) and SOC contents
larger than the 90th percentile (carb) as events and exe-
cute

ca.out <- CC.eca.ts(carb,crop.inter,...
... sigtest="poisson")

resulting in:

NH precursor: TRUE
NH trigger: TRUE
p-value precursor: 0.08495326
p-value trigger: 0.07630266
precursor coincidence rate: 0.11
trigger coincidence rate: 0.33

In this case, despite the quite high trigger coincidence rate
of 0.33, both null hypotheses cannot be rejected at the
α = 0.05 conﬁdence level. The high trigger coincidence
rate in this example means that 33% of the plots charac-
terized by intensive intercropping also show very high SOC
values. But since the precursor coincidence rate is rather
small, only few plots characterized by high SOC contents
have also been cultivated with intercropping. The large

diﬀerence between trigger and precursor coincidence rate
in this example arises again because the numbers of events
in the two data sets diﬀer markedly (there are 18 events
in the carbon data set and only six events in the crop
cover data set). Therefore, to reach a signiﬁcant p-value
at α = 0.05, in the present example at least three of the
six intercropping ﬁelds would have to coincide with events
in the carbon data set (i.e. rp ≥ 0.5).

For comparison, we apply the shuﬄe surrogates-based

signiﬁcance test to the same data set using

ca.out <- CC.eca.ts(carb,crop.inter,...
... sigtest="shuffle",reps=10000)

and obtain the following result:

NH precursor: TRUE
NH trigger: TRUE
p-value precursor: 0.0621
p-value trigger: 0.0522
precursor coincidence rate: 0.11
trigger coincidence rate: 0.33

This example illustrates that in cases where the rejection
of the null hypothesis is based on p-values close to the
desired conﬁdence level α, the utilization of both signiﬁ-
cance tests is recommended. Speciﬁcally, if we relieve the
requested conﬁdence level only slightly (say, α = 0.06),
the null hypothesis of the trigger test could already be
rejected, indicating that high top-layer SOC contents are

7

TimePrecursor Coincidence Rate : 0.5Event Series AEvent Series B195919611968200520072009TimeTrigger Coincidence Rate : 0.5Event Series AEvent Series B196119661993200020072009actually supported by intercropping. Although the num-
bers of “events” in both data sets (NA = 18 and NB = 6)
are relatively small in comparison with the sample size
(T = 218), the observed changes in the obtained p-values
for both precursor and trigger test indicate that the ana-
lytical signiﬁcance test is actually much more conservative
than necessary for appropriately testing for the presence
of statistical interrelationships between both event series.
This suggests that in case of any doubts regarding the
validity of the implicit assumptions underlying the analyt-
ical signiﬁcance test, the surrogate-based tests should be
preferred.

5. Conclusions

The new R package CoinCalc allows performing event
coincidence analysis (ECA), a novel statistical tool for
quantifying the degree of simultaneity between two event
series [7, 8, 16], for diﬀerent types of event series. The
package provides six functions: (i) binarization of continu-
ous time series, (ii) data conversion of binary (event) time
series to event sequence format, (iii) conversion of event
sequences to binary time series, (iv) ECA for event se-
quences, (v) ECA for binary event time series, and (vi) a
plotting function for visualizing events and coincidences.
Based on two geoscientiﬁc example problems, we have
illustrated the utilization of the package and interpretation
of the obtained results. CoinCalc is freely available via the
CRAN repository (www.r-project.org) and planned to
be regularly updated and further extended. At the present
stage, CoinCalc provides all necessary functions for per-
forming ECA under relatively general conditions. Future
extensions shall include (among others) a more sophisti-
cated surrogate-based signiﬁcance test for serially depen-
dent event sequences (i.e., series with correlated waiting
times between subsequent events) and functions for multi-
variate and conditional ECA. Moreover, the package will
be expanded in order to handle not only pairs of series
of vector format, but to also perform ECA between time
series of grid points in spatially extended data sets (e.g.,
with the dimensions latitude, longitude and time), which
are typical for climatological data sets or remote sensing
products. This extension will also allow analyzing coin-
cidences between time series of diﬀerent regions, opening
the package to a further large ﬁeld of research questions.

Acknowledgements

This work has been ﬁnancially supported by the German
Federal Ministry for Education and Research (BMBF)
within the framework of the BMBF Young Investigators
Group CoSy-CC2: Complex Systems Approaches to Un-
derstanding Causes and Consequences of Past, Present
and Future Climate Change (grant no. 01LN1306A). JFS
acknowledges funding by the Evangelisches Studienwerk
Villigst e.V. The authors are grateful to the SAMPLES

8

project for providing the framework for the soil sample
collection, Gustavo Saiz (IMK-IFU, KIT) for support-
ing the SOC analysis of the soil samples, and Jonathan
Donges and Marc Wiedermann for helpful comments on
earlier versions of this manuscript and the software pack-
age CoinCalc described herein.

References

[1] Baumgartner, M., Thiem, A., 2015. Identifying complex causal
dependencies in conﬁgurational data with coincidence analysis.
The R Journal 7 (1), 176–184.

[2] Boers, N., Bookhagen, B., Marwan, N., Kurths, J., 2015. Spa-
tiotemporal characteristics and synchronization of extreme rain-
fall in South America with focus on the Andes Mountain range.
Climate Dynamics, doi: 10.1007/s00382–015–2601–6.

[3] Boulton, A., 2003. Parallels and contrasts in the eﬀects of
drought on stream macroinvertebrate assemblages. Freshwater
Biology 48 (7), 1173–1185.

[4] Brown, E., Kass, R., Mitra, P., 2004. Multiple neural spike train
data analysis: state-of- the-art and future challenge. Nature
Neuroscience 7 (5), 456–461.

[5] Daan, N., Gislason, H., Pope, J., Rice, J., 2005. Changes in the
north sea ﬁsh community: evidence of indirect eﬀects of ﬁshing?
ICES Journal of Marine Science 62 (2), 177–188.

[6] Deutscher Wetterdienst, Oﬀenbach, 2009. DWD Klimastatio-
nen: Daten der Klimastationen des Deutschen Wetterdienstes.
[7] Donges, J. F., Donner, R. V., Trauth, M. H., Marwan, N.,
Schellnhuber, H.-J., Kurths, J., 2011. Nonlinear detection of
paleoclimate-variability transitions possibly related to human
evolution. Proceedings of the National Academy of Sciences of
the USA 108, 20422–20427.

[8] Donges, J. F., Schleussner, C.-F., Siegmund, J. F., Donner,
R. V., submitted. Coincidence analysis for quantifying statisti-
cal interrelationships between event time series. European Phys-
ical Journal Special Topics, arXiv preprint: 1508.03534.

[9] Iwi, A., Sutton, R., Norton, W., 2006. Inﬂuence of May At-
lantic Ocean initial conditions on the subsequent North Atlantic
winter climate. Quarterly Journal of the Royal Meteorological
Society 132 (621), 2977–2999.

[10] Jones, P. D., Briﬀa, K. R., Osborn, T. J., Lough, J. M., van
Ommen, T. D., Vinther, B. M., Luterbacher, J., Wahl, E. R.,
Zwiers, F. W., Mann, M. E., Schmidt, G. A., Ammann, C. M.,
Buckley, B. M., Cobb, K. M., Esper, J., Goosse, H., Graham, N.,
Jansen, E., Kiefer, T., Kull, C., Kuettel, M., Mosley-Thompson,
E., Overpeck, J. T., Riedwyl, N., Schulz, M., Tudhope, A. W.,
Villalba, R., Wanner, H., Wolﬀ, E., Xoplaki, E., 2009. High-
resolution palaeoclimatology of the last millennium: a review
of current status and future prospects. The Holocene 19 (1),
3–49.

[11] Kumar, A., Hoerling, M., 2003. The nature and causes for the
delayed atmospheric response to El Ni˜no. Journal of Climate
16 (9), 1391–1403.

[12] Letnic, M., Tamayo, B., Dickman, C., 2005. The responses of
mammals to La Ni˜na (El Ni˜no Southern Oscillation)-associated
rainfall, predation, and wildﬁre in central Australia. Journal of
Mammalogy 86 (4), 689–703.

[13] Lewicki, M., 1988. A review of methods for spike sorting: the
detection and classiﬁcation of neural action potentials. Network:
Computation in Neural Systems 9, R53 R78.

[14] Malik, N., Bookhagen, B., Marwan, N., Kurths, J., 2010. Anal-
ysis of spatial and temporal extreme monsoonal rainfall over
South Asia using complex networks. Climate Dynamics 39, 971–
987.

[15] ¨Osterle, H., Werner,

P., Gerstengarbe,

2006.
Qualit¨atspr¨ufung, Erg¨anzung und Homogenisierung der
t¨aglichen Datenreihen in Deutschland, 1951-2003: ein neuer
Datensatz.
7. Deutsche Klimatagung, Klimatrends:
Vergangenheit und Zukunft, 9. - 11 Oktober 2006.

F.,

In:

[16] Rammig, A., Wiedermann, M., Donges, J. F., Babst, F., von
Bloh, W., Frank, D., Thonicke, K., Mahecha, M. D., 2015. Tree-
ring responses to extreme climate events as benchmarks for ter-
restrial dynamic vegetation models. Biogeosciences 12, 373–385.
[17] Reichstein, M., Bahn, M., Ciais, P., Frank, D., Mahecha,
M., Seneviratne, S., Zscheischler, J., Beer, C., Buchmann, N.,
Frank, D., Papale, D., Rammig, A., Smith, P., Thonicke, K.,
van der Velde, M., Vicca, S., Walz, A., Wattenbach, M., 2013.
Climate extremes and the carbon cycle. Nature 500, 287–295.

[18] Siegmund, J. F., Donges, J. F., Wiedermann, M., Donner, R. V.,
2015. Impact of climate extremes on wildlife plant ﬂowering over
germany. Biogeosciences Discussions 12, 18389–18423.

[19] Wedgbrow, C., Wilby, R. L., Fox, H., O’Hare, G., 2002.
Prospects for seasonal forecasting of summer drought and low
river ﬂow anomalies in England and Wales. International Jour-
nal of Climatology 22 (2), 219–236.

[20] Woodborne, S., Hall, G., Robertson, I., Patrut, A., Rouault,
M., Loader, N., Hofmeyr, M., 2015. A 100-Year CCarbon Iso-
tope Rainfall Proxy Record from South African Baobab Trees
(Adansonia digitata L.). PLOS ONE 10 (5), e0124202.

[21] Zaborov, D., 2009. Coincidence Analysis

in ANTARES:
Potassium-40 and Muons. Physics of Atomic Nuclei 72 (9),
1537–1542.

[22] Zscheischler, J., Mahecha, M., Harmeling, S., Reichstein, M.,
2013. Detection and attribution of large spatiotemporal extreme
events in earth observation data. Ecological Informatics 15, 66–
73.

9

