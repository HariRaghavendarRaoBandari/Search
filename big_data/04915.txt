Safe Learning of Regions of Attraction for Uncertain, Nonlinear

Systems with Gaussian Processes

Felix Berkenkamp, Riccardo Moriconi, Angela P. Schoellig, Andreas Krause

6
1
0
2

 
r
a

 

M
5
1

 
 
]

Y
S
.
s
c
[
 
 

1
v
5
1
9
4
0

.

3
0
6
1
:
v
i
X
r
a

Abstract— Control theory can provide useful insights into
the properties of controlled, dynamic systems. One important
property of nonlinear systems is the region of attraction (ROA),
which is a safe subset of the state space in which a given
controller renders an equilibrium point asymptotically stable.
The ROA is typically estimated based on a model of the system.
However, since models are only an approximation of the real
world, the resulting estimated safe region can contain states
outside the ROA of the real system. This is not acceptable
in safety-critical applications. In this paper, we consider an
approach that learns the ROA from experiments on a real
system, without ever leaving the ROA of the real system. This
approach enables us to ﬁnd an estimate of the real ROA,
without risking safety-critical failures. Based on regularity
assumptions on the model errors in terms of a Gaussian process
prior, we determine a region in which an equilibrium point is
asymptotically stable with high probability, according to an
underlying Lyapunov function. Moreover, we actively select
areas of the state space to evaluate in order to expand the ROA.
We demonstrate the effectiveness of this method in simulated
experiments.

I. INTRODUCTION

Estimating the region of attraction (ROA) of an equilib-
rium point is an important problem when analyzing nonlinear
systems. It speciﬁes the region within which a given control
law renders an equilibrium point asymptotically stable. Thus,
the system can be operated safely within the ROA. This is
of practical importance, since controllers for safety-critical
systems are required to guarantee safety over a certain
domain of operation, before they can be implemented on
the real system. Typically, ROAs are estimated based on a
model of the system. However, due to model errors, ROAs of
the real system can be drastically different, raising questions
about the viability of such purely model-based methods.

In this paper, we address the problem of safely estimating
the ROA from experiments on a real system, while always
remaining in the real ROA with high probability. As a result,
the equilibrium point of the closed-loop system will be attrac-
tive for any state visited throughout the learning process. This
avoids failures and makes the method applicable to safety-
critical domains. To expand the estimated safe region over

Felix Berkenkamp and Andreas Krause are with the Learning & Adaptive
Systems Group (www.las.ethz.ch), Department of Computer Science, ETH
Zurich, Switzerland. Email: {befelix, krausea}@ethz.ch
ETH Zurich, Switzerland. Email: rmoricon@ethz.ch

Riccardo Moriconi is with the Department of Mechanical Engineering,

Angela P. Schoellig is with the Dynamic Systems Lab (www.dynsyslab.
org), Institute for Aerospace Studies, University of Toronto, Canada. Email:
schoellig@utias.utoronto.ca

This research was supported by SNSF grant 200020 159557, NSERC

grant RGPIN-2014-04634, and the Connaught New Researcher Award.

time, we actively select states that improve our estimate of
the underlying uncertain model.

Related work: In the literature, ROAs are computed based
on system models [1]. One typical approach is to use the
level sets of a Lyapunov function [2] in order to quantify the
ROA. Given a Lyapunov function, the problem of ﬁnding
the ROA reduces to a line search for the maximum, safe
level set of the Lyapunov function. For polynomial systems,
Lyapunov functions can be found efﬁciently by solving a
system of linear matrix inequalities (LMIs) [3]. A relaxation
to Lyapunov-like functions for ROA computation can be
found in [4], and saturating control inputs were considered
in [5]. A review of numerical methods to compute Lyapunov
functions can be found in [6]. The approach in [7] considered
quadratic systems and the problem of testing whether a given
polytope lies inside the ROA. Beyond Lyapunov functions,
the ROA can be found via constraint solving [8] or by
optimizing over state trajectories [9].

An area that is of particular relevance to this paper is
the computation of ROAs for systems with uncertainties.
In [10], quadratic Lyapunov functions for uncertain, linear
systems were used, while [11] considered polynomial Lya-
punov functions. A more general approach for systems with
a polynomial dependence on uncertainties contained in a
polytope was shown in [12]. Similar ideas also lie at the
center of robust control [13].

The previous methods for estimating ROAs are based on a
model with ﬁxed uncertainties. The method proposed in this
paper uses learning in order to reduce the model uncertainty
over time. Learning the dynamics of nonlinear systems from
data has been considered before. In particular, Gaussian
processes (GPs) [14] have been of interest
in this area,
since they provide both a mean estimate and the associated
uncertainty in the estimate. This can be used to derive high-
probability safety guarantees. For example, in [15] Gaussian
process regression was combined with robust control theory
to provide stability and performance guarantees for a linear
controller applied to a nonlinear system, while [16] used
reachability analysis to compute robust invariant sets.

Gaussian processes have also been used in Bayesian opti-
mization [17], where the goal is to ﬁnd the global optimum
of an a priori unknown function based on noisy evaluations.
Bayesian optimization algorithms provably converge close to
the global optimum after a ﬁnite number of evaluations [18].
In the context of Bayesian optimization, safety has been
considered in terms of high-probability constraint satisfaction
in [19], [20] and has been generalized to arbitrary constraints
in [21]. The latter two applied Bayesian optimization to

˙x(t) = f (x(t), u(t))

+ g(x(t), u(t)),

(1)

(cid:124)

(cid:123)(cid:122)

(cid:125)

(cid:124)

(cid:123)(cid:122)

(cid:125)

nonlinear control problems.

Our contributions: In this paper, we combine ideas from
GP learning, safe Bayesian optimization, and ROA compu-
tation based on Lyapunov functions for uncertain systems.
Using previous results on model-based ROA computation
as a starting point, we compute ROA estimates for the real
system by approximating the model uncertainties with a GP.
We use ideas from safe Bayesian optimization in order to
actively learn about the real dynamics from experiments
executed in the estimated ROA. As we learn more about
the dynamics of the real system, the ROA expands, until we
reach a desired accuracy of the ROA estimate. As a result,
we are able to learn the ROA of a general, nonlinear system
through experiments, without leaving the real system’s ROA
and thereby without violating safety requirements.

II. PROBLEM STATEMENT

In this section, we state the objective of the paper and the
assumptions made in our analysis. We consider a continuous-
time system,

a priori model

unknown model

where x(t) ∈ X ⊆ Rq is the state at time t within a con-
nected set, X , and u(t) ∈ U ⊆ Rp is the control
input.
The system dynamics consist of a known model, f (x, u),
and an unknown model, g(x, u), that accounts for unknown
dynamics and model errors. We assume a control pol-
icy, u = π(x) is given, which has been designed for the prior
model, f. The resulting, closed-loop dynamics are denoted
by fπ(x) := f (x, π(x)) and gπ(x) := g(x, π(x)).

The goal is to estimate the ROA of (1) under the control
policy, π(x), based on experiments on the real system. We
want to actively learn the a priori unknown dynamics, gπ,
without leaving the ROA. This is, of course, impossible
without further assumptions about the model in (1).

the form gπ(x) =(cid:80)

We assume that the unknown model, gπ, has low ‘com-
plexity’, as measured under the norm of a reproducing
kernel Hilbert space (RKHS, [22]). An RKHS, Hk(X ),
is a complete subspace of L2(X ) (space of square in-
tegrable functions), which includes nicely behaved func-
i αik(x, xi) with αi ∈ R
tions of
and representer points, xi ∈ X . Here and in the fol-
lowing,
the subscript k refers to a symmetric, positive-
function, k(·,·), which can be inter-
deﬁnite kernel
preted as a similarity measure. The RKHS has an in-
ner product, (cid:104)·,·(cid:105)k, which obeys the reproducing prop-
erty: (cid:104)gπ, k(x,·)(cid:105)k = gπ(x) for all gπ ∈ Hk(X ). The in-
duced RKHS norm, (cid:107)gπ(cid:107)2
k = (cid:104)gπ, gπ(cid:105)k, measures smooth-
ness of gπ with respect
to the kernel, k(·,·), [18]. For
universal kernels [22], members of Hk(X ) can uniformly
approximate any continuous function on any compact subset
of X . We make assumptions about
the smoothness, or
complexity, of gπ.
Assumption 1. The function gπ has bounded RKHS norm
with respect to a continuously differentiable, bounded ker-
nel, k(x, x(cid:48)); that is, (cid:107)gπ(cid:107)k ≤ Bg.

Moreover, we make the following, standard assumption on

the a priori model, fπ(x):
Assumption 2. The function fπ is Lipschitz continuous with
Lipschitz constant Lf and bounded in X with (cid:107)fπ(cid:107)∞ ≤ Bf .
In the following sections, we use a Lyapunov function
in order to compute the ROA. While in practice one may
want to recompute good Lyapunov functions during the ex-
perimentation phase using, for example, the methods in [6],
here we consider a ﬁxed Lyapunov function. In particular,
we assume the following:
Assumption 3. The origin is an equilibrium point of (1)
with fπ(0) = gπ(0) = 0.
Assumption 4. A ﬁxed, two-times continuously differentiable
Lyapunov function, V (x), is given. Moreover, there exists
a constant c0 > 0 such that ∂V (x)
∂x (fπ(x) + gπ(x)) < 0 for
all x ∈ S0 = V(c0), where V(c) = {x ∈ X | V (x) ≤ c}.

Assumption 3 implies that the origin is an equilibrium
point of (1) with the unknown model gπ(x). Without this
assumption, there is no hope to prove that the origin is
attractive, but showing boundedness would be still possible.
Assumption 4 implies that we have chosen the policy, π(x),
and the Lyapunov function, V (x), such that the origin of (1)
is locally asymptotically stable. This is, for example, fulﬁlled
if the origin of the a priori dynamics, ˙x = fπ(x), is locally
asymptotically stable and ∂gπ
∂x is zero at the origin; that is,
the prior dynamics dominate close to the origin.

Lastly, we require measurements of gπ(x) in order to
learn about the unknown dynamics. That is, we must be
able to measure x and ˙x (fπ(·) is known). In practice, we
may consider a discrete-time approximation of
˙x, which
only requires measurements of the state, x, rather than its
derivative, ˙x. We discuss practical aspects in Sec. VII.
Assumption
to measurements,
ˆgπ(x) = ˙x − fπ(x) + ω, which are corrupted by zero-
mean, independent, and bounded noise, |ω| ≤ σ.

5. We

access

have

For ease of notation, we consider a one-dimensional
system of the form of (1) in the following. We explain how
to generalize the results to multiple dimensions in Sec. VI.

III. GAUSSIAN PROCESSES (GPS)

The function gπ(x) in (1) is unknown a priori. Given As-
sumption 1, GPs are good models to approximate this
unknown function over its domain, X , see [18] and (5) below.
The following introduction to GPs is based on [14].
GPs are a popular choice for nonparametric regression in
machine learning, where the goal is to ﬁnd an approximation
of a nonlinear map, gπ(x) : X → R, from a state, x, to the
function value gπ(x). This is accomplished by assuming that
function values, gπ(x), associated with different states, x, are
random variables and that any ﬁnite number of these random
variables have a joint Gaussian distribution [14].
The Bayesian, nonparametric regression is based on a prior
mean function and a covariance function, k(x, x(cid:48)), which

deﬁnes the covariance of any two function values, gπ(x)
and gπ(x(cid:48)), x, x(cid:48)
∈ X . The latter is also known as the
kernel. In this work,
the mean is zero, since any prior
knowledge about the dynamics is captured by fπ(x); that
is, gπ(x) ∼ GP(0, k(x, x(cid:48))). In general, the choice of kernel
function is problem-dependent and encodes assumptions
about the smoothness and rate of change of the unknown
function. A review on kernels can be found in [14]. In the
following, we use the same kernel as in Assumption 1.

We can prediction a function value, gπ(x), at an arbitrary
state, x ∈ X , by conditioning the GP distribution of gπ, on
a set of n past measurements, yn = [ˆgπ(x1), . . . , ˆgπ(xn)]
at states Dn = {x1, . . . , xn}, where ˆgπ(x) = ˙x − fπ(x) + ω
with ω ∼ N (0, σ2). The posterior over gπ(x) is a GP
distribution again, with mean µn(x), covariance kn(x, x(cid:48)),
and variance σn(x):

(2)
(3)
(4)

µn(x) = kn(x)(Kn + Inσ2)

−1yn,

(cid:48)

n (x

)

−1kT

(cid:48)
kn(x, x

(cid:48)
) = k(x, x

) − kn(x)(Kn + Inσ2)

σ2
n(x) = kn(x, x)

where the vector kn(x) =(cid:2)k(x, x1), . . . , k(x, xn)(cid:3) contains
the covariances between the new input, x, and the observed
data at states in Dn, Kn ∈ Rn×n is the positive-deﬁnite
kernel matrix with [Kn](i,j) = k(xi, xj), i, j ∈ {1, . . . , n},
and In ∈ Rn×n is the identity matrix.
Assumption 1 allows us to model gπ(x) as a GP. In
particular, we have the following result:
Lemma
(cid:107)gπ(cid:107)2
the noise, ω,
by
for all n ≥ 1 and x ∈ X ,
least 1 − δ, δ ∈ (0, 1), that

that
zero-mean and uniformly bounded
βn = 2Bg + 300γn log3(n/δ).
Then,
it holds with probability at

is
σ. Choose

k ≤ Bg,

Supposed

that

and

1.

|gπ(x) − µn−1(x)| ≤ β1/2

n σn−1(x).

(5)

Proof. See [18, Theorem 6].

(5) allows us to make high-probability statements about
the function values of gπ(x), even though the GP makes
different model assumptions than we do in Assumption 1
(e.g., about the noise, ω). The scalar βn depends on the
information gain, γn = maxx1,...,xn I(gπ, yn), which is the
maximal mutual information that can be obtained about the
GP prior from n noisy samples, yn, at states Dn. In [18]
it was shown that
the information gain has a sublinear
dependence on n for many commonly used kernels and that
it can be efﬁciently approximated up to a constant. As a
result, even though βn is increasing with n, we are able to
learn about the true values of gπ(x) over time by making
appropriate choices for data samples in Dn, see Sec. V.

IV. LYAPUNOV STABILITY

In this section, we show how the assumptions in Sec. II
can be used in order to compute the ROA for the nonlinear
system in (1) based on a GP model of gπ from Sec. III.
We do not actively select data points yet, but rather base

the analysis on a GP model of gπ(x) from Sec. III based
on n measurements of gπ at arbitrary states.

We start by observing the following:

Lemma 2. The function gπ is Lipschitz continuous with
Lipschitz constant Lg and bounded by (cid:107)gπ(cid:107)∞ ≤ Bg(cid:107)k(cid:107)∞.
Proof. Boundedness by [22, Lemma 4.23]. From As-
k dk(x, x(cid:48)), where
|gπ(x) − gπ(x(cid:48))|2 ≤ (cid:107)gπ(cid:107)2
sumption 1,
dk(x, x(cid:48)) = k(x, x) + k(x(cid:48), x(cid:48)) − 2k(x, x(cid:48))
the kernel
metric [22, Lemma 4.28]. Since k(·,·) is continuously dif-
ferentiable and bounded, |gπ(x) − gπ(x(cid:48))|2 ≤ L2
g|x − x(cid:48)
|2,
where L2

g = 2Bg(cid:107)k(cid:107)∞(cid:107) ∂k

∂x(cid:107)∞.

is

Since the closed-loop dynamics in (1) are Lipschitz con-
tinuous based on Lemma 2 and Assumption 2, existence and
uniqueness of the solutions of (1) are guaranteed.

The goal of this section is to quantify the ROA based
on the Lyapunov function in Assumption 4. From Lyapunov
stability theory we have the following [2]:
Lemma 3. The origin of the dynamics in (1) is asymptot-
ically stable within a level set, V(c) = {x ∈ X | V (x) ≤ c}
with c ∈ R>0, if, for all x ∈ V(c),

(cid:0)fπ(x) + gπ(x)(cid:1) < 0.

(6)

˙V (x) =

∂V (x)

∂x

The goal of this section is to use the GP model of gπ(x)
from Sec. III in order to ﬁnd the largest c such that (6) holds
true within V(c) with high probability. The existence of such
a value c0 > 0 is ensured by Assumption 4.
In order to quantify the ROA, we require a model of ˙V (x)
in terms of the GP model of gπ(x), which allows us to check
that (6) holds. From Sec. III we know that GPs are are a
collection of inﬁnitely many, correlated, normally distributed
random variables. As a consequence, since ˙V (x) in (6) is
˙V (x) itself is a GP. In
afﬁne in the GP model of gπ(x),
˙V (x) is normally distributed with
particular, we have that
mean µn, ˙V (x) and standard deviation σn, ˙V (x) given by:

(cid:0)µn(x) + fπ(x)(cid:1),
(cid:12)(cid:12)(cid:12)(cid:12) σn(x),

∂x

∂V (x)

(cid:12)(cid:12)(cid:12)(cid:12) ∂V (x)

∂x

µn, ˙V (x) =

σn, ˙V (x) =

(7)

(8)

where µn(x) and σn(x) are the GP predictions of the
unknown dynamics, gπ(x), from (2) and (4). We can use (7)
˙V (x) in (6) based
and (8) to directly make predictions about
on measurements of gπ(x).

Since (7) and (8) provide a probabilistic estimate of ˙V (x),
we cannot expect to make deterministic statements about
stability. Instead, we consider the conﬁdence intervals of
˙V (x). We denote the lower and upper
the GP model of
conﬁdence intervals after (n − 1) measurements of gπ(x) by
(9)
(10)

ln(x) := µ ˙V ,n−1(x) − β1/2
un(x) := µ ˙V ,n−1(x) + β1/2

n σ ˙V ,n−1,
n σ ˙V ,n−1,

respectively. The conﬁdence intervals are parameterized by
the scalar βn. In the following we will assume that βn is

˙V (x)
chosen according to (5), which allows us to say that
takes values within the interval [ln(x), un(x)] with high
probability (at least 1 − δ).
Based on these conﬁdence intervals, we can see from (6)
˙V (x) < 0 holds within V(c) for some c > 0 with high
that
probability, if un(x) < 0 for all x ∈ V(c). Unfortunately, it
is impossible to evaluate the upper bound (10) everywhere in
a continuous domain. Nevertheless, it is possible to evaluate
predictions of ˙V (x) from (7) and (8) at a ﬁnite number of
points. In the following, we exploit the continuity properties
˙V (x) in order to derive stability properties within a
of
continuous domain based on a ﬁnite number of predictions.
In particular, we use the following property of ˙V (x):
Lemma 4. The function ˙V (x) is Lipschitz continuous with
Lipschitz constant L.

(cid:48)

(fπ(x

) + gπ(x

(cid:12)(cid:12)(cid:12)(cid:12)

(cid:48)

))

(cid:12)(cid:12)(cid:12)(cid:12),

(cid:48)

Proof. Based on the Lipschitz continuity of (1) from As-
sumption 2 and Lemma 2, we expand | ˙V (x) − ˙V (x(cid:48))| to

(fπ(x) + gπ(x)) −

∂x

(cid:12)(cid:12)(cid:12)(cid:12) ∂V (x)
(cid:12)(cid:12)(cid:12)(cid:12)fπ(x) + gπ(x)
(cid:12)(cid:12)(cid:12)(cid:12) ∂V (x(cid:48))

∂x(cid:48)

≤

∂V (x(cid:48))

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ∂V (x)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)fπ(x) + gπ(x) − fπ(x

∂x
∂V (x(cid:48))
∂x(cid:48)

∂x −

(cid:48)

(cid:12)(cid:12)(cid:12)(cid:12)

(cid:48)

)

) − gπ(x

| + LV (Lg + Lf )|x − x

+
(cid:48)
≤ (Bf + Bg(cid:107)k(cid:107)∞)L∂V |x − x
:= L|x − x
where LV = (cid:107) ∂V (x)
∂x2 (cid:107)∞ are the Lip-
schitz constants of V and its ﬁrst derivative. These are
guaranteed to exist by Assumption 4, since a continuous
function obtains a maximum over a bounded domain.

∂x (cid:107)∞ and L∂V = (cid:107) ∂2V (x)

|,

|,

(cid:48)

The continuity of ˙V (x) allows us to evaluate predictions
of ˙V (x) from (7) and (8) only at a ﬁnite number of points,
without loosing guarantees.
Lemma 5. Let Xτ ⊂ X be a discretization of X with
[x]τ denotes the
|x − [x]τ| ≤ τ /2 for all x ∈ X . Here,
closest point in Xτ to x ∈ X . Choosing βn according to (5),
least (1 − δ) for
the following holds with probability at
all x ∈ X and all n ≥ 1:

(cid:12)(cid:12)(cid:12) ˙V (x) − µ ˙V , n−1([x]τ )

(cid:12)(cid:12)(cid:12) ≤ β1/2

n σ ˙V , n−1([x]τ ) + Lτ.

(11)

Proof. The result follows from the Lipschitz continuity
of ˙V (x) in Lemma 4 and is similar to [18, Lemma 5.7].

Lemma 5 bounds the deviation of

˙V from the mean
estimate, µ ˙V , n−1, in terms of the standard deviation. The
discretization accuracy, τ, trades off the precision with which
we ﬁnd the stable ROA for the computational cost of
computing the conﬁdence intervals for all states in Xτ . A
ﬁner discretization (smaller value of τ) decreases the con-
servativeness of the ROA estimate. This is different from the
number of experiments, n, which decreases the uncertainty
in the model estimate as we obtain more data. Combining
the previous results lets us argue about the stability:

Algorithm 1: Safe ROA exploration
Inputs: Domain X and discretization with τ, Xτ

GP prior k(x, x(cid:48))
Initial safe set S0 ⊆ X
c, subject to

1 for n = 1, . . . do
cn ← argmax

c>0

2

un(x) < −Lτ,

for all x ∈ V(c) ∩ Xτ

Sn ← S0 ∪ V(cn)
xn ← argmaxx∈Sn σn−1(x)
Update GP with measurement of ˆgπ(xn)

3
4
5
6 end

Lemma 6. With a discretization of X , Xτ , according
to Lemma 5 and with βn according to (5), the origin of (1)
is asymptotically stable within V(c) for some c > 0 with
probability at least (1 − δ) if, for all x ∈ V(c) ∩ Xτ ,
n σ ˙V , n−1(x) = ut(x) < −Lτ.

µ ˙V , n−1(x) + β1/2

(12)

Proof. This is a direct consequence of (5), Lemma 5,
and Lemma 3.

Given the previous results, after (n − 1) evaluations of gπ,
it sufﬁces to maximize c such that the condition in Lemma 6
holds for all discretized states in V(c):
Theorem 1. Under the assumptions of Lemma 6, pick

c,

cn = argmax
c∈R>0

subject to (12) for all x ∈ V(c) ∩ Xτ .
Then, Sn = S0 ∪ V(cn) is contained within the ROA of (1)
for all n ≥ 1 with probability at least (1 − δ).
Proof. The set S0 from Assumption 4 is contained in the
true ROA deterministically, while V(cn) is contained with
probability at least (1 − δ) by Lemma 6. The result follows.

The previous result provides a high-probability estimate of
the ROA. In particular, initially only the deterministic safe
set from Assumption 4 can provide information about the
stability, but as more information about gπ becomes available
from measurements, the GP model can be used to guarantee
stability beyond this initial set.

V. ACTIVE LEARNING

In the previous section, we provided an estimate of the
ROA based on a GP model of the unknown dynamics
of gπ(x) in (1), which was informed by arbitrarily selected
measurements/experiments. This on its own could be used
to determine an estimate of the ROA based on pre-existing
recorded data. In this section, we actively select new, safe
states within the ROA estimate at which to obtain measure-
ments in order to efﬁciently expand the estimate of the ROA.
As we update the model with new data, we can compute
the current estimate of the ROA, Sn, using Theorem 1. The
goal is to actively learn about the dynamics and expand
the ROA starting from S0. In order to select states that are

(a) Initial safe set, S0.

(b) Exploration within safe level set, Sn.

Fig. 1. One-dimensional example of Algorithm 1. Initially in Fig. 1a, the estimate of the dynamics is uncertain (top row, blue shadow indicates the 2σ
conﬁdence interval), so that only the initial safe set, S0, is known to be safe (red), while larger level sets of the Lyapunov function are unsafe (blue line).
The algorithm then actively selects new states at which to obtain noisy measurements of the dynamics, which causes the safe set to increase in Fig. 1b.
Iteratively applying the procedure eventually leads to the largest, safe level set in Fig. 1c. No states outside the safe level set are evaluated during the
learning.

(c) Maximum level set found.

relevant for learning gπ, we follow ideas from safe Bayesian
optimization [19] and use the uncertainty estimate of the GP.
In particular, we select

xn = argmax

x∈Sn

σn−1(x),

(13)

as the next state to evaluate according to (4) within a current
estimate of the safe set, Sn. At xn, the GP model is the
most uncertain about the unknown dynamics, gπ(x). The
idea behind this selection criterion is that we want to learn
about the most uncertain state in order to increase the ROA
estimate. By decreasing the uncertainty about the unknown
dynamics, gπ(x), over time, the safe region will expand
eventually. We summarize the method in Algorithm 1 and
a one-dimensional example can be found in Fig. 1.

It follows from Theorem 1, that every state chosen by (13)
lies inside the ROA of (1) with high probability. For a
variant of Algorithm 1 it is possible to even prove a stronger
result: we can prove that the maximum, safe level set can be
found to some accuracy (with probability at least (1 − δ))
after a ﬁnite number of experiments. The corresponding
variant of Algorithm 1 builds up the ROA estimate using the
Lipschitz constant. More details are given in the appendix
in Theorem 2.

VI. EXTENSION TO MULTIPLE DIMENSIONS
So far we have considered a one-dimensional system
in (1). The preceding analysis directly transfers to multiple
dimensions, at the expense of more cumbersome notation.
The only non-trivial parts of the extension are GP mod-
els with vector predictions and the corresponding choice
of βn in (5). The main observation is that vector-valued
functions can be modeled as a single GP over an extended

state space, X × I, where integer elements in I index the
output dimension of gπ(x). At each iteration, we obtain q
measurements of gπ(x); one for each dimension of the state.
As a result, βn(cid:48) = 2Bg + 300γ(cid:48)
nq log3(nq/δ) increases at a
faster rate of n(cid:48) = nq compared to βn in (5). The information
gain, γ(cid:48),
is measured relative to the combined function
over X × I. Refer to [21] for more details.

VII. DISCUSSION

Algorithm 1 allows for the safe exploration of the ROA
without leaving the ROA for a general, uncertain, nonlinear
system. While the discretization in Lemma 5 may be con-
servative and computing the predictions of the GP model
˙V at the discretization points may be computationally
of
intensive, this needs to be compared to the generality of
the statements we have made. In particular, besides mild
continuity assumptions about the Lyapunov function and the
known dynamics, the only assumption we have made about
the unknown dynamics, gπ(x) in (1), is Assumption 1, which
is very general.

In the analysis, we have assumed that a method exists,
which drives the system to any state xn selected by (13),
without leaving the ROA. In practice, this must be further
restricted by reachability properties, see [16]. These can be
included as an additional constraint; that is, if R is the safely
reachable set, we select xn from Sn ∩ R in (13).
Algorithm 1 can be made more data-efﬁcient by only
evaluating states close to the boundary of the level set,
without loosing guarantees. See [19] and [21] for details.

A discrete-time variant of Algorithm 1 that determines
safety with V (fπ,d(x) + gπ,d(x)) − V (x) < 0 only requires
measurements of the states, x, rather than derivatives,
˙x,
in Assumption 5. While mapping a GP model of gπ(x)

fπ(x)+gπ(x)TruedynamicsEstimateddynamicsStatex−Lτ˙V(x)True˙V(x)Estimated˙V(x)S0StatexS4Statex0V(x)S20through a nonlinear Lyapunov function makes this method
not analytically tractable, methods based on approximate
uncertainty propagation may work well in practice.

Lastly, Algorithm 1 can be used to optimize policies
as well. For example, by learning a GP model of g(x, u)
directly and adapting the policy so that the Lyapunov stability
requirements are fulﬁlled.

VIII. EXPERIMENTS

In this section we evaluate Algorithm 1 in a simulated
experiment. We only provide a high-level overview of the
experiment. For details refer to the source code and docu-
mentation at http://github.com/befelix/lyapunov-learning.

We

consider

an inverted pendulum with friction
coefﬁcient,
µ = 0.05 Nms/rad, mass, m = 0.15 kg,
length, l = 0.5 m, and angle θ. The dynamics are given by

¨θ(t) =

mgl sin θ(t) − µ ˙θ(t) + u(t)

ml2

,

(14)

where u(t) is the torque applied to the pendulum. The torque
is limited so that the real system cannot recover from states
with |θ| > 30 deg. The state is x = (θ, ˙θ). We assume that
we only know a linear approximation of the dynamics in (14)
for the upright equilibrium point, where additionally friction
is neglected and the mass is 0.05 kg lighter. We use a Linear
Quadratic Regulator based on this linear approximation in
order to control the origin, x = 0, and use the corresponding,
quadratic Lyapunov function to determine the ROA of (14).
We model the error in (14) as a GP, see Sec. III. In
particular, we use the product of a linear and a Mat´ern kernel
for the GP model, which encodes nonlinear functions that are
two-times differentiable and take values that are bounded by
a linear function from above and below with high probability.
Details about these kernels can be found in [14] and a one-
dimensional sample function of such a kernel can be found
in the upper plot of Fig. 1a. In practice, one can encode more
assumptions in the kernel, such as additivity of errors. The
more assumptions are made about the model error, the faster
the learning algorithm will converge.

In the analysis, we assumed gπ to have bounded RKHS
norm in Assumption 1. Here, we model gπ as a sample
function of the GP and set β1/2
n = 2 for all iterations. We
use τ = 0.002 and high-probability, local Lipschitz constants
encoded by the kernel with Lemma 4. The initial safe set
is S0 = {(θ, ˙θ) ∈ R2 ||θ| ≤ 5 deg, | ˙θ| ≤ 10 deg /s}.
The results can be seen in Fig. 2. While the prior model
with the wrong mass and friction parameters estimates a safe
set that is too large (includes unsafe states), Algorithm 1
provides a conservative estimate. As we gain more data
about the dynamics and if we discretize with smaller values
of τ, the estimate improves and, in the limit, converges to
the true level set. Overall, Algorithm 1 provides a powerful
tool to learn the ROA of general nonlinear systems from
experiments, without leaving the safe region encoded by the
Lyapunov function.

Fig. 2. Prior, true, and estimated ROA level sets after 100 data points (blues
crosses). The prior model estimates a safe region that is larger than the true
ROA, thus it includes unsafe states. In contrast, Algorithm 1 provides a
conservative estimate, since it considers states with ˙V (x) ≤ −Lτ, rather
than ˙V (x) ≤ 0. The level set could be increased by discretizing the space
with a smaller value of τ.

IX. CONCLUSION

We introduced a learning algorithm that, based on an
initial, approximate model and a corresponding Lyapunov
function, is able to learn about the real ROA through experi-
ments on the real system, without leaving the true ROA with
high probability. While some of the assumptions in Sec. II
may be restrictive for practical application, the results in this
paper should be understood as a theoretical foundation for
learning algorithms that learn without risking instability.

REFERENCES

[1] A. Zecevic and D. D. Siljak, Control of complex systems, ser. Com-

munications and Control Engineering. Springer, 2010.

[2] H. K. Khalil and J. W. Grizzle, Nonlinear systems.

Prentice Hall,

1996, vol. 3.

[3] B. Tibken, “Estimation of the domain of attraction for polynomial
systems via LMIs,” in Proc. of the IEEE Conference on Decision and
Control (CDC), vol. 4, 2000, pp. 3860–3864.

[4] S. Ratschan and Z. She, “Providing a basin of attraction to a target
region of polynomial systems by computation of Lyapunov-like func-
tions,” SIAM Journal on Control and Optimization, vol. 48, no. 7, pp.
4377–4394, 2010.

[5] D. Coutinho and J. Gomes da Silva, “Computing estimates of the
region of attraction for rational control systems with saturating actu-
ators,” IET Control Theory Applications, vol. 4, no. 3, pp. 315–325,
2010.

[6] P. Giesl and S. Hafstein, “Review on computational methods for
Lyapunov functions,” Discrete and Continuous Dynamical Systems,
Series B, vol. 20, no. 8, pp. 2291–2337, 2015.

[7] F. Amato, C. Cosentino, and A. Merola, “On the region of attraction of
nonlinear quadratic systems,” Automatica, vol. 43, no. 12, pp. 2119–
2123, 2007.

[8] H. Burchardt and S. Ratschan, “Estimating the region of attraction
of ordinary differential equations by quantiﬁed constraint solving,” in
Proc. of the WSEAS International Conference on Dynamical Systems
and Control, 2007, pp. 241–246.

[9] D. Henrion and M. Korda, “Convex computation of the region of
attraction of polynomial control systems,” IEEE Transactions on
Automatic Control, vol. 59, no. 2, pp. 297–312, 2014.

[10] A. Troﬁno, “Robust stability and domain of attraction of uncertain
nonlinear systems,” in Proc. of the American Control Conference
(ACC), vol. 5, 2000, pp. 3707–3711.

[11] D. Coutinho, A. Troﬁno, and M. Fu, “Guaranteed cost control of
uncertain nonlinear systems via polynomial Lyapunov functions,”
IEEE Transactions on Automatic Control, vol. 47, no. 9, pp. 1575–
1580, 2002.

[12] U. Topcu, A. K. Packard, P. Seiler, and G. Balas, “Robust region-
of-attraction estimation,” IEEE Transactions on Automatic Control,
vol. 55, no. 1, pp. 137–142, 2010.

−0.3−0.2−0.10.00.10.20.3Angleθ[rad]−4−2024Angularvelocity˙θ[rad/s]PriorsafesetTruesafesetEstimatedsafeset[13] K. Zhou and J. C. Doyle, Essentials of robust control. Prentice Hall,

[14] C. E. Rasmussen and C. K. Williams, Gaussian processes for machine

1998, vol. 104.

learning. MIT Press, 2006.

[15] F. Berkenkamp and A. P. Schoellig, “Safe and robust learning control
with Gaussian processes,” in Proc. of the European Control Confer-
ence (ECC), 2015, pp. 2501–2506.

[16] A. K. Akametalu, S. Kaynama, J. F. Fisac, M. N. Zeilinger, J. H.
Gillula, and C. J. Tomlin, “Reachability-based safe learning with
Gaussian processes,” in Proc. of the IEEE Conference on Decision
and Control (CDC), 2014, pp. 1424–1431.

[17] J. Mockus, Bayesian approach to global optimization, ser. Mathemat-
ics and Its Applications, M. Hazewinkel, Ed. Springer, 1989, vol. 37.
[18] N. Srinivas, A. Krause, S. M. Kakade, and M. Seeger, “Gaussian
process optimization in the bandit setting: no regret and experimental
design,” in Proc. of the International Conference on Machine Learning
(ICML), 2010, pp. 1015–1022.

[19] Y. Sui, A. Gotovos, J. W. Burdick, and A. Krause, “Safe exploration
for optimization with Gaussian processes,” in Proc. of the International
Conference on Machine Learning (ICML), 2015, pp. 997–1005.

[20] F. Berkenkamp, A. P. Schoellig, and A. Krause, “Safe controller
optimization for quadrotors with Gaussian processes,” in Proc. of the
IEEE International Conference on Robotics and Automation (ICRA),
2016, (to appear), arXiv:1509.01066 [cs.RO].

[21] F. Berkenkamp, A. Krause, and Angela P. Schoellig, “Bayesian opti-
mization with safety constraints: safe and automatic parameter tuning
in robotics.”

arXiv, 2016, arXiv:1602.04450 [cs.RO].

[22] A. Christmann and I. Steinwart, Support Vector Machines, ser. Infor-

mation Science and Statistics. Springer, 2008.

A. Full Exploration Proof

APPENDIX

We consider a variant of Algorithm 1 that determines
the safe set, Sn, starting from S0 in Assumption 4, using
the Lipschitz constant directly. This allows us to prove
exploration guarantees.

(cid:48)

First, let us consider the best ROA approximation that
we could hope to achieve. We have a probabilistic model
of gπ(x), which means we cannot expect to explore the
full ROA. Instead, we consider learning the true ROA up
to some error, . An algorithm with knowledge up to this
error, could use the Lipschitz constant of ˙V from Lemma 4
in order to learn which discrete states close to S0 fulﬁll the
˙V (x) < −Lτ from Lemma 6. We deﬁne a
requirement of
general operator on a set that determines these states,
R, ˙V (S) = S ∪ {x ∈ Xτ |∃x

(15)
(cid:107) < −Lτ},
(cid:19)
where Rl(S) is an operator that selects the maximum level
set of the Lyapunov function contained in S,

˙V (x) +  + L(cid:107)x − x

∈ Rl(S) :

argmax

subject to V(c) ∩ Xτ ⊆ S

Rl(S) = V
.
(16)
Thus, (15) adds a state x ∈ Xτ to S if there exists a state x(cid:48)
in the current level set estimate of the ROA that can be used
˙V (x) < −Lτ via the Lipschitz
in order to determine that
constant. If we apply this operator iteratively, in the limit
we reach the maximum ROA that can be determined using
knowledge up to . That is, with Ri
, ˙V (S))
and R1
, ˙V (S) = R, ˙V (S), the maximum safe set that could
be determined is

, ˙V (S) = R, ˙V (Ri−1

(cid:18)

c>0

c,

(cid:48)

Rl(R, ˙V (S0)), where R, ˙V (S) = lim

i→∞ Ri

, ˙V (S).

(17)

as

IV.

results

in the

from Sec.

With the baseline deﬁned,

˙V , Qn, ˙V (x) = [ln(x), un(x)]

following we
this baseline us-
provide an algorithm that achieves
Instead of deﬁning
ing the
the GP conﬁdence in-
the ROA directly in terms of
terval of
in Theo-
rem 1, we consider the intersection of these intervals,
Cn(x) = Qn, ˙V (x) ∩ Cn−1(x). We initialize these intervals
so that points in S0 are safe;
is, C0(x) = [−∞, 0)
if x ∈ S0 and [−∞, ∞] otherwise. As a consequence of (5),
˙V (x) ∈ Cn(x) for all n ≥ 1 with probability
we have that
at
least (1 − δ). Besides ensuring that points in S0 are
always considered safe, this deﬁnition guarantees that the
estimated safe set will not decrease over time. We de-
ﬁne ln,c(x) = minx∈X Cn(x) and un,c(x) = maxx∈X Cn(x).
Based on these conﬁdence intervals, after (n − 1) mea-
surements we can quantify the following data points as
having a value ˙V (x) that is sufﬁciently negative,

that

Sn, ˙V = S0 ∪ {x ∈ X |∃x

(cid:48)

(cid:48)

∈ Rl(Sn−1, ˙V ) :
) + L(cid:107)x − x

un,c(x

and estimate the ROA based on this set as

(cid:48)

(18)

(cid:107) < −Lτ},

(19)

Sn = Rl(Sn, ˙V ).

Intuitively this selection criterion is similar to the one of the
baseline in (15), except that (18) uses the GP conﬁdence
intervals instead of perfect model knowledge. This allows
us to prove that we explore the maximum level set up to 
accuracy:
Theorem 2. Under the assumptions of Theorem 1, choose βn
as in (5) and let n∗ be the smallest positive integer so that

n∗

CL2

∂V (|R0, ˙V (S0)| + 1)

,

2

βn∗ γn∗ ≥

(20)
where C = 8/ log(1 + σ−2). For any  > 0, and δ ∈ (0, 1),
under the selection criterion (13) within Sn from (19), the
following holds jointly with probability at least (1 − δ):
(i) Sn is contained in the ROA of (1) for all n ≥ 1,
(ii) Rl(R, ˙V (S0)) ⊆ Sn∗ ⊆ Rl(R0, ˙V (S0)).
Proof. Statement (i) follows from Theorem 1. For (ii), we
have from [19, Lemma 5] that
if the safe set does not
increase, then the maximum uncertainty of gπ and ˙V within
the safe set is bounded by /(cid:107)∂V /∂x(cid:107)∞ = /L∂V and ,
respectively, after a ﬁnite number of iterations. At that point,
(cid:16)
either the safe set increases similarly to the baseline, or we
have explored the full safe set [19, Lemma 7]. Applying this
|R0, ˙V (S0)| + 1
That is, after a ﬁnite number of evaluations, n∗, we explore
at least as much as the baseline up to accuracy , but not more
than we could determine as safe if we knew the function
perfectly; that is, the baseline with  = 0.

times provides the result [19, Cor. 4].

(cid:17)

