6
1
0
2

 
r
a

 

M
8
1

 
 
]
t
a
l
-
p
e
h
[
 
 

1
v
8
8
9
5
0

.

3
0
6
1
:
v
i
X
r
a

DEFLATION AS A METHOD OF VARIANCE REDUCTION FOR ESTIMATING THE

ARJUN SINGH GAMBHIR †‡ , ANDREAS STATHOPOULOS § , AND KOSTAS ORGINOS †‡

TRACE OF A MATRIX INVERSE

Abstract. Many ﬁelds require computing the trace of the inverse of a large, sparse matrix. Since dense matrix methods are
not practical, the typical method used for such computations is the Hutchinson method which is a Monte Carlo (MC) averaging
over matrix quadratures. To improve its slow convergence, several variance reductions techniques have been proposed. In this
paper, we study the eﬀects of deﬂating the near null singular value space. We make two main contributions: One theoretical
and one by engineering a solution to a real world application.

We ﬁrst analyze the variance of the Hutchinson method as a function of the deﬂated singular values and vectors. Although
this provides good intuition in general, by assuming additionally that the singular vectors are random unitary matrices, we
arrive at concise formulas for the deﬂated variance that include only the variance and the mean of the singular values. We
make the remarkable observation that deﬂation may increase variance for Hermitian matrices but not for non-Hermitian ones.
This is a rare, if not unique, property where non-Hermitian matrices outperform Hermitian ones. The theory can be used as a
model for predicting and quantifying the beneﬁts of deﬂation. Experimentation shows that the model is robust even when the
singular vectors are not random.

Second, we use deﬂation in the context of a large scale application of “disconnected diagrams” in Lattice QCD. On lattices,
Hierarchical Probing (HP) has previously provided an order of magnitude of variance reduction over MC by removing “error”
from neighboring nodes of increasing distance in the lattice. Although deﬂation used directly on MC yields a limited improvement
of 30% in our problem, when combined with HP they reduce variance by a factor of about 150 over MC. We explain this synergy
theoretically and provide a thorough experimental analysis. One of the important steps of our solution is the pre-computation
of 1000 smallest singular values of an ill-conditioned matrix of size 25 million. Using the state-of-the-art packages PRIMME
and a domain-speciﬁc Algebraic Multigrid preconditioner, we solve one of the largest eigenvalue computations performed in
Lattice QCD on 32 nodes of Cray Edison in about 1.5 hours and at a fraction of the cost of our trace computation.

1. Introduction. The estimation of the trace of the inverse, Tr(A−1), of a large, sparse matrix appears
in many applications, including statistics [13], data mining [6], and uncertainty quantiﬁcation [5]. Our appli-
cation comes from Lattice quantum chromodynamics (QCD), in the context of which ab initio calculations
of properties of hadrons can be performed. This is fundamental for understanding the most basic properties
of matter [11]. The size of the matrix, N , in these problems makes it prohibitive to use matrix factorization
methods to compute the exact trace, and usually the trace is not needed in high accuracy. For these reasons,
the typical tool for such calculations has been a Monte Carlo (MC) method due to Hutchinson [13].

The Hutchinson method is based on the observation that for a matrix A, Tr(A) = E(zH Az), i.e., the
expectation value of quadratures of A with random vectors. Thus, given a sequence of s random vectors zj
whose components are random variables that satisfy the property E(zj(k)zj(k(cid:48))) = δkk(cid:48), the following is an
unbiased estimator for Tr(A−1),

(1.1)

t(A−1) =

j A−1zj, with E(t(A−1)) = Tr(A−1).
zH

the error of the estimator reduces as (cid:112)Var(t(A−1))/s which can be very slow. Therefore, most eﬀort has

Here a superscript H denotes the Hermitian conjugate as we allow A to be complex. As in all MC processes,

concentrated in reducing the variance of the estimator [14, 25, 2]. It is known that among various choices of
random vectors zi, minimum variance is achieved by the Rademacher vectors [2]. When A is real these are
the Z2 vectors, with ±1 uniformly distributed elements. When A is complex these are the Z4 vectors, with
elements uniformly distributed from ±1,±i. For this choice, the variance of the Hutchinson method is

s(cid:88)

j=1

1
s

(cid:32)

F − L(cid:88)

(cid:33)

(1.2)

Var(t(A−1)) = (cid:107) ˜A−1(cid:107)2

F = 2

(cid:107)A−1(cid:107)2

|A−1
i,i |2

.

Note that the variance for vectors zi with elements following a Gaussian distribution is 2(cid:107)A−1(cid:107)2
F which can
be much larger than (1.2). On the other hand, reducing (1.2) is more complicated than reducing 2(cid:107)A−1(cid:107)2
F .

i=1

†Department of Physics, College of William and Mary, Williamsburg, Virginia 23187-8795, U.S.A.
‡Jeﬀerson National Laboratory, 12000 Jeﬀerson Avenue, Newport News, Virginia, 23606, U.S.A.
§Department of Computer Science, College of William and Mary, Williamsburg, Virginia 23187-8795, USA

1

From (1.2), it is clear that large oﬀ diagonal elements of A−1 slow down the convergence of the MC
estimator. Therefore, it would be beneﬁcial if the largest elements of A−1 were removed with some deter-
ministic process, and MC were allowed to converge on the remaining matrix. Several such variance reduction
mechanisms have been discussed in the past. One idea is to use Hadamard (instead of random) vectors which
annihilate the contribution of speciﬁc diagonals of A−1 [6]. This works only if the large magnitude elements
of A−1 happen to be on those diagonals.

An extension of that idea is probing [23]. If the sparsity pattern of A is known, graph coloring can be
used to generate a number of vectors equal to the number of colors that, if multiplied by A, reveal the exact
diagonal of A. However, A−1 is dense in general so it is only meaningful to apply probing to a sparsiﬁed
version of A−1; one that keeps the largest elements of A−1. A large class of matrices (including the Dirac
i,j | as a function of the graph
operator in Lattice QCD) exhibit an exponential decay of the elements |A−1
theoretical distance dist(i, j) in the graph of A. For such matrices, the non-zero structure of Ak, k = 1, 2, . . .,
provides a natural sparsiﬁcation pattern for A−1. Although the actual elements of A−1 need not be known,
coloring the graph of Ak provides the appropriate probing vectors that annihilate the elements of A−1 that
reside on the structure of Ak. This coloring is equivalent with a distance k coloring of A. The larger the
distance, the more elements of A−1 are captured, and therefore removed from the variance. This was followed
in [23] but can be expensive for any but the shortest distances.

A similar scheme called dilution has been used in Lattice QCD [4, 17]. There, the Dirac matrix is a
discrete diﬀerential operator deﬁned on a regular 4D lattice, where each lattice site has 12 degrees of freedom
(representing a dimensionality of 4 for the “spin” space and 3 for the “color” space). Therefore, partitioning
the lattice in a red-black checkerboard can be used to remove the contribution to the variance from nearest
neighbor connections in A−1. Moreover, practitioners often “dilute” the spin and color components as well.
Clearly, this is equivalent to probing for distance 1 and has provided a good reduction of variance. In [4]
more elaborate dilution patterns have also been explored.

√

Hierarchical probing (HP) extends the idea of probing and dilution to all distances k = 2i up to the
diameter of the lattice and forces the colorings to be nested [20]. This nesting allows us to reuse the linear
system solutions with probing vectors from shorter distances, if higher accuracy is required in the Hutchinson
method. These solutions cannot be reused with classical probing. In addition, the HP computational cost
for coloring lattices for all possible distances is negligible and so is the cost for generating the probing vectors
using appropriate permutations of Hadamard or Fourier matrices. HP removes error contributions from the
largest elements of A−1 incrementally and thus, for not too ill-conditioned matrices, achieves an error that
scales as O(1/s) instead of O(1/
s) in MC. In Lattice QCD calculations, we have observed close to an order
of magnitude variance reduction with HP [20, 10].
In this paper we study the eﬀect of deﬂation using a partial singular value decomposition (SVD) of A
in reducing the variance of the Hutchinson method. For ill conditioned matrices, the A−1 is dominated by
the near null singular space of A, and thus does not display the decaying properties of its elements on which
probing is based. We expect that by deﬂating this dominant near null space the variance is reduced, and the
nearest neighbor connections can again be exploited by probing. For linear systems, deﬂation of the smallest
singular triplets of A improves the condition number of A and thus speeds up iterative methods. Similarly,
if Hutchinson is used with Gaussian random vectors, because of the optimality properties of SVD, deﬂation
reduces the variance 2(cid:107)A−1(cid:107)2

F . However, the situation for (1.2) is more complicated.

Our ﬁrst contribution is an analysis of the eﬀects of SVD deﬂation to the variance of the Hutchinson
method for general matrices. We ﬁnd that variance is reduced only if the singular values grow at a fast rate
from small to large. To simplify the rather complicated formulas, we then assume that the singular vectors
are random unitary matrices, which is approximately true for many large scale matrices and especially for
our Lattice QCD application. This leads to concise formulas for the deﬂated variance and to the remarkable
observation that deﬂation may increase variance for Hermitian matrices but not for non-Hermitian ones. The
formulas can be used as a model for predicting and quantifying the beneﬁts of deﬂation. Experimentation
shows that the model is robust even when the singular vectors are not random.

Our second contribution is the combination of deﬂation and HP in Lattice QCD calculations, where
the computation of the trace of the inverse of the Dirac operator is required. The resulting matrix is large
(typical problems have matrices of dimension O(107)) and ill-conditioned, which creates many problems for
both HP and deﬂation. The combination of the two methods has a much bigger eﬀect than each method

2

alone, reducing variance in our test problem by a factor of over 150 compared to MC. We explain this
synergy theoretically and provide a thorough experimental analysis. We also describe how we integrated and
tuned two state-of-the-art software packages to solve our problem eﬃciently on the Cray supper-computers at
NERSC (Edison). The ﬁrst is the PRIMME eigenvalue library [21] and the second is an Algebraic Multigrid
preconditioner developed for Lattice QCD [3], which provides an average speedup of 30 over unpreconditioned
eigensolvers. With these new methods and software we can now address eﬃciently extremely challenging
lattice problems.

2. The eﬀect of deﬂation on variance. Deﬂation is typically understood as the removal of a certain
eigenvector space from the range of an operator [18]. The part of the spectrum removed is problem dependent.
For example, to speed up iterative methods for linear systems we remove the smallest magnitude eigenvalues,
while for variance reduction for Tr(A) we remove the largest magnitude. In this paper we focus on deﬂation
based on the space from the singular value decomposition of A.

Let A be a non-Hermitian matrix and assume without loss of generality that we seek the Tr(A) (A could
be the inverse or some other function of our matrix). Let A = U ΣV T be the singular value decomposition
(SVD) of A, and U1, V1, Σ1 the k largest singular triplets. If U = [U1, U2], V = [V1, V2], and Σ = diag(Σ1, Σ2),
A can be decomposed as

(2.1)

A = U1Σ1V H

1 + U2Σ2V H

2 ≡ AD + AR,

and Tr(A) = Tr(AD) + Tr(AR). If the triplet U1, V1, Σ1 has been pre-computed, using the cyclic property of
the trace, we can explicitly compute Tr(AD) = Tr(Σ1V H
1 U ) with only O(kN ) operations. What remains is
to compute Tr(AR), the trace of our matrix projected on the remaining singular vectors, AR = A(I − V1V H
1 ).
This can be estimated stochastically. Because AD is the best k-rank approximation of A in the least squares
sense, one would expect that the variance on AR will always be smaller than (1.2). We show next that this
only happens under certain conditions.
Theorem 2.1. Let (σ1 ≥ . . . ≥ σN ≥ 0, U , V ) be the singular triplets of A and ∆ = (U (cid:12) ¯V )H (U (cid:12) ¯V ),
where (cid:12) is the elementwise product of matrices, and ¯V is the elementwise conjugate of V . This gives,

(2.2)

∆ml =

¯uimvimuil¯vil,

m, l = 1, . . . , N.

Consider the decomposition in (2.1) produced by deﬂating the largest k singular triplets. The variance of the
stochastic estimator for Tr(AR) satisﬁes,

N(cid:88)

i=1

m=k+1

N(cid:88)

σ2

N(cid:88)

m − N(cid:88)
m(1 − ∆mm) − k(cid:88)

m=k+1

l=k+1

σ2

k(cid:88)

σmσl∆ml.

N(cid:88)

(2.3)

1
2

Var(t(AR)) =

The variance of the stochastic estimator for Tr(A) follows (2.3) with k = 0. Then, their diﬀerence is

(2.4)

(Var(t(A)) − Var(t(AR))) =

1
2

m=1

m=1

l=m+1

σmσl(∆ml + ∆lm).

Proof. Using the MATLAB diag() operator, deﬁne the vectors D = diag(A) and DR = diag(AR) and
the traceless matrices ˜A = A− diag(D) and ˜AR = AR − diag(AR). According to (1.2), we need to obtain an
expression for Var(t(A)) − Var(t(AR)) = 2((cid:107) ˜A(cid:107)2

F ). From the properties of the SVD we have

F − (cid:107) ˜AR(cid:107)2

(2.5)

(cid:107)A(cid:107)2

F =

(cid:107)AR(cid:107)2

F =

i = (cid:107) ˜AR(cid:107)2
σ2

F + (cid:107)DR(cid:107)2
F .

N(cid:88)

i=1

i = (cid:107) ˜A(cid:107)2
F + (cid:107)D(cid:107)2
σ2
F ,
N(cid:88)

D(i) =

σmuim¯vim,

m=1

N(cid:88)

i=k+1

N(cid:88)

m=k+1

DR(i) =

3

σmuim¯vim.

Next we represent the diagonals in terms of the SVD

(2.6)

Then, using (2.2) we obtain expressions for the norms of the diagonals,

(cid:107)D(cid:107)2

i=1((cid:80)N
F = (cid:80)N
m=1 σm ¯uimvim)((cid:80)N
(cid:80)N
= (cid:80)N
m=k+1 σm ¯uimvim)((cid:80)N
i=1((cid:80)N
F = (cid:80)N

l=1 σluil¯vil) =(cid:80)N

(cid:80)N
l=k+1 σluil¯vil) =(cid:80)N

l=1 σmσl∆ml.

m=1

m=1

(cid:107)DR(cid:107)2

(2.7)

(2.8)

(cid:80)N
(cid:80)N

m=k+1

l=k+1 σmσl∆ml.

l=1 σmσl

i=1 ¯uimvimuil¯vil

Utilizing the above and according to (1.2) and (2.5), the variance of the trace estimator for the deﬂated
problem is given by (2.3), and similarly for the original problem with k = 0. To obtain the diﬀerence of
these variances we denote sml = σmσl∆ml for brevity and perform the required algebraic operations,
2 Var(t(A)) − 1

l=k+1 sml)

m=1

1

2 Var(t(AR)) = (cid:80)k
= (cid:80)k
= (cid:80)k
= (cid:80)k
= (cid:80)k
= (cid:80)k

m=1 σ2
m=1 σ2
m=1 σ2
m=1 σ2
m=1 σ2
m=1 σ2

m=1

m=k+1((cid:80)N
l=1 sml −(cid:80)N
(cid:80)N
l=1 sml −(cid:80)N
m −(cid:80)k
(cid:80)k
(cid:80)N
l=1 sml −(cid:80)N
m −(cid:80)k
l=1 sml +(cid:80)N
m −(cid:80)k
m=1((cid:80)N
m=1((cid:80)k
l=1 sml +(cid:80)N
m −(cid:80)k
l=1,l(cid:54)=m sml +(cid:80)N
m=1((cid:80)k
m −(cid:80)k
m=1 smm −(cid:80)k
m(1 − ∆mm) −(cid:80)k
(cid:80)N
F − (cid:107) ˜AR(cid:107)2

m=k+1
l=k+1 slm)
l=k+1(sml + slm))

l=m+1(sml + slm),

l=1 sml

m=1

which yields the desired result.

Example. To achieve variance reduction, we need (cid:107) ˜A(cid:107)2

F > 0. Contrary to low rank matrix
approximations, deﬂation may not achieve this for Var(t(AR)). Consider the following example in MATLAB:

l=k+1(sml + slm))

[U,~] = qr([ -1 d d; d 1 1; d 1 -1]);
A = U*diag([1+2*s, 1+s, 1])*U’;
Ar = U(:,2:3)*diag([1+s, 1])*U(:,2:3)’;
disp(norm(A-diag(diag(A)),’fro’)^2/norm(Ar-diag(diag(Ar)),’fro’)^2);

With d, we control the distance of the U(:,1) and U(:,2:3) singular subspaces (also eigenspaces) from the
ﬁrst orthocanonical vector. With s, we control the separation of the singular values. We deﬂate the largest
singular triplets. For s = 0.5 (i.e., singular values [2, 1.5, 1]), we can verify numerically that Var(t(Ar)) ≥
Var(t(A)) for any d. Deﬂation has a negative eﬀect! Similarly, if d = 0.001 there is no reduction of variance
regardless of the separation of the singular values. On the other hand, for s = 1 (i.e., singular values [3, 2, 1]),
we have Var(t(Ar)) ≥ Var(t(A)) for d ≤ 1 and Var(t(Ar)) < Var(t(A)) for d > 1. Finally, for s = 2 (i.e.,
singular values [5, 3, 1]) deﬂating the largest triplet reduces variance for all d.
Theorem 2.1 diﬀers in two ways from the typical SVD based low rank approximations. The ﬁrst is the
(1 − ∆mm) factor on the sum of σ2
m in (2.4). If ∆mm ≈ 1 then A is almost decomposable and therefore
deﬂation will not remove any oﬀ-diagonal elements from the variance. However, in this uncommon case, the
deﬂated triplet did not contribute to the variance in the beginning. The second diﬀerence is the subtraction
of the double summation term. The hope is that the deﬂated singular values are suﬃciently large to dominate
over the summation of σmσl. However, this is complicated by the presence of the ∆ml.
We attempt to analyze the condition for variance reduction further. If we rewrite (2.4) as, Var(t(A)) −
l=m+1 σl(∆ml + ∆lm)), we observe that for the diﬀerence to be

m=1 σm(σm(1− ∆mm)−(cid:80)N

Var(t(AR)) = 2(cid:80)k

positive, a suﬃcient but not necessary condition is for every term in the sum to be positive. Equivalently,

σl

(∆ml + ∆lm)
(1 − ∆mm)

,

(2.9)

σm >

m = 1, . . . , k.

l=m+1

m=1 ∆ml =(cid:80)N

To simplify further, assume that U = V , i.e., the matrix is Hermitian. Then, ∆ml =(cid:80)N
l=1 ∆ml = 1, or ∆ is doubly stochastic. For k = 1, we have(cid:80)N
and therefore(cid:80)N

i=1 |uim|2|uil|2 ≥ 0,
l=2 ∆1l = 1−∆11
(∆1l+∆l1)
and thus if σ1 > 2σ2, then σ1 > 2σ2
(1−∆11) . So if σ1 > 2σ2 then
deﬂating the largest singular triplet reduces the variance. Inductively, if the singular spectrum decreases
geometrically as σi+1 = 2−iσ1, we guarantee that any deﬂation improves variance. This requirement,
however, is too pessimistic. In the following we show that if U and V are random unitary matrices, which
is approximately true in our LQCD application, the ∆ml are uniformly small with small variance.

(1−∆11) > (cid:80)N

(cid:80)N

1−∆11
1−∆11

= 2σ2

l=2 σl

∆1l

l=2

N(cid:88)

4

2.1. The case of random singular vectors. Let us assume that U and V are standard unitary
matrices, i.e., distributed with the Haar probability measure [12].
In LQCD, the U and V are indeed
random matrices, although they do not follow exactly the above distribution. Our resulting model, however,
will not depend on the nature of the distribution but on the expectation and variance of the elements of U
and V , based on which we will provide a bound for the elements of ∆ as a random variable. Speciﬁcally, we
Proposition 2.2. [12, Prop. 4.2.2]. Let l ∈ N, i1, . . . , il, j1, . . . , jl ∈ {1, . . . , N} and k1, . . . , kl, m1, . . . , ml
jr=i(kr − mr) (cid:54)= 0 for some 1 ≤ j ≤ N , then
¯uml
iljl

base our subsequent analysis on |∆ml − E(∆ml)| = O((cid:112)Var(∆ml)). First we need the following.
∈ Z+. If either(cid:80)

ir=i(kr − mr) (cid:54)= 0 for some 1 ≤ i ≤ N or(cid:80)

)··· (ukl

)) = 0.

E((uk1
i1j1

¯um1
i1j1

)(uk2
i2j2

¯um2
i2j2

iljl

Then we have the following.

Lemma 2.3. For an N × N standard unitary matrix U = [uij] it holds:

(2.10)

(2.11)

(2.12)

(2.13)

(2.14)

(2.15)

(2.16)

(2.17)

(i (cid:54)= k),
(i (cid:54)= k),

E(uij) = 0,
E(uij ¯ukj) = 0
E(uij ¯ukjuim ¯ukm) = 0
E(|uij|2) =
1
N
E(|uij|4) =
E(|uij|8) =

,

2

N (N + 1)

,

4!

N (N + 1)(N + 2)(N + 3)

,

E(|uij|2|umj|2) = E(|uij|2|uik|2) =
N (N + 1)
E(|uij|4|umj|4) = E(|uij|4|uik|4) = E(|uij|8)/6

1

(i (cid:54)= m, j (cid:54)= k),
(i (cid:54)= m, j (cid:54)= k).

Proof. Using Proposition 2.2 with l = 1, k1 = 1, m1 = 0, i1 = i, and j1 = j gives (2.10).
Using the same proposition with l = 2, i1 = i, j1 = j2 = j, i2 = k, m1 = 0, k1 = 1, m2 = 1, k2 = 0 and

picking ir = i1 = i, gives (2.11).
m1 = m3 = 0, m2 = m4 = 1 and picking ir = i with r ∈ {1, 3} gives (2.12).

Choosing l = 4, i1 = i3 = i, j1 = j2 = j, i2 = i4 = k, j3 = j4 = m, k1 = k3 = 1, k2 = k4 = 0 and

Equations (2.13), (2.14), (2.16) are borrowed from [12, Prop.4.2.3, p.138]. Lemma 4.2.4 in [12] states:

(2.18)

E(|Uij|2k) =

(cid:18) N + k − 1

N − 1

(cid:19)−1

.

By setting k = 4 we obtain (2.15). The derivation of (2.17) is based on the proof of Proposition 4.2.3 in
the above book but is more tedious. We exploit the idea that uim and (uim cos θ + ujm sin θ) are identically
distributed, and consequentially have the same expectations and moments. After algebraically expanding
the eighth moment of the absolute value, we observe that most terms vanish because of Proposition 2.2. The
surviving terms are the following:

E(|uim|8) = E(|uim cos θ + ujm sin θ|8)

= E(|uim|8 cos8 θ) + E(|ujm|8 sin8 θ) + 16E(|uim|2|ujm|6 cos2 θ sin6 θ)
+16E(|uim|6|ujm|2 cos6 θ sin2 θ) + 36E(|uim|4|ujm|4 cos4 θ sin4 θ).

(2.19)
First, we note that E(|uim|8) = E(|ujm|8) and similarly E(|uim|2|ujm|6) = E(|uim|6|ujm|2). Then by simple
integration on [0, 2π] we obtain, E(cos8 θ + sin8 θ) = 35/64, E(cos2 θ sin6 θ) = E(cos6 θ sin2 θ) = 5/128, and
E(cos4 θ sin4 θ) = 3/128. By substituting in (2.19) we obtain,

(2.20)

29E(|uim|8) = 80E(|uim|2|ujm|6) + 54E(|uim|4|ujm|4).

5

We still need to determine the term E(|uim|2|ujm|6). We follow the same procedure, noting also that ujm
is identically distributed with − sin θuim + cos θujm. By expanding the following moments and canceling
several terms because of Proposition 2.2 we have,
E(|uim|2|ujm|6) = E(|uim cos θ + ujm sin θ|2|ujm cos θ − uim sin θ|6)
jm cos6 θ sin2 θ) + E(u2

im cos2 θ sin6 θ) + E(u8

jm sin8 θ)

= E(u8

imu2

imu6
jm(9 cos4 θ sin4 θ − 6 cos6 θ sin2 θ)) + E(u6
jm(9 cos2 θ sin6 θ − 18 cos4 θ sin4 θ + 9 cos6 θ sin2 θ)).

jm cos8 θ) + E(u6
imu2

+E(u2
+E(u4

imu6
imu4

jm(9 cos4 θ sin4 θ − 6 cos2 θ sin6 θ))

(2.21)

As before, we consolidate all expectations that are the same and integrate to ﬁnd the following expectations:
E(cos2 θ sin6 θ)+cos6 θ sin2 θ) = 5/64, E(cos8 θ+sin8 θ+18 cos4 θ sin4 θ−6 cos6 θ sin2 θ−6 cos2 θ sin6 θ) = 1/2,
and E(9 cos2 θ sin6 θ − 18 cos4 θ sin4 θ + 9 cos6 θ sin2 θ) = 9/32. This yields the following equation:

(2.22)

E(|uim|2|ujm|6) = 5/32E(|uim|8) + 9/16E(|uim|4|ujm|4).

By substituting (2.22) into (2.20) we obtain the desired (2.17).

fourth moment is given by E(u4
E(u2

Remark 1. When U is a real orthogonal matrix the formulas (2.10)–(2.13) and (2.16) hold, but the
ij) = 3/(N 2 + N ). In the proof of the Proposition 4.2.3 in [12, p.139] the term
11 ¯u2
21) = 2/(N 2 +N ).
Remark 2. The results of the above Lemma can also be obtained by physics and combinatorial con-

21) vanishes in the unitary case, but in the real case it becomes 2E(u2

21)+E(¯u2

11u2

11u2

siderations as for example in Lattice QCD [8].

To study ∆ml, we ﬁrst assume that for general non-Hermitian matrices U is statistically independent
from V . We address Hermitian matrices (U = V ) separately. Statistical independence cannot be claimed
between all the elements of U . In fact, we know that the elements of up to k × k submatrices of U , where
√
k = o(
N ) [15]. We will not use this result
because our formulas involve more than O(N ) elements of ∆. Instead, we will ﬁnd the expected value and
variance of elements ∆ml.

N ), can be approximated by independent Gaussians, N (0, 1/

Lemma 2.4. Let ∆ml be deﬁned in (2.2) and assume m (cid:54)= l. For non-Hermitian matrices it holds,

√

(2.23)

(2.24)

(2.25)

(2.26)

E(∆ml) = 0,

Var(∆ml) = 1/(N (N + 1)2),
E(∆mm) = 1/N,
Var(∆mm) = (N − 1)/(N 2(N + 1)2).

= E(((cid:80)N
= (cid:80)N
= (cid:80)N
= (cid:80)N
= E(((cid:80)N
= (cid:80)N
= (cid:80)N

=

Proof. If m (cid:54)= l, (2.11) implies E(∆ml) = (cid:80)N
From (2.13) we have ∆mm =(cid:80)N
i=1 ¯uimuilvim¯vil)((cid:80)N

Var(∆ml) = E(∆ml∆ml) − |E(∆ml)|2

For the variance of non-diagonal elements we have,

i=1 E(¯uimuim)E(vim¯vim) =(cid:80)N

i=1 uim ¯uil¯vimvil))

i=1 E(¯uimuil)E(vim¯vil) = (cid:80)N

i=1 1/N 2 = 1/N .

i,j=1 E(¯uimuilujm ¯ujl)E(vim¯vil¯vjmvjl)
i=1 E(¯uimuiluim ¯uil)E(vim¯vil¯vimvil)
i=1 E(|uil|2|uim|2)E(|vim|2|vil|2) =

1

N (N +1)2

(since E(∆ml = 0)

(i (cid:54)= j terms are 0 from (2.12))
(from (2.16)).

i=1 E(uim ¯uil)E(vim¯vil) = 0.

For the diagonal we use (2.16) and (2.14),

Var(∆mm) = E(∆mm∆mm) − |E(∆mm)|2

i=1 |uim|2|vim|2)((cid:80)N
i=1,j=1,i(cid:54)=j E(|uim|2|ujm|2)E(|vim|2|vjm|2) +(cid:80)N

i=1 |uim|2|vim|2)) − 1/N 2
i=1,j=1 E(|uim|2|ujm|2)E(|vim|2|vjm|2) − 1/N 2
N 2−N

4N

N 2(N +1)2 − 1

N 2 = N−1

N 2(N +1)2 +

N 2(N +1)2 .
6

i=1 E(|umm|4)E(|vmm|4) − 1/N 2

Lemma 2.5. Let ∆ml be deﬁned in (2.2) and assume m (cid:54)= l. For Hermitian matrices (U = V ) it holds,

(2.27)

(2.28)

(2.29)

E(∆ml) = 1/(N + 1),
Var(∆ml) = 2(N − 1)/((N + 1)2(N + 2)(N + 3)),
E(∆mm) = 2/(N + 1),
Var(∆mm) = 4(N − 1)/((N + 1)2(N + 2)(N + 3)).

(2.30)

i=1 |uim|2|uil|2. Moreover, (cid:80)N

Proof. When U = V , ∆ml =(cid:80)N
∆ml ≥ 0, ∆ is a doubly stochastic matrix. From (2.16) we have E(∆ml) =(cid:80)N
From (2.14) we have E(∆mm) = (cid:80)N
Var(∆mm) = E(((cid:80)N
= (cid:80)N

i=1 |uim|4)2) − |E(∆mm)|2 =(cid:80)N
i=1,j=1,i(cid:54)=j E(|uim|4|ujm|4) +(cid:80)N

∆mm, which is larger than of ∆ml. Using (2.15) and (2.18) we have,

i=1 E(|uim|8) − 4/(N + 1)2

m=1 ∆ml =(cid:80)N

l=1 ∆ml = 1, and because
i=1 E(|uim|2|uil|2) = 1/(N +1).
i=1 E(|uim|4) = 2/(N + 1). We turn our attention to the variance of

i=1,j=1 E(|uim|4|ujm|4) − 4/(N + 1)2

= (N 2 − N )E(|uim|8)/6 + N E(|uim|8) − 4/(N + 1)2
= 4(N + 5)/((N + 1)(N + 2)(N + 3)) − 4/(N + 1)2 = 4(N − 1)/((N + 1)2(N + 2)(N + 3)).

A similar but lengthier strategy yields also Var(∆ml).

use the variance of the elements of ∆ to ascertain their magnitude as |∆ml − E(∆ml)| = O((cid:112)Var(∆ml)).

We are now able to characterize the eﬀect of deﬂation on matrices with random singular vectors. We
The above results show that in the non-Hermitian case |∆ml| = Θ(1/N 1.5), while in the Hermitian case
|∆ml − 1/(N + 1)| = Θ(1/N 1.5), and thus ∆ml = Θ(1/N ). Using these formulas for ∆ml and ∆mm, we
revisit the suﬃcient but pessimistic condition of (2.9) for non-Hermitian and Hermitian matrices,

(cid:82) N

(2.31) non-Hermitian: σm > Θ(

1
N 1.5 )

σl,

Hermitian: σm >

1

N − 1

σl,

m = 1, . . . , k.

√
non-Hermitian: σi = Θ(

We are seeking the singular value distributions that would satisfy (2.31). If we model the summations as
m+1 σ(x)dx, we can readily verify that the least decaying series that satisfy the inequalities for all m are

(2.32)

N − i + 1),

Hermitian: σi = Θ(N − i + 1).
in other words
It is remarkable that it is harder for Hermitian matrices to achieve variance reduction;
the deﬂated singular values must decay much faster (have larger separations) to achieve the same variance
reduction as in a non-Hermitian matrix. On the other hand, the ∆mm and ∆ml are positive and larger for
Hermitian than non-Hermitian matrices, which implies that the subtracting term in (2.4) is always larger
for Hermitian matrices. Therefore, a Hermitian matrix is expected to have lower starting variance than a
non-Hermitian matrix with the same singular spectrum. We conclude that although non-Hermitian matrices
outperform Hermitian ones in variance reduction, it is because they have more variance to reduce.

The above analysis is intuitively useful, but dependent on the pessimistic condition (2.9). The following
theorem gives the expected variance of our trace estimator as an expression of only the mean and variance of
the singular values. Because of the small variance of the ∆ml elements in Lemma 2.5, the expected variance
is very accurate.

Theorem 2.6. Deﬁne the mean and the variance of the N−k singular values of AR, µk = 1
N−k

(cid:80)N
m=k+1(σm − µk)2, respectively. Then, for non-Hermitian matrices it holds

and Vk = 1

N−k

(cid:80)N

m=k+1 σm,

N(cid:88)

l=m+1

N(cid:88)

l=m+1

and for Hermitian matrices,

1
2

E(Var(t(AR))) = (N − k)(1 − 1
N

)(Vk + µ2
k)

(cid:18)

(cid:19)

.

E(Var(t(AR))) = (N − k)

1
2

7

Vk

N

N + 1

+ µ2
k

k

N + 1

In addition, the relative standard deviation of our variance estimator, Var(t(AR)), is bounded by

Proof. First note that Vk = 1

N−k

m=k+1 σ2

m,l=k+1 σmσl

StdDev(Var(t(AR)))

E(Var(t(AR)))

(cid:16)(cid:80)N

≤ O(

(cid:80)N
m − 1
N−k
N(cid:88)
N(cid:88)

1

N − k

m=k+1

l=k+1

N − k
N 1.5 ).

(cid:17)

, which gives

σmσl = (N − k)Vk + (N − K)µ2
k.

N(cid:88)

m=k+1

(2.33)

m = (N − k)Vk +
σ2
N(cid:88)

Taking expectation values in (2.3) we have,

(2.34)

1
2

E(Var(t(AR))) =

m − E(∆mm)
σ2

N(cid:88)

m=k+1

m − E(∆ml)
σ2

N(cid:88)

N(cid:88)

m=k+1

l=k+1,l(cid:54)=m

σmσl.

m=k+1

2 E(Var(t(AR))) =(cid:80)N

1

m=k+1 σ2

Then, for non-Hermitian matrices (2.34), (2.33) and Lemma 2.5 yield,

Similarly, for Hermitian matrices we have,

2 E(Var(t(AR))) = (cid:80)N
= (cid:80)N

1

m=k+1 σ2
m=k+1 σ2

m − 2
m(1 − 1
= (N − k)Vk(1 − 1
= (N − k)Vk(1 − 1

N +1

k).

N )(Vk + µ2

m(1 − 1
N ) = (N − k)(1 − 1
(cid:80)N
(cid:80)N
(cid:80)N
(cid:80)N
(cid:80)N
m − 1
m=k+1 σ2
m=k+1
N +1 ) − 1
l=k+1 σmσl
N +1 ) − (N−k)2
k(1 − 1
N +1 ) + (N − k)µ2
N +1 ) + k N−k
N +1 µ2
k.

m=k+1

N +1

N +1

N +1 µ2

k

l=k+1,l(cid:54)=m σmσl

To gauge the accuracy of the above estimation, we need to compute the variance of our variance approxi-
mation. First note that for both non-Hermitian and Hermitian matrices, Var(∆ml) ≤ c/N 3, with c being
the maximum of the variance constants in Lemmas 2.4 and 2.5. Then, using the rule for the variance of the
sum of random variables we get,

4 Var(Var(t(AR))) = Var((cid:80)N
≤ (cid:80)N

1

(cid:80)N

m=k+1

(cid:80)N
l=k+1 σmσl∆ml) = (cid:80)N

(cid:112)Var(∆ij) Var(∆ml) ≤ c

i,j=k+1

N 3 ((cid:80)N

(cid:80)N

m,l=k+1 σiσjσmσl

m,l=k+1 σiσjσmσl Cov(∆ij, ∆ml)

m,l=k+1 σmσl)2

i,j=k+1

= c(N−k)4

N 3 µ4
k.

Since E(Var(t(AR))) ≥ (N − k)(Vk + µ2
relative error of using E(Var(t(AR))) instead of Var(t(AR)) can be bounded as,

k) ≥ (N − k)µ2

k for both non-Hermitian and Hermitian matrices, the

(cid:113) 1

4 Var(Var(t(AR)))
1
2 E(Var(t(AR)))

≤

√
c(N−k)2
µ2
k
N 1.5
(N − k)µ2

k

√

≤

c(N − k)
N 1.5

.

Remark 3. The bound on the relative error on the estimator is pessimistic. In fact, using the techniques
in Lemmas 2.4 and 2.5 we could prove that the upper bound is O(1/N ), which also agrees with experimental
observations. However, such complexity is unnecessary as our goal is simply to show that our model for
Var(t(AR))) is suﬃciently accurate for large N .
Hutchinson estimator: (N − 1)(V0 + µ2

Remark 4. By setting k = 0 in Theorem 2.6 we obtain expressions for E(V ar(t(A))), the undeﬂated

0) for non-Hermitian A and N 2V0/(N + 1) for Hermitian A.

Remark 5. Our original assumption that the singular vector matrices are random unitary is not required
by Theorem 2.6. It is suﬃcient that the elements of ∆ml have expectation values and variances as given by
Lemmas 2.4 and 2.5.
Corollary 2.7. For non-Hermitian matrices and for any 1 ≤ k ≤ N , E(Var(t(AR))) ≤ E(Var(t(A))).

(cid:80)k

0 − (N−k)2

N 2 µ2

k < 1
N

i=1 σ2
i .

For Hermitian matrices, the expected deﬂated variance reduces only if µ2

8

E(Var(t(AR)))
E(Var(t(A)))

=

(cid:80)N

(N − k)(1 − 1

N )(Vk + µ2
k)

(N − 1)(V0 + µ2
0)
k = 1
N−k

(cid:80)N

(N − k)(Vk + µ2
k)
N (V0 + µ2
0)

=

≤ 1.

Proof. Based on Theorem 2.6 and Remark 4, we want the ratio of deﬂated to undeﬂated variance

i=1 σ2

0 = 1
N

i . and Vk + µ2

i=k+1 σ2
Note that V0 + µ2
expected value of their squares will also be larger and thus V0 + µ2

i . Because σm ≥ σl, m ≤ k, l > k, the
0 > Vk + µ2
(cid:80)N
k, which proves the inequality.
For Hermitian cases the ratio of deﬂated to undeﬂated variance becomes
(cid:80)N

(N − k)(N Vk + kµ2
k)

E(Var(t(AR)))
E(Var(t(A)))

i=k+1 σ2
i=1 σ2

i − (N−k)2
i − N µ2

0

N µ2

k

N V0

=

=

.

Requiring that the above ratio is less than one yields the desired result.

Corollary 2.7 shows that for non-Hermitian matrices the condition (2.32) on the decay of the singular
values is unnecessary—deﬂation will always reduce the variance. This is not guaranteed for Hermitian
matrices, for which condition (2.32) seems to be valid, as we show experimentally in the next section.

Although the above corollary is qualitative, Theorem 2.6 facilitates a quantitative prediction of the
outcome of deﬂation based solely on the the variance and the expectation of the undeﬂated singular values.
Often, users have some idea of the singular spectrum of their matrix and thus can decide not only if deﬂation
works, but also how many singular triplets to deﬂate and what the expected beneﬁt will be. Moreover, as
we show in our numerical experiments, the estimates based on our formulas are extremely robust even when
the eigenvectors are not random unitary matrices.

3. The eﬀect of the singular spectrum. Having factored out the eﬀects of the singular vectors, we
now study the eﬀect of the singular value distribution using the previous theory to predict actual experiments.
Clearly, the larger the gap between deﬂated and undeﬂated singular values, the larger the reduction in (2.4).
In the following experiments we study the eﬀect of deﬂation for six diﬀerent model distributions of σi.

Given a diagonal matrix of singular values Σ, we generate a pair of random unitary matrices U and V , and
construct one Hermitian matrix U ΣU H and one non-Hermitian matrix U ΣV H . For each model distribution
we construct matrices of several sizes. We report the ratio of the variance of the deﬂated matrix, where we
deﬂate various percentages of its largest singular triplets, to the variance of the original undeﬂated matrix.
This can be computed explicitly from (1.2), or through our model in Theorem 2.6. As statistically expected,
beyond small matrices of dimension less than 100, there is perfect agreement between our model predictions
and experimentally determined variances. Thus, we only present results from our model.

In Figure 3.1a we consider a model where the singular values increase at a logarithmic rate with respect
to their index. As Corollary 2.7 predicts, for Hermitian matrices the variance increases with the number
of deﬂated singular triplets, and the problem is more pronounced with larger matrix size. Although for
non-Hermitian matrices the ratio is always below one, it requires deﬂating a substantial part of the spectrum
to reduce the variance appreciably. In Figure 3.1b the spectrum increases as the square root of the index,
and the eﬀects of deﬂation, although improved, still are not beneﬁcial for Hermitian matrices.

In Figures 3.2a, 3.2b, and 3.2c the growth of the singular values is linear, quadratic, and cubic, respec-
tively. The ratio is now below one for both types of matrices (conﬁrming the condition (2.32) for Hermitian
matrices). We can see that with larger growth rates, the variance reduction is larger for a particular fraction
of singular values deﬂated. Additionally, for suﬃciently large growth rates, the diﬀerence between Hermitian
and non-Hermitian matrices vanishes.

√
For spectra that decay as a rational polynomial, the picture is diﬀerent. Figure 3.3a shows an example
i. There are a few large singular values but the rest do not reduce appreciably.
where the spectrum is σi = 1/
The eﬀect of this is that Hermitian matrices experience larger relative improvement with deﬂation over non-
Hermitian matrices. We have observed this eﬀect also for other rational polynomials, 1/ip, but the diﬀerence
i. This observation is particularly relevant to our problem of
between matrix types seems to peak at the 1/
ﬁnding the trace of the inverse of a matrix. In Figure 3.3b we study the spectrum of the inverse of the discrete
Laplacian, a common problem that also has some of the features of our target QCD problem at the free ﬁeld
limit. We see signiﬁcant variance reduction, especially as the lattice size grows. The Hermitian matrices
continue to have an advantage over non-Hermitian matrices, but the diﬀerence for practical problems is
negligible.

√

9

(a) log

(b) square root

Fig. 3.1: On the left is a logarithmic spectrum: σN−i+1 = 1 + 2 · log(i). On the right is a square root
spectrum: σN−i+1 =
i. The dotted red line in both plots is a constant line at y = 1. Points below this line
signify an improvement in variance with deﬂation. Points above the line denote a deﬂated operator with a
higher Frobenious norm than the original matrix, a case in which deﬂation is hurtful and variance increases.

√

(a) linear

(b) quadratic

(c) cubic

Fig. 3.2: Variance reduction ratios for matrices with spectra with linear, σN−i+1 = i, quadratic, σN−i+1 = i2,
and cubic, σN−i+1 = i3, growth rates.

10

0.01%0.1%1%10%100%024681012141618PercentoflargestsingularvectorsdeﬂatedVariance(deflated) / Variance(undeflated)  non−Hermitian N=100non−Hermitian N=10000Hermitian N=100Hermitian N=100000.01%0.1%1%10%100%00.20.40.60.811.21.41.61.8PercentoflargestsingularvectorsdeﬂatedVariance(deflated) / Variance(undeflated)  non−Hermitian N=100non−Hermitian N=10000Hermitian N=100Hermitian N=100000.01%0.1%1%10%100%00.20.40.60.81PercentoflargestsingularvectorsdeﬂatedVariance(deflated) / Variance(undeflated)  non−Hermitian N=100non−Hermitian N=10000Hermitian N=100Hermitian N=100000.01%0.1%1%10%100%00.20.40.60.81PercentoflargestsingularvectorsdeﬂatedVariance(deflated) / Variance(undeflated)  non−Hermitian N=100non−Hermitian N=10000Hermitian N=100Hermitian N=100000.01%0.1%1%10%100%00.20.40.60.81PercentoflargestsingularvectorsdeﬂatedVariance(deflated) / Variance(undeflated)  non−Hermitian N=100non−Hermitian N=10000Hermitian N=100Hermitian N=10000√
(a) 1/

i

√

(b) Laplacian

√

i. The right plot shows the deﬂation of the inverse of a 2D

Fig. 3.3: On the left we have spectrum σi = 1/
discrete Laplacian on a grid

N × √

N with Dirichlet boundary conditions.

11

Percentoflargestsingularvectorsdeﬂated0.01%0.1%1%10%100%Variance(deflated) / Variance(undeflated)00.10.20.30.40.50.60.70.80.91non-Hermitian N=100non-Hermitian N=10000Hermitian N=100Hermitian N=100000.01%0.1%1%10%100%10−510−410−310−210−1100PercentoflargestsingularvectorsdeﬂatedVariance(deflated) / Variance(undeflated)  non−Hermitian N=256non−Hermitian N=1024non−Hermitian N=16384Hermitian N=256Hermitian N=1024Hermitian N=163844. Experiments on general matrices. The previous section studied the eﬀect of spectra on matrices
with random unitary singular vectors.
In this section we investigate the extent to which our theory is
applicable to general matrices with singular vectors that are not random. We choose four matrices from the
University of Florida sparse matrix collection [9] with relatively small sizes (675–2000) that are derived from
real world problems in various ﬁelds, such as chemical transport modeling and magnetohydrodynamics. In
all our results we deﬂate the estimator of the trace of A−1.

(a) BWM2000

(b) MHD1280B

Fig. 4.1: Matrix BWM2000 has a size of N = 2000 and condition number of 2.37869e+5. Matrix MHD1280B
has N = 1280 and a condition number of 4.74959e+12. Both matrices are real, non-symmetric.

(a) NOS6

(b) OLM1000

Fig. 4.2: Matrix NOS6 is symmetric, with N = 675 and condition number of 7.65049e+06. Matrix OLM1000
is non-symmetric, with N = 1000 and a condition number of 1.48722e+06.

For Figures 4.1a and 4.1b, the model and experimental results agree very closely. Both demonstrate
dramatic variance reduction even when deﬂating a small fraction of the SVD space. Both matrices have
a high condition number implying that their small singular values contribute most of the variance of the
matrix inverse estimator. Thus it pays to remove them.

12

0.01%0.1%1%10%100%10−1010−810−610−410−2100PercentoflargestsingularvectorsdeﬂatedVariance(deflated) / Variance(undeflated)  ModelExperiment0.01%0.1%1%10%100%10−3010−2510−2010−1510−1010−5100PercentoflargestsingularvectorsdeﬂatedVariance(deflated) / Variance(undeflated)  ModelExperiment0.01%0.1%1%10%100%00.511.5PercentoflargestsingularvectorsdeﬂatedVariance(deflated) / Variance(undeflated)  ModelExperiment0.01%0.1%1%10%100%00.20.40.60.81PercentoflargestsingularvectorsdeﬂatedVariance(deflated) / Variance(undeflated)  ModelExperimentIn contrast, deﬂation does not improve the variance for the matrix in Figure 4.2a, unless almost the
entire spectrum is deﬂated. For deﬂating less than 10% of the singular triplets—the most realistic situation—
model and experimental results agree. Beyond that number, the experiment performs worse than predicted.
However, the model still captures the overall eﬀect and recommends avoiding deﬂation altogether. In Figure
4.2b the eﬀect of deﬂation is beneﬁcial but limited. The disagreement between model and experiment is
about 10%, and therefore the model can be used to predict the outcome eﬀectively.

In summary, the presence of non random singular vectors could generate a few discrepancies, but these
and other extensive experiments show that our model is useful in predicting the overall eﬀect of deﬂation.
Speciﬁcally, even a rough knowledge of the particular singular value spectrum can help us determine whether
deﬂation would be valuable or hurtful. Finally, we emphasize the small sizes of the above matrices. In large
real world problems, the singular vectors are more likely to behave like random ones.

5. Application of deﬂation to Lattice QCD. In LQCD, we may assume that the singular vectors of
the Dirac operator are approximately random, uniformly distributed unitary matrices. This is justiﬁed by the
random matrix theory [19, 24] approximation to QCD. In this approximation, the Dirac matrix is replaced by
a random matrix with a suitable probability distribution that satisﬁes the fundamental symmetries of QCD.
It has been shown that this approach explains the numerically observed spectral density of the Dirac matrix
very well [24]. Therefore, we expect that our model should capture the essential properties of deﬂation on
the stochastically estimated trace 1.

5.1. How to obtain the deﬂation space. We are interested in the trace of the inverse of a matrix,
so we need to compute its smallest singular triplets. Then, we apply the Hutchinson method on the deﬂated
matrix by solving a series of linear systems of equations.
It would have been desirable to compute the
deﬂation space from the search spaces built by the iterative methods for solving these linear systems. This
idea has been explored eﬀectively for Lattice QCD in the past [16, 22, 1]. However such methods are not
suitable for our current problem for the following reasons.

First, methods such as GMRESDR or eigBICG produce approximations to the lowest magnitude eigen-
values of the non Hermitian matrix A. Much experimentation has shown that this eigenspace is not eﬀective
as deﬂation for reducing the variance of the Hutchinson method. To produce the smallest singular triplets
we would have to work with eigCG on the normal equations AH A [22]. Second, only the lowest few eigen-
pairs produced by eigCG are accurate. The rest may have a positive eﬀect on speeding up the linear solver,
but they do not seem adequate for variance reduction. Third, and most important, we are interested in
large scale problems for which unpreconditioned eigCG would not converge in reasonable time. However,
if a preconditioner M−1 is used, all the above methods ﬁnd the eigenpairs of M−1A or of M−H M−1AH A.
These may help speed up the linear solver but are not relevant for deﬂating A−1 for variance reduction.

The alternative is to compute the deﬂation space through an explicit eigensolver on AH A. Numerical
diﬃculties arising by using the AH A are not an issue for the relatively low accuracy needed for deﬂation.
This is a challenging problem for our large problem sizes because the lower part of the spectrum becomes
increasingly dense and eigenvalue methods converge slowly. Moreover, to achieve suﬃcient deﬂation power,
hundreds or 1000s of eigenpairs must be computed. Although Lanczos type methods are good for approxi-
mating large parts of the spectrum, they cannot use preconditioning so they are unsuitable for our problems.
We have used the state-of-the-art library PRIMME (PReconditioned Iterative MultiMethod Eigensolver)
[21] which oﬀers a suite of near-optimal methods for Hermitian eigenvalue problems. Among several unique
features, PRIMME has recently added support for solving large scale SVD problems, including precondi-
tioning capability, something that is not directly supported by other current software. In Lattice QCD, a
multi-group, multi-year eﬀort has resulted in a highly eﬃcient preconditioner which is based on domain de-
composition and adaptive Algebraic Multigrid (AMG) [3]. In that community, AMG is a game changer but it
has only been used to solve linear systems of equations. We employ AMG as a preconditioner in PRIMME to
ﬁnd 1000 lowest singular triplets. For most methods in PRIMME, AMG accelerates the number of iterations
by orders of magnitude and results in wallclock speedups of around 30.

To obtain the best performance for our problem we have experimented with various PRIMME methods

1Using a non-uniform distribution of the singular vector matrix, such as the distributions used in [7], similar results can be

obtained.

13

and parameters, and AMG conﬁgurations. A determining factor for these optimizations was the accuracy
with which eigenvectors had to be computed. For high accuracy, PRIMME’s near-optimal method GD+k is
the method of choice, but for the low accuracy that is suﬃcient for variance reduction, i.e., residual tolerance
less than 1e-2 or 1e-3, methods with a large block size are typically more eﬃcient. There are a couple of
reasons for this, besides better cache utilization and lower memory traﬃc. Single vector methods with large
tolerance may misconverge to interior eigenvalues before the exterior ones become visible to the method.
In our problem, the smallest few eigenvalues are O(1e-5) so tolerance has to be smaller than that. Large
block size avoids this problem and, additionally, allows many eigenpairs to converge to much lower residual
norms than the requested tolerance. We experimented with various block sizes and methods and settled to
GD+k with block-size of 30 and a total subspace of 90. This is equivalent to the LOBPCG method with
90 vectors that locks converged eigenvectors out of the basis as they converge. This window approach is
far more eﬃcient than the original LOBPCG, which sets the block size equal to the number of required
eigenvalues. In PRIMME this method can be called directly as LOPBCG Orthobasis Window.

The AMG software provides a solver for a non-Hermitian linear system Ax = b, not just a preconditioner.
There are three levels of multigrid with a GCR smoother at each level [3]. Because PRIMME needs a
preconditioner for the normal equations, AH Aδ = r, each preconditioning application involves two calls to
AMG to solve the two systems approximately, AH y = r and Aδ = y. We found 4 GCR iterations at the ﬁne
level and 5 GCR iterations at each of the two coarse levels to be optimal. Preconditioning for eigenvalue
problems diﬀers from linear systems in the sense that it should approximate (AH A − σI)−1 to improve
eigenvalues near σ. In our AMG preconditioner σ is zero, so we expect the quality of the preconditioner to
wane as we ﬁnd eigenvalues inside the spectrum. However, the lowest part of the spectrum is quite clustered
and as such for multigrid this deterioration is small.

As we discuss in the experiments section, our code was able to eﬃciently produce one thousand eigenpairs

in one of the largest eigenvalue calculations we performed in Lattice QCD.

5.2. Deﬂating the trace method and combining with Hierarchical Probing. Given m eigen-
pairs (Λ, V ) of the normal equations, the left singular vectors can be obtained as U = AV Σ−1, where
R ) = Tr(V Σ−1U H ) + Tr(A−1 −
Σ = Λ1/2. Following (2.1), we can decompose Tr(A−1) = Tr(A−1
V Σ−1U H ). Using the cyclic property of the trace, we have Tr(V Σ−1U H ) = Tr(Σ−1U H V ) = Tr(Λ−1V H AH V ).
This means that the trace of A−1
D can be computed explicitly through m matrix vector multiplications and
R ) = Tr(A−1 − V Λ−1V H AH ), so the quadratures required in
m inner products. Similarly, we see that Tr(A−1
Hutchinson’s method can be computed as zH A−1z and zH V Λ−1V H (AH z). This means that we can avoid
the signiﬁcant storage of U .

D ) + Tr(A−1

We now have all the components to run the deﬂated Hutchinson method using random Rademacher
vectors. However, the same deﬂation technique can be used on the Hutchinson method if the vectors come
from the Hierarchical Probing (HP) method. HP uses an implicit distance-k coloring of the lattice to pick the
probing vectors as certain permutations of Hadamard vectors that remove all trace error that corresponds to
A−1
ij elements with i, j having up to k Manhattan distance in the lattice. The hope is that deﬂation removes
error in a complementary way from HP and the two techniques together lead to faster convergence.
To avoid the deterministic bias of the HP method, we follow the technique proposed in [20] which ﬁrst
computes a random Rademacher vector z0, and then in the Hutchinson method uses the vector z = z0 (cid:12) zh,
which is the elementwise product of z0 with each Hadamard vector zh from the HP sequence. We have shown
this method to be unbiased and to reduce the measured error. Algorithm 1 summarizes our approach.
We conclude this algorithmic part of the paper by mentioning an important application of this technique.
In Lattice QCD, we are often interested in computing Tr(ΓA−1) for several diﬀerent Γ matrices whose
application to a vector are inexpensive to compute. In such cases, the SVD decomposition (2.1) still applies,
Tr(ΓA−1) = Tr(ΓV Λ−1V H AH ) + Tr(ΓA−1 − ΓV Λ−1V H AH ). The computations are similar to Algorithm 1,
with a Γ matrix vector product inserted at each step. Therefore, the computational cost of the SVD and the
storage for singular vectors can be amortized by reusing the deﬂation space to compute traces with multiple
Γ matrices.

6. QCD Experiments. We present results from experiments with two representative Dirac matrices.
Both are from 323 × 64, β = 6.3 Clover improved Wilson ensembles. In both cases, the pion mass was about
300M eV . However, the ﬁrst matrix comes from an ensemble with 3 ﬂavors of dynamical quarks, whose

14

Algorithm 1 T race = deﬂatedHP(A)

1: [Λ, V ] = PRIMME(AH A)
2: TD = Tr(Λ−1V H AH V ); TR = 0
3: z0 = randi([0, 1], N, 1); z0 = 2z0 − 1
4: for j = 1 : s do
zh = next vector from Hierarchical Probing or other scheme
5:
z = z0 (cid:12) zh
6:
Solve Ay = z
7:
TR = TR + zH y − zH V Λ−1V H (AH z)
8:
T race = TR/j + TD
9:
10: end for

masses were turned to match the physical strange quark mass.
In this case we employed a lower quark
mass (quark mass mq = −0.250 in lattice units) for our numerical experiments in order to achieve a more
singular matrix. In the second case, the ensemble from which we selected the Dirac matrix is one with 2 light
quark ﬂavors and one strange quark. The strange quark is again, at its physical value, and the light quarks
have masses −0.239 that result in 300MeV pions. The interested reader can ﬁnd further details about these
ensembles in [26]. Subsequently, we will refer to the matrix with a quark mass of mq = −0.250 as the Dirac
operator from ensemble A, and the mq = −0.239 mass matrix as the Dirac matrix from ensemble B.

The above matrices have a size of N = 25,165,824 and condition numbers of 1747 and 1788 respectively.
The subspaces were obtained using PRIMME set to the LOBPCG Orthobasis Window method with a tolerance
of 10−2 and a block size of 30 [21]. This was supplemented with a three level AMG preconditioner with 44
and 24 blocking and a ﬁne/coarse maximum iteration count of 4 and 5 respectively [3].

(a) QCD SVD spectra

(b) Ensemble B Tr A−1 Deﬂation Model

Fig. 6.1: The left plot displays the 1000 lowest magnitude singular values for both matrices as obtained by
PRIMME. The right plot shows results from our deﬂation model using the 1000 computed singular values
of the ensemble B matrix, and simulating the rest of the spectrum as a Wilson Dirac operator in free ﬁeld.

6.1. Monte Carlo with deﬂation. We analyze the singular spectra of these matrices in the context
of our deﬂation theory from Section 2. Figure 6.1a shows the smallest 1000 singular values of A for both
ensembles. The lowest 20 rise rapidly before the spectrum growth slows down to slightly sublinear growth
(ensemble B) or close to linear (ensemble A). Since our focus is the inverse A−1, the situation seems to

15

0200400600800100000.020.040.060.080.10.12SingularValueIndexSVD Spectrum  Ensemble A (Partially Quenched)Ensemble B (Dynamical QCD)5050050000.550.60.650.70.750.80.850.90.951NumberofsingularvectorsdeﬂatedVariance(deflated) / Variance(undeflated)similar to Figure 3.3b. To run this through our model, we wanted a rough estimate of the rest of the
spectrum. We have merged our 1000 smallest, explicitly computed singular values, with the analytically
obtained singular values of the free ﬁeld Wilson Dirac operator to obtain an approximate full spectrum for
A. This was achieved by quadratically ﬁtting the exactly computed singular values up to 4000 vectors and
joining them with the free ﬁeld spectrum via a small line segment. Then, we use our model to simulate the
eﬀects of deﬂation on variance for up to 5000 lowest singular triplets. In Figure 6.1b, our deﬂation model
predicts a variance reduction of approximately 30% for 1000 singular vectors, and 40% for 5000 singular
vectors.

Since the trace and variance of the undeﬂated and deﬂated matrix are not known, the model has to be
compared with the statistically measured variance of Monte Carlo. An experiment with the full dynamical
matrix from ensemble B was conducted to compute Tr(A−1) with the Hutchinson method using random
Rademacher vectors (no HP). Table 6.1 shows the results for both the undeﬂated operator (ﬁrst line) and
the operator deﬂated with a various numbers of singular vectors (from 25 to 1000 starting with the smallest
in magnitude). The three result columns show the statistical variance after 32, 64, and 128 Monte Carlo
steps, respectively. Past the lowest 25-50 singular values, there is little improvement with the Monte Carlo
estimator. With 128 Rademacher vectors the deﬂation speedup is about 30%. The improvement may not
be impressive, but what is impressive is the level of agreement with the prediction of our model in Figure
6.1b. However, this agreement is not surprising since our model assumes uniformly random unitary singular
vector matrices, which is approximately the case in QCD [19, 24].

Table 6.1: Ensemble B Tr(A−1) Variance

Monte Carlo Step

32

64

128

Undeﬂated

25
50
100
200
300
400
500
600
700
800
900
1000

1.0735e+04
7.7396e+03
7.0769e+03
7.0645e+03
6.9917e+03
7.0246e+03
6.9628e+03
7.0002e+03
7.1782e+03
7.2679e+03
7.1029e+03
7.1378e+03
7.0484e+03

5.1764e+03
4.0158e+03
3.8168e+03
3.8108e+03
3.9187e+03
3.8921e+03
3.9373e+03
3.8166e+03
3.8422e+03
3.8326e+03
3.8064e+03
3.8768e+03
3.8355e+03

2.7336e+03
2.3081e+03
2.0751e+03
2.0641e+03
2.1308e+03
2.1127e+03
2.1466e+03
2.1132e+03
2.0921e+03
2.1068e+03
2.0927e+03
2.1036e+03
2.0922e+03

6.2. Synergy between deﬂation and hierarchical probing. HP used with the Hutchinson method
reduces the error (when run deterministically) or the variance (when run stochastically as in Algorithm 1).
Depending on the matrix, improvements over an order of magnitude have been observed [20]. In Figures 6.2a
and 6.2b we present results of Algorithm 1 with the ensemble A and ensemble B matrices respectively, where
HP is augmented by deﬂation. The error bars on the variance were estimated with the Jackknife resampling
procedure on 40 runs of Algorithm 1 with diﬀerent z0 noise vectors. Local minima appear on the y axis of
both plots at every power of two. This is a characteristic of the HP method, which is meaningful only at
these points [20]. At least one order of magnitude improvement in variance is observed with deﬂation over
HP alone. This represents over 100-fold improvement over the original Monte Carlo method.

It is apparent that deﬂation aids the HP estimator in a much more pronounced manner than the basic
noise estimator. This is because of the synergistic way deﬂation and HP work. The idea of HP is based on
the local decay of the Green’s function. By assuming that the neighbors of a source node in matrix A will
have weights in A−1 that decay with their distance from the source, HP kills the error from progressively
larger distance neighborhoods. This works well for well conditioned matrices, but for ill conditioned ones the

16

(a) Ensemble A Tr(A−1) Variance

(b) Ensemble B Tr(A−1) Variance

Fig. 6.2: Above is the variance of the hierarchical probing trace estimator with and without deﬂation. The
full 1000 vector subspace is used as the deﬂated operator in red. Complete color closings are marked with
green circles. For the ensemble A matrix, a factor of 15 is achieved in variance reduction between deﬂated
and undeﬂated probing. Deﬂation yields over a factor of 20 reduction of variance for the ensemble B matrix.

A−1 is dominated by the contributions of the near null eigenspace. Such contributions are typically non-local
which are not captured by HP. Deﬂation, however, captures exactly these contributions and by removing
them, a much easier structure for HP is left. In Lattice QCD, this synergy completely resolves the scaling
problem as the mass approaches the critical mass, and signiﬁcantly reduces the eﬀects of lattice size.

Fig. 6.3: The sum of squared absolute values of matrix elements at speciﬁed Manhattan distances from
the corresponding diagonal elements for 10 randomly sampled rows. Base case is the original Monte Carlo
method. Deﬂation refers to the Monte Carlo with deﬂation. HP and deﬂated HP refer to a space spanned
by the 32 hierarchical probing vectors. A combination of HP and deﬂation suppresses the sum of matrix
elements by orders of magnitude more than probing or deﬂation alone.

We investigate this synergy experimentally on the matrix from ensemble A. We seek to quantify the

17

248163264128256512101102103104105Hierarchical Probing vectorsVarianceEnsemble A  UndeflatedDeflatedColor Closing Point248163264128256512101102103104105Hierarchical Probing vectorsVarianceEnsemble B  UndeflatedDeflatedColor Closing Point0102030405060708010−1010−5100ManhattanDistanceMatrix Elements Squared  BaseDeflatedHierarchical ProbingDeflated Hierarchical ProbingF ), after applying deﬂation ((cid:107)A−1
R (cid:107)2

remaining variance on the original matrix ((cid:107)A−1(cid:107)2
F ), after applying 32
HP probing vectors H ((cid:107)(HH H ) (cid:12) A−1(cid:107)2
F ), and after applying both deﬂation and HP ((cid:107)(HH H ) (cid:12) A−1
R (cid:107)2
F ).
Let B denote any of these four matrices. Since we cannot compute (cid:107)B(cid:107)F explicitly, we randomly sample
10 of its rows, denoting this set as S. Then for each corresponding lattice node i ∈ S, we ﬁnd all its mk
neighbors j that are k hops away in the lattice (i.e., its Manhattan distance-k neighborhood) and sum their
squared absolute values |Bij|2. Averaging these over all mk neighbors and all nodes in S gives us an estimate
of how much variance remains from elements at distance k. These Wk are plotted in Figure 6.3,
|Bij|2/mk, where Nk = {j : dist(i, j) = k} and mk = |Nk|.

Wk = 1/|S|(cid:88)

(cid:88)

i∈S

j∈Nk

The ﬁgure shows how HP eliminates the variance from the ﬁrst 3 distances and repeats this pattern in
multiples of 4 (1,2,3,5,6,7,. . .) [20]. While probing eliminates better short-distance variance, deﬂation is
better at long-distance. Combining them achieves a much greater reduction in variance than either of the
two alone.

6.3. Varying the SVD deﬂation space. We also study the eﬀect of the size of the deﬂation SVD
subspace. By saving all inner products performed in the trace estimator, we are able to play back the trace
simulation deﬂating with diﬀerent numbers of singular triplets. We combine deﬂation and HP and report
results for 32 and 512 probing vectors, which represent the proper color closings for HP in a 4D lattice [20].
As before, the error bars are obtained from 40 diﬀerent runs of Algorithm 1 with diﬀerent z0.

(a) Ensemble A SVD and 32 HP vectors

(b) Ensemble A SVD and 512 HP vectors

Fig. 6.4: Variance for the ensemble A matrix as a function of the deﬂated SVD subspace dimension at two
color closing points of HP. The left plot is with 32 probing vectors, the right is with the full 512.

Figure 6.4a shows that deﬂation with 200 singular vectors reduces variance by a factor of 3, and beyond
200 little improvement is gained. In Figure 6.4b, HP has removed the error for larger distances and therefore
it can use more singular vectors eﬀectively, yielding more than an order of magnitude improvement. Still
there is potential for computational savings since 500 singular vectors have the same eﬀect as 1000 ones.
Figures 6.5a and 6.5b display similar attributes for the ensemble B matrix.

These experiments illustrate that the optimal number of vectors to be used in each of the two techniques
depends on each other. This is only an issue if one needs to ﬁgure out how many singular vectors to compute
a priori, because if these are already available, their application in the method is not computationally
expensive. Moreover, while using a suﬃciently large number of probing vectors is important, the performance
of deﬂation seems to be much less sensitive to the number of singular vectors. Once the near null space has
been removed, there are diminishing returns to deﬂate with bigger subspaces. In general, the eﬀect of this

18

02004006008001000100015002000250030003500400045005000NumberofSingularVectorsVarianceEnsemble A, 32 HP vectors02004006008001000050100150200250300350400NumberofSingularVectorsVarianceEnsemble A, 512 HP vectors(a) Ensemble B SVD and 32 HP vectors

(b) Ensemble B SVD and 512 HP vectors

Fig. 6.5: Variance for the matrix from ensemble B, as a function of the deﬂated SVD subspace dimension at
two color closing points of HP. The left plot is with 32 probing vectors, the right is with the full 512.

can be estimated through the model while computing the singular spectrum. The experiments we provide
in this paper should provide a good rule of thumb when computing disconnected diagrams for a similar class
of Lattice QCD gauge conﬁgurations.

6.4. Wallclock timings and eﬃciency. Implementing either MC or hierarchical probing with deﬂa-
tion requires an additional setup cost from ﬁnding the SVD space. In Lattice QCD, this cost is of little
importance since the subspace may be stored and reused several times for computing various correlation
functions.
Deﬂation is valuable even as a “one shot method” for our QCD matrices. We investigate the case in
which the trace of A−1 only needs to be computed once, and report the time to compute the SVD, ﬁrst
separately and then as the overhead of the preprocessing of Hutchinson’s method. Our experiments were
performed on the Cray Edison using 32 12-core Intel Ivy Bridge nodes clocked at 2.4 GHz, each with only 8
cores enabled due to memory and node topology considerations.

Figure 6.6a shows the timings for PRIMME as a function of the number of eigenvectors found. As
more eigenvectors converge, orthogonalization costs increase resulting in time increasing super linearly. The
expected reduction in the eﬃciency of the AMG preconditioner as we move to the interior of the spectrum
is in fact negligible. Obtaining 1000 eigenvectors takes 1.5 hours, while 500 vectors are computed in less
than half an hour. Indeed with the help of the AMG preconditioner, PRIMME was able to solve for the
eigenvalues of AH A at a fraction of the cost of the probing estimator.

We now add the time to compute the singular space as well as the time to perform the projections with
that space to the timings for the remaining steps of Algorithm 1. We consider two simulations; one with
deﬂation space of 500 vectors and one with 1000 vectors. For each closing point of HP (32, 64, 128, 256,
and 512 probing vectors), Figure 6.6b plots the achieved variance as a function of total wallclock time. We
observe that the variance with deﬂation at probing vector 128 is comparable to the variance of the plain
HP method at 512 probing vectors. This translates to a 4-fold reduction in wallclock, even with the SVD
computations included. Furthermore, at 512 probing vectors, we see a 15-fold reduction in variance with the
SVD time being less than 10% of total wallclock. This suggests that deﬂation can be used equally well as a
one shot method for variance reduction.

7. Conclusion. We have studied theoretically and experimentally the eﬀects of deﬂating the near null
singular value space on reducing the variance of the Hutchinson method. This is a Monte Carlo method for
estimating the trace of the inverse of a large, sparse matrix, which among other areas is also common in

19

020040060080010001000200030004000500060007000NumberofSingularVectorsVarianceEnsemble B, 32 HP vectors02004006008001000050100150200250300NumberofSingularVectorsVarianceEnsemble B, 512 HP vectors(a) PRIMME cost

(b) Variance vs simulation cost

Fig. 6.6: Eigenvectors captures by PRIMME from 100 to 1000 for the matrix from ensemble A. A log plot
of variance and cost. Each case displays 5 points, which represent the variance and wallclock at probing
vectors 32, 64, 128, 256, and 512.

Lattice QCD. Our theoretical analysis showed that variance reduction can only be obtained if the singular
values of the matrix increase at a fast rate. By assuming that the singular vectors are random unitary
matrices, we were able to quantify the above in a concise, elegant formula that requires only the ﬁrst two
moments of the singular values. Experiments have shown that the formulas model even general, non-random
matrices very well. We have also shown an interesting property, where singular vector deﬂation applied to
Hermitian matrices can increase the variance, whereas deﬂation applied to non-Hermitian matrices with the
same spectrum always decreases the variance.

In the second part of the paper we use deﬂation to solve a particularly challenging, large scale QCD
application deﬁned on a 4D regular lattice. The singular values are computed using PRIMME with an
AMG preconditioner in one of the largest SVD computations performed in Lattice QCD. Although deﬂation
on its own has a limited impact on the variance, combining it with the current state-of-the-art method of
Hierarchical Probing (HP) provides a factor of 10-15 speedup over HP. We explain this synergy theoretically
and provide a thorough experimental analysis that conﬁrms our explanation. These Lattice QCD tests,
which were performed on Edison (the Cray supercomputer at the National Energy Research Scientiﬁc Com-
puting Center) show that our method can have signiﬁcant eﬃciency improvements on similar Lattice QCD
calculations that require the computation of the trace of matrices related to the inverse of the Dirac matrix.

Acknowledgments. This work has been supported by NSF under grants No. CCF 1218349 and ACI
SI2-SSE 1440700, and by DOE under a grant No. DE-FC02-12ER41890. KO and AG have been supported
by the U.S. Department of Energy through Grant Number DE- FG02-04ER41302. KO has been supported
through contract Number DE-AC05-06OR23177 under which JSA operates the Thomas Jeﬀerson National
Accelerator Facility. AG has been supported by the U.S. Department of Energy, Oﬃce of Science, Oﬃce of
Workforce Development for Teachers and Scientists, Oﬃce of Science Graduate Student Research (SCGSR)
program. The SCGSR program is administered by the Oak Ridge Institute for Science and Education for the
DOE under contract number DE-AC05-06OR23100. This research used resources of the National Energy
Research Scientiﬁc Computing Center, a DOE Oﬃce of Science User Facility supported by the Oﬃce of
Science of the U.S. Department of Energy under Contract No. DE-AC02-05CH11231.

REFERENCES

20

Singular Vectors Locked02004006008001000Wallclock(seconds)0100020003000400050006000Wallclock(seconds)×10400.511.522.5Variance101102103104Hierarchical ProbingSVD 500SVD 1000[1] A. Abdel-Rehim, K. Orginos, and A. Stathopoulos, Extending the eigCG algorithm to non-symmetric linear systems

with multiple right-hand sides, PoS, LAT2009 (2009), p. 036, arXiv:0911.2285.

[2] H. Avron and S. Toledo, Randomized algorithms for estimating the trace of an implicit symmetric positive semi-denite

matrix, Journal of the ACM, 58 (2011), p. Article 8.

[3] R. Babich, J. Brannick, R. C. Brower, M. A. Clark, T. A. Manteuffel, S. F. McCormick, J. C. Osborn, and
C. Rebbi, Adaptive multigrid algorithm for the lattice Wilson-Dirac operator, Phys. Rev. Lett., 105 (2010), p. 201602,
doi:10.1103/PhysRevLett.105.201602, arXiv:1005.3043.

[4] R. Babich, R. Brower, M. Clark, G. Fleming, J. Osborn, C. Rebbi, and D. Schaich, Exploring strange nucleon

form factors on the lattice, (4 May 2011), arXiv:1012.0562v2.

[5] C. Bekas, A. Curioni, and I. Fedulova, Low cost high performance uncertainty quantication, in In WHPCF 09: Proc.

of the 2nd Workshop on High Performance Computational Finance, New York, NY, USA, 2009, ACM, pp. 1–8.

[6] C. Bekas, E. Kokiopoulou, and Y. Saad, An estimator for the diagonal of a matrix, Appl. Numer. Math., 57 (2007),

pp. 1214–1229.

[7] J. Carlsson, Integrals over SU(N), (2008), arXiv:0802.3409.
[8] M. Creutz, Quarks, Gluons and Lattices, Cambridge Monographs on Mathematical Physics, Cambridge University Press,

1983, https://books.google.com/books?id=mcCyB3ewyeMC.

[9] T. A. Davis and Y. Hu, The university of ﬂorida sparse matrix collection, ACM Trans. Math. Softw., 38 (2011), pp. 1:1–

1:25, doi:10.1145/2049662.2049663, http://doi.acm.org/10.1145/2049662.2049663.

[10] J. Green, S. Meinel, M. Engelhardt, S. Krieg, J. Laeuchli, J. Negele, K. Orginos, A. Pochinsky, and S. Syrit-
syn, High-precision calculation of the strange nucleon electromagnetic form factors, Phys. Rev., D92 (2015), p. 031501,
doi:10.1103/PhysRevD.92.031501, arXiv:1505.01803.

[11] R. Gupta, Introduction to lattice QCD: Course, in Probing the standard model of particle interactions. Proceedings,
Summer School in Theoretical Physics, NATO Advanced Study Institute, 68th session, Les Houches, France, July 28-
September 5, 1997. Pt. 1, 2, 1997, pp. 83–219, http://alice.cern.ch/format/showfull?sysnb=0284452, arXiv:hep-
lat/9807028.

[12] F. Hiai and D. Petz, The Semicircle Law, Free Random Variables and Entropy (Mathematical Surveys & Monographs),

American Mathematical Society, Boston, MA, USA, 2006.

[13] M. F. Hutchinson, A stochastic estimator of the trace of the inﬂuence matrix for Laplacian smoothing splines, J.

Commun. Statist. Simula., 19 (1990), pp. 433–450.

[14] T. Iitaka and T. Ebisuzaki, Random phase vector for calculating the trace of a large matrix, Phys. Rev. E, 69 (2004),

p. 05770110577014.

[15] T. Jiang, How many entries of a typical orthogonal matrix can be approximated by independent normals?, ArXiv Math-

ematics e-prints, (2006), arXiv:math/0601457.

[16] R. Morgan and W. Wilcox, Deﬂated iterative methods for linear equations with multiple right-hand sides, Tech. Report

BU-HEPP-04-01, Baylor University, 2004.

[17] C. Morningstar, J. Bulava, J. Foley, K. Juge, D. Lenkner, M. Peardon, and C. Wong1, Improved stochastic
estimation of quark propagation with Laplacian Heaviside smearing in lattice QCD, Phys. Rev. D, 83 (2011), doi:10.
1103/PhysRevD.83.114505, arXiv:1104.3870v1.

[18] Y. Saad, Iterative Methods for Sparse Linear Systems, Society for Industrial and Applied Mathematics, Philadelphia,

PA, USA, 2nd ed., 2003.

[19] E. V. Shuryak and J. J. M. Verbaarschot, Random matrix theory and spectral sum rules for the Dirac operator in

QCD, Nucl. Phys., A560 (1993), pp. 306–320, doi:10.1016/0375-9474(93)90098-I, arXiv:hep-th/9212088.

[20] A. Stathopoulos, J. Laeuchli, and K. Orginos, Hierarchical probing for estimating the trace of the matrix inverse on

toroidal lattices, (2013), arXiv:1302.4018.

[21] A. Stathopoulos and J. R. McCombs, Primme: Preconditioned iterative multimethod eigensolver: Methods and software

description, 2006.

[22] A. Stathopoulos and K. Orginos, Computing and deﬂating eigenvalues while solving multiple right hand side lin-
ear systems in quantum chromodynamics, SIAM J. Sci. Comput., 32 (2010), pp. 439–462, doi:10.1137/080725532,
arXiv:0707.0131.

[23] J. Tang and Y. Saad, Domain-decomposition-type methods for computing the diagonal of a matrix inverse, Report UMSI

2010/114.

[24] J. J. M. Verbaarschot and T. Wettig, Random matrix theory and chiral symmetry in QCD, Ann. Rev. Nucl. Part.

Sci., 50 (2000), pp. 343–410, doi:10.1146/annurev.nucl.50.1.343, arXiv:hep-ph/0003017.

[25] M. N. Wong, F. J. Hickernell, and K. I. Liu, Computing the trace of a function of a sparse matrix via Hadamard-like

sampling, Tech. Report 377(7/04), Hong Kong Baptist University, 2004.

[26] B. Yoon et al., Controlling Excited-State Contamination in Nucleon Matrix Elements. 2016, arXiv:1602.07737.

21

