Modelling Temporal Information Using Discrete Fourier Transform

for Video Classiﬁcation

Haimin Zhanga, Min Xua,∗, Changsheng Xub, Ramesh Jainc

aFaculty of Engineering and IT, University of Technology Sydney

bNational Lab of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences

cBren School of Information and Computer Sciences, University of California, Irvine

6
1
0
2

 
r
a

 

M
2
2

 
 
]

V
C
.
s
c
[
 
 

2
v
2
8
1
6
0

.

3
0
6
1
:
v
i
X
r
a

Abstract

Recently, video classiﬁcation attracts intensive research eﬀorts. However, most existing works are based on frame-
level visual features, which might fail to model the temporal information, e.g. characteristics accumulated along time.
In order to capture video temporal information, we propose to analyse features in frequency domain transformed by
discrete Fourier transform (DFT features). Frame-level features are ﬁrstly extract by a pre-trained deep convolutional
neural network (CNN). Then, time domain features are transformed and interpolated into DFT features. CNN and
DFT features are further encoded by using diﬀerent pooling methods and fused for video classiﬁcation.
In this
way, static image features extracted from a pre-trained deep CNN and temporal information represented by DFT
features are jointly considered for video classiﬁcation. We test our method for video emotion classiﬁcation and
action recognition. Experimental results demonstrate that combining DFT features can eﬀectively capture temporal
information and therefore improve the performance of both video emotion classiﬁcation and action recognition. Our
approach has achieved a state-of-the-art performance on the largest video emotion dataset (VideoEmotion-8 dataset)
and competitive results on UCF-101.

Keywords: video classiﬁcation, temporal information, discrete Fourier transform, CNN

1. Introduction

As technology advances, millions of video clips are
uploaded to the Internet every day. From video sharing
websites (e.g. YouTube and Flicker), people can easily
access hundreds of video clips. It is an essential and ur-
gent demand to develop intelligent algorithms for clas-
sifying these videos, which form the basis of various
potential applications, such as video retrieval, recom-
mendation and annotation.

Video classiﬁcation is to automatically assign a la-
bel to a video clip. Recent research classify video se-
quences into either emotional classes [1, 2, 3] or action
categories [4, 5, 6]. Compared to static image classi-
ﬁcation, video classiﬁcation is a complex task. This is
mainly because of three reasons: 1) Video classiﬁcation
demands high computation cost since a short video can
contain hundreds of or even thousands of frames. 2)
A video sequence contains multiple frames which are

∗Corresponding author
Email address: Min.Xu@uts.edu.au (Min Xu)

Preprint submitted to Pattern Recognition

grouped together to reﬂect a dominant theme. Within a
video sequence, some frames might not convey (or even
convey an opposite meaning of) the dominant theme. 3)
Spatial and temporal information should be jointly con-
sidered for video classiﬁcation. Static image features
have been extensively studied by researchers over the
decades. Static image features can be utilised to rep-
resent video at a frame-level. In addition to static im-
age features, temporal information is an import clue for
video classiﬁcation. Since the length of video clips are
diﬀerent, it is a challenging task to generate uniform
video representations which can jointly model spatial
and temporal information.

Most conventional approaches of video classiﬁcation
involve mainly three stages: First, local features or
frame-level features are extracted. Then these features
are quantised to a ﬁxed length representation using a vi-
sual dictionary usually learned by K-means algorithm.
Later, video-level representations are obtained by pool-
ing method, such as max-pooling and average-pooling.
Lastly, a classiﬁer is trained on the video-level repre-
sentations to diﬀerentiate diﬀerent classes of video cat-

March 23, 2016

Figure 1: An overview of our method for video classiﬁcation. Given an input video, CNN(C3D) features are ﬁrstly extracted. Then these features
are transformed to frequency domain using DFT. Feature pooling methods are applied for CNN(C3D) feature and DFT features separately. The
concatenation of the aggregated CNN features and DFT features lead to the video-level representation. Finally, an SVM is trained to diﬀerentiate
diﬀerent classes of video categorises.

egorises.

However, during the process of feature quantisation
and pooling, the temporal information (i.e., information
accumulated along time) of videos has not been well
treated. Without modelling temporal information, the
performance of classiﬁers are restricted. Taking activity
recognition as an example, without considering tempo-
ral information, a classiﬁer cannot diﬀerentiate activi-
ties of standing up v.s. sitting down and opening doors
v.s. closing doors.

Most recently, realising the importance of temporal
information, researchers have started exploring how to
take temporal information into account for video clas-
siﬁcation. One dominated approach in recent years is
Dense Trajectories [7], which tracks densely sampled
image patches over time and calculates conventional lo-
cal features, e.g. HOG [8], HOF [9] and MBH [10],
based on the calculated trajectories. However, the ap-
plication of Dense Trajectories is restricted to relatively
small scale dataset since the method needs intensive
computation.

In [11], Long Short Term Memory (LSTM) [12],
which can preserve information for a long time, was
adopted to model temporal information for video clas-
siﬁcation. However,
training an LSTM is a time-
consuming task. Although these initiatives can some-

how capture temporal information, they might failed to
consider the accumulated information along time.

Inspired by Discrete Fourier Transform (DFT), which
can transform a discrete signal form time domain to
frequency domain, this paper proposes to analysis fea-
tures in frequency domain to model temporal informa-
tion for videos. To some extent, signal characteristics
along time can be accumulated and represented through
sampling in frequency domain [13]. In this work, we
integrate CNN features (features extracted from a pre-
trained convolutional neural network) and DFT features
for video representation by leveraging feature encoding
method.

The proposed method consists of ﬁve steps as shown
in Figure 1. The ﬁrst step is to extract CNN or C3D
(convolutional 3D) features from videos. Secondly,
considering each feature dimension as a discrete sig-
nal over time, we apply DFT to transform the signal to
frequency domain. Thirdly, an interpolation method is
adopted to generate a ﬁxed length representation for ev-
ery dimension of DFT features. Fourthly, feature en-
coding methods (i.e. average pooling, LLC, FV and
VLAD) are applied to aggregate CNN features and DFT
features. The combination of the aggregated CNN fea-
tures and DFT features can be regarded as a video-level
representation. Finally, with the features generated in

2

the fourth step, an SVM is trained for video classiﬁ-
cation. Diﬀerent from existing methods for temporal
information analysis, our method has the following ad-
vantages:

• In order to capture video temporal information, we
propose to analyse features in frequency domain
transformed by discrete Fourier transform.

• Video clips having diﬀerent length places a chal-
lenge for obtaining a uniformed feature represen-
tation. Using DFT is resilient to signal length vari-
ation. Moreover, compared to DFT, applying fast
Fourier transform (FFT) can signiﬁcantly reduce
the computational complexity.

• Experiments on two tasks, i.e. video emotion clas-
siﬁcation and action recognitions, demonstrate that
combining DFT features can eﬀectively capture
temporal information and therefore improve the
performance of video classiﬁcation.

The remainder of this paper is organised as follows.
In section 2, we review the related work on video rep-
resentations using temporal information.
In section
3, DFT-based temporal information modelling is intro-
duced in details. Experimental results are presented and
discussed in section 4. Finally, we conclude this paper
in section 5.

2. Related Work

2.1. Low-level Representations

Although video classiﬁcation has been researched
for many years, it is still a challenging task that at-
tracts much research interests over the decades. Early
works on video classiﬁcation focus on employing eﬃ-
cient image representations. Local image features (such
as SIFT [14], HOG (histograms of oriented gradients)
[8] and HOF (histograms of optical ﬂow) [9]) are ex-
tracted from video frames. These features are further
encoded to generate a uniform video-level representa-
tion. As one of the feature encoding method, bag of
visual words (BoVW) [15] is the most commonly used
model for generating a uniform representation. Sparse
coding, which uses sparse constraints, is an extension
of the BoVW model. Sparse coding achieves less quan-
tisation error than BoVW. In [16], sparse coding was
proposed for image classiﬁcation. In addition to sparse
constraints, LLC utilises locality constraints and can
further reduce quantisation error.
In [17], LLC was
proposed for image classiﬁcation. Fisher vector, which

3

was derived from ﬁsher kernel [18] , was ﬁrstly intro-
duced for large-scale image classiﬁcation in [19]. How-
ever, the dimension of features generated by FV is much
higher than BoVW and LLC. Vector of locally aggre-
gated descriptors (VLAD) is anther popular feature en-
coding approach, which was ﬁrstly proposed by Jegou
in [20] for image representation.

2.2. High-level Representations

Due to the limited discriminative capacity of low-
level representations to video semantics, high-level rep-
resentations were introduced. In high-level feature rep-
resentations, an image is represented as a response map
of a large number of pre-trained detectors. In [21], Ob-
ject Bank was proposed for scene classiﬁcation and se-
mantic feature sparsiﬁcation. SentiBank, which con-
sists of 1,200 concepts and associated classiﬁers, was
constructed for sentiment prediction in images in [22].
Action Bank, which is comprised of many individual
action detectors sampled in semantic space and view-
point space, was proposed for action recognition in [23].
Jiang et. al. [1] applied ObjectBank [24] and Sentibank
[22] for video emotion categorisation.

2.3. Deep-learned Features

In contrast to hand-crafted features, the last few years
have witnessed the success of deep features. Deep fea-
tures extracted from the activation of a convolutional
neural work (CNN) pre-trained on a large image dataset
(e.g. ImageNet [25]) have proved to be more discrim-
inative than hand-crafted features [26]. CNN features
have achieved state-of-the-art results on many bench-
marks [27] and are widely used in image classiﬁca-
tion, object detection and attribute detection [28, 27].
Recently, researchers started applying CNN features to
video classiﬁcation [4, 11, 29]. Xu et. al.
[29] pro-
posed a video representation method through leverag-
ing frame-level features extracted by CNN with FV
and VLAD as feature encoding method.
In [4], two
stream convolutional networks was proposed on the top
of video frames and stacked optical ﬂows to capture spa-
tial and motion information.

2.4. Temporal Information Modelling

Researchers started to model temporal information
using motion features. Dense Trajectories [7], which
was inspired by dense sampling method in image clas-
siﬁcation, was proposed for action recognition. As a de-
rived version of DT, improved Dense Trajectories (IDT)
[30] were proposed to improve the performance of DT
by taking camera motion into consideration and tested

on a number of challenging dataset (e.g. HMDB51 [31],
Sports-1M [32]). However, motion features can only
capture temporal information over a couple of consecu-
tive frames. As a special architecture of recurrent neural
network (RNN), long short term memory (LSTM) was
speciﬁcally designed with memory cells to store, mod-
ify and access its internal states, and can persist long
time information. LSTM was successfully used to cap-
ture temporal information for sequence learning tasks,
such as speech recognition [33] and machine transla-
tion [34].
In[5], an unsupervised learning approach
was proposed for video representation using the LSTM
encoder-decoder architecture.

Conventional CNN are only limited to handle frame-
level inputs. As an extension of CNN, 3D convolutional
neural network was proposed for videos classiﬁcation in
[35], which extract features from both spatial and tem-
poral dimensions by performing a 3D convolution and
pooling. In [6], a 3D convolutional neural network was
trained to extract spatial-temporal features. However,
3D CNN architecture can only take video clips with a
short length (usually, 16 frames) as inputs. This stops it
from capturing long term accumulated temporal infor-
mation.

3. The proposed DFT-based Temporal Information

Modelling

The proposed algorithm involves ﬁve steps and is in-

troduced in details step by step in this section.

3.1. Feature Extraction

CNN Features As shown in [27], deep features ex-
tracted from a convolutional neural network which is
pre-trained on a large image dataset can be used as a
powerful feature representation for many visual anal-
ysis tasks.
In this paper, we leverage a deep convo-
lutional neural network [36] pre-trained on ImageNet
[25], which contains 1.2 million images categorised
into 1000 classes, to extract frame-level descriptors for
all video clips. The network consists of ﬁve convolu-
tion layers and three fully connected layers with a ﬁ-
nal 1000-way softmax. All input images are resized
to 256×256 without considering its original aspect ra-
tio before feeding into the network. Considering fea-
tures extracted from fully connected layers can capture
semantic information from the input image, activation
from the fully connected layer are extracted as the frame
descriptor. Following [29], fc6 and fc7 refer to the ac-
tivation of the ﬁrst and second fully-connected layers.
Then ℓ2 normalisation is adopted to all frame-level de-
scriptors.

C3D Features C3D (Convolution 3D) features refers
to features extracted from a pre-trained 3D convolu-
tional networks [6]. Unlike convolutional networks, 3D
convolutional networks takes a short video clip (usually
16 frames) as input and leverages on 3D convolution
and pooling.

Let f denote the set of frame-level descriptors of a
video clip which has N frames, then f can be described
as

f = (f1, ..., fN) =

f1[1]
...
f1[k]
...

f1[D]



. . .
...
. . .
...
. . .

fi[1]
...
fi[k]
...

fi[D]

. . .
...
. . .
...
. . .

fN[1]

...

fN[k]

...

fN[D]



Where fi = (fi[1], ..., fi[D])T represents the descriptor of
the i-th frame with dimension D. In this work, D equals
4096, which is the dimension of fc6. The value of N can
be diﬀerent for diﬀerent video clips.

3.2. Discrete Fourier Transform of CNN Features

The aim of discrete Fourier transform (DFT), which
is widely used in the ﬁeld of signal processing, is to
transform a discrete signal from time domain to fre-
quency domain. At this step, we present how DFT is
applied to CNN features.

As described in 3.1, f = (f1, ..., fN) represents the set
of frame-level CNN features extracted from a video clip
with N frames. Let the k-th dimension of f be denoted
f[k] = (f1[k], f2[k], ...fN[k]). f[k] can be considered as
a discrete signal which has N sample points with equal
sampling time intervals ∆t. We transform f[k] to fre-
quency domain using the following equation

Fs[k] =

N

X

n=1

fn[k]e−2iπ(n−1)(s−1)/N , s = 1, 2, ..., N.

(1)

Let the result be denoted F[k] = (F1[k], ..., FN[k]),
the number of points obtained in frequency domain is
as same as that in time domain. The computed value
Fs[k] ∈ F[k] is a complex number and its absolute value
represents the amplitude of the s-th frequency. In our
work, the absolute value of Fs[k] is used instead of its
original complex value. After transforming all f[k] to
frequency domain, where k = 1, ..., D, we get the fol-

4

lowing feature set

F = (F1, ..., FN) =

F1[1]

...

F1[k]

...

F1[D]



...

... Fi[1]
...
... Fi[k]
...
... Fi[D]

...

...

... FN[1]
...
... FN[k]
...
... FN[D]

...



Where Fi = (Fi[1], ..., Fi[D])T , termed as a DFT feature
in this paper.

3.3. Interpolation

As mentioned in section 3.1, the number of sample
points N is diﬀerent duo to the various video length.
For two video clips u and v with N and M frames re-
spectively, let fu[k] = (fu
N[k]) and fv[k] =
(fv
1[k], fv
M[k]) indicate the k-th dimension of
CNN features. We use ∆t to indicate the sampling time
interval which is uniform for all video clips, i.e.
the
sampling rate is S = 1/∆t.

2[k], ..., fu

2[k], ..., fv

1[k], fu

1[k], Fv

1[k], Fu

After transforming fu[k] and fv[k] to frequency do-
M[k]) and

main, we obtian Fu[k] = (Fu
Fv[k] = (Fv

2[k], ..., Fu
M[k]) respectively.

2[k], ..., Fv

Fv[k] and Fv[k] have the same frequency range from 0
to S with sampling interval S /N and S /M respectively.
From equation 1, we know that the number of points
obtained in frequency domain is as same as that in time
domain. Therefore signals with more sample points in
time domain are more compactly spaced in frequency
domain than signals with less sample points.

clips have the same frequency sample interval from the
frequency range from 0 to S , as shown in Figure 2.

3.4. Feature Pooling and Fusion

So far, the CNN features are extracted at frame-level.
In order to generate a uniform video-level representa-
tion, we need to aggregate the obtained CNN features
and DFT features separately. We apply four most com-
monly used pooling methods, i.e.
average pooling,
locality-constrained linear coding (LLC), Fisher vec-
tor (FV) and vector of locally aggregated descriptors
(VLAD) to aggregate both CNN features and DFT fea-
tures in our experiments, which are brieﬂy reviewed in
this section.

3.4.1. Average Pooling

Average pooling is simply to calculate the mean value
of the feature vector. Suppose that f = (f1, ..., fN) rep-
resent the set of frame-level features extracted from a
video clip, the video-level features generated using av-
erage pooling can be represented as

favg =

1
N

N

X

i=1

fi

(2)

The dimension of the video-level features generated
by average pooling is same as the frame-level features.
The calculation of average pooling is easy. The disad-
vantage is that the temporal information between frames
is totally lost.

3.4.2. LLC Encoding

LLC, which utilises locality constraints, selects k-
nearest codewords from a dictionary learned by K-
means algorithm, and generates a sparse representation
for the input vector. Given an input vector x, which can
be a frame-level CNN feature or a DFT feature in our
case, LLC code can be obtained by solving the follow-
ing ﬁtting problem:

min

c

N

X
i=1 kx − Bck2 + λkdi ⊙ ck
s.t. 1T c = 1

(3)

Figure 2: Illustration of applying interpolation to signals with diﬀerent
points in frequency domain.

where ⊙ denotes element-wise multiplication, and di is
the locality adaptor. The solution of LLC can be derived
analytically [17] by:

Like image resizing, we use cubic interpolation
method [37] to generate a ﬁxed length (L) represen-
tation Fu[k] = (Fu
L[k]) and Fv[k] =
(Fv
L[k]). By this way, diﬀerent video

2[k], ..., Fu

2[k], ..., Fv

1[k], Fu

1[k], Fv

c = ˜c/1T

(4)
i )(B −
i )T . After that, max-pooling strategy is applied to

where ˜c = (Ci + λdiag(d))\1 and Ci = (B − 1xT
1xT
aggregate LLC-based features.

5

3.4.3. Fisher Vector Encoding

3.5. Video Classiﬁcation

Fisher vector representation does not require as many
visual words as LLC. In Fisher vector [38, 29] encoding,
the vocabularies of visual words are represented by the
means of a Gaussian mixture model (GMM), which is
learned in an unsupervised manner. Let a GMM model
with K components be denoted as Θ = {(µk, Σk, πk), k =
1, 2, .., K}, where µk, Σk and πk represent the mean, vari-
ance and prior parameter of the k-th component, respec-
tively. Let f = (f1, ..., fN) denote the set of frame-level
CNN descriptors extracted from a video clip with N
frames, then the mean and covariance deviation vectors
for the k-th component are computed as:

uk =

vk =

1

N √πk

1

N √πk

N

X

i=1
N

X

i=1

qki(

qki(

fi − µk
σk
fi − µk
σk

)

)

(5)

where qki represents the posterior probability. The con-
catenation of uk and vk of all the K components lead to
the ﬁnal Fisher vector representation.

The dimension of video-level features generated by
Fisher vector is 2DK, where D indicates the dimension
of frame-level CNN descriptor.

3.4.4. VLAD Encoding

Vector of locally aggregated descriptors (VLAD) [20]
can be viewed as a simpliﬁcation of the Fisher vector
representation. Same as LLC encoding, a visual dictio-
nary C = {c1, ..., cK} of K visual words is learned by
K-means method. Let f = (f1, ..., fN) denote the set of
frame-level CNN descriptors extracted from a video clip
with N frames. Each vector fi is associated with its near-
est visual word ci = NN(fi). Diﬀerent vector regarding
center ck can be obtained by:

uk = X

i:NN(xi)=ck

(xi − ck)

(6)

The dimension of video-level features generated by
VLAD is KD, where D represents the frame-level fea-
ture dimension. Compared with FV, the cost for calcu-
lating VLAD can be signiﬁcantly reduced.

3.4.5. Feature Fusion

After obtaining aggregated CNN features and DFT
features. We then adopt late feature fusion. The lin-
ear combination of the aggregated CNN features and
DFT features lead to the ﬁnal video-level representa-
tion, which is denoted as ˜x.

6

After obtaining all video-level features, an SVM is
trained by optimising the following equation [39] for
video classiﬁcation.

arg min
w,b

1
2kwk2 + C

T

X

i=1

max(1 − yi(wT xi) + b), 0)

(7)

Where xi and yi represent video-level feature and its cor-
responding label, C and T indicate penalty parameter
and the number of training features respectively.

4. Experiments

Recently, video emotion classiﬁcation and action
recognition have attracted intensive research eﬀorts. In
this section, in order to evaluate the eﬀectiveness of
the proposed DFT-based temporal information model,
two sets of experiments were conducted: video emotion
classiﬁcation and action recognition.

4.1. Video Emotion Classiﬁcation

In our experiments, we intended to evaluate:

(1)
the performance of CNN features with diﬀerent pool-
ing methods; (2) the performance of DFT features with
diﬀerent pooling methods; and (3) the overall perfor-
mance of combined CNN features (with diﬀerent pool-
ing methods) and DFT features (with diﬀerent pooling
methods). Moreover, to prove the eﬃciency of the pro-
posed method, we compared our results with the most
recent three works [1], [2] and [3].

4.1.1. Video Emotion Dataset

VideoEmotion-8 dataset It contains 1,101 user-
generated videos labelled with 8 basic human emotion
categories. There are at least 100 videos in each cat-
egory. The average duration of the 1,101 videos is
107 seconds. Currently, this is the largest dataset avail-
able for recognising emotions in user-generated videos.
These videos were collected from popular video shar-
ing websites, i.e. Youtube and Flicker. Similar as
[1], we randomly selected 2/3 data from each category
for training and the rest for testing. Experiments were
conducted ten times. The average accuracy of the ten
times was calculated to evaluate the classiﬁcation per-
formance. For computation eﬃciency, we sampled a
frame every 15 frames.

4.1.2. Implementation Details

The activation of fc7 were extracted as frame-level
features using the Caﬀe toolkit [26], and were further
ℓ2 normalised. We implemented LLC according to [17].
FV and VLAD representation were generated by utilis-
ing vlfeat [40].

Fast Fourier transform (FFT) [41] was adopted to
compute DFT. The dimension of CNN features were re-
duced from 4,096 to 1,024 using principal component
analysis (PCA). At the interpolation step, L (mentioned
in section 3.3) was set to 500 experimentally. For CNN
and DFT features, diﬀerent feature pooling methods
were applied and compared. We trained a vocabulary
with 1,024 codewords for LLC, 16 codewords for FV,
and 16 codewords for VLAD. In our experiments, the
aggregated CNN features were normalised to 3/5 and
the aggregated DFT features were normalised to 2/5.

We applied the LibLinear toolbox [39] for SVM clas-
siﬁcation. The penalty parameter C was set to 100 ex-
perimentally.

4.1.3. Experimental Results and Discussion

Evaluation of CNN Feature and DFT Features The
performance of CNN features and DFT features, each
with four pooling methods, are shown in Table 1. From
Table 1, we ﬁnd that the performance of CNN fea-
tures are better than DFT features, with the four pool-
ing methods. For both CNN features and DFT features,
FV achieves the best performance, with the accuracy of
65.5% and 50.2% respectively, although the dimension
of video-level features aggregated by FV is the highest
among the four pooling methods. The performances of
LLC and VLAD are similar as average pooling.

Evaluation of Combining CNN features and DFT
features The results of combining CNN features and
DFT features, as listed in Table 2, indicate that, by con-
catenating DFT features, the classiﬁcation accuracy can
be improved. CNN features and DFT features comple-
ment each other to achieve satisfactory results. The con-
catenation of CNN features with FV and DFT features
with FV achieves the best performance 70.2%.

Comparison with State-of-the-arts Results In or-
der to demonstrate the eﬀectiveness of our approach, we
compare our results with the most recent three works,
namely [1, 2, 3]. Comparison results are shown in Ta-
ble 3, from which we can ﬁnd:

(1) In comparison to [1], [2] and [3], the accuracy of
using CNN features with FV encoding improves
19.4%, 15.6% and 14,1% respectively. In [1], [2]
and [3], the authors used low-level visual features,
audio features and attribute features, whereas CNN

7

features were applied in our work. Experimental
results demonstrate that the performance of CNN
features may be superior than hand-crafted fea-
tures.

(2) While using DFT features only can not improve
the classiﬁcation performance, the performance of
DFT features with FV encoding is competitive
with [1, 2, 3].

(3) The highest classiﬁcation accuracy is 70.2%,
which is obtained by the concatenation of CNN
features and DFT features with FV encoding.
Combining DFT features achieves 4.7% better than
the performance of using CNN features only. In
addition, our best results outperform [1] 24.1%, [2]
20.3% and [3] 19.1%, which is a signiﬁcant im-
provement. To the best our knowledge, our method
achieves the best performance at the moment on
the VideoEmotion-8 dataset.

4.2. Action Recognition
4.2.1. Action Recogntion Dataset

UCF-101 dataset It consists of 13,320 videos cate-
gorised into 101 human action categories with an aver-
age of 180 frames per video and a total of 27 hours of
video data. Downloaded from YouTube, these videos
have ﬁxed frame rate (25 FPS) and resolution (320 ×
240). Currently, UCF-101 dataset is one of the most
challenging datasets for action recognition, due to its
large number of categories. Following the original eval-
uation scheme in [42], we use three train/test splits. The
average accuracy over the three splits is used to measure
the ﬁnal performance.

4.2.2. Implementation Details

For UCF-101 dataset, two types of features were ex-

tracted in our experiments.

CNN features Similar as video emotion recognition,
the CNN model pre-trained on ImageNet was
adopted for feature extraction. Unlike video emo-
tion recognition, the activation of fc6 were ex-
tracted as frame-level features, followed by ℓ2 nor-
malisation. The dimension of fc6 is 4,096.

C3D features For C3D feature extraction, we utilised
the public available deep 3-dimension convolu-
tional networks (3D ConvNets) [6], which was pre-
trained on I380K and ﬁne-tuned on Sports-1M. “To
extract C3D features, a video is split into 16 frame
long clips with a 8-frame overlap between two con-
secutive clips.” The activation of fc6 were extracted
as the features, followed by ℓ2 normalisation. The
dimension of fc6 is 4,096.

Method
CNNAvg
CNNLLC
CNNFV
CNNVLAD
DFTAvg
DFTLLC
DFTFV
DFTVLAD

Anger Anticipation Disgust
56.8
60.2
48.5
44.1
42.1
33.7
68.2
46.5

42.5
46.9
71.6
49.4
15.3
15.8
37.2
12.5

58.4
55.6
68.4
70.1
32.4
41.9
42.9
35.8

Fear
67.4
68.3
76.9
65.1
54.4
38.3
53.0
55.2

Joy
68.5
62.6
64.3
54.8
48.8
45.8
33.3
42.5

Sadness

Surprise Trust Overall

64.5
57.1
67.0
64.8
47.3
50.0
68.2
57.6

78.3
72.4
75.5
55.3
69.5
66.2
65.6
72.3

40.3
48.7
51.8
56.4
20.3
23.0
33.3
30.0

59.6
58.9
65.5
57.5
41.3
39.3
50.2
43.7

Table 1: Prediction accuracy (%) of each emotion category using CNN and DFT features on VideoEmotion-8 dataset.

Method
CNNAvg
CNNAvg + DFTAvg
CNNAvg + DFTLLC
CNNAvg + DFTFV
CNNAvg + DFTVLAD
CNNLLC
CNNLLC + DFTAvg
CNNLLC + DFTLLC
CNNLLC + DFTFV
CNNLLC + DFTVLAD
CNNFV
CNNFV + DFTAvg
CNNFV + DFTLLC
CNNFV + DFTFV
CNNFV + DFTVLAD
CNNVLAD
CNNVLAD + DFTAvg
CNNVLAD + DFTLLC
CNNVLAD + DFTFV
CNNVLAD + DFTVLAD

Anger Anticipation Disgust
56.8
60.0
58.2
64.4
58.5
60.2
61.8
60.9
63.5
61.2
48.5
57.1
62.1
66.8
60.6
44.1
55.9
58.2
65.3
61.5

42.5
44.7
46.9
37.8
42.2
46.9
43.8
48.8
40.3
45.0
71.6
55.0
51.9
62.2
50.3
49.4
44.4
46.3
49.7
43.8

58.4
63.9
60.0
65.0
64.2
55.6
64.7
67.9
68.7
65.0
68.4
66.3
65.5
73.7
68.7
70.1
68.4
64.2
79.2
67.9

Fear
67.4
69.1
65.7
69.8
66.1
68.3
62.0
64.1
67.0
62.8
76.9
70.9
77.0
76.1
73.7
65.1
67.2
65.6
74.3
72.4

Joy
68.5
66.2
66.2
57.5
68.2
62.6
67.8
64.5
68.2
65.5
64.3
78.3
80.3
67.0
77.2
54.8
66.0
66.3
60.5
68.3

Sadness

Surprise Trust Overall

64.5
59.7
66.1
69.4
62.4
57.1
57.6
56.7
65.2
66.1
67.0
63.9
68.2
78.5
69.4
64.8
60.3
62.1
71.5
62.1

78.3
76.6
74.3
70.6
74.4
72.4
72.7
72.3
80.6
78.6
75.5
85.6
85.7
82.1
87.8
55.3
76.6
72.5
72.7
74.9

40.3
40.9
46.1
47.3
46.4
48.7
53.0
52.7
57.0
55.2
51.8
52.1
51.5
55.5
51.5
56.4
48.8
52.1
54.2
46.1

59.6
60.1
60.4
60.2
60.3
58.9
60.4
61.6
63.8
62.4
65.5
66.2
67.8
70.2
67.4
57.5
61.0
60.9
65.9
62.1

Table 2: Prediction accuracy (%) of each emotion category using the concatenation of CNN and DFT features using diﬀerent pooling methods.

Method
Jiang[1]
Pang[2]
Pang[3]
CNNFV
DFTFV

CNNFV + DFTFV

7.6
0.34

0

Anger Anticipation Disgust
53.0
50.9
48.5
48.5
68.2
66.8

44.6
39.9
53.8
68.4
42.9
73.7

71.6
37.2
62.2

Fear
47.3
54.5
52.7
76.9
53.0
76.1

Joy
48.3
59.0
54.2
64.3
33.3
67.0

Sadness

Surprise Trust Overall

20.0
21.7
32.4
67.0
68.2
78.5

76.9
82.8
78.7
75.5
65.6
82.1

28.5
31.2
43.8
51.8
33.3
55.5

46.1
49.9
51.1
65.5
50.2
70.2

Table 3: Comparison of our results with the three latest works on VideoEmotion-8 dataset.

8

We trained a vocabulary with 1,024 codewords for
LLC, 32 codewords for FV, and 32 codewords for
VLAD. The value of L was set to 200 for CNN features
and 50 for C3D features. ℓ2 normalisation was applied
for the aggregated CNN features and DFT features. Lin-
ear SVM was applied for action recognition. The cost
parameter C was set to 1.

4.2.3. Experimental Results and Discussion

Method

STIP+BoVW [43]

Deep Net [32]

CNN + LSTM (Motion) [11]

LRCN [44]

Temporal stream ConvNet [4]
Composite LSTM Model [5]
C3D (1 nets) + linear SVM [6]

TDD [45]
CNNLLC
CNNFV
CNNVLAD
CNNAvg

DFTAvg(CNN)

C3DLLC
C3DFV
C3DVLAD
C3DAvg

DFTAvg(C3D)

CNNAvg+C3DAvg

CNNAvg+DFTAvg(CNN)
+C3DAvg+DFTAvg(C3D)

Accuracy(%)

43.9
63.3
81.4
82.9
83.7
84.3
82.3
90.3
52.7
57.9
51.2
68.8
68.8
73.0
75.7
61.2
82.3
82.9
83.7

84.1

Table 4: Prediction accuracy (%) of CNN/C3D, DFT features and the
concatenation of the two features, and comparison of our results with
previous works on UCF-101.

Action recognition results are shown in table 4. From

this table we can ﬁnd:

(1) The

of DFTAvg(C3D)
competitive with

and
using

performances
are

DFTAvg(CNN)
CNN features and C3D features.

(2) To our surprise, the advanced pooling strategies,
e.g. LLC, FV and VLAD, achieve lower accu-
racy than simple average pooling. This might be-
cause that the similarity among frames of UCF-101
videos is much higher than that of VideoEmotion-8
videos.

(3) The combination of CNNAvg, DFTAvg(CNN),
C3DAvg, DFTAvg(C3D) improves 0.4%, compared
to combining CNNAvg with DFTAvg(CNN). The

9

results demonstrate the eﬀectiveness of DFT fea-
tures. The improvement of adding DFT features
for action recognition is not as signiﬁcant as that
for video emotion recognition. One of the pos-
sible reasons might be that combining CNN and
C3D features had already achieved satisfactory ac-
curacy. Only small space was left for further im-
provements.

(4) Compared with the state-of-the-art action recog-
nition results, our best result (84.1%) is competi-
tive. Our result performs 0.2% worse than com-
posite LSTM model. However, composite LSTM
model uses both spatial image features and optical
ﬂow features, whereas we only use CNN features
and C3D features without using optical ﬂow fea-
tures. Our method also outperforms LRCN [44]
1.2% and temporal stream ConvNet [4] 0.4% re-
spectively. Our method achieves lower perfor-
mance than [45]. A possible reason might be that
we directly apply teh CNN model pre-trained on
ImageNet for feature extraction without any ﬁne-
tuning. The main purpose of this work is to prove
the eﬀectiveness of DFT features rather than chal-
lenging the best performance.

5. Conclusions

In this paper, we have proposed to analyse features
in frequency domain transformed by DFT. In our ap-
proach, CNN and DFT features are adopted to jointly
model spatial and temporal information for video classi-
ﬁcation. Capturing temporal information, DFT features
have been proved to be eﬃcient and eﬀective for both
video emotion classiﬁcation and action recognition.
The combination of CNN and DFT features achieves the
state-of-the-art performance on VideoEmotion-8 dataset
and competitive results on UCF-101 dataset.

6. Acknowledgments

Haimin Zhang is supported by UTS-CSC interna-

tional research scholarship.

References

[1] Y.-g. Jiang, B. Xu, X. Xue, Predicting Emotions in User-
Generated Videos, International Conference on Artiﬁcial Intel-
ligence (AAAI) (2014) 73–79.

[2] L. Pang, C.-W. Ngo, Mutlimodal learning with deep boltzmann
machine for emotion prediction in user generated videos, in:
Proceedings of the 5th ACM on International Conference on
Multimedia Retrieval, ACM, 2015, pp. 619–622.

[3] L. Pang, S. Zhu, C.-W. Ngo, Deep multimodal learning for af-
fective analysis and retrieval, Multimedia, IEEE Transactions on
17 (11) (2015) 2008–2020.

[4] K. Simonyan, A. Zisserman, Two-stream convolutional net-
works for action recognition in videos, in: Advances in Neural
Information Processing Systems, 2014, pp. 568–576.

[5] N. Srivastava, E. Mansimov, R. Salakhutdinov, Unsupervised
learning of video representations using lstms, arXiv preprint
arXiv:1502.04681.

[6] D. Tran, L. Bourdev, R. Fergus, L. Torresani, M. Paluri, Learn-
ing spatiotemporal features with 3d convolutional networks,
arXiv preprint arXiv:1412.0767.

[7] H. Wang, A. Kl¨aser, C. Schmid, C.-L. Liu, Action recognition
by dense trajectories, in: Computer Vision and Pattern Recogni-
tion (CVPR), 2011 IEEE Conference on, IEEE, 2011, pp. 3169–
3176.

[8] N. Dalal, B. Triggs, Histograms of oriented gradients for human
detection, in: Computer Vision and Pattern Recognition, 2005.
CVPR 2005. IEEE Computer Society Conference on, Vol. 1,
IEEE, 2005, pp. 886–893.

[9] I. Laptev, M. Marszałek, C. Schmid, B. Rozenfeld, Learning
realistic human actions from movies, in: Computer Vision and
Pattern Recognition, 2008. CVPR 2008. IEEE Conference on,
IEEE, 2008, pp. 1–8.

[10] N. Dalal, B. Triggs, C. Schmid, Human detection using oriented
histograms of ﬂow and appearance, in: Computer Vision–ECCV
2006, Springer, 2006, pp. 428–441.

[11] Z. Wu, X. Wang, Y.-G. Jiang, H. Ye, X. Xue, Modeling spatial-
temporal clues in a hybrid deep learning framework for video
classiﬁcation, arXiv preprint arXiv:1504.01561.

[12] S. Hochreiter, J. Schmidhuber, Long short-term memory, Neural

computation 9 (8) (1997) 1735–1780.

[13] C. Rader, Discrete fourier transforms when the number of data
samples is prime, Proceedings of the IEEE 56 (6) (1968) 1107–
1108.

[14] D. G. Lowe, Distinctive image features from scale-invariant key-
points, International journal of computer vision 60 (2) (2004)
91–110.

[15] J. Yang, Y.-G. Jiang, A. G. Hauptmann, C.-W. Ngo, Evaluat-
ing bag-of-visual-words representations in scene classiﬁcation,
in: Proceedings of the international workshop on Workshop on
multimedia information retrieval, ACM, 2007, pp. 197–206.

[16] J. Yang, K. Yu, Y. Gong, T. Huang, Linear spatial pyramid
matching using sparse coding for image classiﬁcation, in: Com-
puter Vision and Pattern Recognition, 2009. CVPR 2009. IEEE
Conference on, IEEE, 2009, pp. 1794–1801.

[17] J. Wang, J. Yang, K. Yu, F. Lv, T. Huang, Y. Gong, Locality-
IEEE

constrained linear coding for image classiﬁcation,
Conference on CVPR, 2010., pp. 3360–3367.

in:

[18] F. Perronnin, C. Dance, Fisher kernels on visual vocabularies for
image categorization, in: Computer Vision and Pattern Recog-
nition, 2007. CVPR’07. IEEE Conference on, IEEE, 2007, pp.
1–8.

[19] F. Perronnin, J. S´anchez, T. Mensink, Improving the ﬁsher ker-
nel for large-scale image classiﬁcation, in: Computer Vision–
ECCV 2010, Springer, 2010, pp. 143–156.

[20] H. J´egou, M. Douze, C. Schmid, P. P´erez, Aggregating local
descriptors into a compact image representation, in: Computer
Vision and Pattern Recognition (CVPR), 2010 IEEE Conference
on, IEEE, 2010, pp. 3304–3311.

[21] L.-J. Li, H. Su, L. Fei-Fei, E. P. Xing, Object bank: A high-level
image representation for scene classiﬁcation & semantic feature
sparsiﬁcation, in: Advances in neural information processing
systems, 2010, pp. 1378–1386.

[22] D. Borth, T. Chen, R. Ji, S.-F. Chang, Sentibank:

large-scale

10

ontology and classiﬁers for detecting sentiment and emotions in
visual content, in: Proceedings of the 21st ACM international
conference on Multimedia, ACM, 2013, pp. 459–460.

[23] S. Sadanand, J. J. Corso, Action bank: A high-level represen-
tation of activity in video, in: Computer Vision and Pattern
Recognition (CVPR), 2012 IEEE Conference on, IEEE, 2012,
pp. 1234–1241.

[24] L.-J. Li, H. Su, Y. Lim, L. Fei-Fei, Object bank: An object-level
image representation for high-level visual recognition, Interna-
tional journal of computer vision 107 (1) (2014) 20–39.

[25] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, L. Fei-Fei, Ima-
genet: A large-scale hierarchical image database, in: IEEE Con-
ference on CVPR, 2009., IEEE, 2009, pp. 248–255.

[26] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Gir-
shick, S. Guadarrama, T. Darrell, Caﬀe: Convolutional archi-
tecture for fast feature embedding, in: Proceedings of the ACM
International Conference on Multimedia, ACM, 2014, pp. 675–
678.

[27] A. S. Razavian, H. Azizpour, J. Sullivan, S. Carlsson, Cnn
features oﬀ-the-shelf:
an astounding baseline for recogni-
tion, in: Computer Vision and Pattern Recognition Workshops
(CVPRW), 2014 IEEE Conference on, IEEE, 2014, pp. 512–
519.

[28] B. Zhou, A. Lapedriza, J. Xiao, A. Torralba, A. Oliva, Learning
deep features for scene recognition using places database, in:
Advances in Neural Information Processing Systems, 2014, pp.
487–495.

[29] Z. Xu, Y. Yang, A. G. Hauptmann, A discriminative
cnn video representation for event detection, arXiv preprint
arXiv:1411.4006.

[30] H. Wang, C. Schmid, Action recognition with improved trajec-
tories, in: Proceedings of the IEEE International Conference on
Computer Vision, 2013, pp. 3551–3558.

[31] H. Kuehne, H. Jhuang, E. Garrote, T. Poggio, T. Serre, Hmdb:
a large video database for human motion recognition, in: Com-
puter Vision (ICCV), 2011 IEEE International Conference on,
IEEE, 2011, pp. 2556–2563.

[32] A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar,
L. Fei-Fei, Large-scale video classiﬁcation with convolutional
neural networks, in: Proceedings of the IEEE conference on
Computer Vision and Pattern Recognition, 2014, pp. 1725–
1732.

[33] E. Al-Shaer, W. Marrero, A. El-Atawy, K. ElBadawi, Network
conﬁguration in a box: Towards end-to-end veriﬁcation of net-
work reachability and security, in: Network Protocols, 2009.
ICNP 2009. 17th IEEE International Conference on, IEEE,
2009, pp. 123–132.

[34] I. Sutskever, O. Vinyals, Q. V. Le, Sequence to sequence learn-
ing with neural networks, in: Advances in neural information
processing systems, 2014, pp. 3104–3112.

[35] S. Ji, W. Xu, M. Yang, K. Yu, 3d convolutional neural networks
for human action recognition, Pattern Analysis and Machine In-
telligence, IEEE Transactions on 35 (1) (2013) 221–231.

[36] A. Krizhevsky, I. Sutskever, G. E. Hinton, Imagenet classiﬁca-
tion with deep convolutional neural networks, in: Advances in
neural information processing systems, 2012, pp. 1097–1105.

[37] R. G. Keys, Cubic convolution interpolation for digital image
processing, Acoustics, Speech and Signal Processing, IEEE
Transactions on 29 (6) (1981) 1153–1160.

[38] J. S´anchez, F. Perronnin, T. Mensink, J. Verbeek, Image classiﬁ-
cation with the ﬁsher vector: Theory and practice, International
journal of computer vision 105 (3) (2013) 222–245.

[39] R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, C.-J. Lin,
Liblinear: A library for large linear classiﬁcation, The Journal
of Machine Learning Research 9 (2008) 1871–1874.

[40] A. Vedaldi, B. Fulkerson, Vlfeat: An open and portable library
of computer vision algorithms, in: Proceedings of the 18th ACM
international conference on Multimedia, ACM, 2010, pp. 1469–
1472.

[41] M. Frigo, S. G. Johnson, Fftw: An adaptive software architec-
ture for the ﬀt, in: Acoustics, Speech and Signal Processing,
1998. Proceedings of the 1998 IEEE International Conference
on, Vol. 3, IEEE, 1998, pp. 1381–1384.

[42] Y. Jiang, J. Liu, A. R. Zamir, G. Toderici, I. Laptev, M. Shah,
R. Sukthankar, Thumos challenge: Action recognition with
a large number of classes,
ICCV Workshop on Action
Recognition with a Large Number of Classes, http://crcv. ucf.
edu/ICCV13-Action-Workshop, 2013.

in:

[43] K. Soomro, A. R. Zamir, M. Shah, Ucf101: A dataset of 101
human actions classes from videos in the wild, arXiv preprint
arXiv:1212.0402.

[44] J. Donahue, L. Anne Hendricks, S. Guadarrama, M. Rohrbach,
S. Venugopalan, K. Saenko, T. Darrell, Long-term recurrent
convolutional networks for visual recognition and description,
in: Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, 2015, pp. 2625–2634.

[45] L. Wang, Y. Qiao, X. Tang, Action recognition with trajectory-
pooled deep-convolutional descriptors, in: Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition,
2015, pp. 4305–4314.

11

