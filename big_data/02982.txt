6
1
0
2

 
r
p
A
6

 

 
 
]
E
M

.
t
a
t
s
[
 
 

2
v
2
8
9
2
0

.

3
0
6
1
:
v
i
X
r
a

Gaussian Processes for Functional Autoregression

Daniel R. Kowal, David S. Matteson, and David Ruppert∗

April 7, 2016

Abstract

We develop a hierarchical Gaussian process model for forecasting and inference of functional time

series data. Unlike existing methods, our approach is especially suited for sparsely or irregularly sampled

curves and for curves sampled with non-negligible measurement error. The latent process is dynami-

cally modeled as a functional autoregression (FAR) with Gaussian process innovations. We propose a

fully nonparametric dynamic functional factor model for the dynamic innovation process, with broader

applicability and improved computational eﬃciency over standard Gaussian process models. We prove

ﬁnite-sample forecasting and interpolation optimality properties of the proposed model, which remain

valid with the Gaussian assumption relaxed. An eﬃcient Gibbs sampling algorithm is developed for

estimation, inference, and forecasting, with extensions for FAR(p) models with model averaging over the

lag p. Extensive simulations demonstrate substantial improvements in forecasting performance and re-

covery of the autoregressive surface over competing methods, especially under sparse designs. Breakeven

inﬂation (BEI), the diﬀerence between nominal and real yield curves, provides a market-based estimate

of future average inﬂation, and is important for policymakers and investors. We apply the proposed

methods to estimate BEI using weekly U.S. data, which is observed sparsely and with measurement

error, and highlight its superior forecasting performance over competing methods.

KEY WORDS: functional factor analysis; hierarchical Bayes; model averaging; nonpara-

metric covariance function; time series.

∗Kowal is PhD Candidate, Department of Statistical Science, Cornell University, 301 Malott Hall, Ithaca, NY 14853 (E-mail:

drk92@cornell.edu). Matteson is Assistant Professor, Department of Statistical Science and ILR School, Cornell University,

1196 Comstock Hall, Ithaca, NY 14853 (E-mail: matteson@cornell.edu; Webpage: http://www.stat.cornell.edu/~matteson/).

Ruppert is Andrew Schultz, Jr. Professor of Engineering, Department of Statistical Science and School of Operations Research

and Information Engineering, Cornell University, 1196 Comstock Hall, Ithaca, NY 14853 (E-mail: dr24@cornell.edu; Webpage:

http://people.orie.cornell.edu/~davidr/). Financial support from NSF grant AST-1312903 (Kowal and Ruppert) and a

Xerox PARC Faculty Research Award, and NSF grant DMS-1455172 (Matteson) is gratefully acknowledged. We thank Professor

Robert A. Jarrow for suggesting the yield curve application and for his helpful discussions.

1

1

Introduction

We develop a hierarchical Gaussian process model for forecasting and inference of functional

time series data. Unlike existing methods, our approach is especially suited for sparsely or

irregularly sampled curves and for curves sampled with non-negligible measurement error.

These occur frequently in practice. A functional time series is a time-ordered sequence of
random functions, Y1, . . . , YT , on some compact index set T ⊂ RD, typically with D = 1.
Applications of functional time series are abundant, including: yearly sea surface temperature

as a function of time-of-year (Besse et al., 2000); yearly mortality and fertility rates as a

function of age (Hyndman and Ullah, 2007); daily pollution curves as a function of time-of-

day (Damon and Guillas, 2002; Aue et al., 2015); weekly interest rate curves as a function

of time to maturity (Kargin and Onatski, 2008; Kowal et al., 2016); and a vast collection of

spatio-temporal applications in which a time-dependent variable is measured as a function

of spatial location (e.g., Cressie and Wikle, 2011). The primary goal of functional time series
analysis is usually forecasting {Yt}, but we are also interested in performing inference and
obtaining an interpretable representation of the time evolution of {Yt}.

The most prevalent model for functional time series data is the functional autoregressive

model of order 1, written FAR(1):

Yt − µ = Ψ(Yt−1 − µ) + t,

(1)

where Yt ∈ L2(T ), Ψ is a bounded linear operator on L2(T ), t ∈ L2(T ) is a sequence of
independent mean zero random innovation functions with E||t||2 < ∞, and µ is the mean
of {Yt} under stationarity. The FAR(1) model, developed by Bosq (2000), is an extension of
two highly successful models: the functional linear model for function-on-function regression

and the vector autoregressive model for multivariate time series, and has been successfully

applied in a variety of applications. More generally, (1) can be extended for multiple lags to

the FAR(p) model: Yt − µ =(cid:80)p

(cid:96)=1 Ψ(cid:96)(Yt−(cid:96) − µ) + t.

2

Existing approaches for estimating the FAR(p) model typically use an eigendecomposition

of the empirical (contemporaneous and lagged) covariance operators (Damon and Guillas,

2002, 2005; Horv´ath and Kokoszka, 2012; Kokoszka, 2012) or kernel-based procedures for

modeling the conditional expectation (Besse et al., 2000). A related approach is to estimate

a multivariate time series model for the functional principal component (FPC) scores of

the observed data (Aue et al., 2015). Extensions of the FAR(1) model for nonstationary

functional time series are available, such as the time-dependent FAR kernels proposed in

Chen and Li (2015).

In general, existing methods for FAR(p) are designed for functional data observed on dense

grids without measurement error, and typically require pre-smoothing discretized functional

observations. However, such procedures may exhibit erratic behavior for sparse designs and

are inappropriate in such settings. More generally, under an FAR(p) model that includes

measurement error and discretization of the functional observations, we prove that the two

most common approaches for functional data analysis—estimators that are linear in the

FPC scores or the pre-smoothed observations—produce predictions that are inadmissible (in

a decision theory sense). Indeed, the presence of measurement error fundamentally alters the

behavior of the observable process: if an FAR process is observed with measurement error,

then the observable process is no longer an FAR process, but rather a functional autoregres-

sive moving average process (see Proposition 1). Even under dense designs, existing methods

produce poor estimates of the FAR operator Ψ (Didericksen et al., 2012), which inhibits in-
terpretability of the time evolution of {Yt}, and do not provide ﬁnite-sample inference. We
propose new methodology that simultaneously addresses all of these challenges.

We propose a general two-level hierarchy for modeling functional time series: an observa-

tion equation addresses measurement error and discretization of the functional data, while

an evolution equation deﬁnes a process model for the underlying functional time series. The

latent process is dynamically modeled as an FAR(p). We parsimoniously specify the FAR

model with mean zero Gaussian process innovations, which are fully speciﬁed by covariance

3

functions without parameterizing sample paths. The dynamic innovation process is further

speciﬁed by a dynamic functional factor model. In contrast with standard approaches for

Gaussian processes, this avoids selecting and estimating a parametric covariance function,

and allows greater computational stability and eﬃciency, and broader applicability. Inter-

polating curves at unsampled locations and forecasting future curves are primary objectives

in functional time series modeling; the proposed model produces optimal (best linear) pre-

dictions under both sparse and dense designs in the presence of measurement error, even

with the Gaussian assumption relaxed. We propose an eﬃcient Gibbs sampling algorithm

for estimation, inference, and forecasting. Extensive simulations demonstrate substantial

improvements in forecasting performance and recovery of the autoregressive surface over

competing methods, especially under sparse designs.

We apply our methodology to model and forecast market expectations of future inﬂation,

as measured by breakeven inﬂation (BEI), using weekly U.S. data. BEI is the diﬀerence

between the nominal yield curve and the real yield curve at each time t, and may be modeled

as a functional time series. Market expectations of future inﬂation are important for both

investors and policymakers: BEI aggregates information from many investors, and is available

at a higher frequency and for longer time horizons than other measures, such as surveys

(Breedon, 1995). However, real yields, and therefore BEI, are sparsely observed for each

time t, and only at longer maturities, which is problematic for existing functional time series

models. The proposed methods provide a natural hierarchical framework for jointly modeling

BEI, nominal yields, and real yields, and improve upon several competing procedures in

forecasting performance.

Bayesian methods for functional time series are limited, with the exception of Laurini

(2014) and Kowal et al. (2016). The primary contributions of this article are the following:

(i) development of a hierarchical framework for FAR(p) (Section 2), which produces optimal

(best linear) predictions under both sparse and dense designs in the presence of measurement

error; (ii) a dynamic functional factor model for the innovation covariance, which is non-

4

parametric, computationally convenient, and oﬀers useful generalizations to non-Gaussian

distributions (Section 3); (iii) a procedure for model averaging over the lag, p, within a hi-

erarchical FAR(p) model (Section 4); (iv) comparisons of the proposed methods to existing

methods for FAR(p) using theoretical results (Section 5), an extensive simulation study (Sec-

tion 6), and a real data application (Section 7); (v) a new approach for modeling breakeven

inﬂation curves, from which we obtain forecasts and prediction bands for market expecta-

tions of future inﬂation (Section 7); and (vi) an eﬃcient Gibbs sampling algorithm, which

uses common full conditional distributions and existing R software (Appendix). Details of

our Gibbs sampling algorithm and additional theoretical and simulation results are in the

web supplement.

2 Hierarchical Gaussian Processes for FAR

Let Y1, . . . , YT be a time-ordered sequence of random functions in L2(T ), where T ⊂ RD is
a compact index set. We focus on D = 1 with T = [0, 1], but the methods can be developed

more generally. For interpretability and computational convenience, we restrict our attention

to the integral operators deﬁned by Ψ(cid:96)(Y )(τ ) =(cid:82) ψ(cid:96)(τ, u)Y (u) du, so the FAR(p) model is

(cid:90)

p(cid:88)

Yt(τ ) − µ(τ ) =

ψ(cid:96)(τ, u){Yt−(cid:96)(u) − µ(u)} du + t(τ ) ∀τ ∈ T .

(2)

(cid:96)=1

Using integral operators, the FAR(p) model resembles the functional linear model, in which
(Yt − µ) is regressed on (Yt−1 − µ), . . . , (Yt−p − µ). The functional linear model is widely
popular in functional data analysis, and has been extensively studied (e.g., Cardot et al.,

1999; Ramsay, 2006).

In practice, model (2) is incomplete: the functional observations {Yt} are not observed
directly, but rather via discrete samples of each curve, and typically with measurement error.

5

Suppose that we observe yi,t ∈ R sampled with noise νi,t from Yt ∈ L2(T ):

yi,t = Yt(τi,t) + νi,t

(3)

for i = 1, . . . , mt, where τ1,t, . . . , τmt,t are the observation points of Yt and νi,t is a mean zero

measurement error with ﬁnite variance. Typically for functional data, mt will be large and
Tt = {τ1,t, . . . , τmt,t} will be dense in T . However, for our procedures, we allow mt to be
small for some (or all) t, with observation points To ≡ ∪tTt dense or sparse in T . Combining
(3) with (2) for p = 1 and deﬁning µt ≡ Yt − µ, we obtain the two-level hierarchical model

yi,t = µ(τi,t) + µt(τi,t) + νi,t,
µt(τ ) =(cid:82) ψ(τ, u)µt−1(u) du + t(τ ), ∀τ ∈ T

i = 1, . . . , mt,

(4)

for t = 2, . . . , T , where we assume that {νi,t} and {t} are mutually independent sequences.
The measurement error is a nontrivial component of model (4), which we demonstrate in

the following proposition:

Proposition 1. Let Yt−µ =(cid:80)p

(cid:96)=1 Ψ(cid:96)(Yt−(cid:96)−µ)+t, and suppose that we observe yt = Yt +νt,
where {t} and {νt} are independent white noise processes. Then the observable process {yt}
follows a functional autoregressive moving average (FARMA) process of order (p, p).

We deﬁne a FARMA process and prove Proposition 1 in Section B.1 of the web supple-

ment. The implication of Proposition 1 is that, if the true model for Yt is FAR(p), yet Yt is

observed with error, then the FAR(p) model for the observables is inappropriate. As a result,

estimation of Ψ(cid:96) will be ineﬃcient and forecasting will deteriorate, due to both increased

estimation error of Ψ(cid:96) and model misspeciﬁcation. By comparison, the hierarchical model

decomposes the observed data into a functional (autoregressive) process and measurement

error, and in doing so circumvents the model misspeciﬁcation issues implied by Proposition 1.
We model the random functions µ, ψ, and {t} as Gaussian processes: µ ∼ GP(0, Kµ),

6

ψ ∼ GP(0, Kψ), and t
indep∼ GP(0, K), where the notation GP(m, K) denotes a Gaussian
process with mean function m and covariance function K. Gaussian processes have a long

history in machine learning (Rasmussen and Williams, 2006) and spatial statistics (Cressie

and Wikle, 2011), and have seen increased application in functional data analysis, espe-

cially for hierarchical modeling (Behseta et al., 2005; Kaufman et al., 2010; Shi and Choi,
2011; Earls and Hooker, 2014). The conditional distributions of µt = Yt − µ are there-

fore [µt|µt−1, ψ, K] ∼ GP((cid:82) ψ(·, u)µt−1(u) du, K), which models the evolution of µt and

serves as the prior distribution for the observation level of (4). Notably, the model only

requires conditionally Gaussian processes, and therefore may accommodate more general

distributional assumptions, such as scale-mixtures of Gaussian distributions and stochas-

tic volatility. Moreover, the posterior expectations derived from the hierarchical Gaussian

process model are best linear predictors, and therefore are optimal among linear predictors

for interpolation and forecasting of Yt, even for non-Gaussian distributions (see Section 5).

We assume νi,t

iid∼ N (0, σ2

ν) for the measurement errors; priors for σ2

ν and the parameters

associated with Kµ, K, and Kψ will be discussed later.

2.1 Dynamic Linear Models for FAR(p)

For practical implementation of model (4), we must select a ﬁnite set of evaluation points,
Te ≡ {τ1, . . . , τM} ⊂ T , at which we wish to estimate, forecast, or perform inference on
the random functions, in particular µt = Yt − µ. Naturally, we assume that Tt ⊆ Te for
all t, but this assumption may be relaxed. Notably, Te provides a convenient structure for
forecasting and inference of yi,t and Yt(τi,t) at the observations points τi,t ∈ Tt, as well as
interpolation of Yt at any unobserved points, τ∗ ∈ Te \ To. By deﬁnition, for any Gaussian
process x ∼ GP(m, K) deﬁned on T , we have x ∼ N (m, K), where x = (x(τ1), . . . , x(τM ))(cid:48),
m = (m(τ1), . . . , m(τM ))(cid:48), and K = {K(τi, τk)}M
i,k=1. This result is particularly useful for
constructing an estimation procedure and deriving the optimality results of Section 5.

7

By selecting M large and Te dense in T , we can accurately approximate the integral in

(4) using quadrature methods:

(cid:90)

ψ(τ, u)µt−1(u) du ≈ (ψ(τ, τ1), . . . , ψ(τ, τM )) Qµt−1,

(5)

where Q is a known quadrature weight matrix and µt−1 = (µt−1(τ1), . . . , µt−1(τM ))(cid:48). The
approximation in (5) is important for computational tractability in estimation of both µt

and ψ. In practice, the trapezoidal rule for computing Q works well, and in general M =
30 is suﬃciently large. Notably, unlike integrals computed using the observations {yi,t}
in most functional data methods, such as functional principal components analysis, the
integral approximation in (5) is computed using the process {µt}, which is uncorrupted by
measurement error.

Assuming To ⊆ Te, let Z t be the mt × M incidence matrix that identiﬁes the observa-
tions points observed at time t, i.e., (τ1,t, . . . , τmt,t)(cid:48) = Z t(τ1, . . . , τM )(cid:48). We can write the
hierarchical model (4) as a dynamic linear model (DLM; West and Harrison, 1997) in µt:



yt = Z tµ + Z tµt + ν t,

µt = G(ψ)µt−1 + t,
µ1 ∼ N (0, K ),

[ν t|σ2
ν]
[t|K ]

indep∼ N (0, σ2
indep∼ N (0, K ) for t = 2, . . . , T,

νI mt) for t = 1, . . . , T,

(6)

where yt = (y1,t, . . . , ymt,t)(cid:48), µ = (µ(τ1), . . . , µ(τM ))(cid:48), G(ψ) = ΨQ for Ψ = {ψ(τi, τk)}M
and K  = {K(τi, τk)}M

i,k=1. Model (6) can be extended for multiple lags to the FAR(p) model

by replacing the second level with µt = (cid:80)p

i,k=1,

(cid:96)=1 G(ψ(cid:96))µt−(cid:96) + t, where G(ψ(cid:96)) = Ψ(cid:96)Q and

Ψ(cid:96) = {ψ(cid:96)(τi, τk)}M
i,k=1. The DLM formulation of the FAR(p) is useful for MCMC sampling,
since eﬃcient samplers exist for the vector-valued state variables, {µt} (e.g., Durbin and
Koopman, 2002). The proposed Gibbs sampling algorithm for model (6) (see Section A of

the web supplement) is a moderate extension of traditional DLM samplers, and iteratively
samples the state vectors {µt}, the measurement error variance σ2

ν, the innovation covaraince

8

K , and the unknown evolution matrix components of G(ψ).

The evolution equation of (6) resembles a VAR(1) on µt = (µt(τ1), . . . , µt(τM ))(cid:48), where
µt(τ ) = Yt(τ )− µ(τ ), but diﬀers from a standard VAR on yt for a few critical reasons. First,
ﬁtting a VAR to yt is only well-deﬁned if both the dimension mt and the observation points
Tt are ﬁxed over time. If this does not hold, then imputation is necessary. Our procedure
imputes automatically and optimally using the conditional mean function and the conditional

covariance function of the corresponding Gaussian process. Second, the components of yt are

likely highly correlated due to the functional nature of the observations. Strong collinearity in

VARs can cause overﬁtting and adversely aﬀect forecasting and inference. In our model, the

kernel function ψ is regularized using a smoothness prior (see Section 4), which mitigates

the adverse eﬀects of collinearity on estimation of ψ. Finally, the quadrature matrix, Q,

is absorbed into the VAR coeﬃcient matrix G(ψ), and reweights the vector µt−1 using
information from the evaluation points Te. This reweighting incorporates not only the vector
values µt, but also the information that the components of µt correspond to ordered elements
of Te, which need not be equally spaced. The simulations of Section 6 demonstrate the
substantial improvements in forecasting of our procedure relative to a VAR on yt.

3 A Dynamic Functional Factor Model for the Innovation Process

The standard approach for Gaussian process models is to select a parametric covariance

function that only depends on a few parameters, and then estimate those parameters us-

ing either fully Bayesian methods or empirical Bayes (Rasmussen and Williams, 2006). The

choice of the covariance function determines the properties of the sample trajectories, such as

smoothness and periodicity, but notably does not imply a parametric form for the sample tra-

jectories. Indeed, the FAR(1) model (6) may be estimated using these standard approaches;

we provide one implementation in Section 6.

However, there are substantial computational limitations that accompany standard para-

9

metric covariance functions. Even when the covariance function is known up to some param-

eters ρ, in general we cannot directly sample from the full conditional posterior distribution

for ρ. As a result, posterior sampling for ρ can be ineﬃcient. Gaussian processes also require
computation of the M × M innovation covariance matrix K , which must be inverted—both
for evaluating the conditional likelihood of ρ and for sampling {µt} and ψ. Most common
choices for parametric covariance functions do not oﬀer any simplifying structure for com-

puting this inverse, which may be computationally ineﬃcient and unstable.

In addition,

extensions for time-dependent covariance functions or non-Gaussian distributions are not

readily available, and further increase the diﬃculties with posterior sampling.

We propose a low-rank, fully nonparametric approach for modeling the innovation covari-

ance function. Using the functional dynamic linear model (FDLM) of Kowal et al. (2016), we

estimate the unknown covariance function using a functional factor model, which does not

require speciﬁcation of a parametric form for the covariance function. This method avoids
the need for inversion of the full M × M covariance matrix, and is more computationally sta-

ble and eﬃcient. The integration of the FDLM into (6) retains the fully Bayesian hierarchical

structure, and permits joint inference for all parameters via an eﬃcient MCMC sampling

algorithm. A functional factor model is most appropriate because t is a Gaussian process
with covariance function K, so K must be well-deﬁned on T × T . Notably, the FDLM
oﬀers convenient generalizations for stochastic volatility models (Kim et al., 1998) and more

robust models using scale-mixtures of Gaussian distributions (Fernandez and Steel, 2000).
The FDLM decomposes the innovations t into factor loading curves (FLCs), φj ∈ L2(T ),

and time-dependent factors, ej,t ∈ R, for j = 1, . . . , J:

J(cid:88)

t(τ ) =

ej,tφj(τ ) + ηt(τ ),

(7)

j=1

where J is the number of factors and {ηt} is the mean zero approximation error with
η1(τ = u) and 1(·) is the indicator function. We
ηt

iid∼ GP(0, Kη), where Kη(τ, u) = σ2

10

model each FLC φj as a smooth function admitting the basis expansion φj(τ ) = b(cid:48)
where bφ is a Jφ-dimensional vector of known basis functions and ξj is an unknown vector

φ(τ )ξj,

of coeﬃcients. For superior MCMC performance, we prefer the low-rank thin plate spline

basis for bφ (e.g., Crainiceanu et al., 2005) with knot locations selected using the quantiles
of the observation points, To. We place a smoothness prior on each ξj, which is expressed
via a conditionally conjugate Gaussian distribution and is convenient for eﬃcient posterior

sampling (see the Appendix). The smoothness assumption typically produces more inter-
pretable FLCs {φj} and can improve estimation for unobserved points τ∗ (cid:54)∈ To. For the
factors et = (e1,t, . . . , eJ,t)(cid:48), we assume [et|Σe]
for simplicity. By comparison, the factors in Kowal et al. (2016) are time-dependent; we

indep∼ N (0, Σe), with Σe = diag(cid:0){σ2

j}J

(cid:1)

j=1

assume independence to obtain a special case of the FDLM in which the implied innovation
process {t} is an independent sequence. Importantly, we obtain a nonparametric, low-rank
approximation to the innovation covariance, K, with useful computational simpliﬁcations.

For identiﬁability, we order the factors according to variability of t explained, σ2

1 > σ2

2 >

··· > σ2
J > 0, and require orthonormality of the FLCs. It is computationally convenient to
enforce the discrete orthonormality constraint Φ(cid:48)Φ = I J, where Φ = BφΞ is the M × J
matrix of FLCs evaluated at Te, Bφ = (bφ(τ1), . . . , bφ(τM ))(cid:48) is the M × Jφ matrix of basis
functions evaluated at Te, and Ξ = (ξ1, . . . , ξJ) is the Jφ × J matrix of unknown FLC
basis coeﬃcients. The implied covariance matrix for t = (t(τ1), . . . , t(τM ))(cid:48) under (7)
is K  = ΦΣeΦ(cid:48) + σ2
orthonormality constraint oﬀers a substantial simpliﬁcation for computing the inverse of K 

ηI M , conditional on {φj, σ2

Importantly, the discretized

j} and σ2
η.

using the Woodbury identity:

(cid:0)Σ−1

η Φ(cid:48)Φ(cid:1)−1 = diag(cid:0){σ2

where ˜Σe = σ−2

η

e + σ−2

K−1

 = σ−2

η I M − σ−2

η Φ ˜ΣeΦ(cid:48),

(8)

(cid:1). As a result, K−1

 may be

j /(σ2

η + σ2

j )}J

j=1

computed without any matrix inversions. By comparison, parametric covariance functions

11

not only fail to oﬀer computational simpliﬁcations for K−1
computations of K−1
sampling algorithm for the factors {ej,t}, the FLCs {φj}, and the variances {σ2
computationally inexpensive and MCMC eﬃcient. Note that the approximation error is a

in the estimation of the covariance function parameters, ρ. The FDLM

, but also require additional

j} and σ2

η is





nontrivial addition to model (7): ηt is necessary for nondegeneracy of K , which is invertible

only when σ2

η > 0. And while σ2

η > 0 implies that the innovations t, and therefore µt, are

not smooth, we ﬁnd that in practice, the sample paths of t and µt do appear smooth for

suﬃciently small σ2
η.

An important application of the FDLM simpliﬁcation in (8) is given in Theorem 2, in

which we derive a computationally convenient form for estimating the out-of-sample posterior
distribution [µt(τ∗)|{yr}s
r=1] for τ∗ (cid:54)∈ Te, which includes as special cases the forecasting
distribution (s < t), the ﬁltering distribution (s = t), and the smoothing distribution (s > t).

4 Modeling the FAR Kernel

An accurate predictor of ψ is important not only for forecasting and inference, but also for

interpreting the time evolution of the functional time series, Yt. The likelihood for ψ is

speciﬁed by the evolution equation in model (6), which may be extended for multiple lags.

We select a Gaussian process prior for ψ, which encourages smoothness of the surface and
produces more interpretable results. Using the basis approximation ψ(cid:96)(τ, u) = b(cid:48)
we place a Gaussian prior on θψ(cid:96), which induces a Gaussian process prior for ψ(cid:96). A tensor

ψ(τ, u)θψ(cid:96),

product of B-spline basis functions for the bivariate basis bψ is computationally eﬃcient in

our setting, especially for large M . The details are presented in the Appendix. Notably,
the posterior distribution for ψ(cid:96) depends on K−1
many common parametric covariance functions. By comparison, the nonparametric FDLM
estimate of K−1

in (8) is computationally stable, which further stabilizes estimates of ψ(cid:96).

, which is computationally unstable for





An important choice in the FAR(p) model is the maximum lag, p: a poor choice of p

12

can produce suboptimal forecasts and reduce MCMC eﬃciency. A reasonable approach is to

compare the DIC or marginal likelihoods for diﬀerent choices of p. However, this requires re-

computing the model for each choice of p, which can be computationally intensive. Similarly,

Kokoszka and Reimherr (2013) propose a multistage hypothesis testing procedure based on

asymptotic approximations and an FPC decomposition, but would require modiﬁcation for

the hierarchical Bayesian implementation of (6).

Our approach is to select a maximum lag under consideration, pmax, and assign each lag
(cid:96) a state variable, s(cid:96) ∈ {0, 1}, for (cid:96) = 1, . . . , pmax, to assess whether or not ψ(cid:96) is included in
the model:

pmax(cid:88)

(cid:90)

µt(τ ) =

s(cid:96)

ψ(cid:96)(τ, u)µt−(cid:96)(u) du + t(τ ),

(9)

(cid:96)=1

which extends Kuo and Mallick (1998) and Korobilis (2013) to the FAR(p) setting. By
averaging over the states {s(cid:96)}pmax
(cid:96)=1 , the forecasts of model (9) are the model-averaged forecasts
over the FAR((cid:96)) models for (cid:96) = 1, . . . , pmax. Since we restrict s(cid:96) ∈ {0, 1}, rather than
strongly shrinking ψ(cid:96) toward zero, we can substantially improve computational eﬃciency: at
each MCMC iteration, we sample {µt} jointly from the FAR(p∗) extension of the DLM (6),
where p∗ = min{(cid:96) : s(cid:96)+1 = ··· = spmax = 0} is the largest lag of nonzero autocorrelation.

The joint distribution of the states is [s1, s2, . . . , spmax] = [s1](cid:81)pmax

(cid:96)=2 [s(cid:96)|s(cid:96)−1, . . . , s1], where
[s(cid:96)|s(cid:96)−1, . . . , s1] corresponds to the probability that the lag (cid:96) autocorrelation term is included
in the model, given whether the autocorrelation terms of the smaller lags (cid:96) − 1, . . . , 1 are

included in the model. The ordering of the lags is important: we assume that s(cid:96) = 0 implies

that sk is very likely also zero for all k > (cid:96), which induces a more parsimonious model. The

information in this assumption may be approximately quantiﬁed by the computationally con-
venient Markov assumption [s(cid:96)|s(cid:96)−1, . . . , s1] = [s(cid:96)|s(cid:96)−1] with a very small transition probabil-
ity for P(s(cid:96) = 1|s(cid:96)−1 = 0) = q01. The reverse transition probability, P(s(cid:96) = 0|s(cid:96)−1 = 1) = q10,
encourages smaller models when it is large. By default, we select q01 = 0.01, q10 = 0.75, and
complete the joint prior distribution of {s(cid:96)}pmax
(cid:96)=1 with P(s1 = 1) = 0.9; for simulated data,

13

the posterior does not appear to be sensitive to these choices.

5 Finite-Dimensional Optimality

The Gaussian assumptions in model (6) provide convenient posterior distributions for MCMC

sampling and a useful framework for inference, but are not necessary for model (2). Suppose
we relax the Gaussian assumption to t ∼ SP(0, K), where SP(m, K) denotes a second-
order stochastic process with mean function m and covariance function K. Similarly, let
ν and let µ1 ≡ 1. Given a ﬁnite set of

νi,t be a mean zero random variable with variance σ2
evaluation points, Te ⊂ T , model (4) implies the distribution-free DLM

yt = Z tµ + Z tµt + ν t, E[ν t|σ2

µt = G(ψ)µt−1 + t,

ν] = 0, Cov[ν t|σ2

νI mt,
E[t|K ] = 0, Cov[t|K ] = K ,

ν] = σ2

(10)

under the integral approximation (5), where the vectors and matrices are deﬁned as before
and µ1 ≡ 1. Since this holds for any ﬁnite set of evaluation points Te ⊂ T , we may consider
the DLM (10) to be a collection of models indexed by the evaluation points, Te. The error
sequences, {ν t} and {t}, are assumed to be uncorrelated, rather than independent.
If
we additionally assume Gaussianity of {ν t} and {t}, then the uncorrelatedness implies
independence, and model (10) becomes model (6). Extensions for the FAR(p) models are

similar. The results below also hold for time-dependent variances for ν t and t.

Let d be an estimator of δ ∈ L2(T ), and consider the squared error loss using the Euclidean
norm: Le(δ, d) = (δ − d)(cid:48)(δ − d), where Le is indexed by the set of evaluation points,
Te, at which δ and d are evaluated to form the corresponding vectors δ and d. When
Te is a ﬁne grid on T , the loss function Le will approximate the usual loss function for

functional data, LL2(δ, d) =(cid:82) (δ(u) − d(u))2 du, for most reasonable choices of δ and d (up

to a rescaling by M = |Te|). In a standard Bayesian analysis, the goal would be to minimize
the posterior risk, E[Le(δ, d)|{yt}], for which the solution is the posterior expectation, d =

14

E[δ|{yt}].
Gaussian assumptions of model (6). However, by relaxing the distributional assumptions

Indeed, the estimators discussed below minimize the posterior risk under the

in (10) to increase the generality of the model, we no longer have suﬃcient information to

compute posterior distributions or posterior moments. In addition, it is diﬃcult to compare

Bayesian and non-Bayesian procedures under the posterior risk, and most procedures for

functional time series modeling are non-Bayesian. Therefore, we consider the overall risk
Re(δ, d) = E[Le(δ, d)], which is the expected value of the posterior risk with respect to the
sampling distribution. As with the loss function Le, the risk function Re is indexed by the
evaluation points, Te; we seek to minimize Re for any choice of Te.

(cid:9) ∪ D0 be the information available at time t, where D0 repre-

Let Dt =(cid:8)yt, yt−1, . . . , y1

sents the information prior to time t = 1.
Theorem 1. For any ﬁnite set of evaluation points, Te ⊂ T , the unique best linear pre-
dictor of the random vector [δ|Y , Θ], where δ, Y ⊆ DT ∪ {µt(τ ) : τ ∈ Te, t = 1, . . . , T}
and Θ = {µ, σ2
ν, ψ, K}, under the risk Re and conditional on model (4) with the integral
approximation (5), is the conditional expectation ˆδ(Y |Θ) ≡ E[δ|Y , Θ] as computed under

model (6).

The proof of Theorem 1 is in the Appendix, and extends fundamental results for vector-

valued DLMs. The best linear predictors of Theorem 1 equivalently minimize the risk
R(δ, d) = supTe Re(δ, d) among all linear estimators, where the sup is taken over all ﬁnite
Te such that Te ⊂ T . The most useful examples of [δ|Y , Θ] in Theorem 1 are the fore-
casting distributions [yt+h|Dt, Θ] and [µt+h|Dt, Θ] for h > 0, the smoothing distributions
[µt|DT , Θ], and the ﬁltering distributions [µt|Dt, Θ], for t = 1, . . . , T . Theorem 1 depends on
the observation points, To, only via the assumption that Z t is known. In general, we assume
To ⊆ Te, so Z t is simply an incidence matrix and therefore known. Theorem 1 does not
require To to become arbitrarily dense in T , and is valid for both sparse and dense designs.
For implementation purposes, we compute the corresponding expectations within the Gibbs

15

sampling algorithm (see Section A of the web supplement), and then average over the Gibbs

sample of Θ.

There is no intrinsic reason to restrict the estimators to linearity. However, several popular

competing methods are linear, and therefore are dominated by the conditional expectations

computed from model (6) whenever the estimators are distinct. More formally:
Corollary 1. Consider a basis expansion of the observations yt ≈ Btθt, where B(cid:48)
(b(τ1,t), . . . , b(τmt,t)), b is a known J-dimensional vector of basis functions, and θt is the
corresponding J-dimensional vector of unknown basis coeﬃcients. If the estimator ˆθt of θt
is linear in yt, then estimates or forecasts of the form H ˆθt + h, conditional on the matrix
H and the vector h, are inadmissible for all [δ|Y ] whenever H ˆθt + h (cid:54)= ˆδ(Y |Θ).

t =

The most important application of Corollary 1 is to characterize the inadmissibility of

procedures based on FPC scores. In the notation of Corollary 1, let b be the FPC basis, which

we assume is ﬁxed and known. The components of θt correspond to the FPC scores, deﬁned

by θj,t =(cid:82) {Yt(u) − µ(u)}bj(u) du =(cid:82) µt(u)bj(u) du. There are two standard approaches for

computing FPC scores: quadrature methods for dense designs absent measurement error,

and the PACE procedure of Yao et al. (2005), which uses conditional expectations under a

Gaussian assumption and applies more generally. In both cases, the FPC scores are linear

in yt, so Corollary 1 applies.

Among functional time series methods, the most pertinent procedures are Aue et al. (2015)

and Hyndman and Ullah (2007). Aue et al. (2015) provide the more general framework, in

which they compute the best linear predictors for the FPC scores, and then forecast the FPC

scores using multivariate time series methods. For time series methods that are linear in the

FPC scores, such forecasts are inadmissible. While Aue et al. (2015) undoubtedly provide

a simple yet general framework for forecasting a functional time series, the simulations of

Section 6 conﬁrm the consequence of inadmissibility on forecasting performance.

Corollary 2. Consider the common functional data pre-processing procedure in which the

16

discrete, noisy observations, yt, are replaced by estimated functions evaluated on a ﬁne grid,

ˆyt, and then estimates and forecasts are computed using the functional “data” ˆyt. If ˆyt is
linear in {yt}, then any estimator or forecast linear in {ˆyt} is inadmissible for all [δ|Y ]
whenever ˆyt (cid:54)= ˆδ(Y |Θ).

Typically, ˆyt is estimated using splines or kernel smoothers, both of which are linear in

yt. As an application of Corollary 2, the simple forecasting method of ﬁtting a VAR to ˆyt

evaluated on a grid of points, conditional on the VAR coeﬃcient matrix, is inadmissible.
Corollary 3. The unique best linear predictor of [µt(τ∗)|Ds] for any times t, s and any point
τ∗ ∈ T is the corresponding expectation under model (6).

Model (6) achieves the optimality of a kriging estimator for interpolation of any point

τ∗ ∈ T , simply by adding τ∗ to the evaluation set Te.

In practice, we need not include all such τ∗ in Te: we can estimate the out-of-sample
posterior distribution [µt(τ∗)|Ds] for τ∗ (cid:54)∈ Te by sampling from the out-of-sample full con-

(cid:3) within the Gibbs sampler, and then averag-

ditional distribution (cid:2)µt(τ∗)|{µr}T

r=1, Θ,Ds
r=1 and Θ. Let ψ(cid:48)(τ∗) ≡ (ψ(τ∗, τ1), . . . , ψ(τ∗, τM )) and
ing over the Gibbs sample of {µr}T
φ(cid:48)(τ∗) ≡ (φ1(τ∗), . . . , φJ(τ∗)). The FDLM provides a useful simpliﬁcation of this full con-
ditional distribution:
Theorem 2. Suppose τ∗ ∈ T such that τ∗ (cid:54)∈ Te. Under the FDLM (7) and conditional on
model (4) with the integral approximation (5), the out-of-sample full conditional distribution

(cid:3) ∼ N (mt(τ∗), Kt(τ∗)), where mt(τ∗) = ψ(cid:48)(τ∗)Qµt−1 +

r=1, Θ,Ds

of µt(τ∗) is (cid:2)µt(τ∗)|{µr}T
φ(cid:48)(τ∗) ˜ΣeΦ(cid:48)(cid:0)µt − ΨQµt−1

(cid:1) and Kt(τ∗) = σ2

η + σ2

ηφ(cid:48)(τ∗) ˜Σeφ(τ∗).

The proof of Theorem 2 and extensions for p > 1 are in Section B.3 of the web supple-

ment. Using Theorem 2, we can eﬃciently estimate the out-of-sample posterior distribution
[µt(τ∗)|Ds] with minimal adjustments to the Gibbs sampling algorithm (see Section A.2 of the
web supplement). Note that for implementation purposes, the terms µt and µt−1 appearing
r=1|Θ,Ds

in mt(τ∗) are assumed to be sampled from the full conditional distribution(cid:2){µr}T

(cid:3).

17

6 Simulations

We conducted extensive simulations to evaluate the proposed methods for FAR(p) relative to

several competitive alternatives. We are particularly interested in one-step forecasting and

recovery of the FAR kernel ψ1, and in how the associated performance varies with the sample

size T , the location and number of the observation points τ1,t, . . . , τmt,t, the kernel ψ1, and

the smoothness of the innovation process t. We also assess the performance of the model
averaging procedure of Section 4 for p ∈ {1, 2}, and compare the nonparametric FDLM

approach of Section 3 with a more standard parametric Gaussian process implementation.

6.1 Sampling Designs

For all simulations, the mean function is µ(τ ) = 1

10τ 3 sin(2πτ ), which produces the dom-
inate shape in the rightmost panels of Figure 1. The measurement errors are identically

distributed for all simulations: νi,t

iid∼ N (0, σ2

ν) with σν = 0.002. We vary the sample size

from small (T = 50) to large (T = 350) for the FAR(1) simulations, and use a moderate

0.45

sample size (T = 125) for the FAR(2) simulation. The FAR(1) kernel used for Figure 1 is the
Bimodal-Gaussian kernel, ψ(τ, u) ∝ 0.75
π(0.3)(0.4) exp{−(τ − 0.2)2/(0.3)2 − (u − 0.3)2/(0.4)2} +
π(0.3)(0.4) exp{−(τ − 0.7)2/(0.3)2− (u− 0.8)2/(0.4)2}, following Wood (2003). We also present
kernel is rescaled according to a pre-speciﬁed squared norm, Cψ(cid:96) =(cid:82)(cid:82) ψ2
results for the Linear-τ kernel, ψ(τ, u) ∝ τ , and the Linear-u kernel, ψ(τ, u) ∝ u. Each
(cid:80)p

(cid:96)=1 Cψ(cid:96) < 1 for stationarity. We select Cψ1 = 0.8 for the FAR(1) simulations and use

(cid:96) (τ, u) dτ du, with

(Cψ1, Cψ2) = (0.4, 0.2) for the FAR(2) simulation; smaller values of Cψ(cid:96) produce similar

comparative results, but the forecasting performance deteriorates for all methods. For the

innovation process, t, we consider both smooth and non-smooth Gaussian processes. We use

the covariance function parametrization K = σ2Rρ, where Rρ is the Mat´ern correlation func-
tion Rρ(τ, u) = {2ρ1−1Γ(ρ1)}−1 (||τ − u||/ρ2)ρ1 Kρ1(||τ − u||/ρ2), Γ(·) is the gamma function,
Kρ1 is the modiﬁed Bessel function of order ρ1, and ρ = (ρ1, ρ2) are parameters (Mat´ern,

18

2013). We let σ = 0.01 and ρ = (ρ1, 0.1), with ρ1 = 2.5 for smooth (twice-diﬀerentiable)

sample paths and ρ1 = 0.5 for non-smooth (continuous, non-diﬀerentiable) sample paths.

We consider three sampling designs for the observation points: dense, sparse-random,
and sparse-ﬁxed. In each case, the set of evaluation points, Te, is an equally-spaced grid of
M = 30 points on T = [0, 1]. The dense design uses mt = 25 equally-spaced observation
points on [0, 1] for all t, for which the results are representative of denser (mt (cid:29) 25) designs
and similar to those of Didericksen et al. (2012); see Section C of the web supplement.

The sparse-random design is generated by ﬁrst sampling each mt from a zero-truncated
Poisson (5) distribution, and then sampling τ1,t, . . . , τmt,t without replacement from Te. This
is a common design in sparse functional data, in which mt may be small for some t, but To
is dense in T . The sparse-ﬁxed design uses mt = 8 equally-spaced points in T . This is the
most challenging design, and one for which multivariate time series methods should be most

competitive with functional time series methods.

6.2 Competing Estimators

Within the proposed framework and using the FDLM of Section 3 for the innovation co-

variance function, we compute forecasts for p = 1 (FDLM-FAR(1)), and in the FAR(2)

simulation, for p = 2 (FDLM-FAR(2)) and p = 3 (FDLM-FAR(3)). We also compute

forecasts using the model averaging procedure with pmax = 4 (FDLM-FAR(p)). To assess

the performance of the FDLM implementation, we compute forecasts using model (6) with

a parametric covariance function for K = σ2Rρ (GP-FAR(1)). We use the Mat´ern corre-

lation function for Rρ, with ρ1 = 2.5 as in the smooth Gaussian process simulations, and use
the priors σ−2 ∼ Gamma (10−3, 10−3) and ρ2 ∼ Uniform (0, Uρ2), where Uρ2 is the maximum
value of ρ2 for which the correlation function Rρ is less than 0.99 for all pairs of evaluation

points. These models are implemented using the Gibbs sampling algorithm provided in Sec-

tion A of the web supplement, and estimates are based on 5,000 MCMC simulations after a

burn-in of 5,000. For the large sample setting (T = 350), the mean computation time per

19

1,000 MCMC simulations was 2.3 minutes for the FDLM implementation (FDLM-FAR(1))

and 4.4 minutes for the parametric model (GP-FAR(1)).

We consider several important competing methods. Let ˆyt+1 denote the one-step forecast

at time t. For baseline comparisons, we use the random-walk (RW) forecast, ˆyt+1 = yt, and
the mean (Mean) forecast, ˆyt+1 = ˆµ, where ˆµ is a smooth estimate of the mean of {ys}t
Both estimators are robust against overﬁtting, and the mean forecast is optimal when ψ = 0.
We also compute the one-step forecast based on a VAR(1) ﬁt to {ys}t
s=1 (VAR-Y). In the
sparse-random design, the observations yt were used to linear interpolate on Te prior to ﬁtting
the VAR. In the sparse-ﬁxed design, the VAR was ﬁt to the observation points, and then

s=1.

forecasts for the evaluation points were computed by ﬁtting a spline to the VAR forecasts of

the observation points. We also considered two functional data methods. First, we used the

Estimated Kernel procedure outlined in Horv´ath and Kokoszka (2012), which estimates ψ(cid:96)

in (2) using FPCs (FAR Classic); we ﬁx p = 1 for simplicity. This method has well-studied

theoretical properties and is a useful baseline for FAR models. Second, we implemented the

method of Aue et al. (2015), which we brieﬂy described in Section 5, using a VAR(1) on

the FPC scores (VAR-FPC). We compute the FPCs using the fda package with B-spline

basis functions. To avoid the ill-conditioned estimators discussed in Horv´ath and Kokoszka

(2012), we regularize via basis truncation, using 8 equally-spaced interior knots. The number
of components is selected to explain at least 95% of the variability in {yt}. For the sampling
designs considered here, this approach works well. Finally, we report the oracle forecast

(FAR Oracle) computed using the true one-step forecasts(cid:80)p

(cid:82) ψ(cid:96)(τ, u)µt−(cid:96)(u) du within

(cid:96)=1

the simulation. The oracle forecast is not actually an estimator, and is unaﬀected by sparsity

or small sample sizes.

We estimate the one-step forecasts [yT +h|y1:(T +h−1)], h = 1, . . . , 25, for all estimators un-
(cid:80)25
der consideration, and compare them using the mean squared forecast error M SF Ee =
h=1 ||Y T +h − ˆY T +h||2 where Y T +h = (YT +h(τ1), . . . , YT +h(τM )))(cid:48), which measures
the one-step forecasting performance at the evaluation points, and the mean squared er-

25M

1

20

(cid:80)M

(cid:80)M
k=1{ψ1(τi, τk) − ˆψ1(τi, τk)}2, which measures the recovery of the
ror M SEψ1 = 1
M 2
lag-1 kernel ψ1. Because Te is relatively dense in T , M SF Ee and M SEψ1 approximate the

integrated squared errors(cid:82) {YT +h(u) − ˆYT +h(u)}2 du and(cid:82)(cid:82) {ψ1(τ, u) − ˆψ1(τ, u)}2 dτ du, re-

i=1

spectively. Estimators ˆψ1 are available only for the proposed methods and FAR Classic.
For computational convenience in the proposed methods, we update {µt}T +h−1
the data y1:(T +h−1), but sample all other parameters only conditional on y1:T . DLM updating

using all of

t=1

algorithms provide recursive one-step forecasts for µt, but in general there are no convenient

updating algorithms for the other parameters. In practice, this is not a problem, but suggests

that our simulation analysis may underestimate the performance of the proposed model.

6.3 Results

We computed M SF Ee and M SEψ1 under a variety of sampling designs, each for N = 50

simulations, and present the results for a few important cases in Figures 2 and 3, respec-

tively. The ﬁgures are color-coded: multivariate methods are green, existing functional data

methods are red, the proposed methods are blue, and the oracle is gold.

For the sparse designs in Figure 2, the proposed methods are all superior to the com-

petitors, and in some cases nearly achieve the oracle performance, even though the oracle is

unaﬀected by sparsity. Figure 3 shows that the proposed methods also oﬀer a substantial

improvement in ψ1 estimation. Importantly, the proposed model with model averaging is

competitive with the known p model for both forecasting and estimation of ψ1. The model

averaging procedure of Section 4 typically identiﬁes the true p with high probability, with a

mild tendency to overestimate p. However, this behavior is encouraging: the bottom right

panel of Figure 3, in which p = 2, suggests that overestimating the lag (FDLM-FAR(3))

is preferable to underestimating the lag (FDLM-FAR(1), GP-FAR(1)) for ψ1 estimation.

The FDLM implementation of the proposed model is competitive with the parametric Gaus-

sian process implementation, even when the parametric Gaussian process model assumes the

correct (smooth) innovation distribution. Under the dense design (see Section C of the web

21

supplement), the improvements of the proposed methods over existing functional data meth-

ods are less substantial, and for T = 350 the functional data methods all nearly achieve the

oracle performance. The proposed methods, however, again provide superior recovery of ψ1.

In general, we ﬁnd that the functional data methods, in particular the proposed approaches,

outperform the multivariate methods by a wide margin, especially in the dense design. We

conclude that the proposed methods provide highly competitive forecasts and superior FAR

kernel recovery in a wide variety of important settings.

7 Nominal Yields, Real Yields, and Breakeven Inﬂation

We use the proposed methods to model and forecast market expectations of future inﬂation

based on U.S. nominal and real yield curve data. For a given currency and level of risk of a

debt, the nominal yield curve describes the interest rate at time t as a function of the length

of the borrowing period, or time to maturity, τ , while the real yield curve corresponds to an

inﬂation-adjusted interest rate. The diﬀerence between the nominal and real yield curves,

called breakeven inﬂation (BEI), provides a market-based estimate of future average inﬂation.

BEI is an important indicator of contemporaneous market behavior and expectations, and

is useful for policymakers and investors.

In practice, the U.S. real yield curve is estimated using Treasury Inﬂation-Protected

Securities (TIPS), for which payments are adjusted according to the Consumer Price Index

for All Urban Consumers (CPI-U) to provide investors with protection against inﬂation.

U.S. nominal and TIPS yield curve data are published by the Federal Reserve, which uses

actively-traded securities to ﬁt a quasi-cubic spline for each curve. Estimates of the real
and nominal yield curves are provided for maturities T R
{1, 3, 6, 12, 24, 36} ∪ T R

t = {60, 84, 120, 240, 360} and T N

t months, respectively. Notably, the real yield, and therefore BEI, is

t =

observed sparsely, and only for longer maturities. For model implementation, we rescale the
observation points T R

such that T R

t ⊂ T = [0, 1].

,T N

t and T N

t

t

22

To obtain stationarity, we model week-to-week changes in the yield curves. Let Y N

t (τ )

and Y R

t (τ ) be the week-to-week changes in the U.S. nominal yield curve and real yield curve,
t (τ ) ≡
jointly within a hierarchical model

respectively, at time t for time to maturity τ . The week-to-week change in the BEI is Y B
t (τ ) − Y R
Y N
akin to (6), where the BEI is modeled as an FAR(p) process. Suppose that we observe Y N

t (τ ). We propose to model Y N
t

t , and Y B
t

, Y R

t and

Y R
t with measurement errors: yN

i,t = Y N

iid∼ N(cid:0)0, σ2

νN

for k = 1, . . . , mR

νN
i,t

i,t for i = 1, . . . , mN

t (τi,t)+νN
t (uk,t)+νR
k,t
k,t are mean zero errors and τi,t, uk,t ∈ T . We assume
t = µ + µt as in (4), the

(cid:1). Rewriting Y B

iid∼ N(cid:0)0, σ2

t and yR

k,t = Y R

k,t

νR

i,t and νR

t , where νN

(cid:1) independently of νR
t (τ ) =(cid:80)3

nominal equation is equivalently yN

i,t = Y R

t (τi,t) + µ(τi,t) + µt(τi,t) + νN

i,t, where µt is modeled

using an FAR(p). For Y R

t , we use the popular parametric yield curve model of Diebold and
j=1 βj,tfj(τ ), where {fj} are the Nelson-Siegel basis functions
Li (2006) in which Y R
(Nelson and Siegel, 1987; see Section D of the web supplement) and the coeﬃcients {βj,t}
follow independent AR(1) models. Notably, we write the joint model for yN
k,t as
a DLM with state vector (µ(cid:48)
t, β1,t, β2,t, β3,t)(cid:48), where µt is the vector of µt evaluated at the
evaluation points Te, and we select Te to be a set of 30 approximately equally spaced points
t ⊂ Te; see Section D of the web supplement for more details. The
in T such that T R
sparsely-observed real yield curve is modeled parametrically, using data from both real and

i,t and yR

,T N

t

nominal yields, while the nominal yield curve is modeled semiparametrically as the sum

of the parametric real yield curve term, Y R

t , and the nonparametric BEI term, µ + µt, or

equivalently, as an FAR(p) with an endogenous functional predictor, Y R
t .

We assess the performance of the proposed model by computing one-step forecasts and

comparing the results to the competing methods described in Section 6, as well as the

aforementioned Diebold and Li (2006) model. We ﬁt each competing model separately to

i,t, the real yields yR

the nominal yields yN
k,t :
t }. We consider two periods: one during the last ﬁnancial crisis, in which we
τi,t = uk,t ∈ T R
estimate model parameters beginning on June 1, 2005, and compute one-step forecasts from

k,t, and the observed BEI, deﬁned as {yN

i,t − yR

November 1, 2007 to June 1, 2008, and one using more recent data, in which we estimate

23

model parameters beginning on January 1, 2013, and compute one-step forecasts from June
1, 2015 to January 1, 2016. In both periods, there are T ≥ 125 weeks for estimation and 30

weeks for forecasting. For the proposed methods, we update only the DLM state parameters
µt and {βj,t} during the forecast periods for computational convenience, while the parameters
in the competing methods are estimated using all available data.

Mean squared forecasts errors (MSFEs) for all estimators are in Table 1. The proposed

methods perform exceptionally well: in all but one case, they provide the best forecasting

performance. Notably, the FAR(p) with model averaging over p is highly competitive in both

the more volatile period around the last ﬁnancial crisis and the more recent period. The

FDLM implementation of the FAR(1) model performed slightly better than the parametric

Gaussian process implementation, and was superior in forecasting changes in BEI in both

periods.

However, Table 1 provides an incomplete picture of the competing methods: the MS-

FEs only assess performance at the observation points, T N

t

and T R

t

, and do not indicate

whether the estimated curves are reasonable. We illustrate this point in Figure 4 with

the FAR(p) posterior mean and 95% simultaneous prediction bands for the smoothing dis-

tributions (cid:2)yN

(cid:12)(cid:12){yN

k,s}(cid:3) and (cid:2)yR

t (τ ), τ ∈ Te

i,s},{yR

t (τ ), τ ∈ Te

i,s},{yR

(cid:12)(cid:12){yN

k,s}(cid:3), on July 12, 2013,

conditional on yield curve data from January 1, 2013 through January 1, 2016 (T = 156;

see Sections D and E of the web supplement for MCMC diagnostics and prediction band

observed real yields, but provides unreasonable estimates for Y R

computations, respectively). The parametric Nelson-Siegel model accurately estimates the
t over T . Using the same
parametrization, the comparable FAR(p) estimate of the real yield curve incorporates infor-

mation from the nominal yield curve, which introduces bias among the real yield observations

but reduces variance overall.

Given the exceptional forecasting performance of the proposed methods, a promising

extension is to use BEI as a forecast of inﬂation. A more nuanced approach would adjust

the BEI for an inﬂation risk premium, a liquidity premium, and a Jensen’s inequality term

24

(Ang et al., 2008; Christensen et al., 2010; Chen et al., 2010), and include relevant predictors,

such as historical inﬂation.

8 Concluding Remarks

The proposed hierarchical FAR(p) model provides a useful framework for estimation, infer-

ence, and forecasting functional time series data. Our model accommodates measurement

error and sparse observations, and produces best linear predictors in a general FAR(p) set-

ting, thereby dominating many competing functional time series models. The nonparametric

FDLM provides a more ﬂexible, computationally eﬃcient, and stable approach for model-

ing (innovation) covariance functions. Our model averaging procedure provides an eﬀective

solution to the problem of specifying p, and produces highly competitive forecasts. The sim-

ulation analysis and yield curve application suggest that the proposed FAR(p) model may

improve forecasting and estimation in a wide range of settings, and the eﬃcient MCMC sam-

pling algorithm allows us to perform exact (up to MCMC error and prior misspeciﬁcation)

inference for important parameters.

While we assumed independent factors (and therefore independent innovations) in Sec-

tion 3, we can relax this assumption and allow Σe to be a stochastic process evolving over

time. In this more general framework, the FDLM (7) can accommodate stochastic volatility

or heavier-tailed distributions for the factors, yet retains the computational simpliﬁcations

of (8) and Theorem 2. Letting Σt = diag(cid:0){σ2
variance function is Kt(τ, u) ≡ Cov (t(τ ), t(u)) =(cid:80)J

j,t}J

j=1

j=1 σ2

(cid:1), the (time-dependent) innovation co-

modeling each {σ2
(e.g., Kim et al., 1998), the time-dependence of {σ2

η1{τ = u}. By
t=1 for j = 1, . . . , J with an independent stochastic volatility model
j,t} will propagate to the innovation co-
variance functions, Kt. Similar modiﬁcations can accommodate scale-mixtures of Gaussian

j,tφj(τ )φj(u) + σ2

j,t}T

distributions for the factors (Fernandez and Steel, 2000) to induce more general distribu-
tions for the innovation process, {t}. Although these extensions are unnecessary for the

25

subperiods selected for estimation in the yield curve application of Section 7, there is strong

evidence for stochastic volatility of the factors over longer time periods (e.g., greater than 4

years).

Future work will investigate more adaptive FAR(p) models for longer, possibly nonstation-

ary functional time series through stochastic volatility, time-varying ψ(cid:96), and regime shifts. As

demonstrated in Section 7, the hierarchical FAR(p) framework is particularly convenient for

incorporating (time-dependent) endogenous predictors, and can be extended for exogenous

predictors for broader applicability.

References

Ang, A., Bekaert, G., and Wei, M. (2008). The term structure of real rates and expected

inﬂation. The Journal of Finance, 63(2):797–849.

Aue, A., Norinho, D. D., and H¨ormann, S. (2015). On the prediction of stationary functional

time series. Journal of the American Statistical Association, 110(509):378–392.

Behseta, S., Kass, R. E., and Wallstrom, G. L. (2005). Hierarchical models for assessing

variability among functions. Biometrika, 92(2):419–434.

Besse, P. C., Cardot, H., and Stephenson, D. B. (2000). Autoregressive forecasting of some

functional climatic variations. Scandinavian Journal of Statistics, pages 673–687.

Bosq, D. (2000). Linear processes in function spaces: theory and applications, volume 149.

Springer Science & Business Media.

Bosq, D. and Blanke, D. (2008). Inference and prediction in large dimensions, volume 754.

John Wiley & Sons.

Breedon, F. (1995). Bond prices and market expectations of inﬂation. Bank of England

Quarterly Bulletin, 35(2):160–65.

26

Cardot, H., Ferraty, F., and Sarda, P. (1999). Functional linear model. Statistics & Proba-

bility Letters, 45(1):11–22.

Chen, R.-R., Liu, B., and Cheng, X. (2010). Pricing the term structure of inﬂation risk

premia: Theory and evidence from TIPS. Journal of Empirical Finance, 17(4):702–721.

Chen, Y. and Li, B. (2015). An adaptive functional autoregressive forecast model to predict

electricity price curves. Journal of Business & Economic Statistics, (just-accepted):1–56.

Christensen, J. H., Lopez, J. A., and Rudebusch, G. D. (2010). Inﬂation expectations and

risk premiums in an arbitrage-free model of nominal and real bond yields. Journal of

Money, Credit and Banking, 42(s1):143–178.

Crainiceanu, C., Ruppert, D., and Wand, M. P. (2005). Bayesian analysis for penalized

spline regression using WinBUGS. Journal of Statistical Software, 14(14):1–24.

Cressie, N. and Wikle, C. K. (2011). Statistics for spatio-temporal data. John Wiley & Sons.

Damon, J. and Guillas, S. (2002). The inclusion of exogenous variables in functional autore-

gressive ozone forecasting. Environmetrics, 13:759–774.

Damon, J. and Guillas, S. (2005). Estimation and simulation of autoregressive hilbertian

processes with exogenous variables. Statistical Inference for Stochastic Processes, 8(2):185–

204.

Didericksen, D., Kokoszka, P., and Zhang, X. (2012). Empirical properties of forecasts with

the functional autoregressive model. Computational Statistics, 27(2):285–298.

Diebold, F. X. and Li, C. (2006). Forecasting the term structure of government bond yields.

Journal of Econometrics, 130(2):337–364.

Durbin, J. and Koopman, S. J. (2002). A simple and eﬃcient simulation smoother for state

space time series analysis. Biometrika, 89(3):603–616.

27

Earls, C. and Hooker, G. (2014). Bayesian covariance estimation and inference in latent

Gaussian process models. Statistical Methodology, 18:79–100.

Fernandez, C. and Steel, M. F. (2000). Bayesian regression analysis with scale mixtures of

normals. Econometric Theory, 16(01):80–101.

Gelman, A. (2006). Prior distributions for variance parameters in hierarchical models (com-

ment on article by Browne and Draper). Bayesian Analysis, 1(3):515–534.

Horv´ath, L. and Kokoszka, P. (2012). Inference for functional data with applications, volume

200. Springer Science & Business Media.

Hyndman, R. J. and Ullah, M. S. (2007). Robust forecasting of mortality and fertility rates:

a functional data approach. Computational Statistics & Data Analysis, 51(10):4942–4956.

Kargin, V. and Onatski, A. (2008). Curve forecasting by functional autoregression. Journal

of Multivariate Analysis, 99(10):2508–2526.

Kaufman, C. G., Sain, S. R., et al. (2010). Bayesian functional ANOVA modeling using

gaussian process prior distributions. Bayesian Analysis, 5(1):123–149.

Kim, S., Shephard, N., and Chib, S. (1998). Stochastic volatility: likelihood inference and

comparison with ARCH models. The Review of Economic Studies, 65(3):361–393.

Kokoszka, P. (2012). Dependent functional data. ISRN Probability and Statistics, 2012.

Kokoszka, P. and Reimherr, M. (2013). Determining the order of the functional autoregressive

model. Journal of Time Series Analysis, 34(1):116–129.

Korobilis, D. (2013). VAR forecasting using Bayesian variable selection. Journal of Applied

Econometrics, 28(2):204–230.

Kowal, D. R., Matteson, D. S., and Ruppert, D. (2016). A Bayesian multivariate functional

dynamic linear model. Journal of the American Statistical Association. (in press).

28

Krivobokova, T., Kneib, T., and Claeskens, G. (2010). Simultaneous conﬁdence bands for

penalized spline estimators. Journal of the American Statistical Association, 105(490).

Kuo, L. and Mallick, B. (1998). Variable selection for regression models. Sankhy¯a: The

Indian Journal of Statistics, Series B, pages 65–81.

Laurini, M. P. (2014). Dynamic functional data analysis with non-parametric state space

models. Journal of Applied Statistics, 41(1):142–163.

Mat´ern, B. (2013). Spatial variation, volume 36. Springer Science & Business Media.

Neal, R. M. (1999). Regression and classiﬁcation using Gaussian process priors. Bayesian

Statistics, 6:475–501.

Neal, R. M. (2003). Slice sampling. Annals of Statistics, pages 705–741.

Nelson, C. R. and Siegel, A. F. (1987). Parsimonious modeling of yield curves. Journal of

Business, 60(4):473.

Plummer, M., Best, N., Cowles, K., and Vines, K. (2006). CODA: Convergence diagnosis

and output analysis for MCMC. R News, 6(1):7–11.

Ramsay, J. O. (2006). Functional data analysis. Wiley Online Library.

Rasmussen, C. E. and Williams, C. K. (2006). Gaussian processes for machine learning. The

MIT Press.

Shi, J. Q. and Choi, T. (2011). Gaussian process regression analysis for functional data.

CRC Press.

Wand, M. and Ormerod, J. (2008). On semiparametric regression with O’Sullivan penalized

splines. Australian & New Zealand Journal of Statistics, 50(2):179–198.

West, M. and Harrison, J. (1997). Bayesian Forecasting and Dynamic Models. Springer.

29

Wood, S. N. (2003). Thin plate regression splines. Journal of the Royal Statistical Society:

Series B (Statistical Methodology), 65(1):95–114.

Yao, F., M¨uller, H.-G., and Wang, J.-L. (2005). Functional data analysis for sparse longitu-

dinal data. Journal of the American Statistical Association, 100(470):577–590.

Appendix

Priors
The prior for {µt}T
t=1 is determined by (6). Let bψ be a Jψ-dimensional vector of cubic
B-spline basis functions with min{|To|/2, 35} = (Jψ − 4) equally-spaced interior knots. The
tensor product expansion ψ(cid:96)(τ, u) = b(cid:48)
Jψ×Jψ matrix of unknown coeﬃcients and θψ(cid:96) = vec (Θψ(cid:96)), is computationally convenient for
the FAR surfaces {ψ(cid:96)}p
process prior on ψ(cid:96), where Ωψ(cid:96) is a penalty matrix and λψ(cid:96) is a smoothing parameter. The

ψ(τ )(cid:1) θψ(cid:96), where Θψ(cid:96) is a
(cid:1) induces a Gaussian

(cid:96)=1. The Gaussian prior [θψ(cid:96)|λψ(cid:96)] ∼ N(cid:0)0, λ−1

ψ(τ )Θψ(cid:96)bψ(u) =(cid:0)b(cid:48)

ψ(u) ⊗ b(cid:48)

ψ(cid:96)

Ω−1

ψ(cid:96)

standard roughness penalty (cid:82)(cid:82)(cid:110) ∂2

ψ(cid:96)(u1, u2) + 2 ∂2

∂u1∂u2

ψ(cid:96)(u1, u2) + ∂2
∂u2

ψ(cid:96)(u1, u2)

du1 du2

(cid:111)

∂u1

can be expressed as θ(cid:48)

ψ(cid:96)

Ω2θψ(cid:96) for a known singular matrix Ω2. To obtain a proper prior,

which is necessary for our model averaging procedure, we combine the roughness penalty

with a nonstationarity penalty: a suﬃcient condition for stationarity of Yt in model (2) is

(cid:80)p

(cid:96)=1

(cid:82)(cid:82) ψ2

(cid:96) (τ, u) dτ du < 1, which can be expressed as (cid:80)p

(cid:96)=1 θ(cid:48)

ψ(cid:96)

Ω0θψ(cid:96) < 1 where Ω0 is a

known invertible matrix. We use the prior precision matrix Ωψ(cid:96) = Ω2+κ(cid:96)Ω0, which penalizes

roughness of ψ(cid:96) and provides shrinkage toward stationarity, where the trade-oﬀ is determined

by κ(cid:96). Simulations suggest that the posterior distribution is not sensitive to the choice of κ(cid:96);
we ﬁx κ(cid:96) = 1 for the simulations and assume log (κ(cid:96)) ∼ N (0, 4) for the application. For the
smoothing parameter λψ(cid:96), we use the half-Cauchy prior of Gelman (2006), which provides
excellent mixing of the states {s(cid:96)} in the model averaging procedure. The prior may be

expressed hierarchically via the auxiliary variables ˜λψ(cid:96) ∼ Gamma(cid:0) 1

(cid:1), ˜ξψ(cid:96) ∼ N (0, 106),

2, 1

2

30

(cid:16)

and ˜θψ(cid:96) ∼ N

0, ˜λ−1

ψ(cid:96)

Ω−1

ψ(cid:96)

(cid:17)

, with the identiﬁcation θψ(cid:96) = ˜ξψ(cid:96)

We use the conditionally conjugate inverse-Gamma priors σ−2

η ∼ Gamma(10−3, 10−3)
for the measurement error precision and the FDLM approximation error precision, respec-

˜θψ(cid:96) and λψ(cid:96) = ˜ξ−2
ν , σ−2

ψ(cid:96)

˜λψ(cid:96).

tively. In some cases, we may prefer smoother sample paths of µt, but the paths will not

be smooth when σ2

small value, such as σ2

η is large. If increasing J is infeasible or undesirable, ﬁxing σ2

η at some
η = 10−6, often works well, and can be interpreted as a jitter term
for computing a valid inverse of K  (Neal, 1999). Assuming the FDLM (7) for the inno-

vation covariance K , the factors are distributed et

although many generalizations are available (Kowal et al., 2016). To enforce the order-

j}J

iid∼ N (0, Σe) with Σe = diag(cid:0){σ2
(cid:3) =(cid:2)σ−2
(cid:2)σ−2
(cid:3) ∼ Uniform(cid:0)0, σ−2

(cid:1),
(cid:3). A noninformative
(cid:1) for j = 1, . . . , J − 1. The

J > 0, recall that the joint distribution (of the preci-

∼ Gamma (10−3, 10−3)

(cid:3)(cid:81)J−1

|σ−2
j+1, . . . , σ−2

j=1

j=1

J

J

J

j

1 > σ2

ing constraints σ2

sions) may be written(cid:2)σ−2
and (cid:2)σ−2

(cid:3) = (cid:2)σ−2

2 > ··· > σ2
1 , . . . , σ−2

J

joint prior that respects the constraints is fully speciﬁed by σ−2

j

J

|σ−2

|σ−2
j+1, . . . , σ−2
FLCs are φj(τ ) = b(cid:48)
φ(τ )ξj, where bφ is a low-rank thin plate spline basis with knot lo-
cations determined by the quantiles of the observation points, To, ξj ∼ N (0, Λφj ), and Λ−1
is the low-rank thin plate spline penalty matrix. We follow Wand and Ormerod (2008)

j+1

j+1

φj

j

(cid:16)

(cid:17)

in the singular value decomposition-based diagonalization of the penalty matrix, so that

Λφ = diag

108, 108, λ−1

φj

, . . . , λ−1

φj

, which places a noninformative prior on the constant and

linear components of the thin plate spline basis, which are unpenalized. The prior precision

λφj is common among the nonlinear components, and corresponds to the smoothing param-

eter for the regression function φj. Following Gelman (2006), we place uniform priors on
∼ Uniform (0, 104), which implies the prior for the precision
the standard deviations λ
1{λφj > 10−8}. The upper bound for the prior standard deviation is selected
[λφj ] ∝ λ
to match the noninformative components of Λφj . The orthonormality constraint is enforced

−1/2
φj

−3/2
φj

during sampling, which we discuss in Section A of the web supplement. We assume the same
parametrization and prior distribution for the mean function, µ(τ ) = b(cid:48)

φ(τ )θµ.

31

Proof of Theorem 1

To prove Theorem 1, we use the following well-known results:

Proposition 2. For random vectors δ and Y with known mean and covariance, the unique
best linear predictor of δ given Y is EG[δ|Y ], where EG is the expectation computed under
the assumption that (δ(cid:48), Y (cid:48))(cid:48) is jointly Gaussian.

1, . . . , y(cid:48)

1, . . . , µ(cid:48)

Proposition 3 (West and Harrison, 1997). Under a DLM such as model (6), the random
vectors y1:T = (y(cid:48)
T )(cid:48) are jointly Gaussian, conditional on
the remaining parameters. In addition, all conditionals and marginals of the joint distribution
of (y(cid:48)

T )(cid:48) and µ1:T = (µ(cid:48)

1:T )(cid:48) are Gaussian.

1:T , µ(cid:48)

Note that we could extend µ1:T to include µ, which is also a Gaussian random vector.

Following Propositions 2 and 3, the proof of Theorem 1 is straightforward:
Proof. (Theorem 1) Let Te be ﬁxed and ﬁnite such that Te ⊂ T . Given this choice of Te, we
can form the DLM (10) with the appropriately modiﬁed terms. Similarly, we can form the
1:T )(cid:48) under model (6) and conditional
Gaussian DLM (6). Proposition 3 implies that (y(cid:48)
on Θ is jointly Gaussian. Therefore, for any δ, Y ⊆ DT ∪ {µt(τ ) : τ ∈ Te, t = 1, . . . , T},
1:T )(cid:48), the distribution of [δ|Y , Θ] is Gaussian. Proposition (2)
i.e., any subvectors of (y(cid:48)
implies that ˆδ(Y |Θ) ≡ E[δ|Y , Θ], computed under the Gaussian DLM (6), is the unique
best linear predictor of [δ|Y , Θ] under the DLM (10).

1:T , µ(cid:48)

1:T , µ(cid:48)

32

Figure 1: Sample paths of t and Yt = µt + µ as a function of τ , where t is a Gaussian process with the
Mat´ern correlation function, ρ = (ρ1, 0.1), σ = 0.01, and Yt is generated using the Bimodal-Gaussian FAR(1)
kernel, t = 1, . . . , T = 50. Left to right: t(τ ), ρ1 = 2.5; t(τ ), ρ1 = 0.5; Yt(τ ), ρ1 = 2.5; Yt(τ ), ρ1 = 0.5.
Note that we do not observe Yt directly, but rather yi,t = Yt(τi,t) + νi,t, where νi,t ∼ N (0, σ2
ν) is measurement
error with σν = σ/5 = 0.002 and Tt = {τ1,t, . . . , τmt,t} are the observation points at time t.

Figure 2: M SF Ee under various designs. Top left: FAR(1), T = 350, sparse-random design with the
Linear-u kernel and smooth GP innovations. Top right: FAR(1), T = 50, sparse-random design with the
Bimodal-Gaussian kernel and non-smooth GP innovations. Bottom left: FAR(1), T = 350, sparse-ﬁxed
design with the Bimodal-Gaussian kernel and smooth GP innovations. Bottom right: FAR(2), T = 125,
sparse-ﬁxed design with Bimodal-Gaussian and Linear−τ kernels and smooth GP innovations. The proposed
methods provide superior forecasts and nearly achieve the oracle performance, despite the presence of sparsity.

33

0.00.20.40.60.81.0−0.020.000.010.02Smooth Gaussian Processtet(t)0.00.20.40.60.81.0−0.03−0.010.010.03Non−Smooth Gaussian Processtet(t)0.00.20.40.60.81.0−0.08−0.040.000.020.04Smooth Gaussian ProcesstYt(t)0.00.20.40.60.81.0−0.08−0.040.000.04Non−Smooth Gaussian ProcesstYt(t)llllllllllllFAR OracleFDLM−FAR(p)FDLM−FAR(1)GP−FAR(1)VAR−FPCFAR ClassicVAR−YMeanRW1e−042e−043e−044e−045e−04MSFEellllllllFAR OracleFDLM−FAR(p)FDLM−FAR(1)GP−FAR(1)VAR−FPCFAR ClassicVAR−YMeanRW0.00020.00060.00100.0014MSFEellllllllllllllllllFAR OracleFDLM−FAR(p)FDLM−FAR(1)GP−FAR(1)VAR−FPCFAR ClassicVAR−YMeanRW1e−042e−043e−044e−04MSFEellllllllllllllllllllFAR OracleFDLM−FAR(p)FDLM−FAR(1)FDLM−FAR(2)FDLM−FAR(3)GP−FAR(1)VAR−FPCFAR ClassicVAR−YMeanRW0.00020.00040.00060.00080.00100.0012MSFEeFigure 3: M SEψ1 for the simulations in the respective panels of Figure 2. Estimates of ψ1 are far superior
for the proposed methods. FAR(p) with model averaging is competitive with the known p model; the true
p = 1 for top left, top right, and bottom left and p = 2 for bottom right.

RW
Mean
VAR-Y
FAR Classic
VAR-FPC
Diebold-Li
GP-FAR(1)
FDLM-FAR(1)
FDLM-FAR(p)

11/2007 - 6/2008

6/2015 - 1/2016

∆ Nominal ∆ Real
0.01973
0.01198
0.01102
0.01173
0.01137
0.01190
0.01059
0.01099
0.01089

0.04909
0.03477
0.03258
0.03409
0.03245
0.03224
0.03100
0.03269
0.03100

∆ BEI ∆ Nominal ∆ Real
0.00706
0.00805
0.00359
0.00342
0.00417
0.00371
0.00350
0.00365
0.00355
0.00366
0.00366
0.00357
0.00354
0.00395
0.00353
0.00368
0.00355
0.00357

0.00580
0.00323
0.00354
0.00328
0.00330
0.00324
0.00416
0.00326
0.00317

∆ BEI
0.00399
0.00239
0.00240
0.00243
0.00238
0.00243
0.00282
0.00236
0.00236

Table 1: MSFEs for observed changes in nominal yields, real yields, and BEI during the last ﬁnancial crisis
(11/2007 - 6/2008) and more recently (6/2015 - 1/2016), grouped by multivariate methods (top), existing
functional data methods (middle) and proposed hierarchical FAR methods (bottom). The minimum MSFE
in each column is italicized. The proposed methods are highly competitive, and in all but one case (∆ Real,
6/2015 - 1/2016) the proposed methods provide the best forecasting performance. Forecasts for the proposed
methods are computed as the posterior mean of the one-step forecast distribution based on 25,000 MCMC
simulations after a burn-in of 10,000.

34

lllllllFDLM−FAR(p)FDLM−FAR(1)GP−FAR(1)FAR Classic0.00.10.20.30.40.5MSEy1lFDLM−FAR(p)FDLM−FAR(1)GP−FAR(1)FAR Classic0.51.01.52.0MSEy1llllllllllllFDLM−FAR(p)FDLM−FAR(1)GP−FAR(1)FAR Classic0.000.050.100.150.200.250.300.35MSEy1lllllFDLM−FAR(p)FDLM−FAR(1)FDLM−FAR(2)FDLM−FAR(3)GP−FAR(1)FAR Classic0.20.40.60.8MSEy1Figure 4: Changes in the nominal (left) and real (right) yield curves on July 12, 2015, with observed
data (black x), posterior means (blue), 95% pointwise and simultaneous prediction bands (light gray and
dark gray, respectively), and Nelson-Siegel estimates (red). The Nelson-Siegel estimate of the real yield
clearly overﬁts and provides unreasonable estimates at the shorter maturities: the estimated change in the
one-month real yield is -2.17%. Using the same parametrization, the proposed FAR(p) model incorporates
information from the nominal yield and provides a more stable estimate of the real yield. The posterior
distributions are estimated using 25,000 MCMC simulations after a burn-in of 10,000.

35

050100150200250300350−0.2−0.10.00.10.2D Nominal Yield, 7/12/2013Maturity (months)Yield (%)050100150200250300350−0.2−0.10.00.10.2D Real Yield, 7/12/2013Maturity (months)FAR(p) Posterior MeanNelson−Siegel,  l = 0.0609Web Supplement

A Initialization and MCMC Sampling Algorithm

A.1 Initialization

We initialize the unknown functions using splines and the remaining parameters using con-
ditional maximum likelihood estimators. We ﬁrst estimate µ as a smooth mean of {yt}T
t=1,
evaluated at Te. Next, we estimate each µt by ﬁtting a spline to yt − Z tµ for t = 1, . . . , T
using the R function smooth.spline. Since sparse observation points may lead to unstable

initializations of µt, we compute the median degrees of freedom implied by the spline ﬁts

for t = 1, . . . , T , and then recompute the splines for t = 1, . . . , T using this common degrees
ν, {θψ1, . . . , θψp}, and
of freedom parameter. Conditional on these estimates, we estimate σ2
{λψ1, . . . , λψp} using the maximum likelihood estimators, and initialize ˜θψ(cid:96) = θψ(cid:96), ˜λψ(cid:96) = 1,
and ˜ξψ(cid:96) = λ
. From these estimators, we compute the innovations t for t = 1, . . . , T .

−1/2
ψ(cid:96)

We initialize the FDLM parameters using the initialization algorithm of Kowal et al. (2016)
based on the singular value decomposition (SVD) of (1, . . . , T )(cid:48) = U eDeV (cid:48)
e. For the FLCs,
we let Φ equal the ﬁrst J columns of V e and then estimate Ξ to minimize ||Φ − BφΞ||2.
For the factors, we let (e1, . . . , eT )(cid:48) be the ﬁrst J columns of (U eDe), and then estimate
{σ2
k σ2
k

η using the conditional maximum likelihood estimators. Since (cid:80)j

j} and σ2

k/(cid:80)

k=1 σ2

estimates the proportion of variance of t explained by the ﬁrst j factors, we set J to be

the smallest number of factors that explain at least 95% of the variance of t. While more

sophisticated procedures are available for selecting J, such as DIC and marginal likelihood,

we ﬁnd that this simple approach performs well in simulations. For the yield curve applica-

tion, we select J = 3, which reﬂects the common three-factor modeling approach for yield

curves and agrees with the SVD-based procedure described above.

36

A.2 Gibbs Sampling Algorithm

We propose to sample from the joint posterior distribution using a Gibbs sampler with the

following steps:

1. FAR process, Yt:

(a) Centered FAR process, µt:

form the DLM (6) and sample (cid:2){µt}T

t=1|···(cid:3) jointly

using the state space sample of Durbin and Koopman (2002) implemented in the R

package KFAS.

(b) Mean function, µ(τ ) = b(cid:48)

φ(τ )θµ: sample [θµ|··· ] ∼ N (Aµaµ, Aµ) where

T(cid:88)

A−1
µ = Λ−1

µ + σ−2

ν

B(cid:48)

φZ(cid:48)

aµ = σ−2

ν

and Λµ = diag(cid:0)108, 108, λ−1
(cid:80)Jµ

(cid:16) 1
2(Jµ − 3), 1

Gamma

2

(cid:17)

µ , . . . , λ−1
j=3 θ2
µ,j

µ

t=1

tZ tBφ,

B(cid:48)

φZ(cid:48)

T(cid:88)
(cid:1). We sample the smoothing parameter [λµ|··· ] ∼

t(yt − Z tµt),

t=1

restricted to λµ > 10−8 (see the σ−2

j

sampler be-

low), where Jµ (= Jφ) is the dimension of θµ and θµ,j is the jth component of

θµ.

Set Yt = µt + µ or, in vector form, Y t = µt + µ.

2. Measurement error precision, σ−2

ν : sample

ν |··· ] ∼ Gamma
[σ−2

10−3 +

1
2

mt, 10−3 +

(cid:32)

T(cid:88)

t=1

(cid:33)

.

(yi,t − µ(τi,t) − µt(τi,t))2

T(cid:88)

mt(cid:88)

t=1

i=1

1
2

3. The FAR kernels, ψ1, . . . , ψp: using the Gelman (2006) prior and parametrization of
ψ(τ, u)θψ(cid:96) and Bψ = (bψ(τ1), . . . , bψ(τM ))(cid:48), we sample

˜θψ(cid:96), where ψ(cid:96)(τ, u) = b(cid:48)

θψ(cid:96) = ˜ξψ(cid:96)

37

(b) For (cid:96) = 1, . . . , p, sample

A−1
ψ [(cid:96), k] is the ((cid:96), k)th block of A−1
subvector of aψ of length J 2
ψ;

(cid:16)

(cid:32)(cid:34)

(cid:48)
= 10−6 + ˜θ
ψ

(cid:104) ˜ξψ(cid:96)|···(cid:105) ∼ N
(cid:40) T(cid:88)
(cid:34)
(cid:40) T(cid:88)
µt −(cid:88)
(cid:104)˜λψ(cid:96)|···(cid:105) ∼ Gamma(cid:0) 1

(cid:48)
= ˜θ
ψvec

ψK−1

(cid:32)

ψ/2, 1

(B(cid:48)

2 + J 2

ψQ)

B(cid:48)

k(cid:54)=(cid:96)

t=p+1

t=p+1



A−1
˜ξψ(cid:96)

a ˜ξψ(cid:96)

ψ × J 2
(cid:17)

(cid:35)
(cid:35)

⊗(cid:2)B(cid:48)
(cid:41)

µ(cid:48)
t−(cid:96)

(cid:3)(cid:33)
(cid:33)

,

A ˜ξψ(cid:96)

, A ˜ξψ(cid:96)

, where

a ˜ξψ(cid:96)

(cid:41)

µt−(cid:96)µ(cid:48)

t−(cid:96)

(B(cid:48)

ψQ)(cid:48)

ψK−1

 Bψ

˜θψ,

skG(ψk)µt−k

(B(cid:48)

ψQ)(cid:48)

Ωψ(cid:96)θψ(cid:96)/2(cid:1), and, if κ(cid:96) is unknown,

2 + θ(cid:48)

(cid:48)
(cid:48)
ψp)(cid:48) jointly from [ ˜θψ|··· ] ∼ N (Aψaψ, Aψ), where
ψ1, . . . , ˜θ
(a) ˜θψ = ( ˜θ

λψ(cid:96)Ωψ(cid:96) + s(cid:96)

(cid:104)

s(cid:96)sk

˜ξψk

˜ξψ(cid:96)

(cid:32)

(cid:104)

(B(cid:48)

ψQ)

(cid:110)(cid:80)T
(cid:110)(cid:80)T
(cid:40) T(cid:88)

ψQ)

˜ξ2
ψ(cid:96)
(B(cid:48)

t=p+1 µt−(cid:96)µ(cid:48)
t−(cid:96)
(B(cid:48)

t−k

t=p+1 µt−(cid:96)µ(cid:48)

(cid:111)

(cid:41)

(B(cid:48)

ψQ)(cid:48)(cid:105) ⊗(cid:2)B(cid:48)
(cid:111)
ψQ)(cid:48)(cid:105) ⊗(cid:2)B(cid:48)
(cid:33)

ψK−1

ψK−1

 Bψ

(cid:3)

 Bψ

(cid:3) ,

(cid:96) = k
(cid:96) (cid:54)= k

aψ[(cid:96)] = s(cid:96)

˜ξψ(cid:96)vec

B(cid:48)

ψK−1



µtµ(cid:48)

t−(cid:96)

(B(cid:48)

ψQ)(cid:48)

,

A−1

ψ [(cid:96), k] =

t=p+1

ψ of dimension J 2

ψ and aψ[(cid:96)] is the (cid:96)th

ψ(cid:96)

sample
sample κ(cid:96) using the slice sampler (Neal, 2003). Set θψ(cid:96) = ˜ξψ(cid:96)

˜θψ(cid:96) and update Ωψ(cid:96).
(c) For the model averaging procedure, sample [s(cid:96)|··· ] (in random order), i.e., set
10 > log(1/U − 1) and s(cid:96) = 0 otherwise, where U ∼ Uniform(0, 1),
T(cid:88)

is the log-posterior odds

s(cid:96) = 1 if log Opost

log Opost
10

(cid:33)(cid:48)

(cid:32)

(cid:34)

(cid:35)

log Opost

10 = −1
2

t=p+1

t−(cid:96)K−1
µ(cid:48)

 µt−(cid:96) − 2

µt −(cid:88)

k(cid:54)=(cid:96)

skG(ψk)µt−k

K−1

 µt−(cid:96)

+log Oprior

,

10

and log Oprior

10 = log P(s(cid:96) = 1|sk, k (cid:54)= (cid:96)) − log P(s(cid:96) = 0|sk, k (cid:54)= (cid:96)) is the log-prior

odds.

4. The innovation covariance, K , under the FDLM:

(a) The factors, {et}T

t=1: using the prior et

iid∼ N (0, Σe) and the conditional likelihood

38

t = µt −(cid:80)p

(cid:96)=1 G(ψ(cid:96))µt−(cid:96) = Φet + ηt, sample [et|··· ] ∼ N (Aeaet, Ae), where

e = diag(cid:0){σ−2

η + σ−2

j }J

j=1

(cid:1)

A−1
e = σ−2
aet = σ−2

η Φ(cid:48)Φ + Σ−1
η Φ(cid:48)t.

Note that Ae is time-invariant and diagonal, so we can sample {et}T
eﬃciently.

(cid:16)

(b) The factor precisions, σ−2

j

: sample [σ−2

|··· ] ∼ Gamma

J

10−3 + T

2 , 10−3 + 1

2

then, for j = J − 1, . . . , 1, set σ−2
φ (U ; sφ, rφj ), where Fφ is the distribution
function for a Gamma random variable with shape parameter sφ = (T − 1)/2

j = F −1

and rate parameter rφj = (cid:80)T

t=1 e2

j,t/2, and U ∼ Uniform(cid:0)aφj , bφj

(cid:17)

;

J,t

t=1 jointly and

t=1 e2

(cid:80)T
(cid:1) where aφj =

Fφ(0; sφ, rφj ) and bφj = Fφ(σ−2

j+1; sφ, rφj ).
(c) The approximation error precision, σ−2

η : sample

(cid:32)

η |··· ] ∼ Gamma
[σ−2

10−3 +

, 10−3 +

T M

2

(cid:33)

||t − Φet||2

T(cid:88)

t=1

1
2

where || · ||2 denotes the Euclidean distance.

(d) The factor loading curves:

for j = 1, . . . , J (in random order), sample ξj ∼

N (Aξj aξj , Aξj ), where

A−1

ξj

= Λ−1

φj

(cid:32) T(cid:88)
(cid:32)

t=1

ej,t

(cid:33)

e2
j,t

B(cid:48)

t − Bφ

(cid:33)

ξkek,t

.

φBφ,

(cid:88)

k(cid:54)=j

+ σ−2

η

T(cid:88)

t=1

aξj = σ−2

η B(cid:48)

φ

(cid:48)
(Bφξk)

To enforce the orthogonality constraint, we condition on the linear constraints
Bφξj = 0 for k (cid:54)= j; since ξj is Gaussian and ξk is conditioned upon, the
resulting distribution is Gaussian with easily computable moments, which is also

convenient for eﬃcient sampling; see Kowal et al. (2016) for more details. After sam-

39

pling from the conditional distribution, we normalize the sampled vector ξj, so that

the orthonormality constraint is enforced at every MCMC iteration. We sample the
corresponding smoothing parameters [λφj|··· ] ∼ Gamma
restricted to λφj > 10−8, where ξj,k is the kth component of ξj.
Finally, we form the covariance and precision matrices K  and K−1
, respectively, using
the sampled components. Since the orthonormality constraint Φ(cid:48)Φ = I J is enforced
at every MCMC iteration, we can compute K−1

 directly and eﬃciently using (8).

k=3 ξ2
j,k



(cid:16) 1
2 (Jφ − 3) , 1

2

(cid:80)Jφ

(cid:17)

When the sample size T or the number of evaluation points M is large (i.e., T > 10, 000

or M > 50), the Durbin and Koopman (2002) joint sampler is computationally ineﬃcient.
Instead, we may use a single-move sampler for {µt}T
t=1, in which we sample from the full
conditional distribution of each [µt|µs, s (cid:54)= t] separately for t = 1, . . . , T (in random order).
The single-move sampler is more computationally eﬃcient, but is typically less MCMC eﬃ-
cient. The FDLM provides a closed form for K−1

, which substantially reduces computation



time when M is large.

The tensor product basis for ψ(cid:96) provides a computational simpliﬁcation for jointly sam-

pling the FAR kernel basis coeﬃcients, θψ. Importantly, the dimension of the Kronecker
product for computing A−1

ψ is determined by the number of basis functions, Jψ, which is

bounded by 35 in our speciﬁcation, and may be smaller for some applications. For other bi-

variate bases, such as the thin plate spline basis, such simpliﬁcations are not readily available,

and the Kronecker product scales with the number of evaluation points, M .

In the model averaging procedure, there is a nontrivial concern about the ability of the

MCMC sampler to move between states. When s(cid:96) = 0, ψ(cid:96) does not appear in the likelihood

(9), so the Gibbs sampler will draw ψ(cid:96) from its prior. Therefore, the prior for ψ(cid:96) must be

proper; if it is nonetheless noninformative, then the draws of ψ(cid:96) from the prior distribution

may not be reasonable for (9), so the next MCMC sample of s(cid:96) will be zero with high

probability. To alleviate this problem, we ﬁx s(cid:96) = 1 for all (cid:96) during a short burn-in period,

40

so that each ψ(cid:96) is well-estimated and therefore more likely to be included in the model

if it is relevant.

In both simulations and the yield curve application, the Gelman (2006)

parametrization for ψ(cid:96) sampling discussed in the Appendix provides excellent mixing among
the states {s(cid:96)}pmax
(cid:96)=1 .

B Additional Theoretical Results

(cid:96)=1 Ψ(cid:96)(Yt−(cid:96)), where {Ψ(cid:96)}p

Ψ2B2 − ··· − ΨpBp)Yt = Yt −(cid:80)p

B.1 Proof of Proposition 1
Let Ψ(B) be a polynomial in the backshift operator B of order p, so that Ψ(B)Yt = (1−Ψ1B−
(cid:96)=1 are bounded linear operators
on L2(T ). Similarly, let Θ(B) be a polynomial in the backshift operator B of order q, where
{Θ}q
(cid:96)=1 are bounded linear operators on L2(T ). A functional autoregressive moving average
process of order (p, q), written FARMA(p, q), is deﬁned by Ψ(B)(Yt − µ) = Θ(B)t, where
{t} is a white noise process in L2(T ) and µ is the unconditional mean of Yt. The FAR(p)
model may be written compactly as Ψ(B)(Yt − µ) = t. By assumption, we observe the
process {yt}, where yt = Yt + νt and {νt} is a white noise process in L2(T ) independent of
{t}. Rewriting the observation equation yt − µ = Yt − µ + νt and applying Ψ(B), we have
Ψ(B)(yt−µ) = Ψ(B)(Yt−µ)+Ψ(B)νt = t+Ψ(B)νt. It remains to show that Zt ≡ t+Ψ(B)νt
is a functional moving average process of order p, or equivalently, FARMA(0, p). Clearly,
Xt ≡ Ψ(B)νt is FARMA(0, p). By Proposition 10.2 in Bosq and Blanke (2008), C X
(cid:54)= 0
(cid:96) (x) ≡
and C X
is the covariance operator of Xt deﬁned by C X
(cid:96) = 0 for (cid:96) > p, where C X
(cid:96)
E [(cid:104)Xt, x(cid:105)Xt+(cid:96)] for x ∈ L2(T ). Let C Z
(cid:96) denote the covariance operators for Zt and t,
respectively. Then C Z
using independence of {t} and {νt}. Since t is white noise, C 
it follows that C Z

(cid:96) (x) = E [(cid:104)Zt, x(cid:105)Zt+(cid:96)] = E [(cid:104)t + Xt, x(cid:105) (t+(cid:96) + Xt+(cid:96))] = C 

(cid:96) = 0 for (cid:96) > p. Proposition 10.2 in Bosq and Blanke (2008)

p

(cid:96) (x) + C X

(cid:96) (x),

(cid:96) = 0 for (cid:96) > 0, from which

(cid:96) and C 

p (cid:54)= 0 and C Z

implies that Zt is FARMA(0, p), so we conclude that yt is FARMA(p, p).

41

B.2 DLM Recursions and Special Cases of Theorem 1

For completeness, we provide the standard DLM recursion formulas for model (6). Let
Dt = {yt, yt−1, . . . , y1} ∪ D0 be the information available at time t, where D0 represents
the information prior to t = 1. For our purposes—in particular, for the Gibbs sampling
algorithm—we let D0 = {µ, σ2
ν, ψ, K} (denoted by Θ in Theorem 1). We may compute
full conditional posterior distributions from model (6) using standard DLM recursions (e.g.,
West and Harrison, 1997). For simplicity, let G = G(ψ). Suppose that [µt−1|Dt−1] ∼
N (mt−1, C t−1). The prior at time t is [µt|Dt−1] ∼ N (at, Rt), where at = Gmt−1 and
Rt = GC t−1G(cid:48) + K . The one-step forecast at time t is [yt|Dt−1] ∼ N (f t, Qt), where f t =
νI mt. The posterior at time t is [µt|Dt] ∼
Z tµ+Z tat = Z t(µ+Gmt−1) and Qt = Z tRtZ(cid:48)
N (mt, C t), where mt = C−1
ν Z(cid:48)
tZ t, or,
, rt = yt − f t, and C t = Rt − AtQtA(cid:48)
more commonly, mt = at + Atrt, At = RtZ(cid:48)
t.
The h-step forecast of the functional observations is E[yt+h|Dt] = E[Z t+hµ + Z t+hµt+h +
ν t+h|Dt] = Z t+hµ + Z t+hE[µt+h|Dt], where E[µt+h|Dt] = Ghmt, which is the h-step forecast
of µt.

t(yt − Z tµ)(cid:1) and C−1

(cid:0)R−1

t at + σ−2

t = R−1

t + σ−2

ν Z(cid:48)

t

t+σ2

tQ−1

t

Some special cases of Theorem 1 are proved in West and Harrison (1997):

Corollary B.2.1 (Theorem 4.10, West and Harrison, 1997). The unique best linear predictor
of the ﬁltering random variable [µt|Dt] is mt
Corollary B.2.2 (Corollary 4.7, West and Harrison, 1997). The unique best linear predictor
of the one-step forecast [µt|Dt−1] is at. The unique best linear predictor of the one-step
forecast [yt|Dt−1] is f t.

42

B.3 Proof of Theorem 2
Suppose τ∗ ∈ T such that τ∗ (cid:54)∈ Te. The full conditional distribution of µt(τ∗) is

r=1, Θ(cid:3) ×(cid:2)µt(τ∗)|{µr}T

r=1, Θ(cid:3)

(cid:2)µt(τ∗)|{µr}T

r=1, Θ,Ds

(cid:3) ∝(cid:2)y1, . . . , ys|µt(τ∗),{µr}T
r=1, Θ(cid:3) ,
∝(cid:2)µt(τ∗)|{µr}T

since the likelihood term is constant with respect to µt(τ∗): To ⊆ Te, so τ∗ (cid:54)∈ Te implies
τ∗ (cid:54)∈ To, and therefore µt(τ∗) does not appear in the likelihood of model (4). For p = 1,
the conditional Gaussian process prior for µt implied by model (4) under the approximation

(cid:1), where ψ(cid:48)(τ ) = (ψ(τ, τ1), . . . , ψ(τ, τM )), Q is a

(5) is [µt|µt−1, ψ, K] ∼ GP(cid:0)ψ(cid:48)(·)Qµt−1, K

known quadrature weight matrix, and µt−1 = (µt−1(τ1), . . . , µt−1(τM ))(cid:48) is the function µt−1
evaluated at each τ ∈ Te. Notably, τ∗ (cid:54)∈ Te implies that µt(τ∗) does not appear in the
conditional mean function for µt+1, so we may further simplify the distribution of µt(τ∗):

(cid:2)µt(τ∗)|{µr}T

r=1, Θ,Ds

(cid:3) ∝(cid:2)µt(τ∗)|µt, µt−1, Θ(cid:3) .

To compute this distribution, we use the deﬁnition of a Gaussian process, which implies the
following joint distribution of µt(τ∗) and µt, conditional on µt−1, ψ, and K:

µt(τ∗)

 ∼ N

ψ(cid:48)(τ∗)Qµt−1

 ,

 ,
K(τ∗, τ∗) K (τ∗)

µt

ΨQµt−1

K(cid:48)

(τ∗)

K 

where Ψ = {ψ(τi, τk)}M
i,k=1 and K (τ∗) = (K(τ∗, τ1), . . . , K(τ∗, τM )). Conditioning on µt in-
duces the desired distribution [µt(τ∗)|µt, µt−1, ψ, K] ∼ N (mt(τ∗), Kt(τ∗)), where mt(τ∗) =
ψ(cid:48)(τ∗)Qµt−1 + K (τ∗)K−1
(τ∗).
Under the FDLM, the following useful simpliﬁcations are available: K(τ∗, τ∗) = σ2
η +
φ(cid:48)(τ∗)Σeφ(τ∗), K (τ∗) = φ(cid:48)(τ∗)ΣeΦ(cid:48), and using (8), K−1
η Φ ˜ΣeΦ(cid:48), where

(cid:0)µt − ΨQµt−1
φ(cid:48)(τ∗) = (φ1(τ∗), . . . , φJ(τ∗)), Σe = diag(cid:0){σ2

(cid:1) and Kt(τ∗) = K(τ∗, τ∗) − K (τ∗)K−1

 = σ−2

(cid:1), Φ = (φ(τ1), . . . , φ(τM ))(cid:48), and ˜Σe =

η I M − σ−2

 K(cid:48)

j}J

j=1



43

diag(cid:0){σ2

j /(σ2

η + σ2

j )}J

(cid:1). By substitution, we derive

j=1

mt(τ∗) = ψ(cid:48)(τ∗)Qµt−1 + K (τ∗)K−1

(cid:0)µt − ΨQµt−1
(cid:1)
η Φ ˜ΣeΦ(cid:48)(cid:17)(cid:0)µt − ΨQµt−1
= ψ(cid:48)(τ∗)Qµt−1 + φ(cid:48)(τ∗)ΣeΦ(cid:48)(cid:16)
= ψ(cid:48)(τ∗)Qµt−1 + φ(cid:48)(τ∗) ˜ΣeΦ(cid:48)(cid:0)µt − ΨQµt−1
(cid:1) ,

η I M − σ−2
σ−2



(cid:1)

using the constraint Φ(cid:48)Φ = I J and the simpliﬁcation σ−2

η Σe − σ−2

η Σe ˜Σe = ˜Σe. Similarly,

Kt(τ∗) = K(τ∗, τ∗) − K (τ∗)K−1

η + φ(cid:48)(τ∗)Σeφ(τ∗) − φ(cid:48)(τ∗)ΣeΦ(cid:48)(cid:16)

 K(cid:48)

(τ∗)

= σ2

η Φ ˜ΣeΦ(cid:48)(cid:17)

η I M − σ−2
σ−2

ΦΣeφ(τ∗)

= σ2

η + σ2

ηφ(cid:48)(τ∗) ˜Σeφ(τ∗),

which is time-invariant. Extensions for p > 1 only require modiﬁcation of the mean func-

tion: mt(τ∗) = (cid:80)p

(cid:96)=1 ψ(cid:48)

(cid:96)(τ∗)Qµt−(cid:96) + φ(cid:48)(τ∗) ˜ΣeΦ(cid:48)(cid:0)µt −(cid:80)p

(ψ(cid:96)(τ, τ1), . . . , ψ(cid:96)(τ, τM )) and Ψ(cid:96) = {ψ(cid:96)(τi, τk)}M

i,k=1.

(cid:1), where ψ(cid:48)

(cid:96)=1 Ψ(cid:96)Qµt−(cid:96)

(cid:96)(τ ) =

C Additional Simulation Results

In Figure C1, we display the results from FAR(1) simulations under the dense design, while

varying both smoothness of t and the sample size, T . The functional data methods all nearly

achieve the oracle performance, and are superior to the multivariate methods. These results

conﬁrm the ﬁndings of Didericksen et al. (2012): when T is large and the observation points
are dense in T , existing functional data methods can nearly achieve the oracle performance,

even when ψ1 is estimated poorly. The proposed methods, particularly with the FDLM

(FDLM-FAR(1) and FDLM-FAR(p)), outperform existing functional data methods for

non-smooth GP innovations, and again are far superior for ψ1 estimation. The uncertainty

of p incorporated into the lag selection procedure (FDLM-FAR(p)) does not appear to

inhibit forecasting or estimation of ψ1 substantially.

44

Figure C1: M SF Ee (top) and corresponding M SEψ1 (bottom) under various designs. Left: FAR(1),
T = 50, dense design with the Bimodal-Gaussian kernel and non-smooth GP innovations. Right: FAR(1),
T = 350, dense design with the Bimodal-Gaussian kernel and smooth GP innovations. The proposed methods
provide superior forecasts and nearly achieve the oracle performance, despite the presence of sparsity.

D Additional Details for the Yield Curve Application

For the yield curve application of Section 7, we can replace the DLM sampler for {µt}
with a joint DLM sampler for {µt, βc
t = βt − βµ is the centered version of
βt = (β1,t, β2,t, β3,t)(cid:48) with unconditional mean βµ, using the DLM (for p = 1)

t}, where βc

 yN

t − µ

t − F R,tβµ
yR

µt

βc
t

 =
 =

F N,t

I mN
G(ψ) 03

0mR

t

t

F R,t

µt
 +
ν N
 ,
 t
 +
µt−1
 ,

ν R
t

βc
t

t

t

ν N
 ∼ N
 ∼ N
 t

ν R
t

0
 ,
σ2
K 
0
 ,

0

0

νN I mN

t

0M Gβ

βc

t−1

ωt

ωt

0

0 W β



σ2
νRI mR

t

0



0

where F N,t and F R,t are the mN

t × 3 dimensional matrices of Nelson-Siegel
basis functions evaluated at the nominal and real observations points, respectively, at time

t × 3 and mR

t, Gβ = diag (φβ1, φβ2, φβ3) is the diagonal matrix of AR(1) coeﬃcients for βj,t, and W β =

45

lllllllllllllllllFAR OracleFDLM−FAR(p)FDLM−FAR(1)GP−FAR(1)VAR−FPCFAR ClassicVAR−YMeanRW1e−042e−043e−044e−045e−046e−04MSFEellllllllllllllllllllllFAR OracleFDLM−FAR(p)FDLM−FAR(1)GP−FAR(1)VAR−FPCFAR ClassicVAR−YMeanRW1e−042e−043e−044e−04MSFEelllllllFDLM−FAR(p)FDLM−FAR(1)GP−FAR(1)FAR Classic0.00.51.01.52.0MSEy1llllFDLM−FAR(p)FDLM−FAR(1)GP−FAR(1)FAR Classic0.050.100.150.20MSEy1diag(cid:0)σ2

(cid:1). After jointly sampling {µt, βc

, σ2
β2

, σ2
β3

β1

parameters {βj,µ, φβj , σ2
and (φβj + 1)/2 ∼ Beta (3, 2) and the noninformative prior σ−2
priors reﬂect our belief that βj,t should be approximately mean zero and stationary, since

t + βµ. For the AR(1)
} for j = 1, 2, 3, we use the weakly informative priors βj,µ ∼ N (0, 10)
∼ Gamma (10−3, 10−3). The

βj

βj

t}, we set βt = βc

τ λN S

, and f3(τ|λN S) = 1−exp(−τ λN S )

we have diﬀerenced the yield data. For completeness, the Nelson-Siegel basis is deﬁned by
f1(τ ) = 1, f2(τ|λN S) = 1−exp(−τ λN S )
− exp(−τ λN S), where we
ﬁx λN S = 0.0609 following Diebold and Li (2006). For this application, M = 30 corresponds
to the evaluation points Te = {1, 3, 6, 12, 24, 36, 51, 60, 75, 84, 100, 120, 137, 150, 162, 174, 187,
199, 211, 224, 240, 261, 273, 286, 298, 310, 323, 335, 348, 360} months, which are approximately
t ⊂ Te. For implementation, we rescale
equally-spaced integers in [1, 360] such that T R
,T N
the observation and evaluation points such that T R
,T N

,Te ⊂ [0, 1].

τ λN S

t

t

t

We include MCMC diagnostics for the yield curve application. All diagnostics were com-

puted using the R package coda (Plummer et al., 2006). In Figure D1, we provide trace plots

for the smoothing distributions(cid:2)yN

t (τ )(cid:12)(cid:12){yN

i,s},{yR

k,s}(cid:3) and(cid:2)yR

t (τ )(cid:12)(cid:12){yN

i,s},{yR

k,s}(cid:3) of the changes

in nominal and real yield curves, respectively, on July 12, 2013, conditional on yield curve

data from January 1, 2013 through January 1, 2016. These posterior distributions corre-

spond to the posterior mean and 95% pointwise and simultaneous prediction bands in Figure

4 of the main paper. The median eﬀective sample sizes for these smoothing distributions

across all evaluation points are 15280 and 25000 for the nominal and real yields, respectively

(the respective minimum eﬀective sample sizes are 1646 and 6847). The summary statistics
for the eﬀective sample sizes of Ψ(cid:96) = {ψ(cid:96)(τi, τk)}M
(cid:96) = 1, . . . , pmax = 4 are in Table D1. The eﬀective sample sizes are suﬃciently large.

i,k=1 across all pairs of evaluation points for

E Prediction Bands

An appealing feature of the hierarchical Bayesian approach is the ease with which we can

compute simultaneous prediction bands for functions of interest. We are particularly in-

46

Figure D1: Trace plots for the smoothing distributions

(cid:104)

t (τ )(cid:12)(cid:12){yN

yN

i,s},{yR

k,s}(cid:105)

(cid:104)

(top) and

t (τ )(cid:12)(cid:12){yN

yR

i,s},{yR

k,s}(cid:105)

(bottom) of the nominal and real yield curves, respectively, on July 12, 2013, conditional on yield curve data
from January 1, 2013 through January 1, 2016 and evaluated at the 2nd, 25th, 50th, and 75th quantiles of
the evaluation points (maturities of 1, 60, 162, and 261 months, respectively). These posterior distributions
correspond to the estimated curves in Figure 4 of the main paper.

terested in the h-step forecast distributions [µT +h|y1:T ] and [yT +h|y1:T ] and the smoothing
distribution [µt|y1:T ] (e.g., Figure 4). In each case, we only require a sample from the relevant
posterior distribution, which can be obtained via the Gibbs sampling algorithm.

Let g = (g(τ1), . . . , g(τM ))(cid:48) denote the function of interest, and suppose that we have
samples g(1), . . . , g(N ) from the full conditional distribution of g. Using the procedure of

Krivobokova et al. (2010), which is also implemented in Kaufman et al. (2010), we can
compute simultaneous 100(1−α)% prediction bands by ﬁrst computing 100(1−α)% pointwise

intervals, ((cid:96)g(τi), ug(τi)), of g(τi) for i = 1, . . . , M , and then ﬁnding the smallest δg > 0 such
that ˆP{g(τi) ∈ ((cid:96)g(τi) − δg, ug(τi) + δg), i = 1, . . . , M} ≥ 1 − α, where ˆP is the empirical
probability distribution, or sample proportion, based on the MCMC samples g(1), . . . , g(N ).

Importantly, the resulting bands need not be symmetric, but can be made symmetric if

the pointwise intervals are chosen to be symmetric. The pointwise intervals reﬂect the

47

050001500025000−0.040.000.020.04Nominal Yield, 2nd quantileIterations050001500025000−0.040.000.020.04Nominal Yield, 25th quantileIterations050001500025000−0.06−0.020.02Nominal Yield, 50th quantileIterations050001500025000−0.040.000.02Nominal Yield, 75th quantileIterations050001500025000−0.100.000.10Real Yield, 2nd quantileIterations050001500025000−0.100.000.10Real Yield, 25th quantileIterations050001500025000−0.100.000.10Real Yield, 50th quantileIterations050001500025000−0.100.000.050.10Real Yield, 75th quantileIterationsEﬀective Sample Sizes of the FAR kernels

1216
6050
11600
9844

1509
7533
12110
11320

4316
7739
14130
13710

3rd Qu. Max.
6159
9289
17300
19860
Table D1: Summary statistics for the eﬀective sample sizes of Ψ(cid:96) = {ψ(cid:96)(τi, τk)}M
evaluation points for each (cid:96) = 1, . . . , pmax = 4.

1st Qu. Median Mean
2707
7015
12930
11940

Min.
938
4459
11120
7358

Ψ1
Ψ2
Ψ3
Ψ4

i,k=1 across all pairs of

variability in g(τi) for each τi, i = 1, . . . , M , using the information in the MCMC sample,

which is propagated to the simultaneous bands via the uniform expansion of the intervals

by δg. In Figure 4, we use Highest Posterior Density (HPD) pointwise intervals computed

using the R package coda (Plummer et al., 2006).

48

