6
1
0
2

 
r
a

M
6

 

 
 
]

.

R
P
h
t
a
m

[
 
 

1
v
9
9
7
1
0

.

3
0
6
1
:
v
i
X
r
a

Noise Stability and Correlation with Half Spaces

Elchanan Mossel ∗

Joe Neeman †

March 8, 2016

Abstract

Benjamini, Kalai and Schramm showed that a monotone function
f ∶ {−1, 1}n → {−1, 1} is noise stable if and only if it is correlated with
a half-space (a set of the form {x ∶ ⟨x, a⟩ ≤ b}).

We study noise stability in terms of correlation with half-spaces for
general (not necessarily monotone) functions. We show that a function
f ∶ {−1, 1}n → {−1, 1} is noise stable if and only if it becomes correlated
with a half-space when we modify f by randomly restricting a constant
fraction of its coordinates.

Looking at random restrictions is necessary: we construct noise
stable functions whose correlation with any half-space is o(1). The
examples further satisfy that diﬀerent restrictions are correlated with
diﬀerent half-spaces:
for any ﬁxed half-space, the probability that a
random restriction is correlated with it goes to zero.

We also provide quantitative versions of the above statements, and
versions that apply for the Gaussian measure on Rn instead of the
discrete cube. Our work is motivated by questions in learning theory
and a recent question of Khot and Moshkovitz.

∗University

of

California,

Berkeley

and University

of

Pennsylvania;

mossel@wharton.upenn.edu

†UT Austin and University of Bonn; joeneeman@gmail.com

1

1

Introduction

In a seminal paper, Benjamini, Kalai and Schramm [2] related noise stability
to correlation with half-spaces by showing that a monotone boolean function
is noise stable if and only if it is correlated with a half-space. Our interest
in this paper is relating noise stability with correlation with half-spaces for
general boolean functions. Our results are motivated by recent work of Khot
and Moshkovitz whose goal is to construct a Lasserre integrality gap for the
Unique Games problems as well as by natural problems in learning theory.
In the following subsections we introduce the setup and results in the

boolean and Gaussian cases and discuss the motivation for our work.

1.1 The boolean setting

Let µn denote the uniform measure on {−1, 1}n. For t ≥ 0, let Pt denote the
Bonami-Beckner semigroup, deﬁned by

(Ptf )(x) = (Pt,1f )(x) = Ef + e−t(f (x) − Ef )

in the case n = 1 and Pt,n = P ⊗n
set A ⊂ {−1, 1}n is

t,1 otherwise. The boolean noise stability of a

NSt(A) = E[1APt1A],

usual boolean noise sensitivity.

where the expectation is taken with respect to µn. Since Pt = Pt~2Pt~2 and
Pt is self-adjoint, we may also write NSt(A) = E[(Pt~21A)2]. Then
NSt(A) − µn(A)2 = NSt(A) −(EPt~21A)2 = Var(Pt~21A) ≥ 0;
the quantity Var(Pt1A) turns out to be a useful re-parametrization of the
We say that a sequence Ai ∶{−1, 1}ni of sets is noise sensitive if for every
t > 0, Var(Pt1Ai) → 0 is i → ∞. Otherwise, we say that the sequence Ai is
A half-space is a set of the form{x ∈{−1, 1}n ∶⟨x, a⟩ ≤ b}; write Hn for
the set of all half-spaces in{−1, 1}n. Deﬁne
Clearly 0 ≤ M(A) ≤ 1
The set A ⊂{−1, 1}n is monotone if whenever x ∈ A and y ≥ x coor-

dinatewise then y ∈ A. Benjamini, Kalai, and Schramm [2] proved that a

Cov(1A, 1B).

M(A) = sup

A∈Hn

4 for all A.

noise stable.

2

noise-stable sets are not necessarily correlated with any half-spaces.

Although noise-stable sets may not be correlated with half-spaces, there
is a characterization of noise stability in terms of half-spaces; this character-

this article, we explore removing the condition of monotonicity. First, we
show that one direction of Benjamini et al.’s equivalence fails when the Ai
are allowed to be non-monotone. In particular, we construct a sequence of

sequence Ai of monotone sets is noise sensitive if and only if M(Ai) → 0. In
sets Bi ⊂{−1, 1}ni such that M(Bi) → 0 but NSt(Bi) ~→ 0; in other words,
ization requires the notion of a restriction. For z ∈{−1, 0, 1} and y ∈{−1, 1},
deﬁne z ⊘ y ∈{−1, 1} by
For z ∈{−1, 0, 1}n and y ∈{−1, 1}n, deﬁne z ⊘ y ∈{−1, 1}n coordinatewise:
(z ⊘ y)i = zi ⊘ yi. For a set B ⊂{−1, 1}n, deﬁne a restriction of B by
Write µt for the measure on{−1, 0, 1}n under which each coordinate is in-
{−1, 1} otherwise.

z ⊘ y =⎧⎪⎪⎨⎪⎪⎩
Bz ={x ∈{−1, 1}n ∶ z ⊘ x ∈ B}.

Our main theorem, in its qualitative form (its analogous quantitative
versions are Theorem 3.1 and Theorem 3.12), says that a set is noise stable
if and only if we can make it correlated with a half-space by randomly
restricting a constant fraction of its coordinates.

dependent, equal to zero with probability e−t, and chosen uniformly from

if z = 0
otherwise.

y

z

probability at least ǫ, where Z ∼ µt.

Since the notion of taking restrictions may seem artiﬁcial, it is natural
to ask whether taking restrictions in Theorem 1.1 is really necessary. That
fact, this is not the case. As an example, take nm = n2 and consider the sets

Theorem 1.1. The sequence B(i) ⊂{−1, 1}ni is noise stable if and only if
there are some t, ǫ > 0 such that for all suﬃciently large i, M(B(i)Z ) ≥ ǫ with
is, could it be that B(i) noise stable already implies that M(B(i)) ~→ 0? In
B(m) ⊂{−1, 1}nm deﬁned by
B(m) =x ∶
Proposition 1.2. The sets B(m) are noise stable, but M(B(m)) ≤ Cm−1~200

mQ
i=1 1√m

for a universal constant C.

≤ m.

xj2

j=(i−1)m+1

imQ

3

1.2 The Gaussian setting

The preceding results also make sense in a Gaussian setting: Let γn de-
note the standard Gaussian measure on Rn and write Pt for the Ornstein-
Uhlenbeck semigroup, deﬁned by

(Here and elsewhere we will reuse symbols that we also used in the boolean
setting; however, the meaning should always be clear from the context.) The
Gaussian noise stability of a set A ⊂ Rn is

X ∼ γn.

Ptf(x) = Ef(e−tx +√1 − e−2tX),
NSt(A) = E[1APt1A].

we say that Ai is noise stable otherwise. A half-space is a set of the form

As in the boolean case, we have NSt(A)− γn(A)2 = Var(Pt~21A); we say that
a sequence Ai of sets is noise sensitive if Var(Pt1Ai) → 0 for all t > 0, and
{x ∈ Rn ∶⟨x, a⟩ ≤ b}; write Hn for the set of all half-spaces in Rn and deﬁne

In the setting above, we prove that a sequence of sets is noise stable if
and only if by scaling and randomly shifting it, we make them correlated
with half-spaces. Speciﬁcally, given B ⊂ Rn, t ≥ 0, and y ∈ Rn, deﬁne

A∈Hn

M(A) = sup
Cov(1A, 1B).
Bt,y ={x ∈ Rn ∶√1 − e−2tc + e−ty ∈ B}.

Theorem 1.3. The sequence B(i) ⊂ Rn is noise stable if and only if there
probability at least ǫ, where Y ∼ γn.

As in the boolean case, one can ﬁnd examples showing that Theorem 1.3
would be false if we didn’t introduce the scaling and random shifting. In
this case, the example is very easy: let B(n) ⊂ Rn be the Euclidean ball of

are some t, ǫ > 0 such that for all suﬃciently large i, M(B(i)t,Y) ≥ ǫ with
radius√n.
Proposition 1.4. The sets B(n) are noise stable, but M(B(n)) ≤ n−1~2.

One can learn a little more from this example. First, note that any
restrictions of B(n) are also Euclidean balls. In the Gaussian setting, there-
fore, unlike in the boolean one, noise stability does not imply that random
restrictions are correlated with half-spaces. Another observation (since B(n)
is rotationally invariant) is that noise stable sets do not necessarily “encode”
directions. We make this more precise in Proposition 2.4, which says that
even though random shifts and scalings of B(n) are correlated with half-
spaces, the directions in which those half-spaces point are unpredictable.

4

for all x and for the standard basis vectors ei; they asked whether the

f(−x) = f(x + ei) = −f(x),

f ∶ Rn →{−1, +1} which satisfy
most stable functions in this family are of the form sgn(∑i σixi) where
σi ∈{−1, 1}n, and also whether every function that is almost as noise
paper. However, since our functions are not required to satisfy f(x +
ei) = −f(x), our results and examples do not have direct implications

In this context, it is natural to ask whether every noise stable function
is correlated with a half-space. This is the question we address in this

stable as possible must be correlated with a function of this form.

1.3 Motivation

Our work is motivated by extending the results of [2] to non-monotone
functions, as well by the following motivations:

• In a recent work Khot and Moshkovitz [4], proposed a Lasserre inte-
grality gap for the Unique Games problem. The proposed construction
is based on the assumption that in a certain family of functions, the
most stable functions are half-spaces. More speciﬁcally [4] considers

for the proposed Lasserre integrality gap instances.

• It is well known that the class of functions having a constant frac-
tion (resp. most) of their Fourier mass on “low” coeﬃcients can be
weakly (resp. strongly) learned under the uniform distribution [5, 7].
In particular, noise stable functions can be weakly learned. On the
other hand, the most classical learning algorithms involve learning
half-spaces. Thus it is natural to ask if there is more direct relation
between the weak learnability of noise stable functions and the learn-
ability of half-spaces. Our examples seem to provide a negative answer
to this question.

2 The Gaussian case
For this section, let X ∼ γn. Recall that the Ornstein-Uhlenbeck semi-group
is deﬁned by

Ptf(x) = Ef(e−tx +√1 − e−2tX).

For t ∈ R and y ∈ Rn, deﬁne ft,y by ft,y(x) = f(√1 − e−2tx + e−ty).

5

Theorem 2.1. For any measurable f ∶ Rn →[0, 1] and any t > 0,

where c > 0 is a universal constant and Y ∼ γn.

EM(ft,Y) ≥ c(e2t − 1) Var(Ptf),

2.1 An example

It is natural to ask whether one needs to replace f by ft,y in order to ﬁnd a
correlated half-space. Indeed, a simple example shows that f itself may not
be correlated with a half-space: let Bn ⊂ Rn be the Euclidean ball of radius

√n. First, we note that for suﬃciently small t, Var(Pt1Bn) is bounded away

from zero as n → ∞. (This is already well-known [3], since Bn is obtained by
thresholding a quadratic function, but the computation in our special case
is quite easy.)
Proposition 2.2. For any n and any t > 0,

Var(Pt1Bn) ≥ 1

4

In particular Bn is noise stable.

−

arccos(e−2t)
√2π

− on(1).

Proof. For a set of B of smooth boundary, we may deﬁne the Gaussian
perimeter of B as

.

6

S∂B

dγn

dλ(x) dHn−1(x),

the Gaussian perimeter of Bn is

dλ
denotes the Gaussian density with respect to the Lebesgue measure. Since

where Hn−1 denotes the (n − 1)-dimensional Hausdorﬀ measure and dγn
the Gaussian density restricted to ∂Bn takes the constant value(2πe)−n~2
and the Euclidean surface area of Bn is√n
⋅ 2πn~2~Γ(n~2), it follows that
(2e)n~2Γ(n~2) ∼ 1√π
E[1B(1B − Pt1B)] ≤ arccos(e−t)P
√2π

On the other hand, Ledoux [6] proved that if P is the Gaussian perimeter

where the approximation follows from Stirling’s formula.

2nn~2−1~2

of B then

n−1

,

Plugging in our asymptotics for the Gaussian perimeter of Bn, we have

Since Pt = Pt~2Pt~2 and Pt~2 is self-adjoint, this may be rearranged into

.

− Pt1Bn)] ≤(1 + on(1)) arccos(e−t)
E[1Bn(1Bn
√2π
E[(Pt~21Bn)2] ≥ Pr(Bn) −(1 + on(1)) arccos(e−t)
√2π
+ on(1), this proves the claim.

.

Next, we observe that Bn is not correlated with any half-space:

2

Since Pr(Bn) = 1
Proposition 2.3. M(Bn) ≤ n−1~2.

In particular, Propositions 2.2 and 2.3 together imply that Theorem 2.1

would no longer be true if ft,y were replaced by f .

2 ≥ nQ
i=1

Proof. Since Bn is rotationally invariant, it suﬃces to consider half-spaces

Now let fi = 1Ai
Hence,

− Φ(b))].
E[1Bn fi]2 = nE[1Bn f1]2,

of the form Ai ∶={x ∶ xi ≤ b}. Since Pr(Ai) = Φ(b),
Cov(1Bn , 1Ai) = E[1Bn(1Ai
− Φ(b). Then the fi are orthogonal and satisfyfi2 ≤ 1.
1 ≥1Bn2
and so E[1Bn f1] ≤ n−1~2.
Proposition 2.4. Let g = 1Bn and let gt,y(x) = g(√1 − e−2tx + e−ty). For
bility at least 1 − u−2 over Y ∼ N(0, In)

A very similar argument shows that even though shifts of An may be
correlated with half-spaces, the half-spaces are pointed in unpredictable di-
rections.

In particular, Chebyshev’s inequality implies that for any u > 0, with proba-

any half-space A,

.

n

EY[Cov(gt,Y , 1A)2] ≤ 1
  Cov(gt,Y , 1A)  ≤ u

n

.

7

Proof. Let Ai ={x ∶ xi ≤ b} and fi = 1Ai

previous proposition, for any Y and t,

− Φ(b). As in the proof of the

1 ≥ nQ
i=1

E[gt,Y fi]2 = nE[gt,Y f1]2 = n Cov(gt,Y , f1)2.

Taking the expectation over Y completes the proof.

2.2 Proof of Theorem 2.1

is non-negligible then there exists a half-space correlated with f . Then, we

For f ∈ L2(γn), deﬁne w1(f) = ∑i
E[Xif(X)]2. Using the integration by
parts formula E[Xif(X)] = E[ ∂f
∂xi(X)], we may also write w1(f) = E∇f 2.
The proof of Theorem 2.1 goes in two steps: ﬁrst, we show that if w1(f)
show that for a random Y ∼ γn, w1(ft,y) is non-negligible in expectation.
Proposition 2.5. Take f ∶ Rn →[0, 1]. If w1(f) = ǫ2 and Var(f) = σ2 then

there exists a half-space A ⊂ Rn with

Before proving Proposition 2.5, we will show that it suﬃces to ﬁnd a

Cov(f, 1A) ≥ ǫ2

8πσ

.

half-space correlated with Ptf :

Proof. Since Pt is self-adjoint, we have

Lemma 2.6. If there exists a half-space A with Cov(Ptf, 1A) ≥ δ then there
exists a half-space A′ with Cov(f, 1A′) ≥ δ.
Assuming that A ∈{x1 ≤ b}, we can write
In other words, if we set Ay = A −√e2t − 1y then we may write Pt1A as an

Cov(Ptf, 1A) = Cov(f, Pt1A) = E[(f − Ef)Pt1A].
1A(e−tx +√1 − e−2ty) dγn(y)
(Pt1A)(x) = SRn
= SRn

average of other half-spaces: Pt1A = EY 1AY . Hence,

dγn(y).

√e2t−1y}

1
{x∈A−

E[(f − Ef)Pt1A] = E[(f(X) − Ef)1AY(X)

where X and Y are independent standard Gaussian vectors. Then there
exists some y ∈ Rn with

Cov(f, 1Ay) ≥ E[(f(X) − Ef)1AY(X)] = Cov(Ptf, 1B).

8

2], we have

= e−t
≥ e−t
≥ e−t

−

e−2tσ.

ǫ

= ǫ2

8πσ

.

and f2 is orthogonal to both f1 and 1. We may assume by rotational in-

Proof of Proposition 2.5. Write f = Ef + f1 + f2 where f1 ∈ span{x1, . . . , xn}
variance that f1(x) = ǫx1. Let A = {x1 ≥ 0}. Since Ptf1 = e−tf1 and
E[(Ptf2)2] ≤ e−4tE[f 2

E[Ptf 1A] − Ef E1A = E[1APtf1] + E[1APtf2]
ǫ√2π
+ E[1APtf2]
− e−2t1A2f22
ǫ√2π
1√2
ǫ√2π
Now take t so that 2√πe−tσ = ǫ. Then
E[Ptf 1A] − Ef E1A ≥ e−t
2√2π
By Lemma 2.6, there exists some half-space A′ with Cov(f, 1A′) ≥ ǫ2
f is noise stable then it has some shifts ft,y with non-negligible w1(ft,y). In
states that Var(f) ≤ E ∇f 2 for any f with continuous derivatives.
Proposition 2.7. For any f and any t > 0, if Y ∼ N(0, In) then
Ew1(ft,Y) ≥(e2t − 1) Var(Ptf).
Proof. Since smooth functions are dense in L2(γn), and since both w1(f)
and Var(Ptf) are preserved under L2(γn) convergence, we may assume that
f is smooth. Then ∇ft,y =√1 − e−2t(∇f)t,y. Hence,

order to do this, recall the Gaussian Poincar´e inequality (see, e.g. [1]), which

The second step in the proof of Theorem 2.1 is to show that if a function

w1(ft,y) = E∇ft,y 2 =(1 − e−2t) E∇f(√1 − e−2tX + e−ty) 2.
Ew1(ft,Y) =(1 − e−2t)EY EX ∇f(√1 − e−2tX + e−tY) 2

Now set Y to be a standard Gaussian vector in Rn, independent of X. Then

8πσ .

=(1 − e−2t)EY (Pt∇f)(Y) 2
=(e2t − 1)EY (∇Ptf)(Y) 2,

where the last line follows because Pt∇f = et∇Ptf . Finally, the Poincar´e
inequality applied to Ptf yields

Ew1(ft,Y) =(e2t − 1)E ∇Ptf 2 ≥(e2t − 1) Var(Ptf).

9

2.3 The converse of Theorem 2.1

Proof of Theorem 2.1. By Proposition 2.7, there exists some y ∈ Rn such

The following result is a (qualitative) converse of Theorem 2.1. For exam-

variance at most 1. By Proposition 2.5, there exists a half-space A with

that w1(ft,y) ≥ Var(Ptf). Now, ft,y takes values in[0, 1] and hence it has
Cov(ft,y, 1A) ≥ c(e2t − 1) Var(Ptf).
ple, it implies that if M(fs,Y) is non-negligible with constant probability
Theorem 2.8. For any 0 < r < s and any f ∶ Rn →[0, 1],
1 − e−2s1~4
(1 − e−2(s−r)) Var(Prf) ≥ 4EY M 2(fs,Y) − C 1 − e−2r

then f is noise stable. In particular, together with Theorem 2.1 it implies
Theorem 1.3.

Lemma 2.9. For any half-space A and any t > 0,

.

π

arccos(e−t).
E[(1A − Pt1A)2] ≤ 1
E[(1 − 1A)Pt1A] ≤ arccos(e−t)
arccos(e−t)
E[1APt1A] ≥ γn(A) −

2π

2π

.

.

(1)

Proof. Ledoux’s bound gives

Rearranging this,

On the other hand,

E[(1A − Pt1A)2] = γn(A) − 2E[1APt1A] + E[(Pt1A)2] ≤ 2γn(A) − 2E[1APt1A].

Applying (1) completes the proof.

Next, we show that any set which is correlated with a half-space must

be noise stable (indeed, almost as noise stable as the half-space itself).
Proposition 2.10. Suppose that A ⊂ Rn is a half-space. Then for any

f ∶ Rn →[0, 1] and any t > 0,
γn(A)(1 − γn(A))2 Var(Pt1A) −arccos(e−2t)
Cov(A, f)2
√π
≥ 4 Cov(A, f)2

Var(Ptf) ≥

for a universal constant C.

− Ct1~4

10

(2)

Since P2tg − g = P2t1A − 1A, Lemma 2.9 implies that

Var(Ptf) = E[(Pth)2] = E[c2(Ptg)2

Proof. Let g = 1A − γn(A) and h = f − Ef , so that g and h both have mean
zero and E[gh] = Cov(1A, f). Write h = cg + h⊥, where E[gh⊥] = 0; then
c = E[gh]~E[g2] = Cov(1A, f)~ Var(1A). Since Ptf − Ef = Pth, we have
+(Pth⊥)2
+ 2cPtgPth⊥].
Now, E[(Ptg)2] = Var(Pt1A) and E[(Pth⊥)2] ≥ 0. For the last term, since
E[gh⊥] = 0, the Cauchy-Schwarz inequality implies
E[PtgPth⊥] = E[h⊥P2tg] = E[h⊥(P2tg − g)] ≥ −E[(h⊥)2]E[(P2tg − g)2].
E[PthPtg] ≥ −E[(h⊥)2] arccos(e−2t)
Going back to (2) and using the bound E[(h⊥)2] ≤ E[h2] ≤ 1,
Var(Ptf) ≥ c2 Var(Pt1A) −arccos(e−2t)
√π
Recalling that c = Cov(A, f)~ Var(1A), this proves the ﬁrst claimed inequal-

For the second inequality, note that Lemma 2.9 implies that

√π

ity.

.

.

Combining this with the ﬁrst claimed inequality,

Var(Pt1A)
Var(A) ≥ 1 −
Var(Ptf) ≥ Cov(A, f)2
Var(A) 1 −
≥ 4 Cov(A, f)2

arccos(e−2t)
2π Var(A) .
C arccos(e−2t)
Var(A)
Cov(A, f)2
Var(A)

− C

 − Ct1~4
arccos(e−2t) − Ct1~4.

Finally, Cov(A, f)2 ≤ Var(A) and arccos(e−2t) ≤ Ct1~4, thus proving the

second inequality.

In order to relate the noise stability of f to half-spaces correlated with

ft,y, note that

EY E[fs,Y P2tfs,Y] = E[f P2rf]

11

Now, the Poincar´e inequality implies that Var(Psf) ≤ e−2(s−r) Var(Prf);

hence,

By Proposition 2.10 applied to fs,Y ,

when e−2r = e−2s + e−2t − e−2s−2t. Hence,

Var(Prf) = EY Var(Ptfs,Y) + Var(Psf).
(1 − e−2(s−r)) Var(Prf) ≥ EY Var(Ptfs,Y).
(1 − e−2(s−r)) Var(Prf) ≥ 4EY M 2(fs,Y) − Ct1~4.
1−e−2s . For small t, this gives t = Θ( 1−e−2r

To prove Theorem 2.8, note that if we ﬁx r and s and solve for t the we
obtain e−2t = 1 − 1−e−2r
t the Theorem is vacuous anyway).

1−e−2s) (while for large

3 Boolean functions

For this section, Pt denotes the Bonami-Beckner semigroup deﬁned in Sec-

tion 1.1. Recall also the deﬁnition of fz for z ∈{−1, 0, 1}n from that section.
2(1 − e−t)(δ1 + δ−1) on{−1, 0, 1}

Let µt be the probability distribution e−tδ0 + 1
and take Zt ∼ µ⊗n
Zt:

. Then we have the following relationship between Pt and

t

(Ptf)(x) = EfZt(x).

Theorem 3.1. For any f ∶{−1, 1}n →[0, 1] and any t > 0,
where s = − log(1 − e−t), Zs ∼ µs and c > 0 is a universal constant.

EM(fZs) ≥ c(e2t − 1) Var(Ptf),

Before proceeding with the proof of Theorem 3.1, let us make some
remarks about how sharp it is. First of all, it is no longer true if we replace
fZt by f ; that is, noise stable functions are not necessarily correlated with
half-spaces. We demonstrate this using a boolean version of the earlier
Gaussian example; details are in Section 3.2.

Next, Theorem 3.1 has a qualitative converse, which we will state later as

stable. In particular, Theorem 3.1 and Theorem 3.12 imply Theorem 1.1.

Theorem 3.12. That is, if M(fZs) is non-negligible on average then f is noise
Finally, Theorem 3.1 implies that M(fZt) ≥ c′(e2t − 1) Var(Ptf) with

constant probability over Zt.
cannot be substantially improved. As an example, consider the function

It turns out that this probability estimate

f(x) =⎧⎪⎪⎨⎪⎪⎩

x2

∏n
i=3 xi

12

if x1 = 1
if x1 = −1.

Then f is noise-stable, but if z1 = −1 then fz is noise sensitive and uncorre-
2 e−t of failing
lated with any half-space. In other words, fZt has probability 1
to be correlated with any half-space.

3.1 Proof of Theorem 3.1

xi.

The proof of Theorem 3.1 follows the same lines as the proof of Theorem 2.1,
but it requires a little background on Fourier analysis of boolean functions:

χS(x) = M

i∈S

S⊂{−1,1}n

f(x) = Q
w1(f) = nQ

for a set S ⊂{1, . . . , n}, deﬁne χS ∶{−1, 1}n →{−1, 1} by
It is well-known (see e.g. [9]) that {χS ∶ S ⊂ {1, . . . , n}} is an orthonor-
mal basis of L2({−1, 1}n); in particular, every f ∶{−1, 1}n →[0, 1] may be
expanded in this basis: deﬁne ˆf(S) as the coeﬃcients of this expansion:
ˆf(S)χS(x).
Also, we abbreviate ˆf({i}) by ˆf(i), and we deﬁne
ˆf(i)2.
We will show that if w1(f) is non-negligible then there is a half-space cor-
related with f . Then we will show that Ew1(fZt) is non-negligible.
Proposition 3.2. If w1(f) = ǫ2 and Var(f) = σ2 then there exists a half-
space B with Cov(f, 1B) ≥ c ǫ2
Lemma 3.3. If there exists a half-space B with Cov(Ptf, 1B) ≥ δ then there
exists a half-space Cov(f, 1B ′) ≥ δ.
i=1 aixi ≤ b}. Take X and Y to be inde-
Proof. Suppose that B ={x ∶ ∑n
pendent, uniform random variables in{−1, 1}n and let I ⊂{1, . . . , n} be the
If B(I, y) denotes the set{x ∶ ∑i∈I aixi ≤ b − ∑i~∈I aiyi} then
aiYi ≤ b

The proof of Proposition 3.2 require two preparatory lemmas. First, we
observe that it suﬃces to ﬁnd a half-space which is correlated with Ptf for
some t > 0:

random set that includes each element independently with probability e−t.

σ , where c > 0 is a universal constant.

i=1

Pt1B(x) = PrQ

i∈I

aixi + Q
i~∈I

= E1B(I,Y )(x).

13

Since Pt is self-adjoint,

Cov(Ptf, 1B) = E[(f − Ef)Pt1B] = E[(f(X) − Ef)1B(I,Y )(X)].

If the right hand side is larger than δ then in particular there exist I and y
such that

Cov(f, 1B(I,y)) = E[(f − Ef)1B(I,y)] ≥ δ.

Next, we consider the case of linear functions. Up to constant factors,
the best possible correlation between a linear function and a half-space is
determined by the L2 norm of the function’s coeﬃcients. This is the ﬁrst
point where the boolean proof diverges from the Gaussian proof: the Gaus-
sian case of Lemma 3.4 is trivial (with a better constant) because of the
Gaussian measure’s rotational invariance.

Lemma 3.4. If ℓ(x) = ∑ aixi and B ={x ∶ ℓ(x) ≥ 0} then

E[ℓ(X)1B(X)] ≥a2~40.

Proof. Since ℓ has mean zero,

Now, for any M ≥ 0

E ℓ(X) .

2

2

E[ℓ(X)(21B(X) − 1)] = 1

E[ℓ(X)1B(X)] = 1
E ℓ(X)  ≥ E[ ℓ(X) 1{ ℓ(X) ≤M}]
E[ℓ2(X)1{ ℓ(x) ≤M}]
ME[ℓ2(X)] − E[ℓ2(X)1{ ℓ(X) >M}] .

≥ 1
= 1

M

Hoeﬀding’s inequality implies that Pr( ℓ(X)  > ta2) ≤ 2e−t2~2; hence,
Pr(ℓ2(X) ≥ s) ds
E[ℓ2(X)1{ ℓ(X) >M}] = M 2 Pr(ℓ2(X) ≥ M 2) + S ∞
2) + 2S ∞
≤ 2M 2e−M 2~(2a2
= 4M 2e−M 2~(2a2
2).

M 2
e−s~(2a2

2) ds

M 2

Setting M = 10a2, we have

E[ℓ2(X)1{ ℓ(X) >M}] ≤ 400a2

2e−50 ≤ 1

2.

2a2

14

(3)

2; going back to (3), we have

On the other hand, E[ℓ2(X)] =a2
10a2a2
Proof of Proposition 3.2. Write f = Ef +f1 +f2 where f1(x) = ∑i xi ˆf(i), and
f2 is orthogonal to both f1 and 1. Note that Ptf1 = e−tf1, while E(Ptf2)2 ≤

2 =a2
2a2

E ℓ(X)  ≥

2 . Hence,

e−4tEf 2

2 −

20

1

1

.

E[Ptf 1B] − Ef E1B = E[1BPtf1] + E[1BPtf2]
≥ e−tE[1Bf1] − e−2t1B2f22
≥ e−tE[1Bf1] − e−2tσ.

Now, Lemma 3.4 implies that there exists a half-space B with E[1Bf1] ≥
ǫ~40. For this B,
If we take t to solve e−t = ǫ~(80σ) then

E[Ptf 1B] − Ef E1B ≥ e−t
E[Ptf 1B] − Ef E1B ≥ c

for a universal constant c > 0. By Lemma 3.3, there exists some half-space

ǫ − e−2tσ.

ǫ2
σ

40

σ .

Proposition 3.5. For any t > 0, if e−s = 1 − e−t then

B′ with Cov(f, 1B ′) ≥ c ǫ2
Next, we show that E[w1(fZ)] is substantial if f is noise-stable.
E[w1(fZs)] ≥(1 − e−t)Q
S  S  ˆf 2(S)e−2t( S −1) ≥(e2t − et) Var(Ptf).
E[w1(fZ)] = nQ

Proof. Fix t and set Z = Zs. Recalling the deﬁnition of w1, we have

Z(i))].
E[ ˆf 2

i=1

Note that ˆfZ(i) = 0 if Zi = ±1, which happens with probability 1 − e−t.
Otherwise ˆfZ(i) is given by

(4)

Zj.

ˆfZ(i) = Q

S∶i∈S

ˆf(S) M

j∈S∖{i}

15

Therefore

ˆf(S) ˆf(T)E[ M

j∈S∖{i}

Zj M
k∈T∖{i}

Zk]

S,T∶i∈S,i∈T

E[ ˆfZ(i)2] =(1 − e−t) Q
=(1 − e−t) Q
Var(Ptf) = Q

S∶i∈S

 S ≥1

ˆf 2(S)e−2t( S −1).
e−2t S  ˆf 2(S) ≤ Q

S  S e−2t S  ˆf 2(S).

Summing over i proves the ﬁrst inequality; the second follows from the fact
that

3.2 An example

Proof of Theorem 3.1. Take s so that e−s = 1−e−t and apply Proposition 3.5:

Ew1(fZs) ≥(e2t−et) Var(Ptf). By Proposition 3.2 and because Var(fZs) ≤ 1,
Finally, e2t − et = et(et − 1) ≥ 1
Let n = m2, and let Ji ={(i − 1)m, . . . , im − 1}. Let Bn ⊂{−1, 1}n be the set
⎧⎪⎪⎪⎨⎪⎪⎪⎩

EM(fZs) ≥ cEw1(fZs) ≥ c(e2t − et) Var(Ptf).
2(et + 1)(et − 1) = 1
2(e2t − 1).
≤ m⎫⎪⎪⎪⎬⎪⎪⎪⎭
arccos(e−2t)
√2π

stable, with the same estimate as its Gaussian analogue in Section 2.1.
Proposition 3.6. For any n and any t > 0,

From the central limit theorem, one sees immediately that Bn is noise

Var(Pt1Bn) ≥ 1

− on(1).

1√m Q

j∈Ji

mQ

i=1⎛⎝

2

xj⎞⎠

−

4

In particular Bn is noise stable.

x ∶

.

Finally, we show that Bn is not correlated with any half-space. This es-
sentially follows from the invariance principle, which says that nice boolean
functions have almost the same distribution when their arguments are re-
placed by Gaussian variables.

Proposition 3.7. M(Bn) ≤ Cm−1~200

16

a− = a − a+.

For the rest of this section, ﬁx x ∈ Rn and b ∈ R, and suppose that

A ={x ∈{−1, 1}n ∶ ∑i aixi ≤ b}. Let J∗ ⊂{1, . . . , n} be the set containing
the indices of the ⌊m1~3⌋ largest  ai . Deﬁne a+ by a+i = 1{i∈J ∗}ai and set

We split our proof of Proposition 3.7 into two parts, depending on the
decay properties of a. If a− is unbalanced, it follows that a+ must contain
only large coordinates. We apply the Littlewood-Oﬀord theorem to argue
that a− is essentially irrelevant and A depends only on a few coordinates.
Since Bn doesn’t depend on any small set of coordinates, this implies that
A and Bn are uncorrelated. If a− is fairly balanced then we condition on

sup
c∈R

PrQ

i

Pr⎛⎝ Q

j∈J ∗

First, we recall the Littlewood-Oﬀord inequality:

On the other hand, Chebyshev’s inequality implies that

Xiai − c ≤ t min

boolean variables with Gaussian variables and applying Proposition 2.3.

{Xi ∶ i ∈ J∗} and apply an invariance principle to {Xi ∶ i~∈ J∗}, replacing
Theorem 3.8. If X is uniformly distributed in{−1, 1}n then
 ai  ≤ Ctn−1~2.
Lemma 3.9. Ifa−∞ ≥ m−1~24a−2 then Cov(1A, 1Bn) ≤ Cm−1~12.
Proof. By Theorem 3.8 and since ai  ≥a−∞ for all i ∈ J∗,
aj Xj − b ≤ m1~24a−2⎞⎠ ≤ Cm1~12 J∗ −1~2 ≤ Cm−1~12.
Pr⎛⎝ Q
Pr(X ∈ A  Xj ∶ j ∈ J∗) ∈[0, m−1~12] ∪[1 − m−1~12, 1].

1 − Cm−1~12 over{Xj ∶ j ∈ J∗} we have
On the other hand, conditioning on{Xj ∶ j ∈ J∗} has little eﬀect on the event
Bn: each random variable Zi ∶= ∑j∈Ji Xj2
m ± O( Ji ∩ J∗ 2) and conditional variance O(m); moreover, E[ Zi − EZi 3] =
O(m3~2). Then
Xj2

aj Xj ≥ m1~24a−2⎞⎠ ≤ m−1~12.

Putting these two inequalities together, we see that with probability at least

(5)

has conditional expectation

i

j~∈J ∗

mQ
i=1 Q

j∈Ji

17

has conditional expectation m2 ± O( J∗ 2) = m2 ± O(m2~3). By the Berry-

Esseen theorem,

Combined with (5), this implies that

2

Pr(X ∈ Bn  Xj ∶ j ∈ J∗) = 1
E[(1Bn

± O(m−1~2).
− Pr(Bn))1A  Xj ∶ j ∈ J∗] ≤ Cm−1~12

implies the claim.

let hc,ǫ be a function satisfying

canonically extended to subsets of Rn.

remaining case of Proposition 3.7 in two steps: for the rest of the section,

with probability at least 1 − Cm−1~12. Integrating over {Xj ∶ j ∈ J∗}, this
Since Lemma 3.9 implies Proposition 3.7 in the casea−∞ ≥ m−1~24a−2,
we may assume from now on thata−∞ ≤ m−1~24a−2. We will prove the
let X be uniform on{−1, 1}n and take Y ∼ γn; note that A and Bn can be
For any c ∈ R, let hc ∶ R →[0, 1] be the function hc(x) = 1{x≤c}. For ǫ > 0,
• hc,ǫ takes values in[0, 1],
• hc,ǫ(x) = hc(x) for all x such that x − c  ≥ ǫ, and
For z ∈{−1, 1}J ∗
and si = ∑j∈Ji∩J ∗ zi. Next, deﬁne the polynomials

constant C (where h(k) denotes the kth derivative of h).

is uniformly bounded by Cǫ−k for some universal

• for k = 1, 2, 3, h(k)c,ǫ

and let Ωz be the event{Xi = zi ∀i ∈ J∗}. Set J′i = Ji ∖ J∗
i  Q
p(x) = 1
m2 Q
pz(x) = 1
i  Q
m2 Q
qz(x) = 1
a− Q

xj2
xj + si2

aj xj + Q
j∈J ∗

aj zj.

Recalling (from the Berry-Esseen theorem) that Pr(X ∈ Bn) = 1

our goal is to show that

+O(m−1~2),

j~∈J ∗

j∈Ji

j∈J ′

i

2

E1A(X)1Bn(X) −

1

2 ≤ Cm−1~12.

18

We will achieve this by conditioning on Ωz: for an arbitrary z, we claim that

Going back to the deﬁnitions of pz and qz, this is equivalent to

We divide the proof of (6) into several steps: for any ǫ > 0,

1

1

E1A(X)1Bn(X) −
2 Ωz ≤ Cm−1~12.
Ehb′(qz(X))h1(pz(X)) −
2 ≤ Cm−1~12,
E h1(pz(X)) − h1(p(X))  ≤ Cm−1~6
E h1,ǫ(p(X)) − h1(p(X))  ≤ C max{ǫ, m−1~2}
E hb′,ǫ(qz(X)) − hb′(qz(X))  ≤ C max{ǫ, m−1~24}
E h1,ǫ(p(Y)) − h1(p(Y))  ≤ C max{ǫ, m−1~2}
E hb′,ǫ(qz(Y)) − hb′(qz(Y))  ≤ C max{ǫ, m−1~2}
Cov(hb′(qz(Y)), h1(p(Y))) ≤ Cm−1~2.

(6)

(7)

(8)

(9)

(12)

 E[hb′,ǫ(qz(X))h1,ǫ(p(X))] − E[hb′,ǫ(qz(Y))h1,ǫ(p(Y))]  ≤ Cǫ−3m−1~48 (10)

(11)

(13)
Taking ǫ = m−1~200 and combining (7) through (13) using the triangle in-
equality yields (6).

of the ith coordinate to be

to be bounded by m−1~24).

Fortunately, most of the pieces above are easy: (8) follows from the Berry-
Esseen theorem, since h1,ǫ and h1 are both bounded by one, and agree except
on an interval of length 2ǫ. Inequalities (9), (11), and (12) follow by the same
argument (the reason for the worse bound in (9) is because the error term

It remains to check (7), (10), and (13); for these, it helps to introduce the

in the Berry-Esseen theorem depends ona−∞~a−2, which we only know
notion of inﬂuences: for function f ∶{−1, 1}n → R, we deﬁne the inﬂuence
Inf i(f) = Var E[f(X)  X1, . . . , Xi−1, Xi+1, . . . , Xn].
If the range of f is{−1, 1} then Inf i(f) is just the probability that negating
Xi will change the value of f(X).
Sk ∶=∑j∈Jk
Xj implies that with probability at least 1 −Cm−1~6, h1(p(X))
falls outside the interval[1 − 6m−2~3, 1 + 6m−2~3]. Hence, in order to change
the value of h1(p(X)), one would need to change the value of ∑k S 2

For (7), note that the Berry-Esseen theorem applied to the variables

k by at

19

above fails. This proves (7).

function of some half-space, the following Lemma proves (13).

least 6m4~3. On the other hand, Hoeﬀding’s inequality implies that with
change the value of ∑k S 2

k by 6m4~3, one would need to change at least 2m1~3

probability at least 1 − Cm−1~6, maxk Sk  ≤ 2m. On this event, in order to
of the Xj. Since pz(X) is obtained from p(X) by changing at most m1~3
of the Xj, we see that h1(p(X)) = h1(pz(X)) unless one of the two events
Recognizing that h1(p(Y)) = 1Bn(Y) and hb′(qz(Y)) is the indicator
Lemma 3.10. For any half-space A, Cov(1A(Y), 1Bn(Y)) ≤ m−1~2
terms of its contribution in the(1, . . . , 1) direction and the contribution in

Proof. The covariance in question can be written in terms of covariances
between half-spaces and m-dimensional balls, which we may then bound
using Proposition 2.3. To do this, we break each block of m variables in

the orthogonal direction: for each block J of m variables, deﬁne

and

xJ = m−1~2 Q
j∈J

xj ,

rJ =Q

j∈J

aJ = EXJ Q

j∈J

a2
j

− a2
J ,

aj

ajxj(cid:6) = m−1~2 Q
r =Q

r2
J .

j∈J

J

Now deﬁne A′, B′ ⊂ Rm+1 by

A′ =x ∈ Rm+1
B′ =x ∈ Rm+1

∶

∶

mQ
i=1
mQ
i=1

aJixi + rxm+1 ≤ b
i ≤ m .

x2

Note that A′ and B′ are the push-forwards of ˜An and ˜B under a map that
if Πm ∶ Rn → Rm is deﬁned by
preserves the standard Gaussian measure:

Πmx =(xJ1 , . . . , xJm) and Π is deﬁned by

then x ∈ A (resp. B) if and only if Πx ∈ A′ (resp. B′). Since Π pushes
forward γn onto γm+1, we have

Πx =(Πmx, r−1(⟨a, x⟩ −⟨Πma, Πmx⟩)
Cov(1 ˜A, 1 ˜Bn) = Cov(1A′ , 1B ′).

On the other hand, Cov(1A′ , 1B ′) ≤ m−1~2 by Proposition 2.3.

20

Finally, (10) follows from the following multivariate invariance principle

that was proved by the ﬁrst author in [8]:

third partial derivatives uniformly bounded by B,

Theorem 3.11. Suppose p(x) and q(x) are polynomials of degree at most
d such that Inf i(p) ≤ τ and Inf i(q) ≤ τ for all i. For any Ψ ∶ R2 → R with
where Y ∼ γn, X is uniform on{−1, 1}n, and C is a universal constant.
Taking d = 2, τ = m−1~24 and Ψ(x, y) = h1,ǫ(x)hb′,ǫ(y) (which has third

 EΨ(p(X), q(X)) − EΨ(p(Y), q(Y))  ≤ C ddB√τ ,

derivatives bounded by Cǫ−3) proves (10).

3.3 The converse of Theorem 3.1

Here, we state and prove the boolean analogue of Theorem 2.8 (or, the

non-negligible with constant probability then f is noise stable.

qualitative converse of Theorem 3.1). That is, we show that if M(fs,Y) is
Theorem 3.12. For any 0 < r < s and any f ∶{−1, 1}n →[−1, 1],
1 − e−2s1~4
(1 − e−2(s−r)) Var(Prf) ≥ 4EM 2(fZs) − C 1 − e−2r

where Zs ∼ µs and C is a universal constant.

,

The proof of Theorem 3.12 is very much like the proof of Theorem 2.8,
so we give only a sketch. As in the proof of Theorem 2.8, the ﬁrst step is
a bound on the noise stability of half-spaces. However, the bound that we
used to prove Lemma 2.9 is equivalent to an open question (the “majority
is least stable conjecture”) in the boolean case, so we use a weaker (by a
constant factor) bound due to Peres [10]:

Next, we show that any set which is correlated with a half-space must

Theorem 3.13. For any half-space A and any t > 0, E[(1A −Pt1A)2] ≤ C√t,
Proposition 3.14. Suppose that A ⊂{−1, 1}n is a half-space. Then for any
f ∶{−1, 1}n →[0, 1] and any t > 0,

be noise stable (indeed, almost as noise stable as the half-space itself).

where C is a universal constant.

− Ct1~4

for a universal constant C.

Var(Ptf) ≥ 4 Cov(A, f)2

21

The proof of Proposition 3.14 is essentially identical to the proof of
Proposition 2.10, so we omit it. The only diﬀerence is that we use The-
orem 3.13 instead of Lemma 2.9.

Finally, the argument to go from Proposition 3.14 to Theorem 3.12 is also
essentially identical to the Gaussian case: the only property of Gaussians
that we used in that argument was the Poincar´e inequality, which takes the
same form in the boolean case.

3.4 Acknowledgement

We thank Dana Moshkovitz, Gil Kalai and Irit Dinur for encouragement
to complete this work. E.M acknowledges the support of NSF grant CCF
1320105, DOD ONR grant N00014-14-1-0823, and grant 328025 from the
Simons Foundation”.

References

[1] Dominique Bakry, Ivan Gentil, and Michel Ledoux. Analysis and ge-

ometry of Markov diﬀusion operators, volume 348. Springer, 2014.

[2] I. Benjamini, G. Kalai, and O. Schramm. Noise sensitivity of boolean
Inst. Hautes ´Etudes Sci.

functions and applications to percolation.
Publ. Math., 90:5–43, 1999.

[3] D. M. Kane. The gaussian surface area and noise sensitivity of degree-d
polynomial threshold functions. Computational Complexity, 20(2):389–
412, 2011.

[4] S. Khot and D. Moshkovitz. Candidate hard Unique Game. In STOC,

2016. to appear.

[5] A. Klivans, R. O’Donnell, and R. Servedio. Learning intersections and
thresholds of halfspaces. Journal of Computer and System Sciences,
68(4):808–840, 2004.

[6] M. Ledoux.

Semigroup proofs of the isoperimetric inequality in
Euclidean and Gauss space. Bulletin des sciences math´ematiques,
118(6):485–510, 1994.

[7] N. Linial, Y. Mansour, and N. Nisan. Constant depth circuits, fourier
transform and learnability. Journal of the ACM, 40(3):607–620, 1993.

22

[8] E. Mossel. Gaussian bounds for noise correlation of functions. GAFA,

19:1713–1756, 2010.

[9] Ryan O’Donnell. Analysis of Boolean functions. Cambridge University

Press, 2014.

[10] Y. Peres. Noise stability of weighted majority. arXiv:math/0412377,

2004.

23

