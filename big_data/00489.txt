Weakly Supervised Localization using Deep

Feature Maps

Archith J. Bency1, Heesung Kwon2, Hyungtae Lee2, S. Karthikeyan1, and

B. S. Manjunath1

1 University of California, Santa Barbara, CA, USA

2 Army Research Laboratory, Adelphi, MD

Abstract. Object localization is an important computer vision problem
with a variety of applications. The lack of large scale object-level anno-
tations and the relative abundance of image-level labels makes a com-
pelling case for weak supervision in the object localization task. Deep
Convolutional Neural Networks are a class of state-of-the-art methods
for the related problem of object recognition. In this paper, we describe
a novel object localization algorithm which uses classiﬁcation networks
trained on only image labels. This weakly supervised method leverages
local spatial and semantic patterns captured in the convolutional layers
of classiﬁcation networks. We propose an eﬃcient beam search based ap-
proach to detect and localize multiple objects in images. The proposed
method signiﬁcantly outperforms the state-of-the-art in standard object
localization data-sets with a 8 point increase in mAP scores.

Keywords: Weakly Supervised methods, Object localization, Deep Con-
volutional Networks

1 Introduction

6
1
0
2

 
r
a

 

M
9
2

 
 
]

V
C
.
s
c
[
 
 

2
v
9
8
4
0
0

.

3
0
6
1
:
v
i
X
r
a

Given an image, an object localization method aims to recognize and locate
interesting objects within the image. The ability to localize objects in images
and videos eﬃciently and accurately opens up a lot of applications like automated
vehicular systems, searching online shopping catalogues, home and health-care
automation among others. Objects can occur in images in varying conditions of
occlusion, illumination, scale, pose and context. These variations make object
detection a challenging problems in the ﬁeld of computer vision.

The current state of the art in object detection includes methods which in-
volve ‘strong’ supervision. In the context of object detection , strong supervision
entails annotating localization and pose information about present objects of
interest. Generating such rich annotations is a time-consuming process and is
expensive to perform over large data-sets. Weak supervision lends itself to large-
scale object detection for data-sets where only image-level labels are available.
Eﬀective localization under weak supervision enables extensions to new object
classes and modalities without human-generated object bounding box annota-
tions. Also, such methods enable generation of inexpensive training data for
training object detectors with strong supervision.

2

Bency, Kwon, Lee, Karthikeyan, Manjunath

Deep Convolutional Neural Networks (CNNs) [45], [27] have created new
benchmarks in the object recognition challenge [11]. CNNs for object recognition
are trained using image-level labels to predict the presence of objects of interest
in new test images. A common paradigm in analyzing CNNs has emerged where
the convolutional layers are considered as data-driven feature extractors and the
subsequent fully-connected layers constitute hyperplanes which delineate object
categories in the learnt feature space. Non-linearities through Rectiﬁed Linear
Units (ReLU) and sigmoidal transfer functions have helped to learn complex
mapping functions which relate images to labels. The convolutional layers en-
code both semantic and spatial information extracted from training data. This
information is represented by activations from the convolutional units in the
network which are commonly termed as Feature Maps.

Fig. 1. When localizations centered around objects of interest are classiﬁed by Deep
CNNs, the corresponding object classes are assigned high scores.

In this paper, we present a method that exploits correlation between semantic
information present in Feature Maps and localization of an object of interest
within an image. An example of such correlation can be seen in Figure 1. Note
that crudely localized image-patches with the objects of classes,‘chair’, ‘person’
and ‘tv monitor’, generate high classiﬁcation scores for the corresponding classes.

Weakly Supervised Localization using Deep Feature Maps

3

This suggests that one can coarsely localize objects solely by image classiﬁcation
scores in this context.

CNN based classiﬁers are trained for the task of image recognition on large
image classiﬁcation data-sets [11], [14], [13]. The learnt convolutional ﬁlters com-
pute spatially localized activations across layers for a given test image [30]. We
examine the activation values in the outermost convolutional layer and propose
localization candidates (or bounding boxes) which maximize classiﬁcation scores
for a class of interest. Class scores vary across localization candidates because
of the aforementioned local nature of the convolutional ﬁlters. We then pro-
gressively explore smaller and smaller regions of interest till a point is reached
where the classiﬁer is no longer able to discriminate amongst the classes of in-
terest. The localization candidates are organized in a search tree, the root node
being represented by the entire test image. As we traverse from the root node
towards the leaf nodes, we consider ﬁner regions of interest. To approximate
the search for optimal localization candidates, we adopt a beam search strategy
where the number of candidate bounding boxes are restricted as we progress to
ﬁner localizations. This strategy enables eﬃcient localization of multiple objects
of multiple classes in images. We outperform the state-of-the-art in localization
accuracy by a signiﬁcant margin of up to 8 mAP on two standard data-sets with
complex scenes, PASCAL VOC 2012 [14] and the much larger MS COCO [28].

The main contributions of this paper are:

– We present a method that tackles the problem of object localization for im-
ages in a weakly supervised setting using deep convolutional neural networks
trained for the simpler task of image-level classiﬁcation.

– We propose a method where the correlation between spatial and semantic
information in the convolutional layers and localization of objects in images
is used explicitly for the localization problem.

2 Related Work

The task of object detection is one of the fundamental problems in computer
vision with wide applicability. Variability of object appearance in images makes
object detection and localization a very challenging task and thus has attracted
a large body of work. Surveys of the state-of-the-art are provided in [49], [38].

A large selection of relevant work are trained in the strong supervision
paradigm with detailed annotated ground truth in the form of bounding boxes
[47], [15], object masks [4], [26], [20] and 3D object appearance cues [19],[43].
The requirement of rich annotations curb the application of these methods in
data-sets and modalities where training data is limited to weaker forms of label-
ing. Weak supervision for object detection tries to work around this limitation
by learning localization cues from large collection of data with in-expensive an-
notations.

Large data-sets like Imagenet [11] and MS COCO are available with image-
level labels. There has been signiﬁcant work in this direction for object localiza-

4

Bency, Kwon, Lee, Karthikeyan, Manjunath

tion and segmentation [16], [7], [22], [3], [12], [37]. Apart from image-level labels,
other kinds of weak supervision include using eye-tracking data [36], [42].

Deep convolutional neural networks (CNN) have seen a surge of attention
from the computer vision community in the recent years. New benchmarks have
been created in diverse tasks such as image classiﬁcation and recognition [44],
[27], [45], [5], object detection [18], [41], [52], [50], [35] and object segmentation
[29], [6], [32] among others by methods building on deep convolutional network
architectures. These networks perform tasks using feature representations learnt
from training data instead of traditional hand-engineered features [10], [15], [31].
Typical algorithms of this paradigm perform inference over the last layer of the
network. There have been recent works [21], [9], [23] which exploit semantic
information encoded in convolutional feature map activations for semantic seg-
mentation and object detection. A prerequisite for these CNN-based algorithms
is strong supervision with systems focused on detection requiring location masks
or object bounding boxes for training. [51] studies the presence of object detector
characteristics in image-classiﬁcation CNNs, but does not provide a computa-
tional method to carry out object detection.

Oquab et.al. [34] has proposed a weakly supervised object localization system
which learns from training samples with objects in composite scenes by explicitly
searching over candidate object locations and scales during the training phase.
While this method performs well on data-sets with complex scenes, the extent of
localization is limited with respect to estimating one point in the test image. The
extent of the object is not estimated and detecting multiple instances of the same
object class is not considered. In our proposed approach, we estimate both the
location and extent of objects and are capable of estimating multiple instances
of objects in the test image. Also, we use pre-existing classiﬁcation networks for
localization where as [34] proposes training custom adaptation layers.

3 Weakly Supervised Object Localization

3.1 Overview of the method

We aim to localize and recognize objects in images using CNNs trained for clas-
siﬁcation. There are two distinct phases. The ﬁrst phase consists of learning
image-level recognition from training image sets using existing Deep CNN ar-
chitectures. We use the popular Alexnet [27] and VGG-16 [44] networks for our
experiments. The next phase involves generating localization candidates in the
form of bounding boxes for object classes of interest. These candidates are gen-
erated from a spatial grid corresponding to the ﬁnal convolutional layer of the
network and are organized in a search tree. We carry out a beam-search based
exploration of these candidates with the image classiﬁer scoring the candidates
and reach at a set of ﬁnal localization candidates for each class of interest.

3.2 Network architecture and training

The Alexnet network has ﬁve convolutional layers with associated rectiﬁca-
tion and pooling layers C1, C2, . . . , C5, along with three fully connected lay-

Weakly Supervised Localization using Deep Feature Maps

5

Fig. 2. An illustration of how two diﬀerent localization candidates are compared in the
localization process. Candidate # 1 scores higher for the bicycle class than candidate
# 2. The ﬁrst candidate is further iterated upon to achieve ﬁner localization. The green
box in the left image denotes ground-truth location of the bicycle object.

ers F6, F7, F8 with M6 = σ(W6M5 + B6), M7 = σ(W7M6 + B7) and M8 =
γ(W8M7 + B8). Wn, Bn are learn-able parameters for the n-th layer, Mn is the
output of the n-th layer. σ(X) = max(0, X) is the rectiﬁcation function and
γ(X) = [eX[i]/ΣjeX[j]] is the softmax function. Of particular interest to us is the
output of the last convolutional layer C5, M5 which we will refer to subsequent
sections.

We learn the network parameters through stochastic gradient descent and
back-propagation of learning loss error [40] from the classiﬁcation layer back
through the fully connected and convolutional layers. Keeping in mind that
objects of multiple classes can be present in the same training image, we use
the cross entropy loss function to model error loss J between ground truth class
probabilities {pk} and predicted class probabilities {ˆpk}, where k ∈ {0, 1, ..., K−
1} indexes the class labels.

[pk log ˆpk + (1 − pk) log(1 − ˆpk)]

(1)

K−1(cid:88)

k=0

J = − 1
K

As speciﬁed in [33], we remove F8 and add two additional fully connected
adaptation layers Fa, Fb. Similar to the Alexnet network, the ouput of these
layers are computed as Ma = σ(WaM7 + Ba) and Mb = γ(WbMa + Bb). In
order to assess the eﬀectiveness of the proposed method for localization, these
additional layers are added to facilitate re-training of the network from the
Imagenet data-set to the Pascal VOC or MS COCO object detection data-sets.
We initialize network parameters to values trained on the Imagenet data-set and

6

Bency, Kwon, Lee, Karthikeyan, Manjunath

ﬁne-tune them [25] to adapt onto a target data-set. This is achieved by setting
the learning rate parameter for the last layer weights to a higher value relative
to earlier layer weights. An illustration of the network architecture is presented
in Figure 2 of [33].

We train the augmented network on labeled samples from the target data-set.
The trained network produces class scores at the ﬁnal layer which are treated as
probability estimates of the presence of a class in the test image.

The VGG-16 network, being similar to the Alexnet network, has thirteen
convolutional layers C1, C2, C3, ....C13 with associated rectiﬁcation and pooling
layers, along with three fully connected layers F6, F7, F8. Similar to the Alexnet
network, the feature map M13 is of special interest to us. The increased number
of layers and associated learnable parameters provides an improved image recog-
nition performance when compared to the Alexnet network. The improvement
however comes at the cost of increased GPU memory (442 MB vs 735 MB) and
computations (6 milliseconds vs 26 milliseconds for classifying an image).

In addition to using image-labels to train the deep CNNs, we also use label
co-occurrence information to improve classiﬁcation. Some classes tend to occur
together frequently. For example, people and motorbikes or people and chairs
tend to share training samples. We treat the class scores from the classiﬁer as
unary scores and combine them with the likelihood of co-existence of multi-
ple objects of diﬀerent classes in the same object. We model the co-existence
likelihood by building a co-occurrence matrix for class labels from the training
data-set. For the class bi,

(cid:88)

i(cid:54)=j

spair(bi|bj)

scomb(bi) = sunary(bi) + α
spair(bi|bj) = ppair(bi|bj)sunary(bj)
ppair(bi|bj) =

|bi ∩ bj|

|bj|

(2)

(3)

(4)

where sunary is the initial classiﬁcation score for the test image, spair is the
pairwise score, |bi ∩ bj| denotes the number of training samples containing the
labels bi and bj and scomb is the combined score which we use to re-score the
classes for the test image. The parameter α denotes the importance given for
pair-wise information in re-scoring. An optimal value is derived by testing over
a randomly sampled validation sub-set from the training set.

3.3 Localization

In deep CNNs trained for classiﬁcation, feature map activation values are the
result of repeated localized convolutions, rectiﬁcation (or other non-linear op-
erations) and spatial pooling. Hence the structure of the network inherently
provides a receptive ﬁeld for each activation on the input image. The foot-print
region becomes progressively coarser as we go deeper in the layers towards the

Weakly Supervised Localization using Deep Feature Maps

7

fully connected layers. In a ﬁrst attempt, we explore ways to exploit the spatial
information encoded in the last convolutional layer for object localization.

Also, standard state-of-the-art object recognition data-sets (for e.g. Ima-
genet) typically have the object of interest represented in the middle of train-
ing samples. This gives rise to a bias in the classiﬁer performance where more
centered an object is in the input image, higher the corresponding class score
becomes. An example is illustrated in Figure 1. The correlation between the
location of objects and class scores has been observed in other works [33], [18].

Fig. 3. A visual result of the proposed localization strategy on an image. The class
scores for ‘person’ category are used to progressively localize the object of interest.
Blue rectangles represent localization candidates considered in previous iterations and
red rectangles represent current candidates.

A naive approach to exploit the correlation would be to carry out a multi-
scale sliding window sampling of sub-images from the test sample and spatially
pool the classiﬁer scores to generate a heat map of possible object locations for a
given object class C. The number of sub-images required for eﬀective localization
can be in the order of thousands. Although powerful hardware like GPUs have
brought image recognition CNNs into the domain of real-time methods, process-
ing a large number of windows for every test sample is prohibitively expensive.
A class of object detection methods [18] try to reduce the number of candidate
windows by using object region proposal methods [46], [1]. Time taken to detect
objects in each image using these methods still range in tens of seconds when
using powerful GPUs.

For a more computationally eﬃcient approach, we take advantage of the
spatial and semantic information encoded in the ﬁnal convolutional feature maps
to guide the search process. We refer to the maps as M5 for Alexnet and M13 for
VGG-16 in the section 3.2. For a general CNN network, the ﬁnal convolutional
layer is of size L × L × T which means there are T feature maps of size L × L.
For the Alexnet and VGG-16 networks, the feature maps are of size 6 × 6 × 256
and 7 × 7 × 512 respectively.

8

Bency, Kwon, Lee, Karthikeyan, Manjunath

Given a test image I, we forward propagate the layer responses for the image
up-to the ﬁnal convolutional layer Clast and generate the feature map activations
Mlast. We generate localization candidates which are sub-grids of the L× L grid.
In concrete terms, these candidates are parametrised as boxes bi = [xi, yi, wi, hi]
for i = 1, 2, . . . , B where x, y, w and h represent the coordinates of the upper-left
corner, width and height and B is the total number of possible sub-grids. For
each localization candidate, we sample the feature map activations contained
within the corresponding boxes and interpolate them over the entire L× L grid.
This is done independently over all T feature maps. For the box bi,

ˆM t

last(x, y) = f (M t

last(x(cid:48), y(cid:48)))

∀ xi ≤ x(cid:48) ≤ xi + wi − 1,
yi ≤ y(cid:48) ≤ yi + hi − 1,
t ∈ 0, 1, . . . , T − 1

where f (.) is an interpolation function which resizes the activation subset of size
wi×hi to the size L×L. In the above equation, x, y ∈ {0, . . . , L−1} and bi-linear
interpolation is used. After obtaining the reconstructed feature maps ˆMlast, we
forward propagate the activations into the fully connected layers and obtain the
class scores. An illustration of this step is presented in Figure 2.

A limitation of the above approach is related to the fact that interpolating
from a smaller subset to the larger grid will introduce interpolation artifacts into
the reconstructed feature maps. In order to mitigate the eﬀects of the artifacts,
we limit the localization candidates to boxes with L − 1 ≤ wi ≤ L and L − 1 ≤
hi ≤ L. From this limited corpus of localization candidates, we generate the
corresponding ˆMlast and consequently the object class scores, and choose the
candidate with the highest class score. With the resultant localization candidate
box br, we backproject onto the image space by cropping:

xcrop =

wcrop =

xr
L
wr
L

W, ycrop =

W, hcrop =

yr
L
hr
L

H

H

(5)

Icrop(x, y) = I(x + xcrop, y + ycrop) ∀ 0 ≤ x < wcrop
0 ≤ y < hcrop

where x, y indicate pixel locations, and W and H are width and height of the
test image respectively. We then repeat the above described localization process
on Icrop till a predetermined number of iterations. A visual example of progress
in the iterative process is shown in Figure 3.

Weakly Supervised Localization using Deep Feature Maps

9

3.4 Search Strategy

The localization strategy can be visualized as traversing down a search-tree
where each node corresponds to a localization candidate bi. The root node of
such a tree would be b0 = [0, 0, L, L]. The children of a node bi in the tree would
be the candidates {bj} which lie within sub-grid corresponding to bi and whose
parameters {wj} and {hj} satisfy the below conditions:
wi − 1 ≤ wj ≤ wi, hi − 1 ≤ hj ≤ hi

(6)

We consider children nodes whose width or height values, but not both of them
diﬀer from the parent node by 1. This restriction is put in place so that we
are minimally modifying the feature map activations for discriminating amongst
candidates. An example of a parent node bi and the corresponding children node
set {bj} is shown in Figure 4.

Fig. 4. An example of a parent node (represented in red) and it’s children nodes (rep-
resented in blue) displayed on a 6 × 6 grid, as is the case for the Alexnet M5 feature
maps.

During traversal, the child candidate with the highest score for the class C
is selected. This approach is a greedy search strategy where we follow one path
from the root node to a leaf node which represents the ﬁnest localization, and
is susceptible to arrival at a locally optimal solution. Alternatively, we could
evaluate all the nodes in the entire search-tree and could come up with the
localization candidate with the highest score for class C. However, this would be
computationally prohibitive.

To address this, we use the widely known beam-search [39] strategy. At each
level of the search-tree we generate sets of children nodes from the current set of
localization candidates using Equation 6. We then rank them according to the
scores for class C. Only the top M candidates are pursued for further evalua-
tion. An illustration is presented in Figure 5. In the Figure, we show an example

10

Bency, Kwon, Lee, Karthikeyan, Manjunath

Fig. 5. A visual example of beam-search strategy to navigate the search tree amongst
localization candidates. In this speciﬁc case, the class C is ‘car’, M is set to 2 and L is
6.

Weakly Supervised Localization using Deep Feature Maps

11

where the two highest candidates are chosen at each level. The children nodes of
these candidates are evaluated and ranked. We traverse a total of H levels. This
approach helps us achieve a balance between keeping the number of computa-
tions to be tractable and avoiding greedy decisions. An additional advantage is
the ability to localize multiple instances of the same class as the beam-search
increases the set of localization candidates that are evaluated when compared
to the greedy search strategy. Regions in the image corresponding to top-ranked
candidates from each level are spatially sum-pooled using candidates scores to
generate a heat-map. The heat-map is then threshold-ed. Bounding rectangles
for the resulting binary blobs are extracted. The bounding rectangles are pre-
sented as detection results of our method. The average value of the heat-map
values enclosed within detection boxes are assigned as the score of the boxes.
In our experiments, we have set the value of M as 8 and H in the search tree
as 10 for all data-sets. Heat-map thresholds for each class were determined by
evaluation on a small validation sub-set from the training set.

4 Experiments

4.1 Data-sets and Network training

We evaluate our localization method on two large image data-sets, the PASCAL
VOC 2007 [13], 2012 and the MS COCO. The VOC 2012 data-set has labels for
20 object categories and contains 5717 training images, 5823 validation images
and 10991 test images. VOC 2007 shares the same class-labels with 2501 training
images, 2510 validation images and 4952 test images. For the MS COCO data-
set, there are 80000 images for training and 40504 images for validation with
80 object classes being present. These data-sets contain both image-level labels
and object location annotations. For weak supervision we use the image-level
labels from the training set to train classiﬁcation networks and use the location
annotations in the test and validation sets for evaluation.

We ﬁne-tune the original VGG-16 and Alexnet networks (trained on Ima-
genet) by re-training the ﬁnal fully connected layer for the VOC 2007, 2012
and MS COCO data-sets. We set the learning rate parameter to 0.001 which we
decrease by a factor 10 for every 20000 training batches. Each training batch con-
sists of 50 samples and the network was trained with 400000 batches. In order to
balance the data-sets with respect to number of samples per class, we oversam-
pled training samples from under-represented classes. We generate additional
samples by a combination of adding white gaussian noise and random rotations
in the ± 30◦ range. We use Caﬀe [24] as our software platform for training and
deploying classiﬁcation networks on an NVIDIA TITAN X Desktop GPU.

4.2 Metrics

To compare results with the state-of-the-art in weakly supervised localization
methods, we use the localization metric suggested by [34]. From the class-speciﬁc

12

Bency, Kwon, Lee, Karthikeyan, Manjunath

Fig. 6. Visual sample results from the proposed method for Pascal VOC 2007 test
set. Yellow rectangles overlaid on the images represent location and extent predictions.
The locations of objects in the shown images are accurately estimated. Considering
that only image-level labels are used for training, extent estimations are a challenging
problem in this setting.

Weakly Supervised Localization using Deep Feature Maps

13

heat-maps generated by our localization, we extract the region of maximal re-
sponse. If the center location of the maximal response lies within the ground-
truth bounding box of an object of the same class, we label the location predic-
tion as correct. If not, the false positive count is increased as the background was
assigned to the class, and the false negative count is increased because object
was not detected. The maximal value of the heat-map is assigned as conﬁdence
of the localization. The conﬁdence score is then used to rank localizations and
associated precision-recall (p-r) curves are generated for each object class. The
p-r curves are characterized by an estimate of the area under the curve, which
is termed as the Average Precision (AP). The AP score can vary from 0 to
100. An AP score of 100 signiﬁes that all true positives were localized and no
false positives were assigned scores. The AP scores for all classes are averaged
to derive the Mean Average Precision (mAP), which presents a summarized
score for the entire test set. This evaluation metric diﬀers from the traditional
Intersection-over-Union (IoU ) measures to determine bounding box quality w.r.t
the ground-truth, as the extent of the localization is not captured.

In addition to the above metric, we are interested in measuring how eﬀective
our method is in capturing the extent of the object of interest. We calculate
the standard average precision for our detection results, where true positives are
determined when intersection over union (IoU) between the predicted bounding
boxes and the corresponding ground-truth box of the same class exceeds 0.5.

4.3 Results

For obtaining localization results, we ﬁne-tuned the networks using training sam-
ples from the train set of PASCAL VOC 2012 data-set and tested the trained
networks on the validation set. As we use the class-scores from the classiﬁers
to drive our localization strategy, good classiﬁcation performance is essential
for robust object localization. We present the classiﬁcation performance on the
PASCAL VOC 2012 validation set in Table 1. The VGG-16 network provides
improved classiﬁcation with respect to Alexnet and a consequent improvement
can be seen in the localization scores as well.

In Table 1, we also compare the localization results of our method with
respect to recent state-of-the-art weakly supervised localization methods on the
PASCAL VOC 2012 validation set. We achieve a signiﬁcant improvement of 5
mAP over the localization performance of Oquab et.al [34]. We also compare
against the RCNN [18] and Fast RCNN [17] detectors which are trained with
object-level bounding boxes. Similar to the way [34] evaluates [18], we select the
most conﬁdent bounding box proposal per class per image for evaluation. Since
deep neural networks are the state-of-the-art in object detection and localization
tasks, we have compared with CNN-based methods.

We summarize the localization results for the much larger MS COCO vali-
dation data-set in Table 2. Inspite of having weaker classiﬁcation performance
(54.1 mAP vs 62.8 mAP) than the network used by [34], we are able to pro-
duce stronger localization performance by a large margin of 8 mAP. This is a
signiﬁcant improvement in performance over the state-of-the-art method. This

14

Bency, Kwon, Lee, Karthikeyan, Manjunath

Image Classiﬁcation

Proposed
Method

+

Proposed
Method

+

Proposed
Method

+

Proposed
Method

+

VGG-16

Alexnet

VGG-16

Alexnet

93.0
89.7
91.4
89.6
69.5
90.9
81.6
92.0
69.3
88.9

80.2

92.0
82.9
87.2
83.8
54.1
87.3
74.5
87.0
56.4
76.7

71.1

90.1
86.4
86.4
77.6
56.8
90.3
68.3
89.9
54.7
86.8

66.4

90.0
81.2
81.2
82.2
47.5
86.7
64.9
85.7
53.9
75.8

67.9

Localization

Oquab

et.
al.
[34]
90.3
77.4
77.4
79.2
41.1
87.8
66.4
91.0
47.3
83.7

55.1

RCNN∗

[18]

92.0
80.8
80.8
73.0
49.9
86.8
77.7
87.6
50.4
72.1

57.6

Fast-RCNN∗

[17]

79.2
74.7
74.7
65.8
39.4
82.3
64.8
85.7
54.5
77.2

58.8

airplane

bike
bird
boat
bottle

bus
car
cat

chair
cow

dining
table
dog

horse

motorbike

88.8
93.6
85.2
87.4
43.5
86.2
50.8
86.8
66.5
74.5

person
plant
sheep
sofa
train

tv

mAP

83.5
85.5
84.3
88.1
80.1
83.5
64.5
90.8
81.4
79.8

90.4
90.0
90.0
91.6
85.5
90.4
75.5
91.4
89.6
86.5

82.2
84.1
83.4
83.9
71.7
83.1
63.7
89.4
78.2
77.1

88.5
89.0
88.1
78.5
64.1
90.0
67.0
89.9
82.6
79.7

85.1
86.1
80.5
76.6
46.7
79.5
68.3
85.0
60.0
71.3
Table 1. Comparison of Image classiﬁcation and Object Localization scores on the
PASCAL VOC 2012 validation set. For computing localization scores, responses are
labeled as correct when the maximal responses fall within a ground-truth bounding
box of the same class. False negatives are counted when no responses overlap with the
ground-truth annotations. The class scores of the associated image-level classiﬁcation
are used to rank the responses and generate average precision scores. * RCNN and
Fast-RCNN are trained for object detection with object-level bounding box data. We
use the most conﬁdent bounding box per class in every image for evaluation.

82.9
79.1
89.8
88.1
56.1
83.5
50.1
82.0
76.6
74.8

is mainly because the proposed method actively seeks out image regions trig-
gering higher classiﬁcation scores for the class of interest. This form of active
learning, where the localizing algorithm is the weak learner and the classiﬁer
is the strong teacher, lends us an advantage when trying to localize objects in
complex scenes where multiple objects can exist in varying mutual conﬁgura-
tions. This is also observed for the PASCAL VOC 2012 data-set. The ﬁne-tuned
VGG-16 and Alexnet networks produce classiﬁcation performance scores of 74.3
mAP and 82.4 mAP respectively on the test set, where as the network used by
[34] is scored at 86.3 mAP. As noted before, the proposed method outperforms
competing methods on the localization task.

We have provided results on object bounding box detection for the PASCAL
VOC 2007 test set in Table 3. We ﬁne-tuned our network on the VOC 2007 train
and the validation set, where 10% of this joint group of images was set aside for
parameter tuning, and provide test results on the test set. We are comparable in
performance with respect to other state-of-the-art weakly supervised methods
[8], [2] and [48]. Examples of visual results for object detection are provided in
Figure 6. We have also compared with the detection performance of the proposed
method with results from [34] on the VOC 2012 validation set, where we trained
the classiﬁer on the train set. We demonstrate a marked improvement in mAP
scores.

Weakly Supervised Localization using Deep Feature Maps

15

Re-scoring the class likelihood scores using co-occurrence information ref-
erenced in equation 3 contributes to an improvement of 1.2 with the VGG-16
network in classiﬁcation mAP score and 0.8 localization mAP score from Table
1.

Method
Oquab et. al. [34]
Proposed Method + VGG-16

Localization score (mAP)

41.2
49.2

Table 2. Comparison of localization and classiﬁcation mAP scores for the MS COCO
validation set.

mAP
Method
22.4
Multi-fold MIL [8]
27.7
Bilen et. al. [2]
26.9
LCL-kmeans [48]
Proposed Method + VGG-16 25.7

Table 3. Comparison of mean average precision scores for Object Detection task on
the PASCAL VOC 2007 test set.

Method
mAP
Proposed Method + VGG 16
26.5
Oquab et. al. [34] + Selective Search [46] 11.7

Table 4. Comparison of mean average precision scores for Object Detection task on
the PASCAL VOC 2012 validation set.

5 Discussion and Conclusions

The proposed method requires 2.6 sec to localize an object on an image on ma-
chine with a 2.3 GHz CPU with a NVIDIA TITAN X desktop GPU. Compared
to region proposal-based detection methods like RCNN which take around 20
seconds to detect objects, we achieve a signiﬁcant reduction in localization time.
As can be seen from Table 1, an improvement in the classiﬁcation perfor-
mance (e.g. from Alexnet to VGG-16) directly leads to an improvement in the
localization performance. As the state-of-the-art of the classiﬁcation CNNs im-
proves, we can expect a similar improvement in localization performance from
our proposed method.

16

Bency, Kwon, Lee, Karthikeyan, Manjunath

In summary, this method directly leverages feature map activations for ob-
ject localization. This work uses the spatial and semantic information encoded
in the convolutional layers and we have explored methods to utilize activations
in the last convolutional layer. It would be interesting to see the improvements
that could be derived by combining coarser semantic and ﬁner localization in-
formation in earlier convolutional layers as well. Another direction to explore
would be combining fast super-pixel segmentation and localization candidates
from proposed method to improve detection performance.

The proposed method relies on weak supervision, with networks trained for
image classiﬁcation being used for localizing objects in test images with complex
scenes and hence opens up possibilities for extending object localization to new
object categories and image modalities without requiring expensive object-level
annotations.

Weakly Supervised Localization using Deep Feature Maps

17

References

1. Alexe, B., Deselaers, T., Ferrari, V.: Measuring the objectness of image windows.
Pattern Analysis and Machine Intelligence, IEEE Transactions on 34(11), 2189–
2202 (2012)

2. Bilen, H., Pedersoli, M., Tuytelaars, T.: Weakly supervised object detection with
convex clustering. In: Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition. pp. 1081–1089 (2015)

3. Blaschko, M., Vedaldi, A., Zisserman, A.: Simultaneous object detection and rank-
ing with weak supervision. In: Advances in neural information processing systems.
pp. 235–243 (2010)

4. Brox, T., Bourdev, L., Maji, S., Malik, J.: Object segmentation by alignment of
poselet activations to image contours. In: Computer Vision and Pattern Recogni-
tion (CVPR), 2011 IEEE Conference on. pp. 2225–2232. IEEE (2011)

5. Chatﬁeld, K., Simonyan, K., Vedaldi, A., Zisserman, A.: Return of the devil in the
details: Delving deep into convolutional nets. In: British Machine Vision Conference
(2014)

6. Chen, L.C., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L.: Semantic
image segmentation with deep convolutional nets and fully connected crfs. In:
ICLR (2015)

7. Chum, O., Zisserman, A.: An exemplar model for learning object classes. In: Com-
puter Vision and Pattern Recognition, 2007. CVPR’07. IEEE Conference on. pp.
1–8. IEEE (2007)

8. Cinbis, R.G., Verbeek, J., Schmid, C.: Multi-fold mil training for weakly supervised
object localization. In: Computer Vision and Pattern Recognition (CVPR), 2014
IEEE Conference on. pp. 2409–2416. IEEE (2014)

9. Dai, J., He, K., Sun, J.: Convolutional feature masking for joint object and stuﬀ
segmentation. In: Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition. pp. 3992–4000 (2015)

10. Dalal, N., Triggs, B.: Histograms of oriented gradients for human detection. In:
Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer
Society Conference on. vol. 1, pp. 886–893. IEEE (2005)

11. Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A large-scale
hierarchical image database. In: Computer Vision and Pattern Recognition, 2009.
CVPR 2009. IEEE Conference on. pp. 248–255. IEEE (2009)

12. Deselaers, T., Alexe, B., Ferrari, V.: Localizing objects while learning their ap-

pearance. In: Computer Vision–ECCV 2010, pp. 452–466. Springer (2010)

13. Everingham, M., Van Gool, L., Williams, C.K.I., Winn, J., Zisserman, A.:
The PASCAL Visual Object Classes Challenge 2007 (VOC2007) Results.
http://www.pascal-network.org/challenges/VOC/voc2007/workshop/index.html

14. Everingham, M., Van Gool, L., Williams, C.K.I., Winn, J., Zisserman, A.:
The PASCAL Visual Object Classes Challenge 2012 (VOC2012) Results.
http://www.pascal-network.org/challenges/VOC/voc2012/workshop/index.html

15. Felzenszwalb, P.F., Girshick, R.B., McAllester, D., Ramanan, D.: Object detection
with discriminatively trained part-based models. Pattern Analysis and Machine
Intelligence, IEEE Transactions on 32(9), 1627–1645 (2010)

16. Galleguillos, C., Babenko, B., Rabinovich, A., Belongie, S.: Weakly supervised
object localization with stable segmentations. In: Computer Vision–ECCV 2008,
pp. 193–207. Springer (2008)

18

Bency, Kwon, Lee, Karthikeyan, Manjunath

17. Girshick, R.: Fast R-CNN. In: International Conference on Computer Vision

(ICCV) (2015)

18. Girshick, R., Donahue, J., Darrell, T., Malik, J.: Rich feature hierarchies for accu-
rate object detection and semantic segmentation. In: Computer Vision and Pattern
Recognition (CVPR), 2014 IEEE Conference on. pp. 580–587. IEEE (2014)

19. Glasner, D., Galun, M., Alpert, S., Basri, R., Shakhnarovich, G.: Viewpoint-aware
object detection and pose estimation. In: Computer Vision (ICCV), 2011 IEEE
International Conference on. pp. 1275–1282. IEEE (2011)

20. Hariharan, B., Arbel´aez, P., Girshick, R., Malik, J.: Simultaneous detection and

segmentation. In: Computer Vision–ECCV 2014, pp. 297–312. Springer (2014)

21. Hariharan, B., Arbelaez, P., Girshick, R., Malik, J.: Hypercolumns for object seg-
mentation and ﬁne-grained localization. In: The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR) (June 2015)

22. Hartmann, G., Grundmann, M., Hoﬀman, J., Tsai, D., Kwatra, V., Madani, O., Vi-
jayanarasimhan, S., Essa, I., Rehg, J., Sukthankar, R.: Weakly supervised learning
of object segmentations from web-scale video. In: Computer Vision–ECCV 2012.
Workshops and Demonstrations. pp. 198–208. Springer (2012)

23. He, K., Zhang, X., Ren, S., Sun, J.: Spatial pyramid pooling in deep convolutional
networks for visual recognition. Pattern Analysis and Machine Intelligence, IEEE
Transactions on 37(9), 1904–1916 (2015)

24. Jia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J., Girshick, R., Guadar-
rama, S., Darrell, T.: Caﬀe: Convolutional architecture for fast feature embedding.
In: Proceedings of the ACM International Conference on Multimedia. pp. 675–678.
ACM (2014)

25. Karayev, S., Trentacoste, M., Han, H., Agarwala, A., Darrell, T., Hertzmann, A.,
Winnemoeller, H.: Recognizing image style. In: Proceedings of the British Machine
Vision Conference. BMVA Press (2014)

26. Kim, J., Grauman, K.: Shape sharing for object segmentation. In: Computer

Vision–ECCV 2012, pp. 444–458. Springer (2012)

27. Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classiﬁcation with deep con-
volutional neural networks. In: Advances in neural information processing systems.
pp. 1097–1105 (2012)

28. Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll´ar, P.,
Zitnick, C.L.: Microsoft coco: Common objects in context. In: Computer Vision–
ECCV 2014, pp. 740–755. Springer (2014)

29. Long, J., Shelhamer, E., Darrell, T.: Fully convolutional networks for semantic
segmentation. In: Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition. pp. 3431–3440 (2015)

30. Mahendran, A., Vedaldi, A.: Understanding deep image representations by invert-
ing them. In: The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR) (June 2015)

31. Murphy, K., Torralba, A., Eaton, D., Freeman, W.: Object detection and localiza-
tion using local and global features. In: Toward Category-Level Object Recognition,
pp. 382–400. Springer (2006)

32. Noh, H., Hong, S., Han, B.: Learning deconvolution network for semantic seg-
mentation. In: Computer Vision (ICCV), 2015 IEEE International Conference on
(2015)

33. Oquab, M., Bottou, L., Laptev, I., Sivic, J.: Learning and transferring mid-level

image representations using convolutional neural networks. In: CVPR (2014)

Weakly Supervised Localization using Deep Feature Maps

19

34. Oquab, M., Bottou, L., Laptev, I., Sivic, J.: Is object localization for free? weakly-
supervised learning with convolutional neural networks. In: Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition (2015)

35. Ouyang, W., Wang, X., Zeng, X., Qiu, S., Luo, P., Tian, Y., Li, H., Yang, S.,
Wang, Z., Loy, C.C., Tang, X.: Deepid-net: Deformable deep convolutional neural
networks for object detection. In: The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR) (June 2015)

36. Papadopoulos, D.P., Clarke, A.D., Keller, F., Ferrari, V.: Training object class
detectors from eye tracking data. In: Computer Vision–ECCV 2014, pp. 361–376.
Springer (2014)

37. Pourian, N., Vadivel, K.S., Manjunath, B.: Weakly supervised graph based seman-
tic segmentation by learning communities of image-parts. In: Computer Vision
(ICCV), 2015 IEEE International Conference on. IEEE (2015)

38. Roth, P.S., Winter, M.: Survey of appearance-based methods for object recognition

(2008), iCGTR01/08

39. Rubin, S.M., Reddy, R.: The locus model of search and its use in image interpre-

tation. IJCAI 2 (1977)

40. Rumelhart, D.E., Hinton, G.E., Williams, R.J.: Learning representations by back-

propagating errors. Cognitive modeling 5, 3 (1988)

41. Sermanet, P., Eigen, D., Zhang, X., Mathieu, M., Fergus, R., LeCun, Y.: Overfeat:
Integrated recognition, localization and detection using convolutional networks. In:
International Conference on Learning Representations (ICLR 2014). CBLS (April
2014), http://openreview.net/document/d332e77d-459a-4af8-b3ed-55ba

42. Shanmuga Vadivel, K., Ngo, T., Eckstein, M., Manjunath, B.: Eye tracking assisted
extraction of attentionally important objects from videos. In: The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR) (June 2015)

43. Shrivastava, A., Gupta, A.: Building part-based object detectors via 3d geometry.

pp. 1745–1752. IEEE (2013)

44. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale im-
age recognition. In: International Conference on Learning Representations (2015)
45. Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D.,
Vanhoucke, V., Rabinovich, A.: Going deeper with convolutions. In: Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 1–9
(2015)

46. Uijlings, J.R., van de Sande, K.E., Gevers, T., Smeulders, A.W.: Selective search
for object recognition. International journal of computer vision 104(2), 154–171
(2013)

47. Viola, P., Jones, M.: Rapid object detection using a boosted cascade of simple
features. In: Computer Vision and Pattern Recognition, 2001. CVPR 2001. Pro-
ceedings of the 2001 IEEE Computer Society Conference on. vol. 1, pp. I–511.
IEEE (2001)

48. Wang, C., Ren, W., Huang, K., Tan, T.: Weakly supervised object localization with
latent category learning. In: Computer Vision–ECCV 2014, pp. 431–445. Springer
(2014)

49. Zhang, X., Yang, Y.H., Han, Z., Wang, H., Gao, C.: Object class detection: A
survey. ACM Comput. Surv. 46(1), 10:1–10:53 (Jul 2013), http://doi.acm.org/
10.1145/2522968.2522978

50. Zhang, Y., Sohn, K., Villegas, R., Pan, G., Lee, H.: Improving object detection with
deep convolutional networks via bayesian optimization and structured prediction.
In: The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
(June 2015)

20

Bency, Kwon, Lee, Karthikeyan, Manjunath

51. Zhou, B., Khosla, A., Lapedriza, A., Oliva, A., Torralba, A.: Object detectors
emerge in deep scene cnns. In: International Conference on Learning Representa-
tions (ICLR) (2015)

52. Zhu, Y., Urtasun, R., Salakhutdinov, R., Fidler, S.: segdeepm: Exploiting segmen-
tation and context in deep neural networks for object detection. In: The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR) (June 2015)

