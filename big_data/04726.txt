IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. X, NO. X, MARCH 2016

1

The SPURS Algorithm for Resampling an

Irregularly Sampled Signal onto a Cartesian Grid

Amir Kiperwas, Daniel Rosenfeld, Member, IEEE, and Yonina C. Eldar, Fellow, IEEE

6
1
0
2

 
r
a

 

M
6
1

 
 
]
T
I
.
s
c
[
 
 

2
v
6
2
7
4
0

.

3
0
6
1
:
v
i
X
r
a

Abstract—We present an algorithm for resampling a function
from its values on a non-Cartesian grid onto a Cartesian grid.
This problem arises in many applications such as MRI, CT,
radio astronomy and geophysics. Our algorithm, termed SParse
Uniform ReSampling (SPURS), employs methods from modern
sampling theory to achieve a small approximation error while
maintaining low computational cost. The given non-Cartesian
samples are projected onto a selected intermediate subspace,
spanned by integer translations of a compactly supported kernel
function. This produces a sparse system of equations describing
the relation between the nonuniformly spaced samples and a
vector of coefﬁcients representing the projection of the signal
onto the chosen subspace. This sparse system of equations can
be solved efﬁciently using available sparse equation solvers. The
result is then projected onto the subspace in which the sampled
signal is known to reside. The second projection is implemented
efﬁciently using a digital linear shift invariant (LSI) ﬁlter and
produces uniformly spaced values of the signal on a Cartesian
grid. The method can be iterated to improve the reconstruction
results.

We then apply SPURS to reconstruction of MRI data from
nonuniformly spaced k-space samples. Simulations demonstrate
that SPURS outperforms other reconstruction methods while
maintaining a similar computational complexity over a range
of sampling densities and trajectories as well as various input
SNR levels.

Index Terms—Nonuniform sampling, irregular sampling, gen-
eralized sampling, MRI reconstruction, sparse system solvers, LU
factorization, gridding, non-uniform FFT.

I. INTRODUCTION

R ECONSTRUCTION of a signal from a given set of

nonuniformly spaced samples of its representation in the
frequency domain is a problem encountered in a vast range
of scientiﬁc ﬁelds: radio astronomy, seismic and geophysical
imaging such as geophysical diffraction tomography (GDT)
and ground penetrating radar (GPR) [1], [2], SAR imaging [3]
and medical imaging systems including magnetic resonance
imaging (MRI), computerized tomography (CT) and diffrac-
tion ultrasound tomography [4].

In the last decades nonuniform sampling patterns have
become increasingly popular amongst MRI practitioners. In
particular, radial [5], [6] and spiral [7]–[9] trajectories allow
faster and more efﬁcient coverage of k-space, thereby reduc-
ing scan time and giving rise to other desirable properties
such as lower motion sensitivity [10], [11]. Other notable

A. Kiperwas and Y. C. Eldar are with the Department of Electrical

Engineering, Technion, Israel Institute of Technology

D. Rosenfeld is with RAFAEL Advanced Defense Systems Ltd., Israel.
This research was supported by a grant from RAFAEL Advanced Defense

Systems Ltd., Israel.

Manuscript received Month XX, 2016; revised Month XX, 2016.

non-Cartesian sampling patterns in MRI are stochastic [12]
and rosette [13], [14] trajectories which beneﬁt from less
systematic shifting or blurring artifacts. A popular approach
for recovering the original image is to resample the signal
on a Cartesian grid in k-space and then use the inverse fast
Fourier transform (IFFT) in order to transform back into the
image domain. It has been shown [15] that this approach is
advantageous in terms of computational complexity.

In MRI,

the most widely used resampling algorithm is
convolutional gridding [16], [17], which consists of four
steps: 1) precompensation for varying sampling density; 2)
convolution with a Kaiser-Bessel window onto a Cartesian
grid; 3) IFFT; and 4) postcompensation by dividing the image
by the transform of the window.

Two other notable classes of resampling methods em-
ployed in medical imaging are the least-squares (LS) and
the nonuniform-FFT (NUFFT) algorithms. LS techniques, in
particular URS/BURS [18], [19] are methods for calculating
the LS solution for the equation describing the relationship
between the acquired nonuniformly spaced k-space samples
and their uniformly spaced counterparts, as given by the
standard sinc-function interpolation of the sampling theorem.
These methods invert this relationship using the regularized
pseudoinverse by means of a singular value decomposition.
Finding a solution to problems of common sizes using URS
is computationally intractable. BURS offers an approximate
tractable solution to the LS problem.

The NUFFT [20]–[22] is a computationally efﬁcient family
of algorithms for approximating the Fourier transform, its
inverse and its transpose of a function sampled on a Cartesian
grid in one domain onto non-Cartesian locations in the other
domain. A nonuniform Fourier matrix A [23] is approxi-
mated efﬁciently by performing the following three operations
consecutively: 1) Pre-compensation/weighting of the samples
taken on the Cartesian grid; 2) FFT/IFFT onto an oversampled
Cartesian grid; 3) interpolation from this uniform grid to the
nonuniform sample locations using a compactly supported
interpolation kernel. The Hermitian adjoint of A, denoted A∗,
which is approximated by performing the Hermitian conjugate
of operations 1–3 in reverse order1, is used along with A
to solve the inverse problem — transforming from the non-
Cartesian onto the Cartesian grid. This is usually performed
using variants of the conjugate gradient method which operates
with A and A∗ alternately until convergence.

In recent years the concepts of sampling and reconstruction

1It can be shown that the operation performed by convolutional gridding is

equivalent to A∗.

IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. X, NO. X, MARCH 2016

2

have been generalized within the mathematical framework
of function spaces [24]–[26]. Methods were developed for
reconstructing a desired signal, or an approximation of this
signal, beyond the restrictions of the classic Shannon-Nyquist
sampling theorem.

In this paper we apply these concepts to the reconstruction
of a function from non-uniformly spaced samples in the spatial
frequency domain. Resampling is performed onto a Cartesian
grid in a computationally efﬁcient manner while maintaining
a low reconstruction error. First, the given non-Cartesian sam-
ples are projected onto an intermediate subspace, spanned by
integer translations of a compactly supported kernel function.
A sparse system of equations is produced which describes
the relation between the nonuniformly spaced samples and a
vector of coefﬁcients representing the projection of the signal
onto the auxiliary subspace. This sparse system of equations is
then solved efﬁciently using available sparse equation solvers.
The result is next projected onto the subspace in which the
sampled signal is known to reside. The second projection is
implemented efﬁciently using a digital linear shift invariant
(LSI) ﬁlter to produce uniformly spaced values of the signal
on a Cartesian grid in k-space. Finally, the uniform samples are
inverse Fourier transformed to obtain the reconstructed image.
termed SParse Uniform ReSampling
(SPURS) allows handling large scale problems while maintain-
ing a small approximation error at a low computational cost.
We demonstrate that the reconstruction error can be traded
off for computational complexity by controlling the kernel
function spanning the auxiliary subspace and by oversampling
the reconstruction grid.

algorithm,

Our

These methods are applied to the problem of MR image
reconstruction from nonuniformly spaced measurements in k-
space. The SPURS algorithm is compared using numerical
simulations with other prevalent reconstruction methods in
terms of its accuracy, its computational burden and its behavior
in the presence of noise. The results demonstrate that a single
iteration of SPURS outperforms other reconstruction methods
while maintaining a similar computational complexity over a
range of sampling densities and trajectories as well as various
input SNR levels. Iterating SPURS yields further improvement
of the reconstruction result and allows for faster trajectories
employing less sampling points.

We provide a freely available package [27], which con-
tains Matlab (The MathWorks, Inc., Natick, MA, USA) code
implementing the SPURS algorithm along with examples
reproducing some of the results presented herein.

This paper is organized as follows. Section II introduces
generalized sampling methods which are employed through-
out the paper. Section III formulates the non-Cartesian MRI
resampling problem. In Section IV the basic SPURS algorithm
is presented and then extended in Section V. Numerical
simulations and their results are provided in Section VI and
further discussed in Section VII. We conclude in Section VIII.

II. GENERALIZED SAMPLING METHODS

This section reviews some concepts and methods which
generalize the classic approach to sampling and reconstruction
of signals and are used throughout the paper.

(a)

(b)

Fig. 1. Geometrical interpretation. (a) An oblique projection in a perfect
reconstruction scenario; (b) The SPURS scheme.

Unless noted, the notations in this paper are given for a 1D
problem; the extension to higher dimensions is straightforward
using separable functions.

In the classic approach to signal sampling a signal ˆf is
represented by measurements which are its values at given
sampling points. In recent years [24], [26], [28], [29] this
idea was extended and generalized within a function-space
framework. The processes of sampling and reconstruction can
be viewed as an expansion of a signal onto a set of vectors
that span a signal subspace A of a Hilbert space H:

d [n] an = Ad,

(1)

ˆf =(cid:88)n

where d ∈ (cid:96)2, and A : (cid:96)2 → H is a set transform corre-
sponding to a set of vectors {an} which span the subspace A
and constitute a Riesz basis or a frame. Thus, applying A is
equivalent to taking linear combinations of the set of vectors
{an}. Measurements are expressed as inner products of the
function ˆf with a set of vectors {sm} that span the sampling
subspace S ⊆ H. Using this notation, the vector of samples
b is given by b = S∗ ˆf where b [m] = (cid:104)sm, ˆf(cid:105) and S∗ is the
adjoint of S. Note that knowing the samples b [m] is equivalent
to knowing the orthogonal projection of ˆf onto S, denoted by
ˆfS:

ˆf = S(S∗S)−1S∗ ˆf = S(S∗S)−1b,

ˆfS = PS

where

PS = S(S∗S)−1S∗,

is the orthogonal projection operator deﬁned by its range space
R (PS ) = S and its null space N (PS ) = R (PS )⊥.
A standard sampling problem is to reconstruct a signal ˆf ∈
A from its vector of samples b = S∗ ˆf. Geometrically, this
amounts to ﬁnding a signal in A with the projection ˆfS onto
S (see Fig. 1(a)). In order to be able to reconstruct any ˆf in A
from samples in S it is required that A and S⊥ intersect only
at zero. Otherwise, any non-zero signal in the intersection of
A and S⊥ will yield zero samples and cannot be recovered.
For a unique solution we also need A and S to have the same
numbers of degrees of freedom. These two requirements are
fulﬁlled by the direct-sum condition
H = A ⊕ S⊥,

(2)
which implies that A and S⊥ are disjoint, and together span
the space H.
The reconstructed signal ˆg is constructed to lie in the signal
subspace A. Any signal ˆg ∈ A can be represented by ˆg = A˜d,

S⊥ASˆf=ˆg=EAS⊥ˆfPSˆfQS⊥ASˆg=PAEQS⊥ˆfPSˆfˆfQ=EQS⊥ˆfˆfIEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. X, NO. X, MARCH 2016

3

where ˜d ∈ (cid:96)2. Restricting attention to linear recovery methods,
we can write ˜d = Hb for some transformation H : (cid:96)2 → (cid:96)2,
such that

ˆg = A˜d = AHb = AHS∗ ˆf = AHS∗Ad,

where we use (1). Perfect reconstruction means that ˆg = ˆf.
Our problem then reduces to ﬁnding H which satisﬁes

ˆg = AHS∗Ad = Ad = ˆf

for any ˆf ∈ A, i.e., for any choice of d. It is easily seen that
choosing H = (S∗A)−1 [30] satisﬁes this equation, where (2)
ensures that the inverse exists. In this case,

ˆg = A(S∗A)−1S∗ ˆf .

(3)
The operator in (3) is the oblique projection [31] onto A
(4)

along S⊥:

EAS⊥ = A(S∗A)−1S∗.

An operator E is a projection if it satisﬁes E2 = E. The
oblique projection operator (4) is a projection operator that
is not necessarily Hermitian. The notation EAS⊥ denotes an
oblique projection with range space R (EAS⊥ ) = A and null
space N (EAS⊥ ) = S⊥. If A = S, then EAS⊥ = PA. A
geometric interpretation of the perfect reconstruction scheme
of (3) is illustrated in Fig. 1(a).

When (2) is not satisﬁed, we use the Moore-Penrose
pseudoinverse [32], denoted by (S∗A)†. When (S∗A)† is
invertible, we have (S∗A)† = (S∗A)−1. For the sake of
generality, we shall use the pseudoinverse henceforth.

A desired property of a reconstructed signal ˆg is that it obeys
the consistency condition which requires that injecting ˆg back
into the system must result in the same measurements as the
original system itself, i.e., S∗ˆg = b. Even when the input does
not lie entirely in A, for example, due to mismodeling or noise
and regardless of S, the property S∗EAS⊥ = S∗ ensures that
ˆg is consistent.
The consistency principle may be employed to perform
reconstruction into a subspace, say Q, which differs from A.
In this case, perfect reconstruction can no longer be achieved.
Instead, we may seek a signal ˆfQ ∈ Q which satisﬁes the
consistency condition: S∗ ˆfQ = b. It is easily seen that the
desired ˆfQ can be obtained using EQS⊥, the oblique projection
onto Q along S⊥,

ˆfQ = Q(S∗Q)−1S∗ ˆf .

The consistency property of the oblique projection was intro-
duced in [24]. It was later extended in [26], [30], [33], [34]
to a broader framework, alongside a geometric interpretation
of the sampling and reconstruction schemes, and is employed
later on.

The generalized approach to sampling and reconstruction
allows for increased ﬂexibility in the recovery process. This
will be utilized in this paper to develop a computationally
efﬁcient implementation for the reconstruction of ˆf from a
given set of non-uniformly spaced k-space samples b, at the
cost of a small amount of approximation error.

III. THE MRI PROBLEM

An MRI image is represented by a gray level function
f (x), where x denotes the spatial coordinate in 2 or 3 spatial
dimensions. The Fourier transform of the image is denoted
ˆf (k), where k is the spatial frequency domain coordinate,
termed “k-space”:

ˆf (k) =

∞(cid:90)−∞

f (x) e−j2πkxdx.

The MRI tomograph collects a ﬁnite set of k-space raw
data samples { ˆf (κm)}, m = 1,··· , M. The set of sampling
points {κm} may be nonuniformly distributed in 2D or 3D
k-space. The vector of samples is denoted by b, with, b [m] =
(cid:104)sm, ˆf(cid:105) = ˆf (κm), where

sm(k) = δ (k − κm) .

(5)

The sampling subspace is denoted S = span{sm}.
The ﬁeld of view (FOV) in the image domain is limited,
which implies that the k-space function of the image, ˆf (k),
is spanned by a set of shifted sinc functions

an (k) = sinc (k/∆ − n) , n ∈ Z,

(6)

where FOV ∆= 1/∆. We denote the signal subspace by
A = span{an}. We seek a computationally efﬁcient solution
to the reconstruction problem: Given a set of nonuniformly
spaced k-space samples of an unknown image and the corre-
sponding sampling coordinates, ﬁnd a good approximation of
the function on a Cartesian grid in k-space from which we
can subsequently reconstruct an approximation of the image,
using the IFFT.

A straightforward approach to reconstruction is to employ
(3) within the framework described above which results in
perfect reconstruction of ˆf. It is easily shown that this solution
is equivalent to the URS scheme [18], [19] mentioned above.
In practical MRI scenarios, this solution requires inverting a
huge full matrix of sinc coefﬁcients which represents S∗A.
Storing this matrix on the computer, not to mention calculating
its inverse, is intractable due to the sheer size the matrices
involved. Instead, we suggest using an auxiliary subspace and
a series of two projections: an oblique projection onto the
auxiliary subspace followed by an orthogonal projection onto
the signal subspace. The ﬁrst projection is implemented by
solving a sparse system of equations whereas the second is
implemented using an LSI ﬁlter. The details are presented in
the following section.

IV. SPARSE UNIFORM RESAMPLING ALGORITHM

In this section we present the main ideas underlying our
reconstruction method as well as the detailed steps performed
by the algorithm. We also discuss the resulting approximation
error.

IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. X, NO. X, MARCH 2016

4

A. SPURS reconstruction

The straightforward reconstruction approach, performed by
implementation of EAS⊥ is computationally prohibitive for
typical MRI problems. Our algorithm, termed SParse Uni-
form ReSampling (SPURS) trades off reconstruction error
for computational complexity, i.e., perfect reconstruction is
sacriﬁced for the sake of efﬁciency, by relying on the notion
of consistency introduced in Section II.

The pivot of the new algorithm is an interim subspace Q
which is designed to enable efﬁcient reconstruction of ˆf. We
choose Q as a shift invariant subspace spanned by a compactly
supported kernel, designed to be close to the signal subspace
A. The reconstruction process comprises of two projections.
The ﬁrst is an oblique projection onto Q, which recovers a
consistent approximation of ˆf in Q, denoted ˆfQ. Consistency
in this context implies that sampling ˆfQ with S∗ yields the
original samples b. The second projection is an orthogonal
projection onto A, which recovers the closest signal in A to the
signal ˆfQ. It will be shown that the introduction of the interim
subspace Q is instrumental to achieving low computational
complexity while keeping the approximation error at bay. This
is achieved by ensuring that (S∗Q)† is easy to compute.

We begin by introducing an intermediate subspace Q ∈ H
which is spanned by the set {qn}, comprising integer trans-
lations of a compactly supported function q (k), i.e.,

qn (k) = q (k/∆ − n) , n ∈ Z.

(7)

We seek a consistent reconstruction of ˆf in Q, represented
by ˆfQ = Qc which is given by an oblique projection onto Q
along S⊥, i.e.,

ˆfQ = Qc = Q(S∗Q)†b = Q(S∗Q)†S∗ ˆf = EQS⊥

ˆf .

Consistency in this context implies that sampling ˆfQ using S∗
produces the original samples: S∗ ˆfQ = b. As we show below,
the compact support of q (k) allows for efﬁcient computation
of ˆfQ. Choosing Q = A results in perfect reconstruction (3),
however, since A is spanned by a non-compact kernel (6) the
computational burden is prohibitive in practical scenarios.
We obtain c by formulating and solving the equation which
relates the nonuniform samples b to the coefﬁcient vector c:

b = S∗Qc.

(8)

Using c, which deﬁnes ˆfQ, and given the knowledge that ˆf ∈
A, we next project ˆfQ onto A. The closest solution in the L2
sense is an orthogonal projection of ˆfQ onto A, denoted PA.
Due to the fact that both A and Q are shift-invariant subspaces
ˆfQ can be calculated efﬁciently by employing an LSI ﬁlter,
PA
as discussed below.
Summarizing , the SPURS reconstruction process comprises

a sequence of two projections:

ˆg = A(A∗A)†A∗

Q(S∗Q)†S∗

ˆf .

(9)

(cid:124)

PA

(cid:123)(cid:122)

(cid:125)

(cid:124)

E

QS⊥

(cid:123)(cid:122)

(cid:125)

A geometrical interpretation of (9) is depicted in Fig. 1(b).

Let us split the sequence of operators in (9) into two steps.
First, given the vector of samples b, the vector of coefﬁcients
c is calculated by solving (8) in the least squares sense:

c = (S∗Q)†b.

(10)

Subsequently, the vector c is used to calculate the coefﬁcients
d, given by

d = (A∗A)†(A∗Q)c.

(11)

Reconstruction is
then given by ˆg = Ad. Here A,
S and Q are the set
transforms (1) corresponding to
an (k) = sinc (k/∆ − n), sm(k) = δ (k − κm) and qn (k) =
q (k/∆ − n).
We next address the practical implementation details of each
step, and show how the steps are implemented efﬁciently.

B. Projection onto the subspace Q
for sm(k) deﬁned in (5):

In order to calculate c let us ﬁrst formulate (8) explicitly

b [m] =(cid:80)n

c [n] q (κm − kn),

(12)

where we are given the locations in k-space of the nonuni-
formly distributed sampling points {κm} as well as the Carte-
sian reconstruction locations {kn = ∆n}. Due to the compact
support of the function q, only a small number of coefﬁcients
c [n] in (12) contribute to the calculation of each value b [m].
Therefore, (12) represents a sparse relation between the coef-
ﬁcient vectors b and c, which can be expressed by an M × N
sparse matrix Φ, with elements

{Φ}m,n = {S∗Q}m,n = q (κm − kn) ,

(13)

where M and N are the number of coefﬁcients in the vectors
b and c, respectively.

In order to ﬁnd the vector c, we formulate a weighted

regularized least squares problem

+ ρ(cid:107)c(cid:48)(cid:107)2,

(14)

c = arg min

c(cid:48) (cid:13)(cid:13)¯Γ (b − Φc(cid:48))(cid:13)(cid:13)2

1

where ρ > 0 is a Tikhonov regularization parameter [35],
¯Γ = Γ
2 and Γ is an M × M diagonal weighting matrix with
weights wi > 0. The regularization is required in order to
prevent overﬁtting and to cope with the possible ill-posedness
of the problem which is common in real life situations where
the samples are contaminated by measurement noise. The
weights, wi, may contribute in cases when the noise density
varies in k-space and can improve the numerical stability when
facing challenging sampling patterns.

By taking the derivative of (14) we obtain the well known

normal equations,

(cid:0)ΦT ΓΦ + ρI(cid:1) c = ΦT Γb.

To solve (15) we note that although Φ is sparse, there is no
guarantee regarding the sparsity of ΦT ΓΦ. In fact, it could
easily become a full matrix. A useful sparsity conserving for-
mulation of the normal equations is given by the sparse tableau
approach [36] also referred to as the Hachtel augmented matrix
method [37]. We extend this formulation to accommodate for

(15)

IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. X, NO. X, MARCH 2016

5

the weights and the regularization. By deﬁning a residual term
r = ¯Γ (b − Φc) we reformulate (15) as
(cid:18) ¯Γb
ΦT ¯ΓT −ρI (cid:19)(cid:18) r
0 (cid:19) =(cid:18)

c (cid:19) = Ψ(cid:18) r
c (cid:19) .

¯ΓΦ

I

(16)

In this formulation Ψ maintains the sparsity of Φ.

The solution of this system of equations by means of
directly inverting Ψ is of complexity O((M × N )3) and
easily becomes computationally prohibitive; so is the amount
of computer memory required to store the non-sparse matrix
Ψ−1 which is of order O((M ×N )2). Moreover, even if Ψ−1
were known, it would still require O((M × N )2) operations to
compute c from b in (16). Instead, sparse equation solvers are
employed to calculate the LU factors of Ψ. This factorization
reduces both the memory requirements and the computational
effort employed for the solution to the order of O (NNZ (Ψ))
[38], [39], where NNZ (Ψ) is the number of non-zero elements
in the sparse matrix Ψ and NNZ (Ψ) (cid:28) (M × N ) (see
Section V-D). This process enables a computationally efﬁcient
solution of (16), which gives us c.

In practice (16) is solved in two steps: In the ﬁrst step,
the sparse solver package UMFPACK [40], [41] is used to
calculate the LU factorization of Ψ. In particular, the matrix
Ψ is factored as:

P(cid:0)R−1Ψ(cid:1) Q = LU,

where P and Q are permutation matrices, R is a diagonal
scaling matrix which helps achieving a sparser and more stable
factorization, and L, U are lower and upper triangular matrices
respectively. For further details refer to [42]. It is important to
emphasize that the factorization process is performed ofﬂine
only once for a given sampling pattern or trajectory deﬁned by
the set of sampling locations {κm}. The LU factors maintain
the sparsity of Ψ up to a small amount of zero ﬁll-in, and
can be stored for later use with a new sampling data set taken
over the same trajectory.

In the second step, given a set of samples b, calculation of
c using L and U is done by means of forward substitution
and backward elimination, operations which typically achieve
a memory usage and computational complexity which is linear
in the number of non zero (NNZ) elements of the sparse L,
U matrices.

C. Calculation of the values of ˆf on a Cartesian grid

Once the vector of coefﬁcients c is calculated, we proceed to
compute the vector d in (11). Since both Q and A correspond
to integer shifts of a kernel function, Q and A are SI subspaces
and, therefore, (11) can be implemented efﬁciently using an
LSI ﬁlter [26]:

where

HLSI(cid:0)ejω(cid:1) =

RAQ(cid:0)ejω(cid:1)
RAQ(cid:0)ejω(cid:1) = DTFT{raq [n]} =(cid:88)n∈Z

RAA (ejω)

,

(17)

raq [n] e−jωn

Fig. 2. SPURS system block diagram with ﬁltering implemented as convo-
lution in k-space.

is the discrete-time Fourier transform (DTFT) of the sampled
correlation sequence

−∞

1

resulting in

raq [n] = (cid:104)a (k) , q (k + n∆)(cid:105) =(cid:90) ∞
RAQ(cid:0)ejω(cid:1) =
RAA(cid:0)ejω(cid:1) is
Q (ω) = CTFT{q (k/∆)} =(cid:90) ∞

∆(cid:88)n∈Z

A(cid:18) ω

∆ −

−∞

2πn

a (k) q (k + n∆)dk,

∆ (cid:19)Q(cid:18) ω

∆ −

2πn

∆ (cid:19) .

(18)
the

q (k/∆) e−jωkdk,

similarly deﬁned. Q (ω)

in (18)

is

continuous-time Fourier transform (CTFT) of q (k/∆)

where A (ω) is similarly deﬁned. Since ˆf resides in the spatial
frequency domain, the ﬁlter (17) is deﬁned in the spatial
domain — the image space, using the change of variables

ω →

2πx
FOV

= 2π∆x.

d is given by

which, by the convolution property of the DTFT, is equivalent
to

c [n] hLSI [n − k],

d [n] =(cid:88)k∈Z
D(cid:0)ejω(cid:1) = HLSI(cid:0)ejω(cid:1) C(cid:0)ejω(cid:1) ,
ˆg (k) = Ad =(cid:88)n

d [n] sinc (k/∆ − n).

where C(cid:0)ejω(cid:1) and D(cid:0)ejω(cid:1) are the DTFTs of c and d,

respectively. Once d is calculated, it is used to reconstruct
ˆg by

For kn = n∆, d [n] = ˆg (kn), which is a vector of the function
values on a Cartesian grid in k-space.

The reconstruction process is next completed by inverse
Fourier
transforming back into the image domain e =
IFFT{d}. The estimate of the uniformly sampled image is
then given by g (xn) = e[n], where

xn =

n =

FOV

N

n
∆N

, n ∈ [−N/2, N/2) ∩ Z.

(19)

The entire reconstruction process is depicted in Fig. 2, where

2π(cid:90) π

1

−π

HLSI(cid:0)ejω(cid:1) ejωndω.

hLSI [n] = IDTFT{HLSI} =
We note that rather than performing the ﬁltering operation
of (17) in k-space, we can employ the convolution property
of the Fourier transform and implement it as a point-wise
multiplication in the image domain following the IFFT, using
the values of the ﬁlter HLSI at the image grid coordinates

{xn}, i.e. HLSI(cid:0)ejω(cid:1)(cid:12)(cid:12) ω=2π∆xn as depicted in Fig. 3.

ˆf(k)d[n]b[m]c[n]e[n]k=κmg(xn)hLSI[n]IFFT(S∗Q)†sparsesystemIEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. X, NO. X, MARCH 2016

6

Input:

Fig. 3. SPURS system block diagram with ﬁltering implemented as point-
wise multiplication in the image domain.

D. SPURS algorithm summary

To summarize, the SPURS algorithm is divided into two
stages; an ofﬂine stage which is performed only once for
a given sampling trajectory, and an online stage which is
repeated for each new set of samples.
Phase 1 – Ofﬂine preparation and factorization: the

sparse tableau system matrix Ψ of (16) is prepared and
its LU factorization computed. See Algorithm 1.

Phase 2 – Online solution: The sparse system of equations
(16) is solved for a given set of k-space samples b. The
result is subsequently ﬁltered using the digital correc-

tion ﬁlter HLSI(cid:0)ejω(cid:1) of (17) producing the vector of

coefﬁcients d which represent estimates of the function
on the uniform reconstruction grid ˆg (kn). The result is
transformed to the image domain using the IFFT giving
g (xn). See Algorithm 2.
Algorithm outputs: d, g (xn).

Input:

• {κm, m = 1, ..., M}: nonuniform sampling grid.
• {kn = n∆, n = 1, ..., N}: uniform reconstruction grid.
• q (·): a compactly supported kernel (e.g. B-spline).
• Γ: a M × M diagonal weighting matrix, ¯Γ = Γ
2 .
• ρ > 0: a regularization parameter.

1

Algorithm:
1: Construct

the sparse M × N system matrix Φ, with

2: Construct the (M + N )×(M + N ) sparse tableau matrix:

I

¯ΓΦ

{Φ}m,n = q (κm − kn).
Ψ =(cid:18)

ΦT ¯ΓT −ρI (cid:19).
3: Factorize Ψ so that P(cid:0)R−1Ψ(cid:1) Q = LU.
RAQ(cid:0)ejω(cid:1)

HLSI(cid:0)ejω(cid:1) =

Output: HLSI(cid:0)ejω(cid:1) and L, U, P, Q, R.

RAA (ejω)

.

4: Calculate the LSI ﬁlter for use in the online stage:

Algorithm 1: SPURS - Ofﬂine preparation and factorization.

E. SPURS approximation error

We now analyze the error introduced by the SPURS method
and provide geometrical insight for this error using the concept
of angles between subspaces.

• L, U: sparse (M + N ) × (M + N ) lower and upper
diagonal matrices.
• P, Q, R: (M + N )× (M + N ) permutation and scaling
matrices.
• Γ: an M × M diagonal weighting matrix, ¯Γ = Γ
• b: an M × 1 vector of nonuniformly spaced k-space
sample values of ˆf, where b [m] = ˆf (κm).
• HLSI(cid:0)ejω(cid:1).
1: Construct the (M + N ) × 1 vector ˇb =(cid:18) ¯Γb
0 (cid:19) .

2 .

1

Algorithm:

2: Scale ˇb by R and permute the result using P.
Store it in the full length vector y = PR−1ˇb.

3: Solve Lz = y and Uw = z by forward substitution and

backward elimination.

4: Permute w by Q and store the results in the (M + N )×1
vector ˇc, where ˇc = Qw =(cid:18) r
c (cid:19).
5: Filter the N × 1 vector c using HLSI(cid:0)ejω(cid:1) and store the
6: Compute e = IFFT{d}.
Output: d and the image g (xn) = e[n].

results in d.

Algorithm 2: SPURS - Online solution.

We begin by deﬁning the reconstruction error introduced by

SPURS as

εSPURS( ˆf ) = ˆf − ˆg = ˆf − PAEQS⊥

ˆf .

Since ˆf is in A, PA

ˆf = ˆf and

εSPURS( ˆf ) = PA (I − EQS⊥ ) ˆf = PAES⊥Q

(20)
Note that when Q = A, PAEAS⊥ = PA and εSPURS( ˆf ) = 0.
We employ the concept of angles between closed subspaces
[24]:

ˆf .

cos (A,S) =

sin (A,S) =

inf

v∈A,(cid:107)v(cid:107)=1(cid:107)PS v(cid:107) ,
v∈A,(cid:107)v(cid:107)=1(cid:107)PS⊥ v(cid:107) .

sup

(21)

(22)

Proposition 1. Let A, S and Q be closed subspaces of a
Hilbert space H. Then

(cid:107)εSPURS( ˆf )(cid:107)2 ≤

sin2 (A,S)
cos2 (Q,S)(cid:107)PQ⊥

ˆf(cid:107)2

≤

sin2 (A,S)sin2 (A,Q)

cos2 (Q,S)

(cid:107) ˆf(cid:107)2.

(23)

ˆf /(cid:107)ES⊥Q

Proof: The error in (20) can be bounded by deﬁning
ˆf(cid:107) which is a normalized vector in S⊥.
v = ES⊥Q
Orthogonally projecting v onto A and using deﬁnition (22) we
get
(cid:107)PAv(cid:107)2 ≤ sin2(cid:0)S⊥,A⊥(cid:1) .
(24)

ˆf(k)b[m]c[n]k=κmg(xn)HLSI(cid:0)ej2π∆xn(cid:1)IFFT(S∗Q)†sparsesystem×IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. X, NO. X, MARCH 2016

7

obtained by the (p + 1)-fold convolution of of a rectangular
pulse β0 (k):

(25)

Plugging (24) into (20) we obtain

Using the deﬁnition in (21),

(cid:107)εSPURS( ˆf )(cid:107)2 ≤ sin2(cid:0)S⊥,A⊥(cid:1)(cid:107)ES⊥Q
cos(cid:0)S⊥,Q⊥(cid:1) ≤ (cid:107)PQ⊥ u(cid:107)
, ∀u ∈ S⊥\{0} .

ˆf(cid:107)2.

(cid:107)u(cid:107)
ˆf leads to
ˆf(cid:107)
ˆf(cid:107) ≤ (cid:107)PQ⊥ES⊥Q
cos (S⊥,Q⊥)

(cid:107)ES⊥Q

Choosing u = ES⊥Q

.

(26)

Substituting the algebraic expressions for PQ⊥ and ES⊥Q
we can immediately verify that PQ⊥ ES⊥Q = PQ⊥. Using
deﬁnition (22),

ˆf(cid:107)
sin (A,Q) ≥ (cid:107)PQ⊥
(cid:107) ˆf(cid:107)

, ∀ ˆf ∈ A\{0} .

Substituting these results into (26) we obtain,

(cid:107)ES⊥Q

ˆf(cid:107) ≤ (cid:107)PQ⊥

ˆf(cid:107)

sin (A,Q)
cos (S⊥,Q⊥)(cid:107) ˆf(cid:107).

(27)

Plugging (27) into (25) and using the relations [24]

cos (S⊥,Q⊥) ≤
cos (Q,S) = cos(cid:0)S⊥,Q⊥(cid:1)
sin (A,S) = sin(cid:0)S⊥,A⊥(cid:1) ,

completes the proof.

Proposition 1 implies that the reconstruction error is con-
trolled by the angle between the sampling subspace S and
signal subspace A, by the angle between the sampling sub-
ˆf(cid:107)2,
space S and the auxiliary subspace Q as well as by (cid:107)PQ⊥
which is the energy of the signal ˆf which does not reside in Q.
For the MRI problem formulated, we only have control over
Q, whereas A (determined by the FOV) and S (determined by
the sampling pattern) are predeﬁned. Therefore, it would be
ˆf(cid:107)2
preferable to maximize cos2 (Q,S) and minimize (cid:107)PQ⊥
over Q for a given computational budget2. This is done by an
appropriate selection of the kernel function q (k) which spans
Q (Section V-A) and by employing dense grid interpolation
as described in Section V-B.

V. EXTENSIONS OF SPURS
A. Selection of the kernel function spanning Q
The selection of the function q (k) which spans Q, both the
function itself and its support, has a considerable effect on
the quality of the reconstructed image. In this work we use
basis splines (B-splines [43]) which have gained popularity in
signal processing applications [44]. They are commonly used
in image processing because of their ability to represent efﬁ-
ciently smooth signals and the low computational complexity
needed for their evaluation at arbitrary locations. A B-spline of
degree p is a piecewise polynomial with the pieces combined
at knots, such that the function is continuously differentiable
p−1 times. A B-spline of degree p, denoted βp, is the function
2Otherwise, we would choose Q = A and achieve perfect reconstruction

at a high computational cost.

with a support of p + 1, where

βp (k) = β0 (k) ∗ . . . ∗ β0 (k)
(cid:125)

,

(p+1) times

(cid:123)(cid:122)
1, − 1
2 , |k| = 1
0, otherwise.

1

2

2 < k < 1

2

(cid:124)
β0 (k) =

Increasing the degree of the spline increases its order of
approximation and improves the image quality at the expense
of increased computational burden, as a result of the larger
support. It can be shown [44] that as the order of the spline
increases, the subspace Q tends to A, subsequently, decreasing
ˆf(cid:107) in (23). Similarly, it is well known that the selection
(cid:107)PQ⊥
inﬂuence on the
of the kernel function has a signiﬁcant
performance of both NUFFT [22] and convolutional gridding
[16], [17].

For q (k) = βp (k) and a (k) = sinc (k) the LSI ﬁlter (17)

can be expressed explicitly by evaluating

Q (ω) = CTFT(cid:26)βp(cid:18) k
A (ω) = CTFT(cid:26)sinc(cid:18) k

ω

∆(cid:19)(cid:27) = ∆sincp+1(cid:16)∆
2π(cid:17) ,
∆(cid:19)(cid:27) = ∆rect(cid:16)∆
2π(cid:17) ,
, sinc(ξ) =(cid:40) sin(πξ)

, ξ (cid:54)= 0
ξ = 0

1,

ω

πξ

.

Plugging Q (ω) and A (ω) into (18) results in

where,

2

2

rect(ξ) =(cid:26) 1, |ξ| < 1
0, |ξ| ≥ 1
HLSI(cid:0)ejω(cid:1) = (cid:80)n∈Z

2π − n(cid:1) sincp+1(cid:0) ω
rect(cid:0) ω
2π − n(cid:1)
rect(cid:0) ω
(cid:80)n∈Z

2π − n(cid:1)

.

(28)

The reconstruction result in the image domain is calculated
on the Cartesian grid {xn}, as deﬁned in (19). When perform-
ing the ﬁltering operation in the image domain, as illustrated
in Fig. 3, {g (xn)} is obtained by multiplying the IFFT result
of c with the values of HLSI(cid:0)ejω(cid:1) at locations ωn = 2π∆xn.
Since all {xn} as deﬁned in (19) are within the FOV reduces
(28) to
N(cid:17) , n ∈ [−N/2, N/2) ∩ Z.
HLSI(cid:0)ej2π∆xn(cid:1) = sincp+1(cid:16) n
The sparse matrix Φ deﬁned in (13), is given by {Φ}m,n =
βp (κm − kn), for a given set of sampling and reconstruction
coordinates. The number of non-zero elements in the matrix
Φ is a function of the support of the kernel function and the
number of samples M. For the 2D case this amounts to

NNZ (Φ) (cid:39) M π (p + 1)2 .

(29)

IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. X, NO. X, MARCH 2016

8

B. Dense grid interpolation

with

0,

HLSI(cid:0)ejω(cid:1) =(cid:40) RAQ(ejω)
RAA(ejω) , RAA(cid:0)ejω(cid:1) (cid:54)= 0
RAA(cid:0)ejω(cid:1) = 0
2π − σn(cid:1) sincp+1(cid:0) σω
rect(cid:0) σω
RAQ(cid:0)ejω(cid:1)
rect(cid:0) σω
2π − σn(cid:1)
(cid:80)n∈Z

= (cid:80)n∈Z

RAA (ejω)

,

2π − σn(cid:1)

,

We have seen above that

the intermediate subspace Q
introduces an error into the reconstruction process. The ap-
proximation error can be reduced by resampling onto a denser
uniform grid in k-space. This is done by scaling ∆ in (7)
by an oversampling factor σ > 1, i.e., ∆ → ∆/σ. The
oversampling increases N, the total number of cartesian re-
construction points in k-space, as well as the ﬁeld of view
reconstructed in the image domain, by a factor of σ for each
dimension of the problem, i.e., N → N σdim, where dim is the
problem dimension. Increasing σ reduces the approximation
error with a penalty of increasing the computational load.
From a geometric viewpoint increasing the density in the
reconstruction subspace Q, spanned by {q (k/∆ − n)}, causes
the subspace to become larger and consequently closer to A
and to S thereby decreasing the approximation error (23).
For σ > 1, the reconstruction ﬁlter (17) needs to be modiﬁed
accordingly:

where the image domain region beyond the original FOV is
set to 0.

It should be noted that both convolutional gridding and
NUFFT employ an oversampling factor σ to improve perfor-
mance at the expense of increased computational complexity
[45]. In most cases it was found sufﬁcient to use an oversam-
pling factor of σ = 2.

C. Iterating SPURS

Another way to improve the reconstruction results is to use
a simple iterative scheme. In a single iteration of SPURS we
obtain d which is a vector of coefﬁcients from which we can
reconstruct the continuous function ˆg (k) = Ad. By operating
with the sampling operator S∗ to resample the reconstructed
function ˆg (k) on the nonuniform grid we obtain ˜b = S∗ˆg,
which approximates the original set of samples b = S∗ ˆf. We
deﬁne an error vector εεε = ˜b− b. Achieving εεε = 0 means that
a function ˆg ∈ A has been found which is consistent with the
given vector of samples b.
In [46] it was proven that for a function ˆf, known to belong
to a class of spline-like spaces3 A, the exact reconstruction
of ˆf from its samples b [m] = ˆf (κm) can be achieved,
provided that the sampling set {κm} is “sufﬁciently dense”
[47], [48]. A reconstruction process was proposed and proven
to converge to ˆf by iteratively operating with an interpolator
and a bounded projector onto the spline-like space A. It
was noted that the interpolator can be generalized to any set
{qn = q (k − n)} which forms a bounded uniform partition
of unity, i.e.(cid:80)n q (k − n) = 1. The convergence of the result

3Bandlimited functions are a limiting case for this class.

Fig. 4.

Iterative SPURS algorithm block diagram.

to ˆf was proven among others in the Lp-norm and in the sup-
norm which implies uniform convergence. In this section we
utilize SPURS to employ a fast iterative algorithm which ﬁts
into the framework proposed in [46]. By iteratively operating
with EQS⊥ and PA from (9), and as long as the sampling set
is dense enough, ˆg converges to ˆf.
The ﬁrst step of the algorithm, operates on the vector
of samples b with the operator G = (A∗A)†A∗Q(S∗Q)†
to produce the vector of coefﬁcients d. This ﬁrst iteration
is designated d0 = Gb0 = Gb (i.e. b0 = b) which is
performed by (10) and (11). Using d0 we evaluate the function
ˆg0 = Ad0 which is the ﬁrst approximation of ˆf. Let us deﬁne
the continuous error function

εp = ˆf − ˆgp, εp ∈ A

(30)

which can be evaluated on the sampling points for each
iteration p
εp [m] = εp (κm) = f (κm) − ˆgp (κm) = {b − S∗Adp}m .
We now proceed to the second iteration. Using the error
vector εεε0, the new measurement vector b1 = b0 + αεεε0 is
calculated, where α controls the iteration step size, and d1 =
Gb1. Continuing the iterations leads to

b0 = b
dp = Gbp

bp+1 = bp + αpb −
(cid:124)

 .

˜bp

S∗Adp

(cid:122) (cid:125)(cid:124) (cid:123)
(cid:125)
(cid:123)(cid:122)

εεεp

The complete iterative process is depicted in Fig. 4.

According to [46], a sufﬁcient condition for convergence
of ˆgp to ˆf is that the sampling set {κm} is γ0-dense4, which
implies that the maximal distance between a sampling point
and its nearest neighbor is 2γ0. Moreover, for γ0 sufﬁciently
small, (cid:107)εp+1(cid:107) ≤ η (cid:107)εp(cid:107) where η < 1, therefore (cid:107)εp(cid:107) → 0.
From (30), (cid:107)εp(cid:107) → 0 is equivalent to ˆgp (k) → ˆf (k) for all k
as p → ∞. The contraction factor η is a decreasing function
of the density, which means that the algorithm converges more
rapidly for denser sets.

4 A set {κm} is γ0-dense in Rd if Rd = ∪

Bγ (κm) , ∀γ > γ0, where

Bγ is a ball of radius γ with center κm

m

dpbpcpepgp(xn)hLSI[n]IFFT(S∗Q)†sparsesystemS∗Abbp+1+−αp+Gεˆgp(kn)p˜bpIEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. X, NO. X, MARCH 2016

9

The scalar αp controls the iteration step size. For a constant
αp the convergence rate might be slow. In order to improve
the convergence rate αp may be chosen at each iteration
such that the norm of the error (cid:107)εεεp(cid:107) is minimized, where
εεεp = b − S∗Adp. The error progression between iterations is
εεεp+1 = (I − αpS∗AG) εεεp, leading to an optimal step

αp = arg min

α (cid:107)(I − αS∗AG) εεεp(cid:107)2 =

(εεεp)∗S∗AGεεεp
(cid:107)S∗AGεεεp(cid:107)2 .

In our simulations, presented in Section VI, we evaluated
the performance of SPURS both as a direct method and as an
iterative method.

D. Computational Complexity

The computational complexity of SPURS is proportional to
the number of non-zeros (NNZ) in the LU factors of Ψ which
is constructed according to (16) from Φ, Γ and ρ. The number
of non-zeros in the (M + N σ)×(M + N σ) matrix Ψ is twice
the NNZ of Φ as given in (29) for the case of 2D imaging and
a B-spline kernel, plus an additional M + N σdim non-zeros on
the main diagonal. This amounts to

NNZ (Ψ2D) (cid:39) M π((p + 1)σ/2)

2

+ M + N σ2,

dim

+ M + N σdim,

where for the more general case,
(31)
NNZ (Ψ) (cid:39) M πdim(supp (q)σ/2)
where M and N σdim are the number of non-Cartesian and
Cartesian grid points, respectively, dim is the problem dimen-
sion, πdim is a constant that depends on the dimension (for
dim = 2, πdim = π, for dim = 3, πdim = 4π/3 etc.), σ is the
oversampling factor and supp (q) is the support of the kernel
function q, e.g., for B-splines of degree p, supp (βp) = p + 1.
In practice, when Ψ is sparse, its L and U factors which are
used to recover c preserve a similar degree of sparsity, with
a certain increase in NNZ termed “ﬁll-in”. The computational
complexity of the forward and backward substitution stage is
O (NNZ (L + U)) which, despite the ﬁll-in, is of the same
order of magnitude as NNZ (Ψ) given in (31). Assuming
that the ﬁltering stage is performed in the image domain, it
adds a complexity of O (N ), whereas the IFFT stage adds a

N 1/dim pixels in each dimension. Thus, the online solution
phase of SPURS has computational complexity

complexity of O(cid:0)N σdim log(cid:0)N 1/dimσ(cid:1)(cid:1) for an image with
O(cid:16)M (supp (q)σ)

+ N σdim log(cid:16)N 1/dimσ(cid:17)(cid:17) .

It can be shown that (32) is comparable to that of convolutional
gridding or of a single iteration of the NUFFT.

(32)

dim

In the iterative scheme, an additional stage of calculating

operations to each iteration. Therefore, iterative SPURS has
computational complexity

˜bp = S∗Adp is performed. This adds O(cid:0)M × N σdim(cid:1)
+ N σdim(cid:16)M + log(cid:16)N 1/dimσ(cid:17)(cid:17)(cid:17)(33)
O(cid:16)M (supp (q)σ)
(33) is O(cid:0)M N σdim(cid:1). It is noteworthy to compare this to

per iteration. In practical situations, N and M are of a
similar order of magnitude, therefore, the leading term in

dim

the leading term of (32), O(cid:0)N σdim log(cid:0)N 1/dimσ(cid:1)(cid:1), which

is considerably smaller. The latter is also the leading term in
the complexity of convolutional gridding, rBURS or a single
iteration of NUFFT. Therefore, the improved performance ex-
hibited by additional iterations of SPURS comes with a certain
penalty in terms of the computational burden as compared with
a similar number of iterations of NUFFT.

VI. NUMERICAL SIMULATION

In this section we perform image reconstruction from nu-
merically generated k-space samples of analytical phantoms
and compare the performance of SPURS to that of other meth-
ods. The numerical experiments are implemented in Matlab
(The code is available online at [27]). Computer simulations
are used to compare the performance of SPURS with that
of convolutional gridding [16], rBURS [18], [19], and the
(inverse) NUFFT method [20] as implemented by the NFFT
package [49], [50] speciﬁcally using the application provided
for MRI reconstruction [51].

The NUFFT uses a Kaiser-Bessel window with cut-off
parameter m = 6 (i.e., support = 12), Voronoi weights
for density compensation, and oversampling factor of σ = 2.
Convolutional gridding uses the same parameters and is simply
implemented as a single iteration of NUFFT. In rBURS
δκ = 1.2, ∆k = 3 are used, with two values of oversampling
(σ = 1, 2, denoted rBURS and rBURSx2 respectively). Unless
speciﬁed otherwise the SPURS kernel used is a B-spline of
degree 3 (i.e., support = 4) with an oversampling factor of
σ = 2. The results for SPURS are presented for a single
iteration and for the iterative scheme.

The simulation employs a realistic analytical MRI brain

phantom [50], of dimensions 256 × 256, i.e., N = 65536.
Two types of sampling trajectories are demonstrated: radial
and spiral. The k-space coordinates for the radial trajectory
are given by:

(κx, κy)r,s = N(cid:18) r

Nbins − 0.5(cid:19) (cos ωs, sin ωs) ,

(34)

with

ωs =

πs

Nspokes

.

Here, Nspokes denotes the number of radial spokes, with s =
0, . . . , Nspokes − 1; Nbins is the number of sampling points
along each spoke, with r = 0, . . . , Nbins − 1. Thus, M =
Nspokes × Nbins.
The spiral trajectory comprises a single arm Archimedean
constant-velocity spiral with M sampling points along the
trajectory, and k-space coordinates given by:

N

2(cid:114) j

M

(κx, κy)j =

(cos ωj, sin ωj) , j = 0, . . . , M − 1,

density is approximately uniform.

where, ωj = 2π(cid:112)j/π ensures that the k-space sampling

White Gaussian noise (WGN) is added to the samples to
achieve a desired input signal to noise ratio (ISNR). For each
experiment the SNR of the reconstructed image is calculated
with respect to the true phantom image. The SNR measure

(35)

IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. X, NO. X, MARCH 2016

10

Fig. 5. SNR as a function of M for an analytical brain phantom sampled on
a radial trajectory with ISNR = 30 dB.

Fig. 6. MSSIM as a function of M for an analytical brain phantom sampled
on a radial trajectory with ISNR = 30 dB.

assesses the pixel difference between the true and the recon-
structed phantom image, and is deﬁned by

(f (xn))2

N(cid:80)n=1
(g (xn) − f (xn))2

[dB] ,

1
N

N(cid:80)n=1

SNR (g (xn) , f (xn)) = 10 log

1
N

where f (xn) are the pixel values of the original image and
g (xn) of the reconstructed image. The SNR measure does not
take into account structure in the image, and along with other
traditional methods such as PSNR and mean squared error
(MSE) have proven to be inconsistent with the human visual
system (HVS). The Structural Similarity (SSIM) index [52]
was designed to improve on those metrics. SSIM provides
a measure of the structural similarity between the ground
truth and the estimated images by assessing the visual impact
of three characteristics of an image: luminance, contrast and
structure. For each pixel in the image, the SSIM index is
calculated using surrounding pixels enclosed in a Gaussian
window with standard deviation 1.5:

SSIM(f (xn) , g (xn)) =

(2µf µg + c1)(2σf g + c2)
f + µ2

g + c1)(σ2

f + σ2

g + c2)

(µ2

,

where µf is the average of f (xn) in the Gaussian window,
f is the variance of f (xn) in the Gaussian window, σf g is
σ2
the covariance betwenn f (xn) and g (xn) in the Gaussian
window, and c1 and c2 are two variables to stabilize the
division with weak denominator. In our results we present
the mean of the SSIM value over the whole image, denoted
MSSIM.

In the ﬁrst experiment a radial trajectory is used (34), where
the value of Nbins = 512 and Nspokes is varied between
20 and 190, which results in M values between 10240 and
97280. Noise was added to the samples to achieve an ISNR
of 30 dB. Figures 5 and 6 present the SNR and MSSIM
of the reconstructed image as a function of M, the number
of sampling points in k-space. For reconstruction methods

Fig. 7. SNR as a function of M for an analytical brain phantom sampled on
a radial trajectory with no sampling noise added i.e. ISNR = ∞.

which can be iterated, results are shown for both a single
iteration (dashed line) and the ﬁnal result after the algorithm
has converged (solid line). The same experiment was repeated
with noiseless samples and is presented in Figures 7 and 8.

The second experiment is similar to the ﬁrst with a spiral
trajectory as described by (35). The number of samples M is
varied by increments of 5000 between 10000 and 85000 with
ISNR = 30dB. The results are presented in Figures 9 and 10.
Here too, the experiment was repeated with noiseless samples
and the results are presented in Figures 11 and 12.

Fig. 14 exhibits the reconstruction results with the spiral
trajectory with ISNR = 30dB for M = 30000. The recon-
structed images are displayed alongside proﬁle plots of row
113. The same is also presented in Figures 13 for M = 20000.
Figure 15 demonstrates the inﬂuence of the oversampling
factor σ and the degree of the B-spline kernel function on the
approximation error for the analytical brain phantom sampled

M×10412345678910SNR [dB]24681012141618rBURSrBURSx2C.GriddingNUFFTSPURS first it.SPURSM×10412345678910MSSIM0.20.30.40.50.60.70.80.9rBURSrBURSx2C.GriddingNUFFTSPURS first it.SPURSM×10412345678910SNR [dB]468101214161820rBURSrBURSx2C.GriddingNUFFTSPURS first it.SPURSIEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. X, NO. X, MARCH 2016

11

Fig. 8. MSSIM as a function of M for an analytical brain phantom sampled
on a radial trajectory with no sampling noise added i.e. ISNR = ∞.

Fig. 10. MSSIM as a function of M for an analytical brain phantom sampled
on a spiral trajectory with ISNR = 30 dB.

Fig. 9. SNR as a function of M for an analytical brain phantom sampled on
a spiral trajectory with ISNR = 30 dB.

Fig. 11. SNR as a function of M for an analytical brain phantom sampled
on a spiral trajectory with no sampling noise added i.e. ISNR = ∞.

on a spiral trajectory with M = 30000 and ISNR of 30 dB.
Figures 16 and 17 demonstrate the inﬂuence of the input
SNR on the reconstruction result for the analytical brain
phantom sampled on a Radial trajectory with M = 51200.
The same is presented for sampling done on a spiral trajectory
with M = 30000 in Figures 18 and 19, and with M = 60000
in Fig. 20. The ISNR value is varied between 0 dB and a
noiseless input (on the right hand side of the plot).

VII. DISCUSSION

The ﬁrst two experiments compare the performance of the
different reconstruction algorithms as a function of the number
of measurement points M. As expected, the performance of all
the methods deteriorates as the number of samples is reduced.
In general, better results were achieved for the case of spiral
sampling compared to radial sampling, as a function both of

the ISNR and of M. This is most likely due to the fact that the
spiral trajectory has a much more uniform density than that
of the radial trajectory which is over-sampled in the proximity
of the k space origin, with sampling density decreasing as the
distance from the k space origin increases.

Comparing the MSSIM and SNR values for high values of
M, it is easy to see that a single iteration of SPURS performs
better than all the other methods over the entire range of M
and ISNR values. It is important to notice how the performance
gap between SPURS and the other methods increases as the
ISNR decreases. For example, the gap between SPURS and
the other methods is larger in Fig. 6 (or Fig. 10) than in Fig. 8
(or Fig. 12). These results show that SPURS has better noise
performance then the other reconstruction method tested.

When sampling on a radial trajectory, there is a small perfor-
mance gain in iterating SPURS for the noisy case (Fig. 6) and
no gain for the noiseless case (Fig. 8). This means that SPURS

M×10412345678910MSSIM0.30.40.50.60.70.80.91rBURSrBURSx2C.GriddingNUFFTSPURS first it.SPURSM×10412345678910SNR [dB]468101214161820rBURSrBURSx2C.GriddingNUFFTSPURS first it.SPURSM×10412345678910MSSIM0.40.50.60.70.80.91rBURSrBURSx2C.GriddingNUFFTSPURS first it.SPURSM×10412345678910SNR [dB]46810121416182022rBURSrBURSx2C.GriddingNUFFTSPURS first it.SPURSIEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. X, NO. X, MARCH 2016

12

Fig. 12. MSSIM as a function of M for an analytical brain phantom sampled
on a spiral trajectory with no sampling noise added i.e. ISNR = ∞.

reaches its maximal or near-maximal performance within a
single iteration, as opposed to NUFFT which requires 6 to 10
iterations to reach its best reconstruction result, which is still
inferior to that achieved by a single iteration of SPURS. In the
noiseless setting, the quality measures for all the reconstruction
methods improve as M is increased, as is naturally expected.
On the other hand, when sampling noise is introduced, only
SPURS is able to beneﬁt from the extra samples and continues
to improve the MSSIM results for M > 45000 (Fig. 6).

When sampling on a spiral

trajectory, SPURS further
demonstrates its superior performance over the other methods.
For M values high enough, SPURS, NUFFT and rBURS with
σ = 2 achieve very good results, but the performance curve
for each method levels off for different values of M (Figures
9, 10, 11 and 12). Iterative SPURS levels off for values as
low as M = 20000, requiring about 10 iterations to converge
to its best result. For these low M values, signiﬁcant artifacts
appear in the reconstructed image produced by all methods
excluding SPURS as presented in Fig. 13 for M = 20000 and
Fig. 14 for M = 30000. The performance curve of the NUFFT
method and of a single iteration of SPURS level off at around
M = 50000. For M = 50000 and higher, a single iteration of
SPURS produces marginally better results than those produced
by NUFFT, which requires about 10 iterations to converge.
Among the other non-iterative methods, both rBURS with
σ = 2 and convolutional gridding perform similarly well for
M > 50000, however the results are still inferior to those
of a single iteration of SPURS, all of which have similar
computational complexity.

Similar trends are exhibited in Figures 16,17,18 and 19,
which demonstrate the noise performance at a given number
of sampling points M. It is shown once again that a single
iteration of SPURS outperforms the other methods and that
in some cases the results can be further improved by iterating
SPURS. Figure 20 shows the noise performance for a large
number of sampling points M = 60000. For high values
of ISNR, the performance of a single iteration of SPURS is

Fig. 13. Results for an analytic brain phantom sampled on a spiral trajectory
with M = 20000, ISNR = 30 dB. Right column: reconstructed images; left
column: proﬁle plots of row 113. Rows, from top to bottom, SNR and MSSIM
values in parenthesis: SPURS using β3 (18.09 dB, 0.79), NUFFT (8.47 dB,
0.55), BURS with σ = 2(8.32 dB, 0.54), BURS with σ = 1(9.95 dB, 0.61),
convolutional gridding (6.93 dB, 0.55).

similar to that of NUFFT, however, at low values of ISNR the
advantage of SPURS over the other methods is apparent.

Figure 15 shows the inﬂuence of the oversampling factor σ
and the support of the kernel function q on the performance
of SPURS. It can be seen that the SNR levels off at σ = 1.2.
Moreover, the degree of the B-spline has a relatively small
impact on the performance; in particular, even using a B-
spline of degree 1 (which has a support of 2 k-space samples,
and is equivalent to linear interpolation) incurs merely a 0.1

M×10412345678910MSSIM0.40.50.60.70.80.91rBURSrBURSx2C.GriddingNUFFTSPURS first it.SPURS-150-100-5005010015000.20.40.60.811.2GSURS, Brain, M=20000, N=65536 using Bspline(3), OverGridding = 2, ρ = 0.01, using minimizing α, ISNR = 29.9759[dB] line 113,Best Result, after 49 iterations, RMSe = 0.029928 (-30.4784 [dB]), MAXe = 0.30641 (-10.2741 [dB])SPURS-150-100-5005010015000.20.40.60.811.2NUFFT, Brain, M=20000, N=65536, UseW =1, ISNR = 29.9759[dB] line 113,Best Result, after 3 iterations, RMSe = 0.090508(-20.8663 [dB]), MAXe = 0.52336(-5.624 [dB])NUFFT-150-100-5005010015000.20.40.60.811.2rBURSdr, Brain, M=20000, N=65536 OverGridding = 2, using a Cartesian radius of 3, a Non-Cartesian radius of 1.2 ρ = 0.1, ISNR = 29.9759[dB] line 113, RMSe = 0.076318 (-22.3475 [dB]), MAXe = 0.52342 (-5.623 [dB])BURSx2-150-100-5005010015000.20.40.60.811.2rBURS, Brain, M=20000, N=65536 using Bspline(3), ρ = 0.1, ISNR = 29.9759[dB] line 113, RMSe = 0.092043 (-20.7202 [dB]), MAXe = 0.6677 (-3.5084 [dB])BURS-150-100-5005010015000.20.40.60.811.2NUFFT1, Brain, M=20000, N=65536, UseW =1, ISNR = 29.9759[dB] line 113,Best Result, after 1 iterations, RMSe = 0.10806(-19.3264 [dB]), MAXe = 0.55759(-5.0737 [dB])C.GriddingIEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. X, NO. X, MARCH 2016

13

Fig. 15. SNR as a function of the oversampling factor σ and the spline
degree for an analytical brain phantom sampled on a spiral trajectory with
M = 30000 and ISNR = 30 dB.

Fig. 14. Results for an analytic brain phantom sampled on a spiral trajectory
with M = 30000, ISNR = 30 dB. Right column: reconstructed images; left
column: proﬁle plots of row 113. Rows, from top to bottom, SNR and MSSIM
values in parenthesis: SPURS using β3 (19.57 dB, 0.93), NUFFT (9.15 dB,
0.61), BURS with σ = 2(9.40 dB, 0.61), BURS with σ = 1(12.55 dB, 0.69),
convolutional gridding (7.38 dB, 0.61).

dB penalty in SNR with respect to higher degree splines. As
presented in Section V-D, both σ and the spline support affect
the computational complexity in a way that it is advantageous
to keep them at a minimum. For example,
in Fig. 14 a
spiral trajectory with M = 30000 and ISNR = 30dB is
employed, with SPURS using σ = 2 and β3. The recon-
struction results has SNR = 19.57dB. According to Fig. 21,
selecting σ = 1.2 and β1 decreases NNZ (L + U) and thus
the storage requirements by more than tenfold and the total

Fig. 16. SNR as a function of ISNR for an analytical brain phantom sampled
on a Radial trajectory with M = 51200 samples.

number of operations by a factor of about 3. The penalty in
performance is negligible, and in our experiment we obtain
SNR = 19.47dB. These results are signiﬁcantly better than
those of all the other methods presented in Fig. 14.

VIII. CONCLUSION

A new computationally efﬁcient method for reconstruction
of functions from a non-Cartesian sampling set is presented
which derives from modern sampling theory. In the algorithm,
termed SPURS, a sequence of projections is performed, with
the introduction of an interim subspace Q comprised of integer
shifts of a compactly supported kernel. A sparse set of linear
equations is constructed, which allows for the application of
efﬁcient sparse equation solvers, resulting in a considerable
reduction in the computational cost. The purposed method is
used for reconstruction of images sampled nonuniformly in

-150-100-5005010015000.20.40.60.811.2GSURS, Brain, M=30000, N=65536 using Bspline(3), OverGridding = 2, ρ = 100, using minimizing α, ISNR = 29.9869[dB] line 113,Best Result, after 38 iterations, RMSe = 0.025213 (-31.9675 [dB]), MAXe = 0.27131 (-11.3306 [dB])SPURS-150-100-5005010015000.20.40.60.811.2NUFFT, Brain, M=30000, N=65536, UseW =1, ISNR = 29.9869[dB] line 113,Best Result, after 10 iterations, RMSe = 0.083647(-21.551 [dB]), MAXe = 0.77359(-2.2298 [dB])NUFFT-150-100-5005010015000.20.40.60.811.2rBURSdr, Brain, M=30000, N=65536 OverGridding = 2, using a Cartesian radius of 3, a Non-Cartesian radius of 1.2 ρ = 0.1, ISNR = 29.9869[dB] line 113, RMSe = 0.056563 (-24.9494 [dB]), MAXe = 0.48579 (-6.271 [dB])BURSx2-150-100-5005010015000.20.40.60.811.2rBURS, Brain, M=30000, N=65536 using Bspline(3), ρ = 0.1, ISNR = 29.9869[dB] line 113, RMSe = 0.081249 (-21.8037 [dB]), MAXe = 0.66156 (-3.5886 [dB])BURS-150-100-5005010015000.20.40.60.811.2NUFFT1, Brain, M=30000, N=65536, UseW =1, ISNR = 29.9869[dB] line 113,Best Result, after 1 iterations, RMSe = 0.10253(-19.7828 [dB]), MAXe = 1.0343(0.29251 [dB])C.GriddingOversampling factor σ11.522.5SNR [dB]1919.119.219.319.419.519.6SPURS with β1SPURS with β2SPURS with β3SPURS with β4ISNR [dB]010203040506070SNR [dB]02468101214161820rBURSrBURSx2C.GriddingNUFFTSPURS first it.SPURSIEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. X, NO. X, MARCH 2016

14

Fig. 17. MSSIM as a function of ISNR for an analytical brain phantom
sampled on a Radial trajectory with M = 51200 samples.

Fig. 19. MSSIM as a function of ISNR for an analytical brain phantom
sampled on a spiral trajectory with M = 30000 samples.

Fig. 18. SNR as a function of ISNR for an analytical brain phantom sampled
on a spiral trajectory with M = 30000 samples.

Fig. 20. MSSIM as a function of ISNR for an analytical brain phantom
sampled on a spiral trajectory with M = 60000 samples.

k-space, such as in medical imaging: MRI or CT. SPURS
can also be employed for other problems which reconstruct a
signal from a set of non-Cartesian samples, especially those of
considerable dimension and size. After performing the ofﬂine
data preparation step, which is only performed once for a given
set of sampling locations, the computational burden of the
online stage of SPURS is on a par with that of convolutional
gridding or of a single iteration of NUFFT.

In terms of the quality of the reconstructed images, it is
demonstrated that the performance of a single iteration of
the new algorithm, for different sampling SNR ratios and
for various trajectories, exceeds that of both convolutional
gridding, BURS and NUFFT at no additional computational
cost. Iterations can further improve the results at the cost
of higher computational complexity allowing to cope with
reconstruction problems in which the number of available
samples and the SNR are low. These scenarios are of utmost

importance in modern fast imaging techniques.

In this paper we used B-spline functions as the support-
limited kernel function spanning the intermediate subspace Q.
No attempt has been made to optimize this kernel function.
Signiﬁcant research has been performed in order to optimize
the kernel functions employed by other reconstruction methods
such as convolutional gridding [16], [17] and NUFFT [22].
Future research could possibly improve the performance of
the SPURS algorithm by optimizing the kernel used.

The sparse equation solvers used in the present research
employed the default control parameters which were provided
with the software package. The factorization of the sparse
system matrix can possibly be improved to run faster and
produce sparser factors by tuning the control parameters of
the problem or by evaluating other available solvers.

ISNR [dB]010203040506070MSSIM0.20.30.40.50.60.70.80.91rBURSrBURSx2C.GriddingNUFFTSPURS first it.SPURSISNR [dB]010203040506070SNR [dB]246810121416182022rBURSrBURSx2C.GriddingNUFFTSPURS first it.SPURSISNR [dB]010203040506070MSSIM0.20.30.40.50.60.70.80.91rBURSrBURSx2C.GriddingNUFFTSPURS first it.SPURSISNR [dB]010203040506070MSSIM0.30.40.50.60.70.80.91rBURSrBURSx2C.GriddingNUFFTSPURS first it.SPURSIEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. X, NO. X, MARCH 2016

15

functional mri,” Magnetic resonance in medicine, vol. 39, no. 5, pp.
709–716, 1998.

[15] H. Schomberg and J. Timmer, “The gridding method for image re-
construction by Fourier transformation,” IEEE Transactions on Medical
Imaging, vol. 14, no. 3, pp. 596–607, 1995.

[16] J. I. Jackson, C. H. Meyer, D. G. Nishimura, and A. Macovski,
“Selection of a convolution function for Fourier inversion using gridding
[computerised tomography application],” IEEE Transactions on Medical
Imaging, vol. 10, no. 3, pp. 473–478, 1991.

[17] H. Sedarat and D. G. Nishimura, “On the optimality of the gridding
reconstruction algorithm,” IEEE Transactions on Medical Imaging,
vol. 19, no. 4, pp. 306–317, 2000.

[18] D. Rosenfeld, “An optimal and efﬁcient new gridding algorithm us-
ing singular value decomposition,” Magnetic Resonance in Medicine,
vol. 40, no. 1, pp. 14–23, 1998.

[19] ——, “New approach to gridding using regularization and estimation
theory,” Magnetic resonance in medicine, vol. 48, no. 1, pp. 193–202,
2002.

[20] A. Dutt and V. Rokhlin, “Fast Fourier transforms for nonequispaced
data,” SIAM Journal on Scientiﬁc computing, vol. 14, no. 6, pp. 1368–
1393, 1993.

[21] J. Song, Q. H. Liu, S. L. Gewalt, G. Cofer, and G. A. Johnson, “Least-
square nufft methods applied to 2-d and 3-d radially encoded mr image
reconstruction,” IEEE Transactions on Biomedical Engineering, vol. 56,
no. 4, pp. 1134–1142, 2009.

[22] J. A. Fessler and B. P. Sutton, “Nonuniform fast fourier transforms
using min-max interpolation,” Signal Processing, IEEE Transactions on,
vol. 51, no. 2, pp. 560–574, 2003.

[23] N. Nguyen and Q. H. Liu, “The regular Fourier matrices and nonuniform
fast Fourier transforms,” SIAM Journal on Scientiﬁc Computing, vol. 21,
no. 1, pp. 283–293, 1999.

[24] M. Unser and A. Aldroubi, “A general sampling theory for nonideal
acquisition devices,” IEEE Transactions on Signal Processing, vol. 42,
no. 11, pp. 2915–2925, 1994.

[25] Y. C. Eldar and T. Michaeli, “Beyond bandlimited sampling,” Signal

Processing Magazine, IEEE, vol. 26, no. 3, pp. 48–68, 2009.

[26] Y. C. Eldar, Sampling Theory: Beyond Bandlimited Systems. Cambridge

University Press, 2015.

software.php.

[27] “SPURS matlab code,” http://webee.technion.ac.il/people/YoninaEldar/

[28] M. Unser, “Sampling-50 years after shannon,” Proceedings of the IEEE,

vol. 88, no. 4, pp. 569–587, 2000.

[29] P. Th´evenaz, T. Blu, and M. Unser, “Interpolation revisited [medical
images application],” IEEE Transactions on Medical Imaging, vol. 19,
no. 7, pp. 739–758, 2000.

[30] Y. C. Eldar and T. Werther, “General framework for consistent sampling
in Hilbert spaces,” International Journal of Wavelets, Multiresolution
and Information Processing, vol. 3, no. 03, pp. 347–359, 2005.

[31] W.-S. Tang, “Oblique projections, biorthogonal riesz bases and multi-
wavelets in hilbert spaces,” Proceedings of the American Mathematical
Society, vol. 128, no. 2, pp. 463–473, 2000.

[32] A. Ben-Israel and T. N. Greville, Generalized inverses: theory and

applications. Springer Science & Business Media, 2003, vol. 15.

[33] Y. C. Eldar, “Sampling with arbitrary sampling and reconstruction
spaces and oblique dual frame vectors,” Journal of Fourier Analysis
and Applications, vol. 9, no. 1, pp. 77–96, 2003.

[34] ——, “Sampling without input constraints: Consistent reconstruction in
Springer,

arbitrary spaces,” in Sampling, Wavelets, and Tomography.
2004, pp. 33–60.

[35] A. Tikhonov, “Solution of incorrectly formulated problems and the
regularization method,” in Soviet Math. Dokl., vol. 5, 1963, pp. 1035–
1038.

[36] M. T. Heath, “Numerical methods for large sparse linear least squares
problems,” SIAM Journal on Scientiﬁc and Statistical Computing, vol. 5,
no. 3, pp. 497–513, 1984.

[37] I. S. Duff and J. K. Reid, “A comparison of some methods for the
solution of sparse overdetermined systems of linear equations,” IMA
Journal of Applied Mathematics, vol. 17, no. 3, pp. 267–280, 1976.

[38] W. F. Tinney and C. E. Hart, “Power ﬂow solution by newton’s method,”
IEEE Transactions on Power Apparatus and Systems, no. 11, pp. 1449–
1460, 1967.

[39] W. F. Tinney and W. Meyer, “Solution of large sparse systems by or-
dered triangular factorization,” IEEE Transactions on Automatic Control,
vol. 18, no. 4, pp. 333–346, 1973.

[40] T. A. Davis and I. S. Duff, “An unsymmetric-pattern multifrontal method
for sparse lu factorization,” SIAM Journal on Matrix Analysis and
Applications, vol. 18, no. 1, pp. 140–158, 1997.

Fig. 21. The number of non zeros in the L + U matrices which factor Ψ
as a function of the oversampling factor σ and the spline degree for a spiral
sampling trajectory with M = 30000.

REFERENCES

[1] J. Song, Q. H. Liu, K. Kim, and W. Scott, “High-resolution 3-d radar
imaging through nonuniform fast fourier transform (nufft),” Commu. in
Computat. Phys, vol. 1, no. 1, pp. 176–191, 2006.

[2] J. Song, Q. H. Liu, P. Torrione, and L. Collins, “Two-dimensional
and three-dimensional nufft migration method for landmine detection
using ground-penetrating radar,” IEEE Transactions on Geoscience and
Remote Sensing, vol. 44, no. 6, pp. 1462–1469, 2006.

[3] B. Subiza, E. Gimeno-Nieves, J. Lopez-Sanchez, and J. Fortuny-
Guasch, “An approach to sar imaging by means of non-uniform ffts,”
in IGARSS’03. Proceedings. 2003 IEEE International Geoscience and
Remote Sensing Symposium, 2003., vol. 6.
IEEE, 2003, pp. 4089–4091.
[4] M. M. Bronstein, A. M. Bronstein, M. Zibulevsky, and H. Azhari,
“Reconstruction in diffraction ultrasound tomography using nonuniform
fft,” IEEE Transactions on Medical Imaging, vol. 21, no. 11, pp. 1395–
1401, 2002.

[5] V. Rasche, R. W. D. Boer, D. Holz, and R. Proksa, “Continuous radial
data acquisition for dynamic MRI,” Magnetic resonance in medicine,
vol. 34, no. 5, pp. 754–761, 1995.

[6] P. F. Ferreira, P. D. Gatehouse, R. H. Mohiaddin, and D. N. Firmin,
“Cardiovascular magnetic resonance artefacts,” J Cardiovasc Magn
Reson, vol. 15, p. 41, 2013.

[7] C. Ahn, J. Kim, and Z. Cho, “High-speed spiral-scan echo planar nmr
imaging-i,” IEEE Transactions on Medical Imaging, vol. 5, no. 1, pp.
2–7, 1986.

[8] B. Delattre, R. M. Heidemann, L. A. Crowe, J.-P. Vall´ee, and J.-N.
Hyacinthe, “Spiral demystiﬁed,” Magnetic resonance imaging, vol. 28,
no. 6, pp. 862–881, 2010.

[9] J. G. Pipe and N. R. Zwart, “Spiral

trajectory design: A ﬂexible
numerical algorithm and base analytical equations,” Magnetic Resonance
in Medicine, vol. 71, no. 1, pp. 278–285, 2014.

[10] A. F. Gmitro and A. L. Alexander, “Use of a projection reconstruc-
tion method to decrease motion sensitivity in diffusion-weighted mri,”
Magnetic resonance in medicine, vol. 29, no. 6, pp. 835–838, 1993.

[11] R. Van de Walle, I. Lemahieu, and E. Achten, “Two motion-detection
algorithms for projection–reconstruction magnetic resonance imaging:
theory and experimental veriﬁcation,” Computerized medical imaging
and graphics, vol. 22, no. 2, pp. 115–121, 1998.

[12] K. Schefﬂer and J. Hennig, “Frequency resolved single-shot mr imaging
using stochastic k-space trajectories,” Magnetic resonance in medicine,
vol. 35, no. 4, pp. 569–576, 1996.

[13] D. C. Noll, “Multishot rosette trajectories for spectrally selective mr
imaging,” IEEE Transactions on Medical Imaging, vol. 16, no. 4, pp.
372–377, 1997.

[14] D. C. Noll, S. J. Peltier, and F. E. Boada, “Simultaneous multislice
acquisition using rosette trajectories (smart): a new imaging method for

Oversampling factor σ11.522.5NNZ(L+U)×10700.511.522.533.544.55SPURS with β1SPURS with β2SPURS with β3SPURS with β4IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. X, NO. X, MARCH 2016

16

[45] P. J. Beatty, D. G. Nishimura, and J. M. Pauly, “Rapid gridding
reconstruction with a minimal oversampling ratio,” IEEE Transactions
on Medical Imaging, vol. 24, no. 6, pp. 799–808, 2005.

[46] A. Aldroubi and H. Feichtinger, “Exact iterative reconstruction algorithm
for multivariate irregularly sampled functions in spline-like spaces: the
Lp-theory,” Proceedings of the American Mathematical Society, vol.
126, no. 9, pp. 2677–2686, 1998.

[47] A. Aldroubi and K. Gr¨ochenig, “Beurling-landau-type theorems for non-
uniform sampling in shift invariant spline spaces,” Journal of Fourier
Analysis and Applications, vol. 6, no. 1, pp. 93–103, 2000.

[48] ——, “Nonuniform sampling and reconstruction in shift-invariant

spaces,” SIAM review, vol. 43, no. 4, pp. 585–620, 2001.

[41] T. A. Davis, “Algorithm 832: Umfpack v4. 3—an unsymmetric-pattern
multifrontal method,” ACM Transactions on Mathematical Software
(TOMS), vol. 30, no. 2, pp. 196–199, 2004.

[42] ——, Direct methods for sparse linear systems. Siam, 2006, vol. 2.
[43] I. J. Sch¨onberg, “Contributions to the problem of approximation of
equidistant data by analytic functions,” Quart. Appl. Math, vol. 4, no. 2,
pp. 45–99, 1946.

[44] M. Unser, “Splines: A perfect ﬁt for signal and image processing,” Signal

Processing Magazine, IEEE, vol. 16, no. 6, pp. 22–38, 1999.

[49] J. Keiner, S. Kunis, and D. Potts, “Using NFFT 3—a software library
for various nonequispaced fast Fourier transforms,” ACM Transactions
on Mathematical Software (TOMS), vol. 36, no. 4, p. 19, 2009.

[50] M. Guerquin-Kern, L. Lejeune, K. P. Pruessmann, and M. Unser,
“Realistic analytical phantoms for parallel magnetic resonance imaging,”
IEEE Transactions on Medical Imaging, vol. 31, no. 3, pp. 626–636,
2012.

[51] T. Knopp, S. Kunis, and D. Potts, “A note on the iterative mri reconstruc-
tion from nonuniform k-space data,” International journal of biomedical
imaging.

[52] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, “Image
quality assessment: from error visibility to structural similarity,” IEEE
Transactions on Image Processing, vol. 13, no. 4, pp. 600–612, 2004.

