BluePyOpt: Leveraging open source software and cloud

infrastructure to optimise model parameters in neuroscience

Werner Van Geit 1
Jean-Denis Courcol 1, Eilif Muller 1, Felix Sch¨urmann 1, Idan Segev 3

∗, Michael Gevaert 1, Giuseppe Chindemi 1, Christian R¨ossert 1,
4 and Henry

,

,

6
1
0
2

 
r
a

M
1

 

 
 
]

.

C
N
o
i
b
-
q
[
 
 

1
v
0
0
5
0
0

.

3
0
6
1
:
v
i
X
r
a

Markram 1

,

2

,

∗

1Blue Brain Project, ´Ecole Polytechnique F´ed´erale de Lausanne (EPFL) Biotech

2Laboratory of Neural Microcircuitry, Brain Mind Institute, ´Ecole Polytechnique

Campus, Geneva, Switzerland

F´ed´erale de Lausanne, Lausanne, Switzerland

3Department of Neurobiology, Alexander Silberman Institute of Life Sciences, The

Hebrew University of Jerusalem, Jerusalem, Israel

4The Edmond and Lily Safra Centre for Brain Sciences, The Hebrew University of

Jerusalem, Jerusalem, Israel

∗ werner.vangeit@epﬂ.ch, henry.markram@epﬂ.ch

March 3, 2016

Abstract

At many scales in neuroscience, appropriate mathematical models take the form of com-

plex dynamical systems. Parametrising such models to conform to the multitude of available

experimental constraints is a global nonlinear optimisation problem with a complex ﬁtness

landscape, requiring numerical techniques to ﬁnd suitable approximate solutions. Stochastic

optimisation approaches, such as evolutionary algorithms, have been shown to be eﬀective, but

often the setting up of such optimisations and the choice of a speciﬁc search algorithm and its

parameters is non-trivial, requiring domain-speciﬁc expertise. Here we describe BluePyOpt, a

Python package targeted at the broad neuroscience community to simplify this task. BluePy-

Opt is an extensible framework for data-driven model parameter optimisation that wraps and

standardises several existing open-source tools.

It simpliﬁes the task of creating and shar-

ing these optimisations, and the associated techniques and knowledge. This is achieved by

abstracting the optimisation and evaluation tasks into various reusable and ﬂexible discrete

elements according to established best-practices. Further, BluePyOpt provides methods for

setting up both small- and large-scale optimisations on a variety of platforms, ranging from

laptops to Linux clusters and cloud-based compute infrastructures. The versatility of the

BluePyOpt framework is demonstrated by working through three representative neuroscience

speciﬁc use cases.

1

1 Introduction

Advances in experimental neuroscience are bringing an increasing volume and variety of data,

and inspiring the development of larger and more detailed models (Izhikevich and Edelman, 2008;
Merolla et al., 2014; Markram et al., 2015; Eliasmith et al., 2016). While experimental constraints

are usually available for the emergent behaviours of such models, it is unfortunately commonplace

that many model parameters remain inaccessible to experimental techniques. The problem of infer-
ring or searching for model parameters that match model behaviours to experimental constraints

constitutes an inverse problem (Tarantola, 2016), for which analytical solutions rarely exist for com-
plex dynamical systems, i.e. most mathematical models in neuroscience. Historically, such parame-

ter searches were done by hand tuning, but the advent of increasingly powerful computing resources
has brought automated search algorithms that can ﬁnd suitable parameters (Bhalla and Bower,

1993; Vanier and Bower, 1999; Achard and De Schutter, 2006; Gurkiewicz and Korngreen, 2007;
Druckmann et al., 2007; Van Geit et al., 2007, 2008; Huys and Paninski, 2009; Taylor et al., 2009;

Hay et al., 2011; Bahl et al., 2012; Svensson et al., 2012; Friedrich et al., 2014; Pozzorini et al.,
2015; Stefanou et al., 2016). While many varieties of search algorithms have been described and

explored in the literature (Vanier and Bower, 1999; Van Geit et al., 2008; Svensson et al., 2012),
stochastic optimisation approaches, such as simulated annealing and evolutionary algorithms, have

been shown to be particularly eﬀective strategies for such parameter searches (Vanier and Bower,
1999; Druckmann et al., 2007; Gurkiewicz and Korngreen, 2007; Svensson et al., 2012). Never-

theless, picking the right type of stochastic algorithm and setting it up correctly remains a

non-trivial task requiring domain-speciﬁc expertise, and could be model and constraint speciﬁc
(Van Geit et al., 2008).

With the aim of bringing widely applicable and state-of-the-art automated parameter search
algorithms and techniques to the broad neuroscience community, we describe here a Python-based

open-source optimisation framework, BluePyOpt, which is available on Github (see (Blue Brain Project,
2016)), and is designed taking into account model optimisation experience accumulated during the

Blue Brain Project (Druckmann et al., 2007; Hay et al., 2011; Markram et al., 2015; Ramaswamy et al.,
2015) and the ramp-up phase of the Human Brain Project. The general purpose high-level pro-

gramming language Python was chosen for developing BluePyOpt, so as to contribute to, and
also leverage from the growing scientiﬁc and neuroscientiﬁc software ecosystem (Oliphant, 2007;

Muller et al., 2015), including state-of-the-art search algorithm implementations, modelling and
data access tools.

At its core, BluePyOpt is a framework providing a conceptual scaﬀolding in the form of an
object-oriented application programming interface or API for constructing optimization problems

according to established best-practices, while leveraging existing search algorithms and modelling

simulators transparently “under the hood”. For common optimisation tasks, the user conﬁgures
the optimisation by writing a short Python script using the BluePyOpt API. For more advanced

use cases, the user is free to extend the API for their own needs, potentially contributing these
extensions back to the core library. The latter is important for BluePyOpt APIs to remain broadly

applicable and state-of-the-art, as best-practices develop for speciﬁc problem domains, mirroring
the evolution that has occured for neuron model optimization strategies (Bhalla and Bower, 1993;

Hay et al., 2011).

Depending on the complexity of the model to be optimised, BluePyOpt optimisations can

require signiﬁcant computing resources. The systems available to neuroscientists in the commu-

2

nity can be very heterogeneous, and it is often diﬃcult for users to set up the required software.
BluePyOpt therefore also provides a novel cloud conﬁguration mechanism to automate setting up

the required environment on a local machine, cluster system, or cloud service such as Amazon Web
Services.

To begin, this technical report provides an overview of the conceptual framework and open-
source technologies used by BluePyOpt, followed by a presentation of the software architecture

and API of BluePyOpt. Next, three concrete use cases are elaborated in detail, showing how the
BluePyOpt APIs, concepts and techniques can be put to use by potential users. The ﬁrst use case is

an introductory example demonstrating the optimisation of a single compartmental neuron model
with two Hodgkin-Huxley ion channels. The second use case shows a BluePyOpt-based state-of-

the-art optimisation of a morphologically detailed thick-tufted layer 5 pyramidal cell model of the

type used in a recent in silico reconstruction of a neocortical microcircuit (Markram et al., 2015).
The third use case demonstrates the broad applicability of BluePyOpt, showing how it can also be

used to optimise parameters of synaptic plasticity models.

2 Concepts

The BluePyOpt framework provides a powerful tool to optimise models in the ﬁeld of neuroscience,

by combining several established Python-based open-source software technologies. In particular,
BluePyOpt leverages libraries providing optimisation algorithms, parallelisation, compute environ-

ment setup, and experimental data analysis. For numerical evaluation of neuroscientiﬁc models,
many open-source simulators with Python bindings are available for the user to chose from. The

common bridge allowing BluePyOpt to integrate these various softwares is the Python program-
ming language, which has seen considerable uptake and a rapidly growing domain-speciﬁc software

ecosystem in the neuroscience modelling community in recent years (Muller et al., 2015). Python

is recognized as a programming language which is fun and easy to learn, yet also attractive to
experts, meaning that novice and advanced programmers alike can easily use BluePyOpt, and

contribute solutions to neuroscientiﬁc optimisation problems back to the community.

BluePyOpt was developed using an object oriented programming model. Figure 1 shows an

overview of the class hierarchy of BluePyOpt. In its essence, the BluePyOpt object model deﬁnes
the Optimisation class which applies a search algorithm to an Evaluator class. Both are abstract

classes, meaning they deﬁne the object model, but not the implementation. Taking advantage of
Pythonic duck typing, the user can then choose from a menu of implementations, derived classes, or

easily deﬁne their own implementations to meet their speciﬁc needs. This design makes BluePyOpt
highly versatile, while keeping the API complexity to a minimum. The choice of algorithm and

evaluator is up to the user, but many are already provided for various use cases. For many common
use cases, these are the only classes users are required to instantiate.

For neuron model optimizations in particular, BluePyOpt provides further classes to sup-
port feature-based multi-objective optimizations using NEURON, as shown in Figure 1. Classes

CellModel, Morphology, Mechanisms, Protocol, Stimuli, Recordings, Location are speciﬁc to setting

up neuron models and assessing their input-output properties. Other classes Objectives and eFea-
ture are more generally applicable, with derived classes for speciﬁc use cases, e.g. eFELFeature

provides features extracted from voltage traces using the open-source eFEL library discussed below.
They deﬁne features and objectives for feature-based multi-objective optimization, a best-in-class

stochastic optimization strategy (Druckmann et al., 2007, 2011; Hay et al., 2011). We generally

3

Abstract classes

Optimisation

Evaluator

Model

Morphology

Mechanisms

Protocols

Stimuli

Derived classes
DEAPOptimisation

External tools

DEAP

ModelEvaluator, GraupnerBrunelEvaluator

CellModel, NetworkModel, SynapseModel

NrnFileMorphology

NrnModMechanism

SequenceProtocol, SweepProtocol

NrnCurrentPlayStimulus, NrnSquarePulse

Recordings

CompRecording

Simulator

Parameters

Objectives

eFeature

ObjectivesCalculator

NrnSimulator

Neuron

NrnRangeParameter, NrnSectionParameter

SingletonObjective, WeightedSumObjective

eFELFeature

eFEL

Figure 1.Hierarchy of the most important classes in BluePyOpt. Ephys abstraction layer in blue.

recommend it as the ﬁrst algorithm to try for a given problem domain. For example, the third

example for the optimisation of synaptic plasticity models also employs this strategy.

In the sub-sections to follow, an overview is provided for the various software components and

the manner in which BluePyOpt integrates them.

2.1 Optimisation algorithms

Multiobjective evolutionary algorithms have been shown to perform well to optimise parameters

of biophysically detailed multicompartmental neuron models (Druckmann et al., 2007; Hay et al.,
2011). To provide optimisation algorithms, BluePyOpt relies on a mature Python library, Dis-

tributed Evolutionary Algorithms in Python (DEAP), which implements a range of such algo-
rithms (Fortin et al., 2012). The advantage of using this library is that it provides many useful

features out of the box, and it is mature, actively maintained and well documented. DEAP provides
many popular algorithms, such as Non-dominated Sorting Genetic Algorithm-II (Deb et al., 2002),

Covariance Matrix Adaptation Evolution Strategy (Hansen and Ostermeier, 2001), and Particle
Swarm Optimisation (Kennedy and Eberhart, 1995). Moreover, due to its extensible design, im-

plementing new search algorithms in DEAP is straight-forward. Historically, the Blue Brain Project

has used a C implementation of the Indicator Based Evolutionary Algorithm IBEA to optimise the
parameters of biophysically detailed neuron models (Zitzler and K¨unzli, 2004; Bleuler et al., 2003;

Markram et al., 2015), as this has been shown to have excellent convergence properties for these
problems (Schm¨ucker, 2010). Case in point, we implemented a version of IBEA for the DEAP

framework, so this algorithm is consequently available to be used in BluePyOpt.

4

Moreover, DEAP is highly versatile, whereby most central members of its class hierarchy, such
as individuals and operators, are fully customizable with user deﬁned implementations. Classes

are provided to keep track of the Pareto Front or the Hall-of-Fame of individuals during evolution.
Population statistics can be recorded in a logbook, and the genealogy between individuals can

be saved, analysed and visualised. In addition, checkpointing can be implemented in DEAP by
storing the algorithm’s state in a Python pickle ﬁle for any generation, as described in DEAP’s

documentation (DEAP Project, 2016).

Although the use cases below use DEAP as a library to implement the search algorithm, it is

worth noting that BluePyOpt abstracts the concept of a search algorithm. As such, it is entirely
possible to implement algorithms that are independent of DEAP, or that use other third-party

libraries.

2.2 Simulators

To deﬁne a BluePyOpt optimisation, the user must provide an evaluation function which maps
model parameters to a ﬁtness score. It can be a single Python function that maps the parameters

to objectives by solving a set of equations, or a function that uses an external simulator to eval-
uate a complex model under multiple scenarios. For the latter, the only requirement BluePyOpt

imposes is that it can interact with the external simulator from within Python. Often, this inter-
action is implemented through Python modules provided by the user’s neuroscientiﬁc simulator of

choice, as is the case for many simulators in common use, including NEURON (Hines et al., 2009),
NEST (Eppler et al., 2009; Zaytsev and Morrison, 2014), PyNN (Davison et al., 2009), BRIAN

(Goodman and Brette, 2009), STEPS (Wils and De Schutter, 2009), and MOOSE (Ray and Bhalla,
2008). Otherwise, communication through shell commands and input/output ﬁles is also possible,

so long as an interface can be provided as a Python class.

2.3 Feature Extraction

For an evaluation function to compute a ﬁtness score from simulator output, the resulting traces
must be compared against experimental constraints. Voltage recordings obtained from patch clamp

experiments are an example of experimental data that can be used as a constraint for neuron
models. From such recordings the neuroscientist can deduce many interesting values, like the input

resistance of the neuron, the action potential characteristics, ﬁring frequency etc. To standardise
the way these values are measured, the Blue Brain Project has released the Electrophysiology

Feature Extract Library (eFEL) (Blue Brain Project, 2015), also as open-source software. The
core of this library is written in C++, and a Python wrapper is provided. BluePyOpt can interact

with eFEL to compute a variety of features of the voltage response of neuron models. A ﬁtness
score can then be computed by some distance metric comparing the resulting model features to

their experimental counterparts. As we will see for the last example in this article, a similar
approach can also be taken for other optimisation problem domains.

2.4 Parallelisation

Optimisations of the parameters of an evaluation function typically require the execution of this

function repeatedly. For a given optimisation integration step, such executions are often in the
hundreds (scaling e.g. with evolutionary algorithm population size), are compute bound, and are

5

essentially independent, making them ripe for parallelisation. Parallelisation of the optimisation
can be performed in several ways. DEAP provides an easy way to evaluate individuals in a

population on several cores in parallel. The user need merely provide an implementation of a
map function. In its simplest form, this function can be the Python serial map in the standard

library, or the parallel map function in the multiprocessing module to leverage local hardware
threads. To parallelise over a large cluster machine, the DEAP developers encourage the use of

the SCOOP (Hold-Geoﬀroy et al., 2014) map function. SCOOP is a library that builds on top
of ZeroMQ (ZeroMQ Project, 2007), which provides a socket communication layer to distribute

the computation over several computers. Other map functions and technologies can be used like
MPI4Py (Dalcn et al., 2005) or iPython ipyparallel package (P´erez and Granger, 2007). Moreover,

parallelisation doesn’t necessarily have to happen at the population level. Inside the evaluation of

individuals, map functions can also be used to parallelise over stimulus protocols, feature types,
etc., however for the problem examples presented here, such an approach wouldn’t make good use

of anything more than 10 to 20 cores.

2.5 Cloud

To increase the throughput of optimisations, multiple computers can be used to parallelise the work.

Such a group of computers can be composed of machines in a cluster, or they can be obtained from
a cloud provider like Amazon Web Services, Rackspace Public Cloud, Microsoft Azure, Google

Compute Engine, or the Neuroscience Gateway portal (Sivagnanam et al., 2013).

These and other cloud providers allow for precise allocation of numbers of machines and their

storage, compute power and memory. Depending on the needs and resources of an individual or
organization, trade-oﬀs can be made on how much to spend versus how fast the results are needed.

Setting up a cluster or cloud environment with the correct software requirements is often
complicated and error prone: Each environment has to be exactly the same, and scripts and data

need to be available in the same locations. To ease the burden of this conﬁguration, BluePyOpt

includes Ansible (Red Hat, Inc., 2012) conﬁguration scripts for setting up a local test environment
(on one machine, using Vagrant (HashiCorp, 2010)), for setting up a cluster with a shared ﬁle

system, or for provisioning and setting up an Amazon Web Services cluster.

Ansible is open-source software that allows for reproducible environments to be created and

conﬁgured from simple textual descriptions called ’Playbooks’. These Playbooks encapsulate the
discrete steps needed to create an environment, and oﬀer extra tools to simplify things like package

management, user creation and key distribution. Furthermore, when a Playbook is changed and
run against an already existing environment, only the changes necessary will be applied. Finally,

Ansible has the advantage over other systems, like Puppet (Puppet Labs, 2005) and Chef (Chef,
2009), that nothing except a Python interpreter needs to be installed on the target machine and all

environment discovery and conﬁguration is performed through SSH from the machine on which An-
sible is run. This decentralized system means that a user can use Ansible to setup an environment

in their home directory on a cluster, without intervention from the system administrators.

3 Software Architecture

The BluePyOpt software architecture follows an object oriented programming model, whereby the
various concepts of the software are modularised into cleanly seperated and well deﬁned classes

6

Optimisation

Evaluator

Evaluator: evaluator

run() - > results

Parameter

bool: frozen
value
bounds

instantiate(Simulator)

Objective

value

ObjectivesCalculator

Objectives: objectives

Model: model
Protocol[ ]: protocols
Stimulus[ ]: stimuli
ObjectivesCalculator: obj_calc

parameters
objectives
eval(Parameter [ ]) -> Objective []

Model

Parameter []: parameters
Mechanism []: mechanisms
Morphology: morphology

instantiate(Simulator)

Simulator

python_module
simulator_settings

Protocol

Stimulus []
Recordings []

run(Model, Simulator) -> Response []

Stimulus

Location: location

instantiate(Simulator)

Recording

variable
Location: location

instantiate(Simulator)

Location

location_speci! ers

calc(Response []) -> Objectives

run()

instantiate(Simulator) -> handle

Figure 2: General structure of most important classes. Every box represents a class. In every box
the top panel is the name, the middle panel the most important ﬁelds and the bottom panel the
most important methods. Ephys abstraction layer in blue.

which interact, as deﬁned in a class hierarchy (Figure 1) and object model (Figure 2) show to
deﬁne the program control ﬂow, as shown in Figure 3. In what follows, the role of each class and

how it relates to and interacts with other classes in the hierarchy is described.

3.1 Optimisation abstraction layer

At the highest level of abstraction, the BluePyOpt API contains the classes Optimisation and

Evaluator (Figure 2). An Evaluator object deﬁnes an evaluation function that maps Parameters

to Objectives. The Optimisation object accepts the Evaluator as input, and runs a search algorithm
on it to ﬁnd the parameter values that generate the best objectives.

The task of the search algorithm is to ﬁnd the parameter values that correspond to the best
objective values. Deﬁning ’best’ is left to the speciﬁc implementation. As in the use cases below,

this could be a weighted sum of the objectives or a multiobjective front in a multidimensional
space.

The Optimisation class allows the user to control the settings of the search algorithm. In case
of IBEA, this could be the number of individuals in the population, the mutation probabilities,

etc.

3.2 EPhys model abstraction layer

On a diﬀerent level of abstraction, we have classes that are tailored for electrophysiology (ephys)

experiments and can be used inside the Evaluator. The ephys model layer provides an abstraction

to the simulator, so that the person performing the optimisation doesn’t have to have knowledge
of the intricate details of the simulator.

A Protocol is applied to a Model for a certain set of Parameters, generating a Response. An
ObjectivesCalculator is then used to calculate the Objectives based on the Response of the Model.

All these classes are part of the bluepyopt.ephys package.

7

1. run()

14. results

Optimisation

2. evaluate(Parameters)

13. Objectives

Evaluator

12. Objectives

11. calc(Responses)

ObjectiveCalculator

3. run(Parameters)

10. Responses

Protocols

9. Recordings

4. freeze(Parameters)
5. instantiate()

6. instantiate()

7. instantiate()

8. run()

Model

Stimuli

Recordings

Simulator

Figure 3: Graph representing control ﬂow in BluePyOpt. Ordering is clariﬁed by the numbers.
Arrow labels that contain parentheses represent function calls, the other labels data being returned.
This ﬁgure is meant to give a high level description of the control ﬂow, not all function calls and
intermediate objects are included. Ephys abstraction layer in blue.

3.2.1 Model

By making a Model an abstract class, we give users the ability to use our software for a broad
range of use cases. A Protocol can attach Stimuli and Recordings to a Model. When the Simulator

is then run, a Response is generated for each of the Stimuli for a given set of Model parameter
values.

Examples of broad subclasses are a NetworkModel, CellModel and SynapseModel. Speciﬁc
subclasses can be made for diﬀerent simulators, or assuming some level of similarity, the same

model object can know how to instantiate itself in diﬀerent simulators. In the future, functionality
could be added to import/export the model conﬁguration from/to standard description languages

like NeuroML (Cannon et al., 2014) or NineML (Raikov et al., 2011).

Particular parameters of a Model can be in a frozen state. This means that their value is ﬁxed,

and won’t be considered for optimisation. This concept can be useful in multi-stage optimisation

in which subgroups of a model are optimised in a sequential fashion.

Another advantage of this abstraction is that a Model is a standalone entity that can be run

outside of the Optimisation and have exactly the same Protocols applied to it, generating exactly
the same Response. One can also apply extra Protocols to assess how well the model generalises,

or to perfom a sensitivity analysis.

3.2.2 Simulator

Every model simulator should have a subclass of Simulator. Objects of this type will be passed
on to objects that are simulator aware, like the Model and Stimuli when their instantiate method

is invoked. This architecture allows e.g. the same model object to be run in diﬀerent simulators.
Examples of functionality this class can provide are links to the Python module related to the

simulator, the enabling of variable time step integration, etc. Simulators also have run() method
that starts the simulation.

8

3.2.3 Protocol

A Protocol is an object that elicits a Response from a model. In its simplest form it represents,
for example, a step current clamp stimulus, but more complicated versions are possible, such as

stimulating a set of cells in a network with an elaborate protocol and recording the response. A
Protocol can also contain sub-protocols, providing a powerful mechanism to reuse components.

3.2.4 Stimulus, Recording and Response

The Stimulus and Recording objects, which are part of a Protocol are applied to a model and

are aware of the simulator used. Subclasses of Stimulus are concepts like current/voltage clamp,
synaptic activation, etc. Both of these classes accept a Location speciﬁer. Several Recording objects

can be combined in a Response which can be analysed by an ObjectiveCalculator.

3.2.5 Location

Specifying the location on a neuron morphology of a recording, stimulus or parameter in a simulator
can be complicated. Therefore we created an abstract class Location. As arguments the constructor

accepts the location speciﬁcation, e.g. in NEURON this could be a sectionlist name and an index
of the section, or it could point to a section at a certain distance from the soma. Upon request, the

object will return a reference to the object at the speciﬁed location, this could e.g be a NEURON
section or compartment. At a location, a variable can be set or recorded by a Parameter or

Recording, respectively.

3.2.6 ObjectivesCalculator, eFeature

The ObjectivesCalculator takes the Response of a Model and calculates the objective values from
it. When using ephys recordings, one can use the eFEL library to extract eFeatures. Examples

of these eFeatures are spike amplitudes, steady state voltages, etc. The values of these eFeatures
can then be compared with experimental data values, and a score can be calculated based on the

diﬀerence between model and experiment.

4 Example Use Cases

To provide hands on experience how real-world optimisations can be developed using the BluePy-
Opt API, this section provides step-by-step guides for three use-cases. The ﬁrst is a single

compartmental neuron model optimisation, the second is an optimisation of a state-of-the-art
morphologically detailed neuron model, and the third is an optimisation of a synaptic plasticity
model. All examples to follow assume NEURON default units, i.e. ms, mV, nA, µF cm−2, etc.
(Carnevale and Hines, 2016).

4.1 Single compartmental model

The ﬁrst use case shows how to set up an optimisation of single compartmental neuron model with
two free parameters: The maximal conductances of the sodium and potassium Hodgkin-Huxley

ion channels. This example serves as an introduction for the user to the programming concepts in
BluePyOpt. It uses the NEURON simulator as backend.

9

First we need to import the top-level bluepyopt module. This example will also use BluePy-

Opt’s electrophysiology features, so we also need to import the bluepyopt.ephys subpackage.

import bluepyopt as bpop

import bluepyopt.ephys as ephys

Next we load a morphology from a ﬁle. By default a morphology in NEURON has the fol-
lowing sectionlists: somatic, axonal, apical and basal. We create a Location object (speciﬁcally, a

NrnSecListLocation object) that points to the somatic sectionlist. This object will be used later
to specify where mechanisms are to be added etc.

morph = ephys.morphologies.NrnFileMorphology('simple.swc')

somatic_loc = ephys.locations.NrnSeclistLocation('somatic', seclist_name='somatic')

Now we can add ion channels to this morphology. First we add the default NEURON Hodgkin-

Huxley mechanism to the soma, as follows.

hh_mech = ephys.mechanisms.NrnMODMechanism(

name='hh',

prefix='hh',

locations=[somatic_loc])

The name argument can be chosen by the user, and should be unique across mechanisms.

The preﬁx argument string should correspond to the SUFFIX ﬁeld in the NEURON NMODL
description ﬁle (Carnevale and Hines, 2006) of the channel. The locations argument speciﬁes which

sections the mechanism are to be added to.

Next we need to specify the parameters of the model. A parameter can be in two states: frozen
and not-frozen. When a parameter is frozen it has an exact known value, otherwise it has well-

deﬁned bounds but the exact value is not known yet. The parameter for the capacitance of the
soma will be a frozen value.

cm_param = ephys.parameters.NrnSectionParameter(

name='cm',

param_name='cm',

value=1.0,

locations=[somatic_loc],

frozen=True)

The two parameters that represent the maximal conductance of the sodium and potassium

channels are to be optimised, and are therefore speciﬁed as frozen=False, i.e. not-frozen, and
bounds for each are provided with the bounds argument.

gnabar_param = ephys.parameters.NrnSectionParameter(

name='gnabar_hh',

param_name='gnabar_hh',

locations=[somatic_loc],

bounds=[0.05, 0.125],

frozen=False)

gkbar_param = ephys.parameters.NrnSectionParameter(

name='gkbar_hh',

param_name='gkbar_hh',

10

bounds=[0.01, 0.075],

locations=[somatic_loc],

frozen=False)

To create the cell template, we pass all these objects to the constructor of the model.

simple_cell = ephys.cellmodels.NrnCellModel(

name='simple_cell',

morph=morph,

mechs=[hh_mech],

params=[cm_param, gnabar_param, gkbar_param])

To optimise the parameters of the cell, we further need to create a CellEvaluator object. This

object needs to know which protocols to inject, which parameters to optimise, and how to compute

a score, so we’ll ﬁrst create objects that deﬁne these aspects.

A protocol consists of a set of stimuli and a set of responses (i.e. recordings). These responses

will later be used to calculate the score of the speciﬁc model parameter values. In this example, we
will specify two stimuli, two square current pulses delivered at the soma with diﬀerent amplitudes.

To this end, we ﬁrst need to create a location object for the soma.

soma_loc = ephys.locations.NrnSeclistLocation(

name='soma',

seclist_name='somatic',

sec_index=0,

comp_x=0.5)

For each step in the protocol, we add a stimulus (NrnSquarePulse) and a recording (CompRe-

cording) in the soma.

sweep_protocols = {}

for protocol_name, amplitude in [('step1', 0.01), ('step2', 0.05)]:

stim = ephys.stimuli.NrnSquarePulse(

step_amplitude=amplitude,

step_delay=100,

step_duration=50,

location=soma_loc,

total_duration=200)

rec = ephys.recordings.CompRecording(

name='%s.soma.v' % protocol_name,

location=soma_loc,

variable='v')

protocol = ephys.protocols.SweepProtocol(protocol_name, [stim], [rec])

sweep_protocols[protocol.name] = protocol

The step_amplitude argument of the NrnSquarePulse speciﬁes the amplitude of the current
pulse, and step_delay, step_duration, and total_duration specify the start time, length and

total simulation time. Finally, we create a combined protocol that encapsulates both current pulse
protocols.

twostep_protocol = ephys.protocols.SequenceProtocol('twostep', protocols=sweep_protocols)

11

Now to compute the model score that will be used by the optimisation algorithm, we deﬁne
objective objects. For this example, our objective is to match the eFEL “Spikecount” feature to

speciﬁed values for both current injection amplitudes. In this case, we will create one objective
per feature.

efel_feature_means = {'step1': {'Spikecount': 1}, 'step2': {'Spikecount': 5}}

objectives = []

for protocol_name, protocol in protocols.iteritems():

stim_start = protocol.stimuli[0].step_delay

stim_end = stim_start + protocol.stimuli[0].step_duration

for efel_feature_name, mean in

efel_feature_means[protocol_name].iteritems():

feature_name = '%s.%s' % (protocol_name, efel_feature_name)

feature = ephys.efeatures.eFELFeature(

feature_name,

efel_feature_name=efel_feature_name,

recording_names={'': '%s.soma.v' % protocol_name},

stim_start=stim_start,

stim_end=stim_end,

exp_mean=mean,

exp_std=0.05 * mean)

objective = ephys.objectives.SingletonObjective(

feature_name,

feature)

objectives.append(objective)

We then pass these objective deﬁnitions to a ObjectivesCalculator object, calculate the total

scores from a protocol response.

obj_calc = ephys.scorecalculators.ObjectivesCalculator(objectives)

Finally, we can combine everything together into a CellEvaluator. The CellEvaluator construc-
tor has a ﬁeld param names which contains the (ordered) list of names of the parameters that are

used as input (and will be ﬁtted later on).

cell_evaluator = ephys.evaluators.CellEvaluator(

cell_model=simple_cell,

param_names=['gnabar_hh', 'gkbar_hh'],

fitness_protocols=protocols,

fitness_calculator=obj_calc)

Now that we have a cell template and an evaluator for this cell, the Optimisation object can

be created and run.

optimisation = bpop.optimisations.DEAPOptimisation(

evaluator=cell_evaluator,

offspring_size = 100)

final_pop, hall_of_fame, logs, hist = optimisation.run(max_ngen=10)

12

After a short time, the optimisation returns us the ﬁnal population, the hall of fame, a logbook
and an object containing the history of the population during the execution of the algorithm.

Figure 4 shows the results in a graphical form.

4.2 Neocortical pyramidal cell

Our second use case is a more complex example demonstrating the optimisation of a morphologi-

cally detailed model of a thick-tufted layer 5 pyramidal cell (L5PC) from the neocortex (Fig. 5a).
This example uses a BluePyOpt port of the state-of-the-art methods for the optimisation of the

L5PC model described in Markram et al. (2015). The original model is available online from the
Neocortical Microcircuit Collaboration Portal (Ramaswamy et al., 2015). Due to its complexity,

we will not describe the complete optimisation script here. The full code is available from the

BluePyOpt website. What we will do here is highlight the particularities of this model compared
to the introductory single compartmental model optimisation. As a ﬁrst validation and point of

reference, we ran the BluePyOpt model with its original parameter values from (Ramaswamy et al.,
2015), as shown in Figure 5b.

For clarity, the code for setting the parameters, objective and optimisation algorithm is par-
titioned into separate modules. Conﬁguration values are stored and read from JavaScript Object

Notation JSON ﬁles.

4.2.1 Parameters

Evidently, the parameters of this model, as shown in Table 1, far exceed in number those of the
single compartmental use case. The parameters marked as frozen are kept a constant throughout

the optimisation. The parameters to be optimised are the maximal conductances of the ion channels
and two values related to the calcium dynamics. The location of the parameters is based on

sectionlist names, whereby sections are automatically assigned to the somatic, axonal, apical and
basal sectionlists by NEURON when it loads a morphology.

An important aspect of this neuron model is the non-uniform distribution of certain ion channel
conductances. For example, the h-channel conductance is speciﬁed to increase exponentially with

distance from the soma (Kole et al., 2006), as follows.

soma_loc = ephys.locations.NrnSeclistLocation(

seclist_name='somatic',

seclist_index=0,

seg_x=0.5)

exponential_scaler = ephys.parameterscalers.NrnDistanceScaler(

origin=soma_loc,

distribution='(-0.8696 + 2.087*math.exp(({distance})*0.0031))*{value}')

parameter = ephys.parameters.NrnRangeParameter(

name='gIhbar_Ih.apical',

param_name='gIhbar_Ih'

value_scaler=exponential_scaler,

value=8e-5,

frozen=True,

locations=[apical_loc]))

13

)
V
m

(
 
e
g
a
t
l
o
V

)
V
m

(
 
e
g
a
t
l
o
V

60

40

20

0

−20

−40

−60

−80

0

60

40

20

0

−20

−40

−60

−80

0

250

200

150

100

50

j

s
e
v
i
t
c
e
b
o
 
f
o
 
m
u
S

50

50

100

Time (ms)

150

200

100

Time (ms)

150

200

population average
population minimum
population standard deviation

0

0

2

4

6

Generation #

8

10

(a) Top plots: In light blue, voltage traces recording during the two diﬀerent step current injection for all
the individuals found that have objectives sum equal to zero. In dark blue, an example of ones of these
individuals. The target objectives of Step1 and Step2 were 1 and 5 action potentials respectively. Bottom
plot: Evolution of minimal objectives sum during the 10 generations of the evolutionary algorithm.

0.10

0.08

0.06

0.04

0.02

r
a
b
k
g
 
r
e
t
e
m
a
r
a
P

0.00

0.00

5.6

4.8

4.0

3.2

2.4

1.6

0.8

0.0

0.05

0.10

0.15

0.20

Parameter gnabar

j

)
1
 
+
 
s
e
v
i
t
c
e
b
o
 
f
o
 
m
u
s
(
g
o

l

(b) Triangular grid plot of the parameter space. Every point of the grid is a point where the algorithm
evaluated an individual. X- and Y-axis represent the values of the sodium and potassium maximal con-
ductance respectively (units S cm−2). The color represents the average natural logarithm of the objectives
sum of every triangle’s three points. Circles represent the solutions with an objectives sum of 0.

Figure 4: Results of the single compartmental model optimisation.

14

v
.
a
m
o
s
.

1
p
e
t
S

)
V
m

(
 
e
g
a
t
l
o
V

v
.
a
m
o
s
.

2
p
e
t
S

)
V
m

(
 
e
g
a
t
l
o
V

v
.
a
m
o
s
.

3
p
e
t
S

)
V
m

(
 
e
g
a
t
l
o
V

v
.

1
d
n
e
d
.
P
A
b

)
V
m

(
 
e
g
a
t
l
o
V

v
.

2
d
n
e
d
.
P
A
b

)
V
m

(
 
e
g
a
t
l
o
V

v
.
a
m
o
s
.
P
A
b

)
V
m

(
 
e
g
a
t
l
o
V

40
20
0
−20
−40
−60
−80

40
20
0
−20
−40
−60
−80

40
20
0
−20
−40
−60
−80

40
20
0
−20
−40
−60
−80

40
20
0
−20
−40
−60
−80

40
20
0
−20
−40
−60
−80

0

0

0

0

0

0

500

1000

1500

2000

2500

3000

500

1000

1500

2000

2500

3000

500

1000

1500

2000

2500

3000

100

200

300

400

500

600

100

200

300

400

500

600

100

200

300

Time (ms)

400

500

600

(a) Morphological reconstruction of L5PC used
in the model obtained from the NMC portal
(Ramaswamy et al., 2015).

(b) Voltage traces recorded in soma and den-
drites (dend1 660 µm, dend2 800 µm from soma
in apical trunk).

s
e
v
i
t
c
e
b
O

j

bAP.soma.Spikecount

bAP.soma.AP_width

bAP.soma.AP_height

bAP.dend2.AP_amplitude_from_voltagebase

bAP.dend1.AP_amplitude_from_voltagebase

Step3.soma.time_to_first_spike

Step3.soma.mean_frequency

Step3.soma.doublet_ISI

Step3.soma.adaptation_index2

Step3.soma.ISI_CV

Step3.soma.AP_width

Step3.soma.AP_height

Step3.soma.AHP_slow_time

Step3.soma.AHP_depth_abs_slow

Step3.soma.AHP_depth_abs

Step2.soma.time_to_first_spike

Step2.soma.mean_frequency

Step2.soma.doublet_ISI

Step2.soma.adaptation_index2

Step2.soma.ISI_CV

Step2.soma.AP_width

Step2.soma.AP_height

Step2.soma.AHP_slow_time

Step2.soma.AHP_depth_abs_slow

Step2.soma.AHP_depth_abs

Step1.soma.time_to_first_spike

Step1.soma.mean_frequency

Step1.soma.doublet_ISI

Step1.soma.adaptation_index2

Step1.soma.ISI_CV

Step1.soma.AP_width

Step1.soma.AP_height

Step1.soma.AHP_slow_time

Step1.soma.AHP_depth_abs_slow

Step1.soma.AHP_depth_abs

0.0

0.5

1.0

1.5

2.0

2.5

3.0

3.5

Objective value (# std)

(c) Objective scores for the model calculated
based on experimental mean and standard de-
viation.

Figure 5: L5PC model as simulated by BluePyOpt with parameter values from (Markram et al.,
2015)

15

Table 1. List of parameters for L5PC example. Optimised parameters with bounds are in upper
part of the table, lower part lists the frozen parameters with their value. The parameter with exp
distribution is scaled for every morphological segment with the equation −0.8696 + 2.087 · e0.0031·d
with d the distance of the segment to the soma.

Location Mechanism

Parameter name Distribution Units

Lower bound Upper bound

0.04
0.04
0.001
4
4
1
0.1
0.1
2
0.001
0.01
0.05
1000
1
1
0.1
0.001
0.01
0.05
1000

apical
apical
apical
axonal
axonal
axonal
axonal
axonal
axonal
axonal
axonal
axonal
axonal
somatic
somatic
somatic
somatic
somatic
somatic
somatic

NaTs2 t
SKv3 1
Im
NaTa t
Nap Et2
K Pst
K Tst
SK E2
SKv3 1
Ca HVA
Ca LVAst
CaDynamics E2
CaDynamics E2
NaTs2 t
SKv3 1
SK E2
Ca HVA
Ca LVAst
CaDynamics E2
CaDynamics E2

gNaTs2 tbar
gSKv3 1bar
gImbar
gNaTa tbar
gNap Et2bar
gK Pstbar
gK Tstbar
gSK E2bar
gSKv3 1bar
gCa HVAbar
gCa LVAstbar
gamma
decay
gNaTs2 tbar
gSKv3 1bar
gSK E2bar
gCa HVAbar
gCa LVAstbar
gamma
decay

uniform
uniform
uniform
uniform
uniform
uniform
uniform
uniform
uniform
uniform
uniform
uniform
uniform
uniform
uniform
uniform
uniform
uniform
uniform
uniform

S cm−2
S cm−2
S cm−2
S cm−2
S cm−2
S cm−2
S cm−2
S cm−2
S cm−2
S cm−2
S cm−2

ms
S cm−2
S cm−2
S cm−2
S cm−2
S cm−2

ms

Location Mechanism

Parameter name Distribution Units

global
global
all
all
all
all
apical
apical
apical
somatic
somatic
basal
axonal
axonal
basal
apical
somatic

Ih
Ih
Ih

v init
celsius
g pas
e pas
cm
Ra
ena
ek
cm
ena
ek
cm
ena
ek
gIhbar
gIhbar
gIhbar

mV
◦C
S cm−2
mV
µF cm−2
Ω cm
mV
mV
µF cm−2
mV
mV
µF cm−2
mV
mV
S cm−2
S cm−2
S cm−2

uniform
uniform
uniform
uniform
uniform
uniform
uniform
uniform
uniform
uniform
uniform
uniform
uniform
exp
uniform

0
0
0
0
0
0
0
0
0
0
0
0.0005
20
0
0
0
0
0
0.0005
20

Value

-65
34
3e-05
-75
1
100
50
-85
2
50
-85
2
50
-85
8e-05
8e-05
8e-05

16

4.2.2 Protocols

During the optimisation, the model is evaluated using three square current step stimuli applied
and recorded at the soma. For these protocols, a holding current is also applied during the entire

stimulus, the amplitude of which is the same as was used in the in vitro experiments to keep the
cell at a standardised membrane voltage before the step current injection.

Another stimulus protocol checks for a backpropagating action potential (bAP ) by stimulating
the soma with a very short pulse, and measuring the height and width of the bAP at a location of

660 µm and 800 µm from the soma in the apical dendrite. It is speciﬁed as follows.

for loc_name, loc_distance in [('dendloc1', 660), ('dendloc2', 800)]:

loc = ephys.locations.NrnSomaDistanceCompLocation(

name=loc_name,

soma_distance=loc_distance)

recording = nrpel.recordings.CompRecording(

name='bAP.%s.v' % (loc_name),

location=loc)

4.2.3 Objectives

For each of the four stimuli deﬁned above, a set of eFeatures is calculated (Table 2). These are then
compared with the same features extracted from experimental data. As described in Markram et al.

(2015), experiments were performed that applied these and other protocols to L5PCs in vitro. For
these cells, the same eFeatures were extracted, and the mean µexp and standard deviation σexp
calculated. The bAP target values are extracted from Larkum et al. (2001).

For every feature value fmodel, one objective value is calculated:

objective = (cid:12)(cid:12)(cid:12)(cid:12)

µexp − fmodel

σexp

(cid:12)(cid:12)(cid:12)(cid:12)

4.2.4 Optimisation

For the optimisation of this cell model we needed signiﬁcantly more computing resources. The
goal was to ﬁnd a solution that has objective values that are only several standard deviation

away from the experimental mean. For this we ran 200 generations with an oﬀspring size of the
genetic algorithm of 100 individuals. The evaluation of these 100 individuals was parallelised over

50 CPU cores using SCOOP, and took a couple of hours to run. Figure 6 shows the results of
the optimisation, and Figure 7 shows a comparison of the optimised model to its reference under

Gaussian noise current injection (not used during the optimisation).

4.3 Spike-timing dependent plasticity (STDP) model

The BluePyOpt framework was designed to be versatile and broadly applicable to a wide range of
neuroscientiﬁc optimisation problems. In this use case, we demonstrate this versatility by using

BluePyOpt to optimise the parameters of a calcium-based STDP model (Graupner and Brunel,
2012) to summary statistics from in vitro experiments (Nevian and Sakmann, 2006). That is, we

show how to ﬁt the model to literature data, commonly reported just as mean and SEM of the
amount of potentiation (depression) induced by one or more stimulation protocols.

17

Table 2. List of eFeatures for L5PC example. Locations dend1 and dend2 are respectively
660 µm and 800 µm from the soma in the apical trunk. Depending on the eFeature type the units
can be mV or ms.

Stimulus Location

eFeature

Mean

Std

Step1

soma

Step2

soma

Step3

soma

bAP

dend1
dend2
soma

AHP depth abs
AHP depth abs slow
AHP slow time
AP height
AP width
ISI CV
adaptation index2
doublet ISI
mean frequency
time to ﬁrst spike
AHP depth abs
AHP depth abs slow
AHP slow time
AP height
AP width
ISI CV
adaptation index2
doublet ISI
mean frequency
time to ﬁrst spike
AHP depth abs
AHP depth abs slow
AHP slow time
AP height
AP width
ISI CV
adaptation index2
doublet ISI
mean frequency
time to ﬁrst spike
AP amplitude from voltagebase
AP amplitude from voltagebase
AP height
AP width
Spikecount

-60.3636
-61.1513
0.1599
25.0141
3.5312
0.109
0.0047
62.75
6
27.25
-59.9055
-60.2471
0.1676
27.1003
2.7917
0.0674
0.005
44.0
8.5
19.75
-57.0905
-61.1513
0.1968
19.7207
3.5347
0.0737
0.0055
22.75
17.5
10.5
45
36
25.0
2.0
1.0

2.3018
2.3385
0.0483
3.1463
0.8592
0.1217
0.0514
9.6667
1.2222
5.7222
1.8329
1.8972
0.0339
3.1463
0.7499
0.075
0.0067
7.1327
0.9796
2.8776
2.3427
2.3385
0.0112
3.7204
0.8788
0.0292
0.0015
4.14
0.8
1.36
10
9.33
5.0
0.5
0.01

18

v
.
a
m
o
s
.

1
p
e
t
S

)
V
m

(
 
e
g
a
t
l
o
V

v
.
a
m
o
s
.

2
p
e
t
S

)
V
m

(
 
e
g
a
t
l
o
V

v
.
a
m
o
s
.

3
p
e
t
S

)
V
m

(
 
e
g
a
t
l
o
V

v
.

1
d
n
e
d
.
P
A
b

)
V
m

(
 
e
g
a
t
l
o
V

v
.

2
d
n
e
d
.
P
A
b

)
V
m

(
 
e
g
a
t
l
o
V

v
.
a
m
o
s
.
P
A
b

)
V
m

(
 
e
g
a
t
l
o
V

40
20
0
−20
−40
−60
−80

40
20
0
−20
−40
−60
−80

40
20
0
−20
−40
−60
−80

40
20
0
−20
−40
−60
−80

40
20
0
−20
−40
−60
−80

40
20
0
−20
−40
−60
−80

0

0

0

0

0

0

500

1000

1500

2000

2500

3000

500

1000

1500

2000

2500

3000

500

1000

1500

2000

2500

3000

100

200

300

400

500

600

100

200

300

400

500

600

100

200

300

Time (ms)

400

500

600

s
e
v
i
t
c
e
b
O

j

bAP.soma.Spikecount

bAP.soma.AP_width

bAP.soma.AP_height

bAP.dend2.AP_amplitude_from_voltagebase

bAP.dend1.AP_amplitude_from_voltagebase

Step3.soma.time_to_first_spike

Step3.soma.mean_frequency

Step3.soma.doublet_ISI

Step3.soma.adaptation_index2

Step3.soma.ISI_CV

Step3.soma.AP_width

Step3.soma.AP_height

Step3.soma.AHP_slow_time

Step3.soma.AHP_depth_abs_slow

Step3.soma.AHP_depth_abs

Step2.soma.time_to_first_spike

Step2.soma.mean_frequency

Step2.soma.doublet_ISI

Step2.soma.adaptation_index2

Step2.soma.ISI_CV

Step2.soma.AP_width

Step2.soma.AP_height

Step2.soma.AHP_slow_time

Step2.soma.AHP_depth_abs_slow

Step2.soma.AHP_depth_abs

Step1.soma.time_to_first_spike

Step1.soma.mean_frequency

Step1.soma.doublet_ISI

Step1.soma.adaptation_index2

Step1.soma.ISI_CV

Step1.soma.AP_width

Step1.soma.AP_height

Step1.soma.AHP_slow_time

Step1.soma.AHP_depth_abs_slow

Step1.soma.AHP_depth_abs

(a) Similar to Fig. 5b, with the top ten objective
values found by BluePyOpt, and the best one
plotted darker

(b) Objective scores for the best objective values
found by BluePyOpt.

0.0

0.5

1.0

1.5

2.0

2.5

3.0

3.5

Objective value (# std)

population average
population minimum
population standard deviation

l

e
u
a
v
 
r
e
t
e
m
a
r
a
P
 
g
o

l

5

0

−5

−10

−15

t
_
2
s
T
a
N
_
r
a
b
t
_
2
s
T
a
N
g

l
a
c
i
p
a

1
_
3
v
K
S
_
r
a
b
1
_
3
v
K
S
g

l
a
c
i
p
a

m
I
_
r
a
b
m
I
g

l
a
c
i
p
a

t
_
a
T
a
N
_
r
a
b
t
_
a
T
a
N
g

l
a
n
o
x
a

2
t
E
_
p
a
N
_
r
a
b
2
t
E
_
p
a
N
g

l
a
n
o
x
a

t
s
P
_
K
_
r
a
b
t
s
P
_
K
g

l
a
n
o
x
a

t
s
T
_
K
_
r
a
b
t
s
T
_
K
g

l
a
n
o
x
a

2
E
_
K
S
_
r
a
b
2
E
_
K
S
g

l
a
n
o
x
a

1
_
3
v
K
S
_
r
a
b
1
_
3
v
K
S
g

l
a
n
o
x
a

l
a
n
o
x
a

l
a
n
o
x
a

A
V
H
_
a
C
_
r
a
b
A
V
H
_
a
C
g

t
s
A
V
L
_
a
C
_
r
a
b
t
s
A
V
L
_
a
C
g

2
E
_
s
c
i
m
a
n
y
D
a
C
_
a
m
m
a
g
Parameters

l
a
n
o
x
a

2
E
_
s
c
i
m
a
n
y
D
a
C
_
y
a
c
e
d

l
a
n
o
x
a

t
_
2
s
T
a
N
_
r
a
b
t
_
2
s
T
a
N
g

c
i
t
a
m
o
s

1
_
3
v
K
S
_
r
a
b
1
_
3
v
K
S
g

c
i
t
a
m
o
s

2
E
_
K
S
_
r
a
b
2
E
_
K
S
g

c
i
t
a
m
o
s

A
V
H
_
a
C
_
r
a
b
A
V
H
_
a
C
g

c
i
t
a
m
o
s

t
s
A
V
L
_
a
C
_
r
a
b
t
s
A
V
L
_
a
C
g

c
i
t
a
m
o
s

2
E
_
s
c
i
m
a
n
y
D
a
C
_
a
m
m
a
g

c
i
t
a
m
o
s

2
E
_
s
c
i
m
a
n
y
D
a
C
_
y
a
c
e
d

c
i
t
a
m
o
s

8000

6000

4000

j

s
e
v
i
t
c
e
b
o
 
f
o
 
m
u
S

2000

0

0

20

40

60

80

100

Generation #

(c) Parameter diversity in ﬁnal solution. Param-
eter values for best (blue crosses) and 10 best
individuals (black dots) and all individuals with
all objectives below 5 (grey dots).

(d) Evolution of the L5PC optimisation that
found the model in panel 6a. Plot shows the
minimal, maximal and average scores found in
the consecutive generations of the evolutionary
algorithm.

Figure 6: Results of optimising L5PC model using BluePyOpt.

19

Current

 (nA)

1.5
1.0
0.5
0.0
−0.5
−1.0
−1.5
−2.0

0

Voltage
 (mV)

−100

1000

2000

3000

Time (ms)

4000

5000

Figure 7: Comparison of L5PC model solutions found by BluePyOpt to reference model. Top:
Gaussian noise current injected in the models. Middle: Raster plot of model responses to noise
current injection. Bottom: Voltage response of the models to noise current injection.
In red,
model parameters from Markram et al. (2015) model, in light blue the best 10 individuals found
by BluePyOpt, in dark blue the best individual. Figure as in Pozzorini et al. (2015).

In the set of experiments performed by Nevian and Sakmann (2006), a presynaptic action
potential (AP) is paired with a burst of three post-synaptic APs to induce either long-term po-

tentiation (LTP) or long-term depression (LTD) of the postsynaptic neuron response. The time

diﬀerence ∆t between the presynaptic AP and the postsynaptic burst determines the direction of
change: A burst shortly preceding the presynaptic AP causes LTD, with a peak at ∆t = −50 ms;

conversely, a burst shortly after the presynaptic AP results in LTP, with a peak at ∆t = +10 ms
(Nevian and Sakmann, 2006).

The model proposed by Graupner and Brunel (2012) assumes bistable synapses, with plasticity
of their absolute eﬃcacies governed by post-synaptic calcium dynamics. That is, each synapse

is either in an high-conductance state or a low-conductance state; potentiation and depression
translate then into driving a certain fraction of synapses from the low-conductance state to the

high-conductance state and vice versa; synapses switch from one state to another depending on
the time spent by post-synaptic calcium transients above a potentiation (depression) threshold.

Following Graupner and Brunel (2012), the model is described as

τ

dρ
dt

= −ρ(1 − ρ)(ρ⋆ − ρ) + γp(1 − ρ)Θ[c(t) − θp]

− γdρΘ[c(t) − θd] + Noise(t)

dc
dt

= −

c
τCa

+ Cpre X

δ(t − ti − D) + Cpost X

δ(t − tj)

i

j

(1)

(2)

20

where ρ is the absolute synaptic eﬃcacy, ρ⋆ delimits the basins of attraction of the potentiated
and depressed state, γp (γd) is the potentiation (depression) rate, Θ is the Heaviside function,
θp (θd) is the potentiation (depression) threshold, Noise(t) is an activity dependent noise. The
postsynaptic calcium concentration is described by the process c, with time constant τCa. Cpre
is the calcium transient caused by a presynaptic spike with a delay D to account for the slow
activation of NMDARs, while Cpost is the calcium transient caused by a postsynaptic spike.

For periodic stimulation protocols, such as in Nevian and Sakmann (2006), the synaptic tran-
sition probability can be easily calculated analytically (Graupner and Brunel, 2012), allowing es-

timation of the amount of potentiation (depression) induced by the stimulation protocol without
actually running any neuron simulations. The amount of potentiation (depression) obtained with

diﬀerent protocols in vitro become the objectives of the optimisation.

A small Python module stdputil calculating this model is available in the example section on
the BluePyOpt website. To optimise this model, only an Evaluator class has to be deﬁned that

implements an evaluation function:

class GraupnerBrunelEvaluator(bpop.evaluators.Evaluator):

def __init__(self):

super(GraupnerBrunelEvaluator, self).__init__()

# Graupner-Brunel model parameters and boundaries

# From Graupner and Brunel (2012)

self.graup_params = [('tau_ca', 1e-3, 100e-3),

('C_pre', 0.1, 20.0),

('C_post', 0.1, 50.0),

('gamma_d', 5.0, 5000.0),

('gamma_p', 5.0, 2500.0),

('sigma', 0.35, 70.7),

('tau', 2.5, 2500.0),

('D', 0.0, 50e-3),

('b', 1.0, 100.0)]

self.params = [bpop.parameters.Parameter

(param_name, bounds=(min_bound, max_bound))

for param_name, min_bound, max_bound in self.

graup_params]

self.param_names = [param.name for param in self.params]

self.protocols, self.sg, self.stdev, self.stderr = \

stdputil.load_neviansakmann()

self.objectives = [bpop.objectives.Objective(protocol.prot_id)

for protocol in self.protocols]

def get_param_dict(self, param_values):

return gbParam(zip(self.param_names, param_values))

def compute_synaptic_gain_with_lists(self, param_values):

param_dict = self.get_param_dict(param_values)

21

syn_gain = [stdputil.protocol_outcome(protocol, param_dict) \

for protocol in self.protocols]

return syn_gain

def evaluate_with_lists(self, param_values):

param_dict = self.get_param_dict(param_values)

err = []

for protocol, sg, stderr in zip(self.protocols, self.sg, self.stderr):

res = stdputil.protocol_outcome(protocol, param_dict)

err.append(numpy.abs(sg - res) / stderr)

return err

With the evaluator deﬁned, running the optimisation becomes as simple as:

evaluator = GraupnerBrunelEvaluator()

opt = bpop.optimisations.DEAPOptimisation(GraupnerBrunelEvaluator())

results = opt.run(max_ngen=200)

Figure 8 shows the results of the optimisation.

5 Discussion

BluePyOpt was designed to be a state-of-the-art tool for neuroscientiﬁc model parameter search
problems that is both easy to use for inexperienced users, and versatile and broadly applicable

for power users. Three example use cases were worked through in the text to demonstrate how

BluePyOpt serves each of these user communities.

From a software point of view, this dual goal was achieved by an object oriented architecture

which abstracts away the domain-speciﬁc complexities of search algorithms and simulators, while
allowing extension and modiﬁcation of the implementation and settings of an optimisation. Python

was an ideal implementation language for such an architecture, with its very open and minimal
approach to extending existing implementations. Object oriented programming allows users to

deﬁne new subclasses of existing BluePyOpt API classes with diﬀerent implementations. The duck
typing of Python allows parameters and objectives to have any kind of type, e.g. they don’t have

to be ﬂoating point numbers. In extreme cases, function implementations can even be overwritten
at run time by monkey patching. These features of Python gives extreme ﬂexibility to the user,

which will make BluePyOpt applicable to many use cases.

A common issue arising for users of optimisation software is the conﬁguration of computing

infrastructure. The fact that BluePyOpt is coded in Python, an interpreted language, and provides
Ansible scripts for its installation, makes straightforward to run on diverse computing platforms.

This will give the user the ﬂexibility to pick the computing infrastructure which best ﬁts their

needs, be it their desktop computer, university cluster or temporarily rented cloud infrastructure,
such as oﬀered by Amazon Web Services.

22

Best model
In vitro

2.5

2.0

1.5

1.0

e
g
n
a
h
c
 
e
d
u
t
i
l

p
m
a
 
P
S
P
E

0.5
−100

−80

−60

−40

−20

∆t (ms)

0

20

40

60

(a) Comparison between model and experimen-
tal results; the models match the available in
vitro data and predict the outcome of the miss-
ing points.
In light blue, models generated by
individuals having ﬁtness values within one stan-
dard error of the mean from experimental in
vitro data.
In dark blue, best model, deﬁned
as the closest to all experimental data points.
Experimental data from Nevian and Sakmann
(2006) digitized using Ankit Rohatgi (2015).

population average
population minimum

m
u
i
c
l
a
c

m
u
i
c
l
a
c

m
u
i
c
l
a
c

m
u
i
c
l
a
c

m
u
i
c
l
a
c

m
u
i
c
l
a
c

2

1

0

2

1

0

2

1

0

2

1

0

2

1

0

2

1

0

0.0

0.1

0.2

-90ms

θp
θd

-50ms

θp
θd

-30ms

θp
θd

-10ms

θp
θd

+10ms

θp
θd

+50ms

θp
θd

0.3

Time (s)

0.4

0.5

0.6

(b) Calcium transients generated by the best
model (panel a) for each stimulation protocol.
Potentiation and depression thresholds, θp and
θd respectively, are indicated by the dashed
lines.

60

50

40

30

20

10

j

s
e
v
i
t
c
e
b
o
 
f
o
 
m
u
S

0

0

50

100

Generation #

150

200

(c) Evolution of the STDP optimisation that
found the model in panel 8a. Minimal and aver-
age scores found in the consecutive generations
of the evolutionary algorithm.

Figure 8: Results of STDP ﬁtting.

23

This present paper focuses on the use of BluePyOpt as an optimisation tool.

It is worth
noting that the application domain of BluePyOpt needn’t remain limited to this. The ephys

model abstraction can also be used in validation, assessing generalisation, and parameter sensitivity
analyses. E.g. when applying a map function to an ephys model evaluation function which takes

as input a set of morphologies, one can measure how well the model generalises when applied to
diﬀerent morphologies. The present paper expressly does not touch on issues of generalization

power, overﬁtting, or uniqueness of solution. It is worth now making a few points on the latter.
While BluePyOpt could successfully optimise the three examples, Figures 4b and 6c show a diversity

of solutions giving good ﬁtness values. That is, for these neuron model optimisation problems, the
solutions found are non-unique. This is compatible with the observation that Nature itself also

utilises various and non-unique solutions to provide the required phenotype (Schulz et al., 2007;

Taylor et al., 2009). For other problems solutions could be unique, making BluePyOpt useful e.g.
for extracting parameters for models of synapse dynamics (Fuhrmann et al., 2002).

Of course BluePyOpt is not the only tool available to perform parameter optimisations in neu-
roscience (Druckmann et al., 2007; Van Geit et al., 2007; Bahl et al., 2012; Friedrich et al., 2014;

Carlson et al., 2015; Pozzorini et al., 2015). Some tools provide a Graphical User Interface (GUI),
other tools are written in other languages, or use diﬀerent types of evaluation functions or search

algorithms. We explicitly didn’t make a detailed comparison between BluePyOpt and other tools
because many of these tools are developed for speciﬁc and non-overlapping applications, making

a systematic comparison diﬃcult. This suggests perhaps BluePyOpt’s greatest strength, its broad
applicability relative to previous approaches.

While BluePyOpt signiﬁcantly reduces the domain speciﬁc knowledge required to employ pa-
rameter optimisation strategies, some thought from the user in setting up their problem is still

required. For example, BluePyOpt does in principle allow brute force optimisation of all param-
eters of the L5PC model example, including channel kinetics parameters and passive properties,

but such an approach would almost certainly be unsuccessful. Moreover, when it comes to as-

sessing ﬁtness of models, care and experience is also required to avoid the optimisation getting
caught in local minima, or cannibalizing one objective for another. For neuron models for example,

feature-based approaches coupled with multi-objective optimisation strategies have proven espe-
cially eﬀective (Druckmann et al., 2007). Indeed, even the stimuli and features themselves can be

optimised on theoretical grounds to improve parameter optimisation outcomes (Druckmann et al.,
2011). For these reasons, an important companion of BluePyOpt will be a growing library of

working optimisation examples developed by domain experts for a variety of common use cases,
to help inexperienced users quickly adopt a working strategy most closely related to their speciﬁc

needs.

As these examples library grows, so too will the capabilities of BluePyOpt evolve. Some im-

provements planned for the future include the following:

Support for multi-stage optimisations allowing for example the passive properties of a
neuron to optimised in a ﬁrst stage, prior to optimising the full-active dendritic parameters

in a second phase

Embedded optimisation allowing for example an optimisation of a “current at rheobase”
feature requiring threshold detection during the optimisation using e.g. a binary search. Also,

for integrate-and-ﬁre models such as the adapting exponential integrate-and-ﬁre (Brette and Gerstner,
2005), a hybrid of a global stochastic search and local gradient descent has been shown to be

24

a competitive approach (Jolivet et al., 2008)

Fast pre-evaluation of models to exclude clearly bad parameters before computation time
is wasted on them

Support for evaluation time-outs to protect against optimisations getting stuck in long

evaluations, for example when using NEURON’s CVODE solver, which can occasionally get
stuck at excessively high resolutions.

Support for explicit units to make optimisation scripts more readable, and sharing with

others less error prone.

Although parameter optimisations can require appreciable computing resources, the ability
to share the code of an optimisation through a light-weight script or ipython notebook using

BluePyOpt will improve reproducibility in the ﬁeld. It allows for neuroscientists to exchange code
In the future,
and knowledge about search algorithms that perform well for particular models.

making it possible for users to read and write model descriptions from community standards
(Cannon et al., 2014; Raikov et al., 2011), could further ease the process of plugging in a model

into a BluePyOpt optimisation. By providing the neuroscientiﬁc community with BluePyOpt,
an open source tool to optimise model parameters in Python which is powerful, easy to use and

broadly applicable, we hope to catalyse community uptake of state-of-the-art model optimisation
approaches, and encourage code sharing and collaboration.

Downloads

The source code of BluePyOpt, the example scripts and cloud installation scripts are available

on Github at https://github.com/BlueBrain/BluePyOpt, the former under the GNU Lesser
General Public License version 3 (LGPLv3), and the latter two under a BSD license.

Disclosure/Conﬂict-of-Interest Statement

The authors declare that the research was conducted in the absence of any commercial or ﬁnancial
relationships that could be construed as a potential conﬂict of interest.

Author Contributions

WVG, MG and JDC designed the software and contributed code. WVG, GC, MG and CR de-

signed the examples and contributed code. WVG, EM, MG and GC wrote the manuscript. All:
Conception and design, drafting and revising, and ﬁnal approval.

Acknowledgments

We wish to thank Michael Graupner for his support with the implementation of the calcium-based
STDP model (Graupner and Brunel, 2012) and for fruitful discussions, and Elisabetta Iavarone for

testing the cloud installation functionality.

25

Funding: The work was supported by funding from the EPFL to the Laboratory of Neural
Microcircuitry (LNMC) and funding from the ETH Domain for the Blue Brain Project (BBP).

Additional support was provided by funding for the Human Brain Project from the European
Union Seventh Framework Program (FP7/2007- 2013) under grant agreement no. 604102 (HBP).

The BlueBrain IV BlueGene/Q and Linux cluster used as a development system for this work is
ﬁnanced by ETH Board Funding to the Blue Brain Project as a National Research Infrastructure

and hosted at the Swiss National Supercomputing Center (CSCS).

References

Achard, P. and De Schutter, E. (2006). Complex parameter landscape for a complex neuron model.

PLoS Comput Biol 2, e94

Ankit Rohatgi (2015). Webplotdigitizer. [Online; accessed 26-February-2016; Version: 3.9]

Bahl, A., Stemmler, M. B., Herz, A. V., and Roth, A. (2012). Automated optimization of a reduced

layer 5 pyramidal cell model based on experimental data. Journal of Neuroscience Methods 210,
22 – 34. doi:http://dx.doi.org/10.1016/j.jneumeth.2012.04.006. Special Issue on Computational

Neuroscience

Bhalla, U. S. and Bower, J. M. (1993). Exploring parameter space in detailed single neuron models:
simulations of the mitral and granule cells of the olfactory bulb. Journal of Neurophysiology 69,

1948–1965

Bleuler, S., Laumanns, M., Thiele, L., and Zitzler, E. (2003). PISA — a platform and programming
language independent interface for search algorithms. In Evolutionary Multi-Criterion Optimiza-

tion (EMO 2003), eds. C. M. Fonseca, P. J. Fleming, E. Zitzler, K. Deb, and L. Thiele (Berlin:
Springer), Lecture Notes in Computer Science, 494 – 508

Blue Brain Project (2015). eFEL. https://github.com/BlueBrain/eFEL.

[Online; accessed

16-February-2016]

Blue Brain Project (2016). BluePyOpt. https://github.com/BlueBrain/BluePyOpt.

[Online;

accessed 16-February-2016]

Brette, R. and Gerstner, W. (2005). Adaptive exponential integrate-and-ﬁre model as an eﬀective

description of neuronal activity. Journal of Neurophysiology 94, 3637–3642. doi:10.1152/jn.00686.
2005

Cannon, R. C., Gleeson, P., Crook, S., Ganapathy, G., Marin, B., Piasini, E., et al. (2014). LEMS:

A language for expressing complex biological models in concise and hierarchical form and its use
in underpinning neuroml 2. Frontiers in Neuroinformatics 8. doi:10.3389/fninf.2014.00079

Carlson, K. D., Nageswaran, J. M., Dutt, N., and Krichmar, J. L. (2015). An eﬃcient automated

parameter tuning framework for spiking neural networks. Neuromorphic Engineering Systems
and Applications , 168

Carnevale, N. T. and Hines, M. L. (2006). The NEURON Book (Cambridge University Press).

Cambridge Books Online

26

Carnevale, N. T. and Hines, M. L. (2016). Units used in neuron.

[Online; accessed 26-February-

2016]

Chef (2009). Open Source Chef. http://www.chef.io. [Online; accessed 16-February-2016]

Dalcn, L., Paz, R., and Storti, M. (2005). MPI for Python. Journal of Parallel and Distributed

Computing 65, 1108 – 1115. doi:http://dx.doi.org/10.1016/j.jpdc.2005.03.010

Davison, A. P., Brderle, D., Eppler, J. M., Kremkow, J., Muller, E., Pecevski, D., et al. (2009).
PyNN: a common interface for neuronal network simulators. Frontiers in Neuroinformatics 2.

doi:10.3389/neuro.11.011.2008

DEAP Project (2016). DEAP documentation. [Online; accessed 26-February-2016]

Deb, K., Pratap, A., Agarwal, S., and Meyarivan, T. (2002). A fast and elitist multiobjective

genetic algorithm: NSGA-II. Trans. Evol. Comp 6, 182–197. doi:10.1109/4235.996017

Druckmann, S., Banitt, Y., Gidon, A., Sch¨urmann, F., Markram, H., and Segev, I. (2007). A novel

multiple objective optimization framework for constraining conductance-based neuron models
by experimental data. Frontiers in Neuroscience 1, 7–18. doi:10.3389/neuro.01.1.1.001.2007

Druckmann, S., Berger, T. K., Schrmann, F., Hill, S., Markram, H., and Segev, I. (2011). Eﬀective

stimuli for constructing reliable neuron models. PLoS Comput Biol 7, 1–13. doi:10.1371/journal.
pcbi.1002133

Eliasmith, C., Gosmann, J., and Choo, X. (2016). Biospaun: A large-scale behaving brain model

with complex neurons. arXiv preprint arXiv:1602.05220

Eppler, J. M., Helias, M., Muller, E., Diesmann, M., and Gewaltig, M.-O. (2009). PyNEST: a
convenient interface to the NEST simulator. Frontiers in Neuroinformatics 2. doi:10.3389/neuro.

11.012.2008

Fortin, F.-A., De Rainville, F.-M., Gardner, M.-A., Parizeau, M., and Gagn´e, C. (2012). DEAP:

Evolutionary algorithms made easy. Journal of Machine Learning Research 13, 2171–2175

Friedrich, P., Vella, M., Gulys, A. I., Freund, T. F., and Kli, S. (2014). A ﬂexible, interactive

software tool for ﬁtting the parameters of neuronal models. Frontiers in Neuroinformatics 8.
doi:10.3389/fninf.2014.00063

Fuhrmann, G., Segev, I., Markram, H., and Tsodyks, M. (2002). Coding of temporal information

by activity-dependent synapses. Journal of Neurophysiology 87, 140–148

Goodman, D. F. M. and Brette, R. (2009). The Brian simulator. Frontiers in Neuroscience 3.

doi:10.3389/neuro.01.026.2009

Graupner, M. and Brunel, N. (2012). Calcium-based plasticity model explains sensitivity of synap-

tic changes to spike pattern, rate, and dendritic location. Proceedings of the National Academy
of Sciences 109, 3991–3996

Gurkiewicz, M. and Korngreen, A. (2007). A numerical approach to ion channel modelling using

whole-cell voltage-clamp recordings and a genetic algorithm. PLoS Comput Biol 3, e169

27

Hansen, N. and Ostermeier, A. (2001). Completely derandomized self-adaptation in evolution

strategies. Evol. Comput. 9, 159–195. doi:10.1162/106365601750190398

HashiCorp (2010). Vagrant. http://www.vagrantup.com/. [Online; accessed 16-February-2016]

Hay, E., Hill, S., Sch¨urmann, F., Markram, H., and Segev, I. (2011). Models of neocortical layer
5b pyramidal cells capturing a wide range of dendritic and perisomatic active properties. PLoS

Comput Biol 7, e1002107

Hines, M., Davison, A. P., and Muller, E. (2009). NEURON and Python. Frontiers in Neuroin-

formatics 3. doi:10.3389/neuro.11.001.2009

Hold-Geoﬀroy, Y., Gagnon, O., and Parizeau, M. (2014). Once you SCOOP, no need to fork.

In Proceedings of the 2014 Annual Conference on Extreme Science and Engineering Discovery
Environment (ACM), 60

Huys, Q. J. and Paninski, L. (2009). Smoothing of, and parameter estimation from, noisy biophys-

ical recordings. PLoS Comput Biol 5, e1000379

Izhikevich, E. M. and Edelman, G. M. (2008). Large-scale model of mammalian thalamocortical
systems. Proceedings of the National Academy of Sciences 105, 3593–3598. doi:10.1073/pnas.

0712231105

Jolivet, R., Sch¨urmann, F., Berger, T. K., Naud, R., Gerstner, W., and Roth, A. (2008). The

quantitative single-neuron modeling competition. Biological Cybernetics 99, 417–426. doi:10.
1007/s00422-008-0261-x

Kennedy, J. and Eberhart, R. (1995). Particle swarm optimization. In Neural Networks, 1995.

Proceedings., IEEE International Conference on. vol. 4, 1942–1948 vol.4. doi:10.1109/ICNN.

1995.488968

Kole, M. H., Hallermann, S., and Stuart, G. J. (2006). Single Ih channels in pyramidal neuron
dendrites: properties, distribution, and impact on action potential output. The Journal of

neuroscience 26, 1677–1687

Larkum, M. E., Zhu, J. J., and Sakmann, B. (2001). Dendritic mechanisms underlying the coupling

of the dendritic with the axonal action potential initiation zone of adult rat layer 5 pyramidal
neurons. The Journal of Physiology 533, 447–466. doi:10.1111/j.1469-7793.2001.0447a.x

Markram, H., Muller, E., Ramaswamy, S., Reimann, M. W., Abdellah, M., Sanchez, C. A., et al.
(2015). Reconstruction and simulation of neocortical microcircuitry. Cell 163, 456–492. doi:

10.1016/j.cell.2015.09.029

Merolla, P. A., Arthur, J. V., Alvarez-Icaza, R., Cassidy, A. S., Sawada, J., Akopyan, F., et al.

(2014). A million spiking-neuron integrated circuit with a scalable communication network and
interface. Science 345, 668–673. doi:10.1126/science.1254642

Muller, E., Bednar, J. A., Diesmann, M., Gewaltig, M.-O., Hines, M., and Davison, A. P. (2015).

Python in neuroscience. Frontiers in Neuroinformatics 9. doi:10.3389/fninf.2015.00011

Nevian, T. and Sakmann, B. (2006). Spine Ca2+ signaling in spike-timing-dependent plasticity.

The Journal of Neuroscience 26, 11001–11013

28

Oliphant, T. E. (2007). Python for scientiﬁc computing. Computing in Science & Engineering 9,

10–20. doi:http://dx.doi.org/10.1109/MCSE.2007.58

P´erez, F. and Granger, B. E. (2007).

IPython: a system for interactive scientiﬁc computing.

Computing in Science and Engineering 9, 21–29. doi:10.1109/MCSE.2007.53

Pozzorini, C., Mensi, S., Hagens, O., Naud, R., Koch, C., and Gerstner, W. (2015). Automated
high-throughput characterization of single neurons by means of simpliﬁed spiking models. PLoS

Comput Biol 11, e1004275. doi:10.1371/journal.pcbi.1004275

Puppet Labs (2005). Puppet. http://puppetlabs.com/. [Online; accessed 16-February-2016]

Raikov, I., Cannon, R., Clewley, R., Cornelis, H., Davison, A., De Schutter, E., et al. (2011).

NineML: the network interchange for neuroscience modeling language. BMC Neuroscience 12,
P330–P330. doi:10.1186/1471-2202-12-S1-P330

Ramaswamy, S., Courcol, J.-D., Abdellah, M., Adaszewski, S. R., Antille, N., Arsever, S., et al.

(2015). The neocortical microcircuit collaboration portal: a resource for rat somatosensory cortex.
Frontiers in neural circuits 9

Ray, S. and Bhalla, U. S. (2008). PyMOOSE: interoperable scripting in python for moose. Frontiers

in Neuroinformatics 2. doi:10.3389/neuro.11.006.2008

Red Hat, Inc. (2012). Ansible. http://www.ansible.com/. [Online; accessed 16-February-2016]

Schm¨ucker, N. (2010). Advancing Automated Parameter Constraining on Parallel Architectures

for Neuroscientic Applications. Bachelor thesis, Osnabr¨uck University

Schulz, D. J., Goaillard, J.-M., and Marder, E. E. (2007). Quantitative expression proﬁling of iden-

tiﬁed neurons reveals cell-speciﬁc constraints on highly variable levels of gene expression. Pro-
ceedings of the National Academy of Sciences 104, 13187–13191. doi:10.1073/pnas.0705827104

Sivagnanam, S., Majumdar, A., Yoshimoto, K., Astakhov, V., Bandrowski, A., Martone, M. E.,

et al. (2013). Introducing the neuroscience gateway. In IWSG (Citeseer)

Stefanou, S. S., Kastellakis, G., and Poirazi, P. (2016). Advanced Patch-Clamp Analysis for Neuro-

scientists (New York, NY: Springer New York), chap. Creating and Constraining Compartmental
Models of Neurons Using Experimental Data. 325–343. doi:10.1007/978-1-4939-3411-9 15

Svensson, C.-M., Coombes, S., and Peirce, J. W. (2012). Using evolutionary algorithms

for ﬁtting high-dimensional models to neuronal data. Neuroinformatics 10, 199–218. doi:
10.1007/s12021-012-9140-7

Tarantola, A. (2016). Inverse problem. From MathWorld–A Wolfram Web Resource, created by

Eric W. Weisstein. [Online; accessed 26-February-2016]

Taylor, A. L., Goaillard, J.-M., and Marder, E. (2009). How multiple conductances determine
electrophysiological properties in a multicompartment model. The Journal of Neuroscience 29,

5573–5586

Van Geit, W., Achard, P., and De Schutter, E. (2007). Neuroﬁtter: a parameter tuning package

for a wide range of electrophysiological neuron models. Frontiers in Neuroinformatics 1. doi:
10.3389/neuro.11.001.2007

29

Van Geit, W., De Schutter, E., and Achard, P. (2008). Automated neuron model optimization

techniques: a review. Biol Cybern 99, 241–251

Vanier, M. C. and Bower, J. M. (1999). A comparative survey of automated parameter-search

methods for compartmental neural models. J Comput Neurosci 7, 149–171

Wils, S. and De Schutter, E. (2009). STEPS: modeling and simulating complex reaction-diﬀusion

systems with python. Frontiers in Neuroinformatics 3. doi:10.3389/neuro.11.015.2009

Zaytsev, Y. V. and Morrison, A. (2014). CyNEST: a maintainable Cython-based interface for the

NEST simulator. Frontiers in Neuroinformatics 8. doi:10.3389/fninf.2014.00023

ZeroMQ Project (2007). ZeroMQ. http://zeromq.org. [Online; accessed 16-February-2016]

Zitzler, E. and K¨unzli, S. (2004). Indicator-based selection in multiobjective search. In Parallel
Problem Solving from Nature (PPSN VIII), eds. X. Yao et al. (Berlin, Germany: Springer-Verlag),

832–842

30

