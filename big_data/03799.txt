6
1
0
2

 
r
a

 

M
1
1

 
 
]
P
A

.
t
a
t
s
[
 
 

1
v
9
9
7
3
0

.

3
0
6
1
:
v
i
X
r
a

(cid:96)1 Adaptive Trend Filter via Fast Coordinate Descent

Mario Souto, Joaquim D. Garcia and Gustavo C. Amaral

March 15, 2016

Abstract

Identifying the unknown underlying trend of a given noisy signal is extremely useful for a wide
range of applications. The number of potential trends might be exponential, which can be compu-
tationally exhaustive even for short signals. Another challenge, is the presence of abrupt changes
and outliers at unknown times which impart resourceful information regarding the signal’s charac-
teristics.
In this paper, we present the (cid:96)1 Adaptive Trend Filter, which can consistently identify
the components in the underlying trend and multiple level-shifts, even in the presence of outliers.
Additionally, an enhanced coordinate descent algorithm which exploit the ﬁlter design is presented.
Some implementation details are discussed and a version in the Julia language is presented along
with two distinct applications to illustrate the ﬁlter’s potential.

1 Introduction

There are numerous reasons one would like to infer the underlying dynamics of a signal: denoising;
forecasting; ﬁltering; and even more sophisticated analysis. No matter the objective, determining the
elemental components constituting the signal’s trend is fundamental. However, without further informa-
tion, recognizing the set of correct components in a signal of interest is practically impossible since the
computational complexity renders the eﬀort useless. In this sense, the hypothesis that the underlying
trend has a sparse representation is a vital condition for practical implementations. Nevertheless, the
counting problem associated to ﬁnding the most sparse representation also yields computationally in-
tractable problems. For that reason, the (cid:96)1 norm has become a universal proxy for the sparsity in recent
years. The seminal work of Tibshirani on the lasso [1] has exposed the potential of a simple convex
relaxation as the standard tool when dealing with high-dimensional data. Contemporaneously, Chen,
Donoho and Saunders presented the basis pursuit [2][3], a very similar method in the context of sparse
signal representation in over-complete dictionaries. These works, associated with some theoretical results
[4][5] has led to the development of compressed sensing[6][7] and the improvement of existing ﬁelds such
as machine learning, statistics and control.

Following the success of the lasso, there has been a number of extensions with a variety of properties:
the elastic net [8]; group lasso[9] and the sparse-group lasso [10]; the graphical lasso [11], sparse PCA
[12] and the sparse clustering [13]. Among this diversity of generalizations, we are particularly interested
in the adaptive lasso proposed by Zou [14]. The adaptive lasso uses appropriate weights for penalizing
the coeﬃcients based on a previous estimate which can be given by any consistent estimate, e.g. the
least-squares solution. The main advantage of this methodology is the consistency in variable selection.
In other words, as the number of observations increases, the underlying true model should be successfully
identiﬁed.

Besides identifying the components of the underlying signal’s trend, one is often interested in detecting
abrupt level shifts which can impart useful information about the signal’s characteristics. There are
methods based on searching the signal for observations that can be considered a signiﬁcant level shift
which, usually, are accompanied by a statistical test to validate the presence of the level-shift [15, 16, 17].
Also, an eﬃcient technique for searching potential structural breaks is via the Atheoretical Regression
Tree (ART) [18] in which the signal is sequentially split in search for level-shifts. One of the most recent
methods for identifying level shifts is through the minimization of the Potts functional [19].

The (cid:96)1 Trend Filter [20] successfully tackles the problem of jointly estimating the trend of a time-series
and identifying potential abrupt changes at unknown times. This is accomplished by assuming that the
presence of level-shifts and outliers is sparse and that the signal can be approximated by piece-wise linear

1

trends. In this document, we show that sparsity can be enhanced in the (cid:96)1 Trend Filter by combining
it with the adaptive lasso weighted penalty. We also propose a custom coordinate descent algorithm for
the novel ﬁlter based on the covariance update method [21]. Subsequently, a complexity analysis of the
algorithm is performed.

2

(cid:96)1 Adaptive Trend Filter

The (cid:96)1 Adaptive Trend Filter is comprised by the square of the Euclidean norm working as a loss-
function and a set of adaptive regularizers ri : R × R → R based on the (cid:96)1-norm which, in its traditional
form, reads

µalt
(λ,γ) = arg min
(µ)

1

2||y − µ||2

2 + λ

ri(µi, γ).

n(cid:88)

i=1

The main idea is to both select and estimate the trend’s components that best ﬁts the signal y ∈ Rn
in a parsimonious manner. As in the adaptive lasso, the (cid:96)1 penalty is divided by a initial estimate
for µ. Any consistent estimator is suitable, Zou [14] suggests the ridge regression estimator [22], the
multivariate ordinary least squares estimate (ols) or the entry-wise ols estimator. We are going to focus
on the latter choice due its simplicity and eﬀectiveness. It has been shown in [14] that the use of initial
estimate produces a weight for the regularization, which guarantees that the feature selection has the
oracle properties under mild conditions. In other words, the variable selection works asymptotically as if
the correct set of true components were previously known.

In this paper, we are going to decompose the trend as µ = x + w + u + s, where x corresponds to the
piecewise linear trend, w represents the level of the signal, u is the outlier – or spike – component, and
s is the seasonal component. Each of these components has a particular characterization as well as an
associated regularizer which are described as follows:

• To induce the trend to have as fewer linear segments as possible the second diﬀerence of the piecewise

linear trend is penalized as in:

rx(xi, γ) =

(cid:40) |xi−1−2xi+xi+1|

i +xols

i+1|γ ,

|xols
i−1−2xols
0,

if i ∈ [2, n − 1]
otherwise.

• In order to promote the trend to have potential sparse level-shifts, the following adaptive regularizer

is deﬁned.

rw(wi, γ) =

(cid:40) |wi−wi−1|

i −wols

i−1|γ ,

|wols
0,

if i ∈ [2, n]
otherwise.

• The subsequent regularizer is set to avoid the ﬁlter to overﬁll the trend with outliers.

ru(ui, γ) =

|ui|
|uols
|γ

i

.

• The ﬁlter is charged with the responsibility of selecting the best frequencies from an over-complete

dictionary Ω, where the frequency ω does not need to be known a priori.

To discourage a greedy use of sinusoidal components, it is enought to penalize their coeﬃcients as
in:

si =

(aω sin ωi + bω cos ωi).

rs(s, γ) =

|aω|
ω |γ +
|aols

|bω|
ω |γ .
|bols

(cid:88)

ω∈Ω

(cid:88)

ω∈Ω

2

It is important to highlight that the estimate µalt

(λ,γ) is highly dependent on the previously chosen
penalizers λ and γ which work as controllers for the degree of sparsity imposed by the ﬁlter. In practice,
(λ,γ) is evaluated in a two-dimensional grid (λ, γ) ∈ Λ × Γ which is then selected according
the estimate µalt
to an information criteria such as AIC [23], BIC [24], Mallow’s Cp [25], EBIC [26] or cross-validation [27].
The choice of EBIC (Extended Bayesian Information Criteria) is more suitable for the application of this
paper since it is designed to deal with high-dimensional complexity models [28]. Despite its popularity,
cross-validation cannot be applied in a straightforward manner when samples must obey a certain order,
as in the case of a time-series.

One may also want to establish lower and upper bounds to a component, e.g. it can be useful to set all
level-shifts to be non-negative. This kind of constraint can be directly embedded to the aforementioned
ﬁlter in the form of a linear constraint without adding further computational complexity.

3 Fast Covariance Updating Coordinate Descent Algorithm

Let us redeﬁne the (cid:96)1 Adaptive Trend Filter in its matrix notation
|θi|
|θols
|γ

θalt
(λ,γ) = arg min
(θ)

2||y − Aθ||2

2 + λ

1

,

p(cid:88)

i=1

i

where the traditional and matrix forms are linked by µalt
(λ,γ). In this framework, the matrix
A ∈ Rn×p works as an over-complete dictionary (p > n) of potential components to be chosen by the
(λ,γ) ∈ Rp, expected to be sparse due to the (cid:96)1 adaptive regularization, deﬁnes the
ﬁlter. The vector θalt
convex combination of the elected components that best ﬁts the signal. The dictionary matrix can be
seen as a concatenation of diﬀerent matrices, one for each of the previously deﬁned components of µ,
such that A = [Ax Aw Au As Ac] with:

(λ,γ) = Aθalt





Ax =

Aw =

As =

 ;

0
0
...

. . . 0
0
. . . 0
1
...
...
. . .
n−2 n−3 . . . 0
n−1 n−2 . . . 1
0 0 . . . 0
1 0 . . . 0
...
...
1 1 . . . 0
1 1 . . . 1
|

 ;

. . .

. . .

...

|

|

)
t
1
ω
(
n
i
s

|

)
t
2
ω
(
n
i
s

|

. . .

)
t
k
ω
(
n
i
s

|

Au =

 ; Ac =




 ;

 .

|

)
t
k
ω
(
s
o
c

|

...

. . .

1 0 . . . 0
0 1 . . . 0
...
...
0 0 . . . 1
|

. . .

|

)
t
1
ω
(
s
o
c

|

)
t
2
ω
(
s
o
c

|

. . .

It is worth emphasizing that our algorithm is a matrix-free method, i.e., it does not require the storage of
the coeﬃcients of A since we are only interested in the inner products between the columns of A. Also,
note that the number of columns on the matrices As and Ac, as opposed to the other matrices, is related
to the size of the over-complete set Ω and do not depend on the length of y.

The (cid:96)1 Adaptive Trend Filtering is comprised by two main parts: the convex and diﬀerentiable loss-
function, represented by the Euclidean norm, and the convex and separable adaptive regularizer. As it
has been explored in [29], the coordinate descent method is particularly eﬃcient to the lasso problem.
The main idea behind the coordinate descent is very simple: the algorithm minimizes the multivariate
objective function along each coordinate at a time by cyclically iterating through the coordinates until
convergence is reached.

At any given iteration of the algorithm, the minimization is intended to explain the partial residual
ˆθj – the portion of the signal that was not explained by previous coordinates – through

j(cid:54)=i Aj

r = y −(cid:80)

3

solving a univariate optimization problem. In the case of the lasso problem, the solution to this univariate
optimization problem is analytical and is given by the soft-thresholding operator [30] of the inner product
between the signal of interest and the column of A associated with the active coordinate. The optimal
solution is, thus, given by

(cid:18)

(cid:19)

(cid:18)

(cid:19)(cid:18)|(cid:104) r, Ai(cid:105)|

ˆθalt
(λ,γ),i =

1
σ2
i

sign

(cid:104) r, Ai(cid:105)

− λ
|θols

i

|γ

,

+

n

(cid:19)

where (cid:104) r, Ai(cid:105) = (cid:104) y, Ai(cid:105) −(cid:80)

k(cid:54)=i(cid:104) Ak, Ai(cid:105)ˆθalt
k .

The active set A is responsible for tracking the coeﬃcients that are currently nonzero. For each
tuple (λ, γ), convergence is reached if the active-set does not change after an entire run through the p
coordinates. The grid Λ × Γ is evaluated in a decreasing order so the estimate from the previous cycle
can be used as a warm-start. We shall write the coordinate descent algorithm in its pseudo-code form as
proposed by [21]

Algorithm 1 Coordinate Descent
Compute (cid:104)Aj, y(cid:105) ∀ j = 1, . . . , p
for all (λ, γ) ∈ Λ × Γ do

A = ∅
while A did not converge do

for all i ∈ {1, .., p} do

(cid:104) r, Ai(cid:105) = (cid:104)Ai, y(cid:105) − (cid:88)
(cid:18)|(cid:104) r,Ai(cid:105)|

j∈A:j(cid:54)=i

(λ,γ),i = sign((cid:104) r,Ai(cid:105))
ˆθalt
if |ˆθalt

(λ,γ),i| > 0 and j /∈ A then
A = A ∪ {j}

σ2
i

n

(cid:104)Ai, Aj(cid:105) ˆθalt

(λ,γ),j

(cid:19)

− λ|θols

i

|γ

+

end if
end for
end while

end for

The complexity of the algorithm resides on the number of runs through the p coordinates and on the
inner products between the columns of A, which have an associated cost of O(n2). However, instead
of directly computing the inner products, one can beneﬁt from the particular design of matrix A and
establish analytical formulas for the inner products between pairs of columns to achieve a computational
cost of O(1). This modiﬁcation reduces the most expensive step of the algorithm, which will result in a
faster implementation of the (cid:96)1 Adaptive Trend Filter. Such formulas are available at [31].
Supposing the algorithm is at a particular iteration where k features are present in A, every cycle costs
O(pk) operations despite any possible updates in the active-set, as opposed to the traditional coordinate
descent method presented in [21] where the entry of each new feature into the active set requires an
additional of O(nk) computations. This property is specially advantageous for smaller values of (λ, γ)
when more features tend to be included in A.

4 Applications

4.1 Optical Fiber Fault Detection

Despite the great eﬃciency of ﬁber optics, they are made of very fragile material which can result in
mechanical ruptures. As a consequence, data transmission might be compromised or even interrupted.
Since optical ﬁbers links usually are longer than two kilometers, ﬁnding the location of the rupture with
a maximal accuracy is operationally useful.

Optical Time Domain Reﬂectometry (OTDR) oﬀers a method for detecting and evaluating ﬁber faults
without the need of end-to-end measurements thus providing a valuable tool to support the operation

4

of Fiber-Optic links [32]. A raw OTDR ﬁber signature is usually presented as the optical power at the
detector in logarithmic scale versus the distance, for in that scale it is represented as a descending line
with slope equal to the ﬁber’s attenuation coeﬃcient [33]. In this context, the linear trend reﬂects the
power attenuation along the ﬁber, the step trend indicates the presence of optical losses such as splitters,
connectors or ﬁber defects, the spike trend indicates the presence of reﬂective events and the noise, which
should be ﬁltered out from the original series, may represent a number of sources of imprecision during
data acquisition [33][34].
In this case study, we consider an 8-kilometers point-to-multipoint optical ﬁber communication link
in which a ∼ 3-kilometers feeder ﬁber is directed to a 1 × 32 passive optical splitter representing the
remote distribution node. The objective is to be able to identify any kind of losses – both small (≤ 1.0
dB) and high (≥ 1.0 dB) – after the high-splitting ratio device [35]. In Fig.1, we present the original and
ﬁltered data series when the (cid:96)1 Adaptive Filter is employed.

Figure 1

The elimination of the noise component and the accurate description of the signal in terms of its step

and spike is clear, i.e., the (cid:96)1 adaptive ﬁlter selected the best components to ﬁt the observed signal.

4.2 Wind Farm Power Generation

The main challenge of integrating wind power into existing systems is the volatile nature of wind
power generation.
In this sense, one needs a forecasting model that takes into account the seasonal
and intermittent behavior of wind farms. This task is complicated by the presence of the two following
reasons. Firstly, unscheduled periods of maintenance and repair often produce long sequences of zero
power generation in the data. Simple repairs, like electrical equipment failure, are usually resolved in the
range of one to three days but there might be more complicated issues where the maintenance time can
be extended for weeks. Secondly, wind turbines are designed with cut-in speed, i.e. the minimum amount
of wind speed at which the turbine will generate power, and cut-out speed, the wind speed at which the
turbine is shut down for safety reasons.

5

In this case study, we consider a set of fourteen day hourly measurements of power generation from
a Brazilian wind farm. Each measurement is the mean of power generated in the respective hour in
megawatts.
It is previously known that maintenance takes place between the 27th and the 29th of
March. Nevertheless, this information is not given to the ﬁlter. As can be seen in Fig.2, the hourly power
generation is very seasonal: energy production is very low closest to the earliest hours of the day; and
the generation peak is achieved in the evening.

Figure 2

From the ﬁlter’s estimate result, we see that no slope or spike components were selected. The estimated
trend is comprised mainly by trigonometric features, as it was expected due the presence of strong
seasonality. An over-complete set Ω of forty two potential frequencies were given to the ﬁlter, ranging
from a periodicity of six to forty eight hours. The (cid:96)1 adaptive ﬁlter selected an amount of three cosines
components and seven sines. The most important frequency, i.e., the one associated with the largest
absolute value amplitude, corresponds to the twenty four hours periodicity as it would be expected from
a daily-periodic signal.

Through the step component, the ﬁlter was able to accurately identifying the stage of maintenance.
With a diﬀerence of one hour at the beginning and two hours at the end, a cluster of three consecutive
negative steps followed by a cluster of positive steps represents the abrupt change in the wind power
production. This kind of analysis has two main advantages: ﬁrstly, one can automatize the identiﬁcation
of potential failures and maintenance track record; secondly, the presence of step and spike components
avoids anomalous observations to be erroneous embedded in the trigonometric features.

In this brief study, it is shown that the (cid:96)1 adaptive ﬁlter can correctly identify and ﬁt in-sample
wind power data without the need of any data pre-processing techniques. In future work, it is important
to investigate the potential of the ﬁlter as a forecasting tool in comparison to existing technologies
[36][37][38]. This feature has the potential for predicting renewable energy loads in electrical power grids.
Considering the seasonal behavior of other renewable resources, the authors believe that the same method
can be applied to solar and hydro-based power generation.

6

5 Conclusion

In this paper, we proposed the (cid:96)1 adaptive ﬁlter as signal processing tool able to consistently identify
a sparse set of components comprising the trend of the signal of interest. Some advantages of this
methodology are the robustness to outliers, the lack of tuning parameters or data pre-processing.

An extensive literature in enhancing the coordinate descent for the (cid:96)1-regularized loss minimization
is available. In this sense, potential improvements in the Julia implementation needs to be investigated.
In special, the use of strong rules [39], randomization [40][41] and parallelization [42][43].

Acknowledgment

The authors would like to thank Brazilian Agencies CAPES, CNPq and FAPERJ for ﬁnancial support.

6 Supplemental Material

The authors provide the complete Julia implementation of the herewith presented (cid:96)1 Adaptive Filter
as a digital supplemental material in the ”L1AdaptiveTrendFilter.jl” package [31]. The code is open
source and available through the MIT license. We hope this can foster discussions on how to improve the
implementation in order to make it more useful for the signal processing community.

References

[1] R. Tibshirani, “Regression shrinkage and selection via the lasso,” Journal of the Royal Statistical

Society. Series B (Methodological), pp. 267–288, 1996.

[2] S. Chen and D. Donoho, “Basis pursuit,” in Signals, Systems and Computers, 1994. 1994 Conference

Record of the Twenty-Eighth Asilomar Conference on, vol. 1.

IEEE, 1994, pp. 41–44.

[3] S. S. Chen, D. L. Donoho, and M. A. Saunders, “Atomic decomposition by basis pursuit,” SIAM

review, vol. 43, no. 1, pp. 129–159, 2001.

[4] D. L. Donoho and P. B. Stark, “Uncertainty principles and signal recovery,” SIAM Journal on

Applied Mathematics, vol. 49, no. 3, pp. 906–931, 1989.

[5] J. Tropp et al., “Just relax: Convex programming methods for identifying sparse signals in noise,”

Information Theory, IEEE Transactions on, vol. 52, no. 3, pp. 1030–1051, 2006.

[6] D. L. Donoho, “Compressed sensing,” Information Theory, IEEE Transactions on, vol. 52, no. 4,

pp. 1289–1306, 2006.

[7] E. J. Cand`es, J. Romberg, and T. Tao, “Robust uncertainty principles: Exact signal reconstruction
from highly incomplete frequency information,” Information Theory, IEEE Transactions on, vol. 52,
no. 2, pp. 489–509, 2006.

[8] H. Zou and T. Hastie, “Regularization and variable selection via the elastic net,” Journal of the

Royal Statistical Society: Series B (Statistical Methodology), vol. 67, no. 2, pp. 301–320, 2005.

[9] M. Yuan and Y. Lin, “Model selection and estimation in regression with grouped variables,” Journal

of the Royal Statistical Society: Series B (Statistical Methodology), vol. 68, no. 1, pp. 49–67, 2006.

[10] N. Simon, J. Friedman, T. Hastie, and R. Tibshirani, “A sparse-group lasso,” Journal of Computa-

tional and Graphical Statistics, vol. 22, no. 2, pp. 231–245, 2013.

[11] J. Friedman, T. Hastie, and R. Tibshirani, “Sparse inverse covariance estimation with the graphical

lasso,” Biostatistics, vol. 9, no. 3, pp. 432–441, 2008.

[12] H. Zou, T. Hastie, and R. Tibshirani, “Sparse principal component analysis,” Journal of computa-

tional and graphical statistics, vol. 15, no. 2, pp. 265–286, 2006.

7

[13] E. Elhamifar and R. Vidal, “Sparse subspace clustering: Algorithm, theory, and applications,”
Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 35, no. 11, pp. 2765–2781,
2013.

[14] H. Zou, “The adaptive lasso and its oracle properties,” Journal of the American statistical associa-

tion, vol. 101, no. 476, pp. 1418–1429, 2006.

[15] G. C. Chow, “Tests of equality between sets of coeﬃcients in two linear regressions,” Econometrica:

Journal of the Econometric Society, pp. 591–605, 1960.

[16] R. E. Quandt, “Tests of the hypothesis that a linear regression system obeys two separate regimes,”

Journal of the American statistical Association, vol. 55, no. 290, pp. 324–330, 1960.

[17] R. L. Brown, J. Durbin, and J. M. Evans, “Techniques for testing the constancy of regression
relationships over time,” Journal of the Royal Statistical Society. Series B (Methodological), pp.
149–192, 1975.

[18] W. S. Rea, M. Reale, C. Cappelli, and J. A. Brown, “Identiﬁcation of changes in mean with regression
trees: an application to market research,” Econometric Reviews, vol. 29, no. 5-6, pp. 754–777, 2010.

[19] M. Storath, A. Weinmann, and L. Demaret, “Jump-sparse and sparse recovery using potts function-

als,” Signal Processing, IEEE Transactions on, vol. 62, no. 14, pp. 3654–3666, 2014.

[20] S.-J. Kim, K. Koh, S. Boyd, and D. Gorinevsky, “\ell 1 trend ﬁltering,” SIAM review, vol. 51, no. 2,

pp. 339–360, 2009.

[21] J. Friedman, T. Hastie, H. Hoﬂing, R. Tibshirani et al., “Pathwise coordinate optimization,” The

Annals of Applied Statistics, vol. 1, no. 2, pp. 302–332, 2007.

[22] A. E. Hoerl and R. W. Kennard, “Ridge regression: Biased estimation for nonorthogonal problems,”

Technometrics, vol. 12, no. 1, pp. 55–67, 1970.

[23] H. Akaike, “Factor analysis and aic,” Psychometrika, vol. 52, no. 3, pp. 317–332, 1987.

[24] H. Bhat and N. Kumar, “On the derivation of the bayesian information criterion,” School of Natural

Sciences, University of California, 2010.

[25] C. L. Mallows, “Some comments on c p,” Technometrics, vol. 15, no. 4, pp. 661–675, 1973.

[26] J. Chen and Z. Chen, “Extended bayesian information criteria for model selection with large model

spaces,” Biometrika, vol. 95, no. 3, pp. 759–771, 2008.

[27] R. Kohavi et al., “A study of cross-validation and bootstrap for accuracy estimation and model

selection,” in Ijcai, vol. 14, no. 2, 1995, pp. 1137–1145.

[28] J. Chen and Z. Chen, “Extended bic for small-n-large-p sparse glm,” Statistica Sinica, pp. 555–574,

2012.

[29] J. Friedman, T. Hastie, and R. Tibshirani, “Regularization paths for generalized linear models via

coordinate descent,” Journal of statistical software, vol. 33, no. 1, p. 1, 2010.

[30] D. L. Donoho, “De-noising by soft-thresholding,” Information Theory, IEEE Transactions on,

vol. 41, no. 3, pp. 613–627, 1995.

[31] L1AdaptiveTrendFilter.jl Julia Package – Mario Souto and Joaquim D. Garcia, “Julia source ﬁles,”

https://github.com/joaquimg/L1AdaptiveTrendFilter.jl, accessed: 2016-02-01.

[32] M. K. Barnoski, M. D. Rourke, and M. R. T. Jensen, S. M. and, “Optical time domain reﬂectometer,”

Applied Optics, vol. 16, 1977.

[33] G. Amaral, J. Dias Garcia, L. Herrera, G. Temporao, P. Urban, and J. von der Weid, “Automatic
fault detection in wdm-pon with tunable photon counting otdr,” Lightwave Technology, Journal of,
vol. 33, no. 24, pp. 5025–5031, 2015.

8

[34] J. P. von der Weid, M. H. Souto, J. D. Garcia, and G. C. Amaral, “Adaptive ﬁlter for automatic

identiﬁcation of multiple faults in a noisy otdr proﬁle,” arXiv preprint arXiv:1602.04379, 2016.

[35] L. Y. Herrera, F. Calliari, J. D. Garcia, G. C. Amaral, and J. P. von der Weid, “High resolution auto-
matic fault detection in a ﬁber optic link via photon counting otdr,” in Optical Fiber Communication
Conference. Optical Society of America, 2016.

[36] Z. Huang and Z. Chalabi, “Use of time-series analysis to model and forecast wind speed,” Journal

of Wind Engineering and Industrial Aerodynamics, vol. 56, no. 2, pp. 311–322, 1995.

[37] M. Souto, A. Moreira, A. Veiga, A. Street, J. Dias Garcia, and C. Epprecht, “A high-dimensional varx
model to simulate monthly renewable energy supply,” in Power Systems Computation Conference
(PSCC), 2014.

IEEE, 2014, pp. 1–7.

[38] C. T. Larsen, G. Doorman, and B. Mo, “Joint modelling of wind power and hydro inﬂow for power

system scheduling,” Energy Procedia, vol. 87, pp. 189–196, 2016.

[39] R. Tibshirani, J. Bien, J. Friedman, T. Hastie, N. Simon, J. Taylor, and R. J. Tibshirani, “Strong
rules for discarding predictors in lasso-type problems,” Journal of the Royal Statistical Society: Series
B (Statistical Methodology), vol. 74, no. 2, pp. 245–266, 2012.

[40] O. Fercoq and P. Richt´arik, “Accelerated, parallel, and proximal coordinate descent,” SIAM Journal

on Optimization, vol. 25, no. 4, pp. 1997–2023, 2015.

[41] P. Richt´arik and M. Tak´aˇc, “Iteration complexity of randomized block-coordinate descent methods
for minimizing a composite function,” Mathematical Programming, vol. 144, no. 1-2, pp. 1–38, 2014.

[42] J. K. Bradley, A. Kyrola, D. Bickson, and C. Guestrin, “Parallel coordinate descent for l1-regularized

loss minimization,” arXiv preprint arXiv:1105.5379, 2011.

[43] P. Richt´arik and M. Tak´aˇc, “Parallel coordinate descent methods for big data optimization,” Math-

ematical Programming, pp. 1–52, 2015.

9

