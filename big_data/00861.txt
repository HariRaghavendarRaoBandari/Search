6
1
0
2

 
r
a

M
2

 

 
 
]
T
S
h
t
a
m

.

[
 
 

1
v
1
6
8
0
0

.

3
0
6
1
:
v
i
X
r
a

TRUNCATED RANDOM MEASURES

TREVOR CAMPBELL(cid:63), JONATHAN H. HUGGINS(cid:63), JONATHAN HOW,

AND TAMARA BRODERICK

Abstract. Completely random measures (CRMs) and their normalizations
are a rich source of Bayesian nonparametric (BNP) priors. Examples include
the beta, gamma, and Dirichlet processes. In this paper we detail three classes
of sequential CRM representations that can be used for simulation and pos-
terior inference. These representations subsume existing ones that have pre-
viously been developed in an ad hoc manner for speciﬁc processes. Since
a complete inﬁnite-dimensional CRM cannot be used explicitly for computa-
tion, sequential representations are often truncated for tractability. We provide
truncation error analyses for each type of sequential representation, as well as
their normalized versions, thereby generalizing and improving upon existing
truncation error bounds in the literature. We analyze the computational com-
plexity of the sequential representations, which in conjunction with our error
bounds allows us to study which representations are the most eﬃcient. We
include numerous applications of our theoretical results to popular (normal-
ized) CRMs, demonstrating that our results provide a suite of tools that allow
for the straightforward representation and analysis of CRMs that have not
previously been used in a Bayesian nonparametric context.

1.
Introduction
2. Background
2.1. CRMs and truncation
2.2. Rate measures and likelihoods for some CRMs
3. Three sequential representations
3.1. D-representation
3.2. F-representation
3.3. V-representation
3.4. Stochastic mapping
4. Truncation analysis
4.1. D-representation
4.2. F-representation
4.3. V-representation
4.4. Stochastic mapping
4.5. Hyperpriors
5. Normalized truncation analysis
5.1. F- and V-representations
5.2. D-representation
5.3. Hyperpriors
6. Simulation algorithms and computational complexity
6.1. D- and F-representations
6.2. V-representation

Date: March 3, 2016.
(cid:63)First authorship is shared jointly by T. Campbell and J. H. Huggins.

1

2
5
5
6
8
8
9
10
11
13
14
15
17
20
21
21
23
25
27
27
27
28

2

T. CAMPBELL, J. H. HUGGINS, J. HOW, AND T. BRODERICK

6.3. Summary of CRM results
7. Simulation study
8. Discussion
Appendix A. Poisson point process operations
Appendix B. Technical lemmas
Appendix C. Proofs of sequential representation results
C.1. Correctness of D- and F-representations
C.2. Power-law behavior of F-representations
Appendix D. Proofs of CRM truncation bounds
D.1. Protobound
D.2. D-representation truncation
D.3. F-representation truncation
D.4. V-representation truncation
D.5. Stochastic mapping truncation
D.6. Truncation with hyperpriors
D.7. Comparing D- and F-representation truncations
Appendix E. Proofs of normalized truncation bounds
E.1. F- and V-representation truncation
E.2. D-representation truncation
E.3. Truncation with hyperpriors
Appendix F. Proofs of V-representation sampling results
Appendix G. Additional simulation results
Acknowledgments
References

31
31
34
35
35
38
38
40
43
43
45
46
48
49
49
49
50
52
53
55
56
60
62
62

1. Introduction

In many data sets, we can view the data points as exhibiting a collection of
underlying traits. For instance, each document in the New York Times might
touch on a number of topics or themes, an individual’s genetic data might be a
product of the populations to which their ancestors belonged, a user’s activity on
a social network might be dictated by their varied personal interests, etc. In cases
where the traits are not directly observed, a common approach is to model each
trait as having some frequency or rate in the broader population (Airoldi et al.,
2014). The inferential goal is to learn these rates as well as whether—and to what
extent—each data point exhibits each trait. An additional challenge arises since,
when the traits are unknown a priori, their cardinality is also typically unknown.
Moreover, we expect the number of traits, and thus the complexity of the inference,
to increase in size as the size of the data set grows. In the examples above, we expect
to see a greater diversity of topics as we read more news documents, we expect to
observe a greater number of ancestral populations as we examine more individuals’
genetic data, and we expect there to be a greater number of unique interests and
hobbies represented among a larger population on a social network.

Priors from Bayesian nonparametrics (BNP) provide the desired ﬂexibility.
First, they model the number of latent traits exhibited by a given set of data
as a random variable, which may be learned as part of the inferential procedure.
Second, by imagining a countable inﬁnity of potential traits, these models allow the
number of traits to grow with the size of the data. Note that any individual data
point exhibits only ﬁnitely many traits—though the number itself may be unknown

TRUNCATED RANDOM MEASURES

3

and remain to be inferred. After ﬁnitely many data points have been observed, the
countable inﬁnity of (latent) potential traits guarantees that there are always more
novel traits to be discovered in future data.

In practice, though, it is not possible to store a countable inﬁnity of random
variables in memory or learn the distribution over a countable inﬁnity of variables
in ﬁnite time. Conjugate priors and likelihoods have been developed for these prob-
lems (Orbanz, 2010) that theoretically allow us to avoid the inﬁnite representation
altogether and perform exact Bayesian posterior inference (Broderick et al., 2014).
However, these priors and likelihoods are often just a single piece within a more com-
plex generative model, and ultimately an approximate posterior inference scheme
such as Markov Chain Monte Carlo (MCMC) or variational Bayes (VB) is required.
These approximation schemes often necessitate a full and explicit representation of
the latent variables.

One practical option is to approximate the inﬁnite-dimensional prior with a
ﬁnite-dimensional prior that maintains many of its desirable theoretical properties.
For instance, one might approximate the full inﬁnitude of traits by some represen-
tative ﬁnite subset of traits. It remains to decide how to choose the ﬁnite subset.
In theory, since there are a countable inﬁnity of traits in the full model, we should
be able to enumerate the traits and write a pair (ψk, θk) for each trait, where ψk
is a parameter describing the trait (e.g. a topic that might appear in a document)
and θk is the rate or frequency of the trait. Then the discrete measure

∞(cid:88)

θkδψk

(1.1)

k=1

captures the countable inﬁnity of traits in a sequence indexed by k.

When the θk and ψk are unknown, they are typically treated as random in a
Bayesian model, so Eq. (1.1) gives a random measure. In many cases, the distri-
bution of the random measure can be deﬁned by specifying a sequence of (ideally
simple and/or familiar) distributions for the ﬁnite-dimensional θk and ψk. Such
representations are called sequential or, in certain contexts, stick-breaking. Such
sequential representations have been shown to exist (Broderick et al., 2014) for
completely random measures (CRMs) (Kingman, 1967), a wide class of priors that
provides rates or frequencies for traits and encapsulates such popular models as the
beta process (Hjort, 1990; Kim, 1999) and gamma process (Ferguson and Klass,
1972; Kingman, 1975; Brix, 1999; Titsias, 2008). CRM priors are often paired with
likelihood processes—such as the Bernoulli process (Thibaux and Jordan, 2007),
negative binomial process (Zhou et al., 2012; Broderick et al., 2015), and Poisson
likelihood process (Titsias, 2008)—that assign each data point to a ﬁnite subset
of traits. Sequential representations also exist, sometimes up to normalization,
for normalized completely random measures (NCRMs) (Regazzini et al., 2003; Lijoi
and Pr¨unster, 2010),1 another wide class that provides a distribution over traits and
which includes the Dirichlet process (Ferguson, 1973; Sethuraman, 1994). NCRMs
are typically paired with a discrete likelihood that assigns each data point to one
trait using the distribution speciﬁed by the NCRM. Given a sequential representa-
tion as in Eq. (1.1), a natural way to choose a subset of traits is to keep the ﬁrst
K traits for some ﬁnite K and discard the rest. This approach is called truncation.

1NCRMs are sometimes referred to as normalized random measures with independent incre-

ments (NRMIs) (Lijoi and Pr¨unster, 2010; Regazzini et al., 2003; James et al., 2009).

4

T. CAMPBELL, J. H. HUGGINS, J. HOW, AND T. BRODERICK

In practice, popular BNP priors such as the beta process (Teh et al., 2007;
Doshi-Velez et al., 2009; Paisley et al., 2012b; Thibaux and Jordan, 2007) have
multiple diﬀerent representations of the form in Eq. (1.1). It is not immediately
obvious which sequential representation should be chosen in a given application
and, for that representation, what level of truncation should be chosen. Moreover,
the full diversity of desirable behaviors in BNP models is not achieved by the
beta process alone, so we must examine other (N)CRM priors as well. And while
these other priors have fewer sequential representations available in the existing
literature, the practitioner might wonder whether other, as of yet undiscovered,
sequential representations of these priors might aﬀord better performance in their
application.

We answer these questions in the current work. First, we provide a comprehen-
sive characterization of the diﬀerent types of sequential representations that are
available to the practitioner for (N)CRMs. In doing so, we introduce a number of
new sequential representations for (N)CRMs. By contrast, the development of new
sequential representations has previously happened on an ad hoc, case-by-case ba-
sis. These results alone are useful in a variety of approximation schemes, including
the provision of new slice sampling approaches in MCMC (Neal, 2003).

VB though, at least in its present form, necessitates the use of truncation in
the application of nonparametric Bayesian models (Doshi-Velez et al., 2009; Pais-
ley et al., 2012a). And, in fact, truncated models may be used as part of an
MCMC sampler as well (Ishwaran and James, 2001; Fox et al., 2010; Johnson and
Willsky, 2013). Thus, we provide both theoretical and empirical analyses of the
error induced when truncating the sequential representations we introduce. We
give the truncation error as a function of the prior process, the likelihood pro-
cess, and the level of truncation. While truncation error bounds for (N)CRMs
have been studied extensively, past work has focused on speciﬁc combinations of
(N)CRM priors and likelihoods—in particular, the Dirichlet-multinomial (Sethu-
raman, 1994; Ishwaran and James, 2001; Ishwaran and Zarepour, 2002; Blei and
Jordan, 2006), beta-Bernoulli (Paisley et al., 2012b; Doshi-Velez et al., 2009), and
gamma-Poisson (Roychowdhury and Kulis, 2015) conjugate processes. In the cur-
rent work, we give a much more general formulation of truncation error.

Our results ﬁll in a large gap in the analysis of truncation error. They include
the ﬁrst analysis of truncation error for some sequential representations of the beta
process with Bernoulli likelihood (Thibaux and Jordan, 2007), for the beta process
with negative binomial likelihood (Zhou et al., 2012; Broderick et al., 2015), and
for the normalization of the generalized gamma process (Brix, 1999), the σ-stable
process, and the generalized inverse gamma (Lijoi et al., 2005; Lijoi and Pr¨unster,
2010) with discrete likelihood. Moreover, even when truncation results already exist
in the literature (Ishwaran and James, 2001; Doshi-Velez et al., 2009; Paisley et al.,
2012b; Roychowdhury and Kulis, 2015), we improve on those error bounds by a
factor of two. The reduction arises from our use of the point process machinery of
CRMs; we thereby circumvent the total variation bound used originally by Ishwaran
and James (2001, 2002), upon which most modern truncation analyses are built.

The remainder of this paper is organized as follows. In Section 2, we develop the
conceptual and notational background behind CRMs. In our ﬁrst main theoreti-
cal section, Section 3, we characterize sequential CRM representations by deﬁning
three broad types of construction. The ﬁrst, based on Bondesson (1982), is the most

TRUNCATED RANDOM MEASURES

5

specialized and takes the form of a (stochastic) functional of a homogenous Poisson
point process. The second, which has a ﬁxed rate of atoms per round, is a “decou-
pled” version of the ﬁrst type of representation. The ﬁnal representation includes
numbers of atoms at diﬀerent rates per round (Broderick et al., 2014; James, 2014).
We illustrate the broad applicability and ease of use of the three representations
with examples from popular CRMs. Next, we provide a theoretical analysis of the
truncation error of each of the three CRM constructions in Section 4. We provide
analogous theory for the normalized (NCRM) version of each CRM construction
in Section 5 via an inﬁnite extension of the so-called “Gumbel-max trick” (Gum-
bel, 1954) and standard Poisson point process (Kingman, 1993) machinery. We
determine the complexity of simulating from each representation in Section 6. In
Section 7, we provide simulation experiments, which demonstrate the practicality
of our proposed toolset, as well as an example of how to choose between the three
representations in practice and how to choose the appropriate truncation level for
a given representation. Proofs for all results developed in this paper are provided
in the appendices.

2. Background

2.1. CRMs and truncation. Consider a Poisson point process on R+ := [0,∞)
with rate measure ν(dθ) such that

(cid:90)

(cid:88)

ν(R+) = ∞

and

min(1, θ)ν(dθ) < ∞.

(2.1)

k=1 θk < ∞.

an almost surely ﬁnite sum (cid:80)∞

Such a process generates a countable inﬁnity of values (θk)

∞
k=1, θk ∈ R+, having
In a BNP trait model, we interpret
each θk as the rate or frequency of the k-th trait. Typically, each θk is paired
with a parameter ψk associated with the k-th trait (e.g., a topic in a document,
an ancestral population, or a shared interest on a social network). We assume
throughout that ψk ∈ Ψ for some space Ψ and ψk
i.i.d.∼ G for some distribution
G. Constructing a measure by placing mass θk at atom location ψk results in a
completely random measure (CRM) (Kingman, 1967). As shorthand, we will write
CRM(ν) for the completely random measure generated as just described:

Θ :=

θkδψk ∼ CRM(ν).

(2.2)

k

The trait distribution G is left implicit in our notation as it has no eﬀect on the
results of the present work. Further, the possible ﬁxed-location and deterministic
components of a CRM (Kingman, 1967) are not considered in this work for the pur-
pose of clarity; these components can be added (assuming an atomic deterministic
component, if it exists) and the analysis modiﬁed without undue eﬀort. This prior
on the traits and trait frequencies in Θ is typically combined with a likelihood that
generates trait counts for each data point. Consider any conditional distribution
h(·| θ), which we assume to be a proper probability mass function on N ∪ {0} for

6

T. CAMPBELL, J. H. HUGGINS, J. HOW, AND T. BRODERICK

any θ in the support of ν,2 with the additional requirement that

(cid:90)

(1 − h(0| θ))ν(dθ) < ∞.

(2.3)

Then suppose X1:N := {Xn}N
n=1 is a collection of conditionally independent obser-
vations given Θ. In particular, we say that X1:N are distributed according to the
likelihood process LP(h, Θ), i.e.

Xn :=

xnkδψk

i.i.d.∼ LP(h, Θ),

(2.4)

(cid:88)

k

if xnk ∼ h(x| θk) independently across k and i.i.d. across n. The desideratum
that each Xn expresses a ﬁnite number of traits is encoded in Eq. (2.3). Since the
trait counts are typically latent in a full generative model speciﬁcation, deﬁne the
observed data Yn | Xn
indep∼ f (·| Xn) for a conditional density f with respect to a
∞
measure µ on some space. For instance, if the sequence (θk)
k=1 represents the topic
rates in a document corpus, Xn may capture how many words in document n are
generated from each topic, and Yn might be the observed collection of words for
that document.

Since the sequence (θk)∞

proximation scheme is to deﬁne a truncation ΘK :=(cid:80)

k=1 is countably inﬁnite, it may be diﬃcult to simulate
exactly from this generative model—or perform posterior inference in it. One ap-
k∈K θkδψk , where K is some
ﬁnite subset of the true indices N. The truncation ΘK can be used for exact simu-
lation or in posterior inference—but some error arises from not using the full CRM
Θ. To quantify this error, consider its propagation through the above Bayesian
generative model. Deﬁne Z1:N and W1:N for ΘK analogous to the deﬁnitions of
X1:N and Y1:N for Θ:

Zn | ΘK i.i.d.∼ LP(h, ΘK),

Wn | Zn

indep∼ f (·| Zn),

n = 1, . . . , N.

(2.5)

A standard approach to measuring the distance between Θ and ΘK, to which we
adhere, is to use the L1 metric between the marginal densities pN,∞ and pN,K (with
respect to µ) of the ﬁnal observations Y1:N and W1:N (Ishwaran and James, 2001;
Doshi-Velez et al., 2009; Paisley et al., 2012b):

(cid:107)pN,∞ − pN,K(cid:107)1 :=

|pN,∞(y1:N ) − pN,K(y1:N )| µ(dy1:N ).

(2.6)

y1:N

2.2. Rate measures and likelihoods for some CRMs. A few well-studied
CRMs will be making frequent appearances throughout the remainder of the pa-
per. Their deﬁnitions and relevant notation are reviewed here brieﬂy, along with
typical choices of paired likelihood h(x| θ). Unless otherwise noted, we employ
the generalized versions of each CRM (typically involving 3 free parameters) with-
out explicit nomenclature denoting them as such. For example, we refer to the
ΓP(γ, λ, d) as a gamma process, though it is typically referred to in the literature
as a generalized gamma process (Brix, 1999; Lijoi and Pr¨unster, 2010). This is
simply for brevity; it will be clear in context which parameterization is being used.
It should also be noted that the three processes listed below have been generalized

2Likelihoods with support in R may also be considered. As long as Eq. (2.3) holds, all results
except those relating to size-biased V-representations still hold, since they only rely upon the
behavior of h(0 | θ).

(cid:90)

TRUNCATED RANDOM MEASURES

7

by exponential family conjugate processes (Broderick et al., 2014), and that it is
straightforward to apply a similar generalization to the results of the present work.

2.2.1. Beta process. The beta process (Teh and G¨or¨ur, 2009; Broderick et al., 2012),
denoted BP(γ, α, d), with discount parameter d ∈ [0, 1), concentration parameter
α > −d, and mass parameter γ > 0, is a CRM with rate measure

ν(dθ) = γ

Γ(α + 1)

Γ(1 − d)Γ(α + d)

1 [θ ≤ 1] θ−1−d(1 − θ)α+d−1dθ.

(2.7)

Setting d = 0 yields the standard beta process (Hjort, 1990; Thibaux and Jordan,
2007). The beta process is often paired with a Bernoulli likelihood,

h(x| θ) = 1 [x ≤ 1] θx(1 − θ)1−x,

or a negative binomial likelihood with number of failures s ∈ N,

(cid:18)x + s − 1
(cid:19)

x

h(x| θ) =

(1 − θ)sθx.

(2.8)

(2.9)

2.2.2. Beta prime process. The beta prime process (Broderick et al., 2014, 2015),
denoted BPP(γ, α, d), with discount parameter d ∈ [0, 1), concentration parameter
α > −d, and mass parameter γ > 0, is a CRM with rate measure

ν(dθ) = γ

Γ(α + 1)

Γ(1 − d)Γ(α + d)

θ−1−d(1 + θ)−αdθ.

The beta prime process is often paired with an odds Bernoulli likelihood,

h(x| θ) = 1 [x ≤ 1] θx(1 + θ)−1.

(2.10)

(2.11)

2.2.3. Gamma process. The gamma process (Brix, 1999), denoted ΓP(γ, λ, d), with
discount parameter d ∈ [0, 1), scale parameter λ > 0, and mass parameter γ > 0,
is a CRM with rate measure

ν(dθ) = γ

λ1−d
Γ(1 − d)

θ−d−1e−λθdθ.

(2.12)

Setting d = 0 yields the standard gamma process (Ferguson and Klass, 1972; King-
man, 1975; Titsias, 2008). The gamma process is often paired with a Poisson
likelihood,

h(x| θ) =

e−θ.

θx
x!

(2.13)

Throughout the present work, we use the rate parameterization of the gamma
distribution (to match the gamma process parameterization), for which the density
is given by

Gam(x; a, b) =

ba
Γ(a)

xa−1e−bx.

(2.14)

8

T. CAMPBELL, J. H. HUGGINS, J. HOW, AND T. BRODERICK

3. Three sequential representations

k=1

(cid:80)Ck

simple form(cid:80)∞
(cid:80)∞

Sequential representations are at the heart of the study of truncated CRMs.
They provide an iterative method that can be terminated at any point to yield a
ﬁnite approximation to the inﬁnite process, where the choice of termination point
determines the accuracy of the approximation. Thus, the natural ﬁrst step in pro-
viding a coherent treatment of truncation analysis is to do the same for sequential
representations. In past work, three classes of sequential representation have been
used, though the nomenclature we use for them is our own. The ﬁrst take the
k=1 θkδψk ; since there are a deterministic number of atoms for each
outer sum index, we call the ﬁrst type D-representations. The next are of the form
i=1 θkiδψki , where the Ck are i.i.d. Poisson random variables. Since the
Ck are from a f ixed distribution, we call the second type F-representations. The
ﬁnal class are of the same form as F-representations; however, the Ck are inde-
pendent (but not identically distributed) Poisson random variables. Since the Ck
have varying distributions, we call the third type V-representations. This section
examines a D-representation originating with Bondesson (1982), introduces two
novel classes of F-representations, discusses a V-representation due to Broderick
et al. (2014) and James (2014), and places past sequential representations in these
three categories. Finally, we discuss the use of a stochastic mapping procedure,
which is especially useful for obtaining power-law F-representations. Proofs for the
results presented in this section may be found in Appendix C. The proofs in Appen-
dix C as well as later reults make liberal use of the Poisson point process calculus
summarized in Appendix A and some technical lemmas given in Appendix B.

representations. For any c > 0, let Γck = (cid:80)k

of D-
i.i.d.∼ Exp(c), be the
ordered jumps of a homogenous Poisson point process with rate c. Ferguson and
Klass (1972) proposed an elegant and general approach for representing CRMs
with arbitrary rate measures:
θkδψk ∼ CRM(ν),

θk := inf {x : ν ([x,∞)) ≤ Γ1k} .

3.1. D-representation. We

(cid:96)=1 Ec(cid:96), Eck

construction

ﬁrst

investigate

the

(3.1)

with

Θ =

However, inverting the function ν ([x,∞)) is analytically intractable except in a
few cases. A more specialized approach, originating with Bondesson (1982), leads
to tractable representations in a wider variety of settings, as we shall see in the
examples below. For c > 0 and g a distribution or density on R+, we say Θ has a
Bondesson D-representation and write Θ ← B-DRep(c, g) if

∞(cid:88)

k=1

∞(cid:88)

Θ =

θkδψk ,

with

θk := Vke−Γck ,

i.i.d.∼ g,

Vk

i.i.d.∼ G.

ψk

(3.2)

k=1

The following result shows that Bondesson D-representations can be constructed
for a certain class of CRM rate measures.

Theorem 3.1 (Bondesson D-representation). Let ν(dθ) = ν(θ)dθ be a rate mea-
limθ→∞ θν(θ) = 0, and
sure satisfying Eq. (2.1).
cν := limθ→0 θν(θ) < ∞, then gν(v) := −c−1

dv [vν(v)] is a density on R+ and

If θν(θ) is nonincreasing,

d

ν

Θ ← B-DRep(cν, gν)

implies Θ ∼ CRM(ν).

(3.3)

TRUNCATED RANDOM MEASURES

9

We oﬀer a novel, rigorous proof of Theorem 3.1 in Appendix C using the strategy
introduced by Banjevic et al. (2002). The Bondesson D-representation approach is
not as general as those available for F- and V-representations, and can be diﬃcult
to analyze theoretically due to the coupling between the atoms, but it does tend
to produce very simple representations with small truncation error (cf. Sections 4
and 7).
Example 3.1 (Beta process, BP(γ, α, 0), α ≥ 1). If α > 1 and d = 0, then θν(θ) =
γα(1 − θ)α−11[θ ≤ 1] is non-increasing, cν = limθ→0 θν(θ) = γα, and gν(v) =
(α − 1)(1 − v)α−2 = Beta(v; 1, α − 1). Thus, it follows from Theorem 3.1 that if
Θ ← B-DRep(γα, Beta(1, α − 1)), then Θ ∼ BP(γ, α, 0).
In the case of α = 1,
gν(v) = δ1, so Vk ≡ 1. Since exp(−Eck) ∼ Beta(1, c), the representation used in
Teh et al. (2007) is equivalent to the Bondesson D-representation for BP(γ, 1, 0).

Example 3.2 (Gamma process, ΓP(γ, λ, 0)). The following representation for the
gamma process with d = 0 was described by Bondesson (1982) and Banjevic et al.
(2002). Since θν(θ) = γλe−λθ is non-increasing and cν = limθ→0 θν(θ) = γλ,
we obtain gν(v) = λe−λv = Exp(v; λ). Thus, it follows from Theorem 3.1 that if
Θ ← B-DRep(γλ, Exp(λ)), then Θ ∼ ΓP(γ, λ, 0).
Example 3.3 (Beta prime process, BPP(γ, α, 0)). If d = 0, then θν(θ) =
γα(1 + θ)−α is non-increasing and cν = limθ→0 θν(θ) = γα, so gν(v) =
(v; 1, α). Thus, it follows from Theorem 3.1 that if Θ ←
α(1 + v)−α−1 = Beta
B-DRep(γα, Beta

(1, α)), then Θ ∼ BPP(γ, α, 0).

(cid:48)

(cid:48)

3.2. F-representation. For Bondesson D-representations, the atom weights are
coupled by the Poisson process Γck. We now introduce a novel alternative that
decouples the atom weights. For c > 0, ξ > 0 and g a distribution or density
on R+, we say Θ has a decoupled Bondesson F-representation and write Θ ←
B-FRep(c, g, ξ) if

∞(cid:88)

Ck(cid:88)

Θ =

θkiδψki , with Ck

i.i.d.∼ Poiss(c/ξ),

θki := Vkie−Tki ,

(3.4)

k=1

i=1

indep∼ Gam(k, ξ),

Tki

i.i.d.∼ g,

Vki

i.i.d.∼ G.

ψki

The next result shows that if a CRM rate measure admits a Bondesson D-
representation then it also admits a decoupled Bondesson F-representation.

Theorem 3.2 (Decoupled Bondesson F-representation). Let ν(dθ) = ν(θ)dθ be a
rate measure satisfying the conditions in Theorem 3.1, and let cν and gν have the
same deﬁnitions as in Theorem 3.1. Then for any ﬁxed ξ > 0,

Θ ← B-FRep(cν, gν, ξ)

implies Θ ∼ CRM(ν).

(3.5)

The proof of Theorem 3.2 in Appendix C generalizes the arguments from Paisley
et al. (2010) and Roychowdhury and Kulis (2015). The free parameter ξ controls
the number of atoms generated for each outer sum index k. Discussion of the
principled selection of ξ via optimization is delayed until Section 6, as it requires the
truncation error bounds developed in Section 4 and simulation complexity results
found in Section 6.

Example 3.4 (Beta, gamma, and beta prime processes). Arguments paralleling
those made in Examples 3.1 and 3.2 show that the Paisley et al. (2010) BP(γ, α, 0)

10

T. CAMPBELL, J. H. HUGGINS, J. HOW, AND T. BRODERICK

and the Roychowdhury and Kulis (2015) ΓP(γ, λ, 0) representations follow directly
from applications of Theorem 3.2. Constructing a BPP(γ, α, 0) decoupled Bondes-
son F-representation is straightforward using Theorem 3.2 and the arguments in
Example 3.3.

An alternative class of F-representations leads to “power-law” (or “heavy-
tailed”) CRMs. For γ > 0, 0 ≤ d < 1, α > −d, and g a distribution or density on
R+, we say Θ has a power-law F-representation and write Θ ← PL-FRep(γ, α, d, g)
if

∞(cid:88)

Ck(cid:88)

k−1(cid:89)

Θ =

θkiδψki ,

with Ck

i.i.d.∼ Poiss(γ),

θki := VkiUkik

(1 − Ukij),

k=1

i=1

j=1

Urij

indep∼ Beta(1 − d, α + jd),
(3.6)

i.i.d.∼ g,

Vki

i.i.d.∼ G.

ψki

The name of this representation arises from the fact that it exhibits Types I and
II power-law behavior (Broderick et al., 2012) under mild conditions, shown by The-
orem C.1 in Appendix C. A method for constructing power-law F-representations
corresponding to diﬀerent CRMs using stochastic mapping will be discussed in
Section 3.4.

Example 3.5 (Beta process, BP(γ, α, d)). The decoupled Bondesson F-
representation for BP(γ, α, 0) from Paisley et al. (2010) was extended by Brod-
erick et al. (2012) to the BP(γ, α, d) setting. The Broderick et al. (2012) con-
struction for the BP(γ, α, d) is in fact the “trivial” power-law F-representation
PL-FRep(γ, α, d, δ1).

3.3. V-representation. The ﬁnal sequential CRM representation we present is
due to Broderick et al. (2014). For rate measure ν(dθ) and conditional distribution
h(x| θ) on N ∪ {0}, we say Θ has a size-biased V-representation and write Θ ←
SB-VRep(ν, h) if

θkxiδψkxi , with Ckx

indep∼ Poiss (λkx) ,

i.i.d.∼ G,

ψkxi

k=1

x=1

i=1

θkxi

(cid:90)

h(0|θ)k−1h(x| θ)ν(dθ),

indep∼ 1
λkx
h(0| θ)k−1h(x| θ)ν(dθ).

λkx :=

(3.7)

If the rate measure ν and the likelihood h are selected to be a conjugate expo-
nential family, λkx can be computed analytically and θkxi can be sampled using
standard techniques. Under mild conditions, the size-biased V-representation has
the distribution of a CRM with rate measure ν, as speciﬁed by Theorem 3.3.

Theorem 3.3 (Size-biased V-representation, Broderick et al. (2014)). Let ν(dθ)
be a rate measure and let h(x| θ) be a proper distribution on N ∪ {0}, where h and
ν satisfy the conditions in Eqs. (2.1) and (2.3). Then

Θ ← SB-VRep(ν, h)

implies Θ ∼ CRM(ν).

(3.8)

∞(cid:88)

∞(cid:88)

Ckx(cid:88)

Θ =

TRUNCATED RANDOM MEASURES

11

The proof of this result, and its applications, are discussed extensively in Brod-
erick et al. (2014) and so will not be repeated here. Similar ideas and types of
representations were recently explored in James (2014).

(cid:90)

Example 3.6 (Beta process BP(γ, α, 0)). Setting ν to a beta process rate measure
and h to a Bernoulli likelihood, we have

Γ(α + 1)

(1 − θ)k−1θ · θ−1(1 − θ)α−1dθ =

Γ(α)

λk1 = γ

(3.9)
and θkxi ∼ Beta(1, α + k − 1), demonstrating that the construction due to Thibaux
and Jordan (2007) is a special case of the size-biased V-representation for
BP(γ, α, d).

α + k − 1

,

γα

3.4. Stochastic mapping. We next describe how one CRM can be obtained as the
transformation of another CRM. The primary application of this technique will be
to obtain power-law F-representations for processes other than the beta process by
transforming the power-law F-representation for BP(γ, α, d). This transformation
approach, which we call stochastic mapping, is a powerful technique that takes
advantage of operations on the underlying Poisson point process. Indeed, given a
truncation error bound for any CRM, the stochastic mapping of that CRM inherits a
transformed error bound; this is discussed further in Section 4.4. The next lemma,
which forms the basis of stochastic mapping, is a well-known result that follows
immediately from applying Lemmas A.1 and A.4 to CRMs:

Lemma 3.4 (CRM stochastic mapping). Let Θ = (cid:80)∞

k=1 θkδψk ∼ CRM(ν). For

any probability kernel κ(θ, du),

ukδψk ∼ CRM(νκ),

uk | θk ∼ κ(θk,·),

(3.10)

κ(Θ) :=

where

κ(θ, du)ν(dθ).

(3.11)

∞(cid:88)

k=1

νκ(du) :=

(cid:90)

∞(cid:88)

k=1

(cid:12)(cid:12)(cid:12)(cid:12) dτ−1

du

(cid:12)(cid:12)(cid:12)(cid:12) .

If κ(θ, du) = δτ (θ)(du) is deterministic, τ is invertible, and ν(dθ) = ν(θ)dθ, then
uk = τ (θk) and

τ (Θ) :=

τ (θk)δψk ∼ CRM(ντ ),

(3.12)

ντ (du) := ν(τ−1(u))

(3.13)
Thus, given a kernel κ and a representation for Θ ∼ CRM(ν), we immediately
obtain a representation for κ(Θ) ∼ CRM(νκ). As detailed in the following example,
to obtain novel power-law F-representations, we use the heuristic that identities for
exponential family random variables can be extended to exponential CRMs.

(u)

Example 3.7 (F-representation for ΓP(γ, λ, d)). We will transform the beta pro-
cess BP(γ, α, d) into the gamma process ΓP(γ, λ, d). For arbitrary a, b > 0, let
Ba,b

indep∼ Gam(a, b). We use the well-known identity

indep∼ Beta(a, b) and Ga,b

D
= Ga+b,λBa,b

Ga,λ

(3.14)

12

T. CAMPBELL, J. H. HUGGINS, J. HOW, AND T. BRODERICK

extended to a < 0. We make the identiﬁcations for the “distributions” of the
atoms by examining the rate measures of CRMs. For ΓP(γ, λ, d), the rate measure
is ν(dθ) ∝ θ−d−1e−λθdθ, which corresponds to an improper gamma distribution
with parameters −d and λ. For BP(γ, α, d), the rate measure is ν(dθ) ∝ θ−d−1(1−
θ)α+d−1dθ, which corresponds to an improper beta distribution with parameters −d
and α + d. So in the case of the gamma and beta processes, the identity Eq. (3.14)
with a = −d, b = α + d, and λ = α becomes

(3.15)

(3.16)

(3.17)

G−d,α

D
= Gα,αB−d,α+d.

Thus, we apply the stochastic mapping

θ (cid:55)→ u = Gθ,
which corresponds to the kernel

G ∼ Gam(α, α),

κ(θ, du) = Gam(u/θ; α, α)θ−1du.
Using Lemma 3.4 and change of variable w = u(θ−1 − 1),
θ−α−d−1e−αuθ−1

νκ(du) = γα

uα−1

(cid:90)

αα

Γ(1 − d)Γ(α + d)

(cid:90)

(1 − θ)α+d−1 dθ du (3.18)

u−1−de−αu

wα+d−1e−αw dw du

(3.19)

αα

= γα

Γ(1 − d)Γ(α + d)
α1−d
Γ(1 − d)

u−1−de−αu du.

= γ

(3.20)
Thus, we have shown formally that if Θ ∼ BP(γ, α, d) for α > 0, then κ(Θ) ∼
ΓP(γ, α, d). Applying the mapping to PL-FRep(γ, α, d, δ1), the power-law F-
representation for BP(γ, α, d) (Example 3.5), we arrive at a novel power-law F-
representation

Θ ← PL-FRep(γ, λ, d, Gam(λ, λ))

implies Θ ∼ ΓP(γ, λ, d),

(3.21)

which, to the best knowledge of the authors, was not previously available in the
literature.
Example 3.8 (F-representation for BPP(γ, α, d)). Letting B(cid:48)
the beta prime process can be obtained using the parametric identity

indep∼ Beta

(a, b),

a,b

(cid:48)

B(cid:48)

D
= Ga,1/Gb,1.

(3.22)
For BPP(γ, α, d), the rate measure is ν(dθ) ∝ θ−d−1(1+θ)−αdθ, which corresponds
to an improper beta prime distribution with parameters −d and α + d. So choosing
a = −d and b = α + d, the identity becomes

a,b

B(cid:48)
−d,α+d

D
= G−d,1/Gα+d,1.

Thus, applying Lemma 3.4 to the stochastic mapping

θ (cid:55)→ θ/G,

G ∼ Gam(α + d, 1)

(3.23)

(3.24)

and the power-law F-representation for ΓP(γ, 1, d) from Example 3.7 yields the
novel power-law F-representation
Θ ← PL-FRep(γα, 1, d, Beta
(cid:48)

implies Θ ∼ BPP(γ, α, d).

(1, α + d))

(3.25)

TRUNCATED RANDOM MEASURES

13

Example 3.9 (D-representation for BP(γ, α, 0)). A ﬁnal application of stochastic
mapping is to obtain a D-representation for BP(γ, α, 0) for all α > 0 by using the
identity

D
=

Ba,b

Ga,λ

Ga,λ + Gb,λ

(3.26)

and choosing a = 0 and b = λ = α. Applying Lemma 3.4 to the stochastic mapping

θ (cid:55)→ θ/(θ + G),

G ∼ Gam(α, α)

(3.27)

and the Bondesson D-representation for ΓP(γ, α, 0) yields

∞(cid:88)

θkδψk ∼ BP(γ, α, 0), with

θk := (1 + GkV −1

k eΓk,αγ )−1, ψk

i.i.d.∼ G, (3.28)

k=1

i.i.d.∼ Gam(α, α),

Gk

i.i.d.∼ Exp(α).

Vk

4. Truncation analysis

Each of the sequential representations developed in Section 3 shares a common
structural element—an outer inﬁnite sum—which is responsible for generating a
countably inﬁnite number of atoms in the CRM. In this section, we terminate these
outer sums at a ﬁnite truncation level K ∈ N, resulting in a truncated CRM ΘK
possessing a ﬁnite number of atoms. We then develop upper bounds on the error
induced by this truncation procedure. All of the truncated CRM error bounds in
this section rely on Lemma 4.1, which is a tightening (by a factor of two) of the
bound in Ishwaran and James (2001, 2002).3
Lemma 4.1 (CRM Protobound). Let Θ ∼ CRM(ν). For any truncation ΘK, if
(4.1)

Xn | Θ i.i.d.∼ LP(h, Θ),
Yn | Xn
indep∼ f (·| Xn),

Zn | ΘK
Wn | Zn

i.i.d.∼ LP(h, ΘK),
indep∼ f (·| Zn),

(4.2)
then, with pN,∞ and pN,K denoting the marginal densities of Y1:N and W1:N , re-
spectively,

(cid:107)pN,∞ − pN,K(cid:107)1 ≤ 1 − P (supp(X1:N ) ⊆ supp(ΘK)) ,

(4.3)

1
2

The proof of Lemma 4.1 as well as the other results in this section can be found
in Appendix D. Throughout this section, for a given likelihood model h(x| θ) we
deﬁne π(θ) := h(0| θ) for notational brevity. All of the provided truncation results
use the generative model in Lemma 4.1, and specify the error bound B(N, K) in
terms of the L1 error between the marginal densities of Y1:N and W1:N ,

(cid:107)pN,∞ − pN,K(cid:107)1 ≤ B(N, K).

1
2

(4.4)

The asymptotic behavior of truncation error bounds is speciﬁed with tilde notation,

a(K) ∼ b(K), K → ∞ ⇐⇒

lim
K→∞

a(K)
b(K)

= 1.

(4.5)

Standard properties of asymptotic equivalence, which we mostly use implicitly, are
given in Lemma B.8.

3See Lemma D.1 in Appendix D for the generalization of Lemma 4.1 to arbitrary random

measures.

14

T. CAMPBELL, J. H. HUGGINS, J. HOW, AND T. BRODERICK

4.1. D-representation. For the Bondesson D-representation, the truncated CRM
has the form

K(cid:88)

ΘK :=

θkδψk .

(4.6)

The following result provides a truncation error bound, speciﬁes its range, and
guarantees that as K → ∞ the bound decreases to zero.

k=1

Theorem 4.2 (D-representation truncation error). The error in approximating
Θ ← B-DRep(gν, cν) with its truncation ΘK satisﬁes

(cid:26)

(cid:90) ∞

(cid:27)
(cid:0)1 − E(cid:2)π(ve−GK )(cid:3)(cid:1) ν(dv)

B(N, K) = 1 − exp

−N

,
where GK ∼ Gam(K, cν) and G0 = 0. Furthermore, the bound satisﬁes

0

(4.7)

0 ≤ B(N, K) ≤ 1

(4.8)
Example 4.1 (Beta-Bernoulli process, BP(γ, α, 0), α ≥ 1). Noting that π(θ) =
1 − θ and GK ∼ Gam(K, γα), we have

lim
K→∞ B(N, K) = 0.

and

(cid:90) ∞

0

(cid:90) 1
(cid:0)1 − E(cid:2)π(ve−GK )(cid:3)(cid:1) ν(dv) = γα E[e−GK ]
(cid:19)K
(cid:18) γα
(cid:40)
(cid:18) γα

(cid:107)pN,∞ − pN,K(cid:107)1 ≤ 1 − exp

−N γ

1 + γα

= γ

0

1
2

γα + 1

(1 − v)α−1dv

(4.9)

(4.10)

.

(cid:19)K(cid:41)

.

(4.11)

Applying the error bound yields

The result in Eq. (4.11) generalizes that in Doshi-Velez et al. (2009), which only
applies in the case of α = 1.
Example 4.2 (Gamma-Poisson process, ΓP(γ, λ, 0)). Since π(θ) = e−θ and GK ∼
Gam(γλ).

(cid:90) ∞

0

(cid:20)(cid:90) ∞
(cid:0)1 − E(cid:2)π(ve−GK )(cid:3)(cid:1) ν(dv) = γλ E
= γλ E(cid:2)log(1 + e−GK /λ)(cid:3)
≤ γ E(cid:2)e−GK(cid:3)
(cid:19)K
(cid:18) γλ

0

= γ

1 + γλ

.

(1 − e−ve−GK )v−1e−λvdv

(cid:21)

(4.12)

(4.13)

(4.14)

(4.15)

The second equality follows, for example, by using the power series for the expo-
nential integral (Abramowitz and Stegun, 1964, Chapter 5) and the upper bound
follows because log(1 + x) ≤ x for x ≥ 0. Applying the error bound yields

(cid:107)pN,∞ − pN,K(cid:107)1 ≤ 1 − exp

1
2

−N γ

.

(4.16)

(cid:40)

(cid:19)K(cid:41)

(cid:18) γλ

1 + γλ

TRUNCATED RANDOM MEASURES

15

Example 4.3 (Beta prime-odds Bernoulli process, BPP(γ, α, 0)). In this case
π(θ) = (1 + θ)−1 and GK ∼ Gam(K, γα), so

(cid:0)1 − E(cid:2)π(ve−GK )(cid:3)(cid:1) ν(dv) = γα E

(cid:20)

(cid:90) ∞

0

e−GK

(1 + v)−α(1 + ve−GK )−1dv

(cid:21)

0

(cid:90) ∞
(cid:90) ∞
(cid:19)K(cid:90) ∞
(cid:19)K

0

0

,

≤ γα E[e−GK ]

(cid:18) γα
(cid:18) γα

1 + γα

1 + γα

≤ γα

≤ γ

(4.17)
(1 + v)−α(1 + vE[e−GK ])−1dv
(4.18)

(1 + v)−α−1dv

(4.19)

(4.20)

where the ﬁrst upper bound follows from Jensen’s inequality. Thus, the error bound
is the same as for the beta-Bernoulli process.

4.2. F-representation. For the decoupled and power-law F-representations, the
truncated CRM has the form

K(cid:88)

Ck(cid:88)

k=1

i=1

ΘK :=

θkiδψki .

(4.21)

(cid:40)

∞(cid:88)

The following result provides a truncation error bound, speciﬁes its range, and
guarantees that as K → ∞ the bound decreases to zero.
Theorem 4.3 (Decoupled Bondesson F-representation truncation error). The er-
ror in approximating Θ ← B-FRep(c, g, ξ) with its truncation ΘK satisﬁes

B(N, K) = 1 − exp

− c
ξ

N

E[1 − π(βk)]

,

(4.22)

(cid:41)

∞(cid:88)

k=K+1

where

βk := V e−Tk ,

V ∼ g,

indep∼ Gam(k, ξ).

Tk

Furthermore, the bound satisﬁes
0 ≤ B(N, K) ≤ 1

lim
K→∞ B(N, K) = 0.
Example 4.4 (Beta-Bernoulli process, BP(γ, α, 0), α ≥ 1). Since

and

∞(cid:88)

γα
ξ

E[V e−Tk ] =

γα
ξ

k=K+1

k=K+1

1
α

(cid:18) ξ
(cid:40)

1 + ξ

= γ

(cid:19)k
(cid:18) ξ

(cid:19)K
(cid:18) ξ
(cid:19)K(cid:41)

1 + ξ

it follows that

1
2

(cid:107)pN,∞ − pN,K(cid:107)1 ≤ 1 − exp

−N γ

1 + ξ

.

(4.26)

Example 4.5 (Gamma-Poisson process, ΓP(γ, λ, 0)). Using the fact that 1−e−θ ≤
θ and calculations analogous to those in Example 4.4 shows that the bound is the
same as in the beta-Bernoulli case and is equivalent (up to a factor of two) to the
truncation bound in Roychowdhury and Kulis (2015).

(4.23)

(4.24)

,

(4.25)

16

T. CAMPBELL, J. H. HUGGINS, J. HOW, AND T. BRODERICK

Example 4.6 (Beta prime-odds Bernoulli process, BPP(γ, α, 0)). Using the trivial
bound θ/(1 + θ) ≤ θ and calculations analogous to those in Example 4.4, for α > 1
we obtain the upper bound

(cid:107)pN,∞ − pN,K(cid:107)1 ≤ 1 − exp

1
2

−N

γα
α − 1

.

(4.27)

(cid:40)

(cid:19)K(cid:41)

(cid:18) ξ

1 + ξ

Given the similarities between the Bondesson D- and F-representations, one
might expect a relationship between their truncation error bounds to exist.
In-
deed, when c = ξ, the bounds presented in Theorems 4.2 and 4.3 coincide for all
K, N ∈ N. However, we can actually say more in this setting: Theorem D.5 in Ap-
pendix D shows that the protobound (of the form presented in Lemma 4.1) of the
Bondesson D-representation is bounded above by that of the decoupled Bondesson
F-representation, providing some indication that the Bondesson D-representation
may have a lower actual truncation error.

Next, we turn to bounding the error of truncated power-law F-representations.

(cid:40)

∞(cid:88)

(cid:41)

Theorem 4.4 (Power-law F-representation truncation error). The error in approx-
imating Θ ← PL-FRep(γ, α, d, g) with ΘK satisﬁes

B(N, K) = 1 − exp

−γN

E[1 − π(βk)]

,

(4.28)

where

βk := V Uk

k−1(cid:89)

(1 − U(cid:96)),

(cid:96)=1

k=K+1

V ∼ g,

indep∼ Beta(1 − d, α + (cid:96)d).

U(cid:96)

(4.29)

Example 4.7 (Beta-Bernoulli process, BP(γ, α, d)). We have

and

lim
K→∞ B(N, K) = 0.

Furthermore, the bound satisﬁes
0 ≤ B(N, K) ≤ 1

∞(cid:88)

E[1 − π(βk)] =

(4.30)

, (4.31)

βk

=

α + kd

α + kd − d + 1

(cid:35)

K(cid:89)

k=1

(cid:41)

k=K+1

k=K+1

k=K+1

where the ﬁnal equality follows from Ishwaran and James (2001, Theorem 1). Thus,

(cid:107)pN,∞ − pN,K(cid:107)1 ≤ 1 − exp

α + kd

α + kd − d + 1

(cid:34) ∞(cid:88)
K(cid:89)

k=1

E[βk] = E

(cid:40)
(cid:18) α + d

α + 1

−γN

(cid:19)K

∼ γN

∞(cid:88)

1
2

K(cid:89)

k=1

where Eq. (4.33) follows because

Γ((α + d)/d + K)

Γ((α + 1)/d)

Γ((α + d)/d)

Γ((α + 1)/d + K)

α + kd

α + kd − d + 1

=

∼

=

(cid:18) α + d
(cid:18) α + d

d

α + 1

(cid:19)−K

(cid:19)K(cid:18) α + 1
(cid:19)K

d

.

(4.36)

(4.32)

K → ∞,

(4.33)

(4.34)

K → ∞ (4.35)

TRUNCATED RANDOM MEASURES

17

The error bound generalizes that of Paisley et al. (2012b), which was for the special
case of BP(γ, α, 0).
Example 4.8 (Gamma-Poisson process, ΓP(γ, λ, d)). Using the fact that 1−e−θ ≤
θ and calculations analogous to those in Example 4.7, the error bound is the same
as for the beta-Bernoulli model with λ in place of α.

Example 4.9 (Beta prime-odds Bernoulli process, BPP(γ, α, d)). Using the trivial
bound θ/(1 + θ) ≤ θ and calculations analogous to those in Example 4.7, for α > 1
we obtain the upper bound and asymptotic simpliﬁcation

(cid:41)

1 + kd

2 + kd − d

(cid:40)
(cid:18) 1 + d

−N

γα
α − 1

(cid:19)K

K(cid:89)

k=1

2

(4.37)

K → ∞ (4.38)

(cid:107)pN,∞ − pN,K(cid:107)1 ≤ 1 − exp

1
2

∼ N

γα
α − 1

using Eq. (4.36).

4.3. V-representation. For the size-biased V-representation, the truncated CRM
has the form

K(cid:88)

∞(cid:88)

ρkx(cid:88)

(cid:32)
− N(cid:88)

(cid:33)

ΘK :=

θkxjδψkxj .

(4.39)

k=1

x=1

j=1

The following result provides a truncation error bound, speciﬁes its range, and
guarantees that as K → ∞ the bound decreases to zero. We use this result to
analyze truncations of some example processes from Broderick et al. (2014).

Theorem 4.5 (Size-biased V-representation truncation error). The error in ap-
proximating Θ ← SB-VRep(ν, h) with its truncation ΘK is bounded by

B(N, K) = 1 − exp

ΛK+n

,

(4.40)

where

(cid:90)

Λk :=

n=1

π(θ)k−1(1 − π(θ))ν(dθ),

k ≥ 1.

(4.41)

Furthermore, the bound satisﬁes
0 ≤ B(N, K) ≤ 1

and

lim
K→∞ B(N, K) = 0.

(4.42)

also possible to express Λk as the inﬁnite sum Λk =(cid:80)∞

Note that although it is often possible to compute Λk in closed form using
Eq. (4.41)—for instance, in the following applications of Theorem 4.5—there is
no guarantee that this is the case. If the integral proves to be problematic, it is
x=1 λkx, where λkx (deﬁned
in Theorem 3.3) is available in closed-form as long as h, ν form a conjugate ex-
ponential family pair. The series can then be truncated if necessary to obtain an
approximation of arbitrarily high precision.

18

T. CAMPBELL, J. H. HUGGINS, J. HOW, AND T. BRODERICK

Example 4.10 (Beta-Bernoulli process, BP(γ, α, d)). For the beta-Bernoulli pro-
cess, the rate integral is the standard beta integral,

(cid:90)

Λk = γ

= γ

Γ(α + 1)

Γ(1 − d)Γ(α + d)
Γ(α + 1)
Γ(α + d)

Γ(α + k)

Γ(α + d + k − 1)

.

θ−d(1 − θ)α+d+k−2dθ

(cid:80)N

(4.43)

(4.44)

(4.45)

(4.46)

First, we analyze the case where d > 0. Using Lemma B.6 to compute the sum

n=1 ΛK+n, the error bound is

(cid:107)pN,∞ − pN,K(cid:107)1
1
2
≤ 1 − exp

(cid:18) γ

∼ γN

Γ(α + 1)
Γ(α + d)

d

Γ(α + 1)
Γ(α + d)
(α + K)d−1

(cid:18) Γ(α + d + K)

Γ(α + K)

(cid:19)(cid:19)

− Γ(α + d + K + N )

Γ(α + K + N )

K → ∞,

where the asymptotic result follows from Lemmas B.5, B.7 and B.8:

Γ(α + d + K)

Γ(α + K)

− Γ(α + d + K + N )

Γ(α + K + N )

Γ(α + d + K)

d
dK

∼ N
∼ N d(α + K)d−1

Γ(α + K)

K → ∞ (4.47)
K → ∞.

(4.48)

When d = 0, we can again use Lemma B.6 to arrive at

1
2

(cid:107)pN,∞ − pN,K(cid:107)1 ≤ 1 − exp (γα (ψ(α + K) − ψ(α + N + K)))

∼ γαN ψ(cid:48)(α + K)
∼ γαN (α + K)−1

K → ∞
K → ∞,

(4.49)

(4.50)

(4.51)

where ψ(·) is the digamma function, and the asymptotic result follows similarly
from Lemma B.5.

Example 4.11 (Beta-negative binomial process, BP(γ, α, d)). For a ﬁxed number
of failures s > 0, and assuming α + d + (k − 1)s > 1, the rate integral for the beta-
negative binomial process may be evaluated via integration by parts for d > 0:

Λk = γ

Γ(α + 1)

Γ(1 − d)Γ(α + d)

=

γ
d

Γ(α + 1)

Γ(1 − d)Γ(α + d)

(cid:18) Γ(α + d + ks)

Γ(α + ks)

=

γ
d

Γ(α + 1)
Γ(α + d)

θ−d−1(1 − θ)α+d+(k−1)s−1(1 − (1 − θ)s)dθ

(4.52)

θ−d((z + s − 1)(1 − θ)z+s−2 − (z − 1)(1 − θ)z−2)dθ
(4.53)

− Γ(α + d + (k − 1)s)
Γ(α + (k − 1)s)

,

(4.54)

(cid:19)

(cid:90)
(cid:90)

TRUNCATED RANDOM MEASURES

19

where z := α + d + (k − 1)s. The error bound is computed by cancelling terms in

the telescoping sum(cid:80)N

n=1 ΛK+n,

(cid:107)pN,∞−pN,K(cid:107)1
1
2
≤ 1 − exp

(cid:18) γ

Γ(α + 1)
Γ(α + d)

d

(cid:18) Γ(α + d + Ks)

Γ(α + Ks)

− Γ(α + d + (K + N )s)

Γ(α + (K + N )s)

∼ γN s

Γ(α + 1)
Γ(α + d)

(α + Ks)d−1

K → ∞,

(cid:19)(cid:19)

(4.55)

(4.56)

where the asymptotic result follows from Lemmas B.5, B.7 and B.8. To analyze
the d = 0 case, we can use L’Hospital’s rule to take the limit of the rate integral,

lim
d→0

Λk = γα lim
d→0

(cid:18) Γ(α + d + ks)

d−1
Γ(cid:48)(α + d + ks)

Γ(α + ks)

− Γ(α + d + (k − 1)s)
Γ(α + (k − 1)s)
− Γ(cid:48)(α + d + (k − 1)s)
Γ(α + (k − 1)s)

(cid:19)

= γα lim
d→0
= γα(ψ(α + ks) − ψ(α + (k − 1)s)).

Γ(α + ks)

Again computing the error bound by cancelling terms in the telescoping sum

(cid:80)N

n=1 ΛK+n,

(4.57)

(4.58)

(4.59)

(4.60)

(4.61)

(4.62)

(cid:107)pN,∞−pN,K(cid:107)1 ≤ 1 − exp (γα (ψ(α + Ks) − ψ(α + (K + N )s)))

1
2

∼ γαN sψ(cid:48)(α + Ks)
∼ γαN s(α + Ks)−1

K → ∞
K → ∞,

where the asymptotic result follows from an application of Lemma B.5.

Example 4.12 (Gamma-Poisson process, ΓP(γ, λ, d)). The gamma-Poisson pro-
cess rate integral can be evaluated via integration by parts for d > 0:

(cid:90)

Λk = γ

λ1−d
Γ(1 − d)
λ1−d
Γ(1 − d)
λ1−d
γ
Γ(1 − d)
d
γλ1−d

= − γ
d

=

(λ + k − 1)e−(λ+k−1)θ − (λ + k)e−(λ+k)θ(cid:17)

θ−d−1e−(λ+k−1)θ(1 − e−θ)dθ

(cid:90)
θ−d(cid:16)
(cid:0)Γ(1 − d)(λ + k)d − Γ(1 − d)(λ + k − 1)d(cid:1)
(cid:0)(λ + k)d − (λ + k − 1)d(cid:1) .
(cid:18) γλ1−d

n=1 ΛK+n, the error bound is

(cid:0)(λ + K)d − (λ + K + N )d(cid:1)(cid:19)

(4.63)

dθ

(4.64)

(4.65)

(4.66)

=

Thus, cancelling terms in the telecoping sum(cid:80)N

d

(cid:107)pN,∞ − pN,K(cid:107)1 ≤ 1 − exp

1
2

d

∼ γN λ1−d(λ + K)d−1

K → ∞,

(4.67)

(4.68)

where the asymptotic result follows from an application of Lemma B.5. To analyze
the d = 0 case, we again make use of L’Hospital’s rule to take the limit of the rate

20

T. CAMPBELL, J. H. HUGGINS, J. HOW, AND T. BRODERICK

d−1(cid:0)(λ + k)d − (λ + k − 1)d(cid:1)
(cid:0)log(λ + k)(λ + k)d − log(λ + k − 1)(λ + k − 1)d(cid:1)

integral:

lim
d→0

Λk = γλ lim
d→0
= γλ lim
d→0
= γλ (log(λ + k) − log(λ + k − 1)) .

Cancelling terms in the telescopic sum(cid:80)N

n=1 ΛK+n yields the error bound

1
2

(cid:107)pN,∞ − pN,K(cid:107)1 ≤ 1 − exp (γλ (log(λ + K) − log(λ + K + N )))

∼ γλN (λ + K)−1

K → ∞,

where the asymptotic result follows from an application of Lemma B.5.

(4.69)

(4.70)

(4.71)

(4.72)

(4.73)

(4.74)

(4.75)

Example 4.13 (Beta prime-odds Bernoulli process, BPP(γ, α, d)). The rate inte-
gral takes the form

(cid:90)

Λk = γ

= γ

Γ(α + 1)

Γ(1 − d)Γ(α + d)
Γ(α + 1)Γ(d + k + α − 1)

Γ(α + d)Γ(k + α)

.

θ(1−d)−1(1 + θ)−(1−d)−(d+k+α−1)dθ

This is the same as for the beta-Bernoulli process, and thus their error bounds
coincide.

4.4. Stochastic mapping. We now show how truncation bounds developed else-
where in this paper can be applied to CRM representations that have been trans-
formed using Lemma 3.4. For Θ ∼ CRM(ν), we denote its transformation by
˜Θ = τ (Θ) (in the deterministic case) or ˜Θ = κ(Θ) (in the stochastic case). For any
object deﬁned with respect to Θ, the corresponding object is denoted with a tilde.
For example, in place of N and X1:N (for Θ), we use ˜N and ˜X1: ˜N (for ˜Θ). Note that
we make the dependence of the truncation error bound B(N, K) on π(θ) explicit
in the notation of Proposition 4.6; when one applies stochastic mapping to a CRM,
one usually also wants to change the likelihood h(x| θ), and thus π(θ) := h(0| θ).
Proposition 4.6 (Truncation error under a stochastic mapping). Consider a rep-
resentation for Θ ∼ CRM(ν) with truncation error bound B(π, N, K). Then for
any likelihood ˜h(x| u):

(1) if

˜Θ is a stochastic mapping of Θ under

the probability kernel
κ(θ, du), its truncation error bound is B(πκ, ˜N , 1, K), where πκ, ˜N (θ) :=

(cid:82) ˜h(0| u) ˜N κ(θ, du); and

(2) if ˜Θ is a stochastic mapping of Θ under the transformation τ : Ψ → U, its

truncation error bound is B(πτ , ˜N , K), where πτ (θ) := ˜h(0| τ (θ)).

The proof of Proposition 4.6 may be found in Appendix D.

Example 4.14 (Gamma-Poisson process, ΓP(γ, λ, d)). Reusing the stochastic
mapping speciﬁed in Eqs. (3.16) and (3.17),

(cid:90)

(cid:90)

πκ, ˜N (θ) =

e− ˜N uκ(θ, du) =

λλ
Γ(λ)

uλ−1θ−λe−( ˜N +λ/θ)udu =

(cid:18) λ

(cid:19)λ

˜N θ + λ

.

(4.76)

TRUNCATED RANDOM MEASURES

21

Combining Eqs. (4.31) and (4.76), Lemma B.3, and Proposition 4.6 yields the error
bound

(cid:107)p ˜N ,∞ − p ˜N , ˜K(cid:107)1 ≤ 1 − exp

1
2

λ + kd

λ + kd − d + 1

(4.77)

−γ ˜N

˜K(cid:89)

k=1

 .

The bound is identical to the one obtained in Example 4.8, but follows from the
error bound on the underlying beta process used to construct the gamma process
representation.

4.5. Hyperpriors. In practice, prior distributions are typically placed on the hy-
perparameters of the CRM rate measure (i.e. γ, α, λ, d, etc.). We conclude our
investigation of CRM truncation error by showing how bounds developed in this
section can be modiﬁed to account for the use of hyperpriors. Note that we make
the dependence of the truncation error bound B(N, K) on the hyperparameters Φ
explicit in the notation of Proposition 4.7.

Proposition 4.7 (CRM truncation error with a hyperprior). Consider a repre-
sentation for Θ|Φ ∼ CRM(ν) with truncation error bound B(Φ, N, K) given ﬁxed
hyperparameters Φ. Then the corresponding truncation error bound for random Φ
is

B(N, K) = E [B(Φ, N, K)] .

(4.78)
Example 4.15 (Beta-Bernoulli process BP(γ, α, 0), α ≥ 1). A standard choice of
hyperprior for the mass γ is a gamma distribution, i.e. γ ∼ Gam(a, b). Combining
Proposition 4.7 and an earlier beta-Bernoulli truncation bound in Eq. (4.26), along
with the standard gamma integral, we have that

(cid:90) ∞
(cid:18) ξ

0

(cid:40)
(cid:19)K(cid:33)−a

γa−1 exp

−bγ − N γ

(cid:19)K(cid:41)

(cid:18) ξ

ξ + 1

dγ

(4.79)

= 1 −

1 +

N
b

ξ + 1

.

(4.80)

1
2

(cid:107)pN,∞ − pN,K|1 ≤ 1 − ba
(cid:32)

Γ(a)

5. Normalized truncation analysis

In this section, we provide truncation error bounds for normalized CRMs
(NCRMs). Examples include the Dirichlet process (Ferguson, 1973), the normal-
ized generalized gamma process (Brix, 1999; Lijoi and Pr¨unster, 2010), and the
normalized σ-stable process (Kingman, 1975; Lijoi and Pr¨unster, 2010). Given a
CRM Θ on Ψ, we deﬁne the corresponding NCRM Ξ via Ξ(S) := Θ(S)/Θ(Ψ) for
each measurable subset S ⊆ Ψ. Likewise, given a truncated CRM ΘK, we de-
ﬁne its normalization ΞK via ΞK(S) := ΘK(S)/ΘK(Ψ). Note that any simulation
algorithm for ΘK can be used to simulate ΞK by simply normalizing the result.
Furthermore, this construction does not depend on the particular representation of
the CRM, and thus applies equally to the D-, F-, and V-representations described
in Section 3.

The ﬁrst step in the analysis of NCRM truncations is to deﬁne their approxima-
tion error in a manner similar to that of CRM truncations. Since Ξ and ΞK are
both normalized, they are distributions on Ψ; thus, observations X1:N are gener-
ated i.i.d. from Ξ, and Z1:N are generated i.i.d. from ΞK. Y1:N and W1:N have the

22

T. CAMPBELL, J. H. HUGGINS, J. HOW, AND T. BRODERICK

same deﬁnition as for CRMs. As in the developments of Section 4, the theoretical
results of this section rely on a general upper bound, provided by Lemma 5.1:4
Lemma 5.1 (NCRM Protobound). Let Θ ∼ CRM(ν), and let its truncation be
ΘK. Let their normalizations be Ξ and ΞK respectively. If

Xn | Ξ i.i.d.∼ Ξ,
Yn | Xn

indep∼ f (·| Xn),

Zn | ΞK
Wn | Zn

i.i.d.∼ ΞK,
indep∼ f (·| Zn),

(5.1)

(5.2)

(5.3)

(5.4)

(5.5)

then

(cid:107)pN,∞ − pN,K(cid:107)1 ≤ 1 − P (X1:N ⊆ supp(ΘK)) ,

1
2

where pN,∞, pN,K are the marginal densities of Y1:N and W1:N , respectively.

The analysis of CRMs in Section 4 relied heavily on the independence of the
rates in Θ and X1:N ; unfortunately, the rates in Ξ do not possess the same inde-
pendence properties (they must sum to one). Likewise, sampling Xn for each n
does not depend on the atoms of Ξ independently (Xn randomly selects a single
atom based on their rates). Rather than using the basic deﬁnitions of the above
random quantities to derive an error bound, we decouple the atoms of Ξ and X1:N
using a technique from extreme value theory. The Gumbel distribution with loca-
tion µ ∈ R and scale σ > 0, denoted T ∼ Gumbel(µ, σ), is deﬁned by the cumulative
distribution function

P(T ≤ t) = e−e

− t−µ
σ ,

and has density with respect to the Lebesgue measure on R

e−( t−µ

σ )−e

−( t−µ
σ )

.

1
σ

An interesting property of the Gumbel distribution is that if one perturbs the log-
probabilities of a ﬁnite discrete distribution by i.i.d. Gumbel(0, 1) random variables,
the arg max of the resulting set is a sample from the discrete distribution (Gumbel,
1954; Maddison et al., 2014). This technique is invariant to normalization, as the
arg max is invariant to the corresponding constant shift in the log-transformed
space. For present purposes, we require the inﬁnite extension of this result:
Lemma 5.2 (Inﬁnite Gumbel-max sampling). Let (pi)∞

i=1 be a collection of positive
i=1 are i.i.d. Gumbel(0, 1)
random variables, then arg maxi∈N Ti + log pi exists, is unique a.s., and has distri-
bution

numbers such that(cid:80)

i pi < ∞ and let ¯pj := pj(cid:80)

. If (Ti)∞

i pi

(cid:16)

(cid:17)

arg max

i∈N

Ti + log pi ∼ Categorical

∞
j=1

(¯pj)

.

(5.6)

The proof of this result, along with the others in this section, may be found in Ap-
pendix E. The utility of Lemma 5.2 is that it allows the construction of Ξ and X1:N
without the problematic coupling of the underlying CRM atoms due to normaliza-
tion; rather than dealing directly with Ξ, we apply Lemma A.1 to log-transform the
rates of Θ, Lemma A.4 to perturb each rate by an i.i.d. Gumbel(0, 1) random vari-
able, and characterize the distribution of the maximum rate in this process. The

4See Lemma D.1 in Appendix D for the generalization of Lemma 5.1 to arbitrary random

measures.

TRUNCATED RANDOM MEASURES

23

combination of this distribution with Lemma 5.2 yields the key proof technique
used to develop the truncation bounds in Theorem 5.3 and Theorem 5.4. Rather
than following the order of presentation in earlier sections of the paper, we analyze
F- and V-representations before investigating the Bondesson D-representation, as
the analysis is much simpler in the former case.

5.1. F- and V-representations. F- and V-representations (including as special
cases the B-FRep, PL-FRep, and SB-VRep detailed in Sections 3.2 and 3.3) share a
very useful property: the truncated CRM ΘK and the tail measure after truncation
Θ − ΘK are both CRMs, and they are mutually independent. This is a simple
consequence of Lemma A.2 and the fact that F- and V-representations are deﬁned
via an inﬁnite sum of CRMs. Theorem 5.3 provides a truncation error bound for
any truncated NCRM whose underlying CRM satisﬁes this property.
Theorem 5.3 (Normalized F-/V-representation truncation error bound). Let Θ ∼
CRM(ν), and deﬁne its truncation ΘK ∼ CRM(νK) and tail Θ+
K) such
that Θ = ΘK + Θ+
K. Then if Ξ is the normalization of Θ, and ΞK
is the normalization of ΘK, the error of approximating Ξ with ΞK is bounded by

K ∼ CRM(ν+

K and ΘK ⊥⊥ Θ+
(cid:18)

B(N, K) = 1 −

1 −

(cid:90)(cid:90)

where

W (θ, t) := 1 [t ≥ 0] θe−θt exp

(e−θt − 1)ν(dθ)

W (θ, t) ν+

K(dθ) dt

,

(cid:18)(cid:90)

(cid:19)

(5.7)

,

(5.8)

(cid:19)N

Furthermore, the bound satisﬁes

0 ≤ B(N, K) ≤ 1 and

lim
K→∞ B(N, K) = 0.

(5.9)

Example 5.1 (Dirichlet process, DP(γ)). The Dirichlet process with concentration
γ > 0 is a normalized gamma process ΓP(γ, 1, 0). To analyze its truncation error, we
use the decoupled Bondesson F-representation. First, the inner integral in W (θ, t)
is

(cid:90)

(cid:90)

(e−tθ − 1)ν(dθ) = γ

(e−tθ − 1)θ−1e−θdθ

(cid:90) ∞

= γ lim
→0
= −γ lim
→0

θ−1e−(t+1)θdθ −



θ−1e−θdθ.

(cid:90) (t+1)

Since e−θ is a monotonically decreasing function,

−γ lim
→0

e−



θ−1dθ ≤ −γ lim
→0

θ−1e−θdθ ≤ −γ lim
→0

−γ lim
→0

e− log(t + 1) ≤ −γ lim
→0

θ−1e−θdθ ≤ −γ lim
→0

−γ log(t + 1) ≤ −γ lim
→0



θ−1e−θdθ ≤ −γ log(t + 1),





(cid:90) ∞
(cid:90) (t+1)
(cid:90) (t+1)
(cid:90) (t+1)
(cid:90) (t+1)





θ−1e−θdθ

(5.10)

(5.11)

(5.12)

(cid:90) (t+1)

θ−1dθ



(5.13)

e−(t+1)

e−(t+1) log(t + 1)

(5.14)

(5.15)

24

T. CAMPBELL, J. H. HUGGINS, J. HOW, AND T. BRODERICK

demonstrating that the exponential term in W (θ, t) is

(cid:19)

exp

(e−tθ − 1)ν(dθ)

= (t + 1)−γ.

(5.16)

Next, Example 3.2 and Eq. (C.20) immediately yield cν = γ, gν(v) = e−v, and the
tail rate measure ν+
K,

ν+
K(dθ) =

γ
ξ

k=K+1

(− log x)k−1xξ−2e−θx−1

dxdθ.

(5.17)

Substituting this result, using Fubini’s theorem to swap the order of integration and
summation, evaluating the integral over θ, and making the substitution x = e−s

(cid:18)(cid:90)

∞(cid:88)

(cid:90) 1

0

ξk
Γ(k)

∞(cid:88)

(cid:90)(cid:90)

ξk
Γ(k)

yields(cid:90)(cid:90)

(cid:90)(cid:90)

W (θ, t)ν+

K(dθ)dt =

(es + t)2
Noting that es ≥ 1∀s ≥ 0, we have for any a ∈ (0, 1] ∩ (0, γ),

s,t≥0

k=K+1

sk−1e−(ξ−1)s(t + 1)−γ

dsdt.

(5.18)

W (θ, t)ν+

K(dθ)dt ≤ γ
ξ

sk−1e−(ξ+a)sds

(t + 1)−(γ+1−a)dt

γ
ξ

∞(cid:88)

k=K+1

=

γ

(γ − a)ξ

(cid:32)

ξk
Γ(k)

∞(cid:88)

k=K+1

(cid:19)k

0

(cid:90) ∞
(cid:18) ξ
(cid:18) ξ

ξ + a

γ

(cid:90) ∞
(cid:18) ξ

0

γ

=

a(γ − a)

ξ + a

(cid:19)K(cid:33)N

Therefore, the truncation error of the Dirichlet process can be bounded by

(cid:19)K

(5.19)

.

(5.20)

(cid:107)pN,∞ − pN,K(cid:107)1 ≤ 1 −

1
2

1 −

a(γ − a)

ξ + a

∀ a ∈ (0, 1] ∩ (0, γ).

(5.21)

It is straightforward to analytically ﬁnd the minimum of the bound with respect to
a for ﬁxed ξ if desired. Alternatively, if we set ξ = aξ(cid:48) for the ﬁxed free parameter
ξ(cid:48) > 0, we can optimize with respect to a to ﬁnd

(cid:107)pN,∞ − pN,K(cid:107)1 ≤ 1 −

1
2

1 − a(γ)

(cid:32)

(cid:19)K(cid:33)N

(cid:18) ξ(cid:48)

ξ(cid:48) + 1

(cid:26) 4

γ
γ
γ−1

, a(γ) :=

0 < γ ≤ 2
.
γ > 2

(5.22)

Note that the bounds in Example 5.1 have exponential decay, as expected given
early DP truncation error bounds presented by Ishwaran and James (2001); Ish-
waran and Zarepour (2002). In fact, the above can reproduce the decay rate of this
past result by setting ξ(cid:48) = (eγ−1 − 1)−1 in Eq. (5.22). However, the techniques used
in past work do not generalize beyond the Dirichlet process, while those used here
apply to any NCRM.

Example 5.2 (Normalized gamma process, NΓP(γ, λ, d)). It is also possible to nor-
malize the generalized gamma process ΓP(γ, λ, d) (Brix, 1999). For this example,
we use the size-biased V-representation of ΓP(γ, λ, d) with ν(dθ) = γθ−1−de−λθ dθ.

TRUNCATED RANDOM MEASURES

25

Computing the integral of W (θ, t) over θ ﬁrst,

θe−θtν+

K(dθ) = γ

θ−de−(K+t+λ)θ dθ = γΓ(1 − d)(K + t + λ)d−1.

(cid:90)

(cid:90)

(cid:90)(cid:90)

= exp(cid:0)γΓ(−d)((t + λ)d − λd)(cid:1) .

The exponential term can also be computed in closed-form,

Multiplying these and integrating over t ≥ 0 yields

exp

(cid:19)

(e−θt − 1)ν(dθ)

(cid:18)(cid:90)
K(dθ) dt = γΓ(1 − d)e−γΓ(−d)λd(cid:90) ∞

≤ Cγ,λ,d(K + λ)d−1,

λ

W (θ, t)ν+

eγΓ(−d)td

(K + t)d−1 dt

(5.23)

(5.24)

(5.25)

(5.26)

where

Cγ,λ,d := γΓ(1 − d)eσλd

and Γ(·,·) is the upper incomplete gamma function Γ(a, b) := (cid:82) ∞

(5.27)
b θa−1e−θdθ.
Therefore, the truncation error of the normalized generalized gamma process can
be bounded by

σ := −γΓ(−d),

Γ(d−1, λdσ),

d−1σ−d−1

1
2

(cid:107)pN,∞ − pN,K(cid:107)1 ≤ 1 −(cid:0)1 − Cγ,λ,d(K + λ)d−1(cid:1)N
(cid:19)

(cid:18) λ + K

(cid:18)

(cid:107)pN,∞ − pN,K(cid:107)1 ≤ 1 −

1 − γλγ log

λ

1
2

(cid:19)N

Setting λ = 0 in the above expression immediately yields the bound for the normal-
ized d-stable process (Kingman, 1975). Taking the limit d → 0 yields the bound
for the normalized ΓP(γ, λ, 0),

K−1

.

(5.29)

.

(5.28)

It should be noted that truncation of the NΓP(γ, λ, d) has been studied previ-
ously. Argiento et al. (2015) threshold the weights of the unnormalized CRM to
be beyond a ﬁxed level  > 0 prior to normalization, and develop error bounds for
that method of truncation. These results are not directly comparable to those of
the present work due to the diﬀerent methods of truncation (i.e. sequential repre-
sentation termination versus weight thresholding).

5.2. D-representation. The Bondesson D-representation, in contrast to F- and
V-representations, couples the rates via the Poisson process Γck. The truncation
ΘK is not a CRM, nor is it independent of the tail measure, and Theorem 5.3
does not apply. The following result provides a truncation error bound for the
normalization of the Bondesson D-representation.
Theorem 5.4 (Normalized B-DRep truncation error bound). Let Θ ∼ CRM(ν)
where ν satisﬁes the conditions in Theorem 3.1, let cν and gν have the same deﬁ-
nitions as in Theorem 3.1, and let ΘK be the truncation of Θ ← B-DRep(cν, gν).
Then if Ξ is the normalization of Θ, and ΞK is the normalization of ΘK, the error
of approximating Ξ with ΞK is bounded by

(cid:18)(cid:90)(cid:90)

(cid:19)N

B(N, K) = 1 −

W (s, t) ds dt

,

(5.30)

26

T. CAMPBELL, J. H. HUGGINS, J. HOW, AND T. BRODERICK

where

W (s, t) := −1 [t ≥ 0]

(cid:90) (cid:16)

cK−2

Γ(K − 1)

(cid:17)

F (x) :=

e−θe−x − 1

ν(dθ), F ∈ C∞.

tK−2e−cteF (t+s) d2F

ds2 (s)

Furthermore, the bound satisﬁes

0 ≤ B(N, K) ≤ 1 and

lim
K→∞ B(N, K) = 0.

(5.31)

(5.32)

(5.33)

Example 5.3 (Dirichlet process, DP(γ)). The Dirichlet process with concen-
tration γ > 0 is a normalized gamma process ΓP(γ, 1, 0) with rate measure
ν(dθ) = γθ−1e−θdθ. Using the construction in Example 3.2, we have cν = γ.
Example 5.1 also provides F (s) in closed-form,

F (s) = −γ log(cid:0)e−s + 1(cid:1) .

The second derivative of F is

Therefore, using the substitution x = e−s and integration by parts,

(cid:90)

−c−1

eF (t+s) d2F

ds2 (s)ds =

e−s

(e−s + 1)2 ds
−2 dx

e−s

(e−s + 1)2 .

d2F

ds2 (s) = −γ
(cid:90) (cid:16)
(cid:90) ∞

e−(t+s) + 1

(cid:17)−γ
(cid:0)xe−t + 1(cid:1)−γ
(cid:90) ∞

0

0

=

(x + 1)

= 1 − γeγt

(cid:0)x + et(cid:1)−(γ+1)
(cid:0)x + et(cid:1)−(γ+1) ≤ e−(γ+a)t (x + 1)a−1 ,
(cid:90) ∞

(x + 1)−1dx ≥ 1 − γe−at

(x + 1)−1dx.

(5.34)

(5.35)

(5.36)

(5.37)

(5.38)

(5.39)

Noting that for any a ∈ [0, 1),

we have

1 − γeγt

(cid:90) ∞

0

(cid:0)x + et(cid:1)−(γ+1)

and therefore,

(cid:90)(cid:90)

W (s, t)dsdt ≥ γK−1
Γ(K − 1)

(x + 1)−2+adx (5.40)

0

e−at,

(5.41)

(cid:19)

e−at

dt

(5.42)

= 1 − γ

1 − a

(cid:18)

tK−2e−γt

(cid:19)K−1

1 − γ

1 − a

(cid:90) ∞
(cid:18) γ

0

.

(5.43)

= 1 − γ

1 − a

γ + a

(cid:16)

(cid:17)

K+1

Minimizing with respect to a, we ﬁnd that the minimum occurs at a =
max
. Therefore, the truncation error of the Dirichlet process can be

0, K−γ

bounded by

(cid:107)pN,∞ − pN,K(cid:107)1 ≤ 1 −

1
2

(cid:32)

TRUNCATED RANDOM MEASURES

(cid:19)K−1(cid:33)N

(cid:18) γ
(cid:19)K

γ + a

(cid:18)

, a = max

0,

K → ∞.

27

(cid:19)

K − γ
K + 1

(5.44)

(5.45)

1 − γ

1 − a

(cid:18) γ

∼ N (K + 1)

,

γ + 1

5.3. Hyperpriors. As in the CRM case, we can place priors on the hyperparame-
ters of the NCRM rate measure (i.e. γ, α, λ, d, etc.). We conclude our investigation
of NCRM truncation error by showing how bounds developed in this section can
be modiﬁed to account for hyperpriors. Note that we make the dependence of the
truncation error bound B(N, K) on the hyperparameters Φ explicit in the notation
of Proposition 5.5.

Proposition 5.5 (NCRM truncation error with a hyperprior). Consider a rep-
resentation for Θ|Φ ∼ CRM(ν), and let Ξ be its normalization with truncation
error bound B(Φ, N, K) given ﬁxed hyperparameters Φ. Then the corresponding
truncation error bound for random Φ is

B(N, K) = E [B(Φ, N, K)] .

(5.46)

Example 5.4 (Normalized gamma process, NΓP(γ, 1, 0)). A standard choice of
hyperprior for the concentration γ is a gamma distribution, i.e. γ ∼ Gam(a, b).
Combining Proposition 5.5 with the result from Eq. (5.29) with λ = 1, we have

1
2

1 −(cid:0)1 − γK−1 log (1 + K)(cid:1)N(cid:105)
(cid:107)pN,∞ − pN,K|1 ≤ E(cid:104)
≤ 1 −(cid:0)1 − E [γ] K−1 log (1 + K)(cid:1)N
(cid:17)N
= 1 −(cid:16)

K−1 log (1 + K)

.

1 − a
b

(5.47)

(5.48)

(5.49)

6. Simulation algorithms and computational complexity

The D-, F-, and V-representations in Section 3 are each generated from a diﬀerent
ﬁnite sequence of distributions, resulting in a diﬀerent expected computational
cost for the same truncation level. Therefore, the truncation level itself is not
an appropriate parameter with which to compare the error bounds for diﬀerent
representations—we require a characterization of the computational cost. In this
section, we investigate the mean complexity E[R] of each representation, where R is
the number of random variables sampled to generate a truncation, as a function of
the truncation level for each of the representations in Section 3. We further relate
these quantities to the expectation of the number of atoms generated, E[A]. In the
process, we develop a simulation algorithm for size-biased V-representations that
can be implemented in practice to generate a truncated CRM in ﬁnite time.

6.1. D- and F-representations. The complexity of Bondesson D-representations
is straightforward to derive directly from the deﬁnition: for a particular truncation
level K,

E[R] = 3K = 3E[A].

(6.1)

28

T. CAMPBELL, J. H. HUGGINS, J. HOW, AND T. BRODERICK

The constant factor of 3 arises from the need to sample Vk, Eck, and ψk at each
truncation level. The complexity for decoupled Bondesson F-representations is
similarly straightforward to develop: for a particular truncation level K,

E[R] =

+ 1

K =

3 +

E[A].

(6.2)

(cid:19)

(cid:18)

(cid:18) 3c

ξ

(cid:19)

ξ
c

(cid:19)

(cid:18) 1

γ

The 3c/ξ factor corresponds to sampling Vki, Tki, and ψki an average of c/ξ times,
while the additional K accounts for the Poisson random number of atoms at each
round that are sampled. The expression for the expected number of atoms follows
from E[A] = c
ξ K. An interesting consequence of this relatively simple formula is a
method for the principled selection of the free parameter ξ; we can either minimize
expected complexity given a target error bound, or minimize the error bound given
a target complexity. Another consequence is that when ξ = c, both the Bondesson
D-representation and the decoupled Bondesson F-representation have E[A] = K,
and thus the computational complexity of the Bondesson D-representation is less
than that of the decoupled Bondesson F-representation. Combining this insight
with the fact that their truncation error bounds coincide when ξ = c (Section 4.2
and Appendix D.7), the Bondesson D-representation appears to be a better repre-
sentation in this setting.

Finally, for power-law F-representations,

(cid:18)

(cid:19)

E[R] =

1 +

5γ
2

K +

γ
2

K 2 =

+

5
2

E[A] +

E[A]2.

1
2γ

(6.3)

This expression follows from the quadratic number of beta random variables re-
quired to generate the representation, and the fact that E[A] = γK. Algorithms
for generating these representations in practice follow directly from their construc-
tion.

6.2. V-representation. In contrast to D- and F-representations, the analysis of
size-biased V-representations is not as straightforward. In fact, the representation
as given in Section 3.3 cannot in general be used to simulate a truncated CRM—
when h has countable support, simulating according to this construction requires
sampling an inﬁnite number of random variables for each ﬁxed value of k, and thus
cannot be used directly in practice. It is possible to use the superposition property
(Lemma A.2) to analytically collapse the problematic sum, an approach that is
considered in James (2014); however, this technique eliminates the possibility of
generating θkxj from a familiar exponential family distribution, often necessitating
approximate sampling techniques for θkj. There exists another approach—based on
the following lemma—that doesn’t collapse the sum over x, which allows sampling
rates from well-known exponential family distributions.

Lemma 6.1 (Inﬁnite Poisson Sampling). Suppose λj ≥ 0, and(cid:80)
j λj < ∞. Then
(cid:33)
λi(cid:80)

 , Si+1 = Si − Ti, Ti ∼ Binom

(cid:88)

, i ∈ N, (6.4)

S1 ∼ Poiss

given the model

(cid:32)

Si,

λj

j≥i λj

j

the marginal distribution of T1, T2, . . . is

indep∼ Poiss (λj) , j ∈ N.

Tj

(6.5)

TRUNCATED RANDOM MEASURES

29

Further, if I is the largest index i such that Si > 0, then

P (I ≤ i) = e−(cid:80)

j>i λj .

(6.6)

Lemma 6.1 allows us to sample the inﬁnite sequence of Ckx for each ﬁxed k in
a ﬁnite number of iterations, which then reduces sampling rates and locations for
that value of k to a ﬁnite-time operation. The ﬁnal simulation technique is speciﬁed
in the following result.

Theorem 6.2 (Truncated CRM Simulation). Let ¯ΛK = (cid:80)K

k=1 Λk, and let
(k1, x1), (k2, x2), . . . be any ordering of the countable product space {1, . . . , K}× N.
Then the truncated V-representation ΘK may be constructed as follows:

ΘK =

I(cid:88)

Ci(cid:88)
S1 ∼ Poiss(cid:0)¯ΛK
(cid:32)

j=1

i=1

Ci ∼ Binom

indep∼

θij

1

θijδψij

(cid:1) , Si+1 = Si − Ci,
(cid:33)
¯ΛK −(cid:80)i−1

j=1 λkj xj
h(0|θ)ki−1h(xi|θ)ν(dθ).

λkixi

Si,

I = sup{i| Si > 0}

(6.7)

where

(6.11)
It is easy to deduce from Theorem 6.3 that selecting an ordering of {1, 2, . . . , K}×
N such that the sequence λk1x1 , λk2x2 , λk3x3, . . . is non-increasing yields the optimal

i=rλkixi > 1}| .

λkixi

The ﬁrst remark to make about this construction is that the truncation ΘK has

(cid:1) atoms, so E[A] = ¯ΛK. Next, I is almost surely ﬁnite by Lemma 6.1, so
(cid:1) requires K < ∞ (otherwise S1 = ∞ almost surely for

Poiss(cid:0)¯ΛK
the ﬁrst step S1 ∼ Poiss(cid:0)¯ΛK
(cid:80)Ckx
summing the results yields the desired truncation ΘK = (cid:80)K
i=1 θkxiδψkxi , one can
k individually for k = 1, 2, . . . , K;
k=1 Θ(cid:48)
k. In this case,
one can stop the simulation at any desired truncation level K without knowing that
level beforehand.

the procedure in Theorem 6.2 terminates in ﬁnite time. One drawback is that this
simulation algorithm requires a priori knowledge of the truncation level K, since
K = ∞ since ν(R+) = ∞). However, deﬁning Θ(cid:48)
use the proposed algorithm to simulate each Θ(cid:48)

k :=(cid:80)∞

x=1

The next result characterizes the expected complexity of using the procedure in
Eq. (6.7) and provides an upper bound along with a tightness guarantee, in the
event that it is easier to use.

Theorem 6.3 (Expected Simulation Complexity).
1 − e

E [R] = 1 + 2¯ΛK +

∞(cid:88)

−(cid:80)
∞(cid:88)
∞(cid:88)

r=L+1

i=r

i≥r λkixi

λkixi

r=1

≤ 1 + 2¯ΛK + L +

≤ e
e − 1

E[R],

L := |{r ∈ N|(cid:80)∞

(6.8)

(6.9)

(6.10)

30

T. CAMPBELL, J. H. HUGGINS, J. HOW, AND T. BRODERICK

complexity for any choice of K. This makes intuitive sense, as it causes the Ckx
with large expectation to be sampled earlier, leaving less remaining mass.

Example 6.1 (Beta-Bernoulli process, BP(γ, α, d)). Using the result developed
earlier in Eq. (4.44) and the inequality (Gautschi, 1959)

for α + K ≥ 1,

K(cid:88)

k=1

¯ΛK =

Λk =

(1 + x)d−1 ≤ Γ(x + d)
Γ(x + 1)

≤ xd−1

0 ≤ d ≤ 1, x ≥ 1,

(6.12)

(cid:19)

− Γ(α + d)

Γ(α)

(cid:18) Γ(α + d + K)

Γ(α + K)
(α + K)d − γα
d

.

(6.13)

(6.14)

γ
d
≤ γ
d

Γ(α + 1)
Γ(α + d)

Γ(α + 1)
Γ(α + d)

For α + K + d ≥ 2, we have
¯ΛK ≥ γ
d

(6.15)
Next, since the Bernoulli likelihood has bounded support x ∈ {0, 1}, the third term
in Theorem 6.3 can be bounded via

.

Γ(α + 1)
Γ(α + d)

(α + K + d − 1)d − γα
d

∞(cid:88)

r=1

−(cid:80)

1 − e

1 − e−(cid:80)K

i=r Λi ≤ K.

i≥r λkixi =

K(cid:88)

r=1

for K ≥ 2, the mean computational complexity of simulating the V-

Thus,
representation of BP(γ, α, d) is bounded by

(6.16)

E[R] ≤ 1 − 2γα
d

+

2γ
d

Γ(α + 1)
Γ(α + d)

and using the lower bound on ¯ΛK = E[A],
E[R] ≤ 2 − α − d + 2E[A] +

(α + K)d + K

(6.17)

(cid:18) Γ(α + d)

Γ(α)

(cid:19)1/d

+ d E[A]

.

(6.18)

The beta prime-odds Bernoulli process has the same Λk and likelihood support,

and thus has the same mean complexity bound.

For likelihoods with inﬁnite support, bounding the series is not immediately
possible. Corollary 6.4 provides a bound that applies to such likelihoods, given a
sensible choice of the ordering of {1, 2, . . . , K} × N. Note that this bound is quite
loose—in practice, it is often better to approximate the series in Theorem 6.3 with
i=1 λkixi. Obtaining

a ﬁnite truncation, using the fact that(cid:80)

tighter closed-form bounds on E[R] is an open problem.
Corollary 6.4. Given a rectangular ordering of {1, 2, . . . , K} × N, i.e.

i≥r λkixi = ¯ΛK −(cid:80)r−1
(cid:25)
(cid:24) i

ki = ((i − 1) mod K) + 1

xi =

K

,

(6.19)

the mean complexity is bounded by

E[R] ≤ 1 + 2¯ΛK + K 2

Eθ[X]ν(dθ).

(6.20)

(cid:90)

TRUNCATED RANDOM MEASURES

31

Example 6.2 (Beta-negative binomial process, BP(γ, α, d)). Using the result de-
veloped earlier in Eq. (4.54), we have for α + Ks ≥ 1

(cid:19)

− Γ(α+d)
Γ(α)

(cid:18)Γ(α+d+ Ks)

Γ(α+ Ks)

(α + Ks)d − γα
d

¯ΛK =

γ
d
≤ γ
d

Γ(α+1)
Γ(α+d)
Γ(α + 1)
Γ(α + d)

(6.21)

(6.22)

and for α + Ks + d ≥ 2

¯ΛK ≥ γ
d

Γ(α + 1)
Γ(α + d)

(α + Ks + d − 1)d − γα
d

(6.23)
The expectation of the negative binomial distribution is (1− θ)−1θs. When α + d >
1, the integral in Corollary 6.4 is ﬁnite, and for K ≥ 2/s the mean complexity is
bounded by

.

E[R] ≤ 1 − 2γα
d

and

E[R] ≤ 1 + 2E[A] + s−2

+

2γ
d

Γ(α + 1)
Γ(α + d)

(cid:34)(cid:18) Γ(α + d)

Γ(α)

(α + Ks)d +

sγα

α + d − 1

(cid:19)1/d − α − d + 1

K 2.

(cid:35)2

(6.24)

.

(6.25)

+ d E[A]

Example 6.3 (Gamma-Poisson process, ΓP(γ, d, λ)). Using the result developed
earlier in Eq. (4.66), we have

¯ΛK =

Λk =

γλ1−d

d

(λ + K)d − γλ
d

.

(6.26)

K(cid:88)

k=1

The expectation of the Poisson distribution is θ; therefore, computing the integral
in Corollary 6.4 yields the mean complexity bound,

E[R] ≤ 1 − 2γλ
d

+

2γλ1−d

d

(λ + K)d + γK 2.

(6.27)

6.3. Summary of CRM results. Table 1 summarizes our truncation and simula-
tion complexity results as they apply to the beta, gamma, and beta prime processes.
Results for the Bondesson D-representation of BP(γ, 1, 0) as well as the decoupled
Bondesson F-representations of BP(γ, α, 0) and ΓP(γ, λ, 0) were previously known,
and are reproduced by our results. All other results in the table are novel to the best
of the authors’ knowledge. It is interesting to note that the bounds and expected
complexity within each of the D-, F-, and V-representation classes have the same
form, aside from some constants. Across classes, however, they vary signiﬁcantly,
indicating that the chosen sequential representation of a process has more of an
inﬂuence on the truncation error than the process itself.

7. Simulation study

In this section, we present a simulation study of how the developed truncation
error bounds vary with the expected computational complexity E[R] of simulating
the truncation. This comparison yields heuristics for selecting which of the three
sequential representations of an (N)CRM is most advantageous for a particular
application. The results in the main text have the number of observations ﬁxed to

32

T. CAMPBELL, J. H. HUGGINS, J. HOW, AND T. BRODERICK

BP

ΓP

BPP

NΓP

γ = 1, α = λ = 1, d = 0

γ = 1, α = λ = 2, d = 0.5
Figure 1. Truncation error bounds
and V-
for D-, F-,
representations of the beta-Bernoulli (BP), gamma-Poisson (ΓP),
beta prime-odds Bernoulli
(BPP), and normalized gamma-
multinomial (NΓP) processes. Names in parentheses denote the
ﬁrst author of the original source for each sequential representa-
tion.

101102103104105Mean Computational Complexity10 510 410 310 210 1100101L1 ErrorB DRep (Teh)B FRep (Paisley)SB VRep (Thibaux)101102103104105Mean Computational Complexity10 510 410 310 210 1100101L1 ErrorPL FRep (Broderick)SB VRep (Thibaux)101102103104105Mean Computational Complexity10 510 410 310 210 1100101L1 ErrorB DRepB FRep (Roychowdhury)SB VRep101102103104105Mean Computational Complexity10 510 410 310 210 1100101L1 ErrorPL FRepSB VRep101102103104105Mean Computational Complexity10 510 410 310 210 1100101L1 ErrorB DRepB FRepSB VRep101102103104105Mean Computational Complexity10 510 410 310 210 1100101L1 ErrorPL FRepSB VRep101102103104105Mean Computational Complexity10 510 410 310 210 1100101L1 ErrorB DRepB FRepDRep (Sethuraman)SB VRep101102103104105Mean Computational Complexity10 510 410 310 210 1100101L1 ErrorSB VRepTRUNCATED RANDOM MEASURES

33

Table 1. Error bounds and sampling complexity for truncated
CRMs. The Ci vary between models. Be = Bernoulli, OBe =
odds Bernoulli, Poi = Poisson, and NB(s) = negative binomial
with s failures.

Rep. CRM

N γ

γα+1

Truncation Err. Bound

(cid:17)K
(cid:17)K

(cid:16) γα
(cid:16) ξ

h
BP(γ, α ≥ 1, 0) Be
BPP(γ, α, 0)
OBe
ΓP(γ, α, 0)
Poi
BP(γ, α ≥ 1, 0) Be
OBe
BPP(γ, α, 0)
Poi
ΓP(γ, α, 0)
Be
BP(γ, α, d)
OBe
BPP(γ, α, d)
ΓP(γ, λ, d)
Poi
N C0(α + d + K)d−1
Be
BP(γ, α, d)
BPP(γ, α, d)
OBe
NB(s) N C0(α + d + Ks)d−1
BP(γ, α, d)
Poi
ΓP(γ, α, d)

N γ( α+d
α+1 )K
α−1 ( 1+d
2 )K
N γ( α+d
α+1 )K

N C0(α + K)d−1

N γα

N C0

ξ+1

D

F

V

Asymptotic Complexity

(cid:16)

3E[A]

3 + ξ
γα

(cid:17) E[A]
(cid:17) E[A] + 1

2γ

E[A]2

(cid:16) 1

γ + 5

2

C0 + 2E[A] + (C1 + d E[A])1/d

1 + 2E[A] + C0[(C1 + d E[A])1/d − C2]2

N = 10; see Appendix G for a discussion of the eﬀect of increasing N on the plots
shown here.

Fig. 1 shows the results of the simulation study, comparing the beta-Bernoulli
(BP), gamma-Poisson (ΓP), beta prime-odds Bernoulli (BPP), and normalized gen-
eralized gamma-multinomial (NΓP) processes.
In the legend, a name in paren-
theses denotes equivalence to a previously-developed construction. The bound for
the Sethuraman (1994) construction is the only result not generalized by the present
work, but is shown for comparison purposes. Note that all other bounds presented
in the ﬁgure are improved by a factor of two compared to past representations due to
the reliance on Lemmas 4.1 and 5.1 rather than the earlier bound found in Ishwaran
and James (2001). All results without a name are novel to the best knowledge of
the authors, and are possible due to the generality of the present work. The left
column shows results for the two-parameter (i.e. undiscounted) process, with γ = 1,
α = λ = 1, d = 0, and ξ = c = γα. While the D- and F-representations capture the
exponential truncation error tails, the tail of the V-representation bound is consis-
tently polynomial. This appears to be the price of the V-representation’s generality,
which imposes no conditions on ν beyond the basic assumption in Eq. (2.1). The
right column shows results for the discounted (i.e. power-law) process, with γ = 1,
α = λ = 2, and d = 0.5. For discounted processes, the V-representation appears to
outperform the power-law F-representation as the truncation level increases; this is
due to the computational cost of simulating a sequence of beta random variables
for each atom in the power-law F-representation.

From the simulation results, it appears that all three of the D-, F-, and V-
representations have settings in which they should be chosen; there is no single
dominant representation for all situations.
In the undiscounted setting, the D-
and F-representations should be used for their quickly decaying truncation error
bounds. The choice between these two depends on whether decoupling the atoms
across rounds is important; if so, the F-representation should be selected. In the
discounted setting, the V-representation appears to be superior, where its selection

34

T. CAMPBELL, J. H. HUGGINS, J. HOW, AND T. BRODERICK

is based on its simplicity, generality, and improved performance compared to the
power-law F-representation. However, results in Appendix G show that when 0 <
d (cid:28) 1, the power-law F-representation bound approaches the undiscounted bound,
and outperforms the V-representation bound.

8. Discussion

We have investigated sequential representations, truncation error bounds, and
simulation algorithms for (normalized) completely random measures. In past work,
the development of these tools—and the analysis of their properties—has happened
only on an ad hoc basis. The results in the present paper, in contrast, provide a
comprehensive characterization and analysis of the diﬀerent types of sequential
(N)CRM representations available to the practitioner. There do remain, however,
a number of open questions, limitations, and directions for future work.

First, this work does not consider the inﬂuence of observed data: all analyses are
presented from an a priori perspective because truncation is typically performed
before data are incorporated via posterior inference (e.g. in standard variational
inference for the DP mixture model (Blei and Jordan, 2006) and BP latent feature
model (Doshi-Velez et al., 2009)). However, analysis of a posteriori truncation has
been studied in past work as well (Ishwaran and James, 2001; Gelfand and Kot-
tas, 2002; Ishwaran and Zarepour, 2002). In the language of CRMs, observations
typically introduce a ﬁxed-location component in the posterior process, while the
unobserved traits are again drawn from the (possibly normalized) ordinary com-
ponent of a completely random measure (Ishwaran and Zarepour, 2002; Broderick
et al., 2014). We anticipate that this property makes observations reasonably simple
to include: the truncation tools provided in the present paper can be used directly
on the unobserved ordinary component, while the ﬁxed-location component may
be treated exactly.

There are two remaining open questions regarding the three types of sequential
representation developed in this work. Regarding the D- and F-representations
developed in Section 3, it is unknown whether a general extension to power-law
representations of the beta, beta prime, and gamma processes exists. The pro-
posed stochastic mapping approach provides an ad hoc solution in the case of
F-representations, but it is not systematic, and it is not yet easily applicable to
D-representations. Regarding V-representations, one might expect that the use of
conjugate exponential family CRMs (Broderick et al., 2014) would yield a closed-
form expression for the truncation bound. In all of the cases provided in this paper,
this was indeed the case; the integrals were evaluated exactly and a closed-form ex-
pression was found. However, we were unable to identify a general expression appli-
cable to all conjugate exponential family CRMs. Based on the examples provided,
we conjecture that such an expression exists.

A ﬁnal remark is that one of the primary uses of sequential representations in
past work has been in the development of posterior inference procedures (Paisley
et al., 2010; Blei and Jordan, 2006; Doshi-Velez et al., 2009)). The present work
provides no guidance on which truncated representations are best paired with which
inference methods. We leave this as an open direction for future research, which
will require both theoretical and empirical investigation.

TRUNCATED RANDOM MEASURES

35

Appendix A. Poisson point process operations

The following results are used extensively throughout our developments; proofs

of all can be found in prior work (Kingman, 1993).

Lemma A.1 (Mapping). Given a Poisson point process P with rate measure ν on
a space Ω, and a measurable function φ : Ω → Ω, the set of transformed points
P (cid:48) = {u : x ∈ P, u = φ(x)} is a Poisson point process with rate measure ν(cid:48)(B) :=
ν(φ−1(B)).

Lemma A.2 (Superposition). Given an inﬁnite collection of Poisson point pro-
cesses with rate measures (νi)∞
i=1, the union of their points is a Poisson point process

with rate measure(cid:80)

i νi.

Lemma A.3 (Thinning). Given a Poisson point process P with rate measure ν on
a space Ω, a function φ : Ω → [0, 1], and a collection of Bernoulli random variables
qx ∼ Bern(φ(x))∀x ∈ P ,

(1) the thinned process P (cid:48) = {x : x ∈ P, qx = 1} is a Poisson point process with
(2) the removed process P (cid:48)(cid:48) = {x : x ∈ P, qx = 0} is a Poisson point process

B φ(x)ν(dx), and

rate measure ν(cid:48)(B) :=(cid:82)
with rate measure ν(cid:48)(cid:48)(B) :=(cid:82)

B(1 − φ(x))ν(dx).

Lemma A.4 (Displacement). Given a Poisson point process P with rate measure
ν on a space Ω, a function φ : Ω × Ω → R+ with φ(·, x) a density on Ω ∀x ∈ Ω,
φ(y,·) measurable ∀y ∈ Ω, and a collection of points qx ∼ φ(·, x)∀x ∈ P , the set
of displaced points P (cid:48) = {qx : x ∈ P} is a Poisson point process with rate measure

ν(cid:48)(B) =(cid:82)

B

(cid:82) φ(y, x)ν(dx)dy.

Appendix B. Technical lemmas

Lemma B.1. If ν(dθ), an absolutely continuous σ-ﬁnite measure on R+, and con-
tinuous φ : R+ → [0, 1] satisfy

min(1, θ)ν(dθ) < ∞,

φ(θ)ν(dθ) < ∞,

(B.1)

(cid:90)

(cid:90)

ν(R+) = ∞,

then

(B.2)

lim
θ→0

φ(θ) = φ(0) = 0.

0 θν(dθ) < ∞ and(cid:82) ∞

Proof. (cid:82) min(1, θ)ν(dθ) < ∞ implies that(cid:82) 1
ν(R+) = ∞, so(cid:82) 1
 ν(dθ) = ∞ =⇒ (cid:82) 1
(cid:82) 1
0 φ(θ)ν(dθ) ≥ c/2(cid:82) θ(cid:48)
(cid:82) θ(cid:48)

1 ν(dθ) < ∞. But
0 ν(dθ) = ∞. In fact, for all  > 0, ν([0, ]) = ∞ since otherwise
 θν(dθ) = ∞, a contradiction. Since φ is continuous and has
bounded range, limθ→0 φ(θ) = c exists and is ﬁnite. Assume c > 0, so ∃ > 0 such
that ∀(cid:48) < , |φ((cid:48)) − c| < c/2, and in particular φ((cid:48)) > c/2. Thus, for any θ(cid:48) < ,
(cid:3)
Lemma B.2. If h(x|θ) is a proper probability mass function on N ∀θ, ν is a
measure on the space of θ, and h is measurable with respect to ν for all x. Then

0 ν(dθ) = ∞, a contradiction. Thus, c = 0.

(cid:90)

∞(cid:88)

(cid:90) ∞(cid:88)

(cid:90)

h(x|θ)ν(dθ) =

h(x|θ)ν(dθ) =

(1 − h(0|θ))ν(dθ).

(B.3)

x=1

x=1

36

T. CAMPBELL, J. H. HUGGINS, J. HOW, AND T. BRODERICK

Proof. Let φn =(cid:80)n
x=1 h(x|θ), and φ =(cid:80)∞
(cid:90)
∞(cid:88)

x=1 h(x|θ). Since h is a probability mass
function, h(x|θ) ≥ 0∀θ. Therefore φn+1 ≥ φn∀n ∈ N. Thus by the monotone
convergence theorem,

h(x|θ)ν(dθ) = lim
n→∞

h(x|θ)ν(dθ)

x=1

(B.4)

x=1

(cid:90) n(cid:88)
(cid:90)
(cid:90)
(cid:90) ∞(cid:88)

φν(dθ)

x=1

=

=

= lim
n→∞

φnν(dθ)

h(x|θ)ν(dθ)

The ﬁnal equality in the result simply follows from h being a probability mass
(cid:3)
function.
Lemma B.3. For α, β > 0 and x ≥ 0,

(cid:18) β

β + x

(cid:19)α ≤ α

β

1 −

x.

(B.5)

Proof. The result follows from the fact that the two expressions and their derivatives
(cid:3)
in x are equal at x = 0, and the left hand side is concave in x.
Lemma B.4. For N ≥ 1 and x ∈ [0, 1],

(B.6)
Proof. Since 1 − xN|x=1 = N (1 − x)|x=1 = 0 and for all x ∈ [0, 1] and N ≥ 1, it
(cid:3)
holds that d

dx [N (1 − x)] = −N ≤ −N xN−1 = d

dx [1 − xN ], the result follows.

1 − xN ≤ N (1 − x).

Lemma B.5. Assume φ(x) is a twice continuously diﬀerentiable function satisfying
lim supn→∞ supy≥xn
n=1 and
φ(cid:48)(cid:48)(x)/φ(cid:48)(x) → 0 as x → ∞. Then for any constant c > 0 and any increasing
sequence (xn)∞

φ(cid:48)(cid:48)(y)
φ(cid:48)(cid:48)(xn) = C < ∞ for any increasing sequence (xn)∞

n=1,
φ(xn + c) − φ(xn) ∼ cφ(cid:48)(xn)

for n → ∞.

Proof. A second-order Taylor expansion of φ(xn + c) about xn yields

φ(xn + c) − φ(xn) = cφ(cid:48)(xn) +

φ(cid:48)(cid:48)(x∗
n),

c2
2

where x∗

n ∈ [xn, xn + c]. Since x∗
n ≥ xn, our assumptions on φ ensure that,
φ(cid:48)(cid:48)(x∗
n)
φ(cid:48)(xn)

Cφ(cid:48)(cid:48)(xn)
φ(cid:48)(xn)

≤ lim
n→∞

lim
n→∞

= 0

and

φ(xn + c) − φ(xn)

cφ(cid:48)(xn)

lim
n→∞

= lim

n→∞ 1 +

φ(cid:48)(cid:48)(x∗
n)
φ(cid:48)(xn)

c
2

= 1.

(B.7)

(B.8)

(B.9)

(B.10)
(cid:3)

Lemma B.6. For α > 0 and x ≥ −1,

M(cid:88)

m=1

Γ(α + m + x)

Γ(α + m)

=

TRUNCATED RANDOM MEASURES

37

(cid:40) 1

(cid:16) Γ(α+M +x+1)
Γ(α+M ) − Γ(α+x+1)

Γ(α)

1+x

ψ(α + M ) − ψ(α)

(cid:17)

x > −1
x = −1

(B.11)

where ψ(·) is the digamma function.
Proof. When M = 1 and x > −1, analyzing the right hand side yields
− Γ(α + x + 1)

− Γ(α + x + 1)

Γ(α + M + x + 1)

Γ(α + x + 2)

=

Γ(α + M )

Γ(α)

(cid:18) α + x + 1

Γ(α)

(cid:19)

− 1

Γ(α + 1)

Γ(α + x + 1)

=

Γ(α)

α
Γ(α + x + 1)

Γ(α + 1)

= (x + 1)

By induction, supposing that the result is true for M − 1 ≥ 1 and x > −1,

.

(cid:19)

Γ(α + m + x)

Γ(α + m)

(cid:18) Γ(α + M + x)

+

Γ(α + M − 1)

m=1
1

1 + x

Γ(α + M + x)

Γ(α + M )
− Γ(α + x + 1)

Γ(α)

+

Γ(α + M + x)

Γ(α + M )

M−1(cid:88)

M(cid:88)

m=1

Γ(α + m + x)

Γ(α + m)

=

=

=

α + M + x

(1 + x)(α + M − 1)

− Γ(α + x + 1)
(1 + x)Γ(α)

(cid:19)

(B.18)
This demonstrates the desired result for x > −1. Next, when x = −1, we have that

Γ(α + M )

1 + x

Γ(α)

=

.

− Γ(α + x + 1)

Γ(α + M + x)
Γ(α + M − 1)
1

(cid:18) Γ(α + M + x + 1)
M(cid:88)

Γ(α + m − 1)
Γ(α + m)

M(cid:88)

=

m=1

m=1

1

α + m − 1

.

(B.19)

We proceed by induction once again. For M = 1, using the recurrence relation
ψ(x + 1) = ψ(x) + x−1 (Abramowitz and Stegun, 1964, Chapter 6), the right hand
side evaluates to

ψ(α + 1) − ψ(α) = ψ(α) + α−1 − ψ(α) = α−1.

Supposing that the result is true for M − 1 ≥ 1 and x = −1,

M(cid:88)

m=1

M−1(cid:88)

m=1

1

α + m − 1

=

1

α + m − 1

+

1

α + M − 1

= ψ(α + M − 1) − ψ(α) +
= ψ(α + M ) − ψ(α),

demonstrating the result for x = −1.
Lemma B.7. For a > 0, d ∈ R, and xn → ∞,

1

α + M − 1

d
dxn

Γ(a + xn + d)

Γ(a + xn)

∼ d(a + xn)d−1.

(B.12)

(B.13)

(B.14)

(B.15)

(B.16)

(B.17)

(B.20)

(B.21)

(B.22)

(B.23)
(cid:3)

(B.24)

38

T. CAMPBELL, J. H. HUGGINS, J. HOW, AND T. BRODERICK

Proof. We have

d
dx

Γ(a + x + d)

Γ(a + x)

=

Γ(a + x + d)

Γ(a + x)

(ψ(a + x + d) − ψ(a + x)),

(B.25)

where ψ is the digamma function. Using Lemma B.5 and the asymptotic expansion
of ψ(cid:48) (Abramowitz and Stegun, 1964, Chapter 6), we obtain

ψ(a + xn + d) − ψ(a + xn) ∼ dψ(cid:48)(a + xn) ∼

d

xn + a

.

Since

Γ(a + xn + d)

Γ(a + xn)

∼ (a + xn)d,

using Lemma B.8(2) with the previous two displays yields the result.

Lemma B.8 (Standard asymptotic equivalence properties).

(1) If an ∼ bn and bn ∼ cn, then an ∼ cn.
(2) If an ∼ bn and cn ∼ dn, then ancn ∼ bndn.
(3) If an ∼ bn, cn ∼ dn and ancn > 0, then an + cn ∼ bn + dn.

(B.26)

(B.27)

(cid:3)

Appendix C. Proofs of sequential representation results

C.1. Correctness of D- and F-representations.

Proof of Theorem 3.1. First, we show that gν(v) is a density. Since vν(v) is non-
dv [vν(v)] ≤ 0 and hence gν(v) ≥ 0.
decreasing, d

dv [vν(v)] exists almost everywhere, d

Furthermore,(cid:90) ∞

0

(cid:90) ∞

0

d
dv

gν(v)dv = −c−1

ν

[vν(v)]dv = −c−1

ν vν(v)

= 1,

(C.1)

(cid:12)(cid:12)(cid:12)∞

v=0

where the ﬁnal equality follows from the assumed behavior of vν(v) at 0 and ∞.
Since for a partition A1, . . . , An, the random variables Θ(A1), . . . , Θ(An) are inde-
pendent, it suﬃces to show that for any measurable set A (with complement ¯A),
the random variable Θ(A) has the correct characteristic function. Deﬁne the family
of random measures

Θt =

Vke−(Γ1k+t)/cν δψk ,

t ≥ 0,

(C.2)

k=1

so Θ0 = Θ. Conditioning on Γ11,
(Θt(A)| Γ11 = u)

D
= V1e−(u+t)/cν 1[ψ1 ∈ A] + Θt+u(A),

(C.3)

∞(cid:88)

TRUNCATED RANDOM MEASURES

39

and note that the two terms on the left hand side are independent. We can thus
write the characteristic function of Θt(A) as

ϕ(ξ, t, A) := E[eiξΘt(A)]

= E[E[eiξΘt(A) | Γ11 = u]]
= E[E[eiξV1e−(u+t)/cν 1[ψ1∈A]eiξΘt+u(A) | Γ11 = u]]
= E[E[(G(A)eiξV1e−(u+t)/cν + G( ¯A))ϕ(ξ, t + u, A)| Γ11 = u]]
eiξve−(u+t)/cν ϕ(ξ, t + u, A)gν(v)e−u du dv

= G(A)

(cid:90) ∞
(cid:90) ∞
(cid:90) ∞

0

0
+ G( ¯A)

0

ϕ(ξ, t + u, A)e−u du,

where ¯A is the complement of A. Multiplying both sides by e−t and making the
change of variable w = u + t yields

e−tϕ(ξ, t, A) = G(A)

eiξve−w/cν ϕ(ξ, w, A)gν(v)e−w dv dw

0

(cid:90) ∞
(cid:90) ∞
(cid:90) ∞
(cid:90) ∞
(cid:90) ∞

t

t

t
+ G( ¯A)

= G(A)

t
+ G( ¯A)

ϕ(ξ, w, A)e−w dw

ϕgν (ξe−w/cν )ϕ(ξ, w, A)e−wdw

ϕ(ξ, w, A)e−wdw,

where ϕgν (a) :=(cid:82) ∞

0 eiavgν(v) dv is the characteristic function of a random variable
with density gν. Diﬀerentiating both sides with respect to t and rearranging yields

∂ϕ(ξ, t, A)

∂t

= ϕ(ξ, t, A) − G(A)ϕgν (ξe−t/cν )ϕ(ξ, t, A) − (1 − G(A))ϕ(ξ, t, A)

(C.4)

(C.5)

(C.6)

(C.7)

(C.8)

(C.9)

(C.10)

(C.11)

(C.12)

(C.13)

(C.14)

= ϕ(ξ, t, A)G(A)(1 − ϕgν (ξe−t/cν )),

so we conclude that

ϕ(ξ, t, A) = exp

(cid:18)

−G(A)

(cid:19)
(1 − ϕgν (ξe−u/cν )) du

.

(cid:90) ∞

t

Using integration by parts and the deﬁnition of gν, rewrite

(cid:90) ∞
ν vν(v)eiav(cid:12)(cid:12)(cid:12)∞
ϕgν (a) = −c−1
(cid:90) ∞
= −c−1

d
dv

ν

0

[vν(v)]eiav dv

+ c−1

ν

v=0

(cid:90) ∞

0

= 1 +

ν(v)eiav dv,

iav
cν

0

iavν(v)eiav dv

(C.15)

(C.16)

40

T. CAMPBELL, J. H. HUGGINS, J. HOW, AND T. BRODERICK

where ﬁnal equality follows from the assumed behavior of vν(v) at 0 and ∞. Com-
bining the previous two displays and setting t = 0 concludes the proof:

(cid:18)
(cid:18)
(cid:18)

−G(A)

−G(A)

−G(A)

(cid:90) ∞
(cid:90) ∞

0

0

(cid:90) ∞
(cid:90) ∞
(cid:90) ∞

0

0

0

iξv
cν
∂
∂u

(cid:104)−eiξve−u/cν(cid:105)
(cid:19)

(eiξv − 1)ν(v) dv

ϕ(ξ, 0, A) = exp

= exp

= exp

e−u/cν eiξve−u/cν ν(v) dv du

(C.17)

ν(v) du dv

(C.18)

.

(C.19)

(cid:3)

(cid:19)

(cid:19)

Proof of Theorem 3.2. It was already shown that gν(v) is a density. Let Θ(cid:48)
ξ ν(cid:48)
k(dθ) is the law of θki. Using the product distribution formula we have

k is a CRM with rate measure cν

k. Each Θ(cid:48)

(cid:80)Ck
i=1 θkiδψki , so Θ =(cid:80)∞
k=1 Θ(cid:48)
(cid:90) 1

where ν(cid:48)

k =
k(dθ),

(− log w)k−1wξ−2gν(θ/w) dw dθ.

(C.20)

ν(cid:48)
k(dθ) =

ξk
Γ(k)

0

0 gν(x) dx be the cdf derived from gν. From the preceding arguments,

conclude that the rate measure of Θ is

Let Gν(v) =(cid:82) v

(cid:12)(cid:12)(cid:12)θ

x=0

The cdf can be rewritten as

(cid:90) θ

0

d
dx

1 − Gν(θ) = 1 + c−1

ν

[xν(x)] dx = 1 + c−1

ν xν(x)

= c−1

ν θν(θ).

(C.27)

Combining the previous two displays, conclude that ν(cid:48)(dθ) = ν(θ) dθ.

(cid:3)

C.2. Power-law behavior of F-representations. We now formalize the sense
in which power-law F-representations do in fact produce power-law behavior. Let
1[znk ≥ 1]. We analyze the number of

Zn | Θ i.i.d.∼ LP(Poiss, Θ) and yk := (cid:80)N
∞(cid:88)

non-zero features after N observations,

n=1

1[yk ≥ 1],

(C.28)

KN :=

k=1

ξk−1
Γ(k)

(− log w)k−1wξ−2gν(θ/w) dw dθ

ν(cid:48)(dθ) :=

cν
ξ

ν(cid:48)
k(dθ)

k=1

∞(cid:88)
(cid:90) 1
∞(cid:88)
(cid:90) 1
(cid:90) 1

k=1

0

0

= cν

= cν

ξw−2gν(θ/w) dw dθ

[−Gν(θ/w)] dw dθ

0

ξθ−1 ∂
= cν
∂w
= −cνθ−1Gν(θ/w)
= cνθ−1(1 − Gν(θ)) dθ.

(cid:12)(cid:12)(cid:12)1

w=0

dθ

(C.21)

(C.22)

(C.23)

(C.24)

(C.25)

(C.26)

TRUNCATED RANDOM MEASURES

41

∞(cid:88)

and the number of features appearing j > 1 times after N observations,

KN,j :=

1[yk = j].

(C.29)

k=1

In their power law analysis of the beta process, Broderick et al. (2012) use a
Bernoulli likelihood process. However, the Bernoulli process is only applicable if
θk ∈ [0, 1], whereas in general θk ∈ R+. Replacing the Bernoulli process with a
Poisson likelihood process is a natural choice since 1[znk ≥ 1] ∼ Bern(1−e−θk ), and
asymptotically 1 − e−θk ∼ θk a.s. for k → ∞ since limk→∞ θk = 0 a.s. Thus, the
Bernoulli and Poisson likelihood processes behave the same asymptotically, which
is what is relevant to our asymptotic analysis. We are therefore able to show that
all CRMs with power-law F-representations, not just the beta process, have what
Broderick et al. (2012) call Type I and II power law behavior. Our only condition
is that the tails of g are not too heavy.

Theorem C.1. Assume that g is a continuous density such that for some  > 0,

(C.30)
Then for Θ ← PL-FRep(γ, α, d, g) with d > 0, there exists a constant C depending
on γ, α, d, and g such that, almost surely,

g(x) = O(x−1−d−).

KN ∼ Γ(1 − d)CN d,
KN,j ∼ d Γ(j − d)

CN d,

j!

N → ∞
N → ∞

(j > 1).

(C.31)

(C.32)

In order to prove Theorem C.1, we require a number of additional deﬁnitions
and lemmas. Our approach follows that in Broderick et al. (2012), which the
reader is encouraged to consult for more details and further discussion of power law
behavior of CRMs. Throughout this section, Θ ← PL-FRep(γ, α, d, g) with d > 0.
By Lemma 3.4, Θ ∼ CRM(ν), where

ν(dθ) :=

g(θ/u)u−1νBP(du) dθ

(C.33)

and νBP(dθ) is the rate measure for BP(γ, α, d). Let Πk be a homogenous Poisson
point process on R+ with rate θk and deﬁne

(cid:90)

∞(cid:88)
∞(cid:88)

k=1

K(t) :=

Kj(t) :=

1[|Πk ∩ [0, t]| > 0]

1[|Πk ∩ [0, t]| = j].

(C.34)

(C.35)

Furthermore, for N ∈ N, let
ΦN := E[KN ]

and for t > 0, let

k=1

and

ΦN,j := E[KN,j]

(j > 1)

(C.36)

Φ(t) := E[K(t)]

and

Φj(t) := E[Kj(t)]

(j > 1).

(C.37)

42

T. CAMPBELL, J. H. HUGGINS, J. HOW, AND T. BRODERICK

If follows from Campbell’s Theorem (Kingman, 1993) that

Φ(t) = E

(1 − e−tθk )

=

(1 − e−tθ)ν(dθ)

(cid:90)

(cid:35)
(cid:35)

k

(cid:34)(cid:88)
(cid:34)(cid:88)
(cid:19)
(cid:18)N
(cid:19)
(cid:18)N

j

k

E

E

j

(cid:34)(cid:88)
(cid:34)(cid:88)

k

k

Φj(t) =

ΦN,j =

ΦN = E

(1 − e−N θk )

= Φ(N )

tj(1 − e−θk )j

j!

e−tθk

(1 − e−θk )je−(N−j)θk

=

(cid:35)

(cid:90)
(cid:18)N

j

tj
j!

=

(cid:35)

(cid:19)(cid:90)

(1 − e−θ)je−tθν(dθ)

(1 − e−θ)je−(N−j)θν(dθ).

(C.41)

(C.38)

(C.39)

(C.40)

(C.42)

(C.43)

The ﬁrst lemma characterizes the power law behavior of Φ(t) and Φj(t). A slowly

varying function (cid:96) satisﬁes (cid:96)(ax)/(cid:96)(x) → 1 as x → ∞ for all a > 0.
Lemma C.2 (Broderick et al. (2012), Proposition 6.1). If for some d ∈ (0, 1),
C > 0, and slowly varying function (cid:96),

θν(dθ) ∼ d
1 − d

C(cid:96)(1/x)x1−d,

x → 0,

(cid:90) x

¯ν[0, x] :=

then

0

Φ(t) ∼ Γ(1 − d)Ctd,
Φj(t) ∼ d Γ(j − d)

Ctd,

j!

t → ∞
t → ∞

(j > 1).

(C.44)

Transferring the power law behavior from Φ(t) to ΦN is trivial since Φ(N ) = ΦN .
The next lemma justiﬁes transferring the power law behavior from Φj(t) to ΦN,j.

Lemma C.3 (Broderick et al. (2012), Lemmas 6.2 and 6.3). If ν satisﬁes Eq. (2.1),
then

K(t) ↑ ∞ a.s.,

Φ(t) ↑ ∞,

Φ(t)/t ↓ 0.

Furthermore,

|ΦN,j − Φj(N )| <

Cj
N

max{Φj(N ), Φj+2(N )} → 0.

(C.45)

(C.46)

The ﬁnal lemma conﬁrms that the asymptotic behaviors of KN and KN,j is

almost surely the same as the expectations of KN and KN,j.
Lemma C.4. Assume ν satisﬁes Eq. (2.1) and that for some d ∈ (0, 1), C > 0,
Cj > 0, and slowly varying functions (cid:96), (cid:96)(cid:48), Φ(t) ∼ C(cid:96)(t)td and Φj(t) ∼ Cj(cid:96)(t)td.
Then for N → ∞, almost surely

Kn ∼ ΦN

and

ΦN,i.

(C.47)

(cid:88)

KN,i ∼(cid:88)

Proof of Theorem C.1. Combining the three lemmas, the result follows as soon as
we show that ν(dθ) satisﬁes Eq. (C.42). C will be a constant that may change from

i<j

i<j

(cid:90) 1

0

(cid:90) ∞

TRUNCATED RANDOM MEASURES

43

line to line. We begin by rewriting ν(dθ) using the change of variable w = θ(u−1−1):

ν(dθ) = C

g(θ/u)u−2−d(1 − u)α+d−1 du dθ

(C.48)

= Cθ−1−d

g(w + θ)

(C.49)
Since g(x) is integrable and continuous, for x ∈ [0, 1], it is upper-bounded by the
non-integrable function C0x−1 for some C0 > 0. Combining this upper bound with
Eq. (C.30) yields g(x) ≤ φ(x) := C0x−11[x ≤ 1] + C1x−1−d−1[x > 1] for some
C1 > 0, so

0

wα+d−1
(w + θ)α−1 dw dθ.

Since φ(w)wd is integrable, by dominated convergence the limit

g(w + θ)

wα+d−1
(w + θ)α−1 ≤ φ(w)wd.
(cid:90) ∞

wα+d−1
(w + θ)α−1 dw

L = lim
θ→0

0

g(w + θ)

(C.50)

(C.51)

(C.54)

(C.55)

(C.56)

(cid:3)

(cid:90) b−θ

a−θ

(cid:90) x

exists and is ﬁnite. Moreover, since g(x) is a continuous density, there exists M > 0
and 0 < a < b < ∞ such that g(x) ≥ M for all x ∈ [a, b]. Hence, for θ < a,

g(w + θ)

wα+d−1
(w + θ)α−1 ≥ M

wα+d−1
(w + θ)α−1 > 0,

(C.52)

(cid:90) ∞
(cid:90) 1

0

so L > 0. Thus,

g(θ/u)u−2−d(1 − u)α+d−1 du → Cθ−d,

ψ(θ) := θ

(C.53)
and hence for δ > 0 and θ suﬃciently small, |ψ(θ) − Cθ−d| < δ. Thus, for x

θ → 0

0

ψ(θ) dθ ≤

suﬃciently small,(cid:90) x

(cid:90) x
≤ C x1−d
1 − d
∼ C x1−d
1 − d
which shows that Eq. (C.42) holds.

0

0

0

+ δx

,

x → 0,

Cθ−d dθ +

|ψ(θ) − Cθ−d| dθ

Appendix D. Proofs of CRM truncation bounds

D.1. Protobound.

Lemma D.1 (Protobound). Let Θ0 and Θ1 be two discrete random measures, and
deﬁne Θ := Θ0 + Θ1. Let X1:N be a collection of random measures generated
from Θ with supp(Xn) ⊆ supp(Θ), and let Y1:N be a collection of random
i.i.d.
variables where Yn is generated from Xn via Yn | Xn ∼ f (·| Xn). Deﬁne Z1:N and
W1:N analagously for Θ0. Finally, deﬁne Q := 1 [supp(X1:N ) ⊆ supp(Θ0)].
If
(X1:N|Θ, Q = 1)

D
= (Z1:N|Θ0), then

(cid:107)pY − pW(cid:107)1 ≤ 1 − P(Q = 1),

1
2

(D.1)

44

T. CAMPBELL, J. H. HUGGINS, J. HOW, AND T. BRODERICK

where pY , pW are the marginal densities of Y1:N and W1:N .

Proof of Lemma 4.1. This is the direct application of Lemma D.1 to CRMs, where
Θ ∼ CRM(ν), and Θ0 is a truncation Θ0 = ΘK. The technical condition is satisﬁed
(cid:3)
because the weights in X1:N are sampled independently for each atom in Θ.

Proof of Lemma 5.1. This is the direct application of Lemma D.1 to NCRMs, where
Θ is the normalization of a CRM with distribution CRM(ν), and Θ0 is the normal-
ization of its truncation. The technical condition is satisﬁed because the condition-
ing on X1:N ⊆ supp(Θ0) is equivalent to normalization of Θ0.
(cid:3)
Proof of Lemma D.1. We begin by expanding the 1-norm and conditioning on Θ
and Θ0:

(cid:107)pY − pW(cid:107)1 =

f (yn|Zn)

− E

=

f (yn|Zn)|Θ0

− E

f (yn|Xn)|Θ

(cid:90) (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)E
(cid:90) (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)E
(cid:35)
(cid:34) N(cid:89)

n=1

(cid:34) N(cid:89)
(cid:34) N(cid:89)
(cid:34)
(cid:34) N(cid:89)
(cid:34)

n=1

E

E

n=1

f (yn|Zn)|Θ0

(cid:35)

(cid:35)

(cid:34) N(cid:89)
(cid:35)

n=1

(cid:35)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) dy

f (yn|Xn)

n=1

(cid:34) N(cid:89)
(cid:35)
(cid:35)
(cid:34) N(cid:89)

|Θ

(cid:34) N(cid:89)

n=1

E

Then conditioning on Q,

f (yn|Xn)|Θ

= E

f (yn|Xn)|Θ, Q

= P(Q = 1|Θ)E

+ P(Q = 0|Θ)E

n=1

n=1

(cid:35)(cid:35)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) dy.

(D.2)

(D.3)

(cid:35)

f (yn|Xn)|Θ, Q = 0

,

where the ﬁrst term arises from the fact that for any function φ,

E [φ(Z1:N )|Θ0] = E [φ(X1:N )|Θ, Q = 1] ,

(D.4)
because X1:N|Θ, Q = 1 is equal in distribution to Z1:N|Θ0. Substituting this back
in above,
(cid:107)pY − pW(cid:107)1

P(Q = 0|Θ)

f (yn|Zn)|Θ0

− E

f (yn|Xn)|Θ, Q = 0

(cid:35)(cid:33)(cid:35)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) dy
(cid:35)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
(cid:35)
(cid:35)(cid:33)(cid:35)

dy

(D.5)

dy,

(cid:90) (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)E
(cid:34)
(cid:34)
(cid:90)
(cid:34)
(cid:90)

E

E

=

≤

≤

(cid:32)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)E
(cid:32)

n=1

E

(cid:34) N(cid:89)
(cid:34) N(cid:89)
(cid:34) N(cid:89)

n=1

E

(cid:35)

(cid:35)
(cid:35)

n=1

(cid:34) N(cid:89)
(cid:34) N(cid:89)
(cid:34) N(cid:89)

n=1

P(Q = 0|Θ)

f (yn|Zn)|Θ0

− E

f (yn|Xn)|Θ, Q = 0

P(Q = 0|Θ)

f (yn|Zn)|Θ0

+ E

f (yn|Xn)|Θ, Q = 0

n=1

n=1

(cid:34)

(cid:32)

(cid:34)(cid:90) N(cid:89)

and ﬁnally by Fubini’s Theorem,
(cid:107)pY − pW(cid:107)1
≤ E
f (yn|Zn)dy|Θ0
= E [P(Q = 0|Θ) (E [1|Θ0] + E [1|Θ, Q = 0])]
= 2 P(Q = 0) = 2(1 − P(Q = 1))

P(Q = 0|Θ)

n=1

E

(cid:35)

+ E

(cid:34)(cid:90) N(cid:89)

n=1

(cid:35)(cid:33)(cid:35)

f (yn|Xn)dy|Θ, Q = 0

(D.6)

TRUNCATED RANDOM MEASURES

45

(cid:3)

, (D.7)

(D.8)

(D.9)

(D.10)

D.2. D-representation truncation. For this section, let c := cν.
Lemma D.2. For K ≥ 0,
P(supp(X1:N ) ⊆ supp(ΘK)) = E
where GK ∼ Gam(K, c) and G0 = 0.

(cid:90) ∞

(cid:26)

(cid:20)

exp

−

0

(cid:27)(cid:21)
(cid:0)1 − π(ve−GK )N(cid:1) ν(dv)
Vke−(Γ1k+t)/c(cid:17)N(cid:35)

,

Proof. Let

p(t, K) := E

so p(0, K) = P(supp(X1:N ) ⊆ supp(ΘK)). We use the proof strategy from Banjevic
et al. (2002) and induction in K. For K = 0,

(cid:35)(cid:35)
1<j≤k E1j +u+t)/c(cid:17)N(cid:12)(cid:12)(cid:12)Γ11 = u
−((cid:80)

π

Vke

p(t, 0) = E

E

(cid:34) ∞(cid:89)

k=K+1

(cid:16)

π

(cid:16)

(cid:34)
(cid:90) ∞

(cid:34)
(cid:16)
(cid:90) ∞

π

=

0

0

e−tp(t, 0) =

k=2

V1e−(u+t)/c(cid:17)N ∞(cid:89)
(cid:16)
ve−(u+t)/c(cid:17)N
(cid:90) ∞
(cid:90) ∞
(cid:16)

π

π

t

0

p(t + u, 0)e−ugν(v) dv du.

ve−w/c(cid:17)N
(cid:90) ∞
(cid:16)
ve−u/c(cid:17)N
(cid:16)

(cid:90) ∞

π

π

0

ve−t/c(cid:17)N

Multiplying both sides by e−t and making the change of variable w = u + t yields

p(w, 0)e−wgν(v) dv dw.

(D.11)

Diﬀerentiating both sides with respect to t and rearranging yields

∂p(t, 0)

= p(t, 0) − p(t, 0)

∂t

so we conclude that

p(t, 0) = exp

(cid:90) ∞

(cid:18)

(cid:26)

−

1 −

t

0

gν(v) dv,

(D.12)

(cid:19)

(cid:27)

gν(v) dv

du

.

(D.13)

Using the deﬁnition of gν(v) and integration by parts, the inner integral from the
previous display can be rewritten as

(cid:90) ∞

π

0

= − 1
c
= − 1
c

gν(v) dv

ve−u/c(cid:17)N
(cid:16)
(cid:90) ∞
(cid:16)
ve−u/c(cid:17)N d
(cid:12)(cid:12)(cid:12)∞
ve−u/c(cid:17)N
(cid:16)
(cid:90) ∞
π(cid:48)(cid:16)

vν(v)
N ve−u/c

dv

v=0

π

π

0

0

c

= π(0)N +

[vν(v)] dv

(cid:90) ∞
ve−u/c(cid:17)N−1

+

0

c

N ve−u/c

π(cid:48)(cid:16)

ve−u/c(cid:17)N−1

(D.14)

(D.15)

ν(v) dv

(D.16)

ν(v) dv.

(D.17)

46

T. CAMPBELL, J. H. HUGGINS, J. HOW, AND T. BRODERICK

Thus, combining the previous two displays and using Lemma B.1 with φ = 1 − π,
we have

0

(cid:90) ∞
(cid:90) ∞
(cid:90) ∞
(cid:90) ∞

0

0

(cid:26)
(cid:26)
(cid:26)
(cid:26)

−

−

−

−

c

ve−u/c(cid:17)N−1
(cid:27)

(cid:90) ∞
π(cid:48)(cid:16)
− N ve−u/c
ve−u/c(cid:17)N(cid:21)
(cid:20)
(cid:90) ∞
(cid:16)
ve−u/c(cid:17)N(cid:12)(cid:12)(cid:12)∞
(cid:16)
ve−t/c(cid:17)N(cid:19)
(cid:18)
(cid:16)

1 − π

d
du

ν(v)dv

ν(v)dv

(cid:27)

u=t

π

π

.

t

t

du ν(v)dv

p(t, 0) = exp

= exp

= exp

= exp

(cid:27)

du ν(v)dv

(cid:27)

The inductive hypothesis is that

0

p(t, K) = E[p(t + ˜GK, 0)],

˜G0 = 0,
which trivially holds for K = 0. Assume (IH) holds for some K ≥ 0, so

˜GK ∼ Gam(K, 1),

(cid:35)(cid:35)
1<j≤k E1j +u+t)/c(cid:17)N(cid:12)(cid:12)(cid:12)Γ11 = u
−((cid:80)
E1 ∼ Exp(1)

(D.18)

(D.19)

(D.20)

(D.21)

(IH)

(D.22)

(D.23)

(D.24)

(D.25)
(cid:3)

p(t, K + 1) = E

E

(cid:34)

(cid:34) ∞(cid:89)

(cid:16)

π

Vke

k=K+2

= E[p(t + E1, K)],
= E[p(t + ˜GK + E1, 0)]
= E[p(t + ˜GK+1, 0)].

The result follows by setting t = 0 and observing that ˜GK/c

D
= GK.

0

fact that the integral(cid:82) ∞
the function φ : x (cid:55)→ (cid:82) ∞

(cid:0)1 − E(cid:2)π(ve−GK )(cid:3)(cid:1) ν(dv) is non-negative. Next, note that

Proof of Theorem 4.2. First combine Lemmas 4.1 and D.2, then apply Jensen’s
inequality and Lemma B.4. The bounds on B(N, K) follow immediately from the
(1 − π(vx)) ν(dv) is absolutely continuous by Schilling
(2005, Theorem 11.4) since (a) x, v (cid:55)→ 1 − π(vx) is continuous, (b) 1 − π(vx) ≤
¯φ(v) := supy∈[0,v] 1 − π(y), and (c) ¯φ(v) is ν-integrable by the continuity of π and
D
the assumption that 1 − π is ν-integrable. Since HK := e−GK
=⇒ 0, by the
continuous mapping theorem, φ(GK) → φ(0) = 0. Conclude that B(N, K) → 0
since, by continuity of the exponential,

0

lim

K→∞ φ(HK)

K→∞ 1 − exp{−N φ(HK)} = 1 − exp
(cid:80)Ck
(cid:80)∞
i=1 θkiδψki , so Θ =(cid:80)∞
k :=(cid:80)Ck
be written as ΘK =(cid:80)K

D.3. F-representation truncation. Begin by writing an F-representation Θ =
i=1 θkiδψki as a the sum of a countable number of (ﬁnite) CRMs. Let
Θ(cid:48)
k. Since Ck is Poisson-distributed with mean
cξ−1, each Θ(cid:48)
k is the
measure corresponding to the distribution of θki. Thus, the truncated measure can
k and the discarded portion ΘK+ := Θ− ΘK is a CRM

k is itself a CRM with rate measure cξ−1ν(cid:48)

.

(D.26)
(cid:3)

k(dθ), where ν(cid:48)

k=1 Θ(cid:48)

k=1 Θ(cid:48)

k=1

(cid:110)−N lim

(cid:111)

with rate measure

K(dθ) := cξ−1
ν+

ν(cid:48)
k(dθ).

(D.27)

∞(cid:88)

k=K+1

TRUNCATED RANDOM MEASURES

47

Lemma D.3 (Paisley et al. (2012b)). Let (ψ, θ) form a Poisson process on Ψ×R+
with mean measure µ. Mark each (ψ, θ) with a random variable U in a ﬁnite space
S with transition probability kernel Q(θ,·). Then the collection of (ψ, θ, U ) tuples
forms a Poisson process on Ψ × R+ × S with mean measure µ(dψ, dθ)Q(θ, U ).

Lemma D.4 (cf. Paisley et al. (2012b, Theorem 3)).

(cid:26)

(cid:90)

(cid:27)

P(supp(X1:N ) ⊆ supp(ΘK)) = exp

−

(1 − π(θ)N ) ν+

K(dθ)

(D.28)

Proof. Apply Lemma D.3 with µ(dψ, dθ) = G(ψ)ν+

Q(θ, U ) =(cid:81)N
with parameter(cid:82) µ(Ψ, dθ)Q(θ, A) =(cid:82) (1 − π(θ)N )νK+(dθ), the result follows. (cid:3)

K(dθ), S = {0, 1}N , and
n=1 π(θ)1−Un (1−π(θ))Un . Let A = S \0, where 0 is the zero vector, so
K (dψ, dθ, U ) be the corresponding counting measure for
K (Ψ, R+, A) =
K (Ψ, R+, A) is a Poisson-distributed random variable

Q(θ, A) = 1−π(θ)N . Let N +
µ(dψ, dθ)Q(θ, U ). Observe that P(supp(X1:N ) ⊆ supp(ΘK)) = P(N +
0). Since, by Lemma D.3, N +

Proof of Theorem 4.3. In the case of a decoupled Bondesson F-representation,

∞(cid:88)

k=K+1

ν(cid:48)
k(dθ),

(D.29)

where ν(cid:48)

(cid:90)

k(dθ) is the law of θki from Eq. (3.4). We then have that

ξc−1

(1 − π(θ)) ν+

K(dθ) =

(1 − π(θ)) ν(cid:48)

k(dθ)

K(dθ) := cξ−1
ν+
(cid:90)

k=K+1

∞(cid:88)
∞(cid:88)
∞(cid:88)

by deﬁnition of v+
K

(D.30)

=

=

k=K+1

k=K+1

E[1 − π(θki)]

by deﬁnition of ν(cid:48)

k

E[1 − π(βk)]

(D.31)

since θki

D
= βk.

(D.32)

The error bound is now an immediate consequence of Lemmas 4.1, B.4 and D.4,
and the previous display.

The bounds on B(N, K) follow from the fact that the integral(cid:82) (1−π(θ)) ν+

K(dθ)

is non-negative. The convergence of B(N, K) to zero follows since

K→∞ 1 − exp

lim

− c
ξ

N

E[(1 − π(βk))]

E[(1 − π(βk))]

(cid:40)

∞(cid:88)

k=K+1

(cid:41)

∞(cid:88)

k=K+1

≤ lim
K→∞

c
ξ

N

= 0

by monotone convergence.

Proof of Theorem 4.4. The proof is analogous to that for Theorem 4.3.

(D.33)
(cid:3)

(cid:3)

48

T. CAMPBELL, J. H. HUGGINS, J. HOW, AND T. BRODERICK

D.4. V-representation truncation.

Proof of Theorem 4.5. We begin with Lemma 4.1, and expand the probability that
observations are sampled only from the truncated portion of the CRM by iterated
conditioning:

P (supp(X1:N ) ⊆ supp(ΘK))

N(cid:89)

=

P (supp(Xn) ⊆ supp(ΘK)| supp(X1:n−1) ⊆ supp(ΘK)) .

(D.34)

n=1

From the constructive proof of Theorem 3.3 (Broderick et al., 2014, Thm. 5.1), we
have that after K rounds of construction, the remaining ordinary component Θ+
K
is a CRM with distribution Θ+
CRM with ﬁxed-location atoms and independent weights, the posterior of Θ+
K given
that no X(cid:96), (cid:96) = 1, . . . , n − 1, shares an atom with it can be viewed as a thinned
compound Poisson process, yielding

K ∼ CRM(cid:0)π(θ)Kν(dθ)(cid:1). Further, since Xn | Θ is a

K | supp(X1:n−1) ⊆ supp(ΘK) ∼ CRM(cid:0)π(θ)n−1π(θ)Kν(dθ)(cid:1) .

(D.35)
Finally, the statement supp(Xn) ⊆ supp(ΘK)| supp(X1:n−1) ⊆ supp(ΘK) is equiv-
K | supp(X1:n−1) ⊆ supp(ΘK) thinned by (1 − π(θ)) has no
alent to saying that Θ+
atoms (if any Bernoulli trial of probability (1 − π(θ)) succeeded, Xn would have a
with measure µ(dθ) has no atoms with probability e−(cid:82) µ(dθ), so
nonzero-weight atom outside the support of ΘK). Finally, a Poisson point process

Θ+

P (supp(X1:N ) ⊆ supp(ΘK))

=

n=1

N(cid:89)
N(cid:89)
= e−(cid:80)N
= e−(cid:80)N

n=1

=

n=1 ΛK+n.

P (supp(Xn) ⊆ supp(ΘK)| supp(X1:n−1) ⊆ supp(ΘK))
e−(cid:82) (1−π(θ))π(θ)K+n−1ν(dθ)
(cid:82) (1−π(θ))π(θ)K+n−1ν(dθ)

n=1

and therefore,

(cid:107)pN,∞ − pN,K(cid:107)1 ≤ 1 − e−(cid:80)N

1
2

n=1 ΛK+n .

(D.40)
The bounds on B(N, K) are an immediate consequence of Λn ≥ 0 ∀n ∈ N. Next,
to derive the asymptotic behavior of the bound, note that

lim

n=1 ΛK+n = lim

K→∞ 1 − e−(cid:80)N

(cid:82) (1−π(θ))π(θ)K+n−1ν(dθ)
n=1 limK→∞(cid:82) (1−π(θ))π(θ)K+n−1ν(dθ)
by the continuity of the exponential function. Next, since π(θ) ∈ [0, 1],

K→∞ 1 − e−(cid:80)N
= 1 − e−(cid:80)N

(cid:12)(cid:12)(1 − π(θ))π(θ)K+n−1(cid:12)(cid:12) = (1 − π(θ))π(θ)K+n−1

n=1

≤ 1 − π(θ).

(D.36)

(D.37)

(D.38)

(D.39)

(D.41)

(D.42)

(D.43)

(D.44)

By Eq. (2.3), (cid:82) (1 − π(θ)) ν(dθ) < ∞. Therefore, by the Lebesgue dominated

TRUNCATED RANDOM MEASURES

49

(cid:90)

lim
K→∞

convergence theorem,

(1 − π(θ))π(θ)K+n−1ν(dθ) =

(cid:90)

= 0.

K→∞(1 − π(θ))π(θ)K+n−1ν(dθ)

lim

(D.45)

(D.46)
(cid:3)

D.5. Stochastic mapping truncation.
Proof of Proposition 4.6. Let ˜π(u) = ˜h(0| u). For notational brevity, deﬁne Q to
be the event where supp(X1:N ) ⊆ supp(ΘK), and ˜Q to be the corresponding trans-
formed event where supp( ˜X1:N ) ⊆ supp( ˜ΘK). Then we have

P( ˜Q) = E(cid:104)(cid:81)∞

.

k=K+1 ˜π(uk) ˜N(cid:105)
(cid:105)
(cid:82) ˜π(u) ˜N κ(θk, du)
k=K+1 ˜π(τ (θk)) ˜N(cid:105)

For the case of a kernel κ, if h(x| θ) = Bern(x; 1 − πκ, ˜N (θ)) and N = 1, then

(D.48)
For the case of a transformation τ , if h(x| θ) = Bern(x; 1 − ˜π(τ (θ))) and N = ˜N ,
then

k=K+1

= P(Q).

P( ˜Q) = E(cid:104)(cid:81)∞
P( ˜Q) = E(cid:104)(cid:81)∞

= P(Q).

(D.47)

(D.49)
(cid:3)

D.6. Truncation with hyperpriors.

Proof of Proposition 4.7. By repeating the proof of Lemma 4.1 in Appendix D.1,
except with an additional use of the tower property to condition on the hyper-
parameters Φ, and an additional use of Fubini’s theorem to swap integration and
expectation, we have

1
2

(cid:107)pY − pW(cid:107)1 ≤ E [1 − P (supp(X1:N ) ⊆ supp(ΘK)| Φ)]

≤ E [B(Φ, N, K)] .

D.7. Comparing D- and F-representation truncations.

Theorem D.5. Assume ν satisﬁes the conditions of Theorem 3.1 and let

Θ ← B-DRep(cν, gν)
˜Θ ← B-FRep(cν, gν, cν)

Xn | Θ i.i.d.∼ LP(h, Θ)
˜Xn | ˜Θ i.i.d.∼ LP(h, ˜Θ).

(D.50)

(D.51)
(cid:3)

(D.52)

(D.53)

Then for all K, N ∈ N,

P(supp(X1:N ) ⊆ supp(ΘK)) ≥ P(supp( ˜X1:N ) ⊆ supp( ˜ΘK)).

(D.54)
indep∼ Gam(k, cν). Recall (cf. Appen-

Proof. Let V ∼ g, Tk
indep∼ Gam(k, cν), and Gk
dix D.3) that for K ≥ 0 and any function φ(θ),

(cid:90)

∞(cid:88)

k=K+1

φ(θ)ν+

K(dθ) =

E[φ(V e−Tk )]

(D.55)

50

T. CAMPBELL, J. H. HUGGINS, J. HOW, AND T. BRODERICK

and ν = ν+

P(supp(X1:N ) ⊆ supp(ΘK))
= E

0 . Letting φ(θ) = 1 − π(θ)N , we have
(cid:20)
(cid:34)

(cid:27)(cid:21)
(cid:41)(cid:35)

φ(θe−GK )ν(dθ)

exp

= E

exp

E[φ(V e−Tk−GK )]

−

−

(cid:26)
(cid:90)
(cid:40)
∞(cid:88)
∞(cid:88)
∞(cid:88)
∞(cid:88)

k=1

k=1

k=1

k=K+1

(cid:90)

(cid:40)
(cid:40)
(cid:40)
(cid:26)

−

−

−

−

(cid:41)
(cid:41)
(cid:41)

E[φ(V e−Tk−GK )]

E[φ(V e−TK+k )]

E[φ(V e−TK )]

(cid:27)

≥ exp

= exp

= exp

φ(θ)ν+

= exp
= P(supp( ˜X1:N ) ⊆ supp( ˜ΘK))

K(dθ)

(D.56)

Lemma D.2

(D.57)

Eq. (D.55)

(D.58)

Jensen’s inequality

(D.59)

Tk + GK

D
= TK+k

(D.60)

(D.61)

Eq. (D.55)

(D.62)

Lemma D.4.

(D.63)
(cid:3)

Appendix E. Proofs of normalized truncation bounds

Proof of Lemma 5.2. First, we demonstrate that the arg max is well-deﬁned. Note
that

arg max

i∈N

Ti + log pi = arg max

i∈N

exp (Ti + log pi)

(E.1)

if it exists, due to the monotonicity of exp. Similarly, existence of either proves the
existence of the other. Since Ti are i.i.d. Gumbel(0, 1),

P (exp (Ti + log pi) > ) = P (Ti > log  − log pi)

= 1 − exp
= 1 − exp (−pi/) .

(cid:16)−e−(log −log pi)(cid:17)
∞(cid:88)
∞(cid:88)

1 − exp (−pi/)

1 − (1 − pi/)

i=1

∞(cid:88)

i=1

= −1

pi < ∞.

Therefore,

∞(cid:88)

i=1

P (exp (Ti + log pi) > ) =

≤

This is suﬃcient to demonstrate that

exp (Ti + log pi) a.s.→ 0

as i → ∞.

i=1

(E.2)

(E.3)

(E.4)

(E.5)

(E.6)

(E.7)

(E.8)

TRUNCATED RANDOM MEASURES

51

Finally, since any positive sequence converging to 0 can only have a ﬁnite number
of elements greater than any  > 0, set  = exp(T1 + log p1), and thus

arg max

i∈N

exp (Ti + log pi) = arg max

i:Ti+log pi≥

exp (Ti + log pi)

(E.9)

where the right hand side exists because it computes the maximum of a ﬁnite,
nonempty set of numbers. Note that the arg max is guaranteed to be a single
element, since Ti + log pi has a purely diﬀuse distribution on R.

Now that the a.s. existence and uniqueness of the arg max has been demon-

strated, we can compute its distribution. First, note that

P (Ti + log pi ≤ x∀i ∈ N, i (cid:54)= j) =

P (Ti + log pi ≤ )

i=1,i(cid:54)=j

∞(cid:89)
∞(cid:89)
= exp(cid:0)−e−x(s − pj)(cid:1) ,

i=1,i(cid:54)=j

(cid:16)−e−(x−log pi)(cid:17)

exp

=

where s :=(cid:80)
(cid:18)

P

i pi. So if fj(x) is the density of Tj + log pj,

j = arg max

i∈N

Ti + log pi

= P (Ti + log pi ≤ Tj + log pj ∀i ∈ N)

(E.10)

(E.11)

(E.12)

(E.13)

(E.16)

(E.17)

(E.18)

(E.19)

(cid:19)

(cid:90)
(cid:90)

(cid:90)

(cid:90)

(cid:90)

elog s

pj
s

pj(cid:80)
pj(cid:80)

i pi

,

i pi

=

=

=

=

=

P (Ti + log pi ≤ x∀i ∈ N, i (cid:54)= j) fj(x)dx (E.14)
e−e−x(s−pj )e−(x−log pj +e

−(x−log pj ))dx

(E.15)

= pj

e−se−x

e−xdx

e−se−x

e−xdx

e−e−(x−log s)

e−(x−log s)dx

where the last integral is computed as the normalizing constant of the Gumbel(s, 1)
(cid:3)
distribution. This proves the desired result.

Lemma E.1. The function

F : R → R, F (x) :=

e−θe−x − 1

ν(dθ)

(E.20)

(cid:17)

is inﬁnitely diﬀerentiable, and satisﬁes

dkF
dxk (x) =

(E.21)
Proof. First, note that the kth order derivative for any k ∈ N, k ≥ 1 exists and can
be expressed in the form

dxk

e−θe−x − 1

ν(dθ) ∀k ∈ N, k ≥ 1.

e−θe−x − 1

=

akjθje−jx−θe−x

,

(E.22)

(cid:90) dk

(cid:16)

(cid:16)

dk
dxk

(cid:17)

(cid:90) (cid:16)
(cid:17)

k(cid:88)

j=1

52

T. CAMPBELL, J. H. HUGGINS, J. HOW, AND T. BRODERICK

where akj are real coeﬃcients. This function is bounded, continuous, and locally
integrable in the sense that for any compact K ⊂ R,

akjθje−jx−θe−x

K

j=1

(cid:90)

(cid:90) (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
 k(cid:88)
grable since(cid:82)(cid:12)(cid:12)(cid:12)e−θe−x − 1

the desired result.

(cid:90)

(cid:90)

|akj|

θje−jx−θe−x

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ν(dθ)dx ≤ k(cid:88)
(cid:12)(cid:12)(cid:12) ν(dθ) is continuous in x. This is suﬃcient to demonstrate

< ∞,

ν(dθ)dx

(E.23)

(E.24)

j=1

K

(cid:3)

where the ﬁniteness follows since the inner integral is continuous in x. It is also
straightforward to show that e−θe−x − 1 is bounded, continuous, and locally inte-

E.1. F- and V-representation truncation.

Proof of Theorem 5.3. First, we apply Lemma 5.1,

(cid:107)pY − pW(cid:107)1 ≤ 1 − P (X1:N ⊆ supp(ΘK)) .

1
2

Next, by Jensen’s inequality,

P (X1:N ⊆ supp(ΘK)) = E

(cid:34)(cid:18) ΘK (Ψ)
(cid:19)N(cid:35)
(cid:21)N
(cid:20) ΘK (Ψ)

Θ (Ψ)

≥ E
= P(X1 ∈ supp(ΘK))N .

Θ (Ψ)

(E.25)

(E.26)

(E.27)

(E.28)

The remaining part of this proof quantiﬁes the probability that sampling X1 from Ξ
generates an atom in the support of ΞK. Since most of the following developments
are similar for ΘK, νK and Θ+
K, we will focus the discussion on ΘK, νK and
reintroduce the tail quantities when necessary. First, we apply Lemma A.1 to
transform the Poisson point process of rates in ΘK under the mapping w = log θ,
resulting in the rate measure

K, ν+

(E.29)
Next, given the Gumbel(0, 1) density on R with respect to the Lebesgue measure,
e−x−e−x
, we displace each point in the process by an i.i.d. Gumbel(0, 1) random
variable. This yields another Poisson point process with rate measure

ewνK(ew)dw.

e−(t−w)−e−(t−w)

ewνK(ew)dw

dt.

(E.30)

(cid:18)(cid:90)

(cid:19)

The probability that all points in this Poisson point process are less than a value
x is equal to the probability that there are no atoms above x. Deﬁning MK to be
the supremum of the points in this process, combined with the basic properties of
Poisson point processes, we have

P(MK ≤ x) = exp

−

e−(t−w)−e−(t−w)

ewνK(ew)dwdt

(E.31)

(cid:19)

(cid:18)

(cid:90) ∞

(cid:90)

x

TRUNCATED RANDOM MEASURES

53

Using Fubini’s theorem to swap the integrals, we can evaluate the inner integral
analytically by noting the integrand is a Gumbel(w, 1) density,

P(MK ≤ x) = exp

= exp

(1 − e−e−(x−w)

)ewνK(ew)dw

(cid:19)

(cid:17)

e−θe−x − 1

νK(dθ)

.

(E.32)

(E.33)

(cid:18)
(cid:90)
(cid:18)(cid:90) (cid:16)

−

(cid:19)

Using Lemma E.1, we can take the derivative with respect to x to obtain its density
with respect to the Lebesgue measure by swapping integration and diﬀerentiation,

gK(x) :=

dP(MK ≤ x)

dx

= P(MK ≤ x)

θe−x−θe−x

νK(dθ).

(E.34)

(cid:90)

K, and compute the probability that MK exceeds

Now we reintroduce the CRM Θ+
M +
K :
P(M +

K ≤ MK) = 1 − P(MK ≤ M +
K )

(cid:90)
(cid:90) (cid:90)
K(x)P(MK ≤ x)dx
g+
θe−x−θe−x

ν+
K(dθ) exp

= 1 −

= 1 −

Using the transformation t = e−x,

(cid:90) (cid:90)

P(M +

K ≤ MK) = 1 −

1 [t ≥ 0] θe−θtν+

K(dθ) exp

(cid:18)(cid:90) (cid:16)

(cid:17)

e−θe−x − 1

(E.35)

(E.36)

(cid:19)

ν(dθ)

dx.

(E.37)

(cid:18)(cid:90) (cid:0)e−θt − 1(cid:1) ν(dθ)

(cid:19)

dt.

(E.38)

Finally, noting that νK, ν+
K both satisfy Eq. (2.1), the sums of the atom weights in
both ΘK, Θ+
K are ﬁnite a.s.; therefore by Lemma 5.2, there exists a unique atom of
maximum weight in the union of the log-transformed, Gumbel-displaced ΘK and
K ≤ MK) = P(supp(X1) ⊆
K, and this atom is a sample from Ξ. Therefore, P(M +
Θ+
supp(ΘK)), the probability that an atom sampled from Ξ is selected from the set
of points in the support of ΘK. Substituting this into Eq. (E.28) yields the result.
The fact that the error bound asymptotically approaches 0 is a consequence of
the monotone convergence theorem applied to the decreasing sequence of functions
θν+
K(dθ). The fact that the bound lies between 0 and 1 is simple given that the
stated bound is equal to 1 − E [ΘK(Ψ)/Θ(Ψ)]N and 0 ≤ ΘK(Ψ)/Θ(Ψ) ≤ 1 a.s. (cid:3)

E.2. D-representation truncation.

Proof of Theorem 5.4. The same initial technique as in the proof of Theorem 5.3
yields

(cid:107)pY − pW(cid:107)1 ≤ 1 − P (X1 ∈ supp (ΘK))N .

1
2

(E.39)

54

T. CAMPBELL, J. H. HUGGINS, J. HOW, AND T. BRODERICK

∞(cid:88)

The remaining part of this proof quantiﬁes the probability that sampling X1 from
Ξ generates an atom in the support of ΞK. First, we rewrite the sequential repre-
sentation of Θ (proportional to its normalization Ξ) as

(cid:32) K(cid:88)

∞(cid:88)

(cid:33)

Ξ ∝

Vke−Γck δψk = e−ΓcK

VkeΓcK−Γck δψk +

VkeΓcK−Γck δψk

.

k=1

k=1

k=K+1

(E.40)
Since the measure will be normalized, the e−ΓcK factor premultiplying it can be
dropped. Further, using the deﬁnition of Γck and reindexing the rightmost term,
we have

Ξ ∝ Θ(cid:48) + Θ(cid:48)(cid:48),

(E.41)

where Θ(cid:48) and Θ(cid:48)(cid:48) are deﬁned as

K(cid:88)

Θ(cid:48) :=

VkeΓcK−Γck δψk

Θ(cid:48)(cid:48) ∼ CRM(ν)

Θ(cid:48) ⊥⊥ Θ(cid:48)(cid:48).

(E.42)

k=1

Using the same technique employed in the proof of Theorem 5.3, if we deﬁne M
to be the supremum of the rates in Θ(cid:48)(cid:48) after log-transformation and perturbation
with i.i.d. Gumbel(0, 1) random variables, we can characterize its distribution as

(E.43)
where F : R → R is as deﬁned in Lemma E.1. Then if W ∼ Gumbel(0, 1), using
Lemma 5.2, we have

P (M ≤ x) = eF (x),

P (X1 ∈ supp (ΘK)) = P

(cid:33)

(cid:32) K(cid:88)

(cid:32)
≥ P(cid:0)log V1 + Γc(K−1) + W ≥ M(cid:1) .

VkeΓcK−Γck

log

k=1

+ W > M

(cid:33)

Γc(K−1) is used in place of (cid:80)K

The inequality arises from noting that all terms in the sum are positive, and hence
the probability can be bounded below by selecting only the k = 1 term. Note that
k=2 Eck for convenience since they have the same
distribution. Next, we compute the cumulative distribution function of log V1 + W :

P (log V1 + W ≤ x) = −c−1
= −c−1

(evν (ev)) dv

(sν (s)) ds

(E.44)

(E.45)

(E.46)

(E.47)

(E.48)

e−e−(x−v) d
dv
e−se−x d
ds

(cid:90)
(cid:90) ∞
(cid:90) ∞
(cid:90) ∞

0

0

(cid:16)

= 1 − c−1

se−x−se−x

ν(s)ds

(cid:17)

e−se−x − 1

= 1 − c−1 d
dx
= 1 − c−1 dF
dx

0

(x),

ν(ds)

(E.49)

(E.50)

where the ﬁrst step follows from the substitution v = es, the second follows from
integration by parts, and the third follows from Lemma E.1. Using this result to

TRUNCATED RANDOM MEASURES

55

compute the cumulative distribution function of M − (log V1 + W ),

P (M − (log V1 + W ) ≤ x) =

P (log V1 + W ≤ v) dv

(E.51)

(cid:90)

(cid:90)

P (M ≤ x + v)

d
dv
eF (x+v) d2F

= −c−1

dv2 (v)dv.

(E.52)

The second order derivative of F exists by Lemma E.1. Finally, using the fact that
Γc(K−1) ∼ Gam(K − 1, c),

P (X1 ∈ supp (ΘK)) ≥ cK−1
Γ(K − 1)

−cK−2
Γ(K − 1)

=

(cid:90) ∞
(cid:90)(cid:90)

0

P (M − (log V1 + W ) ≤ x) xK−2e−cxdx

(E.53)

1 [x ≥ 0] xK−2e−cxeF (x+v) d2F

dv2 (v)dvdx.

(E.54)

The fact that the bound is between 0 and 1 is a simple consequence of
P (X1 ∈ supp (ΘK)) ≤ 1 and the nonnegativity of the lower bound above. The
asymptotic decay of the bound follows by interpreting the integral above as an
expectation, and noting that F (x) ≤ 0, F (x) is monotonically increasing, and
limx→∞ F (x) = 0. For any  > 0, we can pick a ≥ 0 such that

lim
K→∞
≥ lim
K→∞

=P(cid:16)

E(cid:104)
eF (Γc(K−1)+log V1+W )(cid:105)
P(cid:0)Γc(K−1) ≥ a(cid:1) P(cid:16)
(cid:17)
(cid:1) = 1 and lima→∞ eF ( a
eF (Γc(K−1)+log V1+W )(cid:105)

log V1 + W ≥ − a
2
2 )
eF ( a

= 1,

log V1 + W ≥ − a
2

lim
K→∞

≥(1 − )2,

since lima→∞ P(cid:0)log V1 + W ≥ − a
E(cid:104)

2

(cid:17)

2 )
eF ( a

2 ) = 1. Therefore,

(E.55)

(E.56)

(E.57)

(E.58)

(E.59)

(cid:3)

and the desired asymptotic result follows.

E.3. Truncation with hyperpriors.

Proof of Proposition 5.5. By repeating the proof of Lemma 5.1 in Appendix D.1,
except with an additional use of the tower property to condition on the hyper-
parameters Φ, and an additional use of Fubini’s theorem to swap integration and
expectation, we have

1
2

(cid:107)pY − pW(cid:107)1 ≤ E [1 − P (X1:N ⊆ supp(ΘK)| Φ)]

≤ E [B(Φ, N, K)] .

(E.60)

(E.61)
(cid:3)

56

T. CAMPBELL, J. H. HUGGINS, J. HOW, AND T. BRODERICK

Proof of Lemma 6.1. Deﬁne λ =(cid:80)

i λi, and ¯λm =(cid:80)∞

Appendix F. Proofs of V-representation sampling results

i=m λi. The proof proceeds

by induction. First,

P(T1 = t1) =

=

P(T1 = t1, S1 = n)

P(T1 = t1|S1 = n)P(S1 = n)

(cid:19)t1(cid:18) λ − λ1

(cid:19)n−t1

(cid:18) n

λ

e−λ λn
n!

1

(n − t1)!

(λ − λ1)n−t1

n=0

∞(cid:88)
∞(cid:88)
∞(cid:88)

n=0

(cid:19)(cid:18) λ1
∞(cid:88)

λ

n=t1

eλ−λ1

=

n=t1

t1
= e−λ λt1
1
t1!
= e−λ λt1
1
t1!
λt1
1
t1!

= e−λ1

(F.1)

(F.2)

(F.3)

(F.4)

(F.5)

(F.6)

(cid:80)

and therefore T1 is marginally Poisson with mean λ1. Now assuming T1, . . . , Tm−1
are marginally independent Poisson with means λ1, . . . , λm−1, and noting that S1 =

i Ti, (using shorthand notation for brevity)

P(tm, . . . , t1) =

P(tm|tm−1, . . . , t1, n)P(n|t1, . . . , tm−1)

∞(cid:88)

n=0

m−1(cid:89)

i=1

P(ti)

(F.7)

and since we know Ti is marginally Poisson with mean λi for each i = 1, . . . , m− 1,
we just need to show that

P(tm|tm−1, . . . , t1, n)P(n|t1, . . . , tm−1) =

e−λmλtm

m

tm!

.

(F.8)

∞(cid:88)
If we let k =(cid:80)m−1

n=0

above is

i=1 ti, and sample tm from the binomial distribution, the ﬁrst term

(cid:18) λm

(cid:19)tm(cid:18) ¯λm+1

¯λm

¯λm

(cid:19)n−k−tm

δ(tm ≤ n − k)
(F.9)

P(tm|tm−1, . . . , t1, n) =

(n − k)!

(n − k − tm)!tm!

Poisson random variable S1 = k +(cid:80)∞

while the second term is found by noting that N given T1, . . . , Tm−1 is a shifted

i=m Ti, such that

P(n|t1, . . . , tm−1) =

e−¯λm ¯λn−k
(n − k)!

m

δ(n ≥ k)

(F.10)

TRUNCATED RANDOM MEASURES

57

Substituting all of this into the above expression,

∞(cid:88)
∞(cid:88)

n=0

P(tm|tm−1, . . . , t1, n)P(n|t1, . . . , tm−1)

1

(n − k − tm)!tm!

λtm
m

¯λn−k−tm

m+1

1

(n − k − tm)!

¯λn−k−tm

m+1

e−¯λm

∞(cid:88)

n=k+tm

=

n=k+tm

=e−¯λm

=e−λm

λtm
m
tm!

λtm
m
tm!

And thus the inductive step holds, yielding our desired result about the inﬁnite
sequence Ti, i ∈ N. To prove the ﬁnal result, we simply rely on the fact that the
event where I ≤ i is equivalent to the event that Tj = 0 ∀j = i + 1, . . . ,∞.

(F.11)

(F.12)

(F.13)

(F.14)

(F.15)

(F.16)

(F.17)

(F.18)

(F.19)

(F.20)

P(I ≤ i) = P (Tj = 0 ∀j ≥ i + 1)



Tj = 0

 ∞(cid:88)
= e−(cid:80)

= P

j=i+1

j>i λj

which follows from the independent Poisson distributions of the Tj shown earlier.
(cid:3)

Proof of Theorem 6.2. First, note that by Lemma B.2,

K(cid:88)

∞(cid:88)

k=1

x=1

λkx =

(cid:90)
(cid:90)

x=1

∞(cid:88)
∞(cid:88)
(cid:90)

x=1

k=1

K(cid:88)
≤ K(cid:88)
K(cid:88)

k=1

=

h(0|θ)k−1h(x|θ)ν(dθ)

h(x|θ)ν(dθ)

(1 − h(0|θ))ν(dθ) < ∞

k=1

and therefore, {ρkx}x∈N,1≤k≤K in Theorem 3.3 are an inﬁnite collection of inde-
pendent Poisson random variables satisfying the conditions of Lemma 6.1. Further,
since each collection of variables with constant k, x in the inﬁnite sum construction
of Θ are independent of others, we can simply impose a convenient ordering on
{1, . . . , K} × N, and use Lemma 6.1 to sample the ρkx.
(cid:3)

Proof of Theorem 6.3. There are four types of random variable that must be sam-
pled to construct ΘR: S1, which is sampled exactly once; θij, and ψij, which are
each sampled S1 times; and Ci, which is sampled a random number I of times.
Therefore,

E[R] = E [1 + 2S1 + I] = 1 + 2¯ΛK + E[I]

(F.21)

58

T. CAMPBELL, J. H. HUGGINS, J. HOW, AND T. BRODERICK

∞(cid:88)

Now using the CDF of I from Lemma 6.1,

E[I] =

1 − P (I < r) =

r=1

r=1

∞(cid:88)

−(cid:80)

i≥r λkixi =

1 − e

∞(cid:88)

r=1

1 − e− ¯ΛK +(cid:80)r−1

i=1 λkixi (F.22)

This yields the ﬁrst stated result. Next, let

(cid:40)

φ(x) :=

x if x ≤ 1
if x > 1
1

and note that 1 − e−x ≤ φ(x). Thus, we have

∞(cid:88)

r=1

−(cid:80)

i≥r λkixi ≤

1 − e

=

∞(cid:88)
L(cid:88)

r=1

φ

φ

i≥r

(cid:88)
(cid:88)
∞(cid:88)

i≥r

λkixi


 +
(cid:88)

i≥r

λkixi

r=L+1

φ

λkixi

∞(cid:88)
 ,

r=L+1

r=1

= L +

(F.23)

(F.24)

(F.25)

(F.26)

(cid:88)

i≥r

φ



λkixi

so the ﬁrst inequality in Theorem 6.3 follows. The second inequality is now imme-
diate from the fact that φ(x)

e−1 , which follows since

1−e−x ≤ e

arg max
0≤x≤1

x

1 − e−x = arg max

x≥1

1

1 − e−x = 1.

(F.27)

(cid:3)

Proof of Corollary 6.4. Starting from the ﬁrst result of Theorem 6.3, we simply
need to bound the inﬁnite series:

∞(cid:88)

−(cid:80)

1 − e

i≥r λkixi ≤

r=1

=

=

λkixi

r=1

i≥r

(cid:88)

∞(cid:88)
∞(cid:88)
∞(cid:88)
K(cid:88)
∞(cid:88)

x=1

k=1

i=1

iλkixi

K(cid:88)

≤ K

x

λkx.

x=1

k=1

(K(x − 1) + k)λkx

(F.28)

(F.29)

(F.30)

(F.31)

TRUNCATED RANDOM MEASURES

59

Substituting the formula for λkx,

∞(cid:88)

−(cid:80)

1 − e

i≥r λkixi ≤ K

r=1

= K

(cid:33)
(cid:19)

h(0| θ)k−1

(cid:90) (cid:32) K(cid:88)
(cid:90) (cid:18) 1 − h(0| θ)K
(cid:90)

1 − h(0| θ)

h(x| θ)ν(dθ)

k=1

x

x

x

x=1

∞(cid:88)
∞(cid:88)
∞(cid:88)
(cid:90)

x=1

x=1

≤ K 2

= K 2

Eθ[X]ν(dθ).

h(x| θ)ν(dθ)

h(x| θ)ν(dθ)

(F.32)

(F.33)

(F.34)

(F.35)
(cid:3)

Finally, the exact distribution of the number of random variables sampled by

the construction in Theorem 6.2 is provided by Theorem F.1:

Theorem F.1 (Sampling Complexity Distribution). The number of random vari-
ables R that must be sampled to construct ΘK via the generative model in Theo-
rem 6.2 has distribution

1 [r = 1] + 1 [r > 1]

2 (cid:99)(cid:88)

(cid:98) r−2



r−2j−1 − ¯λj
¯λj

r−2j−2

j=0

j!

(F.36)

P(R = r) = e− ¯ΛK

where ¯λm :=(cid:80)m

i=1 λkixi.

Proof. There are four types of random variable that must be sampled to construct
ΘR: S1, which is sampled exactly once; θij, and ψij, which are each sampled S1
times for a total of R1; and Ci, which is sampled a random number R2 = I of times.
Therefore,

P(R1 = n, R2 ≤ r) = P(2S1 = n, I ≤ r)

(F.39)
Now the event that I ≤ r is equivalent to the event that Cr+1, Cr+2, . . . are all equal
to 0. By Lemma 6.1, the Ci are independent Poisson random variables. Therefore

= P(S1 =
= P(S1 =

n
2
n
2

, I ≤ r) n even, r ≥ 1
|I ≤ r)P(I ≤ r)

(cid:16)(cid:80)r

(cid:17) n

2

−((cid:80)r

e

j=1 λkj xj

j=1 λkj xj )

n
2 !

.

(F.40)

(F.37)

(F.38)

(F.41)

P(S1 =

|I ≤ r) =

n
2

Again by Lemma 6.1, we have that

Therefore,

P(R1 = n, R2 ≤ r) =

−(cid:80)

P(I ≤ r) = e

j>r λkj xj .

(cid:16)(cid:80)r

(cid:17) n

2

j=1 λkj xj

n
2 !

e− ¯ΛK n even, r ≥ 0

(F.42)

60

T. CAMPBELL, J. H. HUGGINS, J. HOW, AND T. BRODERICK

j=1 λkj xj

m=0
comes from the sampling of S1 exactly once),

e− ¯ΛK
n
2 !

where 00 := 1. Thus,

P(R1 = n, R2 = r) =

 e− ¯ΛK δ(n = 0)
(cid:18)(cid:16)(cid:80)r
Finally, noting that P(R − 1 = r) = (cid:80)r

 m(cid:88)

P(R = 1) = e− ¯ΛK

P(R = r + 1) =

(cid:88)
Deﬁning ¯λm =(cid:80)m

(cid:0) r−m

e− ¯ΛR

1≤m≤r
r−m even

2

j=1

(cid:1)!

λkj xj

j=1 λkj xj ,

P(R = 1) = e− ¯ΛK

2 −(cid:16)(cid:80)r−1
(cid:17) n

j=1 λkj xj

2(cid:19)
(cid:17) n

r = 0

r > 0

n even

(F.43)
P(R1 = r − m, R2 = m) (the R − 1

 r−m

2

m−1(cid:88)

j=1

−

(F.44)

2  , r ≥ 1
 r−m

λkj xj

(F.45)

(F.46)

(F.47)

(F.48)

(cid:3)

P(R = r) =

1≤m≤r−1
r−m−1 even
and letting 2j = r − m − 1,

P(R = r) = e− ¯ΛK

e− ¯ΛK

(cid:16)¯λ
(cid:88)
(cid:0) r−m−1
(cid:1)!
1 [r = 1] + 1 [r > 1]

2

(cid:17)

, r ≥ 2

− ¯λ

r−m−1
m−1

2

r−m−1
m

2

2 (cid:99)(cid:88)

(cid:98) r−2

j=0

r−2j−1 − ¯λj
¯λj

r−2j−2

j!

 .

Appendix G. Additional simulation results

In this section, we provide additional simulation results referenced in the main
text. The ﬁrst, Fig. 2, shows the “ﬂattening” eﬀect of increasing N on the bounds
for low mean complexity (i.e. low truncation level). This follows intuition; making
a large number of observations from the truncated process is likely to uncover its
ﬁnite, approximate nature. The observed eﬀect is similar for the ΓP, BPP, and
NΓP. Next, Fig. 3 demonstrates that the V- and F-representations of BP(γ, α, d)
(with default values γ = 1, α = 2, and d = 0.5) perform similarly for ﬁxed trunca-
tion level and varying parameters, except when 0 < d (cid:28) 1 — in this setting, the
F-representation bound approaches that of the 2-parameter BP(γ, α, 0), while the
V-representation bound does not exhibit as dramatic a decrease.

TRUNCATED RANDOM MEASURES

61

BP(1, 1, 0)

Figure 2. Truncation error bounds
representations of the BP.

BP(1, 2, 0.5)

for D-, F-,

and V-

101102103104105Mean Computational Complexity10 510 410 310 210 1100101L1 ErrorB DRep (Teh)B FRep (Paisley)SB VRep (Thibaux)101102103104105Mean Computational Complexity10 510 410 310 210 1100101L1 ErrorPL FRep (Broderick)SB VRep (Thibaux)101102103104105Mean Computational Complexity10 510 410 310 210 1100101L1 ErrorB DRep (Teh)B FRep (Paisley)SB VRep (Thibaux)101102103104105Mean Computational Complexity10 510 410 310 210 1100101L1 ErrorPL FRep (Broderick)SB VRep (Thibaux)101102103104105Mean Computational Complexity10 510 410 310 210 1100101L1 ErrorB DRep (Teh)B FRep (Paisley)SB VRep (Thibaux)101102103104105Mean Computational Complexity10 510 410 310 210 1100101L1 ErrorPL FRep (Broderick)SB VRep (Thibaux)62

T. CAMPBELL, J. H. HUGGINS, J. HOW, AND T. BRODERICK

Figure 3. Truncation error bounds for BP(γ, α, d) with varying
parameters.

Acknowledgments

All authors are supported by the Oﬃce of Naval Research under MURI grant
N000141110688. J. Huggins is supported by the U.S. Government under FA9550-11-
C-0028 and awarded by the DoD, Air Force Oﬃce of Scientiﬁc Research, National
Defense Science and Engineering Graduate (NDSEG) Fellowship, 32 CFR 168a.

References

Abramowitz, M. and Stegun, I. (eds.) (1964). Handbook of Mathematical Functions.

Dover Publications.

Airoldi, E. M., Blei, D., Erosheva, E. A., and Fienberg, S. E. (2014). Handbook of

Mixed Membership Models and Their Applications. CRC Press.

Argiento, R., Bianchini, I., and Guglielmi, A. (2015). “A blocked Gibbs sampler for
NGG-mixture models via a priori truncation.” Statistics and Computing, Online
First.

Banjevic, D., Ishwaran, H., and Zarepour, M. (2002). “A recursive method for

functionals of Poisson processes.” Bernoulli .

Blei, D. M. and Jordan, M. I. (2006). “Variational inference for Dirichlet process

mixtures.” Bayesian Analysis, 1(1): 121–144.

Bondesson, L. (1982). “On simulation from inﬁnitely divisible distributions.” Ad-

vances in Applied Probability.

Brix, A. (1999). “Generalized gamma measures and shot-noise Cox processes.”

Advances in Applied Probability, 31: 929–953.

Broderick, T., Jordan, M. I., and Pitman, J. (2012). “Beta processes, stick-breaking

and power laws.” Bayesian Analysis, 7(2): 439–476.

10 310 210 1100101102103Mass, γ10 510 410 310 210 1100101L1 ErrorPL FRep (Broderick)SB VRep (Thibaux)10 310 210 1100101102103Concentration, α10 510 410 310 210 1100101L1 ErrorPL FRep (Broderick)SB VRep (Thibaux)10 310 210 1100Discount, d10 510 410 310 210 1100101L1 ErrorPL FRep (Broderick)SB VRep (Thibaux)TRUNCATED RANDOM MEASURES

63

Broderick, T., Mackey, L., Paisley, J., and Jordan, M. I. (2015). “Combinato-
rial clustering and the beta negative binomial process.” IEEE Transactions on
Pattern Analysis and Machine Intelligence, 37(2): 290–306.

Broderick, T., Wilson, A. C., and Jordan, M. I. (2014). “Posteriors, conjugacy, and

exponential families for completely random measures.” arXiv.org.

Doshi-Velez, F., Miller, K. T., Van Gael, J., and Teh, Y. W. (2009). “Variational
inference for the Indian buﬀet process.” In International Conference on Artiﬁcial
Intelligence and Statistics.

Ferguson, T. S. (1973). “A Bayesian analysis of some nonparametric problems.”

The Annals of Statistics, 209–230.

Ferguson, T. S. and Klass, M. J. (1972). “A representation of independent increment
processes without Gaussian components.” The Annals of Mathematical Statistics,
43(5).

Fox, E. B., Sudderth, E., Jordan, M. I., and Willsky, A. S. (2010). “A sticky HDP-

HMM with application to speaker diarization.” Annals of Applied Statistics.

Gautschi, W. (1959). “Some elementary inequalities relating to the gamma and
incomplete gamma function.” Journal of Mathematics and Physics, 38(1): 77–
81.

Gelfand, A. and Kottas, A. (2002). “A computational approach for nonparametric
Bayesian inference under Dirichlet process mixture models.” Journal of Compu-
tational and Graphical Statistics, 11(2): 289–305.

Gumbel, E. J. (1954). Statistical theory of extreme values and some practical ap-

plications: A series of lectures. U.S. Govt Print Oﬃce, 1st edition.

Hjort, N. L. (1990). “Nonparametric Bayes estimators based on beta processes in

models for life history data.” The Annals of Statistics, 18(3): 1259–1294.

Ishwaran, H. and James, L. F. (2001). “Gibbs sampling methods for stick-breaking

priors.” Journal of the American Statistical Association, 96.

— (2002). “Approximate Dirichlet Process Computing in Finite Normal Mixtures:
Smoothing and Prior Information.” Journal of Computational and Graphical
Statistics, 11(3): 508–532.

Ishwaran, H. and Zarepour, M. (2002). “Exact and approximate sum representa-
tions for the Dirichlet process.” Canadian Journal of Statistics, 30(2): 269–283.
James, L. F. (2014). “Poisson Latent Feature Calculus for Generalized Indian Buﬀet

Processes.” arXiv.org.

James, L. F., Lijoi, A., and Pr¨unster, I. (2009). “Posterior Analysis for Normal-
ized Random Measures with Independent Increments.” Scandinavian Journal of
Statistics, 36(1): 76–97.

Johnson, M. J. and Willsky, A. S. (2013). “Bayesian nonparametric hidden semi-

Markov models.” Journal of Machine Learning Research, 14: 673–701.

Kim, Y. (1999). “Nonparametric Bayesian estimators for counting processes.” The

Annals of Statistics, 27(2): 562–588.

Kingman, J. F. C. (1967). “Completely random measures.” Paciﬁc Journal of

Mathematics, 21(1): 59–78.

— (1975). “Random discrete distributions.” Journal of the Royal Statistical Society

B , 37(1): 1–22.

Kingman, J. F. C. (1993). Poisson Processes. Oxford Studies in Probability. Oxford

University Press.

64

T. CAMPBELL, J. H. HUGGINS, J. HOW, AND T. BRODERICK

Lijoi, A., Mena, R., and Pr¨unster, I. (2005). “Bayesian nonparametric analysis for a
generalized Dirichlet process prior.” Statistical Inference for Stochastic Processes,
8: 283–309.

Lijoi, A. and Pr¨unster, I. (2010). “Models beyond the Dirichlet process.” In Hjort,
N. L., Holmes, C., M¨uller, P., and Walker, S. (eds.), Bayesian Nonparametrics.
Cambridge University Press.

Maddison, C., Tarlow, D., and Minka, T. P. (2014). “A* Sampling.” In Advances

in Neural Information Processing Systems.

Neal, R. M. (2003). “Slice sampling.” The Annals of Statistics, 31(3): 705–767.
Orbanz, P. (2010). “Conjugate projective limits.” arXiv preprint arXiv:1012.0363 .
Paisley, J., Wang, C., and Blei, D. M. (2012a). “The Discrete Inﬁnite Logistic

Normal Distribution.” Bayesian Analysis, 7(2): 235–272.

Paisley, J. W., Blei, D. M., and Jordan, M. I. (2012b). “Stick-breaking beta pro-
cesses and the Poisson process.” International Conference on Artiﬁcial Intelli-
gence and Statistics.

Paisley, J. W., Zaas, A. K., Woods, C. W., Ginsburg, G. S., and Carin, L. (2010).
“A stick-breaking construction of the beta process.” International Conference on
Machine Learning.

Regazzini, E., Lijoi, A., and Pr¨unster, I. (2003). “Distributional results for means
of normalized random measures with independent increments.” The Annals of
Statistics, 31(2): 560–585.

Roychowdhury, A. and Kulis, B. (2015). “Gamma Processes, Stick-Breaking, and
Variational Inference.” In International Conference on Artiﬁcial Intelligence and
Statistics.

Schilling, R. L. (2005). Measures, Integrals and Martingales. Cambridge University

Press.

Sethuraman, J. (1994). “A constructive deﬁnition of Dirichlet priors.” Statistica

Sinica, 4: 639–650.

Teh, Y. W. and G¨or¨ur, D. (2009). “Indian buﬀet processes with power-law behav-

ior.” In Advances in Neural Information Processing Systems.

Teh, Y. W., G¨or¨ur, D., and Ghahramani, Z. (2007). “Stick-breaking construction for
the Indian buﬀet process.” In International Conference on Artiﬁcial Intelligence
and Statistics.

Thibaux, R. and Jordan, M. I. (2007). “Hierarchical beta processes and the In-
dian buﬀet process.” In International Conference on Artiﬁcial Intelligence and
Statistics.

Titsias, M. (2008). “The inﬁnite gamma-Poisson feature model.” In Advances in

Neural Information Processing Systems.

Zhou, M., Hannah, L., Dunson, D., and Carin, L. (2012). “Beta-negative binomial

process and Poisson factor analysis.” In Artiﬁcial Intelligence and Statistics.

TRUNCATED RANDOM MEASURES

65

Laboratory for Information and Decision Systems (LIDS), Massachusetts Institute

of Technology

URL: http://www.trevorcampbell.me/
E-mail address: tdjc@mit.edu

Computer Science and Artificial Intelligence Laboratory (CSAIL), Massachusetts

Institute of Technology

URL: http://www.jhhuggins.org/
E-mail address: jhuggins@mit.edu

Laboratory for Information and Decision Systems (LIDS), Massachusetts Institute

of Technology

URL: http://www.mit.edu/~jhow/
E-mail address: jhow@mit.edu

Computer Science and Artificial Intelligence Laboratory (CSAIL), Massachusetts

Institute of Technology

URL: http://www.tamarabroderick.com
E-mail address: tbroderick@csail.mit.edu

