6
1
0
2

 
r
a

 

M
6
1

 
 
]

V
C
.
s
c
[
 
 

2
v
9
7
7
4
0

.

3
0
6
1
:
v
i
X
r
a

Revisiting Batch Normalization For Practical

Domain Adaptation

Yanghao Li†, Naiyan Wang‡, Jianping Shi(cid:5), Jiaying Liu†, and Xiaodi Hou‡

† Peking University

‡ TuSimple

(cid:5) SenseTime

lyttonhao@pku.edu.cn winsty@gmail.com shijianping5000@gmail.com

liujiaying@pku.edu.cn xiaodi.hou@gmail.com

Abstract. Deep neural networks (DNN) have shown unprecedented suc-
cess in various computer vision applications such as image classiﬁcation
and object detection. However, it is still a common (yet inconvenient)
practice to prepare at least tens of thousands of labeled image to ﬁne-
tune a network on every task before the model is ready to use. Recent
study [1] shows that a DNN has strong dependency towards the train-
ing dataset, and the learned features cannot be easily transferred to a
diﬀerent but relevant task without ﬁne-tuning.
In this paper, we propose a simple yet powerful remedy, called Adaptive
Batch Normalization(AdaBN), to increase the generalization ability of
a DNN. Our approach is based on the well-known Batch Normalization
technique [2] which has become a standard component in modern deep
learning. In contrary to other deep learning domain adaptation methods,
our method does not require additional components, and is parameter-
free. It archives state-of-the-art performance despite its surprising sim-
plicity. Furthermore, we demonstrate that our method is complementary
with other existing methods. Combining AdaBN with existing domain
adaptation treatments may further improve model performance.

Keywords: Domain adaptation; Batch normalization

1

Introduction

Training a DNN for a new image recognition task is expensive. It requires a
large amount of labeled training images that are not easy to obtain. One com-
mon practice is to use a training set from a diﬀerent source. For instance, one can
borrow training data from an existing dataset, or query images from search en-
gines and then label them using Amazon Turk. These approaches usually suﬀer
from inferior performance due to dataset discrepancies, or “dataset bias”, be-
cause 1) the distributions of the source domains (third party datasets or Internet
images) are often diﬀerent from the target domain (testing images); and 2) DNN
is particularly good at capturing dataset bias in its internal representation [3],
which eventually leads to overﬁtting.

Known as domain adaptation, the eﬀort to bridge the gap between training
and testing data distribution has been discussed several times under the context

2

Yanghao Li†, Naiyan Wang‡, Jianping Shi(cid:5), Jiaying Liu†, and Xiaodi Hou‡

Fig. 1. Illustration of our proposed method. In training and testing, we use separate
statistics of the output of each convolution or fully connected layer in source domain
and target domain for batch normalization layer. The domain speciﬁc normalization
mitigates the domain shift issue.

of deep learning [4,5,6,7]. Under common settings, an algorithm is provided with
labeled data from source domain and unlabeled data from target domain. In
order to make the connection between data from two domains, most of these
methods require additional optimization steps and extra parameters. These extra
computational burden could greatly complicate the training of a DNN which is
already intimidating enough for most people.

In this paper, we propose a simple yet eﬀective approach called AdaBN for
batch normalized DNN domain adaptation.We hypothesize that the label re-
lated knowledge is stored in the weight matrix of each layer, whereas domain
related knowledge is represented by the statistics of the Batch Normalization
(BN) layer. Therefore, we can easily transfer the trained model to a new domain
by modulating the statistics in the BN layer. This approach is straightforward to
implement, has zero parameter to tune, and requires minimal computational re-
sources. Moreover, our AdaBN is ready to extend to more sophisticated scenarios
such as multi domain adaptation and semi-supervised settings. Fig. 1 illustrates
the ﬂow-chart of AdaBN. To summarize, our contributions are as follows:

1. We propose a novel domain adaptation technique called Adaptive Batch
Normalization(AdaBN). We show that AdaBN can naturally dissociate bias
and variance terms of a dataset, which is ideal for domain adaptation tasks.

Revisiting Batch Normalization For Practical Domain Adaptation

3

2. We validate the eﬀectiveness of our approaches on standard benchmarks for
both single source and multiple sources domain adaptation. Our method
outperforms the state-of-the-art methods.

2 Related Work

Domain transfer in visual recognition tasks has gained increasing attention in
recent literature [8,9]. Often referred as covariance shift [10] or dataset bias [3],
this problem poses great challenge to the generalization ability of a learned
model. One key component of domain transfer is to model the diﬀerence between
source and target distributions. In [11], the authors assign each dataset with
an explicit bias vector, and train one discriminative model to handle multiple
classiﬁcation problems with diﬀerent bias terms. A more explicit way to compute
dataset diﬀerence is based on Maximum Mean Discrepancy [12]. It uses a non-
linear mapping to project each data sample into a Reproducing Kernel Hilbert
Space, and then compute the diﬀerence of sample means. To reduce dataset
discrepancies, many methods are proposed, including sample selections [13,14],
explicit projection learning [15,16,17] and principal axes alignment [18,19].

All of these methods face the same challenge of devising an eﬀective domain
transfer function in high-dimensional non-linear space. Due to computational
constraints, most of the proposed transfer functions are in the category of simple
linear projections.

In the ﬁeld of deep learning, feature transferability across diﬀerent domains
is a tantalizing yet generally unsolved topic [20,1]. To transfer the learned rep-
resentations to a new dataset, pre-training and ﬁne-tuning [21] have become
de facto procedures. However, adaptation by ﬁne-tuning is far from perfect. It
requires a considerable amount of labeled data from the target domain, and
non-negligible computational resources to re-train the whole network.

A series of progress has been made in DNN to facilitate domain transfer.
Early works of domain adaptation either focus on reordering ﬁne-tuning sam-
ples [22], or regularizing MMD [12] in a shallow network [23]. It is only until
recently that the problem is directly attacked under the assumption of unlabeled
target domain and modern convolutional neural network architecture. Tzeng et
al. [4] used the classical MMD loss to regularize the representation in the last
layer of CNN. Long et al.
[5] further extend the method to multiple kernel
MMD and multiple layer adaptation. Ganin et al. [7] devised a gradient reverse
layer to reverse the gradient that helps to distinguish the domains of each data
sample. This layer eﬃciently anonymizes the domain information hidden in the
CNN features. Recently, Tzeng et al. [6] proposed to simultaneously transfer
task correlations and maximize domain confusion for (semi)-supervised domain
adaptation.

Another related work is CORAL [24]. Diﬀerent from our approach, this model
focuses on the last layer of CNN. CORAL whitens both the data in source domain
and target domain, and then re-correlate the source domain features to target
domain. This operation aligns the second order statistics of source domain and

4

Yanghao Li†, Naiyan Wang‡, Jianping Shi(cid:5), Jiaying Liu†, and Xiaodi Hou‡

target domain distributions. Surprisingly, such simple approach yields state-of-
the-arts results in various text classiﬁcation and visual recognition tasks.

2.1 Batch Normalization Revisited

In this section, we brieﬂy review Batch Normalization(BN) [2] which is closely
related to our AdaBN. The BN layer is originally designed to alleviate the issue
of internal covariance shifting – a common problem while training a very deep
neural network. It ﬁrst standardizes each feature in a mini-batch, and then learn
a common slope and bias for each mini-batch. Formally, given the input to a BN
layer X ∈ Rn×p, where n denotes the batch size, and p is the feature dimension,
BN layer transform a feature j ∈ {1 . . . p} into:
xj − E[X·j]

(cid:112)Var[X·j]

ˆxj =

(1)

yj = γj ˆxj + βj,

where xj and yj are the input/output scalars of one neuron response in one
data sample; X·j denotes the jth column of the input data; and γj and βj
are parameters to be learned. This transformation guarantees that the input
distribution of each layer remains unchanged across diﬀerent mini-batches. For
Stochastic Gradient Descent (SGD) optimization, a stable input distribution
could greatly facilitate model convergency, leading to much faster training speed
for CNN. Moreover, if training data are shuﬄed at each epoch, one training
sample is transformed, or augmented diﬀerently in each epoch. This property
acts as an additional regularization to combat against overﬁtting. In the testing
phase, we use the global statistics instead of the statistics from one mini-batch
to stabilize the testing results.

Since BN layer can both reduce internal covariant shift and overﬁtting, it
signiﬁcantly reduces the number of iteration to converge, and improve the ﬁnal
performance at the same time. BN layer has become a standard component in
recent top-performing CNN architectures, such as deep residual network [25],
and Inception V3 [26].

3 The Model

In this section, we ﬁrst analyze the domain shift in deep neural network, and
reveal two key observations in Sec. 3.1. Then in Sec. 3.2, we introduce our Adap-
tive Batch Normalization(AdaBN) method based on these observations. At last,
we analyze our method in depth in Sec. 3.3.

3.1 A Pilot Experiment

Although the Batch Normalization (BN) technique is originally proposed to help
SGD optimization, its core idea is about aligning training data from diﬀerent

Revisiting Batch Normalization For Practical Domain Adaptation

5

distributions. From this perspective, it is interesting to examine the BN param-
eters (batch-wise mean and variance) over diﬀerent dataset at diﬀerent layers of
the network.

In this pilot experiment, we use MXNet implementation [27] of the Inception-
BN model [2] pre-trained on ImageNet classiﬁcation task [28] as our baseline
DNN model. Our image data are drawn from [29], which contains the same
classes of images from both Caltech-256 dataset [30] and Bing image search re-
sults. For each mini-batch sampled from one dataset, we ﬁrst choose one layer,
and then concatenate the mean and variance of each neuron to form a fea-
ture vector. Using linear SVM, we can perfectly classify whether the mini-batch
feature vector is from Caltech-256 or Bing image search. Fig. 2 visualizes the
distributions of mini-batch feature vectors in 2D, which further corroborate the
observation that the BN statistics from diﬀerent domains are clearly separated
into clusters.

This pilot experiment suggests:

1. Both shallow layers and deep layers exhibit domain shift in DNN, thus only

adapting the last layer is not enough. Deep adaptation is a must.

2. The statistics of BN layer is a good indication of domains. This conﬁrms the
hypothesis that the domain speciﬁc knowledge are stored in the statistics of
BN layer.

(a) Shallow layer distributions

(b) Deep layer distributions

Fig. 2. t-SNE [31] visualization of the mini-batch BN feature vector distributions in
both shallow and deep layers, across diﬀerent datasets. Each point represents the BN
statistics in one mini-batch. Red dots come from Bing domain, while the blue ones are
from Caltech-256 domain. The batch size of each mini-batch is 64.

Both two observations motivate us to adapt the representation across diﬀer-

ent domains by BN layer.

6

Yanghao Li†, Naiyan Wang‡, Jianping Shi(cid:5), Jiaying Liu†, and Xiaodi Hou‡

3.2 Adaptive Batch Normalization

Given the pre-trained DNN model and a target domain, our Adaptive Batch
Normalization (AdaBN) algorithm is as follows1:

Algorithm 1 Adaptive Batch Normalization (AdaBN)

for neuron j in DNN, image m in target domain do

(cid:113)

Concatenate neuron responses on all images of target domain: xj =
[xj(m), . . .]
Compute the mean and variance of the target domain: µt
σt

j =
end for
for neuron j in DNN, testing image m in target domain do

j = E(xt
j),

Var(xt

j).

(cid:0)

(cid:1)

xj (m)−µt

j

σt
j

+ βj

Compute BN output yj(m) := γj

end for

The intuition behind our method is very straightforward: The standardiza-
tion of each layer by domain ensures that each layer received data from a similar
distribution no matter in source domain or target domain, thus the domain
information is anonymized.

For multi-source domain adaptation, the only change is we must standardize
each sample by the statistics in its own domain. Since in training the statistics
are calculated in one mini-batch, we only need to make sure that the samples in
one mini-batch are from the same domain. While for (semi-)supervised domain
adaptation, we may use the labeled data to ﬁne-tune the weights as well. As a
result, our method could ﬁt in all diﬀerent settings of domain adaptation with
minor eﬀort.

3.3 Further thoughts about AdaBN

One natural question to ask is whether such simple translation and scaling of
features could deal with complicated domain shift? The answer is yes. As we
all know, neural networks is a highly non-linear composite functions. Consider
a simple neural network with input input x ∈ Rp1×1. It has one BN layer with
mean and variance of each feature being µi and σ2
i , one fully connected layer with
weight matrix W ∈ Rp1×p2 and bias b ∈ Rp2×1, and a non-linear transformation
layer f (·). The output of this network is f (WT
a x + b(cid:48)

Wa = WT Σ−1,
Σ = diag(σ2

1, ..., σ2
p1

a), where
ba = −WT Σ−1µ + b,

), µ = (µ1, ..., µp1).

(2)

1 In practice we adopt an online algorithm [32] to eﬃciently estimate the mean and

variance.

Revisiting Batch Normalization For Practical Domain Adaptation

7

The output without BN is simply f (WT x + b). We can see that the transforma-
tion is highly non-linear even for a simple network with one computation layer.
As CNN architecture goes deeper, it will gain increasing power to represent more
complicated transformations.

Another question is why we transform the neuron responses independently,
not decorrelate and then re-correlate the responses as suggested in [24]. We admit
that decorrelation may improve the performance, however in the optimization
of CNN the mini-batch size is usually smaller than the feature dimension. As
a result, the covariance matrix is always singular. In addition, decorrelation re-
quires to compute the inverse of the covariance matrix which is computationally
intensive, especially if we plan to apply AdaBN for every layer of the network.

4 Experiments

In this section, we demonstrate the eﬀectiveness of AdaBN on standard domain
adaptation datasets, and empirically analyze the adapted features.

4.1 Experimental Settings

We ﬁrst introduce our experimental settings on two standard datasets: Oﬃce [33]
and Caltech-Bing [29], some baselines and the conﬁgurations of our experiments.
Oﬃce [33] is a standard benchmark for domain adaptation, which is a col-
lection of 4652 images in 31 classes from three diﬀerent domains: Amazon(A),
DSRL(D) and Webcam(W). We evaluate all the six adaptation tasks in our
experiments, which are commonly adopted by other domain adaptation meth-
ods [4,24,5]. For multi-source domain adaptation, we evaluate our method on
the three transfer tasks (A, W) → D, (A, D) → W, (D, W) → W.

Caltech-Bing [29] is a much larger domain adaptation dataset, which con-
tains 30,607 and 121,730 images in 256 categories from two domains Caltech-
256(C) and Bing(B), respectively. The images in the Bing set are collected from
Internet by Bing image search with the corresponding keywords, which has sig-
niﬁcantly diﬀerent data distribution from that from Caltech-256.

We compare our method with a variety of methods, including three shallow
methods: SA [18], GFK [19], CORAL [24], and three deep methods: DDC [4],
DAN [5], RevGrad [7]. Speciﬁcally, GFK models domain shift by integrating an
inﬁnite number of subspaces that characterize changes in statistical properties
from the source to the target domain. SA and CORAL align the source and
target subspaces by explicit feature space transformations that would map source
distribution into the target ones. DDC and DAN are the deep learning based
methods which maximizes domain invariance by adding to AlexNet one or several
adaptation layers using MMD. RevGrad incorporates a gradient reversal layer
in the deep model to encourage learning domain-invariant features. It should be
noted that these deep learning methods only add adaptation layers on several
top layers of the DNN, in contrast that our method delves into early convolution
layers as well with the help of the BN layers.

8

Yanghao Li†, Naiyan Wang‡, Jianping Shi(cid:5), Jiaying Liu†, and Xiaodi Hou‡

We follow the full protocol [21] for the single source setting; while for multiple
sources setting, we use all the samples in the source domains as training data,
and use all the samples in the target domain as testing data. We ﬁne-tune the
Inception-BN [2] model on source domain in each task for 100 epochs. The
learning rate starts from 0.01, and then decreases by factor 0.1 every 40 epoch.
Since the oﬃce dataset is quite small, following the best practice in [5], we freeze
the ﬁrst three groups of Inception modules, and set the learning rate of fourth
and ﬁfth group one tenth of the base learning rate to avoid overﬁtting. For
Caltech-Bing dataset, we ﬁne-tune the whole model with the same base learning
rate.

4.2 Results

Oﬃce Dataset We report our results on Oﬃce in the single and multiple
sources settings in Table 1 and Table 2, respectively. Note that the models in
the ﬁrst part of the Table 1 are pre-trained on AlexNet [34] instead of the
Inception-BN [2] model, because there is no public pre-trained Inception BN
model in Caﬀe [35]. Thus, the absolute number cannot be directly compared,
the relative improvements over baseline is a more reasonable metric.

From Table 1, we ﬁrst notice that the Inception BN [2] model indeed im-
proves over the AlexNet [34] model on average, which means that the CNN
pre-trained on ImageNet indeed learns general features, the improvements on
ImageNet can be transferred on new tasks. Among the methods based on In-
ception BN features, our method improves the most over the baseline, which
validates the need of deep adaptation. Moreover, since our method is comple-
mentary to other ones, we simply apply CORAL on the adapted features by our
method. Not surprisingly, this simple combination indeed exhibits 0.5% increase
in performance. This preliminary test reveals the further potential of our method
if combining with other advanced domain adaptation methods. Finally, we could
improve 1.7% over the baseline, and advance the state-of-the-art results for this
dataset. Compared to other methods based on AlexNet, our method is better
than DDC and RevGrad, and worse than DAN in terms of relative improvements
over corresponding baselines.

Since all the compared methods do not report the performance on multi-
source domain adaptation setting, here we only compare AdaBN with the best
algorithm CORAL in the single source setting. Analyzing the results of the
baseline in Table 2, we ﬁnd that simply combining two domains will not lead to
better performance. The result is generally worse than the higher one of using
each source domain separately. This phenomenon suggests that if we cannot
properly cope with domain bias, the increase of training samples may be reversely
harmful to the testing performance. This result conﬁrms the necessity of domain
adaptation. In this more challenging setting, our method still outperforms the
baseline and CORAL on average. Again, when combining with CORAL, our
method demonstrates further improvements. At last, our method archives 2.3%
gain over the baseline.

Revisiting Batch Normalization For Practical Domain Adaptation

9

Method
AlexNet [34]
DDC [4]
DAN [5]
RevGrad [7]

A → W D → W W → D A → D D → A W → A Avg
70.1
61.6
70.6
61.8
68.5
72.9
67.3

99.0
98.5
99.0
93.7

95.4
95.0
96.0
94.0

63.8
64.4
67.0

49.8
52.2
53.1

-

51.1
52.1
54.0

-

-

-

70.3
Inception BN [2]
69.8
SA [18]
66.7
GFK [19]
70.9
CORAL [24]
AdaBN
74.2
AdaBN + CORAL 75.4

94.3
95.5
97.0
95.7
95.7
96.2

100
99.0
99.4
99.8
99.8
99.6

70.5
71.3
70.1
71.9
73.1
72.7

60.1
59.4
58.0
59.0
59.8
59.0

75.5
57.9
75.3
56.9
74.7
56.9
76.3
60.2
57.4
76.7
60.5 77.2

Table 1. Single source domain adaptation results on Oﬃce-31 [33] dataset with stan-
dard unsupervised adaptation protocol.

Method
Inception BN [2]
CORAL [24]
AdaBN
AdaBN + CORAL

A, D → W A, W → D D, W → A Avg
82.1
83.3
83.6
84.4

95.4
96.4
97.2
97.8

90.8
92.1
94.2
95.0

60.2
61.4
59.3
60.5

Table 2. Multi-source domain adaptation results on Oﬃce-31 [33] dataset with stan-
dard unsupervised adaptation protocol.

Caltech-Bing Dataset To further evaluate our method on the large-scale
dataset, we show our results on Caltech-Bing Dataset in Table 3. Compared
with CORAL, AdaBN achieves better performance, which improves 1.8% over
the baseline. Note that all the domain adaptation methods show minor improve-
ments over the baseline in the task C → B. We hypothesize the reason is that
the images in Bing dataset are collected from Internet, which are more diverse
and noisy. Thus, it is not easy to adapt on the Bing dataset from the relatively
clean dataset Caltech-256.

Method
Inception BN [2]
CORAL [24]
AdaBN
AdaBN + CORAL

C → B
35.1
35.3
35.2
35.0

B → C
64.6
67.2
68.1
67.5

Avg
49.9
51.3
51.7
51.2

Table 3. Single source domain adaptation results on Caltech-Bing [29] dataset.

10

Yanghao Li†, Naiyan Wang‡, Jianping Shi(cid:5), Jiaying Liu†, and Xiaodi Hou‡

4.3 Empirical Analysis

In this section, we conduct two experiments to empirically analyze the features
adapted by our method and another experiment to illustrate how the number of
samples in target domain aﬀects the performance.

Visualization of Features. We ﬁrst visualize the features of the last layer
before and after adaptation using t-SNE [31] in Fig. 3. We choose two adaption
settings for illustration: Amazon to webcam and Amazon to DSLR. Each red
circle represents one training sample, while each blue circle represents one testing
sample. We can see that the features of testing data after adaption are blended
more evenly with the training data compared to those without adaption. In other
words, the distribution of testing samples is more consistent with the training
one. This intuitive illustration again conﬁrms that our method is eﬀective against
domain shift.

(a) A → W, with adaptation

(b) A → W, w/o adaptation

(c) A → D, with adaptation

(d) A → D, w/o adaptation

Fig. 3. Visualization of last layer features. Red circles are training samples, while blues
ones are testing samples. Best viewed in color.

Revisiting Batch Normalization For Practical Domain Adaptation

11

Analysis of Feature Divergence. In the second experiment, we analyze the
statistics of the output of one shallow layer (the output of second convolution
layer) and one deep layer (the output of last Inception module before ReLU) in
the network. In particular, we compute the distance of source domain distribu-
tion and target domain distribution before and after adaptation. We denote each
feature i as Fi, and assume that the output of each feature generally follows a
Gaussian distribution with mean µi and variance σ2
i . Then we use the symmetric
KL divergence as our metric:

D(Fi || Fj) = KL(Fi || Fj) + KL(Fj || Fi),
− 1
KL(Fi || Fj) = log
2

i + (µi − µj)2
σ2

2σ2
j

σj
σi

+

.

(3)

We plot the distribution of the distances in Fig. 4. Obviously, our method both
reduce the domain discrepancy in shallow layer and deep layer. We also report
the quantitative results in Table. 4. This experiment once more veriﬁes the ef-
fectiveness of our proposed method.

(a) A → W, shallow layer

(b) A → W, deep layer

(c) A → D, shallow layer

(d) A → D, deep layer

Fig. 4. Distribution of the symmetric KL divergence of the outputs in shallow layer
and deep layer, respectively. Best viewed in color.

12

Yanghao Li†, Naiyan Wang‡, Jianping Shi(cid:5), Jiaying Liu†, and Xiaodi Hou‡

A → W, shallow A → W, deep A → D, shallow A → D, deep

Before Adapt
After Adapt

0.0716
0.0227

0.0614
0.0134

0.2307
0.0266

0.0502
0.0140

Table 4. The average symmetric KL divergence of the outputs in shallow layer and
deep layer, respectively.

Sensitivity to Target Domain Size. Since the key of our method is to cal-
culate the mean and variance of the target domain on diﬀerent BN layers, it is
very natural to ask how many target images is necessary to obtain the stable
statistics. Therefore, we randomly select a part of images in target domain to
calculate the statistics and then evaluate the performance on the whole target
images set in the task B → C and A → W, respectively. Fig. 5 illustrates the
eﬀect of using diﬀerent number of batches. The results demonstrate that our
method can obtain good results when using only a small part of the target ex-
amples. It should also be noted that even using one batch of target examples,
our method still achieves better results than the baseline method. This is very
valuable in practice use since a large number of targets are often not available.

(a) A → W

(b) B → C

Fig. 5. Accuracy when varying the number of mini-batches used for calculating the
statistics of BN layers in A → W and B → C, respectively. For B → C, we only show
the results of using less than 100 batches, since the results are very stable when adding
more examples. The batch size is 64 in this experiment.

5 Conclusion and Future Works

In this paper, we have described an amazingly simple yet eﬀective approach for
domain adaptation for batch normalized neural networks. We have exploited
another functionality of BN layer besides its original uses: domain adaptation.
The main idea is to replace the statistics of each BN layer in source domain

Revisiting Batch Normalization For Practical Domain Adaptation

13

with those on target domain. The proposed method is easy to implement and
parameter-free, and it takes almost no eﬀort to extend to multiple source domains
and semi-supervised settings. Moreover, our method is not sensitive to the target
domain size. Thus it is more favorable for practitioners compared with other deep
learning based domain adaptation methods. At last, we have tested our method
on standard benchmarks. Our method established new state-of-the-art results on
both single source and multiple source domain adaptation settings. We believe
our method opens up a new direction for domain adaptation.

In contrary to other methods that use MMD or domain confusion loss to
update the weights in CNN for domain adaptation, our method only modiﬁes
the statistics of BN layer. Therefore, our method is fully complementary to other
existing deep learning based methods. It is interesting to see how they collaborate
in one uniﬁed model.

References

1. Tommasi, T., Patricia, N., Caputo, B., Tuytelaars, T.: A deeper look at dataset

bias. arXiv preprint arXiv:1505.01257 (2015) 1, 3

2. Ioﬀe, S., Szegedy, C.: Batch normalization: Accelerating deep network training by

reducing internal covariate shift. In: ICML. (2015) 448–456 1, 4, 5, 8, 9

3. Torralba, A., Efros, A.A.: Unbiased look at dataset bias. In: CVPR. (2011) 1521–

1528 1, 3

4. Tzeng, E., Hoﬀman, J., Zhang, N., Saenko, K., Darrell, T.: Deep domain confusion:
Maximizing for domain invariance. arXiv preprint arXiv:1412.3474 (2014) 2, 3, 7,
9

5. Long, M., Cao, Y., Wang, J., Jordan, M.: Learning transferable features with deep

adaptation networks. In: ICML. (2015) 97–105 2, 3, 7, 8, 9

6. Tzeng, E., Hoﬀman, J., Darrell, T., Saenko, K.: Simultaneous deep transfer across

domains and tasks. In: ICCV. (2015) 4068–4076 2, 3

7. Ganin, Y., Lempitsky, V.: Unsupervised domain adaptation by backpropagation.

In: ICML. (2015) 1180–1189 2, 3, 7, 9

8. Beijbom, O.: Domain adaptations for computer vision applications. arXiv preprint

arXiv:1211.4860 (2012) 3

9. Patel, V.M., Gopalan, R., Li, R., Chellappa, R.: Visual domain adaptation: A
survey of recent advances. IEEE Signal Processing Magazine 32(3) (2015) 53–69
3

10. Shimodaira, H.: Improving predictive inference under covariate shift by weighting
the log-likelihood function. Journal of statistical planning and inference 90(2)
(2000) 227–244 3

11. Khosla, A., Zhou, T., Malisiewicz, T., Efros, A.A., Torralba, A.: Undoing the

damage of dataset bias. In: ECCV. Springer (2012) 158–171 3

12. Gretton, A., Borgwardt, K.M., Rasch, M.J., Sch¨olkopf, B., Smola, A.: A kernel
two-sample test. The Journal of Machine Learning Research 13(1) (2012) 723–773
3

13. Huang, J., Gretton, A., Borgwardt, K.M., Sch¨olkopf, B., Smola, A.J.: Correcting

sample selection bias by unlabeled data. In: NIPS. (2006) 601–608 3

14. Gong, B., Grauman, K., Sha, F.: Connecting the dots with landmarks: Discrim-
inatively learning domain-invariant features for unsupervised domain adaptation.
In: ICML. (2013) 222–230 3

14

Yanghao Li†, Naiyan Wang‡, Jianping Shi(cid:5), Jiaying Liu†, and Xiaodi Hou‡

15. Pan, S.J., Tsang, I.W., Kwok, J.T., Yang, Q.: Domain adaptation via transfer
component analysis. IEEE Transactions on Neural Networks 22(2) (2011) 199–
210 3

16. Gopalan, R., Li, R., Chellappa, R.: Domain adaptation for object recognition: An

unsupervised approach. In: ICCV. (2011) 999–1006 3

17. Baktashmotlagh, M., Harandi, M., Lovell, B., Salzmann, M.: Unsupervised domain

adaptation by domain invariant projection. In: ICCV. (2013) 769–776 3

18. Fernando, B., Habrard, A., Sebban, M., Tuytelaars, T.: Unsupervised visual do-
In: ICCV. (2013) 2960–2967 3, 7,

main adaptation using subspace alignment.
9

19. Gong, B., Shi, Y., Sha, F., Grauman, K.: Geodesic ﬂow kernel for unsupervised

domain adaptation. In: CVPR. (2012) 2066–2073 3, 7, 9

20. Yosinski, J., Clune, J., Bengio, Y., Lipson, H.: How transferable are features in

deep neural networks? In: NIPS. (2014) 3320–3328 3

21. Donahue, J., Jia, Y., Vinyals, O., Hoﬀman, J., Zhang, N., Tzeng, E., Darrell, T.:
DeCAF: A deep convolutional activation feature for generic visual recognition. In:
ICML. (2014) 647–655 3, 8

22. Chopra, S., Balakrishnan, S., Gopalan, R.: DLID: Deep learning for domain adap-
tation by interpolating between domains. In: ICML Workshop on Challenges in
Representation Learning. Volume 2. (2013) 3

23. Ghifary, M., Kleijn, W.B., Zhang, M.: Domain adaptive neural networks for object
recognition. In: PRICAI 2014: Trends in Artiﬁcial Intelligence. (2014) 898–904 3
24. Sun, B., Feng, J., Saenko, K.: Return of frustratingly easy domain adaptation.

arXiv preprint arXiv:1511.05547 (2015) 3, 7, 9

25. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.

arXiv preprint arXiv:1512.03385 (2015) 4

26. Szegedy, C., Vanhoucke, V., Ioﬀe, S., Shlens, J., Wojna, Z.: Rethinking the in-
ception architecture for computer vision. arXiv preprint arXiv:1512.00567 (2015)
4

27. Chen, T., Li, M., Li, Y., Lin, M., Wang, N., Wang, M., Xiao, T., Xu, B., Zhang,
C., Zhang, Z.: MXNet: A ﬂexible and eﬃcient machine learning library for hetero-
geneous distributed systems. arXiv preprint arXiv:1512.01274 (2015) 5

28. Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z.,
Karpathy, A., Khosla, A., Bernstein, M., et al.: ImageNet large scale visual recog-
nition challenge. International Journal of Computer Vision 115(3) (2015) 211–252
5

29. Bergamo, A., Torresani, L.: Exploiting weakly-labeled web images to improve
object classiﬁcation: a domain adaptation approach. In: NIPS. (2010) 181–189 5,
7, 9

30. Griﬃn, G., Holub, A., Perona, P.: Caltech-256 object category dataset. (2007) 5
31. Van der Maaten, L., Hinton, G.: Visualizing data using t-sne. Journal of Machine

Learning Research 9(2579-2605) (2008) 85 5, 10

32. Donald, E.K.: The art of computer programming. Sorting and searching 3 (1999)

426–458 6

33. Saenko, K., Kulis, B., Fritz, M., Darrell, T.: Adapting visual category models to

new domains. In: ECCV. (2010) 213–226 7, 9
34. Krizhevsky, A., Sutskever, I., Hinton, G.E.:

Imagenet classiﬁcation with deep

convolutional neural networks. In: NIPS. (2012) 1097–1105 8, 9

35. Jia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J., Girshick, R., Guadar-
rama, S., Darrell, T.: Caﬀe: Convolutional architecture for fast feature embedding.
In: ACM MM. (2014) 675–678 8

