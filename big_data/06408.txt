Sharp sup-norm Bayesian curve estimation

Department of Economics, University of Verona, Via Cantarane 24, 37129 Verona, Italy

Catia Scricciolo∗

6
1
0
2

 
r
a

 

M
1
2

 
 
]
E
M

.
t
a
t
s
[
 
 

1
v
8
0
4
6
0

.

3
0
6
1
:
v
i
X
r
a

Abstract

Sup-norm curve estimation is a fundamental statistical problem and, in principle, a premise for the construction of
conﬁdence bands for inﬁnite-dimensional parameters. In a Bayesian framework, the issue of whether the sup-norm-
concentration-of-posterior-measure approach proposed by Gin´e and Nickl (2011), which involves solving a testing
problem exploiting concentration properties of kernel and projection-type density estimators around their expecta-
tions, can yield minimax-optimal rates is herein settled in the aﬃrmative beyond conjugate-prior settings obtaining
sharp rates for common prior-model pairs like random histograms, Dirichlet Gaussian or Laplace mixtures, which can
be employed for density, regression or quantile estimation.

Keywords: McDiarmind’s inequality, Nonparametric hypothesis testing, Posterior distributions, Sup-norm rates

1. Introduction

The study of the frequentist asymptotic behaviour of Bayesian nonparametric (BNP) procedures has initially fo-
cused on the Hellinger or L1-distance loss, see Shen and Wasserman (2001) and Ghosal et al. (2000), but an extension
and generalization of the results to Lr-distance losses, 1 ≤ r ≤ ∞, has been the object of two recent contributions by
Gin´e and Nickl (2011) and Castillo (2014). Sup-norm estimation has particularly attracted attention as it may con-
stitute the premise for the construction of conﬁdence bands whose geometric structure can be easily visualized and
interpreted. Furthermore, as shown in the example of Section 3.2, the study of sup-norm posterior contraction rates
for density estimation can be motivated as being an intermediate step for the ﬁnal assessment of convergence rates for
quantile estimation.

While the contribution of Castillo (2014) has a more prior-model speciﬁc ﬂavour, the article by Gin´e and Nickl
(2011) aims at a uniﬁed understanding of the drivers of the asymptotic behaviour of BNP procedures by developing
a new approach to the involved testing problem constructing nonparametric tests that have good exponential bounds
on the type-one and type-two error probabilities that rely on concentration properties of kernel and projection-type
density estimators around their expectations.

Even if Gin´e and Nickl (2011)’s approach can only be useful if a ﬁne control of the approximation properties of
the prior support is possible, it has the merit of replacing the entropy condition for sieve sets with approximating
conditions. However, the result, as presented in their Theorem 2 (Theorem 3), can only retrieve minimax-optimal
rates for Lr-losses when 1 ≤ r ≤ 2, while rates deteriorate by a genuine power of n, in fact n1/2, for r > 2. Thus, the
open question remains whether their approach can give the right rates for 2 < r ≤ ∞ for non-conjugate priors and
sub-optimal rates are possibly only an artifact of the proof. We herein settle this issue in the aﬃrmative by reﬁning
their result and proof and showing in concrete examples that this approach retrieves the right rates.

The paper is organized as follows. In Section 2, we state the main result whose proof is postponed to Appendix A.

Examples concerning diﬀerent statistical settings like density and quantile estimation are presented in Section 3.

∗Corresponding author.
Email address: catia.scricciolo@univr.it (Catia Scricciolo)

2. Main result

In this section, we describe the set-up and present the main contribution of this note. Let (cid:0)(X, A, P), P ∈ P(cid:1)
be a collection of probability measures on a measurable space (X, A) that possess densities with respect to some
σ-ﬁnite dominating measure µ. Let Πn be a sequence of priors on (P, B), where B is a σ-ﬁeld on P for which
the maps x 7→ p(x) are jointly measurable relative to A ⊗ B. Let X1, . . . , Xn be i.i.d. (independent, identically
distributed) observations from a common law P0 ∈ P with density p0 on X with respect to µ, p0 = dP0/dµ. For a
probability measure P on (X, A) and an A-measurable function f : X → Rk, k ≥ 1, let P f denote the integralR f dP,
where, unless otherwise speciﬁed, the set of integration is understood to be the whole domain. When this notation
is applied to the empirical measure Pn associated with a sample X(n) := (X1, . . . , Xn), namely the discrete uniform
measure on the sample values, this yields Pn f = n−1Pn
i=1 K j(·, Xi)
be a kernel or projection-type density estimator based on X1, . . . , Xn at resolution level j, with K j as in Deﬁnition
(1) below. Its expectation is then equal to Pn
0 ˆpn( j)(·) = P0K j(·, X1) = K j(p0)(·), where we have used the notation
K j(p0)(·) = R K j(·, y)p0(y) dy. In order to reﬁne Gin´e and Nickl (2011)’s result, we use concentration properties of
k ˆpn( j) − K j(p0)k1 around its expectation by applying McDiarmind’s inequality for bounded diﬀerences functions.
result.
Deﬁnition 1. Let X = R, X = [0, 1] or X = (0, 1]. The sequence of operators

i=1 f (Xi). For each n ∈ N, let ˆpn( j)(·) = n−1Pn

The following deﬁnition, which corresponds to Condition 5.1.1 in Gin´e and Nickl (2015), is essential for the main

is called an admissible approximating sequence if it satisﬁes one of the following conditions:

K j(x, y) := 2 jK(2 jx, 2 jy),

x, y ∈ X,

j ≥ 0,

a) convolution kernel case, X = R: K(x, y) = K(x − y), where K ∈ L1(R) ∩ L∞(R), integrates to 1 and is of
bounded p-variation for some ﬁnite p ≥ 1 and right (left)-continuous;
b) multi-resolution projection case, X = R: K(x, y) = Pk∈Z φ(x − k)φ(y − k), with K j as above or K j(x, y) =
K(x, y) + P j−1
ℓ=0Pk ψlk(x)ψlk(y), where φ, ψ ∈ L1(R) ∩ L∞(R) deﬁne an S -regular wavelet basis, have bounded
p-variation for some p ≥ 1 and are uniformly continuous, or deﬁne the Haar basis, see Chapter 4, ibidem;
c) multi-resolution case, X = [0, 1]: K j,bc(x, y) is the projection kernel at resolution j of a Cohen-Daubechies-Vial
(CDV) wavelet basis, see Chapter 4, ibidem;
d) multi-resolution case, X = (0, 1]: K j,per(x, y) is the projection kernel at resolution j of the periodization of a

scaling function satisfying b), see (4.126) and (4.127), ibidem.

Remark 1. A useful property of S -regular wavelet bases is the following: there exists a non-negative measurable
function Φ ∈ L1(R) ∩ L∞(R) such that |K(x, y)| ≤ Φ(|x − y|) for all x, y ∈ R, that is, K is dominated by a bounded and
integrable convolution kernel Φ.

In order to state the main result, we recall that a sequence of positive real numbers Ln is slowly varying at ∞ if,
(L[λn]/Ln) = 1. Also, for s ≥ 0, let L1(µs) be the space of µs-integrable functions,

for each λ > 0, it holds that limn→∞
dµs(x) := (1 + |x|)sdx, equipped with the norm k fkL1(µs) := R | f (x)|(1 + |x|)s dx.
Theorem 1. Let ǫn and Jn be sequences of positive real numbers such that ǫn → 0, nǫ2
n). For
each r ∈ {1, ∞} and a slowly varying sequence Ln,r → ∞, let ǫn,r := Ln,rǫn. Suppose that, for K as in Deﬁnition (1),
with K2, Φ2 and p0 that integrate (1 + |x|)s for some s > 1 in cases a) and b),

n → ∞ and 2Jn = O(nǫ2

kKJn(p0) − p0kr = O(ǫn,r)

(1)

and, for a constant C > 0, sets Pn ⊆ {P ∈ P : kKJn(p) − pkr ≤ CK ǫn,r}, where CK > 0 only depends on K, we have
(i) Πn(P \ Pn) ≤ exp(cid:0)−(C + 4)nǫ2
n(cid:1),
n , P0 log2(p/p0) ≤ ǫ2
(ii) Πn(cid:0)P ∈ P : −P0 log(p/p0) ≤ ǫ2

n(cid:1) ≥ exp (−Cnǫ2

n).

2

Then, for suﬃciently large Mr > 0,

Pn
0

Πn(cid:0)P ∈ P : kp − p0kr ≥ Mrǫn,r | X(n)(cid:1) → 0.

(2)

If the convergence in (2) holds for r ∈ {1, ∞}, then, for each 1 < s < ∞. Pn
where ¯ǫn := (Ln,1 ∨ Ln,∞

)ǫn.

0

Πn(cid:0)P ∈ P : kp − p0ks ≥ Ms ¯ǫn | X(n)(cid:1) → 0,

The assertion, whose proof is reported in Appendix A, is an in-probability statement that the posterior mass
outside a sup-norm ball of radius a large multiple M of ǫn is negligible. The theorem provides the same suﬃcient
conditions for deriving sup-norm posterior contraction rates that are minimax-optimal, up to logarithmic factors, as
in Gin´e and Nickl (2011). Condition (ii), which is mutuated from Ghosal et al. (2000), is the essential one:
the
prior concentration rate is the only determinant of the posterior contraction rate at densities p0 having sup-norm
approximation error of the same order against a kernel-type approximant, provided the prior support is almost the set
of densities with the same approximation property.

3. Examples

In this section, we apply Theorem 1 to some prior-model pairs used for (conditional) density or regression esti-
mation, including random histograms, Dirichlet Gaussian or Laplace mixtures, that have been selected in an attempt
to reﬂect cases for which the issue of obtaining sup-norm posterior rates was still open. We do not consider Gaus-
sian priors or wavelets series because these examples have been successfully worked out in Castillo (2014) taking a
diﬀerent approach. We furthermore exhibit an example with the aim of illustrating that obtaining sup-norm posterior
contraction rates for density estimation can be motivated as being an intermediate step for the ﬁnal assessment of
convergence rates for estimating single quantiles.

3.1. Density estimation
Example 1 (Random dyadic histograms). For Jn ∈ N, consider a partition of [0, 1] into 2Jn intervals (bins) of equal
length A1,2Jn = [0, 2−Jn] and A j,2Jn = (( j − 1)2−Jn, j2−Jn], j = 2, . . . , 2Jn. Let Dir2Jn denote the Dirichlet distribution
on the (2Jn − 1)-dimensional unit simplex with all parameters equal to 1. Consider the random histogram

2Jn

Xj=1

w j,2Jn 2Jn1A j,2Jn (·),

(w1,2Jn , . . . , w2Jn ,2Jn ) ∼ Dir2Jn .

Denote by Π2Jn the induced law on the space of probability measures with Lebesgue density on [0, 1]. Let X1, . . . , Xn
be i.i.d. observations from a density p0 on [0, 1]. Then, the Bayes’ density estimator, that is the posterior expected
histogram, has expression

ˆpn(x) =

2Jn

Xj=1

1 + Nl(x)
2Jn + n

2Jn1A j,2Jn (x),

x ∈ [0, 1],

where l(x) identiﬁes the bin containing x, i.e., Al(x),2Jn ∋ x, and Nl(x) stands for the number of observations falling
into Al(x),2Jn . Let Cα([0, 1]) denote the class of H¨older continuous functions on [0, 1] with exponent α > 0. Let
ǫn,α := (cid:0)n/ log n(cid:1)−α/(2α+1) be the minimax rate of convergence over (Cα([0, 1]), k · k∞
Proposition 1. Let X1, . . . , Xn be i.i.d. observations from a density p0 ∈ Cα([0, 1]), with α ∈ (0, 1], satisfying p0 > 0
Π2Jn (P : kp− p0k∞ ≥ Mǫn,α | X(n)) → 0.
on [0, 1]. Let Jn be such that 2Jn ∼ ǫ
Consequently, Pn
0k ˆpn − p0k∞ ≍ ǫn,α.

1/α
n,α . Then, for suﬃciently large M > 0, Pn
0

The ﬁrst part of the assertion, which concerns posterior contraction rates, immediately follows from Theorem
(1) combined with the proof of Proposition 3 of Gin´e and Nickl (2011), whose result, together with that of Theorem

).

3 in Castillo (2014), is herein improved to the minimax-optimal rate (cid:0)n/ log n(cid:1)−α/(2α+1) for every 0 < α ≤ 1. The
second part of the assertion, which concerns convergence rates for the histogram density estimator, is a consequence of
Jensen’s inequality and convexity of p 7→ kp−p0k∞
, combined with the fact that the prior Π2Jn is supported on densities
uniformly bounded above by 2Jn and that the proof of Theorem 1 yields the exponential order exp (−Bnǫ2
n,α) for the
convergence of the posterior probability of the complement of an (Mǫn,α)-ball around p0, in symbols, Pn
0k ˆpn − p0k∞ <
Mǫn,α + 2Jn Pn
= O(ǫn,α).
0

Π2Jn (P : kp − p0k∞ ≥ Mǫn,α | X(n)) ≤ Mǫn,α + 2Jn exp (−Bnǫ2

0k ˆpn − p0k∞

n,α), whence Pn

3

0

Example 2 (Dirichlet-Laplace mixtures). Consider, as in Scricciolo (2011), Gao and van der Vaart (2015), a Laplace
mixture prior Π thus deﬁned. For ϕ(x) := 1

2 exp (−|x|), x ∈ R, the density of a Laplace (0, 1) distribution, let

0k ˆpn − p0k∞ ≍ (n/ log n)−3/8.

• pG(·) := R ϕ(· − θ) dG(θ) denote a mixture of Laplace densities with mixing distribution G,
• G ∼ Dα, the Dirichlet process with base measure α := αR ¯α, for 0 < αR < ∞ and ¯α a probability measure on R.
Proposition 2. Let X1, . . . , Xn be i.i.d. observations from a density pG0, with G0 supported on a compact interval
[−a, a]. If α has support on [−a, a] with continuous Lebesgue density bounded below away from 0 and above from
Π(P : kp − p0k∞ ≥ M(n/ log n)−3/8 | X(n)) → 0. Consequently, for the Bayes’
∞, then, for suﬃciently large M > 0, Pn
estimator ˆpn(·) = R pG(·)Π(dG | X(n)) we have Pn
Proof. It is known from Proposition 4 in Gao and van der Vaart (2015) that the small-ball probability estimate in
condition (ii) of Theorem 1 is satisﬁed for ǫn = (n/ log n)−3/8. For the bias condition, we take Pn to be the support
of Π and show that, for 2Jn ∼ ǫ−1/3
= (n/ log n)1/8 and any symmetric density K with ﬁnite second moment, we have
kKJn(pG) − pGk∞
= O(ǫn) uniformly over the support of Π. Indeed, by applying Lemma 1 with β = 2, for each x ∈ R
ϕ × I2[ ˜K])(22Jn)−3, which
2 ≤ R | ˜ϕ(t)|2| ˜K(2−Jnt) − 1|2 dt ∼ (2π)−1(B2
it results |KJn(pG)(x) − pG(x)|2 ≤ kKJn(pG) − pGk2
implies that both conditions (1) and (i) are satisﬁed. The assertion on the Bayes’ estimator follows from the same
arguments laid out for random histograms together with the fact that pG ≤ 1/2 uniformly in G.
Example 3 (Dirichlet-Gaussian mixtures). Consider, as in Ghosal and van der Vaart (2001, 2007), Shen et al. (2013),
Scricciolo (2014), a Gaussian mixture prior Π × G thus deﬁned. For φ the standard normal density, let
• pF,σ(·) := R φσ(· − θ) dF(θ) denote a mixture of Gaussian densities with mixing distribution F,
• F ∼ Dα, the Dirichlet process with base measure α := αR ¯α, for 0 < αR < ∞ and ¯α a probability measure on
R, which has continuous and positive density α′(θ) ∝ e−b|θ|δ as |θ| → ∞, for some constants 0 < b < ∞ and
0 < δ ≤ 2,
• σ ∼ G which has continuous and positive density g on (0, ∞) such that, for constants 0 < C1, C2, D1, D2 < ∞,
0 ≤ s, t < ∞,

C1σ−s exp (−D1σ−1 logt(1/σ) ≤ g(σ) ≤ C2σ−s exp (−D2σ−1 logt(1/σ))

n

for all σ in a neighborhood of 0.

Let Cβ(R) denote the class of H¨older continuous functions on R with exponent β > 0. Let ǫn,β := (cid:0)n/ log n(cid:1)−β/(2β+1) be
). For any real β > 0, let ⌊β⌋ stand for the largest integer strictly
the minimax rate of convergence over (Cβ(R), k · k∞
smaller than β.
Proposition 3. Let X1, . . . , Xn be i.i.d. observations from a density p0 ∈ L∞(R) ∩ Cβ(R) such that condition (ii) is
satisﬁed for ǫn,β. Then, for suﬃciently large M > 0, Pn
Proof. Let K ∈ L1(R) be a convolution kernel such that
• R xkK(x) dx = 1
• the Fourier transform ˜K has supp( ˜K) ⊆ [−1, 1].
Let 2Jn ∼ ǫ
does not depend on x. Thus, kKJn(p0) − p0k∞
1/2 < ψ < t and a suitable constant 0 < E < ∞. For every σ ≥ σn and uniformly in F,

n,β . For every x ∈ R, |KJn(p0)(x)−p0(x)| ≤ C1(2−Jn)β . ǫn,β, where the constant C1 ∝ (1/⌊β⌋!)R |x|β|K(x)| dx
n,β)−1(log n)ψ, with

(k), k = 0, . . . , ⌊β⌋, andR |x|β|K(x)| dx < ∞,

0(Π × G)((F, σ) : kpF,σ − p0k∞ ≥ Mǫn,β | X(n)) → 0.

= O(ǫn,β). For the bias condition, let σn := E(nǫ2

{0}

1/β

kKJn(pF,σ) − pF,σk∞

= sup

sup

Z Z KJn(u)[φσ(x − v − u) − φσ(x − v)] du dF(v)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
x∈R (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
x∈R Z Z |e−it(x−v)|| ˜φσ(t)|| ˜K(2−Jnt) − 1| dt dF(v)
π Z|t|>2Jn | ˜φσ(t)| dt . σ−1

n exp (−(ρσn2Jn)2) . n−1 < εn,β

1
2π
1

≤

≤

because (σn2Jn)2 ∝ (log n)2ψ & (log n) as ψ > 1/2. Now, G(σ < σn) . σ−s
exp (−(C + 4)nǫ2

n) because ψ < t, which implies that the remaining mass condition (ii) is satisﬁed.

n exp (−[D2σ−1

n logt(1/σn)]) .

4

Remark 2. Conditions on the density p0 under which assumption (ii) of Theorem 1 is satisﬁed can be found, for
instance, in Shen et al. (2013) and Scricciolo (2014).

3.2. Quantile estimation

For τ ∈ (0, 1), consider the problem of estimating the τ-quantile qτ

0 of the population distribution function F0 from
observations X1, . . . , Xn. For any (possibly unbounded) interval I ⊆ R and function g on I, deﬁne the H¨older norm as

kgkCα(I) :=

⌊α⌋

Xk=0 kg(k)kL∞(I) + sup
x, y∈I: x,y

|g⌊α⌋(x) − g⌊α⌋(y)|

|x − y|α−⌊α⌋

.

Let C0(I) denote the space of continuous and bounded functions on I and Cα(I, R) := {g ∈ C0(I) : kgkCα(I) ≤ R},
R > 0.
Proposition 4. Suppose that, given τ ∈ (0, 1), there are constants r, ζ > 0 so that p0(· + qτ

0) ∈ Cα([−ζ, ζ], R) and

inf
[qτ
0−ζ, qτ

0

p0(x) ≥ r.

+ζ]

(3)

0) ∈ Cα([−ζ, ζ], R). If, for suﬃciently
Π(P : kp − p0k∞ ≥ Mǫn,α | X(n)) → 0, then, there exists M′ > 0 so that

Consider a prior Π concentrated on probability measures having densities p(·+qτ
large M, the posterior probability Pn
0
| X(n)) → 0.
Pn
0

Π(|qτ − qτ
Proof. We preliminarily make the following remark. Let F(x) := R x

quantile of F. By Lagrange’s theorem, there exists a point qτ
∗
Consequently,

between qτ and qτ

0| ≥ M′ǫ

1+1/α
n,α

−∞

p(y) dy, x ∈ R. For τ ∈ (0, 1), let qτ be the τ-
0).
)(qτ− qτ

0 so that F(qτ)− F(qτ

0) = p(qτ
∗

p(x) dx −Z qτ

0

−∞

p0(x) dx = Z qτ

qτ
0

p(x) dx +Z qτ

0

−∞

[p(x) − p0(x)] dx = p(qτ
∗

)(qτ − qτ

0) + [F(qτ

0) − F0(qτ

0)].

0 = τ − τ = Z qτ

−∞
) > 0, then

If p(qτ
∗

qτ − qτ

0 = −

[F(qτ

0) − F0(qτ
0)]
p(qτ
∗

)

= −

[F(qτ

0) − τ]
p(qτ
∗

)

.

(4)

0|, by appealing to relationship (4), we can separately control |F(qτ

0)− F0(qτ

0)| and p(qτ
∗

).

In order to upper bound |qτ− qτ
Let the kernel function K ∈ L1(R) be such that
• R xkK(x)dx = 1
• its Fourier transform ˜K has supp( ˜K) ⊆ [−1, 1].
By Lemma 5.2 in Dattner et al. (2013),

{0}

(k), k = 0, . . . , ⌊α⌋ + 1, and R |x|α+1|K(x)| dx < ∞,

with D := [R/(⌊α⌋ + 1)! + 2ζ−(α+1)]R |x|α+1|K(x)| dx. Write

0

sup

p0(·+qτ

Z qτ
0)∈Cα([−ζ, ζ], R)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
[Kb ∗ p0 − p0](x) dx +Z qτ

[Kb ∗ p0 − p0](x) dx(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
[Kb ∗ (p − p0)](x) dx +Z qτ

−∞

0

0

≤ Dbα+1,

0

0) = Z qτ

−∞

F(qτ

0) − F0(qτ

[p − Kb ∗ p](x) dx =: T1 + T2 + T3.
By inequality (5), we have |T1| = O(bα+1). By the same reasoning, |T3| = O(bα+1). We now consider T2. Taking into
account thatR K(x) dx = 1 and
0) = Z 1
T2 := [Kb∗(F−F0)](qτ

! (F−F0)(u) du = −Z K(z)(F−F0)(qτ

0−bz) dz = Z K(z)(F0−F)(qτ

0−bz) dz,

K  qτ

0 − u
b

−∞

−∞

b

(5)

5

T2 = [Kb ∗ (F − F0)](qτ

0) ∓ (F0 − F)(qτ

0)
0)] dz + (F0 − F)(qτ

for some point ξ between qτ

0 − bz and qτ

0 (clearly, ξ depends on qτ

0, z, b),
0) = Z K(z)[(F0 − F)(qτ
= Z K(z)(−bz)[D1(F0 − F)(ξ)] dz + (F0 − F)(qτ
0)
= (−b)Z zK(z)[(p0 − p)(ξ)] dz + (F0 − F)(qτ
0).

0 − bz) − (F0 − F)(qτ

Then, F(qτ

0)− F0(qτ

0)], which implies that 2[F(qτ

0) = T1 + T3 + (−b)R zK(z)[(p0 − p)(ξ)] dz− [(F − F0)(qτ

T1 +T3 +(−b)R zK(z)[(p0− p)(ξ)] dz. It follows that 2|F(qτ
account that R |z||K(z)| dz < ∞, |T1| = O(bα+1) and |T3| = O(bα+1), choosing b = O(ǫ
|T1| +|T3| + bkp0 − pk∞ . bα+1 + bkp0 − pk∞ . ǫ
for every 0 < η < r. In fact, for any interval I ⊇ [qτ
intermediate point qτ
∗
for every ˜x ∈ I. It follows that p(qτ
∗
that, in virtue of (4), Pn
0
follows.

0)] =
0)| ≤ |T1|+|T3|+bkp0− pk∞R |z||K(z)| dz. Taking into
0) − F0(qτ
n,α ), we have |F(qτ
0)| .
. If kp− p0k∞ . ǫn,α then, under condition (3), p(qτ
) > r − η > 0
∗
+ ζ] that includes the point qτ so that it also includes the
0 − ζ, qτ
0, for any η > 0 we have η & kp − p0k∞ ≥ supI |p(x) − p0(x)| ≥ |p( ˜x) − p0( ˜x)|
+ζ] p0(x) − η ≥ r − η. Conclude the proof by noting
) − η ≥ inf x∈[qτ
| X(n)). The assertion then
Π(|qτ − qτ

0−ζ, qτ
Π(P : kp − p0k∞ < Mǫn,α | X(n)) ≤ Pn
0

between qτ and qτ

0)− F0(qτ

) > p0(qτ
∗

0)−F0(qτ

0| < M′ǫ

1+1/α
n,α

1+1/α
n,α

1/α

0

0

Remark 3. Proposition 4 considers local H¨older regularity of p0, which seems natural for estimating single quan-
tiles. Clearly, requirements on p0 are automatically satisﬁed if p0 is globally H¨older regular and, in this case,
the minimax-optimal sup-norm rate is ǫn,α = (n/ log n)−α/(2α+1) so that the rate for estimating single quantiles is
1+1/α
= (n/ log n)−(α+1)/(2α+1). The conditions on the random density p are automatically satisﬁed if the prior is
ǫ
n,α
concentrated on probability measures possessing globally H¨older regular densities.

Appendix A. Proof of Theorem 1

Proof. Using the remaining mass condition (i) and the small-ball probability estimate (ii), by the proof of Theorem
2.1 in Ghosal et al. (2000), it is enough to construct, for each r ∈ {1, ∞}, a test Ψn,r for the hypothesis

H0 : P = P0

vs. H1 : {P ∈ Pn : kp − p0kr ≥ Mrǫn,r},

with Mr > 0 large enough, where Ψn,r ≡ Ψn,r(X(n); P0) : Xn → {0, 1} is the indicator function of the rejection region
of H0, such that

Pn
0

Ψn,r → 0 as n → ∞ and

sup

P∈Pn: kp−p0kr≥Mrǫn,r

Pn(1 − Ψn,r) ≤ exp(cid:0)−Kr M2

r nǫ2

n,r(cid:1) for suﬃciently large n,

where Kr M2
constant C0,r > 0 such that kPn
M0,r > C0,r, deﬁne the event An,r := (Tn,r > M0,rǫn,r) and the test Ψn,r := 1An,r. For

r ≥ (C + 4), the constant C > 0 being that appearing in (i) and (ii). By assumption (1), there exists a
0 ˆpn − p0kr = kKJn(p0) − p0kr ≤ C0,rǫn,r. Deﬁne Tn,r := k ˆpn − p0kr. For a constant

• r = 1, the triangular inequality Tn,1 ≤ k ˆpn − Pn
0 ˆpn − p0k1 > M0,1ǫn,1 − kPn

0 ˆpnk1 ≥ Tn,1 − kPn

k ˆpn − Pn

0 ˆpnk1 + kPn
0 ˆpn − p0k1 ≥ (M0,1 − C0,1)ǫn,1;

0 ˆpn − p0k1 implies that, when Tn,1 > M0,1ǫn,1,

• r = ∞, we have | ˆpn(x) − p0(x)| ≤ | ˆpn(x) − Pn
every x ∈ R. It follows that Tn,∞ ≤ k ˆpn − Pn
k ˆpn − Pn

0 ˆpn(x)| + |Pn
0 ˆpnk1 + kPn
0 ˆpn − p0k∞ > M0,∞ǫn,∞ − kPn

0 ˆpnk1 ≥ Tn,∞ − kPn

0 ˆpn(x) − p0(x)| ≤ k ˆpn − Pn
)ǫn,∞

0 ˆpn − p0k∞
0 ˆpn − p0k∞ ≥ (M0,∞ − C0,∞

for
, which implies that, when Tn,∞ > M0,∞ǫn,∞
,

0 ˆpnk1 + kPn
.

0 ˆpn − p0k∞

Let h : Xn → [0, 2] be the function deﬁned as h(X(n)) := k ˆpn−Pn
0 ˆpnk1. Thus, for each r ∈ {1, ∞}, when Tn,r > M0,rǫn,r,
the inequality h(X(n)) > (M0,r − C0,r)ǫn,r holds. Therefore, to control the type-one error probability, it is enough to
bound above the probability on the right-hand side of the following display

0Ψn,r ≤ Pn
Pn

0(cid:0)h(X(n)) > (M0,r − C0,r)ǫn,r(cid:1),

6

(A.1)

which can be done using McDiarmind’s inequality, McDiarmid (1989). Given any x(n) := (x1, . . . , xn) ∈ Xn, for each
1 ≤ i ≤ n, let xi be the ith component of x(n) and x′i := (xi + δ) a perturbation of the ith variable with δ ∈ R so that
x′i ∈ X. Letting ei be the canonical vector with all zeros except for a 1 in the ith position, the vector with the perturbed
ith variable can be expressed as x(n) + δei. If

(a) the function h has bounded diﬀerences: for some non-negative constants c1, . . . , cn,

sup
x(n), x′i

|h(x(n)) − h(x(n) + δei)| ≤ ci,

1 ≤ i ≤ n,

(b) Pn

0h(X(n)) = O(ǫn),

then, for C := Pn

i=1 c2

i , by McDiarmind’s bounded diﬀerences inequality,

∀ t > 0, Pn

0(cid:0)|h(X(n)) − Pn

0h(X(n))| ≥ t(cid:1) ≤ 2 exp(cid:0)−2t2/C(cid:1).

We show that (a) and (b) are veriﬁed.
(a) Using the inequality ||a| − |b|| ≤ |a − b|, setting Φ = K under condition a) of Deﬁnition (1),

∀ i ∈ {1, . . . , n},

sup
x(n), x′i

|h(x(n)) − h(x(n) + δei)| = sup

n

KJn(x, xi) − KJn(p0)(x)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
Xi=1
Xi,i′

KJn(x, xi) +

1
n

n

1
n

1
n

x(n), x′i Z "(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
−(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

1
nkKJn(·, xi) − KJn(·, x′i)k1 ≤

2
nkΦk1.

≤ sup
xi, x′i

# dx(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
KJn(x, x′i) − KJn(p0)(x)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

Hence, h has bounded diﬀerences with ci = 2kΦk1/n, 1 ≤ i ≤ n.

for the constant L:

(b) By Theorem 5.1.5 in Gin´e and Nickl (2015), Pn

• under conditions a) and b) of Deﬁnition (1), setting Φ = K in case a), L ≤ √2/(s − 1)kΦ2k
• under conditions c) and d), L ≤ C(φ)(1 ∨ kp0k1/2)1/2, where the constant C(φ) only depends on φ.

L1(µs)kp0k

0h(X(n)) ≤ Lp2Jn/n = O(ǫn), with the following upper bounds
1/2
L1(µs);

1/2

For α ∈ (0, 1), taking t = √2α(M0,r − C0,r)ǫn,r,

Pn

0(cid:0)|h(X(n)) − Pn

0h(X(n))| ≥

√2α(M0,r − C0,r)ǫn,r(cid:1) ≤ 2 exp(cid:0)−α2(M0,r − C0,r)2nǫ2

0h(X(n)) ≤ L′ǫn = (L′/Ln,r)ǫn,r. Hence, |h(X(n)) − Pn
By (b), there exists a constant L′ ≥ L so that Pn
0h(X(n)) ≥ h(X(n))−(L′/Ln,r)ǫn,r. Thus, for suﬃciently large Ln,r so that [(M0,r−C0,r)−(L′/Ln,r)] ≥
Pn
√2α(M0,r − C0,r)ǫn,r(cid:1)
n,r/kΦk2
1(cid:1).

0h(X(n))| ≥
≤ 2 exp(cid:0)−α2(M0,r − C0,r)2nǫ2

0 ˆpnk1 ≥ (M0,r − C0,r)ǫn,r(cid:1) ≤ Pn

0(cid:0)|h(X(n)) − Pn

0(cid:0)k ˆpn − Pn

Pn

n,r/kΦk2
1(cid:1).
0h(X(n))| ≥ h(X(n)) −
√2α(M0,r −C0,r),

We now provide an exponential upper bound on the type-two error probability. For r ∈ {1, ∞}, let P ∈ Pn be such

that kp − p0kr ≥ Mrǫn,r. For

• r = 1, when Tn,1 ≤ M0,1ǫn,1,

kp − p0k1 ≤ kp − Pn ˆpnk1 + k ˆpn − Pn ˆpnk1 + Tn,1 ≤ kp − Pn ˆpnk1 + k ˆpn − Pn ˆpnk1 + M0,1ǫn,1,

7

• r = ∞, when Tn,∞ ≤ M0,∞ǫn,∞

,

|p(x) − p0(x)| ≤ kp − Pn ˆpnk∞

∀ x ∈ X,
which implies that kp − p0k∞ ≤ kp − Pn ˆpnk∞

+ k ˆpn − Pn ˆpnk1 + Tn,∞ ≤ kp − Pn ˆpnk∞

+ k ˆpn − Pn ˆpnk1 + M0,∞ǫn,∞,

+ k ˆpn − Pn ˆpnk1 + M0,∞ǫn,∞

.

Summarizing, for r ∈ {1, ∞}, when Tn,r ≤ M0,rǫn,r, we have kp − p0kr ≤ kp − Pn ˆpnkr + k ˆpn − Pn ˆpnk1 + M0,rǫn,r. If
supP∈Pn kp − Pn ˆpnkr = supP∈Pn kp − KJn(p)kr ≤ CK ǫn,r, we have k ˆpn − Pn ˆpnk1 ≥ kp − p0kr − kp − Pn ˆpnkr − M0,rǫn,r ≥
[Mr − (CK + M0,r)]ǫn,r. Using, as before, McDiarmind’s inequality with P playing the same role as P0, we get that for
a constant α ∈ (0, 1) small enough and [Mr − (CK + M0,r)] > 0,

sup

P∈Pn: kp−p0kr≥Mrǫn,r

Pn(1 − φn,r) = Pn(k ˆpn − p0kr ≤ M0,rǫn,r) = P(cid:0)k ˆpn − Pn ˆpnk1 ≥ [Mr − (CK + M0,r)]ǫn,r(cid:1)
n,r/kΦk2
1).

≤ 2 exp (−α2[Mr − (CK + M0,r)]2nǫ2

We need that α2[Mr − (CK + M0,r)]2/kΦk2
concludes the proof of the ﬁrst assertion.

1 ≥ (C + 4), which implies that [Mr − (CK + M0,r)] ≥ α−1kΦk1 √C + 4. This
If the convergence in (2) holds for r = 1 and r = ∞, then the last assertion of the statement follows from the

interpolation inequality: for every 1 < s < ∞, kp − p0ks ≤ max{kp − p0k1, kp − p0k∞}.
Appendix B. Auxiliary results for Proposition 3

Following Parzen (1962), Watson and Leadbetter (1963), we adopt the subsequent deﬁnition.

Deﬁnition 2. The Fourier transform or characteristic function of a Lebesgue probability density function p on R,
denoted by ˜p, is said to decrease algebraically of degree β > 0 if

lim
|t|→∞|t|β| ˜p(t)| = Bp,

0 < Bp < ∞.

The following lemma is essentially contained in the ﬁrst theorem of section 3B in Watson and Leadbetter (1963).
Lemma 1. Let p ∈ L2(R) be a probability density with characteristic function that decreases algebraically of degree
β > 1/2. Let h ∈ L1(R) have Fourier transform ˜h satisfying

Iβ[˜h] := Z |1 − ˜h(t)|2
|t|2β

dt < ∞.

(B.1)

Then, δ−2(β−1/2)kp − p ∗ hδk2
Proof. Since p ∈ L1(R) ∩ L2(R), then kp ∗ hδkq ≤ kpkqkhδk1 < ∞, for q = 1, 2. Thus, p ∗ hδ ∈ L1(R) ∩ L2(R). It
follows that (p − p ∗ hδ) ∈ L1(R) ∩ L2(R). Hence,

2 → (2π)−1B2

p × Iβ[˜h] as δ → 0.

kp − p ∗ hδk2

2

=

δ2β−1
2π (cid:26)B2

p × Iβ[˜h] +Z |1 − ˜h(z)|2
|z|2β

[|z/δ|2β| ˜p(z/δ)|2 − B2

p] dz(cid:27),

where the second integral tends to 0 by the dominated convergence theorem because of assumption (B.1).

In the next remark, which is essentially due to Davis (1977), section 3, we consider a suﬃcient condition for a

t−2β|1 − ˜h(t)|2 dt < ∞ for β > 1/2. Suppose further that there exists an integer r ≥ 2

function h ∈ L1(R) to satisfy requirement (B.1).
Remark 4. If h ∈ L1(R), then R ∞
such thatR xmh(x) dx = 0, for m = 1, . . . , r − 1, and R xrh(x) dx , 0. Then,
r−1
[1 − ˜h(t)]
(r − 1)! Z xrh(x)Z 1
Xj=0

= −t−rZ (cid:20)eitx −

(itx) j

ir

1

tr

r! Z xrh(x) dx,
as t → 0. For r ≥ β, the integral R 1
0 t−2β|1 − ˜h(t)|2 dt < ∞. Conversely, for r < β, the integral diverges. Therefore,
for 1/2 < β ≤ 2, any symmetric probability density h with ﬁnite second moment is such that Iβ[˜h] < ∞ and condition
(B.1) is veriﬁed.

(1 − u)r−1eitxu du dx → −

h(x)(cid:21) dx = −

j!

0

ir

8

References

Barron, A., Schervish, M.J., Wasserman, L., 1999. The consistency of posterior distributions in nonparametric prob-
lems. The Annals of Statistics 27 (2), 536–561.

Castillo, I., 2014. On Bayesian supremum norm contraction rates. The Annals of Statistics, 42 (5), 2058–2091.

Dattner, I., Reiß, M., Trabs, M., 2013. Adaptive quantile estimation in deconvolution with unknown error distribution.
Technical Report. URL <http://arxiv.org/pdf/1303.1698.pdf>. Bernoulli, to appear

Davis, K.B., 1977. Mean integrated square error properties of density estimates. The Annals of Statistics 5 (3), 530–
535.

Gao, F., van der Vaart, A., 2015. Posterior contraction rates for deconvolution of Dirichlet-Laplace mixtures. Technical
Report. URL <http://arxiv.org/pdf/1507.07412v1.pdf>.

Ghosal, S., Ghosh, J.K., van der Vaart, A.W., 2000. Convergence rates of posterior distributions. The Annals of
Statistics 28 (2), 500–531.

Ghosal, S., van der Vaart, A.W., 2001. Entropies and rates of convergence for maximum likelihood and Bayes estima-
tion for mixtures of normal densities. The Annals of Statistics 29 (5), 1233–1263.

Ghosal, S., van der Vaart, A., 2007. Posterior convergence rates of Dirichlet mixtures at smooth densities. The Annals
of Statistics 35 (2), 697–723.
Gin´e, E., Nickl, R., 2011. Rates of contraction for posterior distributions in Lr-metrics, 1 ≤ r ≤ ∞. The Annals of
Statistics 39 (6), 2883–2911.

Gin´e, E., Nickl, R., 2015. Mathematical Foundations of Inﬁnite-dimensional Statistical Models. Cambridge Series in
Statistical and Probabilistic Mathematics.

McDiarmid, C., 1989. On the method of bounded diﬀerences. In: Surveys in Combinatorics, Cambridge University
Press, Cambridge, pp. 148–188.

Parzen, E., 1962. On estimation of a probability density function and mode. Annals of Mathematical Statistics 33 (3),
1065–1076.

Scricciolo, C., 2007. On rates of convergence for Bayesian density estimation. Scandinavian Journal of Statistics 34
(3), 626–642.

Scricciolo, C., 2011. Posterior rates of convergence for Dirichlet mixtures of exponential power densities. Electronic
Journal of Statistics 5, 270–308.

Scricciolo, C., 2014. Adaptive Bayesian density estimation in Lp-metrics with Pitman-Yor or normalized inverse-
Gaussian process kernel mixtures. Bayesian Analysis 9 (2), 475–520.

Shen, W., Tokdar, S.T., Ghosal, S., 2013. Adaptive Bayesian multivariate density estimation with Dirichlet mixtures.
Biometrika 100 (3), 623–640.

Shen, X., Wasserman, L., 2001. Rates of convergence of posterior distributions. The Annals of Statistics 29 (3),
687–714.

Watson, G.S., Leadbetter, M.R., 1963. On the estimation of the probability density, I. Annals of Mathematical Statis-
tics 34 (2), 480–491.

9

