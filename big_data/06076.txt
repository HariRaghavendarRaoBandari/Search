Improving Hypernymy Detection

with an Integrated Path-based and Distributional Method

Vered Shwartz

Ido Dagan
Computer Science Department, Bar-Ilan University, Ramat-Gan, Israel

Yoav Goldberg

6
1
0
2

 
r
a

 

M
9
1

 
 
]
L
C
.
s
c
[
 
 

1
v
6
7
0
6
0

.

3
0
6
1
:
v
i
X
r
a

vered1986@gmail.com

yoav.goldberg@gmail.com

dagan@cs.biu.ac.il

Abstract

Detecting hypernymy relations is a key
task in NLP, which is addressed in the
literature using two complementary ap-
proaches. Distributional methods, whose
supervised variants are the current best
performers, and path-based methods who
receive less research attention. We sug-
gest an improved path-based algorithm,
in which the dependency paths are en-
coded using a recurrent neural network,
and achieve results comparable to distri-
butional methods. We then extend the ap-
proach to integrate both path-based and
distributional signals,
signiﬁcantly im-
proving the state-of-the-art on this task.

1

Introduction

Hypernymy is an important lexical-semantic rela-
tion for NLP tasks. For instance, knowing that
Tom Cruise is an actor can help a question an-
swering system answer the question “which ac-
tors are involved in Scientology?”. While seman-
tic taxonomies, like WordNet (Fellbaum, 1998),
deﬁne hypernymy relations between word types,
they are limited in scope and domain. Therefore,
automated methods have been developed to iden-
tify, for a given term-pair (x, y), whether y is an
hypernym of x, based on their occurrences in a
large corpus.

For a couple of decades, this task has been ad-
dressed by two types of approaches: distributional,
and path-based. In distributional methods, the de-
cision whether y is a hypernym of x is based on
the distributional representations of these terms.
Lately, with the popularity of word embeddings
(Mikolov et al., 2013), most focus has shifted to-
wards supervised distributional methods, in which

each (x, y) term-pair is represented using some
combination of the terms’ embedding vectors.

In contrast to distributional methods, in which
the decision is based on the separate contexts of
x and y, path-based methods base the decision on
the lexico-syntactic paths connecting the joint oc-
currences of x and y in a corpus. Hearst (1992)
identiﬁed a small set of frequent paths that indicate
hypernymy, e.g. Y such as X. Snow et al. (2004)
represented each (x, y) term-pair as the multiset of
dependency paths connecting their co-occurrences
in a corpus, and trained a classiﬁer to predict hy-
pernymy, based on these features.

Using individual paths as features results in a
huge, sparse feature space. While some paths
are rare, they often consist of certain unimportant
components. For instance, “Spelt is a species of
wheat” and “Fantasy is a genre of ﬁction” yield
two different paths: X be species of Y and X be
genre of Y, while both indicating that X is-a Y. A
possible solution is to generalize paths by replac-
ing words along the path with their part-of-speech
tags or with wild cards, as done in PATTY (Nakas-
hole et al., 2012).

Overall, the state-of-the-art path-based methods
perform worse than the distributional ones. This
stems from a major limitation of path-based meth-
ods:
they require that the terms of the pair oc-
cur together in the corpus, limiting the recall of
these methods. While distributional methods have
no such requirement, they are usually less precise
in detecting a speciﬁc semantic relation like hy-
pernymy, and perform best on detecting broad se-
mantic similarity between terms. Though these
approaches seem complementary, there has been
rather little work on integrating them (Mirkin et
al., 2006; Kaji and Kitsuregawa, 2008).

In this paper, we propose an integrated path-
based and distributional method for hypernymy
detection. Inspired by recent progress in relation

classiﬁcation, we use a long short-term memory
(LSTM) network (Hochreiter and Schmidhuber,
1997) to encode dependency paths.
In order to
create enough training data for our network, we
followed previous methodology of constructing a
dataset based on knowledge resources.

We ﬁrst show that our path-based approach, on
its own, substantially improves performance over
prior path-based methods, yielding performance
comparable to state-of-the-art distributional meth-
ods. Our analysis suggests that the neural path rep-
resentation enables better generalizations. While
coarse-grained generalizations, such as replacing a
word by its POS tag, capture mostly syntactic sim-
ilarities between paths, our method captures also
semantic similarities.

We then show that we can easily integrate dis-
tributional signals in the network. The integration
results conﬁrm that the distributional and path-
based signals indeed provide complementary in-
formation, with the combined model yielding an
improvement of up to 14 F1 points over each indi-
vidual model.

2 Background

We introduce the two main approaches for hyper-
nymy detection: distributional (section 2.1), and
path-based (section 2.2). We then discuss the re-
cent use of recurrent neural networks in the related
task of relation classiﬁcation (section 2.3).

2.1 Distributional Methods
Hypernymy detection is commonly addressed us-
ing distributional methods. In these methods, the
decision whether y is a hypernym of x is based on
the distributional representations of the two terms,
i.e., the contexts with which each term occurs sep-
arately in the corpus.

Earlier methods developed unsupervised mea-
sures for hypernymy, starting with symmetric sim-
ilarity measures (Lin, 1998), and followed by di-
rectional measures based on the distributional in-
clusion hypothesis (Weeds and Weir, 2003; Kotler-
man et al., 2010). This hypothesis states that the
contexts of a hyponym are expected to be largely
included in those of its hypernym. More recent
work (Santus et al., 2014; Rimell, 2014) introduce
new measures, based on the assumption that the
most typical linguistic contexts of a hypernym are
less informative than those of its hyponyms.

More recently, the focus of the distributional ap-

proach shifted to supervised methods.
In these
methods, the (x, y) term-pair is represented by a
feature vector, and a classiﬁer is trained on these
vectors to predict hypernymy. Several methods
are used to represent term-pairs as a combination
of each term’s embeddings vector: concatenation
(cid:126)x⊕(cid:126)y (Baroni et al., 2012), difference (cid:126)y−(cid:126)x (Roller
et al., 2014; Fu et al., 2014; Weeds et al., 2014),
and similarity (cid:126)x · (cid:126)y. Based on neural word em-
beddings (Mikolov et al., 2013; Pennington et al.,
2014), these methods are easy to apply, and show
good results (Baroni et al., 2012; Roller et al.,
2014).

2.2 Path-based Methods

A different approach to detecting hypernymy be-
tween a pair of terms (x, y) considers the lexico-
syntactic paths that connect the joint occurrences
of x and y in a large corpus. Automatic acquisi-
tion of hypernyms from free text, based on such
paths, was ﬁrst proposed by Hearst (1992), who
identiﬁed a small set of lexico-syntactic paths that
indicate hypernymy relations (e.g. Y such as X, X
and other Y).

In a later work, Snow et al. (2004) learned to de-
tect hypernymy. Rather than searching for speciﬁc
paths that indicate hypernymy, they represent each
(x, y) term-pair as the multiset of all dependency
paths that connect x and y in the corpus, and train
a logistic regression classiﬁer to predict whether y
is a hypernym of x, based on these paths.

Paths that indicate hypernymy are those that
were assigned high weights by the classiﬁer. The
paths identiﬁed by this method were shown to
subsume those found by Hearst (1992), yield-
ing improved performance. Variations of Snow
et al.’s (2004) method were later used in tasks
such as taxonomy construction (Snow et al., 2006;
Kozareva and Hovy, 2010; Carlson et al., 2010;
Riedel et al., 2013), analogy identiﬁcation (Tur-
ney, 2006), and deﬁnition extraction (Borg et al.,
2009; Navigli and Velardi, 2010).

A major

limitation in relying on lexico-
syntactic paths is the sparsity of the feature space.
Since similar paths may somewhat vary at the lex-
ical level, generalizing such variations into more
abstract paths can increase recall. The PATTY al-
gorithm (Nakashole et al., 2012) applied such gen-
eralizations for the purpose of acquiring a taxon-
omy of term relations from free text. For each
path, they added generalized versions in which a

Figure 1: An example dependency tree of the sentence “par-
rot is a bird”, with x=parrot and y=bird, represented in our
notation as X/NOUN/nsubj < be/VERB/ROOT > Y/NOUN/attr.

subset of words along the path were replaced by
either their POS tags, their ontological types or
wild-cards. This generalization increased recall
while maintaining the same level of precision.

2.3 RNNs for Relation Classiﬁcation
Relation classiﬁcation is a related task whose goal
is to classify the relation that is expressed between
two target terms in a given sentence, to one of
predeﬁned relation classes. To illustrate, consider
the following sentence, from the SemEval-2010
relation classiﬁcation task dataset (Hendrickx et
al., 2009): “The [apples]e1 are in the [basket]e2”.
Here, the relation expressed between the target en-
tities is Content − Container(e1, e2).

The shortest dependency paths between the tar-
get entities were shown to be informative for this
task (Fundel et al., 2007). Recently, deep learning
techniques showed good performance in capturing
the indicative information in such paths.

In particular, several papers show improved per-
formance using recurrent neural networks (RNN)
that process a dependency path edge-by-edge. Xu
et al. (2015; 2016) apply a separate long short-
term memory (LSTM) network to each sequence
of words, POS tags, dependency labels and Word-
Net hypernyms along the path. A max-pooling
layer on the LSTM outputs is used as the in-
put of a network that predicts the classiﬁcation.
Other papers suggest incorporating additional net-
work architectures to further improve performance
(Nguyen and Grishman, 2015; Liu et al., 2015).

While relation classiﬁcation and hypernymy de-
tection are both concerned with identifying se-
mantic relations that hold for pairs of terms, they
differ in a major respect. In relation classiﬁcation,
the relation should be expressed in the given text,
while in hypernymy detection, the goal is to rec-
ognize a generic lexical-semantic relation between
terms, that holds in many contexts. Accordingly,
in relation classiﬁcation, a term-pair is represented
by a single dependency path, while in hypernymy
detection, it is represented by the multiset of all

dependency paths in which they co-occur in the
corpus.

3 LSTM-based Hypernymy Detection

We present an LSTM-based method for hyper-
nymy detection. We ﬁrst focus on improving path
representation (section 3.1), and then integrate dis-
tributional signals into our network, creating a
combined method (section 3.2).

3.1 Path-based Network
Similarly to prior work, we represent each depen-
dency path as a sequence of edges that leads from
x to y in the dependency tree.1 Each edge contains
the lemma and part-of-speech tag of the source
node, the dependency label, and the edge direction
between two subsequent nodes. For readability,
we denote each edge as lemma/P OS/dep, plac-
ing the direction signs between two subsequent
edges. See ﬁgure 1 for an illustration.

Rather than treating an entire dependency path
as a single feature, we encode the sequence of
edges using a long short-term memory (LSTM)
network. The vectors obtained for the different
paths of a given (x, y) pair are pooled, and the re-
sulting vector is used for classiﬁcation. Figure 2
depicts the overall network structure, which is de-
scribed below.

Edge Representation We represent each edge
by the concatenation of its components’ vectors:

(cid:126)ve = [(cid:126)vl, (cid:126)vpos, (cid:126)vdep, (cid:126)vdir]

where (cid:126)vl, (cid:126)vpos, (cid:126)vdep, (cid:126)vdir represent the embed-
ding vectors of the lemma, part-of-speech, depen-
dency label and dependency direction (along the
path from x to y), respectively.

Path Representation For a path p composed of
edges e1, ..., ek, the edge vectors (cid:126)ve1, ..., (cid:126)vek are
fed in order to an LSTM encoder, resulting in
a vector (cid:126)op representing the entire path p. The
LSTM architecture is effective at capturing tem-
poral patterns in sequences. We expect the train-
ing procedure to drive the LSTM encoder to focus
on parts of the path that are more informative for
the classiﬁcation task while ignoring others.

1Like Snow et al. (2004), we added for each path, addi-
tional paths containing single daughters of x or y not already
contained in the path, to include paths such as Such Y as X.

Figure 2: An illustration of term-pair classiﬁcation. Each term-pair is represented by several paths. Each path is a sequence of
edges, and each edge consists of four components: lemma, POS, dependency label and dependency direction. Each edge vector
is fed in sequence into the LSTM, resulting in a path embedding vector (cid:126)op. The averaged path vector becomes the term-pair’s
feature vector, used for classiﬁcation. The dashed (cid:126)vwx , (cid:126)vwy vectors refer to the integrated network described in section 3.2.

Term-Pair Classiﬁcation Each (x, y) term-pair
is represented by the multiset of lexico-syntactic
paths that connected x and y in the corpus, de-
noted as paths(x, y), while the supervision is
given for the term pairs. We represent each (x, y)
term-pair as the weighted-average of its path vec-
tors, by applying average pooling on its path vec-
tors, as follows:

(cid:80)
(cid:80)
p∈paths(x,y) fp,(x,y)· (cid:126)op
p∈paths(x,y) fp(x,y)

(1)

(cid:126)vxy = (cid:126)vpaths(x,y) =

is

where fp,(x,y)

the frequency of p in
paths(x, y). We then feed this path vector to a
single-layer network that performs binary classiﬁ-
cation to decide whether y is a hypernym of x.

c = sof tmax(W · (cid:126)vxy)

(2)

c is a 2-dimensional vector whose components
sum to 1, and we classify a pair as positive if
c[1] > 0.5.

Implementation Details To train the network,
we used PyCNN2. We minimize the cross en-
tropy loss using gradient-based optimization, with
mini-batches of size 10 and the Adam update rule
(Kingma and Ba, 2014). Regularization is applied
by a dropout on each of the components’ embed-
dings. We tuned the hyper-parameters (learning
rate and dropout rate) on the validation set. Table 3
displays the chosen hyper-parameters values.

We initialized the lemma embeddings with
the 50-dimensional pre-trained GloVe word em-
trained on
beddings (Pennington et al., 2014),

2https://github.com/clab/cnn

Wikipedia. The other embeddings, as well as out-
of-vocabulary lemmas, are initialized randomly.
We update all embedding vectors during training.

Integrated Network

3.2
The network presented in section 3.1 classiﬁes
each (x, y) term-pair based on the paths that con-
nect x and y in the corpus. Our goal was to
improve upon previous path-based methods for
hypernymy detection, and we show in section 6
that our network indeed outperforms them. Yet,
as path-based and distributional methods are con-
sidered complementary, we present a simple way
to integrate distributional features in the network,
yielding improved performance.

information on each term.

We extended the network to take into account
distributional
In-
spired by the supervised distributional concatena-
tion method (Baroni et al., 2012), we simply con-
catenate x and y word embeddings to the (x, y)
feature vector, redeﬁning (cid:126)vxy:

(cid:126)vxy = [ (cid:126)vwx, (cid:126)vpaths(x,y), (cid:126)vwy ]

(3)

where (cid:126)vwx and (cid:126)vwy are x and y’s word embed-
dings, respectively, and (cid:126)vpaths(x,y) is the averaged
path vector deﬁned in equation 1. This way, each
(x, y) pair is represented using both the distribu-
tional features of x and y, and their path-based
features.

4 Dataset
4.1 Creating Instances
Neural networks typically require a large amount
of training data, whereas the existing hypernymy
datasets, like BLESS (Baroni and Lenci, 2011),

resource
WordNet
DBPedia
Wikidata

Yago

relations

type

instance hypernym, hypernym

subclass of, instance of

subclass of

Table 1: Hypernymy relations in each resource.

random split
lexical split

train
49,475
20,335

test
17,670
6,610

validation

3,534
1,350

all

70,679
28,295

Table 2: The number of instances in each dataset.

are relatively small. Therefore, we followed the
common methodology of creating a dataset us-
ing distant supervision from knowledge resources
(Snow et al., 2004; Riedel et al., 2013). Fol-
lowing Snow et al. (2004), who constructed their
dataset based on WordNet hypernymy, and aiming
to create a larger dataset, we extract hypernymy
relations from several resources: WordNet (Fell-
baum, 1998), DBPedia (Auer et al., 2007), Wiki-
data (Vrandeˇci´c, 2012) and Yago (Suchanek et al.,
2007).

All instances in our dataset, both positive and
negative, are pairs of terms that are directly re-
lated in at least one of the resources. These re-
sources contain thousands of relations, some of
which indicate hypernymy with varying degrees of
certainty. To avoid including questionable relation
types, we consider as denoting positive examples
only indisputable hypernymy relations (Table 1),
which we manually selected from the set of hyper-
nymy indicating relations in Shwartz et al. (2015).
Term-pairs related by other relations (including
hyponymy), are considered as negative instances.
Using related rather than random term-pairs as
negative instances tests our method’s ability to dis-
tinguish between hypernymy and other kinds of
semantic relatedness. We maintain a ratio of 1:4
positives to negatives in the dataset.

Like Snow et al. (2004), we include only term-
pairs that have joint occurrences in the corpus, re-
quiring at least two different dependency paths for
each pair.

4.2 Random and Lexical Dataset Splits
As our primary dataset, we perform standard ran-
dom splitting, with 70% train, 25% test and 5%
validation sets.

As pointed out by Levy et al. (2015), super-
vised distributional lexical inference methods tend
to perform “lexical memorization”, i.e., instead of
learning a relation between the two terms, they

mostly learn an independent property of a single
term in the pair: whether it is a “prototypical hy-
pernym” or not. For instance, if the training set
contains term-pairs such as (dog, animal), (cat,
animal), and (cow, animal), all annotated as posi-
tive examples, the algorithm may learn that animal
is a prototypical hypernym, classifying any new (x,
animal) pair as positive, regardless of the relation
between x and animal. Levy et al. (2015) sug-
gested to split the train and test sets such that each
will contain a distinct vocabulary (“lexical split”),
in order to prevent the model from overﬁtting by
lexical memorization.

To investigate such behaviors, we present re-
sults also for a lexical split of our dataset. In this
case, we split the train, test and validation sets
such that each contains a distinct vocabulary. We
note that this differs from Levy et al. (2015), who
split only the train and the test sets, and dedicated a
subset of the train for validation. We chose to devi-
ate from Levy et al. (2015) because we noticed that
when the validation set contains terms from the
train set, the model is rewarded of lexical mem-
orization when tuning the hyper-parameters, con-
sequently yielding suboptimal performance on the
lexically-distinct test set. When each set has a dis-
tinct vocabulary, the hyper-parameters are tuned
to avoid lexical memorization and are likely to
perform better on the test set. We tried to keep
roughly the same 70/25/5 ratio in our lexical split.3
The sizes of the two datasets are shown in Table 2.
Indeed, training a model on a lexically split
dataset may result in a more general model, that
can better handle pairs consisting of two unseen
terms during inference. However, we argue that
in the common applied scenario, the inference in-
volves an unseen pair (x, y), in which x and/or
y have already been observed separately. Models
trained on a random split may introduce the model
with a term’s “prior probability” of being a hyper-
nym or a hyponym, and this information can be
exploited beneﬁcially at inference time.

5 Baselines

We compare our method with several state-of-the-
art methods for hypernymy detection, as described
in section 2: path-based methods (section 5.1), and
distributional methods (section 5.2). Due to differ-
ent works using different datasets and corpora, we

3The lexical split discards many pairs consisting of cross-

set terms.

Path-based

Distributional

method

Snow

Snow + Gen

LSTM (this paper)

SLQS (Santus et al., 2014)

Best Supervised

Combined

LSTM-Integrated (this paper)

random split

lexical split

regularization: L2
regularization: L1

dropout: d = 0.5

N = 50, threshold = 0.01

regularization: L2
regularization: L2

dropout: d = 0.5

N = 50, threshold = 0.02

GloVe-100-Wikipedia, learning rate: α = 0.001

GloVe-50-Wikipedia, learning rate: α = 0.001

concatenation, SVM, GloVe-300-Wikipedia

GloVe-50-Wikipedia, learning rate: α = 0.001

concatenation, SVM, GloVe-100-Wikipedia

GloVe-50-Wikipedia, learning rate: α = 0.001

word dropout: d = 0.3

word dropout: d = 0.3

Table 3: The best hyper-parameters in every model.

path

X/NOUN/dobj > establish/VERB/ROOT < as/ADP/prep < Y/NOUN/pobj

X/NOUN/dobj > VERB < as/ADP/prep < Y/NOUN/pobj

X/NOUN/dobj > * < as/ADP/prep < Y/NOUN/pobj

X/NOUN/dobj > establish/VERB/ROOT < ADP < Y/NOUN/pobj
X/NOUN/dobj > establish/VERB/ROOT < * < Y/NOUN/pobj

Table 4: Example generalizations of X was established as Y.

replicated the baselines rather than comparing to
the reported results.

We use the Wikipedia dump from May 2015 as
the underlying corpus of all the methods, and parse
it using spaCy4. We perform model selection on
the validation set to tune the hyper-parameters of
each method.5 The best hyper-parameters are re-
ported in Table 3.

5.1 Path-based Methods
Snow We follow the original paper, and extract
all shortest paths of four edges or less between
terms in a dependency tree. Like Snow et al.
(2004), we add paths with “satellite edges”, i.e.,
single words not already contained in the depen-
dency path, which are connected to either X or Y,
allowing paths like such Y as X. The number of
distinct paths was 324,578. We apply χ2 feature
selection to keep only the 100,000 most informa-
tive paths and train a logistic regression classiﬁer.

Generalization We also compare our method to
a baseline that uses generalized dependency paths.
Following PATTY’s (Nakashole et al., 2012) ap-
proach to generalizing paths, we replace edges
with their part-of-speech tags as well as with wild
cards. We generate the powerset of all possible
generalizations, including the original paths. See
Table 4 for examples. The number of features af-
ter generalization went up to 2,093,220. Similarly
to the ﬁrst baseline, we apply feature selection,
this time keeping the 1,000,000 most informative
paths, and train a logistic regression classiﬁer over
the generalized paths.6

4https://spacy.io/
5We applied grid search for a range of values, and picked
the ones that yield the highest F1 score on the validation set.
6We also tried keeping the 100,000 most informative

paths, but the performance was worse.

5.2 Distributional Methods
Unsupervised SLQS (Santus et al., 2014) is an
entropy-based measure for hypernymy detection,
reported to outperform previous state-of-the-art
unsupervised methods (Weeds and Weir, 2003;
Kotlerman et al., 2010). Following the original
paper, we set the number of each term’s most as-
sociated contexts to N=50. We use the validation
set to tune the threshold for classifying a pair as
positive. As our low results suggest, while this
method is state-of-the-art for unsupervised hyper-
nymy detection, it is basically designed for classi-
fying speciﬁcity level of related terms, rather than
hypernymy in particular.

Supervised To represent term-pairs with distri-
butional features, we tried several state-of-the-art
methods: concatenation (cid:126)x⊕(cid:126)y (Baroni et al., 2012),
difference (cid:126)y−(cid:126)x (Roller et al., 2014; Fu et al., 2014;
Weeds et al., 2014), and similarity (cid:126)x· (cid:126)y. We down-
loaded several pre-trained embeddings (Mikolov
et al., 2013; Pennington et al., 2014) of different
sizes, and trained a number of classiﬁers: logis-
tic regression, SVM, and SVM with RBF kernel,
which was reported by Levy et al. (2015) to per-
form best in this setting. We perform model selec-
tion on the validation set to select the best word
vectors, method and regularization factor (see Ta-
ble 3).

6 Results

Table 5 displays performance scores of our
method and the baselines. LSTM is our path-based
recurrent neural network model (section 3.1) and
LSTM-Integrated is our combined method (sec-
tion 3.2). Comparing the path-based methods
shows that generalizing paths improves recall
while maintaining similar levels of precision, re-
assessing the behavior found in Nakashole et al.
(2012). Our LSTM-based method outperforms
both path-based baselines by a signiﬁcant im-
provement in recall and with slightly lower pre-
cision. The recall boost is due to better path gen-

method

Snow
Snow + Gen
LSTM (this paper)
SLQS (Santus et al., 2014)
Best supervised (concatenation)
LSTM-Integrated (this paper)

Path-based

Distributional

Combined

0.843
0.852
0.811
0.246
0.901
0.913

random split

precision

recall
0.452
0.561
0.716
0.213
0.637
0.890

precision

lexical split
recall
0.438
0.530
0.632
0.222
0.551
0.617

0.760
0.759
0.691
0.270
0.754
0.809

F1
0.556
0.624
0.660
0.243
0.637
0.700

F1
0.589
0.676
0.761
0.228
0.746
0.901

Table 5: Performance scores on our method compared to the path-based baselines and the state-of-the-art distributional methods
for hypernymy detection, on both variations of the dataset – with lexical and random split to train / test / validation.

eralization, as demonstrated in section 7.1.

Regarding distributional methods, the unsuper-
vised SLQS baseline performed worse on our
dataset. The low precision stems from its inability
to distinguish between hypernyms and meronyms,
which are common in our dataset, causing many
false positive pairs such as (zabrze, poland) and
(kibbutz, israel). Out of a sample of 50 false posi-
tive pairs, 38% were holonym-meronym pairs.

In accordance with previously reported results,
the supervised embedding-based method is the
best performing baseline on our dataset as well.
Our purely path-based method performs slightly
better, achieving state-of-the-art results. Adding
distributional features to our method shows that
these two approaches are indeed complementary.
On both dataset splits, the performance differences
between the integrated method and our pure path-
based method, as well as the supervised distribu-
tional method, are substantial, and statistically sig-
niﬁcant with p-value of 1% (paired t-test).

We also reassess that indeed supervised distri-
butional methods perform worse on a lexical split
(Levy et al., 2015). We further observe a simi-
lar reduction when using the LSTM-based meth-
ods, which is not a result of lexical memorization,
but rather stems from over-generalization (sec-
tion 7.1).

7 Analysis
7.1 Qualitative Analysis of Learned Paths
We analyze our method’s ability to generalize over
path structures, by comparing prominent indica-
tive paths which were learned by each of the path-
based methods. We do so by ﬁnding high-scoring
paths that contributed to the classiﬁcation of true-
positive pairs in the dataset.
In the path-based
baselines, these are the highest-weighted features
as learned by the logistic regression classiﬁer. In
the LSTM-based method, it is less straightforward
to identify the most indicative paths. We assess the
contribution of a certain path p to classiﬁcation by

regarding it as the only path that appeared for the
term-pair, and compute its TRUE label score from
the class distribution: sof tmax(W · (cid:126)vxy)[1], set-
ting (cid:126)vxy = [(cid:126)0, (cid:126)op,(cid:126)0].

A notable pattern is that Snow’s method learns
speciﬁc paths, like X is Y from (e.g. Megadeth
is an American thrash metal band from Los An-
geles). While Snow’s method can only rely on
verbatim paths, limiting its recall, the generalized
version of Snow often makes coarse generaliza-
tions, such as X VERB Y from. Clearly, such a
path is too general, and almost any verb assigned
to it results in a non-indicative path (e.g. X take
Y from). Efforts by the learning method to avoid
such generalization, again, lower the recall. Our
method provides a better midpoint, making ﬁne-
grained generalizations by learning additional se-
mantically similar paths such as X become Y from
and X remain Y from. See table 6 for additional
example paths which illustrate these behaviors.

We also noticed that while on the random split
our model learns a range of speciﬁc paths such as
X is Y published (learned for e.g. Y=magazine)
and X is Y produced (Y=ﬁlm), in the lexical split
it only learns the general X is Y path for these re-
lations. We note that X is Y is a rather “noisy”
path, which may occur in ad-hoc contexts with-
out indicating generic hypernymy relations (e.g.
chocolate is a big problem in the context of chil-
dren’s health). While such a model may identify
hypernymy relations between unseen terms, based
on general paths, it is prone to over-generalization,
hurting its performance, as seen in Table 5. As
discussed in section 4.2, we suspect that this sce-
nario, in which both terms are unseen, is usually
not common enough to justify this limiting train-
ing setup.

7.2 Error Analysis
False Positive We categorized the false positive
pairs on the random split according to the rela-
tion holding between each pair of terms in the re-

method
Snow

Snow +

Gen

LSTM-
Integrated

X/NOUN/nsubj > be/VERB/ROOT < Y/NOUN/attr > direct/VERB/acl
X/NOUN/nsubj > be/VERB/ROOT < Y/NOUN/attr > publish/VERB/acl

X/NOUN/compound > NOUN∗ be/VERB/ROOT < Y/NOUN/attr > base/VERB/acl

path

X/NOUN/compound > NOUN < Y/NOUN/compound
X/NOUN/nsubj > be/VERB/ROOT < Y/NOUN/attr
> (release|direct|produce|write)/VERB/acl

X/NOUN/compound > (association|co.|company|corporation|

foundation|group|inc.|international|limited|ltd.)/NOUN/nsubj

> be/VERB/ROOT < Y/NOUN/attr > (create|found|headquarter|

own|specialize)/VERB/acl

example text

Eyeball is a 1975 Italian-Spanish ﬁlm directed by...
Allure is a U.S. beauty magazine published monthly
Calico Light Weapons Inc. (CLWS) is an American
privately held manufacturing company based in...

Weston Town Council

Blinky is a 1923 American comedy-Western

ﬁlm directed by...

Retalix Ltd. is a software company

Table 6: Examples of indicative paths learned by each method, with corresponding true positive term-pairs from the random
split test set. Hypernyms are marked red and hyponyms are marked blue.

Relation
synonymy
hyponymy

holonymy / meronymy
hypernymy-like relations

other relations

%

21.37%
29.45%
9.36%
21.03%
18.77%

Table 7: Distribution of relations holding between each pair
of terms in the resources among false positive pairs.

sources used to construct the dataset. We grouped
several semantic relations from different resources
to broad categories, e.g. synonym includes also
alias and Wikipedia redirection. Table 7 displays
the distribution of semantic relations among false
positive pairs.

More than 20% of the errors stem from confus-
ing synonymy with hypernymy, which are known
to be difﬁcult to distinguish. An additional 30%
of the term-pairs are reversed hypernym-hyponym
pairs (y is a hyponym of x). Examining a sample
of these pairs suggests that they are usually near-
synonyms, i.e., it is not that clear whether one term
is truely more general than the other or not. For in-
stance, ﬁction is annotated in WordNet as a hyper-
nym of story, while our method classiﬁed ﬁction
as its hyponym.

A possible future research direction might be
to quite simply extend our network to classify
term-pairs simultaneously to multiple semantic re-
lations, as in Pavlick et al. (2015). Such a multi-
class model can hopefully better distinguish be-
tween these similar semantic relations.

Another notable category is hypernymy-like re-
lations: these are other relations in the resources
that could also be considered as hypernymy (e.g.
occupation), but were annotated as negative due
to our restrictive selection of only indisputable
hypernymy relations from the resources (see sec-
tion 4.1).

Lastly, other errors made by the model often
correspond to term-pairs that co-occur very few
times in the corpus, e.g. xebec, a studio produc-
ing Anime, was falsely classiﬁed as a hyponym of
anime.

Error Type
low statistics
infrequent term

rare hyponym sense

annotation error

%
80%
36%
16%
8%

1
2
3
4

Table 8: (Overlapping) categories of false negative pairs:
(1) x and y co-occurred less than 25 times (average co-
occurrences for true positive pairs is 99.7). (2) Either x or
y is infrequent. (3) The hypernymy relation holds for a rare
sense of x. (4) (x, y) was incorrectly annotated as positive.

False Negative We sampled 50 term-pairs that
were falsely annotated as negative, and analyzed
the major (overlapping) types of errors (Table 8).
Most of these pairs had only few co-occurrences
in the corpus. This is often either due to infrequent
terms (cbc.ca), or a rare sense of x in which the
hypernymy relation holds ((night, play)). Such a
term-pair may have too few hypernymy-indicating
paths, leading to classifying it as negative.

8 Conclusion

We presented a neural-networks-based method for
hypernymy detection. First, we focused on im-
proving path representation using LSTM, suggest-
ing a path-based model that performs signiﬁcantly
better than prior path-based methods, and matches
the previously superior distributional methods. We
demonstrated that the increase in recall is a result
of generalizing semantically-similar paths, in con-
trast to prior methods, which either make no gen-
eralizations or over-generalize paths.

We then extended our network by integrating
distributional signals, yielding an improvement of
additional 14 F1 points, and demonstrating that the
path-based and the distributional approaches are
indeed complementary.

Finally, our architecture seems straightfor-
wardly applicable for multi-class classiﬁcation,
which, in future work, could be used to classify
term-pairs to multiple semantic relations.

References
[Auer et al.2007] S¨oren Auer, Christian Bizer, Georgi
Kobilarov, Jens Lehmann, Richard Cyganiak, and
Zachary Ives. 2007. Dbpedia: A nucleus for a web
of open data. Springer.

[Baroni and Lenci2011] Marco Baroni and Alessandro
Lenci. 2011. How we blessed distributional seman-
In Proceedings of the GEMS 2011
tic evaluation.
Workshop on GEometrical Models of Natural Lan-
guage Semantics, pages 1–10.

[Baroni et al.2012] Marco Baroni, Raffaella Bernardi,
Ngoc-Quynh Do, and Chung-chieh Shan. 2012. En-
tailment above the word level in distributional se-
mantics. In EACL, pages 23–32.

[Borg et al.2009] Claudia Borg, Mike Rosner, and Gor-
don Pace. 2009. Evolutionary algorithms for deﬁni-
tion extraction. In Proceedings of the 1st Workshop
on Deﬁnition Extraction, pages 26–32.

[Carlson et al.2010] Andrew Carlson, Justin Betteridge,
Bryan Kisiel, Burr Settles, Estevam R Hruschka Jr,
and Tom M Mitchell. 2010. Toward an architec-
In AAAI,
ture for never-ending language learning.
volume 5, page 3.

[Fellbaum1998] Christiane Fellbaum. 1998. WordNet.

Wiley Online Library.

[Fu et al.2014] Ruiji Fu, Jiang Guo, Bing Qin, Wanx-
iang Che, Haifeng Wang, and Ting Liu.
2014.
Learning semantic hierarchies via word embed-
dings. In ACL, pages 1199–1209.

[Fundel et al.2007] Katrin Fundel, Robert K¨uffner, and
Ralf Zimmer. 2007. Relexrelation extraction using
dependency parse trees. Bioinformatics, 23(3):365–
371.

[Hearst1992] Marti A Hearst. 1992. Automatic acqui-
sition of hyponyms from large text corpora. In ACL,
pages 539–545.

[Hendrickx et al.2009] Iris Hendrickx, Su Nam Kim,
Zornitsa Kozareva, Preslav Nakov, Diarmuid
´O S´eaghdha, Sebastian Pad´o, Marco Pennacchiotti,
Lorenza Romano, and Stan Szpakowicz.
2009.
Semeval-2010 task 8: Multi-way classiﬁcation of se-
mantic relations between pairs of nominals. In Se-
mEval, pages 94–99.

[Hochreiter and Schmidhuber1997] Sepp Hochreiter
and J¨urgen Schmidhuber. 1997. Long short-term
memory. Neural computation, 9(8):1735–1780.

[Kaji and Kitsuregawa2008] Nobuhiro Kaji and Masaru
Kitsuregawa. 2008. Using hidden markov random
ﬁelds to combine distributional and pattern-based
word clustering. In COLING, pages 401–408.

[Kingma and Ba2014] Diederik Kingma and Jimmy
Ba. 2014. Adam: A method for stochastic opti-
mization. arXiv preprint arXiv:1412.6980.

[Kotlerman et al.2010] Lili Kotlerman, Ido Dagan, Idan
Szpektor, and Maayan Zhitomirsky-Geffet. 2010.
Directional distributional similarity for lexical infer-
ence. NLE, 16(04):359–389.

[Kozareva and Hovy2010] Zornitsa Kozareva and Ed-
uard Hovy. 2010. A semi-supervised method to
learn and construct taxonomies using the web.
In
EMNLP, pages 1110–1118.

[Levy et al.2015] Omer Levy, Steffen Remus, Chris
Biemann, and Ido Dagan. 2015. Do supervised dis-
tributional methods really learn lexical inference re-
lations. NAACL.

[Lin1998] Dekang Lin.

1998.

theoretic deﬁnition of similarity.
ume 98, pages 296–304.

An information-
In ICML, vol-

[Liu et al.2015] Yang Liu, Furu Wei, Sujian Li, Heng
Ji, Ming Zhou, and Houfeng Wang.
2015. A
dependency-based neural network for relation clas-
siﬁcation. arXiv preprint arXiv:1507.04646.

[Mikolov et al.2013] Tomas Mikolov, Ilya Sutskever,
Kai Chen, Gregory S Corrado, and Jeffrey Dean.
2013. Distributed representations of words and
phrases and their compositionality. In NIPS, pages
3111–3119.

[Mirkin et al.2006] Shachar Mirkin, Ido Dagan, and
Maayan Geffet. 2006. Integrating pattern-based and
distributional similarity methods for lexical entail-
ment acquisition. In COLING and ACL, pages 579–
586.

[Nakashole et al.2012] Ndapandula Nakashole, Ger-
hard Weikum, and Fabian Suchanek. 2012. Patty: a
taxonomy of relational patterns with semantic types.
In EMNLP and CoNLL, pages 1135–1145.

[Navigli and Velardi2010] Roberto Navigli and Paola
Velardi.
2010. Learning word-class lattices for
deﬁnition and hypernym extraction. In ACL, pages
1318–1327.

[Nguyen and Grishman2015] Thien Huu Nguyen and
Ralph Grishman. 2015. Combining neural networks
and log-linear models to improve relation extraction.
arXiv preprint arXiv:1511.05926.

[Pavlick et al.2015] Ellie Pavlick, Johan Bos, Malvina
Nissim, Charley Beller, Benjamin Van Durme, and
Chris Callison-Burch. 2015. Adding semantics to
data-driven paraphrasing. In ACL.

[Pennington et al.2014] Jeffrey Pennington, Richard
Socher, and Christopher D. Manning. 2014. Glove:
Global vectors for word representation. In EMNLP,
pages 1532–1543.

[Riedel et al.2013] Sebastian Riedel, Limin Yao, An-
drew McCallum, and Benjamin M Marlin. 2013.
Relation extraction with matrix factorization and
universal schemas. In NAACL.

[Rimell2014] Laura Rimell. 2014. Distributional lexi-
cal entailment by topic coherence. In EACL, pages
511–519.

[Roller et al.2014] Stephen Roller, Katrin Erk, and
Inclusive yet selective:
In

Gemma Boleda.
Supervised distributional hypernymy detection.
COLING, pages 1025–1036.

2014.

[Santus et al.2014] Enrico Santus, Alessandro Lenci,
Qin Lu, and Sabine Schulte Im Walde. 2014. Chas-
ing hypernyms in vector spaces with entropy.
In
EACL, pages 38–42.

[Shwartz et al.2015] Vered Shwartz, Omer Levy, Ido
Dagan, and Jacob Goldberger. 2015. Learning to
exploit structured resources for lexical inference. In
CoNLL, page 175.

[Snow et al.2004] Rion Snow, Daniel Jurafsky, and An-
drew Y Ng. 2004. Learning syntactic patterns for
automatic hypernym discovery. In NIPS.

[Snow et al.2006] Rion Snow, Daniel Jurafsky, and An-
drew Y Ng. 2006. Semantic taxonomy induction
In ACL, pages 801–
from heterogenous evidence.
808.

[Suchanek et al.2007] Fabian M Suchanek, Gjergji
Kasneci, and Gerhard Weikum. 2007. Yago: a core
of semantic knowledge. In WWW, pages 697–706.
ACM.

[Turney2006] Peter D Turney. 2006. Similarity of se-

mantic relations. CL, 32(3):379–416.

[Vrandeˇci´c2012] Denny Vrandeˇci´c. 2012. Wikidata: A
In

new platform for collaborative data collection.
WWW, pages 1063–1064. ACM.

[Weeds and Weir2003] Julie Weeds and David Weir.
2003. A general framework for distributional sim-
ilarity. In EMLP, pages 81–88.

[Weeds et al.2014] Julie Weeds, Daoud Clarke, Jeremy
Refﬁn, David Weir, and Bill Keller. 2014. Learn-
ing to distinguish hypernyms and co-hyponyms. In
COLING, pages 2249–2259.

[Xu et al.2015] Yan Xu, Lili Mou, Ge Li, Yunchuan
Chen, Hao Peng, and Zhi Jin. 2015. Classifying re-
lations via long short term memory networks along
shortest dependency paths. In EMNLP.

[Xu et al.2016] Yan Xu, Ran Jia, Lili Mou, Ge Li,
Yunchuan Chen, Yangyang Lu, and Zhi Jin. 2016.
Improved relation classiﬁcation by deep recurrent
arXiv
neural networks with data augmentation.
preprint arXiv:1601.03651.

