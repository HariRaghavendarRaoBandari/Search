Graph Balancing for Distributed Subgradient Methods over Directed

Graphs

Ali Makhdoumi∗ and Asuman Ozdaglar∗

6
1
0
2

 
r
a

 

M
0
1
 
 
]

.

C
O
h
t
a
m

[
 
 

1
v
1
6
4
3
0

.

3
0
6
1
:
v
i
X
r
a

Abstract— We consider a multi agent optimization problem
where a set of agents collectively solves a global optimization
problem with the objective function given by the sum of locally
known convex functions. We focus on the case when information
exchange among agents takes place over a directed network
and propose a distributed subgradient algorithm in which
each agent performs local processing based on information
obtained from his incoming neighbors. Our algorithm uses
weight balancing to overcome the asymmetries caused by
the directed communication network, i.e., agents scale their
outgoing information with dynamically updated weights that
converge to balancing weights of the graph. We show that
both the objective function values and the consensus violation,
the estimates generated by the
at
√T ), where T is the number
algorithm, converge with rate O( log T
of iterations. A special case of our algorithm provides a new
distributed method to compute average consensus over directed
graphs.

the ergodic average of

A. Motivation

I. INTRODUCTION

Many of today’s optimization problems in data science
(including statistics, machine learning, and data mining) use
distributed computation. Modern processors have access to
thousands of data points much more than they can process.
Therefore, there is a need to distribute data among different
data centers and then process it in a decentralized way based
on the information that is provided to them. Distributed
computation can lead to gains in computational efﬁciency
in statistical learning, as shown by a number of authors
([4], [37], [1], [9]). The applications in statistical learning
along with other applications in distributed control (see
e.g. distributed sensor networks [6], coordination [13], and
ﬂow control [21]) have motivated a ﬂurry of research on
distributed approaches for solving optimization problems
where the objective function is the sum of local objective
functions of agents (nodes) that are connected through a
network (see [7], [45], [44], [27], [26], [20], [19], [36], [18],
[34]).

Most of the existing algorithms assume information ex-
change over undirected networks where the connection be-
tween nodes are bidirectional, meaning that if node i can
send information to node j,
then node j can also send
information to node i.1 However, in many applications, the
underlying graph is directed because nodes typically broad-
cast at different power levels and have varying interference
and noise patterns, implying communication capability in
one direction, but not the other. In this paper, we consider

∗ MIT, Cambridge, MA 02139, Emails: makhdoum@mit.edu,

asuman@mit.edu

1We use the terms graph and network interchangeably.

a multi-agent optimization problem where a set of agents
collectively minimize the sum of locally known convex
functions fi(x), using information exchange over a directed
network.

B. Related Works and Contribution

i.e.,

Our paper is related to a large recent literature on dis-
tributed methods for solving multi agent optimization prob-
lems over networks. Much of this literature builds on the
seminal works [42], [43], which proposed gradient methods
that can parallelize computations across multiple processors.
We summarize subgradient based distributed methods for
solving multiagent optimization problems over undirected
and directed graphs.
Undirected Graphs: A number of recent papers proposed
subgradient type distributed methods that use consensus or
averaging mechanisms for aggregating information among
the agents over an undirected network (see [26], [27], [20],
[35], [15], [41], [22], [23], [48], [2], [8]). For convex
local objective functions, these methods achieve O( log T
)
√T
convergence rate, where T is the number of iterations. The
recent paper [47] adopts stronger assumptions on the local
objective functions,
they are convex with Lipschitz
continuous gradients, and shows that a distributed gradient
method with averaging converges at rate O( 1
T ) to an error
neighborhood (with the further assumption of strongly con-
vex local objective functions, linear rate is achieved to an
error neighborhood). Another contribution [14] assumes local
objective functions have continuous and bounded gradients
and uses Nesterov’s acceleration to design a distributed
algorithm with rate O( log T
T ). The recent paper [40], proposes
a gradient-based distributed algorithm for convex objective
function with Lipschitz continuous gradients and shows its
convergence with rate O( 1
Directed Graphs: A few recent papers proposed and studied
distributed subgradient methods over directed graphs [41],
[24], [25]. The key idea used in these papers is the incor-
poration of the push-sum algorithm, which is a distributed
algorithm presented in [16] to obtain the average of initial
values of nodes over a directed graph. Push-sum, while
being an effective approach in obtaining averages over a
directed graph, involves updates that include a nonlinear
operation (division by weight estimates), which makes the
analysis within a subgradient optimization method involved
(see e.g. [24]). In this paper, we use an alternative ap-
proach based on weight-balancing to design a distributed
subgradient algorithm over directed networks. The notion of
weights that balance a directed graph was introduced in [12]

T ).

and studied recently in a number of papers with the goal
of designing algorithms that enable their computation in a
distributed manner (see [10], [11], [38], [32]). We combine
such an algorithm for updating weights together with a
distributed subgradient algorithm and show that these updates
implemented simultaneously in the same time scale solves
multiagent optimization problem over directed graphs. The
update step for agent’s estimates involves operators linear
in each estimate, allowing the analysis to use techniques
from time-varying non-homogeneous non-negative matrix
theory [39]. Although we do not pursue this here, our algo-
rithm can be generalized to work over time-varying directed
graphs (see e.g. [26], [24] for the analysis of distributed
subgradient methods over time-varying graphs). Note that
our algorithm does not require the local functions to have
Lipschitz continuous gradient nor being strongly convex (the
only requirement of our algorithm is that the functions are
convex and not necessarily smooth). Indeed with these strong
assumptions the rate of convergence can be improved as
shown in [47] and [25] for algorithms over undirected graphs.
Our work also contributes to the vast literature on the
consensus problem, where agents have a more speciﬁc goal
of aligning their estimates (see [13], [30], [46], [31] for
consensus over undirected graphs and [16], [5], [33] for
consensus over directed graphs). In particular, a special case
of our algorithm provides a new distributed method for
computing average of initial values over a directed graph.
Out contribution here is most closely related to [33] and [5],
which proposed distributed algorithms for average consensus
over directed graphs using balancing weights. Reference [33]
builds on earlier work [32], which presented a distributed
algorithm for computing balancing node weights based on
approximating the left eigenvector associated with the zero
eigenvalue of the Laplacian matrix of the underlying directed
network. In [33], the authors used this algorithm for updating
weights in the same time scale as the update for estimates and
showed convergence to average of initial values. Reference
[5] provides a similar algorithm for average consensus based
on an earlier work [11], which proposes an update rule for
computing balancing edge weights.
C. Outline

The organization of paper is as follows. In Section II,
we give the problem formulation and present the distributed
algorithm. In Section III, we show the convergence of
weights to balancing weights of the graph. In Section IV,
we consider the sequence of ergodic (time) averages of the
estimates generated by our algorithm. We then show optimal-
ity convergence of our algorithm and establish O( log T
) rate
√T
of convergence. Finally, in Section V, we provide numerical
results that illustrate the performance of our algorithm, which
leads to concluding remarks in Section VI.
D. Basic Notations

A vector x is viewed as a column vector. For a matrix
A, we write [A]i to denote the ith column of matrix A,
[A]i to denote the ith row of matrix A, and Aij to denote
the entry at ith row and jth column. For a vector x, xi

denotes the ith component of the vector. We show the L1
norm of a vector x ∈ Rn with ||x||1 , Pn
i=1 |xi|. We also
let ||x||∞ = maxi |xi|. For a set S, |S| denotes the number
of elements of S.

II. PROBLEM SETUP AND ALGORITHM

A. Formulation

We consider a set of agents (nodes) V = {1, . . . , n}
connected through a directed graph G = (V, E) where
E ⊂ V × V is the set of directed edges, i.e., (i, j) ∈ E
represents a directed edge from agent i to agent j. We denote
the in-neighbors and out-neighbors of an agent i by N in(i) =
{j ∈ V | (j, i) ∈ E} and N out(i) = {j ∈ V | (i, j) ∈ E},
i = |N out(i)|
respectively. We also use din
to denote the number of in-neighbors and out-neighbors.

i = |N in(i)| and dout

We consider the following optimization problem

n

fi(x),

(1)

min
x∈R

Xi=1

where fi
: R → R is a convex function (possibly non-
smooth), known to agent i only. The goal is to solve (1)
using an algorithm that
involves each agent performing
computations based on his local objective function fi and ex-
changing information over the directed graph (i.e., receiving
information from his in-neighbors and sending the outcome
of his computation to his out-neighbors).

We adopt the following standard assumption on the un-

derlying graph G.

Assumption 1 (Strongly connected graph):

The graph
G = (V, E) is strongly connected, i.e., for all nodes i, j ∈ V ,
there exists a directed path from i to j.
This assumption ensures that every node i receives informa-
tion from every other node j in the graph.
B. Algorithm

Our algorithm generalizes the distributed subgradient al-
gorithm presented in [7] to allow its implementation over
directed graphs. The algorithm in [7] requires some form of
symmetry in information exchange between pairs of nodes
(essentially that node j’s information in node i’s update
is scaled with the same weight used in scaling node i’s
information in node j’s update). This kind of symmetry does
not exist in directed graphs with directed communication
over edges. Hence, a direct application of the algorithm given
in [7] will lead to information from high out-degree nodes
to be disproportionately represented in the estimate formed
by agents. To alleviate this, our proposed algorithm scales
outgoing information from each node with time-varying
weights which in the limit ensures the incoming and the
outgoing information of a node to be balanced. The following
deﬁnition introduces node weights that satisfy this balancing
requirement. The notion of node weights that balance a
directed graph was proposed in [12] and used recently for
deriving a Lyapunov function for convergence analysis of
average-consensus [29], [30], consensus on general functions
[3], design of stable ﬂocking algorithms [17], and trafﬁc-ﬂow
problems [12].

wj (t)xj (t)

+
 Xj∈N in(i)

(2)

 − α(t)gi(t),

Deﬁnition 1 (Balancing weights): The node weights wi
for i ∈ V balance a directed graph G if for any i, we have

widout

i = Xj∈N in(i)

wj.

This deﬁnition ensures that the total weight outgoing from
node i (measured by widout
) is equal to the total weight
i

incoming to node i (measured by Pj∈N in(i) wj), hence the
term balancing weights.
We next describe our algorithm. Let xi(t) ∈ R denote the
estimate of agent i at time t for the optimal solution of (1).
Each agent i starts from arbitrary value xi(0) ∈ R and weight
wi(0) ∈ R. At time t, agent i updates its estimate xi(t) as
xi(t + 1) =xi(t)(cid:0)1 − wi(t)dout
i (cid:1)

where gi(t) is a subgradient of fi at xi(t), i.e., gi(t) ∈
∂fi(xi(t)), α(t) is a step size sequence and wi(t) is a scalar
weight. Each agent i linearly combines the estimates of his
incoming neighbors and his own estimate and takes a step
along the negative subgradient of his local objective function.
This is followed by the following weight update at node i

wi(t + 1) =

1
2

wi(t) +

1
dout

i Xj∈N in(i)

1
2

wj (t).

(3)

In order to understand this update rule, note that if the
sequence of weights {wi(t)}t, i = 1, . . . , n converges, it
follows from (3) that the limiting weights balance the graph.
With these balancing weights, update (2) ensures that in the
limit the incoming information and the outgoing information
of node i is properly scaled.

In principle, update (2) is similar to the algorithm consid-
ered in [7] for updating estimates, i.e., each agent updates
his estimate by linearly combining estimates of neighbors
together with a local optimal step. However, note that the
weight matrix, i.e., the matrix that contains the scalars that
multiply the estimates of the agents, is not doubly stochastic.
We will show in Sections III-IV that it is column stochastic
and becomes doubly stochastic only in the limit and our
analysis shows this property sufﬁces to guarantee that the
estimates (obtained by (2)) converge to the optimal solution
of problem (1).

III. CONVERGENCE OF WEIGHTS

In this section, we show that the sequence of weights
generated by the update (3) converges to balancing weights
for G. Using the notation w(t) = [w1(t), . . . , wn(t)]′,
we can write the weight updates of our algorithm more
compactly as

where D = diag(dout
n ) and A is the adjacency matrix
of the directed graph deﬁned as Aij = 1 for all i, j ∈ N in(i).
We let P = 1
2 (I + D−1A) and rewrite the weight updates as

1 , . . . , dout

w(t + 1) = P w(t).

Next, we show the weight sequence generated by the update
(3) converges to balancing weights.

Lemma 1: Under Assumption 1, the sequence w(t) con-

verges to w and w = [w1, . . . , wn]′ balances the graph.

1
2

1
dout
j

2 (I + AD−1) and show it

2 +Pi : j∈N in(i)

Proof: We ﬁrst show that P is primitive.2 Note that all
entries of P are non-negative, the diagonal entries of P are
positive, and the underlying graph is strongly connected. The
result follows from these facts. We next show the spectral
radius of P , ρ(P ), is one. We deﬁne an auxiliary matrix
¯P = 1
is column stochastic.
This holds because the summation of the entries on the
= 1. Since ¯P
jth column of ¯P is 1
is column stochastic, we have ρ( ¯P ) = 1. Therefore, we
have 1 = ρ( ¯P ) = ρ( ¯P DD−1) = ρ(D−1 ¯P D) = ρ( 1
2 (I +
D−1AD−1D)) = ρ(P ), where the equality ρ( ¯P DD−1) =
ρ(D−1 ¯P D) holds because the set of eigenvalues of AB is
the the same as the set of eigenvalues of BA for two arbitrary
matrices A and B. Finally, since P is a primitive matrix
with ρ(P ) = 1, using Perron-Frobenius Theorem, the limit
limt→∞ P t exists and as a result w(t) converges to some
w that satisﬁes w = P w. Using the deﬁnition of P , this
yields wi = 1
wj, which results in
dout
2 wj. Therefore, {w(t)}∞t=0 converges
i
to weights that balance the graph.

2 wi + Pj: j∈N in(i)

2 wi = Pj∈N in(i)

1
2

1
dout
i

1

1

The generated weight sequence w(t) depends on the initial
value of w(0). We consider the sequence update (2) and
note that in order to guarantee the contribution of agent i’s
estimate at time t in his estimate at time t + 1 is positive,
we need the coefﬁcient of xi(t) which is 1− wi(t)dout
to be
positive at each iteration. In the next lemma we show that by
choosing a small w(0), we can guarantee this. Let D denote
the diameter and d∗ = maxi dout
denote the maximum out-
degree of the graph G.

i

i

then wi(t)dout

i < 1 for all i and t ≥ 0.

Lemma 2: If for all i = 1, . . . , n, wi(0) ≤ (1/d∗)2D+1,
Proof: Let u and v be left and right eigenvectors of P
corresponding to eigenvalue one; i.e. u′P = u′ and P v =
v. From Perron-Frobenius Theorem, we have limt→∞ P t =
vu′, where u > 0, v > 0,Pn
i=1 viui = 1.
Next, we will bound the entries of v. Let vM = maxi vi and
vm = mini vi denote the maximum and minimum entries
among vi’s. The i-th equation of P v = v can be written as

i=1 vi = 1, andPn

vi = Xk∈N in(i)

1
dout
i − 1

vk

for all k ∈ N in(i).

(5)

w(t + 1) =

1
2

(I + D−1A)w(t),

(4)

2A matrix P is primitive if there exists n ∈ N such that all entries of

P n are positive.

i

n

vk

(6)

vi ≥

i=1 vi = 1, we obtain

Using (5), for any i we can bound vi by a factor of vk for
k ∈ N in(i) as follows
1
d∗
where d∗ = maxi dout

. Using the above inequality, for any
uM , for all i, where D is the
the diameter of the graph. Using this inequality along with

for all k ∈ N in(i),
d∗(cid:1)D
k, we obtain that vi ≥(cid:0) 1
Pn
vi ≥ vM n(cid:18) 1

Xi=1
n (d∗)D. Next, we ﬁnd a lower
which yields to vM ≤ 1
bound on the value of vm. Again, using (6), we obtain that
d∗(cid:1)D
vm ≥ (cid:0) 1
vi, for all i. Using this inequality along with
i=1 vi = 1, we obtain 1 = Pn
Pn
i=1 vi ≤ vmnd∗D, which
d∗(cid:1)D.
yields to vm ≥ 1
n(cid:0) 1
Next, we will use the derived bounds on vm and vM in order
to bound the summation of entries of each row of P t. Since
P v = v, for any t, we have P t
v = v. Because the entries
of P t are non-negative, for any i we have

d∗(cid:19)D

1 =

,

n

vM = max
1≤i≤n

vi ≥ vi = [P tv]i =

[P t]ijvj ≥ vm
Plugging in the bounds on vm and vM , we obtain

Xj=1

[P t]ij .

n

Xj=1

Therefore, if we let wi(0) <

n

Xj=1

[P t]ij ≤

vM
vm ≤

1

1

1

= d∗2D.

n d∗D
n ( 1
d∗ )D
d∗(2D+1) for all i ∈ V , then
i d∗2D||w(0)||∞ < 1,
i > 0 for all t ≥ 0, and i ∈ V .

[P t]ijwj (0) ≤ dout
Xj=1

n

dout
i wi(t) = dout

i

and we obtain 1 − wi(t)dout

IV. CONVERGENCE OF ALGORITHM

A. Preliminary Results

We can write the updates of our algorithm (2) in a compact

form as

where

x(t + 1) = Q(t)x(t) − α(t)g(t),

(7)

g(t) = [g1(t), . . . , gn(t)]′,

x(t) = [x1(t), . . . , xn(t)]′,

and Q(t) is a matrix such that [Q(t)]ii = 1 − wi(t)dout
[Q(t)]ij = −wj(t) for any i and j ∈ N in(i).
Using (7) recursively, we can relate the estimate at time
t + 1 to initial estimates and the intermediate matrices and
subgradients as follows
x(t + 1) = [Q(t)Q(t − 1) . . . Q(0)] x(0)

and

i

−

t−1

Xs=0

α(s) [Q(t) . . . Q(s + 1)] g(s) − α(t)g(t).
(8)

This motivates the following deﬁnition

Φ(t : s) = Q(t) . . . Q(s),

for t ≥ s,

with the convention that Φ(t : t + 1) = I. Using this
deﬁnition (8) can be written as

x(t + 1) = Φ(t : 0)x(0) −

t

Xs=0

Φ(t : s + 1)g(s)α(s).

(9)

Next, we show that for any t the matrix Q(t) is column
stochastic and becomes doubly stochastic only in the limit
(i.e., Q is doubly stochastic, where limt→∞ Q(t) = Q) and
our analysis shows this sufﬁces to guarantee the entries of
Φ(t : s) converge to 1
n exponentially fast as t − s grows
large. In the analysis that follows, we will use Theorem 4.14
and Theorem 4.19 of [39].

Proposition 1 ([39]):

1) Let Q(t) be a sequence of
column stochastic matrices such that limt→∞ Q(t) =
Q (entry-wise), where Q is a primitive matrix. Then,
for any s, we have that limt→∞[Φ(t : s)]ij = pj,
where p is the unique probability vector (non-negative
with summation equal to one) such that Qp = p.

2) Let Q(t) be a sequence of column stochastic primitive
matrices such that min+
ij [Q(t)]ij ≥ γ, where min+
denotes the minimum among positive entries. Then
there exist a probability vector p(s) such that

Lemma 3: There exist constants C and λ such that for

|[Φ(t : s)]ij − pj(s)| ≤
any t ≥ s and i, j ∈ V , we have

1
1 − γ

(1 − γ)t−s+1.

i

1
n| ≤ Cλ(t−s+1).

|[Φ(t : s)]ij −

Proof: Using Lemma 2, since for any i, dout

(10)
i wi(t) <
1, all
the entries of matrices Q(t) are non-negative for
t ≥ 0. Furthermore, for any t the matrix Q(t) is column
stochastic because for any i, Pn
i=1 Qij(t) = 1− wj(t)dout
j +
Pi: j∈N in(i) wj (t) = 1. Using Lemma 1, limt→∞ Q(t) = Q,
where Qii = 1 − widout
and Qij = wi for j ∈ N in(i).
The matrix Q is row stochastic as well because for any j,
Pn
i=1 Qij = 1 − dout
j wj +Pi: j∈N in(i) wj = 1. Since Q is

both column and row stochastic the unique probability vector
p for which Qp = p holds, is a vector with all entries equal
to 1
n . Also note that since all entries of Q are non-negative,
the diagonal entries of Q are positive, and the underlying
graph is strongly connected, we know that the matrix Q is
primitive (with the same argument Q(t) is primitive for any
t).
We now have all the conditions to use the ﬁrst part of
Proposition 1, to obtain limt→∞[Φ(s : t)]ij = 1
n , for all
i, j ∈ V .
Next, we use the second part of Proposition 1 in order
to establish the exponential rate of convergence. Let δ =
min+
ij Qij. Since limt→∞ Q(t) = Q, there exists t0 such that
2 . Therefore,
for any t ≥ t0, we have that ||Q(t) − Q||∞ ≤ δ
for t ≥ t0, we have that min+[Q(t)]ij ≥ δ
2 . We let γ =
t≤t0, i,j [Q(t)]ij} to obtain min+
ij [Q(t)]ij ≥ γ,
min{ δ

2 , min+

for any t. Now we can use the second part of Proposition 1
to obtain that for any s,

|[Φ(t : s)]ij −

1
n| ≤

This completes the proof.

1
1 − γ

(1 − γ)t−s+1.

Remark 1: When fi(x) = 0 for all i ∈ V and all x ∈ R
(implying gi(t) = 0, for all i ∈ V and all t ≥ 0), the update
step (2) simpliﬁes to
xi(t + 1) = xi(t)(cid:0)1 − wi(t)dout

i (cid:1) + Xj∈N in(i)

wj(t)xj (t).

(11)

For this special case combining (9) with Lemma 3 shows that
the estimates xi(t) generated by algorithm (2)-(3) converges
to the average of the initial values, i.e., limt→∞ xi(t) =
nPn
1
i=1 xi(0), and the rate of convergence is exponential.
Hence, our algorithm provides a new distributed method for
computing average of initial values over a directed graph (see
[33], [5] for similar algorithms).

B. Consensus in Estimates

We will ﬁrst show that under some mild assumptions on
the step size sequence, the disagreement between estimates
of agents goes to zero, i.e., limt→∞ |xi(t) − xj (t)| = 0, for
all i, j ∈ V . To that end, deﬁne an auxiliary sequence as

We let y(0) = 1

i=1 xi(0) to obtain

α(t)

n

n

Xi=1

y(t + 1) = y(t) −
nPn
Xi=1

xi(0) −

1
n

n

1
n

t−1

Xs=0

Xi=1

gi(t).

(12)

n

y(t) =

α(s)

gi(s).

(13)

Using the compact form (9), the update of node i at time
t + 1 can be written as

xi(t + 1) = [Φ(t : 0)x(0)]i −

t

Xs=0

[Φ(t : s + 1)g(s)]i α(s).

(14)

We establish the convergence rate of our algorithm using
the ergodic average of the sequence {xi(t)} generated by
algorithm (2)-(3), deﬁned as

and

ˆxi(T ) =

ˆy(T ) =

1

t=0 α(t)

PT

1

t=0 α(t)

PT

α(t)xi(t),

(15)

α(t)y(t).

(16)

T

Xt=0

T

Xt=0

Lemma 4: Let the sequence {α(t)}∞t=0 be non-negative.

For all i ∈ V , we have

t−1

Xs=0

|xi(t) − y(t)| ≤ Cλt||x(0)||1 + nL

Cλt−s−1α(s).

(17)
Moreover, if the sequence {α(t)}∞t=0 is non-increasing and
converges to zero, then for any i and j,
t→∞|xi(t) − xj (t)| = 0.

lim

Proof:

Using (14), (13), and H¨older’s inequality, we have

n

n

−

1
n

t−1

t−1

xi(0) +

|xi(t) − y(t)| = | [Φ(t − 1 : 0)x(0)]i
Xs=0
[Φ(t − 1 : s + 1)g(s)]i α(s)
−
Xi=1
Xs=0
Xi=1
1
≤ max
|[Φ(t − 1 : 0)]ij −
n| × ||x(0)||1
j
t−1
Xs=0

α(s)L max

gi(s)|

α(s)

+ n

1
n

j

1
n|

|[Φ(t − 1 : s + 1)]ij −
Xs=0

Cλt−1−sα(s),

t−1

≤ Cλt||x(0)||1 + nL

Xs=0
≤(cid:18) max
0≤s≤t0
ǫ(1 − λ)

+

where we used Lemma 3 to obtain the last inequality. For
a given ǫ, let t0 be such that for t ≥ t0, α(t) ≤ ǫ(1−λ)
to
obtain
t−1

t−1

t0

2

α(s)λt−1−s =

α(s)λt−1−s +

α(s)λt−1−s

Xs=0

Xs=t0+1

λs

t0

Xs=0

α(s)(cid:19) λ−t0 λt−1
Xs=t0+1

t−1

1

ǫ
2

.

2

lim

λt−1 +

1 − λ

Therefore, for any i, j, we obtain

λt−1−s ≤ α(0)λ−t0
Using this relation, we have limt→∞Pt−1
2 .
s=0 α(s)λt−1−s ≤ ǫ
2λt||x(0)||1(cid:17) + nLCǫ = nLCǫ.
t→∞|xi(t) − xj (t)| ≤(cid:16) lim
Since ǫ is arbitrary, by taking ǫ → 0, we conclude
limt→∞ |xi(t) − xj(t)| = 0.
Lemma 4 shows the convergence of the sequences {xi(t) −
xj(t)}∞t=0 to zero. Next, we bound the difference between
the ergodic average sequences {ˆxi(t)}∞t=0 and {ˆy(t)}∞t=0.
Theorem 1: The ergodic average of the sequence gener-
ated by algorithm (2)-(3), satisﬁes

t→∞

Suppose that

Assumption 2:

the functions fi have
bounded subgradient, i.e., there exists L such that for any
i ∈ V and x

|gi| ≤ L,

for any gi ∈ ∂fi(x) and any x.

2

T

t=0 α(t)

|ˆxi(T ) − ˆy(T )| ≤
Xt=0

α(t) Cλt||x(0)||1 + Ln

PT

Cλt−1−sα(s)! (18)

t−1

Xs=0

Proof: Using convexity of norm, for any i, we have

|ˆxi(T ) − ˆy(T )| ≤

1

t=0 α(t)

T

Xt=0

α(t)|xi(t) − y(t)|.

PT

We next use Lemma 4 to upper bound each term on the right
hand side of the previous relation, which yields to

T

1

|ˆxi(T ) − ˆy(T )| ≤
Xt=0
PT

t=0 α(t)

α(t) Cλt||x(0)||1 + Ln

Cλt−1−sα(s)! .

t−1

Xs=0

C. Optimality Convergence

Let F (x(t)) = Pn
and F (x∗) =
Pn
i=1 fi(x∗), where x∗ = (x∗, . . . , x∗) ∈ Rn is an optimal
Lemma 5: The ergodic average of the sequence generated

solution of (1). Also let ˆx(T ) = (ˆx1(T ), . . . , ˆxn(T )).

i=1 fi(xi(t))

by algorithm (2)-(3), satisﬁes

Proof: Using bounded subgradient assumption and

Cλt−sα(s)!! (19)

1

F (ˆx(T )) − F (ˆy(T )) ≤ L
  T
Xt=0

PT
α(t) Cλt||x(0)||1 + Ln

t=0 α(t)
t−1

Xs=0

convexity of norm function, we obtain that

n

n

Xj=1
F (ˆx(T )) − F (ˆy(T )) ≤
Xj=1
L|ˆxj(T ) − ˆy(T )| ≤ L
≤
PT
  T
α(t) Cλt||x(0)||1 + Ln
Xt=0
Xs=0

1

t=0 α(t)
t−1

Cλt−sα(s)!! ,

gj(ˆxj (T ))(ˆxj(T ) − ˆy(T ))

where we used Theorem 1 to obtain the last inequality.

Lemma 6: The ergodic average of the sequence generated

by algorithm (2)-(3), satisﬁes

n
t=0 α(t)

(y(0) − x∗)2

T

+

nL2
t=0 α(t)

F (ˆy(T )) − F (x∗) ≤
Xt=0
Xt=0

2L
t=0 α(t)

2PT
PT

+

T

2PT

α(t)2

n

Xi=1

Proof: Using (12), we have that

α(t)

|y(t) − xi(t)|.

(20)

(y(t + 1) − x∗)2 = (y(t) − x∗)2 +

2α(t)

n

−

n

Xi=1

gi(t)(y(t) − x∗).

α(t)2

n2   n
Xi=1

gi(t)!2

(21)

Since |gi(t)| ≤ L, we also have that
gi(t)(y(t) − x∗) = gi(t)(y(t) − xi(t)) + gi(t)(xi(t) − x∗)
≥ gi(t)(y(t) − xi(t)) + fi(xi(t)) − fi(x∗)
≥ −L|y(t) − xi(t)|
+ fi(xi(t)) − fi(y(t)) + fi(y(t)) − fi(x∗).
Using fi(xi(t))−fi(y(t)) ≥ −L|xi(t)−y(t)| in the previous
relation, we obtain
gi(t)(y(t) − x∗) ≥ −2L|y(t) − xi(t)| + fi(y(t)) − fi(x∗).
Using the previous relation and |gi(t)| ≤ L in (21), we have
(y(t + 1) − x∗)2 ≤ (y(t) − x∗)2 + L2α(t)2
4Lα(t)
Xi=1
−

(fi(y(t)) − fi(x∗)) +

Xi=1

2α(t)

Rearranging the terms of the previous relation yields that

n

n

n

n

|y(t) − xi(t)|.

2α(t)

n

Xi=1

(fi(y(t)) − fi(x∗))

n
≤ − (y(t + 1) − x∗)2 + (y(t) − x∗)2 + L2α(t)2
+

4Lα(t)

n

|y(t) − xi(t)|.

n

Xi=1

Taking summation of the previous relation from t = 0 to T
and noting the telescopic cancellation of terms, we obtain

T

2
n

=

+ L2

T

n

2α(t)

Xt=0
α(t) (F (y(t)) − F (x∗))
Xt=0
Xt=0

Xi=1

α(t)2 +

n

T

T

n

4Lα(t)

(fi(y(t)) − fi(x∗)) ≤ (y(0) − x∗)2
Xt=0
t=0 α(t)PT

|y(t) − xi(t)|.

Xi=1

n

1

PT

t=0 α(t)y(t). Using convexity of
Since ˆy(T ) =
the functions along with the previous relation, we obtain (20),
which completes the proof.
Next, we will bound the difference between the objective
function value at {ˆx(t)}∞t=0 and the optimal point.
ated by algorithm (2)-(3), satisﬁes

Theorem 2: The ergodic average of the sequence gener-

n
t=0 α(t)

(y(0) − x∗)2

T

+

α(t)2

2PT

nL2
t=0 α(t)

F (ˆx(T )) − F (x∗) ≤
Xt=0
Xt=0

2PT
PT
  T
α(t) Cλt||x(0)||1 + Ln
Xt=0

2L
t=0 α(t)

Xi=1

α(t)

+

n

T

|y(t) − xi(t)| + L

1

t=0 α(t)

PT

Cλt−sα(s)!! (22)

t−1

Xs=0

Proof: Follows by combining Lemma 5 and Lemma 6.

D. Convergence Rate: Choice of Step size

In this section we characterize the convergence rate of the
algorithm for α(t) = 1√t+1
, t ≥ 0, as the step size sequence.
We will show this choice of step size yields O( log T
) conver-
√T
gence iteration complexity, which is the optimal convergence
rate of any ﬁrst-order optimization method over the class of
convex functions as shown in [28] (see Section 3 of [28] for
general lower complexity bounds).

Theorem 3: Using the ergodic average of the sequence
generated by the proposed algorithm with α(t) = 1√t+1
,
both rate of convergence to the optimal solution and rate of
convergence of estimates to a consensus point are O( log T
).
√T
In particular, with x(0) = 0, for all i, j ∈ V we have

|ˆxi(T ) − ˆxj (T )| ≤ 2LnC

and

4

1 − λ

log T
√T

,

(23)

∗

X
−
n
X

4

log T
√T

F (ˆx(T )) − F (x∗) ≤ n (x∗)2
+ 2L2nC
Proof: With α(t) = 1√t+1
Xt=0

1 − λ
α(t) ≥Z T +2

T

1

T

T

T

1

t−1

√T .

λt−s−1α(s)α(t) = 1 +

dx = 2(√T + 2 − 1) ≥
t=1 α(t)λt ≤ 1
1−λ , and
λi−1
T−i+1
Xj=1


1
√x
Since α(t) ≤ 1, we have PT
Xs=0
Xt=0
λi−1Z T−i+1
Xi=1
≤ 1 +
λi−12(cid:16)log(√T − i + 1 + √T − 1)(cid:17)
Xi=1
1 − λ(cid:18)log 2 +
≤ 1 + 2
for T ≥ 2. Using the previous three relations in Theorem 1,
for any i and T we obtain

Xi=1
√x√x + i

log T(cid:19) ≤

1 − λ

= 1 +

√j√i + j


log T,

1
2

x=0

1

4

1

T

4

1

+ LnC

1
√T

1 − λ||x(0)||1

log T
√T
|ˆxi(T ) − ˆy(T )| ≤ C
This shows that rate of convergence to a consensus point is
O( log T
). In particular, using this relation for i and j and
√T
setting x(0) = 0, we obtain (23). Using the three relations
t=0 α(t)2 ≤ 1 + log(T + 1) ≤

along with Theorem 2 and PT
2 log T (for T ≥ 4), yields

1 − λ

.

+

F (ˆx(T )) − F (x∗)
≤ n (y(0) − x∗)2
+ 2LC

1
2√T
1
√T
1 − λ||x(0)||1
1
√T
1 − λ||x(0)||1

+ LC

1

1

2

nL2

2 log T
√T
+ 2L2nC

+ nL2C

4

1 − λ
4

log T
√T
log T
√T
,

1 − λ

Fig. 1: Sample Network with n = 20 nodes.

−0.045

−0.05

−0.055

−0.06

−0.065

−0.07

−0.075

0

−0.55

−0.6

−0.65

−0.7

−0.75

−0.8

−0.85

∗

X
−

1

X

500

−0.9

0

100

200

400
N umb e r of i t e r at i on s

300

100

200

400
N umb e r of I t e r at i on s

300

500

2 log T
√T

+

1
2√T
4

nL2

2
log T
√T

+ nL2C

1 − λ
, we have

.

(24)

Fig. 2: Performance of algorithm over sample network:
ˆxi(T ) − x∗ for i = 1 and i = 20 as a function of T .

This shows that
solution is O( log T
√T
obtain (24).

the rate of convergence to the optimal
). In particular, setting x(0) = 0, we

V. NUMERICAL RESULTS

2Pn

In this section, we show numerical results based on the
proposed distributed subgradient algorithm to demonstrate
the performance of the algorithm. We consider minimizing
the function F (x) = 1
i=1(x − ai)2 where ai is a scalar
is known only to node i. This problem appears in
that
distributed estimation where the goal
is to estimate the
parameter x∗, using local measurements ai = x∗ + Ni at
each node i = 1, . . . , n. Here Ni represents measurements
noise, which we assume to be jointly Gaussian with mean
zero and variance one. The maximum likelihood estimate is
the minimizer x∗ of F (x). We let n = 20, ai = i, and
consider a directed graph shown in Figure 1. We plot the
differences ˆx1(T ) − x∗ and ˆxn(T ) − x∗ as a function of T
in Figure 2.

VI. CONCLUSION

We considered a multi agent optimization problem where
a directed network of agents collectively solves a global
optimization problem with the objective function given by
the sum of locally known convex functions. We propose
a distributed subgradient algorithm in which each agent
performs local processing based on his local information
and information obtained from his incoming neighbors. In
our algorithm, agents scale the information they send to
others with dynamically updated weights that converge to
balancing weights of the graph and we show this technique
overcomes the asymmetries caused by the directed commu-
nication network. We show that both the rate of convergence
of the objective function to optimal value, and the rate

[26] A. Nedic and A. Ozdaglar. On the rate of convergence of distributed
subgradient methods for multi-agent optimization. In IEEE Conference
on Decision and Control (CDC), 2007.

[27] A. Nedic, A. Ozdaglar, and P. A. Parrilo. Constrained consensus and
optimization in multi-agent networks. IEEE Transactions on Automatic
Control, 2010.

[28] Y. Nesterov. Introductory lectures on convex optimization, volume 87.

Springer Science & Business Media, 2004.

[29] R. Olfati-Saber, J. A. Fax, and R. M. Murray. Consensus and
cooperation in networked multi-agent systems. Proceedings of the
IEEE, 2007.

[30] R. Olfati-Saber and R. M. Murray. Consensus problems in networks
of agents with switching topology and time-delays. IEEE Transactions
on Automatic Control, 2004.

[31] A. Olshevsky and J. N. Tsitsiklis. Convergence speed in distributed
consensus and averaging. SIAM Journal on Control and Optimization,
2009.

[32] A. Priolo, A. Gasparri, E. Montijano, and C. Sagues. A decentralized
In

algorithm for balancing a strongly connected weighted digraph.
American Control Conference (ACC), 2013.

[33] A. Priolo, A. Gasparri, E. Montijano, and C. Sagues. A distributed
algorithm for average consensus on strongly connected weighted
digraphs. Automatica, 2014.

[34] M. Rabbat and R. Nowak. Distributed optimization in sensor networks.
In International symposium on Information processing in sensor
networks, 2004.

[35] S. S. Ram, A. Nedi´c, and V. V. Veeravalli. Distributed stochastic
subgradient projection algorithms for convex optimization. Journal of
optimization theory and applications, 2010.

[36] S. S. Ram, V. V. Veeravalli, and A. Nedic. Distributed non-autonomous
In IEEE

through distributed convex optimization.

power control
INFOCOM, 2009.

[37] B. Recht, C. Re, S. Wright, and F. Niu. Hogwild: A lock-free approach
In Advances in Neural

to parallelizing stochastic gradient descent.
Information Processing Systems, 2011.

[38] A. I. Rikos, T. Charalambous, and C. N. Hadjicostis. Distributed
IEEE Transactions on Control of

weight balancing over digraphs.
Network Systems, 2014.

[39] E. Seneta. Non-negative matrices and Markov chains. Springer, 2006.
[40] W. Shi, Q. Ling, G. Wu, and W. Yin. Extra: An exact ﬁrst-order
algorithm for decentralized consensus optimization. arXiv preprint
arXiv:1404.6264, 2014.

[41] K. I. Tsianos, S. Lawlor, and M. G. Rabbat. Push-sum distributed dual
averaging for convex optimization. In IEEE Conference on Decision
and Control (CDC), 2012.

[42] J. N. Tsitsiklis.

Problems in decentralized decision making and

computation. Technical report, DTIC Document, 1984.

[43] J. N. Tsitsiklis, D. P. Bertsekas, M. Athans, et al. Distributed
asynchronous deterministic and stochastic gradient optimization al-
gorithms. IEEE Transactions on Automatic Control, 1986.

[44] E. Wei and A. Ozdaglar. On the o (1/k) convergence of asynchronous
distributed alternating direction method of multipliers. arXiv preprint
arXiv:1307.8254, 2013.

[45] E. Wei and A. E. Ozdaglar. Distributed alternating direction method
of multipliers. In IEEE Conference on Decision and Control (CDC),
2012.

[46] L. Xiao, S. Boyd, and S.-J. Kim. Distributed average consensus
with least-mean-square deviation. Journal of Parallel and Distributed
Computing, 2007.

[47] K. Yuan, Q. Ling, and W. Yin. On the convergence of decentralized

gradient descent. arXiv preprint arXiv:1310.7063, 2013.

[48] M. Zhu and S. Mart´ınez. On distributed convex optimization under
inequality and equality constraints. IEEE Transactions on Automatic
Control, 2012.

of convergence of the estimates to a consensus point is
O( log T
√T

), where T is the number of iterations.

REFERENCES

[1] A. Agarwal and J. C. Duchi. Distributed delayed stochastic optimiza-

tion. In Advances in Neural Information Processing Systems, 2011.

[2] J. Chen and A. H. Sayed. Diffusion adaptation strategies for distributed
optimization and learning over networks. IEEE Transactions on Signal
Processing, 2012.

[3] J. Cort´es. Distributed algorithms for reaching consensus on general

functions. Automatica, 44(3):726–737, 2008.

[4] O. Dekel, R. Gilad-Bachrach, O. Shamir, and L. Xiao. Optimal
The Journal of

distributed online prediction using mini-batches.
Machine Learning Research, 2012.

[5] A. D. Dominguez-Garcia and C. N. Hadjicostis. Distributed matrix
scaling and application to average consensus in directed graphs. IEEE
Transactions on Automatic Control, 2013.

[6] M. F. Duarte and Y. H. Hu. Vehicle classiﬁcation in distributed sensor

networks. Journal of Parallel and Distributed Computing, 2014.

[7] M. F. Duarte and Y. H. Hu. Vehicle classiﬁcation in distributed sensor

networks. Journal of Parallel and Distributed Computing, 2004.

[8] J. C. Duchi, A. Agarwal, and M. J. Wainwright. Dual averaging for
distributed optimization: convergence analysis and network scaling.
IEEE Transactions on Automatic Control, 2012.

[9] J. C. Duchi, M. I. Jordan, M. J. Wainwright, and Y. Zhang. Opti-
mality guarantees for distributed statistical estimation. arXiv preprint
arXiv:1405.0782, 2014.

[10] B. Gharesifard and J. Cort´es. Distributed continuous-time convex
IEEE Transactions on

optimization on weight-balanced digraphs.
Automatic Control, 2014.

[11] C. N. Hadjicostis and A. Rikos. Distributed strategies for balancing a
weighted digraph. In IEEE Mediterranean Conference on Control &
Automation (MED), 2012.

[12] L. Hooi-Tong. On a class of directed graphs with an application to

trafﬁc-ﬂow problems. Operations Research, 18(1):87–94, 1970.

[13] A. Jadbabaie, J. Lin, and A. S. Morse. Coordination of groups
IEEE

of mobile autonomous agents using nearest neighbor rules.
Transactions on Automatic Control, 2003.

[14] D. Jakovetic, J. Xavier, and J. M. Moura. Fast distributed gradient

methods. IEEE Transactions on Automatic Control, 2014.

[15] B. Johansson, T. Keviczky, M. Johansson, and K. H. Johansson.
Subgradient methods and consensus algorithms for solving convex
optimization problems. In IEEE Conference on Decision and Control
(CDC), 2008.

[16] D. Kempe, A. Dobra, and J. Gehrke. Gossip-based computation
In IEEE Symposium on Foundations of

of aggregate information.
Computer Science, 2003.

[17] D. Lee and M. W. Spong. Stable ﬂocking of multiple inertial agents
on balanced graphs. IEEE Transactions on Automatic Control, 2007.
[18] H. Li and Z. Han. Competitive spectrum access in cognitive radio
networks: graphical game and learning. In Wireless Communications
and Networking Conference (WCNC), 2010.

[19] I. Lobel and A. Ozdaglar. Distributed subgradient methods for convex
optimization over random networks. IEEE Transactions on Automatic
Control, 2011.

[20] I. Lobel, A. Ozdaglar, and D. Feijer. Distributed multi-agent optimiza-
tion with state-dependent communication. Mathematical Program-
ming, 2011.

[21] S. H. Low and D. E. Lapsley. Optimization ﬂow controli: basic
algorithm and convergence. IEEE/ACM Transactions on Networking
(TON), 1999.

[22] I. Matei and J. S. Baras. Performance evaluation of the consensus-
based distributed subgradient method under random communication
topologies.
IEEE Journal of Selected Topics in Signal Processing,
2011.

[23] A. Nedic. Asynchronous broadcast-based convex optimization over a

network. IEEE Transactions on Automatic Control, 2011.

[24] A. Nedic and A. Olshevsky. Distributed optimization over time-
varying directed graphs. In IEEE Conference on Decision and Control
(CDC), 2013.

[25] A. Nedic and A. Olshevsky. Distributed optimization of strongly con-
vex functions over time-varying graphs. In IEEE Global Conference
on Signal and Information Processing, 2013.

