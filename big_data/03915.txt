1

Robust Scene Text Recognition with Automatic Rectiﬁcation

Baoguang Shi, Xinggang Wang, Pengyuan Lv, Cong Yao, Xiang Bai

Huazhong University of Science and Technology
1037 Luoyu Road, Wuhan, Hubei, China 430074

{shibaoguang, wxghust, lvpyuan, yaocong2010}@gmail.com; xbai@hust.edu.cn

6
1
0
2

 
r
a

 

M
2
1

 
 
]

V
C
.
s
c
[
 
 

1
v
5
1
9
3
0

.

3
0
6
1
:
v
i
X
r
a

Abstract

Recognizing texts in natural images is a challenging
task with many unsolved problems. Different from texts
in printed documents, scene texts are often in irregular
arrangements, or are seriously distorted. In this paper,
we propose a recognition method that is robust to ir-
regular texts, such as perspective texts and curved texts.
Speciﬁcally, we construct a deep neural network model,
which consists of a spatial transformer network (STN)
and a sequence recognition network (SRN). The network
can be trained end-to-end using only images and text
labels, thus the spatial transformation is learned using the
supervision of the recognition loss, rather than human-
labeled geometric ground truth. In testing, the image is
ﬁrstly automatically rectiﬁed via a predicted thin-plate-
spline transformation, into a more “readable” image for
the followed SRN. We show that the model is able to read
word images containing several types of irregular texts,
including perspective texts, curved texts and multi-oriented
texts. Requiring minimal labels, the model is easy to train
and deploy in a practical system. The state-of-the-art or
highly competitive performance on several benchmarks for
scene text recognition demonstrates the effectiveness of the
proposed method.

1. Introduction

In natural scenes,

texts appear on various kinds of
objects, e.g. road signs, bill boards and product packaging.
They carry rich, high level semantics that are important
cues for image understanding. Recognizing texts from
images facilitates many real world applications, such as ge-
olocation, driverless car and image-based machine transla-
tor. For these reasons, scene text recognition have attracted
great interests of the computer vision community [25],
[36], [14], [30]. Despite the maturity of the Optical Charac-

Figure 1. Semantic overview of the proposed model. The model
consists of two parts, namely the spatial transformer network
(STN) and sequence recognition network (SRN). The STN
transforms the input image to a new image, which we call the
rectiﬁed image. The SRN takes the rectiﬁed image as the input
and recognizes the text. The two networks can be trained jointly
by back-propagation (represented by dashed arrows).

ter Recognition (OCR) research [23], recognizing texts in
the form of natural images, rather than scanned documents,
is still challenging. Taken by cameras under uncontrolled
environments, text images exhibit large variations due to
several factors of illumination change, blur, fonts, colors,
etc. Besides, there exists several types of irregular texts.
For example, some texts have irregular character place-
ments, such as the curved texts. Some texts are perspective
texts, i.e. texts that are distorted by perspective projections.
Usually, a text recognizer works the best when the input
texts are tightly-bounded, horizontal and frontal. We call
them regular texts. This motivates us to apply a spatial
transformation prior to recognizing the text. In this paper,
we propose a recognition method that is robust to irregular
texts. Speciﬁcally, we construct a deep neural network that
combines a spatial transformer network [17] (STN) and a
sequence recognition network (SRN). An overview of the
network architecture is shown in Fig. 1. The network can
be trained end-to-end, using only word images and their
corresponding text labels.

In the STN part, the input image is spatially transformed
into a new image which we call the rectiﬁed image. This
part can be seen as an image preprocessor. Ideally, given
the input image, the STN produces an image that contains a
piece of regular text, which is a more appropriate input for
a text recognition system than the original input image. The

SpatialTransformerNetworkSequenceRecognitionNetworkRectiﬁed ImageInput Image"MOON"transformation we use is a non-linear thin-plate-spline [6]
(TPS) transformation conﬁgured by a set of ﬁducial points.
The coordinates of the ﬁducial points are regressed from
the input image, using a convolutional neural network.
Due to the ﬂexibility of TPS, we are able to transform
several types of irregular texts to horizontal and frontal
ones, including multi-oriented texts, perspective texts and
curved texts.

The SRN recognizes the rectiﬁed image in a sequence-
to-sequence manner, i.e. from the sequence-based repre-
sentation of the image, to the character sequence. Speciﬁ-
cally, we build the SRN as an attention-based model [4],
which consists of an encoder and a decoder. The en-
coder generates the sequence-based representation from
the image. In our case, it is a network that combines
several convolution layers and a recurrent network. The
decoder is a recurrent sequence generator. At each step
of the generation process, the decoder outputs a character
prediction, based on the encoder output and its stored
memory state. Both the encoder and the decoder leverage
the rich context information on the image. Consequently,
the SRN is able to produce accurate predictions.

We show that, with some proper initialization strategies,
one can train the whole network end-to-end, jointly opti-
mizing the spatial transformer network and the sequence
recognition network. Therefore, in the STN part, we do
not need to label any geometric ground truth, e.g. the
locations of the ﬁducial points, but let it automatically
adjust its weights with respect to the gradients. In practice,
we observe that the training eventually makes the STN tend
to produce frontal and horizontal text images, which are
desirable inputs for the SRN.

The contributions of this paper are as followed: Firstly,
we propose a scene text recognition method that is robust
to irregular texts. Secondly, the proposed network is an
extension to the recently proposed spatial transformer net-
work [17], which was deployed into convolutional neural
networks for object recognition tasks. We further connect
it with the recurrent neural networks. Thirdly, we extend
the attention-based model proposed in [4] with the con-
volution layers, and apply it to the image-based sequence
recognition problem. Finally, we quantitatively validate the
effectiveness of the proposed method with state-of-the-art
performances on several public benchmarks.

2. Related Work

In recent years, a rich body of works concerning scene
text recognition has been published. A comprehensive
review is provided in [39]. Among the traditional methods,
many adopt the bottom-up approaches, where individual
characters are ﬁrstly detected by methods based on sliding
window [35], [34], connected components [25], or Hough
voting [38]. The recognized text is then the integration of

the character predictions. Other methods can be catego-
rized as the top-down approaches, where texts are directly
predicted from the entire image without detecting the
characters. Almázan et al. [2] propose to predict the label
embedding vectors from the the input images. In [16], the
recognition is handled by a 90k-class convolutional neural
network, where each class represents an English word.
In [15], a CNN with structured output layer is used for
unconstrained recognition. Some recent works model the
problem as a sequence recognition problem, where images
and texts are modeled as patch sequences and character
sequences respectively. Su and Lu [32] represent images
by sequences of HOG features, and predict the character
sequence with the recurrent neural network (RNN). Shi
et al. [30] propose an end-to-end neural network that
combines CNN and RNN. Our model also treats the
recognition as sequence recognition problem, but it has a
spatial transformation network which can rectify the input
image before recognition. Additionally, our model is able
to map an input sequence of arbitrary length to the output
sequence, both have arbitrary lengths, and is thus more
ﬂexible than [30].

Irregular text is a usual situation during text detection
and text recognition in photo OCR. Various methods have
been proposed to address this problem. To name a few, in
the text detection stage, Yao et al. [37] ﬁrstly propose the
irregular text problem and provide a solution by carefully
designing rotation invariant low-level image features. In
the text recognition stage. Zhang et al. [41] propose a char
rectiﬁcation method according to the low-rank structure of
text. Phan et al. propose to explicitly rectify the perspective
distortions via SIFT [20] descriptor matching. In all of
the above-mentioned methods, text rectiﬁcation is regarded
as a standalone component
is separated from the
recognition and detection processes. Our method treats
irregular problem from a totally different view, we are
aiming to get a rectiﬁed image, which is more suitable
for neural network to recognize. Thus, text rectiﬁcation
is built together with recognition, and directly guided by
recognition in the training process.
3. Proposed Model

that

In this section we formulate the proposed model. The
model takes an input image I and predicts a sequence L =
{l1, . . . , lT}, where lt represents the t-th character, T is the
text length. The model consists of two parts, the spatial
transformer network (STN) and the sequence recognition
network (SRN).
3.1. Spatial Transformer Network

The STN transforms the input image I to the rectiﬁed
image I(cid:48). It ﬁrst regresses a set of ﬁducial points from
I, then transforms I using the TPS transformation with

respect to the ﬁducial points. The structure of the STN is
illustrated in Fig. 2. The localization network regresses a
set of ﬁducial points, while the grid generator produces a
sampling grid on the input image I. The sampler takes the
sampling grid and the input image I, it produce the new
image I(cid:48).

(right). The base ﬁducial points are evenly distributed
along the top and bottom of the rectiﬁed image I(cid:48). Since
we use normalized coordinates, C(cid:48) is a constant.

Figure 3. Fiducial points and the transformation. The cross mark-
ers on the images are the predicted ﬁducial points C (in green)
and the base ﬁducial points C(cid:48) (in cyan). The transformation T
is illustrated by the pink arrow. For each point (x(cid:48)
i) on I(cid:48), the
transformation T ﬁnds the corresponding point (xi, yi) on I.

i, y(cid:48)

Figure 2. The structure of the spatial transformer network. The
localization network regresses a set of ﬁducial points C, with
which the grid generator produces the sampling grid P. The
sampler produces the rectiﬁed image I(cid:48), given I and P.

A distinctive property of the STN is that the sampler
is differentiable, thus, if we create a differentiable local-
ization network and a differentiable grid generator, the
STN can back-propagate error differentials in the direction
opposite to the arrows in Fig. 2. We can hence integrate
the STN with other networks, and jointly train them.

3.1.1 Localization Network
The localization network regresses the coordinates of the
ﬁducial points, which are denoted by C = [c1, . . . , cK] ∈
(cid:60)2×K, whose k-th column ck = [xk, yk]
contains the
2-D coordinates of the ﬁducial point. We use a normal-
ized coordinate system whose origin is the center of the
image. x, y coordinates are within the range [−1, 1]. The
localization network is a convolutional neural network, it
regresses the coordinates C directly from the input image
I:

(cid:124)

C = CNN(I, θloc)

(1)

where θloc is the parameters of the CNN. The CNN we
use contains four convolution+maxpooling layers, followed
by two fully-connected layers. On its top, we use a tanh(·)
output
layer to constrain the output within the range
(−1, 1), so that ﬁducial points do not fall outside the
image. Being a CNN, the localization network is obviously
differentiable. We jointly train it with other networks, so
that we do not need to label the ﬁducial points C for
training.

To generate the sampling grid on the input image I,
ﬁrstly we calculate the TPS transformation, represented
by T ∈ (cid:60)2×(K+3):

(cid:18)

∆−1
C(cid:48)

(cid:21)(cid:19)(cid:124)

(cid:20) C

(cid:124)
03×2

.

T =

(2)
where ∆C(cid:48) ∈ (cid:60)(K+3)×(K+3) is a matrix determined
only by C(cid:48), thus it is also a constant. The grid of pixels
i}i=1,...,N ,
on the rectiﬁed image I(cid:48) is denoted by P(cid:48) = {p(cid:48)
where p(cid:48)
is the x,y-coordinates of the i-th
pixel, and N is the number of pixels. As illustrated in
Fig. 3, for each point p(cid:48)
i, we ﬁnd the corresponding point
(cid:124)
on the input image I, by applying the
pi = [xi, yi]
transformation:

i = [x(cid:48)

i, y(cid:48)
i]

(cid:124)

i =(cid:2)1, x(cid:48)

pi = Tˆp(cid:48)
ˆp(cid:48)

i

(cid:3)(cid:124)

(3)
(4)

i,K

Here, r(cid:48)

i,1, . . . , r(cid:48)

i, r(cid:48)
i, y(cid:48)
i,k, where di,k is the euclidean
i,k ln d2
i,k = d2
distance between p(cid:48)
i and the k-th base ﬁducial point c(cid:48)
k.
By iterating over all points in the grid P(cid:48), we generate the
grid P = {pi}i=1,...,N . The grid generation involves two
matrix productions, Eq. 2 and Eq. 3. Therefore, the grid
generator is also differentiable.

3.1.3 Sampler
Lastly, in the sampler, the pixel value of I(cid:48) on p(cid:48)
i is bi-
linearly interpolated from the pixels near pi on the input
image. Setting all the pixels in I(cid:48), we get the rectiﬁed
image I(cid:48):

I(cid:48) = V (P, I)

(5)

3.1.2 Grid Generator
The grid generator generates a sampling grid. We ﬁrst
deﬁne another set of ﬁducial points, called the base ﬁducial
K] ∈ (cid:60)2×K. As illustrated in Fig. 3
points C(cid:48) = [c(cid:48)

1, . . . , c(cid:48)

where V represents the bi-linear sampler [17], which is

also a differentiable function.

The ﬂexibility of the TPS transformation allows the
images into rectiﬁed

STN to transform irregular text

Input ImageFiducial PointsLoalizationNetworkSampler VGridGeneratorRectiﬁed ImageInput ImageRectiﬁed Imagedecoder. The encoder produces the sequence-based repre-
sentation for the image I(cid:48), while the decoder generates the
prediction sequence conditioned on the encoder output.

3.2.1 Encoder: Convolution-Recurrent Network
A naïve approach for representing I(cid:48) as a sequence is to
take columns of raw pixels on the image from left to right.
However, raw pixels are noisy and may not be the optimal
choice for representation. Instead, following [30], in the
encoder part, we build a robust and learnable sequence-
based representation, using a network that combines con-
volution layer and recurrent network. As illustrated in 6,
on the bottom of the encoder is several convolution layers.
These layers provide the image representation that is robust
to noise and distortions. The convolution layers output
dconv maps of size Wconv×Hconv. We take out the columns
of these maps, from left to right, resulting in a sequence
with Wconv items, each a feature vector with dconvHconv
dimensions (“map-to-sequence” in Fig. 6).

We further use a two-layer Bi-directional Long-Short
Term Memory (BLSTM) [13], [12] network to model the
long-range dependencies within the sequence. The BLSTM
outputs a sequence, whose length is also Wconv. The
vectors in this sequence capture long-range, bidirectional
context information, and is the strong representation for
the image. We denote the sequence by {x1, . . . , xWconv},
it is the output of the encoder.

encoder output: ot =(cid:80)Wconv

3.2.2 Decoder: Recurrent Character Generator
The decoder is another recurrent neural network. The
decoding is a T -step process: at step t, the decoder ﬁrstly
calculates the alignment weights αt ∈ (cid:60)Wconv. Then a
context vector is calculated as the weighted sum of the
i=1 αtixi. The decoder predicts
a character conditioned on the context vector ot, the last
ground truth character ˆlt−1 (during test,
is the last
character predicted), and the memory state st−1 it stored:

it

st = GRU (ˆlt−1, ot, st−1)
yt = sof tmax(W

st)

(cid:124)

(6)
(7)

Here, GRU stands for Gated Recurrent Unit [7]. Sim-
ilar to LSTM, GRU also has the capability of capturing
long-range dependencies. We choose GRU as the generator
because it has a simpler structure than LSTM. yt is the
predicted label distribution, and W is the classiﬁcation
weights. The decoder needs to generate sequences of vari-
able lengths. Following [33], a special “end-of-sequence”
(EOS) symbol is added to the label space. Once the EOS
symbol is predicted by the decoder, the decoding process
terminates.

The encoder-decoder structure allows us to map a se-
quence of arbitrary length to another sequence of arbitrary

Figure 4. Examples of the spatial transformation. We regress the
ﬁducial points (green crosses) from the input image (left column),
and transform the image into the rectiﬁed image (right column)
using TPS transform. The TPS is able to transform several types
of irregular text images into the rectiﬁed regular text images,
including (a) loosely-bounded text; (b) multi-oriented text; (c)
perspective text and (d) curved text.

images that contain regular texts. Illustrated in Fig. 4,
some common types of irregular texts include (a) loosely-
bounded text, caused by imperfect cropping or bounding
box detection; (b) multi-oriented texts, caused by non-
horizontal shooting; (c) perspective texts, caused by non-
frontal view; (d) curved texts, a common artistic style.
3.2. Sequence Recognition Network

The input to the SRN is the rectiﬁed image I(cid:48). Ideally,
the image contains a piece of text that is written hori-
zontally from left to right. It is natural to model I(cid:48) as
a sequence. Also, texts are inherently sequences of char-
acters. Therefore, we can model the recognition problem
as a sequence-to-sequence [33] learning problem, where
the input sequence is the sequence-based representation of
I(cid:48), and the output sequence is the character predictions
L = {l1, . . . , lT}. Speciﬁcally, the SRN is an attention-
based model [4], [8], which consists of an encoder and a

Figure 5. The SRN recognizes in a sequence-to-sequence manner.
It maps the sequence-based representation of I(cid:48), to the character
sequence. “EOS” is the end-of-sequence symbol.

Input ImageRectiﬁed Image(a)(b)(c)(d)Sequence-BasedRepresentationCharaccterSequence"S""A""L""E"EOSFigure 6. The structure of the sequence recognition network. The
network consists of an encoder and a decoder. The encoder uses
the convolution layers (ConvNet) and the BLSTM to extract a
robust sequence-based representation for the input image. The
decoder generates the prediction sequence conditioned on the
sequence produced by the encoder.

Figure 7. Some initialization patterns for the ﬁducial points.

length, with a differentiable function. With this structure,
we can train the recognition model directly using only the
images and the text labels.

3.3. Network Training

The STN and the SRN can be cascaded into one
network. During training, we minimize the negative log-
likelihood on the training set X :

p(l(i)
t

|I (i), θ)

(cid:88)

T (i)(cid:89)

E =

log

I (i),L(i)∈X

t=1

The superscript (i) denotes the i-th training sample.
θ is a vector that combines all the network parameters.
The network is trained by standard back-propagation [29].
However, in practice, directly training the whole network
with random initialization results in poor performance.
Some special initialization tricks for the STN are neces-
sary. We initialize the STN parameters to make the initially
regressed ﬁducial points on the locations shown in Fig. 7.a.
We have also tested the other two patterns, namely Fig. 7.b
and Fig. 7.c, but end with relatively poorer performance.

Figure 8. Illustration of the decoding with a preﬁx tree. Ω
is the end-of-sequence symbol. Refer to the text for detailed
explanation.

3.4. Decoding

Decoding is the process of converting the sequence of
probability distributions, i.e. {yt}, into the sequence of
characters {lt}. In the case of unconstrained recognition,
we simply take the character with the highest probability
at each t. In the case of constrained recognition, the output
is constrained to a lexicon. For small or medium lexicons,
we calculate the conditional sequence probabilities for
all
lexicon words and pick the one with the highest
probability.

On a very large lexicon, e.g. the Hunspell [1] which
contains more than 50k words, calculating the conditional
probabilities for all
lexicon words would be computa-
tionally expensive. Instead, we propose an approximate
decoding method. First we construct a preﬁx tree for the
lexicon. As illustrated in Fig. 8, the preﬁx tree compactly
represents the lexicon. The decoding process starts from
the root node. At each step, the decoder predicts a distribu-
tion yt conditioned on the previously predicted characters.
The node with the highest conditional probability, among
the child nodes of the current node, is taken as the next
node. When a leaf node, i.e. the EOS node, is reached,
the decoding process terminates. As an example, Fig. 8
shows a part of a preﬁx tree. The blue node “h” is
the current node. The decoder predicts the next-character
probabilities, namely 0.8 for “e” and 0.2 for Ω (end-of-
sequence). Then the node “e” is taken as the next node.
We can further extend this decoding process with a beam-
search scheme, by recording the top-B nodes with the
highest path probabilities, B is the beam width.

4. Experiments

We conduct several experiments to validate the effec-
tiveness of the proposed model, especially on recognizing
irregular texts. First, we evaluate our model on some
general recognition benchmarks. These datasets mainly
consists of frontal and horizontal texts. But still, many
irregular texts exist. Next, we evaluate our model on

Input ImageGRUGRUGRUGRU"M""O""O""N"ConvNetBLSTMDecoderweightedsumGRUEOSMap-to-sequence(a)(b)(c)heot0.80.2tt+1t-1Table 1. Recognition accuracies on general recognition datasets. The “50”, “1k” and “50k” are the lexicon sizes. “Full” lexicon is the
combined lexicon of all images in the dataset. “None” means lexicon-free recognition.

None

SVT

None

IC03

None

IC13
None

Method
ABBYY [34]
Wang et al. [34]
Mishra et al. [22]
Wang et al. [36]
Goel et al. [10]
Bissacco et al. [5]
Alsharif and Pineau [3]
Almazán et al. [2]
Yao et al. [38]
Rodríguez-Serrano et al. [28]
Jaderberg et al. [18]
Su and Lu [32]
Gordo [11]
Jaderberg et al. [16]
Jaderberg et al. [15]
Shi et al. [30]
Ours
Ours (SRN only)

IIIT5k

1k
-
-

57.5

-
-
-
-

-
-

82.1
69.3
57.4

86.6
92.7
89.6
94.4
93.8
92.8

50
24.3

64.1

-

-
-
-
-

-
-

91.2
80.2
76.1

93.3
97.1
95.5
97.6
96.2
96.5

-
-
-
-
-
-
-
-
-
-
-
-
-
-
-

78.2
81.9
79.7

50
35.0
57.0
73.2
70.0
77.3
90.4
74.3
89.2
75.9
70.0
86.1
83.0
91.8
95.4
93.2
96.4
95.5
96.1

-
-
-
-
-

-
-
-
-
-
-
-

78.0

80.7
71.7
80.8
81.9
81.5

50
56.0
76.0
81.8
90.0
89.7

93.1

88.5

96.2
92.0

98.7
97.8
98.7
98.3
97.8

-

-

-

-

Full
55.0
62.0
67.8
84.0

-
-

-

-

-

88.6

80.3

91.5
82.0

98.6
97.0
97.6
96.2
96.4

50k
-
-
-
-
-
-

85.1

-
-
-
-
-
-

93.3
93.4
95.5
94.8
93.7

-
-
-
-
-
-
-
-
-
-
-
-
-

93.1
89.6
89.4
90.1
88.7

-
-
-
-
-

-
-
-
-
-
-
-

87.6

90.8
81.8
86.7
88.6
87.5

some datasets that are specially designed for irregular
text recognition. In these datasets, the majority of the test
images contain irregular texts.

4.1. Implementation Details
Spatial Transformer Network: The localization network
of STN has 4 convolution layers, each followed by a
2 × 2 max-pooling layer. The ﬁlter size, padding size
and stride are 3, 1, 1 respectively, for all convolution
layers. The number of ﬁlters are respectively 64, 128, 256
and 512. Following the convolution and the max-pooling
layers is a fully-connected layers with 1024 hidden units.
The localization network regresses the coordinates of 20
ﬁducial points, meaning that the output of the network is a
vector with 40 dimensions. For all non-linearities we use
the ReLU [24], except the output layer, where tanh(·) is
used, as we have mentioned.
Sequence Recognition Network: For the encoder, we use
7 convolutional layers, similar as the structure proposed
in [31]. The {ﬁlter size, number of ﬁlters, stride, padding
size} for them are {3,64,1,1}, {3,128,1,1}, {3,256,1,1},
{3,256,1,1,}, {3,512,1,1}, {3,512,1,1} and {2,512,1,0}.
The 1st, 2nd, 4th, 6th convolution layers are each followed
by a 2×2 max-pooling layer. On the top of the convolution
layers is a two-layer BLSTM network. Each LSTM has
256 hidden units. For the decoder, we use a GRU that has
256 memory blocks and 37 output units (26 letters, 10
digits, and 1 EOS symbol).
Model Training: We train our model on the 8-million syn-
thetic data released by Jaderberg et al. [14]. No extra data
is used. Many samples in the synthetic dataset are irregular
texts. We train the model with the ADADELTA [40]
optimization method. The batch size is set to 64 in training.
Since most of the tested datasets contain only word images

that have limited lengths, for simplicity, we scale the
images to 100×32 in both training and testing. The output
size of the spatial transformer network is also 100 × 32.
In training, our model processes about 160 samples per
second, and converges in 2 days after about 3 epochs over
the training set.
Implementation: Our method is implemented under the
Torch7 framework [9]. We use the CUDA backend ex-
tensively in our implementation, so that most modules
in our model are GPU-accelerated. Our experiments are
carried out on a workstation with one Intel Xeon(R) E5-
2620 2.40GHz CPU, an NVIDIA GTX-Titan GPU, and
64GB RAM.
Running Speed: The recognition speed depends on the
size of the associated lexicon. Without a lexicon, the model
takes less than 2ms per image. With a lexicon that contains
1k words, the precise decoding scheme takes 100ms per
image. On the 50k-words lexicon of IC03, we adopt the
approximate decoding scheme with the beam width set to
7. The recognition process takes about 200ms per image.

4.2. Results on General Recognition Datasets

Our model is ﬁrstly evaluated on several general recog-
nition benchmarks. These datasets mainly consist of hor-
izontal and frontal texts. But still, irregular texts exist.
Our rectiﬁcation scheme may improve the recognition
performance on these samples. The datasets include:

• IIIT 5K-Words [22] (IIIT5K) contains 3000 cropped
word images in its test set. The images are collected
from the Internet. For each image, there is a 50-
word lexicon and a 1000-word lexicon. The lexicons
contain the ground truth words as well as other
randomly picked words.

• Street View Text [34] (SVT) is collected from the

Google Street View. Its test dataset consists of 647
word images. Many images of SVT are severely cor-
rupted by noise and blur, or have very low resolutions.
Each image is associated with a 50-word lexicon.

• ICDAR 2003 [21] (IC03) contains 251 scene images,
labeled with text bounding boxes. Each image is
associated with a 50-word lexicon deﬁned by Wang
et al. [34]. For fair comparison, we discard images
that contain non-alphanumeric characters or have less
than three characters, following [34]. The resulting
dataset contains 860 cropped images. The lexicons
include the 50-word lexicons, the full lexicon which
combines all lexicon words, and the Hunspell [1] 50k
lexicon.

• ICDAR 2013 [19] (IC13) is the successor of IC03,
from which most of its data is inherited. It contains
1015 cropped text images. No lexicon is associated.

Tab. 1 lists the results produced by our method, and
the previous methods. In the unconstrained recognition
benchmarks (where the lexicon is “None” in Tab. 1), our
model outperforms all the other methods. On IIIT5k, our
outperforms the previous art [30] by nearly 4 percentages,
a clear improvement. We observe that IIIT5k contains a lot
of irregular texts such as curved texts, which explain for
the improvement, since our model rectiﬁes texts before
recognition. On the other datasets, our method achieves
state-of-the-art or highly competitive accuracies. It is worth
mentioning that, although our method falls behind [16]
on some datasets, it differs from [16] in that our model
is not constrained to a particular lexicon or dictionary.
Thus,
is able to recognize random strings such as
telephone numbers. In the constrained cases, our method
also achieves competitive results. On IIIT5k, SVT and
IC03, the lexicon-based recognition accuracies are on par
with [16], and are slightly lower than [30].

it

We also train a model that contains only the SRN.
The recognition accuracies of this model are listed in
the last row of Tab. 1. One can see that the SRN alone
is already a very strong recognizer. Comparing it with
another RNN-based model [30], we see that
the SRN
achieves higher or highly competitive performance on most
of the benchmarks. Moreover, we emphasize that the SRN
is more ﬂexible than the model proposed in [30], in that the
lengths of both the input sequence and the output sequence
are arbitrary. In [30], the input sequence must be longer
than the output sequence, limiting the maximum length of
the the output sequence.

Lastly, by comparing the SRN model with the proposed
STN+SRN model, we observe that, on the majority of the
benchmarks, the STN+SRN model outperforms the SRN
model. This further validates the effectiveness of the STN.

Figure 9. Examples of irregular texts. (a) Perspective texts. The
images contain texts that are taken from a non-frontal view
angle. The images are from the SVT-Perspective [26] dataset;
(b) Curved texts. The characters are placed in a curve or an arc.
The images are from the CUTE [27] dataset.

4.3. Recognizing Perspective Texts

We further evaluate our model on the task of perspective
text recognition, in order to validate the effectiveness of
the rectiﬁcation scheme we adopt. SVT-Perspective [26]
is speciﬁcally designed for evaluating perspective text
recognition algorithms. The dataset is constructed by pick-
ing the side-view angles in Google Street View. Some
examples of this dataset are shown in Fig. 9.a. Most
images in this dataset are heavily distorted by perspective
projections. The dataset consists of 639 cropped images for
testing. Each image is associated with a 50-word lexicon.
The lexicons are inherited from the SVT [34] dataset. In
addition, there is a “Full” lexicon which contains all the
lexicon words.

We evaluate our trained model on this dataset, and
compare the recognition accuracies with other methods.
For [30], we obtain the trained models and test it on this
dataset directly. Our method, together with [30], are eval-
uated with and without lexicons. For the other methods,
including [34], [22], [36], [26], we report the lexicon-based
recognition accuracies reported in [26].

Tab. 2 summarizes the results. In the second and third
columns, we compare the recognition accuracies using
the 50-word lexicon and the full lexicon. Our method
outperforms [26], a perspective text recognition method,
by a large margin on both lexicons. However, that may
be explained by that we use a much larger training set
than [26]. In the comparisons with [30], which uses the
same training set as our method, we still observe signiﬁcant

Table 2. Recognition accuracies on SVT-Perspective [26]. “50”
and “Full” means 50-word lexicon and full lexicon respectively.
“None” means recognition without a lexicon.

Method
Wang et al. [34]
Mishra et al. [22]
Wang et al. [36]
Phan et al. [26]
Shi et al. [30]
Ours

50
40.5
45.7
40.2
75.6
92.6
91.2

Full
26.1
24.7
32.4
67.0
72.6
77.4

None

-
-
-
-

66.8
71.8

improvements, on both the Full lexicon and the lexicon-
free benchmarks. Furthermore, recall the comparisons we
made in Tab. 1, on this dataset we outperforms [30]
by a even larger margin on the lexicon free benchmark.
The reason is that
the SVT-perspective dataset mainly
consists of perspective texts, which may be inappropri-
ate for direct recognition. Our rectiﬁcation scheme can
signiﬁcantly alleviate this problem. In Fig. 10 we make
some qualitative analysis. From the ﬁducial points (green
crosses), we see that the STN tends to place the ﬁducial
points along the upper and lower edges of the text lines,
so that the transformation results in horizontal and frontal
texts. However, it fails sometimes, in the case of heavy
perspective distortion.

Figure 10. Examples showing the rectiﬁcations our model makes,
and the recognition results. The left column is the input image,
where green crosses are the regressed ﬁducial points. The middle
column is the rectiﬁed images, we use gray-scale images for
recognition. The right column is the predictions and the ground
truth texts. Green and red characters are correctly and mistakenly
recognized characters, respectively. The ﬁrst ﬁve rows of images
are from SVT-Perspective [26], the rest are from CUTE80 [27].

Table 3. Recognition accuracies on CUTE80 [26].

Method

Accuracy

Jaderberg et al. [16]
Shi et al. [30]
Ours

42.7
54.9
59.2

4.4. Recognizing Curved Texts

Our model is also able to recognize curved texts, whose
characters are placed along curves or arcs. Curved texts
frequently appear as artistic-style texts in natural scenes.
Due to the irregular character placements, recognizing
curved texts is very challenging. CUTE80 [27] is a dataset
that mainly consists of curved texts. The dataset con-
tains 80 high-resolution images taken in natural scenes.
Originally, the dataset is proposed for detection tasks. We
manually crop the words in the images, resulting in 288
word images for testing. For comparisons, we evaluate the
trained models of [16] and [30]. All methods are evaluated
without any lexicon.

From the results summarized in Tab. 3, we see that
our method outperforms the other two methods by a large
margin. [16] is a constrained recognition model, it cannot
recognize the words that are not in its dictionary. [30]
can recognize arbitrary text strings, but it does not have a
speciﬁc mechanism to handle the curved texts. Our model
tries to rectify the curved texts before recognizing them.
Therefore, it takes advantages on the task of recognizing
curved texts.

In Fig. 10 we show some examples of the rectiﬁcations
our model makes on this dataset. Although the rectiﬁca-
tions are generally not perfect generally, they alleviate the
recognition difﬁculties to some extent. Our method tends
to fail in the case that the angle of the arc is too large,
shown in the last two rows in In Fig. 10.

5. Conclusion

We study a common but difﬁcult problem in scene
text recognition, called irregular texts problem. Traditional
solutions either use a separate text rectiﬁcation component
or overﬁt deep neural networks using a huge number of
synthetic irregular texts. We deal with this problem in a
more feasible and elegant way by adopting a differentiable
spatial transformer network module. In addition, the spatial
transformer network is connected with an attention based
sequence recognizer, for end-to-end training through back-
propagation. The extensive experiment results show that
(1) without geometric supervision, the learned network
can automatically generate more “readable” image for
both human and the sequence recognition network; (2)
the proposed text rectiﬁcation method can signiﬁcantly

restaurant restaurantquiznos    quiznossheraton   sheratonmobil      mobil                                                                                           Pred                                                                                                     GTInput ImageRectified Imagemercato    marcatofootball   footballstaming    starbucksstinker    denverSVT-PerspectiveCUTE80windwin      wyndhamimprove irregular scene text recognition accuracy and
obvious better than the competitors; and (3) the proposed
scene text recognition system is competitive with the state-
of-the-arts. In the future, we plan to extent the proposed
method for irregular text detection and other alignment
tasks, such as face alignment.

[28] J. A. Rodríguez-Serrano, A. Gordo, and F. Perronnin. Label embed-
ding: A frugal baseline for text recognition. IJCV, 113(3):193–207,
2015. 6

[29] D. E. Rumelhart, G. E. Hinton, and R. J. Williams. Neurocomput-
ing: Foundations of research. chapter Learning Representations by
Back-propagating Errors, pages 696–699. MIT Press, 1988. 5

[30] B. Shi, X. Bai, and C. Yao. An end-to-end trainable neural network
for image-based sequence recognition and its application to scene
text recognition. CoRR, abs/1507.05717, 2015. 1, 2, 4, 6, 7, 8

[31] K. Simonyan and A. Zisserman. Very deep convolutional networks
for large-scale image recognition. CoRR, abs/1409.1556, 2014. 6
[32] B. Su and S. Lu. Accurate scene text recognition based on recurrent

neural network. In Proc. of ACCV, 2014. 2, 6

[33] I. Sutskever, O. Vinyals, and Q. V. Le. Sequence to sequence

learning with neural networks. In Proc. of NIPS, 2014. 4

[34] K. Wang, B. Babenko, and S. Belongie. End-to-end scene text

recognition. In Proc. of ICCV, 2011. 2, 6, 7, 8

[35] K. Wang and S. Belongie. Word spotting in the wild. In Proc. of

abs/1212.5701, 2012. 6

[41] Z. Zhang, A. Ganesh, X. Liang, and Y. Ma. TILT: transform

invariant low-rank textures. IJCV, 99(1):1–24, 2012. 2

ECCV, 2010. 2

[36] T. Wang, D. J. Wu, A. Coates, and A. Y. Ng. End-to-end text
recognition with convolutional neural networks. In Proc. of ICPR,
2012. 1, 6, 7, 8

[37] C. Yao, X. Bai, W. Liu, Y. Ma, and Z. Tu. Detecting texts of
arbitrary orientations in natural images. In Proc. of CVPR, 2012. 2
[38] C. Yao, X. Bai, B. Shi, and W. Liu. Strokelets: A learned multi-
scale representation for scene text recognition. In Proc. of CVPR,
2014. 2, 6

[39] Q. Ye and D. S. Doermann. Text detection and recognition in

imagery: A survey. TPAMI, 37(7):1480–1500, 2015. 2

[40] M. D. Zeiler. ADADELTA: an adaptive learning rate method. CoRR,

References

[1] Hunspell. http://hunspell.sourceforge.net/. 5, 7
[2]

J. Almazán, A. Gordo, A. Fornés, and E. Valveny. Word spotting
and recognition with embedded attributes. TPAMI, 36(12):2552–
2566, 2014. 2, 6

[3] O. Alsharif and J. Pineau. End-to-end text recognition with hybrid

hmm maxout models. Proc. of ICLR, 2014. 6

[6]

[4] D. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation
by jointly learning to align and translate. CoRR, abs/1409.0473,
2014. 2, 4

[5] A. Bissacco, M. Cummins, Y. Netzer, and H. Neven. Photoocr:
Reading text in uncontrolled conditions. In Proc. of ICCV, 2013. 6
F. L. Bookstein.
Principal warps: Thin-plate splines and the
decomposition of deformations. TPAMI, 11(6):567–585, 1989. 2

[7] K. Cho, B. van Merrienboer, D. Bahdanau, and Y. Bengio. On
the properties of neural machine translation: Encoder-decoder ap-
proaches. CoRR, abs/1409.1259, 2014. 4
J. Chorowski, D. Bahdanau, D. Serdyuk, K. Cho, and Y. Ben-
CoRR,
gio. Attention-based models for speech recognition.
abs/1506.07503, 2015. 4

[9] R. Collobert, K. Kavukcuoglu, and C. Farabet. Torch7: A matlab-
like environment for machine learning. In BigLearn, NIPS Work-
shop, 2011. 6

[10] V. Goel, A. Mishra, K. Alahari, and C. V. Jawahar. Whole is
greater than sum of parts: Recognizing scene text words. In Proc.
of ICDAR, 2013. 6

[11] A. Gordo. Supervised mid-level features for word image represen-

[8]

tation. In Proc. of CVPR, 2015. 6

[12] A. Graves, A. Mohamed, and G. E. Hinton. Speech recognition
with deep recurrent neural networks. In Proc. of ICASSP, 2013. 4
[13] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural

Computation, 9(8):1735–1780, 1997. 4

[14] M. Jaderberg, K. Simonyan, A. Vedaldi, and A. Zisserman. Syn-
thetic data and artiﬁcial neural networks for natural scene text
recognition. NIPS Deep Learning Workshop, 2014. 1, 6

[15] M. Jaderberg, K. Simonyan, A. Vedaldi, and A. Zisserman. Deep
In

structured output learning for unconstrained text recognition.
Proc. of ICLR, 2015. 2, 6

[16] M. Jaderberg, K. Simonyan, A. Vedaldi, and A. Zisserman. Reading
text in the wild with convolutional neural networks. IJCV, 2015.
2, 6, 7, 8

[17] M. Jaderberg, K. Simonyan, A. Zisserman, and K. Kavukcuoglu.
Spatial transformer networks. CoRR, abs/1506.02025, 2015. 1, 2,
3

[18] M. Jaderberg, A. Vedaldi, and A. Zisserman. Deep features for text

spotting. In Proc. of ECCV, 2014. 6

[19] D. Karatzas, F. Shafait, S. Uchida, M. Iwamura, L. G. i Bigorda,
S. R. Mestre, J. Mas, D. F. Mota, J. Almazán, and L. de las Heras.
ICDAR 2013 robust reading competition. In Proc. of ICDAR, 2013.
7

[20] D. G. Lowe. Distinctive image features from scale-invariant

keypoints. IJCV, 60(2):91–110, 2004. 2

[21] S. M. Lucas, A. Panaretos, L. Sosa, A. Tang, S. Wong, R. Young,
K. Ashida, H. Nagai, M. Okamoto, H. Yamamoto, H. Miyao,
J. Zhu, W. Ou, C. Wolf, J. Jolion, L. Todoran, M. Worring, and
X. Lin. ICDAR 2003 robust reading competitions: entries, results,
and future directions. IJDAR, 7(2-3):105–122, 2005. 7

[22] A. Mishra, K. Alahari, and C. V. Jawahar. Scene text recognition
using higher order language priors. In Proc. of BMVC, 2012. 6, 7,
8

[23] G. Nagy. Twenty years of document image analysis in PAMI.

TPAMI, 22(1):38–62, 2000. 1

[24] V. Nair and G. E. Hinton. Rectiﬁed linear units improve restricted

boltzmann machines. In Proc. of ICML, 2010. 6

[25] L. Neumann and J. Matas. Real-time scene text localization and

recognition. In Proc. of CVPR, 2012. 1, 2

[26] T. Q. Phan, P. Shivakumara, S. Tian, and C. L. Tan. Recognizing
text with perspective distortion in natural scenes. In Proc. of ICCV,
2013. 7, 8

[27] A. Risnumawan, P. Shivakumara, C. S. Chan, and C. L. Tan. A
robust arbitrary text detection system for natural scene images.
Expert Syst. Appl., 41(18):8027–8048, 2014. 7, 8

