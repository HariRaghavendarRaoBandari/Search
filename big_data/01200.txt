6
1
0
2

 
r
a

M
3

 

 
 
]

.

R
P
h
t
a
m

[
 
 

1
v
0
0
2
1
0

.

3
0
6
1
:
v
i
X
r
a

Typical behavior of the harmonic measure in critical

Galton–Watson trees with inﬁnite variance oﬀspring distribution

Shen LIN ∗

Ecole Normale Supérieure, Paris, France

E-mail: shen.lin.math@gmail.com

Abstract

We study the typical behavior of the harmonic measure in large critical Galton–Watson
trees whose oﬀspring distribution is in the domain of attraction of a stable distribution with
index α ∈ (1, 2]. Let µn denote the hitting distribution of height n by simple random walk on
the critical Galton–Watson tree conditioned on non-extinction at generation n. We extend
the results of [10] to prove that, with high probability, the mass of the harmonic measure
carried by a random vertex uniformly chosen from height n is approximately equal to n−λα,
α−1 depends only on the index α. In the analogous continuous
where the constant λα > 1
model, this constant λα turns out to be the typical local dimension of the continuous har-
monic measure. Using an explicit formula for λα, we are able to show that λα decreases with
respect to α ∈ (1, 2], and it goes to inﬁnity at the same speed as
α−1 when α approaches 1.
1
Keywords. size-biased Galton–Watson tree, reduced tree, harmonic measure, uniform mea-
sure, simple random walk and Brownian motion on trees.
AMS 2010 Classiﬁcation Numbers. 60J80, 60G50, 60K37.

Introduction

1
In this paper, we continue our previous work [10] by extending its results to the critical Galton–
Watson trees whose oﬀspring distribution may have inﬁnite variance. To be more precise, let ρ
be a non-degenerate probability measure on Z+ with mean one, and we assume throughout this
paper that ρ is in the domain of attraction of a stable distribution of index α ∈ (1, 2], which
means that

X

k≥0

ρ(k)rk = r + (1 − r)αL(1 − r)

for any r ∈ [0, 1),

(1)

where the function L(x) is slowly varying as x → 0+. The ﬁnite variance condition for ρ is
suﬃcient for the previous statement to hold with α = 2. When α ∈ (1, 2), by classical results
of [7, Chapters XIII and XVII], the condition (1) is satisﬁed if and only if the tail probability

X

k≥x

ρ(k) = ρ([x, +∞))

varies regularly with exponent −α as x → +∞.

Under the probability measure P, for every integer n ≥ 0, we let T(n) be a Galton–Watson
tree with oﬀspring distribution ρ, conditioned on non-extinction at generation n. Conditionally

∗Supported in part by the grant ANR-14-CE25-0014 (ANR GRAAL)

1

n

given the tree T(n), we consider a simple random walk on T(n) starting from the root. The
probability distribution of its ﬁrst hitting point of generation n will be called the harmonic
measure µn, which is supported on the set T(n)
consisting of all vertices of T(n) at generation n.
Let qn > 0 be the probability that a critical Galton–Watson tree T(0) survives up to gen-
eration n. It has been shown by Slack [15] that, as n → ∞, the probability qn decreases as
− 1
α−1 up to multiplication by a slowly varying function, and qn#T(n)
converges in distribution
n
to a non-trivial limit distribution on R+ that we will specify shortly. The following theorem
generalizes Theorem 1 of
Theorem 1. Let Ωn be a random vertex uniformly chosen from T(n)
n . If the oﬀspring distribution
ρ has mean one and belongs to the domain of attraction of a stable distribution of index α ∈ (1, 2],
α−1, which only depends on α, such that for every δ > 0, we have
there exists a constant λα > 1
(2)
lim
n→∞ P

n−λα−δ ≤ µn(Ωn) ≤ n−λα+δ(cid:17) = 1.

[10] from the ﬁnite variance case (α = 2) to all α ∈ (1, 2].

(cid:16)

n

µn

Roughly speaking, the theorem asserts that if we look at a typical vertex at level n of the
conditional Galton–Watson tree T(n), then this random vertex carries with high probability a
mass of order n−λα given by the harmonic measure µn. Since λα > 1
α−1, we see particularly
that the harmonic measure on the conditional Galton–Watson tree is not uniformly spread and
exhibits a fractal behavior.

(cid:16)(cid:8)v ∈ T(n)

: n−βα−δ ≤ µn(v) ≤ n−βα+δ(cid:9)(cid:17) (P)−−−→

The fractal nature of the harmonic measure in critical Galton–Watson tree has been initially
investigated by Curien and Le Gall [4] from another perspective. To compare with Theorem 1,
their main result in the ﬁnite variance case has also been generalized to the present setting as
Theorem 1.1 in [9] : under the same assumption of Theorem 1, we have the existence of another
constant βα ∈ (0,
α−1) only depending on α, such that for every δ > 0, we have the convergence
1
in P-probability
(3)
In other words, with high probability the mass given by the harmonic measure µn to a random
vertex at level n drawn with respect to the same harmonic measure is of order n−βα with
α−1. We can thus say that the harmonic measure µn is mainly supported on a subset of
βα < 1
size approximately equal to nβα, which is much smaller than the whole size of T(n)
n . Notice that
the family (βα, 1 < α ≤ 2) is shown in [9, Theorem 1.3] to be bounded from above in R+.
As the hitting distribution µn of generation n by simple random walk on T(n) is unaﬀected
if we remove the branches of T(n) that do not reach height n, we can simplify the model by
considering merely simple random walk on the reduced tree T∗n associated with T(n), which
consists of all vertices of T(n) that have at least one descendant at generation n.
When the critical oﬀspring distribution ρ has inﬁnite variance, the study of scaling limits of
T∗n goes back to Vatutin [16] and Yakymiv [17]. If we scale the graph distance by the factor
n−1, as n → ∞ the discrete reduced tree n−1T∗n converges to a random continuous tree ∆(α)
that we now describe. For every α ∈ (1, 2], we deﬁne the α-oﬀspring distribution θα as follows.
If α = 2, we let θ2 be the Dirac measure at 2. If α < 2, θα is the probability measure on Z+
given by

n→∞

1 .

n

θα(0) = θα(1) = 0,
θα(k) = α Γ(k − α)

k! Γ(2 − α) = α(2 − α)(3 − α)··· (k − 1 − α)

k!

∀k ≥ 2,

,

2

where Γ(·) denotes the usual Gamma function of Euler. The mean of θα is equal to α
α−1. To
construct the random tree ∆(α), we begin with the genealogical tree ∆(α)
of a family of particles
that evolves in continuous time according to the following rules. At time 0 there is a single
particle, this particle lives for a random time uniformly distributed over [0, 1], then dies and gives
birth to a random number of new particles. This number of oﬀspring is distributed according
to θα and is assumed to be independent of the lifetime of the initial particle. Inductively, all the
particles evolve independently of each other, and each new particle appeared at time t ∈ (0, 1)
dies at a time uniformly distributed over [t, 1] and gives birth to an independent number of new
particles according to the supercritical oﬀspring distribution θα. After taking completion of ∆(α)
0
with respect to its natural intrinsic metric d, we obtain a random compact rooted R-tree ∆(α)
that will be called the reduced stable tree of parameter α. Its boundary ∂∆(α) is composed of
all points of ∆(α) at height 1. We refer to Section 2.1 for a precise deﬁnition of ∆(α).

0

The description above makes clear the recursive structure of ∆(α): let U be a uniform random
variable over [0, 1] and let Nα ∈ N be a random variable of law θα. We take (∆(α)
)i≥1 to be a
sequence of independent copies of ∆(α), and we assume the independence between U, Nα and
, i ≥ 1. If we attach to the top of a single line segment of length U the roots of the rescaled
∆(α)
trees (1 − U)∆(α)
, the resulting tree rooted at the origin of the initial line
segment will have the same distribution as ∆α. See Fig. 1 for an illustration.

1 , . . . , (1 − U)∆(α)

Nα

i

i

Figure 1: Recursive structure of the reduced stable tree ∆(α)

As the continuous analog of simple random walk, Brownian motion on ∆(α) starting from
the root can be deﬁned up until its ﬁrst hitting time of ∂∆(α). It behaves like linear Brownian
motion as long as it stays inside a line segment of ∆(α).
It is reﬂected at the root of ∆(α)
and when it arrives at a branching point, it chooses one of the adjacent line segments with
equal probabilities. We deﬁne the (continuous) harmonic measure µα on ∆(α) as the (quenched)
distribution of the ﬁrst hitting point of ∂∆(α) by Brownian motion.
The behavior of µα is closely related to that of the discrete harmonic measure µn. In order
to state a result analogous with Theorem 1 in the continuous setting, we must ﬁrst make sense
of a “typical” point chosen from the boundary ∂∆(α). To this end, we introduce another (non-
compact) random rooted R-tree Γ(α) endowed with the same branching structure as ∆(α), such
that each point of Γ(α) at height y ∈ [0,∞) corresponds bijectively to a point of ∆(α)
at height
0
1− e−y ∈ [0, 1). It is easy to verify that the resulting new tree Γ(α) is a continuous-time Galton–
Watson tree of branching rate 1 with the supercritical oﬀspring distribution θα. In particular,

3

Height1Height0∅U(1−U)∆(α)1(1−U)∆(α)2(1−U)∆(α)NαΓ(2) is the Yule tree which describes the genealogy of the classical Yule process. By deﬁnition,
the boundary ∂Γ(α) of Γ(α) is the set of all inﬁnite geodesics in Γ(α) starting from the root (these
will be called geodesic rays). Since ∆(α) and Γ(α) share the same branching structure, both
∂∆(α) and ∂Γ(α) can be canonically identiﬁed with a common random subset of NN.
E[#Γ(α)

for the level set of Γ(α) at height r, and we know that

α−1). By a martingale argument, one can deﬁne

For every r > 0, we write Γ(α)

] = exp( r

r

r

W(α) := lim

− r

α−1 #Γ(α)

AsP θα(k)k log k < ∞, the Kesten–Stigum theorem (for continuous-time Galton–Watson trees,
see e.g. [1, Theorem III.7.2]) implies that the previous convergence holds in the L1-sense,
E[W(α)] = 1, and W(α) > 0 almost surely. It follows from Theorem III.8.3 in [1] that

r→∞ e

.

r

h
e−uW(α)i = 1 −

E

u

for any u ∈ (0,∞).

(1 + uα−1) 1
α−1
Moreover, according to Theorem 1 of [15], qn#T(n)
converges in distribution to this martingale
limit W(α) as n → ∞. For every x ∈ Γ(α), we let H(x) denote the height of x in Γ(α), and we
write Γ(α)[x] for the tree of descendants of x in Γ(α), viewed as an inﬁnite random R-tree rooted
at x. For every r > 0, we write Γ(α)
[x] for the level set at height r of the tree Γ(α)[x]. If one
thinks of Γ(α)[x] as a subtree of Γ(α), the set Γ(α)
[x] consists of all the points of Γ(α) at height
r + H(x) that are descendants of x. We similarly deﬁne

n

r

r

W(α)[x] := lim

r→∞ e

− r

α−1 #Γ(α)

[x],

r

which has the same distribution as W(α). The uniform measure ¯ωα on ∂Γ(α) is deﬁned as the
unique probability measure on ∂Γ(α) satisfying that, for every x ∈ Γ(α) and for every geodesic
ray v ∈ ∂Γ(α) passing through x,

¯ωα(B(v, H(x))) = exp(cid:16) − H(x)

α − 1

(cid:17)W(α)[x]
W(α)

,

where B(v, H(x)) stands for the set of all geodesic rays in Γ(α) that coincide with v up to
height H(x). In existing literature, we also call ¯ωα the branching measure on the boundary of
Γ(α). Since ∂∆(α) can be identiﬁed with ∂Γ(α) as explained above, we let ωα be the (random)
probability measure on ∂∆(α) induced by ¯ωα, which will be referred to as the uniform measure
on ∂∆(α). The following result is an extension of Theorem 2 in [10].
Theorem 2. For every α ∈ (1, 2], with the same constant λα as in Theorem 1, we have P-
a.s. ωα(dv)-a.e.

log µα(Bd(v, r))
log ωα(Bd(v, r))

log r

log r

lim
r↓0
lim
r↓0

= λα ,
1

=

α − 1 ,

(4)

(5)

where Bd(v, r) stands for the closed ball of radius r centered at v in the metric space (∆(α), d).

4

By Duquesne and Le Gall [6, Theorem 5.5], the Hausdorﬀ measure of ∂∆(α) with respect
α−1. According to Lemma 4.1 in [11], assertion (5) of the preceding
to d is P-a.s. equal to
1
theorem implies that P-a.s. the uniform measure ωα has the same Hausdorﬀ dimension as the
whole boundary of the reduced stable tree. Meanwhile, taking account of (4), we may interpret
λα as the local dimension of the harmonic measure µα at a typical point of the boundary ∂∆(α).
In contrast, the Hausdorﬀ dimension of the harmonic measure µα is P-a.s. equal to the constant
βα appearing in (3), since Theorem 1.2 of [9] shows that P-a.s. µα(dv)-a.e.,

log µα(Bd(v, r))

log r

lim
r↓0

= βα.

Since βα < 1

α−1 < λα, the set

B =(cid:8)v ∈ ∂∆(α) : lim

r↓0

log µα(Bd(v, r))

log r

(cid:9)

= βα

satisﬁes P-a.s. that µα(B) = 1 whereas ωα(B) = 0.
Corollary 3. For every α ∈ (1, 2], P-a.s. the two measures µα and ωα on the boundary of ∆(α)
are mutually singular.

When α approaches 1, the Hausdorﬀ dimension βα of the harmonic measure µα remains
bounded (see Theorem 1.3 in [9]). At the same time, it is intuitively tempting to think that
α−1 for some positive
the typical local dimension λα of the harmonic measure µα behaves like C
constant C.
Proposition 4. We have that

1 ≤ lim inf
α↓1

(α − 1)λα ≤ lim sup
α↓1

(α − 1)λα < ∞.

Our proof of Proposition 4 relies on the fact that the constant λα appearing in Theorems 1
and 2 can be expressed explicitly with help of the conductance of ∆(α). Informally, if we think
of the random tree ∆(α) as an electric network of resistors with unit resistance per unit length,
the eﬀective conductance C(α) between the root and the boundary ∂∆(α) is a random variable
larger than 1. From a probabilistic point of view, it is the mass under the Brownian excursion
measure for the excursion paths away from the root that hit height 1. Following the deﬁnition
of ∆(α) and its electric network interpretation, the distribution of C(α) satisﬁes the recursive
distributional equation

(cid:18)

C(α)

(d)==

U +

1 − U
2 + ··· + C(α)

Nα

C(α)
1 + C(α)

(cid:19)−1

,

(6)

i

where (C(α)
)i≥1 are i.i.d. copies of C(α), the integer-valued random variable Nα is distributed
according to θα, and U is uniformly distributed over [0, 1]. All these random variables are
supposed to be independent.
Proposition 5. For any α ∈ (1, 2], the constant λα appearing in Theorems 1 and 2 is given by
(7)

λα = E(cid:2)W(α)C(α)(cid:3) − 1.

Moreover, λα is decreasing for all α ∈ (1, 2].

5

The surprisingly simple formula (7) provides a uniﬁed expression of λα in terms of W(α) and
conductance bC(α) of a size-biased version of the reduced stable tree ∆(α).
C(α). We will see in Section 2.9 that the product W(α)C(α) has the same ﬁrst moment as the
Note that for 1 < α ≤ 2, the α-oﬀspring distribution θα is decreasing for the usual stochastic
partial order. This property allows one to construct simultaneously all reduced stable trees
∆(α), α ∈ (1, 2] as a nested family, so that ∆(α2) ⊆ ∆(α1) for all 1 < α1 ≤ α2 ≤ 2 (see Section 2.4
in [9]). While the monotonicity of the typical local dimension λα is aﬃrmed by the previous
result, one question still unanswered is whether the Hausdorﬀ dimension βα of the continuous
harmonic measure µα is also decreasing with respect to α.
The rest of this paper is divided into three parts. The continuous model of Brownian motion
on ∆(α) is studied in Section 2, where we prove Theorem 2, Propositions 4 and 5. Then, we set
up notation and terminology for the discrete setting in Section 3. Finally Section 4 is devoted
to proving of Theorem 1. The arguments are basically adapted from those used in the ﬁnite
variance case. However, we emphasize that several modiﬁcations are indispensable to circumvent
the problem of inﬁnite variance. Instead of repeating some highly analogous reasoning, we refer
the reader to [10] for details that we omit.

2 The continuous setting
2.1 The reduced stable tree
Let us begin with a formal deﬁnition of the reduced stable tree ∆(α) of parameter α ∈ (1, 2].
We set

∞[

n=0

V =

Nn ,

1, . . . , v0

where by convention N = {1, 2, . . .} and N0 = {∅}. If v = (v1, . . . , vn) ∈ V, we set |v| = n
the generation of the vertex v (in particular, |∅| = 0), and if n ≥ 1, we deﬁne the parent of v
as ¯v = (v1, . . . , vn−1) and then say that v is a child of ¯v. For two elements v = (v1, . . . , vn)
) belonging to V, their concatenation is vv0 := (v1, . . . , vn, v0
and v0 = (v0
). The
notions of a descendant and an ancestor of an element of V are deﬁned in the obvious way, with
the convention that every v ∈ V is both an ancestor and a descendant of itself.
An inﬁnite subset Π of V is an inﬁnite discrete (rooted ordered) tree without leaves if there
exists a collection of positive integers kv = kv(Π) ∈ N for every v ∈ V such that
Π = {∅} ∪ {(v1, . . . , vn) ∈ V : vj ≤ k(v1,...,vj−1) for every 1 ≤ j ≤ n}.

1, . . . , v0

m

m

Recall that the generating function of the α-oﬀspring distribution θα is given (see for in-
stance [5, p.74]) as X
For ﬁxed α ∈ (1, 2], we introduce a collection (Kα(v))v∈V of independent random variables
distributed according to θα under the probability measure P, and deﬁne a random inﬁnite discrete
tree

θα(k) rk = (1 − r)α − 1 + αr

∀r ∈ (0, 1].

α − 1

(8)

k≥0

,

Π(α) := {∅} ∪ {(v1, . . . , vn) ∈ V : vj ≤ Kα((v1, . . . , vj−1)) for every 1 ≤ j ≤ n} .

We point out that Π(α) is just a supercritical Galton–Watson tree with oﬀspring distribution θα.
In particular, Π(2) is an inﬁnite binary tree.

6

Let (Uv)v∈V be another collection, independent of (Kα(v))v∈V, consisting of independent
real random variables uniformly distributed over [0, 1] under the same probability measure P.
We set now Y∅ = U∅ and then by induction, for every v ∈ Π(α) diﬀerent from the root,
Yv = Y¯v + Uv(1 − Y¯v). Note that a.s. 0 ≤ Yv < 1 for every v ∈ Π(α). Consider the set

:=(cid:0){∅} × [0, Y∅](cid:1) ∪

∆(α)

0

(cid:18) [

(cid:19)

{v} × (Y¯v, Yv]

.

v∈Π(α)\{∅}
There is a straightforward way to deﬁne a metric d on ∆(α)
R-tree and, for every x = (v, r) ∈ ∆(α)
of the tree ∆(α)

0 when α < 2.

0 , d) is a (non-compact)
0 , we have d((∅, 0), x) = r. See Figure 2 for an illustration

0 , so that (∆(α)

Figure 2: The random tree ∆(α)

0 when 1 ≤ α < 2

The reduced stable tree ∆(α) is the completion of ∆(α)

immediate to see that (∆(α), d) is a compact R-tree, and
0 ∪ ∂∆(α)

∆(α) = ∆(α)

0 with respect to the metric d. It is

where ∂∆(α) := {x ∈ ∆(α) : d((∅, 0), x) = 1} is the boundary of ∆(α), which can be identiﬁed
with a random subset of NN. The point (∅, 0) is called the root of ∆(α). For every x ∈ ∆(α), we
set its height H(x) = d((∅, 0), x). A genealogical order on ∆(α) can be deﬁned by setting x ≺ y
if and only if x belongs to the geodesic path from the root to y.
Conditionally on ∆(α), we can deﬁne Brownian motion (Bt)t≥0 on ∆(α) starting from the
root and up to its ﬁrst hitting time T of ∂∆(α). See e.g. [4, Section 2.1] for the details of
this construction. The harmonic measure µα is the distribution of BT−, which is a (random)
probability measure on ∂∆(α) ⊆ NN.
2.2 The continuous-time Galton–Watson tree
To introduce a new tree sharing the same branching structure as ∆(α), we start with the same
inﬁnite discrete tree Π(α) introduced in Section 2.1. Consider now a collection (Vv)v∈V of in-
dependent real random variables exponentially distributed with mean 1 under the probability

7

Height1Height0Y∅Y3Y1Y2∅12311122122233132measure P. We set Z∅ = V∅ and then by induction, for every v ∈ Π(α) diﬀerent from the root,
Zv = Z¯v + Vv. The continuous-time Galton–Watson tree (hereafter to be called CTGW tree for
short) of stable index α is the set

(cid:18) [

(cid:19)
{v} × (Z¯v, Zv]

,

Γ(α) :=(cid:0){∅} × [0, Z∅](cid:1) ∪

v∈Π(α)\{∅}

which is equipped with the metric d deﬁned in the same way as d in the preceding subsection.
For this metric, Γ(α) is a.s. an inﬁnite R-tree. For every x = (v, r) ∈ Γ(α), we keep the notation
H(x) = r = d((∅, 0), x) for the height of x.
Observe that if U is uniformly distributed over [0, 1], the random variable − log(1 − U) is
exponentially distributed with mean 1. Hence we may and will suppose that the collection
(Vv)v∈V is constructed from the collection (Uv)v∈V in the previous subsection via the formula
Vv = − log(1 − Uv) for every v ∈ V. A homeomorphism from ∆(α)
onto Γ(α) is given by the
0
mapping Ψ deﬁned as Ψ(v, r) := (v,− log(1 − r)) for every (v, r) ∈ ∆(α)
0 .
(cid:17)
(1 − H(Bs))−2 ds

By stochastic analysis, we can write for every t ∈ [0, T),

Ψ(Bt) = W

(cid:16)Z t

(9)

0

where (W(t))t≥0 is Brownian motion with constant drift 1/2 towards inﬁnity on the CTGW
tree Γ(α) (this process is deﬁned in a similar way as Brownian motion on ∆(α), except that it
behaves like Brownian motion with drift 1/2 on every line segment of the tree). It is easy to
see that the Brownian motion W is transient. From now on, when we speak about Brownian
motion on the CTGW tree or on other similar inﬁnite trees, we will always mean Brownian
motion with drift 1/2 towards inﬁnity.
By deﬁnition, the boundary of Γ(α) is the set of all geodesic rays in Γ(α) starting from the
root (∅, 0), and it can be canonically embedded into NN. Due to the transience of Brownian
motion on Γ(α), there is an a.s. unique geodesic ray denoted by W∞ that is visited by (W(t))t≥0
at arbitrarily large times. The distribution of the exit ray W∞ yields a (random) probability
measure να on NN. Thanks to (9), we have in fact να = µα, provided we think of both µα and
να as probability measures on NN.
Inﬁnite continuous trees. We let T be the set of all pairs (Π, (zv)v∈Π) that satisfy the
following conditions:

(i) Π is an inﬁnite discrete tree without leaves in the sense of Section 2.1;
(ii) zv ∈ [0,∞) for all v ∈ Π ;
(iii) z¯v < zv for every v ∈ Π\{∅} ;
(iv) for every v ∈ Π∞ := {(v1, v2, . . .) ∈ NN : (v1, v2, . . . , vn) ∈ Π,∀n ≥ 1}, we have

n→∞ z(v1,...,vn) = ∞.
lim

If (Π, (zv)v∈Π) ∈ T , we consider the associated “tree”

T :=(cid:0){∅} × [0, z∅](cid:1) ∪

(cid:18) [

(cid:19)
{v} × (z¯v, zv]

,

v∈Π\{∅}

8

equipped with the natural distance deﬁned as above. The set Π∞ can be identiﬁed with the
boundary ∂T of the tree T , which is deﬁned as the collection of all geodesic rays in T . We keep
the notation H(x) = r for the height of a point x = (v, r) ∈ T . The genealogical order on T is
deﬁned as previously and again is denoted by ≺. If v = (v1, v2, . . .) ∈ Π∞ and x = (v, r) ∈ T ,
we write x ≺ v if v = (v1, v2, . . . , vk) for some integer k ≥ 0.
When we say that we consider a tree T ∈ T , it means that we are given a pair (Π, (zv)v∈Π)
satisfying the above properties, with T being the associated tree. Clearly, for every α ∈ (1, 2],
the CTGW tree Γ(α) is a random element in T , and we write Θα(dT ) for its distribution.
Let us ﬁx T = (Π, (zv)v∈Π) ∈ T . Under our previous notation, the root ∅ has the oﬀspring
number k∅. We denote by T(1),T(2), . . . ,T(k∅) the subtrees of T rooted at the ﬁrst branching
point (∅, z∅). To be more precise, for every 1 ≤ i ≤ k∅, we deﬁne the shifted discrete tree
Π[i] = {v ∈ V : iv ∈ Π}, and T(i) is the inﬁnite continuous tree corresponding to the pair
(Π[i], (ziv − z∅)v∈Π[i]). Under Θα(dT ), k∅ is distributed according to θα, conditioned on which,
the branching property of the CTGW tree says that the subtrees T(1), . . . ,T(k∅) are i.i.d. following
the same law Θα.
If r > 0, the level set of T at height r is Tr = {x ∈ T : H(x) = r}. If x ∈ Tr, let T [x] denote
the subtree of descendants of x in T . To deﬁne it formally, we write vx for the unique element of
V such that x = (vx, r), then T [x] is the tree associated with the pair (Π[vx], (zvxv − r)v∈Π[vx]).
As usual, Tr[x] stands for the level set at height r of the tree T [x].

As we have seen in the Introduction, the martingale limit
− r
α−1 #Tr

W(α)(T ) = lim

exists Θα(dT )-a.s., andR W(α)(T )Θα(dT ) = 1. For every x ∈ T , we similarly set

r→∞ e

W(α)(T [x]) = lim

r→∞ e

− r
α−1 #Tr[x].

If v ∈ ∂T is a geodesic ray passing through x, let B(v, H(x)) denote the set of geodesic rays
in T that coincide with v up to height H(x). Then Θα(dT )-a.s., the uniform measure ¯ω
on
∂T is the probability measure on ∂T characterized by

(α)
T

T (B(v, H(x))) = exp(cid:16) − H(x)

(α)

¯ω

(cid:17)W(α)(T [x])
W(α)(T )

α − 1

for every x ∈ T and v ∈ ∂T such that x ≺ v.
For a ﬁxed inﬁnite continuous tree T , we deﬁne the harmonic measure µT on ∂T as the
distribution of the exit ray chosen by Brownian motion on T (with drift 1/2 towards inﬁnity).

2.3 The invariant measure
We write

T ∗ := T × NN

for the set of all pairs consisting of a tree T ∈ T and a distinguished geodesic ray v. We deﬁne
is, S(T , v) = (T(v1),ev), where ev = (v2, v3, . . .) and T(v1) is the tree rooted at the ﬁrst branching
a transformation S on T ∗ by shifting (T , v = (v1, v2, . . .)) at the ﬁrst branching point of T , that
point of T that is chosen by v.

The next result extends Proposition 5 in [9] from the Yule tree Γ(2) to the CTGW tree Γ(α).

9

Proposition 6. For all α ∈ (1, 2], the probability measure W(α)(T )Θα(dT )¯ω
invariant under S.
Proof. Fix a bounded measurable function F on T ∗. Under Θα(dT ), the height z∅ of the
distributed according to θα. Recall thatP kθα(k) = α
ﬁrst branching point is exponentially distributed with mean 1, while the oﬀspring number k∅ is

T (dv) on T ∗ is
(α)

α−1 and

W(α)(T ) =

− z∅
α−1W(α)(T(i)).

e

(10)

k∅X

i=1

Using these remarks and the branching property of the CTGW tree, we see that

=

Z

Z
F ◦ S(T , v)W(α)(T )Θα(dT )¯ω
∞X
= (cid:16) ∞X
Z

θα(k) kX
kθα(k)(cid:17) ×(cid:16)Z ∞

k=2
F(T , u)W(α)(T )Θα(dT )¯ω

F(T(i), u)e

k=2

i=1

=

0

− z
α−1 e−zdz

e

(α)
T (dv)
Z

(cid:17) ×

(α)
T (du),

− z∅
α−1W(α)(T(i))Θα(dT | k∅ = k) ¯ω

(α)
T(i)

(du)

F(T , u)W(α)(T ) Θα(dT ) ¯ω

(α)
T (du)

which shows the required invariance.

2.4 The continuous conductance
Recall that the random variable C(α) is deﬁned, from an electric network point of view, as the
eﬀective conductance between the root and the boundary ∂∆(α) in ∆(α). This is equivalent to
deﬁning C(α) as the mass assigned by the excursion measure away from the root for Brownian
motion (with no drift) on ∆(α), to the set of trajectories that reach height 1 before coming
back to the root. This probabilistic deﬁnition of the conductance has the advantage of being
easily generalized to a more general setting. If T is an inﬁnite continuous tree, we deﬁne its
conductance C(T ) between the root and ∂T as the mass assigned by the excursion measure
away from the root for Brownian motion with drift 1/2 on T , to the set of trajectories that go
to inﬁnity without returning to the root. By the correspondence between the CTGW tree Γ(α)
and the reduced stable tree ∆(α), we can verify that C(Γ(α)) has the same law as C(α). See [4,
Section 2.3] for details.
In Section 2.3 of [9], it is shown that for α ∈ (1, 2] the law of C(α) is characterized by the
distributional identity (6) in the class of all probability measures on [1,∞]. Now let us take into
account the martingale limit W(α). From (10) and the branching property of Γ(α), it follows
that the joint distribution of (W(α),C(α)) satisﬁes the distributional equation

(cid:0)W(α),C(α)(cid:1) (d)==

(cid:18)

(1 − U) 1

α−1 (W(α)

1 + ··· + W(α)

Nα

),

(cid:16)

U +

1 − U

C(α)
1 + ··· + C(α)

Nα

(cid:17)−1(cid:19)

,

(11)

,C(α)

in which (W(α)
)i≥1 are i.i.d. copies of (W(α),C(α)), the integer-valued random variable Nα
is distributed according to θα, and U is uniformly distributed over [0, 1]. All these random
variables are supposed to be independent.

i

i

10

For every α ∈ (1, 2], we deﬁne under P a positive random variable bC(α) that has the same law
as C(T ) under the probability measure W(α)(T )Θα(dT ). We shall see in Section 2.9 that bC(α) is
Lemma 7. The random variable bC(α) satisﬁes the distributional identity

distributed as the conductance of a size-biased version of the CTRW tree Γ(α), or equivalently,
as the conductance of a size-biased version of the reduced stable tree ∆(α).

(cid:18)

bC(α)

(d)==

Vα +

(cid:19)−1

bC(α) + C(α)

1 − Vα
2 + ··· + C(α)bNα

,

(12)

where in the right-hand side the random variable Vα has density function α

[0, 1], the integer-valued random variable bNα has the size-biased distribution of θα, and (C(α)
α−1(1 − x) 1
are i.i.d. copies of C(α). All these random variables Vα, bNα, bC(α) and (C(α)
Proof. The law of bC(α) is determined by E[g(bC(α))] = E[W(α) g(C(α))] for every nonnegative
measurable function g on R+. Using (11) under the same notation, we have
1 − U

α−1 over
)i≥2
)i≥2 are independent.

(cid:17)−1(cid:19)#

(cid:18)(cid:16)

i

i

E

α−1 (W(α)

(1 − U) 1

"
g(bC(α))i = E
h
"
Nα(1 − U) 1
= E
"
(1 − U) 1
E[Nα]E

g(bC(α))i = 1

α−1W(α)
1

h

E

C(α)
1 + ··· + C(α)
Deﬁning bNα as the size-biased version of Nα, we see that
(cid:18)(cid:16)

1 − U

Nα

α−1W(α)
1

g

U +

1 + ··· + W(α)

Nα

) g

U +

(cid:18)(cid:16)

U +

g

C(α)
1 + ··· + C(α)

Nα

(cid:17)−1(cid:19)#

.

1 − U

(cid:17)−1(cid:19)#

.

α−1 ]−1. The statement of the lemma thus follows if we let

1 + ··· + C(α)bNα
C(α)
)i≥2, satisfying

(cid:21)

α−1 f(U)

(13)

α−1 = E[(1 − U) 1

Vα be a random variable independent of bNα, bC(α) and (C(α)
Recall that E[Nα] = α
(cid:20) α
α − 1(1 − U) 1

E[f(Vα)] = E

i

for every nonnegative measurable function f.

(cid:18)

(cid:19)−1

We now discuss some similar properties of bC(α). For every v ∈ (0, 1), n ≥ 2, x ∈ [1,∞) and
The law γα of the conductance C(α) has been studied at length in Proposition 2.1 of [9].
(ci)i≥2 ∈ [1,∞)N, we deﬁne

G(v, n, x, (ci)i≥2) :=

v +

1 − v

x + c2 + ··· + cn

,

(14)

so that (12) can be reformulated asbC(α) (d)= G(Vα, bNα, bC(α), (C(α)
where Vα, bNα, bC(α), (C(α)
[1,∞] and let bΦα : M → M map a probability distribution σ to
bΦα(σ) = Law(cid:0)G(Vα, bNα, X, (C(α)
)i≥2)(cid:1)
where X is distributed according to σ and independent of Vα, bNα, (C(α)

i

i

(15)
)i≥2 are as in (12). Let M be the set of all probability measures on

)i≥2)

i

)i≥2.

i

11

α

Proposition 8. We ﬁx the parameter α ∈ (1, 2].
(1) The distributional equation (12) characterizes the law bγα of bC(α) in the sense that, bγα is
the unique ﬁxed point of the mapping bΦα on M , and for every σ ∈ M , the k-th iterate
bΦk
(σ) converges to bγα weakly as k → ∞.
(2) The law bγα has a continuous density over [1,∞).
(3) If 1 < α < 2, only the ﬁrst moment of bC(α) is ﬁnite, while all moments of bC(2) are ﬁnite.
(4) For any monotone continuously diﬀerentiable function g : [1,∞) → R+, we have
g(bC(α))i = α
2 + ··· + C(α)bNα

g(bC(α) + C(α)

h
α − 1 E

h
α − 1E

, (16)

hbC(α)(bC(α) − 1)g0(bC(α))i + α
where bC(α), bNα, and (C(α)

E

)i≥2 are as in (15).

)i

i

(5) We deﬁne, for all ‘ ≥ 0, the Laplace transforms ϕα(‘) := E[exp(−‘C(α)/2)] and

bϕα(‘) = E[exp(−‘ bC(α)/2)] :=
Then bϕα solves the linear diﬀerential equation
2‘ ψ00(‘) + ‘ ψ0(‘) − α

Z ∞

1

e−‘x/2bγα(dx).

α − 1(1 − ϕα(‘))α−1ψ(‘) = 0.

(17)

Proof. The results for α = 2 are stated without proof in [10]. The arguments given below is

similar in spirit to that of Proposition 2.1 in [9], but due to the fact that bNα has an inﬁnite
mean for α < 2, sharper estimates are necessary in the present setting.
First of all, the stochastic partial order (cid:22) on M is deﬁned by saying that σ (cid:22) σ0 if and
α ∈ [1, 2], the mapping bΦα is increasing for the stochastic partial order. We endow the set M1 of
only if there exists a coupling (X, Y ) of σ and σ0 such that a.s. X ≤ Y . It is clear that for any
all probability measures on [1,∞] that have a ﬁnite ﬁrst moment with the 1-Wasserstein metric

d1(σ, σ0) := inf(cid:8)E(cid:2)|X − Y |(cid:3): (X, Y ) coupling of (σ, σ0)(cid:9).

Let us show that bΦα maps M1 into M1 for any α ∈ (1, 2]. For any a, b > 0 and 0 < r < 1,

The metric space (M1, d1) is Polish and its topology is ﬁner than the weak topology on M1.
the weighted harmonic-geometric means inequality says that

We ﬁx some r ∈ (2 − α, 1) and apply the preceding inequality to obtain

(cid:18)

Vα +

X + C(α)

1 − Vα
2 + ··· + C(α)bNα
(cid:20)

1

E

(Vα)r(1 − Vα)1−r

a

(cid:16) r
+ 1 − r
(cid:19)−1 ≤(cid:16) r

b

Vα

(cid:17)−1 ≤ arb1−r.
(cid:17)r(cid:16) 1 − r
1 − Vα

(cid:17)1−r(cid:0)X + C(α)

2 + ··· + C(α)bNα

(cid:1)1−r.

(cid:21)

= E

(cid:20) α
α − 1(1 − U) 1
12

α−1

(cid:21)

1

U r(1 − U)1−r

It suﬃces to show that the right-hand side of the last display has a ﬁnite ﬁrst moment provided
that X has a ﬁnite mean. Notice that by (13),

is ﬁnite. On the other hand, since a.s. X ≥ 1 and C(α)

h(cid:0)X + C(α)

E

2 + ··· + C(α)bNα

(cid:1)1−ri ≤ E
= α − 1

i ≥ 1 for all i ≥ 2,
" X + C(α)
2 + ··· + C(α)bNα
(cid:1)r
(cid:0)bNα
X
kθα(k)

E[X] + E(cid:2)C(α)

(cid:16)

#

2

α

k≥2

kr

(cid:3) + ··· + E(cid:2)C(α)

k

(cid:3)(cid:17)

. (18)

i

i

)i≥2) has a ﬁnite ﬁrst moment.

formula that θα(k) = O(k−(1+α)) when k → ∞. As r > 2− α, it follows thatP k2−rθα(k) < ∞.
It is shown in [9, Proposition 2.1] that C(α) has a ﬁnite mean. Meanwhile, we know by Stirling’s
The sum in (18) is ﬁnite and therefore G(Vα, bNα, X, (C(α)
Next, we observe that bΦα is strictly contractant on (M1, d1). To see this, let (X, Y ) be a
coupling between σ, σ0 ∈ M1 under the probability measure P, and assume that Vα, bNα, (C(α)
and (X, Y ) are independent under P. Then G(Vα, bNα, X, (C(α)
)i≥2) and G(Vα, bNα, Y, (C(α)
provide a coupling of bΦα(σ) and bΦα(σ0). Using the fact that a.s. X, Y,C(α)
i ≥ 1, we have
)i≥2)(cid:12)(cid:12)(cid:12)
1 − Vα
2 + ··· + C(α)bNα
2 + ··· + C(α)bNα

(cid:12)(cid:12)(cid:12)G(Vα, bNα, X, (C(α)
)i≥2) − G(Vα, bNα, Y, (C(α)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
(cid:18)
1 − Vα
2 + ··· + C(α)bNα
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
X + C(α)
(cid:0)Vα
(cid:0)X + C(α)
2 + ··· + C(α)bNα
(1 + (bNα − 1)Vα)2|X − Y |.
(cid:21)
(cid:20)

(cid:18)
(cid:19)−1 −
Vα +
(cid:1) + 1 − Vα
(cid:1)(cid:0)Vα

(cid:19)−1(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
(cid:1) + 1 − Vα

(cid:0)Y + C(α)

(1 − Vα)(X − Y )

By deﬁnition,

)i≥2
)i≥2)

Y + C(α)

1 − Vα

Vα +

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:21)

(cid:1)

≤

=

i

i

=

i

i

1 − Vα

(1 + (bNα − 1)Vα)2

E

(1 + (k − 1)Vα)2

X

(cid:20)
kθα(k) E

1 − Vα
(cid:21)
(cid:21)

(cid:20) k(1 − U) α
k≥2
θα(k) E
(cid:20)
θα(k) E

α−1
(1 + (k − 1)U)2

(1 + (k − 1)U)2

k(1 − U)

α

k≥2

= α − 1
= X
≤ X
(cid:21)

k≥2

13

where U is uniformly distributed over [0, 1]. Notice that for for any integer k ≥ 2,

(cid:20)

k(1 − U)

E

(1 + (k − 1)U)2

= 1 + k − 1 − k log k

(k − 1)2

< 1.

Thus, taking expectation and minimizing over the couplings between σ and σ0, we get

(cid:20)
d1(bΦα(σ),bΦα(σ0)) ≤ E
(cid:18)

≤

(cid:21)
(1 − Vα)
d1(σ, σ0)
h Nα − 1 − Nα log Nα

(1 + (bNα − 1)Vα)2
1 + E
(Nα − 1)2

i(cid:19)

d1(σ, σ0) =: cα d1(σ, σ0)

with cα < 1. So for α ∈ (1, 2], the mapping bΦα is contractant on M1 and by completeness it has
a unique ﬁxed point eγα in M1. Furthermore, for every σ ∈ M1, we have bΦk
(σ) → eγα for the
metric d1, hence also weakly, as k → ∞. However, bγα is also a ﬁxed point of bΦα according to
(15). The equality bγα = eγα will follow if we can verify that eγα is the unique ﬁxed point of bΦα
in M . To this end, it will be enough to show that bΦk
(σ) →eγα as k → ∞, for every σ ∈ M .
Let us apply bΦα to the Dirac measure δ∞ at inﬁnity to see that

α

α

bΦα(δ∞) = Law(cid:0)eV −1
 (cid:18)
bΦ2

α

(cid:1) ,

α

Vα +

(δ∞) = Law

1 − Vα
2 + ··· + C(α)bNα
where eVα is a copy of Vα being independent of Vα, bNα and (C(α)
bΦ2
(δ∞) has a ﬁnite ﬁrst moment. In fact, applying (13) yields
"(cid:18)
(cid:18)

eV −1
α + C(α)

(cid:19)−1#

α

i

Vα +

E

eV −1
α + C(α)

1 − Vα
2 + ··· + C(α)bNα

(cid:19)−1!

,

)i≥2 under P. We claim that

(cid:19)−1#

1 − U
2 + ··· + C(α)bNα
(cid:19)−1#
(cid:19)−1#

.

α

α−1

k≥2

(cid:18)

U +

U +

U +

"(cid:18)

α
α − 1

kθα(k)E

"
= E
"
≤ E
= X
(cid:19)−1#

eV −1
α − 1(1 − U) 1
α + C(α)
1 − U
eV −1
2 + ··· + C(α)bNα
α + C(α)
eV −1
α + C(α)
" log(cid:0)eV −1
1 − (eV −1
hlog(cid:0)eV −1
i + E

1 − U
2 + ··· + C(α)
(cid:1)
2 + ··· + C(α)
(cid:1)i
2 + ··· + C(α)
2 + ··· + C(α)
(cid:1)i

+ C(α)
α + C(α)
+ C(α)
a + log b for a, b ≥ 1, we see that

hlog(cid:0)C(α)

2 + ··· + C(α)

heV −1/2

(cid:1)i ≤ E

= E
≤ 2 E

)−1

α

α

α

k

k

k

k

k

.

k

#

.

We integrate with respect to U to obtain

"(cid:18)

U +

E

eV −1
α + C(α)

1 − U
2 + ··· + C(α)

Using the inequality log(a + b) ≤ √

hlog(cid:0)eV −1
The random variable eV
log a ≤ 1

E

α

+ C(α)

2 + ··· + C(α)
−1/2
α

k

r ar for every a ≥ 1, and thus

has clearly a ﬁnite mean. Meanwhile, for any r ∈ (0, 1), we have

E

hlog(cid:0)C(α)
X

2 + ··· + C(α)

k

2 + ··· + C(α)

k

2 + ··· + C(α)

k

(cid:1)i ≤ 1

r E

h(cid:0)C(α)
(cid:1)i ≤ 1

hC(α)

r E

(cid:1)ri ≤ 1
hC(α)

kθα(k)E

2 + ··· + C(α)

k

If we take r < α − 1, by the same arguments following (18) we deduce that

hlog(cid:0)C(α)
X
kθα(k)E
(δ∞) ∈ M1. By monotonicity, bΦ2
Therefore, we get bΦ2
the preceding results we have bΦk
unique ﬁxed point of bΦα in M .

2 + ··· + C(α)

k≥2

k≥2

r

α

α

k

(σ) → eγα for every σ ∈ M . This implies that bγα = eγα is the
(σ) ∈ M1 for every σ ∈ M , and from

α

ir

.

ir

< ∞.

(19)

14

α

For every t ∈ [1,∞) we set bFα(t) := P(bC(α) ≥ t), and

1 + C(α)

2 + ··· + C(α)

k ≥ t)
for every integer k ≥ 2. It follows from (12) that, for every t > 1,
(cid:19)
1 − Vα
≤ 1
2 + ··· + C(α)bNα
(cid:17)
(cid:16) t − vt
α−1 bF (k)
dv (1 − v) 1
1 − vt
E(cid:2)NαbF (Nα)

bF (k)
(t) := P(bC(α)
(cid:18)
bFα(t) = P
Vα +
∞X
= (cid:16) t − 1

bC(α) + C(α)
Z 1/t
kθα(k)
α−1Z ∞
(cid:17) α

dx

k=2

α−1

=

x

0

α

t

1

(x)(cid:3),

t

t

where Nα is distributed according to θα. Since bF
(cid:17) α

obtain from the last display that

bFα(t) = 1 −(cid:16) t − 1

α−1

α

(x − 1) 2α−1
α−1
α (t) = 1 for every t ∈ [1, 2] and k ≥ 2, we
(k)

(20)

(α)
0 ,

∀t ∈ [1, 2],

(21)

A

t

Z ∞

1

A

(α)
0

where

:= 2 α

α−1 −

E(cid:2)NαbF (Nα)
From (20) and (21), we observe that bFα is continuous on [1,∞). Recall that C(α) has a continuous
density (see [9, Proposition 2.1]). It follows that all functions bF
[1,∞). By dominated convergence the function x 7→ E[NαbF
α , k ≥ 2 are continuous on
(k)
Using (20) again we obtain that bFα is continuously diﬀerentiable on [1,∞) and consequentlybγα
(x)] is also continuous on [1,∞).

(x)(cid:3) ∈ [1, 2 α

(x − 1) 2α−1
α−1

α−1 ).

(Nα)
α

dx

α−1

x

2

α

has a continuous density on [1,∞).

To derive the identity (16), we ﬁrst diﬀerentiate (20) with respect to t to see that

holds for t ∈ (1,∞). Let g : [1,∞) → R+ be a monotone continuously diﬀerentiable function.
We multiply both sides of the last display by g0(t) and integrate for t from 1 to ∞ to obtain

)(cid:3),

2 + ··· + C(α)

Nα

(22)

α

dt

− α

(t)(cid:3)

t(t − 1)dbFα(t)

E(cid:2)bC(α)(bC(α) − 1)g0(bC(α))(cid:3) + α

α − 1bFα(t) = −E(cid:2)NαbF (Nα)
α − 1E(cid:2)g(bC(α))(cid:3) = E(cid:2)Nα g(bC(α) + C(α)
α − 1E(cid:2)bNα − 1(cid:3)E(cid:2)C(α)(cid:3) .
(cid:3) = ∞ for every α ∈ (1, 2), the second moment of bC(α) is inﬁnite.

E(cid:2)(bC(α))2(cid:3) = E(cid:2)bC(α)(cid:3) + α

As E(cid:2)bNα

which gives (16). When g(x) = exp(−x‘/2) for ‘ > 0, we obtain (17) by using the generating
function of Nα given in (8). Finally, taking g(x) = x in (16) yields

15

Z

2.5 Proof of Theorem 2
The following proof of Theorem 2 for all α ∈ (1, 2] proceeds in much the same way as in the
case α = 2. We skip some details, for which we refer the reader to Section 2.5 in [10].

First, we have seen in Proposition 6 that the probability measure W(α)(T )Θα(dT )¯ω

(α)
T (dv)
on T ∗ is invariant under the shift S. Taking into account that W(α)(T ) > 0, Θα(dT )-a.s., we
can verify, in a similar fashion as in [9, Proposition 2.6], that the shift S acting on the probability
space (T ∗,W(α)(T )Θα(dT )¯ω

Let Hn(T , v) denote the height of the n-th branching point on the geodesic ray v. For every
i=0 H1 ◦ Si where Si stands for the i-th iterate of the shift S. It follows

n ≥ 1, it holds Hn =Pn−1

(α)
T (dv)) is ergodic.

from the ergodic theorem that W(α)(T )Θα(dT )¯ω

T (dv)-a.s. and thus Θα(dT )¯ω
(α)

(α)
T (dv)-a.s.

1
n

Hn −→
n→∞

H1(T , v)W(α)(T ) Θα(dT )¯ω

(α)
T (dv).

(23)

One calculates as in the proof of Proposition 6 to get

H1(T , v)W(α)(T ) Θα(dT )¯ω

(24)
For a ﬁxed geodesic ray v = (v1, v2, . . .), we write xn,v = ((v1, . . . , vn), Hn+1(T , v)) for the
T ({u ∈ ∂T : xn,v ≺ u}). Using the
(α)

n + 1-st branching point on v, and we set Fn(T , v) := log ¯ω
ergodic theorem again, we have Θα(dT ) ¯ω

(α)
T (dv) =

z∅W(α)(T ) Θα(dT ) = α − 1

(α)
T (dv)-a.s.,

α

.

Z

1
n

Fn −→
n→∞
where the limit is identiﬁed with
− 1
α − 1

(α)
T (dv),

F1(T , v)W(α)(T ) Θα(dT )¯ω
Z

z∅W(α)(T ) Θα(dT ) = −α−1.

The calculation can be done analogously as in display (18) of [10]. We only point out that

we need R W(α)(T )| log W(α)(T )| Θα(dT ) < ∞, which is true because P θα(k)k(log k)2 < ∞

(cf. Theorem I.10.2 in [1]). By considering the ratio Fn/Hn and taking n → ∞, we derive that
Θα(dT )-a.s. ¯ω

(α)
T (dv)-a.e.,

(25)

lim
r→∞

1
r

log ¯ω

(α)

T (B(v, r)) = − 1

α − 1 ,

from which the convergence (5) readily follows.

Then we turn to the harmonic measure µT and set Gn(T , v) := log µT ({u ∈ ∂T : xn,v ≺ u}).
Using the ﬂow property of µT (see Lemma 2.3 in [9]) and the ergodic theorem, we obtain the
Θα(dT )¯ω

(α)
T (dv)-almost sure convergence

G1(T , v)W(α)(T ) Θα(dT )¯ω

(α)
T (dv).

(26)

We can evaluate the preceding limit as

Z

Z

Z

1
n

Gn −→
n→∞

Z k∅X
= α − 1

i=1

α

Z

− z∅
α−1W(α)(T(i)) log
e

k∅W(α)(T(1)) log

C(T(i))

C(T(1)) + ··· + C(T(k∅)) Θα(dT )
C(T(1)) + ··· + C(T(k∅)) Θα(dT ).

C(T(1))

16

Putting (26) together with (23) and (24), we see that Θα(dT )-a.s. ¯ω
C(T(1))

Z

log µT (B(v, r)) =

lim
r→∞

1
r

C(T(1)) + ··· + C(T(k∅)) Θα(dT ).

(α)
T (dv)-a.e.,

which implies that the convergence (4) holds with the limit

k∅W(α)(T(1)) log
"
λα := E

log C(α)

NαW(α)
1

#

,

(27)

1 + ··· + C(α)

Nα

C(α)
1

where in the right-hand side we keep the same notation as in (11).

Finally it remains to check that λα deﬁned by (27) satisﬁes that

α−1 < λα < ∞. Since
1

1 ≥ 1, an upper bound for λα is
C(α)
h

E
which equals E[Nα log(1+C(α)
2 +···+C(α)
] = 1. For
any ﬁxed r ∈ (0, α − 1), there exists a positive constant M < ∞ such that log(1 + x) ≤ M + xr
for every x > 0. For similar reasons as in (19), we have

log(cid:0)1 + C(α)
)] by independence and the fact that E[W(α)
1

2 + ··· + C(α)

NαW(α)
1

(cid:1)i

Nα

Nα

,

λα ≤ X

k≥2

kθα(k)E

h log(cid:0)1 + C(α)
2 + ··· + C(α)
(cid:18) k∅X

Z

k

On the other hand, from (24) and the display following (26) we also know that
W(α)(T(i)) log C(α)(T(1)) + ··· + C(α)(T(k∅))

− z∅
α−1
e

(cid:19)
Θα(dT ).

λα = α
α − 1

C(α)(T(i))

i=1

(cid:1)i ≤ αM
α − 1 +(cid:0)E(cid:2)C(α)(cid:3)(cid:1)rX

k≥2

θα(k)k1+r < ∞. (28)

By concavity of the logarithm,

k∅X

W(α)(T(i)) log C(α)(T(1)) + ··· + C(α)(T(k∅))

C(α)(T(i))

i=1
This inequality is strict if and only if for some i ∈ {1, . . . , k∅},

i=1

W(α)(T(i)) log W(α)(T(1)) + ··· + W(α)(T(k∅))

.

W(α)(T(i))

≥ k∅X

Since the latter property holds with positive probability under Θα(dT ),

W(α)(T(i))

W(α)(T(1)) + ··· + W(α)(T(k∅)) 6=
Z

(cid:18) k∅X

− z∅
α−1
e

C(α)(T(i))

C(α)(T(1)) + ··· + C(α)(T(k∅)) .
(cid:19)

W(α)(T(i)) log W(α)(T(1)) + ··· + W(α)(T(k∅))

Θα(dT )

W(α)(T(i))

i=1

λα >

α
α − 1
= − α
α − 1

Z

(α)
T (dv).
By (25), the right-hand side of the last display is equal to 1

F1(T , v)W(α)(T ) Θα(dT ) ¯ω

α−1. Therefore, we have λα > 1
α−1.

17

2.6 Proof of Proposition 5

To verify that λα = E[W(α)C(α)] − 1 = E[bC(α)] − 1, we take g(x) = log(x) in (22) to see that

h
E(cid:2)W(α)C(α) − 1(cid:3) = E
NαW(α)
"
1
NαW(α)
= E
1

log(cid:0)C(α)
log C(α)

(cid:1)i − α
1 + ··· + C(α)
#
1 + ··· + C(α)

Nα

Nα

.

α − 1E

hW(α)

1

i

log C(α)
1

C(α)
1

prove that the law bγα of the size-biased conductance decreases for α ∈ (1, 2] in the sense of the
Let us show the monotonicity of λα to complete the proof of Proposition 5. Indeed, we will
usual stochastic (partial) order. Observe that the function G(v, n, x, (ci)i≥2) deﬁned in (14) is

• decreasing with respect to v ,
• increasing with respect to n ,
• increasing with respect to ci for every i ≥ 2.

Since P(Vα > x) = (1 − x) α
respect to α ∈ (1, 2] for the stochastic order. Meanwhile, we deduce from (8) that

α−1 for every x ∈ [0, 1], the random variable Vα is increasing with

∀r ∈ (0, 1].

(29)

Putting it into the identity

and recalling that a.s. bNα ≥ 2, we get

k=1

∞X

E(cid:2)rbNα(cid:3) = r − r(1 − r)α−1,
rk−1 P(cid:0)bNα ≥ k(cid:1) =
∞X

rk P(cid:0)bNα > k(cid:1) = 1 − E[r
rk−2 P(cid:0)bNα ≥ k(cid:1) = (1 − r)α−2 − 1 ,

∞X

k=0

1 − r

ˆNα]

which implies that for every integer k ≥ 3,

k=3

P(cid:0)bNα ≥ k(cid:1) = (−1)k

(α − 2)(α − 3)··· (α − k + 1)

= k(k − 1)θα(k)

.

(k − 2)!

α

By taking the derivative with respect to α, we see that the size-biased oﬀspring number bNα is
decreasing with respect to α ∈ (1, 2] for the stochastic order. Recall that the conductance C(α)
is also decreasing with respect to α ∈ (1, 2] for the stochastic order (see Section 2.4 of [9]).
Now we take 1 < α1 ≤ α2 ≤ 2. According to the previous discussion, for any ﬁxed x ≥ 1,
)i≥2(cid:1) (cid:22) G(cid:0)Vα1, bNα1, x, (C(α1)
which implies that bΦα2(σ) (cid:22) bΦα1(σ) for any probability distribution σ on [1,∞]. By Proposi-
tion 8, for any α ∈ (1, 2], the law bγα of bC(α) has no atom and the iterates of bΦα(σ) converge
weakly to bγα. As a result, bC(α2) is stochastically dominated by bC(α1).
Therefore we have shown that the law of the size-biased conductance bC(α) is decreasing with
respect to α ∈ (1, 2] for the stochastic order. In particular, the ﬁrst moment E[bC(α)] decreases
with respect to α ∈ (1, 2] and so does λα.

G(cid:0)Vα2, bNα2, x, (C(α2)

)i≥2(cid:1),

i

i

18

2.7 The asymptotic behavior of λα as α ↓ 1
We give here the proof of Proposition 4. Since λα > 1

α−1, we only need to show that

lim sup

(α − 1)λα < ∞.

(30)

α↓1
To this end, we recall that for every s ≥ 0,

We set r = r(α) := α−1

Fα(s) :=X
Z ∞

k≥2

θα(k)e−sk = E(cid:2)e−sNα(cid:3) = (1 − e−s)α − 1 + αe−s
i = Γ(1 − r) × E(cid:2)N 1+r

2 . By Fubini’s theorem, it holds that
(s) s−r ds = E

α e−sNαs−r ds
N 2

hZ ∞

α − 1

F 00

α

α

.

(cid:3),

0

and hence

E(cid:2)N 1+r

α

0

(cid:3) =

=

Z ∞

1

0

Γ(1 − r)
α
(α − 1)Γ(1 − r)
Z ∞

0

F 00

α

(s) s−r ds
Z ∞

e−ss−r(cid:0)(α − 1)e−s(1 − e−s)α−2 − (1 − e−s)α−1 + 1(cid:1)ds.

It is elementary to check that

e−ss−r(cid:0)(α − 1)e−s(1 − e−s)α−2 − (1 − e−s)α−1 + 1(cid:1)ds < ∞,

lim sup

α↓1

0

which yields

lim sup

α↓1

(α − 1)

∞X

k=2

θα(k) k1+r = lim sup

α↓1

(α − 1)E(cid:2)N 1+r

α

(cid:3) < ∞.

According to Section 2.4 in [9], the family (E[C(α)], α ∈ (1, 2]) is uniformly bounded by some
constant C0 ≥ 1. Therefore, we get from (28) and the preceding display that

lim sup

α↓1

(α − 1)λα ≤ M + C0

(cid:16) lim sup

α↓1

(α − 1)

θα(k) k1+r(cid:17)

∞X

k=2

< ∞,

which establishes (30).

Following the nowadays well-known idea of size-biasing a Galton–Watson tree (see e.g. [3, 12]),

2.8 The size-biased CTGW tree bΓ(α)
we can construct a size-biased version bΓ(α) of the CTGW tree Γ(α).
It lives for a random exponential lifetime with parameter
Let us start with the root ∅.
distributed as the size-biased oﬀspring number bNα. We pick one of these children uniformly
α−1, then it dies and simultaneously gives birth to a random number N1 of children, with N1
at random, say v1 ∈ {1, 2, . . . , N1}. We give independently the other children independent de-
scendant trees distributed as Γ(α), whereas v1 lives for another independent exponential lifetime
with parameter
α−1 and then reproduces a random number N2 of children with N2 an inde-
pendent copy of N1. Again, we choose one of the children of v1 uniformly at random, call it

α

α

19

ray in Γ(α) sampled according to the uniform measure ¯ω(α).

v2 ∈ {1, 2, . . . , N2}, and give the others independent CTGW descendant trees distributed as
inﬁnite tree bΓ(α) which will be called the size-biased CTGW tree with parameter α ∈ (1, 2].
Γ(α). Repeating this procedure independently for inﬁnitely many times, we obtain a random
A formal deﬁnition of bΓ(α) with the distinguished geodesic ray bv = (v1, v2, . . .) as a random
)k≥1 of the vertices on the “spine” bv are distributed as a homogeneous Poisson process on
element of T ∗ can be given similarly as in Section 2.6 of [10]. Notice that the successive heights
(Zvk
The next lemma explains the convenience of considering bΓ(α) to study a random geodesic
(0,∞) with intensity α
α−1.
Lemma 9. The pair (cid:0)bΓ(α),bv(cid:1) ∈ T ∗ follows the distribution W(α)(T )Θα(dT )¯ω
2.9 The size-biased reduced tree b∆(α)
apply its inverse Ψ−1(v, s) = (v, 1 − e−s) to the size-biased CTGW tree bΓ(α) and then take the
Recall the bijection Ψ: (v, r) ∈ ∆(α)
7→ (v,− log(1 − r)) ∈ Γ(α) introduced in Section 2.2. If we
natural compactiﬁcation, we obtain a random compact rooted tree b∆(α) called the size-biased
0
reduced tree of parameter α ∈ (1, 2]. A Poissonian description of b∆(α) is given as follows.
First, under the mapping Ψ−1 the geodesic ray bv = (v1, v2, . . .) in bΓ(α) corresponds to a
distinguished point at height 1 in b∆(α) that we will still denote by bv. Along the ancestral line
of bv in b∆(α), we keep the same notation (vk)k≥1 for the branching points, with their respective

(α)
T (dv).

k ≥ 1 ,
increasing to 1 as k → ∞. It is immediate to see that (Yvk
Poisson process with intensity measure α
sequence

:= 1 − exp(−Zvk

Yvk

) ,

)k≥1 is distributed on (0, 1) as a
α−1(1− x)−11x∈(0,1)dx. Moreover, if we set Yv0 = 0, the

heights

Vk := Yvk − Yvk−1
1 − Yvk−1

,

k ≥ 1,

For any r > 0, we write r∆(α)
0

consists of i.i.d. random variables with the density function α

α−1(1 − x) 1
for the “same” random tree as ∆(α)

)k≥1, independently to each branching point vk of height Yvk

α−1 over [0, 1].
0 with the distance d
multiplied by the factor r, which means that the inﬁnite discrete tree Π(α) associated to ∆(α)
0
remains the same while all the uniform random variables Uv, v ∈ V involved in the deﬁnition of
∆(α)
0
on the

are replaced by rUv, v ∈ V.
ancestral line of bv in b∆(α), we graft a random number Jk of independent trees, all distributed
Conditionally on (Yvk
and it has the same distribution as bNα − 1. As in the case of the size-biased CTGW tree bΓ(α),
according to (1 − Yvk
0 . The random number Jk is independent of those trees grafted at vk,
To ﬁnally get the size-biased reduced tree b∆(α), we take the completion of the tree obtained
we assume the independence of Jk and the trees at grafted vk among all diﬀerent levels k ≥ 1.
above with respect to its natural distance (see Fig. 3). This compactiﬁcation is equivalent to
distinguished one bv form the boundary ∂b∆(α), which is deﬁned as the set of all points in b∆(α)
adding all the points on the boundary of the grafted trees. All these points together with the
If we think of b∆(α) as a network of ideal resistors with unit resistance per unit length, its
eﬀective conductance between the root and ∂b∆(α) clearly satisﬁes the distributional identity (12)

according to the series/parallel law of electric conductance. Using Proposition 8 or Lemma 9,

that are at height 1.

)∆(α)

20

Figure 3: Schematic representation of the size-biased reduced tree b∆(α) when 1 ≤ α < 2

we know that the conductance of b∆(α) has the same law as bC(α) deﬁned in Section 2.4. For this
reason, we can call bC(α) the conductance of the size-biased reduced tree b∆(α).

Remark. The α-stable continuum random tree arises as the scaling limit of rescaled discrete
Galton–Watson trees with a critical oﬀspring distribution in domain of attraction of a stable
distribution of index α ∈ (1, 2] (see e.g. the monograph [5] of Duquesne and Le Gall). Under the
same assumption, if we condition the critical Galton–Watson tree to survive up to generation n
and rescale it by the factor n−1, after letting n → ∞, we get as limit an α-stable tree conditioned
to reach height 1 (see Theorem 4.1 in [6]), from which the reduced stable tree ∆(α) can be
obtained as a pruned tree by keeping only those points at height less than 1 that possess a
reduced tree b∆(α) can be derived from a spinal decomposition (Theorem 4.5 in [6]) of the
descendant that reaches height 1. Furthermore, the probabilistic structure of the size-biased

conditional α-stable tree along the ancestral line of a typical point at height 1. Since this point
of view is not needed for proving our main results, we omit the details.

3 The discrete setting
3.1 Galton–Watson trees
We brieﬂy introduce the general notion of discrete rooted ordered trees, which extends the case
of inﬁnite trees without leaves that we have seen in Section 2.1. A discrete rooted ordered tree t
is a subset of V such that the following holds:
(i) ∅ ∈ t ;
(ii) If u = (u1, . . . , un) ∈ t\{∅}, then ¯u = (u1, . . . , un−1) ∈ t ;
(iii) For every u = (u1, . . . , un) ∈ t, there exists an integer ku(t) ≥ 0 such that, for every j ∈ N,

(u1, . . . , un, j) ∈ t if and only if 1 ≤ j ≤ ku(t).

The quantity ku(t) in (iii) is called the number of children of u in t. A vertex with no child is
called a leaf. From now on, we will just say tree instead of discrete rooted ordered tree for short.

21

Height1Height0∅bvv2v1Yv1Yv2(1−Yv1)∆(α)v3J1=3J2=1J3=2We write ≺ for the (non-strict) genealogical order on t. A tree t is always viewed as a graph
whose vertices are the elements of t and whose edges are the pairs {¯u, u} for all u ∈ t\{∅}.
Since the lexicographical order on the vertices is not really used in our arguments, we will not
pay much attention to this order structure.

For an inﬁnite tree t, we say it has a single inﬁnite line of descent if there exists a unique
sequence of positive integers (un)n≥1 such that (u1, u2, . . . , un) ∈ t for all n ≥ 1.
The height of a tree t is written as h(t) := sup{|u|: u ∈ t}. The set of all vertices of t at
generation n is denoted by tn := {u ∈ t: |u| = n}. If u ∈ t, the tree of descendants of u is
t[u] := {w ∈ V : uw ∈ t}.
Let t be a tree of height larger than n, and consider a simple random walk X = (Xk)k≥0
on t starting from the root ∅, which is deﬁned under the probability measure P t. We write
τn := inf{k ≥ 0: |Xk| = n} for the ﬁrst hitting time of generation n by X, and we deﬁne the
discrete harmonic measure µt
Critical Galton–Watson trees. For every integer n ≥ 0, we let T(n) be a discrete Galton–
Watson tree with critical oﬀspring distribution ρ, conditioned on non-extinction at generation n,
viewed as a random subset of V. We suppose that the random trees T(n), n ≥ 0 are deﬁned under
the probability measure P.
We let T∗n be the reduced tree associated with T(n), consisting of all vertices of T(n) that
have (at least) one descendant at generation n. Note that |u| ≤ n for every u ∈ T∗n. When
talking about a reduced tree, we always implicitly assume that the correct relabeling of the
vertices has been done to make it a tree without changing the genealogical order.

Set qn := P(cid:0)h(T(0)) ≥ n(cid:1) the non-extinction probability up to generation n for a Galton–

supported on tn as the law of Xτn

Watson tree of oﬀspring distribution ρ. If L is the slowly varying function appearing in (1), it
has been established in [15, Lemma 2] that

under P t.

n

n L(qn) ∼
qα−1

1

(α − 1)n

as n → ∞.

(31)

qn ∼ n

as n → ∞,

By the asymptotic inversion property of slowly varying functions (see e.g. [2, Section 1.5.7]), it
follows that

− 1
α−1 ‘(n)
Size-biased Galton–Watson tree. We denote bybT a size-biased Galton–Watson tree with
for some other function ‘ slowly varying at ∞.
oﬀspring distribution ρ. It is deﬁned similarly as the size-biased CTGW tree bΓ(α). We refer to
in bT is called its spine. If bN denotes a random variable distributed according to the size-biased
Lyons, Pementle and Peres [12] for the details. The unique inﬁnite line of descent (v1, v2, . . .)
distribution of ρ, that is, P(bN = k) = k ρ(k) for every k ≥ 0, then the oﬀspring numbers of
(vk)k≥1 are i.i.d. copies of bN.
Let [bT](n) be the ﬁnite tree obtained from bT by keeping only its ﬁrst n generations, i.e.,
As shown in [12], the law of the random tree [bT](n) is that of T(n) biased by #T(n)
conditionally given the ﬁrst n levels of bT, the vertex vn on the spine is uniformly distributed on
the n-th level of bT.
For every integer n ≥ 1, let [bT]n be the ﬁnite tree obtained from bT by erasing the (inﬁnite)
tree of descendants of the vertex vn. By convention, the vertex vn itself is kept in [bT]n. We
emphasize that in general [bT]n 6= [bT](n), since the height of [bT]n can be strictly larger than n.

[bT](n) := {u ∈ bT: |u| ≤ n}.

n . Moreover,

22

We let [bT]∗n be the reduced tree associated with the tree [bT]n up to generation n, which
consists of all vertices of [bT]n that have (at least) one descendant at generation n. Notice that
[bT]∗n is also the reduced tree associated with [bT](n) up to generation n.

3.2 Convergence of discrete conductances
Take a tree t of height larger than n and consider the new tree t0 obtained by adding to the
graph t an edge between its root ∅ and an extra vertex ∂. We deﬁne under the probability
measure P t0 a simple random walk X on t0 starting from the root ∅. Let τ∂ be the ﬁrst hitting
time of ∂ by X, and for every integer 1 ≤ i ≤ n, let τi be the ﬁrst hitting time of generation i
(of the tree t) by X. We write

Ci(t) := P t0(τi < τ∂).

This notation is justiﬁed by the fact that Ci(t) can be interpreted as the eﬀective conductance
between ∂ and the i-th generation of t in the graph t0, see e.g. [13, Chapter 2].
Here are some basic facts concerning the conductance of the reduced Galton–Watson tree T∗n.
Proposition 10. Suppose that the critical oﬀspring distribution ρ is in the domain of attraction
of a stable distribution of index α ∈ (1, 2]. Then it holds that

nCn(T∗n)

(d)−−−→
n→∞ C(α).

Recall that C(α) is the conductance of the continuous reduced tree ∆(α). Given the conver-
gence result [9, Proposition 3.2] of rescaled discrete trees n−1T∗n towards ∆(α), the previous
proposition can be shown in the same way as Proposition 11 in [10].

The following moment estimate for the conductance Cn(T∗n) is Lemma 3.9 in [9].

Lemma 11. For every r ∈ (0, α), there exists a constant K = K(r, ρ) ≥ 1 depending on r and
the oﬀspring distribution ρ such that, for every integer n ≥ 1,

h(cid:0)nCn(T∗n)(cid:1)ri ≤ K.

E

3.3 Backward size-biased Galton–Watson tree

is constructed. More details can be found in Section 3.4 of [10].

In the proof of Theorem 1, we will need a rear-view variant Tb of the size-biased Galton–Watson
tree bT, which can be tracked back to the inﬂated Galton–Watson tree introduced by Peres and
Zeitouni in [14]. Let us explain succinctly how this backward size-biased Galton–Watson tree Tb
First, the random tree Tb has a unique inﬁnite ray of vertices (u0, u1, u2, . . .), referred to as
its spine. We ﬁx a genealogical order on the spine by declaring that, for every n ≥ 0, the vertex
un is at generation −n of Tb. Under this rule, u0 is a child of u1, which is a child of u2, etc.
To describe the ﬁnite subtrees in Tb branching oﬀ every node of the spine, we take an i.i.d. se-
quence (bNn)n≥1 of random variables following the size-biased distribution of ρ. Independently
to each vertex un, n ≥ 1, we graft bNn − 1 independent copies of an ordinary ρ-Galton–Watson
descendant tree, so that un has exactly bNn children: one of them, un−1, is on the spine while
Last but not least, we deﬁne the genealogical order on Tb by keeping the genealogical orders
inherited from the grafted Galton–Watson trees and combining them with the genealogical order
on the spine. For instance, u2 is an ancestor of any vertex in one of the subtrees grafted at u1.

the rest of them are the roots of independent ρ-Galton–Watson trees.

23

The notion of generation for every vertex in Tb can also be deﬁned in a consistent manner: for
any vertex v not on the spine, there is a unique vertex um on the spine such that v belongs to a
ﬁnite subtree grafted at um, then we say that the generation of v in Tb is equal to −m + 1 plus
the initial generation of v inside the corresponding grafted tree. See Figure 4 for an illustration.

Figure 4: Schematic representation of the backward size-biased Galton–Watson tree ˇT

Comparing with the inﬂated Galton–Watson tree described in Peres and Zeitouni [14], notice

that the vertex u0 in Tb has no strict descendant.
For every n ≥ 1, let [Tb]n be the tree obtained from Tb by only keeping the ﬁnite tree above un.
The vertex un is taken as the root of [Tb]n. We observe that [Tb]n has the same distribution as the
random tree [bT]n deﬁned at the end of Section 3.1. Moreover, the root un of [Tb]n corresponds
to the root ∅ of [bT]n, and the vertex u0 in [Tb]n corresponds to the vertex vn in [bT]n.

4 Proof of Theorem 1
To prove Theorem 1, we will follow the same idea developed in [10] for the ﬁnite variance case:
the problem can be reduced to calculating a certain hitting probability for simple random walk

on the backward size-biased Galton–Watson tree Tb (see Proposition 15 below). We suppose
in the remainder of this section that the parameter α ∈ (1, 2] is ﬁxed. For simplicity, all the
Recall that λα = E[bC(α)] − 1 > 1
random trees involved are deﬁned under the same probability measure P.
goes from the conditional Galton–Watson tree T(n) to the size-biased Galton–Watson tree bT.
α−1 is the constant that appears in (4). Our ﬁrst reduction
Using a result of Slack [15, Theorem 1], we can easily adapt the arguments at the beginning of
Section 4 in [10] to see that convergence (2) holds if for every δ > 0,

(cid:16)

n−λα−δ ≤ P [bT]n(Xτn

= vn) ≤ n−λα+δ(cid:17) = 1.

lim
n→∞ P

Furthermore, owing to the correspondence between the truncated trees [bT]n and [Tb]n, we can

24

bN4=4u3u2u1u0ρ-GWρ-GWρ-GWρ-GWρ-GWρ-GWρ-GWu4Generation0Generation−1Generation−2Generation−3Generation−4bN3=2bN2=3bN1=2rewrite the previous display as

(cid:16)

lim
n→∞ P

n−λα−δ ≤ P [ˇT]n(Xτn

= u0) ≤ n−λα+δ(cid:17) = 1.

(32)

4.1 SRW on the inﬁnite backward tree
In order to show (32), we denote by −M1,−M2, . . . the generations of the vertices on the spine
of Tb where there is at least one grafted tree that has a descendant of generation 0. This sequence
of negative integers (−Mk)k≥1 is listed in the strict decreasing order, and we set by convention
M0 = 0. For every k ≥ 1, we also set Lk := Mk − Mk−1 ≥ 1.
For every n ≥ 1, let kn := kn(Tb) be the index such that Mkn ≤ n < Mkn+1. We need the
next result to study the asymptotic behavior of kn.
Lemma 12. Let bN be a random variable distributed according to the size-biased distribution
of ρ. Then there exists a function eL slowly varying at 0+ such that for every r ∈ (0, 1),

E(cid:2)rbN−1(cid:3) = 1 − α(1 − r)α−1eL(1 − r).

(33)

(34)

Moreover, we have

eL(x)
L(x) = 1,
where L is the slowly varying function appearing in (1).
Proof. We claim that the slowly varying function L appearing in (1) can be rewritten as

lim
x→0+

L(x) = C exp(cid:16)Z x

ε(t)
t

1

(cid:17)

dt

,

x ∈ (0, 1),

x → 0+. Indeed, we follow Slack [15] to set x = 1 − r and Λ(x) = xα−1L(x). SinceP kρ(k) = 1

where C 6= 0 is a constant and ε(·) is a continuous function on (0, 1) such that ε(x) → 0 as
and ρ(1) 6= 1, we know that

xΛ(x) = xαL(x) =X

ρ(k)rk − r

k≥0

is strictly positive. From the latter expression, we also deduce that xΛ(x) is analytic and has
monotone derivative for x ∈ (0, 1). It follows from a result of Lamperti [8, Theorem 2] that

lim
x→0+

We deﬁne

lim
x→0+

xΛ0(x)
Λ(x) = α − 1 .

x(xΛ(x))0
xΛ(x) = α and thus
ε(x) := xΛ0(x)

Λ(x) − α + 1 ,

which is continuous for x ∈ (0, 1) and satisﬁes that ε(x) → 0 as x → 0+. Integrating

we obtain

+ ε(x)

,

x

Λ(x) = α − 1
Λ0(x)
Λ(x) = Cxα−1 exp(cid:16)Z x

x

ε(t)
t

1

(cid:17)

dt

,

25

where C 6= 0 as ρ(1) 6= 1. The claimed representation for L readily follows.

As an immediate consequence, L0(x) = L(x) ε(x)

for x ∈ (0, 1). By diﬀerentiating (1), we see

x

that

E(cid:2)rbN−1(cid:3) =X

k≥1

kρ(k)rk−1 = 1 − α(1 − r)α−1L(1 − r) − (1 − r)αL(1 − r) ε(1 − r)
1 − r

= 1 − α(1 − r)α−1eL(1 − r) ,

1−r

). When r → 1−, we get (34) as ε(1 − r) → 0.

where we have put eL(1 − r) := L(1 − r)(1 + ε(1−r)
The asymptotic equivalence of eL and L implies that eL also varies slowly at 0+.
Lemma 13. We have P-a.s.
Proof. Recall that for every j ≥ 1, there are bNj −1 independent Galton–Watson trees grafted at
uj in Tb. Consider the event that at least one of those trees grafted at uj reaches generation 0,
and let j be the corresponding indicator function. Then by deﬁnition,

kn
log n

α − 1 .

lim
n→∞

= α

h(1 − qj−1)bN−1i

.

P(j = 0) = E

P(j = 0) = 1 −

h(1 − qj−1)bNj−1i = E
(cid:1) ,
+ o(cid:0)1
(cid:0)1 − P(j = 0)(cid:1) ∼ α

(α − 1)j

α

j

α − 1 log n ,

E[kn] = nX

j=1

as j → ∞.

(35)

as n → ∞.

Applying (31) and Lemma 12 to the latter formula yields

Since kn = 1 + 2 + ··· + n, we deduce that

α

Since 1, . . . , n are independent, we also have var(kn) = O(log n), and the L2-convergence
of kn/ log n to
α−1 follows immediately. The a.s. convergence is then obtained by standard
monotonicity and Borel–Cantelli arguments.
Given Tb, for every j ≥ 0 we write P
for the (quenched) probability measure under which
we consider a simple random walk X = (Xk)k≥0 on Tb starting from the vertex uj. Under P
ˇT
,
j
we denote by S0 the hitting time of generation 0 by the simple random walk X, and for every
i ≥ 0, we write Πi := inf{k ≥ 0: Xk = ui} for the hitting time of vertex ui.
The following extension of [10, Proposition 14] compares simple random walks on the trun-
cated ﬁnite tree [Tb]n and on the backward inﬁnite tree Tb. We skip its proof because one can
easily adapt the original proof of Proposition 14 in [10] to the present setting.
Proposition 14. For every δ > 0, there exists an integer n0 ∈ N such that for every n ≥ n0,
we have

ˇT
j

P

(cid:16)
P [ˇT]n(Xτn
(cid:16)
P [ˇT]n(Xτn

= u0) ≥ n−λα+δ(cid:17) ≤ 2 2α
= u0) ≤ n−λα−δ(cid:17) ≤ 2 2α

P

α−1 P

P

(cid:16)
(cid:16)

and

ˇT
Mkn

(XS0 = u0, S0 < ΠMkn+1

α−1 P

ˇT
Mkn

P

(XS0 = u0, S0 < ΠMkn+1

) ≥ n−λα+δ/2(cid:17)
) ≤ n−λα−δ(cid:17)

.

,

26

Combining (32) with the previous result, we can derive Theorem 1 from the next proposition.

Proposition 15. For every δ > 0, it holds that

(cid:16)

lim
n→∞ P

n−λα−δ ≤ P

ˇT
Mkn

(XS0 = u0, S0 < ΠMkn+1

Under the probability measure P, we set therefore

) ≤ n−λα+δ(cid:17) = 1.

(36)

pk = pk(Tb) := P

ˇT
Mk

(XS0 = u0, S0 < ΠMk+1

)

for every k ≥ 1. By the deﬁnition of Mk, there exists at least one subtree grafted to uMk
distinct from
that reaches generation 0. The root of this subtree is necessarily a child of uMk
uMk−1. If such a subtree is unique, we let ck = ck(Tb) be the probability that a simple random
. If there is more than one
walk starting from its root reaches generation 0 before hitting uMk
such grafted trees, we take ck to be the sum of the corresponding probabilities. We justify this
and
deﬁnition by the fact that ck can be interpreted as the eﬀective conductance between uMk
and all the subtrees grafted to it.
generation 0 in the graph that consists only of the vertex uMk

We also set, for every k ≥ 1,

hk = hk(Tb) := P

ˇT
Mk−1(S0 < ΠMk

),

which is the probability that a simple random walk starting from uMk−1 reaches generation 0
. We write in addition ‘k = 1/Lk = (Mk − Mk−1)−1. A recurrence relation
before hitting uMk
between pk and pk−1 is obtained in Section 4.1 of [10], which states that
(cid:17)

‘j

−

(cid:16)1 + cj + ‘j+1
p1 = pk × kY
Qj = Qj(Tb) := log(cid:16)1 + cj + ‘j+1

j=2

‘j

‘j

‘j + cj−1 + hj−1

.

(cid:17)

.

−

‘j

‘j + cj−1 + hj−1

kX

j=2

1
k

Qj

L2(P)−−−→
k→∞

α − 1
α

λα.

(37)

Thus we deﬁne, for every j ≥ 2,

Lemma 16. We have

Proposition 15 (and thus Theorem 1) can be easily derived from this key lemma. In fact, on

account of Lemma 13 and Lemma 16, for any δ > 0, the event

n(λα − δ) log n ≤ knX

j=2

Qj ≤ (λα + δ) log n

o =n

p1n−λα−δ ≤ pkn ≤ p1n−λα+δo

holds with P-probability tending to 1 as n → ∞. Since δ is arbitrary, the required convergence
(36) follows from the last display. Therefore, we are left with the task of proving Lemma 16.

27

4.2 Proof of Lemma 16
Let us begin with several auxiliary results. For every k ≥ 1, we denote by Ik the number of
independent ρ-Galton–Watson trees grafted at uk that reach generation 0 in Tb.
Lemma 17. As k tends to ∞, the conditional generating function E[rIk | Ik ≥ 1] converges to
in distribution to bNα − 1.
1 − (1 − r)α−1. Thus according to (29), the conditional distribution of Ik given Ik ≥ 1 converges
Proof. For ﬁxed r ∈ (0, 1), by independence of the bNk − 1 subtrees grafted at uk, we have
E(cid:2)rIk(cid:3) = E

h(rqk−1 + 1 − qk−1)bN−1i

It follows from Lemma 12 that

h(rqk−1 + 1 − qk−1)bNk−1i = E
E(cid:2)rIk(cid:3) = 1 − α(qk−1(1 − r))α−1eL(qk−1(1 − r)),
where eL is the slowly varying function appearing in (33). Thus we obtain
= 1 − α(1 − r)α−1 (qk−1)α−1eL(qk−1(1 − r))

E(cid:2)rIk | Ik ≥ 1(cid:3) = E(cid:2)rIk(cid:3) − P(Ik = 0)

P(Ik ≥ 1)

.

.

P(Ik ≥ 1)

Recall that it has been observed in the proof of Lemma 13 that

(α − 1)k
(qk−1)α−1eL(qk−1(1 − r))

P(Ik ≥ 1)

(cid:16)1

(cid:17)

k

−→
k→∞

1
α

.

P(Ik ≥ 1) = P(k = 1) =

α

+ o

,

as k → ∞.

Using (31) and (34), one readily veriﬁes that

The proof is therefore ﬁnished.

Note that if ρ has a ﬁnite variance, the previous lemma becomes trivial because P-a.s. for
that reaches generation 0.

all suﬃciently large k, there will be a unique subtree grafted at uMk
The next result generalizes Lemma 17 in [10].
Lemma 18. We have

(cid:17) (d)−→

k→∞

(cid:0)Rα,R0

α,PC(α),P0 C(α), bC(α)(cid:1),

(cid:16) Lk+1

Mk

,

Lk

Mk−1

, Mkck, Mk−1ck−1, Mk−1hk−1

where in the limit
• Rα and R0

α

are two positive random variables with the same distribution given by

P(Rα > x) = (1 + x)− α

α−1

for all x ≥ 0 ;

28

• Take bNα and bN0
tion of θα. Let (C(α)
distributed according to γα. Then

α

k

)k≥2 and (eC(α)
2 + ··· + C(α)bNα

k

PC(α) := C(α)

two integer-valued random variables following the size-biased distribu-
)k≥2 be two sequences of random variables identically

and P0 C(α) := eC(α)

2 + ··· + eC(α)bN0

;

• bC(α) is distributed according to bγα.
Furthermore, we suppose that all the random variables listed above in the description of the limit
are deﬁned under the probability measure P, and they are independent.
Proof. We only give an outline of the proof, which is divided into three steps. First of all, using
(35) one can verify that

α

(cid:16) Lk

,

Lk+1
Mk

Mk−1

(cid:17) (d)−→

k→∞

(cid:0)R0
α,Rα

(cid:1)

holds for a similar reason as display (32) in [10].

Then notice that conditionally on Mk and on the number IMk

of subtrees grafted at uMk
that reach generation 0, these subtrees are independent ρ-Galton–Watson trees conditioned to
have height greater than Mk − 1. Recall that ck is the sum of the conductances between uMk
and generation 0 in those subtrees. On account of Proposition 10 and Lemma 17, we can obtain

the convergence in distribution of Mkck toPC(α) by calculating the Laplace transform of Mkck.

A slight change of this argument by ﬁrst conditioning on Mk−1 gives the joint convergence

(cid:0)Mk−1ck−1, Mkck

(cid:1)

(d)−→
k→∞

(cid:0)P0 C(α),PC(α)(cid:1),

which holds jointly with (38), provided we keep all the limiting random variables independent.

Finally, recall that with the notation of Section 3.2, hk−1 = CMk−1−1([Tb]Mk−1−1). The

convergence

(38)

(39)

can be shown by the same approximation method used in the proof of Lemma 17 in [10]. Since
in Tb, the last
convergence holds jointly with (38) and (39), if we take bC(α) to be independent of all the random
hk−1 only depends on Mk−1 and the ﬁnite tree strictly above the vertex uMk−1
variables involved in the deﬁnition of (Rα,R0
Lemma 18 in [10] states that in the ﬁnite variance case (α = 2), the second moment of Mkhk

is uniformly bounded. When α < 2, we have the following lemma.
Lemma 19. For every r ∈ (0, α), it holds that

Mk−1hk−1

(d)−→

k→∞ bC(α)
α,PC(α),P0 C(α)).

E(cid:2)(Mkhk)r(cid:3) < ∞.

sup
k≥1

Proof. We ﬁx r ∈ (0, α). For any η > 0, there exists a positive constant C(η) suﬃciently large
so that (a + b)r ≤ C(η)ar + (1 + η)br for every a, b > 0. Using this inequality together with
Lemma 11, we can easily adapt the proof of [10, Lemma 18] to show the existence of positive
constants C < ∞ and ξ < 1, both independent of k, such that for all k ≥ 2,

E(cid:2)(Mkhk)r(cid:3) ≤ C + ξ E(cid:2)(Mk−1hk−1)r(cid:3).

The uniform boundedness of the sequence (E[(Mkhk)r])k≥1 thus follows.

29

2 + ··· + eC(α)bN0
(cid:19)

α

Mk
Lk

(Mk−1hk−1 + Mk−1ck−1)

.

Using the same notation as in Lemma 18, we set

 

Q∞ := log

1 +

2 + ··· + C(α)bNα
C(α)

1+R0
αR0

α

+ 1Rα

−

1 + R0

α

1

(cid:0)bC(α) + eC(α)

Recall that

(cid:18)

Qk = log

1 + Mkck + Mk

Lk+1

Mk
Lk

−

Mk
Lk

+ Mk
Mk−1

The next result is the analog of Lemma 19 in [10].
Lemma 20.
(ii) We have the equality E[Q∞] = α−1
(iii) It holds that

(i) We have limk→∞ E[Qk] = E[Q∞].

α λα.

E(cid:2)|QiQj|(cid:3) < ∞ and

sup
i,j≥1

Moreover, for any integer n ≥ 2, we have

E(cid:2)(QiQj)2(cid:3) < ∞.

sup
i,j≥1

E(cid:2)(Qi)2n(cid:3) < ∞ .

sup
i≥1

!

.

(cid:1)

(40)

Proof. We leave it to the reader to verify that the same proof of assertion (i) in Lemma 19 of [10]
works in the present setting. For the calculation of E[Q∞], we set

They are independent with the same law of density α

 
V 0

α

Q∞ = log

= log(V 0

α

) + log

and hence

E[Q∞] = E[log(V 0

α

"
)] + E

E[Q∞] = E[log(V 0

However, since Rα = Vα1−Vα
,
(cid:20)
log(cid:16)C(α)
+ 1
Rα

2 +···+C(α)bNα

E

Vα := Rα
1 + Rα

and

.

α

α

α

V 0

:= R0
1 + R0
α−1(1 − x) 1
(cid:0)bC(α) + eC(α)
V 0
1 − V 0
(cid:18)

+ V 0

α

α

+

α

V 0
(cid:18)

α

+

(cid:17) +
+ 1
Rα

(cid:16)C(α)
2 + ··· + C(α)bNα
+ 1
Rα
 
2 + ··· + C(α)bNα
C(α)
 
2 + ··· + C(α)bNα
+ 1
C(α)
Rα
log(cid:16)C(α)
h log(cid:16)1−Vα+Vα

+bC(α)(cid:17)(cid:21)

(cid:20)
)] + E

log

= E

+

α

α

α

α

,

α

α−1 on [0, 1]. Notice that

!
(cid:1)
(cid:19)−1!

(cid:1)
2 + ··· + eC(α)bN0
(cid:0)bC(α) + eC(α)
2 + ··· + eC(α)bN0
1 − V 0
2 + ··· + eC(α)bN0
bC(α) + eC(α)
1 − V 0
bC(α) + eC(α)
2 + ··· + eC(α)bN0
+ bC(α)(cid:17)(cid:21)
+bC(α)(cid:1)(cid:17)i−E[log(Vα)].

(cid:19)−1!#

α

.

.

α

V 0

α

+

+ 1
Rα

2 + ··· + C(α)bNα
(cid:0)C(α)
2 +···+C(α)bNα

30

Using (12) to rewrite the second term in the right-hand side, we obtain

α

E[Q∞] = E

)] > −∞, we deduce that

h log(cid:16)1 − Vα + Vα

log(cid:16)bC(α) + C(α)

+ bC(α)(cid:1)(cid:17)i
As E[log(Vα)] = E[log(V 0
(cid:0)C(α)
2 + ··· + C(α)bNα
Now we apply (12) again to see that log(bC(α)) has the same distribution as
(cid:0)C(α)
2 + ··· + C(α)bNα
#

(cid:17) − log(cid:16)1 − Vα + Vα
bC(α) + C(α)
2 + ··· + C(α)bNα
bC(α)

Accordingly, we can conclude by (27) that

"
E[Q∞] = E

2 + ··· + C(α)bNα

= α − 1

log

λα .

α

.

+ bC(α)(cid:1)(cid:17)

.

Assertion (iii) can be analogously shown as assertion (iii) in [10, Lemma 19]. Regarding the
supplementary result (40), it can be treated in a similar way since for any small r > 0, there
exists a constant M > 0 such that log(1 + x) ≤ M + xr for every x > 0.

We are now ready to ﬁnish the proof of Lemma 16. Assertions (i) and (ii) of Lemma 20 give

(cid:21)

(cid:20)1

kX

k

j=2

= lim
k→∞

1
k

Qj

kX

j=2

lim
k→∞ E

(cid:20)(cid:16)1
lim sup
k→∞ E

k

kX

j=2

Qj

With all the ingredients prepared above, we can prove the inequality

λα.

α

E[Qj] = E[Q∞] = α − 1
(cid:17)2(cid:21)
≤(cid:0)E[Q∞](cid:1)2

in a similar manner as Lemma 20 in [10].
In the corresponding arguments, notice that for
controlling the error of approximation, although Lemma 18 in [10] is now replaced by its weaker
analog Lemma 19, we are able to compensate by (40) to apply the Hölder inequality as required.
Finally, the desired L2-convergence follows from

(cid:20)(cid:16)1
lim sup
k→∞ E

k

kX

j=2

Qj−E[Q∞](cid:17)2(cid:21)

(cid:20)(cid:16)1
≤ lim sup
k→∞ E

k

kX

j=2

(cid:17)2(cid:21)
−2E[Q∞] lim

k→∞ E

(cid:20)1

kX

Qj

Qj

k

j=2

(cid:21)
+E[Q∞]2 ≤ 0.

References
[1] K. B. Athreya, P. E. Ney, Branching Processes. Springer, New York–Heidelberg 1972.
[2] N. H. Bingham, C. M. Goldie, and J. L. Teugels, Regular variation, Encyclopedia

of Mathematics and its Applications 27, Cambridge University Press, 1987, xx+491.

[3] B. Chauvin, A. Rouault, and A. Wakolbinger, Growing conditioned trees, Stoch. Pro-

cess. Appl., 39 (1991), 117–130.

[4] N. Curien, J.-F. Le Gall, The harmonic measure of balls in random trees, to appear in

Ann. Probab., available at arxiv:1304.7190.

[5] T. Duquesne, J.-F. Le Gall, Random Trees, Lévy Processes and Spatial Branching

Processes, Astérisque 281 (2002).

31

[6]

, Probabilistic and fractal aspects of Lévy trees, Probab. Theory Related Fields, 131

(2005), 553–603.

[7] W. Feller, An introduction to probability theory and its applications. Vol. II., Second

edition, John Wiley & Sons Inc., New York, 1971.

[8] J. Lamperti, An occupation time theorem for a class of

Trans. Amer. math. Soc., 88 (1958), 380–387.

stochastic processes,

[9] S. Lin, The harmonic measure of balls in critical Galton–Watson trees with inﬁnite variance

oﬀspring distribution, Electron. J. Probab. 19 (2014), no. 98, 1–35.

[10]

, Typical behavior of the harmonic measure in critical Galton–Watson trees, to ap-
pear in Annales de l’Institut Henri Poincaré, Probabilités et Statistiques, available at
arxiv:1502.05584.

[11] R. Lyons, R. Pemantle, and Y. Peres, Ergodic theory on Galton–Watson trees: speed
of random walk and dimension of harmonic measure, Ergodic Theory Dynam. Systems, 15
(1995), 593–619.

[12]

, Conceptual proofs of l log l criteria for mean behavior of branching processes, Ann.

Probab., 23 (1995), 1125–1138.

[13] R. Lyons and Y. Peres, Probability on Trees and Networks, Cambridge University Press,

to appear, 2016. Available at http://mypage.iu.edu/∼rdlyons/.

[14] Y. Peres, O. Zeitouni, A central limit theorem for biased random walks on Galton-

Watson trees, Probab. Theory Related Fields, 140 (2008), 595–629.

[15] R. S. Slack, A branching process with mean one and possibly inﬁnite variance,

Z. Wahrscheinlichkeitstheorie und Verw. Gebiete, 9 (1968), 139–145.

[16] V. A. Vatutin, Limit theorems for critical Markov branching processes with several types
of particles and inﬁnite second moments, Mat. Sb. (N.S.), 103(145):2(6) (1977), 253–264.
[17] A. L. Yakymiv, Reduced branching processes, Theory Probab. Appl. 25 (1980), 584–588.

32

