6
1
0
2

 
r
a

M
3

 

 
 
]
L
M

.
t
a
t
s
[
 
 

1
v
9
2
0
1
0

.

3
0
6
1
:
v
i
X
r
a

Whitening-Free Least-Squares Non-Gaussian Component

Analysis

Hiroaki Shiino

shiino@ms.k.u-tokyo.ac.jp

Graduate School of Information Science and Engineering,

Tokyo Institute of Technology, Tokyo, Japan

Hiroaki Sasaki

hsasaki@is.naist.jp

Graduate School of Information Science,

Nara Institute of Science & Technology, Nara, Japan

Gang Niu

gang@ms.k.u-tokyo.ac.jp

Graduate School of Frontier Sciences,
The University of Tokyo, Tokyo, Japan

Masashi Sugiyama
sugi@k.u-tokyo.ac.jp

Graduate School of Frontier Sciences,
The University of Tokyo, Tokyo, Japan

Abstract

Non-Gaussian component analysis (NGCA) is an unsupervised linear dimension reduction
method that extracts low-dimensional non-Gaussian “signals” from high-dimensional data con-
taminated with Gaussian noise. NGCA can be regarded as a generalization of projection pur-
suit (PP) and independent component analysis (ICA) to multi-dimensional and dependent non-
Gaussian components. Indeed, seminal approaches to NGCA are based on PP and ICA. Recently,
a novel NGCA approach called least-squares NGCA (LSNGCA) has been developed, which gives
a solution analytically through least-squares estimation of log-density gradients and eigendecom-
position. However, since pre-whitening of data is involved in LSNGCA, it performs unreliably
when the data covariance matrix is ill-conditioned, which is often the case in high-dimensional
data analysis. In this paper, we propose a whitening-free LSNGCA method and experimentally
demonstrate its superiority.

1 Introduction
Dimension reduction is a common technique in high-dimensional data analysis to mitigate the curse
of dimensionality [Vapnik, 1998]. Among various approaches to dimension reduction, we focus on
unsupervised linear dimension reduction in this paper.

1

Table 1: NGCA methods.

MIPP

IMAK

SNGCA

LSNGCA

WF-LSNGCA

(proposed)

Necessary Unnecessary
Efﬁcient
Inefﬁcient
(iterative)
(iterative)
Necessary
Necessary

Necessary

Unnecessary Unnecessary

Inefﬁcient
(iterative)

Unnecessary

Efﬁcient
(analytic)
Necessary

Efﬁcient
(analytic)

Unnecessary

Manual

NGIF Design
Computational

Efﬁciency

Pre-whitening

It is known that the distribution of randomly projected data is close to Gaussian [Diederichs et al.,
2013]. Based on this observation, non-Gaussian component analysis (NGCA) [Blanchard et al.,
2006] tries to ﬁnd a subspace that contains non-Gaussian signal components so that Gaussian noise
components can be projected out. NGCA is formulated in an elegant semi-parametric framework and
non-Gaussian components can be extracted without specifying their distributions. Mathematically,
NGCA can be regarded as a generalization of projection pursuit (PP) [Friedman and Tukey, 1974]
and independent component analysis (ICA) [Hyv¨arinen and Oja, 2000] to multi-dimensional and
dependent non-Gaussian components.

The ﬁrst NGCA algorithm is called multi-index PP (MIPP). PP algorithms such as Fas-
tICA [Hyv¨arinen and Oja, 2000] use a non-Gaussian index function (NGIF) to ﬁnd either a super-
Gaussian or sub-Gaussian component. MIPP uses a family of such NGIFs to ﬁnd multiple non-
Gaussian components and apply principal component analysis (PCA) to extract a non-Gaussian sub-
space. However, MIPP requires us to prepare appropriate NGIFs, which is not necessarily straight-
forward in practice. Furthermore, MIPP requires pre-whitening of data, which can be unreliable
when the data covariance matrix is ill-conditioned.

To cope with these problems, MIPP has been extended in various ways. The method called it-
erative metric adaptation for radial kernel functions (IMAK) [Kawanabe et al., 2007] tries to avoid
the manual design of NGIFs by learning the NGIFs from data in the form of radial kernel functions.
However, this learning part is computationally highly expensive and pre-whitening is still necessary.
Sparse NGCA (SNGCA) [Diederichs et al., 2010, 2013] tries to avoid pre-whitening by imposing
an appropriate constraint so that the solution is independent of the data covariance matrix. How-
ever, SNGCA involves semi-deﬁnite programming which is computationally highly demanding, and
NGIFs still need to be manually designed.

Recently, a novel approach to NGCA called least-squares NGCA (LSNGCA) has been pro-
posed [Sasaki et al., 2016]. Based on the gradient of the log-density function, LSNGCA constructs
a vector that belongs to the non-Gaussian subspace from each sample. Then the method of LS log-
density gradients (LSLDG) [Cox, 1985, Sasaki et al., 2014] is employed to directly estimate the
log-density gradient without density estimation. Finally, the principal subspace of the set of vec-
tors generated from all samples is extracted by eigendecomposition. LSNGCA is computationally
efﬁcient and no manual design of NGIFs is involved. However, it still requires pre-whitening of data.
The existing NGCA methods reviewed above are summarized in Table 1. In this paper, we pro-
pose a novel NGCA method that is computationally efﬁcient, no manual design of NGIFs is involved,
and no pre-whitening is necessary. Our proposed method is essentially an extention of LSNGCA so
that the covariance of data is implicitly handled without explicit pre-whitening or explicit constraints.
Through experiments, we demonstrate that our proposed method, called whitening-free LSNGCA
(WF-LSNGCA), performs very well even when the data covariance matrix is ill-conditioned.

2

2 Non-Gaussian Component Analysis

In this section, we formulate the problem of NGCA and review the MIPP and LSNGCA methods.

2.1 Problem Formulation
Suppose that we are given a set of d-dimensional i.i.d. samples of size n, {xi|xi ∈ Rd}n
are generated by the following model:

i=1, which

xi = Asi + ni,

(1)
where si ∈ Rm (m ≤ d) is an m-dimensional signal vector independently generated from an un-
known non-Gaussian distribution (we assume that m is known), ni ∈ Rd is a noise vector indepen-
dently generated from a centered Gaussian distribution with an unknown covariance matrix Q, and
A ∈ Rd×m is an unknown mixing matrix of rank m. Under this data generative model, probability
density function p(x) that samples {xi}n
i=1 follow can be expressed in the following semi-parametric
form [Blanchard et al., 2006]:

p(x) = f (B(cid:62)x)φQ(x),

(2)
where f is an unknown smooth positive function on Rm, B ∈ Rd×m is an unknown linear mapping,
φQ is the centered Gaussian density with the covariance matrix Q, and ·(cid:62) denotes the transpose.
We note that decomposition (2) is not unique; multiple combinations of B and f can give the same
probability density function. Nevertheless, the following m-dimensional subspace E, called the non-
Gaussian index space, can be determined uniquely [Theis and Kawanabe, 2006]:

E = Null(B(cid:62))⊥ = Range(B),

(3)
where Null(B(cid:62)) denotes the null space of B(cid:62), ⊥ denotes the orthogonal complement, and
Range(B) denotes the column space of B.

The goal of NGCA is to estimate the non-Gaussian index space E from samples {xi}n

i=1.

2.2 Multi-Index Projection Pursuit
MIPP [Blanchard et al., 2006] is the ﬁrst algorithm of NGCA.

Let us whiten the samples {xi}n

i=1 so that their covariance matrix becomes identity:

yi := Σ− 1

2 xi,

where Σ is the covariance matrix of x. In practice, Σ is replaced by the sample covariance matrix.
Then, for an NGIF h, the following vector β(h) was shown to belong to the non-Gaussian index
space E [Blanchard et al., 2006]:

where ∇y denotes the differential operator w.r.t. y and E[·] denotes the expectation over p(x):

β(h) := E [yh(y) − ∇yh(y)] ,

(cid:90)

A(x)p(x)dx.
MIPP generates a set of such vectors from various NGIFs {hl}L

E[A(x)] :=

(cid:98)βl :=

1
n

n(cid:88)

l=1:
[yihl(yi) − ∇yhl(yi)] ,

i=1

3

(4)

where the expectation is estimated by the sample average. Then(cid:98)βl is normalized as

(cid:98)βl

(cid:98)βl ←

(cid:118)(cid:117)(cid:117)(cid:116) 1

n

n(cid:88)

(cid:107)yihl(yi) − ∇yhl(yi)(cid:107)2 − (cid:107)(cid:98)βl(cid:107)2

,

(5)

by which (cid:107)(cid:98)βl(cid:107) is proportional to its signal-to-noise ratio. Then vectors(cid:98)βl with their norm less than
a pre-speciﬁed threshold τ > 0 are eliminated. Finally, PCA is applied to the remaining vectors(cid:98)βl

i=1

to obtain an estimate of the non-Gaussian index space E.

The performance of MIPP strongly depends on the choice of NGIF h. To improve the perfor-

mance, MIPP actively searches informative h as follows. First, the form of h is restricted to

where w ∈ Rd denotes a unit-norm vector and s is a smooth real function. Then, estimated vector(cid:98)β

h(y) = s(w(cid:62)y),

is written as

(cid:98)β =

1
n

n(cid:88)

i=1

(cid:0)yis(w(cid:62)yi) − s(cid:48)(w(cid:62)yi)w(cid:1) ,

where s(cid:48) is the derivative of s. This equation is actually equivalent to a single iteration of the PP
algorithm called FastICA [Hyv¨arinen, 1999]. Based on this fact, the parameter w is optimized by
iteratively applying the following update rule until convergence:

(cid:0)yis(w(cid:62)yi) − s(cid:48)(w(cid:62)yi)w(cid:1)

(cid:80)n
(cid:107)(cid:80)n
i=1 (yis(w(cid:62)yi) − s(cid:48)(w(cid:62)yi)w)(cid:107) .

i=1

w ←

The superiority of MIPP has been investigated both theoretically and experimentally [Blanchard
et al., 2006]. However, MIPP has weaknesses that NGIFs should be manually designed and pre-
whitening is necessary.

2.3 Least-Squares Non-Gaussian Component Analysis
LSNGCA [Sasaki et al., 2016] is a recently proposed NGCA algorithm that does not require manual
design of NGIFs (Table 1). Here the algorithm of LSNGCA is reviewed, which will be used for
further developing a new method in the next section.

2.3.1 Derivation
For whitened samples {yi}n
as
where (cid:101)f is an unknown smooth positive function on Rm and (cid:101)B ∈ Rd×m is an unknown linear

i=1, the semi-parametric form of NGCA given in Eq.(2) can be simpliﬁed

p(y) = (cid:101)f ((cid:101)B

mapping. Under this simpliﬁed semi-parametric form, the non-Gaussian index space E can be rep-
resented as

y)φI d (y),

(6)

(cid:62)

Taking the logarithm and differentiating the both sides of Eq.(6) w.r.t. y yield

(cid:62)

y),

(7)

E = Σ− 1

∇y ln p(y) + y = (cid:101)B∇(cid:101)B

2 Range((cid:101)B).
ln(cid:101)f ((cid:101)B

(cid:62)

y

4

(cid:62)

(cid:62)

y

denotes the differential operator w.r.t. (cid:101)B

where ∇(cid:101)B
(cid:80)n
i=1 u(yi)u(yi)(cid:62) and extracting the m leading eigenvectors allow us to recover Range((cid:101)B). In

to the non-Gaussian index space E.

Then applying eigendecomposition to

u(y) := ∇y ln p(y) + y

y. This implies that

LSNGCA, the method of LSLDG [Cox, 1985, Sasaki et al., 2014] is used to estimate the log-density
gradient ∇y ln p(y) included in u(y), which is brieﬂy reviewed below.

belongs

2.3.2 LSLDG

Let ∂j denote the differential operator w.r.t. the j-th element of y. LSLDG ﬁts a model g(j)(y) to
∂j ln p(y), the j-th element of log-density gradient ∇y ln p(y), under the squared loss:

J(g(j)) := E[(g(j)(y) − ∂j ln p(y))2] − E[(∂j ln p(y))2]

= E[g(j)(y)2] − 2E[g(j)(y)∂j ln p(y)].

(8)

The second term in Eq.(8) yields

(cid:90)

(cid:90)

(cid:90)

E[g(j)(y)∂j ln p(y)] =

g(j)(y)(∂j ln p(y))p(y)dy

g(j)(y)∂jp(y)dy = −

=
= −E[∂jg(j)(y)],

∂jg(j)(y)p(y)dy

where the second-last equation follows from integration by parts under
lim|y(j)|→∞ g(j)(y)p(y). Then sample approximation yields

the assumption

J(g(j)) = E[g(j)(y)2 − 2∂jg(j)(y)]

[g(j)(yi)2 + 2∂jg(j)(yi)].

≈ 1
n

n(cid:88)

i=1

b(cid:88)

k=1

As a model of the log-density gradient, LSLDG uses a linear-in-parameter form:

g(j)(y) =

θk,jψk,j(y) = θ

(cid:62)
j ψj(y),

(9)

(10)

where b denotes the number of basis functions, θj := (θ1,j, . . . , θb,j)(cid:62) is a parameter vector to be
estimated, and ψj(y) := (ψ1,j(y), . . . , ψb,j(y))(cid:62) is a basis function vector. The parameter vector
θj is learned by solving the following regularized empirical optimization problem:

where λj > 0 is the regularization parameter,

(cid:98)θj = argmin
n(cid:88)

θj

(cid:98)Gj =

1
n

(cid:104)

j (cid:98)Gjθj + 2θ

(cid:62)

θ

j (cid:98)hj + λj(cid:107)θj(cid:107)2(cid:105)

(cid:62)

,

ψj(yi)ψj(yi)(cid:62), (cid:98)hj =

n(cid:88)

i=1

1
n

∂jψj(yi).

i=1

5

Algorithm 1 Pseudo-code of WF-LSNGCA.
input Element-wise standardized data samples: {xi}n

1: Obtain an estimate (cid:98)v(x) of v(x) = ∇x ln p(x) − ∇2
2: Apply eigendecomposition to(cid:80)n

Section 3.2.

i=1.

orthonormal basis of non-Gaussian index space E.

i=1(cid:98)v(xi)(cid:98)v(xi)(cid:62) and extract the m leading eigenvectors as an

x ln p(x)x by the method described in

This optimization problem can be analytically solved as

(cid:17)−1(cid:98)hj,

(cid:98)θj = −(cid:16)(cid:98)Gj + λjI b
(cid:98)g(j)(y) =(cid:98)θ

(cid:62)
j ψj(y).

where I b is the b-by-b identity matrix. Finally, an estimator of the log-density gradient g(j)(y) is
obtained as

All tuning parameters such as the regularization parameter λj and some parameters included in the
basis function ψk,j(y) can be systematically chosen based on cross-validation w.r.t. Eq.(9).

3 Whitening-Free LSNGCA

In this section, we propose a novel NGCA algorithm that does not involve pre-whitening. A pseudo-
code of the proposed method, which we call whitening-free LSNGCA (WF-LSNGCA), is summa-
rized in Algorithm 1.

3.1 Derivation
Unlike LSNGCA which used the simpliﬁed semi-parametric form (6), we directly use the original
semi-parametric form (2) without whitening. Taking the logarithm and differentiating the both sides
of Eq.(2) w.r.t. x yield

∇x ln p(x) + Q−1x = B∇B(cid:62)x ln f (B(cid:62)x),

(11)
where ∇x denotes the derivative w.r.t. x and ∇B(cid:62)x denotes the derivative w.r.t. B(cid:62)x. Further taking
the derivative of Eq.(11) w.r.t. x yields
Q−1 = −∇2

B(cid:62)x ln f (B(cid:62)x)B(cid:62),

x ln p(x) + B∇2

(12)

where ∇2

x denotes the second derivative w.r.t. x. Substituting Eq.(12) back into Eq.(11) yields

∇x ln p(x) − ∇2

(cid:16)∇B(cid:62)x ln f (B(cid:62)x) − ∇2

x ln p(x)x

(cid:17)
B(cid:62)x ln f (B(cid:62)x)B(cid:62)x

.

(13)

= B

This implies that

(cid:80)n
x ln p(x)x
Then we apply eigendecomposition to
i=1 v(xi)v(xi)(cid:62) and extract the m leading eigenvectors as an orthonormal basis of non-Gaussian

belongs to the non-Gaussian index space E.

v(x) := ∇x ln p(x) − ∇2

index space E.

Now the remaining task is to approximate v(x) from data, which is discussed below.

6

3.2 Estimation of v(x)
Let v(j)(x) be the j-th element of v(x):

v(j)(x) = ∂j ln p(x) − (∇x∂j ln p(x))

(cid:62)

x.

To estimate v(j)(x), let us ﬁt a model w(j)(x) to it under the squared loss:
R(w(j)) := E[(w(j)(x) − v(j)(x))2] − E[v(j)(x)2]

The second term in Eq.(14) yields

= E[w(j)(x)2] − 2E[w(j)(x)v(j)(x)]
= E[w(j)(x)2] − 2E[w(j)(x)∂j ln p(x)]
+ 2E[w(j)(x) (∇x∂j ln p(x))

x].

(cid:62)

(cid:90)

(cid:90)

(cid:90)

E[w(j)(x)∂j ln p(x)] =

w(j)(x)(∂j ln p(x))p(x)dx

w(j)(x)∂jp(x)dx = −

=
= −E[∂jw(j)(x)],

∂jw(j)(x)p(x)dx

where the second-last equation follows from integration by parts under
the assumption
lim|x(j)|→∞ w(j)(x)p(x). ∂j ln p(x) included in the third term in Eq.(14) may be replaced with

R(w(j)) ≈ E[w(j)(x)2 + 2∂jw(j)(x)

the LSLDG estimator(cid:98)g(j)(x) reviewed in Section 2.3.2. Then we have
+ 2w(j)(x)(∇x(cid:98)g(j)(x))(cid:62)x]
+ 2w(j)(xi)(∇x(cid:98)g(j)(xi))(cid:62)xi].
b(cid:88)

Here, let us employ the following linear-in-parameter model as w(j)(x):

[w(j)(xi)2 + 2∂jw(j)(xi)

n(cid:88)

≈ 1
n

i=1

αk,jϕk,j(x) = α(cid:62)

j ϕj(x),

w(j)(x) :=

k=1

:= (α1,j, . . . , αb,j)(cid:62) is a parameter vector to be estimated, and ϕj(x)

where αj
:=
(ϕ1,j(x), . . . , ϕb,j(x))(cid:62) is a basis function vector. The parameter vector αj is learned by mini-
mizing the following regularized empirical optimization problem:

(cid:104)

j (cid:98)Sjαj + 2α(cid:62)

α(cid:62)

j(cid:98)tj(x) + γj(cid:107)αj(cid:107)2(cid:105)

,

where γj > 0 is the regularization parameter,

(14)

(15)

(16)

αj

(cid:98)αj = argmin
(cid:98)Sj =
(cid:98)tj =

n(cid:88)
n(cid:88)

1
n

i=1

1
n

ϕj(xi)ϕj(xi)(cid:62),
(cid:18)

∂jϕj(xi) + ∂jϕj(xi)

(cid:16)∇x(cid:98)g(j)(xi)
(cid:17)(cid:62)

(cid:19)

xi

.

i=1

7

This optimization problem can be analytically solved as

Finally, an estimator of v(j)(x) is obtained as

(cid:17)−1(cid:98)tj.

(cid:98)αj = −(cid:16)(cid:98)Sj + γjI b
(cid:98)v(j)(x) = (cid:98)α

(cid:62)
j ϕj(x).

All tuning parameters such as the regularization parameter γj and some parameters included in the
basis function ϕk,j(y) can be systematically chosen based on cross-validation w.r.t. Eq.(15).

3.3 Theoretical Analysis
Here, we investigate the convergence rate of WF-LSNGCA in a parametric setting.

Let g∗(x) be the optimal estimate to ∇x ln p(x) given by LSLDG based on the linear-in-

parameter model g(x), and let
S∗
t∗
j = E
α∗
j = argminα

j = E(cid:2)ϕj(x)ϕj(x)(cid:62)(cid:3) ,
(cid:8)α(cid:62)S∗

(cid:20)

∂jϕj(x) + ∂jϕj(x)

j α + 2α(cid:62)t∗

(cid:21)
(cid:16)∇xg∗(j)(x)
(cid:17)(cid:62)
j α(cid:62)α(cid:9) ,

j + γ∗

x

,

w∗(j)(x) = α∗(cid:62)

j ϕj(x).

where (S∗
j + γ∗
deﬁnite, and γ∗

j I b) must be strictly positive deﬁnite. In fact, S∗
j = 0 is thus allowed in our theoretical analysis.

We have the following theorem (its proof is given in Section 3.4):

Theorem 1. As n → ∞, for any x,

(cid:107)(cid:98)v(x) − w∗(x)(cid:107)2 = Op

(cid:16)

n−1/2(cid:17)

,

j should already be strictly positive

provided that γj for all j converge in O(n−1/2) to γ∗

j , i.e., limn→∞ n1/2|γj − γ∗

j | < ∞.

It guarantees that for any x, the estimate(cid:98)v(x) in WF-LSNGCA converges to the optimal estimate

Theorem 1 is based on the theory of perturbed optimizations [Bonnans and Cominetti, 1996,
Bonnans and Shapiro, 1998] as well as the convergence of LSLDG shown in Sasaki et al. [2016].
w∗(x) based on the linear-in-parameter model w(x), and the convergence rate is the optimal para-
metric convergence rate Op(n−1/2). Moreover, Theorem 1 is about the estimation error only, because
the approximation error is difﬁcult to analyze. Indeed, approximation errors exist in two places, from
w∗(x) to v(x) in WF-LSNGCA itself and from g∗(x) to ∇x ln p(x) in the plug-in LSLDG estima-
tor. Anyway, LSNGCA also relies on LSLDG and thus can neither avoid the approximation error
introduced by LSLDG, so the convergence of WF-LSNGCA should be as good as LSNGCA.

Theorem 1 is basically a theoretical guarantee that is similar to Part One in the proof of Theorem
1 in Sasaki et al. [2016]. Hence, based on Theorem 1, we can go along the line of Part Two in the
proof of Theorem 1 in Sasaki et al. [2016] and obtain the following corollary.

(cid:80)n
Corollary 2. Deﬁne matrices (cid:98)Γ = 1
i=1(cid:98)v(xi)(cid:98)v(xi)(cid:62) and Γ∗ = E[w∗(x)w∗(x)(cid:62)] for eigen-
decomposition. Given the estimated subspace (cid:98)E based on n samples and the optimal estimated sub-
space E∗ based on inﬁnite data, denote by(cid:98)E ∈ Rd×m the matrix form of an arbitrary orthonormal
basis of (cid:98)E and by E∗ ∈ Rd×m that of E∗. Deﬁne the distance between subspaces as

n

D((cid:98)E, E∗) = inf(cid:98)E,E∗ (cid:107)(cid:98)E − E∗(cid:107)Fro,

8

where (cid:107) · (cid:107)Fro stands for the Frobenius norm. Then, as n → ∞,

D((cid:98)E, E∗) = Op

(cid:16)

n−1/2(cid:17)

,

provided that

1. γj for all j converge in O(n−1/2) to γ∗
j ;
2. ϕj(x) are well-chosen basis functions, such that the ﬁrst m eigenvalues of Γ∗ are neither 0

nor +∞.

3.4 Proof of Theorem 1
Here, Theorem 1 is proved.

Step 1. First of all, we establish the growth condition (see Deﬁnition 6.1 in Bonnans and Shapiro
[1998]). Denote the expected and empirical objective functions by
j + γ∗

j α(cid:62)α,

j α + 2α(cid:62)t∗

(cid:98)Rj(α) = α(cid:62)(cid:98)Sjα + 2α(cid:62)(cid:98)tj + γjα(cid:62)α.
j (α) = α(cid:62)S∗
R∗
j (α),(cid:98)αj = argminα (cid:98)Rj(α), and we have

j = argminα R∗

Then α∗
Lemma 1. Let j be the smallest eigenvalue of (S∗
condition holds
j (α∗

j (α) ≥ R∗
R∗

j + γ∗
j ) + j(cid:107)α − α∗
j(cid:107)2
2.

j I b), then the following second-order growth

Proof. R∗

j (α) must be strongly convex with parameter at least 2j. Hence,

j (α) ≥ R∗
R∗

j (α∗
+ (α − α∗
j (α∗
where we used the optimality condition ∇R∗

j ) + (∇R∗
j )(cid:62)(S∗

j ))(cid:62)(α − α∗
j (α∗
j )
j I b)(α − α∗
j + γ∗
j )
j ) + j(cid:107)α − α∗
j(cid:107)2
2,
j (α∗

j ) = 0.

≥ R∗

Step 2. Second, we study the stability (with respect to perturbation) of R∗

j (α) at α∗

j . Let

+, ut ∈ Rb, uγ ∈ R}
+ ⊂ Rb×b is the cone of b-by-b symmetric positive

u = {uS ∈ S b
be a set of perturbation parameters, where S b
semi-deﬁnite matrices. Deﬁne our perturbed objective function by
j + uS)α + 2α(cid:62)(t∗
j + uγ)α(cid:62)α.

Rj(α, u) = α(cid:62)(S∗
+ (γ∗

j (α) = Rj(α, 0), and then the stability of R∗

It is clear that R∗
follows.
Lemma 2. The difference function Rj(α, u) − R∗

ω(u) = O((cid:107)uS(cid:107)Fro + (cid:107)ut(cid:107)2 + |uγ|)

on a sufﬁciently small neighborhood of α∗
j .

9

j + ut)

j (α) at α∗

j can be characterized as

j (α) is Lipschitz continuous in α modulus

Proof. The difference function is

Rj(α, u) − R∗

j (α) = α(cid:62)uSα + 2α(cid:62)ut + uγα(cid:62)α,

with a partial gradient

(Rj(α, u) − R∗
Notice that due to the (cid:96)2-regularization in R∗
δ-ball of α∗

∂
∂α
j ) = {α | (cid:107)α − α∗

j , i.e., Bδ(α∗

j (α)) = 2uSα + 2ut + 2uγα.
j (α), ∃M > 0 such that (cid:107)α∗

j(cid:107)2 ≤ δ}, it is easy to see that ∀α ∈ Bδ(α∗
j ),

j(cid:107)2 ≤ M. Now given a

and consequently

(cid:13)(cid:13)(cid:13)(cid:13) ∂

(cid:107)α(cid:107)2 ≤ (cid:107)α − α∗

j(cid:107)2 + (cid:107)α∗

j(cid:107)2 ≤ δ + M,

(Rj(α, u) − R∗

j (α))

∂α
≤ 2(δ + M )((cid:107)uS(cid:107)Fro + |uγ|) + 2(cid:107)ut(cid:107)2.
∂α (Rj(α, u) − R∗

(cid:13)(cid:13)(cid:13)(cid:13)2

j (α)) has a bounded norm of order O((cid:107)uS(cid:107)Fro +
j (α) is Lipschitz continuous on

This says that the gradient ∂
(cid:107)ut(cid:107)2 +|uγ|), and proves that the difference function Rj(α, u)− R∗
the ball Bδ(α∗

j ), with a Lipschitz constant of the same order.
j (α) grows quickly when α leaves α∗
Step 3. Lemma 1 ensures the unperturbed objective R∗
j ;
Lemma 2 ensures the perturbed objective Rj(α, u) changes slowly for α around α∗
j , where the
slowness is compared with the perturbation u it suffers. Based on Lemma 1, Lemma 2, and Propo-
sition 6.1 in Bonnans and Shapiro [1998],
j(cid:107)2 ≤ ω(u)
j

since (cid:98)αj is the exact solution to (cid:98)Rj(α) = Rj(α, u) given uS = (cid:98)Sj − S∗
j , ut = (cid:98)tj − t∗
According to the central limit theorem (CLT), (cid:107)uS(cid:107)Fro = Op(n−1/2). Consider(cid:98)tj − t∗

= O((cid:107)uS(cid:107)Fro + (cid:107)ut(cid:107)2 + |uγ|),

(cid:107)(cid:98)αj − α∗

uγ = γj − γ∗
j .

j , and

j :

(cid:98)tj − t∗

j =

1
n

i=1

n(cid:88)
∂jϕj(xi) − E(cid:2)∂jϕj(x)(cid:3)
(cid:17)(cid:62)
(cid:16)∇x(cid:98)g(j)(xi)
n(cid:88)
(cid:20)
(cid:21)
(cid:16)∇xg∗(j)(x)
(cid:17)(cid:62)

∂jϕj(xi)

∂jϕj(x)

i=1

x

+

1
n
− E

xi

.

The ﬁrst half is clearly Op(n−1/2) due to CLT. For the second half, the estimate(cid:98)g(j)(x) given by
1 in Sasaki et al. [2016], and ∇x(cid:98)g(j)(x) converges to ∇xg∗(j)(x) in the same order because the basis

LSLDG converges to g∗(j)(x) for any x in Op(n−1/2) according to Part One in the proof of Theorem

functions in ψj(x) are all derivatives of Gaussian functions. Consequently,

1
n

n(cid:88)

i=1

− 1
n

∂jϕj(xi)

n(cid:88)

(cid:16)∇x(cid:98)g(j)(xi)
(cid:17)(cid:62)
(cid:16)∇xg∗(j)(xi)
(cid:17)(cid:62)

xi

∂jϕj(xi)

i=1

10

xi = Op(n−1/2),

since ∇x(cid:98)g(j)(x) converges to ∇xg∗(j)(x) for any x in Op(n−1/2), and

1
n

n(cid:88)

i=1

− E

∂jϕj(xi)

(cid:20)

∂jϕj(x)

xi

(cid:21)

(cid:16)∇xg∗(j)(xi)
(cid:17)(cid:62)
(cid:17)(cid:62)
(cid:16)∇xg∗(j)(x)
(cid:16)

j(cid:107)2 = Op

= Op(n−1/2)

x

due to CLT, which proves (cid:107)ut(cid:107)2 = Op(n−1/2). Furthermore, we have already assumed that |uγ| =
O(n−1/2). Hence, as n → ∞,

(cid:107)(cid:98)αj − α∗

n−1/2(cid:17)
|(cid:98)v(j)(x) − w∗(j)(x)| ≤ (cid:107)(cid:98)αj − α∗

Step 4. Finally, for any x, the gap of(cid:98)v(j)(x) and w∗(j)(x) is bounded by
j(cid:107)2 · (cid:107)ϕj(x)(cid:107)2,
(cid:16)

|(cid:98)v(j)(x) − w∗(j)(x)| ≤ O((cid:107)(cid:98)αj − α∗

j(cid:107)2) = Op

.

n−1/2(cid:17)

.

where the Cauchy-Schwarz inequality is used. Since the basis functions in ϕj(x) are again all
derivatives of Gaussian functions, (cid:107)ϕj(x)(cid:107)2 must be bounded uniformly, and then

Applying the same argument for all j = 1, . . . , d completes the proof.

4 Experiments

In this section, we experimentally investigate the performance of MIPP, LSNGCA, and WF-
LSNGCA.

4.1 Conﬁgurations of NGCA Algorithms
Each algorithm is conﬁgured as follows.

MIPP: We use the MATLAB script which was used in the original MIPP paper [Blanchard et al.,
2006]1. In this script, NGIFs of the form sk

(cid:18)

(cid:19)

m(z) (m = 1, . . . , 1000, k = 1, . . . , 4) are used:
− z2
2σ2
m
s3
m(z) = sin(bmz), s4
m(z) = cos(bmz),

m(z) = tanh (amz) ,

s1
m(z) = z3exp

, s2

where σm, am, and bm are scalars chosen at the regular intervals from σm ∈ [0.5, 5], am ∈ [0.05, 5],
and bm ∈ [0.05, 4]. The cut-off threshold τ is set at 1.6 and the number of FastICA iterations is set
at 10 (see Section 2.2).

1http://www.ms.k.u-tokyo.ac.jp/software.html

11

Independent Gaussian

(a)
Mixture

Dependent

(b)
Gaussian

Super-

(c) Dependent Sub-Gaussian (d) Dependent Super- & Sub-

Gaussian

Figure 1: Distributions of non-Gaussian components.

Independent Gaussian

(a)
Mixture

Dependent

(b)
Gaussian

Super-

(c) Dependent Sub-Gaussian (d) Dependent Super- and

Sub-Gaussian

Figure 2: Averages and standard deviations of the subspace estimation error by MIPP, LSNGCA, and
WF-LSNGCA as the function of the condition-number controller r over 50 simulations on artiﬁcial
datasets.

LSNGCA: Following Sasaki et al. [2014], the derivative of the Gaussian kernel is used as the basis
function ψk,j(y) in the linear-in-parameter model (10):

(cid:32)
−(cid:107)y − ck(cid:107)2

(cid:33)

ψk,j(y) = ∂jexp

,

2σ2
j

where σj > 0 is the Gaussian bandwidth and ck is the Gaussian center randomly selected from
the whitened data samples {yi}n
i=1. The number of basis functions is set at b = 100. For model
selection, 5-fold cross-validation is performed with respect to the hold-out error of Eq.(9) using 10
candidate values at the regular intervals in logarithmic scale for Gaussian bandwidth σj ∈ [10−1, 101]
and regularization parameter λj ∈ [10−5, 101].

WF-LSNGCA: Similarly to LSNGCA, the derivative of the Gaussian kernel is used as the basis
function ϕk,j(x) in the linear-in-parameter model (16). For model selection, 5-fold cross-validation
is performed with respect to the hold-out error of Eq.(15) in the same way as LSNGCA.

4.2 Artiﬁcial Datasets
Let x = (s1, s2, n3, . . . , n10)(cid:62), where (s1, s2)(cid:62) are the 2-dimensional non-Gaussian signal compo-
nents and (n3, . . . , n10)(cid:62) are the 8-dimensional Gaussian noise components. For the non-Gaussian
signal components, we consider the four distributions plotted in Figure 1, which are detailed in Ap-
pendix A. For the Gaussian noise components, we include a certain parameter r ≥ 0, which controls
the condition number; the larger r is, the more ill-posed the data covariance matrix is. The details
are also described in Appendix A.

12

00.20.40.60.8r for Noise Covariances00.511.5Subspace Estimation Error×10-3MIPPLSNGCAWF-LSNGCA00.20.40.60.8r for Noise Covariances00.010.020.030.040.05Subspace Estimation ErrorMIPPLSNGCAWF-LSNGCA00.20.40.60.8r for Noise Covariances00.010.020.030.040.05Subspace Estimation ErrorMIPPLSNGCAWF-LSNGCA00.20.40.60.8r for Noise Covariances0246Subspace Estimation Error×10-3MIPPLSNGCAWF-LSNGCA2(cid:88)

i=1

(cid:107)(cid:98)ei − ΠE(cid:98)ei(cid:107)2 ,

(17)

We generate n = 2000 samples for each case, and standardize each element of the data before
applying NGCA algorithms. The performance of NGCA algorithms is measured by the following
subspace estimation error:

ε(E,(cid:98)E) :=
i=1 is an orthonormal basis in (cid:98)E.

1
2

where E is the true non-Gaussian index space, (cid:98)E is its estimate, ΠE is the orthogonal projection on
E, and {(cid:98)ei}2

The averages and the standard derivations of the subspace estimation error over 50 runs for MIPP,
LSNGCA, and WF-LSNGCA are depicted in Figure 2. This shows that, for all 4 cases, the error of
MIPP grows rapidly as r increases. On the other hand, LSNGCA and WF-LSNGCA perform much
stably against the change in r. However, LSNGCA performs poorly for (a). Overall, WF-LSNGCA
is shown to be much more robust against ill-conditioning than MIPP and LSNGCA.

4.3 Benchmark Datasets
Finally, we evaluate the performance of NGCA methods using the LIBSVM binary classiﬁcation
benchmark datasets2 [Chang and Lin, 2011]. For an m-dimensional dataset, we append (d − m)-
dimensional noise dimensions following the standard Gaussian distribution so that all datasets have d
dimensions. Then we use MIPP, LSNGCA, WF-LSNGCA, as well as PCA to obtain m-dimensional
expressions, and apply SVM3 to evaluate the test misclassiﬁcation rate. As a baseline, we also
evaluate the misclassiﬁcation rate by the raw SVM without dimension reduction.

The averages and standard deviations of the misclassiﬁcation rate over 50 runs for d =
30, 50, 100 are summarized in Table 2 and 3. The results show that WF-LSNGCA overall compares
favorably with other methods.

5 Conclusions

In this paper, we proposed a novel NGCA algorithm which is computationally efﬁcient, no manual
design of non-Gaussian index functions is required, and pre-whitening is not involved. Through
experiments, we demonstrated that the effectiveness of the proposed method.

References

G. Blanchard, M. Kawanabe, M. Sugiyama, V. Spokoiny, and K. M¨uller. In search of non-Gaussian
components of a high-dimensional distribution. Journal of Machine Learning Research, 7:247–
282, 2006.

F. Bonnans and R. Cominetti. Perturbed optimization in Banach spaces I: A general theory based on
a weak directional constraint qualiﬁcation; II: A theory based on a strong directional qualiﬁcation
condition; III: Semiinﬁnite optimization. SIAM Journal on Control and Optimization, 34:1151–
1171, 1172–1189, and 1555–1567, 1996.

F. Bonnans and A. Shapiro. Optimization problems with perturbations, a guided tour. SIAM Review,

40(2):228–264, 1998.

2 Preprocessing of the data is detailed in Appendix B.
3We used LIBSVM with MATLAB [Chang and Lin, 2011].

13

Table 2: Averages (and standard deviations in the parentheses) of the misclassiﬁcation rates for the
LIBSVM datasets over 50 runs. The best and comparable algorithms judged by the two-sample t-test
at the signiﬁcance level 5% are expressed as boldface.

Dataset [m,n] No Dim. Red. PCA MIPP LSNGCA WF-LSNGCA

d = 30

vehicle
[18, 200]
svmguide3
[21, 200]

skin nonskin

[3, 2000]
svmguide1
[3, 2000]
shuttle
[9, 2000]
SUSY

[18, 2000]

ijcnn1

[22, 2000]

0.290
(0.038)
0.317
(0.039)
0.041
(0.005)
0.078
(0.006)
0.024
(0.004)
0.231
(0.011)
0.073
(0.006)

0.242
0.335
(0.037) (0.031)
0.334
0.315
(0.041) (0.032)
0.347
0.020
(0.013) (0.004)
0.152
0.068
(0.010) (0.006)
0.013
0.129
(0.003) (0.009)
0.253
0.228
(0.014) (0.010)
0.098
0.239
(0.009) (0.014)

0.248
(0.030)
0.309
(0.042)
0.020
(0.004)
0.068
(0.005)
0.111
(0.009)
0.222
(0.011)
0.239
(0.015)

0.249
(0.042)
0.293
(0.034)
0.016
(0.009)
0.051
(0.010)
0.007
(0.002)
0.224
(0.011)
0.056
(0.006)

Dataset [m,n] No Dim. Red. PCA MIPP LSNGCA WF-LSNGCA

d = 50

vehicle
[18, 200]
svmguide3
[21, 200]

skin nonskin

[3, 2000]
svmguide1
[3, 2000]
shuttle
[9, 2000]
SUSY

[18, 2000]

ijcnn1

[22, 2000]

0.338
(0.037)
0.342
(0.035)
0.041
(0.004)
0.088
(0.007)
0.031
(0.004)
0.238
(0.010)
0.102
(0.007)

0.401
0.321
(0.030) (0.037)
0.346
0.350
(0.034) (0.038)
0.354
0.070
(0.012) (0.025)
0.158
0.074
(0.012) (0.006)
0.024
0.130
(0.007) (0.010)
0.271
0.229
(0.012) (0.010)
0.274
0.232
(0.014) (0.012)

0.325
(0.049)
0.330
(0.037)
0.027
(0.015)
0.076
(0.006)
0.118
(0.014)
0.222
(0.012)
0.230
(0.011)

0.284
(0.043)
0.309
(0.035)
0.030
(0.014)
0.054
(0.008)
0.007
(0.002)
0.228
(0.010)
0.057
(0.006)

C. Chang and C. Lin. LIBSVM: A library for support vector machines. ACM Transactions on

Intelligent Systems and Technology, 2(3):27, 2011.

D. D. Cox. A penalty method for nonparametric estimation of the logarithmic derivative of a density

function. Annals of the Institute of Statistical Mathematics, 37(1):271–288, 1985.

E. Diederichs, A. Juditsky, V. Spokoiny, and C. Sch¨utte. Sparse non-Gaussian component analysis.

IEEE Transactions on Information Theory, 56(6):3033–3047, 2010.

E. Diederichs, A. Juditsky, A. Nemirovski, and V. Spokoiny. Sparse non Gaussian component anal-

ysis by semideﬁnite programming. Machine learning, 91(2):211–238, 2013.

14

Table 3: Averages (and standard deviations in the parentheses) of the misclassiﬁcation rates for the
LIBSVM datasets over 50 runs. The best and comparable algorithms judged by the two-sample t-test
at the signiﬁcance level 5% are expressed as boldface.

Dataset [m,n] No Dim. Red. PCA MIPP LSNGCA WF-LSNGCA

d = 100

vehicle
[18, 200]
svmguide3
[21, 200]

skin nonskin

[3, 2000]
svmguide1
[3, 2000]
shuttle
[9, 2000]
SUSY

[18, 2000]

ijcnn1

[22, 2000]

0.381
(0.034)
0.361
(0.034)
0.044
(0.005)
0.102
(0.008)
0.038
(0.005)
0.250
(0.010)
0.145
(0.008)

0.428
0.454
(0.031) (0.036)
0.366
0.435
(0.033) (0.034)
0.368
0.177
(0.010) (0.047)
0.175
0.091
(0.013) (0.010)
0.069
0.176
(0.013) (0.028)
0.280
0.233
(0.010) (0.011)
0.317
0.230
(0.012) (0.012)

0.442
(0.038)
0.423
(0.035)
0.179
(0.057)
0.103
(0.063)
0.250
(0.060)
0.228
(0.011)
0.242
(0.013)

0.346
(0.051)
0.348
(0.039)
0.070
(0.061)
0.065
(0.012)
0.019
(0.005)
0.227
(0.010)
0.083
(0.014)

J. H. Friedman and J. W. Tukey. A projection pursuit algorithm for exploratory data analysis. IEEE

Transactions on Computers, C-23(9):881–890, 1974.

A. Hyv¨arinen. Fast and robust ﬁxed-point algorithms for independent component analysis. IEEE

Transactions on Neural Networks, 10(3):626–634, 1999.

A. Hyv¨arinen and E. Oja. Independent component analysis: algorithms and applications. Neural

Networks, 13(4):411–430, 2000.

M. Kawanabe, M. Sugiyama, G. Blanchard, and K. M¨uller. A new algorithm of non-Gaussian com-
ponent analysis with radial kernel functions. Annals of the Institute of Statistical Mathematics, 59
(1):57–75, 2007.

H. Sasaki, A. Hyv¨arinen, and M. Sugiyama. Clustering via mode seeking by direct estimation of the
gradient of a log-density. In Machine Learning and Knowledge Discovery in Databases, pages
19–34. Springer, 2014.

H. Sasaki, G. Niu, and M. Sugiyama. Non-Gaussian component analysis with log-density gradient

estimation. Technical Report 1601.07665, arXiv, 2016.

F. J. Theis and M. Kawanabe. Uniqueness of non-Gaussian subspace analysis.

In Independent

Component Analysis and Blind Signal Separation, pages 917–925. Springer, 2006.

V. N. Vapnik. Statistical Learning Theory, volume 1. Wiley New York, 1998.

A Details of Artiﬁcial Datasets

Here, we describe the details of the artiﬁcial datasets used in Section 4.2.

15

Figure 3: Condition number of the data covariance matrix as a function of experiment parameter r.

• (a)

Let x = (s1, s2, n3, . . . , n10)(cid:62), where the 2-dimensional non-Gaussian signal components s =

(s1, s2)(cid:62) are generated from the following distributions (see Figure 1):

Independent Gaussian Mixture:

exp(cid:0)−(si + 3)2/2(cid:1)).
• (c) Dependent sub-Gaussian: p(s) is the uniform distribution on(cid:8)s ∈ R2|(cid:107)s(cid:107) ≤ 1(cid:9).

• (b) Dependent super-Gaussian: p(s) ∝ exp (−(cid:107)s(cid:107)).

p(s1, s2) ∝ (cid:81)2

i=1(exp(cid:0)−(si − 3)2/2(cid:1) +

• (d) Dependent super- and sub-Gaussian: p(s1) ∝ exp (−|s1|) and p(s2) is the uniform

distribution on [c, c + 1], where c = 0 if |s1| ≤ log 2 and c = −1 otherwise.

(n3, . . . , n10)(cid:62)

are

The

noise

components

First,
(n3, . . . , n10)(cid:62) is sampled from the centered Gaussian distribution with covariance matrix
diag(10−2r, 10−2r+4r/7, 10−2r+8r/7, . . . , 102r), where diag(·) denotes the diagonal matrix. Then
(n3, . . . , n10)(cid:62) is rotated by applying the following rotation matrix R(i,j) for all i, j = 3, . . . , 10
such that i < j:

generated

follows.

as

R(i,j)
i,i = cos(π/4), R(i,j)
R(i,j)
j,i = sin(π/4), R(i,j)
k,k = 1 (k (cid:54)= i, k (cid:54)= j), R(i,j)
R(i,j)

i,j = − sin(π/4),
j,j = cos(π/4),

k,l = 0 (otherwise).

By this construction, increasing r corresponds to increasing the condition number of the data co-
variance matrix (see Figure 3). Thus, the larger r is, the more ill-posed the data covariance matrix
is.

B Preprocessing of Benchmark Datasets

In the experiments in Section 4.3, we preprocessed the LIBSVM binary classiﬁcation benchmark
datasets as follows.

• vehicle: convert original labels ‘1’ and ‘2’ to the positive label and original labels ‘3’ and ‘4’

to the negative label.

• SUSY: convert original label ‘0’ to the negative label.
• shuttle: use only the data labeled as ‘1’ and ‘4’ and regard them as positive and negative labels.

16

00.20.40.60.8r for Noise Covariances0102030Condition Number• svmguide1: mix the original training and test datasets.

Then n points are selected as training (test) samples from a dataset so that the number of positive and
negative samples are equal.

17

