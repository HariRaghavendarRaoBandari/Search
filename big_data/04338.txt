6
1
0
2

 
r
a

 

M
4
1

 
 
]

O
R
.
s
c
[
 
 

1
v
8
3
3
4
0

.

3
0
6
1
:
v
i
X
r
a

Grasping for a Purpose: Using Task Goals for

Efﬁcient Manipulation Planning

Ana Huam´an Quispe

Heni Ben Amor

Henrik I. Christensen

M. Stilman†

Abstract—In this paper we propose an approach for efﬁcient
grasp selection for manipulation tasks of unknown objects. Even
for simple tasks such as pick-and-place, a unique solution is rare
to occur. Rather, multiple candidate grasps must be considered
and (potentially) tested till a successful, kinematically feasible
path is found. To make this process efﬁcient, the grasps should
be ordered such that those more likely to succeed are tested
ﬁrst. We propose to use grasp manipulability as a metric to
prioritize grasps. We present results of simulation experiments
which demonstrate the usefulness of our metric. Additionally, we
present experiments with our physical robot performing simple
manipulation tasks with a small set of different household objects.

I. INTRODUCTION

The ability to grasp objects in order to accomplish a task
is one of the hallmarks of human intelligence. Numerous
psychological studies show that humans grasp selection de-
pends on the goal to be accomplished [14]. Decision making
during grasping is therefore not only based on stability during
manipulation, but also based on task requirements. If a speciﬁc
grasp does not facilitate the execution of the upcoming sub-
tasks, it is omitted from the reasoning process.

In contrast to that, research on robot grasp synthesis has
been tilted towards optimizing stability metrics only. A promi-
nent approach is to generate a set of physically stable grasps,
one of which is then selected by the high-level planner. If
a high-level
task planner cannot achieve the goals of the
task, it has to back track and try a different grasp. Since no
information is ﬂowing between high-level planning and lower-
level grasp generation, a large number of grasps may have to
evaluated. If the required grasp is not within the optimized set
of candidates, the entire task will fail.

In this paper, we introduce a method for manipulation
planning which uses foresight to identify tasks constraints.
Constraints extracted from subsequent sub-tasks are used to
synthesize grasps that facilitate overall task completion. Our
goal is to derive a fast planning algorithm that can efﬁciently
generate manipulation sequences for previously unseen ob-
jects. These latter properties, hence, allow a robot to perform
manipulation tasks in new environments without resorting to
prior 3D models of the object or pre-calculated grasp sets.
This ability to generalize is realized by using a super-quadric
representation of objects. We show how super-quadrics can be
extracted from a single depth image and how they can be used
to generate a large set grasp candidates.

Institute for Robotics and Intelligent Machines, Georgia Institute of
Technology, Atlanta, GA 30332, USA. ahuaman3@gatech.edu,
hbenamor@cc.gatech.edu, hic@cc.gatech.edu

Fig. 1: Example of grasp selection based on goal constraints:
The goal location of the chips box is surrounded by nearby
objects, hence an overhead grasp must be selected for ma-
nipulation. If not for the obstacles present, a different grasp
(overlayed in red) from the side would be easier to execute.

Short planning times are realized by introducing a heuristics
that efﬁciently guides the search by incorporating arm kine-
matics. Inspired by the end-comfort effect [20] in humans,
grasps are preferred which lead to a comfortable arm con-
ﬁguration at the end of a task. We borrow ideas from this
work and propose to use a metric based on manipulability as
a measurement of end-comfort.

The contributions of this paper are threefold, namely (1)
a framework for online grasp planning that incorporates fu-
ture task constraints into the grasp synthesis process, (2) an
efﬁcient grasp generation approach based on super-quadrics
that works with previously unseen objects, (3) the end-comfort
heuristics for efﬁcient search during manipulation planning.

The rest of this paper is organized as follows: Section
II present relevant work in the area of grasp synthesis and
grasp selection. Section IV presents our grasp generation
method using a primitive-based approach and in Section V
we introduce our manipulability-based strategies to prioritize
the generated grasps. Section VI shows the results of the
comparisons in simulation and the metrics we used to compare
their performance. Finally, we present the application of our
approach in our physical robot. We conclude this paper with
section VII, where we provide some discussion regarding
future work, and the advantages and shortcomings of our
approach.

Fig. 2: Manipulation planning pipeline: a partial point cloud of the object is ﬁrst analyzed for symmetries and then turned
into a superquadric representation. Grasps at the goal and the start position are generated and then prioritized according to
end-comfort. Potential grasps are then analyzed within the plan and then executed.

II. RELATED WORK

In this section we review work concerning grasp synthesis
and grasp selection. For a more detailed review of previous
research in the area, we suggest the interested reader to consult
the excellent reviews from Bohg [3] and Sahbani [21].

Pioneering work on grasp selection was developed by
Cutkosky [5], who observed that humans select grasps in order
to satisfy 3 main types of constraints: Hand, object and task-
based constraints. As pointed out by Bohg et al. in [3], there
is little work on task-dependent grasping when compared to
work focused on the ﬁrst two constraints. Hence, the main
goal for a planner is to ﬁnd a grasp such that the robot can
approach the object and execute the said grasp, without further
regard of what the robot will do once the object is picked.

Grasp generation methods vary widely depending on the
assumptions considered. In the case of grasp planning for
known objects, Ciocarlie et al. [4] presented the concept
of eigengrasps, which was exploited to generate candidate
grasps searching in a low dimensional hand posture space
using their GraspIt! simulator. Diankov generated grasps by
sampling the surfaces of object meshes and using the normals
at the sample points to guide the approach direction of the
hand [6]. Approaches using primitive representations were
also proposed such that
the grasp generation depends on
the particular primitive characterization: Miller et al. [15]
proposed to use a set of primitive shapes (cylinder, box, ball)
to decompose complex objects. Huebner and Kragic [12] used
bounding boxes, Przybylski et al. proposed the Medial Axis
representation [18], Goldfeder et al. [9] used superquadrics
due to their versatility to express different geometry types with
only 5 parameters.

In all the cases mentioned, the grasps are generated ofﬂine
and stored in a database for future use. These grasps are
usually ranked based on their force-closure properties, which
theoretically express the robustness and stability of a grasp.
One of the most popular metrics () was proposed by Ferrari
and Canny [7]. However,
it has been noted by different
authors that analytical metrics do not guaranteee a stable
grasp when executed in a real robot. This can be explained

by the fact that these classical metrics consider assumptions
that don’t always hold true in real scenarios (dynamics,
perceptual and modelling inaccuracies, friction conditions). On
the other hand, studies that consider human heuristics to guide
grasp search have shown remarkable results, outperforming
classical approaches. In [1], Balasubramanian observed that
when humans kinestetically teach a robot how to grasp objects,
they strongly tend to align the robotic hand along one of
the object’s principal axis, which later results in more robust
grasps. The author termed skewness to the metric measuring
the axis deviation. In [19], Przybylski et al. combine the latter
metric with  and use it to rank grasps produced with GraspIt!.
Berenson et al. [2] proposed a score combining 3 measures: ,
object clearance and the robot relative position to the object.
In this work we are interested in manipulation of unknown
objects. Multiple approaches of this kind have ﬂourished
during the last few years, particularly due to the advent of
affordable RGB-Depth sensors. Since the 3D information is
partial and noisy, classical approaches to grasp generation
cannot be directly used. Rather, most of the current work uses
heuristics to guide grasp generation based on local representa-
tion of the object geometry features (or global features if the
object shape is approximated). In [10], Hsiao et al. use the
bounding box of the object segmented pointcloud to calculate
grasp approach directions using a set of heuristics. We should
notice that for most of these approaches, their effectiveness
can only be veriﬁed empirically.

As we mentioned at

the beginning of this section,

the
metrics we discussed above do not consider the task to be
executed after the grasp is achieved. Some authors, however,
have investigated this issue at some level. In pioneering work,
[13] Li and Sastry proposed the concept of the task ellipsoid,
which maximizes the forces to be applied in the direction
of the task. More recently, Pandey et al. [16] proposed a
framework to select a grasp such that the object grasped can
be manipulated in a human-robot interaction scenario in which
the goal pose of the object is not entirely constrained.

Finally, although one of our main concerns is to select a
grasp that is suitable for the task to be executed, we also
consider important to use a grasp that allows for a simple, easy

Superquadric FittingGrasp Generationat Goal PoseValidation atStart PosePop First GraspPlan Pick and PlaceFoundSolution?EndNoYesPrioritizationFig. 3: System setup and problem description.

arm execution. Interestingly, the problem of grasp planning is
usually considered isolated from arm planning. In some recent
work, Vahrenkamp et al. proposed Grasp-RRT [22] in order to
perform both grasp and arm planning combined. In a similar
vein, Roa et al. also proposed an approach that solve both
problems simultaneously [8]. Both approaches focus on reach-
ing tasks. Our approach tackles pick-and-place tasks in which
reaching is only the ﬁrst half of the solution (object placing
being the second). We make use of our proposed heuristics
to solve the complete pick-and-place problem in a manner as
efﬁcient as possible by means of grasp prioritization.

III. PROBLEM DEFINITION AND ASSUMPTIONS

Our problem description can be explained as follows: Given
a bimanual manipulator R and a simple object O, the manip-
ulation task consists on transporting O from a given start pose
wTs to a ﬁnal pose wTg.

constraints are considered:

Figure 3 depicts the problem described. The following
• A 3D model of O is not available beforehand.
• A one-view pointcloud of the scenario is available from
the Kinect sensor mounted on top of the robot shoulders.
• Each limb of R consists of a 7-DOF arm (AL, AR) and
a 3-ﬁngered hand (HL, HR). A semi-analytical IK solver
is available for AL and AR

In the following sections we will describe our basic ap-
proach for problems in which only the use of one arm is
required to solve the manipulation task described.

IV. GRASP GENERATION FOR UNKNOWN OBJECTS

As our problem description stated, our approach must ﬁnd
a plan such that O can be grasped at wTs, transported and
ﬁnally repositioned at wTg. Our approach consists on 4 main
steps, shown in Figure 2. The ﬁrst 3 steps (object ﬁtting, grasp
generation and grasp validation) will be explained in the rest
of this section.

A. Object Representation using Superquadrics

Requiring complete 3D models of objects before grasp
synthesis severely limits the application domains of robot
manipulation. Modern depth cameras partly solve the prob-
lem, since they allow the robot to estimate the surface of
an object. Yet, since the point clouds are acquired from a
speciﬁc perspective, they only hold partial shape information
about the visible frontal part. To ﬁll any gaps and produce

a complete point cloud, multiple images can be acquired by
either iteratively moving the camera or the object. This process
is time-consuming and introduces new challenges such as the
precise matching of the individual point clouds of each view.
To solve this problem, we use a super-quadric representation
of objects and reason about symmetry in order to infer the
shape of any invisible part. Superquadrics are a family of
geometric shapes that can represent a wide range of diverse
objects using a limited set of parameters. Superquadrics can
be expressed with their implicit equation:

(cid:18)(cid:16) x

(cid:17) 2

(cid:16) y

(cid:17) 2

2

(cid:19) 2

1

(cid:16) z

(cid:17) 2

2 +

a

b

+

c

1 = 1

(1)

In our approach, we generate a super-quadric representation
using a single depth image. Fitting of the parameters can
be performed online by minimizing the difference between
the model and the partial point cloud [11]. However, since
only one side of the object is visible, a standard approach to
ﬁtting will result in erroneous approximations of the object. To
reproduce the entire shape from a partial point cloud, we added
an additional pre-processing step to the superquadric ﬁtting
process. Instead of using the original point cloud as input, we
generate a mirrored version by ﬁnding an optimal symmetry
plane [11]. The goals of this step is to exploit symmetries to
infer invisible parts of an object.
The output of this process for a given object O consists on
a transformation wTo in world coordinates and the parameters,

psq= { a, b, c, 1, 2 }

deﬁning its approximated geometry. A good number of
household objects can be easily described with generic shapes
such as boxes, cylinders and ovoids, for which we can further
bound the shape parameters considered:
1,2 ∈ [0.1, 1.9]

Figure 4 shows different geometric shapes corresponding to

superquadrics with different values for 1 and 2.

The superquadric approach turns the pointcloud-based rep-
resentation into a parametric representation, which can be
much more efﬁciently used during grasp synthesis. Calcula-
tions of principal-axes, normals and other features are much
faster and less susceptible to noise.

B. Generating Valid Candidates

Once the shape of O is approximated, we can proceed to
generate candidate grasps gi using a simulation of the robot,
and the object O, whose mesh is reconstructed by using the
superquadric parameters found in the previous section.
The candidates grasps must be kinematically feasible to
execute with O located at both start and goal conditions (wTs
and wTg). Our approach accomplish this with Algorithm 1.
First, we set O at its goal pose wTg and generate a set of
kinematically feasible grasps for it (G). Next, we set O at its
start pose wTs. Finally, we test each of the grasps gi from G
in this scenario, discarding the grasps for which there exist

superquadrics equation. We use the method proposed by Pilu
and Fischer [17] to obtain an evenly-spaced set of points and
normals.
To deﬁne a grasp we calculate the transformation of the
hand H w.r.t. O(oTh). We use the samples to generate this
transformation (lines 3 to 7 of Algorithm 2). After positioning
the TCP of the H at wTp, we close the ﬁngers. If there are
not collisions with the environment, we proceed to evaluate
if there exists at least an arm conﬁguration that allows the
hand to execute the grasp. If so, then a corresponding grasp
is stored.
Algorithm 2: GenerateGrasps(H,A,
Input: H, A, wTo, O, psq
Output: A feasible set of grasps G
/* psq= {1,2,a,b,c}
1 S= sample_SQ(psq)
2 foreach (pi,ni) ∈ S do

wTo,O,psq)

*/

/* p: TCP point in the hand H
oTp.trans = pi
/* z: Approach direction of H
oTp.z = -ni
/* x: Fingers closing direction
oTp.x = smallest_Axis(a,b,c)
oTp.y = oTh.z × oTh.x
wTp = wTo·oTp
/* h: Origin of hand H
wTh = wTp·pTh
setHand_Tcp(H,
close_Hand(H)
if check_collision(H) is false then

wTp)

*/

*/

*/

*/

if exist_IK_conf(H, A,wTh) is true then

G ← Grasp(H,

oTp·pTh)

Fig. 4: Examples of superquadrics with different shapes. A
variety of shapes can be represented using a small number of
parameters.

not a single IK solution. The surviving grasps in G are then
grasps that can be executed for the object O at both wTs and
wTg.

Algorithm 1: get Valid Candidates
Input: H, A, wTs, wTg, O, psq
Output: Set of Candidate grasps G
1 set_Pose(O,wTg)
/* Generate grasps with O at wTg
2 G ← generate_Grasps(H,A,wTg,O, psq)
3 set_Pose(O,wTs)
/* Discard grasps invalid with O at wTs
4 foreach gi ∈ G do

if exist_IK_sol(gi, H, A) is false then

5
6

G.erase(gi)

*/

*/

3

4

5
6
7

8
9
10
11
12
13

7
8 return G

14
15 return G

1) Grasp Generation at Goal Pose: The

function
generate_Grasps, which we use to produce grasps ex-
ploiting the shape parameters of O is shown in Algorithm 2.
First, we uniformly sample the surface of O. This is easily
done by using the explicit equation deﬁning the points in a
superquadric and their corresponding normals:

 with

x

y
z

b cos1 η sin2 ω

a cos1 η cos2 ω
 =

 =
nx

c sin1 η

1
a
1
b

ny
nz

cos2−1 η cos2−2 ω
cos2−1 η sin2−2 ω

sin2−1 η

1
c

π

2 < η < π
2
π < ω < π



(2)

(3)

Sampling uniformly ω and η does not produce a uniform
sampling of surface points due to the high nonlinearity of the

Algorithm 2 generates at most one grasp per each sampled
point. Optionally, we generated 2 additional possible grasps
per each point by rotating the hand an angle ±α around the
x axis of oTp. We added this since we noticed that, when
executing the grasps on the physical robot, a slight inclination
usually made the grasp much easier to reach. In this paper we
used α = 30o. An example of the variated grasps generated
using α is shown in Figure 5.
2) Validation at Start Pose: Once a set of grasps feasible
to execute on O at wTg is obtained, our algorithm discards
the grasps that cannot be executed with O at wTs (lines 4 to
6).

V. GRASP PRIORITIZATION

Once a set of feasible grasps G is generated, paths for reach-
ing and placing the object must be produced. A brute-force
approach would be to exhaustively try each grasp in a random

1.21.0Fig. 5: Generating grasps varying the approach direction by
rotating the hand around the local x axis(pointing out the
page): Left: Original. Middle,right: After rotating by ±α

order until a solution is found. However, arm planning can be
a time-consuming process, particularly when using sampling-
based methods. It is therefore desirable to ﬁrst evaluate grasps
that are more likely to produce a solution. Since there is
likely more than one solution in G, it is preferable to choose
grasps such that the solution is quickly found. We propose
to use situated grasp manipulability as a metric to prioritize
the grasps and, hence, as a heuristic for guiding the search
process.

Manipulability(m) measures how dexterous the end-effector
of a robotic arm is at a given joint conﬁguration q. Initially
proposed by Yoshikawa [23], m(q) is deﬁned as:

(cid:113)(cid:12)(cid:12)J(q)J T (q)(cid:12)(cid:12).

m(q) =

(4)

Manipulability is typically deﬁned for a single joint con-
ﬁguration. In our scenario, we describe a situated grasp gi
for which multiple q might exist, due to redundancy. This
naturally leads to the deﬁnition of situated grasp manipula-
bility(mg). Given a target object O located at wTo, and its
corresponding grasp gi, we deﬁne mg as the average manip-
ulability of a uniform set of collision-free arm conﬁgurations
qi that allow executing gi:

N(cid:88)

i=1

mg =

1
N

m(qi)

(5)

Please note that mg depends on both gi, wTo and the
environment (for collisions) since only collision-free grasps
that reach the object are considered. Figure 6 shows an
example of a pick-and-place task wherein the green and red
markers indicate the wTs and wTg. In this case, mg at wTg
is bigger than wTs (where Ns = 76 and Ng = 108 are the
number of IK solutions for both situations). When the object
is at wTg, the arm movement requires less effort.

Fig. 6: Examples of mg measured at wTs and wTg

From the shown example, it becomes evident that for a
pick-and-place manipulation problem, there are at least two
possible metrics to use per grasp gi: mg measured either at
wTs or wTg. Choosing the ﬁrst option means that we prioritize
grasps in which the pick phase is executed comfortably (wTs),
whereas by choosing the latter, we favor grasps in which the
arm conﬁguration used at placing the object (wTg) is more
relaxed. In section VI, we present the results of experiments
comparing these two metrics and an additional control measure
to analyze their performance and choose which one is best
suited for our problem.

VI. EXPERIMENTS AND RESULTS

In this section, we perform a set of experiments in simula-
tion and on the real robot in order to evaluate the introduced
manipulation planning algorithm. The simulation experiments
are used to analyze the situated grasp manipulability using
a large number of trials. Experiments on the real robots are
performed to show the generation of manipulations based on
task goals and previously unknown objects. Generation of
superquadric object models was performed on the spot within
1 second.
A. Simulation Experiments

In this experiment, we consider three alternatives:
• Measure mg for grasp situated at wTs
• Measure mg for grasp situated at wTg
• Average of both measures above
We use 3 measures to compare the performance of the 3

evaluated metrics.

• First success: The main goal of the metrics evaluated is
to prioritize the grasps such that the ﬁrst one tried is the
most likely to succeed. This metric indicates the number
of times that a solution is found by evaluating only the
grasp with the biggest value for the evaluated metric.

• Planning time: It measures the average planning time of
the successing grasps. The planning time is the total time
to plan a reach and transport path for the given grasp.

• Path length: It indicates the number of steps required
for the pick-and-place solution. The step length is a
normalized value in joint space, so this metric compare
the paths in conﬁguration space.

The scenario we used is depicted in Figure 7. We ﬁx the wTg
to the middle of the table (red marker) and vary the start pose
wTs to 35 positions, each separated 0.1 m (green markers). We
devised 2 kind of experiments: In the ﬁrst, wTs and wTg have
the same orientation, with only the position being changed (35
scenarios). In the second case, wTs presents a rotation around
the Z axis w.r.t. wTg in the interval [0, 2π] at each π
steps,
so in total 35 × 8 = 280 scenarios are tested.
4

We used an standard IK-BiRRT to perform the arm plan-
ning. To account for its randomness, the results presented
in Table I and Table II are an average of 5 runs per each
experiment.

From the tables, we can observe that using mg evaluated at
wTg produces the best results in terms of success at the ﬁrst

TABLE II: Evaluation with rotation change

Metric Type
mg at wTs
mg at wTg
Avg. mg

Path Steps

Planning Time

100.7
99.6
100.4

4.70
4.49
3.83

Success
255/278
275/278
260/278

Fig. 7: Setup for unimanual evaluation experiments

TABLE I: Evaluation with no rotation change

Metric Type
mg at wTs
mg at wTg
Avg. mg

Path Steps

Planning Time

82.92
89.28
92.92

2.17
2.218
2.29

Success
21.8/35
33/35
31.4/35

trial, whereas wTs present the worst results. The average path
length for the general case of Table II is rather similar for
the 3 cases. Regarding planning times, the average mg gives
better results.

Given the results presented, we chose to use the mg at wTg.
Its next best competitor (the avg. mg) was not considered
since in order to calculate it, the mg at both wTs and wTg
must be calculated, which increases the computation time (for
the examples presented, the computation time of mg was 2
seconds). Given that the advantage of planning time is not
signiﬁcant, we chose mg at wTg.

B. Robot Experiments

Next, we perform a set of experiments on the real robot.
All performed experiments are pick-and-place tasks. However,
in some tasks we add environmental constraints at the goal
location which limit the range of applicable grasps. Figure 8
depcits two trials without any environmental constraint. The
robot has to pick an object at the starting location (green) and
move it successfully to the goal location (orange). The robot
has no prior knowledge of the object and needs to extract
shape information from a single depth image produced by a
depth sensor mounted in the head. As can be seen in the ﬁgure,
the grasp direction and the hand shape is adapted to suit the
object.

A different set of experiments can be seen in Figure 9.
Here, environmental constraints at the goal are introduced. In
the top row, the object has to be placed in a box. Accordingly,
the robot has to choose a grasp that allows it to place the
object in the box without colliding with it. Hence, the selected
grasps are mostly from above. The middle row show a different
scenario, in which the object has to be placed on a box which
is farther away. Choosing the wrong approach direction, e.g.
from above, would prevent the robot from successfully ﬁnish-
ing the manipulation process, due to workspace limitations.
The bottom row shows normal runs without any environmental
constraints.

Fig. 8: Two examples of a pick-and-place without constraints.
The robot can identify suitable grasp for novel objects using
the superquadric representation.

VII. CONCLUSION

In this paper, we introduced a new method for manipulation
planning with task constraints. Given a previously unknown
object and goals of the task, the method synthesizes online
a grasp that facilitates task completion. Planning and grasp
synthesis are effectively merged to efﬁciently produce ma-
nipulation sequences. Object acquisition, representation, grasp
synthesis and planning can be performed within a couple
of seconds, i.e., 2-5 seconds, for the presented examples.
We showed how superquadrics and a new heuristic, i.e., the
situated grasp manipulability can be used towards this end.
These properties, hence, allow a robot to perform successful,
goal-driven manipulation tasks in new environments without
resorting to prior 3D models of the object or pre-calculated
grasps.

While superquadrics can be efﬁciently calculated, they lack
accuracy when representing complex shapes and objects. In
this paper, we showed that many objects, in particular house-
hold objects can be represented by superquadrics. In our future
work, we want to explore extensions of this representation
that can model a larger set of objects. In particular, we want
to build upon our previous research on the identiﬁcation of
rotational and linear extrusions [11] to represent more complex
shapes. In addition, we want to verify the introduced planning
approach on longer manipulation sequences.

Fig. 9: Final grasp conﬁguration during manipulation with goal constraints (top and middle) and without goal constraints
(bottom).

REFERENCES

[1] R. Balasubramanian, L. Xu, P. D Brook, J.R. Smith, and Y. Matsuoka.
Physical human interactive guidance: Identifying grasping principles
In The Human Hand as an Inspiration
from human-planned grasps.
for Robot Hand Development. Springer, 2014.

[2] D. Berenson, R. Diankov, K. Nishiwaki, S. Kagami, and J. Kuffner.
In 7th IEEE-RAS Int. Conf. on

Grasp planning in complex scenes.
Humanoid Robots, 2007.

[3] J. Bohg, A. Morales, T. Asfour, and D. Kragic. Data-driven grasp

synthesis: A survey. IEEE Transactions on Robotics, 2014.

[4] M. Ciocarlie, C. Goldfeder, and P. Allen. Dimensionality reduction for
In IEEE/RSJ Int. Conf.

hand-independent dexterous robotic grasping.
on Intelligent Robots and Systems (IROS), pages 3270–3275, 2007.

[5] M.R. Cutkosky. On grasp choice, grasp models, and the design of
IEEE Transactions on Robotics and

hands for manufacturing tasks.
Automation, 1989.

[6] R. Diankov and J. Kuffner. Openrave: A planning architecture for
autonomous robotics. Robotics Institute, Pittsburgh, PA, CMU-RI-TR-
08-34, 79, 2008.

[7] C. Ferrari and J. Canny. Planning optimal grasps. In IEEE Int. Conf.

on Robotics and Automation (ICRA), 1992.

[8] J. Fontanals, B.A. Dang-Vu, O. Porges, J. Rosell, and M. Roa. Integrated
grasp and motion planning using independent contact regions. 14th
IEEE-RAS Int. Conf. on Humanoid Robots (Humanoids), 2014.

[9] C. Goldfeder, P.K. Allen, C. Lackner, and R. Pelossof. Grasp planning
via decomposition trees. In IEEE Int. Conf. on Robotics and Automation
(ICRA), pages 4679–4684, 2007.

[10] K. Hsiao, S. Chitta, M. T. Ciocarlie, and E. G. Jones. Contact-reactive
In IEEE/RSJ Int.
grasping of objects with partial shape information.
Conf. on Intelligent Robots and Systems (IROS), volume 99, page 124,
2010.

[11] A. Huam´an Quispe, B. Milville, MA. Guti´errez, C. Erdogan, M. Stil-
man, HI. Christensen, and H. Ben Amor. Exploiting symmetries and
extrusions for grasping household objects. IEEE Int. Conf. on Robotics
and Automation (ICRA)(to appear), 2015.

[12] K. Huebner and D. Kragic. Selection of robot pre-grasps using box-
based shape approximation. In IEEE/RSJ Int. Conf. on Intelligent Robots
and Systems (IROS), 2008.

[13] Z. Li and S.S. Sastry. Task-oriented optimal grasping by multiﬁngered

robot hands. IEEE Journal of Robotics and Automation, 1988.

[14] J. Loucks and J. A. Sommerville. The role of motor experience in
understanding action function: The case of the precision grasp. Child
development, 83(3):801–809, 2012.

[15] A.T. Miller, S. Knoop, H.I. Christensen, and P.K. Allen. Automatic
grasp planning using shape primitives. In IEEE Int. Conf. on Robotics
and Automation,(ICRA), 2003.

[16] A.K. Pandey, J-P Saut, D. Sidobre, and R. Alami. Towards planning
human-robot interactive manipulation tasks: Task dependent and human

In 4th IEEE
oriented autonomous selection of grasp and placement.
RAS & EMBS Int. Conf. on Biomedical Robotics and Biomechatronics
(BioRob), 2012.

[17] M. Pilu and R.B. Fisher. Equal-distance sampling of superellipse
models. University of Edinburgh, Department of Artiﬁcial Intelligence,
1995.

[18] M. Przybylski, T. Asfour, and R. Dillmann. Unions of balls for shape
approximation in robot grasping. In IEEE/RSJ Int. Conf. on Intelligent
Robots and Systems (IROS), 2010.

[19] M. Przybylski, T. Asfour, R. Dillmann, R. Gilster, and H. Deubel.
Human-inspired selection of grasp hypotheses for execution on a hu-
manoid robot. In 11th IEEE-RAS Int. Conf. on Humanoid Robots, 2011.
[20] D. A. Rosenbaum, C. M. van Heugten, and G. E. Caldwell. From
cognition to biomechanics and back: The end-state comfort effect and
the middle-is-faster effect. Acta psychologica, 94(1):59–85, 1996.

[21] A. Sahbani, S. El-Khoury, and P. Bidaud. An overview of 3d object grasp
synthesis algorithms. Robotics and Autonomous Systems, 60(3):326–336,
2012.

[22] N. Vahrenkamp, T. Asfour, and R. Dillmann. Simultaneous grasp and
motion planning: Humanoid robot armar-iii. Robotics & Automation
Magazine, 2012.

[23] T. Yoshikawa. Manipulability of robotic mechanisms. The International

Journal of Robotics Research, 4(2):3–9, 1985.

