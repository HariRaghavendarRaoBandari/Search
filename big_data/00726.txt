Single-pixel 3D imaging with time-based depth resolution

Ming-Jie Sun,1, 2, ∗ Matthew. P. Edgar,2 Graham M. Gibson,2

Baoqing Sun,2 Neal Radwell,2 Robert Lamb,3 and Miles J. Padgett2, †

1Department of Opto-electronic Engineering, Beihang University, Beijing, 100191, China

2SUPA, School of Physics and Astronomy, University of Glasgow, Glasgow, G12 8QQ, UK

3Selex ES, Edinburgh, UK

Time-of-ﬂight three dimensional imaging is an important tool for many applications,

such as object recognition and remote sensing. Unlike conventional imaging approach

using pixelated detector array, single-pixel imaging based on projected patterns, such

as Hadamard patterns, utilises an alternative strategy to acquire information with

sampling basis. Here we show a modiﬁed single-pixel camera using a pulsed illumi-
nation source and a high-speed photodiode, capable of reconstructing 128×128 pixel
resolution 3D scenes to an accuracy of ∼ 3 mm at a range of ∼ 5 m. Furthermore, we
demonstrate continuous real-time 3D video with a frame-rate up to 12 Hz. The sim-

plicity of the system hardware could enable low-cost 3D imaging devices for precision

ranging at wavelengths beyond the visible spectrum.

Introduction

Whilst a variety of 3D imaging technologies are suited for diﬀerent applications, time-of-ﬂight (TOF) systems have

set the benchmark for performance with regards to a combination of accuracy and operating range. Time-of-ﬂight

imaging is performed by illuminating a scene with a pulsed light source and observing the back-scattered light.

Correlating the detection time of the back-scattered light with the time of the illumination pulse allows the distance,

d, to objects within the scene to be estimated by d = tc/2, where t is the TOF and c is the propagation speed of light.

The transverse spatial resolution of the image obtained is retrieved either by using a pixelated array or by using

a single-pixel detector with a scanning approach [1–7]. In both cases, the inherent speed of light demands the use

of detectors with a fast response time and high-speed electronic readout in order to obtain high precision depth

resolution. Advances in sensor development have enabled the ﬁrst TOF single-photon avalanche detector (SPAD)
array cameras to enter the commercial, having a resolution of 32 × 32 pixels, however such devices are still in their
infancy [8–10]. On contrary, there are mature single-pixel detectors in the market which provide stable time-resolved

measurements, and by employing compressed sensing principles for image reconstruction, which takes advantage of

the sparsity in natural scenes, the acquisition times of the scanning approach is largely reduced [11–15].

Recently there have been some interesting developments in 3D imaging utilising single-pixel detectors. One tech-

nique utilises structured illumination and spatially separated photodiodes to obtain multiple images with diﬀerent

∗Electronic address: mingjie.sun@buaa.edu.cn
†Electronic address: miles.padgett@glasgow.ac.uk

6
1
0
2

 
r
a

M
2

 

 
 
]
s
c
i
t
p
o

.
s
c
i
s
y
h
p
[
 
 

1
v
6
2
7
0
0

.

3
0
6
1
:
v
i
X
r
a

2

shading properties from which 3D images can be reconstructed via photometric stereo [16]. Another scheme scans a

scene, pixel by pixel, using a pulsed illumination source and measures the reﬂected light using an avalanche photodiode

(APD), whereupon the ﬁrst detected photon is used to recover depth and reﬂectivity via TOF [17]. An alternative

method for scanning a scene and recovering depth and reﬂectivity via TOF has also been demonstrated utilising

structured pulsed illumination [18–21].

Among the mentioned demonstrations, many [5, 17, 19–21] employed photon counting detection (i.e. Geiger-mode),

which is well suited for low light level imaging. However, one limitation of photon counting detectors is the inherent

electronic dead-time between successive measurements, often 10’s of nanoseconds, which prohibits the retrieval of short

range timing information from a single illumination pulse. Instead, an accurate temporal response from a 3D scene

requires summing the data over many back-scattered photons and hence many illumination pulses (usually several

hundreds or thousands [20, 21]). In contrast, as ﬁrst demonstrated by Kirmani [18], a high-speed photodiode can

retrieve the temporal response from a single illumination pulse, which can be advantageous in certain circumstances,

for instance when the reﬂected light intensity is comparatively large. Incidentally, photon counting cannot operate

under such conditions since the detection will always be triggered by back-scattered photons from the nearest, or

most reﬂective, object, rendering more distant objects invisible.

In this paper, we present a single-pixel 3D imaging system employing pulsed structured illumination and a high-speed

(short response time) photodiode for sampling the time-varying intensity of the back-scattered light from a scene. We

show that by using an analogue photodiode to record the full temporal form of the back-scattered light, along with our

original 3D reconstruction algorithm, it is possible to recover surface proﬁles of objects with an accuracy much better
than that implied by the ﬁnite temporal bandwidth of the detector and digitisation electronics. At distances of ∼ 5 m
we demonstrate a range proﬁle accuracy of ∼ 3 mm with image resolutions of 128 × 128 pixels, whilst simultaneously
recovering reﬂectivity information of the object. This accuracy is achieved despite a detection bandwidth and a

digitisation interval corresponding to distances of 150 mm and 60 mm respectively. We further demonstrate that by

employing a compressive sampling scheme, the system is capable of performing continuous real-time 3D video with a

frame-rate up to 12 Hz.

Experimental setup

The single-pixel 3D imaging system, illustrated in Fig. 1, consists of a pulsed laser and a digital-micro mirror-device

(DMD) to provide time varying structured illumination. A high-speed photodiode is used in conjunction with a fresnel

lens condenser system to measure the back-scattered intensity resulting from each pattern. The analogue photodiode

output is passed through a low-noise ampliﬁer and sampled using a high-speed digitiser. In our work we chose to

use the Hadamard matrices [22] for providing structured illumination.

In order to remove sources of noise, such

as ﬂuctuations in ambient light levels, we obtain diﬀerential signals by displaying each Hadamard pattern, followed

by its inverse pattern, and taking the diﬀerence in the measured intensities [23, 24]. Detailed information about

experimental setup is provided in Methods.

A 3D image of the scene is reconstructed utilising the time-varying back-scattered intensities (measured for each

output pulse of the laser) and the associated set of N patterns used to provide the structured illumination. An

overview of the reconstruction algorithm is shown in Fig. 2 (the result in this diagram represents a scene of three

3

FIG. 1: Single-pixel 3D imaging system. A pulsed laser uniformly illuminates a digital-micromirror-device (DMD), used

to provide structured illumination onto a scene, and the back-scattered light is collected onto a photodiode. The measured

light intensities are used in a 3D reconstruction algorithm to reconstruct both depth and reﬂectivity images.

objects ∼ 0.5 m apart in depth). The high-speed digitiser converts the ampliﬁed analog signals (Fig. 2b) into discrete
data points (Fig. 2c), which are subsequently processed by the computer algorithm. Whereas typical single-pixel

imaging schemes use the integrated signal for each illumination pattern to reconstruct a 2D image, our algorithm

utilises M discretely sampled intensity points from the time varying signal to reconstruct M 2D images, resulting in

an x, y, z image cube (Fig. 2d). In the image cube, each transverse pixel (x, y) has an intensity distribution (Fig. 2e)

along the longitudinal axis (z), which is related to the temporal shape of the pulse, the detector response, the readout

digitisation and the pixel depth and reﬂectivity information.

To enhance the range precision beyond the limits imposed by the sampling rate of the system, methods such

as parametric deconvolution [18, 20] and curve ﬁtting can be employed. However, often these methods can be

computationally intensive, which makes them unsuitable for real-time applications.

Instead we choose to apply

cubic spline interpolation to the reconstructed temporal signal at each pixel location, which introduces minimal

computational overhead. The depth of the scene (Fig. 2f) is subsequently determined by ﬁnding the maximum in

these interpolated signals. In addition, the scene reﬂectivity (Fig. 2g) can be calculated by averaging the image cube

along the longitudinal axis. Utilizing both the depth and reﬂectivity information, a 3D image of the scene is then

reconstructed.

It is worth mentioning that, with the assumption that there is only one surface at each transverse pixel location,

our depth estimation (see detail in Methods) works well for scenes that have smooth features, such as the mannequin

head and ball, since the reconstructed temporal signals should be slowly varying between sample points. We note that

the depth accuracy is limited by the amplitude noise on the data points, over-interpolation only increases the depth

precision, but not necessarily the accuracy. In addition, more interpolation adds more processing time, therefore in

VtVtPulsedlaserBeamexpanderDMDProjectionlensSceneCollectinglensSingle-pixelphotodiodeLaserpulseBroadenedsignalBallHeadScreenour experiments we chose to interpolate by a factor of 5 times when investigating the static scenes with 20 pulses per

pattern (see Fig. 3-5), and 4 times when investigating scenes with motion (see Fig. 6), balancing the computational

overhead and 3D image quality.

4

FIG. 2: Overview of the reconstruction algorithm. The incident laser pulses (a) back-scattered from a 3D scene are

temporally broadened (b) and discretely sampled using a high-speed digitizer (c). An image cube (d) is obtained using a

reconstruction algorithm, from which the depth map (f) and the reﬂectivity (g) of the scene can be estimated.

Results

In one experiment, a scene containing a 140 mm diameter polystyrene soccer ball, a life size skin-tone mannequin
head and a screen was located at a distance of ∼ 5.5 m from the imaging system (Fig. 3a). The objects were closely
separated in distance such that the total depth of the scene was ∼ 360 mm. A complete Hadamard set of 16,384
patterns, and their inverse, were used as the structured illumination and the back-scattered intensities measured for
reconstructing a 128 × 128 pixel resolution 3D image (Fig. 3e). The illumination time of each pattern was 2.66 ms,
corresponding to twenty laser pulses. The total time for acquisition, data transfer from the digitizer buﬀer to the
computer, and image processing was ∼ 130 seconds. The 3D reconstruction shown in Fig. 3e exhibits distinguishable
features, such as the proﬁle of the head and the ball.

In order to quantitatively determine the accuracy of our 3D imaging system, the scene was modiﬁed to contain
only a polystyrene mannequin head (180 × 270 × 250 mm), for which we had reference 3D data obtained via a high-

abcObjectsreflectionDigitizersamplingImagecomputationdeImage cube510555610Greyscale2550Greyscale2550Greyscale2550Depth (cm)Depth (cm)Depth (cm)450450450750750750ProcessingLaser pulseVTime(ns)010Broadened signalVTime (ns)3050VTime discretized signalTime (ns)3050Intensity distribution of depth for different transverse pixelsReflectivityestimationDepthestimationDepth mapReflectivityfg................Distance (m)6.305.10zyx5

FIG. 3: 3D image of a scene. (a) Illustration of the scene containing multiple objects in close proximity. (b) Reﬂected

intensity measured for uniform illumination, indicating temporally indistinguishable objects. (c) The estimated depth map of

the scene. (d) The reconstructed scene reﬂectivity. (e) A textured 3D image of the scene.

accuracy stereophotogrammetric camera system [16, 25]. The mannequin head was located at a distance of 5.5 m

from the imaging system. To further demonstrate the system capability for retrieving reﬂectivity in addition to the

depth, two grey stripes were placed on the head. Performing the same acquisition and imaging processing used in

Fig. 3, we obtained the results shown in Fig. 4a-c. Figure 4d & e show the front and side view comparisons between our

3D reconstruction (green) and photographs of the head (white), respectively. After lateral and angular registration

and subsequent depth scaling, an error map representing the absolute diﬀerences for a chosen region of interest was

obtained (shown in Fig. 4f). From this comparison we ﬁnd our single-pixel 3D imaging system has a root mean square

error (RMSE) of 2.62 mm.

One advantage of time-resolved imaging is the ability to distinguish objects at diﬀerent depths, by artiﬁcially time-

gating the measured intensity. In certain cases this enables obscuring objects to be isolated from objects of interest.

Similar to previous demonstrations [19], we constructed a 3D scene containing a polystyrene mannequin head (located
at a distance of ∼ 3.5 m) and black-coloured netting used to obstruct the line-of-sight (located at a distance of ∼ 3 m),
as illustrated in Fig. 5a. An image of the scene taken using a conventional camera is shown in Fig. 5b, which contains

no information of the head because of the black netting’s obscuring. Performing the same acquisition and imaging

processing used in Fig. 3 and 4, along with an artiﬁcial gating on the photodiode data to ensure no reﬂected signals

from the black netting are included in the 3D reconstruction, we obtained the results shown in Fig. 5c-e. As before,

we note the characteristic features of the mannequin head can be resolved.

In addition to obtaining high-quality 3D images of static scenes, many applications demand video frame-rates for

motion tracking in dynamic scenes. A key merit of single-pixel imaging is the ability to take advantage of the sparsity

of the scene and use compressive sensing to reduce the acquisition time. Most compressive sensing schemes are

performed by minimizing a certain measure of the sparsity, such as L1-norm, to ﬁnd the sparsest image as the optimal
reconstruction of the scene. However, for resolutions greater than 32 × 32 pixels, the time taken by the construction

VTime (ns)3252bcDistance (m)6.005.50ea0.36m0.25mSceneTemporal signalDepth mapReflectivity3D imagexyzd6

FIG. 4: Quantitative analysis of 3D reconstruction. The depth estimation (a), reﬂectivity (b) and 3D reconstruction
(c) of a white polystyrene mannequin head at a range of ∼ 5.5 m. Superposed depth reconstruction and photograph of the
mannequin head, viewed from the front (d) and side (e). (f) For a chosen region of interest an error map showing the absolute

diﬀerences between our depth result and that obtained using a stereophotogrammetric camera system.

FIG. 5: 3D imaging through obscuring material. (a) Illustration of scene containing a mannequin head with black netting

material obscuring the line-of-sight. (b) A photograph of the scene taken from the perspective of the 3D imaging system. The

scene depth (c) and reﬂectivity (d) reconstructed by time-gating the measured intensity data. (e) 3D reconstruction of the

mannequin head.

algorithm often prohibits real-time application [21, 26].

In this work we employ an alternative scheme, known as evolutionary compressive sensing [6, 7], which aims to

reconstruct the image with signiﬁcant less time than conventional compressive sensing by performing a linear iteration.

In short, the evolutionary compressive sensing scheme chooses a subset of the Hadamard basis to display, by selecting

the patterns with the most signiﬁcant intensities measured in the previous frame, in addition to a fraction of randomly

selected patterns that were not displayed. In this experiment, a scene consisting of a static polystyrene mannequin

Distance (m)5.705.5009Depth mapReflectivity3D imageError (mm)Side viewFront viewError mapxyz50mmbcaefd0.5mNettingHeadLine-of-sightSceneConventional photoTime gated reflectivity3D imageDepth mapDistance (m)3.673.50xyzbcead7

FIG. 6: Real-time 3D video. (a) Illustration of the scene containing a static mannequin head and a swinging ball. (b)-(h)
Sample of consecutive depth and reﬂectivity frames reconstructed at ∼ 5 Hz frame-rate in real-time for a transverse resolution
of 64 × 64 pixels.

head and a polystyrene white ball (140 mm diameter) swinging along the line of sight with a period of ∼ 3 s (Fig. 6a).
The scene was located at a distance of ∼ 4 m from the imaging system. Two laser pulses were used per illumination
pattern. With the approach described above we obtained continuous real-time 3D video with a frame-rate of 5 Hz
using 600 patterns (including their inverse) from the available 64 × 64 Hadamard set, equivalent to a compression
ratio of (cid:39) 7%. The experimental parameters for this result were chosen to balance the inherent trade-oﬀ between
frame-rate and image quality. Figure 6b-h show a sample of consecutive frames from the 3D video. The result shows

an identiﬁable 3D image of the mannequin head and ball, in addition to the real-time motion of the ball. Importantly

however, 3D reconsctruction can be performed using fewer patterns to achieve higher frame-rates if required, for

instance using 256 patterns provides 12 Hz video.

Discussion

We have demonstrated that our single-pixel 3D imaging system is capable of reconstructing a scene with millimetric

ranging accuracy using modest hardware. Additionally, we obtained real-time video rates by taking advantage of a

modiﬁed compressive sensing scheme that does not rely on lengthy post-processing.

The performance of the system in this work was mainly limited by the repetition rate of the laser employed, 7.4 kHz.

t=11.4 st=11.6 st=11.8 st=12 st=12.2 st=12.4 st=12.6 sDistance (m)4.33.7Reflectivity25500.6 mSceneaDistance (m)4.33.7Reflectivity2550bcdefghUsing a laser with a repetition rate greater than or equal to the DMD modulation rate, could enable faster 3D video

rates by a factor of three and/or increase reconstruction accuracy by increased averaging.

Furthermore, the broad operational spectrum (400 nm − 2500 nm) of the DMD could enable the system to be
extended to the non-visible wavelength, such as the infrared (IR), using modiﬁed source and detection optics. The use

of DMDs in the IR have already been demonstrated in microscopy [6] and real-time video cameras [7]. The potential

application of 3D imaging in the infrared could provide enhanced visibility at long-range, due to reduced atmospheric

scattering [27].

8

Methods

The following components were used in the experimental setup (Fig. 1): a pulsed laser (Teem Photonics SNG-

03E-100, 532 nm), a DMD (Texas Instruments Discovery 4100 DMD), a projection lens (Nikon ED, f=180 mm), a

collection lens (customised fresnel condenser lens, f=20 mm), a Si biased photodiode (Thorlabs DET10A) and a high-
speed USB digitizer (PicoScope 6407, 2.5 GSs−1 for 2 channels acquisition). There are several important points worth
mentioning. (a) The modulation rate of the DMD can reach up to 22.7 kHz, however, in this experiment the DMD

is operated in slave-mode, meaning the modulation rate is determined by the repetition rate of the laser at 7.4 kHz.

(b) The active area of the photodiode is 0.8 mm2, used in conjuction with a 20 mm focal length fresnel lens system,
giving a 2.6 ◦ ﬁeld-of-view, which matches that of the projection system. The depth estimation includes Gaussian
smoothing, intensity calibration, cubic spline interpolation, and depth determination.

References

[1] Schwarz, B. Lidar: Mapping the world in 3d. Nat. Photonics 4, 429–430 (2010).

[2] McCarthy, A. et al. Long-range time-of-ﬂight scanning sensor based on high-speed time-correlated single-photon counting.

Appl. Opt. 48, 6241–6251 (2009).

[3] Shapiro, J. H. Computational ghost imaging. Phys. Rev. A 78, 061802 (2008).

[4] Bromberg, Y., Katz, O. & Y., S. Ghost imaging with a single detector. Phys. Rev. A 79, 053840 (2009).

[5] McCarthy, A. et al. Kilometer-range, high resolution depth imaging via 1560 nm wavelength single-photon detection. Opt.

Express 21, 8904–8915 (2013).

[6] Radwell, N. et al. Single-pixel infrared and visible microscope. Optica 1, 285–289 (2014).

[7] Edgar, M. P. et al. Simultaneous real-time visible and infrared video with single-pixel detectors. Sci. Rep. 5, 10669 (2015).
[8] Niclass, C. & Charbon, E. A single photon detector array with 64× 64 resolution and millimetric depth accuracy for
3d imaging. In Solid-State Circuits Conference, 2005. Digest of Technical Papers. ISSCC. IEEE International, 364–604

(2005).

[9] Richardson, J. et al. A 32x32 50ps resolution 10 bit time to digital converter array in 130nm cmos for time correlated

imaging. In Custom Integrated Circuits Conference, CICC’09 (2009).

[10] Entwistle, M. et al. Geiger-mode apd camera system for single-photon 3d ladar imaging. In SPIE Defense, Security, and

Sensing, 83750D–83750D (2012).

9

[11] Baraniuk, R. G. Compressive sensing [lecture notes]. IEEE Signal Process. Mag. 24, 118–121 (2007).

[12] Duarte, M. F. et al. Single-pixel imaging via compressive sampling. IEEE Signal Process. Mag. 25, 83–91 (2008).

[13] Young, M. et al. Real-time high-speed volumetric imaging using compressive sampling optical coherence tomography.

Biomed. Opt. Express 2, 2690?697 (2011).

[14] Hatef, M., Sina, J., Matan, G. & Donoho, D. L. Deterministic matrices matching the compressed sensing phase transitions

of gaussian random matrices. Proc. Natl. Acad. Sci. U. S. A. 110, 1181–1186 (2013).

[15] Herman, A. M. A., Tidman, J., Hewitt, D., Weston, T. & Mcmackin, L. A higher-speed compressive sensing camera

through multi-diode design. In SPIE Defense, Security, and Sensing, 871706 (2013).

[16] Sun, B. et al. 3d computational imaging with single-pixel detectors. Science 340, 844–847 (2013).

[17] Kirmani, A. et al. First-photon imaging. Science 343, 58–61 (2014).

[18] Kirmani, A., Cola¸co, A., Wong, F. N. & Goyal, V. K. Exploiting sparsity in time-of-ﬂight range acquisition using a single

time-resolved sensor. Opt. Express 19, 21485–21507 (2011).

[19] Howland, G. A., Dixon, P. B. & Howell, J. C. Photon-counting compressive sensing laser radar for 3D imaging. Appl. Opt.

50, 5917–5920 (2011).

[20] Colaco, A., Kirmani, A., Howland, G. A., Howell, J. C. & Goyal, V. K. Compressive depth map acquisition using a

single photon-counting detector: Parametric signal processing meets sparsity. In IEEE Conf. Computer Vision Pattern

Recognition(CVPR), 96–102 (2012).

[21] Howland, G. A., Lum, D. J., Ware, M. R. & Howell, J. C. Photon counting compressive depth mapping. Opt. Express 21,

23822–23837 (2013).

[22] Pratt, W. K., Kane, J. & Andrews, H. C. Hadamard transform image coding. Proceedings of the IEEE 57, 58–68 (1969).

[23] Sun, B. et al. Diﬀerential computational ghost imaging. In Computational Optical Sensing and Imaging, CTu1C–4 (Optical

Society of America, 2013).

[24] Sun, M.-J., Li, M.-F. & Wu, L.-A. Nonlocal imaging of a reﬂective object using positive and negative correlations. Appl.

Opt. 54, 7494–7499 (2015).

[25] Khambay, B. et al. Validation and reproducibility of a high-resolution three-dimensional facial imaging system. Br. J.

Oral Maxillofac. Surg. 46, 27 (2008).

[26] Aβmann, M. & Bayer, M. Compressive adaptive computational ghost imaging. Sci. Rep. 3, 1545 (2013).

[27] Bucholtz, A. Rayleigh-scattering calculations for the terrestrial atmosphere. Appl. Opt. 34, 2765–2773 (1995).

Acknowledgements

We would like to thank Prof. Adrian Bowman and Dr. Liberty Vittert for providing the stereophotogrammetric

3D data of the polystyrene mannequin head. MJP acknowledges ﬁnancial support from UK Quantum Technology

Hub in Quantum Enhanced Imaging (Grant No. EP/M01326X/1) the Wolfson foundation and the Royal Society. MS

acknowledges the support from National Natural Foundation of China (Grant No. 61307021) and China Scholarship

Council (Grant No. 201306025016).

Author contributions

MS, RL and MJP conceived the concept of the experiment. MS, MPE and GMG. designed and performed the

experiments. MS, MPE and MJP designed the reconstruction algorithm. NR developed the evolutionary compressed

sensing algorithm. MS, MPE, BS and MJP analysed the results. MS, MPE and MJP wrote the manuscript and other

authors provided editorial input.

10

Competing ﬁnancial interests

The authors declare no competing ﬁnancial interests.

