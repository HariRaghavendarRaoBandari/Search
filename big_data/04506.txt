6
1
0
2

 
r
a

 

M
4
1

 
 
]

G
L
.
s
c
[
 
 

1
v
6
0
5
4
0

.

3
0
6
1
:
v
i
X
r
a

Conformal Predictors for Compound Activity

Prediction

Paolo Toccaceli, Ilia Nouretdinov and Alexander Gammerman

Royal Holloway, University of London, Egham, United Kingdom,

(paolo, ilia, alex)@cs.rhul.ac.uk

WWW home page: http://clrc.rhul.ac.uk/

Abstract. The paper presents an application of Conformal Predictors
to a chemoinformatics problem of identifying activities of chemical com-
pounds. The paper addresses some speciﬁc challenges of this domain:
a large number of compounds (training examples), high-dimensionality
of feature space, sparseness and a strong class imbalance. A variant of
conformal predictors called Inductive Mondrian Conformal Predictor is
applied to deal with these challenges. Results are presented for several
non-conformity measures (NCM) extracted from underlying algorithms
and diﬀerent kernels. A number of performance measures are used in
order to demonstrate the ﬂexibility of Inductive Mondrian Conformal
Predictors in dealing with such a complex set of data.

Keywords: Conformal Prediction, Conﬁdence Estimation, Chemoinfor-
matics, Non-Conformity Measure

1

Introduction

Compound Activity Prediction is one of the key research areas of Chemoinfor-
matics. It is of critical interest for the pharmaceutical industry, as it promises
to cut down the costs of the initial screening of compounds by reducing the
number of lab tests needed to identify a bioactive compound. The focus is on
providing a set of potentially active compounds that is signiﬁcantly “enriched”
in terms of prevalence of bioactive compounds compared to a purely random
sample of the compounds under consideration. The paper is an extension of our
work presented in [17].

While it is true that this objective in itself could be helped with the classical
machine learning techniques that usually provide a bare prediction, the hedged
predictions made by Conformal Predictors (CP) provide some additional infor-
mation that can be used advantageously in a number of respects.

First, CPs will supply the valid measures of conﬁdence in the prediction of
bioactivities of the compounds. Second, they can provide prediction and conﬁ-
dence for individual compounds. Third, they can allow the ranking of compounds
to optimize the experimental testing of given samples. Finally, the user can con-
trol the number of errors and other performance measures like precision and
recall by setting up a required level of conﬁdence in the prediction.

2

2 Machine learning background

2.1 Conformal Predictors

Conformal Predictors described in [13, 7] revolves around the notion of Confor-
mity (or rather of Non-Conformity).

Intuitively, one way of viewing the problem of classiﬁcation is to assign a label
ˆy to a new object x so that the example (x, ˆy) does not look out of place among
the training examples (x1, y1), (x2, y2), . . . , (x(cid:96), y(cid:96)). To ﬁnd how “strange” the
new example is in comparison with the training set, we use the Non-Conformity
Measure (NCM) to measure (x, ˆy).

The advantage of approaching classiﬁcation in this way is that this leads to
a novel way to quantify the uncertainty of the prediction, under some rather
general hypotheses.

A Non-Conformity Measure can be extracted from any machine learning
algorithm, although there is no universal method to choose it. Note that we
are not necessarily interested in the actual classiﬁcation resulting from such
“underlying” machine learning algorithm. What we are really interested in is an
indication of how “unusual” an example appears, given a training set.

Armed with an NCM, it is possible to compute for any example (x, y) a p-
value that reﬂects how good the new example from the test set ﬁts (or conforms)
with the training set. A more accurate and formal statement is: for a chosen
 ∈ [0, 1] it is possible to compute p-values for test objects so that they are
(in the long run) smaller than  with probability at most . Note that the key
assumption here is that the examples in the training set and the test objects
are independent and identically distributed (in fact, even a weaker requirement
of exchangeability is suﬃcient).

The idea is then to compute for a test object a p-value of every possible

choice of the label.

Once the p-values are computed, they can be put to use in one of the following

ways:

– Given a signiﬁcance level, , a region predictor outputs for each test object
the set of labels (i.e., a region in the label space) such that the actual label
is not in the set no more than a fraction  of the times. If the prediction set
consists of more than one label, the prediction is called uncertain, whereas
if there are no labels in the prediction set, the prediction is empty.

– Alternatively, a forced prediction (chosen by the largest p-value) is given,
alongside with its credibility (the largest p-value) and conﬁdence (the com-
plement to 1 of the second largest p-value).

2.2 Inductive Mondrian Conformal Predictors

In order to apply conformal predictors to both big and imbalanced datasets, we
combine two variants of conformal predictors from [13, 7]: Inductive (to reduce
computational complexity) and Mondrian (to deal with imbalanced data sets)
Conformal Predictors.

3

To combine the Mondrian Conformal Prediction with that of Inductive Con-
formal Prediction, we have to revise the deﬁnition of p-value for the Mondrian
case so that it incorporates the changes brought about by splitting the training
set and evaluating the αi only in the calibration set.
It is customary to split the training set at index h so that examples with
index i ≤ h constitute the proper training set and examples with index i > h
(and i ≤ (cid:96)) constitute the calibration set.

The p-values for a hypothesis y(cid:96)+1 = y about the label of x(cid:96)+1 are deﬁned as

p(y) =

|{i = h + 1, . . . , (cid:96) + 1 : yi = y, αi ≥ α(cid:96)+1}|

|{i = h + 1, . . . , (cid:96) + 1 : yi = y}|

In other words, the formula above considers only αi associated with those exam-
ples in the calibration set that have the same label as that of the completion we
are currently considering (note that also α(cid:96)+1 is included in the set of αi used
for the comparison). As in the previous forms of p-value, the fraction of such αi
that are greater than or equal to α(cid:96)+1 is the p-value.

Finally, it is important to note that Inductive Conformal Predictors can be
applied under less restrictive conditions. The requirement of i.i.d. can in fact be
dropped for the proper training set, as the i.i.d. property is relevant only for the
populations on which we calculate and compare the αi, that is, the calibration
and testing set.

This will allow us to use NCM based on such methods as Cascade SVM

(described in sec.3.1), including a stage of splitting big data into parts.

3 Application to Compound Activity Prediction

To evaluate the performance of CP for Compound Activity Prediction in a real-
istic scenario, we sourced the data sets from a public-domain repository of High
Throughput assays, PubChem BioAssay [20].

The data sets on PubChem identify a compound with its CID (a unique
compound identiﬁer that can be used to access the chemical data of the com-
pound in another PubChem database) and provide the result of the assay as
Active/Inactive as well as providing the actual measurements on which the re-
sult was derived, e.g. viability (percentage of cells alive) of the sample after
exposure to the compound.

To apply machine learning techniques to this problem, the compounds must
be described in terms of a number of numerical attributes. There are several
approaches to do this. The one that was followed in this study is to compute sig-
nature descriptors [6]. Each signature corresponds to the number of occurrences
of a given labelled subgraph in the molecule graph, with subgraphs limited to
those with a given depth. In this exercise the signature descriptors1 had at most

1 The signature descriptors and other types of descriptors (e.g. circular descriptors)
can be computed with the CDK Java package or any of its adaptations such as the
RCDK package for the R statistical software.

4

height 3. Examples can be found in [18]. The resulting data set is a sparse matrix
of attributes (the signatures, on the columns) and examples (the compounds, on
the rows).

We evaluated Conformal Predictors ﬁrst with various underlying algorithms
on the smallest of the data sets and then with various data sets using the un-
derlying algorithm that performed best in previous set of tests.

3.1 Underlying algorithms

As a ﬁrst step in the study, we set out to extract relevant non-conformity mea-
sures from diﬀerent underlying algorithms: Support Vector Machines (SVM),
Nearest Neighbours, Na¨ıve Bayes. The Non Conformity Measures for each of the
three underlying algorithms are listed in Table 1.

Table 1. The Non Conformity Measures for the three underlying algorithms

Underlying

Non Conformity
Measure αi

Comment

SVM

kNN

−yid(xi)

(cid:80)(k)
(cid:80)(k)

j(cid:54)=i:yj =yi

j(cid:54)=i:yj(cid:54)=yi

(signed) distance from separating
hyperplane

d(xj, xi)

d(xj, xi)

here the summation is on the k
smallest values of d(xj, xi)

Na¨ıve Bayes

−log p(yi = c|xi)

p is the posterior probability esti-
mated by Na¨ıve Bayes

There are a number of considerations arising from the application of each of

these algorithms to Compound Activity Prediction.

SVM. The usage of SVM in this domain poses a number of challenges. First
of all, the number of training examples was large enough to create a problem for
our computational resources. The scaling of SVM to large data sets is indeed an
active research area [2, 5, 15, 16], especially in the case of non-linear kernels2. We
turned our attention to a simple approach proposed by V.Vapnik et al. in [9],
called Cascade SVM.

The sizes of the training sets considered here are too large to be handled
comfortably by generally available SVM implementations, such as libsvm [4].
The approach we follow could be construed as a form of training set editing.

2 In the case of linear SVM, it is possible to tackle the formulation of the quadratic
optimization problem at the heart of the SVM in the primal and solve it with tech-
niques such as Stochastic Gradient Descent or L-BFGS, which lend themselves well
to being distributed across an array of computational nodes

5

Vapnik proved formally that it is possible to decompose the training into an
n-ary tree of SVM trainings. The ﬁrst layer of SVMs is trained on training sets
obtained as a partition of the overall training set. Each SVM in the ﬁrst layer
outputs its set of support vectors (SVs) which is generally smaller than the
training set. In the second layer, each SVM takes as training set the merging
of n of the SVs sets found in the ﬁrst layer. Each layer requires fewer SVMs.
The process is repeated until a layer requires only one SVM. The set of SVs
emerging from the last layer is not necessarily the same that would be obtained
by training on the whole set (but it is often a good approximation). If one wants
to obtain that set, the whole training tree should be executed again, but this
time the SVs obtained at the last layer would be merged into each of the initial
training blocks. A new set of SVs would then be obtained at the end of the tree
of SVMs. If this new set is the same as the one in the previous iteration, this is
the desired set. If not, the process is repeated once more. In [9] it was proved
that the process converges and that it converges to the same set of SVs that one
would obtain by training on the whole training set in one go.

To give an intuitive justiﬁcation, the fundamental observation is that the
SVM decision function is entirely deﬁned just by the Support Vectors. It is as
if these examples contained all the information necessary for the classiﬁcation.
Moreover, if we had a training set composed only of the SVs, we would have
obtained the same decision function. So, one might as well remove the non-SVs
altogether from the training set.

In experiments discussed here, we followed a simpliﬁed approach. Instead of

a tree of SVMs, we opted for a linear arrangement as shown in Fig.1.

While we have no theoretical support for this semi-online variant of the Cas-
cade SVM, the method appears to work satisfactorily in practice on the data
sets we used.

The class imbalance was addressed with the use of per-class weighting of
the C hyperparameter, which results in a diﬀerent penalization of the margin
violations. The per-class weight was set inversely proportional to the class rep-
resentation in the training set.

Another problem is the choice of an appropriate kernel. While we appreciated
the computational advantages of linear SVM, we also believed that it was not
necessarily the best choice for the speciﬁc problem. It can easily be observed that
the nature of the representation of the training objects (as discrete features) war-
ranted approaches similar to those used in Information Retrieval, where objects
are described in terms of occurrences of patterns (bags of words). The topic of
similarity searching in chemistry is an active one and there are many alternative
proposals (see [1]). We used as a kernel a notion called Tanimoto similarity3. The
Tanimoto similarity extends the well-known Jaccard coeﬃcient in the sense that
whereas the Jaccard coeﬃcient considers only presence or absence of a pattern,
the Tanimoto similarity takes into account the counts of the occurrences.

To explore further the beneﬁts of non-linear kernels, we also tried out a kernel

consisting of the composition the Tanimoto similarity with Gaussian RBF.

3 See [8] for a proof that Tanimoto Similarity is a kernel.

6

Fig. 1. Linear Cascade SVM
At each step, the set of Support Vectors from the previous stage is merged with a
block of training examples from the partition of the original training set. This is used
as training set for an SVM, whose SVs are then fed to the next stage.

Table 2 provides the deﬁnitions of the kernels used in this study.

Table 2. SVM Kernels Deﬁnitions (where A = (a1, . . . , ad), B = (b1, . . . , bd) are two
objects, each described by a vector of d counts) )

Tanimoto Coeﬃcient

T (A, B) =

(cid:80)d

i=1

(cid:80)d
(ai+bi)−(cid:80)d

i=1

min(ai,bi)

min(ai,bi)

i=1

Tanimoto with Gaussian RBF

T G(A, B) = e

− |T (A,A)+T (B,B)−2T (A,B)|

γ

Na¨ıve Bayes. Na¨ıve Bayes and more speciﬁcally Multinomial Na¨ıve Bayes are
widely regarded as eﬀective classiﬁers when features are discrete (for instance,
in text classiﬁcation), despite their relative simplicity. This made Multinomial
Na¨ıve Bayes a natural choice for the problem at issue here.

7

In addition, Na¨ıve Bayes has a potential of providing some guidance for
feature selection, via the computed posterior probabilities. This is of particular
interest in the domain of Compound Activity Prediction, as it may provide
insight as to the molecular structures that are associated with Activity in a
given assay. This knowledge could steer further testing in the direction of a class
of compounds with higher probability of Activity.

Nearest Neighbours. We chose Nearest Neighbours because of its good per-
formance in a wide variety of domains. In principle, the performance of Nearest
Neighbours could be severely aﬀected by the high-dimensionality of the training
set (Talble 3 shows how in one of the data sets used in this study the number
of attributes exceeds by ≈ 20% the number of examples), but some preliminary
small-scale experiments did not show that this causes the “curse of dimension-
ality”.

3.2 Tools and Computational Resources

The choice of the tools for these experiments was inﬂuenced primarily by the ex-
ploratory nature of this work. For this reason, tools, programming languages and
environments that support interactivity and rapid prototyping were preferred to
those that enable optimal CPU and memory eﬃciency.

The language adopted was Python 3.4 and the majority of programming
was done using IPython Notebooks in the Jupyter environment. The overall
format turned out to be very eﬀective for capturing results (and for their future
reproducibility).

Several third-party libraries were used. The computations were run initially
on a local server (8 cores with 32GB of RAM, running OpenSuSE) and in later
stages on a supercomputer (the IT4I Salomon cluster located in Ostrava, Czech
Republic).The Salomon cluster is based on the SGI ICE X system and comprises
1008 computational nodes (plus a number of login nodes), each with 24 cores (2
12-core Intel Xeon E5-2680v3 2.5GHz processors) and 128GB RAM, connected
via high-speed 7D Enhanced hypercube InﬁniBand FDR and Ethernet networks.
It currently ranks at #48 in the top500.org list of supercomputers and at #14
in Europe4.

Parallelization and computation distribution relied on the ipyparallel[3]
package, which is a high-level framework for the coordination of remote exe-
cution of Python functions on a generic collection of nodes (cores or separate
servers). While ipyparallel may not be highly optimized, it aims at providing a
convenient environment for distributed computing well integrated with IPython
and Jupyter and has a learning curve that is not as steep as that of the al-
ternative frameworks common in High Performance Computing (OpenMPI, for
example). In particular, ipyparallel, in addition to allowing the start-up and
shut-down of a cluster comprising a controller and a number of engines where

4 According

to https://www.sgi.com/company_info/newsroom/press_releases/

2015/september/salomon.html.

8

the actual processing (each is a separate process running a Python interpreter)
is performed via integration with the job scheduling infrastructure present on
Salomon (PBS, Portable Batch System), took care of the details such as data
serialization/deserialization and transfer, load balancing, job tracking, exception
propagation, etc. thereby hiding much of the complexity of parallelization. One
key characteristic of ipyparallel is that, while it provides primitives for map()
and reduce(), it does not constrain the choice to those two, leaving the imple-
menter free to select the most appropriate parallel programming design patterns
for the speciﬁc problem (see [21] for a reference on the subject).

In this work, parallelization was exploited to speed up the computation of
the Gram matrix or of the decision function for the SVMs or the matrix of
distances for kNN. In either case, the overall task was partitioned in smaller
chunks that were then assigned to engines, which would then asynchronously
return the result. Also, parallelization was used for SVM cross-validation, but
at a coarser granularity, i.e. one engine per SVM training with a parameter.
Data transfers were minimized by making use of shared memory where possible
and appropriate. A key speed-up was achieved by using pre-computed kernels
(computed once only) when performing Cross-Validation with respect to the
hyperparameter C.

3.3 Results

To assess the relative merits of the diﬀerent underlying algorithms, we applied
Inductive Mondrian Conformal Predictors on data set AID827, whose charac-
teristics are listed in Table 3.

Table 3. Characteristics of the AID827 data set

Total number of examples
Number of features
Number of non-zero entries
Density of the data set
Active compounds
Inactive compounds
Unique set of signatures

138,287
165,786 High dimensionality

7,711,571

0.034% High sparsity

1,658 High imbalance (1.2%)

136,629
137,901 Low degeneracy

The test was articulated in 20 cycles of training and evaluation. In each
cycle, a test set of 10,000 examples was extracted at random. The remaining
examples were split randomly into a proper training set of 100,000 examples
and a calibration set with the balance of the examples (28,387).

During the SVM training, 5-fold stratiﬁed Cross Validation was performed
at every stage of the Cascade to select an optimal value for the hyperparameter
C. Also, per-class weights were assigned to cater for the high class imbalance

9

in the data, so that a higher penalization was applied to violators in the less
represented class.

In Multinomial Na¨ıve Bayes too, Cross Validation was used to choose an

optimal value for the smoothing parameter.

The results are listed in Table 4, which presents the classiﬁcation arising from
the region predictor for  = 0.01. The numbers are averages over the 20 cycles
of training and testing.

Note that a compound is classiﬁed as Active (resp. Inactive) if and only if
Active (resp. Inactive) is the only label in the prediction set. When both labels
are in the prediction, the prediction is considered Uncertain.

Table 4. CP results for AID827 with signiﬁcance  = 0.01. All results are averages
over 20 runs, using the same test sets of 10,000 objects across the diﬀerent underlying
algorithms. “Active predicted Active” is the (average) count of actually Active test
examples that were predicted Active by Conformal Prediction. Uncertain predictions
occur when both labels are output by the region predictor. Empty predictions occur
when both labels can be rejected at the chosen signiﬁcance level. For the speciﬁc
signiﬁcance level chosen here, there were never empty predictions.

Underlying

Na¨ıve Bayes
3NN
Cascade SVM:
- linear
- RBF kernel
- Tanimoto kernel
- Tanimoto-RBF kernel

Active
pred
Active
38.20
43.95

Inactive
pred
Active
104.30
100.55

Inactive
pred
Inactive
183.30
361.55

Active
pred
Inactive
1.10
0.80

34.20
47.20
48.45
47.65

99.00
101.80
97.65
94.10

591.85
1126.75
986.85
1044.90

1.20
1.80
0.80
0.95

Uncertain

Empty
pred

0 9673.10
0 9493.15

0 9273.75
0 8722.45
0 8866.25
0 8812.40

It has to be noted at this stage that there does not seem to be an established
consensus on what the best performance criteria are in the domain of Compound
Activity Prediction (see for instance [10]), although Precision (fraction of actual
Actives among compounds predicted as Active) and Recall (fraction of all the
Active compounds that are among those predicted as Active) seem to be gener-
ally relevant. In addition, it is worth pointing out that these (and many others)
criteria of performance should be considered as generalisations of classical per-
formance criteria since they include dependence of the results on the required
conﬁdence level.

At the shown signiﬁcance level of  = 0.01, 34% of the compounds predicted
as active by Inductive Mondrian Conformal Prediction using Tanimoto composed
with Gaussian RBF were actually Active compared to a prevalence of Actives
in the data set of just 1.2%. At the same time, the Recall was ≈ 41% (ratio of
Actives in the prediction to total Actives in the test set).

10

Table 5. CP results for AID827 using SVM with Tanimoto+RBF kernel for diﬀerent
signiﬁcance levels. The “Active Error Rate” is the ratio of “Active predicted Inactive”
to the total number of Active test examples. The “Inactive Error Rate” is the ratio of
“Inactive predicted Active” to the total number of Inactive test examples.

Inactive
Signiﬁcance Active
pred
pred
Active
Active
94.10
47.65
490.40
67.20
999.25
76.15
82.10 1484.85
86.55 1982.25

1%
5%
10%
15%
20%

Inactive
pred
Inactive
1044.90
3091.75
4703.75
6021.80
6928.95

Active
pred
Inactive
0.95
5.20
10.60
17.30
22.80

Empty
pred

Uncertain Active
Error
Rate

Inactive
Error
Rate
0.82% 0.95%
0.0 8812.40
4.52% 4.96%
0.0 6345.45
9.22% 10.11%
0.0 4210.25
0.0 2393.95 15.04% 15.02%
0.0
979.45 19.83% 20.05%

We selected Cascade SVM with Tanimoto+RBF as the most promising un-
derlying algorithm on the basis of the combination of its high Recall (for Actives)
and high Precision (for Actives), assuming that the intended application is in-
deed to output a selection of compounds that has a high prevalence of Active
compounds.

Note that in Table 4 the values similar to ones of confusion matrix are calcu-
lated only for certain predictions. In this representation, the concrete meaning
of the property of class-based validity can be clearly illustrated as in Table 5:
the two rightmost columns report the prediction error rate for each label, where
by prediction error we mean the occurrence of “the actual label not being in the
predictions set”. When there are no Empty predictions, the Active Error rate is
the ratio of the number of “Active predicted Inactive” to the number of Active
examples in the test set (which was 115 on average).

Fig. 2 shows the test objects according to the base-10 logarithm of their
pactive and pinactive. The dashed lines represent the thresholds for p-value set at
0.01, i.e. the signiﬁcance value  used in Table 4. The two dashed lines parti-
tion the plane in 4 regions, corresponding to the region prediction being Active
(pactive >  and pinactive ≤ ), Inactive (pactive ≤  and pinactive > ), Empty
(pactive ≤  and pinactive ≤ ), Uncertain (pactive >  and pinactive > ).

As we said in Sec. 2.1, the alternative is forced prediction with individual

conﬁdence and credibility.

It is clear that there are several beneﬁts accruing from using Conformal
Predictors. For instance, a high p-value for the Active hypothesis might suggest
that Activity cannot be ruled out, but the same compound may exhibit also a
high p-value for the Inactive hypothesis, which would actually mean that neither
hypothesis could be discounted.

In this speciﬁc context it can be argued that the p-values for Active hypoth-
esis are more important. They can be used to rank the test compounds like it
was done in [19] for ranking potential interaction. A high p-value for the Ac-
tive hypothesis might suggest that Activity cannot be ruled out. For example
it is possible to output the prediction list of all compounds with p-values above

Fig. 2. Test objects plotted by the base-10 log of their their pactive and pinactive. Note
that many test objects are overlapping. Note that some of the examples may have
identical p-values, so for example 1135 objects predicted as ”Inactives” are presented
as 4 points on this plot.

11

a threshold  = 0.01. A concrete activity which is not yet discovered will be
covered by this list with probability 0.99. All the rest examples are classiﬁed as
Non-Active with conﬁdence 0.99 or larger.

Special attention should be also paid to low credibility examples where both
p-values are small. Intuitively, low credibility means that either the training set is
non-random or the test object is not representative of the training set. For such
examples, the label assignment does not conform to the training data.. They
may be considered as anomalies or examples of compound types not enough
represented in the training set. This may suggest that it would be beneﬁcial to
the overall performance of the classiﬁer to perform a lab test for those compounds
and include the results in training set. Typically credibility will not be low
provided the data set was generated independently from the same distribution:
the probability that credibility will be less than some threshold  (such as 1%)
is less than .

Finally, Conformal Predictors provide the user with the additional degree of
freedom of the signiﬁcance or conﬁdence level. By varying either of those two
parameters, a diﬀerent trade-oﬀ between Precision and Recall or any of the other
metrics that are of interest can be chosen. Fig. 3 illustrates this point with two
examples. The Precision and Recall shown in the two panes were calculated on
the test examples predicted Active which exceeded both a Credibility threshold

12

and a given the Conﬁdence threshold. In the left pane, the Credibility threshold
was ﬁxed and the Conﬁdence threshold was varied; vice versa in the right pane.

Fig. 3. Trade-oﬀ between Precision and Recall by varying Credibility or Conﬁdence

3.4 Application to diﬀerent data sets

We applied Inductive Conformal Predictors with underlying SVM using Tan-
imoto+RBF kernel to other data sets extracted from PubChem BioAssay to
verify if the same performance would be achieved for assays covering a range of
quite diﬀerent biological targets and to what extent the performance would vary
with diﬀerences in training set size, imbalance, and sparseness of the training
set. The main characteristics of the data sets are reported in Table 6.

As in the previous set of experiments, 20 cycles of training and testing were
performed and the results averaged over them. In each cycle, a test set of 10,000
examples was set aside and the rest was split between calibration set (≈ 30, 000)
and proper training set. The results are reported in Table 7.

It can be seen that ﬁve data sets diﬀer in their hardness for machine learn-
ing some of the produce more uncertain predictions using the same algorithms,
number of examples and the same signiﬁcance level.

3.5 Mondrian ICP with diﬀerent active and inactive

When applying Mondrian ICP, there is no constraint to use the same signiﬁcance
 for the two labels. There may be an advantage in allowing diﬀerent “error”
rates for the two labels given that the focus might be in identifying Actives
rather than Inactives.

This allows to vary relative importance of the two kinds of errors. Validity
of Mondrian machines implies that the expected number of certain but wrong
predictions is bounded by act for (true) actives and by inact for (true) non-
actives. It is interesting to study its eﬀect also on the precision and recall (within
certain prediction).

Table 6. Data sets and their characteristics. Density refers to the percentage of non-
zero entries in the full matrix of ‘Number of Compounds × Number of Features’ ele-
ments

13

Data
Set

827

Assay Description

Number of
Compounds

Number of

Actives

Density

Features

(%)

(%)

High Throughput Screen to Iden-
tify Compounds that Suppress the
Growth of Cells with a Deletion of
the PTEN Tumor Suppressor.

138,287

165,786

1.2% 0.034%

1461

qHTS Assay for Antagonists of the
Neuropeptide S Receptor: cAMP
Signal Transduction.

208,069

211,474

1.11% 0.026%

1974

polarization-based
Fluorescence
counterscreen
for RBBP9
in-
hibitors: primary biochemical high
throughput
to
identify inhibitors of the oxidore-
ductase glutathione S-transferase
omega 1(GSTO1).

screening

assay

302,310

237,837

1.05% 0.024%

2553

High throughput screening of in-
hibitors of transient receptor poten-
tial cation channel C6 (TRPC6)

2716

Luminescence Microorganism Pri-
mary HTS to Identify Inhibitors of
the SUMOylation Pathway Using a
Temperature Sensitive Growth Re-
versal Mutant Mot1-301

305,308

236,508

1.06% 0.024%

298,996

237,811

1.02% 0.024%

Table 7. Results of the application of Mondrian ICP with  = 0.01 using SVM with
Tanimoto+RBF as underlying. Test set size: 10,000

DataSet

827
1461
1974
2553
2716

Active
pred
Active

47.65
29.45
62.50
34.00
3.55

Inactive
pred
Active

94.10
101.30
97.40
101.00
98.20

Inactive
pred
Inactive

1044.90
1891.10
880.85
337.90
97.00

Active
pred
Inactive

0.95
1.20
1.00
1.00
1.00

Empty
pred

Uncertain

0
0
0
0
0

8812.40
7976.95
8958.25
9526.10
9800.25

14

Fig. 4 shows the trade-oﬀ between Precision and Recall that results from

varying inact.

For very low values of the signiﬁcance , a large number of test examples
have pact > act as well as pinact > inact. For these test examples, we have an
‘Uncertain’ prediction.

As we increase inact, fewer examples have a pinact larger than inact. So
‘Inactive’ is not chosen any longer as a label for those examples. If they happen
to have a pact > act, they switch from ’Uncertain’ to being predicted as ‘Active’
(in the other case, they would become ‘Empty’ predictions).

Fig. 4. Trade-oﬀ between Precision and Recall by varying inact

Fig. 5 shows how Precision varies with Recall using three methods: varying
the threshold applied to the Decision Function of the underlying SVM, varying
the signiﬁcance inact for the Inactive class, varying the credibility. The three
methods give similar results.

4 Conclusions

This paper summarized a methodology of applying conformal prediction to big
and imbalanced data with several underlying methods like nearest neighbours,
Bayes, SVM and various kernels. The results have been compared from the point
of view of eﬃciency of various methods and various sizes of the data sets.

Fig. 5. Precision vs. Recall: three methods

15

The paper also presents results of using Inductive Mondrian Conformal Pre-

dictors with diﬀerent signiﬁcance levels for diﬀerent classes.

The most interesting direction of the future extension is to study the possible
strategies of active learning (or experimental design). In this paper one of the
criteria of performance is a number of uncertain predictions. It might be useful
to select among them the compounds that should be checked experimentally ﬁrst
– in other words the most “promising” compounds. How to select though may
depend on practical scenarios of further learning and on comparative eﬃciency
of diﬀerent active learning strategies.

5 Acknowledgments

This project (ExCAPE) has received funding from the European Unions Horizon
2020 Research and Innovation programme under Grant Agreement no. 671555.
We are grateful for the help in conducting experiments to the Ministry of
Education, Youth and Sports (Czech Republic) that supports the Large In-
frastructures for Research, Experimental Development and Innovations project
“IT4Innovations National Supercomputing Center LM2015070”. This work was
also supported by EPSRC grant EP/K033344/1 (“Mining the Network Be-
haviour of Bots”). We are indebted to Lars Carlsson of Astra Zeneca for provid-
ing the data and useful discussions. We are also thankful to Zhiyuan Luo and
Vladimir Vovk for many valuable comments and discussions.

16

References

1. Monve, V. Introduction to Similarity Searching in Chemistry. MATCH - Comm.

Math. Comp. Chem. 51, 7-38, (2004)

2. L´eon Bottou, Olivier Chapelle, Dennis DeCoste, and Jason Weston. Large-Scale

Kernel Machines (Neural Information Processing). The MIT Press, 2007.

3. Matthias Bussonnier.

Interactive

parallel

computing

in Python.

https://github.com/ipython/ipyparallel.

4. Chih-Chung Chang and Chih-Jen Lin. LIBSVM: A library for support vector
machines. ACM Transactions on Intelligent Systems and Technology, 2:27:1–27:27,
2011. Software available at http://www.csie.ntu.edu.tw/˜cjlin/libsvm.

5. Edward Y. Chang. PSVM: Parallelizing Support Vector Machines on Distributed
Computers. In Foundations of Large-Scale Multimedia Information Management
and Retrieval, pages 213–230. Springer Berlin Heidelberg, 2011.

6. Jean-Loup Faulon, Jr. Donald P. Visco, and Ramdas S. Pophale. The signature
molecular descriptor. 1. using extended valence sequences in qsar and qspr studies.
Journal of Chemical Information and Computer Sciences, 43(3):707–720, 2003.
PMID: 12767129.

7. Alexander Gammerman and Vladimir Vovk. Hedging predictions in machine learn-

ing. Comput. J., 50(2):151–163, March 2007.

8. Thomas G¨artner. Kernels For Structured Data. World Scientiﬁc Publishing Co.,

Inc., River Edge, NJ, USA, 2009.

9. Hans Peter Graf, Eric Cosatto, Leon Bottou, Igor Durdanovic, and Vladimir Vap-
In In Advances in

nik. Parallel Support Vector Machines: The Cascade SVM.
Neural Information Processing Systems, pages 521–528. MIT Press, 2005.

10. Ajay N. Jain and Anthony Nicholls. Recommendations for evaluation of compu-
tational methods. Journal of Computer-Aided Molecular Design, 22(3-4):133–139,
2008.

11. F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel,
M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos,
D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine
learning in Python. Journal of Machine Learning Research, 12:2825–2830, 2011.

12. Glenn Shafer and Vladimir Vovk. A tutorial on conformal prediction. J. Mach.

Learn. Res., 9:371–421, June 2008.

13. Vladimir Vovk, Alex Gammerman, and Glenn Shafer. Algorithmic Learning in a

Random World. Springer-Verlag New York, Inc., Secaucus, NJ, USA, 2005.

14. Derick C. Weis, Donald P. Visco Jr., and Jean-Loup Faulon. Data mining pubchem
using a support vector machine with the signature molecular descriptor: Classiﬁ-
cation of factor {XIa} inhibitors. Journal of Molecular Graphics and Modelling,
27(4):466 – 475, 2008.

15. Kristian Woodsend and Jacek Gondzio. Hybrid MPI/OpenMP parallel linear sup-
port vector machine training. J. Mach. Learn. Res., 10:1937–1953, December 2009.
16. Yang You, Haohuan Fu, Shuaiwen Leon Song, Amanda Randles, Darren Kerbyson,
Andres Marquez, Guangwen Yang, and Adolfy Hoisie. Scaling support vector
machines on modern HPC platforms. J. Parallel Distrib. Comput., 76(C):16–31,
February 2015.

17. Paolo Toccaceli, Ilia Nouretdinov, Zhiyuan Luo, Vladimir Vovk, Lars Carlsson,
and Alex Gammerman. Conformal Predictors. Technical Report for EU Horizon
2020 programme ExCape project; Dec.2015, Royal Holloway, London.

17

18. Lars Carlsson, Ernst Ahlberg, Henrik Bostr¨om, Ulf Johansson, Henrik Linusson.

Modiﬁcations to p-Values of Conformal Predictors. SLDS 2015: 251-259

19. I. Nouretdinov, A. Gammerman, Y. Qi, J. Klein-Seetharaman. Determining conﬁ-
dence of predicted interactions between HIV-1 and human proteins using conformal
method. Paciﬁc Symposium on Biocomputing, 311, 2012.

20. Wang Y, Suzek T, Zhang J, Wang J, He S, Cheng T, Shoemaker BA, Gindulyte
A, Bryant SH. PubChem BioAssay: 2014 update. Nucleic Acids Res. 2014 Jan
1;42(1):D1075-82

21. Michael McCool, Arch D. Robison, and James Reinders. Structured Parallel Pro-

gramming: Patterns for Eﬃcient Computation, Morgan-Kaufmann, 2012

