A Probabilistic Machine Learning Approach to Detect

Industrial Plant Faults

Wei Xiao

SAS Institute Inc, Cary, NC, 27513, USA

Wei.Xiao@sas.com

Abstract: Fault detection in industrial plants is a hot research area as more and

more sensor data are being collected throughout the industrial process. Auto-

matic data-driven approaches are widely needed and seen as a promising area

of investment. This paper proposes an eﬀective machine learning algorithm to

predict industrial plant faults based on classiﬁcation methods such as penalized

logistic regression, random forest and gradient boosted tree. A fault’s start time

and end time are predicted sequentially in two steps by formulating the original

prediction problems as classiﬁcation problems. The algorithms described in this

paper won ﬁrst place in the Prognostics and Health Management Society 2015

Data Challenge.

Key words and phrases: Fault detection; Machine learning; Random forest; Gra-

dient boosted tree.

6
1
0
2

 
r
a

 

M
8
1

 
 
]
L
M

.
t
a
t
s
[
 
 

1
v
0
7
7
5
0

.

3
0
6
1
:
v
i
X
r
a

1

1

Introduction

Fault detection in industrial plants is a hot research topic as more and more sensor data are

being collected throughout the industrial process, and standard systems based on univari-

ate statistical process control lack power in these more complex systems. Early detection

of faults can help to avoid system shut-down and component failure or even catastrophes

[Korbicz et al., 2012].

Many machine learning algorithms used in pattern classiﬁcation are now being utilized

in fault detection. Dimension reduction techniques, such as principal component analysis,

partial least squares, and Fisher’s discriminant analysis have been applied to detect faults in

chemical processes [Chiang et al., 2000, Chiang et al., 2004, Yin et al., 2012]. Support vector

machine and artiﬁcial neural networks are also widely used methods for fault detection; they

have been applied to gearbox failure detection [Samanta, 2004] and chemical process fault

diagnosis [Wang and Yu, 2005]. K-Nearest Neighbor and fuzzy-logic are two other powerful

methods that have been used to detect faults in semiconductor manufacturing processes

[He and Wang, 2007] and mechanical systems [Korbicz et al., 2012]. Tree based algorithms

such as random forest and gradient boosted tree are useful machine learning algorithms in

situations where one expects nonlinear and interactive eﬀects between covariates. They have

been applied to fault detection in aircraft systems [Lee et al., 2014].

This year’s Prognostics and Health Management (PHM) Society data challenge focused on

plant fault detection. We try many of the above machine learning techniques and ultimately

use a combination of several in our ﬁnal detection strategy described herein. The rest of

the paper is organized as follows. Section 2 discusses the data challenge problem. Section

3 introduces the relative methodologies and our algorithm. Finally, Section 4 concludes the

paper and discusses future work.

2 Problem Statement

The objective of this year’s challenge is to design an algorithm to predict plant faults. Correct

prediction involves predicting the type of fault (one of ﬁve), as well as the start and end time

of each fault, within one hour.

Three datasets are given, training, test, and validation; they contain information on 33,

2

15, and 15 plants, respectively. For each plant three ﬁles are provided: plant-#a.csv, plant-

#b.csv, plant-#c.csv, where # is the plant id. File (a) contains time series readings of 4

sensors (S1-S4) and 4 reference signals (R1-R4) from each plant component. The number

of components (N m) varies by plant; data on Sj and Rj for ith component are denoted

mi Sj and mi Rj, respectively, where i ∈ {1, . . . , N m} and j ∈ {1, 2, 3, 4}. File (b) contains

time series data for cumulative energy consumed (E1) and instantaneous power (E2) from

a ﬁxed number of zones within a given plant. Each plant zone covers one or more of the

plant components and the number of zones (N n) varies by plant. The notation ni Ej is used

to represent the reading of Ej for the ith zone. File (c) contains plant fault events, each

characterized by a start time, an end time, and a failure type. Data are given on 6 diﬀerent

fault types (F 1 − F 6), but only faults 1-5 are to be predicted. The training dataset has

complete fault event data, and is used to train the model. The test dataset has complete

fault event data for the ﬁrst half of the sample, but approximately 50% of the events in the

second half of the data have been randomly removed. The boundary between the ﬁrst and

second half of the data is given, and referred to as the boundary time. Our goal is to predict

the deleted fault events. The validation dataset is similar in structure to the test dataset.

Each team participating in the contest is permitted to submit their predictions of the

missing faults in the test data (fault type, and start and end time) once each week to assess

their prediction performance and use the score as feedback to improve their model. The

ﬁnal team rank is determined by the score of a submission of predictions based on the vali-

dation dataset. The data can be download from NASA Ames Prognostics Data Repository

[Rosca et al., 2015].

2.1 Data Description and Preprocessing

We began our analysis by ﬁrst studying the data to garner any information that would be

useful in predicting the faults. Not only do the number of both zones and components vary

by plant, but the proportion of each fault type (P F i) varies quite dramatically. To illustrate,

Table 1 summarizes the data for the ﬁrst ﬁve plants in both the training and test datasets.

Note that F 3 (fault type 3) never occurs in plants 2, 3, 5 and 42, and F 5 never occurs in

plants 2, 3, 5, 41, 42 and 46. We also notice that S3, R1, R2, R3, R4 appear to be categorical

variables, and S1, S2, S4 appear to be continuous variables. The number of unique levels

3

of all categorical variables for the same sample plants are summarized in Table 2. Given the

above diﬀerences across plants and variables, we built a separate model for each plant.

Table 1: Summary statistics of faults by plant. N m: number of components; N n: number of
zones; P F 1-P F 6: proportion of each fault type. (Plants in training set: 1, 2, 3, 4, 5; plants
in test set: 41, 42, 43, 45, 46)

Plant Nm Nn PF1 PF2 PF3 PF4 PF5 PF6

1
2
3
4
5

41
42
43
45
46

6
13
10
8
3

5
10
6
7
5

3
2
2
4
2

2
3
2
2
2

0.25
0.46
0.17
0.30
0.27

0.29
0.39
0.08
0.17
0.19

0.18
0.03
0.01
0.16
0.07

0.10
0.15
0.16
0.47
0.21

0.12
0.00
0.00
0.02
0.00

0.02
0.00
0.02
0.05
0.14

0.04
0.02
0.02
0.02
0.07

0.05
0.07
0.22
0.06
0.05

0.11
0.00
0.00
0.02
0.00

0.00
0.00
0.11
0.03
0.00

0.29
0.49
0.80
0.48
0.58

0.54
0.40
0.41
0.22
0.41

Table 2: Counts of unique levels of all categorical variables. (Plants in training set: 1, 2, 3,
4, 5; plants in test set: 41, 42, 43, 45, 46)

Plant S3 R1 R2 R3 R4

1
2
3
4
5

41
42
43
45
46

12
11
12
12
12

12
8
12
12
12

38
26
30
34
12

33
38
23
23
40

6
6
7
7
7

4
5
3
4
4

8
6
8
7
6

6
7
4
5
5

3
3
3
3
3

3
3
3
3
3

The sampling interval for the data provided was theoretically 15 minutes, however some

logging delays resulted in irregular intervals. To preprocess the data, we rounded all times-

tamps to obtain regular 15-minute gaps, and then combined all three ﬁles. We deﬁne new

variables T T F F k, k = 1, . . . , 6, to represent time to failure of fault type k. A negative

value, −i, means the next fault is i intervals in the future (1 interval is 15 minutes), and a

4

positive value, i, means the current fault started i intervals ago and has not yet ended. We

deﬁne E3 as the ﬁrst order diﬀerence of E1, i.e., E3(t) = E1(t) − E1(t − 1). E3 measures

the energy consumed in the most recent 15 minutes, which similar as E2 is a way to measure

instantaneous power. We also deﬁne start F k, k = 1, . . . , 6, as a binary indicator of whether

any type k fault starts within one hour of the corresponding timestamp, and deﬁne end F k,

k = 1, . . . , 6, as the binary indicator of whether any type k fault ends within one hour of the

corresponding timestamp. Occasionally observations of covariates on some timestamps are

missing. Forward imputation was applied to all covariates to impute these missing values,

except for T T F F k, start F k and end F k, which were imputed with values -999, 0 and 0,

respectively. To illustrate these preprocessing steps, a small proportion of plant 1’s data are

shown in Table 3. The imputation simpliﬁes the analysis, and from the authors’ observation,

it has little inﬂuence on the modeling results.

Table 3: A sample of data from plant 1 after preprocessing.

Timestamp

m1 R1 m1 S1 TTF F1 start F1 end F1

2009-09-04 09:00:00
2009-09-04 09:15:00
2009-09-04 09:30:00
2009-09-04 09:45:00
2009-09-04 10:00:00
2009-09-04 10:15:00
2009-09-04 10:30:00
2009-09-04 10:45:00
2009-09-04 11:00:00
2009-09-04 11:15:00
2009-09-04 11:30:00
2009-09-04 11:45:00
2009-09-04 12:00:00
2009-09-04 12:15:00
2009-09-04 12:30:00
2009-09-04 12:45:00

739
739
739
700
700
700
700
700
700
700
700
700
700
700
700
700

763
763
759
711
711
712
720
714
716
711
720
716
712
711
716
718

-7
-6
-5
-4
-3
-2
-1
0
1
2
-41
-40
-39
-38
-37
-36

0
0
0
1
1
1
1
1
1
1
1
1
0
0
0
0

0
0
0
0
0
0
1
1
1
1
1
1
1
1
1
0

There are segments of time where all covariates are missing and fault type 6 is happening.

We assumed the plant must be in some type of maintenance mode during these periods, and

we excluded these observations in the following analysis.

5

2.2 Visualization

Visualization was key to our understanding of the data.

First, we observe that R2, R3 and R4 are highly positively correlated, and S2 and S4 are

highly negatively correlated, across all components in all plants. To illustrate this ﬁnding,

Figure 1 shows the correlation heatmap of plant 1 for the ﬁrst two components, where each

cell represents the Pearson correlation between two features. Pearson correlation is calculated

as

r =

i=1(xi − ¯x)(yi − ¯y)

Pn
i=1(xi − ¯x)2pPn

pPn

.

i=1(yi − ¯y)2

Second, by observing the correlation heatmap of either mi R2, mi R3, or mi R4, across

all components for a given plant, one can identify which components are in the same zone;

components in the same zone are highly correlated. For example, Figure 2 shows the cor-

relation heatmap of mi R4 across all 6 components in plant 1. Based on the heatmap, it

seems components 1, 3, 5 of plant 1 belong to one zone, components 2, 4 belong to another

zone, and component 6 itself belongs to the third zone. Although one can identify which

components are zoned together, the groups of components could not always be linked to a

speciﬁc zone, so this information was ultimately not utilized in our modeling approach.

We also ﬁnd that month and hour are important categorical variables to predict the faults.

Count plots of F 2 by month and hour are shown in Figure 3 and 4 to illustrate this point.

F 2 starts most frequently between May and November and between 6 o’clock and 23 o’clock.

But its distribution varies across plants.

Lastly, we observe that, before a fault happens, sensor readings are often increasing or

decreasing. These unique patterns can be utilized to predict the start time of the fault.

See Figure 5 for an example, where the mean value of m2 R2 and its corresponding 95%

conﬁdence bands are plotted against time to failure of F 1.

3 Methodology

In this section we introduce our approach and the related methodologies utilized for the PHM

competition. The overall approach consists of two parts: preprocessing and modeling. Figure

6 provides an overview of the process implemented. Details of the data preprocessing have

been discussed in Section 2.1. After preprocessing, we divide the training data into two parts:

6

cross validation training data and cross validation test data. Mimicking the test dataset and

the validation dataset, the cross validation training data has complete fault event data for

the ﬁrst half of the sample, and 50% randomly selected events in the second half. The cross

validation test data contains the 50% deleted events in the second half. Our basic approach

is to try various models using the cross validation training data and then evaluate their

performance based on their ability to forecast faults in the cross validation test data. The

winning model is then applied to the test data and the subsequent predictions submitted to

PHM for assessment. Here we are not learning the exact model with cross validation training

and test data, as we ﬁt a plant speciﬁc model to each plant. However, we learn things such

as which classiﬁer to use, which threshold value to apply, etc. Please refer to Section 3.3-3.5

for more details.

There are two steps to the modeling process: predict fault start times and then, given

these start times, predict fault end time. A detailed ﬂowchart of the modeling process is shown

in Figure 7. The modeling procedure outlined is implemented for each fault type, plant by

plant. Given a fault type and plant, F 1 in plant 5 for example, we translate the prediction

problem into a classiﬁcation problem (start F 1=1 vs start F 1=0). From the classiﬁcation

model we estimate the probability that F 1 starts during each time interval. We derive the

set of predicted fault start times, ΩF 1, based on these estimated probabilities. For each start

time in ΩF 1, we then solve another classiﬁcation problem (end F 1=1 vs end F 1=0) and

estimate the probability the F 1 will end in the next 1 to tmax time intervals, where tmax is

an estimated upper bound of fault F 1’s duration. These estimated probabilities are used to

ﬁnd the fault end time.

Various machine learning algorithms were tried to solve these classiﬁcation problems: K-

nearest neighbors (KNN), naive bayes, gradient boosting machine (GBM), random forest,

penalized logisitic regression (with ℓ2 penalty), etc. In the ﬁnal algorithm, we use gradient

boosting machine, random forest and penalized logisitic regression. All methods are imple-

mented in SAS or Python Scikit-learn.

In Section 3.1, we give a quick review of the machine learning algorithms used. Section

3.2 discusses how we evaluate the eﬀectiveness of these diﬀerent approaches. Section 3.3

describes all the features we use in the model and ﬁnally, the speciﬁc algorithm details to

predict a fault’s start time and end time are given in Section 3.4 and 3.5, respectively.

7

3.1 Machine Learning Algorithms

Data-driven or statistical approaches based solely on historical data are seen as the most cost-

eﬀective approach for fault detection in complex systems [Aldrich and Auret, 2013]. Machine

learning is the key to any data-driven algorithm.

Machine learning algorithms can be categorized as either supervised or unsupervised. In

supervised learning, the goal is to predict a response Y based on input features X. All

methods in our algorithm belong to supervised learning.

K-nearest neighbor is an instance-based learning algorithm which has a very simple form

but works extremely well on many problems. The algorithm is simple. For a test point x0, we

ﬁrst ﬁnd k training points that are closest in distance to x0, and then classify using majority

vote. K-nearest neighbor can learn very ﬂexible decision boundaries. However, when dealing

with high dimensional data, it is likely to suﬀer from over-ﬁtting and perform poorly due to

the curse of dimensionality.

Naive Bayes [Rish, 2001] is a classiﬁcation technique based on applying Bayes’ theorem.

It assumes conditional independence between features given a class Y = i. Given a class

response Y and a p-dimensional feature x = (x1, . . . , xp), we have

Pr(Y = i|x1, . . . , xp) ∝ Pr(Y = i)

p

Y

k=1

Pr(xk|Y = i)

based on Bayes’ theorem and conditional independence assumption, and Naive Bayes classiﬁes

Y as

ˆY = argmaxi Pr(Y = i|x1, . . . , xp).

Despite its oversimpliﬁed and sometimes unrealistic conditional independence assumption, it

often outperforms other more sophisticated algorithms. Naive Bayes is widely used in text

mining and natural language processing.

Logistic regression [Hosmer Jr and Lemeshow, 2004] is widely used in classiﬁcation prob-

lems. However, when the number of input features is large, it performs poorly due to over-

ﬁtting. Penalized logisitic regression avoids the overﬁtting problems of logistic regression by

imposing a penalty on large ﬂuctuations in the estimated parameters. In this paper, we use

a penalized logistic regression with ℓ2 penalty. Besides avoiding overﬁtting and improving

prediction accuracy, this ridge type penalty is also very computationally eﬃcient.

8

Random forest [Breiman, 2001] is an ensemble learning method which averages over a large

collection of de-correlated decision trees. Similar as bagging, random forest builds decision

tress on bootstrapped samples. But unlike bagging method, when building the decision

tree, each time random forest only use a portion of randomly selected features. Thus, it

decorrelates the decision thees and makes their ensemble less variable. Random forest allows

for interaction eﬀects among features just like any tree based algorithm, but it corrects for

the likely overﬁtting of decision trees. The performance of random forest is comparable to

boosting, and they are easier to train and tune [Friedman et al., 2001].

Gradient boosting machine (GBM) [Friedman, 2001, Friedman, 2002] is an ensemble method

which combines weak classiﬁers to form a strong classiﬁer. We use decision tree as our weak

classiﬁer. Unlike random forest which ﬁt a large number of decision trees in parallel, GBM

works in a forward “stagewise” fashion.

In each step, GBM ﬁrstly calculate the pseudo-

residuals (negative ﬁrst-order derivative of the loss function) at the current model, and then

ﬁt a decision tree to the pseudo-residuals. GBM then add the ﬁtted decision tree to the

previous model. There are many parameters that we can tune in GBM. For example, we

can set the order of interaction we want to consider by specifying the depth of the decision

tree; We can avoid overﬁtting by specifying a small learning rate in GBM. GBM is also very

ﬂexible as users can provide their own loss function. GBM has been implemented in many

data mining competition winning strategies.

3.2 Evaluation

Evaluation is the key step to obtain feedback and ﬁnd the approach that works well predicting

faults with the data at hand. In this competition, each team was allowed to submit a set of

predictions only once a week to score their model. However, this is not frequent enough given

that there are a large number of possible models and tuning parameters to try. To remedy

this problem and allow us to try many approaches, we built our own evaluation system, based

on the idea of cross validation. Our cross validation system was basically designed to mimic

the competition evaluation/scoring system.

To build our own scoring system, we randomly remove 50% of the faults in the second

half of each training dataset. We build the model using the remaining fault data and attempt

to predict the deleted events. We then compare the predicted faults EP with the deleted true

9

events ET , and score our model. If a fault event in ET has been correctly predicted in EP

(i.e., there exists an event in EP with start time and end time within one hour of actual start

time and end time, and fault type also matches), it is a true positive and receives 10 point.

If a fault event in EP has correct start time and end time, and incorrect fault type, it is a

misclassiﬁcation and that prediction receives -0.01 point. If a fault event in EP has incorrect

start time or end time, it is considered as a false positive and receives -0.1 point. If a fault

event in ET has not been identiﬁed in EP , it is considered a false negative and receives -0.1

point.

We found that the above scoring system worked very well in the sense that the order of

magnitude of improvement of one classiﬁcation algorithm over another based on our scoring

system was similar to the improvement seen on the leader board. In this way, we could use

our scoring system and experiment with many diﬀerent algorithms and tuning parameters.

The ﬁnal model achieves a score of 79570 on the training data, and 21015 on the validation

data. The average score per plant of the ﬁnal model on the training data is 2411 with 90%

conﬁdence interval from 60 to 5006. The average score per plant on the validation data is

1401.

3.3 Feature engineering

We did feature engineering and added features like month, hour, weekday and time (the

number of minutes since 00:00 of the ﬁrst day of the corresponding year / (60*24)) in our

classiﬁcation models to predict faults’ start and end time. A complete list of the features to

predict faults’ start and end time is given in Table 4. Here elapsed t represents elapsed time

since the fault ﬁrst occurred which is deﬁned in Section 3.5.

We also added lagged covariates of all sensor readings (R1-R4, S1-S4, E2 and E3) to the

model. Speciﬁcally, for any sensor reading, X(t), we included X(t − k), for all nonzero k,

where k ≥ min lag and k ≤ max lag. To deﬁne these new variables we introduce the following

notation: Lk mi Sj(t) = mi Sj(t − k), where k > 0 and thus, represents the lagged covariate

of mi Sj. In contrast, Rk∗ mi Sj(t) = mi Sj(t − k), where k < 0 and k∗ = −k, and thus,

represents the lead (future) covariate of mi Sj. The time interval is 15 minutes. Based on

the description of the competition, we know a fault is independent of data outside a three

hour window of time. So the smallest min lag and the largest max lag we considered are -12

10

and 12, respectively. All covariates were standardized to have mean 0 and variance 1 before

feeding to the classiﬁers.

model

features in the model

predict start time

predict end time

month, hour, weekday, time, R1-R4, S1-S4, E2, E3

and lagged covariates of all sensor readings

month, hour, weekday, time, R1-R4, S1-S4, E2, E3, elapsed t

and lagged covariates of all sensor readings

Table 4: Features include in the models.

3.4 Predict Start Time

For every plant and fault type in the cross validation training, test or validation datasets, we

built a separate classiﬁcation model to predict the start time of deleted events. Start F k,

the binary indicator of whether a type k fault starts within one hour, is the response variable

Y . To train each model, we include all data from the ﬁrst half of the sample where we

know exactly when all faults do or do not occur.

In addition, we also include data from

the second half where start F k(t) = 1. That is, we only include the data for the faults

that we know occur (i.e., the faults that have not been randomly deleted). We deﬁne Xtrain

and Ytrain to be the resulting covariate and response matrices used to train the model. We

stack the data from the second half that are not used to train the model to deﬁne Xtest.

Xtest contains the data where the response start F k (Ytest) is unknown. We then estimate

ˆptest (ptest = Pr(Ytest = 1)), and predict deleted fault start time based on the magnitude of

ˆptest. We specify our model as follows to gain the optimal performance. The optimal tuning

parameter and thresholds are found by cross validation.

• We tried diﬀerent min lag and max lag combinations, and the best one we found is

min lag = −8 and max lag = 4.

• For k consecutive estimates of ptest (i.e., the estimated probability a fault starts for

k consecutive time intervals), we found the largest probability and compared it to a

threshold p. If it exceeded p, the corresponding timestamp was saved as a predicted

start time. We tested diﬀerent combinations of values for k and p. The best performing

combination was k = 6 and p = 0.75. See Figure 8 for an illustration.

11

• We compared the performance of the various algorithms modeling covariates (month,

hour, S3, R1, R2, R3, R4, etc) as categorical versus continuous variables. No real

improvement was made modeling them as categorical variables, so all covariates are

treated as continuous.

• We experimented with various diﬀerent classiﬁers including KNN, Naive Bayes, GBM,

random forest, and penalized logistic regression. We found that random forest and

penalized logistic regression performed the best. Our ﬁnal algorithm was an ensemble

of these latter two models, where we kept all predicted start times from random forest,

and then added all predicted start times that were found in penalized logistic regression

but not found in random forest.

One can determine which covariates are most important in predicting fault start times by

looking at the random forest results. Each covariate can be scored based on mean decrease in

impurity. Speciﬁcally, we add up the total amount that the Gini index is decreased by splits

over a given predictor averaged over all trees, and this value is the measurement of feature

importance. Gini index is deﬁned as

G =

K

X

i=1

ˆpmk(1 − ˆpmk),

where ˆpmk represents the proportion of observations in the mth node that belongs to the kth

class and K is the total number of classes [James et al., 2013]. Table 5 lists the top 15 most

important covariates and their corresponding score (standardized) for each one of the ﬁve

fault types in plant 1 and 6. The importance of covariates vary from fault to fault and from

plant to plant. S3 is the most important covariate in predicting the start time of F1 in plant

1, while both R1 and S3 seem to be important in predicting F1’s start time in plant 6; S3 is

the most important covariate in predicting F2’s start time, while E3 is the most important

covariate in predicting F4’s start time in plant 1.

Table 6 shows the percentage of times that each covariate ranked in the top 15 importance

score (random forest to predict fault start time) averaged over all plants. S1-S4 seem to be

more important than R1-R4, and the covariates time, month, E2 and E3 are also important

features in the models.

12

Table 5: Top 15 most important covariates to predict fault start time for each of ﬁve faults
in plant 1 and 6.

Plant Rank

F1

F2

F3

F4

F5

1

6

covariate

score covariate

score covariate

score covariate

score covariate

score

1 R4 m4 S3 0.0136 R1 m5 S3
2 R5 m4 S3 0.0118 R4 m5 S3
3 R3 m4 S3 0.0104 R8 m5 S3
4 time
0.0095 R3 m5 S3
5 R8 m6 S3 0.0089 R2 m5 S3
6 R2 m4 S3 0.0088 R6 m5 S3
7 R7 m4 S3 0.0086 R7 m5 S3
8 L4 m6 S3 0.0073 R5 m5 S3
9 month
0.0072 L1 m5 S3

10 R5 m6 S3 0.0067 m5 S3
11 R6 m4 S3 0.0067 L4 m5 S3
12 R1 m4 S3 0.0060 R7 m2 S2
13 m4 S3
0.0059 R8 m2 S4
14 R4 m5 S3 0.0059 R7 m2 S4
15 R6 m6 S3 0.0058 R5 m2 S3

0.0293 R7 m2 S4
0.0268 R7 m2 S2
0.0247 R6 m2 S4
0.0245 R1 m5 S3
0.0241 R2 m5 S3
0.0229 R6 m5 S3
0.0190 R6 m2 S2
0.0181 R1 m2 S2
0.0169 R8 m2 S4
0.0119 R3 m5 S3
0.0108 m5 S3
0.0095 R4 m5 S3
0.0088 R4 m2 S2
0.0086 R8 m2 S2
0.0080 R5 m2 S2

0.0180 R5 n3 E3 0.0241 R4 m3 R1 0.0131
0.0179 n3 E3
0.0230 R8 m1 S1 0.0105
0.0163 R1 n3 E3 0.0194 R5 m5 S4 0.0099
0.0153 R3 n3 E3 0.0179 R3 m3 R1 0.0091
0.0147 R6 n3 E3 0.0176 R7 m5 S2 0.0085
0.0146 R8 n3 E3 0.0166 R5 m3 R1 0.0084
0.0145 R2 n3 E3 0.0160 R7 m1 S1 0.0082
0.0143 R4 n3 E3 0.0139 R4 m5 S2 0.0081
0.0140 R7 n3 E3 0.0137 R3 m1 S3 0.0080
0.0137 R1 m2 S2 0.0131 m6 R1
0.0080
0.0133 L1 m2 S2 0.0124 R1 m1 R1 0.0076
0.0130 L2 n3 E3 0.0116 R5 m1 S3 0.0076
0.0125 R1 m2 S4 0.0114 L4 m2 S3 0.0073
0.0123 L1 n3 E3 0.0110 L1 m2 S3 0.0072
0.0119 L3 m2 S2 0.0104 R6 m2 S3 0.0070

1 R7 m1 S3 0.0110 L4 m7 S1
2 R3 m4 R1 0.0103 R3 m9 S3
3 R1 m4 R1 0.0095 R2 m9 S3
4 m4 R1
0.0095 R1 m9 S3
5 L2 m4 R1 0.0091 R4 m9 S3
6 R7 m4 R1 0.0090 L3 m7 S1
7 R5 m4 R1 0.0079 R4 n1 E3
8 R8 m1 S3 0.0078 L1 m7 S1
9 R7 m7 S3 0.0072 R7 m7 S3

0.0116 L4 m7 S1
0.0151 R5 m8 S1 0.0187 R5 m8 S4 0.0244
0.0095 R4 m9 R2 0.0130 R6 m8 S1 0.0170 R4 n2 E3 0.0237
0.0122 R4 m8 S1 0.0155 R4 n1 E3 0.0197
0.0090 R4 m9 S3
0.0119 R8 m8 S1 0.0151 R4 m8 S2 0.0195
0.0084 L4 m3 S1
0.0075 hour
0.0110 R7 m8 S1 0.0138 time
0.0160
0.0100 R2 m8 S1 0.0137 R1 m8 S2 0.0147
0.0074 R8 m9 S1
0.0070 R3 m9 S3
0.0092 R1 m8 S1 0.0124 R1 m8 S4 0.0147
0.0064 R4 m9 R3 0.0087 R3 m8 S1 0.0113 R5 m4 R1 0.0146
0.0064 R4 m9 R4 0.0084 m8 S1
0.0092 R7 m4 R1 0.0141
0.0064 R5 m10 R2 0.0082 R5 n1 E3 0.0069 R3 m4 R3 0.0140
10 R4 m7 S3 0.0070 m7 S1
0.0062 R4 m9 S1
11 time
0.0082 R4 n1 E3 0.0064 R3 m4 R1 0.0132
12 R2 m7 S3 0.0061 L4 m3 S1
0.0079 R2 n2 E3 0.0061 R5 n1 E3 0.0129
0.0061 R6 m9 S1
13 R4 m4 R1 0.0061 L2 m7 S1
0.0078 R3 n2 E3 0.0059 R7 n2 E3 0.0128
0.0059 R3 m9 S1
14 R7 n1 E2 0.0060 R5 m10 R2 0.0056 R5 m9 S1
0.0078 R2 n1 E3 0.0059 R4 m4 R3 0.0127
15 R6 m7 S3 0.0059 R5 m9 S3
0.0053 R5 m10 R4 0.0076 R5 n2 E3 0.0057 R5 n2 E3 0.0126

0.0069 time

3.5 Predict End Time

To predict the end time of a plant fault, we built another classiﬁcation model. As with the

start time prediction problem, we estimate a separate model for each fault type and plant.

To explain our modeling approach, suppose we want to ﬁnd the end time of a predicted type

1 fault (F 1). We ﬁrst estimate an upper bound for the duration of a F 1 fault (tmax) based

on all known F 1 events. We estimate tmax as follows: tmax = max(8, q0.95), where q0.95 is the

95% upper quantile of all historical F1 durations.

Intuitively, we could predict the fault end time by calculating the elapsed time since the

13

Table 6: Percentage that each covariate rank top 15 in the importance score (random forest
to predict fault start time) averaged over all plants.

Covariate

F1

F2

F3

F4

F5

time
month
hour
weekday
S1
S2
S3
S4
R1
R2
R3
R4
E2
E3

60% 48% 31% 32% 18%
27% 14% 17% 11% 4%
5% 12% 8% 4% 0%
2% 0% 0% 2% 2%
32% 40% 81% 82% 64%
54% 48% 58% 36% 88%
65% 78% 33% 34% 20%
49% 52% 56% 41% 90%
19% 5% 10% 14% 10%
17% 19% 23% 16% 14%
19% 12% 8% 12% 12%
2% 3% 6% 4% 4%
19% 31% 12% 25% 8%
25% 41% 17% 39% 16%

fault ﬁrst occurred. We denote it as elapsed t which is measured in units of 15 minutes. We

ﬁnd that the model only based on elapsed t isn’t accurate enough, so we add other covariates

to the model.

The classiﬁcation model is trained using data from the tmax intervals following the onset

of each known F 1 event. These data, stacked together, form the matrix of covariates for

the training model (Xtrain). end F 1, the binary indicator of whether fault 1 ends within

one hour of the corresponding timestamp, serves as the response variable, Ytrain. Once the

classiﬁcation model is trained, it is used to estimate the probability that each of our predicted

events will end in any one of the tmax time periods following the predicted event start time.

These predictions are based on Xtest, formed by stacking the tmax intervals following the

onset of each predicted F 1 event.

Given the small penalty for false negative predictions relative to the reward for a true

positive prediction, we allow our system to predict as many as two end times for each predicted

event. The ﬁrst estimated end time is made by ﬁnding the time period within the tmax periods

following our predicted event with the largest estimated probability that end F 1=1. This

is our ﬁrst end time prediction. To look for a possible second prediction, we delete all

observations with timestamps within one hour of our ﬁrst estimated end time. We then ﬁnd

14

the (remaining) time period with the largest probability. If the estimated probability in this

period is larger than our threshold p2, this is a second end time prediction. If the probability

is less than p2, only one end time is predicted. See Figure 9 for an illustration, where the

second end time prediction is elapsed t = 7 and is kept as ˆp > p2(= 0.2).

The following list details the speciﬁcs of our ﬁnal algorithm for the end time classiﬁcation

problem. The optimal tuning parameter and thresholds are found by cross validation.

• The optimal threshold probability for deciding on a second end time is p2=0.2.

• The optimal lag choice is min lag = −8 and max lag = 8.

• We model all covariates as continuous variables.

• We have compared the performance of various diﬀerent classiﬁer methodologies includ-

ing GBM, random forest, and penalized logistic regression. We ﬁnd that GBM has the

best performance. We choose tree number=200 and tree depth=5 for the GBM.

As with the start times, we ﬁnd the most important covariates in predicting fault end

time by calculating a score based on mean decrease impurity in GBM. In Table 7 we list

the top 15 most important covariates and their corresponding score in predicting fault end

time for each one of the ﬁve fault types in plant 1 and 6. The most important covariate is

elapsed t, which ranks number one in all cases.

Table 8 shows the percentage of times that each covariate ranked in the top 15 impor-

tance score (GBM to predict fault end time) averaged over all plants. elapsed t is the most

important covariate. S1-S4 are again more important than R1-R4, and the covariates time,

hour, weekday, E2 and E3 are also important features in the models.

4 Conclusion

In this paper, we proposed and implemented a machine learning based algorithm to detect

industrial plant faults. The encouraging results demonstrated the usefulness of data-driven

algorithms in fault detection of complex systems. Several extensions to our algorithms were

considered but not implemented due to the time constraints of the PHM Society Data Chal-

lenge. These additional approaches are left as future work.

15

Table 7: Top 15 most important covariates to predict fault end time for each of ﬁve faults in
plant 1 and 6.

Plant Rank

F1

F2

F3

F4

F5

covariate

score covariate

score covariate

score covariate

score covariate

score

1

6

0.0338 time

0.0707 elapsed t 0.1406 elapsed t 0.2616 elapsed t
0.0285 time
0.0182 L1 m5 S1 0.0121 L1 m5 S1 0.0101 L2 m3 S1 0.0131 time

0.2444 elapsed t 0.1472
1 elapsed t
0.0374 L1 m3 S1 0.0186 L1 m3 S1 0.0678
2 time
3 L1 m4 S3
0.0574
4 R8 m4 S3 0.0155 R8 m2 S3 0.0116 hour
0.0098 L3 m6 S1 0.0127 L2 m3 S1 0.0516
5 R7 m4 S3 0.0110 R8 m2 S1 0.0110 L1 m2 S1 0.0092 L2 m6 R1 0.0104 L3 m2 S1 0.0159
6 L2 m4 S3
0.0093 L1 m2 S1 0.0095 L8 m2 S1 0.0076 R6 n3 E2 0.0088 L3 m3 S1 0.0142
7 R8 m1 S4 0.0076 R7 m2 S1 0.0091 R3 n2 E2 0.0074 R4 m3 S1 0.0085 L6 m5 S1 0.0093
8 L1 m4 S4
0.0083 L1 m3 S2 0.0088
9 R8 m1 S3 0.0069 R6 m2 S1 0.0079 R8 m4 S4 0.0059 R2 m3 S1 0.0075 R8 m1 S1 0.0087
10 R5 m4 S3 0.0067 L1 m3 S1 0.0075 R8 m2 S4 0.0056 R6 n3 E3 0.0075 R8 m3 S1 0.0075
11 R6 m4 S4 0.0066 L2 m5 S1 0.0073 L2 m5 S1 0.0055 R5 m2 S1 0.0074 R8 m1 S4 0.0074
12 R7 m1 S4 0.0064 R8 m5 S1 0.0067 L2 m2 S1 0.0051 weekday
0.0073 L1 m2 S1 0.0068
13 R8 m4 S4 0.0063 R8 m5 S4 0.0063 L5 m2 S1 0.0048 L1 m6 R1 0.0068 L8 m6 S1 0.0066
0.0062 L1 m2 S4 0.0060 L8 m1 S1 0.0048 R8 n3 E3 0.0066 L5 m4 S1 0.0064
14 L2 m1 S4
15 L2 m4 S4
0.0061 R8 m5 S2 0.0060 R7 n3 E2 0.0046 R3 n3 E2 0.0063 L1 m3 S4 0.0064

0.0083 weekday

0.0070 hour

0.0065 time

0.0100 L1 m9 S3 0.0114 m6 S1

0.0434 L1 m9 S1 0.0683 L2 m8 S1 0.0380 time

0.1210 elapsed t 0.1633 elapsed t 0.0934 elapsed t
0.0292 time

0.3109 elapsed t 0.1330
1 elapsed t
0.0276
2 time
3 R8 m9 S3 0.0111 R8 m9 S3 0.0144 R8 m9 S1 0.0365 L1 m8 S1 0.0366 R8 m8 S4 0.0151
4 L1 m9 S3
0.0229 L8 m3 S4 0.0263 L1 m8 S2 0.0147
5 R8 m10 S3 0.0081 L1 m9 S1 0.0067 L1 m6 S1 0.0224 L8 m3 S2 0.0167 R8 m8 S2 0.0144
0.0071 L1 m9 S2 0.0060 L1 m3 S1 0.0162 L1 m3 S4 0.0166 L1 m8 S4 0.0118
6 hour
7 L1 m10 S3 0.0059 R7 m9 S3 0.0059 L1 m4 R1 0.0161 L1 m6 S1 0.0164 R8 m8 S1 0.0113
8 L1 m10 S4 0.0058 L1 m9 S4 0.0055 L6 m9 S4 0.0115 L1 m3 S2 0.0151 R6 m8 S1 0.0081
0.0095 m7 S1
9 L1 m7 S2
0.0124 L1 m8 S1 0.0073
0.0052 R8 m1 S3 0.0053 n1 E2
10 R8 m9 S4 0.0051 L2 m9 S3 0.0048 m8 S1
0.0086 R8 m8 S1 0.0117 L2 m8 S1 0.0072
11 L1 m7 S4
0.0046 R8 n2 E2 0.0047 L6 m9 S2 0.0082 L1 m7 S1 0.0109 L8 m8 S1 0.0072
0.0045 R8 m9 S1 0.0047 L2 m9 S1 0.0076 R8 n1 E3 0.0092 R4 m8 S1 0.0070
12 weekday
13 R7 m10 S1 0.0044 L1 m1 S3 0.0044 L1 m2 S1 0.0075 m3 S2
0.0089 L7 m2 S1 0.0069
14 R8 n1 E3
0.0042 R5 m9 S4 0.0059 L1 m10 S1 0.0085 R8 m1 S1 0.0067
15 R8 m9 S1 0.0042 L1 m7 S3 0.0042 R7 m4 S4 0.0057 R4 m6 S1 0.0081 R5 m8 S1 0.0062

0.0043 hour

One such approach would be to not model each plant independently. Alternatively, we

could try to ﬁrst group the plants into clusters of like plants (based on like distributions

and/or timing of faults, for example), and then model plants in each cluster together.

Another untried approach is deep learning neural networks. Convolutional neural net-

works or recurrent neural networks, which have been shown to be powerful tools when mod-

eling with large and complex datasets, may yield good results. Convolutional neural network

can automatically consider lagged observations by modeling temporal contiguous observa-

tions jointly together. Recurrent neural network can create an internal state of the network

which allows it to exhibit dynamic temporal behavior. These facts make deep learning neural

16

Table 8: Percentage that each covariate rank top 15 in the importance score (GBM to predict
fault end time) averaged over all plants.

Covariate

F1

F2

F3

F4

F5

elapsed t
time
month
hour
weekday
S1
S2
S3
S4
R1
R2
R3
R4
E2
E3

0%

93%
79%
0%
45%
30%

95%
98% 95% 93%
63%
94% 89% 78%
0%
0%
0%
12%
57% 58% 50%
21% 25% 35%
12%
65% 86% 98% 100% 100%
98%
90% 91% 91%
89% 82% 20%
14%
95%
97% 93% 93%
9%
11% 18% 15%
5%
2%
2%
4%
2%
7% 11%
7%
7%
0%
7%
2%
16%
22% 33% 35%
29% 37% 37%
40%

73%
12%
73%
25%
7%
9%
7%
30%
32%

networks potentially very useful in fault detection for the PHM data.

Lastly, in our approach, lagged covariates are added to the model which creates high

dimensional features. Curse of dimensionality may damnify the classiﬁers’ performances.

Techniques such as principal component analysis and functional data analysis can be applied

to extract key features from time series covariates and reduce the feature dimension.

Acknowledgment

The author wants to give thanks to SAS colleges Anya Mcguirk, Sergiy Peredriy, Arin Chaud-

huri, Alex Chien, Deovrat Kakde and Gul Ege for their help in the Prognostics and Health

Management Society 2015 data challenge competition.

References

[Aldrich and Auret, 2013] Aldrich, C. and Auret, L. (2013). Unsupervised process monitoring

and fault diagnosis with machine learning methods. Springer.

[Breiman, 2001] Breiman, L. (2001). Random forests. Machine learning, 45(1):5–32.

17

[Chiang et al., 2004] Chiang, L. H., Kotanchek, M. E., and Kordon, A. K. (2004). Fault

diagnosis based on ﬁsher discriminant analysis and support vector machines. Computers

& chemical engineering, 28(8):1389–1401.

[Chiang et al., 2000] Chiang, L. H., Russell, E. L., and Braatz, R. D. (2000). Fault diag-

nosis in chemical processes using ﬁsher discriminant analysis, discriminant partial least

squares, and principal component analysis. Chemometrics and intelligent laboratory sys-

tems, 50(2):243–252.

[Friedman et al., 2001] Friedman, J., Hastie, T., and Tibshirani, R. (2001). The elements of

statistical learning, volume 1. Springer series in statistics Springer, Berlin.

[Friedman, 2001] Friedman, J. H. (2001). Greedy function approximation: a gradient boost-

ing machine. Annals of statistics, pages 1189–1232.

[Friedman, 2002] Friedman, J. H. (2002). Stochastic gradient boosting. Computational Statis-

tics & Data Analysis, 38(4):367–378.

[He and Wang, 2007] He, Q. P. and Wang, J. (2007). Fault detection using the k-nearest

neighbor rule for semiconductor manufacturing processes. Semiconductor manufacturing,

IEEE transactions on, 20(4):345–354.

[Hosmer Jr and Lemeshow, 2004] Hosmer Jr, D. W. and Lemeshow, S. (2004). Applied lo-

gistic regression. John Wiley & Sons.

[James et al., 2013] James, G., Witten, D., Hastie, T., and Tibshirani, R. (2013). An intro-

duction to statistical learning, volume 112. Springer.

[Korbicz et al., 2012] Korbicz, J., Koscielny, J. M., Kowalczuk, Z., and Cholewa, W. (2012).

Fault diagnosis: models, artiﬁcial intelligence, applications. Springer Science & Business

Media.

[Lee et al., 2014] Lee, S., Park, W., and Jung, S. (2014). Fault detection of aircraft system

with random forest algorithm and similarity measure. The Scientiﬁc World Journal, 2014.

[Rish, 2001] Rish, I. (2001). An empirical study of the naive bayes classiﬁer. In IJCAI 2001

workshop on empirical methods in artiﬁcial intelligence, volume 3, pages 41–46. IBM New

York.

18

[Rosca et al., 2015] Rosca, J., Song, Z., Willard, N., and Eklund, N. (2015). PHM15

Challenge Competition and Data Set: Fault Prognostics, NASA Ames Prognostics Data

Repository (http://ti.arc.nasa.gov/project/prognostic-data-repository), NASA Ames Re-

search Center, Moﬀett Field, CA.

[Samanta, 2004] Samanta, B. (2004). Gear fault detection using artiﬁcial neural networks

and support vector machines with genetic algorithms. Mechanical Systems and Signal

Processing, 18(3):625–644.

[Wang and Yu, 2005] Wang, L. and Yu, J. (2005). Fault feature selection based on modi-

ﬁed binary pso with mutation and its application in chemical process fault diagnosis. In

Advances in Natural Computation, pages 832–840. Springer.

[Yin et al., 2012] Yin, S., Ding, S. X., Haghani, A., Hao, H., and Zhang, P. (2012). A

comparison study of basic data-driven fault diagnosis and process monitoring methods on

the benchmark tennessee eastman process. Journal of Process Control, 22(9):1567–1581.

19

m1

R1 R2 R3 R4 S1 S2 S3 S4

(a) Component 1

m2

1
R

2
R

3
R

4
R

1
S

2
S

3
S

4
S

1
R

2
R

3
R

4
R

1
S

2
S

3
S

4
S

0.8

0.4

0.0

0.4

0.8

0.8

0.4

0.0

0.4

0.8

R1 R2 R3 R4 S1 S2 S3 S4

(b) Component 2

Figure 1: Correlation heatmap for the ﬁrst two components of plant 1.

20

plant1 (3 zones)

1
m

2
m

3
m

4
m

5
m

6
m

0.8

0.4

0.0

0.4

0.8

m1 m2 m3 m4 m5 m6

Figure 2: Correlation heatmap of R4 across all components in plant 1.

300

250

200

t
n
u
o
c

150

100

50

0

Count plot of F2 by month

plant
1
6
12

1

2

3

4

5

6
7
month

8

9

10 11 12

Figure 3: Histogram of fault 2 start times by month (January = 1) for plants 1, 6 and 12.

21

t

n
u
o
c

120

100

80

60

40

20

0

Count plot of F2 by hour

plant
1
6
12

0 1 2 3 4 5 6 7 8 9 1011121314151617181920212223

hour

Figure 4: Histogram of fault 2 start times by hour for plants 1, 6 and 12.

mean
lb
ub

e
u
a
V

l

100

90

80

70

60

50

40

30

−20 −15 −10

−5

0

5

Time to failure

Figure 5: Plot of m2 R2 against time to failure of F1. lb and ub represents 95% lower bound
and upper bound respectively.

22

Figure 6: Overall ﬂowchart.

Figure 7: Modeling ﬂowchart.

23

Figure 8: An example to predict the fault’s start time.

Figure 9: An example to predict the fault’s end time.

24

