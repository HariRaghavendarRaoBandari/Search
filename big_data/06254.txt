6
1
0
2

 
r
a

 

M
0
2

 
 
]

.

A
N
h
t
a
m

[
 
 

1
v
4
5
2
6
0

.

3
0
6
1
:
v
i
X
r
a

Stability Analysis

of

Bilinear Iterative Rational Krylov Algorithm✩

Rajendra Choudharya, Kapil Ahujaa,∗

aDiscipline of Computer Science and Engineering, Indian Institute of Technology Indore

Abstract

A real world physical or abstract phenomenon can be described mathematically by a dynam-
ical system. Dynamical systems arise in many applications like weather prediction, air-quality
data assimilation, biological systems, industrial optimal cooling, liquid ﬂow model, etc. Gener-
ally these dynamical systems are nonlinear in behaviour. In the past, substantial work has been
done on linear dynamical systems. Here, we work on a special case of nonlinear dynamical sys-
tem called bilinear dynamical system. Bilinear dynamical systems are a bridge between linear
and nonlinear dynamical systems.

Models coming from diﬀerent physical applications are very large in size. Simulation with
such systems is expensive so one usually obtains a reduced model (by model reduction) that
replicates the input-output behaviour of the original full model.

A recently proposed algorithm for model reduction of bilinear dynamical systems, Bilinear
Iterative Rational Krylov Algorithm (BIRKA), does so in an optimal way. The algorithm requires
solving very large linear systems of equations. Usually these systems are solved by direct meth-
ods (e.g., LU), which are very expensive. A better choice is iterative methods (e.g., Krylov).
However, iterative methods introduce errors in linear solves because they are not exact. They
solve the linear system up to a certain tolerance. The main contribution of this work is that we
prove that BIRKA is stable with respect to the error introduced by inexact linear solves. We
support our result by experiments.

Keywords: Bilinear Dynamical Systems, Model Reduction, Iterative Solves, Krylov Subspace
Methods, Petrov-Galerkin, Backward Stability.
2010 MSC: 34C20, 41A05, 65F10, 65G99

1. Introduction

A dynamical system describes a relation between two or more measurable quantities by a set
of diﬀerential equations. The system may be linear or nonlinear. Bilinear dynamical system is

✩ This material is based upon work supported by Council of Scientiﬁc and Industrial Research (India) Grant Number

25/(0220)/13/EMR-II.

∗Corresponding author
Email addresses: phd1301201004@iiti.ac.in (Rajendra Choudhary), kahuja@iiti.ac.in (Kapil Ahuja)
URL: http://www.iiti.ac.in/people/~kahuja/ (Kapil Ahuja)

Preprint submitted to Journal of Linear Algebra and its Applications Templates

March 22, 2016

one such weakly nonlinear system. The system can be described both in time and in frequency
domain.
In time domain, a Single Input Single Output (SISO) bilinear dynamical system is
represented as follows [1, 2]1:

˙x(t) = Ax(t) + Nx(t) · u(t) + bu(t),
y(t) = cx(t),

(1)

where A, N ∈ Rn×n, b ∈ Rn×1, and c ∈ R1×n. u(t) and y(t) : R → R represent input and output
of the bilinear system, respectively.
It is diﬃcult to formulate the transfer function of the complete bilinear dynamical system,
therefore, in [2, 3] the authors represent the transfer function of a SISO bilinear dynamical system
in frequency domain by a sequence of subsystem transfer functions, i.e.,

ζ = {H1(s1), H2(s1, s2), H3(s1, s2, s3), . . . Hk (s1, s2, . . . , sk) , . . .∞},

where s1, s2, . . . , sk are the frequencies, and kth order subsystem transfer function is given as
follows:

Hk (s1, s2, . . . , sk) = c (skIn − A)−1 N (sk−1In − A)−1 N . . . (s2In − A)−1 N (s1In − A)−1 b,

where In is the identity matrix of order n. If in (1), the matrix N is a zero matrix, then the system
is a linear dynamical system. That is, a SISO linear dynamical system is represented as

˙x(t) = Ax(t) + bu(t),
y(t) = cx(t).

(2)

The transfer function of the linear dynamical system in frequency domain is deﬁned as follows:

H(s) = c(sIn − A)−1b.

In general, dynamical systems corresponding to real world applications are extremely large
in size. Simulation and computation with such systems requires large amount of space and
time. By using model reduction techniques [4], these large dynamical systems are reduced into
smaller size, which makes the simulation and computation easier. Model reduction can be done
in many ways, i.e., by using balanced truncation, Hankel approximations or Krylov projection
[4]. Projection methods obtain the reduced model by projecting the original full model on a
lower dimensional subspace, and are quite popular. In literature, there are several techniques of
projecting [5, 4, 6, 7, 8] a dynamical system. Petrov-Galerkin projection is one such projection
technique that gives nice properties in the reduced model. Interpolation is usually used to obtain
the subspaces involved in Petrov-Galerkin projection.

Based on the theory of Petrov-Galerkin based interpolatory model reduction, authors in [9,
5, 10] have proposed Iterative Rational Krylov Algorithm (IRKA) for model reduction of linear
dynamical system. IRKA provides the reduced model which is optimal (the kind of optimality is
discussed in the next section). Similar to IRKA, authors in [2, 1, 11, 12] have proposed Bilinear
Iterative Rational Krylov Algorithm (BIRKA) for model reduction of bilinear dynamical system.

1We focus on SISO bilinear dynamical system, but the results of this paper carry to Multiple Input Multiple Output

(MIMO) dynamical system as well.

2

The main computational bottleneck in reducing larger models (or dynamical systems) is solv-
ing large sparse linear systems of equations. The reason for this is that typically, model reducers
use direct solvers, e.g. LU factorization to solve such linear systems of equations, which are
expensive. The solution to this scaling problem is to use iterative methods, e.g. Krylov subspace
methods.

Application of Krylov subspace methods for IRKA has been done [13, 14, 15]. Iterative
methods are inexact, i.e., they solve the linear systems of equations upto a certain stopping
tolerance. Hence, it becomes important to check if the model reduction algorithm (IRKA or
BIRKA) is stable with respect to these inexact solves. In other words, we need to check that
small error in linear solves does not substantially deteriorate the quality of the reduced model.
For IRKA, stability analysis has been done in [16]. We do the same for BIRKA, i.e., prove that
BIRKA is stable with respect to the inexact linear solves. This is a substantial contribution since
users will have more conﬁdence in using iterative solvers for BIRKA.

In the next section (Section 2) we discuss model reduction of linear and bilinear systems by
Petrov-Galerkin based interpolatory model reduction framework. We discuss stability of both
IRKA and BIRKA in Section 3. In Section 4, we support our proof with numerical experiments.
Finally, we give concluding remarks and discuss future work in Section 5. For the rest of this
paper we use the terms and notations as listed below.

a. In literature, generally H2−norm is used for getting the error between two dynamical sys-

tems, which for a multivariable system is deﬁned as [2]

kHk(s1, . . . , sk)k2

H2

=  1
2π!kZ ∞

−∞

. . .Z ∞

−∞

kHk(iω1, . . . , iωk)k2

F dω1 . . . dωk.

There is one more functional norm that is used often. This is the H∞−norm. For a multi-
variable system it is given as [2]

kHk(s1, . . . , sk)kH∞

= max

ω1, ..., ωk∈R kHk(iω1, . . . , iωk)k2 .

If the type of norm is not written, then in the case of functional norm it can either be a
H2−norm or a H∞−norm. In the case of matrices it is a 2-norm.

b. The Kronecker product between two matrices P (of size m × n), and Q (of size s × t) is

deﬁned as

P ⊗ Q =

p11Q · · ·
...
. . .
pm1Q · · ·

p1nQ
...

pmnQ

,



where pi j is an element of matrix P and order of P ⊗ Q is ms × nt.

c. vec operator on a matrix P is deﬁned as

vec(P) = (p11, . . . , pm1, p12, . . . , pm2, . . . . . . , p1n, . . . , pmn)T .

d. Also, R denotes the set of real numbers, and F denotes a discrete subset of real numbers.

3

2. Petrov-Galerkin Based Interpolatory Model Reduction Framework

According to Petrov-Galerkin projection, the residual of a dynamical system obtained after
projecting on a lower dimension subspace, is made orthogonal to some other subspace deﬁned
by a test basis. Let ηi denotes the residual of this dynamical system, then according to the Petrov-
Galerkin condition, ηi ⊥ L, where L denotes any test subspace.
The subspace on which we project, and the orthogonal subspace are not known to us. We
can arbitrarily pick these subspaces, but then we cannot guarantee good input-output behaviour
from the reduced model. For the reduced model to provide a high ﬁdelity approximation to the
input-output behaviour of the original full model, we use interpolation to obtain these subspaces.
In the next subsection, we apply Petrov-Galerkin based interpolation for model reduction of a
linear dynamical system [5]. In Subsection 2.2, we apply the same to the bilinear dynamical
system [1].

2.1. Petrov-Galerkin Based Interpolatory Model Reduction of Linear Dynamical Systems

Let the reduced linear dynamical system be represented as [5, 6, 7]

˙xr(t) = Ar xr(t) + bru(t),
yr(t) = cr xr(t),

(3)

where Ar ∈ Rr×r, br ∈ Rr×1, cr ∈ R1×r, and r << n. The transfer function of the reduced model is

Hr(s) = cr(sIr − Ar)−1br.

Let, two r-dimensional subspaces, Vr and Wr, be chosen in such a way that Vr = Range(Vr)
and Wr = Range(Wr), where Vr ∈ Rn×r and Wr ∈ Rn×r are matrices. We project the original full
model (2) to a lower dimensional subspace, i.e., x (t) ≈ Vr · xr(t), and enforce the Petrov-Galerkin
condition [5]

WT
r (Vr ˙xr(t) − AVr xr(t) − bu(t)) = 0,

y(t) = cVr xr(t).

Comparing the above equations with (3), we get

Ar =(cid:16)WT

r Vr(cid:17)−1

WT

r AVr,

br =(cid:16)WT

r Vr(cid:17)−1

WT

r b,

and cr = cVr.

Diﬀerent choice of subspaces Vr and Wr give diﬀerent reduced models. We focus on Her-
mite interpolation of the transfer function to obtain these subspaces. Hermite interpolation is a
popular method from interpolatory theory, where a function and its derivative are interpolated.
Here, the transfer function of the original full model H(s) (and its derivative) and reduced model
(and its derivative) are interpolated at collection of interpolation points σi ∈ C. That is, [8, 17, 10]

H (σi) = Hr (σi) and H′ (σi) = H′r (σi),

for i = 1, 2, . . . , r. Theorem 1 below provides us Vr and Wr, such that the above interpolation
conditions are satisﬁed.
Theorem 1. [7, 13] Given H(s) = c(sIn − A)−1b and r distinct points σ1, σ2, . . . , σr ∈ C, let
Vr =h(σ1In − A)−1 b, (σ2In − A)−1 b, . . . , (σrIn − A)−1 bi and
Wr =h(σ1In − A)−T cT , (σ2In − A)−T cT , . . . , (σrIn − A)−T cTi.
If using Petrov-Galerkin condition, we deﬁne the reduced model Hr(s) = cr(sIr − Ar)−1br, then

4

H (σi) = Hr (σi) and H′ (σi) = H′r (σi)

provided that (σiIn − A) and (σiIr − Ar) are invertible for i = 1, 2, . . . , r.

The above Theorem 1 does not tell at what points to interpolate, such that we obtain the
optimal reduced model. Optimality means that the error between the original full model and the
reduced one should be minimum.

We want that for large number of input classes u(t), yr(t) ≈ y(t). One way to achieve this
t>0 |y(t) − yr(t)| uniformly small over all inputs. This can be related to the transfer

is to make max
function as follows [5]:

t>0 |y(t) − yr(t)| ≤ kH − HrkH2 .
max

This leads to the H2−optimal condition

kH − HrkH2 = min

dim(eHr)=r(cid:13)(cid:13)(cid:13)H − eHr(cid:13)(cid:13)(cid:13)H2

.

This is a non-convex optimization problem, and hence, ﬁnding its global minimum is hard. To
obtain a H2−optimal reduced model, one usually satisﬁes the ﬁrst order necessary conditions for
H2−optimality [5]. These conditions requires interpolation to be done on the mirror image of the
poles of the reduced system. Theorem 2 below summarizes this.
Theorem 2. [5, 13] For a SISO linear dynamical system H(s), let Hr(s) be an H2−optimal
reduced model of order r, having distinct poles λ1, λ2, . . . , λr. Then,

H (−λi) = Hr (−λi)

and H′ (−λi) = H′r (−λi)

for i = 1, 2, . . . , r.

To be able to satisfy the conditions of Theorem 2, we need the poles of the reduced model
(which we don’t have). Hence, in [5] authors give an algorithm called IRKA (Iterative Rational
Krylov Algorithm), which for any set of starting interpolation points converges to the reduced
model such that at convergence, conditions of Theorem 2 are satisﬁed. That is, at convergence,
IRKA gives the H2−optimal reduced model. Algorithm 1 lists IRKA.
Algorithm 1 IRKA [9, 5]

1: Given an input linear dynamical system A, b, and c.
2: Select initial interpolation points σi for i = 1, . . . , r, and stopping tolerance itol.

3: Vr =h(σ1In − A)−1 b, (σ2In − A)−1 b, . . . , (σrIn − A)−1 bi.
4: Wr =h(σ1In − A)−T cT , (σ2In − A)−T cT , . . . , (σrIn − A)−T cTi.
5: while(cid:0)relative change in (cid:8)interpolation points(cid:9) ≥ itol(cid:1)

r AVr.

a. Ar = WT
b. σi ← −λi (Ar) f or i = 1, . . . , r.
c. Vr =h(σ1In − A)−1 b, (σ2In − A)−1 b, . . . , (σrIn − A)−1 bi.
d. Wr =h(σ1In − A)−T cT , (σ2In − A)−T cT , . . . , (σrIn − A)−T cTi.

6: Ar = (WT

r Vr)−1WT

r AVr,

br = (WT

r Vr)−1WT

r b,

cr = cVr.

5

2.2. Petrov-Galerkin Based Interpolatory Model Reduction of Bilinear Dynamical Systems

After reduction, the bilinear system (1) can be represented as [1]

˙xr(t) = Ar xr(t) + Nr xr(t) · u(t) + bru(t),
yr(t) = cr xr(t),

(4)

where Ar, Nr ∈ Rr×r, br ∈ Rr×1, and cr ∈ R1×r with r << n. The reduced system in frequency
domain is represented by a sequence of transfer functions. That is,

ζr =(cid:8)Hr1(s1), Hr2(s1, s2), Hr3(s1, s2, s3), . . .∞(cid:9) .

As in the linear case, let two r-dimensional subspaces, Vr and Wr, be chosen in such a way
that Vr = Range(Vr) and Wr = Range(Wr), where Vr ∈ Rn×r and Wr ∈ Rn×r are matrices. We
project the original full model (1) to a lower dimensional subspace, i.e., x (t) ≈ Vr · xr(t), and
enforce the Petrov-Galerkin condition [1, 2]

WT
r (Vr ˙xr(t) − AVrxr(t) − NVr xr(t) · u(t) − bu(t)) = 0,
y(t) = cVrxr(t).

Comparing the above equations with (4), we get

WT

r Vr(cid:17)−1

WT

WT

r Vr(cid:17)−1

r Vr(cid:17)−1

r AVr, Nr =(cid:16)WT

r NVr, br =(cid:16)WT

Ar =(cid:16)WT
As in the linear case, here too diﬀerent selection of subspaces Vr and Wr give diﬀerent
reduced models, but we choose the subspaces Vr and Wr by enforcing interpolation. In the case
of bilinear systems, there are two ways of doing interpolation [2].
As earlier, the transfer function of a bilinear system can be written in the form of subsystem
transfer functions. If we apply the interpolation conditions on such ﬁnite number of subsystems
then, it is called subsystem interpolation. Another way is Volterra series interpolation. The
response of SISO bilinear system, represented by (1), can be computed as [2]

r b, and cr = cVr.

y(t) =

∞Xk=1Z t

0 Z σ1
0 Z σ2

0

ceA(t−σ1)NeA(σ1−σ2) . . . . . . NeA(σk−1−σk)

. . .Z σk−1
bu (σk) u (σk−1) . . . . . . u (σ1) dσkdσk−1 . . . . . . dσ1.

0

The above representation is called the Volterra series representation [11, 18] of bilinear dynami-
cal system, and

hk (σ1, σ2, . . . , σk) = ceA(t−σ1)NeA(σ1−σ2)N . . . NeA(σk−1−σk)b

is called the Volterra kernel. In Volterra series interpolation, all Volterra kernels are interpolated.
As subsystem interpolation approach is unable to satisfy any optimality condition [2] (error
between original full model and reduced model is minimum in some norm), so our focus is on
Volterra series interpolation. As in linear case, we need to know how to build Vr and Wr such
that conditions of Volterra series interpolation are satisﬁed (equivalent of Theorem 1).

We also need to decide where to interpolate so that we get optimal reduced model. As earlier,
optimality means error between the original full model and the reduced model is minimum.
In linear case, we ﬁrst deﬁned H2−optimality and then listed the interpolation conditions for

6

obtaining the H2−optimal reduced model (in Theorem 2). In bilinear system, the following error
system expression is diﬀerentiated for getting H2−optimality conditions [1]:

=(cid:16)hc −ˇˇci ⊗hc −ˇci(cid:17) ×

0

Ir# −"In

0

0

Ir# ⊗"A 0

ˇA# −"N

0

0

0

ˇˇNT# ⊗"N 0

ˇN#!−1

0

kζ − ζrkH2
0 Λ# ⊗"In
 −"A 0
ˇb#! ,
× " b
ˇˇbT# ⊗"b

0

where ˇA, ˇb, ˇc, and ˇN are initial guesses for the reduced system, and ˇA = RΛR−1, ˇˇb = ˇbT R−T , ˇˇc =
ˇcR, and ˇˇN = RT ˇNT R−T . As in linear case, doing interpolation on inverse images of reduced
system helps achieve H2−optimality. Theorem 3 below summarizes this.
Theorem 3. [2] Let ζ be a SISO bilinear system of order n. Let ζr be an H2−optimal ap-
proximation of order r. Then, ζr satisﬁes the following multi-point Volterra series interpolation
conditions:

∞Xk=1
rXl1=1

. . .

∞Xk=1

rXl1=1
∞Xk=1
rXl1=1
∞Xk=1

. . .

. . .

. . .

and

φl1, l2, ..., lk Hk(cid:0)−λl1 , −λl2 , . . . , −λlk(cid:1) =

rXl1=1
rXlk =1
rXlk=1
φl1, l2, ..., lk Hrk(cid:0)−λl1 , −λl2 , . . . , −λlk(cid:1) ,
φl1, l2 , ..., lk
kXj=1
rXlk=1
φl1, l2, ..., lk
rXlk =1
kXj=1

Hk(cid:0)−λl1 , −λl2 , . . . , −λlk(cid:1) =
Hrk(cid:0)−λl1 , −λl2 , . . . , −λlk(cid:1) ,

∂
∂s j

∂
∂s j

where φl1, l2 , ..., lk and λl1 , λl2 , . . . , λlk are residues and poles of the transfer function Hrk associ-
ated with ζr, respectively.

As in the linear case, obtaining residues and poles of reduced system is not possible (since
we do not have the reduced system). In [1, 5] authors purpose Bilinear Iterative Rational Krylov
Algorithm (BIRKA), which for any set of starting points converges to the reduced model such
that at convergence, conditions of Theorem 3 are satisﬁed. Algorithm 2 lists BIRKA.

3. Backward Stability

In general, numerical algorithms for a problem are continuous in nature but, a digital com-
puter solves them in a discrete manner. The reason is limitation on the representation of real /
complex numbers. Since complex numbers can be represented by real numbers, we focus on
latter only. Let fd : R → F be a function giving a ﬁnite approximation to a real number. It
provides rounded equivalent as [19]

fd(x) = x(1 + ǫmachine) for all x ∈ R,

7

Algorithm 2 BIRKA [1, 2]

1: Given an input bilinear dynamical system A, N, b, c.
2: Select an initial guess for the reduced system as ˇA,

ˇN, ˇb, ˇc. Also select stopping tolerance

btol.

3: while(cid:16)relative change in eigenvalues of ˇA ≥ btol(cid:17)

a. RΛR−1 = ˇA, ˇˇb = ˇbT R−T , ˇˇc = ˇcR,
b. vec (V) =(cid:18)−Λ ⊗ In − Ir ⊗ A −
c. vec (W) =(cid:18)−Λ ⊗ In − Ir ⊗ AT −
ˇN =(cid:16)WT

ˇˇN = RT ˇNR−T .
ˇˇNT ⊗ N(cid:19)−1(cid:18)ˇˇbT ⊗ b(cid:19).
ˇˇN ⊗ NT(cid:19)−1(cid:16)ˇˇcT ⊗ cT(cid:17).
r Vr(cid:17)−1

d. Vr = orth (V) , Wr = orth (W).

r Vr)−1WT

ˇA = (WT

WT

r NVr,

r AVr,

e.

ˇb =(cid:16)WT

r Vr(cid:17)−1

4: Ar = ˇA Nr = ˇN,

WT
r b,
br = ˇb,

ˇc = cVr.

cr = ˇc.

where ǫmachine is the machine precision. Also, for every operation between two ﬁnite numbers,
the result is exact up to a relative error, i.e., for all x, y ∈ F

fd(x ⊕ y) = (x ⊕ y)(1 + ǫmachine),

where ⊕ can be any operation as +, −, ∗, /.
Consider a continuous mathematics algorithm f : X → Y. Say executing this algorithm on a
digital computer (that uses ﬁnite precision arithmetic) is represented asef : X → Y. To check how
good the approximated algorithm ef is, one usually computes accuracy of ef . We say an algorithm
ef is accurate if [19]
where x ∈ X. From the above equation, we ﬁnd that computing accuracy is not possible since
we do not know f (x). A more easier parameter to check the goodness of ef is stability. There are
multiple notions of stability. One such notion is backward stability, which says that an algorithm
f is backward stable if [19]

(cid:13)(cid:13)(cid:13)(cid:13) f (x) − ef (x)(cid:13)(cid:13)(cid:13)(cid:13)

= (cid:13)(ǫmachine),

k f (x)k

ef (x) = f (ex)
kx −exk
kxk

for someex with
= (cid:13)(εmachine).

This notion of backward stability is useful since one can easily compute accuracy for a backward
stable algorithm.
Theorem 4. [19] If f : X → Y is a backward stable algorithm, and k(x) is the condition number
of the problem, then the relative error

(cid:13)(cid:13)(cid:13)(cid:13) f (x) − ef (x)(cid:13)(cid:13)(cid:13)(cid:13)

k f (x)k

= (cid:13) (k(x) · εmachine) ,
8

where εmachine is the machine precision (or perturbation in x).

Let’s look at lines 5c. and 5d. in IRKA (Algorithm 1); and lines 3b. and 3c. in BIRKA (Algo-
rithm 2). There we need to solve linear systems to compute Vr, Wr and vec(V), vec(W) in IRKA
and BIRKA, respectively. Solving these linear systems by direct methods (such as LU, Gauss

over the linear systems here have sparse matrices. For such systems, iterative methods, e.g.

elimination, etc.) is too expensive(cid:16)time complexity of (cid:13) (n3), where n is the system size(cid:17). More
Krylov subspace methods [4], are preferred because of reduced complexity(cid:0)time complexity of
(cid:13)(n · nnz), where nnz is number of nonzeros in the matrix). Iterative methods are inexact in na-
ture, which means they don’t solve the linear system say, Ax = b exactly, instead Ax = b +
is solved, where δ is the stopping tolerance. Goal now is to ﬁnd that, if one uses iterative
δ
solver (also called inexact solver from now on) in IRKA or BIRKA, are these algorithms stable
with respect to these inexact solves. As earlier, we check for backward stability.

Let Vr and Wr in IRKA be calculated exactly, and f be the functional representation of the

interpolation process that uses Vr and Wr in IRKA (i.e., exact IRKA). Similarly, leteVr and eWr in
IRKA be calculated inexactly (i.e., by iterative solves), and ef be the functional representation of
the interpolation process that useseVr and eWr in IRKA (i.e., inexact IRKA). Then, from backward

stability deﬁnition, IRKA is backward stable if

ef (H(s)) = f (eH(s))
kH(s) − eH(s)kH2 or H∞
kH(s)kH2 or H∞

for some eH(s) with
= (cid:13)(kFk),

(5)

(6)

in inexact IRKA. This perturbation is denoted by F. For future, we denote f (H(s)) = Hr(s) and

As in IRKA, let in BIRKA vec(V) and vec(W) be calculated exactly, and g be the func-
tional representation of interpolation process that uses vec(V) and vec(W) in BIRKA (i.e., exact

where eH(s) is a perturbed full model corresponding to the error in the linear solves foreVr and eWr
ef (H(s)) = eHr(s).
BIRKA). Similarly, let vec(eV) and vec(eW) be calculated inexactly (i.e., by an iterative solver),
andeg be the functional representation of interpolation process that uses vec(eV) and vec(eW) in

BIRKA (i.e., inexact BIRKA). Then, from backward stability deﬁnition, BIRKA is backward
stable if

eg(ζ) = g(eζ)
kζ −eζkH2 or H∞
kζkH2 or H∞

for someeζ with
= (cid:13)(kFk),

(7)

(8)

whereeζ is perturbed full model corresponding to the error in the linear solves for vec(eV) and
vec(eW) in inexact BIRKA. This perturbation is denoted by F.

For IRKA, backward stability analysis has been done in [16]. In Subsection 3.1, we sum-
marize this. In Subsection 3.2, we give our original contribution where we show BIRKA is also
backward stable.

3.1. Backward Stability: IRKA [16]

In Algorithm 1, steps 5c. and 5d. require Vr and Wr, respectively, and each column of Vr and

Wr is calculated for diﬀerent interpolation points as

Vr(:,

j) = (σ jIn − A)−1b

and
9

Wr(:,

j) = (σ jIn − A)−T cT ,

where σ j ∈ {σ1, σ2, . . . , σr}.
As discussed earlier, we compute Vr and Wr by inexact solver, and denote them as eVr and
eWr. Let the residual associated with inexact linear solves for computing Vr(:,

η j and ξ j, respectively, for j = 1, 2, . . . , r. Then, above equations lead to

j) and Wr(:,

j) be

j) = b + η j
j) = cT + ξ j.

and

(9)

(10)

(σ jIn − A)eVr(:,
(σ jIn − A)T eWr(:,
r AeVr, ebr =(cid:16)eWT
eWT

Petrov-Galerkin projection connects the reduced model parameters (obtained by inexact IRKA)
to the original full model parameters as

eAr =(cid:16)eWT

r eVr(cid:17)−1

r eVr(cid:17)−1

r b,

eWT

and ecr = ceVr.

By backward stability deﬁnition, next we ﬁnd a perturbed full model whose exact interpola-
tion will give the reduced model as obtained by inexact interpolation of the original full model.
Suppose that perturbation F is in the matrix A, then

j) = b

and

j) = cT .

(12)

(13)

(11)

(14)

(15)

(16)

(17)

As earlier, using Petrov-Galerkin projection to connect reduced model parameters (obtained by
exact IRKA) with perturbed full model parameters we get

(cid:16)σ jIn − (A + F)(cid:17) eVr(:,
(cid:16)σ jIn − (A + F)(cid:17)T eWr(:,
r (A + F)eVr, bbr =(cid:16)eWT
eWT
F eVr(:,

j) = η j

and

bAr =(cid:16)eWT

r eVr(cid:17)−1

Comparing (9) with (12), and (10) with (13) we get

and bcr = ceVr.

r b,

r eVr(cid:17)−1
eWT
FT eWr(:,

j) = ξ j.

Let, the residual matrices Rb and Rc be deﬁned as Rb =hη1, η2, . . . , ηri and Rc =hξ1, ξ2, . . . , ξri.

Then, (15) can be written as

Combining above two expressions gives perturbation

or

RT
c .

and

and

FeVr = Rb
FeVr = Rb
r eVr(cid:17)−1
F = Rb(cid:16)eWT
eWr ⊥ Rb

r F = RT
c .

FTeWr = Rc
eWT
r eVr(cid:17)−1
r +eVr(cid:16)eWT
eWT
eVr ⊥ Rc,

If we multiple eWT

r from left andeVr from right in (16), and use Petrov-Galerkin conditions

and

we get

Using (17) in (14), and comparing with (11) we get

eWT
r FeVr = 0.
r FeVr(cid:17) =(cid:16)eWT
r AeVr + eWT

10

r eVr(cid:17)−1(cid:16)eWT
bAr =(cid:16)eWT
bbr =ebr, and bcr =ecr.

r eVr(cid:17)−1

eWT
r AeVr = eAr,

Thus,

ef (H (s)) = f (cid:16)eH (s)(cid:17) = eHr (s) , where
H(s) = c(sIn − A)−1b, eH(s) = c (sIn − (A + F))−1 b, and
eHr (s) =ecr(sIr − eA)−1ebr =bcr(sIr −bA)−1bbr.

Thus, we have satisﬁed the backward stability condition given in (5), which we call the ﬁrst
condition.

Next we show that the other condition given in (6) is also held, this we call as the second
condition. According to that condition, the diﬀerence between the original full model and per-
turbed full model will be the order of perturbation [16]. This we prove in absolute sense, but the
results holds in relative sense too. Consider

H (s) − eH (s) = c(sIn − A)−1b − c (sIn − (A + F))−1 b
= c(cid:16)(sIn − A)−1 (sIn − (A + F)) − In(cid:17) (sIn − (A + F))−1 b
= c (sIn − A)−1 (−F)(cid:16)In − (sIn − A)−1 F(cid:17)−1
(sIn − A)−1 b.

Thus,

. Taking H2−norm on both sides of (18) and squaring

H (s) − eH (s) = c (sIn − A)−1 M (s) (sIn − A)−1 b,

we get

where M (s) = (−F)(cid:16)In − (sIn − A)−1 F(cid:17)−1
2πZ ∞
−∞(cid:13)(cid:13)(cid:13)c (iωIn − A)−1 M (iω) (iωIn − A)−1 b(cid:13)(cid:13)(cid:13)
2(cid:13)(cid:13)(cid:13)(iωIn − A)−1 b(cid:13)(cid:13)(cid:13)

From the compatible norm property (i.e., kABkF ≤ kAkF kBk2) we get

2πZ ∞
−∞(cid:13)(cid:13)(cid:13)c (iωIn − A)−1(cid:13)(cid:13)(cid:13)

(cid:13)(cid:13)(cid:13)H (s) − eH (s)(cid:13)(cid:13)(cid:13)

(cid:13)(cid:13)(cid:13)H (s) − eH (s)(cid:13)(cid:13)(cid:13)

F kM (iω)k2

H2 ≤

2
H2

1

1

=

2

2

2
F dω.

According to the mean value theorem for integrals, if f (x) and g(x) are continuous functions and
g(x) is not changing sign for any x, then

(18)

2
2 dω.

(19)

Using this property in (19) we get

R ∞

−∞

f (x)g(x)dx ≤ max
c∈R

f (x)dx.

−∞

g(c)R ∞
ω∈R (cid:13)(cid:13)(cid:13)(iωIn − A)−1 b(cid:13)(cid:13)(cid:13)

2

2

1

2
2

or

2 · max

H2 ≤ max

ω∈R kM (iω)k2

(cid:13)(cid:13)(cid:13)H (s) − eH (s)(cid:13)(cid:13)(cid:13)
(cid:13)(cid:13)(cid:13)H (s) − eH (s)(cid:13)(cid:13)(cid:13)
Here, second and last terms in the right hand side are independent of perturbation F. Let’s take
a look at the term kM (s)k2
H∞

2πZ ∞
−∞(cid:13)(cid:13)(cid:13)c (iωIn − A)−1(cid:13)(cid:13)(cid:13)
H∞(cid:13)(cid:13)(cid:13)c (sIn − A)−1(cid:13)(cid:13)(cid:13)
H∞(cid:13)(cid:13)(cid:13)(sIn − A)−1 b(cid:13)(cid:13)(cid:13)
M (s) =(cid:16)M (s) (sIn − A)−1 − In(cid:17) F.

. M (s) can be rewritten as

H2 ≤ kM (s)k2

2
F dω

2
H2

11

2

.

Hence,

Thus,

kM (s)kH∞

= max

ω∈R kM (iω)k ≤ max

ω∈R (cid:13)(cid:13)(cid:13)M (iω) (iωIn − A)−1 − In(cid:13)(cid:13)(cid:13)kFk
≤(cid:16)1 + kM (s)kH∞(cid:13)(cid:13)(cid:13)(sIn − A)−1(cid:13)(cid:13)(cid:13)H∞(cid:17)kFk .
(cid:16)1 − kFk(cid:13)(cid:13)(cid:13)(sIn − A)−1(cid:13)(cid:13)(cid:13)H∞(cid:17) .

kFk

kM (s)kH∞ ≤

From the above inequality we get that

Since both the conditions of backward stability given in (5) and (6) are satisﬁed by IRKA, thus,
IRKA is backward stable with respect to inexact linear solves.

kM (s)kH∞

(cid:13)(cid:13)(cid:13)H (s) − eH (s)(cid:13)(cid:13)(cid:13)H2

= (cid:13)(kFk) or
= (cid:13)(kFk).

Next, we relate the norm of perturbation F with norm of residuals. This is required for

determining the accuracy of the reduced model (as discussed in Theorem 4).
Theorem 5. [16] Given an original full model H(s) = c(sIn−A)−1b, interpolation points {σ j}r
Letev j andew j be inexact solution of (σ jIn − A)−1b and (σ jIn − A)−T cT , respectively, andeVr and
eWr denote the corresponding inexact interpolatory bases, i.e.,

j=1.

eVr = [ev1, ev2, . . . , evr] and eWr = [ew1, ew2, . . . , ewr].
η j = (σ jI − A)ev j − b and ξ j = (σ jI − A)Tew j − cT ,

Rb = [η1, η2, . . . , ηr] and Rc = [ξ1, ξ2, . . . , ξr],

Deﬁne residuals

residual matrices

and the rank 2r matrix

RT
c .

r . Then, the

r eVr(cid:17)−1

perturbation F satisﬁes

Assume eWT

r +eVr(cid:16)eWT
eWT

r eVr(cid:17)−1
F = Rb(cid:16)eWT
r eVr is nonsingular. Deﬁne an oblique projectoreφr = eVr(cid:16)eWT
kFk2 ≤ kFkF ≤ √r(cid:13)(cid:13)(cid:13)eφr(cid:13)(cid:13)(cid:13) · max
kξik
kηik
ςmin(eVrDv)−1 + max
kewik
kevik
eDv = diag  1
and eDw = diag  1
kevrk!
kev1k
kew1k

r eVr(cid:17)−1
eWT
ςmin(eWrDw)−1! ,
kewrk! .

where ςmin denotes the smallest singular value, and eDv and eDw are diagonal scaling matrices

deﬁned as follows:

Remark 1. From Theorem 5, we see that Frobenius norm of the perturbation F is proportional to
the norms of linear solve residuals. That is, as we decrease the residuals (which are the stopping
criteria for inexact linear solves), perturbation F is reduced, so we get a more accurate reduced
model from Theorem 4.

1

kev2k

1

kew2k

,

, . . . ,

i

1

,

, . . . ,

1

i

12

3.2. Backward Stability of BIRKA

In Algorithm 2, steps 3b. and 3c. require vec(V) and vec(W), respectively, which are com-

puted as follows:

vec (V) =(cid:18)−Λ ⊗ In − Ir ⊗ A −
vec (W) =(cid:18)−Λ ⊗ In − Ir ⊗ AT −

ˇˇNT ⊗ N(cid:19)−1(cid:18)ˇˇbT ⊗ b(cid:19)
ˇˇN ⊗ NT(cid:19)−1(cid:16)ˇˇcT ⊗ cT(cid:17) .

and

As discussed earlier, we compute vec(V) and vec(W) by iterative solves (or by inexact BIRKA),

and denote them as vec(eV) and vec(eW), respectively. Let residuals associated with iterative solves

for computing vec(V) and vec(W) be vec(Rb) and vec(Rc), respectively. Then, above equations
leads to

(cid:18)−Λ ⊗ In − Ir ⊗ A −
(cid:18)−Λ ⊗ In − Ir ⊗ AT −

ˇˇNT ⊗ N(cid:19) vec(cid:16)eV(cid:17) =(cid:18)ˇˇbT ⊗ b(cid:19) + vec (Rb)
ˇˇN ⊗ NT(cid:19) vec(cid:16)eW(cid:17) =(cid:16)ˇˇcT ⊗ cT(cid:17) + vec (Rc) .

and

(20)

(21)

Let eVr = orth(eV) and eWr = orth(eW). Petrov-Galerkin projection connects the reduced model

parameters (obtained by inexact BIRKA) to the original full model parameters as

eAr =(cid:16)eWT
ebr =(cid:16)eWT

r eVr(cid:17)−1
r eVr(cid:17)−1

r AeVr, eNr =(cid:16)eWT
eWT
eWT
r b, and ecr = ceVr.

r eVr(cid:17)−1

eWT
r NeVr,

(22)

By backward stability deﬁnition, next we ﬁnd a perturbed full model whose exact interpola-
tion will give the reduced model as obtained by inexact interpolation of the original full model.
Suppose that perturbation F is in the matrix A, then we have

or

and

(cid:18)−Λ ⊗ In − Ir ⊗ A −
(cid:18)−Λ ⊗ In − Ir ⊗ AT −

(cid:18)−Λ ⊗ In − Ir ⊗ (A + F) −
(cid:18)−Λ ⊗ In − Ir ⊗ (A + F)T −

ˇˇNT ⊗ N(cid:19) vec(cid:16)eV(cid:17) =(cid:18)ˇˇbT ⊗ b(cid:19)
ˇˇN ⊗ NT(cid:19) vec(cid:16)eW(cid:17) =(cid:16)ˇˇcT ⊗ cT(cid:17) .
ˇˇNT ⊗ N(cid:19) vec(cid:16)eV(cid:17) =(cid:18)ˇˇbT ⊗ b(cid:19) + (Ir ⊗ F) vec(cid:16)eV(cid:17)
ˇˇN ⊗ NT(cid:19) vec(cid:16)eW(cid:17) =(cid:16)ˇˇcT ⊗ cT(cid:17) +(cid:16)Ir ⊗ FT(cid:17) vec(cid:16)eW(cid:17) .
r eVr(cid:17)−1
r (A + F)eVr, bNr =(cid:16)eWT
eWT
r eVr(cid:17)−1 eWT
r b, and bcr = ceVr.
vec (Rb) = (Ir ⊗ F) vec(cid:16)eV(cid:17)

and vec (Rc) =(cid:16)Ir ⊗ FT(cid:17) vec(cid:16)eW(cid:17)

bAr =(cid:16)eWT
bbr =(cid:16)eWT

eWT
r NeVr,

r eVr(cid:17)−1

and

or

and

Rb = FeVr

r F.

RT

c = eWT

13

(23)

(24)

(25)

As earlier, using Petrov-Galerkin projection to connect reduced model parameters (obtained by
exact BIRKA) with perturbed full model parameters we get

Comparing (20) with (23), and (21) with (24) we get

Combining the above two expressions gives us the perturbation

r +eVr(cid:16)eWT
This is similar to the perturbation derived in the Subsection 3.1. If we multiply eWT
eVr from right in (26), and use Petrov-Galerkin conditions

F = Rb(cid:16)eWT

r eVr(cid:17)−1 eWT

r eVr(cid:17)−1

RT
c .

and

(26)

r from left and

we get

eWr ⊥ Rb

eVr ⊥ Rc,

eWT
r FeVr = 0.

(27)

r FeVr = eAr,

Recall, the original full model is represented as ζ : A, N, b, c. If the perturbed full model is

Using (27) in (25), and comparing with (22) we get

r eVr(cid:17)−1 eWT
and bcr =ecr.

bAr =(cid:16)eWT

r eVr(cid:17)−1 eWT

r (A + F)eVr = eAr +(cid:16)eWT
bNr = eNr, bbr =ebr,
represented aseζ : A + F, N, b, c. Then, we have shown that
eg (ζ) = g(cid:16)eζ(cid:17) =eζr,
whereeζr

backward stability .

: eAr, eNr, ebr, ecr or bAr, bNr, bbr, bcr. Thus we have satisﬁed the ﬁrst condition of

Next we show that the second condition given in (8) is also held. According to that con-
dition, diﬀerence between the original full model and perturbed full model should be order of
perturbation, i.e.,

kζ −eζkH2 or H∞
kζkH2 or H∞

= (cid:13)(kFk).

We prove the above condition in absolute sense (the same result will hold in relative sense also),
i.e.,

We have already seen earlier that the transfer function of a bilinear system can be written as a
series of subsystem transfer functions [2], i.e.,

kζ −eζkH2 or H∞

= (cid:13)(kFk)

ζ = {H1 (s1) , H2 (s1, s2) , H3 (s1, s2, s3) , . . . , Hk (s1, s2, . . . , sk) , . . . , ∞} ,

where

and

where

Hk (s1, s2, . . . sk) = c (skIn − A)−1 N (sk−1In − A)−1 N . . . (s1In − A)−1 b,
eζ =neH1 (s1) , eH2 (s1, s2) , eH3 (s1, s2, s3) , . . . , eHk (s1, s2, . . . , sk) , . . . , ∞o ,
eHk (s1, s2, . . . sk) = c (skIn − A − F)−1 N (sk−1In − A − F)−1 N . . . (s1In − A − F)−1 b.

14

Hence, for the backward stability we prove

(cid:13)(cid:13)(cid:13)H1 (s1) − eH1 (s1)(cid:13)(cid:13)(cid:13)H2 ∝ O (kFk)
(cid:13)(cid:13)(cid:13)H2 (s1, s2) − eH2 (s1, s2)(cid:13)(cid:13)(cid:13)H2 ∝ O (kFk)
(cid:13)(cid:13)(cid:13)Hk (s1, s2, . . . , sk) − eHk (s1, s2, . . . , sk)(cid:13)(cid:13)(cid:13)H2 ∝ O (kFk) ,

...

...

and

and

with k → ∞.
Comparing the kth original subsystem transfer function with kth perturbed subsystem transfer
function (and not with the other 1, 2, 3, . . ., (k-1), (k+1), . . ., ∞ perturbed subsystem transfer
functions) is correct. This is because the output of a bilinear dynamical system is a sum of
individual subsystem outputs, and kth subsystem output is proportional to kth subsystem transfer
function only (and not other 1, 2, 3, . . ., (k-1), (k+1), . . ., ∞ transfer functions) [3].
the proofs below exist.
Base Case

We use induction to prove this. Consider matrix N as invertible, and also all inverses used in

(i) First subsystem

This is the linear system case, which we have already proved in Subsection 3.1.

(cid:13)(cid:13)(cid:13)H1 (s1) − eH1 (s1)(cid:13)(cid:13)(cid:13)H2 ∝ O (kFk).

(ii) Second subsystem

In the second subsystem, the H2−norm of the diﬀerence between the original full subsys-
tem and perturbed full subsystem is

=

=

1

1
2π

(cid:13)(cid:13)(cid:13)H2 (s1, s2) − eH2 (s1, s2)(cid:13)(cid:13)(cid:13)H2

1

1
2π

2πZ ∞
−∞Z ∞
−∞(cid:13)(cid:13)(cid:13)c (iω2In − A)−1 N (iω1In − A)−1 b −
c (iω2In − (A + F))−1 N (iω1In − (A + F))−1 b(cid:13)(cid:13)(cid:13)
2πZ ∞
−∞Z ∞
−∞(cid:13)(cid:13)(cid:13)c (iω2In − A)−1
(cid:18)N −(cid:16)In − F (iω2In − A)−1(cid:17)−1
(iω1In − A)−1 b(cid:13)(cid:13)(cid:13)
(cid:13)(cid:13)(cid:13)H2 (s1, s2) − eH2 (s1, s2)(cid:13)(cid:13)(cid:13)H2
2πZ ∞
−∞Z ∞
−∞(cid:13)(cid:13)(cid:13)c (iω2In − A)−1(cid:13)(cid:13)(cid:13)F
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:18)N −(cid:16)In − F (iω2In − A)−1(cid:17)−1
(cid:13)(cid:13)(cid:13)(iω1In − A)−1 b(cid:13)(cid:13)(cid:13)F dω1dω2.

2
F dω1dω2.

15

1

1
2π

≤

Using kXYZkF ≤ kXkF kYZkF and kYZkF ≤ kYk2 kZkF we get

2
F dω1dω2

N(cid:16)In − (iω1In − A)−1 F(cid:17)−1(cid:19)

N(cid:16)In − (iω1In − A)−1 F(cid:17)−1(cid:19)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2

(28)

Now from mean value theorem of integration we have

Z ∞
−∞Z ∞

−∞

f (iω2)g (iω1, iω2) h (iω1) dω1dω2

−∞

=Z ∞
≤Z ∞
≤ Z ∞
−∞
≤ max
c,d∈R
Using this property in (28) we get

−∞

−∞

f (iω2) Z ∞
f (iω2) max

c∈R
f (iω2) max
c∈R

(g(ic, id))Z ∞

−∞

h (iω1) dω1! dω2

g (iω1, iω2) h (iω1) dω1! dω2
(g(ic, iω2))Z ∞
(g(ic, iω2)) dω2!Z ∞
f (iω2) dω2Z ∞

−∞
h (iω1) dω1.

h (iω1) dω1

−∞

−∞

2

2

N(cid:16)In − (iω1In − A)−1 F(cid:17)−1(cid:13)(cid:13)(cid:13)(cid:13)
2πZ ∞
−∞(cid:13)(cid:13)(cid:13)(iω1In − A)−1 b(cid:13)(cid:13)(cid:13)

1

2

2
F dω1

N(cid:16)In − (s1In − A)−1 F(cid:17)−1(cid:13)(cid:13)(cid:13)(cid:13)
H2(cid:13)(cid:13)(cid:13)(s1In − A)−1 b(cid:13)(cid:13)(cid:13)

2
H2

2

H∞
.

1

2
F dω2

≤ max

(cid:13)(cid:13)(cid:13)H2 (s1, s2) − eH2 (s1, s2)(cid:13)(cid:13)(cid:13)H2
ω1,ω2∈R(cid:13)(cid:13)(cid:13)(cid:13)N −(cid:16)In − F (iω2In − A)−1(cid:17)−1
2πZ ∞
−∞(cid:13)(cid:13)(cid:13)c (iω2In − A)−1(cid:13)(cid:13)(cid:13)
≤(cid:13)(cid:13)(cid:13)(cid:13)N −(cid:16)In − F (s2In − A)−1(cid:17)−1
(cid:13)(cid:13)(cid:13)c (s2In − A)−1(cid:13)(cid:13)(cid:13)
(cid:13)(cid:13)(cid:13)H2 (s1, s2) − eH2 (s1, s2)(cid:13)(cid:13)(cid:13)H2
≤(cid:13)(cid:13)(cid:13)c (s2In − A)−1(cid:13)(cid:13)(cid:13)
H2(cid:13)(cid:13)(cid:13)(s1In − A)−1 b(cid:13)(cid:13)(cid:13)
M(s1, s2) = N −(cid:16)In − F (s2In − A)−1(cid:17)−1

2

or

where

2

H2 kM(s1, s2)k2
H∞

,

(29)

N(cid:16)In − (s1In − A)−1 F(cid:17)−1

.

Here, ﬁrst and second terms in the right hand side are independent of perturbation F.
If T1 := (s1In − A)−1, T2 := (s2In − A)−1, and
Let’s look at the term kM(s1, s2)k2
H∞
M2 := M(s1, s2), then

.

M2 = N − (In − FT2)−1 N (In − T1F)−1

N − M2 = (In − FT2)−1 N (In − T1F)−1

N = (In − FT2) (N − M2) (In − T1F)
M2 = FT2 (M2 − N) + (M2 − N) T1F + FT2 (N − M2) T1F.

Taking H∞−norm of both sides of the above equation we get

kM2kH∞

= kFT2 (M2 − N) + (M2 − N) T1F + FT2 (N − M2) T1FkH∞

16

or
or
or

.

Matrices F and N are free from s1 and s2, so in place of H∞, 2−norm is taken out for them
in the simpliﬁcation below.

kM2kH∞ ≤ kFkkT2kH∞ kM2 − NkH∞

+ kT1kH∞
+ kNk(cid:17)
+ kT1kH∞

+ kFk2 kT1kH∞ kT2kH∞ kN − M2kH∞
≤ kFkkM2 − NkH∞
(cid:16)kT2kH∞
≤ kFk(cid:16)kM2kH∞
(cid:16)kT2kH∞
kFkkNk(cid:16)kT2kH∞
1 − kFk(cid:16)kT2kH∞

+ kM2 − NkH∞ kT1kH∞ kFk
+ kFkkT2kH∞ kT1kH∞(cid:17)
+ kFkkT2kH∞ kT1kH∞(cid:17) .
+ kFkkT2kH∞ kT1kH∞(cid:17)
+ kFkkT2kH∞ kT1kH∞(cid:17) .

+ kT1kH∞
+ kT1kH∞

kM2kH∞ ≤

or

From the above equation it is clear that

kM2kH∞

= (cid:13) (kFk) .

(30)

So from (29) and (30)

(iii) Third subsystem

(cid:13)(cid:13)(cid:13)H2 (s1, s2) − eH2 (s1, s2)(cid:13)(cid:13)(cid:13)H2 ∝ (cid:13) (kFk) .

In the third subsystem, H2−norm of the diﬀerence between the original full subsystem and
perturbed full subsystem is

=

1

2
H2

1
2π

1
2π

2πZ ∞

−∞Z ∞

−∞Z ∞

(cid:13)(cid:13)(cid:13)H3 (s1, s2, s3) − eH3 (s1, s2, s3)(cid:13)(cid:13)(cid:13)
(cid:13)(cid:13)(cid:13)c (iω3In − A)−1 N (iω2In − A)−1 N (iω1In − A)−1 b −
c (iω3In − (A + F))−1 N (iω2In − (A + F))−1 N (iω1In − (A + F))−1 b(cid:13)(cid:13)(cid:13)

dω1dω2dω3.

−∞

2
F

1
2π

1
2π

1

2πZ ∞

−∞Z ∞

−∞Z ∞

||c (iω3In − A)−1

−∞

(cid:18)N (iω2In − A)−1 N −(cid:16)In − F (iω3In − A)−1(cid:17)−1
(iω2In − A − F)−1 N(cid:16)In − (iω1In − A)−1 F(cid:17)−1(cid:19) (iω1In − A)−1 b||2

N

F

dω1dω2dω3.

||c (iω3In − A)−1 N (iω2In − A)−1

1
2π

1
2π

1

2πZ ∞

−∞Z ∞

−∞Z ∞

−∞

(cid:18)N − (iω2In − A) N−1(cid:16)In − F (iω3In − A)−1(cid:17)−1
N(cid:16)In − (iω1In − A)−1 F(cid:17)−1(cid:19) (iω1In − A)−1 b||2

F

dω1dω2dω3.

N (iω2In − A − F)−1

17

=

=

By applying the mean value theorem of integration in the above equation we get

2

2
H2

2
H2

max

(cid:13)(cid:13)(cid:13)H3 (s1, s2, s3) − eH3 (s1, s2, s3)(cid:13)(cid:13)(cid:13)
H2(cid:13)(cid:13)(cid:13)N (s2In − A)−1(cid:13)(cid:13)(cid:13)
≤(cid:13)(cid:13)(cid:13)c (s3In − A)−1(cid:13)(cid:13)(cid:13)
ω1, ω2, ω3∈R(cid:13)(cid:13)(cid:13)(cid:13)N − (iω2In − A) N−1(cid:16)In − F (iω3In − A)−1(cid:17)−1
N (iω2In − A − F)−1 N(cid:16)In − (iω1In − A)−1 F(cid:17)−1(cid:13)(cid:13)(cid:13)(cid:13)
(cid:13)(cid:13)(cid:13)(s1In − A)−1 b(cid:13)(cid:13)(cid:13)

2
H2

2

2

.

or

(cid:13)(cid:13)(cid:13)H3 (s1, s2, s3) − eH3 (s1, s2, s3)(cid:13)(cid:13)(cid:13)
H2(cid:13)(cid:13)(cid:13)N (s2In − A)−1(cid:13)(cid:13)(cid:13)
≤(cid:13)(cid:13)(cid:13)c (s3In − A)−1(cid:13)(cid:13)(cid:13)

2
H2

2

2

H2(cid:13)(cid:13)(cid:13)(s1In − A)−1 b(cid:13)(cid:13)(cid:13)

2

where

H2 kM(s1, s2, s3)k2
H∞

,

M(s1, s2, s3) = N − (s2In − A)N−1(cid:16)In − F(s3In − A)−1(cid:17)−1
(s2In − A − F)−1 N(cid:16)In − (s1In − A)−1F(cid:17)−1

N

.

Here, ﬁrst, second, and third terms in the right hand side are independent of perturbation
. If T1 := (s1In − A)−1, T2 := (s2In − A)−1,
F. Let’s look at the term kM(s1, s2, s3)k2
H∞
T3 := (s3In − A)−1, and M3 := M(s1, s2, s3), then
2 N−1 (In − FT3)−1 N(cid:16)T −1

N (In − T1F)−1 .

2 − F(cid:17)−1

M3 = N − T −1

Following the same steps as in case of second subsystem. We can easily show that
kM3kH∞ ∝ (cid:13) (kFk). Hence,

(cid:13)(cid:13)(cid:13)H3 (s1, s2, s3) − eH3 (s1, s2, s3)(cid:13)(cid:13)(cid:13)H2 ∝ (cid:13) (kFk) .

Induction Hypothesis
Following the ﬁrst, second, and third subsystem derivations, for the kth subsystem the diﬀerence
between original full subsystem and perturbed full subsystem is given below.

where

2

2

2

2

2
H2

2
H2

H2(cid:13)(cid:13)(cid:13)N (sk−2In − A)−1(cid:13)(cid:13)(cid:13)
H2 kM(s1, s2, . . . , sk)k2
H∞

(cid:13)(cid:13)(cid:13)Hk (s1, s2, . . . , sk) − eHk (s1, s2, . . . , sk)(cid:13)(cid:13)(cid:13)
H2(cid:13)(cid:13)(cid:13)N (sk−1In − A)−1(cid:13)(cid:13)(cid:13)
≤(cid:13)(cid:13)(cid:13)c (skIn − A)−1(cid:13)(cid:13)(cid:13)
H2(cid:13)(cid:13)(cid:13)(s1In − A)−1 b(cid:13)(cid:13)(cid:13)
. . .(cid:13)(cid:13)(cid:13)N (s2In − A)−1(cid:13)(cid:13)(cid:13)
M(s1, s2, . . . , sk) = N − (s2In − A)N−1(s3In − A)N−1 . . . (sk−1In − A)N−1
(cid:16)In − F(skIn − A)−1(cid:17)−1
N (sk−1In − A − F)−1 N (sk−2In − A − F)−1 . . . N (s2In − A − F)−1
N(cid:16)In − (s1In − A)−1F(cid:17)−1

.

,

18

(31)

As earlier if T1 := (s1In − A)−1, T2 := (s2In − A)−1, . . . . . ., Tk := (skIn − A)−1, and Mk :=
M(s1, s2, . . . , sk), then

We assume kMkkH∞ ∝ (cid:13) (kFk), which from (31) would imply

Induction Step
As in earlier cases, we can easily show

2 N−1T −1

3 N−1 . . . T −1

. . . N(cid:16)T −1

k−2 − F(cid:17)−1

k−1 − F(cid:17)−1

k−1N−1 (In − FTk)−1 N(cid:16)T −1
Mk = N − T −1
2 − F(cid:17)−1
N(cid:16)T −1
N (In − T1F)−1 .
(cid:13)(cid:13)(cid:13)Hk (s1, s2, . . . , sk) − eHk (s1, s2, . . . , sk)(cid:13)(cid:13)(cid:13)H2 ∝ (cid:13) (kFk).
(cid:13)(cid:13)(cid:13)Hk+1 (s1, s2, . . . , sk+1) − eHk+1 (s1, s2, . . . , sk+1)(cid:13)(cid:13)(cid:13)
≤(cid:13)(cid:13)(cid:13)c (sk+1In − A)−1(cid:13)(cid:13)(cid:13)
. . .(cid:13)(cid:13)(cid:13)N (s2In − A)−1(cid:13)(cid:13)(cid:13)
M(s1, s2, . . . , sk+1) = N − (s2In − A)N−1(s3In − A)N−1 . . . (skIn − A)N−1
(cid:16)In − F(sk+1In − A)−1(cid:17)−1
N (skIn − A − F)−1 N (sk−1In − A − F)−1 . . . N (s2In − A − F)−1
N(cid:16)In − (s1In − A)−1F(cid:17)−1

H2(cid:13)(cid:13)(cid:13)N (skIn − A)−1(cid:13)(cid:13)(cid:13)
H2(cid:13)(cid:13)(cid:13)(s1In − A)−1 b(cid:13)(cid:13)(cid:13)

H2(cid:13)(cid:13)(cid:13)N (sk−1In − A)−1(cid:13)(cid:13)(cid:13)
H2 kM(s1, . . . , sk+1)k2
H∞

2
H2

2
H2

2

2

2

2

.

,

(32)

where

or

As earlier if T1 := (s1In − A)−1, T2 := (s2In − A)−1, . . ., Tk+1 := (sk+1In − A)−1, and Mk+1 :=
M(s1, s2, . . . , sk+1), then

Mk+1 = N − T −1
N(cid:16)T −1

k−1 − F(cid:17)−1

2 N−1T −1

3 N−1 . . . T −1

k N−1 (In − FTk+1)−1 N(cid:16)T −1
2 − F(cid:17)−1

N (In − T1F)−1 .

k − F(cid:17)−1

. . . N(cid:16)T −1

By using induction, we show that kMk+1kH∞ ∝ (cid:13) (kFk). This with (32) would imply
Mk+1 in terms of Mk we get

(cid:13)(cid:13)(cid:13)Hk+1 (s1, s2, . . . , sk+1) − eHk+1 (s1, s2, . . . , sk+1)(cid:13)(cid:13)(cid:13)H2 ∝ (cid:13) (kFk) and we would be done. Writing

Mk+1 = N + T −1

2 N−1T −1

(cid:16)T −1
k − F(cid:17)−1

k N−1 (In − FTk+1)−1 N

3 N−1 . . . T −1
(In − FTk) NTk−1NTk−2 . . . NT2 (Mk − N) .
k − F(cid:17)−1

(In − FTk+1) NTkNTk−1 . . . NT2 (Mk+1 − N) = N(cid:16)T −1
(In − FTk) NTk−1NTk−2 . . . NT2 (Mk − N) .

(33)

Let NTkNTk−1 . . . NT2 = σk, and NTk−1NTk−2 . . . NT2 = σk−1, i.e., σk = NTkσk−1 and σk−1 =
T −1
k N−1σk. Then, (33) can be rewritten as

(In − FTk+1) σk (Mk+1 − N) = N(cid:16)T −1

k − F(cid:17)−1

19

(In − FTk) σk−1 (Mk − N)

or

That is,

(cid:16)T −1
k − F(cid:17) N−1 (In − FTk+1) σk (Mk+1 − N) = (In − FTk) σk−1 (Mk − N)
k N−1σk (Mk − N)
k − F(cid:17) N−1σk (Mk − N) .

= (In − FTk) T −1
= (cid:16)T −1

σk Mk+1 − FTk+1σk Mk+1 + FTk+1σkN = σk Mk
Mk+1 = Mk + σ−1

or
k FTk+1σkN.

k FTk+1σk Mk+1 − σ−1
Taking the H∞−norm on both sides of the above equation we get

kMk+1kH∞
kMk+1kH∞ ≤ kMkkH∞

or

=(cid:13)(cid:13)(cid:13)Mk + σ−1
k FTk+1σkN(cid:13)(cid:13)(cid:13)H∞
k FTk+1σk Mk+1 − σ−1
k (cid:13)(cid:13)(cid:13)H∞ kFkkTk+1kH∞ kσkkH∞ kMk+1kH∞
+(cid:13)(cid:13)(cid:13)σ−1
k (cid:13)(cid:13)(cid:13)H∞ kFkkTk+1kH∞ kσkkH∞ kNk
+(cid:13)(cid:13)(cid:13)σ−1
+(cid:13)(cid:13)(cid:13)σ−1
k (cid:13)(cid:13)(cid:13)H∞ kFkkTk+1kH∞ kσkkH∞ kNk
k (cid:13)(cid:13)(cid:13)H∞ kFkkTk+1kH∞ kσkkH∞
1 −(cid:13)(cid:13)(cid:13)σ−1

kMkkH∞

or

kMk+1kH∞ ≤

< 1. Using induction hypothesis we know kMkkH∞ ∝ (cid:13) (kFk)

if(cid:13)(cid:13)(cid:13)σ−1

k (cid:13)(cid:13)(cid:13)H∞ kFkkTk+1kH∞ kσkkH∞

and hence,

Thus,

kMk+1kH∞ ∝ (cid:13) (kFk) .

(cid:13)(cid:13)(cid:13)ζ −eζ(cid:13)(cid:13)(cid:13)H2

= (cid:13)(kFk).

Since BIRKA satisﬁes both conditions given in (7) and (8), it is backward stable.

Remark 2. As in linear case, Theorem 5 holds for bilinear case as well. That is, as we solve
linear systems more accurately, kFk reduces. Hence, from Theorem 4, by solving linear system
more accurately we get a more accurate reduced model.

4. Experimental Results

Since BIRKA is backward stable, we show that result of Theorem 4 holds experimentally.

That is,

kg (ζ) −eg (ζ)kH2
kg (ζ)kH2

= (cid:13) (k(ζ) · kFk) ,

where ζ is the original full model, g denotes exact BIRKA,eg denotes inexact BIRKA, k(ζ) is the
condition number of ζ, and F is the perturbation in ζ (leading toeζ; recall (7)-(8) )
We deﬁne, g (ζ) = ζr, and as in Subsection 3.2,eg (ζ) =eζr. Hence, we need to show

(cid:13)(cid:13)(cid:13)ζr −eζr(cid:13)(cid:13)(cid:13)H2

kζrkH2

= (cid:13) (k(ζ) · kFk) .
20

From Theorem 5, we know that solving linear systems for computing Vr and Wr more accurately
reduces kFk. Hence, we need to show that for a well conditioned original full model, as we

solve such linear systems more accurately, we get a more accurate reduced model or (cid:13)(cid:13)(cid:13)ζr −eζr(cid:13)(cid:13)(cid:13)H2
reduces or(cid:13)(cid:13)(cid:13)ζr −eζr(cid:13)(cid:13)(cid:13)H2

We do experiments on a “ﬂow model” [20], which consists of a one dimensional viscid

reduces since kζrkH2 is independent of kFk.

kζrkH2

Burgers equation. That is,

∂w
∂t

+ w

∂w
∂x

=

∂

∂x v

∂w

∂x! ,

for (x, t) ∈ (0, L) × (0, T ) ,

w (0, t) = u (t) ,

for t ∈ (0, T ) ,

where w(x, t) is velocity at a particular point x and a time t; and v(x, t) is the viscosity coeﬃcient
that we take as a constant (v). After performing spatial discretization of the viscid Burgers
equation with equidistant step size h =
, where N is the number of interior points in the
interval (0, L), we obtain the bilinear control system [20]. We brieﬂy show these steps below.

N + 1

L

d
dt

=

w1
w2
·
·wi
·
·wN







or

+

−w1w2
2h
(w3 − w1) +

−w2
2h

−wi
2h

(wi+1 − wi−1) +

v
h2 (w2 − 2w1)
v
h2 (w3 − 2w2 + w1)
·
·
v
h2 (wi+1 − 2wi + wi−1)
·
·
v
h2 (−2wN + wN−1)

w1
2h



+



+
0
·
·0
·
·0

v
h2

u



−wNwN−1

2h

+

dw
dt

= f (w) + g(w)u,

where ω =hω1, ω2, . . . , ωNiT

below.

, and f (w) and g(w) can be written in Kronecker product form as

f (w) = A1w +

1
2

A2(w ⊗ w),

g(w) = B0 + B1w,

where B0 ∈ RN×1, A1, B1 ∈ RN×N are Jacobians of f (w) and g(w), respectively, and A2 ∈ RN×N2
is the second derivative of f (w). Let,

˙x =

dx
dt

,

and

˙ω =

dω
dt

.

21

Finally, we get the bilinear system of order N + N2 as

y =

where,

˙x ="A1

1
2 A2

0
1

A1 ⊗ I + I ⊗ A1# x +"
N "1 · · · 1
N2 times # x,
0 · · ·· · · 0
|{z}N times
|    {z    }
x =" w
w ⊗ w# ,

and

B1

B0 ⊗ I + I ⊗ B0

0

0# xu +"B0
0# u,

˙x ="

˙w

˙w ⊗ w + w ⊗ ˙w#.

We refer the reader to [20] for exact structure of A1, A2, B0 and B1. For our experiments, we
take N = 10, L = 1, and v = 0.1 that gives us a well conditioned SISO bilinear system of order
110 × 110. We perform model reduction on this by using BIRKA to reduce it up to an order of
6 × 6. Stopping tolerance for BIRKA is taken as 10−6.
The experiments are done on Matlab 2015a, on machine Intel Xeon(R) CPU E5-1620 V3 @
3.50 GHz., frequency 1200MHz., 8 CPU, 64 GB primary memory. For solving linear systems
while computing Vr and Wr by a direct method (exact BIRKA), we use a backslash in Matlab.
This uses Gaussian elimination as the underlying algorithm. The most popular iterative methods
are the Krylov subspace methods [21].
In BIRKA, we solve the linear system of equations
coming from a Petrov-Galerkin framework, and hence, Bi-Conjugate Gradient (BiCG) algorithm
is best suited [13]. Thus, we use BiCG algorithm for solving linear systems while computing Vr
and Wr by an iterative method (inexact BIRKA). Here we use two diﬀerent stopping tolerances
(i.e., BiCG-ToL) as 10−8 and 10−2. Starting with the same initial guess for the reduced system
(random), inexact BIRKA converges in 20 steps for BiCG-ToL = 10−2 and in 21 steps for BiCG-
ToL = 10−8. Exact BIRKA is executed on the side for comparison.

on the y-axis and BIRKA iterations on x-axis. Table 1 gives the corresponding data. In Table 1,

and BiCG iteration count at convergence, respectively, when BiCG stopping tolerance is 10−2.

The result for this are given in Figure 1 and Table 1. In Figure 1, we have accuracy(cid:13)(cid:13)(cid:13)ζr −eζr(cid:13)(cid:13)(cid:13)H2
ﬁrst column denotes BIRKA iterations, second and third columns give H2−error(cid:16)i.e., (cid:13)(cid:13)(cid:13)ζr −eζr(cid:13)(cid:13)(cid:13)H2(cid:17)
Fourth and ﬁfth columns give H2−error(cid:16)i.e., (cid:13)(cid:13)(cid:13)ζr −eζr(cid:13)(cid:13)(cid:13)H2(cid:17) and BiCG iteration count at conver-

gence, respectively, when BiCG stopping tolerance is 10−8. The ﬁrst observation from both Fig-
ure 1 and Table 1 is that, the large stopping tolerance does not substantially change the accuracy
of the reduced model (both dotted and solid lines are fairly close).

Secondly, ideally solid line, which corresponds to the smaller BiCG stopping tolerance
should be below the dotted line, which corresponds to larger BiCG stopping tolerance. However
we see a wiggly behaviour. The reason for this is that initially, BIRKA’s convergence behaviour
is erratic [16]. Towards the end of iterations, as BIRKA converges to ideal interpolation points
(inverse images of poles of reduced system), it gets consistent. Hence, from BIRKA iteration 14
onwards until convergence (20 for BiCG-ToL 10−2 and 21 for BiCG-ToL 10−8) the solid line is
always below the dotted line (i.e., reduced system for BiCG-ToL = 10−8 is more accurate then
reduced system for BiCG-ToL = 10−2). This is shown in Figure 2, which is enlarged version of
Figure 1 (also supported by Table 1).

22

BiCG−ToL =10−2

BiCG−ToL =10−8

5

10

15

BIRKA Iterations

20

Figure 1: Evolution of H2 error during exact BIRKA and inexact BIRKA.

BiCG−ToL =10−2

BiCG−ToL =10−8

 

 

0.04

0.035

0.03

0.025

0.02

0.015

0.01

0.005

2

H
2
...
r
eζ
−

r
ζ

...

0

 

0

x 10−9

9

8

7

6

5

4

3

2

1

0

2

H
2
...
r
eζ
−

r
ζ

...

 

0

2

4

6

8

10

12

14

16

18

20

BIRKA Iterations

Figure 2: Enlarging Figure 1 towards the end of BIRKA iterations.

23

BiCG-ToL of 10−2
BiCG
iteration
count

(cid:13)(cid:13)(cid:13)ζr −eζr(cid:13)(cid:13)(cid:13)

2
H2

BIRKA
Iteration

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21

7.6766e-03
1.0213e-02
8.3017e-03
7.6568e-04
1.0280e-04
2.3294e-05
5.7412e-06
1.3816e-06
3.6191e-07
8.4436e-08
2.5736e-08
4.6543e-09
2.6688e-09
5.9563e-10
9.7627e-10
6.6426e-10
8.2477e-10
7.5816e-10
8.0430e-10
7.8775e-10

2
H2

(cid:13)(cid:13)(cid:13)ζr −eζr(cid:13)(cid:13)(cid:13)

BiCG-ToL of 10−8
BiCG
iteration
count
158
85
409
118
92
87
88
90
87
90
90
90
90
90
90
90
90
90
90
90
90

1.0913e-02
4.1496e-02
1.8332e-02
1.3326e-03
1.8424e-04
4.1286e-05
1.0056e-05
2.5639e-06
6.5805e-07
1.7048e-07
4.4193e-08
1.1481e-08
2.9832e-09
7.7569e-10
2.0173e-10
5.2470e-11
1.3651e-11
3.5444e-12
9.2706e-13
2.3662e-13
5.7705e-14

74
36
38
43
39
39
44
39
44
44
44
44
44
44
44
44
44
44
44
44
44

Table 1: Evolution of model reduction error as BiCG-ToL is decreased.

5. Conclusions

BIRKA provides an H2−optimal reduced model. The most expensive part of BIRKA is
ﬁnding solution of large linear systems of equations. Since iterative algorithms are a method
of choice for such systems and they ﬁnd solution only up to a certain tolerance, we show that
BIRKA is backward stable with respect to these inexact linear solves. We support our result with
experiments.

In literature [2], another cheaper variant of BIRKA, called Truncated BIRKA has been pro-
posed (also called TBIRKA). TBIRKA uses a truncated Volterra series (see Subsection 3.2),
and hence, it is cheaper than BIRKA. Our future work involves proving backward stability of
TBIRKA.

Acknowledgement

We would like to thank Prof. Dr. Peter Benner (at Max Planck Institute for Dynamics of
Complex Technical Systems, Magdeburg, Germany) and Dr. Serkan Gugercin (at Department
of Mathematics, Virginia Tech, Blacksburg, VA, USA) for stimulated discussion regarding this
project. We would also like to thank Dr. Tobias Breiten (at Institute for Mathematics and Scien-
tiﬁc Computing, University of Graz, Austria) for help in understanding BIRKA code.

24

References

[1] P. Benner, T. Breiten, Interpolation-based H2-model reduction of bilinear control systems, SIAM Journal on Matrix
[2] G. M. Flagg, Interpolation methods for the model reduction of bilinear systems, Ph.D. thesis, Virginia Polytechnic

Analysis and Applications 33 (3) (2012) 859–885.

Institute and State University, Blacksburg, VA, USA (2012).

[3] J. R. Philips, Projection-based approaches for model reduction of weakly nonlinear, time-varying systems, IEEE

Transactions on Computer-Aided Design of Integrated Circuits and Systems 22 (02) (2003) 171–187.

[4] A. C. Antoulas, Approximation of Large-Scale Dynamical Systems, SIAM Advances in Design and Control,

Philadelphia, PA, USA, 2005.

[5] S. Gugercin, A. C. Antoulas, C. Beattie, H2 model reduction for large-scale linear dynamical systems, SIAM
[6] S. Gugercin, Projection methods for model reduction of large-scale dynamical systems, Ph.D. thesis, ECE Dept.,

Journal on Matrix Analysis and Applications 30 (2) (2008) 609–638.

Rice University, Houston, TX, USA (2002).

[7] Z. Bai, D. Skoogh, A projection method for model reduction of bilinear dynamical systems, Linear Algebra and its

Applications 415 (2-3) (2006) 406–425.

[8] K. Ahuja, Recycling Krylov subspaces and preconditioners, Ph.D. thesis, Virginia Polytechnic Institute and State

University, Blacksburg, VA, USA (2011).

[9] C. A. Beattie, S. Gugercin, Inexact solves in Krylov-based model reduction, Decision and Control, 2006 45th IEEE

Conference on, San Diego, CA (2006) 3405–3411.

[10] A. Bunse-Gerstner, D. Kubali´nska, G. Vossen, D. Wilczek, h2-norm optimal model reduction for large scale discrete

dynamical MIMO systems, Journal of Computational and Applied Mathematics 233 (5) (2010) 1202 – 1216.

[11] G. M. Flagg, S. Gugercin, Multipoint volterra series interpolation and H2 optimal model reduction of bilinear

systems, SIAM Journal on Matrix Analysis and Applications 36 (2) (2015) 549–579.

[12] T. Breiten, Interpolatory methods for model reduction of large-scale dynamical systems, Ph.D. thesis, Otto-von-

Guericke University Magdeburg, Magdeburg, Germany (2013).

[13] K. Ahuja, E. De Sturler, S. Gugercin, E. R. Chang, Recycling BiCG with an application to model reduction, SIAM

Journal on Scientiﬁc Computing 34 (4) (2012) A1925–A1949.

[14] S. Wyatt, Inexact solves in interpolatory model reduction, Master’s thesis, Virginia Polytechnic Institute and State

University, Blacksburg, VA, USA (2009).

[15] S. Wyatt, Issues in interpolatory model reduction: Inexact solves, second-order systems and DAEs, Ph.D. thesis,

Virginia Polytechnic Institute and State University, Blacksburg, VA, USA (2012).

[16] C. Beattie, S. Gugercin, S. Wyatt, Inexact solves in interpolatory model reduction, Linear Algebra and its Applica-

tions 436 (8) (2012) 2916–2943.

[17] E. J. Grimme, Krylov projection methods for model reduction, Ph.D. thesis, University of Illinois at Urbana-

Champaign, Urbana, IL, USA (1997).

[18] W. J. Rugh, Nonlinear System Theory : The Volterra/ Wiener Approach, Johns Hopkins series in information

sciences and systems, Johns Hopkins University Press, Baltimore, 1981.

[19] L. N. Trefethen, D. Bau, Numerical Linear Algebra, SIAM, 1997.
[20] T. Breiten, T. Damm, Krylov subspace methods for model order reduction of bilinear control systems, Systems &

Control Letters 59 (8) (2010) 443–450.

[21] Y. Saad, Iterative Methods for Sparse Linear Systems, 2nd Edition, SIAM, Philadelphia, PA, USA, 2003.

25

