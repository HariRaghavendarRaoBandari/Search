c(cid:13)2016. This manuscript version is made available under the CC-BY-NC-ND 4.0
license http://creativecommons.org/licenses/by-nc-nd/4.0/

Gland Segmentation in Colon Histology Images: The GlaS

Challenge Contest

Korsuk Sirinukunwattana∗1, Josien P. W. Pluim2, Hao Chen3, Xiaojuan Qi3,

Pheng-Ann Heng3, Yun Bo Guo4, Li Yang Wang4, Bogdan J. Matuszewski4, Elia

Bruni5, Urko Sanchez5, Anton B¨ohm6, Olaf Ronneberger6,7, Bassem Ben
Cheikh8, Daniel Racoceanu8, Philipp Kainz9,10, Michael Pfeiﬀer10, Martin

Urschler11,12, David R. J. Snead13, and Nasir M. Rajpoot†‡1

1Department of Computer Science, University of Warwick, Coventry, UK, CV4

7AL

2Department of Biomedical Engineering, Eindhoven University of Technology,

3Department of Computer Science and Engineering, The Chinese University of

Eindhoven, Netherlands

Hong Kong.

4School of Engineering, University of Central Lancashire, Preston, UK

5ExB Research and Development

6Computer Science Department, University of Freiburg, Germany

7BIOSS Centre for Biological Signalling Studies, University of Freiburg,

Germany and Google-DeepMind, London, UK

8Sorbonne Universit´es, UPMC Univ Paris 06, CNRS, INSERM, Biomedical

9Institute of Biophysics, Center for Physiological Medicine, Medical University of

Imaging Laboratory (LIB), Paris, France

10Institute of Neuroinformatics, University of Zurich and ETH Zurich, Zurich,

Graz, Graz, Austria

Switzerland

11Institute for Computer Graphics and Vision, BioTechMed, Graz University of

Technology, Graz, Austria

12Ludwig Boltzmann Institute for Clinical Forensic Imaging, Graz, Austria

13Department of Pathology, University Hospitals Coventry and Warwickshire,

Walsgrave, Coventry, CV2 2DX, UK

6
1
0
2

 
r
a

M
1

 

 
 
]

V
C
.
s
c
[
 
 

1
v
5
7
2
0
0

.

3
0
6
1
:
v
i
X
r
a

∗k.sirinukunwattana@warwick.ac.uk
†n.m.rajpoot@warwick.ac.uk
‡corresponding author

1

Abstract

Colorectal adenocarcinoma originating in intestinal glandular structures is the
most common form of colon cancer.
In clinical practice, the morphology of in-
testinal glands, including architectural appearance and glandular formation, is used
by pathologists to inform prognosis and plan the treatment of individual patients.
However, achieving good inter-observer as well as intra-observer reproducibility of
cancer grading is still a major challenge in modern pathology. An automated ap-
proach which quantiﬁes the morphology of glands is a solution to the problem.

This paper provides an overview to the Gland Segmentation in Colon Histology
Images Challenge Contest (GlaS) held at MICCAI’2015. Details of the challenge,
including organization, dataset and evaluation criteria, are presented, along with
the method descriptions and evaluation results from the top performing methods.

Index terms— Histology Image Analysis, Segmentation, Colon Cancer, Intestinal

Gland, Digital Pathology

1 Introduction

Glands are important histological structures that are present in most organ systems
as the main mechanism for secreting proteins and carbohydrates. A colonic crypt, an
intestinal gland found in the epithelial layer of the colon, is made up of a single sheet of
columnar epithelium, forming a ﬁnger-like tubular structure that extends from the inner
surface of the colon into the underlying connective tissue [Rubin et al., 2008, Humphries
and Wright, 2008]. There are millions of glands in the human colon. Intestinal glands
are responsible for absorption of water and nutrients, secretion of mucus to protect the
epithelium from a hostile chemical and mechanical environment [Gibson et al., 1996],
as well as being a niche for epithelial cells to regenerate [Shanmugathasan and Jothy,
2000, Humphries and Wright, 2008]. The loss of integrity in cell proliferation, through
a mechanism that is not yet clearly understood, results in colorectal adenocarcinoma,
the most common type of colon cancer.

The morphology of intestinal glands, including architectural appearance and gland
formation, is one of the primary features used in clinical practice to inform prognosis and
plan the treatment of individual patients [Compton, 2000, Bosman et al., 2010, Wash-
ington et al., 2009]. The diﬀerentiation grade of the tumor is based on morphological
assessment. Some recent studies indicate that diﬀerentiation is an important prognos-
tic factor, and poorly diﬀerentiated colorectal adenocarcinoma is associated with nodal
metastasis. [Fleming et al., 2012, Loughrey et al., 2014]. Achieving good reproducibil-
ity in grading any type of cancer remains one of the challenges in modern pathology.
Automated image analysis extracts from histology images a set of numerical features
representing the glandular morphology. This approach could increase the eﬀectiveness
of cancer grading. To obtain reliable morphological statistics, accurate segmentation of
glands is often a crucial step.

The Gland Segmentation in Colon Histology Images (GlaS) challenge1 brought to-
gether computer vision and medical image computing researchers to solve the problem of
gland segmentation in digitized images of Hematoxylin and Eosin (H&E) stained tissue
slides. Participants developed gland segmentation algorithms, which were applied to
benign tissue and to colonic carcinomas. A training dataset was provided, together with
ground truth annotations by an expert pathologist. The participants developed and

1http://www.warwick.ac.uk/bialab/GlaScontest

2

optimized their algorithms on this dataset. The results were judged on the performance
of the algorithms on test datasets. Success was measured by how closely the automated
segmentation matched the pathologist’s.

2 Related Work

Recent papers [Wu et al., 2005a,b, Gunduz-Demir et al., 2010, Fu et al., 2014, Sirinukun-
wattana et al., 2015, Cohen et al., 2015] indicate the increasing interest in histology
image analysis applied to intestinal gland segmentation. In this section, we review some
of these methods.

Wu et al. [2005a] presented a region growing method, which ﬁrst thresholds an image,
in order to separate nuclei from other tissue components. Large empty regions, which
potentially correspond to lumen found in the middle of glands, are then used to initialize
the seed points for region growing. The expanding process for each seed is terminated
when a surrounding chain of epithelial nuclei is reached, and subsequently false regions
are removed. As a result of the assumption that a regular gland is always enclosed by an
epithelial boundary that has only small gaps between neighboring epithelial nuclei, this
algorithm is inevitably susceptible to malformed and incomplete glands, and to various
artefacts arising from the tissue preparation process.

In contrast to the above method, which mainly uses pixel-level information, Gunduz-
Demir et al. [2010] represented each tissue component as a disk. Each disk is represented
by a vertex of a graph, with nearby disks joined by an edge between the corresponding
vertices. They proposed an algorithm, using graph connectivity to identify initial seeds
for region growing. To avoid an excessive expansion beyond the glandular region, caused,
for example, by large gaps in the surrounding epithelial boundary, edges between nuclear
objects are used as a barrier to halt region growing. Those regions that do not show
glandular characteristics are eliminated at the last step. Although the methods by Wu
et al. [2005a] and Gunduz-Demir et al. [2010] perform well in segmenting healthy and
benign glands, they are less applicable to cancer cases, where the morphology of glands
can be substantially deformed.

Fu et al. [2014] introduced a segmentation algorithm based on polar coordinates. A
neighborhood of each gland and a center chosen inside the gland were considered. Using
this center to deﬁne polar coordinates, the neighborhood is displayed in (r, θ) coordinates
with the r-axis horizontal and the θ-axis vertical. One obtains a vertical strip, periodic
with period 2π in the vertical direction. As a result, the closed glandular boundary is
transformed into an approximately vertical periodic path, allowing fast inference of the
boundary through a conditional random ﬁeld model. Support vector regression is later
deployed to verify whether the estimated boundary corresponds to the true boundary.
The algorithm performs well in both benign and malignant cases stained by Hematoxylin
and DAB. However, the validation on routine H&E stained images was limited only to
healthy cases.

Sirinukunwattana et al. [2015] recently formulated a segmentation approach based
on Bayesian inference, which allows prior knowledge of the spatial connectivity and the
arrangement of neighboring nuclei on the epithelial boundary to be taken into account.
This approach treats each glandular structure as a polygon made of a random number
of vertices. The idea is based on the observation that a glandular boundary is formed
from closely arranged epithelial nuclei. Connecting edges between these epithelial nuclei
gives a polygon that encapsulates the glandular structure. Inference of the polygon is

3

Table 1: Details of the dataset.

Number of Images (Width x Height in Pixels)

Training Part

Test Part A

Histologic Grade

Benign

37

Malignant

48

 1
 1

1
35

3
44

 1
 1

4
28

2
24

(574 × 433)
(589 × 453)
(775 × 522)
(567 × 430)
(589 × 453)
(775 × 522)

33

27

Test Part B

4 (775 × 522)

16 (775 × 522)

(574 × 433)
(589 × 453)
(775 × 522)
(578 × 433)
(581 × 442)
(775 × 522)

made via Reversible-Jump Markov Chain Monte Carlo. The approach shows favorable
segmentation results across all histologic grades (except for the undiﬀerentiated grade)
of colorectal cancers in H&E stained images. This method is slow but eﬀective.

Most of the works for intestinal gland segmentation have used diﬀerent datasets
and/or criteria to assess their algorithms, making it diﬃcult to objectively compare their
performance. This challenge was a ﬁrst attempt to address the issues of reproducibility
and comparability. It was also aimed at speeding up even further the development of
algorithms for gland segmentation. Note that none of above methods participated in
this competition.

3 Materials

The dataset used in this challenge consists of 165 images derived from 16 H&E stained
histological sections of stage T3 or T4 colorectal adenocarcinoma. Each section belongs
to a diﬀerent patient, and sections were processed in the laboratory on diﬀerent occa-
sions. Thus, the dataset exhibits high inter-subject variability in both stain distribution
and tissue architecture. The digitization of these histological sections into whole-slide
images (WSIs) was accomplished using a Zeiss MIRAX MIDI Slide Scanner with a pixel
resolution of 0.465µm. The WSIs were subsequently rescaled to a pixel resolution of
0.620µm (equivalent to 20× objective magniﬁcation).

A total of 52 visual ﬁelds from both malignant and benign areas across the entire set
of the WSIs were selected in order to cover as wide a variety of tissue architectures as
possible. An expert pathologist (DRJS) then graded each visual ﬁeld as either ‘benign’
or ‘malignant’, according to the overall glandular architecture. The pathologist also
delineated the boundary of each individual glandular object on that visual ﬁeld. We
used this manual annotation as ground truth for automatic segmentation. Note that
diﬀerent glandular objects in an image may be part of the same gland. This is because
a gland is a 3-dimensional structure that can appear as separated objects on a single
tissue section. The visual ﬁelds were further separated into smaller, non-overlapping
images, whose histologic grades (i.e. benign or malignant) were assigned the same value
as the larger visual ﬁeld. Representative example images of the two grades can be seen
in Figure 1. This dataset was also previously used in the gland segmentation study by
Sirinukunwattana et al. [2015].

In the challenge, the dataset was separated into Training Part, Test Part A, and
Test Part B. A breakdown of the details of the dataset is shown in Table 1. The ground
truth as well as the histologic grade which reﬂects morphology of glandular structures
were provided for every image in the Training Part at the time of release. We used Test

4

(a)

(b)

Figure 1: Example images of diﬀerent histologic grades in the dataset: (a) benign and
(b) malignant.

5

50µm50µm50µm50µmPart A and Test Part B as oﬀ-site and on-site test datasets respectively. Furthermore,
to ensure blindness of evaluation, the ground truth and histologic grade of each image
in the test parts were not released to the participants.

4 Challenge Organization

The GlaS challenge contest was oﬃcially launched by the co-organizers (KS, JPWP,
DRJS, NMR) on April 21st, 2015, and was widely publicized through several channels.
At the same point, a challenge website2 was set up to disseminate challenge-related
information and to serve as a site for registration, submission of results, and commu-
nication between the organizers and contestants. The challenge involved 4 stages, as
detailed below:

Stage 1: Registration and Release of the Training Data The registration was
open for a period of about two months (April 21st to June 30th, 2015).
Interested
individuals or groups of up to 3 people that were aﬃliated with an academic institute
or an industrial organization could register and download the training data (Training
Part, see Section 3 for details) to start developing their gland segmentation algorithms.
From this point forward, we will refer to a separate individual or a group of registrants
as a ‘team’.

Stage 2: Submission of a Short Paper
In order to gain access to the ﬁrst part of the
test data, each registered team was required to submit a 2-page document containing
a general description of their segmentation algorithms and some preliminary results
obtained from running each algorithm on the training data. Each team could submit
up to 3 diﬀerent methods. The intention of this requirement was for the organizers to
identify teams who were serious about participating in the challenge. The organizers
based their reviews on two criteria: clarity of the method description and soundness of
the validation strategy. Segmentation performance was not considered in this review.
The submission of this document was due by July 17th, 2015.

Stage 3: Release of the Test Data Part A and Submission of Segmentation
Results The ﬁrst part of the test data (Test Part A, see Section 3 for details) was
released on August 14th, 2015 to those teams selected from the previous stage which
also agreed to participate in the GlaS contest. The teams were given a month to further
adjust and optimize their segmentation algorithms, and carry out segmentation on Part
A of the test data. Each team could hand-in up to 3 sets of results per method submitted
in Stage 2. The submission of the segmentation results was due by September 14th,
2015. Evaluation of the submitted results was not disclosed to the teams until after the
challenge event.

Stage 4: GlaS’2015 Challenge Event The event was held in conjunction with
MICCAI’2015 on October 5th, 2015. All teams were asked to produce segmentation
results on the second part of the test data (Test Part B, see Section 3) within 45 minutes.
The teams could either bring their own machines or conduct an experiment remotely.
There was no restriction on the number of machines that the teams could use to produce

2http://www.warwick.ac.uk/bialab/GlaScontest

6

results. Those teams that could not be present at the event provided implementations of
their algorithms with which the organizers carried out the segmentation on their behalf.
Each team was also asked to give a short presentation, discussing their work. At the
end of the event, the complete evaluation of segmentation results across both parts of
the test data was announced, which included a ﬁnal ranking of the submitted methods.
This information is also available on the challenge website.

4.1 Challenge Statistics

By the end of Stage 1, a total of 110 teams from diﬀerent academic and industrial
institutes had registered. A total of 21 teams submitted the 2-page document for review
in Stage 2, and 20 teams were invited to participate in the GlaS competition event. In
Stage 3, only 13 teams submitted results on Part A of the test data in time. Late entries
were neither evaluated nor considered in the next stage of the competition. On the day
of the challenge event, 11 of the 13 teams that submitted the results on time in Stage 3
attended the on-site competition and presented their work. The organizers carried out
the segmentation on behalf of the other two teams that could not be present.

5 Evaluation

The performance of each segmentation algorithm was evaluated based on three crite-
ria: 1) accuracy of the detection of individual glands; 2) accuracy of the segmentation
of individual glands; and 3) shape similarity between glands and their corresponding
segmentation. It may seem that segmentation accuracy would entail shape similarity
between a gland and its segmentation. However, in practice, this is not always the case.
The metric for segmentation accuracy used in this challenge, was deﬁned and calculated
using the label that the algorithm had assigned to each pixel, but the metric for shape
similarity used the position assigned by the algorithm to the boundary of each gland.
Pixels labels may be fairly accurate, while the boundary curves are very diﬀerent. The
remainder of this section describes all metrics employed in the evaluation.
We use the concept of a pair of corresponding segmented and ground truth objects as
proposed in Sirinukunwattana et al. [2015]. Let S denote a set of all segmented objects
and G denote a set of all ground truth objects. We also include in each of these sets
the empty object ∅. We deﬁne a function G∗ : S → G, by setting, for each segmented
object S ∈ S, G∗(S) = G ∈ G where G has the largest possible overlapping area with
S. Although there could be more than one G ∈ G that maximally overlaps S, this in
practice is extremely rare, and it is good enough to consider one of these G as the value
of G∗(S). If there is no overlapping G, we set G∗(S) = ∅. (However, in the context of
Hausdorﬀ distance – see Section 5.3 – G∗ will be extended in a diﬀerent way.) Similarly,
we deﬁne S∗ : G → S, by setting, for each G ∈ G, S∗(G) = S ∈ S, where S has the largest
possible overlapping area with G. Note that G∗ and S∗ are, in general, neither injective,
nor surjective. Nor are they inverse to each other, in general. They do, however, assign
to each G an S = S∗(G), and to each S a G = G∗(S).

5.1 Detection Accuracy

The F1 score is employed to measure the detection accuracy of individual glandular
objects. A segmented glandular object that intersects with at least 50% of its ground
truth object is counted as true positive, otherwise it is counted as false positive. A

7

ground truth glandular object that has no corresponding segmented object or has less
than 50% of its area overlapped by its corresponding segmented object is considered as
false negative. Given these deﬁnitions, the F1 score is deﬁned by

F1score =

2 · Precision · Recall
Precision + Recall

,

TP

(1)

(2)

,

where

Precision =

TP

TP + FP

, Recall =

TP + FN

and TP, FP, and FN denote respectively the number of true positives, false positives,
and false negatives from all images in the dataset.

5.2 Segmentation Accuracy

5.2.1 Object-Level Dice Index

The Dice index [Dice, 1945] is a measure of agreement or similarity between two sets of
samples. Given G, a set of pixels belonging to a ground truth object, and S, a set of
pixels belonging to a segmented object, the Dice index is deﬁned as follows:

2|G ∩ S|
|G| + |S| ,

Dice(G, S) =

(3)
where | · | denotes set cardinality. The index ranges over the interval [0, 1], where the
higher the value, the more concordant the segmentation result and the ground truth. A
Dice index of 1 implies a perfect agreement. It is conventional that the segmentation
accuracy on an image is calculated by Dice(Gall, Sall), where Gall denotes the set of pixels
of all ground truth objects and Sall denotes the set of pixels of all segmented objects.
The calculation made in this way measures the segmentation accuracy only at the pixel
level, not at the gland level, which was the main focus of the competition.

To take the notion of an individual gland into account, we employ the object-level
Dice index [Sirinukunwattana et al., 2015]. Let nG be the number of non-empty ground
truth glands, as annotated by the expert pathologist. Similarly let nS be the number of
glands segmented by the algorithm, that is the number of non-empty segmented objects.
Let Gi ∈ G denote the ith ground truth object, and let Sj ∈ S denote the jth segmented
object. The object-level Dice index is deﬁned as

 ,

(4)

(5)

Diceobj(G,S) =

1
2

γiDice(Gi, S∗(Gi)) +

σjDice(G∗(Sj), Sj)

where

γi = |Gi|/

σj = |Sj|/

|Sq|

 nG(cid:88)

i=1

nG(cid:88)

p=1

|Gp|,

nS(cid:88)
nS(cid:88)

j=1

q=1

On the right hand side of (4), the ﬁrst summation term reﬂects how well each ground
truth object overlaps its segmented object, and the second summation term reﬂects how
well each segmented object overlaps its ground truth objects. Each term is weighted
by the relative area of the object, giving less emphasis to small segmented and small
ground truth objects.
In the competition, the object-level Dice index of the whole test dataset was calcu-
lated by including all the ground truth objects from all images in G and all the segmented
objects from all images in S.

8

5.2.2 Adjusted Rand Index

We also included the adjusted Rand index [Hubert and Arabie, 1985] as another evalu-
ation measure of segmentation accuracy. This index was used for additional assessment
of the algorithm performance in Section 8.3.
The adjusted Rand index measures similarity between the set of all ground truth
objects G and the set of all segmented objects S, based on how pixels in a pair are
labeled. Two possible scenarios for the pair to be concordant are that (i) they are
placed in the same ground truth object in G and the same segmented object in S, and
(ii) they are placed in diﬀerent ground truth objects in G and in diﬀerent segmented
objects in S. Deﬁne nij as the number of pixels that are common to both the ith ground
truth object and the jth segmented object, ni,· as the total number of pixels in the ith
ground truth object, n·,j as the total number of pixels in the jth segmented object, and
n as the total number of pixels. Following a simple manipulation, it can be shown that
the probability of agreement is equal to

(cid:19)

(cid:18)n

2

nG(cid:88)

nS(cid:88)

(cid:18)nij

(cid:19)

i=1

j=1

2

−

nG(cid:88)

(cid:18)ni,·

(cid:19)

2

i=1

− nS(cid:88)

j=1

(cid:18)n·,j

2

(cid:19)(cid:44)(cid:18)n

(cid:19)

2

Pagreement =

+ 2

.

(6)

Here, the numerator term corresponds to the total number of agreements, while the
denominator term corresponds to the total number of all possible pairs of pixels. Under
the assumption that the partition of pixels into ground truth objects in G and segmented
objects in S follows a generalized hypergeometric distribution, the adjusted Rand index
can be formulated as
ARI(G,S) =

(cid:0)ni,j
(cid:80)nG
(cid:80)nS
(cid:104)(cid:80)nG
(cid:1) +(cid:80)nS
(cid:0)ni,·

(cid:1)
(cid:0)n·,j
(cid:1)(cid:14)(cid:0)n
(cid:0)n·,j
(cid:1)(cid:80)nS

(cid:1)(cid:14)(cid:0)n
(cid:1) .

(cid:1)(cid:80)nS
(cid:1) −(cid:80)nG
(cid:0)ni
(cid:1)(cid:105) −(cid:80)nG
(cid:0)ni,·
(cid:0)n·,j

i=1

2

(7)

j=1

j=1

i=1

2

2

2

i=1

2

j=1

2

2

j=1

2

i=1

2

1
2

The adjusted Rand index is bounded above by 1, and it can be negative.

5.3 Shape Similarity
We measure the shape similarity between the segmented objects in S and the ground
truth objects in G using the object-level Hausdorﬀ distance. The usual deﬁnition of a
Hausdorﬀ distance between ground truth object G and segmented object S is

H(G, S) = max{sup
x∈G

inf
y∈S

d(x, y), sup
y∈S

inf
x∈G

d(x, y)}

(8)

where d(x, y) denotes the distance between pixels x ∈ G and y ∈ S. In this work, we use
the Euclidean distance. According to (8), Hausdorﬀ distance is the most extreme value
from all distances between the pairs of nearest pixels on the boundaries of S and G.
Thus, the smaller the value of the Hausdorﬀ distance, the higher the similarity between
the shapes of S and G, and S = G if their Hausdorﬀ distance is zero.

To calculate the overall shape similarity between a pair of corresponding segmented
and ground truth objects, we now introduce object-level Hausdorﬀ distance by imitating
the deﬁnition of object-level Dice index (4). The object-level Hausdorﬀ distance is
deﬁned as

nS(cid:88)

 ,

γiH(Gi, S∗(Gi)) +

σjH(G∗(Sj), Sj)

(9)

 nG(cid:88)

Hobj(G,S) =

1
2

i=1

j=1

9

where the meaning of the mathematical notation is similar to that given in Section 5.2.1.
In case a ground truth object G does not have a corresponding segmented object (i.e.
S∗(G) = ∅), the Hausdorﬀ distance is calculated between G and the nearest segmented
object S ∈ S to G (in the Hausdorﬀ distance) in that image instead. The same applies
for a segmented object that does not have a corresponding ground truth object.

6 Ranking Scheme

Each submitted entry was assigned one ranking score per evaluation metric and set of
test data. Since there were 3 evaluation metrics (F1 score for gland detection, object-
level Dice index for segmentation accuracy, and object-level Hausdorﬀ index for shape
similarity) and 2 sets of test data, the total number of ranking scores was 6. The best
performing entry was assigned ranking score 1, the second best was assigned ranking
score 2, and so on. In care of a tie, the standard competition ranking was applied. For
instance, F1 score 0.8, 0.7, 0.7, and 0.6 would result in the ranking scores 1, 2, 2, and
4. The ﬁnal ranking was then obtained by adding all 6 ranking scores (rank sum). The
entry with smallest sum was placed top in the ﬁnal ranking.

7 Methods

The top ranking methods are described in this section. They are selected from the
total of 13 methods that participated in all stages of the challenge. The cut-oﬀ for the
inclusion in this section was made where there was a substantial gap in the rank sums
(see A, Figure 3). Of the 7 selected methods, only 6 preferred to have their methods
described here.

7.1 CUMedVision3

A novel deep contour-aware network was presented. This method explored the multi-
level feature representations with fully convolutional networks (FCN) [Long et al., 2015].
The network outputted segmentation probability maps and depicted the contours of
gland objects simultaneously. The network architecture consisted of two parts: a down-
sampling path and an up-sampling path. The down-sampling path contained convolu-
tional and max-pooling layers while the up-sampling path contained convolutional and
up-sampling layers, which increased the resolutions of feature maps and outputted the
prediction masks.
In total, there were 5 max-pooling layers and 3 up-sampling lay-
ers. Each layer with learned parameters was followed by a non-linear mapping layer
(element-wise rectiﬁed linear activation).

In order to separate touching glands, the feature maps from hierarchical layers were
up-sampled with two diﬀerent branches to output the segmented object and contour
masks respectively. The parameters of the down-sampling path were shared and up-
dated for these two kinds of masks. This could be viewed as a multi-task learning
framework with feature representations, simultaneously encoding the information of
segmented objects and contours. To alleviate the problem of insuﬃcient training data
[Chen et al., 2015], an oﬀ-the-shelf model from DeepLab [Chen et al., 2014], trained on
the 2012 PASCAL VOC dataset4, was used to initialize the weights for layers in the

3Department of Computer Science and Engineering, The Chinese University of Hong Kong.
4http://host.robots.ox.ac.uk:8080/pascal/VOC/voc2012/index.html

10

down-sampling path. The parameters of the network were obtained by minimizing the
loss function with standard back-propagation 5.

The team submitted two entries for evaluation. CUMedVision1 was produced by
FCN with multi-level feature representations relying only on gland object masks, while
CUMedVision2 was the results of the deep contour-aware network, which considers
gland object and contour masks simultaneously.

7.2 CVML6

In the ﬁrst, preprocessing, stage the images were corrected to compensate for varia-
tions in the appearance due to a variability of the tissue staining process. This was
implemented through histogram matching, where the target histogram was calculated
from the whole training data, and the individual image histograms were used as inputs.
The main processing stage was based on two methods: a convolutional neural network
(CNN) [Krizhevsky et al., 2012] for a supervised pixel classiﬁcation, and a level set seg-
mentation for grouping pixels into spatially coherent structures. The employed CNN
used an architecture with two convolutional, pooling and fully connected layers. The
network was trained with three target classes. The classes were designed to represent (1)
the tubular interior of the glandular structure (inner class), (2) epithelial cells forming
boundary of the glandular structure (boundary class) and (3) inter-gland tissue (outer
class). The inputs to the CNN were 19 × 19 pixel patches sliding across the adjusted
RGB input image. The two convolutional layers used 6× 6 and 4× 4 kernels with 16 and
36 feature maps respectively. The pooling layers, implementing the mean function, used
2 × 2 receptive ﬁelds and 2 × 2 stride. The ﬁrst and second fully connected layers used
the rectiﬁed linear unit and softmax functions respectively. The outputs from the CNN
were two probability maps representing the probability of each image pixel belonging to
the inner and boundary classes. These two probability maps were normalized between
-1 and 1 and used as a propagation term, along with an advection term and a curvature
ﬂow term. These terms were part of the hybrid level set model described in Zhang
et al. [2008]. In the post-processing stage, a sequence of morphological operations was
performed to removed small objects, ﬁll holes and disconnect weakly connected objects.
Additionally, if an image boundary intersecting an object forms a hole, the correspond-
ing pixels was labeled as part of that object. The team submitted a single entry for
evaluation, henceforth referred to as CVML.

7.3 ExB7

This method ﬁrst preprocessed the data by performing per channel zero mean and unit
variance normalization, where the mean and variance were computed from the training
data. The method then exploited the local invariance properties of the task by applying
a set of transformations to the data. At training time, the dataset was augmented by
applying aﬃne transformations, Gaussian blur and warping. During testing, both image
mirroring and rotation were applied.

The main segmentation algorithm consisted of a multi-path convolutional neural net-
work. Each path was equipped with a diﬀerent set of convolutional layers and conﬁgured

5More details will be available at: http://www.cse.cuhk.edu.hk/~hchen/research/2015miccai_

gland.html

6School of Engineering, University of Central Lancashire, Preston, UK.
7ExB Research and Development.

11

to capture features from diﬀerent views in a local-global fashion. All the diﬀerent paths
were connected to a set of two fully connected layers. A leaky rectiﬁed linear unit was
used as a default activation function between layers, and a softmax layer was used after
the last fully connected layer. Every network was trained via stochastic gradient descent
with momentum, using a step-wise learning rate schedule [Krizhevsky et al., 2012]. The
network was randomly initialized such that unit variance was preserved across layers. It
was found that using more than three paths led to heavy over-ﬁtting – this was due to
insuﬃcient training data.
Simple-path networks were trained to detect borders of glands. The ground truth
for these networks was constructed using a band of width K ∈ [5, 10] pixels along a
real gland border. These values of K were found to produce optimal and equivalent
quantitative results, measured by the F1 score and the object-Dice index. The output
of these networks was used to better calibrate the ﬁnal prediction.

In the post-processing step, a simple method was applied to clean noise and ﬁll holes
in the structures. Thresholding was applied to remove spurious structures with diameter
smaller than a certain epsilon. Filling-hole criteria based on diameter size was also used.
Using the initial class discrimination (benign and malignant), a simple binary clas-
siﬁer constructed from a convolutional neural network with 2 convolutional and 1 fully
connected layers was trained. This binary classiﬁer used the raw image pixels as input.
The output of the classiﬁer was used together with the border networks and the post-
processing method to apply a diﬀerent set of parameters/thresholds depending on the
predicted class. The hyperparameters for the entire pipeline, including post-processing
and border networks, were obtained through cross-validation.

For this method, the team submitted 3 entries. ExB 1 was a two-path network
including both the border network for detecting borders of glands and the binary classi-
ﬁcation to diﬀerentiate between the post-processing parameters. ExB 2 was similar to
ExB 1 without the use of the border network. ExB 3 used a two-path network without
any post-processing.

7.4 Image Analysis Lab Uni Freiburg8

The authors applied a u-shaped deep convolutional network “u-net”9 [Ronneberger et al.,
2015] for the segmentation. The input was the raw RGB image and the output was a bi-
nary segmentation map (glands and background). The network consisted of an analysis-
path constructed from a sequence of convolutional layers and max-pooling layers, fol-
lowed by a synthesis path with a sequence of up-convolutional layers and convolutional
layers, resulting in 23 layers in total. Additional shortcut-connections propagated the
feature maps at all detail levels from the analysis to the synthesis path. The network
was trained from scratch in an end-to-end fashion with only the images and ground
truth segmentation maps provided by the challenge organizers. To teach the network
the desired invariances and to avoid overﬁtting, the training data were augmented with
randomly transformed images and the correspondingly transformed segmentation maps.
The applied transformations were random elastic deformations, rotations, shifts, ﬂips,
and blurs. The color transformations were random multiplications applied in the HSV
color space. To avoid accidentally joining touching objects, a high pixel-wise loss weight

8Computer Science Department and BIOSS Centre for Biological Signalling Studies, University of

Freiburg, Germany.

9The implementation of the u-net is freely available at http://lmb.informatik.uni-freiburg.de/

people/ronneber/u-net/.

12

was introduced for pixels in thin gaps between objects in the training dataset (see Ron-
neberger et al. [2015]). The exact same u-net layout with the same hyperparameters as
in Ronneberger et al. [2015] was used for the challenge. The only diﬀerence were more
training iterations and a slower decay of the learning rate.

The team submitted two entries. The ﬁrst entry Freiburg1 was a connected com-
ponent labelling applied to the raw network output. The second entry Freiburg2
post-processed the segmentation maps with morphological hole-ﬁlling and deletion of
segments smaller than 1000 pixels.

7.5 LIB10

Intestinal glands were divided according to their appearance into three categories: hol-
low, bounded, and crowded. A hollow gland was composed of lumen and goblet cells
and it could be a hole in the tissue surface. A bounded gland had the same composi-
tion, but in addition, it was surrounded by a thick epithelial layer. A crowded gland
was composed of bunches of epithelial cells clustered together and it might have shown
necrotic debris.

The tissue was ﬁrst classiﬁed into one of the above classes before beginning the
segmentation. The classiﬁcation relied on the characterization of the spatial distribution
of cells and the topology of the tissue. Therefore, a closing map was generated with a
cumulative sum of morphological closing by a disk of increasing radius (1 to 40 pixels)
on the binary image of nuclear objects, which were segmented by the k-means algorithm
in the RGB colour space. The topological features were calculated from a normalized
closing map in MSER fashion (Maximally Stable Extremal Region, Matas et al. [2004])
as the number of regions below three diﬀerent thresholds (25%, 50% and 62.5%) and
above one threshold (90%), their sizes and the mean of their corresponding values in
the closing map. The ﬁrst three thresholds characterized the holes and the fourth one
characterized the thickness of nuclear objects. After classifying the tissue with a Naive
Bayes classiﬁer trained on these features, a speciﬁc segmentation algorithm was applied.
Three segmentation algorithms were presented, one for each category. Hollow glands
were delineated by morphological dilation on regions below 50%. Bounded gland candi-
dates were ﬁrst detected as hollow glands, then the thickness of nuclear objects surround-
ing the region was evaluated by generating a girth map and a solidity map [Ben Cheikh
et al., 2016], then after classifying nuclear objects, the epithelial layer was added or the
candidate was removed. Crowded glands were identiﬁed as populous regions (regions
above 90%), and then morphological ﬁltering was applied for reﬁnement. The team
submitted a single entry labeled as LIB for evaluation.

7.6 vision4GlaS11

Given an H&E-stained RGB histopathological section, the gland segmentation method
was based on a pixel-wise classiﬁcation and an active contour model, and it proceeded
in three steps [Kainz et al., 2015]. In a ﬁrst preprocessing step the image was rescaled to

10Sorbonne Universit´es, UPMC Univ Paris 06, CNRS, INSERM, Biomedical Imaging Laboratory

(LIB), Paris, France.

11Institute of Biophysics, Center for Physiological Medicine, Medical University of Graz, Graz, Austria;
Institute of Neuroinformatics, University of Zurich and ETH Zurich, Zurich, Switzerland; Institute for
Computer Graphics and Vision, BioTechMed, Graz University of Technology, Graz, Austria; Ludwig
Boltzmann Institute for Clinical Forensic Imaging, Graz, Austria.

13

half the spatial resolution, and color deconvolution separated the stained tissue compo-
nents. The red channel of the deconvolved RGB image represented the tissue structure
best and was therefore considered for further processing. Next, two convolutional neural
networks (CNNs) [LeCun et al., 2010] of seven layers each were trained for pixel-wise
classiﬁcation on a set of image patches. Each network was trained with ReLU nonlin-
earities, and stochastic gradient descent with momentum, weight decay, and dropout
regularization to minimize a negative log-likelihood loss function. The ﬁrst CNN, called
Object-Net, was trained to distinguish four classes: (i) benign background, (ii) benign
gland, (iii) malignant background, and (iv) malignant gland. For each image patch the
probability distribution over the class labels was predicted, using a softmax function.
The Object-Net consisted of three convolutional layers followed by max-pooling, a ﬁ-
nal convolutional layer and three fully connected layers. The second – architecturally
similar – CNN called Separator-Net, learned to predict pixels of gland-separating struc-
tures in a binary classiﬁcation task. Ground truth was generated by manually labeling
image locations, close to two or more gland borders, as gland-separating structures. In
the ﬁnal step the segmentation result was obtained by combining the outputs of the
two CNNs. Predictions for benign and malignant glands were merged, and predictions
of gland-separating structures were subtracted to emphasize the foreground probabil-
ities. Background classes were handled similarly. Using these reﬁned foreground and
background maps, a ﬁgure-ground segmentation based on weighted total variation was
employed to ﬁnd a globally optimal solution. This approach optimized a geodesic ac-
tive contour energy, which minimized contour length while adhering to the reﬁned CNN
predictions [Bresson et al., 2007]. The team submitted a single entry, referred to as
vision4GlaS.

8 Results and Discussion

8.1 Summary of the Methods

The methods described above take one of the following two approaches to segmentation:
(a) they start by identifying pixels corresponding to glands which are then grouped
together to form separated, spatially coherent objects; (b) they begin with candidate
objects that are then classiﬁed as glands or non-glands. All methods that are based on
CNNs (CUMedVision, CVML, ExB, Freiburg, and vision4GlaS) follow the former ap-
proach. CVML, ExB, and vision4GlaS built CNN classiﬁers that assign a gland-related
or non-gland-related label to every pixel in an image, by taking patch(es) centered at
the pixel as input. ExB, in particular, use multi-path networks into which patches at
diﬀerent sizes are fed, in order to capture contextual information at multiple scales.
CUMedVision and Freiburg, on the other hand, base their pixel classiﬁer on a fully
convolutional network architecture [Long et al., 2015], allowing simultaneous pixel-wise
label assignment at multiple pixel locations. To separate gland-related pixels into indi-
vidual objects, CVML and vision4GlaS deploy contour based approaches. ExB trains
additional networks for glandular boundary, while CUMedVision and Freiburg explic-
itly include terms for boundary in the training loss function of their networks. The
only method that follows the latter approach for object segmentation is LIB. In this
method, candidate objects forming part of a gland (i.e., lumen, epithelial boundary)
are ﬁrst identiﬁed, and then classiﬁed into diﬀerent types, followed by the ﬁnal step of
segmentation.

A variety of data transformation and augmentation were employed to deal with vari-

14

Table 2: Summary results. The evaluation is carried out according to the challenge
criteria described in Section 6. A ranking score is assigned to each algorithm according
to its performance in each evaluation measure, obtained from each test part. The entries
are listed in a descending order based on their rank sum

F1score

Diceobj

Hobj

Method

Part A

Part B

Part A

Part B

Part A

Part B

Rank Sum

Score Rank Score Rank Score Rank Score Rank Score Rank Score Rank

CUMedVision2 0.912

ExB1

ExB3

Freiburg2

0.891

0.896

0.870

CUMedVision1 0.868

ExB2

Freiburg1

CVML

LIB

vision4GlaS

0.892

0.834

0.652

0.777

0.635

1

4

2

5

6

3

7

9

8

10

0.716

0.703

0.719

0.695

0.769

0.686

0.605

0.541

0.306

0.527

3

4

2

5

1

6

7

8

10

9

0.897

0.882

0.886

0.876

0.867

0.884

0.875

0.644

0.781

0.737

1

4

2

5

7

3

6

10

8

9

0.781

0.786

0.765

0.786

0.800

0.754

0.783

0.654

0.617

0.610

5

2

6

3

1

7

4

8

9

45.418

57.413

57.350

57.093

74.596

54.785

57.194

1

6

5

3

7

2

4

160.347

145.575

159.873

148.463

153.646

187.442

146.607

155.433

10

176.244

112.706

190.447

6

1

5

3

4

8

2

7

9

10

107.491

210.105

10

9

8

17

21

22

24

26

29

30

52

53

56

ation within the data. In order to counter the eﬀect of stain variation, CVML and ExB
performed transformations of the RGB color channels, vision4GlaS used a stain decon-
volution technique to obtain only the basophilic channel in their preprocessing step.
By contrast, Freiburg tackled the issue of stain variability through data augmentation,
which implicitly forces the networks to be robust to stain variation to some extent. As
is common among methods using CNNs, spatial transformations, such as aﬃne transfor-
mations (e.g. translation, rotation, ﬂip), elastic deformations (e.g. pincushion and barrel
distortions), and blurring, were also used in the data augmentation to teach the network
to learn features that are spatially invariant. The other beneﬁt of data augmentation is
it provides, to some extent, avoidance of over-ﬁtting.

ExB, LIB, and vision4GlaS incorporated histologic grades of glands in their seg-
mentation approach. In ExB, procedures and/or parameter values used in boundary
detection and post-precessing steps were diﬀerent, subject to the predicted histologic
grade of an image. vision4GlaS classiﬁed pixels based on histological information. Al-
though not explicit, LIB categorized candidate objects forming glands according to their
appearance, related to histologic grades, before treating them in diﬀerent ways.

As a post-processing step, many segmentation algorithms employed simple criteria
and/or a sequence of morphological operations to improve their segmentation results.
A common treatment was to eliminate small spurious segmented objects. Imperfections
in pixel labelling can result in the appearance of one or more holes in the middle of an
object. Filling such holes is often necessary. In addition to these operations, CVML
performed morphological operations to separate accidentally joined objects.

8.2 Evaluation Results

Table 2 summarizes the overall evaluation scores and ranks achieved by each entry from
each test part. We list the entries according to the order of their rank sum, which
indicates the overall performance across evaluation measures and tasks of the entries.
The lower the rank sum, the more favorable the performance. The top three entries
according to the overall rank sum in descending order are CUMedVision2, ExB1, and

15

Figure 2: Performance scores achieved by diﬀerent entries on the combined test data.
Evaluation is conducted on three subsets of the data: (1st row) the whole test data,
(2nd row) benign, and (3rd row) malignant.

ExB3. However, if rank sum is considered with respect to the test part, the three best
entries are CUMedVision2, ExB2, and ExB3 for part A; whereas in part B, CUMedVi-
sion1, ExB1, and Freiburg2 come at the top. A summary of the ranking results from
the competition can be found in A.

8.3 Additional Experiments

In the challenge, the split of the test data into two parts – Part A (60 images) for oﬀ-site
test and Part B (20 images) for on-site test – to some extent introduces bias into the
performance evaluation of the segmentation algorithms due to equal weight given to
performance on the two test parts. The algorithms that perform particularly well on
Test Part B would therefore get a better evaluation score even though they may not
have performed as well on Test Part A, where the majority of the test dataset is to be
found. In addition, the imbalance between the benign and malignant classes in Test Part
B, only 4 benign (20%) and 16 malignant (80%) images, would also favor algorithms
that perform well on the malignant class. In order to alleviate these issues, we merged
the two test parts and re-evaluated the performance of all the entries. In addition, as
suggested by one of the participating teams, the adjusted Rand index is included as
another performance measurement for segmentation.

The evaluation scores calculated from the combined two test parts are presented
as bar chart in Figure 2. The ﬁnal rankings based on the rank sums of evaluation
scores calculated from the combined two test parts are reported in Table 3. Here,

16

F1.ScoreObject.DiceObject.HausdorffARI0.8560.8870.6370.8680.8680.8750.8040.8490.7040.620.8720.9190.6910.9150.920.9190.8930.9160.80.6510.7880.7710.4390.6980.6860.7140.5210.6160.3880.4780.850.8680.6470.8580.8510.8550.8520.8530.7440.7050.8730.9070.6630.8920.9020.90.9030.9030.8170.7380.8260.8270.630.8230.7990.810.7980.8010.6680.67394.51574.731160.50679.76189.1883.48379.32679.842130.345133.15867.86243.025128.2553.97949.3352.61749.11849.11578.436102.277122.187106.979192.555105.986129.422114.722111.037111.959184.51164.0050.8520.850.6360.850.8370.8440.840.840.7570.710.870.8680.6020.8640.8710.8690.8670.8670.7860.6940.8380.8360.6610.8390.8120.8250.8190.820.7340.72vision4GlaSLIBFreiburg1Freiburg2ExB3ExB2ExB1CVMLCUMedVision2CUMedVision1vision4GlaSLIBFreiburg1Freiburg2ExB3ExB2ExB1CVMLCUMedVision2CUMedVision1vision4GlaSLIBFreiburg1Freiburg2ExB3ExB2ExB1CVMLCUMedVision2CUMedVision1OverallBenignMalignant0.000.250.500.750.000.250.500.750501001502000.000.250.500.75ValueMethodTable 3: Ranking results of the entries when the two parts of test data are combined.
Two set of ranking scheme are considered: a) F1score + Diceobj+Hobj and b) F1score +
ARI + Hobj. In addition to the evaluation on the whole test data (overall), the entries
are evaluated on a subset of the data according to the histologic labels, i.e. benign and
malignant.

Entry

F1score + Diceobj+Hobj

F1score + ARI + Hobj

Overall Benign Malignant Overall Benign Malignant

Final Ranking

CUMedVision1

CUMedVision2

CVML

ExB1

ExB2

ExB3

Freiburg1

Freiburg2

LIB

vision4GlaS

7

1

10

2

6

3

4

5

8

9

7

1

10

6

3

5

4

2

8

9

3

1

10

2

7

4

6

5

9

8

4

1

10

2

7

3

6

5

8

9

6

2

10

7

1

3

5

4

8

9

3

2

10

1

7

4

6

5

9

8

two set of rank sums are considered: one calculated according to the criteria of the
competition (i.e., F1score + Diceobj + Hobj), and the other where the adjusted Rand
index is used instead of the object-level Dice index to evaluate segmentation accuracy
(i.e., F1score + ARI + Hobj). For both sets of rank sums, the new ranking orders are
largely similar to those reported in Section 8.2, with a few swaps in the order, while the
top three entries remaining the same, namely CUMedVision2, ExB1, ExB3.

The main factors that negatively aﬀect the performance of the methods are a number
of challenges presented by the dataset. Firstly, large white empty areas corresponding
to the lumen of the gastrointestinal tract which are not in the interior of intestinal
glands can easily confuse the segmentation algorithms. Secondly, characteristics of non-
glandular tissue can sometimes resemble that of the glandular tissue. For instance,
connective tissue in muscularis mucosa or sub-mucosa layers of the colon is stained
white and pinkish and has less dense nuclei, thus resembling the inner part of glands.
In the case where there is less stain contrast between nuclei and cytoplasm due to
elevated levels of Hematoxylin stain, non-glandular tissue with dense nuclei can look
similar to malignant epithelial tissue. Thirdly, small glandular objects are blended
into the surrounding tissue and can be easily mis-detected. A careful inspection of the
segmentation results generated by each entry showed that methods by CUMedVision,
ExB, and Freiburg better avoid over-segmentation or under-segmentation when facing
the above-mentioned pitfalls.

The performance of each entry with respect to the histologic grade of cancer was also
examined. Their evaluation scores based on benign and malignant samples are reported
in the second and the third rows of Figure 2 respectively, and the ranking orders derived
from the rank sums of the scores are shown in Table 3. Based on these results, one can

17

get a better contrast between the performance of the entries that enforce border separa-
tion and those that do not. By applying a predicted border mask to separate clumped
segmented objects, CUMedVision2 performs better than CUMedVision1, which tends
to produce segmentation results that merge neighboring glands together, in both benign
and malignant cases. Similarly, ExB1 is able to segment malignant glands better than
ExB2 and ExB3 that do not utilize border separation. However, this can have an ad-
verse eﬀect if the algorithm already yields segmentation results that separate individual
objects well, such as in the case of ExB1 which under-segments benign glandular objects
as compared to its counterparts ExB2 and ExB3.

8.4 General Discussion

The objectives of this challenge were to raise the research community’s awareness of
the existence of the intestinal gland segmentation problem in routine stained histol-
ogy images, and at the same time to provide a platform for a standardized comparison
of the performance of automatic and semi-automatic algorithms. The challenge at-
tracted a lot of attention from researchers, as can be seen from the number of registered
teams/individuals and the number of submissions at each stage of the competition.
Interestingly, some of the teams had no experience in working with histology images
before. We would like to emphasize that ﬁnding the best performing approach is not
the main objective of the competition, but rather pushing the boundaries of the-state-
of-the-art approaches. Already, we have seen quite interesting developments from many
participating teams and the leading algorithms have produced excellent results, both
qualitatively and quantitatively.

As noted in the introduction, morphometric analysis of the appearance of cells and
tissues, especially those forming glands from which tumors originate, is one of the key
components towards precision medicine, and segmentation is the ﬁrst step to attain
morphological information. Some may have argued that there is no need to perform
segmentation, but instead, to follow conventional pattern recognition approaches by ex-
tracting mathematical features which normally capture local and/or global tissue archi-
tecture and then identifying features that are most suited to the objective of the study.
It is true that there are a number of successful works that follow such an approach
[Jafari-Khouzani and Soltanian-Zadeh, 2003, Tabesh et al., 2007, Altunbay et al., 2010,
Basavanhally et al., 2010, Ozdemir and Gunduz-Demir, 2013, Gultekin et al., 2015].
However, because these extracted features are often physically less interpretable in the
eyes of practitioners, it is diﬃcult to adopt such an approach in clinical settings. On the
other hand, the appearance of glands such as size and shape obtained through segmen-
tation is easy to interpret. Segmentation also helps to localize other type of information
(e.g., texture, spatial arrangement of cells) that is speciﬁc to the glandular areas.

Even though the dataset used in the challenge included images of diﬀerent histologic
grades taken from several patients, it lacked other aspects. First of all, inter-observer
variability was not taken into account as the ground truth was generated by a single
expert. This is because the intricate and arduous nature of the problem makes it dif-
ﬁcult to ﬁnd several volunteer experts to perform manual segmentation. Considerable
experience is required in order to delineate boundaries of malignant glands, which are
not so well-deﬁned as those of the benign ones. Moreover, a single image can contain
a large number of glands to be segmented, making the task very laborious. Secondly,
digitization variability was also not considered in this dataset. It is, in fact, very im-
portant to evaluate the robustness of algorithms when the data are scanned by diﬀerent

18

instruments. As whole-slide scanners are becoming increasingly available, this type of
real-world problem should be expected.

The choice of evaluation measures would also aﬀect the comparative results. In this
challenge, we emphasized object segmentation and accordingly deﬁned the object-level
Dice index and the object-level Hausdorﬀ distance to measure segmentation accuracy at
the object level rather than at the pixel level. Nonetheless, it has been suggested that
these measures are too strict, as they put a severe penalty on mismatch of the objects.
One could replace these measures by less conservative ones, for example, adjusted Rand
index [Hubert and Arabie, 1985] or a topology preserving warping error [Jain et al.,
2010] for segmentation and elastic distance [Younes, 1998, Joshi et al., 2007] for shape
similarity. For this reason, we included adjusted rand index as an alternative to object-
level Dice index in Section 8.3. As we have already pointed out, this results in only a
minor change in the ranking order of the entries. Another aspect that was not explicitly
included in the evaluation was execution times. Nevertheless, all the algorithms were
capable of completing the segmentation task on the on-site test data (Part B) in the
given amount of time with or without limitation of resources. Time eﬃciency is required
to process large scale data, such as whole-slide images, whose volume is growing by the
day as slides are routinely scanned. Still, in medical practice, accuracy is far more
important than speed.

The challenge is now completed, but the dataset will remain available for research
purposes so as to continually attract newcomers to the problem and to encourage devel-
opment of state-of-the-art methods. Extension of the dataset to address inter-observer
and inter-scanner variability seems to be the most achievable aim in the near future.
Beyond the scope of segmentation, there lie various extremely interesting future research
directions. Previous studies have shown the strong association between the survival of
colorectal cancer patients and tumor-related characteristics, including lymphocytic inﬁl-
tration [Galon et al., 2006, Fridman et al., 2012], desmoplasia [Tommelein et al., 2015],
tumor budding [Mitrovic et al., 2012], and necrosis [Richards et al., 2012]. A systematic
analysis of these characteristics with the help of gland segmentation as part of automatic
image analysis framework could lead to a better understanding of the relevant cancer
biology as well as bring precision and accuracy into assessment and prediction of the
outcome of the cancer.

9 Conclusions

This paper presented a summary of the Gland Segmentation in Colon Histology Im-
ages (GlaS) Challenge Contest which was held in conjunction with the 18th Inter-
national Conference on Medical Image Computing and Computer Assisted Interven-
tions (MICCAI’2015). The goal of the challenge was to bring together researchers
interested in the gland segmentation problem, to validate the performance of their
existing or newly invented algorithms on the same standard dataset.
In the ﬁnal
round, the total number of submitted entries for evaluation was 19, and we presented
here in this paper 10 of the leading entries. The dataset used in the challenge has
been made publicly available and can be accessed at the challenge website (http:
//www.warwick.ac.uk/bialab/GlasContest/). Those who are interested in develop-
ing or improving their own approaches are encouraged to use this dataset for quantitative
evaluation.

19

10 Acknowledgements

This paper was made possible by NPRP grant number NPRP5-1345-1-228 from the
Qatar National Research Fund (a member of Qatar Foundation). The statements made
herein are solely the responsibility of the authors. Korsuk Sirinukunwattana acknowl-
edges the partial ﬁnancial support provided by the Department of Computer Science,
University of Warwick, UK. CUMedVision team acknowledges Hong Kong RGC General
Research Fund (Project No. CUHK 412513). The work of Olaf Ronneberger was sup-
ported by the Excellence Initiative of the German Federal and State Governments (EXC
294). Philipp Kainz was supported by the Excellence Grant 2014 of the Federation of
Austrian Industries (IV), and Martin Urschler acknowledges funding by the Austrian
Science Fund (FWF): P 28078-N33.

The authors thank Professor David Epstein for his extensive help with the wording
and mathematical notation in this paper, and Nicholas Trahearn for his help with the
wording.

A The Complete Contest Results

A summary of the ranking results from the contest is given in Figure 3.

Figure 3: The ranking results from the GlaS Challenge Contest.

References

Dogan Altunbay, Celal Cigir, Cenk Sokmensuer, and Cigdem Gunduz-Demir. Color
graphs for automated cancer diagnosis and grading. Biomedical Engineering, IEEE

20

MethodRankSumF1 ScoreObject DiceObject HausdorffPart APart BPart APart BPart APart BCUMedVision 213151617ExB 144426121ExB 322265522Freiburg 2a55533324CUMedVision 161818428ExB 236372829Freiburg 1a88644232CVIP Dundeeb777871046CVML10911911757LIB9179129965vision4GlaS11101014101166LIST13111411141477Ching-Wei Wang 1c12121513161684Bioimage Informatics16151710181288Ching-Wei Wang 2c14131615171792SUTECH15181318131592ISI Kolkatta18161819121396FIMM191912171519101Ching-Wei Wang 3c171419161918103a Image Analysis Lab Uni Freiburg:  Freiburg 2 = post-processing, Freiburg 1 = rawb CVIP Dundee: feature level fusion c Ching-Wei Wang: Ching-Wei Wang 1 = no preprocess ﬁll hole, Ching-Wei Wang 2 = no preprocess hole, Ching-Wei Wang 3 = preprocess ﬁll holeTransactions on, 57(3):665–674, 2010.

Ajay Nagesh Basavanhally, Shridar Ganesan, Shannon Agner, James Peter Monaco,
Michael D Feldman, John E Tomaszewski, Gyan Bhanot, and Anant Madabhushi.
Computerized image-based detection and grading of
lymphocytic inﬁltration in
HER2+ breast cancer histopathology. Biomedical Engineering, IEEE Transactions
on, 57(3):642–653, 2010.

Bassem Ben Cheikh, Philippe Bertheau, and Daniel Racoceanu. A structure-based
approach for colon gland segmentation in digital pathology. In SPIE Medical Imaging.
International Society for Optics and Photonics, 2016.

Fred T Bosman, Fatima Carneiro, Ralph H Hruban, Neil D Theise, et al. WHO classiﬁ-
cation of tumours of the digestive system. Number Ed. 4. World Health Organization,
2010.

Xavier Bresson, Selim Esedolu, Pierre Vandergheynst, Jean-Philippe Thiran, and Stan-
ley Osher. Fast global minimization of the active contour/snake model. Journal of
Mathematical Imaging and Vision, 28(2):151–167, 2007.

Hao Chen, Qi Dou, Dong Ni, Jie-Zhi Cheng, Jing Qin, Shengli Li, and Pheng-Ann
Heng. Automatic fetal ultrasound standard plane detection using knowledge trans-
ferred recurrent neural networks. In Medical Image Computing and Computer-Assisted
Intervention–MICCAI 2015, pages 507–514. Springer, 2015.

Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L
Yuille. Semantic image segmentation with deep convolutional nets and fully connected
CRFs. arXiv preprint arXiv:1412.7062, 2014.

Assaf Cohen, Ehud Rivlin, Ilan Shimshoni, and Edmond Sabo. Memory based active
contour algorithm using pixel-level classiﬁed images for colon crypt segmentation.
Computerized Medical Imaging and Graphics, 43:150–164, 2015.

Carolyn C Compton. Updated protocol for the examination of specimens from patients
with carcinomas of the colon and rectum, excluding carcinoid tumors, lymphomas,
sarcomas, and tumors of the vermiform appendix: a basis for checklists. Archives of
Pathology & Laboratory Medicine, 124(7):1016–1025, 2000.

Lee R Dice. Measures of the amount of ecologic association between species. Ecology,

26(3):297–302, 1945.

Matthew Fleming, Sreelakshmi Ravula, Sergei F Tatishchev, and Hanlin L Wang. Col-
orectal carcinoma: Pathologic aspects. Journal of Gastrointestinal Oncology, 3(3):
153, 2012.

Wolf Herman Fridman, Franck Pag`es, Catherine Saut`es-Fridman, and J´erˆome Galon.
impact on clinical outcome. Nature

The immune contexture in human tumours:
Reviews Cancer, 12(4):298–306, 2012.

Hao Fu, Guoping Qiu, Jie Shu, and M. Ilyas. A novel polar space random ﬁeld model
for the detection of glandular structures. Medical Imaging, IEEE Transactions on, 33
(3):764–776, March 2014. ISSN 0278-0062. doi: 10.1109/TMI.2013.2296572.

21

J´erˆome Galon, Anne Costes, Fatima Sanchez-Cabo, Amos Kirilovsky, Bernhard Mlecnik,
Christine Lagorce-Pag`es, Marie Tosolini, Matthieu Camus, Anne Berger, Philippe
Wind, et al. Type, density, and location of immune cells within human colorectal
tumors predict clinical outcome. Science, 313(5795):1960–1964, 2006.

Peter R Gibson, Robert P Anderson, John M Mariadason, and Andrew J Wilson. Pro-
tective role of the epithelium of the small intestine and colon. Inﬂammatory bowel
diseases, 2(4):279–302, 1996.

Tunc Gultekin, Can Fahrettin Koyuncu, Cenk Sokmensuer, and Cigdem Gunduz-Demir.
Two-tier tissue decomposition for histopathological image representation and classiﬁ-
cation. Medical Imaging, IEEE Transactions on, 34(1):275–283, 2015.

Cigdem Gunduz-Demir, Melih Kandemir, Akif Burak Tosun, and Cenk Sokmensuer.
Automatic segmentation of colon glands using object-graphs. Medical Image Analysis,
14(1):1–12, 2010.

Lawrence Hubert and Phipps Arabie. Comparing partitions. Journal of Classiﬁcation,

2(1):193–218, 1985.

Adam Humphries and Nicholas A Wright. Colonic crypt organization and tumorigenesis.

Nature Reviews Cancer, 8(6):415–424, 2008.

Kourosh Jafari-Khouzani and Hamid Soltanian-Zadeh. Multiwavelet grading of patho-
logical images of prostate. Biomedical Engineering, IEEE Transactions on, 50(6):
697–704, 2003.

Viren Jain, Benjamin Bollmann, Mark Richardson, Daniel R Berger, Moritz N Helm-
staedter, Kevin L Briggman, Winfried Denk, Jared B Bowden, John M Mendenhall,
Wickliﬀe C Abraham, et al. Boundary learning by optimization with topological
constraints. In Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Con-
ference on, pages 2488–2495. IEEE, 2010.

Shantanu H Joshi, Eric Klassen, Anuj Srivastava, and Ian Jermyn. Removing shape-
preserving transformations in square-root elastic (SRE) framework for shape analysis
of curves. In Energy Minimization Methods in Computer Vision and Pattern Recog-
nition, pages 387–398. Springer, 2007.

Philipp Kainz, Michael Pfeiﬀer, and Martin Urschler. Semantic segmentation of colon
glands with deep convolutional neural networks and total variation segmentation.
arXiv preprint arXiv:1511.06919, 2015.

Alex Krizhevsky, Ilya Sutskever, and Geoﬀrey E Hinton. Imagenet classiﬁcation with
deep convolutional neural networks. In Advances in Neural Information Processing
Systems, pages 1097–1105, 2012.

Yann LeCun, Koray Kavukcuoglu, and Cl´ement Farabet. Convolutional networks and
applications in vision. In Circuits and Systems (ISCAS), Proceedings of 2010 IEEE
International Symposium on, pages 253–256. IEEE, 2010.

Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for
semantic segmentation. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 3431–3440, 2015.

22

Maurice B Loughrey, Philip Quirke, and Neil A Shepherd. Dataset for colorectal cancer

histopathology reports, 3ed. London: Royal College of Pathologists, 2014.

Jiri Matas, Ondrej Chum, Martin Urban, and Tom´as Pajdla. Robust wide-baseline
stereo from maximally stable extremal regions. Image and Vision Computing, 22(10):
761–767, 2004.

Bojana Mitrovic, David F Schaeﬀer, Robert H Riddell, and Richard Kirsch. Tumor
budding in colorectal carcinoma: time to take notice. Modern Pathology, 25(10):
1315–1325, 2012.

Engin Ozdemir and Cigdem Gunduz-Demir. A hybrid classiﬁcation model for digital
pathology using structural and statistical pattern recognition. Medical Imaging, IEEE
Transactions on, 32(2):474–483, 2013.

CH Richards, CSD Roxburgh, JH Anderson, RF McKee, AK Foulis, PG Horgan, and
DC McMillan. Prognostic value of tumour necrosis and host inﬂammatory responses
in colorectal cancer. British Journal of Surgery, 99(2):287–294, 2012.

Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks
for biomedical image segmentation. In Nassir Navab, Joachim Hornegger, WilliamM.
Wells, and AlejandroF. Frangi, editors, Medical Image Computing and Computer-
Assisted Intervention MICCAI 2015, volume 9351 of Lecture Notes in Computer
Science, pages 234–241. Springer International Publishing, 2015.
ISBN 978-3-319-
24573-7.

Raphael Rubin, David S Strayer, Emanuel Rubin, et al. Rubin’s pathology: clinicopatho-

logic foundations of medicine. Lippincott Williams & Wilkins, 2008.

Minalini Shanmugathasan and Serge Jothy. Apoptosis, anoikis and their relevance to

the pathobiology of colon cancer. Pathology International, 50(4):273–279, 2000.

K. Sirinukunwattana, D.R.J. Snead, and N.M. Rajpoot. A stochastic polygons model for
glandular structures in colon histology images. Medical Imaging, IEEE Transactions
on, 34(11):2366–2378, Nov 2015. ISSN 0278-0062. doi: 10.1109/TMI.2015.2433900.

Ali Tabesh, Mikhail Teverovskiy, Ho-Yuen Pang, Vinay P Kumar, David Verbel, Ange-
liki Kotsianti, and Olivier Saidi. Multifeature prostate cancer diagnosis and Gleason
grading of histological images. Medical Imaging, IEEE Transactions on, 26(10):1366–
1378, 2007.

Joke Tommelein, Laurine Verset, Tom Boterberg, Pieter Demetter, Marc Bracke, and
Olivier De Wever. Cancer-associated ﬁbroblasts connect metastasis-promoting com-
munication in colorectal cancer. Frontiers in Oncology, 5, 2015.

Mary Kay Washington, Jordan Berlin, Philip Branton, Lawrence J Burgart, David K
Carter, Patrick L Fitzgibbons, Kevin Halling, Wendy Frankel, John Jessup, Sanjay
Kakar, et al. Protocol for the examination of specimens from patients with primary
carcinoma of the colon and rectum. Archives of Pathology & Laboratory Medicine,
133(10):1539, 2009.

H-S Wu, R Xu, N Harpaz, D Burstein, and J Gil. Segmentation of intestinal gland
images with iterative region growing. Journal of Microscopy, 220(3):190–204, 2005a.

23

Hai-Shan Wu, Ruliang Xu, Noam Harpaz, David Burstein, and Joan Gil. Segmentation
of microscopic images of small intestinal glands with directional 2-d ﬁlters. Analytical
and Quantitative Cytology and Histology/the International Academy of Cytology and
American Society of Cytology, 27(5):291–300, 2005b.

Laurent Younes. Computable elastic distances between shapes. SIAM Journal on Ap-

plied Mathematics, 58(2):565–586, 1998.

Yan Zhang, Bogdan J Matuszewski, Lik-Kwan Shark, and Christopher J Moore. Medical
image segmentation using new hybrid level-set method. In BioMedical Visualization,
2008. MEDIVIS’08. Fifth International Conference, pages 71–76. IEEE, 2008.

24

