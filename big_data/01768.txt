Semantic Style Transfer and Turning Two-Bit Doodles into Fine Artwork

6
1
0
2

 
r
a

M
5

 

 
 
]

V
C
.
s
c
[
 
 

1
v
8
6
7
1
0

.

3
0
6
1
:
v
i
X
r
a

Alex J. Champandard
nucl.ai Research Laboratory

alexjc@nucl.ai

Abstract

Convolutional neural networks (CNNs) have proven
highly effective at image synthesis and style transfer.
For most users, however, using them as tools can be
a challenging task due to their unpredictable behavior
that goes against common intuitions. This paper intro-
duces a novel concept to augment such generative archi-
tectures with semantic annotations, either by manually
authoring pixel labels or using existing solutions for se-
mantic segmentation. The result is a content-aware gen-
erative algorithm that offers meaningful control over the
outcome. Thus, we increase the quality of images gen-
erated by avoiding common glitches, make the results
look signiﬁcantly more plausible, and extend the func-
tional range of these algorithms—whether for portraits
or landscapes, etc. Applications include semantic style
transfer and turning doodles with few colors into mas-
terful paintings!

Introduction

to CNNs

Image processing algorithms have improved dramati-
cally thanks
trained on image classiﬁca-
tion problems to extract underlying patterns from large
datasets (Simonyan and Zisserman 2014).
As a result,
deep convolution layers in these networks provide a more
expressive feature space compared to raw pixel layers,
which proves useful not only for classiﬁcation but also
generation (Mahendran and Vedaldi 2014).
For transfer-
ring style between two images in particular, results are
astonishing—especially with painterly, sketch or abstract
styles (Gatys, Ecker, and Bethge 2015).

However, to achieve good results using neural style trans-
fer in practice today, users must pay particular attention to
the composition and/or style image selection, or risk see-
ing unpredictable and incorrect patterns. For portraits, facial
features can be ruined by incursions of background colors
or clothing texture, and for landscapes pieces of vegetation
may be found in the sky or other incoherent places. There’s
certainly a place for this kind of glitch art, but many users
become discouraged not being able to get results they want.
Through our social media bot that ﬁrst provided these
algorithms as a service (Champandard 2015), we observe
that users have clear expectations how style transfer should

∗This research was funded out of the marketing budget.

nucl.ai Conference 2016∗

Artiﬁcial Intelligence in Creative Industries

July 18-20, Vienna/Austria.

http://events.nucl.ai/

Figure 1: Synthesizing paintings with deep neural networks
via analogy. (a) Original painting by Renoir, (b) semantic
annotations, (c) desired layout, (d) generated output.

occur: most often this matches semantic labels, e.g. hair
style and skin tones should transfer respectively regardless
of color. Unfortunately, while CNNs routinely extract se-
mantic information during classiﬁcation, such information
is poorly exploited by generative algorithms—as evidenced
by frequent glitches.

We attribute these problems to two underlying causes:

1. While CNNs used for classiﬁcation can be re-purposed to
extract style features (e.g. textures, grain, strokes), they
were not architected or trained for correct synthesis.

2. Higher-level layers contain the most meaningful infor-
mation, but this is not exploited by the lower-level lay-
ers used in generative architectures: only error back-
propagation indirectly connects layers from top to bottom.

To remedy this, we introduce an architecture that bridges
the gap between generative algorithms and pixel labeling
neural networks. The architecture commonly used for im-
age synthesis (Simonyan and Zisserman 2014) is augmented
with semantic information that can be used during gener-
ation. Then we explain how existing algorithms can be
adapted to include such annotations, and ﬁnally we show-

Figure 2: Comparison and breakdown of synthesized portraits, chosen because of extreme color and feature mismatches.
Parameters were adjusted to make the style transfer most faithful while reducing artifacts such as patch repetition or odd
blends—which proved challenging for the second column, but more straightforward in the last column thanks to semantic
annotations. The top row shows transfer of painted style onto a photo (easier), and the bottom turning the painting into a photo
(harder); see area around the nose and mouth for failures. [Original painting by Mia Bergeron.]

case some applications in style transfer as well as image
synthesis by analogy (e.g. Figure 1).

Related Work

The image analogy algorithm (Hertzmann et al. 2001) is
able to transfer artistic style using pixel features and their
local neighborhoods. While more recent algorithms using
deep neural networks generate better quality results from a
stylistic perspective, this technique allows users to synthe-
size new images based on simple annotations. As for recent
work on style transfer, it can be split into two categories:
specialized algorithms or more general neural approaches.

The ﬁrst neural network approach to style transfer
is gram-based (Gatys, Ecker, and Bethge 2015), using so-
called “Gram Matrices” to represent global statistics about
the image based on output from convolution layers. These
statistics are computed by taking the inner product of in-
termediate activations—a tensor operation that results in a
N × N matrix for each layer of N channels. During this
operation, all local information about pixels is lost, and
only correlations between the different channel activations
remain. When glitches occur, it’s most often due to these
global statistics being imposed onto the target image regard-
less of its own statistical distribution, and without any un-
derstanding of local pixel context.

A more recent alternative involves a patch-based ap-
proach (Li and Wand 2016), which also operates on the out-
put of convolution layers. For layers of N channels, neural
patches of 3 × 3 are matched between the style and content

image using a nearest neighbor calculation. Operating on
patches in such a way gives the algorithm local understand-
ing of the patterns in the image, which overall improves the
precision of the style transfer since fewer errors are intro-
duced by globally enforcing statistical distributions.

Both gram- and patch-based approaches struggle to pro-
vide reliable user controls to help address glitches. The pri-
mary parameter exposed is a weighting factor between style
and content; adjusting this results in either an abstract-styled
mashup that mostly ignores the input content image, or the
content appears clearly but its texture looks washed out (see
Figure 2, second column). Finding a compromise where
content is replicated precisely and the style is faithful re-
mains a challenge—in particular because the algorithm lacks
semantic understanding of the input.

Thankfully, recent CNN architectures are capable of pro-
viding such semantic context, typically by performing pixel
labeling and segmentation (Thoma 2016). These models
rely primarily on convolutional layers to extract high-level
patterns, then use deconvolution to label the individual pix-
els. However, such insights are not yet used for synthesis—
despite beneﬁts shown by non-neural approaches.

The state-of-the-art specialized approaches to style trans-
fer exploit semantic information to great effect, performing
color transfer on photo portraits using speciﬁcally crafted
image segmentation (Yang et al. 2015). In particular, facial
features are extracted to create masks for the image, then
masked segments are processed independently and colors
can be transferred between each corresponding part (e.g.

Figure 3: Our augmented CNN that uses regular ﬁlters of N channels (top), concatenated with a semantic map of M=1 channel
(bottom) either output from another network capable of labeling pixels or as manual annotations.

background, clothes, mouth, eyes, etc.) Thanks to the addi-
tional semantic information, even simpler histogram match-
ing algorithms may be used to transfer colors successfully.

result is a new output with N + M channels, denoted sl and
labeled accordingly for each layer (e.g. sem4 1).

Before concatenation, the semantic channels are weighted
by parameter γ to provide an additional user control point:

Model
on

builds

contribution

Our
ap-
proach (Li and Wand 2016) to style transfer, using op-
timization to minimize content reconstruction error Ec
(weighted by α) and style remapping error Es (weight β).
See (Gatys, Ecker, and Bethge 2015) for details about Ec.

patch-based

a

E = αEc + βEs

(1)

First we introduce an augmented CNN (Figure 6) that in-
corporates semantic information, then we deﬁne the input
semantic map and its representation, and ﬁnally show how
the algorithm is able to exploit this additional information.

Architecture
The most commonly used CNNs for image synthesis is
VGG (2014), which combines pooling and convolution lay-
ers l with 3 × 3 ﬁlters (e.g.
the ﬁrst layer after third
pool is named conv4 1). Intermediate post-activation re-
sults are labeled xl and consist of N channels, which cap-
ture patterns from the images for each region of the im-
age: grain, colors, texture, strokes, etc. Other architec-
tures tend to skip pixels regularly, compress data, or op-
timized for classiﬁcation—resulting in low-quality synthe-
sis (Nikulin and Novak 2016).

Our augmented network concatenates additional semantic
channels ml of size M at the same resolution, computed by
down-sampling a static semantic map speciﬁed as input. The

l

s

l = x

lkγm

(2)
For style images, the activations for the input image and
s. For the
its semantic map are concatenated together as sl
output image, the current activations xl and the input con-
tent’s semantic map are concatenated as sl. Note that the
semantic part of this vector is, therefore, static during the
optimization process (implemented using L-BFGS).

This architecture allows specifying manually authored se-
mantic maps, which proves to be a very convenient tool for
user control—addressing the unpredictability of current gen-
erative algorithms. It also lets us transparently integrate re-
cent pixel labeling CNNs (Thoma 2016), and leverage any
advances in this ﬁeld to apply them to image synthesis.

Representation
The input semantic map can contain an arbitrary number of
channels M . Whether doing image synthesis or style trans-
fer, there are only two requirements:
• Each image has its own semantic map of the same aspect
ratio, though it can be lower resolution (e.g. 4x smaller)
since it’ll be downsampled anyway.

• The semantic maps may use an arbitrary number of chan-
nels and representation, as long as they are consistent for
the current style and content (so M must be the same).
Common representations include single greyscale chan-
nels or RGB+A colors—both of which are very easy to au-
thor. The semantic map can also be a collection of layer

mask per label as output by existing CNNs, or even some
kind of “semantic embedding” that compactly describes im-
age pixels (i.e. the representation for hair, beards, and eye-
brows in portraits would be in close proximity).

Algorithm
Patches of k × k are extracted from the semantic layers and
denoted by the function Ψ, respectively Ψ(s
s) for the in-
put style patches and Ψ(sl) for the current image patches.
For any patch i in the current image and layer l, its near-
est neighbor NN(i) is computed using normalized cross-
correlation—taking into account weighted semantic map:

l

NN(i) := arg min

j

Ψi(s) · Ψj(ss)

|Ψi(s)| · |Ψj(ss)|

(3)

The style error Es between all the patches i of layer l in
the current image to the closest style patch is deﬁned as the
sum of the Euclidean distances:

Es(s, ss) = X

||Ψi(s) − ΨNN(i)(ss)||2

(4)

i

Note that the information from the semantic map in ml is
used to compute the best matching patches and contributes
to the loss value, but is not part of the derivative of the loss
relative to the current pixels; only the differences in activa-
tion x
l compared to the style patches cause an adjustment of
the image itself via the L-BFGS algorithm.

By using an augmented CNN that’s compatible with the
original, existing patch-based implementations can use the
additional semantic information without changes. If the se-
mantic map and m
l is zero, the original algorithm (2016)
is intact. In fact, the introduction of the γ parameter from
Equation 2 provides a convenient way to introduce semantic
style transfer incrementally.

Experiments

The following experiments were generated from VGG19
network using augmented layers sem3 1 and sem4 1, with
3 × 3 patches and no additional rotated or scaled versions
of the style images. The semantic maps used were man-
ually edited as RGB images, thus channels are in the range
[0..255]. The seed for the optimization was random, and
rendering completed in multiple increasing resolutions—as
usual for patch-based approaches (Li and Wand 2016). On a
GTX970 with 4Gb of GPU RAM, rendering takes from 3 to
8 minutes depending on quality and resolution.

Precise Control via Annotations
Transferring style in faces is arguably the most challenging
task to meet expectations—and particularly if the colors in
the corresponding segments of the image are opposed. Typ-
ical results from our solution are shown in portraits from
Figure 2, which contains both success cases (top row) and
sub-optimal results (bottom row). The input images were
chosen once upfront and not curated to showcase represen-
tative results; the only iteration was in using the semantic
map as a tool to improve the quality of the output.

Figure 4: Examples of semantic style transfer with Van
Gogh painting. Annotations for nose and mouth are not re-
quired as the images are similar, however carefully anno-
tating the eyeballs helps when generating photo-quality por-
traits. [Photo by Seth Johnson, concept by Kyle McDonald.]

In the portraits, the semantic map includes four main la-
bels for background, clothing, skin and hair—with minor
color variations for the eyes, mouth, nose and ears. (The se-
mantic maps in this paper are shown as greyscale, but con-
tain three channels.)

In practice, using semantic maps as annotations helps al-
leviate issues with patch- or gram-based style transfer. Of-
ten, repeating patches appear when setting style weight β too
high (Figure 2, second row). When style weight is low, pat-

terns are not transferred but lightly blended (Figure 2, ﬁrst
row). The semantics map prevents these issues by allowing
the style weight to vary relative to the content without suffer-
ing from such artifacts; note in particular that the skin tones
and background colors are transferred more faithfully.

Parameter Ranges
Given a ﬁxed weight for the content loss α = 10, the style
loss β for images in this paper ranges from 25 to 250 de-
pending on image pairs. Figure 5 shows a grid with visual-
izations of results as β and γ vary; we note the following:

• The quality and variety of the style degenerates as γ in-
creases too far, without noticeably improving the preci-
sion wrt. annotations.

• As γ decreases, the algorithm reverts to its semantically
unaware version that ignores the annotations provided,
but also indirectly causes an increase in style weight.

• The default value of γ is chosen to equalize the value
range of the semantic channels ml and convolution ac-
tivations x

l, in this case γ = 50.

• Lowering γ from its default allows style to be reused
across semantic borders, which may be useful for certain
applications if used carefully.

In general, with the recommended default value of γ, ad-
justing style weight β now allows meaningful interpolation
that does not degenerate into abstract patchworks.

Analysis

Here we report observations from working with the algo-
rithm, and provide our interpretations.

Semantic Map Values Since the semantic channels ml
are integrated into the same patch-based calculation, it af-
fects how the normalized cross-correlation takes place. If
the channel range is large, the values from convolution xl
will be scaled very differently depending on the location in
the map. This may be desired, but in most cases it seems
sensible to make sure values in ml have similar magnitude.

Authored Representations We noticed that when users
are asked to annotate images, after a bit of experience with
the system, they implicitly create “semantic embeddings”
that compactly describe pixel classes. For example, the rep-
resentation of a stubble would be a blend between hair and
skin, jewelry is similar but not identical to clothing, etc.
Such representations seem better suited to semantic style
transfer than plain layer masks.

Content Accuracy vs. Style Quality When using seman-
tic maps, only the style patches from the appropriate seg-
ment can be used for the target image. When the number
of source patches is small, this causes repetitive patterns, as
witnessed in parts of Figure 2. This can be addressed by
loosening the style constraint and lowering γ, at the cost of
precision.

Figure 5: Varying parameters for the style transfer. First col-
umn shows changes in style weight β: 0) content reconstruc-
tion, 10 to 50) artifact-free blends thanks to semantic con-
straint, 250) best style quality. Second column shows values
of semantic weight γ: 0) style overpowers content without
semantic constraint, 10) low semantic weight strengthens in-
ﬂuence of style, 50) default value that equalizes channels,
250) high semantic weight lowers quality of style.

Figure 6: Deep image analogy for a Monet painting based on a doodle; it’s effectively semantic style transfer with no content
loss. This result was achieved in only eight attempts, showcasing the potential for the algorithm as an interactive tool.

Blending Segments
In the examples shown and others,
the algorithm does a great job of painting the borders be-
tween image segments, often using appropriate styles from
the corresponding image. Smoothing the semantic map can
help in some cases, however, crisp edges still generate sur-
prising results.
Weight Sensitivity The algorithm is less fragile to adjust-
ments in style weight; typically as the weight increases, the
image degenerates and becomes a patchwork of the style
content. The semantic map helps maintain the results more
consistent for a wider range of the parameter space.
Performance Due to the additional channels in the model,
our algorithm requires more memory as well as extra com-
putation compared to its predecessor. When using only RGB
images this is acceptable: around 1% extra memory for all
patches and all convolution output, approximately 5% extra
computation. However with pixels labeled using individ-
ual classes this grows quickly. This is a concern, although

patch-based solutions in general would beneﬁt from signiﬁ-
cant optimization that would apply here too.

Conclusion

Existing style transfer techniques perform well when colors
and/or accuracy don’t matter too much for the output im-
age (painterly, abstract or sketch styles, glitch art) or when
both image patterns are already similar—which obviously
reduces the appeal and applicability of such algorithms. In
this paper, we resolved these issues by annotating input im-
ages with a semantic map, either manually authored or from
pixel labeling algorithms. We introduced an augmented
CNN architecture to leverage this information at runtime,
while further tying advances in image segmentation to image
synthesis. We showed that existing patch-based algorithms
require minor adjustments and perform very well using this
additional information.

The examples shown for style transfer show how

this technique helps deal with completely opposite pat-
terns/colors in corresponding image regions, and we ana-
lyzed how it helps users control the output of these algo-
rithms better. Reducing the unpredictability of neural net-
works certainly is a step forward towards making them more
useful as a tool to enhance creativity and productivity.

Acknowledgments

This work was possible thanks to the nucl.ai Conference
2016’s marketing budget that was repurposed for this re-
search. As long as you visit us in Vienna on July 18-20,
it’s better this way! http://nucl.ai

Thanks to Petra Champandard-Pail, Roelof Pieters,
Sander Dieleman, Jamie Blondin, Samim Winiger, Timmy
Gilbert, Kyle McDonald, Mia Bergeron, Seth Johnson.

References

2015.
Deep
famous artists.

Paint photos in the style of

[Champandard 2015] Champandard, A.
forger:
https://deepforger.com/.
[Gatys, Ecker, and Bethge 2015] Gatys, L. A.; Ecker, A. S.;
and Bethge, M. 2015. A neural algorithm of artistic style.
CoRR abs/1508.06576.
[Hertzmann et al. 2001] Hertzmann, A.; Jacobs, C.; Oliver,
N.; Curless, B.; and Salesin, D. 2001.
Image analogies.
SIGGRAPH Conference Proceedings.
[Li and Wand 2016] Li, C., and Wand, M. 2016. Combining
markov random ﬁelds and convolutional neural networks for
image synthesis. abs/1601.04589.
[Mahendran and Vedaldi 2014] Mahendran, A., and Vedaldi,
A. 2014. Understanding deep image representations by in-
verting them. CoRR abs/1412.0035.
[Nikulin and Novak 2016] Nikulin, Y., and Novak, R. 2016.
Exploring the neural algorithm of artistic style. CoRR
abs/1602.07188.
[Simonyan and Zisserman 2014] Simonyan, K., and Zisser-
man, A. 2014. Very deep convolutional networks for large-
scale image recognition. CoRR abs/1409.1556.
[Thoma 2016] Thoma, M. 2016. A survey of semantic seg-
mentation. CoRR abs/1602.06541.
[Yang et al. 2015] Yang, Y.; Zhao, H.; You, L.; Tu, R.; Wu,
X.; and Jin, X. 2015. Semantic portrait color transfer with
internet images. Multimedia Tools and Applications 1–19.

