6
1
0
2

 
r
a

M
4

 

 
 
]
S
D
.
s
c
[
 
 

1
v
3
8
5
1
0

.

3
0
6
1
:
v
i
X
r
a

Randomized algorithms for ﬁnding a majority element

Pawe(cid:32)l Gawrychowski1, Jukka Suomela2, and Przemys(cid:32)law Uzna´nski3

1Institute of Informatics, University of Warsaw, Poland

2Helsinki Institute for Information Technology HIIT, Department of Computer Science,

3Department of Computer Science, ETH Z¨urich, Switzerland

Aalto University, Finland

Abstract

Given n colored balls, we want to detect if more than (cid:98)n/2(cid:99) of them have the same color, and if so ﬁnd
one ball with such majority color. We are only allowed to choose two balls and compare their colors, and
the goal is to minimize the total number of such operations. A well-known exercise is to show how to ﬁnd
such a ball with only 2n comparisons while using only a logarithmic number of bits for bookkeeping. The
resulting algorithm is called the Boyer–Moore majority vote algorithm. It is known that any deterministic
method needs (cid:100)3n/2(cid:101) − 2 comparisons in the worst case, and this is tight. However, it is not clear what is
the required number of comparisons if we allow randomization. We construct a randomized algorithm
which always correctly ﬁnds a ball of the majority color (or detects that there is none) using, with high
probability, only 7n/6 + o(n) comparisons. We also prove that the expected number of comparisons used
by any such randomized method is at least 1.038n.

1

Introduction

A classic exercise in undergraduate algorithms courses is to construct a linear-time constant-space algorithm
for ﬁnding the majority in a sequence of n numbers a1, a2, . . . , an, that is, a number x such that more than
(cid:98)n/2(cid:99) numbers ai are equal to x, or detect that there is no such x. The solution is to sweep the sequence
from left to right while maintaining a candidate and a counter. Whenever the next number is the same as the
candidate, we increase the counter; otherwise we decrease the counter and, if it drops down to zero, set the
candidate to be the next number. It is not diﬃcult to see that if the majority exists, then it is equal to the
candidate after the whole sweep, therefore we only need to count how many times the candidate occurs in
the sequence. This simple yet beautiful solution was ﬁrst discovered by Boyer and Moore in 1980; see [3] for
the history of the problem. Even though this might seem like a toy problem, it has some serious motivation
for voting in hardware systems.

The only operation on the input numbers used by the Boyer–Moore algorithm is testing two numbers for
equality, and furthermore at most 2n such checks are ever being made. This suggests that the natural way
to think about the algorithm is that the input consists of n colored balls and the only possible operation is
comparing the colors of any two balls. Now the obvious question is how many such comparisons are necessary
and suﬃcient in the worst possible case. Fischer and Salzberg [10] proved that the answer is (cid:100)3n/2(cid:101) − 2.
Their algorithm is a clever modiﬁcation of the original Boyer–Moore algorithm that reuses the results of some
previously made comparisons during the veriﬁcation phase. They also show that no better solution exists by
an adversary-based argument. However, this argument assumes that the strategy is deterministic, so the next
step is to allow randomization.

Surprisingly, not much seems to be known about randomized algorithms for computing the majority in
the general case. For the special case of only two colors, Christoﬁdes [4] gives a randomized algorithm that
uses 2

3 )n comparisons in expectation and returns the correct answer with probability 1 − , and he

3 (1 − 

1

also proves that this is essentially tight; this improves on a previous lower bound of Ω(n) by De Marco and
Pelc [11]. Note that in the two-color case any deterministic algorithm needs precisely n − B(n) comparisons,
where B(n) is the number of 1s in the binary expansion of n, and this is tight [1, 13, 14]. For a random input,
with each ball declared to be red or blue uniformly at random, roughly 2n/3 comparisons are suﬃcient and
necessary in expectation to ﬁnd the majority color [2].

However, much less is known about randomized algorithms for the general case of multiple colors. Chung
et al. [5] studies oblivious algorithms, that is, algorithms in which subsequent comparisons do not depend on
the previous answers, but to our knowledge upper and lower bounds on the expected number of comparisons
for adaptive algorithms without any restrictions on the number of colors have not been studied before.

While it might seem that minimizing the number of comparisons is a purely academic exercise, it is feasible
that a single comparison is so expensive that their number is the bottleneck in the algorithm. Such line of
thought motivated a large body of work studying the related questions of the smallest number of comparisons
required to ﬁnd the median element; see [7, 8, 12] and the references therein. Given the original motivation in
hardware systems, it is natural to consider Las Vegas algorithms, that is, the number of comparisons depends
on the random choices of the algorithm but the answer is always correct. This way the result will be correct
even if the source of random bits is compromised; an adversary that is able to control the random number
generator can only inﬂuence the running time.

Model. We identify balls with numbers 1, 2, . . . , n. We write cmp(i, j) for the result of comparing the
colors of balls i and j (true for equality, false for inequality). We consider randomized algorithm that, after
performing a number of such comparisons, either ﬁnds a ball of the majority color or detects that there is
no such color. A majority color is a color with the property that more than (cid:98)n/2(cid:99) balls are of such color.
The algorithm should always be correct, irrespectively of the random choices made during the execution.
However, the colors of the balls are assumed to be ﬁxed in advance, and therefore the number of comparisons
is a random variable. We are interested in minimizing its expectation.

Contributions. We construct a randomized algorithm, which always correctly determines a ball of the
majority color or detects that there is none, using 7n/6+o(n) comparisons with high probability (in particular,
in expectation). We also show that the expected number of comparisons used by any such algorithm must be
at least 1.038n. Therefore, randomization allows us to circumvent the lower bound of Fischer and Salzberg
and construct a substantially better algorithm.

2 Preliminaries
We denote the set of balls (items) by M = {1, 2, . . . , n}. We write color(x) for the color of ball x, and
cmp(x, y) returns true if the colors of balls x and y are identical.
An event occurs with very high probability (w.v.h.p.) if it happens with probability at least 1 −
exp(−Ω(log2 n)). Observe that the intersection of polynomially many very high probability events also
happens with very high probability.

√
2 + O(

n log n).

Lemma 2.1 (symmetric Chernoﬀ bound). The number of successes for n independent coin ﬂips is w.v.h.p.
at most n
Lemma 2.2 (sampling). Let X ⊆ M such that |X| = m. Let m(cid:48) denote the number of hits on elements
from X if we sample uniformly at random k ≤ n elements from M without replacement. Then w.v.h.p.
|m(cid:48)/k − m/n| = O(k−1/2 log n).
Proof. Let δ = Θ(k−1/2 log n): From the tail bound for hypergeometric distribution (see Chv´atal [6]), we
have

(cid:17) ≤ 2 exp(−2δ2k) = exp(−Θ(log2 n)).

(cid:16)(cid:12)(cid:12)(cid:12) m(cid:48)

k

Pr

(cid:12)(cid:12)(cid:12) ≥ δ

− m
n

Worth noting is that the error bound here would be essentially the same if we replace “without replacement”

with “with replacement”. In that case, we would be invoking a tail bound for binomial distribution.

2

(cid:12)(cid:12)(cid:12)(cid:12)uXX − |X|2

(cid:12)(cid:12)(cid:12)(cid:12) = O(cid:0)(cid:112)|X| log n(cid:1).

Now we consider a process of pairing the items without replacement (choosing a random perfect matching
on M ; if n is odd then one item remains unpaired). For any X ⊆ M , let uXX be a random variable counting
the pairs with both elements belonging to X when choosing uniformly at random n
2 pairs of elements from
M without replacement. Of course E[uXX ] =
Lemma 2.3 (concentration for pairs). For any X ⊆ M w.v.h.p.

|X|(|X|−1)

2(n−1)

.

Proof. For simplicity, we can assume that n is even. Set ∆ = Θ((cid:112)|X| log n). Instead of choosing a random

2n

perfect matching on M , we consider an equivalent two-step random process:

1. We choose uniformly at random a partition of M into M1, M2 such that |M1| = |M2| = n

partition induces a partition of X into X1, X2, such that X1 = M1 ∩ X and X2 = M2 ∩ X.

Observe that, by Lemma 2.2, |X1|,|X2| ∈(cid:2) 1

2|X| + ∆(cid:3) w.v.h.p.

2|X| − ∆, 1

2 . This

2. Now, instead of pairing uniformly at random the elements from M1 with the elements from M2, for our
2 ⊆ M2 of elements that are paired with

purposes it is enough to choose uniformly at random a set M(cid:48)
X1, and count how many elements of X2 we have chosen. Thus uXX = |M(cid:48)

2 ∩ X2|.

By Lemma 2.2 we have(cid:12)(cid:12)(cid:12)(cid:12)|M(cid:48)

2 ∩ X2|
|X2|
so the following holds (since |M(cid:48)

Hence

− |M(cid:48)
2|
|M2|

2| = |X1|) w.v.h.p.:

(cid:12)(cid:12)(cid:12)(cid:12) = O(|X2|−1/2 log |M2|) w.v.h.p.,
(cid:12)(cid:12)(cid:12)(cid:12) = O((cid:112)|X2| log |M2|).
(cid:12)(cid:12)(cid:12)(cid:12)uXX − |X1||X2|
(cid:12)(cid:12)(cid:12)(cid:12) = O((cid:112)|X2| log |M2|) + O
(cid:18) ∆2
(cid:19)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) = O(n2/3 log n).
uXX − (cid:88)

|X|2
2n

n/2

n/2

X∈F

= O(∆).

2n

(cid:12)(cid:12)(cid:12)(cid:12)uXX − |X|2
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:88)

X∈F

Lemma 2.4 (pairs in partition). Let F = {X1, . . . , Xm} be a partition of M . Then w.v.h.p.

Proof. Let F(cid:48) be a partition {X(cid:48)
m(cid:48)} obtained from F by merging all sets Xi such that |Xi| < n2/3
into larger sets of size between n2/3 and 2n2/3 (this can be done in a greedy fashion). Let A be the family of
original large sets, B the family of original small sets, and C the family of new larger sets.

1, . . . , X(cid:48)

Since B is a ﬁner partition than C, we have that

Since all sets in C are smaller than 2n2/3, we obtain

(cid:88)

X∈B

uXX ≤ (cid:88)
(cid:88)

X∈C

|X|2
2n

X∈C

≤

uXX ,

(cid:18)(cid:88)

X∈C

(cid:88)

X∈B

|X|2
2n

≤ (cid:88)

X∈C

|X|2
2n

.

n−1/3 ≤ n2/3.

(cid:19)

|X|

3

Then, because all sets in C are large, by a a direct application of Lemma 2.3, w.v.h.p.

Similarly, because all sets in F(cid:48) are large, by an application of Lemma 2.3, w.v.h.p.

(cid:88)

X∈C

2n

X∈C

(cid:18)|X|2
uXX ≤ (cid:88)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:88)

X∈F(cid:48)

uXX − (cid:88)

X∈F(cid:48)

uXX

X∈C

X∈B

uXX −(cid:88)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:88)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:88)
uXX − (cid:88)

X∈F

X∈F

|X|2
2n

X∈F(cid:48)

(cid:88)

X∈C

|X|2
2n

(cid:112)|X| ≤ O(n2/3 log n).

≤ n2/3 + O(log n)

O(cid:0)(cid:112)|X| log n(cid:1) = O(n2/3 log n).

(cid:19)
+ O((cid:112)|X| log n)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≤ (cid:88)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:88)
uXX − (cid:88)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:88)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) +
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) .
uXX − (cid:88)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≤ O(n2/3 log n) + O(n2/3 log n) + O(n2/3) = O(n2/3 log n).

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) +

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:88)

−(cid:88)

|X|2
2n

|X|2
2n

|X|2
2n

|X|2
2n

X∈F(cid:48)

X∈F(cid:48)

X∈F

X∈F

X∈B

X∈C

Now our goal is to bound

which, because |a + b| ≤ |a| + |b|, is at most

The ﬁrst and the third addend can be bounded by O(n2/3 log n) because if 0 ≤ a ≤ b then |b − a| ≤ b, hence
by plugging in the previously derived bounds we obtain

Lemma 2.5. Let X ⊆ M such that |X| = m. Let k(m(cid:48), m, n) denote the number of draws without replacement
until we hit m(cid:48) elements from X. Then w.v.h.p.

m(cid:48) + O(cid:16) n√

(cid:17)

· log n

.

m

k(m(cid:48), m, n) ≤ n
m

Proof. Denoting by S the number hits on X when drawing without replacement k items, then by Lemma 2.2
w.v.h.p.

for some constant C > 0. Substituting k = n

S ≥ m
n

k − C · m1/2 log n

m m(cid:48) + C · n√

m log n we obtain

meaning that indeed w.v.h.p. after k draws we will have at least m(cid:48) elements from X.

S ≥ m(cid:48) + C · m1/2 log n − C · m1/2 log n = m(cid:48)

3 Algorithm

In this section we describe a randomized algorithm for ﬁnding majority. Recall that the algorithm is required
to always either correctly determine a ball of the majority color or decide that there is no such color, and the
majority color is a color of more than (cid:98)n/2(cid:99) balls. For simplicity we will assume for the time being that n is
even, as the algorithm can be adjusted for odd n in a straightforward manner without any change to the
asymptotic cost. Hence to prove that there is a majority color, it is suﬃcient to ﬁnd n/2 + 1 balls with the
same color. In such case our algorithm will actually calculate the multiplicity of the majority color. To prove
that there is no majority color, it is suﬃcient to partition the input into n/2 pairs of balls with diﬀerent
colors.

4

The algorithm consists of three parts. Intuitively, by choosing a small random sample we can approximate
the color frequencies and choose the right strategy: (i) There is one color with a large frequency. We will use
algorithm heavy. In essence, we have only one candidate for the majority, and we compute the frequency of
the candidate in a naive manner. If the frequency is too small, we need to form suﬃciently many pairs of
balls with diﬀerent colors. This can be done by reusing the results of the previous comparisons. (ii) There
are two colors with frequencies close to 0.5. Now we will use algorithm balanced. In essence, we can now
reduce the size of the input by a pairing process, and then ﬁnd majority recursively. (iii) All frequencies are
small. In this case we will use light which, again, resorts to a recursive strategy.

We start with presenting the main procedure of the algorithm; see Algorithm 1. The parameters are
In fact we could chose any β ∈ (β1, β2), where

3 , ε = n−1/10 and β = 0.45.

≈ 0.4226 and β2 ≈ 0.47580 is a root to p3 − 19p2 − 8p + 8 = 0.

chosen by setting α = 1
β1 = 1 − 1√

3

|M(cid:48)| = nα

Algorithm 1: majority(M )
1 if |M| = 1 then return M [1] is the majority with multiplicity 1 in M sample M(cid:48) ⊆ M such that
2 let v1, v2, . . . , vk be the representatives of the colors in M(cid:48)
3 let qi|M(cid:48)| be the frequency of color(vi) in M(cid:48), where q1 ≥ q2 ≥ . . . ≥ qk
2 − 4ε, 1
4 if q1, q2 ∈ [ 1
6 else if q1 ≥ β and q2

2 + 4ε] then
return balanced(M )
1 ≥ q2
return heavy(M, v1)

2 + . . . + q2

k + 2ε then

5

7
8 else
9

return light(M )

Before we proceed to describe the subprocedures, we elaborate on the sampling performed in line 3.
Intuitively, we would like to compute the frequencies of all colors in M . This would be too expensive, so
we select a small sample M(cid:48) and claim that the frequencies of all colors in M(cid:48) are not too far from the
frequencies of all colors in M . Formally, let p1, p2, p3, . . . , p(cid:96) be the frequencies of all colors in M , that is
there are pi · n balls of color i in M and let qi be the frequency of color i in the sample M(cid:48). By Lemma 2.2,

w.v.h.p. |pi − qi| = O(n−α/2 log n) = o(ε). We argue that(cid:80)

i is a good estimation of(cid:80)

i p2
i .

i q2

p2

q2
i

i

i

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:88)

i −(cid:88)

1. Partition the elements of M into n

Proof. Let m = nα. We analyze the following two sampling methods.

Lemma 3.1. Let pi be the frequency of color i in M and qi be its frequency in M(cid:48), where M(cid:48) ⊆ M a random
sample without replacement of size nα. Then w.v.h.p.

2 of these pairs uniformly
at random. Denote by A1 and A2 the pairs with both elements of the same colors in the ﬁrst and the

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) = O(n−α/3 log n) = o(ε).
(cid:12)(cid:12) = O(n2/3 log n). Observe that
second pairing, respectively. By Lemma 2.4, w.v.h.p.(cid:12)(cid:12)|A1| − n
(cid:80)
by Lemma 2.2 w.v.h.p.(cid:12)(cid:12)|A2| − m
n |A1|(cid:12)(cid:12) = O(m1/2 log n). Thus, by the triangle inequality, w.v.h.p.
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) |A2|
−(cid:88)
(cid:12)(cid:12)(cid:12) = O(m−1/3 log n).

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) = O(n−1/3 log n) + O(m−1/2 log n).
with both elements of the same color. By Lemma 2.4, w.v.h.p.(cid:12)(cid:12)|B| − m
(cid:80)

(cid:12)(cid:12) = O(m2/3 log n), or

2 disjoint pairs uniformly at random, and denote by B all pairs

2. Partition the elements of M(cid:48) into m

2 disjoint pairs uniformly at random. Select m

(cid:12)(cid:12)(cid:12) |B|
m/2 −(cid:80)

equivalently

i q2
i

i p2
i

2

i q2
i

2

p2
i

i

m/2

5

Now, because A2 and B have identical distributions, by the triangle inequality we have

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) = O(n−1/3 log n) + O(m−1/2 log n) + O(m−1/3 log n) = O(m−1/3 log n).

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:88)

i

i −(cid:88)

p2

q2
i

i

Now we present the subprocedures; see Algorithms 2–4.

Algorithm 2: balanced(M )
1 randomly shuﬄe M
2 X ← [], Y ← []
3 for i = 1 to |M|/2 do

if cmp(M [2i − 1], M [2i]) then

4

5

6

else

append M [2i] to X
append M [2i − 1] and M [2i] to Y

multiplicity k in X

7
8 run majority(X)
9 if there is no majority in X then return no majority in M let color(v) be the majority with
10 cnt ← 2k
11 for i = 1 to |Y |/2 do
cnt ← cnt + 1
cnt ← cnt + 1
16 if cnt ≤ |M|/2 then

if cmp(v, Y [2i − 1]) then

else if cmp(v, Y [2i]) then

15

12

13

14

return no majority in M

17
18 else
19

return color(v) is the majority with multiplicity k in X

Lemma 3.2. Algorithm 1 always returns the correct answer.

Proof. We analyze separately every subprocedure.

balanced(M ).

If the majority exists then removing two elements with diﬀerent colors preserves it.
Hence if the recursive call returns that there is no majority in X then indeed there is no majority in M ,
and otherwise color(v) is the only possible candidate for the majority in M . The remaining part of the
subprocedure simply veriﬁes it.

heavy(M, v). The subprocedure ﬁrst checks if color(v) is the majority. Hence it is enough to analyze
what happens if color(v) is not the majority. Then X contains all elements with other colors. We partition
the elements in X into pairs and check which of these pairs consists of elements with diﬀerent colors. If the
number of elements in all the remaining pairs is smaller than the number of elements of color color(v), then
clearly we can partition all elements in M into disjoint pairs of elements with diﬀerent colors, hence indeed
there is no majority. Otherwise, we revert to the simple 2n algorithm, which is always correct.

light(M ). Again, if the majority exists then removing two elements with diﬀerent color preserves it.
Hence we can assume that color(v) is the only possible candidate for the majority. Then, Y consists of pairs
of two elements with diﬀerent colors. From the recursive call we also know what is the frequency of color(v)
in M \ Y . We iterate through the elements of Y and check if their color is color(v). However, if the color of
the ﬁrst element in a pair is color(v), then the second element has a diﬀerent color. So the subprocedure

6

Algorithm 3: heavy(M, v)
1 cnt ← 0, X ← []
2 for i = 1 to |M| do
cnt ← cnt + 1

if cmp(v, M [i]) then

3

4

5

else

6

append M [i] to X

7 if cnt > |M|/2 then return color(v) is the majority with multiplicity k in M k ← |M|/2 − cnt
8 randomly shuﬄe X
9 for i = 1 to |X|/2 do

if ¬cmp(X[2i − 1], X[2i]) then

10

11

k ← k − 1

if k = 0 then return no majority in M

12
13 return Boyer–Moore(M)

(cid:46) fallback, 2n comparisons

Algorithm 4: light(M )
1 randomly shuﬄe M
2 X ← [], Y ← []
3 for i = 1 to |M|/2 do

if cmp(M [2i − 1], M [2i]) then

else

append M [2i] to X
append M [2i − 1] and M [2i] to Y

4

5

6

12

13

14

7
8 run majority(X)
9 if there is no majority in X then return no majority in M let color(v) be the majority with
multiplicity k in X
10 cnt ← 2k − |X|
11 for i = 1 to |Y | do

if ¬cmp(v, Y [2i − 1]) then
if ¬cmp(v, Y [2i]) then

cnt ← cnt − 1

15

if cnt = 0 then return no majority in M

16 return color(v) is the majority with multiplicity (|M|/2 + cnt) in M

either correctly determines the frequency of the majority color(v), or ﬁnd suﬃciently many elements with
diﬀerent colors to conclude that color(v) is not the majority.

Theorem 3.3. Algorithm 1 w.v.h.p. uses at most 7
number of comparisons is also at most 7
6 n + o(n).

6 n + o(n) comparisons on an input of size n. The expected

6 n + C · n9/10 for a ﬁxed constant C that is suﬃciently large. In the analysis

Proof. Let T (n) be a random variable counting the comparisons on the given input of size n. We will
inductively prove that T (n) ≤ 7
we will repeatedly invoke Lemmas 2.2, 2.3, 2.4, 2.5, 3.1 and Chernoﬀ bound to bound diﬀerent quantities. We
will assume that each such the application succeeds. Since there will be a polynomial number of applications,
each on a polynomial number of elements, this happens w.v.h.p. with respect to the size of the input. We
also assume that n is large enough. Algorithm 1 uses at most O(n2α) = O(n2/3) comparisons in the sampling
stage. We bound the number of subsequent comparisons used by each subprocedure as follows.

balanced(M ). We have that p1, p2 = 1
i ) ± O(n2/3 log n), thus |X| = ( 1

|X| = ( n

i p2

2

4 ± O(ε))n. Also |Y | = n − 2|X| = ( 1

2 ± O(ε))n.

i p2

i = 1

2 ± O(ε). By Lemma 2.4,

(cid:80)

2 ± O(ε). Thus also (cid:80)

7

List Y consists of pairs of elements with diﬀerent colors. Because at most O(εn) of all elements are not
of color 1 or 2, there are at most O(εn) pairs not of the form {1, 2}. Since the relative order of elements
Y [2i − 1] and Y [2i] is random, for each pair {1, 2} we pay 1 with probability 1/2 and pay 2 with probability
1/2, and for any other pair we pay always 2. Thus the total cost incurred by the loop in line 11 is (by Chernoﬀ
bound) at most

Thus the total cost is

O(εn) · 2 +

3
2

T (n) ≤ T(cid:0)( 1

4 + ε)n(cid:1) + 1

and 7

6 n + O(n9/10) + C · ( 1

3 )9/10 · n9/10 ≤ 7

|Y |/2 + O((cid:112)|Y | log n) ≤ 3

n ± O(εn).

8 n + O(εn) ≤ 7

2 n + 3
6 n + C · n9/10 for a large enough C.

8

6 n + O(n9/10) + C ·(cid:0) 1
1 and(cid:80)

3 n(cid:1)9/10

1 −(cid:80)

If p1 > 1

2 ]. Because by Lemmas 2.2 and 3.1 both p2

2 , then we terminate in line 7 after n comparisons. Thus we can assume that
i are estimated within an absolute error

heavy(M, v).
p1 ∈ [0.45 − ε, 1
of o(ε), we have that p2
We argue that the loop in line 9 will eventually ﬁnd suﬃciently many pairs of elements with diﬀerent
colors, and thus return without falling back to the 2n algorithm. By deﬁnition, |X| = (1 − p1)n and initially
k = (1/2 − p1)n. By Lemma 2.4, after the random shuﬄe the number of pairs of elements (X[2i − 1], X[2i])
with diﬀerent colors, denoted by D, can be bounded by

i ≥ 2ε − 2o(ε) ≥ ε.

i≥2 p2

i p2

n − o(εn) ≥(cid:0) 1

2 − p1

(cid:1)n;

n +

ε
2

(cid:80)

D ≥ |X|
−
≥ 1 − p1

2

2

j≥2(pjn)2
2|X|
n − p2

1 − ε
2(1 − p1)

− O(cid:0)|X|2/3 log |X|(cid:1) ≥
n − o(εn) ≥ 1 − 2p1
2(1 − p1)
(cid:19)

(cid:19)

(cid:18) |X|√
(cid:114) ε
(cid:18)(cid:114) n

D

n log n

(cid:19)

≤

(cid:19)

log n

ε

(cid:18)

(cid:18) 1

|X|
D
2
(1 − p1)2

≤ n +

n + O

n/

2

3
≤ (1 + 0.552/2)n + O(εn) + O

thus indeed there are suﬃciently many pairs. Hence, because the pairs are being considered in a random
order, the total cost can be bounded using Lemma 2.5 by

T (n) ≤ n +

− p1

n + O

log |X|

≤

= 1.15125n + O(n9/10),

(cid:80)

where we used D ≥ ε

2 − o(n) ≥ ε

3 n for a large enough n.

light(M ). We start with bounding the sizes of X and Y . By Lemma 2.4, |X| = n

Also, by Lemma 2.3 there are n
O(n1/2 log n) of elements from A1 in Y (each paired with an element not from A1).

1 ± O(n1/2 log n) elements from A1 in X, thus there are n(p1 − p2
i ) ≤ ε. If there is no majority in X, then p1 ≤ 1

We know that either p1 ≤ 0.45 + ε or p2

1 −(cid:80)

2 p2

i≥2(p2

i ±O(n2/3 log n).
1) ±

i p2

2 and

2

the total cost is bounded by

T (n) ≤ n
2

+ T (|X|) ≤ n
2
4 +O(n2/3 log n), is less than 19

|X| + C · |X|9/10,

+

7
6

24 n + o(n). Hence we can assume that there is a majority

which, because |X| ≤ n
in X. In such case, cnt is set to

(cid:18)

1 −(cid:88)

p2

i≥2

c =

n
2

(cid:19)

8

± O(n2/3 log n).

p2
i

We denote by I the total number of iterations of the loop in line 11. By Lemma 2.5

where E = |Y |/

Thus E ≤(cid:112) n

1

2|Y |

1

I ≤

i≥2 p2

i , by Lemma 2.4 we have

2|Y | − |A1 ∩ Y | · c + O(E),
(cid:113) 1
2|Y | − |A1 ∩ Y |. Substituting S =(cid:80)
|Y | =(cid:0)1 − p2
1 − S ± O(n−1/3 log n)(cid:1)n,
|Y | − 2|A1 ∩ Y | =(cid:0)(1 − p1)2 − S ± O(n−1/3 log n)(cid:1)n,
(cid:0)p2
1 − S ± O(n−1/3 log n)(cid:1)n.
2 − 3ε(cid:1)2 − (3ε)2 − o(ε) = 3ε − 18ε2 − o(ε) ≥ 2ε.

(1 − p1)2 − S − o(ε) ≥(cid:0) 1

(cid:1)2 −(cid:0) 1

c =

1
2

2

2ε . Now, since |Y | = Θ(n) we can bound I from above by
I ≤

|Y |

2

|Y | − 2|A1 ∩ Y | · 1
1 − p2
1 − S + O(n−1/3 log n)
≤ 1
(1 − p1)2 − S − O(n−1/3 log n)
2
≤ 1
2

(1 − p1)2 − S − O(n−1/3 log n)

1 − p2

1 − S

(cid:19)
1 − S)n + O(1/ε) · O(n2/3 log n) + O(E) ≤
(p2

(cid:18) n2/3 log n

1 − S)n + O
(p2

+ O

(cid:18)(cid:114) n

(cid:19)

4ε

≤

ε

O(n2/3 log n)

1 − S)n +
(p2

+ O(n23/30 log n),

Since p1 ≤ 1
subprocedure to be used), we have

2 and p2 ≤ 1

2 − 3ε (as for a larger p2 the sampled q2 would be suﬃciently large for other

2ε
which, because (1 − p1)2 − S is suﬃciently large, can be bounded by
O(n−1/3 log n)
(1 − p1)2 − S
O(n−1/3 log n)

1 − S)n ·
(p2

(cid:19)
(cid:19)

(cid:18)
(cid:18)

1 +

1 − p2
1 − S
(1 − p1)2 − S
1 − p2
1 − S
(1 − p1)2 − S
1 − p2
1 − S
(1 − p1)2 − S
(1 − p2
1 − S)

I ≤ 1
2
≤ 1
2
≤ 1
2
1
2

=

1 − S)n ·
(p2

1 − S)n +
(p2
1 − S
p2

(1 − p1)2 − S

+ O(n23/30 log n) ≤

+ O(n23/30 log n) ≤

1 +
2ε
O(n2/3 log n)

8ε2

n + O(n26/30 log n).

+ O(n23/30 log n) ≤

For each of c iterations we pay 2, and for each of the remaining I−c iterations we pay only 3

2 in expectation

(for each iteration independently). Thus, by Chernoﬀ bound the total cost is

T (n) ≤ 1
2

n + T (|X|) +

3
2

√
(I − c) + O(

I − c log(I − c)) + 2c ≤

(cid:18)
(cid:18)

≤ 1
2

=

=

n
2

n
2

n +

7
12

n(p2

1 + S) +

3
4

(1 − p2

1 +

7
6

(p2

1 + S) +

3
2

1 − S)
(p2

1 − S)
1 − p2

1 − S
p2

1
4

n +

(1 − p1)2 − S
1 − S − ((1 − p1)2 − S)

(1 − p1)2 − S

1 +

19
6

1 − 5
p2
6

S + 3(p2

1 − S)

p1 − p2

1

(1 − p1)2 − S

(cid:19)

+ O(n9/10).

9

1 − S)n + O(n26/30 log n) + C · |X|9/10 =
(p2

(cid:19)
1 − S)
(p2

+

3
2

1 − S) +
(p2

1
2

+ O(n9/10) =

We reason that, for a ﬁxed p1, the quantity
1 − 5
p2
6

19
6

1 +

S + 3(p2

1 − S)

p1(1 − p1)
(1 − p1)2 − S

is a decreasing function of S, since p2
above estimation either with p2

1 ≤ (1 − p1)2. Now we consider two cases. If p2
1 − S ≤ 0, or, since (1 − p1)2 − S ≥ 2ε, with 0 ≤ p2

1 − S ≤ ε then simplifying
1−S
(1−p1)2−S ≤ 1
2 , we get either

(cid:18)

(cid:19)

T (n) ≤ n
2

1 +

7
3

p2
1

+ O(n9/10),

which, since p1 ≤ 1

2 , is bounded from above by 19

(cid:18)

24 n + o(n), or
p1 − 3
2

p2
1 +

3
2

p2
1

(cid:19)

+ O(n9/10),

T (n) ≤ n
2

1 +

7
3

which is bounded from above by 47

48 n + o(n).

Otherwise, p1 ≤ 0.45 + ε and substituting S = 0 (since the cost is decreasing in S) we obtain

(cid:18)

(cid:19)

T (n) ≤ n
2

1 +

19
6

p2
1 + 3

p3
1
1 − p1

+ O(n9/10) = 1.06915n + O(n9/10).

Wrapping up. We see that in each subprocedure, the number of comparisons is bounded by 7

6 n+C·n9/10.
Each subprocedure makes at most one recursive call, where the size of the input is reduced by at least a
factor of 2. Thus the worst-case number of comparison is always bounded by O(n). Recall that the bound on
the number of comparisons used by every recursive call holds w.v.h.p. with respected to the size of the input
to the call. Eventually, the size of the input might become very small, and then w.v.h.p. with respect to the
size of the input is no longer w.v.h.p. with respect to the original n. However, as soon as this size decreases
to, say, n0.1, the number of comparisons is O(n) irrespectively of the random choices made by the algorithm.
6 n + O(n9/10), and the expected number of comparisons
Thus w.v.h.p. the number of comparisons is at most 7
is also bounded by 7

6 n + O(n9/10).

4 Lower bound

We consider only Las Vegas algorithms. That is, the algorithm must always correctly determine whether
a majority elements exists. We want to prove that the expected number of comparisons used by any such
algorithm is at least c · n − o(n), for some constant c > 1. By Yao’s principle, it is suﬃcient to construct
a distribution on the inputs, such that the expected number of comparisons used by any deterministic
algorithms run on an input chosen from the distribution is at least c · n − o(n). The distribution is that
with probability 1
n every ball has a color chosen uniformly at random from a set of n possible colors. With
probability 1 − 1
n every ball is either black or white, with both possibilities equally probable. We ﬁx a correct
deterministic algorithm A and analyze its behavior on an input chosen from the distribution. As a warm-up,
we will prove ﬁrst that A needs n − o(n) comparisons in expectation on such input.
4.1 A lower bound of n − o(n)
In every step A compares two balls, thus we can describe its current knowledge by deﬁning an appropriate
graph as follows. Every node corresponds to a ball. Two nodes are connected with a negative edge if the
corresponding balls have been compared and found out to have diﬀerent colors. Two nodes are connected
with a positive edge if the corresponding balls are known to have the same colors under the assumption that
every ball is either black or white (either because they have been directly compared and found to have the
same color, or because such knowledge has been indirectly inferred from the assumption). After every step of

10

the algorithm the graph consists of a number of components C1, C2, . . .. Every components is partitioned into
two parts Ci = Ai ·∪ Bi, such that both Ai and Bi are connected components in the graph containing only
positive edges and there is at least one (possibly more than one) negative edge between Ai and Bi. There are
no other edges in the graph. Now we describe how the graph changes after A compares two balls x ∈ Ci and
y ∈ Cj assuming that every ball is either black or white. If i = j then the result of the comparison is already
determined by the previous comparisons and the graph does not change. Otherwise, i (cid:54)= j and assume by
symmetry that x ∈ Ai, y ∈ Aj. The following two possibilities are equally probable:

1. color(x) = color(y), then we merge both components into a new component C = A ·∪ B, where
A = Ai ·∪ Aj and B = Bi ·∪ Bj by creating new positive edges (x, y) and (x(cid:48), y(cid:48)) for some x(cid:48) ∈ Bi, y(cid:48) ∈ Bj
(if Bi, Bj (cid:54)= ∅).

2. color(x) (cid:54)= color(y), then we merge both components into a new component C = A ·∪ B, where
A = Ai ·∪ Bj and B = Bi ·∪ Aj by creating new positive edges (x, y(cid:48)) for some y(cid:48) ∈ Bj (if Bj (cid:54)= ∅) and
(x(cid:48), y) for some x(cid:48) ∈ Bi (if Bi (cid:54)= ∅). We also create a new negative edge (x, y). Observe that here we
crucially use the assumption that every ball is either black or white.

The graph exactly captures the knowledge of A about a binary input.

Any binary input contains a majority and A must report so. However, because with very small probability

the input is arbitrary, this requires some work due to the following lemma.
Lemma 4.1. If A reports that a binary input contains a majority element, then the graph contains a
component C = A ·∪ B such that |A| > n
Proof. Assume otherwise, that is, A reports that a binary input contains a majority element even though
both parts of every component are of size less than n
2 . Construct another input by choosing, for every
component C = A ·∪ B, two fresh colors cA and cB and setting color(x) = cA for every x ∈ A, color(y) = cB
for every y ∈ B. Every comparison performed by A is an edge of the graph, so its behavior on the new input
is exactly the same as on the original binary input. Hence A reports that there is a majority element, while
the frequency of every color in the new input is less than n

2 or |B| > n
2 .

2 , which is a contradiction.

From now on we consider only binary inputs. If we can prove that the expected number of comparisons
used by A on such input is n − o(n), then the expected number of comparisons on an input chosen from
our distribution is also n − o(n). Because every comparison decreases the number of components by one,
it is suﬃcient to argue that the expected size of some component when A reports that there is a majority
is n − o(n). We already know that there must exists a component C = A ·∪ B such that (by symmetry)
2 . We will argue that |B| must also be large. To this end, deﬁne balance of a component Ci = Ai ·∪ Bi
|A| > n

as balance(Ci) = (|Ai| − |Bi|)2, and the total balance as(cid:80)

i balance(Ci).
Lemma 4.2. The expected total balance when A at any given step is n.
Proof. In the very beginning the total balance is n because every component is a singleton. Recall that
when A compares two balls x ∈ Ci and y ∈ Cj then Ci and Cj are merged into a new component C.
It is easy to verify that if balance(Ci) = b2
j then either balance(C) = (bi + bj)2 or
balance(C) = (bi − bj)2, with both possibilities equally probable. Hence the expected total balance after
the comparison is equal to the previous total balance increased by −b2
2 ((bi − bj)2 + (bi + bj)2) = 0.
Consequently, the expected total balance after any comparison is equal to the original total balance n.

i and balance(Cj) = b2

i − b2

j + 1

(cid:12)(cid:12)|A| − |B|(cid:12)(cid:12) ≤ n2/3 and |B| ≥ n

Total balance when A reports a majority is a random variable with expected value n. By Markov’s
inequality, with probability 1 − 1/n1/3 its value is at most n4/3, which implies that for any component
Ci = Ai ·∪ Bi, balance(Ci) ≤ n4/3, and there exists a component C = A ·∪ B such that |A| > n
2 , so
2 − n2/3. We obtain that with probability 1 − 1/n1/3 there is a component
containing n − n2/3 nodes. Every comparison decreased the number of components by at most 1, and if there
is such a large component then the total number of components cannot exceed 1 + n − (n − n2/3) = n2/3 + 1,
which means that the algorithm must have performed n − n2/3 − 1 comparisons. This makes the expected
number of comparisons at least (1 − 1/n1/3)(n − n2/3 − 1) = n − o(n).

11

4.2 A stronger lower bound

To obtain a stronger lower bound we extend the deﬁnition of the graph capturing the current knowledge of
A. Now a positive edge can be veriﬁed or non-veriﬁed. A veriﬁed positive edge (x, y) is created only after
comparing two balls x and y such that color(x) = color(y). All other positive edges are non-veriﬁed. The
algorithm can also turn a non-veriﬁed positive edge (x, y) into a veriﬁed positive edge by comparing x and y.
By the same reasoning as in Lemma 4.1 we obtain the following.
Lemma 4.3. If A reports that a binary input contains a majority element, then the graph consisting of all
veriﬁed positive edges contains a connected component on at least n

2 nodes.

Now that the goal is to construct a large component in the graph consisting of all veriﬁed positive edges,
it also make sense for A to compare two balls from the same component. However, without loss of generality,
such comparisons are executed after having identiﬁed a large component in the graph consisting of all positive
edges. Then, A asks suﬃciently many queries of the form (x, y), where (x, y) is a non-veriﬁed edge from the
identiﬁed component. In other words, A ﬁrst isolates a candidate for a majority, and then makes sure that
all inferred equalities really hold, which is necessary because with very small probability the input is not
binary. This allows us to bound the total number of comparisons from below as follow.
Lemma 4.4. The expected number of comparisons used by A on a binary input is at least n − o(n) plus the
expected number of non-veriﬁed edges between nodes of the majority color.
Proof. Recall that with probability 1 − 1/n1/3 when there exists a component C = A ·∪ B such that |A| > n
then |B| ≥ n
2 − n2/3. Set A consists of nodes of the majority color, although possibly not all nodes of the
majority color are there. However, because B is large, there are at most n2/3 nodes of the majority color
√
outside of A. Also, because we consider binary inputs chosen uniformly at random, by Chernoﬀ bound
|A| ≤ n
2 + O(
The expected number of comparisons used by A to construct a component C = A ·∪ B such that |A| > n
is n − n2/3 − 1. Then, A needs to verify suﬃciently many non-veriﬁed edges inside A to obtain a connected
component of size n
2 in the graph consisting of veriﬁed positive edges. By construction, there are no cycles
in the graph consisting of positive edges, hence with probability 1 − 1
n there will be no more than
n log n) non-veriﬁed positive edges between nodes outside of B when A reports a majority.
n2/3 + O(
Consequently, the additional veriﬁcation cost is the expected number of non-veriﬁed edges between nodes of
the majority color minus n2/3 + O(

n log n) with probability 1 − 1
n .

n1/3 − 1

n log n) = o(n).

2

2

√

√

In the remaining part of this section we analyze the expected number of non-veriﬁed edges between nodes
of the majority color constructed during the execution of the algorithm. The goal is to show that this is at
least (c − 1)n − o(n). Then, Lemma 4.4 implies the claimed lower bound.
A component C = A ·∪ B is called monochromatic when A = ∅ or B = ∅ (by symmetry, we will assume the
latter) and dichromatic otherwise. With probability 1 − 1/n1/3, when A reports a majority there is one large
dichromatic component on at least n − n2/3 nodes, so the total number of components is at most n2/3 + 1.
It is convenient to think about execution of A as of a process of eliminating components by merging two
components into one. Each such merge might create a new non-veriﬁed edge. The cost of such non-veriﬁed
edge (x, y) is the probability that x and y are nodes of the majority color. We want to argue that because
all but n2/3 components will be eventually eliminated, the total cost of all created non-veriﬁed edges is
(c − 1)n − o(n).
We analyze in more detail the merging process in terms of mono- and dichromatic components. Let
predictk be the random variable denoting the probability that, after k steps of A, a node from the larger
part of a component is of the majority color. It is rather diﬃcult to calculate predictk exactly, so we will
actually use a rather crude upper bound instead. An important property of the upper bound will be that it
is nondecreasing in k. When A compares two balls x ∈ Ci and y ∈ Cj with i (cid:54)= j to obtain a new component
C = A ·∪ B there are the following cases.

1. Ci and Cj are monochromatic. Then with probability 1

2 the new component C is also monochromatic,

and with probability 1

2 it is dichromatic.

12

2. Ci is dichromatic and Cj is monochromatic. The new component is dichromatic and, with probability

1

2 (1 − predictk), contains a new non-veriﬁed edge.

3. Ci and Cj are dichromatic. Then with probability 1

2 we create a new non-veriﬁed edge inside both A

and B.

We analyze the expected total cost of all non-veriﬁed edges when only one component remains. When A
reports a majority up to n2/3 components might remain, but this changes only the lower order terms of the
bound.

(cid:80)2n/3

k=1

E(cid:2)min(cid:0) 1

2 (1 − predictk)(cid:1)(cid:3).

6 , 1

Lemma 4.5. The expected total cost of all non-veriﬁed edges when only one component remains is at least

Proof. We start with n components and need to eliminate all but at most one of them. To each component
we associate credit, 1
6 to each monochromatic one. The algorithm can collect the
credit from both of the components it merges, but it has to pay for credit of newly created one. Additionally
algorithm has to pay for any non-veriﬁed edge created by merging.

2 to each dichromatic and 1

In every step we have three possibilities:

1. Merge two monochromatic components into one. With probability 1

2 new component is dichromatic,
2 new component is monochromatic. Thus the expected amortized cost for this

and with probability 1
step is 0.

2. Merge a monochromatic components with a dichromatic component. Then the total number of

monochromatic components decreases by 1 and we add with probability at least 1
non-veriﬁed edge. The expected amortized cost for this step is 1

2 (1 − predictk) − 1
6 .

2 (1 − predictk) a

3. Merge two dichromatic components while adding with probability 1

2 a non-veriﬁed edge. The expected

amortized cost for this step is 0.

In total algorithm has to pay for initial credits and for each step, making the total expected cost at least

n−1(cid:88)

k=1

n
6

+

E(cid:2)min(cid:0)0, 1

2 (1 − predictk) − 1

6

(cid:1)(cid:3) ≥

2/3n(cid:88)

k=1

E(cid:2)min(cid:0) 1

2 (1 − predictk)(cid:1)(cid:3).

6 , 1

We note that by truncating the sum at 2

estimation for predictk gives 1.

3 n we do not lose any cost estimation , as for k ≥ 2

3 n our

Now we focus on upper bounding the expression obtained in Lemma 4.5. To bound predictk we use an
approach due to Christoﬁdes [4]. At any given step k we will look at all the nonzero balances of the components.
Speciﬁcally, we introduce two new random variables: Mk being the largest balance of a component, and
Nk being the number of components with nonzero balance. Since at each step, number of nonzero balance
2 (k− 1), and w.v.h.p., by Chernoﬀ
components is decreased at most in expectation by 3
bound Nk ≥ n − 3
the sum, we have E[Mk] ≤ n − E[Nk − 1] = 3

Since by Lemma 4.2 expected sum of balances is n, and each nonzero component contributes at least 1 to
Now to proceed, for a component Ci = Ai ·∪ Bi we deﬁne δi = ||Ai| − |Bi||, a positive value such that
δ2
i = balance(Ci). Thus, at any given step k, algorithm observes nonzero values δ1, δ2, . . . , δNk . Without loss
of generality we can narrow our focus on a component C1. We are interested in bounding the probability

2 , we have E[Nk] ≥ n− 3

√
2 k − O(

2 k − 1
2 .

k log n).

Pr(A1 in majority) = Pr(δ1 + ε2δ2 . . . + εNk δNk ≥ 0) = 1

2 + 1

2 Pr(ε2δ2 . . . + εNk δNk ∈ [−δ1, δ1]),

where 2, 3, . . . , Nk ∈ {−1, 1} are drawn independently and uniformly at random. By a result of Erd˝os [9],
the above probability, when δ2, . . . , δNk ≥ 1 is maximized when δ2, . . . , δNk are all set to 1.

13

We now approximate binomial distribution using the symmetric case of de Moivre–Laplace Theorem.

Recall that

Φ(x) =

1√
2π

e−t2/2 dt

is the cumulative distribution function of the normal distribution.

(cid:90) x

−∞

Theorem 4.6 (De Moivre–Laplace). Let Sn be the number of successes in n independent coin ﬂips. Then

(cid:16) n

2

Pr

+ x1

√

n ≤ Sn ≤ n
2

+ x2

(cid:17) ∼ Φ(2x2) − Φ(2x1).
(cid:18)

(cid:19)

√

n

(cid:18)

In our case we are interested in Nk − 1 coin ﬂips and the number of successes in the range [(Nk − 1)/2 −

δ1/2, (Nk − 1)/2 + δ1/2]. Thus probability that 1 is the majority can be bounded from above by

Pr(A1 is the majority) ≤ Φ

δ1√
Nk − 1

− Φ

−

δ1√
Nk − 1

= 2Φ

δ1√
Nk − 1

− 1.

Because Mk is the largest balance of a component, δ1, δ2, . . . , δNk are bounded from above by
tionally, w.v.h.p. Nk ≥ n − 3

n log n), thus

√
2 k − O(

√

Mk. Addi-

(cid:18)

(cid:19)

(cid:32)(cid:115)

(cid:19)

(cid:33)

− 1.

(cid:33)

− 1.

(cid:32)(cid:115) 3

√

Since Φ(

predictk ≤ 2Φ
(cid:32)(cid:115)

n − 3

√
Mk
2 k − O(
(cid:33)

n log n)

x/const) is a concave function, we can apply expected value, and get

E[predictk] ≤ 2Φ

E[Mk]
√
2 k − O(

n − 3

n log n)

− 1 ∼ 2Φ

2 k
n − 3
2 k

Now we are ready to bound the sum from Lemma 4.5. Using the linearity of expectation and inequality
6 , 1

6 x for x ≥ 0 we obtain:

2 x) ≥ 1

min( 1

(cid:18) 1

2n/3(cid:88)
2n/3(cid:88)

k=1

E

≥

min

,

1
2

6

(1 − predictk)

(1 − E[predictk]) ≥ n ·

1
6

(cid:32)

(cid:90) 2/3

0

1
3

1 +

1 − Φ

2n/3(cid:88)
(cid:32)

k=1

(cid:19) =
(cid:90) 2/3
(cid:32)(cid:115) 3

0

1
3

2 x
1 − 3
2 x

1 − Φ

(cid:33)(cid:33)

(cid:20)

E

min

(cid:18) 1
(cid:32)(cid:115) 3

6

,

1
2

(cid:19)(cid:21)
(1 − predictk)
(cid:33)(cid:33)

≥

dx − o(n).

2 x
1 − 3
2 x

dx ≈ 1.0382578.

Finally, we calculate

k=1

Theorem 4.7. Any algorithm that reports majority exactly requires in expectation at least 1.038n comparisons.

Acknowledgments

Most of the work has been done while PU was aﬃliated to Aalto University, Finland.

14

References

[1] L. Alonso, E. M. Reingold, and R. Schott. Determining the majority. Inf. Process. Lett., 47(5):253–255,

1993.

[2] L. Alonso, E. M. Reingold, and R. Schott. The average-case complexity of determining the majority.

SIAM J. Comput., 26(1):1–14, 1997.

[3] R. S. Boyer and J. S. Moore. MJRTY: A fast majority vote algorithm. In Automated Reasoning: Essays

in Honor of Woody Bledsoe, pages 105–118, 1991.

[4] D. Christoﬁdes. On randomized algorithms for the majority problem. Discrete Applied Mathematics,

157(7):1481–1485, 2009.

[5] F. R. K. Chung, R. L. Graham, J. Mao, and A. C. Yao. Oblivious and adaptive strategies for the

majority and plurality problems. Algorithmica, 48(2):147–157, 2007.

[6] V. Chv´atal. The tail of the hypergeometric distribution. Discrete Mathematics, 25(3):285–287, 1979.

[7] D. Dor and U. Zwick. Selecting the median. SIAM J. Comput., 28(5):1722–1758, 1999.

[8] D. Dor and U. Zwick. Median selection requires (2 + )n comparisons. SIAM J. Discrete Math.,

14(3):312–325, 2001.

[9] P. Erd˝os. On a lemma of Littlewood and Oﬀord. Bull. Amer. Math. Soc., 51(12):898–902, 12 1945.

[10] M. Fischer and S. Salzberg. Finding a majority among n votes: solution to problem 81-5. Journal of

Algorithms, 1982.

[11] G. D. Marco and A. Pelc. Randomized algorithms for determining the majority on graphs. Combinatorics,

Probability & Computing, 15(6):823–834, 2006.

[12] M. Paterson. Progress in selection. In Algorithm Theory–SWAT’96, pages 368–379. Springer, 1996.

[13] M. E. Saks and M. Werman. On computing majority by comparisons. Combinatorica, 11(4):383–387,

1991.

[14] G. Wiener. Search for a majority element. Journal of Statistical Planning and Inference, 100(2):313–318,

2002.

15

