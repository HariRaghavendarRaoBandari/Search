6
1
0
2

 
r
a

 

M
6
1

 
 
]
L
M

.
t
a
t
s
[
 
 

1
v
0
6
0
5
0

.

3
0
6
1
:
v
i
X
r
a

Short-term time series prediction using Hilbert space embeddings of

autoregressive processes

Edgar A. Valencia♯, Mauricio A. ´Alvarez†

♯ Department of Mathematics, Universidad Tecnol´ogica de Pereira, Colombia, 660003.

† Faculty of Engineering, Universidad Tecnol´ogica de Pereira, Colombia, 660003.

Abstract

Linear autoregressive models serve as basic representations of discrete time stochastic processes. Different
attempts have been made to provide non-linear versions of the basic autoregressive process, including different
versions based on kernel methods. Motivated by the powerful framework of Hilbert space embeddings of
distributions, in this paper we apply this methodology for the kernel embedding of an autoregressive process
of order p. By doing so, we provide a non-linear version of an autoregressive process, that shows increased
performance over the linear model in highly complex time series. We use the method proposed for one-step
ahead forecasting of different time-series, and compare its performance against other non-linear methods.

1 Introduction

Autoregressive processes are useful probabilistic models for discrete time random processes. The basic idea in
an autoregressive process is that the random variable at time n, can be described as a linear combination of the p
past random variables associated to the process, plus white Gaussian noise. The value of p determines the order
of the autoregressive process [14].
Different authors have proposed non-linear extensions of the above model including NARMAX (non-linear au-
toregressive moving average model with exogenous inputs) [13], and also including the use of more general
non-linear regression methods for extending the classical autoregressive process to non-linear setups. Exam-
ples of non-linear regression methods used are neural networks [11], Gaussian processes [8], and kernel-based
learning methods [7].
Within the kernel methods literature, different versions for kernelizing an autoregressive process of order p have
been proposed [9, 7]. In [9], the authors propose an AR process built over a feature space. The coefﬁcients of
the autoregressive model are estimated by minimizing the quadratic error between the feature map of the input at
time n, and the prediction given by the linear combination of the last p mapped inputs. Predictions are presented
only for ﬁnite dimensional feature mappings, for which the inverse mapping from a feature space to the input
space is easily computed. In [7], the authors also propose an AR process built over a feature space, by this time,
the coefﬁcients of the autoregressive model are estimated by using Yule-Walker equations, where the correlations
between random variables are replaced by inner products between the feature maps of those random variables.
Predictions are obtained by solving a pre-image problem.
Our objective in this paper is to introduce a non-linear version of the autoregressive model of order p based on
Hilbert space embeddings of joint probability distributions.
Hilbert space embeddings are a recent trend in kernel methods that map distributions into inﬁnite-dimensional
feature spaces using kernels, such that comparisons and manipulations of these distributions can be performed
using standard feature space operations like inner products or projections [17]. Hilbert space embeddings have

1

been successfully used as alternatives to traditional parametric probabilistic models like hidden Markov models
[19] or linear dynamical systems [18]. They have also been used as non-parametric alternatives to statistical tests
[16].
Motivated by this powerful framework, we develop a kernelized version of an autoregressive model by means
of the Yule-Walker algorithm, and instead of computing correlations (as in the classical AR linear model) or
inner products (as in [7]), we compute cross-covariance operators for pairs of random variables. For time-
series prediction, one additionally needs to solve a pre-image problem [5], to map from the space of covariance
operators to the original input space. We develop an algorithm that uses ﬁxed point iterations for solving the
pre-image problem. The performance of the proposed model is compared against the linear AR model, the kernel
method proposed in [7], neural networks, and Gaussian processes, for one-step ahead forecasting in different
time series.
The paper is organized as follows. In section 2, we brieﬂy review Hilbert space embeddings methods. In section
3, we present the embedding of the AR model using cross-covariance operators, including parameter estimation,
and solving the pre-image problem. In section 4, we present some related work. In section 5 we describe the
experimental setup that includes four datasets, and in section 6, we show the results for one-step ahead prediction
over the different datasets. Conclusions appear in section 7.

2 Review of Hilbert space embeddings

In this paper, we use upper-case letters to refer to random variables (for example, X, Y ), and lower-case letters
to refer to particular values that those random variables can take (for example, x, y). Upper-case bold letters are
used to refer to matrices, and lower-case bold letters are used for vectors.
We brieﬂy review the deﬁnitions of a reproducible kernel Hilbert space (RKHS), Hilbert space embeddings of
distributions, and covariance operators, which are the key for developing Hilbert space embeddings of autore-
gressive processes.

2.1 Reproducing Kernel Hilbert Space

A reproducing kernel Hilbert space (RKHS) H with kernel k(x, x′), for x, x′ ∈ X , is a space of functions
g : X → R that satisfy the following properties:

1. For all x ∈ X , k(x, ·) : X → R belongs to H.

2. hg(·), k(x, ·)iH = g(x) and consequently hk(x, ·), k(y, ·)iH = k(x, y).

An alternative deﬁnition for a kernel function, which is usually used when designing algorithms, is given by
k(x, x′) = hφ(x), φ(x′)iH, where φ : X → H.
Kernel methods are widely popular in signal processing and machine learning, and there are several textbooks
where they are described in detail [12, 15, 2].

2.2 Embedding distributions

Recently, the authors in [16] introduced a method for embedding probability distributions in a RKHS. Let P be
the space of all probability distributions P on X . Let X be a random variable with distribution function P ∈ P.
In [16], the authors deﬁne the mapping from a probability distribution P ∈ P to a RKHS H using the mean map
µX deﬁned as

µX(P) = EX[k(X, ·)] = EX[φ(X)].

2

The mean map µX satisﬁes hµX , φ(·)iH = EX[φ(X)]. If the kernel k(x, x′) used for the embedding is charac-
teristic, 1 then µX is injective.
Given an i.i.d. set of observations {xl}m

1
m

k(xl, ·).

l=1 of the random variable X, an estimator forbµX is given as
mPm

bµX =
l=1 φ(xl). The estimatorbµX converges to µX, in the norm of H, at a

mXl=1

It can be shown that hbµX, φ(·)iH = 1

rate of Op(m−1/2) (see [16] for details).

2.3 Cross-covariance operator

If H1 and H2 are RKHS with kernels k(·, ·) and ℓ(·, ·), and feature maps φ and ϕ, respectively, the uncentered
cross-covariance operator is deﬁned as [1]

CXY = EXY [φ(X) ⊗ ϕ(Y )],

where ⊗ is the tensor product.2 The cross-covariance operator CXY can be seen as an element of a tensor product
reproducing kernel Hilbert space (TP-RKHS), H1 ⊗ H2.
Given two functions f ∈ H1 and g ∈ H2 then

hf, CXY giH1 = hf ⊗ g, CXY iH1⊗H2

= EXY [hf ⊗ g, φ(X) ⊗ ϕ(Y )iH1⊗H2]
= EXY [hf, φ(X)iH1 hg, ϕ(Y )iH2]
= EXY [f (X)g(Y )] ,

where φ(x) = k(x, ·), ϕ(y) = l(y, ·), and EXY [f (x)g(y)] is the covariance matrix (for details see [4]).
The operator CXY allows the embedding of the set of joint distributions P(X, Y ) in the TP-RKHS H1 ⊗ H2.
Given an i.i.d of set of pairs of observations DXY = {(x1, y1), (x2, y2), · · · , (xm, ym)}, a cross-covariance

estimator bCXY for CXY is deﬁned as:

1
m

mXl=1

φ(xl) ⊗ ϕ(yl) =

1
m

ΦΥ⊤,

(1)

bCXY =

where Φ = (φ(x1), φ(x2), . . . , φ(xm)), and Υ = (ϕ(y1), ϕ(y2), . . . , ϕ(ym)) are design matrices [2].

3 Hilbert space embedding of an autoregressive process

In this section, we describe how the basic autoregressive model can be embedded in a TP-RKHS. We then provide
an estimation method for the parameters of the embedded method, by means of the Yule-Walker equations.
Finally, we describe a procedure for solving the pre-image problem for the kernel embedding of the autoregressive
process. We solve the pre-image problem for forecasting in time-series.

1A characteristic kernel is a reproducing kernel for which µX (P) = µY (Q) ⇐⇒ P = Q, P, Q ∈ P, where P denotes the set of all

Borel probability measures on a topological space (M, A).

2Given f, h ∈ H1, and g ∈ H2, we deﬁne the tensor product f ⊗ g as an operator that maps h from H1 to H2 such that (f ⊗ g)h →

hh, f iH1 g.

3

3.1 Autoregressive models in TP-RKHS

Let X1, X2, · · · , Xn a stationary discrete time stochastic process. A p-order AR model is deﬁned by [14]

Xi = λ1Xi−1 + λ2Xi−2 + · · · + λpXi−p + ǫi =

λjXi−j + ǫi,

(2)

pXj=1

for i = p + 1, p + 2, · · · , n, where λ1, λ2, · · · , λp are the model parameters, and ǫi is white noise with E(ǫi) = 0
and var(ǫi) = σ2. We use λ = [λ1, λ2, . . . , λp]⊤.
The Yule-Walker equations are a set of linear of equations used to estimate the coefﬁcients λ. The basic idea is
to deﬁne a set of p linear equations, where the unknowns are the p coefﬁcients in λ. Each linear equation in the
Yule-Walker system is formed by computing the covariance between Xi, and Xi−k according to

hXi, Xi−ki =

pXj=1

λjhXi−j, Xi−ki + hǫi, Xi−ki,

for k = 1, . . . , p. Assuming independence between ǫi, and Xi−k, the set of linear equations reduce to

hXi, Xi−ki =

pXj=1

λjhXi−j, Xi−ki,

(3)

for k = 1, . . . , p. Given a set of observations for the discrete time random process, and a suitable estimator for
the covariance terms like hXi, Xi−ki, it is possible to solve the set of equations for estimating λ.
The authors in [7] propose a non-linear extension of the AR process in (3), by applying a non-linear transforma-
tion ϕ : X → H to the random variables Xi in the AR model,

ϕ(Xi) =

pXj=1

αjϕ(Xi−j) + ϕ(ǫi).

(4)

Notice that we use a set of coefﬁcients λ for the autoregressive model in X , and a set of coefﬁcients α =
[α1, . . . , αp]⊤ for the autoregressive model in H. To estimate the parameters α in the transformed space, the
authors follow a procedure similar to the Yule-Walker equations, but instead of computing covariances between
random variables like Xi, and Xi−k, they compute inner products between ϕ(Xi), and ϕ(Xi−k). With the proper
independence assumptions, the Yule-Walker system of equations in then given as 3

hϕ(Xi), ϕ(Xi−k)i =

pXj=1

αjhϕ(Xi−j ), ϕ(Xi−k)i,

(5)

for k = 1, . . . , p. Inner products like the ones above can be replaced by kernel functions. This is usually known as
the kernel trick [12, 15]. Given a set of observations for the discrete time random process {xi}m
i=1, the following
set of equations can be used to compute α,

k(xi, xi−k) =

pXj=1

αjk(xi−j, xi−k),

(6)

3For ease of exposition, we have assumed that the transformed random variables ϕ(Xi) have been substracted the mean of the

transformed variable µϕ = EXi [ϕ(Xi)].

4

for k = 1, . . . , p. Since the values for k(xi, xi−j), and k(xi−j, xi−k) are themselves random variables that
depend on the values of the observations in a particular time series, and assuming that the discrete time random
process is stationary, the authors in [7] propose the following set of equations to get an estimate for α

E[k(xi, xi−k)] =

pXj=1

αj E[k(xi−j, xi−k)],

(7)

for k = 1, . . . , p. Expectations are estimated over the set of available samples.
Our key contribution in this paper is that we embedd the autoregressive model in a TP-RKHS by mapping joint
distributions like P(Xi, Xi−k), and P(Xi−j, Xi−k) to points in H1 ⊗ H2. Embeddings are performed by using
cross-covariance operators, instead of inner products.
Let us start with Equation (4). If we apply a tensor product with φ(Xi−k), at both sides of Equation (4), and take
expected values, we obtain

EXi,Xi−k [ϕ(Xi) ⊗ φ(Xi−k)] =

αjEXi−j ,Xi−k [ϕ(Xi−j ) ⊗ φ(Xi−k)]

(8)

pXj=1

+ EǫiXi−k [ϕ(ǫi) ⊗ φ(Xi−k)],

for k = 1, · · · , p. If we assume that φ(Xi−k), and ϕ(ǫi) are uncorrelated, then the expression above reduces to

CXiXi−k =

pXj=1

αjCXi−j Xi−k ,

(9)

where CXiXi−k, and CXi−j Xi−k are cross-covariance operators, deﬁned as

CXiXi−k = EXi,Xi−k [ϕ(Xi) ⊗ φ(Xi−k)]

CXi−j Xi−k = EXi−j,Xi−k [ϕ(Xi−j ) ⊗ φ(Xi−k)].

3.2 Parameter estimation for autoregressive models in TP-RKHS

In this section, we provide a method for estimating the parameters α in the autoregressive model in Equation (9).
For this, we use the estimator for the cross-covariance operators, as in Equation (1).
Let DXiXi−j = {(x1
i.i.d from the distributions P(Xi, Xi−j). We denote by Φi the design matrix built from the elements {φ(xl
and Υi−j the design matrix built from the elements {ϕ(xl

i−j)}, for j = 1, 2, · · · , p, be different sets of samples drawn
l=1,

i−j), · · · , (xm

i−j), (x2

i , xm

i)}m

i , x1

i , x2

i−j )}m

l=1,

Φi = (φ(x1
Υi−j = (ϕ(x1

i ), φ(x2
i−j ), ϕ(x2

i ), · · · , φ(xm

i )),
i−j ), . . . , ϕ(xm

i−j )).

Estimators for the cross-covariance operators CXiXi−k and CXi−j Xi−k are given as (see Equation (9) and reference
[19])

bCXiXi−k =
bCXi−jXi−k =

1
m

1
m

mXl=1
mXl=1

φ(xl

i) ⊗ ϕ(xl

i−k) =

1
m

ΦiΥ⊤

i−k

ϕ(xl

i−j) ⊗ ϕ(xl

i−k) =

1
m

Υi−j Υ⊤

i−k.

5

(10)

(11)

Equation (9) can now be written approximately as

ΦiΥ⊤

i−k =

αj Υi−j Υ⊤

i−k.

We pre-multiply Equation (12) by Υ⊤

i−k, and post-multiply by Φi, obtaining

Υ⊤

i−k

ΦiΥ⊤

i−k

Φi =

Simplifying

We can write the expression above as

Υ⊤

i−k

Φi =

Hi−k,i =

αj Υ⊤

i−k

Υi−j Υ⊤

i−k

Φi.

αj Υ⊤

i−k

Υi−j.

αj Ki−k,i−j,

i−k

i−k, xs

i−k
i−k)⊤φ(xs

Φi, Ki−k,i−j = Υ⊤

where Hi−k,i = Υ⊤
Hi−k,i are the inner products {ϕ(xr
function {h(xr
which again can be computed using a kernel function {k(xr
Given a time-series dataset and a value for p, the values of Hi−k,i, and Ki−k,i depend on the values chosen for
i, and m. Assuming that the discrete time random process is stationary, we can get an estimate for α using the
following set of equations

Υi−j, and k = 1, 2, . . . , p. Notice that the entries for the matrix
i )}m,m
r=1,s=1. These inner products can be computed using a kernel
i−j)}m,m

r=1,s=1. Likewise, entries of Ki−k,i−j are given by inner products {ϕ(xr

i−k)⊤φ(xs

i )}m,m

i−j)}m,m

r=1,s=1.

i−k, xs

r=1,s=1,

E[Hk] =

αj E[Kk,j],

(16)

for k = 1, . . . , p. We have suppresed the subindex i from the equation above to keep the notation uncluttered. As
in Equation in (7), expectations can be estimated over the set of available samples.
We can use the system of equations in (16) to estimate the parameters α. The system of equations is given as

pXj=1
pXj=1
pXj=1
pXj=1

pXj=1

(12)

(13)

(14)

(15)

(17)

(18)

H1
H2

...
Hp





K1,1 K1,2
K2,1 K2,2
...
...
Kp,1 Kp,2

=
bα = arg min

α

· · · K1,p
· · · K2,p
...
...
· · · Kp,p





α1I
α2I
...
αpI



kH − Kαmk2
2 ,

where I is the identity matrix of dimension m. We can ﬁnd an estimator for α by solving

where H ∈ Rmp×m is a block-wise matrix with blocks given by {Hk}p
matrix with blocks given by {Kk,j}p,p
as {αk I}p
given by {Kk,i}p
It can be shown that the optimization problem in (18) can be cast into a least-squares problem as

k=1. For convenience, we also deﬁne bKi ∈ Rmp×m as a block-wise matrix taken from K, with blocks

k=1; K ∈ Rmp×mp is a block-wise
k=1,j=1; and αm ∈ Rmp×m is also a block-wise matrix with blocks given

k=1.

where A ∈ Rp×p with entries {tr(bK⊤

i bKj)}p,p

α

kAα − bk2
2 ,

bα = arg min
i=1,j=1, and b ∈ Rp×1 with entries {tr(H⊤bKi)}p

6

i=1.

(19)

3.3 Solving the pre-image problem for forecasting in a time-series

We want to use the method above for forecasting a new value x∗
series. For now on, our method allows us to make predictions in the feature space by means of

i using α, and the p previous values of the time

τ ∗
i =

pXj=1

αjϕ(xi−j),

(20)

i to the input space, to get the predicted x∗

where the values for {αj}p
j=1 have been estimated as explained in section 3.2. We would like to map back the
value of τ ∗
i . In the kernel literature this problem is known as the pre-
image problem [5], and it is an ill-posed problem due to the higher dimensionality of the feature space, meaning
that the transformed point τ ∗
We apply a tensor product to both sides of expression (20), leading to

i may not have a corresponding x∗

i such that ϕ(x∗

i .
i ) = τ ∗

τ ∗
i ⊗ φ(x∗

i ) =

(21)

In order to get an estimate for x∗

i , we can solve the following minimization problem in H1 ⊗ H2

x∗
i = arg min

x

where we have deﬁned

f (x) = arg min

Expression for f (x) can also be written as

i ).

αjϕ(xi−j) ⊗ φ(x∗

pXj=1
αjϕ(xi−j) ⊗ φ(x) − ϕ(x) ⊗ φ(x)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

x (cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
pXj=1
αjϕ(xi−j) ⊗ φ(x) − ϕ(x) ⊗ φ(x)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
αkϕ(xi−k) ⊗ φ(x)+

H1⊗H2

2

pXk=1

H1⊗H2

αjϕ(xi−j) ⊗ φ(x), ϕ(x) ⊗ φ(x)+

H1⊗H2

αjϕ(xi−j) ⊗ φ(x),

f (x) =(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
pXj=1
f (x) =* pXj=1
− 2* pXj=1

2

,

H1⊗H2

.

(22)

+ hϕ(x) ⊗ φ(x), ϕ(x) ⊗ φ(x)iH1⊗H2 .

By using the property hu ⊗ v, a ⊗ biH1⊗H2 = hu ⊗ aiH1 hv ⊗ biH2

, we get

αkϕ(xi−k)+

f (x) =* pXj=1
− 2* pXj=1

αjϕ(xi−j ),

pXk=1
αjϕ(xi−j), ϕ(x)+

hφ(x), φ(x)iH2

H1

hφ(x), φ(x)iH2

H1

+ hϕ(x), ϕ(x)iH1 hφ(x), φ(x)iH2 .

7

(23)

(24)

Noticing that C =DPp

j=1 αjϕ(xi−j),Pp

kernels k(x, x′) of the form g(kx − x′k2), we can simplify expression (24) as follows

is a constant (it does not depend on x), and using

f (x) = Cg(0) − 2g(0)

αjk(xi−j, x) + g2(0).

Taking the derivative with respect to x, we get

If we use an squared exponential (SE) kernel or a radial basis function (RBF) kernel

where ℓ2 is known as the bandwidth, the expression (26) follows as

df (x)

dx

αjk(xi−j, x)(xi−j − x).

Equating to zero, and solving for x, we get the following ﬁxed-point equation

df (x)

= −2g(0)

k=1 αkϕ(xi−k)EH1
pXj=1
pXj=1
k(x, x′) = exp(cid:18)−
pXj=1

2g(0)

= −

ℓ2

dx

αj

dk(xi−j, x)

dx

.

kx − x′k2

2ℓ2

(cid:19) ,

x∗

i = Pp
Pp

j=1 αjk(xi−j, x∗

i )xi−j

k=1 αkk(xi−k, x∗
i )

.

(25)

(26)

(27)

(28)

(29)

4 Related work

As we mentioned in the introduction, the authors in [9], and [7] introduced a kernelized version of an autoregres-
sive process based on the kernel trick idea [12, 15]. In particular, the autoregressive model is built in a feature
space, and the parameters of the model are estimated in two different ways, either by minimizing a quadratic error
[9], or by means of the Yule-Walker algorithm [7]. In [9], the pre-image problem, this is, the problem of inverse
transforming a point in the feature space, to the input space, is only solved for ﬁnite-dimensional feature spaces
for which the inverse transformation can be readily be computed. In [7], the pre-image problem is solved by using
a ﬁxed-point algorithm similar to equation (29). When assuming a stationary kernel, this is k(x, x′) = k(x − x′),
the method in [7] turns out to be a particular example of the system in equation (15) for any particular values of
r, and s in the kernel matrices Hi−k,i, and Ki−k,i−j.
Expression (10) follows closely Equation (5.15) in [3]. The expression in [3] is obtained as the Yule-Walker equa-
tions for a so called autoregressive Hilbertian process of order p, ARH(p), that corresponds to an autoregressive
process deﬁned in a Hilbert space. In [3], the {αj}p
j=1are bounded linear operators, in contrast to equation (10),
where they correspond to scalar values. Estimation of αj, and prediction are different though. The estimation
for the bounded linear operators {αj}p
j=1 is obtained by projecting the observations in a Hilbert space of ﬁnite
dimension. Predictions are performed directly by applying the estimated operators over the input data.
In [10], the author use kernel mean embeddings to provide one step ahead distribution prediction. In particular,
distributions at any time t are represented by kernel mean maps. A mean map at time t + 1 can be obtained as
a mean map at time t, linearly transformed by a bounded linear operator. In fact, this corresponds to a ARH(1),
where the functions in H correspond to kernel mean embeddings. The distribution at time t + 1 in the input space
is approximated by a weighted sum of historic input samples. The weigths in the approximation are computed
from particular kernel expressions [10]. Our method considers models of order p, and our predictions are point
estimates in contrast to [10]. Also, we use embeddings of joint probability distributions, P(Xi, Xi−k) instead of
embeddings of marginal distributions, P(Xi) (mean maps).

8

5 Experimental evaluation

In this section, we provide details for the experimental evaluation performed in this paper. We describe tha
datasets we use, and the procedure that we follow for validating the results.

d
n
o
c
e
s

a

f
o

5
−
0
1

600

400

200

0

-200

-400

e
d
u
t
i
l

p
m
A

1.4

1.2

1

0.8

0.6

0.4

0.2
0

345

340

335

m
p
p

330

325

320

1850

1900
Year

(a) Earthrot

1950

315

1965 1967 1969 1971 1973 1975 1977 1979 1981

Year

(b) CO2

50

40

30

)
t
(

20z

10

0
-20

200

400

600

Time step

(c) MG30

-10

0
x(t)

10

20

-50

(d) Lorenz

50

0

y(t)

Figure 1: The four time-series used in this paper to compare the performance of the method proposed.

5.1 Datasets

We use four time-series to evaluate the performance of the different methods. The ﬁrst two datasets belong to the
Time Series Data Library (TSDL), and can be found in [6]. The last two datasets were generated by the authors.

– Earthrot. With the name Earthrot, we refer to the Annual changes in the earth ˆA’s rotation, day length
(sec*10**-5) 1821-1970 dataset, available at [6]. Units are in 10−5 of a second. The time-series contains
150 samples. We use the ﬁrst 130 samples for the experiments.

– CO2. We use the dataset CO2 (ppm) mauna loa, 1965-1980 from the TSDL, which corresponds to monthly
measures of CO2 in parts per million from the Mauna Loa observatory. The time-series exhibit a periodic,
and approximately linear behavior. The dataset contains 192 samples. For the experiments we use the ﬁrst
150 samples.

– MG30. The time-series MG30 refers to the time-series obtained from the Mackey-Glass non-linear time

9

delay differential equation given as

dx(t)

dt

= −0.1x(t) +

0.2x(t − τ )
1 + x(t − τ )

,

with τ = 30 [7]. This time series exhibits chaotic dynamics. We generate a time-series of length 600. For
the experiments, we use the ﬁrst 400 samples.

– Lorenz. The Lorenz attractor refers to a set of three coupled ordinary differential equations given as

dx(t)

dt

dy(t)

dt

dz(t)

dt

= −ax + ay

= −xz + rx − y

= xy − bz,

where a, r, and b are constants. For certain values a, r, and b, the system exhibits chaotic behavior. We
set values for the parameters as a = 10, r = 28, and b = 8/3. For these values, the three-dimensional
multi-variate time-series (x(t), y(t), z(t)) displays chaotic dynamics. We generate 500 samples per output
dimension, and use the ﬁrst 400 samples for the experiments. We perform prediction over the three time-
series x(t), y(t), and z(t), treating them as independent from each other.

Figure 1 shows the four datasets described above, and used for testing the methods.

5.2 Validation

The validation of the method proposed in this paper is done by performing one-step ahead prediction over each
of the time series described above. For performing one step-ahead prediction, we use sliding frames of w + 1
samples, where the ﬁrst w samples are used for training, and the additional last sample is used for validation. The
sliding frames are organized consecutively, with an overlap of w samples. The training data is used for setting
the parameters of each of the models used for comparison, including the order of the autoregressive model. For
the order of the model, we evaluate values of p from one to ﬁve. We compute the mean-squared error over the
validating samples. We next describe the particular setup used for training in each of the models used in the
experiments.

– Linear AR model (LAR). The coefﬁcients λ for the linear AR model are estimated using the Yule-Waker
equations. Within each frame of length w, we again use a sliding window of size w/2 + 1, where the ﬁrst
w/2 samples are used to compute λ, and the last sample is used for performing one-step ahead prediction
for different values of p. The sliding windows are organized consecutively with an overlap of w/2 samples.
The results of the one-ahead step prediction withing the frame of length w, are used to select the value of p,
which is selected as the value that ocurred more frequently offering the best prediction performance. Once
the value for p has been selected, we compute again the values for λ using all the datapoints within w, and
used this new λ for performing one-ahead step prediction over the time step w + 1.

– Kernel autoregressive model (KAM). We implement the method proposed in [7]. To compute the expecta-
tions, we use sample means of the quantites of interest. For the kernel function, we use an SE kernel as
in expression (27). The pre-image problem is solved as explained in [7], which has the same ﬁxed-point
solution as in expression (29). The values for ℓ, and p are chosen as follows: within the frame of length
w, we generate sliding frames of size w/2 + 1. The sliding frames are organized consecutively with an
overlap of w/2 samples. The ﬁrst w/2 data points are used for estimating the values for α by solving the

10

system of equations in expression (7). We then use the data point at time step w/2 + 1 for selecting the
best value for ℓ, and p, as the ones that on average, within the window of length w, yield the lowest error.
We use a grid of values for ℓ by taking a grid of percentages, ℓp, of the median of the training data within
the frame of size w/2. The percentages that we consider are 0.01, 0.01, 0.5, 1, 2, or 5 of the median of the
training data within the frame of length w/2. Once we select the value for ℓp, we compute a new value for
ℓ as the percentage ℓp of the median of the training data within the frame of size w. Having chosen ℓ, and
p, we use all the training data of the frame of size w for ﬁnding a new set of coefﬁcients α, and ﬁnally,
provide a forecasting at time step w + 1 by solving again a pre-image problem.

– Kernel embedding method (KEM). We implement the method described in section 3. We also use an SE
kernel. The values for ℓ, and p used for one-step ahead forecasting for time step w + 1 are computed as
follows: within the frame of length w, we use an sliding window of length w/2 + 1. The sliding windows
are set up consecutively with an overlap of w/2 samples. The ﬁrst w/2 data points are used for estimating
the coefﬁcients α by solving equation (19). For selecting ℓ and p, we follow a similar procedure to the
one used for the kernel autoregressive model above: the sliding data point at time step w/2 + 1 is used to
choose the values for ℓ, and p, that on average lead to the lowest prediction errors. Prediction at time step
w/2 + 1 is performed by solving the pre-image problem in expression (29). In fact, we used percentages
of the median of the training data, ℓp, in the windows of length w/2, in order to test different values for ℓ.
The percentages that we used were 0.01, 0.01, 0.5, 1, 2, and 5. Once the best percentage of the median of
the training data for ℓ, and the best value of the order of the model p have been chosen, we compute again
the value for ℓ using the best value for ℓp and the training data in the whole frame of size w. We again
compute α using the training data in frame w, and forecast one-step ahead for the time setp w + 1 solving
the pre-image problem in expression (29).

– Gaussian processes (GP). We follow the model proposed in [8], in which the random variable of the

process at time Xn can be described using

Xn = f (Xn−1, . . . , Xn−p) + ǫ,

(30)

where ǫ ∼ N (0, σ2), and f (x) is assumed to follow a Gaussian process prior f ∼ GP(0, k(x, x))), with
covariance function k(x, x). For the covariance function, we use a SE kernel as in equation (27). The
parameter of the covariance function ℓ, and the parameter σ for the likelihood model, are estimated by
maximizing the log marginal likelihood using a scaled conjugate gradient procedure. We use the GPmat
Toolbox4 for all Gaussian processes related routines. For selecting the value of p, we use a sliding frame of
length w/2 + 1, within the frame of length w. The sliding windows of length w/2 + 1 are established as in
the other methods. A number of w/2 data points are used for learning the hyperparameters of the Gaussian
process, and the data point at time step w/2 + 1 is used for cross-validating the value for p. The value for p
is chosen as the one that on average (within the frame of size w) leads to the lowest errors. Once the value
for p has been chosen, we use again the w samples for training a new GP. This new ﬁtted GP is used for
forecasting the data point at time step w + 1.

– Neural networks (NN). We use a neural network with one hidden layer for learning a similar mapping as
in equation (30). For choosing the number of neurons nh of the hidden layer, and the value for p, we use
a similar procedure as for the methods above: within the frame of length w, we generate sliding windows
of length w/2 + 1, in a similar way as they were slid in the other approaches. The w/2 ﬁrst datapoints
are used for ﬁtting the weights of the neural network, and the data point at time step w/2 + 1 is used for
choosing the value for nh, and the value for p. These values are chosen as the ones that on average, within
the frame of length w, lead to the lowest error. We allow the number of neurons in the hidden layer to

4Available at https://github.com/SheffieldML/GPmat

11

be any of the following values: 5, 10, 15, 20, 25, or 30. For the the neural networks routines, we use the
Neural Networks toolbox for MATLAB, with all the default settings, except for the number of neurons in
the hidden layer.

6 Results

We compare the performance of the different methods for short-term prediction over each of the time-series
described in section 5. Figures 2, 3, 4, and 5 show the performance of the classical linear autoregressive model,
the kernel autoregressive model, and the kernel embeddings of autoregressive model over the four time series
described in Section 5. The mean squared error (MSE) for one step ahead prediction is shown as the title in each
ﬁgure.

d
n
o
c
e
s

a

f
o

5
−
0
1

600

400

200

0

-200

-400

MSE 689.3491

1880

1900

1920

1940

Year

(a) Earthrot using linear AR

d
n
o
c
e
s

a

f
o

5
−
0
1

600

400

200

0

-200

-400

MSE 254.0535

MSE 313.5737

1880

1900

1920

1940

Year

(b) Earthrot using KAM

d
n
o
c
e
s

a

f
o

5
−
0
1

600

400

200

0

-200

-400

1880

1900

1920

1940

Year

(c) Earthrot using KEM

Figure 2: One-step ahead prediction over the dataset Earthrot given by the linear AR model, the method proposed by Kallas
et. al. in [7], and the method based on kernel embeddings proposed in this paper. Solid lines are the test data, dashed lines
are the predictions given by the methods. The title of each ﬁgure displays the mean squared error between the test data, and
the predicted output.

Figure 2 shows the one-step ahead prediction results for the time series Earthrot. For this example, we used
sliding windows of length 51. The ﬁrst 50 observations of each sliding window were used for training, and the
forecast was perfomed for the time step 51-st of each sliding window. Since we used the ﬁrst 130 samples from
the original time series for the experiment, and a sliding window of 51 points, the MSE is computed over a total
of 80 observation points. We notice that both kernel methods (ﬁgures 2(b) and 2(c)) are able to follow the original
time series even from the ﬁrst time steps, contrary to the linear model (ﬁgure 2(a)), where the prediction is far

12

away from the time series. With respect to the 80 values of p that were chosen for each method, we computed a
simple linear correlation coefﬁcient between the series of values of p’s for the linear method, and the two kernel
approaches. As expected, there is a higher similarity between the values picked by the KAM, and the KEM,
0.5494, compared to −0.3349 for the correlation coefﬁcient between the linear AR model and the KAM, and
0.2405 for the correlation coefﬁcient between the linear AR model and KEM. A further comparison between
the values of p chosen by the kernel methods, show that they disagreed in 22 trials out of 80. With respect to
the values of ℓ chosen by the two kernel methods, in only 4 out of the 80 trials, both methods chose different
bandwidth values. The values for the MSE show that the method based on kernel embeddings offers the best
performance when compared to the kernel autoregressive method, and the linear AR model.

340

335

m
p
p

330

325

320

MSE 0.6572

1970

1972

1974

1976

Year

(a) CO2 using linear AR

MSE 0.6122

1970

1972

1974

1976

Year

(b) CO2 using KAM

340

335

m
p
p

330

325

320

MSE 0.5188

340

335

m
p
p

330

325

320

1970

1972

1974

1976

Year

(c) CO2 using KEM

Figure 3: One-step ahead prediction over the dataset CO2, given by the linear AR model, the method proposed by Kallas
et. al. in [7], and the method based on kernel embeddings proposed in this paper. Solid lines are the test data, dashed lines
are the predictions given by the methods. The title of each ﬁgure displays the mean squared error between the test data, and
the predicted output.

Figure 3 shows the results of one-step ahead forecasting for the linear AR model, the KAM, and the KEM. As in
the previous example, we used sliding windows of length 51 samples, where the ﬁrst 50 samples in each window
are used for ﬁnding parameters of the models, and the last sample (number 51) is used to test the forecasting
ability of the methods. From the CO2 time series that is originally available, we used the ﬁrst 150 samples to
assess the prediction performance in several points of the time-series. Since we use window frames of 51 points,
the MSE error for the prediction is computed over 100 samples of the time series.
It can be noticed how the KEM method is able to follow more closely the low and high peak values of the
time series, when compared to the linear method, and KAM. This can be explained by the fact that the kernel

13

embbeding method is able to take into account the particular structure in the time series, which for the KAM is
lost when averaged. With respect to the values of p, the linear AR model chooses a value of p = 2, or p = 5,
mostly. The KAM consistently worked better with p = 2, and the KEM with p = 5. The values chosen for ℓ in
the kernel methods were equal 80% of the trails. The MSE values (appearing on the title of each ﬁgure) indicate
that the KEM outperforms the linear AR method and the KAM.

1.4

1.2

1

0.8

0.6

0.4

e
d
u
t
i
l

p
m
A

0.2

100

1.4

1.2

1

0.8

0.6

0.4

e
d
u
t
i
l

p
m
A

0.2

100

1.4

1.2

1

0.8

0.6

0.4

e
d
u
t
i
l

p
m
A

0.2

100

MSE 372.0770×10−6

200

300

Time step

(a) MG30 using linear AR

MSE 11.0855×10−6

200

300

Time step

(c) MG30 using KAM

MSE 2.3910×10−6

MSE 331.3391×10−6

315

320

Time step

325

330

(b) MG30 using linear AR

MSE 35.8321×10−6

1.4

1.2

1

0.8

e
d
u
t
i
l

p
m
A

400

0.6

310

1.4

1.2

1

0.8

e
d
u
t
i
l

p
m
A

400

0.6

310

315

320

Time step

325

330

(d) MG30 using KAM

MSE 1.2992×10−6

1.4

1.2

1

0.8

e
d
u
t
i
l

p
m
A

200

300

Time step

(e) MG30 using KEM

400

0.6

310

315

320

Time step

(f) MG30 using KEM

325

330

Figure 4: One-step ahead prediction over the MG30 dataset given by the linear AR model, the method proposed by Kallas
et. al.in [7], and the method based on kernel embeddings proposed in this paper. Solid lines are the test data, dashed lines
are the predictions given by the methods. Figures 4(a), 4(c), and 4(e) show results for the MG30 time-series. Figures 4(b),
4(d), and 4(f) show results for the MG30 time-series within a shorter time period, between time steps 311 and 330. The title
of each ﬁgure displays the mean squared error between the test data, and the predicted output.

14

Figure 4 shows the one-step ahead prediction for the Mackey-Glass chaotic time series. For this time series,
we use sliding windows of length 101. The ﬁrst 100 samples of the sliding window are used for training the
models, and the sample 101-st is used for one-step ahead prediction. We perform the one-step ahead prediction
over consecutive 300 samples, one at a time, and the MSE reported is the average over these 300 one-step ahead
forecasting values. It can be noticed from ﬁgures 4(a), 4(c), and 4(e) that the methods based on kernels yield
better prediction results than the linear method. Since it seems that qualitatively, the prediction performance
for KAM and KEM is similar, we included additional ﬁgures where we zoom in a particular range where the
difference in performace can be noticed. Figures 4(b), 4(d), and 4(f) show results for the MG30 time-series
within a shorter time period, between time steps 311 and 330. With respect to the p values, the linear model
favored a value of p = 5 (250 over the 300 trials). The KAM and the KEM predominantly used higher values of
p: 70 for p = 4, and 163 for p = 5, for the KAM; and 43 for p = 4, and 257 for p = 5, for the KEM. In contrast
to the experiments above, this time the kernel methods only selected the same value for ℓ in 76 cases out of 300.
In the terms of the average MSE over the 300 trials, the experiment shows that both kernel mehods outperform
the linear AR method. The MSE obtained by the KEM is lower than the one obtained by KAM.

MSE x(t): 0.3051; MSE y(t): 0.9905; MSE z(t): 0.4371

MSE x(t): 0.0284; MSE y(t): 0.0454; MSE z(t): 0.1242

)
t
(
z

20

10

0

-10

-20
-20

)
t
(
z

20

10

0

-10

-20
-20

-10

0
x(t)

10

20

-50

(a) Lorenz using linear AR

50

0

y(t)

-10

0
x(t)

10

20

-50

(b) Lorenz using KAM

50

0

y(t)

MSE x(t): 0.0239; MSE y(t): 0.0252; MSE z(t): 0.1276

)
t
(
z

20

10

0

-10

-20
-20

-10

0
x(t)

10

20

-50

(c) Lorenz using KEM

50

0

y(t)

Figure 5: One-step ahead prediction over the Lorenz dataset given by the linear AR model, the method proposed by Kallas
et. al. (2013), and the method based on kernel embeddings proposed in this paper. Solid lines are the test data, dashed lines
are the predictions given by the methods. The title of each ﬁgure displays the mean squared error between the test data, and
the predicted output.

Figure 5 shows the prediction results over the Lorenz dataset. As explained before, prediction is performed over
each component (x(t), y(t), z(t)) of the 3D time series, in an independent manner. For each of the three time
series, we use sliding windows of length 101, where the forecasting is done over the last time step of each frame.

15

The prediction performance is evaluated over 300 successive frames, all of them of length 101. With respect
to the order p for the different models, the linear AR model picked p = 2 almost 22% of the time, and p = 5
almost 75% of the time. The KAM chose p = 2 almost 70% of all trials, and p = 3 almost 22% of all the
repetitions. Finally, the KEM picked p = 2 almost 42% of the time, p = 4 almost 23% of the time, and p = 5
approximately 30% of the trials. As for the previous experiments, the kernel methods outperform the linear AR
model. Although the prediction error of the KAM for z(t) is lower than the prediction error for the KEM, on
average, the KEM outperforms the KAM.

Table 1: Mean squared error for the test data and the predicted outputs, given by the linear autoregressive process (Linear
AR), the kernel autoregressive model proposed by Kallas et al in [7] (KAM), and the kernel embeddings of autoregressive
processes proposed in this paper (KEM). The values of the MSE for the MG30 should be multiplied by 10−6.

Database
Earthrot

CO2
MG30

Lorenz x(t)
Lorenz y(t)
Lorenz z(t)

Linear AR
689.3491
0.6572
372.0770
0.3051
0.9905
0.4371

KAM

KEM

313.5737 254.0535

0.6122
11.0855
0.0284
0.0454
0.1242

0.5188
2.3910
0.0239
0.0252
0.1276

Table 1 shows a summary of the MSE obtained by the linear AR model, the kernel autoregressive model, and
the kernel embedding method for the four datasets. It is clear from that table that the method that uses the kernel
embeddings lead to better results, except of the component z(t) of the Lorenz time series.

Table 2: Mean squared error for the test data and the predicted outputs, given by a neural network (NN), a Gaussian process
regressor (GP), the kernel autoregressive model proposed by Kallas et al in [7] (KAM), and the kernel embeddings of
autoregressive processes proposed in this paper (KEM). The values of the MSE for the MG30 should be multiplied by 10−6.

Database
Earthrot

CO2
MG30

Lorenz x(t)
Lorenz y(t)
Lorenz z(t)

NN

827.8469
0.6027
41.4519
0.0595
0.1114
0.2041

GP

KAM

KEM

570.1689
0.4631
2.0991
0.0118
0.0129
0.0263

182.3038 134.2564
0.4107
6.3064
0.0088
0.0146
0.0211

0.3177
1.1052
0.0037
0.0038
0.0140

Table 2 shows the performance of neural networks, Gaussian processes, and the kernel autoregressive method
compared to the performance of the kernel embeddings proposed in this paper. The value of w for all the time
series was ﬁxed to 50, and the one-step ahead forecasting was performed for 80 time steps for Earthrot, 100 time
steps for CO2, and 300 for both MG30, and Lorenz. The numerical optimization methods used for NN and GP
are based on gradient-descent-like procedures, which heavily depend on a good parameter initialization to deliver
sensible results. A bad parameter initialization often leads to poor prediction performance. In order to reduce
the number of outliers for the prediction for NN and GP, we only computed the mean for those squared errors
that were between quartiles 25-th and 75-th of all the squared errors computed for each time series. For a fair
comparison, we also computed the MSE for the KAM, and the MSE for the KEM removing outliers, as explained
before. We noticed from table 2 that the methods based on kernels, GP, KAM and KEM, yield better prediction
performance than NN. Gaussian processes outperform KAM for the MG30 time series, and the component y(t)
of the Lorenz time series. The KEM method shows improved performance over all the other competing models.

16

7 Conclusions

In this paper, we have introduced kernel embeddings of joint probability distributions by means of an autoregres-
sive process of order p placed over covariance operators. The solution to the model is done through a Yule-Walker
system of equations for empirical estimates of the cross-covariance operators. Predictions in the input space are
performed by solving a pre-image problem, for which a ﬁxed-point algorithm is developed. Experimental results
show that the method proposed here outperforms several non-linear versions of the autoregressive model, in the
task of one-step ahead forecasting of time series. An important extension of this line work would be the for-
mulation of a non-linear vector-valued autoregressive model, for which coefﬁcients {αj}p
j=1 would need to be
considered as more general linear operators.

Acknowledgements

E. A. Valencia is being partly funded by University Tecnol´ogica de Pereira. The authors would like to thank
Colciencias and British Council for funding under the project “Hilbert space embeddings of Autoregressive
processes”. The authors would also like to thank Arthur Gretton, Zolt´an Szab´o and Kenji Fukumizu for their
insightful comments and suggestions.

References

[1] C. R. Baker. Joint measures and cross-covariance operators. American Mathematical Society, 9:273–289,

186.

[2] C. Bishop. Pattern Recognition and Machine Learning. Springer, 2006.

[3] D. Bosq. Linear processes in function spaces: theory and applications. Springer, 2000.

[4] Arthur Gretton, Olivier Bousquet, Alex Smola, and Bernhard Sch¨olkopf. Algorithmic Learning Theory:
16th International Conference, ALT 2005, Singapore, October 8-11, 2005. Proceedings, chapter Measuring
Statistical Dependence with Hilbert-Schmidt Norms, pages 63–77. Springer Berlin Heidelberg, Berlin,
Heidelberg, 2005.

[5] P. Honeine and C. Richard. Preimage problem in kernel-based machine learning. IEEE Signal Processing

Magazine, 28:73–88, 2011.

[6] Rob Hyndman. Time series data library. http://data.is/TSDLdemo.

[7] M. Kallas, P. Honeine, C. Francis, and H. Amoud. Kernel autoregressive models using Yule-Walker equa-

tions. Signal Processing, 93:3053–3061, 2013.

[8] Jus. Kocijan, A. Girard, B. Banko, and R. Murray-Smith. Dynamic systems identiﬁcation with, Gaussian

processes. Mathematical and Computer modelling of Dynamical Systems, 11(4):411–424, 2005.

[9] R. Kumar and C. V. Jawahar. Kernel approach to autoregressive modeling. In 13th National Conference on

Comunications (NCC) Kanpur, India, 2007.

[10] C. H. Lampert. Predicting the future behavior of a time-varying probability distribution. In Computer Vision

and Pattern Recognition (CVPR), 2015 IEEE Conference on, pages 942–950, June 2015.

[11] O. Nelles. Nonlinear System Identiﬁcation: from Classical approaches to Neural Networks and fuzzy

models. Springer, ﬁrst edition edition, 2001.

17

[12] B. Sch¨olkopf and A.J. Smola. Learning with Kernels: Support Vector Machines, Regularization, Optimiza-

tion, and Beyond. Adaptive computation and machine learning. MIT Press, 2002.

[13] R. H. Schunway and D. S. Stoffer. Time series analysis an its aplications: with R examples. Springer, third

edition edition, 2011.

[14] K. S. Shanmugan and A. M. Breipohi. Random Signals: Detection, Estimation and Data Analysis. Wiley,

ﬁrst edition, 1988.

[15] J. Shawe-Taylor and N. Cristianini. Kernel Methods for Pattern Analysis. Kernel Methods for Pattern

Analysis. Cambridge University Press, 2004.

[16] A. J. Smola, A. Gretton, L. Song, and B. Sch¨ollkopf. A Hilbert space embedding for distributions. In 18th
international conference on algorithmic learning theory: Springer-Verlag, Berlin, Germany, pages 13–31,
2011.

[17] L. Song, A. Gretton, and K. Fukumizu. Embedding of conditional distribution. IEEE Signal Processing

Magazine, 30:98–111, 2013.

[18] L. Song, J. Huang, A. Smola, and K. Fukumizu. Hilbert embeddings of conditional distributions with ap-
plications to dynamical systems. In 26th Annual International Conference on Machine Learning Montreal-
Canada, pages 961–968, 2009.

[19] Le Song, Sajid M. Siddiqi, Geoffrey Gordon, and Alex Smola. Hilbert space embeddings of hidden Markov
In Johannes F¨urnkranz and Thorsten Joachims, editors, Proceedings of the 27th International

models.
Conference on Machine Learning (ICML-10), pages 991–998, Haifa, Israel, June 2010. Omnipress.

18

