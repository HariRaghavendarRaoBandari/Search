Effective Mean-Field Inference Method for

Nonnegative Boltzmann Machines

Muneki Yasuda

Graduate School of Science and Engineering, Yamagata University

Jyounan 4-3-16, Yonezawa, Yamgata pref., 992-8510, Japan

Email: muneki@yz.yamagata-u.ac.jp

6
1
0
2

 
r
a

M
8

 

 
 
]
L
M

.
t
a
t
s
[
 
 

1
v
4
3
4
2
0

.

3
0
6
1
:
v
i
X
r
a

Abstract—Nonnegative Boltzmann machines (NNBMs) are
recurrent probabilistic neural network models that can describe
multi-modal nonnegative data. NNBMs form rectiﬁed Gaussian
distributions that appear in biological neural network models,
positive matrix factorization, nonnegative matrix factorization,
and so on. In this paper, an effective inference method for
NNBMs is proposed that uses the mean-ﬁeld method, referred
to as the Thouless–Anderson–Palmer equation, and the diagonal
consistency method, which was recently proposed.

I.

INTRODUCTION

Techniques for inferencing statistical values on Markov
random ﬁelds (MRFs) are fundamental techniques used in
various ﬁelds involving pattern recognition, machine learning,
and so on. The computation of statistical values on MRFs
is generally intractable because of exponentially increasing
computational costs. Mean-ﬁeld methods, which originated
in the ﬁeld of statistical physics, are the major techniques
for computing statistical values approximately. Loopy belief
propagation methods are the most widely-used techniques in
the mean-ﬁeld methods [1], [2], and various studies on them
have been conducted.

Many mean-ﬁeld methods on MRFs with discrete random
variables have been developed thus far. However, as compared
with the discrete cases, mean-ﬁeld methods on MRFs with
continuous random variables have not been developed well.
One reason for this is thought to be that the application of
loopy belief propagation methods to continuous MRFs is not
straightforward and has not met with no success except in
Gaussian graphical models [3]. Although it
is possible to
approximately construct a loopy belief propagation method
on a continuous MRF by approximating continuous functions
using histograms divided by bins, this is not practical.

Nonnegative Boltzmann machines (NNBMs) are recurrent
probabilistic neural network models that can describe multi-
modal nonnegative data [4]. For continuous nonnegative data,
an NNBM is expressed in terms of a maximum entropy
distribution that matches the ﬁrst and second order statistics of
the data. Because random variables in NNBMs are bounded,
NNBMs are not standard multivariate Gaussian distributions,
but rectiﬁed Gaussian distributions that appear in biological
neural network models of the visual cortex, motor cortex, and
head direction system [6], positive matrix factorization [7],
nonnegative matrix factorization [8], nonnegative independent
component analysis [9], and so on. A learning algorithm for
NNBMs was formulated by means of a variational Bayes
method [10].

Because NNBMs are generally intractable, we require an
effective approximate approach in order to provide proba-

bilistic inference and statistical learning algorithms for the
NNBM. A mean-ﬁeld method for NNBMs was ﬁrst proposed
in reference [4]. This method corresponds to a naive mean-
ﬁeld approximation, which is the most basic mean-ﬁeld tech-
nique. A higher-order approximation than the naive mean-ﬁeld
approximation for NNBMs was proposed by Downs [5] and
recently, his method was improved by the author [11]. These
methods correspond to the Thouless–Anderson–Palmer (TAP)
equation in statistical physics, and they are constructed using
a perturbative approximative method referred to in statistical
physics as the Plefka expansion [12]. Recently, a mean-
ﬁeld method referred to as the diagonal consistency method
was proposed [13]. The diagonal consistency method is a
powerful method that can increase the performance of mean-
ﬁeld methods by using a simple extension for them [13], [14],
[15].

In this paper, we propose an effective mean-ﬁeld inference
method for NNBMs in which the TAP equation, proposed
in reference [11], is combined with the diagonal consistency
method. The remainder of this paper is organized as follows. In
section II the NNBM is introduced, and subsequently, the TAP
equation for the NNBM is formulated in section III in accor-
dance with reference [11]. The proposed methods are described
in section IV. The main proposed method combined with the
diagonal consistency method is formulated in section IV-B.
In section V, we present some numerical results of statistical
machine learning on NNBMs and verify the performance of
the proposed method numerically. Finally, in section VI we
present our conclusions.

II. NONNEGATIVE BOLTZMANN MACHINES

In probabilistic information processing, various applica-
tions involve continuous nonnegative data, e.g., digital image
ﬁltering, automatic human face recognition, etc. In general,
standard Gaussian distributions, which are unimodal distribu-
tions, are not good approximations for such data. Nonnegative
Boltzmann machines are probabilistic machine learning mod-
els that can describe multi-modal nonnegative data, and were
developed for the speciﬁc purpose of modeling continuous
nonnegative data [4].

Let us consider an undirected graph G(V, E), where V =
{i | i = 1, 2, . . . , n} is the set of vertices in the graph and E =
{(i, j)} is the set of undirected links. On the undirected graph,
an NNBM is deﬁned by the Gibbs-Boltzmann distribution,

P (x | b, w) :=

1

Z(b, w)

exp(cid:0) − E(x; b, w)(cid:1),

(1)

where

where

E(x; b, w) := −

bixi +

n

Xi=1

1
2

n

Xi=1

wiix2

i + X(i,j)∈E

wij xixj

is the energy function. The second summation in the energy
function is taken over all the links in the graph. The expres-
sion Z(b, w) is the normalized constant called the partition
function. The notations b denote the bias parameters and the
notations w denote the coupling parameters of the NNBM.
All variables x take nonnegative real values, and wii > 0
and wij = wji are assumed. The distribution in equation
(1) is called the rectiﬁed Gaussian distribution, and it can
represent a multi-modal distribution, unlike standard Gaussian
distributions [6]. In the rectiﬁed Gaussian distribution, the
symmetric matrix w is a co-positive matrix. The set of co-
positive matrices is larger than the set of positive deﬁnite
matrices that can be used in a standard Gaussian distribution,
and is employed in co-positive programming.

III. VARIATIONAL FREE ENERGY AND THE TAP

EQUATION FOR THE NNBM

To lead to the TAP equation for the NNBM, which is an
alternative form of the TAP equation proposed in reference
[11], and a subsequent proposed approximate method for the
NNBM in this paper, let us introduce a variational free energy,
which is called the Gibbs free energy in statistical physics,
for the NNBM. The variational free energy of the NNBM in
equation (1) is expressed by

F (m, v; b, w) := −

bimi +

n

Xi=1
Xi=1
limi −

1
2

n

+ max

l,r n

n

Xi=1

where

1
2

n

Xi=1

wiivi

rivi − lnZ(l, r, w)o,

(2)

Z(l, r, w)
:=Z ∞

0

exp(cid:16)

n

Xi=1

lixi −

1
2

n

Xi=1

rix2

i − X(i,j)∈E

wij xixj(cid:17)dx.

|

A brief derivation of this variational free energy is given
in Appendix A. The notations l = {li
i ∈ V } and
r = {ri | i ∈ V } are variables that originated in the Lagrange
multiplier. It should be noted that the independent variables
mi and vi represent the expectation value and the variance of
xi, respectively. In fact, the values of m = {mi | i ∈ V } and
v = {vi | i ∈ V }, which minimize the variational free energy,
coincide with expectations, hxii, and variances, hx2
i i, on the
NNBM, respectively, where h···i :=R ∞
0 (··· )P (x | b, w)dx.
The minimum of the variational free energy is equivalent to
− ln Z(b, w).

However, the minimization of the variational free energy
is intractable, because it is not easy to evaluate the partition
function Z(l, r, w). Therefore, an approximative instead of
the exact minimization. By using the Plefka expansion [12],
the variational free energy can be expanded as

F (m, v; b, w) = F2nd(m, v; b, w) + O(w3

ij ),

(3)

n

n

1
2
l∗
i

wiivi +

Xi=1

F2nd(m, v; b, w)
bimi +
= −
− ln erfcx(cid:16) −
+ X(i,j)∈E

Xi=1
i (cid:17) −
p2r∗
2 X(i,j)∈E
wijmimj −

1
2

ln

1

n

π
2r∗

i mi −

Xi=1nl∗
i o
ij(cid:0)vi − m2

w2

1
2

r∗
i vi

i(cid:1)(cid:0)vj − m2
j(cid:1),

(4)

where the function erfcx(x) is the scaled complementary error
function deﬁned by

erfcx(x) :=

2ex2

√π Z ∞

x

e−z2

dz.

Equation (3) is the perturbative expansion of the variational
i and
free energy with respect to wij (i 6= j). The notations l∗
i are deﬁned by
r∗
li√2ri(cid:17)

rivi − ln erfcx(cid:16) −

{l∗
i , r∗

1
2

li,ri nlimi −
i } = arg max
ln rio.

1
2

+

Let us neglect the higher-order terms and approximate the true
variational free energy, F (m, v; b, w), by the variational free
energy in equation (4). The minimization of the approximate
variational free energy in equation (4) is straightforward. It can
be done by computationally solving the nonlinear simultaneous
equations,

(5)

mi =

i ,

l∗
i

πr∗
i

l∗
i
r∗
i

+s 2
i mi(cid:1)/r∗
vi =(cid:0)1 + l∗
i = bi − Xj∈∂(i)
i = wii − Xj∈∂(i)

erfcx(cid:16) −
p2r∗
wij mj − Xj∈∂(i)
ij(cid:0)vj − m2
j(cid:1),

w2

l∗

r∗

,

i (cid:17)−1
ij mi(cid:0)vj − m2
j(cid:1),

w2

(6)

(7)

(8)

(9)

by using an iteration method, such as a successive iteration
method. The notation ∂(i) is the set of nearest-neighbor
vertices of vertex i. Equations (8) and (9) come from the
minimum condition of equation (4) with respect to m and
v, respectively, and equations (6) and (7) come from the
maximum conditions in equation (5).

Equations (6)–(9) are referred to as the TAP equation for
the NNBM. Although the expression of this TAP equation
is different from the expression proposed in reference [11],
they are essentially almost the same. Since we neglect higher-
order effects in the true variational free energy, the expectation
values and the variances obtained by the TAP equation are
approximations, except in special cases. It
is known that,
on NNBMs which are deﬁned on complete graphs whose
sizes are inﬁnitely large and whose coupling parameters w
are distributed in accordance with a Gaussian distribution
whose variance is scaled by n−1, the TAP equation gives true
expectations [11].

IV. PROPOSED METHOD

In this section, we propose an effective mean-ﬁeld in-
ference method for the NNBM in which the TAP equation
in equations (6)–(9) is combined with the the diagonal con-
sistency method proposed in reference [13]. The diagonal
consistency method is a powerful method that can increase
the performance of mean-ﬁeld methods by using a simple
extension for them.

terms of the TAP equation. This scheme for computing sus-
ceptibilities has a decade-long history and is referred to as the
susceptibility propagation method or to as the variational liner
response method [16], [17], [18], [19]. By using this method,
we can obtain not only the expectation values of one variable
but also the covariances on NNBMs. In the next section, we
propose a version of this susceptibility propagation method
that is improved by combining it with the diagonal consistency
method.

A. Liner response relation and susceptibility propagation
method

Before addressing the proposed method, let us formulate
the linear response relation for the NNBM. The linear re-
sponse relation enables us to evaluate higher-order statistical
values, such as covariances. Here, we deﬁne the matrix χ as

Mij :=

∂m∗
i
∂bj

,

(10)

i

|

where m∗ = {m∗
i ∈ V } is the values of m that
minimize the (approximate) variational free energy. Since
relations Mij = hxixji − hxiihxji exactly hold when m∗ are
the true expectation values, we can interpret the matrix M
as the covariant matrix on the NNBM. The quantity Mij is
referred to as the susceptibility in physics, and is interpreted
as a response on vertex i for a quite small change of the
bias on vertex j. In practice, we use m∗ obtained by a
mean-ﬁeld method instead of the intractable true expectations
and approximately evaluate M by means of the mean-ﬁeld
method.

Let us use m∗ obtained by the minimization of the
approximate variational free energy in equation (4), that is the
solutions to the TAP equation in equations (6)–(9), in equation
(10) in order to approximately compute the susceptibilities. By
using equations (6)–(10), the approximate susceptibilities are
obtained by using the solutions to the nonlinear simultaneous
equations, which are

and

B. Improved susceptibility propagation method for the NNBM

In order to apply the diagonal consistency method to the
present framework, I modify the approximate variational free
energy in equation (4) as

F †
2nd(m, v; b, w, Λ) := F2nd(m, v; b, w)
Λi(cid:0)vi − m2
i(cid:1),

Xi=1

−

1
2

n

(15)

where the variables Λ = {Λi | i ∈ V } are auxiliary inde-
pendent parameters that play an important role in the diagonal
consistency method [13], [15]. We formulate the TAP equation
and the susceptibility propagation for the modiﬁed variational
free energy, F †
in the same manner as
that described in the previous sections. By this modiﬁcation,
equations (8), (9), and (13) are changed to

2nd(m, v; b, w, Λ),

l∗

i = bi − Xj∈∂(i)
− miΛi,
i = wii − Xj∈∂(i)

r∗

wij mj − Xj∈∂(i)
w2
ij vj − Λi,

wijmi(cid:0)vj − m2
j(cid:1)

(16)
(17)

wikMkj

Lij = δij − Xk∈∂(i)
− Xk∈∂(i)
− MijΛi,

w2

ik(cid:8)Mij(cid:0)vk − m2

k(cid:1) + mi(cid:0)Vkj − 2mkMkj(cid:1)(cid:9)

(18)

respectively. Equations (6)–(8) and (17) are the TAP equation,
and equations (11), (12), (14), and (18) are the susceptibility
propagation for the modiﬁed variational free energy. The
solutions to these equations obviously depend on the values
of Λ. Since the values of Λ cannot be speciﬁed by using only
the TAP equation and the susceptibility propagation, we need
an additive constraint in order to specify them.

As mentioned in sections III and IV-A, relations vi = hx2
i i
i i − hxii2 hold in the scheme with no approxi-

and Mii = hx2
mation. Hence, the relations

Mii = vi − m2

i

(19)

should always hold in the exact scheme, and therefore, these
relations can be regarded as important information that the
present system has. The diagonal consistency method is a
technique for inserting these diagonal consistency relations
into the present approximation scheme. In the diagonal con-
sistency method, the values of the auxiliary parameters Λ are
determined so that the solutions to the modiﬁed TAP equation
in equations (6)–(8) and (17) and the modiﬁed susceptibility
propagation in equations (11), (12), (14), and (18) satisfy

(11)

(12)

(13)
(14)

1
2r∗

i vi − m2
i (cid:0)l∗
i l∗
i(cid:1)Lij −
i Mij − viRij
,
r∗
i
wikMkj

i + mi(cid:1)Rij ,

Mij =(cid:0)vi − m2

miLij + l∗

Vij =

Lij = δi,j − Xk∈∂(i)

− Xk∈∂(i)
Rij = − Xk∈∂(i)

k(cid:1) + mi(cid:0)Vkj − 2mkMkj(cid:1)(cid:9),

w2

ik(cid:8)Mij(cid:0)vk − m2
ik(cid:0)Vkj − 2mkMkj(cid:1),

w2

i /∂bj, and Rij := ∂r∗

where Vij := ∂vi/∂bj, Lij := ∂l∗
i /∂bj.
The expression δi,j is the Kronecker delta. The variables m,
v, l∗, and r∗ in equations (11) and (12) are the solutions to
the TAP equation in equations (6)–(9). Equations (11)–(14)
are obtained by differentiating equations (6)–(9) with respect
to bj. Note that, the expression in equations (11) and (12) is
simpliﬁed by using equations (6) and (7).

After obtaining solutions to the TAP equation in equations
(6)–(9), by computationally solving equations (11)–(14) by
using an iterative method, such as the successive iteration
method, we can obtain approximate susceptibilities M in

the diagonal consistency relations in equation (19). From
equations (11), (18), and (19), we obtain
Bi
(vi − m2

vi − m2

i )2 ,

Λi =

(20)

Ai

+

i

where

w2

wikχki

1
2r∗

k(cid:1) + mi(cid:0)Vki − 2mkχki(cid:1)(cid:9),

i(cid:1)(cid:0)vk − m2
i + mi(cid:1)Rij.

ik(cid:8)(cid:0)vi − m2
i vi − m2
i l∗
i (cid:0)l∗

Ai := − Xk∈∂(i)
− Xk∈∂(i)
Bi := −
Equation (20) is referred to as the diagonal matching equation,
and we determine the values of Λ by using this equation in
the framework of the diagonal consistency method [13], [15].
By computationally solving the modiﬁed TAP equation, the
modiﬁed susceptibility propagation, and the diagonal matching
equation, the expectations m and susceptibilities (covariances)
M that we can obtain are improved by applying the diagonal
consistency method. It is noteworthy that the order of the
computational cost of the proposed method is the same as
that of the normal susceptibility propagation method, which is
O(n|E|).
In the normal scheme presented in section IV-A, the results
obtained from the susceptibility propagation method are not
fed back to the TAP equation. In contrast, in the improved
scheme proposed in this section, they are fed back to the TAP
equation through parameters Λ, because the TAP equation and
the susceptibility propagation method share the parameters Λ
in this scheme (see ﬁgure 1). Instinctually, the role of the

V. NUMERICAL VERIFICATION OF PROPOSED METHOD

To verify performance of the proposed method, let us
consider statistical machine learning on NNBMs. For a given
complete data set D = {x(µ) | µ = 1, 2, . . . , N} generated
from a generative NNBM, the log-likelihood function of the
learning NNBM is deﬁned as

L(b, w) :=

1
N

ln P (x(µ) | b, w).

N

Xµ=1

This log-likelihood is rewritten as

L(b, w) =

bihxiiD −

n

Xi=1
− X(i,j)∈E

n

1
2

i iD

Xi=1

wiihx2
wijhxixjiD + min

m,v F (m, v; b, w),

(21)
where notation h···iD denotes the sample average with respect
to the given data set. The learning is achieved by maximizing
the log-likelihood function with respect to b and w. The gra-
dients of the log-likelihood function are expressed as follows:

∆bi ∝ hxiiD − mi,
∆wii ∝ −hx2
i iD + vi,
∆wij ∝ −hxixjiD +(cid:0)Mij + mimj(cid:1).

We use m, v, and M obtained by the approximative methods
presented in the previous sections in these gradients, and
approximately maximize the log-likelihood function. More
accurate estimates can be expected to give better learning
solutions. We measured the quality of the learning by the
mean absolute errors (MAEs) between the true parameters and
learning solutions deﬁned by

TAP equation:  m, v

TAP equation:  m, v

diagonal matching:  Λ

susceptibility propagation:  M, V

susceptibility propagation:  M, V

eb :=

ew :=

(a)

(b)

Fig. 1.
Illustration of schemes of (a) normal susceptibility propagation and
(b) proposed method. The procedure of the normal susceptibility propagation
method is straightforward. In contrast, the results of the susceptibility propa-
gation method are fed back to the TAP equation through parameters Λ in the
improved susceptibility propagation.

parameters Λ can be interpreted as follows. As mentioned
in section IV-A,
the susceptibilities M are interpreted as
responses to the small changes in the biases on the vertices.
Self-response Mii, which is a response on vertex i to a small
change in the bias on vertex i, is interpreted as a response
coming back through the whole system. Thus,
it can be
expected that the self-responses have some information of the
whole system. In the proposed scheme, the self-responses,
which differ from the true response by the approximation, are
corrected by the diagonal consistency condition in equation
(19), and subsequently the information of the whole system,
which the self-responses have, are embedded into parameters
Λ and are transmitted to the TAP equation through parameters
Λ.

n

1
n

i

1

i − blearn

Xi=1(cid:12)(cid:12)btrue
(cid:12)(cid:12),
n + |E|(cid:16) X(i,j)∈E(cid:12)(cid:12)wtrue
ij − wlearn
(cid:12)(cid:12)(cid:17),
Xi=1(cid:12)(cid:12)wtrue

ii − wlearn

ij

ii

n

+

(cid:12)(cid:12)

(22)

where btrue and wtrue are the true parameters on the generative
NNBM. In the following sections, the results of the learning
on two kinds of generative NNBMs are shown. In both cases,
we set to n = 36 and N = 10000 and generated the data
sets by using the Markov chain Monte Carlo method on the
generative NNBMs.

A. Square grid NNBM with random biases

Let us consider the generative NNBM on a 6 × 6 square

grid graph whose parameters are

bi ∼i.i.d. U(−0.4, 0.4), wii = 1, wij = 0.8,

(23)

where U(α, β) is the unique distribution from α to β. Data set
D was generated from this generative NNBM, and used to train
the learning NNBM. Table I shows the MAEs obtained using
the approximative methods. “SusP” are the results obtained
by the normal susceptibility propagation presented in section
IV-A, and “I-SusP” are the results obtained by the improved

TABLE I.
MAES BETWEEN TRUE PARAMETERS AND LEARNING
PARAMETERS IN EQUATION (22). THE COMPLETE DATA SET D ARE

GENERATED FROM THE SQUARE GRID NNBM IN EQUATION (23). EACH

RESULT IS THE AVERAGE OVER 50 TRIALS.

Downs [5]

0.515
0.376

SusP
0.297
0.095

I-SusP
0.153
0.060

eb
ew

APPENDIX A

BRIEF DERIVATION OF VARIATIONAL FREE ENERGY

To derive the variational free energy in equation (2), let
us consider the following functional with a trial probability
density function q(x):

susceptibility propagation presented in section IV-B. “Downs“
are the results obtained by the mean-ﬁeld learning algorithm
proposed by Downs [5]. We can see that the proposed im-
proved method (I-SusP) outperformed the other methods.

B. Neural network model for orientation tuning

Let us consider a neural network model for orientation
tuning in the primary visual cortex, which can be represented
by a cooperative NNBM distribution [20], [4], [5]. The energy
function of the neural network model
is a translationally
invariant function of the angles of maximal response of the
n neurons, and can be mapped directly onto the energy of an
NNBM whose parameters are
1
n −

bi = β, wij = β(cid:16)δi,j +
cos(cid:0)2πn−1|i − j|(cid:1)(cid:17), (24)
where β = 10 and ε = 2. Data set D was generated from
this generative NNBM, and used to train the learning NNBM.
Table II shows the MAEs obtained using the approximative
methods. We can see that the proposed improved method (I-

ε
n

MAES BETWEEN TRUE PARAMETERS AND LEARNING
TABLE II.
PARAMETERS IN EQUATION (22). THE COMPLETE DATA SET D WAS

GENERATED FROM THE SQUARE GRID NNBM IN EQUATION (24). EACH

RESULT IS THE AVERAGE OVER 50 TRIALS.

Downs [5]

10.772
0.700

SusP
1.644
0.151

I-SusP
0.480
0.107

eb
ew

SusP) again outperformed the other methods.

VI. CONCLUSION

In this paper, an effective inference method for NNBMs
was proposed. This inference method was constructed by using
the TAP equation and the diagonal consistency method, which
was recently proposed, and performed better than the normal
susceptibility propagation in the numerical experiments of
statistical machine learning on NNBMs described in section
V. Moreover,
the order of the computational cost of the
proposed inference method is the same as that of the normal
susceptibility propagation, which is O(n|E|).
The proposed method was developed based on the second-
order perturbative approximation of the free energy in equation
(4). Since the Plefka expansion allows us to derive higher-
order approximations of the free energy, we should be able
to formulate improved susceptibility propagations base on the
higher-order approximations in a similar way to the presented
method. It should be addressed in future studies. In reference
[21], a TAP equation for more general continuous MRFs,
where random variables are bounded within ﬁnite values, was
proposed. We can formulate an effective inference method for
continuous MRFs by combining the TAP equation with the
diagonal consistency method in a way similar to that described
in this paper. This topic should be also considered in future
studies.

f [q] :=Z ∞

0

E(x; b, w)q(x)dx +Z ∞

0

q(x) ln q(x)dx,

and consider the minimization of this functional under con-
straints:

mi =Z ∞

0

xiq(x)dx,

x2
i q(x)dx.

vi =Z ∞

0

By using the Lagrange multipliers, this minimization yields
equation (2), i.e.,

F (m, v; b, w) = min

q (cid:8)f [q] | constraints(cid:9).

This variational free energy is referred to as the Gibbs free en-
ergy in statistical physics, and the minimum of the variational
free energy coincides with − ln Z(b, w) [11].

ACKNOWLEDGMENT

This work was partly supported by Grants-In-Aid (No.
24700220) for Scientiﬁc Research from the Ministry of Ed-
ucation, Culture, Sports, Science and Technology, Japan.

REFERENCES

[1]

[2]

[3]

J. Pearl: Probabilistic reasoning in intelligent systems: Networks of
plausible inference (2nd ed.), San Francisco, CA: Morgan Kaufmann,
1988.
J. S. Yedidia and W. T. Freeman: Constructing free-energy approxima-
tions and generalized belief propagation algorithms, IEEE Trans. on
Information Theory, Vol. 51, pp. 2282–2312, 2005.
J. S. Yedidia and W. T. Freeman: Correctness of belief propagation in
gaussian graphical models of arbitrary topology, Neural Computation,
Vol. 13, pp. 2173-2200, 2001.

[4] O. B. Downs, D. J. C. MacKay and D. D. Lee: The nonnegative Boltz-
mann machine, Advances in Neural Information Processing Systems,
Vol. 12, pp. 428–434, 2000.

[5] O. B. Downs: High-temperature expansions for learning models of
nonnegative data, Advances in Neural Information Processing Systems,
Vol. 13, pp. 465–471, 2001.

[6] N. D. Socci, D. D. Lee and H. S. Seung: The rectiﬁed Gaussian
distribution, Advances in Neural Information Processing Systems, Vol.
10, pp. 350–356, 1998.

[7] P. Paatero, U. Tapper: Positive matrix factorization: a nonnegative
factor model with optimal utilization of error estimates of data values,
Environmetrics, Vol. 5, pp. 111–126, 1994.

[8] D. D. Lee, H. S. Seung: Learning the parts of objects by nonnegative

matrix factorization, Nature, Vol. 401, pp. 788–791, 1999.

[9] M. Plumbley, E. Oja: A “nonnegative PCA” algorithm for independent
component analysis, IEEE Trans. Neural Networks, Vol. 15, pp. 66–76,
2004.

[10] M. Harva and A. Kab´an: Variational

learning for rectiﬁed factor

analysis, Signal Processing, Vol.87, pp.509–527, 2007.

[11] M. Yasuda and K. Tanaka: TAP equation for nonnegative Boltzmann

machine, Philosophical Magazine, Vol. 92, pp. 192–209, 2012.

[12] T. Plefka: Convergence condition of the TAP equation for the inﬁnite-
range Ising spin glass model, J. Phys. A: Math. & Gen., Vol. 15, pp.
1971–1978, 1982.

[13] M. Yasuda and K. Tanaka: Susceptibility propagation by using diagonal

[14]

consistency, Physical Review E, Vol. 87, pp. 012134, 2013.
J. Raymond and F. Ricci-Tersenghi: Correcting beliefs in the mean-
ﬁeld and Bethe approximations using linear response, Proc. IEEE
ICC’13 Workshop on Networking across Disciplines: Communication
Networks, Complex Systems and Statistical Physics (NETSTAT), 2013.

[15] M. Yasuda: A generalization of improved susceptibility propagation,

Journal of Physics: Conference Series, Vol. 473, pp. 012006, 2013.

[16] K. Tanaka: Probabilistic inference by mean of cluster variation method
and linear response theory, IEICE Trans. on Information and Systems,
Vol. E86-D, pp. 1228–1242, 2003.

[17] M. Welling and Y. W. Teh: Approximate inference in Boltzmann

machines, Artiﬁcial Intelligence, Vol.143, pp.19–50, 2003.

[18] M. Welling and Y. W. Teh: Linear response algorithms for approximate
inference in graphical models, Neural Computation, Vol.16, 197–221,
2004.

[19] M. M´ezard and T. Mora: Constraint satisfaction problems and neural
networks: A statistical physics perspective, Journal of Physiology-Paris,
Vol. 103, pp. 107–113, 2009.

[20] R. Ben-Yishai, R. Lev Bar-Or and H. Sompolinsky: Theory of orien-
tation tuning in visual cortex, Proc. Nat. Acad. Sci. USA, Vol. 92, pp.
3844–3848, 1995.

[21] M. Yasuda and K. Tanaka: Boltzmann machines with bounded contin-
uous random variables, Interdisciplinary Information Sciences, Vol. 13,
pp. 25–31, 2007.

