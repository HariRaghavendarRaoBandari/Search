6
1
0
2

 
r
a

M
2

 

 
 
]

.

R
P
h
t
a
m

[
 
 

1
v
7
9
8
0
0

.

3
0
6
1
:
v
i
X
r
a

THE DEVIATION OF RANDOM MATRICES ON GEOMETRIC SETS

A SIMPLE TOOL FOR BOUNDING

CHRISTOPHER LIAW, ABBAS MEHRABIAN, YANIV PLAN, AND ROMAN VERSHYNIN

Abstract. Let A be an isotropic, sub-gaussian m × n matrix. We prove that the process Zx :=
kAxk2−√m kxk2 has sub-gaussian increments, that is, kZx − Zykψ2 ≤ Ckx− yk2 for any x, y ∈ Rn.
Using this, we show that for any bounded set T ⊆ Rn, the deviation of kAxk2 around its mean
is uniformly bounded by the Gaussian complexity of T . We also prove a local version of this
theorem, which allows for unbounded sets. These theorems have various applications, some of
which are reviewed in this paper. In particular, we give a new result regarding model selection in
the constrained linear model.

1. Introduction

Recall that a random variable Z is sub-gaussian if its distribution is dominated by a normal
distribution. One of several equivalent ways to deﬁne this rigorously is to require the Orlicz norm

kZkψ2 := inf(cid:8)K > 0 : E ψ2(|Z|/K) ≤ 1}

to be ﬁnite, for the Orlicz function ψ2(x) = exp(x2) − 1. Also recall that a random vector X in
Rn is sub-gaussian if all of its one-dimensional marginals are sub-gaussian random variables; this
is quantiﬁed by the norm

kXkψ2 := sup

θ∈Sn−1(cid:13)(cid:13)hX, θi(cid:13)(cid:13)ψ2

.

For basic properties and examples of sub-gaussian random variables and vectors, see e.g. [27].

In this paper we study isotropic, sub-gaussian random matrices A. This means that we require

the rows Ai of A to be independent, isotropic, and sub-gaussian random vectors:

E AiAT

i = I,

(1.1)
Suppose A is an m × n isotropic, sub-gaussian random matrix, and T ⊂ Rn is a given set. We
are wondering when A acts as an approximate isometry on T , that is, when kAxk2 concentrates
near the value (EkAxk2
Such a uniform deviation result must somehow depend on the “size” of the set T . A simple way
to quantify the size of T is through the Gaussian complexity

2)1/2 = √mkxk2 uniformly over vectors x ∈ T .

kAikψ2 ≤ K.

γ(T ) := E sup

x∈T |hg, xi| where g ∼ N (0, In).

(1.2)

One can often ﬁnd in the literature the following translation-invariant cousin of Gaussian complex-
ity, called the Gaussian width of T :

w(T ) := E sup

x∈T hg, xi =

1
2

x∈T −T hg, xi .
E sup

Date: March 4, 2016.
C.L. is partially supported by supervisor’s NSERC grant. A.M. is supported by an NSERC Postdoctoral Fellowship.
Y.P. is partially supported by NSERC grant 22R23068. R.V. is partially supported by NSF grant DMS 1265782 and
USAF Grant FA9550-14-1-0009.

1

These two quantities are closely related. Indeed, a standard calculation shows that

1

3(cid:2)w(T ) + kyk2(cid:3) ≤ γ(T ) ≤ 2(cid:2)w(T ) + kyk2(cid:3)

for every y ∈ T.

(1.3)

The reader is referred to [20, Section 2], [28, Section 3.5] for other basic properties of Gaussian
width. Our main result is that the deviation of kAxk2 over T is uniformly bounded by the Gaussian
complexity of T .

Theorem 1.1 (Deviation of random matrices on sets). Let A be an isotropic, sub-gaussian random
matrix as in (1.1), and T be a bounded subset of Rn. Then

E sup

x∈T(cid:12)(cid:12)kAxk2 − √mkxk2(cid:12)(cid:12) ≤ CK 2 · γ(T ).

(Throughout, c and C denote absolute constants that may change from line to line). For Gaussian
random matrices A, this theorem follows from a result of G. Schechtman [23]. For sub-gaussian
random matrices A, one can ﬁnd related results in [10, 13, 4]. Comparisons with these results can
be found in Section 3.

The dependence of the right-hand-side of this theorem on T is essentially optimal. This is not
hard to see for m = 1 by a direct calculation. For general m, optimality follows from several
consequences of Theorem 1.1 that are known to be sharp; see Section 2.5.

Our proof of Theorem 1.1 given in Section 4.1 is particularly simple, and is inspired by the
approach of G. Schechtman [23]. He showed that for Gaussian matrices A, the random process
Zx := kAxk2 − (EkAxk2

2)1/2 indexed by points x ∈ Rn, has sub-gaussian increments, that is
kZx − Zykψ2 ≤ Ckx − yk2

for every x, y ∈ Rn.

(1.4)
Then Talagrand’s Majorizing Measure Theorem implies the desired conclusion that1 E supx∈T |Zx| .
γ(T ).
However, it should be noted that G. Schechtman’s proof of (1.4) makes heavy use of the rotation
invariance property of the Gaussian distribution of A. When A is only sub-gaussian, there is no
rotation invariance to rely on, and it was unknown if one can transfer G. Schechtman’s argument to
this setting. This is precisely what we do here: we show that, perhaps surprisingly, the sub-gaussian
increment property (1.4) holds for general sub-gaussian matrices A.

Theorem 1.2 (Sub-gaussian process). Let A be an isotropic, sub-gaussian random matrix as in
(1.1). Then the random process Zx := kAxk2 − (EkAxk2
increments:

2)1/2 = kAxk2 − √mkxk2 has sub-gaussian
for every x, y ∈ Rn.

kZx − Zykψ2 ≤ CK 2kx − yk2

(1.5)

The proof of this theorem, given in Section 5, essentially consists of a couple of non-trivial
applications of Bernstein’s inequality; parts of the proof are inspired by G. Schechtman’s argument.
Applying Talagrand’s Majorizing Measure Theorem (see Theorem 4.1 below), we immediately
obtain Theorem 1.1.

We also prove a high-probability version of Theorem 1.1.

Theorem 1.3 (Deviation of random matrices on sets: tail bounds). Under the assumptions of
Theorem 1.1, for any u ≥ 0 the event

sup

x∈T(cid:12)(cid:12)kAxk2 − √mkxk2(cid:12)(cid:12) ≤ CK 2(cid:2)w(T ) + u · rad(T )(cid:3)

holds with probability at least 1 − exp(−u2). Here rad(T ) := supx∈T kxk2 denotes the radius of T .

1In this paper, we sometimes hide absolute constants in the inequalities marked ..

2

sup

x∈T(cid:12)(cid:12)kAxk2 − √mkxk2(cid:12)(cid:12) ≤ CK 2u · γ(T )

(1.6)

This result will be deduced in Section 4.1 from a high-probability version of Talagrand’s theorem.
In the light of the equivalence (1.3), notice that Theorem 1.3 implies the following simpler but

weaker bound

if u ≥ 1. Note that even in this simple bound, γ(T ) cannot be replaced with the Gaussian width
w(T ), e.g. the result would fail for a singleton T . This explains why the radius of T appears in
Theorem 1.3.

Restricting the set T to the unit sphere, we obtain the following corollary.

Corollary 1.4. Under the assumptions of Theorem 1.1, for any u ≥ 0 the event

sup

x∈T ∩Sn−1(cid:12)(cid:12)kAxk2 − √m(cid:12)(cid:12) ≤ CK 2(cid:2)w(T ∩ Sn−1) + u(cid:3)

holds with probability at least 1 − exp(−u2).

In Theorems 1.1 and 1.3, we assumed that the set T is bounded. For unbounded sets, we can still
prove a ‘local version’ of Theorem 1.3. Let us state a simpler form of this result here. In Section 6,
we will prove a version of the following theorem with a better probability bound.

Theorem 1.5 (Local version). Let (Zx)x∈Rn be a random process with sub-gaussian increments as
in (1.5) and with Z0 = 0. Let T be a star-shaped2 subset of Rn, and let t ≥ 1. With probability at
least 1 − exp(−t2), we have

|Zx| ≤ t · CK 2γ (T ∩ kxk2Bn
2 )

for all x ∈ T.

(1.7)

Combining with Theorem 1.2, we immediately obtain the following result.

Theorem 1.6 (Local version of Theorem 1.3). Let A be an isotropic, sub-gaussian random matrix
as in (1.1), and let T be a star-shaped subset of Rn, and let t ≥ 1. With probability at least
1 − exp(−t2), we have

for all x ∈ T.

(1.8)

(cid:12)(cid:12)(cid:12)kAxk2 − √mkxk2(cid:12)(cid:12)(cid:12) ≤ t · CK 2γ (T ∩ kxk2Bn

2 )

Remark 1.7. We note that Theorems 1.5 and 1.6 can also apply when T is not a star-shaped set,
simply by considering the smallest star-shaped set that contains T :

star(T ) := [λ∈[0,1]

λ · T.

Then one only needs to replace T by star(T ) in the right-hand side of Equations (1.7) and (1.8).

Results of the type of Theorems 1.1, 1.3 and 1.6 have been useful in a variety of applications.

For completeness, we will review some of these applications in the next section.

2. Applications

Random matrices have proven to be useful both for modeling data and transforming data in
a variety of ﬁelds. Thus, the theory of this paper has implications for several applications. A
number of classical theoretical discoveries as well as some new results follow directly from our main
theorems. In particular, the local version of our theorem (Theorem 1.6), allows a new result in
model selection under the constrained linear model, with applications in compressed sensing. We
give details below.

2Recall that a set T is called star-shaped if t ∈ T implies λt ∈ T for all 0 ≤ λ ≤ 1.

3

2.1. Singular values of random matrices. The singular values of a random matrix are an
important topic of study in random matrix theory. A small sample includes covariance estimation
[26], stability in numerical analysis [29], and quantum state tomography [8].

Corollary 1.4 may be specialized to bound the singular values of a sub-gaussian matrix. Indeed,

take T = Sn−1 and note that w(T ) ≤ √n. Then the corollary states that, with high probability,

(cid:12)(cid:12)kAxk2 − √m(cid:12)(cid:12) ≤ CK 2√n

for all x ∈ Sn−1.

This recovers the well-known result that, with high probability, all of the singular values of A
reside in the interval [√m − CK 2√n,√m + CK 2√n] (see [27]). When nK 4 ≪ m, all of the
singular values concentrate around √m. In other words, a tall random matrix is well conditioned
with high probability.

2.2. Johnson-Lindenstrauss lemma. The Johnson-Lindenstrauss lemma [9] describes a simple
and eﬀective method of dimension reduction. It shows that a (ﬁnite) set of data vectors X belonging
to a very high-dimensional space, Rn, can be mapped to a much lower dimensional space while
roughly preserving pairwise distances. This is useful from a computational perspective since the
storage space and the speed of computational tasks both improve in the lower dimensional space.
Further, the mapping can be done simply by multiplying each vector by the random matrix A/√m.
Indeed, take
T ′ = X − X . To construct T , remove the 0 vector from T ′ and project all of the remaining vectors
onto Sn−1 (by normalizing). Since T belongs to the sphere and has fewer than |X|2 elements, it is
not hard to show that γ(T ) ≤ Cplog |X|. Then by Corollary 1.4, with high probability,

The classic Johnson-Lindenstrauss lemma follows immediately from our results.

Equivalently, for all x, y ∈ X
(1 − δ)kx − yk2 ≤

sup

x∈T(cid:12)(cid:12)(cid:12)(cid:12)

1

√mkAxk2 − 1(cid:12)(cid:12)(cid:12)(cid:12)

CK 2plog |X|

√m

.

≤

1
√mkA(x − y)k2 ≤ (1 + δ)kx − yk2,

δ =

CK 2plog |X|

√m

.

This is the classic Johnson-Lindenstrauss lemma. It shows that as long as m ≫ K 4 log |X|, the
mapping x → Ax/√m nearly preserves pair-wise distances. In other words, X may be embedded
into a space of dimension slightly larger than log |X| while preserving distances.
In contrast to the classic Johnson-Lindenstrauss lemma that applies only to ﬁnite sets X , the
argument above based on Corollary 1.4 allows X to be inﬁnite.
In this case, the size of X is
quantiﬁed using the notion of Gaussian width instead of cardinality.
To get even more precise control of the geometry of X in Johnson-Lindenstrauss lemma, we may
use the local version of our results. To this end, apply Theorem 1.6 combined with Remark 1.7 to
the set T = X − X . This shows that with high probability, for all x, y ∈ X ,

CK 2γ (star(X − X ) ∩ kx − yk2Bn
2 )

.

(2.1)

√m

One may recover the classic Johnson-Lindenstrauss lemma from the above bound using the con-
tainment star(X − X ) ⊂ cone(X − X ). However, the above result also applies to inﬁnite sets, and
further can beneﬁt when X −X has diﬀerent structure at diﬀerent scales, e.g., when X has clusters.
2.3. Gordon’s Escape Theorem. In [7], Gordon answered the following question: Let T be an
arbitrary subset of Sn−1. What is the probability that a random subspace has nonempty intersection
with T ? Gordon showed that this probability is small provided that the codimension of the subspace
exceeds w(T ). This result also follows from Corollary 1.4 for a general model of random subspaces.

4

(cid:12)(cid:12)(cid:12)(cid:12)

1

√mkA(x − y)k − kx − yk(cid:12)(cid:12)(cid:12)(cid:12)

≤

Indeed, let A be an isotropic, sub-gaussian m × n random matrix as in (1.1). Then its kernel
ker A is a random subspace in Rn of dimension at least n− m. Corollary 1.4 implies that, with high
probability,

(2.2)
provided that m ≥ CK 4w(T )2. To see this, note that in this case Corollary 1.4 yields that
|kAxk2 − √m| < √m for all x ∈ T , so kAxk2 > 0 for all x ∈ T , which in turn is equivalent to (2.2).
We also note that there is an equivalent version of the above result when T is a cone. Then,

ker A ∩ T = ∅

with high probability,

ker A ∩ T = {0} provided that m ≥ CK 4γ(T ∩ Sn−1)2.

(2.3)

The conical version follows from the spherical version by expanding the sphere into a cone.

2.4. Sections of sets by random subspaces: the M ∗ Theorem. The M ∗ theorem [14, 15, 18]
answers the following question: Let T be an arbitrary subset of Rn. What is the diameter of the
intersection of a random subspace with T ? We may bound the radius of this intersection (which
of course bounds the diameter) using our main results, and again for a general model of random
subspaces.

Indeed, let us consider the kernel of an m × n random matrix A as in the previous section. By

Theorem 1.3 (see (1.6)), we have

(2.4)

with high probability. On the event that the above inequality holds, we may further restrict the
supremum to ker A ∩ T , giving

sup

x∈T(cid:12)(cid:12)kAxk2 − √mkxk2(cid:12)(cid:12) ≤ CK 2γ(T )
√mkxk2 ≤ CK 2γ(T ).

sup

x∈ker A∩T

The left-hand side is √m times the radius of T ∩ ker A. Thus, with high probability,

rad(ker A ∩ T ) ≤

CK 2γ(T )

√m

.

(2.5)

This is a classical form of the so-called M ∗ estimate. It is typically used for sets T that contain
the origin. In these cases, the Gaussian complexity γ(T ) can be replaced by Gaussian width w(T ).
Indeed, (1.3) with y = 0 implies that these two quantities are equivalent.

2.5. The size of random linear images of sets. Another question that can be addressed using
our main results is how the size of a set T in Rn changes under the action of a random linear
transformation A : Rn → Rm. Applying (1.6) and the triangle inequality, we obtain

rad(AT ) ≤ √m · rad(T ) + CK 2γ(T )

(2.6)
with high probability. This result has been known for random projections, where A = √nP and P
is the orthogonal projection onto a random m-dimensional subspace in Rn drawn according to the
Haar measure on the Grassmanian, see [2, Proposition 5.7.1].

It is also known that the bound (2.6) is sharp (up to absolute constant factor) even for random
projections, see [2, Section 5.7.1]. This in particular implies optimality of the bound in our main
result, Theorem 1.1.

5

2.6. Signal recovery from the constrained linear model. The constrained linear model is
the backbone of many statistical and signal processing problems. It takes the form

x ∈ T,

y = Ax + z,

(2.7)
where x ∈ T ⊂ Rn is unknown, y ∈ Rm is a vector of known observations, the measurement matrix
A ∈ Rm×n is known, and z ∈ Rm is unknown noise which can be either ﬁxed or random and
independent of A.
For example, in the statistical linear model, A is a matrix of explanatory variables, and x is
It is common to assume, or enforce, that only a small percentage of the
a coeﬃcient vector.
explanatory variables are signiﬁcant. This is encoded by taking T to be the set of vectors with less
than s non-zero entries, for some s ≤ n. In other words, T encodes sparsity. In another example,
y is a vector of MRI measurements [12], in which case x is the image to be constructed. Natural
images have quite a bit of structure, which may be enforced by bounding the total variation, or
requiring sparsity in a certain dictionary, each of which gives a diﬀerent constraint set T . There
are a plethora of other applications, with various constraint sets T , including low-rank matrices,
low-rank tensors, non-negative matrices, and structured sparsity. In general, a goal of interest is to
estimate x.

When T is a linear subspace, it is standard to estimate x via least squares regression, and the
performance of such an estimator is well known. However, when T is non-linear, the problem can
become quite complicated, both in designing a tractable method to estimate x and also analyzing
the performance. The ﬁeld of compressed sensing [6, 5] gives a comprehensive treatment of the
case when T encodes sparsity, showing that convex programming can be used to estimate x, and
that enforcing the sparse structure this way gives a substantial improvement over least squares
regression. A main idea espoused in compressed sensing is that random matrices A give near
optimal recovery guarantees.

Predating, but especially following, the works in compressed sensing, there have also been several
works which tackle the general case, giving results for arbitrary T [11, 25, 17, 16, 1, 3, 19, 21]. The
deviation inequalities of this paper allow for a general treatment as well. We will ﬁrst show how to
recover several known signal recovery results, and then give a new result in Section 2.7.

Consider the constrained linear model (2.7). A simple and natural way to estimate the unknown

signal x is to solve the optimization problem

ˆx := arg min

x′∈T kAx′ − yk2

2

(2.8)

We note that depending on T , the constrained least squares problem (2.8) may be computationally
tractable or intractable. We do not focus on algorithmic issues here, but just note that T may be
replaced by a larger tractable set (e.g., convexiﬁed) to aid computation.

Our goal is to bound the Euclidean norm of the error
h := ˆx − x.
Since ˆx minimizes the squared error, we have kAˆx − yk2

2 ≤ kAx − yk2

2. Simplifying this, we obtain
(2.9)

kAhk2

2 ≤ 2hh, AT zi.

We now proceed to control khk2 depending on the structure of T .
2.6.1. Exact recovery. In the noiseless case where z = 0, inequality (2.9) simpliﬁes and we have

(The second constraint here follows since h = ˆx − x and ˆx ∈ T .)
In many cases of interest, T − x is a cone, or is contained in a cone, which is called the tangent
cone or descent cone. Gordon-type inequality (2.3) then implies that h = 0, and thus we have exact

h ∈ ker A ∩ (T − x).

(2.10)

6

recovery ˆx = x, provided that the number of observations m signiﬁcantly exceeds the Gaussian
complexity of this cone: m ≥ CK 4γ((T − x) ∩ Sn−1)2.
For example, if x is a sparse vector with s non-zero entries, and T is an appropriately scaled ℓ1
ball, then T − x is contained in a tangent cone, D, satisfying γ(D)2 ≤ Cs log(n/s). This implies
that ˆx = x with high probability, provided m ≥ CK 4s log(n/s).
2.6.2. Approximate recovery. In the cases where T − x is not a cone or cannot be extended to a
narrow cone (for example, when x lies in the interior of T ), we can use the M ∗ Theorem for the
analysis of the error. Indeed, combining (2.10) with (2.5), we obtain

khk2 ≤

CK 2w(T )

√m

.

Here we also used that since T − T contains the origin, we have γ(T − T ) ∼ w(T ) according to (1.3).
In particular, this means that x can be estimated up to an additive error of ε in the Euclidean
norm provided that the number of observations satisﬁes m ≥ CK 4w(T )2/ε2.
plications for the constrained linear model, see [28].

For a more detailed description of the M ∗ Theorem, Gordon’s Escape Theorem, and their im-

2.7. Model selection for constrained linear models. It is often unknown precisely what con-
straint set to use for the constrained linear model, and practitioners often experiment with diﬀerent
constraint sets to see which gives the best performance. This is a form of model selection. We
focus on the case when the form of the set is known, but the scaling is unknown. For example,
in compressed sensing, it is common to assume that x is compressible, i.e., that it can be well
approximated by setting most of its entries to 0. This can be enforced by assuming that x belongs
to a scaled ℓp ball for some p ∈ (0, 1]. However, generally it is not known what scaling to use for
this ℓp ball.
Despite this need, previous theory concentrates on controlling the error for one ﬁxed choice of
the scaling. Thus, a practitioner who tries many diﬀerent scalings cannot be sure that the error
bounds will hold uniformly over all such scalings. In this subsection, we remove this uncertainty by
showing that the error in constrained least squares can be controlled simultaneously for an inﬁnite
number of scalings of the constraint set.

version of T :

Assume x ∈ T , but the precise scaling of T is unknown. Thus, x is estimated using a scaled
(2.11)

x′∈λT kAx′ − yk2
2,
The following corollary controls the estimation error.

ˆxλ := arg min

λ ≥ 1.

Corollary 2.1. Let T be a convex, symmetric set. Given λ ≥ 1, let ˆxλ be the solution to (2.11).
Let hλ := ˆxλ − x, let vλ = hλ/(1 + λ), and let δ = kvλk2. Then with probability at least 0.99, the
following occurs. For every λ ≥ 1,

.

(2.12)

CK 2γ(T ∩ δBn
2 )

√m

δ ≤

+ CKsγ(T ∩ δBn

m(1 + λ)

2 ) · kzk2

The corollary is proven using Theorem 1.6. To our knowledge, this corollary is new. It recovers
previous results that only apply to a single, ﬁxed λ, as in [19, 11]. It is known to be nearly minimax
optimal for many constraint sets of interest and for stochastic noise term z, in which case kzk2 would
be replaced by its expected value [21].
The rather complex bound of Equation (2.12) seems necessary in order to allow generality. To
aid understanding, we specialize the result to a very simple set—a linear subspace—for which
the behaviour of constrained least squares is well known, the scaling becomes irrelevant, and the
result simpliﬁes signiﬁcantly. When T is a d-dimensional subspace, we may bound the Gaussian

7

complexity as γ(T ∩ δB2) ≤ δ√d. Plugging in the bound on γ(T ∩ δBn

2 ) into (2.12), substituting

hλ back in, and massaging the equation gives
dkzk2
m2

2 ≤ CK 4 ·

khλk2

2

as long as m ≥ CK 4d.

If z is Gaussian noise with standard deviation σ, then it’s norm concentrates around √mσ, giving
(with high probability)

khλk2

2 ≤ CK 4 ·

dσ2
m

as long as m ≥ CK 4d.

In other words, the performance of least squares is proportional to the noise level multiplied by the
dimension of the subspace, and divided by the number of observations, m. This is well known.

In this corollary, for simplicity we assumed that T is convex and symmetric. Note that this
already allows constraint sets of interest, such as the ℓ1 ball. However, this assumption can be
weakened. All that is needed is for T − λT to be contained in a scaled version of T , and to be star
shaped. This also holds, albeit for more complex scalings, for arbitrary ℓp balls with p > 0.

Proof of Corollary 2.1. For simplicity of notation, we assume K ≤ 10 (say), and absorb K into
other constants. The general case follows the same proof. First note that hλ ∈ λT − T . Since T is
convex and symmetric, we have λT − T ⊂ (1 + λ)T and as vλ = hλ/(1 + λ), we get

Moreover, (2.9) gives

vλ ∈ T.

kAvλk2

2 ≤ hvλ, AT zi

1 + λ

,

vλ ∈ T.

(2.13)

(2.14)

We will show that, with high probability, any vector vλ satisfying (2.13) and (2.14) has a small
norm, thus completing the proof. We will do this by upper bounding hvλ, AT zi and lower bounding
kAvλk2 by kvλk2 minus a deviation term.
For the former goal, let w := AT z/kzk2. Recall that the noise vector z is ﬁxed (and in case z
random and independent of A, condition on z to make it ﬁxed). Then w is a sub-gaussian vector
with independent entries whose sub-gaussian norm is upper-bounded by a constant; see [27]. Thus,
the random process Zx := hx, wi has the sub-gaussian increments required in Theorem 1.5 (again,
see [27]). By this theorem, with probability ≥ 0.995,
|Zx| ≤ Cγ(T ∩ kxk2Bn
2 )

for all x ∈ T.

Let F be the ‘good’ event that the above equation holds.

To control kAvλk2, consider the ‘good’ event G that

kAxk2 ≥ √mkxk2 − Cγ(T ∩ kxk2Bn

2 )

for all x ∈ T.

By Theorem 1.6, G holds with probability at least 0.995.

Now, suppose that both G and F hold (which occurs with probability at least 0.99 by the union

bound). We will show that for every λ > 1, vλ is controlled. The event G gives

The event F gives

hvλ, AT zi ≤ Cγ(T ∩ kvλk2Bn
2 ) · kzk2.
kAvλk2 ≥ √mkvλk2 − Cγ(T ∩ kvλk2Bn

2 ).

8

Taking square roots of both sides of (2.14) and plugging in these two inequalities gives (2.12). (cid:3)

3. Comparison with known results

Several partial cases of our main results have been known. As we already mentioned, the special
case of Theorem 1.1 where the entries of A have standard normal distribution follows from the
main result of the paper by G. Schechtman [23].

Generalizing the result of [23], B. Klartag and S. Mendelson proved the following theorem.

Theorem 3.1 (Theorem 4.1 in [10]). Let A be an isotropic, sub-gaussian random matrix as in
(1.1), and let T ⊆ Sn−1. Assume that w(T ) ≥ C ′(K).3 Then with probability larger than 1/2,

(3.1)

Here C ′(K) and C(K) may depend on K only.

sup

x∈T(cid:12)(cid:12)kAxk2 − √m(cid:12)(cid:12) ≤ C(K)w(T ).

A similar but slightly more informative statement follows from our main results. Indeed, Corol-
lary 1.4 gives the same conclusion, but with explicit dependence on K (the sub-gaussian norms of
the rows of A) as well as probability of success. Moreover, our general results, Theorems 1.1 and
1.3, do not require the set T to lie on the unit sphere.

Another related result was proved by S. Mendelson, A. Pajor, and N. Tomczak-Jaegermann.

Theorem 3.2 (Theorem 2.3 in [13]). Let A be an isotropic, sub-gaussian random matrix as in
(1.1), and T be a star-shaped subset of Rn. Let 0 < θ < 1. Then with probability at least 1 −
exp(−cθ2m/K 4) we have that all vectors x ∈ T with

kxk2 ≥ r∗ := inf(cid:8)ρ > 0 : ρ ≥ CK 2γ(cid:0)T ∩ ρ · Sn−1(cid:1) /(θ√m)(cid:9)

satisfy

(1 − θ)kxk2

2 ≤ kAxk2

2

m ≤ (1 + θ)kxk2
2 .

Applying our Theorem 1.3 to the bounded set T ∩ r∗ · Sn−1 precisely implies Theorem 3.2
with the same failure probability (up to the values of the absolute constants c, C). Moreover, our
Theorem 1.3 treats all x ∈ T uniformly, whereas Theorem 3.2 works only for x with large norm.
Yet another relevant result was proved by Dirksen [4, Theorem 5.5]. He showed that the inequal-
ity

2 − mkxk2

2(cid:12)(cid:12) . K 2w(T )2 + √mK 2 rad(T )w(T ) + u√mK 2 rad(T )2 + u2K 2 rad(T )2

(cid:12)(cid:12)kAxk2
holds uniformly over x ∈ T with probability at least 1 − exp(−u2). To compare with our results,
one can see that Theorem 1.3 implies that, with the same probability,
2 − mkxk2
(cid:12)(cid:12)kAxk2
2(cid:12)(cid:12) .
K 4w(T )2 + √mK 2kxk2w(T ) + u√mK 2 rad(T )kxk2 + uK 4 rad(T )w(T ) + u2K 4 rad(T )2,
which is stronger than (3.2) when K = O(1), since kxk2 ≤ rad(T ) and w(T ) . √m rad(T ) whenever

(3.2)

m & n.

3This restriction is not explicitly mentioned in the statement of Theorem 4.1 in [10], but it is used in the proof.
Indeed, this result is derived from their Theorem 1.3, which explicitly requires that γ(T ) be large enough. Without
such requirement, Theorem 4.1 in [10] fails e.g. when T is a singleton, since in that case we have w(T ) = 0.

9

4. Preliminaries

4.1. Majorizing Measure Theorem, and deduction of Theorems 1.1 and 1.3. As we men-
tioned in the Introduction, Theorems 1.1 and 1.3 follow from Theorem 1.2 via Talagrand’s Majoriz-
ing Measure Theorem (and its high-probability counterpart). Let us state this theorem specializing
to processes that are indexed by points in Rn. For T ⊂ Rn, let diam(T ) := supx,y∈T kx − yk2.
Theorem 4.1 (Majorizing Measure Theorem). Consider a random process (Zx)x∈T indexed by
points x in a bounded set T ⊂ Rn. Assume that the process has sub-gaussian increments, that is
there exists M ≥ 0 such that

(4.1)

Then

kZx − Zykψ2 ≤ Mkx − yk2

for every x, y ∈ T.

x,y∈T |Zx − Zy| ≤ CM E sup
E sup

x∈T hg, xi ,

where g ∼ N (0, In). Moreover, for any u ≥ 0, the event
x,y∈T |Zx − Zy| ≤ CM(cid:2) E sup

holds with probability at least 1 − exp(−u2).

sup

x∈T hg, xi + u diam(T )(cid:3)

The ﬁrst part of this theorem can be found e.g. in [24, Theorems 2.1.1, 2.1.5]. The second part,

a high-probability bound, is borrowed from [4, Theorem 3.2].

Let us show how to deduce Theorems 1.1 and 1.3. According to Theorem 1.2, the random process

Zx := kAxk2 − √mkxk2 satisﬁes the hypothesis (4.1) of the Majorizing Measure Theorem 4.1 with
M = CK 2. Fix an arbitrary y ∈ T and use the triangle inequality to obtain

x∈T |Zx| ≤ E sup
E sup

x∈T |Zx − Zy| + E|Zy|.

Majorizing Measure Theorem bounds the ﬁrst term: E supx∈T |Zx − Zy| . K 2w(T ). (We suppress
absolute constant factors in this inequality and below.) The second term can be bounded more
easily as follows: E|Zy| . kZykψ2 . K 2kyk2, where we again used Theorem 1.2 with x = 0. Using
(1.3), we conclude that

as claimed in Theorem 1.1.

E sup

x∈T |Zx| . K 2(w(T ) + kyk2) . K 2γ(T ),

We now prove Theorem 1.3. Since adding 0 to a set does not change its radius, we may assume

that 0 ∈ T . Let Zx := kAxk2 − √mkxk2. Since Z0 = 0, and since Zx has sub-gaussian increments
by Theorem 1.2, Theorem 4.1 gives that with probability at least 1 − exp(−u2),
x∈T hg, xi + u · rad(T )(cid:3). (cid:3)
x∈T |Zx| = sup
sup
4.2. Sub-exponential random variables, and Bernstein’s inequality. Our argument will
make an essential use of Bernstein’s inequality for sub-exponential random variables. Let us brieﬂy
recall the relevant notions, which can be found, e.g., in [27]. A random variable Z is sub-exponential
if its distribution is dominated by an exponential distribution. More formally, Z is sub-exponential
if the Orlicz norm

x∈T hg, xi + u · diam(T )(cid:3) . K 2(cid:2) E sup

x∈T |Zx − Z0| . K 2(cid:2) E sup

is ﬁnite, for the Orlicz function ψ1(x) = exp(x) − 1. Every sub-gaussian random variable is sub-
exponential. Moreover, an application of Young’s inequality implies the following relation for any
two sub-gaussian random variables X and Y :

kZkψ1 := inf(cid:8)K > 0 : E ψ1(|Z|/K) ≤ 1}

kXY kψ1 ≤ kXkψ2kY kψ2.

10

(4.2)

The classical Bernstein’s inequality states that a sum of independent sub-exponential random

variables is dominated by a mixture of sub-gaussian and sub-exponential distributions.

Theorem 4.2 (Bernstein-type deviation inequality, see e.g. [27]). Let X1, . . . , Xm be independent
random variables, which satisfy E Xi = 0 and kXikψ1 ≤ L. Then

1
m

m

Xi=1

P((cid:12)(cid:12)(cid:12)

Xi(cid:12)(cid:12)(cid:12)

> t) ≤ 2 exph − cm min(cid:16) t2

L2 ,

t

L(cid:17)i,

t ≥ 0.

5. Proof of Theorem 1.2

Proposition 5.1 (Concentration of the norm). Let X ∈ Rm be a random vector with independent
coordinates Xi that satisfy E X 2

i = 1 and kXikψ2 ≤ K. Then

Remark 5.2. If E Xi = 0, this proposition follows from [22, Theorem 2.1], whose proof uses the
Hanson-Wright inequality.

(cid:13)(cid:13)(cid:13)kXk2 − √m(cid:13)(cid:13)(cid:13)ψ2 ≤ CK 2.

Proof. Let us apply Bernstein’s deviation inequality (Theorem 4.2) for the sum of independent
random variables kXk2
i − 1). These random variables have zero means and
sub-exponential norms

i=1(X 2

2 − m = Pm

kX 2

i − 1kψ1 ≤ 2kX 2

i kψ1 ≤ 2kXik2

ψ2 ≤ 2K 2.

(Here we used a simple centering inequality which can be found e.g. in [27, Remark 5.18] and the
inequality (4.2).) Bernstein’s inequality implies that

To deduce a concentration inequality for kXk2−√m from this, let us employ the numeric bound
|x2− m| ≥ √m|x−√m| valid for all x ≥ 0. Using this together with (5.1) for t = s/√m, we obtain

t

2 − m(cid:12)(cid:12) > tm(cid:9) ≤ 2 exph − cm min(cid:16) t2

P(cid:8)(cid:12)(cid:12)kXk2
P(cid:8)(cid:12)(cid:12)kXk2 − √m(cid:12)(cid:12) > s(cid:9) ≤ P(cid:8)(cid:12)(cid:12)kXk2

K 2(cid:17)i,
2 − m(cid:12)(cid:12) > s√m(cid:9)

≤ 2 exp(−cs2/K 4)

K 4 ,

for s ≤ K 2√m.

t ≥ 0.

(5.1)

To handle large s, we proceed similarly but with a diﬀerent numeric bound, namely |x2 − m| ≥
(x − √m)2 which is valid for all x ≥ 0. Using this together with (5.1) for t = s2/m, we obtain

P(cid:8)(cid:12)(cid:12)kXk2 − √m(cid:12)(cid:12) > s(cid:9) ≤ P(cid:8)(cid:12)(cid:12)kXk2

≤ 2 exp(−cs2/K 2)

2 − m(cid:12)(cid:12) > s2(cid:9)

for s ≥ K√m.

Since K ≥ 1, in both cases we bounded the probability in question by 2 exp(−cs2/K 4). This
completes the proof.
(cid:3)

Lemma 5.3 (Concentration of a random matrix on a single vector). Let A be an isotropic, sub-
gaussian random matrix as in (1.1). Then

(cid:13)(cid:13)(cid:13)kAxk2 − √m(cid:13)(cid:13)(cid:13)ψ2 ≤ CK 2

for every x ∈ Sn−1.

Proof. The coordinates of the vector Ax ∈ Rm are independent random variables Xi := hAi, xi.
The assumption that E AiAT
i = 1, and the assumption that kAikψ2 ≤ K
implies that kXikψ2 ≤ K. The conclusion of the lemma then follows from Proposition 5.1.
(cid:3)

i = I implies that E X 2

11

Lemma 5.3 can be viewed as a partial case of the increment inequality of Theorem 1.2 for

x ∈ Sn−1 and y = 0, namely

kZxkψ2 ≤ CK 2

for every x ∈ Sn−1.

(5.2)

Our next intermediate step is to extend this by allowing y to be an arbitrary unit vector.

Lemma 5.4 (Sub-gaussian increments for unit vectors). Let A be an isotropic, sub-gaussian random
matrix as in (1.1). Then

Proof. Given s ≥ 0, we will bound the tail probability

(cid:13)(cid:13)(cid:13)kAxk2 − kAyk2(cid:13)(cid:13)(cid:13)ψ2 ≤ CK 2kx − yk2
p := P((cid:12)(cid:12)kAxk2 − kAyk2(cid:12)(cid:12)

kx − yk2

for every x, y ∈ Sn−1.

> s) .

(5.3)

Case 1: s ≥ 2√m. Using the triangle inequality we have kAxk2−kAyk2 ≤ kA(x−y)k2. Denoting
u := (x − y)/kx − yk2, we ﬁnd that

p ≤ P {kAuk2 > s} ≤ P(cid:8)kAuk2 − √m > s/2(cid:9) ≤ exp(−Cs2/K 4).

Here the second bound holds since s ≥ 2√m, and the last bound follows by Lemma 5.3.
Case 2: s ≤ 2√m. Multiplying both sides of the inequality deﬁning p in (5.3) by kAxk2 +kAyk2,

we can write p as

In particular,

p = P(cid:8)|Z| > s(cid:0)kAxk2 + kAyk2(cid:1)(cid:9) where Z := kAxk2
s√m
2 (cid:27) + P(cid:26)kAxk2 ≤

p ≤ P {|Z| > skAxk2} ≤ P(cid:26)|Z| >

2 − kAyk2
kx − yk2

2

.

√m
2 (cid:27) =: p1 + p2.

We may bound p2 using Lemma 5.3:
(√m/2)2
C 2K 4 (cid:17) = 2 exp(cid:16) −

p2 ≤ 2 exp(cid:16) −

Next, to bound p1, it will be useful to write Z as

Z = hA(x − y), A(x + y)i

kx − yk2

= hAu, Avi , where u :=

m

4C 2K 4(cid:17) ≤ 2 exp(cid:16) −
x − y
kx − yk2

(5.4)

s2

16C 2K 4(cid:17).

,

v := x + y.

Since the coordinates of Au and Av are hAi, ui and hAi, vi respectively, Z can be represented as a
sum of independent random variables:

Z =

hAi, uihAi, vi .

(5.5)

m

Xi=1

Note that each of these random variables hAi, uihAi, vi has zero mean, since
EhAi, x − yihAi, x + yi = E(cid:2)hAi, xi2 − hAi, yi2(cid:3) = 1 − 1 = 0.

(Here we used the assumptions that E AiAT
i = I and kxk2 = kyk2 = 1.) Moreover, the assumption
that kAikψ2 ≤ K implies that khAi, uikψ2 ≤ Kkuk2 = K and khAi, vikψ2 ≤ Kkvk2 ≤ 2K.
Recalling inequality (4.2), we see that hAi, uihAi, vi are sub-exponential random variables with

12

khAi, uihAi, vikψ1 ≤ CK 2. Thus we can apply Bernstein’s inequality (Theorem 4.2) to the sum of
mean zero, sub-exponential random variables in (5.5), and obtain

p1 = P(cid:26)|Z| >

s√m
2 (cid:27) ≤ 2 exp(−cs2/K 4),

since s ≤ 2K 2√m.

Combining this with the bound on p2 obtained in (5.4), we conclude that

This completes the proof.

(cid:3)

p = p1 + p2 ≤ 2 exp(−cs2/K 4).

Finally, we are ready to prove the increment inequality in full generality, for all x, y ∈ Rn.

Proof of Theorem 1.2. Without loss of generality we may assume that kxk2 = 1 and kyk2 ≥ 1.
Consider the unit vector ¯y := y/kyk2 and apply the triangle inequality to get
kZx − Zykψ2 ≤ kZx − Z¯ykψ2 + kZ¯y − Zykψ2 =: R1 + R2.

By Lemma 5.4, R1 ≤ CK 2kx− ¯yk2. Next, since ¯y and y are collinear, we have R2 = k¯y−yk2·kZ¯ykψ2 .
Since ¯y ∈ Sn−1, inequality (5.2) states that kZ¯ykψ2 ≤ CK 2, and we conclude that R2 ≤ CK 2k¯y −
yk2. Combining the bounds on R1 and R2, we obtain

It is not diﬃcult to check that since kyk2 ≥ 1, we have kx− ¯yk2 ≤ kx− yk2 and k¯y− yk2 ≤ kx− yk2.
This completes the proof.
(cid:3)

kZx − Zykψ2 ≤ CK 2(cid:0)kx − ¯yk2 + k¯y − yk2(cid:1).

We will prove a slightly stronger statement. For r > 0, deﬁne

6. Proof of Theorem 1.5

Er := sup

|Zx|.

1

2

x∈

r T ∩Bn
r T ∩ Bn

Set W := limr→rad(T )− γ(cid:0) 1
for every r < rad(T ), it follows that W ≥ p2/π. We will show that, with probability at least
1 − exp(cid:0)−c′t2W 2(cid:1), one has

2 contains at least one point on the boundary

2(cid:1). Since 1

r T ∩ Bn

Er ≤ t · CK 2γ(cid:18) 1

r

T ∩ Bn

2(cid:19) for all r ∈ (0,∞),

which will clearly imply the theorem with a stronger probability.

Fix ε > 0. Let ε = r0 < r1 < . . . < rN be a sequence of real numbers satisfying the following

conditions:

ri

• γ(cid:16) 1
• γ(cid:16) 1

rN

T ∩ Bn
T ∩ Bn

2(cid:17) = 2 · γ(cid:16) 1
2(cid:17) ≤ 2 · W .

ri+1

T ∩ Bn

2(cid:17) for i = 0, 1, . . . , N − 1, and

The quantities r1, . . . , rN exist since the map r 7→ γ(cid:0) 1

r T ∩ Bn
Applying the Majorizing Measure Theorem 4.1 to the set 1

T is star-shaped.

2(cid:1) is decreasing and continuous when
r T ∩ Bn

2 and noting that Z0 = 0, we

obtain that

Er . K 2(cid:20)γ(cid:18) 1

r
13

T ∩ Bn

2(cid:19) + u(cid:21)

r T ∩ Bn

2(cid:1). We get

with probability at least 1 − exp(−u2). Set c := 10 ·p π
u = ctγ(cid:0) 1
Er . t · K 2γ(cid:18) 1
holds with probability at least 1 − exp(cid:16)−c2t2γ(cid:0) 1
Eri . t · K 2γ(cid:18) 1
with probability at least 1 − exp(cid:18)−c2t24N −iγ(cid:16) 1

r
r T ∩ Bn

have

rN

2 ≥ 10/W and use the above inequality for

(6.1)

2(cid:19)
T ∩ Bn
2(cid:1)2(cid:17). Thus for each i ∈ {0, 1, . . . , N}, we
2(cid:19)
T ∩ Bn
2(cid:17)2(cid:19) ≥ 1 − exp(cid:0)−c2t24N −iW 2(cid:1). By our

ri
T ∩ Bn

(6.2)

choice of c and the union bound, (6.2) holds for all i simultaneously with probability at least

N

Xi=0

1 −

exp(cid:0)−c2t24N −iW 2(cid:1) ≥ 1 − 2 · exp(−100t2W 2) =: 1 − exp(−c′t2W 2).

We now show that if (6.2) holds for all i, then (6.1) holds for all r ∈ (ε,∞). This is done via
an approximation argument. To this end, assume that (6.2) holds and let r ∈ (ri−1, ri) for some
i ∈ [N ]. Since T is star-shaped, we have 1
T ∩ Bn

T ∩ Bn
T ∩ Bn

2 ⊆ 1
r T ∩ Bn
2(cid:19) = 2t · K 2γ(cid:18) 1

2(cid:19) ≤ 2t · K 2γ(cid:18)1

2(cid:19) .
T ∩ Bn

2 , so

ri−1

ri

r

Er ≤ Eri−1 . t · K 2γ(cid:18) 1
ri−1
Also, for rad(T ) ≥ r > rN we have
Er . t · K 2γ(cid:18) 1

rN

T ∩ Bn

2(cid:19) ≤ 2t · K 2W ≤ 2t · K 2γ(cid:18) 1

r

2(cid:19) .
T ∩ Bn

Let Fk be the event that (6.1) holds for all r ∈ (1/k,∞). We have just shown that P {Fk} ≥
1 − exp(cid:0)−c′t2W 2(cid:1) for all k ∈ N. As F1 ⊇ F2 ⊇ . . . and ∩kFk =: F∞ is the event that (6.1) holds
for all r ∈ (0,∞), it follows by continuity of measure that P {F∞} ≥ 1 − exp(cid:0)−c′t2W 2(cid:1), thus

completing the proof.

7. Further Thoughts

In the deﬁnition of Gaussian complexity γ(T ) = E supx∈T |hg, xi|, the absolute value is essential
to make Theorem 1.1 hold. In other words, the bound would fail if we replace γ(T ) by the Gaussian
width w(T ) = E supx∈T hg, xi. This can be seen by considering a set T that consists of a single
point.
However, one-sided deviation inequalities do hold for Gaussian width. Thus a one-sided version

of Theorem 1.1 states that

E sup

x∈T(cid:16)kAxk2 − √mkxk2(cid:17) ≤ CK 2 · w(T ),

(7.1)

and the same bound holds for E supx∈T (cid:0) − kAxk2 + √mkxk2(cid:1). To prove (7.1), one modiﬁes the
2(cid:1)1/2 = √mkyk2, we
argument in Section 4.1 as follows. Fix a y ∈ T . Since EkAyk2 ≤ (cid:0)EkAyk2
have E Zy ≤ 0, thus
x∈T |Zx − Zy| . K 2w(T )

(Zx − Zy) ≤ E sup

Zx ≤ E sup

E sup
x∈T

x∈T

where the last bound follows by Majorizing Measure Theorem 4.1. Thus in this argument there is
no need to separate the term E|Zy| as was done before.

14

References

[1] Dennis Amelunxen, Martin Lotz, Michael B. McCoy, and Joel A. Tropp. Living on the edge: phase transitions

in convex programs with random data. Inf. Inference, 3(3):224–294, 2014.

[2] Shiri Artstein-Avidan, Apostolos Giannopoulos, and Vitali D. Milman. Asymptotic geometric analysis. Part I,

volume 202 of Mathematical Surveys and Monographs. American Mathematical Society, Providence, RI, 2015.

[3] Venkat Chandrasekaran, Benjamin Recht, Pablo A Parrilo, and Alan S Willsky. The convex geometry of linear

inverse problems. Foundations of Computational mathematics, 12(6):805–849, 2012.

[4] Sjoerd Dirksen. Tail bounds via generic chaining. Electron. J. Probab., 20(53):1–29, 2015.
[5] Yonina C Eldar and Gitta Kutyniok. Compressed sensing: theory and applications. Cambridge University Press,

2012.

[6] Simon Foucart and Holger Rauhut. A mathematical introduction to compressive sensing. Applied and Numerical

Harmonic Analysis. Birkh¨auser/Springer, New York, 2013.

[7] Y. Gordon. On Milman’s inequality and random subspaces which escape through a mesh in Rn. In Geometric
aspects of functional analysis (1986/87), volume 1317 of Lecture Notes in Math., pages 84–106. Springer, Berlin,
1988.

[8] David Gross, Yi-Kai Liu, Steven T Flammia, Stephen Becker, and Jens Eisert. Quantum state tomography via

compressed sensing. Physical review letters, 105(15):150401, 2010.

[9] William B Johnson and Joram Lindenstrauss. Extensions of lipschitz mappings into a hilbert space. Contemporary

mathematics, 26(189-206):1, 1984.

[10] B. Klartag and S. Mendelson. Empirical processes and random projections. J. Funct. Anal., 225(1):229–245,

2005.

[11] Guillaume Lecu´e and Shahar Mendelson. Learning subgaussian classes: Upper and minimax bounds. 2013.

Available at http://arxiv.org/abs/1305.4825.

[12] Michael Lustig, David Donoho, and John M Pauly. Sparse MRI: The application of compressed sensing for rapid

MR imaging. Magnetic resonance in medicine, 58(6):1182–1195, 2007.

[13] Shahar Mendelson, Alain Pajor, and Nicole Tomczak-Jaegermann. Reconstruction and subgaussian operators in

asymptotic geometric analysis. Geom. Funct. Anal., 17(4):1248–1282, 2007.

[14] VD Milman. Geometrical inequalities and mixed volumes in the local theory of banach spaces. Ast´erisque,

131:373–400, 1985.

[15] VD Milman. Random subspaces of proportional dimension of ﬁnite dimensional normed spaces: approach through

the isoperimetric inequality. In Banach spaces, pages 106–115. Springer, 1985.

[16] Samet Oymak, Christos Thrampoulidis, and Babak Hassibi. Simple bounds for noisy linear inverse problems

with exact side information. 2013. Available at http://arxiv.org/abs/1312.0641.

[17] Samet Oymak, Christos Thrampoulidis, and Babak Hassibi. The squared-error of generalized lasso: A precise
analysis. In 51st Annual Allerton Conference on Communication, Control, and Computing, pages 1002–1009.
IEEE, 2013.

[18] Alain Pajor and Nicole Tomczak-Jaegermann. Subspaces of small codimension of ﬁnite-dimensional banach

spaces. Proceedings of the American Mathematical Society, 97(4):637–642, 1986.

[19] Y. Plan and R. Vershynin. The generalized lasso with non-linear observations. IEEE Transactions on Information

Theory, 62(3):1528–1537, March 2016.

[20] Yaniv Plan and Roman Vershynin. Robust 1-bit compressed sensing and sparse logistic regression: a convex

programming approach. IEEE Trans. Inform. Theory, 59(1):482–494, 2013.

[21] Yaniv Plan, Roman Vershynin, and Elena Yudovina. High-dimensional estimation with geometric constraints.

2014. Available at http://arxiv.org/abs/1404.3749.

[22] Mark Rudelson and Roman Vershynin. Hanson-Wright inequality and sub-Gaussian concentration. Electron.

Commun. Probab., 18(82):1–9, 2013.

[23] Gideon Schechtman. Two observations regarding embedding subsets of Euclidean spaces in normed spaces. Adv.

Math., 200(1):125–135, 2006.

[24] Michel Talagrand. The generic chaining: Upper and lower bounds of stochastic processes. Springer Monographs

in Mathematics. Springer-Verlag, Berlin, 2005.

[25] Christos Thrampoulidis, Samet Oymak, and Babak Hassibi. Simple error bounds for regularized noisy linear
inverse problems. In IEEE International Symposium on Information Theory (ISIT), pages 3007–3011. IEEE,
2014.

[26] Roman Vershynin. How close is the sample covariance matrix to the actual covariance matrix? Journal of

Theoretical Probability, 25(3):655–686, 2012.

[27] Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. In Compressed sensing,

pages 210–268. Cambridge Univ. Press, Cambridge, 2012.

15

[28] Roman Vershynin. Estimation in high dimensions: a geometric perspective. In Sampling Theory, a Renaissance,

pages 3–66. Birkhauser Basel, 2015.

[29] John Von Neumann. Collected works. Oxford: Pergamon, 1961, edited by Taub, AH, 1961.

C.L.: Department of Computer Science, University of British Columbia, 2366 Main Mall, Vancou-

ver, BC V6T 1Z4, Canada

E-mail address: cvliaw@cs.ubc.ca

A.M.: Department of Computer Science, University of British Columbia, 2366 Main Mall, Vancou-
ver, BC V6T 1Z4, Canada, and School of Computing Science, Simon Fraser University, 8888 University
Drive, Burnaby, BC V5A 1S6, Canada
E-mail address: amehrabi@uwaterloo.ca

Y.P.: Department of Mathematics, University of British Columbia, 1984 Mathematics Rd, Vancou-

ver, BC V6T 1Z2, Canada

E-mail address: yaniv@math.ubc.ca

R.V.: Department of Mathematics, University of Michigan, 530 Church St., Ann Arbor, MI 48109,

U.S.A.

E-mail address: romanv@umich.edu

16

