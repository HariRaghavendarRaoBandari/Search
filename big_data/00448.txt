Guided Cost Learning: Deep Inverse Optimal Control via Policy Optimization

Chelsea Finn
Sergey Levine
Pieter Abbeel
University of California, Berkeley, Berkeley, CA 94709 USA

CBFINN@EECS.BERKELEY.EDU
SVLEVINE@EECS.BERKELEY.EDU
PABBEEL@EECS.BERKELEY.EDU

6
1
0
2

 
r
a

M
3

 

 
 
]

G
L
.
s
c
[
 
 

2
v
8
4
4
0
0

.

3
0
6
1
:
v
i
X
r
a

Abstract

Reinforcement learning can acquire complex be-
haviors from high-level speciﬁcations. However,
deﬁning a cost function that can be optimized
effectively and encodes the correct task is chal-
lenging in practice. We explore how inverse op-
timal control (IOC) can be used to learn behav-
iors from demonstrations, with applications to
torque control of high-dimensional robotic sys-
tems. Our method addresses two key challenges
in inverse optimal control: ﬁrst, the need for in-
formative features and effective regularization to
impose structure on the cost, and second, the dif-
ﬁculty of learning the cost function under un-
known dynamics for high-dimensional continu-
ous systems. To address the former challenge,
we present an algorithm capable of learning ar-
bitrary nonlinear cost functions, such as neural
networks, without meticulous feature engineer-
ing. To address the latter challenge, we formu-
late an efﬁcient sample-based approximation for
MaxEnt IOC. We evaluate our method on a series
of simulated tasks and real-world robotic manip-
ulation problems, demonstrating substantial im-
provement over prior methods both in terms of
task complexity and sample efﬁciency.

1. Introduction
Reinforcement learning can be used to acquire complex be-
haviors from high-level speciﬁcations. However, deﬁning
a cost function that can be optimized effectively and en-
codes the correct task can be challenging in practice, and
techniques like cost shaping are often used to solve com-
plex real-world problems (Ng et al., 1999). Inverse optimal
control (IOC) or inverse reinforcement learning (IRL) pro-
vides an avenue for addressing this challenge by learning
a cost or reward function directly from expert demonstra-
tions, e.g. Ng et al. (2000); Abbeel & Ng (2004); Ziebart
et al. (2008). However, designing an effective IOC algo-
rithm for learning from demonstration is difﬁcult for two

Ddemo

Dsamp

Dtraj

cθ

θ (cid:0) argmin

θ

cθ
LIOC

Figure 1. Right: Guided cost learning uses policy optimization
to adaptively sample trajectories for estimating the IOC partition
function. Bottom left: PR2 learning to gently place a dish in a
plate rack.

reasons. First, IOC is fundamentally underdeﬁned in that
many costs induce the same behavior. Most practical algo-
rithms therefore require carefully designed features to im-
pose structure on the learned cost. Second, many standard
IRL and IOC methods require solving the forward problem
(ﬁnding an optimal policy given the current cost) in the in-
ner loop of an iterative cost optimization. This makes them
difﬁcult to apply to complex, high-dimensional systems,
where the forward problem is itself exceedingly difﬁcult,
particularly real-world robotic problems with unknown dy-
namics.
To address the challenge of representation, we propose
to use expressive, nonlinear function approximators, such
as neural networks, to represent the cost. This reduces
the engineering burden required to deploy IOC methods,
and makes it possible to learn cost functions for which
expert intuition is insufﬁcient for designing good fea-
tures. Such expressive function approximators, however,
can learn complex cost functions that lack the structure
typically imposed by hand-designed features. To mitigate
this challenge, we propose two regularization techniques
for IOC, one which is general and one which is speciﬁc to
episodic domains, such as to robotic manipulation skills.
In order to learn cost functions for real-world robotic tasks,
our method must be able to handle unknown dynamics and
high-dimensional systems. To that end, we propose a cost
learning algorithm based on policy optimization with lo-

run policy qion robotpolicyoptimization stepoptimize costinitialdistribution q0qi+1humandemonstrationsGuided Cost LearningGuided Cost Learning

cal linear models, building on prior work in reinforcement
learning (Levine & Abbeel, 2014).
In this approach, as
illustrated in Figure 1, the cost function is learned in the
inner loop of a policy search procedure, using samples col-
lected for policy improvement to also update the cost func-
tion. The cost learning method itself is a nonlinear gener-
alization of maximum entropy IOC (Ziebart et al., 2008),
with samples used to approximate the partition function. In
contrast to previous work that optimizes the policy in the
inner loop of cost learning, our approach instead updates
the cost in the inner loop of policy search, making it prac-
tical and efﬁcient. One of the beneﬁts of this approach is
that we can couple learning the cost with learning the pol-
icy for that cost. For tasks that are too complex to acquire a
good global cost function from a small number of demon-
strations, our method can still recover effective behaviors
simply by running our policy learning method and retain-
ing the learned policy. We elaborate on this point further in
Section 4.4.
The main contribution of our work is an algorithm that
learns nonlinear cost functions from user demonstrations,
at the same time as learning a policy to perform the task.
Since the policy optimization “guides” the cost toward
good regions of the space, we call this method guided cost
learning. Unlike prior methods, our algorithm can handle
complex, nonlinear cost function representations and high-
dimensional unknown dynamics, and can be used on real
physical systems with a modest number of samples. Our
evaluation demonstrates the performance of our method on
a set of simulated benchmark tasks, showing that it outper-
forms previous methods. We also evaluate our method on
two real-world tasks learned directly from human demon-
strations. These tasks require using torque control and vi-
sion to perform a variety of robotic manipulation behaviors,
without any hand-speciﬁed cost features.

2. Related Work
One of the basic challenges in inverse optimal control
(IOC), also known as inverse reinforcement learning (IRL),
is that ﬁnding a cost or reward under which a set of demon-
strations is near-optimal is underdeﬁned. Many different
costs can explain a given set of demonstrations. Prior work
has tackled this issue using maximum margin formulations
(Abbeel & Ng, 2004; Ratliff et al., 2006), as well as prob-
abilistic models that explain suboptimal behavior as noise
(Ramachandran & Amir, 2007; Ziebart et al., 2008). We
take the latter approach in this work, building on the max-
imum entropy IOC model (Ziebart, 2010). Although the
probabilistic model mitigates some of the difﬁculties with
IOC, there is still a great deal of ambiguity, and an impor-
tant component of many prior methods is the inclusion of
detailed features created using domain knowledge, which

can be linearly combined into a cost, including: indicators
for successful completion of the task for a robotic ball-in-
cup game (Boularias et al., 2011), learning table tennis with
features that include distance of the ball to the opponent’s
elbow (Muelling et al., 2014), providing the goal position
as a known constraint for robotic grasping (Doerr et al.,
2015), and learning highway driving with indicators for
collision and driving on the grass (Abbeel & Ng, 2004).
While these features allow for the user to impose struc-
ture on the cost, they substantially increase the engineering
burden. Several methods have proposed to learn nonlinear
costs using Gaussian processes (Levine et al., 2011) and
boosting (Ratliff et al., 2007; 2009), but even these meth-
ods generally operate on features rather than raw states. We
instead use rich, expressive function approximators, in the
form of neural networks, to learn cost functions directly
on raw state representations. While neural network cost
representations have previously been proposed in the lit-
erature (Wulfmeier et al., 2015), they have only been ap-
plied to small, synthetic domains. Previous work has also
suggested simple regularization methods for cost functions,
based on minimizing (cid:96)1 or (cid:96)2 norms of the parameter vec-
tor (Boularias et al., 2011; Kalakrishnan et al., 2013) or by
using unlabeled trajectories (Audiffren et al., 2015). When
using expressive function approximators in complex real-
world tasks, we must design substantially more powerful
regularization techniques to mitigate the underspeciﬁed na-
ture of the problem, which we introduce in Section 5.
Another challenge in IOC is that, in order to determine the
quality of a given cost function, we must solve some variant
of the forward control problem to obtain the correspond-
ing policy, and then compare this policy to the demon-
strated actions. Most early IRL algorithms required solv-
ing an MDP in the inner loop of an iterative optimization
(Ng et al., 2000; Abbeel & Ng, 2004; Ziebart et al., 2008).
This requires perfect knowledge of the system dynamics
and access to an efﬁcient ofﬂine solver, neither of which
is available in, for instance, complex robotic control do-
mains. Several works have proposed to relax this require-
ment, for example by learning a value function instead of
a cost (Todorov, 2006), solving an approximate local con-
trol problem (Levine & Koltun, 2012; Dragan & Srinivasa,
2012), generating a sparse discrete graph of states (Byravan
et al., 2015), or only obtaining an optimal trajectory rather
than policy (Ratliff et al., 2006; 2009). However, these
methods still require knowledge of the system dynamics.
Given the size and complexity of the problems addressed
in this work, solving the optimal control problem even ap-
proximately in the inner loop of the cost optimization is im-
practical. We show that good cost functions can be learned
by instead learning the cost in the inner loop of a pol-
icy optimization. Our inverse optimal control algorithm is
most closely related to other previous sample-based meth-

Guided Cost Learning

ods based on the principle of maximum entropy, including
relative entropy IRL (Boularias et al., 2011) and path inte-
gral IRL (Kalakrishnan et al., 2013), which can also handle
unknown dynamics. However, unlike these prior methods,
we adapt the sampling distribution using policy optimiza-
tion. We demonstrate in our experiments that this adapta-
tion is crucial for obtaining good results on complex robotic
platforms, particularly when using complex, nonlinear cost
functions.
To summarize, our proposed method is the ﬁrst to combine
several desirable features into a single, effective algorithm:
it can handle unknown dynamics, which is crucial for real-
world robotic tasks, it can deal with high-dimensional,
complex systems, as in the case of real torque-controlled
robotic arms, and it can learn complex, expressive cost
functions, such as multilayer neural networks, which re-
moves the requirement for meticulous hand-engineering of
cost features. While some prior methods have shown good
results with unknown dynamics on real robots (Boularias
et al., 2011; Kalakrishnan et al., 2013) and some have pro-
posed using nonlinear cost functions (Ratliff et al., 2006;
2009; Levine et al., 2011), to our knowledge no prior
method has been demonstrated that can provide all of these
beneﬁts in the context of complex real-world tasks.

exp(−cθ(τ )),

1
Z

3. Preliminaries and Overview
We build on the probabilistic maximum entropy inverse op-
timal control framework (Ziebart et al., 2008). The demon-
strated behavior is assumed to be the result of an expert
acting stochastically and near-optimally with respect to an
unknown cost function. Speciﬁcally, the model assumes
that the expert samples the demonstrated trajectories {τi}
from the distribution

p(τ ) =

cθ(τ ) = (cid:80)

(1)
where τ = {x1, u1, . . . , xT , uT} is a trajectory sample,
t cθ(xt, ut) is an unknown cost function pa-
rameterized by θ, and xt and ut are the state and action
at time step t. Under this model, the expert is most likely
to act optimally, and can generate suboptimal trajectories
with a probability that decreases exponentially as the tra-
jectories become more costly. The partition function Z
is difﬁcult to compute for large or continuous domains,
and presents the main computational challenge in max-
imum entropy IOC. The ﬁrst applications of this model
computed Z exactly with dynamic programming (Ziebart
et al., 2008). However, this is only practical in small, dis-
crete domains. More recent methods have proposed to es-
timate Z by using the Laplace approximation (Levine &
Koltun, 2012), value function approximation (Huang & Ki-
tani, 2014), and samples (Boularias et al., 2011). As dis-
cussed in Section 4.1, we take the sample-based approach
in this work, because it allows us to perform inverse opti-

mal control without a known model of the system dynam-
ics. This is especially important in robotic manipulation
domains, where the robot might interact with a variety of
objects with unknown physical properties.
To represent the cost function cθ(xt, ut), IOC or IRL meth-
ods typically use a linear combination of hand-crafted fea-
tures, given by cθ(ut, ut) = θTf (ut, xt) (Abbeel & Ng,
2004). This representation is difﬁcult to apply to more
complex domains, especially when the cost must be com-
puted from raw sensory input.
In this work, we explore
the use of high-dimensional, expressive function approxi-
mators for representing cθ(xt, ut). As we discuss in Sec-
tion 6, we use neural networks that operate directly on the
robot’s state, though other parameterizations could also be
used with our method. Complex representations are gener-
ally considered to be poorly suited for IOC, since learning
costs that associate the right element of the state with the
goal of the task is already quite difﬁcult even with simple
linear representations. However, as we discuss in our evalu-
ation, we found that such representations could be learned
effectively by adaptively generating samples as part of a
policy optimization procedure, as discussed in Section 4.2.

4. Guided Cost Learning
In this section, we describe the guided cost learning al-
gorithm, which combines sample-based maximum en-
tropy IOC with forward reinforcement learning using time-
varying linear models. The central idea behind this method
is to adapt the sampling distribution to match the maximum
Z exp(−cθ(τ )), by di-
entropy cost distribution p(τ ) = 1
rectly optimizing a trajectory distribution with respect to
the current cost cθ(τ ) using a sample-efﬁcient reinforce-
ment learning algorithm. Samples generated on the physi-
cal system are used both to improve the policy and more ac-
curately estimate the partition function Z. In this way, the
reinforcement learning step acts to “guide” the sampling
distribution toward regions where the samples are more
useful for estimating the partition function. We will ﬁrst
describe how the IOC objective in Equation (1) can be esti-
mated with samples, and then describe how reinforcement
learning can adapt the sampling distribution.

4.1. Sample-Based Inverse Optimal Control

the partition function Z = (cid:82) exp(−cθ(τ ))dτ is estimated

In the sample-based approach to maximum entropy IOC,

with samples from a background distribution q(τ ). Prior
sample-based IOC methods use a linear representation of
the cost function, which simpliﬁes the corresponding cost
learning problem (Boularias et al., 2011; Kalakrishnan
et al., 2013). In this section, we instead derive a sample-
based approximation for the IOC objective for a gen-
eral nonlinear parameterization of the cost function. The

Guided Cost Learning

negative log-likelihood corresponding to the IOC model
in Equation (1) is given by:
LIOC(θ) =

cθ(τi) + log Z

(cid:88)

cθ(τi) +log

1
M

exp(−cθ(τj))

,

τj∈Dsamp

q(τj)

(cid:88)
(cid:88)

τi∈Ddemo

τi∈Ddemo

1
N
≈ 1
N

where Ddemo denotes the set of N demonstrated trajecto-
ries, Dsamp the set of M background samples, and q de-
notes the background distribution from which trajectories
τj were sampled. Prior methods have chosen q to be uni-
form (Boularias et al., 2011) or to lie in the vicinity of the
demonstrations (Kalakrishnan et al., 2013). To compute the
gradients of this objective with respect to the cost parame-
ters θ, let wj = exp(−cθ(τj ))
j wj. The gradient
is then given by:

q(τj )

and Z =(cid:80)
(cid:88)

(cid:88)

dLIOC
dθ

=

1
N

dcθ
dθ

(τi) − 1
Z

τi∈Ddemo

τj∈Dsamp

wj

dcθ
dθ

(τj)

When the cost is represented by a neural network or some
other function approximator, this gradient can be com-
puted efﬁciently by backpropagating − wj
Z for each trajec-
tory τj ∈ Dsamp and 1

N for each trajectory τi ∈ Ddemo.

4.2. Adaptive Sampling via Policy Optimization

function (cid:82) exp(−cθ(τ ))dτ is q(τ ) ∝ | exp(−cθ(τ ))| =

The choice of background sample distribution q(τ ) for es-
timating the objective LIOC is critical for successfully ap-
plying the sample-based IOC algorithm. The optimal im-
portance sampling distribution for estimating the partition
exp(−cθ(τ )). Designing a single background distribution
q(τ ) is therefore quite difﬁcult when the cost cθ is un-
known.
Instead, we can adaptively reﬁne q(τ ) to gener-
ate more samples in those regions of the trajectory space
that are good according to the current cost function cθ(τ ).
To this end, we interleave the IOC optimization, which at-
tempts to ﬁnd the cost function that maximizes the like-
lihood of the demonstrations, with a policy optimization
procedure, which improves the trajectory distribution q(τ )
with respect to the current cost.
Since one of the main advantages of the sample-based
IOC approach is the ability to handle unknown dynam-
ics, we must also choose a policy optimization procedure
that can handle unknown dynamics. To this end, we adapt
the method presented by Levine & Abbeel (2014), which
performs policy optimization under unknown dynamics
by iteratively ﬁtting time-varying linear dynamics to sam-
ples from the current trajectory distribution q(τ ), updat-
ing the trajectory distribution using a modiﬁed LQR back-
ward pass, and generating more samples for the next itera-
tion. The trajectory distributions generated by this method

Algorithm 1 Guided cost learning
1: Initialize qk(τ ) as either a random initial controller or from

demonstrations

2: for iteration i = 1 to I do
3:
4:
5:
6:

Generate samples Dtraj from qk(τ )
Append samples: Dsamp ← Dsamp ∪ Dtraj
Use Dsamp to update cost cθ using Algorithm 2
Update qk(τ ) using Dtraj and the method from (Levine &
Abbeel, 2014) to obtain qk+1(τ )

7: end for
8: return optimized cost parameters θ and trajectory distribu-

tion q(τ )

are Gaussian, and each iteration of the policy optimiza-
tion procedure satisﬁes a KL-divergence constraint of the
form DKL(q(τ )(cid:107)ˆq(τ )) ≤ , which prevents the policy from
changing too rapidly (Bagnell & Schneider, 2003; Peters
et al., 2010; Rawlik & Vijayakumar, 2013). This has the
additional beneﬁt of not overﬁtting to poor initial estimates
of the cost function. With a small modiﬁcation, we can
use this algorithm to optimize a maximum entropy version
of the objective, given by minq Eq[cθ(τ )] − H(τ ), as dis-
cussed in prior work (Levine & Abbeel, 2014). This variant
of the algorithm allows us to recover the trajectory distribu-
tion q(τ ) ∝ exp(−cθ(τ )) at convergence (Ziebart, 2010),
providing a good sampling distribution. For completeness,
this policy optimization procedure is summarized in Ap-
pendix A.
Our sample-based IOC algorithm with adaptive sampling
is summarized in Algorithm 1. We call this method guided
cost learning because policy optimization is used to guide
sampling toward regions with lower cost. The algorithm
consists of taking successive policy optimization steps,
each of which generates samples Dtraj from the latest tra-
jectory distribution qk(τ ). After sampling, the cost func-
tion is updated using all samples collected thus far for the
purpose of policy optimization. No additional background
samples are required for this method. This procedure re-
turns both a learned cost function cθ(xt, ut) and a trajec-
tory distribution q(τ ), which corresponds to a time-varying
linear-Gaussian controller q(ut|xt). This controller can be
used to execute the learned behavior.

4.3. Cost Optimization and Importance Weights

The IOC objective can be optimized using standard
nonlinear optimization methods and the gradient dLIOC
dθ .
Stochastic gradient methods are often preferred for high-
dimensional function approximators, such as the neural
networks. Such methods are straightforward to apply to
objectives that factorize over the training samples, but the
partition function does not factorize trivially in this way.
Nonetheless, we found that our objective could still be op-
timized with stochastic gradient methods by sampling a

Guided Cost Learning

subset of the demonstrations and background samples at
each iteration. When the number of samples in the batch is
small, we found it necessary to add the sampled demon-
strations to the background sample set as well; without
adding the demonstrations to the sample set, the objective
can become unbounded and frequently does in practice.
The stochastic optimization procedure is presented in Al-
gorithm 2, and is straightforward to implement with most
neural network libraries based on backpropagation.
Estimating the partition function requires us to use impor-
tance sampling. Although prior work has suggested drop-
ping the importance weights (Kalakrishnan et al., 2013;
Aghasadeghi & Bretl, 2011), we show in Appendix B
that this produces an inconsistent likelihood estimate and
fails to recover good cost functions.
Since our sam-
ples are drawn from multiple distributions, we compute
a fusion distribution to evaluate the importance weights.
Speciﬁcally,
if we have samples from k distributions
q1(τ ), . . . , qκ(τ ), we can construct a consistent estima-
tor of the expectation of a function f (τ ) under a uni-
form distribution as E[f (τ )] ≈ 1
(cid:80)
κ qκ(τj ) f (τj).
the importance weights are given by
Accordingly,
(cid:88)
κ qκ(τj)]−1, and the objective is now given by:
zj = [ 1
k
LIOC(θ) =
zj exp(−cθ(τj))
1
N

(cid:80)
(cid:88)

cθ(τi) + log

1
M

(cid:80)

M

1

τj

1
k

τi∈Ddemo

τj∈Dsamp

The distributions qκ underlying background samples are
obtained from the controller at iteration k. We must also
append the demonstrations to the samples in Algorithm 2,
yet the distribution that generated the demonstrations is un-
known. To estimate it, we assume the demonstrations come
from a single Gaussian trajectory distribution and compute
its empirical mean and variance. We found this approxi-
mation sufﬁciently accurate for estimating the importance
weights of the demonstrations, as shown in Appendix B.

4.4. Learning Costs and Controllers

In contrast to many previous IOC and IRL methods, our
approach can be used to learn a cost while simultaneously
optimizing the policy for a new instance of the task not in
the demos, such as a new position of a target cup for a pour-
ing task, as shown in our experiments. Since the algorithm

Algorithm 2 Nonlinear IOC with stochastic gradients
1: for iteration k = 1 to K do
2:
3:
4:

Sample demonstration batch ˆDdemo ⊂ Ddemo
Sample background batch ˆDsamp ⊂ Dsamp
demonstration
Append
to
ˆDsamp ← ˆDdemo ∪ ˆDsamp
dθ (θ) using ˆDdemo and ˆDsamp
Estimate dLIOC
Update parameters θ using gradient dLIOC

5:
6:
7: end for
8: return optimized cost parameters θ

dθ (θ)

sample

batch

batch:

produces both a cost function cθ(xt, ut) and a controller
q(ut|xt) that optimizes this cost on the new task instance,
we can directly use this controller to execute the desired
behavior. In this way, the method actually learns a policy
from demonstration, using the additional knowledge that
the demonstrations are near-optimal under some unknown
cost function, similar to recent work on IOC by direct loss
minimization (Doerr et al., 2015). The learned cost func-
tion cθ(xt, ut) can often also be used to optimize new poli-
cies for new instances of the task without additional cost
learning. However, we found that on the most challenging
tasks we tested, running policy learning with IOC in the
loop for each new task instance typically succeeded more
frequently than running IOC once and reusing the learned
cost. We hypothesize that this is because training the policy
on a new instance of the task provides the algorithm with
additional information about task variation, thus producing
a better cost function and reducing overﬁtting. The intu-
ition behind this hypothesis is that the demonstrations only
cover a small portion of the degrees of variation in the task.
Observing samples from a new task instance provides the
algorithm with a better idea of the particular factors that
distinguish successful task executions from failures.
5. Representation and Regularization
We parametrize our cost functions as neural networks, ex-
panding their expressive power and enabling IOC to be ap-
plied to the state of a robotic system directly, without hand-
designed features. Our experiments in Section 6.2 conﬁrm
that an afﬁne cost function is not expressive enough to learn
some behaviors. Neural network parametrizations are par-
ticularly useful for learning visual representations on raw
image pixels. In our experiments, we make use of the unsu-
pervised visual feature learning method developed by Finn
et al. (2015) to learn cost functions that depend on visual in-
put. Learning cost functions on raw pixels is an interesting
direction for future work, which we discuss in Section 7.
While the expressive power of nonlinear cost functions pro-
vide a range of beneﬁts, they introduce signiﬁcant model
complexity to an already underspeciﬁed IOC objective.
To mitigate this challenge, we propose two regularization
methods for IOC. Prior methods regularize the IOC objec-
tive by penalizing the (cid:96)1 or (cid:96)2 norm of the cost parame-
ters θ. For high-dimensional nonlinear cost functions, this
regularizer is often insufﬁcient, since different entries in
the parameter vector can have drastically different effects
on the cost. We use two regularization terms. The ﬁrst
term encourages the cost of demo and sample trajectories
to change locally at a constant rate (lcr), by penalizing the
second time derivative:
glcr(τ ) =

[(cθ(xt+1)− cθ(xt))− (cθ(xt)− cθ(xt−1))]2

(cid:88)

xt∈τ

This term reduces high-frequency variation that is often

Guided Cost Learning

symptomatic of overﬁtting, making the cost easier to reop-
timize. Although sharp changes in the cost slope are some-
times preferred, we found that temporally slow-changing
costs were able to adequately capture all of the behaviors
in our experiments.
The second regularizer
is more speciﬁc to one-shot
episodic tasks, and it encourages the cost of the states of a
demo trajectory to decrease strictly monotonically in time
using a squared hinge loss:

(cid:88)

xt∈τ

gmono(τ ) =

[max(0, cθ(xt) − cθ(xt−1) − 1)]2

The rationale behind this regularizer is that, for tasks that
essentially consist of reaching a target condition or state,
the demonstrations typically make monotonic progress to-
ward the goal on some (potentially nonlinear) manifold.
While this assumption does not always hold perfectly, we
again found that this type of regularizer improved perfor-
mance on the tasks in our evaluation. We show a detailed
comparison with regard to both regularizers in Appendix E.
6. Experimental Evaluation
We evaluated our sampling-based IOC algorithm on a set
of robotic control tasks, both in simulation and on a real
robotic platform. Each of the experiments involve complex
second order dynamics with force or torque control and no
manually designed cost function features, with the raw state
provided as input to the learned cost function.
We also tested the consistency of our algorithm on a toy
point mass example for which the ground truth distribu-
tion is known. These experiments, discussed fully in Ap-
pendix B, show that using a maximum entropy version of
the policy optimization objective (see Section 4.2) and us-
ing importance weights are both necessary for recovering
the true distribution.

6.1. Simulated Comparisons

In this section, we provide simulated comparisons between
guided cost learning and prior sample-based methods. We
focus on task performance and sample complexity, and also
perform comparisons across two different sampling distri-
bution initializations.
To compare guided cost learning to prior methods, we ran
experiments on three simulated tasks of varying difﬁculty,
all using the MuJoCo physics simulator (Todorov et al.,
2012). The ﬁrst task is 2D navigation around obstacles,
modeled on the task by Levine & Koltun (2012). This
task has simple, linear dynamics and a low-dimensional
state space, but a complex cost function, which we visu-
alize in Figure 2. The second task involves a 3-link arm
reaching towards a goal location in 2D, in the presence of
physical obstacles. The third, most challenging, task is 3D

green: goal
red: obstacles

initial
state

goal
state

Figure 2. Comparison to prior work on simulated 2D navigation,
reaching, and peg insertion tasks. Reported performance is aver-
aged over 4 runs of IOC on 4 different initial conditions . For peg
insertion, the depth of the hole is 0.1m, marked as a dashed line.
Distances larger than this amount failed to insert the peg.

peg insertion with a 7 DOF arm. This task is signiﬁcantly
more difﬁcult than tasks evaluated in prior IOC work as it
involves complex contact dynamics between the peg and
the table and high-dimensional, continuous state and ac-
tion spaces. The arm is controlled by selecting torques at
the joint motors at 20 Hz. More details on the experimental
setup are provided in Appendix D.
In addition to the expert demonstrations, prior methods re-
quire a set of “suboptimal” samples for estimating the parti-
tion function. We obtain these samples in one of two ways:
by using a baseline random controller that randomly ex-
plores around the initial state (random), and by ﬁtting a
linear-Gaussian controller to the demonstrations (demo).
The latter initialization typically produces a motion that
tracks the average demonstration with variance propor-
tional to the variation between demonstrated motions.
Twenty to thirty-two demonstrations were generated from
a policy learned using the method of Levine & Abbeel
(2014), with a ground truth cost function determined by
the agent’s pose relative to the goal. We found that for
the more precise peg insertion task, a relatively complex
ground truth cost function was needed to afford the neces-
sary degree of precision. We used a cost function of the
form wd2 + v log(d2 + α), where d is the distance be-
tween the two tips of the peg and their target positions, and

2D Navigationsamplestrue cost525456585-450-400-350PIIRL, demo initPIIRL, rand. initRelEnt, demo initRelEnt, rand. initours, demo initours, rand. inituniformReachingsamplesdistance52545658500.20.40.60.8Peg Insertionsamplesdistance52545658500.10.20.30.40.5Guided Cost Learning

v and α are constants. Note that the afﬁne cost is inca-
pable of exactly representing this function. We generated
demonstration trajectories under several different starting
conditions. For 2D navigation, we varied the initial posi-
tion of the agent, and for peg insertion, we varied the posi-
tion of the peg hole. We then evaluated the performance of
our method and prior sample-based methods (Kalakrishnan
et al., 2013; Boularias et al., 2011) on each task from four
arbitrarily-chosen test states.
We used a neural network cost function with two hidden
layers with 24–52 units and rectifying nonlinearities of the
form max(z, 0) followed by linear connections to a set of
features yt, which had a size of 20 for the 2D navigation
task and 100 for the other two tasks. The cost is then given
by

cθ(xt, ut) = (cid:107)Ayt + b(cid:107)2 + wu(cid:107)ut(cid:107)2

(2)

with a ﬁxed torque weight wu and the parameters consist-
ing of A, b, and the network weights. These cost functions
range from about 3,000 parameters for the 2D navigation
task to 16,000 parameters for peg insertion. For further de-
tails, see Appendix C. Although the prior methods learn
only linear cost functions, we can extend them to the non-
linear setting following the derivation in Section 4.1.
Figure 2 illustrates the tasks and shows results for each
method after different numbers of samples from the test
condition. In our method, ﬁve samples were used at each
iteration of policy optimization, while for the prior meth-
ods, the number of samples corresponds to the number of
“suboptimal” samples provided for cost learning. For the
prior methods, additional samples were used to optimize
the learned cost. The results indicate that our method is
generally capable of learning tasks that are more complex
than the prior methods, and is able to effectively handle
complex, high-dimensional neural network cost functions.
In particular, adding more samples for the prior methods
generally does not improve their performance, because all
of the samples are drawn from the same distribution.

6.2. Real-World Robotic Control

We also evaluated our method on a set of real robotic ma-
nipulation tasks using the PR2 robot, with comparisons to
relative entropy IRL, which we found to be the better of
the two prior methods in our simulated experiments. We
chose two robotic manipulation tasks which involve com-
plex dynamics and interactions with delicate objects, for
which it is challenging to write down a cost function by
hand. We evaluated relative entropy IRL and guided cost
learning using a two-layer neural network cost parametriza-
tion and the regularization objective described in Section 5,
and compared to an afﬁne cost function on one of the tasks
to evaluate the importance of non-linear cost representa-
tions. The afﬁne cost followed the form of equation 2 but

human demo

initial pose

ﬁnal pose

h
s
i
d

g
n
i
r
u
o
p

Figure 3. Dish placement and pouring tasks. The robot learned
to place the plate gently into the correct slot, and to pour al-
monds, localizing the target cup using unsupervised visual fea-
tures. A video of the learned controllers can be found at
http://rll.berkeley.edu/gcl
with yt equal to the input xt.1 For both tasks, between
25 and 30 human demonstrations were provided via kines-
thetic teaching, and each IOC algorithm was initialized by
automatically ﬁtting a controller to the demonstrations that
roughly tracked the average trajectory. Full details on both
tasks are in Appendix D, and summaries are below.
In the ﬁrst task, illustrated in Figure 3, the robot must gen-
tly place a grasped plate into a speciﬁc slot of dish rack.
The state space consists of the joint angles, the pose of the
gripper relative to the target pose, and the time derivatives
of each; the actions correspond to torques on the robot’s
motors; and the input to the cost function is the pose and
velocity of the gripper relative to the target position. Note
that we do not provide the robot with an existing trajec-
tory tracking controller or any manually-designed policy
representation beyond linear-Gaussian controllers, in con-
trast to prior methods that use trajectory following (Kalakr-
ishnan et al., 2013) or dynamic movement primitives with
features (Boularias et al., 2011). Our attempt to design a
hand-crafted cost function for inserting the plate into the
dish rack produced a fast but overly aggressive behavior
that cracked one of the plates during learning.
The second task, also shown in Figure 3, consisted of pour-
ing almonds from one cup to another. In order to succeed,
the robot must keep the cup upright until reaching the target
cup, then rotate the cup so that the almonds are poured. In-
stead of including the position of the target cup in the state
space, we train autoencoder features from camera images
captured from the demonstrations and add a pruned feature
point representation and its time derivative to the state, as
proposed by Finn et al. (2015). The input to the cost func-
tion includes these visual features, as well as the pose and
velocity of the gripper. Note that the position of the target
cup can only be obtained from the visual features, so the al-
gorithm must learn to use them in the cost function in order

1Note that a cost function that is quadratic in the state is linear
in the coefﬁcients of the monomials, and therefore corresponds to
a linear parameterization.

Guided Cost Learning

dish (NN)
RelEnt IRL
GCL (ours)
pouring (afﬁne)
RelEnt IRL
GCL (ours)
pouring (NN)
RelEnt IRL
GCL (ours)

success rate

q(ut|xt)

n/a
100%
q(ut|xt)

n/a
0%

q(ut|xt)

reopt. cθ

0%
100%
reopt. cθ

0%
–

# samples

100
90

150
120

n/a

84.7%

reopt. cθ

10%
34%

150, 150
75, 130

Table 1. Performance of guided cost learning (GCL) and relative
entropy (RelEnt) IRL on placing a dish into a rack and pouring al-
monds into a cup. We report the rate at which the robot places the
plate into the correct slot and the percentage of almonds poured
into the target cup, both averaged over 10 rollouts. Sample counts
are for IOC, omitting those for optimizing the learned cost. An
afﬁne cost is insufﬁcient for representing the pouring task, thus
motivating using a neural network cost (NN). The pouring task
with a neural network cost is evaluated for two positions of the
target cup; average performance is reported.

to succeed at the task.
The results, presented in Table 1, show that our algorithm
successfully learned both tasks. The prior relative entropy
IRL algorithm could not acquire a suitable cost function,
due to the complexity of this domain. On the pouring task,
where we also evaluated a simpler afﬁne cost function, we
found that only the neural network representation could re-
cover a successful behavior, illustrating the need for rich
and expressive function approximators when learning cost
functions directly on raw state representations.2
The results in Table 1 also evaluate the generalizability of
the cost function learned by our method and prior work. On
the dish rack task, we can use the learned cost to optimize
new policies for different target dish positions successfully,
while the prior method does not produce a generalizable
cost function. On the harder pouring task, we found that the
learned cost succeeded less often on new positions. How-
ever, as discussed in Section 4.4, our method produces both
a policy q(ut|xt) and a cost function cθ when trained on a
novel instance of the task, and although the learned cost
functions for this task were worse, the learned policy suc-
ceeded on the test positions when optimized with IOC in
the inner loop using our algorithm. This indicates an inter-
esting property of our approach: although the learned cost
function is local in nature due to the choice of sampling
distribution, the learned policy tends to succeed even when
the cost function is too local to produce good results in very
different situations. An interesting avenue for future work
is to further explore the implications of this property, and to
improve the generalizability of the learned cost by succes-
sively training policies on different novel instances of the

2We did attempt to learn costs directly on image pixels, but
found that the problem was too underdetermined to succeed. Bet-
ter image-speciﬁc regularization is likely required for this.

task until enough global training data is available to pro-
duce a cost function that is a good ﬁt to the demonstrations
in previously unseen parts of the state space.

7. Discussion and Future Work
We presented an inverse optimal control algorithm that
can learn complex, nonlinear cost representations, such as
neural networks, and can be applied to high-dimensional
systems with unknown dynamics. Our algorithm uses a
sample-based approximation of the maximum entropy IOC
objective, with samples generated from a policy learning
algorithm based on local linear models (Levine & Abbeel,
2014). To our knowledge, this approach is the ﬁrst to com-
bine the beneﬁts of sample-based IOC under unknown dy-
namics with nonlinear cost representations that directly use
the raw state of the system, without the need for manual
feature engineering. This allows us to apply our method
to a variety of real-world robotic manipulation tasks. Our
evaluation demonstrates that our method outperforms prior
IOC algorithms on a set of simulated benchmarks, and
achieves good results on several real-world tasks.
Our evaluation shows that our approach can learn good
cost functions for a variety of simulated tasks. For com-
plex robotic motion skills, the learned cost functions tend
to explain the demonstrations only locally. This makes
them difﬁcult to reoptimize from scratch for new condi-
tions. It should be noted that this challenge is not unique
to our method. In our comparisons, no prior sample-based
method was able to learn good global costs for these tasks.
However, since our method interleaves cost optimization
with policy learning, it still recovers successful policies for
these tasks. For this reason, we can still learn from demon-
stration simply by retaining the learned policy, and discard-
ing the cost function. This allows us to tackle substantially
more challenging tasks that involve direct torque control of
real robotic systems with feedback from vision.
To incorporate vision into our experiments, we used unsu-
pervised learning to acquire a vision-based state represen-
tation, following prior work (Finn et al., 2015). An ex-
citing avenue for future work is to extend our approach
to learn cost functions directly from natural images. The
principal challenge for this extension is to avoid overﬁt-
ting when using substantially larger and more expressive
networks. Our current regularization techniques mitigate
overﬁtting to a high degree, but visual inputs tend to vary
dramatically between demonstrations and on-policy sam-
ples, particularly when the demonstrations are provided by
a human via kinesthetic teaching. One promising avenue
for mitigating these challenges is to introduce regulariza-
tion methods developed for domain adaptation in computer
vision (Tzeng et al., 2015), to encode the prior knowledge
that demonstrations have similar visual features to samples.

Guided Cost Learning

References
Abbeel, P. and Ng, A. Apprenticeship learning via inverse
reinforcement learning. In International Conference on
Machine Learning (ICML), 2004.

Aghasadeghi, N. and Bretl, T. Maximum entropy inverse
reinforcement learning in continuous state spaces with
path integrals. In International Conference on Intelligent
Robots and Systems (IROS), 2011.

Audiffren, J., Valko, M., Lazaric, A., and Ghavamzadeh,
M. Maximum Entropy Semi-Supervised Inverse Rein-
forcement Learning. In International Joint Conference
on Artiﬁcial Intelligence (IJCAI), July 2015.

Bagnell, J. A. and Schneider, J. Covariant policy search. In
International Joint Conference on Artiﬁcial Intelligence
(IJCAI), 2003.

Boularias, A., Kober, J., and Peters, J. Relative entropy
inverse reinforcement learning. In International Confer-
ence on Artiﬁcial Intelligence and Statistics (AISTATS),
2011.

Byravan, A., Monfort, M., Ziebart, B., Boots, B., and Fox,
D. Graph-based inverse optimal control for robot manip-
ulation. In International Joint Conference on Artiﬁcial
Intelligence (IJCAI), 2015.

Doerr, A., Ratliff, N., Bohg, J., Toussaint, M., and Schaal,
S. Direct loss minimization inverse optimal control. In
Proceedings of Robotics: Science and Systems (R:SS),
Rome, Italy, July 2015.

Levine, S. and Koltun, V. Continuous inverse optimal con-
trol with locally optimal examples. In International Con-
ference on Machine Learning (ICML), 2012.

Levine, S., Popovic, Z., and Koltun, V. Nonlinear in-
verse reinforcement learning with gaussian processes.
In Advances in Neural Information Processing Systems
(NIPS), 2011.

Levine, S., Wagener, N., and Abbeel, P. Learning contact-
rich manipulation skills with guided policy search.
In
International Conference on Robotics and Automation
(ICRA), 2015.

Muelling, K., Boularias, A., Mohler, B., Sch¨olkopf, B., and
Peters, J. Learning strategies in table tennis using inverse
reinforcement learning. Biological Cybernetics, 108(5),
2014.

Ng, A., Harada, D., and Russell, S. Policy invariance under
reward transformations: Theory and application to re-
ward shaping. In International Conference on Machine
Learning (ICML), 1999.

Ng, A., Russell, S., et al. Algorithms for inverse reinforce-
ment learning. In International Conference on Machine
Learning (ICML), 2000.

Peters, J., M¨ulling, K., and Alt¨un, Y. Relative entropy pol-
icy search. In AAAI Conference on Artiﬁcial Intelligence,
2010.

Ramachandran, D. and Amir, E. Bayesian inverse rein-
In AAAI Conference on Artiﬁcial

forcement learning.
Intelligence, volume 51, 2007.

Dragan, Anca and Srinivasa, Siddhartha. Formalizing as-
In Proceedings of Robotics: Sci-
sistive teleoperation.
ence and Systems (R:SS), Sydney, Australia, July 2012.

Ratliff, N., Bagnell, J. A., and Zinkevich, M. A. Maxi-
mum margin planning. In International Conference on
Machine Learning (ICML), 2006.

Finn, Chelsea, Tan, Xin Yu, Duan, Yan, Darrell, Trevor,
Levine, Sergey, and Abbeel, Pieter. Deep spatial au-
arXiv preprint
toencoders for visuomotor learning.
arXiv:1509.06113, 2015.

Huang, D. and Kitani, K. Action-reaction: Forecasting the
dynamics of human interaction. In European Conference
on Computer Vision (ECCV), 2014.

Kalakrishnan, M., Pastor, P., Righetti, L., and Schaal,
In
S. Learning objective functions for manipulation.
International Conference on Robotics and Automation
(ICRA), 2013.

Levine, S. and Abbeel, P. Learning neural network poli-
cies with guided policy search under unknown dynamics.
In Advances in Neural Information Processing Systems
(NIPS), 2014.

Ratliff, N., Bradley, D., Bagnell, J. A., and Chestnutt,
J. Boosting structured prediction for imitation learning.
2007.

Ratliff, N., Silver, D., and Bagnell, J. A. Learning to
search: Functional gradient techniques for imitation
learning. Autonomous Robots, 27(1), 2009.

Rawlik, K. and Vijayakumar, S. On stochastic optimal
control and reinforcement learning by approximate in-
ference. Robotics, 2013.

Todorov, E. Linearly-solvable markov decision problems.
In Advances in Neural Information Processing Systems
(NIPS), 2006.

Todorov, E., Erez, T., and Tassa, Y. MuJoCo: A physics
engine for model-based control. In International Con-
ference on Intelligent Robots and Systems (IROS), 2012.

Guided Cost Learning

Tzeng, E., Hoffman, J., Darrell, T., and Saenko, K. Si-
multaneous deep transfer across domains and tasks. In
International Conference on Computer Vision (ICCV),
2015.

Wulfmeier, M., Ondruska, P., and Posner, I. Maximum
arXiv

entropy deep inverse reinforcement learning.
preprint arXiv:1507.04888, 2015.

Ziebart, B. Modeling purposeful adaptive behavior with
the principle of maximum causal entropy. PhD thesis,
Carnegie Mellon University, 2010.

Ziebart, B., Maas, A., Bagnell, J. A., and Dey, A. K. Max-
imum entropy inverse reinforcement learning. In AAAI
Conference on Artiﬁcial Intelligence, 2008.

Guided Cost Learning

A. Policy Optimization under Unknown

Dynamics

tory distributions q(τ ) = q(x1)(cid:81)

The policy optimization procedure employed in this
work follows the method described by Levine and
Abbeel (Levine & Abbeel, 2014), which we summarize
in this appendix. The aim is to optimize Gaussian trajec-
t q(xt+1|xt, ut)q(ut|xt)
with respect to their expected cost Eq(τ )[cθ(τ )]. This
optimization can be performed by iteratively optimiz-
ing Eq(τ )[cθ(τ )] with respect to the linear-Gaussian con-
ditionals q(ut|xt) under a linear-Gaussian estimate of
the dynamics q(xt+1|xt, ut). This optimization can be
performed using the standard linear-quadratic regulator
(LQR). However, when the dynamics of the system are not
known, the linearization q(xt+1|xt, ut) cannot be obtained
directly.
Instead, Levine and Abbeel (Levine & Abbeel,
2014) propose to estimate the local linear-Gaussian dy-
namics q(xt+1|xt, ut) using samples from q(τ ), which
can be obtained by running the linear-Gaussian controller
q(ut|xt) on the physical system. The policy optimization
procedure then consists of iteratively generating samples
from q(ut|xt), ﬁtting q(xt+1|xt, ut) to these samples, and
updating q(ut|xt) under these ﬁtted dynamics by using
LQR.
This policy optimization procedure has several important
nuances. First,
the LQR update can modify the con-
troller q(ut|xt) arbitrarily far from the previous controller.
However, because the real dynamics are not linear, this
new controller might experience very different dynamics
on the physical system than the linear-Gaussian dynamics
q(xt+1|xt, ut) used for the update. To limit the change
in the dynamics under the current controller, Levine and
Abbeel (Levine & Abbeel, 2014) propose solving a mod-
iﬁed, constrained problem for updating q(ut|xt), given as
following:

q∈N Eq[cθ(τ )] s.t. DKL(q(cid:107)ˆq) ≤ ,

max

where ˆq is the previous controller. This constrained prob-
lem ﬁnds a new trajectory distribution q(τ ) that is close to
the previous distribution ˆq(τ ), so that the dynamics viola-
tion is not too severe. The step size  can be chosen adap-
tively based on the degree to which the linear-Gaussian dy-
namics are successful in predicting the current cost (Levine
et al., 2015). Note that when policy optimization is inter-
leaved with IOC, special care must be taken when adapting
this step size. We found that an effective strategy was to
use the step size rule described in prior work (Levine et al.,
2015). This update involves comparing the predicted and
actual improvement in the cost. We used the preceding cost
function to measure both improvements since this cost was
used to make the update.
The second nuance in this procedure is in the scheme used

true
distribution

ground truth
demo i.w.

empirically
estimated
demo i.w.

no maxent
trajopt

no i.w.

KL: 0

230.66

272.71

726.28

9145.35

Figure 4. KL divergence between trajectories produced by our
method, and various ablations, to the true distribution. Guided
cost learning recovers trajectories that come close to both the
mean and variance of the true distribution using 40 demonstrated
trajectories, whereas the algorithm without MaxEnt policy opti-
mization or without importance weights recovers the mean but
not the variance.

to estimate the dynamics q(xt+1|xt, ut). Since these dy-
namics are linear-Gaussian, they can be estimated by solv-
ing a separate linear regression problem at each time step,
using the samples gathered at this iteration. The sample
complexity of this procedure scales linearly with the di-
mensionality of the system. However, this sample com-
plexity can be reduced dramatically if we consider the
fact that they dynamics at nearby time steps are strongly
correlated, even across iterations (due to the previously
mentioned KL-divergence constraint). This property can
be exploited by ﬁtting a crude global model to all of the
samples gathered during the policy optimization proce-
dure, and then using this global model as a prior for the
linear regression. A good choice for this global model
is a Gaussian mixture model (GMM) over tuples of the
form (xt, ut, xt+1), as discussed in prior work (Levine &
Abbeel, 2014). This GMM is reﬁtted at each iteration using
all available interaction data, and acts as a prior when ﬁtting
the time-varying linear-Gaussian dynamics q(xt+1|xt, ut).

B. Consistency Evaluation
We evaluated the consistency of our algorithm by generat-
ing 40 demonstrations from 4 known linear Gaussian tra-
jectory distributions of a second order point mass system,
each traveling to the origin from different starting posi-
tions. The purpose of this experiment is to verify that,
in simple domains where the exact cost function can be
learned, our method is able to recover the true cost function
successfully. To do so, we measured the KL divergence
between the trajectories produced by our method and the
true distribution underlying the set of demonstrations. As
shown in Figure 4, the trajectory distribution produced by
guided cost learning with ground truth demo importance
weights (weights based on the true distribution from which
the demonstrations were sampled, which is generally un-
known) comes very close to the true distribution, with a KL
divergence of 230.66 summed over 100 timesteps. Empiri-

Guided Cost Learning

cally estimating the importance weights of the demos pro-
duces trajectories with a slightly higher KL divergence of
272.71, costing us very little in this domain. Dropping the
demo and sample importance weights entirely recovers a
similar mean, but signiﬁcantly overestimates the variance.
Finally, running the algorithm without a maximum entropy
term in the policy optimization objective (see Section 4.2)
produces trajectories with similar mean, but 0 variance.
These results indicate that correctly incorporating impor-
tance weights into sample-based maximum entropy IOC is
crucial for recovering the right cost function. This contrasts
with prior work, which suggests dropping the importance
weights (Kalakrishnan et al., 2013).

C. Neural Network Parametrization and

Initialization

We use expressive neural network function approximators
to represent the cost, using the form:

cθ(xt, ut) = (cid:107)Ayt + b(cid:107)2 + wu(cid:107)ut(cid:107)2

that

This parametrization can be viewed as a cost
is
quadratic in a set of learned nonlinear features yt = fθ(xt)
where fθ is a multilayer neural network with rectifying
nonlinearities of the form max(z, 0). Since simpler cost
functions are generally preferred, we initialize the fθ to be
the identity function by setting the parameters of the ﬁrst
fully-connected layer to contain the identity matrix and the
negative identity matrix (producing hidden units which are
double the dimension of the input), and all subsequent lay-
ers to the identity matrix. We found that this initialization
improved generalization of the learned cost.

D. Detailed Description of Task Setup
All of the simulated experiments used the MuJoCo simu-
lation package (Todorov et al., 2012), with simulated fric-
tional contacts and torque motors at the joints used for ac-
tuation. All of the real world experiments were on a PR2
robot, using its 7 DOF arm controlled via direct effort con-
trol. Both the simulated and real world controllers were
run for 5 seconds at 20 Hz resulting in 100 time steps per
rollout. We describe the details of each system below.
In all tasks except for 2D navigation (which has a small
state space and complex cost), we chose the dimension of
the hidden layers to be approximately double the size of the
state, making it capable of representing the identity func-
tion.

2D Navigation: The 2D navigation task has 4 state di-
mensions (2D position and velocity) and 2 action dimen-
sions. Forty demonstrations were generated by optimiz-
ing trajectories for 32 randomly selected positions, with at

least 1 demonstration from each starting position. The neu-
ral network cost was parametrized with 2 hidden layers of
dimension 40 and a ﬁnal feature dimension of 20.

Reaching: The 2D reaching task has 10 dimensions (3
joint angles and velocities, 2-dimensional end effector po-
sition and velocity). Twenty demonstrations were gen-
erated by optimizing trajectories from 12 different initial
states with arbitrarily chosen joint angles. The neural net-
work was parametrized with 2 hidden layers of dimension
24 and a ﬁnal feature dimension of 100.

Peg insertion: The 3D peg insertion task has 26 dimen-
sions (7 joint angles, the pose of 2 points on the peg in
3D, and the velocities of both). Demonstrations were gen-
erated by shifting the hole within a 0.1 m × 0.1 m region
on the table. Twenty demonstrations were generated from
sixteen demonstration conditions. The neural network was
parametrized with 2 hidden layers of dimension 52 and a
ﬁnal feature dimension of 100.

Dish: The dish placing task has 32 dimensions (7 joint
angles, the 3D position of 3 points on the end effector, and
the velocities of both). Twenty demonstrations were col-
lected via kinesthetic teaching on nine positions along a
43 cm dish rack. A tenth position, spatially located within
the demonstrated positions, was used during IOC. The in-
put to the cost consisted of the 3 end effector points in
3D relative to the target pose (which fully deﬁne the pose
of the gripper) and their velocities. The neural network
was parametrized with 1 hidden layer of dimension 64 and
a ﬁnal feature dimension of 100. Success was based on
whether or not the plate was in the correct slot and not bro-
ken.

Pouring: The pouring task has has 40 dimensions (7 joint
angles and velocities, the 3D position of 3 points on the
end effector and their velocities, 2 learned visual feature
points in 2D and their velocities). Thirty demonstrations
were collected via kinesthetic teaching. For each demon-
stration, the target cup was placed at a different position on
the table within a 28 cm × 13 cm rectangle. The autoen-
coder was trained on images from the 30 demonstrations
(consisting of 3000 images total). The input to the cost was
the same as the state but omitting the joint angles and veloc-
ities. The neural network was parametrized with 1 hidden
layer of dimension 80 and a ﬁnal feature dimension of 100.
To measure success, we placed 15 almonds in the grasped
cup and measured the percentage of the almonds that were
in the target cup after the executed motion.

Guided Cost Learning

Figure 5. Comparison showing ablations of our method with leav-
ing out one of the two regularization terms. The monotonic regu-
larization improves performance in three of the four task settings,
and the local constant rate regularization signiﬁcantly improves
performance in all settings. Reported distance is averaged over
four runs of IOC on four different initial conditions.
E. Regularization Evaluation
We evaluated the performance with and without each of
the two regularization terms proposed in Section 5 on the
simulated reaching and peg insertion tasks. As shown in
Figure 5, both regularization terms help performance. No-
tably, the learned trajectories fail to insert the peg into the
hole when the cost is learned using no local constant rate
regularization.

Reachingsamplesdistance52545658500.20.40.60.8Peg Insertionsamplesdistance52545658500.10.20.30.40.5ours, demo initours, rand. initours, no lcr reg, demo initours, no lcr reg, rand. initours, no mono reg, demo initours, no mono reg, rand. init