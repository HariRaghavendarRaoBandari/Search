Subdiﬀusivity of a random walk among a Poisson system of moving

traps on Z

Siva Athreya∗

Alexander Drewitz†

Rongfeng Sun‡

March 8, 2016

Abstract

We consider a random walk among a Poisson system of moving traps on Z.

In earlier work
[DGRS12], the quenched and annealed survival probabilities of this random walk have been inves-
tigated. Here we study the path of the random walk conditioned on survival up to time t in the
annealed case and show that it is subdiﬀusive. As a by-product, we obtain an upper bound on the
number of so-called thin points of a one-dimensional random walk, as well as a bound on the total
volume of the holes in the random walk’s range.

AMS 2010 Subject Classiﬁcation : 60K37, 60K35, 82C22.
Keywords : parabolic Anderson model, random walk in random potential, trapping dynamics, subd-
iﬀusive, thin points of a random walk.

1 Introduction

Trapping problems have been studied in the statistical physics and probability literature for decades,
where a particle modeled by a random walk or Brownian motion is killed when it meets one of the
traps. When the traps are Poisson distributed in space and immobile, much has been understood, see
e.g. the seminal works of Donsker and Varadhan [DV75, DV79] as well as the monograph by Sznitman
[Szn98] and the references therein. However, when the traps are mobile, surprisingly little is known.
In a previous work [DGRS12] (see also [PSSS13]), the long-time asymptotics of the annealed and
quenched survival probabilities were identiﬁed in all dimensions, extending earlier work in the physics
literature [MOBC03, MOBC04]. The goal of the current work is to investigate the path behavior of
the one-dimensional random walk conditioned on survival up to time t in the annealed setting, which is
the ﬁrst result of this type to our best knowledge. Note that the model of random walk among mobile
traps is a natural model for many physical and biological phenomena, such as foraging predators vs
prey, or diﬀusing T-cells vs cancer cells in the blood stream.

We now recall the model considered in [DGRS12]. Given an intensity parameter ν > 0, we consider
a family of i.i.d. Poisson random variables (Ny)y∈Zd with mean ν. Given (Ny)y∈Zd, we then start a
family of independent simple symmetric random walks (Y j,y)y∈Zd, 1≤j≤Ny on Zd, each with jump rate
ρ ≥ 0, with Y j,y := (Y j,y
)t≥0 representing the path of the j-th trap starting from y at time 0. We will

t

6
1
0
2

 
r
a

M
5

 

 
 
]

.

R
P
h
t
a
m

[
 
 

1
v
9
0
7
1
0

.

3
0
6
1
:
v
i
X
r
a

∗8th Mile Mysore Road, Indian Statistical Institute, Bangalore 560059, India. Email: athreya@ms.isibang.ac.in
†Universit¨at

zu K¨oln, Mathematisches

50931 K¨oln, Germany.

Email:

Institut, Weyertal

86–90,

drewitz@math.uni-koeln.de

‡Department of Mathematics, National University of Singapore, S17, 10 Lower Kent Ridge Road Singapore, 119076.

Email: matsr@nus.edu.sg

1

refer to these as ‘Y -particles’ or ‘traps’. For t ≥ 0 and x ∈ Zd, we denote by

ξ(t, x) := Xy∈Zd, 1≤j≤Ny

δx(Y j,y

t

)

(1.1)

the number of traps at site x at time t.

Let X := (Xt)t≥0 denote a simple symmetric random walk on Zd with jump rate κ ≥ 0 (and later
on a more general random walk, see Theorem 1.2) that evolves independently of the Y -particles. At
each time t, the X particle is killed with rate γξ(t, Xt), where γ ≥ 0 is the interaction parameter
– i.e., the killing rate is proportional to the number of traps that the X particle sees at that time
instant. We denote the probability measure underlying the X and Y particles by P, and if we consider
expectations or probabilities with respect to only a subset of the deﬁned random variables, we give
those as a superscript, and sometimes also specify the starting conﬁguration as a subscript, such as
PX
0 .

Conditional on the realization of ξ, the survival probability of X up to time t is then given by

This quantity is also referred to as the ‘quenched survival probability’. Taking expectation with respect
to ξ yields the ‘annealed survival probability’

(1.2)

(1.3)

Since we will mainly be interested in the behavior of X, it is useful to integrate out ξ in order to

obtain the annealed survival probability for a given realization of X, i.e.,

Note that the annealed survival probability Z γ

t is also given by EX
asymptotics for the annealed survival probability have been derived.
Theorem 1.1. [DGRS12, Thm. 1.1] Assume that γ ∈ (0,∞], κ ≥ 0, ρ > 0 and ν > 0, then

0 [Z γ

t,X ]. In [DGRS12], the following

ξ(s, Xs) dsoi.

Z γ

0

EX

0h expn − γZ t
0h expn − γZ t
t := EξhEX
t,X := Eξh expn − γZ t

Z γ

ξ(s, Xs) dsoii.

0

ξ(s, Xs) dsoi.

0

0 [Z γ
EX

t,X ] =

d = 1,

π (1 + o(1))o,
ln t (1 + o(1))o,

expn − νq 8ρt
expn − νπρ t
expn − λd,γ,κ,ρ,ν t(1 + o(1))o, d ≥ 3,

d = 2,



where λd,γ,κ,ρ,ν depends on d, γ, κ, ρ, ν, and is called the annealed Lyapunov exponent.

Remark 1. The annealed and quenched survival probabilities introduced above are closely related
to the parabolic Anderson model, namely, the solution of the lattice stochastic heat equation with a
random potential ξ:

(cid:26) ∂

∂t u(t, x) = κ∆u(t, x) − γ ξ(t, x) u(t, x),

u(0, x) = 1,

(t, x) ∈ [0,∞) × Z,
x ∈ Z.

See [DGRS12] for more details.

It is natural to ask how the asymptotics in Theorem 1.1 are actually achieved, both in terms of
the behavior of X as well as that of ξ. We consider the case d = 1 and investigate the typical behavior
of X conditioned on survival. In the next section we state the model precisely and the main results of
the paper.

2

1.1 Main Results

We shall consider the model considered in [DGRS12] but will allow the following generalisations:

X is a continuous time random walk on Z with jump rate κ > 0, and possess a jump kernel

pX which is non-degenerate with zero mean.

Y -particles (traps) are independent continuous time random walks on Z with jump rate

ρ > 0, whose jump kernel pY is symmetric.

(1.4)

(1.5)

As deﬁned earlier, ξ is as in (1.1) and we shall assume that the interaction parameter γ ∈ (0,∞] and
the trap intensity ν > 0.
Before stating our results, we introduce some notation. For t ∈ (0,∞) and a c`adl`ag function

f ∈ D([0, t], R) (with D([0, t], R) denoting the Skorokhod space), we deﬁne its supremum norm by

kfkt := sup

x∈[0,t]|f (x)|.

(1.6)

1.1.1 Sub-diﬀusivity of X

We are interested in the (non-consistent) family of Gibbs measures

P γ
t (X ∈ ·) :=

EX

0hEξh expn − γR t

0 [Z γ
EX

0 ξ(s, Xs) dsoi1X∈·i

t,X]

,

t ≥ 0,

(1.7)

on the space of c`adl`ag paths on Z. We will bound typical ﬂuctuations of X with respect to P γ
primary result is the following bound on the ﬂuctuation of X conditioned on survival up to time t.

t . Our

Theorem 1.2. Let X and Y be as (1.4) and (1.5) respectively. Assume that ∃λ∗ > 0 such that

(1.8)

(1.9)

Then there exists α > 0 such that for all ǫ > 0,

Xx∈Z

eλ∗|x|pX(x) < ∞ and Xx∈Z
t(cid:16)kXkt ∈(cid:0)αt

1
3 , t

P γ

eλ∗|x|pY (x) < ∞.

11

24 +ǫ(cid:1)(cid:17) −→t→∞

1.

24 < 1

2 , the above result shows that X is sub-diﬀusive under P γ

Remark 2. Since 11
t . We believe
that X in fact ﬂuctuates on the scale of t1/3 (modulo lower order corrections).
Interestingly, this
would coincide with the ﬂuctuation known for the case of immobile traps in dimension one (see e.g.
[S90, S03]), and we conjecture that even the rescaled path converges to the same limit. We also note
that this should happen even though the annealed survival probability decays at a diﬀerent rate when
the traps are mobile. However, the mobile trap case presents fundamental diﬃculties that are not
present in the immobile case.

1.1.2 Thin points of X

As by-product of our analysis, we obtain bounds on the number of thin-points of a one-dimensional
random walk which is of independent interest. Let X be as in (1.4) and

Lt(x) := LX

t (x) :=Z t

0

3

δx(Xs) ds

(1.10)

denote the local time of the random walk X at x up to time t. Typically, for x ∈ Z in the bulk of the
t (x) will be of order √t. We are interested in thin points. More precisely,
range of X, the local time LX
for M > 0, a point x in the range of X is called ‘M -thin at time t’ if Lt(x) ∈ (0, M ], and we denote by
(1.11)

Tt,M := {x ∈ Z : Lt(x) ∈ (0, M ]}

the set of M -thin points at time t.

For γ > 0, we introduce the local time functional

F γ

t (X) :=Xx∈Z

e−γLX

t (x)1

t (x)>0.
LX

(1.12)

For X with mean zero and ﬁnite variance for its increments, Proposition 2.1 below implies the existence
of constants c(γ), C(γ) ∈ (0,∞) such that for all t ∈ (0,∞),

(1.13)

t (X)oi ≤ C(γ).
t (X) ≥ e−γM|Tt,M|, we immediately obtain the following result
Theorem 1.3. Let γ ∈ (0,∞), and let X be as in (1.4). Assume that

0h expn c(γ)

1 ∨ ln t

Since F γ

EX

F γ

Xx∈Z

x2pX(x) < ∞.

Then, for any positive M ,

P(cid:0)|Tt,M| ≥ a) ≤ C(γ)e− c(γ)e−γM

1∨ln t

a

for all a > 0.

(1.14)

Remark 3. Thin points of Brownian motion in dimension d ≥ 2 have been studied in [DPRZ00] using
L´evy’s modulus of continuity. Dimension 1 is diﬀerent and could be analyzed by using the Ray-Knight
theorem. When X is a simple random walk on Z, there is still a Ray-Knight theorem to aid our
analysis. But for a general random walk X as in Theorem 1.3, this approach fails.

1.1.3 Holes in the range of X

For a simple random walk X on Z, its range equals the interval [inf 0≤s≤t Xs, sup0≤s≤t Xs], which is no
longer true for non-simple random walks. However, for X as in (1.4), we can control the diﬀerence

Gt(X) :=(cid:0) sup

0≤s≤t

Xs − inf
0≤s≤t

Xs(cid:1) − |Ranges∈[0,t](Xs)| =

X

infs∈[0,t] Xs<x<sups∈[0,t] Xs

1

t (x)=0.
LX

(1.15)

This is the total volume of the holes in the range of X by time t, which will appear in the proof of
Theorem 1.2 for non-simple random walks.

Theorem 1.4. Let X be as in (1.4). Assume that ∃λ∗ > 0 such that

Then there exist c, C > 0 such that for λt := c

Xx∈Z

eλ∗|x|pX (x) < ∞.

1∨ln t , we have

EX

0(cid:2) exp{λtGt(X)}(cid:3) ≤ C

4

for all t ∈ (0,∞).

(1.16)

(1.17)

As a consequence of (1.17), we have

EX

0 [Gt(X)] =Z ∞

0

PX

0 (Gt(X) ≥ m) dm ≤Z ∞

0

Ce− mc

1∨ln t dm ≤ C ln t.

(1.18)

Remark 4. We note that (1.17) cannot hold ifP|x|>L pX(x) has power law decay. This is easily seen

by considering the strategy that the random walk makes a single jump from 0 to a position x ≥ t, and
then never falls below x before time t. The probability of this strategy decays polynomially in t, while
the gain eλtGt(X) is more than the stretched exponential.

Throughout the paper, c and C will denote generic constants, whose values may change from line
Indexed constants such as c1 and C2 will denote values that will be ﬁxed from their ﬁrst
to line.
occurrence onwards. In order to emphasise dependence of a constant on a parameter, we will write
C(p) for instance.

Layout : The rest of the paper is organised as follows. In Section 2 we prove Theorem 1.3, in
Section 3 we prove Theorem 1.4. We conclude the paper with Section 4 where we prove Theorem 1.2.

2 Proof of Theorem 1.3

Recall from (1.12) that F γ
t (x)>0. As remarked before the statement of
LX
Theorem 1.3, it suﬃces to establish the following result

t (x)1

t (X) := Px∈Z e−γLX

Proposition 2.1. Let X be a random walk satisfying the assumptions of Theorem 1.3. Then for each

γ > 0, there exist constants c(γ), C(γ) ∈ (0,∞), such that for λt := c(γ)

1∨ln t , we have

EX

0(cid:2) exp{λtF γ

t (X)}(cid:3) ≤ C(γ)

for all t ∈ (0,∞).

(2.1)

We will prove Proposition 2.1 by approximating X by a sequence of discrete time random walks.
κ , where κ is the jump rate of X, let X q denote the discrete time

More precisely, for any 0 < q < 1
random walk with transition probability

PX q
0 (X q(1) = 0) = 1 − κq,

and

PX q
0 (X q(1) = x) = κqpX(x) ∀x ∈ Z,

(2.2)

where pX(·) is the jump probability kernel of X. Let X q(s) := X q(⌊s⌋) for all s ≥ 0. It is then a
standard fact that the sequence of discrete time random walks (X q(s/q))s≥0 converges in distribution
to (Xs)s≥0 as q ↓ 0. Proposition 2.1 will then follow from its analogue for (X q(s/q))s≥0, together with
the following lemma.

Lemma 2.2. Let X be an arbitrary continuous time random walk on Z, and let X q(·/q) be its discrete
time approximation deﬁned above. Then for any λ, t ∈ [0,∞),

1
n

EX
0

lim
n→∞

h expnλF γ

t (X

1

n (·n))oi = EX

0(cid:2) exp{λF γ

1

Proof. By coupling the successive non-trivial jumps of X
local time process (LX
t
also converges in distribution to F γ
that (exp{λF γ
Note that

n (·n))})n∈N are uniformly integrable.

n (·n)

t (X

(x))x∈Z converges in distribution to (LX

1
n with those of X, it is easily seen that the
n (·n))
t (X) as n → ∞. Therefore to establish (2.3), it suﬃces to show

t (X

1

1

1

F γ
t (X

n (·n)) ≤ |Ranges∈[0,nt](X

1

n (s))|

(2.4)

(2.3)

t (X)}(cid:3).
t (x))x∈Z, and hence F γ

5

1
is bounded by the number of non-trivial jumps of X
n before time nt, which is a binomial random vari-
able Bin(nt, κ/n). Since the exponential moment generating function of the sequence of Bin(nt, κ/n)
random variables converges to that of a Poisson random variable with mean κt, the uniform integra-

bility of (exp{λF γ

t (X

1

n (·n))})n∈N then follows.

We will also need the following result.

Lemma 2.3. Let X be a continuous time random walk on Z with jump rate κ > 0, whose jump kernel
has mean zero and variance σ2 ∈ (0,∞). Let LX
t (0) be its local time at 0 by time t, and τ0 the ﬁrst
hitting time of 0. Then there exists C > 0 such that

EX

0(cid:2)e−γLX

t (0)(cid:3) ∼

1

σ

γr 2κ

πt

as t → ∞ and PX

z (τ0 ≥ t) ≤ 1 ∧

C|z|√t ∀ t > 0, z ∈ Z.

(2.5)

Furthermore, if X
exists C′ > 0 such that for any T > 0,

n (·n) denote the random walks that approximate X as in Lemma 2.2, then there

1
n

EX
0

1
n

n LX
nt

(cid:2)e− γ

(0)(cid:3) ≤ 1 ∧

C′
√t

and PX
z

1
n

(τ0 ≥ nt) ≤ 1 ∧

C′|z|√t

(2.6)

uniformly in t ∈ [0, T ], z ∈ Z\{0}, and n suﬃciently large.
Proof. When X is a continuous time simple symmetric random walk, the ﬁrst part of (2.5) was proved
in [DGRS12, Section 2.2] using the local central limit theorem and Karamata’s Tauberian theorem.
The same proof can also be applied to general X with mean zero and ﬁnite variance. The second part
of (2.5) follows from Theorem 5.1.7 of [LL10].

By (2.5),

EX

0(cid:2)e−γLX

t (0)(cid:3) ≤ 1 ∧

C
√t

for some C uniformly in t > 0. By the same reasoning as in the proof of Lemma 2.2, EX
is a family of decreasing continuous functions in t that converge pointwise to the continuous function
EX

(0)(cid:3)
t (0)(cid:3) as n → ∞. Therefore this convergence must be uniform on [0, T ], which implies the

ﬁrst part of (2.6). The second part of (2.6) follows by the same argument.

0(cid:2)e−γLX

0(cid:2)e− γ

n LX
nt

1
n

Proof of Proposition 2.1. We may restrict our attention to t ≥ t0 for some large t0, since otherwise
(2.1) is easily shown if we bound F γ

t (X) by the number of jumps of X before time t.

Due to Lemma 2.2, it then suﬃces to show that for some C(γ) < ∞ and for all t ≥ t0,

1
n

EX
0

lim
n→∞

h exp(cid:8)λtF γ

t (X

1

n (· n))(cid:9)i ≤ C(γ).

Denote L(n)

nt (·) := LX

1
n

nt (·) for simplicity, and for x ∈ Z, let τx denote the ﬁrst time X

(2.7)

1
n visits x.

6

By Taylor expansion and the deﬁnition of F γ

t , we have

1
n

EX
0

h exp(cid:8)λtF γ

t (X

1

= 1 +

n (· n))(cid:9)i = 1 +
∞Xk=1
∞Xk=1
∞Xk=1
∞Xm=1

≤ 1 +

= 1 +

1
n

λk
t

λk
t

EX
0

k! Xx1,...,xk∈Z
k! Xx1,...,xk∈Z
kXm=1

0≤s1,...,sk≤nt
mk−m
(k − m)!

λk
t

EX
0

1
n

L(n)

e− γ

e− γ

n L(n)

n L(n)

nt (xi)1

h kYi=1
h kYi=1
X0≤t1<t2<···<tm≤nt
h mYi=1

nt (xi)>0i
nt (xi)1τxi =sii
h mYi=1
nt (xi)1τxi =sii,

y1,...,ym∈Z

n L(n)

n L(n)

e− γ

e− γ

EX
0

1
n

1
n

emλtλm
t

X0≤s1<s2<···<sm≤nt

EX
0

x1,...,xm∈Z

nt (yi)1τyi =tii

(2.8)

where in the inequality, we took advantage of the fact that for any 0 ≤ t1 < t2 < ··· < tm ≤ nt with
1 ≤ m ≤ k, the number of ways of choosing s1, . . . , sk from {t1, . . . , tm} so that each ti is chosen at
k!mk−m
least once is given by m!S(k, m) ≤ 1
(k−m)! , where S(k, m) is called a Stirling number of the second
kind [RD69, Theorem 3]. We also used that when τxi = si = τxj = sj, we must have xi = xj.

2

Using L(n)

nt (xi) ≥ L(n)

sm (xi) for 1 ≤ i ≤ m − 1, and applying the strong Markov property at time

τxk = sk, we can bound the expectation in (2.8) by

EX
0

≤ EX

0

1
n

1
n

h m−1Yi=1
h m−1Yi=1

e− γ

n L(n)

sm (xi)

e− γ

n L(n)

sm (xi)

mYi=1
mYi=1

n L(n)

0

1
n

1τxi =siiEX
he− γ
1τxi =sii · Cφ(cid:0)t −

nt−sm (0)i
n (cid:1),

sm

where φ(u) := 1 ∧ 1√u and we applied (2.6) to obtain the inequality.

r (xi) for 1 ≤ i ≤ m − 1 and applying the Markov property at time r gives

We now bound the expectation in (2.9), summed over xm ∈ Z. Let r := ⌊ sm−1+sm
sm (xi) ≥ L(n)
L(n)
Xxm∈Z
≤ Xy∈Z

h m−1Yi=1
n (r)=yi · Xxm∈Z

mYi=1
(cid:0)τxm = sm − r(cid:1).

1τxi =sii = Xxm,y∈Z

h m−1Yi=1
h m−1Yi=1

mYi=1
m−1Yi=1

1τxi =si · 1

1τxi =si · 1

n L(n)

n L(n)

n L(n)

e− γ

e− γ

e− γ

sm (xi)

sm (xi)

EX
0

EX
0

EX
0

PX
y

(xi)

X

1
n

1
n

1
n

1
n

2

X

1

r

(2.9)

⌋. Using

1

n (r)=yi

(2.10)

1
n denotes the time-reversal of X

1

time reversal and translation invariance, we have

n , which has the same increment distribution as −X

1
n , then by

1

1
n

eX
0

n (1) 6= 0, eτ0 > sm − r(cid:1) =
(cid:0)τxm = sm − r(cid:1) = P
(cid:0)eX
n(cid:17) ≤
|z|pX (z)(cid:16)1 ∧
nXz∈Z

≤

κ

C′
p sm
n − r

1
n

κ

eX
z

pX(z)P

nXz∈Z
φ(cid:0) sm − sm−1

n

C
n

(τ0 ≥ sm − r)

(cid:1),

(2.11)

n (i) = 0}, and we applied (2.6) in the ﬁrst inequality. Note that this bound

If eX
Xxm∈Z

1
n

PX
y

whereeτ0 := min{i ≥ 1 : eX

no longer depends on y.

1

7

Substituting the bound of (2.11) into (2.10), and then successively into (2.9) and (2.8), we obtain

Xx1,...,xm∈Z
φ(cid:0)t −

C 2
n

≤

1
n

EX
0

e− γ

n L(n)

h mYi=1
n (cid:1)φ(cid:0) sm − sm−1

nt (xi)1τxi =sii
(cid:1) Xx1,...,xm−1∈Z

n

sm

1
n

EX
0

h m−1Yi=1

We can now iterate this bound to obtain

e− γ

n L(n)

r

(xi)1τxi =sii.

1
n

n

C m

EX
0

e− γ

n L(n)

h mYi=1

n(cid:1) mYi=2

nm φ(cid:0) s1

Xx1,...,xm∈Z

nt (xi)1τxi =sii ≤

φ2(cid:0) si − si−1

(cid:1) · φ(cid:0)t −
where φ(u) = 1 ∧ 1√u . Therefore the inner summand in (2.8) can be bounded by
n(cid:1) mYi=2
φ2(cid:0) si − si−1
n (cid:1)
(cid:1) · φ(cid:0)t −
mYi=2
φ2(ti − ti−1)dt1 ··· dtm.

X0≤s1<s2<...<sm≤nt
≤ C m Z ···Z

nm φ(cid:0) s1

φ(t1)φ(t − tm)

C m

sm

n

0<t1<···<tm<t

sm

n (cid:1),

Note that given tj−1 < tj+1,

Z tj+1

tj−1

φ2(tj − tj−1)φ2(tj+1 − tj)dtj =Z tj+1

tj−1 (cid:16)1 ∧
≤ 4(ln t)(cid:16)1 ∧

1

1

tj+1 − tj(cid:17)dtj

tj − tj−1(cid:17)(cid:16)1 ∧
tj+1 − tj−1(cid:17) = 4(ln t)φ2(tj+1 − tj−1),

1

(2.12)

(2.13)

(2.14)

(2.15)

where the bound clearly holds when tj+1 − tj−1 ≤ 1. When tj+1 − tj−1 > 1, the inequality is obtained
by dividing the interval of integration into [tj−1, (tj+1 − tj−1)/2] and [(tj+1 − tj−1)/2, tj+1], where
, and in the second case we use the bound
in the ﬁrst case we use the bound
tj−tj−1 ≤
side of (2.14) from above by

Applying (2.15) repeatedly to (2.14) to integrate out t2, . . . , tm−1, we can bound the right-hand

tj+1−tj ≤

tj+1−tj−1

tj+1−tj−1

1

1

2

2

.

C m(4 ln t)m−2

ZZ
0<t1<tm<t(cid:16)1 ∧

1

√t1(cid:17)(cid:16)1 ∧

1

tm − t1(cid:17)(cid:16)1 ∧

1

√t − tm(cid:17) dt1dtm ≤ eC m(ln t)m−1,

(2.16)

where the integral is bounded by considering the three cases: t1 ≥ t/3, tm − t1 ≥ t/3, or t − tm ≥ t/3.
Substituting this bound for (2.14) back into (2.8) then gives

1
n

EX
0

(cid:2) exp{λtF γ

t (X

1

n (·n))}(cid:3) ≤ 1 +

∞Xm=1

emλt λm

t eC m(ln t)m−1 ≤

uniformly in n if c(γ) is chosen small enough such that ec(γ)c(γ) < 1/eC. This ﬁnishes the proof.

1

1 − ec(γ)c(γ)eC

=: C(γ) < ∞

8

3 Proof of Theorem 1.4

The proof follows the same line of argument as that of Proposition 2.1, except for some complications.
We ﬁrst approximate X by the family of discrete time random walks X
n (·n), n ∈ N. Recall from
(1.15) that

1

Gt(X) :=(cid:0) sup

0≤s≤t

Xs − inf
0≤s≤t

Xs(cid:1) − |Ranges∈[0,t](Xs)| =

The following is an analogue of Lemma 2.2.

X

inf s∈[0,t] Xs<x<sups∈[0,t] Xs

1

t (x)=0.
LX

(3.1)

Lemma 3.1. Let X be a continuous time random walk on Z, whose jump kernel pX satisﬁes
n (·n) be the discrete time approximation of X de-

1

ﬁned as in (2.2). Then for any λ < λ∗ and t ∈ [0,∞), we have

Px∈Z pX(x)eλ∗|x| < ∞ for some λ∗ > 0. Let X
h expnλGt(X

n (·n))oi = EX

lim
n→∞

EX
0

1
n

1

1

0(cid:2) exp{λGt(X)}(cid:3).

n (·n)) converges in distribution to Gt(X) as n → ∞. It remains to show the

1

(3.2)

Proof. Clearly Gt(X
uniform integrability of (exp{λGt(X
Gt(X

1

n (·n))})n∈N. Similarly, as in (2.4) we have,
n (·n)) ≤ sup
0≤i≤nt

n (i) − inf
0≤i≤nt

1
n (i)

X

X

1

is bounded by the sum of the sizes of the jumps of X

1
n before time nt, which is a compound bi-

pX(x)1x≥0 + pX (−x)1x>0. As n → ∞, this converges to a compound Poisson random variable with

nomial random variable with binomial parameters (nt, κ/n) and summand distribution bpX (x) =
Poisson parameter κt and summand distributionbpX. Since we assumePx∈Z eλ∗|x|pX(x) < ∞ for some

λ∗ > 0, it is then easily seen that (exp{λGt(X
Proof of Theorem 1.4. As in the proof of Proposition 2.1, it suﬃces to show that for some C < ∞ and
for all t suﬃciently large,

n (·n))})n∈N is uniformly integrable for λ < λ∗.

1

EX
0

lim
n→∞

1
n

h exp(cid:8)λtGt(X

1

n (· n))(cid:9)i ≤ C.

(3.3)

if x ≥ 0
if x ≤ 0

,

τx := min{i ≥ 0 : X

1

n (i) = x}.

Using (3.1), as in (2.8), we can expand

Given X

1

1

1

1

1
n

λk
t

n (0) = 0, for x ∈ Z, deﬁne

min{i ≥ 0 : X

n (i) ≥ x}
n (i) ≤ x}

eτx :=( min{i ≥ 0 : X
h exp(cid:8)λtGt(X
n (· n))(cid:9)i
h kYi=1
∞Xk=1
k! Xx1,...,xk∈Z
∞Xk=1
kXm=1 X0<t1<···<tm≤nt
I1,...,Im⊢{1,...,k} Xx1,...,xk∈Z
∞Xk=1
kXm=1 X0<t1<···<tm≤nt
I1,...,Im⊢{1,...,k} Xx1,...,xk∈Z

λk
t
k!

λk
t
k!

EX
0

1
n

y1,...,ym∈Z
z1,...,zm∈Z

EX
0

= 1 +

= 1 +

= 1 +

0≤s1,...,sk≤nt

1
n

λk
t

1eτxi≤nt<τxii = 1 +
∞Xk=1
k! Xx1,...,xk∈Z
1eτxj =ti,τxj >nti
h mYi=1Yj∈Ii
h mYi=1(cid:16)1

n (ti−1)=yi,X

EX
0

EX
0

X

1
n

1

1

9

1
n

EX
0

h kYi=1

1eτxi =si,τxi >nti

n (ti)=zi Yj∈Ii

1eτxj =ti,τxj >nt(cid:17)i,

(3.4)

where in the third line, we summed over all ordered non-empty disjoint sets (I1, . . . , Im) which partition

{1, . . . , k}. Note that when eτxj = tm for all j ∈ Im, xj must be strictly between ym and zm for all

j ∈ Im. By the Markov property at time tm and by Lemma 2.3, we can bound

1
n

EX
0

1
n

≤ EX

0

h mYi=1(cid:16)1
h m−1Yi=1 (cid:16)1

X

1

n (ti−1)=yi,X

1

X

n (ti−1)=yi,X
κ
n

pX(zm − ym)

×

1

n (ti)=zi Yj∈Ii
n (ti)=zi Yj∈Ii

1eτxj =ti,τxj >nt(cid:17)i
1eτxj =ti,τxj >tm(cid:17)(cid:16) Yj∈Im

1

max

x∈(ym∧zm,ym∨zm)

PX

1
n

≤ EX

0

h m−1Yi=1 (cid:16)1

1

n (ti)=zi Yj∈Ii

1

X

n (ti−1)=yi,X
κ
n

×

pX(zm − ym) C|zm − ym|φ(cid:0)t −

1eτxj =ti,τxj >tm(cid:17)(cid:16) Yj∈Im
n(cid:1) · Yj∈Im

tm

1
n

X

1τxj >tm(cid:17)1
zm (τx > nt − tm) · Yj∈Im
1τxj >tm(cid:17)1

X

1

n (tm−1)=ymi

1xj∈(ym∧zm,ym∨zm)

1

n (tm−1)=ymi

1xj∈(ym∧zm,ym∨zm),

(3.5)

where as before φ(u) = 1 ∧ 1√u , and this is the analogue of (2.9) in the proof of Proposition 2.1.

⌋. Applying the Markov property at time r and summing the above bound over

Let r := ⌊ tm−1+tm

2

ym, zm and (xj)j∈Im then gives

C

κ
n

φ(cid:0)t −

tm

n(cid:1) Xym,zm∈Z
× EX

0

xj∈(ym∧zm,ym∨zm): j∈Im

1
n

PX

pX(zm − ym)|zm − ym|Xw∈Z
h m−1Yi=1 (cid:16)1

w h(cid:16) Yj∈Im
n (ti)=zi Yj∈Ii

1τxj ≥tm−r(cid:17)1
1eτxj =ti,τxj >r(cid:17)1

n (ti−1)=yi,X

X

1
n

X

1

1

X

1

n (tm−1−r)=ymi
n (r)=wi

1

pX(v)|v||Im|+1 max

x∈(0∧v,0∨v)

1
n

P

eX
0

(τx ≥ tm − r)

≤ C

κ
n

φ(cid:0)t −

tm

n(cid:1)Xv,w∈Z

≤ C 2 κ

n

φ(cid:0)t −

1
n

tm

0

× EX
n(cid:1)φ(cid:0) tm − tm−1
× EX

2

0

1
n

1eτxj =ti,τxj >r(cid:17)1

1

n (r)=wi

X

1eτxj =ti,τxj >r(cid:17)i,

(3.6)

1

1

1

X

X

n (ti−1)=yi,X

pX(v)|v||Im|+2

n (ti)=zi Yj∈Ii

h m−1Yi=1 (cid:16)1
(cid:1)Xv∈Z
h m−1Yi=1 (cid:16)1
n on the time interval [r, tm−1], with eX
n (ti)=zi Yj∈Ii

n (ti)=zi Yj∈Ii

h mYi=1(cid:16)1

n (ti−1)=yi,X

n (ti−1)=yi,X

EX
0

X

1
n

1

1

1

1

1
where we have reversed time for X
n denoting the time-reversed
random walk, and in the last inequality we again applied Lemma 2.3. This bound is the analogue of
(2.12), which can now be iterated. The calculations in (2.13)–(2.16) then give

X0<t1<···<tm≤nt Xx1,...,xk∈Z

y1,...,ym∈Z
z1,...,zm∈Z
M (|Ii| + 2),

mYi=1

≤ eC m(ln t)m−1

10

1eτxj =ti,τxj >nt(cid:17)i

(3.7)

λk
t
k!

mYi=1

M (ki + 2)

mYi=1

M (ki + 2)

λk
t
k!

λk1+···+km
t

1
n

EX
0

1

≤ 1 +

= 1 +

= 1 +

M (|Ii| + 2)

λk1+···+km
t

(k1 + ··· + km)!

I1,...,Im⊢{1,...,k1+···+km}

|I1|=k1,...,|Im|=km

where M (α) :=Px∈Z |x|αpX (x). Substituting this bound into (3.4) then gives (uniformly in n)

kXm=1 XI1,...,Im⊢{1,...,k}eC m(ln t)m−1

n (· n))(cid:9)i ≤ 1 +
h exp(cid:8)λtGt(X
∞Xk=1
∞Xk1,...,km=1
∞Xm=1
(eC ln t)m
∞Xk1,...,km=1
∞Xm=1
(eC ln t)m
(eC ln t)m(cid:16) ∞Xk=1
∞Xm=1
1∨ln t is chosen small enough. Indeed, let V be a random variable with M (α) = E[|V |α].

X
(k1 + ··· + km)!(cid:18)k1 + ··· + km
M (k + 2)(cid:17)m
1 − eC ln tP∞k=1
if c in λt = c
Since assumption (1.16) implies E[eλ∗|V |] < ∞, we have
(1 ∨ ln t)kk!|V |k+2i
ckeC ln t

k1, . . . , km (cid:19) mYi=1

λk
t
k!

∞Xk=1
eC ln t
≤ c eC Eh|V |3 ∞Xk=1

M (k + 2) = Eh ∞Xk=1
(1 ∨ ln t)k−1(k − 1)!|V |k−1i ≤ ceC E(cid:2)|V |6(cid:3) 1
2 Ehe

ck−1

2c

1∨ln t|V |i 1

λk
k! M (k + 2)
t

< C < ∞

(3.8)

=

1

if t is large enough and c is chosen small enough. This completes the proof.

2 < 1

4 Proof of Theorem 1.2

To prepare for the proof, we recall here the strategy for the lower bound on the annealed survival
probability employed in [DGRS12], and we show how to rewrite the survival probability in terms of
the range of random walks.

4.1 Strategy for lower bound

The lower bound on the annealed survival probabilities in Theorem 1.1 follows the same strategy as
for the case of immobile traps in previous works. Denote Br = {x ∈ Zd : kxk∞ ≤ r}. For a ﬁxed time
t, we force the environment ξ to create a ball BRt of radius Rt around the origin, which is free of traps
from time 0 up to time t. We then force the random walk X to stay inside BRt up to time t. This
leads to a lower bound on the survival probability that is independent of γ ∈ (0,∞].

To be more precise, we consider the following events:

• Let Et denote the event that Ny = 0 for all y ∈ BRt.
• Let Ft denote the event that Y j,y
/∈ BRt for all y /∈ BRt, 1 ≤ j ≤ Ny, and s ∈ [0, t].
• Let Gt denote the event that X with X(0) = 0 does not leave BRt before time t.

s

Then, by the strategy outlined above, the annealed survival probability

0 [Z γ
EX

t,X] ≥ P(Et ∩ Ft ∩ Gt) = P(Et)P(Ft)P(Gt),

(4.1)

since Et, Ft, and Gt are independent.

11

In order to lower bound (4.1), note that

To estimate P(Gt), Donsker’s invariance principle implies that there exists α > 0 such that for all

P(Et) = e−ν(2Rt+1)d

.

(4.2)

t suﬃciently large,

inf

x∈B√t/2

PX

0 (cid:16)Xs ∈ B√t ∀ s ∈ [0, t] , Xt ∈ B√t/2(cid:12)(cid:12)(cid:12) X0 = x(cid:17) ≥ α.

Now if 1 ≪ Rt ≪ √t as t → ∞, then by partitioning the time interval [0, t] into intervals of length R2

t

and applying the Markov property at times iR2

t , we obtain

P(Gt) ≥ PX

≥ α⌈t/R2

0(cid:16)Xs ∈ BRt ∀ s ∈ [(i − 1)R2

t ⌉ = (1 + o(1))et ln α/R2
t .

t , iR2

t ], and XiR2

t ⌉(cid:17)
t ∈ BRt/2, i = 1, 2,··· ,⌈t/R2

(4.3)

This actually gives the correct logarithmic order of decay for P(Gt). Indeed, by Donsker’s invariance
principle, uniformly in t large and X0 = x ∈ BRt,
t ]) ≥ PW

0 (Ws /∈ B3 for some s ∈ [0, 1]) =: ρ > 0,

PX
x (Xs /∈ BRt for some s ∈ [0, R2

where W is a standard d-dimensional Brownian motion. Therefore by a similar application of the
Markov inequality as in (4.3), we ﬁnd that

In dimension d = 1, which is our main focus, integrating out the Poisson initial distribution of the

Y -particles gives

P(Gt) ≤ e⌊t/R2

t ⌋ln(1−ρ).

(4.4)

P(Ft) = expn − ν Xy∈Z\BRt
= expn − ν Xy∈Z\{0}

PY

y (τ Y (BRt) ≤ t)o = expn − ν Xy∈Z\{0}
0 (τ Y ({−y}) ≤ t)o = expn − ν(cid:0)EY

PY

PY

y (τ Y ({0}) ≤ t)o

0(cid:2)(cid:12)(cid:12)Ranges∈[0,t](Ys)(cid:12)(cid:12)(cid:3) − 1(cid:1)o,

(4.5)

where τ Y (B) denotes the ﬁrst hitting time of a set B ⊂ Zd by Y , and in the second equality, we
used the assumption that Y makes nearest-neighbor jumps. Note that it was shown in [DGRS12] that
− ln P(Ft) ∼ νq 8ρt

Substituting the bounds (4.2)–(4.5) into (4.1), we ﬁnd that in dimension d = 1, the optimal choice
3 , which is determined by the interplay between P(Et) and P(Gt) as t → ∞. If this lower

is Rt = t
bound strategy is optimal, then under P γ

t , X will ﬂuctuate on the scale of t

π .

1
3 .

1

4.2 Rewriting in terms of the range

Averaging out the Poisson initial condition of ξ, we can rewrite (1.3) as

with

Z γ

t,X = expnνXy∈Z

(vX (t, y) − 1)o,
(0)oi,

yh expn − γLY −X

t

vX(t, y) = EY

12

(4.6)

where LY −X

t

When γ = ∞, we deﬁne vX (t, Y ) = PY

y (LY −X

t

0 δ0(Ys − Xs) ds is the local time of Y − X at 0, introduced in (1.10).

(0) = 0), and it is easily seen that

(0) =R t

Z∞t,X = expn − νXy∈Z

PY

y(cid:0)LY −X

t

(0) > 0(cid:1)o = exp(cid:8) − νEY
= exp(cid:8) − νEY

0(cid:2)(cid:12)(cid:12)Ranges∈[0,t](Ys − Xs)(cid:12)(cid:12)(cid:3)(cid:9)
0(cid:2)(cid:12)(cid:12)Ranges∈[0,t](Ys + Xs)(cid:12)(cid:12)(cid:3)(cid:9),

where we used the assumption that Y is symmetric, and for any f ∈ D([0, t], Z),

Ranges∈[0,t](f (s)) := {f (s) : s ∈ [0, t]}

(4.7)

(4.8)

denotes the range.

When γ < ∞, Z γ

t,X admits a similar representation in terms of the range of Y + X. Indeed, let
Nt := {J1 < J2 < ··· } be an independent Poisson point process on [0,∞) with rate γ ∈ (0,∞), and
deﬁne
(4.9)

SoftRanges∈[0,t](f (s)) := {f (Jk) : k ∈ N, Jk ∈ [0, t]}.

Probability and expectation for N will be denoted by adding the superscript N to P and E. We can
then rewrite (4.6) as

Z γ

t,X = expn − νXy∈Z
= exp(cid:8) − νEY,N

0

PY,N

y

(cid:0)0 ∈ SoftRanges∈[0,t](Ys − Xs)(cid:1)o
(cid:2)(cid:12)(cid:12)SoftRanges∈[0,t](Ys + Xs)(cid:12)(cid:12)(cid:3)(cid:9).

4.3 Proof of Sub-diﬀusivity of X

(4.10)

To control the path measure P γ
t (cf. (1.7)) we could try to proceed with the bounds outlined in (4.1)–
(4.5). We cannot use (4.5) directly as we had assumed that Y was a simple random walk for that
particular bound. To circumvent this we use Theorem 1.4. However note that, we are free to use the
bounds (4.1)–(4.4) for Y as in (1.5). Using these with Rt = t

1
3 and (4.10) we observe that

P γ
t (X ∈ ·) ≤

EX

0h exp(cid:8) − νEY,N

0

(cid:2)(cid:12)(cid:12)SoftRanges∈[0,t](Ys + Xs)(cid:12)(cid:12)(cid:3)(cid:9)1X∈·i

3 P(Ft)

e−ct

1

(4.11)

When Y is as in (1.5) and satisﬁes (1.8) then using Theorem 1.4 we have

PY

P(Ft) = expn − ν Xy∈Z\BRt
0h sup
≥ expn − ν(cid:16)EY

y (τ Y (BRt ) ≤ t)o = expn − ν Xy∈Z\BRt
Ysi(cid:17)o ≥ expn − ν(cid:16)EY

Ys − inf
s∈[0,t]

s∈[0,t]

PY

0 (τ Y (y + BRt) ≤ t)o
0h(cid:12)(cid:12)(cid:12)Ranges∈[0,t](Ys)(cid:12)(cid:12)(cid:12)i + C ln t(cid:17)o,

1
3 , this and (4.11) implies that

where we used translation invariance and (1.18). Since ln t ≪ t

P γ
t (X ∈ ·) ≤
1
3 EX

ec1t

0h expn − ν(cid:16)EY,N

0

(cid:2)(cid:12)(cid:12)SoftRanges∈[0,t](Ys + Xs)(cid:12)(cid:12)(cid:3) − EY

0(cid:2)(cid:12)(cid:12)Ranges∈[0,t](Ys)(cid:12)(cid:12)(cid:3)(cid:17)o1X∈·i

for t suﬃciently large. This will be the starting point of our analysis of P γ
t .

(4.12)

13

Proof of Theorem 1.2 for simple random walks. We ﬁrst bound the ﬂuctuation of X under P γ
from
t
below. Since Y is an irreducible symmetric random walk, Pascal’s principle (see [DGRS12, Prop. 2.1],
in particular, [DGRS12, (38) & (49)] for γ < ∞), implies that the expected (soft) range (cf. (4.8)-(4.9))
of a Y walk increases under perturbations, i.e., PX

0 -a.s.,

Also note that by the deﬁnition of F γ

t (·) in (1.12) and the deﬁnition of soft range in (4.9), we have

EY,N

0

0

F γ

(cid:2)(cid:12)(cid:12)SoftRanges∈[0,t](Ys)(cid:12)(cid:12)(cid:3) ≥ 0.

(cid:2)(cid:12)(cid:12)SoftRanges∈[0,t](Ys + Xs)(cid:12)(cid:12)(cid:3) − EY,N
t (Y ) = |Ranges∈[0,t](Ys)| − EN(cid:2)|SoftRanges∈[0,t](Ys)|(cid:3).
t (Y )] =Z ∞

t (Y ) ≥ m) dm ≤Z ∞

1∨ln t dm ≤ C ln t

C(γ)e− mc(γ)

0 (F γ
PY

0

0

0 [F γ
EY

(4.13)

(4.14)

(4.15)

Furthermore, Proposition 2.1 applied to Y , combined with the exponential Markov inequality, gives

for all t large enough. Hence, in combination with (4.12), we obtain

1

3(cid:1)

1
3 EX

t(cid:0)kXkt ≤ αt
P γ
≤ ec1t
≤ ec1t

1
3 EX

0

0

0h expn − ν(cid:16)EY,N
(cid:2)(cid:12)(cid:12)SoftRanges∈[0,t](Ys + Xs)(cid:12)(cid:12)(cid:3) − EY
0h expn − ν(cid:16) EY,N
(cid:2)(cid:12)(cid:12)SoftRanges∈[0,t](Ys + Xs)(cid:12)(cid:12)(cid:3) − EY,N
|
× exp(cid:8)νEY
t (Y )](cid:9)1
0(cid:16)kXkt ≤ αt

3i
3(cid:17) ≤ ec1t

3 +Cν ln teα−2 ln(1−ρ) t

{z

kXkt≤t

t (Y )]PX

0 [F γ

0 [F γ

≥ 0

(4.13)

1

1

1

1

3 eνEY

≤ ec1t

0(cid:2)(cid:12)(cid:12)Ranges∈[0,t](Ys)(cid:12)(cid:12)(cid:3)(cid:17)o1
(cid:2)(cid:12)(cid:12)SoftRanges∈[0,t](Ys)(cid:12)(cid:12)(cid:3)
}

0

kXkt≤αt

(cid:17)o

1

3i

1

3 → 0

as t → ∞,

(4.16)

where we applied (4.4) in the last inequality, with α > 0 chosen suﬃciently small.

Since X and Y are independent continuous time simple random walks, Y + X is also a simple

It remains to bound the ﬂuctuation of X under P γ
t

or γ ∈ (0,∞).
Case 1: γ = ∞. By (4.12), it suﬃces to show that

lim inf
t→∞

t− 1

3−ǫ inf
X:kXkt>t

24 +ǫ(cid:16)EY

11

0(cid:2)(cid:12)(cid:12)Ranges∈[0,t](Ys + Xs)(cid:12)(cid:12)(cid:3) − EY

random walk, which allows us to write

EY

s∈[0,t]

0h(cid:12)(cid:12)Ranges∈[0,t](Ys + Xs)(cid:12)(cid:12)i = EY

0h sup
0h sup
0h sup
where the last equality follows since Y is symmetric.
24 +ǫ}, for
and τ (X) := arg mins∈[0,t]Xs ∈ [0, t],

(Ys + Xs) − inf
s∈[0,t]
(Ys + Xs) + sup
s∈[0,t]
(Ys + Xs) + sup
s∈[0,t]

Now observe that on the event {kXkt ≥ t

σ(X) := arg maxs∈[0,t]Xs

s∈[0,t]

s∈[0,t]

= EY

= EY

11

14

from above. We divide into two cases: γ = ∞

0(cid:2)(cid:12)(cid:12)Ranges∈[0,t](Ys)(cid:12)(cid:12)(cid:3)(cid:17) > 0.
(Ys + Xs)i
(−Ys − Xs)i
(Ys − Xs)i,

(4.17)

(4.18)

where ties are broken by choosing the minimum value, one of the sets

S := {s ∈ [0, t] : Xσ(X) − Xs ≥ t

11

24 +ǫ/2}

and T := {r ∈ [0, t] : Xs − Xτ (X) ≥ t

11

24 +ǫ/2}

has Lebesgue measure λ(S) ≥ t/2 or λ(T ) ≥ t/2. Without loss of generality, we can assume that
λ(S) ≥ t/2. We then have

EY

= EY

s∈[0,t]

(Ys + Xs) + sup
s∈[0,t]
(Ys + Xs) + sup
s∈[0,t]

(Ys − Xs) − 2 sup
s∈[0,t]
(Ys − Xs) − 2 sup
s∈[0,t]

0h(cid:12)(cid:12)Ranges∈[0,t](Ys + Xs)(cid:12)(cid:12)i − EY
0h sup
0h(cid:0) sup
≥ EY
0h(cid:0)Yσ(X) + Xσ(X) + Yσ(Y ) − Xσ(Y ) − 2Yσ(Y )(cid:1)1σ(Y )∈S 1
≥ EY
24 , σ(Y ) ∈ S(cid:1),
≥ (t

0h(cid:12)(cid:12)Ranges∈[0,t](Ys)(cid:12)(cid:12)i
(Ys)i
(Ys)(cid:1)1σ(Y )∈S 1

0(cid:0)Yσ(Y ) − Yσ(X) ≤ t

24 +ǫ/2 − t

24 ) PY

s∈[0,t]

11

11

11

Yσ(Y )−Yσ(X)≤t

11

24i

(4.19)

11

24i

Yσ(Y )−Yσ(X)≤t

where the ﬁrst inequality uses that the diﬀerence of the sup’s in the expectation is non-negative.

It remains to lower bound the probability

PY

0(cid:0)Yσ(Y ) − Yσ(X) ≤ t

Note that PY

11

24 , σ(Y ) ∈ S(cid:1) =ZS

with density

PY

0(cid:0)Yσ(Y ) − Yσ(X) ≤ t

11

24 | σ(Y ) = r(cid:1)PY

0 (σ(Y ) ∈ dr(cid:1).

(4.20)

0 (σ(Y ) ∈ dr) is absolutely continuous with respect to the Lebesgue measure λ(dr)

lim
δ↓0

δ−1PY

0 (σ(Y ) ∈ [r, r + δ]) = ρ PY

0 (Ys ≤ 0∀ s ∈ [0, t − r])Xz<0

pY (z)PY

z (Ys < 0∀ s ∈ [0, r]),

(4.21)

where ρ is the jump rate of Y , pY its jump kernel, σ(Y ) is the ﬁrst time when Y reaches its global
maximum in the time interval [0, t], and we used the observation that given σ(Y ) = r and the size of
the jump at time r, (Ys)0≤s≤r and (Ys)r≤s≤t are two independent random walks. By [LL10, Theorem
5.1.7], if τ[0,∞) denotes the ﬁrst hitting time of [0,∞), then there exist C1, C2 > 0 such that for all
z < 0 and s > |z|2,

.

(4.22)

C1 |z|√s ≤ PY

z(cid:0)τ[0,∞) ≥ s(cid:1) ≤ C2 |z|√s

Substituting the lower bound into (4.21), we ﬁnd that for any δ > 0, there exists a constant c(δ) > 0
such for all t > 0, we have

PY
0 (σ(Y ) ∈ dr) ≥

c(δ)λ(dr)

t

on [δt, (1 − δ)t].

(4.23)

Since λ(S) ≥ t/2 by assumption, to lower bound the probability in (4.20), it only remains to lower
bound
(4.24)

PY

0(cid:0)Yσ(Y ) − Yσ(X) ≤ t

11

24 | σ(Y ) = r(cid:1)

uniformly in r ∈ [δt, (1 − δ)t] with |r − σ(X)| ≥ δt, and in t > 0, for any δ < 1/8.
Note that conditioned on σ(Y ) = r and Yσ(Y )− − Yσ(Y ) = z < 0, (Yσ(Y )−s − Yσ(Y ))s∈(0,r] and
(Yσ(Y )+s − Yσ(Y ))s∈[0,t−r] are two independent conditioned random walks, starting respectively at z
and 0, and conditioned respectively to not visit [0,∞) and [1,∞). Note that such conditioned random
walks are comparable to a Bessel-3 process, although we will only use random walk estimates. We will

15

only consider the case s := σ(X) − r > 0, the case s < 0 is entirely analogous. We then get for (4.24)
the lower bound

PY
0 (Ys ≥ −t

24 , τ[1,∞) ≥ t − r)

11

PY
0 (τ[1,∞) ≥ t − r)

.

(4.25)

By the Markov property, the numerator in (4.25) equals

0 (Ys = x, τ[1,∞) ≥ s)PY
PY

x(cid:0)τ[1,∞) ≥ t − r − s(cid:1)

0 (Ys/3 = y, Y2s/3 = z, Ys = x, τ[1,∞) ≥ s)PY
PY

x(cid:0)τ[1,∞) ≥ t − r − s(cid:1)

PY
0 (Ys/3 = y, τ[1,∞) > s/3)PY

y (Ys/3 = z, τ[1,∞) > s/3)

11

11

11

24 ⌋

0Xx=−⌊t
0Xx=−⌊t
24 ⌋ X√s≤y,z≤2√s
0Xx=−⌊t
24 ⌋ X√s≤y,z≤2√s
0Xx=−⌊t
24 ⌋ X√s≤y,z≤2√s
0Xx=−⌊t
0Xx=−⌊t
0Xx=−⌊t

|x| PY

|x| PY

24 ⌋

11

24 ⌋

11

24 ⌋

C
t

C
t

C
t2

11

11

≥

≥

≥

≥

≥

× PY

z (Ys/3 = x, τ[1,∞) > s/3)PY

≥ C

PY
0 (Ys/3 = y, τ[1,∞) > s/3) ·

x(cid:0)τ[1,∞) ≥ t − r − s(cid:1)

1

x (Ys/3 = z, τ[1,∞) > s/3) ·

ps/3 · PY
x(cid:0)Ys/3 ∈ [√s, 2√s], τ[1,∞) > s/3(cid:1)

0(cid:0)Ys/3 ∈ [√s, 2√s], τ[1,∞) > s/3(cid:1) PY
0(cid:0)τ[1,∞) > s/3(cid:1) PY
x(cid:0)τ[1,∞) > s/3(cid:1)

|x|2 ≥ Ct−2t3· 11

24 = Ct− 5
8 ,

|x − 1|
√t − r − s

(4.26)

where in the third inequality we applied the local limit theorem and (4.22), in the fourth inequality
we used s ≥ δt, and in the ﬁfth inequality we used the fact that conditioned on {τ[1,∞) > s/3},
(Yus/√s)0≤u≤1/3 converges in distribution to a Brownian meander if Y0 ≪ √s as s → ∞ (cf. [B76]).

Since t − r ≥ δt, again by (4.22), we ﬁnd that

PY
0 (Ys ≥ −t

24 , τ[1,∞) ≥ t)

11

PY
0 (τ[1,∞) ≥ t − r)

≥ Ct− 5

8 t

1

2 = Ct−1/8.

Plugging this into (4.20) (recall (4.23)) and the resulting inequality into (4.19), we ﬁnd that (4.17)
holds, which concludes the proof for the case γ = ∞ and X, Y are simple random walks.

16

Case 2: γ ∈ (0,∞). We ﬁrst use (4.12) to upper bound

11

24 +ǫ(cid:1)

t(cid:0)kXkt ≥ t
P γ
≤ ec1t
= ec1t

1
3 EX

1
3 EX

0

0h expn − ν(cid:16)EY,N
0h expn − ν(cid:16) EY
|
× expnνEY

(cid:2)(cid:12)(cid:12)SoftRanges∈[0,t](Ys + Xs)(cid:12)(cid:12)(cid:3) − EY
0(cid:2)(cid:12)(cid:12)Ranges∈[0,t](Ys + Xs)(cid:12)(cid:12)(cid:3) − EY
{z
24 +ǫi,
t (X + Y )]o1

kXkt≥t

0 [F γ

≥ ct

1
3 +ǫ

(4.17)

11

0(cid:2)(cid:12)(cid:12)Ranges∈[0,t](Ys)(cid:12)(cid:12)(cid:3)(cid:17)o1

0(cid:2)(cid:12)(cid:12)Ranges∈[0,t](Ys)(cid:12)(cid:12)(cid:3)
}

(cid:17)o

11

24 +ǫi

kXkt≥t

(4.27)

where we applied (4.14) to Y + X in the last equality.

It is clear from the above bound that
11

On the other hand, by the same calculations as in (4.16), we have

P γ

t(cid:0)kXkt ≥ t

24 +ǫ, EY

0 [F γ

t (X + Y )] ≤ t

1

3 + ǫ

2(cid:1) −→t→∞

0.

(4.28)

P γ

t(cid:16)EY

0 [F γ

t (X + Y )] > t

1

3 + ǫ

0 [F γ

t (X + Y )] > t

1

2(cid:17) ≤ ec1t
≤ ec1t
≤ ec1t
≤ C(γ)ec1t

1

1

0 [F γ

t (Y )] PX

0(cid:0)EY

3 eνEY
3 +Cν ln te− c(γ)t1/3+ǫ/2
3 +Cν ln t− c(γ)t1/3+ǫ/2

1∨ln t EX

1∨ln t EX
0
3 +Cν ln t− c(γ)t1/3+ǫ/2

1∨ln t

1

1

3 + ǫ

2(cid:1)
t (X+Y )]i
t (X+Y )i

0 [F γ
EY

c(γ)
1∨ln t

EY

0he
0he
−→t→∞

0,

c(γ)

1∨ln t F γ

(4.29)

where we have applied Jensen’s inequality and Proposition 2.1. Combined with (4.28), this concludes
the proof for the case γ ∈ (0,∞).

Proof of Theorem 1.2 for general X and Y . When X and Y are non-simple random walks, identities
¡such as (4.18) fails because the range of the walk is no longer the interval bounded between the walk’s
inﬁmum and supremum. Theorem 1.4 allows us to salvage the argument.

The lower bound (4.16) on the ﬂuctuation of X under P γ

t remains valid, using (4.12) for Y as in

For the upper bound on the ﬂuctuation of X under P γ

t , γ ∈ (0,∞], note that by (4.12),

(1.5) and (1.8).

11

24 +ǫ(cid:1)

t(cid:0)kXkt ≥ t
P γ
≤ ec1t
= ec1t

1
3 EX

1
3 EX

0(cid:2)(cid:12)(cid:12)Ranges∈[0,t](Ys)(cid:12)(cid:12)(cid:3)(cid:17)o1

11

24 +ǫi

kXkt≥t

0

0 [F γ

0h expn − ν(cid:16)EY,N
(cid:2)(cid:12)(cid:12)SoftRanges∈[0,t](Ys + Xs)(cid:12)(cid:12)(cid:3) − EY
0h expn − ν(cid:16)EY
0(cid:2)(cid:12)(cid:12)Ranges∈[0,t](Ys)(cid:12)(cid:12)(cid:3)(cid:17)o
0(cid:2)(cid:12)(cid:12)Ranges∈[0,t](Ys + Xs)(cid:12)(cid:12)(cid:3) − EY
24 +ǫi
t (X + Y )]o1
× expnνEY
0h expn − ν(cid:16)EY
0(cid:2) sup
(Ys + Xs)(cid:3) − EY
t (X + Y )]o1
× expnνEY
0h expn − ν(cid:16)EY
0(cid:2) sup
t (X + Y )]o1
× expnνEY

kXkt≥t
(Ys + Xs) − inf
s∈[0,t]
s∈[0,t]
0 [F γ
0 [Gt(Y + X)] + νEY

0(cid:2) sup
24 +ǫi
Ys(cid:3)(cid:17)o
24 +ǫi

(Ys + Xs) + sup
s∈[0,t]
0 [F γ

s∈[0,t]
0 [Gt(Y + X)] + νEY

kXkt≥t
(Ys − Xs) − 2 sup
s∈[0,t]

kXkt≥t

s∈[0,t]

11

11

11

Ys − inf
s∈[0,t]

Ys(cid:3)(cid:17)o

(4.30)

= ec1t

= ec1t

1
3 EX

1
3 EX

17

where we recall from (1.15) that Gt(X) :=(cid:0) sup0≤s≤t Xs − inf 0≤s≤t Xs(cid:1) − |Ranges∈[0,t](Xs)|, and we
used the symmetry of Y in the last equality. Note that when γ = ∞, F γ
implies that

The proof of (4.19) does not require X and Y to be simple random walks, and in particular, it

t (Y + X) = 0.

lim inf
t→∞

t− 1

3−ǫ inf
X:kXkt>t

11
24 +ǫ

EY

0(cid:2) sup

s∈[0,t]

(Ys + Xs) + sup
s∈[0,t]

(Ys − Xs) − 2 sup
s∈[0,t]

Ys(cid:3) > 0.
2(cid:1) −→t→∞

0.

Therefore it follows from (4.30) that

P γ

t(cid:0)kXkt ≥ t

The argument for P γ
as in (4.29) with F γ
have P γ

t (EY

0 [Gt(Y + X)] > t

11

24 +ǫ, EY

0 [F γ

t (X + Y )] ≤ t

1

3 + ǫ

2 , EY

0 [Gt(Y + X)] ≤ t

1

3 + ǫ

(4.31)

0 [F γ

t (EY
2 ) → 0 in (4.29) is still valid, while the same argument
t (Y + X) replaced by Gt(Y + X), together with Theorem 1.4, shows that we also

t (X + Y )] > t

1

3 + ǫ

1

3 + ǫ

2 ) → 0 as t → ∞. This completes the proof.

Acknowledgement. The authors would like to thank the Columbia University Mathematics Depart-
ment, the Forschungsinstitut f¨ur Mathematik at ETH Z¨urich, Indian Statistical Institute Bangalore,
and the National University of Singapore for hospitality and ﬁnancial support. R.S. is supported by
NUS grant R-146-000-185-112. S.A is supported by CPDA grant from the Indian Statistical Institute.

References

[B76]

E. Bolthausen. On a functional central limit theorem for random walks conditioned to stay
positive. Ann. Prob. 4(3):480-485, 1976.

[DGRS12] A. Drewitz, J. G¨artner, A.F. Ram´ırez, and R. Sun. Survival probability of a random walk
among a Poisson system of moving traps. Probability in Complex Physical Systems — In
honour of Erwin Bolthausen and J¨urgen G¨artner, Springer Proceedings in Mathematics.,
11:119–158, 2012.

[DPRZ00] A. Dembo, Y. Peres, J. Rosen, and O. Zeitouni. Thin points for Brownian motion. Ann.

Inst. H. Poincar´e Probab. Statist., 36(6):749–774, 2000.

[DV75]

[DV79]

[LL10]

M. D. Donsker and S. R. S. Varadhan. Asymptotics for the Wiener sausage. Comm. Pure
Appl. Math., 28(4):525–565, 1975.

M. D. Donsker and S. R. S. Varadhan. On the number of distinct sites visited by a random
walk. Comm. Pure Appl. Math., 32(6):721–747, 1979.

G.F. Lawer and V. Limic. Random walk: a modern introduction, volume 123 of Cambridge
Studies in Advanced Mathematics. Cambridge University Press, Cambridge, 2010.

[MOBC03] M. Moreau, G. Oshanin, O. B´enichou and M. Coppey. Pascal principle for diﬀusion-

controlled trapping reactions. Phys. Rev. E 67, 045104(R), 2003.

[MOBC04] M. Moreau, G. Oshanin, O. B´enichou and M. Coppey. Lattice theory of trapping reactions

with mobile species. Phys. Rev. E 69, 046101, 2004.

[PSSS13] Y. Peres, A. Sinclair, P. Sousi, and A. Stauﬀer. Mobile geometric graphs: detection, cov-

erage and percolation. Probab. Theory Related Fields 156, 273–305, 2013.

18

[RD69]

B.C. Rennie and A.J. Dobson. On Stirling numbers of the second kind. Journal of Combi-
natorial Theory 7(2):116–121, 1969.

[S90]

[S03]

U. Schmock. Convergence of the normalized one-dimensional Wiener sausage path measures
to a mixture of Brownian taboo processes. Stochastics Stochastics Rep. 29(2):171–183, 1990.

S. Sethuraman. Conditional survival distributions of Brownian trajectories in a one dimen-
sional Poissonian environment. Stochastic Process. Appl. 103(2):169–209, 2003.

[Szn98]

A.-S. Sznitman. Brownian motion, obstacles and random media. Springer Monographs in
Mathematics. Springer-Verlag, Berlin, 1998.

19

