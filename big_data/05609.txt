6
1
0
2

 
r
a

 

M
7
1

 
 
]

G
C
.
s
c
[
 
 

1
v
9
0
6
5
0

.

3
0
6
1
:
v
i
X
r
a

ABCDepth: eﬃcient algorithm

for Tukey depth

Milica Bogi´cevi´c∗ and Milan Merkle†

University of Belgrade, Faculty of Electrical Engineering, Bulevar Kralja

Aleksandra 73, 11120 Belgrade, Serbia

Abstract. We present a new algorithm for Tukey (halfspace) depth level sets
and its implementation. Given d-dimensional data set for any d ≥ 2, the algorithm
is based on representation of level sets as intersections of balls in Rd, and can be
easily adapted to related depths (Type D, Zuo and Serﬂing (Ann. Stat. 28 (2000),
461–482)). The algorithm complexity is O(dn2 + n2 log n) where n is the data
set size. Examples with real and synthetic data show that the algorithm is much
faster than other implemented algorithms and that it can accept thousands of mul-
tidimensional observations, while other algorithms are tested with two-dimensional
data or with a couple of hundreds multidimensional observations.

Keywords: Data centrality, Multivariate medians, Tukey depth, Algorithm, Com-

plexity computation.

1. Introduction

A basic statistical tasks is to simplify a large amount of data using some
values derived from the dataset as representative points. Among many ways
to choose representative points, a natural idea is to choose those that are
located in the center of data set. One way to deﬁne a center is to deﬁne
what is meant by deepness, and then to deﬁne the center as the set of
deepest points. Although this paper is about multivariate medians and
related notions, for completeness and understanding some ideas, we start
from the univariate case. Talking in terms of probability distributions, let X
be a random variable and let µ = µX be the corresponding distribution, i.e.,
a probability measure on (R,B) so that P (X ≤ x) = µ{(−∞, x]}. A median
of X (or a median of µX ) is any number m such that P (X ≤ m) ≥ 1/2 and
P (X ≥ m) ≥ 1/2. In terms of data set, this property means that to reach
any median point from outside of the data set, we have to pass at least 1/2
of data points, so this is the deepest point within the data set. Here we can
also ﬁnd the depth of any point x ∈ R as
(1) D(x, µ) = min{P (X ≤ x), P (X ≥ x)} = min{µ((−∞, x]), µ([x, +∞))}
The set of all median points {Med µ} is a non-empty compact interval (can
be a singleton). It can be shown that (see [11], [12])

(2)

{Med µ} =

∗antomripmuk@yahoo.com
†emerkle@etf.rs

(cid:92)

J,

J=[a,b]: µ(J)>1/2

1

2

and (2) can be taken for an alternative (equivalent) deﬁnition of univariate
median set. In Rd with d > 1, there are several diﬀerent concepts of depth
and medians (see for example [15], [17], [22]); the algorithm that we propose
is applicable to so called type D depth functions (as in [22]) which can be
obtained by generalizations of (1) to higher dimensions.

2. Depth functions based on families of convex sets

Deﬁnition 2.1. Let V be a family of convex sets in Rd, d ≥ 1, such that:
(i) V is closed under translations and (ii) for every ball B ∈ Rd there exists
a set V ∈ V such that B ∈ V. Let U be the collection of complements of sets
in V. For a given probability measure µ on Rd, let us deﬁne
(3) DV (x; µ) = inf{µ(U ) | x ∈ U ∈ U} = 1 − sup{µ(V ) | V ∈ V, x ∈ V (cid:48)}
The function x (cid:55)→ D(x; µ,V) will be called a depth function based on the
family V.

Remark. Deﬁnition 2.1 is a special case of Type D depth functions as
deﬁned in [22]. The conditions stated in [12] that provide desirable behav-
ior of the depth function, are satisﬁed in this special case, with additional
requirements that sets in V are closed or compact.
Example 2.1. 1◦ Let V be the family of all compact intervals [a, b] ⊂ R. As
shown in [12], the depth function based on V is the same as the one deﬁned
by 1.
2◦ With d = 2, consider the family V of rectangles with sides parallel to
coordinate axes. The corresponding depth function reaches its maximum
Dmax ≥ 1/2 at coordinate-wise median. The same holds for d > 2, with
”boxes” whose sides are parallel to coordinate hyper-planes.
3◦ For d > 1, let K be a closed convex cone in Rd, with vertex at origin,
and suppose that there exists a closed hyperplane π, such that π ∩ K = {0}
(that is, K \ {0} is a subset of one of open halfspaces determined by π).
Deﬁne a relation (cid:22) by x (cid:22) y ⇐⇒ y − x ∈ K. Generalized intervals based
on this partial order can be deﬁned as

[a, b] = {x | x − a ∈ K ∧ b − x ∈ K} = (a + K) ∩ (b − K).

Now let us take V to be a collection of all such intervals with ﬁnite endpoints
and deﬁne the depth by (3).
It can be shown ([12, Section 3]) that the
maximal depth is always ≥ 1/2, and the median set can be found using
formula (2) with generalized intervals.
4◦ Let us consider a family V of all closed halfspaces in R2. Let X be
a random point in R2 with P (X = A) = P (X = B) = P (X = C) = 1/3,
where ABC is an equilateral triangle. Here all points inside and on the
border of the triangle ABC have the depth 1/3 and the depth of other points
is equal to zero. Similar examples can be made for arbitrary dimension (see
Example 4.1. in [12]).

3

From the above examples we see that

• A family V is not uniquely determined by the depth function; if we
start with diﬀerent collections V1 and V2, the corresponding depth
functions based on them can be the same (see [12, Theorem 4.2] for
a set of suﬃcient conditions).
• The maximal depth in d > 1 doesn’t need to be 1/2 as in the scalar
case. The following general result ([12, Theorem 4.1]) says that the
maximal depth has to be 1/(d + 1) or bigger:

Theorem 2.1. Let V be any non-empty family of compact convex subsets of
Rd satisfying the conditions as in Deﬁnition 2.1. Then for any probability
measure µ on Rd there exists a point x ∈ Rd such that DV (x; µ) ≥ 1
d+1 .

The set of points with maximal depth is called the center of distribution
In general, one can observe level sets or depth

and denoted as C(µ,V).
regions of level α which are deﬁned by

(4)

Sα = Sα(µ,V) := {x ∈ Rd | D(x; µ,V) ≥ α}.

Clearly, if α1 < α2 then Sα1 ⊇ Sα2 and Sα = ∅ for α > αm, where αm is

the maximal depth for given probability measure µ.

The borders of depth level sets are called depth contours (in two dimen-
sions) or depth surfaces in general. Let us note that the all statistical in-
ference based on multidimensional depths is performed using level sets and
contours (see [6, 7, 21, 14]), and that it is rarely necessary to ﬁnd a depth
of a particular point. On the other hand, in order to describe level sets and
the center of distribution we do not need to calculate depth functions, as
the next result shows (see Lemma 2.1 and Theorem 2.2. in [12]).
Theorem 2.2. Let D(x; µ,V) be deﬁned for x ∈ Rd as in Deﬁnition 2.1.
Then for any α ∈ (0, 1]

(5)

Sα(µ,V) =

(cid:92)

V.

V ∈V,µ(V )>1−α

(cid:92)

α:Sα(cid:54)=∅

Sα(µ,V)

The center of a distribution is then the smallest non-empty level set;

equivalently,

(6)

C(µ,V) =

Since sets in V are convex, the level sets are also convex.
From (4) and (5) we can see that the depth function can be uniquely

reconstructed starting from level sets.
Corollary 2.1. For given µ and V, let Sα, α ≥ 0 be deﬁned as in (5), with
Sα = ∅ for α > 1. Then the function D : Rd (cid:55)→ [0, 1] deﬁned by
(7)

D(x) = h ⇐⇒ x ∈ Sα, α ≤ h and x (cid:54)∈ Sα, α > h

is the unique depth function such that (4) holds.

4

The algorithm that we propose in this paper ﬁnds level sets, rather then
the depth of particular points, based on the formula (5). The algorithm will
be demonstrated in the case of half-space depth, which is described in the
next section.

3. Half-space depth

The most popular choice among depth functions of Deﬁnition 2.1 is the
one which is based on half-spaces, also called Tukey’s depth [20]. Here V
is the family of all open half-spaces, and the complements are closed half-
spaces, so the usual deﬁnition of Tukey depth is obtained from (3) as

(8)

D(x; µ) = inf{µ(H) | x ∈ H ∈ H},

where H is the family of all closed halfspaces. In this section we consider
only half-space depth, so we use the notation D(x, µ) instead of DV (x, µ).
As already noticed in Section 2, a depth function can be deﬁned based
on diﬀerent families V. We say that families V1 and V2 are depth-equivalent
if DV1(x; µ) = DV2(x; µ) for all x ∈ Rd and all probability measures µ.
Suﬃcient conditions for depth-equivalence are given in [12, Theorem 2.1],
and it was shown there that in the case of half-space depth the following
families are depth-equivalent: a) Family of all open halfspaces; b) all closed
halfspaces; c) all convex sets; d) all compact convex sets; e) all balls.
we can deﬁne V as a set of all balls (hyper-spheres) and level sets as

For determining level sets we choose the last alternative - balls, and so

(9)

Sα(µ,V) =

B.

B∈V,µ(B)>1−α

In the setup with data sets, we have a set of n points {x1, . . . , xn} (with

repetitions allowed) and we may use the counting measure deﬁned as

(cid:92)

n

(cid:92)

(10)
Hence, the level sets in (9) for α ∈ (0, 1] can be found as

µ(A) =

.

#{xi : xi ∈ A}

(11)

Sα(µ,V) =

B∈V,#{xi: xi∈B}≥(cid:98)n(1−α)+1(cid:99)

B.

In practical realization, in the step i we add one ball (Bi) that contains
at least (cid:98)n(1 − α) + 1(cid:99) data points to intersection (11), so that in the k-the
step we have Sα,k = ∩k
i=1Bi. Clearly, Sα,k+1 ⊆ Sα,k. The process can end in
two ways: (i) if for some K we do not have any data points in Sα,K or (ii)
if the number of data points in Sk remains the same for k ≥ K. In the case
(i) we conclude that there are no data points x with D(x) ≥ α. In the case
(ii) the data points x ∈ Sα,K have D(x) ≥ α as well as all points in their
convex hull. In the similar way we can ﬁnd the deepest point(s) among the
points of given data set.

5

However, if we want to estimate level sets and especially the center of the
underlying distribution from its sample, the described procedure may not
be suﬃcient. As an example, consider a uniform distribution µ in the region
bounded by circles x2 + y2 = r2
i , r1 = 1 and r2 = 2. It is easy to see (from
the original deﬁnition (8)) that the depth monotonically increases from 0
outside of the larger circle, to 1/2 at the origin, which is the true median.
With a sample from this distribution we will not have data points inside the
inner circle.

In similar cases and whenever we have sparse data we can still visually
identify depth regions and center, simply by adding artiﬁcial points to the
data set. Let the data set contain points x1, . . . , xn and let xn+1, . . . , xN
be points chosen from uniform distribution in some convex domain that
contains the whole data set. Then we use a modiﬁcation of the described
procedure in such a way that we use augmented data set (all N points) as
a criterium for stopping (in cases (i) and (ii) above), but n in formulas (10)
and (11) is the cardinality of original data set.

In Section 4 we demonstrate the algorithm output for the sparse data set.
The algorithm can be easily adapted to work with other convex sets, so
the appropriate name is Algorithm based on Balls or other Convex sets
to evaluate Depth, or ABCDepth. Although half-space depth is the only
aﬃne invariant among examples in Section 3, other options can be used in
conjunction with data-driven coordinate system [3] and using the geometry
of data sets.

4. ABCDepth: Implementation and The Output

According to ABCDepth, median and level sets can be obtained in three
phases.
In this section we describe each phase and its complexity. The
detailed pseudocode is given in the appendix. For any further details about
implementation authors will share the source code gladly.

2

Phase 1: In order to determine hypersphere that contain (cid:98)n(1−α)+1(cid:99)
data points, we need n(n−1)
inter-distances. This triangular matrix
of distances is stored as a list of lists, where i-th list (i = 1, . . . , n−1)
contains distances di+1,j, j = 1, . . . , i.
Each point is represented as a list of its coordinates, i.e. Xi =
(xi1, xi2, ..., xid), so distance calculation between each two points
needs to be performed taking d steps.
Finally, the complexity of this phase is:

(12)

n(n − 1)

2

O(d

) ∼ O(dn2).

Phase 2: Now we need to populate tuples. The tuples are repre-
sented as a map, where the key is a point, and the value is a list
of (point, distance) pairs. In general, each entry in the map can be
represented as Xi → list(Xj, dij), where j ∈ {1...n} \ i and dij is a

6

distance between Xi and Xj obtained from Phase 1. To populate
this map, the algorithm takes n steps for n points, so the complexity
of this step is O(n).
Each value in the map, i.e.
list of (point, distance) pairs, is sorted
by distance. To sort this list, we use the quicksort algorithm [9]. Its
complexity is O(n log n).
At the end of Phase 2, we have populated and sorted tuples and the
ﬁnal complexity of this phase is:

(13)

O(n2 log n).

Phase 3: In Theorem 2.1 it is shown that αmax ≥ 1
d+1 .

For each value from the map (O(n)) populated in Phase 2, i.e. for
each sorted list with (point, distance) pairs, we take sublist that
contains ﬁrst (cid:98)n(1− α) + 1(cid:99) points (O(1)) and calculate intersections
between those sublists (O(n)).
The algorithm repeats those steps iteratively for each obtained level
set increasing α starting from 1
d+1 in such manner that for each Sk
it holds: αk = αk−1 + 1
d+1 , until Sk = Sk+1 or until
Sk+1 = ∅. The data points x ∈ Sk and all points in their convex hull
are considered as a median and they have D(x) ≥ αk.
Those steps result in an overall complexity of:

n , where α1 = 1

(14)

O(n2).

ABCDepth produces a list of level sets where the last set in the list rep-
resents the median, i.e. points with the maximal depth. Namely, the sets in
the list are ordered and S1 ⊃ S2 ⊃ ... ⊃ Sk, where k is the total number of
level sets, with S0 representing the original data set.

The total algorithm complexity is:

(15)

O(dn2) + O(n2 log n) + O(n2) ∼ O(dn2 + n2 log n).

If the input set is sparse, ABCDepth optionally creates a new data set,
i.e. multiset, R(cid:48) = R1 (cid:93) R2 = {x1 . . . xn} (cid:93) {xn+1 . . . xN}, where R1 =
{x1 . . . xn} is the original data set and R2 = {xn+1 . . . xN} is the data set that
contains N − n points generated from uniform distribution. The algorithm
creates hyperspheres with the center in xi ∈ R(cid:48), i = 1 . . . N that contain
(cid:98)n(1−α)+1(cid:99) points from the original data set, R1. The rest of the algorithm
takes three phases we described above.

In Figure 1. we demonstrate an example when the data is rather sparse.
The data set is the same data set as in [18]; it contains 23 four-dimensional
observations in period from 1966 to 1967 that represent seasonally adjusted
changes in auto thefts in New York city. For the sake of clarity, we take
only two dimensions: percent changes in manpower, and seasonally adjusted

7

changes in auto thefts. The data is downloaded from http://lib.stat.
cmu.edu/DASL/Datafiles/nycrimedat.html. There are 4 level sets. The
ﬁrst level set, Sα,1, is bounded by the orange line and data points x ∈ Sα,1
have 0 ≤ D(x) < 0.33. The second level set, Sα,2, is bounded by red
line and its data points x ∈ Sα,2 have 0.33 ≤ D(x) < 0.37. The third
level set, Sα,3, is bounded by purple line and its data points x ∈ Sα,3 have
0.37 ≤ D(x) < 0.42. The last level set, Sα,4 is considered as a median since
its data points x ∈ Sα,4 has the maximum depth, D(x) ≥ 0.42.

Figure 1. Level sets for the New York crime data set

On Figure 2. and Figure 3. are shown how n and d aﬀect execution
times with respect to the calculated complexity. Tests are performed on
three-dimensional normal distribution, Nd(0, I). Measurements were taken
for d ∈ {2, ..., 10},
n ∈ {40, 80, 160, 320, 640, 1280, 2560, 3000, 3500, 4000, 4500, 5000, 5500, 6000, 6500, 7000},
and averaged on 10 runs.

Figure 4. shows level sets generated from 1000 points in dimension 2.
The distribution is bivariate normal distribution and covariance matrix is
identity matrix.

Figure 5. shows multivariate normal distribution constructed from 1000

points.

The initial version of ABCDepth algorithm was presented at [2]. All data
generators we use in this paper in order to verify and plot the algorithm
output was presented at [1] and it’s available as an open source project at
https://bitbucket.org/antomripmuk/generators.

8

Figure 2. When number of points increases execution time
has quadratic growth.

5. Comparisons and Performances

During the last two decades, a couple of algorithms were proposed. One of
the ﬁrst was HALFMED [13]. Its complexity is also quadratic, O(n2 log2 n),
but the algorithm is limited to bivariate data sets.

Another one, arguably the most popular, is DEEPLOC [18].

It calcu-
lates Tukey median in any dimension with complexity O(kmn log(n + kdn +
md3 + mdn)), where k is the number of steps taken by the program and
m is the number of directions, i.e. vectors constructed by the program.
Measurements in DEEPLOC go up to 1000 points in dimension 5.

In [5] the approximation of the third method shown in [16] is represented.
For one-dimensional and two-dimensional data Tukey depth is computed in
O(n) and O(n log n), respectively. For d ≥ 3 becomes more complicated.
Chan in [4] proposed an algorithm with complexity O(n log n) for d <
3 and O(nd−1) for d ≥ 3.
It has lower computational complexity than
ABCDepth for up to 3 dimensions, but it degrades quickly for higher di-
mensions. The algorithm from [4] has not been implemented so far.

Two algorithms are proposed in [10] for p ≥ 3, with complexity O(nd−1 log n)

and O( nd−1
dimension 3, but degrade for higher dimensions.

2d−2 log n). Their complexity is comparable to ABCDepth for up to

Recently, several algorithms for exact computation of Tukey depth was
proposed in [8] . The ﬁrst one reduces dimensionality to dimension one. The

9

Figure 3. The execution time grows linearly with dimensionality.

overall complexity of the algorithm is O(nd). The second algorithm projects
the data points into the 2-dimensional data space. The complexity of this
algorithm is of order O(nd−1 log n). The third algorithm is recursive which
results in the overall complexity of O(nd−1 log n).

Figure 6. shows IBM stock data distribution given in [10]; daily simple
returns of IBM stock from January 1, 1970 to December 25, 2008. The
data contains 9842 four-dimensional observations. For more informations
about the data see [19]. The data is downloaded from: http://faculty.
chicagobooth.edu/ruey.tsay/teaching/fts3/d-ibm3dx7008.txt.

Table 1. shows computation times for larger sizes of data sets and higher
dimensions. The ABCDepth algorithm has been implemented in Java and
tests are run using one kernel of Intel Core i7 (2.2 GHz) processor.

Although ABCDepth has demonstrated eﬀectiveness of Tukey depth cal-
culation, our plan is to implement a randomized algorithm that reduces the
initial number of points, that should be considered as hyperspheres’ cen-
ters. Also, the eﬀectiveness can be improved by dimensionality reduction,
i.e. feature extraction.

An interesting idea would be to calculate the points’ distances as an ap-
proximation of Euclidian distance. If this is feasible, ABCDepth complexity
would not depend on the number of dimensions.

The most obvious purpose of ABCDepth is classiﬁcation of multidimen-
sional data. Once the median and the level sets have been extracted from

10

640
1280
2560
3000
3500
4000
4500
5000
5500
6000
6500
7000

Figure 4. The deepest point is at the origin.

10
9
4
0.098
0.094
0.088
0.309
0.310
0.273
1.834
1.634
1.312
2.231
2.166
1.846
2.830
2.887
2.692
3.817
3.854
3.320
4.567
4.854
4.100
5.613
5.550
4.986
6.994
7.272
6.195
8.242
8.266
7.348
10.164 10.467
8.973
10.404 10.811 10.883 11.364 11.339 11.285 11.634

7
6
0.094
0.088
0.301
0.286
1.403
1.530
2.261
1.931
2.768
2.709
3.734
3.662
4.720
4.388
5.264
5.304
6.608
6.563
8.943
7.880
10.447 9.591

5
0.091
0.286
1.668
1.921
2.557
3.560
4.237
5.355
6.591
7.507
9.041

8
0.097
0.315
1.453
2.367
2.821
3.817
4.983
5.451
6.689
8.111
9.788

Table 1. Computation times (in seconds)

the initial set, we can do an online classiﬁcation of new data with same fea-
tures. The data that belongs to same level set would be considered similar,
based on minimal diﬀerences of their features.

11

Figure 5. 1000 points generated from 3D Gaussian distribution.

Acknowledgement

We would like to show our gratitude to Anja Struyf, Univ.

Instelling
Antwerpen, for sharing the code and the data that was used in their papers
that have the immense importance in this area.

Appendix A. Pseudocode

The complete code takes three successive phases:
(1) Populate structure that stores distances between pairs of points.

c a l c u l a t e d i s t a n c e s ( data ) :

# i n i t
l i s t o f

s t r u c t u r e

f o r

s t o r i n g d i s t a n c e s :

l i s t o f

l i s t s

l i s t s

f o r

t o data . l e n g t h :

i
r e f p o i n t = data [ i ] ;

# O(n)

12

Figure 6. IBM stock, data distribution. The median is rep-
resented as the smallest circle in the center.

# i n i t d i s t a n c e
l i s t

l i s t

f o r ith p o i n t

f o r

t o j < i :

j = i
p o i n t = data [ j ] ;
l i s t . add ( r e f p o i n t . g e t d i s t a n c e ( p o i n t ) ) # O(d)

# O( n−1
2 )

# add p o p u l a t e d l i s t
l i s t o f
l i s t s . add ( i ,

t o ith p o s i t i o n i n l i s t o f
l i s t ) ;

l i s t s

(2) Populate map: key is a point, and the value is a list of (point, distance)

pairs.

p o p u l a t e t u p l e s ( data ) :

# i n i t
t u p l e s m a p

s t r u c t u r e

f o r

s t o r i n g t u p l e s : map o f

l i s t s

f o r

i

t o data . l e n g t h :

# O(n)

13

r e f p o i n t = data [ i ] ;

# i n i t
l i s t

t u p l e s

l i s t

f o r ith p o i n t

f o r

j = 0 t o j < data . l e n g t h :

# O(n)

t u p l e o b j e c t

# i n i t
t u p l e = t u p l e . s e t ( data [ j ] ) ;
t u p l e = t u p l e . s e t d i s t a n c e ( l i s t o f

l i s t s ( i ,

j ) ) ;

l i s t . add ( t u p l e ) ;

# s o r t
s o r t ( t u p l e s )

t u p l e s

# O(n log n)

t u p l e s m a p . put ( r e f p o i n t ,

l i s t )

(3) Get the median and level sets.

g e t m e d i a n l e v e l s e t s ( data , α ) :

# i n i t
s e t

s t r u c t u r e

f o r

s t o r i n g median p o i n t s :

s e t

# c a l c u l a t e number o f p o i n t s
# t h a t
c o n t a i n .
num = Math . f l o o r ( data . l e n g t h ∗ ( 1 − α ) + 1 )

s p h e r e s w i l l

f o r

t o t u p l e s m a p . s i z e :

i
s u b l i s t = t u p l e s m a p . g e t v a l ( i ) . s u b l i s t ( 0 , num ) ;

# O(n)

s e t . r e t a i n ( s u b l i s t ) ;

# O(n)

r e t u r n s e t ;

References

[1] M. Bogicevic and M. Merkle. Multivariate Medians and Halfspace Depth: Algorithms
and Implementation. In Proc. 1st International Conference on Electrical, Electronic
and Computing Engineering (IcETRAN 2014), Vrnjaˇcka Banja, Serbia, volume 1,
page 27, 2014.

14

[2] M. Bogicevic and M. Merkle. Data Centrality Computation: Implementation and
Complexity Calculation. In Proc. 2nd International Conference on Electrical, Elec-
tronic and Computing Engineering (IcETRAN 2015), Srebrno Jezero, Serbia, vol-
ume 1, page 23, 2015.

[3] B. Chakraborty and P. Chaudhuri. On a transformation and re-transformation tech-
nique for constructing an aﬃne equivariant multivariate median. Proc. Amer. Math.
Soc., 124(8):2539–2547, 1996.

[4] T. M. Chan. An Optimal Randomized Algorithm for Maximum Tukey Depth. In
Proceedings of the Fifteenth Annual ACM-SIAM Symposium on Discrete Algorithms,
pages 430–436. ACM, New York, 2004.

[5] Dan Chen, Pat Morin, and Uli Wagner. Absolute approximation of Tukey depth:

Theory and experiments. Comput. Geom., 46:566–573, 2013.

[6] D. L. Donoho and M. Gasko. Breakdown properties of location estimates based on

halfspace depth and projected outlyingness. Ann. Statist., 20:1803–1827, 1992.

[7] S. Dutta, A. K. Ghosh, and P. Chaudhuri. Some intriguing properties of Tukey’s

half-space depth. Bernoulli, 17:1420–1434, 2011.

[8] Rainer Dyckerhoﬀ and Pavlo Mozharovskyi. Exact computation of the halfspace

depth. Computational Statistics & Data Analysis, 98:19–30, 2016.

[9] C. A. R. Hoare. Algorithm 64: Quicksort. NSERC Research Grant, Comm. Acm.:321,

1961.

[10] Xiaohui Lui. Fast Implementation of the Tukey depth. arXiv, 2014.
[11] Milan Merkle. Jensen’s inequality for medians. Stat. Prob. Letters, 71:277–281, 2005.
[12] Milan Merkle. Jensen’s inequality for multivariate medians. J. Math. Anal. Appl.,

370:258–269, 2010.

[13] P. J. Rousseeuw and I. Ruts. Constructing the Bivariate Tukey Median. Statist.

Sinica, 8:827–839, 1998.

[14] P. J. Rousseeuw and I. Ruts. The depth function of a population distribution. Metrika,

49:213–244, 1999.

[15] Peter J. Rousseeuw and Mia Hubert. Statistical depth meets computational geometry:

a short survey. arXiv, 2015.

[16] Peter J. Rousseeuw and Anja Struyf. Computing location depth and regression depth

in higher dimension. Statistics and Computing, 8:193–203, 1998.

[17] C. G. Small. A survey of multidimensional medians. arXiv, 58:263–277, 1990.
[18] A. Struyf and P. J. Rousseeuw. High-dimensional computation of the deepest location.

Comp. Statist. & Data Anal., 34:415–426, 2000.

[19] R.S. Tsay. Analysis of Financial Time Series, Third Edition. John Wiley & Sons.,

2010.

[20] John Tukey. Mathematics and Picturing Data. In Proc. International Congress of

Mathematicians, Vancouver 1974, volume 2, pages 523–531, 1975.

[21] Y. Zhou and R. Serﬂing. Multivariate spatial U-quantiles: A Bahadur-Kiefer rep-
resentation, a Theil-Sen estimator for multiple regression, and a robust dispersion
estimator. J. Statist. Plann. Inference, 138:1660–1678, 2008.

[22] Y. Zuo and R. Serﬂing. General notions of statistical depth function. Ann. Stat.,

28:461–482, 2000.

