Solutions of Word Equations over Partially

Commutative Structures

Volker Diekert1, Artur Je˙z2, Manfred Kufleitner1,∗

1 Institut f¨ur Formale Methoden der Informatik, Universit¨at Stuttgart, Germany

2 Institute of Computer Science, University of Wroc law, Poland

6
1
0
2

 
r
a

M
9

 

 
 
]
L
F
.
s
c
[
 
 

1
v
6
6
9
2
0

.

3
0
6
1
:
v
i
X
r
a

Abstract. We give NSPACE(n log n) algorithms solving the following decision problems.
Satisﬁability: Is the given equation over a free partially commutative monoid with involution
(resp. a free partially commutative group) solvable? Finiteness: Are there only ﬁnitely many
solutions of such an equation? PSPACE algorithms with worse complexities for the ﬁrst problem
are known, but so far, a PSPACE algorithm for the second problem was out of reach. Our results
are much stronger: Given such an equation, its solutions form an EDT0L language eﬀectively
representable in NSPACE(n log n). In particular, we give an eﬀective description of the set of
all solutions for equations with constraints in free partially commutative monoids and groups.

1. Introduction

Free partially commutative monoids (a.k.a. trace monoids) and groups (a.k.a. RAAGs: right-angled Artin
groups) are well-studied objects, both in computer science (latest since [18]) and in mathematics (with
increasing impact since [24]). For years, decidability of the satisﬁability problem (i.e., the problem whether
a given equation is solvable) over these structures was open. A positive solution for trace monoids was
obtained by Matiyasevich [17] and for RAAGs by Diekert and Muscholl [8]. The known techniques did
not cope with the ﬁniteness problem (i.e., the problem whether a given equation has only ﬁnitely many
solutions). Decidability of ﬁniteness for trace monoids was wide open, whereas for RAAGs a sophisti-
cated generalization of Razborov-Makanin diagrams and geometric methods, available for groups, yielded
decidability [3], but without any complexity estimation.

We give a simple (almost trivial) and eﬀective method to describe the set of all solutions for equations
with constraints in free partially commutative monoids and groups; the correctness proof is mathematically
challenging. Once the correctness is established, the simplicity is also reﬂected in a surprisingly low
complexity for this kind of result. We give an upper bound of NSPACE(n log n) for both satisﬁability and
ﬁniteness—each problem for trace monoids as well as for RAAGs. Even for satisﬁability this complexity
improves the previously known upper bounds. On the other hand these problems are NP-hard. It remains
open whether NSPACE(n log n) is optimal.

To obtain these results we apply a recent recompression technique [12], which was used as a simple
method to solve word equations. It applies simple compression operations: compress ab into a letter c;
and modify the equation so that such operations are sound. An algebraic setting of the current paper
enables a shift of perspective: the inverse operation, replacing c by ab, is an endomorphism. Thus, the set
of all solutions of an equation (solvable or not) can be represented as a graph, whose nodes are labeled with
equations and edges by endomorphisms of free monoids. This graph can also be seen as a nondeterministic
ﬁnite automaton (NFA) that accepts a rational set of endomorphisms over a free monoid. (Recall that a
subset in a monoid M is rational if it is accepted by some NFA whose transitions have labels from M .)
It is known that applying a rational set of endomorphisms to a letter yields an EDT0L language [1], and
our construction guarantees that the obtained EDT0L language describes exactly the set of all solution

∗The last author was supported by the DFG grants DI 435/5-2 and KU 2716/1-1.

1

of the given equation. Moreover, as usual in automata theory, the structure of the NFA reﬂects whether
the solution set is ﬁnite. Last not least, our method is conceptually simpler than all previously known
approaches to solving equations over free partially commutative structures.

Related work. Studying word equations is part of combinatorics on words for more than half a century
[2]. From the very beginning, motivation came partly from group theory: the goal was to understand
and parametrize solutions for equations in free groups. For example, Lyndon and Sch¨utzenberger needed
sophisticated combinatorial arguments to give a parametrized solution to the equation am = bncp in a
free group [14]. On the other hand, it is known that a parametric description of the solution set is not
always possible [10]. The satisﬁability of word equations in free monoids and free groups became a main
open problem due to its connection with Hilbert’s tenth problem. The problem was solved aﬃrmative by
Makanin in his seminal papers [15, 16]. His algorithms became famous also due to the diﬃculty of the ter-
mination proof and the extremely high complexity. A breakthrough to lower the complexity was initiated
by Plandowski and Rytter [21], who were the ﬁrst to apply compression techniques on word equations.
More precisely, they showed that every solution is highly compressible. Since this result, compression is
a key tool for solving word equations. Indeed compression was essential in showing that the satisﬁability
of word equations is in PSPACE [19]. This approach was further developed [12] using the “recompression
technique”, which simpliﬁed all existing proofs for solving word equations; in particular, it provided an
eﬀective description of all solutions; a similar representation was given earlier by Plandowski [20]. In free
groups, an algorithmic description of all solutions was known much earlier due to Razborov [22]. His de-
scription became known as a Makanin-Razborov diagram, a major tool in the positive solution of Tarski’s
conjectures about the elementary theory in free groups [13, 23]. None of these results provided a structural
result on the set of all solutions, interest in such results was explicitly expressed [11] by asking whether
it is an “indexed language” language. Apparently, this question was posed without too much hope that a
positive answer is within reach. However, the answer was positive for quadratic equations [9] (which is a
severe restriction); the general case was established in [4]. Actually, a stronger result holds: the set of all
solutions for equations in free monoids (as well as in free groups) is an EDT0L language, which is a proper
subclass of indexed languages. The closest results on word equations with partial commutation are in [8],
but techniques used there do not apply here as they boil down to a purely combinatorial construction of
a normal form and ignore the algebraic structure as well as the set of all solutions.

2. Main result

Resource monoids and groups. Given a ﬁnite alphabet Γ, the free monoid Γ∗ is the set of all ﬁnite words
over Γ with concatenation. The empty word is denoted by 1. The length of a word w is denoted by |w|;
by |w|a we count how often the letter a appears in w. A resource function ρ : Γ → 2R maps elements of Γ
to subsets of a ﬁnite set of resources R. We assume that R is of constant size. The pair (Γ, ρ) is called a
resource alphabet. If ρ(a) = S, then a is called an S-constant ; a nonempty sequence of S-constants is an
S-run.

A resource monoid M (Γ, ρ) is the quotient of all ﬁnite words Γ∗ by a partial commutation: M (Γ, ρ) =
Γ∗/ {ab = ba | ρ(a) ∩ ρ(a) = ∅} , i.e., letters a 6= b commute if and only if they do not share a resource.
Resource monoids can equivalently be seen as free partially commutative monoids or trace monoids. We
choose the resource-based approach as it best suits our purposes. Elements of a resource monoid are called
traces. The natural projection π maps elements of the free monoid Γ∗ to traces in M (Γ, ρ); this is not
a bijection and we view w ∈ Γ∗ as a word representation of the trace π(w).
In a monoid, an element
v is a factor of w if w = pvq for some p, q. We assume that the monoid M (Γ, ρ) is equipped with an
involution, that is, a bijection x 7→ x on M (Γ, ρ) such that x = x, xy = y x for all x, y ∈ M (Γ, ρ). To
make the deﬁnition well deﬁned, we require that ρ(x) = ρ(x) for x ∈ Γ. In the following, a trace monoid
means a resource monoid with involution. A morphism ϕ : M → M ′ between monoids with involution is
a homomorphism additionally respecting the involution. If ∆ is a subset of M , then we often denote the
restriction of ϕ to ∆ by ϕ. If ϕ(d) = d for all d ∈ ∆, then ϕ is a ∆-morphism.

If there is no letter a ∈ Γ with a = a, then, by adding deﬁning relations aa = 1 for all a ∈ Γ, we
obtain the free partially commutative group G(Γ, ρ). Free partially commutative groups are also known as
right-angled Artin groups or RAAGs for short. As a set, we can identify a RAAG G(Γ, ρ) with the subset

2

traces of the trace monoid M (Γ, ρ) without factors aa. Such traces are called reduced. We take inversion
on groups as involution; the canonical projection of the monoid M (Γ, ρ) onto the group G(Γ, ρ) respects
the involution.

Equations with constraints. Let (Γ, ρ) be a resource alphabet. An equation is a pair of words (U, V )
over an alphabet Γ = A ∪ X has a partition into constants A and variables X , both sets are closed
under involution. A constraint is a morphism µ : M (Γ, ρ) → N , where N is a ﬁnite monoid with
involution. For our purposes, it is enough to consider constraints such that the elements of N can be
represented by O(log |Γ|) bits, and that all necessary computations in N (multiplication, involution, etc.)
can be performed in space O(log |Γ|) and the speciﬁcation of these operations requires O(|Γ| log |Γ|) space.
If (U, V ) is an equation over (Γ, ρ), then we deﬁne the input size of an equation with constraints as
n = |U V | + |Γ|.

We write (U, V, µ) for an equation (U, V ) with constraints µ. A solution of (U, V, µ) over M (A, ρ) is an
A-morphism σ : M (A ∪ X , ρ) → M (A, ρ) such that σ(U ) = σ(V ) and µσ(X) = µ(X) for all X ∈ X . If
the equation is over G(A, ρ), then instead of σ(U ) = σ(V ) we require πσ(U ) = πσ(V ) for the canonical
projection π : M (A, ρ) → G(A, ρ). We also say that σ solves (U, V, µ) in M (A, ρ) (resp. in G(A, ρ)). For
equations over G(A, ρ) we only allow solutions where the trace σ(X) is reduced for all X ∈ X . The main
result of this paper is that the set of all solutions of a trace equation (resp. an equation in a RAAG) with
rational constraints is an eﬀectively computable EDT0L language, and the underlying automaton reﬂects
whether there are inﬁnitely many solutions.

Theorem 1. [Monoid version] There is an NSPACE(n log n) algorithm for the following task. The input
is a resource alphabet (A ∪ X , ρ) with involution and a trace equation (U, V, µ) with constraints µ in
constants A and variables X = {X1, . . . , Xk}. The algorithm computes an alphabet C ⊇ A of size O(n),
constants c1, . . . , ck ∈ C, and an NFA A accepting a rational set R of A-endomorphisms on C∗ such that:
h(C∗) ⊆ A∗ for all h ∈ R and under the canonical projection π : A∗ → M (A, ρ) we have

{(πh(c1), . . . , πh(ck)) | h ∈ R} = {(σ(X1), . . . , σ(Xk)) | σ solves (U, V, µ) in M (A, ρ)} .

Thus, the set of all solutions is an eﬀectively computable EDT0L language. Furthermore, (U, V, µ) has a
solution if and only if A accepts a nonempty set; (U, V, µ) has inﬁnitely many solutions if and only if A
has a directed cycle. These conditions can be tested in NSPACE(n log n).
[Group version] The same, but solutions σ satisfy σ(U ) = σ(V ) in the RAAG G(A, ρ) and for a variable
X the solution σ(X) is restricted to be a reduced trace.

Theorem 1 generalizes to systems of equations. Another generalization are ﬁnitely generated graph
products with involution over free monoids, free groups, and ﬁnite groups. See [7] for a deﬁnition and the
known results concerning solvability of equations in graph products. This generalization is rather technical
but does not reveal new ideas; it is done elsewhere.

3. Basic concepts

Groups via monoids. Equations in RAAGs can be reduced to equations in resource monoids [8], such
an approach is standard since its introduction for free groups [6], which are reduced to free monoids. In
essence, the reduction simulates the inverse operation by involution and it enforces that the solution in
the monoids is in the reduced form by (additional) constraints. We employ a similar approach; thus, our
presentation focuses on the equations over resource monoids.

Simpliﬁcations. Using a standard technique one can ensure that there are no self-involuting constants
in the initial equation [8]. This step is not needed for RAAGs as a = a = a−1 implies a2 = 1 in groups,
but RAAGs are torsion-free. For technical reasons we introduce a new special symbol #, which serves as
a marker and becomes the only self-involuting constant; set ρ(#) = R and ∅ 6= ρ(a)   R for all other
constants. We let

Winit = #X1# · · · Xk#U #V #U #V #Xk# · · · X1#.

During the process, the #’s will not be touched, so we keep control over the preﬁx corresponding to
#X1# · · · #Xk# which encodes the tuples (σ(X1), . . . , σ(Xk)). Moreover, we have σ(U ) = σ(V ) if and

3

only if σ(W ) = σ(W ). Thus, we can treat a single trace as an equation. Solutions become A-morphisms
σ satisfying σ(W ) = σ(W ).

Monoids and equations. The equations that we consider are over a more general structure than a
trace monoid. To simplify the notion, we denote the equation and a monoid over which it is by a tuple
(W, B, X , ρ, θ, µ), where W ∈ (B ∪ X )∗ is the “equation” with constants B and variables X , the mapping
ρ : B ∪ X → 2R is the resource function, and µ : M (B ∪ X , ρ) → N represents the constraints (given by
the mapping µ : B ∪ X → N ). The symbol θ refers to a “type” which adds partial commutation. A type
is given by a list of certain pairs (x, y) ∈ (B ∪ X )+ × B+; and each such pair yields a deﬁning equation
xy = yx. For example, we typically have (X, y) ∈ θ when considering a solution σ with σ(X) ∈ y+. Then
ρ(X) = ρ(y), but we wish to use the fact that σ(X)y = yσ(X). This is the purpose of a type. We only
use types in the subprocedures of block and quasi-block compression, see Section 4.2.1. Such a monoid is
denoted as M (B ∪ X , ρ, θ, µ). In most cases, θ is empty. Then we use (W, B, X , ρ, µ) as an abbreviation
of (W, B, X , ρ, ∅, µ) and M (B ∪ X , ρ, θ, µ) as an abbreviation of M (B ∪ X , ρ, µ).

A B-solution of (W, B, X , ρ, µ) is a B-morphism σ : M (B ∪ X , ρ, µ) → M (B, ρ, µ) such that σ(W ) =

σ(W ) (i.e., it solves the equation) and µ(σ(X)) = µ(X) (i.e., it satisﬁes the constraints).

Increasing the resources. During the algorithm we “increase the resources” of constants. It is useful to
assume that for every constant a ∈ A and every set of resources S with ρ(a)   S the alphabet A has a
corresponding constant with set of resources S. We denote such a constant by (a, S), the involution on it
is deﬁned by (a, S) = (a, S). We naturally identify a with (a, ρ(a)). We assume that the initial alphabet
A is closed under taking such constants, i.e., if a ∈ A and ρ(a) ⊆ S, then (a, S) ∈ A.

In some cases, when we “increase the resources”, we prefer to use a fresh constant of appropriate
resources: For a constant a with ρ(a)   S, by [a, S] we denote a “fresh” S-constant outside A such that
ρ([a, S]) = S, µ([a, S]) = µ(a) and [a, S] = [a, S]; replacing a with [a, S] is called lifting.

Operations, involution and constraints. During the algorithm we perform various operations on variables
and constants. As a rule, whenever we perform such an operation, we perform a symmetric action on the
involuted constants/variables. That is, whenever we replace X by aX, we replace X by X a; and when
we replace ab by c, then we also replace b a by c. This simpliﬁes the description, as actions performed on
“the right side” of X are in fact actions performed on “the left side” of X.

Whenever we perform operations on variables/constants, we want the constraints and resources to
remain unaﬀected (except for lifting, in which case we explicitly change the set of resources); if we replace
a trace W by a trace W ′, then (if not explicitly stated otherwise) we ensure that ρ(W ) = ρ(W ′) and
µ(W ) = µ(W ′). For instance, when replacing X by aX ′, we set µ(X ′) so that µ(aX ′) = µ(X). The same
applies to ρ. Similarly, when replacing ab by c, we set ρ(c) = ρ(ab) and µ(c) = µ(ab). In particular, we do
not mention in the description of the procedures that we perform such operations.

Hasse diagrams and arcs. A trace a1a2 · · · an has many word representations and we would like to
formalize a notion that some constants occur before others (in all word representations). To this end
consider a set of positions {1, 2, . . . , n} and the smallest partial order (cid:22) such that i (cid:22) j if both i ≤ j
and ρ(ai) ∩ ρ(aj) 6= ∅. A Hasse diagram H(W ) of a trace W = a1a2 · · · an is a graph with a set of nodes
{1, 2, . . . , n}, labeled with a1, a2, . . . , an. It contains (directed) edges between immediate successors, i.e.,
(i, j) is an edge if i ≺ j and i (cid:22) k (cid:22) j implies k ∈ {i, j}. By a standard result in trace theory [18], we have
W = W ′ in M (Γ, ρ) if and only if H(W ) and H(W ′) are isomorphic as abstract node-labeled directed
graphs. When considering traces we usually work with their Hasse diagrams. If this causes no confusion,
we identify W = a1 · · · an with H(W ) and refer to labels a1, a2, . . . rather than to node names.

A constant a ∈ A is minimal in a trace W if it is minimal in its Hasse diagram, which means that
W = aY for some trace Y . We denote the set of minimal elements of W by min(W ). Maximal elements
are left-right dual; they are denoted by max(W ).

An arc a → b is an S-arc if S ∈ {ρ(a), ρ(b)}; it is balanced if ρ(a) = ρ(b), unbalanced otherwise.

4. NFA recognising the set of all solutions

In this section we deﬁne the NFA A that recognises the set of all solutions of a trace equation, treated as
a set of endomorphisms End(C∗) over an alphabet C ⊇ A.

4

4.1. The automaton

Alphabet. We ﬁrst ﬁx an alphabet of constants C ⊇ A of size κn, where κ is a suitable constant
(which depends on the number of resources |R|, viewed as O(1)) and a set of variables Ω with |Ω| ≤ |C|.
Henceforth, we assume A ⊆ B ⊆ C and X ⊆ Ω.

States. The states of the automaton A are equations of the form (W, B, X , ρ, θ, µ). Each state V =
(W, B, X , ρ, θ, µ) has a weight kV k which is 5-tuple of natural numbers:

kV k = (|W | , ω, ω′, |W | − |θ| , |B|) ∈ N5

with ω =Pa∈B(|R|−|ρ(a)|)·|W |a and ω′ = |W |−|{a ∈ B | |W |a ≥ 1}|. We order tuples in N5 lexicograph-

ically. The NFA contains only states V whose max-norm kV k∞ = max {|W | , ω, ω′, |W | − |θ| , |B|} ∈ N is
at most κ′n for a suitable constant κ′.

The initial state is (Winit, Ainit, Xinit, ρinit, µinit), it corresponds to the input equation. A state (W, B, ∅, ρ, µ)

without variables is ﬁnal if W = W and W has the preﬁx #c1# · · · #ck#, where c1, . . . , ck are the dis-
tinguished constants. We require that the initial state has no incoming and the ﬁnal states no outgoing
transitions.
Transitions. The transitions (say between the states V = (W, B, X , ρ, θ, µ) and V ′ = (W ′, B′, X ′, ρ′, θ′, µ′))
are labeled by A-morphisms and they either aﬀect the variables (substitution transitions), or the monoid
(compression transitions). The former is formalized using a B-morphism τ : M (B ∪ X , ρ, θ, µ) →
M (B′ ∪ X ′, ρ′, θ′, µ′). In this case we put several requirements on the equations: the new equation should
be obtained by substitution τ (X) for each X, there are no new constants, resources and constraints of X
and τ (X) should be the same; this is formalized as

W ′ = τ (W ), B′ = B,

ρ = ρ′τ, µ = µ′τ.

(1)

Moreover, τ (X) is either the empty word (it removes X from W ) or τ (X) ∈ X ∗B+X ∗ (at least one constant
pops up in the substituted variable). Note that the requirement W ′ = τ (W ) implicitly upper-bounds the

size kτ k, deﬁned asPa∈B∪X |τ (a)|, to be linear.

Furthermore, as B′ = B we have a natural identity morphism from M (B′, ρ′, θ′, µ′) to M (B, ρ, θ, µ),
call it the associated morphism and denote it by ε. This morphism labels the transition, its direction is
opposite of the transition and τ ; we denote the transitions from V to V ′ with a corresponding morphism
h by V h−→ V ′.

A compression transition leaves the variables invariant and so it is deﬁned by an (A ∪ X )-morphism
h : M (B′ ∪ X , θ′, ρ′) → M (B ∪ X , θ, ρ), note that it could be that θ 6= θ′ which corresponds to a type
introduction or removal; this is the associated morphism in this case. A morphism h deﬁned by, say
h(c) = ab, represents a compression of a factor ab into a single letter c. For its properties, W is obtained
by decompression of new constants, and the resources and constraints are preserved:

W = h(W ′),

ρ′ = ρh, µ′ = µh.

(2)

As in the case of substitutions, the assumption that W = h(W ′) implicitly upper-bounds khk to linear
values.

The transition in the NFA is in the direction of the compression which is opposite to direction of
the morphism h. Note that W = h(W ′) implies kV ′k < kV k. For technical reasons we do not allow
compression transitions which introduce self-involuting letters (such as c 7→ aa); we never compress the
marker symbol #. Moreover, following the last compression transition to ﬁnal states, the restriction
kV ′k < kV k is not applied.

Trimming. So far the deﬁned NFA can have many useless states, so as a last step we trim the automaton,
i.e., we remove all vertices not appearing on some accepting path.

Conclusion. The algorithmic part is ﬁnished: A can be constructed using standard arguments in
NSPACE(n log n).

Recognised language. By the usual deﬁnition, the recognized language R consists of all A-morphisms
h1h2 · · · hk, where h1, h2, . . . , hk are consecutive labels on an accepting path. We claim that the set of all
solutions is exactly {(πh(c1), . . . , πh(ck)) | h ∈ R} where π : A∗ → M (A, ρ) is the natural projection.

5

The correctness proof boils down to show that we can calculate the exact constants κ, κ′ (depending on
R but not on n) and to prove soundness and completeness, i.e., that h ∈ R yields a solution and that every
solution can be obtained in this way. Out of those, soundness is relatively easy to show, see Section 4.1.1,
the completeness argument spans over Sections 4.2–4.3. Those arguments also show the other claims on
the automaton (conditions for emptiness and acyclicity).

4.1.1. Soundness

As the ﬁnal states have only one solution (identity), using an induction on the following Lemma, any
accepting path labeled with h1, . . . , hk yields a solution πh1 · · · hk, which shows soundness.

Lemma 2. Given two states V = (W, B, X , ρ, θ, µ) and V ′ = (W ′, B′, X ′, ρ′, θ′, µ′), if V h−→ V ′ and V ′
has a B′-solution σ′ then V has a B-solution σ = hσ′.

The proof follows by mechanical application of (1) or (2).

4.2. On-the-ﬂy construction of the NFA

While we described the NFA recognizing all solutions, we did not discuss how to ﬁnd the appropriate
constants κ, κ′ nor how to show completeness. For this it is easier to ﬁrst describe the construction as an
“on-the-ﬂy” algorithm, that is, given an equation (W, B, X , ρ, θ, µ) (= current state V of the NFA) and
its B-solution σ we will transform it into a diﬀerent equation (W ′, B′, X ′, ρ′, θ′, µ′) (= next state V ′ of the
NFA) and a corresponding B′-solution σ′, where V h−→ V ′ and σ = hσ′. Thus we moved from one state
of the NFA to the other, without the knowledge of the full NFA. Note that the solutions are not given
explicitly, but they are “used” in the nondeterministic choices of the algorithm.

For a ﬁxed set of resources S traces consisting only of S-constants and variables behave as words and
we apply to them the known recompression approach: we iteratively apply compression operations to
S-runs (so we replace S-runs by new single S-constants). Those operations are applied on constants in
the equation, but conceptually we apply them to a solution of the trace equation. To make this approach
sound, we also modify the variables, by popping S-constants from them. We apply these operations
until each S-run (in the solution) is reduced to a single S-constant; when the compressions and popping
operations are applied in appropriate order, the size of the trace equation remains linear.

Compression of S-runs alone is not enough, as there are constants of diﬀerent resources in the solution
of the trace equation. To remedy this, we gradually linearize the solution. This is done by increasing the
set of resources of particular constants: when we compressed each S-run to a single constant, we lift all
S-constants, so that all S-constants and S-variables are eliminated from the equation. To make the whole
approach work, we deﬁne an order ≤ on sets of resources: it is any linear order that extends the partial
ordering by the size, i.e., |S| ≤ |T | implies S ≤ T . A set of resources S is called minimal (for a solution
σ), if it is minimal according to ≤ in the set {T | there is a T -constant in σ(W )}. We process the sets of
resources according to ≤, each time treating a minimal sets of resources.

4.2.1. Fixed resources

We deﬁne the actions of the algorithm eliminating the S-constants for a ﬁxed minimal set of resources S.
To this end, we need some notions of “easy” and “diﬃcult” factors of σ(W ).

Deﬁnition 3. Let (W, B, X , ρ, µ) be a state and σ its B-solution. A factor v of σ(W ) is visible if for
some occurrence at least one of its positions is obtained form a position labeled by a constant in W ; a
factor is invisible if it is not visible. A trace v is crossing if for some occurrence of v in σ(W ) some but
not all positions belong to the substitution of a variable X by σ(X); and this occurrence is visibly crossing.
A trace is noncrossing if it is not crossing.

The factors that we typically consider are pairs, i.e., ab where a 6= b 6= a, a-blocks, i.e., a maximal factor
of the form aℓ (this occurrence of aℓ if not part of a factor aℓ+1), and a-quasi-blocks, i.e., (aa)ℓ that is
not part of a factor (aa)ℓ+1. In the latter case, aa is called a quasi-letter. The intuitive meaning of a
quasi-letter is that we cannot compress aa into a single constant as it is would be self-involuting, hence
we treat those two letters as if they were a single constant.

6

Given a subalphabet S±, we consider an involuting partition (S+, S−) that satisﬁes the conditions
S+ = S−, S+ ∩ S− = ∅ and S+ ∪ S− = S±. Such a partition is crossing if at least one pair ab ∈ S+S−
is; it has crossing quasi-blocks if there is a ∈ S+ that has crossing quasi-blocks. Lastly, S± has crossing
blocks if there is a ∈ S± that has crossing blocks.

Pair compression. Pair compression is implemented essentially in the same way as in the case of word
equations. Given a pair ab with a 6= b 6= a we want to replace each factor ab in σ(W ) with a fresh constant
c. This is easy, when ab is noncrossing:
it is enough to perform this operation on W and each σ(X),
the latter is done implicitly and we obtain a diﬀerent solution σ′ in this way. We also set ρ and µ for
c appropriately: ρ(c) = ρ(ab) and µ(c) = µ(ab). Performing several such compressions is possible for
ab ∈ S+S−, where (S+, S−) is a noncrossing involuting partition, as for each constant in σ(W ) we can
uniquely determine to which replaced pair it belongs (if any). We do not compress pairs aa, though, as
this would create a self-involuting letter.

We need to ensure that indeed (S+, S−) is noncrossing. A pair ab ∈ S+S− is crossing if aX is a factor
of W and b ∈ min(σ(X)). The other option is that Xb is a factor and a ∈ max(σ(X)); it is taken care
by considering b X and the pair ba. Then we replace X with bX. After doing this, for all variables, the
partition (S+, S−) is noncrossing and so we can compress pairs in this partition.

Block compression. Pair compression cannot be applied to aa, as it makes the compression of longer
blocks ambiguous. However, when a has no crossing block, (in several steps) we replace each a-block aλ
by cλ. Similarly as in the case of pair compression, we can compress blocks of several letters in parallel,
as blocks of diﬀerent letters do not overlap.

Again, to apply this subprocedure we need to ensure that each a ∈ S± has no crossing blocks. Given a
visibly crossing block aℓ, popping one node may be not enough as this block may still be crossing. Thus
for each variable X we pop its whole a-preﬁx whenever a ∈ min(X) ∩ S, where aℓ is the a-preﬁx of a trace
V when ℓ is maximal with V = aℓV ′.

Quasi-block compression. We do not apply the pair compression to aa as this introduces self-involuting
letters. Instead, we perform a variant of block compression on them: the quasi-block compression. We
replace each a-quasi-block (aa)λ with cλcλ; note that we treat a and a asymmetrically. We again perform
this operation in parallel (in several steps), for all a ∈ S+, where (S+, S−) is an involuting partition.

For uncrossing of quasi-blocks we act the same as for uncrossing of blocks, but we pop the whole
(aa)-preﬁx when a ∈ S+; the aa preﬁx of V is the longest factor V ′ ∈ a(aa)∗ ∪ (aa)∗ such that V = V ′V ′′.

Algorithm for a ﬁxed resource set. Using those operations we can process a minimal set of resources S:
We iterate the following operations as long as something changes in the equation. For each variable we
guess whether it has a minimal S-letter and if so we pop this letter. Then we compute the set S± of visible
S-constants. We uncross blocks from S± and then compress blocks of S±. We then arbitrarily partition
S± into an involuting partition (S+, S−). Then we uncross quasi-blocks for S+ and then compress quasi-
blocks from S+. We again partition S± into an involuting partition (S+, S−); the partition is chosen so
that there are many occurrences of pairs in S+S− in the equation, see the appendix. Finally, we uncross
(S+, S−) for pair compression and perform the pair compression for (S+, S−).

Using similar arguments as in the case of word equations, one can show that FixedResources(S) uses
linear space. Concerning the S-runs after FixedResources(S), ideally all S-runs are of length 1 and are
either visible or invisible. This is not entirely true, as aa cannot be compressed, but those are the longest
visible S-runs that can prevail.

Lemma 4. Let S be minimal. The length of the equation during FixedResources(S) is linear. After
FixedResources(S) there are no crossing S-runs, no S-variables. Furthermore, visible S-runs have length
at most 2.

4.2.2. Lifting arcs

Compression of S-runs alone is not enough, as there are runs for diﬀerent sets of resources. To remedy
this we linearize the trace, for technical reasons it is easier to lift whole Hasse arcs rather than individual
nodes.

To lift a Hasse arc e = (a → b) we want to relabel its ends by [a, ρ(a) ∪ ρ(b)] and [b, ρ(a) ∪ ρ(b)], i.e., by
fresh (ρ(a) ∪ ρ(b))-constants. For correctness reasons we need to also lift the edges that “correspond” to

7

e; moreover, as in the case of compression, lifting may be diﬃcult when an arc connects constants in the
equation with constants in the substitution for a variable. Those notions are formalized below.

Deﬁnition 5. Let (W, B, X , ρ, µ) be a state and σ its B-solution. A Hasse-arc a → b in σ(W ) is visible
( invisible, visibly crossing) if the corresponding factor ab in σ(W ) has this property. Let ∼ be the smallest
equivalence relation which satisﬁes the following conditions:

• If e = (a → b) in σ(W ) and f = (b → a) is the corresponding arc in σ(W ), then e ∼ f .

• If e is invisible and inside some σ(X) where X ∈ X and f is a corresponding arc in some diﬀerent

σ(X), then e ∼ f .

We say that e is crossing if there is exists a visibly crossing f with f ∼ e; e is free otherwise.

Note that for arcs the notion of crossing/free is ﬁner than for traces: since it is possible that e 6∼ e′

while both are of the form (a → b), in particular e could be free and e′ crossing.

When e = (a → b) is a free unbalanced arc, the promised linearization of traces is done through lifting:
let S = ρ(a)∪ρ(b), then for f ∼ e we change the label on each of its ends from c ∈ {a, b, a, b} to [c, S]. Note
that this balances f . To make this operation well deﬁned, we partially linearizes a trace: each position
that was before (after) any of relabeled a, b is now before (after) both of [a, S], [b, S] (the same is done for
arc b → a).

We can lift free arcs “for free”, but some S-arcs may be crossing. Freeing them is similar to uncrossing
factors, but we need to take into the account that ρ(a) 6= ρ(b). Thus ab could be a crossing arc in aX
and b is not a minimal element of σ(X), so it cannot be popped. Freeing is done in two stages: ﬁrst we
deal with the case when b is an S-letter. Then for σ(X) = P bQ, such that S 6= ρ(P )   ρ(X) we pop the
whole P , which is done by introducing a fresh variable, i.e., we substitute X 7→ X ′bX. The new solution
is σ′(X ′) = P and σ′(X) = Q. Then we deal with the case when a is an S-letter (and b not). Thus for
σ(X) = P bQ, where ρ(a) ∩ ρ(P ) = ∅, we substitute X 7→ X ′bX. The new solution is σ′(X ′) = P and
σ′(X) = Q. Those operations are called splitting of variables. Observe that the ﬁrst splitting can be done
for any set of S-constants and all variables in parallel, while the second can be performed in parallel for
all variables and any set of constants that is a subset of {b | ρ(b) ∩ S 6= ∅}.

We want to lift all unbalanced S arcs, but this is not possible for all such arcs in parallel due to
involution: for an S-letter a and a trace bac we have to choose which arc, b → a or a → c, we lift. But
it can be done in stages:
let (S+, S−) and (T+, T−) be involuting partitions of all S-constants and all
constants having a common resource with S, i.e., {a | ρ(a) ∩ S 6= ∅}. Then we process all S arcs in four
groups S+T+, S−T+, S+T− and S−T−; processing of each one is similar, we describe processing of one —
S+T+. We ﬁrst split the variables for S+ and then for T+, as described above. Then each arc (a → b)
with ab ∈ S+T+ is free, thus we lift those arcs. We continue with groups S−T+, S+T− and S−T−. Note
that the processing may introduce new crossing arcs, but it can be shown that they are always in next
groups. Afterwards, there are no S arcs.

It is easy to show that after Remove(S) all S-constants and S-variables are eliminated.

Lemma 6. After Remove(S) there are no S-constants nor S-variables in σ(X).

4.2.3. The algorithm

TrEqSat considers possible sets of resources S in order ≤ on them. For a ﬁxed S it ﬁrst runs FixedResources(S)
and then Remove(S).

4.3. Analysis

We begin with estimating the space usage. Firstly we upper-bound the number of introduced variables:
they are introduced only during splitting of variables, which happens O(1) times per resource set, and each
variable introduces O(1) variables, which have less resources; this yields that the number of occurrences
of variables is linear in the size of the input equation.

We then estimate the length of the equation, which is also linear in the size of the input equation: Here
the estimations are similar as in the case of word equations. For a ﬁxed resource set S we claim that

8

the number of S-constants in the equation stays linear and that processing S introduces in total O(1)
constants per variable. Together with the estimation on the number of variables this yields a bound on
the size of the equation.

This guarantees that our algorithm does not exceed a space limit, but may loop forever. Thus we want
to show that solutions in consecutive steps get “smaller”. Unfortunately, the length of σ(W ) is not good
enough for our purposes, but we can deﬁne the weight of the solutions (for an equation) and indeed show
that our subprocedures decrease it. This guarantees termination.

We then move to the correctness of the algorithm, i.e., we show how the algorithm transforms the
solutions between diﬀerent equations obtained on the way. In a ﬁrst step we equip each solution with a
function that tells us, what solution of the input equation it represents. Then we show that if subprocedure
transforms one equation into the other, then the morphism associated with this transition transforms the
solution of the latter equation to a solution of the former, so that they they represent the same solution
of the input equation.

4.3.1. Space usage

The below estimations of space usage do not depend on the nondeterministic choices, they apply to all
executions of the algorithm.

Number of variables. Comparing to the algorithms in the free monoid case, the main diﬀerence is that
our algorithm introduces new variables to the equation. This is potentially a problem, as the whole
recompression is based on the assumption that the number of constants is not altered. However, we can
still bound the number of introduced variables.

Lemma 7. During TrEqSat there are O(n) occurrences of variables in the trace equation.

Fix a variable X for which initially T = ρ(X). Observe that ρ(X) cannot increase, though it can
decrease: resources increase by lifting arcs and we only lift free arcs, thus, each resource of the new
constant was present on one of the ends of the arc. On the other hand, popping constants as well as
splitting may decrease the resources of a variable.

We say that X directly created an occurrence of X ′ when X ′ was created in Split when it considered
X; X created X ′ when there is a sequence X = X1, X2, . . . , Xk = X ′ such that Xi directly created Xi+1.
Consider a variable X, it can be split at most eight times during lifting of crossing arcs when we consider
T ′ ⊆ R. This gives all variables that are directly created by X. Note, that each of the directly generated
variable has less resources than X: when we replace X with X ′bX, then we require that ρ(X ′)   ρ(X ′bX).
Let f (k) be the maximal number of occurrences of variables that can be created by a variable with
at most k resources. Using the above analysis we can write a recursive formula for f ; as the number of
resources is a constant, this yields the bound.

Length of the equation. We show that during TrEqSat the length of the trace equation is linear in the
size of the variables, this is similar as in the case of word equations and in fact the proof proceeds using
similar steps. First, we focus on FixedResources and its processing of a ﬁxed set of resources S. In each
application of the while loop we introduce O(1) S-constants per variable (in case of block and quasi-block
compression we may introduce long blocks but they are replaced with O(1) constants afterwards). On the
other hand, using standard expected value argument we can show that compression of a randomly chosen
partition results in removal of a constant fraction of S-constants from the equation.

Comparing the number of constants in the equation before and after processing S, it increases only by
the S-factors that were popped from variables. There are O(1) such factors for a variable and each is of
length at most 2. Thus for a ﬁxed set of resources the size of the equation increases by O(n). Summing
over possible sets of resources (which is of constant size) yields the claim.

Lemma 8. During TrEqSat the length of the trace equation is O(n).

4.3.2. Weight of solutions

To guarantee the termination, we show that all subprocedures decrease the (appropriately deﬁned) weight
of a solution. This weight is in fact deﬁned with respect to the original solution: The B-solution σ

9

corresponds to some solution of the input equation, as letters of B correspond to some traces in the
original equation. To keep track of those traces we use an A-morphism α : M (B, ρ, θ, µ) → M (A, ρ0, µ0);
the idea is that c ∈ B represents a trace α(c) in M (A, ρ0, µ0). Conceptually, α(σ(W )) is the corresponding
solution of the input equation. We call a pair (σ, α) a solution at (W, B, X , ρ, µ), where σ is a B-solution.
Note that this morphism is a tool of analysis and proof, it is neither computed nor stored anywhere by
the algorithm.

Using the morphism we can deﬁne a weight kα, σk of a solution (α, σ) of an equation W :

kα, σk = XX∈X

|ασ(X)| .

All subprocedures performed by our algorithm do not increase the weight. In order to ensure that they
all decrease some “weight”, we take into the account also the weight of the equations and deﬁne a weight of
a solution (α, σ) at a state V as (kα, σk , kV k) which is evaluated in lexicographic order. All subprocedures
decrease such deﬁned weight. Thus, the path in NFA for a ﬁxed solution is ﬁnite and terminates in a ﬁnal
state.

4.3.3. Internal operations

So far all the described operations were performed on the equation and had some inﬂuence also on the
solutions. However, there are also operations that are needed for the proof but are performed either on
the monoid or on the solutions alone, hence they do not aﬀect the equation at all. For this reason we call
them internal. In essence, we apply them to the equation whenever this is possible.

Useless constants and variables. A constant a ∈ B \ A is useless if it does not occur in σ(W ); it is useful
otherwise; useless constants are invisible. A variable is useless if it does not occur in W . We remove from
the monoid all useless constants and variables.

Invisible constants. Due to compression we can be left with invisible but useful constants, i.e., such that
they occur in σ(W ) but not in W .

We cannot remove such constants from B, as we deal with all solutions. However, we can replace
them with corresponding traces over M (A, ρ0, µ0). The idea is that we replaced α(c) with c too ea-
gerly. We revert this compression. We do not revert the linearization of the trace, though. Thus we
lift each letter in α(c) so that it has the same resources as c: we replace every invisible letter c with
(a1, ρ(c))(a2, ρ(c)) · · · (aℓ, ρ(c)), where α(c) = a1a2 · · · aℓ, i.e., with a chain of letters corresponding to the
trace compressed into c but lifted into current resources of c. Note that we use letters from A ⊆ B, so the
procedure is not applicable to letters that it just introduced.

4.3.4. Completeness

The last step to show completeness is an observation that each given subprocedure corresponds to a
composition of ﬁnitely many substitution and compression transitions: indeed, this is done by mechanical
veriﬁcation.

The completeness, formulated below, easily follows: given an equation with a solution (α, σ) we apply
the subprocedures that lead to a ﬁnal state. By observation above each subprocedure corresponds to a
short path in the NFA. The guarantee on the size of the states follows from Lemma 8. Finally, we cannot
iterate forever, as each subprocedure decreases the weight of the solution at a state.

Lemma 9. There is constant κ′′ ≥ 1 (depending on R but independent of n) such that for all states V , if
kV k ≤ κ′′ · n and V has a solution (α, σ), then there exists a path to ﬁnal state labeled with h1, h2, . . . , hk
such that

σ = h1h2 · · · hk.

10

References

[1] P. R. Asveld. Controlled iteration grammars and full hyper-AFL’s. Information and Control, 34(3):248

– 269, 1977.

[2] J. Berstel and D. Perrin. The origins of combinatorics on words. Eur. J. Comb., 28:996–1022, 2007.

[3] M. Casals and I. Kazachkov. On systems of equations over partially commutative groups. Memoirs

Amer. Math. Soc., 212:1–153, 2011.

[4] L. Ciobanu, V. Diekert, and M. Elder. Solution sets for equations over free groups are EDT0L
languages.
In M. Halld´orsson, K. Iwama, N. Kobayashi, and B. Speckmann, editors, Proc. 42nd
International Colloquium Automata, Languages and Programming (ICALP 2015), Part II, Kyoto,
Japan, July 6-10, 2015, volume 9135 of Lecture Notes in Computer Science, pages 134–145. Springer,
2015.

[5] V. Diekert. More than 1700 years of word equations. In A. Maletti, editor, Algebraic Informatics - 6th
International Conference, CAI 2015, Stuttgart, Germany, September 1-4, 2015. Proceedings, volume
9270 of Lecture Notes in Computer Science, pages 22–28. Springer, 2015.

[6] V. Diekert, C. Guti´errez, and Ch. Hagenah. The existential theory of equations with rational con-
straints in free groups is PSPACE-complete. Information and Computation, 202:105–140, 2005. Con-
ference version in STACS 2001, LNCS 2010, 170–182, 2004.

[7] V. Diekert and M. Lohrey. Word equations over graph products. International Journal of Algebra

and Computation, 18:493–533, 2008. Conference version in FSTTCS 2003.

[8] V. Diekert and A. Muscholl. Solvability of equations in free partially commutative groups is decidable.
International Journal of Algebra and Computation, 16:1047–1070, 2006. Journal version of ICALP
2001, 543–554, LNCS 2076.

[9] J. Fert´e, N. Marin, and G. S´enizergues. Word-mappings of level 2. Theory Comput. Syst., 54:111–148,

2014.

[10] Ju. I. Hmelevski˘ı. Equations in Free Semigroups. Number 107 in Proc. Steklov Institute of Math-
ematics. American Mathematical Society, 1976. Translated from the Russian original: Trudy Mat.
Inst. Steklov. 107, 1971.

[11] S. Jain, A. Miasnikov, and F. Stephan. The complexity of verbal languages over groups. In Proceedings
of the 27th Annual IEEE Symposium on Logic in Computer Science, LICS 2012, Dubrovnik, Croatia,
June 25-28, 2012, pages 405–414. IEEE Computer Society, 2012.

[12] A. Je˙z. Recompression: a simple and powerful technique for word equations.

In N. Portier
and T. Wilke, editors, STACS, volume 20 of LIPIcs, pages 233–244, Dagstuhl, Germany,
2013. Schloss Dagstuhl–Leibniz-Zentrum f¨ur Informatik.
To appear in J. ACM with DOI
http://dx.doi.org/10.1145/2743014.

[13] O. Kharlampovich and A. Myasnikov. Elementary theory of free non-abelian groups. J. of Algebra,

302:451–552, 2006.

[14] R. C. Lyndon and M.-P. Sch¨utzenberger. The equation aM = bN cP in a free group. Michigan Math.

J., 9:289–298, 1962.

[15] G. S. Makanin. The problem of solvability of equations in a free semigroup. Math. Sbornik, 103:147–

236, 1977. English transl. in Math. USSR Sbornik 32 (1977).

[16] G. S. Makanin. Equations in a free group. Izv. Akad. Nauk SSR, Ser. Math. 46:1199–1273, 1983.

English transl. in Math. USSR Izv. 21 (1983).

11

[17] Yu. Matiyasevich. Some decision problems for traces. In S. Adian and A. Nerode, editors, Proceed-
ings of the 4th International Symposium on Logical Foundations of Computer Science (LFCS’97),
Yaroslavl, Russia, July 6–12, 1997, volume 1234 of Lecture Notes in Computer Science, pages 248–
257, Heidelberg, 1997. Springer-Verlag. Invited lecture.

[18] A. Mazurkiewicz. Concurrent program schemes and their interpretations. DAIMI Rep. PB 78, Aarhus

University, Aarhus, 1977.

[19] W. Plandowski. Satisﬁability of word equations with constants is in PSPACE. J. ACM, 51:483–496,

2004.

[20] W. Plandowski. An eﬃcient algorithm for solving word equations. In J. M. Kleinberg, editor, STOC,

pages 467–476. ACM, 2006.

[21] W. Plandowski and W. Rytter. Application of Lempel-Ziv encodings to the solution of word equa-
tions. In K. G. Larsen et al., editors, Proc. 25th International Colloquium Automata, Languages and
Programming (ICALP’98), Aalborg (Denmark), 1998, volume 1443 of Lecture Notes in Computer
Science, pages 731–742, Heidelberg, 1998. Springer-Verlag.

[22] A. A. Razborov. On Systems of Equations in Free Groups. PhD thesis, Steklov Institute of Mathe-

matics, 1987. In Russian.

[23] Z. Sela. Diophantine geometry over groups VIII: Stability. Ann. of Math., 177:787–868, 2013.

[24] D. Wise. From Riches to Raags: 3-Manifolds, Right-Angled Artin Groups, and Cubical Geometry.

American Mathematical Society, 2012.

12

Appendix

How to read the appendix. There is a major diﬀerence in the presentation between the extended abstract
and this appendix. The main text should put the reader into a position to “understand” on a high level
how the procedures lead to the assertions in Theorem 1. For such a high-level understanding the technical
details are not necessary, in fact they may obfuscate the big picture, so they are omitted. The appendix
has a diﬀerent purpose. It should put the reader in a position to verify the mathematical statements,
including the technical details. As a result, the content of the appendix is given in the inverse order to the
one in the extended abstract. Moreover, the appendix contains redundancy: we repeat some statements
and deﬁnitions from the extended abstract.

A. Reductions and simpliﬁcations

We give some general properties which streamline the argument, as they allow avoiding considerations of
several special cases later on.

A.1. The reduction from RAAGs to trace monoids

This subsection contains material expanding the Groups via monoids in Section 3.

This section reduces the case of RAAGs (=free partially commutative monoids) to the case of trace
monoids (=free partially commutative monoids). Readers interested mainly in trace monoids can skip this
section.

The reduction is fairly standard and it is also used to ensure that solutions of equations over RAAGs
are in a reduced normal form. The reduction does not create any self-involuting letters, but our statement
concerning trace monoids is more general.

In principle, we can use the same reduction as in [8], but we additionally require that the solutions are

reduced. The set of all reduced traces can be deﬁned by a ﬁnite list L of forbidden factors:

L = {aa | a ∈ A} .

For computational complexity results it is enough to observe that |L| ∈ O(|A|). (As a matter of fact, a
quadratic bound on L would not suﬃce.)

We ensure that factors from L do not occur in the solution by deﬁning an additional constraint: Consider
a ﬁnite monoid with involution NL whose elements form a subset of 2A × 2R × 2A and it has an additional
zero element “0”, for which 0 · x = x · 0 = 0 for all x ∈ NL. Intuitively, a triple (P, S, R) represents traces
V such that min(V ) = P , max(V ) = R and ρ(V ) = S and V does not have a factor from L. In particular,
NL contains only those triples, for which ρ(P ∪ R) ⊆ S and each of P, R contains pairwise independent
letters, only (recall that u, v are independent if ρ(u) ∩ ρ(v) = ∅).

The multiplication (P, S, R) and (P ′, S′, R′) is deﬁned so that it satisﬁes this intuitive property: if RP ′
and L have a common element, then the result is 0, as it corresponds to a non-reduced trace. Otherwise it
is (P ′′, S′′, R′′), where P ′′ contains elements in P and those elements in P ′ that do not share a resource with
S (note that those correspond to minimal elements of a trace), S′′ = S ∪ S′ (the resources of concatenation
is the union of the resources) and R′′ contains elements in R′ and those elements in R that do not share
a resource with S′. Formally:

(P, S, R) · (P ′, S′, R′)

=((P ∪ {a ∈ P ′ | ρ(a) ∩ S = ∅} , S ∪ S′, R′ ∪ {a ∈ R | ρ(a) ∩ S′ = ∅})

0

if RP ′ ∩ L = ∅,
otherwise.

The involution is deﬁned by 0 = 0 and (P, S, R) = (R, S, P ). There is canonical morphism µL : M (A, ρ) →
NL deﬁned by µL(a) = ({a} , ρ(a), {a}) for a ∈ A.

It is easy to see that g ∈ M (A, ρ) is in reduced normal form if and only if µ(g) 6= 0. By replacing a given
constraint monoid N by the direct product N × NL we may henceforth assume that µ(x) = µ(y) implies
µL(x) = µL(y). Moreover, we may assume that N contains a zero 0 and µ(X) 6= 0 for all X ∈ X . Note

13

that N still satisﬁes our assumption from Section 2: the description of element of N needs O(log |A|) bits
and all operations in N can be performed in space O(log |A|).

The next steps are well-known and standard, for details we refer to [8]. These steps transform (in
nondeterministic linear space) an equation (U, V ) over G(A, ρ) into a system of equations over the trace
monoid M (A, ρ) with the help of additional variables.
In order to come back to a single equation we
introduce a marker symbol #. It is convenient and non-restrictive to assume that

# = #, µ(#) = 0, and ρ(#) = R.

Thus, in nondeterministic linear space a system of equations {Vi}k
i=1 with constraints over a RAAG G(A, ρ)
can be transformed into a single equation V ′ over M (A, ρ). Due to nondeterminism several outcomes (i.e.,
equations V ′ ) are possible. But the reduction never transforms an unsolvable equation into a solvable
one and every solution of V ′ has a corresponding solution of at least one possible outcome V ′, see [8]. As
a conclusion and for a later reference we state the following lemma.

Lemma 10. The proof of Theorem 1 follows from its monoid version.

A.2. Resources

This subsection contains material expanding the Resource monoids and groups in Section 2.

By increasing the set of resources R by one element, we can assume that

ρ(X) 6= R = ρ(#) for every variable X,

where # is a special symbol in the alphabet.

This has the advantage that in a solution σ(X) cannot use any letter a with ρ(a) = R since a solution

must satisfy ρσ(X) ⊆ ρ(X). In particular, # cannot occur in σ(X).

Moreover, we assume that every constant a uses some resource. If this is not the case, then letters from
D = {a | ρ(a) = ∅} commute with everything, and it is easy to show that our theorem holds in such a
setting as well: essentially we can move all those letters to the left, popping them from variables if needed.
As there are no dependency between them, the equation on them reduces to linear Diophantine equations,
which can be solved in NP and also in nondeterministic linear space [12]. Moreover, all solutions of systems
of linear Diophantine can be easily represented in the desired form as an EDT0L language [5]. Since linear
Diophantine equations are not our focus, we henceforth assume (for simplicity of the presentation) that

ρ(a) 6= ∅ for all constants.

A.3. Self-involuting letters

This subsection contains material expanding the Simpliﬁcations in Section 3.

Our results hold in the context of free partially commutative monoids in the original deﬁnition of
Mazurkiewicz, as they are frequently used in computer science. However, this setting originally does not
use involution. To make the results formally applicable one can take involution as an identity. Hence all
letters are self-involuting, but the involution on a trace means to read them from right-to left. A self-
involuting trace becomes a palindrome. This opens an interesting avenue: we can solve trace equations
that use the palindrome predicate.

The reduction from a RAAGs to trace monoids did not introduce any self-involuting letter other than
#; and the convention µ(#) = 0 ensures that # does not appear in any solution obeying the constraint.
However, Theorem 1 allows a resource monoid to have self-involuting letters. (This is necessary because
we wish that the identity to be a possible involution on the alphabet A.) As in Section A.1, the technique
to cope with that problem has been described in [8]. However, in [8] the constraints are represented by
Boolean matrices, which causes a quadratic increase on space. As we aim at O(n log n) space, we need to
improve on that.

This involves a simple construction on monoids. If N is a monoid, then we deﬁne its dual monoid N T
In
for x ∈ N we

to have the same carrier set N T = N , but N T is equipped with a new multiplication x ◦ y = yx.
order to indicate whether we view an element in the monoid N or N T , we use a ﬂag:

14

write xT to indicate the same element in N T . Thus, we can suppress the symbol ◦ and we simply write
xT yT = (yx)T ; the notation is intended to mimic transposition in matrix calculus. Similarly, we write 1
instead of 1T which is true for the identity matrix as well. The direct product N × N T becomes a monoid
with involution by letting (x, yT ) = (y, xT ). Indeed,

(x1, yT

1 ) · (x2, yT

2 ) = (y2y1, (x1x2)T ) = (y2, xT

2 ) · (y1, xT

1 ) = (x2, yT

2 ) · (x1, yT

1 ).

Clearly, if N is ﬁnite then N ×N T is ﬁnite, too. The projection π1 : N ×N T → N onto the ﬁrst component
deﬁnes a homomorphism of monoids, thus:

Remark 11. Let N be any monoid (with or without involution) and let M be a monoid with involution. If
ν : M → N is a homomorphism of monoids, then there is unique morphism µ : M → N × N T of monoids
with involution such that ν = π1µ. Indeed, it is suﬃcient and necessary to deﬁne µ(x) = (ν(x), ν(x)T ).

We are now ready to eliminate the self-involuting letters from the alphabet.

Lemma 12. The proof of Theorem 1 follows from its monoid version where # is the only self-involuting
letter of A and where the constraint µ guarantees that # cannot appear in any solution σ(X) of a variable
X.

Proof. Due to Lemma 10 we can start with the monoid version with a constraint µ : M (A, ρ) → N . Deﬁne

As = {a ∈ A | # 6= a = a} .

If As = ∅, then we are done. Otherwise deﬁne two disjoint copies: A+ = {a+ | a ∈ As} and A− =
{a− | a ∈ As}. We may assume that A+ ∩ A− = ∅. We let a+ = a− and a− = a+. In this way, A± =
A+ ∪ A− becomes an alphabet without self-involuting letters. Moreover, we let ρ(a+) = ρ(a−) = ρ(a).
Now, we perform the following steps.

• We replace A by the new alphabet A′ = A± ∪ A \ As; and we embed M (A ∪ X , ρ) into M (A′ ∪ X , ρ)

by the morphism ι where ι(a) = a+a− for a ∈ As and ι(x) = x for x ∈ X ∪ A \ As.

• We replace the equation (U, V ) by the new equation (ι(U ), ι(V )). Deﬁne a homomorphism of monoids
ν : M (A′ ∪X , ρ) → N by ν(x) = µ(x) for x ∈ X ∪A\As, ν(a+) = µ(a) for a ∈ A+, and ν(a−) = 1 for
a ∈ A−. Note that ν never respects the involution, unless µ is trivial. However, we have νι(a) = µ(a)
for all a ∈ A.

• Deﬁne the constraint by a morphism µ′ : M (A′, ρ) → N × N T as in Remark 11. Thus, we have

µ′(a) = (ν(a), ν(a)T ) for a ∈ A′. We obtain

π1µ′ι(x) = µ(x) for all x ∈ M (A, ρ).

• Finally, add an additional constraint which makes sure that for a variable X and a solution σ of

(ι(U ), ι(V ), µ′) we can guarantee σ(X) ∈ ι(M (A, ρ)).

Let us comment, what we have done. The morphism ι embeds M (A ∪ X , ρ) into M (A′ ∪ X , ρ). In A′ ∪ X
there is no other self-involuting letter than #. Every solution σ : M (A ∪ X , ρ) → M (A, ρ) yields a solution
σ′ : M (A′ ∪X , ρ) → M (A′, ρ) of (ι(U ), ι(V ), µ′) such that σ′(X) ∈ ι(M (A, ρ)) for all X ∈ X (take σ′ = ισ).
It remains to show that if σ′ is a solution (ι(U ), ι(V ), µ′) such that σ′(X) ∈ ι(M (A, ρ)) for all X ∈ X
then we can ﬁnd a corresponding solution of (U, V, µ). Deﬁne a homomorphism η : M (A′, ρ) → M (A, ρ) by
η(a) = a for a ∈ A′ \ A±, η(a+) = a for a+ ∈ A+, and η(a−) = 1 for a− ∈ A+. Then ησ′ι : M (A′ ∪ X , ρ) →
M (A′, ρ) is a solution of (U, V, µ). Without loss of generality we may assume that A ∪ A′ ⊆ C and η is the
endomorphism of C∗ which ﬁxes all letters outside A− and sends A− to the empty word 1. We may also
assume that all A−endomorphisms of C∗ under consideration ﬁx A′ with the single exception of η. This
will also be the only endomorphism of C∗ which sends some letters to 1, and which does not respect the
involution. Nevertheless, if we can prove Theorem 1 in the better setting without self-involuting letters
(up to #), then we can apply in a ﬁnal step the endomorphism η to the EDT0L set of solutions and we
have the statement of Theorem 1 in the general form. One piece is still missing, we need to explain how
to realize the constraint that a solution σ′ satisﬁes σ′(X) ∈ ι(M (A, ρ)) for all X ∈ X . Fortunately, this is

15

easy and similar to the monoid construction which guarantees solution in reduced traces. We work again
with a list of forbidden factors L. We do not need the list in explicit form (this might be too long): we
content ourselves that we can list all pairs (a+, a−) for a ∈ As in linear space. Each time see an Hasse arc
a → b with a ∈ A+ or b ∈ A−, then we must have a = a+ and b = a− for some a ∈ As. Actually, this is
not enough. We must also make sure that no a− appears as ﬁrst letter and no a+ appears as last letter in
any σ(X). Again, this is standard and left to the reader.

A.4. The initial trace Winit as an equation

This subsection contains material expanding the Simpliﬁcations in Section 3.

Note that the previous reduction introduced additional variables, but the original variables Xi for
1 ≤ i ≤ k are still present in X . Intuitively, if X1, X1, . . . , Xk, Xk is the set of all variables occurring in
the word U V U V , then the set of all solutions of an equation (U, V ) with constraints µ is a set of tuples

S(U,V,µ) = {(σ(X1), . . . , σ(Xk)) | σ solves (U, V ) in M (A, ρ) and satisﬁes µ} .

In the following it is more convenient to think of a solution as a single trace. Thus, we encode the values
of the variables in the equation. Therefore, we deﬁne Winit ∈ M (A ∪ X ) as follows:

Winit = #X1# · · · Xk#U #V # U #V #Xk# · · · #X1#.

(3)

A solution of Winit is an A-morphism σ : M (A ∪ X , ρ) → M (A, ρ) such that σ(Winit) = σ(Winit) and
µσ(X) = µ(X) for all X ∈ Ω. Thus

S(U,V,µ) = {σ(X1), . . . , σ(Xk) | σ solves Winit in M (A, ρ)} .

(4)

Consequently, we can also write S(U,V,µ) = S(Winit,µ).

In order to prove Theorem 1 it is enough to show the following statement. The main diﬀerence is that we
removed self-involuting letters. Other than that, the next theorem is a stronger and more precise version
of Theorem 1.

Theorem 13. There is an NSPACE(n log n) algorithm for the following task. On input a trace Winit as in
Equation (3), constraints given by a morphism µ : M (A ∪ X , ρ) over constants A and variables X (with in-
volutions where # is the only self-involuting letter in A and the constraint µ does not allow to use # in any
solution), and a list X1, . . . , Xk of variables in X , the algorithm computes an alphabet C of size O(n) with
A ⊆ C, constants c1, . . . , ck ∈ C, and an NFA A accepting a rational set R of A-endomorphisms of C∗ such
that S = {(h(c1), . . . , h(ck) | h ∈ R} is a subset of (A∗)k. The set S is mapped under the canonical projec-
tion πA : A∗ → M (A, ρ) onto the set S(Winit,µ) = {(σ(X1), . . . , σ(Xk)) | σ solves (Winit, µ) in M (A, ρ)}.
Thus:

{(πAh(c1), . . . , πAh(ck) | h ∈ R} = {(σ(X1), . . . , σ(Xk)) | σ solves (Winit, µ) in M (A, ρ)} .

(5)

The solution set is an eﬀectively computable EDT0L language. Furthermore, the NFA A accepts a
nonempty set if and only if (U, V, µ) has some solution; A has a directed cycle if and only if R is in-
ﬁnite if and only if there are inﬁnitely many solutions. These conditions can be tested NSPACE(n log n).

B. Framework (monoids)

This subsection contains material expanding Sections 2 and 3.

Let us repeat and introduce some technical notation which will be used throughout the proof. First of
all, in order to use the notation A, X , ρ, µ in a more ﬂexible way, we replace the statement Theorem 13 by
using Ainit, Xinit, ρinit, µinit. Thus, the initial situation is given by the tuple

(Winit, Ainit, Xinit, ρinit, µinit).

Replace Ainit by some larger alphabet A. This is additional material to Increasing the resources in
Section 3. As we increase the set of resources of constants, it is better to use a larger alphabet from the

16

very beginning which allows to increase the set of resources without creating a fresh letter. Remember
that we have # ∈ Ainit and no other symbol in Ainit ∪ Xinit is self-involuting. We let

A =(cid:8)(a, S) ∈ Ainit × 2R(cid:12)(cid:12) ρinit(a) ⊆ S(cid:9) .

This is a resource alphabet with involution by letting ρ0(a, S) = S and (a, S) = (a, S). In the following
we will use A as the basic alphabet and we ﬁx this notation. We use the convention ((a, S), T ) = (a, T )
for ρ(a) ⊆ S ⊆ T .

We have a canonical embedding M (Ainit, ρinit) ⊆ M (A, ρ0) by a 7→ (a, ρinit(a)). Moreover, the free
∗ with involution embeds into M (A, ρ0) by a 7→ (a, R). We use both embedding throughout.
monoid Ainit
For a ∈ A we write a = (a, ρ(a)). Note that # = (#, R) is the only self-involuting letter in A. We
deﬁne µ0(a, S) = µ(a) for a ∈ A; and we adopt the notation for variables X ∈ Xinit: we let X0 = Xinit,
µ0(X) = µinit(X), and ρ0(X) = ρinit(X). The projection on the ﬁrst component induces a canonical
length-preserving projection:

π0 : M (A, ρ0) → M (Ainit, ρinit),

π0(a, S) = a for a ∈ Ainit.

(6)

Let Vinit = (Winit, Ainit, Xinit, µinit, ρinit) be any equation with constraints which is the input to The-
orem 13. Then we can switch to V0 = (Winit, A, X0, µ0, ρ0) over the larger alphabet of constants A. If
σ : M (Ainit ∪ X0, ρinit) → M (Ainit, ρinit) is a solution of Vinit, then σ is also a solution of V0. Vice versa, if
σ′ : M (A ∪ X0, ρ0) → M (A, ρ0) is a solution of V0, then σ = πAσ′ is a solution of Vinit. Moreover, Vinit has
inﬁnitely many solutions if and only if V0 has inﬁnitely many solutions: in both cases there are inﬁnitely
many solutions if and only if there exist arbitrary long solutions. So, we can start right away with the
state V0 = (Winit, A, X0, ρ0, µ0).

The index 0 is used because we wish to use the notation X , µ, ρ freely in intermediate steps of the

procedure. We deﬁne the input size of Vinit and V0 by the same number:

n = |Winit| + |A| + |X0| .

(7)

The notation for the alphabets Ainit, A, and the projection π0 according to (6) and the corresponding
input size n in Equation (7) remain ﬁxed. Let us summarize the eﬀect of replacing Ainit by the larger
alphabet A.

Lemma 14. The change of Vinit to V0 allows to prove Theorem 13 in a more speciﬁc setting. Instead of
using the morphism πA : A∗ → M (A, ρ0) we can use the composition

π0 : A∗ πA−→ M (A, ρ0)

π1−→ M (Ainit, ρinit).

Moreover, instead of showing Equation (5) it is enough to show the weaker assertion.

{(π0h(c1), . . . , π0h(ck) | h ∈ R} = {(π0σ(X1), . . . , π0σ(Xk)) | σ solves (Winit, µ) in M (A, ρ)} .

(8)

In other words, it is enough to show that the rational set of endomorphisms R = L(A) ⊆ A∗ is mapped
under the canonical projection π0 onto π0(S(Winit, µ0)).

Proof. This is clear from the construction above: indeed if σ solves (Winit, µ) in M (A, ρ), then replacing
σ(X) by π0σ(X) deﬁnes another solution since π0(Winit) = Winit.

We use (6) in order to focus on Equation (8). This will become important when, later, we introduce a

notion of “forward property” in Deﬁnition 25.

Choosing the frame. This part extends the material presented in Groups via monoids in Section 3.

We let κ ∈ N be a large enough constant (which actually depends exponentially on R). A suitable upper
bound on κ can be calculated easily from the exposition and the explicit calculation is omitted. Next, we
deﬁne an alphabet of constants C and a disjoint alphabet of variables Ω such that A ⊆ C, X0 ⊆ Ω, and
|C| = |Ω| = κ · n ∈ O(n). We assume that C and Ω have involutions and we let Σ = C ∪ Ω. We assume
that # ∈ A is the only self-involuting symbol in Σ. This ﬁxes a frame Σ of linear size. Henceforth, all
constants and variables are in Σ

17

We reserve the notation B and X to denote sets of constants and variables such that A ⊆ B ⊆ C and
X ⊆ Ω. We assume that B and X are closed under involution. Moreover, µ and ρ refer to morphisms
µ : B ∪ X → N and ρ : B ∪ X → 2R. This applies to “primed symbols” B′ and X ′, too. If B and X are
given, then a fresh constant /variable refers to an element x ∈ Σ \ (B ∪ X ). Another convention: Small
letters a, b, c, a′, b′, c′, . . . refer to constants, capital letters X, Y, X ′, Y ′, . . . refer to variables.

If a function f is deﬁned on a domain ∆′, then we typically use the same symbol f to denote its

restriction to a subset ∆ of ∆′.

Endomorphisms. An endomorphism of a free monoid ∆∗ with involution is given by mapping h : S → ∆∗
where S ⊆ ∆. The convention is that h is extended to h : ∆∗ → ∆∗ by letting h(c) = c if c, c /∈ S and
h(c) = h(c) for c ∈ S \ S. Thus we typically assume S ∩ S = ∅, which means that the deﬁnition of h
respects the involution by the convention above. If S = ∅, then h = id∆∗ is the identity. If we have S ⊆ ∆′
and h(S) ⊆ ∆′′∗, then we also view h as a homomorphism h : ∆′∗ → ∆′′∗.

B.1. Σ-homogeneous monoids, resource monoids with types, structured monoids

This Section extends the material presented in Monoids and equations in Section 3.

While we are interested in word equations over trace monoids, in its intermediate steps our algorithm

encounters also word equations in more general structures, which are described in this Section.

B.1.1. Σ-homogeneous monoids

In algebra, the notion of homogeneous monoid refers to a ﬁnitely presented monoids where the two words
in each deﬁning relation have the same length, for instance, resource monoids are of this type. In our
paper, a monoid M is called Σ-homogeneous if it is a monoid with involution, generated by a subset
∆ ⊆ Σ and a ﬁnite set of deﬁning relations S such that (ℓ, r) ∈ S implies |ℓ|x = |r|x for letters x ∈ Σ.
For a Σ-homogeneous monoid M we can write M = ∆∗/S where ∆ = ∆ and that (ℓ, r) ∈ S implies both,
|ℓ| = |r| and (ℓ, r) ∈ S.

Whenever ∆′ ⊆ ∆ and S′ = S ∩ ∆′∗ × ∆′∗, then ∆′∗/S′ naturally embeds into ∆∗/S.
During our algorithm we are often tasked with a question, whether a trace V is a factor of a trace W .
In general, we need to cope with such question also in the more general framework of Σ-homogeneous
monoids.

Formally, the uniform factor problems in Σ-homogeneous monoids is given as:

• Input. The alphabet Σ, a pair (∆, S) describing a Σ-homogeneous monoid M = ∆∗/S, and a pair

of words U, V ∈ ∆∗ such that

|Σ| + |U V | + X(ℓ,r)∈S

|ℓr| ∈ O(n).

• Question. Are there p, q ∈ ∆∗ such that pU q = V holds in M ?

Lemma 15. The uniform factor problem in Σ-homogeneous monoids is decidable in NSPACE(n log n).

Proof. The encoding of U and V uses at most O(n log n) bits. Scanning V from left to right and guessing for
each letter of V whether it belongs to p or to U or to q, we compute nondeterministically the words p and q;
and we write W = pU q to the work space with O(n log n) bits. Finally, we modify W nondeterministically
by applying deﬁning relations ℓ = r where (ℓ, r) ∈ S. Each time before applying a relation we may guess
to stop this process. We accept U as a factor of V if the current W coincides with V as a word in ∆∗.

Resource monoids over Σ with types We consider a subclass of Σ-homogeneous monoids, which extend
the framework of partial commutation by using types. Let us ﬁx the notation: (Γ, ρ) is a resource alphabet
and Γ = Γ = B ∪ X .

The idea of a type comes from a diﬀerent approach to block compression. Instead replacing the whole
aλ with a fresh constant cλ in one go we make the compression in stages. We replace a block aλ, which
we intend to compress, ﬁrst by factor cλcλ−1 where cλ and c are fresh letters; this is just a renaming
at certain positions labeled by a. (The notation cλ is purely formal, we do not write down the natural

18

number λ which can be arbitrary large.) Then cλ should commute with c, as it indeed represents a single
c, even though cλ and c have the same resources. Moreover, a variable X may also commute with c, when
in the solution it represents a block of c. We want to formalise this by giving c, cλ and X the same type
θ(c) = θ(cλ) = θ(X) = c and allow a commutation xy = yx when θ(x) = y.

A similar approach is applied also to quasi-block compression, this time though the type represents

quasi-blocks of aa.

The idea of types is formalised as follows: Formally, a type θ for (Γ, ρ) is a relation θ ⊆ (B ∪ B2 ∪ X ) ×

(B ∪ B2) such that following conditions are satisﬁed.

• (x, y) ∈ θ implies (x, y) ∈ θ (intuitively: when x is a block of y then x should be a block of y)

• (x, y) ∈ θ and (x, y′) ∈ θ implies y = y′ (intuitively: x can represent a block of only one letter).

• (u, y) ∈ θ and u ∈ B+ implies |u| = |y| and u ∈ {a, a, aa} for some a ∈ B \ A (intuitively: types
are deﬁned only for x = c or x = cc or x being a variable, in two ﬁrst cases the type is of the same
length as its argument).

Note that we can read θ as a partially deﬁned function where the domain is a subset of variables,
letters, and quasi-letters. If θ(x) is deﬁned, then θ(x) is also called the type of x. Moreover, there is a
letter c ∈ B \ A such that either {θ(x), θ(x)} = {c, c} or θ(x) = θ(x) = cc.

Each tuple (Γ, ρ, θ) deﬁnes a Σ-homogeneous monoid with involution M (Γ, ρ, θ) as follows:

M (Γ, ρ, θ) = Γ∗/ {xy = yx | (x, y) ∈ θ ∨ ρ(x) ∩ ρ(y) = ∅} .

(9)

Since the range of θ does not involve any symbol from A ∪ X we obtain a canonical embedding of the
resource monoid M (A ∪ X , ρ) into M (Γ, ρ, θ). The monoid M (Γ, ρ, θ) is called a resource monoid with a
type.

B.1.2. Structured monoids and their morphisms

We now give the deﬁnition of monoids used in the automaton from Theorem 13; this extends the material
presented in Equations during the algorithm in Section 3.

Deﬁnition 16 (Structured monoid). A resource monoid M (B ∪ X , ρ, θ) with type θ and constraints
µ : M (B ∪ X , ρ, θ) to the ﬁnite monoid N is a structured monoid, if

• A ⊆ B = B ⊆ C and X = X ⊆ Ω.

• As a resource monoid with type we have M (B, X , ρ, θ, µ) = M (∆, ρ, θ) for ∆ = B ∪ X .

• ρ : ∆ → 2R with ρ(a) = ρ0(a) for a ∈ A.

• µ : ∆ → N is a morphism with µ(a) = µ0(a) for a ∈ A.

• The morphism µ induces a morphism of monoids with involution µ : M (∆, ρ, θ) → N.

We denote this structured monoid by M (B, X , ρ, θ, µ)

We use M (B, ρ, θ, µ) as an abbreviation for the submonoid of M (B, X , ρ, θ, µ) which is generated by
the constants from B. The inclusion M (B, ρ, θ, µ) ⊆ M (B, X , ρ, θ, µ) is a morphism. Note that the earlier
notation M (B, X , ρ, µ) without types is now replaced by M (B, X , ρ, ∅, µ). Thus, we have shorthands:

M (B, ρ, µ) = M (B, ∅, ρ, µ) = M (B, ∅, ρ, ∅, µ),
M (B, X , ρ, µ) = M (B, X , ρ, ∅, µ).

Deﬁnition 17 (Morphism of structured monoids). A morphism between structured monoids M (B, X , ρ, θ, µ)
and M (B′, X ′, ρ′, θ′, µ′) is an A-morphism ϕ between M (B ∪ X , ρ, θ) and M (B′ ∪ X ′, ρ′, θ′) such that

∀x : ρ′ϕ(x) ⊆ ρ(x)
µ = µ′ϕ.

19

(10a)

(10b)

A morphism ϕ always respects the constraints but it may “forget” resources, thus, a morphism can

introduce additional partial commutation.

The norm kϕk of a morphism ϕ is

We are interested only in morphisms ϕ where kϕk ∈ O(n) because such a ϕ can be speciﬁed by O(n log n)
bits.

kϕk = Xx∈B∪X

|ϕ(x)| .

(11)

C. Extended equations

As explained, in Section A.4, it is enough to consider a single word W as an equation and in this setting σ
is a solution of W with constraints µ if and only if σ(W ) = σ(W ) and µ0σ = µ0. So elements in structured
monoids M (B, X , ρ, θ, µ) represent equations. We use the shorthand M to denote the initial structured
monoid without variables and types:

M = M (A, ∅, ρ0, ∅, µ0).

We now deﬁne the elements of the structured monoids that represent equations.

Deﬁnition 18. An element W ∈ M (B, X , ρ, θ, µ) is called well-formed if the following conditions hold.

• |W | < |C| and |W |# = |Winit|#. Moreover, # appears as the ﬁrst and last symbol in W .

• Every factor x ≤ W and every x ∈ B ∪ X satisﬁes: µ(x) 6= 0 ⇐⇒ |x|# = 0.

• If x ≤ W is a factor with |x|# = 0, then x ≤ W , too.

An extended equation is a tuple (W, B, X , ρ, θ, µ), where W is well-formed and M (B, X , ρ, θ, µ) is a
structured monoid.

The intuition behind the conditions is as follows: the ﬁrst ensures that the equation is short enough and
that it can be separated into parts that encode the substitutions for variables and equations. The second
ensures that no # was compressed into the letters in B. The third holds initially and guarantees that we
can consider a factor together with its involution at the same time.

For example, (Winit, A, X0, ρ0, ∅, µ0) is an extended equation.
Let us deﬁne the notion of solution to an extended equation. We deﬁne the notions of B-solution and

a reﬁned notion of solution.

Deﬁnition 19 (cf. 4.3.2). Let V = (W, B, X , ρ, θ, µ) be an extended equation.

• A B-solution at V is a B-morphism σ : M (B, X , ρ, θ, µ) → M (B, ρ, θ, µ) such that σ(W ) = σ(W )

and σ(X) ∈ y∗ whenever (X, y) ∈ θ.

• A solution at V is is a pair (α, σ) where α : M (B, ρ, θ, µ) → M is an A-morphism and σ is a

B-solution.

Due to the notion of morphisms between structured monoids we have µσ = µ and µ0α = µ for all

solutions (α, σ). Moreover, α and σ leave letters from A invariant.

Recall that if σ is a B-solution at V , then we can write W = x1 · · · xℓ and σ(W ) = a1 · · · am where
xi ∈ B ∪ X and aj ∈ B. Each position i where xi ∈ B can be uniquely identiﬁed with a corresponding
position j ∈ {1, . . . , m} with xi = aj. These positions j and the corresponding letter aj are called visible
in W .

D. From the nondeterministic “on-the-ﬂy” construction to an NFA

This Section extends the material in Section 4.

In the Section 4.2 of the extended abstract we described a nondeterministic algorithm (running in space
O(n log n)) that given an equation constructs “on-the-ﬂy” a path in the NFA recognizing the set of all
solutions; each transition of this NFA is labeled with an endomorphism of C.

In this Section we will give a precise deﬁnition of this NFA and show two its main properties:

20

Soundness: Given any path from the initial state to a ﬁnal state the composition of the endomorphisms

of the transitions yields a solution of the initial equation

Completeness: Every solution can be obtained in the way described above (for some path from the initial

state to a ﬁnal state).

We ﬁrst construct an NFA, see Section D.1, in particular we deﬁne all technician conditions that should
be satisﬁed by its transitions and labels of those transitions. Then we argue that every A-morphism
recognised by it is a solution of the original equation (soundness), see Section D.2. Then we show that
indeed every solution is recognised by this NFA (completeness), see Section E. Finally, the constructed
NFA does not satisfy all the conditions required by Theorem 13: it may have directed cycles even though
it has only ﬁnitely many solutions. This is due to the fact that some of the morphisms labeling the edges
do not aﬀect the solution but rather only permute the letters. We identify such labels and modify the NFA
so that they are not used, essentially we compute a transitive closure and then delete those problematic
edges, see Section D.3.2.

D.1. The ambient NFA F

In this section we deﬁne an NFA F of singly exponential size in n which accepts some rational set of
A-endomorphisms of End(C∗).

D.1.1. States

The states of F are extended equations (W, B, X , ρ, θ, µ) according to Deﬁnition 18. Observe that there
are no more than 2O(n) states in F .

The initial state is (Winit, A, X0, ρ0, ∅, µ0). Final states are without any variables and they are deﬁned

w.r.t. some distinguished letters c1, . . . , ck ∈ C. Formally, a state (W, B, ∅, ρ, ∅, µ) is ﬁnal if

1. W = W ,

2. The element W ∈ M (B, ρ, ∅, µ) has a preﬁx of the form #c1# · · · #ck#, where c1, . . . , ck are distin-

guished letters.

D.1.2. Transitions

This extend the material in Transitions in Section 4.1. Transitions between states in F are labeled
transitions where the label is an A-endomorphism of C∗. The initial state does not have any incoming
transition and ﬁnal states do not have outgoing transitions.

There are two types of transitions: those which transform variables (substitution transitions) and those

which transform constants (compression transitions).

D.1.3. Substitution transitions

This extends the deﬁnition of the substitution transitions as given in (1).

Substitution transitions transform the variables and do not aﬀect the constants. For an equation

(W, B, X , ρ, θ, µ) such a transition is deﬁned by a B-morphism τ

τ : (W, B, X , ρ, θ, µ) → (W ′, B′, X ′, ρ′, θ′, µ′).

Intuitively, τ (X) is the substitution for X. Thus τ should satisfy the following properties

• W ′ is obtained by substituting variables;

• τ is non-trivial, i.e., it substitutes something;

• the alphabet is not modiﬁed;

• the resources of each letter are the same and the resources of X before the substitution and τ (X)

after the substitution are the same;

21

• constraints are preserved;

• types of letters are preserved;

• the norm of τ is linear.

Those are formalised in the following conditions:

W ′ = τ (W ) 6= W, B = B′, ρ′ = ρτ, µ′ = µτ, ∀a ∈ B : θ′(a) = θ(a), kτ k = O(n).

(12)

Under these restrictions, a substitution transition

(W, B, X , ρ, θ, µ)

ε−→ (W ′, B, X ′, ρ′, θ′, µ′)

is deﬁned. As the label for that transition we identity morphism idC ∗, denoted by ε.
It induces the
identity from M (B, ρ′, θ′, µ′) to M (B, ρ, θ, µ), it is its associated morphism. Notice that the direction of
the morphism ε opposite to the direction of the transition and the direction of the morphism τ .

We now show that when the solution is deﬁned at a target equation, it can be transformed using the

associated homomorphism also to a solution of the source equation.

ε−→ (W ′, B, X ′, ρ′, θ′, µ′) = V ′ be a substitution transition. Let
Lemma 20. Let V = (W, B, X , ρ, θ, µ)
α : M (B, ρ, θ, µ) → M be an A-morphism at state V and let σ′ be a B-solution to V ′. Deﬁne a B-morphism
σ : M (B, X , ρ, θ, µ) → M (B, ρ, θ, µ) by σ(X) = σ′τ (X). Then (α, σ) is a solution at V and (α, σ′) is a
solution at V ′. Moreover, ασ(W ) = αεσ′(W ′) where ε is the identity morphism from M (B, ρ′, θ′, µ′) to
M (B, ρ, θ, µ).

Proof. Since W ′ = τ (W ) and σ′ is a B-solution to V ′ we have

σ(W ) = σ′(τ (W ))

= σ′(W ′)
= σ′(W ′)

= σ′(W ′)

= σ′τ (W )

= σ(W )

= σ(W )

by deﬁnition of σ

by (12)
as σ′ is a B′-solution of W ′
as σ′ is morphism of monoids with involution

by (12)

by deﬁnition of σ

as σ is a morphism of monoids with involution.

So σ is a B-solution and hence (α, σ) is a solution at W . Since M (B, ρ, θ, µ) = M (B′, ρ′, θ′, µ′) we see
that (α, σ′) is a solution at V ′. The assertion ασ(W ) = αεσ′(W ′) is trivial.

D.1.4. Compression transitions

We now generalise the properties of compression transitions, as deﬁned in (2).

These transitions transform the constants, but do not aﬀect the variables. For states (W, B, X , ρ, θ, µ)
and (W ′, B′, X ′, ρ′, θ′, µ′) such a transition is deﬁned by a A-morphism h : M (B′, ρ′, θ′, µ′) → M (B, ρ, θ, µ).
Intuitively, it should satisfy the following constraints:

We assume that the states satisfy the following conditions:

• W is obtained by applying h to W ′;

• h is non-trivial and it is not length-decreasing;

• it does not aﬀect variables: types, resources and constraints of variables are preserved.

Additionally, there are two technical conditions, all those conditions are formalised as:

W = h(W ′), B 6= B′ or ∃a ∈ Bh(a) 6= a, B ⊆ B′ or B′ ⊆ B, ∀a ∈ B′ |h(a)| ≥ 1, X = X ′,

ρ(X) = ρ′(X), θ(X) = θ′(X), µ(X) = µ′(X), ∃Xh(X) = c =⇒ h(y) ∈ y∗, khk ∈ O(n)

(13)

22

Note that condition “h(y) ∈ y∗ whenever θ′(X) = y and X ∈ X ” implies that h can be lifted to an
A ∪ X -morphism h : M (B′, X ′, ρ′, θ′, µ′) → M (B, X , ρ, θ, µ). Indeed, in M (B′, X , ρ′, θ′, µ′) the deﬁning
relations in between variables and constants have the form yX = Xy for θ′(X) = y. Due to θ(X) = θ′(X)
this implies y ∈ (B′ ∩ B)∗ and hence, h(yX) = h(y)X = Xh(y) = h(Xy) in M (B, X , ρ, θ, µ) because
h(y) ∈ y∗.

If (13) is satisﬁed, then there is a compression transition

(W, B, X , ρ, θ, µ) h−→ (W ′, B′, X , ρ′, θ′, µ′).

The direction of the morphism h is again opposite to that of the transition.

As for substitution transition, if the target equation has a solution, then the source equation has a

corresponding solution, which is essentially obtained by applying h on the target solution.

h−→ (W ′, B′, X , ρ′, θ′, µ′) = V ′ be a compression transition. Let
Lemma 21. Let V = (W, B, X , ρ, θ, µ)
α : M (B, ρ, θ, µ) → M be an A-morphism at the state V and let σ′ be a B′-solution at V ′. Deﬁne
σ : M (B, X , ρ, θ, µ) → M (B, ρ, θ, µ) by σ(X) = hσ′(X) for X ∈ X and σ(a) = a for a ∈ B.

Then σ is a well-deﬁned B-morphism, (αh, σ′) is a solution at V ′, and σh = hσ′.

In particular,

ασ(W ) = αhσ′(W ′) and (α, σ) is a solution at V .

Proof. In order to see that σ is a well-deﬁned B-morphism, it is enough to show that if xy = yx in
M (B, X , ρ, θ, µ), then σ(xy) = σ(yx) in M (B, ρ, θ, µ). This is clear for x, y ∈ B∗. Thus, we may assume
that x = X ∈ X . For y ∈ X this is also clear, because σ(Z) = hσ′(Z) for all Z ∈ X and hσ′ is a morphism.
It remains to consider x = X ∈ X and y ∈ B∗. There are two options: the commutation may be due to
resources or due to types. If ρ(X) ∩ ρ(y) = ∅, then ρ(σ(X)) = ρ(hσ′(X)) ⊆ ρ(σ′(X)) = ρ(X) and hence,
ρ(σ(X)) ∩ ρ(y) = ∅, too. Finally, if θ(X) = y, then σ′(X) ∈ y∗. Hence σ(X) = hσ′(X) ∈ y∗ (due to
h(y) ∈ y∗) and therefore

σ(Xy) = σ(X)σ(y) = σ(X)y = yσ(X) = σ(yX).

By deﬁnition, µh = µ′ and µ0α = µ. Hence (αh, σ′) is a solution at V ′. Now, h(X) = X for all X ∈ X .
Hence, σ(h(X)) = σ(X) = hσ′(X). For b′ ∈ B′ we obtain σh(b′) = h(b′) = hσ′(b′) since σ′ and σ are the
identity on B′ and B respectively. It follows σh = hσ′ and hence, ασ(W ) = αhσ′(W ′). Next,

σ(W ) = σ(h(W ′)) = h(σ′(W ′)) = h(σ′(W ′)) = σ(h(W ′)) = σ(h(W ′)) = σ(W ).

Thus, σ is a B-solution to V and, consequently, (α, σ) solves V .

D.1.5. Initial transitions

At initial states we allow only outgoing transitions, all of them are substitution transitions. To be more
precise, we allow a transition

(Winit, A, Xinit, ρinit, θinit, µinit) h−→ (W ′, A, X ′, ρ′, θ′, µ′)

only if is it deﬁned by some τ such that for each X we have τ (X) = u or τ (X) = uX where 0 ≤ |u| ≤ |R|
and X ⊆ X .

The idea is as follows. If σ is an A-solution for Vinit then we can follow an outgoing transition to put
us in some convenient situation. We can remove all X with σ(X) = 1. We can adjust ρ(X) = ρσ(X). No
crossing Hasse arc involves the special letter #.

D.1.6. Final transitions

For ﬁnal states (W, B, ∅, ρ, ∅, µ) we need additional incoming “singular” compression transitions, in general.
They are singular in the sense that the labeling h is allowed to send letters to the empty word. This
corresponds to the case that the initial solutions substitutes a variable to the empty word. More precisely,
we add transitions

(h(W ), B′, ∅, ρ, ∅, µ) h−→ (W, B, ∅, ρ, ∅, µ)

whenever (W, B, ∅, ρ′, ∅, µ) is ﬁnal, B′ = B \ {c1, c1, . . . , ck, ck}, and the transitions satisﬁes the condition
of a compression transition with the relaxation that we may have h(ci) = 1 for some ci.

23

D.2. Soundness

This extends the material in Section 4.1.1.

In this section we prove that by following the transitions in the NFA F from an initial to a ﬁnal state

and applying the corresponding endomorphisms in reverse order gives a solution to the equation Winit.

Recall that we have chosen distinguished letters c1, . . . , ck ∈ C and that if (W, B, ∅, ρ, ∅, µ) is a ﬁnal

state, then W = W and W ∈ #c1# · · · #ck#B∗.

h1−→ · · · ht−→ Vt be a path in F , where V0 = (Winit, A, X0, ρ0, ∅, µ0) is the initial
Proposition 22. Let V0
and Vt = (W, B, ∅, ρ, ∅, µ) is a ﬁnal state. Then V0 has a solution (idM, σ) with σ(Winit) = h1 · · · ht(W ).

Moreover, for 1 ≤ i ≤ k we have σ(Xi) = h1 · · · ht(ci).

Proof. For all 0 ≤ i ≤ t let αi = idMh1 · · · hi and Vi = (Wi, Bi, Xi, ρi, θi, µi). For 0 ≤ s ≤ t consider the
preﬁx V0

h1−→ · · · hs−→ Vs. If Vs has a Bs-solution σs, then deﬁne for all 0 ≤ i ≤ s:

σi = hi+1 · · · hsσs.

Then for each i the (αi, σi) is a solution at Vi. In particular, Vs has a solution (idM h1 · · · hs, σs) and V0
has some solution (idM, σ). Moreover, we claim that we have for all 0 ≤ s ≤ t:

σ(Winit) = h1 · · · hsσs(W ).

(14)

Claim (14) is trivial for s = 0. For larger s it follows by induction using Lemma 20 or Lemma 21,
depending on whether hs is a substitution transition or a compression transition; note that in order to use
the induction we need that αi is an A-morphism to M, but this holds as all hi are A-morphisms and the
image of hi is within M (Bi−1, ρi−1, θi−1, µi−1) and M (B0, ρ0, θ0, µ0) = M.

Consider s = t. We have W = W by the deﬁnition of a ﬁnal state. Since no variables occur in W ,

σt = idB∗ is the (unique) B-solution of W , so σ(Winit) = h1 · · · ht(W ).

By deﬁnition #X1# · · · #Xk# is a preﬁx of Winit and #c1# · · · #ck# is a preﬁx of W for the ﬁnal state

Vt, but h = h1 · · · ht is an A-morphism from B∗ to M with |h(c)|# = 0 for all c ∈ B. This implies

σ(#X1# · · · #Xk#) = h(#c1# · · · #ck#).

In particular, σ(Xi) = h1 · · · ht(ci) holds in the trace monoid M for 1 ≤ i ≤ k.

The soundness of the constructed NFA F follows from Proposition 22: every endomorphism recognized

by F is a solution of the input equation.

Corollary 23. Deﬁne R = L(F ) to be the accepted set of endomorphisms in End(C∗) and let π0 : A∗ →
M (Ainit, ρinit) be the canonical projection. Then we have h(ci) ∈ A∗ for all 1 ≤ i ≤ k (hence, π0h(ci) is
deﬁned) and

{(π0h(c1), . . . , π0h(ck)) ∈ C∗ × · · · × C∗ | h ∈ R} ⊆

{(π0σ(X1), . . . , π0σ(Xk)) ∈ M (Ainit, ρinit)k | σ(Winit) = σ(Winit)}.

Here, σ runs over all morphisms σ : M (A, X0, ρ0, ∅, µ0) → M of structured monoids.

D.3. The trimmed NFA

The NFA F is sound: every path from the initial to some ﬁnal state witnesses some solution σ and reading
the labels of that path yields the value for σ(X): this is what Corollary 23 says. The automaton is however
not good enough for our purposes. It has many states and transitions which lead to nowhere as they do
not belong to any accepting path. Moreover, it may be that there are cycles on accepting path although
there are ﬁnitely many solutions. These problems are resolved by removing unnecessary transitions and
by “trimming”. We are deﬁning the NFA A a subautomaton of F in Section D.3.2, Before we do so, we
need a reﬁned notion of weight.

24

D.3.1. Weights and the max-norm at states

In the construction of the NFA A we want to guarantee that for a given solution (α, σ) the path on which
we obtain this solution is of bounded (in terms of α, σ and initial equation) length. The weight of a
solution (α, σ) is deﬁned as

kα, σk = XX∈X

|ασ(X)| .

We need to take into the account also the state, as our algorithm sometimes changes the state without
modifying the solution. The weight of a state V = (W, B, X , ρ, θ, µ) is given as a tuple:

We also deﬁne the derives notion of max-norm

kV k = (|W | , ω, ω′, |W | − |θ| , |B|) ∈ N5.

kV k∞ = k(|W | , ω, ω′, |W | − |θ| , |B|)k∞ = max {|W | , ω, ω′, |W | − |θ| , |B|)} ∈ N.

We order tuples over linear orders lexicographically; thus, for example (1, 1, 1, 42, 1) < (1, 1, 2, 0, 0), but
k(1, 1, 1, 42, 1)k∞ = 42 > k(1, 1, 2, 0, 0)k∞. Recall that the lexicographic order on Nk is a well-founded
order, i.e., there are no inﬁnite descending chains.

The values ω and ω′ are deﬁned as follows.

• ω = Pa∈B(|R| − |ρ(a)|) · |W |a. Thus, the more resources are put on constants the better if the

length of W is not changed.

• ω′ = |W | − |{a ∈ B | |W |a ≥ 1}|. Thus, the more labels of constants are used in W the better, if

|W | and ω did not change.

Consequently, the weight of state V with a solution (α, σ) is

kα, σ, V k = (kα, σk , kV k) ∈ N6.

Again, the tuples are ordered lexicographically.

Finally we say that a transition V

h−→ V ′ is weight reducing if either V

h−→ V ′ is a substitution

transition or V h−→ V ′ is a compression transition with kV ′k < kV k.

The rational behind this deﬁnition is as follows: Assume that we start at a state V = (W, B, X , ρ, θ, µ)
with a solution (α, σ) and if we follow a weight-reducing transition in the NFA F , then we arrive at
V ′ = (W ′, B′, X ′, ρ′, θ′, µ′) with solution (α′, σ′).
If the transition is the substitution transition then
|ασ(X)| ≥ |ασ′(X)| and the strict inequality holds for at least one variable. Thus kα′, σ′, V ′k < kα, σ, V k.
On the other hand, if the transition is the compression transition then for each variable |ασ(X)| = |α′σ′(X)|
and so kV ′k < kV k implies kα′, σ′, V ′k < kα, σ, V k.

D.3.2. The trimmed NFA A

This section deﬁnes the NFA A which is used in Theorem 13. We deﬁne it inside F using the notion of
weight from Section D.3.1. In a ﬁrst step we keep all substitution transitions, but we keep only those
compression transitions V h−→ V ′ where kV k > kV ′k. In other words we keep only those transitions which
are weight reducing. Let us call the new (intermediate) NFA F ′. The ﬁnal step is to remove all states and
the adjacent transitions which do not belong to some accepting path.; this procedure is called trimming
and the resulting automaton A is trim. By deﬁnition it contains weight reducing transitions, only. From
now on we work in the NFA A.

Proposition 24. The trim NFA A can be constructed in NSPACE(n log n). If it is nonempty, then the
original equation is solvable. If it contains a directed cycle, then the original equation has inﬁnitely many
solutions.

Proof. We show that each step can be performed in NSPACE(n log n). In NSPACE(n log n) we can list
all candidates for transitions V −→ V ′ and decide, whether kV k , kV ′k are small enough and h is weight
reducing.

25

To perform the trimming, again we can NSPACE(n log n) list all V −→ V ′ and check that they survived
the ﬁrst step. If so, we run a standard nondeterministic graph reachability procedure to check whether V
is reachable within A from the initial state and that from V ′ there is a path in F ′ to a ﬁnal state. This
shows that A can be constructed in NSPACE(n log n).

Now, if A is a nonempty NFA, then there exists at least one accepting path as A is trim: thus there

exists a solution σ.

Finally, assume that A contains a directed cycle. Since the compression transitions reduce the weight
of the equation, this cycle has to include at least one substitution transition. Let V h−→ V ′ be transition
in A and (α′, σ′) be a solution at V ′. If this is a substitution transition then Lemma 20 shows how to
construct a solution (α, σ) at V with kα, σk > kα′, σ′k; if this is a compression transition then Lemma 21
shows how to construct a solution (α, σ) at V with kα, σk ≥ kα, σk. Thus, the weight of the solution
decreases along a cycle.

Fix a cycle in A and consider the paths P1, P2, . . . such that i-th of them goes i times through this
cycle. Consider also the solutions at the initial state constructed according to those paths. Then by the
observation above those solutions have arbitrary large weights, thus there are inﬁnitely many of them.

E. Completeness

The main task at hand is to show “completeness”: Starting with any solution σ at the initial state we can
reconstruct it by some path in F such that σ is obtained by composing endomorphisms on this path.

Ideally, we shall construct such a path step by step, which leads to the following notion.

h−→ (W ′, B′, X ′, ρ′, θ′, µ′) = V ′ be a transition in F and let
Deﬁnition 25. Let V = (W, B, X , ρ, θ, µ)
(α, σ) be a solution at V . We say that V h−→ V ′ satisﬁes the forward-property with respect to (α, σ) if
V ′ has a solution (αh, σ′) such that

π0ασ(W ) = π0αhσ′(W ′).

The mapping π0 is applied because it may be that some constants in ασ(W ) had their resources in-
creased and so the corresponding constants in αhσ(W ′) may have more resources and so be diﬀerent. But
projecting by π0 from (A) to Ainit resolves this problem.

The notion is also used for paths of transitions satisfying the forward property. As for every A-morphism

h we have

π0π0 = π0 and π0h = hπ0.

then for such a path it is enough to apply π0 once at the end.

By abuse of language: if V h−→ V ′ is a transition in F and if the solution (α, σ) at the source V is clear
from the context, then we also say that the transition V h−→ V ′ satisﬁes the forward property. Moreover,
we implicitly assume that we continue with the solution (αh, σ′) at V ′. The same convention holds for
h1−→ V1 · · · ht−→ Vt
paths. Consider a state V with a solution (α, σ) and assume that we start at V a path V
of transitions satisfying the forward property. Then we arrive at state Vt = (W ′, B′, X ′, ρ′, θ′, µ′) with a
solution (α′, σ′) such that π0αh1 · · · htσ′(W ′) = π0ασ(W ).

In particular, assume that the initial state V0 = (Winit, A, X0, ρ0, ∅, µ0) has a solution (idM, σ). Then
h1−→ V1 · · · ht−→ Vt
(idM, σ′) is another solution where σ′ is deﬁned by σ′(X) = π0σ(X). Follow a path V0
where Vt = (W, B, ∅, ρ, ∅, µ) is ﬁnal, then Vt has a solution of the form (h1 · · · ht, idB∗ ). We obtain
π0σ(Winit) = π0h1 · · · ht(W ) and h1 · · · ht ∈ L(F ), which shows the completeness of our construction.

We cannot guarantee that for each state and each solution there is a forward transition. It is enough
to show that for some class of vertices, that include the initial state, for each solution there is a forward
path. Formally, a state V = (W, B, X , ρ, θ, µ) is a standard state if there is no type: θ = ∅; it is small, if

kV k∞ ≤

|C|
100

.

26

It will be enough to consider only paths in F which start and end in small standard vertices, but in
between we need vertices which do have types and/or which are not small. The main technical result in
the paper is to show that for every solution at (Winit, A, X0, ρ0, ∅, µ0) there exists a path in F to some
ﬁnal state where all intermediate transitions satisfy the forward property.

Proposition 26. Let Vs = (Ws, Bs, Xs, ρs, ∅, µs) be a small standard state with a solution (αs, σs). Then
the NFA A contains a path from Vs to some ﬁnal state satisfying the forward property.

The rest of the paper is mostly devoted to a proof of Proposition 26.
As induction base we use the case where there are no variables: Xs = ∅. Since there are no variables we
have σ(W ) = W = W and W has a preﬁx of the form #u1# · · · #uk# where ui ∈ B∗
s . Let c1, . . . , ck ∈ C
be the distinguished letters, which are not allowed to appear in nonﬁnal vertices, and ρ(ci) = R. We let
µ(ci) = µ(ui). Deﬁne an endomorphism ht of C∗ by ht(ci) = ui and ht(c) = 1 for C \ B. Then we have
khtk ≤ |Ws| and Ws = ht(W ′) for some ﬁnal state (W, B′, ∅, ρ′, θ′, µ′). Clearly, this transition satisﬁes the
forward property.

Thus, we need only to consider the case where Xs 6= ∅. Due to the deﬁnition of the outgoing transitions at
the initial state in Section D.1.5 no crossing transition involves # and ρ(X) = ρσ(X) for all X. Henceforth
we keep this as an invariant.

The running theme. Choose a linear order ≤ on 2R such that S ≤ T implies S ⊆ T . Let S be
minimal in that order such that S = ρ(a) for some a ∈ Bs. If S = R, then we back in the situation that
there are no more variables, so we are done. If S 6= R it is enough to ﬁnd a path to some standard state
V ′ = (W ′, B′, X ′, ρ′, ∅, µ′) such that

kV ′k ≤ kVsk + O(n)

and for all a′ ∈ B′: either S < ρ′(a′) for each a′ ∈ B′ or X ′ = ∅. Indeed, if we reach X ′ = ∅ we are done.
If S < ρ′(a′), then we cannot have ρ′(X) = S for any variable, because the forward property guarantees a
solution at V ′. Thus, S < ρ′(X), too. So assume that removing one set S can be aﬀorded for an increase
in the weight by O(n). But the number of subsets of R is bounded by 2|R| which in turn is constant.
Thus, starting with larger sets C and Ω (still of linear size) is enough to conclude Proposition 26.

Basic operations. We now describe some basic operations, for which it is easy to show that they are
weight reducing and have the forward property.

Let V = (W, B, X , ρ, θ, µ) be a state with a given solution (α, σ) and X 6= ∅. We deﬁne and name the

following rules.

1. Substitution.

If there exists a variable with σ(X) 6= 1, then we substitute τ (X) = yX. We
if θ(X) = ∅ then we allow y to be any constant or variable; if θ(X) 6= ∅,
distinguish two cases:
then y = θ(X), in particular, it is a constant. The corresponding substitution transition is deﬁned
by τ . If we had σ(X) = vw and σ(y) = v then following this transition, the new solution becomes
σ′(X) = w, and if θ(X) = y then w ∈ y∗. In the latter case we need to verify that σ′ is a morphism:
indeed, if θ(X) = y and σ(X) 6= 1, then w ∈ y∗. Hence,

σ′(Xy) = wy = yw = σ′(yX).

For the weight observe that |ασ(X)| drops and so does the weight of the solution.

2. Remove “useless” letters. Consider a state V = (W, B, X , ρ, θ, µ) such that B contains a letter
a ∈ B \ A but a does not appear in σ(W ). Such a letter is called useless and can be removed.
More precisely, let B′ = A ∪ {c ∈ B | c appears in σ(W )} and suppose B′
6= B. Then we have
W ∈ M (B′, X , ρ, θ, µ) and σ(W ) ∈ M (B′, X , ρ, θ, µ); and we can view σ as a morphism σ′ from
M (B′, X , ρ, θ, µ) to M (B′, X , ρ, θ, µ). Let α′ be the restriction of α to M (B′, X , ρ, θ, µ). Then we
ε−→ (W, B′, X , ρ, θ, µ).
have ασW = α′σ′W ; and we obtain a compression transition (W, B, X , ρ, θ, µ)
Since this is a compression transition, it is enough to show that kV k < kV ′k. This is true as the size
of B is smaller, and so the last component of the weight goes down (and it is easy to see that other
cannot increase).

3. Remove “invisible” letters at standard vertices. We assume that there is no type and thus,
V has the form V = (W, B, X , ρ, ∅, µ). We want to make the alphabet B as small as possible. This

27

a non-trivial issue because the solution σ might use letters from B which are neither visible in W
nor belong to A (hence they are useful). Let c be such a constant. Deﬁne B′ = B \ {c, c}. Consider
the trace

α(c) = a1 · · · ak ∈ M (A, ρ0), where a1, . . . , ak ∈ A.

Deﬁnes a trace

and observe that

β(c) = (a1, ρ(c)) · · · (ak, ρ(c)) ∈ M (A, ρ0)

π0β(c) = π0α(c) ∈ M (Ainit, ρinit).

Indeed, π0(a, S) = π0(a) for all (a, S) ∈ A. Since α is a morphism, we have ρ(ai) ⊆ ρ(c) for all
ai; and we have µ(c) = µα(c) = µβ(c). We extend β to a morphism β : M (W, B, X , ρ, ∅, µ) →
M (W, B′, X , ρ, ∅, µ) by leaving all symbols in B′ ∪ X invariant. This is morphism because there are
no types: It is enough to verify that ρ(x) ∩ ρ(y) = ∅ implies β(xy) = β(yx) (which in turn is trivial).
It is clear that σ′ = σβ deﬁnes a B′-solution at V ′ = (W, B′, X , ρ, ∅, µ). Moreover, for all x ∈ B′ ∪ X
we have

(15)
Thus, if α′ denotes the restriction of α to B′∗, then we obtain π0α′σ′(W ) = π0ασ(W ). As a
consequence, the transition

π0ασ(x) = π0αβσ(x) = π0ασ′(x).

(W, B, X , ρ, ∅, µ)

ε−→ (W, B′, X , ρ, ∅, µ)

satisﬁes the forward property. Note that the transition is labeled ε = idC ∗. Any composition of such
transitions is a transition in F : clearly if the composition changes, then one more letter is removed
from the alphabet and there are only O(n) such letters. Clearly the removal of invisible letters
satisﬁes the forward property. Hence, we can remove all invisible letters using a single transition.
(The reader might also notice that Equation (15) needs the preﬁx π0, otherwise it would not hold,
in general.)

For the weight reduction observe that this is a compression transition, we should show that the
weight of the equation decreases. This is the case, as we decrease the size of the alphabet, so the
last component of the weight decreases and it is easy to see that other cannot increase.

4. Introduce letters via renaming. The idea is to introduce a letter from B′ \B such that it becomes
h−→ (W ′, B′, X , ρ′, θ′, µ′). Suppose that the morphism h is
visible W by using a renaming arc V
deﬁned by h(c) = a and W = h(W ′) 6= W ′. Since |W | = |W ′| and |B| ≤ |B′| to ensure that this is
weight-decreasing we replacement of visible (in W ′) letter c only if c does not appear in W and, in
addition, one of the following holds:

a) ρ(a)   ρ′(c).
b) θ′(c) = a.
c) The letter c appears in the range of a type, but a does not.

It is easy to see that each of those cases indeed corresponds to a decrease of weight of the equation.

5. Compression of visible letters. Let V = (W, B, X , ρ, θ, µ) be a state and let B   B′. Consider a
word ab ∈ (B \ {#})∗ where a, b are letters such that ab occurs as a factor in W . We assume a 6= b.
Choosing any letter c ∈ B′ \ B we can deﬁne a compression arc V h−→ (W ′, B′, X , ρ′, θ′, µ′) where h
is deﬁned by h(c) = ab. Suppose that W = h(W ′) 6= W , then we have |W | < |W ′| and hence the
weight decreases as this is a compression operation and the length of the equation drops.

Lemma 27. Let V = (W, B, X , ρ, θ, µ) be a state with a solution (α, σ). All basic operations above deﬁne
transitions V h−→ V ′ which satisfy the forward property. Moreover, if (α′, σ′) is the corresponding solution
at state V ′, then we have

kα′, σ′, V ′k < kα, σ, V k .

Proof. The assertion about the weight was already addressed in the construction. The forward property
has to veriﬁed. This is easy and left to the reader: The most involved case is the removal of invisible
letters, which was analyzed in detail above.

28

E.1. Processing a minimal set of resources

This sections contains additional material to Section 4.2.1.

E.1.1. Lifting free unbalanced arcs at standard vertices

Recall the deﬁnition of a free Hasse arc e = (a → b) in σ(W ) which was given in Deﬁnition 5. Our primary
target are the free unbalanced Hasse arcs. Consider a free Hasse arc e = (a → b) with ρ(a) 6= ρ(b).

First case. There exists some Hasse arc f in σ(W ) with e ∼ f where one position of the arc is
visible. In this case, as f is free, both positions of f are visible. We may assume that f = (a → b) and
|ρ(a)| ≤ |ρ(b)|, In particular, a 6= #. We choose a fresh letter c with ρ(c) = ρ(a) ∪ ρ(b); and we replace
every Hasse arc g = (a → b) ∼ f by c and every Hasse arc g = (b → a) ∼ f by c. This can be realized
using a compression transition deﬁned by h(c) = ab.

Second case. For all Hasse arcs f in σ(W ) with e ∼ f no position of f is visible. Let T = ρ(a) ∪ ρ(b).

We can write α(ab) = a1 · · · ak where ai ∈ A. Deﬁne

αT (ab) = (a1, T ) · · · (ak, T ).

Replacing all f = (a → b) in σ(W ) with e ∼ f by the word αT (ab) and all f = b → a in σ(W )
with e ∼ f by the word αT (ab), we obtain a new solution (α, σ′) at the very same state V . Note that
ab 6= αT (ab) 6= α(ab) since a → b is unbalanced. However, π0α(ab) = π0αT (ab). We also have π0σ = π0σ′.

Lemma 28. Let V = (W, B, X , ρ, ∅, µ) be a standard state with solution (α, σ) such that |B \ A| ≤ |W | ≤
|C|/2. Then following compression transitions, we can move to a state V ′ = (W ′, B′, X , ρ, ∅, µ) with a
solution (α′, σ′) such that:

1. B′ does not have useless or invisible letters.

2. |W ′| ≤ |W |.

3. π0ασ(W ) = π0α′σ′(W ).

4. Every unbalanced arc in σ′(W ′) is crossing.

In particular, the switch from V to V ′ follows a path satisfying the forward property.

Proof. First of all, if there is a useless or invisible letter, we may remove it by following appropriate
compression transitions that satisfy the forward property by Lemma 27. This does not aﬀect 2 (as the
equation is unchanged), nor 3. For 4 observe that removal of useless letters does not inﬂuence it and for
invisible letters, we may introduce only balanced arcs and unbalanced crossing arcs remain crossing. Thus,
enforcing 1 can be done at any moment, also after ensuring 2–4.

If every unbalanced arc in σ(W ) crossing, there is nothing more to do. Thus, we may assume that there
is at least one unbalanced free arc e = (a → b). We shall eliminate such arcs one by one, each elimination
will not increase the size of the equation. So when we ﬁnish, all claims of the Lemma will hold.

Take any free arc e. If there exists some f with e ∼ f where f has a visible position, then we are in
the ﬁrst case above. We follow a compression transition deﬁned by h(c) = ab where c is a fresh letter. It
satisﬁes the forward property by Lemma 27 and so it satisﬁes 4.

In the second case we can replace every f = (a → b) with e ∼ f by αT (ab) deﬁning thereby a new
B-solution σ′. This happens without leaving the state or following any path: we only switch to the solution
(α, σ′). Furthermore,

π0ασ(W ) = απ0σ(W ) = απ0σ′(W ) = π0ασ′(W ).

so 4 still holds. This ends the proof.

29

E.1.2. Strategy: if S = ρ(x) is minimal, then remove all S-letters

We consider a standard state V = (W, B, X , ρ, ∅, µ) where X 6= ∅ with a solution (α, σ). Recall our general
assumption made at the very beginning that ρ(X) 6= R for all X. As a consequence, let S ⊆ R be of
minimal size such that S = ρ(x) for some x ∈ B ∪ X . Then we have S 6= R.

The idea is to remove in B all S-constants without introduction of T -constants, where T < S. If we
succeed to do so by keeping the length of W in O(n), then we are done: after at most 2|R| steps we have
ρ(a) = R for all a ∈ B but still ρ(X) 6= R for the remaining variables, which simply means there are no
variables in any solvable equation. We are done because we view |R| as a constant. In the following we
ﬁx a minimal S.

E.1.3. Visibility of maximal S-runs

We want to ensure that every maximal S-run has an occurrence with at least one visible position. The
intuition is as follows: Let b be a ﬁrst S-letter in a σ(X). Then there is a constant a such that the arc
a → b is in σ(w) and by Lemma 28 it is crossing. Ideally we would like to substitute X with bX, claiming
that b is minimal, but this is not necessarily the case.

In general, an S-factorisation of σ(X) (if it exists) is deﬁned as ubv, where ρ(b) = S and S 6⊆ ρ(u),
to make the factorisation unique we additionally assume that |u| is minimal. Observe that if a → b is a
crossing arc in aX (for a solution σ) then σ(X) has an S-factorisation ubv for some u, v.

We iterate over the list of all variables and for each X we guess, whether it has an S-factorisation. If

so, then we follow a substitution transition X 7→ X ′bX. We deﬁne (resp. redeﬁne):

• ρ(X ′) = ρ(u) and ρ(X) = ρ(v),

• µ(X ′) = µ(u) and µ(X) = µ(v).

Lifting all free unbalanced arcs followed by the above procedure establishes the following lemma.

Lemma 29. Let V = (W, B, X , ρ, ∅, µ) be a standard state and (α, σ) be a solution. Increasing the length
of W by at most O(n), we may assume that every maximal S-run has an occurrence with at least one
visible position.

Proof. We use Lemma 28 to convert the state, by lifting free unbalanced arcs, to some state where all
unbalanced S-arcs are crossing. Consider a maximal S-run b1b2 · · · bm. Take any arcs a → b1 and bm → c,
they are unbalanced, as a, c are not S-constants. Thus by Lemma 28 they are crossing.

As a ﬁrst step, we want to show that for every maximal S-run there is one of its occurrences such that

one of the S-letters is part of a visibly crossing arc.

We extend the relation ∼ from arcs to sequences of arcs (of which we think as factors, as we in fact
apply this notion only to ab1b2 · · · bmc) if e1, e2, . . . , ek are consecutive arcs in a trace then ∼ is the smallest
equivalence relation satisfying

• If e1, e2, . . . , ek and f1, f2, . . . , fk are the corresponding arcs in σ(W ) then e1e2 · · · ek ∼ f1f2 · · · fk.

• If all e1, e2, . . . , ek are invisible within the same occurrence of a variable σ(X) and f1, f2, . . . , fk are

the corresponding arcs in some diﬀerent occurrence of σ(X), then e1e2 · · · ek ∼ f1f2 · · · fk.

We know that e = (a → b1) is crossing, thus there is a visible crossing e′ = (a → b) (or e′ = (b → a),
but we consider only the ﬁrst for simplicity of presentation) such that e ∼ e′. Consider, whether our
a → b1 → b2 → · · · → bm → c has a sequence of arcs in ∼ beginning with e′.
If so then this is a
maximal S-run with a letter in a visibly crossing arc: b1. If not, then let us retrace the sequence of arcs
e = e1 ∼ e2 ∼ · · · ∼ ek = e′ and check, what is the last such edge ei which begins a sequence of arcs in ∼
with a → b1 → b2 → · · · → bm → c.

It cannot be that ei+1 is the corresponding involuted arc (of ei), as then the same would apply also
to the arcs in the rest of a → b1 → b2 → · · · → bm → c. So ei is in σ(X) for some variable X and
ei+1 is the corresponding arc in a diﬀerent occurrence of σ(X). Since this does not apply to arcs in
a → b1 → b2 → · · · → bm → c, at least one of them is not in this occurrence of σ(X), and so some arc in
a → b1 → b2 → · · · → bm → c is visibly crossing.

30

Thus we know that for a maximal S-run there is an occurrence of it with at least one position which is
part of a crossing arc. By symmetry we may assume that a → b, where b is a S-letter and a may but not
have to be an S-letter, crosses the left border at the occurrence of some σ(X), which means that σ(X)
has an S-factorisation.

We iterate over the list of all variables, for each one of them we guess, whether it has an S factorisation.
If so we follow the substitution transition X 7→= X ′bX, which makes the position of b visible. After doing
this for every variable, every maximal S-run has an occurrence with a visible position.

Note that every X created at most two variables, which gives the claim on the size of the equation.

We keep the property in Lemma 29 from now on as an invariant.

E.2. How to ﬁnish if all maximal S-runs are short

After iterating over the S-run compression procedure in Section E.3 we arrive eventually at a standard
state V = (W, B, X , ρ, ∅, µ) with given a solution (α, σ) where all maximal S-runs s are short, meaning
here |s| ≤ 2.

There will be no more compression, we remove the remaining S-constants by lifting S-arcs.

Lemma 30. Let V = (W, B, X , ρ, ∅, µ) be a standard state with solution (α, σ) where all S-runs have
length at most two. By increasing the length of W by at most O(n) (and, hence, with at most O(n) fresh
variables), we can follow transitions satisfying the forward property such that we arrive at a standard state
V ′ = (W ′, B′, X ′, ρ′, ∅, µ′) where we have ρ(b) > S for all b ∈ B.

Proof. We begin with a preprocessing. Consider, one after another, all variables X where σ(X) has an
occurrence with a visibly crossing Hasse arc a → b at the left border with ρ(b) = S.
In such a case
we can factorize σ(X) = ubb′v where ρ(a) ∩ ρ(u) = ∅ and bb′ is a maximal S-run of length one or two.
In particular, S 6= ρ(u)   ρ(X). For each such an X, we follow a substitution transition induced by
X 7→ X ′bb′X where X ′ is a fresh variable. We deﬁne (resp. redeﬁne):

• ρ(X ′) = ρ(u) and ρ(X) = ρ(v),

• µ(X ′) = µ(u) and µ(X) = µ(v).

Since all maximal S-runs have length at most two, after this substitution it holds that

S1 If for some b and X we have σ(X) ∈ bB∗ ∪ B∗b, then ρ(b) 6= S.

S2 If in any occurrence of some S-run one position is visible, then all positions are visible. In particular,

if an S-arc a → b is crossing, then it is unbalanced.

S3 If an unbalanced Hasse arc a → b visibly crosses an occurrence of σ(X) on the left (resp. right), then

ρ(b) 6= S (resp. ρ(a) 6= S).

We lift all free unbalanced arcs one by one, and after that remove useless and invisible letters; note that
this preserves (S1)–(S3) In particular, we may assume that there are no Hasse arcs # → b where ρ(b) = S.
Let us choose four disjoint subsets of B \ {#}:

• Let (S+, S−) be an involuting partition of all S-letters. Deﬁne S± = S+ ∪ S−.

• Let (T+, T−) be an involuting partition of {b ∈ B | ρ(b) 6= S}. Deﬁne T± = T+ ∪ T−.

The choices of S+ and T+ give rise to four disjoint sets H1, . . . H4 of Hasse arcs in σ(W ).

H1 = {a → b | ab ∈ S+T+ ∪ T−S−}
H2 = {a → b | ab ∈ S−T+ ∪ T−S+}
H3 = {a → b | ab ∈ S+T− ∪ T+S−}
H4 = {a → b | ab ∈ S−T− ∪ T+S+}

We ﬁrst uncross all arcs from H1 that occur in σ(W ), then arcs from H2, then H3 and ﬁnally H4. We
investigate what happens for an occurrence of σ(X) on the left border. (This is the same as to investigate

31

an occurrence of σ(X) on the right border. Since in each round we consider X as well as X (in some
order) we investigate in fact the situation at both borders.)

Consider any unbalanced S-arc (a → b) ∈ H1 ∪ H2 which visibly crosses some occurrence of σ(X) on
the left border. Then ρ(b) 6= S by (S3). We can factorise σ(X) = ubv, where ρ(u) ∩ S = ∅, as otherwise
a → b is not an arc. We follow a substitution transition X 7→ X ′bX and we change ρ, µ, σ accordingly.
Afterwards, all arcs in σ(W ) from H1 ∪ H2 that were crossing are free.

Note that ρ(σ(X ′)) ∩ S = ∅, so no unbalanced S-arc can start or end in any occurrence of σ(X ′).
However, due to the splitting, some new unbalanced S-arc might be visibly crossing on the left for X (or,
equivalently, right of X). The crucial observation is that they all are in H3 ∪ H4: by the construction we
have that b ∈ T+ and so an S-arc b → a′ is in H3 ∪ H4. Thus, all arcs from σ(W ) that are in H1 ∪ H2 are
free and so they can be lifted, one after another.

Note that when we lift a to a′ then we may assign it to T− or T+. The rule is that if a ∈ T+ (T−) then
a′ ∈ T+ (T−, respectively); if a ∈ S± then a′ is not assigned to neither T± nor to S±. As a result, all
unbalanced S-arcs are in H3 ∪ H4: we lifted existing H1 ∪ H2 arcs, no new H1 ∪ H2 arc was introduced
and lifting the non-S end of a Hasse arc does to change to which Hi this arc belongs.

Thus, in σ(W ) contains only S-arcs from H3 ∪ H4. We treat them in the same way as we treated arcs
from H1 ∪ H2: ﬁrst we perform the preprocessing that ensures (S1)–(S3). Then we free all those arcs and
ﬁnally we lift them. The analysis is the same as in the case of arcs from H1 ∪ H2. In particular, all arcs
from H3 ∪ H4 are freed and only arcs in H1 ∪ H2 could be be made crossing; but as there are none, this
does not happen. Arcs in H3 ∪ H4 are lifted one by one and in the end there are no S-arcs, so no position
with S-constants.

E.3. S-run compression

This section contains three procedures: S-block compression, S-quasi-block compression, and S-pair com-
pression. They are repeated, in this order, until all maximal S-runs have length at most 2. The S-run
compression is always applied to a minimal set of resources, i.e., such that ρ(a) = S for some constant
a ∈ σ(W ) and for each T < S there is no T -constant in σ(W ) (recall that “<” is an arbitrary, but ﬁxed,
linear order on the sets of resources that extends the ordering by size, i.e., |S| > |T | implies S > T ).

For the space requirements during the cycle we use the following well-known fact.

Lemma 31. Let p be any positive real number with p < 1 and let s0, s1, . . . be sequence of natural number
such that s0 ≤ s · n for some positive constant s and such that for all i ≥ 1 we have

si ≤ p · si−1 + s · n.

Then there is a constant q such that si ≤ q · n for all i.

E.3.1. S-block compression

Block compression as employed here was described in [4] in the setting of free monoids. Since S-block
compression applies only to factors of constants with the same resources, this method can be applied with
virtually no changes, as long as we apply it for a ﬁxed set S ⊆ R of resources.

We begin at a standard state V = (W, B, X , ρ, ∅, µ) with a solution (α, σ) where σ(X) 6= 1 for at least
one variable. Without restriction we assume that there are no invisible letters and that all maximal S-
runs have at least one occurrence with a visible position, see Lemma 29. Recall that S is a minimal set of
resources. W contains some factor aa where a ∈ B and ρ(a) = S. The purpose of this phase is to remove
all visible factors of the form aa, where ρ(a) = S. Note that after the S-block compression we do allow
that there is such a factor in σ(W ), but it cannot be visible. During the procedure we will increase the
length of the equation by O(n). Moreover, after the S-block compression the set of variables X ′ satisﬁes
X ′ ⊆ X , i.e., we have only removed variables.

The ﬁrst step is to create a list L of all pairs (a, λ) ∈ B × N such that σ(W ) contains a sequence of

Hasse arcs

d → a → a → · · · → a

→ e

(16)

|

λ times

}

{z

32

where ρ(a) = S, λ ≥ 2, d 6= a 6= e and at least one position labeled with an S-constant in the sequence
above is visible. (Thus, it could be d or e, but in such a case we have ρ(d) = S or ρ(e) = S.) The inner
factor aλ in such a sequence is called a maximal visible S-block. Since such a block can be associated
with a visible position in W and at most 2 such blocks can be associated with one position of S-letter,
|L| ≤ 2 |W | ∈ O(n).

For each a ∈ B, where some (a, λ) ∈ L, we take a fresh letter ca and for each (a, λ) ∈ L a fresh letter
ca,λ. The notation is purely symbolic, so we do not write λ as natural number. Moreover, we deﬁne a
type by θ(ca,λ) = ca.

The number of fresh letters is at most 2 |W | + |B|. As usual, we let ca,λ = ca,λ and ca = ca. We split

the list L into sublists La where for each a ∈ B

Each La deﬁnes a set of variables

La = {(a, λ) | (a, λ) ∈ L} .

Xa = {X ∈ X | σ(X) ∈ aB∗ for some (a, λ) ∈ La} .

Note that Xa ∩ Xb 6= ∅ implies a = b. We treat one Xa ∪ Xa after another. For simpliﬁcation of notation we
ﬁx a and we rename c = ca and cλ = ca,λ. Thus, the range of θ is just the set {c, c}, but for the moment
we are still at the standard state V = (W, B, X , ρ, ∅, µ) with its solution (α, σ).

When we change B to B′ = B∪{c, c, cλ, cλ | (a, λ) ∈ La} we introduce a type. We deﬁne W ′ by replacing
a. We can write W = h(W ′) where h is deﬁned by h(c) = h(cλ) = a.

all maximal visible S-blocks aλ by cλ
Thus, we follow a renaming transition

V h−→ (W ′, B′, X , ρ′, θ, µ′) = V ′.

Note that kV ′k < kV k, due to introduction of type. It is obvious how to deﬁne (α′, σ′) at V ′.

Next, we consider all variables X ∈ Xa ∪ Xa, one after another in any order. For each X exactly one of

the following rules is applied.

• If σ(X) ∈ cB∗, then choose m ∈ N maximal such that σ(X) = cmw and follow a substitution
transition deﬁned by τ (X) = cX ′X where X ′ is a fresh variable. We let σ(X ′) = cm−1 and
σ(X) = w. We adjust ρ(X ′) = ρ(cm−1), µ(X ′) = µ(cm−1), ρ(X) = ρ(w), and µ(X) = µ(w). We
remove X ′ if m = 1 and otherwise we give a type θ(X ′) = c.

• If σ(X) ∈ cB∗, then we do the analogous change as just above with c instead of c.

As usual, whenever we reach a situation with σ(X) = 1, then we remove the variable X. We scan σ(W )
from left to right and we stop at each occurrence of a maximal c-block cλ with λ ∈ Λ. If this occurrence
contains a visible position, choose any of them and replace one c by cλ at such a visible position. In the
other case, if no position is visible replace the occurrence of the c-block cλ by cλcλ−1. After this ﬁrst scan
we repeat the procedure with c instead of c.

Note that at this point, all constants cλ are visible: we create cλ only for maximal visible blocks, thus
there was aλ as in (16) such that some S-constant was visible. This was not necessarily a, but if this was
not a then a is a minimal/maximal constant in σ(X). Thus after replacing a with c and the substitution
we introduced at least one c from this maximal visible block to the equation. And so it was chosen as the
position, in which c was replaced with cλ.

Before we start the loop observe that for all ℓ1, ℓ2 ∈ N we have

Furthermore, for everyec ∈ {c, c} and every occurrence of a maximal ec-block ecλ with λ ∈ Λ has been
replaced by some blockecℓ1ecλecℓ2 with ℓ1 + ℓ2 = λ − 1. We mark in each such block, the positions of the
ec ’s with the label (ec, λ).
This holds becauseecλ andec commute. The following loop is performed until there are no more marked

ecℓ1ecλecℓ2 =ecλcℓ1+ℓ2 .

positions:

1. Use substitution transitions in order to guarantee for all variables: if θ(X) = c, then |σ(X)| is even.

33

2. Scan σ(W ) from left to right. Stop at each marked position. If the label is (ec, λ), then choose ℓ
maximal such that this position occurs in a factorecℓ. If ℓ is even do nothing. Otherwise, ℓ is odd.

Then use a compression transition deﬁned h(cλ) = ccλ. Note that this is possible, since all |σ(X)|
are even when θ(X) = c, there is marked position available which is not covered by any variable.

3. Use a compression transition deﬁned h(c) = c2. This halves the number of marked positions.

4. Remove all variables with σ(X) = 1; and if there remains a variable with θ(X) = c, then use a

substitution transition which is deﬁned by τ (X) = cX.

It is clear that the loop eventually removes all marked positions, in particular every typed variable was
removed, as it had only marked positions.

After that c is useless: the last compression transition compressed factors cλc into cλ. We can extend
this transition so that it also removes the type and the letter c, it is still weight-decreasing, as it reduces
the size of the equation.

Hence, we are back at a standard state with empty type and without invisible letters. There are no

invisible letters because all cλ are visible in W .

For the space analysis we must show that we do not run out of space. This is clear by Lemma 31. But

actually we need more.

A simple analysis shows that the total length increase by this phase is bounded by

X{a∈B| ρ(a)=S}

2 · |Xa| ≤ 2 · |X | ≤ 2 · |Ω| ∈ O(n).

Lemma 32. After S-block compression there is no visible factor aa for an S-letter a in the corresponding
solution σ. There are no invisible nor useless letters in the obtained equation.

The size of the equation increases by O(m), where m is the number of occurrences of variables; all new

introduced symbols are constants.

E.3.2. S-quasi-block compression

Quasi-block compression has been described in the arXiv version of [4] in the setting of free monoids. We
therefore content ourselves to give a sketch which highlights the diﬀerences between an S-quasi-block and
an S-block compression.

S-quasi-block compression is similar to S-block compression, but there are subtle diﬀerences. We will
introduce new constants cλ and c as above, but as we compress blocks of quasi-letters we need θ(cλcλ) = cc.
We would like to deﬁne a morphism satisfying cλcλ 7→ cλcλcc, but such morphism is impossible. What is
possible is a morphism with cλcλ 7→ cλcλ(cc)2, see below. Such a morphism is enough for our purposes.

More precisely, we start at a standard state V = (W, B, X , ρ, ∅, µ) with a solution (α, σ) where σ(X) 6=
1 for at least one variable. Without restriction we assume that there are no invisible letters and, by
Lemma 29, that all maximal S-runs have at least one occurrence with a visible position. Due to Lemma 32,
there is no visible factor aa, where ρ(a) = S. But there can be quasi-letters aa. The purpose of presented
subprocedure is to remove all factors aaa from W , where ρ(a) = S. We may assume that there is at least
one such factor, as otherwise there is nothing to do.

The ﬁrst step is an arbitrary choice of an involuting partition

S+ ∪ S− = {a ∈ B | ρ(a) = S} .

We say that a letter a and a quasi-letter aa is positive, if a ∈ S+, it is negative otherwise. We concentrate
if σ(X) ∈ aB∗ and
on positive quasi-letters; and we ignore the others. For all X in any order we do:
a ∈ S+, then follow a substitution transition deﬁned by X 7→ aX. After that there are no more crossing
arcs a → a where a ∈ S+.

The Hasse arcs a → a and the factors aa are self-involuting; and we are not allowed to compress them
Imagine that we nevertheless do that. This means we take a fresh self-involuting

into a single letter.

letterea and compress a positive quasi-letters aa intoea. Afterwards we would like to perform theea block
compression. As noted, we cannot compress aa toea. Instead, we simulate the desired block compression
of (ea)ℓ by a quasi-block compression of (aa)ℓ.

34

|

λ times

{z

}

Similarly as in the case of block compression, we ﬁrst create a list L of all pairs (aa, λ) ∈ S+S− × N

when σ(W ) contains a sequence of Hasse arcs

d → (a → a) → · · · → (a → a)

→ e

(17)

such that λ ≥ 1, and at least one position in the sequence above is visible where the resource is equal
to S. (Thus, it could be d or e, but then ρ(d) = S or ρ(e) = S.) Note that we allow λ = 1. We might
have d = a (e = a), but in such a case there is no Hasse arc a → d (e → a). The inner factor (aa)λ in
such a sequence is called a maximal visible S-quasi-block. Note that W contains a sequence of Hasse arcs
a → a → a and a → a → a, and if we have (aa, λ) ∈ L for some λ ≥ 2, then there is a self-involuting
sequence a → a → a → a.

For each a ∈ S+, where some (aa, λ) occurs in L, we create a fresh letter ca and for each pair (aa, λ) ∈ L
we create a fresh letter ca,λ with a ∈ S+. As in the case of block compression, we content ourselves to
assume S+ = {a} for a single letter a. Thus, we abbreviate ca = c and ca,λ = cλ. Clearly, these letters
are not self-involuting and we can use renaming, deﬁned by h(c) = h(cλ) = a to replace maximal visible
S-quasi-block (aa)λ by (cc)λ. The type θ is now between quasi-letters; and introducing the type reduced
the weight at the new state. We deﬁne

θ(cλcλ) = cc.

Note that we have cccλcλ = cλcλcc, but we do not have any commutation between c (or c) and cλcλ. In
particular, ccλcλ 6= cλcλc.

The next step introduces fresh variables. For each X such that σ(X) ∈ ccB∗ let m be maximal such
that σ(X) = (cc)m+1w. We follow a substitution transition deﬁned by X 7→ ccX ′X, where σ(X ′) = (cc)m.

For every variable X with σ(X) = (cc)m for some m ∈ N, we deﬁne a type:

θ(X) = cc.

The type says Xcc = ccX. But, nevertheless Xc 6= cX and Xc 6= cX. If additionally the m such that
σ(X ′) = (cc)m is odd, we follow a substitution transition X ′ 7→ ccX ′, so that |σ(X ′)| is divisible by 4.
Thus, m becomes even. (If we have m = 0, we remove X ′.)

We ﬁrst show the main compression step and then explain how to ensure that its assumption holds.
Assuming, that every λ-block (cc)ℓ we have ℓ ≡ 0 mod 4 (thus, the length is divisible by 8), we follow a
compression transition deﬁned by h(c) = cc (halving).

After each halving we follow for each X such that θ(X) = cc a substitution transition: either deﬁned by
τ (X) = ccX or deﬁned by τ (X) = ccccX. The choice is done to keep the invariant that |σ(X)| is divisible
by 4.

The additional diﬃculty for quasi-block compression (as compared to the block compression) arise from
halving the number of (cc)s. Consider in σ(W ) a maximal visible quasi-block (cc)ℓ, suppose that it
originated from a block (aa)λ. Halving it leads to a ﬁrst situation where this block is compressed into
some (cc)ℓ+1 where ℓ + 1 is odd. So, the length of (cc)ℓ is divisible by 2 but not by 4. At this moment we
introduce cλcλ. If there is a visible position in this occurrence then we replace cc by cλcλ. If no position
is visible, we choose any position of a cc which we replace by cλcλ. This done by a renaming transition
deﬁned by cλ 7→ c. Note that each time we use this transition, the letter cλ becomes visible in the equation;
the proof of this fact is similar as in the case of blocks compression.

So, a maximal cc-quasi-block (cc)ℓ+1 is turned into a block cλcλ(cc)ℓ. To have a name we say that
cλcλ(cc)ℓ is maximal λ-block. The point is that having an even exponent ℓ in a maximal λ-block cλcλ(cc)ℓ
is not good enough. We need that before compressing a letter c into cc that ℓ is divisible by 4. The
solution for a maximal λ-block cλcλ with ℓ ≡ 2 mod 4 is to follow a compression transition with label h
deﬁned by h(cλ) = cccλ. Note that

h(cλcλ) = cccλcλcc = (cc)2cλcλ

After that the remaining process becomes essentially as before in block compression. The remaining
diﬀerence is that we distinguish in the exponent of powers of quasi-letters between 2 mod 4 and 0 mod 4
rather than between odd and even.

35

To summarize, if we started with

d →ea → · · · →ea
}

{z

λ times

|

then we end up with

|

→ e = d → (a → a) → · · · → (a → a)

→ e,

λ times

{z

}

d → cλ → cλ → e.

As in case of block compression, we remove all X as soon as σ(X) = 1. We clean-up by removing invisible
letters, to make this operation weight-reducing we perform it when the last cc is compressed with cλcλ, as
in the case of block compression.

Lemma 33. After S-quasi block compression there is no visible factor aa, aaa for an S-letter a in the
corresponding solution σ. There are no invisible nor useless letters in the obtained equation.

The size of the equation increases by O(m), where m is the number of occurrences of variables; all new

introduced symbols are constants.

E.3.3. S-pair compression

The S-pair compression is used for S-block compression followed by an S-quasi-block compression. It is
essentially the same method as in [12], so we focus only on the diﬀerence to the case of free monoids.

We begin at a standard state V = (W, B, X , ρ, ∅, µ) with a solution (α, σ). Without restriction we may

assume that the following holds.

• For some a ∈ B we have ρ(a) = S.

• σ(X) 6= 1 for at least one variable.

• There are no invisible letters.

• All maximal S-runs have at least one occurrence with a visible position (Lemma 29). Hence, some

constant a ∈ B with ρ(a) = S is visible in W .

• Let a ∈ B with ρ(a) = S. Then there is no visible factor from the set (cid:8)a2, aaa(cid:9). (Due to the

previous block and quasi-block compressions, see Lemma 32 and 33.)

If all maximal S-runs have length at most 2, then we are done. Otherwise we choose an involuting partition

The choice is done to maximize the number of visible occurrences from

S+ ∪ S− = {a ∈ B | ρ(a) = S} = S±.

{ab ∈ S+S− | a 6= a} .

A simple probabilistic argument shows that a constant fraction of S-letters occurring in W can be covered
for some choice of a partition.

Lemma 34. Let V = (W, B, X , ρ, ∅, µ) be a standard vertex and σ its B-solution such that there are no
visible factors aa, aaa for each S-letter a. Let the number of occurrences of S-letters that are part of visible
S-factors of length at least 3 be k. Then there exists a partition (S+, S−) of all S-letters such that number
of occurrences of factors from S+S− in W is at least k/16.

Proof. Take the random partition, every pair of S-letters a, a being assigned to either (S+, S−) or (S−, S+)
with the same probability. Consider any S-factor of length ℓ > 2 and look at each of its ℓ − 1 factors of
length two. At least half (rounding down) of them are of the form ab where b 6= a and all of them are not
of the form aa. Thus the chance for those two constants that a ∈ S+ and b ∈ S− is 1/4. It remains to

4 ≥ ℓ

16 . Summing over all factors yields the claim.

verify that the expected value is at least(cid:4) ℓ−1
2 (cid:5) · 1

36

We create a list

L = {ab ∈ S+S− | ab is visible} .

if σ(X) ∈ S− B∗, then write
We compress all factors ab ∈ L. For all X ∈ X we do the following:
σ(X) = bw with b ∈ S−; and follow a substitution transition X 7→ bX. After such a substitution there is
no crossing factor from L. This increments the length of W by at most 2 S-letters per variable occurrence,
so by O(n). Note that ab ∈ S+ S− implies ba ∈ S+ S−. Thus, for each ab ∈ L we choose a fresh letter
c ∈ C and compress all factors ab in σ(W ) into a single letter c. This can be realized by a compression
transition deﬁned by h(c) = ab.

Once, the list L is empty, we remove invisible and useless letters. (We are not allowed to do it earlier,

since all Hasse arcs a → b with ab ∈ L must remain free.) We also remove all variables with σ(X) = 1.

As we started S-pair compression with an S-letter which was visible in W , we are sure to end with an

equation with a visible S-letter.

Lemma 35. After S-pair compression There are no invisible nor useless letters in the obtained equation.
The size of the equation increases by O(m), where m is the number of occurrences of variables; all new
introduced symbols are constants.

Let the number of occurrences of S-letters that are part of visible S-factors of length at least 3 be k.

Then we also decrease the size of the equation by k/16.

E.3.4. Space requirements for S-run compression

First observe that during the S-run compression the only new occurrences of variables are those inside
the S-block compression and S-quasi block compression. Those occurrences are removed till the end of
the respective subprocedure. Otherwise, the occurrences of variables are introduced only due to lifting of
crossing arcs. We show that number of such occurrences can be bounded and so all states encountered on
a forward path from a small standard state to a ﬁnal state have O(n) occurrences of variables.

Lemma 36. The number of occurrences of variables in equation on a forward path from a small standard
state is O(n).

Proof. We say that X directly created an occurrence of X ′ when X ′ was created during lifting of cross-
ing arcs and X ′ was introduced in a substitution for X; X created X ′ when there is a sequence X =
X1, X2, . . . , Xk = X ′ such that Xi directly created Xi+1. Consider a variable X, it can be split during
lifting of crossing arcs at most 4 times for a ﬁxed set of resources. This gives all variables that are directly
created by X. Note, that each of the directly generated variable has less resources than X: when we
replace X with X ′bX; then we require that ρ(X ′) is strictly smaller than ρ(X ′bX).

Let f (k) be the maximum number of variables that can be created by a variable with k resources, over
the whole algorithm (including itself). Using the previous discussion we get a recursive estimation the
number of variables introduced by X for which initially |ρ(X)| = k and

1 + XS⊆R

4f (k − 1) = 1 + 2k+2f (k − 1)

= 1 + 24+5+···+k+2f (1)
= 1 + 2(k+6)(k−1)/2f (1)

Since the initial equation contains at most n variables and each of them has at most |R| resources and
f (1) = 1 (such a variable cannot introduce anything) the maximal number of occurrences of variables in
the equations on a forward path is at most nf (n) < n(1 + 2(|R|+6)(|R|−1)/2). As we treat |R| as a constant,
we get the desired linear bound.

When a bound on occurrences of variables is established, we can give a bound on the number of letters

introduced during the S-run compression.

Lemma 37. Given a state V with equation W and solution (α, σ), the whole forward path induced by the
S-run compression goes only through states of size |W | + O(n).

37

Proof. First of all, we know that each maximal S-run has a visible position, so there are at most |W | many
of them and at most 2m of them have a letter that is not visible, where m is the number of occurrences
of variables. Hence we can disregard maximal S-runs that are reduced to 2 or less letters, as in total they
can have at most |W | + 2m letters, i.e., the claimed |W | + O(n).

The number of new constants introduced during one iteration of S-run compression is linear in terms of
number of occurrences of variables, so linear in n. On the other hand, we show that a constant fraction of
S-letters in runs of length at least 3 present at the beginning of the iteration is removed from the equation
till the end of this iteration. Clearly, S-block compression and S-quasi block compression can only lower
this amount from some k to k′ (and perhaps introduce some other letters). Then the pair compression
removes a constant fraction of those k′ letters, see Lemma 35. Thus in the end we loose at least a constant
fraction of those original k letters.

Using Lemma 31 we get that the amount of S-letters is linear during all iterations of S-run compression.

E.4. Termination and space requirement for a forward path

The forward path for any solution is necessarily ﬁnite: by deﬁnition it decreases the weight of a solution
at a state and so it needs to terminate.

It is left to show that a forward path exists in the sense that we do not consider too large equations.

Lemma 38. Given a small standard state V its forward path to a ﬁnal state passes only through states
with equations of size additively larger by O(n) than the size of the equation of V .

Proof. For each resource set S the size of the equation is increased by O(n) during the lifting of crossing
arcs and by O(n) during the S-run compression. Since the number of resources is constant, this shows
that the total increase is at most O(n).

Now, the proof of Proposition 26 follows by Lemma 38, an appropriate choice of the constants, and by

the observation that every forward path terminates.

38

