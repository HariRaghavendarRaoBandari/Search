Generative Image Modeling using Style and

Structure Adversarial Networks

Xiaolong Wang, Abhinav Gupta

Robotics Institute, Carnegie Mellon University

Abstract. Current generative frameworks use end-to-end learning and
generate images by sampling from uniform noise distribution. However,
these approaches ignore the most basic principle of image formation: im-
ages are product of: (a) Structure: the underlying 3D model; (b) Style:
the texture mapped onto structure. In this paper, we factorize the image
generation process and propose Style and Structure Generative Adversar-
ial Network (S2-GAN). Our S2-GAN has two components: the Structure-
GAN generates a surface normal map; the Style-GAN takes the surface
normal map as input and generates the 2D image. Apart from a real vs.
generated loss function, we use an additional loss with computed surface
normals from generated images. The two GANs are Ô¨Årst trained inde-
pendently, and then merged together via joint learning. We show our
S2-GAN model is interpretable, generates more realistic images and can
be used to learn unsupervised RGBD representations.

1 Introduction

6
1
0
2

 
r
a

 

M
7
1

 
 
]

V
C
.
s
c
[
 
 

1
v
1
3
6
5
0

.

3
0
6
1
:
v
i
X
r
a

Unsupervised learning of visual representations is one of the most fundamental
problems in computer vision. There are two common approaches for unsuper-
vised learning: (a) using a discriminative framework with auxiliary tasks where
supervision comes for free, such as context prediction [1,2] or temporal embed-
ding [3,4,5,6]; (b) using a generative framework where the underlying model is
compositional and attempts to generate realistic images [7,8,9,10]. The underly-
ing hypothesis of the generative framework is that if the model is good enough
to generate novel and realistic images, it should be a good representation for vi-
sion tasks as well. Most of these generative frameworks use end-to-end learning
to generate RGB images from control parameters (z also called noise since it
is sampled from a uniform distribution). Recently, some impressive results [11]
have been shown on restrictive domains such as faces and bedrooms.

However, these approaches ignore one of the most basic underlying principles
of image formation. Images are a product of two separate phenomena: Struc-
ture: this encodes the underlying geometry of the scene. It refers to the underly-
ing mesh, voxel representation etc. Style: this encodes the texture on the objects
and the illumination. In this paper, we build upon this IM101 principle of im-
age formation and factor the generative adversarial network (GAN) [7] into two
generative processes as Fig. 1. The Ô¨Årst, a structure generative model (namely
Structure-GAN), takes ÀÜz and generates the underlying 3D structure (y3D) for the
scene. The second, a conditional generative network (namely Style-GAN), takes

2

Xiaolong Wang and Abhinav Gupta

Fig. 1:
(a) Generative Pipeline: Given ÀÜz sampled from uniform distribution, our
Structure-GAN generates a surface normal map as output. This surface normal map is
then given as input with another noise vector Àúz to a second generator network (Style-
GAN) and outputs an image. (b) We show some examples of surface normal maps and
images generated by our S2-GAN network. (c) Our Style-GAN can also be considered
as a deep rendering engine: given a synthetic scene, we can use it to render a realistic
image. (To visualize the normals, we represent facing right with blue, horizontal surface
with green, facing left with red, i.e., blue ‚Üí X; green ‚Üí Y; red ‚Üí Z. )

y3D as input and noise Àúz to generate the image yI . We call this factored gener-
ative network Style and Structure Generative Adversarial Network (S2-GAN).
Why S2-GAN? We believe there are fourfold advantages of factoring the
style and structure in the image generation process. Firstly, factoring style and
structure simpliÔ¨Åes the overall generative process and leads to more realistic
high-resolution images. It also leads to a highly stable and robust learning pro-
cedure. Secondly, due to the factoring process, S2-GAN is more interpretable as
compared to its counterparts. One can even factor the errors and understand
where the surface normal generation failed as compared to texture generation.
Thirdly, as our results indicate, S2-GAN allows us to learn RGBD representation
in an unsupervised manner. This can be crucial for many robotics and graphics
applications. Finally, our Style-GAN can also be thought of as a learned render-
ing engine which, given any 3D input, allows us to render a corresponding image.
It also allows us to build applications where one can modify the underlying 3D
structure of an input image and render a completely new image.

However, learning S2-GAN is still not an easy task. To tackle this challenge,
we Ô¨Årst learn the Style-GAN and Structure-GAN in an independent manner. We
use the NYUv2 RGBD dataset [12] with more than 200K frames for learning
the initial networks. We train a Structure-GAN using the ground truth sur-
face normals from Kinect. Because the perspective distortion of texture is more
directly related to normals than to depth, we use surface normal to represent

Structure  GAN ùëßÃÇ Style GAN ùëßÃÉ Uniform Noise  Distribution Output1: Surface Normal Uniform Noise  Distribution Output2: Natural Indoor Scenes (a) Generative Pipeline (b) Generated Examples (c) Synthetic Scenes Rendering S2-GAN

3

image structure in this paper. Note that here no RGB images are seen by either
the generator or the discriminator network. We learn in parallel our Style-GAN
which is conditional on the surface normals. We use ground truth surface nor-
mals as inputs in this scenario. While training the Style-GAN, we have two loss
functions: the Ô¨Årst loss function takes in an image and the surface normals and
tries to predict if they correspond to a real scene or not. However, this loss func-
tion alone does not enforce any pixel based constraints. To enforce the pixel-wise
constraints, we make the following assumption: if the generated image is realistic
enough, we should be able to reconstruct or predict the 3D structure based on it.
We achieve this by adding another discriminator network on top of the genera-
tor network. More speciÔ¨Åcally, the generated image is not only forwarded to the
discriminator network in GAN but also a input for the trained surface normal
predictor network. Once we have trained an initial Style-GAN and Structure-
GAN, we combine them together and perform end-to-end learning jointly where
images are generated from ÀÜz, Àúz and fed to discriminators for real/fake task.

2 Related Work

Unsupervised learning of visual representation is one of the most challenging
problems in computer vision. There are two primary approaches to unsupervised
learning. The Ô¨Årst is the discriminative approach where we use auxiliary tasks
such that ground truth can be generated without labeling. The hope is that a
representation learned from auxiliary tasks would generalize to other tasks. Some
examples of these auxiliary tasks include predicting: the relative location of two
patches [2], ego-motion in videos [13,14], optical Ô¨Çow for a static image [15].

A more common approach to unsupervised learning is to use a generative
framework. Two types of generative frameworks have been used in the past.
Non-parametric approaches perform matching of an image or patch with the
database for tasks such as texture synthesis [16] or super-resolution [17]. In this
paper, we are interested in developing a parametric model of images. One com-
mon approach is to learn a low-dimensional representation which can be used
to reconstruct an image by enforcing regularization such as sparsity. Some ex-
amples include the deep auto-encoder [18,19] or Restricted Boltzmann machines
(RBMs) [20,21,22,23,24]. However, in most of the above scenarios it is hard to
generate new images since sampling in latent space is not an easy task. The
recently proposed Variational auto-encoders (VAE) [8] tackles this problem by
generating images with variational sampling approach. Following this work, Gre-
gor et al. [9] proposed the Deep Recurrent Attention Writer, which embedded
an attention mechanism into the VAE approach. However, these approaches are
restricted to simple datasets such as MNIST. To generate interpretable images
with richer information, the VAE is extended to be conditioned on captions [25]
and graphics code [26]. Besides RBMs and auto-encoders, there are also many
novel generative models in recent literature [27,28,29,30]. For example, Dosovit-
skiy et al. [27] proposed to use CNNs to generate chair given its type, viewpoint,
and color.

In this work, we build our model based on the Generative Adversarial Net-
works (GANs) framework proposed by Goodfellow et al. [7]. This framework

4

Xiaolong Wang and Abhinav Gupta

was extended by Denton et al. [31] to generate images. SpeciÔ¨Åcally, they pro-
posed to use a Laplacian pyramid of adversarial networks to generate images in a
coarse to Ô¨Åne scheme. However, training these networks is still tricky and unsta-
ble. Therefore, an extension DCGAN [11] proposed good practices for training
adversarial networks and demonstrated promising results in generating images.
There are more extensions include using conditional variables [32,33,34]. For in-
stance, Mathieu et al. [33] introduced to predict future video frames conditioned
on the previous frames. In this paper, we further simplify the image generation
process by factoring out the generation of 3D structure and style.

In order to train our S2-GAN we combine adversarial loss with 3D sur-
face normal prediction loss to provide extra constraints during learning. This
is related to the idea of combining multiple losses for better generative model-
ing [35,36,37]. For example, Makhzani et al. [36] proposed an adversarial auto-
encoder which takes the adversarial loss as an extra constraint for the latent code
during training the auto-encoder. Larsen et al. [37] introduced to combine gen-
erative adversarial networks and variational auto-encoders so that they can use
the discriminator in GAN for optimizing the variational auto-encoder objective.
Finally, the idea of factorizing image into two separate phenomena has been
well studied in [38,39,40,41], which motivates us to decompose the generative
process to structure and style. We use the RGBD data from NYUv2 to factorize
and learn a S2-GAN model. It is also closely related to the idea of learning
representations for surface normal estimation [42,43,44,45]. In fact we use the
losses used in these papers and combine it with adversarial loss to generate high
quality images.

3 Background for Generative Adversarial Networks

The Generative Adversarial Networks (GAN) [7] is a framework for training
generative models. This approach trains two models at the same time: generator
G and discriminator D. The generator G takes the input which is a latent random
vector z sampled from uniform noise distribution and tries to generate a realistic
image. The discriminator D performs binary classiÔ¨Åcation to distinguish whether
an image is generated from G or it is a real image. Thus the two models are
competing against each other (hence, adversarial): network G will try to generate
images which will be hard for D to diÔ¨Äerentiate from real image, meanwhile
network D will learn to avoid getting fooled by G. We apply Convolutional
Neural Networks for both discriminator and generator in this paper.

Formally, we assume that we are optimizing the networks with mini-batch
gradient descent with batch size of M . We are given a batch of samples as X =
(X1, ..., XM ) and a set of z sampled from uniform distribution as Z = (z1, ..., zM ).
The training of GAN is an iterative procedure with 2 steps: (i) Ô¨Åx the parameters
of network G and optimize network D; (ii) Ô¨Åx network D and optimize network
G. We introduce the losses for training both networks as following. The loss for
training network D can be represented as,

M/2(cid:88)

M(cid:88)

LD(X, Z) =

L(D(Xi), 1) +

L(D(G(zi)), 0).

(1)

i=1

i=M/2+1

Inside a batch, half of images are real and the rest G(zi) are images generated by
G given zi. D(Xi) ‚àà [0, 1] represents the binary classiÔ¨Åcation score given input
image Xi. L is the binary entropy loss which is deÔ¨Åned as,
‚àó
)].

) + (1 ‚àí y)log(1 ‚àí y

L(y

(2)

‚àó

, y) = ‚àí[y log(y

‚àó

S2-GAN

5

Thus the loss LD(X, Z) for network D is optimized to classify the real image
as label 1 and the generated image as 0. On the other hand, the generator G is
optimized via minimizing the loss:

M(cid:88)

LG(Z) =

L(D(G(zi)), 1),

(3)

i=M/2+1

which takes the Z as input and tries to fool the discriminator D to classify the
generated image as a real image. In each iteration of training, we Ô¨Årst optimize
D with Eq. 1 by Ô¨Åxing G, and then we optimize G with Eq. 3 by Ô¨Åxing D.

4 Style and Structure GAN
GAN and DCGAN approaches directly generate images from the sampled z.
Instead, we use the fact that image generation has two components: (a) gener-
ating the underlying structure based on the objects in the scene; (b) generating
the texture/style on top of this 3D structure. We use this simple observation
to decompose the generative process into two procedures: (i) Structure-GAN -
this process generates surface normals from sampled ÀÜz and (ii) Style-GAN - this
model generates the images taking as input the surface normals and another
latent variable Àúz sampled from uniform distribution. We train both models with
RGBD data, and the ground truth surface normals are obtained from the depth.

4.1 Structure-GAN
We can directly apply GAN framework to learn how to generate surface normal
maps. The input to the network G will be ÀÜz sampled from uniform distribution
and the output is a surface normal map. We use a 100-d vector to represent the
ÀÜz and the output is in size of 72 √ó 72 √ó 3. The discriminator D will learn to
classify the generated surface normal maps from the real maps obtained from
depth. We visualize some examples of generated surface normal maps in Fig. 2.
Network architecture. We use a deeper architecture as compared to the net-
work architecture proposed in DCGAN [11]:

Generator network. As Table 1 (top row) illustrates, we apply a 10-layer
model for the generator. Given a 100-d ÀÜz as input, it is Ô¨Årst fully connected to a
3D block (9√ó9√ó64). Then we further perform convolutional operations on top of
it and generate the surface normal map in the end. Note that ‚Äúuconv‚Äù represents
fractionally-strided convolution [11], which is also called as deconvolution. We
follow the settings in [11] and use Batch Normalization [46] and ReLU activations
after each layer except for the last layer, where a TanH activation is applied.

Discriminator network. We show the network architecture in Table 1 (bot-
tom left). It is a 6-layer network with 5 convolutional layers and 1 fully connected
layer. Taking an image as input, the network outputs a single number which pre-
dicts the input surface normal is real or generated. We use LeakyReLU [47,48]
for activation functions as in [11]. However, we do not apply Batch Normaliza-
tion here. In our case, we Ô¨Ånd that the discriminator network easily Ô¨Ånds trivial
solutions with Batch Normalization.

6

Xiaolong Wang and Abhinav Gupta

Structure-GAN(G)
Input Size
Kernel Number
Kernel Size
Stride

fc
‚àí
‚àí
‚àí

uconv conv conv conv conv uconv conv uconv conv

9
9 √ó 9 √ó 64 128
4

2(up)

18

18
18
128 256 512 512

18

3
1

3
1

3
1

3
1

18
256

4

2(up)

36
128

3
1

36
64
4

2(up)

72
3
5
1

Structure-GAN(D) conv conv conv conv conv fc
9 ‚àí
36
Input Size
Kernel Number
128 256 512 128 1
3 ‚àí
Kernel Size
1 ‚àí
Stride

72
64
5
2

3
2

3
2

5
1

36

18

Style-GAN(D) conv conv conv conv conv fc
8 ‚àí
64
Input Size
128
Kernel Number 64
128 256 512 128 1
3 ‚àí
Kernel Size
5
1 ‚àí
2
Stride

3
2

3
2

5
2

16

32

Table 1: Network architectures. Top: generator of Structure-GAN; bottom: discrimi-
nator of Structure-GAN (left) and discriminator of Style-GAN (right). ‚Äúconv‚Äù means
convolutional layer, ‚Äúuconv‚Äù means fractionally-strided convolutional (deconvolutional)
layer, where 2(up) stride indicates 2x resolution. ‚Äúfc‚Äù means fully connected layer.

Fig. 2: Left: 4 Generated Surface Normal maps. Right: 2 Pairs of rendering results on
ground truth surface normal maps using the Style-GAN without pixel-wise constraints.

4.2 Style-GAN
Given the RGB images and surface normal maps from Kinect, we train another
GAN in parallel to generate images conditioned on surface normals. We call this
network Style-GAN. First, we modify our generator network to a conditional
GAN as proposed in [32,31]. The conditional information, i.e., surface normal
maps, are given as additional inputs for both the generator G and the discrim-
inator D. Augmenting surface normals as an additional input to D not only
forces the generated image to look real, but also implicitly enforces the gener-
ated image to match the surface normal map. While training this discriminator,
we only consider real RGB images and their corresponding surface normals as
the positive examples. Given more cues from surface normals, we generate higher
resolution of 128 √ó 128 √ó 3 images with the Style-GAN.

Formally, we have a batch of RGB images X = (X1, ..., XM ) and their corre-
sponding surface normal maps C = (C1, ..., CM ), as well as samples from noise
distribution ÀúZ = (Àúz1, ..., ÀúzM ). We reformulate the generative function from G(Àúzi)
to G(Ci, Àúzi) and discriminative function is changed from D(Xi) to D(Ci, Xi).
Then the loss of discriminator network in Eq. 1 can be reformulated as,

LD

cond(X, C, ÀúZ) =

L(D(Ci, Xi), 1) +

L(D(Ci, G(Ci, Àúzi)), 0),

(4)

M/2(cid:88)

i=1

M(cid:88)

i=M/2+1

M(cid:88)

and the loss of generator network in Eq. 3 can be reformulated as,

LG

cond(C, ÀúZ) =

L(D(Ci, G(Ci, Àúzi)), 1).

(5)

i=M/2+1

We apply the same scheme of iterative training. By doing this, we can generate
the images with network G as visualized in Fig. 2 (right).
Network architecture. We show our generator network as Fig. 3. Given a 128√ó
128√ó 3 surface normal map as input, it is Ô¨Årst passed through two convolutional

S2-GAN

7

Fig. 3: Generator of Style-GAN. It takes 128 √ó 128 surface normal maps and 100-d
noise Àúz as inputs and outputs 128 √ó 128 RGB images.

Fig. 4: Our Style-GAN. Given the ground truth surface normals and Àúz as inputs,
the generator G learns to generate RGB images. The supervision comes from two
networks: The discriminator network takes the generated images, real images and their
corresponding normal maps as inputs to perform classiÔ¨Åcation; The FCN takes the
generated images as inputs and predict the surface normal maps.
layers which leads to 128-channel of 32√ó32 feature maps. Meanwhile, the network
also takes a 100-d Àúz as input. It is Ô¨Årst fully connected to a block of 8 √ó 8 √ó 64
dimension, and then the fractionally-strided convolutions (deconvolutions) are
applied on it. The result is a 32 √ó 32 √ó 64 block. These two 3-D blocks from two
input sources are further concatenated together into the dimension of 32 √ó 32 √ó
192 and 7 layers of convolutions as well as fractionally-strided convolutions are
performed on top of it. The output of the network is a 128√ó 128√ó 3 RGB image.
For the discriminator in Style-GAN, we apply the similar architecture of the one
in Structure-GAN (bottom right in Table. 1). The input for the network is the
concatenation of surface normals and images (128 √ó 128 √ó 6).

4.3 Multi-task Learning with Pixel-wise Constraints

The Style-GAN can make the generated image look real and also enforce it to
match the provided 3D structure (surface normal maps) implicitly. However, as
shown Fig. 2 (right), the images are still noisy and the edges are not well aligned
with the edges in the surface normal maps. Thus, we propose to add a pixel-wise
constraint to explicitly guide the generative model to align the outputs with the
input surface normal maps.

We make the following assumption: If the generated image is real enough,
it can be used for reconstructing the surface normal maps. To encode this con-
straint, we train another network for surface normal estimation. We modify the

64 64 64 32 32 128 128 128 64 8 8 64 16 16 32 32 256 5x5 conv 64 5x5 conv 4x4 uconv 4x4 uconv 3x3 conv 3x3 conv 3x3 conv 4x4 uconv 4x4 uconv 4x4 uconv 5x5 conv 32 32 16 16 512 512 16 16 256 32 32 128 64 64 64 128 128 128 128 Concat ùëßÃÉ 100 fc Style Generator  Network Fully Convolutional Network Style Discriminator Network Binary  Classification Input ‚àí Surface Normal Estimation Generated  Images Real Images ùëßÃÉ Uniform Noise Distribution + 8

Xiaolong Wang and Abhinav Gupta

Fully Convolutional Network (FCN) [49] with the classiÔ¨Åcation loss as men-
tioned in [42] for this task. More speciÔ¨Åcally, we quantize the surface normals to
40 classes with k-means clustering as in [42,50] and the loss is deÔ¨Åned as

M(cid:88)

K√óK(cid:88)

LF CN (X, C) =

Ls(Fk(Xi), Ci,k),

(6)

i=1

k=1

where Ls means the softmax loss and the output surface normal map is in
K √ó K dimension, and K = 128 is in the same size of input image. Fk(Xi) is the
output of kth pixel in the ith sample. Ci,k(1 (cid:54) Ci,k (cid:54) 40) is the label for the kth
pixel in sample i. Thus the loss is designed to enforce each pixel in the image
to generate accurate surface normal. Note that when training the FCN, we use
the RGBD data which provides indoor scene images and ground truth surface
normals. The model is trained from scratch without ImageNet pre-training.

FCN architecture. We apply the standard AlexNet [51] following the same
training and testing scheme as [49], with small modiÔ¨Åcations on the fully con-
nected layers. As we are training the network without ImageNet pre-training,
we use smaller kernel numbers of 1024 and 512 for the two layers before the
last layer. We also switch the second-to-last layer from convolutional layer to
fractionally-strided convolutional (deconvolution with stride 2) layer, so that we
can have higher resolution outputs. A bilinear upsampling layer (4x resolution)
is further applied on the last convolution layer to generate the Ô¨Ånal results.

Given the trained FCN model, we can use it as an additional supervision
(constraint) in the adversarial learning. Our Ô¨Ånal model is illustrated in Fig. 4.
During training, not only the gradients from the classiÔ¨Åcation loss of D will be
passed down to G, but also the surface normal estimation loss from the FCN is
passed through the generated image to G. This way, the adversarial loss from
D will make the generated images look real, and the FCN will give pixel-wise
constraints to make the generated images aligned with surface normal maps.

Formally, we combine the two losses in Eq. 5 and Eq. 6 for the generator G,

LG

multi(C, ÀúZ) = LG

cond(C, ÀúZ) + LF CN (G(C, ÀúZ), C),

(7)

where G(C, ÀúZ) represents the generated images given a batch of surface normal
maps C and noise ÀúZ. The training procedure for this model is similar to the
original adversarial learning, which includes three steps in each iteration:

‚Äì (a) Fix the generator network G, optimize the discriminator network D with

Eq. 4.

‚Äì (b) Fix the FCN model and the discriminator network D, optimize the gen-

erator network G with Eq. 7.

‚Äì (c) Fix the generator network G, Ô¨Åne-tune FCN model using generated im-

ages as well as real images.

Note that the parameters of FCN model are Ô¨Åxed in the beginning of multi-task
learning, i.e., we do not Ô¨Åne-tune FCN with step (c) in the beginning. The reason
is the generated images are not good in the beginning, so feeding bad examples
to FCN seems to make the surface normal prediction worse.

S2-GAN

9

Fig. 5: Full model of our S2-GAN. It can directly generate RGB images given ÀÜz, Àúz as
inputs. For simplicity, we do not visualize the positive samples in training. During joint
learning, the loss from Style-GAN is also passed down to the Structure-GAN.

4.4 Joint Learning for S2-GAN
After training the Structure-GAN and Style-GAN independently, we merge all
networks and train them jointly. As Fig. 5 shows, our full model includes surface
normal generation from Structure-GAN, and based on it the Style-GAN gener-
ates the image. Note that the generated normal maps are Ô¨Årst passed through
an upsampling layer with bilinear interpolation before they are forwarded to the
Style-GAN. Since we do not use ground truth surface normal maps to generate
the images, we remove the FCN constraint from the Style-GAN. The discrim-
inator in Style-GAN takes generated normals and images as negative samples,
and ground truth normals and real images as positive samples.

For the Structure-GAN, the generator network receives not only the gradients
from the discriminator of Structure-GAN, but also the gradients passed through
the generator of Style-GAN. In this way, the network is forced to generate surface
normals which not only are realistic but also help generate better RGB images.
Formally, the loss for the generator network of Structure-GAN can be represented
as combining Eq. 3 and Eq. 5,

joint( ÀÜZ, ÀúZ) = LG( ÀÜZ) + Œª ¬∑ LG
LG

cond(G( ÀÜZ), ÀúZ)

(8)

where ÀÜZ = (ÀÜz1, ..., ÀÜzM ) and ÀúZ = (Àúz1, ..., ÀúzM ) represent two sets of samples
drawn from uniform distribution for Structure-GAN and Style-GAN respectively.
The Ô¨Årst term in Eq. 8 represents the adversarial loss from the discriminator of
Structure-GAN and the second term represents that the loss of the Style-GAN
is also passed down. We set the coeÔ¨Écient Œª = 0.1 in the experiments to prevent
the generated normals from over Ô¨Åtting to the task of generating RGB images via
Style-GAN. In our experiments, we Ô¨Ånd that if we set Œª to a higher number such
as Œª = 1, the loss LG( ÀÜZ) easily diverges to high values and the Structure-GAN
can no longer generate reasonable surface normal maps.

5 Experiments
We demonstrate the eÔ¨Äectiveness of S2-GAN using two types of experiments: (a)
In the Ô¨Årst set of experiments we qualitatively and quantitatively evaluate the
quality of images generates using our model; (b) Next, we evaluate the quality of
unsupervised representation learning by applying the network for diÔ¨Äerent tasks
such as image classiÔ¨Åcation and object detection.
Dataset. In our experiment, we train and test our model using the NYUv2
dataset [12]. We use the raw video data during training and follow the oÔ¨Écial

Style Generator Network Style Discriminator Network Generated  Images ùëßÃÉ Structure Generator Network ùëßÃÇ Structure Discriminator Network Uniform Noise  Distribution Uniform Noise  Distribution Generated  Normals Generated  Normals 10

Xiaolong Wang and Abhinav Gupta

Fig. 6: Results of Style-GAN conditioned on ground truth surface normals (Ô¨Årst 3
rows) and synthetic scenes (last 2 rows). For ground truth normals, we show the input
normals, our generated images and the original corresponding images.

split with 249 training and 215 testing scenes. From the 249 training video scenes,
we extract 200K frames of RGBD data. We compute the surface normals from
the depth data using the provided development kit and TV-denoising [45,42].

Parameter Settings. We follow the parameters suggested in [11] for training.
We trained the models using the stochastic gradient descent with batch size
M = 128. The inputs and outputs for all networks are scaled to [‚àí1, 1] (including
surface normals and RGB images). We apply the Adam optimizer [52] with
momentum term Œ≤1 = 0.5, Œ≤2 = 0.999. During training the Style and Structure
GANs separately, we set the learning rate to 0.0002. We train the Structure-
GAN for 25 epochs. For Style-GAN, we Ô¨Årst Ô¨Åx the FCN model and train it for
25 epochs, then the FCN model are Ô¨Åne-tuned together with 5 more epochs. For
joint learning of two GANs, we use the learning rate as 0.00002 and train them
for 5 epochs. The networks are initialized using Normal distribution with zero
mean and 0.02 standard deviation. Negative slope for LeakyReLU is 0.2.

Baselines. We have 4 baseline models trained on NYUv2 training set: (a) DC-
GAN [11]: it takes uniform noise as input and generate 64 √ó 64 images; (b)
DCGAN+LAPGAN: we train a LAPGAN [31] on top of DCGAN, which takes
lower resolution images as inputs and generates 128 √ó 128 images. We apply
the same architecture as our Style-GAN for LAPGAN (Fig. 3 and Table. 1). (c)
DCGANv2: we train a DCGAN with the same architecture as our Structure-
GAN (Table. 1). (d) DCGANv2+LAPGAN: we train another LAPGAN on top
of DCGANv2 as (b) with the same architecture. Note that baseline (d) has the
same model complexity as our model.

Input Output Original Input Output Original Input Output Original Input Output Input Output Input Output Input Output S2-GAN

11

Fig. 7: Comparison between models with and without pixel-wise constraints.

Fig. 8: (a) Pairs of surface normals and images generated by S2-GAN. (b) Results of
DCGAN. (c) Results of DCGAN+LAPGAN. For each pair, result on the left is from
DCGAN and on the right is applying LAPGAN after it.

5.1 Qualitative Results for Image Generation
Style-GAN Visualization. Before showing the image generation results of
the full S2-GAN model, we Ô¨Årst visualize the results of our Style-GAN given the
ground truth surface normals on the NYUv2 test set. As illustrated in the Ô¨Årst
3 rows of Fig. 6, we can generate nice rendering results which are well aligned
with the surface normal inputs. We also compare our results with the original
RGB images, and show that we can generate a diÔ¨Äerent style (illumination, color,
texture) of image with the same structure. We also make comparisons on the
results of Style-GAN with/without pixel-wise constraints as visualized in Fig. 7.
We show that if we train the model without the pixel-wise constraint, the output
is less smooth and noisier than our approach.
Rendering on Synthetic Scenes. One application of our Style-GAN is ren-
dering synthetic scenes. We use the 3D models annotated in [53] to generate the
synthetic scenes. We use the scenes corresponding to the NYUv2 test set and
make some modiÔ¨Åcations by rotation, zooming in/out. As the last two rows of
Fig. 6 show, we can obtain very realistic rendering results on 3D models.

W/O  Constraint With Constraint W/O  Constraint With Constraint W/O  Constraint With Constraint (a) Indoor scenes generated by our method (b) Indoor scenes generated by DCGAN (c) Indoor scenes generated by DCGAN + LAPGAN 12

Xiaolong Wang and Abhinav Gupta

Fig. 9: Walking the latent space: Our latent space is more interpretable and we obtain
smooth transitions of generated results by interpolating the inputs.

S2-GAN Visualization. We now show the results of our full generative model.
Given the noise ÀÜz, Àúz, our model generate both surface normal maps (72 √ó 72)
and RGB images (128√ó 128) after that, as shown in Fig. 8(a). We compare with
the baselines including DCGAN(Fig. 8(b)) and DCGAN+LAPGAN (Fig. 8(c)).
We can see that our method can generate more structured indoor scenes, i.e., it
is easier to Ô¨Ågure out the structure and objects in our image. We also Ô¨Ånd that
using LAPGAN does not help much improving the qualitative results.
Walking the latent space. One big advantage of our model is that it is in-
terpretable. Recall that we have two random uniform vectors ÀÜz, Àúz as inputs for
Structure and Style networks. We conduct two experiments here: (i) Fix Àúz (style)
and manipulate the structure of images by changing ÀÜz; (ii) Fix ÀÜz (structure) and
manipulate the style of images by changing Àúz. SpeciÔ¨Åcally, given an initial set of
ÀÜz and Àúz, we pick up a series of 10 random points in ÀÜz or Àúz and gradually add 0.1
to these points for 6 ‚àí 7 times. We show that we can obtain smooth transitions
in the outputs by interpolating the inputs as Fig. 9. For the example in the Ô¨Årst
two rows of Fig. 9, we show that by interpolating ÀÜz, we can gradually ‚Äúgrow‚Äù
a 3D cubic in the room and the style of the RGB images are consistent since
we Ô¨Åx the Àúz. For the last rows in Fig. 9, we Ô¨Åx the structure of the image and
interpolate the Àúz so that the window of the room is gradually shut down.
Nearest Neighbors Test. To estimate the novelness of our generated images,
we apply nearest neighbors test on them. We apply the AlexNet pre-trained on
the Places dataset [54] as feature extractor. We extract the Pool5 feature of the
generated images as well as the real images (both training and testing) from the

Fix Structure & Change Style Fix Structure & Change Style Fix Style & Change Structure Fix Style & Change Structure S2-GAN

13

Fig. 10: Nearest neighbors test on generated images.

dataset. We show the results as Fig. 10. In each row, the Ô¨Årst image is generated
by our model, which is used as a query. We show the top 7 retrieved real images.
We observe that while the images are semantically related, they have diÔ¨Äerent
style and structure as compared to nearest neighbors.

5.2 Quantitative Results for Image Generation
To evaluate the generated images quantitatively, we apply the AlexNet pre-
trained (supervised) on Places [54] and ImageNet dataset [55] to perform classi-
Ô¨Åcation and detection on them. The motivation is: If the generated images are
realistic enough, state of the art classiÔ¨Åers and detectors should Ô¨Åre on them
with high scores. We compare our method with the three baselines mentioned in
the beginning of experiment: DCGAN, DCGANv2 and DCGANv2+LAPGAN.
We generate 10K images for each model and perform evaluation on them. Note
that we rescale the images generated from diÔ¨Äerent methods into the same size.
ClassiÔ¨Åcation on generated images. We apply the AlexNet model pre-
trained on Places dataset [54], namely Places-AlexNet, to perform classiÔ¨Åcation
on the generated images. If the image is real enough, it is very likely that the
Places-AlexNet will give high response in one class during classiÔ¨Åcation. Thus,
we can use the inÔ¨Ånity norm || ¬∑ ||‚àû of the softmax output (i.e., the maximum
probability) of Places-AlexNet to represent the image quality. We compute the
results for this metric on all generated images and show the mean for diÔ¨Äerent
models as Fig. 11(a). We show that we are around 2% better than the baselines.
Object detection on generated images. In this experiment, we Ô¨Årst Ô¨Åne-
tune the ImageNet pre-trained AlexNet model on the NYUv2 object detection

Query Nearest Neighbors Results 14

Xiaolong Wang and Abhinav Gupta

Fig. 11:
(a) The InÔ¨Ånity Norm of classiÔ¨Åcation results on generated images. (b)
Number of Ô¨Åres over diÔ¨Äerent thresholds for object detection on generated images. (c)
Scene classiÔ¨Åcation on SUN RGB-D with our model and other methods (no Ô¨Åne-tuning).

mean bath bed book box chair count- desk door dress- garba- lamp monit- night pillow sink sofa table tele toilet

tub

shelf

-er

-ge bin

Ours

-er
32.4 44.0 67.7 28.4 1.6 34.2 43.9 10.0 17.3 33.9
Scratch
30.9 35.6 67.7 23.1 2.1 33.1 40.5 10.1 15.2 31.2
DCGAN 30.4 38.9 67.6 26.3 2.9 32.5 39.1 10.6 16.9 23.6
9.8 14.4 30.8

stand
41.7 31.3 33.1 50.2 21.9 25.1 54.9
39.9 30.5 36.6 43.8 20.4 29.5 52.8
44.5 29.6 37.0 45.2 21.0 28.5 38.4
DCGANv2 31.1 35.3 69.0 21.5 2.0 32.6 36.4
39.6 32.2 34.6 47.9 21.1 27.2 54.4
Imagenet 37.6 33.1 69.9 39.6 2.3 38.1 47.9 16.1 24.6 40.7 26.5 37.8 45.6 49.5 36.1 34.5 53.2 25.0 35.3 58.4

-or
28.1 24.8
26.8 29.1
26.5 25.1
29.2 27.3

22.6
19.4
23.0
25.4

vision

Table 2: Detection results on NYU test set.

dataset with Fast-RCNN [56]. Then we apply this detector on the generated
images. We assume that if the image is realistic enough, the detector should
Ô¨Ånd meaningful objects (door, bed, sofa, table, counter etc) in the images. Thus
we want to investigate whether the detector can Ô¨Ånd more foreground objects
on the images generated by us. We plot the curves as Fig. 11(b), in which the
x-coordinate represents the probability threshold of detection results, and the
y-coordinate represents average number of detections (Ô¨Åres) over all images gen-
erated by one model. We show that the detector can Ô¨Ånd more foreground objects
in the images generated by us. At the start point of 0.3 threshold, there are on
average 2.2 detections per image and 1.72 detections on the images generated
by DCGAN. Interestingly, results of DCGANv2 is very close to DCGAN and
DCGANv2 with LAPGAN has worse performance. It seems adding LAPGAN
make the detector classify more proposals as backgrounds.

5.3 Representation Learning for Recognition Tasks
In this experiment, we try to explore whether the representation learned by the
discriminator network in our Style-GAN can be transferred to other vision tasks
such as scene classiÔ¨Åcation and object detection. Since the input for the network
is RGB image and surface normal map, our model can be applied to recognition
tasks in RGBD data. We perform the experiments on scene classiÔ¨Åcation on SUN
RGB-D dataset [57,58,59,12] as well as object detection on NYUv2 dataset.

Scene ClassiÔ¨Åcation. We use the standard train/test split for scene clas-
siÔ¨Åcation in SUN RGB-D dataset, which includes 19 classes with 4852 images
for training and 4660 for testing. We use our model, taking RGB images and
normals as inputs, to extract the feature of the second-to-last layer and train

(a) Classification on generated images. (b) Object detection on generated images. S2-GAN Infinity Norm DCGAN DCGANv2 DCGANv2 + LAPGAN 29.0 25.6 27.1 27.2 S2-GAN RGB Accuracy DCGAN GIST Places-AlexNet - 21.3 19.7 D Accuracy RGBD Accuracy - 35.3 19.1 27.1 20.1 23.0 38.1 27.7 39.0 (c) Classification on SUN RGB-D dataset. S2-GAN

15

SVM on top of it. We compare our method with the discriminator network in
DCGAN and the baselines reported in [57]: GIST [60] feature as well as Places-
AlexNet (AlexNet trained on Places dataset in a supervised manner) [54]. For
the networks trained with only RGB data, we follow [57,61,62], which directly
use them to extract feature on the depth representation. Then the features ex-
tracted from both RGB and Depth data are concatenated together as inputs
for SVM classiÔ¨Åer. Note that all models are not Ô¨Åne-tuned on the dataset. As
Fig. 11(c) shows, the representation learned with our method is much better
than DCGAN (8.2%) and 3.7% away from the Places-AlexNet.
Object Detection. In this task, we perform RGBD object detection on the
NYUv2 dataset. We follow the Fast-RCNN pipeline [56] and use the code and
parameter settings provided in [61]. In our case, we use surface normal to rep-
resent the depth. To apply our model for the detection task, we stacked two
fully connected layer (4096-d) on top of the last convolutional layer and Ô¨Åne-
tune the network end-to-end. Note that the convolutional layers are initialized
by our method but the fully connected layers are randomly initialized. We com-
pare against four baselines: network with the same architecture trained from
scratch, network pre-trained with DCGAN discriminator, DCGANv2 discrim-
inator, and ImageNet pre-trained AlexNet. For networks pre-trained on only
RGB data, we Ô¨Åne-tune them on both the RGB and surface normal inputs sep-
arately and average the detection results during testing as [61]. We apply Batch
Normalization [46] except for ImageNet pre-trained AlexNet.

We show the results in Table 2. Our approach has 1.5% improvement com-
pared to the model trained from scratch. The models initialized using DCGAN
and DCGANv2 do not give much boost. Compared to ImageNet pre-trained
model, we are around 5% away. However, it is worth mentioning that the result
of ImageNet pre-trained network uses an ensemble of two networks including the
RGB model with 27.8% mAP and surface normal model with 24.5% mAP.
6 Conclusion
In this paper, we factorize the image generation process and propose a novel Style
and Structure GAN. We show our model is more interpretable and generates
more realistic images compared to the baselines. We also show that our method
can be applied to learn RGBD representations in an unsupervised manner.
Acknowledgement: This work was supported by ONR MURI N000141010934, ONR
MURI N000141612007 and gift from Google. The authors would like to thank Yahoo!
and Nvidia for the compute cluster and GPU donations respectively. The authors would
also like to thank David Fouhey for many helpful discussions.

16

Xiaolong Wang and Abhinav Gupta

References

1. Doersch, C., Gupta, A., Efros, A.A.: Context as supervisory signal: Discovering

objects with predictable context. In: ECCV. (2014)

2. Doersch, C., Gupta, A., Efros, A.A.: Unsupervised visual representation learning

by context prediction. In: ICCV. (2015)

3. Wang, X., Gupta, A.: Unsupervised learning of visual representations using videos.

In: ICCV. (2015)

4. Goroshin, R., Bruna, J., Tompson, J., Eigen, D., LeCun, Y.: Unsupervised learning

of spatiotemporally coherent metrics. ICCV (2015)

5. Zou, W.Y., Zhu, S., Ng, A.Y., Yu, K.: Deep learning of invariant features via

simulated Ô¨Åxations in video. In: NIPS. (2012)

6. Li, Y., Paluri, M., Rehg, J.M., Dollar, P.: Unsupervised learning of edges.

In:

CVPR. (2016)

7. Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S.,

Courville, A., Bengio, Y.: Generative adversarial nets. In: NIPS. (2014)

8. Kingma, D., Welling, M.: Auto-encoding variational bayes. In: ICLR. (2014)
9. Gregor, K., Danihelka, I., Graves, A., Rezende, D.J., Wierstra, D.: Draw: A recur-

rent neural network for image generation. CoRR abs/1502.04623 (2015)

10. Li, Y., Swersky, K., Zemel, R.: Generative moment matching networks. In: ICML.

(2014)

11. Radford, A., Metz, L., Chintala, S.: Unsupervised representation learning with
deep convolutional generative adversarial networks. CoRR abs/1511.06434
(2015)

12. Silberman, N., Hoiem, D., Kohli, P., Fergus, R.: Indoor segmentation and support

inference from RGBD images. In: ECCV. (2012)

13. Agrawal, P., Carreira, J., Malik, J.: Learning to see by moving. In: ICCV. (2015)
14. Jayaraman, D., Grauman, K.: Learning image representations tied to ego-motion.

In: ICCV. (2015)

15. Walker, J., Gupta, A., Hebert, M.: Dense optical Ô¨Çow prediction from a static

image. In: ICCV. (2015)

16. Efros, A.A., Leung, T.K.: Texture synthesis by non-parametric sampling.

In:

ICCV. (1999)

17. Freeman, W.T., Jones, T.R., Pasztor, E.C.: Example-based super-resolution. In:

Computer Graphics and Applications. (2002)

18. Bengio, Y., Lamblin, P., Popovici, D., Larochelle, H.: Greedy layer-wise training

of deep networks. In: NIPS. (2007)

19. Le, Q.V., Ranzato, M.A., Monga, R., Devin, M., Chen, K., Corrado, G.S., Dean,
J., Ng, A.Y.: Building high-level features using large scale unsupervised learning.
In: ICML. (2012)

20. Ranzato, M.A., Krizhevsky, A., Hinton, G.E.: Factored 3-way restricted boltzmann

machines for modeling natural images. In: AISTATS. (2010)

21. Osindero, S., Hinton, G.E.: Modeling image patches with a directed hierarchy of

markov random Ô¨Åelds. In: NIPS. (2008)

22. Hinton, G.E., Salakhutdinov, R.R.: Reducing the dimensionality of data with

neural networks. Science 313 (2006) 504‚Äì507

23. Lee, H., Grosse, R., Ranganath, R., Ng, A.Y.: Convolutional deep belief networks
for scalable unsupervised learning of hierarchical representations. In: ICML. (2009)
24. Taylor, G.W., Hinton, G.E., Roweis, S.: Modeling human motion using binary

latent variables. In: NIPS. (2006)

S2-GAN

17

25. Mansimov, E., Parisotto, E., Ba, J.L., Salakhutdinov, R.: Generating images from

captions with attention. CoRR abs/1511.02793 (2015)

26. Kulkarni, T.D., Whitney, W.F., Kohli, P., Tenenbaum, J.B.: Deep convolutional

inverse graphics network. In: NIPS. (2015)

27. Dosovitskiy, A., Springenberg, J.T., Brox, T.: Learning to generate chairs with

convolutional neural networks. In: CVPR. (2015)

28. Tatarchenko, M., Dosovitskiy, A., Brox, T.: Single-view to multi-view: Recon-
structing unseen views with a convolutional network. CoRR abs/1511.06702
(2015)

29. Theis, L., Bethge, M.: Generative image modeling using spatial lstms. CoRR

abs/1506.03478 (2015)

30. Oord, A.V.D., Kalchbrenner, N., Kavukcuoglu, K.: Pixel recurrent neural networks.

CoRR abs/1601.06759 (2016)

31. Denton, E., Chintala, S., Szlam, A., Fergus, R.: Deep generative image models

using a laplacian pyramid of adversarial networks. In: NIPS. (2015)

32. Mirza, M., Osindero, S.:

Conditional generative adversarial nets.

CoRR

abs/1411.1784 (2014)

33. Mathieu, M., Couprie, C., LeCun, Y.: Deep multi-scale video prediction beyond

mean square error. CoRR abs/1511.05440 (2015)

34. Im, D.J., Kim, C.D., Jiang, H., Memisevic, R.: Generating images with recurrent

adversarial networks. CoRR abs/1602.05110 (2016)

35. Dosovitskiy, A., Brox, T.: Generating images with perceptual similarity metrics

based on deep networks. CoRR abs/1602.02644 (2016)

36. Makhzani, A., Shlens, J., Jaitly, N., Goodfellow, I.J.: Adversarial autoencoders.

CoRR abs/1511.05644 (2015)

37. Larsen, A.B.L., S√∏nderby, S.K., Winther, O.: Autoencoding beyond pixels using a

learned similarity metric. CoRR abs/1512.09300 (2015)

38. Barrow, H.G., Tenenbaum, J.M.: Recovering intrinsic scene characteristics from

images. In: Computer Vision Systems. (1978)

39. Tenenbaum, J.B., Freeman, W.T.: Separating style and content with bilinear mod-

els. In: Neural Computation. (2000)

40. Fouhey, D.F., Hussain, W., Gupta, A., Hebert, M.: Single image 3d without a

single 3d image. In: ICCV. (2015)

41. Zhu, S.C., Wu, Y.N., Mumford, D.: Filters, random Ô¨Åelds and maximum entropy

(frame): Towards a uniÔ¨Åed theory for texture modeling. In: IJCV. (1998)

42. Wang, X., Fouhey, D.F., Gupta, A.: Designing deep networks for surface normal

estimation. In: CVPR. (2015)

43. Eigen, D., Fergus, R.: Predicting depth, surface normals and semantic labels with

a common multi-scale convolutional architecture. In: ICCV. (2015)

44. Fouhey, D.F., Gupta, A., Hebert, M.: Data-driven 3D primitives for single image

understanding. In: ICCV. (2013)

45. Ladick¬¥y, L., Zeisl, B., Pollefeys, M.: Discriminatively trained dense surface normal

estimation. In: ECCV. (2014)

46. IoÔ¨Äe, S., Szegedy, C.: Batch normalization: Accelerating deep network training by

reducing internal covariate shift. CoRR abs/1502.03167 (2015)

47. Maas, A.L., Hannun, A.Y., Ng, A.Y.: RectiÔ¨Åer nonlinearities improve neural net-

work acoustic models. In: ICML. (2013)

48. Xu, B., Wang, N., Chen, T., Li, M.: Empirical evaluation of rectiÔ¨Åed activations

in convolutional network. CoRR abs/1505.00853 (2015)

49. Long, J., Shelhamer, E., Darrell, T.: Fully convolutional networks for semantic

segmentation. In: CVPR. (2015)

18

Xiaolong Wang and Abhinav Gupta

50. Ladick¬¥y, L., Shi, J., Pollefeys, M.: Pulling things out of perspective.

In: cvpr.

(2014)

51. Krizhevsky, A., Sutskever, I., Hinton, G.E.:

Imagenet classiÔ¨Åcation with deep

convolutional neural networks. In: NIPS. (2012)

52. Kingma, D., Ba, J.: Adam: A method for stochastic optimization. CoRR

abs/1412.6980 (2014)

53. Guo, R., Hoiem, D.: Support surface prediction in indoor scenes. In: ICCV. (2013)
54. Zhou, B., Lapedriza, A., Xiao, J., Torralba, A., Oliva, A.: Learning deep features

for scene recognition using places database. In: NIPS. (2014)

55. Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z.,
Karpathy, A., Khosla, A., Bernstein, M., Berg, A.C., Fei-Fei, L.: ImageNet Large
Scale Visual Recognition Challenge. IJCV 115(3) (2015) 211‚Äì252

56. Girshick, R.: Fast r-cnn. In: ICCV. (2015)
57. Song, S., Lichtenberg, S., Xiao, J.: Sun rgb-d: A rgb-d scene understanding bench-

mark suite. In: CVPR. (2015)

58. Janoch, A., Karayev, S., Jia, Y., Barron, J., Fritz, M., Saenko, K., Darrell, T.: A
category-level 3-d object dataset: Putting the kinect to work. In: Workshop on
Consumer Depth Cameras in Computer Vision (with ICCV). (2011)

59. Xiao, J., Owens, A., Torralba, A.: Sun3d: A database of big spaces reconstructed

using sfm and object labels. In: ICCV. (2013)

60. Oliva, A., Torralba, A.: Modeling the shape of the scene: A holistic representation

of the spatial envelope. IJCV (2011)

61. Gupta, S., HoÔ¨Äman, J., Malik, J.: Cross modal distillation for supervision transfer.

In: CVPR. (2016)

62. Gupta, S., Girshick, R., Arbelez, P., Malik, J.: Learning rich features from rgb-d

images for object detection and segmentation. In: ECCV. (2014)

7 Supplementary Material: Generated Normals and

Images from S2-GAN

S2-GAN

19

OutputNormalsOutputImagesOutputNormalsOutputImagesOutputNormalsOutputImages20

Xiaolong Wang and Abhinav Gupta

OutputNormalsOutputImagesOutputNormalsOutputImagesOutputNormalsOutputImagesS2-GAN

21

OutputNormalsOutputImagesOutputNormalsOutputImagesOutputNormalsOutputImages22

Xiaolong Wang and Abhinav Gupta

OutputNormalsOutputImagesOutputNormalsOutputImagesOutputNormalsOutputImages