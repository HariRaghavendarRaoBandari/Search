6
1
0
2

 
r
a

 

M
6
1

 
 
]

G
L
.
s
c
[
 
 

1
v
4
5
9
4
0

.

3
0
6
1
:
v
i
X
r
a

Online Optimization in Dynamic Environments: Improved Regret Rates

for Strongly Convex Problems

Aryan Mokhtari, Shahin Shahrampour, Ali Jadbabaie, and Alejandro Ribeiro

Abstract— In this paper, we address tracking of a time-
varying parameter with unknown dynamics. We formalize the
problem as an instance of online optimization in a dynamic
setting. Using online gradient descent, we propose a method
that sequentially predicts the value of the parameter and in turn
suffers a loss. The objective is to minimize the accumulation of
losses over the time horizon, a notion that is termed dynamic
regret. While existing methods focus on convex loss functions,
we consider strongly convex functions so as to provide better
guarantees of performance. We derive a regret bound that
captures the path-length of the time-varying parameter, deﬁned
in terms of the distance between its consecutive values. In other
words, the bound represents the natural connection of tracking
quality to the rate of change of the parameter. We provide
numerical experiments to complement our theoretical ﬁndings.

I. INTRODUCTION

Convex programming is a mature discipline that has been a
subject of interest among scientists for several decades [1]–
[3]. The central problem of convex programming involves
minimization of a convex cost function over a convex fea-
sible set. Traditional optimization has focused on the case
that the cost function is time-invariant. However, in wide
range of applications, the cost function i) varies over time,
and ii) there is no prior information about the dynamics
of the cost function. It is therefore important to develop
online convex optimization techniques, which are adapted
for non-stationary environments. The problem is ubiquitous
in various domains such as machine learning, control theory,
industrial engineering, and operations research.

Online optimization (learning) has been extensively stud-
ied in the literature of machine learning [4], [5], proving
to be a powerful tool to model sequential decisions. The
problem can be viewed as a game between a learner and
an adversary. The learner (algorithm) sequentially selects
actions, and the adversary reveals the corresponding convex
losses to the learner. The term online captures the fact
that the learner receives a streaming data sequence, and it
processes that adaptively. The popular performance metric
for online algorithms is called regret. Regret often measures
the performance of algorithm versus a static benchmark [4]–
[7]. For instance, the benchmark could be the optimal point
of the temporal average of losses, had the learner known all

Work in this paper is supported by ARO W911NF-10-1-0388, NSF
CAREER CCF-0952867, and ONR N00014-12-1-0997. Aryan Mokhtari,
Alejandro Ribeiro, and Ali
Jadbabaie are with the Department of
Electrical and Systems Engineering at
the University of Pennsyl-
vania, Philadelphia, PA 19104 USA.
(email: aryanm, aribeiro,
jadbabai@seas.upenn.edu). Shahin Shahrampour is with the De-
partment of Electrical Engineering at Harvard University, Cambridge, MA
02138 USA. (e-mail: shahin@seas.harvard.edu).

the losses in advance. In a broad sense, when the benchmark
is a ﬁxed sequence, the regret is called static. Furthermore,
improved regret bounds are derived for the case that the
losses are strongly convex [8].

Recent works on online learning have investigated an im-
portant direction, which involves the notion of dynamic regret
[6], [9]–[11]. The dynamic regret can be recognized in the
form of the cumulative difference between the instantaneous
loss and the minimum loss. Previous works on dynamic
setting investigated convex loss functions. Motivated by the
fact that in the static setting curvature gives advantage to the
algorithm [8], we aim to demonstrate that strong convexity
of losses yields an improved rate in dynamic setting.

Therefore, we consider the online optimization problem
with strongly convex losses in dynamic setting, where the
benchmark sequence varies without following any particular
dynamics. We track the sequence using online gradient
descent and prove that the dynamic regret can be bounded in
terms of the path-length of the sequence. The path-length is
deﬁned in terms of the distance between consecutive values
of the sequence. Interestingly, our result exhibits a smooth
interpolation between the static and dynamic setting. In other
words, the bound directly connects the tracking quality to the
rate of change of the sequence. We further provide numerical
experiments that verify our theoretical result.

Of particular relevance to our setup is Kalman ﬁltering
[12]. In the original Kalman ﬁlter, there are strong assump-
tions on the dynamical model, such as linear state-space and
Gaussian noise model. However, we depart from the classical
setting by assuming no particular dynamics for the states, and
observing them only through the gradients of loss functions.
Instead, we provide a worst-case guarantee which captures
the trajectory of the states.

We remark that the notion of dynamic regret is also related
to adaptive, shifting, and tracking regret (see e.g. [13]–[17])
in the sense of including dynamics in the problem. However,
each case represents a different notion of regret.

II. PROBLEM FORMULATION

We consider an online optimization problem where at each
step t, a learner selects an action xt ∈ X and an adversary
chooses a loss function ft : X → R. The loss associated with
the action xt and function ft is given by ft(xt). Once the
action is chosen, the algorithm (learner) receives the gradient
∇ft(xt) of the loss at point xt.
In static online learning, we measure the performance of
the algorithm with respect to a ﬁxed reference x ∈ X in
hindsight. In other words, we aim to minimize a static regret

TABLE I: Summary of related works on dynamic online learning

[6]

[10]

Reference

Regret notion

(cid:80)T
(cid:80)T
t=1 ft(xt) − ft(ut)
(cid:80)T
t=1 ft(xt) − ft(ut)
(cid:80)T
(cid:80)T
This work (cid:80)T
t=1 ft(xt) − ft(x∗
t )
t=1 ft(xt) − ft(x∗
t )

E [ft(xt)] − ft(x∗
t )
E [ft(xt)] − ft(x∗
t )

[11]

[9]

[9]

t=1

t=1

(cid:17)
(cid:17)

T (1 + CT (u1, . . . , uT ))
T (1 + C(cid:48)

T (u1, . . . , uT ))

Convex

Convex

Regret rate

Loss function

O(cid:16)√
O(cid:16)√
O(cid:16)
T 2/3(1 + VT )1/3(cid:17)
Strongly convex O(cid:16)(cid:112)T (1 + VT )
(cid:17)
O(cid:16)√
(cid:17)
Strongly convex O(cid:16)

DT + 1 + min

Convex

Convex

1 + CT

(cid:110)(cid:112)(DT + 1)CT , [(DT + 1)VT T ]1/3(cid:111)(cid:17)

The second measure of interest in dynamic settings is the

variation in gradients DT which is measured by

ft(x),

(1)

DT :=

(cid:107)∇ft(xt) − Mt(cid:107)2 ,

(4)

T(cid:88)

t=1

where Mt is a causally predictable sequence available to the
algorithm prior to time t [18], [19]. A simple choice is to
select the previous gradient Mt = ∇ft−1(xt−1) [20], but Mt
represents any predicted value for the next objective function
gradient ∇ft.

The third common measure to capture dynamics is the
variation in the sequence of reference points u1, . . . , uT .
This variation is deﬁned as the accumulation of the norm
of the difference between subsequent reference points

CT (u1, . . . , uT ) :=

(cid:107)ut − ut−1(cid:107),

(5)

T(cid:88)

t=2

T(cid:88)

t , we drop the arguments in CT (x∗

The measure in (5) accumulates variations between two arbi-
trary consecutive reference points ut and ut−1. Whenever the
reference points are the optimal points in (2), i.e., whenever
ut = x∗
T ) and
simply use CT to represent such variation. The variation CT
captures the difference between the optimal arguments of the
two consecutive losses ft and ft−1 over time. Another notion
for the variation in reference points is deﬁned as

1, . . . , x∗

C(cid:48)
T (u1, . . . , uT ) :=

(cid:107)ut − Φt(ut−1)(cid:107).

(6)

t=2

where Φt(ut−1) is the predicted reference point for step t
evaluated at step t − 1 by the learner [10]. If the prediction
is Φt(ut−1) = ut−1 we recover the measure in (5). In
general, the variation C(cid:48)
T (u1, . . . , uT ) presents the variation
of reference points with respects to a given dynamic Φt(·).
The measures in (3)-(6) are different but largely com-
patible. They differ in that the comparisons are between
functions in (3), gradients in (4) and a sequence of given
– interesting is some sense, e.g., optimal – arguments in
(5) and (6), but all of them yield qualitatively comparable
verdicts. The comparisons in (4) and (6) further allow for
the incorporation of a prediction if a model for the evolution
of dynamics over time is available. The variation measures
in (3)-(6) have been used to bound regret in different settings
with results that we summarize in Table I. The work in

deﬁned as

T(cid:88)

t=1

ft(xt) − T(cid:88)

t=1

Regs

T (x) :=

argminx(cid:48)∈X(cid:80)T
gate loss(cid:80)T

and a particularly interesting value for x is x∗

:=
t=1 ft(x(cid:48)), i.e., the minimizer of the aggre-
t=1 ft. A successful algorithm generates a set of
actions {xt}T
t=1 that yields to a sub-linear regret. Though
appealing in various applications, static regret does not
always serve as a comprehensive performance metric. For
instance, static regret can be used in the context of static
parameter estimation. However, when the parameter varies
over time, we need to bring forward a new notion of regret.
In this work, we are interested to evaluate the algo-
rithm with respect to a more stringent benchmark, which
is the sequence of instantaneous minimizers. In particular,
let x∗
t := argminx∈X ft(x) be the minimizer of the loss ft
associated with time t. Then, dynamic regret is deﬁned as

Regd

T (x∗

1, . . . , x∗

T ) :=

ft(x∗
t ).

(2)

t=1

t=1

The dynamic regret in (2) captures how well the action xt
matches the optimal action x∗
t for each time t. It is well-
known that in the worst-case, it is not possible to achieve a
sub-linear dynamic regret, because drastic ﬂuctuations in the
minimum points can make the problem intractable. In this
work, we would like to present a regret bound that maps
the hardness of problem to variation intensity. We introduce
a few complexity measures relevant to this context in the
following section.

A. Measures of variation and bounds on dynamic regret

There are three common complexity measures to capture
variations in the choices of the adversary. The ﬁrst measure
is the variation in losses VT which is characterized by

T(cid:88)

ft(xt) − T(cid:88)

T(cid:88)

t=1

VT :=

|ft(x) − ft−1(x)| .

sup
x∈X

(3)

The variation in losses VT accumulates the maximum varia-
tion between the two consecutive functions ft and ft−1 for
any feasible point x ∈ X .

√
[6] uses online gradient descent (OGD) with a diminishing
steprsize to establish a regret of order O(
T (1 + CT ))
when the losses are convex. In [10], the authors study the
√
performance of mirror descent in the dynamic setting and
establish a regret bound of order O(
T )) when
the environment follows a adynamical model Φt(·) and the
loss functions are convex. The work in [9] evaluates the
performance of OGD for the case when a noisy estimate
of the gradient is available and an upper bound on VT is
assumed as prior knowledge. They establish regret bounds
of order O(T 2/3(1 + VT )1/3) for convex loss functions and

of order O((cid:112)T (1 + VT )) for strongly convex loss functions.

T (1 + C(cid:48)

In [11], using optimistic mirror descent, the authors propose
an adaptive algorithm which achieves a regret bound in terms
of CT , DT , and VT simultaneously, while they assume that
the learner receives each variation measure online.
Motivated by the fact that in static regret problems strong
convexity results in better regret bounds – order O(log T )
√
instead of order O(
T ), [8] – we study dynamic regret prob-
lems under strong convexity assumptions. Our contribution is
to show that the regret associated with the OGD algorithm
(deﬁned in Section III, analyzed in Section IV) grows not
faster than 1 + CT ,
Regd

√
This result improves the regret bound O(
T (1 + CT )) for
OGD when the functions ft are convex but not necessarily
strongly convex [6]. We remark that our algorithm assumes
neither a prior knowledge nor an online feedback about CT
and the only available information for the learner is the loss
function gradient ∇ft(xt).

T ) ≤ O (1 + CT ) .

1, . . . , x∗

T (x∗

(7)

III. ONLINE GRADIENT DESCENT

Consider the online learning problem for T iterations.
At the beginning of each iteration t, the learner chooses
the action xt ∈ X where X is a given convex set. Then,
the adversary chooses a function ft and evaluates the loss
associated with the iterate xt which is given by the difference
ft(xt) − ft(x∗
t is the minimizer of the function
ft over the set X . The learner does not receive the loss
ft(xt) − ft(x∗
t ) associated with the action xt. Rather, she
receives the gradient ∇ft(xt) of the cost function ft com-
puted at xt. After receiving the gradient ∇ft(xt), she uses
this information to update the current iterate xt.

t ) where x∗

We consider a setting in which the learner uses the online
gradient descent (OGD) method with a constant stepsize to
update the iterate xt using the released instantaneous gradient
∇ft(xt). To be more precise, consider xt as the sequence
of actions that the learner chooses and deﬁne ˆxt ∈ X as a
sequence of auxiliary iterates. At each iteration t, given the
iterate xt and the instantaneous gradient ∇ft(xt), the learner
computes the auxiliary variable ˆxt as

(cid:18)

(cid:19)

ˆxt = ΠX

xt − 1
γ

∇ft(xt)

,

(8)

where γ is a positive constant and ΠX denotes the pro-
jection onto the nearest point in the set X , i.e, ΠX (y) =

Algorithm 1 Online Gradient Descent
Require: Initial vector x1 ∈ X , constants h and γ.
1: for t = 1, 2, . . . , T do
2:
3:

Play xt
Observe the gradient of the current action ∇ft(xt)
Compute the auxiliary var.:
Compute the next action:

ˆxt = ΠX
xt+1 = xt + h(ˆxt − xt)

xt − 1
γ

(cid:18)

4:

∇ft(xt)

(cid:19)

5:
6: end for

xt+1 = xt + h(ˆxt − xt),

argminx∈X (cid:107)x − y(cid:107). Then, the action xt+1 is evaluated as
(9)
where h is chosen from the interval (0, 1]. The online
gradient descent method is summarized in Algorithm 1.
The updated action xt+1 in (9) can be written as xt+1 =
(1 − h)xt + hˆxt. Therefore, we can reinterpret the updated
action xt+1 as a weighted average of the previous iterate xt
and the auxiliary variable ˆxt which is evaluated using the
gradient of the function ft. It is worth mentioning that for a
small choice of h ≈ 0, xt+1 is close to the previous action
xt, while the previous action xt has less impact on xt+1
when h is close to 1.

The auxiliary variable ˆxt is the result of applying projected
gradient descent on the current iterate xt as shown in (8).
This update can be interpreted as minimizing a ﬁrst-order
approximation of the cost function ft added to a proximal
term (γ/2)(cid:107)x−xt(cid:107)2 as we show in the following proposition.
Proposition 1 Consider the update in (8). Given the iterate
xt, the instantaneous gradient ∇ft(xt), and the positive
constant γ, the optimal argument of the optimization problem

(cid:110)∇ft(xt)T (x − xt) +

(cid:107)x − xt(cid:107)2(cid:111)

,

(10)

˜xt = argmin

x∈X

γ
2

is equal to the iterate ˆxt generated by (8).
Proof: See Section VII-A.

The result in Proposition 1 shows that the updates in (8)
and (10) are equivalent. In the implementation of OGD we
use the update in (8), since the computational complexity
of the update in (8) is lower than the complexity of the
minimization in (10). On the other hand, the update in (10)
is useful in the regret analysis of OGD that we undertake in
the following section.

IV. REGRET ANALYSIS
to

show that

the

T (x∗

dynamic

T ) = Regd

proceed
1, . . . , x∗

regret
We
Regd
T deﬁned in (2) associated
with the actions xt generated by the online gradient
descent algorithm in (9) has an upper bound on the order
of the variation in the sequence of optimal arguments
t−1(cid:107). In proving this result, we assume

CT =(cid:80)T

t − x∗

t=2 (cid:107)x∗

the following conditions are satisﬁed.

Assumption 1 The functions ft are strongly convex over the
convex set X with constant µ > 0, i.e.,
ft(x) ≥ ft(y) + ∇ft(y)T (x − y) +

(cid:107)x − y(cid:107)2,

(11)

µ
2

for any x, y ∈ X and 1 ≤ t ≤ T .
Assumption 2 The gradients ∇ft are Lipschitz continuous
over the set X with constant L < ∞, i.e.,

(cid:107)∇ft(y) − ∇ft(x)(cid:107) ≤ L(cid:107)x − y(cid:107),

(12)

for any x, y ∈ X and 1 ≤ t ≤ T .
Assumption 3 The gradient norm (cid:107)∇ft(cid:107) is bounded above
by a positive constant G or equivalently

sup

x∈X ,1≤t≤T

(cid:107)∇ft(x)(cid:107) ≤ G.

(13)

According to Assumption 1, the instantaneous functions ft
are strongly convex over the convex set X which implies that
there exists a unique minimizer x∗
t for the function ft over
the convex set X . The Lipschitz continuity of the gradients
∇ft in Assumption 2 is customary in the analysis of descent
methods. Notice that we only assume for a ﬁxed function
ft the gradients are Lipschitz continuous and we do not
assume any conditions on the difference of two gradients
associated with two different instantaneous functions. To be
more precise, there is no condition on the norm (cid:107)∇ft(y) −
∇ft(cid:48)(x)(cid:107) where t (cid:54)= t(cid:48). The bound on the gradients norm in
Assumption 3 is typical in the analysis of online algorithms
for constrained optimization.

Our main result on the regret bound of OGD in dynamic
settings is derived from the following proposition that bounds
t(cid:107) in terms of the distance (cid:107)xt−x∗
t(cid:107).
the difference (cid:107)xt+1−x∗
Proposition 2 Consider the online gradient descent method
(OGD) deﬁned by (8) and (9) or the equivalent (10). Recall
the deﬁnition of x∗
t as the unique minimizer of the function
ft over the convex set X . If Assumptions 1 and 2 hold and
the stepsize parameter γ in (8) is chosen such that γ ≥ L,
then the sequence of actions xt generated by OGD satisﬁes
(14)
where 0 ≤ ρ := (1 − hµ/γ)1/2 < 1 is a non-negative
constant strictly smaller than 1.

t(cid:107) ≤ ρ (cid:107)xt − x∗
t(cid:107),

(cid:107)xt+1 − x∗

Proof: See Section VII-B.

t

The result in Proposition 2 shows that the distance between
the action xt+1 and the optimal argument x∗
is strictly
smaller than the difference between the previous action xt
and the optimal argument x∗
t at step t. The inequality in (14)
implies that if the optimal arguments of the functions ft and
ft+1 which are x∗
t and x∗
t+1, respectively, are not far away
from each other the iterates xt can track the optimal solution
sequence x∗
t . Notice that if in the left hand side of (14)
instead of x∗
t we had the optimal argument x∗
t+1 at step t+1,
then we could show that the sequence of actions xt generated
by OGD asymptotically converges to the sequence of optimal
arguments x∗
t . Thus, the performance of OGD depends on
the rate that the sequence of optimal arguments changes. This
conclusion is formalized in the following Theorem.

Theorem 1 Consider the online gradient descent method
(OGD) deﬁned by (8) and (9) or the equivalent (10). Suppose
that the constant h is chosen from the interval (0, 1] and
the constant γ satisﬁes the condition γ ≥ L, where L is the
gradients Lipschitz continuity constant. If Assumptions 1 and
2 hold, then the sequence of actions xt generated by OGD
satisﬁes

(cid:107)xt − x∗

t(cid:107) ≤ K1

(cid:107)x∗

t − x∗

t−1(cid:107) + K2,

(15)

t=1

t=2

where the constants K1 and K2 are explicitly given by

T(cid:88)

T(cid:88)

, K2 :=

1

(1 − ρ)

. (16)

K1 :=

(cid:107)x1 − x∗

1(cid:107) − ρ(cid:107)xT − x∗
T(cid:107)
(1 − ρ)
Proof: See Section VII-C.

t −x∗

t=1 (cid:107)xt−x∗

gate variable error(cid:80)T
variation in the optimal arguments CT =(cid:80)T

From Theorem 1, we obtain an upper bound for the aggre-
t(cid:107) in terms of the aggregate
t=2 (cid:107)x∗
t−1(cid:107).
This result matches the intuition that for the scenarios that the
t}T
sequence of the optimal arguments {x∗
t=1 is not varying
fast, the sequence of actions generated by the online gradient
descent method can achieve a sublinear regret bound. In
particular, when the optimal arguments are all equal to each
the aggregate error
other,
t(cid:107) is bounded above by the constant K2
which is independent of T . We use the bounded gradients
assumption in (13) to translate the result in (15) into an upper
bound for the dynamic regret Regd

(cid:80)T
t=1 (cid:107)xt − x∗

1 = ··· = x∗
T ,

i.e., when x∗

T deﬁned in (2).

Corollary 1 Adopt the same deﬁnitions and hypothesis of
Theorem 1 and furhter assume that
the gradient norms
(cid:107)∇ft(x)(cid:107) are upper bounded by the constant G for all
x ∈ X as in Assumption 3. Then, the dynamic regret Regd
T
for the sequence of actions xt generated by OGD is bounded
above by

Regd

T ≤ GK1

(cid:107)x∗

t − x∗

t−1(cid:107) + GK2.

(17)

T(cid:88)

t=2

Proof: Notice that for any vectors x and y we know that there
exists a vector z such that ft(x)− ft(y) = ∇ft(z)T (x− y)
where z is a point from the set {v | v = αx+(1−α)y, 0 ≤
α ≤ 1}. Thus, the objective function difference |ft(x) −
ft(y)| is bounded above by G(cid:107)x − y(cid:107). Setting x = xt and
y = x∗

t implies that

ft(xt) − ft(x∗

t ) ≤ G(cid:107)xt − x∗
t(cid:107).

(18)

t ) ≤ G(cid:80)T

By summing up (18) for all times t, we conclude that the
t=1 ft(xt)−
dynamic regret is bounded above as Regd
t(cid:107). This observation combined
ft(x∗
with the result in (15) yields (17).

t=1 (cid:107)xt − x∗

The result in Corollary 1 states that under the conditions
that the functions ft are strongly convex and their gradients
are bounded and Lipschitz continuous, the dynamic regret
Regd
T associated with the online gradient descent method
satisﬁes the order bound that we previewed in (7). As already

T =(cid:80)T

√
mentioned, this bound improves the OGD rate O(
T (1 +
CT )) when the functions ft are convex but not ncessarily
strongly convex and the stepsize is diminishing [6].

Some interesting conclusions can be derived if we consider

speciﬁc rates of variability:
Constant functions.
If the functions are constant, i.e., if
ft = f for all times, we have CT = 0 and it follows that the
regret grows at a rate O(1). This means that xt converges to
x∗ and we recover a convergence proof for gradient descent.
Linearly decreasing variability. If the difference between
consecutive arguments decreases as 1/t, we have that CT =
O(log T ) and that the regret then grows at a logarithmic
rate as well. Since this implies that the normalized regret
T )/T ≤ O(log T /T )
1, . . . , x∗
grows not faster than Regd
we must have that xt converges to x∗
t .
Decreasing variability. If the variability decreases as 1/tα
with α ∈ (0, 1), the regret is of order O(1 + CT ) = O(1 +
T 1−α). As before, this must imply that xt converges to x∗
t .
Constant variability. If the variability between functions
t−1(cid:107) ≤ C for all T , the regret is
stays constant, say (cid:107)u∗
of order O(1 + CT ) = O(1 + CT ). The normalized regret
T )/T ≤ O(C). This means
is then of order Regd
that we have a steady state tracking error, where the tracking
error depends on how different adjacent functions are.

t − u∗
T (x∗

1, . . . , x∗

T (x∗

V. NUMERICAL EXPERIMENTS

We numerically study the performance of OGD in solving
a sequence of quadratic programming problems. Consider
the decision variable x = [x1; x2] ∈ R2 and the quadratic
function ft at time t which is deﬁned as
ft(x) = ft(x1, x2) = ρ(cid:107)x1 − at(cid:107)2 + (cid:107)x2 − bt(cid:107)2 + ct, (19)
where at, bt, and ct are time-variant scalars and ρ > 0 is
a positive constant. The coefﬁcient ρ controls the condition
number of the objective function ft. In particular for ρ > 1,
the problem condition number is equal to ρ. The convex
set X is deﬁned as x2
2 = r2 which is the circle with
center [0; 0] and radius r. The radius r is chosen such that
the optimal argument of the function ft over R2, which is
[at, bt], is not included in the set X . This way we ensure that
the constraint x ∈ X is active at the optimal solution.

1 + x2

In our experiments we pick ρ = 100 to have a quadratic
optimization problem with large condition number 100. Note
that if we choose ρ = 1, the condition number of the function
ft is 1 and OGD can minimize the cost ft in a couple
of iterations. The constant γ in OGD is set as γ = 2ρ in
all experiments, since the Lipschitz continuity constant of
gradients is L = 2ρ. Moreover, the OGD parameter h is set
as h = 1 which implies xt+1 = ˆxt.
we deﬁne ft(xt) − ft(x∗
function error at

To characterize the instantaneous performance of OGD
t ) as the instantaneous objective
:=
s) as the dynamic regret up to step t.
s−1(cid:107) as the total

(cid:80)t
Likewise, we deﬁne Ct :=(cid:80)t
s=1 fs(xs) − fs(x∗

s=2 (cid:107)x∗
optimal argument variation until step t.

time t. Further, we deﬁne Regd
t

s − x∗

We consider two different cases to study the regret bound
of OGD in dynamic online settings. First, we consider a
switching problem that the adversary switches between two
quadratic functions after a speciﬁc number of iterations.
Then, we study the case that
the sequence of optimal
arguments x∗
t changes at each iteration, while the difference
(cid:107)x∗
A. Switching problem

t−1(cid:107) diminishes as time progresses.

t − x∗

1 + x2

s − x∗

= [−50; 0] and x(2)∗

Consider the case that the adversary chooses between two
functions where each of them is of the form of the quadratic
function ft in (19). In particular, consider the case that the
adversary chooses the parameters at, bt, and ct from the
two sets S (1) = {a, b, c} and S (2) = {a(cid:48), b(cid:48), c(cid:48)}. Therefore,
at each iteration the adversary chooses either f (1) = ρ(cid:107)x1 −
a(cid:107)2 +(cid:107)x2 − b(cid:107)2 + c or f (2) = ρ(cid:107)x1 − a(cid:48)(cid:107)2 +(cid:107)x2 − b(cid:48)(cid:107)2 + c(cid:48).
We run OGD for a ﬁxed number of iterations T = 100 and
assume that the adversary switches between the functions
f (1) and f (2) every τ iterations.
In our experiments we set a = −100, b = 0, c = 30,
a(cid:48) = 100, b(cid:48) = 20, and c(cid:48) = −50. The convex set X is
deﬁned as x2
2 = 502 which is the circle with center
[0; 0] and radius 50. Note that this circle does not contain
the points [a; b] = [−100; 0] and [a(cid:48); b(cid:48)] = [100; 20]. The
optimal argument of f (1) and f (2) over the convex set X
are x(1)∗
= [49.99; 0.19], respectively.
We set the initial iterate x0 = [0; 40] which is a feasible
point for the set X . We consider three different cases that
τ = 4, τ = 8, and τ = 16. The performance of OGD for the
(cid:80)t
three different choices of τ are illustrated in Figure 1.
Figure 1a demonstrates the variable variation Ct
s=2 (cid:107)x∗

:=
s−1(cid:107) over time t. For the case τ = 16, the
value of Ct increases every 16 iterations and the increment
is equal to the norm of the difference between the optimal
arguments of f (1) and f (2) which is (cid:107)x(1)∗ − x(2)∗(cid:107) = 100.
After T = 100 iterations, we observe 6 jumps which implies
that the total variable variation is CT = 600. Likewise, for
the cases that τ = 8 and τ = 4, we observe 12 and 24
jumps in their corresponding plots, and the aggregate variable
variations are CT = 1200 and CT = 2400, respectively.
Figure 1b showcases the instantaneous function error
ft(xt) − ft(x∗
t ) versus number of iterations t for τ = 16,
τ = 8, and τ = 4. In all of the cases, the sequence of errors
ft(xt) − ft(x∗
t ) converges linearly to 0 until the time the
adversary switches the objective function ft. By increasing
the number of times that the adversary switches between
the functions f (1) and f (2), the phase of linear convergence
becomes shorter and the algorithm restarts more often.

(cid:80)t
Thus, we expect
s=1 fs(xs) − fs(x∗

narios that τ is smaller. The dynamic regret Regd

to observe larger regret for the sce-
t =
s) versus the number of iterations t
is shown in Figure 1c for τ = 16, τ = 8, and τ = 4.
As we expect, for the case that τ = 4 the dynamic regret
Regd
T of
OGD for τ = 16, τ = 8, and τ = 4 after T = 100
T = 2.48 × 107,
iterations are Regd
and Regd

t grows faster. Note that the dynamic regret Regd

T = 1.28 × 107, Regd

T = 4.88 × 107, respectively.

(a)

minimizers every τ iterations. The variation in the sequence of optimizers Ct =(cid:80)t

Fig. 1: Performance of OGD for the case that the adversary switches between two quadratic functions with different
s=2 (cid:107)x∗
s−1(cid:107), instantaneous objective
function error ft(xt) − ft(x∗
s) are shown in Figures 1a, 1b, and 1c,
respectively. In the case that τ is small and the adversary switches more often between the two quadratic functions, the
variation Ct and the dynamic regret Regd

t grow faster. Moreover, the growth patterns of Ct and Regd

t ), and dynamic regret Regd

s=1 fs(xs) − fs(x∗

t =(cid:80)t

t are similar.

s − x∗

(b)

(c)

(a)

(b)

(c)

t over the set X . The variation in the sequence of optimizers Ct := (cid:80)t
t =(cid:80)t

Fig. 2: Performance of OGD for the case that the adversary chooses a sequence of quadratic functions ft where the sequence
of optimal arguments x∗
t is convergent. Figure 2a illustrates the paths for the function parameters [at; bt] and the optimal
s−1(cid:107) and the dynamic
arguments x∗
regret Regd
s) are shown in Figures 2b and 2c, respectively. The sequence of optimal arguments is
s−1(cid:107) is summable. Likewise, the

convergent in a way that the variation in the sequence of optimizers Ct :=(cid:80)t

s=1 fs(xs) − fs(x∗

s=2 (cid:107)x∗

s − x∗

s − x∗

s=2 (cid:107)x∗

dynamic regret Regd

T associated with the OGD method does not grow as the total number of iterations T increases.

the growth patterns of Ct and Regd

Comparing the variation Ct and the dynamic regret Regd
T
for the three cases τ = 16, τ = 8, and τ = 4 shows
that
T are similar.
This observation is consistent with the theoretical result in
Corollary 1 which indicates that the upper bound for the
dynamic regret Regd

T is of order O(1 + CT ).

B. Diminishing variations

In this section we consider the case that the adversary
picks a sequence of functions ft as in (19) such that the
sequence of optimizers x∗

t is convergent.

We use the parameters in Figure 1 except the total number
of iterations which is set as T = 250. We set the initial
values for [a1; b1] as [−60; 100]. Further, we assume that
bt is time-invariant and for all steps t we have bt = b1 =
100. On the other hand, we assume that the parameter at
changes as time passes and it satisﬁes the recursive formula

at+1 = at + 5(cid:112)1/t. The sequence of parameters [at, bt],

which are the optimal arguments of the function ft over
R2, is illustrated in Figure 2a. Moreover, the set of optimal
arguments of ft over the set X , which are indicated by x∗
t ,
are also demonstrated in Figure 2a. This plot shows that as

t and x∗

time progresses and the difference between the functions ft
and ft−1 becomes less signiﬁcant, the difference between
we demonstrate the variable variation Ct := (cid:80)t
the optimal arguments x∗
t−1 diminishes. To formally
study the variation in the sequence of optimal arguments x∗
t ,
s −
s=2 (cid:107)x∗
s−1(cid:107) in terms of number of iterations t in Figure 2b. As
x∗
we expect, the variation Ct converges as time progresses,
since the difference (cid:107)x∗
(cid:80)t
Figure 2c illustrates
s=1 fs(xs) − fs(x∗

t =
s) of OGD in terms of number of
iterations t. We observe that the dynamic regret of OGD
follows the pattern of the variation Ct and converges to a
constant value as time progresses. Thus, the dynamic regret
Regd
T does not grow with the number of iterations T . This
observation matches the theoretical result in Corollary 1 that
the dynamic regret Regd
T converges to a constant value when
s−1(cid:107)

the variation in optimal arguments CT :=(cid:80)T

t−1(cid:107) is diminishing.
the dynamic regret Regd

t − x∗

s=2 (cid:107)x∗

s−x∗

does not grow by the number of iterations T .

VI. CONCLUSIONS

This paper studies the performance of the online gradient
descent (OGD) algorithm in online dynamic settings. We

020406080100Timeindext05001000150020002500Pts=1kx∗s−x∗s−1kτ=16τ=8τ=4020406080100Timeindext10-51001051010ft(xt)−ft(x∗t)τ=16τ=8τ=4020406080100Timeindext00.511.522.533.544.55Pts=1fs(xs)−fs(x∗s)×107τ=16τ=8τ=4-60-40-200204060801000102030405060708090100sequenceof[at;bt]sequenceofminimizersx∗t050100150200250Timeindext020406080100120140160Pts=1kx∗s−x∗s−1k050100150200250Timeindext3.43.63.844.24.44.64.855.2Pts=1fs(xs)−fs(x∗s)×105t − x∗

t=2 (cid:107)x∗

t deﬁned by CT = (cid:80)T

established an upper bound for the dynamic regret of OGD in
terms of the variation in the sequence of optimal arguments
t−1(cid:107). We showed that
x∗
if the functions ft chosen by the adversary are strongly
convex, the online gradient descent method with a proper
constant stepsize has a regret of order O(1 + CT ). This
result indicates that the dynamic regret bound of OGD for
√
strongly convex functions is signiﬁcantly smaller than the
regret bound of order O(
T (1 + CT )) for convex settings.
Numerical experiments on a dynamic quadratic programming
veriﬁed our theoretical result that the dynamic regret of OGD
has an upper bound of order O(1 + CT ).

B. Proof of Proposition 2

(cid:107)x − xt(cid:107)2 ≥ ft(xt) + ∇ft(xt)T (x − xt),

Strong convexity of the function ft implies that
ft(x) − µ
(25)
2
for any x ∈ X . By adding and subtracting the inner product
∇ft(xt)T (ˆxt − xt) to the right hand side of (25) we obtain
ft(x) − µ
(26)
2
≥ ft(xt) + ∇ft(xt)T (ˆxt − xt) + ∇ft(xt)T (x − ˆxt).
Observe that the optimality condition of the update of ˆxt in
(10) implies that

(cid:107)x − xt(cid:107)2

VII. APPENDIX

(cid:20)

A. Proof of Proposition 1

Let’s deﬁne ˜xt as the minimizer of the program ˜xt :=

as deﬁned in (8). Our goal is to show that ˆxt = ˜xt. Based
on the deﬁnition of the update in (8) we can write

argminx∈X(cid:8)∇ft(xt)T (x − xt) + (γ/2)(cid:107)x − xt(cid:107)2(cid:9) and ˆxt
(cid:21)(cid:13)(cid:13)(cid:13)(cid:13) ,
(cid:13)(cid:13)(cid:13)(cid:13)ˆxt−

xt − 1
γ
(20)
for any y ∈ X . Now assume that ˆxt
(cid:54)= ˜xt. Therefore,
since the set X is convex the minimization minx∈X (cid:107)x −
(xt − (1/γ)∇ft(xt))(cid:107) has a unique solution which is ˆxt.
Therefore, by setting y = ˜xt in (20) we can replace ≤ by
< which implies that

(cid:21)(cid:13)(cid:13)(cid:13)(cid:13) ≤

(cid:13)(cid:13)(cid:13)(cid:13)y−

xt − 1
γ

∇ft(xt)

∇ft(xt)

(cid:20)

xt − 1
γ

∇ft(xt)

xt − 1
γ

∇ft(xt)

(21)
Computing the square of both sides of (21), simplifying the
resulted expression, and multiplying both sides by γ/2 yield

(cid:21)(cid:13)(cid:13)(cid:13)(cid:13),

(cid:20)

(cid:13)(cid:13)(cid:13)(cid:13)ˆxt−

(cid:21)(cid:13)(cid:13)(cid:13)(cid:13) <

(cid:20)

(cid:13)(cid:13)(cid:13)(cid:13)˜xt−

γ
2

<

γ
2

(cid:107)ˆxt − xt(cid:107)2 + (ˆxt − xt)T∇ft(xt)

(cid:107)˜xt − xt(cid:107)2 + (˜xt − xt)T∇ft(xt).

(22)

minx∈X(cid:8)∇ft(xt)T (x − xt) + (γ/2)(cid:107)x − xt(cid:107)2(cid:9). The opti-

However, we know that ˜xt is a minimizer of the program

mality condition of this minimization implies that

γ
2

(cid:107)˜xt − xt(cid:107)2 + (˜xt − xt)T∇ft(xt)
≤ γ
2

(cid:107)y − xt(cid:107)2 + (y − xt)T∇ft(xt),

(23)

for any y ∈ X . By setting y = ˆxt in (23) we obtain that

γ
2

(cid:107)˜xt − xt(cid:107)2 + (˜xt − xt)T∇ft(xt)
≤ γ
2

(cid:107)ˆxt − xt(cid:107)2 + (ˆxt − xt)T∇ft(xt),

(24)

which contradicts the result in (22). Therefore, the assump-
tion that ˆxt (cid:54)= ˜xt leads to a contradiction. Hence, we can
conclude that ˆxt = ˜xt which implies that the updates in (8)
and (10) are equivalent.

L
2
γ
2

(cid:107)x − xt(cid:107)2

(cid:107)x − xt(cid:107)2

(∇ft(xt) + γ(ˆxt − xt))T (x − ˆxt) ≥ 0,

(27)
for any x ∈ X . From the result in (27), it follows that the
inner product ∇ft(xt)T (x− ˆxt) is bounded below by γ(xt−
ˆxt)T (x − ˆxt). Applying this substitution into (26) yields
ft(x) − µ
2

(28)
≥ ft(xt) + ∇ft(xt)T (ˆxt − xt) + γ(xt − ˆxt)T (x − ˆxt).
According to the Lipschitz continuity of the instantaneous
gradients ∇ft in Assumption 2 and Taylor’s series of the
objective function ft(ˆxt) near the point xt we can write
ft(ˆxt) ≤ ft(xt)+∇ft(xt)T(ˆxt − xt) +
≤ ft(xt)+∇ft(xt)T(ˆxt − xt) +

(cid:107)ˆxt−xt(cid:107)2
(cid:107)ˆxt−xt(cid:107)2, (29)
where the second inequality holds since γ ≥ L. Thus,
the sum ft(xt) + ∇ft(xt)T (ˆxt − xt) is bounded below by
ft(ˆxt)− (γ/2)(cid:107)ˆxt − xt(cid:107)2. By applying this substitution into
(28) we obtain
ft(x) − µ
2

≥ ft(ˆxt) − γ
2

(cid:107)ˆxt − xt(cid:107)2 + γ(xt − ˆxt)T (x − ˆxt). (30)
By adding and subtracting xt we can expand the inner
product (xt− ˆxt)T (x− ˆxt) as the sum of (xt− ˆxt)T (x− xt)
and (xt− ˆxt)T (xt− ˆxt). From applying this substitution into
(30) it follows that
ft(x) − µ
2

t − xt(cid:107)2 +

(cid:107)ˆxt − xt(cid:107)2 + γ(xt − ˆxt)T (x∗

(cid:107)ˆxt − xt(cid:107)2 + γ(xt − ˆxt)T (x − xt). (31)

γ
2
t in (31) and regroup the terms to obtain

≥ ft(ˆxt) +
Now set x = x∗
t ) − ft(ˆxt)
ft(x∗
(32)
t − xt).
(cid:107)x∗
≥ µ
2
Note that the optimal objective function value ft(x∗
t ) is
smaller than ft(ˆxt). Thus, the left hand side of (32) is non-
positive which implies that the right hand side is also smaller
than 0, i.e., (µ/2)(cid:107)x∗
t − xt(cid:107)2 + (γ/2)(cid:107)ˆxt − xt(cid:107)2 + γ(xt −
t − xt) ≤ 0. Therefore, by dividing both sides of the
ˆxt)T (x∗
inequality by γ and regrouping the terms it follows that
(xt − ˆxt)T (xt − x∗

(cid:107)ˆxt − xt(cid:107)2 +

t − xt(cid:107)2. (33)

(cid:107)x − xt(cid:107)2

(cid:107)x∗

γ
2

t ) ≥ 1
2

µ
2γ

Now by showing a lower bound for the inner product (xt−
t ) in terms of the squared norms (cid:107)ˆxt− xt(cid:107)2 and
ˆxt)T (xt− x∗
t − xt(cid:107)2 we proceed to prove the claim in (14). Consider
(cid:107)x∗
the update xt+1 = (1− h)xt + hˆxt in (9). By subtracting x∗
t
from both sides of the equality and computing the squared
norm of the resulted expression we obtain

t(cid:107)2
(cid:107)xt+1 − x∗
= (cid:107)xt − x∗
t(cid:107)2 + h2(cid:107)xt − ˆxt(cid:107)2 − 2h(xt − x∗

(34)
t )T (xt − ˆxt).

Substitute the inner product (xt − x∗
its lower bound in (33) to obtain

t )T (xt − ˆxt) in (34) by

T(cid:107) to the right hand side of the resulted expression to obtain
x∗
T(cid:88)

(38)

(cid:107)xt − x∗

t=1

t(cid:107) ≤ (cid:107)x1 − x∗
T(cid:88)

1(cid:107) − ρ(cid:107)xT − x∗
T(cid:107)
T(cid:88)

(cid:107)xt − x∗

t(cid:107) +

+ ρ

t=1

t=2

(cid:107)x∗

t − x∗

t−1(cid:107).

(cid:80)T
t=1 (cid:107)xt − x∗
T(cid:88)
(cid:107)xt − x∗

t=1

Regrouping the term in (38) implies that the aggregate error

t(cid:107) is bounded above by
t(cid:107) ≤ (cid:107)x1 − x∗
1(cid:107) − ρ(cid:107)xT − x∗
T(cid:107)
(1 − ρ)
T(cid:88)

1

+

(1 − ρ)

t=2

(cid:107)x∗

t − x∗

t−1(cid:107).

(39)

(cid:18)

(cid:19)
t(cid:107)2
(cid:107)xt+1 − x∗
≤
1 − hµ
γ

(cid:107)xt − x∗

t(cid:107)2 + h(h − 1)(cid:107)xt − ˆxt(cid:107)2.

(35)

Considering the deﬁnitions of K1 and K2 in (16), and the
result in (39) it follows that the claim in (14) holds and the
proof is complete.

First note that h is a constant from the interval (0, 1],
therefore, the term h(h − 1)(cid:107)xt − ˆxt(cid:107)2 is smaller than zero
and we can simplify the right hand side of (35) as

(cid:18)

(cid:19)

(cid:107)xt+1 − x∗

t(cid:107)2 ≤

1 − hµ
γ

(cid:107)xt − x∗

t(cid:107)2.

(36)

Note that the strong convexity constant µ is smaller than the
constant of gradients Lipschitz continuity L, i.e., µ ≤ L.
Thus, we obtain that µ ≤ γ, since the constant γ is chosen
such that γ ≥ L. Moreover, h ≤ 1 which implies that 0 <
hµ/γ ≤ 1. Thus, the coefﬁcient in in (36) satisﬁes 0 ≤
1 − hµ/γ < 1. Hence, the constant ρ := (1 − hµ/γ)1/2 is
from the interval [0, 1). This observation in conjunction with
the inequality in (36) follows the claim in (14).

C. Proof of Theorem 1

t=1 (cid:107)xt−x∗

t=1 (cid:107)xt − x∗

We use the result in Proposition 2 to derive an upper bound
t(cid:107). First, decompose the
1(cid:107)
t(cid:107) as the sum of the ﬁrst term (cid:107)x1 − x∗
t(cid:107). Now using the
triangle inequality we can show that each term (cid:107)xt − x∗
t(cid:107)
t−1(cid:107)+
for t = 2, . . . , T is bounded above by the sum (cid:107)xt−x∗
t−1(cid:107). Considering this upper bound we can show that
(cid:107)x∗
t(cid:107) is bounded above by

for the aggregate error(cid:80)T
sum(cid:80)T
and the remaining terms (cid:80)T
the aggregate error(cid:80)T
t −x∗
T(cid:88)

t=2 (cid:107)xt − x∗

t=1 (cid:107)xt − x∗

(37)

(cid:107)xt − x∗
t(cid:107)

t=1

≤ (cid:107)x1 − x∗

1(cid:107) +

T(cid:88)

t=2

(cid:107)xt − x∗

t−1(cid:107) +

t − x∗

t−1(cid:107).

(cid:107)x∗

T(cid:88)
the sum (cid:80)T

t=2

t−1(cid:107) is bounded above by ρ(cid:80)T
the sum (cid:80)T
ρ(cid:80)T

According to the result
x∗
t=2 (cid:107)xt − x∗

t=2 (cid:107)xt −
t−1(cid:107). Replace
t−1(cid:107) in (37) by the upper bound
t−1(cid:107), and add and subtract the term ρ(cid:107)xT −

t=2 (cid:107)xt−1 − x∗

t=2 (cid:107)xt−1−x∗

in (14),

REFERENCES

[1] S. Boyd and L. Vandenberghe, Convex optimization.

Cambridge

university press, 2004.

[2] A. S. Nemirovski and D. B. Yudin, Problem complexity and method
efﬁciency in optimization. Wiley (Chichester and New York), 1983.
[3] A. Beck and M. Teboulle, “Mirror descent and nonlinear projected
subgradient methods for convex optimization,” Operations Research
Letters, vol. 31, no. 3, pp. 167–175, 2003.

[4] N. Cesa-Bianchi and G. Lugosi, Prediction, learning, and games.

Cambridge University Press, 2006.

[5] S. Shalev-Shwartz, “Online learning and online convex optimization,”
Foundations and Trends in Machine Learning, vol. 4, no. 2, pp. 107–
194, 2011.

[6] M. Zinkevich, “Online convex programming and generalized in-
ﬁnitesimal gradient ascent,” in International Conference on Machine
Learning, 2003.

[7] Y. Freund and R. E. Schapire, “A decision-theoretic generalization of
on-line learning and an application to boosting,” Journal of computer
and system sciences, vol. 55, no. 1, pp. 119–139, 1997.

[8] E. Hazan, A. Agarwal, and S. Kale, “Logarithmic regret algorithms
for online convex optimization,” Machine Learning, vol. 69, no. 2-3,
pp. 169–192, 2007.

[9] O. Besbes, Y. Gur, and A. Zeevi, “Non-stationary stochastic optimiza-

tion,” Operations Research, vol. 63, no. 5, pp. 1227–1244, 2015.

[10] E. Hall and R. Willett, “Online convex optimization in dynamic
environments,” IEEE Journal of Selected Topics in Signal Processing,
vol. 9, no. 4, pp. 647–662, June 2015.

[11] A. Jadbabaie, A. Rakhlin, S. Shahrampour, and K. Sridharan, “Online
optimization: Competing with dynamic comparators,” in Proceedings
of the Eighteenth International Conference on Artiﬁcial Intelligence
and Statistics, 2015, pp. 398–406.

[12] R. E. Kalman, “A new approach to linear ﬁltering and prediction
problems,” Journal of Fluids Engineering, vol. 82, no. 1, pp. 35–45,
1960.

[13] M. Herbster and M. K. Warmuth, “Tracking the best expert,” Machine

Learning, vol. 32, no. 2, pp. 151–178, 1998.

[14] E. Hazan and C. Seshadhri, “Adaptive algorithms for online decision
problems,” in Electronic Colloquium on Computational Complexity
(ECCC), vol. 14, no. 088, 2007.

[15] N. Cesa-Bianchi, P. Gaillard, G. Lugosi, and G. Stoltz, “A new look

at shifting regret,” CoRR abs/1202.3323, 2012.

[16] A. Daniely, A. Gonen, and S. Shalev-shwartz, “Strongly adaptive
online learning,” in Proceedings of the 32nd International Conference
on Machine Learning (ICML-15), 2015, pp. 1405–1411.

[17] H. Luo and R. E. Schapire, “Achieving all with no parameters:
Adanormalhedge,” in Proceedings of The 28th Conference on Learning
Theory, 2015, pp. 1286–1304.

[18] A. Rakhlin and K. Sridharan, “Online learning with predictable
sequences,” in Conference on Learning Theory, 2013, pp. 993–1019.
[19] S. Rakhlin and K. Sridharan, “Optimization, learning, and games with
predictable sequences,” in Advances in Neural Information Processing
Systems, 2013, pp. 3066–3074.

[20] C.-K. Chiang, T. Yang, C.-J. Lee, M. Mahdavi, C.-J. Lu, R. Jin, and
S. Zhu, “Online optimization with gradual variations.” in Conference
on Learning Theory, 2012.

