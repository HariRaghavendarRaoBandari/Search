6
1
0
2

 
r
a

M
5

 

 
 
]

B
D
.
s
c
[
 
 

1
v
2
8
6
1
0

.

3
0
6
1
:
v
i
X
r
a

Frequent-Itemset Mining using

Locality-Sensitive Hashing

Debajyoti Bera1 and Rameshwar Pratap2

1 Indraprastha Institute of Information Technology-Delhi (IIIT-D), India

dbera@iiitd.ac.in

2 TCS Innovation Labs, India
rameshwar.pratap@gmail.com

Abstract. The Apriori algorithm is a classical algorithm for the fre-
quent itemset mining problem. A signiﬁcant bottleneck in Apriori is the
number of I/O operation involved, and the number of candidates it gener-
ates. We investigate the role of LSH techniques to overcome these prob-
lems, without adding much computational overhead. We propose ran-
domized variations of Apriori that are based on asymmetric LSH deﬁned
over Hamming distance and Jaccard similarity.

1

Introduction

Mining frequent itemsets in a transactions database appeared ﬁrst in the context
of analyzing supermarket transaction data for discovering association rules [2, 1],
however this problem has, since then, found applications in diverse domains like
ﬁnding correlations [12], ﬁnding episodes [8], clustering [13]. Mathematically,
each transaction can be regarded as a subset of the items (“itemset”) those
that present in the transaction. Given a database D of such transactions and a
support threshold θ ∈ (0, 1), the primary objective of frequent itemset mining
is to identify θ-frequent itemsets (denoted by FI, these are subsets of items that
appear in at least θ-fraction of transactions).

Computing FI is a challenging problem of data mining. The question of de-
ciding if there exists any FI with k items is known to be NP-complete [6] (by
relating it to the existence of bi-cliques of size k in a given bipartite graph) but on
a more practical note, simply checking support of any itemset requires reading
the transaction database – something that is computationally expensive since
they are usually of an extremely large size. The state-of-the-art approaches try
to reduce the number of candidates, or not generate candidates at all. The best
known approach in the former line of work is the celebrated Apriori algorithm [2].
Apriori is based on the anti-monotonicity property of partially-ordered sets
which says that no superset of an infrequent itemset can be frequent. This al-
gorithm works in a bottom-up fashion by generating itemsets of size l in level
l, starting at the ﬁrst level. After ﬁnding frequent itemsets at level l they are
joined pairwise to generate l+1-sized candidate itemsets; FI are identiﬁed among
the candidates by computing their support explicitly from the data. The algo-
rithm terminates when no more candidates are generated. Broadly, there are

two downsides to this simple but eﬀective algorithm. The ﬁrst one is that the
algorithm has to compute support 3 of every itemset in the candidate, even the
ones that are highly infrequent. Secondly, if an itemset is infrequent, but all its
subsets are frequent, Apriori doesn’t have any easy way of detecting this without
reading every transaction of the candidates.

A natural place to look for fast algorithms over large data are randomized
techniques; so we investigated if LSH could be be of any help. An earlier work by
Cohen et al. [4] was also motivated by the same idea but worked on a diﬀerent
problem (see Section 1.2). LSH is explained in Section 2, but roughly, it is a
randomized hashing technique which allows eﬃcient retrieval of approximately
“similar” elements (here, itemsets).

1.1 Our contribution

In this work, we propose LSH-Apriori – a basket of three explicit variations of
Apriori that uses LSH for computing FI. LSH-Apriori handles both the above
mentioned drawbacks of the Apriori algorithm. First, LSH-Apriori signiﬁcantly
cuts down on the number of infrequent candidates that are generated, and further
due to its dimensionality reduction property saves on reading every transaction;
secondly, LSH-Apriori could eﬃciently ﬁlter our those infrequent itemset without
looking every candidate. The ﬁrst two variations essentially reduce computing FI
to the approximate nearest neighbor (cNN) problem for Hamming distance and
Jaccard similarity. Both these approaches can drastically reduce the number of
false candidates without much overhead, but has a non-zero probability of error
in the sense that some frequent itemset could be missed by the algorithm. Then
we present a third variation which also maps FI to elements in the Hamming
space but avoids the problem of these false negatives incurring a little cost of
time and space complexity. Our techniques are based on asymmetric LSH [11]
and LSH with one-sided error [9] which are proposed very recently.

1.2 Related work

There are a few hash based heuristic to compute FI which outperform the Apriori
algorithm and PCY [10] is one of the most notable among them. PCY focuses
on using hashing to eﬃciently utilize the main memory over each pass of the
database. However, our objective and approach both are fundamentally diﬀerent
from that of PCY.

The work that comes closest to our work is by Cohen et al. [4]. They devel-
oped a family of algorithms for ﬁnding interesting associations in a transaction
database, also using LSH techniques. However, they speciﬁcally wanted to avoid
any kind of ﬁltering of itemsets based on itemset support. On the other hand, our
problem is the vanilla frequent itemset mining which requires ﬁltering itemsets
satisfying a given minimum support threshold.

3 Note that computing support is an I/O intensive operation and involves reading

every transaction.

1.3 Organization of the paper

In Section 2, we introduce the relevant concepts and give an overview of the
problem. In Section 3, we build up the concept of LSH-Apriori which is required
to develop our algorithms. In Section 4, we present three speciﬁc variations
of LSH-Apriori for computing FI. Algorithms of Subsections 4.1 and 4.2 are
based on Hamming LSH and Minhashing, respectively. In Subsection 4.3, we
present another approach based on CoveringLSH which overcomes the problem
of producing false negatives. In Section 5, we summarize the whole discussion.

2 Background

Notations

D Database of transactions: {t1, . . . , tn} n Number of transactions
Dl FI of level-l: {I1, . . . Iml }
αl Maximum support of any item in Dl m Number of items
ε Error tolerance in LSH, ε ∈ (0, 1)
δ Probability of error in LSH, δ ∈ (0, 1) |v| Number of 1′s in v

ml Number of FI of size l

θ Support threshold, θ ∈ (0, 1)

The input to the classical frequent itemset mining problem is a database D
of n transactions {T1, . . . , Tn} over m items {i1, . . . , im} and a support threshold
θ ∈ (0, 1). Each transaction, in turn, is a subset of those items. Support of itemset
I ⊆ {i1, . . . , im} is the number of transactions that contain I. The objective of
the problem is to determine every itemset with support at least θn. We will often
identify an itemset I with its transaction vector hI[1], I[2], . . . , I[n]i where I[j]
is 1 if I is contained in Tj and 0 otherwise. An equivalent way to formulate the
objective is to ﬁnd itemsets with at least θn 1’s in their transaction vectors. It
will be useful to view D as a set of m transaction vectors, one for every item.

2.1 Locality Sensitive Hashing

We ﬁrst brieﬂy explain the concept of locality sensitive hashing (LSH).

Deﬁnition 1 (Locality sensitive hashing [7]). Let S be a set of m vectors
in Rn, and U be the hashing universe. Then, a family H of functions from S to
U is called as (S0, (1 − ε)S0, p1, p2)-sensitive (with ε ∈ (0, 1] and p1 > p2) for
the similarity measure Sim(., .) if for any x, y ∈ S:

– if Sim(x, y) ≥ S0, then Pr
h∈H

[h(x) = h(y)] ≥ p1,

– if Sim(x, y) ≤ (1 − ε)S0, then Pr
h∈H

[h(x) = h(y)] ≤ p2.

Not all similarity measures have a corresponding LSH. However, the following
well-known result gives a suﬃcient condition for existence of LSH for any Sim.

Lemma 1 If Φ is a strict monotonic function and a family of hash function H
satisﬁes Prh∈H[h(x) = h(y)) = Φ(Sim(x, y)] for some Sim : Rn × Rn → {0, 1},
then the conditions of Deﬁnition 1 are true for Sim for any ε ∈ (0, 1).

The similarity measures that are of our interest are Hamming and Jac-
card over binary vectors. Let |x| denote the Hamming weight of a binary vec-
tor x. Then, for vectors x and y of length n, Hamming distance is deﬁned as
Ham(x, y) = |x ⊕ y|, where x ⊕ y denotes a vector that is element-wise Boolean
XOR of x and y. Jaccard similarity is deﬁned as hx, yi/|x ∨ y|, where hx, yi in-
dicates inner product, and x ∨ y indicates element-wise Boolean OR of x and y.
LSH for these similarity measures are simple and well-known [7, 5, 3]. We recall
them below; here I is some subset of {1, . . . , n} (or, n-length transaction vector).

Deﬁnition 2 (Hash function for Hamming distance). For any particular
bit position i, we deﬁne the function hi(I) := I[i]. We will use hash functions of
the form gJ (I) = hhj1 (I), hj2 (I), . . . , hjk (I)i, where J = {j1, . . . , jk} is a subset
of {1, . . . , n} and the hash values are binary vectors of length k.

Deﬁnition 3 (Minwise Hash function for Jaccard similarity). Let π be
some permutations over {1, . . . , n}. Treating I as a subset of indices, we will use
hash functions of the form hπ(I) = arg mini π(i) for i ∈ I.

The probabilities that two itemsets hash to the same value for these hash func-
tions are related to their Hamming distance and Jaccard similarity, respectively.

2.2 Apriori algorithm for frequent itemset mining

As explained earlier, Apriori works in level-by-level, where the objective of level-l
is to generate all θ-frequent itemsets with l-items each; for example, in the ﬁrst
level, the algorithm simply computes support of individual items and retains the
ones with support at least θn. Apriori processes each level, say level-(l + 1), by
joining all pairs of θ-frequent compatible itemsets generated in level-l, and further
ﬁltering out the ones which have support less than θn (support computation
involves fetching the actual transactions from disk). Here, two candidate itemsets
(of size l each) are said to be compatible if their union has size exactly l + 1. A
high-level pseudocode of Apriori is given in Algorithm 1.

Input: Transaction database D, support threshold θ;
Result: θ-frequent itemsets;

1 l = 1 /* level */;

2 F = (cid:8){x} | {x} is θ-frequent in D(cid:9) /* frequent itemsets in level-1 */ ;

3 Output F ;
4 while F is not empty do

5

6

7

8

9

10

11

l = l + 1;
C = {Ia ∪ Ib | Ia ∈ F, Ib ∈ F, Ia and Ib are compatible};
F = ∅;
for itemset I in C do

Add I to F if support of I in D is at least θn /* reads database*/ ;

end
Output F ;

12 end

Algorithm 1: Apriori algorithm for frequent itemset mining

3 LSH-Apriori

The focus of this paper is to reduce the computation of processing all pairs
of itemsets at each level in line 6 (which includes computing support by going
through D). Suppose that level l outputs ml frequent itemsets. We will treat
the output of level l as a collection of ml transaction vectors Dl = {I1, . . . Iml},
each of length n and one for each frequent itemset of the l-th level. Our approach
involves deﬁning appropriate notions of similarity between itemsets (represented
by vectors) in Dl similar to the approach followed by Cohen et al.[4]. Let Ii, Ij
be two vectors each of length n. Then, we use |Ii, Ij | to denote the number of
bit positions where both the vectors have a 1.

Deﬁnition 4. Given a parameter 0 < ε < 1, we say that {Ii, Ij} is θ-frequent
(or similar) if |Ii, Ij| ≥ θn and {Ii, Ij} is (1−ε)θ-infrequent if |Ii, Ij| < (1−ε)θn.
Furthermore, we say that Ij is similar to Ii if {Ii, Ij } is θ-frequent.

Let Iq be a frequent itemset at level l − 1. Let FI(Iq, θ) be the set of itemsets
Ia such that {Iq, Ia} is θ-frequent at level l. Our main contributions are a few
randomized algorithms for identifying itemsets in FI(Iq, θ) with high-probability.

Deﬁnition 5 (FI(Iq, θ, ε, δ)). Given a θ-frequent itemset Iq of size l − 1, toler-
ance ε ∈ (0, 1) and error probability δ, FI(Iq, θ, ε, δ) is a set F ′ of itemsets of
size l, such that with probability at least 1 − δ, F ′ contains every Ia for which
{Iq, Ia} is θ-frequent.

It is clear that FI(Iq, θ) ⊆ FI(Iq, θ, ε, δ) with high probability. This motivated
us to propose LSH-Apriori, a randomized version of Apriori, that takes δ and ε
as additional inputs and essentially replaces line 6 by LSH operations to com-
bine every itemset Iq with only similar itemsets, unlike Apriori which combines
all pairs of itemsets. This potentially creates a signiﬁcantly smaller C without
missing out too many frequent itemsets. The modiﬁcations to Apriori are pre-
sented in Algorithm 2 and the following lemma, immediate from Deﬁnition 5,
establishes correctness of LSH-Apriori.

Input: Dl = {I1, . . . , Iml}, θ, (Additional) error probability δ, tolerance ε;

6a (Pre-processing) Initialize hash tables and add all items Ia ∈ Dl;
6b (Query) Compute FI(Iq, θ, ε, δ) ∀Iq ∈ Dl by hashing Iq and checking

collisions;

6c C ← {Iq ∪ Ib | Iq ∈ Dl, Ib ∈ FI(Iq, θ, ε, δ)};

Algorithm 2: LSH-Apriori level l +1 (only modiﬁcations to Apriori line:6)

Lemma 2 Let Iq and Ia be two θ-frequent compatible itemsets of size (l − 1)
such that the itemset J = Iq ∪ Ia is also θ-frequent. Then, with probability at
least 1 − δ, FI(Iq, θ, ε, δ) contains Ia (hence C contains J).

In the next section we describe three LSH-based randomized algorithms to
compute FI(Iq, θ, ε, δ) for all θ-frequent itemset Iq from the earlier level. The

input to these subroutines will be Dl, the frequent itemsets from earlier level,
and parameters θ, ε, δ. In the pre-processing stage at level l, the respective LSH
is initialized and itemsets of Dl are hashed; we speciﬁcally record the itemsets
hashing to every bucket. LSH guarantees (w.h.p.) that pairs of similar items hash
into the same bucket, and those that are not hash into diﬀerent buckets. In the
query stage we ﬁnd all the itemsets that any Iq ought to be combined with by
looking in the bucket in which Iq hashed, and then combining the compatible
ones among them with Iq to form C. Rest of the processing happens `a la Apriori.
The internal LSH subroutines may output false-positives – itemsets that are
not θ-frequent, but such itemsets are eventualy ﬁltered out in line 9 of Algo-
rithm 1. Therefore, the output of LSH-Apriori does not contain any false pos-
itives. However, some frequent itemsets may be missing from its output (false
negatives) with some probability depending on the parameter δ as stated below
in Theorem 3 (proof follows from the union bound and is give in the Appendix).

Theorem 3 (Correctness). LSH-Apriori does not output any itemset which
is not θ-infrequent. If X is a θ-frequent itemset of size l, then the probability that
LSH-Apriori does not output X is at most δ2l.

The tolerance parameter ε can be used to balance the overhead from using
hashing in LSH-Apriori with respect to its savings because of reading fewer
transactions. Most LSH, including those that we will be using, behave somewhat
like dimensionality reduction. As a result, the hashing operations do not operate
on all bits of the vectors. Furthermore, the pre-condition of similarity for joining
ensure that (w.h.p.) most infrequent itemsets can be detected before verifying
them from D. To formalize this, consider any level l with ml θ-frequent itemsets
Dl. We will compare the computation done by LSH-Apriori at level l + 1 to what
Apriori would have done at level l + 1 given the same frequent itemsets Dl. Let
cl+1 denote the number of candidates Apriori would have generated and ml+1
the number of frequent itemsets at this level (LSH-Apriori may generate fewer).
Overhead: Let τ (LSH) be the time required for hashing an itemset for a par-
ticular LSH and let σ(LSH) be the space needed for storing respective hash
values. The extra overhead in terms of space will be simply mlσ(LSH) in level
l + 1. With respect to overhead in running time, LSH-Apriori requires hashing
each of the ml itemsets twice, during pre-processing and during querying. Thus
total time overhead in this level is ϑ(LSH, l + 1) = 2mlτ (LSH).
Savings: Consider the itemsets in Dl that are compatible with any Iq ∈ Dl.
Among them are those whose combination with Iq do not generate a θ-frequent
itemset for level l + 1; call them as negative itemsets and denote their number by
r(Iq) itemsets in order to
reject them. Some of these negative itemsets will be added to FI by LSH-Apriori
– we will call them false positives and denote their count by F P (Iq); the rest
those which correctly not added with Iq – lets call them as true negatives and
r(Iq) =
2(cl+1 −ml+1). Suppose φ(LSH) denotes the number of transactions a particular
LSH-Apriori reads for hashing any itemset; due to the dimensionality reduction

denote their count by T N (Iq). Clearly, r(Iq) = T N (Iq)+F P (Iq) andPIq

r(Iq). Apriori will have to read all n transactions ofPIq

property of LSH, φ(LSH) is always o(n). Then, LSH-Apriori is able to reject all
itemsets in T N by reading only φ transactions for each of them; thus for itemset
Iq in level l + 1, a particular LSH-Apriori reads (n − φ(LSH)) × T N (Iq) fewer
transactions compared to a similar situation for Apriori. Therefore, total savings

at level l + 1 is ς(LSH, l + 1) = (n − φ(LSH)) ×PIq

In Section 4, we discuss this in more detail along with the respective LSH-

T N (Iq).

Apriori algorithms.

4 FI via LSH

Our similarity measure |Ia, Ib| can also be seen as the inner product of the
binary vectors Ia and Ib. However, it is not possible to get any LSH for such
similarity measure because for example there can be three items Ia, Ib and Ic
such that |Ia, Ib| ≥ |Ic, Ic| which implies that Pr(h(Ia) = h(Ib)) ≥ Pr(h(Ic) =
h(Ic)) = 1, which is not possible. Noting the exact same problem, Shrivastava
et al. introduced the concept of asymmetric LSH [11] in the context of binary
inner product similarity. The essential idea is to use two diﬀerent hash functions
(for pre-processing and for querying) and they speciﬁcally proposed extending
MinHashing by padding input vectors before hashing. We use the same pair of
padding functions proposed by them for n-length binary vectors in a level l:
P(n,αl) for preprocessing and Q(n,αl) for querying are deﬁned as follows.

– In P (I) we append (αln − |I|) many 1′s followed by (αln + |I|) many 0′s.
– In Q(I) we append αln many 0′s, then (αln − |I|) many 1′s, then |I| 0′s.

Here, αln (at LSH-Apriori level l) will denote the maximum number of ones in
any itemset in Dl. Therefore, we always have (αln − |I|) ≥ 0 in the padding
functions. Furthermore, since the main loop of Apriori is not continued if no
frequent itemset is generated at any level, (αl − θ) > 0 is also ensured at any
level that Apriori is executing.

We use the above padding functions to reduce our problem of ﬁnding similar
itemsets to ﬁnding nearby vectors under Hamming distance (using Hamming-
based LSH in Subsection 4.1 and Covering LSH in Subsection 4.3) and under
Jaccard similarity (using MinHashing in Subsection 4.2).

4.1 Hamming based LSH

In the following lemma (proof is given in appendix), we relate Hamming distance
of two itemsets Ix and Iy with their |Ix, Iy|.

Lemma 4 For two itemsets Ix and Iy, Ham(P (Ix), Q(Iy)) = 2(αln − |Ix, Iy|).

Therefore, it is possible to use an LSH for Hamming distance to ﬁnd sim-
ilar itemsets. We use this technique in the following algorithm to compute
FI(Iq, θ, ε, δ) for all itemset Iq. The algorithm contains an optimization over the
generic LSH-Apriori pseudocode (Algorithm 2). There is no need to separately

execute lines:7–10 of Apriori; one can immediately set F ← C since LSH-Apriori
computes support before populating FI.

Input: Dl = {I1, . . . , Iml}, query item Iq, threshold θ, tolerance ε, error δ.
Result: FIq = FI(Iq, θ, ε, δ) for every Iq ∈ Dl.

6a Preprocessing step: Setup hash tables and add vectors in Dl;

Set ρ = αl−θ

αl−(1−ε)θ , k = log(cid:16)

(1+2(1−ε)θ) (cid:17) ml and L = mρ

1+2αl

l log(cid:0) 1
δ(cid:1);

i

ii

iii

Select functions g1, . . . , gL u.a.r.;
For every Ia ∈ Dl, pad Ia using P () and then hash P (Ia) into buckets
g1(P (Ia)), ..., gL(P (Ia));

6b Query step: For every Iq ∈ Dl, we do the following ;

i

ii

S ← all Iq-compatible itemsets in all buckets gi(Q(Iq)), for i = 1 . . . L;
for Ia ∈ S do

If |Ia, Iq| ≥ θn, then add Ia to FIq /* reads database*/;
(*) If no itemset similar to Iq found within L

δ tries, then break loop;

end

Algorithm 3: LSH-Apriori (only lines 6a,6b) using Hamming LSH
Correctness of this algorithm is straightforward. Also, ρ < 1 and the space
required and overhead of reading transactions is θ(kLml) = o(m2
l ). It can be
further shown that E[F P (Iq)] ≤ L for Iq ∈ Dl which can be used to prove that
E[ς] ≥ (n − φ)(2(cl+1 − ml+1) − mlL) where φ = kL. Details of these calculations
including complete proof of the next lemma is given in Appendix.

Expected savings outweigh time overhead if n ≫ ml, cl+1 = θ(m2

ings can be bounded by E[ς(l+1)] ≥ (cid:0)n−o(ml)(cid:1)(cid:0)(cl+1 −2ml+1)+(cl+1 −o(m2

Lemma 5 Algorithm 3 correctly outputs FI(Iq, θ, ε, δ) for all Iq ∈ Dl. Additional
space required is o(m2
l ), which is also the total time overhead. The expected sav-
l ))(cid:1).
l ) and
cl+1 > 2ml+1, i.e., in levels where the number of frequent itemsets generated
are fewer compared to the number of transactions as well as to the number of
candidates generated. The additional optimisation (*) essentially increases the
savings when all l + 1-extensions of Iq are (1 − ε)θ-infrequent — this behaviour
will be predominant in the last few levels. It is easy to show that in this case,
δ with probability at least 1 − δ; this in turn implies that |S| ≤ L
F P (Iq) ≤ L
δ .
So, if we did not ﬁnd any similar Ia within ﬁrst L
δ tries, then we can be sure,
with reasonable probability, that there are no itemsets similar to Iq.

4.2 Min-hashing based LSH

Cohen et al. had given an LSH-based randomized algorithm for ﬁnding inter-
esting itemsets without any requirement for high support [4]. We observed that
their Minhashing-based technique [3] cannot be directly applied to the high-
support version that we are interested in. The reason is roughly that Jaccard
similarity and itemset similarity (w.r.t. θ-frequent itemsets) are not monotonic

to each other. Therefore, we used padding to monotonically relate Jaccard sim-
ilarity of two itemsets Ix and Iy with their |Ix, Iy| (proof is given in Appendix).

|Ix,Iy|

Lemma 6 For two padded itemsets Ix and Iy, JS(P (Ix), Q(Iy)) =

2αln−|Ix,Iy| .
Once padded, we follow similar steps (as [4]) to create a similarity preserv-
ing summary ˆDl of Dl such that the Jaccard similarity for any column pair in
Dl is approximately preserved in ˆDl, and then explicitly compute FI(Iq, θ, ε, δ)
from ˆDl. ˆDl is created by using λ independent minwise hashing functions (see
Deﬁnition 3). λ should be carefully chosen since a higher value increases the
accuracy of estimation, but at the cost of large summary vectors in ˆDl. Let us
deﬁne ˆJS(Ii, Ij ) as the fraction of rows in the summary matrix in which min-wise
entries of columns Ii and Ij are identical. Then by Theorem 1 of Cohen et al. [4],
we can get a bound on the number of required hash functions:

Theorem 7 (Theorem 1 of [4]). Let 0 < ǫ, δ < 1 and λ ≥ 2
δ . Then
for all pairs of columns Ii and Ij following are true with probability at least 1 − δ:
– If JS(Ii, Ij) ≥ s∗ ≥ ω, then ˆJS(Ii, Ij) ≥ (1 − ǫ)s∗,
– If JS(Ii, Ij) ≤ ω, then ˆJS(Ii, Ij) ≤ (1 + ǫ)ω.

ωǫ2 log 1

Input: Dl, query item Iq, threshold θ, tolerance ε, error δ
Result: FIq = FI(Iq, θ, ε, δ) for every Iq ∈ Dl.

6a Preprocessing step: Prepare ˆDl via MinHashing;
αl+(αl−θ)(1−ε) and λ = 2

Set ω = (1−ε)θ
Choose λ many independent permutations (see Theorem 7);
For every Ia ∈ Dl, pad Ia using P () and then hash P (Ia) using λ
independent permutations;

2αl−(1−ε)θ , ǫ =

i

ii

iii

4

αlε

ωǫ2 log 1
δ ;

6b Query step: For every Iq ∈ Dl, we do the following ;

i Hash Q(Iq) using λ independent permutations;

ii

for compatible Ia ∈ Dl do

If ˆJS(P (Ia), Q(Iq)) ≥ (1−ǫ)θ

2αl−θ for some Ia, then add Ia to FIq;

end

Algorithm 4: LSH-Apriori (only lines 6a,6b) using Minhash LSH

Lemma 8 Algorithm 4 correctly computes FI(Iq, θ, ε, δ) for all Iq ∈ Dl. Addi-
tional space required is O(λml), and the total time overhead is O((n + λ)ml).
The expected savings is given by E[ς(l + 1)] ≥ 2(1 − δ)(n − λ)(cl+1 − ml+1).

See Appendix for details of the above proof. Note that λ depends on αl but
is independent of n. This method should be applied only when λ ≪ n. And in
that case, for levels with number of candidates much larger than the number of
frequent itemsets discovered (i.e., cl+1 ≫ {ml, ml+1}), time overhead would not
appear signiﬁcant compared to expected savings.

4 This algorithm can be easily boosted to o(λml) time by applying banding technique

(see Section 4 of

[4]) on the minhash table.

4.3 Covering LSH

Due to their probabilistic nature, the LSH-algorithms presented earlier have the
limitation of producing false positives and more importantly, false negatives.
Since the latter cannot be detected unlike the former, these algorithms may
miss some frequent itemsets (see Theorem 3). In fact, once we miss some FI at
a particular level, then all the FI which are “supersets” of that FI
(in the
subsequent levels) will be missed. Here we present another algorithm for the same
purpose which overcomes this drawback. The main tool is a recent algorithm due
to Pagh [9] which returns approximate nearest neighbors in the Hamming space.
It is an improvement over the seminal LSH algorithm by Indyk and Motwani [7],
also for Hamming distance. Pagh’s algorithm has a small overhead over the latter;
to be precise, the query time bound of [9] diﬀers by at most ln(4) in the exponent
in comparison with the time bound of [7]. However, its big advantage is that it
generates no false negatives. Therefore, this LSH-Apriori version also does not
miss any frequent itemset.

The LSH by Pagh is with respect to Hamming distance, so we ﬁrst reduce
our FI problem into the Hamming space by using the same padding given in
Lemma 4. Then we use this LSH in the same manner as in Subsection 4.1. Pagh
coined his hashing scheme as coveringLSH which broadly mean that given a
threshold r and a tolerance c > 1, the hashing scheme guaranteed a collision for
every pair of vectors that are within radius r. We will now brieﬂy summarize
coveringLSH for our requirement; refer to the paper [9] for full details.

Similar to HammingLSH, we use a family of Hamming projections as our
hash functions: HA := {x 7→ x ∧ a| a ∈ A}, where A ⊆ {0, 1}(1+2αl)n. Now,
given a query item Iq, the idea is to iterate through all hash functions h ∈ HA,
and check if there is a collision h(P (Ix)) = h(Q(Iq)) for Ix ∈ Dl. We say that this
scheme doesn’t produce false negative for the threshold 2(αl − θ)n, if at least one
collision happens when there is an Ix ∈ Dl when Ham(P (Ix), Q(Iq)) ≤ 2(αl−θ)n,
and the scheme is eﬃcient if the number of collision is not too many when
Ham(P (Ix), Q(Iq)) > 2(αl − (1 − ε)θ)n (proved in Theorem 3.1, 4.1 of [9]). To
make sure that all pairs of vector within distance 2(αl −θ)n collide for some h, we
need to make sure that some h map their “mismatching” bit positions (between
P (Ix) and Q(Iq)) to 0. We describe construction of hash functions next.
ν

n′

θ′

t

c

(1+2αl)n 2(αl−θ)n ⌈

ln ml

2(αl−(1−ε)θ)n ⌉ αl−(1−ε)θ

αl−θ

ǫ
ǫ ∈ (0, 1) s.t.
2(αl−(1−ε)θ)n + ǫ ∈ N

ln ml

t+ǫ
ct

CoveringLSH: The parameters relevant to LSH-Apriori are given above. No-
tice that after padding, dimension of each item is n′, threshold is θ′ (i.e., min-
support is θ′/n′), and tolerance is c. We start by choosing a random function
ϕ : {1, . . . , n′} → {0, 1}tθ ′+1, which maps bit positions of the padded itemsets
to bit vectors of length tθ′ + 1. We deﬁne a family of bit vectors a(v) ∈ {0, 1}n′
,
where a(v)i = hϕ(i), vi, for i ∈ {1, . . . , n′}, v ∈ {0, 1}tθ ′+1 and hm(i), vi denotes
the inner product over F2. We deﬁne our hash function family HA using all such
vectors a(v) except a(0): A = na(v)|v ∈ {0, 1}tθ ′+1/{0}o.

Pagh described how to construct A′ ⊆ A [9, Corollary 4.1] such that HA′ has
a very useful property of no false negatives and also ensuring very few false pos-
itives. We use HA′ for hashing using the same manner of Hamming projections
as used in Subsection 4.1. Let ψ be the expected number of collisions between
any itemset Iq and items in Dl that are (1 − ε)θ-infrequent with Iq. The fol-
lowing Theorem captures the essential property of coveringLSH that is relevant
for LSH-Apriori, described in Algorithm 5. It also bounds the number of hash
functions which controls the space and time overhead of LSH-Apriori. Proof of
this theorem follows from Theorem 4.1 and Corollary 4.1 of [9].

Theorem 9. For a randomly chosen ϕ, a hash family HA′ described above and
distinct Ix, Iq ∈ {0, 1}n :

– If Ham(cid:0)P (Ix), Q(Iq)(cid:1) ≤ θ′, then there exists h ∈ HA′ s.t. h(cid:0)P (Ix)(cid:1) = h(cid:0)Q(Iq)(cid:1),
– Expected number of false positives is bounded by E[ψ] < 2θ ′ǫ+1m
– |HA′ | < 2θ ′ǫ+1m

l ,

1
c

1
c

l .

Input: Dl, query item Iq, threshold θ, tolerance ε, error δ.
Result: FIq = FI(Iq, θ, ε, δ) for every Iq ∈ Dl.

6a Preprocessing step: Setup hash tables according to HA′ and add items;

i

For every Ia ∈ Dl, hash P (Ia) using all h ∈ HA′ ;

6b Query step: For every Iq ∈ Dl, we do the following ;

i

ii

S ← all itemsets that collide with Q(Iq);
for Ia ∈ S do

If |Ia, Iq| ≥ θn, then add Ia to FIq /* reads database*/;
(*) If no itemset similar to Iq found within ψ

δ tries, break loop;

end

Algorithm 5: LSH-Apriori (only lines 6a,6b) using Covering LSH

Lemma 10 Algorithm 5 outputs all θ-frequent itemsets and only θ-frequent

l

l

itemsets. Additional space required is O(cid:0)m1+ν
overhead. The expected savings is given by E[ς(l + 1)] ≥ 2(cid:16)n − log ml
(cid:0)(cl+1 − ml+1) − m1+ν

(cid:1), which is also the total time
c − 1(cid:17)

(cid:1).

See Appendix for the proof. The (*) line is an additional optimisation similar
to what we did for HammingLSH 4.1; it eﬃciently recognizes those frequent
itemsets of the earlier level none of whose extensions are frequent. The guarantee
of not missing any valid itemset comes with a heavy price. Unlike the previous
algorithms, the conditions under which expected savings beats overhead are quite
5 > ml > 2n/2 and ǫ < 0.25 (since
stringent, namely, cl+1 ∈ {ω(m2
1 < c < 2, these bounds ensure that ν < 1 for later levels when αl ≈ θ).

l+1)}, 2n

l ), ω(m2

5 Conclusion

In this work, we designed randomized algorithms using locality-sensitive hashing
(LSH) techniques which eﬃciently outputs almost all the frequent itemsets with

high probability at the cost of a little space which is required for creating hash
tables. We showed that time overhead is usually small compared to the savings
we get by using LSH.

Our work opens the possibilities for addressing a wide range of problems that
employ on various versions of frequent itemset and sequential pattern mining
problems, which potentially can eﬃciently be randomized using LSH techniques.

References

1. R. Agrawal, T. Imielinski, and A. N. Swami. Mining association rules between sets
of items in large databases. In Proceedings of the 1993 ACM SIGMOD Interna-
tional Conference on Management of Data, Washington, D.C., May 26-28, 1993.,
pages 207–216, 1993.

2. R. Agrawal and R. Srikant. Fast algorithms for mining association rules in large
databases. In Proceedings of 20th International Conference on Very Large Data
Bases, September 12-15, 1994, Santiago de Chile, Chile, pages 487–499, 1994.

3. A. Z. Broder, M. Charikar, A. M. Frieze, and M. Mitzenmacher. Min-wise inde-

pendent permutations. J. Comput. Syst. Sci., 60(3):630–659, 2000.

4. E. Cohen, M. Datar, S. Fujiwara, A. Gionis, P. Indyk, R. Motwani, J. D. Ullman,
and C. Yang. Finding interesting associations without support pruning. IEEE
Trans. Knowl. Data Eng., 13(1):64–78, 2001.

5. A. Gionis, P. Indyk, and R. Motwani. Similarity search in high dimensions via
hashing. In VLDB’99, Proceedings of 25th International Conference on Very Large
Data Bases, September 7-10, 1999, Edinburgh, Scotland, UK, pages 518–529, 1999.
6. D. Gunopulos, R. Khardon, H. Mannila, S. Saluja, H. Toivonen, and R. S. Sharma.
Discovering all most speciﬁc sentences. ACM Trans. Database Syst, 28(2):140–174,
2003.

7. P. Indyk and R. Motwani. Approximate nearest neighbors: Towards removing the
curse of dimensionality. In Proceedings of the Thirtieth Annual Symposium on the
Theory of Computing, Dallas, Texas, USA, May 23-26, 1998, pages 604–613, 1998.
8. H. Mannila, H. Toivonen, and A. I. Verkamo. Discovery of frequent episodes in

event sequences. Data Min. Knowl. Discov., 1(3):259–289, 1997.

9. R. Pagh. Locality-sensitive hashing without false negatives. In Proceedings of the
Twenty-Seventh Annual ACM-SIAM Symposium on Discrete Algorithms, SODA
2016, Arlington, VA, USA, January 10-12, 2016, pages 1–9, 2016.

10. J. S. Park, M. Chen, and P. S. Yu. An eﬀective hash based algorithm for mining
association rules. In Proceedings of the ACM SIGMOD International Conference
on Management of Data, San Jose, California, May 22-25, pages 175–186, 1995.
11. A. Shrivastava and P. Li. Asymmetric minwise hashing for indexing binary inner
products and set containment. In Proceedings of the 24th International Conference
on World Wide Web, 2015, Florence, Italy, May 18-22, 2015, pages 981–991, 2015.
12. C. Silverstein, S. Brin, and R. Motwani. Beyond market baskets: Generalizing
association rules to dependence rules. Data Min. Knowl. Discov., 2(1):39–68, 1998.
13. H. Wang, W. Wang, J. Yang, and P. S. Yu. Clustering by pattern similarity in large
data sets. In Proceedings of the 2002 ACM SIGMOD International Conference on
Management of Data, Madison, Wisconsin, June 3-6, 2002, pages 394–405, 2002.

Appendix

Theorem 3 (Correctness). LSH-Apriori does not output any itemset which
is not θ-infrequent. If X is a θ-frequent itemset of size l, then the probability that
LSH-Apriori does not output X is at most δ2l.

Proof. LSH-Apriori does not output X, whose size we denote by l, if at least
one of these hold.

– Any 1 size subset of X is not generated by LSH-Apriori in level-1
– Any 2 size subset of X is not generated by LSH-Apriori in level-2

...

– Any l size subset of X (i.e., X itself) is not generated in level-l

By Lemma 2, δ is the probability that any particular frequent itemset is
not generated at the right level, even though all its subsets were identiﬁed as

frequent in earlier level. Since there are (cid:0) l

probability can be upper bounded using Union Bound to

k(cid:1) subsets of X of size k, the required

(cid:18)l
1(cid:19)δ +(cid:18)l

2(cid:19)δ + ... +(cid:18)l

l(cid:19)δ ≤ 2lδ.

To get the the necessary background, Lemma 13 provide bounds on the
hashing parameters k, L for Hamming distance case. Their proof is adapted
from [5, 7, 4]. We ﬁrst require Lemma 11, 12 for the same.

Lemma 11 Let {Ii, Ij } be a pair of items s.t. Ham(Ii, Ij) ≤ r, then the proba-
bility that Ii and Ij hash into at least one of the L bucket of size k, is at least
1 − (1 − p1

k)L, where p1 = 1 − r
n .

Proof. Probability that Ii and Ij matches at some particular bit position ≥ p1.
Now, probability that Ii and Ij matches at k positions in a bucket of size k
k. Probability that Ii and Ij don’t matches at k positions in a bucket of size
≥ p1
k. Probability that Ii and Ij don’t matches at k positions in none of
k ≤ 1 − p1
k)L. Probability that Ii and Ij matches in at k positions
the L buckets ≤ (1 − p1
positions in at least one of the L buckets ≥ 1 − (1 − p1

k)L.

Lemma 12 Let {Ii, Ij } be a pair of items s.t. Ham(Ii, Ij) ≥ (1 + ǫ′)r, then
k, where p2 =
probability that {Ii, Ij} hash in a bucket of size k, is at most p2
1 − (1+ǫ′)r

.

n

Proof. Probability that Ii and Ij matches 1 at some particular bit position < p2.
Probability that Ii and Ij matches at k positions in a bucket of size k < p2

k.

i=1 be a set of m vectors in Rn, Iq be a given query vector,
Lemma 13 Let {Ii}m
and Ix∗ (with, 1 ≤ x∗ ≤ m) s.t. Ham(Ix∗ , Iq) ≤ r. If we set our hashing param-
n , p2 = 1 − r(1+ǫ′)
,
eters k = log 1
p2
≤ 1
1+ǫ′ ), then the following two cases are true with probability > 1−δ :

δ(cid:1) (where, p1 = 1 − r

m, and L = mρ log(cid:0) 1

ρ =

n

log 1
p1
log 1
p2

1. for some i ∈ {1, ..., L}, gi(Ix∗ ) = gi(Iq); and
2. total number of collisions with Ix′ s.t. Ham(Ix′ , Iq) > (1 + ǫ′)r is at most L
δ .

Proof. Consider the ﬁrst case, by Lemma 11, we have the following:

Pr[∃i : gi(Ix∗ ) = gi(Iq)] ≥ 1 − (1 − p1

k)L.

If we choose k = log 1
p2

m, we get p1

k = p1

log 1
p2

m

= m

−

log 1
p1
log 1
p2 . Let us denote

log 1
p1
log 1
p2

ρ =

. Then, Pr[∃i : gi(Ix∗ ) = gi(Iq)] ≥ 1 − (1 − m−ρ)L. Now, if we
δ ) ≥

δ(cid:1), then the required probability is 1 − (1 − m−ρ)mρ log( 1

1 − 1
e
Now, let us consider the case 2. Let Ix′ be an item such that Ham(Iq, Ix′ ) >

set L = mρ log(cid:0) 1

> 1 − δ.

log( 1
δ )

r(1 + ǫ′). Then by Lemma 12, we have the following:

Pr[gi(Iq) = gi(Ix′ )] ≤ p2

k = p2

log 1
p2

m

=

1
m

(as we choosed k = log 1
p2

m).

Thus, the expected number of collisions for a particular i is at most 1, and the
expected total number of collisions is at most L (by linearity of expectation).
Now, by Markov’s inequality Pr[Number of Ix′ which are colliding with Iq >
L
δ ] < L
δ )
( L

(after simpliﬁcation).

n−(1+ǫ′)r ≤ 1

= δ. Further, ρ =

= n−r

log 1
p1
log 1
p2

1+ǫ′

Lemma 4 For two itemsets Ix and Iy, Ham(P (Ix), Q(Iy)) = 2(αln − |Ix, Iy|).

Proof. It is easy to verify that with this mapping |P (Ix), Q(Iy)| = |Ix, Iy|.
Let Ham(Ix, Iy) denote the hamming distance between items Ix and Iy. Then,
Ham(P (Ix), Q(Iy)) = |P (Ix)| + |Q(Iy)| − 2|P (Ix), Q(Iy)| = αln − |Ix| + |Ix| +
αln − |Iy| + |Iy| − 2|Ix, Iy| = 2(αln − |Ix, Iy|).

Lemma 5 Algorithm 3 correctly outputs FI(Iq, θ, ε, δ) for all Iq ∈ Dl. Additional
space required is o(m2
l ), which is also the total time overhead. The expected sav-
l ))(cid:1).

ings can be bounded by E[ς(l+1)] ≥ (cid:0)n−o(ml)(cid:1)(cid:0)(cl+1 −2ml+1)+(cl+1 −o(m2

Proof. First, we show that for any query item Iq, Algorithm 3 correctly outputs
FI(Iq, θ, ε, δ) for any query Iq ∈ Dl. Now, if there is an item Ix∗ such that
|Ix∗ , Iq| ≥ θn, then Ham(P (Ix∗ ), Q(Iq)) ≤ 2(αl − θ)n (by Lemma 4). Let p1 be
the probability that P (Ix∗ ) and Q(Iq) matches at some particular bit position,
then p1 ≥ 1 − 2(αln−θn)
. Similarly, if there is an item Ix′ such that
|Ix′ , Iq| ≤ (1 − ε)θn, then Ham(P (Ix′ ), Q(Iq)) ≥ 2(αl − (1 − ε)θ)n. Let p2 be the
probability that P (Ix′ ) and Q(Iq) matches at some particular bit position, then
p2 ≤ 1 − 2(αl−(1−ε)θ)n

n+2αln = 1+2θ
1+2αl

(1+2αl)n = 1+2(1−ε)θ

.
= log(cid:16) 1+2αl
(after simpliﬁcation); the proof easily follows

1+2(1−ε)θ (cid:17) ml; and L = mρ

l log 1

δ , where ρ =

1+2αl
Now, as we have set k = log 1
p2

=

log 1+2θ
1+2αl

log 1
p1
log 1
p2
from Lemma 13.

log 1+2(1−ε)θ

1+2αl

= αl−θ

αl−(1−ε)θ

The space required for hashing an itemset Iq is σ = O(kL) = ˜O (ml

l

, where ρ =

mo(1)
O(kL) = ˜O (ml
which immediately proves the required space and time overhead.

αl−(1−ε)θ = o(1). Time τ require for hashing Ii

. Thus, total time and space overhead is ml

ρ) = mo(1)

αl−θ

l

ρ) =
is also
1+o(1),

O(kL) = ml

The number of bits of any vector required by this LSH-function is φ =
o(1). We know that T N and F P for an itemset Iq ∈ Dl are related by
T N (Iq) = r(Iq) − F P (Iq), and PIq
r(Iq) = 2(cl+1 − ml+1). So, E[P T N (Iq)] =
P r(Iq) − E[P F P (Iq)]. From Lemma 13, E[F P (Iq)] ≤ L. Combining these
facts, we get E[P T N (Iq)] ≥ 2(cl+1 − ml+1) − mlL = 2(cl+1 − ml+1) − mlmρ
l =
2(cl+1 − ml+1) − m1+o(1)
. Now, using the formula for expected savings from
Section 3,

l

E[ς(l + 1)] ≥ (n − O(kL))(cid:16)2(cl+1 − ml+1) − m1+o(1)
(cid:17)
o(1)(cid:17) (2(cl+1 − ml+1) − m1+o(1)

= (cid:16)n − ml
≥ (cid:0)n − o(ml)(cid:1)(cid:0)(cl+1 − 2ml+1) + (cl+1 − o(m2

)

l

l

Lemma 6 For two padded itemsets Ix and Iy, JS(P (Ix), Q(Iy)) =

l ))(cid:1).
2αln−|Ix,Iy| .

|Ix,Iy|

Proof. The Jaccard Similarity between items P (Ix) and Q(Iy) is as follows:
JS(P (Ix), Q(Iy)) = |P (Ix)∩Q(Iy)|
=

|P (Ix)∪Q(Iy)| =

|P (Ix)|+|Q(Iy)|−|P (Ix),Q(Iy )|

αln−|Ix|+|Ix|+αln−|Iy|+|Iy|−|Ix,Iy| =

2αln−|Ix,Iy| .

|P (Ix),Q(Iy)|

|Ix,Iy|

|Ix,Iy|

Lemma 8 Algorithm 4 correctly computes FI(Iq, θ, ε, δ) for all Iq ∈ Dl. Addi-
tional space required is O(λml), and the total time overhead is O((n + λ)ml).
The expected savings is given by E[ς(l + 1)] ≥ 2(1 − δ)(n − λ)(cl+1 − ml+1).

θn

Proof. Now, if there is an item Ix∗ such that |Ix∗ , Iq| ≥ θn, then JS(P (Ix∗ ), Q(Iq)) ≥
(2αl−θ)n (by Lemma 6). As we set ω = (1−ε)θ
orem 7 we have ˆJS(P (Ix∗ ), Q(Iq)) ≥ (1−ǫ)θn

2αl−(1−ε)θ in Algorithm 4, then by The-

(2αl−θ)n , with probability at least 1 − δ.

Similarly, if there is an item Ix′ such that |Ix′ , Iq| < (1 − ε)θn, then

2(αl−(1−ε)θ)n = (1−ε)θ
JS(P (Ix′ ), Q(Iq)) <
rem 7, we have ˆJS(P (Ix′ ), Q(Iq)) < (1+ǫ)(1−ε)θ

(1−ε)θn

We need to set Minhash parameter ǫ such that ˆJS(P (Ia), Q(Iq)) ≥ (1−ǫ)θ

2αl−(1−ε)θ (by Lemma 6). Then by Theo-
2αl−(1−ε)θ , with probability at least 1 − δ.
2αl−θ ≥
αl+(αl−θ)(1−ε) , which ensures |Iq, Ia| ≥ (1 − ε)θn

αlε

(1+ǫ)(1−ε)θ
2αl−(1−ε)θ . This gives, ǫ <
with probability at least 1 − δ.

ωǫ2 log 1

The space required for hashing an itemset Ii (for 1 ≤ i ≤ ml) is σ = O(λ).
Where, λ ≥ 2
αl+(αl−θ)(1−ε) . Total space required for storing
hash table is O(mlλ). Creating hash table require one pass over Dl, then pre-
processing time overhead is O(nml). We perform query on the hash table, query
time overhead is O(λml). Thus, total time overhead ϑ(l + 1) = O((n + λ)ml).

δ and ǫ =

αlε

Now, if an itemset Ia is infrequent with Iq, then Pr[Ia is not reported] ≥ 1−δ.
As there are (cl+1 − ml+1) number of infrequent itemsets at level l + 1, then,

expected number of infrequent items that are not reported E[T N ] ≥ (1−δ)(cl+1−
ml+1). Therefore, E[ς(l + 1)] = (n − λ)E[T N ] ≥ (n − λ)(1 − δ)(cl+1 − ml+1).

Lemma 10 Algorithm 5 outputs all θ-frequent itemsets and only θ-frequent

l

itemsets. Additional space required is O(cid:0)m1+ν
overhead. The expected savings is given by E[ς(l + 1)] ≥ 2(cid:16)n − log ml
(cid:0)(cl+1 − ml+1) − m1+ν

(cid:1), which is also the total time
c − 1(cid:17)

Proof. By Theorem 9, and our choice of hash function HA′ , any pair of similar
itemset will surely collide by our hash function, and will not get missed by the
algorithm. Moreover, false positives will be ﬁlter out by the algorithm in Line 6b
of Algorithm 5. Thus, our algorithm outputs all θ-frequent itemsets and only
θ-frequent itemsets.

(cid:1).

l

l

1
c

ǫ
ct

The space required for hashing an itemset Iq is σ = |HA′ |. Total space re-
l m

quired for creating hash table is O(ml|HA′ |) = O(cid:16)ml2θ ′ǫ+1m
= O(cid:16)m
). Time τ require for hashing Ii is also |HA′|. Thus,
total time overhead required (including both preprocessing and querying) is
ϑ(l + 1) = O(ml|HA′|) = O(m1+ν
), which proves the required space and time
overhead.

l (cid:17) = O(cid:16)mlm

(cid:17) = O(m1+ν

1+ t+ǫ
ct
l

The number of bits of any item required by our hash function is φ = log ml

c +1.
We know that T N and F P for an itemset Iq ∈ Dl are related by T N (Iq) =
r(Iq) − F P (Iq), and PIq
r(Iq) = 2(cl+1 − ml+1). So, E[P T N (Iq)] = P r(Iq ) −
E[P F P (Iq)]. From Theorem 9, E[F P (Iq)] ≤ ψ. Then, we get E[P T N (Iq)] ≥

2(cl+1 − ml+1) − mlψ.

l

1
c

l (cid:17)

As expected savings is ς(LSH, l + 1) = (n − φ) ×PIq

T N (Iq). We have,

− 1(cid:19) (2(cl+1 − ml+1) − mlψ) .
− 1(cid:19)(cid:16)2(cl+1 − ml+1) − ml2θ ′ǫ+1m
l (cid:17) .
− 1(cid:19)(cid:16)2(cl+1 − ml+1) − m
(cid:17) .
− 1(cid:19)(cid:0)2(cl+1 − ml+1) − (m1+ν
)(cid:1) .

1+ t+ǫ
ct
l

1
c

l

E[ς(l + 1)] ≥ (cid:18)n −
≥ (cid:18)n −
≥ 2(cid:18)n −
= 2(cid:18)n −

log ml

c

log ml

c
log ml

log ml

c

c

We end with a quick proof that the following are suﬃcient to ensure overhead
5 > ml > 2n/2, ǫ < 0.25

is less than expected savings. cl+1 ∈ {ω(m2

l+1)}, 2n

l ), ω(m2

and α ≈ θ. Note that, 2cn > 2n and 5ml > 2c·c1/d

ml for any d > 0. These imply,

nc > log(ml) + c · c1/d

n − c1/d >

n − 1 >

log(ml)

c

log(ml)

c

since, c1/d > 1

Furthermore, our conditions imply that 4ǫ(αl − θ)c < c − 1. This implies,

2ǫ(αl − θ)c

c − 1

< 1/2

n − 2 > n/2 >

log(m) >

2ǫ(αl − θ)cn

c − 1

2ǫ(αl − θ)cn

1 =

1
c

+

c − 1

c

>

1
c

+

c − 1
2ǫ(αl − θ)n

log(m)

=

1
c

+

ǫ
ct

= ν

