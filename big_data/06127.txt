6
1
0
2

 
r
a

 

M
9
1

 
 
]
L
C
.
s
c
[
 
 

1
v
7
2
1
6
0

.

3
0
6
1
:
v
i
X
r
a

Sentence Pair Scoring: Towards Uniﬁed Framework

for Text Comprehension

Petr Baudiˇs

FEE CTU Prague

Jan ˇSediv´y

FEE CTU Prague

Department of Cybernetics

Department of Cybernetics

Technick´a 2, Prague, Czech Republic

Technick´a 2, Prague, Czech Republic

baudipet@fel.cvut.cz

sedivjan@fel.cvut.cz

Abstract

We review the task of Sentence Pair Scor-
ing, popular in the literature in various
forms — slanted as Answer Sentence Se-
lection, Paraphrasing, Semantic Text Scor-
ing, Next Utterance Ranking, Recognizing
Textual Entailment or e.g. a component of
Memory Networks.
We argue that such tasks are similar
from the model perspective (especially in
the context of high-capacity deep neu-
ral models) and propose new baselines
by comparing the performance of popu-
lar convolutional, recurrent and attention-
based neural models across many Sen-
tence Pair Scoring tasks and datasets. We
discuss the problem of evaluating ran-
domized models, propose a statistically
grounded methodology, and attempt
to
improve comparisons by releasing new
datasets that are much harder than some
of the currently used well explored bench-
marks.
To address the current research fragmen-
tation in a future-proof way, we introduce
a uniﬁed open source software framework
with easily pluggable models, allowing
easy evaluation on a wide range of seman-
tic natural language tasks. This allows us
to outline a path towards a universal ma-
chine learned semantic model for machine
reading tasks. We support this plan by
experiments that demonstrate reusability
of models trained on different tasks, even
across corpora of very different nature.

1 Introduction

A typical NLP machine learning task involves
classifying a sequence of tokens such as a sen-

tence or a document, i.e. approximating a function
f1(s) ∈ [0, 1] (where f1 may determine a domain,
sentiment, etc.). But there is a large class of prob-
lems that involve classifying a pair of sentences,
f2(s0, s1) ∈ R (where s0, s1 are sequences of to-
kens, typically sentences).

Typically, the function f2 represents some sort
of semantic similarity, that is whether (or how
much) the two sequences are semantically related.
This formulation allows f2 to be a measure for
tasks as wide as topic relatedness, paraphrasing,
degree of entailment, a pointwise ranking task for
answer-bearing sentences or next utterance classi-
ﬁcation.

In this work, we adopt the working assumption
that there exist certain universal f2 type measures
that may be successfuly applied to a wide variety
of semantic similarity tasks — in the case of fully
differentiable models, both architecture-wise and
weight-wise, trained to represent universal seman-
tic comprehension of sentences and adapted to the
given task by just ﬁne-tuning or adapting the ﬁnal
dense layer. Our argument for preferring f2 to f1
in this pursuit is the fact that the other sentence in
the pair is essentially a very complex label when
training the sequence model, which can therefore
discern semantically rich structures and dependen-
cies.

Determining and demonstrating such universal
semantic comprehension models across multiple
tasks remains a few steps ahead, since the re-
search landscape is fragmented in this regard, with
model research typically reported within the con-
text of just a single f2-type task, each dataset re-
quiring sometimes substantial engineering work
before measurements are possible, and results re-
ported in ways that make meaningful model com-
parisons problematic. The primary purpose of
this work is to unify research within a single
framework that employs task-independent mod-

els, task-speciﬁc adaptation modules and uniﬁed
statistically appropriate methodology for report-
ings. To demonstrate the feasibility of pursuing
universal, task-independent f2 models, we show
that even simple neural models learn universal se-
mantic comprehension as we improve their perfor-
mance (even on relatively large datasets) by em-
ploying cross-task transfer learning.

The paper is structured as follows.

In Sec. 2,
we outline possible speciﬁc f2 tasks and available
datasets; in Sec. 3, we survey the popular basic
baselines and the simpler currently employed neu-
ral models on these tasks; ﬁnally, in Sec. 4, we
present model-task performances within a uniﬁed
framework to establish the watermark for future
research as well as gain insight into the suitability
of models across a variety of tasks. In Sec. 5, we
demonstrate that transfer learning across tasks is
helpful to powerfully seed models. We conclude
with Sec. 6, summarizing our ﬁndings and outlin-
ing several future research directions.

2 Tasks and Datasets

The tasks we are aware of that can be phrased as
f2-type problems are listed below. In general, we
have decided to primarily focus on tasks that have
reasonably large and realistically complex datasets
freely available. On the contrary, we have explic-
itly avoided datasets that have licence restrictions
on availability or commercial usage.

2.1 Answer Sentence Selection

Given a question and a set of candidate answer-
bearing sentences, the task here is to rank higher
sentences that are more likely to contain the an-
swer to the question. As it is fundamentally an
Information Retrival task in nature, the model per-
formance is commonly evaluated in terms of Mean
Average Precision (MAP) and Mean Reciprocial
Rank (MRR).

This task is popular in the NLP research com-
munity thanks to the dataset introduced in (Wang
et al., 2007) which we refer to as wang, with
six papers published between February 2015 and
2016 alone and neural models substantially im-
proving over classical approaches based primar-
ily on parse tree edits.1 We can certainly call it
the main research testbed for f2-style task models.

1http://aclweb.org/aclwiki/index.php?
title=Question_Answering_(State_of_the_
art)

This task has also immediate applications e.g. in
Question Answering systems.

In the context of practical applications, the so-
far standard wang dataset has several downsides
we observed when tuning and evaluating our mod-
els, illustrated numerically in Fig. 1 — the set
of candidate sentences is often very small and
quite uneven (which also makes rank-based mea-
sures unstable) and the total number of questions
as well as individual sentence pairs is relatively
small. Furthermore, the validation and test set
are very small which makes for noisy performance
measurements; the splits also seem quite different
in the nature of questions since we see minimum
correlation between performance on the validation
and set tests, which calls the parameter tuning pro-
cedures and epoch selection for early stopping into
question.

Alternative datasets WikiQA (Yang et al., 2015)
and InsuranceQA (Tan et al., 2015) were pro-
posed, but are encumbered by licence restrictions.
Furthermore, we speculate that they may suffer
from many of the problems above2 (even if they
are somewhat larger) and they did not gain much
traction in the research community.

To alleviate the problems listed above, we are
introducing a new dataset yodaqa/curatedv2
based on the curatedv2 question dataset (in-
troduced in (Baudiˇs and ˇSediv´y, 2015), further
denoisiﬁed by Mechanical Turkers) with candi-
date sentences as retrieved by the YodaQA ques-
tion answering system (Baudiˇs, 2015) from En-
glish Wikipedia. For models that can make sub-
stantial use of more data, we also include an even
larger dataset yodaqa/large2470 (though it
has seen less gold standard denoisiﬁcation), with
its splits as supersets of the smaller splits.3

Fig. 1 compares the critical characteristics of
the datasets.
Furthermore, as apparent below,
the baseline performances on the newly proposed
datasets are much lower, which suggests that fu-
ture improvements will be more apparent in eval-
uation.

2For example, InsuranceQA is effectively a classiﬁcation
task rather than a ranking task, which we do not ﬁnd as ap-
pealing in the context of practical applications.

3Note that the wang and yodaqa datasets however share
a common ancestry regarding the set of questions and there
may be some overlaps, even across train and test splits.
Therefore, mixing training and evaluation on wang and yo-
daqa datasets within a single model instance is not advisable.

Dataset
wang

yodaqa/curatedv2
yodaqa/large2470

Ubuntu Dialogue v2

Train size Val. size Test size Val.-Test r #s0
1492
860
2470
38480

1518
82847
120069
189200

1149
17406
55052
195600

-0.111
0.336
0.546

44648
69382
220846

1M

#s1 per s0
34.9 ±158%
196.8 ±98%
160.2 ±97%

10

Figure 1: The Val.-Test column shows inter-trial Pearson’s r of validation and test MRRs, averaged across the models we
benchmarked (see below). The last column includes relative standard deviation of the number of candidate sentences per
question, which corresponds to the variation in the difﬁculty of the ranking task (as well as variation in expected measure
values for individual questions). Ubuntu dataset shows s0 and s1 statistics just on val and test sets (training set are individual
pairs).

2.2 Next Utterance Ranking
(Lowe et al., 2015) proposed a new large-scale
and realistic dataset for an f2-style task of ranking
candidates for the next utterance in a chat dialog,
given the dialog context. The technical formula-
tion of the task is the same as for Answer Sentence
Selection, but semantically, choosing the best fol-
lowup involves different concerns than choosing
an answer-bearing sentence.

The newly proposed Ubuntu Dialogue dataset
is based on IRC chat logs of the Ubuntu commu-
nity technical support channels and contains casu-
ally typed interactions regarding computer-related
problems.4 While the training set consists of in-
dividual labelled pairs of token sequences, evalua-
tion is done by ranking 10 followups to given mes-
sage(s) that are potentially relatively long (even
more than 200 tokens).

Our primary motivation for using this dataset
is its size. The numerical characteristics of this
dataset are shown in Table 1.5 We use the v2
version of the dataset.6 Research published on
this dataset so far relies on simple neural models.
(Lowe et al., 2015) (Kadlec et al., 2015)

2.3 Semantic Textual Similarity
One of the canonical problems for f2-type tasks is
the STS track of the SemEval conferences (Agir-
rea et al., 2015). This is an annual competition
of scoring pairs of sentences from 0 to 5 with
the objective of maximizing correlation (Pearson’s
r) with manually annotated gold standard;
the
data is composed from diverse per-source splits
that range from almost word-by-word paraphrases
of newspaper titles to loosely related user forum

4In a manner, they resemble tweet data, but without the
length description and with heavily technical jargon, com-
mand sequences etc.

5As in past papers, we use only the ﬁrst 1M pairs (10%)

of the training set.

6https://github.com/rkadlec/
ubuntu-ranking-dataset-creator

questions. Every year, the past years are used as
the training set.

Since 2016 results weren’t released at the time
of writing this paper, we use 2012 to 2014 as the
training set, 2014 tweet-news split as the valida-
tion set, and 2015 splits (and the mean score across
splits) as the testing set, making our results com-
parable to 2015 competition entrants. This means
3000 training pairs, 750 validation pairs and 8500
testing pairs. Contrary to Answer Sentence Selec-
tion, state-of-art methods are based on parse tree
alignments (Sultan et al., 2015) and weren’t beaten
by neural models yet.

We also report results for this task on another
dataset from the SemEval conferences, SICK-
2014 (Marelli et al., 2014).
In contrast to the
STS track, it is geared at speciﬁcally benchmark-
ing semantic compositional methods, aiming to
capture only similarities on purely language and
common knowledge level, without relying on do-
main knowledge, and there are no named entities
or multi-word idioms. It consists of 4500 training
pairs, 500 validation pairs and 4927 testing pairs.

2.4 Paraphrase Identiﬁcation
Finally, a popular separately considered task is
Paraphrase Identiﬁcation, which can be thought of
as a special case of Semantic Textual Scoring, but
the task goal is binary classiﬁcation rather than re-
gression of a score on continuous scale.

The canonical dataset is the Microsoft Research
Paraphrase Corpus (Dolan and Brockett, 2005),
consisting of 4076 training and 1725 test pairs; 2/3
of the samples are labelled as 1 (is-a-paraphrase).
State-of-art model uses an ensemble of hand-
crafted overlap features (Ji and Eisenstein, 2013)
and weren’t beaten by neural models (Cheng and
Kartsaklis, 2015) (He et al., 2015) yet.7

7http://aclweb.org/aclwiki/index.php?
title=Paraphrase_Identification_(State_
of_the_art)

2.5 Other

Due to the very wide scope of the ﬁeld, we leave
some popular tasks and datasets as future work. In
particular, this concerns the Recognizing Textual
Entailment task (supported by the SNLI dataset)
(Bowman et al., 2015) and the problem of mem-
ory selection in Memory Networks (supported by
the baBi dataset) (Weston et al., 2015). A more
realistic large paraphrasing dataset based on the
AskUbuntu Stack Overﬂow forum had been re-
cently proposed (Lei et al., 2015).

3 Models

As our goal is a universal text comprehension
model, architecture-wise we focus on neural net-
work models. We assume that the sequence is
transformed using N-dimensional word embed-
dings on input, and employ models that produce
a pair of sentence embeddings E0, E1 from the
sequences of word embeddings e0, e1. Unless
noted otherwise, a Siamese architecture is used
that shares weights among both sentenes.

A scorer module that compares the E0, E1 sen-
tence embeddings to produce a scalar result is con-
nected to the model; for speciﬁc task-model con-
ﬁgurations, we use either the dot-product mod-
1 (representing non-normalized vector
ule E0 · E T
angle, as in e.g. (Yu et al., 2014) or (Weston et
al., 2014)) or the MLP module that takes element-
wise product and sum of the embeddings and feeds
them to a two-layer perceptron with hidden layer
of width 2N (as in e.g. (Tai et al., 2015)).8 For the
STS task, we follow this by score regression using
class interpolation as in (Tai et al., 2015).

When training for a ranking task (Answer Sen-
tence Selection), we use the bipartite ranking ver-
sion of Ranknet (Burges et al., 2005) as the objec-
tive; when training for STS task, we use Pearson’s
r formula as the objective; for binary classiﬁcation
tasks, we use the binary crossentropy objective.

3.1 Baselines

In order to anchor the reported performance, we
report several basic methods. Weighed word
overlaps metrics TF-IDF and BM25 (Robertson
et al., 1995) are inspired by IR research and pro-
vide strong baselines for many tasks. We treat s0

8The motivation is to capture both angle and euclid dis-
tance in multiple weighed sums. Past literature uses absolute
difference rather than sum, but both performed equally in our
experiments and we adopted sum for technical reasons.

as the query and s1 as the document, counting the
number of common words and weighing them ap-
propriately. IDF is determined on the training set.
The avg metric represents the baseline method
when using word embeddings that proved success-
ful e.g. in (Yu et al., 2014) or (Weston et al., 2014),
simply taking the mean vector of the word em-
bedding sequence and training an U weight matrix
N ×2N that projects both embeddings to the same
vector space, Ei = tanh(U · ¯ei), where the MLP
scorer compares them. During training, p = 1/3
standard (elementwise) dropout is applied on the
input embeddings.

A simple extension of the above are the DAN
Deep Averaging Networks (Iyyer et al., 2015),
which were shown to adequately replace much
more complex models in some tasks. Two dense
perceptron layers are stacked between the mean
and projection, relu is used instead of tanh as the
non-linearity, and word-level dropout is used in-
stead of elementwise dropout.

3.2 Recurrent Neural Networks

RNN with memory units are popular models for
processing sentenes (Tan et al., 2015) (Lowe et al.,
2015) (Bowman et al., 2015). We use a bidirec-
tional network with 2N GRU memory units9 (Cho
et al., 2014) in each direction; the ﬁnal unit states
are summed across the two per-direction GRUs to
yield a 2N vector representation of the sentence.
Like in the avg baseline, a projection matrix is ap-
plied on this representation and ﬁnal vectors com-
pared by an MLP scorer. We have found that ap-
plying massive dropout p = 4/5 both on the input
and output of the network helps to avoid overﬁt-
ting even early in the training.

3.3 Convolutional Neural Networks

CNN with sentence-wide pooling layer are also
popular models for processing sentences (Yu et al.,
2014) (Tan et al., 2015) (Severyn and Moschitti,
2015) (He et al., 2015) (Kadlec et al., 2015). We
apply a multi-channel convolution (Kim, 2014)
with single-token channel of N convolutions and
2, 3, 4 and 5-token channels of N/2 convolu-
tions each, relu transfer function, max-pooling
over the whole sentence, and as above a projec-
tion to shared space and an MLP scorer. Dropout

9While the LSTM architecture is more popular, we have
found the GRU results are equivalent while the number of
parameters is reduced.

p = 1/2 is applied both on the input and output of
the convolution network.

3.4 RNN-CNN Model
The RNN-CNN model aims to combine both re-
current and convolutional networks by using the
memory unit states in each token as the new repre-
sentation of the token which is then fed to the con-
volutional network. Inspired by (Tan et al., 2015),
the aim of this model is to allow the RNN to model
long-term dependencies and model contextual rep-
resentations of words, while taking advantage of
the CNN and pooling operation for crisp selection
of the gist of the sentence. We use the same pa-
rameters as for the individual models, except that
we ﬁnd applying dropout detrimental and need to
reduce the number of parameters by using only N
memory units per direction.

3.5 Attention-Based Models
The idea of attention models is to attend preferren-
tially to some parts of the sentence when building
its representation (Hermann et al., 2015) (Tan et
al., 2015) (dos Santos et al., 2016) (Rockt¨aschel et
al., 2015). There are many ways to model atten-
tion, as the initial proof of concept we adopt the
(Tan et al., 2015) model attn1511 which is con-
ceptually simple and easy to implement. It asym-
metrically extends the RNN-CNN model by ex-
tra links from s0 CNN output to the post-recurrent
representation of each s1 token, determining an at-
tention level for each token by weighed sum of
the token vectors, and focusing on the relevant s1
segment by transforming the attention levels using
softmax and multiplying the token representations
by the attention levels before they are fed to the
convolutional network.

Convolutional network weights are not shared
between the two sentences and the convolutional
network output is not projected before applying
the MLP scorer. The CNN used here is single-
channel with 2N convolution ﬁlters 3 tokens wide.

4 Model Performance

4.1 dataset-sts framework
To easily implement models, dataset
loaders
and task adapters in a modular fashion so that
any model can be easily run on any f2-type
task, we have created a new software pack-
age dataset-sts that integrates a variety of
datasets, a Python dataset adapter PySTS and a

Python library for easy construction of deep neu-
ral NLP models for semantic sentence pair scoring
KeraSTS that uses the Keras machine learning li-
brary (Chollet, 2015). The framework is available
for other researchers as open source on GitHub.10

4.2 Experimental Setting

We use N = 300 dimensional GloVe embed-
dings matrix pretrained on Wikipedia 2014 + Gi-
gaword 5 (Pennington et al., 2014) that we keep
adaptable during training; words in the training set
not included in the pretrained model are initial-
ized by random vectors uniformly sampled from
[−0.25, +0.25] to match the embedding standard
deviation.

Word overlap is an important feature in many
f2-type tasks (Yu et al., 2014) (Severyn and Mos-
chitti, 2015), especially when the sentences may
contain named entities, numeric or other data
for which no embedding is available. As a
workaround, ensemble of world overlap count and
neural model score is typically used to produce the
ﬁnal score. We try to adopt a more ﬂexible ap-
proach, extending the embedding of each input to-
ken by several extra dimensions carrying boolean
ﬂags — bigram overlap, unigram overlap (except
stopwords and interpunction), and whether the to-
ken starts with a capital letter or is a number.

Particular hyperparameters are tuned primar-
ily on the yodaqa/curatedv2 dataset unless
noted otherwise in the respective results table
caption. We apply 10−4 L2 regularization and
use Adam optimization with standard parameters
(Kingma and Ba, 2014). In the answer selection
tasks, we train on 1/4 of the dataset in each epoch.
After training, we use the epoch with best vali-
dation performance; sadly, we typically observe
heavy overﬁtting as training progresses and rarely
use a model from later than a couple of epochs.

4.3 Evaluation Methodology

We report model performance averaged across 16
training runs (with different seeds). A consid-
eration we must emphasize is that randomness
plays a large role in neural models both in terms
of randomized weight initialization and stochastic
dropout. For example, the typical methodology for
reporting results on the wang dataset is to evalu-
ate and report a single test run after tuning on the

10Link redacted. Src snapshot included in submission.

Model

MSR acc MSR F1

(Ji and Eisenstein, 2013)

(He et al., 2015)
always y = 1

TF-IDF
BM25

avg

DAN

RNN

CNN

RNN-CNN
attn1511

0.804
0.786
0.665
0.695
0.695
0.704
±0.004
0.704
±0.005
0.687
±0.030
0.695
±0.016
0.702
0.708

0.859
0.847
0.799
0.811
0.811
0.803
±0.005
0.802
±0.010
0.786
±0.030
0.792
±0.020
0.812
0.804

Figure 5: Model accuracy and F-measure on the MSR Para-
phrase dataset.

dev set,11 but wang test MRR has empirical stan-
dard deviation of 0.025 across repeated runs of our
attn1511 model, which is more than twice the gap
between every two successive papers pushing the
state-of-art on this dataset! See the ∗-marked sam-
ple in Fig. 2 for a practical example of this phe-
nomenon.

Furthermore, on more complex tasks (Answer
Sentence Selection in particular, see Fig. 1) the
validation set performance is not a great approx-
imator for test set performance and a strategy like
picking training run with best validation perfor-
mance would lead just to overﬁtting on the vali-
dation set.

To allow comparison between models (and with
future models), we therefore report also 95% con-
ﬁdence intervals for each model performance es-
timate, as determined from the empirical standard
deviation using Student’s t-distribution.12

4.4 Results
In Fig. 2 to 5, we show the cross-task performance
of our models. We can observe an effect analo-
gous to what has been described in (Kadlec et al.,
2015) — when the dataset is smaller, CNN models
are preferrable, while larger dataset allows RNN
models to capture the text comprehension task bet-

11Conﬁrmed by personal communication with paper au-

thors.

12Over larger number of samples, this estimate converges
to the normal distribution conﬁdence levels. Note that the
conﬁdence interval covers the range of the true expected per-
formance, not performance individually measured samples.

ter. IR baselines provide strong competition and
ﬁnding new ways to ensemble them with models
should prove beneﬁcial in the future.13 This is
especially apparent in the new Answer Sentence
Selection datasets that have very large number of
sentence candidates per question. The attention
mechanism also has the highest impact in this kind
of Information Retrieval task.

While our models clearly yet lag behind the
state-of-art on the paraphrasing and STS tasks, it
establishes the new baseline on the Ubuntu Dia-
logue dataset and it is not possible to statistically
determine its relation to state-of-art on the wang
Answer Sentence Selection dataset.

Note to readers: We apologize for the fact that
the numbers listed here are not entirely ﬁnal. The
Ubuntu dataset shows single val samples rather
than 16-way test results and some other measure-
ments are with fewer than 16 runs and conﬁdence
intervals not included. We expect to catch up with
the measurements in just a couple of days. and do
not expect paper conclusions to be affected in any
way.

5 Model Reusability

To conﬁrm the hypothesis that our models learn a
generic task akin to some form of text comprehen-
sion, we tried to train a model on the large Ubuntu
Dialogue dataset (Next Utterance Ranking task)
and then transfer the weights and retrain the model
instance on the smaller curatedv2 dataset (An-
swer Sentence Selection task).

for

We used the RNN model

the experi-
ment in a conﬁguration with dot-product scorer
(which works much better on the Ubuntu dataset).
The conﬁguration, when trained from scratch on
curatedv2, achieves MRR 0.371±0.023, while
with Ubuntu Dialogue pre-training it achieves
MRR 0.493 ± 0.021. This represents a jump of
about 0.12 points and the resulting performance
is comparable to a much more sophisticated at-
tention model that is trained exclusively on the
curatedv2 dataset.

During our experiments, we have noticed that
it is important not to apply dropout during re-
training if it wasn’t applied during the original
training. We have also tried freezing the weights
of some layers, but this never yielded a signiﬁcant

13We have tried simple averaging of predictions (as per
(Kadlec et al., 2015)), but the beneﬁt was small and incon-
sistent.

Model

wang MAP wang MRR y.c.v2 MAP y.c.v2 MRR l2470 MAP l2470 MRR

(Tan et al., 2015)

(dos Santos et al., 2016)

TF-IDF
BM25

avg

DAN

RNN

CNN

RNN-CNN

attn1511

∗attn1511

0.728
0.753
0.578
0.630
0.607
±0.006
0.643
±0.100
0.649
±0.011
0.691
±0.007
0.717
±0.007
0.708
±0.009
0.756

0.832
0.851
0.709
0.765
0.690
±0.010
0.735
±0.009
0.743
±0.010
0.770
±0.010
0.798
±0.011
0.790
±0.013
0.859

0.243
0.294
0.230
±0.002
0.233
±0.003
0.229
±0.006
0.229
±0.005
0.238
±0.008
0.275
±0.007

0.338
0.485
0.329
±0.004
0.354
±0.010
0.342
±0.011
0.309
±0.010
0.345
±0.015
0.469
±0.014

0.267
0.314
0.263
±0.002
0.273
±0.003
0.262
±0.003

0.363
0.491
0.362
±0.006
0.387
±0.008
0.381
±0.008

0.288
±0.006

0.431
±0.018

Figure 2: Model results on the Answer Sentence Selection task, as measured on the wang, yodaqa/curatedv2 and
yodaqa/large2470 datasets.
∗ Demonstration of the problematic single-measurement result reporting in past literature — an outlier sample in our 16-trial
attn1511 benchmark that would score as a state of art; in total, two outliers in the trial (12.5%) scored better than (Tan et al.,
2015).

improvement.

6 Conclusion

We have uniﬁed a variety of tasks in a single sci-
entiﬁc framework of sentence pair scoring, and
demonstrated a platform for general modelling of
this problem and aggregate benchmarking of these
models across many datasets. Promising initial
transfer learning results suggest that a quest for
generic neural model capable of task-independent
text comprehension is becoming a meaningful pur-
suit. The open source nature of our framework
and the implementation choice of a popular and
extensible deep learning library allows for high
reusability of our research and easy extensions
with further more advanced models.

6.1 Future Work
Due to a very wide breadth of the f2-problem
scope, we were not able to cover all major tasks
at once. Developing adapters, models and bench-
marks for the task of Recognizing Textual Entail-
ment remains the major next step, as well as for
the problem of Hypothesis Evidencing where we
would like to study the task of producing a binary
classiﬁcation of a hypothesis sentence s0 based
on a number of memory sentences s1. We are in
the process of developing realistic, hard datasets

based on real-world event yes/no questions and
newspaper snippets, as well as solving school test
exams based on textbook and encyclopedia snip-
pets.

We also did not include several major classes of
models in our initial evaluation. Most notably, this
includes serial RNNs with attention as used e.g.
for the RTE task (Rockt¨aschel et al., 2015), and
the skip-thoughts method of sentence embedding.
(Kiros et al., 2015)

We believe that the Ubuntu Dialogue Dataset
results demonstrate that the time is ripe to push
the research models further towards the real world
by allowing for wider sentence variability and less
explicit supervision. But in particular, we believe
that new models should be developed and tested
on tasks with long sentences and wide vocabulary.
Pushing the models to their limits will allow better
differentiation regarding how much semantic text
comprehension they are able to exhibit, which in
turn should improve reusability, but datasets with
low baseline performance will also better empha-
size relative performance difference of individual
models and make statistical signiﬁcance more at-
tainable.

In terms of models, recent work in many NLP
domains (dos Santos et al., 2016) (Cheng et al.,
2016) (Kumar et al., 2015) clearly points towards

Model MRR 1-2 R@1

1-10 R@1

1-10 R@2

1-10 R@5

∗ TF-IDF
∗ RNN
∗ LSTM

avg
DAN
RNN
CNN

attn1511

0.618
0.616
0.786
0.659
0.773

0.749
0.777
0.869
0.787
0.783
0.910
0.805
0.903

0.488
0.379
0.552
0.464
0.465
0.671
0.528
0.654

0.587
0.561
0.721
0.602
0.594
0.805
0.639
0.788

0.763
0.836
0.924
0.831
0.823
0.953
0.842
0.948

Figure 3: Model results on the Ubuntu Dialogue next utterance ranking task. Models use slightly speciﬁc conﬁguration due
to much bigger dataset (in terms of both samples and sentence lengths) — only 160 tokens are considered per input, no dropout
is applied, RNN use N memory units, projection matrix is only N × N and the dot-product scorer is used for comparison. The
attn1511 model furthermore has only N/2 RNN memory units and N/2 CNN ﬁlters.
∗ Exact models from (Lowe et al., 2015) reran on the v2 version of the dataset (by personal communication with Ryan Lowe)
— note that the results in (Lowe et al., 2015) and (Kadlec et al., 2015) are on v1 and not directly comparable.

Model

DLS@CU-S1
ECNU run1

(Tai et al., 2015)

TF-IDF
BM25

avg

DAN

RNN

CNN

RNN-CNN

ans.for.
0.739

ans.stud
0.773

belief
0.749

headline
0.825

images
0.864

mean
0.802

SICK2014

0.687

0.534

0.622
0.632
0.512

0.676
0.690
0.654

0.714
0.718
0.676

0.725
0.725
0.670

0.669
0.607
0.678
0.626
0.403
0.583
±0.027 ±0.009 ±0.039 ±0.011 ±0.013 ±0.134
0.476
0.620
±0.020 ±0.006 ±0.035 ±0.006 ±0.008 ±0.119
0.384
0.559
±0.049 ±0.040 ±0.047 ±0.067 ±0.044 ±0.110
0.495
0.647
±0.006 ±0.006 ±0.006 ±0.006 ±0.004 ±0.098
0.523
0.670
±0.007 ±0.005 ±0.010 ±0.007 ±0.005 ±0.094

0.608

0.575

0.697

0.707

0.606

0.623

0.689

0.727

0.658

0.667

0.699

0.676

0.717

0.734

0.841
0.868
0.478
0.474
0.621
±0.022
0.649
±0.020
0.686

0.750

Figure 4: Model results (Pearson’s r) on the STS task — individual splits of the STS 2015 competition, mean STS 2015
performance and SICK2014 test set performance.

various forms of attention modelling to remove the
bottleneck of having to compress the full spectrum
of semantics into a single vector of ﬁxed dimen-
sionality. Another promising approach might be
giving the network more ﬂexibility regarding the
ﬁnal representation, for example by allowing it to
remember a set of “facts” derived from each sen-
tence; related work has been done on end-to-end
differentiable shift-reduce parsers with LSTM as
stack cells (Dyer et al., 2015).

In this paper, we have shown the beneﬁt of train-
ing a model on a single dataset and then apply-
ing it on another dataset. One open question is
whether we could jointly train a model on multi-
ple tasks simultaneously (even if they do not share
some output layers), for example by task-by-task
batch interleaving during training. Another way to

improve training would be to include extra super-
vision similar to the token overlap features that we
already employ; for example, in the new Answer
Sentence Selection task datasets, we can explicitly
mark the actual tokens representing the answer.

Acknowledgments

This work was ﬁnancially supported by the Grant Agency of
the Czech Technical University in Prague, grant No. SGS16/
084/OHK3/1T/13, and the Forecast Foundation. Computa-
tional resources were provided by the CESNET LM2015042
and the CERIT Scientiﬁc Cloud LM2015085, provided under
the programme “Projects of Large Research, Development,
and Innovations Infrastructures.”

We’d like to thank Tom´aˇs Tunys, Rudolf Kadlec, Ryan
Lowe, Cicero Nogueira dos santos and Bowen Zhou for help-
ful discussions and their insights, and Silvestr Stanko and Jiˇr´ı
N´advorn´ık for their software contributions.

References
Eneko Agirrea, Carmen Baneab, Claire Cardiec,
Daniel Cerd, Mona Diabe, Aitor Gonzalez-Agirrea,
Weiwei Guof, Inigo Lopez-Gazpioa, Montse Mar-
itxalara, Rada Mihalceab, et al. 2015. Semeval-
2015 task 2: Semantic textual similarity, english,
spanish and pilot on interpretability.

Petr Baudiˇs and Jan ˇSediv´y. 2015. Modeling of the
question answering task in the YodaQA system. In
Experimental IR Meets Multilinguality, Multimodal-
ity, and Interaction, pages 222–228. Springer.

Petr Baudiˇs. 2015. YodaQA: A Modular Question An-
swering System Pipeline. In POSTER 2015 - 19th
International Student Conference on Electrical En-
gineering.

Samuel R. Bowman, Gabor Angeli, Christopher Potts,
and Christopher D. Manning. 2015. A large an-
notated corpus for learning natural language infer-
ence.
In Proceedings of the 2015 Conference on
Empirical Methods in Natural Language Processing
(EMNLP). Association for Computational Linguis-
tics.

Chris Burges, Tal Shaked, Erin Renshaw, Ari Lazier,
Matt Deeds, Nicole Hamilton, and Greg Hullender.
2005. Learning to rank using gradient descent. In
Proceedings of the 22nd international conference on
Machine learning, pages 89–96. ACM.

Jianpeng Cheng and Dimitri Kartsaklis. 2015. Syntax-
aware multi-sense word embeddings for deep com-
positional models of meaning.
arXiv preprint
arXiv:1508.02354.

Jianpeng Cheng, Li Dong, and Mirella Lapata. 2016.
Long short-term memory-networks for machine
reading. CoRR, abs/1601.06733.

KyungHyun Cho, Bart van Merrienboer, Dzmitry Bah-
danau, and Yoshua Bengio. 2014. On the properties
of neural machine translation: Encoder-decoder ap-
proaches. CoRR, abs/1409.1259.

Franc¸ois Chollet. 2015. Keras. https://github.

com/fchollet/keras.

William B Dolan and Chris Brockett. 2005. Automati-
cally constructing a corpus of sentential paraphrases.

C´ıcero Nogueira dos Santos, Ming Tan, Bing Xiang,
and Bowen Zhou. 2016. Attentive pooling net-
works. CoRR, abs/1602.03609.

Chris Dyer, Miguel Ballesteros, Wang Ling, Austin
Matthews, and Noah A. Smith. 2015. Transition-
based dependency parsing with stack long short-
term memory. CoRR, abs/1505.08075.

Hua He, Kevin Gimpel, and Jimmy Lin. 2015. Multi-
perspective sentence similarity modeling with con-
volutional neural networks.

Karl Moritz Hermann, Tomas Kocisky, Edward
Grefenstette, Lasse Espeholt, Will Kay, Mustafa Su-
leyman, and Phil Blunsom. 2015. Teaching ma-
chines to read and comprehend. In Advances in Neu-
ral Information Processing Systems, pages 1684–
1692.

Mohit Iyyer, Varun Manjunatha, Jordan Boyd-Graber,
and Hal Daum´e III. 2015. Deep unordered compo-
sition rivals syntactic methods for text classiﬁcation.

Yangfeng Ji and Jacob Eisenstein. 2013. Discrimina-
tive improvements to distributional sentence similar-
ity. In EMNLP.

Rudolf Kadlec, Martin Schmid, and Jan Kleindienst.
2015. Improved deep learning baselines for ubuntu
corpus dialogs. arXiv preprint arXiv:1510.03753.

Yoon Kim.

2014.

works for sentence classiﬁcation.
arXiv:1408.5882.

Convolutional neural net-
arXiv preprint

Diederik Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,
Richard Zemel, Raquel Urtasun, Antonio Torralba,
and Sanja Fidler.
Skip-thought vectors.
In Advances in Neural Information Processing Sys-
tems, pages 3276–3284.

2015.

Ankit Kumar, Ozan Irsoy, Jonathan Su, James Brad-
bury, Robert English, Brian Pierce, Peter Ondruska,
Ishaan Gulrajani, and Richard Socher. 2015. Ask
me anything: Dynamic memory networks for natu-
ral language processing. CoRR, abs/1506.07285.

Tao Lei, Hrishikesh Joshi, Regina Barzilay, Tommi S.
Jaakkola, Kateryna Tymoshenko, Alessandro Mos-
chitti, and Llu´ıs M`arquez i Villodre. 2015. De-
noising bodies to titles: Retrieving similar ques-
tions with recurrent convolutional models. CoRR,
abs/1512.05726.

Ryan Lowe, Nissan Pow, Iulian Serban, and Joelle
Pineau. 2015. The ubuntu dialogue corpus: A large
dataset for research in unstructured multi-turn dia-
logue systems. CoRR, abs/1506.08909.

Marco Marelli, Luisa Bentivogli, Marco Baroni, Raf-
faella Bernardi, Stefano Menini, and Roberto Zam-
parelli. 2014. Semeval-2014 task 1: Evaluation of
compositional distributional semantic models on full
sentences through semantic relatedness and textual
entailment. SemEval-2014.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vectors for
word representation. In Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 1532–
1543.

Stephen E Robertson, Steve Walker, Susan Jones, et al.

1995. Okapi at trec-3.

Tim Rockt¨aschel, Edward Grefenstette, Karl Moritz
Hermann, Tom´as Kocisk´y, and Phil Blunsom. 2015.
Reasoning about entailment with neural attention.
CoRR, abs/1509.06664.

Aliaksei Severyn and Alessandro Moschitti.

2015.
Learning to rank short text pairs with convolutional
deep neural networks.
In Proceedings of the 38th
International ACM SIGIR Conference on Research
and Development in Information Retrieval, pages
373–382. ACM.

Md Arafat Sultan, Steven Bethard, and Tamara Sum-
ner. 2015. Dls@ cu: Sentence similarity from word
alignment and semantic vector composition. In Pro-
ceedings of the 9th International Workshop on Se-
mantic Evaluation, pages 148–153.

Kai Sheng Tai, Richard Socher, and Christopher D.
Manning. 2015.
Improved semantic representa-
tions from tree-structured long short-term memory
networks. CoRR, abs/1503.00075.

Ming Tan, Bing Xiang, and Bowen Zhou. 2015. Lstm-
based deep learning models for non-factoid answer
selection. CoRR, abs/1511.04108.

Mengqiu Wang, Noah A Smith, and Teruko Mita-
mura. 2007. What is the jeopardy model? a quasi-
synchronous grammar for qa.
In EMNLP-CoNLL,
volume 7, pages 22–32.

Jason Weston, Sumit Chopra, and Antoine Bordes.

2014. Memory networks. CoRR, abs/1410.3916.

Jason Weston, Antoine Bordes, Sumit Chopra, and
Tomas Mikolov. 2015. Towards ai-complete ques-
tion answering: A set of prerequisite toy tasks.
CoRR, abs/1502.05698.

Yi Yang, Wen-tau Yih, and Christopher Meek. 2015.
Wikiqa: A challenge dataset for open-domain ques-
tion answering.

Lei Yu, Karl Moritz Hermann, Phil Blunsom, and
Stephen Pulman. 2014. Deep learning for answer
sentence selection. CoRR, abs/1412.1632.

