6
1
0
2

 
r
a

 

M
5
1

 
 
]

.

C
O
h
t
a
m

[
 
 

1
v
2
6
5
4
0

.

3
0
6
1
:
v
i
X
r
a

A Gradient-based Kernel Optimization Approach for Parabolic

Distributed Parameter Control Systems∗
Zhigang Ren1, Chao Xu1,†, Qun Lin2, and Ryan Loxton2

1State Key Laboratory of Industrial Control Technology and Institute of Cyber-Systems & Control, Zhejiang

2Department of Mathematics & Statistics, Curtin University, Perth, Western Australia, Australia

University, Hangzhou, Zhejiang, China

Abstract

This paper proposes a new gradient-based optimization approach for designing optimal feedback
kernels for parabolic distributed parameter systems with boundary control. Unlike traditional kernel
optimization methods for parabolic systems, our new method does not require solving non-standard
Riccati-type or Klein-Gorden-type partial diﬀerential equations (PDEs). Instead, the feedback kernel
is parameterized as a second-order polynomial whose coeﬃcients are decision variables to be tuned
via gradient-based dynamic optimization, where the gradient of the system cost functional (which
penalizes both kernel and output magnitude) is computed by solving a so-called “costate” PDE in
standard form. Special constraints are imposed on the kernel coeﬃcients to ensure that, under mild
conditions, the optimized kernel yields closed-loop stability. Numerical simulations demonstrate the
eﬀectiveness of the proposed approach.

Key words: Gradient-based Optimization, Feedback Kernel Design, Boundary Stabilization, Parabolic
PDE Systems

1

Introduction

Spatial-temporal evolutionary processes (STEPs) are usually modeled by partial diﬀerential equations

(PDEs), which are commonly referred to as distributed parameter systems (DPS) in the ﬁeld of auto-

matic control. The parabolic system is an important type of DPS that describes a wide range of natural

phenomena, including diﬀusion, heat transfer, and fusion plasma transport. Over the past few decades,

control theory for the parabolic DPS has developed into a mature research topic at the interface of

engineering and applied mathematics [2, 4, 14].

There are two main control synthesis approaches for the parabolic DPS: the design-then-discretize

framework and the discretize-then-design framework. In the design-then-discretize framework, analyti-

cal techniques are ﬁrst applied to derive equations for the optimal controller—e.g., the Riccati equation

for optimal control or the backstepping kernel equation for boundary stabilization—and then these equa-

tions are solved using numerical discretization techniques. In the discretize-then-design framework, the

sequence is reversed: the inﬁnite dimensional PDE system is ﬁrst discretized to obtain a ﬁnite dimen-

sional system, and then various controller synthesis and numerical optimization techniques are applied
∗This work was supported by the National Natural Science Foundation of China (F030119-61104048, 61320106009)

and Fundamental Research Funds for the Central Universities (2014FZA5010).

†Correspondence to: Chao Xu, Email: cxu@zju.edu.cn

1

to solve the corresponding discretized problem. Popular approaches for obtaining the ﬁnite dimen-

sional approximation in the discretize-then-design framework include mesh-associated discretization

techniques and model order reduction (MOR) techniques. Examples of mesh-associated discretization

techniques include the ﬁnite diﬀerence method [17], the ﬁnite element method [17], the ﬁnite volume

method [17], and the spectral method [5]. MOR methods include the proper orthogonal decomposi-

tion method [6] and the balanced truncation method [15], both of which exploit system input-output

properties [1]. Since MOR techniques can generate low-order models without compromising solution

accuracy, they are popular for dealing with complex STEPs, which arise frequently in applications such

as plasma physics, ﬂuid ﬂow, and heat and mass transfer (e.g., see [24] and the references cited therein).

The linear quadratic (LQ) control framework, a widely-used technique in controller synthesis, is well-

deﬁned in inﬁnite dimensional function spaces to deal with the parabolic DPS (e.g., [2,4]). However, the

LQ control framework requires solving Riccati-type diﬀerential equations, which are nonlinear parabolic

PDEs of dimension one greater than the original parabolic PDE system. For example, to generate an

optimal feedback controller for a scalar heat equation, a Riccati PDE deﬁned over a rectangular domain

must be solved [16]. Hence, the LQ approach does not actually solve the controller synthesis problem

directly, but instead converts it into another problem (i.e., solve a Riccati-type PDE) that is still

extremely diﬃcult to solve from a computational point of view.

One of the major advances in PDE control in recent years has been the so-called inﬁnite dimensional

Voltera integral feedback, or the backstepping method (e.g., [9, 13]). Instead of Riccati-type PDEs, the

backstepping method requires solving the so-called kernel equations—linear Klein-Gorden-type PDEs

for which the successive approach can be used to obtain explicit solutions. This method was originally

developed for the stabilization of one dimensional parabolic DPS and then extended to ﬂuid ﬂows [21],

magnetohydrodynamic ﬂows [22,25], and elastic vibration [8]. In addition, the backstepping method can

also be applied to achieve full state feedback stabilization and state estimation of PDE-ODE cascade

systems [18].

In this paper, we propose a new framework for control synthesis for the parabolic DPS. This new

framework does not require solving Riccati-type or Klein-Gorden-type PDEs.

Instead, it requires

solving a so-called “costate” PDE, which is much easier to solve from a computational viewpoint. In

fact, many numerical software packages, such as Comsol Multiphysics and MATLAB PDE ToolBox,

can be used to generate numerical solutions for the costate PDE. The Riccati PDEs, on the other hand,

are usually not in standard form and thus cannot be solved using oﬀ-the-shelf software packages. The

optimization approach proposed in this paper is motivated by the well-known PID tuning problem, in

which the coeﬃcients in a PID controller need to be selected judiciously to optimize system performance.

Relevant literature includes reference [7], where extremum seeking algorithms are used to tune the PID

parameters; reference [10], where the PID tuning problem is reformulated into a nonlinear optimization

problem, and subsequently solved using numerical optimization techniques; and reference [23], where

the iterative learning tuning method is used to update the PID parameters whenever a control task is

repeated. The current paper can be viewed as an extension of these optimization-based feedback design

ideas to inﬁnite dimensional systems.

The remainder of this paper is organized as follows.

In Section 2, we formulate two parameter

optimization problems for a class of unstable linear parabolic diﬀusion-reaction PDEs with control

actuation at the boundary: the ﬁrst problem involves optimizing a set of parameters that govern the

feedback kernel; the second problem is a modiﬁcation of the ﬁrst problem with additional constraints

to ensure closed-loop stability. In Section 3, we derive the gradients of the cost and constraint functions

2

for the optimization problems in Section 2. Then, in Section 4, we present a numerical algorithm, which

is based on the results obtained in Section 3, for determining the optimal feedback kernel. Section 5

presents the numerical simulation results. Finally, Section 6 concludes the paper by proposing some

further research topics.

2 Problem Formulation

2.1 Feedback Kernel Optimization

We consider the following parabolic PDE system:



yt(x, t) = yxx(x, t) + cy(x, t),

y(0, t) = 0,

y(1, t) = u(t),

y(x, 0) = y0(x),

(2.1a)

(2.1b)

(2.1c)

(2.1d)

where c > 0 is a given constant and u(t) is a boundary control. It is well known that the uncontrolled

version of system (2.1) is unstable when the constant c is suﬃciently large [9]. Thus, it is necessary

to design an appropriate feedback control law for u(t) to stabilize the system. According to the LQ

control [16] and backstepping synthesis approaches [9], the optimal feedback control law takes the

following form:

(2.2)
where the feedback kernel K(1, ξ) is obtained by solving either a Riccati-type or a Klein-Gorden-type
PDE. By introducing the new notation k(ξ) = K(1, ξ), we can write the feedback control policy (2.2)
in the following form:

u(t) =

0

K(1, ξ)y(ξ, t)dξ,

(cid:90) 1

(cid:90) 1

0

u(t) =

k(ξ)y(ξ, t)dξ.

The corresponding closed-loop system is

yt(x, t) = yxx(x, t) + cy(x, t),

y(x, 0) = y0(x),

y(0, t) = 0,

(cid:90) 1

0

y(1, t) =

k(ξ)y(ξ, t)dξ.

(2.3)

(2.4a)

(2.4b)

(2.4c)

(2.4d)

In reference [9], the backstepping method is used to express the optimal feedback kernel in terms of the

ﬁrst-order modiﬁed Bessel function. More speciﬁcally,

,

(2.5)

where I1 is the ﬁrst-order modiﬁed Bessel function given by

K(1, ξ) = −cξ

∞(cid:88)

I1(ω) =

I1((cid:112)c(1 − ξ2))
(cid:112)c(1 − ξ2)

ω2n+1

.

22n+1n!(n + 1)!

n=0

3

Figure 2.1: The feedback kernel (2.5) for various values of c.

The feedback kernel (2.5) is plotted in Figure 2.1 for diﬀerent values of c. Note that its shape is
similar to a quadratic function. Note also that K(1, ξ) = 0 when ξ = 0. Accordingly, motivated by the
quadratic behavior exhibited in Figure 2.1, we express k(ξ) in the following parameterized form:

k(ξ; Θ) = θ1ξ + θ2ξ2,

where Θ = (θ1, θ2)(cid:62) is a parameter vector to be optimized.

Moreover, we assume that the parameters must satisfy the following bound constraints:

a1 ≤ θ1 ≤ b1,

a2 ≤ θ2 ≤ b2,

(2.6)

(2.7)

where a1, a2, b1 and b2 are given bounds.

Let y(x, t; Θ) denote the solution of the closed-loop system (2.4) with the parameterized kernel

(2.6). The results in [20] ensure that such a solution exists and is unique. Our goal is to stabilize the

closed-loop system with minimal energy input. Accordingly, we consider the following cost functional:

(cid:90) T

(cid:90) 1

0

0

(cid:90) 1

0

g0(Θ) =

1
2

y2(x, t; Θ)dxdt +

1
2

k2(x; Θ)dx.

(2.8)

This cost functional consists of two terms: the ﬁrst term penalizes output deviation from zero (stabi-

lization); the second term penalizes kernel magnitude (energy minimization). We now state our kernel

optimization problem formally as follows.

Problem P1. Given the PDE system (2.4) with the parameterized kernel (2.6), ﬁnd an optimal pa-
rameter vector Θ = (θ1, θ2)(cid:62) such that the cost functional (2.8) is minimized subject to the bound
constraints (2.7).

2.2 Closed-Loop Stability

Since (2.8) is a ﬁnite-time cost functional, there is no guarantee that the optimized kernel (2.6) generated
by the solution of Problem P1 stabilizes the closed-loop system (2.4) as t → ∞. Nevertheless, we

4

00.20.40.60.81−70−60−50−40−30−20−100ξK(1,ξ)  c=10c=12c=14c=16c=20c=30now show that, by analyzing the solution structure of (2.4), additional constraints can be added to
Problem P1 to ensure closed-loop stability.

Using the separation of variables approach, we decompose y(x, t) as follows:

y(x, t) = X (x)T (t).

Substituting (2.9) into (2.4a), we obtain

X (x) ˙T (t) = X (cid:48)(cid:48)(x)T (t) + cX (x)T (t),

where

˙T (t) =
X (cid:48)(cid:48)(x) =

Furthermore, from the boundary conditions (2.4c) and (2.4d),

dT (t)
,
dt
d2X (x)
dx2

.

(cid:90) 1

0

X (0)T (t) = 0, X (1)T (t) =

k(ξ; Θ)X (ξ)T (t)dξ.

(2.9)

(2.10)

(2.11)

(2.12)

(2.13)

Thus, we immediately obtain

X (1) =

(cid:90) 1

X (0) = 0,

k(ξ; Θ)X (ξ)dξ.

Rearranging (2.10) gives

0

X (cid:48)(cid:48)(x) + cX (x)

X (x)

˙T (t)
T (t)

.

=

This equation must hold for all x and t. Hence, there exists a constant σ (an eigenvalue) such that

Clearly,

X (cid:48)(cid:48)(x) + cX (x)

X (x)

˙T (t)
T (t)

=

= σ.

T (t) = T0eσt,

(2.14)

(2.15)

where T0 = T (0) is a constant to be determined.

To solve for X (x), we must consider three cases: (i) c < σ; (ii) c = σ; (iii) c > σ. In cases (i) and

(ii), the general solutions of (2.14) are, respectively,

and

√
X (x) = X0e

σ−cx + X1e−√

σ−cx,

X (x) = X0 + X1x,

where X0 and X1 are constants to be determined from the boundary conditions (2.12) and (2.13). Then
the corresponding solutions of (2.4) are

y(x, t) = X0T0e

√

σ−cx+σt + X1T0e−√

σ−cx+σt,

5

and

y(x, t) = X0T0eσt + X1T0xeσt.

These solutions are clearly unstable because 0 < c ≤ σ. Thus, we want to choose the parameters θ1
and θ2 so that the unique solution of (2.4) satisﬁes case (iii) instead of cases (i) and (ii).

In case (iii), the general solution of (2.14) is
√
√
c − σx) + X1 sin(

X (x) = X0 cos(

c − σx),

(2.16)

where X0 and X1 are constants to be determined from the boundary conditions (2.12) and (2.13).
Substituting (2.16) into (2.12), we obtain

X (0) = X0 = 0.

(2.17)

Hence,

√
X (x) = X1 sin(

c − σx).

To simplify the notation, we introduce a new variable α =

√

(2.18)
c − σ. Substituting (2.18) into condition

(cid:90) 1
(cid:90) 1

0

0

(cid:90) 1

0

θ1ξ sin(αξ)dξ = θ1

(cid:90) 1

0

θ2ξ2 sin(αξ)dξ = −θ2

(cid:18) cos α

α

(cid:90) 1

0

0

(cid:90) 1
(cid:18) sin α
α2 − cos α
(cid:19)

α

− 2 sin α
α2

+

(cid:19)

.

(2.19)

(2.20)

.

(2.21)

2θ2(cos α − 1)

α3

(2.13), we have

and thus

X1 sin α = X1

θ1ξ sin(αξ)dξ + X1

θ2ξ2 sin(αξ)dξ,

sin α =

θ1ξ sin(αξ)dξ +

θ2ξ2 sin(αξ)dξ.

Evaluating the ﬁrst integral on the right hand side of (2.19) gives

Evaluating the second integral on the right hand side of (2.19) gives

Thus, using (2.20) and (2.21), (2.19) can be simpliﬁed as

(θ1α2 + θ2α2 − 2θ2) cos α + (α3 − θ1α − 2θ2α) sin α + 2θ2 = 0.

(2.22)

The following result, the proof of which is deferred to the appendix, is fundamental to our subsequent

analysis.
Lemma 2.1. Suppose Θ = (θ1, θ2)(cid:62) satisﬁes the following inequality:

1 + θ2
θ2

2 + 2θ1θ2 − 2θ1 − 4θ2 ≥ 0.

(2.23)

Then equation (2.22) has an inﬁnite number of positive solutions.

For any α satisfying (2.22), there exists a corresponding solution of (2.14) in the form (2.18). Let

6

{αn}∞

n=1 be a sequence of positive solutions of (2.22). Then the general solution of (2.14) is

∞(cid:88)

X (x) =

An sin(αnx),

where An are constants to be determined. The corresponding eigenvalues are

n=1

Hence, using (2.15),

σn = c − α2

n, n = 1, 2, 3, . . .

∞(cid:88)

n=1

y(x, t) =

T0Ane(c−α2

n)t sin(αnx).

(2.24)

(2.25)

(2.26)

By virtue of (2.12) and (2.13), this solution satisﬁes the boundary conditions (2.4c) and (2.4d). The
constants T0 and An must be selected appropriately so that the initial condition (2.4b) is also satisﬁed.
To ensure stability as t → ∞, each eigenvalue σn = c− α2
n in (2.26) must be negative. Thus, we impose
the following constraints on Θ = (θ1, θ2)(cid:62):

2 + 2θ1θ2 − 2θ1 − 4θ2 ≥ 0,

θ2
1 + θ2
c − α2 ≤ −,
(θ1α2 + θ2α2 − 2θ2) cos α + (α3 − θ1α − 2θ2α) sin α + 2θ2 = 0,

(2.27a)

(2.27b)

(2.27c)

where  is a given positive parameter and α is the smallest positive solution of (2.22). Note that α here

is treated as an additional optimization variable. Constraint (2.27a) ensures that there are an inﬁnite

number of eigenvalues (see Lemma 2.1) and thus the solution form (2.26) is valid. Constraints (2.27b)

and (2.27c) ensure that the largest eigenvalue is negative, thus guaranteeing solution stability. Adding
constraints (2.27) to Problem P1 yields the following modiﬁed problem.
Problem P2. Given the PDE system (2.4) with the parameterized kernel (2.6), choose Θ = (θ1, θ2)(cid:62)
and α such that the cost functional (2.8) is minimized subject to the bound constraints (2.7) and the

nonlinear constraints (2.27).

The next result is concerned with the stability of the closed-loop system corresponding to the

optimized kernel from Problem P2.
Theorem 2.1. Let (Θ∗, α∗) be an optimal solution of Problem P2, where α∗ is the smallest positive
solution of equation (2.27c) corresponding to Θ∗. Suppose that there exists a sequence {α∗
n=1 of
positive solutions to equation (2.27c) corresponding to Θ∗ such that y0(x) ∈ span{sin(α∗
nx)}. Then the
closed-loop system (2.4) corresponding to Θ∗ is stable.
Proof. Because of constraint (2.27a), the solution form (2.26) with αn = α∗
satisfy (2.4a), (2.4c) and (2.4d). If y0(x) ∈ span{sin(α∗
that

n is guaranteed to
nx)}, then there exists constants Yn, n ≥ 1, such

n}∞

∞(cid:88)

y0(x) =

Yn sin(α∗

nx).

Taking Yn = T0An ensures that (2.26) with αn = α∗
n also satisﬁes the initial conditions (2.4b), and
is therefore the unique solution of (2.4). Since α∗ is the ﬁrst positive solution of equation (2.27c), it

n=1

7

follows from constraint (2.27b) that for each n ≥ 1,

c − (α∗

n)2 ≤ c − (α∗)2 ≤ − < 0.

This shows that all eigenvalues are negative.

(cid:90) 1

0

(cid:12)(cid:12)(cid:12)(cid:12)y0(x) − N(cid:88)

n=1

(cid:12)(cid:12)(cid:12)(cid:12)2

Yn sin(α∗

nx)

Theorem 2.1 requires that the initial function y0(x) be contained within the linear span of sinusoidal
n is a solution of equation (2.22) corresponding to Θ∗. The good thing
functions sin(α∗
about this condition is that it can be veriﬁed numerically by solving the following optimization problem:
choose span coeﬃcients Yn, 1 ≤ n ≤ N , to minimize

nx), where each α∗

J =

dx,

(2.28)

where N is a suﬃciently large integer and each α∗
n is a solution of equation (2.22) corresponding to the
optimal solution of Problem P2. If the optimal cost value for this optimization problem is suﬃciently
small, then the span condition in Theorem 2.1 is likely to be satisﬁed, and therefore closed-loop stability

is guaranteed.

Based on our computational experience, the span condition in Theorem 2.1 is usually satisﬁed.

2 π), there exists at least one solution of (2.22) in the interval [kπ−, kπ +] when k is suﬃciently

This can be explained as follows. In the proof of Lemma 2.1 (see the appendix), we show that for any
 ∈ (0, 1
large. It follows that kπ is an approximate solution of (2.22) for all suﬃciently large k—in a sense,
the solutions α∗
n of (2.22) converge to the integer multiples of π. In our computational experience, this
nx)} is
convergence occurs very rapidly. Thus, it is reasonable to expect that the linear span of {sin(α∗
“approximately” the same as the linear span of {sin(nπx)}, which is known to be a basis for the space
of continuous functions deﬁned on [0, 1].

3 Gradient Computation

Problem P2 is an optimal parameter selection problem with decision parameters θ1, θ2 and α. In prin-
ciple, such problems can be solved as nonlinear optimization problems using the Sequential Quadratic

Programming (SQP) method or other nonlinear optimization methods. However, to do this, we need

the gradients of the cost functional (2.8) and the constraint functions (2.27) with respect to the de-

cision parameters. The gradients of the constraint functions can be easily derived using elementary

diﬀerentiation. Deﬁne

2 + 2θ1θ2 − 2θ1 − 4θ2,

1 + θ2
g1(Θ) = θ2
g2(α) = c − α2,
g3(Θ, α) = (θ1α2 + θ2α2 − 2θ2) cos α + (α3 − θ1α − 2θ2α) sin α + 2θ2.

Then the corresponding constraint gradients are given by

∇θ1g1(Θ) = 2θ1 + 2θ2 − 2, ∇θ2g1(Θ) = 2θ2 + 2θ1 − 4, ∇αg2(α) = −2α,

(3.1)

8

and

∇θ1g3(Θ, α) = α2 cos α − α sin α,
∇θ2g3(Θ, α) = (α2 − 2) cos α − 2α sin α + 2,
∇αg3(Θ, α) = (α3 + θ1α) cos α + (3α2 − θ1α2 − θ2α2 − θ1) sin α.

(3.2a)

(3.2b)

(3.2c)

Since the constraint functions in (2.27) are explicit functions of the decision variables, their gradients

are easily obtained. The cost functional (2.8), on the other hand, is an implicit function of Θ because

it depends on the state trajectory y(x, t). Thus, computing the gradient of (2.8) is a non-trivial task.

We now develop a computational method, analogous to the costate method in the optimal control of

ordinary diﬀerential equations [11, 12, 19], for computing this gradient.

We deﬁne the following costate PDE system:

 vt(x, t) + vxx(x, t) + cv(x, t) + y(x, t; Θ) − k(x; Θ)vx(1, t) = 0,

v(0, t) = v(1, t) = 0,

v(x, T ) = 0.

Let v(x, t; Θ) denote the solution of the costate PDE system (3.3) corresponding to the parameter

vector Θ. Then we have the following theorem.

(3.3a)

(3.3b)

(3.3c)

(3.4a)

(3.4b)

(3.5)

(3.6)

Theorem 3.1. The gradient of the cost functional (2.8) is given by

(cid:90) T
(cid:90) T

0

(cid:90) 1
(cid:90) 1

0

0

0

∇θ1g0(Θ) = −
∇θ2g0(Θ) = −

xvx(1, t; Θ)y(x, t; Θ)dxdt +

θ1 +

θ2,

x2vx(1, t; Θ)y(x, t; Θ)dxdt +

θ1 +

θ2.

1
3
1
4

1
4
1
5

Proof. Let ν(x, t) be an arbitrary function satisfying

ν(x, T ) = 0,

ν(0, t) = ν(1, t) = 0.

Then we can rewrite the cost functional (2.8) in augmented form as follows:

(cid:90) T
(cid:90) T

0

(cid:90) 1
(cid:90) 1

0

g0(Θ) =

1
2

+

(cid:90) 1

1
2

y2(x, t; Θ)dxdt +

ν(x, t)(cid:8) − yt(x, t; Θ) + yxx(x, t; Θ) + cy(x, t; Θ)(cid:9)dxdt.

k2(x; Θ)dx

0

0

0

Using integration by parts and applying the boundary condition (2.4c), we can simplify the augmented

cost functional (3.6) to obtain

9

y2(x, t; Θ)dxdt +

k2(x; Θ)dx

ν(x, T )y(x, T ; Θ)dx +

ν(x, 0)y(x, 0)dx

νt(x, t)y(x, t; Θ)dxdt +

(cid:2)ν(x, t)yx(x, t; Θ)(cid:3)x=1

x=0dt

νx(1, t)y(1, t; Θ)dt +

νxx(x, t)y(x, t; Θ)dxdt

0

0

ν(x, t)y(x, t; Θ)dxdt.

(3.7)

g0(Θ) =

1
2
−

+

−

(cid:90) 1

0

(cid:90) 1

0

(cid:90) T
(cid:90) 1
(cid:90) T
(cid:90) T
(cid:90) T

0

0

0

0

(cid:90) 1

+ c

0

0

(cid:90) 1
(cid:90) 1

0

(cid:90) T
(cid:90) T
(cid:90) 1

0

0

g0(Θ) =

1
2

+

+

(cid:90) 1
(cid:90) 1

0

1
2

(cid:90) T
(cid:90) 1

0

0

(cid:90) T

(cid:90) 1

1
2

Thus, recalling the conditions (2.4b) and (3.5), we obtain

y2(x, t; Θ)dxdt +

(cid:8)νt(x, t) + νxx(x, t) + cν(x, t)(cid:9)y(x, t; Θ)dxdt

k2(x; Θ)dx

0

(cid:90) T

0

ν(x, 0)y0(x)dx −

νx(1, t)y(1, t; Θ)dt.

0

0

Now, consider a perturbation ερ in the parameter vector Θ, where ε is a constant of suﬃciently small

magnitude and ρ is an arbitrary vector. The corresponding perturbation in the state is,

y(x, t; Θ + ερ) = y(x, t; Θ) + ε(cid:104)∇Θy(x, t; Θ), ρ(cid:105) + O(ε2),

and the perturbation in the feedback kernel is,

k(x; Θ + ερ) = k(x; Θ) + ε(cid:104)∇Θk(x; Θ), ρ(cid:105) + O(ε2),

(3.8)

(3.9)

where O(ε2) denotes omitted second-order terms such that ε−1O(ε2) → 0 as ε → 0. For notational
simplicity, we deﬁne η(x, t) = (cid:104)∇Θy(x, t; Θ), ρ(cid:105). Obviously, η(x, 0) = 0, because the initial proﬁle y0(x)
is independent of the parameter vector Θ. Based on (3.8) and (3.9), the perturbed augmented cost

functional takes the following form:

(cid:90) 1
(cid:90) 1

0

0

(cid:90) T
(cid:90) T
(cid:90) 1
(cid:90) 1

0

0
1
2

(cid:8)y(x, t; Θ) + εη(x, t)(cid:9)2dxdt
(cid:8)νt(x, t) + νxx(x, t) + cν(x, t)(cid:9)(cid:8)y(x, t; Θ) + εη(x, t)(cid:9)dxdt
(cid:8)k(x; Θ) + ε(cid:104)∇Θk(x; Θ), ρ(cid:105)(cid:9)2dx + O(ε2).

νx(1, t)(cid:8)y(1, t; Θ) + εη(1, t)(cid:9)dt

(cid:90) T

ν(x, 0)y0(x)dx −

0

0

g0(Θ + ερ) =

1
2

+

+

+

(3.10)

0

10

From the boundary condition in (2.4d), we have

y(1, t; Θ) + εη(1, t) =

(cid:90) 1
k(x; Θ)(cid:8)y(x, t; Θ) + εη(x, t)(cid:9)dx
(cid:90) 1

0

ε(cid:104)∇Θk(x; Θ), ρ(cid:105)y(x, t; Θ)dx + O(ε2).

+

0

Substituting (3.11) into (3.10) gives

g0(Θ + ερ) =

1
2

+

+

−

+

(cid:90) 1
(cid:90) 1

0

0

(cid:90) T
(cid:90) T
(cid:90) 1
(cid:90) T
(cid:90) 1

0

0

0
1
2

0

0

(cid:8)y(x, t; Θ) + εη(x, t)(cid:9)2dxdt
(cid:8)νt(x, t) + νxx(x, t) + cν(x, t)(cid:9)(cid:8)y(x, t; Θ) + εη(x, t)(cid:9)dxdt
(cid:20)(cid:90) 1
k(x; Θ)(cid:8)y(x, t; Θ) + εη(x, t)(cid:9)dx
(cid:20)(cid:90) 1

(cid:21)
(cid:8)k(x; Θ) + ε(cid:104)∇Θk(x; Θ), ρ(cid:105)(cid:9)2dx + O(ε2).

ε(cid:104)∇Θk(x; Θ), ρ(cid:105)y(x, t; Θ)dx

(cid:90) T

νx(1, t)

dt

0

0

0

ν(x, 0)y0(x)dx −

νx(1, t)

Taking the derivative of (3.12) with respect to ε and setting ε = 0 gives

(cid:104)∇Θg0(Θ), ρ(cid:105) =

=

dε

dg0(Θ + ερ)

(cid:12)(cid:12)(cid:12)(cid:12)ε=0
(cid:90) 1
(cid:90) T
(cid:8)y(x, t; Θ) + νt(x, t) + νxx(x, t) + cν(x, t)(cid:9)η(x, t)dxdt
(cid:90) 1
(cid:90) T
(cid:90) 1
(cid:90) T
(cid:90) 1

νx(1, t)(cid:104)∇Θk(x; Θ), ρ(cid:105)y(x, t; Θ)dxdt

νx(1, t)k(x; Θ)η(x, t)dxdt

0

0

0

0

0

0

−

−

k(x; Θ)(cid:104)∇Θk(x; Θ), ρ(cid:105)dx.

+

0

(3.11)

(cid:21)

dt

(3.12)

(3.13)

Choosing the multiplier ν(x, t) to be the solution of the costate system (3.3), the gradient in (3.13)

(cid:90) 1

(cid:90) T
(cid:90) 1

0

0

(cid:90) 1

(cid:90) T

becomes

(cid:104)∇Θg0(Θ), ρ(cid:105) = −

+

Taking ρ = (1, 0)(cid:62) gives

Similarly, taking ρ = (0, 1)(cid:62) gives

0

∇θ1g0(Θ) = −
(cid:90) T

∇θ2g0(Θ) = −

0

(cid:90) 1

0

0

vx(1, t; Θ)(cid:104)∇Θk(x; Θ), ρ(cid:105)y(x, t; Θ)dxdt

0

k(x; Θ)(cid:104)∇Θk(x; Θ), ρ(cid:105)dx.

xvx(1, t; Θ)y(x, t; Θ)dxdt +

1
3

θ1 +

1
4

θ2.

x2vx(1, t; Θ)y(x, t; Θ)dxdt +

1
4

θ1 +

1
5

θ2.

11

This completes the proof.

4 Numerical Solution Procedure

Based on the gradient formulas derived in Section 3, we now propose a gradient-based optimization
framework for solving Problem P2. This framework is illustrated in Figure 4.1 and described in detail
below.

Algorithm 4.1. Gradient-based optimization procedure for solving Problem P2.

(a) Choose an initial guess (θ1, θ2, α).

(b) Solve the state PDE system (2.4) corresponding to (θ1, θ2).

(c) Solve the costate PDE system (3.3) corresponding to (θ1, θ2).

(d) Compute the cost and constraint gradients at (θ1, θ2, α) using (3.1), (3.2), and (3.4).

(e) Use the gradient information obtained in Step (d) to perform an optimality test.

If (θ1, θ2, α) is

optimal, then stop; otherwise, go to Step (f).

(f) Use the gradient information obtained in Step (d) to calculate a search direction.

(g) Perform a line search to determine the optimal step length.

(h) Compute a new point (θ1, θ2, α) and return to Step (b).

Note that Steps (e)-(h) of Algorithm 4.1 can be performed automatically by standard nonlinear

optimization solvers such as FMINCON in MATLAB.

Recall from Theorem 2.1 that to guarantee closed-loop stability, the optimal value of α must be

the ﬁrst positive solution of (2.22). In practice, this can usually be achieved by choosing α = 0 as the
initial guess. Moreover, after solving Problem P2, it is easy to check whether the optimal value of α is
indeed the smallest positive solution by plotting the left-hand side of (2.22).

4.1 Simulation of the state system

To solve the state system (2.4) numerically, we will develop a ﬁnite-diﬀerence method. This method

involves discretizing both the spatial and the temporal domains into a ﬁnite number of subintervals,

i.e.,

x0 = 0, x1 = h, x2 = 2h, . . . , xn = nh = 1,

t0 = 0, t1 = τ, t2 = 2τ, . . . , tm = mτ = T,

(4.1a)

(4.1b)

where n and m are positive integers and h = 1/n and τ = T /m. Using the Taylor expansion, we obtain

the following approximations:

∂y(xi, tj)

∂t

=

∂2y(xi, tj)

∂x2

=

h2

12

y(xi, tj + τ ) − y(xi, tj)
y(xi + h, tj) − 2y(xi, tj) + y(xi − h, tj)

+ O(τ ),

τ

+ O(h2),

(4.2a)

(4.2b)

Figure 4.1: Gradient-based optimization framework for solving Problem P2.

where O(τ ) and O(h2) denote, respectively, omitted ﬁrst- and second-order terms such that O(τ ) → 0
as τ → 0 and h−1O(h2) → 0 as h → 0. Substituting (4.2) into (2.4a) gives

yi,j+1 − yi,j

τ

=

yi+1,j − 2yi,j + yi−1,j

h2

+ cyi,j,

where yi,j = y(xi, tj), i = 0, 1, . . . , n, j = 0, 1, . . . , m. Simplifying this equation, we obtain

yi,j+1 = (1 − 2r + cτ )yi,j + r(yi−1,j + yi+1,j),

(4.3)

(4.4)

where 1 ≤ i ≤ n − 1, 0 ≤ j ≤ m − 1 and

(4.5)
The explicit numerical scheme (4.4) is convergent when 0 < r ≤ 0.5 (see reference [3] for the relevant
convergence analysis). Thus, in this paper, we assume that τ and h are chosen such that 0 < r ≤ 0.5.
From (2.4b), we obtain the initial condition

r =

τ
h2 .

yi,0 = y(xi, 0) = y0(xi),

i = 0, 1, . . . , n.

Moreover, from (2.4c) and (2.4d), we obtain the boundary conditions

and

yn,j = y(1, tj) =

k(ξ; Θ)y(ξ, tj)dξ,

j = 1, 2, . . . , m.

y0,j = y(0, tj) = 0,

j = 1, 2, . . . , m,

(cid:90) 1

Using the composite trapezoidal rule [3], the integral in (4.8) becomes

0

13

(4.6)

(4.7)

(4.8)

Costate system solverCost/constraint gradientcomputationGradient-based optimizationState system solverh(cid:8)k(x0; Θ)y(x0, tj) + k(xn; Θ)y(xn, tj)(cid:9) + h

n−1(cid:88)

i=1

k(xi; Θ)y(xi, tj)

(cid:35)

(4.9)

yn,j =

=

1
2

1
2

hk(xn; Θ)yn,j + h

n−1(cid:88)

i=1

Rearranging this equation yields

yn,j =

(cid:20)

1 − 1
2

k(xi; Θ)yi,j.

(cid:21)−1(cid:34)

n−1(cid:88)

i=1

hk(xn; Θ)

h

k(xi; Θ)yi,j

.

(4.10)

By using the initial condition (4.6) and the boundary conditions (4.7) and (4.10), numerical approxi-

mations of y(x, t) at the pre-deﬁned nodes can be calculated forward in time recursively from (4.4).

4.2 Simulation of the costate system

As with the state system, we will use the ﬁnite-diﬀerence method to solve the costate system (3.3)

numerically. Using the Taylor expansion, we obtain the following approximations:

∂v(xi, tj)

∂t

∂2v(xi, tj)

∂x2

∂v(1, tj)

∂x

=

=

=

τ

+ O(τ ),

v(xi, tj) − v(xi, tj − τ )
v(xi + h, tj) − 2v(xi, tj) + v(xi − h, tj)
v(xn, tj) − v(xn − h, tj)

h2

+ O(h).

h

+ O(h2),

(4.11a)

(4.11b)

(4.11c)

Substituting (4.11) into (3.3) gives

vi,j − vi,j−1

τ

+

vi+1,j − 2vi,j + vi−1,j

h2

+ cvi,j + yi,j − k(xi; Θ)

vn,j − vn−1,j

h

= 0,

(4.12)

where vi,j = v(xi, tj). We rearrange this equation to obtain

vi,j−1 = (1 − 2r + cτ )vi,j + r(vi+1,j + vi−1,j) + τ yi,j − τ k(xi; Θ)

h

(vn,j − vn−1,j),

(4.13)

where 1 ≤ i ≤ n − 1, 1 ≤ j ≤ m and r is as deﬁned in (4.5). From (3.3c), we obtain the terminal
condition

vi,m = v(xi, T ) = 0,

i = 0, 1, . . . , n.

(4.14)

Moreover, from (3.3b), we obtain the boundary conditions

v0,j = v(0, tj) = 0, vn,j = v(1, tj) = 0,

j = 0, 1, . . . , m.

(4.15)

Using the recurrence equation (4.13), together with (4.14) and (4.15), we can compute approximate

values of v(x, t) backward in time. The ﬁnite-diﬀerence schemes for solving the state and costate PDEs

are summarized in Table 1.

14

Table 1: Numerical computation of y(x, t) and v(x, t)

Procedure 1. Evaluation of y(xi, tj).
1: Set 1/n → h, T /m → τ .
2: Compute yi,0 for each i = 0, 1, . . . , n using (4.6).
3: Set 1 → j.
4: Compute yi,j for each i = 1, 2, . . . , n − 1 by solving (4.4).
5: Compute y0,j using (4.7).
6: Compute yn,j using (4.10).
7: If j = m, then stop. Otherwise, set j + 1 → j and go to Step 4.
Procedure 2. Evaluation of v(xi, tj).

1: Compute vi,m for each i = 0, 1, . . . , n using (4.14).
2: Set j = m − 1.
3: Compute vi,j for each i = 1, 2, . . . , n − 1 using (4.13).
4: Compute v0,j and vn,j using (4.15).
5: If j = 0, then stop. Otherwise, set j − 1 → j and go to Step 3.

4.3 Numerical integration

Recall the cost functional (2.8):

g0(Θ) =

=

(cid:90) T
(cid:90) T

0

(cid:90) 1
(cid:90) 1

0

0

0

1
2
1
2

(cid:90) 1

0

k2(x; Θ)dx

θ2
1 +

1
10

θ2
2 +

1
4

θ1θ2.

y2(x, t; Θ)dxdt +

y2(x, t; Θ)dxdt +

1
2
1
6

Furthermore, recall the cost functional’s gradient from (3.4):

(cid:90) T
(cid:90) T

0

(cid:90) 1
(cid:90) 1

0

0

0

∇θ1g0(Θ) = −
∇θ2g0(Θ) = −

xvx(1, t; Θ)y(x, t; Θ)dxdt +

θ1 +

θ2,

x2vx(1, t; Θ)y(x, t; Θ)dxdt +

θ1 +

θ2.

1
3
1
4

1
4
1
5

(cid:90) T

(cid:90) 1

Clearly, both the cost functional (2.8) and its gradient (3.4) involve evaluating double integrals of the

form

(4.16)
where ψ(x, t) = y2(x, t; Θ) for the cost functional and ψ(x, t) = −∇θik(x; Θ)vx(1, t; Θ)y(x, t; Θ), i = 1, 2,
for the cost functional’s gradient. To evaluate these integrals, we partition the space and temporal
domains using the same equally-spaced mesh points x0, x1, . . . , xn and t0, t1, . . . , tm as in Sections 4.1
and 4.2. These subintervals deﬁne step sizes h = 1/n and τ = T /m. The integral in (4.16) can be

ψ(x, t)dxdt,

0

0

written as the following iterated integral:

(cid:90) T

(cid:90) 1

(cid:90) T

(cid:18)(cid:90) 1

(cid:19)

ψ(x, t)dxdt =

ψ(x, t)dx

dt.

(4.17)

0

0

0

0

15

(cid:27)
(cid:21)

.

Applying the composite Simpson’s rule [3] twice, we obtain the following approximation:

(cid:90) T

(cid:90) 1

(cid:26)

1
3

τ

(m/2)−1(cid:88)

m/2(cid:88)

ψ(x, t)dxdt =

φ(t0) + 2

φ(t2l) + 4

φ(t2l−1) + φ(tm)

,

(4.18)

where

0

0

φ(tj) =

1
3

(cid:20)

h

ψ(x0, tj) + 2

(n/2)−1(cid:88)

k=1

l=1

ψ(x2k, tj) + 4

n/2(cid:88)

k=1

l=1

ψ(x2k−1, tj) + ψ(xn, tj)

More details on numerical integration algorithms are available in [3]. Using (4.18), the cost functional

(2.8) and its gradient (3.4) can be evaluated successfully.

5 Numerical Simulations

Our numerical simulations were conducted within the MATLAB programming environment running on

a desktop computer with the following conﬁguration: Intel Core i7-2600 3.40GHz CPU, 4.00GB RAM,

64-bit Windows 7 Operating System. For the ﬁnite-diﬀerence discretization, we used n = 14 spatial

intervals and m = 5000 temporal intervals over a time horizon of [0, T ] = [0, 4]. Our code implements

the gradient-based optimization procedure in Algorithm 4.1 by combining the FMINCON function in

MATLAB with the gradient computation method described in Section 3.

Consider the uncontrolled version of (2.4) in which u(t) = 0. In this case, the exact solution is

∞(cid:88)

n=1

(cid:90) 1

y(x, t) = 2

Cne(c−n2π2)t sin(nπx)dx,

(5.1)

where Cn are the Fourier coeﬃcients deﬁned by

Cn =

y0(x) sin(nπx)dx.

The eigenvalues of (5.1) are c − n2π2, n = 1, 2, . . . The largest eigenvalue is therefore c − π2, which
indicates that system (2.4) with u(t) = 0 is unstable for c > π2 ≈ 9.8696. We report the numerical
results from our algorithm for three diﬀerent scenarios.

0

5.1 Scenario 1

For the ﬁrst scenario, we choose c = 10 and y0(x) = sin(πx). The corresponding uncontrolled
open-loop response (see equation (5.1)) is shown in Figure 5.1. As we can see from Figure 5.1, the

state of the uncontrolled system grows as time increases. For the feedback kernel optimization, we
suppose that the lower and upper bounds for the optimization parameters are ai = −10 and bi = 10,
respectively. We also choose  = 1 in (2.27b). Starting from the initial guess (θ1, θ2, α) = (−1.0, 2.0, 0),
our program terminates after 23 iterations and 15.8358 seconds. The optimal cost value is g0 = 0.1712
and the optimal solution of Problem P2 is (θ∗

The spatial-temporal response of the controlled plant corresponding to (θ∗

2) is shown in Fig-
ure 5.2(a). The ﬁgure clearly shows that the controlled system (2.4) with optimized parameters (θ∗
1, θ∗
2)
is stable. The corresponding boundary control and kernel function are shown in Figures 5.2(b) and

2, α∗) = (−1.0775, 0.5966, 3.3486).
1, θ∗

1, θ∗

5.2(c), respectively.

16

Table 2: Solutions of (2.22) and corresponding optimal span coeﬃcients Yn in (2.28) for Scenario 1.

n

1
2
3
4
5
6
7
8
9
10

n

α∗
3.3486
6.3838
9.4952
12.6173
15.7493
18.8835
22.0205
25.1582
28.2971
31.4363

Yn

α∗
n/π
1.0658
1.0364
2.0320 −0.0915
0.0505
3.0224
4.0162 −0.0360
0.0268
5.0131
6.0108 −0.0206
7.0093
0.0161
8.0081 −0.0126
0.0098
9.0072
10.0064 −0.0074

Figure 5.1: Uncontrolled open-loop response for Scenario 1.

nx)}, where each α∗

n is a solution of equation (2.22) corresponding to (θ∗

Recall from Theorem 2.1 that closed-loop stability is guaranteed if α∗ = 3.3486 is the ﬁrst pos-
itive solution of equation (2.22) and the initial function y0(x) is contained within the linear span of
{sin(α∗
2). It is clear from
Figure 5.2(d) that α∗ is indeed the ﬁrst positive solution of equation (2.22). To verify the linear span
condition, we use FMINCON in MATLAB to minimize (2.28) for N = 10. The ﬁrst 10 positive so-
lutions of (2.22) corresponding to the optimal parameters θ∗
2 = 0.5966 are given in
Table 2. The optimal span coeﬃcients that minimize (2.28) are also given. The optimal value of J
in (2.28) is 2.184832 × 10−5, which indicates that the span condition holds. Note also from Table 2
that α∗

n/π converges to an integer as n → ∞ (recall the discussion of the end of Section 2.2).

1 = −1.0775 and θ∗

1, θ∗

5.2 Scenario 2

For the second scenario, we choose c = 11 and y0(x) = (1 + x) sin(πx).
The corresponding un-
controlled open-loop trajectory is shown in Figure 5.3. Starting from the initial guess (θ1, θ2, α) =
(−1.0, 1.5, 0), our program converges after 26 iterations and 11.5767 seconds with an optimal cost value
2, α∗) = (−2.9141, 1.7791, 3.6056).
of g0 = 0.5515. The corresponding optimal parameter values are (θ∗

1, θ∗

17

00.5100.5100.511.5xty(x,t)(a) Closed-loop response y(x, t).

(b) Optimal boundary control y(1, t).

(c) Optimal kernel k(x).

(d) Left-hand side of (2.22).

Figure 5.2:
α∗ = 3.3486).

Simulation results for Scenario 1 (optimized parameters θ∗

1 = −1.0775, θ∗

2 = 0.5966,

Table 3: Solutions of (2.22) and optimal span coeﬃcients Yn in (2.28) for Scenario 2.

n

1
2
3
4
5
6
7
8
9
10
11
12
13
14

n

α∗
3.6056
6.4595
9.5520
12.6561
15.7817
18.9096
22.0433
25.1778
28.3147
31.4520
34.5905
37.7291
40.8685
44.0081

Yn

α∗
n/π
1.1476 −0.4185
2.0562
1.4867
3.0404
1.4965
4.0285 −0.7676
5.0234
0.4462
6.0191 −0.3391
0.2493
7.0166
8.0143 −0.1992
9.0128
0.1520
10.0114 −0.1189
0.0871
11.0104
12.0095 −0.0622
0.0429
13.0088
14.0086
1.0061

18

00.511.522.533.54−0.25−0.2−0.15−0.1−0.050ty(1,t)00.20.40.60.81−0.5−0.45−0.4−0.35−0.3−0.25−0.2−0.15−0.1−0.050xk(x)012345678−600−500−400−300−200−1000100200αg3(Θ,α)Figure 5.3: Uncontrolled open-loop response for Scenario 2.

We show the spatial-temporal response for the controlled system with optimized feedback parameters
(θ∗
1, θ∗
2) in Figure 5.4(a). Again, as with Scenario 1, the controlled plant corresponding to the optimal
solution of Problem P2 is stable. The optimal boundary control and optimal kernel function are shown
in Figures 5.4(b) and 5.4(c), respectively. Figure 5.4(d) shows the left-hand side of (2.22). Note that
α∗ = 3.6056 is the ﬁrst positive root, as required by Theorem 2.1. Using MATLAB to minimize (2.28)
for N = 14, we obtain an optimal cost of 1.249410 × 10−12, which indicates that the span condition in
Theorem 2.1 holds. The values of α∗

n and Yn in (2.28) are given in Table 3.

5.3 Scenario 3

For the ﬁnal scenario, we choose c = 14 and y0(x) = (2 + x) sin(2.5πx). The corresponding uncontrolled
open-loop trajectory is shown in Figure 5.5. Starting from the initial guess (θ1, θ2, α) = (−2.0, 1.5, 0),
our program terminates after 22 iterations and 10.0226 seconds with an optimal cost value of
2, α∗) = (−9.1266, 6.4093, 4.1231). The spatial-
g0 = 3.1006. The corresponding optimal solution is (θ∗
temporal response of the controlled plant corresponding to (θ∗
2) is shown in Figure 5.6(a), which
clearly shows that the controlled system (2.4) with optimized parameters (θ∗
2) is stable. The optimal
boundary control and optimal kernel function are shown in Figures 5.6(b) and 5.6(c), respectively.
Minimizing (2.28) for N = 14 yields an optimal cost of 8.045397 × 10−15. We report the corresponding
values of α∗
n and Yn in Table 4. Finally, Figure 5.6(d) shows the left-hand side of equation (2.22)
corresponding to the optimized parameters.

1, θ∗

1, θ∗

1, θ∗

6 Conclusion

In this paper, we have introduced a new gradient-based optimization approach for boundary stabi-

lization of parabolic PDE systems. As with the well-known LQ control and backstepping synthesis

approaches, our new approach involves expressing the boundary controller as an integral state feed-

back in which a kernel function needs to be designed judiciously. However, unlike the LQ control and

backstepping approaches, we do not determine the feedback kernel by solving Riccati-type or Klein-

19

00.5100.51012345xty(x,t)(a) Closed-loop response y(x, t).

(b) Optimal boundary control y(1, t).

(c) Optimal kernel k(x).

(d) Left-hand side of (2.22).

Figure 5.4:
α∗ = 3.6056).

Simulation results for Scenario 2 (optimized parameters θ∗

1 = −2.9141, θ∗

2 = 1.7791,

Table 4: Solutions of (2.22) and corresponding optimal span coeﬃcients Yn in (2.28) for Scenario 3.

n

1
2
3
4
5
6
7
8
9
10
11
12
13
14

n

α∗
4.1231
6.6959
9.7345
12.7804
15.8861
18.9930
22.1166
25.2406
28.3713
31.5022
34.6366
37.7711
40.9075
44.0440

Yn

α∗
n/π
1.3124 −0.4383
2.1314
1.7274
3.0986
1.2549
4.0683 −0.7124
5.0564
0.4225
6.0456 −0.3256
0.2415
7.0399
8.0343 −0.1951
9.0308
0.1510
10.0274 −0.1208
0.0922
11.0251
12.0229 −0.0723
0.0662
13.0212
14.0196
1.0151

20

00.511.522.533.54−1−0.9−0.8−0.7−0.6−0.5−0.4−0.3−0.2−0.10ty(1,t)00.20.40.60.81−1.4−1.2−1−0.8−0.6−0.4−0.20xk(x)012345678−600−500−400−300−200−1000100200αg3(Θ,α)Figure 5.5: Uncontrolled open-loop response for Scenario 3.

(a) Closed-loop response y(x, t).

(b) Optimal boundary control y(1, t).

(c) Optimal kernel k(x).

(d) Left-hand side of (2.22).

Figure 5.6:
α∗ = 4.1231).

Simulation results for Scenario 3 (optimized parameters θ∗

1 = −9.1266, θ∗

2 = 6.4093,

21

00.5100.51−30−20−10010xty(x,t)00.511.522.533.54−0.2−0.100.10.20.30.40.50.60.7ty(1,t)00.20.40.60.81−3.5−3−2.5−2−1.5−1−0.50xk(x)012345678−600−500−400−300−200−1000100200αg3(Θ,α)Gorden-type PDEs; instead, we approximate the feedback kernel by a quadratic function and then

optimize the quadratic’s coeﬃcients using dynamic optimization techniques. This approach requires

solving a so-called “costate PDE”, which is much easier to solve numerically than the Riccati and

Klein-Gorden PDEs. Indeed, as shown in Section 4, the costate PDE can be solved easily using the

ﬁnite-diﬀerence method. Based on the work in this paper, we have identiﬁed several unresolved research

questions described as follows: (i) Is it possible to prove, or at least weaken, the linear span condition

in Theorem 2.1?

(ii) Can the proposed kernel optimization approach be applied to other classes of

PDE plant models? (iii) Is it possible to develop methods for minimizing cost functional (2.8) over an

inﬁnite time horizon? These issues will be explored in future work.

Acknowledgements

This paper is dedicated to Professor Kok Lay Teo on the occasion of his 70th birthday. Professor Teo

has been valued colleague and mentor to each of the authors of this paper. The gradient computation

procedure in Section 3 was inspired by Professor Teo’s seminal work in [19]; see also the recent survey

papers [11, 12].

A Proof of Lemma 2.1

We prove the lemma in three steps.

A.1 Preliminaries

Let

Q(α) =

=

(cid:112)
(cid:113)

(θ1α2 + θ2α2 − 2θ2)2 + (α3 − θ1α − 2θ2α)2
2 − 2θ1 − 4θ2)α4 + θ2
α6 + (θ2

1 + 2θ1θ2 + θ2

1α2 + 4θ2
2.

Furthermore, let ϕ(α) ∈ (−π, π] be the unique angle satisfying

and

cos(ϕ(α)) =

α3 − θ1α − 2θ2α

Q(α)

,

sin(ϕ(α)) =

θ1α2 + θ2α2 − 2θ2

Q(α)

.

Using the deﬁnitions of Q(α) and ϕ(α), equation (2.22) can be rewritten as follows:

Q(α) sin(ϕ(α)) cos(α) + Q(α) cos(ϕ(α)) sin(α) = −2θ2.

Thus, using the angle sum trigonometric identity, we obtain

Q(α) sin(α + ϕ(α)) = −2θ2.
Now, under condition (2.23), Q(α) → ∞ as α → ∞. Furthermore,

lim
α→+∞ cos(ϕ(α)) = 1,

lim
α→+∞ sin(ϕ(α)) = 0.

Hence, ϕ(α) → 0 as α → ∞.

22

(A.1)

A.2 Angle ϕ(α) is Continuous at all Suﬃciently Large α
Since ϕ(α) → 0 as α → ∞, there exists a constant ¯α such that − 1
Consider an arbitrary point α(cid:48) > ¯α. We will show that ϕ(·) is continuous at α(cid:48).

4 π < ϕ(α) < 1

4 π for all α > ¯α.

Let δ > 0. In view of the deﬁnition of sin(ϕ(α)), there exists an ε > 0 such that

|α − α(cid:48)| < ε =⇒ − 1√
2

δ < sin(ϕ(α)) − sin(ϕ(α(cid:48))) <

1√
2

δ.

Now, using Taylor’s Theorem,

sin(ϕ(α)) − sin(ϕ(α(cid:48))) = cos(ζ)(ϕ(α) − ϕ(α(cid:48))),

(A.2)

(A.3)

where ζ belongs to the interval bounded by ϕ(α) and ϕ(α(cid:48)). Suppose α satisﬁes |α−α(cid:48)| < min(ε, α(cid:48)− ¯α).
Then

− 1
4

π < ϕ(α) <

1
4

π, − 1
4

π < ϕ(α(cid:48)) <

1
4

π.

Hence,

and

− 1
4

π < ζ <

1
4

π

cos ζ >

1√
2

.

(A.4)

Combining (A.2)-(A.4) yields

1√
2

δ > | sin(ϕ(α)) − sin(ϕ(α(cid:48)))| = | cos(ζ)| · |ϕ(α) − ϕ(α(cid:48))| ≥ 1√
2

|ϕ(α) − ϕ(α(cid:48))|.

Hence, we have established the following implication:

|α − α(cid:48)| < min(ε, α(cid:48) − ¯α)

=⇒ |ϕ(α) − ϕ(α(cid:48))| < δ.

This shows that ϕ(·) is continuous at α(cid:48), as required.

A.3 Roots of Equation (A.1)
Let  ∈ (0, 1

2 π) and deﬁne

ak = kπ − ,
Clearly, for each integer k ≥ 0, ak < bk < ak+1 and

bk = kπ + .

sin(),
− sin(),

− sin(),

sin(),

if k is odd,

if k is even,

if k is odd,

if k is even.

sin(ak) =

sin(bk) =

Using Taylor’s Theorem, we have

sin(α + ϕ(α)) = sin(α) + cos(ζ)ϕ(α),

23

where ζ = ζ(α) belongs to the interval bounded by α and α + ϕ(α). Thus,

| sin(α + ϕ(α)) − sin(α)| = | cos(ζ)ϕ(α)| ≤ |ϕ(α)|.

(A.5)

Since ϕ(α) → 0 as α → ∞, there exists an integer k1 ≥ 1 such that for all k ≥ k1,

|ϕ(ak)| <

1
2

sin(),

|ϕ(bk)| <

1
2

sin().

Hence, substituting α = ak and α = bk into (A.5) gives, for k ≥ k1,

> 1
< − 1

< − 1

> 1

2 sin(),

2 sin(),

2 sin(),

2 sin(),

if k is odd,

if k is even,

if k is odd,

if k is even.

sin(ak + ϕ(ak))

sin(bk + ϕ(bk))

Since Q(α) → ∞ as α → ∞, there exists an integer k2 ≥ 1 such that for all k ≥ k2,

− 1
2

− 1
2

Thus, for all k ≥ max{k1, k2},

Q(ak) sin() ≤ −2θ2 ≤ 1
2
Q(bk) sin() ≤ −2θ2 ≤ 1
2

Q(ak) sin(),

Q(bk) sin().

> 1
< − 1

< − 1

> 1

2 Q(ak) sin() ≥ −2θ2,
2 Q(ak) sin() ≤ −2θ2,

2 Q(bk) sin() ≤ −2θ2,
2 Q(bk) sin() ≥ −2θ2,

if k is odd,

if k is even,

if k is odd,

if k is even.

Q(ak) sin(ak + ϕ(ak))

Q(bk) sin(bk + ϕ(bk))

Since ϕ is continuous when α is large, this implies that, for all suﬃciently large k, there exists a solution
of (A.1) within the interval [ak, bk]. The result follows immediately.

References

[1] A. C. Antoulas, Approximation of Large-scale Dynamical Systems, SIAM, Philadelphia, 2005.

[2] A. Bensoussan, G. Da Prato, M. C. Delfour and S. K. Mitter, Representation and Control of

Inﬁnite Dimensional Systems, Birkhauser, Boston, 2007.

[3] R. L. Burden and J. D. Faires, Numerical Analysis, Cengage Learning, Boston, 1993.

[4] R. F. Curtain and H. Zwart, An Introduction to Inﬁnite-dimensional Linear Systems Theory,

Springer, New York, 1995.

[5] J. H. Ferziger and M. Peri´c, Computational Methods for Fluid Dynamics, Springer, Berlin, 1996.

[6] P. Holmes, J. L. Lumley and G. Berkooz, Turbulence, Coherent Structures, Dynamical Systems

and Symmetry, Cambridge University Press, London, 1998.

24

[7] N. J. Killingsworth and M. Krstic, PID tuning using extremum seeking: Online, model-free per-

formance optimization, IEEE Control Syst. Mag. 26 (2006) 70-79.

[8] M. Krstic, B. Guo, A. Balogh and A. Smyshlyaev, Control of a tip-force destabilized shear beam

by observer-based boundary feedback, SIAM J. Control Optim. 47 (2008) 553-574.

[9] M. Krstic and A. Smyshlyaev, Boundary Control of PDEs: A Course on Backstepping Designs,

SIAM, Philadelphia, 2008.

[10] B. Li, K. L. Teo, C. Lim and G. Duan, An optimal PID controller design for nonlinear constrained

optimal control problems, Discrete Contin. Dyn. Syst. Ser. B 16 (2011) 70-79.

[11] Q. Lin, R. Loxton and K. L. Teo, Optimal control of nonlinear switched systems: Computational

methods and applications, J. Oper. Res. Soc. China 1 (2013) 275-311.

[12] Q. Lin, R. Loxton and K. L. Teo, The control parameterization method for nonlinear optimal

control: A survey, J. Ind. Manag. Optim. 10 (2014) 275-309.

[13] W. Liu, Boundary feedback stabilization of an unstable heat equation, SIAM J. Control Optim.

42 (2003) 1033-1043.

[14] W. Liu, Elementary Feedback Stabilization of the Linear Reaction-convection-diﬀusion Equation

and the Wave Equation, Springer, Berlin, 2010.

[15] B. Moore, Principal component analysis in linear systems: Controllability, observability, and model

reduction, IEEE Trans. Automat. Control 26 (1981) 17-32.

[16] S. J. Moura and H. K. Fathy, Optimal boundary control of reaction-diﬀusion partial diﬀerential

equations via weak variations, ASME J. Dyn. Syst. Meas. Control 135 (2013) 034501(1-8).

[17] J. Strikwerda, Finite Diﬀerence Schemes and Partial Diﬀerential Equations, SIAM, Philadelphia,

2007.

[18] G. A. Susto and M. Krstic, Control of PDE-ODE cascades with Neumann interconnections, J.

Franklin Inst. 347 (2010) 284-314.

[19] K. L. Teo, C. J. Goh and K. H. Wong, A Uniﬁed Computational Approach to Optimal Control

Problems, Longman Scientiﬁc and Technical, Essex, 1991.

[20] R. Triggiani, Well-posedness and regularity of boundary feedback parabolic systems, J. Diﬀerential

Equations 36 (1980) 347-362.

[21] R. Vazquez and M. Krstic, A closed-form feedback controller for stabilization of the linearized 2-D

Navier-Stokes poiseuille system, IEEE Trans. Automat. Control 52 (2007) 2298-2312.

[22] R. Vazquez and M. Krstic, Control of Turbulent and Magnetohydrodynamic Channel Flows: Bound-

ary Stabilization and State Estimation, Springer, Boston, 2008.

[23] J. Xu, D. Huang and S. Pindi, Optimal tuning of PID parameters using iterative learning approach,

SICE J. Control Meas. Syst. Integr. 1 (2008) 143-154.

[24] C. Xu, Y. Ou and E. Schuster, Sequential linear quadratic control of bilinear parabolic PDEs based

on POD model reduction, Automatica J. IFAC 47 (2011) 418-426.

25

[25] C. Xu, E. Schuster, R. Vazquez and M. Krstic, Stabilization of linearized 2D magnetohydrodynamic

channel ﬂow by backstepping boundary control, Systems Control Lett. 57 (2008) 805-812.

26

