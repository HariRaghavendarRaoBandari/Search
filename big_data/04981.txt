6
1
0
2

 
r
a

 

M
6
1

 
 
]
T
G
.
s
c
[
 
 

1
v
1
8
9
4
0

.

3
0
6
1
:
v
i
X
r
a

Regret-optimal Strategies for Playing Repeated Games with

Discounted Losses

Vijay Kamble ∗
Dept. of EECS

UC Berkeley

Patrick Loiseau †

EURECOM

Jean Walrand ‡
Dept. of EECS

UC Berkeley

Abstract

The regret-minimization paradigm has emerged as a powerful technique for designing al-
gorithms for online decision-making in adversarial environments. But so far, designing exact
minmax-optimal algorithms for minimizing the worst-case regret has proven to be a diﬃcult task
in general, with only a few known results in speciﬁc settings. In this paper, we present a novel
set-valued dynamic programming approach for designing such exact regret-optimal policies for
playing repeated games with discounted losses.

Our approach ﬁrst draws the connection between regret minimization, and determining min-
imal achievable guarantees in repeated games with vector-valued losses. We then characterize
the set of these minimal guarantees as the ﬁxed point of a dynamic programming operator de-
ﬁned on the space of Pareto frontiers of convex and compact sets. This approach simultaneously
results in the characterization of the optimal strategies that achieve these minimal guarantees,
and hence of regret-optimal strategies in the original repeated game. As an illustration of our
approach, we design a simple near-optimal strategy for prediction using expert advice for the
case of 2 experts.

1

Introduction

In several online decision-making problems in uncertain environments, one desires robust perfor-
mance guarantees; examples include managing investment portfolios, weather forecasting, online
spam detection etc. The well-known regret minimization paradigm captures this objective by mod-
eling the problem as a repeated game between the decision-maker and an adversary, where the
decision-maker’s goal is to design an adaptive and possibly randomized algorithm (henceforth, al-
gorithm/strategy/policy) for choosing actions so as to minimize regret, which is deﬁned as the
diﬀerence between the actual loss incurred by the algorithm, and the loss incurred by the best
ﬁxed action that could have been chosen in hindsight against the sequence of actions chosen by
the adversary. Since its inception by Hannan in the 1950s [Han57], this formulation of an online
decision-making problem under uncertainty has been widely popular both in theory and practice,
and is now fundamental to the ﬁeld of online learning.

In this setting, several no-regret learning algorithms are now known, which ensure that the
time-averaged regret asymptotically vanishes in the long run, not just in expectation, but with

∗
†
‡

vijaykamble.iitkgp@gmail.com
patrick.loiseau@eurecom.fr
walrand@berkeley.edu

1

probability 1, irrespective of the sequence of actions chosen by the adversary. But in many realistic
cases, e.g., when decision-making horizons are ﬁnite, the minimal expected regret one can achieve
in the worst case over all the sequence of actions chosen by the adversary (henceforth, just optimal
regret) is non-zero. In such cases, standard no-regret algorithms can perform quite poorly compared
to the optimum and except for a very few special cases, the exact optimal regret and strategies are
unknown.

The main contribution of this paper is a characterization of exact regret-optimal strategies for
playing repeated games with discounted losses based on a novel set-valued dynamic programming
approach. In these games, the loss criterion is the weighted sum of per-stage losses, with the loss
at time t weighted by (1− β)βt−1, where β ∈ (0, 1) is called the discount factor.1 Such discounting
is natural in practice, where current losses are more important (or devastating) than future ones. 2
In this case, since the losses incurred in the initial stages have a non-vanishing contribution to the
cumulative loss as the number of stages increases, the optimal long-run average discounted regret
for any ﬁxed β ∈ (0, 1) is non-zero (see [CBL03], Thm 2.7). Many existing no-regret algorithms
guarantee a regret of O(√1 − β) in this setting. But the performance of these algorithms can be
far from optimal for a ﬁxed β, and the exact minmax optimal regret that can be attained was
unknown. Our approach bridges this important gap by giving a procedure to approximate the
optimal regret to an arbitrarily high precision, and compute -regret-optimal strategies for any
 > 0. These -regret-optimal strategies are extremely simple to implement and require a ﬁnite
memory (which grows as  decreases).

Our solution begins with a well-known approach in regret minimization: transforming the re-
peated game into a repeated game with vector losses. In this game, the number of vector compo-
nents is the number of actions available to the decision-maker, where each component keeps track
of the additional loss incurred relative to the loss incurred if the corresponding action was always
chosen in the past. The goal of regret minimization in the original game now translates to the
goal of simultaneously minimizing the worst-case losses on all the components in this vector-valued
game. But there is a tradeoﬀ here: a better guarantee on one component implies a worse guarantee
on some other. Thus one needs to characterize the entire Pareto-frontier of the minimal losses that
can be simultaneously guaranteed across the diﬀerent components. The main technical contribution
of this paper is an eﬀective characterization of this set and of the strategies that achieve diﬀerent
points on it using a set-valued dynamic programming approach. Based on this characterization, we
then also design an iterative scheme to approximate this set and compute approximately optimal
strategies for arbitrarily low approximation error.

The paper is organized as follows. In Section 2, we formally introduce the worst-case regret min-
imization problem in repeated games, and show its transformation into the problem of computing
simultaneous guarantees on losses in a vector-valued repeated game. In Section 3 we introduce the
set-valued dynamic programming approach for computing these guarantees, and give a characteri-
zation of both the optimal regret and the optimal policies. In Section 4, we present an approximate
value iteration procedure to approximate the optimal regret and compute an -optimal strategy in
the case where the decision-maker has only 2 available actions. Section 5 illustrates our approach
on the problem of prediction using expert advice with 2 experts. We compare the performance of a
near-optimal policy that we compute with that of two algorithms: one is the classical exponentially
weighted average forecaster, and another one is the optimal algorithm for prediction using expert
advice with 2 experts and with geometrically distributed stopping times, characterized recently by

1(1 − β) is just a normalizing factor, which ensures that the sum of the weights over an inﬁnite horizon is 1.
2For instance, such a discounting can capture the existence of a risk-free interest rate on your investments.

2

[GPS14]. We summarize our contribution and discuss future directions in Section 6. The proofs of
all our results can be found in the appendix.

1.1 Related Literature

The ﬁrst study of regret minimization in repeated games dates back to the pioneering work of
Hannan [Han57], who introduced the notion of regret optimality in repeated games and proposed
the earliest known no-regret algorithm. Since then, numerous other such algorithms have been pro-
posed, particularly for the problem of prediction using expert advice, see [LW94, Vov90, CBFH+97,
FS99], one particularly well-known being the multiplicative weights update algorithm, also known
as Hedge. Other settings with limited feedback have been considered, most notably the multi-
armed bandit setting [ACBFS02, BCB12]. Stronger notions of regret such as internal regret, have
also been studied [FV97, CBL03, BM05, SL05].

Several works consider regret minimization with non-uniformly weighted losses, of which the
discounted loss is a special case. While the average regret goes to zero if the weights satisfy a
non-summability condition, lower bounds exist ([CBL03], Thm 2.7) that show that the optimal
regret is bounded away from 0 if the weights are summable, which is the case with discounting.

Natural extensions of no-regret algorithms incur a regret of O(√1 − β) in this case ([CBL03], Thm

2.8 and [Per14], Prop. 2.22). [CZ10] derive better bounds for the case where future losses are given
a higher weight that current ones.

The results on exact regret minimization are few. In an early work, [Cov66] gave the optimal
algorithm for the problem of prediction using expert advice over any ﬁnite horizon T , for the case
of 2 experts, and where the losses are {0, 1}.
[GPS14] recently extended the result to the case
of 3 experts for both the ﬁnite horizon and geometrically distributed random horizon problems.3
[AWY08] considered a related problem, where a gambler places bets from a ﬁnite budget repeatedly
on a ﬁxed menu of events, the outcomes of which are adversarially chosen from {0, 1} (you win
or you lose), and characterized the minmax optimal strategies for the gambler and the adversary.
[LS13] considered a similar repeated decision-making problem where an adversary is restricted
to pick loss vectors (i.e., a loss for each action of the decision-maker in a stage) from a set of
basis vectors, and characterized the minmax optimal strategy for the decision-maker under both,
a ﬁxed and an unknown horizon.4 Most of the approaches in these works are speciﬁc to their
settings, and exploit the assumptions on the structure of the loss vectors.5 But in general if the
loss vectors are arbitrary, none of these approaches can be extended and indeed it is recognized
that characterizing the optimal regret and algorithm is diﬃcult, cf. [LS13]. On the other hand, we
are able to characterize the optimal regret and algorithm for any choice of bounded loss vectors.

Similar to our approach, the idea of characterizing the Pareto Frontier of all achievable regrets
with respect to diﬀerent actions has been explored by [Koo13], again in the speciﬁc context of

3Although a geometric time horizon model seems to be related to the inﬁnite horizon model with discounted losses,
the two problem formulations deﬁne regret diﬀerently, and thus lead to diﬀerent optimal regrets. We discuss this in
Section 5.1.

4All of these are examples where games with ﬁnite action spaces (called matrix games) are repeated, which is the
setting that we are concerned with. But there are many works that consider exact minmax optimality in continuous
repeated games with speciﬁc types of loss functions, see [KMB14, BKM+15, KMBA15] are references therein.

5Many of these works rely on a particular nice property of these settings, which is that the optimal strategy of the
adversary is a controlled random walk that makes any algorithm incur the same regret. If the losses are simple, for
instance if they are the basis vectors, then this random walk can be exactly analyzed to compute the optimal regret.
Knowing the optimal regret then simpliﬁes the computation of the optimal strategy of the decision-maker.

3

prediction with expert advice with 2 experts and {0, 1} losses, and for the ﬁnite time horizon
problem. Our focus on the inﬁnite horizon setting with discounted losses entails the development
and use of a more involved machinery of contractive set-valued dynamic programming operators,
and at the end it gives results that are much cleaner than those in the ﬁnite horizon setting. Set-
valued dynamic programs have been used in other contexts in the theory of dynamic games, for
instance to compute the set of equilibrium payoﬀs in non-zero sum repeated games with imperfect
monitoring [APS86, APS90]. For the use of dynamic programming in zero-sum dynamic games
one can refer to the classic paper by Shapley [Sha53] on Stochastic games. For a general theory of
dynamic programming, see [Ber05].

An important step in our approach is a reduction of the problem to a vector-valued repeated
game. The study of vector-valued repeated games was pioneered by Blackwell [Bla56a]. He gave
suﬃcient conditions for a set to be approachable by a player, which means that there exists a strat-
egy for a player that ensures that the average loss approaches this set regardless of the adversary’s
actions. Moreover, he explicitly deﬁned an adaptive randomized strategy that ensures this. Later
he also showed that this theory can be used to obtain no-regret strategies as formulated by Han-
nan [Han57] , using the transformation of the repeated game into a vector-valued game that we
described earlier [Bla56b]. This theory was subsequently extended in various ways [Vie92, Leh03],
and stronger connections with regret minimization and other learning problems like calibration were
shown [ABH11, Per14]. But as far as we know, there has been no prior work on the discounted loss
criterion. In this paper, as a by-product of our analysis, we successfully bridge this important gap
in the theory of vector-valued repeated games. As a result, this theory bridges signiﬁcant gaps in
other decision-making problems where Blackwell’s approachability theory has found applications.6

2 Model

Let G be a two player game with m actions A = {1, . . . , m} for player 1, who is assumed to be
the minimizer and who we will call Alice (the decision-maker), and n actions B = {1, . . . , n} for
player 2, who is the adversary and who we will call Bob. For each pair of actions a ∈ A and b ∈ B,
the corresponding loss for Alice is l(a, b) ∈ R. The losses for diﬀerent pairs of actions are known
to Alice. The game G is played repeatedly for T stages t = 1, 2,··· , T . In each stage, both Alice
(cid:80)T
and Bob simultaneously pick their actions at ∈ A and bt ∈ B and Alice incurs the corresponding
loss l(at, bt). The loss of the repeated game is deﬁned to be the total discounted loss given by
t=1 βt−1l(at, bt), where β ∈ (0, 1)7. We deﬁne the total discounted regret of Alice as:

T(cid:88)

t=1

T(cid:88)

t=1

βt−1l(at, bt) − min
a∈A

βt−1l(a, bt),

(1)

which is the diﬀerence between her actual discounted loss, and the loss corresponding to the single
best action that could have been chosen against the sequence of actions chosen by Bob in hindsight.
An adaptive randomized strategy πA for Alice speciﬁes for each stage t, a mapping from the set
of observations till stage t, i.e., Ht = (a1, b1,··· , at−1, bt−1), to a probability distribution on the
action set A, denoted by ∆(A). Let ΠA be the set of all such policies of Alice.

The adversary Bob is assumed to choose a deterministic oblivious strategy, i.e., his choice is
simply a sequence of actions πB = (b1, b2, b3,··· , bT ) chosen before the start of the game. Let ΠB
6A notable example is the analysis of zero-sum repeated games with incomplete information [Zam92, AM95, Sor02]
7We drop the normalizing factor 1 − β to simplify the presentation.

4

be the set of all such sequences.8 We would like to compute the worst case or minmax expected
discounted regret which is deﬁned as:

(cid:18)

(cid:20) T(cid:88)

t=1

min
πA∈ΠA

max
πB∈ΠB

EπA

(cid:21)

T(cid:88)

t=1

− min
a∈A

βt−1l(at, bt)

βt−1l(a, bt)

,

(2)

and the strategy for Alice that guarantees this value. Here the expectation is over the randomness in
Alice’s strategy. Here one can see that there is no loss of generality in assuming that the adversary
is deterministic. Indeed if ΠB is allowed to be the set of possible randomizations over T-length
sequences of Bob’s actions, the optimal policy of Bob in the problem

(cid:19)

(cid:21)

(cid:20) T(cid:88)

T(cid:88)

t=1

EπA,πB

max
πB∈ΠB

βt−1l(at, bt) − min
a∈A

t=1

βt−1l(a, bt)

is a deterministic sequence.

We can now equivalently write (2) as:

min
πA∈ΠA

max
πB∈ΠB

max
a∈A

EπA

(cid:20) T(cid:88)

t=1

(cid:21)
βt−1(l(at, bt) − l(a, bt))

.

(3)

In order to address this objective, it is convenient to deﬁne a vector-valued game G, in which,
for a pair of actions a ∈ A and b ∈ B, the vector of losses is r(a, b) with m components (recall that
|A| = m), where
(4)
for k = 1,··· , m. rk(a, b) is the single-stage additional loss that Alice bears by choosing action a
instead of action k, when Bob chooses b: the so called single-stage regret with respect to action k.
For a choice of strategies πA ∈ ΠA and πB ∈ ΠB of the two players, the expected loss on component
k in this vector-valued repeated game over horizon T is given by

rk(a, b) = l(a, b) − l(k, b)

RT

k (πA, πB) = EπA

βt−1rk(at, bt)

,

(5)

(cid:20) T(cid:88)

t=1

B∈ΠB RT
(cid:19)

k=1,··· ,m

(cid:21)

(cid:27)

where the expectation is over the randomness in Alice’s strategy. Now observe that by playing a
ﬁxed policy πA ∈ ΠA, irrespective of the strategy chosen by Bob, Alice guarantees that the total
expected loss on component k is no more than maxπk
B). Suppose that we determine
the set of all simultaneous guarantees that correspond to all the strategies πA ∈ ΠA, deﬁned as:

k (πA, πk

(cid:26)(cid:18)

RT

k (πA, πk
B)

max
πk
B∈ΠB

: πA ∈ ΠA

.

(6)

WT (cid:44)

Then it is clear that

(cid:20) T(cid:88)

t=1

(cid:21)
βt−1(l(at, bt) − l(a, bt))

= min
x∈WT

max

k

xk.

min
πA∈ΠA

max
πB∈ΠB

max
a∈A

EπA

8Having an oblivious adversary is a standard assumption in regret-minimization literature [CBL03] and in fact it

is known that in this case, an oblivious adversary is as powerful as a non-oblivious (adaptive) adversary.

5

In fact, we are only interested in ﬁnding the minimal points in the set WT , i.e., its Lower Pareto
frontier, which is the deﬁned as the set

Λ(WT ) (cid:44) {x ∈ WT : ∀ x(cid:48) ∈ WT \ {x}, ∃ k s.t. xk < x(cid:48)k},

(7)

since all other points are strictly sub-optimal. Let this set be denoted as VT . Our goal in this
paper is to characterize and compute the set V∞ that can be achieved in the inﬁnite horizon game
and compute policies for Alice in ΠA that guarantee diﬀerent points in it.

3 Set-valued dynamic programming

In the remainder of the paper, we will consider a general model for a vector-valued repeated game
G with m actions for Alice and n actions for Bob, where m and n are arbitrary but ﬁnite, and
we assume that the number of components in the vector of losses r(a, b) is K, where K may be
diﬀerent from m. For ease of exposition, we will consider the case where K = 2, but the analysis
holds for any ﬁnite K.

Our goal is to compute the set of minimal guarantees V∞ that Alice can achieve in the inﬁnite
horizon vector-valued discounted repeated game and characterize the policies that achieve it. Our
results will hold for any vector-valued repeated game, not just the one that arises from the regret
minimization formulation discussed before.

In our analysis, we will assume that Bob is not restricted to playing oblivious deterministic
strategies, but can play any adaptive randomized strategy. An adaptive randomized strategy
πB for Bob speciﬁes for each stage t, a mapping from the set of observations till stage t, i.e.,
Ht = (a1, b1,··· , at−1, bt−1), to a probability distribution on the action set B, denoted by ∆(B).
There is no loss in doing so, since will later show in Section 3.6 that the optimal set V∞ and the
optimal policies for Alice remain the same irrespective of which of the two strategy spaces are
assumed for Bob. Although the analysis is simpler if we restrict Bob to using only deterministic
oblivious strategies, the approach we take allows us to demonstrate that our results hold at a higher
level of generality in the broad context of characterizing minimal guarantees in repeated games with
vector losses.

3.1 Overview of results

Our main contributions are as follows:

1. We show that the set V∞ of minimal losses that Alice can simultaneously guarantee in an
inﬁnitely repeated vector-valued zero-sum repeated game with discounted losses is the ﬁxed
point of a set-valued dynamic programming operator deﬁned on the space of lower Pareto
frontiers of closed convex sets with an appropriately deﬁned metric. We then show that the
optimal policies that guarantee diﬀerent points in this set are of the following form. V∞ can be
parametrized so that each point corresponds to a parameter value, which can be thought of as
an “information state”. Each state is associated with an immediate optimal randomized action
and a transition rule that depends on the observed action of the adversary. In order to attain
a point in V∞, Alice starts with the corresponding state, plays the associated randomized
action, transitions into another state depending on Bob’s observed action as dictated by the
rule, plays the randomized action associated with the new state and so on. In particular, the

6

strategy does not depend on Alice’s past actions and it depends on Bob’s past actions only
through this state that the minimizing player keeps track of.

2. For the case where Alice has only 2 actions, we give a value-iteration based procedure to
approximate V∞ and to compute an approximately optimal policy that only uses a coarse
ﬁnite quantization of the parameter space. This strategy can be simply implemented by
a randomized ﬁnite-state automaton. Any desired diminishing approximation error can be
attained by choosing the appropriate quantization granularity and number of iterations. Our
procedure in principle can be extended to an arbitrary number of actions. We ﬁnally illustrate
our theory and the approximation procedure on a simple model of prediction with expert
advice with 2 experts.

To improve the ﬂow of the arguments presented in the paper, the proofs of all of our results

have been moved to the appendix. We ﬁrst present an informal description of our approach.

3.2 Overview of the approach
Let GT denote the T - stage repeated game and let G∞ denote the inﬁnitely repeated game. Let
V0 = {(0, 0)}. We can show that one can obtain the set VT +1 from the set VT , by decomposing
Alice’s strategy in GT +1 into a strategy for the 1st stage, and a continuation strategy for the
remainder of the game from stage 2 onwards, as a function of the action chosen by both the players
in the 1st stage. The induction results from the fact that the minimal guarantees that she can
guarantee from stage 2 onwards are exactly the set VT . Suppose that at the start of GT +1, Alice
ﬁxes the following plan for the entire game: she will play a mixed strategy α ∈ ∆(A) in stage 1.
Then depending on her realized action a and Bob’s action b, from stage 2 onwards she will play
a continuation strategy that achieves the guarantee R(a, b) ∈ VT (she will choose one such point
R(a, b) for every a ∈ A and b ∈ B). Note that it is strictly sub-optimal for Alice to choose any
points outside VT from stage 2 onwards. Now this plan for the entire game GT +1 gives Alice the
following expected simultaneous guarantees on the two components:

(cid:2)r2(a, b) + βR2(a, b)(cid:3)(cid:27)(cid:19)

.

αa

(cid:18)

(cid:26)(cid:88)

a∈A

max
b∈B

(cid:2)r1(a, b) + βR1(a, b)(cid:3)(cid:27)

αa

(cid:26)(cid:88)

a∈A

, max
b∈B

By varying the choice of α and the map R(a, b) we can obtain the set of all the simultaneous
guarantees that Alice can achieve in the (T + 1)-stage game. The Lower Pareto frontier of this set
is exactly VT +1. Thus there is an operator Φ, such that

VT +1 = Φ(VT )

for any T ≥ 0, where V0 is deﬁned to be the singleton set {(0, 0)}. In what follows, we will show
that this operator is a contraction in the space of Lower Pareto frontiers of closed convex sets,
with an appropriately deﬁned metric. This space is shown to be complete, and thus the sequence
VT converges in the metric d to a set V∗, which is the unique ﬁxed point of this operator Φ. As
one would guess, this V∗ is indeed the set V∞ of minimal simultaneous guarantees that Alice can
achieve in the inﬁnitely repeated game G∞.

The rest of this section formalizes these arguments. We will begin the formal presentation of

our results by ﬁrst deﬁning the space of Pareto frontiers that we will be working with.

7

3.3 The space of Pareto frontiers in [0, 1]2
Deﬁnition 3.1. (a) Let u, v ∈ R2. We say that u (cid:22) v if u1 ≤ v1 and u2 ≤ v2. Also, we say that
u ≺ v if u (cid:22) v and u (cid:54)= v. If u ≺ v, we say that v is dominated by u.
(b) A Pareto frontier in [0, 1]2 is a subset V of [0, 1]2 such that no v ∈ V is dominated by another

element of V.

(c) The Lower Pareto frontier (or simply Pareto frontier) of S ⊂ [0, 1]2, denoted by Λ(S), is the

set of elements of S that do not dominate any another element of S.

The Pareto frontier of a set may be empty, as is certainly the case when the set is open. But
one can show that the Pareto frontier of a compact set is always non-empty. Since compactness is
equivalent to a set being closed and bounded in Euclidean spaces, any closed subset of [0, 1]2 has
a non-empty Pareto frontier. We deﬁne the following space of Pareto frontiers:

Deﬁnition 3.2. F is the space of Pareto frontiers of closed and convex subsets of [0, 1]2.

We will now deﬁne a metric on this space. We ﬁrst deﬁne the upset of a set, illustrated in Figure

1.
Deﬁnition 3.3. Let A be a subset of [0, 1]2. The upset up(A) of A is deﬁned as up(A) = {x ∈
[0, 1]2 | x1 ≥ y1 and x2 ≥ y2, for some y ∈ A}. Equivalently, up(A) = {x ∈ [0, 1]2 | x =
y + v, for some y ∈ A and v (cid:23) 0}.

It is immediate that the upset of the Pareto frontier of a closed convex set in [0, 1]2 is closed

and convex. We recall the deﬁnition of Hausdorﬀ distance induced by the ∞-norm.

Figure 1: A Pareto frontier V and its upset up(V).

Deﬁnition 3.4. Let A and B be two subsets of R2. The Hausdorﬀ distance h(A, B) between the
two sets is deﬁned as

h(A, B) = max{sup
x∈A

inf

y∈B||x − y||∞, sup
y∈B

inf

x∈A||x − y||∞}.

We now deﬁne the distance between two pareto frontiers in F as the Hausdorﬀ distance between

their upsets.

Deﬁnition 3.5. For two pareto frontiers U and V in F, we deﬁne the distance d(U, V) between
them as d(U, V) = h(up(U), up(V)).

8

up(V)We can then show that F is complete in the metric d. This follows from the completeness of the

Hausdorﬀ metric for closed convex subsets of [0, 1]2.

Proposition 3.1. Let (cid:0)Vn

(cid:1)

there exists a unique V ∈ F such that d(Vn, V) → 0.

n∈N be a sequence in F. Suppose that supm,k>n d(Vm, Vk) → 0. Then

3.4 Dynamic programming operator and the existence of a ﬁxed point

By scaling and shifting the losses, we assume without loss of generality that rk(a, b) ∈ [0, 1 − β] for
all (a, b, k). Accordingly, the total discounted rewards of the game take values in [0, 1] irrespective
of the time horizon. Now, for a closed set S ⊆ [0, 1]2, deﬁne the following operator Ψ that maps S
to a subset of R2:

(cid:26)(cid:18)

(cid:26)(cid:88)

a∈A

Ψ(S) =

max
b∈B

(cid:2)r1(a, b) + βR1(a, b)(cid:3)(cid:27)

αa

(cid:26)(cid:88)

a∈A

(cid:2)r2(a, b) + βR2(a, b)(cid:3)(cid:27)(cid:19)
(cid:27)

αa

, max
b∈B

: α ∈ ∆(A), R(a, b) ∈ S∀ a ∈ A, b ∈ B

.

(8)

This operator can be interpreted as follows. Assuming that S is the set of pairs of expected
guarantees on losses that Alice can ensure in GT , Ψ(S) is the set of pairs of expected guarantees
that she can ensure in GT +1. We will show that if S is closed then Ψ(S) is closed as well. But if S
is convex then Ψ(S) is not necessarily convex. Nevertheless we can show that the Pareto frontier
of Ψ(S) is the Pareto frontier of some convex and compact set.
Lemma 3.1. Let S ⊆ [0, 1]2 be a closed set. Then Ψ(S) ⊆ [0, 1]2 is closed. If in addition, S is
convex, then:

1. Any point u in Λ(Ψ(S)) is of the form:

(cid:18)

(cid:26)(cid:88)

a∈A

u =

max
b∈B

where Q(b) ∈ Λ(S).

(cid:27)

(cid:27)(cid:19)

αar2(a, b) + βQ2(b)

.

(cid:26)(cid:88)

a∈A

αar1(a, b) + βQ1(b)

, max
b∈B

2. Λ(Ψ(S)) ∈ F.
In the context of our discussion in Section 3.2, the ﬁrst point of the Lemma implies that in any
optimal plan for Alice in GT +1, the continuation strategy from stage 2 onwards need not depend on
her own action in stage 1 (but it depends on Bob’s action in the ﬁrst stage). Deﬁne the following
dynamic programming operator Φ on F.
Deﬁnition 3.6. (Dynamic programming operator) For V ∈ F, we deﬁne Φ(V) = Λ(Ψ(V)).

Now since V is the Lower Pareto frontier of some closed convex subset of R2, say S, and since
Λ(Ψ(V)) = Λ(Ψ(S)), from Lemma 3.1, we know that Φ(V) ∈ F whenever V ∈ F. Next, we claim
that Φ is a contraction in the metric d.

9

Lemma 3.2.

d(Φ(U), Φ(V)) ≤ βd(U, V).

(9)

Finally we show that the dynamic programming operator has a unique ﬁxed point and starting
from a Pareto frontier in F, the sequence of frontiers obtained by a repeated application of this
operator converges to this point.
Theorem 3.1. Let V ∈ F. Then the sequence (An = Φn(V))n∈N converges in the metric d to a
Pareto frontier V∗ ∈ F, which is the unique ﬁxed point of the operator Φ, i.e., the unique solution
of Φ(V) = V.

We can then show that V∗ is indeed the optimal set V∞ that we are looking for.

Theorem 3.2. V∞ = V∗.

3.5 Optimal policies: Existence and Structure

For a Pareto frontier V ∈ F, one can deﬁne a one-to-one function from some parameter set P
to V. Such a function parameterizes the Pareto frontier. For instance, consider the function
F : [−1, 1] × F → R2, where one deﬁnes

F(p, V) = arg min

x + y

(x,y)

(10)

F(p, V) is essentially the component-wise smallest point of intersection of the line y = x + p with
the upset of V in [0, 2]2 (see Figure 2).

s.t. x ≥ u1, y ≥ u2, u ∈ V, y = x + p.

Figure 2: Parameterization of V.

Then for a given V, the function F(., V) : [−1, 1] → V is indeed one-to-one since any point on V
intersects exactly one of the lines. We can now express the dynamic programming operator in the
form of such a parametrization. Assume that V∗ is such that V∗ = Φ(V∗). Then for p ∈ P, one can
choose α(p) ∈ ∆(A) and q(b, p) ∈ P for each b ∈ B such that for k ∈ {1, 2},
αa(p)rk(a, b) + βFk(q(b, p), V∗)}.

(cid:88)

(11)

Fk(p, V∗) = max
b∈B {

a∈A

Then we have the following result.

10

02211y=x+pVF(p,V)Theorem 3.3. For any p1 ∈ P, the upper bound F(p1, V∗) in V∗ is guaranteed on losses by Alice
in the inﬁnite horizon game by ﬁrst choosing action a1 ∈ A with probability αa1(p1). Then if Bob
chooses an action b1 ∈ B, the optimal guarantees to choose from the second step onwards are then
βF(p2, V∗) in βV∗, where p2 = q(b1, p1), which can be guaranteed by Alice by choosing action a2 ∈ A
with probability αa2(p2), and so on.

This implies that P can be thought of as a state space for the policy. Each state is associated
with an immediate optimal randomized action and a transition rule that depends on the observed
action of Bob. In order to attain a point in V∗, Alice starts with the corresponding state, plays the
associated randomized action, transitions into another state depending on Bob’s observed action
as dictated by the rule, plays the randomized action associated with the new state and so on. In
particular, the policy does not depend on the past actions of Alice and it depends on the past actions
of Bob only through this information state that Alice keeps track of. Since Alice’s optimal policy
itself does not depend on her own past actions, Bob’s optimal response does not depend on them
either. Hence one can see that Bob has an oblivious best response to any optimal policy of Alice.
This fact is important for the following discussion.

3.6 Oblivious adversary

Recall that when we deﬁned the dynamic programming operator, we assumed that Bob is allowed
to choose any adaptive non-oblivious strategy. We will now argue that even if we restrict Bob to
an oblivious deterministic policy, the set of minimal losses that Alice can guarantee is still V∗, the
ﬁxed point of our operator. First recall that Bob’s best response to Alice’s optimal strategy that
achieves diﬀerent points on the frontier V∗ is deterministic and oblivious (oﬄine). This is because
Alice does not take into account her own actions to determine the information state transitions.
In fact, if Alice is restricted to use strategies that do not depend on her own actions chosen in the
past, then the best response to such policy is always an oﬄine deterministic policy, and hence the
minimal achievable frontier if Bob is restricted to use oﬄine deterministic policies is V∗. So all that
we need to verify is that Alice does not gain by using policies that depend on her own past actions,
when Bob is restricted to using only oﬄine deterministic strategies. To see this, suppose that VT is
the set of guarantees that Alice by using general randomized adaptive strategies in GT , assuming
that Bob is restricted to using deterministic oﬄine policies. Then in GT +1, Alice’s policy is a choice
of a distribution over her actions, α, and then a mapping from the realized actions (a, b) to some
continuation (randomized adaptive) policy π(a, b) ∈ ΠA. But since Bob’s responses that maximize
the losses on the diﬀerent components cannot depend on the realization of Alice’s action a, and
can only depend on α, his best responses from time 2 onwards would eﬀectively be against the
strategy π(cid:48)(b) of Alice that chooses the policy π(a, b) with probability αa for each realized action
a. This policy better guarantee a point in VT since all other policies are strictly-suboptimal. Thus
the guarantees that Alice can achieve in GT +1 is given by the set:

(cid:18)(cid:26)

(cid:88)

(cid:88)

VT = Λ

max
b∈B

αar1(a, b) + βQ1(b), max
b∈B

a∈A
a∈A
| α ∈ ∆(A), Q(b) ∈ Vt+1 ∀ b ∈ B

αar2(a, b) + βQ2(b)

(cid:27)(cid:19)

.

But this is exactly the dynamic programming operator in Deﬁnition 3.6. Hence we can conclude
that V∗ is indeed the set of minimal guarantees, even if Bob is restricted to using deterministic

11

oﬄine policies.

4 Approximation for K = 2

Finding the minmax regret and the optimal policy that achieves it requires us to compute V∗ and
{(α(p), q(b, p) : p ∈ P} that satisﬁes (11). Except in simple examples, it is not possible to do so
analytically. Hence we now propose a computational procedure to approximate the optimal Pareto
frontier in R2 and devise approximately optimal policies. In order to do so, we need to deﬁne an
approximation of any Pareto frontier. Consider the following approximation scheme for a Pareto
frontier V ∈ F. For a ﬁxed positive integer N , deﬁne the approximation operator to be

(cid:18)

(cid:18)(cid:8)F(p, V) : p ∈ {0,±

ΓN (V) = Λ

ch

1
N

2
N

,±

,··· ,±

N − 1
N

,±1}

(12)

(cid:9)(cid:19)(cid:19)

.

where F(p, V) was deﬁned in (10). Here ch denotes the convex hull of a set. Thus ΓN (V) is a Lower
Pareto frontier of a convex polygon with at most 2N + 1 vertices.

Now suppose that V is the Pareto frontier of a convex and compact set. Then we know that
Φ(V) is also the Pareto frontier of a convex and compact set, and we can express the compound
operator ΓN ◦ Φ(V) via a set of explicit optimization problems as in (10) that only take V as input:
(13)

F(p, Φ(V)) = arg min

x + y

(x,y)

αar1(a, b) + βQ1(b) ∀ b ∈ B, y ≥

s.t. x ≥

αar2(a, b) + βQ2(b) ∀ b ∈ B,

(cid:88)

a∈A

(cid:88)

a∈A

y = x + p, α ∈ ∆(A), Q(b) ∈ V ∀ b ∈ B

If V ∈ F is a Lower Pareto frontier of a convex polygon, then this is a linear programming
problem, and further ΓN ◦ Φ(V) is a Lower Pareto frontier of a convex polygon with at most 2N + 1
vertices. We then we have the following result:
Proposition 4.1. Let G0 = {(0, 0)} and let Gn = (ΓN ◦ Φ)n(G0). Then

(14)

d(V∗, Gn) ≤

1
N

(cid:19)

(cid:18) 1 − βn
(cid:18)

1 − β

+ βn.

(cid:19)

.

And thus

lim sup
n→∞

d(V∗, Gn) = O

1

N (1 − β)

Hence for any , there is a pair (N, n) such that d(V∗, Gn) ≤ .This result implies an iterative
procedure for approximating V∗ by successively applying the compound operator ΓN ◦ Φ to V0,
by solving the above linear program at each step. Since Gn is a Lower Pareto frontier of a convex
polygon with at most 2N + 1 vertices for each n, the size of the linear program remains the same
throughout.

12

4.1 Extracting an approximately optimal policy

From Gn, one can also extract an approximately optimal strategy πn in the inﬁnite horizon game.
Suppose α∗(p) and Q∗(b, p) for b ∈ B are the optimal values that solve the program (13) to compute
F(p, Φ(Gn)) for diﬀerent p ∈ {0,± 1
N ,±1}. Then these deﬁne an approximately
optimal policy in the following class:
Deﬁnition 4.1. A (2N+1)-mode policy π is a mapping from each p ∈ {0,± 1
to the pair

N ,··· ,± N−1

N ,± 2

N ,··· ,± N−1

N ,±1}

N ,± 2

1. α(p) ∈ ∆(A), and
2.

(cid:19)
(cid:18)
q(b, p), q(cid:48)(b, p), κ(b, p)
that |q(b, p) − q(cid:48)(b, p)| = 1

, where for all b ∈ B, q(b, p), q(cid:48)(b, p) ∈ {0,± 1
N , and κ(b, p) ∈ [0, 1].

N ,± 2

N ,··· ,±1} such

The interpretation is as follows. One starts with some initial mode,

i.e., a value of p ∈
{0,± 1
N ,··· ,±1}. Then at any step, if the current mode is p, then Alice ﬁrst chooses action
a ∈ A with probability αa(p). Then if Bob plays action b ∈ B, Alice considers the new mode to
be q(b, p) with probability κ(b, p), and q(cid:48)(b, p) with probability 1 − κ(b, p) and plays accordingly
thereafter.

Now, α∗(p) deﬁnes α(p) in the policy, and(cid:0)q(b, p), q(cid:48)(b, p), κ(b, p)(cid:1) are deﬁned such that they

satisfy

Q∗(b, p) = κ(b, p)F(q(b, p), Gn) + (1 − κ(b, p))F(q(cid:48)(b, p), Gn).

Let Vπn be the corresponding Pareto frontier that is attained by the policy πn (each point on
this frontier is guaranteed by choosing diﬀerent possible initial randomizations over the 2N + 1
modes). We have the following result.

Proposition 4.2.

And thus

d(Vπn, V∗) ≤

1
N

(cid:19)

(cid:18) 1 − βn

1 − β

(cid:18) 2 − βn − βn+1

(cid:19)

(1 − β)2

(cid:19)

.

+ 2βn +

1
N

(cid:18)

d(Vπn, V∗) = O

lim sup
n→∞

1

N (1 − β)2

.

(15)

Remark: The procedure to approximate the frontier and extract an approximately optimal
policy illustrates that our characterization of the minmax optimal policy via the ﬁxed point of a
dynamic programming operator opens up the possibility of using several dynamic programming
based approximation procedures. Here, we have not tried to determine a computation procedure
that achieves the optimal error-complexity tradeoﬀ. For ﬁxed (N, n), in order to approximate the
optimal frontier, the procedure needs to solve nN linear programs, each with O(N ) variables and
constraints, to give the corresponding error bound in the theorem. One can split the error into
N (1−β) and the iteration
two terms: the ﬁrst term is the quantization error which is bounded by
error which is bounded by βn. The second term is relatively benign but the ﬁrst term requires
N = 1
(1−β) , which grows rapidly when β is close to 1. For ﬁnding an approximately optimal policy,
(1−β)2 , which grows even faster. Nevertheless, all of this computation can be
the scaling is like
done oﬄine, and needs to be performed only once. The resulting approximately optimal policy is
very simple to implement, and requires a bounded memory (O(log

1

1

1

(1−β)2 ) bits).

13

Table 1: Possible loss scenarios

Expert 1
Expert 2

1
0

0
1

1
1

0
0

Table 2: Single-stage regret w.r.t. Expert 1 & 2

Expert 1
Expert 2

(0,1)
(-1,0)

(0,-1)
(1,0)

(0,0)
(0,0)

(0,0)
(0,0)

5 Example: Combining expert advice

Consider the following simpliﬁed model of combining expert advice. There are two experts who
give Alice recommendation for a decision-making task: say predicting whether a particular stock
will rise the next day or fall. Each day Alice decides to act on the recommendation made by one
of the experts. The experts’ recommendations may be correct or wrong, and if Alice acts on an
incorrect recommendation, she bears a loss of 1. Otherwise she does not incur any loss. This
model can be represented by Table 1. The rows correspond to the choice made by Alice and the
columns correspond to the four diﬀerent possibilities: either one of the experts gives the correct
recommendation and the other doesn’t, or both give the correct recommendation or both give the
incorrect one. The matrix of single-stage regrets is given in Table 2.

In Figure 3, the computed approximately optimal Pareto frontiers for a range of values of β are

shown with the corresponding (theoretical) approximation errors as given by Theorem 4.1.

Figure 3: Approximations of (1 − β)V∗
for diﬀerent β values with correspond-
ing errors

Figure 4: Average regret incurred by the 77-mode
approximately optimal policy, GPS, and Hedge
against 5 arbitrarily chosen strategies of the ad-
versary, for a discount factor β = 0.8.

5.1 Comparison with other algorithms

In this section, we compare the performance of an approximately optimal algorithm derived from our
approach to two known algorithms for the problem of prediction with expert advice with 2 experts.

14

00.20.40.60.8100.10.20.30.40.50.60.70.80.91  β = 0.95 err ≈ 0.15β = 0.9 err ≈ 0.1β = 0.8 err ≈ 0.05β = 0.7 err ≈ 0.05β = 0.6 err ≈ 0.02β = 0.5 err ≈ 0.02β = 0.4 err ≈ 0.02β = 0.2 err ≈ 0.02123450.120.130.140.150.160.170.18  77−modeHedgeGPSWe will assume a discount factor of 0.8. One algorithm is the well known the exponentially weighted
average forecaster, which belongs to the Hedge or “Multiplicative Weights” class of algorithms. In
this algorithm, if Lt(i) is the cumulative loss of expert i till time t, then the probability of choosing
expert i at time t + 1 is

(cid:112)

pi(t + 1) ∝ exp(−ηtLt(i)),

where ηt =

8 log K/t, where K is the number of experts, which in this case is 2. If one uses the

discounted cumulative losses in this algorithm, then it achieves an upper bound of O(√1 − β) on

the average discounted regret [CBL03].

The second algorithm is the optimal algorithm given by [GPS14] for the problem with a geo-
metrically distributed time horizon, where at each stage, the game ends with a probability (1 − β)
and continues with probability β. This is essentially the same as our model of discounted losses,
where the discount factor is reinterpreted as the probability of continuation at each stage. But
the diﬀerence in that formulation and our formulation is in the deﬁnition of regret. In their for-
mulation, the loss of the decision-maker is compared to the expected loss of the best expert in
the realized time horizon (where the expectation over the randomness in the time horizons), i.e.,
to ET [mini=1,2
t=1 lt(i)], where lt(i) is the loss of expert i at time t. On the other hand, in
the reinterpretation of our formulation, the loss of the decision-maker is compared to the expert
t=1 βt−1lt(i)].
Naturally, the optimal regret in their formulation is at least as high as the optimal regret in our
formulation, and in fact it turns out to be strictly higher. For example, for β = 0.8, the optimal
regret in their formulation is 1/6 ≈ 0.166, while in our formulation, the regret in this case is 0.136,
within an error of up to 0.05 (see Figure 3).

with the lowest expected loss, i.e., to mini=1,2 ET [(cid:80)T

t=1 lt(i)] = mini=1,2 ET [(1 − β)(cid:80)T

(cid:80)T

Figure 4 compares the expected regret incurred by the two algorithms (labeled as Hedge and
GPS), to the expected regret incurred by a 77 − mode approximately optimal policy, against 5
diﬀerent arbitrarily chosen strategies of the adversary.9 Note that the expected regret incurred by
our policy remains close to the near-optimal value of 0.136, while that of the other two algorithms
exceed it by a considerable amount. Also note that as expected, the regret incurred by the GPS
algorithm never exceeds the optimal regret value of 0.166.

6 Conclusion and future directions

We presented a novel set-valued dynamic programming approach to characterize and approximate
the set of minimal guarantees that a player can achieve in a discounted repeated game with vector
losses. In particular, this gives us a way to compute approximately regret-minimizing strategies for
playing repeated games with discounted losses. We showed that this optimal set is the ﬁxed point
of a contractive dynamic programming operator, and it is the Pareto frontier of some convex and
closed set. We also established the structure of the optimal strategies that achieve the diﬀerent
points on this set. Finally we gave a value-iteration based procedure to approximately compute
this set and also ﬁnd approximately optimal strategies for the case where there are two actions.
Of course, this set-valued dynamic programming approach can also be used to determine exactly
regret-optimal strategies for repeated games with a ﬁnite time horizon. In this case, one can show
that the optimal policy will depend on an information state and also the stage t.

9In adversary’s strategy i = 1, 2,··· , 5, the probability that expert 1 incurs a loss (and expert 2 doesn’t) at time

t is [1 − (1/i + 1)]1/t if t is odd, and [1 − (1/i + 1)]t if t is even.

15

The extension of this approach to the case of long-run average losses in inﬁnitely repeated
games is much less straightforward, despite the fact that average cost dynamic programming for
standard dynamic optimization problems like MDPs is quite well understood. Such an extension
could improve our understanding of the construction of no-regret algorithms and would ﬁll the last
remaining gap in seeing the classical dynamic programming paradigm as a holistic and methodical
approach to regret minimization.

References

[ABH11]

Jacob Abernethy, Peter L. Bartlett, and Elad Hazan. Blackwell approachability and
no-regret learning are equivalent. In Proceedings of COLT, 2011.

[ACBFS02] Peter Auer, Nicol`o Cesa-Bianchi, Yoav Freund, and Robert E. Schapire. The non-
stochastic multiarmed bandit problem. SIAM Journal on Computing, 32(1):48–77,
2002.

[AM95]

[APS86]

[APS90]

Robert J. Aumann and Michael Maschler. Repeated Games with Incomplete Informa-
tion. MIT Press, 1995.

Dilip Abreu, David Pearce, and Ennio Stacchetti. Optimal cartel equilibria with im-
perfect monitoring. Journal of Economic Theory, 39(1):251 – 269, 1986.

Dilip Abreu, David Pearce, and Ennio Stacchetti. Toward a theory of discounted
repeated games with imperfect monitoring. Econometrica, 58(5):pp. 1041–1063, 1990.

[AWY08]

Jacob Abernethy, Manfred K Warmuth, and Joel Yellin. Optimal strategies from
random walks. 2008.

[BCB12]

S´ebastien Bubeck and Nicol`o Cesa-Bianchi. Regret analysis of stochastic and non-
stochastic multi-armed bandit problems. Foundations and Trends in Machine Learn-
ing, 5(1):1–122, 2012.

[Ber05]

Dimitri P. Bertsekas. Dynamic Programming and Optimal Control, volume 1. Athena
Scientiﬁc, 2005.

[BKM+15] Peter L Bartlett, Wouter M Koolen, Alan Malek, Eiji Takimoto, and Manfred K
Warmuth. Minimax ﬁxed-design linear regression. In Proceedings of The 28th Annual
Conference on Learning Theory (COLT), pages 226–239, 2015.

[Bla56a]

[Bla56b]

David Blackwell. An analog of the minimax theorem for vector payoﬀs. Paciﬁc J.
Math., 6(1):1–8, 1956.

David Blackwell. Controlled random walks.
In J De Groot and J.C.H Gerretsen,
editors, Proceedings of the International Congress of Mathematicians 1954, volume 3,
pages 336–338, 1956.

[BM05]

Avrim Blum and Yishay Mansour. From external to internal regret. In Proceedings of
COLT, pages 621–636, 2005.

[CBFH+97] Nicol`o Cesa-Bianchi, Yoav Freund, David Haussler, David P. Helmbold, Robert E.
Schapire, and Manfred K. Warmuth. How to use expert advice. J. ACM, 44(3):427–
485, 1997.

16

[CBL03]

[Cov66]

[CZ10]

[FS99]

[FV97]

[GPS14]

[Han57]

Nicol Cesa-Bianchi and Gbor Lugosi. Potential-based algorithms in on-line prediction
and game theory. Machine Learning, 51(3):239–261, 2003.

Thomas M Cover. Behavior of sequential predictors of binary sequences. Technical
report, DTIC Document, 1966.

Alexey Chernov and Fedor Zhdanov. Prediction with expert advice under discounted
loss. In Algorithmic Learning Theory, pages 255–269. Springer, 2010.

Yoav Freund and Robert E. Schapire. Adaptive game playing using multiplicative
weights. Games and Economic Behavior, 29(1–2):79–103, 1999.

Dean P. Foster and Rakesh V. Vohra. Calibrated learning and correlated equilibrium.
Games and Economic Behavior, 21(1–2):40–55, 1997.

Nick Gravin, Yuval Peres, and Balasubramanian Sivan. Towards optimal algorithms
for prediction with expert advice. arXiv preprint arXiv:1409.3040, 2014.

James Hannan. Approximation to Bayes risk in repeated plays. In M. Dresher, A. W.
Tucker, and P. Wolfe, editors, Contributions to the Theory of Games, volume 3, pages
97–139. Princeton University Press, 1957.

[KMB14] Wouter M Koolen, Alan Malek, and Peter L Bartlett. Eﬃcient minimax strategies
for square loss games. In Advances in Neural Information Processing Systems, pages
3230–3238, 2014.

[KMBA15] Wouter M Koolen, Alan Malek, Peter L Bartlett, and Yasin Abbasi. Minimax time
series prediction. In Advances in Neural Information Processing Systems, pages 2548–
2556, 2015.

[Koo13]

[Leh03]

[LS13]

[LW94]

[Per14]

Wouter M Koolen. The pareto regret frontier.
Processing Systems, pages 863–871, 2013.

In Advances in Neural Information

Ehud Lehrer. Approachability in inﬁnite dimensional spaces. International Journal of
Game Theory, 31(2):253–268, 2003.

Haipeng Luo and Robert E Schapire. Towards minimax online learning with unknown
time horizon. arXiv preprint arXiv:1307.8187, 2013.

N. Littlestone and M.K. Warmuth. The weighted majority algorithm. Information
and Computation, 108(2):212–261, 1994.

Vianney Perchet. Approachability, regret and calibration: Implications and equiva-
lences. Journal of Dynamics and Games, 1(2):181–254, 2014.

[Rud86]

Walter Rudin. Real and Complex Analysis. McGraw-Hill, 3rd edition, 1986.

[Sha53]

[SL05]

Lloyd S Shapley. Stochastic games. Proceedings of the National Academy of Sciences
of the United States of America, 39(10):1095, 1953.

Gilles Stoltz and Gbor Lugosi. Internal regret in on-line portfolio selection. Machine
Learning, 59(1–2):125–159, 2005.

[Sor02]

Sylvain Sorin. A First Course on Zero Sum Repeated Games. Springer, 2002.

17

[Vie92]

[Vov90]

[Zam92]

Nicolas Vieille. Weak approachability. Mathematics of Operations Research, 17(4):pp.
781–791, 1992.

Volodimir G. Vovk. Aggregating strategies. In Proceedings of COLT, pages 371–386,
1990.

Shmuel Zamir. Chapter 5 repeated games of incomplete information: Zero-sum. In
Robert Aumann and Sergiu Hart, editors, Handbook of Game Theory with Economic
Applications, volume 1, pages 109–154. Elsevier, 1992.

7 Appendix

Proof of Proposition 3.1. In order to prove the result, we need the following set of results about
the Hausdorﬀ distance:
Lemma 7.1. a) h is a metric on the space of closed subsets of R2.

b) Assume that {An} is a Cauchy sequence of closed subsets of [0, 1]2. Then there is a unique

closed subset A of [0, 1]2 such that h(An, A) → 0. This set A is deﬁned as follows:

A = {x ∈ [0, 1]2 | ∃xn ∈ An s.t. xn → x}.

c) If the sets {An} in b) are convex , then A is convex.
d) h(up(A), up(B)) ≤ h(A, B).

Proof. a)-b) This is the well-known completeness property of the Hausdorﬀ metric; see [Rud86].
c) Say that x, y ∈ A. Then x = limn xn and y = limn yn for xn ∈ An and yn ∈ An. By
convexity of each An, ¯zn := λxn + (1 − λ)yn ∈ An. But then, ¯zn → ¯z := λx + (1 − λ)y. It follows
that ¯z ∈ A, so that A is convex.

d) Let  := h(A, B). Pick x ∈ up(A). Then x = y + v for some y ∈ A and v (cid:23) 0. There is
some y(cid:48) ∈ B with (cid:107)y − y(cid:48)(cid:107)∞ ≤ . Then x(cid:48) = min{y(cid:48) + v, 1} ∈ up(B), where the minimization is
component-wise. We claim that (cid:107)x(cid:48) − x(cid:107)∞ ≤ .

If y(cid:48) + v ∈ [0, 1]2, this is clear. Assume y(cid:48)1 + v1 > 1. Then,

x(cid:48)1 = 1 < y(cid:48)1 + v1 and x1 = y1 + v1 ≤ 1.

Thus,

Hence, |x(cid:48)1 − x1| ≤ |y(cid:48)1 − y1|. Similarly, |x(cid:48)2 − x2| ≤ |y(cid:48)2 − y2|. Thus, one has

0 ≤ x(cid:48)1 − x1 < y(cid:48)1 + v1 − y1 − v1 = y(cid:48)1 − y1.

(cid:107)x(cid:48) − x(cid:107)∞ ≤ (cid:107)y(cid:48) − y(cid:107)∞ ≤ .

Now we can prove the proposition. Under the given assumptions, {up(V)n, n ≥ 1} is Cauchy
in the Hausdorﬀ metric, so that, by Lemma 7.1, there is a unique closed convex set such that
h(up(V)n, A) → 0. But since h(up(V)n, up(A)) ≤ h(up(V)n, A) (from Lemma 7.1), we have that

18

h(up(V)n, up(A)) → 0 and hence up(A) = A. Thus the Pareto frontier V of A is then such that
d(Vn, V) → 0.

To show uniqueness of V, assume that there is some U ∈ F such that d(Vn, U) → 0. Then,
the closed convex set up(U) is such that h(up(V)n, up(U)) → 0. By Lemma 7.1, this implies that
up(U) = up(V), so that U = V.

Proof of Lemma 3.1. In order to prove this lemma, we need a few intermediate results. First, we
need the following fact:

Lemma 7.2. Let V be the Lower Pareto frontier of a closed convex set. Then V is closed.

Proof. Suppose that {xn} is a sequence of points in V that converge to some point x. Then since
S is closed, x ∈ S. We will show that x ∈ V. Suppose not. Then there is some u ∈ V such that
u (cid:22) x. Suppose ﬁrst that u1 < x1 and u2 < x2. Then let  = min(x1−u1,x2−u2)
and consider the ball

2

of radius  around x, i.e.

Bx() = {y ∈ R2 : (cid:107)y − x(cid:107)2 ≤ }.

Then for any point y in Bx(), we have that u (cid:22) y. But since {xn} converges to x, there exists some
point in the sequence that is in Bx(), and u is dominated by this point, which is a contradiction.
Hence either u1 = x1 or u2 = x2. Suppose w.l.o.g. that u1 < x1 and u2 = x2. See Figure 5. Let
δ = x1−u1
and consider the ball of radius δ centered at x, i.e. Bx(δ). Let xn be a point in the
sequence such that xn ∈ Bx(δ). Now xn

2 < u2.
Now some λ ∈ (0, 1), consider a point r = λu + (1 − λ)xn such that r1 = u1 + δ
1 > u1 + δ

1 > u1 and hence it must be that xn

2

1 − x1| ≤ δ, and hence xn

2 . It is possible
2 (please see the
2 < x1 and also r2 < u2 = x2 since λ > 0

to pick such a point since x1 = u1 + 2δ and |xn
ﬁgure). Now r ∈ S since S is convex. Now r1 = x1 − 3δ

Figure 5: Construction in the proof of Lemma 7.2.

2 < u2. Let δ(cid:48) = x2−r2

. Then consider the ball Bx(δ(cid:48)) centered at x. Clearly r (cid:22) y for
and xn
any y ∈ Bx(δ(cid:48)). But since {xn} converges to x, there exist some point in the sequence that is in
Bx(δ(cid:48)), and r is dominated by this point, which is again a contradiction. Thus x ∈ V and hence V
is closed.

2

Remark: The Pareto frontier of a closed set need not be a closed set, as the example in Figure 6
shows.

Next we deﬁne the following notion of convexity of pareto frontiers.

19

¯x¯u¯xnδδ′¯rδFigure 6: A closed set S whose Pareto frontier V is not closed.

Deﬁnition 7.1. A Pareto frontier V is p-convex if for any v, u ∈ V and for each λ ∈ [0, 1], there
exists a point r(λ) ∈ V such that r(λ) (cid:22) λv + (1 − λ)u.

We then show the following equivalences.

Lemma 7.3. For a Pareto frontier V ⊂ [0, 1]2, the following statements are equivalent:

1. V is p-convex and a closed set.
2. V is p-convex and the lower Pareto frontier of a closed set S ⊆ [0, 1]2.
3. V is the lower Pareto frontier of a closed convex set H ⊆ [0, 1]2.

Proof. 1 is a special case of 2 and hence 1 implies 2. To show that 2 implies 3, if S is convex then
there is nothing to prove. So assume S is not convex. Then let H be the convex hull of S. First,
since [0, 1] × [0, 1] is convex, H ⊆ [0, 1] × [0, 1]. Then since S is closed and bounded, it is also
compact. Hence H is the convex hull of a compact set, which is compact and hence closed and
bounded. Now we will show that V is the Lower Pareto frontier of H. To see this, any u ∈ S is of
the form u = λx + (1 − λ)y where x, y ∈ S. But then there are points x(cid:48), y(cid:48) ∈ V such that x(cid:48) (cid:22) x
and y(cid:48) (cid:22) y. Thus we have that

λx(cid:48) + (1 − λ)y(cid:48) (cid:22) λx + (1 − λ)y = u.

But since V is convex, there exists some r(λ) ∈ V such that

r(λ) (cid:22) λx(cid:48) + (1 − λ)y(cid:48) (cid:22) λx + (1 − λ)y = u.

Thus the Pareto frontier of H is a subset of V, but since V ∈ H and it is a Pareto frontier, V is the
Lower Pareto frontier of H. Finally Lemma 7.2 shows that 3 implies that V is closed. To show it is
convex, suppose that u and v are two points in V. Since they also belong to H, which is convex, for
each λ ∈ [0, 1], λu + (1 − λ)v ∈ S and thus there is some r(λ) ∈ V such that r(λ) (cid:22) λu + (1 − λ)v.
Thus V is convex.

We can now ﬁnally prove the lemma. Note that Ψ(S) is the image of the continuous function f
from the product space Sm×n × ∆(A) to a point in R2, which is a Hausdorﬀ space. Since S is closed
and bounded, it is compact. Also the simplex ∆(A) is compact. Thus by Tychonoﬀ’s theorem, the
product space Sm×n × ∆(A) is compact. Hence by the closed map lemma, f is a closed map and
hence Ψ(S) is closed.

Now assume that S is a closed convex set. Then Λ(S) exists by Lemma 3.1 and further it is
p-convex by Lemma 7.3. Let U = Λ(S). Clearly, Λ(Ψ(S)) = Λ(Ψ(U)). Recall that any point u in

20

012123012123SVΛ(Ψ(U)) is of the form:

(cid:18)

u =

max
b∈B

(cid:26)(cid:88)

a∈A

Q(b) ∈ U such that Q(b) (cid:22)

Now let

u =

and

max
b∈B

v =

max
b∈B

αa

(cid:2)r1(a, b) + βR1(a, b)(cid:3)(cid:27)
(cid:80)m
(cid:26)(cid:88)
(cid:26)(cid:88)

αar1(a, b) + βQ1(b)

(cid:27)
(cid:27)

a∈A

ηar1(a, b) + βR1(b)

be two points in Λ(Ψ(U)), where α, η ∈ ∆(A) and Q(b), R(b) ∈ V.

For a ﬁxed λ ∈ [0, 1], let κa = αaλ + ηa(1 − λ). Then

λu + (1 − λ)v =

λ max
b∈B

αar1(a, b) + βQ1(b)

+ (1 − λ) max
b∈B

a∈A

(cid:26)(cid:88)

a∈A

(cid:18)
(cid:18)

(cid:18)
(cid:26)(cid:88)

a∈A

for some α ∈ ∆(A) and R(a, b) ∈ U. But since U is p-convex, for each b ∈ B, there exists some

a=1 αaR(a, b). Hence statement 1 follows.

(cid:26)(cid:88)

a∈A

, max
b∈B

(cid:2)r2(a, b) + βR2(a, b)(cid:3)(cid:27)(cid:19)

αa

(cid:27)(cid:19)
(cid:27)(cid:19)

αar2(a, b) + βQ2(b)

ηar2(a, b) + βR2(b)

(cid:26)(cid:88)
(cid:26)(cid:88)

a∈A

a∈A

, max
b∈B

, max
b∈B

(cid:27)

(cid:26)(cid:88)

a∈A

ηar1(a, b) + βR1(b)

(cid:27)(cid:19)

(cid:27)

,

(cid:27)(cid:19)

(cid:27)(cid:19)

(cid:27)

+ (1 − λ) max
b∈B

(cid:26)(cid:88)
(cid:26)(cid:88)
(cid:26)(cid:88)

a∈A

a∈A

a∈A

(cid:27)
(cid:27)

, max
b∈B

, max
b∈B

(cid:18)

(cid:23)

max
b∈B

λ max
b∈B

(cid:26)(cid:88)

a∈A

(cid:18)

(cid:26)(cid:88)

a∈A

(cid:23)

max
b∈B

αar2(a, b) + βQ2(b)

ηar2(a, b) + βR2(b)

κar1(a, b)+βλQ1(b)+(1−λ)R1(b)

κar2(a, b)+βλQ2(b)+(1−λ)R2(b)

κar1(a, b) + βL1(b)

κar2(a, b) + βL2(b)

The ﬁrst inequality holds since max is a convex function and the second follows since U is p-convex,
and hence L(b) = (L1(b), L2(b)) ∈ U that satisfy the given relation exist. Thus Λ(Ψ(S)) is p-convex.
And hence from Lemma 7.3, it is the Lower Pareto frontier of a closed convex set in [0, 1]2, i.e., it
is in F.

Proof of Lemma 3.2. In order to prove this Lemma, we ﬁrst deﬁne another metric on the space F
that is equivalent to d.
Deﬁnition 7.2. For two Pareto frontiers A and B of [0, 1]2, we deﬁne

e(A, B) (cid:44)

inf{ ≥ 0 : ∀ u ∈ A, ∃ v ∈ B s.t. v (cid:22) u + 1 and ∀ v ∈ B, ∃ u ∈ A s.t. u (cid:22) v + 1}.
Here 1 = (1, 1).

(16)

We can show that the two metrics are equivalent.

21

Lemma 7.4.

e(A, B) = d(A, B).

Proof. Suppose that e(A, B) ≤ . Consider a point x ∈ up(A) such that x = y + v where y ∈ A
and v (cid:23) 0. Suppose that there is no x(cid:48) ∈ up(B) such that (cid:107)x − x(cid:48)(cid:107)∞ ≤ . This means that up(B)
is a subset of the region S shown in the Figure 7. But since y = x − v, y is in region S(cid:48). But for
any u ∈ S and ¯w ∈ S(cid:48), (cid:107)u − ¯w(cid:107)∞ ≥ . This contradicts the fact that for y there is some y(cid:48) ∈ B,
such that y + 1 (cid:23) y(cid:48). Thus d(A, B) ≤ . Now suppose that d(A, B) ≤ . Then for any x ∈ A,

Figure 7: Construction for the proof of Lemma 7.4

there is a x(cid:48) ∈ up(B) such that (cid:107)x − x(cid:48)(cid:107)∞ ≤  where x(cid:48) = y + v for y ∈ B and v (cid:23) 0. Thus
x + 1 (cid:23) x(cid:48) = y + v. Thus e(A, B) ≤ .

Now suppose e(U, V) = . Let

(cid:18)

(cid:26)(cid:88)

a∈A

max
b∈B

αar1(a, b) + βR1(b)

, max
b∈B

αar2(a, b) + βR2(b)

be some point in Φ(V), where α ∈ ∆(A). Then for each R(b), for each b, we can choose Q(b) ∈ U
such that Q(b) (cid:22) R(b) + 1. We then have

(cid:27)

(cid:27)

αar1(a, b) + βR1(b) + β(Q1(b) − R1(b))

(cid:26)(cid:88)

a∈A

max
b∈B

αar1(a, b) + βQ1(b)

= max
b∈B

≤ max
b∈B

αar1(a, b) + βR1(b) + β

(cid:27)

= max
b∈B

αar1(a, b) + βR1(b)

+ β.

(cid:27)(cid:19)

(cid:27)

(cid:27)
(cid:27)(cid:19)

(cid:26)(cid:88)

a∈A

(cid:27)

(cid:26)(cid:88)
(cid:26)(cid:88)
(cid:26)(cid:88)

a∈A

a∈A

a∈A

(cid:27)
(cid:27)

a∈A

(cid:26)(cid:88)
(cid:26)(cid:88)
(cid:26)(cid:88)

a∈A

, max
b∈B

a∈A

≤ max
b∈B

, max
b∈B

(cid:27)

22

Similarly, we can show that

Thus

a∈A

(cid:26)(cid:88)
(cid:26)(cid:88)
(cid:26)(cid:88)

a∈A

max
b∈B

a∈A

max
b∈B

(cid:18)

max
b∈B

(cid:18)

(cid:22)

αar2(a, b) + βQ2(b)

αar2(a, b) + βR2(b)

αar1(a, b) + βQ1(b)

αar2(a, b) + βQ2(b)

αar1(a, b) + βR1(b)

αar2(a, b) + βR2(b)

+ β1.

+ β.

(17)

(cid:27)(cid:19)

011BBASS0xyABut since (cid:18)

(cid:26)(cid:88)

a∈A

max
b∈B

(cid:27)

αar1(a, b) + βQ1(b)

, max
b∈B

(cid:27)(cid:19)

αar2(a, b) + βQ2(b)

(cid:26)(cid:88)

a∈A

and since Φ(U) = Λ(Ψ(U)), there exists some (L1, L2) ∈ Φ(U) such that

(cid:18)

max
b∈B

(cid:26)(cid:88)
(cid:26)(cid:88)

a∈A

αar1(a, b) + βQ1(b)

(cid:27)

(cid:27)

, max
b∈B

(cid:26)(cid:88)
(cid:26)(cid:88)

a∈A

, max
b∈B

a∈A

Thus

(L1, L2) (cid:22)

(cid:18)

(L1, L2) (cid:22)

max
b∈B

a∈A

∈ Ψ(U),
(cid:27)(cid:19)
(cid:27)(cid:19)

αar2(a, b) + βQ2(b)

.

αar1(a, b) + βR1(b)

αar2(a, b) + βR2(b)

+ β1.

We can show the other direction (roles of Φ(U) and Φ(V) reversed) similarly and thus we have that

e(Φ(U), Φ(V)) ≤ β = βe(U, V).

(18)

Proof of Theorem 3.1. Since Φ is a contraction in the metric d, the sequence {An} is Cauchy in F.
Hence by Lemma 7.1, {An} converges to a Pareto frontier V∗ ∈ F. The continuity of the operator
further implies that V∗ = Φ(V∗). To show uniqueness, observe that if there are two ﬁxed points U
and V, then we have d(U, V) = d(Φ(U, Φ(V)) ≤ βd(U, V), which implies that d(U, V) = 0 and hence
U = V.
Proof of Theorem 3.2. In G∞, ﬁx T ≥ 1 and consider a truncated game where Alice can guarantee
the cumulative losses in βT +1V∗ after time T + 1. Then the minimal losses that she can guarantee
after time T is the set:

Λ

max
b∈B

αar1(a, b) + βT +1Q1(b), max
b∈B

αar2(a, b) + βT +1Q2(b)

(cid:18)(cid:26)

βT(cid:88)

a∈A

βT(cid:88)
(cid:27)(cid:19)

a∈A

| α ∈ ∆(A), Q(b) ∈ V∗ ∀ b ∈ B

.

1−β

(cid:26)

(cid:27)

This set is βT V∗. By induction, this implies that the set of minimal losses that she can guarantee
after time 0 is V ∗.

The losses of the truncated game and of the original game diﬀer only after time T + 1. Since
the losses at each step are bounded by (1 − β), the cumulative losses after time T + 1 are bounded
by βT +1(1−β)

= βT +1. Consequently, the minimal losses of the original game must be in the set

u ∈ [0, 1]2 : u1 ∈ [x1 − βT +1, x1 + βT +1], u2 ∈ [x2 − βT +1, x2 + βT +1], x ∈ V∗

.

Since T ≥ 1 is arbitrary, the minimal losses that Alice can guarantee in the original game must be
in V∗.

23

Proof of Theorem 3.3. Assume that Alice can guarantee every pair βT +1u of cumulative losses with
u ∈ V∗ after time T + 1 by choosing some continuation strategy in ΠA. Let x = F(p, V∗). We
claim that after time T , Alice can guarantee a loss of no more than βT x on each component by ﬁrst
choosing aT = a with probability αa(p) and then if Bob chooses b ∈ B, choosing a continuation
strategy that guarantees her F(p(cid:48), V∗), where p(cid:48) = q(b, p). Indeed by following this strategy, her
expected loss after time T is then

αa(p)rk(a, b) + βT +1Fk(q(b, p), V∗)} ≤ βT Fk(p, V∗) = βT xk

{βT(cid:88)

a

when the game is Gk. Thus, this strategy for Alice guarantees that her loss after time T is no more
than βT V∗. Hence by induction, following the indicated strategy (in the statement of the theorem)
for the ﬁrst T steps and then using the continuation strategy from time T + 1 onwards, guarantees
that her loss is not more than F(p1, V∗) after time 0. Now, even if Alice plays arbitrarily after time
T + 1 after following the indicated strategy for the ﬁrst T steps, she still guarantees that her loss
is no more than F(p1, V∗) + βT +1(1, 1)T . Since this is true for arbitrarily large values of T , playing
the given policy indeﬁnitely guarantees that her loss is no more than F(p1, V∗).

Proof of Proposition 4.1. We ﬁrst need the following lemma about the approximation operator.
Lemma 7.5. Consider a V ∈ F. Then

d(V, ΓN (V)) ≤

1
N

.

Proof. Any point in ΓN (V) is of the form λu + (1 − λ)v where u, v ∈ V. By the p-convexity of V,
(cid:27)
there is some r(λ) ∈ V, such that r(λ) (cid:22) λu + (1 − λ)v. Also clearly for any u ∈ V,
1
N

(cid:27)
||u − v||∞ : v ∈ ΓN (V)

||F(p, V) − F(p(cid:48), V)||∞ : |p(cid:48) − p| =

≤ max

(cid:26)

(cid:26)

1
N

min

=

.

Next, we have Gn = ΓN ◦ Φ(Gn−1)). Consider another sequence of Pareto frontiers

(cid:18)

An = Φn(G0)

(cid:19)

n∈N

Then we have

d(An, Gn) = d(Φ(An−1), ΓN (Φ(Gn−1)))

(a)

where inequality (a) is the triangle inequality and (b) follows from (18) and Lemma 7.5. Coupled
with the fact that d(A0, G0) = 0, we have that

(b)

1
N

≤ d(Φ(An−1), Φ(Gn−1)) + d(Φ(Gn−1), ΓN (Φ(Gn−1)))
≤ βd(An−1, Gn−1) +
(cid:18)
(cid:18) 1 − βn
1 + β + β2 + ··· βn−1

d(An, Gn) ≤

(cid:19)

(cid:19)

1
N

=

1
N

1 − β
24

(19)

(20)

(21)

Since Φ is a contraction, the sequence {An} converges to some pareto frontier V∗. Suppose that
we stop the generation of the sequences {An} and {Gn} at some n. Now since A0 = G0 = {(0, 0)},
and since the stage payoﬀs rk(a, b) ∈ [0, 1 − β], we have that d(A1, A0) ≤ 1 − β. This implies from
the contraction property that d(V∗, An) ≤ βn(1−β)
1−β = βn and thus by triangle inequality we have
1
N

(cid:18) 1 − βn

d(V∗, Gn) ≤

+ βn.

(cid:19)

(22)

1 − β

Proof of Proposition 4.2. In order to prove this result, we need a few intermediate deﬁnitions and
results. First, we need to characterize the losses guaranteed by any (2N + 1)-mode policy. Such a
policy π deﬁnes the following operator on any function F : {0,± 1

N ,±1} → R2 :

N ,± 2

max
b∈B

N ,··· ,± N−1

αa(p)r1(a, b) + κ(b, p)βF1(q(b, p)) + (1 − κ(b, p))βF1(q(cid:48)(b, p))(cid:9),

(cid:8)(cid:88)
αa(p)r2(a, b) + κ(b, p)βF2(q(b, p)) + (1 − κ(b, p))βF2(q(cid:48)(b, p))(cid:9)(cid:19)

a∈A

.

∆π

N (F)(p) =

(cid:18)
(cid:8)(cid:88)

a∈A

max
b∈B

For a function F : {0,± 1

N ,± 2

N ,··· ,± N−1

N ,±1} → R2, deﬁne the following norm:

(cid:107)F(cid:107) =

p∈{0,± 1

N ,± 2

max
N ,··· ,± N−1

N ,±1}(cid:107)F(p)(cid:107)∞.

We can easily show that ∆π

N is a contraction in the norm.

Lemma 7.6.

(cid:107)∆π

N (F) − ∆π

N (G)(cid:107) ≤ β(cid:107)F − G(cid:107).

We can then show the following result.

Lemma 7.7. Consider a (2N + 1)-mode policy π. Then there is a unique function

Fπ : {0,±

1
N

2
N

,±

,··· ,±

N − 1
N

,±1} → R2

(23)

(24)

such that ∆π
guarantees the vector of losses Fπ(p).

N (Fπ) = Fπ. Further, The policy π initiated at mode p where p ∈ {0,± 1

N ,± 2

N ,··· ,±1},

The ﬁrst part of the result follows from the fact that the operator is a contraction and the
completeness of the space of vector-valued functions with a ﬁnite domain for the given norm. The
second part follows from arguments similar to those in the proof of Theorem 3.3.

Vπn = Λ

ch({Fπn(p) : p ∈ {0,±

1
N

,··· ,±

N − 1
N

,±1}})

,

Now let

(cid:18)

(cid:19)

where Fπn is the ﬁxed point of the operator ∆πn
N .

25

Now deﬁne a sequence of functions Fn : {0,± 1

F(p, Φ(Gn−1)) = F(p, Gn). We then have that

N ,± 2

N ,±1} → R2 where Fn(p) =
N ,··· ,± N−1
(cid:19)

(25)

d(Vπn, V∗) ≤ d(Vπn, Gn) + d(Gn, V∗)

≤ d(Vπn, Gn) +

1
N

+ βn.

(cid:18) 1 − βn

1 − β

From the deﬁnition of d, it is clear that d(Vπn, Gn) ≤ (cid:107)Fπn − Fn(cid:107). Next we have

(cid:107)Fπn − Fn(cid:107) ≤ (cid:107)Fπn − ∆πn

N (Fn)(cid:107) + (cid:107)∆πn

N (Fn) − Fn(cid:107)

(a)

N (Fπn) − ∆πn

= (cid:107)∆πn
≤ β(cid:107)Fπn − Fn(cid:107) + (cid:107)Fn+1 − Fn(cid:107).

N (Fn)(cid:107) + (cid:107)Fn+1 − Fn(cid:107)

(b)

(26)

(27)

(28)

(29)

(30)

(31)

(32)

Here (a) holds because ∆πn
ﬁxed point of the operator ∆πn

N (Fn) = Fn+1 by the deﬁnition of the policy πn, and because Fπn is a

N is a contraction. Thus we have

N . (b) holds because ∆πn
d(Vπn, Gn) ≤ (cid:107)Fπn − Fn(cid:107) ≤ (cid:107)Fn+1 − Fn(cid:107)

.

1 − β

And ﬁnally we have:

(cid:19)

(cid:18) 1 − βn

1 − β

d(Vπn, V∗) ≤

1
N

+ βn + (cid:107)Fn+1 − Fn(cid:107)

1 − β

.

To ﬁnish up, we need the following result:

Lemma 7.8.

(cid:107)Fn+1 − Fn(cid:107) ≤ d(Gn+1, Gn).

Proof. Let u = Fn+1(p) and v = Fn(p) for some p. Now u is the point of intersection of Gn+1
and the line y = x + p. v is the point of intersection of the frontier Gn and the line y = x + p.
Now suppose that (cid:107)u − v(cid:107)∞ > d(Gn+1, Gn). Then either for u, there is no r ∈ Gn such that
r (cid:22) u + 1d(Gn+1, Gn) or for v, there is no r ∈ Gn+1 such that r (cid:22) v + 1d(Gn+1, Gn). Either of the
two cases contradict the deﬁnition of d(Gn+1, Gn). Thus (cid:107)u − v(cid:107)∞ ≤ d(Gn+1, Gn).

Finally, by the triangle inequality we have

d(Gn+1, Gn) ≤ d(An+1, An) + d(Gn+1, An+1) + d(Gn, An)

(cid:18) 1 − βn+1

(cid:19)

1 − β

1
N

(cid:18) 1 − βn

1 − β

+

1
N

(cid:19)

.

(33)

(34)

≤ (1 − β)βn +

Combining with (32) we have the result.

26

