6
1
0
2

 
r
a

 

M
4
1

 
 
]

G
L
.
s
c
[
 
 

1
v
3
5
1
4
0

.

3
0
6
1
:
v
i
X
r
a

Top-K Ranking from Pairwise Comparisons: When Spectral Ranking is Optimal

Top-K Ranking from Pairwise Comparisons: When Spectral

Ranking is Optimal

Minje Jang
Electrical Engineering, KAIST

Sunghyun Kim
Electronics and Telecommunications Research Institute
Daejeon, Korea

Changho Suh
Electrical Engineering, KAIST

Sewoong Oh
Industrial and Enterprise Systems Engineering, UIUC

Editor:

jmj427@kaist.ac.kr

koishkim@etri.re.kr

chsuh@kaist.ac.kr

swoh@illinois.edu

Abstract

We explore the top-K rank aggregation problem. Suppose a collection of items is com-
pared in pairs repeatedly, and we aim to recover a consistent ordering that focuses on the
top-K ranked items based on partially revealed preference information. We investigate the
Bradley-Terry-Luce model in which one ranks items according to their perceived utilities
modeled as noisy observations of their underlying true utilities. Our main contributions
are two-fold. First, in a general comparison model where item pairs to compare are given
a priori, we attain an upper and lower bound on the sample size for reliable recovery of the
top-K ranked items. Second, more importantly, extending the result to a random compar-
ison model where item pairs to compare are chosen independently with some probability,
we show that in slightly restricted regimes, the gap between the derived bounds reduces to
a constant factor, hence reveals that a spectral method can achieve the minimax optimality
on the (order-wise) sample size required for top-K ranking. That is to say, we demonstrate
a spectral method alone to be suﬃcient to achieve the optimality and advantageous in
terms of computational complexity, as it does not require an additional stage of maximum
likelihood estimation that a state-of-the-art scheme employs to achieve the optimality. We
corroborate our main results by numerical experiments.

Keywords: Bradley-Terry-Luce models, Optimal sample complexity, Pairwise measure-
ments, Spectral methods, Top-K ranking.

1. Introduction

Rank aggregation has been investigated in a variety of contexts such as social choice (Caplin
and Nalebuﬀ, 1991; Azari Souﬁani et al., 2014), web search and information retrieval (Dwork
et al., 2001), recommendation systems (Baltrunas et al., 2010), and crowd sourcing (Chen

1

Jang, Kim, Suh and Oh

et al., 2013), to name a few. The task aims to bring a consistent ordering to a collection of
items, given only partial preference information.

Due to its broad range of applications, a sheer volume of work on ranking has been
done. Of numerous ranking schemes developed in the literature, arguably most dominant
paradigms are spectral ranking algorithms (Brin and Page, 1998; Dwork et al., 2001; Negah-
ban et al., 2012; Seeley, 1949; Wei, 1952; Vigna, 2009) and maximum likelihood estimation
(MLE) (Ford, 1957; Hunter, 2004). Postulating the existence of underlying real-valued true
preferences of the items, these paradigms intend to produce preference estimates that are
consistent in a global sense, usually measured by (cid:96)2 estimation error, to order the items.
While it can be understood that such estimates are faithful globally with respect to the
latent preferences, it is not necessarily guaranteed that they result in optimal ranking accu-
racy. Accurate ranking has more to do with how well the ordering of the estimates matches
that of the true preferences, and less to do with how close the estimates are to the true
preferences minimizing overall estimation error.

In many realistic applications of interest, however, what we expect from accurate ranking
is not an ordering that respects the entire item preferences in a global sense. Instead, we
expect an ordering that precisely separates only a few items that have the highest ranks
from the rest.
In light of this, recent work (Chen and Suh, 2015) investigated top-K
identiﬁcation which aims to recover the correct set of top-ranked items only. As a result, it
characterized the minimax limit on the sample size (i.e., sample complexity) under a long-
lasting prominent statistical model, namely the Bradley-Terry-Luce (BTL) model (Bradley
and Terry, 1952; Luce, 1959).
In achieving the fundamental limit, its proposed scheme
called Spectral MLE merges the two popular paradigms in series so as to yield low (cid:96)∞
estimates, shown therein to be crucial in identifying top-ranked items with respect to their
preferences. To start with, Spectral MLE ﬁrst obtains preference estimates via a spectral
method, particularly Rank Centrality (Negahban et al., 2012), which produces estimates
with low squared loss. And by performing additional point-wise MLEs on the estimates, it
makes them have low (cid:96)∞ estimation error, leading to successful top-K ranking.

Analyzing (cid:96)∞ error bounds can be interesting, as we can see in (Chen and Suh, 2015)
where it has led to characterizing the minimax limit on the sample size for top-K ranking.
What makes it even more appealing is its technical challenge. Even after decades of research
since the introduction of spectral methods and MLE, two dominating approaches in the
literature, yet we lack notable results for (cid:96)∞ error bounds. Analytical techniques that have
proven useful to obtain tight (cid:96)2 error bounds do not translate well into obtaining meaningful
(cid:96)∞ error bounds. There lie our main contributions: tangible progress in (cid:96)∞ analysis.

Main contributions. In this work, we provide a tight analysis of (cid:96)∞ error bounds
of a spectral method, making progress toward richer understanding of rank aggregation.
The analysis makes it possible for us to characterize conditions under which the spectral
method achieves the minimax optimal performance, by comparing it with a fundamental
bound that delineates the performance limit beyond which any ranking algorithm cannot
achieve. To be more concrete, we investigate reliable recovery of top-K ranked items under
the BTL pairwise comparison model in which one ranks items according to their perceived
utilities modeled as noisy observations of their underlying true utilities. We consider mainly
two comparison models: one is a deterministic model in which item pairs we compare are
given a priori; the other is a random model in which item pairs we compare are chosen in

2

Top-K Ranking from Pairwise Comparisons: When Spectral Ranking is Optimal

a random and non-adaptive manner. As our main results, in the former model, we derive
an upper and lower bound on the sample size for reliable recovery of top-K ranked items
(Theorems 1 and 2), which respectively correspond to suﬃcient and necessary conditions
for reliable top-K identiﬁcation, when a spectral method Rank Centrality is employed.
Inspecting the gap between the derived bounds allows us to identify conditions under which
Rank Centrality can be optimal. We observe that, for well-balanced cases where the number
of distinct items that an item is compared to (which we call degree) does not deviate greatly
from its minimum to its maximum, how the gap scales can be nicely expressed in terms
of degree (details in Section 3 after Theorem 2). In the random model we consider, item
pairs we compare are ﬁrst chosen independently with probability p and the chosen pairs
are repeatedly compared (hence random and non-adaptive). Finding the random model ﬁt
for the well-balanced case, we extend the aforementioned results and get a stronger one.
We demonstrate that the gap shrinks to the order of constant (Theorem 3), hence show
that a spectral method alone can achieve the order-wise optimal sample complexity for
top-K ranking that has recently been characterized under the same model in (Chen and
Suh, 2015). There are two distinctions to note in comparison with the results in (Chen and
Suh, 2015). First, we show that a spectral method can achieve the limit in so-called dense
regimes where the number of distinct item pairs we compare is somewhat large. That is,
in comparison to the regimes in which Chen and Suh characterized the limit, the regimes
in which we achieve it are slightly restricted. Second, we show that applying only Rank
Centrality is suﬃcient to achieve the limit in the regimes mentioned earlier, hence is more
advantageous in terms of computational complexity in comparison to Spectral MLE that
merges a spectral method (particularly Rank Centrality in (Negahban et al., 2012)) and an
additional stage performing coordinate-wise MLEs.

Related work. Perhaps most relevant are (Chen and Suh, 2015) and (Negahban et al.,
2012). To the best of our knowledge, Chen and Suh focused on top-K identiﬁcation under
the random comparison model of our interest for the ﬁrst time. A key distinction with our
work is that while we employ only a spectral method to obtain bounds on (cid:96)∞ estimation
error, they incorporated an additional reﬁnement stage that performs successive point-wise
MLEs. Negahban et al. developed Rank Centrality on which our proposed ranking scheme
is solely based. Perhaps surprisingly, it was proved that Rank Centrality, a simple spectral
method, achieves the same performance as MLE in (cid:96)2 error. A priori, there is no reason
to believe that a spectral method can achieve such a strong minimax optimal performance.
In a similar spirit, we show that this spectral method is also minimax optimal in (cid:96)∞ error,
achieving the same optimality guarantee as the MLE based algorithm in (Chen and Suh,
2015). The main objective of our work is in identifying the regimes where spectral methods
are as good as MLE, and proving the minimax optimality of Rank Centrality in those
regimes.

Maystre and Grossglauser (2015) recently developed an algorithm that also shares a
spirit of spectral ranking, called Iterative Luce Spectral Ranking (I-LSR), and showed its
performance to be the same as MLE for underlying preference scores. Rajkumar and Agar-
wal (2014) put forth statistical assumptions that ensure the convergence of several rank
aggregation methods including Rank Centrality and MLE to an optimal ranking. They
derived sample complexity bounds, although the statistical optimality is not rigorously jus-
tiﬁed and total ordering instead of top-K ranking is concerned. In the pairwise preference

3

Jang, Kim, Suh and Oh

setting, many works with diﬀerent interests from ours have been done. Some studied active
ranking where samples are obtained adaptively. Jamieson and Nowak (2011) considered
perfect total ranking and characterized the query complexity gain of adaptive sampling in
the noise-free case, and the works of (Braverman and Mossel, 2008; Jamieson and Nowak,
2011; Ailon, 2012; Wauthier et al., 2013) explored the query complexity in the presence of
noise while aiming at approximate total rankings. Eriksson (2013) proposed a scheme that
intends to ﬁnd top-K queries when observation errors are assumed to be i.i.d. Some works
looked into models diﬀerent from the BTL model. The works of (Lu and Boutilier, 2011;
Busa-Fekete et al., 2014) considered ranking problems with pairwise comparison data under
the Mallows model (Mallows, 1957). Azari Souﬁani et al. (2013) broke full rankings into
pairwise comparisons toward parameter estimation under the Plackett-Luce (PL) model
(Plackett and Luce, 1975). Hajek et al. (2014), under the PL model, derived minimax
lower bounds of parameter estimation error when schemes that break partial rankings into
pairwise comparisons are used.

Very recently, Shah and Wainwright (2015) showed that a simple counting method
(Borda, 1781) can achieve the fundamental limit on the sample size, up to constant factors,
for top-K ranking under a general parametric model in which observations depend only on
the predeﬁned probabilities of one item preferred to another (Shah et al., 2015), including
the BTL model as a special case. However, their assumption that the number of comparisons
for each item pair follows a Binomial distribution led to a nearly complete observation model
where almost every item pair is compared at least once. On the contrary, we examine
a diﬀerent observation model (which we will describe in detail soon), also considered in
(Negahban et al., 2012) and (Chen and Suh, 2015), which well captures the comparison
graph structure that aﬀects the sample complexity (see Theorem 1 of (Negahban et al.,
2012) and numerical experiments in Section 4).

Notation. Unless speciﬁed otherwise, we use [n] to represent {1, 2, . . . , n}, and Gn,p
to represent an Erd˝os-R´enyi random graph where total n vertices reside and each pair of
vertices is connected by an edge independently with probability p, and di to represent the
out-degrees of vertex i.

2. Problem Formulation

Comparison model and assumptions. Suppose we perform a few pairwise evaluations
on n items. To gain a statistical understanding toward the ranking limits, we assume the
pairwise comparison outcomes are generated based on the Bradley-Terry-Luce (BTL) model
(Bradley and Terry, 1952; Luce, 1959), a long-established model that has been studied in
numerous applications (Agresti, 2014; Hunter, 2004).

• Preference scores. The BTL model postulates the existence of an underlying prefer-
ence vector w := {w1, w2, . . . , wn}, where wi represents the preference score of item i.
The outcome of each pairwise comparison depends solely on the latent scores of the
items being compared. Without loss of generality, we assume that

w1 ≥ w2 ≥ ··· ≥ wn > 0.

(1)

4

Top-K Ranking from Pairwise Comparisons: When Spectral Ranking is Optimal

We assume that the range of the scores is ﬁxed irrespective of n. For some positive
constants wmin and wmax:

wi ∈ [wmin, wmax],

1 ≤ i ≤ n,

(2)

In fact, the case in which the range wmax
grows with n can be translated into the
wmin
above ﬁxed-range regime by separating out those items with vanishing scores (e.g.
via a voting method like Borda count (Borda, 1781; Ammar and Shah, 2011)).

• Comparison model. We denote by G = ([n],E) a comparison graph in which items
i and j are compared if and only if (i, j) belongs to the edge set E. We take into
account two kinds of comparison graphs. We examine general comparison graphs
which can exhibit all possible topologies described by an edge set E given a vertice
set [n]. Furthermore, we investigate random comparison graphs constructed by the
Erd˝os-R´enyi random graph model in which each pair of vertices is connected by an
edge independently with probability p.

• Pairwise comparisons. For each (i, j) ∈ E, we observe L comparisons between items i
((cid:96))
ij , is generated

and j. The outcome of the (cid:96)th comparison between them, denoted by y
based on the BTL model:

((cid:96))

ij =(cid:40) 1 with probability

otherwise,

0

y

wi

wi+wj

(3)

((cid:96))

((cid:96))
ij = 1 indicates that item i is preferred over item j. We adopt the convention
where y
((cid:96))
ji = 1−y
y
ij ’s are jointly independent over all
(cid:96) and i < j. For ease of presentation, we represent the collection of suﬃcient statistics
as

ij . We assume that conditional on G, y

((cid:96))

y := {yij : i < j,∈ E}, yij :=

1
L

L(cid:88)(cid:96)=1

((cid:96))
ij .

y

(4)

Performance metric and goal. Given the pairwise comparisons, one wishes to know
whether or not the top-K ranked items are identiﬁable. In light of this, we consider the
probability of error Pe in identifying the correct set of the top-K ranked items, namely,

Pe(ψ) := P{ψ(y) (cid:54)= [K]} ,

(5)

where ψ is any ranking scheme that returns a set of K indices and [K] is the set of the
ﬁrst K indices. Our goal in this work is to characterize the admissible region Rw of L in
which top-K ranking is feasible for a given BTL parameter w, in other words, Pe can be
vanishingly small as n grows. The admissible region Rw is deﬁned as

Rw :=(cid:110)L :

lim

n→∞ Pe(ψ(y)) = 0(cid:111) .

5

(6)

Jang, Kim, Suh and Oh

For a comparison graph G = ([n],E), we are interested in the sample complexity deﬁned

as,

Sδ := min
L∈Z+

a∈Ωδ {L|E| : L ∈ Ra} ,
sup

(7)

where Ωδ = {a ∈ Rn : (aK − aK+1)/amax ≥ δ}. Note that the way the sample complexity

is deﬁned as (7) shows that we investigate minimax scenarios in which nature may behave
in an adversarial manner with the worst-case preference scores w.

3. Main Results

The most crucial part of top-K ranking hinges on separating the two items near the decision
boundary, i.e., the Kth and (K + 1)th ranked items. Unless the gap is large enough, noise in
the observations can lead to erroneous estimates. In view of this, we pinpoint a separation
measure as

∆K :=

wK − wK+1

wmax

.

(8)

This measure turns out to play a key role in determining the fundamental limits of top-K
identiﬁcation.

As noted in (Ford, 1957), if the comparison graph G is not connected, then it is impossible
to determine the relative preferences between two disconnected components. Hence, we
assume all comparison graphs considered in this paper are connected. For Erd˝os-R´enyi
model, we make the following assumption for the connectivity:

p >

log n

n

.

(9)

Our main ﬁndings are suﬃcient and necessary conditions derived for reliable top-K
identiﬁcation for a general comparison graph. Especially, for a random comparison graph
according to the Erd˝os-R´enyi model, we can attain an order-wise tight suﬃcient condition
for feasible top-K ranking. We ﬁrst state our results for general comparison graphs.

Theorem 1 Given a comparison graph G = ([n],E) and L ≥(cid:24)c1
γdmin (cid:107)L2(cid:107)2,∞(cid:19)2 |E| log n

L|E| ≥(cid:18)c2 + c3

√ndmax

dmax∆2
K

,

(10)

log n

dmax(cid:16) dmax

γdmin(cid:17)2(cid:25), if

then Rank Centrality correctly identiﬁes the top-K ranked items with probability at least 1−
2n−2, where c1, c2 and c3 are some numerical constants, L is the Laplacian matrix of graph
|Aij|2(cid:33).
G whose entries are deﬁned as Lij := 1
Here γ is the spectral gap of matrix L deﬁned as the diﬀerence between the two largest
absolute eigenvalues of L, dmax is the maximum out-degree of vertices in E and dmin is the
minimum.

I[(i, j) ∈ E], and (cid:107)A(cid:107)2,∞ := max

j (cid:32)(cid:114)(cid:80)i

di

6

Top-K Ranking from Pairwise Comparisons: When Spectral Ranking is Optimal

Note that in terms of the sample complexity deﬁned as (7), this theorem establishes a
suﬃcient condition of the sample complexity for reliable top-K ranking. Precisely,

S∆K

(cid:46) |E|

dmax(cid:18)1 +

√ndmax

γdmin (cid:107)L2(cid:107)2,∞(cid:19)2 log n

∆2
K

.

(11)

We provide the proof of this theorem in Section 6.

What follows next is a necessary condition for reliable top-K ranking.

Theorem 2 Fix  ∈ (0, 1

2 ). Given a comparison graph G = ([n],E), if

L|E| ≤ c4(1 − )

n log n

∆2
K

,

(12)

for some numerical constant c4, then for any ranking scheme ψ, there exists a preference
score vector w with seperation ∆K such that Pe(ψ) ≥ .
This result implies that we need at least L|E| ≥ c4
when we express the result of Theorem 2 in terms of the sample complexity, that is

for reliable top-K ranking. Then,

n log n

∆2
K

S∆K

(cid:38) n log n

∆2
K

.

(13)

The proof is a generalized version of Theorem 2 in (Chen and Suh, 2015). We provide the
proof of this theorem in Section 7.

and

|E|
dmax

For well-balanced cases where dmin = Θ(dmax), one can verify that (cid:107)L2(cid:107) is on the order
of (cid:113) 1
n + 1
is on the order of n. Taking these two together, the gap between
d2
min

the necessary condition and the suﬃcient condition can be shown as a factor of 1 + n
.
d2
We note that for well-balanced graphs, when dmin is at least O(√n), the gap disappears.
min
That is, Rank Centrality is optimal. We make this point precise in the following theorem
where we analyze comparisons over Erd˝os-R´enyi graphs.

Theorem 3 Suppose G ∼ Gn,p. There exist positive numerical constants c4 > 1, c5 and c6

such that if p ≥ c4(cid:113) log n

n , L ≥(cid:108)c5

log n

np (cid:109), and

n2pL
2 ≥ c6

n log n

∆2
K

,

(14)

then Rank Centrality correctly identiﬁes the top-K ranked items with probability at least

1 − 4n−α, where α := min(1, 3
28 c2

4).

This result oﬀers a much tighter bound than Theorem 1. In terms of the sample complexity,
a suﬃcient condition of the sample complexity on a random comparison graph is

S∆K

(cid:46) |E| log n
np∆2

K (cid:16)

n log n

∆2
K

,

7

(15)

Jang, Kim, Suh and Oh

since the suﬃcient condition for reliable ranking on a random comparison graph given by

(14) is L|E| (cid:38) |E| log n

np∆2
K

, and the number of item pairs being compared |E| concentrates to n2p

2

for Erd˝os-R´enyi random comparison graphs. Note that this suﬃcient condition for reliable
top-K ranking matches the necessary condition in (13). That is, for random comparison
graphs that follow the Erd˝os-R´enyi model, we can establish the minimax optimality of Rank
Centrality. Precisely,

We provide the proof of this theorem in Section 8.

S∆K (cid:16)

n log n

∆2
K

.

(16)

Figure 1: Spectral MLE, which merges Rank Centrality and an additional reﬁnement stage
that performs coordinate-wise MLEs, achieves reliable top-K ranking in the entire
admissible region depicted above. Our analysis reveals that in the dense regime
Rank Centrality alone suﬃces to achieve it.

Our main contribution is the establishment of a suﬃcient condition for top-K identiﬁ-
cation, which matches the necessary condition derived in (Chen and Suh, 2015) for random
comparison graphs constructed by the Erd˝os-R´enyi model. It is important to point out two
notable distinctions in achievability compared to (Chen and Suh, 2015). First, a spectral
method such as Rank Centrality (Negahban et al., 2012) suﬃces to achieve the order-wise
tight sample complexity, without relying on an additional process of local reﬁnement em-
ployed in (Chen and Suh, 2015). Second, our main results concern a slightly denser regime,

n , in which many distinct item pairs are likely to be

indicated by the condition p (cid:38)(cid:113) log n
compared. As shown in (Chen and Suh, 2015), the dense regime condition p (cid:38)(cid:113) log n

is not
necessary for top-K identiﬁcation. However, it is not clear yet whether or not the condition
is required under our approach that employs only a spectral method. Our speculation is
that the sparse regime condition, indicated by log n
n , may not be suﬃcient for
n
spectral methods to achieve reliable top-K identiﬁcation (to be discussed in Section 4).

(cid:46) p (cid:46)(cid:113) log n

n

experiments (to be illustrated in Section 4). In the dense regime indicated by p (cid:38)(cid:113) log n

To validate our main result based on the Erd˝os-R´enyi model, we conducted numerical
n ,
the experimental results clearly illustrate that Rank Centrality alone (a spectral method)
achieves reliable top-K identiﬁcation as Spectral MLE does. In the sparse regime indicated

8

Lp∼lognn∼!lognnSPARSEREGIMEDENSEREGIMEADMISSIBLEREGIONINFEASIBLEREGIONTop-K Ranking from Pairwise Comparisons: When Spectral Ranking is Optimal

by log n
n
aforementioned speculation.

(cid:46) p (cid:46)(cid:113) log n

n , however, Rank Centrality fails to achieve it, which leads us to the

Remark 4 As mentioned earlier, our ranking algorithm is based solely on a spectral method,
Rank Centrality in (Negahban et al., 2012), which enjoys nearly-linear time computational
complexity. Hence, not only can the information-theoretic limit promised by (16) be achieved
by a computationally eﬃcient low-complexity algorithm, but also we can achieve it with much
less computational overhead as compared to Spectral MLE in (Chen and Suh, 2015), which
employs an additional reﬁnement stage.

Remark 5 By the hypothesis p (cid:38) (cid:113) log n

that, unless n log n
to characterize is on the order of n√n log nL, not n log n

in Theorem 3, n2pL (cid:38) n√n log nL. It means
(cid:38) n√n log nL, the minimax optimality of the sample complexity we claim
. We note that we consider a regime

∆2
K

n

∆2
K

nL2(cid:17) 1
where ∆K is not on the constant order, so it is reasonable to assume ∆K (cid:46)(cid:16) log n

(cid:38) n√n log nL. Note that since there are n items each with wi ∈ [wmin, wmax],

leads to n log n
a typical regime of ∆K scales as 1/n. Therefore, we conclude that the minimax optimality
of the sample complexity is on the order of n log n

4 , which

∆2
K

.

∆2
K

4. Experimental Results

We consider both dense (p (cid:38)(cid:113) log n

We conduct a series of synthetic experiments to corroborate our main result in Theorem 3.
n ) regimes. To be more
precise, we set constant c1 = 2, and set pdense = 0.25 and psparse = 0.025, to make each be
in its proper range. To specify the implementation parameters, we use n = 500, K = 10,
and ∆K = 0.1. Each result in all numerical simulations is obtained by averaging over 10000
Monte Carlo trials.

(cid:46) p (cid:46)(cid:113) log n

n ) and sparse ( log n

n

Figure 2: Dense regime (pdense = 0.25): empirical (cid:96)∞ estimation error v.s. L (left); empiri-

cal success rate v.s. L (right).

Figure 2 illustrates the numerical experiments conducted in the dense regime. We see
that as L increases, meaning as we get to obtain pairwise evaluation samples beyond the
minimal sample complexity, (1) the (cid:96)∞ estimation error of Rank Centrality decreases and

9

L:numberofrepeatedcomparisons1510152025ℓ∞normofestimationerrors00.10.20.30.40.5RankCentrality:p=0.25SpectralMLE:p=0.25L:numberofrepeatedcomparisons1510152025empiricalsuccessrate00.10.20.30.40.50.60.70.80.91RankCentrality:p=0.25SpectralMLE:p=0.25BordaCount:p=0.25Jang, Kim, Suh and Oh

Figure 3: Sparse regime (psparse = 0.025): empirical (cid:96)∞ estimation error v.s. L (left);

empirical success rate v.s. L (right).

soon meets that of Spectral MLE (left); (2) the success rate of Rank Centrality increases and
soon hits 100% along with Spectral MLE (right). The curves clearly support our results; in
n , Rank Centrality (a spectral method) alone can

the dense regime speciﬁed by p (cid:38)(cid:113) log n

achieve reliable top-K ranking.

Figure 3 illustrates the numerical experiments conducted in the sparse regime. We
see that, in contrast with the experiments in the dense regime, as L increases, (1) the (cid:96)∞
estimation error of Rank Centrality decreases but does not meet that of Spectral MLE (left);
(2) the success rate of Rank Centrality increases but does not reach that of Spectral MLE
which hits nearly 100% (right). The curves lead us to speculate that the sparse regime
condition speciﬁed by log n
n
achieve reliable top-K identiﬁcation.

n may not be suﬃcient for spectral methods to

(cid:46) p (cid:46) (cid:113) log n

5. Conclusion and Future Work

We investigated top-K rank aggregation from pairwise data. We demonstrated that a
spectral method alone, which features nearly-linear time computational complexity, is suf-
ﬁcient in achieving the minimal sample complexity in the dense regime. Some limitations
of our results suggest future directions. Exploring if a spectral method can also achieve
reliable top-K identiﬁcation in (part of) the sparse regime would be the most interesting
one. Maystre and Grossglauser (2015) proposed an Iterative Luce Spectral Ranking (I-LSR)
algorithm that has a spirit of spectral ranking, and showed that surprisingly the perfor-
mance of I-LSR is the same as MLE for underlying preference scores. Motivated by their
results, we can set out to investigate I-LSR to see if it can achieve the minimax optimality
in the sparge regime. Analyzing spectral methods under comparison graphs not limited
to the Erd˝os-R´enyi graphs as well as other choice models such as the Plackett-Luce model
(Plackett and Luce, 1975; Hajek et al., 2014; Maystre and Grossglauser, 2015) could be
another.

10

L:numberofrepeatedcomparisons1050100150200250ℓ∞normofestimationerrors00.10.20.30.40.5RankCentrality:p=0.025SpectralMLE:p=0.025L:numberofrepeatedcomparisons1050100150200250empiricalsuccessrate00.10.20.30.40.50.60.70.80.91RankCentrality:p=0.025SpectralMLE:p=0.025BordaCount:p=0.025Top-K Ranking from Pairwise Comparisons: When Spectral Ranking is Optimal

6. Proof of Theorem 1

6.1 Algorithm Description

Algorithm 1 Rank Centrality (Negahban et al., 2012)

Input: The collection of suﬃcient statistics y =(cid:110)yij : (i, j) ∈ E, yij = 1

Compute the transition matrix ˆP = [ ˆPij]1≤i,j≤n:

(cid:96)=1 y

L(cid:80)L

((cid:96))

ij (cid:111).

ˆPij =

1

dmax

yij
dmax(cid:80)k:(k,j)∈E ykj
1 − 1
0

if (i, j) ∈ E;
if i = j;
otherwise.

Output the stationary distribution of matrix ˆP .

In an ideal scenario where we obtain an inﬁnite number of samples per pairwise com-
. Then, constructed matrix

parison, i.e., L → ∞, suﬃcient statistics yij converge to
ˆP deﬁned in Algorithm 1 becomes a matrix P whose entries [Pij]1≤i,j≤n are deﬁned as

wi+wj

wi

Pij =

1

wi

dmax

wi+wj

dmax(cid:80)k:(k,j)∈E
1 − 1
0

wk

wk+wj

if (i, j) ∈ E;
if i = j;
otherwise.

(17)

The entries for observed pairs of items, Pij = 1
, represent the relative likelihood
dmax
of item i being preferred to item j. Intuitively, random walks of P over the long run will
visit some states (corresponding to items) more often, if they have been preferred to other
frequently-visited states or preferred to many other states.

wi+wj

wi

The random walks have some properties that lead us to a desirable outcome. We can
see that the walks are reversible, as wiPji = wjPij holds, thus have a stationary distribution
equal to the preference score vector w = {w1, . . . , wn}, up to some constant scaling. We can
also see that under the assumption that guarantees connectivity, the walks are irreducible,
thus the stationary distribution is unique. To ﬁnd the stationary distribution of the walks
of P is to retrieve the precise underlying preference scores.

It is clear that random walks of ˆP , which can be viewed as a noisy version of P , will
give us an approximation to the ground-truth preference scores. The algorithm described
above adopts a power method to compute the stationary distribution. Power methods are
known to be computationally eﬃcient in obtaining the leading eigenvalue of a sparse matrix
(Meirovitch, 1997). Initially starting with the uniform distribution over [n], the algorithm
iteratively computes the following until convergence:
p(t) = ˆP p(t−1),

(18)

where p(t) is a vector that represents the distribution of a random walk at iteration t. When
convergence is reached, the algorithm returns the indices of the K largest components of
the distribution, which are the top-K ranked items.

11

Jang, Kim, Suh and Oh

6.2 Proof Outline

To distinguish the top-K items from the rest, the pointwise error of each item becomes a
fundamental bottleneck for top-K ranking. It will be impossible to separate the Kth and
(K + 1)th ranked items unless their score separation exceeds the aggregate error of the
score estimates for the two items. Based on this observation, we focus on ﬁguring out the
maximal pointwise error (cid:107) ˆw − w(cid:107)∞.
For the sake of clear demonstration, we use our stronger result in Theorem 3 attained
by extending the results in Theorems 1 and 2 to the Erd˝os-R´enyi model. The explanations
carry over to the general model of our interest just as well, illustrating the motivation and
the needed steps toward proving Theorem 1. Now, let us see how the following (cid:96)∞ norm
bound on the pointwise error (derived under the Erd˝os-R´enyi random comparison graphs
model) plays a key role in top-K identiﬁcation:

(cid:46)(cid:115) log n

npL

,

(cid:107) ˆw − w(cid:107)∞
(cid:107)w(cid:107)∞
np (cid:109) where c4 and c5 are some constants.

and L ≥(cid:108)c5

given p > c4(cid:113) log n
We assume (cid:107)w(cid:107)∞ = wmax = 1 for ease of presentation. Suppose ∆K = wK − wK+1 (cid:38)
(cid:113) log n
npL , then

ˆwi − ˆwj ≥ wi − wj − | ˆwi − wi| − | ˆwj − wj| ≥ wK − wK+1 − 2(cid:107) ˆw − w(cid:107)∞ > 0,

(19)

(20)

log n

n

items as desired. Hence, as long as ∆K (cid:38)(cid:113) log n

for all 1 ≤ i ≤ K and j ≥ K + 1, indicating that the algorithm will output the top-K
npL holds (coinciding with the claimed bound
in (19)), in other words, pL (cid:38) log n
holds, reliable top-K ranking is guaranteed with the
sample size n2pL
2

(cid:38) n log n

n∆2
K

.

What remains toward proving Theorem 1 is the proof of the following, which is an (cid:96)∞
norm bound on the maximal pointwise error in general comparison graphs (i.e., a generalized
version of (19)):

∆2
K

(cid:107) ˆw − w(cid:107)∞

(cid:107)w(cid:107)∞

(cid:46)(cid:18)1 +

√ndmax

γdmin (cid:107)L2(cid:107)2,∞(cid:19)(cid:114) log n

Ldmax

,

(21)

To prove (21), we ﬁrst derive an upper bound (which we will prove at the end of this
section) on the pointwise error between the score estimate of item i at iteration t and the
true score, which consists of three terms:

log n

dmax(cid:16) dmax

given L ≥(cid:24)c1

γdmin(cid:17)2(cid:25) where c1 is some constant.
− wj| ˆPij +(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:88)j:j(cid:54)=i
We consider the regime where n is suﬃciently large. For L ≥(cid:24)c1

− wi| ˆPii + (cid:88)j:j(cid:54)=i

i − wi| ≤|p

(t−1)
|p
j

(t−1)
i

|p

(t)

.

(22)

(wi + wj)(cid:16) ˆPji − Pji(cid:17)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
dmax(cid:16) dmax

γdmin(cid:17)2(cid:25), applying

log n

Then, we use the three lemmas stated below (which we prove in the following sections).

12

Top-K Ranking from Pairwise Comparisons: When Spectral Ranking is Optimal

Lemmas 6, 7 and 8 to (22) and solving it, we get

√ndmax

γdmin (cid:107)L2(cid:107)2,∞(cid:19)(cid:114) log n

Ldmax

+ t,

(23)

1−λ and t > 0 is a term that van-

√

ndmax

γdmin (cid:107)L2(cid:107)2,∞(cid:17)(cid:113) log n

Ldmax

as t tends to inﬁnity. Since it holds for all i, we complete the proof of (21).

(t)

(0)

1−λ , c3 := c9

Lemma 6 For a comparison graph G = ([n],E),

i − wi(cid:12)(cid:12)(cid:12) +(cid:18)c2wmax + c3wmax
(cid:12)(cid:12)(cid:12)p
i − wi(cid:12)(cid:12)(cid:12) ≤ λt(cid:12)(cid:12)(cid:12)p
where λ < 1, c1 := max(cid:0)c7, 4(1 + b)2γ2(cid:1), c2 := c8+2
ishes as t tends to inﬁnity. The above bound converges to wmax(cid:16)c2 + c3
(wi + wj)(cid:16) ˆPji − Pji(cid:17)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≤ 2wmax(cid:114) log n
dmin(cid:17)2
dmax(cid:16) dmax

with probability at least 1 − 2n−2.
Lemma 7 Suppose L ≥ 4(1 + b)2 log n

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:88)j:j(cid:54)=i

, where b := wmax
wmin

Ldmax

. Then,

ˆPii < 1

(24)

(25)

with probability at least 1 − 2n−2.
dmax(cid:16) dmax
γdmin(cid:17)2
Lemma 8 Suppose L ≥ c7
j − wj| ˆPij ≤ wmax(cid:18)c8 + c9

|p

log n

(t)

(cid:88)j:j(cid:54)=i

. Then, in the regime where n is suﬃciently large,

√ndmax

γdmin (cid:107)L2(cid:107)2,∞(cid:19)(cid:114) log n

Ldmax

+ 6wmax√btct

10

(26)

with probability at least 1 − 2n−2, where c7, c8, c9 and c10 < 1 are some constants.

We prove (22) here, and the proofs of the three lemmas are to follow.

Proof of (22): For ﬁxed i, applying p(t) = ˆP p(t−1), we get

(t)
i = p

p

(t−1)
i

(t−1)
p
j

ˆPij.

ˆPii + (cid:88)j:j(cid:54)=i

(27)

Using the fact that random walks on an ideal version of matrix ˆP (matrix P ) are reversible,
we get

wi = wi(cid:32)1 − (cid:88)j:j(cid:54)=i
=(cid:40)wi ˆPii + (cid:88)j:j(cid:54)=i

Pji = wi(cid:32)1 − (cid:88)j:j(cid:54)=i

Pji(cid:33) + wi (cid:88)j:j(cid:54)=i
wi(cid:16) ˆPji − Pji(cid:17)(cid:41) +(cid:40)(cid:88)j:j(cid:54)=i

Pji(cid:33) + (cid:88)j:j(cid:54)=i
wj ˆPij − (cid:88)j:j(cid:54)=i

wjPij

wj(cid:16) ˆPij − Pij(cid:17)(cid:41).

(28)

13

Jang, Kim, Suh and Oh

Using (27) and (28), we get

p

(t)

i − wi =(cid:16)p

(t−1)
i

wi(cid:16) ˆPji − Pji(cid:17) + (cid:88)j:j(cid:54)=i(cid:16)p

− wi(cid:17) ˆPii − (cid:88)j:j(cid:54)=i
dmax − ˆPij from yji = 1 − yij. Similarly, Pji = 1

− wj(cid:17) ˆPij + (cid:88)j:j(cid:54)=i

(t−1)
j

We note that ˆPji = 1

ˆPji − Pji = −(cid:16) ˆPij − Pij(cid:17). Applying this equality and the triangle inequality to (29), we

get the recursive relation (22).

dmax − Pij. Thus,

wj(cid:16) ˆPij − Pij(cid:17) .

(29)

6.3 Proof of Lemma 6
From the deﬁnitions of ˆPji and Pji,

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:88)j:j(cid:54)=i
(wi + wj)(cid:16) ˆPji − Pji(cid:17)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
P(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:88)j:j(cid:54)=i

L(cid:88)(cid:96)=1(cid:16)(wi + wj) y

((cid:96))

First, let us bound the absolute value of the summations in (30). Under the model of
((cid:96))
our interest, all pairwise comparison samples y
ji ’s are independent over (i, j) pair and (cid:96).
Applying the Hoeﬀding inequality, conditional on G ∼ Gn,p, we get

Then we choose t = 2wmax√Ldi log n, to get the tail probability as follows:

.

((cid:96))

1

=

(30)

ji − wj(cid:17) I [(i, j) ∈ E](cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
Ldi(2wmax)2(cid:19) . (31)

L(cid:88)(cid:96)=1(cid:16)(wi + wj) y

Ldmax(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:88)j:j(cid:54)=i
> t(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
ji − wj(cid:17) I [(i, j) ∈ E](cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
2 exp(cid:32)−
2(cid:0)2wmax√Ldi log n(cid:1)2
ji − wj(cid:17) I [(i, j) ∈ E](cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≤ 2wmax(cid:112)Ldi log n.

G ≤ 2 exp(cid:18)−
(cid:33) = 2n

(32)

(33)

Ldi(2wmax)2

−2.

2t2

((cid:96))

(wi + wj)(cid:16) ˆPji − Pji(cid:17)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≤ 2wmax(cid:114) log n

Ldmax

.

(34)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:88)j:j(cid:54)=i(cid:16) ˆPji − Pji(cid:17)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≤(cid:114) log n

Ldmax

14

.

(35)

Therefore, with probability at least 1 − 2n−2,

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:88)j:j(cid:54)=i

L(cid:88)(cid:96)=1(cid:16)(wi + wj) y
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:88)j:j(cid:54)=i

6.4 Proof of Lemma 7

probability at least 1 − 2n−2,

Now, let us put (33) into (30) and use di ≤ dmax. Conditional on G ∼ Gn,p, with probability
at least 1 − 2n−2,

Using the Hoeﬀding inequality, as in the proof of Lemma 6, one can easily verify that, with

Top-K Ranking from Pairwise Comparisons: When Spectral Ranking is Optimal

Using (35), we get

ˆPii = 1 − (cid:88)j:j(cid:54)=i

ˆPji ≤ 1 − (cid:88)j:j(cid:54)=i

Pji +(cid:114) log n

Ldmax

.

(36)

1

1

dmax

1 + b

I [(i, j) ∈ E] =

di

dmax

1
1 + b ≥

dmin
dmax

1

1 + b

.

(37)

(38)

(39)

(40)

(42)

(43)

We let b = wmax
wmin

. From the deﬁnition of Pji,

(cid:88)j:j(cid:54)=i

Pji = (cid:88)j:j(cid:54)=i

1

1

dmax

1 + wi
wj

I [(i, j) ∈ E] ≥ (cid:88)j:j(cid:54)=i

Putting (37) into (36), we get

Choosing L = 4(1 + b)2 log n
dmax

)2, we complete the proof of Lemma 7.

dmin
dmax

1

1 + b

+(cid:114) log n

Ldmax

.

ˆPii ≤ 1 −
( dmax
dmin

6.5 Proof of Lemma 8

We deﬁne a sequence as follows.

From Lemma 6, with probability at least 1 − 2n−2,

(t)

j − wj(cid:12)(cid:12)(cid:12) ˆPij.

A(t) := (cid:88)j:j(cid:54)=i(cid:12)(cid:12)(cid:12)p
(wi + wj)(cid:16) ˆPji − Pji(cid:17)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≤ 2wmax(cid:114) log n
ˆPij + (cid:88)j:j(cid:54)=i (cid:88)k:k(cid:54)=j(cid:12)(cid:12)(cid:12)p

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:88)j:j(cid:54)=i
− wj(cid:12)(cid:12)(cid:12) ˆPjj ˆPij + 2wmax(cid:114) log n
Ldmax (cid:88)j:j(cid:54)=i

Ldmax

.

Putting (22) with (40) into (39), we get

(t−1)
k

− wk(cid:12)(cid:12)(cid:12) ˆPjk ˆPij.

(41)

(t−1)
j

A(t) ≤ (cid:88)j:j(cid:54)=i(cid:12)(cid:12)(cid:12)p
ˆPij gives(cid:80)j:j(cid:54)=i

Putting(cid:80)j:j(cid:54)=i

We simplify the last two terms. The ﬁrst of the two is straightforward. The deﬁnition of
ˆPij ≤ 1. The last term needs an extra eﬀort. We defer the proof to a later

part of this section, stating the following for now.

(t−1)
k

(cid:88)j:j(cid:54)=i (cid:88)k:k(cid:54)=j(cid:12)(cid:12)(cid:12)p

− wk(cid:12)(cid:12)(cid:12) ˆPjk ˆPij ≤(cid:13)(cid:13)(cid:13)p(t−1) − w(cid:13)(cid:13)(cid:13)2 (cid:107)L2(cid:107)2,∞.

ˆPij ≤ 1 and (42) into (41), we get

(t−1)
j

A(t) ≤ (cid:88)j:j(cid:54)=i(cid:12)(cid:12)(cid:12)p

− wj(cid:12)(cid:12)(cid:12) ˆPjj ˆPij + 2wmax(cid:114) log n

Ldmax

+(cid:13)(cid:13)(cid:13)p(t−1) − w(cid:13)(cid:13)(cid:13)2 (cid:107)L2(cid:107)2,∞.

15

Jang, Kim, Suh and Oh

From Lemma 7, we can ﬁnd a constant β such that ˆPjj ≤ β < 1 for all j. Using such β, we
get

We now use an upper bound that prior work derived on (cid:13)(cid:13)p(t) − w(cid:13)(cid:13)2 (see Lemma 2 of

, for some constants c11 < 1 and

(Negahban et al., 2012)). When L ≥ c7
c12 > 0,

log n

c12

≤

Ldmax

√bct

(cid:107)w(cid:107)2

A(t) ≤ βA(t−1) + 2wmax(cid:114) log n

+(cid:13)(cid:13)(cid:13)p(t−1) − w(cid:13)(cid:13)(cid:13)2 (cid:107)L2(cid:107)2,∞.
dmax(cid:16) dmax
γdmin(cid:17)2
11(cid:13)(cid:13)p(0) − w(cid:13)(cid:13)2
(cid:13)(cid:13)p(t) − w(cid:13)(cid:13)2
(cid:13)(cid:13)(cid:13)p(t) − w(cid:13)(cid:13)(cid:13)2 ≤ √nwmax(cid:32)√bct
A(t) ≤ wmax(cid:18)c8 + c9

γdmin (cid:107)L2(cid:107)2,∞(cid:19)(cid:114) log n

γdmin(cid:114) dmax log n

γdmin(cid:114) dmax log n

1−β , c9 := c12

√ndmax

(cid:33) .

(cid:107)w(cid:107)2

Ldmax

11 +

c12

+

.

L

L

We let c8 := 2
it, we get

1−β and c10 := max(c11, β) < 1. Putting (45) into (44) and solving

+ 7wmax√btct
10.

(46)

We use (cid:107)w(cid:107)2 ≤ √n(cid:107)w(cid:107)∞ = √nwmax and(cid:13)(cid:13)p(0) − w(cid:13)(cid:13)2 ≤ √n(cid:13)(cid:13)p(0) − w(cid:13)(cid:13)∞ ≤ √nwmax for

uniformly distributed p(0), to get

(44)

(45)

(48)

From the deﬁnition of A(t), we complete the proof of Lemma 8.
Proof of (42): By changing the order of the summations and the Cauchy-Schwarz inequal-
ity, we get

(cid:88)j:j(cid:54)=i (cid:88)k:k(cid:54)=j(cid:12)(cid:12)(cid:12)p

(t)

(t)

ˆPjk ˆPij

k − wk(cid:12)(cid:12)(cid:12) ˆPjk ˆPij =(cid:88)k (cid:12)(cid:12)(cid:12)p

k − wk(cid:12)(cid:12)(cid:12) (cid:88)j:j /∈{i,k}
≤(cid:13)(cid:13)(cid:13)p(t) − w(cid:13)(cid:13)(cid:13)2(cid:118)(cid:117)(cid:117)(cid:117)(cid:116)(cid:88)k (cid:32) (cid:88)j:j /∈{i,k}
I[(j, k) ∈ E]I[(i, j) ∈ E] ≤ (cid:88)j:j /∈{i,k}

1
dk

From the deﬁnitions of ˆPjk ˆPij, we can bound the term(cid:80)j:j /∈{i,k} ˆPjk ˆPij as follows.
(cid:88)j:j /∈{i,k}

I[(k, j) ∈ E]

ˆPjk ˆPij ≤

1
d2

1
dj

I[(j, i) ∈ E]

ˆPjk ˆPij(cid:33)2

.

(47)

max (cid:88)j:j /∈{i,k}
= (cid:88)j:j /∈{i,k}LkjLji = (cid:88)j:j /∈{i,k}

[L2]ki,

where the inequality comes from yjk ≤ 1 and yij ≤ 1. L is the Laplacian matrix whose
entries are deﬁned as Lij := 1

I[(i, j) ∈ E]. Putting (48) into (47), we get

di

(cid:88)j:j(cid:54)=i (cid:88)k:k(cid:54)=j(cid:12)(cid:12)(cid:12)p

(t)

k − wk(cid:12)(cid:12)(cid:12) ˆPjk ˆPij ≤(cid:13)(cid:13)(cid:13)p(t) − w(cid:13)(cid:13)(cid:13)2(cid:115)(cid:88)k

([L2]ki)2

16

Top-K Ranking from Pairwise Comparisons: When Spectral Ranking is Optimal

≤(cid:13)(cid:13)(cid:13)p(t) − w(cid:13)(cid:13)(cid:13)2

max

i 
(cid:115)(cid:88)k

([L2]ki)2

=(cid:13)(cid:13)(cid:13)p(t) − w(cid:13)(cid:13)(cid:13)2 (cid:107)L2(cid:107)2,∞.

(49)

7. Proof of Theorem 2

Theorem 2 establishes a lower bound of S∆K in the minimax scenario. The proof in this
section is modiﬁed from the proof of Theorem 2 in (Chen and Suh, 2015) to make the
arguments therein hold valid in the general deterministic model of our interest. Similarly as
in (Chen and Suh, 2015), we intend to bound the minimax probability of error to characterize
the conditions under which the probability cannot be made arbitrarily close to zero. We
construct a ﬁnite set of hypotheses M and carry out an analysis based on classical Fano-
type arguments. Each hypothesis is represented by a permutation σm ∈ M over [n] and
we denote by σm(i) and σm([K]) the index of the ith ranked item and the index set of all
top-K items respectively.

We choose a set of hypotheses and some prior to be imposed on them. Suppose that

the values of w are ﬁxed up to permutation in such a way that

∀σm ∈ M, wσm(i) =(cid:26) wK

wK+1

if 1 ≤ i ≤ K
if K < i ≤ n,

where we abuse the notation wK, wK+1 to represent any two values satisfying

wK − wK+1

wmax

= ∆K > 0.

(50)

(51)

Additionally, we impose a uniform prior over a collection M of M := max(K, n− K) + 1

hypotheses regarding the permutation: if K < n

2 , then

∀σm ∈ M, P [σm] =

1
M

and if K ≥ n
2 , then
∀σm ∈ M, P [σm] =

1
M

, σm ([K]) = Sm, for Sm = {2, ..., K} ∪ {m}, (m = 1, K + 1, ..., n),
(52)

, σm ([K]) = Sm, for Sm = {1, ..., K + 1}\{m}, (m = 1, ..., K + 1).
(53)

In words, each alternative hypothesis is made by interchanging two indices of the hypothesis
complying to σ([K]) = [K]. Denoting by Pe,M the average probability of error with respect
to the constructed prior, one can verify the minimax probability of error to be at least Pe,M .
Below is where we begin to modify the arguments in (Chen and Suh, 2015) for the
general deterministic model of our interest. The modiﬁcations are mainly about adapting
expressions that depend on the structure of comparison graphs such as the out-degrees of
vertices. Random graphs constructed by the Erd˝os-R´enyi model are concerned in (Chen
and Suh, 2015), whereas we consider deterministic graphs in this paper.

17

Jang, Kim, Suh and Oh

We bound the Bayesian probability of error using classical Fano-type bounds. To take
(L)
ij )

partial observations into account, we introduce an erased version of yij := (y
such that

(1)
ij , ..., y

and set Z := {zij}1≤i<j≤n. We apply the generalized Fano inequality (Han and Verdu,
1994) to get

erasure

zij =(cid:26) yij
log M 

1

1

M 2 (cid:88)σa,σb∈M

Pe,M ≥ 1 −

D(cid:0)PZ|σ=σa||PZ|σ=σb(cid:1) = L (cid:88)(i,j):(i,j)∈E

if (i, j) ∈ E
else,

(54)

,

D(PZ|σ=σa||PZ|σ=σb) + log 2
ij |σ=σb(cid:19) ,
D(cid:18)P

ij |σ=σa||P

((cid:96))
ij ,

y(1)

y(1)

(55)

(56)

where D(P||Q) denotes the Kullback-Leibler (KL) divergence of Q from P . The proof will
be ﬁnished once we show that (55) can be further bounded by some positive constant. To
that end, ﬁrst we get, by (54) and the independence assumption of y

where the derivation follows the same line of arguments in (Chen and Suh, 2015).

We now intend to ﬁnd an upper bound on (56). Note the following diﬀerence between
two hypotheses σa and σb when K < n/2: according to the deﬁnitions given in (50) and
(52), wa = wK and wb = wK+1 for hypothesis σa, and wa = wK+1 and wb = wK for

hypothesis σb. Hence, we can get D(cid:18)P

ij |σ=σa||P

y(1)

ij |σ=σb(cid:19) = 0 if both i and j are neither

y(1)

a nor b.
In other words, as long as we are concerned with pairwise observations that
involve neither a nor b, the distributions based on two hypotheses σa and σb are identical.
We can get the same result when K ≥ n/2 in a similar way. Using the upper bound

∆2

max

min

K derived in Lemma 3 of (Chen and Suh, 2015),

ij |σ=σa||P

y(1)

ij |σ=σb(cid:19) ≤ (da + db)

y(l)

w4
w4

max

min

∆2

K,

(57)

since in summing over all edges, there are da + db − 1 edges in total that result in non-zero
KL divergences. Putting (56) and (57) into (55), we ﬁnally get a lower bound on Pe,M .

D(cid:18)P

ij |σ=σa||P

y(1)

y(1)

w4

ij |σ=σb(cid:19) ≤ w4
D(cid:18)P
(cid:88)(i,j):(i,j)∈E
log M 
log M 
log M (cid:40) L

L
M 2

M 2

1

1

1

Pe,M ≥ 1 −

= 1 −

= 1 −

L

M 2 (cid:88)σa∈M (cid:88)σb∈M

(da + db)

w4
w4

max

∆2

min

w4
w4

max

∆2

min

K (cid:88)σb∈M (cid:88)σa∈M
K(cid:32)2M (cid:88)σa∈M

min

max

∆2

w4
w4

K + log 2
da + (cid:88)σa∈M (cid:88)σb∈M
da(cid:33) + log 2(cid:41)

db + log 2

18

Top-K Ranking from Pairwise Comparisons: When Spectral Ranking is Optimal

1

log M (cid:40) 2L

M

≥ 1 −

max

∆2

w4
w4

min

K(cid:88)i

di + log 2(cid:41) ,

where the last inequality follows by the fact that(cid:80)i di ≥(cid:80)σa∈M da.
One would have Pe ≥ Pe,M ≥  for ﬁxed  ∈ (0, 1
8w4
K|E| + log 2 ≤ (1 − ) log n ⇐⇒ L|E| ≤
nw4

2 ), if
w4
8w4

L∆2

max

min

max

min

n((1 − ) log n − log 2)

∆2
K

since M ≥ n

2 and |E| =

. This completes the proof of Theorem 2.

(cid:80)

i di
2

8. Proof of Theorem 3

The proof of Theorem 3 comes down to showing

(cid:107)w − ˆw(cid:107)∞

(cid:107)w(cid:107)∞

(cid:46)(cid:115) log n

npL

.

Once it is shown, together with Theorem 2, the optimal sample complexity (16) is charac-
terized. Hence, we mainly seek to show (60) in this section.
2 np ≤ di ≤ 3

n , it is
2 in Lemma 7 of (Negahban et al., 2012). We now show that √n(cid:107)L2(cid:107)2,∞

By the Bernstein inequality in Lemma 9, 1

2 np for all i. Given p > log n

shown that γ ≥ 1
is bounded by a constant.

(58)

,

(59)

(60)

(61)

(62)

(63)

2

1
dj

I[(k, j) ∈ E]

I[(j, i) ∈ E]
2
I[(k, j) ∈ E]I[(j, i) ∈ E]

.

and k (cid:54)= i, using the Bernstein

≤

max

1
dk

√n
d2
min

(cid:118)(cid:117)(cid:117)(cid:117)(cid:116)(cid:88)k
i 
 (cid:88)j:j /∈{i,k}
√n(cid:13)(cid:13)L2(cid:13)(cid:13)2,∞ = √n max
(cid:118)(cid:117)(cid:117)(cid:117)(cid:116)(cid:88)k
i 
 (cid:88)j:j /∈{i,k}
(cid:88)j:j /∈{i,k}
I[(i, j) ∈ E]I[(j, i) ∈ E] = (cid:88)j:j(cid:54)=i

Given a comparison graph G ∼ Gn,p where p > c4(cid:113) log n
I[(i, j) ∈ E]I[(j, k) ∈ E] ≤

inequality in Lemma 9, we get

(cid:88)j:j(cid:54)=i

and

n

both with probability at least 1 − 2n
Putting (62) and (63) into (61), we get

− 3

28 c2
4.

np2,

3
2

I[(i, j) ∈ E] ≤

3
2

np,

√n(cid:13)(cid:13)L2(cid:13)(cid:13)2,∞ ≤

√n

2 np(cid:1)2(cid:114)(n − 1)
(cid:0) 1

9
4

n2p4 +

9
4

n2p2 ≤

1
3
1
2 p2
4 n

3
2

n

3

2 p2(cid:114)1 +

1
np2 = 12

(64)

19

Jang, Kim, Suh and Oh

(cid:46)(cid:115) log n

npL

.

(65)

(66)

Putting (64), Lemma 9, and Lemma 7 of (Negahban et al., 2012) into equation (21), we get

n .

for large n where p > c4(cid:113) log n
(cid:46)(cid:18)1 +

(cid:107)w − ˆw(cid:107)∞

(cid:107)w(cid:107)∞

√ndmax

γdmin (cid:107)L2(cid:107)2,∞(cid:19)(cid:114) log n

Ldmax ≤ 73(cid:115) log n

L( 1

2 np)

Therefore, for reliable ranking, we need the condition as follows,

n2pL
2 ≥ c6

n log n

∆2
K

.

for some constant c6. Similarly, using Lemma 9, and Lemma 7 of (Negahban et al., 2012),
condition of L becomes as follows, which completes the proof of Theorem 3.

L ≥(cid:38)c1

log n

dmax(cid:18) dmax

γdmin(cid:19)2(cid:39) ≥(cid:24)24c1

log n

np (cid:25) =(cid:24)c5

log n

np (cid:25) .

(67)

Appendix A. Concentration of Degrees of Items (Lemma 9)

Although it is clear that the empirical mean of a random variable converges to its true mean
with high probability, we prove the following for rigorous proofs in this paper.

Lemma 9 Suppose independent and identically distributed (i.i.d.) random variables Xi
follow Bernoulli(q) and q > c log n

n . Then, with probability at least 1 − 2n

28 c,

− 3

1
2

nq ≤

n(cid:88)i=1

Xi ≤

3
2

nq.

Proof Applying the Bernstein inequality, we get

> t(cid:35) ≤ 2 exp(cid:32)−

1
2 t2
nq + 1

3 t(cid:33) .

(68)

(69)

n(cid:88)i=1

P(cid:34)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
P(cid:34)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
n(cid:88)i=1

Xi − nq(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
Xi − nq(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

Then we choose t = 1
completes the proof.

2 nq and use q > c log n

n , to get the following tail probability, which

>

nq(cid:35) ≤ 2n

1
2

− 3

28

nq
log n < 2n

− 3

28 c.

(70)

20

Top-K Ranking from Pairwise Comparisons: When Spectral Ranking is Optimal

References

A. Agresti. Categorical data analysis. John Wiley & Sons, 2014.

N. Ailon. Active learning ranking from pairwise preferences with almost optimal query

complexity. Journal of Machine Learning, 13:137–164, 2012.

A. Ammar and D. Shah. Ranking: Compare, don’t score. In Allerton Conference, pages

776–783. IEEE, 2011.

H. Azari Souﬁani, W. Chen, D. C. Parkes, and L. Xia. Generalized method-of-moments for

rank aggregation. In Neural Information Processing Systems, pages 2706–2714, 2013.

H. Azari Souﬁani, D. C. Parkes, and L. Xia. A statistical decision-theoretic framework for

social choice. In Neural Information Processing Systems, pages 3185–3193, 2014.

L. Baltrunas, T. Makcinskas, and F. Ricci. Group recommendations with rank aggregation
and collaborative ﬁltering. In ACM Conference on Recommender Systems, pages 119–126.
ACM, 2010.

J. C. Borda. M´emoire sur les ´elections au scrutin. 1781.

R. A. Bradley and M. E. Terry. Rank analysis of incomplete block designs: I. the method

of paired comparisons. Biometrika, 39(3-4):324–345, 1952.

M. Braverman and E. Mossel. Noisy sorting without resampling. In ACM-SIAM symposium

on Discrete algorithms, pages 268–276, 2008.

S. Brin and L. Page. The anatomy of a large-scale hypertextual web search engine. Computer

Networks and ISDN systems, 30(1):107–117, 1998.

R. Busa-Fekete, E. H¨ullermeier, and B. Sz¨or´enyi. Preference-based rank elicitation using
statistical models: The case of mallows. In International Conference on Machine Learn-
ing, pages 1071–1079, 2014.

A. Caplin and B. Nalebuﬀ. Aggregation and social choice: a mean voter theorem. Econo-

metrica, pages 1–23, 1991.

X. Chen, P. N. Bennett, K. Collins-Thompson, and E. Horvitz. Pairwise ranking aggregation
in a crowdsourced setting. In ACM Conference on Web Search and Data Mining, pages
193–202. ACM, 2013.

Y. Chen and C. Suh. Spectral MLE: Top-K rank aggregation from pairwise comparisons.

In International Conference on Machine Learning, pages 371–380, 2015.

C. Dwork, R. Kumar, M. Naor, and D. Sivakumar. Rank aggregation methods for the web.

In International conference on World Wide Web, pages 613–622. ACM, 2001.

B. Eriksson. Learning to top-K search using pairwise comparisons. In International Con-

ference on Artiﬁcial Intelligence and Statistics, pages 265–273, 2013.

21

Jang, Kim, Suh and Oh

L. R. Ford. Solution of a ranking problem from binary comparisons. American Mathematical

Monthly, pages 28–33, 1957.

B. Hajek, S. Oh, and J. Xu. Minimax-optimal inference from partial rankings. In Neural

Information Processing Systems, pages 1475–1483, 2014.

T. Han and S. Verdu. Generalizing the fano inequality. IEEE Transactions on Information

Theory, 40:1247–1251, 1994.

D. R. Hunter. MM algorithms for generalized Bradley- Terry models. Annals of Statistics,

pages 384–406, 2004.

K. G. Jamieson and R. Nowak. Active ranking using pairwise comparisons.

In Neural

Information Processing Systems, pages 2240–2248, 2011.

T. Lu and C. Boutilier. Learning Mallows models with pairwise preferences. In International

Conference on Machine Learning, pages 145–152, 2011.

R. D. Luce. Individual choice behavior: A theoretical analysis. Wiley, 1959.

C. L. Mallows. Non-null ranking models. I. Biometrika, pages 114–130, 1957.

L. Maystre and M. Grossglauser. Fast and accurate inference of Plackett-Luce models. In

Neural Information Processing Systems, pages 172–180. 2015.

L. Meirovitch. Principles and techniques of vibrations, volume 1. Prentice Hall, 1997.

S. Negahban, S. Oh, and D. Shah. Rank centrality: Ranking from pair-wise comparisons.

2012. URL http://arxiv.org/abs/1209.1688.

R. L. Plackett and R. D. Luce. The analysis of permutations. Applied Statistics, pages

193–202, 1975.

A. Rajkumar and S. Agarwal. A statistical convergence perspective of algorithms for rank
aggregation from pairwise data. In International Conference on Machine Learning, pages
118–126, 2014.

J. R. Seeley. The net of reciprocal inﬂuence. Canadian Journal of Psychology, 3(4):234–240,

1949.

N. B. Shah and M. J. Wainwright. Simple, robust and optimal ranking from pairwise

comparisons. 2015. URL http://arxiv.org/abs/1512.08949.

N. B. Shah, S. Balakrishnan, A. Guntuboyina, and M. J. Wainright. Stochastically transitive
models for pairwise comparisons: Statistical and computational issues. 2015. URL http:
//arxiv.org/abs/1510.05610.

S. Vigna. Spectral ranking. 2009. URL http://arxiv.org/abs/0912.0238.

F. Wauthier, M. Jordan, and N. Jojic. Eﬃcient ranking from pairwise comparisons.

In

International Conference on Machine Learning, pages 109–117, 2013.

22

Top-K Ranking from Pairwise Comparisons: When Spectral Ranking is Optimal

T. H. Wei. The algebraic foundations of ranking theory. PhD thesis, University of Cam-

bridge, 1952.

23

