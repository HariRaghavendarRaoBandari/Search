6
1
0
2

 
r
a

 

M
4
1

 
 
]

V
C
.
s
c
[
 
 

1
v
5
6
2
4
0

.

3
0
6
1
:
v
i
X
r
a

Dynamic Scene Deblurring using

a Locally Adaptive Linear Blur Model

Tae Hyun Kim, Seungjun Nah, and Kyoung Mu Lee

1

Abstract—State-of-the-art video deblurring methods cannot handle blurry videos recorded in dynamic scenes, since they are built
under a strong assumption that the captured scenes are static. Contrary to the existing methods, we propose a video deblurring
algorithm that can deal with general blurs inherent in dynamic scenes. To handle general and locally varying blurs caused by various
sources, such as moving objects, camera shake, depth variation, and defocus, we estimate pixel-wise non-uniform blur kernels. We
infer bidirectional optical ﬂows to handle motion blurs, and also estimate Gaussian blur maps to remove optical blur from defocus in our
new blur model. Therefore, we propose a single energy model that jointly estimates optical ﬂows, defocus blur maps and latent frames.
We also provide a framework and efﬁcient solvers to minimize the proposed energy model. By optimizing the energy model, we
achieve signiﬁcant improvements in removing general blurs, estimating optical ﬂows, and extending depth-of-ﬁeld in blurry frames.
Moreover, in this work, to evaluate the performance of non-uniform deblurring methods objectively, we have constructed a new realistic
dataset with ground truths. In addition, extensive experimental on publicly available challenging video data demonstrate that the
proposed method produces qualitatively superior performance than the state-of-the-art methods which often fail in either deblurring or
optical ﬂow estimation.

Index Terms—video deblurring, non-uniform blur, motion blur, defocus blur, optical ﬂow

!

1 INTRODUCTION

M OTION blurs are the most common artifacts in videos

recorded from hand-held cameras. In low-light conditions,
these blurs are caused by camera shake and object motions during
exposure time. In addition, fast moving objects in the scene cause
blurring artifacts in a video even when the light conditions are
acceptable. For decades, this problem has motivated considerable
works on deblurring and different approaches have been sought
depending on whether the captured scenes are static or dynamic.
Early works on a single image deblurring problem are based
on assumptions that the captured scene is static and has constant
depth [1], [2], [3], [4], [5], [6] and they estimated uniform or
non-uniform blur kernel by camera shake. These approaches were
naturally extended to video deblurring methods. Cai et al. [7]
proposed a deconvolution method with multiple frames using
sparsity of both blur kernels and clear images to reduce errors
from inaccurate registration and render high-quality latent image.
However, this approach removes only uniform blur caused by
two-dimensional translational camera motion, and the proposed
approach cannot handle non-uniform blur from rotational camera
motion around z-axis, which is the main cause of motion blurs [6].
To solve this problem, Li et al. [8] adopted a method parameter-
izing spatially varying motions with 3x3 homographies based on
the previous work of Tai et al. [9], and could handle non-uniform
blurs by rotational camera shake. In the work of Cho et al. [10],
camera motion in three-dimensional space was estimated without
any assistance of specialized hardware, and spatially varying blurs
caused by projective camera motion were obtained. Moreover, in
the works of Paramanand et al. [11] and Lee and Lee [12], spatially

•

The authors are with the Department of Electrical Engineering and Com-
puter Science, Automation and Systems Research Institute, Seoul National
University, 1 Ganak-ro, Gwanak-gu, Seoul 151-744, South Korea (E-mail:
lliger9@snu.ac.kr, seungjun.nah@gmail.com, kyoungmu@snu.ac.kr).

varying blurs by depth variation in a static scene were estimated
and removed.

However, these previous methods, which assume static scene,
suffer from spatially varying blurs from not only camera shake
but also moving objects in a dynamic scene. Because it is difﬁcult
to parameterize the pixel-wise varying blur kernel in the dynamic
scene with simple homography, kernel estimation becomes more
challenging task. Therefore, several researchers have studied on
removing blurs in dynamic scenes, which are grouped into two ap-
proaches: segmentation-based deblurring approach, and exemplar-
based deblurring approach.

Segmentation-based approaches usually estimate multiple mo-
tions, kernels, and associated segments. In the work of Cho
et al. [13], a method that segments homogeneous motions and
estimates segment-wise different 1D Gaussian blur kernels, was
proposed. However, it cannot handle complex motions by rota-
tional camera shakes due to the limitation of Gaussian kernels.
In the work of Bar et al. [14], a layered model was proposed
that segments images into foreground and background layers, and
estimates a linear blur kernel within the foreground layer. By using
the layered model, explicit occlusion handling is possible, but
the kernel is restricted to linear. To overcome these limitations,
Wulff and Black [15] improved the previous layered model of
Bar et al. by estimating the different motions of both foreground
and background layers. However, these motions are restricted to
afﬁne models and it is difﬁcult to extend to multi-layered scenes
because such task requires depth ordering of the layers. To sum
up, segmentation-based deblurring approaches have the advantage
of removing blurs caused by moving objects in dynamic scenes.
However, segmentation itself is very difﬁcult problem and remains
still an challenging issue as reported in [16]. Moreover, they fail to
segment complex motions like motions of people, because simple
parametric motion models used in [14], [15] cannot ﬁt the complex
motions accurately.

2

Fig. 1: (a) A Blurry frame in a dynamic scene. (b) Our deblurring result. (c) Our color coded optical ﬂow estimation result.

Exemplar-based approaches were proposed in the works of
Matsushita et al. [17] and Cho et al. [18]. These methods usually
do not rely on accurate segmentation and deconvolution. Instead,
the latent frames are rendered by interpolating lucky sharp frames
that frequently exist in videos, thus avoiding severe ringing arti-
facts. However, the work of Matsushita et al. [17] cannot remove
blurs caused by moving objects. In addition the work of Cho
et al. [18] allows only slow-moving objects in dynamic scenes
because it searches sharp patches corresponding to blurry patch
after registration with homography. Therefore, it cannot handle
fast moving objects which have distinct motions from those of
backgrounds. Moreover, since it does not use deconvolution with
spatial priors but simple interpolation, it degrades mid-frequency
textures such as grasses and trees, and renders smooth results.

On the other hand, defocus from limited depth-of-ﬁeld (DOF)
of conventional digital cameras also results in blurry effects in
videos. Although shallow DOF is often used to render aesthetic
images and highlight the focused objects, frequent misfocus of
moving objects in video yields image degradation when the
motion is large and fast. Moreover, depth variation in the scene
generates spatially varying defocus blurs, making the estimation of
defocus blur map is also a difﬁcult problem. Thus many researches
have studied to estimate defocus blur kernel. Most of them have
approximated the kernel as simple Gaussian or disc model, making
the kernel estimation problem becomes a parameter (e.g. standard
deviation of Gaussian blur, disc radius) estimation problem [19],
[20], [21], [22].

To magnify focus differences, Bae and Durand [19] estimated
defocus blur map at the edges ﬁrst, and then propagated the
results to other regions. However,
the estimated blur map is
inaccurate where the blurs are strong, since it is image-based
approach and depends on the detected edges that can be localized.
Similarly, Zhuo and Sim [22] propagated the amount of blur
at the edges to elsewhere, that obtained by measuring the ratio
between the gradients of the defocused input and re-blurred input
with a Gaussian kernel. To reduce reliance on strong edges in
the defocused image, Zhu et al. [21] utilized statistics of blur
spectrum within the defocused image, since statistical models
could be applicable where there are no strong edges. Speciﬁcally,
local image statistics is used to measure the probability of defocus
scale and determine the locally varying scale of defocus blur in
a single image. However, local image statistics-based methods do
not work when there are motion blurs as well as defocus blurs

within a single image; Motion blurs change local statistics and
yield much complex blurs combined with defocus blurs.

In the recent work of Kim and Lee [23], a new and generalized
video deblurring (GVD) method that estimates latent frames
without using global motion parametrization and segmentation
was proposed to remove motion blurs in dynamic scenes. In GVD,
bidirectional optical ﬂows are estimated and used to infer pixel-
wise varying kernels. Therefore, the proposed method naturally
handle coexisting blurs by camera shake, and moving objects with
complex motions. Because estimating ﬂow ﬁelds and restoring
sharp frames are a joint problem, both variables are simultaneously
estimated in GVD. To do so, a new single energy model to solve
the joint problem was proposed and efﬁcient solvers to optimize
the model is provided.

However, since GVD method is based on piece-wise linear
kernel approximation, it cannot handle non-linear blurs combined
with motion and defocus blurs which are common in videos cap-
tured from hand-held cameras. Therefore, in this work, we propose
an extended and more generalized method of GVD that can handle
not only motion blur but also defocus blur which further improves
the deblurring quality signiﬁcantly. Under an assumption that, the
complex non-linear blur kernel can be decomposed into motion
and defocus blur kernels, we estimate bidirectional optical ﬂows
to approximate motion blur kernel, scales of Gaussian blurs to
approximate defocus blur kernel, and the latent frames jointly. The
result of our system is shown in Fig.1, in which the motion blurs of
differently moving people and Gaussian blurs in the background
are successfully removed and accurate optical ﬂows are jointly
estimated.

Finally, we provide a new realistic blur dataset with ground
truth sharp frames captured by a high-speed camera to overcome
the lack of realistic ground truth dataset in this ﬁeld. Though
there have been some evaluation datasets for deblurring problem,
they are not appropriate to carry out meaningful evaluation for the
deblurring of spatially varying blurs. First, synthetically generated
uniform blur kernels and blurry images from sharp images were
provided in the work of Levin et al. [24]. Next, 6D camera motion
in 3D space was recorded with a hardware-assisted camera to
represent blur from camera shake during exposure time in the
work of K¨ohler et al. [25]. Moreover, there have been some recent
approaches to generate synthetic dataset for the sake of machine
learning algorithms. To beneﬁt from large training data, lots of
blur kernels and blurry images were synthetically generated. In the

(a)(b)(c)KIM et al.: DYNAMIC SCENE DEBLURRING USING A LOCALLY ADAPTIVE LINEAR BLUR MODEL

3

Fig. 2: (a) Blurry frame from a dynamic scene. (b) Deblurring result by Cho et al. [18]. (c) Our result.

work of Xu et al. [26], more than 2500 blurry images are generated
using decomposable symmetric kernels. Schuler et al. [27] sam-
pled naturally looking blur kernels with Gaussian Process, and Sun
et al. [28] used a set of linear kernels to synthesize blurry images.
However, these datasets are generated under an assumption that
the scene is static and cannot synthesize inﬁnitely many blurs in
real world. Real blurs in dynamic scenes are complex and spatially
varying, so synthesizing realistic dataset is a difﬁcult problem. To
solve this problem, we construct a new blur dataset that provides
pairs of realistically blurred videos and sharp videos with the use
of a high-speed camera.

Using the proposed dataset and real challenging videos as
shown in Fig.2, we demonstrate the signiﬁcant improvements
of the proposed deblurring method in both quantitatively and
qualitatively. Moreover, we show empirically that more accurate
optical ﬂows are estimated by our method compared with the state-
of-the-art optical ﬂow method that can handle blurry images.

2 MORE GENERALIZED VIDEO DEBLURRING
Most conventional video deblurring methods suffer from the coex-
istence of various motion blurs from dynamic scenes because the
motions cannot be fully parameterized using global or segment-
wise blur models. To make things worse, frequent misfocus of
moving objects in dynamic scenes yields more complex non-linear
blurs combined with motion blurs.

To handle these joint motion and defocus blurs, we propose a
new blur model that estimates locally (pixel-wise) different blur
kernels rather than global or segment-wise kernel estimation. As
blind deblurring problem is highly ill-posed, we propose a single
energy model consists of not only data and spatial regularization
terms but also a temporal term. The model is expressed as follows:

E = Edata + Etemporal + Espatial,

(1)

and the detailed models of each term in (1) are given in the
following sections.

2.1 Data Model based on Kernel Approximation
Motion blurs are generally caused by camera shake and moving
objects, and defocus blurs are mainly due to the aperture size,
focal length, and the distance between camera and focused object.
These two different blurs are combined and yield more complex
blurs in real video. For example, Fig. 3 shows how different the
blurred images are when point light sources are captured by the
same moving camera with and without defocused blur. We observe

Fig. 3: (a) Two light sources. (b) Light streak of the focused light
source. (c) Light streak of the defocused light source.

that the light streak of the defocused light source is much smoother
and non-linear in comparison with the focused one. Notably, the
light streaks indicate the blur kernel shapes.

However, it is difﬁcult to directly remove the complex blur
in Fig. 3 (c). Thus, to alleviate the problem, we assume that
the combined blur kernel can be decomposed into two different
kernels, which are motion blur kernel and defocus blur kernel.
Our assumption holds when the depth change in the scene during
exposure period is relatively small, and it is acceptable since
we treat video of rather short exposure time. So, the underly-
ing blurring-procedure can be modeled as sequential process of
defocus blurring followed by motion blur as illustrated in Fig. 4.
Note that in conventional video deblurring works [8], [14],
[15], [18], the motion blurs of each frame are usually approx-
imated by parametric models such as homography and afﬁne
models. However, these kernel approximations are only valid
when the motions are parameterizable within an entire frame or
a segment, and cannot cope with spatially varying motion blurs.
To solve this problem, we approximate the pixel-wise motion blur
kernel using bidirectional optical ﬂows as suggested in previous
works [16], [29], [30].

Spatially varying defocus blur is approximated by using Gaus-
sian or disc model in conventional works [20], [21]. Therefore, the
defocus maps and scales of local blurs are determined by simply
estimating the standard deviations of Gaussian models or the radii
of disc models. In particular, local image statistics is widely used

Fig. 4: Blurring process underlying in the proposed method.

(a)(b)(c)(a) (b) (c) Sharp frameDefocus blurMotion blurBlurry frame4

Fig. 5: (a) A sharp patch. (b) A patch blurred by defocus blur only
(Gaussian blur with standard deviation 5). (c) A patch blurred by
defocus blur (Gaussian blur with standard deviation 5) and motion
blur (linear kernel with length 11). (d) Comparisons of ﬁdelities at
the center of the blurry patches by changing the scale of defocus
blur. The ground truth scale of the defocus blur is 5 and the arrows
indicate peaks estimated by ML estimator.

to estimate spatially varying defocus blur. Speciﬁcally, within
a uniformly blurred patch, local frequency spectrum provides
information on the blur kernel and can be used to determine
the likelihood of speciﬁc blur kernel [21]; thus scales of defocus
blurs can be estimated by measuring the ﬁdelities of the likeli-
hood model. However, it is difﬁcult to apply this statistics-based
technique when the blurry image has motion blurs in addition to
defocus blurs. In Fig. 5, we observe that the maximum likelihood
(ML) estimator used in [21] ﬁnds the optimal defocus blur kernel
when a patch is blurred by only defocus blur, however ML cannot
estimate true defocus kernel when a blurry patch contains motion
blur as well as defocus blur. Therefore we cannot adopt local
image statistics to remove defocus blurs in dynamic scenes with
motion blurs. In this study, we approximate the pixel-wise varying
defocus blur using Gaussian model, and determine the standard
deviation of defocus blur by jointly estimating the defocus blur
maps and latent frames unlike conventional works that utilize only
local image statistics.

Therefore, under two assumptions that the latent frames are
blurred by defocus blurs, and subsequently blurred by motion, and
the velocity of the motion is constant between adjacent frames, our
blur model is expressed as follows:

Bi(x) = (ki,x ⊗ gi,x ⊗ Li)(x),

(2)

where Bi and Li denote the blurry frame and the latent frame at
the ith frame, respectively, and x denotes pixel location on 2D
image domain. At x, the motion blur kernel is denoted as ki,x and
the Gaussian blur kernel by defocus is denoted as gi,x, and the
operator ⊗ means convolution.

To handle locally varying motion blurs, we should reduce the
size of solution space using approximation and use parametrized

Fig. 6: (a) Bidirectional optical ﬂows and corresponding piece-
wise linear motion blur kernel at pixel location x. (b) Gaussian
blur kernel with standard deviation σ at x to handle blur from
defocus.

kernel, because the solution space of locally varying kernel in
video is extremely large; the dimension of kernel is W × H ×
T × w× h when the size of image is W × H, length of the image
sequence is T , and the size of local kernel is w × h. Therefore,
we approximate the motion blur kernel as piece-wise linear using
bidirectional optical ﬂows as illustrated in Fig. 6 (a). Although
our motion blur kernel is based on simple approximation, our
model is valid since we assume that the videos have relatively
short exposure time. The pixel-wise kernel ki,x using bidirectional
ﬂow can be written by,



0,

ki,x(ui→i+1, ui→i−1) =
,

δ(uvi→i+1−vui→i+1)
δ(uvi→i−1−vui→i−1)

2τi(cid:107)ui→i+1(cid:107)
2τi(cid:107)ui→i−1(cid:107)

,

if u ∈ [0, τiui→i+1], v ∈ [0, τivi→i+1]
if u ∈ (0, τiui→i−1], v ∈ (0, τivi→i−1]
otherwise

,

(3)

where ui→i+1 = (ui→i+1, vi→i+1),
and ui→i−1 =
(ui→i−1, vi→i−1) denote pixel-wise bidirectional optical ﬂows at

-30-25-20-15-10-5012345678910Log likelihoodScales of defocus blurDefocus blur onlyDefocus + Motion Blur(a)(b)(c)(d)𝐋𝑖−1𝐋𝒊𝐋𝑖+1𝐮𝑖→𝑖+1𝜏𝑖=exposuretime2∗(timeinterval)𝐱u𝑘𝑖,𝑥(𝑢,𝑣)v𝜏𝑖(𝑢𝑖→𝑖+1,𝑣𝑖→𝑖+1)𝜏𝑖(𝑢𝑖→𝑖−1,𝑣𝑖→𝑖−1)𝑡𝑖−1𝑡𝑖𝑡𝑖+1time intervalexposure time(a)(b)𝑔𝑖,𝒙(𝝈𝑖)xy1𝜎𝑖2𝜋KIM et al.: DYNAMIC SCENE DEBLURRING USING A LOCALLY ADAPTIVE LINEAR BLUR MODEL

5

Fig. 7: (a) Blurry frame of a video in dynamic scene. (b) Locally
varying kernel using homography. (c) Our pixel-wise varying
motion blur kernel using bidirectional optical ﬂows.

frame i. Camera duty cycle of the frame is τi and denotes relative
exposure time as used in [8], and δ denotes Kronecker delta.

Using this pixel-wise motion blur kernel approximation, we
can easily manage multiple different motion blurs in a frame, un-
like conventional methods. The superiority of our locally varying
kernel model is shown in Fig. 7. Our kernel model ﬁts blurs from
differently moving objects and camera shake much better than the
conventional homography-based model.

Moreover, we approximate the spatially varying defocus blur
kernel using Gaussian model as shown in Fig. 6 (b), and estimate
the pixel-wise different standard deviation σi of the Gaussian
kernel gi,x. Although we cannot utilize the features in the blurred
frame, which has been used signiﬁcantly in conventional meth-
ods [21], [31], due to combined motion blurs, we determine the
scales of defocus blurs with the simultaneous estimation of latent
frames and achieve signiﬁcant improvements in comparison with
the state-of-the-art defocus blur map estimator [31] when there
exist both motion and defocus blurs in a real blurry frame as
shown in Fig. 8. Moreover, even when the motion blurs are not
existing, we achieve competitive result as shown in Fig. 9.

Now, the proposed data model that handles both motion and

defocus blurs is expressed as follows:
Edata(L, u, σ, B) =
λ

(cid:88)

(cid:88)

(cid:107)∂∗Ki(τi, ui→i+1, ui→i−1)Gi(σi)Li − ∂∗Bi(cid:107)2, (4)

Fig. 8: (a) Real blurry frame. (b) Our jointly estimated latent
frame. (c) Blur map of Shi et al. [31]. (d) Our defocus blur map.

Fig. 9: (a) Partially blurred image which has sharp foreground and
blurred background by Gaussian blur. (b) Ground truth blur map.
(c) Defocus blur map of Shi et al. [31]. (d) Our defocus blur map.

i

∂∗

where the row vector of the motion blur kernel matrix Ki, which
corresponds to the motion blur kernel at pixel x, is the vector
form of ki,x(.), and its elements are non-negative and their sum is
equal to one. Similarly, the row vector of the defocus blur kernel
matrix Gi, which corresponds to the Gaussian kernel at x, is the
vector form of gi,x(.) and σi denotes the scales (standard deviation
of Gaussian kernel) of defocus blurs. Linear operator ∂∗ denotes
the Toeplitz matrices corresponding to the partial (e.g., horizontal
and vertical) derivative ﬁlters. Parameter λ controls the weight

of the data term, and L, u, σ, and B denote the set of latent
frames, optical ﬂows, scales of defocus blurs and blurry frames,
respectively.

2.2 A new Optical Flow Constraint and Temporal Regu-
larization
As discussed above, to remove locally varying motion blurs, we
employ bidirectional optical ﬂow model in (4). However, for
optical ﬂow estimation, conventional optical ﬂow constraints such

(c)(a)(b)(a)(b)(c)(d)03scale(a)(b)(c)(d)6

Fig. 10: (a) Real blurry frame. (b) Our jointly estimated latent
frame. (c) Color coded optical ﬂow from [30]. (d) Our optical
ﬂow.

as brightness constancy and gradient constancy can not be utilized
directly, since such constraints do not hold between two blurry
frames. A blur-aware optical ﬂow estimation method from blurry
images has been proposed by Portz et al. [30]. This method is
based on the commutative law of shift-invariant kernels such
that the brightness of the corresponding points is constant after
convolving the blur kernel of each image with the other image.
However, the commutative law does not hold when the motion
is not translational and when the blur kernels vary spatially.
Therefore, this approach only works when the motion is very
smooth.

To address this problem, we propose a new model that es-
timates optical ﬂow between two latent sharp frames to enable
abrupt changes in motions and the blur kernels. In using this
model, we need not restrict our motion blur kernels to be shift
invariant. Our model is based on the conventional optical ﬂow
constraint between latent frames, that is, brightness constancy. The
formulation of this model is given by,

important to adopt spatial regularizers. In doing so, we enforce
spatial coherence to penalize spatial ﬂuctuations while allowing
discontinuities in latent frames, ﬂow ﬁelds, and defocus blur maps.
With the assumption that spatial priors for the latent frames,
optical ﬂows, and defocus blur maps are independent, we can
formulate the spatial regularization as follows:

(cid:88)

(cid:88)

(cid:88)
|∇Li| + νσ
(cid:88)
(cid:88)
N(cid:88)

i

νu

i

x

n=−N

gi(x)|∇σi|+

i

x

gi(x)|∇ui→i+n|),

(6)

Espatial(L, u) =

wherer parameters νσ and νu control the weights of the second
and third terms.

The ﬁrst term in (6) denotes the spatial regularization term for
the latent frames. Although more sparse Lp norms (e.g. p = 0.8)
ﬁt the gradient statistics of natural sharp images better [32], [33],
[34], we use conventional total variation (TV) based regulariza-
tion [16], [35], [36], as TV is computationally less expensive
and easy to minimize. The second and third terms enforce spatial
smoothness for defocus blur maps and optical ﬂows, respectively.
These regularizers are also based on TV regularization, and
coupled with edge-map to preserve discontinuities at the edges
in both vector ﬁelds. Similar to the edge-map used in conventional
optical ﬂow estimation method [37], our edge-map is expressed as
follows:

gi(x) = exp(−(

|∇Li,0|2

vI

)),

(7)

where the ﬁxed parameter vI controls the weight of the edge-map,
and Li,0 is an initial latent image in the iterative optimization
framework.

3 OPTIMIZATION FRAMEWORK
Under the condition that the camera duty cycle τi is known, by
combining Edata, Etemporal, and Espatial, we can have the ﬁnal
objective function as follows: and the ﬁnal objective function when
camera duty cycle τi is known becomes as follows:

(cid:88)
(cid:88)
(cid:107)∂∗Ki(ui→i+1, ui→i−1)Gi(σi)Li − ∂∗Bi(cid:107)2+
(cid:88)
(cid:88)
N(cid:88)
(cid:88)
n=−N
|∇Li| + νσ
N(cid:88)
(cid:88)
(cid:88)

|Li(x) − Li+n(x + ui→i+n)|+
(cid:88)
(cid:88)

gi(x)|∇σi|+

∂∗

µ

x

x

i

i

i

i

gi(x)|∇ui→i+n|).

νu

i

x

n=−N

(8)
Note that contrast with Cho et al. [18] that performs multiple
approaches sequentially, our model ﬁnds a solution by minimizing
the proposed single objective function in (8). However, because
of its non-convexity, our model needs to adopt a practical opti-
mization method to obtain an approximated solution. Therefore,
we divide the original problem into several simple sub-problems
and then use conventional iterative and alternating optimization
techniques [1], [15], [16] to minimize the original non-convex
objective function. In the following sections, we introduce efﬁcient
solvers and describe how to estimate unknowns L, u and σ
alternatively.

Etemporal(L, u) =

(cid:88)

(cid:88)

N(cid:88)

i

x

n=−N

µ

|Li(x) − Li+n(x + ui→i+n)|,

(5)

min
L,u,σ

λ

where n denotes the index of neighboring frames at i, and the
parameter µ controls the weight. We apply the robust L1 norm for
robustness against outliers and occlusions.

Notably, a major difference between the proposed model and
the conventional optical ﬂow estimation methods is that our
problem is a joint problem. That is, the latent frames and optical
ﬂows should be solved simultaneously. Therefore, the proposed
model in (5) estimates the latent frames which are temporally
coherent among neighboring frames, and optical ﬂows between
neighboring frames, simultaneously. Therefore we can estimate
accurate ﬂows at the motion boundaries as shown in Fig. 10.
Notice that, our ﬂows at the motion boundaries of moving car
is much clearer in comparison with the blur-aware ﬂow estimation
method by [30].

2.3 Spatial Regularization
To alleviate the difﬁculties of highly ill-posed deblurring, optical
ﬂow estimation, and defocus blur map estimation problems, it is

(a)(b)(c)(d)KIM et al.: DYNAMIC SCENE DEBLURRING USING A LOCALLY ADAPTIVE LINEAR BLUR MODEL

7

3.1 Sharp Video Restoration
If the motion blur kernels K and the defocus blur kernels G are
ﬁxed, then the objective function in (8) becomes convex with
respect to L, and it can be reformulated as follows:

(cid:88)
(cid:88)
(cid:107)∂∗KiGiLi − ∂∗Bi(cid:107)2 +
(cid:88)
(cid:88)
N(cid:88)

∂∗

i

(cid:88)

i

µ

i

x

n=−N

|Li(x) − Li+n(x + ui→i+n)|.

|∇Li|+

min

L

λ

(9)

Fig. 11: Temporally consistent optical ﬂows over three frames.

To restore the latent frames L, we adopt the conventional convex
optimization method proposed in [38], and derive the primal-dual
update scheme as follows:

Now, we can apply the convex optimization technique in [38]
to the approximated convex function, and the primal-dual update
process is expressed as follows:

pm+1 =

pm+ηu(νuWuAu)um

max(1, abs(pm+ηu(νuWuAu)um))

um+1 = (um − u(νuWuAu)T pm+1) − u∇ρu(u0),

(14)
where p denotes the dual variable of u on the vector space. Weight-
ing matrix Wu is diagonal, and its sub-matrix associated with
ui→i+n is deﬁned as diag(gi(x)). Linear operator Au calculates
the spatial difference between four nearest neighboring pixels, and
parameters ηu and u denote the update steps.

3.3 Defocus Blur Map Estimation
When the latent frames L and the motion blur kernels K are ﬁxed,
we can estimate the defocus blur maps from (8). However, the
data term is non-convex, and thus an approximation technique is
required to optimize the objective function. Similar to our optical
ﬂows estimation technique, we approximate and convexify the
original function using linearization.

First, we deﬁne a non-convex data function ρσ(.), and approx-

imate it near an initial values σ0 as follows:

ρσ(σ) = λ

(cid:107)∂∗KiGi(σi)Li − ∂∗Bi(cid:107)2

(cid:88)

(cid:88)

i

∂∗

≈ ρσ(σ0) + ∇ρσ(σ0)T (σ − σ0),

and the approximated convex function for defocus blur map
estimation is given by,

min

σ

ρσ(σ0)+∇ρσ(σ0)T (σ − σ0)+
gi(x)|∇σi|.

νσ

(cid:88)

(cid:88)

i

x

(15)

(16)

Similarly, (16) can be optimized using [38], and the primal-

dual update formulation is given by,

rm+ησ(νσWσA)σm

max(1, abs(rm+ησ(νσWσA)σm))

rm+1 =

σm+1 = (σm − σ(νσWσA)T rm+1) − σ∇ρσ(σ0),

(17)
where r denotes the dual variable of σ on the vector ﬁeld.
Weighting matrix Wσ is diagonal, and its sub-matrix associated
with σi is deﬁned as diag(gi(x)). Parameters ησ and σ denote
the update steps.

4 IMPLEMENTATION DETAILS
To handle large blurs and guide fast convergence, we imple-
ment our algorithm on the conventional coarse-to-ﬁne framework



sm+1 =

sm+ηLALm

max(1, abs(sm+ηLALm))

qm+1 =

qm+ηLµDLm

max(1, abs(qm+ηLµDLm))

(cid:88)

(cid:88)

i

∂∗

2L

Lm+1 = arg min
Lm+1

λ

(cid:107)∂∗KiGiLm+1

i

− ∂∗Bi(cid:107)2+

(cid:107)Lm+1 − (Lm − L(AT sm+1 + µDT qm+1))(cid:107)2

,

(10)
where m ≥ 0 indicates the iteration number, and, sm and qm
denote the dual variables of the concatenated latent frames Lm.
Parameters ηL and L denote the update steps. Linear operator A
calculates the spatial difference between neighboring pixels, and
operator D calculates the temporal differences among neighboring
frames using ﬁxed optical ﬂows. The last formulation in (10) is to
update and optimize the primal variable Lm+1, and we apply the
conjugate gradient method to minimize it since it is a quadratic
function.

3.2 Optical Flows Estimation
Note that , although the latent frames L and the defocus blur
kernels G are ﬁxed, the temporal coherence term Etemporal and
the data term Edata are still non-convex. So, let us denote those
two terms as a non-convex function ρu(.) as follows:

(cid:88)
N(cid:88)
(cid:88)
|Li(x) − Li+n(x + ui→i+n)|+
(cid:88)
(cid:88)
(cid:107)∂∗Ki(ui→i+1, ui→i−1)GiLi − ∂∗Bi(cid:107)2.

n=−N

x

i

λ

ρu(u) = µ

(11)

i

∂∗

To ﬁnd the optimal optical ﬂows u, we ﬁrst convexify the non-
convex function ρu(.) by applying the ﬁrst-order Taylor expan-
sion. Similar to the technique in [16], we linearize the function
near an initial u0 in the iterative process as follows:
ρu(u) ≈ ρu(u0) + ∇ρu(u0)T (u − u0).

(12)

In doing so, (8) can be approximated by a convex function w.r.t u
as follows:

ρu(u0)+∇ρu(u0)T (u − u0)+

min

u

(cid:88)

(cid:88)

N(cid:88)

i

x

n=−N

νu

gi(x)|∇ui→i+n|.

(13)

𝐋𝑖𝐋𝑖+1𝐋𝑖+2𝐱𝐱+𝐮𝒊→𝒊+𝟏𝐱+𝐮𝒊→𝒊+𝟏+𝐮𝒊+𝟏→𝒊+𝟐8

with empirically determined parameters. In the coarse-to-ﬁne
framework, we build image pyramid with 17 levels for a high-
deﬁnition(1280x720) video, and use the scale factor 0.9.

Moreover, to reduce the number of unknowns in optical ﬂows,
we only estimate ui→i+1 and ui→i−1. We approximate ui→i+2
using ui→i+1 and ui+1→i+2. For example, it satisﬁes, ui→i+2 =
ui→i+1 + ui+1→i+2, as illustrated in Fig. 11, and we can easily
apply this for n (cid:54)= 1. Please see our publicly available source code
for more details 1.

The overall process of our algorithm is in Algorithm 1. Further
details on initialization, estimating the duty cycle τi and post-
processing step that reduces artifacts are given below.

Algorithm 1 Overview of the proposed method
Input: Blurry frames B
Output: Latent frames L, optical ﬂows u, and defocus blur maps

σ

1: Initialize u, τi, and σ. (Sec. 4.1)
2: Build image pyramid.
3: Restore L with ﬁxed u and σ. (Sec. 3.1)
4: Estimate u with ﬁxed L and σ. (Sec. 3.2)
5: Estimate σ with ﬁxed L and u. (Sec. 3.3)
6: Detect occlusion and perform post-processing. (Sec 4.2)
7: Propagate variables to the next pyramid level if exists.
8: Repeat steps 3-7 from coarse to ﬁne pyramid level.

4.1 Initialization and Duty Cycle Estimation
In this study, we assume that the camera duty cycle τi is known
for every frame. However, when we conduct deblurring with
conventional datasets, which do not provide exposure information,
we apply the technique proposed in [18] to estimate the duty
cycle. Contrary to the original method [18], we use optical ﬂows
instead of homographies to obtain initially approximated blur
kernels. Therefore, we ﬁrst estimate ﬂow ﬁelds from blurry images
with [39], which runs in near real-time. We then use them as initial
ﬂows and approximate the kernels to estimate the duty cycle.
Moreover, we use σ = 0.8 as initial defocus blur scale.

4.2 Occlusion Detection and Reﬁnement
Our pixel-wise kernel estimation naturally results in approxi-
mation error and it causes problems such as ringing artifacts.
Speciﬁcally, our data model in (4), and temporal coherence model
in (5) are invalid at occluded regions.

To reduce such artifacts from kernel approximation errors and
occlusions, we use spatio-temporal ﬁltering as a post-processing:

Lm+1

i

(x)= 1
Z(x)

wi,n(x, y) · Lm

i+n(y),

(18)

N(cid:88)

(cid:88)

n=−N

y

(cid:80)

(cid:80)N

where y denotes a pixel in the 3x3 neighboring patch at location
(x + ui→i+n) and Z is the normalization factor (e.g. Z(x) =
y wi,n(x, y)). Notably, we enable n = 0 in (18) for
spatial ﬁltering. Our occlusion-aware weight wi,n is deﬁned as
follows:

n=−N

wi,n(x, y) = oi,n(x, y) · exp(−(cid:107)Pi(x) − Pi+n(y)(cid:107)2

),

(19)

2σ2
w

where occlusion state oi,n(x, y) ∈ {0.01, 1} is determined by
cross-checking forward and backward ﬂows similar to the occlu-
sion detection technique used in [40]. The 5x5 patch Pi(x) is
centered at x in frame i. The similarity control parameter σw is
ﬁxed as σw = 25/255.

5 MOTION BLUR DATASET
Because conventional evaluation datasets for deblurring [24], [25]
are generated under static scene assumption, complex and spatially
varying blurs in dynamic scenes are not provided. Therefore, in
this section, we provide a new method generating blur dataset for
the quantitative evaluation of non-uniform video deblurring algo-
rithms and later studies of learning-based deblurring approaches.

5.1 Dataset Generation
As we assume motion blur kernels can be approximated by
using bidirectional optical ﬂows in (3), we can generate blurry
frames adversely by averaging consecutive frames whose relative
motions between two neighboring frames are smaller than 1 pixel.
In doing so, we use GOPRO Hero4 hand-held camera which
supports taking 240 fps video of 1280x720 resolution. Similar
approach was introduced in [41], which uses high-speed camera
to generate blurry images. However, they captured only linearly
moving objects with a static camera.

Our captured videos include various dynamic scenes as well as
static scenes. We calculate the average of k successive frames to
generate a single blurry frame. By averaging k successive frames,
realistic motion blurs from both moving objects and the camera
shake can be rendered in the blurry frame and the 240/k fps blurry
video can be generated (i.e. 16 fps video is generated by averaging
every 15 frames). Notably, ground truth sharp frame is chosen to
be the mid-frame used in averaging, since we aim to restore the
latent frame captured in the middle of exposure time as shown in
ﬁg. 6. Thus the duty cycle is τi = 0.5, in our whole dataset. The
videos are recorded with caution so that the motions should be
no greater than 1 pixel between two neighboring frames to render
more smooth and realistic blurry frame.

Our dataset mainly captured outdoor scenes to avoid ﬂickering
effect of ﬂuorescent light which occurs when we capture indoor
scenes with the high-speed camera. We captured numerous scenes
in both dynamic and static environments, and each frame has HD
(1280x720) size. In Fig. 12, some examples of our ground truth
frames and rendered blurry frames are shown. We can see that
the generated blurs are locally varying according to the depth
changes and moving objects. Our dataset is publicly available on
our website 2. We provide the generated blurry and corresponding
sharp videos as well as the original videos we recorded.

6 EXPERIMENTAL RESULTS
In this section, we empirically demonstrate the superiority of the
proposed method over conventional methods.

In Table 1 and Table 2, our deblurring results are quantita-
tively evaluated with the proposed dataset. For evaluation, we
use ﬁxed parameters and the values are λ = 250, µ = 2,
νu = νσ = 0.08λ, vI = ( 25
255 )2, and N = 2. Since the source
codes of other video deblurring methods that can handle non-
uniform blur are not available, we evaluate our method in different

1. http://cv.snu.ac.kr/research/∼VD/

2. http://cv.snu.ac.kr

KIM et al.: DYNAMIC SCENE DEBLURRING USING A LOCALLY ADAPTIVE LINEAR BLUR MODEL

9

Fig. 12: (a) Ground truth sharp frames. (b) Generated blurry frames. Spatially varying blurs by object motions and camera shakes are
synthesized realistically.

settings. First, we calculate and compare both the PSNR and SSIM
values of each original blurry sequence and the corresponding
deblurred one. As our dataset contains only motion blurs in it,
we restore the latent frames without considering the defocus
blur (defocus blur kernel is set to be identity matrix). Next, to
demonstrate the good performance of the proposed method in
removing defocus blurs, we re-generate blurry dataset by adding
Gaussian blur (σ = 1.5) to the original sharp video before
averaging. Using this dataset which contains both motion blur
and defocus blur, we compare the our result against each original
blurry sequence and our deblurring result that does not consider
defocus blur. We verify that, our approach improves the deblurring
results signiﬁcantly in terms of PSNR and SSIM by removing
blurs from defocus. In Fig. 13, qualitative comparisons using our
dataset are shown. Ours restores the edges of buildings, letters,
and moving persons, clearly. However, we observe some failure
cases in our results. In Fig. 14, we fail to estimate motions of fast
moving hand, and thus fail in deblurring, since it is difﬁcult to
estimate accurate ﬂows of small structure with distinct motions in
the coarse-to-ﬁne framework as reported in [42].

Next, we compare our deblurring results with those of the
state-of-the art exemplar based method [18] with the videos
used in [18]. As shown in Fig. 15,
the captured scenes are
dynamic and contain multiple moving objects. The method [18]
fails in restoring the moving objects, because the object motions
are large and distinct from the backgrounds. By contrast, our
results show better performances in deblurring moving objects and
backgrounds. Notably, the exemplar-based approach also fails in
handling large blurs, as shown in Fig. 16, as the initially esti-

Fig. 14: A failure case. (a) A blurry frame in the proposed dataset.
(b) Our deblurring result.

mated homographies in the largely blurred images are inaccurate.
Moreover, this approach renders excessively smooth results for
mid-frequency textures such as trees, as the method is based on
interpolation without spatial prior for latent frames.

We also compare our method with the state-of-the-art
segmentation-based approach [15]. The test video is shown in
Fig. 17, which is a bilayer scene used in [15]. Although the
bi-layer scene is a good example to verify the performance of
the layered model, inaccurate segmentation near the boundaries
causes serious artifacts in the restored frame. By contrast, since
our method does not need segmentation and it restores the bound-
aries much better than the layered model.

In Fig. 18, we quantitatively compare the optical ﬂow accu-
racies with [30] on synthetic blurry images. As publicly available
code of [30] cannot handle Gaussian blur, we synthesize blurry
frames which have motion blurs only. Although [30] was proposed
to handle blurry images in optical ﬂow estimation, its assumption
does not hold in motion boundaries, which is very important for

(a)(b)(a)(b)10

Fig. 13: Comparative deblurring results using our dataset. (a) Ground truth sharp frames. (b) Generated blurry frames. (c) Our results
without considering defocus blurs. (d) Our ﬁnal results.

deblurring. Therefore, their optical ﬂow is inaccurate in the motion
boundaries of moving objects. However, our model can cope with
abrupt motion changes, and thus performs better than the previous
models.

Moreover, we show the deblurring results with and without
using the temporal coherence term in (5), and verify that our
temporal coherence model clearly restores edges and signiﬁcantly
reduces ringing artifacts near the edges in Fig. 19.

Finally, other deblurring results from numerous real videos
are shown in Fig. 20. Notably, our model successfully restores
the face which has highly non-uniform blurs because the person
moves rotationally (Fig. 20(e)).

The video demo is provided in the supplementary material.

For additional results, please see the supplementary material.

7 CONCLUSION
In this study, we introduced a novel method that removes general
blurs in dynamic scenes which conventional methods fail to. We
inferred bidirectional optical ﬂows to approximate motion blur
kernels, and estimated the scales of Gaussian blurs to approximate
defocus blur kernels. Therefore we handled general blurs, by

estimating a pixel-wise different blur kernel. In addition, we
proposed a new single energy model that estimates optical ﬂows,
defocus blur maps and latent frames, jointly. We also provided a
framework and efﬁcient solvers to minimize the proposed energy
function and it has been shown that our method yields supe-
rior deblurring results to several state-of-the-art deblurring meth-
ods through intensive experiments with real challenging blurred
videos. Moreover, we provided the publicly available benchmark
dataset to evaluate the non-uniform deblurring methods and we
quantitatively evaluated the performance of the proposed method
using the proposed dataset. Nevertheless, our model has its limita-
tions in handling large displacement ﬁelds. Therefore, improving
the proposed algorithm to handle large displacements is required.
Moreover, since our current work is implemented on Matlab, it
is time consuming and needs large resources. Thus, for practical
applications, reducing the running time by code optimization and
parallel implementation as well as efﬁcient memory management
will be considered in our future work.

REFERENCES
[1] S. Cho and S. Lee, “Fast motion deblurring,” in SIGGRAPH, 2009.

(a)(b)(c)(d)KIM et al.: DYNAMIC SCENE DEBLURRING USING A LOCALLY ADAPTIVE LINEAR BLUR MODEL

11

Fig. 15: Left to right: Blurry frames of dynamic scenes, deblurring results of [18], and our results.

Fig. 16: Left to right: Blurry frame, deblurring result of [18], and ours.

[2] R. Fergus, B. Singh, A. Hertzmann, S. T. Roweis, and W. Freeman,
“Removing camera shake from a single photograph,” in SIGGRAPH,
2006.

[3] A. Gupta, N. Joshi, L. Zitnick, M. Cohen, and B. Curless, “Single image

deblurring using motion density functions,” in ECCV, 2010.

[4] M. Hirsch, C. J. Schuler, S. Harmeling, and B. Scholkopf, “Fast removal
of non-uniform camera shake,” in Computer Vision (ICCV), 2011 IEEE
International Conference on.

IEEE, 2011, pp. 463–470.

[5] Q. Shan, J. Jia, and A. Agarwala, “High-quality motion deblurring from

a single image,” in SIGGRAPH, 2008.

[6] O. Whyte, J. Sivic, A. Zisserman, and J. Ponce, “Non-uniform deblurring
for shaken images,” International Journal of Computer Vision, vol. 98,
no. 2, pp. 168–186, 2012.
J.-F. Cai, H. Ji, C. Liu, and Z. Shen, “Blind motion deblurring using
multiple images,” Journal of computational physics, vol. 228, no. 14, pp.
5057–5071, 2009.

[7]

[8] Y. Li, S. B. Kang, N. Joshi, S. M. Seitz, and D. P. Huttenlocher, “Gen-
erating sharp panoramas from motion-blurred videos,” in Proc. IEEE
International Conference on Computer Vision and Pattern Recognition,
2010.

[9] Y.-W. Tai, P. Tan, and M. S. Brown, “Richardson-lucy deblurring for
scenes under a projective motion path,” Pattern Analysis and Machine
Intelligence, IEEE Transactions on, vol. 33, no. 8, pp. 1603–1618, 2011.
[10] S. Cho, H. Cho, Y.-W. Tai, and S. Lee, “Registration based non-uniform
motion deblurring,” in Computer Graphics Forum, vol. 31, no. 7. Wiley
Online Library, 2012, pp. 2183–2192.

[11] C. Paramanand and A. N. Rajagopalan, “Non-uniform motion deblurring
for bilayer scenes,” in Proc. IEEE International Conference on Computer
Vision and Pattern Recognition, 2013.

[12] H. S. Lee and K. M. Lee, “Dense 3d reconstruction from severely blurred
images using a single moving camera,” in Proc. IEEE International
Conference on Computer Vision and Pattern Recognition, 2013.

#032#032#036#040#032#036#040#032#036#040#014#018#022#014#018#022#014#018#022#01412

Fig. 17: Comparison with segmentation-based approach. Left to right: Blurry frame, result of [15], and ours.

TABLE 1: Deblurring performance evaluations in terms of PSNR.
Motion blur + Gaussian blur (σ = 1.5)
Blurry

Motion blur only
ours

Blurry

ours

Seq.

(w/o defocus)

(w/o defocus)

#1
#2
#3
#4
#5
#6
#7
#8
#9
#10
#11
Avg.

26.8
26.5
33.28
37.07
24.34
26.83
29.03
24.80
28.55
26.13
29.24
28.42

27.79
27.68
34.78
36.94
23.62
29.07
30.52
29.81
31.41
30.55
33.61
30.52

25.88
24.29
30.55
36.52
23.78
24.04
25.95
23.57
27.19
24.83
27.73
26.76

27.53
25.18
31.27
36.50
23.05
25.18
27.31
26.05
29.05
27.61
30.47
28.11

TABLE 2: Deblurring performance evaluations in terms of SSIM.
Motion blur + Gaussian blur (σ = 1.5)
Blurry

Motion blur only
ours

Blurry

ours

Seq.

(w/o defocus)

(w/o defocus)

#1
#2
#3
#4
#5
#6
#7
#8
#9
#10
#11
Avg.

0.8212
0.8571
0.9327
0.9701
0.7154
0.8362
0.8751
0.8068
0.8322
0.8083
0.9176
0.8521

0.8611
0.8847
0.9473
0.9695
0.7181
0.9178
0.9244
0.9269
0.9100
0.9198
0.9608
0.9037

0.7898
0.7526
0.8750
0.9652
0.6853
0.7362
0.7928
0.7529
0.7908
0.7620
0.8945
0.7997

0.8374
0.7809
0.8849
0.9665
0.6598
0.7880
0.8360
0.8320
0.8427
0.8432
0.9283
0.8363

[13] S. Cho, Y. Matsushita, and S. Lee, “Removing non-uniform motion
blur from images,” in Computer Vision, 2007. ICCV 2007. IEEE 11th
International Conference on.

IEEE, 2007, pp. 1–8.

[14] L. Bar, B. Berkels, M. Rumpf, and G. Sapiro, “A variational framework
for simultaneous motion estimation and restoration of motion-blurred
video,” in Proc. IEEE International Conference on Computer Vision and
Pattern Recognition, 2007.

[15] J. Wulff and M. J. Black, “Modeling blurred video with layers,” in ECCV,

2014.

[16] T. H. Kim and K. M. Lee, “Segmentation-free dynamic scene deblurring,”
in Proc. IEEE International Conference on Computer Vision and Pattern
Recognition, 2014.

[17] Y. Matsushita, E. Ofek, W. Ge, X. Tang, and H.-Y. Shum, “Full-
frame video stabilization with motion inpainting,” Pattern Analysis and
Machine Intelligence, IEEE Transactions on, vol. 28, no. 7, pp. 1150–

ours
(full)
27.67
26.18
32.65
36.36
24.24
26.46
28.55
27.01
27.74
28.25
30.86
28.73

ours
(full)
0.8476
0.8197
0.9087
0.9657
0.7293
0.8334
0.8694
0.8582
0.8854
0.8645
0.9367
0.9367

1163, 2006.

[18] S. Cho, J. Wang, and S. Lee, “Video deblurring for hand-held cameras
using patch-based synthesis,” ACM Transactions on Graphics, vol. 31,
no. 4, pp. 64:1–64:9, 2012.

[19] S. Bae and F. Durand, “Defocus magniﬁcation,” in Computer Graphics

Forum, vol. 26, no. 3. Wiley Online Library, 2007, pp. 571–579.

[20] E. Kee, S. Paris, S. Chen, and J. Wang, “Modeling and removing
spatially-varying optical blur,” in Computational Photography (ICCP),
2011 IEEE International Conference on.

IEEE, 2011, pp. 1–8.

[21] X. Zhu, S. Cohen, S. Schiller, and P. Milanfar, “Estimating spatially
varying defocus blur from a single image,” Image Processing, IEEE
Transactions on, vol. 22, no. 12, pp. 4879–4891, 2013.

[22] S. Zhuo and T. Sim, “Defocus map estimation from a single image,”

Pattern Recognition, vol. 44, no. 9, pp. 1852–1858, 2011.

[23] T. Hyun Kim and K. Mu Lee, “Generalized video deblurring for dynamic
scenes,” in Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, 2015, pp. 5426–5434.

[24] A. Levin, Y. Weiss, F. Durand, and W. T. Freeman, “Understanding and
evaluating blind deconvolution algorithms,” in Proc. IEEE International
Conference on Computer Vision and Pattern Recognition, 2009.

[25] R. K¨ohler, M. Hirsch, B. Mohler, B. Sch¨olkopf, and S. Harmeling,
“Recording and playback of camera shake: Benchmarking blind decon-
volution with a real-world database,” in Computer Vision–ECCV 2012.
Springer, 2012, pp. 27–40.

[26] L. Xu, J. S. Ren, C. Liu, and J. Jia, “Deep convolutional neural network
for image deconvolution,” in Advances in Neural Information Processing
Systems, 2014, pp. 1790–1798.

[27] C. J. Schuler, M. Hirsch, S. Harmeling, and B. Sch¨olkopf, “Learning to
deblur,” IEEE Transactions on Pattern Analysis and Machine Intelligence
(PAMI), 2015.

[28] J. Sun, W. Cao, Z. Xu, and J. Ponce, “Learning a convolutional
neural network for non-uniform motion blur removal,” arXiv preprint
arXiv:1503.00593, 2015.

[29] S. Dai and Y. Wu, “Motion from blur,” in Proc. IEEE International

Conference on Computer Vision and Pattern Recognition, 2008.

[30] T. Portz, L. Zhang, and H. Jiang, “Optical ﬂow in the presence of
spatially-varying motion blur,” in Proc. IEEE International Conference
on Computer Vision and Pattern Recognition, 2012.

[31] J. Shi, L. Xu, and J. Jia, “Just noticeable defocus blur detection and
estimation,” in Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, 2015, pp. 657–665.

[32] D. Krishnan and R. Fergus, “Fast image deconvolution using hyper-

laplacian priors,” in NIPS, 2009.

[33] D. Krishnan, T. Tay, and R. Fergus, “Blind deconvolution using a
normalized sparsity measure,” in Proc. IEEE International Conference
on Computer Vision and Pattern Recognition, 2009.

[34] A. Levin and Y. Weiss, “User assisted separation of reﬂections from
a single image using a sparsity prior,” IEEE Trans. Pattern Analysis
Machine Intelligence, vol. 29, no. 9, pp. 1647–1654, 2007.

[35] Z. Hu, L. Xu, and M.-H. Yang, “Joint depth estimation and camera
shake removal from single blurry image,” in Proc. IEEE International
Conference on Computer Vision and Pattern Recognition, 2014.

[36] T. H. Kim, B. Ahn, and K. M. Lee, “Dynamic scene deblurring,”
in Computer Vision (ICCV), 2013 IEEE International Conference on.
IEEE, 2013, pp. 3160–3167.

[37] T. H. Kim, H. S. Lee, and K. M. Lee, “Optical ﬂow via locally adaptive
fusion of complementary data costs,” in Computer Vision (ICCV), 2013
IEEE International Conference on.

IEEE, 2013, pp. 3344–3351.

[38] A. Chambolle and T. Pock, “A ﬁrst-order primal-dual algorithm for
convex problems with applications to imaging,” Journal of Mathematical

KIM et al.: DYNAMIC SCENE DEBLURRING USING A LOCALLY ADAPTIVE LINEAR BLUR MODEL

13

Fig. 18: EPE denotes average end point error. (a) Color coded
ground truth optical ﬂow between blurry images. (b) Optical ﬂow
estimation result of [30]. (c) Our result.

Fig. 19: (a) A blurry frame of a video. (b) Our deblurring
result without using Etemporal. (c) Our deblurring result with
Etemporal.

Imaging and Vision, vol. 40, no. 1, pp. 120–145, May 2011. [Online].
Available: http://dx.doi.org/10.1007/s10851-010-0251-1

[39] A. Wedel, T. Pock, C. Zach, H. Bischof, and D. Cremers, “An improved
algorithm for tv-l 1 optical ﬂow,” in Statistical and Geometrical Ap-
proaches to Visual Motion Analysis. Springer, 2009, pp. 23–45.

[40] C. Rhemann, A. Hosni, M. Bleyer, C. Rother, and M. Gelautz, “Fast cost-
volume ﬁltering for visual correspondence and beyond,” in Computer
Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on.
IEEE, 2011, pp. 3017–3024.

[41] A. Agrawal and R. Raskar, “Optimal single image capture for motion
deblurring,” in Computer Vision and Pattern Recognition, 2009. CVPR
2009. IEEE Conference on.

IEEE, 2009, pp. 2560–2567.

[42] L. Xu, J. Jia, and Y. Matsushita, “Motion detail preserving optical ﬂow
estimation,” IEEE Trans. Pattern Analysis Machine Intelligence, vol. 34,
no. 9, pp. 1744–1757, 2012.

Fig. 20: Left to right: Numerous real blurry frames and our
deblurring results.

(b)(c)EPE = 23.3EPE = 2.32(a)(a)(b)(c)(a)(b)(c)(c)(d)(e)(a)(b)14

Tae Hyun Kim received the BS degree and the
MS degree in the department of electrical en-
gineering from KAIST, Daejeon, Korea, in 2008
and 2010, respectively. He is currently working
toward the PhD degree in Electrical and Com-
puter Engineering at Seoul National University.
His research interests include motion estimation,
and deblurring. He is a student member of the
IEEE.

Seungjun Nah received the BS degree in Elec-
trical and Computer Engineering from Seoul Na-
tional University (SNU), Seoul, Korea in 2014.
He is currently working towards PhD degree in
Electrical and Computer Engineering at Seoul
National University. He is interested in computer
vision problems including deblurring and visual
saliency. He is a student member of the IEEE.

Kyoung Mu Lee received the BS and MS de-
grees in control and instrumentation engineering
from Seoul National University (SNU), Seoul,
Korea in 1984 and 1986, respectively, and the
PhD degree in electrical engineering from the
University of Southern California (USC), Los An-
geles, California in 1993. He received the Ko-
rean Government Overseas Scholarship during
the PhD courses. From 1993 to 1994, he was
a research associate in the Signal and Image
Processing Institute (SIPI) at USC. He was with
the Samsung Electronics Co. Ltd. in Korea as a senior researcher
from 1994 to 1995. In August 1995, he joined the Department of Elec-
tronics and Electrical Engineering of the Hong-Ik University, and was
an assistant and associate professor. Since September 2003, he has
been with the Department of Electrical and Computer Engineering at
Seoul National University as a professor, and leads the Computer Vision
Laboratory. His primary research is focused on statistical methods in
computer vision that can be applied to various applications including
object recognition, segmentation, tracking and 3D reconstruction. He
has received several awards, in particular, the Most Inﬂuential Paper
over the Decade Award by the IAPR Machine Vision Application in 2009,
the ACCV Honorable Mention Award in 2007, the Okawa Foundation
Research Grant Award in 2006, and the Outstanding Research Award by
the College of Engineering of SNU in 2010. He served as an Associate
Editor in Chief, Editorial Board member of the EURASIP Journal of
Applied Signal Processing, and is an associate editor of the Machine
Vision Application Journal, the IPSJ Transactions on Computer Vision
and Applications, and the Journal of Information Hiding and Multimedia
Signal Processing. He has (co)authored more than 100 publications in
refereed journals and conferences including PAMI, IJCV, CVPR, ICCV,
and ECCV.

