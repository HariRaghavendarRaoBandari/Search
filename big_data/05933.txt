Distributed Iterative Learning Control for a Team of Quadrotors

Andreas Hock and Angela P. Schoellig

6
1
0
2

 
r
a

 

M
8
1

 
 
]

O
R
.
s
c
[
 
 

1
v
3
3
9
5
0

.

3
0
6
1
:
v
i
X
r
a

Abstract— The goal of this work is to enable a team of
quadrotors to learn how to accurately track a desired trajectory
while holding a given formation. We solve this problem in a
distributed manner, where each vehicle has only access to the
information of its neighbors. The desired trajectory is only
available to one (or few) vehicles. We present a distributed
iterative learning control (ILC) approach where each vehicle
learns from the experience of its own and its neighbors’ previous
task repetitions and adapts its feedforward input to improve
performance. Existing algorithms are extended in theory to
make them more applicable for real-world experiments. In
particular, we prove stability for any causal learning function
with gains chosen according to a simple scalar condition.
Previous proofs were restricted to a speciﬁc learning function,
which only depends on the tracking error derivative (D-type
ILC). This extension provides more degrees of freedom in
the ILC design and, as a result, better performance can be
achieved. We also show that stability is not affected by a linear
dynamic coupling between neighbors. This allows us to use an
additional consensus feedback controller to compensate for non-
repetitive disturbances. Experiments with two quadrotors attest
the practical applicability of the proposed distributed multi-
agent ILC approach. This is the ﬁrst work to show distributed
ILC in experiment.

I. INTRODUCTION

Multi-agent systems (MAS) and machine learning are two
exciting trends in robotics. In the past decades, theoretic
contributions on MAS have come from ﬁelds such as bi-
ology, computer science and control theory. One problem
of particular interest is consensus, which is concerned with
all agents agreeing on some quantity of interest by only
communicating with their neighbors. Many other problems
can be transformed into a consensus problem; examples
include ﬂocking, rendezvous or formation control [1]. Based
on distributed algorithms, autonomous agents can achieve
consensus without a central control unit. Machine learning
on the other hand, aims to make autonomous systems more
capable by enabling them to adapt to unknown situations,
autonomously correct for modeling errors, and improve their
performance without human instructions. As the number of
autonomous systems increases in all areas including indus-
trial and service robots, commercial drones, and even cars,
the question arises how their cooperation can be improved
and,
therefore, how MAS and machine learning can be
combined.

at
(UTIAS),
schoellig@utias.utoronto.ca

of Toronto
Email:

Institute

The authors are with the Dynamic Systems Lab (www.dynsyslab.org)
Studies
andreas.hock@robotics.utias.utoronto.ca,

the University
Canada.

for Aerospace

This research was supported in part by NSERC grant RGPIN-2014-
04634, the Connaught New Researcher Award and the Baden-W¨urttemberg-
STIPENDIUM.

x1(t)

x4(t)

v1

v4

v2

v3

x2(t)

x3(t)

Fig. 1. A team of quadrotors learns to accurately track a desired trajectory
while holding a given formation. The black arrows between the vehicles vi
represent the communication paths. The blue arrows depict the vehicle
trajectories xi(t).

In this paper, we focus on Iterative Learning Control
(ILC) approaches, where a system learns to track a desired
trajectory by repetitively executing the same task. Based on
the tracking error from previous trials, the feedforward input
is adapted to gradually improve performance. As such, ILC
is able to compensate for repetitive disturbances. Initially
developed in 1984 [2], it has since been studied widely
in theory and experiments, see [3]. In [4], ILC was used
for improving the trajectory tracking of a single quadrotor
vehicle.

Distributed ILC achieves formation control of MAS,
where only a subset of agents has direct access to the
reference trajectory and only neighboring agents can com-
municate, see Figure 1. The goal is to follow the reference
with all agents holding a predeﬁned formation. To achieve
this, every agent updates its input trajectory based on the
information about its own and its neighbors’ state trajectories
during the last trial. The idea of distributed ILC was ﬁrst
introduced in 2009 [5], where stability was proven for a D-
type input update rule; that is, the input for the next iteration
is computed based on the previous input and the derivative
of the tracking error. Furthermore, this ﬁrst paper assumes
communication graphs in the form of directed spanning
trees. The heterogeneous agent dynamics are described in
continuous time and are assumed to be nonlinear with rela-
tive degree one. In [6], the proof was extended to arbitrary
directed graphs and the stability criterion was simpliﬁed
for the case of homogeneous agents. Even time-varying
dynamics can be handled with the approach in [6]. To extend
the algorithm to systems of higher relative-degree, higher
derivatives (or for discrete-time systems appropriate time-
shifts) were used in the input update rule, see [7]. This
approach is, however, restricted to linear agent dynamics, but
holds for weighted and directed communication graphs. All
these papers ([5], [6], [7]) are using the same D-type input
update rule. The shortcoming of this control strategy is a
poor convergence behavior; in particular, a constant offset

in the initial condition cannot be compensated for as D-type
ILC only adapts the input based on errors in the velocity.

In this paper, we prove that for linear agents any causal
learning function with gains chosen according to a simple
scalar condition is stable. This allows more options in the
choice of the input-update rule (beyond the typical D-type
update rule) and thus faster convergence can be achieved.
Moreover, a constant error in the initial position can now
be compensated for by incorporating position errors into the
learning function.

Several other multi-agent ILC approaches and extensions
are found in the literature including approaches for switching
graph structures [8], and adaptive [9], robust [10] or optimal
[11] update laws. Nevertheless, all of these approaches have
only been implemented in simulation and, to the authors’
best knowledge, multi-agent ILC has not been tested on a
real system before.

This paper demonstrates the proposed generalized multi-
agent ILC algorithm in experiment on quadrotor vehicles.
To increase robustness against non-repetitive disturbances
occurring in real-world experiments, we include a distributed
feedback controller, which improves the performance during
single iterations. Furthermore, we show that neither the
distributed feedback controller nor any other linear dynamic
coupling between neighboring agents affect stability of the
ILC algorithm.

The paper is structured as follows: ﬁrst, we introduce
some deﬁnitions and basic terminology from graph theory. In
Section III, formation tracking is formulated as a consensus
problem. Then, we derive the ILC stability conditions for an
arbitrary linear causal learning function in Section IV. Based
on these results, we show that a dynamic coupling between
neighboring agents does not inﬂuence stability. In Section VI,
we apply the theory to a team of two quadrotors and show
corresponding experimental results. Finally, conclusions are
provided in Section VII.

II. PRELIMINARIES ON GRAPH THEORY

To describe the information exchange between agents,
graph theory is commonly used. The vehicles are the nodes
of the graph and the edges represent the information ﬂow
between vehicles, see Figure 1. In the following, we provide
some useful deﬁnitions and notation.
Let G = (V,E,A) denote a directed graph with a set
i ∈ {1, 2, ..., N}}, and the
of vertices V(G) = {vi
edge set E(G) ⊆ {(vi, vj) : vi, vj ∈ V(G)}. The edge
(vi, vj) means that agent vi receives information from vj,
in which case vi is called the child and vj the parent vertex.
We deﬁne A = (aij) ∈ RN×N as the adjacency matrix
of G with elements representing the information exchange
between any two agents; that is, aij = 1 if (vi, vj) ∈ E(G)
and aij = 0 otherwise. The in-degree of node vi is deﬁned
as din
j=1 aij; that is, it describes the number of
edges entering a node. The (in-degree) Laplacian matrix is
deﬁned as LG = D − A, where D = diag(din
N ).
If there exists a special vertex, the root, with no parents
(din = 0) which all other nodes can be connected to through

i = (cid:80)N

2 , ..., din

:

1 , din

v1

v4

v2

v5

v3

v6

Fig. 2.
Spanning tree structure for a directed graph. Node v1 has no
incoming edges and is called the root node. All other vertices have exactly
one parent (one incoming edge) and can be connected to the root through
directed paths.

directed paths and every other vertex has exactly one parent,
then the graph is called a spanning tree, see Figure 2. Note
that some authors also use the term ’rooted out-branching’
to distinguish between directed and undirected graphs. For
further information about graph theory, see [12].

III. PROBLEM FORMULATION

We consider a group of N autonomous, homogeneous
agents with ﬁxed interaction topology deﬁned by G =
(V,E,A).
A. Agent Dynamics

The linear discrete-time single-input, single-output (SISO)
dynamics of the ith agent at the kth iteration of the task
k ∈ {1, 2, . . .} and for sampling times t ∈ {0, 1, . . . , T} are
described by

xi,k(t + 1) = Axi,k(t) + Bui,k(t)

yi,k(t) = Cxi,k(t),

(1)
where xi,k(t) ∈ Rn is the state vector, ui,k(t) ∈ R the input
and yi,k(t) ∈ R the output. Accordingly, A ∈ Rn×n, B ∈
Rn, C ∈ R1×n.
Using the time-shift operator q−1, deﬁned by q−1x(t) =
x(t − 1), the state-space system (1) can be represented as
(2)

yi,k(t) = P (q)ui,k(t) + di(t),

where di(t) = CAtxi(0) is the free response to the initial
condition, which is assumed to be repeatable for every
iteration. The input-output mapping P (q) is given by an
inﬁnite power series,

P (q) = p1q−1 + p2q−2 + p3q−3 + . . . ,

(3)
with Markov parameters pm = CAm−1B. The relative
degree r of the system (1) deﬁnes the number of coefﬁcients
equal zero; that is, pm = 0 for m < r, pm (cid:54)= 0 for m = r.
B. Reference Trajectory

A reference trajectory ydes(t) is given, which a certain
subset of agents has access to. The goal is to track this
reference signal with all agents simultaneously. The result
can be directly applied to formation control by simply
deﬁning ﬁxed or time-varying relative distances between the
agents. We model the reference as an additional node v0,
the virtual leader. This results in the extended graph G∗.
Let bi ∈ {0, 1} be deﬁned for every agent analogously to

(cid:21)

(cid:20) 0

the entries of the adjacency matrix: bi = 1 if agent i has
access to the reference trajectory, and bi = 0 otherwise.
The corresponding Laplacian LG∗, in the following simply
denoted by L∗, is

L∗ =

01×N
LG + B

−b

(4)
with b = (b1, b2, . . . , bN ), B = diag(b) and 01×N denoting
a matrix of size (1 × N ) with all entries being zero.
C. Error Signals

The only information exchanged between agents is the
relative position, which can be easily obtained in practical
implementations. For example, an agent’s own camera sys-
tem could detect the neighbors and measure the distances;
no communication between agents would be necessary. Let
us deﬁne an error signal ei,k(t) for each agent as the sum
of relative distances to all its neighbors and to the virtual
leader if accessible:

j=1

ei,k(t) =

aij(yj,k(t) − yi,k(t)) + bi(ydes(t) − yi,k(t)).
(5)
Remark 1. The error function (5) is for the consensus case,
in which all agents aim to follow the same trajectory. If
the goal is a desired constant or time-varying formation,
additional iteration-invariant terms, the desired distances
∆ij(t), have to be added. Thus,

N(cid:88)

ei,k(t) =

aij(yj,k(t) − yi,k(t) + ∆ij(t))

j=1

+ bi(ydes(t) − yi,k(t) + ∆i(t)).
IV. DISTRIBUTED LEARNING

(6)

As ILC approach, we use the distributed input update rule

ui,k+1(t) = ui,k(t) + L(q)ei,k(t + r),

(7)
with i ∈ {1, . . . , N} and t ∈ {1, . . . , T − r} and causal
learning function

L(q) = l0 + l1q−1 + l2q−2 + . . . ,

(8)

where r is the relative degree. The block diagram is shown in
Figure 3. In the following, the relative degree is assumed to
be one without loss of generality. A higher relative degree
can be compensated for by a corresponding time shift of
the error vector. The acausality in the update rule is not a
problem as the input update is computed after the previous
execution was completed. Algorithm (7) can be interpreted
as a simple, ﬁrst-order ILC algorithm commonly used in
literature, see [3]. The order of an ILC algorithm is deﬁned
as the number of previous iterations taken into account.

Multi-agent ILC was ﬁrst presented in [5] and similar ap-
proaches in [6], [7]. They use D-type ILC-algorithms which
are special cases of the learning function L(q) proposed in
this work.

For analyzing stability, we use the lifted-system represen-
tation according to [3], where all samples of a signal are

N(cid:88)

Memory

ei

Memory

L(q)

ui

yi

P(q)

Fig. 3. Basic ILC structure for vehicle vi. P (q) denotes the plant, L(q)
the learning function. The input is computed based on the previous input
and previous distributed tracking error ei, which are saved in memory units.

stacked in a large vector. The system dynamics (2) and the
input update (7) are now represented by

=P

+


(cid:124)

yi,k(1)
yi,k(2)

...
(cid:123)(cid:122)

yi,k(T )

yi,k
ui,k+1(0)
ui,k+1(1)

...
(cid:123)(cid:122)

ui,k+1


(cid:125)

(cid:125)


(cid:125)


(cid:124)


(cid:124)

ui,k(T − 1)

ui,k(0)
ui,k(1)

...
(cid:123)(cid:122)

ui,k
ui,k(0)
ui,k(1)

...
(cid:123)(cid:122)

ui,k


(cid:125)

=

+L

ui,k+1(T − 1)

ui,k(T − 1)


(cid:124)

with lower-triangular, Toeplitz matrices




p1
p2
...
pT
l0
l1
...
lT−1

P =

L =

0
p1
...
pT−1
0
l0
...
lT−2

. . .
. . .
...
. . .

. . .
. . .
...
. . .

(9)

(10)

,


(cid:125)

(cid:125)

,

ei,k(1)
ei,k(2)

ei,k(T )

di(T )

di

ei,k

di(2)
. . .

 di(1)
(cid:123)(cid:122)
(cid:124)

(cid:124)

...
p1
0
0

...
(cid:123)(cid:122)
 ,
 .


u1,k
u2,k
...
(cid:124) (cid:123)(cid:122) (cid:125)
uN,k

...
l0

0
0

Uk

+

Let us now collect all single-agent dynamics into one equa-

tion

y1,k
y2,k
...
(cid:124) (cid:123)(cid:122) (cid:125)
yN,k

Yk


(cid:124)

P
0

...

=

0
P

...

. . .
. . .
...
. . .

(cid:123)(cid:122)

IN⊗P

0

. . .


(cid:125)

0
0

...

P



d1
d2
...
(cid:124) (cid:123)(cid:122) (cid:125)
dN

D

,

(11)
where ⊗ denotes the Kronecker product and IN the (N ×
N ) identity matrix. Analogously, using (5) and the graph-
theoretic deﬁnitions from Sec. II, the multi-agent version of
(10) is

Uk+1 =Uk − (IN ⊗ L)·

(cid:16)(cid:0)(LG + B) ⊗ IT

(cid:1)Yk − (b ⊗ IT )ydes

(cid:17)

,

(12)

with ydes = (ydes(1), ydes(2), . . . , ydes(T ))T . For the sake
of simplicity, we collect all iteration-invariant terms, i.e.
terms only depending on the initial conditions or the ref-
erence trajectory, in δ. Then, by plugging (11) into (12)

the Kronecker product

that

and using the property of
(A ⊗ B)(C ⊗ D) = AC ⊗ BD, it follows

Uk+1 = Uk − (IN ⊗ L)(cid:0)(LG + B) ⊗ IT
= Uk −(cid:0)IN (LG + B)IN ⊗ LIT P(cid:1)Uk + δ
=(cid:0)IN T − (LG + B) ⊗ LP(cid:1)Uk + δ.

(cid:1)(IN ⊗ P)Uk + δ

(13)

Based on stability theory for discrete systems, conditions for
the stability of single-agent ILC were developed in [13] and
presented in [3] slightly modiﬁed. The following deﬁnitions
are taken from the latter and adapted to MAS.
Deﬁnition 1. An ILC system is called asymptotically stable
(AS) if there exists ¯U ∈ RN T such that ∀k = {0, 1, . . .}

|Uk| ≤ ¯U and

lim
k→∞ Uk

exists.

Using this deﬁnition, equation (13), and the notation of
the spectral radius ρ as the maximum absolute eigenvalue,
we can state:
Theorem 1. The multi-agent ILC system (11)-(13) is AS if
and only if

ρ(cid:0)IN T − (LG + B) ⊗ LP(cid:1) < 1,

or equivalently,

|1 − λil0p1| < 1,
where λi are the eigenvalues of (LG + B).

max

i

(14)

(15)

Note that we assumed a relative degree r = 1, and thus
p1 (cid:54)= 0. The eigenvalues of the graph Laplacian can be
computed easily. As stability depends only on l0, the learning
function L(q) can be tuned with many degrees of freedom
to achieve good convergence behavior.

Proof. The ﬁrst statement (14) follows directly from [13]
applied to (13). The latter equation (15) is obtained by
applying a similarity transformation to (14) similarly to how
it is done for undirected graphs in [6].
As all entries in (LG + B) are real, this matrix can be
transformed to Jordan normal form, i.e. it exists a regular
matrix S ∈ RN×N and a Jordan matrix J ∈ RN×N
with eigenvalues on the diagonal and possible ones on the
subdiagonal, such that

S−1(LG + B)S = J.

(16)

Usually J is deﬁned with ones on the superdiagonal but, for
simplicity, we use this less common deﬁnition to get lower-
triangular matrices. As eigenvalues and thus the spectral
radius remain the same under similarity transformations, it
follows

ρ(cid:0)IN T − (LG + B) ⊗ LP(cid:1)
(cid:16)
(S ⊗ IT )−1(cid:0)IN T − (LG + B) ⊗ LP(cid:1)(S ⊗ IT )

(cid:17)

=ρ
=ρ(IN T − J ⊗ LP),

(17)

where L and P are lower triangular matrices, as is J. As a
result, the eigenvalues are the diagonal entries. Multiplication

of triangular matrices does not affect this property, thus (14)
is equivalent to the scalar condition (15).

Theorem 2. For AS of the ILC it is necessary that the graph
G∗ contains a spanning tree with the virtual leader as root.
Proof. It is easy to see that (15) only holds if λi (cid:54)= 0. L∗ has
exactly one eigenvalue at zero if and only if the graph G∗
contains a spanning tree [12]. It is obvious that the virtual
leader must be the root node as it has indegree of 0. Therefore
(LG + B) is full rank, see (4), and equivalently, λi (cid:54)= 0.

Furthermore, using the Gershgorin circle theorem [14], the

spectrum of (LG + B) can be bounded as
0 < λi ≤ 2(N − 1) + 1.

(18)

This can be derived from the special structure of the Lapla-
cian, where N − 1 is the maximum number of incoming
edges, thus an upper bound on the row sum. The diagonal
entries can also be bounded by this number increased by
one for the possible inﬂuence of the virtual leader. From the
proof of Theorem 2, it follows that λi > 0.
Corollary 1. Under the condition stated in Theorem 2, the
multi-agent ILC algorithm (7) is asymptotically stable for all
possible communication topologies if the learning function
(8) is chosen such that

sgn(l0) = sgn(p1) and

|l0| <

2

(2N − 1)|p1| .

Remark 2. The extension of
these results to quadratic
multiple-input, multiple-output (MIMO) agent dynamics is
straightforward. Condition (15) stays the same, but is no
longer scalar as l0 and p0 are quadratic matrices.

V. COMBINATION WITH FEEDBACK

Assume there is an additional feedback term in time-
domain that can be described as a function of the relative
distances between neighbors,

uFB
i,k (t) = C(q)ei,k(t)

with ei,k(t) as in (5) and a causal feedback function

C(q) = c0 + c1q−1 + c2q−2 + . . . .

(19)

(20)

This could be a feedback controller or any other dynamic
coupling, which does not have to be known.
Theorem 3. Given an arbitrary feedback component in the
form of (19), The stability of the distributed ILC algorithm
(7) is not affected by the feedback if applied in parallel
structure, see [3] and Figure 4.

Proof. The new input signal is ui,k = uILC
i,k , where
i,k represents the ILC input described in section IV. Using
uILC
again the lifted-system representation and inserting into (9)
yields

i,k + uFB

yi,k =P(cid:0)uILC

i,k + Cei,k + C0ei(0)(cid:1) + di,k,

(21)

ILC

Memory

ei

Memory

L(q)

C(q)

uILC

i

uFB

i

ui

P(q)

yi

Fig. 4.
Time-domain feedback control combined with iteration-domain
ILC for vehicle vi. The feedback term C(q) computes updates in every
time step, while the ILC part computes updates after each iteration.

with

C =



0
c0
c1
...
cT−2

0
0
c0
...
cT−3

. . .
. . .
. . .
...
. . .

0
0
0

...
c0

 , C0 =

0
0
0

...

0

 .



c0
c1
c2
...
cT−1

The different sample times in the deﬁnitions of ei,k and
ui,k cause the additional initial condition term C0ei(0) in
(21) and the subdiagonal shift in C. Analogously to before,
the single-agent signals can be collected into the multi-agent
form
Yk =(IN ⊗ P)

k − (IN ⊗ C)(cid:0)(LG + B) ⊗ IT
k −(cid:0)(LG + B) ⊗ PC(cid:1)Yk + δ

UILC
=(IN ⊗ P)UILC

=(cid:0)IN + (LG + B) ⊗ PC(cid:1)−1

(IN ⊗ P)UILC

(cid:1)(cid:17)

Yk + δ

(cid:16)

(22)

k + δ,

UILC

k+1 =UILC

where all iteration-invariant terms are gathered in δ. Invert-
ibility is guaranteed as we will see later. Inserting (22) into
(12) leads to

(cid:1)

k − (IN ⊗ L)(cid:0)(LG + B) ⊗ IT
(cid:0)IN + (LG + B) ⊗ PC(cid:1)−1
(cid:18)
IN T −(cid:16)(cid:0)(LG + B) ⊗ L(cid:1)
(cid:0)IN + (LG + B) ⊗ PC(cid:1)−1

(IN ⊗ P)

=

(cid:17)(cid:19)

(IN ⊗ P)UILC

k + δ

=M UILC

k + δ.

UILC

k + δ

(23)

VI. QUADROTOR EXPERIMENTS

To verify the practicability and effectiveness of the theo-
retically motivated multi-agent learning framework, we im-
plemented the proposed algorithm on a group of quadrotors.
The vehicle we used is the Parrot AR.Drone 2.0 which comes
with a blackbox onboard controller. Its inputs are the desired
roll, φdes, and pitch, θdes, Euler angles, the desired turn
rate around the vehicle’s vertical axis, ωz, and the desired
velocity, ˙zdes, in global z-direction. Commands can be sent
at a frequency f = 66, 67Hz. The state estimation is provided
by a central overhead motion capture camera system. As the
camera system and an appropriate state estimator provide
all necessary position, velocity and rotation information, an
exact
linearization can be applied [15], and
the resulting quadrotor dynamics can be approximated by
continuous-time double integrators decoupled for x- and y-
direction. Discretization using the Taylor series expansion
with time constant ∆t = 1

input-output

f = 0.015s leads to

(cid:20)1
yi,k(t) =(cid:2)1

0

xi,k(t + 1) =

(cid:20) 1

(cid:21)

xi,k(t) +

2 0.0152
0.015

(cid:21)
0(cid:3) xi,k(t).

0.015

1

ui,k(t − τ )
(25)

We can now compute the critical parameter of the system
2 0.0152. Based on (25), we choose an
dynamics p1 = 1
underlying consensus feedback controller

uFB
i

(t) =

2η
τc

˙ei,k(t) +

1
τ 2
c

ei,k(t),

(26)

with error function ei,k(t) as deﬁned in (5). With positive
controller gains, η and τc, this controller guarantees asymp-
totic stability for double-integrator agents under the condition
that the communication graph contains a spanning tree. As
this paper does not focus on the stability of consensus feed-
back controllers, we refer to [14] for further explanations.
Approximating the velocity by the difference quotient of the
position, the controller can be discretized and written in the
form described in (19).

For the iterative learning, a PD-type input update rule is

applied,

uILC
i,k+1(t) =uILC

i,k (t) + kpei,k(t + r − 1)
+ kd

ei,k(t + r) − ei,k(t + r − 2)

2∆t

,

(27)

As stability is determined by the spectral radius ρ(M ), we
must investigate the eigenvalues of this matrix. The similarity
transformation (16) can be applied to get
ρ(M ) = ρ(IN T − (J ⊗ L)(IN + J ⊗ PC)−1(IN ⊗ P)).
(24)

with learning gains, kp and kd, step size ∆t, and relative
degree r. The central difference quotient is used for better
noise suppression [16]. This is a special case of (7) with

L(q) =

kd
2∆t

(cid:124)(cid:123)(cid:122)(cid:125)

l0

+kpq−1 − kd
2∆t

q−2.

(28)

As all matrices are in lower triangular form, the eigenvalues
are the diagonal entries. As all diagonal entries of C are 0,
it can be seen that the diagonal entries of (IN + J ⊗ PC) are
all 1. This allows us to invert the matrix and the diagonal
entries of (IN + J ⊗ PC)−1 are also all 1. Finally, we end
up with the same condition as in Theorem 1.

To determine the relative degree of the real vehicles, several
effects must be taken into account
including underlying
dynamics from the onboard controller and motors that were
neglected in the modeling, and system time delays mainly
due to the wireless communication between the computer and
the vehicle. Since these effects are difﬁcult to measure, we

Fig. 5. Trajectories over time in x- and y-direction for the ILC algorithm
with underlying consensus feedback. Vehicle 1 (blue) and vehicle 2 (red)
learn to follow the desired reference trajectory (dashed black). Highlighted
are the ﬁrst (dash-dotted) and the third (dotted) iteration, and the mean over
iterations 12-20 (solid).

Fig. 6.
Trajectories in the workspace (y over x) for the ILC algorithm
with underlying consensus feedback. Vehicle 1 (blue) and vehicle 2 (red)
signiﬁcantly improve their performance with respect to the desired reference
trajectory (dashed black). Highlighted are the ﬁrst (dash-dotted) and the third
(dotted) iteration, and the mean over iterations 12-20 (solid).

identiﬁed the relative degree experimentally. The communi-
cation delays can also destabilize the closed-loop system with
the feedback controller (26), if the graph contains cycles, see
[17] or [18].

The team consists of two quadrotors with agent v1 getting
information from the virtual leader, and agent v2 only from
agent v1. Due to space and wireless communication limita-
tions, it was not possible to include more agents in the current
experimental setup. However, simulations veriﬁed that the
presented theoretic results work as expected even for larger
teams and more complex graphs. We chose the following
setup: controller parameters η = 0.707 and τc = 1.7,
ILC learning gains kp = 0.35 and kd = 17.3, ILC time
shift r = 49. Assuming the time shift matches the relative
degree and with the eigenvalues of the corresponding graph
Laplacian λ1,2 = 1, we can see that (15) holds and thus
asymptotic stability is guaranteed.

Figures 5-9 show the experimental results for the ILC
with the underlying consensus feedback controller over 20
iterations. Both quadrotors were ﬂown at the same time in
a given formation with a ﬁxed distance apart. For the plots,
the distance offset was subtracted. We repeated the whole
learning experiment ten times and show the average error
convergence and standard deviation in Figures 8 and 9.

Figure 5 shows the position trajectories over time. It can be
seen that in the ﬁrst iteration, where the ILC input is zero, the
ﬁrst vehicle (in blue) is delayed and shows lower amplitudes.
As the second vehicle (in red) only follows the ﬁrst one,
its performance is even worse. But after some iterations
(see solid lines), both drones learn to track the reference.
In Figure 6, the corresponding workspace trajectories are
depicted. Note that the goal was not to track this D-shaped
trajectory but to follow the timed reference signal in Figure 5
separately for x and y. The performance of both vehicles

improves signiﬁcantly over iterations. However, it can be
seen that agent v2 learns slower as it has no access to
the desired trajectory. Figure 7 shows the corresponding
input trajectories. For space reasons only the x-direction is
plotted. It can be seen that the ILC input is zero and the
consensus feedback component dominates. Whereas after
convergence is reached, the feedback input is nearly zero
and mainly compensates for non-repetitive errors, while the
ILC feedforward input compensates for the repetitive error.
Comparing the converged ILC input with the initial, purely
feedback-based input shows that ILC causes larger input
magnitudes with peaks being time-shifted to the left. Instead
of being reactive, the ILC is proactive and sends aggressive
input signals that keep the vehicle on track.

The learning performance can be deduced from the con-
vergence of the errors (5) over iterations shown in Figure 8.
The error of agent v2 is computed relative to agent v1; that is,
it describes the formation error. It increases ﬁrst as vehicle v1
learns faster than vehicle v2, leading to a larger relative error.
Accordingly, not only disturbances on the second agent but
also on the ﬁrst lead to larger relative errors. This explains
the slightly higher error of agent v2 after convergence is
reached.

For comparison, we did the same learning experiments
with the consensus feedback controller (26) disabled. That is,
each vehicle’s position feedback controller is based only on
the vehicle’s own tracking error. We focus on the different be-
havior after iteration twelve, where convergence is achieved.
In Figure 9, a comparison of the mean values of the position
errors and the corresponding standard deviations is shown. It
can be seen that the values for agent v1 are almost the same
with and without the consensus feedback controller. But the
error of agent v2, which can be interpreted as the formation
error, and its standard deviation decrease signiﬁcantly if the

Fig. 7.
Input trajectories in x-direction for vehicle 1 (top) and vehicle
2 (bottom). The ILC-generated feedforward input (black) increases over
iterations to compensate for the learned repetitive disturbances, while the
input from the consensus feedback controller (green) decreases and only
accounts for non-repetitive disturbances at the end. Highlighted are the ﬁrst
(dash-dotted) and the third (dotted) iteration and the mean over iterations
12-20 (solid).

Fig. 8. Error convergence plots over iterations for the ILC algorithm with
feedback. The error is computed as 1
with
T
ei,k(t) as in (5) for x- and y-direction respectively. The blue and red lines
are the mean errors for agent v1 and agent v2 over ten experiments. The
bars denote the standard deviations.

i,k(t)| + |ey

(cid:80)

T

(cid:16)|ex

i,k(t)|(cid:17)

consensus controller is enabled. The relative error decreases
by 31% and its standard deviation by 66% with the consensus
controller enabled. As a result, this distributed feedback has
a positive impact on the performance of formation ﬂying as
it accounts for non-repetitive, relative disturbances.

VII. CONCLUSIONS

We developed a distributed ILC algorithm for multi-
agent systems, which allows for arbitrary, linear and causal,
learning functions. As a result, we were able to consider
a PD-type input update rule, which leads to a signiﬁcantly
better overall tracking performance and was a prerequisite for
the quadrotor experiments. A D-type ILC algorithm, which is
stable but cannot compensate for position disturbances, could
not have been implemented as it leads to collisions with walls
and/or other agents. We derived a simple scalar condition for
stability of the proposed learning algorithm in theory, but to

Fig. 9. Comparison of position errors for the ILC experiments with and
without the distributed, consensus feedback controller (26). The blue bars
denote agent v1, the red ones agent v2. The light bars are the mean values
of the error after convergence (iterations 12-20) over ten experiments, the
dark bars the corresponding standard deviations, see Figure 8. The relative
error between vehicle 1 and 2 (see red bars) signiﬁcantly improves when
adding the consensus controller.

achieve a good learning performance in practice, parameter
tuning in simulations and experiment was necessary.

As ILC only compensates for repetitive disturbances, we
included a consensus-based feedback controller to increase
robustness against non-repetitive disturbances and noise.
That this feedback controller does not affect stability of the
ILC algorithm was proven theoretically. Moreover, it was
shown that the same holds for any dynamic coupling between
neighboring agents. Experimental results showed that the
proposed, distributed ILC algorithm achieves better reference
tracking and lower formation error for a team of quadrotors.
Furthermore, the distributed, consensus feedback controller
decreases the inﬂuence of non-repetitive disturbances and
thus leads to a better overall formation tracking performance.

REFERENCES

[1] W. Ren, R. W. Beard, and E. M. Atkins, “A survey of consensus
the American

problems in multi-agent coordination,” in Proc. of
Control Conference (ACC), pp. 1859–1864, 2005.

[2] S. Arimoto, S. Kawamura, and F. Miyazaki, “Bettering operation of
robots by learning,” Journal of Robotic Systems, vol. 1, no. 2, pp. 123–
140, 1984.

[3] D. A. Bristow, M. Tharayil, and A. G. Alleyne, “A survey of iterative
learning control,” Control Systems, IEEE, vol. 26, no. 3, pp. 96–114,
2006.

[4] A. P. Schoellig, F. L. Mueller, and R. D’Andrea, “Optimization-
based iterative learning for precise quadrocopter trajectory tracking,”
Autonomous Robots, vol. 33, no. 1-2, pp. 103–127, 2012.

[5] H.-S. Ahn and Y. Chen, “Iterative learning control for multi-agent
formation,” in Proc. of ICROS-SICE International Joint Conference,
pp. 3111–3116, 2009.

[6] S. Yang, J.-X. Xu, and D. Huang, “Iterative learning control for multi-
agent systems consensus tracking,” in Proc. of the IEEE Conference
on Decision and Control (CDC), pp. 4672–4677, 2012.

[7] D. Meng, Y. Jia, J. Du, and F. Yu, “Tracking control over a ﬁnite inter-
val for multi-agent systems with a time-varying reference trajectory,”
Systems and Control Letters, vol. 61, no. 7, pp. 807–818, 2012.

[8] Y. Liu and Y. Jia, “An iterative learning approach to formation control
of multi-agent systems,” Systems & Control Letters, vol. 61, no. 1,
pp. 148–154, 2012.

[9] J. Li and J. Li, “Adaptive iterative learning control for coordination
of second-order multi-agent systems,” International Journal of Robust
and Nonlinear Control, vol. 24, pp. 3282–3299, 2014.

[10] D. Meng, Y. Jia, and J. Du, “Robust iterative learning protocols for
ﬁnite-time consensus of multi-agent systems with interval uncertain
topologies,” International Journal of Systems Science, vol. 46, no. 5,
pp. 857–871, 2015.

[11] S. Yang, J.-X. Xu, D. Huang, and Y. Tan, “Optimal iterative learning
control design for multi-agent systems consensus tracking,” Systems
& Control Letters, vol. 69, pp. 80–89, 2014.

[12] M. Mesbahi and M. Egerstedt, Graph theoretic methods in multiagent

networks. Princeton: Princeton University Press, 2010.

[13] M. Norrl¨of and S. Gunnarsson, “Time and frequency domain conver-
gence properties in iterative learning control,” International Journal
of Control, vol. 75, no. 14, pp. 1114–1126, 2002.

[14] W. Ren and R. W. Beard, Distributed consensus in multi-vehicle

cooperative control. Springer, 2008.

[15] Q. L. Zhou, Y. Zhang, C. A. Rabbath, and D. Theilliol, “Design of
feedback linearization control and reconﬁgurable control allocation
with application to a quadrotor UAV,” in Proc. of the IEEE Conference
on Control and Fault Tolerant Systems, pp. 371–376, 2010.

[16] Y. Chen and K. L. Moore, “An optimal design of PD-type iterative
learning control with monotonic convergence,” in Proc. of the IEEE
International Symposium on Intelligent Control, pp. 55–60, 2002.

[17] U. M¨unz, A. Papachristodoulou, and F. Allg¨ower, “Delay-dependent
rendezvous and ﬂocking of large scale multi-agent systems with
communication delays,” in Proc. of the IEEE Conference on Decision
and Control (CDC), pp. 2038–2043, 2008.

[18] J. Hu and Y. Lin, “Consensus control for multi-agent systems with
double-integrator dynamics and time delays,” IET Control Theory &
Applications, vol. 4, no. 1, pp. 109–118, 2010.

