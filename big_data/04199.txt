Causal Consistency: Beyond Memory

Matthieu Perrin

Achour Mostefaoui

Claude Jard

LINA – University of Nantes, Nantes, France

[ﬁrstname.lastname]@univ-nantes.fr

6
1
0
2

 
r
a

 

M
4
1

 
 
]

C
D
.
s
c
[
 
 

1
v
9
9
1
4
0

.

3
0
6
1
:
v
i
X
r
a

Abstract
In distributed systems where strong consistency is costly
when not impossible, causal consistency provides a valu-
able abstraction to represent program executions as par-
tial orders. In addition to the sequential program order of
each computing entity, causal order also contains the seman-
tic links between the events that affect the shared objects –
messages emission and reception in a communication chan-
nel, reads and writes on a shared register. Usual approaches
based on semantic links are very difﬁcult to adapt to other
data types such as queues or counters because they require
a speciﬁc analysis of causal dependencies for each data type.
This paper presents a new approach to deﬁne causal consis-
tency for any abstract data type based on sequential speciﬁ-
cations. It explores, formalizes and studies the differences be-
tween three variations of causal consistency and highlights
them in the light of PRAM, eventual consistency and se-
quential consistency: weak causal consistency, that captures
the notion of causality preservation when focusing on con-
vergence; causal convergence that mixes weak causal con-
sistency and convergence; and causal consistency, that coin-
cides with causal memory when applied to shared memory.

Categories and Subject Descriptors E.1 [data structures]:
distributed data structures

Keywords Causal consistency, Consistency criteria, Pipelined
consistency, Sequential consistency, Shared objects, Weak
causal consistency.

Introduction

1.
Overview. Distributed systems are often viewed as more
difﬁcult to program than sequential systems because they re-
quire to solve many issues related to communication. Shared
objects, that can be accessed concurrently by the processes
of the system, can be used as a practical abstraction of com-
munication to let processes enjoy a more general view of the
system and tend to meet the classical paradigm of parallel
programming. A precise speciﬁcation of these objects is es-
sential to ensure their adoption as well as the reliability is-
sues of distributed systems. The same reasoning also holds
for parallel and multicore processors [5, 21].

Permission to make digital or hard copies of part or all of this work for personal or classroom use is
granted without fee provided that copies are not made or distributed for proﬁt or commercial advantage
and that copies bear this notice and the full citation on the ﬁrst page. Copyrights for components of this
work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise,
to republish, to post on servers, or to redistribute to lists, contact the Owner/Author. Request permissions
from permissions@acm.org or Publications Dept., ACM, Inc., fax +1 (212) 869-0481. Copyright 2016 held by
Owner/Author. Publication Rights Licensed to ACM.

PPoPP’16 March 12-16, 2016, Barcelona, Spain
Copyright c(cid:13) 2016 ACM 978-1-4503-4092-2/16/03. . . $15.00
DOI: http://dx.doi.org/10.1145/2851141.2851170

Many models have been proposed to specify shared mem-
ory [1]. Linearizability [13] and sequential consistency [15]
guarantee that all the operations appear totally ordered, and
that this order is compatible with the program order, the order
in which each process performs its own operations. These
strong consistency criteria are very expensive to implement
in message-passing systems. In terms of time, the duration
of either the read or the write operations has to be linear
with the latency of the network for sequential consistency
[16] and for all kind of operations in the case of linearizabil-
ity [3]. Concerning fault-tolerance, strong hypotheses must
be respected by the system: it is impossible to resist parti-
tioning (CAP Theorem) [9].

In order to gain in efﬁciency, researchers explored weak
consistency criteria, especially for parallel machine in the
nineties like PRAM [16] and causal memory [2] that are the
best documented. PRAM ensures that each process individ-
ually sees a consistent local history, that respects the order
in which the other processes performed their own writes
on a register (small piece of memory). Recently, we have
seen a resurgence of interest for this topic due to the devel-
opment of multicore processors [12] and cloud computing
(large modern distributed systems such as Amazon’s cloud,
data centers [25]) from the one side and the necessity to cir-
cumvent the CAP impossibility result and to ensure high ef-
ﬁciency. Among recent works, CRDT distributed data types
[22] and a parallel speciﬁcation of eventual consistency [6].
Eventual consistency ensures that all the processes will even-
tually reach a common state when they stop writing.

Causality models a distributed execution as a partial or-
der. It was ﬁrst deﬁned for message-passing systems, on
top of Lamport’s happens-before relation [14], a partial order
that contains the sequential program order of each process,
and in which a message emission happens-before its recep-
tion. Causal reception [4, 20] ensures that, if the emission of
two messages sent to the same process are related by the
happens-before relation, their reception will happen in the
same order. In shared memory models, the exchange of infor-
mation is ensured by shared registers where processes write
and read information. Causal memory [2] aims at building
a causal order, that also contains the sequential program or-
der of the processes, but in which the emission and reception
of messages are replaced by read and write operations on
shared registers. The guarantees that are ensured by causal
consistency have been identiﬁed as four session guarantees by
Terry et. al in [24]. Read your writes ensures that a read cannot
return a value older than a value written earlier by the same
process. Monotonic writes ensures that, if a process writes into
two different registers, and another process reads the second
write, then this process must read a value at least as recent
as the ﬁrst write in the other register. Monotonic reads ensures
that, if a process writes twice the same registers, and another

Eventual consistency

Causal convergence

EC

C C v

Weak causal
consistency W C C

SC Sequential
consistency

P C

C C

Pipelined consistency

Causal consistency

Figure 1: Relative strength of causality criteria

process reads the second writes, then this process can never
read the ﬁrst value again in the future. Finally, writes follow
reads ensures that, if a process reads a value in a register and
then writes a value in another register, another process can-
not read the lastly written value and a value older than the
value read by the ﬁrst process.

Motivation. On the one hand, strong consistency criteria
(linearizability and sequential consistency) are costly both
in time and space and turn out to be impossible to imple-
ment in some systems. On the other hand, there are many
weak consistency criteria (e.g. eventual consistency, PRAM
consistency, causal consistency) that can be implemented in
any distributed system where communication is possible and
with a time complexity that does not depend on commu-
nication delays. The natural question is then ”what is the
strongest consistency criterion” that enjoys such property?
Unfortunately, it has been proven in [19] that when wait-free
distributed systems are considered (all but one process may
crash), PRAM consistency (or, a fortiori, causal consistency)
and eventual consistency cannot be provided together. This
means that there are at least two separate branches in the hi-
erarchy of weak consistency criteria: one that contains PRAM
and causal consistency and the other that contains eventual
consistency. As a consequence, a question arises ”what are
the causality properties that can be provided together with
eventual consistency?”.

Causal consistency, as is known up till now, is only de-
ﬁned for memory (registers) and it has been deﬁned assum-
ing it is implemented using messages. Indeed, memory is a
good abstraction in sequential programming models. Things
are more complicated for distributed computing because of
race conditions: complex concurrent editing can often lead
to inconsistent states. Critical sections and transactions offer
generic solutions to this problem, but at a high cost: they re-
duce parallelism and fault-tolerance and may lead to dead-
locks. Another approach is to design the shared objects di-
rectly, without using shared memory. For example, in the
context of collaborative editing, the CCI model [23] requires
convergence, causality and intention preservation. In this
model, causality preservation is weaker than causal consis-
tency as deﬁned for memory and can be provided together
with eventual consistency.

The deﬁnition of causal memory is based on a semantic
matching between the reads and the writes. In particular, a
read operation depends only on the last write operation. For
other abstract data types (e.g. graphs, counters or queues) the
value returned by a query does not depend on one particular
update, but on all or part of the updates that happened before

it. Moreover, for a queue, the order of these updates is impor-
tant. It would be interesting to deﬁne causal consistency for
any object having a sequential speciﬁcation independently
from any implementation mechanism.

Contributions of the paper. As said above, this paper aims
at extending the deﬁnition of causal consistency to all ab-
stract data types. To this end, it introduces a clear distinc-
tion between two facets that are necessary to fully specify
shared objects: a sequential speciﬁcation using state transi-
tion automata, and a consistency criterion, that deﬁnes a link
between (distributed) histories and sequential speciﬁcations.
Its main contribution is the formal deﬁnition of three varia-
tions of causal consistency that can be provided in any dis-
tributed system independently from communication delays
(network latency) meaning that an operation returns without
waiting any contribution from other processes. These criteria
complete and help to better understand the map (Fig. 1) that
gives an overview of the relative strength of consistency cri-
teria. An arrow from a criterion C1 to a criterion C2 in Fig. 1
denotes the fact that C2 is stronger that C1.

• Weak causal consistency. It can be seen as the causal com-
mon denominator between the two branches and can be
associated with any weak consistency criteria to form a
new criterion that can be implemented in a wait-free dis-
tributed system.

• Causal convergence. It is the combination of eventual
consistency and weak causal consistency. This can be a
candidate to replace eventual consistency.

• Causal consistency. When applied to registers, it matches

the deﬁnition of causal memory.

In order to illustrate the notions presented in this paper, a
data structure called window stream of size k is introduced.
This data structure allows to capture the diversity of data
structures thanks to the parameter k.

The remainder of this paper is organized as follows. Sec-
tion 2 presents a formalization of abstract data types as well
as the notion of consistency criteria. Section 3 deﬁnes and il-
lustrates weak causal consistency. Section 4 deﬁnes and illus-
trates causal consistency and compares it to causal memory.
Section 5 deﬁnes and illustrates causal convergence. Section
6 discusses the implementation of causal consistency and
causal convergence in asynchronous message-passing dis-
tributed systems where crashes may occur. Finally, Section
7 concludes the paper.

2. Specifying shared objects

Shared objects can be speciﬁed by two complementary facets:
an abstract data type that has a sequential speciﬁcation, de-
ﬁned in this paper by a transition system that characterizes
the sequential histories allowed for this object and a con-
sistency criterion that makes the link between the sequen-
tial speciﬁcations and the distributed executions that invoke
them, by a characterization of the histories that are admissi-
ble for a program that uses the objects, depending on their
type. As shared objects are implemented in a distributed sys-
tem, typically using replication, the events in a distributed
history are partially ordered.

2.1 Abstract data types

To our knowledge, the only attempt to deﬁne weakly consis-
tent objects for arbitrary abstract data types is based on paral-
lel speciﬁcations [6], in which the state accessed by a process
at each operation is deﬁned by a function on the operations in
its past, ordered by a visibility and an arbitration relations. The
ﬁrst limit of this approach is that parallel speciﬁcations are,
by design, only suitable to express strong eventually consis-
tent objects, in which two processes must see the same state
as soon as they have received the same updates. The second
limit is that parallel speciﬁcations require to specify a state
for any possible partial order of events, which leads to spec-
iﬁcations as complex as the programs they specify. Conse-
quently, they are non-intuitive and error-prone as they can-
not rely on the well-studied and understood notions of ab-
stract states and transitions.

We use transition systems to specify sequential abstract
data types. We model abstract data types as transducers, very
close to Mealy machines [17], except that we do not restrict
the analysis to ﬁnite state systems. The input alphabet Σi
consists of the set of the methods available on the data type.
Each method can have two effects. On the one hand, they
can have a side effect that usually affects all processes. In
the transition system, it corresponds to a transition between
abstract states formalized by the transition function δ. On the
other hand, they can return a value from the output alphabet
Σo depending on the abstract state and the output function
λ. Both the transition and the output functions must be total,
as shared objects evolve according to external calls to their
operations, to which they must respond in all circumstances.
For example, the pop method from a stack deletes the head of
the stack (the side effect) and returns its value (the output).
More formally, abstract data types correspond to Def. 1.

Deﬁnition 1. An abstract data type (ADT) is a 6-tuple
T = (Σi, Σo, Q, q0, δ, λ) such that:

• Σi and Σo are countable sets called input and output alpha-
bets. An operation is an element (σi, σo) of Σi × Σo, denoted
by σi/σo;

• Q is a countable set of states and q0 ∈ Q is the initial state;

• δ : Q × Σi → Q and λ : Q × Σi → Σo are the transition and

output functions.

We distinguish two kinds of operations depending on
their behavior: updates and queries. An input σi is an update
if the transition part is not always a loop, i.e. there is a
state q such that δ(q, σi) 6= q. It is a query if the output
depends on the state, i.e. there are two states q and q′ such
that λ(q, σi) 6= λ(q′, σi). Some operations are both update

and query. For example, the pop operation in a stack deletes
the ﬁrst element (the update part) and returns its value (the
query part). An operation that is not an update (resp. query)
is called a pure query (resp. pure update).

Sequential speciﬁcation. We now deﬁne the sequential
speciﬁcation L(T ) of an ADT T . A sequential speciﬁcation
is a set of sequences of operations that label paths in the
transition system, starting from the initial state. We need to
take into account two additional features in our model: pre-
ﬁxation and hidden operations (Def. 2).

• We need to take into consideration both ﬁnite and inﬁnite
sequences. To do so, we ﬁrst deﬁne inﬁnite sequences
recognized by T , and we then extend the concept to the
ﬁnite preﬁxes of these sequences.

• In weak consistency criteria deﬁned on memory, and es-
pecially in causal memory, reads and writes usually play
a different role. To extend these concepts to generic ADTs
where some operations are both an update and a query,
we need a way to express the fact that the side effect of an
operation must be taken into account, but not its return
value. To do so, we introduce the notion of hidden opera-
tions, in which the method called is known, but not the
returned value. Thus, sequential histories admissible for
T are sequences of elements of Σ = (Σi × Σo) ∪ Σi: each
element of Σ is either an operation σi/σo ∈ (Σi × Σo) or
a hidden operation σi ∈ Σi.

Deﬁnition 2. Let T = (Σi, Σo, Q, q0, δ, λ) be an abstract data
type.

i /σk

An inﬁnite sequence (σk

o )k∈N of operations is recognized
by T if there exists an inﬁnite sequence of states (qk)k∈N such that
q0 = q0 is the initial state and for all k ∈ N, δ(qk, σk
i ) = qk+1
and λ(qk, σk

i ) = σk
o .

A ﬁnite or inﬁnite sequence u = (uk)k∈D where D is either N
or {0, ..., |u|−1} is a sequential history admissible for T if there
exists an inﬁnite sequence (σk
o )k∈N of operations recognized by
T such that, for all k ∈ D, uk = σk

o or uk = σk
i .

i /σk

i /σk

The set of all sequential histories admissible for T , denoted by

L(T ), is called the sequential speciﬁcation of T .

Window stream data type. Causal consistency has been de-
ﬁned only for memory. The memory abstract data type is
very restrictive as a write on a register erases the complete
past of all the previously written values on that register. For
more complex objects like stacks and queues, the value re-
turned by a query may depend on more than one update op-
eration, and the order in which the different updates were
done is important. To illustrate our work on consistency cri-
teria, we need, as a guideline example, a data type with a
simple speciﬁcation and whose behaviour shows all these
features.

We thus introduce the window stream data type. In short, it
can be seen as a generalization of a register in the sense that
the read operation returns the sequence of the last written
values instead of the very last one. A window stream of size
k (noted Wk) can be accessed by a write operation w(v) where
v ∈ N is the written value and a read operation r that returns
the sequence of the last k written values. Missing values
are replaced by the default value 0 (any different default
value can be considered). More formally, a window stream
corresponds to the ADT given in Def. 3.

Deﬁnition 3. An integer window stream of size k (k ∈ N) is
an ADT Wk = (Σi, Σo, Q, q0, δ, λ) with Σi = ∪v∈N{r, w(v)},
Σo = Nk ∪ {⊥}, Q = Nk, q0 = (0, ..., 0) and, for all v ∈ N and
q = (q1, ..., qk) ∈ Q, δ(q, w(v)) = (q2, ..., qk, v), δ(q, r) = q,
λ(q, w(v)) = ⊥ and λ(q, r) = q.

The window stream data type has also a great interest in
the classiﬁcation of synchronization objects. The notion of
consensus number has been introduced in [11] to rank the
synchronization power of objects. An object has a consensus
number equal to c if it allows to reach consensus among c
processes and not among c + 1 processes. Recall that a con-
sensus object can be invoked by a given number of processes.
Each process invokes it once with a proposed value and gets
a return value such that the returned value has been pro-
posed by some process and all invoking processes obtain the
same return value. An object has a consensus number of c if
it can emulate a consensus object that can be invoked by at
most c processes. As an example the consensus number of a
register and a stack are respectively 1 and 2 while the well-
known synchronization object compare-and-swap allows to
reach consensus among any number of processes. It is inter-
esting to note that a window stream of size k has a consen-
sus number of k: if k processes write their proposed values
in a sequentially consistent window stream and then return
the oldest written value (different from the default value),
they will all return the same value. Consequently, a window
stream of size at least 2 cannot be implemented using any
number of registers (window streams of size 1).

Additional examples with queues and the complete deﬁ-

nition of the memory ADT are given in section 4.

2.2 Distributed histories

During the execution of a distributed program, the partic-
ipants/processes call methods on shared objects (registers,
stacks, queues, etc.), an object being an instance of an abstract
data type. An event is the execution of a method by a process.
Thereby, each event is labelled by an operation from a set Σ,
that usually contains the same symbols as the alphabet of the
sequential speciﬁcation L(T ).

In a distributed system composed of communicating se-
quential processes, all the events produced by one process
are totally ordered according to the program order, while two
events produced by different processes may be incompara-
ble according to the program order. In this model, the par-
tially ordered set of events is a collection of disjoint maxi-
mal chains: each maximal chain of the history corresponds
to the events of one process. We identify the processes and
the events they produce, calling a maximal chain in the his-
tory a ”process”.

Parallel sequences of maximal chains are a too restrictive
model to encode the complex behaviour of many distributed
systems, such as multithreaded programs in which threads
can fork and join, Web services orchestrations, sensor net-
works, etc. Instead, we allow the program order to be any
partial order in which all events have a ﬁnite past. In this
general model, an event can be contained is several maximal
chains. This causes no problems in our deﬁnitions.

Deﬁnition 4. A distributed history (or simply history) is a 4-tuple
H = (Σ, E, Λ, 7→) such that: Σ is a countable sets of operations
in the form σi/σo or σi; E is a countable set of events (denoted
by EH for any history H); Λ : E → Σ is a labelling function;
7→ ⊂ (E × E) is a partial order called program order, such that
each event e ∈ E has a ﬁnite past {e′ ∈ E : e′ 7→ e}.

Let H = (Σ, E, Λ, 7→) be a distributed history. Let us

introduce a few notations.

• PH denotes the set of the maximal chains of H, i.e. max-
imal totally-ordered sets of events. In the case of sequen-
tial processes, each p ∈ PH corresponds to the events
produced by a process. In the remainder of this article,
we use the term ”process” to designate such a chain, even
in models that are not based on a collection of communi-
cating sequential processes.

• A linearization of H is a sequential history that contains
the events of H in an order consistent with the program
order. More precisely, it is a word Λ(e0) . . . Λ(ei) . . . such
that {e0, . . . , ei, . . .} = EH and for all i < j, ej 6
7→ ei.
lin(H) denotes the set of all linearizations of H.

• We also deﬁne a projection operator p that removes
part of the information of the history. For E ′, E′′ ⊂ E,
H.π(E′, E′′) only keeps the operations that are in E ′,
and hides the output of the events that are not in E ′′:
H.π(E′, E′′) = (Σ, E ′, Λ′, 7→ ∩ E′2) with
Λ′(e) = σi if Λ(e) = σi/σo and e 6∈ E ′′
Λ′(e) = Λ(e) otherwise.

Considering memory, H.π(E′, E′′) contains the writes of
E′ and the reads of E′ ∩ E′′.

• Finally, we deﬁne a projection on the histories to replace
the program order by another order →: if → respects the
deﬁnition of a program order (i.e. all events have a ﬁnite
past in →), H → = (Σ, E, Λ, →) is the history that contains
the same events as H, but ordered according to →.

Note that the discreteness of the space of the events does
not mean that the operations must return immediately, as our
model does not introduce any notion of real time.

2.3 Consistency criteria

A consistency criterion characterizes which histories are ad-
missible for a given data type. Graphically, we can imagine
a consistency criterion as a way to take a picture of the dis-
tributed histories so that they look sequential. More formally,
it is a function C that associates a set of consistent histories
C(T ) with any ADT T . An implementation of a shared ob-
ject is C-consistent for a consistency criterion C and an ADT
T if all the histories it admits are in C(T ). For the sake of clar-
ity, we will deﬁne consistency criteria by a predicate P (T, H)
that depends on an ADT T and a distributed history H. A cri-
terion is deﬁned as the function that associates to each T , the
set of all the histories H such that P (T, H) is true.

We say that a criterion C1 is stronger than a criterion C2
if for any ADT T , C1(T ) ⊂ C2(T ). A strong consistency
criterion guarantees stronger properties on the histories it
admits. Hence, a C1-consistent implementation can always
be used instead of a C2-consistent implementation of the
same abstract data type if C1 is stronger than C2. We now
deﬁne sequential consistency [15] and pipelined consistency
[16] to illustrate this formalism.

Sequential consistency. was originally deﬁned by Lamport
in [15]: the result of any execution is the same as if the operations
of all the processors were executed in some sequential order, and the
operations of each individual processor appear in this sequence in
the order speciﬁed by its program. In our formalism, such a se-
quence is a word of operations that has two properties: it is
correct with respect to the sequential speciﬁcation of the ob-

ject (i.e. it belongs to L(T )) and the total order is compatible
with the program order (i.e. it belongs to lin(H)).

Deﬁnition 5. A history H is sequentially consistent (SC) with
an ADT T if: lin(H) ∩ L(T ) 6= ∅.

Pipelined consistency. The PRAM consistency criterion
(for ”Pipelined Random Access Memory”) has been deﬁned
for shared memory [16]. In PRAM consistency, the processes
only have a partial view of the history. More precisely, they
are aware of their own reads and all the writes. PRAM con-
sistency ensures that the view of each process is consistent
with the order in which the writes were made by each pro-
cess. Each process must be able to explain the history by a
linearization of its own knowledge. Pipelined consistency is
weaker than sequential consistency, for which it is addition-
ally required that the linearizations seen by different pro-
cesses be identical. The PRAM consistency is local to each
process. As different processes can see concurrent updates in
a different order, the values of the registers do not necessarily
converge.

Pipelined consistency is an extension of PRAM consistency
to other abstract data types. As not all operations are either
pure updates or pure queries, we use the projection operator
to hide the return values (the output alphabet) of all the
events that are not made by a process. For each process p,
H.π(EH, p) is the history that contains all the events of p
unchanged, and the return values of the operations labelling
the events of the other processes are unknown. Pipelined
consistency corresponds to Def. 6.

Deﬁnition 6. H is pipelined consistent (PC) with T if:
∀p ∈ PH, lin (H.π(EH, p)) ∩ L(T ) 6= ∅.

3. Weak causal consistency
3.1 Causal orders and time zones

Causal consistency is based on the thought that a distributed
system is depicted by a partial order that represents a log-
ical time in which the processes evolve at their own pace.
This partial order, called causal order, contains the sequen-
tial arrangement imposed by the processes. Additionally, an
event cannot be totally ignored by a process forever (see Def.
7), which corresponds to the eventual reception in message-
passing systems. There are three reasons why coﬁniteness is
important in our model.

1. For inﬁnite histories, coﬁniteness usually prevents the
causal order to be the program order. If we did not im-
pose this restriction, the obtained criteria would be much
weaker, as it would not force the processes to interact
at all. Such criteria could be implemented trivially, each
process updating its own local variable. However, they
would not be so useful in distributed systems.

2. It is usually stated that causal memory is stronger than
PRAM. From Def. 6, the operation associated with each
event stands at some ﬁnite position in the linearization
required for each process. Thus, a criterion in which
processes are not required to communicate would not
strenghten pipelined consistency.

3. It is also important to ensure that causal convergence is
stronger than eventual consistency: convergence can only
be achieved when all processes have the same updates
in their causal past; to strenghten eventual consistency,
we must ensure that, if all processes stop updating then,
eventually, all processes will have all the updates in their
causal past.

Deﬁnition 7. Let H be a distributed history. A causal order is a
partial order → on all the events of EH, that contains 7→, and such
that for all e ∈ EH , {e′ ∈ EH : e 6→ e′} is ﬁnite.

In a distributed history augmented with a causal order, for
each event e, the history can be divided into six zones: the
causal (resp. program) past that contains the predecessors
of e in the causal (resp. program) order, the causal (resp.
program) future that contains the successors of e in the causal
(resp. program) order, the present that contains only e and
the concurrent present that contains the events incomparable
with e for both orders. These zones are depicted in Fig. 2. The
causal past of e is denoted by ⌊e⌋ = {e′ ∈ EH : e′ → e}.

Causal consistency aims at providing a causal order that
can be helpful for the ﬁnal user when designing an applica-
tion at a higher level. Causality is not an order imposed by
outer conditions (e.g. the network system), even if causal re-
ception can help in the implementation. Thus, the existence
of a causal order is only required, but not necessarily unique.
An illustration of this point is the fact that no communication
is required to insert pure update operations into the causal
order.

The total order of sequential consistency is a causal or-
der that veriﬁes two additional properties: (1) as the causal
order is total, the concurrent present of each operation is
empty and (2) the value returned by each operation must be
plausible with respect to the linearization of its causal past
(which is unique because of (1)). In our formalism, for all
events e ∈ EH , lin((H →).π(⌊e⌋, ⌊e⌋)) ∩ L(T ) 6= ∅, where
→ is the causal order (Fig. 2d). Note that the existence of
a causal order verifying (2) is equivalent to sequential con-
sistency for inﬁnite histories: because any concurrent events
e and e′ have events in common in their respective future,
a linearization for any of these future events must order e
and e′, so we can build a new causal order in which e and
e′ are ordered as in this linearization (the complete proof is
very close to the one for Proposition 2). As processes cannot
know their future, any algorithm implementing (2) must also
ensure (1). Different ﬂavours of causal consistency that can
be implemented in wait-free systems correspond to different
ways to weaken (2), as illustrated in Fig. 2.

The differences between the criteria introduced in this
paper are illustrated with small examples on instances of
window streams of size 2 (W2), of two kinds of queues (Q
and Q′) and of memory on Fig. 3. In these histories, the
dummy values returned by update operations are ignored
for the sake of clarity. The program order is represented
by solid arrows, and semantic causal relations are repre-
sented by dashed arrows (a read value is preceded by the
corresponding write operation, a popped value needs to
be pushed ﬁrst, etc.). For example, the history on Fig. 3d
shows two processes sharing a window stream of size 2.
The ﬁrst process ﬁrst writes 1 and then reads (0, 1), while
the second process writes 2 and then reads (1, 2). As the
word w(1)/⊥.r/(0, 1).w(2)/⊥.r/(1, 2) is in both lin(H) and
L(W2), this history is sequentially consistent.

3.2 Weak causal consistency

Weak causal consistency precludes the situation where a pro-
cess is aware of an operation done in response to another
operation, but not of the initial operation (e.g. a question and
the answer in a forum). In this scenario, the answer is a conse-
quence of the question, so the reading of the answer, that is a
consequence of the question, should also be a consequence of
the question. Weak causal consistency ensures that, when a
process performs an operation, it is aware of its whole causal

•
i /σ1
σ1
o

•
σ4
i /σ4

o

causal past

causal past

i /σ2
σ2
o
•

program past

•
i /σ1
σ1
o

•
i /σ4
σ4
o

causal past

concurrent present

causal future

causal past

concurrent present

causal future

(a) Pipelined consistency.

(b) Weak causal consistency.

concurrent present

σ6
i /σ6
•

o

causal future
σ12
i /σ12

o

•

σ9
i /σ9
•

o

causal past

concurrent present

σ2

i /σ2
•

o

σ8

i /σ8
•

o

causal future
σ12
i /σ12

o

•

σ9
i /σ9
•

o

present

program future

program past

present

program future

•
i /σ3

o

σ3

•
i /σ5

o

σ5

•
i /σ7

o

σ7

•

σ10
i /σ10

o

•
i /σ3

o

σ3

•
i /σ5

o

σ5

•
i /σ7

o

σ7

•
σ8
i /σ8

o

•

σ11
i /σ11

o

•
i /σ1

o

σ1

•
i /σ4

o

σ4

concurrent present

causal future

causal past

concurrent present

causal future

(c) Causal consistency.

(d) Sequential consistency.

causal past

i /σ2
σ2
o
•

program past

concurrent present

σ6
i /σ6
•

o

causal future
σ12
i /σ12

o

•

σ9
i /σ9
•

o

causal past

i /σ2
σ2
o
•

concurrent present

σ6
i /σ6
•

o

causal future
σ12
i /σ12

o

•

σ9
i /σ9
•

o

present

program future

program past

present

program future

•
i /σ3

o

σ3

•
i /σ5

o

σ5

•
i /σ7

o

σ7

•

σ10
i /σ10

o

•

σ11
i /σ11

o

•
i /σ3
σ3
o

•
i /σ5
σ5
o

•
i /σ1
σ1
o

•
i /σ4
σ4
o

•
i /σ7

o

σ7

•
σ8
i /σ8

o

•

σ10
i /σ10

o

•

σ11
i /σ11

o

•
σ8
i /σ8

o

•

σ10
i /σ10

o

•
σ8
i /σ8

o

•

σ11
i /σ11

o

Figure 2: The differences between causality criteria can be explained in terms of time zones. The more constraints the past
imposes on the present, the stronger the criterion. The zones in plain blue must be respected totally, and the updates of the
zones in striped orange must be taken into account.

w(1)

•

•

w(2)

r/(0, 1)

•

r/(1, 2)

•

•

r/(0, 2)

•

r/(1, 2)

w(1)

•

•

r/(0, 1)

w(2)

•

•

r/(0, 2)

w(1)

•

•

w(2)

r/(2, 1)

•

•

r/(1, 2)

w(1)

•

•

w(2)

r/(0, 1)

•

•

r/(1, 2)

(a) W2: CCv, not PC

(b) W2: PC, not WCC

(c) W2: CC, not CCv

(d) W2: SC

push(1)

pop/1

•

•

pop/1

•

•

push(3)
•

push(1)

•

•

push(2)

pop/3

push(1)

•

•

push(2)

pop/1

•

•

pop/1

pop/⊥

•

•

pop/⊥

push(1)

•

•

push(2)

hd/1

•

•

hd/1

rh(1)

•

•

rh(1)

hd/2

•

•

hd/2

rh(2)

•

•

rh(2)

(e) Q: WCC and PC, not CC

(f) Q: CC, not SC

(g) Q′: CC, not SC

wa(1)

•

wc(2)

•

wd(1)

•

•

wb(1)

•

wc(3)

•

we(1)

rb/0

•

•

ra/0

re/1

•

•

rd/1

rc/3

•

•

rc/3

wa(1)

•

wa(2)

•

wb(3)

•

•

wc(1)

•

wc(2)

•

wd(3)

rd/3

•

•

rb/3

rc/1

•

•

ra/1

wa(1)

•

•

wc/1

(h) M[a−z]: CCv but not CC

(i) M[a−z]: CM but not CC

Figure 3: Distributed histories for instances of W2, Q, Q′ and M[a−z] with different consistency criteria.

past. In terms of time zones, the value returned by each op-
eration must be consistent with regard to a linearization of
the side effect of all operations that appear in its causal past
– and only them. More formally, it corresponds to Def. 8.
Weak causal consistency roughly corresponds to the notion
of causality preservation in the CCI model [23] used in col-
laborative editing, that requires causality, convergence and
intention preservation. The difference between weak causal
consistency and causality preservation stems from the fact
that the model considered in this paper is based on sequen-
tial speciﬁcations that replaces the notion of ”intention” of
the CCI model.

Deﬁnition 8. A history H is weakly causally consistent
(WCC) for an ADT T if there exists a causal order →, such that:
∀e ∈ EH, lin((H →).π(⌊e⌋, {e})) ∩ L(T ) 6= ∅.

In the history on Fig. 3b, the operation read r/(0, 1) must
have w(1)/⊥ in its his causal history for the execution to
be weak causally consistent. Similarly, w(2)/⊥ → r/(2, 1).
The causal order of this history is total, so it has only one
possible linearization for the last read: w(1).r.w(2).r/(2, 1),
which does not conform to the sequential speciﬁcation, thus
the history is not weak causally consistent.

On the contrary, the history of Fig. 3a is weak causally
consistent: w(1)/⊥, w(1).w(2).r/(0, 1), w(1).w(2).r.r/(1, 2),
w(2)/⊥, w(1).w(2).r/(0, 2) and w(1).w(2).r.r/(1, 2) are cor-
rect linearizations for the six events. This history illustrates
why pipelined consistency and eventual consistency cannot
be achieved together for all objects in wait-free message-
passing systems [19] (all processes but one may crash). In a
similar execution, a sequentially consistent window stream
would verify three properties: (termination) all the opera-
tions must return; (validity) all the reads must return at least
one non-null value; (agreement) the oldest value seen by
each process must be the same. This problem is similar to
Consensus, that is impossible to solve in asynchronous dis-
tributed systems in the presence of process crashes [7]. In
pipelined consistency, for their second read, the ﬁrst process
can only return (0, 1) or (1, 2) and the second process only
(0, 2) or (2, 1); they can never converge. Pipelined consis-
tency sacriﬁces agreement to ensure termination of the ﬁrst
read, while eventual consistency relaxes termination to en-
sure agreement (the states will eventually be the same, but
we do not know when).

In wait-free distributed systems, pipelined consistency
and eventual consistency cannot be achieved together, but
weak causal consistency can be enriched with either pipelined
consistency to form causal consistency (Sec. 4) or with even-
tual consistency to form causal convergence (Sec. 5).

3.3 Behaviour in absence of data races

In [2], causal memory is justiﬁed by the context in which
it may be used. If a causal memory is never subject to race
conditions, it behaves exactly like a sequential memory. This
is actually a property of weak causal consistency : a weakly
causally consistent history that does not contain concurrent
writes is sequentially consistent (Proposition 1). Thus, for a
program in which synchronisation does not rely on memory,
a weakly causally consistent memory ensures the same qual-
ity of service as a sequentially consistent memory with a bet-
ter time efﬁciency. Indeed, concurrent writes need to be syn-
chronized to get a sequentially consistent shared memory [3],
but it is not necessary for weak causal consistency (see Sec-
tion 6).

Proposition 1. Let T be an ADT and H = (Σ, E, Λ, 7→) be a
history such that H ∈ W CC(T ) and, for all update operations
u, u′ ∈ E, u → u′ or u′ → u. Then H ∈ SC(T ).

Proof. Let T be an ADT and H = (Σ, E, Λ, 7→) ∈ W CC(T )
such that, for all update operations u, u′ ∈ E, u → u′ or
u′ → u.

Let ≤ be a total order on E that extends →, and let l be
the unique linearization of lin(H ≤). As 7→⊂→≤, l ∈ lin(H).
Suppose that l /∈ L(T ). As the transition system of T is
deterministic, there exists a ﬁnite preﬁx of l that does not
belong to L(T ). Let l′ ∈ Σ⋆ and e ∈ E such that l′ · Λ(e)
is the shortest such preﬁx. As H ∈ W CC(T ), there exists
a linearization l′′ · Λ(e) ∈ lin((H →).π(⌊e⌋, {e})) ∩ L(T ).
as e is the maximum of ⌊e⌋ according to e. Now, l′ and l′′
are composed of the same updates in the same order, as
→ is total considering only the updates, so l′ and l′′ lead
to the same state. Under these conditions, it is absurd that
l′′ · Λ(e) ∈ L(T ), l′ ∈ L(T ) and l′ · Λ(e) /∈ L(T ). It results that
l ∈ L(T ), so H ∈ SC(T ).

4. Causal consistency
4.1 Deﬁnition

Among the four session guarantees, weak causal consistency
and causal convergence ensure Read your writes, Monotonic
writes and Writes follows reads, but not Monotonic reads while
causal consistency is supposed to ensure the four session
guarantees. The difference between pipelined consistency
and weak causal consistency can be understood in terms
of the time zones illustrated on Fig. 2. On the one hand, in
pipelined consistency, the present must be consistent with
the whole program past, writes as well as reads, and the
writes of a preﬁx of the other processes, but there is no ref-
erence to a causal order (Fig. 2a). On the other hand, weak
causal consistency focuses on causal order, but only requires
consistency with the writes (Fig. 2b). Causal consistency en-
forces both weak causal consistency and pipelined consis-
tency by considering differently the program past and the
rest of the causal past: the value returned by each read must
respect a linearization containing all the writes of the causal
history and the reads of its program history (Fig. 2c). More
formally, it corresponds to Def. 9.

Deﬁnition 9. A history H is causally consistent (CC) for
an ADT T ∈ T if there exists a causal order → such that:
∀p ∈ PH, ∀e ∈ p, lin((H →).π(⌊e⌋, p)) ∩ L(T ) 6= ∅.

As causal consistency is a strengthening of both pipelined
consistency and weak causal consistency, the histories of
ﬁgures 3a and 3b are not causally consistent. On the con-
trary, the history of Fig. 3c is causally consistent: w(1)/⊥,
w(2).w(1)/⊥.r/(2, 1), w(2)/⊥ and w(2)/⊥.w(1).r/(1, 2) are
linearizations for the four events.

Causal consistency is more than the exact addition of
pipelined consistency and weak causal consistency as shown
by Fig. 3e that features a ﬁrst-in-ﬁrst-out queue. Several
kinds of queues are instantiated in this paper, so their cor-
responding ADTs are only informally described. In this his-
tory, the queue has two operations push(v) that adds an
integer value v at the end of the queue, and pop that re-
moves and returns the ﬁrst element, i.e. the oldest element
pushed and not popped yet. This history can be interpreted
for weak causal consistency: when the ﬁrst process pops for
the ﬁrst time, it is only aware of its own push, so it returns
1. When it receives the notiﬁcation for the push(2) opera-
tion, it notices that value 2 should be before value 1 in the
queue, so the ﬁrst pop should have returned 2, and the sec-
ond 1. The linearization push(2).push(1).pop.pop/1 is correct
for weak causal consistency. It is also pipelined consistent,
as push(2).pop.push(1).push(1)/⊥.pop/1.pop/1.push(3)/⊥
and push(2)/⊥.push(1).pop.pop.push(3).pop/3.push(1)/⊥
are linearizations for the two processes. Note that the 1 re-
turned by the second pop does not correspond to the same
push(1) for the two criteria. That is why, even if the history
is both pipelined consistent and weakly causally consistent,
it is not causally consistent.

The history on Fig. 3f is causally consistent: both processes
concurrently pop the queue when in same state [1, 2], so they
both get 1. Then they integrate the fact that the other pro-
cess removed the head, which they consider is the value 2; at
their next pop, the queue is empty. Weakly consistent criteria
cannot ensure that all elements inserted will be popped once
and only once even if an inﬁnity of pops are performed, but,
this example shows that causal consistency, neither guaran-
tees existence (2 is never popped) nor unicity (1 is popped
twice). The reason is that, in weak consistency criteria, the

transition and output parts of the operations are loosely cou-
pled. In Fig. 3g, the pop operation is split into a hd (head)
operation, that returns the ﬁrst element without removing it,
and a rh(v) (remove head) operation that removes the head
if and only if it is equal to v. The previous pattern also may
happen and both processes read 1 and perform rh(1). How-
ever, they do not delete 2 at the head of the queue. Using this
technique, all the values are read at least once.

The fact that causal consistency is stronger than pipelined
consistency is not trivial given the deﬁnitions: the existence
of linearizations for all the events does not directly imply the
existence of a linearization for the whole history. We prove
the following proposition, that will be useful in Section 4.2.
The fact that CC is stronger than P C is a direct corollary, as
7→ ⊂ →.

Proposition 2.
∀p ∈ PH, lin ((H →).π(EH, p)) ∩ L(T ) 6= ∅.

If H is a causally consistent history, then

Proof. Let H be causally consistent and p ∈ PH. If p is ﬁnite,
it has a biggest element e. As H is causally consistent, there
exists a linearization le ∈ lin((H →).π(⌊e⌋, p)) ∩ L(T ). As
7→⊂→, there exists a linearization l of (H →).π(EH, p) whose
le is a preﬁx. l ∈ L(T ) as le ∈ L(T ) and all the events that are
in l and not in le are hidden.

If p is inﬁnite, it is not possible to consider its last element.
Instead, we build a growing sequence (lk) of linearizations
that converges to the whole history. The successive lineariza-
tions of the events are not necessarily preﬁxes of each other,
so the linearizations we build also contain a part of the con-
current present. We number the events of p by e1 7→ e2 7→ ...
and we deﬁne, for all k, the set Lk such that l.ek ∈ Lk
if and only if it can be completed, by a word l′ such that
l.ek.l′ ∈ lin((H →).π({e ∈ EH : ek 6→ e}, p)) ∩ L(T ). In
other words, Lk contains the linearizations of the causal past
and the concurrent present of ek, truncated to ek. As Lk con-
tains the correct linearizations for causal consistency, it is
not empty. It is also ﬁnite because → is a causal order, so
EH \ {e ∈ EH : ek → e} is ﬁnite. Notice that all the lineariza-
tions in Lk+1 have a preﬁx in Lk as ek → ek+1 and L(T ) is
closed by preﬁxing.

As Lk is ﬁnite for all k and all lj ∈ Lj has a preﬁx in Lk for
j ≥ k, there is a lk that is the preﬁx of a lj for all j ≥ k. We can
build by induction a sequence (lk)k∈N of words of L(T ) such
that for all k, lk ∈ Lk and lk+1 is a preﬁx of lk. The sequence
(lk) converges to an inﬁnite word l. All the preﬁxes of l are
in L(T ), so l ∈ L(T ). Moreover, l contains all the events of
H.π(EH, p) because → is a causal order (so all events are in
the causal history of a ek for some k), and the causal order
is respected for each pair of events, because it is respected
by all the preﬁxes of l that contain those two events. Finally,
l ∈ lin ((H →).π(EH, p)) ∩ L(T ).

4.2 Causal consistency versus causal memory

Memory is a particular abstract data type; as an example,
causal memory has been deﬁned in [2]. In this section, we
compare causal consistency applied to memory and causal
memory. We ﬁrst recall the formal deﬁnitions of memory
and causal memory, then we exhibit a difference between the
two associated consistency criteria when the same value is
written twice in the same register (a register being a piece
of memory). We ﬁnally prove that, when all the values writ-
ten are different, causally consistent memory corresponds ex-
actly to causal memory.

We now deﬁne memory as an abstract data type. A mem-
ory is a pool of integer registers. As causal consistency is
not composable, it is important to deﬁne a causal memory
as a causally consistent pool of registers rather than a pool of
causally consistent registers, which is very different. An in-
teger register x is isomorphic to a window stream of size 1. It
can be accessed by a write operation wx(v), where v ∈ N is
the written value and a read operation rx that returns the last
value written, if there is one, or the default value 0 otherwise.
The integer memory MX is the collection of the integer regis-
ters of X. More formally, it corresponds to the ADT given in
Def. 10. In all the section, let MX be a memory abstract data
type.

the

integer memory

Deﬁnition 10. Let X be any set of symbolic register names. We
deﬁne
the ADT
Mx = (Σi, Σ0, Q, q0, δ, λ) with Q = NX , q0 : x 7→ 0,
Σi = {rx, wx(v) : v ∈ N, x ∈ X}, Σo = N ∪ {⊥}, and for
all x 6= y ∈ X, v ∈ N and q ∈ X → N, δ(q, wx(v))(x) = v,
δ(q, wx(v))(y) = q(y), λ(q, wx(v)) = ⊥, δ(q, rx) = q and
λ(q, rx) = q(x).

on X by

The dichotomy between causal consistency and causal
convergence also exists for memory. On Fig. 3h, assuming the
ﬁrst read of each process only has the writes of the same pro-
cess in their causal past, all the writes of the other processes
must be placed after this read. In order to satisfy causal con-
sistency, the register c must be set to 3 for the ﬁrst register
and to 2 for the second register in the end, which cannot be
reconciled with causal convergence.

Causal memory deﬁnes a causal order explicitly from the
history by considering the reads and the writes. This causal
order has the same use as the program order in pipelined
consistency. More formally, it corresponds to Def. 11.

Deﬁnition 11. A relation   is a writes-into order if:

• for all e, e′ ∈ EH such that e   e′, there are x ∈ X and v ∈ N

such that Λ(e) = wx(v) and Λ(e′) = rx/v,
  e}| ≤ 1,

• for all e ∈ EH , |{e′ ∈ EH : e′
• for all e ∈ EH such that Λ(e) = rx/v and there is no e′ ∈ EH

such that e′

  e, then v = 0.

A history H is MX -causal (CM) if there exists a writes-into order
  such that:

• there is a causal order → that contains   and 7→,
• ∀p ∈ PH, lin ((H →).π(EH, p)) ∩ L(MX ) 6= ∅.

Causal consistency and causal memory are not identical.
This comes from the fact that the writes-into order is not
unique. This weakens the role of the logical time, as the in-
tuition that a read must be bound to its corresponding write
is not always captured by the deﬁnition. Let us illustrate this
point with the history on Fig. 3i. In this history, we consider
the writes-into order in which the reads on x and z are related
to the ﬁrst write of the other process. This writes-into order
is correct, as each read is related to exactly one write, and
the registers and the values are the same. Moreover, the lin-
earizations wa(1)/⊥.wa(2)/⊥.wb(3)/⊥.wc(1).wc(2).wd(3).
rd/3.rb.ra.wc(1).rc/1.wa(1)/⊥ and
wa(1).wa(2).wb(3).
wc(1)/⊥.wc(2)/⊥.wd(3)/⊥.rb/3.rd.rc.wa(1).ra/1.wc(1)/⊥
for the two processes are correct, so this history is correct for
causal memory. However, in these linearizations, the value
read by the two last reads was not written by their prede-
cessors in the writes-into relation. If we change this relation
to restore the real data dependencies, we obtain a cycle in
the causal order. This example shows that the approach of

Def. 11, that uses the semantics of the operations, is not well
suited to deﬁne the consistency criteria.

This issue is usually solved [18] by the hypothesis that
all written values are distinct. Even if this can be achieved by
the addition of unique timestamps on the values stored in the
memory, this solution is not acceptable because it changes the
way the ﬁnal object can be used. We now prove that, under
this hypothesis, causal consistency and causal memory are
equal. It means that causal consistency solves the problem
raised above, while remaining as close as possible to the
original criterion.

Proposition 3. Let H be a distributed history. If H ∈ CC(MX ),
then H is MX -causal.

Proof. Suppose H is causally consistent. For each event e,
there exists a process pe with e ∈ pe and a linearization le ∈
lin((H →).π(⌊e⌋, pe))∩L(MX ). Note that these processes and
linearizations are not necessarily unique, but we ﬁx them for
each event now. Let us deﬁne the writes-into order   by
e   e′ if Λ(e′) = rx/v and e is the last write on x in le. As le ∈
L(MX ), Λ(e) = wx(v). e′ also has at most one antecedent by
CM−−→
 , and if it has none, then v = 0. The transitive closure
of   ∪ 7→ is a partial order contained into →. By Proposition
2, for all p ∈ PH , lin ((H →).π(EH, p)) ∩ L(MX ) 6= ∅, so H
is MX -causal.

Proposition 4. Let H be a distributed history such that, for all
e 6= e′ ∈ EH with Λ(e) = wx(v)/⊥ and Λ(e′) = wy(v′)/⊥,
(x, v) 6= (y, v′). If H is MX -causal, then H ∈ CC(MX ).

CM−−→ is a causal or-
Proof. Suppose that H is MX-causal.
der. Let p ∈ PH and e ∈ p. There exists a linearization
CM−−→).π(EH, p))∩L(MX), associated with a total
lp ∈ lin((H
order of the events ≤p. Let le be the unique linearization of
lin((H ≤p).π(⌊e⌋), p). Let e′ ∈ ⌊e⌋ labelled by rx/v. If e′ has
no antecedent in the writes-into order, v = 0. Otherwise, this
antecedent e′′ is the last write on x before e′ in lp, because
it is the only event labelled wx(v) in the whole history. As
e′′ CM−−→ e′, it is also the last write on x before e′ in le. All in
all, le ∈ L(MX ) and H is causally consistent.

5. Causal convergence
5.1 Deﬁnition

Eventual consistency [25] requires that, if at one point, all the
processes stop doing updates (i.e. operations with a side ef-
fect), then eventually, all local copies of the object will con-
verge to a common state.

Causal convergence assumes weak causal consistency
and eventual consistency. It strengthens weak causal con-
sistency by imposing that the linearizations obtained for all
the events correspond to the same total order. Consequently,
in causal convergence, the updates are totally ordered and
the state read by each operation is the result of the updates
in its causal past, ordered by this common total order. Thus,
two operations with the same causal past are done in the
same state.

Deﬁnition 12. A history H is causally convergent (CCv) for an
ADT T if there exists a causal order →, and a total order ≤ that
contains → such that:
∀e ∈ EH, lin((H ≤).π(⌊e⌋, {e})) ∩ L(T ) 6= ∅.

The history on Fig. 3a is causally convergent: the causal
order and the linearizations introduced in Section 3 could be
obtained considering any total order ≤ in which
w(1)/⊥ ≤ w(2)/⊥. The history on Fig. 3c, yet, is not causally
convergent: both writes must be in the causal past of both
reads as both values are read, but they were not applied in
the same order.

A consistency criterion called strong update consistency
has been introduced in [19] as a strengthening of both update
consistency and strong eventual consistency [6], that both
strengthen eventual consistency. It is interesting to observe
that causal convergence is stronger than strong update con-
sistency, as it imposes to the visibility relation to be a transi-
tive causal order. In other words, there is the same relation
between strong update consistency and causal convergence
as between pipelined consistency and causal consistency.

5.2 Behaviour in absence of data races

Because causal convergence is stronger than weak causal
consistency, Proposition 1 also applies to it. Besides it, there is
another situation in which causal convergence behaves like
sequential consistency: Proposition 5 proves that causally
convergent histories in which no updates happen concur-
rently to queries are also sequentially consistent.

Proposition 5. Let T be an ADT and H = (Σ, E, Λ, 7→) be a
concurrent history such that H ∈ W CC(T ) and, for all update
operations u ∈ E and query operations q ∈ E, u → q or q → u.
Then H ∈ SC(T ).

Proof. Let T be an ADT and H = (Σ, E, Λ, 7→) ∈ W CC(T )
such that, for all update operations u ∈ E and query opera-
tions q ∈ E, u → q or q → u. As H ∈ CCv(T ), there exists a
total order ≤ that contains → and, for all e ∈ E, a lineariza-
tion le · Λ(e) ∈ lin((H ≤).π(⌊e⌋, {e}) ∩ L(T ).

Let l be the unique linearization of lin(H ≤). As 7→⊂≤,
l ∈ lin(H). Suppose that l /∈ L(T ). As in Proposition 1, l
has a preﬁx l′ · Λ(e) /∈ L(T ) with l′ ∈ L(T ). As the transition
system of T is complete, e can not be a pure update. It means
e is a query operation, so it is not concurrent with an update
operation. As →⊂≤, e has the same updates in its causal past
and in its predecessors by ≤, which means that le and l′ are
composed of the same updates in the same order, so l′ and
l′′ lead to the same state. Under these conditions, it is absurd
that l′′ · Λ(e) ∈ L(T ), l′ ∈ L(T ) and l′ · Λ(e) /∈ L(T ). It results
that l ∈ L(T ), so H ∈ SC(T ).

Implementation in wait-free systems

6.
In this section, we illustrate how causally consistent data
structures can be implemented in a special kind of dis-
tributed systems: wait-free asynchronous message-passing
distributed systems. We ﬁrst introduce our computing model,
then we give an implementation of an array of K window
streams of size k for causal consistency and causal conver-
gence.

6.1 Wait-free asynchronous message-passing distributed

systems

A message-passing distributed system is composed of a
known number n of sequential processes that communi-
cate by sending and receiving messages. Processes are asyn-
chronous. This means that the processes execute each at its
own pace, and there is no bound on the time between the
sending and the reception of a message. Moreover, processes

1 object CC(W K
k )
2

var stri ∈ NK×k ← [[0, ..., 0], ..., [0, ..., 0]];
fun read (x ∈ [0, K[) ∈ Nk

return stri[x];

end
fun write (x ∈ [0, K[, v ∈ N)

causal broadcast Mess (x, v);

end
on receive Mess (x ∈ [0, K[, v ∈ N)

for y ∈ [0, k − 2] do

stri[x][y] ← stri[x][y + 1];

end
stri[x][k − 1] ← v;

3

4

5

6

7

8

9

10

11

12

13

can crash. A process that crashes simply stops operating. A
process that never crashes is said to be non-faulty.

Communication is done by the mean of a reliable causal
broadcast communication primitive [10]. Such a communi-
cation primitive can be implemented on any system where
eventually reliable point-to-point communication is possible.
Processes can use two operations broadcast and receive with
the following properties:

• If a process receives a message m, then m was broadcast

by some process.

• If a process receives a message m, then all non-faulty

processes eventually receive m;

• When a non-faulty process broadcasts a message, this

message is immediately received locally at this process.

• If a process broadcasts a message m after receiving a

message m′ then no process receives m before m′.

We make no assumption on the number of crashes that
can occur during one execution. In such a context a pro-
cess cannot wait for the contribution of other processes with-
out risking to remain blocked forever. Consequently, the ex-
ecution speed of a process does not depend on other pro-
cesses or underlying communication delays, hence the name
”wait-free” we give to this system. Wait-free asynchronous
message-passing distributed systems are a good abstraction
of systems where the synchronisation of is impossible (e.g.
clouds where partitions can occur) or too costly (e.g. high
performance parallel computing where synchronisation is a
limitation to performances).

At the shared objects level, processes invoke operations
on shared objects. These calls entail the execution of the algo-
rithms corresponding to their message-passing implementa-
tion based on the causal reliable broadcast. An execution of
a program using an object T is represented (at the shared
object level) by a concurrent history (Σ, E, Λ, 7→) where Σ is
deﬁned as in L(T ), E is the set of all the calls to operations
of T during the whole execution, an event e ∈ E is labelled
by Λ(e) = σi/σo if σi is the input symbol of the operation
called by e and σo is the return value. For e, e′ ∈ E, e 7→ e′ if
e happened before e′, on the same process.

6.2 Implementation of causal consistency

Algorithm given in Fig. 4 shows an implementation of a
causally consistent array of K window streams of size k. The
algorithm provides the user with two primitives, read(x),
where x < K is a stream identiﬁer, that corresponds to a call
to operation r on the xth stream of the array, and write(x, v),
that corresponds to a call to operation w(v) on the xth stream
of the array.

Process pi maintains one variable stri that reﬂects the
local state of the K window streams. When pi wants to read
a stream, it simply returns the corresponding local state.
To write a value v in a stream x, pi causally broadcasts a
message composed of x and v. Upon the reception of such
a message, a process applies the writes locally by shifting
the old values and inserting the new value at the end of the
stream.

Whenever a read or write operation is issued, it is com-
pleted without waiting for any other process. This corre-
sponds to wait-free executions in shared memory distributed
systems and implies fault-tolerance.

end

14
15 end

Figure 4: Implementation of causal consistency for an
array of K window streams of size k (code for pi)

Proposition 6. All histories admitted by the algorithm of Fig. 4
are causally consistent for the array of K window streams of size
k.

Proof. Let H = (Σ, E, Λ, 7→) be a history admitted by algo-
rithm of Fig. 4. For two processes p and p′ and an event e ∈ E
invoked by p′, we deﬁne the time tp
e as the maximum be-
tween the termination time of e and the moment when p has
received all the messages sent by p′ before the termination of
e. We use it to deﬁne two kinds of relations:

• A causal order → by, for any two events e, e′ ∈ E invoked
by processes p and p′ respectively, e → e′ if e = e′ or tp′
is a time before the beginning of e′;

e

• For each process p, a total order odrer ≤p by, for all events

e, e′ ∈ E, e ≤p e′ if tp

e ≤ tp
e′ .

The causal order → is reﬂexive by deﬁnition, antisymmetric
because it is contained into the interval order deﬁned by
real-time and transitive because the broadcast is causal. As
the messages are received instantly by their transmitter, →
contains 7→, and as messages are eventually received by all
non-faulty processes, → is a causal order. The relation ≤p is
a total order that contains →.

Let p ∈ PH, e ∈ p and lp the unique linearization of
(H ≤p).π(E, p). As stri is only modiﬁed when a message
is received and the only reads we consider are those of p,
lp ∈ L(W K
k ). For all e ∈ p, the preﬁx le of lp until e is in
lin((H →).π(⌊e⌋, p)) ∩ L(W K

k ), so H ∈ CC(W K

k ).

For shared memory, it is well-known [8] that causal recep-
tion implements a little more than causality. The same thing
happens for other kinds of objects. For example, the history
on Fig. 3c presents an example of false causality: this history
is causally consistent but is not admitted by the algorithm of
Fig. 4. Indeed, at least one of the messages sent during the
execution of each of the events wx(1) and wx(2) must be re-
ceived by one of the processes after the second write has been
enforced locally – otherwise each of the events would pre-
cede the other in the happened-before relation. As a result, it
is not possible that both processes read the value they pro-
posed before the other value. Actually, the algorithm of Fig.
4 ensures a slightly stronger property: for each process p, the

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

linearizations required by causal consistency for successive
events are preﬁx one from another.

6.3 Implementation of causal convergence

Eventual consistency received such interest because it can
be wait-free implemented independently from communica-
tion delays and moreover, it allows processes to share in
ﬁne the same ﬁnal state. Causal convergence can thus be
seen as an improvement of eventual consistency as it ensures
stronger consistency properties still wait-free implementable
and hence weaker that sequential consistency.

The algorithm given in Fig. 5 shows an implementation
of a causally convergent array of K window streams of size
k. The algorithm provides the user with the same interface
as the algorithm of Fig. 4: a primitive read(x) that corre-
sponds to an operation r on the xth stream and a primitive
write(x, v), that corresponds to an operation w(v) on the xth
stream of the array.

The principle is to build a total order on the write opera-
tions on which all the participants agree, and to sort the cor-
responding values in the local state of each process with re-
spect to this total order. In the algorithm of Fig. 5, this order is
built from a Lamport’s clock [14] that contains the happened-
before precedence relation, and thus is compatible with the
causal order we build on top of this relation. A logical Lam-
port’s clock is a pre-total order as some events may be associ-
ated with the same logical time. In order to have a total order,
the writes are timestamped with a pair composed of the log-
ical time and the id of the process that produced it (process
ids are assumed unique and totally ordered).

Process pi maintains two variables: a Lamport clock
vtimei and an array stri. Each cell of stri is an array of size
k that encodes the k values of a window stream as struc-
tures (v, (vt, j)) where v is the value itself, and (vt, j) is the
timestamp of the write event that proposed it. Timestamps
can be compared : (vt, j) < (vt′, j′) if vt < vt′ or vt = vt′
and j < j′. With zeach write operation is associated a virtual
time greater than 1, so the timestamps (0, 0), present in the
initial value of stri, are smaller than the timestamps of all the
writes.

When pi wants to read a stream, it removes the times-
tamps from the corresponding local state. To write a value
v in a stream x, pi causally broadcasts a message composed
of x and v and a new timestamp (vt, i). At reception of such
a message, it increments its variable vtimei to implement the
logical time, and it inserts the new value at its correct location
in the corresponding stream.

Proposition 7. All histories admitted by Algo. of Fig. 5 are
causally convergent for the array of K objects Wk.

Proof. Let H = (Σ, E, Λ, 7→) be a history admitted by the al-
goritm of Fig. 5. We deﬁne the causal order as in Proposi-
tion 6 and a total order ≤w on the write operations by, for
all writes ei, ej invoked by pi and pj when their variable
vtime is equal to vti and vtj, e ≤ e′ if (vti, i) ≤ (vtj, j).
Let us remark that, thanks to lines 8 and 11, if e → e′, then
e ≤w e′, which means → ∪ ≤w is a partial order on E
that can be extended into a total order ≤. Let e ∈ E and
le be the unique linearization of (H ≤).π(⌊e⌋, {e}). If e is a
write, le contains no return value, so le ∈ L(T ). Otherwise,
Λ(e) = read(x)/[v0, ..., vk−1], where, by construction at the
reception of the messages, v0, ..., vk−1 are the k newest val-
ues written on x with respect to the order ≤, which means
le ∈ L(T ). Finally, H ∈ CCv(W K

k ).

1 object CCv(W K
k )
2

var stri ∈ NK×k×(1+2) ← [[[0, (0, 0)], ...], ...];
var vtimei ∈ N ← 0;
fun read (x ∈ [0, K[) ∈ Nk

return [stri[x][0][0], ..., stri[x][k − 1][0]];

end
fun write (x ∈ [0, K[, v ∈ N)

causal broadcast Mess (x, v, vtime + 1, i) ;

end
on receive Mess (x ∈ [0, K[, v ∈ N, vt ∈ N, j ∈ N)

vtimei ← max(vtimei, vt);
var y ∈ N ← 0;
while y < k − 1 ∧ stri[x][y][1] ≤ (vt, j) do

stri[x][y] ← stri[x][y + 1];
y ← y + 1;

end
if y 6= 0 then

stri[x][y − 1] ← v

end

end

20
21 end

Figure 5: Implementation of causal convergence for an array
of K window streams of size k (code for pi)

7. Conclusion
Sharing objects is essential to abstract communication com-
plexity in large scale distributed systems. Until now, a lot of
work has been done to specify many kinds of shared mem-
ory, but they cannot always be easily extended to other ab-
stract data types. In this paper, we extend causal consistency
to all abstract data types. We also explore the variations of
causal consistency, around three consistency criteria.

Each of these three consistency criteria is pertinent. Weak
causal consistency can be seen as the causal common denom-
inator of the two branches of weak consistency criteria (even-
tual consistency and pipelined consistency). Indeed, it can
be combined with any of them in wait-free distributed sys-
tems. Causal convergence is the result of the integration on
weak causal consistency in the eventual consistency branch.
Finally, causal consistency is from one side the generalization
of the consistency criterion of causal memory to any abstract
data type and on the other side it covers both weak causal
consistency and pipelined consistency.

To sum up, this paper allows to better understand the con-
nections between weak consistency criteria. The two criteria
to keep in mind are causal convergence and causal consis-
tency as representatives of the two irreconcilable branches of
consistency in wait-free distributed systems (sequential con-
sistency and linearizability cannot be implemented in such a
context).

Acknowledgment
This work has been partially supported by the French ANR
project Socioplug (ANR-13-INFR-0003), which is devoted
to large scale distributed programming, and the Franco-
German ANR project DISCMAT devoted to connections be-
tween mathematics and distributed computing.

SIGPLAN-SIGACT Symp. on Principles of Programming Lan-
guages, POPL 2011, Austin, TX, USA, January, pages 43–54, 2011.
[22] Marc Shapiro, Nuno M. Preguic¸a, Carlos Baquero, and Marek
In Stabilization,
Zawirski. Conﬂict-free replicated data types.
Safety, and Security of Distributed Systems - 13th International Sym-
posium, SSS 2011, Grenoble, France, October 10-12, 2011. Proceed-
ings, pages 386–400, 2011.

[23] Chengzheng Sun, Xiaohua Jia, Yanchun Zhang, Yun Yang,
and David Chen. Achieving convergence, causality preserva-
tion, and intention preservation in real-time cooperative edit-
ing systems. ACM Transactions on Computer-Human Interaction
(TOCHI), 5(1):63–108, 1998.

[24] Douglas B Terry, Alan J Demers, Karin Petersen, Mike J Spre-
itzer, Marvin M Theimer, and Brent B Welch. Session guarantees
for weakly consistent replicated data. In Parallel and Distributed
Information Systems, 1994., Proceedings of the Third International
Conference on, pages 140–149. IEEE, 1994.

[25] Werner Vogels. Eventually consistent. Queue, 6(6):14–19, 2008.

References
[1] Sarita V Adve and Kourosh Gharachorloo. Shared memory

consistency models: A tutorial. computer, 29(12):66–76, 1996.

[2] Mustaque Ahamad, Gil Neiger, James E Burns, Prince Kohli,
and Phillip W Hutto. Causal memory: Deﬁnitions, implementa-
tion, and programming. Distributed Computing, 9(1):37–49, 1995.
Sequential consistency
versus linearizability. ACM Transactions on Computer Systems
(TOCS), 12(2):91–122, 1994.

[3] Hagit Attiya and Jennifer L Welch.

[4] Kenneth P Birman and Thomas A Joseph. Reliable communica-
tion in the presence of failures. ACM Transactions on Computer
Systems (TOCS), 5(1):47–76, 1987.

[5] Hans-Juergen Boehm and Sarita V. Adve. Foundations of the
C++ concurrency memory model. In Proc. of the ACM SIGPLAN
2008 Conf. on Programming Language Design and Implementation
(PLDI08), Tucson, AZ, USA, June, pages 68–78, 2008.

[6] Sebastian Burckhardt, Alexey Gotsman, Hongseok Yang, and
Marek Zawirski. Replicated data types: speciﬁcation, veriﬁca-
In The 41st Annual ACM SIGPLAN-SIGACT
tion, optimality.
Symposium on Principles of Programming Languages, POPL ’14,
San Diego, CA, USA, January 20-21, 2014, pages 271–284, 2014.

[7] Michael J Fischer, Nancy A Lynch, and Michael S Paterson.
Impossibility of distributed consensus with one faulty process.
Journal of the ACM (JACM), 32(2):374–382, 1985.

[8] Pranav Gambhire and Ajay D Kshemkalyani. Reducing false
causality in causal message ordering. In High Performance Com-
putingHiPC 2000, pages 61–72. Springer, 2000.

[9] Seth Gilbert and Nancy Lynch. Brewer’s conjecture and the fea-
sibility of consistent, available, partition-tolerant web services.
ACM SIGACT News, 33(2):51–59, 2002.

[10] Vassos Hadzilacos and Toueg Sam. Reliable Broadcast and Related
Problems. In Dis- tributed Systems (S. Mullender Ed.). ACM Press,
1993.

[11] Maurice Herlihy. Wait-free synchronization. ACM Transactions
on Programming Languages and Systems (TOPLAS), 13(1):124–149,
1991.

[12] Maurice Herlihy. Technical perspective - highly concurrent data

structures. Communications of the ACM, 52(5):99, 2009.

[13] Maurice P Herlihy and Jeannette M Wing. Linearizability: A
correctness condition for concurrent objects. ACM Transactions
on Programming Languages and Systems (TOPLAS), 12(3):463–492,
1990.

[14] Leslie Lamport. Time, clocks, and the ordering of events in a
distributed system. Communications of the ACM, 21(7):558–565,
1978.

[15] Leslie Lamport. How to make a multiprocessor computer that
correctly executes multiprocess programs. Computers, IEEE
Transactions on, 100(9):690–691, 1979.

[16] Richard J Lipton and Jonathan S Sandberg. PRAM: A scalable
shared memory. Princeton University, Department of Computer
Science, 1988.

[17] George H Mealy. A method for synthesizing sequential circuits.

Bell System Technical Journal, 34(5):1045–1079, 1955.

[18] Jayadev Misra. Axioms for memory access in asynchronous
hardware systems. ACM Transactions on Programming Languages
and Systems (TOPLAS), 8(1):142–153, 1986.

[19] Matthieu Perrin, Achour Most´efaoui, and Claude Jard. Update
consistency for wait-free concurrent objects. In Proceedings of the
29th IEEE International Parallel and Distributed Processing Sympo-
sium. IEEE, 2015.

[20] Michel Raynal, Andr´e Schiper, and Sam Toueg. The causal or-
dering abstraction and a simple way to implement it. Informa-
tion processing letters, 39(6):343–350, 1991.

[21] Jaroslav Sevc´ık, Viktor Vafeiadis, Francesco Zappa Nardelli,
Suresh Jagannathan, and Peter Sewell. Relaxed-memory con-
In Proc. of the 38th ACM
currency and veriﬁed compilation.

