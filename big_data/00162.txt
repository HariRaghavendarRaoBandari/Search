Convolutional Rectiﬁer Networks as Generalized Tensor Decompositions

Nadav Cohen

The Hebrew University of Jerusalem
cohennadav@cs.huji.ac.il

Amnon Shashua

The Hebrew University of Jerusalem

shashua@cs.huji.ac.il

6
1
0
2

 
r
a

M
1

 

 
 
]
E
N
.
s
c
[
 
 

1
v
2
6
1
0
0

.

3
0
6
1
:
v
i
X
r
a

Abstract

Convolutional rectiﬁer networks, i.e. convolutional neu-
ral networks with rectiﬁed linear activation and max or av-
erage pooling, are the cornerstone of modern deep learning.
However, despite their wide use and success, our theoretical
understanding of the expressive properties that drive these
networks is partial at best. On other hand, we have a much
ﬁrmer grasp of these issues in the world of arithmetic cir-
cuits. Speciﬁcally, it is known that convolutional arithmetic
circuits posses the property of ”complete depth efﬁciency”,
meaning that besides a negligible set, all functions that can
be implemented by a deep network of polynomial size, re-
quire exponential size in order to be implemented (or even
approximated) by a shallow network.

In this paper we describe a construction based on gener-
alized tensor decompositions, that transforms convolutional
arithmetic circuits into convolutional rectiﬁer networks. We
then use mathematical tools available from the world of
arithmetic circuits to prove new results. First, we show
that convolutional rectiﬁer networks are universal with max
pooling but not with average pooling. Second, and more im-
portantly, we show that depth efﬁciency is weaker with con-
volutional rectiﬁer networks than it is with convolutional
arithmetic circuits. This leads us to believe that developing
effective methods for training convolutional arithmetic cir-
cuits, thereby fulﬁlling their expressive potential, may give
rise to a deep learning architecture that is provably supe-
rior to convolutional rectiﬁer networks but has so far been
overlooked by practitioners.

1. Introduction

Deep neural networks are repeatedly proving themselves
to be extremely effective machine learning models, pro-
viding state of the art accuracies on a wide range of tasks
(see [17, 9]). Arguably, the most successful deep learning
architecture to date is that of convolutional neural networks
(ConvNets, [16]), which prevails in the ﬁeld of computer
vision, and is recently being harnessed for many other ap-
plication domains as well (e.g. [25, 31, 2]). Modern Con-

1

vNets are formed by stacking layers one after the other,
where each layer consists of a linear convolutional oper-
ator followed by Rectiﬁed Linear Unit (ReLU [21]) acti-
vation (σ(z) = max{0, z}), which in turn is followed by
max or average pooling (P{cj} = max{cj} or P{cj} =
mean{cj} respectively). Such models, which we refer to as
convolutional rectiﬁer networks, have driven the resurgence
of deep learning ([15]), and represent the cutting edge of the
ConvNet architecture ([28, 27]).

Despite their empirical success, and the vast attention
they are receiving, our theoretical understanding of convo-
lutional rectiﬁer networks is partial at best. It is believed
that they enjoy depth efﬁciency, i.e. that when allowed to go
deep, such networks can implement with polynomial size
computations that would require super-polynomial size if
the networks were shallow. However, formal arguments
that support this are scarce.
It is unclear to what extent
convolutional rectiﬁer networks leverage depth efﬁciency,
or more formally, what is the proportion of weight settings
that would lead a deep network to implement a computation
that cannot be efﬁciently realized by a shallow network. We
refer to the most optimistic situation, where this takes place
for all weight settings but a negligible (zero measure) set, as
complete depth efﬁciency.

Compared to convolutional rectiﬁer networks, our theo-
retical understanding of depth efﬁciency for arithmetic cir-
cuits, and in particular for convolutional arithmetic circuits,
is much more developed. Arithmetic circuits (also known as
Sum-Product Networks, [24]) are networks with two types
of nodes: sum nodes, which compute a weighted sum of
their inputs, and product nodes, computing the product of
their inputs. The depth efﬁciency of arithmetic circuits has
been studied by the theoretical computer science commu-
nity for the last ﬁve decades, long before the resurgence of
deep learning. Although many problems in the area remain
open, signiﬁcant progress has been made over the years,
making use of various mathematical tools. Convolutional
arithmetic circuits form a speciﬁc sub-class of arithmetic
circuits. Namely, these are ConvNets with linear activation

(σ(z) = z) and product pooling (P{cj} =(cid:81) cj). Recently,

[5] analyzed convolutional arithmetic circuits through ten-

sor decompositions, essentially proving, for the type of net-
works considered, that depth efﬁciency holds completely.
Although convolutional arithmetic circuits are known to be
equivalent to SimNets ([3]), a new deep learning archi-
tecture that has recently demonstrated promising empirical
performance ([4]), they are fundamentally different from
convolutional rectiﬁer networks. Accordingly, the result es-
tablished in [5] does not apply to models commonly used in
practice.

In this paper we present a construction, based on the no-
tion of generalized tensor decompositions, that transforms
convolutional arithmetic circuits of the type described in [5]
into convolutional rectiﬁer networks. We then use the avail-
able mathematical tools from the world of arithmetic cir-
cuits to prove new results concerning the expressive power
and depth efﬁciency of convolutional rectiﬁer networks.
Namely, we show that with ReLU activation, average pool-
ing leads to loss of universality, whereas max pooling is
universal but enjoys depth efﬁciency to a lesser extent than
product pooling with linear activation (convolutional arith-
metic circuits). These results indicate that from the point
of view of expressive power and depth efﬁciency, convo-
lutional arithmetic circuits (SimNets) have an advantage
over the prevalent convolutional rectiﬁer networks (Con-
vNets with ReLU activation and max or average pooling).
This leads us to believe that developing effective methods
for training convolutional arithmetic circuits, thereby ful-
ﬁlling their expressive potential, may give rise to a deep
learning architecture that is provably better than convolu-
tional rectiﬁer networks but has so far been overlooked by
practitioners.

The remainder of this paper is organized as follows. In
sec. 2 we review existing works relating to depth efﬁciency
of arithmetic circuits and networks with ReLU activation.
Sec. 3 presents our deﬁnition of generalized tensor decom-
positions, followed by sec. 4 which employs this concept
to frame convolutional rectiﬁer networks.
In sec. 5 we
make use of this framework for an analysis of the expres-
sive power and depth efﬁciency of such networks. Finally,
sec. 6 concludes.

2. Related Work

The literature on the computational complexity of arith-
metic circuits is far too wide to cover here, dating back
over ﬁve decades. Although many of the fundamental ques-
tions in the ﬁeld remain open, signiﬁcant progress has been
made over the years, developing and employing a vast share
of mathematical tools from branches of geometry, algebra,
analysis, combinatorics, and more. We refer the interested
reader to [26] for a survey written in 2010, and mention
here the more recent works [7] and [19] studying depth ef-
ﬁciency of arithmetic circuits in the context of deep learn-
ing (Sum-Product Networks). Compared to arithmetic cir-

cuits, the literature on depth efﬁciency of neural networks
with ReLU activation is far less developed, primarily since
these models were only introduced several years ago ([21]).
There have been some notable works on this line, but these
employ dedicated mathematical machinery, not making use
of the plurality of available tools from arithmetic circuits.
[22] and [20] use combinatorial arguments to characterize
the maximal number of linear regions in functions gener-
ated by ReLU networks, thereby establishing existence of
depth efﬁciency. [30] uses semi-algebraic geometry to ana-
lyze the number of oscillations in functions realized by neu-
ral networks with semi-algebraic activations, ReLU in par-
ticular. The fundamental result proven in [30] is the exis-
tence, for every k ∈ N, of functions realizable by networks
with Θ(k3) layers and Θ(1) nodes per layer, which cannot
be approximated by networks with O(k) layers unless these
are exponentially large (have Ω(2k) nodes). The work in [8]
makes use of Fourier analysis to show existence of func-
tions that are efﬁciently computable by depth-3 networks,
yet require exponential size in order to be approximated by
depth-2 networks. The result applies to various activations,
including ReLU. [23] also compares the computational abil-
ities of deep vs. shallow networks under different activations
that include ReLU. However, the complexity measure con-
sidered in [23] is the VC dimension, whereas our interest
lies in network size.

None of the analyses above account for convolutional
networks 1, thus they do not apply to the deep learning
architecture most commonly used in practice. Recently,
[5] introduced convolutional arithmetic circuits, which may
be viewed as ConvNets with linear activation and product
pooling. These networks are shown to correspond to hier-
archical tensor decompositions (see [11]). Tools from lin-
ear algebra, functional analysis and measure theory are then
employed to prove that the networks are universal, and ex-
hibit complete depth efﬁciency. Although similar in struc-
ture, convolutional arithmetic circuits are inherently differ-
ent from convolutional rectiﬁer networks (ConvNets with
ReLU activation and max or average pooling). Accordingly,
the analysis carried out in [5] does not apply to the networks
at the forefront of deep learning.

Closing the gap between the networks analyzed in [5]
and convolutional rectiﬁer networks is the topic of this pa-
per. We achieve this by generalizing tensor decomposi-
tions, thereby opening the door to mathematical machinery
as used in [5], harnessing it to analyze, for the ﬁrst time, the
depth efﬁciency of convolutional rectiﬁer networks.

1 By this we mean that in all analyses, the deep networks shown to
beneﬁt from depth (i.e. to realize functions that require super-polynomial
size from shallow networks) are not ConvNets.

2

3. Generalized Tensor Decompositions

We begin by establishing basic tensor-related terminol-
ogy and notations. For our purposes, a tensor is simply a
multi-dimensional array:

Ad1,...,dN ∈ R , di ∈ [Mi]

The order of a tensor is deﬁned to be the number of in-
dexing entries in the array, which are referred to as modes.
The term dimension stands for the number of values an in-
dex can take in a particular mode. For example, the ten-
sor A above has order N and dimension Mi in mode i,
i ∈ [N ]. The space of all possible conﬁgurations A can
take is called a tensor space and is denoted, quite naturally,
by RM1×···×MN .
The fundamental operator in tensor analysis is the tensor
product, denoted by ⊗. It is an operator that intakes two
tensors A ∈ RM1×···×MP and B ∈ RMP +1×···×MP +Q (or-
ders P and Q respectively), and returns a tensor A ⊗ B ∈
RM1×···×MP +Q (order P + Q) deﬁned by:

(A ⊗ B)d1,...,dP +Q

= Ad1,...,dP · BdP +1,...,dP +Q

(1)

Notice that in the case P = Q = 1, the tensor product
reduces to the standard outer product between vectors, i.e.
if u ∈ RM1 and v ∈ RM2, then u ⊗ v is no other than the
rank-1 matrix uv(cid:62) ∈ RM1×M2.

Tensor decompositions (see [14] for a survey) may be
viewed as schemes for expressing tensors using tensor prod-
ucts and weighted sums. For example, suppose we have a
tensor A ∈ RM1×···×MN given by:

J(cid:88)

A =

cj1...jN · aj1,1 ⊗ ··· ⊗ ajN ,N

j1...jN =1

This expression is known as a Tucker decomposition, pa-
rameterized by the coefﬁcients {cj1...jN ∈ R}j1...jN∈[J] and
vectors {aj,i ∈ RMi}i∈[N ],j∈[J]. It is different from the CP
(rank-1) and Hierarchical Tucker decompositions our anal-
ysis will rely upon (see sec. 4). All decompositions however
are closely related, speciﬁcally in the fact that they are based
on iterating between tensor products and weighted sums.

Our construction and analysis are facilitated by gener-
alizing the tensor product, which in turn generalizes ten-
sor decompositions.
For an associative and commuta-
i.e. a function g : R × R →
tive binary operator g,
R such that ∀a, b, c ∈ R : g(g(a, b), c) = g(a, g(b, c))
and ∀a, b ∈ R :
the generalized
tensor product ⊗g, an operator intaking tensors A ∈
RM1×···×MP ,B ∈ RMP +1×···×MP +Q and returning tensor
A ⊗g B ∈ RM1×···×MP +Q, is deﬁned as follows:
(A ⊗g B)d1,...,dP +Q

= g(Ad1,...,dP ,BdP +1,...,dP +Q)

g(a, b) = g(b, a),

(2)

Generalized tensor decompositions are simply obtained by
plugging in the generalized tensor product ⊗g in place of
the standard tensor product ⊗.

4. From Networks to Tensors

The ConvNet architecture analyzed in this paper is pre-
sented in ﬁg. 1. The input to a network, denoted X, is
composed of N patches x1 . . . xN ∈ Rs. The ﬁrst layer,
referred to as representation, can be thought of as a gener-
alized convolution. Namely, it consists of applying M rep-
resentation functions fθ1. . .fθM : Rs → R to the patches
of the input, thereby creating M feature maps. In the case
where the representation functions are standard neurons, i.e.
d x + bd) for parameters θd = (wd, bd) ∈
fθd (x) = σ(w(cid:62)
Rs × R and some chosen activation σ(·), we obtain a con-
ventional convolutional layer. More elaborate settings are
possible, for example modeling the representation as a cas-
cade of convolutional layers with pooling in-between.
Following the representation, a network includes L hid-
den layers indexed by l = 0. . .L − 1. Each hidden layer l
begins with a 1×1 conv operator, which is simply a 3D con-
volution with rl channels and receptive ﬁeld 1× 1 followed
by point-wise activation σ(·). We allow the convolution to
operate without weight sharing, in which case the ﬁlters that
generate feature maps by sliding across the previous layer
may have different coefﬁcients at different spatial locations.
This is often referred to in the deep learning community as
a locally-connected layer (see [29]). We refer to it as the un-
shared case, in contrast to the shared case that gives rise to
a standard 1×1 convolution. The second (last) operator in a
hidden layer is spatial pooling. Feature maps generated by
1 × 1 conv are decimated, by applying the pooling operator
P (·) (e.g. max or average) to non-overlapping 2D windows
that cover the spatial extent. The last of the L hidden layers
(l = L− 1) reduces feature maps to singletons (i.e. its pool-
ing operator is global), creating a vector of dimension rL−1.
This vector is mapped into Y network outputs through a ﬁ-
nal dense linear layer.

Altogether, the architectural parameters of a ConvNet are
the type of representation functions (fθd), the pooling win-
dow sizes (which in turn determine the number of hidden
layers L), the setting of conv weights as shared or unshared,
the number of channels in each layer (M for representa-
tion, r0. . .rL−1 for hidden layers, Y for output), and the
choice of activation and pooling operators (σ(·) and P (·) re-
spectively). Given these architectural parameters, the learn-
able parameters of a network are the representation weights
(θd), the conv weights (al,j,γ for hidden layer l, location j
and channel γ in the unshared case; al,γ for hidden layer l
and channel γ in the shared case), and the output weights
(aL,1,y).

The choice of activation and pooling operators deter-
mines the type of network we arrive at. For linear acti-

3

Figure 1. ConvNet architecture analyzed in this paper. The representation convolves functions fθd (·) across input patches (a standard
d x + bd)). L hidden layers follow, each comprising 1× 1 convolution (optionally
convolutional layer is obtained by setting fθd (x) = σ(w(cid:62)
without spatial weight sharing) followed by activation σ(·) and pooling P (·). The last hidden layer reduces feature maps to singletons, and
these are mapped to network outputs through a dense linear layer. Convolutional arithmetic circuits as analyzed in [5] correspond to linear
(σ(z) = max{0, z}) and max or average pooling (P{cj} = max{cj} or P{cj} = mean{cj} respectively). Best viewed in color.

activation (σ(z) = z) and product pooling (P{cj} = (cid:81) cj). Convolutional rectiﬁer networks are obtained by setting ReLU activation
vation (σ(z) = z) and product pooling (P{cj} = (cid:81) cj)

mension M in each mode deﬁned by:

we get a convolutional arithmetic circuit as analyzed in [5].
For ReLU activation (σ(z) = max{0, z}) and max or av-
erage pooling (P{cj} = max{cj} or P{cj} = mean{cj}
respectively) we get the commonly used convolutional rec-
tiﬁer networks, on which we focus in this paper.

In terms of pooling window sizes and network depth,
we direct our attention to two special cases representing
the extremes. The ﬁrst is a shallow network that includes
global pooling in its single hidden layer – see illustration
in ﬁg. 2. The second is the deepest possible network, in
which all pooling windows cover only two entries, result-
ing in L = log2 N hidden layers. These ConvNets, which
we refer to as shallow and deep respectively, will be shown
to correspond to canonical tensor decompositions. It is for
this reason, and for the sake of simplicity, that we limit our-
selves to these special cases. One may just as well con-
sider networks of intermediate depths with different pool-
ing window sizes, and that would correspond to other, non-
standard, tensor decompositions. The analysis carried out
in sec. 5 can easily be adapted to such cases.

In a classiﬁcation setting, the Y outputs of a network cor-
respond to different categories, and prediction follows the
output with highest activation. Speciﬁcally, if we denote by
hy(·) the mapping from network input to output y, the pre-
dicted label for the instance X = (x1, . . . , xN ) ∈ (Rs)N is
determined by the following classiﬁcation rule:

ˆy = argmax
y∈[Y ]

hy(X)

We refer to hy as the score function of category y. In this
paper we study score functions through the notion of grid
tensors. Given templates x(1) . . . x(M ) ∈ Rs, the grid ten-
sor of hy, denoted A(hy), is the tensor of order N and di-

A(hy)d1...dN = hy(x(d1), . . . , x(dN ))

(3)

That is to say, the grid tensor of a score function under M
templates, is a tensor of order N and dimension M in each
mode, holding score values on an exponentially large grid of
instances, where each instance in the grid has its N patches
chosen from the set of M templates. Obviously two score
functions are different if their grid tensors are different. We
argue in app. A that in the case of structured compositional
data (e.g. natural images), M ∈ Ω(100) sufﬁces in order
for the converse to hold as well. Speciﬁcally, if M is on
the order of hundreds or more, the templates x(1) . . . x(M )
can be chosen such that a score function is fully determined
by its grid tensor, i.e. two score functions are effectively
equivalent if their grid tensors are identical. Before head-
ing on to our analysis of grid tensors generated by Con-
vNets, to simplify notation, we deﬁne F ∈ RM×M to be
the matrix holding the values taken by the representation
functions fθ1. . .fθM : Rs → R on the selected templates
x(1) . . . x(M ) ∈ Rs:

(4)

 fθ1 (x(1))

...

fθ1 (x(M ))

F :=



···
...
···

fθM (x(1))

...

fθM (x(M ))

To express the grid tensor of a ConvNet’s score function
using generalized tensor decompositions (see sec. 3), we set
the underlying function g : R×R → R to be the activation-
pooling operator deﬁned by:

g(a, b) = P (σ(a), σ(b))

(5)
where σ(·) and P (·) are the network’s activation and pool-
ing functions, respectively. Notice that the activation-
pooling operator meets the associativity and commutativity

4

,direpidfxinputrepresentation1x1 convpooling1x1 convpoolingdense (output)hidden layer 0hidden layer L-1ixM0r0r1Lr1LrY0,,0,,,:jconvjrepja00',',jwindowjpooljPconvj11'coversspace',LLjpoolPconvj,1,1,:LyLoutypoolaXrequirements under product pooling with linear activation
(g(a, b) = a·b), and under max pooling with ReLU activa-
tion (g(a, b) = max{a, b, 0}). To account for the case of
average pooling with ReLU activation, which a-priori leads
to a non-associative activation-pooling operator, we sim-
ply replace average by sum, i.e. we analyze sum pooling
with ReLU activation (g(a, b) = max{a, 0} + max{b, 0}),
which from the point of view of expressiveness is com-
pletely equivalent to average pooling with ReLU activation
(scaling factors can always blend in to linear weights that
follow pooling).

With the activation-pooling operator g in place, it is
straightforward to see that the grid tensor of hS
y – a score
function generated by the shallow ConvNet (ﬁg. 2), is given
by the following generalized tensor decomposition:

z · (F az,1) ⊗g ··· ⊗g (F az,N )
ay

(6)

A(cid:0)hS

y

(cid:1) =

Z(cid:88)

z=1

Z here is the number of channels in the network’s single
hidden layer, {az,i ∈ RM}z∈[Z],i∈[N ] are the weights in the
hidden conv, and ay ∈ RZ are the weights of output y. The
factorization in eq. 6 generalizes the classic CP (CANDE-
COMP/PARAFAC) decomposition (see [14] for a historic
survey), and we accordingly refer to it as the generalized
CP decomposition.

Turning to the deep ConvNet (ﬁg. 1 with size-2 pooling
windows and L = log2 N hidden layers), the grid tensor of
its score function hD
y is given by the hierarchical general-
ized tensor decomposition below:

a1,j,γ
α

(F a0,2j−1,α) ⊗g (F a0,2j,α)

φ1,j,γ =
···

φl,j,γ =

···
φL−1,j,γ =

A(cid:0)hD

y

(cid:1) =

α=1

r0(cid:88)
rl−1(cid:88)
rL−2(cid:88)
rL−1(cid:88)

α=1

α=1

α=1

(cid:124)

al,j,γ
α

φl−1,2j−1,α
order 2l−1

⊗g φl−1,2j,α
order 2l−1

(cid:123)(cid:122)
(cid:124)

(cid:125)
(cid:123)(cid:122)

(cid:123)(cid:122)
(cid:124)
(cid:123)(cid:122)

(cid:125)
(cid:123)(cid:122)
(cid:125)

(cid:124)
(cid:125)
(cid:124)

aL−1,j,γ

α

φL−2,2j−1,α

⊗g φL−2,2j,α

aL,1,y
α

order N
4

order N
4

(cid:124)

φL−1,1,α
order N
2

⊗g φL−1,2,α

(cid:123)(cid:122)

(cid:125)

order N
2

(cid:125)

(7)

r0. . .rL−1 ∈ N here are the number of channels in
the network’s hidden layers, {a0,j,γ ∈ RM}j∈[N ],γ∈[r0]
are the weights in the ﬁrst hidden conv, {al,j,γ ∈
Rrl−1}l∈[L−1],j∈[N/2l],γ∈[rl] are the weights in the follow-
ing hidden convs, and aL,1,y ∈ RrL−1 are the weights of
output y. The factorization in eq. 7 generalizes the Hierar-
chical Tucker decomposition introduced in [10], and is ac-
cordingly referred to as the generalized HT decomposition.

5

Figure 2. Shallow ConvNet with global pooling in its single hid-
den layer. Best viewed in color.

To conclude this section, we presented a ConvNet ar-
chitecture (ﬁg. 1) whose activation and pooling operators
may be chosen to realize convolutional arithmetic circuits
(linear activation, product pooling) or convolutional recti-
ﬁer networks (ReLU activation, max/average pooling). We
then deﬁned the grid tensor of a network’s score function as
a tensor holding function values on an exponentially large
grid whose points are sequences with elements chosen from
a ﬁnite set of templates. Then, we saw that the grid tensor
of a shallow ConvNet (ﬁg. 2) is given by the generalized
CP decomposition (eq. 6), and a grid tensor of a deep Con-
vNet (ﬁg. 1 with L = log2 N) is given by the generalized
HT decomposition (eq. 7). In the next section we utilize the
connection between ConvNets and generalized tensor de-
compositions for an analysis of the expressive power and
depth efﬁciency of convolutional rectiﬁer networks.

5. Capacity Analysis

In this section we analyze score functions expressible
by the shallow and deep ConvNets (ﬁg. 2, and ﬁg. 1 with
L = log2 N, respectively) under ReLU activation with max
or average pooling (convolutional rectiﬁer networks), com-
paring these settings against linear activation with product
pooling (convolutional arithmetic circuits). Score functions
are analyzed through grid tensors (eq. 3), represented by the
generalized tensor decompositions established in the pre-
vious section:
the generalized CP decomposition (eq. 6)
corresponding to the shallow network, and the generalized
HT decomposition (eq. 7) corresponding to the deep net-
work. The analysis is organized as follows.
In sec. 5.1
we present preliminary material required in order to follow
our proofs. Sec. 5.2 discusses templates and representation
functions, which form the bridge between score functions
and generalized tensor decompositions. Sec. 5.3 presents
matricization – a technical tool that facilitates the use of
matrix theory for analyzing generalized tensor decomposi-
tions. The actual analysis begins in sec. 5.4, where we ad-
dress the question of universality, i.e. of the ability of net-
works to realize any score function when their size is unlim-
ited. This is followed by sec. 5.5 which studies depth efﬁ-

,direpidfxinputrepresentation1x1 convglobal poolingdense (output)hidden layerixMZZY,,,,:ziconvizrepiacoversspace,ipoolzPconviz,:youtypoolaXciency, namely, situations where functions efﬁciently com-
putable by deep networks require shallow networks to have
super-polynomial size. Finally, sec. 5.6 analyzes the case
of coefﬁcient sharing, in which the conv operators of our
networks are standard convolutions (as opposed to the more
general locally-connected layers).

5.1. Preliminaries

For evaluating the completeness of depth efﬁciency, and
for other purposes as well, we are often interested in the
“volume” of sets in a Euclidean space, or more formally, in
their Lebesgue measure. While an introduction to Lebesgue
measure theory is beyond the scope of this paper (the inter-
ested reader is referred to [13]), we restate here several con-
cepts and results our proofs will rely upon. A zero measure
set can intuitively be thought of as having zero volume. A
union of countably many zero measure sets is itself a zero
measure set. If we randomize a point in space by some con-
tinuous distribution, the probability of hitting a zero mea-
sure set is always zero. A useful fact (proven in [1] for
example) is that the zero set of a polynomial, i.e. the set of
points on which a polynomial vanishes, is either the entire
space (when the polynomial in question is the zero polyno-
mial), or it must have measure zero. An open set always
has positive measure, and when a point in space is drawn
by a continuous distribution with non-vanishing continuous
probability density function, the probability of hitting such
a set is positive.

i=1 fθdi

(cid:81)M

Apart from measure theory, we will also be using tools
from the ﬁeld of tensor analysis. Here too, a full introduc-
tion to the topic is beyond our scope (we refer the inter-
ested reader to [11]), and we only list some concepts and
results that will be used. First, a fact that relates to ab-
stract tensor products over function spaces is the follow-
If fθ1. . .fθM : Rs → R are linearly independent
ing.
functions, then the product functions {(x(1), . . . , x(M )) (cid:55)→
(x(i))}d1...dM∈[M ] from (Rs)M to R are linearly
independent as well. Back to tensors as we have deﬁned
them (multi-dimensional arrays), a very important concept
is that of rank, which for order-2 tensors reduces to the stan-
dard notion of matrix rank. A tensor is said to have rank 1
if it may be written as a tensor product between non-zero
vectors (A = v1 ⊗ ··· ⊗ vN ). The rank of a general tensor
is deﬁned to be the minimal number of rank-1 tensors that
may be summed up to produce it. A useful fact is that the
rank of an order-N tensor with dimension Mi in each mode
i Mi/ maxi Mi. On the other
hand, all such tensors, besides a zero measure set, have
i odd Mi}. As in
the special case of matrices, the rank is sub-additive, i.e.
rank(A+B)≤rank(A)+rank(B) for any tensors A,B of
matching dimensions. The rank is sub-multiplicative w.r.t.
the tensor product, i.e. rank(A ⊗ B)≤rank(A)·rank(B)

i ∈ [N ], is no greater than(cid:81)
rank equal to at least min{(cid:81)

i even Mi,(cid:81)

for any tensors A,B. Finally, we use the fact that permut-
ing the modes of a tensor does not alter its rank.
5.2. Templates and Representation Functions

The expressiveness of our ConvNets obviously depends
on the possible forms that may be taken by the representa-
tion functions fθ1. . .fθM : Rs → R. For example, if repre-
sentation functions are limited to be constant, the ConvNets
can only realize constant score functions. We denote by
F := {fθ : Rs → R : θ ∈ Θ} the parametric family from
which representation functions are chosen, and make two
mild assumptions on this family:

• Continuity: fθ(x) is continuous w.r.t. both θ and x.
• Non-degeneracy: For any x(1) . . . x(M ) ∈ Rs such
(cid:54)= xj ∀i(cid:54)=j, there exist fθ1. . .fθM ∈ F for

that xi
which the matrix F deﬁned in eq. 4 is non-singular.

Both of the assumptions above are met for most reasonable
choices of F. In particular, non-degeneracy holds when rep-
resentation functions are standard neurons:

Claim 1. The parametric family:

F =(cid:8)fθ(x) = σ(w(cid:62)x + b) : θ = (w, b) ∈ Rs × R(cid:9)

(8)
where σ(·) is any sigmoidal activation 2 or the ReLU ac-
tivation, meets the non-degeneracy condition (i.e. for any
distinct x(1) . . . x(M ) ∈ Rs there exist fθ1 . . .fθM ∈ F such
that the matrix F deﬁned in eq. 4 is non-singular).
Proof. We ﬁrst show that given distinct x(1) . . . x(M ) ∈ Rs,
there exists a vector w ∈ Rs such that w(cid:62)x(i) (cid:54)= w(cid:62)x(j)
for all 1≤i < j≤M. w satisﬁes this condition if it is not
perpendicular to any of the ﬁnitely many non-zero vectors
{x(i) − x(j) : 1≤i < j≤M}. If for every 1≤i < j≤M
we denote by P (i,j) ⊂ Rs the set of points perpendicular to
x(i) − x(j), we obtain that w satisﬁes the desired condition
1≤i<j≤M P (i,j). Each P (i,j)
is the zero set of a non-zero polynomial, and in particular
1≤i<j≤M P (i,j) thus
has measure zero as well, and accordingly cannot cover the
1≤i<j≤M P (i,j)
indeed exists.
Assume without loss of generality w(cid:62)x(1) < . . . <
w(cid:62)x(M ). We may then choose b1. . .bM ∈ R such that
−w(cid:62)x(M ) < bM < . . . < −w(cid:62)x(1) < b1. For i, j ∈ [M ],
w(cid:62)x(i) + bj is positive when j≤i and negative when j > i.
Therefore, if σ(·) is chosen as the ReLU activation, deﬁn-
ing fθj (x) = σ(w(cid:62)x + bj) for every j ∈ [M ] gives rise
to a matrix F (eq. 4) that is lower triangular with non-zero
2 σ(·) is sigmoidal if it is monotonic with limz→−∞ σ(z) = c and

if it does not lie in the union(cid:83)
has measure zero. The ﬁnite union(cid:83)
entire space. This implies that w ∈ Rs \(cid:83)

limz→+∞ σ(z) = C for some c(cid:54)=C in R.

6

values on its diagonal. This proves the desired result for the
case of ReLU activation.
Consider now the case of sigmoidal activation, where
σ(·)
is monotonic with limz→−∞ σ(z) = c and
limz→+∞ σ(z) = C for some c(cid:54)=C in R. Letting w ∈ Rs
and b1. . .bM ∈ R be as above, we introduce a scaling fac-
tor α > 0, and deﬁne fθj (x) = σ(αw(cid:62)x + αbj) for every
j ∈ [M ]. It is not difﬁcult to see that as α → +∞, the
matrix F tends closer and closer to a matrix holding C on
and below its diagonal, and c elsewhere. The latter matrix
is non-singular, and in particular has non-zero determinant
d (cid:54)= 0. The determinant of F converges to d as α → +∞,
so for large enough α, F is non-singular.

Non-degeneracy means that given distinct templates, one
may choose representation functions for which F is non-
singular. We may as well consider the opposite situation,
where we are given representation functions, and would like
to choose templates leading to non-singular F . Apparently,
so long as the representation functions are linearly indepen-
dent, this is always possible:
Claim 2. Let fθ1. . .fθM : Rs → R be any linearly indepen-
dent continuous functions. Then, there exist x(1) . . . x(M ) ∈
Rs such that F (eq. 4) is non-singular.
Proof. We may view the determinant of F (eq. 4) as a func-
tion of (x(1), . . . , x(M )):

det F (x(1), . . . , x(M )) =

sign(δ)

fθδ(i)(x(i))

(cid:88)

δ∈SM

M(cid:89)

i=1

tions {(x(1), . . . , x(M )) (cid:55)→ (cid:81)M

where SM stands for the permutation group on [M ],
and sign(δ) ∈ {±1} is the sign of
the permuta-
tion δ. This in particular shows that det F (x(1), . . . , x(M ))
is a non-zero linear combination of the product func-
(x(i))}d1...dM∈[M ].
Since these product functions are linearly independent (see
sec. 5.1), det F (x(1), . . . , x(M )) cannot be the zero func-
tion. That is to say, there exist x(1) . . . x(M ) ∈ Rs such that
det F (x(1), . . . , x(M )) (cid:54)= 0.

i=1 fθdi

As stated previously, the analysis carried out in this paper
studies score functions expressible by ConvNets through
the notion of grid tensors. The translation of score func-
tions into grid tensors is facilitated by the choice of tem-
plates x(1) . . . x(M ) ∈ Rs (eq. 3). For general templates,
the correspondence between score functions and grid ten-
sors is not injective – a score function corresponds to a
single grid tensor, but a grid tensor may correspond to
more than one score function. We use the term cover-
ing to refer to templates leading to an injective correspon-
dence, i.e. to a situation where two score functions asso-
ciated with the same grid tensor are effectively identical.
In other words, the templates x(1) . . . x(M ) are covering if

7

Figure 3. Shallow ConvNet with conv receptive ﬁeld expanded
from 1 × 1 to w×h. The weight vectors ai,z ∈ RM have been
replaced by matrices Ai,z ∈ RM×w·h, and we denote by ρ(j; i)
the spatial location of element j in the w×h window revolving
around i. Best viewed in color.

grid(cid:8)Xd1...dN := (x(d1), . . . , x(dN )) : d1. . .dN ∈ [M ](cid:9) is

the value of score functions outside the exponentially large

irrelevant for classiﬁcation. Some of the claims in our
analysis will assume existence of covering templates (it
will be stated explicitly when so). We argue in app. A
that for structured compositional data (e.g. natural images),
M ∈ Ω(100) sufﬁces in order for this assumption to hold.
5.3. Matricization

j=i+1 M2j.

in row index 1 +(cid:80)N/2
umn index 1 +(cid:80)N/2

i=1(d2i−1 − 1)(cid:81)N/2
i=1(d2i − 1)(cid:81)N/2

When analyzing grid tensors, we will often consider their
arrangement as matrices. The matricization of a tensor A,
denoted [A], is its arrangement as a matrix with rows corre-
sponding to odd modes and columns corresponding to even
modes. Speciﬁcally, if A ∈ RM1×···×MN , and assuming
for simplicity that the order N is even, the matricization
[A] ∈ R(M1·M3·...·MN−1)×(M2·M4·...·MN ) holds Ad1,...,dN
j=i+1 M2j−1 and col-
The matrix analogy of the tensor product ⊗ (eq. 1) is
called the Kronecker product, and is denoted by (cid:12). For
A ∈ RM1×M2 and B ∈ RN1×N2, A(cid:12)B is the matrix in
RM1N1×M2N2 holding AijBkl in row index (i − 1)N1 + k
and column index (j − 1)N2 + l. The relation [A ⊗ B] =
[A] (cid:12) [B], where A and B are arbitrary tensors of even or-
der, implies that the tensor and Kronecker products are in-
deed analogous, i.e. they represent the same operation un-
der tensor and matrix viewpoints, respectively. We gen-
eralize the Kronecker product analogously to our general-
ization of the tensor product (eq. 2). For an associative
and commutative binary operator g(·,·), the generalized
Kronecker product (cid:12)g, is an operator that intakes matri-
ces A ∈ RM1×M2 and B ∈ RN1×N2, and returns a matrix
A(cid:12)gB ∈ RM1N1×M2N2 holding g(Aij, Bkl) in row index
(i− 1)N1 + k and column index (j − 1)N2 + l. The relation
between the tensor and Kronecker products holds for their
generalized versions as well, i.e. [A ⊗g B] = [A] (cid:12)g [B] for
arbitrary tensors A,B of even order.

,direpidfxinputrepresentationwxh convglobal poolingdense (output)hidden layerixMZZY,,11,;,MwhzidjdjconvizArepjidcoversspace,ipoolzPconviz,:youtypoolaXEquipped with the matricization operator [·] and the gen-
eralized Kronecker product (cid:12)g, we are now in a position
to translate the generalized HT decomposition (eq. 7) to an
expression for the matricization of a grid tensor generated
by the deep ConvNet:

φ1,j,γ =
···

φl,j,γ(cid:105)
(cid:104)
φL−1,j,γ(cid:105)
(cid:104)
(cid:104)A(cid:16)
(cid:17)(cid:105)

hD
y

=

···

=

=

a1,j,γ
α

al,j,γ
α

r0(cid:88)
rl−1(cid:88)

α=1

α=1

rL−2(cid:88)
rL−1(cid:88)

α=1

α=1

(F a0,2j−1,α) ⊗g (F a0,2j,α)

(9)

M 2l−2

(cid:12)g

-by-M 2l−2

φl−1,2j−1,α(cid:105)
(cid:104)
(cid:123)(cid:122)
(cid:125)
(cid:124)
φL−2,2j−1,α(cid:105)
(cid:104)
(cid:124)
(cid:123)(cid:122)
(cid:125)
φL−1,1,α(cid:105)
(cid:104)
(cid:124)
(cid:123)(cid:122)
(cid:125)

M N/8-by-M N/8

(cid:12)g

M N/4-by-M N/4

M 2l−2

(cid:104)
(cid:124)

-by-M 2l−2

φl−1,2j,α(cid:105)
(cid:123)(cid:122)
(cid:125)
φL−2,2j,α(cid:105)
(cid:104)
(cid:124)
(cid:123)(cid:122)
(cid:125)
φL−1,2,α(cid:105)
(cid:125)
(cid:123)(cid:122)

M N/8-by-M N/8

(cid:12)g

(cid:104)
(cid:124)

M N/4-by-M N/4

aL−1,j,γ

α

aL,1,y
α

We refer to this factorization as the matricized general-
ized HT decomposition. Notice that the expression above
for φ1,j,γ is the same as in the original generalized HT de-
composition, as order-2 tensors need not be matricized.

For the matricization of a grid tensor generated by the
shallow ConvNet, we translate the generalized CP decom-
position (eq. 6) into the matricized generalized CP decom-
position:

(cid:104)A(cid:16)

(cid:17)(cid:105)

hS
y

=

Z(cid:88)

z=1

z ·(cid:16)
(cid:16)

ay

(F az,1) (cid:12)g (F az,3) (cid:12)g ··· (cid:12)g (F az,N−1)

(F az,2) (cid:12)g (F az,4) (cid:12)g ··· (cid:12)g (F az,N )

(10)

(cid:17) (cid:12)g
(cid:17)(cid:62)

The matricized generalized CP and HT decompositions
(eq. 10 and 9 respectively) will be used throughout our
proofs to establish depth efﬁciency. This is generally done
by providing a lower bound on rank[A(hD
y )] – the rank
of the deep ConvNet’s matricized grid tensor, and an upper
bound on rank[A(hS
y )] – the rank of the shallow ConvNet’s
matricized grid tensor. The upper bound on rank[A(hS
y )]
will be linear in Z, and so requiring A(hS
y ) = A(hD
y ), and
y )] = rank[A(hD
in particular rank[A(hS
y )], will give us a
lower bound on Z. That is to say, we obtain a lower bound
on the number of hidden channels in the shallow ConvNet,
that must be met in order for this network to replicate a
grid tensor generated by the deep ConvNet. Our analysis of
depth efﬁciency is given in sec. 5.5. As a prerequisite, we
ﬁrst head on to sec. 5.4 to analyze universality.

8

5.4. Universality

Universality refers to the ability of a network to realize
(or approximate) any function of choice when no restric-
tions are imposed on its size. It is well-known that fully-
connected neural networks are universal under all types of
non-linear activations typically used in practice, even if the
number of hidden layers is restricted to one ([6, 12, 18]).
To the best of our knowledge universality has never been
studied in the context of convolutional rectiﬁer networks.
This is the purpose of the current section. Speciﬁcally, we
analyze the universality of our shallow and deep ConvNets
(ﬁg. 2, and ﬁg. 1 with L = log2 N, respectively) under
ReLU activation and max or average pooling.

We begin by stating a result similar to that given in [5],
according to which convolutional arithmetic circuits are
universal:
Claim 3. Assuming covering templates exist, with linear
activation and product pooling the shallow ConvNet is uni-
versal (hence so is the deep).
Proof. Let x(1) . . . x(M ) ∈ Rs be distinct covering tem-
plates, and fθ1. . .fθM be representation functions for which
F is invertible (non-degeneracy implies that such functions
exist). With linear activation and product pooling the gen-
eralized CP decomposition (eq. 6) reduces to its standard
version, which is known to be able to express any tensor
when size is large enough (e.g. Z≥M N sufﬁces). The shal-
low ConvNet can thus realize any grid tensor on covering
templates, precisely meaning that it is universal. As for
the deep ConvNet, setting r0 = ··· = rL−1 = Z and
α = 1 [α = γ], where l ∈ [L − 1] and 1 [·] is the in-
al,j,γ
dicator function, reduces its decomposition (eq. 7) to that of
the shallow ConvNet (eq. 6). This implies that all grid ten-
sors realizable by the shallow ConvNet are also realizable
by the deep ConvNet.

Heading on to convolutional rectiﬁer networks, the fol-
lowing claim tells us that max pooling leads to universality:
Claim 4. Assuming covering templates exist, with ReLU ac-
tivation and max pooling the shallow ConvNet is universal
(hence so is the deep).
Proof. The proof follows the same line as that of claim 3,
except we cannot rely on the ability of the standard CP de-
composition to realize any tensor of choice.
Instead, we
need to show that the generalized CP decomposition (eq. 6)
with g(a, b) = max{a, b, 0} can realize any tensor, so long
as Z is large enough. We will show that Z≥2·M N sufﬁces.
For that, it is enough to consider an arbitrary indicator ten-
sor, i.e. a tensor holding 1 in some entry and 0 in all other
entries, and show that it can be expressed with Z = 2.
Let A be an indicator tensor of order N and dimension
M in each mode, its active entry being (d1, . . . , dN ). De-
note by 1 ∈ RM the vector holding 1 in all entries, and

for every i ∈ [N ], let ¯edi ∈ RM be the vector hold-
ing 0 in entry di and 1 elsewhere. With the following
weight settings, a generalized CP decomposition (eq. 6)
with g(a, b) = max{a, b, 0} and Z = 2 produces A, as
required:
• ay
• a1,1 = ··· = a1,N = 1
• ∀i ∈ [N ] : a2,i = ¯edi

2 = −1

1 = 1, ay

Turning to the matricized generalized HT decomposition
(eq. 9), which corresponds to the deep ConvNet, we have
with g(a, b) = max{a, 0} + max{b, 0}:

(cid:2)A(cid:0)hD

(cid:1)(cid:3) = V (cid:12)O + O(cid:12)U

y

where (cid:12) is the standard Kronecker product (see deﬁnition
in sec. 5.3), O ∈ RM N/4×M N/4 is a matrix holding 1 in all
entries, and the matrices V, U ∈ RM N/4×M N/4 are given
by:

At this point we encounter the ﬁrst somewhat surprising
result, according to which convolutional rectiﬁer networks
are not universal with average pooling:

V

:=

U :=

aL,1,y

α max(cid:8)(cid:2)φL−1,1,α(cid:3) , 0(cid:9)
α max(cid:8)(cid:2)φL−1,2,α(cid:3) , 0(cid:9)

aL,1,y

rL−1(cid:88)
rL−1(cid:88)

α=1

α=1

Claim 5. With ReLU activation and average pooling, both
the shallow and deep ConvNets are not universal.
Proof. Let x(1) . . . x(M ) ∈ Rs be any templates of choice,
and consider grid tensors produced by the generalized CP
and HT decompositions (eq. 6 and 7 respectively) with
g(a, b) = max{a, 0} + max{b, 0} (this corresponds to sum
pooling and ReLU activation, but as stated in sec. 4, sum
and average pooling are equivalent in terms of expressive-
ness). We will show that such grid tensors, when arranged
as matrices, necessarily have low rank. This obviously im-
plies that they cannot take on any value. Moreover, since
the set of low rank matrices has zero measure in the space
of all matrices (see sec. 5.1), the set of values that can be
taken by the grid tensors has zero measure in the space of
tensors with order N and dimension M in each mode.
In accordance with the above, we complete our proof by
showing that with g(a, b) = max{a, 0} + max{b, 0}, the
matricized generalized CP and HT decompositions (eq. 10
and 9 respectively) give rise to low-rank matrices. For the
matricized generalized CP decomposition (eq. 10), corre-
sponding to the shallow ConvNet, we have with g(a, b) =
max{a, 0} + max{b, 0}:

(cid:2)A(cid:0)hS

y

(cid:1)(cid:3) = v1(cid:62) + 1u(cid:62)

where 1 is the vector in RM N/2 holding 1 in all entries, and
v, u ∈ RM N/2 are deﬁned as follows:

Z(cid:88)
Z(cid:88)

z=1

z · max(cid:8)(F az,1) (cid:12)g ··· (cid:12)g (F az,N−1), 0(cid:9)
z · max(cid:8)(F az,2) (cid:12)g ··· (cid:12)g (F az,N ), 0(cid:9)
(cid:1)(cid:3) ∈ RM N/2×M N/2 has rank 2

ay

u :=

z=1

Obviously the matrix(cid:2)A(cid:0)hS

y

or less.

v :=

ay

The rank of O is obviously 1, and since the Kro-
i.e. rank(A(cid:12)B) =
necker product multiplies ranks,
rank(A)·rank(B) for any matrices A and B, we have that

(cid:1)(cid:3) ∈ RM N/2×M N/2 is at most 2·M N/4.
(cid:1)(cid:3) cannot have full rank.

the rank of(cid:2)A(cid:0)hD
In particular,(cid:2)A(cid:0)hD

y

y

One may wonder if perhaps the non-universality of
ReLU activation and average pooling is merely an artifact
of the conv operator in our ConvNets having 1× 1 receptive
ﬁeld. Apparently, as the following claim shows, expand-
ing the receptive ﬁeld does not remedy the situation, and
indeed non-universality is an inherent property of convolu-
tional rectiﬁer networks with average pooling:
Claim 6. Consider the network illustrated in ﬁg. 3, ob-
tained by expanding the conv receptive ﬁeld in the shallow
ConvNet from 1×1 to w×h, where w·h < N/2+1−logM N
(conv windows cover less than half the feature maps that
precede them). Such a network, when equipped with ReLU
activation and average pooling, is not universal.

Proof. Compare the original shallow ConvNet (ﬁg. 2) to the
shallow ConvNet with expanded receptive ﬁeld that we con-
sider in this claim (ﬁg. 3). The original shallow ConvNet
has 1× 1 receptive ﬁeld, with conv entry in location i ∈ [N ]
and channel z ∈ [Z] assigned through a cross-channel linear
combination of the representation entries in the same loca-
tion, the combination weights being az,i ∈ RM . In the shal-
low ConvNet with receptive ﬁeld expanded to w×h, linear
combinations span multiple locations. In particular, conv
entry in location i and channel z is now assigned through a
linear combination of the representation entries at all chan-
nels that lie inside a spatial window revolving around i. We
denote by {ρ(j; i)}j∈[w·h] the locations comprised by this
window. More speciﬁcally, ρ(j; i) is the j’th location in the
window, and the linear weights that correspond to it are held
in the j’th column of the weight matrix Az,i ∈ RM×w·h.
We assume for simplicity that conv windows stepping out of

9

bounds encounter zero padding 3, and adhere to the conven-
tion under which indexing the row of a matrix with dρ(j;i)
produces zero when location j of window i steps out of
bounds.
We are interested in the case of ReLU activation
(σ(z) = max{0, z}) and average pooling (P{cj} =
mean{cj}). Under this setting, for any selected templates
x(1) . . . x(M ) ∈ Rs, the grid tensor of hS(w×h)
– network’s
y’th score function, is given by:

y

A(hS(w×h)

y

)d1,...,dN =

Bi
dρ(1;i),...,dρ(w·h;i)

where for every i ∈ [N ], Bi is a tensor of order w·h and
dimension M in each mode, deﬁned by:



i=1

N(cid:88)
 w·h(cid:88)
N(cid:88)

j=1

Z(cid:88)

z=1

Bi
c1,...,cw·h

=

ay
z
N

max

(F Az,i)cj ,j, 0

Let O be a tensor of order N − w·h and dimension M in
each mode, holding 1 in all entries. We may write:

A(hS(w×h)

y

) =

pi(Bi ⊗ O)

(11)

i=1

where for every i ∈ [N ], pi(·) is an appropriately chosen
operator that permutes the modes of an order-N tensor.

y

We now make use of some known facts related to tensor
rank (see sec. 5.1), in order to show that eq. 11 is not univer-
sal, i.e. that there are many tensors which cannot be realized
by A(hS(w×h)
). Being tensors of order w·h and dimen-
sion M in each mode, the ranks of B1 . . .BN are bounded
above by M w·h−1. Since O is an all-1 tensor, and since per-
muting modes does not alter rank, we have: rank(pi(Bi ⊗
O))≤M w·h−1 ∀i ∈ [N ]. Finally, from sub-additivity of the
rank we get: rank(A(hS(w×h)
))≤N·M w·h−1. Now, we
know by assumption that w·h < N/2 + 1 − logM N, and
this implies: rank(A(hS(w×h)
)) < M N/2. Since there ex-
ist tensors of order N and dimension M in each mode hav-
ing rank at least M N/2 (actually only a negligible set of ten-
sors do not meet this), eq. 11 is indeed not universal. That
is to say, the shallow ConvNet with conv receptive ﬁeld ex-
panded to w×h (ﬁg. 3) cannot realize all grid tensors on the
templates x(1) . . . x(M ).

y

y

We conclude this section by noting that

the non-
universality result
the
known universality of shallow (single hidden layer) fully-
connected neural networks.
Indeed, a shallow fully-
connected network corresponds to the ConvNet considered

in claim 6 does not contradict

3 Modifying our proof to account for different padding schemes (such
as duplication or no padding at all) is trivial – we choose to work with zero
padding merely for notational convenience.

10

Figure 4. Shallow fully-connected network obtained by expand-
ing the conv receptive ﬁeld in the shallow ConvNet to cover the en-
tire spatial extent. The hidden layer consists of a Z-channel dense
linear operator weighted by {Az ∈ RN×M}z∈[Z], and followed
by point-wise activation σ(·). The resulting Z-dimensional vector
is mapped to Y network outputs through a dense linear operator
weighted by {ay ∈ RZ}y∈[Y ]. Best viewed in color.

in claim 6 (ﬁg. 3) with conv receptive ﬁeld covering the
entire spatial extent (w·h = N), thereby effectively re-
moving the pooling operator (assuming the latter realizes
the identity on singletons). In claim 7 below we show that
such a network, when equipped with ReLU activation, is
universal. On the other hand, in claim 6 we assumed that
the receptive ﬁeld covers less than half the spatial extent
(w·h < N/2 + 1 − logM N), and have shown that with
ReLU activation and average pooling, this leads to non-
universality. Loosely speaking, our ﬁndings imply that for
networks with ReLU activation, which are known to be uni-
versal when fully-connected, introducing locality disrupts
universality with average pooling (and maintains it with
max pooling).

exist

there

covering

Claim 7. Assume
templates
x(1) . . . x(M ), and corresponding representation func-
tions fθ1 . . .fθM leading to a matrix F (eq. 4) that has
non-recurring rows and a constant non-zero column 4.
Consider the fully-connected network illustrated in ﬁg. 4,
obtained by expanding the conv receptive ﬁeld in the
shallow ConvNet to cover the entire spatial extent. Such a
network, when equipped with ReLU activation, is universal.

y

Proof. Let hS(f c)
be the y’th score function of our shallow
fully-connected network (ﬁg. 4) when equipped with ReLU
activation (σ(z) = max{0, z}). We would like to show
4 The assumption that such representation functions exist differs from
our usual non-degeneracy assumption. The latter requires F to be non-
singular, whereas here we pose the weaker requirement of F having non-
recurring rows. On the other hand, here we also demand that F have a
constant non-zero column, i.e. that there be a representation function fθd
such that fθd (x(1)) = · · · = fθd (x(M )) = c (cid:54)= 0.
In claim 1 we
showed that standard neurons meet the non-degeneracy assumption. A
slight modiﬁcation to its proof shows that they also meet the assumption
made here. Namely, if we modify the constructions for the cases of ReLU
activation and sigmoidal activation by setting fθ1 (x) = σ(0(cid:62)x + 1) and
fθ1 (x) = σ(0(cid:62)x + α) respectively, we get matrices F that are not only
non-singular, but also have a constant non-zero column.

,direpidfxinputrepresentationdense(hidden)dense (output)hidden layerixMZY,11,idNMzidhiddenzArepid,:youtyhiddenaXwhere (cid:104)·,·(cid:105) stands for the inner-product operator,

i.e.

The fact that:

A(hS(f c)

y

)d1...dN =

ay
z max

Z(cid:88)

z=1

(cid:110)

0,

(cid:68) ˜F (d1...dN ), ˜Az(cid:69)

(cid:111)

+ bz

For i > 1 we have:

w(cid:62)
j vi + bj =

y

) – the grid tensor of hS(f c)

that A(hS(f c)
w.r.t. the cover-
ing templates x(1) . . . x(M ), may take on any value when
hidden and output weights ({Az}z∈[Z] and ay respectively)
are chosen appropriately.

For any d1. . .dN ∈ [M ], deﬁne the following matrix:

y

 fθ1(x(d1))

...

fθ1(x(dN ))

 ∈ RN×M

···
...
···

fθM (x(d1))

...

fθM (x(dN ))

F (d1...dN ) :=

In words, F (d1...dN ) is the matrix obtained by taking rows
d1. . .dN from F (recurrence allowed), and stacking them
one on top of the other. It holds that:

(cid:110)

F (d1...dN ), Az(cid:69)(cid:111)
(cid:68)

ay
z max

0,

A(hS(f c)

Z(cid:88)
(cid:10)F (d1...dN ), Az(cid:11) :=(cid:80)N

)d1...dN =

z=1

y

(cid:80)M

i=1

d=1 F (d1...dN )

i,d

Az

i,d.

By assumption F has a constant non-zero column. This
implies that there exist j ∈ [M ], c (cid:54)= 0 such that for any
d1. . .dN ∈ [M ], all entries in column j of F (d1...dN ) are
equal to c. For every d1. . .dN ∈ [M ] and z ∈ [Z], denote
by ˜F (d1...dN ) and ˜Az the matrices obtained by removing the
j’th column from F (d1...dN ) and Az respectively. Deﬁning
b ∈ RZ to be the vector whose z’th entry is given by bz =

c ·(cid:80)N

i=1 Az

i,j, we may write:

y

1...d(cid:48)

1. . .d(cid:48)

N ) for (d1. . .dN ) (cid:54)= (d(cid:48)

noting that for every z ∈ [Z], ˜Az and bz may take on
any values with proper choice of Az. Since by assump-
tion F has non-recurring rows, and since all rows hold the
same value (c) in their j’th entry, we have that ˜F (d1...dN ) (cid:54)=
˜F (d(cid:48)
N ). An application of
lemma 1 now shows that when Z≥M N , any value for the
grid tensor A(hS(f c)
) may be realized with proper assign-
ment of { ˜Az}z∈[Z], b and ay. Since { ˜Az}z∈[Z] and b may
be set arbitrarily through {Az}z∈[Z], we get that with proper
choice of hidden and output weights ({Az}z∈[Z] and ay re-
spectively), the grid tensor of our network w.r.t. the cover-
ing templates may take on any value, precisely meaning that
universality holds.
Lemma 1. Let v1 . . . vk ∈ RD be distinct vectors (vi (cid:54)= vj
for i(cid:54)=j), and c1. . .ck ∈ R be any scalars. Then, there exist
w1 . . . wk ∈ RD, b ∈ Rk and a ∈ Rk such that ∀i ∈ [k]:

k(cid:88)

aj max{0, w(cid:62)

j vi + bj} = ci

(12)

j=1

11

Proof. As shown in the proof of claim 1, for distinct
v1 . . . vk ∈ RD there exists a vector u ∈ RD such that
u(cid:62)vi (cid:54)= u(cid:62)vj for all 1≤i < j≤k. We assume without loss
of generality that u(cid:62)v1 < . . . < u(cid:62)vk, and set w1. . .wk,
b and a as follows:

• w1 = ··· = wk = u
• b1 = −u(cid:62)v1 + 1
• bj = −u(cid:62)vj−1 for j = 2. . .k
• a1 = c1
• aj =

−(cid:80)j−1

cj−cj−1

u(cid:62)vj−u(cid:62)vj−1

t=1 at for j = 2. . .k

To complete the proof, we show below that this assignment
meets the condition in eq. 12 for i = 1. . .k.

(cid:26) u(cid:62)v1 − u(cid:62)v1 + 1 = 1 , j = 1

u(cid:62)v1 − u(cid:62)vj−1 < 0

, 2≤j≤k

w(cid:62)
j v1 + bj =

implies that the condition in eq. 12 indeed holds for i = 1:

aj max{0, w(cid:62)

aj·0 = a1 = c1

k(cid:88)

j=1

j v1 + bj} = a1·1 +

k(cid:88)
 u(cid:62)vi − u(cid:62)v1 + 1 > 0

u(cid:62)vi − u(cid:62)vj−1 > 0
u(cid:62)vi − u(cid:62)vj−1 ≤ 0

j=1

, j = 1
, 2≤j≤i
, i < j≤k

j vi + bj} =
j=2 aj(u(cid:62)vi − u(cid:62)vj−1)
Comparing this to the same expression with i replaced by

j=1 aj max{0, w(cid:62)

which implies:(cid:80)k
a1(u(cid:62)vi − u(cid:62)v1 + 1) +(cid:80)i
i − 1 we obtain:(cid:80)k
(cid:80)k
j=1 aj max{0, w(cid:62)
(u(cid:62)vi − u(cid:62)vi−1)(cid:80)i
j=1 aj max{0, w(cid:62)

j vi + bj} =
j vi−1 + bj}+

j=1 aj

if we follow an inductive argument and assume
i.e. that

Now,
that the condition in eq. 12 holds for i − 1,
j vi−1 + bj} = ci−1, we get:

(cid:80)k
j=1 aj max{0, w(cid:62)
(cid:80)k
ci−1 + (u(cid:62)vi − u(cid:62)vi−1)(cid:80)i
j=1 aj max{0, w(cid:62)

j vi + bj} =
j=1 aj

Plugging in the deﬁnition ai =
gives:

(cid:80)k
j=1 aj max{0, w(cid:62)
ci−1 + (u(cid:62)vi − u(cid:62)vi−1)

ci−ci−1

u(cid:62)vi−u(cid:62)vi−1

j vi + bj} =
u(cid:62)vi−u(cid:62)vi−1

ci−ci−1

−(cid:80)i−1

j=1 aj

= ci

Thus the condition in eq. 12 holds for i as well. We
have therefore shown by induction that our assignment of
w1. . .wk, b and a meets the lemma’s requirement.
5.5. Depth Efﬁciency

The driving force behind deep learning is the expres-
sive power that comes with depth. It is generally believed
that deep networks with non-linear layers efﬁciently express
functions that cannot be efﬁciently expressed by shallow
networks, i.e. that would require the latter to have super-
polynomial size. We refer to such scenario as depth efﬁ-
ciency. Being concerned with the minimal size required
by a shallow network in order to realize (or approximate)
a given function, the question of depth efﬁciency implicitly
assumes universality, i.e. that there exists some (possibly
exponential) size with which the shallow network is capa-
ble of expressing the target function.

To the best of our knowledge, at the time of this writing
the only work to formally analyze depth efﬁciency in the
context of ConvNets is [5]. This work focused on convolu-
tional arithmetic circuits, showing that with such networks
depth efﬁciency is complete, i.e. besides a negligible set,
all functions realizable by a deep network enjoy depth efﬁ-
ciency. We frame this result in our setup:
Claim 8 (adaptation of theorem 1 in [5]). Let fθ1 . . .fθM be
any set of linearly independent representation functions for
a deep ConvNet (ﬁg. 1 with L = log2 N) with linear activa-
tion and product pooling. Suppose we randomize the linear
weights (al,j,γ) of the network by some continuous distri-
bution. Then, with probability 1, we obtain score functions
that cannot be realized by a shallow ConvNet (ﬁg. 2) with
linear activation and product pooling if the number of hid-
den channels in the latter (Z) is less than min{r0, M}N/2.
Proof. Let x(1) . . . x(M ) ∈ Rs be templates such that F
is invertible (existence follows from claim 2). The deep
network generates grid tensors on x(1) . . . x(M ) through the
standard HT decomposition (eq. 7 with g(a, b) = a·b). The
proof of theorem 1 in [5] shows that when arranged as ma-
trices, such tensors have rank at least min{r0, M}N/2 al-
most always, i.e. for all weight (al,j,γ) settings but a set
of (Lebesgue) measure zero. On the other hand, the shal-
low network generates grid tensors on x(1) . . . x(M ) through
the standard CP decomposition (eq. 6 with g(a, b) = a·b),
possibly with a different matrix F (representation functions
need not be the same). Such tensors, when arranged as ma-
trices, are shown in the proof of theorem 1 in [5] to have

12

rank at most Z. Therefore, for them to realize the grid ten-
sors generated by the deep network, we almost always must
have Z ≥ min{r0, M}N/2.

We now turn to convolutional rectiﬁer networks, for
which depth efﬁciency has yet to be analyzed. In sec. 5.4
we saw that convolutional rectiﬁer networks are universal
with max pooling, and non-universal with average pooling.
Since depth efﬁciency is only applicable to universal archi-
tectures, we focus on the former setting. The following
claim establishes existence of depth efﬁciency for ConvNets
with ReLU activation and max pooling:

2

Claim 9. There exist weight settings for a deep ConvNet
with ReLU activation and max pooling, giving rise to score
functions that cannot be realized by a shallow ConvNet with
ReLU activation and max pooling if the number of hidden
channels in the latter (Z) is less than min{r0, M}N/2·
M·N .
Proof. The proof traverses along the following path. Let-
ting x(1) . . . x(M ) ∈ Rs be any distinct templates, we
show that when arranged as matrices, grid tensors on
x(1) . . . x(M ) generated by the shallow network have rank
at most Z· M·N
. Then, deﬁning fθ1 . . .fθM to be represen-
tation functions for the deep network giving rise to an in-
vertible F (non-degeneracy implies that such functions ex-
ist), we show explicit linear weight (al,j,γ) settings under
which the grid tensors on x(1) . . . x(M ) generated by the
deep network, when arranged as matrices, have rank at least
min{r0, M}N/2.

2

In light of the above, the proof boils down to showing

that with g(a, b) = max{a, b, 0}:

• The matricized generalized CP decomposition (eq. 10)

produces matrices with rank at most Z· M·N

.

2

• For an invertible F , there exists a weight (al,j,γ) set-
ting under which the matricized generalized HT de-
composition (eq. 9) produces a matrix with rank at
least min{r0, M}N/2.
We begin with the ﬁrst point, showing that for every

v1, . . . , vN/2 ∈ RM and u1, . . . , uN/2 ∈ RM :

rank

v1 (cid:12)g ··· (cid:12)g v N

u1 (cid:12)g ··· (cid:12)g u N

2

2

2

2
(13)
This would imply that every summand in the matricized
generalized CP decomposition (eq. 10) has rank at most
M·N
, and the desired result readily follows. To prove
eq. 13, note that each of the vectors ¯v := v1 (cid:12)g ··· (cid:12)g v N
and ¯u := u1(cid:12)g···(cid:12)g u N
are of dimension M N/2, but have
unique values. Let δv, δu : [M N/2] →
only up to M·N
[M N/2] be permutations that arrange the entries of ¯v and
¯u in descending order. Permuting the rows of the matrix

2

2

2

(cid:16)

(cid:16)

(cid:17)(cid:12)g

(cid:17)(cid:62) ≤ M·N

¯v(cid:12)g ¯u(cid:62) via δv, and the columns via δu, obviously does not
change its rank. On the other hand, we get a M N/2×M N/2
matrix with a M·N
block structure, each block be-
ing constant (i.e. all entries of a block hold the same value).
This implies that the rank of ¯v(cid:12)g ¯u(cid:62) is at most M·N
, which
is what we set out to prove.

2 × M·N

2

2

Moving on to the matricized generalized HT decompo-
sition (eq. 9), for an invertible F we deﬁne the following
weight setting (0 and 1 here denote the all-0 and all-1 vec-
tors, respectively):

• a0,j,γ =

, where ¯eγ ∈ RM is
deﬁned to be the vector holding 0 in entry γ and 1 in
all other entries.

0

(cid:26) F −1¯eγ
(cid:26) 1 , γ = 1 , l ∈ [L − 1]

, γ≤M
, γ > M

0 , γ > 1 , l ∈ [L − 1]

• al,j,γ =

• aL,1,y = 1

Under this setting, the produced matrix (cid:2)A(cid:0)hD

(cid:1)(cid:3) holds

min{r0, M} everywhere besides min{r0, M}N/2 entries on
its diagonal, where it holds min{r0, M} − 1. The rank of
this matrix is at least min{r0, M}N/2.

y

Nearly all results in the literature that relate to depth efﬁ-
ciency merely show its existence, and claim 9 is no different
in that respect. From a practical perspective, the implica-
tions of such results are slight, as a-priori, it may be that
only a small fraction of the functions realizable by a deep
network enjoy depth efﬁciency, and for all the rest shallow
networks sufﬁce.
In sec. 5.5.2 we extend claim 9, argu-
ing that with ReLU activation and max pooling, depth efﬁ-
ciency becomes more and more prevalent as the number of
hidden channels in the deep ConvNet grows. However, no
matter how large the deep ConvNet is, with ReLU activation
and max pooling depth efﬁciency is never complete – there
is always positive measure to the set of weight conﬁgura-
tions that lead the deep ConvNet to generate score functions
efﬁciently realizable by the shallow ConvNet:

Claim 10. Suppose we randomize the weights of a
deep ConvNet with ReLU activation and max pooling by
some continuous distribution with non-vanishing continu-
ous probability density function. Then, assuming covering
templates exist, with positive probability, we obtain score
functions that can be realized by a shallow ConvNet with
ReLU activation and max pooling having only a single hid-
den channel (Z = 1).
Proof. Let x(1) . . . x(M ) ∈ Rs be covering templates, and
fθ1 . . .fθM be representation functions for the deep network
under which F is invertible (non-degeneracy implies that
such functions exist). We will show that there exists a lin-
ear weight (al,j,γ) setting for the deep network with which

it generates a grid tensor that is realizable by a shallow net-
work with a single hidden channel (Z = 1). Moreover, we
show that when the representation parameters (θd) and lin-
ear weights (al,j,γ) are subject to small perturbations, the
deep network’s grid tensor can still be realized by a shal-
low network with a single hidden channel. Since templates
are covering grid tensors fully deﬁne score functions. This,
along with the fact that open sets in Lebesgue measure
spaces always have positive measure (see sec. 5.1), imply
that there is positive measure to the set of weight conﬁgura-
tions leading the deep network to generate score functions
realizable by a shallow network with Z = 1. Translating
the latter statement from measure theoretical to probabilis-
tic terms readily proves the result we seek after.

In light of the above, the proof boils down to the fol-
lowing claim, framed in terms of our generalized tensor de-
compositions. Fixing g(a, b) = max{a, b, 0}, per arbitrary
invertible F there exists a weight (al,j,γ) setting for the gen-
eralized HT decomposition (eq. 7), such that the produced
tensor may be realized by the generalized CP decomposi-
tion (eq. 6) with Z = 1, and this holds even if the weights
al,j,γ and matrix F are subject to small perturbations 5.

We will now show that the following weight setting
meets our requirement (0 and 1 here denote the all-0 and
all-1 vectors, respectively):

(cid:26) F −11 , j odd
(cid:26) 1 , j odd , l ∈ [L − 1]

, j even

0 , j even , l ∈ [L − 1]

0

• a0,j,γ =

• al,j,γ =

• aL,1,y = 1

Let E F be an additive noise matrix applied to F ,
and {l,j,γ}l,j,γ be additive noise vectors applied to
{al,j,γ}l,j,γ. We use the notation o() to refer to vectors
that tend to 0 as E F → 0 and l,j,γ → 0, with the dimen-
sion of a vector to be understood by context. Plugging in
the noisy variables into the generalized HT decomposition
(eq. 7), we get for every j ∈ [N/2] and α ∈ [r0]:
((F + E F )(a0,2j−1,α + 0,2j−1,α))
⊗g((F + E F )(a0,2j,α + 0,2j,α))
= ((F + E F )(F −11 + 0,2j−1,α))

⊗g((F + E F )(0 + 0,2j,α))

= (1 + o()) ⊗g o()

If the applied noise (E F , l,j,γ) is small enough this is equal
to (1 + o()) ⊗ 1 (recall that ⊗ stands for the standard
5 Recall that by assumption representation functions are continuous
w.r.t. their parameters (fθ(x) is continuous w.r.t. θ), and so small perturba-
tions on representation parameters (θd) translate into small perturbations
on the matrix F (eq. 4).

13

tensor product), and we in turn get for every j ∈ [N/4] and
γ ∈ [r1]:

=(cid:0)(cid:80)r0
(cid:0)(cid:80)r0
=(cid:0)(cid:80)r0
(cid:0)(cid:80)r0

⊗g

φ1,2j−1,γ ⊗g φ1,2j,γ
α=1 a1,2j−1,γ
α=1 a1,2j,γ

α

α

(1 + o()) ⊗ 1(cid:1)
(1 + o()) ⊗ 1(cid:1)
)(1 + o()) ⊗ 1(cid:1)
(1 + o()) ⊗ 1(cid:1)

⊗g
= ((r01 + o()) ⊗ 1) ⊗g (o() ⊗ 1)

α=1 1,2j,γ

α

α=1(1 + 1,2j−1,γ

α

With the applied noise (E F , l,j,γ) small enough this be-
comes (r01 + o() ⊗ 1 ⊗ 1 ⊗ 1. Continuing in this fashion
over the levels of the decomposition, we get that with small
enough noise, for every l ∈ [L − 1], j ∈ [N/2l+1] and
γ ∈ [rl]:
φl,2j−1,γ⊗gφl,2j,γ =

⊗(cid:16)⊗2l+1−1

(cid:18)(cid:89)l−1

rl(cid:48) · 1 + o()

(cid:19)

1

i=1

l(cid:48)=0

i=1

where ⊗2l+1−1
1 stands for the tensor product of the vector
1 with itself 2l+1 − 1 times. We readily conclude from this
that with small enough noise, the tensor produced by the
decomposition may be written as follows:

A(cid:0)hD

y

(cid:1) =

(cid:18)(cid:89)L−1

l=0

(cid:19)

⊗(cid:0)⊗N−1
i=1 1(cid:1)

(14)

rl · 1 + o()

To ﬁnish our proof, it remains to show that a tensor as in
eq. 14 may be realized by the generalized CP decomposition
(eq. 6) with Z = 1 (and g(a, b) = max{a, b, 0}). Indeed,
we may assume that the latter’s F , which we denote by ˜F
to distinguish from the matrix in the generalized HT decom-
position (eq. 7), is invertible (non-degeneracy ensures that
this may be achieved with proper choice of representation
functions for the shallow ConvNet). Setting the weights of
the generalized CP decomposition (eq. 6) through:

• ay

1 = 1

• a1,i =

leads to A(cid:0)hS

y

(cid:40) ˜F −1(cid:16)(cid:81)L−1
(cid:1) = A(cid:0)hD

0

(cid:17)
l=0 rl · 1 + o()

(cid:1), as required.

y

, i = 1
, i > 1

Comparing claims 8 and 10, we see that depth efﬁciency
is complete under linear activation with product pooling,
and incomplete under ReLU activation with max pooling.
We interpret this as indicating that convolutional arith-
metic circuits beneﬁt from the expressive power of depth
more than convolutional rectiﬁer networks do. This result
is rather surprising, especially given the fact that convolu-
tional rectiﬁer networks are much more commonly used in
practice. We attribute the discrepancy primarily to histori-
cal reasons, and conjecture that developing effective meth-
ods for training convolutional arithmetic circuits, thereby

14

fulﬁlling their expressive potential, may give rise to a deep
learning architecture that is provably better than convolu-
tional rectiﬁer networks but has so far been overlooked by
practitioners.

Loosely speaking, we have shown that the gap in ex-
pressive power between the shallow and deep ConvNets is
greater with linear activation and product pooling than it is
with ReLU activation and max pooling. One may wonder
at this point if it is plausible to deduce from this which ar-
chitectural setting is more expressive, as a-priori, altering
the shallow vs. deep ConvNet comparisons such that one
network has linear activation with product pooling and the
other has ReLU activation with max pooling, may change
the expressive gaps in favor of the latter. Claims 11 and 12
below show that this is not the case. Speciﬁcally, they show
that the depth efﬁciency of the deep ConvNet with linear
activation and product pooling remains complete when the
shallow ConvNet has ReLU activation and max pooling
(claim 11), and on the other hand, the depth efﬁciency of
the deep ConvNet with ReLU activation and max pooling
remains incomplete when the shallow ConvNet has linear
activation and product pooling (claim 12). This afﬁrms our
stand regarding the expressive advantage of convolutional
arithmetic circuits over convolutional rectiﬁer networks.

(cid:17)

2

M·N .

Claim 11. Let fθ1. . .fθM be any set of linearly indepen-
dent representation functions for a deep ConvNet with lin-
ear activation and product pooling. Suppose we randomize
the weights of the network by some continuous distribution.
Then, with probability 1, we obtain score functions that can-
not be realized by a shallow ConvNet with ReLU activation
and max pooling if the number of hidden channels in the
latter (Z) is less than min{r0, M}N/2 ·
Proof. The proof here follows readily from those of
claims 8 and 9. Namely, in the proof of claim 8 we state
that for templates x(1) . . . x(M ) ∈ Rs chosen such that F
is invertible (these exist according to claim 2), a grid ten-
sor produced by the deep ConvNet with linear activation
and product pooling, when arranged as a matrix, has rank
at least min{r0, M}N/2 for all linear weight (al,j,γ) set-
tings but a set of measure zero. That is to say, a matrix
produced by the matricized generalized HT decomposition
(eq. 9) with g(a, b) = a·b, has rank at least min{r0, M}N/2
for all weight (al,j,γ) settings but a set of measure zero.
On the other hand, we have shown in the proof of claim 9
that a shallow ConvNet with ReLU activation and max
pooling generates grid tensors that when arranged as ma-
trices, have rank at most Z· M·N
. More speciﬁcally, we
have shown that the matricized generalized CP decompo-
sition (eq. 10) with g(a, b) = max{a, b, 0} produces ma-
trices with rank at most Z· M·N
. This implies that under
almost all linear weight (al,j,γ) settings for a deep ConvNet
with linear activation and product pooling, the generated

2

2

2

M·N hidden channels.

grid tensor cannot be replicated by a shallow ConvNet with
ReLU activation and max pooling if the latter has less than
Z = min{r0, M}N/2 ·
Claim 12. Suppose we randomize the weights of a
deep ConvNet with ReLU activation and max pooling by
some continuous distribution with non-vanishing continu-
ous probability density function. Then, assuming covering
templates exist, with positive probability, we obtain score
functions that can be realized by a shallow ConvNet with
linear activation and product pooling having only a single
hidden channel (Z = 1).

Proof. The proof here is almost identical to that of claim 10.
The only difference is where we show that a tensor as in
eq. 14 may be realized by the generalized CP decompo-
sition (eq. 6) with Z = 1.
In the proof of claim 10 the
underlying operation of the decomposition was g(a, b) =
max{a, b, 0} (corresponding to ReLU activation and max
pooling), whereas here it is g(a, b) = a·b (correspond-
ing to linear activation and product pooling). To account
for this difference, we again assume that ˜F – the matrix
F of the generalized CP decomposition, is invertible (non-
degeneracy ensures that this may be achieved with proper
choice of representation functions for the shallow Con-
vNet), and modify the decomposition’s weight setting as
follows:
• ay

1 = 1

(cid:40) ˜F −1(cid:16)(cid:81)L−1
(cid:1) = A(cid:0)hD

(cid:17)
l=0 rl · 1 + o()
(cid:1), as required.

˜F −11

y

• a1,i =

This leads to A(cid:0)hS

y

, i = 1

, i > 1

5.5.1 Approximation

In their current form, the results in our analysis establishing
depth efﬁciency (claims 8, 9, 11 and the analogous ones
in sec. 5.6) relate to exact realization. Speciﬁcally, they
provide a lower bound on the size of a shallow ConvNet
required in order for it to realize exactly a grid tensor gen-
erated by a deep ConvNet. From a practical perspective, a
more interesting question would be the size required by a
shallow ConvNet in order to approximate the computation
of a deep ConvNet. A-priori, it may be that although the
size required for exact realization is exponential, the one re-
quired for approximation is only polynomial. As we brieﬂy
discuss below, this is not the case, and in fact all of the
lower bounds we have provided apply not only to exact re-
alization, but also to arbitrarily-well approximation.

When proving that a grid tensor generated by a shallow
ConvNet beneath a certain size cannot be equal to a grid
tensor generated by a deep ConvNet, we always rely on
matricization rank. Namely, we arrange the grid tensors as

15

matrices, and derive constants R, r ∈ N, R > r, such that
the matrix corresponding to the deep ConvNet has rank at
least R, while that corresponding to the shallow ConvNet
has rank at most r. While used in our proofs solely to show
that the matrices are different, this actually entails informa-
tion regarding the distance between them. Namely, if we
denote the singular values of the matrix corresponding to
the deep ConvNet by σ1 ≥ σ2 ≥ . . . ≥ 0, the squared
Euclidean (Frobenius) distance between the matrices is at
R. Since the matrices are merely rear-
least σ2
rangements of the grid tensors, we have a lower bound on
the distance between the shallow ConvNet’s grid tensor and
the target grid tensor generated by the deep ConvNet, so in
particular arbitrarily-well approximation is not possible.

r+1 + ··· + σ2

5.5.2 On the Incidence of Depth Efﬁciency

In claim 8 we saw that depth efﬁciency is complete with
linear activation and product pooling. That is to say, with
linear activation and product pooling, besides a negligible
set, all weight settings for the deep ConvNet (ﬁg. 1 with
size-2 pooling windows and L = log2 N hidden layers)
lead to score functions that cannot be realized by the shal-
low ConvNet (ﬁg. 2) unless the latter has super-polynomial
size. We have also seen (claims 9 and 10) that replacing
the activation and pooling operators by ReLU and max re-
spectively, makes depth efﬁciency incomplete. There are
still weight settings leading the deep ConvNet to generate
score functions that require the shallow ConvNet to have
super-polynomial size, but these do not occupy the entire
space.
In other words, there is now positive measure to
the set of deep ConvNet weight conﬁgurations leading to
score functions efﬁciently realizable by the shallow Con-
vNet. A natural question would then be just how frequent
depth efﬁciency is under ReLU activation and max pool-
ing. More formally, we may consider a uniform distribu-
tion over a compact domain in the deep ConvNet’s weight
space, and ask the following. Assuming weights for the
deep ConvNet are drawn from this distribution, what is the
probability that generated score functions exhibit depth efﬁ-
ciency, i.e. require super-polynomial size from the shallow
ConvNet? In this appendix we address this question, argu-
ing that the probability tends to 1 as the number of chan-
nels in the hidden layers of the deep ConvNet grows. We
do not prove this formally, but nonetheless provide a frame-
work we believe may serve as a basis for establishing formal
results concerning the incidence of depth efﬁciency. The
framework is not limited to ReLU activation and max pool-
ing – it may be used under different choices of activation
and pooling operators as well.

The central tool used in this paper for proving depth
efﬁciency is the rank of grid tensors when these are ar-
ranged as matrices. We establish upper bounds on the rank

of matricized grid tensors produced by the shallow Con-
vNet through the matricized generalized CP decomposition
(eq. 10). These upper bounds are typically linear in the
size of the input (N) and the number of hidden channels
in the network (Z). The challenge is then to derive a super-
polynomial (in N) lower bound on the rank of matricized
grid tensors produced by the deep ConvNet through the ma-
tricized generalized HT decomposition (eq. 9). In the case
of linear activation and product pooling (g(a, b) = a·b), the
generalized Kronecker product (cid:12)g reduces to the standard
Kronecker product (cid:12), and the rank-multiplicative property
of the latter (rank(A(cid:12)B) = rank(A)·rank(B)) can be
used to show (see [5]) that besides in negligible (zero mea-
sure) cases, rank grows rapidly through the levels of the ma-
tricized generalized HT decomposition (eq. 9), to the point
where the ﬁnal produced matrix has exponential rank. This
situation does not persist when the activation and pooling
operators are replaced by ReLU and max (respectively).
Indeed, in the proof of claim 10 we explicitly presented
a non-negligible (positive measure) case where the matri-
cized generalized HT decomposition (eq. 9) produces a ma-
trix of rank 1. To study the incidence of depth efﬁciency
under ReLU activation and max pooling, we assume the
weights (al,j,γ) of the matricized generalized HT decompo-
sition (eq. 9) are drawn independently and uniformly from
a bounded interval (e.g. [−1, 1]), and question the proba-

bility of the produced matrix [A(cid:0)hD
To study rank[A(cid:0)hD

(cid:1)] having rank super-
(cid:1)], we sequentially traverse

y

through the levels l = 1. . .L of the matricized generalized
HT decomposition (eq. 9), at each level going over all lo-
cations j ∈ [N/2l]. When at location j of level l, for each
α ∈ [rl−1], we draw the weights al−1,2j−1,α and al−1,2j,α
(independently of the previously drawn weights), and ob-
serve the random variable Rl,j,α, deﬁned as the rank of
the matrix [φl−1,2j−1,α] (cid:12)g [φl−1,2j,α]. Given the weights
drawn while traversing through the previous levels of the
decomposition, the random variables {Rl,j,α ∈ N}α∈[rl−1]
are independent and identically distributed. The random
variable Rl,j := maxα∈[rl−1]{Rl,j,α} thus tends to concen-
trate on higher and higher values as rl−1 (number of chan-
nels in hidden layer l−1 of the deep ConvNet) grows. When
the next level (l + 1) of the decomposition will be traversed,
the weights {al,j,γ}γ∈[rl] will be drawn, and the matrices
{[φl,j,γ]}γ∈[rl] will be generated. According to claim 13
below, with probability 1, all of these matrices will have
rank equal to at least Rl,j. And so, assuming the general-
ized Kronecker product (cid:12)g has the potential of increasing
the rank of its operands, ranks will generally ascend across
the levels of the matricized generalized HT decomposition
(eq. 9), with steeper ascends being more and more probable
as the number of channels in the hidden layers of the deep
ConvNet (r0. . .rL−1) grows.

polynomial in N.

y

Figure 5. Simulation results demonstrating that under ReLU ac-
tivation and max pooling, the incidence of depth efﬁciency in-
creases as the number of channels in the hidden layers of the
deep ConvNet (r0. . .rL−1) grows. The plots show histograms of
the ranks produced by the matricized generalized HT decomposi-
tion (eq. 9) with g(a, b) = max{a, b, 0}. The number of levels
in the decomposition was set to L = 3 (implying input size of
N = 2L = 8). The size of the representation matrix F was
set through M = 3, and the matrix itself was ﬁxed to the iden-
tity. Weights (al,j,γ) were drawn at random independently and
uniformly from the interval [−1, 1]. Three channel-width conﬁg-
urations were tried: (i) r0 = r1 = r2 = 2 (ii) r0 = r1 = r2 = 4
(ii) r0 = r1 = r2 = 8. For each conﬁguration 1000 random tests
were run, creating the histograms presented in the ﬁgure (each test
produced a single matrix [A(hD
y )], accounting for a single entry
in a histogram). As can be seen, the distribution of the produced
rank (rank[A(hD
y )]) tends towards the maximum (M N/2 = 81)
as the numbers of hidden channels grow.

The main piece that is missing in order to complete
the sketch we have outlined above into a formal proof,
is the behavior of rank under the generalized Kronecker
product (cid:12)g. This obviously depends on the choice of
In the case of linear activation
underlying operator g.
and product pooling g(a, b) = a·b, the generalized Kro-
necker product (cid:12)g reduces to the standard Kronecker prod-
uct (cid:12), and ranks always increase multiplicatively,
i.e.
rank(A(cid:12)B) = rank(A)·rank(B) for any matrices A and
B. The fact that there is a simple law governing the be-
havior of ranks makes this case relatively simple to ana-
lyze, and we indeed have a full characterization (claim 8).
In the case of linear activation and max pooling the un-
derlying operator is given by g(a, b) = max{a, b}, and
it is not difﬁcult to see that (cid:12)g does not decrease rank,
i.e. rank(A(cid:12)gB)≥ min{rank(A), rank(B)} for any ma-

16

0102030405060708090rank020406080100frequencynumbers of hidden channels: [2, 2, 2]0102030405060708090rank051015202530frequencynumbers of hidden channels: [4, 4, 4]0102030405060708090rank0100200300400500600frequencynumbers of hidden channels: [8, 8, 8]trices A and B 6. For ReLU activation and max pooling,
corresponding to the choice g(a, b) = max{a, b, 0}, there
is no simple rule depicting the behavior of ranks under (cid:12)g,
and in fact, for matrices A and B holding negative val-
ues, the rank of rank(A(cid:12)gB) necessarily drops to zero.
Nonetheless, it seems reasonable to assume that at least in
some cases, a non-linear operation such as (cid:12)g does increase
rank, and as we have seen, beneﬁting from these cases is
more probable when the hidden layers of the deep ConvNet
include many channels. To this end, we provide in ﬁg. 5
simulation results for the case of ReLU activation and max
pooling (g(a, b) = max{a, b, 0}), demonstrating that in-
deed ranks produced by the matricized generalized HT de-
composition (eq. 9) tend to be higher as r0. . .rL−1 grow.
We leave a complete formal analysis of this phenomenon to
future work.

tor α ∈ Rm deﬁne the matrix A(α) := (cid:80)m

Claim 13. Let A1. . .Am be given matrices of the same size,
having ranks r1. . .rm respectively. For every weight vec-
i=1 αiAi, and
suppose we randomize α by some continuous distribution.
Then, with probability 1, we obtain a matrix A(α) having
rank at least maxi∈[m] ri.

Proof. Our proof relies on concepts and results from
Lebesgue measure theory (see sec. 5.1 for a brief discus-
sion). The result to prove is equivalent to stating that there
is measure zero to the set of weight vectors α for which
rank(A(α)) < maxi∈[m] ri.
Assume without loss of generality that maxi∈[m] ri is
equal to r1, and that the top-left r1×r1 block of A1 is non-
singular. For every α deﬁne p(α) := det(A(α)1:r1,1:r1),
i.e. p(α) is the determinant of the r1×r1 top-left block of
the matrix A(α). p(α) is obviously a polynomial in the en-
tries of α, and by assumption p(e1) (cid:54)= 0, where e1 ∈ Rm is
the vector holding 1 in its ﬁrst entry and 0 elsewhere. Since
a non-zero polynomial vanishes only on a set of zero mea-
sure (see [1] for example), the set of weight vectors α for
which p(α) = 0 has measure zero. This implies that the
top-left r1×r1 block of A(α) is non-singular almost every-
where, and in particular rank(A(α))≥r1 = maxi∈[m] ri
almost everywhere.
5.6. Shared Coefﬁcients for Convolution

To this end, our analysis has focused on the unshared
setting, where the coefﬁcients of the 1 × 1 conv ﬁlters (see
ﬁg. 1) may vary across spatial locations. In practice, Con-
vNets typically enforce sharing, which in our framework
implies that the coefﬁcients of the 1 × 1 conv ﬁlter in chan-
nel γ of hidden layer l, are the same for all locations j. In
this section we analyze the shared setting, following a line
6 To see this, simply note that under the choice g(a, b) = max{a, b}
there is either a sub-matrix of A(cid:12)gB that is equal to A, or one that is equal
to B.

similar to that of our analysis for the unshared setting given
above. For brevity, we assume the reader is familiar with
the latter, and do not repeat discussions given there.

As described in sec. 4, the shared setting refers to the
case where the 1×1 conv ﬁlters in our networks are spatially
invariant, giving rise to standard convolutions (as opposed
to the more general locally-connected operators). Specif-
ically, the shallow ConvNet (ﬁg. 2) would have a single
weight vector az for every hidden channel z, as opposed
to the unshared setting where it had a weight vector az,i
for every location i in every hidden channel z. Grid tensors
produced by the shallow ConvNet in the shared setting are
given by what we call the shared generalized CP decompo-
sition:

A(cid:0)hS

y

(cid:1) =

Z(cid:88)

z=1

(cid:125)
(cid:124)
z · (F az) ⊗g ··· ⊗g (F az)
ay

(cid:123)(cid:122)

N times

(15)

As for the deep ConvNet (ﬁg. 1 with size-2 pooling win-
dows and L = log2 N hidden layers), in the shared setting,
instead of having a weight vector al,j,γ for every hidden
layer l, channel γ and location j, there is a single weight
vector al,γ for all locations of channel γ in hidden layer l.
Produced grid tensors are then given by the shared general-
ized HT decomposition:

φ1,γ =
···

φl,γ =

···
φL−1,γ =

A(cid:0)hD

y

(cid:1) =

α=1

r0(cid:88)
rl−1(cid:88)
rL−2(cid:88)
rL−1(cid:88)

α=1

α=1

α=1

⊗g φl−1,α
order 2l−1

α (F a0,α) ⊗g (F a0,α)
a1,γ
(cid:124) (cid:123)(cid:122) (cid:125)
(cid:124) (cid:123)(cid:122) (cid:125)
α φl−1,α
al,γ
order 2l−1
(cid:124) (cid:123)(cid:122) (cid:125)
(cid:124) (cid:123)(cid:122) (cid:125)
(cid:124) (cid:123)(cid:122) (cid:125)
(cid:124) (cid:123)(cid:122) (cid:125)
α φL−1,α
aL,y

φL−2,α
order N
4

⊗g φL−1,α

⊗g φL−2,α

aL−1,γ

order N
4

α

order N
2

order N
2

(16)

We now turn to analyze universality and depth efﬁciency in
the shared setting.

5.6.1 Universality

In the unshared setting we saw (sec. 5.4) that linear acti-
vation with product pooling and ReLU activation with max
pooling both lead to universality, whereas ReLU activation
with average pooling does not. We will now see that in the
shared setting, no matter how the activation and pooling op-
erators are chosen, universality is never met.

A shallow ConvNet with shared weights produces grid
tensors through the shared generalized CP decomposition

17

(eq. 15). A tensor A generated by this decomposition is nec-
essarily symmetric, i.e. for any permutation δ : [N ] → [N ]
and indexes d1. . .dN it meets: Ad1...dN = Aδ(d1)...δ(dN ).
Obviously not all tensors share this property, so indeed
a shallow ConvNet with weight sharing is not univer-
sal. A deep ConvNet with shared weights produces grid
tensors through the shared generalized HT decomposition
(eq. 16). For this decomposition, a generated tensor A
is invariant to replacing the ﬁrst and second halves of its
modes, i.e. for any indexes d1. . .dN it meets: Ad1,...,dN =
AdN/2+1,...,dN ,d1,...,dN/2
. Although this property is much
less stringent than symmetry, it is still not met by most ten-
sors, and so a deep ConvNet with weight sharing is not uni-
versal either.

5.6.2 Depth Efﬁciency

Depth efﬁciency deals with the computational complexity
of replicating a deep network’s function using a shallow
network. In order for this question to be applicable, we re-
quire that the shallow network be a universal machine. If
this is not the case, then it is generally likely that the deep
network’s function simply lies outside the reach of the shal-
low network, and we do not obtain a quantitative insight
into the true power of depth. Since our shallow ConvNets
are not universal with shared weights (sec. 5.6.1), we evalu-
ate depth efﬁciency of deep ConvNets with shared weights
against shallow ConvNets with unshared weights. Speciﬁ-
cally, we do this for the activation-pooling choices leading
shallow ConvNets with unshared weights to be universal:
linear activation with product pooling, and ReLU activation
with max pooling (see sec. 5.4).

For linear activation with product pooling, the following
claim, which is essentially a derivative of theorem 1 in [5],
tells us that in the shared setting, as in the unshared setting,
depth efﬁciency holds completely:
Claim 14 (shared analogy of claim 8). Let fθ1 . . .fθM be
any set of linearly independent representation functions for
a deep ConvNet with linear activation, product pooling and
weight sharing. Suppose we randomize the weights of the
network by some continuous distribution. Then, with proba-
bility 1, we obtain score functions that cannot be realized by
a shallow ConvNet with linear activation and product pool-
ing (not limited by weight sharing), if the number of hidden
channels in the latter (Z) is less than min{r0, M}N/2.
Proof. The proof here is almost identical to that of claim 8.
The only difference is that in the latter, we used the fact that
the generalized HT decomposition (eq. 7), when equipped
with g(a, b) = a·b, almost always produces tensors whose
matrix arrangements have rank at least min{r0, M}N/2,
whereas here, we require an analogous result for the shared
generalized HT decomposition (eq. 16). Such result is pro-
vided by the proof of theorem 1 in [5].

Heading on to ReLU activation and max pooling, we will
show that here too, the situation in the shared setting is the
same as in the unshared setting. Speciﬁcally, depth efﬁ-
ciency holds, but not completely. We prove this via two
claims, analogous to claims 9 and 10 from sec. 5.5:

M·N .

Claim 15 (shared analogy of claim 9). There exist weight
settings for a deep ConvNet with ReLU activation, max
pooling and weight sharing, giving rise to score functions
that cannot be realized by a shallow ConvNet with ReLU
activation and max pooling (not limited by weight sharing),
if the number of hidden channels in the latter (Z) is less
than min{r0, M}N/2 ·
Proof. In the proof of claim 9 we have shown, for arbitrary
distinct templates x(1) . . . x(M ) ∈ Rs, an explicit weight
setting for the deep ConvNet with ReLU activation and max
pooling, leading the latter to produce a grid tensor that can-
not be realized by a shallow ConvNet with ReLU activation
and max pooling, if that has less than min{r0, M}N/2·
M·N
hidden channels. Since the given weight setting was loca-
tion invariant, i.e. the assignment of al,j,γ did not depend
on j, it applies as is to a deep ConvNet with weight sharing,
and the desired result readily follows.

2

2

Claim 16 (shared analogy of claim 10). Suppose we ran-
domize the weights of a deep ConvNet with ReLU activa-
tion, max pooling and weight sharing by some continuous
distribution with non-vanishing continuous probability den-
sity function. Then, assuming covering templates exist, with
positive probability, we obtain score functions that can be
realized by a shallow ConvNet with ReLU activation and
max pooling having only a single hidden channel (Z = 1).

Proof. The proof is similar in spirit to that of claim 10,
which dealt with incompleteness of depth efﬁciency under
ReLU activation and max pooling in the unshared setting.
Our focus here is on the shared setting, or more speciﬁ-
cally, on the case where the deep ConvNet is limited by
weight sharing while the shallow ConvNet is not. Ac-
cordingly, we would like to show the following. Fixing
g(a, b) = max{a, b, 0}, per arbitrary invertible F there ex-
ists a weight (al,γ) setting for the shared generalized HT de-
composition (eq. 16), such that the produced tensor may be
realized by the generalized CP decomposition (eq. 6) with
Z = 1, and this holds even if the weights al,γ and matrix F
are subject to small perturbations.

Before heading on to prove that a weight setting as above
exists, we introduce a new deﬁnition that will greatly sim-
plify our proof. We refer to a tensor A of order P and di-
mension M in each mode as basic, if there exists a vector
u ∈ RM with non-decreasing entries (u1≤ . . .≤uM ), such
that A = u ⊗g ··· ⊗g u (i.e. A is equal to the general-
ized tensor product of u with itself P times, with underly-
ing operation g(a, b) = max{a, b, 0}). A basic tensor can

18

obviously be realized by the generalized CP decomposition
(eq. 6) with Z = 1 (given that non-degeneracy is used to en-
sure the latter’s representation matrix is non-singular), and
so it sufﬁces to ﬁnd a weight (al,γ) setting for the shared
generalized HT decomposition (eq. 16) that gives rise to a
basic tensor, and in addition, ensures that small perturba-
tions on the weights al,γ and matrix F still yield basic ten-
sors. Two trivial facts that relate to basic tensors and will be
used in our proof are: (i) the generalized tensor product of a
basic tensor with itself is basic, and (ii) a linear combination
of basic tensors with non-negative weights is basic.

Turning to the main part of the proof, we now show that

the following weight setting meets our requirement:

• a0,γ = F −1v
• al,γ = 1, l ∈ [L − 1]
• aL,y = 1

v here stands for the vector [1, 2, . . . , M ](cid:62) ∈ RM , and 1
is an all-1 vector with dimension to be understood by con-
text. Let E F be an additive noise matrix applied to F , and
{l,γ}l,γ be additive noise vectors applied to {al,γ}l,γ. We
would like to prove that under the weight setting above,
when applied noise (E F , l,γ) is small enough, the grid ten-
sor produced by the shared generalized HT decomposition
(eq. 16) is basic.
For convenience, we adopt the notation o() as referring
to vectors that tend to 0 as E F → 0 and l,γ → 0, with the
dimension of a vector to be understood by context. Plug-
ging in the noisy variables into the shared generalized HT
decomposition (eq. 16), we get for every α ∈ [r0]:

((F + E F )(a0,α + 0,α)) ⊗g ((F + E F )(a0,α + 0,α))

= ((F + E F )(F −1v + 0,α)) ⊗g ((F + E F )(F −1v + 0,α))

= ˜vα ⊗g ˜vα

where ˜vα = v+o(). If the applied noise (E F , l,γ) is small
enough the entries of ˜vα are non-decreasing and ˜vα ⊗g ˜vα
is a basic tensor (matrix). Moving to the next level of the
decomposition, we have for every γ ∈ [r1]:

r0(cid:88)

α=1

φ1,γ =

(a1,γ

α + 1,γ

α ) · ˜vα ⊗g ˜vα

When applied noise (E F , l,γ) is small enough the weights
of this linear combination are non-negative, and together
with the tensors (matrices) ˜vα ⊗g ˜vα being basic, this leads
φ1,γ to be basic as well. Continuing in this fashion over the
levels of the decomposition, we get that with small enough
noise, for every l ∈ [L − 1] and γ ∈ [rl], φl,γ is a ba-
sic tensor. A ﬁnal step in this direction shows that under

small noise, the produced grid tensor A(cid:0)hD

(cid:1) is basic as

y

well. This is what we set out to prove.

19

To recapitulate this section, we have shown that intro-
ducing weight sharing into the 1 × 1 conv operators of our
networks, thereby limiting the general locally-connected
linear mappings to be standard convolutions, disrupts uni-
versality, but leaves depth efﬁciency intact – it remains to
hold completely under linear activation with product pool-
ing, and incompletely under ReLU activation with max
pooling.

6. Discussion

The contribution of this paper is twofold. First, we
introduce a construction in the form of generalized ten-
sor decompositions,
that enables transforming convolu-
tional arithmetic circuits into convolutional rectiﬁer net-
works (ConvNets with ReLU activation and max or aver-
age pooling). This opens the door to various mathematical
tools from the world of arithmetic circuits, now available
for analyzing convolutional rectiﬁer networks. As a second
contribution, we make use of such tools to prove new results
on the expressive properties that drive this important class
of networks.

Our analysis shows that convolutional rectiﬁer networks
are universal with max pooling, but not with average pool-
ing. This implies that if non-linearity originates solely from
ReLU activation, increasing network size alone is not sufﬁ-
cient for expressing arbitrary functions. More interestingly,
we analyze the behavior of convolutional rectiﬁer networks
in terms of depth efﬁciency, i.e. of cases where a func-
tion generated by a deep network of polynomial size re-
quires shallow networks to have super-polynomial size. It
is known that convolutional arithmetic circuits exhibit com-
plete depth efﬁciency, i.e. that besides a negligible (zero
measure) set, all functions generated by deep networks of
this type are depth efﬁcient. We show that this is not the
case with convolutional rectiﬁer networks, for which depth
efﬁciency exists, but is weaker in the sense that it is not
complete (there is positive measure to the set of functions
generated by a deep network that may be efﬁciently realized
by shallow networks).

Depth efﬁciency is believed to be the key factor behind
the success of deep learning. Our analysis indicates that
from this perspective, the widely used convolutional rec-
tiﬁer networks are inferior to convolutional arithmetic cir-
cuits. This leads us to believe that convolutional arithmetic
circuits bear the potential to improve the performance of
deep learning beyond what is witnessed today. Of course, a
practical machine learning model is measured not only by
its expressive power, but also by our ability to train it. Over
the years, massive amounts of research have been devoted
to training convolutional rectiﬁer networks. Convolutional
arithmetic circuits on the other hand received far less atten-
tion, although they have been successfully trained in recent
works on the SimNet architecture ([3, 4]), demonstrating

how the enhanced expressive power can lead to state of the
art performance in computationally limited settings.

We believe that developing effective methods for train-
ing convolutional arithmetic circuits, thereby fulﬁlling their
expressive potential, may give rise to a deep learning archi-
tecture that is provably superior to convolutional rectiﬁer
networks but has so far been overlooked by practitioners.
Acknowledgments

This work is partly funded by Intel grant ICRI-CI no. 9-
2012-6133 and by ISF Center grant 1790/12. Nadav Cohen
is supported by a Google Fellowship in Machine Learning.
References
[1] Richard Caron and Tim Traynor. The zero set of a polyno-

mial. WSMR Report 05-02, 2005.

[2] Christopher Clark and Amos Storkey. Teaching deep con-
arXiv preprint

volutional neural networks to play go.
arXiv:1412.3409, 2014.

[3] Nadav Cohen and Amnon Shashua. SimNets: A Generaliza-
tion of Convolutional Networks. NIPS Deep Learning and
Representation Learning Workshop, 2014.

[4] Nadav Cohen, Or Sharir, and Amnon Shashua. Deep Sim-

Nets. arXiv.org, June 2015.

[5] Nadav Cohen, Or Sharir, and Amnon Shashua. On the ex-
pressive power of deep learning: a tensor analysis. arXiv
preprint arXiv:1509.05009, 2015.

[6] G Cybenko. Approximation by superpositions of a sigmoidal
function. Mathematics of Control, Signals and Systems, 2(4):
303–314, 1989.

[7] Olivier Delalleau and Yoshua Bengio. Shallow vs. deep sum-
product networks. In Advances in Neural Information Pro-
cessing Systems, pages 666–674, 2011.

[8] Ronen Eldan and Ohad Shamir. The power of depth for feed-
forward neural networks. arXiv preprint arXiv:1512.03965,
2015.

[9] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep
learning. Book in preparation for MIT Press, 2016. URL
http://goodfeli.github.io/dlbook/.

[10] W Hackbusch and S K¨uhn. A New Scheme for the Tensor
Representation. Journal of Fourier Analysis and Applica-
tions, 15(5):706–722, 2009.

[11] Wolfgang Hackbusch. Tensor Spaces and Numerical Tensor
Calculus, volume 42 of Springer Series in Computational
Mathematics. Springer Science & Business Media, Berlin,
Heidelberg, February 2012.

[12] Kurt Hornik, Maxwell B Stinchcombe, and Halbert White.
Multilayer feedforward networks are universal approxima-
tors. Neural networks, 2(5):359–366, 1989.

[13] Frank Jones.

Lebesgue integration on Euclidean space.

Jones & Bartlett Learning, 2001.

[14] Tamara G Kolda and Brett W Bader. Tensor Decompositions

and Applications. SIAM Review (), 51(3):455–500, 2009.

[15] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Im-
ageNet Classiﬁcation with Deep Convolutional Neural Net-
works. Advances in Neural Information Processing Systems,
pages 1106–1114, 2012.

[16] Yann LeCun and Yoshua Bengio. Convolutional networks
for images, speech, and time series. The handbook of brain
theory and neural networks, 3361(10), 1995.

[17] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep

learning. Nature, 521(7553):436–444, May 2015.

[18] Moshe Leshno, Vladimir Ya Lin, Allan Pinkus, and Shimon
Schocken. Multilayer feedforward networks with a nonpoly-
nomial activation function can approximate any function.
Neural networks, 6(6):861–867, 1993.

[19] James Martens and Venkatesh Medabalimi. On the expres-
arXiv preprint

sive efﬁciency of sum product networks.
arXiv:1411.7717, 2014.

[20] Guido F Montufar, Razvan Pascanu, Kyunghyun Cho, and
Yoshua Bengio. On the number of linear regions of deep
In Advances in Neural Information Pro-
neural networks.
cessing Systems, pages 2924–2932, 2014.

[21] Vinod Nair and Geoffrey E Hinton. Rectiﬁed linear units im-
prove restricted boltzmann machines. In Proceedings of the
27th International Conference on Machine Learning (ICML-
10), pages 807–814, 2010.

[22] Razvan Pascanu, Guido Montufar, and Yoshua Bengio. On
the number of inference regions of deep feed forward net-
arXiv preprint
works with piece-wise linear activations.
arXiv, 1312, 2013.

[23] Tomaso Poggio, Fabio Anselmi, and Lorenzo Rosasco.

I-
theory on depth vs width: hierarchical function composition.
Technical report, Center for Brains, Minds and Machines
(CBMM), 2015.

[24] Hoifung Poon and Pedro Domingos. Sum-product networks:
In Computer Vision Workshops
A new deep architecture.
(ICCV Workshops), 2011 IEEE International Conference on,
pages 689–690. IEEE, 2011.

[25] Yelong Shen, Xiaodong He, Jianfeng Gao, Li Deng, and
Gr´egoire Mesnil. Learning semantic representations using
convolutional neural networks for web search. In Proceed-
ings of the companion publication of the 23rd international
conference on World wide web companion, pages 373–374.
International World Wide Web Conferences Steering Com-
mittee, 2014.

20

[26] Amir Shpilka and Amir Yehudayoff. Arithmetic circuits: A
survey of recent results and open questions. Foundations
and Trends in Theoretical Computer Science, 5(3–4):207–
388, 2010.

[27] Karen Simonyan and Andrew Zisserman. Very deep convo-
lutional networks for large-scale image recognition. arXiv
preprint arXiv:1409.1556, 2014.

[28] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,
Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent
Vanhoucke, and Andrew Rabinovich. Going Deeper with
Convolutions. CVPR, 2015.

[29] Yaniv Taigman, Ming Yang, Marc’Aurelio Ranzato, and Lior
Wolf. DeepFace: Closing the Gap to Human-Level Perfor-
mance in Face Veriﬁcation. In CVPR ’14: Proceedings of
the 2014 IEEE Conference on Computer Vision and Pattern
Recognition. IEEE Computer Society, June 2014.

[30] Matus Telgarsky. Beneﬁts of depth in neural networks. arXiv

preprint arXiv:1602.04485, 2016.

[31] Izhar Wallach, Michael Dzamba, and Abraham Heifets.
Atomnet: A deep convolutional neural network for bioac-
tivity prediction in structure-based drug discovery. arXiv
preprint arXiv:1510.02855, 2015.

[32] Daniel Zoran and Yair Weiss. ”Natural Images, Gaussian
Mixtures and Dead Leaves”. Advances in Neural Informa-
tion Processing Systems, pages 1745–1753, 2012.

21

A. Existence of Covering Templates

In this paper we analyze the expressiveness of networks, i.e.
the functions they can realize, through the notion of grid tensors.
Recall from sec. 4 that given templates x(1) . . . x(M ) ∈ Rs, the
grid tensor of a score function hy : (Rs)N → R realized by
some network, is deﬁned to be a tensor of order N and dimen-
sion M in each mode, denoted A(hy), and given by eq. 3.
In
particular, it is a tensor holding the values of hy on all instances
X = (x1, . . . , xN ) ∈ (Rs)N whose patches xi are taken from
the set of templates {x(1) . . . x(M )} (recurrence allowed). Some
of the claims in our analysis (sec. 5) assumed that there exist tem-
plates for which grid tensors fully deﬁne score functions. That is to
say, there exist templates such that score function values outside
the exponentially large grid {Xd1...dN := (x(d1), . . . , x(dN )) :
d1. . .dN ∈ [M ]} are irrelevant for classiﬁcation. Templates meet-
ing this property where referred to as covering (see sec. 5.2). In
this appendix we address the existence of covering templates.

If we allow M to grow arbitrarily large then obviously covering
templates can be found. However, since in our construction M is
tied to the number of channels in the ﬁrst (representation) layer of
a network (see ﬁg. 1), such a trivial observation does not sufﬁce,
and in fact we would like to show that covering templates exist
for values of M that correspond to practical network architectures,
i.e. M ∈ Ω(100). For such an argument to hold, assumptions must
be made on the distribution of input data. Given that ConvNets are
used primarily for processing natural images, we assume here that
data is governed by their statistics. Speciﬁcally, we assume that
an instance X = (x1, . . . , xN ) ∈ (Rs)N corresponds to a natural
image, represented through N image patches around its pixels:
x1. . .xN ∈ Rs.

If the dimension of image patches is small then it seems rea-
sonable to believe that relatively few templates can indeed cover
the possible appearances of a patch. For example, in the extreme
case where each patch is simply a gray-scale pixel (s = 1), having
M = 256 templates may provide the standard 8-bit resolution,
leading grid tensors to fully deﬁne score functions by accounting
for all possible images. However, since in our construction input
patches correspond to the receptive ﬁeld in the ﬁrst layer of a Con-
vNet (see ﬁg. 1), we would like to establish an argument for image
patch sizes that more closely correlate to typical receptive ﬁelds,
e.g. 5×5. For this we rely on various studies (e.g. [32]) charac-
terizing the statistics of natural images, which have shown that for
large ensembles of images, randomly cropped patches of size up
to 16×16 may be relatively well captured by Gaussian Mixture
Models with as few as 64 components. This complies with the
common belief that there is a moderate number of appearances
taken by the vast majority of local image patches (edges, Gabor
ﬁlters etc.). That is to say, it complies with our assumption that
covering templates exist with a moderate value of M. We refer
the reader to [5] for a more formal argument on this line.

22

