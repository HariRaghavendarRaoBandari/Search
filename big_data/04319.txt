6
1
0
2

 
r
a

 

M
4
1

 
 
]

G
L
.
s
c
[
 
 

1
v
9
1
3
4
0

.

3
0
6
1
:
v
i
X
r
a

Learning Network of Multivariate Hawkes Processes:

A Time Series Approach

Jalal Etesami

Department of Industrial and

Enterprise Systems Engineering

University of Illinois
at Urbana-Champaign
Urbana, IL 6180, USA

etesami2@illinois.edu

Negar Kiyavash

Department of Industrial and

Enterprise Systems Engineering

and Department of Electrical
and Computer Engineering

UIUC, Urbana, IL 6180, USA
kiyavash@illinois.edu

Kun Zhang

Department of Philosophy
Carnegie Mellon University
Pittsburgh, PA 15213, USA
kunz1@andrew.cmu.edu

Kushagra Singhal

Department of Electrical
and Computer Engineering

UIUC, Urbana, IL 6180, USA
ksingha2@illinois.edu

Abstract

Learning the inﬂuence structure of multiple time series data is of great interest to
many disciplines. This paper studies the problem of recovering the causal struc-
ture in network of multivariate linear Hawkes processes. In such processes, the
occurrence of an event in one process affects the probability of occurrence of new
events in some other processes. Thus, a natural notion of causality exists between
such processes captured by the support of the excitation matrix. We show that
the resulting causal inﬂuence network is equivalent to the Directed Information
graph (DIG) of the processes, which encodes the causal factorization of the joint
distribution of the processes. Furthermore, we present an algorithm for learning
the support of excitation matrix (or equivalently the DIG). The performance of the
algorithm is evaluated on synthesized multivariate Hawkes networks as well as a
stock market and MemeTracker real-world dataset.

1

Introduction

In many disciplines, including biology, economics, social sciences, and computer science, it is im-
portant to learn the structure of interacting networks of stochastic processes. In particular, succinct
representation of the causal interactions in the network is of interest.
A lot of studies in the causality ﬁelds focus on causal discovery from time series. To ﬁnd causal
relations from time series, one may ﬁt vector autoregressive models on the time series, or more
generally, evaluate the causal inﬂuences with transfer entropy [22] or directed information [19].
This paper considers learning causal structure for a speciﬁc type of time series, multivariate linear
Hawkes process [8]. Hawkes processes were originally motivated by the quest for good statistical
models for earthquake occurrences. Since then, they have been successfully applied to seismology
[15], biology [21], criminology [13], computational ﬁnance [5, 12, 14], etc. It is desirable to develop
speciﬁc causal discovery methods for such processes and study the properties of existing methods
in this particular scenario.
In multivariate or mutually exciting point processes, occurrence of an event (arrival) in one process
affects the conditional probability of new occurrences, i.e., the intensity function of other processes

1

in the network. Such interdependencies between the intensity functions of a linear Hawkes process
are modeled as follows: the intensity function of processes j is assumed to be a linear combination of
different terms, such that each term captures only the effects of one other process (See Section 2.1).
Therefore, a natural notion of functional dependence (causality) exists among the processes in the
sense that in linear mutually exciting processes, if the coefﬁcient pertaining to the effects of process
i is non-zero in the intensity function of process j, we know that process i is inﬂuencing process
j. This dependency is captured by the support of the excitation matrix of the network. As a result,
estimation of the excitation (kernel) matrix of multivariate processes is crucial both for learning the
structure of their causal network and for other inference tasks and has been the focus of research. For
instance, maximum likelihood estimators were proposed for estimating the parameters of excitation
matrices with exponential and Laguerre decay in [16, 25]. These estimators depend on existence of
i.i.d. samples. However, often we do not have access to i.i.d. samples when analyzing time series.
Second-order statistics of the multivariate Hawkes processes were used to estimate the kernel matrix
of a subclass of multivariate Hawkes processes called symmetric Hawkes processes [1]. Utilizing the
branching property of the Hawkes processes, an expectation maximization algorithm was proposed
to estimate the excitation matrix in [10].
We aim to investigate efﬁcient approaches to estimation of excitation matrix of Hawkes processes
from time series that does not require i.i.d. samples and investigate how the concept of causality in
such processes is related to other established approaches to analyze causal effects in time series.

1.1 Summary of Results and Organization

Our contribution in this paper is two fold. First, we prove that for linear multivariate Hawkes pro-
cesses, the causal relationships implied by the excitation matrix is equivalent to a speciﬁc factor-
ization of the joint distribution of the system called minimal generative model. Minimal generative
models encode causal dependencies based on a generalized notion of Granger causality, measured
by causally conditioned directed information [20]. One signiﬁcance of this result is that it provides
a surrogate to directed information measure for capturing causal inﬂuences for Hawkes processes.
Thus, instead of estimating the directed information, which often requires estimating a high dimen-
sional joint distribution, it sufﬁces to learn the support of the excitation matrix. Our second contri-
bution is indeed providing an estimation method for learning the support of excitation matrices with
exponential form using second-order statistics of the Hawkes processes.
Our proposed learning approach, in contrast with the previous work [1, 24], is not limited to sym-
metric Hawkes processes. In a symmetric Hawkes process, it is assumed that the Laplace transform
of the excitation matrix can be factored into product of a diagonal matrix and a constant unitary
matrix. Moreover, it is assumed that the expected values of all intensities are the same. A numerical
method to approximate the excitation matrix from a set of coupled integral equations was recently
proposed in [3]. Our approach is based on an exact analytical solution to ﬁnd the excitation matrix.
Interestingly, the exact approach turns out to be both more robust and less expensive in terms of
complexity compared to the numerical method of [3].
The rest of this paper is organized as follows. Background material, some deﬁnitions, and the
notation are presented in Section 2. Speciﬁcally, therein, we formally introduce multivariate Hawkes
processes and directed information graphs. In Section 3, we establish the connection between the
excitation matrix and the corresponding DIG. In Section 4, we propose an algorithm for learning
the excitation matrix or equivalently the DIG of a class of stationary multivariate linear Hawkes
processes. Section 5 illustrates the performance of the proposed algorithm in inferring the causal
structure in a network of synthesized mutually exciting linear Hawkes processes and in stock market.
Finally, we conclude our work in Section 6.

2 Preliminary Deﬁnitions

In this Section we review some basic deﬁnitions and our notation. We denote random processes
by capital letters and a collection of m random processes by X [m] = {X1, ..., Xm}, where [m] :=
{1, ..., m}. We denote the ith random process at time t by Xi(t), the random process Xi from time
i,s, and a subset K ⊆ [m] of random process up to time t by X tK. The Laplace
s up to time t by X t

2

transform and Fourier Transform of Xi are denoted, respectively by

(cid:90) ∞
(cid:90) ∞

0

−∞

L[Xi](s) =
F[Xi](ω) =

Xi(t)e−stdt,

Xi(t)e−jωtdt,

(1)

(2)

(cid:82)

√−1. The convolution between two functions f and g is deﬁned as f ∗ g(t) :=

where j =
R f (x)g(t − x)dx. The joint distribution of processes {X n

1 , ..., X n

m} is represented by PX (n).

2.1 Multivariate Hawkes Processes
Fix a complete probability space (Ω,F, P ). Let N (t) denotes the counting process representing the
cumulative number of events up to time t and let {F t}t≥0 be a set of increasing σ-algebras such that
F t = σ{N t}. The non-negative, F t-measurable process λ(t) is called the intensity of N (t) if

P (N (t + dt) − N (t) = 1|F t) = λ(t)dt + o(dt).

A classical example of mutually exciting processes, a multivariate Hawkes process [8], is a multidi-
mensional process N (t) = {N1, ..., Nm} such that for each i ∈ [m]

P(cid:0)dNi(t) = 1|F t(cid:1) = λi(t)dt + o(dt),

P (dNi(t) > 1|F t) = o(dt),

(cid:90) t

m(cid:88)
(cid:90) t

k=1

0

where F t = σ{N t}. The above equations imply that E[dNi(t)/dt|F t] = λi(t). Furthermore, the
intensities are all positive and are given by

λi(t) = vi +

γi,k(t − t(cid:48))dNk(t(cid:48)).

(3)

The exciting functions γi,k(·)s are in (cid:96)1 such that λi(t) ≥ 0 for all t > 0. Equivalently, in matrix
representation:

Λ(t) = v +

(4)
where Γ(·) denotes an m × m matrix with entries γi,j(·); dN , Λ(·), and v are m × 1 arrays with
entries dNi, λi(·), and vi, respectively. Matrix Γ(·) is called the excitation (kernel) matrix. Figure
1 illustrates the intensities of a multivariate Hawkes process comprised of two processes (m = 2)
with the following parameters

0

Γ(t − t(cid:48))dN (t(cid:48)),

(cid:18)0.5
(cid:19)

0.4

v =

, Γ(t) =

(cid:18) 0.1e−t

0.5e−0.9t

(cid:19)

0.3e−1.1t
0.3e−t

u(t),

where u(t) is the unit step function.
Assumption 1. A joint distribution is called positive (non-degenerate), if there exists a reference
measure φ such that PX (cid:28) φ and dPX
dφ > 0, where PX (cid:28) φ denotes that PX is absolutely
continuous with respect to φ1.

Note that the Assumption 1 states that none of the processes is fully determined by the other pro-
cesses.

2.2 Causal Structure

A causal model allows the factorization of the joint distribution in some speciﬁc ways. Generative
model graphs are a type of graphical model that similar to Bayesian networks [17] represent a causal
factorization of the joint [19]. More precisely, it was shown in [19] that under Assumption 1, the

1A measure PX on Borel subsets of the real line is absolutely continuous with respect to measure φ if for

every measurable set B, φ(B) = 0 implies PX (B) = 0.

3

Figure 1: Intensities of the multivariate Hawkes process.

joint distribution of a causal2 discrete-time dynamical system with m processes can be factorized as
follows,

(5)
where B(i) ⊆ −{i} is the minimal3 set of processes that causes process Xi, i.e., parent set of node
i in the corresponding minimal generative model graph. Such factorization of the joint distribution
is called minimal generative model. In Equation (5),

PXi||X Bi

PX =

,

m(cid:89)

i=1

n(cid:89)

t=1

PXi||X Bi

:=

PXi(t)|F t−1

B∪{i}

,

B∪{i}}.

B∪{i} = σ{X t−1

and F t−1
Extending the deﬁnition of generative model graphs to continuous-time systems requires some tech-
nicalities which are not necessary for the purpose of this paper. Hence we illustrate the general idea
through an example.
The following example demonstrates the minimal generative model graph of a simple continuous-
time system.
Example 1. Consider a dynamical system in which the processes evolve over time horizon [0, T ]
through the following coupled differential equations:

dX1 = f (X1, X2)dt + dW,
dX2 = g(X2)dt + dU,
dX3 = h(X1, X2, X3)dt + dV,

where W, U and V are independent exogenous noises. For small time dt, this becomes,

dX1(t + dt) ≈ ∆f (X1(t), X2(t)) + dW (t),
dX2(t + dt) ≈ ∆g(X2(t)) + dU (t),
dX3(t + dt) ≈ ∆h(X1(t), X2(t), X3(t)) + dV (t).

(6)

In this example, since the system is causal, the corresponding joint distribution can be factorized as
follows,

PX =

PXj (T−kdt)|F T−(k+1)dt ,

(7)

3(cid:89)

(cid:89)

j=1

k≥0

2In causal systems, given the full past of the system, the present of the processes become independent.
3Minimal in terms of its cardinality.

4

05101500.20.40.60.81Intensity, λ1Event−occurrence time of N105101500.511.52Intensity, λ2Event−occurrence time of N2shhhhhhhhhhhhhhhh
+VVVVVVVVVVVVVVVV

X1

X2

X3

Figure 2: Minimal generative model graph of Example 1.

where F T−(k+1)dt = σ{X T−(k+1)dt

{1,2,3}

}. Due to (6), we can rewrite (7) as

PX = PX1||X2PX2PX3||X1,X2.

(8)

Figure 2 demonstrates the corresponding generative model graph of the factorization in (8).

the joint distribution of a causal dynamical system can be factorized as PX =
, where B(i) ⊆ −{i} is the parent set of node i in the corresponding minimal gen-

(cid:81)m

In general,

i=1 PXi||X Bi

erative model graph, and

(cid:89)

k≥0

PXi||X Bi

=

PXi(T−kdt)|F T−(k+1)dt

Bi

.

3 Two Equivalent Notions of Causality for Multivariate Hawkes Processes

In linear multivariate Hawkes processes, a natural notion of causation exists in the following sense:
if γi,j (cid:54)= 0, then occurrence of an event in jth process will affect the likelihood of the arrivals in ith
process. Next, we establish the relationship between the excitation matrix of multivariate Hawkes
processes and their generative model graph. To do so, ﬁrst, we discuss the equivalence of directed
information graphs and generative models graphs which was established in [20].

3.1 Directed Information Graphs (DIGs)

An alternative graphical model to encode statistical interdependencies in stochastic causal dynam-
ical systems are directed information graphs (DIGs) [19]. Such graphs are deﬁned based on an
information-theoretic quantity, directed information (DI) and it was shown in [20] that under some
mild assumptions, they are equivalent to the minimal generative model graphs. Hence, DIGs also
represent a minimal factorization of the joint distribution.
In a DIG, to determine whether Xj causes Xi over a time horizon [0, T ] in a network of m random
processes, two conditional probabilities are compared in KL-divergence sense: one is the conditional
probability of Xi(t + dt) given full past, i.e., F t := σ{X t} and the other one is the conditional
probability of Xi(t + dt) given full past except the past of Xj, i.e., F t−{j} := σ{X t−{j}}. It is
declared that there is no inﬂuence from Xj on Xi, if the two conditional probabilities are the same.
More precisely, there is an inﬂuence from Xj on Xi if and only if the following directed information
measure is positive [19],

IT (Xj → Xi||X−{i,j}) := inf

(9)
where −{i, j} := [m] \ {i, j}, T denotes the set of all ﬁnite partitions of the time interval [0, T ]
[23], and

t∈T (0,T )

˜It(Xj → Xi||X−{i,j}),

where t := (0 = t0, t1, ..., tn = T ). Finally, I(X; Y |Z) represents the conditional mutual informa-
tion between X and X given Z and it is given by

˜It(Xj → Xi||X−{i,j}) :=

I

X tk

i,tk−1

; X tk

n(cid:88)

k=0

(cid:16)

(cid:20)

(cid:17)

,

j,0|F tk−1−{j}
(cid:21)

log

dPX|Y,Z
dPX|Z

.

I(X; Y |Z) := EPX,Y,Z

5

s
+


3.2 Equivalence between Generative Model Graph and Support of Excitation Matrix

As mentioned earlier, the corresponding minimal generative model graph and the DIG of a causal
dynamical system are equivalent. Thus, to characterize the corresponding minimal generative model
graphs of a multivariate Hawkes system, we study the properties of its corresponding DIG.
Proposition 1. Consider a set of mutually exciting processes N with excitation matrix Γ(t). Under
Assumption 1, IT (Nj → Ni||N−{i,j}) = 0 if and only if γi,j ≡ 0 over time interval [0, T ].
Proof: See Section 7.1. (cid:3)
Proposition 1 signiﬁes that the support of the excitation matrix Γ(·) determines the adjacency ma-
trix of the DIG and vice versa. Therefore, learning DIG of a mutually exciting Hawkes processes
satisfying Assumption 1 is equivalent to learning the excitation matrix given samples from each of
the processes. In other word, in the presence of side information that the processes are Hawkes, it
is more efﬁcient to learn the causal structure through learning the excitation matrix rather than the
directed information needed for learning the DIG in general.

4 Learning the Excitation Matrix

In this section, we present an approach for learning the causal structure of a stationary Hawkes
network with exponential exciting functions through learning the excitation matrix. This method
is based on second order statistic of the Hawkes processes and it is suitable for the case when no
i.i.d. samples are available. Note that when i.i.d. samples are available, non-parametric methods
for learning the excitation matrix such as MMEL algorithm [25] exist. In this approach the exciting
functions are expressed as linear combination of a set of base kernels and a penalized likelihood
is used to estimate the parameters of the model. As mentioned earlier, we focus on learning the
excitation matrix of multivariate Hawkes processes with exponential exciting functions. This class
of Hawkes processes has been widely applied in many areas such as seismology, criminology, and
ﬁnance [15, 21, 13, 5].
Deﬁnition 1. The excitation matrix of a multivariate Hawkes processes with exponential exciting
functions is deﬁned as follows

Exp(m) := { D(cid:88)

d=1

D(cid:88)

Ade−βdtu(t) : Ad ∈ Rm×m,

D(cid:88)

Ade−βdt)i,j ≥ 0, ρ(
where {βd} > 0 is called the set of exciting modes.
Example 2. Consider a set of m = 5 mutually exciting processes with the following exponential

) < 1, D ∈ N},

Ad
βd

(10)

d=1

d=1

(

excitation matrix 2
1

0
0
0
0

+

0
0
0
0
1

 e−t
 e−2t

20

+

20

 0

0
0
.1
0

0
0
1
0
0

.5
0
0
0
0

0
0
2.5
0
1

 e−1.4t

20

0
2
0
0
0

(11)

0
0
1.5
0
0

0
.5
0
0
0

0
0
0
1.3
0

1.5
0
0
0
0

1
0
2
0
0

0
0
0 −1
0
0
0
0
0
0

0
0
2
0

In this example D = 3 and the exciting modes are {1, 1.4, 2}. By Proposition 1, the adjacency
matrix of the corresponding DIG of this network is given by the support of its excitation matrix.
Figure 3 depicts the corresponding DIG.

Before describing our algorithm, we need to derive some useful properties of moments of the pro-
cess. A multivariate Hawkes process with the excitation matrix Γ has stationary increments, i.e., the
intensity processes is stationary, if and only if the following assumption holds [8, 6]:

6

N1



U**************

aBBBBBB
K


N2

N5

S''''''

Figure 3: Corresponding DIG of the network in Example 2 with the excitation matrix given by (11)

N4

N3

Assumption 2. The spectral radius (the supremum of the absolute values of the eigenvalues) of the
matrix Γ, where [Γ]i,j = ||γi,j||1 is strictly less than one, i.e., ρ(Γ) < 1.
In this case, from (4) and Equation (2):

Λ = E[Λ(t)] = v +

Γ(t − t(cid:48))E[dN (t(cid:48))]

= v +

0

Γ(t − t(cid:48))Λdt(cid:48) = v + ΓΛ.

(12)

(cid:90) t

(cid:90) t

0

(cid:20)(cid:90) t+z

By Assumption 2,(cid:80)

i converges to (I − Γ)−1, thus Λ = (I − Γ)−1v. The normalized co-
variance matrix of a stationary multivariate Hawkes process with lag τ and window size z > 0 is
deﬁned by

i≥0 Γ

(cid:90) t+τ +z

(cid:21)

(dN (y))T

− ΛΛT z,

(13)

Σz(τ ) :=

E

1
z

dN (x)

t

t+τ

dN (x) denotes the number of events in time interval (t, t + t(cid:48)].

where(cid:82) t+t(cid:48)

t

Theorem 1. [1] The Fourier transform of the normalized covariance matrix of a stationary multi-
variate Hawkes process with lag τ and window size z > 0 is given by

F[Σz](−ω)

(14)

= 4

sin2 zω/2

ω2z

(I − F[Γ](ω))

−1 diag(Λ) (I − F[Γ](ω))

−†

,

where A† denotes the Hermitian conjugate of matrix A, and diag(Λ) is a diagonal matrix with
vector Λ as the main diagonal.

In order to learn the excitation matrix with exponential exciting functions, we need to learn the
exciting modes {βd}, the number of components D, and coefﬁcient matrices {Ad}. Next results
establishes the relationship between the exciting modes and the number of components D with the
normalized covariance matrix of the process.
Corollary 1. Consider a network of a stationary multivariate Hawkes processes with excitation
matrix Γ(t) belonging to Exp(m). Then the exciting modes of Γ(t) are the absolute values of the
zeros of 1/T rF[Σz]−1(ω).
Proof: See Section 7.2. (cid:3)
Next, we need to ﬁnd the coefﬁcient matrices {Ad}. To do so, we use the covariance density of the
processes. The covariance density of a stationary multivariate Hawkes process for τ > 0 is deﬁned
as [8]

Ω(τ ) := E(cid:2)(dN (t + τ )/dt − Λ)(dN (t)/dt − Λ)T(cid:3) .

(15)

Since the processes have stationary increments, we have Ω(−τ ) = ΩT (τ ).
Lemma 1. [8]

Ω(τ ) = Γ(τ )diag(Λ) + Γ ∗ Ω(τ ), τ > 0.

(16)

7

a
U
K
/
/

/
/

S
It has been shown in [3] that the above equation admit a unique solution for Γ(τ ). Next proposition
provides a system of linear equations that allows us to learn the coefﬁcient matrices.
Proposition 2. Consider a network of a stationary multivariate Hawkes processes with excitation
matrix Γ(t) ∈ Exp(m), and exciting modes {β1, ..., βD}. Then {Ad} are a solution of the linear
system of equations: S = AH, where Hm2×m2 is a block matrix with (i, j)th block given by

diag(Λ) + L[Ω](βj) + L[Ω]T (βi)

,

Hi,j =

βj + βi
and A = [A1, ..., AD] and S = [L[Ω](β1), ...,L[Ω](βD)].
Proof: See Section 7.3.(cid:3)
Combining the results of Corollary 1 and Proposition 2 allows us to learn the excitation matrix of
exponential multivariate Hawkes processes from the second order moments. Consequently applying
Proposition 1, the causal structure of the network can be learned by drawing an arrow from node i

to j, when(cid:80)D

d=1|(Ad)j,i|> 0.

4.1 Estimation and Algorithm

This section discusses estimators for the second order moments, namely the normalized covariance
matrix and the covariance density of a stationary multivariate Hawkes processes from data. Once
such estimators are available, the approach of previous section maybe used to learn the network. The
most intuitive estimator for Λ deﬁned by Equation (12) is N (T )/T . It turns out that this estimator
converges almost surely to Λ as T goes to inﬁnity [2]. Furthermore, [2] proposes an empirical
estimator for the normalized covariance matrix as follows

(cid:98)Σz,T (τ ) :=

(cid:98)T /z(cid:99)(cid:88)

i=1

1
T

(Xiz − X(i−1)z)(Xiz+τ − X(i−1)z+τ )T ,

(17)

where Xt := N (t) − Λt. In the same paper, it has been shown that under Assumption 2, the above
T→∞Σz(τ ). Notice
that the normalized covariance matrix and the covariance density are related by Σdt(τ )/dt = ΩT (τ ).
Therefore, we can estimate the covariance density matrix using Equation (17) by choosing small

estimator converges in (cid:96)2 to the normalized covariance matrix (13), i.e.,(cid:98)Σz,T (τ ) −→
enough window size z = ∆. Namely,(cid:98)ΩT

∆(τ ) =(cid:98)Σ∆(τ )/∆.

Algorithm 1
1: Input : N T .
2: Output : DIG.

3: (cid:98)Λ ← N (T )/T
5: Compute(cid:98)Σz,T (τ ) and(cid:98)Ω∆(τ ) using (17).
4: Choose σ > 0, z > 0, and small ∆ > 0.
6: {(cid:98)βd}(cid:98)D
7: Compute L[(cid:98)Ω∆]((cid:98)βd) for d = 1, ...,(cid:98)D.
d=1 ← Zeros of 1/T rF[Σz]−1(ω).
8: Solve the set of equations arises from (20) for (cid:98)Ad.
9: Draw (j, i) if(cid:80)(cid:98)D
d=1|((cid:98)Ad)i,j|≥ σ.

Algorithm 1 summarizes the steps of our proposed approach for learning the excitation matrix and
consequently the causal structure of an exponential multivariate Hawkes process.

5 Experimental Results

In this section, we present our experimental results for both synthetic and real data.

8

N5

N1



U++++++++++++

aCCCCC
K


N2

N5

S'''''

N1



U++++++++++++

aCCCCC
K


N2

N1

aCCCCC
U++++++++++++

xrrrrrrrrrrr


N2

N5

S'''''

N5

S'''''

N1

H
U++++++++++++
aCCCCC

8rrrrrrrrrrr
K


N2

N4

N3

N4

N3

N4

N3

N4

N3

(a) T = 1000

(b) T = 2100

(c) Numerical method

(d) MMEL

Figure 4: Recovered DIG of the network in Example 2 with the excitation matrix given by (11), (a),
(b) Algorithm 1 with ∆ = 0.2, z = 2, and T ∈ {1000, 2100}, (c) the numerical method of [3] with
Q = 70 and T = 2100, and (d) MMEL with 35 i.i.d. samples each of length 60. Our approach
learns the graph with T = 2100, while other approaches fail at the same sample size.

N15

N14

N2

N3

N5

N4

N1

lZZZZZZ
,ZZZZZZ
gNNNNNNNNNNNNNNNNNNNN
'NNNNNNNNNNNNNNNNNNNN
9rrrrrrrrrrrrrrrr
ukkkkkkkkkkk
X00000000000000000000000000000
];;;;;;;;;;;;;;;;;;;;;;;;;;;;
E
00000000000000000000000000000
;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"FFFFFFFFFFFFFFFFFFFFFFF
o_____________
2dddddddddddddddddddd
lYYYYYYYYYYYYYYYYYYYY
4jjjjjjjjjjjjjjjjjjjjjjjj
jUUUUUUUUUUUUUUUUUUUUUUUUUU

tjjjjjjjjjjjjjjjjjjjjjjjj
gNNNNNNNNNNNNNNNNNNNNNNNNNNNNN
E
W0000000000000000000000
J
#FFFF
99999999999999999
]::::::::::::::::::::::::::::::::
bFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF
=zzzzzzzzz
I
0000000000000000000000000000000
::::::::::::::::::::::::::::::::
&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&
ysssssssssssssssssssssssssss
xqqq
~|||||||||||||||||||||||||||||
}zzzzzzzzzu



.^^^^^^^^^^^^^^^^^^^^^^^^^
lZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZ
,ZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZ
jTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTT
fNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN
&NNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN
1ccccccccccccccccccccccccccccc
bEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE
B
::::::::::::::::::::::::::::::::
............................
qccccccccccccccccccccccccccccc
5jjjjjjjjjjjjjjjjjjjjjjjjjjjjjjj
ujjjjjjjjjjjjjjjjjjjjjjjjjjjjjjj
T)))))))))))))))
aCCC
F

!CCC

lZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZ
,ZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZ
jTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTT
*TTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTT
fMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMM
Q$$$$$$$$$$$
bEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE
W...................
77777777777777777777777777
ukkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkk
F
ztttttttttttttttttttttttttttttt

vmmmmmmmmmmmmmmmmmmmmmmmmmmmm
;wwwwwwwwwwwwwwwwwwwwwww
L
B
F
jUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUU
*UUUUUUUUUUUUUUUUUUUUUUUUUUUUUUU
gNNNNNNNNNNNNNNNNNNNNNNNNNN

[888888888888
))))
!CCCCCCCCCCCCCCCCCCC
/````````````````````````````````
o````````````````````````````````
3ffffffffffffffffffffffffff
7nnnnnnnnnnnnnnnnnnn
mZZZZZZZZZZZZZZZZZZZZZZZZZZZ
-ZZZZZZZZZZZZZZZZZZZZZZZZZZZ
;wwwwwwwwww 8
*UUUUUUUUUUUUUUUUUUUUUU
bFFFFF
%LLLLLLLLLLLLL
"FFFFF
4iiiiiiiiiiii
tiiiiiiiiiiii
8qqq

hQQQQQ
(QQQQQ

N13

N12

N9

N6

N7

N8

N11

N10

Figure 5: True causal structure of the synthesized example.

5.1 Synthetic Data

We apply the proposed algorithms to learn the causal structure of the multivariate Hawkes net-
work of Example 2 with v = (0.5, 0.4, 0.5, 1, 0.3)T . This network satisﬁes Assumption 2, since
ρ(Γ) ≈ 0.16. The exciting modes are {1, 1.4, 2}. We observed the arrivals of all processes during
a time period T . Figure 4 depicts the outputs of algorithms 1 for ∆ = 0.2, z = 2, and observation
lengths T ∈ {1000, 2100}. As illustrated in Figure 4, by increasing the length of observation T , the
output graph converges the true DIG shown in Figure 3. As a comparison, we applied the MMEL
algorithm proposed in [25] to learn the excitation matrix for this example and the numerical method
based on Nystrom method proposed in [3] with T = 2100 and the number of quadrature Q = 70.
Since MMEL requires i.i.d. samples, we generate 35 i.i.d. samples each of length 60 to obtain
Figure 4(MMEL). Our proposed algorithm outperforms both MMEL and the numerical method of
[3].
Furthermore, we conducted another experiment for a network of 15 processes with 102 edges il-
lustrated in Figure 5. For a sample of length T = 2500, our algorithm was able to recover 70
edges correctly but identiﬁed 34 false arrows. MMEL could only recover 58 arrows correctly while
detecting another 41 false arrows. The input for MMEL was 25 sequences each of length 100.

5.2 Stock Market Data

As an example of how our approach may discover causal structure in real-world data, we analyzed
the causal relationship between stock prices of 12 technology companies of the New York Stock

9

a
U
K
/
/

/
/
o
o

a
U
K
/
/

/
/

S
a
U
/
/

o
o
S
/
/

x
a
U
H
K
/
/
/
/

S
8
2
.
,
&



x




}
,
'
"



o
l
#





~
y
t
l
!

u
q
l
j
g
a

z
u
j
g

v
l
f
b
]
W
T
L
o
j
b
]
X
O
O
F
t
m
j
f
F
B
;
g
b
O
O
J
F
;
7
4
h
E
3
(
b
[
W
I
E
/
-
*
%
"
Q
5
*
!

B
=
9
4
1
,
*

Apple

EMC

Xerox

qddd

Dell

Google

)SSS
W//////////////////////
Q########################
\888888888888888888
cFFFFFFFFFFFF
D


















########################
#FFFFFFFFFFFF
Cisco
hQQQQQQQQQQQQQQQQQ
C
cGGGGGGGGGGGGGGGGGGGGG
R%%%%%%%%%%%%%%%%%%%%%
V,,,,,,,,,,,,,,,,,,,,,,,,
qdddddddddddddd
Z44444
***********

(QQQQQQQQQQQQQQQQQQQQQ
Z6666666666666666666666
Q$$$$$$$$$$$$$
6666666666666666666666
.................
tiiiiiiiiiiiiiiiiiiii
8pppppppppppppppppppp


hQQQQQQQQQQQQQQQQQQQQQ
3gggggggggggggggggg
?

7nnnn
wnnnn

Texas

Oracle

IBM

MSFT

(a) Algorithm 1.

Apple

HP

EMC

Intel

Xerox

qddd

)SSS

Dell

C

Cisco

Google

Q########################
cFFFFFFFFFFFF
\888888888888888888
W//////////////////////
hQQQQQQQQQQQQQQQQQ
V,,,,,,,,,,,,,,,,,,,,,,,,
R%%%%%%%%%%%%%%%%%%%%%
cGGGGGGGGGGGGGGGGGGGGG
F
K
Z44444
(QQQQQQQQQQQQQQQQQQQQQ
Q$$$$$$$$$$$$$
Z6666666666666666666666
6666666666666666666666
.................
tiiiiiiiiiiiiiiiiiiii
8pppppppppppppppppppp


hQQQQQQQQQQQQQQQQQQQQQ
3gggggggggggggggggg
?
eKKKKKKKKKKKKK
qccccccccc
7nnnn
wnnnn
MSFT
(b) DIG.

Oracle

Texas

IBM

Apple

qddd

Dell

Google

)SSS
Cisco
C
qdddddddddddddd

\888888888888888888
W//////////////////////
cFFFFFFFFFFFF
D


















Q########################
########################
888888888888888888
hQQQQQQQQQQQQQQQQQ
R%%%%%%%%%%%%%%%%%%%%%
V,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,
Z44444
K
$$$$$$$$$$$$$$$$$
(QQQQQQQQQQQQQQQQQQQQQ
Q$$$$$$$$$$$$$
.................
6666666666666666666666
8pppppppppppppppppppp

,YYYYYYYYYYYYYYYYYYYYY

3gggggggggggggggggg
?
eKKKKKKKKKKKKK

7nnnn
wnnnn

Texas

Oracle

IBM

HP

Intel

HP

EMC

Intel

Xerox

MSFT

(c) MMEL.

Figure 6: Causal structures for the S&P (a) using Algorithm 1, (b) by estimating the directed infor-
mation DIG, and (c) using MMEL algorithm.

Exchange sourced from Google Finance. The prices were sampled every 2 minutes for twenty
market days (03/03/2008 - 03/28/2008). Every time a stock price changed by ±1% of its current
price an event was logged on the stock’s process. In order to prevent the substantial changes in
stock’s prices due to the opening and closing of the market, we ignored the samples at the beginning
and at the end of each working day. For this part, we have assumed that the jumps occurring in
stock’s prices are correlated through a multivariate Hawkes process. This model class was advocated
in [11, 2]. Figure 6(a) illustrate the causal graph resulting from Algorithm 1, with z = 30 and ∆ = 2
minutes.
To compare our learning approach with other approaches, we applied the MMEL algorithm to learn
the corresponding causal graph. For this scenario, we assumed that the data collected from each day
is generated i.i.d. Hence, a total of 20 i.i.d. samples were used. Figure 6(c) illustrates the resulting
graph. As one can see, Figures 6(a) and 6(c) convey pretty much a similar causal interactions in the
dataset. For instance both of these graphs suggest that one of the most inﬂuential companies in that
period of time was Hewlett-Packard (HP). Looking into the global PC market share during 2008, we
ﬁnd that this was indeed the case.4
To use another modality, we derive the corresponding DIG of this network applying Equation (9).
For this part, we used the market based on the Black-Scholes model [4] in which the stock’s prices
are modeled via a set of coupled stochastic PDEs. We assumed that the logarithm of the stock’s
prices are jointly Gaussian and therefore the corresponding DIs were estimated using Equation (24)
in [7]. The resulting DIG is shown in Figure 6(b). Note that this DIG is derived from the logarithm
of prices and not the jump processes we used earlier. Still it shares a lot of similarities with the
two other graphs. For instance, it also identiﬁes HP as one of the most inﬂuential companies and
Microsoft as one the most inﬂuenced companies in that time period.

Alg. 1
DIG
MMEL

Alg. 1 DIG MMEL
33
25
26

26
24
34

25
30
24

This table shows the number of edges that each of the above approaches recovers and the number of
edges that they jointly recover. This demonstrates the power of exponential kernels even when data
does not come from such a model class.

5.3 MemeTracker Data

We also studied causal inﬂuences in a blogosphere. The causal ﬂow of information between media
sites may be captured by studying hyperlinks provided in one media site to others. Speciﬁcally, the

4Gartner, http://www.gartner.com/newsroom/id/856712

10

C


(
q
)
Q
#

q



h
c
Z

t
c
\

Z
Q
V
?
7
R
W
w
h
8
3
D
C
(


q
)
h
c
Z


t
c
\
q
w
h
W
e
Z
V
Q
K
?
7
R
F
Q
8
3
q
Q
C
V
h
R
q
c
D
Q
)
Z
K
8
(
?



,
3




w
W

\
j
j



7
e
craigslist.org
yelp.com

Cr
Ye
Am amazon.com
Sp
spiegel.de
Wi
wikipedia.org
Yo
youtube.com
Cn
cnn.com
Gu
guardian.co.uk
Hu
humanevents.com
Bb
bbc.co.uk

Table 1: List of websites studied in MemeTracker experiment.

time of such linking can be modeled using a linear multivariate Hawkes processes with exponential
exciting functions [25, 18]. This model is also intuitive in the sense that after emerging a new hot
topic, in the ﬁrst several days, the blogs or websites are more likely feature that topics and it is
also more likely that the topic would trigger further discussions and create more hyperlinks. Thus,
exponential exciting functions are well suited to capture such phenomenon as the exiting functions
should have relatively large values at ﬁrst and decay fast as time elapses.
For this experiment, we used the MemeTracker5 dataset. The data contains time-stamped phrase and
hyperlink information for news media articles and blog posts from over a million different websites.
We extracted the times that hyperlinks to 10 well-known websites listed in Table 1 are created during
August 2008 to April 2009. When a hyperlink to a website is created at a certain time, an arrival
events is recorded at that time. More precisely, in this experiment, we picked 30 different phrases
that appeared on different websites at different times. If a website that published one of the phrases
at time t also contained a hyperlink to one of the 10 listed websites, an arrival event was recorded at
time t for that website in our list.
Figure 7(a) illustrates the resulting causal structure learned by Algorithm 1 for z = 12 hours and
∆ = 1 hour. In this graph, an arrow from a node to another, say node Ye to Yo, means creating a
hyperlink to yelp.com triggers creation of further hyperlinks to youtube.com.
We also applied the MMEL algorithm with one exponential kernel function to learn the excitation
matrix. For this experiment, the data corresponding to each phrase was treated as an i.i.d. realization
of the system. The resulting causal structure is depicted in Figure 7(b).
As Figure 7(a) illustrates, the nodes can be clustered into two main groups: {Cr, Ye, Am, Yo} and
{Bb, Cn, Gu, Hu, Sp, Wi}. The ﬁrst group consists of mainly merchandise and reviewing websites
and the second group contains the broadcasting websites. However, this is not as clear in Figure
7(b). This is because MMEL requires more i.i.d. samples (phrases) to be able to identify the correct
arrows. Note that as we increase the number of phrases (110), Figure 7(c), both graphs become
similar with two clearly visible main clusters.

6 Conclusion and Future Work

Learning the causal structure (DIG) of a stochastic network of processes requires estimation of
conditional directed information (9). Estimating this quantity in general has high complexity and
requires a large number of samples. However, the complexity of the learning task could be signiﬁ-
cantly reduced, if side information about the underlying structure of system dynamics is available.
As proved in 1, for multivariate Hawkes processes, estimating the support of the excitation matrix
sufﬁces to learn the associated DIG. Therefore, all approaches for learning the excitation matrix
of the multivariate Hawkes processes such as ML estimation [16, 25], EM algorithm [10], non-
parametric estimation techniques proposed in [2], and the proposed method in this paper may be
used to learn the causal interactions in such networks. The previous estimation approaches either

5http://memetracker.org/data/links.html

11

Am

Sp

Ye

Wi

J
B
 J


H

X111111
111111

7ooow
wooo
`AAAA
,ZZZZZZZZZZZZZZZ

))))))))))))))))
88888888888

J
J
/////

G
A


o_______________
4hhhhhhhhhh
yttt9
9ttt

Cn

Yo

Hu

Gu

Cr

Bb

Cr

Bb

Cr

Am

Sp

Ye

Wi

7ooo
J
B
 J
[88888888888
)))))))))))))))) [
88888888888`
`AAAA


,ZZZZZZZZZZZZZZZl
lZZZZZZZZZZZZZZZ

3hhhhhhhhhhhhhhhhhhh

J
J
/////

tiiiiiiiiiiiiiiiiiii4
4iiiiiiiiiiiiiiiiiii
X111111
111111
A


/_______________t
o_______________/
4hhhhhhhhhh
yttt9
9ttt

Cn

Yo

/____

Hu

Gu

Am

7ooow
wooo

Ye

J
B
 J



X111111
111111

Yo

Sp

Wi

`AAAA

88888888888`
))))))))))))))))
J
/////

G
A


4hhhhhhhhhh
yttt9
9ttt

Cn

Hu

Gu

Bb

(a) Alg. 1 (30).

(b) MMEL.

(c) Both Alg. 1 & MMEL (110).

Figure 7: Recovered causal structure of the MemeTracker dataset using (a) Algorithm 1, (b) MMEL
for 30 different phrases, and (c) both Algorithm 1 and MMEL for 110 different phrases.

require i.i.d. samples such as MMEL or are limited to the class of symmetric Hawkes processes.
The proposed algorithm in this work allows us to learn the support of the excitation matrix in a larger
class of matrices in the absence of i.i.d. samples.

7 Technical Proofs

7.1 Proof of Proposition 1
Suppose γi,j ≡ 0. (3) implies that for every t ≤ T , λi(t) is F t−{j}(= σ{N t−{j}})-measurable and
from (2), we have
Equivalently, for every 0 ≤ tk−1 < tk,

P(cid:0)dNi(t) = 1|F t(cid:1) = P (dNi(t) = 1|F t−{j}).

(cid:16)

(cid:17)

N tk

i,tk−1

; N tk

j,0|F tk−1−{j}

I

= 0,

(18)

and thus, ˜It(Nj → Ni||N−{i,j}) = 0, for any ﬁnite partition t ∈ T (0, T ).
For the converse we use proof by contradiction. Suppose IT (Nj → Ni||N−{i,j}) = 0 and γi,j (cid:54)= 0.
Using the deﬁnition in (9), it is straightforward to observe that for any t < T ,

Similarly, It+dt(Nj → Ni||N−{i,j}) = 0. Consequently,

0 = It+dt(Nj → Ni||N−{i,j}) − It(Nj → Ni||N−{i,j})

It(Nj → Ni||N−{i,j}) = 0.
(cid:17)
(cid:16)

= I

dNi(t); N t

j,0|F t−{j}

.

This implies P (dNi(t) = 1|F t−{j}) = λi(t)dt + o(dt), or λi(t) is F t−{j}-measurable. Since, we
have assumed γi.j (cid:54)= 0, we obtain Nj(t) is F t−{j}-measurable, for all t ≤ T . In words, jth process
is determined by other processes which contradicts with the Assumption 1 that states there is no
deterministic relationships between processes.

7.2 Proof of Corollary 1
If the excitation matrix belongs to Exp(m), from Equation (14) we have

AT
d

jω + βd

diag(Λ)−1

Ad−jω + βd

(cid:32)
I − D(cid:88)

d=1

(cid:33)

(cid:32)

I − D(cid:88)

d=1

(cid:33)

=

4 sin2 zω/2

ω2z

F[Σz]−1(ω).

12

A
B
X


4


H
`








O
O



7
G
y
,
o
A
B
X


4









O
O



7

y
,
/
o
3
A
B
X


4


J




O
O



7
G
y
where ai,j = (cid:80)D

By evaluating the trace of the above equation, we obtain

m(cid:88)

|1 − ai,i|2

+

i=1

λi

(cid:88)

i(cid:54)=j

|ai,j|2
λi

=

4 sin2 zω/2

ω2z

T rF[Σz]

−1(ω),

(19)

 m(cid:88)

i=1

g(ω) :=

λi

+

i(cid:54)=j

a(d)

i,j−jω+βd

d=1

, and Ad = [a(d)

i,j ]. To learn the entire set {±jβd}, we have to show
that there are no pole zero cancellations in (19). That is, the nominator and denominator of (19)
(cid:88)
have no common roots. Let

|1 − ai,i|2

|−jω + βd|2,

 D(cid:89)

d=1

|ai,j|2
λi

which is the nominator of Equation (19). It is straightforward to check that for ω = −jβk, the above
quantity is non-zero, due to the fact that βds are distinct and Ak (cid:54)= 0. Since g(ω) is a polynomial
with real coefﬁcients, from complex conjugate root theorem [9], we have g(jβk) (cid:54)= 0. Therefore,
the set {±jβd} contains all the poles of (19).

7.3 Proof of Proposition 2

From Lemma 1, the Laplace transform of the covariance density can be written as

(cid:90) ∞

(cid:90) ∞

L[Ω](s) = L[Γ](s) (diag(Λ) + L[Ω](s))
Γ(t(cid:48))ΩT (t)e−s(t(cid:48)−t)dt(cid:48)dt.

+

When Γ(t) ∈ Exp(m), it can be shown that (1) becomes

0

t

D(cid:88)

d=1

L[Ω](s) =

Ad

s + βd

(cid:0)diag(Λ) + L[Ω](s) + L[Ω]T (βd)(cid:1) .

(20)

If the set of exciting modes are given, we can insert s = βd, for d = 1, . . . , D in the above equation
and obtain the system of D equations.

References

[1] Emmanuel Bacry, Khalil Dayri, and Jean-Francois Muzy. Non-parametric kernel estimation
for symmetric hawkes processes. application to high frequency ﬁnancial data. The European
Physical Journal B, 85(5):1–12, 2012.

[2] Emmanuel Bacry, Sylvain Delattre, Marc Hoffmann, and Jean-Francois Muzy. Some limit
theorems for hawkes processes and application to ﬁnancial statistics. Stochastic Processes and
their Applications, 123(7):2475–2499, 2013.

[3] Emmanuel Bacry and Jean-Francois Muzy. Second order statistics characterization of hawkes

processes and non-parametric estimation. preprint arXiv:1401.0903, 2014.

[4] Fischer Black and Myron Scholes. The pricing of options and corporate liabilities. The journal

of political economy, pages 637–654, 1973.

[5] Clive G Bowsher. Modelling security market events in continuous time: Intensity based, mul-

tivariate point process models. Journal of Econometrics, 141(2):876–912, 2007.

[6] Pierre Br´emaud and Laurent Massouli´e. Stability of nonlinear hawkes processes. The Annals

of Probability, pages 1563–1588, 1996.

[7] Jalal Etesami and Negar Kiyavash. Directed information graphs: A generalization of linear

dynamical graphs. In American Control Conference (ACC), 2014, pages 2563–2568. IEEE.

[8] Alan G Hawkes.

Spectra of some self-exciting and mutually exciting point processes.

Biometrika, 58(1):83–90, 1971.

[9] Alan Jeffrey. Complex analysis and applications, volume 10. CRC Press, 2005.

13

[10] Erik Lewis and George Mohler. A nonparametric em algorithm for multiscale hawkes pro-

cesses. Preprint, 2011.

[11] Scott W Linderman and Ryan P Adams. Discovering latent network structure in point process

data. preprint arXiv:1402.0914, 2014.

[12] Thomas Josef Liniger. Multivariate hawkes processes. PhD thesis, Diss., Eidgen¨ossische

Technische Hochschule ETH Z¨urich, Nr. 18403, 2009, 2009.

[13] George O Mohler, Martin B Short, P Jeffrey Brantingham, Frederic Paik Schoenberg, and
George E Tita. Self-exciting point process modeling of crime. Journal of the American Statis-
tical Association, 106(493), 2011.

[14] Ioane Muni Toke and Fabrizio Pomponio. Modelling trades-through in a limited order book

using hawkes processes. Economics discussion paper, (2011-32), 2011.

[15] Yosihiko Ogata. Seismicity analysis through point-process modeling: A review. Pure and

Applied Geophysics, 155(2-4):471–507, 1999.

[16] T Ozaki. Maximum likelihood estimation of hawkes’ self-exciting point processes. Annals of

the Institute of Statistical Mathematics, 31(1):145–155, 1979.

[17] Judea Pearl. Probabilistic reasoning in intelligent systems: networks of plausible inference.

Morgan Kaufmann, 1988.

[18] Julio Cesar Louzada Pinto, Tijani Chahed, and Eitan Altman. Trend detection in social net-
works using hawkes processes. In Proceedings of the 2015 IEEE/ACM International Confer-
ence on Advances in Social Networks Analysis and Mining 2015, pages 1441–1448. ACM,
2015.

[19] Christopher Quinn, Negar Kiyavash, and Todd P Coleman. Directed information graphs.

Transactions on Information Theory, 61(12):6887–6909, 2015.

[20] Christopher J Quinn, Negar Kiyavash, and Todd P Coleman. Equivalence between minimal
generative model graphs and directed information graphs. In Information Theory Proceedings
(ISIT), 2011 IEEE International Symposium on, pages 293–297. IEEE, 2011.

[21] Patricia Reynaud-Bouret, Sophie Schbath, et al. Adaptive estimation for hawkes processes;

application to genome analysis. The Annals of Statistics, 38(5):2781–2822, 2010.

[22] Thomas Schreiber. Measuring information transfer. Physical review letters, 85(2):461, 2000.
[23] Tsachy Weissman, Young-Han Kim, and Haim H Permuter. Directed information, causal es-
timation, and communication in continuous time. Information Theory, IEEE Transactions on,
59(3):1271–1287, 2013.

[24] Shuang-Hong Yang and Hongyuan Zha. Mixture of mutually exciting processes for viral dif-
fusion. In Proceedings of the 30th International Conference on Machine Learning (ICML-13),
pages 1–9, 2013.

[25] Ke Zhou, Hongyuan Zha, and Le Song. Learning triggering kernels for multi-dimensional
hawkes processes. In Proceedings of the 30th International Conference on Machine Learning
(ICML-13), pages 1301–1309, 2013.

14

