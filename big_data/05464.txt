Hierarchy and Expansiveness in Two-Dimensional Subshifts of

Finite Type

Charalampos Zinoviadis

March 18, 2016

6
1
0
2

 
r
a

 

M
7
1

 
 
]
S
D
h
t
a
m

.

[
 
 

1
v
4
6
4
5
0

.

3
0
6
1
:
v
i
X
r
a

Abstract

Subshifts are sets of conﬁgurations over an inﬁnite grid deﬁned by a set of forbidden patterns. In this thesis,
we study two-dimensional subshifts of ﬁnite type (2D SFTs), where the underlying grid is Z2 and the set
of forbidden patterns is ﬁnite. We are mainly interested in the interplay between the computational power
of 2D SFTs and their geometry, examined through the concept of expansive subdynamics. 2D SFTs with
expansive directions form an interesting and natural class of subshifts that lie between dimensions 1 and 2.
An SFT that has only one non-expansive direction is called extremely expansive. We prove that in many
aspects, extremely expansive 2D SFTs display the totality of behaviours of general 2D SFTs.

For example, we construct an aperiodic extremely expansive 2D SFT and we prove that the emptiness
problem is undecidable even when restricted to the class of extremely expansive 2D SFTs. We also prove
that every Medvedev class contains an extremely expansive 2D SFT and we provide a characterization of
the sets of directions that can be the set of non-expansive directions of a 2D SFT. Finally, we prove that
for every computable sequence of 2D SFTs with an expansive direction, there exists a universal object that
simulates all of the elements of the sequence. We use the so called hierarchical, self-simulating or ﬁxed-point
method for constructing 2D SFTs which has been previously used by G´acs, Durand, Romashchenko and
Shen.

i

Contents

1 Historical overview

2 Preliminaries

2.1 Basic deﬁnitions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.2 Computation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.2.1 Turing machines
2.2.2 Computability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.2.3 Degrees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.3 Symbolic dynamics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.4 Cellular automata . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.4.1 Expansiveness

3 Simulation

3.1 Simulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.2 Nested simulations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.3 Expansiveness and simulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.4 Explicit simulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4 The programming language

4.1 Deﬁnitions and basic permutations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.2 Conventions about deﬁning IPPA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

5 The universal simulator

5.1
Imposing a periodic structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5.2 Simulating TM with IPPA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5.3 Computing the simulated permutation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5.4 Shifting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5.5 Simulating any ﬁxed rule
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Satisfying the inequalities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

5.5.1

6 Inﬁnite hierarchies

6.2.1

6.1 Son-father checks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6.2 Self-simulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Satisfying the inequalities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6.3 Hierarchical simulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Satisfying the inequalities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6.4 Universality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6.4.1
Satisfying the inequalities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6.4.2 Domino problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6.4.3
Intrinsic universality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

6.3.1

1

6
6
8
8
9
10
10
11
13

15
15
18
21
24

26
26
28

30
30
32
34
38
39
40

42
42
43
45
46
48
51
52
53
53

ii

6.5 Synchronizing computation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Satisfying the inequalities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

6.5.1
6.5.2 Realizing computational degrees

7 Expansive directions

7.1 Directive encoding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7.2 Computing directions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7.3 Realization of sets of non-expansive directions . . . . . . . . . . . . . . . . . . . . . . . . . . .
7.3.1
Satisfying the inequalities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7.3.2 Realization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

54
56
56

58
58
60
60
62
63

iii

Chapter 1

Historical overview

This thesis is about two-dimensional subshifts of ﬁnite type (2D SFTs), and more speciﬁcally, the behaviour
of 2D SFTs with respect to a dynamical-geometrical notion called expansive subdynamics.

The mathematical study of 2D SFTs began with the paper of Wang [40]. A Wang tile set consists of
a ﬁnite number of unit squares with coloured edges, which are called tiles. A valid tiling is a way to ﬁll
the entire plane with tiles such that the squares are edge-to-edge and such that the colors of abutting edges
are the same. Wang asked the following question about Wang tile sets, which is called the tiling problem:
Does there exist an algorithm that takes as input an arbitrary Wang tile set and decides whether it admits
a valid tiling? He conjectured that the answer to this question is positive and proved that the problem is
strongly correlated to the problem of the existence of an aperiodic tile set, that is a tile set that admits
some valid tiling but no periodic valid tiling.

However, Berger [4] proved that this is not the case.

In fact, he proved that the tiling problem is
undecidable. In addition, his proof contained an explicit construction of an aperiodic tile set. Later, several
authors have given alternative constructions of aperiodic tile sets and proofs of the undecidability of the
tiling problem [37, 25, 26].
There is an alternative way of looking at and talking about the same problem. Let A be a ﬁnite set,
called the alphabet. A (two-dimensional, or 2D) conﬁguration is a map c : Z2 → A. The set of all
conﬁgurations AZ2
is called the full shift. A pattern is a map p : D → A, where D ⊆ Z2 is a ﬁnite set.
Let F be a set of forbidden patterns. The corresponding (2D) subshift XF is the set of all conﬁgurations
that avoid the patterns of F: for all ﬁnite D ⊆ Z2 and c ∈ XF , c|D /∈ F. If X = XF for some ﬁnite set of
forbidden patterns, then it is called a 2D subshift of ﬁnite type (SFT). In this thesis, we will only talk about
2D subshifts and SFTs, so that we will usually omit the dimension, except in statements of theorems.

It is not diﬃcult to see that the set of valid tilings of a Wang tile set is an SFT. In addition, for every
SFT, we can construct a Wang tile set whose set of valid tilings is, in some sense, equivalent to the given
SFT. The tiling problem can thus be rephrased as the emptiness problem for SFTs: Given a ﬁnite set of
forbidden patterns, can we algorithmically decide whether XF (cid:54)= ∅? The undecidability of the tiling problem
then is then immediately translated to the undecidability of the emptiness problem of SFTs.

Wang tiles and forbidden patterns give a geometrical deﬁnition of SFTs, but there also exists an equivalent
dynamical deﬁnition. First of all, the full shift can be endowed with the product topology of the discrete
topology on A. This gives rise to a compact, metrizable topological space. The horizontal and vertical
shifts, which consist in moving a conﬁguration one step to the left and up, respectively, are continuous with
respect to this topology and obviously commute. This deﬁnes a Z2 action over the full shift and we can
study it using the usual tools of topological dynamics.

For example, one can prove that subshifts are exactly the closed, shift-invariant subsets of the full shift,
or, equivalently the subsystems of the full shift. SFTs correspond to the chain-mixing subsystems of the
full shift. More importantly, for the purposes of this thesis, we can study 2D SFTs from the point of view
of their expansive subdynamics. This notion was deﬁned by Boyle and Lind [6] as a tool for studying
multidimensional dynamical systems by looking at the (lower-dimensional) actions induced by the subgroups

1

of the original action. Intuitively, this is the same as when we look at the lower-dimensional projections of
a surface in order to understand some of its properties.
The general deﬁnition of expansive subdynamics and the main results of [6] fall out of the scope of this
thesis. However, for 2D subshifts there exists an equivalent, natural geometrical deﬁnition. Let X ⊆ AZ2
be
a subshift, l ∈ P := R (cid:116) {∞} a slope and l ⊂ R2 the corresponding line that passes through the origin. We
say that l is an expansive direction of X if there exists a ﬁnite shape V ⊂ R2 such that, for all x, y ∈ X,

x|(l+V )∩Z2 = y|(l+V )∩Z2 ⇒ x = y .

In other words, there exists a ﬁxed width b > 0 such that every conﬁguration of X is uniquely deﬁned by
its restriction to the strip of slope l and width b that goes through the origin (in fact, by shift invariance,
by any strip). Geometrically, this means that in X the (2D) information of the conﬁguration is “packed”
inside the one-dimensional strip of slope l. In some sense, even though X is a two-dimensional object, it
is determined by a one-dimensional strip, so that subshifts with directions of expansiveness are somewhere
between dimensions 1 and 2.
A direction that is not expansive is called non-expansive. Let N (X) be the set of non-expansive
directions of X. Boyle and Lind proved that N (X) is closed in the one-point compactiﬁcation topology
of P and that N (X) (cid:54)= ∅ if and only if X is inﬁnite. Since ﬁnite subshifts are rather trivial, the most
restricted non-trivial case with respect to non-expansive directions is the case when X has a unique direction
of non-expansiveness. We call such a subshift extremely expansive. Extremely expansive SFTs form
the main object of interest in this thesis. We prove that in many aspects, extremely expansive SFTs are
computationally as powerful as general SFTs.

Before stating the results, we ﬁnd it useful to talk about another class of SFTs with many directions of non-
expansiveness, namely those that arise from deterministic tile set. A tile set is called NW-deterministic
(the initials stand for North and West) if every tile is uniquely determined by the colors of its top and left
sides[23]. Similarly, we can deﬁne SW, SE and NE deterministic tile sets (S and E stand for South and East,
respectively). A tile set is called 4-way deterministic if it is SW,NW,SE and NE deterministic [22]. One
can easily see that for the SFT associated to a 4-way deterministic tile set and for every direction l that is not
the vertical or the horizontal one (slopes ∞ and 0, respectively), l is an expansive direction. Guillon, Kari and
Zinoviadis recently proved [13] that the vertical and the horizontal direction must indeed be non-expansive
unless the associated SFT is in some sense trivial, namely vertically or horizontally periodic.

We can now start stating the results of the thesis. The ﬁrst result concerns the existence of an aperiodic
extremely expansive SFT. As mentioned earlier, for the unrestricted case, there exist various constructions
of aperiodic SFTs. Kari and Papasoglou [22] have constructed an aperiodic 4-way deterministic tile set.
According to what was said in the previous paragraph, the SFT associated to this tile set has exactly two
non-expansive directions, the vertical and the horizontal one. We prove that

Theorem 1. There exists an aperiodic extremely expansive 2D SFT.

Of course, our construction does not use a 4-way deterministic tile set. It might seem that this result is
strictly better than the one using 4-way deterministic tile sets, since we have one non-expansive direction
less. However, there exists a small nuance here: 4-way deterministic tile sets give rise to SFTs with so-called
bounded radii of expansiveness, while our construction does not have this property.
In addition, in
[13] it is also proved that every aperiodic SFT with bounded radii of expansiveness must have at least two
non-expansive directions. Therefore, the 4-way deterministic construction is also optimal, in the class of
SFTs with bounded radii of expansiveness, and it might be more precise to say that the two results are
incomparable.

As mentioned already, the existence of an aperiodic tile set was originally constructed in order to prove
that the tiling problem is undecidable. Kari [23] prove that the tiling problem for NW-deterministic tile sets
is undecidable. In addition, Lukkarila [31] used the 4-way deterministic tile set of Kari and Papazoglou in
order to prove that the tiling problem is undecidable for 4-way deterministic tile sets as well. As the reader
has probably guessed already, we prove that

2

Theorem 2. The emptiness problem of extremely expansive 2D SFTs is undecidable. More precisely, the
emptiness problem is undecidable for 2D SFTs such that the vertical direction is the only non-expansive
direction.

One should understand the previous statement in the following sense: even if one is given an SFT X
(as a ﬁnite set of forbidden patterns) and is given the additional information that X is either empty or
extremely-expansive (and in this case N (X) = {∞}), even then it is not possible to decide whether X = ∅.
In other words, it is not possible to algorithmically separate the sets of forbidden patterns that deﬁne empty
SFTs from those that deﬁne extremely expansive non-empty SFTs.

The third result can be considered a stronger version of the undecidability of the emptiness problem. We
prove that there exist extremely expansive SFT whose conﬁgurations are computationally as complicated as
possible.
In order to describe this result, we need to introduce some classical notions of computation theory. For
the purposes of this introduction, a computable function will mean a function f : AN → AN
such that
there exists a Turing Machine that outputs f (c) when originally its reading tape contains c (i.e., it outputs
f (c) with oracle c). Using an eﬀective enumeration of Z2, it is possible to talk about computable functions
with domain or range AZ2

, and in general AM
is reducible to c ∈ AM(cid:48)
if there exists a computable function f such that f (c) = d.
This means that c is computationally at least as complicated as d, since it is possible to obtain d using c
is called Medvedev reducible to X ⊆ AM(cid:48)
and a computable function. A subset Y ⊆ AM
if every point of
Y is reducible to some point of X. Intuitively, we can compute any point of Y with the help of a suitable
point of X and a computable function. The relation of Medvedev reducibility is a pre-order on subsets.

, where M is any eﬀectively enumerable set.

We say that d ∈ AM

A set X ⊆ AM

Two sets are called Medvedev equivalent if they are Medvedev reducible to each other. This is an
equivalence relation, whose equivalence classes are called Medvedev degrees. There exists a partial order
on the set of Medvedev degrees given by the natural lift of the Medvedev reducibility pre-order. Computable
sets are the least element of this order and, in a certain sense, the higher a set is in this hierarchy, the more
diﬃcult it is to compute a point of this set relative to the sets that lie lower in the hierarchy. The survey
[16] contains a thorough study of Medvedev degrees.

is called eﬀectively closed if its complement is semi-decidable. Eﬀectively closed sets
form the so-called Π1
0 sets and they play a very important role in computation theory. It is easy to see that
SFTs are eﬀectively closed, even though there exist many eﬀectively closed sets (and even eﬀectively closed
subshifts) that are not SFTs. However, Simpson [38] proved that every eﬀective Medvedev degree (i.e., the
Medvedev degree of an eﬀectively closed set) contains a 2D SFT. Therefore, in some sense, not only is the
emptiness problem undecidable for 2D SFTs, but their points can be as diﬃcult to compute as possible. We
improve this result to the extremely expansive case:

Theorem 3. Every eﬀective Medvedev degree contains an extremely expansive 2D SFT. In other words,
for every eﬀectively closed set Z ⊆ AM
, there exists an extremely expansive 2D SFT Y that is Medvedev
equivalent to Z.

In fact, we prove something stronger, giving a complete characterization of the so-called Turing degrees

of Y relative to those of Z, but it is not necessary to go into these details here.

The next result is of a dynamical ﬂavour and it does not concern extremely expansive SFTs, but sequences
of SFTs with a common rational direction of expansiveness. It also uses the notion of simulation, which is
of central importance in the proofs of the previous results and, in general, for the whole thesis, even though
it wasn’t mentioned until now.
with parameters (S, T ) if there exists a B-
colouring of the S × T blocks of X with the following property: Every conﬁguration of X can be partitioned
in a unique way into S × T rectangles such that when we color these rectangles with the B-colouring we
obtain a conﬁguration of Y . Inversely, every conﬁguration of Y can be obtained in this way.

We say that subshift X ⊆ AZ2

simulates subshift Y ⊆ BZ2

This is weaker than the notion of simulation that we actually use, but it follows from it, is enough to

describe the result and is much easier to describe. It corresponds to the deﬁnitions in [8].

3

It was proved in [28] that for every computable sequence of SFTs, there exists an SFT that simulates all
of them. This is a surprising and really strong result. We prove a version of it in the case where all the SFTs
of the sequence have a common, rational expansive direction (which without loss of generality we assume to
be the horizontal one):
Theorem 4. Let X0, X1, . . . be a computable sequence of 2D SFTs such that 0 ∈ N (Xi), for all i ∈ N.
Then, there exists a 2D SFT X such that X simulates Xi for all i ∈ N and 0 ∈ N (X).

We note that there cannot exist a 2D SFT with an expansive direction that simulates all 2D SFTs
with the same expansive direction, because this would imply the decidability of the emptiness problem for
extremely expansive SFTs, according to an argument of Hochman [17].

The ﬁnal result of the thesis answers a natural question which arises immediately after the construction
of an extremely expansive SFT. As stated already, the unique non-expansive direction of the SFT that we
construct is the vertical one. Which other directions can be the unique direction of non-expansiveness for
2D SFTs? Obviously, we can achieve any rational direction by rotating with elements of SL2(Z), but can
we do more? More generally, what are the sets of directions that can be the set of non-expansive directions
of a 2D SFT?

Hochman [19] proved that for general 2D subshifts (not necessarily of ﬁnite type, or even eﬀective), any
closed set of directions can be the set of non-expansive directions, while any direction can be the unique
direction of non-expansiveness. Recall that Boyle and Lind proved that the sets of non-expansive directions
must be closed, so it turns out that in the case of general subshifts this necessary topological condition is
also suﬃcient.

In the case of SFTs, there is an additional necessary condition, namely that the set of non-expansive
directions be eﬀectively closed, which is equivalent to saying that its complement is the union of an
increasing, computable sequence of open intervals. It turns out that this condition is necessary and suﬃcient
for 2D SFTs:
Theorem 5. A set of directions N is the set of non-expansive directions of a 2D SFT if and only if it is
eﬀectively closed. More precisely, a direction l is the unique direction of non-expansiveness of a 2D SFT if
and only if it is computable.

This answers Question 11.2 in Boyle’s Open Problems for Symbolic Dynamics [5].
Using our methods, we could easily prove Theorems 1-3 for SFTs whose unique direction of non-
expansiveness is l, where l is any computable direction. This is a stronger version of the results, which
we do not prove for lack of space. In any case, once one has mastered our method, it is possible to prove
various new results and variants of already proved ones. Since this method is as important (if not more) as
some of our results, it is probably worth saying some words about its history, too.

It is the so-called ﬁxed-point tile or self-simulating method for constructing 2D SFTs. It was ﬁrstly
described by Kurdyumov [27] in order to give a counterexample to the Positive Rates conjecture, even though
only a sketch of a proof was included in this paper. It was G´acs [9] who elaborated Kurdyumov’s idea into
a full proof of the positive rates conjecture and formalized the notion of a hierarchy of simulating SFTs (he
talks about 1D cellular automata, but this does not make a big diﬀerence). Later, he signiﬁcantly improved
his construction and the result in a notoriously lengthy and diﬃcult paper [10]. Gray’s reader guide to that
paper [12] and the description therein of self-simulation and the problems one encounters when trying to
construct a self-simulating SFT are also a very useful exposition of the ideas of G´acs and Kurdymov. It was
not until the work of Durand, Romashchenko and Shen [8] that the method became accessible to a broader
mathematical audience. They work in the framework of 2D SFTs, which allows for a more clear, geometrical
description of the basic ideas.

G´acs’ construction did not have any direction of expansiveness, because it was a non-reversible cellular
automaton. Nonetheless, it had the horizontal direction as a direction of “semi-expansiveness”. On the other
hand, the construction of Durand, Romashchenko and Shen did not have neither directions of expansiveness
neither directions of “semi-expansiveness”. A large part of this thesis consists in making their construction
expansive in the horizontal direction. We need to introduce some tricks in order to do this, but once we

4

achieve it, then self-simulation and a previous result of Hochman immediately give an extremely expansive
aperiodic SFT. Something similar was also done in [14], but the construction of that paper was signiﬁcantly
easier because we dealt with non-reversible cellular automata, so that we only needed a direction of “semi-
expansiveness”. Our current construction can be seen as an improvement of the construction of that paper,
and using it we can easily retrieve its main result, which was a characterization of the numbers that can
appear as the topological entropy of a (not necessarily reversible) CA.

One thing that all the constructions have in common, including ours, is that they are complicated and
rather diﬃcult to explain (for the writer) and understand (for the reader). This is unavoidable, in some
degree, and the author’s personal opinion is that there does not exist a “perfect” way to write them. Either
the exposition is very formal, covering all details and deﬁning every little thing, which is the road that we
have chosen, or the construction is informal, in which case it is not clear what exactly the constructed SFT
is, over which alphabet it is deﬁned etc., which is the choice made by Durand, Romashchenko and Shen.
Taking the middle road, as was more or less done by G´acs, does not help very much, either.

Our opinion is that the best thing is to be familiar with all the constructions and use them accordingly.
On the one hand, the constructions of Durand, Romashchenko and Shen are convincing for someone already
familiar with the technique and they allow to explain a new idea concisely and eﬃciently, as was recently
done in [7], while on the other hand our more formal presentation can be used to acquire mastery with
the technique by dealing with all the unexpected little problems that arise during the construction and to
convince those people who want to understand all the details.

Let us now describe the structure of the thesis:
In Chapter 2, we give the basic deﬁnition that we will need throughout the paper. In Chapter 3, we
deﬁne the precise notion of simulation that we will use and give some of its properties. We believe that some
of the results of this chapter are of independent interest. In Chapter 4, we describe a pseudo-programming
language that will be used to describe 2D SFTs in a concise way. In Chapter 5, we construct a family of
SFT (which depend on the parameters S, T ) with 0 as a direction of expansiveness which are, in some sense,
universal: They can simulate every SFT with 0 as a direction of expansiveness, provided that its alphabet
size is small compared to S, T and it can be computed fast compared to S, T . This family of SFTs is of great
importance for all subsequent constructions. This is the part of the thesis where we modify the construction
of Durand, Romashchenko and Shen so as to make it reversible. In Chapter 6, we prove Theorems 1 - 4. The
constructions and the proofs all follow the same pattern, but we give as many details as possible for all of
them for reasons of completeness. Finally, in Chapter 7, we prove Theorem 5. This proof is a modiﬁcation
of the proof of the result in [19]. We try to explain what are the diﬀerences between that construction and
ours and why the changes that we make are necessary.

Finally, let us mention that all of the aforementioned results have been obtain in collaboration with Pierre
Guillon during various visits by him in Turku as well as of the author in Marseille. Currently, a series of
joint papers is under construction that will contain even more applications of our method. Theorem 1 has
also appeared in [41], even though because of lack of space, most of the details of the construction do not
appear in that paper.

5

Chapter 2

Preliminaries

real numbers, respectively, by(cid:74)i, j(cid:74) and(cid:74)i, j(cid:75) the integer intervals {i, . . . , j − 1} and {i, . . . , j}, respectively,

2.1 Basic deﬁnitions
We will denote by Z, N, N1, Q and R the sets of integers, non-negative integers, positive integers, rational and
while [ε, δ] will denote an interval of real numbers. If f, g : N → N, then we will use the classical f ∈ O(g)
notation to denote that f (n) ≤ cg(n), for some constant c and all n ∈ N.
If f : X (cid:57) Y is a partial function, then its domain D(f ) ⊆ X is the set of elements of X whose image
through f is deﬁned. Two partial functions are equal when they have the same domain and they agree on
their common domain. If f : X (cid:57) Y and g : Y (cid:57) Z are partial functions, then g ◦ f : X (cid:57) Z is the partial
function deﬁned in the usual way (i.e., g(f (w)) does not exist if either w /∈ D(f ) or f (w) /∈ D(g)). A partial
permutation is a bijection over its domain onto its range, i.e., an injective partial map. In the following,
when deﬁning a partial function, it will be implicit that any non-treated argument has an undeﬁned image,
and that saying that two partial functions are equal means in particular that their domains are the same.
If Z ⊂ X and f : X (cid:57) Y , we may abusively consider f|Z as a partial map from X to Y whose domain is
Z ∩ D(f ).
(cid:80)
For m ≥ n, T := (Ti)0≤i<m and t := (ti)0≤i<n such that for all i, 0 ≤ ti < Ti, we note tT :=
0≤i<n ti
0≤j≤i Tj the numeric value represented by the adic representation t in base T. In general, Ti
and ti can belong in R, not necessarily in N. By convention, if t has length 0, then tT := 0. Similarly, for a
sequence T := (Ti)i∈N, we note tT := tT(cid:74)0,n(cid:74) . For a sequence t := (ti)i∈N, we note tT := lim t(cid:74)0,n(cid:74)T(cid:74)0,n(cid:74) , when
(cid:83)
n∈N An denotes the set of ﬁnite words over A, and A∗∗ :=(cid:83)
An alphabet is any ﬁnite set, whose elements are often called symbols. If A is an alphabet, A∗ :=
(Notice that the notation A∗∗ is a little ambiguous as it could also stand for the set(cid:83)
m∈N (A∗)m the set of ﬁnite tuples of words.
m∈N (Am)∗. Obviously,
the two interpretations are isomorphic, but they are diﬀerent objects.) The empty word is denoted by
 ∈ A∗.
If u ∈ (A∗)m, we write
u = (u0, . . . , um−1), and |u| := (|u0| , . . . ,|um−1|). For every i ∈ N, we deﬁne the projection πi as a partial
function πi : A∗∗ (cid:57) A∗: πi(u) = ui if u ∈ (A∗)m with m ≥ i (and πi(u) is undeﬁned otherwise). A ﬁeld
is a projection πi together with a label Field, written in type-writer form. The notion of ﬁelds is simply a
convenient way of talking about tuples of words. The names of the ﬁelds will be chosen so as to reﬂect the
role that the ﬁeld plays in the construction.
m∈N Nm the set of integer tuples of any dimension, where m is the dimension of the
tuple k := (k0, . . . , km−1) ∈ Nm. Let Ak := Ak0 × . . . × Akm−1; any subalphabet of Ak is said to have
constant lengths.
We will mainly use the special alphabets Fn := {0, . . . , n − 1}, for n ∈ {2, 3, 4, 5}. Of course, instead of
F5 we could use any alphabet with n letters. However, since some letters will have a ﬁxed role throughout

If w ∈ An, we write w = w0 ··· wn−1, and call |w| := n the length of w.

(cid:81)

this limit exists.

We note N∗ :=(cid:83)

6

The non-negative integers can be easily embedded into F∗

the thesis, it is better to ﬁx the notation and get used to these roles.
2 thanks to the injection n (cid:55)→ n which gives
the shortest binary representation of n ≥ 1. (cid:107)n(cid:107) := |n| = (cid:100)log2 n(cid:101) + 1 is the length of n. By deﬁnition,
0 :=  and (cid:107)(cid:107) := 0. Inversely, if u ∈ F∗
2, then u is the number represented by u in the binary representation
system: for all u ∈ F∗
2, u is the suﬃx of u that is obtained after removing the initial 0s. (The “lower bar” is
applied before the “top bar”.)
2. For instance, we will say that {−1, +1} is F2 by
identifying −1 with 0 and +1 with 1. Finite alphabets of bigger cardinality can be embedded into Fk
2, for
some suitable k.
encode tuples into words. If u ∈ (F∗

Now, in the perspective of computing functions with many arguments, we are going to use symbol 2 to

We will also need to embed some ﬁnite sets in F∗

5)m for some m ∈ N, then χ (u) is deﬁned as the concatenation
χ (u0) χ (u1) . . . . . . χ (um−1) ∈ F∗
3,

N
5 .

F5 := 000, 1

F5 := 001, 2

where χ (v) := 2vF5, and v (cid:55)→ vF5 is some monoid injection (i.e., code) from F∗
use the code deﬁned by 0
of the encoding of word tuples depends only on |u|. We can also deﬁne χ (u) := χ (u0) χ (u1) . . . ∈ F
u ∈ F

2. In this paper, we will
F5 := 100. Note that the structure
N
3 for
Let us now prove a basic fact about χ (·). Namely, for every k ∈ N∗, there exists an easily computable
5 and the positions of the encodings of the

function that gives the positions of the 2s in encodings of Fk
components of a letter.

Fact 6. Let M ∈ N and k ∈ NM . For all 0 ≤ i < M , let us deﬁne lk,i := 3(cid:80)i−1

F5 := 010, 3

F5 := 011, 4

j=0 kj + i. Then, for all

5 to F∗

u ∈ Fk
5 :

1. (cid:107)χ (u)(cid:107) = lk,M ,
2. χ (u)(cid:74)lk,i,lk,i+1(cid:74) = 2πi(u)

F5

These statements correspond to what Durand, Romashchenko and Shen refer to as “the TM know the

place where such and such information is held in the encoding”.

Symbol 3 will be used in Subsection 2.2.1 to encode the start and the end of the tape of a Turing machine.
Symbol 4 will be used in order to construct alphabets with constant lengths.
In the computation,
we indeed want words of various lengths to be able to represent the same objects. For this, we deﬁne
(cid:104)u(cid:105)l
:= 4l−|u|u, for every l ∈ N and u ∈ F∗
4 with |u| ≤ l ((cid:104)u(cid:105)l is undeﬁned otherwise). For instance,
(cid:104)n(cid:105)(cid:107)n(cid:107) = n for any integer n ∈ N, and the encoding (cid:104)(cid:105)l = 4l of the empty word is a sequence of 4s. It is
clear that the partial function

4, in such a way that (cid:105)(cid:104)u(cid:105)l(cid:104) = u for any l ≥ |u| and u ∈ F∗

is injective (over its domain) and surjective; let us write (cid:105)w(cid:104) ∈ F∗
4 of a word
w ∈ 4∗F∗
4. These two maps can be adapted to vectors
in the obvious way: (cid:104)u(cid:105)k := ((cid:104)u0(cid:105)k0
) for any k ∈ Nm, m ∈ N and u := (u0, . . . , um−1) ∈
4 . Note that this is deﬁned if and only if k ≥ |u|. Similarly, (cid:105)w(cid:104) := ((cid:105)w0(cid:104), . . . ,(cid:105)wm−1(cid:104)) for any w :=
F∗m
(w0, . . . , wm−1) ∈ (4∗F∗
permutation that preserves the number of ﬁelds (i.e., α(F∗
an equivalent permutation that also preserves the lengths:

is a partial
4)l for all l ∈ N), we can transform it into

Recall that a partial permutation is simply an injective partial map.

4 for the longest suﬃx in F∗

, . . . ,(cid:104)um−1(cid:105)km−1

4)l ⊆ (F∗

If α : F∗∗

(cid:57) F∗∗

4)m.

4

4

N × F∗
4
(l, u)

(cid:57) 4∗F∗
(cid:55)→ (cid:104)u(cid:105)l

4

Remark 7.

(cid:104)α(cid:105) :

(4∗F∗

4)∗
4)∗ (cid:57) (4∗F∗
w (cid:55)→ (cid:104)α((cid:105)w(cid:104))(cid:105)|w| .

7

• For any k ∈ N∗, (cid:104)α(cid:105) is also a partial permutation.
• The restriction of α to any subalphabet is implemented by that of (cid:104)α(cid:105) to large enough words:

∀u ∈ F∗∗

4 ,∀k ≥ max{|u| ,|α(u)|},(cid:104)α(cid:105)((cid:104)u(cid:105)k) = (cid:104)α(u)(cid:105)k .

Proof. For the ﬁrst part, assume that (cid:104)α(cid:105)(w) = (cid:104)α(cid:105)(w(cid:48)). This implies that |w| = |w(cid:48)|.
In addition,
α((cid:105)w(cid:104)) = (cid:105)(cid:104)α(cid:105)(w)(cid:104) = (cid:105)(cid:104)α(cid:105)(w(cid:48))(cid:104) = α((cid:105)w(cid:48)(cid:104)). Since α is a partial permutation, this implies that (cid:105)w(cid:104) = (cid:105)w(cid:48)(cid:104).
Therefore, w = (cid:104)(cid:105)w(cid:104)(cid:105)|w| = (cid:104)(cid:105)w(cid:48)(cid:104)(cid:105)|w(cid:48)| = w(cid:48).
4 and k ≥ max{|u| ,|α(u)|}. Then, (cid:104)u(cid:105)k and (cid:104)α(u)(cid:105)k exist and |(cid:104)u(cid:105)k| =
|(cid:104)α(u(cid:105)k)| = |k|. Therefore, (cid:104)α(cid:105)((cid:104)u(cid:105)k) = (cid:104)α((cid:105)(cid:104)u(cid:105)k(cid:104))(cid:105)k = (cid:104)α(u)(cid:105)k.

For the second part, let u ∈ F∗∗

In the rest of the paper, we will often implicitly use Remark 7 both to construct partial permutations that
preserve the lengths of the ﬁelds, as well as to state and prove things about them. It allows us to describe
the behaviour of a partial permutation α, and then translate this result into the behaviour of (cid:104)α(cid:105), provided
that the lengths of the ﬁelds are suﬃciently large, thus omitting the (confusing) (cid:105)·(cid:104) and (cid:104)·(cid:105) symbols.

Let i1, . . . , il be a set of ﬁelds, and w ∈ F∗
:= { u ∈ F∗∗

S w

i1,...,il

5. Then,
5 |(cid:105)πik (u)(cid:104) = w, for k = 1, . . . , l}
(cid:111)

(cid:12)(cid:12)(cid:12)(cid:105)πik (u)(cid:104) = n, for k = 1, . . . , l

5

(cid:110)

S n

i1,...,il

:=

u ∈ F∗∗

is the set of all symbols that have ﬁelds i1, . . . , il equal to w (up to the application of (cid:105)·(cid:104)). If n ∈ N, let

be the set of all symbols who have the values n (in binary form) in the ﬁelds i1, . . . , il.

2.2 Computation

2.2.1 Turing machines

The reader is assumed to be familiar with classical concepts in computability theory. We just ﬁx some termi-
nology and give a variant of a deﬁnition of Turing machines, imposing some additional technical restrictions
which, however, do not restrict the computational power.

A Turing machine (TM) is a partial (“global”) map M from F
2 is
a ﬁnite set of states containing the initial state 0 and the accepting state , and depending on a partial
transition map δM : F4 × Q \ {} (cid:57) F4 × Q × {−1, +1} such that:

4 × Q × Z into itself, where Q ⊂ F∗
Z

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) (z, q, j)

(z(cid:48), q(cid:48), j(cid:48))

M(z, q, j) =

if q = 
otherwise, where (z(cid:48)
i = zi,∀i (cid:54)= j ,
and z(cid:48)

j, q(cid:48), j(cid:48) − j) = δM(zj, q)

for any (z, q, j) ∈ F
being the tape content, the second the (head) internal state, the third the head position.

4 ×Q×Z, which will sometimes be called a machine conﬁguration, the ﬁrst component
Z

The model of TM that we use satisﬁes the following assumptions, which, as can be easily seen, do not

restrict the computational power of TM.

• There is only one tape, from which the TM reads the input and on which it writes the output.
• The internal states are words of F∗
• All machines have the same initial and accepting states 0 and , respectively.
• The global map is still deﬁned after having accepted, and is then equal to the identity.
• There is no precise rejecting state (instead, we use undeﬁned transitions over non-accepting states).

2 (this is just a semantic restriction).

8

• In every accepting transition, the head disappears and moves to the right.

In other words, every
accepting transition is of the form δ(q, a) = (a(cid:48), , +1). This is a technical assumption which simpliﬁes
the construction of an IPPA that simulates M in Section 5.2.

We denote by Mt the t’th power of the (global) map M. If Mt(∞3.χ (u) 3∞, 0, 0) = (∞3.χ (u(cid:48)) 3∞, , j),
5 , and outputs u(cid:48) ∈ F∗∗
for some t ∈ N, j ∈ Z, then we say that M halts over (or accepts) input u ∈ F∗∗
5 ,
and we deﬁne fM(u) := u(cid:48) and tM(u) as the minimal t for which this holds (if this never holds, or if
∞3.χ (u) 3∞ is rejected, then tM(u) is undeﬁned).

is no more modiﬁed.

Notice that fM(u) is well-deﬁned, since when the accepting state  appears, the machine conﬁguration
We say that M computes the partial map fM : F∗∗

5 , with time complexity

(cid:57) F∗∗

5

tM : N → N

n (cid:55)→ max|χ(u)|=n tM(u) ,

where, by deﬁnition, the max is taken only over accepted inputs. tM is well-deﬁned since there are only

ﬁnitely many accepted inputs of each length.

N
2

N
2

4

5

5

5

(cid:57) F

Z
2

(cid:57) F

Z
2

(cid:57) F

(cid:57) F∗∗

A partial function Φ : F

2.2.2 Computability
is called computable if there exists a TM M such that f = fM. Recall
A partial function f : F∗∗
that integers (and ﬁnite sets) can be identiﬁed to words, hence allowing us to talk about computable maps
between Cartesian products involving N and ﬁnite sets. We also say that a set X ⊆ F∗∗
is computable
4 → F2 is computable, and that it is computably enumerable if it is
if its characteristic function ιX : F∗∗
the domain of a computable function. We will say that a partial function f : X (cid:57) F∗∗
is
computable if both X and the extension of f to F∗∗
5 (by not deﬁning images outside of X) are computable.
2 is called computable if there exists a TM M such that x ∈ D(Φ) if
N
and only if for all n ∈ N, there exists m ∈ N such that fM(x(cid:74)0,m(cid:74), n) is deﬁned, in which case it is equal
Z
to Φ(x)n. Finally, by parametrizing Z with N, we can talk about computable functions Φ : F
2 . An
2 is computable if there exists a TM M such that x ∈ D(Φ) if and
Z
equivalent deﬁnition is that Φ : F
only if for all n ∈ N, there exists m ∈ N(cid:48) such that fM(x(cid:74)−m,m(cid:74), n) is deﬁned, in which case it is equal to
Φ(x)n.
N
2 , we can also talk about computable functions of real numbers. A
partial function Ψ : R (cid:57) R is computable if there exists a computable function f : R × N → Q with the
following property: |Ψ(x) − f (x, n)| < 2−n. This is the classical deﬁnition of computability for real functions
and it says that we can compute better and better approximations of x.

Since R can be identiﬁed with F

5 , with X ⊂ F∗∗

ZM :=(cid:8) z ∈ F

(cid:12)(cid:12)∀t ∈ N,Mt(3∞.z, 0, 0) exists and is not in F

4 × {} × Z(cid:9)
sequences that can be encoded with words, in particular over ﬁnite alphabets: a subset X ⊂(cid:81)

be the set of one-sided binary sequences over which M runs for an inﬁnite amount of time. We say that
1) if χ (X) = ZM for some TM M, or equivalently if the set
a subset X ⊂ F
of words that do not preﬁx any sequence in it is computably enumerable. This can be extended to sets of
t∈N At, where
5, is eﬀectively closed if χ (X) = ZM for some program M (we encode
At is a ﬁnite subalphabet of F∗
every ﬁnite alphabet with Fk
M is called polynomial if tM ∈ O(P ), for some polynomial P . A partial function f is called polynomi-
ally computable if fM = f for some polynomial TM M. It is easy to see that the class of (polynomially)
computable functions with this version of TM corresponds to the classical one. Analogously, X is a polyno-
mially computable set if its characteristic function ιX is polynomially computable. We say that a function
(or sequence) f is polynomially checkable if it can be computed in time O(P (log f )), for some polynomial
P . The terminology comes from the fact that even though f might not be polynomially computable, its

2, for some suitable k which depends on t ∈ N).

N
2 is eﬀectively closed (or Π0

If M is a TM, let

Z

9

graph (i.e., the set of pairs element-image) is a polynomially computable set. For example f (n) = 22n
polynomially checkable sequence even though it is not polynomially computable.

is a

Instead of a universal TM, we use the following essentially equivalent:

Fact 8. There exists an injection that associates to each TM M a program pM ∈ F∗
by Qp the state set of the TM corresponding to program p, then

4 such that if we denote

• The language { pM|M is a TM} ⊆ F∗
• The characteristic function (p, q) (cid:55)→ ιQp (q) that checks whether q ∈ Qp is polynomially computable.
• The “universal” transition rule

4 is polynomially decidable.

δU : F4 × F∗

4 × F∗
4
(a, q, pM)

(cid:57) F4 × F∗
(cid:55)→ δM(a, q)

4 × {−1, +1}

is polynomially computable.

• In addition, |Qp| ≤ |p|. (We can assume that p contains a list of the states of Qp.)
We will use the following notations: If p is the program of a TM that computes a reversible function
f , then p−1 will denote the program of the inverse function f−1 (it will always be computable in our
constructions). Also, tp and Zp will be used to denote tMp and ZMp , where Mp is the TM that corresponds
to the program p.
The ﬁrst examples of polynomially computable functions, which will be most useful in the sequel, are the
encodings presented in Subsection 2.1. Clearly, (cid:104)·(cid:105)· and its (right) inverse (cid:105)·(cid:104) are polynomially computable.
5, for i ∈ N, are polynomially computable and so are the functions
Moreover, the projections πi : F∗∗
(k, i) → lk,i (as deﬁned in Fact 6) and χ (·).

5 → F∗

2.2.3 Degrees
In the following, M and M(cid:48) can stand for either N or Z.

them.

is Turing-reducible to c ∈ F

Two sets X, Y ∈ F
We say that d ∈ F

M
2 are computably homeomorphic if there exists a computable bijection between
M(cid:48)
2

M
2 if d = Φ(c), for some computable function Φ.
This yields a preorder over conﬁgurations, whose equivalence classes are called Turing degrees. If d is
Turing-reducible to c, then in a computational sense, c is more complicated than d. A cone over degree d is
the set of Turing degrees that are higher than d.

Moreover, we say that subset Y ⊂ F
M(cid:48)
2

M(cid:48)
if there is a
such that D(Φ) ⊇ X and Φ(X) ⊆ Y . This also yields a pre-order
2
computable partial function Φ : F
over sets, whose equivalence classes are called Medvedev degrees. Finally, we say that subset Y ⊂ F
is
Muˇcnik-reducible to subset X ⊂ F
M
2 if every point of X is Turing-reducible to some point of Y (but not
in a uniform way, as in Medvedev-reducibility). This again yields a pre-order over sets, whose equivalence
classes are called Muˇcnik degrees.

is Medvedev-reducible to subset X ⊂ F

M
2

(cid:57) F

M(cid:48)
2

M
2

Medvedev and Muˇcnik degrees of a set are an attempt to formalize the notion of how computationally
diﬃcult it is to compute a point of the set. Of course, computable homeomorphism implies having the same
Turing degrees, which implies Medvedev-equivalence, which in turns implies Muˇcnik-equivalence.

We do not get too much into details, but the notion holds in the large setting of eﬀective topological

spaces (see for instance [11]).

2.3 Symbolic dynamics
AZd
is the set of d-dimensional conﬁgurations, endowed with the product of the discrete topology, and
with the shift dynamical system σ, deﬁned as the action of Zd by (σi)i∈Zd , where σi(x)k := xi+k for any
conﬁguration x ∈ AZd

and any i, k ∈ Zd.

10

j = 1, 2. Inductively, we can deﬁne(cid:87)

A pattern over a (usually ﬁnite) support D ⊂ Zd is a map p ∈ AD.
Two patterns u1 : D1 → A and u2 : D2 → A are called disjoint if D1 and D2 are disjoint shapes of Zd.
If u1, u2 are disjoint, let u1 ∨ u2 be the pattern over shape D1 (cid:116) D2 deﬁned by (u1 ∨ u2)(i) = uj(i), if i ∈ Dj,
Let E, D ⊂ Z2 be two shapes, and u ∈ AD be a 2D pattern. We denote uE the restriction of u to D ∩ E
(this is a pattern with support D ∩ E).
, |(ci)i∈I denotes the (possibly inﬁnite) pattern
If I ⊆ Z and (ci)i∈I is a family of conﬁgurations of AZ
u : Z × I → A such that uZ×{i} = ci, for all i ∈ I. Here we implicitly identify patterns on horizontal strips
up to vertical translation. Formally, the domains of uZ×{i} and ci are not the same.
top of each other (in this order). If I = Z, then we obtain a conﬁguration in AZ2

If I =(cid:74)0, n(cid:74), then |(c0, . . . , cn−1) is the horizontal strip of width n obtained by putting c0, . . . , cn−1 on

1≤i≤k ui, when u1, . . . , uk are mutually disjoint pattens.

.

1. The S-bulking (or higher-power representation) of x is the

Let x ∈ AZd

and S := (S0, . . . , Sd−1) ∈ Nd

conﬁguration x[S] ∈ (AS0×...Sd−1 )
Zd

X is a subshift if and only if there exists a family of patterns F ⊂(cid:83)

A (d-dimensional) subshift is a closed set X ⊂ AZd

such that σi(X) = X for all i ∈ Zd. Equivalently,

D⊂finiteZd AD such that

such that for any i = (i0, . . . , id−1) ∈ Zd,
x[S]i := x(cid:74)i0S0,(i0+1)S0(cid:74)×...×(cid:74)id−1Sd−1,(id−1+1)Sd−1(cid:74).
x ∈ AZd(cid:12)(cid:12)(cid:12)∀i ∈ Zd,∀D ⊂ﬁnite Zd, σi(x)|D /∈ F(cid:111)
(cid:110)

.

X =

If F can be chosen ﬁnite, we say that X is a subshift of ﬁnite type (SFT).

If F can be chosen computably enumerable, then X is called an eﬀective subshift.
A continuous map Φ from subshift X to subshift Y is a morphism if Φσ = σΦ. If it is surjective, then it
is a factor map, and Y is a factor of X (this deﬁnes a preorder); if it is bijective, then it is a conjugacy,
and X and Y are conjugate (this deﬁnes an equivalence relation). A subshift Y ⊆ AZd
is called soﬁc if it
is a factor of some SFT, which is then called a cover for Y .
is called periodic with period j (cid:54)= 0 ∈ Zd if σj(x) = x. A subshift X is called
5 and n ∈ N) also for conﬁg-
for all i ∈ Z. Finally, for

aperiodic if it does not contain any periodic conﬁgurations.
, we will say that c ∈ S w
urations. For example, if c ∈ (F∗∗
Z
5 )

A conﬁguration x ∈ AZd
Abusing notation, we use the notations S w

(where w ∈ F∗∗

if ci ∈ S w

and S n

i1,...,il

i1,...,il

i1,...,il

i1,...,il

N ∈ N and n ∈(cid:74)0, N(cid:74), let

P n,N

i1,...,il

:= {c ∈ (F∗∗
Z
5 )

: (cid:105)πik (cj)(cid:104) = j + n mod N, for all j ∈ Z, 1 ≤ k ≤ l}

be the set of all conﬁgurations such that

∞
πik (c) = .∞(n . . . (N − 1)01 . . . (n − 1))

, for 1 ≤ k ≤ l,

where for all w, .∞w∞ denotes the conﬁguration c which satisﬁes that c(cid:74)j|w|,(j+1)|w|(cid:74) = w, for all j ∈ Z.
2.4 Cellular automata
A (1D) partial cellular automaton (PCA) is a partial (“global”) continuous function F : AZ (cid:57) AZ
whose
domain is an SFT, and such that F σ = σF . Equivalently by some extension of the so-called Curtis-Lyndon-
Hedlund theorem, there exist a neighbourhood V ⊂ﬁnite Z and a partial local rule f : AV (cid:57) A such that
, F (z) is deﬁned if and only if f (z|i+V ) is deﬁned for all i ∈ Z, in which case F (z)i := f (z|i+V ).
for all z ∈ AZ

If V ⊆(cid:74)−r, r(cid:75), then r is called a radius of the PCA. The radius of a PCA is not uniquely determined.

A PCA is called reversible (RPCA) if it is injective. In this case, it is known that there exists another
RPCA, denoted by F −1, such that F F −1 and F −1F are restrictions of the identity, and D(F −1) = F (AZ
)
(the argument for this is similar to the one in [15]). In particular, there exist so-called inverse radius and

11

t∈Z Ωt

) ∩ F −t(AZ

F := F t(AZ

F :=(cid:84)

For t ∈ N, the tth−order range of F is the (soﬁc) subshift Ωt

inverse local rule. If r is both a radius and an inverse radius for an RPCA F , we call it a bi-radius for F .
In the rest of the paper, we only consider RPCA with bi-radius 1. This is not a signiﬁcant restriction, since
these PCA and RPCA exhibit the whole range of computational and dynamical properties of general PCA
and RPCA.
) and its limit set
is the (eﬀective) subshift ΩF := Ω∞
F , containing all the conﬁgurations that are not ultimately
rejected (either in the past or the future). There is a canonical way to associate a 2D SFT OF to an RPCA
it consists of the inﬁnite space-time diagrams of the conﬁgurations that are not ultimately rejected.
F :
Formally, OF := {OF (x)| x ∈ ΩF}, where OF (x) := |(F t(x))t∈Z ∈ AZ2
for any x ∈ ΩF . One can see that
OF is conjugate to the Z2-action of (F, σ) over ΩF . Note nevertheless that the same SFT may correspond
to distinct RPCA (if the RPCA have diﬀerent transient phases, i.e., they reject some conﬁgurations after
diﬀerent amounts of steps).
A pattern w ∈ AD, with D ⊂ Z2, is locally valid for f if for any (i, t) ∈ D such that C := (i +

(cid:74)−1, 1(cid:75)) × {t − 1} ⊂ D, we have p(i,t) = f (p|C ). Note that, in general, this notion depends on the local rule

and not only on the RPCA. By compactness, if there exist locally valid square patterns of arbitrarily large
height and width, then OF (cid:54)= ∅, i.e., there are conﬁgurations which are never rejected. If x ∈ F −t(AZ
),
then |(x, F (x), . . . , F t(x)) is a locally valid horizontal strip of height t + 1. The notion of a locally-valid
horizontal strip depends only on the RPCA and not on the local rule, i.e., it is a ”global“ notion.
For every m ∈ N, δ = (δ0, . . . , δm−1) ∈ {−1, 0, 1}m, we deﬁne the shift product σδ = σδ0 × . . .× σδm−1 . A
partial partition (cellular) automaton (PPA) is a PCA F = σδ◦α over some alphabet A = A0×. . .×Am−1,
where α is (the parallel synchronous application of) a partial permutation of A. −δi is called the direction
of ﬁeld i. The (counter-intuitive) “−” is due to the fact that the normal deﬁnition of σ shifts everything to
the left, while we are used to thinking of the positive direction as going to the right. So, if we want to have
a ﬁeld with speed +1, then we should apply σ−1 to it.

Every PPA is a RPCA with bi-radius 1 and conversely every RPCA is essentially a PPA (see for instance
[24, Proposition 53]). Note, however, that the inverse of a PPA is not, formally, exactly a PPA: the per-
mutation is performed after the shifts, in the form α−1 ◦ σ−δ. Nevertheless, it is conjugate, via α, to the
corresponding PPA.

5

Z

5 → F∗m

Z (cid:57) (F∗m
Z
5 )

and i ∈ Z), and α : F∗m

In order to deﬁne families of PPA that are somehow uniform, we consider the corresponding objects
acting on inﬁnite alphabets. A partial partition automaton with inﬁnite alphabet (IPPA) is a partial
, where m ∈ N, F = (σδ0 × . . . × σδm−1) ◦ α, the σδj are shifts over inﬁnite F∗
map F : (F∗m
5 )
(that is σ(y)i = yi+1 for any y ∈ (F∗
5
is a partial (inﬁnite) permutation.
5)
By restricting the domain and the co-domain of an IPPA to ﬁnite subsets of F∗m
5 , we obtain normal (ﬁnite)
PPA. In our constructions, the permutation α will always be length-preserving and the restriction will be
taken over an alphabet of the form Fk
5 .
If F : AZ → AZ
and G : BZ → BZ
are PCA, then we say that G is a factor of F if there exists a continuous
map H : AZ → BZ
such that GH = HF . If F and G are RPCA and F factors onto G, then it is easy to see
that OF factors onto OG through the map that sends Ox to OH(x), for all x ∈ AZ
. However, the notion of
factoring for RPCA is stronger, since it also takes into account the transient times of the RPCA, i.e., the
number of steps for which the image of an ultimately rejected conﬁguration is deﬁned before it is rejected
(which are not relevant in the corresponding 2D SFTs).

Let F0, . . . , Fn−1 be RPCA such that D(Fi)∩D(Fj) = ∅, for all i (cid:54)= j. Then,(cid:70)
i∈(cid:74)0,n(cid:74) D(Fi) and that agrees with Fi on D(Fi), for all i ∈(cid:74)0, n(cid:74). (cid:70)
with domain(cid:70)
i∈(cid:74)0,n(cid:74) Fi denotes the map
i∈(cid:74)0,n(cid:74) Fi is not always an
domains (which are SFTs). However, and this will always be the case in this paper, (cid:70)
RPCA, since there might be a conﬁguration that is not in any D(Fi) but that is locally everywhere in the
i∈(cid:74)0,n(cid:74) Fi =(cid:70)
i∈(cid:74)0,n(cid:74) Fi is also an
RPCA if D(Fi) and D(Fj) are over disjoint alphabets, for i (cid:54)= j. In this case, Ω(cid:70)
i∈(cid:74)0,n(cid:74) Fi =(cid:70)
i∈(cid:74)0,n(cid:74) ΩFi and
O(cid:70)
i∈(cid:74)0,n(cid:74) OFi.

12

2.4.1 Expansiveness
The projective line P := R (cid:116) {∞} is seen as the set of slopes to the vertical direction. Here, quite
unconventionally, the horizontal direction is represented by ∞ and the vertical one by 0. The relevance of
this choice will appear later, but in any case it does not aﬀect any set-theoretical, topological or computable
property because the inversion map over P is a computable homeomorphism.

The projective line P admits a natural eﬀective topology if seen as the quotient of the circle by central
symmetry: a subset is eﬀectively closed if the corresponding subset of the circle is eﬀectively closed as a
subset of [0, 1]2. This topology is equivalent to the one-point compactiﬁcation of the R and renders P a
compact, metric space.
Let X be a 2D subshift, l ∈ P a slope and l ⊂ R2 the corresponding vectorial line. We say that direction
l is expansive for X if there exists a bounded shape V ⊂ R2 such that, for all x, y ∈ X,

x|(l+V )∩Z2 = y|(l+V )∩Z2 ⇒ x = y .

We denote by N (X) the set of non-expansive directions (i.e., the set of directions that are not expansive).
The terminology comes from the fact that if l = p/q is rational (or inﬁnite), then l is expansive for X if and
only if the dynamical system (X, σ(p,q)) is expansive, in the classical sense of expansive dynamical systems.
Expansive directions were ﬁrst introduced by Boyle and Lind [6] in a more general setting. The following

fact is a particular case of [6, Theorem 3.7].
Proposition 9. Let X be a 2D subshift. Then, N (X) is closed. In addition, N (X) is empty if and only if
X is ﬁnite.

We say that X is extremely expansive if |N (X)| = 1, which is, according to Proposition 9, the most

constrained non-trivial case.

non-expansive directions that comes from computation theory, as is usually the case, see [18, 20].

In the case of SFTs (actually, of all eﬀective subshifts), we have an additional restriction on the set of
A direction l ∈ P can be represented as the pair of coordinates of the intersection of the line l with the
unit circle. This gives two (symmetric with respect to the origin) representations for each direction which
are computably equivalent. Computability questions about expansive directions can then be transferred to
computability questions about pairs of real numbers, which we already know how to deal with.
It can be noted that eﬀectively closed subsets that do not contain {∞} are exactly the eﬀectively closed
subsets of R. The restriction map from P (with the above-deﬁned eﬀective topology) onto R is actually
computable, and it can be noted that the pre-image of an eﬀectively closed set by a computable function is
eﬀectively closed.
Lemma 10. Let X be a 2D SFT. Then, N (X) is eﬀectively closed.

In particular, if an SFT X has a unique direction of non-expansiveness, then this direction must be

computable.

Proof. The statement follows from the following two facts: First, it is semi-decidable whether a direction
is expansive, i.e., there exists a TM that takes as input a (rational direction) and halts if the direction is
expansive. This follows from [6, Lemma 3.2]. Secondly, it is semi-decidable whether two expansive directions
belong in the same expansive component. (The expansive component of an expansive direction is the largest
connected set that includes the direction and is included in the set of expansive directions. One can see that
it is always an open interval.) This follows from [34], as described in [5, Appendix C].
Having these two facts in mind, it is not diﬃcult to see that the following algorithm enumerates a
sequence of intervals whose union is the complement of N (X): For each rational direction, check whether it
is expansive. Every time you ﬁnd an expansive direction, check whether it is in the same component with
one of the expansive directions that you have already found. Every time this is the case, output the whole
interval of directions that is between them.

13

A subshift Y is called extremely-expansively soﬁc if there exists an extremely expansive SFT that
factors onto Y . Since expansive directions are not preserved through block maps, an extremely-expansively
soﬁc subshift need not be extremely expansive itself. In fact, as we will see, there exist extremely-expansively
soﬁc subshifts that do not have any direction of expansiveness.
Lemma 11. Let X0, X1, . . . be 2D subshifts over the same alphabet A.

w Xw) =(cid:83)

w N (Xw).

• If X0 ⊆ X1, then N (X0) ⊆ N (X1).
• If N (X0) ∩ N (X1) = ∅, then X0 ∩ X1 is a ﬁnite subshift.

w Xw is a closed disjoint (possibly uncountable) union, then N ((cid:70)

• If(cid:70)
Finally, for the last claim, the inclusion N (Xw) ⊆ N ((cid:70)
For the other inclusion, assume l ∈ N ((cid:70)

Proof. The ﬁrst claim follows immediately from the deﬁnitions.
For the proof of the second claim, we have that N (X0 ∩ X1) ⊆ N (X0) ∩ N (X1) = ∅ according to the
ﬁrst claim. Therefore, N (X0 ∩ X1) = ∅, and since X0 ∩ X1 is a subshift, Proposition 9 gives that it is ﬁnite.

w Xw) Then, there exist x, y ∈(cid:70)

w Xw) comes from the ﬁrst point.

w Xw which coincide over an
open half-plane Hl ⊆ R2 of slope l and disagree somewhere outside it. The orbits of x and y under the shift
action have a common limit point z. Then, z is in the intersection Xx∩Xy of the subshifts that contain x and
y, respectively. By disjointness, we get that Xx = Xy = Xw(cid:48), for some w(cid:48), which means that l ∈ N (Xw(cid:48)).

If F is an RPCA, then we denote N (F ) := N (OF ). It is straightforward that the horizontal direction
(which according to our deﬁnition is ∞) is expansive for F . It is not much more complicated to see that, if
the bi-radius is 1, N (F ) ⊆ [−1, 1] (directions around the horizontal are expansive).
expansive is equal to OF , for some RPCA F .

Conversely, it can be shown that, up to a recoding, every 2D SFT for which the horizontal direction is

14

Chapter 3

Simulation

S,T

S,T,Φ

S,T,Q

S,T,Q,Φ

G, F (cid:23)

G−1Φ = Φσ−QF −T and the simulating subshift ˜D(Φ) :=(cid:70)

3.1 Simulation
If S, T ∈ N1 and Q ∈ Z, we say that RPCA F : AZ (cid:57) AZ
if there is a partial continuous decoding surjection Φ : AZ (cid:57) BZ
0≤t<T
0≤s<S

(S, T, Q)-simulates RPCA G : BZ (cid:57) BZ
such that σΦ = ΦσS, GΦ = ΦσQF T ,
σsF t(D(Φ)) is a disjoint union.In other
words, 1 step of G is encoded into T steps of F , up to some shift by Q, and the intermediary steps used are
(cid:23)
G, or when some parameters are clear from the context or not so
not valid encodings. We note F
G, or F (cid:23) G (each time this symbol will be used, F and G are meant
, F (cid:23)
important, F (cid:23)
to be RPCA).
We remind the reader that according to our notations, σΦ = ΦσS and GΦ = ΦσQF T and G−1Φ =
Φσ−QF −T imply that the domains of the two partial functions are identical. This is in fact crucial for
understanding the notion of simulation and it will be used extensively in the proofs and constructions to come.
For example, this means that the equality GΦ = ΦσQF T does not immediately imply G−1Φ = Φσ−QF −T ,
because the domains of G−1Φ and Φσ−QF −T might be diﬀerent (if we only had the equality GΦ = ΦσQF T ,
it could happen that x ∈ D(G−1Φ) but x /∈ F T σQ(AZ
In fact, one can see that the couple of conditions GΦ = ΦσQF T and G−1Φ = Φσ−QF −T is equivalent to
the triple of conditions GΦ = ΦσQF T , D(GΦ) = D(Φσ−QF −T ) and D(G−1Φ) = D(ΦF T σQ).
F exactly simulates G if Φ is actually bijective. In other words, there exists a well-deﬁned encoding
function Φ−1 : BZ → D(Φ). F completely simulates G if, besides, ΩF ⊂ ˜D(Φ). In other words, every
bi-inﬁnite orbit of F will eventually encode some orbit of G. Actually, in our constructions we will even have
the stronger D(F t(cid:48)
Remark 12.

) ⊂ ˜D(Φ), for some t(cid:48) ∈ Z.

)).

1. D(Φ) = σS(D(Φ)).
2. F (cid:23)

G if and only if F (cid:23)

σDG.

S,T,0

S,T,DS

3. For any s ∈(cid:74)0, S(cid:74) , t ∈(cid:74)0, T(cid:74), σsF t(D(Φ))[S] is an SFT.
determines the (unique) s ∈(cid:74)0, S(cid:74) and t ∈(cid:74)0, T(cid:74) such that x ∈ σsF t(D(Φ)).

4. Since the union ˜D(Φ) is disjoint, there exists a shape U ⊂ﬁnite Z such that for any x ∈ ˜D(Φ) , x|U

Proof. The ﬁrst two claims follow immediately from the deﬁnitions.
of a PCA over AZ

For the third claim, notice that since Φ is continuous and σΦ = ΦσS, this means that D(F ) is the domain
[S], so it is an SFT. Since σ and F are invertible maps and the property of being an SFT

is preserved under invertible maps, we have that σsF t(D(Φ))[S] is an SFT for all s ∈(cid:74)0, S(cid:74) and t ∈(cid:74)0, t(cid:74).

15

The last claim follows easily from the disjointness using a classical compactness argument.

We can prove an analogue of Curtis-Lyndon-Hedlund theorem for decoding and encoding functions.

Remark 13. The decoding function Φ admits a neighbourhood V ⊂ﬁnite Z and a partial bulked local rule
φ : AV (cid:57) B such that for all x ∈ AZ
, Φ(x) is deﬁned if and only if φ(x|iS+V ) is deﬁned for any i ∈ Z, in
which case the latter is equal to Φ(x)i.
If the simulation is exact, the encoding function Φ−1 admits a neighbourhood V ⊂ﬁnite Z and a partial
unbulked local rule, abusively noted φ−1 : BV (cid:57) AS such that for all y ∈ BZ
, Φ−1(y) is deﬁned if and
only if φ−1(y|i+V ) is deﬁned for any i ∈ Z, in which case the latter is equal to Φ−1(y)(cid:74)iS,(i+1)S(cid:74).

Exact complete vertical (i.e., Q = 0) simulation is stronger than most notions found in the literature. In

particular:

• OF simulates OG in the sense of [8].
• The Z2-action (F, σ) over the limit set ΩF (or the 2D SFT OF ) is conjugate to a suspension of ΩG in

the sense of a homeomorphism

Ψ : ΩF → ΩG ×(cid:74)0, S(cid:74) ×(cid:74)0, T(cid:74)

x (cid:55)→ (ΦF −tσ−s(x), s, t), where F −tσ−s(x) ∈ D(Φ)

• The Z2-action (G, σ) over the limit set ΩG (or the 2D SFT OG) is conjugate to the Z2-action (F T , σS)

restricted to D(Φ) ∩ ΩF (see [10]);

• G is a sub-automaton of a rescaling of F , so that F simulates G according to the deﬁnition of simulation
given in [35]. While it is not necessary to formally deﬁne this notion of simulation, we can intuitively
say that rescaling corresponds to the role of parameters S and T in our deﬁnition, while the sub-
automaton condition corresponds to the decoding function Φ. We notice, however, that Ollinger’s
deﬁnition is more general than ours, since it does not require ˜D(Φ) to be a disjoint union, while the
simulated can also be rescaled.

But the deﬁnition above also involves the transient part: every locally valid horizontal strip of height

t + 1 for G gives a locally valid horizontal strip of height T t + 1 for F .

The following facts about our notion of simulation follow directly from the deﬁnition:
• Each kind of simulation is a conjugacy invariant.
• If F simulates G (resp. exactly), then it simulates (resp. exactly) any of its subsystems (but clearly,

completeness is not preserved). If F factors onto G, then F (cid:23)

G completely.
F completely if G does not have empty domain. Also F × G (cid:23)

1,1,0

• F × G (cid:23)

1,1,0

F exactly if G includes

1,1,0

a singleton subsystem (recall that G is a PCA, so that it does not necessarily have periodic points).
The simulation is simultaneously exact and complete if G is a singleton system.

• The surjectivity of Φ implies that only systems with empty domain can be simulated by systems with

empty domain.

We will mainly focus on non-trivial simulations: this means that S, T > 1 and G does not have empty
domain.
Remark 14. If F (cid:23)

G non-trivially, then for all j ∈(cid:74)0, T(cid:75), F j(D(GΦ)) = σ−QF −(T−j)(D(G−1Φ) (cid:54)= ∅.

S,T,Q,Φ

16

More speciﬁcally, a conﬁguration “in the middle” of the work period, i.e., when j = (cid:98)T /2(cid:99) has at least

(cid:98)T /2(cid:99) forward and backward images, or, in other words, it belongs to Ω

(cid:98)T /2(cid:99)
F

.

The following lemma states that the limit sets correspond, in the case of complete simulation. It is a
more mathematical and detailed version of the comment that we made earlier, that a valid strip horizontal
of height t + 1 in G gives a valid horizontal strip of height T t + 1 in F (provided that the strip is simulated).
Lemma 15. Assume F (cid:23)

G.

1. If j ∈ N (cid:116) {∞}, then

S,T,Q,Φ

˜Dj(Φ) :=

(cid:71)

0≤t<T
0≤s<S

σsF tΦ−1(Ωj
G)

(cid:84)

is a disjoint union and a subshift, included in Ω(j−1)T +1
j∈N ˜Dj(Φ).
2. ΩF ⊃ ˜D∞(Φ).
3. If the simulation is complete, then ΩF = ˜D∞(Φ).

F

. In addition, ˜Dj(Φ) ⊃ ˜Dj+1(Φ) and ˜D∞(Φ) =

Proof.

1. It is clear that ˜Dj(Φ) is a disjoint union and a subshift, each subset in the union being (syntactically)

included in one in the expression of ˜D(Φ). Assume that F (cid:23)

G for some Q ∈ Z. Now,

S,T,Q,Φ

Φ−1(Ωj

G) = D(GjΦ) ∩ D(G−jΦ)

= D(ΦσjQF jT ) ∩ D(Φσ−jQF −jT )
⊂ D(F jT ) ∩ D(F −jT ) = ΩjT
F .
G) ⊂ ΩjT−T +1

F

the deﬁnitions.

Hence, for any s ∈(cid:74)0, S(cid:74) and any t ∈(cid:74)0, T(cid:74), σsF tΦ−1(Ωj
2. It is obvious from the previous point that(cid:84)
F ⊃(cid:84)
and s ∈(cid:74)0, S(cid:74) , t ∈(cid:74)0, T(cid:74) such that σsF t(y) = x. Disjointness and a direct induction give that for all

3. Conversely, assume x ∈ ΩF , so that clearly ∀k ∈ Z, F k(x) ∈ ΩF . By completeness, there exist y ∈ D(Φ)
k ∈ Z, F k(y) ∈ F k mod T σQ(cid:98)k/T(cid:99)(D(Φ)). In particular, for all j ∈ Z, GjΦ(y) = ΦF jT σjQ(y) is deﬁned.
This gives that Φ(y) ∈ ΩG, so x ∈ σ−sF −tΦ−1(ΩG) = σS−sF T−tΦ−1(ΩG).

j∈N ˜Dj(Φ) = ˜D∞(Φ).

. The other claims follow from

j∈N ΩjT

The following remark links the periodic points of the simulating and simulated systems. It is essential
for proving aperiodicity of the subshifts that we construct. The same result appears in [8, 36], even though
the argument essentially goes back to the kite-and-dart tile set of Penrose. We give a slightly more general
version of the usual result also takes into consideration the shift by Q.
Remark 16. If F (cid:23)
OG admits a conﬁguration with period (k, l), where s = kS and t = lT .

G completely, then OF admits a conﬁguration with period (s − lQ, t) if and only if

S,T,Q

We will only use the case Q = 0, for which it is intuitively clear to see that it holds true. When q (cid:54)= 0,
one has to have in mind that for every T time steps of a conﬁguration of F , the simulated conﬁguration is
shifted Q steps to the left.

17

3.2 Nested simulations
In the sequel, we will be most interested in inﬁnite sequences of simulations of the form: F0 (cid:23) F1 (cid:23) F2 (cid:23) . . ..
This looks like a formidable task, since every RPCA of the sequence must contain the information about an
inﬁnite number of conﬁgurations and update this information within a determined time, but, as the results
of this section will imply, an inﬁnite sequence of simulations gives RPCA with very useful properties. The
construction of these sequences forms the basic part of our constructions and will be done in the following
chapters.

If S = (Si)0≤i≤n−1 is a sequence of numbers, then 1S is the sequence whose ﬁrst element is equal to 1
with the elements of S shifted by one after it. If S = (Si)0≤i≤n−1 and T = (Ti)0≤i≤n−1 are ﬁnite sequences
of non-zero numbers, then 1S/T is the sequence (Si−1/Ti)0≤i≤n−1, where S−1 := 1. A short calculation
shows that

1S/T(cid:89)

Q

Ti =

(cid:88)

Qi

(cid:89)

(cid:89)

 .

0≤i≤n−1

0<j<i

i<j≤n−1

Sj

Tj

Lemma 17. Simulation (resp. exact, complete, exact and complete) is a preorder.
More precisely, if F0
n ∈ N, then F0 (cid:23)

Fn (resp. exactly, completely), where

Sn−1,Tn−1,Qn−1,Φn−1

S0,T0,Q0,Φ0

S1,T1,Q1,Φ1

(cid:23)

(cid:23)

(cid:23)

. . .

F1

S,T,Q,Φ

(cid:89)

(cid:89)

1S/T(cid:89)

(S, T, Q, Φ) = (

Si,

Ti, Q

Ti, Φn−1 ··· Φ0).

Fn (resp. exactly, completely) for some

The products range from 0 to n − 1. If there were no shifts in the simulation (i.e., if Qi = 0 for all i)
the above statement would be more or less trivial. Even in the presence of shifts, the proof is essentially a
simple veriﬁcation.

Proof.

• Clearly F (cid:23)

F .

1,1,0,id

H. Then it is clear that σΦ(cid:48)Φ = Φ(cid:48)σS(cid:48)

Φ = Φ(cid:48)ΦσS(cid:48)S and HΦ(cid:48)Φ =

• Now suppose F (cid:23)

G

Φ(cid:48)σQ(cid:48)

GT (cid:48)

Φ = Φ(cid:48)ΦσQT (cid:48)+SQ(cid:48)

S,T,Q,Φ

(cid:71)

0≤t<T
0≤s<S

0≤t<T
0≤s<S

S(cid:48),T (cid:48),Q(cid:48),Φ(cid:48)

(cid:23)
F T (cid:48)T . Moreover:

σsF t(D(Φ)) ⊃ (cid:71)
(cid:71)
(cid:71)
(cid:71)

=

=

0≤t<T
0≤s<S

0≤t<T
0≤s<S

=

(cid:71)

σsF tΦ−1(

σs(cid:48)

Gt(cid:48)

(D(Φ(cid:48))))

(cid:71)
(cid:71)

0≤t(cid:48)<T (cid:48)
0≤s(cid:48)<S(cid:48)

0≤t(cid:48)<T (cid:48)
0≤s(cid:48)<S(cid:48)

σsF tΦ−1(σs(cid:48)

Gt(cid:48)

(D(Φ(cid:48))))

F t+t(cid:48)T σs+s(cid:48)S+t(cid:48)Q(D(Φ(cid:48)Φ))

0≤t(cid:48)<T (cid:48)
0≤s(cid:48)<S(cid:48)
σsF t(D(Φ(cid:48)Φ)) =: ˜D(Φ(cid:48)Φ) .

0≤t<T T (cid:48)
0≤s<SS(cid:48)

This proves that F

(cid:23)

SS(cid:48),T T (cid:48),QT (cid:48)+SQ(cid:48),Φ(cid:48)Φ

H.

• If Φ and Φ(cid:48) are bijections, then Φ(cid:48)Φ is also a bijection.

18

• If both simulations are complete, then by Point 3 of Lemma 15,
σsF tΦ−1(ΩG)

ΩF =

(cid:71)
⊂ (cid:71)

0≤t<T
0≤s<S

0≤t<T
0≤s<S

= ˜D(Φ(cid:48)Φ).

σsF tΦ−1( ˜D(Φ(cid:48)))

• A direct induction gives the expected results.

Similarly to simulations, which involve a decomposition of the system in terms of how much is shifted the
grid on which to read the encoding, a sequence of simulations involves a nested decomposition, which gives
a full skeleton, inside each conﬁguration, as expressed by the following lemma. Here, and in the following,
we use gothic letters to denote sequences, but the corresponding normal letters to denote the elements of

the sequences. Also, if S is an inﬁnite sequence and n ∈ N, then S(cid:74)0,n(cid:74) is the ﬁnite preﬁx of length n of S.
Finally, if (Φi)i∈N is a sequence of decoding functions, then Φ(cid:74)0,n(cid:74) will be the decoding function Φn−1 ··· Φ0.

Lemma 18.

1. If F0 (cid:23)

S0,T0,Φ0

F1 (cid:23)

S1,T1,Φ1

. . .

Sn−1,Tn−1,Φn−1

Fn

(cid:23)

. . . and j ∈ N (cid:116) {∞}, then

Sn,Tn,Φn

(cid:23)

(cid:92)

:=

n∈N

˜Dj(Φ)

˜Dj(Φ(cid:74)0,n(cid:74))
(cid:92)
(cid:71)
i∈N(cid:74)0,Ti(cid:74)
i∈N(cid:74)0,Si(cid:74)
In addition, ˜Dj(Φ) ⊃ ˜Dj+1(Φ) and ˜D∞(Φ) =(cid:84)

is a disjoint union and a subshift.

t∈(cid:81)
s∈(cid:81)

n∈N

=

j∈N ˜Dj(Φ).

σs(cid:74)0,n(cid:74)S

t(cid:74)0,n(cid:74) T

F
0

Φ−1

0

··· Φ−1

n−1(Ωj
Fn

)

2. If, besides, all simulations are nontrivial, then ˜D2(Φ) = ˜D∞(Φ) ⊂ ΩF0 is uncountable.
3. If the simulations (in the hypothesis of Point 1) are complete, then ˜D∞(Φ) = ΩF0.
4. If the sequence (Φn)n∈N is computable, then the map x ∈ ˜D∞(Φ) → (si, ti)i∈N, where (si, ti)i∈N is the

(unique) sequence such that x ∈(cid:84)

n σs(cid:74)0,n(cid:74)S

F
0

t(cid:74)0,n(cid:74) T

D(Φ(cid:74)0,n(cid:74)), is computable.

Point 2 implies nonemptiness of ΩF0 and OF0 , and of any ΩFn , since all those statements can be applied
to the sequence starting from n. Point 4 states that we can always recover the skeleton from a valid
conﬁguration. In particular the skeleton map is continuous.

Proof.

1. By Lemma 17 and compactness, it is clear that ˜Dj(Φ) is a subshift. The equality is rather easily
m, t(cid:48)
m), then
), which is,

if (s, t) (cid:54)= (s(cid:48), t(cid:48)), say (sm, tm (cid:54)= s(cid:48)
m−1(Ωj

checkable. We can see that the union is disjoint:

) is included in σs(cid:74)0,m(cid:74)S

··· Φ−1

··· Φ−1

t(cid:74)0,n(cid:74) T

Φ−1

Φ−1

F
0

0

0

Fm

(cid:84)
n∈N σs(cid:74)0,n(cid:74)S
(cid:84)
according to Lemma 17, disjoint from σs(cid:48)(cid:74)0,m(cid:74)S
n∈N σs(cid:48)(cid:74)0,n(cid:74)S

n−1(Ωj
Fn

··· Φ−1

t(cid:48)(cid:74)0,n(cid:74) T

n−1(Ωj
Fn

Φ−1

) .

F
0

0

t(cid:74)0,m(cid:74) T
m−1(Ωj

F
0
··· Φ−1

Fm

t(cid:48)(cid:74)0,m(cid:74) T

F

0

Φ−1

0

) which includes

19

2. Since for any n ∈ N and m ≥ n, Fn

(cid:23)

Fm, then Point 1 of Lemma 15 says

F

n

that Ω

Tn···Tm−1+1

⊃(cid:84)

Sn···Sm−1,Tn···Tm−1,Φm−1···Φn

ﬁxed and m → ∞, and ΩFn ⊃(cid:84)
inclusion in the deﬁnition of ˜D∞(Φ(cid:74)0,n(cid:74)) gives that ˜D∞(Φ) ⊃(cid:84)

⊃ ˜D2(Φ(cid:74)n,m(cid:74)) . If the simulations are nontrivial, then Tn ··· Tm−1 → ∞ when n is
m∈N ˜D2(Φ(cid:74)n,m(cid:74)) = ˜D2(Φ(cid:74)n,∞(cid:74)) . Injecting this

˜D2(Φ(cid:74)0,m(cid:74)) ⊃ ˜D2(Φ).
The converse is trivially true, and Point 3 of Lemma 15 already tells us that ˜D∞(Φ) ⊂ ΩF0.
Fn+2 with TnTn+1 ≥ 4, then Remark 14 gives that Ω2
is non-empty.
Moreover, since Fn
Therefore, each of the uncountably many subsets in the disjoint union expressing ˜D2(Φ) is a closed
non-empty intersection.

T0···Tm−1+1

SnSn+1,TnTn+1

m∈N Ω

m≥0

(cid:23)

Fn

F

n

3. If n ∈ N is such that F0
ΩF0 = ˜D∞(Φ(cid:74)0,n(cid:74)).
4. This follows from repeated application of Remark 5.4 and the fact that Φ(cid:74)0,n(cid:74) is a decoding function
for all n ∈ N.

Fn completely, then by Point 3 of Lemma 15,

S0···Sn−1,T0···Tn−1,Φn−1···Φ0

(cid:23)

The following extends Lemma 18 (which can be recovered by Bi being singletons). In this case, every
RPCA simulates a disjoint union of RPCA, each one of which simulates a disjoint union of RPCA and so
on. In this way, we obtain an “inﬁnite tree” of simulations. Along any branch of this tree, Lemma 18 is true,
but, more importantly, something similar is true even when we take all the (possibly uncountable) branches
of this tree together.

Lemma 19.

z(Φ) :=(cid:84)

there exist Su, Tu, Qu ∈ N, a decoding function Φu and a RPCA Fu such that Fu
Let ˜Dj

1. Let (Bn)n∈N be a sequence of ﬁnite alphabets, such that for any word u ∈ (cid:81)
Then, for any j ∈ N (cid:116) {∞} and any closed Y ⊂(cid:81)
(cid:12)(cid:12)(cid:12) ˜D∞
z (Φ) (cid:54)= ∅(cid:111)

n∈N ˜Dj(Φz(cid:74)0,n(cid:74) ) for all j ∈ N (cid:116) {∞}, z ∈(cid:81)

(cid:110)
Y (Φ) ⊂ ΩF.
Y (Φ) = ˜D∞
i∈N Bi

Y (Φ) :=(cid:70)

and a subshift, and ˜D2

z ∈(cid:81)

2. Besides, the set Z :=

i∈N Bi, ˜Dj

i∈N Bi.

z∈Y

(cid:70)

i<n Bi of length n ∈ N,
Fub.

(cid:23)

b∈Bn

Su,Tu,Φu

˜Dj
z(Φ) is a disjoint union

ulations is closed. If the simulations are complete, then ˜D2
In the above statement, the notation Φz(cid:74)0,n(cid:74) stands for the composition Φz(cid:74)0,n(cid:74) ··· Φz0 Φ, which is the

Z(Φ) = ˜D∞

Z (Φ) = ΩF .

corresponding to nested nontrivial, non-empty sim-

decoding function from Fz(cid:74)0,n(cid:74) onto F.
Proof.

1. Point 2 of Lemma 18 gives that ˜D∞

these RPCA have non-empty domain. The converse is obvious.
By the same distributivity of decreasing intersections over unions as for Point 1 of Lemma 18, it can
be easily seen that

z (Φ) (cid:54)= ∅ if Fz(cid:74)0,n(cid:74) (cid:23) Fz(cid:74)0,n+1(cid:75) non trivially for any n ∈ N, i.e., all
(cid:71)

(cid:92)

σsF t

 Φ−1

 Φ−1

u0

··· Φ−1

u (Ωj
Fu

) ,

˜Dj
Y (Φ) =

n∈N

u∈Ln(Y )

(cid:71)
i<n Tu(cid:74)0,i(cid:74)
i<n Su(cid:74)0,i(cid:74)

0≤t<(cid:81)
0≤s<(cid:81)

which is a decreasing intersection of ﬁnite unions of subshifts, and we have ˜D2
all z ∈ Y .

z(Φ) = ˜D∞

z (Φ) ⊂ ΩF for

20

2. If Fu (cid:23)(cid:70)

a∈Bn

Fua completely, then Point 3 of Lemma 15 gives

(cid:71)

(cid:71)

a∈Bn

ΩFu =

uσsΦ−1
F t
u (

ΩFua) .

0≤t<Tu
0≤s<Su
An immediate induction gives for any n ∈ N,
0≤t<(cid:81)
0≤s<(cid:81)

u∈Ln+1(Z)

(cid:71)

ΩF =

(cid:71)
i<n Tu(cid:74)0,i(cid:74)
i<n Su(cid:74)0,i(cid:74)

σsF t

 Φ−1

 Φ−1

u0

··· Φ−1

u(cid:74)0,n(cid:74) (ΩFu ) .

Being true for any n, this gives the result.

Lemmas 18 and 19 can be seen as extensions of Lemma 15 in the case of an inﬁnite nested simulation.

The following lemma can be seen as such an extension of Remark 16.
Lemma 20. If F0 (cid:23)
OF0 is aperiodic.

Fn (cid:23)

F1 (cid:23)

Sn−1,Tn−1

Sn,Tn

S0,T0

S1,T1

(cid:23)

. . .

. . . completely, with Sn, Tn > 1 for any n ∈ N, then

In particular, either ΩFn = ∅ (= OFn) for all n ∈ N or, ΩFn (and OFn ) is aperiodic uncountable, for all

n ∈ N.

(cid:23)

S0···Sn−1,T0···Tn−1

Fn completely. By Remark 16, OF0 cannot have any nontrival
Proof. From Lemma 17, F0
period less than S0 ··· Sn−1 horizontally and less than T0 ··· Tn−1 vertically. If these two products go to
inﬁnity, we get that there cannot exist any periodic points.

In fact, it follows from the proof that it is enough that one of the products (cid:81)

i∈N Ti is
It is well known that a non-empty, aperiodic 2D SFT is uncountable. Lemma 18 gives some

inﬁnite.
additional information about how uncountability occurs in the case of an inﬁnite nested simulation.

i∈N Si and (cid:81)

3.3 Expansiveness and simulation

The following lemmas highlight the relation between the notions of simulation and expansive directions.
This subsection extends slightly Section 5 in [19]. The following lemmas correspond to Lemma 5.1 and
Lemma 5.3 in [19], which examine how the so-called “shape of prediction” evolves. It also motivates the
choice of considering the horizontal direction as ∞, which will make many future expressions clearer.
Lemma 21. Suppose F (cid:23)
complete, then N (F ) = 1

T (Q + SN (G)). Moreover, if the simulation is

G exactly. Then N (F ) ⊇ 1

S,T,Q

T (Q + SN (G)).

In particular, N (σ−QG) = N (G) + Q and N (GT ) = 1

T N (G).

(cid:20) S Q

(cid:21)

Proof. Let us consider the matrix M :=
corresponding vectorial line, l(cid:48) := M l the vectorial line corresponding to slope S
corresponds to l for G.

as acting over R2. Consider a slope l ∈ P, l ⊂ R2 the
T . Roughly, l(cid:48) for F

T l + Q

0 T

21

• Consider a ﬁnite shape W (cid:48) ⊂ R2, U and f the neighbourhood and local rule of F , V and φ−1 those

of Φ−1, as deﬁned in Remark 13. Without loss of generality, we can assume that U =(cid:74)−uS, uS(cid:75), for
Let W := M−1W (cid:48) + (T(cid:74)−u, u(cid:75) + V +(cid:74)−Q, 0(cid:74)) × {0} + [−1, 2[×[0, 1[. If l ∈ N (G), then there exist

some u ∈ N.

conﬁgurations x (cid:54)= y ∈ ΩG such that OG(x)|l+W = OG(y)|l+W .
Then, OF (Φ−1(x)) (cid:54)= OF (Φ−1(y)), but we claim that

OF (Φ−1(x))|l(cid:48)+W (cid:48) = OF (Φ−1(y))|l(cid:48)+W (cid:48) .

1

T ∈ N (F ), which proves that N (F ) ⊇
Since W (cid:48) was an arbitrary ﬁnite shape, this implies that S
T (Q + SN (G)).
Let us proceed with the proof of the claim. Let (p1, p2) ∈ l(cid:48) + W (cid:48) and write p1 =: mS + r, p2 =: nT + q
and n := m(cid:48)S + r(cid:48), where m, r, n, q, m(cid:48), r(cid:48) ∈ Z and 0 ≤ r, r(cid:48) < S and 0 ≤ q < T . Intuitively, we can
think that (p1, p2) belongs to the encoding of the m’th letter of σ−m(cid:48)QGn(x) and Gn(y).
More precisely, a straightforward computation shows that

T l + Q

M−1(p1, p2) = (m − Qm(cid:48) + r/S − r(cid:48)/S + q/T, n + q/T ),

so that (m − Qm(cid:48), n) ∈ M−1(p1, p2) + [−1, 2[×[0, 1[. This, in turn, implies that (m − Qm(cid:48), n) +

(T(cid:74)−u, u(cid:75) +(cid:74)−Q, 0(cid:74) + V ) × {0} is included in l + W , so that

OG(x)|(m−Qm(cid:48),n)+(T(cid:74)−u,u(cid:75)+(cid:74)−Q,0(cid:74)+V )×{0} == OG(y)|(m−Qm(cid:48),n)+(T(cid:74)−u,u(cid:75)+(cid:74)−Q,0(cid:74)+V )×{0} .

Using the facts that V is the neighbourhood of φ−1 and that Φ−1 “blows-up” letters into blocks of size
S × T with an additional shift of Q for every vertical time step, we deduce that
OF (Φ−1(x))|((cid:74)(m−Qm(cid:48))S,(m−Qm(cid:48)+1)S(cid:74)+T(cid:74)−uS,uS(cid:75)+(cid:74)−QS,0(cid:74)+nQ)×{nT} =
= OF (Φ−1(y))|((cid:74)(m−Qm(cid:48))S,(m−Qm(cid:48)+1)S(cid:74)+T(cid:74)−uS,uS(cid:75)+(cid:74)−QS,0(cid:74)+nQ)×{nT} .
Notice that nQ − Qm(cid:48)S = r(cid:48)Q. Now, using the fact that T(cid:74)−uS, uS(cid:75) is a neighbourhood for f q and
(cid:74)−QS, 0(cid:74) for σ−r(cid:48)Q, we obtain that
σ−r(cid:48)QF q(cid:0)OF (Φ−1(x))(cid:1)

|(cid:74)mS+r(cid:48)Q,(m+1)S+r(cid:48)Q(cid:74)×{nT} =

= σ−r(cid:48)QF q(cid:0)OF (Φ−1(y))(cid:1)

|(cid:74)mS+r(cid:48)Q,(m+1)S+r(cid:48)Q(cid:74)×{nT} .

The last equality implies that OF (Φ−1(x))|(p1,p2) = OF (Φ−1(y))|(p1,p2) , because

σ−r(cid:48)QF q(cid:0)OF (Φ−1(x))(cid:1)
= OF (Φ−1(x))|(cid:74)mS,(m+1)S(cid:74)×{nT +q}
= OF (Φ−1(x))|(cid:74)mS,(m+1)S(cid:74)×{p2}

|(cid:74)mS+r(cid:48)Q,(m+1)S+r(cid:48)Q(cid:74)×{nT}

• Consider a ﬁnite shape W ⊂ R2, U the synchronizing shape as deﬁned in Remark 12, V and φ the

and p1 ∈(cid:74)mS, (m + 1)S(cid:74).
neighbourhood and local rule of Φ as deﬁned in Remark 13, and W (cid:48) := M W + (V ∪ U )×{0}−(cid:74)0, S(cid:74)×
(cid:74)0, T(cid:74).

22

If S

T l + Q

T ∈ N (F ), then there exist conﬁgurations x (cid:54)= y ∈ ΩF such that OF (x)|l(cid:48)+W (cid:48) = OG(y)|l(cid:48)+W (cid:48) .
By Remark 12 and completeness of the simulation, there exist common s ∈(cid:74)0, S(cid:74) and t ∈(cid:74)0, T(cid:74) such
that x(cid:48) := σ−sF −t(x) and y(cid:48) := σ−sF −t(y) are in D(Φ). It follows easily from the deﬁnitions that
OF (x(cid:48))|l(cid:48)+MW +V ×{0} = OF (y(cid:48))|l(cid:48)+MW +V ×{0} .
By injectivity of Φ, OG(Φ(x(cid:48))) and OG(Φ(y(cid:48))) are also distinct, but we claim that they coincide
in l + W . Since W is an arbitrary ﬁnite shape, this implies that l ∈ N (G), which proves that
N (F ) ⊆ 1
Let (p1, p2) ∈ l + W . Then, M (p1, p2) + V × {0} ⊂ l(cid:48) + M W + V × {0}; it follows from this that

T (Q + SN (G)).

OF (x(cid:48))|(p1S+p2Q+V,p2T ) = OF (y(cid:48))|(p1S+p2Q+V,p2T ) .

In addition, we have that

OG(Φ(x(cid:48)))(p1,p2) = Gp2Φ(x(cid:48))p1

= Φσp2QF p2T (x(cid:48))p1
= φ(σp2QF p2T (x(cid:48))|p2S+V )
= φ(OF (x(cid:48))|(p1S+V +p2Q,p2T ) )

The same holds for y(cid:48), and since, as we have noticed earlier, the ﬁnal expression is the same for x(cid:48) and
y(cid:48), we get that OG(Φ(x(cid:48)))|(p1,p2) = OG(Φ(y(cid:48)))|(p1,p2) , as claimed.

Lemmas 17 and 21 can be combined to obtain expansive directions in nested simulations, which will be

used extensively in Section 7.

Lemma 22. If F0

(cid:23)

S0,T0,D0S0

F1

have bi-radius 1, then

(cid:23)

S1,T1,D1S1

Sn−1,Tn−1,Dn−1Sn−1

Fn completely exactly, and all these RPCA

(cid:23)

. . .

(cid:32)(cid:89)

(cid:33)

Si
Ti

(cid:33)

(cid:32)(cid:89)

i<n

Si
Ti

N (F0) ⊆ SD

1S/T

+

[−1, 1] = D

S/T

+

Proof. We already noted that the radius of a RPCA Fn with bi-radius 1 has N (Fn) ⊆ [−1, 1]. From Lemma
17, we know that F0 (cid:23)
S,T,Q
Lemma 21, we deduce that:

Fn exactly completely, where (S, T, Q) = ((cid:81) Si,(cid:81) Ti, SD

1S/T(cid:81) Ti) and from

i<n

i<n

N (F0) = SD

1S/T

+

N (Fn) ⊆ SD

1S/T

+

[−1, 1] .

(cid:32)(cid:89)

(cid:33)

[−1, 1] .

Si
Ti

(cid:33)

(cid:32)(cid:89)

i<n

Si
Ti

Also, by deﬁnition we have that SD

1S/T

= D

S/T

.

In the limit case of an inﬁnite nested simulation, we obtain the following proposition, which slightly

Fi+1 completely exactly, for all i ∈ N, then

extends Theorem 5.4 in [19].
(cid:23)

Proposition 23. If Fi

(cid:32)
In particular, if the simulations are non-trivial and(cid:81)

N (F0) ⊆ D

Si,Ti,DiSi

S/T

+

(cid:33)

(cid:89)

i<n

Si
Ti

inf
n∈N

[−1, 1] .

i<n Si/Ti converges to 0, then N (F0) = {D

S/T}.

23

Proof. From Lemma 22, we know that

N (F0) ⊆ (cid:92)

n∈N

(cid:33)

(cid:32)(cid:89)

i<n

Si
Ti

[−1, 1] + D(cid:74)0,n(cid:74)S(cid:74)0,n(cid:74)/T(cid:74)0,n(cid:74) ,

which gives the wanted inclusion, when n goes to ∞.

For the second claim, if all the simulations are non-trivial, then from Lemma 18 we know that OF0 is
uncountable, hence by Proposition 9, it has at least one non-expansive direction. In addition, by the ﬁrst
S/T}, and we must actually have

claim and the assumption (cid:81)

i<n Si/Ti → 0, we know that N (F0) ⊆ {D

equality.

3.4 Explicit simulation

In the previous sections of this chapters, we deﬁned a notion of simulation and then proved some facts about
this notion, which suggest that it is a good choice. However, we have not given any non-trivial example of
simulation until now, nor have we explained how this could happen. For example, the decoding function Φ
could be anything.

The simulation that we construct all have the same basic “form”. We call these simulation explcit,
because the simulated conﬁguration is explicitly written letter by letter in the simulating conﬁguration. In
order to make this more precise, we need to give some more deﬁnitions and notations.

Let us ﬁx a some ﬁelds Addr, Addr+1, Clock and Clock+1 (In fact, these are just distinct numbers that

we use to project letters on). These are sometimes called coordinate ﬁelds. For s ∈(cid:74)0, S(cid:74) and t ∈(cid:74)0, T(cid:74), let

Σs,t,S,T := P s,S
. In Σs,t,S,T the values of Addr grow by 1 modulo S from left to right
and the value of Clock is constant and equal to t, while the origin has Addr s. This is the usual way to
break up a conﬁguration into blocks, with one small diﬀerence. Normally, we only need the ﬁelds Addr and
Clock to do this. However, since we are using PPA, we need to have some right- (or left-) moving copies of
these ﬁelds in order to check the compatibility of these ﬁelds. Having this in mind, we deﬁne Σs,t,S,T in the

above way, since it will make notation a little lighter later on. The union(cid:70)

Addr,Addr+1 ∩ S t

Clock,Clock+1

0≤t<T
0≤s<S

Σs,t,S,T is disjoint.

In addition, let Σs,S := P s,S

constant). Clearly, Σs,t,S,T ⊆ Σs,S. For c ∈ Σs,S and i ∈ Z, the pattern

Addr,Addr+1 . In Σs,S, we do not care about the value of Clock (or if it is even

is called a colony of c. Clearly, (Bc
0 to S − 1 from left to right.

i , the value of Addr (and Addr+1) grows from

Bc

i = c(cid:74)−s+iS,−s+(i+1)S(cid:74)
i )i∈Z = σ−s(c)[S] and in Bc

Let ˜φ : (F∗

5 be the following function, which is the basis of all the decoding functions that we
5)∗ be a word over the inﬁnite alphabet F∗
5 (we look at w as a ﬁnite part of some 1D
5). If (cid:105)w(cid:104) = χ (u) 3|w|−|χ(u)|, where u ∈ F∗∗
5 (we look at u as a tuple of elements of F∗
5),

This is the natural way to break a conﬁguration into colonies of size S. Now, we are going to use every
colony to encode one letter of the simulated conﬁguration. For this, we have to deﬁne the appropriate
decoding function.
5)∗ (cid:57) F∗∗
will use: Let w ∈ (F∗
conﬁguration over F∗
then we deﬁne ˜φ(w) = u.
Notice that χ (u) ∈ F∗
(this gives a word in F∗
word in (F∗
necessary condition so that ˜φ(w) = u is that |w| ≥ |χ (u)|.

˜φ is well-deﬁned because χ (·) is an injection and because 3 does not appear as a letter of χ (u) ∈ F∗
5 )∗ (cid:57) F∗∗
Let Field be a new ﬁeld and ˜φField : (F∗∗
We can extend ˜φ in a natural way to a map ˜Φ : (F∗
5)

3. In other words, w is equal to χ (u) up to appending some 3s at the end of χ (u)
4) and then adding some 4s in front of every letter of χ (u) 3|w|−|χ(u)| (which gives a

5 be deﬁned as ˜φπField. ˜φField can read words over
and i ∈ Z,
Z (cid:57) (F∗∗
Z
5 )

˜Φ(c)i = ˜φ(c|(cid:74)iS,(i+1)S(cid:74) ). Similarly, ˜φField can be naturally extended to a map ˜ΦField : (F∗∗

5)∗). Unless w has this very speciﬁc form, ˜φ(w) is not deﬁned.

letters with many ﬁelds by ignoring the other ﬁelds and using ˜φ on Field.

as follows: for all c ∈ (F∗
Z
5)

Z (cid:57) (F∗∗
5 )

3. A

Z

5 )

.

24

The idea is that every conﬁguration will be divided into colonies using the coordinate ﬁelds and then
˜φField will be used on every colonies so as to obtain a letter. Putting these letters together, we obtain the
simulated conﬁguration.
Formally, a decoding function Φ will be equal to ˜ΦField|Σ , where Σ ⊆ Σ0,S, for some S that is large
i ), then Φ(c) = ˜ΦField(c) = (bi)i∈Z. We call bi the simulated letter of the i’th

enough. If bi = ˜φField(Bc
colony and the letters of c are the simulating letters.
The decoding functions that we will use in our constructions will always be of the form ˜ΦField|Σ , where
Σ ⊆ Σ0,S. For such functions, we immediately obtain two of the conditions of a decoding function of a
simulation:
Remark 24. Let us ﬁx a ﬁeld list C = [Addr, Addr+1, Clock, Clock+1, Tape], S ∈ N1 and vectors k, k(cid:48) ∈ N∗

such that the following inequalities hold:  kAddr ≥ (cid:107)S(cid:107)
S ≥(cid:12)(cid:12)(cid:12)χ
(cid:16)

kTape ≥ 1

Fk(cid:48)

5

(cid:17)(cid:12)(cid:12)(cid:12) ,

Let Σ := (Fk
5 )

Z ∩ Σ0,S ∩ ˜Φ−1

Tape((Fk(cid:48)
Z
5 )
In addition, for every b ∈ (F∗∗
Z
5 )

). Then Φ := ˜ΦTape|Σ : (Fk
5 )
, we are free to chose the values of the anonymous ﬁelds in any way we

is surjective and ΦσS = σΦ.

Z → (Fk(cid:48)
5 )

Z

like in a pre-image.

5 that are well-structured (i.e., divided into
colonies with the origin having address 0) and such that in the i’th colony we have the encoding of a letter
of Fk(cid:48)

In the above remark, Σ contains those conﬁgurations over Fk
5 , for all i ∈ Z.

25

Chapter 4

The programming language

4.1 Deﬁnitions and basic permutations

In our constructions, we want to use permutations that are computed fast. It is not possible to formally state
what fast means, but polynomially computable and, more generally, polynomially checkable permutations
is fast enough. This is a common feature of all self-similar and hierarchical constructions and the reasons
why it is needed are explained very thoroughly in [12]. For our purposes, it is enough to describe a pseudo-
programming language, with which we will write “programs” that are interpreted as permutations α : F∗∗
(cid:57)
F∗∗
5 .
Let us start describing this programming language: It has four types, terms (that are denoted t, t(cid:48) . . .),
valuations (that are denoted v, v(cid:48), . . .), conditions (that are denoted c, c(cid:48), . . .) and permutations (that are
denoted α, α(cid:48), . . .). Each type is semantically interpreted as a diﬀerent kind of mathematical object. Terms
are interpreted as maps t : F∗∗
5. They represent some word information that can be extracted from a
tuple. Valuations are interpreted as functions v : F∗∗
(cid:57) N. Valuations represent numerical information that
(cid:57) {0, 1}.
can be extracted from tuples. Conditions are predicates over F∗∗
5
Finally, permutations are, rather predictably, interpreted as (partial) permutations F∗∗
5 which will
be used to deﬁne IPPA.

5 , or equivalently maps q : F∗∗
(cid:57) F∗∗

(cid:57) F∗

5

5

5

5

Let us describe each type with more details. We are not going to try to give a formal deﬁnition of the
programming language, since it is would be unnecessarily complicated. It would involve a global induction
on the various types, starting from some basic objects and taking a closure under some inductive operations.
Instead, we will simply list the objects that we are actually going to use in the rest of the thesis. The proofs
that they are polynomially computable are often trivial and will be omitted in most cases.

Terms

5 is a term (understood as the constant function);

• Every word w ∈ F∗
• for all i ∈ N, the projection πi of the i’th ﬁeld is a term;
• if t is a term, then χ (t) is also a term (χ (t) (u) = χ (t(u)), for all u in F∗∗
5 );
• if v is a valuation and t is a term, then t|v is also a term, where t|v (u) := t(u)|v(u) . In other words,

t|v uses v as a pointer for t and it gives the letter at the v(u)’th position of t(u).

Valuations

• Every natural n ∈ N is a valuation, understood as a constant function;
• if t is a term, then |t| is a valuation;

26

• For all vectors k ∈ N∗ and i ∈ N, the function lk,i deﬁned in Fact 6 is a valuation.
• If S : N → N is a sequence of numbers and v a valuation, then Sv (where Sv(u) := Sv(u)) is also a
valuation. (In general, the complexity of this valuation depends on the complexity of S and it is not
polynomially computable if S is not.)

• Basic arithmetical operations (addition, subtraction, multiplication etc) of valuations are still valua-

tions.

In fact, we will need the following, more general version of the third bullet:
• For all valuations v, vector sequences k : N → NM and i ∈ N, lkv,i (where lkv,i(u) := lkv(u),i(u)) is
also a valuation. In this version, the vector whose structure lkv,i gives depends on the input letter. Of
course, if k is not a polynomially computable sequence, then neither is lkv,i.

A vector valuation is a collection v = (vi)0≤i≤M−1 of valuations, for some M ∈ N. Vector valuations

are used to obtain lengths of alphabets in a polynomially computable way.

Conditions

• If v1, v2 are valuations, then v1 ≥ v2 is a condition whose interpretation is clear;
• if t1, t2 are terms, then t1 = t2 is a condition;
• if t, t1 are terms and (Qw)w∈F∗

is a sequence of subsets of F∗

t1 ∈ Qt if t1(u) ∈ Qt(u).)

5

5, then t1 ∈ Qt is a condition. (u satisﬁes

is a condition (that is true for u if and only if

• if t is a term and i1, . . . , in are ﬁelds, then S t

i1,...,in

);

i1,...,in

u ∈ S t(u)
p(t) is a condition, where u satisﬁes Hv
within v(u) steps over term t(u);

• Hv

p(t) if and only if the TM deﬁned by program p does not stop

• boolean operations of conditions are also conditions.

Permutations

• For every condition q, Check[q] is a permutation. Check[q](u) is equal to u if and only if u satisﬁes q

(and is undeﬁned otherwise). This is an involution.

• For every valuation v and ﬁeld i ∈ N, incr[v, i] is a permutation deﬁned in the following way: Let
u ∈ F∗∗
i := (cid:104)γ(cid:105)|ui|(ui), where
γ(w) := w + 1 mod v(u) when w < v(u) (undeﬁned otherwise); then incr[v; i](u) := u(cid:48) if v(u) = v(u(cid:48))
(undeﬁned otherwise).

5 and deﬁne u(cid:48) in the following way: u(cid:48)

j := uj for all j (cid:54)= i, and u(cid:48)

Essentially incr[v; i] adds 1 modulo v(u) to the i’th ﬁeld of u. The additional complications are due
to the fact that we want this rule to always be reversible (which would not necessarily be true if v(u(cid:48))
is not equal to v(u)) and length preserving (which is the reason that we use the strange γ function).
• αU [t; Tape, Head−1, Head+1] is a permutation for every term t and ﬁelds Tape, Head−1, Head+1. We
direct the reader to Section 5.2 for the deﬁnition of this permutation, since it uses a permutation that
is deﬁned and examined therein.

27

• Let t be a term and i be a ﬁeld such that t does not depend on i. In other words, if u, u(cid:48) ∈ F∗∗

πj(u) = πj(u(cid:48)) for all j (cid:54)= i, then t(u) = t(u(cid:48)).
Then, W rite[t; i] is a permutation deﬁned as follows: Let u ∈ F∗∗
if (cid:105)ui(cid:104) = . In this case, all ﬁelds remain the same except for i which becomes equal to (cid:104)t(u)(cid:105)|ui|.

5 and

5 . W rite[t; i](u) is deﬁned if and only

Essentially, we check that the ﬁeld i is empty and then write t(u) on it, while preserving the lengths.
The condition that t does not depend on i is essential to ensure reversibility.
W rite[t; i]−1 ﬁrst checks that the i’th ﬁeld is equal to t(u) and then empties it, while preserving the
lengths. This is a way to reversibly erase some information from a letter, namely compare it with some
other place of the letter where the same information is held.

• For all ﬁelds i, i(cid:48), Swap[i, i(cid:48)] is a permutation deﬁned as follows: Let u ∈ F∗∗

5 . Swap[i, i(cid:48)](u) is deﬁned
if and only if |ui| = |ui(cid:48)|. In this case, all ﬁelds are unchanged except for i and i(cid:48) whose values are
exchanged.

This is a length-preserving involution.

• For every condition q and permutation α, if q then α is a permutation. On input u ∈ F∗∗

5 , it applies
α if condition q(u) is satisﬁed and q(u) = q(α(u)). If q(u) is satisﬁed and q(u) (cid:54)= q(α(u)), then it is
not deﬁned on u (this ensures reversibility). Finally, if q(u) is not satisﬁed, it is equal to the identity.
• The composition of permutations is also a permutation. In constructions, we will denote the composi-

tion α2 ◦ α1 by writing α2 below α1.

In the deﬁnition, we check that the values of the valuations, terms and conditions that are given as
parameters do not change. This is a technical point that ensures that they are interpreted as reversible
functions. In all our constructions, these conditions will easily be satisﬁed because the valuations, terms and
conditions will either be constant or depend on ﬁelds that are not modiﬁed by the rule at hand.

If we were giving a complete, formal description of a language, then this would be the point where by
a large, tedious induction we would prove that, given some natural conditions on the parameters, every
permutation of the language is polynomially computable, or, more precisely, polynomially computable in its
parameters (this means that its complexity is a polynomial of the complexity of its parameters) and that
short programs exist for the permutations. Namely, the size of the program is O(pt,v,...), where t, v etc. are
the parameters of the permutation.

We can also prove that the size of a program of a permutation is approximately the same as the size of

the program of its inverse.

4.2 Conventions about deﬁning IPPA

In the ﬁrst part of this chapter, we gave a short exposition of the programming language that will be used in
the rest of the thesis in order to deﬁne permutations of F∗∗
5 . However, in order to deﬁne a PPA, the number
of ﬁelds and the directions of the ﬁelds also have to be ﬁxed.
Recall that we want to deﬁne PPA, i.e., RPCA of the form F = σδ ◦ α, where δ ∈ {−1, 0, +1}M is the
shift vector and α is a partial permutation of A = A0 × . . .AM−1, for some M ∈ N. In our case, F will
always be the restriction of an IPPA, i.e., A will be equal to Fk
will be
the restriction of some (inﬁnite) permutation β deﬁned in the programming language.

5 , for some k ∈ NM and α := β|Fk

5

We will use the following conventions when constructing such PPA:
• We ﬁrst give a list of so-called explicit ﬁeld labels. Such a list will often be noted in the form C :=
e(cid:48)], where e, e(cid:48) ∈ {−1, 0, +1}. The subscripts e, . . . , e(cid:48) correspond to the directions
[Fielde, . . . , Field(cid:48)
of the ﬁelds (if the direction is equal to 0, then it will be omitted). The ﬁeld list is a tuple of pairwise
diﬀerent natural projections, that are used by the permutation, together with their directions, that
will be used by the shift. (The labels of the ﬁelds will make the permutations more understandable

28

than the corresponding indices i, i(cid:48), . . .). The ﬁeld list is not ﬁxed, so in fact for every ﬁeld list, we give
a diﬀerent permutation, even though they only diﬀer in the enumeration of the ﬁelds.
The permutation is assumed to reject any element of F∗∗
5 that does not involve all ﬁeld numbers in the
list, but note that it does not reject tuples that have more ﬁelds; the so-called anonymous ﬁelds, that
are not in the list, are not modiﬁed by the permutation (but they might be used by some other PPA
with which we compose). This allows us to deﬁne some simple PPA with few ﬁelds and then use them
as “building blocks” in order to build more complicated ones in the following sense: the complicated
PPA has more ﬁelds than the simple one, but, if it does not “touch” any of its ﬁelds, its behaviour on
those ﬁelds is described by the corresponding behaviour of the building block.
If C and C(cid:48) are two lists of ﬁeld labels, then C∪C(cid:48) is the list that contains the ﬁelds of C and C(cid:48). Usually,
the lists will be disjoint, so that we will use the notation C (cid:116) C(cid:48).

• After giving the ﬁeld list, we describe an (inﬁnite) permutation using the programming language deﬁned

in the ﬁrst part of this chapter.

• Then, we need to ﬁx M ∈ N and k ∈ NM . If we do not care about the existence of anonymous ﬁelds,
then we always assume that M is some number greater than or equal to the largest natural appearing
in the ﬁeld list C. In this way, we ensure that the conﬁgurations will not be rejected simply because
the program tries to access a ﬁeld that is not there.

When we do not want anonymous ﬁelds to exist (for example, when we want to achieve exactness of a
simulation), then we assume that the ﬁeld list C is equal to [0, . . . , M − 1] and we choose this M for
the number of ﬁelds.
In any case, after choosing M , we ﬁx some vector k ∈ NM satisfying some appropriate conditions
(which are case-speciﬁc).

• Finally, we need to deﬁne the directions of the ﬁelds. However this has already been done in the
deﬁnition of the ﬁeld list with the use of the subscripts e, e(cid:48) etc. The directions of the anonymous ﬁelds
can be anything. In fact, our statements will be true for all directions of the anonymous ﬁelds, since
we will not refer to them.

29

Chapter 5

The universal simulator

In this chapter, our aim is to construct an RPCA (a family of RPCA in fact, depending on some parameters)
that can simulate every other RPCA that satisﬁes some conditions. This is done in Lemma 33. This RPCA
is extremely helpful and it will be part of all our subsequent constructions. Since it is diﬃcult to overstress
the importance of this RPCA, we will give a step-by-step description of its construction with as many details
as possible.

In Section 5.1, we will embed a periodic rectangular grid in every conﬁguration. This is a standard
procedure in hierarchical constructions and it will allow us to partition every conﬁguration into colonies
and use the decoding function ˜Φ. In Section 5.2, we will make a slight digression and show how we can
simulate any TM with an RPCA in real-time. This is needed in order to preserve the expansiveness of the
horizontal direction. Then, in Section 5.3, we construct an RPCA to simulate an RPCA whose direction
vectors are null (all its ﬁelds are still). There are some tricks involved in this phase, mainly having to do
with deleting the previous simulated letter and synchronizing the computations. Then, in Section 5.4, we
construct an RPCA that can simulate any RPCA whose permutation is the identity i.e., any shift. Finally,
in Section 5.5, we construct the universal IPPA Simulate that can simulate any RPCA, when it is restricted
to the appropriate alphabet.

5.1 Imposing a periodic structure
Let CGrid = [Addr, Addr+1, Clock, Clock+1].

• Clock and Addr are meant to localize the cell in its macrocell, and they correspond to the projections

involved in the deﬁnition of explicit simulation in Section 3.4.

• Clock+1 and Addr+1 are used to communicate with the neighbour cells, so that consistency between

the Clock and Addr ﬁelds is achieved.

Grid[vMAddr, vMClock]

1: Check[πAddr+1 = πAddr and πClock+1 = πClock] {Check left-neighbour information coherence.}
2: incr[vMAddr; Addr+1] {Increment Addr+1 so that the right neighbour can check coherence.}
3: incr[vMClock; Clock] {Update Clock.}
4: incr[vMClock; Clock+1] {Update Clock+1.}

By the discussion of Chapter 4, we know that Grid[vMAddr, vMClock;CGrid] is polynomially computable with

respect to its parameters vMAddr and vMClock.

30

In practice, the two valuation parameters vMAddr and vMClock will be constant over the alphabet of the

PPA, in which case the behaviour will be described by the following:
Lemma 25. Let us ﬁx a ﬁeld list CGrid ∈ N4 and integers S, T ∈ N1.
Let F be the IPPA deﬁned by the permutation Grid[S, T ;CGrid] and directions νGrid given by the label indices,

and let k ∈ N∗ be a vector satisfying: (cid:26) kAddr, kAddr+1 ≥ (cid:107)S(cid:107)

Let c ∈ (Fk
Z
5 )
In this case, F (c) ∈ Σs,t+1 mod T,S,T .

. Then, c ∈ F −2((Fk
5 )

Z

kClock, kClock+1 ≥ (cid:107)T(cid:107) .

) if and only if there exist s ∈(cid:74)0, S(cid:74) and t ∈(cid:74)0, T(cid:74) such that c ∈ Σs,t,S,T .

In the previous statement, S and T should be understood as the width and height of the macrocells.
Notice, also, that the statement holds for all vectors k ∈ N∗ that satisfy the inequalities, which means that
there can be other ﬁelds in the alphabet. This means that if we use Grid together with other rules that do
not change the values of the ﬁelds in CGrid, the statement of the lemma will still be true.

The restrictions about the lengths of k ensure that ﬁelds are large enough that we can write the binary

not be deﬁned at cell n, F (c) would not exist, which is a contradiction.

representation of S and T on them.
Proof. We prove the stronger claim that if F 2(c) exists, then there exist 0 ≤ s < S and 0 ≤ t < T such that
for all n ∈ Z, πAddr(cn) = πAddr+1(cn) = s + n mod S and πClock(cn) = πClock+1(cn) = t.
Suppose that πAddr(cn) (cid:54)= πAddr+1(cn) or πClock(cn) (cid:54)= πClock+1 (cn), for some n ∈ Z. Then, line 1 would
Suppose, then, that there exists n ∈ Z with πAddr(cn+1) (cid:54)= πAddr(cn) + 1 mod S. Line 2 and the fact that
Addr+1 is a right-going ﬁeld imply that πAddr+1F (c)n+1 = πAddr(cn) + 1 mod S. Then, line 1 is not deﬁned
at cell n + 1 of F (c) since πAddr+1F (c)n+1 = πAddr(cn) + 1 mod S (cid:54)= πAddr(cn+1). Therefore F 2(c) does not
exist, which contradicts the hypothesis. Similarly, we can prove that cn.Clock = cn+1.Clock, for all n ∈ Z.
Thus, the stronger claim we made at the beginning of the proof is true.
If πAddr(c0) = s and πClock(c0) = t, then the previous claim implies that for all n ∈ Z, πAddr(cn) =
s + n mod S and πClock(cn) = t. Furthermore, since the value of Addr is not changed by F and the value
of Clock is increased by 1 mod T every time step by line 3, we have that πAddrF (c)n = s + n mod S and
πClockF (c)n = t + 1 mod T , for all n ∈ Z.

In general, when using IPPA, we have to use a similar rule every time we want to impose some horizontal
restriction on the conﬁguration. Namely, we have to use an additional right-moving (or left-moving, it does
not make a diﬀerence) ﬁeld, and then we need 2 steps in order to verify that the ﬁeld is constant.
All of the rules we construct will factor onto Grid[S, T ;CGrid], for some S, T ∈ N1. The following remark

will give the disjointness condition in the deﬁnition of simulation.
Remark 26. Assume that F : AZ (cid:57) AZ
Σs,t

F := H−1(Σs,t,S,T ). Then, the union(cid:70)

factors onto Grid[S, T ;CGrid] through the factor map H and let
0≤t<T
0≤s<S

F , for some s, t, then the union(cid:70)

Σs,t
F is disjoint and F (Σs,t

F ) ⊆ Σs,t+1 mod T

F tσs(D(Φ))

F

.

0≤t<T
0≤s<S

satisﬁes that D(Φ) ⊆ Σ0,0

Therefore, if Φ : AZ (cid:57) BZ

is disjoint.

Σs,t

F implicitly depends on the factor map H. However, in applications, H will be equal to πCGrid so that

no ambiguity arises by omitting it.

31

5.2 Simulating TM with IPPA

The IPPA Grid allows us to divide every conﬁguration into colonies with a periodical clock. We want to use
this space-time structure in order to do computations within the work-periods (the “time” between two
subsequent steps where the clock is 0). We are going to introduce the elements needed for this one by one,
since, hopefully, it will make some of the ideas more clear. First, let us show how to simulate TMs in real
time with PPA.
4, we construct an IPPA that simulates M in real-time. This subsection is

For all programs p = pM ∈ F∗
Let CU := [Tape, Head−1, Head+1].
The key item to maintaining reversibility, is to keep track of the history of the computation. Some kind
of archive of each past step is shifted in the direction opposite to the head, in order for the head to always
have space to write the new history. Recall the deﬁnition of the function U from Subsection 2.2.1. Let
γU [p] : (F∗

4)3 be deﬁned by the following transitions: (a, h−1, h+1) is mapped to

4)3 (cid:57) (F∗

inspired by [32].

• (a(cid:48), χ (a, q, δ) , χ (a, q, δ)), if (hδ, h−δ) = (q, ) (Headδ contains the TM head) and U(a, q, p) = (a(cid:48), , +1).
If Headδ contains a head and the transition is an accepting one, then we write an encoding of the last
transition on the Head ﬁelds, modify Tape and the TM heads disappear. Here, the assumption that
the TM head (which has disappeared) moves to the right is convenient to ensure injectivity.

• (a(cid:48), h(cid:48)

δ(cid:48), h(cid:48)

−1, h(cid:48)

+1), where (h(cid:48)

−δ(cid:48)) = (q(cid:48), χ (a, q, δ)) if (hδ, h−δ) = (q, ) and U(a, q, p) = (a(cid:48), , δ(cid:48)). If the
transition is not an accepting one, then Tape is modiﬁed, the TM head is written on the appropriate
Head ﬁeld and on the other Head ﬁeld we write an encoding of the transition and of the position of the
head before the transition.

• (a, h−1, h+1) if h−1, h+1 /∈ Qp \ {}. If none of the Head ﬁelds contains a TM head, then do nothing.
It is not diﬃcult (by a tedious case enumeration) to see that γU [p] is a partial permutation and polyno-
2 and χ (F4 × Qp × {−1, 1}) ⊂ 2F∗
mially computable, thanks in particular to the disjointness of Qp ⊂ F∗
3.
Basically, γU [p] identiﬁes the accepting state  with the absence of state (for which it just performs identity).
In other cases, it prevents from having two (non-accepting) head states at the same cell; then it applies the
transition rule and sends the new state to the correct direction (depending on δ), while sending an archive
of the last performed operation in the opposite direction. At the moment that the accepting state appears,
it just sends two (identical) archives in opposite directions (there is no new state to send).
4 × Q × Z of the machine M corresponding to program
Z

We say that c ∈ (F∗∗
Z
4 )

represents (z, q, j) ∈ F

p if:

• For all i ∈ Z, πTape(ci) = zi;
• (πHead−1(cj), πHead+1(cj)) ∈ {q} × {} ∪ {} × {q};
• For all i (cid:54)= j and δ ∈ {−1, +1}, πHeadδ (ci) /∈ Qp \ {}
• For all i (cid:54)= j and δ ∈ {−1, +1}, if πHeadδ (ci) (cid:54)=  then δ has the sign of j − i.

Intuitively, this means that in c there is at most one (non-accepting) head at position j, no head elsewhere,
and nothing (represented by , like the accepting state) in Head−1 on its right nor in Head+1 on its left. The
possible archives go away from the head position. We can thus see that the head will never move into a cell
where there is an archive, so that one of the transitions of γU [p] will always be applicable.

Formally, we have the following lemma about the behaviour of γU [p].
Lemma 27. Let us ﬁx a ﬁeld list CU ∈ N3 and a program p = pM ∈ F∗
4.
Consider the IPPA F deﬁned by permutation (cid:104)γU [p](cid:105) and directions νU given by the label indices.

Let k ∈ N∗ be a vector satisfying:(cid:26) kHead−1 , kHead+1 ≥ (cid:107)χ (F4 × Qp × {−1, +1})(cid:107)

kTape ≥ 1 .

32

Let c ∈ (Fk
Z
5 )
(cid:105)F t(c)(cid:104) represents Mt(z, q, j).

and suppose that (cid:105)c(cid:104) represents conﬁguration (z, q, j) ∈ F

4 × Q× Z of M. Then, for all t ∈ N,
Z

As in Lemma 25, the inequalities about the lengths of k simply state that the ﬁelds are long enough.
Using Lemma 7, we will omit the (cid:104)·(cid:105) and (cid:105)·(cid:104) from (cid:104)γU [p](cid:105) and (cid:105)c(cid:104) in the following proof, since they are only
used to make F have constant lengths.

Proof. We will prove the claim for t = 1; the general claim then follows by induction.
Suppose, ﬁrst, that M(z, q, j) does not exist. This means that δM(zj, q) does not exist, or equivalently,
that U(zj, q, p) = U(cj.Tape, q, p) does not exist. From the deﬁnition of γU [p] and the fact that c represents
(z, q, j), we have that γU [p](cj) does not exist, which implies that F (c) does not exist.
Suppose, then, that M(z, q, j) = (z(cid:48), q(cid:48), j(cid:48)) exists. This means that U(zj, q, p) = (z(cid:48)
i = zi
for any i (cid:54)= j. By assumption, for any i ∈ Z, (πTape(ci), πHead−1(ci), πHead+1 (ci)) = (zi, h−1,i, h+1,i) for some
h−1,i, h+1,i ∈ Qp ∪ χ (F4 × Qp × {−1, +1}) ∪ {}.
Moreover, for any i (cid:54)= j, and δ ∈ {−1, +1}, hδ,i /∈ Qp, so the identity rule is applied. After applying the
shifts, it gives that for any i < j − 1,

j, q(cid:48), j(cid:48)−j), and z(cid:48)

(πTapeF (ci), πHead−1F (ci), πHead+1F (ci)) = (zi, h−1,i+1, h+1,i−1) = (z(cid:48)

i, h−1,i+1, )

with h−1,i+1 /∈ Qp, and for any i > j + 1,

(πTapeF (ci), πHead−1F (ci), πHead+1F (ci)) = (zi, h−1,i+1, h+1,i−1) = (z(cid:48)

i, , h+1,i−1)

Now, assume (h−1,i, h+1,i) = (q, ) and that U(zj, q, p) = (z(cid:48)

with h+1,i−1 /∈ Q.
a similar way). Then the transition (zj, q, ) → (z(cid:48)
of γU [p] and the shifts, we obtain

j, q(cid:48),−1) (the other cases can be dealt with in
j, q(cid:48), χ (zj, q,−1)) is applied by γU [p]. After the application

(πTapeF (cj), πHead−1F (cj), πHead+1FU [p](cj)) = (z(cid:48)
(πTapeF (cj−1), πHead−1 F (cj−1), πHead+1F (cj−1)) = (z(cid:48)
(πTapeF (cj+1), πHead−1F (cj+1), πHead+1F (cj+1)) = (z(cid:48)

j, , ),
j−1, q(cid:48), ) and,
j+1, , χ (zj, q,−1)).

All conditions are hence satisﬁed for F (c) to represent M(z, q, j).

Note that due to the parallel nature of IPPA, some conﬁgurations may involve several machine heads,
and valid simulations may take place in parallel, provided that there is enough space between them so that
the archive and the heads do not collide. For this reason, we need to give a “ﬁnite version” of the previous
lemma.
Lemma 28. Let us ﬁx a ﬁeld list CU ∈ N3 and a program p = pM ∈ F∗
4.
Consider the IPPA F deﬁned by permutation (cid:104)γU [p](cid:105) and directions νU given by the label indices. Let k ∈ N∗
be a vector satisfying:

kHead−1, kHead+1 ≥ (cid:107)χ (F4 × Qp × {−1, +1})(cid:107)
kTape ≥ 1.

Let c ∈ (Fk
5 )

Z

, c(cid:48) = (cid:105)c(cid:104) and assume that there exists n ∈ N such that the set

J :=(cid:8) j ∈ Z(cid:12)(cid:12) (πHead−1(c(cid:48)

j)) (cid:54)= (, )(cid:9)

j), πHead+1(c(cid:48)

satisﬁes that for any j (cid:54)= j(cid:48) ∈ J, we have |j(cid:48) − j| > 2n, and that for all j ∈ J, (c(cid:48)
Qp ×{}∪{}× Qp. For j ∈ J, let qj := c(cid:48)
j.Head−1, c(cid:48)
if (c(cid:48)
Then, F n(c) exists if and only if (zj, q(cid:48)

j.Head−1 if (c(cid:48)
j, j(cid:48)) := Mn(c(cid:48).Tape, qj, j) exists, for all j ∈ J. In addition:

j.Head+1) ∈ Qp ×{} and qj := c(cid:48)

j.Head+1) ∈ {} × Qp.

j.Head−1, c(cid:48)

j.Head−1, c(cid:48)

j.Head+1) ∈
j.Head+1

33

• πTapeF n(c)(cid:74)j−n,j+n(cid:75) = zj(cid:74)j−n,j+n(cid:75), for all j ∈ J;
• πTapeF n(c)i = ci if i /∈ J +(cid:74)−n, n(cid:75);
• (πHead−1 F n(c)j(cid:48), πHead+1 F n(c)j(cid:48)) ∈ {(q(cid:48)
• (πHead−1 F n(c)i, πHead+1F n(c)i) /∈ Qp × {} ∪ {} × Qp, if i /∈ { j(cid:48)| j ∈ J}
outside J +(cid:74)−t, t(cid:75) at time t ∈ N (because F has radius 1) and initially the heads are only in the positions

Proof. First note that the identity is always applied when the head is absent; in particular it is applied

j}, for all j ∈ J;

j, )} ∪ {(, q(cid:48)

in J.
According to the assumptions, for all j ∈ J, c(cid:48)j is obtained by turning all (ci.Head−1, ci.Head+1) to (, )
except at position j represents (c(cid:48).Tape, qj, j). Thanks to Lemma 27, for all 0 ≤ t ≤ n, F t(c(cid:48)j) exists if and
only if Mn(c(cid:48).Tape, qj, j) exists.

In that case, since c(cid:48)j coincides with c(cid:48) over interval(cid:74)j − 2n, j + 2n(cid:75) and since the radius is 1, a simple
induction can show that F t(c(cid:48)j) coincides with F t(c(cid:48)) over interval(cid:74)j − 2n + t, j + 2n − t(cid:75). Lemma 27 hence
exists, and by Lemma 27 involves a unique (non-accepting) head, in some cell j(cid:48) ∈(cid:74)j − t, j + t(cid:75). Therefore,

Conversely, suppose that F t(c(cid:48)j) is undeﬁned for some j ∈ J with t ≤ n minimal. Then, F t−1(c(cid:48)j)
γU [p](F t−1(c(cid:48)j)i is deﬁned for any i (cid:54)= j(cid:48). This means that γU [p](F t−1(c(cid:48)j)j(cid:48)) is undeﬁned; we have already
noted that this is equal to γU [p](F t−1(c(cid:48))j(cid:48)), which proves that F t(c(cid:48)) is undeﬁned.

gives the main claim.

Lemma 28 will be used in the following way: Every conﬁguration will be divided into colonies by Grid.
Initially (when the clock is equal to 0), inside every colony there will be exactly one TM head at the leftmost
cell of the colony. These TM will perform some computation for a small amount of time compared to the
the width of the colonies (the S of Lemma 25) so that the heads will not meet. Lemma 28 will immediately
imply that at the end of the computation, in every colony, Tape contains the output of the computation.
Finally, the output of the computation will be copied onto some new ﬁeld and then the computation will be
run backwards (remember that γU [p] is a permutation).
Let u ∈ F∗∗

We are now ready to give the details of the deﬁnition of the permutation αU [t; Tape, Head−1, Head+1]:

5 and deﬁne u(cid:48) in the following way:

(u(cid:48).Tape, u(cid:48).Head−1, u(cid:48).Head+1) = (cid:104)γU [t(u)](cid:105)(u.Tape, u.Head−1, u.Head+1),

and u(cid:48)

j := uj for all j /∈ {Tape, Head−1, Head+1}. Then,

αU [t; Tape, Head−1, Head+1](u) := u(cid:48),

if t(u) = t(u(cid:48)) (and it is undeﬁned otherwise.).

Again, the deﬁnition gets a little more complicated due to the need to preserve the lengths, to have
arbitrarily many ﬁelds and to ensure reversibility. When t = πProg, where Prog is a new ﬁeld, then the
condition t(u) = t(u(cid:48)) is always satisﬁed.

5.3 Computing the simulated permutation
Let CCompute = CU (cid:116) [NTape].

• Head−1, Head+1 are used by to simulate a TM with the rule of Subsection 5.2.
• The output of this computation is written on NTape and then the computation is reversed (the Bennett

trick, see [3]).

34

Compute[vAddr, vClock, vAlarm, tProg, tRProg]

1: if vClock = 0 and vAddr = 0 then
2: W rite[0; Head−1] {Write the machine initial state in the left head ﬁeld.}
3: end if
4: if 0 ≤ vClock < vAlarm then

(cid:10)γU [tProg; Tape, Head−1, Head+1](cid:11) {Run the machine in order to compute the permutation.}

5:
6: else if vClock = vAlarm then
7:
8: W rite[Tape; NTape] {Copy the output onto a diﬀerent tape.}
10: else if vAlarm < vClock ≤ 2vAlarm then

Check[πHead−1 , πHead+1 /∈ QtProg \ {}] {Check that the computation halted.}
Swap[Head−1, Head+1] {The directions of the ﬁelds of FU [p]−1 are opposite to those of FU [p].}

(cid:10)γU [tProg; Tape, Head+1, Head−1]−1(cid:11) {Unwind the computation in order to delete the archive.}
(cid:10)γU [tRProg; NTape, Head−1, Head+1](cid:11) {Compute the inverse of the permutation, in order to recover

11:
12: end if
13: if 2vAlarm ≤ vClock < 3vAlarm then

14:

9:

15: else if vClock = 3vAlarm then
16:
17: W rite[Tape; NTape]−1 {Empty NTape.}
19: else if 3vAlarm < vClock ≤ 4vAlarm then

Tape.}
Check[πHead−1, πHead+1 /∈ QtRProg \ {}] {Check that the computation halted.}
Swap[Head−1, Head+1] {Reverse the directions again.}

(cid:10)γU [tRProg; Tape, Head+1, Head−1]−1(cid:11) {Unwind the second computation, too.}

18:

20:
21: end if
22: if vClock = 4vAlarm and vAddr = 0 then
23: W rite[0; Head−1]−1 {Erase the machine initial state.}
24: end if

Compute[vAddr, vClock, vAlarm, vProg, vRProg;CCompute] is polynomially computable with respect to its param-

eters.

Note that, depending on the value of vAddr, only a small amount of these permutation are applied.
In applications, the three parameters vAlarm, tProg, tRProg will be constant. vAlarm contains a natural number
that controls how long the computation lasts. tProg and tRProg are interpreted as the program and the reverse
program (i.e., the program of the inverse IPPA) of the IPPA that we want to simulate.

We are now able to simulate uniformly RPCA with “radius 0” (null direction vector).

Lemma 29. Let us ﬁx a ﬁeld list CGrid (cid:116) CCompute ∈ N8, vectors k, k(cid:48) ∈ N∗, integers S, T ∈ N1, t0, U ∈ N
and programs p, p−1 ∈ F∗
5 and its inverse α−1, respectively and let
G the IPPA corresponding to permutation α and null direction vector.

4 of a partial permutation α : F∗∗

(cid:57) F∗∗

5

Consider the IPPA F with directions νGrid∪Compute, and permutation

Grid[S, T ] ◦ Compute[πAddr, πClock − t0, U, p, p−1] ,

and assume that the following inequalities hold:



5 )}

Fk(cid:48)

(cid:13)(cid:13)(cid:13)χ

(cid:17)(cid:13)(cid:13)(cid:13)}

(cid:16)
U ≥ max{tp(Fk(cid:48)
5 ), tp−1 (Fk(cid:48)
S ≥ max{2t0,
T ≥ 4U + t0
kAddr, kAddr+1 ≥ (cid:107)S(cid:107)
kClock, kClock+1 ≥ (cid:107)T(cid:107)
kTape, kNTape ≥ 1.

5

kHead−1, kHead+1 ≥(cid:12)(cid:12)χ(cid:0)F4 × (Qp ∪ Qp−1) × {−1, +1}(cid:1)(cid:12)(cid:12)

35

(cid:23)

S,T,0,Φ

G|(Fk(cid:48)

Then, F|(Fk
5 )Z
˜Φ−1
Tape((Fk(cid:48)
Z
).
5 )
The number t0 should be understood as a delay before which to apply this rule, and U as the maximal

5 )Z , where Φ := ˜ΦTape|Σ and Σ := (Fk
5 )

time that we allow to the (forward and backward) computation.

Proof. Remarks 24 and 26 imply that Φ is surjective, ΦσS = σΦ and that D(Φ) =(cid:70)

NTape,Head−1,Head+1

Z ∩ Σ0,0,S,T ∩ S 

∩

F tσs(D(Φ)) is a

0≤t<T
0≤s<S

disjoint union.

Therefore, in order to prove the simulation we only have to prove that GΦ = ΦF T . This is equivalent
(since we are talking about partial functions) to the facts that if Φ(c) = b, then F T (c) exists if and only if
G(b) exists, and in that case ΦF T (c) = G(b).

We are actually going to prove the following stronger:
∩ ˜Φ−1
NTape,Head−1,Head+1

Fact 30. If c ∈ (Fk
5 )
and in that case F 4U (c) ∈ Σ0,t0+4U,S,T ∩ S 

Z ∩ Σ0,t0,S,T ∩S 

NTape,Head−1,Head+1

∩ ˜Φ−1

Tape(G(b)).

Tape(b), then F 4U (c) exists if and only if G(b) exists,

ΦF T (c) = G(b), which concludes the proof of the lemma.
Z ∩ Σ0,t0,S,T ∩ S 

Since the only rule applied outside t0 ≤ Clock ≤ t0 + 4U is Grid, Fact 30 implies that if Φ(c) = b, then
For the rest of the proof, let c ∈ (Fk
5 )
Suppose, ﬁrst, that F 4U (c) exists.
• Clock = t0: Initially, c ∈ S 
leftmost cell of every colony.

. Line 2 writes the initial head of the TM on Head−1 of the

NTape,Head−1,Head+1

NTape,Head−1,Head+1

∩ ˜Φ−1

Tape(b).

• t0 ≤ Clock < t0 + U : Only the permutation of line 5 is applied. Together with the directions of

νCompute, this implies that we apply U steps of the IPPA FU [p] on conﬁguration
(πTape(c), πHead−1(c), πHead+1(c)). This conﬁguration has a starting TM head at the leftmost cell of every
colony, and since c ∈ Φ−1(b), we have that in the i’th colony the input of the TM is χ (bi).

• t0 = t0 + U : Line 7 checks that in no place of the tape does there appear a head of the TM deﬁned by
p. This means that all the TM have accepted within the ﬁrst U steps and since p is the program of α,
we take that α(bi) exists, for all i ∈ Z, or equivalently that G(b) exists.

Therefore, if F 4U (c) exists, then G(b) exists.
For the other direction, suppose that G(b) exists, or, equivalently, that α(bi) exists, for all i ∈ N.
• Clock = t0: By assumption, c ∈ S 

.Line 2 writes the initial state of the TM on the

NTape,Head−1,Head+1

Head−1 of the leftmost cell of every colony.

• t0 ≤ Clock < t0 + U : Only the permutation of line 5 is applied. Together with the directions of

νCompute, this implies that at each step, we apply the IPPA FU [p] on the conﬁguration
(πTape(c), πHead−1(c), πHead+1(c)). There is a TM head at the leftmost position of every colony and the
i ) = bi, for all i ∈ Z.
input in the i’th colony is equal to χ (bi). In other words, ˜φTape(Bc
• Clock = t0 + U Since α(bi) is deﬁned for all i ∈ Z, U ≥ tp(Fk(cid:48)

5 ) and S ≥ 2U , we can see that the
conditions of Corollary 28 are satisﬁed with n = U . This means that the computation of the TM in
every colony has accepted and that the output of the computation is written on the Tape of every
colony. In other words, ˜φTape(BF U (c)
The check of line 7 is true, since by assumption all of the TM have accepted before Clock = U , and
when a TM halts its head disappears. Line 8 copies the contents of Tape on NTape. Therefore, after
the application of line 8, we have that ˜φNTape(BF U (c)
Finally, line 9 swaps the ﬁelds Head−1 and Head+1. This can be thought of as “reversing” the directions
of these ﬁelds. We do this because we want to reverse the computation done by FU [p] and in order to

achieve this, it is not enough to apply(cid:10)γU [p]−1(cid:11), but we also need to use directions −νCompute.

) = α(bi), for all i ∈ Z.

) = α(bi), for all i ∈ Z.

i

i

36

• t0 + U + 1 ≤ Clock ≤ t0 + 2U : Only the permutation of line 11 is applied. Together with the fact that
the shift directions have been reversed and that the ﬁelds Head−1, Head+1 have also been exchanged
in the rules of lines 5 and 11, this implies that the IPPA (FU [p])−1 is applied for U time steps on the
conﬁguration (FU [p])U (πTape(c), πHead−1 (c), πHead+1 (c)).
Therefore, (πTapeF 2U (c), πHead−1F 2U (c), πHead+1 F 2R(c)) = (πTape(c), πHead−1(c), πHead+1(c)).
In other words, the computation has been “run backwards” until the beginning, but the output of
the computation is on NTape. This is the trick used by Bennet in [3] to simulate arbitrary TM with
reversible ones.
At this point, F 2U (c) ∈ S 
contains the initial state 0, and ﬁnally, φTape(BF 2U (c)

, Head−1 is empty except at the left-most cell of every colony, where it
) = α(bi), for all i ∈ Z.
• t0 + 2U ≤ Clock < t0 + 3U : Only the permutation of line 14 is applied. Together with the directions

of νCompute, this implies that at each step, we apply the IPPA FU [p−1] on the conﬁguration
(πNTape(c), πHead−1(c), πHead+1(c)). Notice that we use NTape as the TM tape and we use the program
p−1 = tRProg.

) = bi and φNTape(BF 2U (c)

Head+1

i

i

• Clock = t0 + 3U : Since α−1(α(bi)) is deﬁned for all i ∈ Z, U > tp−1(Fk(cid:48)
of Corollary 28 are satisﬁed with n = U . This implies that φNTape(BF 3U (c)
The check of line 16 is true, since all of the TM have accepted before Clock = 3U . Line 17 copies the
contents of Tape on NTape. Since, at this point these ﬁelds are equal in every cell, this is equivalent
to emptying ﬁelds NTape (in a reversible way, though). Therefore, after applying this permutation,
F 3U (c) ∈ S 
We still have to empty the Head ﬁelds, too. For this, we have to run the computation backwards.
Line 18 swaps the ﬁelds Head−1 and Head+1, “reversing” the directions of these ﬁelds.

5 ) and S ≥ 2U , the conditions

) = bi, for all i ∈ Z.

NTape.

i

• t0 + 3U + 1 ≤ Clock ≤ t0 + 4U : Only the permutation of line 20 is applied. Together with the
fact that the shift directions have been reversed and that the head ﬁelds inside the rules are also
exchanged, this implies that the IPPA FU [p−1]−1 is applied for U time steps on the conﬁguration
FU [p−1]3U (πTape(c), πHead−1(c), πHead+1(c)).
Therefore, (πTapeF 4U (c), πHead−1F 4U (c), πHead+1F 4U (c)) = (πTapeF 2U (c), πHead−1F 2U (c), πHead+1F 2U (c)).
Notice that now we are using Tape as the tape of the TM, while during the forward computation we
used NTape. This is not a problem, though, because the two ﬁelds were equal at the end of the forward
computation at step 3U .
) = α(bi), for all i ∈ Z. Also, in the Head ﬁelds, there exists
At this point, we have that φTape(BF 4U (c)
the initial state 0 of the TM on Head−1 of the leftmost cell of every colony, while the rest of them are
empty.

i

NTape,Head−1,Head+1

Tape(α(b)), which ﬁnishes the proof of the lemma.

• Finally, line 23 deletes the initial state, and we get that F 4U (c) ∈ S 
Therefore, we have proved that if G(b) exists, then F 4U (c) exists and F 4U (c) ∈ (Fk
5 )
S 
Head−1,Head+1,NTape ∩ ˜Φ−1
In a nutshell, this is how the construction works. First, use the program p to compute α. At the end
of this phase, Tape contains α(b) (in the colonies). Copy α(b) onto NTape and in the second phase, run the
computation backwards so as to erase all auxiliary information written by the TM during the computation.
At the end of the second phase, Tape contains b and NTape contains α(b). In the third and fourth phases of
the construction, perform the reverse of what was done in the ﬁrst two phases, while exchanging the roles of
NTape and Tape. First, use p−1 with tape ﬁeld NTape so as to compute α−1(α(b)) = b, then copy Tape onto
NTape (thus emptying NTape) and then perform the computation backwards. At the end, NTape is again
empty and Tape contains α(b) and everything was done in a reversible way.

.
Z ∩ Σ0,t0+4U,S,T ∩

37

Notice for all b ∈ (Fk(cid:48)

5 ) and all c ∈ Φ−1(b), the values of the ﬁelds in CGrid (cid:116) CCompute of c are uniquely
determined. This implies that if there are no anonymous ﬁelds, or if the values of the anonymous ﬁelds were
determined by the ﬁelds of CGrid (cid:116) CCompute, then the simulation is also exact.

5.4 Shifting
Let CM acroShif t := [Tape, Tape−1, Tape+1].

Tape+1 and Tape−1 are used to exchange the information of Tape between colonies.
In the following algorithm, M ∈ N1 has to be thought of as the number of ﬁelds in the simulated alphabet,
ν ∈ {−1, 0, +1}M as the vector of directions of the simulated IPPA, and k(cid:48) : F∗∗
(cid:57) NM is a vector valuation
that gives the lengths of the alphabet of the simulated IPPA. k(cid:48) represents the ﬁeld lengths of the simulated
letters, whose information is then “known” to all the letters of the simulating PPCA.

5

M acroShif t[M, ν, k(cid:48), vAddr, vClock, vMAddr]

for 0 ≤ i < M do

1: if vClock = 0 or vClock = vMAddr then
2:
3:
4:

if lk(cid:48),i ≤ vAddr < lk(cid:48),i+1 then

Swap[Tape, Tapeνi
vMAddr steps.}

] {Letters are moved to the corresponding moving ﬁelds, and back after

end if
end for

5:
6:
7: end if

This is polynomially computable in the parameters.

Lemma 31. Let us ﬁx a ﬁeld list CGrid (cid:116) CM acroShif t ∈ N7, an integer M ∈ N1, a direction vector ν ∈
{−1, 0, +1}M , a vector k(cid:48) ∈ NM , a vector k ∈ N∗ and integers S, T ∈ N1, t0 ∈ N.

Consider the IPPA F deﬁned by directions νGrid(cid:116)Compute, given by the label indices, and permutation

Grid[S, T ] ◦ M acroShif t[M, νGrid(cid:116)Compute, k(cid:48), πAddr, πClock − t0, S],

and assume that the following inequalities hold:

Then F|(Fk

5 )Z (cid:23)

S,T,0,Φ

σ−ν|(Fk(cid:48)

that D(Φ) :=(cid:70)

0≤t<T
0≤s<S
deﬁned for all b ∈ (Fk(cid:48)
5 )

Recall that, by convention, the directions of the ﬁelds are the opposite of the shift that is actually applied.

Proof. Again, by the deﬁnition of Φ and Remarks 24 and 26, we know that Φ is surjective, ΦσS = σΦ and

F tσs(D(Φ)) is a disjoint union.

Therefore, we only have to show that σ−νΦ = ΦF T , which is equivalent to showing, since σ−ν(b) is

Z

, that if Φ(c) = b, then ΦF T (c) = σ−ν(b).

As in the proof of Lemma 29, we are going to prove the following stronger

Fact 32. If c ∈ (Fk
5 )
˜Φ−1(σ−ν(b)).

Z ∩ Σ0,t0,S,T ∩ S 

Tape−1,Tape+1

∩ ˜Φ−1

Tape(b), then F S(c) ∈ Σ0,t0+S,S,T ∩ S 

Tape−1=Tape+1

∩

38



S ≥(cid:13)(cid:13)(cid:13)χ

(cid:16)

(cid:17)(cid:13)(cid:13)(cid:13)

5

Fk(cid:48)
T ≥ t0 + S
kAddr, kAddr+1 ≥ (cid:107)S(cid:107)
kClock, kClock+1 ≥ (cid:107)T(cid:107)
, kTape+1
kTape, kTape−1
5 )Z , where Φ := ˜Φ|Σ and Σ := (Fk
5 )

≥ 1 .
Z ∩ Σ0,0,S,T ∩ S 

Tape−1,Tape+1

∩ ˜Φ−1((Fk(cid:48)
Z
5 )

).

• Clock = t0: By assumption, c ∈ S 

Tape−1,Tape+1

.

Lines 3 and 4 copy the encodings of the ﬁelds of b on the correct Tape, in the following sense:
Since c ∈ Φ−1(b), πTape(Bc
letter in Tape to Tapeνj
ﬁeld of bj onto Tape+1 if j is a right-moving ﬁeld and to Tape−1 if j is a left-moving ﬁeld.
This means that after the application of line 4, we have that
πTapeνj
all i ∈ Z and 0 ≤ j < M .

i )|(cid:74)lk(cid:48) ,j ,lk(cid:48) ,j+1(cid:74) = χ (πj(bi)), for all i ∈ Z and 0 ≤ j < M . Line 4 moves the
i )|(cid:74)lk(cid:48),j ,lk(cid:48) ,j+1(cid:74) = , for

i )|(cid:74)lk(cid:48) ,j ,lk(cid:48) ,j+1(cid:74) = χ (πj(bi)), while πTape−νj

when lv,j ≤ Addr < lv,j+1, or in other words, moves the encoding of the j’th

i )|(cid:74)lk(cid:48),j ,lk(cid:48) ,j+1(cid:74) = πTape(Bc

(Bc

(Bc

• t0 ≤ Clock < t0 + S: No permutation is applied during these time steps. Only the Tape ﬁelds are

shifted to the corresponding direction.

• t0 = t0 + S: Every symbol that was part of the encoding of the j’th ﬁeld of b has travelled S steps
to the direction indicated by νj. This means that before the application of line 4, we have that
πTapeνi
πTape−νj
Line 4 moves the letter from the Tape ﬁelds back to Tape. Therefore, after the application of line 4,
we have that F S(c) ∈ S 
and Φ(F S(c)) = σ−ν(b), which concludes the proof of the Lemma.

)|(cid:74)lk(cid:48),j ,lk(cid:48) ,j+1(cid:74) = χ (πi(bi−νi)), while
)|(cid:74)lk(cid:48) ,j ,lk(cid:48),j+1(cid:74) = πTape(BF S (c)

)|(cid:74)lk(cid:48) ,j ,lk(cid:48) ,j+1(cid:74) = , for all i ∈ Z and 0 ≤ j < M .

(BF S (c)
(BF S (c)

n

i

i

Tape−1,Tape+1

In this proof, it is of great importance that all the letters of b have the same lengths, because this implies
that the j’th ﬁeld of every letter of b is encoded at the same positions inside every colony. In fact, the reason
that we deal only with alphabets of constant lengths is that this shifting procedure can work so easily.

5.5 Simulating any ﬁxed rule

In this subsection, we will use Lemma 29 and 31 to construct an IPPA that can simulate non-trivially any
PCA, when restricted to an appropriate ﬁnite subalphabet.

Let CSimulate := CCompute ∪ CM acroShif t ∈ N6 (CCompute and CM acroShif t share the ﬁeld Tape).

Simulate[M, ν, k(cid:48), vAddr, vClock, vMAddr, vAlarm, tProg, tRProg]

1: Compute[vAddr, vClock, vAlarm, tProg, tRProg]
2: M acroShif t[M, ν, vAddr, vClock − 4vAlarm, vMAddr, k(cid:48)]

This is easily seen to be polynomially computable in the parameters.
Notice that M acroShif t and Compute are used at “diﬀerent time steps”, i.e., at diﬀerent values of
vClock. Compute starts being used when vClock = 0 and, by deﬁnition of vCompute, is equal to the identity
when vClock ≥ 4vAlarm, while M acroShif t starts being used when vClock = 4vAlarm (it has a delay of 4vAlarm).
Formally, this means that for every value of vClock, at most one of the rules Compute and M acroShif t is
not equal to the identity.

The following lemma is the fruit of all our eﬀorts until now. It provides an IPPA that can simulate any

PCA when restricted to a suﬃciently large alphabet.
Lemma 33. Let us ﬁx a ﬁeld list CSimulate (cid:116) CGrid ∈ N10, an integer M ∈ N1, programs p, p−1 ∈ F∗
partial permutation α : F∗∗
vector k(cid:48) ∈ NM , a vector k ∈ N∗ and integers S, T, t0, U ∈ N.

2 of a
5 and its inverse α−1, respectively, a direction vector ν ∈ {−1, 0, +1}M , a

(cid:57) F∗∗

5

39

Let G = σ−να and F be the IPPA deﬁned by directions νGrid(cid:116)Simulate, given by the label indices, and

permutation

Grid[S, T ] ◦ Simulate[M, νSimulate, k(cid:48), πAddr, πClock − t0, S, T, U, p, p−1] ,

and assume that the following inequalities hold:



5 )}

(cid:13)(cid:13)(cid:13)χ

(cid:17)(cid:13)(cid:13)(cid:13)}

(cid:16)
U ≥ max{tp(Fk(cid:48)
5 ), tp−1 (Fk(cid:48)
S ≥ max{2U,
T ≥ 4U + S + t0
kAddr, kAddr+1 ≥ (cid:107)S(cid:107)
kClock, kClock+1 ≥ (cid:107)T(cid:107)

Fk(cid:48)

5

kHead−1, kHead+1 ≥(cid:12)(cid:12)χ(cid:0)F4 × (Qp ∪ Qp−1) × {−1, +1}(cid:1)(cid:12)(cid:12)

kTape, kNTape, kTape−1
5 )Z completely, where Φ := ˜ΦTape|Σ and

, kTape+1

G|(Fk(cid:48)

≥ 1.

Then, F|(Fk

5 )Z (cid:23)

S,T,0,Φ

Σ := (Fk
5 )

Z ∩ Σ0,t0,S,T ∩ S 

Head−1,Head+1,Tape−1,Tape+1,NTape ∩ ˜Φ−1

Tape((Fk(cid:48)
5 )

Z

).

Proof. Notice that line 1 together with Grid make up the permutation whose behavior is described in
Lemma 29, while line 2 and Grid make up the permutation of Lemma 31. Also, as already noted, for any
value of Clock, at most one permutation of lines 1 and 2 is not equal to the identity.

Like in the proofs of Lemmas 29 and 31, we can easily see that we only have to show that if Φ(c) = b,

then ΦF T (c) = G(b), and that this follows from the following stronger fact:
Fact 34. If c ∈ (Fk
5 )
only if G(b) exists, and in that case F 4U +S(c) ∈ Σ0,t0+4U +S,S,T ∩ S 

Z ∩ Σ0,t0,S,T ∩ S 

NTape,Head−1,Head+1,Tape−1,Tape+1

∩ ˜Φ−1

Z
Tape(b)

, then F 4U +S(c) exists if and

NTape,Head−1,Head+1

∩ ˜Φ−1

Tape(G(b)).

Indeed, according to Fact 30, we have that F 4U (c) exists if and only if α(b) exits, or, equivalently, if and

only if G(b) exists, and in this case

F 4U (c) ∈ Σ0,t0+4U,S,T ∩ S 

Head−1,Head+1,Tape−1,Tape+1,NTape ∩ ˜Φ−1

Tape(α(b)).
Head−1,Head+1,Tape−1,Tape+1,NTape ∩ ˜Φ−1

Tape(σ−να(b)), which

Fact 32 implies that F 4U +S(c) ∈ Σ0,t0+4U +S,S,T ∩S 
concludes the proof of the lemma, since G = σ−να(b).

Simulate is a rule (family of rules in fact, since they depend on parameters) that can simulate any PPA.
Every IPPA F that we will construct later will factor onto Simulate. They might have some additional ﬁelds
for which we apply a diﬀerent rule, and this rule might even take into consideration the ﬁelds of CSimulate, but
none of these other rules is going to change the ﬁelds of CSimulate. Therefore, by projecting onto CSimulate,
we will immediately obtain that F simulates G, even though the simulation might not be exact.
CGrid (cid:116) CSimulate are uniquely determined by Φ.

However, if k does not have any anonymous ﬁelds, then the simulation is exact, since all the ﬁelds of

5.5.1 Satisfying the inequalities

Lemma 33 is true only under the assumption that the following set of inequalities are satisﬁed:

40



5 )}

(cid:13)(cid:13)(cid:13)χ

(cid:17)(cid:13)(cid:13)(cid:13)}

(cid:16)
U ≥ max{tp(Fk(cid:48)
5 ), tp−1(Fk(cid:48)
S ≥ max{2U,
T ≥ 4U + S + t0
kAddr, kAddr+1 ≥ (cid:107)S(cid:107)
kClock, kClock+1 ≥ (cid:107)T(cid:107)

Fk(cid:48)

5

kHead−1, kHead+1 ≥(cid:12)(cid:12)χ(cid:0)F4 × (Qp ∪ Qp−1 ) × {−1, +1}(cid:1)(cid:12)(cid:12)

kTape, kNTape, kTape−1

, kTape+1

≥ 1 .

We will denote this set of inequalities by I(k, k(cid:48), S, T, U, t0, p, p−1). When t0 = 0, i.e., when the com-
putation starts at Clock = 0, we will omit it and write I(k, k(cid:48), S, T, U, p), instead. Let us explain again
intuitively why each inequality is needed:

• kAddr, kAddr+1 ≥ (cid:107)S(cid:107): The ﬁelds kAddr and kAddr+1 have to be large enough so that we can write the

binary representation of S in them.

• kClock, kClock+1 ≥ (cid:107)T(cid:107): The ﬁelds kClock and kClock+1 have to be large enough so that we can write the

binary representation of T in them.

• U ≥ max{tp(Fk(cid:48)

5 ), tp−1 (Fk(cid:48)
the encoded letters halts.

(cid:16)

(cid:13)(cid:13)(cid:13)χ

Fk(cid:48)

5

5 )}: We have to run the TM long enough so that the computation of p onto

(cid:17)(cid:13)(cid:13)(cid:13)}: The colonies have to be wide enough so that we can encode the letters of

5 in them. In addition, they have to be wide enough so that the heads of the computation do not

• S ≥ max{2U,

Fk(cid:48)
“collide”.

• T ≥ 4U + S + t0: T has to be large enough so that the computation and the shifting are done before

• kHead−1 , kHead+1 ≥(cid:12)(cid:12)χ(cid:0)F4 × (Qp ∪ Qp−1 ) × {−1, +1}(cid:1)(cid:12)(cid:12): The head ﬁelds have to be large enough so that

the next working period starts.

states of γU [p] and γU [p−1] can be written on them.

• kTape, kNTape, kTape−1

, kTape+1

≥ 1. Empty ﬁelds are of no use, in general.

2, k(cid:48) ∈ NM and t0 ∈ N are ﬁxed, then we can choose k, U, S and T such that the

Remark 35. If p, p−1 ∈ F∗
inequalities of Lemma 33 is satisﬁed.
Proof. Since k(cid:48), p and t0 are ﬁxed, given U, S, T we can choose k := kU,S,T such that all of the inequalities
except for the three ﬁrst are satisﬁed as equalities. Then, given S, U we can choose T := TS,U such that the
third inequality is satisﬁed as an equality. Similarly, given U we can choose S := SU such that the second
inequality is satisﬁed. Finally, U can be chosen independently from the rest of the parameters, since it only
depends on p and k(cid:48), which are ﬁxed.

In later constructions, the choice of U will not be so straightforward, as k(cid:48) will depend on k. In this case,
we ﬁrst ﬁx G and then look for the suitable values of the parameters. The situation becomes trickier when
the simulated RPCA depends on the choice of the parameters, as will be the case in the following chapters.
Then, we have to be careful not to fall into a circular argument.

41

Chapter 6

Inﬁnite hierarchies

For every PPA G and suﬃciently large S, T , Lemma 33 shows that is is possible to construct a PPA F that
(S, T, 0)-simulates G. In addition, the simulation can be made exact. We also want to make it complete.
Tape(DG), which, by deﬁnition makes the simulation complete.
The most direct way is to restrict F to ˜Φ−1
However, this is not good because it is a radius-S SFT condition and, if we wanted to have an inﬁnite nested
simulation, we would have to impose an inﬁnite number of such restrictions, so that the subshift we would
obtain would not be an SFT.

The idea, which is the basic idea behind all hierarchical constructions, is that if the simulated alphabet
is determined in an easy way by the simulating alphabet, then it is possible to design a simple IPPA that
ensures that the simulating conﬁguration is in ˜Φ−1

Tape(DG).

6.1 Son-father checks

k(cid:48) ∈ F∗∗

The ﬁrst thing is to check that the simulated letter, which is written in an encoded form bit by bit in Tape,
has the correct structure, i.e., it is the encoding of a letter with the correct number of ﬁelds and lengths.

(cid:57) NM is a vector valuation, that gives the lengths of the simulated alphabet as a function of
the lengths of the simulating alphabet. In applications, it will be easily computable from every letter of the
simulating alphabet, or, in other words, the information about the structure of the simulated letter will be
known to all of the letters of the simulating IPPA.

5

CkAlph[M, vAddr, tTape, k(cid:48)]

Check(tTape = 3) {On the right side of the encoding, Tape is 3.}

1: if vAddr ≥ lk(cid:48),M then

2:
3: end if
4: for 0 ≤ i < M do

5:

if vAddr = lk(cid:48),i then

Check[tTape = 2] {Field separators are at the expected positions.}
Check[tTape ∈ F2] {Proper ﬁeld encodings are binary.}

else if lk(cid:48),i < vAddr < lk(cid:48),i+1 then

6:
7:
8:
end if
9:
10: end for

easily checkable that the parameters are polynomially computable.)

This permutation is polynomially computable in its parameters. (In every case that we use it, it will be
The following lemma follows simply by inspection of the deﬁnition of χ (·) and CkAlph[M, vAddr, tTape, k(cid:48)]:

42

Lemma 36. Let us ﬁx a ﬁeld list [Addr, Tape] ∈ N2, an integer M ∈ N1, a vector k(cid:48) ∈ NM , S ∈ N1 and a
vector k ∈ N∗.
Let F be the IPPA deﬁned by a null direction vector and permutation CkAlph[M, πAddr, πTape, k(cid:48)], and

assume that the following inequalities hold: S ≥(cid:13)(cid:13)(cid:13)χ

Fk(cid:48)
kAddr ≥ (cid:107)S(cid:107)
kTape ≥ 1 .

(cid:16)

5

(cid:17)(cid:13)(cid:13)(cid:13)

Let c ∈ (Fk
5 )

Z ∩ Σs,S ∩ ˜Φ−1

and in this case F (c) = c.

b ∈ (Fk(cid:48)
Z
5 )

Tape(b), where s ∈ (cid:74)0, S(cid:74) and b ∈ (F∗∗

Z
5 )

. Then, F (c) exists if and only if

In other words, if a conﬁguration is split into colonies using the Addr ﬁeld and every colony has the
encoding of some letter on its Tape tape, then CkAlph[M, πAddr, πTape, k(cid:48)] ensures that this encoded letter
belongs in Fk(cid:48)
5 .

We can also check that some ﬁeld i in the simulated letter has a prescribed preﬁx (given by a term t).

HCheck[M, vAddr, tTape, k(cid:48), i, t]

1: if lk(cid:48),i < vAddr ≤ lk(cid:48),i + |χ (t)| then

Check[tTape = χ (t)|vAddr−lk(cid:48),i

]

2:
3: end if

5 → F∗

k(cid:48) ∈ NM , S ∈ N, a term t : F∗∗

Let F be the IPPA deﬁned by a null direction vector and permutation HCheck[M, πAddr, Tape, k(cid:48), i, t], and

Lemma 37. Let us ﬁx a ﬁeld list [Addr, Tape] ∈ N2, an integer M ∈ N1, a ﬁeld i ∈ (cid:74)0, M(cid:74), a covector
assume that the following inequalities hold: S ≥(cid:13)(cid:13)(cid:13)χ

5 and a vector k ∈ N∗.
(cid:16)

Fk(cid:48)
kAddr ≥ (cid:107)S(cid:107)
kTape ≥ 1 .

(cid:17)(cid:13)(cid:13)(cid:13)

5

Let c ∈ (Fk
5 )

Z ∩ Σs,S ∩ φ−1(b), where 0 ≤ s < S and b ∈ (Fk(cid:48)
Z
5 )

all n, n(cid:48) ∈ Z.
Then, F (c) exists if and only if πi(bj)(cid:74)0,(cid:107)tc(cid:107)(cid:74) = tc, for all j ∈ Z and in this case F (c) = c.

We implicitly assume that if l > (cid:107)w(cid:107), where w ∈ A∗, then wl = .
In other words, if all letters of c have the “same idea” about what πi(bj) should be, then, they can
check in one step that this indeed happens. In practice, t will usually be equal to πField, where Field is a
horizontally constant ﬁeld, so that the condition t(cn) = t(cn(cid:48)) will be true. In this case, we just check that
πField(cn) is a preﬁx of πField(bj), for all j, n ∈ Z.

and assume that t(cn) = t(cn(cid:48)) := tc for

Lemmas 36 and 37 correspond to what in [8] is achieved by mentioning that “the TM knows at which place
the information of every ﬁeld is held”. For many people, this argument is one of the most confusing things
in that construction. This is the reason why we have tried to explain this point as clearly as possible and
show exactly how the cells of the simulating IPPA can collectively check that some constant information of
the simulating alphabet is the same in the simulated alphabet. In fact, we use a general term t in Lemma 37,
which essentially allows us to impose any (polynomially computable) condition on the simulated alphabet.

6.2 Self-simulation

We are now ready to construct a self-simulating RPCA. This is the simplest and ﬁrst example of nested
simulation. We just check that the simulated letter has the same lengths as the simulating ones and that

43

some “hierarchical” ﬁelds (which contain the values p, p−1, U, S, T that are ﬁxed in Lemma 33) have the
same value in the simulated letter as in the simulating ones (where their values is already ﬁxed).

Let CSelf := CSimulate (cid:116) [MAddr, MClock, Alarm, Prog, RProg] ∈ N15.
MAddr and MClock are used to obtain the values of vMAddr and vMClock. Similarly, Prog, RProg and Alarm

are used to obtain the values tProg, tRProg and vAlarm. All of these ﬁelds will be horizontally constant.

Self[M, ν]

1: if πClock = 0 then
2:

Head−1,Head+1,Tape−1,Tape+1,NTape]

Check[S 
CkAlph[M, πAddr, πTape, (|πj|)j<M ] {Check that the lengths of the simulated letter are the same}
for i ∈ {MAddr, MClock, Alarm, Prog, RProg} do
HCheck[M, πAddr, Tape, (|πj|)j<M , i, πi] {Check that the hierarchical ﬁelds of the simulated let-
ter are the same}

3:

4:
5:

end for

6:
7: end if
8: Simulate[M ,ν,(|πj|)j<M ,πAddr,πClock, πMAddr, πMClock, πAlarm, πProg, πRProg]{The alphabet is as ex-

pected; we can simulate.}

9: Grid[πMAddr, πMClock]

In the next lemma, we do not want to have any anonymous ﬁelds, but only those ﬁelds that are used in
Self. There are 15 ﬁelds in CSelf, so we take the ﬁeld list [0, . . . , 14], which means that we assign a number

of(cid:74)0, 15(cid:74) to every ﬁeld in CSelf in some random (but ﬁxed) way. Once we have done this, the corresponding

vector of directions is also well-deﬁned.
Lemma 38. Let us ﬁx the ﬁeld list CSelf := [0, . . . , 14], the corresponding direction vector νSelf, integers
S, T, U ∈ N1 and vector k ∈ N15. Let F be the IPPA with directions νSelf and permutation Self[15, νSelf]
and p, p−1 be the programs for this permutation and its inverse, respectively.

Let Fk,S,T,U be the restriction of F to the subalphabet

Ak,S,T,U := Fk

5 ∩ S S

MAddr ∩ S T

MClock ∩ S U

Alarm ∩ S p

Prog ∩ S p−1
RProg,

and assume that the following inequalities are satisﬁed:



kRProg ≥(cid:13)(cid:13)p−1(cid:13)(cid:13)

I(k, k, S, T, U, p, p−1)
kProg ≥ (cid:107)p(cid:107)
kMAddr ≥ (cid:107)S(cid:107)
kMClock ≥ (cid:107)T(cid:107)
kAlarm ≥ (cid:107)U(cid:107) .

Then, Fk,S,T,U (cid:23)

S,T,0,Φ

Fk,S,T,U completely exactly, where Φ := ˜ΦTape|Σ and
Head−1,Head+1,Tape−1,Tape+1,NTape ∩ ˜Φ−1

k,S,T,U ∩ Σ0,0,S,T ∩ S 

Σ := AZ

Tape(AZ

k,U,S,T,p).

It is important to notice that F is a ﬁxed rule that does not depend on k, S, T, U . Therefore, its program
p is a ﬁxed word which we can “feed” to itself by restricting the alphabet to S p
Prog. This is the basic idea of
self-simulation. Notice also that the ﬁelds for which we do a hierarchical check are exactly those that are
ﬁxed in the deﬁnition of Ak,S,T,U . We need to ensure that these ﬁelds have the correct value in the simulated
letter. The correct way to do this is to check that the value of the simulated letter is in a good relation
with the values in the letters of the simulating IPPA. Here, the relation is simply equality. Later it will be
something more complicated.

44

We will try to give as many details as possible in the following proof because it will serve as a prototype

for the rest of the hierarchical simulations.

Proof. We have to show three things: First of all, that Fk,S,T,U (S, T, 0)-simulates Fk,S,T,U with decod-
ing function Φ (simulation), second, that Φ is injective (exactness) and, ﬁnally, that ΩFk,S,T ,U,p ⊆ ˜D(Φ)
(completeness).
Head−1,Head+1,Tape−1,Tape+1,NTape ∩

For the simulation part, let b ∈ AZ

k,S,T,U ∩ Σ0,0,S,T ∩ S 

k,S,T,U and c ∈ AZ

˜Φ−1(b). By deﬁnition, c is not rejected by the checks of lines 2,3 and 5.

Indeed, line 2 checks that the ﬁelds Head−1, Head+1, Tape−1, Tape+1, NTape are empty, which is true

since

c ∈ Σ0,0,S,T ∩ S 

Head−1,Head+1,Tape−1,Tape+1,NTape.

Line 3 checks that the lengths of the ﬁelds of b and c are the same, while the checks of line 5 check that b
and c have the same values in the ﬁelds MAddr, MClock, Prog and RProg, which are true by deﬁnition.

Since c is not rejected by these checks and Fk,S,T,U is a subrule of

Grid[S, T ] ◦ Simulate15,νSelf[(|πj|)j<M , πAddr, πClock, S, T, U, p, p−1]

and, by assumption, the inequalities of Lemma 33 are satisﬁed, and p is the program of Self[15, νSelf],
Lemma 33 gives that Fk,S,T,U (S, T, 0)-simulates Fk,S,T,U with decoding function Φ.
For the exactness part, we have already noted various times that the values of the ﬁelds in CSimulate are
uniquely determined for all c ∈ Φ−1(b). For the hierarchical ﬁelds (i.e., MAddr, MClock, Alarm, Prog, RProg)
the values are ﬁxed for all c ∈ AZ
k,S,T,U . In addition, there do not exist any anonymous ﬁelds (since we chose
M = 15). Therefore, Φ is injective and the simulation is exact.
then c ∈ Φ−1(AZ

In order to show that the simulation is complete, we will ﬁrst show that if c ∈ Σ0,0,S,T ∩F −T

k,S,T,U (AZ

k,S,T,U ),

k,S,T,U ).

Indeed, line 2 checks that c ∈ Σ0,0,S,T ∩ S 

i ) has the structure of the encoding of a letter in Fk

i , πTape(Bc
i ) is the encoding of a letter in Fk

Head−1,Head+1,Tape−1,Tape+1,NTape (in the sense that if this is not
true, then Fk,S,T,U,p would not be deﬁned, so there would be a contradiction). According to Lemma 36,
line 3 checks that for every colony Bc
5 . (We
cannot immediately say that πTape(Bc
5 because there are some triplets that
are not used by χ (·). So for example, if the ﬁrst three letters in the Tape tape are 111, then πTape(Bc
i ) is
not the encoding of a letter in Fk
5 , even though the 2’s and 3’s are in the correct positions.) In addition,
k,S,T,U exists and the inequalities I(k, k, S, T, U, p, p−1) are satisﬁed, this means that the computation
since F T
of p on input πTape(Bc
5 . Lemma 36 now implies that
Alarm∩S p
˜φTape(Bc
RProg by checking
that the hierarchical ﬁelds of bi have the same values as the corresponding ﬁelds of the letters of c (notice
that the hierarchical ﬁelds are constant for the letters of c, so that Lemma 37 applies). Summarizing, we
have that b ∈ AZ

5 . Finally, line 5 checks that ˜φTape(bi) ∈ S S

i ) halts, therefore for all i ∈ Z, ˜φTape(Bc

i ) = bi ∈ F∗∗
MClock∩S U

Head−1,Head+1,Tape−1,Tape+1,NTape ∩ ˜Φ−1

k,S,T,U , so that c ∈ AZ

k,S,T,U ∩ Σ0,0,S,T ∩ S 

i ) = bi ∈ Fk

Prog∩S p−1

MAddr∩S T

Tape(AZ

Finally, if c ∈ F −2T
k,S,T,U (AZ
k,S,T,U σs(c) ∈ Σ0,0,S,T ∩ F −T
F t
ΩFk,S,T ,U ⊆ F −2T

k,S,T,U ) ⊆(cid:70)

k,S,T,U (AZ

k,S,T,U ) then F tσs(c) ∈ Σ0,0,S,T , for some s ∈(cid:74)0, S(cid:74) and t ∈(cid:74)0, T(cid:74). Therefore,
k,S,T,U (AZ

k,S,T,U ). This implies that
F tσs(D(Φ)), which means that the simulation is also complete.

k,S,T,U σs(c) ∈ Φ−1(AZ

k,S,T,U ) so that F t

k,S,T,U ).

0≤t<T
0≤s<S

6.2.1 Satisfying the inequalities
It is not as straightforward to see that the inequalities I(k, k, S, T, U, p, p−1) can be satisﬁed as it was for
Lemma 33, because in this case k(cid:48) is equal to k, which means that we cannot ﬁx k(cid:48) and then choose k, S, T, U
suﬃciently big.

Remark 39. We can ﬁnd k, S, T, U such that the inequalities of Lemma 38 are satisﬁed. In addition, for
all  > 0, S/T can be made larger than 1 − . (Intuitively, the macro-tiles can be made as close to a square
as we want.)

45

Proof. We have to satisfy the following inequalities:



5

(cid:1)(cid:13)(cid:13)}

5 )}
5 ), tp−1(Fk

U ≥ max{tp(Fk
T ≥ 4U + S
kAddr, kAddr+1 ≥ (cid:107)S(cid:107)
kClock, kClock+1 ≥ (cid:107)T(cid:107)

S ≥ max{2U,(cid:13)(cid:13)χ(cid:0)Fk
kHead−1, kHead+1 ≥(cid:12)(cid:12)χ(cid:0)F4 × (Qp ∪ Qp−1 ) × {−1, +1}(cid:1)(cid:12)(cid:12)
kRProg =(cid:13)(cid:13)p−1(cid:13)(cid:13)

, kTape+1

≥ 1

kTape, kNTape, kTape−1
kProg = (cid:107)p(cid:107)
kMAddr = (cid:107)S(cid:107)
kMClock = (cid:107)T(cid:107)
kAlarm = (cid:107)U(cid:107) .

(cid:1)(cid:13)(cid:13) ≤ P1(log S, log T, log U ) and max{tp(Fk
 U ≥ P2(log S, log T, log U )

S ≥ max{2U, P1(log S, log T, log U )}
T ≥ 4U + S .

equalities. Then,(cid:13)(cid:13)χ(cid:0)Fk

For all S, T, U , let us choose k := kS,T,U such that all of the inequalities except the ﬁrst three are
5 )} ≤ P2(log S, log T, log U ), for
some polynomials P1, P2. These follow by deﬁnition of χ (·) and kS,T,U and the fact that the program p is
ﬁxed and has polynomial complexity.

5 ), tp−1(Fk

5

Therefore, it is enough to ﬁnd S, T, U that satisfy the following inequalities:

For all S, U , let us choose T := TS,U = S + 4U . Also, for all S, S0, r, let us choose U := US,S0,r =

logr(S + S0). Then, the third inequality is satisﬁed and the other two are written as follows:

 logr(S + S0) ≥ P2(log S, log(S + 4 logr(S + S0)), log(logr(S + S0))

P1(log S, log(S + 4 logr(S + S0)), log(logr(S + S0)))}.

S ≥ max{2 logr(S + S0),

There exist ﬁxed r, S0 ∈ N such that the ﬁrst inequality is satisﬁed for all S, because P2 is a ﬁxed
polynomial (hence its degree is a ﬁxed number). Let us choose such r, S0. Then, if S is suﬃciently large we
also have that the second inequality is also satisﬁed (since r, S0 are now ﬁxed), because the right hand side
grows only polylogarithmically in S, which ﬁnishes the proof of the claim.
S+logr(S+S0) , which can be made larger than 1−  by choosing S suﬃciently

For the second claim, S/T =

large.
Corollary 40. There exists an RPCA G such that OG is non-empty, aperiodic and N (G) = {0}.
Proof. Let G := Fk,S,T,U : AZ
is possible, according to Remark 39. By deﬁnition, we have that S < T .
imply that OG is non-empty, uncountable, aperiodic and N (G) = {0}.

k,U,S,T → AZ
It is not diﬃcult to see that G−1(AZ

k,U,S,T , for some parameters that satisfy I(k, k, S, T, U, p, p−1). This

k,U,S,T ) is nonempty. Then, Lemmas 38, 20, 18 and Proposition 23

S

This ﬁnishes the construction of an extremely-expansive, aperiodic 2D SFT. Once we achieved self-

simulation, then extreme expansiveness follows immediately from Proposition 23.

6.3 Hierarchical simulation

We now want to construct more general nested hierarchical simulations, where the parameters of the sim-
ulation might vary in every simulation level. This structure is more ﬂexible than a simple self-simulating
RPCA, and it will be more useful in the various applications.

46

Let us ﬁx the ﬁeld list CHSimul := CSimulate (cid:116) [Level, Prog, RProg] and let νHSimul be the corresponding

vector of directions.

• Prog and RProg are used as in the previous section.
• Level is used to obtain the values of vMAddr, vMClock and vAlarm, not through a direct projection, as in

the previous case, but in a polynomially computable way.

In the following, let S, T, U ∈ NN

be sequences of integers and (k : N → NM )n∈N is a sequence of vectors

depending on n (It can give rise to a vector valuation by using πField as the index of the sequence).

HSimul[M, ν, k, S, T, U]

1: if πClock = 0 then
2:

3:

Head−1,Head+1,Tape−1,Tape+1,NTape]

Check[S 
CkAlph[M, πAddr, πTape, kπLevel+1] {Check that the lengths of the simulated letter are correct}

4: HCheck[M, πAddr, πTape, kπLevel+1, Prog, πProg] {Prog of the simulated letter is the same}
5: HCheck[M, πAddr, πTape, kπLevel+1, RProg, πRProg] {RProg is also the same}
6: HCheck[M, πAddr, πTape, kπLevel+1, Level, πLevel + 1] {Level of the simulated letter increases by 1}
7: end if
8: Simulate[M , ν, kπLevel, πAddr ,πClock, SπLevel, TπLevel, UπLevel, πProg, πRProg] {Simulate}
9: Grid[SπLevel, TπLevel]

We will now construct a nested simulation of RPCA where the simulation parameters are diﬀerent at

every level.
Lemma 41. Let U, S, T be polynomially checkable sequences of integers. Let us ﬁx the ﬁeld list CHSimul :=
[0, . . . , 12], the corresponding ﬁxed direction vector νHSimul and a polynomially checkable sequence of 13-uples
k ∈ (N13)
Let F be the IPPA with directions νHSimul and permutation HSimul[13, νHSimul, k, S, T, U;CHSimul] and
p, p−1 be the programs for this permutation and its inverse, respectively.
For all n ∈ N, let Fn be the restriction of F to the subalphabet

N

.

and assume that the following inequalities hold for all n ∈ N:

An := Fkn

5 ∩ S n

Level ∩ S p

Prog ∩ S p−1
RProg,



kn,RProg ≥(cid:13)(cid:13)p−1(cid:13)(cid:13)

I(kn, kn+1, Sn, Tn, Un, p, p−1)
kn,Prog ≥ (cid:107)p(cid:107)
kn,Level ≥ (cid:107)n(cid:107) .

(cid:23)

Fn+1 completely exactly, where Φn := ˜ΦTape|Σn and Σn := AZ

n ∩ Σ0,0,Sn,Tn ∩

Then, Fn
S 
Head−1,Head+1,Tape−1,Tape+1,NTape ∩ ˜Φ−1(AZ

Sn,Tn,0,Φn

n+1)

The proof is very similar to the proof of Lemma 38. There exists some diﬀerences, though. For example,
we do not have the ﬁelds MAddr, MClock and Alarm. These ﬁelds are computed with the aid of ﬁeld Level,
so we perform a hierarchical check for Level. Apart from that, the proof follows the same pattern.

Another diﬀerence, which will be important when we prove that the inequalities can be satisﬁed is that
the program is not ﬁxed once we ﬁx M and ν, as in the self-similar case, but depends on k, S, T and U.
Therefore, its complexity also depends on these parameters. More precisely, tp(An) = P (|An| + tk(n) +
tS(n) + tT(n) + tU(n)), for some polynomial P that does not depend on the parameters. This is due to

47

the fact that the program consists in a bounded number (independent of n) of polynomially computable
functions and a bounded number of calls to the parameters. Similarly, |p| = O(|pk| + |pS| + |pT| + |pU|).
(The same things hold for p−1.)
Proof. Let us ﬁx n ∈ N. We have to show three things: that Fn (Sn, Tn, 0)-simulates Fn+1 with decoding
function Φn (simulation), that Φn is injective (exactness) and that ΩFn ⊆ D(Φn) (completeness).

For the simulation part, let b ∈ AZ

n+1 and c ∈ Φ−1(b) ∈ AZ

n ∩ Σ0,0,Sn,Tn ∩ S 

Head−1,Head+1,Tape−1,Tape+1,NTape.

By deﬁnition, c is not rejected by the checks of lines 2,3,4, 5 and 6, .

Since c is not rejected by these checks and Fn factors onto

Simulate[13, νSimulate, (|πj|)j<M , πAddr, πClock, Sn, Tn, Un, p, p−1]
Grid[Sn, Tn]

and, by assumption, the inequalities of Lemma 33 are satisﬁed by kn and kn+1, and p is the program of
HSimul[13, νHSimul, k, S, T, U], Lemma 33 gives that Fn (Sn, Tn, 0)-simulates Fn+1 with decoding function
Φn.
For the exactness part, we have already noted various times that the values of the ﬁelds in CSimulate
are uniquely determined for all c ∈ Φ−1(b). For the hierarchical ﬁelds (i.e., Level, Prog, RProg) the values
are ﬁxed for all c ∈ AZ
n. In addition, there do not exist any anonymous ﬁelds (since we chose M = 13).
Therefore, Φn is injective and the simulation is exact.

For the completeness part, we will only show that if c ∈ Σ0,0,Sn,Tn ∩ F −T

n+1).
Having shown this, it is easy to conclude that the simulation is complete using the same argument as in the
proof of Lemma 38

n), then c ∈ Φ−1(AZ

n (AZ

Indeed, line 2 checks that c ∈ Σ0,0,Sn,Tn ∩ S 

line 3 checks that for every colony Bc
addition, since F Tn
the computation of p on input πTape(Bc
implies that ˜φTape(Bc

Head−1,Head+1,Tape−1,Tape+1,NTape. According to Lemma 36,
. In
n (c) exists and the equations I(kn, kn+1, Sn, Tn, Un, p, p−1) are satisﬁed, this means that
i ) = bi ∈ F∗∗
5 . Lemma 36 now
Prog ∩ S p−1
RProg.
Head−1,Head+1,Tape−1,Tape+1,NTape ∩

i , πTape(Bc
i ) halts, therefore for all i ∈ Z, ˜φTape(Bc
n+1, so that c ∈ AZ
n ∩ Σ0,0,Sn,Tn ∩ S 

. Finally, lines 6,4 and 5 check that ˜φTape(bi) ∈ S n+1

i ) has the structure of the encoding of a letter in F

Summarizing, we have that b ∈ AZ
Tape(AZ
˜Φ−1

n+1) = Σn, or equivalently that c ∈ Φ−1(AZ

i ) = bi ∈ F

Level ∩ S p

kn+1
5

kn+1
5

n+1).

6.3.1 Satisfying the inequalities

In addition,(cid:81)

Let us now show that the inequalities of Lemma 41 can be satisﬁed:
Remark 42. We can ﬁnd k ∈ (N13)

and S, T, U ∈ NN
i<n Si/Ti can be made both 0 and (cid:54)= 0.

N

1 such that the inequalities of Lemma 41 are satisﬁed.

We have to deal with two problems, which were not present in the previous cases: First, there is an inﬁnite
set of inequalities, since there is also an inﬁnite set of RPCA, and they must all be satisﬁed simultaneously.
Second, the size of the program and the complexity of the permutations depends on the choice of the
parameters S and T.

48

Proof. We have to satisfy the following inequalities, for all n ∈ N



)}

(cid:16)

kn+1
5

kn+1
5

(cid:17)(cid:13)(cid:13)(cid:13)}

), tp−1 (F
F

Un ≥ max{tp(F
Sn ≥ max{2Un,
Tn ≥ 4Un + Sn
kn,Prog ≥ (cid:107)p(cid:107)

(cid:13)(cid:13)(cid:13)χ
kn,RProg ≥(cid:13)(cid:13)p−1(cid:13)(cid:13)
kn,Head−1, kn,Head+1 ≥(cid:12)(cid:12)χ(cid:0)F4 × (Qp ∪ Qp−1) × {−1, +1}(cid:1)(cid:12)(cid:12)

kn+1
5

kn,Addr, kn,Addr+1 ≥ (cid:107)Sn(cid:107)
kn,Clock, kn,Clock+1 ≥ (cid:107)Tn(cid:107)
kn,Tape, kn,NTape, kn,Tape−1
kn,Level = (cid:107)n(cid:107) .

, kn,Tape+1

≥ 1

For all n ∈ N and S, T and U, let us choose kn := kn,S,T,U such that the last four inequalities are satisﬁed

as equalities. Then, we can see that

(cid:16)

(cid:13)(cid:13)(cid:13)χ

Fkn

5

(cid:17)(cid:13)(cid:13)(cid:13) ≤ P1(log Sn, log Tn, log n, kn,Head−1, kn,Head+1 , kn,RProg, kn,Prog),

for some polynomial P1.

We claim that |p| ≤ c(|pk| +|pS| +|pT| +|pU|), for some constant c. (The same holds for p−1 and we can
assume that the constant c is the same.) This is because, as we have already noticed, the program of p uses
a ﬁxed number of polynomial operations and a bounded number of calls to the parameters |pk|, |pS|, |pT|,
|pU|.

For the same reason, we have that
max{tp(Fk

5 ), tp−1 (Fk

5 )} ≤ P2(log Sn, log Tn, log n, kn,Head−1 ,

kn,Head+1 , kn,RProg, kn,Prog, tk(n), tS(n), , tT(n), tU(n)),

for some ﬁxed polynomial P2 that does not depend on the parameter sequences.

Therefore, it is enough to ﬁnd sequences k, S, T, U that satisfy the following inequalities, for all n ∈ N:



kn,Head−1, kn,Head+1 ≥ (cid:12)(cid:12)χ(cid:0)F4 × (Qp ∪ Qp−1) × {−1, +1}(cid:1)(cid:12)(cid:12)

kn,Prog, kn,RProg ≥ c(|pk| + |pS| + |pT| + |pU|)

Un ≥ P2(log Sn+1, log Tn+1, log(n + 1),

kn+1,Head−1, kn+1,Head+1, kn+1,RProg, kn+1,Prog,
tk(n + 1), tS(n + 1), tT(n + 1), tU(n + 1))
Sn ≥ max{2Un, P1(log Sn+1, log Tn+1, log(n + 1),
|pk| ,|pS| ,|pT| ,|pU|)}
Tn ≥ 4Un + Sn .

(cid:81)

Recall that in the above inequalities, Qp and Qp−1 depend on the choice of parameter sequences.
We will show two ways to do this. The ﬁrst one does not give an extremely-expansive SFT, because
i<n Si/Ti does not converge to 0, while the second one does.
1. For all sequences S and U, let us choose Tn := Tn,S,U = Sn + 4Un. Also, for all n0, r and Q ≥ 2,
let us choose Un,n0,r := Un = (n + n0)r, Sn,n0 := Sn = Qn+n0, kn,Prog = kn,RProg = n0Qr and
kn,Head−1 = kn,Head+1 = n0 for all n ∈ N.
Then, the last inequality is satisﬁed by deﬁnition. In addition, for all n0, r, Q, we have that |pS| ≤
(cid:107)c1n0Q(cid:107), |pU| ≤ (cid:107)c2rn0(cid:107) and |pT|, |pk| ≤ (cid:107)c3rn0Q(cid:107), for some constants c1, c2, c3. This is true because
the sequence (n + n0)r is uniformly (polynomially) computable in n, n0, r, which means that there
exists an algorithm that takes as input (n, n0, r) and outputs (n + n0)r, for all values of n, n0 and r.
If we use the program for this algorithm together with a description of n0 and r, then we obtain a

49

program of length bounded by (cid:107)c2rn0(cid:107) for the sequence ((n + n0)r)n∈N. A similar argument holds for
the sequence Qn+n0.

Since this algorithm works for all choices of n0, Q, r, it means that Qp and Qp−1 are actually ﬁxed.
In addition, all of the algorithms are polynomially computable, which means that
tS(n), tT(n), tk(n) ≤ P3(log Qn+n0 ), tU(n) ≤ P4(log(n + n0)r),

for some ﬁxed polynomials P3, P4.
Therefore, substituting these in the inequalities above and doing some regrouping of the terms in
parentheses (that is omitted), the inequalities that need to be satisﬁed are written as follows:



n0Qr ≥ c(cid:48) log(n0Qr)

n0 ≥ c(cid:48)

(n + n0)r ≥ P5(log Qn+n0+1, log(n + n0 + 1)r, log(n + 1))

Qn+n0 ≥ max{2(n + n0 + 1)r, P6(log Qn+n0+1, log(n + 1))} ,

for some polynomials P5, P6 and constant c(cid:48) that do not depend on r, n0 or Q.
Since c(cid:48) is ﬁxed, the ﬁrst two inequalities are true for all but a ﬁnite number of triples n0, Q, r. Without
loss of generality, we assume that it is always true. We can choose nQ and r such that the second
inequality is true for all n ∈ N and all n0 ≥ nQ, because the right hand of the inequality is bounded
by a ﬁxed polynomial of (n + n0) and r, while the left-hand side grows like nr. With ﬁxed r, we can
also ﬁnd n(cid:48)
Q, because the
left-hand side grows exponentially in (n + n0) and the right hand only polynomially (since r is ﬁxed).
By choosing n0 = max{nQ, n(cid:48)

Q} we can satisfy both inequalities for all n at the same time.
i∈N(1 + (n + n0)r/Qn+n0) (cid:54)= 0. Therefore, if we choose the sequences like
this, we do not obtain a unique direction of non-expansiveness, but rather a cone of non-expansive
directions.

Q such that the third inequality is satisﬁed for all n ∈ N and all n0 ≥ n(cid:48)

i∈N Si/Ti =(cid:81)

Note that(cid:81)

2. For all n0 ∈ N and Q ≥ 2, let us choose Sn,n0 := Sn = Qn+n0, Tn,n0 := Tn = 2Sn and Un,n0 :=
2Q , kn,Prog = kn,RProg = n0Q and kn,Head−1 = kn,Head+1 = n0 for all n ∈ N. We can use a similar

Un = Sn
argumentation as in previous case to show that it is enough to satisfy the following inequalities:

for some polynomials P3, P4 and constant c that do not depend on n0 and Q.
Obviously, for all Q these inequalities are satisﬁed when n0 is suﬃciently large.

In this case,(cid:81)

i∈N Si/2Si = 0, therefore the corresponding SFT is extremely expansive.

For both cases, we have a lot of freedom in choosing the sequences.

In the previous proof, we just
described two of the possible ways which are enough for the results we want to obtain and help in presenting
the basic ideas of the proof that is needed in any possible case.

50

n0Q ≥ (cid:107)Qn0(cid:107)
Qn+n0

≥ P3(log Qn+n0+1, log(n + 1))

Qn+n0 ≥ max{ Qn+n0+1

, P4(log Qn+n0+1, log(n + 1))} ,

2Q

4


i∈N Si/Ti =(cid:81)

6.4 Universality
Let COther = [OTape−1, OTape, OTape+1] and let us ﬁx the ﬁeld list CU niv = CHSimul (cid:116) COther and the corre-
sponding direction vector νU niv.
5 )3 over COther. This is not a strict
restriction in itself: all RPCA can be represented in this way, up to a simple alphabet renaming and use of
Remark 7
Gn, for all n ∈ N.

If the sequence of permutations (αn)n∈N is polynomially computable, we can build a PPA that simulates

For any n, consider an RPCA Gn with permutation αn : (Fln

5 )3 → (Fln

U niv[M, ν, k, S, T, U, α]

1: αLevel[COther] {Gn on the COther ﬁelds.}
2: if πClock = 0 then
3:

Check[S 
CkAlph[M, πAddr, πTape, kπLevel+1]

Head−1,Head+1,Tape−1,Tape+1,NTape]

4:
5: HCheck[M, πAddr, πTape, kπLevel+1, Prog, πProg]
6: HCheck[M, πAddr, πTape, kπLevel+1, RProg, πRProg]
7: HCheck[M, πAddr, πTape, kπLevel+1, Level, πLevel + 1]
8: end if
9: Simulate[M , ν, kπLevel, πAddr, πClock, SπLevel, TπLevel, UπLevel, πProg, πRProg] {Simulate}
10: Grid[SπLevel, TπLevel;CGrid]

The only diﬀerence of this rule with HSimul is that it has 3 additional ﬁelds (which implies that k will
) and that we apply αLevel onto the ﬁeld list COther independently from what we do on

N
be chosen in (N16)
CHSimul.
Lemma 43. Let U, S, T be polynomially checkable sequences of integers and α a polynomially computable
sequence of permutations. Let us ﬁx the ﬁeld list CU niv := [0, . . . , 15], the corresponding ﬁxed direction vector
νU niv and a polynomially checkable sequence of M -uples k ∈ (N15)
. Let F be the IPPA with directions
νU niv and permutation U niv[15, νU niv, k, S, T, U;CU niv] and p, p−1 be the programs for this permutation and
its inverse, respectively.

N

For all n ∈ N, let Fn be the restriction of F to the subalphabet

5 ∩ S n
and assume that the following inequalities hold:

An := Fkn

Level ∩ S p

Prog ∩ S p−1
RProg,



kn,RProg ≥(cid:12)(cid:12)p−1(cid:12)(cid:12)

I(kn, kn+1, Sn, Tn, Un, p)
kn,Prog ≥ |p|
kn,Level ≥ (cid:107)n(cid:107)
kn,OTape = kn,OTape−1

= kn,OTape+1

≥ ln .

Σn := AZ

If ΩGn (cid:54)= ∅, then Fn completely (Sn, Tn, 0)-simulates Fn+1 with decoding function Φn = ˜ΦTape|Σn , where
The simulation is exact if and only if ΩGn is a singleton. In addition, if c ∈ F −1

Head−1,Head+1,Tape−1,Tape+1,NTape ∩ Φ−1(AZ

n ∩ Σ0,0,S,T ∩ S 

n), then GnπCOther(c) =

n (AZ

n+1).

πCOtherFn(c).

As mentioned before, the proof is very similar to the proof of Lemma 41. Therefore, we are going to omit

most of the details and only stress those points where there is a diﬀerence.

51

Proof. Let us ﬁx n ∈ N. For the ﬁrst claim, we have to show that Fn (Sn, Tn, 0)-simulates Fn+1 with decoding
function Φn (simulation), that Φn is an injection if and only if ΩGn is a singleton and that ΩFn ⊆ D(Φn)
(completeness).
n that simulates b: we choose c ∈ Φ−1(b) ∈
For the simulation part, let b ∈ AZ
Head−1,Head+1,Tape−1,Tape+1,NTape such that πCOther(c) ∈ ΩGn (this is possible by the assumption
AZ
n ∩ Σ0,0,S,T ∩ S 
that ΩGn (cid:54)= ∅). Then, it is easy to see that c simulates b, because COther is only “touched” by αLevel := αn,
p is the program of U niv[17, νU niv, k, S, T, U;CU niv] and GT

n+1,p. We can ﬁnd c ∈ AZ

n (πCOther(c)) exists.

Finally, for the completeness part, an argument almost identical to the argument in the proof of Lemma 41
n+1). As we know, this is enough to show that the

For the exactness part, as usual πCHSimul(c) is uniquely determined by b. However, πCOther(c) can be
chosen independently from b to be any element of ΩGn , so that the simulation is exact if and only if ΩGn is
a singleton.
shows that if c ∈ Σ0,0,S,T ∩ F −T
simulation is complete.
deﬁnition of Fn, since the only rule that “touches” the ﬁelds COther is Gn.
Remark 44.

n), then c ∈ Φ−1(AZ
n (AZ

n), then GnπCOther(c) = πCOtherFn(c) is straightforward from the

The second claim, that if c ∈ F −1

1. ΩF0 (cid:54)= ∅ if and only if ΩGn (cid:54)= ∅, for all n ∈ N.
2. If ΩF0 (cid:54)= ∅, then F0 completely simulates Gn for all n ∈ N.

n (AZ

1. If ΩGn = ∅ for some n ∈ N, then ΩFn = ∅, so that since F0 simulates Fn (by transitivity of

Proof.

simulation), we obtain that ΩF0 = ∅ by Lemma 20.
If, on the other hand, ΩGn (cid:54)= ∅ for all n ∈ N, then Lemma 18 gives that ΩF0 (cid:54)= ∅.

2. If ΩF0 (cid:54)= ∅, then the second claim of Lemma 43 implies that Fn factors onto Gn. Since F0 simulates

Fn, for all n ∈ N, we obtain that F0 simulates Gn, for all n ∈ N.

i∈N Si/Ti = 0, F0 is not necessarily extremely expansive, since we might have non-
expansive directions coming from the Gn part. However, in the special case that ΩGn is a singleton for
all n ∈ N, then all the simulations are exact and it is straightforward to see that N (F0) = {0}, because
Proposition 23 applies.

Remark 45. Even if(cid:81)

6.4.1 Satisfying the inequalities
Remark 46. We can ﬁnd k ∈ (N16)
N

and(cid:81)
We only state the case(cid:81)

i∈N Si/Ti = 0.

need and use in the applications.

and S, T, U ∈ NN

1 such that the inequalities of Lemma 43 are satisﬁed

i∈N Si/Ti = 0 (even though we can make it (cid:54)= 0) too, because it is what we will

Proof. Let us write explicitly the inequalities that we need to satisfy:



(cid:16)

kn+1
5

(cid:12)(cid:12)(cid:12)χ

(cid:17)(cid:12)(cid:12)(cid:12)}

), tp−1 (F
F

kn+1
5

)}

kn+1
5

Un ≥ max{tp(F
Sn ≥ max{2Un,
Tn ≥ 4Un + Sn
kn,Prog ≥ |p|

kn,RProg ≥(cid:12)(cid:12)p−1(cid:12)(cid:12)
kn,Head−1, kn,Head+1 ≥(cid:12)(cid:12)χ(cid:0)F4 × (Qp ∪ Qp−1) × {−1, +1}(cid:1)(cid:12)(cid:12)

kn,Addr, kn,Addr+1 ≥ (cid:107)Sn(cid:107)
kn,Clock, kn,Clock+1 ≥ (cid:107)Tn(cid:107)
kn,Tape, kn,NTape, kn,Tape−1
kn,Level = (cid:107)n(cid:107)
kn,OTape = kn,OTape−1

, kn,Tape+1

≥ 1

= kn,OTape+1

= ln .

52

For all S, T and U , let us choose kn := kn,S,T,U such that the last ﬁve inequalities are satisﬁed as

equalities. Then, we have the crucial inequality

(cid:16)

(cid:13)(cid:13)(cid:13)χ

(cid:17)(cid:13)(cid:13)(cid:13) ≤ P1(log Sn, log Tn, log n, ln, kn,Head−1 , kn,Head+1, kn,RProg, kn,Prog),

Fkn

5

where ln is the size of the alphabet of αn.

The other crucial inequality of the proof of Lemma 42 also holds without any essential changes:

max{tp(Fk

5 ), tp−1(Fk

5 )} ≤ P2(log Sn, log Tn, log n, ln, kn,Head−1,

kn,Head+1, kn,RProg, kn,Prog, tk(n), tS(n), , tT(n), tU(n))

for some polynomial P2 that does not depend on the parameters.

This holds because the permutation applied consists in a number of polynomial operations (recall that
α is polynomially computable and ﬁxed for this speciﬁc construction) and a bounded number of calls to S,
T,U and k.

Also, since α is polynomially computable, ln (which is part of the output of αn) is also bounded by a
polynomial of n so that we can “remove” ln from the right-hand side of the previous inequalities and “incor-
porate” it in the polynomials P1, P2. From this point on, the proof is identical to the proof of Remark 42.

(We are free to chose whether(cid:81)

i∈N Si/Ti is equal to 0 or not.)

6.4.2 Domino problem

that satisfy the inequalities and for which(cid:81)

Theorem 47. It is undecidable whether an extremely expansive SFT is empty.
Proof. Let M be an arbitrary TM with program p(cid:48). For all n ∈ N, we deﬁne αn as follows:

ln = 1. αn(0, 0, 0) = (0, 0, 0) if Hn
p(cid:48)(0n) is true (i.e., if M does not halt within n steps). αn is undeﬁned
(αn)n∈N is a polynomially computable sequence of permutations. ΩGn is a singleton, equal to {∞0∞}, if

in all other cases.
and only if M does not halt within n steps. Otherwise ΩGn is empty.

Let us construct the sequence of RPCA (Fn)n∈N as in Lemma 43 corresponding to the α and k, S, T, U
i∈N Si/Ti = 0. Then, Remark 44 implies that ΩF0 (equivalently,
OF0) is non-empty if and only if ΩGn is non-empty for all n, which is equivalent to that M does not halt
over input 0∞.

In addition, Remark 45 implies that if ΩF0 is non-empty, then N (F0) = {0}.
Therefore, for every TM M, we have constructed a 2D SFT OF0 that is non-empty if and only if M does
not halt over input 0∞ and if OF0 (cid:54)= ∅, then OF0 is extremely expansive. This concludes the proof of the
undecidability.

It follows from the previous proof that we have actually proved the following: Let A be the family of
forbidden patterns that deﬁne empty SFT, and let B be the family that deﬁnes non-empty extremely expan-
sive SFT (with unique direction of non- expansiveness ∞). There does not exists a recursively enumerable
set X that contains B and is disjoint from A. In other words, if an algorithm correctly recognizes all non-
empty extremely-expansive SFT, then it must also (falsely) recognize an empty SFT.

6.4.3

Intrinsic universality

The second application concerns the universality properties of RPCA.

Theorem 48. For any computably enumerable set of non-empty PPA, there exists a PPA that completely
simulates all of them.

53

Proof. First of all, we can assume that all the PPA are over the ﬁeld list COther with the corresponding
directions. This is true because we can encode, in polynomial time, all the left-moving ﬁelds into a unique
left-moving ﬁeld, and similarly for the other types of ﬁelds. Then, saying that a set of PPA is computably
enumerable is equivalent to saying that the corresponding set of permutations that deﬁne these PPA is
computable enumerable.
In addition, for every computably enumerable set of PPA X (over the ﬁeld list COther), there exists a poly-
nomially computable sequence (Gn)n∈N of PPA that contains exactly the elements of X. Equivalently, there
exists a polynomially computable sequence of permutations (αn)n∈N that contains exactly the permutations
of the PPA in X.

(Let g be a ﬁxed element of X. The polynomial algorithm of (αn)n∈N takes as input n, runs the algorithm
that enumerates X for n steps and sets αn equal to the last permutation that was output. If no permutation
has yet been output, then αn is set equal to g.)
If we use this sequence α and sequences k, S, T, U that satisfy the inequalities to deﬁne the sequence
(Fn)n∈N as in Lemma 43, then, since by assumption ΩGn (cid:54)= ∅ for all n ∈ N, Remark 44 implies that F0
completely simulates Gn, for all n ∈ N.

Theorem 48 applies, up to a conjugacy, to computably enumerable sets of nonempty RPCA. In some
sense, it gives a deterministc version of the result in [29]. The same result is not true for the non-computably-
enumerable set of all nonempty RPCA, thanks to an argument by Hochman [17] and Ballier [2]. Nevertheless,
the corollary applies to the family of all reversible (complete) cellular automata, since the family of RCA is
computably enumerable. Unfortunately, it gives an RPCA (partial CA) that simulates all RCA (full CA)
instead of an RCA. The existence of an RCA that simulates all RCA seems to be a much more diﬃcult
question and is still open, (see for instance [39].

6.5 Synchronizing computation

We now introduce one more trick in our construction: the encoding of an inﬁnite sequence inside an inﬁnite
nested simulation by encoding increasing ﬁnite preﬁxes of the inﬁnite sequence inside the alphabets of the
RPCA of the nested simulation.
Let CSyncComput := CSimulate (cid:116) [MHist, MHist+1, Prog, RProg]. In this simulation, we do not use a ﬁeld
Level in order to store the parameter n. Instead, it will be obtained as the length of ﬁeld MHist. p(cid:48) is the
program of a TM. It is used to reject some nested simulation sequences, depending on the inﬁnite sequence
that is stored (through its increasing ﬁnite preﬁxes) in the alphabets of the RPCA.

SyncComput[M, ν, k, S, T, U, p(cid:48)]

(MHist)]

5:

1: Check[πMHist = πMHist+1]
2: if πClock = 0 then
Check[H|MHist|
p(cid:48)
3:
Check[S 
Head−1,Head+1,Tape−1,Tape+1,NTape]
4:
CkAlph[M, πAddr, πTape, k|MHist|+1] {Check that the lengths of the simulated letter are correct}

6: HCheck[M, πAddr, πTape, k|MHist|+1, Prog, πProg] {Prog of the simulated letter is the same}
7: HCheck[M, πAddr, πTape, k|MHist|+1, RProg, πRProg] {RProg is also the same}
8: HCheck[M, πAddr, πTape, k|MHist|+1, MHist, πMHist] {MHist of the simulating letters is a preﬁx of

MHist of the simulated}

9: end if
10: Simulate[M , ν, k|MHist|, πAddr, πClock, S|MHist|, T|MHist|, U|MHist|, πProg, πRProg]
11: Grid[S|MHist|, T|MHist|]

54

Lemma 49. Let S, T, U be polynomially checkable sequences of integers and p(cid:48) be the program of a TM.
Let us ﬁx the ﬁeld list CSyncComput := [0, . . . , 13], the corresponding ﬁxed direction vector νSyncComput and
a polynomially checkable sequence of 14-uples k ∈ (N14)
. Let F be the IPPA with directions νSyncComput
and permutation

N

SyncComput[14, νSyncComput, k, S, T, U, πMHist, p(cid:48)]

and p, p−1 be the programs for this permutation and its inverse, respectively.

For all w ∈ F∗

2, let Sw := S|w|, Tw := T|w|and Fw be the restriction of F to the subalphabet

∩ S p
and assume that the following inequalities hold for all n ∈ N:

Aw := F

∩ S w

MHist,MHist+1

k|w|
5

Prog ∩ S p−1
RProg,



kn,RProg ≥(cid:12)(cid:12)p−1(cid:12)(cid:12)

I(kn, kn+1, Sn, Tn, Un, p, p−1)
kn,Prog ≥ |p|
kn,Level ≥ (cid:107)n(cid:107) .

a∈F2

Fwa completely exactly, where

(cid:70)

Then, Fw

(cid:23)

Sw,Tw,0,Φw

(cid:71)

a∈F2

Σw := AZ

w ∩ Σ0,0,Sw,Tw ∩ S 

Head−1,Head+1,Tape−1,Tape+1,NTape ∩ ˜Φ−1(

AZ
wa), and Φw := ˜ΦTape|Σw .

in this case, wa will also be rejected by p(cid:48) within n + 1 steps, for all a ∈ F2, so that D((cid:70)
Fw (Sn, Tn, 0)-simulates(cid:70)

Proof. Let w ∈ F∗
2 and |w| = n. By deﬁnition, Sw := Sn, Tw := Tn and Uw := Un. If p(cid:48) halts on input w
within n steps, then the check of line 3 will reject every conﬁguration, which means that D(Fw) = ∅. But,
Fwa) = ∅, too.
By deﬁnition, the empty PCA strongly, completely simulates itself for all possible choices of the simulating
parameters, so that the claim is true in this case.
Suppose, then, that p(cid:48) does not halt on input w within n steps. Then, the check of line 3 is always true,
so that we can ignore it in the rest of the proof. As in the previous proofs, we have to show three things: that
Fwa with decoding function Φw (simulation), that Φw is injective (exactness)
and that ΩFw ⊆ D(Φw) (completeness).

For the simulation, it is easy to see that if b ∈ AZ

w (b), then c is not rejected
by the checks of lines 1,4,5,6, 7 and 8. Then, simulation follows easily from the choice of the program p and
Lemma 33.

wa, where a ∈ F2 and c ∈ Φ−1

a∈F2

a∈F2

Exactness is also direct. The values of all the ﬁelds of c are uniquely determined by b and the form Φw.
Completeness also follows the general pattern of the previous proofs, but there is a small diﬀerence: we
w) (the diﬀerence is that we have 2T instead of T in the exponent),

(AZ

AZ
wa). This is enough to ensure completeness of the simulation.

can show that if c ∈ Σ0,0,Sn,Tn ∩ F −2T
then c ∈ Φ−1
Indeed, if

a∈F2

w

w ((cid:70)

c ∈ Σ0,0,Sn,Tn ∩ F −T

w (AZ
w),

then lines 4,6,7 and 8 ensure that

c ∈ AZ

w ∩ Σ0,0,Sn,Tn ∩ S 

Head−1,Head+1,Tape−1,Tape+1,NTape ∩ Φ−1((

Let b ∈ ((cid:70)

b ∈(cid:70)

a∈F2

Awa)
Z

be such that c ∈ Φ−1(b). The problem is that we still cannot know that πMHist(b) is
the same in all cells, because line 8 only checks that at every cell i, πMHist(c) = w (which we know that it is
constant) is a preﬁx of πMHist(bi). However, we could still have that πMHist(bi) = w0 and πMHist(bj) = w1, for
some i (cid:54)= j. This is why we need to take 2T steps instead of T steps.
w (c) exists, F 2(b) also exists, and line 1 ensures that πMHist(bi) = πMHist+1(bi) = πMHist(bj),
for all i, j ∈ Z. The argument for this is similar to the argument used in the proof of Lemma 25. Therefore,

Indeed, since F 2T

AZ
wa and this concludes the proof of the Lemma.

a∈F2

(cid:71)

a∈F2

Awa)

Z

).

55

6.5.1 Satisfying the inequalities
Remark 50. We can ﬁnd k ∈ (N14)
N

and S, T, U ∈ NN

and(cid:81)

i<n Si/Ti = 0.

1 such that the inequalities of Lemma 49 are satisﬁed

First of all, the inequalities depend on w ∈ F∗

Proof. The proof is almost identical to the proof of Remark 46 and is omitted. We just make a few comments:
2, but in fact, if |w| = |w(cid:48)|, then we have exactly the same
inequalities for w and w(cid:48), so that actually the inequalities can be translated to a set of inequalities that
depend on n.
Second, notice that line 3 is computable in polynomial time, and since the program p(cid:48) is ﬁxed in advance,
its contribution to (cid:107)Aw(cid:107), tp and |p| is constant and does not depend on the choice of parameters.
Finally, we can choose kw,MHist := n, (where n := |w|) which means that this ﬁeld only contributes a
polynomial of n to the various inequalities, so that it can be “incorporated” into the polynomials and the
problem can be reduced to the cases that have already been dealt with.

6.5.2 Realizing computational degrees
The statement of Lemma 49 falls exactly into the situation described in Lemma 19. For all n ∈ N, let
Bn = F2. Then, for all w ∈ Fn
i<n Bi), we have deﬁned Sw, Tw, Fw and Φw such that Fw exactly,
Fub.

2 (= (cid:81)
completely (Sw, Tw, 0)-simulates(cid:70)
The check of line 3 forces that if z ∈(cid:81)
words, z ∈ Zp. Indeed, we have that D(Fz(cid:74)0,n(cid:74) ) = ∅ for some n if and only if H∞
words, if and only if p(cid:48) halts over z within n steps.
Therefore, Lemma 19 implies that ΩF = ˜D∞

p(cid:48) (z) is true, or in other
p(cid:48) (z) is not true, or in other

z (Φ) (cid:54)= ∅ if and only if H∞

Z (Φ) =(cid:70)

i∈N Bi, then ˜D∞

˜D∞
z (Φ).

b∈Bn

z∈Zp

Lemma 51. For any eﬀectively closed subset Z ⊂ F
computable, left-invertible map from ΩF onto the Cartesian product Z × F

N
2 , there exists an extremely expansive RPCA F and a

N
2 .

One could even prove that the computable map is two-to-one, and almost one-to-one for any reasonable

(topological or measure-theoretical) notion. Also, this SFT can be eﬀectively constructed from Z.
Proof. We construct SyncComput[14, νSyncComput, k, S, T, U, πMHist, p(cid:48)] for some sequences that satisfy

the inequalities and a program p(cid:48) that recognizes Z. In addition, assume that(cid:81)

i<n Si/Ti = 0.

σs(cid:74)0,n(cid:74)S

t(cid:74)0,n(cid:74) T



F

Φ−1

z0

··· Φ−1

z(cid:74)0,n(cid:74) (ΩFn ).

It follows from Lemma 19 that
˜D∞
z (Φ) =

ΩF =

(cid:71)

z∈Zp

(cid:92)

n∈N

(cid:71)
i∈N(cid:74)0,Ti(cid:74)
i∈N(cid:74)0,Si(cid:74)

z∈Zp

t∈(cid:81)
s∈(cid:81)

z0

F

0

Φ−1

t(cid:74)0,n(cid:74) T

··· Φ−1

n∈N σs(cid:74)0,n(cid:74)S

Consider the map that associates, to each conﬁguration x ∈ ΩF , the unique triple (z, s, t) such that
z(cid:74)0,n(cid:74) (ΩFn ). This map is computable, since, for all n ∈ N, Sn is for

x ∈ (cid:84)
instance given by πClockΦ0Φ1 ··· Φn−1 and z(cid:74)0,n(cid:74) is given by πMHistΦ0Φ1 ··· Φn−1.
(cid:81)
Finally, N (F) = {0}, because OF =(cid:70)
i∈N(cid:74)0, Si(cid:74) ×(cid:74)0, Ti(cid:74) and F
simulation and(cid:81)

Conversely, from the tripe (z, s, t), one can build construct a conﬁguration in ΩF, as explained in [10,
The result follows from the obvious computable homeomorphisms between ΩF and OF , and between
) = {0}, due to the exact complete

and N (OF ˜D∞

Proposition 4.2].

OF ˜D∞

z∈Zp

N2
2 .

z (Φ)

z (Φ)

i<n Si/Ti = 0.

The following was proven in [38] for general 2D SFT. Here, we can also restrict the set of non-expansive

directions.

Theorem 52. For any eﬀectively closed subset X, there exists a Medvedev-equivalent extremely expansive
2D SFT whose Turing degrees are the cones above the Turing degrees of X.

56

A fortiori, all Medvedev (and Muˇcnik) degrees contain an extremely expansive SFT.

Proof. It is enough to notice that ΩF and OF are computably homeomorphic.

The second component in the computable homeomorphism cannot easily be taken out: it is pointed in
[21] that all aperiodic subshifts admit a cone of Turing degrees (that is one degree and all degrees above it).
Let us make some ﬁnal comments: In this chapter, we are inspired and draw mainly on the work of
Durand, Romashchenko and Shen [8]. Reading that paper, one has the feeling that the construction of
that paper can be done in a reversible way, except for the exchange of information. Working out the details
needed to make that intuition work is (as proven by this chapter) messy and even tedious, sometimes, but we
manage to obtain results for which there is no known alternative proof. We also feel that our construction
can also shed some light on the construction of Durand, Romashchenko and Shen. More speciﬁcally, we
always write explicitly the inequalities that need to be satisﬁed, and for each one we explain at least once
why it is needed. Also, we construct “once and for all” the rules and then prove that they have the desired
behaviour, instead of using their more informal approach where some rule is created and then it is modiﬁed,
resulting in a new rule, for which it is taken for granted that the previous argumentation still holds.

57

Chapter 7

Expansive directions

In Lemma 10, we described a necessary condition for the set of non-expansive directions of an SFT: if X
is an SFT, then N (X) is eﬀectively closed.
In this section, we are going to show that this is in fact a
characterization of sets of non-expansive directions of SFTs.
Theorem 53. If N0 ⊆ P is eﬀectively closed, then there exists an SFT X ⊆ AZ2

such that N (X) = N0.

This is mentioned as Open Problem 11.1 in Mike Boyle’s Open Problems for Symbolic Dynamics [5].
We only answer the ﬁrst part of that problem, since our constructions do not have any SFT direction. The
second part of the problem, concerning 2D SFT with an SFT direction is much more diﬃcult to answer,
since it is inextricably related to the expansiveness of RCA.
It is enough to prove this for sets of non-expansive directions that are included in [−1, 1], or even [0, r], for
some 0 < r < 1, because we can cover the set of directions with a ﬁnite number of rotations of [−1, 1] (and
[0, r]). Therefore, even though the fact that we are using PPA might seem problematic (since N (F ) ⊆ [−1, 1]
in this case), this is not the case.

The key idea consists in constructing subshifts with a unique direction of non-expansiveness through a
nested simulation of RPCA, so that we can use Lemma 23. This idea was introduced in [19], in a non-
eﬀective way; we will try to emphasize the obstacle that has to be overcome when trying to “SFTize” this
construction.

7.1 Directive encoding

S/T

Proposition 23 states that, if we manage to implement a certain kind of nested simulation, then we will obtain
a subshift with a unique direction of non-expansiveness, equal to D
. [19,
Lemma 5.6] shows that all directions can be written in this form (when the sequences S, T, D are allowed to
be chosen without any constraints). But the sequences of nested simulations that are possible with our SFT
construction are more constrained: for example, the sequences S and T must be polynomially checkable.This
immediately imposes some restrictions, since, for example, it implies that Si cannot grow like an exponential
tower of height i. This is not excluded from the construction of Hochman, since he takes S “suﬃciently
large”, in order to make some “error term” suﬃciently small. A large part of our construction is to show
that we can satisfy these restrictions at the same time, or, in other words, that the error terms can be made
suﬃciently small even if S grows relatively slowly. At the same time, we have to take care of some technical
details.

, where S, T ∈ NN

1 and D ∈ ZN

Let us begin the construction by giving some additional necessary deﬁnitions:
To any vector ε ∈ Rn

we associate the direction interval Θε(d) := ((cid:81)
(cid:81)
0≤i<n Ri)[−1, 1] + D
1 + εi) ≤ 1/2; recall that D
0≤j≤i Rj.
Θε(d) = R0(Θε|(cid:74)1,n(cid:74) (d|(cid:74)1,n(cid:74) ) + D0) for any d = (Di, Wi)0≤i<n.

+ and any directive word d := (Di, Wi)0≤i<n ∈ (N2 \ {(0, 0)})n, where n ∈ N,
R ⊂ P, where Ri := 1/(Di + Wi +
It follows immediately by the deﬁnition that

:= (cid:80)

0≤i<n Di

R

58

the unique element of(cid:84)

We extend these deﬁnitions for inﬁnite sequences in the natural way: To any sequence ε ∈ RN
we associate the direction θε(d) := D

directive sequence d := (Dn, Wn)n∈N ∈ (N2 \ {(0, 0)})
N
i ∈ N).
If F0

+ and any
R ∈ R,
n∈N Θε|(cid:74)0,n(cid:74) ((Di, Wi)0≤i<n) (uniqueness follows from the fact that Ri ≤ 1/2, for all
. . . and for all n ∈ N, Tn = (Dn + Wn + 1 + n)Sn, then observe that
Proposition 23 can be seen as saying that N (F0) = {θε(D, W)}. This is point of contact between this section
and the rest of the thesis.

S1,T1,D1S1

S0,T0,D0S0

(cid:23)

(cid:23)

F1

[19, Lemma 5.6] can now be reformalized as the following.
Lemma 54. For all x ∈ [0, 1], there exist a sequence ε ∈ RN

+ and a directive sequence d such that θε(d) = x.

We reﬁne this statement in two ways: ﬁrst, we will show that the sequence ε can be ﬁxed and second,
we will restrict the alphabet of acceptable directive sequences to B := {(0, 1), (1, 1), (1, 0)}. By doing that,
we will “lose” a small part on the right endpoint of the interval [0, 1].
Lemma 55. Let ε ∈ [0,

be a sequence. Then,

2 − 1]

√

N

Though the interval [0, 1[ cannot be covered fully with a ﬁxed, non-trivial sequence, the convergence of

θε(BN

) = [0, 111 . . .

(1/(2+εi)i)

].

the sequence (εn)n∈N to 0 can be sped up suitably, in order to realise any number arbitrarily close to 1.
Proof. Let us prove by induction over n ∈ N that for any such sequence ε, we have that

θε|(cid:74)0,n(cid:74) (Bn) = [−(cid:89)
The base of the induction follows from 0 ≤ √

i<n

1

2 + εi

2 − 1. Let us assume that the inductive hypothesis is true

(1/(2+εi))1≤i<n+1(cid:17)

for some n ∈ N, and prove it for n + 1. Note that 1 . . . 12
By the induction hypothesis (which we can apply to the truncated sequence (n)n≥1, since it satisﬁes the
assumption, too) and ε0 ≤ √

2 − 1, we have 1 . . . 12

(1/(2+εi))i<n+1 ≥ 1
√

(1/(2+εi))i<n+1 = 1
2+ε0

The set θε|(cid:74)0,n+1(cid:74) (Bn+1) can be decomposed, in terms of the ﬁrst directive letter, into a union of three

1 + 1√
2

1 + 1 . . . 12

= 1√
2

(cid:16)

(cid:17)

1+

2

.

intervals

, 1 . . . 12

(1/(2+εi))i<n] ⊃ [0,

1√
2

].

(cid:16)

1

2 + ε0

θε|(cid:74)1,n(cid:75) (Bn) ∪

1

2 + ε0

(θε|(cid:74)1,n(cid:75) (Bn) + 1) .

Following the induction hypothesis, the ﬁrst interval is equal to

3 + ε0

1

(θε|(cid:74)1,n(cid:75) (Bn) + 1) ∪
(1/(2+εi))1≤i≤n ] = [− (cid:89)

1

2 + εi

, 1 . . . 12

1

2 + εi

,

1

2 + ε0

1 . . . 12

(1/(2+εi))1≤i≤n ].

0≤i≤n

1

3 + ε0

1

2 + εi

, 1 . . . 12

(1/(2+εi))1≤i≤n]) = [

1

3 + ε0

1

2 + εi

),

1

3 + ε0

1 . . . 12

(1/(2+εi))1≤i≤n ].

The second interval is equal to

1

2 + ε0

1≤i≤n

[− (cid:89)
(1 + [− (cid:89)
(1 + [− (cid:89)

1≤i≤n

1

2 + ε0

1≤i≤n

The third interval is equal to

(1 − (cid:89)
− (cid:89)

1≤i≤n

1

2 + ε0

0≤i≤n

1

2 + εi

, 1 . . . 12

(1/(2+εi))1≤i≤n]) = [

59

1

2 + εn

, 1 . . . 12

(1/(2+εi))0≤i≤n ].

It is clear that the smallest point of these three intervals is −(cid:81)

(the last two intervals are in R+),
(1/(2+εi))0≤i≤n (the ﬁrst interval is obtained through a translation by −1 of the third

0≤i≤n

2+εi

1

= 1√
2
1

(2+ε0)

2

. It follows from this that the upper
√

, while the smaller bound of the

, which is less than

, as can be easily veriﬁed.In other words, there

of the second one).

(1/(2+εi))i<n+1 ≥ 1
√

1+

2

1 + 1√
2

(cid:16)

(cid:17)

(1/(2+εi))1≤i≤n of the ﬁrst interval is larger than

and the largest is 1 . . . 12
one, or through a homothecy by 2+ε0
3+ε0

We proved earlier that 1 . . . 12

1

2+ε0

1 . . . 12

bound
second interval is lower than 1
is no hole between these two intervals.

3+ε0

1

(2+ε0)

√

2

√
2

1 . . . 12
than 1

2+ε0

(1/(2+εi))1≤i≤n+1), which is larger than 1+1/
√
3+ε0

(1/(2+εi))0≤i≤n(cid:105)
The proof of the statement is ﬁnished by observing that (cid:81)

, which is smaller than 1+1/
3+ε0
1

(cid:104)−(cid:81)

2− 1 + ε0

, 1 . . . 12

0≤i≤n

, since

1√
2

2+εi

.

√

2

we get the full interval

1 . . .

(1/(2+εi)i∈N)

.

7.2 Computing directions

Using the same arguments, one can easily see that the upper bound of the second interval is

1

3+ε0

(1 +

while the lower bound of the third interval is smaller

> 0. There is no hole here either, and globally,

0≤i<n

→ 0 and 1 . . . 12

(1/(2+εi)i<n) →

1

2+εi

Lemma 10 stated that the set of non-expansive directions of an SFT is eﬀectively closed, which means that
there exists a program which takes as (inﬁnite) input the description of a direction in P and halts (after
having read ﬁnitely many bits of the input) if and only if the direction is expansive. In Subsection 2.2.2,
it was suggested that the good way to represent directions in order to compute with them was by the two
coordinates of some intersection with the unit circle. Each slope then has two (opposite) valid representations.
When restricting to closed subsets of R ⊆ P (i.e., when we are not talking about the horizontal direction),
the notion of eﬀectively closed set of direction is the same with the above representation as with the usual
deﬁnition of R. This is due to the facts that the functions sin and cos and their inverses are computable and
that the function x → 1/x is uniformly continuous away from 0.

The following remark states that directive sequences give another, equivalent representation for directions.

Remark 56. Let ε ∈ RN

be computable. Then, θε is a computable function.

The computation is actually uniform in ε, in the sense that it could be considered as part of the input.

Proof. This follows from the fact that the diameter of Θε|(cid:74)0,n(cid:74) (d) is at most 2−n, since Ri ≤ 1/2, for all
i ∈ N and directive sequence d.

Remark 56 implies that eﬀectively closed sets of slopes can be equivalently described by an eﬀectively
closed set of directive sequences. This is the computational description of directive sequences that we are
going to use in the next chapter.

7.3 Realization of sets of non-expansive directions
Let CReali = CSimulate (cid:116) [MHist, MShift, MShift+1, Prog, RProg].
The following permutation will also be parametrized by an eﬀectively closed set N0 ⊆ [0, 1/2], which is
represented by the program p(cid:48) of the TM that recognizes N0 as a set of directive sequences. N0 is the set of
non-expansive directions that we are trying to realize.
We identify the set B := {(0, 1), (1, 1), (1, 0)} with F3 (through any bijection). If a ∈ B, then Da, Wa will
In the following algorithm, p(cid:48) is the program of a TM that recognizes some set of non-expansive directions.

denote the projection of a onto the ﬁrst and second coordinate, respectively.

60

Reali[M, ν, p(cid:48), k, S, U]

1: Check[πMShift = πMShift+1]
2: if πClock = 0 then
Check[H|MHist|
p(cid:48)
3:
Check[S 
Head−1,Head+1,Tape−1,Tape+1,NTape]
4:
CkAlph[M, πAddr, πTape, k|MHist|+1]

(MHist)]

5:
6: HCheck[M, πAddr, πTape, k|MHist|+1, Prog, πProg]
7: HCheck[M, πAddr, πTape, k|MHist|+1, RProg, πRProg]
8: HCheck[M, πAddr, πTape, k|MHist|+1, MHist, πMHistπMShift]
9: end if
10: if πClock = 0 then
11:
12: end if
13: if πClock = DMShiftS|MHist| then
14:
15: end if
16: Simulate[M , ν, k|MHist|, πAddr, πClock−DMShiftS|MHist|, S|MHist|, S|MHist|(DMShift+WMShift+1)+4U|MHist|

Swap[Tape, Tape+1]

Swap[Tape, Tape+1]

, U|MHist|, πProg, πRProg]

17: Grid[S|MHist|, S|MHist|(DMShift + WMShift + 1) + 4U|MHist|]

This is like the simulation of the computation degrees, only that we keep a more complicated register in
the MHist ﬁeld and we use the values of MShift to perform a “macro-shift” before the simulation. We also
note that T is not given as a parameter of the construction. Instead, it is determined by the sequences S, U
and the value of ﬁeld MShift.
Lemma 57. Let S, U be polynomially checkable sequences of integers and p(cid:48) be the program of a TM.
Let us ﬁx the ﬁeld list CReali := [0, . . . , 14], the corresponding direction vector νReali and a polynomially
checkable sequence of 15-uples k ∈ (N14)
N
. Let F be the IPPA with directions νReali and permutation
Reali[14, νReali, p(cid:48), k, S, U] and p, p−1 be the programs for this permutation and its inverse, respectively.
For all w ∈ B∗ and a ∈ B, let kw,a := k|w|, Sw,a := S|w|, Uw,a := U|w|, Tw,a := Sw,a(Da +Wa +1)+4Uw,a

and Fw,a be the restriction of F to the subalphabet

Aw,a := F

k|w|
5

∩ S w

MShift,MShift+1

∩ S p

Prog ∩ S p−1
RProg.

Assume that the following inequalities hold, for all w ∈ B∗ and a, a(cid:48) ∈ B:

MHist ∩ S a
(cid:16)

(cid:12)(cid:12)(cid:12)χ

(cid:17)(cid:12)(cid:12)(cid:12)}

kwa,a(cid:48)
5

)}

kwa,a(cid:48)
5

Uw,a ≥ max{tp(F
), tp−1(F
Sw,a ≥ max{2Uw,a,
kwa,a(cid:48)
F
5
kw,a,Addr, kw,a,Addr+1 ≥ (cid:107)Sw,a(cid:107)
kw,a,Clock, kw,a,Clock+1 ≥ (cid:107)Tw,a(cid:107)

kw,a,Head−1 , kw,a,Head+1 ≥ max(cid:12)(cid:12)χ(cid:0)F4 × (Qp ∪ Qp−1 ) × {−1, +1}(cid:1)(cid:12)(cid:12)
kw,a,RProg ≥(cid:12)(cid:12)p−1(cid:12)(cid:12)

kw,a,Tape, kw,a,NTape, kw,a,Tape−1
kw,a,Prog ≥ |p|
kw,a,MHist ≥ |w|
kw,a,MShift = kw,a,MShift+1

, kw,a,Tape+1

≥ 1 .

≥ 1



Then, Fw,a completely exactly simulates (cid:70)

Φw,a = ˜ΦTape|Σw,a and Σw,a := AZ

w,a ∩ Σ0,0,Sw,a,Tw,a ∩ S 

a(cid:48)∈B Fwa,a(cid:48) with parameters (Sw,a, Tw,a, DaSw,a, Φw,a), where

Head−1,Head+1,Tape−1,Tape+1,NTape ∩ Φ−1((cid:70)

a(cid:48)∈B AZ

wa,a(cid:48)).

61

DaSw,a cells (equivalently, Da macro-cells) to the right before starting the simulation.

The only diﬀerence between this proof and the proof of Lemma 49 is that we shift all the encodings
Also, it is not diﬃcult to see that the usual inequalities hold: tp(An) = tp−1(An) = P (|An| + tk(n) +

tS(n) + tT(n) + tU(n)) and |p| ,(cid:12)(cid:12)p−1(cid:12)(cid:12) = O(|pk| + |pS| + |pT| + |pU|). Recall that p(cid:48) is ﬁxed in advance so it
(cid:70)
things: that Fw,a (Sw,a, Tw,a) simulates(cid:70)

is a constant in what matters complexity.
Proof. If p(cid:48) halts on input w within |w| steps, then the check of line 3 will reject every conﬁguration, which
means that Fw,a = ∅. But, in this case, wa will also be rejected by p(cid:48) within |wa| steps, for all a ∈ B, so that
a(cid:48)∈B Fwa,a(cid:48) = ∅, too. By deﬁnition, the empty PCA strongly, completely simulates itself for all possible
Suppose, then, that p(cid:48) does not halt on input w within |w| steps. Then, the check of line 3 is always
true, so that we can ignore it in the rest of the proof. As in the previous proofs, we have to show three
Fwa,a(cid:48) with decoding function Φw,a (simulation), that Φw,a
is injective (exactness) and that ΩFw,a ⊆ DΦw,a (completeness).

choices of the simulating parameters, so that the claim is true in this case.

a∈F2

For the simulation, it is easy to see that if b ∈ AZ

wa,a(cid:48), where a(cid:48) ∈ F2 and c ∈ Φ−1

w,a(b), then c is not

rejected by the checks of lines 1,4,5,6, 7 and 8.

Then, line 11 copies all the info bits onto Tape+1. During the next Sw,aDa steps, no permutation is
applied. The only thing happening to the conﬁguration is that the encodings that are in Tape+1 travel to
the right at the speed of one cell per time step. After Sw,aDa steps, they are copied back to the Tape tape
by line 14. Every letter has travelled exactly Sw,aDa cells to the right, which corresponds to Da macro-cells.
Formally, Φ(F DaSw,a(c)) = σ−Da (b).
Then, from Fact 34 and since the only rule applied from Clock = DaSw,a is Grid ◦ Simulate, we obtain
that Φ(F DaSw,a+Sw,a+4Uw,a(c)) = Fwa,a(cid:48)(σ−Da (b)). After Clock = DaSw,a + Sw,a + 4Uw,a, nothing else
changes in the conﬁguration until Clock becomes 0 again. Line 17 ensures that Clock goes from 0 to
(Da + Wa + 1)Sw,a + 4Uw,a. This concludes the proof of the simulation part.

Exactness of the simulation is easy to see. The values of all the ﬁelds of c ∈ Φ−1
w,a), then c ∈ Φ−1
w,a), then lines 4,6,7 and 8 ensure that c ∈ AZ
(AZ
a(cid:48)∈B Awa,a(cid:48))
Z

For the completeness, we show that if c ∈ Σ0,0,Sw,a,Tw,a ∩ F
Indeed, if c ∈ Σ0,0,Sw,a,Tw,a ∩ F

S 
We still cannot know that πMShift(bi) is the same for all i ∈ Z.
w,a(c) exists, this means that
F 2(b) exists, and line1 ensures that πMShift(bi) = πMShift+1 (bi) = πMShift(bj), for all i, j ∈ Z. Therefore,

We deal with this problem in a similar way as in Section 6.5, since F 2T

Head−1,Head+1,Tape−1,Tape+1,NTape ∩ Φ−1(((cid:70)
b ∈(cid:70)

wa,a(cid:48)).
w,a ∩ Σ0,0,Sw,a,Tw,a ∩
be such that c ∈ Φ−1(b).

). Let b ∈ ((cid:70)

AZ
wa,a(cid:48) and this concludes the proof.

determined by b and Σw,a.

a(cid:48)∈B Awa,a(cid:48))

Z

w,a((cid:70)

w,a(b) are uniquely

−2Tw,a
w,a

(AZ

a(cid:48)∈B AZ

−Tw,a
w,a

a(cid:48)∈F2

7.3.1 Satisfying the inequalities

Unlike the previous cases, the set of inequalities that we want to satisfy does not depend on n, but instead
on a word w ∈ B∗ and a, a(cid:48) ∈ B. However, we will now see that the inequalities can be translated to some
inequalities about the polynomially computable sequences S, U.
Remark 58. We can ﬁnd k ∈ (N15)
N

such that the inequalities of Lemma 57 are satisﬁed.

and S, U ∈ NN

In addition, we can have εn := 4Un/Sn <

2 − 1, for all n ∈ N and θε(BN

) ⊇ [0, 1/2].

√

Proof. Let w ∈ B∗ and a, a(cid:48) ∈ B. Let n := |w| be the length of w. Let us write again the inequalities:

62

kwa,a(cid:48)
5

(cid:16)

(cid:12)(cid:12)(cid:12)χ

kwa,a(cid:48)
5

)}

(cid:17)(cid:12)(cid:12)(cid:12)}

Uw,a ≥ max{tp(F
Sw,a ≥ max{2Uw,a,
kw,a,Prog ≥ |p|

kw,a,RProg ≥(cid:12)(cid:12)p−1(cid:12)(cid:12)
kw,a,Head−1, kw,a,Head+1 ≥(cid:12)(cid:12)χ(cid:0)F4 × (Qp ∪ Qp−1) × {−1, +1}(cid:1)(cid:12)(cid:12)

), tp−1 (F
kwa,a(cid:48)
F
5

kw,a,Addr, kw,a,Addr+1 ≥ (cid:107)Sw,a(cid:107)
kw,a,Clock, kw,a,Clock+1 ≥ (cid:107)Tw,a(cid:107)
kw,a,Tape, kw,a,NTape, kw,a,Tape−1
kw,a,MHist ≥ |w|
kw,a,MShift = kw,a,MShift+1 ≥ 1 .

, kw,a,Tape+1

≥ 1




According to the deﬁnition, kw,a = kn, Sw,a = Sn, Uw,a = Un and Tw,a = Sn(Da + Wa + 1) + 4Un:

Therefore, we can write the above inequalities as follows:

(cid:16)

kn+1
5

(cid:12)(cid:12)(cid:12)χ

(cid:17)(cid:12)(cid:12)(cid:12)}

), tp−1 (F
F

)}

kn+1
5

Un ≥ max{tp(F
Sn ≥ max{2Un,
kn,Prog ≥ |p|

kn+1
5

kn,RProg ≥(cid:12)(cid:12)p−1(cid:12)(cid:12)
kn,Head−1, kn,Head+1 ≥(cid:12)(cid:12)χ(cid:0)F4 × (Qp ∪ Qp−1) × {−1, +1}(cid:1)(cid:12)(cid:12)

kn,Addr, kn,Addr+1 ≥ (cid:107)Sn(cid:107)
kn,Clock, kn,Clock+1 ≥ (cid:107)Sn(Da + Wa + 1) + 4Un(cid:107)
kn,Tape, kn,NTape, kn,Tape−1
kn,MHist ≥ n
kn,MShift = kn,MShift+1 ≥ 1 .

, kn,Tape+1

≥ 1

Unlike the previous proofs, we cannot choose kS,T ∈ NN

such that all of the inequalities except the ﬁrst
two are satisﬁed as equalities. This is because the inequalities about Clock and Clock+1 depend on a ∈ B
and not only on n. However, Da + Wa ≤ 2, for all a ∈ B, so that we can replace these inequalities with
kn,Clock, kn,Clock+1 ≥ (cid:107)3Sn + 4Un(cid:107) and show that this new set of inequalities can be satisﬁed. (Here, it is
essential that B is a ﬁnite set. Bounding Da + Wa from above is one of the reasons that we had to do a little
more work with the directive sequences.)

The rest of the proof follows the usual pattern. We choose Sn = Qn+n0 and Un = (n + n0)r for some

suitable values of n0, r and Q.

For the second claim, let us recall more speciﬁcally in which order n0, r and Q are chosen. In the proof of
Remark 42, we showed that there exists n0 and r that work for every Q. Therefore, by choosing Sn = Qn+n0
2 − 1 and
and Un = (n + n0)r, for some suﬃciently large Q, we can make ε := 4Un/Sn smaller than
111 . . .

larger than 1/2.

(1/(2+εi)i)

√

For all w, a, we have Tw,a = (Da + Wa + 1)Sw,a + 4Uw,a = (Da + Wa + 1 + 4Uw,a/Sw,a)Sw,a = (Da +
2 − 1, so that we are in the situation described in Lemma 55 and
) ⊇ [0, 1/2]. Since N0 ⊆ [0, 1/2] by assumption, this means that every direction in N0 is representable

Wa + 1 + w,a)Sw,a, where w,a <
θε(BN
as a sequence in θε(BN

).

√

7.3.2 Realization
For all z ∈ Zp(cid:48), we have a sequence of complete, exact simulations given by

Fz(cid:74)0,n(cid:74),zn

(cid:23)

Sz(cid:74)0,n(cid:74),zn ,Tz(cid:74)0,n(cid:74) ,zn ,Dzn Sz(cid:74)0,n(cid:74) ,zn

Fz(cid:74)0,n+1(cid:74),zn+1,

for all n ∈ N.

63

z (Φ)

˜D∞

z∈Zp(cid:48)

Therefore, according to a Lemma 19, we have that ΩF =(cid:70)
z (Φ), and OF =(cid:70)
sented in such a way, by Lemma 55. Finally, we know by Lemma 11 that N (OF) =(cid:70)
addition, we know that N (OF ˜D∞
(cid:70)
z∈Zp(cid:48) θ(z) = Np(cid:48) = N0.
Therefore, for every eﬀectively closed set of directions which is included in [0, 1/2], recognized by a TM
with program p(cid:48), we have constructed a 2D SFT with exactly this set as set of non-expansive directions.
According to our previous discussions, this is enough to realize arbitrary eﬀectively closed sets of directions
as the set of non-expansive directions of 2D SFT. This concludes the proof of Theorem 53.

) = θ(z), by Lemma 22 and that every direction in(cid:74)0, 1/2(cid:75) can be repre-

z∈Zp(cid:48) OF ˜D∞
z∈Zp(cid:48) N (OF ˜D∞

) =

z (Φ)

. In

z (Φ)

64

Conclusion and Open Questions

We have provided a general method for constructing extremely-expansive 2D SFT’s of ﬁnite type and we have
shown that this class of 2D SFT’s has very rich computational, dynamical and geometrical properties. At
the same time, our method throws some light on the essence of self-similar and hierarchical constructions and
we hope that it might help to better understand previous works with hierarchical constructions, especially
[8]. ( On the other hand, the diﬃculty of [10] only partly comes from the hierarchical simulation, so our
work is certainly not suﬃcient to explain this construction better.)

Regarding future work, we believe that the following questions about extremely-expansive 2D SFTs are
very natural: First of all, can (a variant of) our method produce a minimal extremely-expansive SFT? Is
the emptiness problem undecidable for minimal extremely-expansive SFT’s? Recently, Durand and Ro-
mashchenko [7] described a method for constructing minimal (but not extremely-expansive) SFT’s and an-
swer the second question positively. It seems that their technique can be readily generalized to our framework.
Second, is it possible to realize all eﬀective subshifts, in the sense of [18, 8, 1] with 2D extremely-expansive
SFT’s? This would be an improvement with of the result of [8, 1] since it would (in some sense) further lower
the dimension of the realizing subshift by one. Third, is it possible to construct extremely expansive SFT
covers for square substitutions? This question goes back to the construction of Mozes [33], which constructs
SFT covers for square substitutions without any directions of expansivess. Recently, Ollinger and Legloannec
[30] constructed 4-way deterministic covers. We believe that the answer to this question is also positive.
Finally, and this is certainly the most interesting, but also diﬃcult question, is it possible to use our method
in order to construct reversible, self-simulating CA, i.e., is it possible to turn the partial rules, with which
we have been working in this thesis, to complete rules, while at the same time keeping the good properties
of self-simulation? This could ﬁnd an application to the problem of the undecidability of expansiveness for
reversible CA.

65

Bibliography

[1] Nathalie Aubrun and Mathieu Sablik. Simulation of eﬀective subshifts by two-dimensional subshifts of

ﬁnite type. Acta Applicandae Mathematicae, 126(1):35–63, 2013.

[2] Alexis Ballier. Universality in symbolic dynamics constrained by medvedev degrees.

CoRR,

abs/1304.5418, 2013.

[3] C. H. Bennett. Logical reversibility of computation.

IBM Journal of Research and Development,

17(6):525–532, November 1973.

[4] R. Berger. The Undecidability of the Domino Problem. American Mathematical Society memoirs.

American Mathematical Society, 1966.

[5] Mike Boyle. Open problems in symbolic dynamics. Contemporary mathematics, 469:69–118, 2008.

[6] Mike Boyle and Douglas Lind. Expansive subdynamics. Transactions of the American Mathematical

Society, 349(1):55–102, 1997.

[7] Bruno Durand and Andrei Romashchenko. Quasiperiodicity and non-computability in tilings. CoRR,

abs/1504.06130, 2015.

[8] Bruno Durand, Andrei Romashchenko, and Alexander Shen. Fixed-point tile sets and their applications.

Journal of Computer and System Sciences, 78(3):731 – 764, 2012.

[9] Peter G´acs. Reliable computation with cellular automata. Journal of Computer and System Sciences,

32(1):15–78, 1986.

[10] Peter G´acs. Reliable cellular automata with self-organization. Journal of Statistical Physics, 102(1–

2):45–267, 2001.

[11] Peter G´acs, Mathieu Hoyrup, and Crist´obal Rojas. Randomness on computable probability spaces—a

dynamical point of view. Theory of Computing Systems, 48(3):465–485, 2011.

[12] Lawrence Gray. A reader’s guide to G´acs’s “Positive Rates” paper. Journal of Statistical Physics,

103(1–2):1–44, 2001.

[13] Pierre Guillon, Jarkko Kari, and Charalampos Zinoviadis. On determinism in subshifts. Unpublished

manuscript, 2015.

[14] Pierre Guillon and Charalampos Zinoviadis. Densities and entropies in cellular automata. In How the
World Computes - Turing Centenary Conference and 8th Conference on Computability in Europe, CiE
2012, Cambridge, UK, June 18-23, 2012. Proceedings, pages 253–263, 2012.

[15] G.A. Hedlund. Endomorphisms and automorphisms of the shift dynamical system. Mathematical

systems theory, 3(4):320–375, 1969.

66

[16] Peter G. Hinman. A survey of muˇcnik and medvedev degrees. Bull. Symbolic Logic, 18(2):161–229, 06

2012.

[17] Michael Hochman. A note on universality in multidimensional symbolic dynamics. Discrete and Con-

tinuous Dynamical Systems - Series S, 2(2):301–314, 2009.

[18] Michael Hochman. On the dynamics and recursive properties of multidimensional symbolic systems.

Inventiones Mathematicæ, 176(1):131–167, April 2009.

[19] Michael Hochman. Expansive directions for Z2 actions. Ergodic Theory & Dynamical Systems, 31(1):91–

112, 2011.

[20] Michael Hochman and Tom Meyerovitch. A characterization of the entropies of multidimensional shifts

of ﬁnite type. Annals of Mathematics, 171(3):2011–2038, 2010.

[21] Emmanuel Jeandel and Pascal Vanier. Turing degrees of multidimensional SFTs. Theor. Comput. Sci.,

505:81–92, 2013.

[22] J. Kari and P. Papasoglu. Deterministic aperiodic tile sets. Geometric and Functional Analysis GAFA,

9(2):353–369, 1999.

[23] Jarkko Kari. The nilpotency problem of one-dimensional cellular automata. SIAM Journal on Com-

puting, 21(3):571–586, 1992.

[24] Jarkko Kari. Representation of reversible cellular automata with block permutations. Mathematical

Systems Theory, 29(1):47–61, 1996.

[25] Jarkko Kari. A small aperiodic set of wang tiles. Discrete Mathematics, 160(1-3):259–264, 1996.

[26] Jarkko Kari. On the undecidability of the tiling problem. In SOFSEM 2008: Theory and Practice of
Computer Science, 34th Conference on Current Trends in Theory and Practice of Computer Science,
Nov´y Smokovec, Slovakia, January 19-25, 2008, Proceedings, pages 74–82, 2008.

[27] G. L. Kurdyumov. An example of a non-ergodic one-dimensional homogeneous random medium with

positive transition probabilities. Soviet Mathematics Doklady, 19(1), 1978.

[28] Gr´egory Laﬁtte and Michael Weiss. Computability of tilings.

In IFIP, volume 273 of International

Federation for Information Processing, pages 187–201, 2008.

[29] Gr´egory Laﬁtte and Michael Weiss. Tilings: simulation and universality. Mathematical Structures in

Computer Science, 20(5):813–850, 2010.

[30] Bastien Le Gloannec and Nicolas Ollinger. Substitutions and strongly deterministic tilesets. In How the

World Computes, pages 462–471. Springer, 2012.

[31] Ville Lukkarila. The 4-way deterministic tiling problem is undecidable. Theoretical Computer Science,

410(16):1516 – 1533, 2009. Theory and Applications of Tilings.

[32] Kenichi Morita. Computation-universality of one-dimensional one-way reversible cellular automata.

Information Processing Letters, 42(6):325 – 329, 1992.

[33] Shahar Mozes. Tilings, substitution systems and dynamical systems generated by them. Journal

d’analyse math´ematique, 53:139–186, 1988.

[34] Masakazu Nasu. Textile systems and one-sided resolving automorphisms and endomorphisms of the

shift. Ergodic Theory and Dynamical Systems, 28:167–209, 2 2008.

67

[35] Nicolas Ollinger. The intrinsic universality problem of one-dimensional cellular automata. In Helmut
Alt and Michel Habib, editors, STACS 2003, volume 2607 of Lecture Notes in Computer Science, pages
632–641. Springer Berlin Heidelberg, 2003.

[36] Nicolas Ollinger. Two-by-two substitution systems and the undecidability of the domino problem. In
Arnold Beckmann, Costas Dimitracopoulos, and Benedikt L¨owe, editors, CiE’2008, volume 5028 of
LNCS, pages 476–485, Athens, Greece, June 2008. Springer Berlin / Heidelberg.

[37] Raphael M. Robinson. Undecidability and nonperiodicity for tilings of the plane. Inventiones Mathe-

maticæ, 12(3), 1971.

[38] Stephen G. Simpson. Medvedev degrees of 2-dimensional subshifts of ﬁnite type, 2007.

[39] Guillaume Theyssier. How common can be universality for cellular automata? In STACS 2005, volume

3404 of Lecture Notes in Computer Science, pages 121–132. 2005.

[40] Hao Wang. Proving theorems by pattern recognition ii. Bell System Technical Journal, The, 40(1):1–41,

Jan 1961.

[41] Charalampos Zinoviadis. Hierarchy and expansiveness in 2d subshifts of ﬁnite type. In Language and
Automata Theory and Applications - 9th International Conference, LATA 2015, Nice, France, March
2-6, 2015, Proceedings, pages 365–377, 2015.

68

