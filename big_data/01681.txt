6
1
0
2

 
r
a

M
 
5

 
 
]

.

C
O
h
t
a
m

[
 
 

1
v
1
8
6
1
0

.

3
0
6
1
:
v
i
X
r
a

A single-phase, proximal path-following framework

Department of Statistics and Operations Research, UNC-Chapel Hill, USA, quoctd@email.unc.edu

Quoc Tran-Dinh

Anastasios Kyrillidis

University of Texas at Austin, USA, anastasios@utexas.edu

Volkan Cevher

Laboratory for Information and Inference Systems (LIONS), EPFL, Switzerland, volkan.cevher@epﬂ.ch

Abstract

We propose a new proximal, path-following framework for a class of—possibly non-smooth—constrained con-
vex problems. We consider settings where the non-smooth part is endowed with a proximity operator, and the
constraint set is equipped with a self-concordant barrier. Our main contribution is a new re-parametrization
of the optimality condition of the barrier problem, that allows us to process the objective function with its
proximal operator within a new path following scheme. In particular, our approach relies on the following
two main ideas. First, we re-parameterize the optimality condition as an auxiliary problem, such that a
“good” initial point is available; by doing so, a family of alternative paths towards the optimum is generated.
Second, we combine the proximal operator of the objective and path-following ideas to design a single phase,
proximal, path-following algorithm.

Our method has several advantages. First, it allows handling non-smooth objectives via proximal oper-
ators; this avoids lifting the problem dimension via slack variables and additional constraints. Second, it
consists of only a single phase: While the overall convergence rate of classical path-following schemes for
smooth objectives does not suﬀer from the initialization phase, state-of-the-art proximal path-following
√
schemes undergo slow convergence, in order to obtain a “good” starting point [47]. In this work, we show how
to overcome this diﬃculty in the proximal setting and prove that our scheme has the same O(
ν log(1/ε))
worst-case iteration-complexity with standard approaches [30, 33], but our method can handle nonsmooth
objectives, where ν is the barrier parameter and ε is a desired accuracy. Finally, our framework allows errors
in the calculation of proximal-Newton search directions, without sacriﬁcing the worst-case iteration com-
plexity. We demonstrate the merits of our algorithm via three numerical examples, where proximal operators
play a key role to improve the performance over oﬀ-the-shelf interior-point solvers.

Key words : Proximal-Newton method, path-following schemes, non-smooth convex optimization.
MSC2000 subject classiﬁcation : 90C06; 90C25; 90-08
OR/MS subject classiﬁcation : Interior-point methods, non-smooth convex programming

G(cid:63) := min

x∈Rp(cid:110)G(x) := (cid:104)c, x(cid:105) + g(x) : x ∈ X(cid:111),

(1)

1. Introduction. This paper studies the following constrained convex optimization problem:

where c ∈ Rp, g is a possibly non-smooth, proper, closed and convex function from Rp to R∪{+∞}
and X is a nonempty, closed and convex set in Rp.1
For generic X and for G just linear or quadratic, interior point methods (IPMs) often consti-
tute the method-of-choice for the numerical solutions of (1), with a well-characterized worst-case
complexity. A non-exhaustive list of special instances of (1) includes linear programs, quadratic

1 In our discussion, we separate the linear term (cid:104)c, x(cid:105) from g for our convenience in processing numerical examples in
the last section. This linear term can be absorbed into g, and does not aﬀect our analysis. The structure of X highly
aﬀects the eﬃciency of optimization schemes for (1). While simple constraints are suitable for projected optimization
methods, complicated linear constraints can be handled by penalty or augmented Lagrangian techniques, combined
with splitting or alternating direction methods [8, 9, 35]. In this work though, we even consider problem cases with
more complicated structures, such as hyperbolic or nonstandard cones, where such approaches may not apply or may
be no longer eﬃcient.

1

2

Q. Tran-Dinh, A. Kyrillidis, V. Cevher: A single-phase, proximal path-following framework
Technical Report 00(0), pp. 000–000, © 0000 INFORMS

programs, second order cone programs and, semi-deﬁnite programs [1, 6, 7, 16, 27, 28, 30, 34, 39,
42, 51, 52].

At the heart of IPMs lies the notion of interior barriers: these mimic the eﬀect of the constraint
set X in (1) by appropriately weighting the objective function with a barrier f over the set X , as
follows:
(2)

F (cid:63)

t := min

x∈int(X )(cid:110)Ft(x) := G(x) + tf (x)(cid:111);

here, f models the structure of the feasible set X and t > 0 is a penalty parameter. For diﬀerent
values of t, the regularized problem generates a sequence of solutions {x(cid:63)(t) : t > 0}, known as
central path, converging to x(cid:63) of (1), as t goes to 0+. Path-following IPMs operate along the central
path: Starting from a decent initial point and, for a properly decreasing sequence of t values,
they solve (2) only approximately, by performing a “few”2 Newton iterations for each t value. For
path-following schemes to work with attractive guarantees, the initial point must lie within the
quadratic convergence region of the Newton sub-problems. Indeed, the standard path-following
strategy guarantees that each approximate solution of (2) lies within Newton’s method quadratic
convergence region for the next value of t, and operates as warm-start for that new problem instance
[7, 29, 33, 30]. In their seminal work [33], Nesterov and Nemirovski showed that such path-following
schemes admit a polynomial worst-case complexity, as long as the underlying Newton method has
a polynomial complexity.

Based on the above, standard path-following schemes [27, 30, 33] could be characterized by two
phases: Phase I and Phase II. In Phase I and for an initial value of t, say t0, one has to determine
a good initial point for Phase II; this requires solving (2) up to suﬃcient accuracy, such that the
Newton method for (2) admits fast convergence. In Phase II and using the output of Phase I as
a warm-start, we path-follow with a provably polynomial time complexity.

Taking into account both phases, standard path-following schemes—where (1) is a smooth
objective—are characterized by the following iteration complexity: The total number of iterations
required to obtain an ε-solution is

O(√ν log(1/ε)),

(3)

where ν is a barrier parameter (see Section 2 for details) and ε is the approximate parameter,
according to the following deﬁnition:

Definition 1. Given a tolerance ε > 0, we say that x(cid:63)

ε is an ε-solution for (1) if

x(cid:63)
ε ∈ X , and G(x(cid:63)

ε)− G(cid:63) ≤ ε.

1.1. Path-following schemes for non-smooth objectives. For many applications in
machine learning, optimization and signal processing [7, 37, 47], g is usually non-smooth in order to
leverage the true underlying structure in x(cid:63). An exemplar of such g is the (cid:96)1-norm regularization,
i.e., g(x) = (cid:107)x(cid:107)1, with applications in high-dimensional statistics, compressive sensing, scientiﬁc
and medical imaging [17, 21, 45, 38, 53, 26, 12, 19], among others. Other examples for g include
the (cid:96)1,2-group norm [3, 22, 23] and the nuclear norm [11].

Unfortunately, non-smoothness of the objective reduces the optimization eﬃciency. In such set-
tings, one can often reformulate (1) into a standard conic program, by introducing slack variables
to model g. Such a technique is known as disciplined convex programming (DCP) [18] and has
been incorporated in well-known software packages, such as CVX [18] and YALMIP [25]. Existing
oﬀ-the-shelf solvers are then utilized to solve the resulting problem. However, DCP could poten-
tially increase the problem dimension signiﬁcantly; this, in sequence, reduces the eﬃciency of the
IPMs. For instance, in the example above where g(x) = (cid:107)x(cid:107)1, DCP introduces p slack variables to
reformulate g into p additional second-order cone constraints.

2 Standard path-following schemes perform just one Newton iteration.

Q. Tran-Dinh, A. Kyrillidis, V. Cevher: A single-phase, proximal path-following framework
Technical Report 00(0), pp. 000–000, © 0000 INFORMS

3

In this paper, we focus on cases where g is endowed with a low-cost proximity operator:

proxg(x) := arg min

u∈Rp (cid:8)g(u) + 1/2·(cid:107)u− x(cid:107)2(cid:9) .

If g has a tractable proximity operator, then we can often preserve the optimization eﬃciency;
e.g., for simple objectives, using such proximity operators have been proven to be practical in real
applications [5, 13, 30]. However, for generic X constraints in (1), the resulting interior barrier f in
(2) does not have Lipschitz continuous gradients and, thus, prevents us from using such schemes.
This fact necessitates the design of a new breed of path-following schemes, that can accommodate
non-smooth terms in the objective.

[47] is one of the ﬁrst works that treat jointly interior barrier path-following schemes and prox-
imity operators, in order to design a new proximal path-following algorithm for problems as in (1).
According to [47], the proposed algorithm follows a two-phase approach, with Phase II having the
same worst-case iteration-complexity (3) (up to constants) with standard smooth path-following
schemes [30, 33]. However, the initialization Phase I in the proposed scheme requires substantial
computational eﬀort, which usually dominates the overall computational time. In particular, using
a damped-step proximal-Newton scheme to ﬁnd a good initial point to be used in Phase II, the
algorithm in [47] requires

(cid:22) Ft0(x0)− Ft0(x(cid:63)

ω ((1− κ)β)

t0)

(cid:23)

Newton iterations in Phase I, for arbitrary selected t0 > 0 and x0, and κ ∈ (0, 1), β ∈ (0, 0.15],
ω(q) = q− log(1 + q); see [47, Theorem 4.4] for more details. I.e., in stark contrast to the global iter-
ation complexity (3) of smooth path-following schemes, Phase I of [47] for non-smooth objectives
undergoes a sublinear convergence rate.

1.2. Motivation. From our discussion so far, it is clear that most existing works on path-
following IPMs require two phases. In the case of smooth objectives in (1), Phase I is often
implemented as a damped-step Newton scheme, which has a sublinear convergence rate, or an
auxiliary path-following scheme, with a linear convergence rate that satisﬁes the global, worst-case
complexity in (3) [30, 33]. In standard conic programming, one can unify a two-phase algorithm in a
single-phase IP path-following scheme via homogeneous and self-dual embedded strategies; see, e.g.,
[43, 50, 52]. Such strategies parameterize the KKT condition of the primal and dual conic program
so that one can immediately have an initial point, without performing Phase I. Unfortunately, to
the best of our knowledge, such single-phase approaches have not been yet studied for proximal,
path-following IPMs, in order to handle non-smooth, nonlinear constrained convex problems.

1.3. Our contributions. The goal of this paper is to develop a new single-phase, proximal
path-following algorithm for (1). To do so, we ﬁrst re-parameterize the optimality condition of the
barrier problem associated with (1) as a parametric monotone inclusion (PMI). Then, we design
an appropriate proximal path-following scheme to approximate the solution of such PMI, while
controlling the penalty parameter. Finally, we show how to recover an approximate solution of (1),
from the approximate solution of the PMI. Thus, with an appropriate choice of parameters, we
show how we can eliminate Phase I, while we still maintain the global, polynomial time, worst-case
iteration-complexity.

The main contributions of this paper can be summarized as follows:
(i) We introduce a new parameterization for the optimality condition of (2) to appropriately
select the parameters such that much less computation for initialization is needed. Hence, we can
eliminate Phase I in the traditional path-following scheme.

4

Q. Tran-Dinh, A. Kyrillidis, V. Cevher: A single-phase, proximal path-following framework
Technical Report 00(0), pp. 000–000, © 0000 INFORMS

(ii) We design a single-phase, path-following algorithm to compute an ε-solution of (1). For
each t value, the resulting algorithm only requires a single proximal Newton iteration of a strongly
convex quadratic composite subproblem. The algorithm also allows inexact proximal Newton steps,
with a veriﬁable stopping criterion (cf. eq. (20)).
In particular, we establish the following result:

Theorem 1. The total number of proximal Newton iterations required by the proposed algo-

rithm to reach an ε-solution of (1) is upper bounded by O(cid:0)√ν log(cid:0) ν
ε(cid:1)(cid:1).

A complete and formal description of the above theorem and its proof are provided in Section 4.
Our proximal algorithm admits the same iteration-complexity, as standard path-following methods
[30, 33] (up to a constant).

1.4. The structure of the paper. This paper is organized as follows. Sections 2 and 3
contain basic deﬁnitions and notions, used in our analysis. There, we also introduce a new re-
parameterization of the central path in order to obtain a predeﬁned initial point. Section 4 presents
a novel algorithm and its complexity theory for the non-smooth objective function. Section 5
provides three numerical examples that highlight the merits of our algorithm. Technical discussions
and proofs are deferred to the appendix.

2. Preliminaries.

In this section, we provide the basic notation used in the rest of the paper,

as well as two key concepts: proximity operators and self-concordant (barrier) functions.

2.1. Basic deﬁnitions. Given x, y ∈ Rp, we use (cid:104)x, y(cid:105) or xT y to denote the inner product in
Rn. For a proper, closed and convex function g, we denote by dom(g) its domain, (i.e., dom(g) :=
{x ∈ Rn : g(x) < +∞}), and by ∂g(x) := {v ∈ Rn : g(y) ≥ g(x) +(cid:104)v, y − x(cid:105), ∀y ∈ dom(g)} its subd-
iﬀerential at x. We also denote by Dom(g) := cl(dom(g)) the closure of dom(g) [40]. We use C3(X )
to denote the class of three times continuously diﬀerentiable functions from X ⊆ Rp to R.
For a given twice diﬀerentiable function f such that ∇2f (x) (cid:31) 0 at some x ∈ dom(f ), we deﬁne

the local norm, and its dual, as

(cid:107)u(cid:107)x := (cid:104)∇2f (x)u, u(cid:105)1/2,∀u ∈ Rn, and (cid:107)v(cid:107)∗

x := max

(cid:107)u(cid:107)x≤1(cid:104)u, v(cid:105) = (cid:104)∇2f (x)−1v, v(cid:105)1/2,

respectively, for u, v ∈ Rp. Note that the Cauchy-Schwarz inequality holds, i.e., (cid:104)u, v(cid:105) ≤ (cid:107)u(cid:107)x (cid:107)v(cid:107)∗
x.

2.2. Proximity operators. The proximity operator of a proper, closed and convex function

g is deﬁned as the following strongly convex program:

proxg(x) := arg min

u∈Rp (cid:8)g(u) + 1/2·(cid:107)u− x(cid:107)2(cid:9) .

(4)

In general, computing proxg is nearly as hard as minimizing g itself. However, there exist several
structured smooth and non-smooth convex functions g that have a closed-form solution or a low-
cost evaluation of the proximity operator. We capture this idea in the following deﬁnition.

Definition 2 (Tractable proximity operator ). A proper, closed and convex function g : Rp →
R ∪ {+∞} has a tractable proximity operator if (4) can be computed eﬃciently via a closed-form

solution or via a polynomial time algorithm.

Examples of such functions include the (cid:96)1-norm—where the proximity operator is the well-known
soft-thresholding operator [13]—and the indicator functions of simple sets (e.g., boxes, cones and
simplexes)—where the proximity operator is simply the projection operator. Further examples can
be found in [4, 13, 37].

Q. Tran-Dinh, A. Kyrillidis, V. Cevher: A single-phase, proximal path-following framework
Technical Report 00(0), pp. 000–000, © 0000 INFORMS

5

2.3. Self-concordant functions and self-concordant barriers. A concept used in our

analysis is the self-concordance property, introduced by Nesterov and Nemirovskii [30, 33].

Definition 3. A univariate convex function ϕ ∈ C3(dom(ϕ)) is called standard self-concordant
if |ϕ(cid:48)(cid:48)(cid:48)(τ )| ≤ 2ϕ(cid:48)(cid:48)(τ )3/2 for all τ ∈ dom(ϕ), where dom(ϕ) is an open set in R. Moreover, a function
f : dom(f ) ⊆ Rn → R is standard self-concordant if, for any X ∈ dom(f ) and v ∈ Rn, the univariate
function ϕ where τ (cid:55)→ ϕ(τ ) := f (x + τ v) is standard self-concordant.
Definition 4. A standard self-concordant function f : dom(f ) ⊂ Rn → R is a ν-self-concordant
barrier for the set Dom(f ) with parameter ν > 0, if

sup

u∈Rn(cid:8)2(cid:104)∇f (x), u(cid:105)−(cid:107)u(cid:107)2

x(cid:9) ≤ ν, ∀x ∈ dom(f ).

In addition, f (x) → ∞ as x tends to the boundary of dom(f ).
[30, Theorem 4.1.3.]), a ν-self-concordant function f satisﬁes

We note that when ∇2f is non-degenerate (particularly, when dom(f ) contains no straight line

(cid:107)∇f (x)(cid:107)∗

x ≤ √ν, ∀x ∈ dom(f ).

(5)

Self-concordant functions have non-global Lipschitz gradients and can be used to analyze the
complexity of Newton-methods [10, 30, 33], as well as ﬁrst-order variants [15]. For more details on
self-concordant functions and self-concordant barriers, we refer the reader to Chapter 4 of [30].

i=1 log(xi) is a n-self-concordant barrier of the orthant cone Rn

Several simple sets are equipped with a self-concordant barrier. For instance, fRn
+, f (x, t) = − log(t2 −(cid:107)x(cid:107)2

+(x) :=
−(cid:80)n
2) is a
2-self-concordant barrier of the Lorentz cone Ln+1 := {(x, t) ∈ Rn × R+ : (cid:107)x(cid:107)2 ≤ t}, and the semidef-
inite cone Sn

+ is endowed with the n-self-concordant barrier fSn

+(X) := − log det(X).

Finally, we deﬁne the analytical center ¯x(cid:63)

f of f as

¯x(cid:63)
f := arg min{f (x) : x ∈ int (X )} ⇔ ∇f (¯x(cid:63)
If X is bounded, then ¯x(cid:63)
f exists and is unique [31]. Some properties of the analytical center,
important for our scheme, are presented in Section 3. In this paper, we develop algorithms for (1)
with a general self-concordant barrier f of X as deﬁned by Deﬁnition 4.

f ) = 0.

(6)

2.4. Basic assumptions. We make the following assumption, regarding problem (1).
Assumption 1. The solution set X (cid:63) of (1) is nonempty. The objective function g in (1) is
proper, closed and convex, and X ⊆ dom(g). The feasible set X is nonempty, closed and convex
(with nonempty interior int (X ) ) and is endowed with a ν-self-concordant barrier f such that
Dom(f ) := cl(dom(f )) = X . The analytical center ¯x(cid:63)
Except for the last condition, Assumption 1 is common for interior-point methods. The last
condition can be satisﬁed by adding an auxiliary constraint (cid:107)x(cid:107)2 ≤ R for suﬃciently large R; this
technique has been also used in [33] and it does not aﬀect the solution of (1) when R is large.

f of f exists.

3. Re-parameterizing the central path.

In this section, we introduce a new parameteri-

zation strategy, which will be used in our scheme for (1).

3.1. Barrier formulation and central path of (1). Since X is endowed with a ν-self-

concordant barrier f , according to Assumption A.1, the barrier formulation of (1) is given by

F (cid:63)

t := min

x∈int(X )(cid:110)Ft(x) := G(x) + tf (x) ≡ (cid:104)c, x(cid:105) + g(x) + tf (x)(cid:111),

(7)

where t > 0 is the penalty parameter. We denote by ¯x(cid:63)
t the solution of (7) at a given value t > 0.
Deﬁne rt(x) := c + ∂g(x) + t∇f (x). The optimality condition of (7) is necessary and suﬃcient for
¯x(cid:63)
t to be an optimal solution of (7), and can be written as follows:
t ) + t∇f (¯x(cid:63)
t ).

(8)
t : t > 0} the set of solutions of (7), which generates a central path (or a

We also denote by ¯C := {¯x(cid:63)
solution trajectory) associated with (1). We refer to each solution ¯x(cid:63)

t ) ≡ c + ∂g(¯x(cid:63)

0 ∈ rt(¯x(cid:63)

t as a central point.

6

Q. Tran-Dinh, A. Kyrillidis, V. Cevher: A single-phase, proximal path-following framework
Technical Report 00(0), pp. 000–000, © 0000 INFORMS

3.2. Parameterization of the optimality condition. Let us ﬁx x0 ∈ dom(f ); a speciﬁc
selection of x0 is provided later on. For given x0, let ξ0 ∈ ∂g(x0) be an arbitrary subgradient of g
at x0, and set ζ0 := ∇f (x0) + t−1

0 (c + ξ0). For a given parameter η > 0, deﬁne

hη(x) := f (x)− η(cid:104)ζ0, x(cid:105) and rt,η(x) := c + ∂g(x) + t∇hη(x).

(9)

with the gradient ∇hη(x) := ∇f (x)− ηζ0. We further deﬁne a η-parameterized version of (7) as

H (cid:63)

t := min

x {Ht(x) := (cid:104)c, x(cid:105) + g(x) + thη(x)} .

Observe that, for a ﬁxed value of η > 0, the optimality condition of (10) is given by

0 ∈ rt,η(x(cid:63)

t ) ≡ c + ∂g(x(cid:63)

t ) + t∇hη(x(cid:63)
t ).

(10)

(11)

Due to the convexity of f and g, both hη and rt,η are monotone in the sense of nonlinear operator
theory [41]. Since f is a barrier function, its domain is not the whole space and, hence, hη and rt,η
are not maximal. Clearly, when g is smooth, (11) reduces to a system of nonlinear equations.

We provide next some remarks regarding the η-parameterized problem in (10):

• Observe that, if we set η = 0, hη(x) ≡ f (x) and thus, (10) is equivalent to (7). Therefore, for
any other value η > 0, the problem in (10) diﬀers from the original formulation (7) by a factor
−tη(cid:104)ζ0, x(cid:105).
solution ¯x(cid:63)
and ¯x(cid:63)

t be the solution of (10), which is diﬀerent from the
t of (7), given the remark above. However, as t → 0 in a path-following scheme, both x(cid:63)
t
• Based on the above, for ﬁxed t > 0 and diﬀerent values of η, (10) leads to a family of paths

• Fix parameters η > 0, t > 0 and let x(cid:63)

t converge to an optimum x(cid:63) of (1).

towards x(cid:63) of (1).

Our aim in this paper is to properly combine the quantities t0, x0 and η, such that (i) solving
iteratively (10) always has fast convergence (even at the initial point x0) and, (ii) while (10) diﬀers
from (7), its solution trajectory is closely related to the solution trajectory of the original barrier
formulation. The above are further discussed in the next subsections.

3.3. A functional connection between solutions of (7) and (10) and the key role of
¯x(cid:63)
f . Given the deﬁnitions above, let us ﬁrst study the relationship between exact solutions of (7)
and (10), for ﬁxed values t > 0 and η > 0.

Lemma 1. Let t > 0 be ﬁxed. Assume η > 0 and ζ0 be chosen such that ¯m0 = η(cid:107)ζ0(cid:107)∗¯x(cid:63)

t

< 1.
t , the solutions of (7) and (10),

Deﬁne ¯∆t := (cid:107)x(cid:63)
respectively. Then,

t − ¯x(cid:63)

t(cid:107)¯x(cid:63)

t

as the local distance between ¯x(cid:63)

t and x(cid:63)

¯∆t ≤

¯m0
1− ¯m0

.

The proof is provided in Appendix 7.1. The above lemma indicates that, while (7) and (10) deﬁne
t and x(cid:63)
t ,
cannot be evaluated a priori,

diﬀerent central paths towards x(cid:63), there is an upper bound on the distance between ¯x(cid:63)
which is controlled by the selection of η, t0 and x0. However, (cid:107)ζ0(cid:107)∗¯x(cid:63)
since ¯x(cid:63)

t is unknown.

We can overcome this diﬃculty by using the analytical center point ¯x(cid:63)
f in (6). A key property
f is the following [30, Corollary 4.2.1]: Deﬁne nν := ν + 2√ν, where ν is the self-concordant
for any x ∈ int(X ) and v ∈ Rp. If f is a logarithmically

of ¯x(cid:63)
barrier parameter. Then, (cid:107)v(cid:107)∗x ≤ nν(cid:107)v(cid:107)∗¯x(cid:63)
homogeneous barrier function, then nν := 1 and (cid:107)v(cid:107)∗x ≤ (cid:107)v(cid:107)∗¯x(cid:63)
Corollary; the proof easily follows from that of Lemma 1 and the properties above.

. The observation leads to the following

f

f

t

Q. Tran-Dinh, A. Kyrillidis, V. Cevher: A single-phase, proximal path-following framework
Technical Report 00(0), pp. 000–000, © 0000 INFORMS

7

Corollary 1. Consider the conﬁguration in Lemma 1 and deﬁne m0 = ηnν(cid:107)ζ0(cid:107)∗¯x(cid:63)

f

< 1. Then,

Moreover, if we choose as the initial point x0 := ¯x(cid:63)

¯∆t ≤

.

m0
1− m0
f , then m0 = nνt−1

0 η(cid:107)c + ξ0(cid:107)∗¯x(cid:63)

f

(12)

.

In the corollary above, we bound the quantity m0 using the local norm at the analytical center
¯x(cid:63)
f . This allows us to estimate the theoretical worst-case bound in Theorem 3, described next. In
practice—and if a “better” starting point is provided—one can alternatively redeﬁne the quantities
above and remove the dependency on ¯x(cid:63)

f , without aﬀecting the analysis followed in our scheme.
The above observations lead to the following lemma: given a point x, we bound (cid:107)x − ¯x(cid:63)(cid:107)¯x(cid:63)

t , using the bound (12); the proof is given in Appendix 7.2.

t by

Lemma 2. Consider the conﬁguration in Corollary 1, such that m0 < 1

t(cid:107)x(cid:63)
, for any x ∈ int (X ). Then, the following connection between λt(x) and
¯λt(x) ≤

2 . Let λt(x) := (cid:107)x− x(cid:63)

(1− m0)λt(x)

+ ¯∆t ≤

(13)

+

.

t

λt(x)
1− ¯∆t

1− 2m0

m0
1− m0

the distance (cid:107)x− x(cid:63)(cid:107)x(cid:63)
and ¯λt(x) := (cid:107)x − ¯x(cid:63)
¯λt(x) holds:

t(cid:107)¯x(cid:63)

t

The above lemma indicates that, given ﬁxed t > 0, any approximate solution (cid:98)xt to (10), that is
“good” enough (i.e., the metric λt((cid:98)xt) is small), signiﬁes that (cid:98)xt is also “close” to the optimal of
(7) (i.e., the metric ¯λt((cid:98)xt) is bounded by λt((cid:98)xt) and, thus, can be controlled). This fact allows the

use of (10), instead of (7), and provides freedom to cleverly select initial parameters t0 and η for
faster convergence. The next section proposes such an initialization procedure.

3.4. The choice of initial parameters. Here, we describe how we initialize t0 and η. For
f . Observe that, for such
Lemma 2 suggests that, for some β ∈ (0, 1), if we can bound λt(·) as λt(·) ≤ β, then ¯λt(·) is
. This observation leads to the following lemma; the proof is

ease of presentation and based on the discussion above, we choose x0 = ¯x(cid:63)
x0, we have ∇f (x0) = ∇f (¯x(cid:63)
bounded as ¯λt(·) ≤ (1−m0)β
1−2m0
provided in Appendix 7.3.

+ m0
1−m0

f ) = 0.

Lemma 3. Let λt0(x0) := (cid:107)x0 − x(cid:63)

t0(cid:107)x(cid:63)

t0

, where x(cid:63)
t0

∂g(x0) and, from (9), rt0,η(x0) := c + ξ0 + t0∇hη(x0). Then, we have

is the solution of (10) at t := t0. Let ξ0 ∈

λt0(x0) ≤

1− γt0 −(cid:112)1− 6rt0,η + γ2

2

t0

,

(14)

provided that γt0 := (cid:107)rt0,η(x0)(cid:107)∗x0 ≡ |1− η|(cid:107)ζ0(cid:107)∗x0 < 3− 2√2.
In plain words, Lemma 3 provides a recipe for initial selection of parameters: Our goal is to choose
an initial point x0 such that λt0(x0) ≤ β, for a predeﬁned constant β ∈ (0, 1). Using (14), we observe
that in order to satisfy λt0(x0) ≤ β, it is suﬃcient to require

1− γt0 −(cid:113)1− 6γt0 + γ2

t0 ≤ 2β ⇒ γt0 ≤

β(1− β)
1 + β

.

Since

the inequality γt0 ≤ β(1−β)

1+β

γt0 = |1− η|(cid:107)ζ0(cid:107)∗
x0 ,

further implies

|1− η| ≤

β(1− β)
(1 + β)|(cid:107)ζ0(cid:107)∗

x0

.

8

Q. Tran-Dinh, A. Kyrillidis, V. Cevher: A single-phase, proximal path-following framework
Technical Report 00(0), pp. 000–000, © 0000 INFORMS

Hence, we obtain

x0(cid:21) .
β(1− β)
(1 + β)|(cid:107)ζ0(cid:107)∗
As we describe next, by our theory, it holds ¯λt(·) ≤ (1−m0)β
+ m0
1−m0
1−2m0

β(1− β)
(1 + β)|(cid:107)ζ0(cid:107)∗

η ∈(cid:20)1−

, 1 +

x0

have

< 1. Since m0

1−m0 ≤ m0
1−2m0

(15)

, we

This further leads to m0 < 1−β

+

m0

1− m0 ≤

(1− m0)β + m0

(1− m0)β
1− 2m0
3+β and, by deﬁnition of m0 and x0 ≡ ¯x(cid:63)
.

1− 2m0

ηnν
t0 (cid:107)c + ξ0(cid:107)∗¯x(cid:63)

ηnν(cid:107)ζ0(cid:107)∗¯x(cid:63)

1− β
3 + β

<

f

=

f

< 1.

f , we have

If we take η = 1, which is satisﬁes (15), then we can choose t0 such that

t0 >

(1− β)

(3 + β)nν(cid:107)c + ξ0(cid:107)∗¯x(cid:63)

f

.

(16)

This condition provides a rule to select t0. In general, t0 can be chosen based on the value of η
selected from (15) as t0 >

(1−β)

.

η(3+β)nν(cid:107)c+ξ0(cid:107)

∗
¯x(cid:63)
f

4. The single phase proximal path-following algorithm.

In this section, we present the
main ideas of our algorithm. According to the previous section, to solve (1), one can parameterize
the path-following scheme (7) into (10) and, given proper initialization, solve iteratively (10)—i.e.,
in a path-following fashion, for decreasing values of t.

In Subsections 4.1 and 4.2, we describe schemes to solve (10) up to some accuracy and how
the errors, due to approximation, propagate into our theory. Based on these ideas, Subsection
4.3 describes the main recursion of our algorithm, along with the update rule for t parameter.
Subsection 4.4 provides a practical stopping criterion procedure, such that an ε-solution is achieved.
Subsection 4.5 provides an overview of the algorithm and its theoretical guarantees.

4.1. An exact proximal Newton scheme.

t denotes the exact
solution to (10), for a given value of paramter t. Since optimizing the objective in its original form
is a diﬃcult task, it is common practice to iteratively solve (10) via ﬁrst- or second-order Taylor
approximations of the smooth part.

In our discussions so far, x(cid:63)

In this work, we focus on Newton-type solutions. Let Q(·; y) be the second-order Taylor approx-

imation of hη(·) around y, i.e.:

Q(x; y) := (cid:104)∇hη(y), x− y(cid:105) +

= (cid:104)∇f (y)− ηζ0, x− y(cid:105) +

1
2(cid:104)∇2hη(y)(x− y), x− y(cid:105)

1
2(cid:104)∇2f (y)(x− y), x− y(cid:105).

Then, x(cid:63)

t can be obtained by iteratively solving

(17)

(18)

with perfect accuracy, and xk

t is in a convergence region of such a proximal Newton method. Then,

xk+1
t ←− arg min

x∈int(X )(cid:110) ˆFt(x; xk

t ) := tQ(x; xk

t ) + G(x)(cid:111),

x∞t ≡ x(cid:63)
t .

We note that, for given point xk

t , we can write the optimality condition of (17) as follows:

To solve (17), one can use convex quadratic composite minimization solvers; see, e.g., [5, 9, 32].

0 ∈ t(cid:2)∇hη(xk

t ) +∇2hη(xk

t )(x(cid:63)

t − xk

t )(cid:3) + c + ∂g(x(cid:63)

t ).

Q. Tran-Dinh, A. Kyrillidis, V. Cevher: A single-phase, proximal path-following framework
Technical Report 00(0), pp. 000–000, © 0000 INFORMS

9

4.2. Inexact proximal Newton scheme.

In practice, we can not solve (17) exactly due to
the nonsmooth part g, but only hope for an approximate solution, up to a given accuracy δ > 0
[24]. The next deﬁnition characterizes such inexact solutions.

Definition 5. Fix t > 0 and let w be an anchor point (in (17), w = xk

t be
the exact solution, obtained by solving (17) perfectly. We say that a point z ∈ int (X ) is a δ-solution
to (17) if
(19)

t ). Moreover, let x(cid:63)

(cid:107)z − x(cid:63)

t(cid:107)w ≤ δ,

for a given tolerance δ ≥ 0. We denote this notion by z :≈ x(cid:63)
t .
however holds indirectly, when the following holds [47]:

Unfortunately, x(cid:63)
t

is unknown and, thus, we can not check the condition (19). This condition

(20)
where ˆFt(·;·) is deﬁned in (17). This last condition can be evaluated via several convex optimization
algorithms, including ﬁrst-order methods, e.g., [5, 32].

t ; w) ≤

ˆFt(z; w)− ˆFt(x(cid:63)

,

tδ2
2

We will use these ideas next to deﬁne our inexact proximal-Newton path-following scheme.

4.3. A new, inexact proximal-Newton path-following scheme. Here, we design a new,
path-following scheme that operates over the re-parameterized central path in (10). This new
algorithm chooses an initial point, as described in Section 3, and selects values for parameter t via
a new update rule, that diﬀers from that of [47].

At the heart of our approach lies the following recursion:

tk+1
:= tk + dk,
xk+1
tk+1 :≈ argmin

x∈int(X )(cid:8) ˆFtk+1(x; xk

tk ) := tk+1Q(x; xk

tk ) + G(x)(cid:9).

(21)

That is, starting from initial points t0 and x0 ≡ x0
t0, we update the penalty parameter t from tk to
tk+1 via the rule tk+1 := tk + dk, at the k-th iteration; see next for details. Then, we perform a single
proximal-Newton iteration, in order to approximate the solution to the minimization problem in
(21). Observe that, while such a step roughly approximates the minimizer of (21) satisfying (19),
in our analysis we can still guarantee convergence close to x(cid:63) of (1), using ideas in Subsection 4.2.
tk the exact

tk being the inexact solution of (21) and x(cid:63)

Update rule for parameter t. With xk

solution of (11) at t = tk, we deﬁne the local distances

λt(x) := (cid:107)x− x(cid:63)

t(cid:107)x(cid:63)

t

, and ∆k := (cid:107)x(cid:63)

tk+1 − x(cid:63)

tk(cid:107)x(cid:63)

tk+1

,

(22)

for any t > 0 and x ∈ int (X ). Before we provide a closed-form solution for dk in the update rule
tk+1 := tk + dk, we require the following lemma, which shows the relation between λtk+1(xk+1
tk+1) and
λtk+1(xk
tk ) and ∆k. The proof of this result can be found
in Appendix 7.4.

tk ), as well as the relation between λtk (xk



be an approximation of x(cid:63)
tk ) = (cid:107)xk

tk − x(cid:63)

tk+1
and λtk+1(xk+1

computed by the inex-
tk+1 −

tk+1) = (cid:107)xk+1

tk+1

Lemma 4. Given xk
tk

and tk+1, let xk+1
tk+1

tk+1

be deﬁned by (22). If λtk+1(xk

act proximal-Newton scheme (21). Let λtk+1(xk
x(cid:63)
tk+1(cid:107)x(cid:63)

tk ) ∈ [0, 0.118975], then we have
tk )2.
Moreover, the right-hand side of (23) is nondecreasing w.r.t. λtk+1(xk

tk+1) ≤ 1.135042δk+1 + 5λtk+1(xk

λtk+1(xk+1

tk+1(cid:107)x(cid:63)

Let ∆k be deﬁned by (22). Then, we have the following estimate:

provided that ∆k < 1.

λtk+1(xk

tk ) ≤

λtk (xk
tk )
1− ∆k

+ ∆k,

tk ) and δk+1 ≥ 0.

(23)

(24)

10

Q. Tran-Dinh, A. Kyrillidis, V. Cevher: A single-phase, proximal path-following framework
Technical Report 00(0), pp. 000–000, © 0000 INFORMS

In words, (23) reveals that the quadratic convergence rate of consecutive inexact proximal-
tk )) is preserved per iteration. Moreover, (23) describes

Newton steps in (21) (measured by λtk (xk
how the approximation parameter δk+1 accumulates over iterations (i.e., it is an additive term).

Now, we need to bound the distance ∆k. The next lemma shows how we can bound ∆k based
on the update rule tk+1 = tk + dk for dk (cid:54)= 0; the proof is provided in Appendix 7.5. This lemma
also provides a rule for dk selection.

Lemma 5. Given constant cβ > 0, let σβ := cβ

(1+cβ )√ν . Then, ∆k deﬁned by (22) satisﬁes

tk+1 ≤ |dk|√ν
tk
Moreover, if we choose dk := −σβtk, the ∆k is bounded by ∆k ≤ cβ.

1 + ∆k ≤ |dk|

tk (cid:107)∇f (x∗tk+1)(cid:107)∗x∗

∆k

.

(25)

Based on the above, we describe next the main result of this section: Assume that the point
xk
tk is in the quadratic convergence region of the inexact proximal-Newton method (21), i.e.,
λtk (xk
tk ) ≤ β for given β ∈ (0, 0.118975]. The following theorem describes a condition on ∆k such
that λtk+1(xk+1
tk+1) ≤ β. This in sequence determines the update rule of t values. The following the-
orem summarizes this requirement.

Theorem 2. Let {λtk (xk

tk )} be the sequence generated by the inexact proximal-Newton scheme

(21). For any β ∈ (0, 0.118975], if we choose δk and ∆k such that

δk+1 ≤ 0.066517β

and ∆k ≤

1

2(cid:20)1 + 0.43(cid:112)β −(cid:113)(1− 0.43(cid:112)β)2 + 4β(cid:21) ,

then the condition λtk (xk
updated by

tk ) ≤ β implies λtk+1(xk+1

tk+1) ≤ β. Consequently, the penalty parameter tk is

which guarantees that ∆k satisﬁes the condition (26), where

cβ

(1 + cβ)√ν(cid:19) tk,

tk+1 := (1− σβ)tk =(cid:18)1−
2(cid:20)1 + 0.43(cid:112)β −(cid:113)(1− 0.43(cid:112)β)2 + 4β(cid:21) ∈ (0, 0.044183].

1

cβ :=

(26)

(27)

In addition, cmax

β

:= max{cβ : β ∈ (0, 0.118975]} = 0.044183 when β = 0.042231.

Proof. Under the assumption λtk (xk
we can obtain from (23) and (24) that λtk+1(xk+1
λtk+1(xk+1

tk+1) ≤ β, we have

tk ) ≤ β and the ﬁrst condition (26) with δk+1 ≤ 0.066517β,
. To guarantee

tk+1) ≤ 0.0755β + 5(cid:16) β

0.0755β + 5(cid:18) β

1− ∆k

+ ∆k(cid:19)2

≤ β ⇒

β

1− ∆k

The last condition implies

1−∆k

+ ∆k(cid:17)2
+ ∆k ≤ 0.43(cid:112)β.

∆k ≤

1

2(cid:20)1 + 0.43(cid:112)β −(cid:113)(1− 0.43(cid:112)β)2 + 4β(cid:21) .

This is the second condition of (26), provided that β ∈ (0, 0.118975]. The second statement of this
(cid:3)
theorem follows from (26) and Lemma 5, while the last statement is computed numerically.

Q. Tran-Dinh, A. Kyrillidis, V. Cevher: A single-phase, proximal path-following framework
Technical Report 00(0), pp. 000–000, © 0000 INFORMS

11

4.4. Stopping criterion. We require a stopping criterion that guarantees an ε-solution for
(1) according to Deﬁnition 1. To achieve this, we present the following Lemma; the proof is provided
in Appendix 7.6.

Lemma 6. Let(cid:8)xk

tk(cid:9) be the sequence generated by (21). Then, it holds that
tk+1)− G(cid:63) ≤ tk+1 · ψ(cid:16)ν, ζ0, ¯λtk+1(xk+1
tk ), δk+1(cid:17),

tk+1), ¯λtk+1(xk

0 ≤ G(xk+1

where ψ is deﬁned as

(28)

ψ(ν, m0, λ, λ+, δ) := ν +√ν

λ+
1− λ

+

λ

(1− λ)2 (λ + λ+ + δ) +

δ2
2

+ m0λ+,

(29)

and m0 := ηnν(cid:107)ζ0(cid:107)∗¯x(cid:63)
Since λtk+1(xk+1

f

= ηnν(cid:107)∇f (x0) + t−1

tk+1) ≤ β and λtk+1(xk

¯λtk+1(xk+1

m0
1− m0
By using Lemma 6, we can see that

(1− m0)β
1− 2m0

tk+1) ≤

+

f

.

0 (c + ξ0)(cid:107)∗¯x(cid:63)
tk ) ≤ 0.43√β, δk+1 ≤ ¯δ and x0 = x(cid:63)
:= γ0, and ¯λtk+1(xk

tk ) ≤

0.43√β(1− m0)

1− 2m0

f , we can show that

+

m0
1− m0

:= ˆγ0.

(30)

0 ≤ G(xk+1

tk+1)− G(cid:63) ≤ tk+1 · ψβ(ν),

where ψβ(ν) := ψ(ν, m0, γ0, ˆγ0, ¯δ). Then, if tk · ψβ(ν) ≤ ε, we are guaranteed that xk+1
and we can terminate our algorithm.

tk+1 is a ε-solution

4.5. Overview of our scheme. We summarize the proposed scheme in Algorithm 1.

Algorithm 1 Single-phase, proximal path-following scheme

Input: Tolerance ε > 0.
Initialization:

f and set x0 := x(cid:63)

1. Compute x(cid:63)
2. Compute c0 := (cid:107)c + ξ0(cid:107)∗x(cid:63)
3. Choose t0 > (1−β)
4. Set γ0 := (1−m0)β
1−2m0
5. Set ψβ(ν) := ψ(ν, m0, γ0, ˆγ0, ¯δ).
6. Set cβ := 1

+ m0
1−m0

(3+β)nν c0

f

f . Compute a subgradient ξ0 ∈ ∂g(x0).

, η := 1 and compute m0 from Lemma 1.

. Choose β ∈ (0, 0.118975] and set ¯δ := 0.066517β.
and ˆγ0 := 0.43√β(1−m0)

.

1−2m0

+ m0
1−m0

for k := 0 to kmax do

2(cid:104)1 + 0.43√β −(cid:112)(1− 0.43√β)2 + 4β(cid:105) and σβ := cβ

(1+cβ )√ν .

7. If tkψβ(ν) ≤ ε, then terminate.
8. Update tk+1 := (1− σβ)tk.
9. Perform the inexact full-step proximal-Newton iteration by solving

xk+1
tk+1 :≈ argmin
up to a given accuracy δk+1 ≤ ¯δ.

end for

x∈int(X )(cid:8) ˆFtk+1(x; xk

tk ) := tk+1Q(x; xk

tk ) + G(x)(cid:9)

It is clear that the computational bottleneck of Algorithm 1 lies in Step 9, where we need to
approximately solve a strongly convex quadratic composite subproblem. We comment on this step
and its solution in Section 5.

The following theorem summarizes the worst-case iteration-complexity of Algorithm 1.

12

Q. Tran-Dinh, A. Kyrillidis, V. Cevher: A single-phase, proximal path-following framework
Technical Report 00(0), pp. 000–000, © 0000 INFORMS

Theorem 3. Let {(xk, tk)} be the sequence generated by Algorithm 1. Then, the total number

of iterations required to reach an ε-solution xk of (1) does not exceed

kmax :=

log(cid:16) ψβ (ν)
t0ε (cid:17)
− log(1− σβ) + 1.

(31)

(32)

(33)

t0ε(cid:17)(cid:17).

Thus, the worst-case iteration-complexity of Algorithm 1 is O(cid:16)√ν log(cid:16) ν
(cid:18) ψβ (ν)
Proof. From (27), we can see that tk = (1 − σβ)kt0. Hence, to obtain 0 ≤ G(xk+1
tk+1) − G(cid:63) ≤
ε, using (8), we require tk ≥ ψβ (ν)
− log(1−σβ ) . By rounding up this estimate, we obtain
kmax as in (31). We note that − log(1 − σβ) = O(1/√ν). In addition, by (29), we have ψβ(ν) =
O(cid:16)ν + t−1
f(cid:17) = O (ν) due to (16). Hence, the worst-case iteration-complexity of Algo-
rithm 1 is O(cid:16)√ν log(cid:16) ν
t0ε(cid:17)(cid:17).
(cid:3)
We note that the worst-case iteration-complexity stated in Theorem 3 is a global worst-case
complexity, which is diﬀerent from the one in [47]. As already mentioned in the Introduction, in
the latter case we require

0 nν(cid:107)c + ξ0(cid:107)∗x(cid:63)

, or k ≥

(cid:19)

t0ε

log

ε

(cid:22) Ft0(x0)− Ft0(x(cid:63)

ω ((1− κ)β)

t0)

(cid:23)

iterations in Phase I, for arbitrary selected t0 and x0, and κ ∈ (0, 1), β ∈ (0, 0.15], ω(q) = q− log(1 +
q), i.e., Phase I has a sublinear convergence rate to the initial point x0
t0.
We illustrate the basic idea of our single-phase scheme compared to the two-phase scheme in
[47] in Figure 1. Our method follows diﬀerent central path generated by the solution trajectory of
the re-parameterized barrier problem, where an initial point x0 is immediately available.

4.6. The exact variant. We consider a special case of Algorithm 1, where the subproblem
tk+1. In this case, we can

(17) at Step 9 can be solved exactly to obtain xk+1
enlarge the constant cβ in (27) to obtain a better factor σβ. More precisely, we can use

tk+1 such that xk+1

tk+1 = ¯xk+1

¯cβ :=

1

2(cid:20)1 + 0.45(cid:112)β −(cid:113)(1− 0.45(cid:112)β)2 + 4β(cid:21) > cβ,

where β ∈ (0, 0.116764]. Hence, we obtain a faster convergence (up to a constant factor) in this
case. For instance, we can numerically check that ¯cmax
:= max{¯cβ : β ∈ (0, 0.116764]} = 0.048186 >
cmax
β = 0.044183 with respect to β = 0.045864.

β

5. Numerical experiments.

In this section, we ﬁrst discuss some implementation aspects
of Algorithm 1: (i) how one can solve eﬃciently the subproblem in step 9 of Algorithm 1 and, (ii)
how we can compute the analytical center ¯x(cid:63)
f . In sequence, we illustrate the merits of our approach
via three numerical examples, where we compare with state-of-the-art interior-point algorithms.

Inexact proximal-Newton step. The key step of Algorithm 1 is the proximal Newton direc-

tion. This corresponds to solving the following strongly convex quadratic composite problem:

min

d∈Rp(cid:8)q(d) := (cid:104)hk, d(cid:105) + 1/2·(cid:104)Hkd, d(cid:105) + g(xk + d)(cid:9) ,

where xk, hk ∈ Rp, and Hk is a symmetric positive deﬁnite matrix.

There exist many eﬃcient ﬁrst-order and proximal quasi-Newton methods to solve (33), see, e.g.,
[5, 6] for concrete instances of proximal methods, as well as [48, 49] for primal and dual approaches

Q. Tran-Dinh, A. Kyrillidis, V. Cevher: A single-phase, proximal path-following framework
Technical Report 00(0), pp. 000–000, © 0000 INFORMS

13

(cid:23)

(x(cid:63)
t0

)

(cid:22) Ft0

(x0)−Ft0
ω((1−κ)β)

Figure 1. Illustration of diﬀerences between path-following trajectories followed by single-phase Algorithm 1
and two-phase algorithm in [47]. In the latter case and given an initial point, say x0 ≡ ¯x(cid:63)
f , [47] ﬁrst performs
iterations for Phase I to obtain an initial point ¯x0, within the quadratic convergence region of
Newton method. Then, the fast convergent Phase II follows the central path (in red color) towards x(cid:63). Our algorithm
avoids the sublinearly convergent Phase I by properly selecting t0, η and x0, and follows a diﬀerent central path
generated by the solution trajectory of the re-parameterized barrier problem (blue curve).

on that matter. The eﬃciency of such algorithms strongly depends on the computation of proxg. In
addition, since (33) is strongly convex, restart strategies, as in [36, 44] for ﬁrst order methods, can
achieve fast convergence rate. When g is absent, (33) reduces to a positive deﬁnite linear system
Hkd = −hk, which can be eﬃciently solved by conjugate gradient scheme or Cholesky methods.

Concluding, we state that problem (33) has special features, which can be exploited in practice:
(i) Often, both matrices Hk and its inverse H−1
are available, which allow us to estimate both
the Lipschitz constant of ∇q and the strong convexity parameter of q. Hence, one can design
accelerated gradient methods that have linear convergence rate.
(ii) By following a “warm-start” strategy, i.e., each iteration is initialized with the previously

k

computed estimate, (33) quickly reaches a high accuracy solution in a few iterations.

The analytical center point. To obtain the theoretical complexity bound of Theorem 3, we
require the computation of the analytical center ¯x(cid:63)
f of the barrier function f . While computing ¯x(cid:63)
f
might be challenging for some problem instances, there are several practical cases where can be
i ), i.e., f is the barrier of the box set
f = 0 ∈ Rp. In the case of f (X) := − log det(X) −
+ denotes the set of positive semi-
f = 0.5U , where ¯X (cid:63)
f denotes the analytical center
f can be computed after a few Newton iterations. More details

computed analytically. For example, if f (x) := −(cid:80)p
X := {x ∈ Rp : −1 ≤ xi ≤ 1, i = 1,··· , p}, then ¯x(cid:63)
log det(U − X) for the set X := {X ∈ Sp
deﬁnite matrices in p× p dimensions, we have ¯X (cid:63)
in a matrix form. In general cases, ¯x(cid:63)
on computation of ¯x(cid:63)

i=1 log(1− x2
+ : 0 (cid:22) X (cid:22) U}, where Sp

f can be found in [30, 33].

Next, we study three numerical examples. We ﬁrst compare with the two-phase algorithm in
[47]; then, we compare Algorithm 1 with some oﬀ-the-shelf interior-point solvers such as SDPT3
[46], SeDuMi [43] and Mosek [2].

(cid:9)(cid:24)(cid:19)(cid:23)(cid:28)(cid:27)(cid:1)(cid:24)(cid:23)(cid:1)(cid:13)((cid:23)(cid:28)(cid:26)(cid:11)(cid:21)(cid:1)(cid:25)(cid:11)(cid:28)(cid:18)(cid:27)(cid:5)((cid:23)(cid:28)(cid:26)(cid:11)(cid:21)(cid:1)(cid:25)(cid:11)(cid:28)(cid:18)(cid:1)(cid:24))(cid:1)(cid:28)(cid:18)((cid:1)(cid:26)((cid:4)(cid:25)(cid:11)(cid:26)(cid:11)(cid:22)((cid:28)((cid:26)(cid:19)(cid:34)((cid:14)(cid:1)(cid:25)(cid:26)(cid:24)(cid:12)(cid:21)((cid:22)(cid:10)(cid:24)(cid:21)(cid:29)(cid:28)(cid:19)(cid:24)(cid:23)(cid:27)(cid:1)(cid:13)(cid:24)(cid:22)(cid:25)(cid:29)(cid:28)((cid:14)(cid:1)(cid:12)(cid:33)(cid:1)(cid:24)(cid:23)((cid:1)(cid:19)(cid:23)((cid:32)(cid:11)(cid:13)(cid:28)(cid:1)(cid:25)(cid:26)(cid:24)(cid:32)(cid:19)(cid:22)(cid:11)(cid:21)(cid:1)(cid:8)((cid:31)(cid:28)(cid:24)(cid:23)(cid:1)(cid:19)(cid:28)((cid:26)(cid:11)(cid:28)(cid:19)(cid:24)(cid:23)···¯x?t0¯x?t1¯x?tkx?x0x1x2xkx?t0x?t1x?tk¯x02 (cid:9)(cid:18)(cid:11)(cid:27)((cid:1)(cid:6)(cid:6)(cid:10)(cid:19)(cid:23)(cid:17)(cid:21)((cid:1)(cid:25)(cid:18)(cid:11)(cid:27)((cid:5)((cid:23)(cid:28)(cid:26)(cid:11)(cid:21)(cid:1)(cid:25)(cid:11)(cid:28)(cid:18)(cid:1)(cid:24))(cid:1)(cid:28)(cid:18)((cid:1)(cid:24)(cid:26)(cid:19)(cid:17)(cid:19)(cid:23)(cid:11)(cid:21)(cid:1)(cid:25)(cid:26)(cid:24)(cid:12)(cid:21)((cid:22)x?=argminx2X{G(x):=hc,xi+g(x)}(cid:7)((cid:30)((cid:21)(cid:1)(cid:27)((cid:28)(cid:27)(cid:1)(cid:24))(cid:1)(cid:28)(cid:18)((cid:1)(cid:24)(cid:12)-((cid:13)(cid:28)(cid:19)(cid:30)((cid:1)(cid:24))(cid:1)(cid:28)(cid:18)((cid:1)(cid:12)(cid:11)(cid:26)(cid:26)(cid:19)((cid:26)(cid:1)(cid:25)(cid:26)(cid:24)(cid:12)(cid:21)((cid:22)¯x1¯x2··· Ft0(x0) Ft0(¯x⇤t0)! (1 )  ⌫iterations···(cid:9)(cid:18)(cid:11)(cid:27)((cid:1)(cid:6)(cid:1)(cid:1)(cid:2)(cid:27)(cid:29)(cid:12)(cid:21)(cid:19)(cid:23)((cid:11)(cid:26)(cid:1)(cid:26)(cid:11)(cid:28)(	 14

Q. Tran-Dinh, A. Kyrillidis, V. Cevher: A single-phase, proximal path-following framework
Technical Report 00(0), pp. 000–000, © 0000 INFORMS

5.1. The Max-Cut problem.

In this example, we consider the SDP relaxation of the well-

known Max-Cut problem as a test case. In particular, consider the following problem:

X {(1/4)(cid:104)L, X(cid:105) : X (cid:23) 0, diag(X) = e} ,
max

(34)

+ is the positive semi-deﬁnite optimization variable, L is the Laplacian matrix of
the corresponding underlying graph of the problem, diag(X) is the diagonal of X and e :=

where X ∈ Sp
(1, 1,··· , 1)(cid:62) ∈ Rp. The purpose of this section is to compare Algorithm 1 with the two-phase algo-

rithm in [47]. We note that in the latter case, the algorithm is also an inexact proximal interior
point method, that follows a two-phase procedure.
the feasible set X :=
If we deﬁne c := −(1/4)L, g(X) := δX (X),
{X ∈ Sp
+ : diag(X) = e}, then (34) can be reformulated into (1). In this case, the proximal operator
of g is just the projection onto the aﬃne subspace X , which can be computed in a closed form.
Moreover, (17) can be solved in a closed from: it requires only one Cholesky decomposition and
two matrix-matrix multiplications.

the indicator of

SDPT3(cid:107)F
Table 1. Summary of results on the small-sized Max-Cut problems. Here, Error := (cid:107)X k − X (cid:63)
and f (cid:63)
SDPT3 denotes the objective value obtained by using IPM solver SDPT3 [46] with high accuracy. For the case
of [47], the two quantities in Iters column denote the number of iterations required for Phase I and Phase II,
respectively. g05 n.0 is for unweighted graphs with edge probability 0.5; pm1s 100.0 is for a weighted graph with
edge weights chosen uniformly from {−1, 0, 1} and density 0.1; wd09 100.0 is for a 0.1 density ten graph with integer
edge weights chosen from [−10, 10]; t2g20 5555 is for each dimension three two-dimensional toroidal grid graphs with
gaussian distributed weights and dimension 20× 20; t3g7 5555 is for each dimension three three-dimensional toroidal
grid graphs with gaussian distributed weights and dimension 7 × 7 × 7. In these two last problems, the adjacency

matrix A is normalized by(cid:112)max|Aij|.

SDPT3(cid:107)F /(cid:107)X (cid:63)

[47]

Algorithm 1

Name

p

f (cid:63)
SDPT3

60
80

-59.00
g05 60.0
g05 80.0
-80.00
g05 100.0 100 -100.00
-52.58
pm1s 100.0 100
w09 100.0 100
-80.75
t3g7 5555 343 -20620.30
t2g20 5555 400 -31163.19

f (X)

Error

Iters Time[s]

f (X)

Error

Iters Time[s]

-58.94 4.35e-03 160/680
-79.92 4.38e-03 292/772
-99.90 4.41e-03 351/877
-52.52 3.76e-03 233/1015
-80.67 4.20e-03 729/968
107/32
159/99

-20616.76 4.45e-03
-31153.93 1.33e-02

0.40
0.63
0.94
1.40
1.30
2.23
3.41

704
-58.94 4.35e-03
799
-79.92 4.39e-03
-99.90 4.38e-03
910
-52.52 3.77e-03 1042
-80.67 4.21e-03
996
89
-20599.78 1.99e-03
-31154.04 1.24e-02
157

0.32
0.48
0.75
0.85
0.87
1.30
2.21

We test both algorithms on 7 small-sized Max-Cut problems generated by Rudy3. We also
consider 4 medium-sized problems from the Gset data set4, which were also generated from Rudy.
Both algorithms are tested in Matlab R2015a environment, running on a MacBook Pro. Laptop
2.6GHz Intel Core i7 with 16GB memory. The initial value of t0 is set at t0 := 0.025 for both cases.
We terminate the execution if |f (X k)− f (cid:63)
The results are provided in Tables 1-2. Algorithm 1 outperforms [47] in terms of total computa-
tional time, while achieving the same, if not better, solution w.r.t. objective value. We observe the
following trade-oﬀ w.r.t. the algorithm in [47]: if we increase the initial value of t0 in [47], then the
number of iterations in Phase I is deceasing, but the number of iterations in Phase II is increasing.
We emphasize that both algorithms use the worst-case update rule without any line-search on the
step-size as in oﬀ-the-shelf solvers.

SDPT3| ≤ 10−3, where f (X) := −trace(LX).

SDPT3|/|f (cid:63)

3 http://biqmac.uni-klu.ac.at/biqmaclib.
4 http://www.cise.ufl.edu/research/sparse/matrices/Gset/index.html.

Q. Tran-Dinh, A. Kyrillidis, V. Cevher: A single-phase, proximal path-following framework
Technical Report 00(0), pp. 000–000, © 0000 INFORMS

15

Table 2. Summary of results on the medium-sized Max-Cut problems. Here, Error := (cid:107)X k− X (cid:63)
SDPT3(cid:107)F
and f (cid:63)
SDPT3 denotes the objective value obtained by using IPM solver SDPT3 [46] with high accuracy. For the case
of [47], the two quantities in Iters column denote the number of iterations required for Phase I and Phase II,
respectively. Each problem Gxx is sparse with %1 to %3 upper triangle nonzero, binary entries.

SDPT3(cid:107)F /(cid:107)X (cid:63)

[47]

Algorithm 1

Name (Gxx)

p

f (cid:63)
SDPT3

f (X)

Error

Iters Time[s]

f (X)

Error

Iters Time[s]

G01
G43
G22
G48

800 -12080.12
1000 -7029.29
2000 -14116.01
3000 -5998.57

104.48
-12080.12 1.46e-02 149/805
208.82
-7029.30 2.03e-02 153/1031
-14116.03 3.57e-02 215/623
805.99
-5998.57 1.38e-02 225/2893 8487.08

62.43
-12080.13 1.46e-02
143.00
-7029.30 2.03e-02
-14116.06 3.57e-02
741.90
-5998.59 1.40e-02 1978 7978.35

569
712
561

5.2. The Max-k-Cut problem. Here, we consider the SDP relaxation of the Max-k-Cut

problem, proposed in [14, eq. (3)]:

max

X (cid:26) k − 1

2k (cid:104)L, X(cid:105) : X (cid:23) 0, diag(X) = e, X ≥ −

Ep(cid:27) ,

1
k − 1

(35)

where L is the Laplacian matrix of the corresponding graph, e := (1, 1,··· , 1)T , and Ep is
the p × p all-ones matrix. Observe that X ≥ Y , for two matrices X, Y , correspond to entry-
2k L and g(X) := δX (X) with X :=
wise inequality. Similarly to (34),

if we deﬁne c := − (k−1)

k−1 Ep(cid:111), (35) is a special instance of the class of problems described

+ : diag(X) = e, X ≥ − 1

(cid:110)X ∈ Sp

by (1).

We compare Algorithm 1 with three well-established, oﬀ-the-shelf interior-point solvers: SDPT3
[46], SeDuMi [43]5, and Mosek [2]6. We consider synthetically generated p-node graphs, where each
edge is generated from a Bern(1/4, 3/4) probability distribution; we also set k = 4. The parameters
of Algorithm 1 are set as in the previous example, and all algorithms are terminated if |f (X k) −
f (cid:63)|/|f (cid:63)| ≤ 10−5, where f (cid:63) is the best optimal value produced by three oﬀ-the-shelf solvers. We solve
(17) with a fast projected gradient method, with adaptive restart and a warm-start strategy [44]:
Such conﬁguration requires few iterations to achieve our desired accuracy δ = 2.8× 10−2 or higher.
Table 3. Comparison results on the Max-k-Cut problem. Here, Iters is the number of iterations; Time[s] is the
computational time in second; f (X) = −trace(LX); svars is the number of slack variables; and cnstr is the number
of linear constraints. In addition, we have p(p + 1)/2 variables in X and one SDP constraint.

Size

Lifting

Algorithm 1

SeDuMi

SDPT3

Mosek

p

svars cnstr

f (X)

Time[s]

f (X)

Time[s]

f (X)

Time[s]

f (X)

Time[s]

1,225 1,275
50
75
2,775 2,850
100 4,950 5,050
150 11,175 11,325
200 19,900 20,100

-86.174
4.76
-87.733
-166.236
55.41
-166.237
-316.746
732.16
-316.741
-654.703
-654.684 5,121.34
-1185.784 169.08 -1185.783 25,521.39

7.32
9.80
18.37
73.63

2.02
-86.160
10.76
-166.214
48.67
-316.709
-654.539
484.46
-1185.760 2,122.91

3.84
-86.138
8.91
-166.214
26.63
-316.653
-654.673
366.36
-1185.647 2,048.95

Table 3 contains some experimental results. Observe that, if p is small, all algorithms perform
well with the oﬀ-the-shelf solvers returning faster a good solution. However, when p increases, their
computational time signiﬁcantly increases, as compared to Algorithm 1. One reason that this hap-
pens is that standard SDP solvers require p(p− 1)/2 slack variables and p(p− 1)/2 additional linear
5 Both implementations include Matlab and optimized C-coded parts.
6 Available for academic use at https://mosek.com.

16

Q. Tran-Dinh, A. Kyrillidis, V. Cevher: A single-phase, proximal path-following framework
Technical Report 00(0), pp. 000–000, © 0000 INFORMS

constraints, in order to process the component-wise inequality constraints. Such reformulation of
the problem signiﬁcantly increases variable and constraint size and, hence, lead slower execution. In
stark contrast, Algorithm 1 handles both linear and inequality constraints by a simple projection,
which requires only p(p + 1)/2 basic operations. Figure 2 graphically illustrates the scalability of
the four algorithms under comparison, based on the results contained in Table 3.

Figure 2. Overall execution time, as a function of problem dimension. Left panel: Max-k-Cut problem (35); Right
panel: Clustering problem (36).

5.3. Max-norm clustering.

In this last problem case, we consider the max-norm clustering
task [20], where we seek a clustering matrix K that minimizes the disagreement with a given aﬃnity
matrix A:

min

x:=[L,R,K]∈Rp×p (cid:107)vec(K − A)(cid:107)1
Q(x) :=(cid:20) L K

s.t.

K T R(cid:21) (cid:23) 0, Lii ≤ 1, Rii ≤ 1, i = 1,··· , p.

(36)

Here, vec is the vectorization operator of a matrix (i.e., vec(X) := (X T
n )T , where Xi is
the i-th column of X). Note that (36) is an SDP convex relaxation to the correlation clustering
problem; see [20] for details. While (36) comes with rigorous theoretical guarantees and can be
formulated as a standard conic program, we need to add O(p2) slack variables to process the
(cid:96)1-norm term and the linear constraints. Moreover, the scaling factors (e.g., the Nesterov-Todd
scaling factor regarding the semideﬁnite cone [34]) can create memory bottlenecks in practice, by
destroying the sparsity of the underlying problem (e.g., by leading to dense KKT matrices in the
Newton systems).

1 ,··· , X T

Here, we solve (36) using our path-following scheme. In particular, by deﬁning x := vec([K, L, R],
f (x) := − log det(Q(x)) and g(x) := (cid:107)vec(K − A)(cid:107)1 + δC(x), we can transform (36) into (1), where
δC is the indicator function of C := {x : Lii ≤ 1, Rii ≤ 1, i = 1,··· , p}.
We compare the following solvers: Algorithm 1, the two-phase algorithm in [47], SDPT3 and
Mosek. The initial penalty parameter t0 is set to t0 := 0.25 and the relative tolerance is ﬁxed at
10−4 for all algorithms. The data is generated as suggested in [20, 47]. The results of 5 test problem
instances are shown in Table 4 sizes p ranging from 50 to 2007.

7 Since SDPT3 and Mosek cannot run for bigger problems in our personal computer, we restrict to problem sizes up
to p = 200.

Problem dimension p5075100125150175200Overall computational time [second] (in log-scale)100101102103104105Algorithm 1SeDuMiSDPT3MosekProblem dimension p5075100125150175200Overall computational time [second] (in log-scale)100101102103104105Algorithm 1Two-phase algorithmSDPT3MosekQ. Tran-Dinh, A. Kyrillidis, V. Cevher: A single-phase, proximal path-following framework
Technical Report 00(0), pp. 000–000, © 0000 INFORMS

17

Table 4. The performance of Algorithm 1, as compared to three methods on the clustering problem (36). Here,
Time[s] is the computational time in second; f (X) = (cid:107)vec(K − A)(cid:107)1, and s% is the sparsity of K − A.

Size

Algorithm 1

[47]

SDPT3

Mosek

p

f (X) Time[s]

s%

f (X)

Time[s]

s%

f (X)

Time[s]

f (X)

Time[s]

50
75
100
150
200

49%

17.30
563.90
563.90
18.27
29.38 49.5%
563.86
1,308.19
59.30 43.8% 1,308.18
121.74
77.05 43.9% 1,308.15
2,228.62 114.59 35.8% 2,228.61
192.79 35.9% 2,228.59
975.10
5,328.12 344.29 42.4% 5,327.99
344.32 42.5% 5,327.84 4,584.03 5,328.14 11,665.52
9,883.92 899.10 45.8% 9,883.81 1,102.97 47.9% 9,883.68 35,974.60 9,884.21 62,835.42

563.92
47.40 1,308.32
334.76 2,228.78

9.60

Both SDPT3 and Mosek are approximately 40 and 60 times slower than Algorithm 1 and [47],
especially when p > 100. We note that such solvers require p2 + 2p slack variables, p2 additional sec-
ond order cone constraints and 2p additional linear constraints to reformulate (36) into a standard
SDP problem. Hence, the size of the resulting SDP problem is much larger than of the original
one in (36). We also see that Algorithm 1 is faster than the two-phase algorithm, in terms of total
execution time.

We note that SDPT3 gives a slightly better objective value than Algorithm 1. However, its
solution K is fully dense, in contrast to those of Algorithm 1 and [47], reducing its interpretation in
applications. Figure 2 (right) also reveals the scalability of these four algorithms for solving (36).

6. Conclusions.

In this work, we propose a new path-following framework for a, possibly
non-smooth, constrained convex minimization template, which includes linear programming as a
special case. For our framework, we assume that the constraints in the optimization template
are endowed with a self-concordant barrier and, the non-smooth term has a tractable proximity
operator. Our workhorse is a new re-parameterization of the optimality condition of the convex
optimization problem, which allows us to select a diﬀerent central path towards x(cid:63), without relying
on the sublinear convergent Phase I of proximal path-following approaches, as in [47].

We illustrate that the new scheme retains the same global, worst-case, iteration-complexity
with standard approaches [30, 33]. Moreover, we theoretically show that inexact solutions to sub-
problems do not sacriﬁce the worst-case complexity, when controlled appropriately. Finally, we
numerically illustrate the eﬀectiveness of our framework on Max-Cut and clustering problems,
where the proximal operator play a key role in space eﬃcient optimization.

Acknowledgments: We would like to thank Yurii Nesterov for useful discussions on the ini-
tialization technique used in this work. We also thank Cong Bang Vu for his overall suggestions on
the ﬁnal version of this manuscript. This work was supported in part by ERC Future Proof, SNF
200021-146750 and SNF CRSII2-147633.

7. Appendix: proofs of main results. This section contains proofs of technical results,

presented in the main text.

7.1. Proof of Lemma 1. Let x(cid:63)

t be the solution of (10) and ¯x(cid:63)
t ) ∈ ∂G(¯x(cid:63)
t − ¯x(cid:63)
t ), x(cid:63)

t be the solution of (7). By
t ) and −t∇hη(x(cid:63)
t ) ∈ ∂G(x(cid:63)
t ).
t(cid:105) ≥ 0. Using the deﬁnition

the optimality conditions in (8) and (11), we have −t∇f (¯x(cid:63)
Moreover, by the convexity of G, we have (cid:104)∇f (¯x(cid:63)
t ) − ∇hη(x(cid:63)
∇hη(x) := t∇f (x)− ηζ0, the last inequality leads to
t − ¯x(cid:63)

t )−∇f (¯x(cid:63)

(cid:104)∇f (x(cid:63)

t(cid:105) ≤ η(cid:104)ζ0, x(cid:63)

t ), x(cid:63)

t − ¯x(cid:63)
t(cid:105).

Further, by [30, Theorem 4.1.5] and the Cauchy-Schwarz inequality, this inequality implies

(cid:107)x(cid:63)
1 +(cid:107)x(cid:63)

t − ¯x(cid:63)
t(cid:107)¯x(cid:63)
t − ¯x(cid:63)
t(cid:107)¯x(cid:63)

t

t ≤ η(cid:107)ζ0(cid:107)∗¯x(cid:63)

t

=⇒ ¯∆t ≤

¯m0
1− ¯m0

,

(37)

18

Q. Tran-Dinh, A. Kyrillidis, V. Cevher: A single-phase, proximal path-following framework
Technical Report 00(0), pp. 000–000, © 0000 INFORMS

which completes the proof of this Lemma.

For the Corollary 1, one observes [30, Corollary 4.2.1] that (cid:107)ζ0(cid:107)∗¯x(cid:63)

t ≤ nν(cid:107)ζ0(cid:107)∗¯x(cid:63)
analytical center of f . Following the same motions, one can easily obtain (12).

f

, where ¯x(cid:63)

f is the
(cid:3)

7.2. Proof of Lemma 2. By deﬁnition of the local norm ¯λt(x), we have:

¯λt(x) = (cid:104)∇2f (¯x(cid:63)
≤ (cid:104)∇2f (¯x(cid:63)

t )(x− ¯x(cid:63)
t )(x(cid:63)
t − ¯x(cid:63)
t − ¯x(cid:63)
.

≤ ¯∆t +(cid:0)1−(cid:107)x(cid:63)
λt(x)
1− ¯∆t

= ¯∆t +

t(cid:105)1/2
t ), x− ¯x(cid:63)
t − ¯x(cid:63)
t ), x(cid:63)
t(cid:105)1/2 +(cid:104)∇2f (¯x(cid:63)
t )(x− x(cid:63)
(cid:104)∇2f (x(cid:63)
t(cid:107)¯x(cid:63)

t(cid:1)−1

t )(x− x(cid:63)
t ), x− x(cid:63)

t ), x− x(cid:63)
t(cid:105)1/2

t(cid:105)1/2

Here, in the ﬁrst inequality, we use the triangle inequality for the weighted norm (cid:107)·(cid:107)∇2f (x(cid:63)
),
tk+1
while in the second inequality we apply [30, Theorem 4.1.6]. The proof is completed when we use
(cid:3)
(12) to upper bound the above inequality.

such that: t0hη(x(cid:63)

7.3. Proof of Lemma 3. Since x(cid:63)
t0 = 0. Hence,
(ξ0 − ξ(cid:63)

t0) + c + ξ(cid:63)

t0) = rt0,η(x0)− t0[∇f (x0)−∇f (x(cid:63)

t0)].

t0 is the solution of (10) at t = t0, there exists ξ(cid:63)

t0 ∈ ∂g(x(cid:63)
t0)

By convexity of g, we have
0 ≤ (cid:104)ξ0 − ξ(cid:63)

This inequality leads to t0(cid:104)∇f (x0) − ∇f (x(cid:63)

t0), x0 − x(cid:63)

concordance of f in [30, Theorem 4.1.7] and the Cauchy-Schwarz inequality, we can derive

t0, x0 − x(cid:63)

t0(cid:105) = (cid:104)rt0,η(x0)− t0[∇f (x0)−∇f (x(cid:63)

t0)], x0 − x(cid:63)
t0(cid:105)
t0(cid:105) ≤ (cid:104)rt0,η(x0), x0 − x(cid:63)
t0(cid:105). Using the self-

t0λt0(x0)2
1 + λt0(x0) ≤ t0(cid:104)∇f (x0)−∇f (x(cid:63)
t0), x0 − x(cid:63)
t0(cid:105)
≤ (cid:104)rt0,η(x0), x0 − x(cid:63)
t0(cid:105)
≤ (cid:107)rt0,η(x0)(cid:107)∗x(cid:63)

λt0(x0).

t0

t0λt0 (x0)
1+λt0 (x0) ≤ (cid:107)rt0,η(x0)(cid:107)∗x(cid:63)
Hence,
∗
(cid:107)rt0,η (x0)(cid:107)
x0
1−λt0 (x0)

t0

. Combining these two inequalities, we obtain

. Moreover, by [30, Theorem 4.1.6], we have (cid:107)rt0,η(x0)(cid:107)∗x(cid:63)

t0 ≤

λt0(x0)(1− λt0(x0))

1 + λt0(x0)

≤ t−1

0 (cid:107)rt0,η(x0)(cid:107)∗x0

After few elementary calculations, we can easily show that if (cid:107)rt0,η(x0)(cid:107)∗x0 < t0(3 − 2√2), then we
(cid:3)

obtain (14), which also guarantees its right-hand side to be positive.

7.4. Proof of Lemma 4. Let λtk+1(xk+1

tk+1) and λtk+1(xk

tk ) be deﬁned by (22). It was proved

in [47, Theorem 3.3] that

λtk+1(xk+1
tk ) < 1 − 1/√2. Now, we consider the function m(t) := 3−2t

where λtk+1(xk
We can numerically check that if t ∈ [0, 0.118975] then m(t) ≤ 5. In this case, we also have

tk )2(cid:33) λtk+1(xk
1−4t+2t2 for t ∈ [0, 1 − 1/√2).
1−t ≤

3− 2λtk+1(xk
tk )

1− λtk+1(xk
tk )

1− 4λtk+1(xk

tk ) + 2λtk+1(xk

+(cid:32)

tk+1) ≤

tk )2,

(38)

δk

1

Q. Tran-Dinh, A. Kyrillidis, V. Cevher: A single-phase, proximal path-following framework
Technical Report 00(0), pp. 000–000, © 0000 INFORMS

19

1.135042. Using these upper bounds into (38), we obtain λtk+1(xk+1
which is exactly (23), whenever λtk+1(xk

tk ) ∈ [0, 0.118975].

The proof of the estimate (23) can be found in [47]. We only prove (24). We note that

tk+1) ≤ 1.135042δk + 5λtk+1(xk

tk )2,

λtk+1(xk

tk ) = (cid:104)∇2f (x(cid:63)
≤ (cid:104)∇2f (x(cid:63)

tk+1)(xk
tk+1)(x(cid:63)

tk − x(cid:63)
tk − x(cid:63)
tk − x(cid:63)
λtk (xk
tk )
,
1− ∆k

≤ ∆k +(cid:16)1−(cid:107)x(cid:63)

= ∆k +

tk+1), xk
tk+1), x(cid:63)
tk+1(cid:107)x(cid:63)

tk − x(cid:63)
tk − x(cid:63)

tk+1(cid:17)−1

tk+1(cid:105)1/2
tk+1(cid:105)1/2 +(cid:104)∇2f (x(cid:63)
(cid:104)∇2f (x(cid:63)
tk − x(cid:63)

tk )(xk

tk+1)(xk

tk ), xk

tk − x(cid:63)
tk − x(cid:63)

tk ), xk
tk(cid:105)1/2

tk − x(cid:63)

tk(cid:105)1/2

where is indeed (24). Here, in the ﬁrst inequality, we use the triangle inequality for the weighted
(cid:3)
norm (cid:107)·(cid:107)∇2f (x(cid:63)

), while in the second inequality we apply [30, Theorem 4.1.6].

tk+1

7.5. Proof of Lemma 5. Since x(cid:63)

tk and x(cid:63)

tk+1 are the solutions of (11) at t = tk and tk+1,

respectively, we have

tk ) + ∂G(x(cid:63)
tk ) and v(cid:63)

0 ∈ tk∇hη(x(cid:63)
tk ∈ ∂G(x(cid:63)

Hence, there exist v(cid:63)
−tk+1∇hη(x(cid:63)

tk ) and 0 ∈ tk+1∇hη(x(cid:63)
tk+1 ∈ ∂G(x(cid:63)
tk+1). Using the convexity of G, we have
(cid:104)tk+1∇hη(x(cid:63)
tk+1 − x(cid:63)
Hence, we can show that

tk+1)− tk∇hη(x(cid:63)

tk(cid:105) = −(cid:104)v(cid:63)

tk ), x(cid:63)

tk+1) + ∂G(x(cid:63)

tk+1).

tk+1) such that v(cid:63)

tk = −tk∇hη(x(cid:63)

tk ) and v(cid:63)

tk+1 =

tk+1 − v(cid:63)

tk , x(cid:63)

tk+1 − x(cid:63)

tk(cid:105) ≤ 0.

(cid:104)tk+1∇hη(x(cid:63)

tk+1)− tk∇hη(x(cid:63)

tk ), x(cid:63)

tk+1 − x(cid:63)

tk(cid:105) ≤ 0.

By the deﬁnition ∇hη and the update rule (21) of tk, we have

tk+1∇hη(x(cid:63)

tk+1)− tk∇hη(x(cid:63)

tk ) = tk[∇f (x(cid:63)

tk+1)−∇f (x(cid:63)

tk )] + dk∇f (x(cid:63)

tk+1).

(39)

(40)

Combining (39) and (40), then using [30, Theorem 4.1.7], the Cauchy-Schwarz inequality, and the
deﬁnition of ∆k in (22) we obtain

tk∆2
k
1 + ∆k ≤ tk(cid:104)∇f (x(cid:63)
≤ −dk(cid:104)∇f (x(cid:63)
≤ |dk|(cid:107)∇f (x(cid:63)

tk+1)−∇f (x(cid:63)

tk+1), x(cid:63)
tk+1)(cid:107)∗x(cid:63)

tk+1

tk+1 − x(cid:63)
tk ), x(cid:63)
tk(cid:105)
tk+1 − x(cid:63)
tk(cid:105)
∆k,

(41)

which implies the ﬁrst inequality of (25). The second inequality of (25) follows from the fact that
(cid:107)∇f (x(cid:63)
consequence of (25).

tk+1 ≤ √ν due to [30, formula 2.4.2]. The last statement of this lemma is a direct
(cid:3)

tk+1)(cid:107)∗x(cid:63)

7.6. Proof of Lemma 6. By (20) and given ˆFtk+1(xk+1

tk+1; xk

we have

tk+1; xk

tk ) ≤

tk+1δ2
2

k+1

,

tk ) − ˆFtk+1(¯xk+1
tk+1δ2
2

tk ) +

k+1

tk+1; xk

G(xk+1

tk+1) ≤ G(¯xk+1
= G(¯xk+1

tk+1) + tk+1Qk(¯xk+1
tk+1) + tk+1(cid:104)∇f (xk

tk+1; xk
tk ), ¯xk+1
tk+1 − xk

tk )− tk+1Qk(xk+1
tk+1 − xk+1
tk(cid:107)2

tk −(cid:107)xk+1

xk

tk+1 − xk

tk(cid:107)2

xk

+

tk+1

2 (cid:18)(cid:107)¯xk+1

tk+1(cid:105)− tk+1η(cid:104)ζ0, ¯xk+1

tk+1 − xk+1
tk+1(cid:105)
tk+1δ2
2

tk(cid:19) +

k+1

.

(42)

20

Q. Tran-Dinh, A. Kyrillidis, V. Cevher: A single-phase, proximal path-following framework
Technical Report 00(0), pp. 000–000, © 0000 INFORMS

Now, since ¯xk+1

tk+1 is the exact solution of (17), there exists ¯vk+1 ∈ ∂G(¯xk+1
tk+1 − xk

tk )− ηζ0(cid:1)− tk+1∇2f (xk

¯vk+1 = −tk+1(cid:0)∇f (xk

tk )(¯xk+1

tk ).

tk+1) such that

G(¯x(cid:63)

Next, using the convexity of G, with ¯vk+1 ∈ ∂G(¯xk+1
tk+1 − ¯xk+1
tk+1(cid:105)
tk+1 − ¯xk+1
tk ), ¯x(cid:63)
tk )(¯xk+1
tk+1 − xk

= −tk+1(cid:104)∇f (xk
− tk+1(cid:104)∇2f (xk

tk+1) ≥ (cid:104)¯vk+1, ¯x(cid:63)

tk+1)− G(¯xk+1

tk+1), we have

tk+1(cid:105) + tk+1η0(cid:104)ζ0, ¯x(cid:63)
tk+1 − ¯xk+1
tk ), ¯x(cid:63)
tk+1(cid:105)

tk+1 − ¯xk+1
tk+1(cid:105)

Summing up (42) and (44), and rearranging the result, we can derive

(43)

(44)

G(¯x(cid:63)

tk+1)− G(xk+1

xk

tk+1

2 (cid:18)(cid:107)¯xk+1

tk+1) ≥ −tk+1(cid:104)∇f (xk
− tk+1(cid:104)∇2f (xk
−
− tk+1(cid:104)∇f (xk
= −tk+1(cid:104)∇f (xk
− tk+1(cid:104)∇2f (xk
−

tk+1 − ¯xk+1
tk ), ¯x(cid:63)
tk+1 − xk
tk )(¯xk+1
tk(cid:107)2
tk+1 − xk
tk+1 − xk+1
tk ), ¯xk+1
tk ), ¯x(cid:63)
tk+1 − xk+1
tk )(¯xk+1
tk+1 − xk
tk(cid:107)2
tk+1 − xk

2 (cid:18)(cid:107)¯xk+1

tk+1

xk

tk+1(cid:105) + tk+1η(cid:104)ζ0, x(cid:63)
tk ), ¯x(cid:63)
tk+1 − ¯xk+1
tk+1(cid:105)
tk(cid:107)2
tk+1 − xk
tk −(cid:107)xk+1

xk

tk+1(cid:105) + tk+1η(cid:104)ζ0, ¯xk+1
tk+1(cid:105) + tk+1η(cid:104)ζ0, ¯x(cid:63)
tk+1 − ¯xk+1
tk ), ¯x(cid:63)
tk+1(cid:105)
tk+1 − xk
tk(cid:107)2
tk −(cid:107)xk+1

xk

tk(cid:19)−

tk+1 − ¯xk+1
tk+1(cid:105)

k+1

tk(cid:19)−

tk+1δ2
2
tk+1 − xk+1
tk+1(cid:105)
tk+1 − xk+1
tk+1(cid:105)

tk+1δ2
2

k+1

.

(45)

tk+1δ2
2

k+1

.

(46)

Now, by using the Cauchy-Schwarz inequality, we can further estimate (45) as

G(¯x(cid:63)

tk+1)− G(xk+1

tk+1(cid:107)¯x(cid:63)

tk+1) ≥ −tk+1(cid:107)∇f (xk
− tk+1 |η|(cid:107)ζ0(cid:107)∗¯x(cid:63)
− tk+1(cid:104)∇2f (xk
−

tk )(cid:107)∗¯x(cid:63)
tk+1(cid:107)¯x(cid:63)
tk+1 − xk
tk )(¯xk+1
tk(cid:107)2
tk+1 − xk

tk+1 − xk+1
tk ), ¯x(cid:63)
tk −(cid:107)xk+1

tk+1 − xk+1
tk+1(cid:107)¯x(cid:63)

2 (cid:18)(cid:107)¯xk+1

tk+1

xk

tk+1(cid:107)¯x(cid:63)

tk+1

tk+1

tk+1 − ¯xk+1
tk+1(cid:105)
tk(cid:107)2
tk+1 − xk

xk

tk(cid:19)−

We consider the term

T[1] := (cid:107)¯xk+1

+ 2(cid:104)∇2f (xk
Similarly to the proof of [47, Lemma 5.1], we can show that

tk −(cid:107)xk+1

tk+1 − xk

tk+1 − xk

tk(cid:107)2

tk(cid:107)2

xk
tk

xk

tk )(¯xk+1

tk+1 − xk

tk ), ¯x(cid:63)

tk+1 − ¯xk+1

tk+1(cid:105).

T[1] ≤

2¯λtk+1(xk
tk )
(1− ¯λtk+1(xk

tk ))2(cid:16)¯λtk+1(xk

tk ) + ¯λtk+1(xk+1

Next, by using the self-concordance of f and the deﬁnition of λt(x), we have

(cid:0)1− ¯λtk+1(xk
tk )(cid:1)2
On the one hand, using (48) and (cid:107)∇f (xk
tk+1 ≤ (1− ¯λtk+1(xk

tk ) (cid:22)(cid:0)1− ¯λtk+1(xk
tk ≤ √ν, we easily get
tk )(cid:107)∗xk
tk ))−1(cid:107)∇f (xk

tk+1) (cid:22) ∇2f (xk

(cid:107)∇f (xk

∇2f (¯x(cid:63)

tk )(cid:107)∗xk

tk )(cid:107)∗¯x(cid:63)

tk+1) + δk+1(cid:17) .
tk )(cid:1)−2
tk ≤ (1− ¯λtk+1(xk

∇2f (¯x(cid:63)

tk+1).

tk ))−1√ν.

(47)

(48)

(49)

Q. Tran-Dinh, A. Kyrillidis, V. Cevher: A single-phase, proximal path-following framework
Technical Report 00(0), pp. 000–000, © 0000 INFORMS

On the other hand, by [30, Corollary 4.1.7], we can show that

Substituting (48), (49) and (50) into (46), we ﬁnally obtain

(cid:107)ζ0(cid:107)∗¯x(cid:63)

tk+1 ≤ nν(cid:107)ζ0(cid:107)∗x(cid:63)

f

:= nνp0.

G(¯x(cid:63)

tk+1)− G(xk+1

tk+1) ≥ −tk+1(cid:32) √ν ¯λtk+1(xk+1
(1− ¯λtk+1(xk
¯λtk+1(xk
tk )
(1− ¯λtk+1(xk
which is (28) by combing with G(cid:63) − G(¯x(cid:63)
are proved as [47, Lemma 5.1].

+

tk+1)
tk ))2

21

(50)

δ2
k+1

2 (cid:33),

+ ηnνp0

¯λtk+1(xk+1
tk+1)

tk ))2(cid:16)¯λtk+1(xk

tk ) + ¯λtk+1(xk+1

tk+1) + δk+1(cid:17) +

tk+1) ≥ −νtk+1. The remaining statements of this lemma
(cid:3)

References
[1] M. S. Andersen, J. Dahl, Z. Liu, and L. Vandenberghe.

Interior-point methods for large-scale cone
programming. In: S. Sra, S. Nowozin, S. J. Wright (ed.) Optimization for Machine Learning, MIT
Press:55–83, 2011.

[2] MOSEK ApS. The MOSEK optimization toolbox for MATLAB manual. Version 7.1 (Revision 28).,

2015.

[3] Luca Baldassarre, Nirav Bhan, Volkan Cevher, Anastasios Kyrillidis, and Siddhartha Satpathi. Group-

sparse model selection: Hardness and relaxations. arXiv preprint arXiv:1303.3207, 2013.

[4] H.H. Bauschke and P. Combettes. Convex analysis and monotone operators theory in Hilbert spaces.

Springer-Verlag, 2011.

[5] A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding agorithm for linear inverse problems.

SIAM J. Imaging Sci., 2(1):183–202, 2009.

[6] S. Becker, E. J. Cand`es, and M. Grant. Templates for convex cone problems with applications to sparse

signal recovery. Math. Program. Compt., 3(3):165–218, 2011.

[7] A. Ben-Tal and A. Nemirovski. Lectures on modern convex optimization: Analysis, algorithms, and

engineering applications, volume 3 of MPS/SIAM Series on Optimization. SIAM, 2001.

[8] Dimitri P. Bertsekas. Constrained Optimization and Lagrange Multiplier Methods. Athena Scientiﬁc,

1996.

[9] S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein. Distributed optimization and statistical learning
via the alternating direction method of multipliers. Foundations and Trends in Machine Learning,
3(1):1–122, 2011.

[10] S. Boyd and L. Vandenberghe. Convex Optimization. University Press, Cambridge, 2004.

[11] Jian-Feng Cai, Emmanuel J Cand`es, and Zuowei Shen. A singular value thresholding algorithm for

matrix completion. SIAM Journal on Optimization, 20(4):1956–1982, 2010.

[12] E. Candes, Y. Eldar, T. Strohmer, and V. Voroninski. Phase retrieval via matrix completion. SIAM

Journal on Imaging Sciences, 6(1):199–225, 2013.

[13] P. Combettes and Pesquet J.-C. Signal recovery by proximal forward-backward splitting. In Fixed-Point

Algorithms for Inverse Problems in Science and Engineering, pages 185–212. Springer-Verlag, 2011.

[14] Alan Frieze and Mark Jerrum. Improved approximation algorithms for maxk-cut and max bisection.

Algorithmica, 18(1):67–81, 1997.

[15] Roy Frostig, Rong Ge, Sham M Kakade, and Aaron Sidford. Competing with the empirical risk minimizer

in a single pass. arXiv preprint arXiv:1412.6606, 2014.

[16] J. Gondzio.

Interior point methods 25 years later. European Journal of Operational Research,

218(3):587–601, 2012.

22

Q. Tran-Dinh, A. Kyrillidis, V. Cevher: A single-phase, proximal path-following framework
Technical Report 00(0), pp. 000–000, © 0000 INFORMS

[17] A. Gramfort and M. Kowalski. Improving M/EEG source localizationwith an inter-condition sparse

prior. In IEEE International Symposium on Biomedical Imaging, 2009.

[18] M. Grant, S. Boyd, and Y. Ye. Disciplined convex programming. In L. Liberti and N. Maculan, editors,
Global Optimization: From Theory to Implementation, Nonconvex Optimization and its Applications,
pages 155–210. Springer, 2006.

[19] K. Jaganathan, S. Oymak, and B. Hassibi. Sparse phase retrieval: Uniqueness guarantees and recovery

algorithms. arXiv preprint arXiv:1311.2745, 2013.

[20] A. Jalali and N. Srebro. Clustering using max-norm constrained optimization. In Proc. of International

Conference on Machine Learning (ICML2012), pages 1–17, 2012.

[21] R. Jenatton, A. Gramfort, V. Michel, G. Obozinski, F. Bach, and B. Thirion. Multi-scale mining of
fmri data with hierarchical structured sparsity. In Pattern Recognition in NeuroImaging (PRNI), 2011.

[22] Rodolphe Jenatton, Jean-Yves Audibert, and Francis Bach. Structured variable selection with sparsity-

inducing norms. The Journal of Machine Learning Research, 12:2777–2824, 2011.

[23] Anastasios Kyrillidis, Luca Baldassarre, Marwa El Halabi, Quoc Tran-Dinh, and Volkan Cevher. Struc-
In Compressed Sensing and its Applications, pages

tured sparsity: Discrete and convex approaches.
341–387. Springer, 2015.

[24] Anastasios Kyrillidis, Rabeeh Karimi Mahabadi, Quoc Tran Dinh, and Volkan Cevher. Scalable sparse
covariance estimation via self-concordance. In Proceedings of the Twenty-Eighth AAAI Conference on
Artiﬁcial Intelligence, 2014.

[25] J. L¨oefberg. YALMIP : A Toolbox for Modeling and Optimization in MATLAB. In Proceedings of the

CACSD Conference, Taipei, Taiwan, 2004.

[26] RP Millane. Phase retrieval in crystallography and optics. JOSA A, 7(3):394–411, 1990.

[27] A. Nemirovski and A. Shapiro. Convex approximations of chance constrained programs. SIAM Journal

on Optimization, 17(4):969–996, 2006.

[28] A. Nemirovski and M. J. Todd. Interior-point methods for optimization. Acta Numerica, 17(1):191–234,

2008.

[29] Arkadi Nemirovski. Lectures on modern convex optimization. In Society for Industrial and Applied

Mathematics (SIAM). Citeseer, 2001.

[30] Y. Nesterov. Introductory lectures on convex optimization: A basic course, volume 87 of Applied Opti-

mization. Kluwer Academic Publishers, 2004.

[31] Y. Nesterov. Barrier subgradient method. Math. Program., Ser. B, 127:31–56, 2011.

[32] Y. Nesterov. Gradient methods for minimizing composite objective function. Math. Program.,

140(1):125–161, 2013.

[33] Y. Nesterov and A. Nemirovski. Interior-point Polynomial Algorithms in Convex Programming. Society

for Industrial Mathematics, 1994.

[34] Y. Nesterov and M.J. Todd. Self-scaled barriers and interior-point methods for convex programming.

Math. Oper. Research, 22(1):1–42, 1997.

[35] J. Nocedal and S.J. Wright. Numerical Optimization. Springer Series in Operations Research and

Financial Engineering. Springer, 2 edition, 2006.

[36] B. O’Donoghue and E. Candes. Adaptive Restart for Accelerated Gradient Schemes. Found. Comput.

Math., 15:715–732, April 2015.

[37] N. Parikh and S. Boyd. Proximal algorithms. Foundations and Trends in Optimization, 1(3):123–231,

2013.

[38] F. Rapaport, E. Barillot, and J.P. Vert. Classiﬁcation of arrayCGH data using fused SVM. Bioinfor-

matics, 24(13):i375–i382, 2008.

[39] J. Renegar. A Mathematical View of Interior-Point Methods in Convex Optimization, volume 2 of

MPS/SIAM Series on Optimization. SIAM, 2001.

Q. Tran-Dinh, A. Kyrillidis, V. Cevher: A single-phase, proximal path-following framework
Technical Report 00(0), pp. 000–000, © 0000 INFORMS

23

[40] R. T. Rockafellar. Convex Analysis, volume 28 of Princeton Mathematics Series. Princeton University

Press, 1970.

[41] R.T. Rockafellar and R. J-B. Wets. Variational Analysis. Springer-Verlag, 1997.

[42] C. Roos, T. Terlaky, and J.-Ph. Vial. Interior Point Methods for Linear Optimization. Springer Science,
(Note: This book is a signiﬁcantly revised new edition of Interior Point

Heidelberg/Boston, 2006.
Approach to Linear Optimization: Theory and Algorithms).

[43] F. Sturm. Using SeDuMi 1.02: A Matlab toolbox for optimization over symmetric cones. Optim. Methods

Software, 11-12:625–653, 1999.

[44] Weijie Su, Stephen Boyd, and Emmanuel Candes. A diﬀerential equation for modeling nesterovs accel-
erated gradient method: Theory and insights. In Advances in Neural Information Processing Systems,
pages 2510–2518, 2014.

[45] A. Subramanian, P. Tamayo, V.K. Mootha, S. Mukherjee, B.L. Ebert, M.A. Gillette, A. Paulovich, S.L.
Pomeroy, T.R. Golub, and E.S. Lander. Gene set enrichment analysis: a knowledge-based approach for
interpreting genome-wide expression proﬁles. Proceedings of the National Academy of Sciences of the
United States of America, 102(43):15545–15550, 2005.

[46] K.-Ch. Toh, M.J. Todd, and R.H. T´’ut¨unc¨u. On the implementation and usage of SDPT3 – a Mat-
lab software package for semideﬁnite-quadratic-linear programming, version 4.0. Tech. report, NUS
Singapore, 2010.

[47] Q. Tran-Dinh, A. Kyrillidis, and V. Cevher. An inexact proximal path-following algorithm for con-

strained convex minimization. SIAM J. Optim., 24(4):1718–1745, 2014.

[48] Q. Tran-Dinh, A. Kyrillidis, and V. Cevher. Composite self-concordant minimization. J. Mach. Learn.

Res., 15:374–416, 2015.

[49] Quoc Tran Dinh, Anastasios Kyrillidis, and Volkan Cevher. A proximal Newton framework for composite
minimization: Graph learning without Cholesky decompositions and matrix inversions. In Proceedings
of The 30th International Conference on Machine Learning, pages 271–279, 2013.

[50] R.H. T¨ut¨unk¨u, K.C. Toh, and M.J. Todd. Solving semideﬁnite-quadratic-linear programs using SDPT3.

Math. Program., 95:189–217, 2003.

[51] Robert Vanderbei, Han Liu, Lie Wang, and Kevin Lin. Optimization for compressed sensing: the simplex

method and kronecker sparsiﬁcation. arXiv preprint arXiv:1312.4426, 2013.

[52] S.J. Wright. Primal-Dual Interior-Point Methods. SIAM Publications, Philadelphia, 1997.

[53] H. Zhou, M.E. Sehl, J.S. Sinsheimer, and K. Lange. Association screening of common and rare genetic

variants by penalized regression. Bioinformatics, 26(19):2375, 2010.

