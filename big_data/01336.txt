6
1
0
2

 
r
a

M
4

 

 
 
]

R

I
.
s
c
[
 
 

1
v
6
3
3
1
0

.

3
0
6
1
:
v
i
X
r
a

Simpliﬁed Relative Citation Ratio

for Static Paper Ranking

UFMG/LATIN at WSDM Cup 2016

Sabir Ribas
CS Dept, UFMG

Belo Horizonte, Brazil
sabir@dcc.ufmg.br

Alberto Ueda
CS Dept, UFMG

Belo Horizonte, Brazil
ueda@dcc.ufmg.br

Rodrygo L. T. Santos

CS Dept, UFMG

Belo Horizonte, Brazil

rodrygo@dcc.ufmg.br

Berthier Ribeiro-Neto

CS Dept, UFMG & Google Inc

Belo Horizonte, Brazil

berthier@dcc.ufmg.br

Nivio Ziviani

CS Dept, UFMG & Zunnit Tech

Belo Horizonte, Brazil
nivio@dcc.ufmg.br

ABSTRACT
Static rankings of papers play a key role in the academic
search setting. Many features are commonly used in the lit-
erature to produce such rankings, some examples are citation-
based metrics, distinct applications of PageRank, among
others. More recently, learning to rank techniques have been
successfully applied to combine sets of features producing ef-
fective results. In this work, we propose the metric S-RCR,
which is a simpliﬁed version of a metric called Relative Cita-
tion Ratio — both based on the idea of a co-citation network.
When compared to the classical version, our simpliﬁcation
S-RCR leads to improved eﬃciency with a reasonable eﬀec-
tiveness. We use S-RCR to rank over 120 million papers in
the Microsoft Academic Graph dataset. By using this single
feature, which has no parameters and does not need to be
tuned, our team was able to reach the 3rd position in the
ﬁrst phase of the WSDM Cup 2016.

Keywords
Ranking Papers; Relative Citation Ratio; WSDM Cup

1. RANKING PAPERS

Finding the most relevant papers of a ﬁeld of knowledge
is a task with many motivations. From the researcher’s per-
spective, it is important for instance to quickly discern the
papers with major impact in his/her study area from those
with less relevance. On the other hand, from an academic
search engine perspective, a common task is to present the
papers by using rankings, which demands a sort criterion as
relevance. Also, establishing a relative order of importance
of papers could help in other tasks such as providing grants
or research awards for individual researchers and graduate

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
WSDM’16, February 22–25, 2016, San Francisco, CA, USA.
c(cid:13) 2016 ACM. ISBN 978-1-4503-3716-8/16/02. . . $15.00
DOI: http://dx.doi.org/XXXX.XXXX

programs. The problem of ranking papers was addressed in
the WSDM Cup 2016, a competition that brought together
32 research teams from all over the world.
1.1 The Competition — WSDM Cup 2016

The WSDM Cup Ranker Challenge1 was created by the

WSDM2 organizers and supported by Microsoft Research.

1.1.1 The Task

The task for each competitor in WSDM Cup 2016 was to
provide the best static rank values for publication entities
in the Microsoft Academic Graph3 (MAG) [10]. The goal
behind it was to assess the query-independent importance
of academic papers.

1.1.2 Evaluation

The evaluation in WSDM Cup 2016 was conducted in two
phases, as we now describe. During Phase 1, submissions
were scored based on the agreements with human judge-
ment data. A group of Computer Science researchers were
invited by the organizers to conduct a pairwise ranking of
papers in the ﬁelds they actively conduct research. The pair-
wise judgement data were then randomly segregated into an
Evaluation and a Test set. Submissions during Phase 1 were
automatically scored against the Evaluation set and added
to a public leaderboard that was sorted based on the per-
centage of agreements with the judgement data. At the end
of Phase 1, the most recent submission from each team was
evaluated against the Test set and the scores (ranked by the
percentage agreements with the Test set) were announced
to the leaderboard.

The top eight teams on the leaderboard at the end of
Phase 1 were invited to participate in Phase 2. Each partic-
ipant of Phase 2 was asked to re-run the algorithms over an
updated graph and to submit the ﬁnal rank values. Phase 2
of the Challenge was conducted by Microsoft Research in
cooperation with Bing. Each of the ﬁnalist results was ap-
plied to Bing search results and powered the ranker used by
Bing for academic queries.

1http://wsdmcupchallenge.azurewebsites.net
2http://www.wsdm-conference.org/2016/
3http://research.microsoft.com/en-us/projects/mag/

1.2 This Report

In this paper, we report the participation of our team,
named UFMG/LATIN, in the WSDM Cup 2016. Before
getting into the ﬁnal model, we performed a set of tests con-
sidering distinct approaches, which include distinct citation-
based metrics, PageRank, among others. Our ﬁnal approach
was based on a simpliﬁed version of a metric called Relative
Citation Ratio [4]. In here, we describe this metric, how and
why we choose to run a simpliﬁed version.

The remainder of this paper is organized as follows.

In
Section 2, we present the related works on paper rankings.
In Section 3, we describe the RCR metric as well as our
simpliﬁed version, the S-RCR. In Section 4, we discuss some
additional techniques we applied to rank papers.
In Sec-
tion 5, we report our experiments. Finally, in Section 6, we
provide a ﬁnal discussion and concluding remarks.

2. LITERATURE ON PAPER RANKINGS

The most common approach to produce paper rankings
in the literature is by using citation-based metrics. These
metrics provide a natural way to reason about the relative
quality of academic entities, such as scientiﬁc papers, indi-
vidual researchers and publication venues. One of the ear-
liest metrics proposed to quantify academic impact was the
Impact Factor [1]. Since them, many alternatives have been
proposed, including other citation-based metrics like the H-
Index [3], random walks and machine learning techniques.

Another common approach to rank academic entities is
by considering the structural information. The structure
of the citation network can be used to produce academic
rankings by applying random walk techniques, such as the
PageRank [6]. A natural approach is to apply random walks
in the paper-paper citation network. However, some authors
also apply random walks in heterogeneous graphs. In [9], for
example, the authors propose a novel random walk model
to identify the most reputable entities of a domain based on
a conceptual framework of reputation ﬂows.

Another concept that is worth mentioning is the Altmet-
rics movement, which points out the need for novel evalua-
tion metrics as alternative to classic citation-based metrics.
According to Piwowar et al. [7], citation-based metrics are
useful, but not suﬃcient to evaluate research. In particular,
they observe that citations are slow — their main argument
is the fact that a paper’s ﬁrst citation can take years.

Learning to rank techniques [5] have been used over the
last few years to improve the quality of rankings by eﬀec-
tively combining multiple sources of evidence. The large
amount of available features related to some ranking tasks
motivates the adoption of learning to rank methods in dis-
tinct contexts, including in academic search.

Our approach is inspired by the work of Hutchins et al. [4],
which proposes a metric called Relative Citation Ratio. It
is a paper-level and ﬁeld-independent score that provides
an alternative to classic citation-based metrics to identify
inﬂuential papers. They show that the rankings produced by
their metric strongly correlate with the opinions of experts
in biomedical research and suggest that the same approach
should be generally applicable in all areas of science.

While complex models, such as heterogeneous random
walks or learning to rank methods, are able to produce ef-
fective results, in this work, we investigate a single well-
designed feature to rank papers given its citation network.

3. RELATIVE CITATION RATIO

In this section, we describe the metric we propose, the
S-RCR, which is a simpliﬁcation of a metric called Rela-
tive Citation Ratio (RCR) [4]. Before describing S-RCR it
is worth reasoning about the basic concepts of the original
RCR metric. The RCR metric is based upon the idea of
using the co-citation network of each paper to normalize it
in terms of time and area of study, by calculating an ex-
pected citation rate of the target paper from the aggregated
citation behavior of its neighborhood. Basically, this strat-
egy consists of computing the average citation rate of this
neighborhood which is used as the RCR denominator; the
numerator is the citation rate of the target paper.

3.1 Co-citation Network

The basis of the RCR metric and our proposed simpliﬁca-
tion is the notion of co-citation network. Hutchins et al. [4]
deﬁne this co-citation network as the papers’ area of inﬂu-
ence. As described in [4], when a paper is ﬁrst cited, the
other papers appearing in the reference list along with this
paper comprise its co-citation network, see Figure 1. As the
paper continues to be cited, the papers appearing in the new
reference lists alongside it are added to its co-citation net-
work. This network provides a dynamic view of the paper’s
ﬁeld of research, taking advantage of information provided
by the experts who have found the study useful enough to
cite. The co-citation network of a paper can be viewed as a
representative sample of its area of research allowing us to
perform a reasonable cross-ﬁeld evaluation of papers.

e
l
c
i
t
r

A
 
e
c
n
e
r
e
f
e
R

RA

N

N

RA

N

N

Papers 
citing RA

Co-citation

network

Papers 

cited by RA

Figure 1: Schematic view of a co-citation network [4]

In Figure 1, we present the schematic view of a co-citation
network for the RCR computation. The Reference Article
(RA, in red) cites previous papers from the literature (in
orange) and other papers (in blue) cite the RA. The co-
citation network (or neighborhood) of the Reference Article
is the set of papers (in green) that appear alongside the RA.

3.2 Article Citation Ratio

The Article Citation Ratio (ACR) of a given paper p is

deﬁned as:

ACR(p) =

Citations(p)
Age(p) + 1

,

(1)

where Citations(p) is the total number of citations paper p
received since its publication and Age(p) is the time in years
since the publication date of paper p.

By keeping a counter of citations and paper ages, we can
store the data in a hash table and compute the ACR of a
paper p in Θ(1) time complexity.

3.3 The Simpliﬁed RCR

5. EXPERIMENTS

To produce ﬁeld independent rankings, RCR metrics nor-
malize the ACR of a paper p based on the information of
its co-citation network. In the original version of RCR [4],
this information is used through a complex normalization
process. Computing the original RCR of a single paper p
depends on performing a linear regression on its co-citation
network using the journal citation ratio [2] of the venues p’s
neighbors were published. For a full description of the RCR
metric, we refer the reader to the work of Hutchins et al. [4].
In particular, we deﬁne the Simpliﬁed Relative Citation

Ratio (S-RCR) of a given paper p as follows:

S-RCR(p) =

ACR(p)
(1/|Np|) Pp′ ∈Np

ACR(p′)

,

(2)

where ACR(p) is the Article Citation Ratio (see Section 3.2)
of paper p and Np is the set of neighbors of paper p (see Sec-
tion 3.1). Similarly to the classic RCR, the numerator is the
ACR of paper p and the denominator acts as a normalizer,
forcing the ACR of paper p to be relative to its neighbors.
The main diﬀerence between our proposal and the original
RCR is its normalization step, which, in our metric, is much
simpler. Speciﬁcally, we normalize the ACR of a paper p
by the average of the ACR values of p’s neighbors. While
this simpliﬁcation is brieﬂy mentioned in the original paper,
the authors discard it for its numerical limitations, e.g., for
papers with no neighbors. In contrast, we overcome these
limitations by smoothing Eq. (2) via additive smoothing.

If, for example, a target paper has the same ACR than the
average of its neighborhood, its S-RCR value is equal to 1.
An S-RCR higher than 1 indicates that the paper has a rele-
vance signal stronger than its co-citation network. Similarly,
an S-RCR value lower than 1 indicates low relevance of the
paper within its neighborhood.

Since the ACR function can be computed in Θ(1), the
time complexity to compute the S-RCR of a given paper p
is Θ(|Np|), where |Np| is the neighborhood size of paper p.
This time complexity is lower than the time complexity of
the classic RCR since, to compute the classic version of RCR,
we need to perform a linear regression on p’s neighborhood.

4. OTHER FEATURES

Besides the S-RCR metric, we performed a set of tests
using features that ended up not being used in our ﬁnal
ranking. These features include paper raw citation counts
and normalizations, distinct aggregations of citation-based
metrics of authors and publication venues, among others.
We also tried some Random Walk techniques, like an ap-
plication of PageRank on the paper citation graph and a
Random Walk on a heterogeneous Pseudo-Tripartite graph
composed by paper, author and venue nodes. In this last
approach, there is an edge between an author a and a paper
p if author a is one of the authors of paper p. Also, there
is an edge between paper p and venue v if paper p was pub-
lished in venue v. The interaction between papers was given
by the paper citation graph. While we believe that a proper
investigation of this last approach would lead to eﬀective re-
sults, it depends on the calibration of the parameters needed
to control the amount of probability mass between nodes of
distinct types. A plus of our ﬁnal approach based on the
proposed S-RCR metric is that it produces eﬀective results
without the need of any parameter tuning.

In this section, we report some experiments performed by

our team during the 1st phase of the WSDM Cup 2016.

5.1 Dataset Description

The Microsoft Academic Graph (MAG) [10] is a heteroge-
neous graph containing scientiﬁc publication records, cita-
tion relationships between publications – as well as authors,
institutions, journals, conferences, and ﬁelds of study. The
ﬁle size (zipped) is 37GB and it contains individual informa-
tion about more than 120 million papers. During the com-
petition, three versions of this dataset were released. Here,
we describe only the last one (version of Nov. 6, 2015).

Table 1: Relevant statistics

Papers with citation information
Papers without citation information
Total number of papers
Average neighborhood size

49,870,036
71,017,797
120,887,833
891

In Table 1, we present some statistics of the MAG dataset
that are relevant to this work. Notice that a large fraction
(59%) of the papers in this dataset have no information on
citations, that is, the paper can be represented as a node
in the citation graph that has neither inlinks nor outlinks.
There are two possible reasons for this to happen. The ﬁrst
alternative is the zero degree (i.e., both in- and out-degree)
is a true representation of reality, it is a paper that in fact
does not receive any citation yet and does not cite any other
paper. The second alternative is due to the fact that any
big repository oﬀers an approximation of the reality, which
also happens in the MAG dataset. Collecting and organiz-
ing a real-world dataset of such size is not a trivial task.
In fact, it is a process that involves the treatment of huge
amounts of semi-structured data, which usually causes some
inconsistencies.

e
z
i
S
 
d
o
o
h
r
o
b
h
g
e
N

i

107
106
105
104
103
102
101
100

0

1

2

Paper

3

4

1e7

Figure 2: Distribution of neighborhood sizes

In Figure 2, we plot the distribution of neighborhood sizes.
To compute this distribution, we consider only the approxi-
mately 50 million papers that have information on citations.
This ﬁgure helps us to characterize the neighborhood sizes
of the graph. We notice that the neighborhood sizes follow
a long tail distribution, there are many papers with just a
few neighbors and few papers with large neighborhoods.

5.2 Submissions

6. DISCUSSION

Our team made many submissions in the ﬁrst phase of
the competition. In this section, we discuss some of them.
As mentioned previously, each submission is a ranking where
each item is a paper ID and a probability of that paper being
important. The only evaluation sign available was the score
reported by the competition’s page for each submission, a
value between zero and one representing the quality of the
submitted ranking — the higher the better. While we know
that the evaluation score is based on previously computed
pair-wise expert judgments, the exact evaluation metric was
not disclosed by the organizers.

In Table 2, we present the evaluation scores our rankings
received in the ﬁrst phase of the competition and also the
time elapsed to produce each submission ﬁle.

Table 2: Our ranking scores in the 1st phase of WSDM Cup

Leaderboard

Feature
Citations
PageRank
ACR
S-RCR

Public Private Time
0.675
0h08m
1h29m
0.687
0h16m
0.685
0.697
1h50m

0.671

-
-
-

A ﬁrst guess for one who is somehow familiar with the
problem is to count paper citations in order to rank them.
It was our ﬁrst submission and received the score of 0.675
in the public leaderboard. Using PageRank is also a natural
approach to rank papers in a citation graph. Our PageRank-
based submission scored 0.687, which represents an improve-
ment of 1.8% over citation counts. Before submitting the S-
RCR, we submitted the ranking produced by its component
ACR (see Section 3.2). The ACR submission was scored
higher than citation counts but lower than PageRank. Fi-
nally, our highest scored submission was based on the S-RCR
metric. Scoring 0.697 in the public leaderboard, it corre-
sponds to an improvement of 3.3% over citation counts.

Since the ranking produced by S-RCR received the highest
public score among our submissions, we used it as the ﬁnal
ranking of the ﬁrst phase. As part of the competition, it was
evaluated using a private test set and received the score of
0.671, which was suﬃcient to bring our team UFMG/LATIN
to the 3rd position at the end of the ﬁrst phase. Eight teams
were promoted to the second phase of the competition: the
score of the 1st-placed team was 0.684, while the score of
the 8th-placed team was 0.642.
It is noteworthy that our
score dropped a little in the ﬁnal evaluation of the 1st phase,
from 0.697 to 0.671. Some teams experienced higher drops,
apparently due to overﬁtting.

We ran our experiments on a machine with 64 GB RAM,
24 processors of 3.33GHz — Intel(R) Xeon(R) CPU X5680
— under Ubuntu 14.04.2 LTS. While we did not take advan-
tage of the full computational power available (by not us-
ing parallelism in a 24-cored machine), the processing times
were pretty low for a graph of such size. The time elapsed
to produce our ﬁnal submission ﬁle was of only 1h50m.

Critical parts of our approach, like graph processing, were
implemented in C++, while other parts, like intermediate
analysis or ﬁle treating/formatting, were implemented in
Python — Pandas and Jupyter were useful tools.

In this work, we have proposed the S-RCR metric and
applied it to produce static rankings of academic papers
in the Microsoft Academic Search dataset. The interest-
ing point here is that a single well-designed feature (which
is a simpliﬁcation of a more complex one) was able to pro-
duce eﬀective results, promoting our team to the 3rd place
in the ﬁrst phase of the WSDM Cup 2016 competition. This
fact reinforces the argument that feature engineering is at
least as important as complex models, since we apply a sin-
gle well-designed feature that leads to better results than
complex models with the advantage of less tuning and less
computational eﬀort. Also, single features tend to be more
interpretable. A future direction that is worth investigating
is the impact of using the S-RCR metric together with other
features in learning to rank techniques. Another direction is
to study approaches to address ranking ties, specially how
to break ties between papers with no information on cita-
tions. Using reputation-based metrics [8, 9] seems to be a
reasonable approach to address these issues.

ACKNOWLEDGEMENTS
This work was partially sponsored by the Brazilian National
Institute of Science and Technology for the Web (MCT/
CNPq 573871/2008-6) and the authors’ individual grants
and scholarships from CNPq and FAPEMIG.

7. REFERENCES
[1] E. Garﬁeld. Citation indexes for science. Science,

122(3159):108–111, 1955.

[2] E. Garﬁeld. Citation analysis as a tool in journal

evaluation: Journals can be ranked by frequency and
impact of citations for science policy studies. Science,
(178):471–479, 1972.

[3] J. Hirsch. An index to quantify an individual’s

scientiﬁc research output. Proc. Nat. Acad. Sciences,
pages 16569–16572, 2005.

[4] B. I. Hutchins, X. Yuan, J. M. Anderson, and G. M.

Santangelo. Relative citation ratio (rcr): A new metric
that uses citation rates to measure inﬂuence at the
article level. bioRxiv, page 029629, 2015.

[5] H. Li. A short introduction to learning to rank. IEICE

Transactions, pages 1854–1862, 2011.

[6] L. Page, S. Brin, R. Motwani, and T. Winograd. The
pagerank citation ranking: Bringing order to the web.
In Proc. of WWW, pages 161–172, 1998.

[7] H. Piwowar. Altmetrics: Value all research products.

Nature, 493(7431):159–159, 2013.

[8] S. Ribas, B. Ribeiro-Neto, E. de Souza e Silva, A. H.

Ueda, and N. Ziviani. Using reference groups to assess
academic productivity in computer science. In Proc. of
WWW, pages 603–608, 2015.

[9] S. Ribas, B. Ribeiro-Neto, R. L. Santos, E. de Souza e
Silva, A. Ueda, and N. Ziviani. Random walks on the
reputation graph. In Proc. of ICTIR, pages 181–190.
ACM, 2015.

[10] A. Sinha, Z. Shen, Y. Song, H. Ma, D. Eide, B. Hsu,

and K. Wang. An overview of microsoft academic
service (mas) and applications. WWW — World Wide
Web Consortium (W3C), May 2015.

