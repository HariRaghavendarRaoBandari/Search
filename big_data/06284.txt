6
1
0
2

 
r
a

 

M
0
2

 
 
]
E
M

.
t
a
t
s
[
 
 

1
v
4
8
2
6
0

.

3
0
6
1
:
v
i
X
r
a

Bayesian correction for covariate measurement error: a frequentist

evaluation and comparison with regression calibration

Jonathan W. Bartlett

Statistical Innovation Group, AstraZeneca

Riverside 2, Granta Park, Cambridge, CB21 6GP UK

Ruth H. Keogh

Department of Medical Statistics

London School of Hygiene & Tropical Medicine

Abstract

Bayesian approaches for handling covariate measurement error are well established, and yet arguably
are still relatively little used by researchers. For some this is likely due to unfamiliarity or disagreement
with the Bayesian inferential paradigm. For others a contributory factor is the inability of standard
statistical packages to perform such Bayesian analyses. In this paper we ﬁrst give an overview of the
Bayesian approach to handling covariate measurement error, and contrast it with regression calibration
(RC), arguably the most commonly adopted approach. We then argue why the Bayesian approach has
a number of statistical advantages compared to RC, and demonstrate that implementing the Bayesian
approach is usually quite feasible for the analyst. Next we describe the closely related maximum
likelihood and multiple imputation approaches, and explain why we believe the Bayesian approach
to generally be preferable. We then empirically compare the frequentist properties of RC and the
Bayesian approach through simulation studies. The ﬂexibility of the Bayesian approach to handle both
measurement error and missing data is then illustrated through an analysis of data from the Third
National Health and Nutrition Examination Survey.

Keywords: measurement error, Bayesian inference, regression calibration, multiple imputation

1

Introduction

Many epidemiological studies are aﬀected by measurement error in one or more of the covariates of
interest. It is well known that error in covariates results in biased estimates of true covariate(s)-outcome

associations and in a loss of power to detect such associations [1]. In this paper we focus on correcting

for the eﬀects of measurement error in continuous covariates in three models which are commonly used

in epidemiological analysis; linear regression models for continuous outcomes, logistic regression models

for binary outcomes, and Cox proportional hazards models for survival or time to event outcomes.

1

Throughout most of the paper we will focus on the situation in which there is one main exposure of

interest, which is subject to measurement error, and one or more other covariates to be adjusted for, which

are assumed to be measured without error. The variable which is measured with error could equally be one

of the confounders, and indeed the approaches we describe also extend to the more general case of multiple

covariates measured with error. While exposure measurement error is commonly prioritized, measurement

error in confounders is also a serious and highly prevalent issue, and causes estimates of exposure eﬀects

to only be partially adjusted for the poorly measured confounder. Error in continuous variables can take a
number of forms. The most simple, and most commonly assumed form is the classical measurement error

model, under which the measured exposure is equal to the true exposure plus an independent random

error term. Under this model, the measured exposure is an unbiased measure of the true exposure. The

error terms are assumed to have zero mean and, typically, constant variance.

To make corrections for the eﬀects of covariate measurement error in regression models requires some

information about the relationship between the true exposure and the measured exposure, i.e. regarding

the parameters of a measurement error model. One way of gaining information about the error model is

to use a validation study within the main study sample, in which the true exposure is observed alongside

the measured exposure. It is often not feasible or even possible however to obtain a validation sample

and a more common alternative is to obtain one or more replicate observations of the measured exposure

for a subset of individuals within the main study sample. We refer to this as a replication study. In this

paper we focus on replication studies.

Many methods have been described for correcting for the eﬀects of measurement error in regression

models [1]. The most widely used correction method is regression calibration (RC), which is popular

due to its simplicity and applicability in diﬀerent types of regression models. In RC, the true exposure,

which is unobserved in the main study sample, is replaced when ﬁtting the outcome regression model by

the expected value of the true exposure, conditional on the measured exposure and the other error-free

covariates for each individual. Regression calibration gives consistent estimates of the true associations

between the explanatory variables and the outcome in a linear regression model, and approximately

consistent estimates in non-linear models, including logistic regression models [2, 3] and Cox proportional

hazards models [4].

Regression calibration has some drawbacks, however. First, for non-linear models estimates can have

moderately large biases even when the sample size is large, particularly if the eﬀect size (odds ratio or

hazard ratio) is large [5]. Second, RC does not automatically accommodate uncertainty in the parameters

indexing the measurement process. Measures of uncertainty require use of approximate methods (the
‘delta method’ approach), bootstrapping methods, which are computationally intensive, or estimating

equation methods, whose validity relies on asymptotic conditions and are complex to implement in prac-

tice. Third, extending the basic RC approach to more complex situations, such as when the outcome

model is assumed to depend on non-linear functions of the true covariate [6], or when the measurement

error models is more complex (e.g. heteroscedastic error [7]) is not trivial.

The Bayesian approach has been often advocated as a natural route to accommodating sources of

uncertainty, including measurement error, misclassiﬁcation, and missing data. Early papers include those

by Richardson and Gilks, who described a Bayesian approach to handling measurement error [8, 9]. By

taking a Bayesian approach to handle covariate measurement error, uncertainty in the parameters indexing

2

the measurement process is automatically accommodated. Like the method of maximum likelihood (ML),

the posterior distributions involved typically involve intractable likelihoods, but this diﬃculty is obviated

by Markov Chain Monte Carlo methods, which are now implemented in a number of standard and

Bayesian speciﬁc packages. A further strength is that these software packages allow one to deﬁne and

ﬁt quite complex user deﬁned Bayesian models, meaning that there is great ﬂexibility in adapting the

modelling assumptions to the situation at hand. Lastly, and in contrast to methods such as ML or

multiple imputation, whose inferences typically rely on various large sample assumptions (e.g. to handle
nuisance parameters or in deriving simple imputation combination rules), Bayesian methods do not. In

the setting of covariate measurement error, estimators which allow for the error typically have skewed

sampling distributions, and this is automatically accommodated in a Bayesian approach, since the entire

posterior distribution is simulated.

Despite excellent book length treatments of covariate measurement error methods [1], including one

speciﬁcally focusing on Bayesian methods [10], in our view it nevertheless continues to be underused by

the epidemiological and clinical research communities. This may be for a number of reasons, but principal

among them may be the apparent need to move from a frequentist to a Bayesian inferential approach and

the fact that standard statistical packages have (with exceptions) not enabled such Bayesian models to be

ﬁtted. To the ﬁrst of these reasons, as has been noted by others (e.g. [11]), Bayes procedures often have

good frequentist properties, and indeed in small samples can have better frequentist properties than ML

methods. As such, one may be able to use a Bayesian method without necessarily adopting the Bayesian

inferential paradigm. To this end, we present simulation results to examine the frequentist properties

of the Bayesian approach to covariate measurement error, using certain default priors. To the second

reason, major steps forward have been made over the last 25 years in terms of accessible MCMC software,

such that software and computational power are usually not a hinderance to using a Bayesian approach.

Moreover, we make all of our code available online to faciliate increased use of the Bayesian approach.

In Section 2 we begin by describing the assumed setup and notation for the covariate measurement

error problem. Next, in Section 3 we review the regression calibration approach. In Section 4 we describe

the Bayesian approach, both in terms of modelling choices and statistical properties, and its practical

implementation. We contrast the Bayesian approach with ML and multiple imputation in Section 5. In

Section 6 we evaluate the frequentist properties of RC and Bayesian analysis in a series of simulation

studies of the most common outcome model types. In Section 7 we present results of illustrative anal-

yses using data from the Third National Health and Nutrition Examination Survey (NHANES III). We

conclude in Section 8 with a discussion.

2 Setup and notation

In this section we describe the general setup used for the remainder of the paper.

2.1 Outcome model

We assume data are available for an i.i.d. sample of n individuals. For individual i, we let Yi, Xi and
Zi respectively denote the outcome, true covariate which is subject to measurement error and error-
free covariates. We consider three types of outcome models for Yi (i) linear, (ii) logistic, and (iii) Cox

3

proportional hazards regression. We assume that the outcome model includes only main eﬀects of Xi and
Zi. For a linear regression outcome model, we thus assume that

Yi = β0 + βXXi + βT
Z

Zi + ǫi

where ǫi ∼ N (0, σ2) is an independent normally distributed residual error. For a logistic regression
outcome model we assume that

logit {P (Yi = 1|Xi, Zi)} = β0 + βX Xi + βT
Z

Zi

(1)

Lastly, in the case of a censored time to event outcome, the outcome Yi = (Ti, Di) where Ti denotes the
observed event or censoring time and Di denotes the event indicator. We then assume a Cox proportional
hazards outcome model, such that the hazard given Xi and Zi is given by

h(t|Xi, Zi) = h0(t) exp(βX Xi + βT
Z

Zi)

where h0(t) denotes the baseline hazard function. In the standard frequentist analysis based on the Cox
proportional hazards model the baseline hazard is left unspeciﬁed and inferences about the hazard ratio
parameters (βX , βZ ) are made via a partial likelihood [12].

2.2 Measurement error model

We assume that for each study individual, an error-prone measurement Wi1 is available, rather than the
covariate of interest Xi. We assume a classical error model:

Wi1 = Xi + Ui1

where E(Ui1|Xi) = 0. We also assume that the errors Ui1 are independent of all other random variables.
This implies that the error is non-diﬀerential with respect to the outcome Yi.

In order to allow for the error in Wi1, we assume the existence of an internal replication sub-study. This
means that for a randomly selected group of individuals a second error-prone measurement Wi2 = Xi +Ui2
is obtained, where the error Ui2 is assumed independent of Ui1. We let Wi denote the vector of error-
prone measurements on individual i, and let Ni denote its length, which is two for those individuals in the
replication sub-study and one for those not. In the following we specify further assumptions as required

by RC and Bayesian methods.

3 Regression calibration

In the simplest version of regression calibration (RC), the outcome model is ﬁtted as usual, with the
unobserved Xi replaced by an estimate of E(Xi|Wi1, Zi) [13]. Typically the latter conditional expectation
is assumed to be linear in Wi1 and Zi, and it can be estimated by linearly regressing Wi2 on Wi1 and Zi
in the individuals from the internal replication substudy. We note that this version of RC does not rely
on an assumption that the two errors Ui1 and Ui2 have the same variance.

4

If one is willing to make additional assumptions, a somewhat more eﬃcient version of RC can be used,
in which Xi is replaced by an estimate of E(Xi|Wi, Zi), and the parameters involved in the latter are
estimated using all study individuals. A common assumption is to assume that Xi|Zi ∼ N (γ0+γT
X|Z )
Z
and that the measurement errors Ui1 and Ui2 are normally distributed with mean zero and common
variance σ2
U . The parameters can be estimated by ML as a random-intercepts mixed model for the Wi
conditional on Zi. It then follows from standard properties of the multivariate normal distribution that
Xi|Wi, Zi is normally distributed, with

Zi, σ2

E(Xi|Wi, Zi) = γ0 + γT
Z

Zi +

Var(Xi|Wi, Zi) = σ2

X|Z 1 −

σ2
X|Z
σ2
X|Z + σ2
σ2
X|Z
σ2
X|Z + σ2

U /Ni

U /Ni!

(W i − (γ0 + γT
Z

Zi))

(2)

where W i denotes the mean of individual i’s Ni error-prone measurements.

As noted in the introduction, RC gives consistent parameter estimates in the case of a linear outcome

model. For logistic and Cox outcome models, RC is approximately consistent. Armstrong ﬁrst gave
justiﬁcation for RC in generalized linear models under the assumption that Var(Xi|Wi) is ‘small’, using
the delta method [2]. This condition will be satisﬁed when the measurement error variance is ‘small’.

Rosner et al later justiﬁed its use in logistic regression under the assumptions that the outcome is rare
and that Xi|Wi is normal [3]. Subsequently, Kuha showed that RC could be justiﬁed as an approximate
method for logistic regression provided that β2
X V ar(Xi|Wi) is small, without the rare outcome assumption
[14]. This condition holds when βX is small or the measurement error variance is small. For a Cox
proportional hazards outcome model, RC can be justiﬁed when the event rate is low or the measurement
error variance is small [4, 15].

For valid inferences, the estimation of the parameters involved in E(Xi|Wi, Zi) should be allowed
for. One approach is to use bootstrapping. Alternatively, it is possible to construct sandwich variance

estimators by stacking the estimating equations used in the two stages [1]. One drawback with this

approach is that the resulting Wald type symmetric conﬁdence intervals do not reﬂect the asymmetric

sampling distribution of the RC estimator, which may lead to conﬁdence interval coverage which deviates

from the nominal level.

4 Bayesian approach

In this section we describe the key elements of a Bayesian analysis of the covariate measurement error

problem.

4.1 Model speciﬁcation

First, we specify a joint parametric model for (Yi, Xi, Wi1, Wi2|Zi). We condition on the fully observed
Zi, thereby avoiding the need to model its distribution. Assuming that the measurement error is non-

5

diﬀerential with respect to both Yi and Zi, this joint model can be decomposed as

f (Yi|Xi, Zi, β, η)f (Wi|Xi, σ2

U )f (Xi|Zi, γ)

(3)

The ﬁrst component is the outcome model, which contains regression parameters of primary interest
β = (β0, βX , βZ ) in the case of linear or logistic regression and β = (βX , βZ ) in the case of Cox regression,
and possibly additional parameters η (e.g. a residual variance in the case of a linear regression outcome

model). The second component is the measurement model, and as described previously the simplest
assumption is that the error prone measurements Wij follow a classical error model, with independent
normally distributed errors Uij ∼ N (0, σ2
U ). The ﬁnal component speciﬁes a model for the unobserved
covariate Xi, conditional on Zi, with a default choice being a normal linear regression model. We return
later to questions of robustness and to model extensions to relax such distributional assumptions. In the
case of a Cox proportional hazards outcome model the outcome Yi has two components (Ti, Di) and the
additional parameters, η, denote the baseline hazard function H0(t).

4.2 Prior speciﬁcation

In the Bayesian approach we must specify priors for the model parameters. The ﬁrst ‘Bayesian’ analyses

made use of ﬂat or constant priors, based on the notion that these represent a priori ignorance regarding

the value(s) of the model parameter(s) [16]. The key issue with such priors is that while a ﬂat prior

expresses ignorance on one scale, a transformation of the parameter implies a non-ﬂat prior on the

transformed parameter. The latter half of the 20th century witnessed the growth of the subjective

Bayesian approach, in which the analyst carefully choose the priors to represent their beliefs about the
model parameters in advance of seeing the data. Arguably the majority of Bayesian analyses which are

now performed by researchers make use of so called non-informative or reference priors [17]. Such priors

do not (and cannot) represent total ignorance about the model parameters, but can be viewed as default

priors that one might use when subjective prior information is either not available, or one does not want

to use such information in the analysis. The intention of such priors is usually that they have minimal

impact on inferences.

For the joint model in equation (3), prior independence is typically assumed for the parameters

in the three sub-models. For the outcome model regression coeﬃcients β and the coeﬃcients in γ a

common default prior is a very diﬀuse normal prior centred at zero. For the variance parameters, the

conjugate inverse Gamma distribution has traditionally been advocated. In the context of adjustment

for covariate measurement error, Gustafson has proposed using a Ga(0.5, 0.5) prior for the precision

(reciprocal of variance) parameters [10]. This prior equates to the likelihood that would be obtained from

one observation, with the best guess for the precision of one.

Bayesian analysis of the Cox model requires speciﬁcation of a prior for the baseline cumulative hazard
process H0(t) in addition to priors for the regression coeﬃcients β and the other submodel parameters.
The prior distribution for the baseline cumulative hazard process H0(t) is assumed to be independent
of the other priors, including that for β. Here we use a Gamma process prior for H0(t) as described
0 (t) is a prior guess
by Kalbﬂeisch [18] and Sinha et al.
at the mean and c is a parameter which represents the conﬁdence in that guess, with small values

[19], denoted H0(t) ∼ GP(cH ∗

0 , c), where H ∗

6

of c corresponding to a diﬀuse prior. We let t(1) < t(2) < · · · < t(n∗) denote the ordered observed
event times. Under the assumption that the hazard is degenerate at 0 except at the observed event
times Ti where Di = 1 it follows from the Gamma process prior for H0(t) that the increments in the
cumulative baseline hazard from time t(j) to time t(j+1) (j = 1, . . . , n∗ − 1) have independent Gamma
distributions ; dH0(t(j)) ∼ Gamma(c(H ∗(t(j+1))−H ∗(t(j)), c). In the later application of this approach we
use H ∗(t(j+1))−H ∗(t(j) = r(t(j+1)−t(j)) where r is a guess at the event rate per unit time. It can be shown
[18, 19] that under the Gamma process prior for the cumulative hazard the likelihood for (β, H0(t), c)
tends to the partial likelihood in the limit as c tends to 0, and that it tends to the full likelihood with
H0(t) = H ∗ as c tends to inﬁnity.

4.3 Posterior inference and simulation

Given speciﬁcation of the model and priors, Bayesian inference is then based on the posterior distributions

of the model parameters. For the purposes of point estimation the posterior mean is commonly used. To

form a 95% credible interval for a particular parameter, we take the 2.5% and 97.5% centiles of the pos-

terior distribution. An advantage of this in the present context of adjustment for covariate measurement

error is that asymmetry in the posterior distribution, which typically occurs when adjusting for covariate

measurement error, is automatically accounted for in credible intervals.

Except for very speciﬁc choices of model and prior, in general the posteriors are not available ana-

lytically. Instead, we can utilise Markov Chain Monte Carlo (MCMC) methods to simulate draws from

the posteriors distributions (see e.g. part III of [20]). The most common approach is the method of

Gibbs sampling, in which taking each parameter in turn, a new value is drawn from its full conditional

distribution given all other quantities. Often these conditional distributions do not belong to standard

parametric families, necessitating the use of more sophisticated sampling techniques (see [20, 17] for fur-

ther details). However, these are implemented in the software packages we describe in the following, such

that the analyst need not generally concern themselves with the details.

4.4 Frequentist properties

Under certain regularity conditions, as the sample size tends to inﬁnity, the choice of prior has no impact

on the posterior distribution, since the latter is then dominated by the likelihood function. Consequently

Bayes estimators and uncertainty intervals enjoy the same large sample properties as maximum likliehood

methods: the Bayes posterior mean estimator is consistent, asymptotically normal, and eﬃcient [20]. In
reality of course all samples are ﬁnite, and the choice of prior can sometimes have a material aﬀect on

inferences. Importantly however, in small samples or sparse data situations, Bayesian methods can have

better frequentist properties than ML procedures, particularly if sensible priors are adopted [21].

4.5 Software

The explosion of Bayesian data analyses being performed over the last few decades is largely thanks

to both the MCMC methods developed and their implementation in acccessible software. Chief among

these is the WinBUGS software package, developed in the 1990s [22].

It allows the user to deﬁne,

using a simple language syntax or graphical interface, the model and priors. MCMC methods are then

7

automatically chosen by the package, depending on the speciﬁed model and priors. One can then run the

MCMC sampler, and after a suﬃcient number of burnin iterations, draws can be saved as draws from the

respective posterior distributions. More recently, new packages have been developed, with developments

in various directions. These include the OpenBUGS project (www.openbugs.net) and Stan (mc-stan.org).

In the simulations described later, we make use of the Just Another Gibbs Sampler (JAGS) program [23],

whose model language is very similar to the BUGS language used by WinBUGS, and which can be easily

called from R.

5 Maximum likelihood and multiple imputation

5.1 Maximum likelihood

Maximum likelihood estimation and inference is based on the likelihood function, but unlike the Bayesian

approach, does not involve speciﬁcation of prior distributions for parameters. Maximum likelihood meth-

ods enjoy many favourable frequentist properties - assuming correct model speciﬁcation the ML estimator

is consistent, asymptotically normal, and eﬃcient. In the speciﬁc context of adjustment for covariate mea-

surement error, a drawback of ML is that the likelihood function typically involves intractable integrals

[1], such that numerical methods such as numerical intergration are required in order to obtain estimates.

The same obstacle is overcome in the Bayesian appproach through the use of MCMC methods. A further

drawback of ML in the present context is that in small samples inference based on symmetric Wald based

conﬁdence intervals may perform poorly due to the lack of regularity of the likelihood function. Software

to ﬁt user deﬁned models which allow for covariate measurement error is also somewhat limited. Lastly,

the absence of prior distributions prevents the incorporation of external information which may sometimes

be available regarding the measurement process.

5.2 Multiple imputation

Multiple imputation (MI) has become an extremely popular approach for handling missing data, and has
also been advocated as an approach for handling measurement error, in which Xi is multiply imputed
[24, 25, 26, 27]. There is a very close connection between MI and ‘direct’ Bayesian inference.
In its

originally devised form, MI is based on repeatedly drawing imputations of missing values from their

posterior distribution based on a Bayesian imputation model. The analysis model of interest is then

ﬁtted, typically using ML, to each of the complete datasets. The resulting estimates and standard errors

are then pooled using rules developed by Rubin [28]. MI can most directly be viewed as an approximation

to a full Bayesian analysis [29], although its frequentist properties can of course also be evaluated [30].

From the Bayesian perspective, application of MI and Rubin’s rules can be viewed as a particular route

to performing a Bayesian analysis, in which one eﬀectively assumes that the posterior distributions for
the parameters are normally distributed.

As described by Carpenter and Kenward in the context of missing data, there are a number of settings

where use of MI may be preferable to a direct Bayesian analysis [29]. However, in the context of covariate

measurement error in parametrically speciﬁed outcome models, we argue that the advantages of a direct

Bayesian approach far outweigh the disadvantages, relative to MI. First, when only replicate error-prone

8

measurements are available, standard software for performing MI cannot be applied, since Xi is missing
for all individuals. Second, standard parametric imputation models which might be used to impute Xi
in general may not be compatible with the assumed outcome model [31]. This will in particular occur

when the outcome model is itself non-linear, or the imprecisely measured covariate is assumed to have

a non-linear eﬀect on the outcome. Third, when allowance is made for covariate measurement error, as

noted earlier, the posterior distributions for the outcome model parameters are typically skewed in small

to moderate sample sizes, such that symmetric credible/conﬁdence intervals constructed using Rubin’s
rules may perform poorly, either from a subjective Bayesian perspective or in a frequentist evaluation.

Lastly, we note that software for performing Bayesian inference will typically also permit saving of the
imputed values of Xi as a by-product, such that if the analyst really wants imputed datasets they can
still be obtained.

6 Simulations

In this section we present simulation results for the cases of a linear, logistic and Cox proportional

hazards outcome model, comparing the popular RC approach with the Bayesian approach. We adopt

the standard frequentist type simulation setup, in which datasets are repeatedly generated using ﬁxed
population parameter values.

6.1 Linear regression

We ﬁrst present simulation results for a simple linear regression outcome model. Datasets of size n = 1, 000
were simulated, with covariates Xi and Zi drawn from a bivariate normal distribution with means zero,
variances one, and covariance 0.25. Continuous outcomes Yi were generated from the linear regression
model given in equation (1), with β0 = 0 and βX = βZ = 1. The normally distributed residual variance
σ2 was chosen in order to given R2 = 0.1, 0.5, 0.9. Each individual had an error-prone measurement
Wi1 = Xi + Ui1, with Ui1 ∼ N (0, σ2
U ). A random subset of 10% of individuals had a second error-prone
measurement, with the same error variance σ2
U . This variance was chosen to give reliability (unconditional)
values of 0.5, 0.7, 0.9, corresponding to low, moderate and high reliability.

We ﬁrst estimated the outcome model parameters using RC, by ﬁtting a random intercepts model for
the error-prone measurements, with Zi entering as a ﬁxed eﬀect, as described in Section 3. Next we ﬁtted
a Bayesian model, calling JAGS from R using the rjags package. We adopted non-informative priors for

all model parameters, following those proposed by Gustafson [10]. Speciﬁcally we assumed independent
normal priors for β0, βX , βZ , γ0, γZ , with mean zero and variance 10,000, and inverse gamma IG(0.5, 0.5)
priors for each of the variance parameters. As discussed by Gustafson, the latter prior can be thought of

as being equivalent to a best guess for the variance of one, coming from a single observation. We ran ﬁve

parallel chains with 1,000 burnin iterations and 5,000 main iterations. If the Rubin-Gelman convergence
statistic Rhat was greater than 1.05 for any of β0, βX , βZ we extended the chains until this was met.

Table 1 shows the simulation results, with 1,000 simulations per scenario. RC had slight upward
bias for βX when the reliability of the error prone measurements was 0.5, and was unbiased for the
higher reliability values. The Bayes mean estimator was upwardly biased for reliability equal to 0.5 and
R2 = 0.1 and R2 = 0.5. Inspection of the estimates showed that the sampling distribution of the Bayes

9

mean estimator had greater skew than the RC estimator, with the larger estimated values inducing the
upward bias. For reliability of 0.5 and 0.7, and R2 = 0.9 the Bayes estimators had lower empirical SD
than the RC estimator, and consequently had lower mean squared error in these scenarios. For the other

scenarios the RC and Bayes estimators performed similarly. Lastly, the 95% Bayesian credible intervals

had frequentist coverage close to 95%.

We emphasize that the performance of the Bayesian estimators here depends on the choice of priors.

In particular, the use of more informative priors for the measurement error variance and the variance for
X|Z could be used to reduce the bias variability of the Bayesian estimators.

Table 1: Linear regression results. 1,000 simulations per scenario. Empirical means and SDs for estimates
of βX , and coverage of 95% Bayesian credible intervals.

Reliability R2 RC
0.5
0.5
0.5
0.7
0.7
0.7
0.9
0.9
0.9

0.1
0.5
0.9
0.1
0.5
0.9
0.1
0.5
0.9

1.04 (0.29)
1.03 (0.19)
1.02 (0.16)
1.01 (0.20)
1.00 (0.09)
1.00 (0.07)
1.01 (0.16)
1.00 (0.06)
1.00 (0.03)

Bayes mean Bayes CI
1.17 (0.37)
1.16 (0.28)
0.98 (0.07)
1.04 (0.21)
1.03 (0.10)
1.00 (0.05)
1.03 (0.16)
1.02 (0.06)
1.01 (0.03)

0.94
0.92
0.98
0.95
0.95
0.97
0.95
0.95
0.93

6.2 Logistic regression

Next we performed simulations with a logistic regression outcome model. The covariates Xi and Zi,
and error-prone measurements were generated as described previously. The binary outcome Yi was then
generated according to a logistic regression model (1). The intercept β0 was chosen so that P (Yi = 1) = 0.2
approximately. We performed simulations with log odds ratios βX = 0.1, 0.5, 2, representing small,
moderate and large eﬀects of Xi. As before, we set βZ = βX.

Regression calibration was implemented as described previously. For the Bayesian approach, we again
used independent normal priors for each of β0, βX and βZ . For β0 we used, as before, the non-informative
prior β0 ∼ N (0, 10000). For βX and βZ we adopted the N (0, 1.38) prior suggested by Hamra et al [32].
As described by Hamra et al , the use of such a mildly informative prior can help in terms of stabilizing

estimates. This prior corresponds to assuming, a priori, that we are 95% sure that the odds ratios
exp(βX ) and exp(βZ ) lie between 0.1 and 10, an assumption that is arguably generally reasonable in most
epidemiology studies (provided the predictor has been suitably standardized). For the γ parameters and

the variance parameters we assumed the same priors as in the case of linear regression.

Table 2 shows the results of the simulations. For βX = 0.1, RC and Bayes performed very similarly,
both being essentially unbiased and having similar empirical SD. For βX = 0.5 and reliability of 0.5, while
RC is unbiased, Bayes showed some upward bias and was more variable than RC. For reliability ratios of
0.7 and 0.9 the performance was similar for both. For βX = 2 and reliability of 0.5, RC showed downward
bias, consistent with the known properties of RC for logistic regression, in that bias is larger for large

covariate eﬀects. In contrast, Bayes showed only a slight downward bias. The bias of RC reduced, again

10

as expected, as the reliability was increased to 0.7 and then 0.9, although some downward bias remained

even for the latter case. In contrast Bayes estimates were essentially unbiased. Lastly, the Bayesian 95%

credible intervals had approximately 95% coverage across all scenarios.

Table 2: Logistic regression outcome model simulation results, with 1,000 simulations per scenario. Monte-
Carlo means and SDs for estimates of βX from regression calibration (RC) and Bayes, and empirical
coverage of 95% Bayesian credible intervals

Reliability βX RC
0.5
0.5
0.5
0.7
0.7
0.7
0.9
0.9
0.9

0.1
0.5
2
0.1
0.5
2
0.1
0.5
2

0.10 (0.12)
0.51 (0.15)
1.64 (0.31)
0.10 (0.10)
0.50 (0.11)
1.74 (0.20)
0.10 (0.09)
0.50 (0.10)
1.91 (0.17)

Bayes mean Bayes CI
0.12 (0.14)
0.58 (0.19)
1.94 (0.32)
0.11 (0.10)
0.52 (0.12)
2.01 (0.27)
0.10 (0.09)
0.51 (0.10)
2.01 (0.20)

0.94
0.94
0.97
0.95
0.94
0.97
0.94
0.95
0.96

6.3 Cox proportional hazards regression

Lastly, we performed simulations for time-to-event data based on a Cox proportional hazard model. The
covariates Xi and Zi were generated as described as described for linear regression. Event times Ti were
generated according to the Weibull hazard model h(t|Xi, Zi) = κλtκ−1eβX Xi+βZ Zi. We used κ = 2 and λ
was chosen so that approximately 10% of individuals had an event time before the end of follow-up which
was ﬁxed at time 10 (e.g. 10 years); the remaining individuals were censored at time 10 (Di = 0). We
performed simulations with log hazard ratios βX = 0.1, 0.5, 2 and as before we set βZ = βX .

RC was performed as described previously. For the Bayesian approach we used independent normal
priors for βX and βZ and we chose the N (0, 1.38) prior as was used in the logistic regression simulations,
corresponding here to an assumption that we are 95% sure that the hazard ratios exp(βX ) and exp(βZ )
lie between 0.1 and 10. For the γ parameters and the variance parameters we assumed the same priors as

in the case of linear and logistic regression. As outlined in Section 4.2, we assumed a process prior for the
baseline cumulative hazard which implies Gamma priors for the increments in the hazards; dH0(t(j)) ∼
Gamma(c(H ∗(t(j+1)) − H ∗(t(j))), c). We used c = 0.001, representing low conﬁdence in the prior mean of
the Gamma Process, H ∗
0 (t). We used H ∗(t(j+1)) − H ∗(t(j)) = r(t(j+1) − t(j)) with r = 0.01, since the data
were simulated so that 10% of individuals have the event during 10 time units of follow-up. The analysis

requires the data to be speciﬁed using a counting process format and this is illustrated in example code

given online. Due to the higher computational burden of ﬁtting the Cox model, only three parallel chains

were used, and 100 (rather than 1000) simulations were performed for each scenario.

Table 3 shows the simulation results. For βX = 0.1 and βX = 0.5, RC and Bayes performed very
similarly across all three reliability values. For βX = 2 and reliability of 0.5, RC showed bias toward the
null, in line with previous simulation evidence [33]. This bias was reduced as the reliability increased,

although there was still downward bias with reliability of 0.9. In contrast, the Bayes estimator was much

less biased. The Bayesian credible intervals had approximately 95% coverage across all nine scenarios.

11

Table 3: Cox regression outcome model simulation results, with 100 simulations per scenario. Monte-
Carlo means and SDs for estimates of βX from regression calibration (RC) and Bayes, and empirical
coverage of 95% Bayesian credible intervals

Reliability βX RC
0.5
0.5
0.5
0.7
0.7
0.7
0.9
0.9
0.9

0.1
0.5
2
0.1
0.5
2
0.1
0.5
2

0.10 (0.09)
0.49 (0.11)
1.49 (0.15)
0.11 (0.09)
0.49 (0.11)
1.67 (0.16)
0.11 (0.10)
0.51 (0.10)
1.84 (0.15)

Bayes mean Bayes CI
0.10 (0.09)
0.48 (0.11)
1.92 (0.20)
0.11 (0.09)
0.48 (0.11)
1.98 (0.18)
0.11 (0.10)
0.50 (0.10)
1.96 (0.15)

0.98
0.94
0.92
0.98
0.93
0.97
0.96
0.96
0.95

7

Illustrative example

To illustrate the potential ﬂexibility and advantages of the Bayesian approach, we consider data from

the Third National Health and Nutrition Examination Survey (NHANES III). NHANES III was a survey

conducted in the US between 1988 and 1994 in 33,994 individuals aged two months and older. Here we

consider an illustrative analyses of the data available on those aged 60 years and above at the time of

the original survey. Fitting Cox models to large datasets is very slow using JAGS, particularly for large

datasets. We therefore considered inference for a Weibull regression model for hazard for death due to

cardiovascular disease (CVD), with age, sex, smoking status, diabetes status and systolic blood pressure

(SBP) at the time of the survey as covariates:

h(t) = rtr−1 exp(β0 + β1sbpi + β2sexi + β3agei + β4smokeri + β5diabetesi)

where r is a shape parameter and β1, .., β5 are log hazard ratios. We take the ﬁrst SBP measurement
taken at the survey, sbpi1, to be an error-prone measurement of each individual’s underlying SBP, subject
to classical error. A 5% subset of individuals was selected to participate in a second examination, during
which SBP was again measured. We assume this second measure, sbpi2, is an independent error-prone
measurement of each individual’s underlying SBP. After deleting 7 individuals who were missing diabetes

status, data were available on 6,519 individuals. Of these, 1,469 (22.5%) subsequently died due to CVD,

with a median follow-up of 10.8 years. 5,033 (77.2%) had a ﬁrst SBP measurement available from the ﬁrst

examination. An SBP measurement at the second examination was available in 401 (6.2%) of individuals.

Unfortunately, smoking status was only recorded in 3,433 (52.3%) of individuals. The analysis thus

required handling of both the measurement error in the SPB measurements and the substantial missingness

in the smoking and SBP variables.

7.1 Naive analyses

We ﬁrst ﬁtted the Weibull regression using sbpi1 (ignoring measurement error) to the 2,667 complete cases,
whose estimates are shown in the ﬁrst column of Table 4. Strong evidence was found for independent

associations between each of the covariates and hazard of death due to CVD, with associations in the

12

expected directions. To check that a Weibull assumption was appropriate, we additionally ﬁtted a Cox

proportional hazards model with the same covariates. The estimates of the log hazard ratios between the

two models were very similar, suggesting a Weibull assumption is reasonable here. Secondly, we performed

a global test of the proportional hazards assumption using the Schoenfeld residuals following ﬁtting the

Cox model, which gave p = 0.08, indicating no evidence to reject an assumption of proportional hazards.

Through ﬁtting a logistic regression model for the missingness indicator of the smoking variable, we found

evidence that smoking was more likely to be missing for females, older individuals, diabetics, and those
individuals with longer follow-up times. The latter ﬁnding suggests that the complete case analysis (CCA)

may be biased.

Next we ﬁtted the same Weibull model to the complete cases, again ignoring measurement error, using

the Bayesian approach. We assumed an exponential prior for the shape parameter r with parameter 0.001.
Rather than placing a prior on the log hazard ratios, we placed independent N (0, 106) priors on −βk/r,
since this leads to improved MCMC mixing [34]. Five independent chains were used, with 5,000 burnin

iterations and 5,000 main sample iterations. The estimates are 95% credible intervals are shown in Table

4. In line with theory, due to the large sample size, the Bayesian estimates and intervals were almost

identical to those from maximum likelihood.

7.2 Regression calibration

We then applied RC to the complete cases. To do this we ﬁtted a linear mixed model to the available SBP

measurements, with a random eﬀect for individual and ﬁxed eﬀects of sex, age, smoking and diabetes. We

also included a ﬁxed eﬀect to allow for a systematic shift in mean between the ﬁrst and second exams. The

resulting predicted true SBP values at exam one were then used as a covariate to ﬁt the Weibull regression

model. We used 2,000 non-parametric bootstrap samples to obtain percentile 95% conﬁdence intervals

for the estimates, in order to take into account the two stage estimation process. Based on the mixed

model ﬁt to the complete cases, the estimated reliability (conditional on the error-free covariates) was

0.75. Adjusting for measurement error using RC led to the estimated log hazard ratio for SBP increasing,

from 0.085 to 0.115, as expected by approximately 4/3 (1 divided by the reliability 0.75). Estimates for

the other covariates did not materially change.

In order to apply RC to the full dataset, we contemplated use of its use in combination with MI. This is
problematic however. First, one could use MI to impute the missing smoking, sbpi1 and sbpi2 values. For
example, one could apply the full conditional speciﬁcation MI approach, imputing the smoking variable
using logistic regression and the SBP variables using linear regression models. In these models one must

include the error-free covariates, plus the outcome. In the case of a time to event outcome modelled using

a proportional hazards model, an approximately compatible imputation model for covariates includes the

event indicator and an estimate of the cumulative hazard function [35]. Having generated the imputed

datasets, RC could then be applied to each imputed datasets. However, in order to apply Rubin’s rules, one

requires valid within imputation estimates. As described in Section 3, for RC these can only be obtained

by bootstrapping or by programming large sample theory estimating equation variance estimators. While

both could in principle be programmed, they are not entirely straightforward to implement, and so we do

not pursue MI in combination with RC.

13

7.3 Bayesian analyses adjusting for covariate measurement error

Next we modiﬁed the Bayesian complete case analysis to accommodate measurement error. We assumed

that each individual’s true underlying SBP around the time at which the ﬁrst measurement was obtained,
sbpi, was normally distributed conditional on smoking, sex, age and diabetes, with N (0, 104) priors on the
regression coeﬃcients and a Ga(0.5, 0.5) prior on the precision parameter. For the ﬁrst SBP measurement,
sbpi1, we assumed

with Ui1 ∼ N (0, σ2

U ). For the second SBP measurement, sbpi2, we assumed

sbpi1 = sbpi + Ui1

sbpi2 = ν + sbpi + Ui2

where ν is a parameter to allow for a systematic shift in mean between the two exams, and Ui2 ∼ N (0, σ2
U ).
The errors Ui1 and Ui2 are assumed to be independent. For σ−2
U a Ga(0.5, 0.5) prior was assumed, and for
ν a N (0, 104) prior was assumed. The posterior mean and credible intervals are shown in Table 4, under
‘Bayes adj. CCA’. The results were very similar to those based on RC CCA, except that the credible

interval for SBP was slightly narrower.

A strength of the Bayesian approach is its ﬂexibility to simultaneously handle missing data and

measurement error. To accommodate missingness in the smoking and SBP variables under a missing

at random assumption, we assumed a model for the distribution of smoking, conditional on the fully

observed error free covariates sex, age, and diabetes. In our analysis we assumed a logistic model for this

conditional distribution:

logit {P (smokeri = 1)} = α0 + α1sexi + α2agei + α3diabetesi,

with independent mean zero normal priors for the regression coeﬃcients, each with variance 10,000. A

major advantage of the Bayesian approach here is that the missing smoking and underlying SBP values

are imputed by the Gibbs sampler, using the conditional distributions implied by a single well speciﬁed

joint model for the data. The posterior means were somewhat diﬀerent to the RC and Bayes complete

case analyses, and as expected the credible intervals were narrower, due to the inclusion of observed data

from 3,852 individuals. The changes in coeﬃcient estimates may be indicative of bias in the complete

case analyses.

8 Discussion

In this paper we have empirically compared the frequentist properties of regression calibration and

Bayesian approaches to handling covariate measurement error. Our simulations demonstrate that for

what might be considered a fairly typical epidemiological study setup, the methods often perform very

similarly. As such, we believe that the Bayesian approach for measurement error adjustment may be as

useful for the frequentist as for the Bayesian statistician. When the reliability of error-prone measure-

ments was low, the Bayes estimator performed somewhat worse than RC. However, for larger eﬀect sizes,

14

Covariate
SBP (per 20 mmHg)
Male
Age (per 10 years)
Smoker
Diabetes

Naive CCA
0.085 (0.014, 0.157)
0.49 (0.30, 0.67)
0.88 (0.77, 0.99)
0.26 (0.07, 0.46)
0.50 (0.29, 0.72)

Naive Bayes
0.086 (0.015, 0.160)
0.49 (0.32, 0.67)
0.87 (0.76, 0.99)
0.25 (0.06, 0.45)
0.50 (0.28, 0.72)

RC CCA
0.115 (0.014, 0.221)
0.49 (0.32, 0.68)
0.87 (0.76, 0.99)
0.26 (0.07, 0.45)
0.50 (0.28, 0.72)

Bayes adj. CCA
0.114 (0.017, 0.211)
0.49 (0.31, 0.69)
0.87 (0.75, 0.98)
0.26 (0.06, 0.46)
0.50 (0.27, 0.71)

Bayes adj. full
0.122 (0.059, 0.186)
0.46 (0.36, 0.57)
1.01 (0.94, 1.09)
0.24 (0.07, 0.41)
0.68 (0.55, 0.81)

1
5

Table 4: Log hazard ratios estimates and 95% conﬁdence/credible intervals for the NHANES III data. CCA - complete case analysis performed
using 2,667 individuals, full analysis performed using 6,519 individuals, RC - regression calibration, naive - ignoring measurement error, adj. -
adjusting for measurement error in SBP, SBP - systolic blood pressure.

RC was biased for logistic and Cox regression, while the Bayes estimator showed much less bias. A critical

point to bear in mind is that there are inﬁnitely many Bayes estimators, corresponding to the diﬀerent

choices of prior distributions - use of diﬀerent priors could lead to, depending on the true data generating

mechanism, better or worse performance. While some analysts dislike the Bayesian approach because of

the requirement to specify priors, they give the analyst the opportunity to exploit external information

about model parameters, potentially leading to more precise estimates.

We have highlighted the fact that Bayesian estimators enjoy the same large sample frequentist proper-
ties as the method of ML, and also described the relationships between these approaches and the popular

MI approach. Software for MI cannot be directly applied to handle covariate measurement error when

replication data are available. Moreover, even when validation data are available, the covariate imputa-

tion models included in MI implementations may not be compatible with the analyst’s outcome model

[31]. A further strength of the Bayesian approach is that uncertainty intervals automatically allow for the

skewness typically found in covariate measurement error adjusted estimators.

As has been noted by many authors before, a key strength of the Bayesian approach is its ﬂexibility to

handle more complicated models and data structures. As we have demonstrated in Section 7, the Bayesian

approach can readily accommodate both covariate measurement error and missing data. Moreover, more

complex measurement error models can in principle be used, for example to allow for heteroscedastic

error, systematically biased measurements, or more ﬂexible modelling of the true covariate’s distribution

[1]. The ﬂexibility of the Bayesian approach also lends itself to the problem of adjusting for covariate

measurement error when the true covariate is assumed to have a complex non-linear association with the

outcome [36]. In this paper we have focused on the setting whereby internal replication data are available;

the Bayesian approach readily handles the situation where validation data are instead available.

Nonetheless, the Bayesian approach has a number of drawbacks. As a fully parametric approach,

a natural concern is sensitivity of inferences to distributional assumptions, particularly those about the

unobserved true covariate and measurement errors. In this regard the Bayesian approach can utilize more

ﬂexible model speciﬁcations, for example by modelling the unobserved true covariate using a normal

mixture model [37]. An important practical issue is that although the software available for ﬁtting

complex analyst deﬁned models using the Bayesian approach has seen dramatic developments over the last

25 years [22], ﬁtting certain models (e.g. Cox proportional hazards models) can still take tremendously

long. Although this concern will be progressively mitigated by increasing computational power, it is

arguably still a material drawback. Further research and eﬀort is therefore warranted to develop software

implementations of the Bayesian approach which mitigate this.

Supplementary material

R and JAGS code demonstrating each of the simulation setups, and code and data for the illustrative

analysis, are provided at the GitHub repository https://github.com/jwb133/bayesMeasurementError.

16

Funding

This paper was written while J.W.B. was a member of the Department of Medical Statistics, London

School of Hygiene and Tropical Medicine, and was supported by a Medical Research Council Fellowship

[MR/K02180X/1]. R.H.K. was supported by a Medical Research Council Fellowship [MR/M014827/1].

References

[1] R. J. Carroll, D. Ruppert, L. A. Stefanski, and C. M. Crainiceanu. Measurement Error in Nonlinear

Models. Chapman & Hall/CRC, 2nd edition, 2006.

[2] B Armstrong. Measurement error in the generalised linear model. Communications in Statistics -

Simulation and Computation, 14:529–544, 1985.

[3] B. Rosner, D. Spiegelman, and W. C. Willett. Correction of logistic regression relative risk estimates

and conﬁdence intervals for measurement error: the case of multiple covariates measured with error.

American Journal of Epidemiology, 132:734–743, 1990.

[4] R L Prentice. Covariate measurement errors and parameter estimation in a failure time regression

model. Biometrika, 69:331–342, 1982.

[5] C Y Wang, L Hsu, D Feng, and R L Prentice. Regression calibration in failure time regression.

Biometrics, 53:131–145, 1997.

[6] A. D. Strawbridge. Modelling non-linear exposure-disease relationships in a large individual partici-

pant meta-analysis allowing for the eﬀects of exposure measurement error. PhD thesis, University of

Cambridge, 2011.

[7] Y Guo and R J Little. Regression analysis with covariates that have heteroscedastic measurement

error. Statistics in Medicine, 30:2278–2294, 2011.

[8] S Richardson and W R Gilks. A bayesian approach to measurement error problems in epidemiology

using conditional independence models. American Journal of Epidemiology, 138(6):430–442, 1993.

[9] S Richardson and W R Gilks. Conditional independence models for epidemiological studies with

covariate measurement error. Statistics in Medicine, 12:1703–1722, 1993.

[10] P. Gustafson. Measurement Error and Misclassiﬁcation in Statistics and Epidemiology: Impacts and

Bayesian Adjustments. Chapman & Hall/CRC, 2003.

[11] R Little. Calibrated bayes, for statistics in general, and missing data in particular. Statistical Science,

26:162–174, 2011.

[12] D R Cox. Regression models and life-tables. Journal of the Royal Statistical Society B, 34:187–220,

1972.

[13] R H Keogh and I R White. A toolkit for measurement error correction, with a focus on nutritional

epidemiology. Statistics in Medicine, 33:2137–2155, 2014.

17

[14] J Kuha. Corrections for exposure measurement error in logistic regression models with an application

to nutritional data. Statistics in Medicine, 13(11):1135–1148, 1994.

[15] M. D. Hughes. Regression dilution in the proportional hazards model. Biometrics, 49:1056–1066,

1993.

[16] J Berger. The Case for Objective Bayesian Analysis. Bayesian Analysis, 1(3):385–402, 2006.

[17] C P Robert. The Bayesian Choice: From Decision-Theoretic Foundations to Computational Imple-

mentation). Springer, New York, 2007.

[18] J D Kalbﬂeisch. Non-parametric Bayesian analysis of survival time data. Journal of the Royal

Statistical Society B, 40:214–221, 1978.

[19] D Sinha, J G Ibrahim, and M Chen. A Bayesian justiﬁcation of Cox’s partial likelihood. Biometrika,

90:629–641, 2003.

[20] A. Gelman, J. B. Carlin, H. S. Stern, and D. B. Rubin. Bayesian Data Analysis. Boca Raton:

Chapman & Hall/CRC, 2nd edition, 2004.

[21] Sander Greenland and Mohammad Ali Mansournia. Penalization, bias reduction, and default priors

in logistic and related categorical and survival regressions. Statistics in Medicine, 34(23):3133–3143,

2015. sim.6537.

[22] D Lunn, D Spiegelhalter, A Thomas, and N Best. The BUGS project: Evolution, critique and future

directions. Statistics in Medicine, 28:3049–3067, 2009.

[23] Martyn Plummer. JAGS: A program for analysis of Bayesian graphical models using Gibbs sampling,

2003.

[24] S R Cole, H Chu, and S Greenland. Multiple-imputation for measurement-error correction. Interna-

tional Journal of Epidemiology, 35:1074–1081, 2006.

[25] L. S. Freedman, D. Midthune, R. J. Carroll, and V. Kipnis. A comparison of regression calibration,

moment reconstruction and imputation for adjusting for covariate measurement error in regression.

Statistics in Medicine, 27:5195–5216, 2008.

[26] K Messer and L Natarajan. Maximum likelihood, multiple imputation and regression calibration for

measurement error adjustment. Statistics in Medicine, 27:6332–6350, 2008.

[27] Y Guo and R J Little. Bayesian multiple imputation for assay data subject to measurement error.

Journal of Statistical Theory and Practice, 7(2):219–232, 2013.

[28] D B Rubin. Multiple imputation for nonresponse in surveys. New York: Wiley, 1987.

[29] J R Carpenter and M G Kenward. Multiple Imputation and its Application. John Wiley & Sons,

Ltd, Chichester, U.K., 2013.

18

[30] N Wang and J M Robins. Large-sample theory for parametric multiple imputation procedures.

Biometrika, 85:935–948, 1998.

[31] J W Bartlett, S R Seaman, I R White, and J R Carpenter. Multiple imputation of covariates by

fully conditional speciﬁcation: Accommodating the substantive model. Statistical Methods in Medical

Research, 24:462–487, 2015.

[32] G B Hamra, R F MacLehose, and S R Cole. Sensitivity analyses for sparse-data problemsusing

weakly informative Bayesian priors. Epidemiology, 24(2):233–239, 2013.

[33] Sharon X. Xie, C. Y. Wang, and Ross L. Prentice. A risk set calibration method for failure time

regression by using a covariate reliability sample. Journal of the Royal Statistical Society: Series B

(Statistical Methodology), 63(4):855–870, 2001.

[34] Post by Martyn Plummer, JAGS discussion forum. https://sourceforge.net/p/mcmc-jags/discussion/61003

Accessed: 21st December 2015.

[35] I. R. White and P. Royston.

Imputing missing covariate values for the Cox model. Statistics in

Medicine, 28:1982–1998, 2009.

[36] S M Berry, R J Carroll, and D Ruppert. Bayesian smoothing and regression splines for measurement

error problems. Journal of the American Statistical Association, 97:160–169, 2002.

[37] S Richardson, L Leblond, I Jaussent, and P J Green. Mixture models in measurement error problems,

with reference to epidemiological studies. Journal of the Royal Statistical Society A, 165:549–566,

2002.

19

