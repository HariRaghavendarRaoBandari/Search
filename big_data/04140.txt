6
1
0
2

 
r
a

 

M
4
1

 
 
]
E
M

.
t
a
t
s
[
 
 

1
v
0
4
1
4
0

.

3
0
6
1
:
v
i
X
r
a

Submitted to the Annals of Statistics

IDENTIFIABILITY OF RESTRICTED LATENT CLASS

MODELS WITH BINARY RESPONSES

By Gongjun Xu

University of Minnesota

Statistical latent class models are widely used in social and psy-
chological researches, yet it is often diﬃcult to establish the identiﬁ-
ability of the model parameters. In this paper we consider the identi-
ﬁability issue of a family of restricted latent class models, where the
restriction structures are needed to reﬂect pre-speciﬁed assumptions
on the related assessment. We establish the identiﬁability results in
the strict sense and specify which types of restriction structure would
give the identiﬁability of the model parameters. The results not only
guarantee the validity of many of the popularly used models, but also
provide a guideline for the related experimental design, where in the
current applications the design is usually experience based and identi-
ﬁability is not guaranteed. Theoretically, we develop a new technique
to establish the identiﬁability result, which may be extended to other
restricted latent class models.

1. Introduction. Statistical latent class models are widely used in so-
cial and psychological researches to model latent traits that are not directly
measurable, with the aim to identify homogeneous subgroups of individuals
based on their surrogate response variables. Although latent class models
have many attractive traits for practitioners, fundamental identiﬁability is-
sues, i.e., the feasibility of recovering model parameters based on the ob-
served data, could be diﬃcult to address. Speciﬁcally, we say a set of pa-
rameters β for a family of distributions {f (x|β) : β ∈ B} is identiﬁable if
distinct values of β correspond to distinct probability density functions, i.e.,
for any β there is no ˜β ∈ B\{β} for which f (x|β) ≡ f (x| ˜β). Identiﬁability is
the prerequisite for most common statistical inferences, especially parameter
estimation, and its study dates back to Koopmans (1950) and Koopmans
and Reiersøl (1950); see also McHugh (1956); Rothenberg (1971); Goodman
(1974); Gabrielsen (1978) for further developments.

For latent class models with ﬁnite mixtures of ﬁnite measure products,
Teicher (1967) established the equivalence between the model identiﬁability
with that of the corresponding one dimensional mixture model. Gyllenberg,

MSC 2010 subject classiﬁcations: Primary 62E10
Keywords and phrases: Identiﬁability, restricted latent class models, Q-matrix, cogni-

tive diagnosis models, multivariate Bernoulli mixture, Kruskal’s tensor decomposition.

1

2

G. XU

Koski, Reilink, and Verlaan (1994) further showed that the latent class mod-
els with binary responses (ﬁnite mixture of Bernoulli products) are not iden-
tiﬁable. Such nonidentiﬁablity results have likely impeded statisticians from
looking further into this problem (Allman, Matias, and Rhodes, 2009). Re-
cently, researchers have considered the generic identiﬁability of such models.
The generic identiﬁability is deﬁned following algebraic geometry terminol-
ogy. It implies that the set of parameters for which the identiﬁability does
not hold has Lebesgue measure zero. Establishing the identiﬁability condi-
tions can be mathematically diﬃcult. The generic identiﬁability problem is
closely related to the algebraic geometry theory, as pointed out by Elmore,
Hall, and Neeman (2005). Elmore et al. (2005) and Allman et al. (2009)
used algebraic-geometric approaches to establish generic identiﬁability re-
sults for a large set of models, including the latent class models and many
other latent variable models. In particular, the work of Allman et al. (2009)
is based on the fundamental result of Kruskal’s trilinear decomposition of
three-way arrays (Kruskal, 1976, 1977) by ‘unfolding’ a high-way array into
a three-way array.

The existing techniques to establish generic identiﬁability, being algebraic-
geometric in nature, necessarily exclude a measure zero set. Therefore, they
do not provide information as to whether the model parameters are iden-
tiﬁable for submodels with additional constraints, where the constrained
parameter spaces usually falls in a measure zero set. To develop the identi-
ﬁability conditions for such restricted models, we need techniques to incor-
porate the additional constraints.

In this paper, we consider a class of restricted latent class models with
binary responses (ﬁnite mixture of Bernoulli products). The class of models
has recently gained great interests in psychological and educational mea-
surement, psychiatry and other research areas, where a classiﬁcation-based
decision needs to be made about an individual’s latent traits, based on his
or her observed surrogate responses (to test problems, questionnaires, etc.).
The model parameters are restricted via a pre-speciﬁed matrix (see Sec-
tion 2.1 for more details) to reﬂect the diagnostic assumptions about the
latent traits. In particular, when there is no restriction, the model becomes
the unrestricted latent class model. Diﬀerently from the unrestricted mod-
els, the restriction matrix provides important information for applications,
and therefore the strict identiﬁability needs to be satisﬁed to guarantee the
validity of the models under diﬀerent parameter constraints. Although re-
searchers have long been aware of the identiﬁability problem of these types
of restricted models (DiBello, Stout, and Roussos, 1995; Maris and Bechger,
2009; Tatsuoka, 2009; DeCarlo, 2011), there is a tendency to gloss over the

IDENTIFIABILITY OF RESTRICTED LATENT CLASS MODELS

3

issue in practice due to a lack of theoretical development on the topic. To
the author’s best knowledge, there are few studies in the literature on the
identiﬁability of the restricted latent class models.

This paper aims to address the identiﬁability issue for these models. Our

main contribution includes the following points.

i) First, we prove the identiﬁability for a class of restricted latent class
models. We show the identiﬁability depends on the structure matrix
and propose a uniﬁed set of suﬃcient conditions under which the model
parameters are estimable from the data. For the restricted latent class
models under consideration, the identiﬁcation results are strict. From
an application perspective, the identiﬁability results would provide a
guideline for designing diagnostic tests, where in the current applica-
tions the design is usually experience based and the identiﬁability is
often not guaranteed.

ii) Second, we develop a new technique to establish the identiﬁability re-
sults for a class of restricted latent class models. Instead of working on
the tensor product, we propose to study the corresponding marginal
matrix, which has a nice algebra structure that can be well incorpo-
rated with the speciﬁed constraints.

The remainder of this paper is organized as follows. Section 2 introduces
the class of restricted models and contains useful background on the di-
agnostic classiﬁcation modeling and applications. Section 3 introduces the
issue of identiﬁability and our main results. The corresponding proofs are
given in Section 4.

2. Models and Applications.

2.1. Model setup. The models begin from the basic setting, in which
subjects (examinees, patients, etc) provide a J-dimensional binary response
vector R = (R1, ..., RJ )(cid:62) to J items (test questions, symptom diagnostic
questions, etc), where the superscript (cid:62) denotes the transpose, and these
responses depend in certain way on K unobserved latent traits (attributes,
skills, etc). A complete set of K latent traits is known as a latent class or
an attribute proﬁle, which is denoted by column vectors α = (α1, . . . , αK)(cid:62),
where αk ∈ {0, 1} indicate the absence or presence, respectively, of the
kth attribute. The above structure of α is often assumed in psychological
and educational measurement for the diagnosis purpose. For instance, in
a diagnostic math exam, teachers aim to estimate whether a student has
mastered certain math skills; in a psychiatry diagnosis, doctors want to
know whether a patient has certain mental depressions. Both α and R are

4

G. XU

subject-speciﬁc; a particular subject i’s attribute and response vectors are
denoted by αi and Ri, respectively, for i = 1, . . . , N . We assume that the
subjects are a random sample of size N from a designated population so that
their attribute proﬁles αi, i = 1, ..., N are i.i.d. random variables following
a multinomial distribution with probabilities

where pα ∈ (0, 1), for any α ∈ {0, 1}K, and(cid:80)

P (αi = α) = pα,

thus characterized by the column vector p = (pα : α ∈ {0, 1}K)(cid:62).

α pα = 1. The distribution is

Given a subject’s attribute proﬁle α, the response Rj to item j under the

corresponding model follows a Bernoulli distribution
P (Rj = r | α) = (θj,α)r(1 − θj,α)1−r,

(2.1)

r = 0, 1,

where we denote

θj,α = P (Rj = 1 | α),

which is the probability of providing positive response to item j for subjects
with α. Let Θ = (θj,α) be a J × 2K matrix containing the θ parameters.
The unknown model parameters of the latent class model include Θ and p.
In the following, we write ei as a standard basis vector, whose ith ele-
ment is one and the rest are zero. We write 0 and 1 as the zero and one
column vectors, i.e., (0, ..., 0)(cid:62) and (1, ..., 1)(cid:62), respectively. When there is no
ambiguity, we omit the index of length.

We consider a class of restricted latent class models where parameters
Θ = (θj,α) are constrained by the relationship between the J items and the
K latent traits. Such relationship is speciﬁed through a Q-matrix, which is
deﬁned as a J × K binary matrix with entries qjk ∈ {0, 1} indicating the
absence or presence, respectively, of a link between the jth item and the
kth latent trait. The row vectors, qj of Q correspond to the full attribute
requirements of each item. Given an attribute proﬁle α and a Q-matrix Q,
we write

α (cid:23) qj

if αk ≥ qjk for any k ∈ {1, . . . , K},

and

α (cid:15) qj

if there exists k such that αk < qjk;

similarly we deﬁne the operations (cid:22) and (cid:14).

If α (cid:23) qj, a subject with α has all the attributes for item j speciﬁed by
the Q-matrix and would be most “capable” to provide a positive answer;
on the other hand, if α(cid:48) (cid:15) qj, the subject with α(cid:48) misses some related
attribute and is expected not to have a higher positive response probability

IDENTIFIABILITY OF RESTRICTED LATENT CLASS MODELS

5

than α (cid:23) qj. In addition, subjects without mastery of any latent traits
(α = 0) is expected to have the lowest positive response probability. Such
constraints on Θ are proposed through the following monotonicity relations:

(2.2)

max
α: α(cid:23)qj

θj,α = min
α: α(cid:23)qj

θj,α ≥ θj,α(cid:48) ≥ θj,0,

for any α(cid:48);

in addition, for any k ∈ {1,··· , K} and item j with qj = ek,

(2.3)

θj,1 > max
α: α(cid:15)ek

θj,α.

Assumption (2.2) requires that, all the most capable subjects with α (cid:23) qj
have the same positive response probability. Assumption (2.3) assumes that
for an item only requiring the kth attribute, the most capable subjects with
α = 1 have higher positive response probability than those not having the
kth attribute. Both assumptions are satisﬁed by many of the restricted latent
class models as introduced in Section 2.2.

The Q-matrix is the key part of the restricted diagnostic models and its
structure makes them distinguished from the unrestricted latent class models
in the literature. Since some θ’s are restricted to be equal, the parameter
space then falls in a measure zero set with respect to the whole parameter
space under the unrestricted model.

2.2. Examples and Applications. The restricted latent class models in
Section 2.1 have recently gained great interests in cognitive diagnosis with
applications in educational assessment, psychiatric evaluation, and many
other disciplines (Rupp, Templin, and Henson, 2010; Tatsuoka, 2009), where
they are often called as diagnostic classiﬁcation models or cognitive diagnos-
tic models. Cognitive diagnosis is the process of arriving at a classiﬁcation-
based decision about an individual’s latent traits, based on his or her ob-
served surrogate responses. Measuring students’ growth and success means
obtaining diagnostic information about their skill set; this is very important
for constructing eﬃcient, focused remedial strategies for improving student
and teacher results. The introduced models are important statistical tools
developed in cognitive diagnosis to detect the presence or absence of multiple
ﬁne-grained skills or attributes.

We use a simple example for an illustration of the model setup.

Example 1. Suppose that we are interested in testing two latent traits:
addition and multiplication. Consider a test containing three problems and

6

G. XU

admitting the following Q-matrix,

(2.4)

Q =

addition multiplication

2 + 1
3 × 2

(2 + 1) × 2

1
0
1

0
1
1

We have four latent classes α = (0, 0), (1, 0), (0, 1), and (1, 1), corresponding
to subjects who do not master either addition or multiplication, who master
only addition, who master only multiplication, and who master both, respec-
tively. Take the ﬁrst item for an example. Under the restrictions in (2.2)
and (2.3), subjects who master addition, α = (1, 0), have a higher correct
response probability than those who do not master addition, α = (0, 0) or
(0, 1); on the other hand, they have the same correct response probability as
those who master both, α = (1, 1), since the ﬁrst item only needs addition.

The restriction structure in Section 2.1 is satisﬁed by many of diagnos-
tic models. An incomplete list of the popularly used restricted latent class
models developed in recent decades includes the DINA (Deterministic Input,
Noisy ‘And’ gate) and NIDA (Noisy Inputs, Deterministic ‘And’ gate) mod-
els (Junker and Sijtsma, 2001; de la Torre and Douglas, 2004), the reparam-
eterized uniﬁed/fusion model (RUM) (DiBello, Stout, and Roussos, 1995;
Hartz, 2002), the DINO (Deterministic Input, Noisy ‘Or’ gate) and NIDO
(Noisy Inputs, Deterministic ‘Or’ gate) (Templin and Henson, 2006), the
rule space method (Tatsuoka, 1983, 2009), the attribute hierarchy method
(Leighton, Gierl, and Hunka, 2004), the Generalized DINA models (de la
Torre, 2011), and the general diagnostic model (von Davier, 2008); see also
Henson et al. (2009) and Rupp et al. (2010). We use the following examples
to introduce some of the popularly used models.

Example 2 (DINA model). The DINA model (Junker and Sijtsma,
2001) assumes a conjunctive relationship among attributes. That is, it is
necessary to possess all the attributes indicated by the Q-matrix to be capable
of providing a positive response. In addition, having additional unnecessary
attributes does not compensate for the lack of necessary attributes. For item
= I(α (cid:23) qj).
j and attribute vector α, we deﬁne the ideal response ξDIN A
The uncertainty is further incorporated at the item level, using the slipping
and guessing parameters s and g. For each item j, the slipping parameter
sj = P (Rj = 0 | ξDIN A
= 1) denotes the probability of the respondent
making a negative response despite mastering all necessary skills; similarly,
the guessing parameter gj = P (Rj = 1 | ξDIN A
= 0) denotes the probabil-
ity of a positive response despite an incorrect ideal response. The response

j,α

j,α

j,α

IDENTIFIABILITY OF RESTRICTED LATENT CLASS MODELS

7

probability θj,α then takes the form

θj,α = (1 − sj)ξDIN A

1−ξDIN A
g
j

j,α

(2.5)
In this case, assumptions (2.2) and (2.3) are equivalent to 1 − sj > gj for
any item j, which is usually assumed in applications.

j,α

.

Example 3 (DINO model).

In contrast to the DINA model, the DINO
model assumes a non-conjunctive relationship among attributes, that is, one
only needs to have one of the required attributes to be capable of providing a
positive response. The ideal response of the DINO model is given by ξDIN O
=
I(αk ≥ qjk for at least one k). Similar to the DINA model, there are two
parameters s and g for each item, and

j,α

θj,α = (1 − sj)ξDIN O

j,α

1−ξDIN O
g
j

j,α

.

Again, assumptions (2.2) and (2.3) are satisﬁed if 1 − sj > gj for any j.

Example 4 (G-DINA model).

de la Torre (2011) generalizes the DINA
model to the G-DINA model. The formulation of the G-DINA model based
on θj,α can be decomposed into the sum of the eﬀects due the presence of
speciﬁc attributes and their interactions. Speciﬁcally,

K(cid:88)

K(cid:88)

K−1(cid:88)

k(cid:48)=k+1

k=1

(cid:89)

θj,α = βj0 +

βjk(qjkαk) +

βjkk(cid:48)(qjkαk)(qjk(cid:48)αk(cid:48))

k=1

+··· + βj12···K

(qjkαk).

k

stance, when qj (cid:54)= 1(cid:62), we do not need parameter βj12···K since(cid:81)

Note that not all β’s in the above equation are included in the model. For in-
k(qjkαk) =
0. To interpret, βj0 represents probability of a positive response when none
of the required attributes is present; when qjk = 1, βjk is included in the
model and it shows the change in the positive response probability as a result
of mastering a single attribute αk; when qjk = qjk(cid:48) = 1, βjkk(cid:48) is in the model
and it shows the change in the positive response probability due to the inter-
action eﬀect of mastery of both αk and αk(cid:48); similarly, when qj = 1(cid:62), βj12···K
represents the change in the positive response probability due to the interac-
tion eﬀect of mastery of all the required attributes. Note that the assumption
in (2.2), maxα: α(cid:23)qj θj,α = minα: α(cid:23)qj θj,α, is automatically satisﬁed from
the model deﬁnition from.

8

G. XU

Example 5 (Linear logistic model and logit-CDM). The linear logistic

model (LLM, see Hagenaars, 1993; Maris, 1999) is given by

(2.6)

θj,α =

Equivalently

exp(βj0 +(cid:80)K
1 + exp(βj0 +(cid:80)K
K(cid:88)

k=1 βjkqjkαk)

k=1 βjkqjkαk)

.

logit θj,α = βj0 +

βjkqjkαk.

k=1

This is also called the compensatory reparameterized uniﬁed model (C-RUM).
The LLM model (2.6) is recognized as a structure in multidimensional item
response theory model or in factor analysis. Again, we have maxα: α(cid:23)qj θj,α =
minα: α(cid:23)qj θj,α from (2.6).

Example 6 (Reduced RUM model and log-CDM). Under the reduced
version of the Reparameterized Uniﬁed Model (Reduced RUM, see DiBello
et al., 1995; Rupp et al., 2010), we have

(2.7)

θj,α = πj

qjk(1−αk),

rj,k

K(cid:89)

k=1

K(cid:88)

where πj is the positive response probability for subjects who possess all re-
quired attributes and rj,k, 0 < rj,k < 1, is the penalty parameter for not
possessing the kth attribute. Note that the model is equivalent to the log-link
model

log θj,α = βj0 +

βjk(qjkαk).

For the reduced RUM in (2.7), it is easy to see that assumptions (2.2) and
(2.3) are satisﬁed by the deﬁnition.

k=1

Psychometricians have long been aware of the identiﬁability issue of the
Q-matrix based latent class models (DiBello et al., 1995; Tatsuoka, 2009;
DeCarlo, 2011; Maris and Bechger, 2009). For these models, identiﬁability
aﬀects the classiﬁcation of respondents according to their latent traits, which
is dependent on the accuracy of the parameter estimates. Unprincipled use
of standard diagnostic models may lead to misleading conclusions about
the respondents’ latent traits (Maris and Bechger, 2009; Tatsuoka, 2009).
In the literature, the identiﬁability issue of diagnostic models has only been
studied for some speciﬁc models. Recently Xu (2013), Chen et al. (2015) and

IDENTIFIABILITY OF RESTRICTED LATENT CLASS MODELS

9

Xu and Zhang (2015) studied the identiﬁability of the slipping and guessing
parameters under the DINA model in Example 2. However, their technique
highly depends on the assumption that the subjects with ξDIN A = 0 having
the same response probability (i.e., the guessing parameters) and therefore
cannot be applied to the general diagnostic models considered in this paper,
where the Q-matrix restricted latent structure is more complicated.

3. Main results. We introduce the identiﬁability results in this section.

Throughout the rest of the discussion, we let Mj,· denote the jth row of a
matrix M and M·,k the kth column. We write Id as the d×d identity matrix.

3.1. Identiﬁability and response marginal T -matrix. The model parame-
ters contain the parameter matrix Θ = (θj,α)J×2K and proportion parameter
p = (pα)2K×1. Note the joint distribution of R, conditional on the latent
class α, is given by a J-dimensional 2 × ··· × 2 table

Note that πr,α(Q, Θ) is the probability of observing r given Q, Θ, and α.
Following the above notation, we can write

P (R = r | Q, Θ, p) =

πr,α(Q, Θ)pα.

We introduce the following identiﬁability deﬁnition for the Q-restricted la-
tent class models in Section 2.1.

Definition 1. We say that (Θ, p) is identiﬁable if the following holds:
(3.2) ∀r, P (R = r | Q, Θ, p) = P (R = r | Q, ¯Θ, ¯p) ⇐⇒ (Θ, p) = ( ¯Θ, ¯p).

Note that the above deﬁnition does not involve label swapping of the la-
tent classes due to the fact that the labels of attributes are pre-speciﬁed from
the knowledge of the Q-matrix. On the other hand, for unrestricted latent
class models, the latent classes can be freely relabeled without changing the

Pα(Q, Θ) =

θj,α
where the r = (r1,··· , rJ )-entry of the table is

j=1

(3.1)

πr,α(Q, Θ) =

(1 − θj,α)1−rj θrj
j,α.

J(cid:79)

(cid:20)1 − θj,α

(cid:21)

,

J(cid:89)

j=1

(cid:88)

α∈{0,1}K

10

G. XU

distribution of the data and the model parameters are therefore identiﬁable
only up to label swapping.
To establish (3.2) for the restricted latent models, directly working with
the vectors P (R = r | Q, Θ, p) is technically challenging. To better incor-
porate the induced restrictions by the Q-matrix, we consider the marginal
matrix as introduced in the following.
Marginal T -matrix. The T -matrix T (Q, Θ) is deﬁned as a 2J × 2K matrix,
where the entries are indexed by row index r ∈ {0, 1}J and column index α.
The r = (r1,··· , rJ )th row and αth column element of T (Q, Θ), denoted by
tr,α(Q, Θ), is the marginal probability that a subject with attribute proﬁle
α answers all items in subset {j : rj = 1} positively. Thus tr,α(Q, Θ) is the
marginal probability that, given Q, Θ, α, the random response R (cid:23) r, i.e.,

tr,α(Q, Θ) = P (R (cid:23) r | Q, Θ, α).

When r = 0, t0,α(Q, Θ) = P (R (cid:23) 0) = 1 for any α; and for any r (cid:54)= 0,

tr,α(Q, Θ) =

P (Rj = rj | Q, Θ, α) =

πr(cid:48),α(Q, Θ).

(cid:89)

(cid:88)

r(cid:48)(cid:23)r

j:rj =1

In particular, for r = ej with 1 ≤ j ≤ J,

tej ,α(Q, Θ) = P (Rj = 1 | Q, Θ, α) = θj,α.

Let Tr,·(Q, Θ) be the row vector corresponding to r. Then we know that for
j = 1,··· , J, Tej ,·(Q, Θ) = Θj,·. In addition, for any r (cid:54)= 0, we can write

(3.3)

Tr,·(Q, Θ) =

Tej ,·(Q, Θ),

(cid:75)

j:rj =1

where (cid:12) is the element-wise product of the row vectors.

By deﬁnition, multiplying the T -matrix by the the distribution of at-
tribute proﬁles p results in a vector containing the marginal probabilities of
successfully answering each subset of items correctly. The rth entry of this
vector is

Tr,·(Q, Θ)p =

tr,α(Q, Θ)pα = P (R (cid:23) r | Q, Θ, p).

(cid:88)

α

We can see that there is a one-to-one mapping between the T -matrix and
the vectors P (R = r | Q, Θ, p), r ∈ {0, 1}J . Therefore, (3.2) directly implies
the following proposition.

Proposition 1.

(Θ, p), there exists r ∈ {0, 1}J such that

IDENTIFIABILITY OF RESTRICTED LATENT CLASS MODELS

11
(Θ, p) is identiﬁable if and only if for any ( ¯Θ, ¯p) (cid:54)=
Tr,·(Q, Θ)p (cid:54)= Tr,·(Q, ¯Θ)¯p.

(3.4)

From Proposition 1, to show the identiﬁability of (Θ, p), we only need to

focus on the T -matrix and prove that if

(3.5)

T (Q, Θ)p = T (Q, ¯Θ)¯p,

then Θ = ¯Θ and p = ¯p. We will use this argument in the proof of the
identiﬁability results.

3.2. Identiﬁability results.

In this subsection, we present the main iden-
tiﬁability results. To illustrate which types of Q-matrix structure is required
to satisfy (3.4), we take as an example the basic DINA model introduced in
Example 2. We consider the ideal case where the jth response Rj = ξj,α,
where ξj,α denotes ξDIN A
as deﬁned in the example. In this ideal case, θj,α
is known as ξj,α and the only unknown parameter is p. Note that here
tej ,α(Q, Θ) = ξj,α and the identiﬁability condition is equivalent to

j,α

(ξj,α; j = 1,··· , J) (cid:54)= (ξj,α(cid:48); j = 1,··· , J)

(3.6)
for all α (cid:54)= α(cid:48). Otherwise, if there exists α (cid:54)= α(cid:48) such that (ξj,α; j =
1,··· , J) = (ξj,α(cid:48); j = 1,··· , J), the corresponding columns of the T -matrix
satisfy T·,α(Q, Θ) = T·,α(cid:48)(Q, Θ). This implies the nonidentiﬁability of p.

To guarantee (3.6), the mathematical requirements on the Q-matrix struc-

ture for the ideal case are speciﬁed in the following deﬁnition.

Definition 2. A Q-matrix is said to be complete if {e(cid:62)
{qj : j = 1,··· , J}; otherwise, we say that Q is incomplete.

j : j = 1, ..., K} ⊂

To interpret, for each attribute there must exist an item requiring that and
only that attribute. The Q-matrix is complete if there exist K rows of Q
that can be ordered to form the K-dimensional identity matrix IK. A simple
(and minimal) example of a complete Q-matrix is the K×K identity matrix
IK. Completeness ensures that there is enough information in the response
data for each attribute proﬁle to have its own distinct ideal response vector.
When a Q-matrix is incomplete, we can easily construct a non-identiﬁable
example. For instance, consider the incomplete Q-matrix

(cid:18) 1 1

0 1

(cid:19)

.

Q =

12

G. XU

The population parameter p is non-identiﬁable in this case. Subjects with
attribute proﬁles α1 = (1, 0)(cid:62) and α2 = (0, 0)(cid:62) have the same ideal re-
sponses, so (3.6) is not satisﬁed. It is easy to see that such argument holds
for general incomplete Q-matrix.

It has been established in the literature that the completeness of the Q-
matrix is a suﬃcient and necessary condition for the identiﬁability of p in
the ideal response case under DINA model with known Θ (Chiu et al., 2009;
Xu and Zhang, 2015). For the diagnostic models with unknown (Θ, p), com-
pleteness of the Q-matrix is not enough to guarantee the identiﬁability of
(Θ, p). For instance, Xu and Zhang (2015) showed that, under the DINA
model, a necessary condition for the identiﬁability of the guessing param-
eters, slipping parameters, and p is: (i) the Q-matrix is complete and (ii)
each latent trait is required by at least three items.

For diagnostic models in Section 2, we provide in the following a uniﬁed
suﬃcient condition that ensures their identiﬁability. Since the DINA model
is a special case of the restricted latent class models, it is necessary that
we need to use a complete Q-matrix for the diagnostic models and we need
at least three items for each attribute. To establish identiﬁability for the
general class of models, we list below the conditions that will be used.

(C1) We assume that the Q-matrix takes the following form (after row swap-

ping):

(3.7)

Q =

 .

 IKIK

Q(cid:48)

(C2) Suppose Q has the structure deﬁned in (3.7). We assume that for any
k ∈ {1,··· , K}, (θj,ek ; j > 2K)(cid:62) (cid:54)= (θj,0; j > 2K)(cid:62). That is, there
exist at least one item in Q(cid:48) such that subjects with α = ek have
diﬀerent positively response probability from that of subjects with
α = 0.

Remark 1. Condition C1 is a little stronger than the necessity of the
complete matrix by requiring two such identify matrices. C1 itself implies
that each attribute is required by at least two items. We need such con-
dition to ensure enough information to identify the model parameters for
each attribute. Condition C2 is satisﬁed if we assume for j > 2K, θj,0 <
minα(cid:54)=0 θj,α. That is, for subjects without any latent traits, the positive re-
sponse probability is the lowest among all latent classes. In practice condition
C2 may be checked by a posteriori empirically after data have been collected.

IDENTIFIABILITY OF RESTRICTED LATENT CLASS MODELS

13
On the other hand, condition C2 is satisﬁed if Q(cid:48) can be written as (after
row swapping):

(cid:19)

(cid:18)IK···

Q(cid:48) =

.

Therefore, if there are three identity matrices in the Q-matrix, both C1 and
C2 are satisﬁed.

Before stating the main theorem, we show in the following result that

condition C1 itself is not enough to establish the identiﬁability of (Θ, p).

Proposition 2. Under the model setup in Section 2.1, there exist Q-

matrices satisfying C1 but (Θ, p) is non-identiﬁable.

The proof of Proposition 2 is given in the appendix. Our main identiﬁa-

bility result is as follows.

Theorem 1. Under the model setup in Section 2.1, if conditions C1

and C2 hold, (Θ, p) is identiﬁable.

The theorem speciﬁes the suﬃcient condition under which the restricted
latent class model parameters (Θ, p) are identiﬁable from the response data.
From an application perspective, the identiﬁability result would provide a
guideline for designing diagnostic tests, where currently the design is usu-
ally experience based and may suﬀer identiﬁability problems. In particular,
for the diagnostic classiﬁcation models introduced in Section 2, the model
parameters are identiﬁable if the Q-matrix satisﬁes the proposed conditions
C1 and C2. Therefore, if single attribute items are possible, it is recom-
mended to have at least two complete matrices in the test which guarantees
C1; moreover, from Remark 1, both C1 and C2 hold if we have three iden-
tity matrices in the Q-matrix. The theoretical result would also help to
improve existing diagnostic tests. For instance, when researchers ﬁnd that
the estimation results are problematic and the Q-matrix does not satisfy the
identiﬁability conditions, it is then recommended to design new items such
that the identiﬁability conditions C1 and C2 are satisﬁed.
When the identiﬁability conditions are satisﬁed, the maximum likelihood
estimators of Θ and p are consistent as the sample size N → ∞. Speciﬁ-
i=1 I(Ri (cid:23)
i=1 I(Ri (cid:23)
1)}. From the deﬁnition of the T -matrix and the law of large numbers, we
know γ → T (Q, Θ)p almost surely as N → ∞. On the other hand, the

cally, we introduce a 2J -dimensional response vector γ = {1, N−1(cid:80)N
i=1 I(Ri (cid:23) e1 + e2),··· , N−1(cid:80)N
e1),··· , N−1(cid:80)N

i=1 I(Ri (cid:23) eJ ), N−1(cid:80)N

14

G. XU

maximum likelihood estimators ˆΘ and ˆp satisfy
(cid:107)γ − T (Q, ˆΘ)ˆp(cid:107) → 0,

where (cid:107) · (cid:107) is the L2 norm. Therefore,

(cid:107)T (Q, Θ)p − T (Q, ˆΘ)ˆp(cid:107) → 0

almost surely. Then from the proof of Theorem 1, we can obtain the con-
sistency result that ( ˆΘ, ˆp) → (Θ, p) almost surely. Furthermore, following a
standard argument of the asymptotic theory, we take Taylor’s expansion of
the loglikelihood function at (Θ, p) and the central limit theorem gives the
asymptotic normality of the estimators ( ˆΘ, ˆp).

Remark 2.

It is worthwhile to mention that our proof is not based on
the trilinear decomposition result in Kruskal (1976). Kruskal’s result is ap-
plied in Allman et al. (2009) to show the generic identiﬁability up to label
swapping. From their Corollary 5, a suﬃcient condition for the generic iden-
tiﬁability is that the number of items J is at least 2K + 1. Such a condition
is weaker than C1 and C2 due to the fact that C2 implicitly requires a non-
empty Q(cid:48) and thus C1 and C2 imply J ≥ 2K + 1. However, their result
can not be directly applied for the Q-restricted latent class models. In addi-
tion, we would like to point out that conditions C1 and C2 are diﬀerent from
the rank conditions required by Kruskal’s result and may be weaker in some
cases.

Remark 3. When the Q-matrix is incomplete, the model parameters
(Θ, p) are nonidentiﬁable. A particular case is when each row of the Q-
matrix is 1(cid:62), then the model becomes similar as the unrestricted latent class
models with 2K classes. In this case, generic identiﬁability results as in All-
man et al. (2009) can still be applied. For a general incomplete Q-matrix,
such results are still unknown in the literature. We plan to study the generic
identiﬁability for the parameters in the constrained parameter space when
the Q-matrix is incomplete. These results would be helpful for practitioners,
especially when it becomes diﬃcult or even impossible to design items with
particular attribute speciﬁcations.

It is also possible in practice that there exist certain hierarchical struc-
tures among the latent attributes. For instance, a certain attribute may be a
prerequisite for other attributes. In this case, some p’s are restricted to be 0.
The method developed in this paper may be extended to this type of restricted
latent class models, and we would like to study this in the future.

IDENTIFIABILITY OF RESTRICTED LATENT CLASS MODELS

15

4. Proof of the main results.

4.1. Proof of Theorem 1. To show the identiﬁability, Proposition 1 im-
plies that it suﬃces to show that for two sets of parameters (Θ, p) and ( ¯Θ, ¯p)
satisfying equation (3.5), we must have (Θ, p) = ( ¯Θ, ¯p).

Without loss of generality, we arrange the rows of Q such that it takes
the form of (3.7) in condition C1. For notational convenience, we write
tej ,α(Q, Θ) and tej ,α(Q, ¯Θ) as tej ,α and ¯tej ,α, respectively. Note that by
the deﬁnition of the T -matrix, tej ,α = θj,α and ¯tej ,α = ¯θj,α for any j ∈
{1,··· , J} and α ∈ {0, 1}K. Therefore to show Θ = ¯Θ, it is equivalent to
show tej ,α = ¯tej ,α for any j ∈ {1,··· , J} and α ∈ {0, 1}K.

We prove the theorem in ﬁve Steps. Given equation (3.5) that T (Q, Θ)p =

T (Q, ¯Θ)¯p, we aim to prove the following conclusions in each step:

Fig 1. An illustration of the proof steps

Step 1 tej ,0 = ¯tej ,0 for j > 2K;
Step 2 tej ,ek = ¯tej ,ek for j > 2K and k ∈ {1,··· , K};
Step 3 tej ,0 = ¯tej ,0, tej ,ek = ¯tej ,ek , p0 = ¯p0 and pek = ¯pek for j ∈ {1,··· , 2K}
for j ∈ {1,··· , J}
for j ∈ {1,··· , J}

and k ∈ {1,··· , K};
= ¯tej ,eh1 +eh2
and 1 ≤ h1 (cid:54)= h2 ≤ K;
= ¯p(cid:80)k
= ¯tej ,(cid:80)k
i=1 ehi
i=1 ehi
and 1 ≤ h1 (cid:54)= ··· (cid:54)= hk ≤ K with any 2 < k ≤ K.

Step 5 tej ,(cid:80)k

and peh1 +eh2

= ¯peh1 +eh2

and p(cid:80)k

i=1 ehi

Step 4 tej ,eh1 +eh2

i=1 ehi

	  Step	  3	  	  	  	  	  	  	  	  	  	  	  	  Steps	  	  	  	  	  4&5	  	  	  Step1	  	  	  	  Step	  2	  Item1	  	  …	  	  	  2K	  	  	  2K+1	  	  …	  	  	  	  J	  	  α	  =	  	  	  0	  	  	  	  	  	  	  	  	  e1	  	  	  	  	  	  	  	  	  	  	  ……	  	  	  	  	  eK	  	  	  	  	  	  	  e1+e2	  	  	  	  	  	  ……	  	  	  	  1	  Matrix	  Θ	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  p	  	  Step	  3	  Steps	  	  4	  &	  5	  α	  =	  0	  	  e1	  	  …	  	  	  eK	  	  e1+e2	  	  	  …	  	  	  1	  16

G. XU

For a better illustration, Figure 1 speciﬁes the corresponding components
of the Θ matrix and the p vector that we will focus on in each step. Com-
bining the results in the ﬁve steps, we have the desired conclusion that
Θ = ¯Θ and p = ¯p.

In order to establish Steps 1–5, we need to incorporate into (3.5) the con-
straints of the parameters under the restricted latent class models. This is
achieved by the following linear transformation of the T -matrix in Propo-
sition 3. We extend the deﬁnition of T -matrix through (3.3) to include
Θ (cid:54)∈ [0, 1]J×2K , where tr,α(Q, Θ) will no longer correspond to probabili-
ties. We order the column indices of the T -matrix from left to right as
(0, e1,··· , eK, e1 + e2,··· , eK−1 + eK,··· , 1) and the row indices from top
to bottom as (0, e1,··· , eJ , e1 + e2,··· , eJ−1 + eJ ,··· , 1).

Proposition 3. For any θ∗ = (θ∗

J )(cid:62) ∈ RJ , there exists an in-
vertible matrix D(θ∗) depending solely on θ∗, such that the matrix D(θ∗) is
lower triangular with diagonal diag{D(θ∗)} = 1, and
T (Q, Θ − θ∗1(cid:62)) = D(θ∗)T (Q, Θ).

1, ..., θ∗

Proposition 3 shows that equation (3.5) is equivalent to
T (Q, Θ − θ∗1(cid:62))p = T (Q, ¯Θ − θ∗1(cid:62))¯p.

j ,··· , θ∗

j 1(cid:62) = (θ∗
j ; α ∈ {0, 1}K). Thus, if we take θ∗

Note that the vector product θ∗1(cid:62) is a J × 2K matrix with the jth row
j )1×2K , and the jth row vector of Θ − θ∗1(cid:62) is
equal to θ∗
(θj,α − θ∗
j equal to θj,α, the correspond-
ing element in Θ − θ∗1(cid:62) will become 0. By properly choosing the vector
θ∗ according to the Q-restrictions, we can then make certain elements in
T (Q, Θ− θ∗1(cid:62)) to be 0. For instance, if we choose θ∗
1 = te1,1(Q, Θ), then we
have the transformed matrix elements te1,α(Q, Θ−θ∗1(cid:62)) = 0 for all α (cid:23) q1.
This nice algebraic structure makes the transformed T -matrix much easier
to work with and plays a key role in the following proof.

(cid:1)(cid:62)

,

θ∗ =(cid:0) ¯te1,1,··· , ¯teK ,1
Step 1. We apply the result in Proposition 3. Deﬁne
(cid:125)
(cid:124)
(cid:125)
, teK+1,1,··· , te2K ,1

(cid:123)(cid:122)

(cid:124)

(cid:123)(cid:122)

K

K

(cid:124) (cid:123)(cid:122) (cid:125)

, 0,··· , 0
J−2K

and (3.5) gives

T (Q, Θ − θ∗1(cid:62))p = T (Q, ¯Θ − θ∗1(cid:62))¯p.

(4.1)
Note that for any k ∈ {1,··· , K}, ¯tek,α − θ∗
and similarly, teK+k,α − θ∗

K+k = teK+k,α − teK+k,1 = 0 if α (cid:23) ek.

k = ¯tek,α − ¯tek,1 = 0 if α (cid:23) ek,

IDENTIFIABILITY OF RESTRICTED LATENT CLASS MODELS

Consider the row vector of T (Q, Θ−θ∗1(cid:62)) corresponding to r =(cid:80)2K
i.e., T(cid:80)2K
k=1 ek,·(Q, Θ−θ∗1(cid:62)). From the deﬁnition form (3.3) of the T -matrix,

k=1 ek,

17

we know

T(cid:80)2K

k=1 ek,·(Q, Θ − θ∗1(cid:62)) =

(cid:111)
(cid:110)
2K(cid:75)
Tek,·(Q, Θ − θ∗1(cid:62))
(cid:32) K(cid:89)
2K(cid:89)
That is, the last 2K − 1 elements of the row vector T(cid:80)2K
are 0. Next we show that the ﬁrst element of T(cid:80)2K

(tek,0 − ¯tek,1)

k=K+1

k=1

k=1

=

nonzero, i.e.,

K(cid:89)

(tek,0 − ¯tek,1)

2K(cid:89)

(tek,0 − tek,1) (cid:54)= 0.

(cid:33)

.

(tek,0 − tek,1), 0(cid:62)
k=1 ek,·(Q, Θ − θ∗1(cid:62))
k=1 ek,·(Q, Θ − θ∗1(cid:62)) is

k=1

k=K+1

We introduce the following lemma, whose proof is in Section 4.2.

Lemma 1. Under the conditions of Theorem 1, if (3.5) holds, then for

any 1 ≤ k ≤ K and α∗ (cid:23) ek
tek,0 (cid:54)= ¯tek,α∗, tek,α∗ (cid:54)= ¯tek,0, teK+k,0 (cid:54)= ¯teK+k,α∗ and teK+k,α∗ (cid:54)= ¯teK+k,0.
Lemma 1 implies that(cid:81)K
tion that tek,0 < tek,1 for k ∈ {1, . . . , 2K}, we have(cid:81)2K
k=1(tek,0− ¯tek,1) (cid:54)= 0. In addition, from the assump-
k=K+1(tek,0 − tek,1) (cid:54)=
0. Thus the ﬁrst element of the row vector T(cid:80)2K
k=1 ek,·(Q, Θ− θ∗1(cid:62)) is not 0.
(cid:33)
2K(cid:89)

Similarly, by doing the same transformation, we have

(¯tek,0 − tek,1), 0(cid:62)

,

(cid:32) K(cid:89)
k=1(¯tek,0 − ¯tek,1)(cid:81)2K

(¯tek,0 − ¯tek,1)

k=1

k=K+1

T(cid:80)2K
k=1 ek,·(Q, ¯Θ − θ∗1(cid:62)) =
where the ﬁrst element (cid:81)K

k=K+1(¯tek,0 − tek,1) (cid:54)= 0 and
Now consider any j > 2K. The row vector of T (Q, Θ− θ∗1(cid:62)) correspond-

the rest elements are 0.

(cid:33)

0(cid:62)

(cid:33)

k=1

18

G. XU

=

=

and similarly

k=1 ek equals

k=1

k=K+1

(tek,0 − ¯tek,1)

(tek,0 − tek,1),

ing to r = ej +(cid:80)2K
Tej +(cid:80)2K
(cid:34) 2K(cid:75)
(cid:111)(cid:35)
k=1 ek,·(Q, Θ − θ∗1(cid:62))
(cid:110)
= Tej ,·(Q, Θ − θ∗1(cid:62)) (cid:12)
Tek,·(Q, Θ − θ∗1(cid:62))
(cid:32)
2K(cid:89)
tej ,0 × K(cid:89)
= tej ,0 · T(cid:80)2K
k=1 ek,·(Q, Θ − θ∗1(cid:62))
Tej +(cid:80)2K
(cid:32)
k=1 ek,·(Q, ¯Θ − θ∗1(cid:62))
¯tej ,0 × K(cid:89)
(¯tek,0 − ¯tek,1)
= ¯tej ,0 · T(cid:80)2K
k=1 ek,·(Q, Θ − θ∗1(cid:62))p = Tej +(cid:80)2K
Tej +(cid:80)2K
T(cid:80)2K
k=1 ek,·(Q, Θ − θ∗1(cid:62))p = T(cid:80)2K
Tej +(cid:80)2K
Tej +(cid:80)2K
T(cid:80)2K
T(cid:80)2K

2K(cid:89)
k=1 ek,·(Q, ¯Θ − θ∗1(cid:62)).

k=1 ek,·(Q, Θ − θ∗1(cid:62))p
k=1 ek,·(Q, Θ − θ∗1(cid:62))p

(¯tek,0 − tek,1), 0(cid:62)

Thus for any j > 2K,

By equation (4.1)

k=1

k=K+1

tej ,0 =

and

k=1 ek,·(Q, ¯Θ − θ∗1(cid:62))¯p
k=1 ek,·(Q, ¯Θ − θ∗1(cid:62))¯p.

=

k=1 ek,·(Q, ¯Θ − θ∗1(cid:62))¯p
k=1 ek,·(Q, ¯Θ − θ∗1(cid:62))¯p

= ¯tej ,0.

This completes Step 1.

Step 2. To better illustrate our idea, we ﬁrst focus on the column with
respect to α = e1 and show

tej ,e1 = ¯tej ,e1 for j > 2K.

We redeﬁne the θ∗ vector as

θ∗ =(cid:0) ¯te1,0, ¯te2,1,··· , ¯teK ,1
(cid:125)

(cid:123)(cid:122)

(cid:124)

K

(cid:124)
(cid:125)
, teK+1,0, teK+2,1,··· , te2K ,1

(cid:123)(cid:122)

K

(cid:1)(cid:62)

,

(cid:124) (cid:123)(cid:122) (cid:125)

, 0,··· , 0
J−2K

where the ﬁrst element is ¯te1,0 and the (K + 1)th element is teK+1,0 while
the other elements are the same as the θ∗ vector taken in Step 1. For the

IDENTIFIABILITY OF RESTRICTED LATENT CLASS MODELS

19
chosen θ∗, the row vectors of the transformed T -matrices corresponding to

items 1,..., 2K, i.e., r =(cid:80)2K

k=1 ek, are

T(cid:80)2K
k=1 ek,·(Q, Θ − θ∗1(cid:62)) =
(cid:18)
0, (te1,e1 − ¯te1,0)

K(cid:89)

k=2

×(teK+1,e1 − teK+1,0)

T(cid:80)2K
k=1 ek,·(Q, ¯Θ − θ∗1(cid:62)) =
(cid:18)
0, (¯te1,e1 − ¯te1,0)

K(cid:89)

k=2

×(¯teK+1,e1 − teK+1,0)

(tek,e1 − ¯tek,1)

k=K+2

2K(cid:75)

k=1

2K(cid:89)

2K(cid:75)

k=1

2K(cid:89)

(cid:111)

(cid:110)
Tek,·(Q, Θ − θ∗1(cid:62))
(tek,e1 − tek,1), 0(cid:62)(cid:19)
(cid:111)
(cid:110)
Tek,·(Q, ¯Θ − θ∗1(cid:62))
(¯tek,e1 − tek,1), 0(cid:62)(cid:19)

.

,

(¯tek,e1 − ¯tek,1)

=

=

(4.2)

and

(4.3)

We now show the second elements of the above two vectors are nonzero. We
need the following lemma, which is proved in Section 4.2.

k=K+2

Lemma 2. Under the conditions of Theorem 1, if (3.5) holds, then for

any 1 ≤ k (cid:54)= h ≤ K,
tek,eh (cid:54)= ¯tek,1,
teK+k,eh (cid:54)= ¯teK+k,1, and teK+k,1 (cid:54)= ¯teK+k,eh.
Consider vector (4.2). Lemma 1 implies that (te1,e1 − ¯te1,0) (cid:54)= 0, and

Lemma 2 implies

tek,1 (cid:54)= ¯tek,eh,
K(cid:89)

(tek,e1 − ¯tek,1) (cid:54)= 0.

Moreover, for the term (teK+1,e1 − teK+1,0), since the (K + 1)th item only
requires the ﬁrst attribute, i.e., the q-vector is e1, we know teK+1,e1 =
teK+1,1 > teK+1,0. Similarly, we have

k=2

2K(cid:89)

k=K+2

(tek,e1 − tek,1) (cid:54)= 0.

20

G. XU

The above results implies that the second element of (4.2) is nonzero. From
a similar argument, the second element of (4.3) is also nonzero.

Now consider any j ≥ 2K + 1. We have
k=1 ek,·(Q, Θ − θ∗1(cid:62))
K(cid:89)

Tej +(cid:80)2K
(cid:18)
0, tej ,e1(te1,e1 − ¯te1,0)

=

and

= tej ,e1 · T(cid:80)2K
Tej +(cid:80)2K
(cid:18)
0, ¯tej ,e1(¯te1,e1 − ¯te1,0)

k=1 ek,·(Q, ¯Θ − θ∗1(cid:62))
K(cid:89)

=

×(teK+1,e1 − teK+1,0)

k=2

(tek,e1 − ¯tek,1)
2K(cid:89)
k=1 ek,·(Q, Θ − θ∗1(cid:62))

(tek,e1 − tek,1), 0(cid:62)(cid:19)

k=K+2

As in Step 1, since

k=2

k=K+2

×(¯teK+1,e1 − teK+1,0)

(¯tek,e1 − tek,1), 0(cid:62)(cid:19)

(¯tek,e1 − ¯tek,1)
2K(cid:89)
= ¯tej ,e1 · T(cid:80)2K
k=1 ek,·(Q, ¯Θ − θ∗1(cid:62)).
Tej +(cid:80)2K
k=1 ek,·(Q, Θ − θ∗1(cid:62))p = Tej +(cid:80)2K
T(cid:80)2K
k=1 ek,·(Q, Θ − θ∗1(cid:62))p = T(cid:80)2K
k=1 ek,·(Q, ¯Θ − θ∗1(cid:62))¯p,
Tej +(cid:80)2K
Tej +(cid:80)2K
k=1 ek,·(Q, Θ − θ∗1(cid:62))p
T(cid:80)2K
T(cid:80)2K
k=1 ek,·(Q, Θ − θ∗1(cid:62))p
θ∗ = (cid:0) ¯te1,1,··· , ¯teh−1,1, ¯teh,0, ¯teh+1,1,··· , ¯teK ,1
(cid:125)
(cid:123)(cid:122)
(cid:124)
(cid:124)
(cid:125)
teK+1,1,··· , teK+h−1,1, teK+h,0, teK+h+1,1,··· , te2K ,1

k=1 ek,·(Q, ¯Θ − θ∗1(cid:62))¯p
k=1 ek,·(Q, ¯Θ − θ∗1(cid:62))¯p

k=1 ek,·(Q, ¯Θ − θ∗1(cid:62))¯p,

(cid:124) (cid:123)(cid:122) (cid:125)

, 0,··· , 0
J−2K

(cid:123)(cid:122)

=

K

,

we have

tej ,e1 =

The above argument can be easily generalized to any 1 < h ≤ K. Redeﬁne

Following a similar argument as above, we can get for any j ≥ 2K + 1 and
k ∈ {1,··· , K}, tej ,ek = ¯tej ,ek . This completes Step 2.

K

= ¯tej ,e1.

(cid:1)(cid:62)

.

IDENTIFIABILITY OF RESTRICTED LATENT CLASS MODELS

21

Step 3. From assumption C2, for any 1 ≤ k ≤ K,

(te2K+1,ek ,··· , teJ ,ek )(cid:62) (cid:54)= (te2K+1,0,··· , teJ ,0)(cid:62).

Then (1, te2K+1,ek ,··· , teJ ,ek )(cid:62) and (1, te2K+1,0,··· , teJ ,0)(cid:62) are not propor-
tional to each other. There exists a (J − 2K + 1)-dimensional row vector uk
such that
bk := uk(1, te2K+1,ek ,··· , teJ ,ek )(cid:62) (cid:54)= 0 and uk(1, te2K+1,0,··· , teJ ,0)(cid:62) = 0.

Consider matrix

A(Q, Θ) =



1(cid:62)

Te2K+1,·(Q, Θ)
Te2K+2,·(Q, Θ)
TeJ ,·(Q, Θ)

...

 .

From the ﬁrst two steps, we know that the ﬁrst K + 1 columns of A(Q, Θ)
and A(Q, ¯Θ) are equal. For simplicity, we write A(Q, Θ) and A(Q, ¯Θ) as A
and ¯A, respectively. Then we have

(4.4)

ukA = (0,∗, . . . ,∗,

uk ¯A = (0,∗, . . . ,∗,

bk(cid:124)(cid:123)(cid:122)(cid:125)
bk(cid:124)(cid:123)(cid:122)(cid:125)

column ek

column ek

, ∗, . . . ,∗),

, ∗, . . . ,∗),

where ∗’s are unspeciﬁed values.

We use the above results to prove Step 3. For h ∈ {1,··· , K}, redeﬁne

θ∗ = (cid:0) ¯te1,1,··· , ¯teh−1,1, 0, ¯teh+1,1,··· , ¯teK ,1
(cid:125)

(cid:124)
(cid:124)
(cid:125)
teK+1,1,··· , teK+h−1,1, 0, teK+h+1,1,··· , te2K ,1

(cid:123)(cid:122)

(cid:123)(cid:122)

,

K

K

(cid:1)(cid:62)

(cid:124) (cid:123)(cid:122) (cid:125)

, 0,··· , 0
J−2K

and we have T (Q, Θ − θ∗1(cid:62))p = T (Q, ¯Θ − θ∗1(cid:62))¯p. With such a choice of
θ∗, for any k ∈ {1, , K} and k (cid:54)= h, ¯tek,α − θ∗
k = ¯tek,α − ¯tek,1 = 0 if α (cid:23) ek,
and similarly, teK+k,α − θ∗

K+k = teK+k,α − teK+k,1 = 0 if α (cid:23) ek.

Consider the row vectors of T -matrices corresponding to items 1,..., 2K

22

except h and K + h, i.e., r =(cid:80)2K

G. XU

k=1 ek − eh − eK+h. We have

T(cid:80)2K
(cid:18) (cid:89)
k=1 ek−eh−eK+h,·(Q, Θ − θ∗1(cid:62))
(tek,0 − ¯tek,1) × (cid:89)
(tek,eh − ¯tek,1) × (cid:89)
(cid:89)

k=1,··· ,K,

k(cid:54)=h

k=K+1,··· ,2K,

k(cid:54)=K+h

=

(4.5)

k=1,··· ,K,

k(cid:54)=h

(cid:124)

k=K+1,··· ,2K,

k(cid:54)=K+h

(cid:123)(cid:122)

column eh

(tek,0 − tek,1), 0(cid:62),

, 0(cid:62)(cid:19)
(tek,eh − tek,1)
(cid:125)

,

where the second product term corresponds to column eh, and

T(cid:80)2K
(cid:18) (cid:89)
k=1 ek−eh−eK+h,·(Q, ¯Θ − θ∗1(cid:62))
(¯tek,0 − ¯tek,1) × (cid:89)
(cid:89)
(¯tek,eh − ¯tek,1) × (cid:89)

k=1,··· ,K,

k(cid:54)=h

k=K+1,··· ,2K,

k(cid:54)=K+h

=

(4.6)

k=1,··· ,K,

k(cid:54)=h

(cid:124)

k=K+1,··· ,2K,

k(cid:54)=K+h

(cid:123)(cid:122)

column eh

(¯tek,0 − tek,1), 0(cid:62)

, 0(cid:62)(cid:19)
(¯tek,eh − tek,1)
(cid:125)

.

binations, the row vectors corresponding to r =(cid:80)2K

From Lemmas 1–2 and the model assumption, we know the product com-
ponents in (4.5) and (4.6) are nonzero. Adding item h into the above com-
k=1 ek − eK+h equal to

=

(4.7)

T(cid:80)2K
(cid:18)
k=1 ek−eK+h,·(Q, Θ − θ∗1(cid:62))
teh,0 × (cid:89)
teh,eh × (cid:89)
(cid:124)

(tek,0 − ¯tek,1) × (cid:89)
(tek,eh − ¯tek,1) × (cid:89)

k=1,··· ,K,

k=1,··· ,K,

(cid:123)(cid:122)

k(cid:54)=h

k(cid:54)=h

column eh

k=K+1,··· ,2K,

k(cid:54)=K+h

k=K+1,··· ,2K,

k(cid:54)=K+h

(tek,0 − tek,1), 0(cid:62)

, 0(cid:62)(cid:19)
(tek,eh − tek,1)
(cid:125)

,

IDENTIFIABILITY OF RESTRICTED LATENT CLASS MODELS

23

and

=

(4.8)

T(cid:80)2K
(cid:18)
k=1 ek−eK+h,·(Q, ¯Θ − θ∗1(cid:62))
¯teh,0 × (cid:89)
¯teh,eh × (cid:89)
(cid:124)

(¯tek,0 − ¯tek,1) × (cid:89)
(¯tek,eh − ¯tek,1) × (cid:89)

k=1,··· ,K,

k=1,··· ,K,

(cid:123)(cid:122)

k(cid:54)=h

k(cid:54)=h

column eh

k=K+1,··· ,2K,

k(cid:54)=K+h

k=K+1,··· ,2K,

k(cid:54)=K+h

(¯tek,0 − tek,1), 0(cid:62)

, 0(cid:62)(cid:19)
(¯tek,eh − tek,1)
(cid:125)

.

Take the element-wise product of the row vectors: uhA deﬁned in (4.4)

and the vector in (4.5). We have

(uhA) (cid:12) T(cid:80)2K
(cid:18)
k=1 ek−eh−eK+h,·(Q, Θ − θ∗1(cid:62))
(cid:89)
(tek,eh − ¯tek,1) × (cid:89)

0, bh

k=1,··· ,K,

k(cid:54)=h

k=K+1,··· ,2K,

k(cid:54)=K+h

column eh

From uh ¯A in (4.4) and the vector in (4.6)

(uh ¯A) (cid:12) T(cid:80)2K
(cid:18)
k=1 ek−eh−eK+h,·(Q, ¯Θ − θ∗1(cid:62))
(¯tek,eh − ¯tek,1) × (cid:89)
(cid:89)

0, bh

k=1,··· ,K,

k(cid:54)=h

k=K+1,··· ,2K,

k(cid:54)=K+h

column eh

=

=

(cid:124)

(cid:124)

, 0(cid:62)(cid:19)
(tek,eh − tek,1)
(cid:125)

.

, 0(cid:62)(cid:19)
(¯tek,eh − tek,1)
(cid:125)

,

(cid:123)(cid:122)

(cid:123)(cid:122)

Similarly, the element-wise product of uhA and (4.7) gives

=

0, bhteh,eh

(uhA) (cid:12) T(cid:80)2K
(cid:18)
(cid:124)
= teh,eh ·(cid:110)

(tek,eh − ¯tek,1) × (cid:89)

k=1 ek−eK+h,·(Q, Θ − θ∗1(cid:62))
(cid:89)
(cid:123)(cid:122)

, 0(cid:62)(cid:19)
(tek,eh − tek,1)
(cid:125)
(cid:111)
k=1 ek−eh−eK+h,·(Q, Θ − θ∗1(cid:62))

k=K+1,··· ,2K,

k=1,··· ,K,

k(cid:54)=K+h

column eh

k(cid:54)=h

(uhA) (cid:12) T(cid:80)2K

,

(4.9)

24

G. XU

and the element-wise product of uh ¯A and (4.8) gives

=

0, bh¯teh,eh

(uh ¯A) (cid:12) T(cid:80)2K
(cid:18)
(cid:124)
= ¯teh,eh ·(cid:110)

(¯tek,eh − ¯tek,1) × (cid:89)

k=1 ek−eK+h,·(Q, ¯Θ − θ∗1(cid:62))
(cid:89)
(cid:123)(cid:122)

, 0(cid:62)(cid:19)
(¯tek,eh − tek,1)
(cid:125)
(cid:111)
k=1 ek−eh−eK+h,·(Q, ¯Θ − θ∗1(cid:62))

k=K+1,··· ,2K,

k=1,··· ,K,

k(cid:54)=K+h

column eh

k(cid:54)=h

(uh ¯A) (cid:12) T(cid:80)2K

.

(4.10)
From the equation that T (Q, Θ − θ∗1(cid:62))p = T (Q, ¯Θ − θ∗1(cid:62))¯p, we know

=

(cid:111)
(cid:111)
k=1 ek−eh−eK+h,·(Q, Θ − θ∗1(cid:62))
k=1 ek−eh−eK+h,·(Q, ¯Θ − θ∗1(cid:62))
(cid:111)
(cid:111)
k=1 ek−eK+h,·(Q, Θ − θ∗1(cid:62))
k=1 ek−eK+h,·(Q, ¯Θ − θ∗1(cid:62))
Therefore, (4.9) and (4.10) imply that for h = 1,··· , K,

(cid:110)
(uhA) (cid:12) T(cid:80)2K
(cid:110)
(uh ¯A) (cid:12) T(cid:80)2K
(cid:110)
(uhA) (cid:12) T(cid:80)2K
(cid:110)
(uh ¯A) (cid:12) T(cid:80)2K

and

=

p

¯p.

p

¯p

(4.11)

teh,eh = ¯teh,eh.

Similarly, we have teK+h,eh = ¯teK+h,eh.

Furthermore, there exists row vector vk such that
vk(1, te2K+1,ek ,··· , teJ ,ek )(cid:62) = 0 and vk(1, te2K+1,0,··· , teJ ,0)(cid:62) (cid:54)= 0.

A similar argument then gives

Before to prove tej ,eh = ¯tej ,eh for the rest j ∈ {1,··· , 2K} and h ∈
{1,··· , K}, we ﬁrst show p0 = ¯p0 and peh = ¯peh for h ∈ {1,··· , K}. Take

teh,0 = ¯teh,0 for h = 1,··· , 2K.
(cid:1)(cid:62)

θ∗ =(cid:0) te1,1,··· , teK ,1
(cid:125)

(cid:124) (cid:123)(cid:122) (cid:125)

, 0,··· , 0
J−K

(cid:123)(cid:122)

(cid:124)

K

.

IDENTIFIABILITY OF RESTRICTED LATENT CLASS MODELS

25

By the results that teh,eh = teh,1 and (4.11), we know

T(cid:80)K
= T(cid:80)K

k=1 ek,·(Q, Θ − θ∗1(cid:62))
k=1 ek,·(Q, ¯Θ − θ∗1(cid:62)) =

(cid:18) K(cid:89)

(tek,0 − tek,1), 0(cid:62)(cid:19)

k=1

where the product element is nonzero under the model assumption. Then
the equation

T(cid:80)K
k=1 ek,·(Q, Θ − θ∗1(cid:62))p = T(cid:80)K

k=1 ek,·(Q, ¯Θ − θ∗1(cid:62))¯p

implies

p0 = ¯p0.

Now for any h ∈ {1,··· , K}, take

θ∗ =(cid:0) te1,1,··· , teh−1,1, teh,0, teh+1,1,··· , teK ,1
(cid:125)

(cid:123)(cid:122)

(4.12)

(cid:124)

K

(cid:1)(cid:62)

.

(cid:124) (cid:123)(cid:122) (cid:125)

, 0,··· , 0
J−K

=

From the results in (4.11), we have

k=1 ek,·(Q, Θ − θ∗1(cid:62)) = T(cid:80)K
T(cid:80)K
(cid:18)
(cid:89)
0(cid:62), (teh,eh − teh,0)
(cid:123)(cid:122)

(cid:124)
k=1 ek,·(Q, Θ − θ∗1(cid:62))p = T(cid:80)K
Then the equation T(cid:80)K
peh = ¯peh for h = 1,··· , K.

k=1,··· ,K,

implies

column eh

(4.13)

k(cid:54)=h

, 0(cid:62)(cid:19)
k=1 ek,·(Q, ¯Θ − θ∗1(cid:62))
(tek,eh − tek,1)
(cid:125)

.

k=1 ek,·(Q, ¯Θ − θ∗1(cid:62))¯p

We continue to show tej ,eh = ¯tej ,eh for the rest j ∈ {1,··· , 2K} and h ∈
{1,··· , K}. Consider any j and h such that K < j ≤ 2K and 1 ≤ h ≤ K.
For θ∗ in (4.12) we have
Tej +(cid:80)K
(cid:18)
k=1 ek,·(Q, Θ − θ∗1(cid:62))
0(cid:62), tej ,eh(teh,eh − teh,0)
(cid:124)
(cid:123)(cid:122)

, 0(cid:62)(cid:19)
(tek,eh − tek,1)
(cid:125)

(cid:89)

k=1,··· ,K,

k(cid:54)=h

=

.

column eh

26

and

=

G. XU

Tej +(cid:80)K
(cid:18)
k=1 ek,·(Q, ¯Θ − θ∗1(cid:62))
0(cid:62), ¯tej ,eh(teh,eh − teh,0)
(cid:124)
(cid:123)(cid:122)

(cid:89)

k=1,··· ,K,

k(cid:54)=h

, 0(cid:62)(cid:19)
(tek,eh − tek,1)
(cid:125)

.

column eh

Then from (4.13) and

Tej +(cid:80)K

k=1 ek,·(Q, Θ − θ∗1(cid:62))p = Tej +(cid:80)K

k=1 ek,·(Q, ¯Θ − θ∗1(cid:62))¯p,

we obtain

For any j and h such that 1 ≤ j ≤ K and 1 ≤ h ≤ K, take

tej ,eh = ¯tej ,eh.

θ∗ =(cid:0) 0,··· , 0
(cid:124) (cid:123)(cid:122) (cid:125)

K

(cid:124)
(cid:125)
, teK+1,1,··· , teK+h−1,1, teK+h,0, teK+h+1,1,··· , te2K ,1

(cid:123)(cid:122)

K

(cid:1)(cid:62)

(cid:124) (cid:123)(cid:122) (cid:125)

, 0,··· , 0
J−2K

and a similar argument gives tej ,eh = ¯tej ,eh. This completes Step 3.
Step 4. The proof for Step 4 and Step 5 uses similar arguments. To better
illustrate our idea, we separate them in two steps. In particular, in Step 4,
we consider the columns corresponding to two attributes. For any h1 and h2
such that 1 ≤ h1 < h2 ≤ K, we ﬁrst prove peh1 +eh2
(cid:1)(cid:62)

θ∗ = (cid:0) te1,1,··· , teh1−1,1, teh1 ,0
(cid:125)
(cid:125)
(cid:124)
, teh1+1,1,··· , teh2−1,1, teh2 ,eh1
(cid:124) (cid:123)(cid:122) (cid:125)

(cid:124)
(cid:125)
(cid:124)
teh2+1,1,··· , teK ,1

, 0,··· , 0
J−K

= ¯peh1 +eh2

(cid:123)(cid:122)

(cid:123)(cid:122)

(cid:123)(cid:122)

h2−h1

. Take

h1

.

,

K−h2

With such a choice of θ∗, for any k ∈ {1,··· , K}\{h1, h2}, tek,α − θ∗
k =
tek,α − tek,1 = 0 if α (cid:23) ek. In addition, teh2 ,eh1
r =(cid:80)K
= 0. Therefore, by
the deﬁnition, the row vector of T -matrix T (Q, Θ − θ∗1(cid:62)) corresponding to
k=1 ek has only two possible nonzero elements, which correspond to

− θ∗

h2

IDENTIFIABILITY OF RESTRICTED LATENT CLASS MODELS

27

the two columns eh2 and eh1 + eh2 in the T -matrix. Speciﬁcally, we have

(cid:124)
(cid:80)K

T(cid:80)K
(cid:18)
k=1 ek,·(Q, Θ − θ∗1(cid:62))
0(cid:62), (teh1 ,eh2

=

− teh1 ,0)(teh2 ,eh2

(cid:124)

(teh1 ,eh1 +eh2

− teh1 ,0)(teh2 ,eh1 +eh2

(tek,eh2

)

k=1,··· ,K,
k(cid:54)=h1,h2

(cid:89)

(cid:89)

(tek,eh1 +eh2

k=1,··· ,K,
k(cid:54)=h1,h2

, 0(cid:62),

− tek,1)
(cid:125)
, 0(cid:62)(cid:19)
− tek,1)
(cid:125)

.

column eh2

− teh2 ,eh1
(cid:123)(cid:122)
− teh2 ,eh1
(cid:123)(cid:122)

)

column eh1 +eh2

Consider the row vector of T -matrix T (Q, ¯Θ − θ∗1(cid:62)) corresponding to r =
k=1 ek. Thanks to the results in Steps 1–3, a similar calculation gives the
following equation for the chosen θ∗

T(cid:80)K
k=1 ek,·(Q, ¯Θ − θ∗1(cid:62)) = T(cid:80)K

k=1 ek,·(Q, Θ − θ∗1(cid:62)).

(cid:89)

k=1,··· ,K,
k(cid:54)=h1,h2

Under the model assumption, we have

teh1 ,eh1 +eh2

−teh1 ,0 > 0, teh2 ,eh1 +eh2

−teh2 ,eh1

> 0,

k=1,··· ,K,k(cid:54)=h1,h2

and(cid:81)
k=1 ek,·(Q, ¯Θ− θ∗1(cid:62)), equivalently T(cid:80)K
element of T(cid:80)K
T(cid:80)K
k=1 ek,·(Q, Θ − θ∗1(cid:62))p = T(cid:80)K

nonzero. From the equation

(tek,eh1 +eh2

k=1 ek,·(Q, ¯Θ − θ∗1(cid:62))¯p

−tek,1) (cid:54)= 0. Therefore the eh1 +eh2 column
k=1 ek,·(Q, Θ− θ∗1(cid:62)), is

(tek,eh2

−tek,1) (cid:54)= 0,

and the result that peh2

= ¯peh2

as proved in Step 3, we thus have

Next we show tej ,eh1 +eh2

. First consider the case when j >
K. For the row vector of T -matrix T (Q, Θ − θ∗1(cid:62)) corresponding to r =

= ¯peh1 +eh2

.

peh1 +eh2
= ¯tej ,eh1 +eh2

G. XU

− teh1 ,0)(teh2 ,eh2

− teh2 ,eh1
(cid:123)(cid:122)
− teh1 ,0)(teh2 ,eh1 +eh2
(cid:123)(cid:122)

column eh2

column eh1 +eh2

− teh2 ,eh1

)

)

(cid:89)
(cid:89)

k=1,··· ,K,
k(cid:54)=h1,h2

(tek,eh2

(tek,eh1 +eh2

k=1,··· ,K,
k(cid:54)=h1,h2

, 0(cid:62),

− tek,1)
(cid:125)
− tek,1)
,
(cid:125)

28

k=1 ek + ej, we have

(cid:80)K
T(cid:80)K
(cid:18)
k=1 ek+ej ,·(Q, Θ − θ∗1(cid:62))
0(cid:62), tej ,eh2

(teh1 ,eh2

(4.14)

=

(cid:124)

.

tej ,eh1 +eh2

(teh1 ,eh1 +eh2

(cid:124)
0(cid:62)(cid:19)
r =(cid:80)K
T(cid:80)K
(cid:18)
k=1 ek+ej ,·(Q, ¯Θ − θ∗1(cid:62))
0(cid:62), tej ,eh2
(cid:124)

(teh1 ,eh2

(4.15)

=

k=1 ek + ej, we can write

¯tej ,eh1 +eh2

(teh1 ,eh1 +eh2

(cid:124)
0(cid:62)(cid:19)

,

Similarly, for the row vector of T -matrix T (Q, ¯Θ − θ∗1(cid:62)) corresponding to

(cid:89)

− teh1 ,0)(teh2 ,eh2

− teh2 ,eh1
(cid:123)(cid:122)
− teh1 ,0)(teh2 ,eh1 +eh2
(cid:123)(cid:122)

column eh2

column eh1 +eh2

(tek,eh2

)

k=1,··· ,K,
k(cid:54)=h1,h2

(cid:89)

− teh2 ,eh1

)

(tek,eh2

k=1,··· ,K,
k(cid:54)=h1,h2

, 0(cid:62),

− tek,1)
(cid:125)
− tek,1)
(cid:125)

,

where the result ¯tej ,eh2
From (4.14), (4.15), and the proved results that peh2
¯peh1 +eh2

, we can derive

= tej ,eh2

is used for the element in column eh2.
=

and peh1 +eh2

= ¯peh2

tej ,eh1 +eh2

= ¯tej ,eh1 +eh2

,

for any 1 ≤ h1 < h2 ≤ K and j > K, from the equation T(cid:80)K
θ∗1(cid:62))p = T(cid:80)K

k=1 ek+ej ,·(Q, ¯Θ − θ∗1(cid:62))¯p.

k=1 ek+ej ,·(Q, Θ−

IDENTIFIABILITY OF RESTRICTED LATENT CLASS MODELS

29

Moreover, for any 1 ≤ J ≤ K and 1 ≤ h1 < h2 ≤ K, we redeﬁne

θ∗ = (cid:0) 0,··· , 0
(cid:124) (cid:123)(cid:122) (cid:125)
(cid:123)(cid:122)
(cid:124)
(cid:124)
(cid:125)
(cid:125)
teK+h1+1,1,··· , teK+h2−1,1, teK+h2 ,eh1
, teK+h2+1,1,··· , te2K ,1
Consider T(cid:80)2K

(cid:124) (cid:123)(cid:122) (cid:125)
(cid:123)(cid:122)
K−h2
k=K+1 ek,·(Q, Θ − θ∗1(cid:62)) instead of T(cid:80)K
k=1 ek,·(Q, Θ − θ∗1(cid:62)). A

(cid:124)
(cid:125)
, teK+1,1,··· , teK+h1−1,1, teK+h1 ,0
(cid:123)(cid:122)

, 0,··· , 0
J−2K

(cid:1)(cid:62)

h2−h1

h1

K

,

.

similar argument as above gives

tej ,eh1 +eh2

= ¯tej ,eh1 +eh2

for any 1 ≤ h1 < h2 ≤ K and j = 1,··· , K. This completes Step 4.
Step 5. We consider the columns corresponding to more than two at-
tributes. We use the induction method and a similar argument as in Step 4.
In particular, consider any integer k such that 3 ≤ k ≤ K. For any l ≤ k− 1,
suppose we have

tej ,(cid:80)l

= ¯tej ,(cid:80)l

and p(cid:80)l

= ¯p(cid:80)l

i=1 ehi

i=1 ehi

i=1 ehi

i=1 ehi

for any j ∈ {1,··· , J} and 1 ≤ h1,··· , hl ≤ K. We next show that the two
equations also hold for l = k.
J )(cid:62)

Consider any 1 ≤ h1,··· , hk ≤ K. Deﬁne the vector θ∗ = (θ∗

1,··· , θ∗

as

tei,0

tei,1
0

θ∗
i =

for i ∈ {h1,··· , hk};
for i ∈ {1,··· , K} \ {h1,··· , hk};
otherwise.

Then under the induction assumption, we have the equivalence of the two
row vectors:

T(cid:80)K
i=1 ei,·(Q, Θ − θ∗1(cid:62)) = T(cid:80)K

i=1 ei,·(Q, ¯Θ − θ∗1(cid:62)).

i=1 ehi

In particular, the element of T(cid:80)K
umn (cid:80)k
column(cid:80)l
p(cid:80)l
T(cid:80)k
i=1 ehi ,·(Q, ¯Θ)¯p gives

= ¯p(cid:80)l

i=1 ehi

i=1 ei,·(Q, Θ − θ∗1(cid:62)) corresponding to col-

is nonzero; for any l < k, the elements corresponding to
i=1 ehi may be zero or nonzero; and the others terms are 0. Since

for any l < k, the equation T(cid:80)k

i=1 ehi ,·(Q, Θ)p =

i=1 ehi

p(cid:80)k

i=1 ehi

= ¯p(cid:80)k

i=1 ehi

.

tei,0

tei,1
0

θ∗
i =

for i ∈ {K + h1,··· , K + hk};
for i ∈ {K + 1,··· , 2K} \ {K + h1,··· , K + hk};
otherwise.

Similarly we can obtain tej ,(cid:80)k

= ¯tej ,(cid:80)k

i=1 ehi

i=1 ehi

. This completes the proof.

4.2. Proofs of Propositions 2–3 and Lemmas 1–2.

Proof of the Proposition 2. We only need to show that there exist
(Θ, p) (cid:54)= ( ¯Θ, ¯p) satisfying equation (3.5). For notational convenience, we
write tej ,α(Q, Θ) and tej ,α(Q, ¯Θ) as tej ,α and ¯tej ,α, respectively.

For simplicity, consider the DINA model in Example 2, under which
tej ,α = tej ,0 if ξj,α = 0 and tej ,α = tej ,1 if ξj,α = 1. Without loss of
generality, we focus on the Q-matrix has the following form:



0(cid:62)
1
0(cid:62)
1
0 IK−1
0 IK−1
0 Q∗

 ,

Q =

where Q∗ is unspeciﬁed. Note that the above Q-matrix does not satisfy
condition C2 under the DINA model. Next we show the item parameters for
the ﬁrst two items are non-identiﬁable.
Let tej ,1 = ¯tej ,1 for j ≥ 3. Consider the row vector of the T -matrix corre-
sponding to r = (r1, r2,··· , rJ )(cid:62). Consider each possible value of (r1, r2) ∈
{0, 1}2. We can show that for any (Θ, p) (cid:54)= ( ¯Θ, ¯p), equation (3.5) is satisﬁed
if the following equations hold for any α ∈ {0, 1}K such that α1 = 0:

30

G. XU

Following a similar argument as in Step 4, we can establish

Moreover, for any j > K, we have Tej +(cid:80)K

i=1 ei,·(Q, Θ)p = Tej +(cid:80)K
= ¯tej ,(cid:80)k
For 1 ≤ j ≤ K and 1 ≤ h1,··· , hk ≤ K, take

tej ,(cid:80)k

.

i=1 ehi

i=1 ehi

i=1 ei,·(Q, ¯Θ)¯p.



(4.16)

pα + pα+e1 = ¯pα + ¯pα+e1,
te1,1pα+e1 + te1,0pα = ¯te1,1 ¯pα+e1 + ¯te1,0 ¯pα,
te2,1pα+e1 + te2,0pα = ¯te2,1 ¯pα+e1 + ¯te2,0 ¯pα,
te1,1te2,1pα+e1 + te1,0te2,0pα

if (r1, r2) = (0, 0);
if (r1, r2) = (1, 0);
if (r1, r2) = (0, 1);

= ¯te1,1¯te2,1 ¯pα+e1 + ¯te1,0¯te2,0 ¯pα,

if (r1, r2) = (1, 1).



¯te1,0 +

¯te2,0 +
tej ,1,

¯tej ,1 =

IDENTIFIABILITY OF RESTRICTED LATENT CLASS MODELS

31
Now we construct (Θ, p) (cid:54)= ( ¯Θ, ¯p) such that (4.16) is satisﬁed. For ρ ∈
(0, 1), choose (Θ, p) such that pα/pα+e1 = ρ for over all α ∈ {0, 1}K with
α1 = 0. Then, for any ¯tej ,0, j = 1,··· , J, deﬁne

(te1,1−¯te1,0)(te2,1−¯te2,0)+ρ(te1,0−¯te1,0)(te2,0−¯te2,0)

(te2,1−¯te2,0)+ρ(te2,0−¯te2,0)

(te1,1−¯te1,0)(te2,1−¯te2,0)+ρ(te1,0−¯te1,0)(te2,0−¯te2,0)

(te1,1−¯te1,0)+ρ(te1,0−¯te1,0)

, if j = 1;

, if j = 2;

¯pα+e1 =

{(te1,1−¯te1,0)+ρ(te1,0−¯te1,0)}{(te2,1−¯te2,0)+ρ(te2,0−¯te2,0)}

(te1,1−¯te1,0)(te2,1−¯te2,0)+ρ(te1,0−¯te1,0)(te2,0−¯te2,0)

¯pα = pα + pα+e1 − ¯pα+e1,

if j = 3, . . . , J;

× pα+e1,

for every α ∈ {0, 1}K such that α1 = 0. This results in a solution to (4.16).
Thus, we have constructed (Θ, p) (cid:54)= ( ¯Θ, ¯p) such that (3.5) holds. This com-
pletes the proof.

Proof of the Proposition 3. In what follows, we construct a D ma-
trix satisfying the conditions in the proposition, i.e., D(θ∗) is a matrix only
depending on θ∗ such that D(θ∗)T (Q, Θ) = T (Q, Θ − θ∗1(cid:62)) for any Q and
Θ. Recall that

tr,α(Q, Θ) =

tej ,α(Q, Θ), ∀ r ∈ {0, 1}J , α ∈ {0, 1}K.

(cid:89)

j:rj =1

For any θ∗ = (θ∗

1,··· , θ∗

J ) ∈ RJ ,
tr,α(Q, Θ − θ∗1(cid:62)) =

(cid:89)

j:rj =1

(cid:80)J
j=1 rj−r(cid:48)
(−1)

(cid:88)

r(cid:48)(cid:22)r

{tej ,α(Q, Θ) − θ∗
j}.
(cid:89)

j (cid:89)

θ∗

j

j:rj−r(cid:48)

j =1

k:r(cid:48)

k=1

tek,α(Q, Θ).

j(cid:81)

(cid:80)J
0
j=1 rj−r(cid:48)
(−1)
1

j:rj−r(cid:48)

j =1 θ∗

j

r(cid:48) (cid:54)(cid:22) r
r(cid:48) (cid:22) r and r(cid:48) (cid:54)= r
r(cid:48) = r

.

By polynomial expansion,
tr,α(Q, Θ − θ∗1(cid:62)) =



dr,r(cid:48)(θ∗) =

Then we have

Deﬁne the entrie dr,r(cid:48)(θ∗) of D(θ∗) corresponding to row r and column r(cid:48) as

T (Q, Θ − θ∗1(cid:62)) = D(θ∗)T (Q, Θ),

where D(θ∗) is a lower triangular matrix depending solely on θ∗ with eigen-
values equal to its diagonal. Since diag{D(θ∗)} = 1, D(θ∗) is invertible.

32

G. XU

(cid:88)

Proof of Lemma 1. We use the method of contradiction. If there exists
k ∈ {1,··· , K} such that tek,0 = ¯tek,α∗ with α∗ (cid:23) ek. Since tek,0 ≤ tek,α
for any α ∈ {0, 1}K and tek,0 < tek,α∗ = tek,1, this implies that for the row
vectors corresponding to r = ek,
Tek,·(Q, Θ)p >
¯tek,1 > Tek,·(Q, ¯Θ)¯p,
which contradicts the equation (3.5) that requires Tek,·(Q, Θ)p = Tek,·(Q, ¯Θ)¯p.
Therefore we conclude that tek,0 (cid:54)= ¯tek,α∗. Similarly, we have tek,α∗
(cid:54)=
¯tek,0, teK+k,0 (cid:54)= ¯teK+k,α∗ and teK+k,α∗ (cid:54)= ¯teK+k,0.

(cid:88)

(cid:88)

pαtek,0 =

¯tek,α∗ =

¯pα

¯pα

α

α

α

,

K

(cid:124)

Take

T(cid:80)K

and we have

, 0,··· , 0
J−K

Proof of Lemma 2. Without loss of generality, we only need to show

θ∗ =(cid:0) te1,0, te2,1,··· , teK ,1
(cid:125)

that for any 1 ≤ h ≤ K, te1,eh (cid:54)= ¯te1,1.
(cid:1)(cid:62)
(cid:124) (cid:123)(cid:122) (cid:125)
(cid:123)(cid:122)
(tek,e1 − tek,1), 0(cid:62)(cid:19)
0, (te1,e1 − te1,0) × K(cid:89)
k=1 ek,·(Q, ¯Θ − θ∗1(cid:62)). Under the equation

(cid:18)
k=1 ek,·(Q, Θ − θ∗1(cid:62)) =
Consider the row vector T(cid:80)K
(3.5), there must exist a nonzero element. We denote the corresponding
column as α∗ and the element then can be written as
(¯te1,α∗ − te1,0) × K(cid:89)

From the model assumption, the product element is nonzero.

(¯tek,α∗ − tek,1) (cid:54)= 0.

k=2

.

k=2

Note that here we do not know whether α∗ equals e1.
Denote Q1 as the Q-matrix corresponding to items from K + 1 to 2K.
Note that Q1 = IK. Consider the 2K × 2K T -matrix, T (Q1, ¯Θ(K+1):2K),
where ¯Θ(K+1):2K denotes the submatrix of Θ containing rows from K + 1 to
2K. Take ˜θ = (¯θK+1,1,··· , ¯θ2K,1)(cid:62), and we know the transformed T -matrix
T (Q1, ¯Θ(K+1):2K − ˜θ1(cid:62)) takes an upper-left triangular form (up to column
swapping) and therefore is full rank. This implies T (Q1, ¯Θ(K+1):2K) is full
rank and thus there exists a row vector m such that

m · T (Q1, ¯Θ(K+1):2K) = (0,··· , 0,

, 0,··· , 0).

1(cid:124)(cid:123)(cid:122)(cid:125)

column α∗

IDENTIFIABILITY OF RESTRICTED LATENT CLASS MODELS

33

On the other hand, consider m· T (Q1, ΘK+1:2K). We use x to denote the el-
ement corresponding to the column e1 (i.e., the second element). Combining
the above results, we know

(cid:33)

k=1 ek,·(Q, Θ − θ∗1(cid:62))
(tek,e1 − tek,1), 0
k=1 ek,·(Q, ¯Θ − θ∗1(cid:62))

{m · T (Q1, Θ)} (cid:12) T(cid:80)K
(cid:32)
0, x × (te1,e1 − te1,0) × K(cid:89)
{m · T (Q1, ¯Θ)} (cid:12) T(cid:80)K
(cid:18)
0,··· , 0, (¯te1,α∗ − te1,0) × K(cid:89)
(cid:123)(cid:122)

(¯tek,α∗ − tek,1)
(cid:125)

(cid:124)

k=2

k=2

;

column α∗

and

=

=

(cid:19)

.

, 0,··· , 0

Under the equation (3.5), we know x (cid:54)= 0 and the above two vectors are
both nonzero. Now consider j > 2K, and we have

(cid:33)

(tek,e1 − tek,1), 0

;

=

and

k=1 ek,·(Q, Θ − θ∗1(cid:62))

{m · T (Q1, Θ)} (cid:12) Tej +(cid:80)K
(cid:32)
0, x × tej ,e1 × (te1,e1 − te1,0) × K(cid:89)
{m · T (Q1, ¯Θ)} (cid:12) Tej +(cid:80)K
(cid:18)
0,··· , 0, ¯tej ,α∗ × (¯te1,α∗ − te1,0) × K(cid:89)

k=2

k=1 ek,·(Q, ¯Θ − θ∗1(cid:62))

=

k=2

(cid:124)

(cid:124)

(cid:123)(cid:122)

column α∗

Therefore as in Step 1, we have for j > 2K, tej ,e1 = ¯tej ,α∗.

(cid:123)(cid:122)
(cid:124) (cid:123)(cid:122) (cid:125)
(cid:125)
Now redeﬁne θ∗ = ( 0, te2,1,··· , teK ,1
T(cid:80)K
k=2 ek,·(Q, Θ − θ∗1(cid:62))
(tek,e1 − tek,1), 0(cid:62)(cid:19)
(cid:18) K(cid:89)
K(cid:89)
(tek,0 − tek,1),
T(cid:80)K
k=1 ek,·(Q, Θ − θ∗1(cid:62))
(cid:18)
K(cid:89)

, 0,··· , 0
J−K

K(cid:89)

k=2

k=2

=

K

,

(tek,0 − tek,1), te1,e1

=

te1,0

(tek,e1 − tek,1), 0(cid:62)(cid:19)

.

(cid:19)

.

, 0,··· , 0

(¯tek,α∗ − tek,1)
(cid:125)

)(cid:62), and we have

k=2

k=2

From the model assumption, the product elements are nonzero. Following
the notation in Step 3, there exists a (J − 2K + 1)-dimensional vector u1

34

G. XU

such that
b1 = u1(1, te2K+1,e1,··· , teJ ,e1)(cid:62) (cid:54)= 0 and u1(1, te2K+1,0,··· , teJ ,0)(cid:62) = 0.
Since for j > 2K, tej ,e1 = ¯tej ,α∗, from a similar argument in Step 3, we have

k=2

;

k=2 ek,·(Q, Θ − θ∗1(cid:62))

k=1 ek,·(Q, Θ − θ∗1(cid:62))

(tek,e1 − tek,1), 0(cid:62)(cid:19)

(u1A) (cid:12) {m · T (Q1, Θ)} (cid:12) T(cid:80)K
(tek,e1 − tek,1), 0(cid:62)(cid:19)
(cid:18)
0, b1 × x × K(cid:89)
(u1A) (cid:12) {m · T (Q1, Θ)} (cid:12) T(cid:80)K
(cid:18)
0, b1 × x × te1,e1 × K(cid:89)
(u1 ¯A) (cid:12) {m · T (Q1, ¯Θ)} (cid:12) T(cid:80)K
k=2 ek,·(Q, ¯Θ − θ∗1(cid:62))
(cid:18)
0,··· , 0, b1 × K(cid:89)
(¯tek,α∗ − tek,1)
(cid:123)(cid:122)
(cid:125)
(u1 ¯A) (cid:12) {m · T (Q1, ¯Θ)} (cid:12) T(cid:80)K
k=1 ek,·(Q, ¯Θ − θ∗1(cid:62))
(cid:18)
0,··· , 0, b1 × ¯te1,α∗ × K(cid:89)
, 0,··· , 0
(¯tek,α∗ − tek,1)
(cid:125)
(cid:123)(cid:122)

, 0,··· , 0

;

(cid:124)

(cid:124)

;

(cid:19)

k=2

k=2

column α∗

=

=

=

=

and

(cid:19)

.

k=2

column α∗

The above equations imply that te1,e1 = ¯te1,α∗. Since under the model as-
sumption te1,e1 > te1,eh, we have the conclusion that te1,eh (cid:54)= ¯te1,1 since
otherwise, we have ¯te1,α∗ > ¯te1,1 which cannot be true under the model
assumption. This completes the proof.

Acknowledgment. The author thanks the editor, the associate editor,

and three reviewers for many helpful and constructive comments.

References.

Allman, E. S., Matias, C., and Rhodes, J. A. (2009), “Identiﬁability of parameters in latent
structure models with many observed variables,” The Annals of Statistics, 3099–3132.
Chen, Y., Liu, J., Xu, G., and Ying, Z. (2015), “Statistical analysis of Q-matrix based
diagnostic classiﬁcation models,” Journal of the American Statistical Association, 110,
850–866.

Chiu, C.-Y., Douglas, J. A., and Li, X. (2009), “Cluster analysis for cognitive diagnosis:

theory and applications,” Psychometrika, 74, 633–665.

IDENTIFIABILITY OF RESTRICTED LATENT CLASS MODELS

35

de la Torre, J. (2011), “The generalized DINA model framework,” Psychometrika, 76,

179–199.

de la Torre, J. and Douglas, J. A. (2004), “Higher order latent trait models for cognitive

diagnosis,” Psychometrika, 69, 333–353.

DeCarlo, L. T. (2011), “On the analysis of fraction subtraction data: the DINA model,
classiﬁcation, class sizes, and the Q-matrix,” Applied Psychological Measurement, 35,
8–26.

DiBello, L. V., Stout, W. F., and Roussos, L. A. (1995), “Uniﬁed cognitive psychometric
diagnostic assessment likelihood-based classiﬁcation techniques,” in Cognitively diag-
nostic assessment, eds. Nichols, P. D., Chipman, S. F., and Brennan, R. L., Hillsdale,
NJ: Erlbaum Associates, pp. 361–390.

Elmore, R., Hall, P., and Neeman, A. (2005), “An application of classical invariant theory
to identiﬁability in nonparametric mixtures,” in Annales de l’institut Fourier, vol. 55,
pp. 1–28.

Gabrielsen, A. (1978), “Consistency and identiﬁability,” Journal of Econometrics, 8, 261–

263.

Goodman, L. A. (1974), “Exploratory latent structure analysis using both identiﬁable and

unidentiﬁable models,” Biometrika, 61, 215–231.

Gyllenberg, M., Koski, T., Reilink, E., and Verlaan, M. (1994), “Non-uniqueness in prob-
abilistic numerical identiﬁcation of bacteria,” Journal of Applied Probability, 542–548.

Hagenaars, J. A. (1993), Loglinear Models with Latent Variables, vol. 94, Sage.
Hartz, S. M. (2002), “A Bayesian framework for the uniﬁed model for assessing cognitive
abilities: Blending theory with practicality,” Ph.D. thesis, University of Illinois, Urbana-
Champaign.

Henson, R. A., Templin, J. L., and Willse, J. T. (2009), “Deﬁning a family of cognitive
diagnosis models using log-linear models with latent variables,” Psychometrika, 74,
191–210.

Junker, B. W. and Sijtsma, K. (2001), “Cognitive assessment models with few assump-
tions, and connections with nonparametric item response theory,” Applied Psychological
Measurement, 25, 258–272.

Koopmans, T. C. (ed.) (1950), Statistical Inference in Dynamic Economic Models, vol. 10,

New York: John Wiley & Sons, Inc.

Koopmans, T. C. and Reiersøl, O. (1950), “The identiﬁcation of structural characteristics,”

Ann. Math. Statist., 21, 165–181.

Kruskal, J. B. (1976), “More factors than subjects, tests and treatments: an indeterminacy
theorem for canonical decomposition and individual diﬀerences scaling,” Psychometrika,
41, 281–293.

— (1977), “Three-way arrays: rank and uniqueness of trilinear decompositions, with ap-
plication to arithmetic complexity and statistics,” Linear algebra and its applications,
18, 95–138.

Leighton, J. P., Gierl, M. J., and Hunka, S. M. (2004), “The attribute hierarchy model
for cognitive assessment: A variation on Tatsuoka’s rule-space approach,” Journal of
Educational Measurement, 41, 205–237.

Maris, E. (1999), “Estimating multiple classiﬁcation latent class models,” Psychometrika,

64, 187–212.

Maris, G. and Bechger, T. M. (2009), “Equivalent diagnostic classiﬁcation models,” Mea-

surement, 7, 41–46.

McHugh, R. B. (1956), “Eﬃcient estimation and local identiﬁcation in latent class analy-

sis,” Psychometrika, 21, 331–347.

Rothenberg, T. J. (1971), “Identiﬁcation in parametric models,” Econometrica: Journal

36

G. XU

of the Econometric Society, 577–591.

Rupp, A. A., Templin, J. L., and Henson, R. A. (2010), Diagnostic Measurement: Theory,

Methods, and Applications, New York: Guilford Press.

Tatsuoka, C. (2009), “Diagnostic models as partially ordered sets,” Measurement, 7, 49–53.
Tatsuoka, K. K. (1983), “Rule space: an approach for dealing with misconceptions based

on item response theory,” Journal of Educational Measurement, 20, 345–354.

— (2009), Cognitive Assessment: An Introduction to the Rule Space Method, New York:

Routledge.

Teicher, H. (1967), “Identiﬁability of mixtures of product measures,” The Annals of Math-

ematical Statistics, 1300–1302.

Templin, J. L. and Henson, R. A. (2006), “Measurement of psychological disorders using

cognitive diagnosis models,” Psychological Methods, 11, 287–305.

von Davier, M. (2008), “A general diagnostic model applied to language testing data,”

British Journal of Mathematical and Statistical Psychology, 61, 287–307.

Xu, G. (2013), “Statistical inference for diagnostic classiﬁcation models,” Ph.D. thesis,

Columbia University.

Xu, G. and Zhang, S. (2015), “Identiﬁability of diagnostic classiﬁcation models,” Psy-

chometrika, to appear.

Gongjun Xu
School of Statistics,
University of Minnesota
224 Church Street SE
Minneapolis, MN, 55455
E-mail: xuxxx360@umn.edu

