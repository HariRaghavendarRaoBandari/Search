6
1
0
2

 
r
a

M
 
7
1

 
 
]

.

A
N
h
t
a
m

[
 
 

1
v
1
2
6
5
0

.

3
0
6
1
:
v
i
X
r
a

OPERATOR NORM INEQUALITIES BETWEEN TENSOR UNFOLDINGS ON

THE PARTITION LATTICE

MIAOYAN WANG1, KHANH DAO DUC1, JONATHAN FISCHER2, AND YUN S. SONG1,2,3

1Department of Mathematics, University of Pennsylvania

2Department of Statistics, University of California, Berkeley

3Computer Science Division, University of California, Berkeley

Abstract. Interest in higher-order tensors has recently surged in data-intensive ﬁelds, with a wide
range of applications including image processing, blind source separation, community detection,
and feature extraction. A common paradigm in tensor-related algorithms advocates unfolding
(or ﬂattening) the tensor into a matrix and applying classical methods developed for matrices.
Despite the popularity of such techniques, how the functional properties of a tensor changes upon
unfolding is currently not well understood.
In contrast to the body of existing work which has
focused almost exclusively on matricizations, we here consider all possible unfoldings of an order-k
tensor, which are in one-to-one correspondence with the set of partitions of {1, . . . , k}. We derive
general inequalities between the lp-norms of arbitrary unfoldings deﬁned on the partition lattice.
In particular, we demonstrate how the spectral norm (p = 2) of a tensor is bounded by that of its
unfoldings, and obtain an improved upper bound on the ratio of the Frobenius norm to the spectral
norm of an arbitrary tensor. For specially-structured tensors satisfying a generalized deﬁnition
of orthogonal decomposability, we prove that the spectral norm remains invariant under speciﬁc
subsets of unfolding operations.

1. Introduction

Tensors of order 3 or greater, known as higher-order tensors, have recently attracted increased
attention in many ﬁelds across science and engineering. Methods built on tensors provide powerful
tools to capture complex structures in data that lower-order methods may fail to exploit. Among
numerous examples, tensors have been used to detect patterns in time-course data [7, 19, 24, 30]
and to model higher-order cumulants [1, 2, 16]. However, tensor-based methods are fraught with
challenges. Tensors are not simply matrices with more indices; rather, they are mathematical
objects possessing multilinear algebraic properties.
Indeed, extending familiar matrix concepts
such as norms to tensors is non-trivial [14, 20], and computing these quantities has proven to be
NP-hard [4–6].

The spectral relations between a general tensor and its lower-order counterparts have yet to be
studied. There are generally two types of approaches underlying many, if not all, existing tensor-
based methods. The ﬁrst approach ﬂattens the tensor into a matrix and applies matrix-based
techniques in downstream analyses, notably HOSVD [3, 12] and TensorFace [27]. Flattening is
computationally convenient because of the ubiquity of well-established matrix-based methods, as
well as the connection between tensor contraction and block matrix multiplication [11,21]. However,
matricization leads to a potential loss of the structure found in the original tensor. This motivates
the key question of how much information a ﬂattening retains from its parent tensor.

The second approach either handles the tensor directly or unfolds it into objects of order-3 or
higher. Recent work on bounding the spectral norm of sub-Gaussian tensors reveals that solving for

2010 Mathematics Subject Classiﬁcation. 15A60; 15A69; 06B99; 05A18.
Key words and phrases. higher-order tensors; general unfoldings, partition lattice, operator norm, orthogonality.

1

2

the convex relaxation of tensor rank by unfolding is suboptimal [26]. Interestingly, in the context
of tensor completion, unfolding a higher-order tensor into a nearly cubic tensor requires smaller
sample sizes than matricization [29]. These results are probabilistic in nature and merely focus on
a particular class of tensors. Assessing the general impact of unfolding operations on an arbitrary
tensor and the role of the tensor’s intrinsic structure remains challenging.

The primary goal of this paper is to study the eﬀect of unfolding operations on functional
properties of tensors, where an unfolding is any lower-order representation of a tensor. Given an
order-k tensor, we represent each possible unfolding operation using a partition π of [k] = {1, . . . , k},
where a block in π corresponds to the set of modes that should be combined into a single mode.
Each unfolding is a rearrangement of the elements of the original tensor into a tensor of lower order.
Here we study the lp operator norms of all possible tensor unfoldings, which together deﬁne what
we coin a “norm landscape” on the partition lattice. A partial order relation between partitions
enables us to ﬁnd a path between an arbitrary pair of unfoldings and establish our main inequalities
relating their operator norms. For specially-structured tensors satisfying a generalized deﬁnition
of orthogonal decomposability, we show that the spectral norm (p = 2) remains invariant under
unfolding operations corresponding to a speciﬁc subset of partitions. To our knowledge, our results
represent the ﬁrst attempt to provide a full picture of the norm landscape over all possible tensor
unfoldings.

The remainder of this paper is organized as follows. In Section 2, we introduce some notation,
and relate the spectral norm and the general lp-norm of a tensor. We then describe in Section 3
general tensor unfoldings deﬁned on the partition lattice. In Section 4, we present our main results
on the inequalities between the operator norms of any two tensor unfoldings and describe how
the norm landscape changes over the partition lattice. In Section 5, we generalize the notion of
orthogonal decomposable tensors and prove that the spectral norm is invariant within a speciﬁc set
of tensor unfoldings. We conclude in Section 6 with discussions about our ﬁndings and avenues of
future work.

2. Higher-order tensors and their operator norms

An order-k tensor A = (cid:74)ai1... ik(cid:75) ∈ Fd1×···×dk over a ﬁeld F is a hypermatrix with dimensions
tensors, F = R. The total dimension of A is denoted by dim(A) =(cid:81)k
(d1, . . . , dk) and entries ai1...ik ∈ F, for 1 ≤ in ≤ dn, n = 1, . . . , k. In this paper, we focus on real
The vectorization of A, denoted Vec(A), is deﬁned as the operation rearranging all elements
of A into a column vector. For ease of notation, we use the shorthand [n] to denote the n-set
n=1xn = x1 ⊗···⊗ xk when space is limited. We use
{1, . . . , n} for n ∈ N+, and sometimes write ⊗k
Sd−1 = {x ∈ Rd : (cid:107)x(cid:107)2 = 1} to denote the d-dimensional unit sphere, and Id to denote the d × d
For any two tensors A =(cid:74)ai1... ik(cid:75), B =(cid:74)bi1... ik(cid:75) ∈ Rd1×···×dk of identical order and dimensions,
identity matrix.

their inner product is deﬁned as

n=1 dn.

(cid:88)

(cid:104)A, B(cid:105) =

while the tensor Frobenius norm of A is deﬁned as

(cid:107)A(cid:107)F =(cid:112)(cid:104)A, A(cid:105) =

i1,...,ik

ai1...ik bi1...ik ,

(cid:115) (cid:88)

i1,...,ik

|ai1...ik|2,

both of which are analogues of standard deﬁnitions for vectors and matrices.

3

Following [14], we deﬁne the covariant multilinear matrix multiplication of a tensor A ∈ Rd1×···×dk

by matrices M1 = (m(1)
i1j1

) ∈ Rd1×s1, . . . , Mk = (m(k)

A(M1, . . . , Mk) =(cid:114) d1(cid:88)

··· dk(cid:88)

ikjk

i1=1

ik=1

) ∈ Rdk×sk as

ai1...ik m(1)
i1j1

··· m(k)

ikjk

(cid:122),

which results in an order-k tensor in Rs1×···×sk . This operation multiplies the nth mode of A by
the matrix Mn for all n ∈ [k]. Just as a matrix may be multiplied in up to two modes by matrices
of consistent dimensions, an order-k tensor can be multiplied by up to k matrices in k modes. In
the case of k = 2, A is a matrix and A(M1, M2) = M T
1 AM2. Sometimes we are interested in
multiplying by vectors rather than matrices, in which case we obtain the k-multilinear functional
A : Rd1 × ··· × Rdk → R given by

A(x1, . . . , xk) =

d1(cid:88)

··· dk(cid:88)

ai1...ik x(1)
i1
= (cid:104)A, x1 ⊗ ··· ⊗ xk(cid:105),

ik=1

i1=1

··· x(k)

ik

(1)

1 , . . . , x(n)
dn

)T ∈ Rdn, n ∈ [k]. Note that multiplying by a vector in r modes in
where xn = (x(n)
the manner deﬁned in (1) reduces the order of the output tensor to k − r, whereas multiplying
by matrices leaves the order unchanged. Although the coordinate representation of a tensor as a
hypermatrix provides a concrete description, viewing it instead as a multilinear functional provides
a coordinate-free, basis-independent perspective which allows us to better characterize the spectral
relations among diﬀerent tensor unfoldings.
We deﬁne the operator norm, or induced norm, of a tensor A using the associated k-multilinear

functional (1).
Deﬁnition 2.1 (Lim [14]). Let A ∈ Rd1×···×dk be an order-k tensor. For any 1 ≤ p ≤ ∞, the
lp-norm of the multilinear functional associated with A is deﬁned as

(cid:107)A(cid:107)p = sup

(cid:40) A(x1, . . . , xk)
(cid:41)
(cid:111)
(cid:110)A(x1, . . . , xk) : (cid:107)xn(cid:107)p = 1, xn ∈ Rdn, n ∈ [k]

: xn (cid:54)= 0, xn ∈ Rdn, n ∈ [k]

(cid:107)x1(cid:107)p ···(cid:107)xk(cid:107)p

,

= sup

(2)
where (cid:107)xn(cid:107)p denotes the vector lp-norm of xn.
Because we restrict all entries of A and {xn}k

n=1 to be real, we need not take the absolute value
of A(x1, . . . , xk) as in [14]. The special case of p = 2 is called the spectral norm, frequently denoted
(cid:107)A(cid:107)σ. It is worth mentioning that the notion of tensor lp-norms deﬁned by (2) are not extensions
of the classical matrix lp-norms when p (cid:54)= 2. To see this, recall that for m× n matrices one usually
deﬁnes the lp operator norm for real vectors x as

(3)

(cid:107)A(cid:107)p = sup

: x (cid:54)= 0, x ∈ Rn

.

In general (2) and (3) are not equal even for matrices, illustrated by the following example:

Example 2.2. Let A =(cid:74)aij(cid:75) be the 2 × 2 matrix A =

have

(cid:26)(cid:107)Ax(cid:107)1

(cid:107)x(cid:107)1

(cid:107)A(cid:107)1 = sup

: x (cid:54)= 0, x ∈ R2

|aij| = 5.

= sup

j

i

and consider p = 1. Solving (3), we

(cid:40)(cid:107)Ax(cid:107)p

(cid:107)x(cid:107)p

(cid:41)

(cid:88)

(cid:21)

(cid:20)1 1
(cid:27)

0 4

4

However, instead using (2) gives
(cid:107)A(cid:107)1 = sup

(cid:26) xT
= sup(cid:8)xT
1 Ax2 : xn ∈ R2,(cid:107)xn(cid:107)1 = 1, n ∈ [2](cid:9)

: xn ∈ R2, xn (cid:54)= 0, n ∈ [2]

(cid:107)x1(cid:107)1 (cid:107)x2(cid:107)1

1 Ax2

(cid:27)

(equal to(cid:80)

i,j |aij| = 6).

= 4,

which is neither the classical matrix l1-norm (equal to 5), nor the entry-wise (Schatten) l1-norm

Throughout this paper, we adopt Deﬁnition 2.1 and always use (cid:107) · (cid:107)p to denote the lp-norm
deﬁned therein, even for matrices. In fact, (3) deﬁnes an operator norm by viewing the matrix
as a linear operator from Rd2 to Rd1, whereas (2) deﬁnes an operator norm in which the matrix
deﬁnes a bilinear functional from Rd1 × Rd2 to R. These two deﬁnitions are equivalent when p = 2,
but otherwise represent two diﬀerent operators and result in two distinct operator norms. To be
consistent with our treatment of tensors as k-multilinear functionals, we formulate matrices as
bilinear functionals.

ison bound is polynomial in the total dimension of the tensor, dim(A) =(cid:81)k

For a given tensor, its lp-norm and spectral norm mutually control each other, and the compar-

n=1 dn.

Proposition 2.3 (lp-norm vs. spectral norm). Let (cid:107)·(cid:107)p denote the lp-norm deﬁned in (2) and (cid:107)·(cid:107)σ
denote the spectral norm (p = 2). Then, the following results hold for an arbitrary order-k tensor
A ∈ Rd1×···×dk :

(a) For all 1 ≤ p ≤ 2,

(b) For all 2 ≤ p ≤ ∞,

Proof. Starting from Deﬁnition 2.1, for any 1 ≤ p ≤ ∞ we have

dim(A)

1

2− 1

p (cid:107)A(cid:107)σ ≤ (cid:107)A(cid:107)p ≤ (cid:107)A(cid:107)σ .

(cid:107)A(cid:107)σ ≤ (cid:107)A(cid:107)p ≤ dim(A)

1

2− 1

p (cid:107)A(cid:107)σ .

(cid:41)

(cid:40) A(x1, . . . , xk)
(cid:40) A(x1, . . . , xk)

(cid:107)x1(cid:107)p ···(cid:107)xk(cid:107)p

(cid:107)x1(cid:107)2 ···(cid:107)xk(cid:107)2

(cid:107)A(cid:107)p = sup

= sup

: xn (cid:54)= 0, xn ∈ Rdn, n ∈ [k]
× (cid:107)x1(cid:107)2 ···(cid:107)xk(cid:107)2
(cid:107)x1(cid:107)p ···(cid:107)xk(cid:107)p

: xn (cid:54)= 0, xn ∈ Rdn, n ∈ [k]

.

(cid:41)

(4)

(5)

(6)

(a) For 1 ≤ p ≤ 2, the equivalence of vector norms tells us

for all x ∈ Rd.

1

p− 1

2 (cid:107)x(cid:107)2 ,

(cid:107)x(cid:107)2 ≤ (cid:107)x(cid:107)p ≤ d
(cid:33) 1
Applying (5) to xn, n ∈ [k], we have

p ≤ k(cid:89)
Inserting (6) into (4) and noting dim(A) =(cid:81)k

(cid:32) k(cid:89)

2− 1

dn

n=1

n=1

(cid:107)xn(cid:107)2
(cid:107)xn(cid:107)p

≤ 1.

n=1 dn, we ﬁnd
p (cid:107)A(cid:107)σ ≤ (cid:107)A(cid:107)p ≤ (cid:107)A(cid:107)σ ,

1

2− 1

dim(A)

which completes the proof.

(b) For 2 ≤ p ≤ ∞, we instead have the bounds

d

which implies

1

p− 1

2 (cid:107)x(cid:107)2 ≤ (cid:107)x(cid:107)p ≤ (cid:107)x(cid:107)2 ,

1 ≤ k(cid:89)

n=1

(cid:107)xn(cid:107)2
(cid:107)xn(cid:107)p

≤

(cid:32) k(cid:89)

n=1

for all x ∈ Rd,

(cid:33) 1

2− 1

p

dn

.

Using these inequalities in (4) yields the desired result.

3. Partitions and general tensor unfoldings

5

(cid:3)

Any higher-order tensor can be transformed into diﬀerent lower-order tensors by modifying its
indices in various ways. The most common transformations are n-mode ﬂattenings, or matriciza-
i(cid:54)=n di matrix. For example,
the n-mode matricization of a tensor A ∈ Rd1×···×dk is obtained by mapping the ﬁxed tensor index
(i1, . . . , ik) to the matrix index (in, m), where

tions, which rearrange the elements of an order-k tensor into a dn ×(cid:81)
(cid:89)

(cid:89)

(ia − 1)Ja, where Ja =

(7)

m = 1 +

a∈[k],a(cid:54)=n

dl.

l∈[a−1],l(cid:54)=n

Recently there has been much interest in studying the relationship between tensors and their matrix
ﬂattenings [9]. Unlike earlier works [10, 11], we present here a more general analysis by considering
all possible lower-order tensor unfoldings rather than just matricizations. Using the blocks of a
partition of [k] to specify which modes are combined into a single mode of the new tensor, we
establish a one-to-one correspondence between the set of all partitions of [k] and the set of lower-
order tensor unfoldings. The partition lattice then describes the underlying relationship between
possible tensor unfoldings of a tensor A.
(cid:96) } of disjoint, nonempty subsets
i = [k]. The set of all partitions of [k] is denoted P[k]. We use |π|
(or blocks) Bπ
to denote the number of blocks in π, and |Bπ
i . We say
that a partition π is a level-(cid:96) partition if |π| = (cid:96). The set of all level-(cid:96) partitions of [k] is denoted
by P (cid:96)
[k], which is a set of S(k, (cid:96)) elements, where S(k, (cid:96)) is the Stirling number of the second kind.
The following partial order naturally relates partitions satisfying a basic compatibility constraint

For any k ∈ N+, a partition π of [k] is a collection {Bπ

i | to denote the number of elements in Bπ

i satisfying ∪(cid:96)

1 , Bπ

2 , . . . , Bπ

i=1Bπ

and the resulting structure plays a key role in our work.
Deﬁnition 3.1 (Partition Lattice). A partition π1 ∈ P[k] is called a reﬁnement of π2 ∈ P[k] if each
block of π1 is a subset of some block of π2; conversely, π2 is said to be a coarsening of π1. This
relationship deﬁnes a partial order, expressed as π1 ≤ π2, and we say that π1 is ﬁner than π2 while
π2 is coarser than π1. If either π1 ≤ π2 or π2 ≤ π1, then π1 and π2 are comparable. According to
this partial order, the least element of P[k] is 0[k] := {{1}, . . . ,{k}}, while the greatest element is
1[k] := {{1, . . . , k}}. Equipped with this notion, P[k] generates a partition lattice by connecting any
two comparable partitions that diﬀer by exactly one level. An example is illustrated in Figure 1.
Henceforth, P[k] may represent either the set of all partitions of [k] or the partition lattice it
generates depending on context.

It is clear that many partitions are not comparable, including every pair of distinct partitions at
the same level. To consider arbitrary partitions in tandem, we require an extension of this partial
order. In general, for any two partitions π1, π2 ∈ P[k], we deﬁne their greatest lower bound π1 ∧ π2
as

π1 ∧ π2 := sup{π ∈ P[k] : π ≤ π1, π ≤ π2}.

6

Figure 1. The partition lattice P[4].

More concretely, π1 ∧ π2 consists of the collection of all nonempty intersections of blocks in π1 and
π2 and is unique for a given pair (π1, π2).
Example 3.2. Figure 1 illustrates P[4] in lattice form. Recall that an edge connects two partitions
if and only if they are comparable and their levels diﬀer by exactly one. Partitions are comparable
if and only if there exists a non-reversing path between them. To clarify the components of Def-
inition 3.1, take π1 = {{1, 4},{2, 3}} and π2 = {{1, 3, 4},{2}}. Then π1 ∧ π2 = {{1, 4},{2},{3}},
0[4] = {{1},{2},{3},{4}}, and 1[4] = {{1, 2, 3, 4}}.

The following deﬁnition generalizes the concept of n-mode ﬂattenings deﬁned in (7) to general

unfoldings induced by arbitrary partitions π ∈ P[k].
Deﬁnition 3.3 (General Tensor Unfolding). Let A ∈ Rd1×···×dk be an order-k tensor and π =
dj] × ··· ×
{Bπ

(cid:96) } ∈ P[k]. The partition π deﬁnes a mapping φπ : [d1] × ··· × [dk] → [(cid:81)

j∈Bπ

1

[(cid:81)

1 , . . . , Bπ
j∈Bπ

(cid:96)

dj] such that

(cid:89)

r∈Bπ

j

where

(8)

mj = 1 +

(ir − 1)Jr, where Jr =

for all j ∈ [(cid:96)].

dl,

φπ(i1, . . . , ik) = (m1, . . . , m(cid:96)),

(cid:89)

l∈Bπ

j ,l<r

j∈Bπ

1

π is well deﬁned. Thus φπ induces an unfolding

dj] × ··· × [(cid:81)

Clearly, φπ is a one-to-one mapping, so its inverse φ−1
action A (cid:55)→ Unfoldπ(A) such that
(9)

(Unfoldπ(A))(m1,...,m(cid:96)) = (A)φ−1

π (m1,...,m(cid:96)),
dj], or, equivalently,
(Unfoldπ(A))φπ(i1,...,ik) = (A)(i1,...,ik),

for all (m1, . . . , m(cid:96)) ∈ [(cid:81)
for all (i1, . . . , ik) ∈ [d1]×···×[dk]. Thus Unfoldπ(A) is an order-(cid:96) tensor of dimensions ((cid:81)
(cid:81)
Example 3.4. Consider an order-4 tensor A = (cid:74)aijkl(cid:75) ∈ R2×2×2×2. We provide a subset of the

di), and we call it the tensor unfolding of A induced by π.

possible tensor unfoldings to elucidate both the manner in which the operation works and the
natural association with partitions.

i∈Bπ

(cid:96)

j∈Bπ

(cid:96)

i∈Bπ

1

di, . . . ,

• For π = {{1, 2},{3},{4}}, Unfoldπ(A) is an order-3 tensor of dimensions (4, 2, 2) with

entries given by

7

(Unfoldπ(A))1kl = a11kl,
(Unfoldπ(A))3kl = a21kl,

(Unfoldπ(A))2kl = a12kl,
(Unfoldπ(A))4kl = a22kl,

for all (k, l) ∈ [2] × [2].

• For π = {{1, 2},{3, 4}}, Unfoldπ(A) is a 4 × 4 matrix

 .

a1211 a1212 a1221 a1222
a2111 a2112 a2121 a2122
a2211 a2212 a2221 a2222

 a1111 a1112 a1121 a1122

 .

a1111 a1112
a1121 a1122
a1211 a1212
a1221 a1222
a2111 a2112
a2121 a2122
a2211 a2212
a2221 a2222

Unfoldπ(A) =

Unfoldπ(A) =

• For π = {{1, 2, 3},{4}}, Unfoldπ(A) is a 8 × 2 matrix

Remark 3.5. There are diﬀerent conventions to order the elements within each transformed mode.
In principle, the ordering of elements within each transformed mode is irrelevant, so we do not
explicitly spell out their orderings hereafter.

Proposition 3.6. Let A,B ∈ Rd1×···×dk be two order-k tensors with the same dimensions. The
unfolding operation has the following properties:

(a) Unfold0[k](A) = A and Unfold1[k](A) = Vec(A).
(b) (cid:104)A,B(cid:105) = (cid:104)Unfoldπ(A), Unfoldπ(B)(cid:105) for all π ∈ P[k].
(c) (cid:107)A(cid:107)F = (cid:107)Unfoldπ(A)(cid:107)F for all π ∈ P[k].
Part (b): Let A =(cid:74)ai1... ik(cid:75) and B =(cid:74)bi1... ik(cid:75), both in Rd1×···dk . For any π ∈ P[k], the deﬁnition

Proof. Part (a): This follows directly from the deﬁnition of φπ in (8).

of the inner product and (9) imply

(cid:104)Unfoldπ(A), Unfoldπ(B)(cid:105) =

(A)φ−1

π (m1,...,m(cid:96))(B)φ−1

π (m1,...,m(cid:96))

(cid:88)
(cid:88)

m1,...,m(cid:96)

=
= (cid:104)A, B(cid:105),

i1,...,ik

ai1,...,ik bi1,...,ik

where the second line follows from the fact that the φ−1
to (i1, . . . , ik).

Part (c): Letting B = A above yields the result.

π is a one-to-one mapping from (m1, . . . , m(cid:96))
(cid:3)

8

4. Operator norm inequalities on the partition lattice

In this section, we compare the operator norms of diﬀerent unfoldings of a tensor, in particular
relative to that of the original tensor. We ﬁrst focus on the spectral norm (p = 2) and then discuss
extensions to general lp-norms. The following lemma will play a key role in proving our main
results:
Lemma 4.1 (One-Step Inequality). For 1 < (cid:96) < k, let B ∈ Rd1×···×d(cid:96) be an order-(cid:96) tensor unfolding
of an order-k tensor A ∈ Rd(cid:48)
k induced by the partition {B1, . . . , B(cid:96)} ∈ P[k], and let C be an
order-((cid:96) − 1) tensor unfolding of B induced by merging blocks Bi and Bj for some i, j ∈ [(cid:96)]. Then,

1×···×d(cid:48)

min (di, dj)

−1/2 (cid:107)C(cid:107)σ ≤ (cid:107)B(cid:107)σ ≤ (cid:107)C(cid:107)σ .

Proof. Without loss of generality, assume C corresponds to the merging of blocks B(cid:96)−1 and B(cid:96)
so that C is a (d1, . . . , d(cid:96)−2, d(cid:96)−1 · d(cid:96))-dimensional tensor. By deﬁnition of the spectral norm and
Proposition 3.6(b),

(cid:107)B(cid:107)σ = sup(cid:8)(cid:104)B, x1 ⊗ ··· ⊗ x(cid:96)(cid:105) : xn ∈ Sdn−1, n ∈ [(cid:96)]}

= sup(cid:8)(cid:104)C, x1 ⊗ ··· ⊗ x(cid:96)−2 ⊗ Vec(x(cid:96)−1 ⊗ x(cid:96))(cid:105) : xn ∈ Sdn−1, n ∈ [(cid:96)]}.
x(cid:96)) : (x(cid:96)−1, x(cid:96)) ∈ Sd(cid:96)−1−1 × Sd(cid:96)−1} is contained in the set(cid:8)y : y ∈ Sd(cid:96)−1·d(cid:96)−1(cid:9). We then have
(cid:107)B(cid:107)σ ≤ sup(cid:8)(cid:104)C, x1 ⊗ ··· ⊗ x(cid:96)−2 ⊗ y(cid:105) : y ∈ Sd(cid:96)−1d(cid:96)−1, xn ∈ Sdn−1, n ∈ [(cid:96) − 2]}

Note that (x(cid:96)−1, x(cid:96)) ∈ Sd(cid:96)−1−1 × Sd(cid:96)−1 implies (cid:107)x(cid:96)−1 ⊗ x(cid:96)(cid:107)F = 1. Hence, the set {Vec(x(cid:96)−1 ⊗

= (cid:107)C(cid:107)σ ,

To prove the lower bound, note that Sd1−1 × ··· × Sd(cid:96)−2−1 × Sd(cid:96)−1d(cid:96)−1 is a compact set, so the
(cid:96)−2, y∗) ∈ Sd1−1 × ··· ×

1, . . . , x∗

which is the desired upper bound.
supremum (2) is attained in that set for C. Then, there exists (x∗
Sd(cid:96)−2−1 × Sd(cid:96)−1d(cid:96)−1 such that
(10)
Deﬁne C∗ = C(x∗
we can rewrite (10) as

(cid:107)C(cid:107)σ = (cid:104)C, x∗

1 ⊗ ··· ⊗ x∗

(cid:96)−2 ⊗ y∗(cid:105).

1, . . . , x∗

(cid:96)−2, Id(cid:96)−1d(cid:96)) ∈ Rd(cid:96)−1d(cid:96). By the self duality of the Frobenius norm in Rd(cid:96)−1d(cid:96),

(cid:110)(cid:104)C∗, y(cid:105) : y ∈ Sd(cid:96)−1d(cid:96)−1(cid:111)

= (cid:107)C∗(cid:107)F .

(cid:107)C(cid:107)σ = sup

1, . . . , x∗

(11)
On the other hand, if we deﬁne Mat(C∗) := B(x∗
the vectorization of Mat(C∗). Hence, by Proposition 3.6(c), we obtain
(12)
Using the deﬁnition of Mat(C∗), we can write
2 ⊗ ··· ⊗ x∗

(cid:110)(cid:104)B, x∗
(cid:110)(cid:104)Mat(C∗), x(cid:96)−1 ⊗ x(cid:96)(cid:105) : (x(cid:96)−1, x(cid:96)) ∈ Sd(cid:96)−1−1 × Sd(cid:96)−1(cid:111)

(cid:107)C∗(cid:107)F = (cid:107)Mat(C∗)(cid:107)F .

(cid:96)−2 ⊗ x(cid:96)−1 ⊗ x(cid:96)(cid:105) : (x(cid:96)−1, x(cid:96)) ∈ Sd(cid:96)−1−1 × Sd(cid:96)−1(cid:111)

1 ⊗ x∗

(cid:96)−2, I(cid:96)−1, I(cid:96)) ∈ Rd(cid:96)−1×d(cid:96), then C∗ is simply

(cid:107)B(cid:107)σ ≥ sup
= sup
= (cid:107)Mat(C∗)(cid:107)σ .

(13)
Recall that Mat(C∗) is a matrix of size d(cid:96)−1 × d(cid:96), meaning [8]
(14)

(cid:107)Mat(C∗)(cid:107)σ ≥ min (d(cid:96)−1, d(cid:96))

−1/2 (cid:107)Mat(C∗)(cid:107)F .

Using (13) in conjunction with (14), (12), and (11), we then obtain

(cid:107)B(cid:107)σ ≥ min (d(cid:96)−1, d(cid:96))
= min (d(cid:96)−1, d(cid:96))

−1/2 (cid:107)Mat(C∗)(cid:107)F
−1/2 (cid:107)C∗(cid:107)F

which proves the lower bound.

= min (d(cid:96)−1, d(cid:96))

−1/2 (cid:107)C(cid:107)σ ,

9

(cid:3)

With Lemma 4.1 in hand, we can easily show that the spectral norm preserves the partial order

on partitions.

Theorem 4.2 (Monotonicity). For all partitions π1, π2 ∈ P[k] satisfying π1 ≤ π2,

(cid:107)Unfoldπ1(A)(cid:107)σ ≤ (cid:107)Unfoldπ2(A)(cid:107)σ .

In particular, we have the global extrema

(cid:13)(cid:13)(cid:13)Unfold0[k](A)
(cid:13)(cid:13)(cid:13)σ
(cid:13)(cid:13)(cid:13)Unfold1[k](A)
(cid:13)(cid:13)(cid:13)σ

(cid:107)A(cid:107)σ =
(cid:107)A(cid:107)F =

(cid:107)Unfoldπ(A)(cid:107)σ ,
(cid:107)Unfoldπ(A)(cid:107)σ .

= min
π∈P[k]

= max
π∈P[k]

(cid:89)

B∈π1

Proof. If π1 ≤ π2, we can obtain Unfoldπ2(A) from Unfoldπ1(A) by a series of single unfoldings
as deﬁned in Lemma 4.1. Thus, applying the Lemma to these successive unfoldings gives the
(cid:3)
result.

More generally, we can establish inequalities relating the spectral norms of two tensor unfoldings
corresponding to two arbitrary partitions π1, π2 ∈ P[k] that are not necessarily comparable. To do
so, we must ﬁrst introduce the following deﬁnition.
Deﬁnition 4.3. Given an order-k tensor A ∈ Rd1×···×dk , we deﬁne the map dimA : P[k]×P[k] → N+
as

dimA(π1, π2) =

max
B(cid:48)∈π2

DA(B, B(cid:48)), where DA(B, B(cid:48)) =

(cid:89)

dn,

n∈B∩B(cid:48)

where π1, π2 ∈ P[k].

We label this quantity as dimA(·,·) because it involves a product of a subset of the dimensions
of A, and dimA(π1, π2) ≤ dim(A) with equality only when π1 = π1 ∧ π2. Intuitively, dimA(π1, π2)
reﬂects the overlap between the unfoldings induced by π1 and π1 ∧ π2. Example 4.5 presents a
concrete illustration.
Remark 4.4. We set DA(B, B(cid:48)) = 0 when B ∩ B(cid:48) = ∅. This does not aﬀect the multiplication
in Deﬁnition 4.3 because a block of π1 cannot be disjoint from every block of π2. Note that
DA(B, B(cid:48)) = DA(B(cid:48), B), but in general dimA(π1, π2) (cid:54)= dimA(π2, π1).
Example 4.5. To illustrate the above map, let A ∈ Rd1×d2×d3×d4 and consider the partitions π1 =
{{1, 2},{3, 4}} and π2 = {{1, 2, 3},{4}}, for which π1∧ π2 = {{1, 2},{3},{4}}. From Deﬁnition 4.3,

DA({1, 2},{1, 2, 3}) = d1d2
DA({3, 4},{1, 2, 3}) = d3
DA({1, 2},{4})
= 0
DA({3, 4},{4})
= d4.

dimA(π1, π2) = max(cid:8)DA({1, 2},{1, 2, 3}), DA({1, 2},{4})(cid:9)

× max(cid:8)DA({3, 4},{1, 2, 3}), DA({3, 4},{4})(cid:9)
dimA(π2, π1) = max(cid:8)DA({1, 2, 3},{1, 2}), DA({1, 2, 3},{3, 4})(cid:9)

= d1d2 max{d3, d4}.

Then,

Exchanging arguments, we ﬁnd

10

× max(cid:8)DA({4},{1, 2}), DA({4},{3, 4})(cid:9)

= d4 max{d1d2, d3}.

Remark 4.6. As in Lemma 4.1, if π2 is a one-step coarsening of π1 obtained by merging two blocks
Bπ1

j of dimensions di and dj into a single block Bπ2

i and Bπ1

1 , then

DA(Bπ1
i
DA(Bπ1

, Bπ2
j , Bπ2

1 ) = di,
1 ) = dj.

Hence, Lemma 4.1 can be written as

min(cid:8)DA(Bπ1

i

1 )(cid:9)−1/2 (cid:107)Unfoldπ2(A)(cid:107)σ ≤ (cid:107)Unfoldπ1(A)(cid:107)σ ≤ (cid:107)Unfoldπ2(A)(cid:107)σ .

, Bπ2

1 ), DA(Bπ1

j , Bπ2

Having introduced Deﬁnition 4.3, we can now state our main result on how the spectral norms

of two arbitrary unfoldings of a tensor are related:
Theorem 4.7 (Spectral norm inequalities). Let A ∈ Rd1×···×dk be an arbitrary order-k tensor, and
(cid:20)
π1, π2 any two partitions in P[k]. Then,

(cid:21)−1/2 (cid:107)Unfoldπ1(A)(cid:107)σ ≤ (cid:107)Unfoldπ2(A)(cid:107)σ ≤

(cid:21)1/2 (cid:107)Unfoldπ1(A)(cid:107)σ .

dim(A)

dim(A)

(cid:20)

dimA(π2, π1)

dimA(π1, π2)

Remark 4.8.

(1) Note that π1 and π2 need not be comparable.
(2) If dn = d for all n ∈ [k], then the result reduces to

where c1 = (k −(cid:80)

d−c1/2 (cid:107)Unfoldπ1(A)(cid:107)σ ≤ (cid:107)Unfoldπ2(A)(cid:107)σ ≤ dc2/2 (cid:107)Unfoldπ1(A)(cid:107)σ ,

maxB(cid:48)∈π2 |B ∩ B(cid:48)|) and c2 = (k −(cid:80)

B∈π1

B∈π2

maxB(cid:48)∈π1 |B ∩ B(cid:48)|).

i

: i ∈ [(cid:96)1]} and π2 = {Bπ2

(cid:107)Unfoldπ1∧π2(A)(cid:107)σ ≤ (cid:107)Unfoldπ1(A)(cid:107)σ .

Proof. The main idea is to apply Lemma 4.1 to some appropriate sequence of partitions connecting
π1 and π2 in the partition lattice. To do so, we consider Unfoldπ1∧π2(A) and compare its spectral
norm to that of Unfoldπ1(A). Since π1 ∧ π2 ≤ π1, from Theorem 4.2 we have
(15)
Let π1 = {Bπ1
[(cid:96)2]}. Now, order the blocks in π2 such that
(16)

1 , Bπ2
mj . Consider a sequence (T1, T2, . . . , T(cid:96)2−1) of unfoldings of A where
and deﬁne B1j = Bπ1
Tj, for 1 ≤ j ≤ (cid:96)2 − 1, is obtained from the tensor Unfoldπ1∧π2(A) by an unfolding operation
corresponding to merging the blocks B1,1, . . . , B1,j+1 into a single block. Using Lemma 4.1 and (16),
we obtain

: j ∈ [(cid:96)2]}, and note that π1 ∧ π2 = {Bπ1

m2) ≥ ··· ≥ DA(Bπ1

DA(Bπ1
1 ∩ Bπ2

m1) ≥ DA(Bπ1

: i ∈ [(cid:96)1], j ∈

i ∩ Bπ2

1 , Bπ2
m(cid:96)2

1 , Bπ2

),

j

j

Similarly for all i ∈ [(cid:96)2 − 2],
(cid:107)Ti(cid:107)σ ≥ min

(cid:110)
=(cid:2)DA(Bπ1

DA(Bπ1

1 , Bπ2

(cid:107)Unfoldπ1∧π2(A)(cid:107)σ ≥ min{DA(Bπ1
1 , Bπ1

=(cid:2)DA(Bπ1

m1), DA(Bπ1

1 , Bπ2

m2)(cid:3)−1/2 (cid:107)T1(cid:107)σ .
i+1(cid:89)
mi+2)(cid:3)−1/2 (cid:107)Ti+1(cid:107)σ .

DA(Bπ1

1 , Bπ2

mi+2) ,

j=1

1 , Bπ2
mj )

1 , Bπ2

m2)}−1/2 (cid:107)T1(cid:107)σ

(cid:111)−1/2 (cid:107)Ti+1(cid:107)σ

Combining these inequalities gives

j=1
max
j∈[(cid:96)2]

=

j=2

(cid:107)Unfoldπ1∧π2(A)(cid:107)σ ≥

 (cid:96)2(cid:89)

(cid:96)2(cid:81)

(cid:107)Unfoldπ1∧π2(A)(cid:107)σ ≥ (cid:96)1(cid:89)

(cid:96)1(cid:81)
(cid:96)1(cid:81)
(cid:34) (cid:81)

j=1
max
j∈[(cid:96)2]

(cid:96)2(cid:81)

(cid:96)2(cid:81)

max
j∈[(cid:96)2]

j=1

i=1

i=1

i=1

=

=

−1/2




−1/2

−1/2

DA(Bπ1

1 , Bπ2
mj )

(cid:107)T(cid:96)2−1(cid:107)σ
−1/2

DA(Bπ1
{DA(Bπ1

1 , Bπ2
j )
j )}
1 , Bπ2

(cid:107)T(cid:96)2−1(cid:107)σ .

DA(Bπ1
i
{DA(Bπ1

, Bπ2
j )
j )}
, Bπ2

i

DA(Bπ1
i

, Bπ2
j )

{DA(Bπ1

j )}
, Bπ2

i

(cid:35)−1/2

(cid:107)Unfoldπ1(A)(cid:107)σ

(cid:107)Unfoldπ1(A)(cid:107)σ

11

(cid:3)

(cid:3)

We can iterate the same line of argument with {Bπ1

i ∩ Bπ2

mj : j ∈ [(cid:96)2]} for i = 2, . . . , (cid:96)1 to obtain

n∈[k] dn

dimA(π1, π2)

(cid:107)Unfoldπ1(A)(cid:107)σ .

Together with (15), this last inequality means

(17)

(cid:107)Unfoldπ1∧π2(A)(cid:107)σ ≤ (cid:107)Unfoldπ1(A)(cid:107)σ ≤

By symmetry,

(18)

(cid:107)Unfoldπ1∧π2(A)(cid:107)σ ≤ (cid:107)Unfoldπ2(A)(cid:107)σ ≤

(cid:34) (cid:81)
(cid:34) (cid:81)

n∈[k] dn

dimA(π1, π2)

n∈[k] dn

dimA(π2, π1)

(cid:35)1/2

(cid:35)1/2

(cid:107)Unfoldπ1∧π2(A)(cid:107)σ .

(cid:107)Unfoldπ1∧π2(A)(cid:107)σ .

Finally, combining (17) and (18) completes the proof.

We may immediately establish several corollaries of Theorem 4.7.

Corollary 4.9. All order-k tensors A ∈ Rd1×···×dk satisfy

(cid:20) dim(A)

maxn∈[k] dn

(cid:21)1/2 (cid:107)A(cid:107)σ .

(cid:107)A(cid:107)F ≤

Proof. Taking π1 = 0[k] and π2 = 1[k] in Theorem 4.7 yields the result.

Corollary 4.9 gives the worst-case ratio of the Frobenius norm to the spectral norm for an
arbitrary tensor. This ratio is sharper than the bound recently found by Friedland and Lim [5,
Lemma 5.1], namely (cid:107)A(cid:107)F ≤ dim(A)1/2 (cid:107)A(cid:107)σ.
at either level k or level 1. For ease of exposition, we assume dn = d for all n ∈ [k].

We now give a set of inequalities comparing the spectral norms of unfoldings at level (cid:96) to that

Corollary 4.10 (Bottom-Up Inequality). Let A ∈ Rd×···×d be an order-k tensor with the same
dimension d in all modes. For all levels 1 ≤ (cid:96) ≤ k and partitions π ∈ P (cid:96)
[k],

12

(19)

d−(k−(cid:96))/2 max
π∈P (cid:96)

(cid:107)Unfoldπ(A)(cid:107)σ ≤ (cid:107)A(cid:107)σ ≤ min
π∈P (cid:96)

[k]

(cid:107)Unfoldπ(A)(cid:107)σ .

[k]

Proof. Take π2 = 0[k] in Theorem 4.7.

(cid:3)

Remark 4.11. The existing work that is most closely related to our own is that of Hu’s [9], in
which the author bounds the nuclear norm of a tensor by that of its matricization. Since the nuclear
norm and spectral norm are dual to each another in tensor space, many of our results apply to the
nuclear norm as well. Particularly, letting (cid:96) = 2 in the bottom-up inequality (19) reproduces Hu’s
results.
Corollary 4.12 (Top-Down Inequality). Let A ∈ Rd×···×d be an order-k tensor with the same
dimension d in all modes. For all levels 1 ≤ (cid:96) ≤ k and partitions π ∈ P (cid:96)
[k],
i |)/2 (cid:107)A(cid:107)F ≤ (cid:107)Unfoldπ(A)(cid:107)σ ≤ (cid:107)A(cid:107)F .

d−(k−maxi∈[(cid:96)] |Bπ
(cid:3)
Proof. Take π1 = 1[k] in Theorem 4.7.
Corollary 4.13. Let A ∈ Rd×···×d be an order-k tensor with the same dimension d in all modes.
For all levels 1 ≤ (cid:96) ≤ k,

d−(k−(cid:100)k/(cid:96)(cid:101))/2 (cid:107)A(cid:107)F ≤ min
π∈P (cid:96)

[k]

(cid:107)Unfoldπ(A)(cid:107)σ ≤ max
π∈P (cid:96)

[k]

(cid:107)Unfoldπ(A)(cid:107)σ ≤ (cid:107)A(cid:107)F .

Proof. Note that the minimum of the maximal block sizes across all level-(cid:96) partitions of [k] is
(cid:3)
min
π∈P (cid:96)
[k]

|B| = (cid:100)k/(cid:96)(cid:101), and apply Corollary 4.12.

max
B∈π

The above corollaries bound the amount by which norms can vary over a speciﬁc level (cid:96). They im-
ply that the ratios (cid:107)Unfoldπ(A)(cid:107)σ /(cid:107)A(cid:107)σ and (cid:107)Unfoldπ(A)(cid:107)σ /(cid:107)A(cid:107)F fall in the intervals [1, d(k−(cid:96))/2]
and [d−(k−(cid:100)k/(cid:96)(cid:101))/2, 1], respectively. Therefore, in the worst case, (cid:107)Unfoldπ(A)(cid:107)σ only recovers (cid:107)A(cid:107)σ
or (cid:107)A(cid:107)F at poly(d) precision. Note that the factor d(k−(cid:96))/2 has an exponent linear in the diﬀerence
between the orders of the original tensor and its ﬂattening. This means that the potential deviation
√
between their spectral norms depends only on the diﬀerence in their orders rather than the actual
d
orders themselves, and that the deviation accumulates in multiplicative fashion with a loss of
in precision at each level. In contrast, the factor d−(k−(cid:100)k/(cid:96)(cid:101))/2 depends on more than just the gap
between k and (cid:96), with a larger impact for unfoldings with orders close to k.

We provide a low-order example that reaches the poly(d) scaling factor in Corollary 4.10.

Example 4.14. Consider the order-4 tensor A = Id ⊗ Id. Straightforward calculation shows that
(cid:107)A(cid:107)σ = 1. Furthermore, by symmetry, the spectral norm of an unfolding induced by any partition
in P 2

[4] must fall into one of the following ﬁve representative cases:

[4] or P 3
• For π1 = {{1},{2},{3, 4}} ∈ P 3
• For π2 = {{1},{3},{2, 4}} ∈ P 3
• For π3 = {{1, 2},{3, 4}} ∈ P 2
• For π4 = {{1},{2, 3, 4}} ∈ P 2
• For π5 = {{1, 3},{2, 4}} ∈ P 2

√
[4], (cid:107)Unfoldπ1(A)(cid:107)σ =
[4], (cid:107)Unfoldπ2(A)(cid:107)σ = 1.
[4], (cid:107)Unfoldπ3(A)(cid:107)σ = d.
√
[4], (cid:107)Unfoldπ4(A)(cid:107)σ =
[4], (cid:107)Unfoldπ4(A)(cid:107)σ = 1.

d.

d.

Therefore,

(cid:107)Unfoldπ(A)(cid:107)σ = d(4−3)/2 (cid:107)A(cid:107)σ

max
π∈P 3
[4]

and min
π∈P 3
[4]

(cid:107)Unfoldπ(A)(cid:107)σ = (cid:107)A(cid:107)σ .

(cid:107)Unfoldπ(A)(cid:107)σ = d(4−2)/2 (cid:107)A(cid:107)σ

max
π∈P 2
[4]

and min
π∈P 2
[4]

(cid:107)Unfoldπ(A)(cid:107)σ = (cid:107)A(cid:107)σ .

13

We conclude this section by generalizing Theorem 4.7 to lp-norms.

Theorem 4.15 (lp-norm inequalities). Let A ∈ Rd1×···×dk be an arbitrary order-k tensor, and
π1, π2 any two partitions in P[k]. Then,

(a) For any 1 ≤ p ≤ 2,

[dim(A)]−1/p
[dimA(π1, π2)]
(b) For any 2 ≤ p ≤ ∞,

−1/2

1

[dim(A)]

p−1
[dimA(π1, π2)]

−1/2

(cid:107)Unfoldπ1(A)(cid:107)p ≤ (cid:107)Unfoldπ2(A)(cid:107)p ≤ [dim(A)]1/p
[dimA(π2, π1)]1/2

(cid:107)Unfoldπ1(A)(cid:107)p ≤ (cid:107)Unfoldπ2(A)(cid:107)p ≤ [dim(A)]1− 1

p

[dimA(π2, π1)]1/2

(cid:107)Unfoldπ1(A)(cid:107)p .

(cid:107)Unfoldπ1(A)(cid:107)p .

of the tensor, dim(A) =(cid:81)

Proof. We only prove (a) since (b) follows similarly. For any given 1 ≤ p ≤ 2, Proposition 2.3
implies that the bound between the lp-norm and spectral norm depends only on the total dimension
n∈[k] dn. Because the total dimension is invariant under any unfolding

operation, we have

(20)
for all π ∈ P[k]. Combining (20) with Theorem 4.7 gives the desired results.

p (cid:107)Unfoldπ(A)(cid:107)σ ≤ (cid:107)Unfoldπ(A)(cid:107)p ≤ (cid:107)Unfoldπ(A)(cid:107)σ ,

dim(A)

1

2− 1

(cid:3)

5. Orthogonal decomposability and norm equality on upper cones

We have seen that the unfolding operation may change the spectral norm by up to a poly(d)
factor for an arbitrary A. This is undesirable in many ﬂattening-based algorithms, such as [3, 27].
However, for some specially-structured tensors, the operator norm on the partition lattice may not
change much either globally or locally. We demonstrate such a behavior for the following class of
tensors:
Deﬁnition 5.1 (π-orthogonal decomposable). Let A ∈ Rd1×···×dk be an order-k tensor and consider
any partition π ∈ P[k]. Then A is called π-orthogonal decomposable, or π-OD, over R if it admits
the decomposition

r(cid:88)

n=1

λna(n)

1 ⊗ ··· ⊗ a(n)
k ,
i ∈ Rdi : i ∈ [k], n ∈ [r]} satisﬁes

(21)

A =

where λn ∈ R+, n ∈ [r], and the set of vectors {a(n)

for all B ∈ π and all n, m ∈ [r].

(cid:104)⊗i∈Ba(n)

i

, ⊗i∈Ba(m)

(cid:105) = δnm,

i

A concept similar to π-OD, referred to as biorthogonal eigentensor decomposition [29], is intro-
duced in the tensor completion literature when k = 3 and π = {{1},{2, 3}}. Informally speaking,
π-OD imposes an orthogonality constraint on every block of singular vectors.

Remark 5.2 (0[k]-OD). When π = 0[k] in Deﬁnition 5.1, we obtain the special case of 0[k]-OD
tensors, which admit the decomposition (21) while satisfying
(cid:105) = δnm,

(cid:104)a(n)

, a(m)

i

i

for all i ∈ [k] and all n, m ∈ [r].

14

The deﬁnition of 0[k]-OD tensors generalizes the deﬁnition of orthogonal decomposable tensors
presented in [23], as we require neither symmetry nor equality of dimension across modes. In fact,
a 0[k]-OD tensor A is a diagonalizable tensor [3], meaning that the core tensor output from HOSVD
is superdiagonal (i.e., entries are zero unless i1 = ··· = ik).
Lemma 5.3. Consider an order-k tensor A ∈ Rd1×···×dk .

(a) Let π1, π2 ∈ P[k] and π1 ≤ π2. If A is π1-OD, then A is π2-OD.
(b) Let π ∈ P[k] and π (cid:54)= 1[k]. If A is π-OD, then

Proof. Part (a): For any two ﬁnite sets of vectors {xi} and {yi} for which xi, yi ∈ Rdi, we have
(22)

(cid:104)⊗i∈Bxi, ⊗j∈Byj(cid:105) =

(cid:104)xi, yi(cid:105),

(cid:107)Unfoldπ(A)(cid:107)σ = max
1≤n≤r
(cid:89)

λn.

i∈B

m(cid:89)

(cid:89)

(cid:89)

for all B ⊂ [k]. Suppose B ∈ π2. If π1 ≤ π2, then there exist subsets C1, . . . , Cm ∈ π1 such that
C1 ∪ ··· ∪ Cm = B. So,

(cid:104)⊗i∈Bxi, ⊗j∈Byj(cid:105) =

(cid:104)xi, yi(cid:105) =

(cid:104)xi, yi(cid:105) =

(cid:104)⊗i∈Caxi, ⊗j∈Cayj(cid:105),

i∈B

Part (b): Suppose π ∈ P (cid:96)

which implies that A is π2-OD if A is π1-OD.
Letting τ = {Bπ
τ ∈ P 2

1 )c} where (Bπ

1 , (Bπ

i∈Ca
[k] is of the form π = {Bπ

a=1

[k] and π ≤ τ . By Lemma 5.3(a), A is τ -OD, so A admits a decomposition of the form

1 )c denotes the complement of Bπ

i : i ∈ [(cid:96)]}. Note that π (cid:54)= 1[k] implies (cid:96) ≥ 2.
1 with respect to [k], we have

m(cid:89)

a=1

r(cid:88)

A =

λna(n)

1 ⊗ ··· ⊗ a(n)
k ,

(23)

where

(cid:68)⊗i∈B1a(n)

i

(cid:69)

, ⊗j∈B1a(m)

j

n=1

= δnm and

(cid:68)⊗i∈(Bπ

1 )ca(n)

i

(cid:69)

= δnm

, ⊗j∈(Bπ

1 )ca(m)

j

(24)
for all n, m ∈ [r].

Now deﬁne xn = Vec(⊗i∈Bπ

a(n)
i

and {yn} are sets of orthonormal vectors. By the deﬁnition of Unfoldτ (A), (23) implies

1

1 )ca(n)

i

) for all n ∈ [r]. By (24), both {xn}

) and yn = Vec(⊗i∈(Bπ
r(cid:88)

Unfoldτ (A) =

λnxnyT
n ,

which is simply the matrix SVD of Unfoldτ (A). Hence (cid:107)Unfoldτ (A)(cid:107)σ = max1≤n≤r λn. Using
monotonicity (Theorem 4.2), we have

n=1

(25)
Conversely, let n∗ = arg max1≤n≤r λn. By deﬁnition of the spectral norm, we have

(cid:107)Unfoldπ(A)(cid:107)σ ≤ (cid:107)Unfoldτ (A)(cid:107)σ = max
1≤n≤r

λn.

(cid:107)Unfoldπ(A)(cid:107)σ ≥(cid:68)
r(cid:88)

(cid:68)

Unfoldπ(A), Vec

=

λn

Vec

n=1

Vec

1

i

a(n∗)

(cid:16)⊗i∈Bπ
(cid:17) ⊗ ··· ⊗ Vec
(cid:17) ⊗ ··· ⊗ Vec

(cid:17) ⊗ ··· ⊗ Vec
(cid:16)⊗i∈Bπ
(cid:16)⊗i∈Bπ

a(n∗)

a(n)
i

(cid:16)⊗i∈Bπ
(cid:17)
(cid:17)(cid:69)

a(n)
i

a(n∗)

,

i

(cid:96)

(cid:96)

i

(cid:96)

i

(cid:16)⊗i∈Bπ
(cid:16)⊗i∈Bπ

1

1

(cid:17)(cid:69)

a(n∗)

r(cid:88)
r(cid:88)

n=1

=

=

(26)

(cid:89)

(cid:68)

j∈[(cid:96)]

λn

(cid:16)⊗i∈Bπ

j

a(n)
i

(cid:17)

Vec

(cid:16)⊗i∈Bπ

j

a(n∗)

i

(cid:17)(cid:69)

, Vec

λnδn,n∗ = λn∗ = max
1≤n≤r

λn,

15

where the third line comes from (22) and the last line follows from the fact that A is π-OD.
Combining (25) and (26), we conclude (cid:107)Unfoldπ(A)(cid:107)σ = max1≤n≤r λn.
(cid:3)

n=1

Theorem 5.4 (Norm equality on upper cones). If A is π-OD, then for any partition in the upper
cone Uπ = {τ ∈ P[k] : π ≤ τ < 1[k]} of π, we have

(cid:107)Unfoldτ (A)(cid:107)σ = (cid:107)Unfoldπ(A)(cid:107)σ .

Proof. If A is π-OD, then by Lemma 5.3(b), (cid:107)Unfoldπ(A)(cid:107)σ = max1≤n≤r λn. Given any τ ≥ π, by
Lemma 5.3(a), A is also τ -OD. Again, using Lemma 5.3(b), we have (cid:107)Unfoldτ (A)(cid:107)σ = max1≤n≤r λn.
Therefore, (cid:107)Unfoldτ (A)(cid:107)σ = (cid:107)Unfoldπ(A)(cid:107)σ.
(cid:3)
Theorem 5.4 states that the spectral norm is invariant for π-OD A under any unfolding induced
by the partitions in the upper cone Uπ of π. This lies in contrast with the poly(d) factor we have
seen for unstructured tensors.

Corollary 5.5. If A is 0[k]-OD, then for all partitions π (cid:54)= 1[k], we have

(cid:107)Unfoldπ(A)(cid:107)σ = (cid:107)A(cid:107)σ .

Corollary 5.5 implies that for 0[k]-OD tensors, the operator norm is invariant under any unfolding

operations except vectorization. Lastly, π1, π2 ∈ Uπ1∧π2 implies the following corollary:
Corollary 5.6. Let π1, π2 ∈ P[k]. If A is (π1 ∧ π2)-OD, then

(cid:107)Unfoldπ1(A)(cid:107)σ = (cid:107)Unfoldπ2(A)(cid:107)σ .

6. Discussion

In this paper, we presented a new framework representing all possible tensor unfoldings by the
partition lattice and established a set of general inequalities quantifying the impact of tensor unfold-
ings on the operator norms of the resulting tensors. We showed that the comparison bounds scale
polynomially in the dimensions {dn} of the tensor, with powers depending on the corresponding
partition and block sizes for any pair of tensor unfoldings being compared. As a direct consequence,
we demonstrated how the operator norm of a general tensor is bounded by that of its unfoldings.
In general, an unfolding operation may inﬂate the operator norm by up to a poly(d) factor, as
seen in Corollary 4.10. Note that the quantity dim(A) plays a key role in the worst-case inﬂation
factor and is a manifestation of the curse of dimensionality. Speciﬁcally, dim(A) can be quite
large as the mode dimensions and tensor order increase, with particular sensitivity to the latter.
In such settings, our main result seems to bode poorly for ﬂattening-based algorithms; however,
we believe that it should be interpreted with caution because our comparison bounds deal with
arbitrary tensors rather than those often sought in applications.
In fact, π-OD tensors permit
much tighter bounds in which some unfoldings, including certain matricizations, leave the operator
norm relatively unaﬀected. In practice, π-OD tensors, or those within a small neighborhood around
π-OD tensors, arise widely in statistical and machine learning applications [1, 13, 18].

Additionally, our work enables us to compare diﬀerent unfoldings at the same level (cid:96). Recent
work on problems featuring nuclear-norm regularization has shown that not all n-mode ﬂattenings

16

Indeed, as illustrated in Example 4.14, the operator norm of level-
are equally preferable [15].
2 unfoldings (i.e. matricizations) can be quite diﬀerent. Recently, several algorithms have been
proposed to account for this behavior. For example, Tomioka et al. [25] consider a weighted sum of
the norms of all single-mode matricizations. A lesser-known technique is square matricization [17,
22], in which the original tensor is reshaped into a nearly-square matrix by ﬂattening along multiple
modes. Our work provides general bounds to evaluate the eﬀectiveness of such schemes.

We have not attempted to characterize the degree to which operator norm relations on the
partition lattice restrict the original tensor. Essentially, this is a converse problem asking whether
π-OD is a necessary condition for Theorem 5.4 and Corollary 5.5 in addition to being suﬃcient. If
not, it would be useful to determine the extent to which such equalities inform us about the intrinsic
structure of the original tensor. From a practical standpoint, norm comparisons between diﬀerent
matricizations are relatively simple, but the optimal manner in which to use this information to
learn about the original tensor remains unknown.

In closing, we emphasize that while this work focuses on theory rather than computational
tractability, it possesses practical implications as well. Because direct calculation of the operator
norm of a level-(cid:96) tensor is generally computationally prohibitive for (cid:96) ≥ 3, exploiting level-2 unfold-
ings may be attractive when the unfolding eﬀect is small enough. Alternatively, for more precise
calculations, a number of approximation algorithms exist for higher-order tensor problems [25, 28]
at the cost of increased computation. Given that the trade-oﬀ between accuracy and computation
is often unavoidable, our work may be of help in ﬁnding an appropriate application-speciﬁc balance
when working with higher-order tensors.

This research is supported in part by a Packard Fellowship for Science and Engineering, and an

NIH training grant T32-HG000047.

Acknowledgements

References

[1] A. Anandkumar, R. Ge, D. Hsu, S. M. Kakade, and M. Telgarsky, Tensor decompositions for learning latent

variable models, The Journal of Machine Learning Research 15 (2014), no. 1, 2773–2832.

[2] J.-F. Cardoso, Eigen-structure of the fourth-order cumulant tensor with application to the blind source separation
problem, International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 1990, pp. 2655–2658.
[3] L. De Lathauwer, B. De Moor, and J. Vandewalle, A multilinear singular value decomposition, SIAM Journal on

Matrix Analysis and Applications 21 (2000), no. 4, 1253–1278.

[4] S. Friedland and L.-H. Lim, Computational complexity of tensor nuclear norm, arXiv:1410.6072 (2014).
[5]
[6] C. J. Hillar and L.-H. Lim, Most tensor problems are NP-hard, Journal of the ACM 60 (2013), no. 6, 45.
[7] P. D. Hoﬀ, Multilinear tensor regression for longitudinal relational data, The Annals of Applied Statistics 9

, The computational complexity of duality, arXiv:1601.07629 (2016).

(2015), no. 3, 1169–1193.

[8] R. A. Horn and C. R. Johnson, Matrix Analysis, Cambridge University Press, 2012.
[9] S. Hu, Relations of the nuclear norm of a tensor and its matrix ﬂattenings, Linear Algebra and its Applications

478 (2015), 188–199.

[10] T. G. Kolda, Multilinear operators for higher-order decompositions, United States Department of Energy, 2006.
[11] T. G. Kolda and B. W. Bader, Tensor decompositions and applications, SIAM Review 51 (2009), no. 3, 455–500.
[12] P. M Kroonenberg, Three-Mode Principal Component Analysis: Theory and Applications, Vol. 2, DSWO Press,

1983.

[13] V. Kuleshov, A. Chaganty, and P. Liang, Tensor factorization via matrix factorization, Proceedings of the 18th

International Conference on Artiﬁcial Intelligence and Statistics (AISTATS), 2015, pp. 507–516.

[14] L.-H. Lim, Singular values and eigenvalues of tensors: a variational approach, Proceedings of the IEEE Interna-
tional Workshop on Computational Advances in Multi-Sensor Adaptive Processing (CAMSAP), 2005, pp. 129–
132.

[15] J. Liu, P. Musialski, P. Wonka, and J. Ye, Tensor completion for estimating missing values in visual data, IEEE

Transactions on Pattern Analysis and Machine Intelligence 35 (2013), no. 1, 208–220.

[16] P. McCullagh, Tensor notation and cumulants of polynomials, Biometrika 71 (1984), no. 3, 461–476.

17

[17] C. Mu, B. Huang, J. Wright, and D. Goldfarb, Square deal: Lower bounds and improved relaxations for tensor

recovery, Proceedings of the 31st International Conference on Machine Learning (ICLM), 2014, pp. 73–81.

[18] M. Nickel, V. Tresp, and H.-P. Kriegel, A three-way model for collective learning on multi-relational data, Pro-

ceedings of the 28th International Conference on Machine Learning (ICML), 2011, pp. 809–816.

[19] L. Omberg, G. H. Golub, and O. Alter, A tensor higher-order singular value decomposition for integrative analysis
of DNA microarray data from diﬀerent studies, Proceedings of the National Academy of Sciences 104 (2007),
no. 47, 18371–18376.

[20] L. Qi, Eigenvalues of a real supersymmetric tensor, Journal of Symbolic Computation 40 (2005), no. 6, 1302–

1324.

[21] S. Ragnarsson and C. F. Van Loan, Block tensor unfoldings, SIAM Journal on Matrix Analysis and Applications

33 (2012), no. 1, 149–169.

[22] E. Richard and A. Montanari, A statistical model for tensor PCA, Advances in Neural Information Processing

Systems (NIPS) 27, 2014, pp. 2897–2905.

[23] E. Robeva, Orthogonal decomposition of symmetric tensors, SIAM Journal on Matrix Analysis and Applications

37 (2016), 86–102.

[24] J. Sun, D. Tao, and C. Faloutsos, Beyond streams and graphs: dynamic tensor analysis, Proceedings of the 12th

ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2006, pp. 374–383.

[25] R. Tomioka, K. Hayashi, and H. Kashima, Estimation of

low-rank tensors via convex optimization,

arXiv:1010.0789 (2010).

[26] R. Tomioka and T. Suzuki, Spectral norm of random tensors, arXiv:1407.1870 (2014).
[27] M. A. O. Vasilescu and D. Terzopoulos, Multilinear analysis of image ensembles: Tensorfaces, Proceedings of

European Conference on Computer Vision (ECCV), 2002, pp. 447–460.

[28] Y. Yu, H. Cheng, and X. Zhang, Approximate low-rank tensor learning, 7th NIPS Workshop on Optimization

for Machine Learning, 2014.

[29] M. Yuan and C.-H. Zhang, On tensor completion via nuclear norm minimization, Foundations of Computational

Mathematics (2015), 1–38.

[30] L. Zhao and M. J. Zaki, Tricluster: an eﬀective algorithm for mining coherent clusters in 3D microarray data,
Proceedings of the 2005 ACM SIGMOD International Conference on Management of Data, 2005, pp. 694–705.

