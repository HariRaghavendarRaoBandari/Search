Hierarchical Haptic Manipulation for Complex Skill Learning

Simon Hangl, Emre Ugur, Sandor Szedmak and Justus Piater1

6
1
0
2

 
r
a

M
2

 

 
 
]

O
R
.
s
c
[
 
 

1
v
4
9
7
0
0

.

3
0
6
1
:
v
i
X
r
a

Abstract— In complex manipulation scenarios (e.g. tasks requiring
complex interaction of two hands or in-hand manipulation), generaliza-
tion is a hard problem. Current methods still either require a substantial
amount of (supervised) training data and / or strong assumptions
on both the environment and the task. In this paradigm, controllers
solving these tasks tend to be complex. We propose a paradigm of
maintaining simpler controllers solving the task in a small number
of speciﬁc situations. In order to generalize to novel situations, the
robot transforms the environment from novel situations to a situation
where the solution of the task is already known. Our solution to this
problem is to play with objects and use previously trained skills (basis
skills). These skills can either be used for estimating or for changing the
current state of the environment and are organized in skill hierarchies.
The approach is evaluated in complex pick-and-place scenarios that
involve complex manipulation. We further show that these skills can
be learned by autonomous playing.

I. INTRODUCTION

Complex object manipulation in uncontrolled environments is a
hard and yet not completely solved problem in robotics. One of the
major issues in this context is the problem of generalizing motor
skills [1]–[4]. Much of it incorporates a paradigm, where the aim
is to adapt the controller itself to the changing environments. This
increases the complexity of the manipulation controller, as it should
deal with a wide range of different situations.

We propose to combine simpler and previously learned skills
in order to achieve more complex tasks. The aim is to exploit
simple skills to transfer the environment into a state where simple
controllers can achieve the desired complex task. This allows the
complexity of the controllers to be reduced, as they do not have to
deal with generalization.

Humans use similar behavioural patterns e.g. in sports such as
golf. The player always tries to stand in the same position relative
to the ball instead of adapting the swing itself in order to hit
the ball from another position. Therefore, the player is able to
execute the same (or very similar) previously learned trajectories.
This can highly reduce the training cost by constraining the search
space. We emphasize that in most approaches in robotics, the robot
would have to adapt the swing in order to hit the ball from many
different positions. A similar strategy seems to be exploited by
human infants. Piaget observed similar patterns in infant playing
at the age between 8 and 12 months [5]. This stage in the life of
infants is called the coordination of secondary schemata and Piaget
calls it the stage of ﬁrst actually intelligent behaviours. Infants use
previously learned skills to bring the objects into a state where they
can perform an intended action (e.g. kicking an obstacle out of the
way to grasp an object; pulling a string attached to an object to
bring it in reach). An important property of this stage is that they
do not predict the effects of these actions directly, but rather learn
to compose previously-known skills to achieve a speciﬁc task. They
do not have an understanding of what the effect of a manipulation
is. However, they know that a composition of certain skills leads
to a successful manipulation.

1Faculty of Mathematics, Informatics and Physics, University of Inns-

bruck, 6020 Innsbruck, Austria first.last@uibk.ac.at

(a) Baby picking up a book (b) Robot inspired by infant strategies pick-

ing up a book

Fig. 1. Book manipulation: Infant vs. autonomous robot

In this paper, a method is proposed that closely relates to this
paradigm. For a novel complex skill, the system plays with the
object and learns how to prepare the environment such that the
complex manipulation can be performed. Grasping a book from a
rigid table is an example of such a complex action – it involves
the coordination of two hands, where one hand prevents the book
from sliding while the other one presses against the binding of the
book and tries to lift it. The second hand has to perform an in-hand
manipulation to position a ﬁnger underneath the book in order to
ﬁnally be able to grasp it (Fig. 1(b)). When the robot intends to
execute a complex manipulation, it ﬁrst performs a sensing action
(either vision or haptic feedback from interacting with the object) to
determine the relevant parts of the environment state (for example
by sliding along the sides of the book to determine its orientation). It
has been shown that even complex manipulations can be constructed
by composing up to 20 different simple manipulation primitives
[6]. These primitives are deﬁned as the relative motion between
two rigid body parts. Some of these primitives can be used as
sensing actions in order to gather information about the environment
and are given beforehand. While running these sensing actions,
haptic feedback can be collected. The purpose is to decide which
previously learned skill should be used in order to transform the
environment into the desired state (preparatory action). In the book
grasping scenario, a rotation skill should be used as a preparatory
action in order to transform the the environment to a desired state
(e.g. a book can be grasped at the binding side - see 1(b)). After
the preparation, the novel complex skill can be executed in order
to achieve the desired task. When a novel complex skill can be
executed with a certain conﬁdence, it is added to the skill set of
the robot and therefore can be used in the playing phase for other
novel complex skills. This results in skill hierarchies that can be
arbitrarily deep. For example in a book pick-and-place task the
system ﬁrst learns how the pick up the book. After learning, this
complex skill is added to the skill set and a placing action can

be shown. The system can learn that if the intended action is a
placement task, it has to pick up the book ﬁrst. In order to do so,
it has to rotate the book.

When a novel skill is presented to the robot, it plays around
with the object and tries different sensing / preparatory action
pairs. Our contribution is a method that is able to learn how to
compose sensing actions and the subsequent preparatory actions by
autonomous playing (and observing success) in order to achieve
more complex tasks. We show that in this paradigm the controllers
can be simple (e.g. just simple repetitions of trained trajectories),
while still being able to achieve complex tasks. The training cost
is highly reduced and even inexperienced users are able to teach
novel skills to the robot (e.g. by kinesthetic teaching). Another
advantage of our approach is that already existing work from
current research (e.g. pushing controllers [7], [8]) can be included
as preparatory skills. As the environment changes, the learning
system must be ﬂexible. Thus, it needs to be easily extensible, as
novel skills are continuously added. To provide those features, we
propose an extension of projective simulation [9], which learns a
probability distribution on percept (outcome of sensing actions) /
action (preparatory skill) pairs from given rewards. I can be used
to learn which sensing action provides useful information about the
environment state for a given complex action. Further, it is utilized
to learn the mapping from the estimated environment state to the
preparatory skills.

II. RELATED WORK

The proposed framework is related to belief-space planning,
where the systems state is partially observed by sensors. In most
belief-space planning methods a control policy is trained, where at
each time step the next commands are predicted [10], [11]. These
commands are given in the action space of the robot, while in our
framework the selection of a complete controller is done, which
signiﬁcantly reduces the learning complexity. Similarly, macro
actions were used in a navigation task to reduce the dimensionality
of the problem [12]. However, this method does not generalize to
manipulation, as the primitives are designed for a navigation task
(e.g. walking along the corridor until reaching the end). In order to
apply a similar idea to manipulation, primitives that span the space,
arbitrary manipulation skills would have to be pre-deﬁned, which
makes it hard to apply the idea directly to manipulation.

Other related work uses haptic feedback to derive information
about the environment. Robots can learn the meaning of haptic
adjectives that were previously assigned to a set of objects by
touching them [13]. Different touching primitives (similar to the
sensing actions in this work), such as tap, squeeze, slide and others,
are used. Similarly, elementary interaction with the object can be
used to gather haptic information for object classiﬁcation by using
K-means [14]. Petrovskaya et. al. estimated the pose of an object
from haptic feedback [15]. Solution regions are approximated by
samples which are followed by subsequent continuous reﬁnements
in a Bayesian setting. These approaches share a spirit similar to our
approach. However, they are only concerned with the estimation of
the environment state and not with further use for manipulation.

Another aspect of our approach, haptic feedback, is used in many
manipulation methods. Associative skill memories [16] use typical
task-speciﬁc force patterns that are assigned to manipulations. From
this data it can be decided already at execution time whether the
manipulation is going to fail. If a failure is detected, the robot can
still react and try to ﬁx the problem. Further related work analyses
the applicability of a force time series database collected in exper-
iments with humans [17]. Stereotypical tasks such as door opening

are executed in order to create force databases. These can be used
to recognize different types of objects or anomalous forces during
the execution. In our approach we use haptic feedback in order
to distinguish between different environment states. Manipulation
primitives have been proposed in order to identify manipulations
to estimate object poses and further afforded actions [18]. These
primitives are composed and chosen such that the object is brought
into a pose in which the desired task can be executed. A follow-
up manipulation is selected according in order to bring the object
closer to a state where the target can be achieved. This approach
uses an idea similar to the method proposed in this paper, however,
our approach is not restricted to pose estimation of objects. It does
not assume that the environment state is only given by the pose of
the object; e.g. the environment state could also include information
about whether a box is open or not. Intrinsic reward can be used
in order to explore new effects when executing skills [19]. These
skills can then be composed in skill hierarchies.

Our method can also be compared to logic-based planning
systems such as STRIPS [20]. These planners derive provably-
correct plans to achieve a certain goal. The problem with these
approaches is that the pre- and post conditions of every single
action in the skill database have to be known and formalized. Even
though these planners produce perfectly valid results when the pre-
and post conditions are well estimated, this assumption does not
hold in many real-world tasks. In contrast, our method does not
deal with the prediction of the outcomes, but learns that a sequence
of actions delivers the desired result. Hence, this does not require
the formalization of all intermediate post conditions. There is also
work on open-loop planners for grasping that rearrange the objects
in the clutter in order to be able to grasp the object in a simple
way [21], [22]. However, this method is restricted to the grasping
domain.

III. SYSTEM ARCHITECTURE

The method comprises two inter-related pathways, the execution
pathway and the playing pathway. The execution pathway is used to
execute a complex skill, i.e. to execute the sensing action, estimate
the environment state from haptic data, perform the preparatory
action and ﬁnally execute the complex skill. Initially, the system
does not know which sensing action and which preparatory skills
are required to achieve a certain task. The playing pathway is
used to acquire this information by playing with the object. In the
scope of this paper only haptic data measured during elementary
interactions (sensing actions) with the object is used to determine
the state of the environment. Three different object-relative sensing
actions are used to determine the environment state: sliding along
the surface, poking and pressing (the object is pressed against the
second hand). In this paper we deﬁne the environment state as every
information that is useful to decide which action should be taken
next (e.g. the orientation of a book or whether or not a box is open).
Further, the system maintains a set of preparatory skills that can
change the state of the environment (e.g. a pushing skill that moves
or rotates the object).

In order to train a novel skill, the robot needs to gather informa-
tion about the environment states it might observe (e.g. the rotation
of a book or information whether a box is opened or closed). In
the ﬁrst playing phase, the robot explores the environment with
its sensing actions. These sensing actions are assumed to leave
the state of the environment unchanged (e.g. they do not change
the rotation of a book) and can therefore be performed multiple
times. This enables the robot to create a large haptic database that
stores information on how each sensing action feels like when the

the sensing action and, consequently, the preparatory action skill is
done by a random walk through the ECM. A transition probability
p (cj|ci) is assigned to each pair of clips ci, cj. It is deﬁned by

p (cj|ci) =

h (ci, cj)
k h (ci, ck)

(cid:80)

(1)

Fig. 2. ECM in the hierarchical skill learning scenario for one complex
skill.

system is in a certain environment state. In the second stage of
playing, the system is trained to select a sensing action and to pick
the preparation skill that ensures the successful execution of the
novel skill given an estimated environment state. This is achieved
by a reinforcement learning method called projective simulation
(PS) [9]. The free parameters of the PS model are adjusted, the
execution pathway is performed and the reward is measured in each
roll-out until the success rate reaches a certain threshold. When
the success rate for the novel skill is high enough, it is added
to the set of preparatory skills. In this way, the construction of
skill hierarchies is possible, as a complex skill can be used as a
preparatory action for another complex skill. For example placing
a book on a shelf requires grasping it ﬁrst. If the robot only knows
how to push objects, it will not be able to perform the complex
placing action. However, as soon as it has learned how to properly
grasp a book from the table, it can do the placement by using
grasping as a preparatory skill. The grasping skill itself might again
require preparatory skills, e.g. pushing the book to a pose in which
it can be grasped.

In order to make the following sections easier to comprehend,
ﬁrst the execution pathway is described. It is part of the playing
pathway and is performed once for each roll-out. It should be noted
that for the ﬁrst roll-out, the learning model is initialized and reﬁned
in each iteration.

A. Execution pathway

In order to execute a novel complex skill that was trained by

playing, the following steps are executed (Fig. 3(b)):

• Select a sensing action in order to collect data for environment

state estimation.

• Perform the sensing action and measuring haptic data.
• Estimate the environment state by classifying the haptic data.
• Select and executing a preparatory skill
to transform the
environment to a state in which the complex action can be
executed successfully.

• Execute the complex skill, e.g. by replay of trained trajectories

or execution hard-coded controllers.

The information on which sensing action should be used to estimate
the environment state and which preparation skill is executed in a
given state is encoded in the relation model.

1) Relation model for sensing / preparation pairs: The relation
model is the heart of the method and is used for two essential
tasks: (1) selecting the best sensing action given a desired task
and (2) selecting the correct preparatory action given the estimated
environment state. The core of relation learning is a modiﬁed
version of the PS model [9]. It consists of an episodic and
compositional memory (ECM) which is a network of so-called clips,
i.e. episodic memory fragments that include percepts and actions.
Each complex skill is encoded by an ECM (Fig. 2). The selection of

where h(ci, cj) is called the transition weight. We propose a
novel ECM structure (Fig. 2) that is tailored to the purpose of
manipulation. The network is layered and the clips in the lowest
layer are preparatory skills. A random walk always starts at the the
entry clip in the highest and ends at a preparatory skill clip.

In the ﬁrst step (transition from layer 1 to layer 2), a sens-
ing action is selected. This sensing action is executed and a
time series of haptic data is measured. It
is given by tSi =
{(tSij, FSij, TSij, PSij)} where tSij is the j-th time step of
the observed time series, FSij
is the Cartesian force, TSij
is the torque and PSij
is the Cartesian end-effector position.
From this data, the environment state is estimated. To estimate
the state given a sensing action, a multi-class classiﬁer, namely
maximum margin regression (MMR) [23] is used. In order to
classify the environment state from the time series ˆtSi measured
during the execution of a sensing action Si, a vector ˆtSi =
(FSi0, TSi0, PSi0, . . . , FSiT , TSiT , PSiT ) is used as input for
MMR. The output is a NSi-tuple, where NSi is the number of
classes for sensing action Si. If the time series belongs to class
k, only the k-th entry is non-zero. MMR is used because it was
shown to work well on a wide variety of problems by determining
the intrinsic structure of the data (e.g. image annotation [24], [25]).
If the sensing action Si was selected and the environment state
k was observed, the walk proceeds with a transition from the clip
Si to the k-th child in layer 3. The transition between layer 3 and
4 and therefore the selection of the preparatory skill is again done
by a random transition according to equation 1. After the execution
of the preparatory skill, the complex skill is executed.

A separate treatment is made for complex skills that require a
grasped object for successful execution. The only sensing action is
weighing. If the external force F = 0, the object is not grasped yet
and only preparatory actions that result in a grasp are considered.
Otherwise (i.e. the object is already grasped) the novel complex
skill can be executed directly. If the complex skill does not require
a grasp, then only the preparatory actions that do not result in a
grasp are considered (Fig. 3(b)).

B. Playing pathway

In this section we describe how a novel complex skill is added
to and trained with the system. When a novel complex skill is
shown (either by hard-coding a controller, kinesthetic teaching or
providing more complex controllers with some limited amount of
generalization), the robot starts playing with the object (Fig. 3(a)).
1) Creation of the sensing database: In Section III-A, the system
performs a random walk through the ECM. One part of the walk
is the estimation of the environment state. The environment state
is a discrete class that determines some aspect of the environment
and does not necessarily have a semantic meaning, even though, it
can have a meaning (e.g. on whether a box is open or not or the
pose of an object). These classes are assigned to haptic time-series
measured in different states of the environment using the sensing
actions. In order to train the classes of the classiﬁer, a database
of haptic time-series labelled with the discrete environment state is
created. This is enough to train the MMR classiﬁer.

The robot executes all three sensing actions in every possible
discrete environment state multiple times in order to measure the

#SlidePoke...Sense N1..N11..N21..NN...ps1ps2ps...psNMMR ClassiﬁerFlip...PushSensingactionsEnvironmentstatesPreparatoryactionsTeach novel

complex

skill

Create
sensing
database

Init /
Update

ECM

Execute
Complex

Skill

Reached
Conﬁdence
Threshold?

Add to
ECM as
Prep

Action

(a) Overview of playing pathway:
Training of the haptic properties, se-
lection of the best sensing actions
and learning a probabilistic mapping
from environment states to prepara-
tion skills is done by reinforcement
learning

(b) Schematic sketch of the execution pathway. The robot senses the task-relevant environment state by haptic
exploration and classiﬁes the measured data. The mapping from the estimated haptic class is mapped to the
preparatory action by the relation learning module. After preparing the environment with the preparatory
action, the complex skill can be executed.

Fig. 3. Schematic sketch of the two parts of the proposed method: the playing pathway (left) and the execution pathway.

haptic feedback with multiple objects that belong to the same
object class (e.g. soft- and hard cover books). In order to do so,
each environment state has to be created. This can be done in a
supervised or unsupervised manner, i.e. either a supervisor prepares
the environment state (e.g. a human shows the robot a box when
it is open or closed) or the robot tries to prepare the states by
itself. If no supervisor is available to play with the robot, it uses
the preparatory controllers to change the state of the environment
(e.g. rotating a book by 90 degrees in each iteration). It performs
all three sensing actions and labels each sample with the number of
preparation actions used so far (e.g. if the object has been rotated
by 90 degrees for 3 times, the environment state is labelled with
the number 3). For each sensing action Si, a time-series of haptic
data is measured.

Not all sensing actions have the same discriminative power for
every task / object (e.g. while a sliding action is good for deciding
the orientation of a book, a poking action is not helpful in that
case). Therefore, for each complex skill and object class we assign
a discrimination score Di to each sensing action Si. This score
is computed by performing cross-validation for the time series
classiﬁer. If the average success rate si is high, the classiﬁer can
easily distinguish between the classes by using sensing action Si.
With an average success rate of si, the discrimination score Di is
given by

Di = exp(αs)

(2)

with a ﬁxed stretch factor α. The higher the stretch factor, the
stronger slight differences in the average success rate inﬂuence
the discrimination score. The discrimination score determines how
likely a sensing action is selected relative to other sensing actions.
Non-discriminative sensing actions should be used very rarely. The
non-linear exponential function proved to be effective in practice.
2) Initialization of the ECM: After the classiﬁer is trained, the
robot can start with the exploration of sensing and preparation
actions. In order to start
the ECM has to be
initialized, i.e. the transition weights h(ci, cj) that are deﬁning the
the selection probability of the sensing and preparatory actions.
For the transition weights between the clip in layer 1 and the

the execution,

clips in layer 2, the discrimination score can be used. Therefore,
the discrimination score provides a relative measure of the initial
importance of the sensing actions. It is very likely that the most
discriminative sensing action provides the best
information on
which preparation action should be selected. Therefore, we initialize
the transition probabilities according to the discrimination score by
pSi ∝ hSi = Di. The transition probabilities between layer 2 and
3 are given by the time series classiﬁer, where all transitions to
the child clips are 0 except the one that the classiﬁer predicts. As
the child clips encode the environment state given sensing action
i, each sensing clip in layer 2 has exactly NSi children. All other
transition weights in the network, i.e. the weights from layer 3 to
layer 4 are initialized with the constant value hinit.

3) Relation learning: Given that the ECM is initialized, the
system can now start to estimate the probability distribution of
sensing actions and resulting preparatory skills by updating the
transition weights. The update is done with a modiﬁed version
of the original PS update rules. Random walk paths should be
more likely in future situations if the action taken was rewarded,
i.e. the complex action succeeded after performing the preparation
action, and should be less likely otherwise. To be more explicit, let
{s = c1 → c2 → ··· → cK = a} be a random walk path through
the ECM that received a reward λ(t) ∈ R. For all clips on the path
the weights are updated by

= max(1.0,

ij(cid:124)(cid:123)(cid:122)(cid:125)

ht

current weight

ij(cid:124)(cid:123)(cid:122)(cid:125)

ht+1

next weight

where ht
rule is given by

ij = h(t) (ci, cj). For all clips not on the path, the update

damping

ij − 1(cid:1)
− γ(cid:0)ht
+ λ(t)(cid:124)(cid:123)(cid:122)(cid:125)
(cid:123)(cid:122)
(cid:125)
(cid:124)
ij − 1(cid:1)
− γ(cid:0)ht
(cid:123)(cid:122)
(cid:125)
(cid:124)

reward

)

ij(cid:124)(cid:123)(cid:122)(cid:125)

ht

)

(3)

(4)

(cid:124)

(cid:123)(cid:122)

(cid:125)

ht+1 (ci, cj)

= max(1.0,

next weight

current weight

damping

The constant γ is called the forgetting factor and deﬁnes how
quickly the model forgets previously achieved rewards for a given
path. It should be chosen according to how fast the dynamics of
the environment change over time. In many situations the dynamics
does not change much (e.g. physical properties such as the friction

poke...slide...presssensing actionsweighpoke...slide...presstime seriesgrasped?Relation learningRelation learningpush 90 deg...push 180 deg...ﬂippreparation actions(grasping not required)preparation actions(grasping required)pinch grasp...complex graspgrasp book...kick ball...complex grasppreparation actions(grasping not required)preparation actions(grasping required)place object...throw(a)

(b)

(c)

(d)

(e)

Fig. 4. Robot setting (Fig. 4(a)) used for the experiments; Figs. 4(b) to 4(e) show a sample execution of the book grasping skill. In Fig. 4(b) the sliding
sensing action is visualized; Afterwards, the book is rotated by 90 degrees (Fig. 4(c)) and lifted (Figs. 4(d), 4(e)).

coefﬁcient do not change for many tasks). Note that equations 3
and 4 are slightly changed compared to the original PS to handle
negative rewards (punishment). This is done in order to reduce
the probability of trying unrewarded sensing / preparatory action
combinations multiple times.

4) Building skill hierarchies: As soon as a complex skill A can
be executed with a certain conﬁdence, it is added to the ECM of
another complex skill B by connecting it to each clip in layer 2
with the initial weight hinit. The robot then goes back to the playing
phase for skill B. If B already has a high conﬁdence, the weights
for certain preparatory actions will be high compared to hinit and
the probability of exploring the new preparatory skill A is low but
non-zero. If the conﬁdence is low, all weights will be low and the
system will very likely start exploring the novel preparatory skill.
This renders PS an ideal approach to construct skill hierarchies.

IV. EVALUATION

We evaluate our method along three dimensions. Firstly, we apply
our approach to a complex book grasping task in an autonomous
playing scenario. The book grasping skill is added as a preparatory
skill. In a placement task a skill hierarchy using the grasping skill
is learned. Secondly, we use the statistical data (e.g. success rates
of the sensing actions, preparatory skills and the complex grasping
action) to evaluate the convergence behaviour in the same setting in
case more preparatory skills are added to the skill set. In the third
part of the evaluation we show that the same set of sensing actions
and preparatory skills can be applied to a completely different
manipulation problem, namely the task of placing an object into
a (closed) box. The system discovers autonomously that the box
needs to be opened before an object can be placed inside.

A. Applicability to real-world tasks

A video of the pick-and-place task described in this section can
be found at 1. In this scenario the main challenge is to get a ﬁnger
underneath the book in order to grasp it. The book is grasped by
squeezing it between both hands, lifting it on the binding side and
then using in-hand manipulation to wrap the ﬁngers around it. It
is easy to teach this skill for a single speciﬁc situation (e.g. by
kinesthetic teaching) but hard to generalize to arbitrary situations.
A general method would have to model physical properties of the
system in order to place the arms in the correct positions with
correct force vectors relative to the other arm and the book.

1https://iis.uibk.ac.at/public/shangl/iros2016/iros.mpg

Slide

Poke

Press

bottom

binding

open

top

1

2

3

4

1

2

3

4

push 90

push 180

push 270

ﬂip

nothing

Fig. 5. Qualitative sketch of the episodic memory after learning how to
grasp a book. Thick coloured lines indicate a strong weight between 2 nodes
and therefore a high probability of being chosen. If a semantic category of
the sensing is known (e.g. the 4 outcomes of the sensing actions correspond
pose of the book), it is given – otherwise the outcomes of the sensing stage
are simply numbered.

1) Experimental setting: The experiments were performed with
two KUKA LWR 4+ robotic arms with Schunk SDH grippers.
Different types of books (soft-cover, hard-cover, varying sizes) were
used to train the system. The complete robot setting is shown in
Fig. 4(a). In order to make the controllers work with different book
dimensions, they were designed to use the built-in impedance mode
of the KUKA arms. Three different sensing actions were used:

• Sliding: A ﬁnger slides along the edge that is parallel to the
table edges, while the second arm keeps the object in place.
• Poking: A stretched ﬁnger is placed on top of the object.
Afterwards, the arm is moved downwards with very low arm
stiffness.

• Press: Two ﬁngers are pressed against the book surface while

the second arm holds the object in place.

In the preparation stage, 2 different general controllers are used. In
order to be able to use them in the scope of the proposed method, the
general space is discretised and the following speciﬁc preparatory
controllers are used:
• Rotate: Rotates
{90, 180, 270}

the object by x degrees with x ∈

• Flip: Flips the object such that the bottom side shows upwards

afterwards.

• Do nothing: Leaves the object in the current state.

When a grasp roll-out was performed, the reward was estimated
by measuring the force on the end-effector. Then, the book was
dropped on the table and a random pushing action was selected to
prepare a random starting state. The learning parameters of the PS

Drop into box

Lean against wall

Tabletop grasp

Handover grasp

Tabletop grasp

Rot 90

Rot 180

Rot 270

Nothing

Rot 90

Rot 180

Rot 270

Nothing

Fig. 6.
Skill hierarchies for given complex skills (sub-skills with very low usage probability are not shown in this hierarchy). A skill hierarchy for a
placement task was constructed. In a leaning task task the orientation in which the book is grasped is important. Therefore only the tabletop grasp (in
which the book is grasped at the binding side) is rewarded. The tabletop grasp requires rotation of the book as preparatory action. For a simple dropping
task it does not matter, in which orientation the book is grasped.

some placement strategies (drop into box, lean against a wall) were
shown to the robot. In this case the system was able to learn that it
has to grasp the book ﬁrst. In that scenario, the robot constructed
the skill hierarchy shown in Fig. 6.

B. Convergence

An important property of the learning method is the success rate
convergence of the learning system. For the book picking task the
ground truth is known, as the sensing outcomes of the sliding action
are semantically meaningful, i.e. the orientation of the book. In
order to simulate the convergence behaviour, the experience gained
during the real-world experiments was used. The probability of
correct classiﬁcation was given by pslide = 0.93 (slide action),
ppoke = 0.27 (poke action), ppress = 0.4 (press action). The success
probability of the grasping action was pp = 0.98 (given that the
correct state was estimated). These values were used to simulate
the behaviour of the system if some dummy preparation actions
that are not useful for the task were added. As the used projective
simulation model is based on a stochastic process, N = 10000
agents were executed over t = 1500 time steps (roll-outs). The
average success in each time step was computed and is shown in
Fig. 7(a). Based on this data it was determined how many roll-outs
are required to achieve a success probability of at least psucc = 0.9.
The relation between the number of preparation actions Np and
the number of required roll-outs Nr to achieve at least a success
rate of 0.9 is shown in Fig. 7(b). For the special case of Np = 6
the number of roll-outs was determined with Nr = 80. A trivial
system that tries every combination of Ns = 3 sensing actions,
No = 4 sensing outcomes per action and Np = 6 preparation
actions would require Nr = NsNoNp = 72 roll-outs to observe
every combination only once. However, from this information it is
difﬁcult to infer knowledge about a certain sensing / preparation
skill combination (e.g. success / failure could be caused by noise,
unexpected temporary circumstances etc.). The proposed method in
this case only requires 80 roll-outs and focusses on regions in the
exploration space that are interesting for the problem. This is due
to the fact that the method tends to keep using the highly distinctive
sensing actions.

C. Task diversity

In Section IV-A, a speciﬁc example for the applicability of the
method was shown. Further, we show that the same setting and skill
set can be used to learn a diverse set of skills.

In the sensing stage the orientation of the book was estimated.
However, it is important to mention that the method does not assume
such semantic categories and is not bound to any speciﬁc property
(e.g. pose) of the object. To illustrate this, the system with the same

(a) Development of
the average
success rate over the number of
roll-outs for different numbers of
preparation actions, Np = 6
(blue), Np = 20 (red), Np = 30
(green), Np = 39 (yellow)

(b) Simulated convergence be-
haviour: Number of roll-outs Nr
required to reach a success rate
of at least 0.9 dependent on the
number of preparation actions Np.

Fig. 7. Convergence behaviour of the proposed hierarchical skill learning
model

model were set to λsucc = 1000 (successful roll-outs), λfail = −30,
hinit = 200 (initial transition weights) and γ = 0 (no forgetting).
As the complex in-hand manipulation skill in order to get the ﬁnger
underneath the book is indeed quite complex, it is unlikely to
succeed out of coincidence. Therefore, the reward was chosen to
be high. A forgetting factor greater 0 can be used in cases where
the environment changes its behaviour over time.

One of the challenges was to design robust controllers for
autonomous play. All the controllers and machine learning tech-
niques were developed within the kukadu robotics framework23.
The code is available online and free to use. The robot was made
to play for 100 roll-outs. In the data gathering phase, the book was
pushed clock-wise by 90 degrees. At each rotation, 50 samples
per sensing controller were collected. After the playing phase the
system produced the episodic memory shown in Fig. 5 (qualitative
sketch). The thick coloured lines correspond to transitions with
high weights and high probability. The dominant sensing action
is the slide action and its child states have a semantic meaning,
i.e. the orientation of the book. Further, the transitions from the
environment states to the preparatory skills match the ground truth
(e.g. if the book is already oriented correctly - ﬁnger slided along
the binding side - nothing has to be done; if the ﬁnger slided along
the top side, it has to be rotated for 90 degrees). The execution
sequence for a speciﬁc environment state after the playing phase
is shown in Figs. 4(c)–4(e). After the training phase, the tabletop
grasping strategy was added to the set of preparatory skills and

2https://github.com/shangl/kukadu
3https://github.com/shangl/iros2016

20040060080010001200140000.20.40.60.81number of roll-outssuccess probability1015202530350100200300400500600number of preparation actionsnumber of roll-outs(a)

(f)

(b)

(g)

(c)

(h)

(d)

(i)

(e)

(j)

Fig. 8. Flipping skill performed in two different tasks – providing two different semantic effects. In the book picking scenario, the book got ﬂipped. In
the box opening scenario, the box got opened.

general sensing and preparation controllers was confronted with a
small rectangular food-box with a removable cover (Fig. 4(a) - right
side). The task was to place an object inside the box. By kinesthetic
teaching the robot was taught to grasp an object and place it inside
the open box. In the playing phase, the box was presented in
a supervised manner in the open and closed conﬁgurations. The
system concluded that the best sensing action to gain reward is to
poke the box. This semantically refers to the state of the box being
open or not. The ﬂipping trajectory that was used to ﬂip the book in
the book picking scenario turns out to be able to remove the cover
from the box as a preparation action to place something inside the
box.

We emphasize that the robot does not have any notion of the
semantic categories of the actions it performed. However, it came
up with a semantically meaningful selection of skills and was able
to achieve the task with the same skill set as in the book picking
scenario. A comparison between the ﬂipping skill in the book
picking scenario and the box opening scenario is shown in Fig. 8.

V. CONCLUSIONS

In this paper, a novel method for robotic object manipulation
is introduced. The key idea is to reuse a set of previously trained
skills in order to prepare the environment in a way that the robot
can simply apply novel skills. This way, the teaching of novel
skills is simple. It is does not require the robot to be taught how
to manipulate an object in many different environment states. It
learns autonomously how to ensure the preconditions of a new
complex skill by playing with the object. The robot senses the
state of the environment by interaction with the object in order
to select the appropriate preparatory action. Then it tries to execute
the novel skill and receives the respective reward. This paradigm is
contrary to approaches where a novel skill is generalized to many
different situations directly. Therefore, it requires a signiﬁcantly
lower amount of supervised teaching effort, but higher autonomous
playing skills. As soon as the system is conﬁdent enough about a
novel skill, it is added to the skill set of the robot and can be used
as a preparatory action for new skills as well. In this way, complete
skill hierarchies can be constructed.

The approach was evaluated in a complex pick-and-place task,
where a book should be placed in several different ways. The system
was able to learn that it has to grasp the book, before it can be

placed. Further it learns that the book has to be pushed to the right
orientation such that it can be grasped from a table with a complex
sequence of in-hand manipulations (in order to get a ﬁnger below
the book). Additionally, the same set of sensing and preparatory
skills can be applied to a wide set of different problems. This was
shown by using the same preparatory skill set for putting an object
inside a closed box. It succeeded to open the box by the ﬂipping
action in order to place an object inside. Further, the convergence
of the learning approach was evaluated in simulation with more
preparatory actions. It was found that during the exploration phase,
the robot focusses on regions that are of particular interest to the
problem. Therefore it can learn a skill with high conﬁdence by just
a slightly higher number of roll-outs compared to executing every
combination only once.

ACKNOWLEDGMENT

The research leading to these results has received funding
from the European Communitys Seventh Framework Programme
FP7/20072013 (Speciﬁc Programme Cooperation, Theme 3, Infor-
mation and Communication Technologies) under grant agreement
no. 610532, Squirrel.

REFERENCES

[1] B. D. Argall, S. Chernova, M. Veloso, and B. Browning, “A
survey of robot learning from demonstration,” Robot. Auton. Syst.,
vol. 57, no. 5, pp. 469–483, May 2009.
[Online]. Available:
http://dx.doi.org/10.1016/j.robot.2008.10.024

[2] C. Smith, Y. Karayiannidis, L. Nalpantidis, X. Gratal, P. Qi, D. V.
Dimarogonas, and D. Kragic, “Dual arm manipulation – a survey,”
Robotics and Autonomous Systems, vol. 60, no. 10, pp. 1340–1353,
2012.

[3] K. M¨ulling, J. Kober, O. Kroemer, and J. Peters, “Learning to
select and generalize striking movements in robot table tennis,” The
International Journal of Robotics Research, vol. 32, no. 3, pp. 263–
279, 2013.

[4] S. Hangl, E. Ugur, S. Szedmak, A. Ude, and J. Piater, “Reactive, Task-
speciﬁc Object Manipulation by Metric Reinforcement Learning,” in
17th International Conference on Advanced Robotics, 7 2015,
to
appear.

[5] J. Piaget, The Origins of Intelligence in Children. New York: Norton,

1952.

[6] J. Morrow and P. Khosla , “Manipulation task primitives for composing
robot skills,” in IEEE International Conference on Robotics and
Automation (ICRA ’97), vol. 4, April 1997, pp. 3354–3359.

[7] M. Kopicki, J. Wyatt, and R. Stolkin, “Prediction learning in robotic
pushing manipulation,” in Advanced Robotics, 2009. ICAR 2009.
International Conference on, June 2009, pp. 1–6.

[8] C. Zito, R. Stolkin, M. Kopicki, and J. Wyatt, “Two-level rrt planning
for robotic push manipulation,” in Intelligent Robots and Systems
(IROS), 2012 IEEE/RSJ International Conference on, Oct 2012, pp.
678–685.

[9] J. Mautner, A. Makmal, D. Manzano, M. Tiersch,

H. Briegel, “Projective simulation for classical
A comprehensive
vol.
2015.
1,
http://dx.doi.org/10.1007/s00354-015-0102-0

and
learning agents:
investigation,” New Generation Computing,
[Online]. Available:

69–114,

no.

33,

pp.

[10] J. Van Den Berg, S. Patil, and R. Alterovitz, “Motion planning under
uncertainty using iterative local optimization in belief space,” Int.
J. Rob. Res., vol. 31, no. 11, pp. 1263–1278, Sept. 2012. [Online].
Available: http://dx.doi.org/10.1177/0278364912456319

[11] N. Du Toit and J. Burdick, “Robot motion planning in dynamic,
uncertain environments,” Robotics, IEEE Transactions on, vol. 28,
no. 1, pp. 101–115, Feb 2012.

[12] G. Theocharous and L. P. Kaelbling, “Approximate planning in
POMDPs with macro-actions,” in Advances in Neural Information
Processing Systems 16 (NIPS03), 2004. [Online]. Available: http:
//people.csail.mit.edu/lpk/papers/theochar-nips03.pdf

[13] V. Chu, I. Mcmahon, L. Riano, C. G. Mcdonald, Q. He, J. M. Perez-
tejada, M. Arrigo, N. Fitter, T. Darrell, and K. J. Kuchenbecker,
“Using robotic exploratory procedures to learn the meaning of haptic
adjectives,” in ICRA.

IEEE, 2013.

[14] A. Schneider, J. Sturm, C. Stachniss, M. Reisert, H. Burkhardt, and
W. Burgard, “Object identiﬁcation with tactile sensors using bag-
of-features,” in Intelligent Robots and Systems, 2009. IROS 2009.
IEEE/RSJ International Conference on, Oct 2009, pp. 243–248.

[15] A. Petrovskaya, O. Khatib, S. Thrun, and A. Y. Ng, “Bayesian
estimation for autonomous object manipulation based on tactile
sensors.” in ICRA.
IEEE, 2006, pp. 707–714. [Online]. Available:
http://dblp.uni-trier.de/db/conf/icra/icra2006.html#PetrovskayaKTN06
[16] P. Pastor, M. Kalakrishnan, F. Meier, F. Stulp, J. Buchli, E. Theodorou,

[17] A.

and S. Schaal, “From dynamic movement primitives to associative skill
memories,” Robotics and Autonomous Systems, 2012.

Jain and C. Kemp, “Improving robot manipulation with
data-driven object-centric models of everyday forces,” Autonomous
Robots, vol. 35, no. 2-3, pp. 143–159, 2013. [Online]. Available:
http://dx.doi.org/10.1007/s10514-013-9344-1

[18] S. Sen, G. Sherrick, D. Ruiken, and R. Grupen, “Choosing informative
actions for manipulation tasks,” in Humanoid Robots (Humanoids),
2011 11th IEEE-RAS International Conference on, Oct 2011, pp. 721–
726.

[19] C. M. Vigorito and A. G. Barto, “Intrinsically motivated hierarchical
skill learning in structured environments,” IEEE T. Autonomous Mental
Development, vol. 2, pp. 132–143, 2010.

application of

[20] R. E. Fikes and N. J. Nilsson, “Strips: A new approach to
the
in
Proceedings of the 2Nd International Joint Conference on Artiﬁcial
Intelligence, ser.
San Francisco, CA, USA: Morgan
Kaufmann Publishers Inc., 1971, pp. 608–620. [Online]. Available:
http://dl.acm.org/citation.cfm?id=1622876.1622939

theorem proving to problem solving,”

IJCAI’71.

[21] M. Dogar and S. Srinivasa, “A framework for push-grasping in clutter,”
in Robotics: Science and Systems VII, N. R. Hugh Durrant-Whyte and
P. Abbeel, Eds. MIT Press, July 2011.

[22] D. Omrcen, C. Boge, T. Asfour, A. Ude, and R. Dillmann, “Au-
tonomous acquisition of pushing actions to support object grasping
with a humanoid robot,” in Humanoid Robots, 2009. Humanoids 2009.
9th IEEE-RAS International Conference on, Dec 2009, pp. 277–283.
[23] S. Szedmak, J. Shawe-Taylor, and E. Parado-Hernandez, “Learning via
linear operators: Maximum margin regression,” in PASCAL Research
Reports, http://eprints.pascal-network.org/, 2005.

[24] H. Xiong, S. Szedmak, and J. Piater, “Scalable, Accurate Image
Annotation with Joint SVMs and Output Kernels,” Neurocomputing,
2015,
[Online]. Available: https://iis.uibk.ac.at/public/
papers/Xiong-2015-NEUCOM.pdf

to appear.

[25] W. Mustafa, H. Xiong, D. Kraft, S. Szedmak, J. Piater, and N. Kr¨uger,
“Multi-Label Object Categorization Using Histograms of Global Re-
lations,” in International Conference on 3D Vision.
IEEE, 10 2015,
to appear.

