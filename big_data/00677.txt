6
1
0
2

 
r
a

M
2

 

 
 
]

.

R
P
h
t
a
m

[
 
 

1
v
7
7
6
0
0

.

3
0
6
1
:
v
i
X
r
a

Karhunen–Lo`eve expansions of L´evy processes

Daniel Hackmann ∗

March 3, 2016

Abstract

Karhunen–Lo`eve expansions (KLE) of stochastic processes are important tools in mathematics, the
sciences, economics, and engineering. However, the KLE is primarily useful for those processes for
which we can identify the necessary components, i.e., a set of basis functions, and the distribution
of an associated set of stochastic coeﬃcients. Our ability to derive these components explicitly is
limited to a handful processes. In this paper we derive all the necessary elements to implement
the KLE for a square-integrable L´evy process. We show that the eigenfunctions are sine functions,
identical to those found in the expansion of a Wiener process. Further, we show that stochastic
coeﬃcients have a jointly inﬁnitely divisible distribution, and we derive the generating triple of the
ﬁrst d coeﬃcients. We also show, that, in contrast to the case of the Wiener process, the coeﬃcients
are not independent unless the process has no jumps. Despite this, we develop a series representation
of the coeﬃcients which allows for simulation of any process with a strictly positive L´evy density.
We implement our theoretical results by simulating the KLE of a variance gamma process.

1

Introduction

√

2

Wt =

sin(cid:0)π(k − 1
2t)(cid:1)
π(cid:0)k − 1
(cid:1)

2

(cid:88)

k≥1

Zk

Fourier series are powerful tools in mathematics and many other ﬁelds. The Karhunen-Lo`eve theorem
(KLT) allows us to create generalized Fourier series from stochastic processes in an, in some sense, optimal
way. Arguably the most famous application of the KLT is to derive the classic sine series expansion of
a Wiener process W on [0, 1]. Speciﬁcally,

(1.1)

where convergence of the series is in L2(Ω, P) and uniform in t ∈ [0, 1], and the {Zk}k≥1 are i.i.d. standard
normal random variables. The main result of this paper is to show that a square integrable L´evy process
admits a similar representation as a series of sine functions; the key diﬀerence is that the stochastic
coeﬃcients are no longer normal nor independent.

The KLT applies much more generally and is thus an important tool in many ﬁelds. For example,
we see applications of the KLT and Principal Component Analysis, its discrete time counterpart, in
physics and engineering [8, 19], [16, Chapter 10], in signal and image processing [26], [7, Chapter 1], in
∗Institute of Financial Mathematics and Applied Number Theory, Johannes Kepler University, Linz, Austria. E-mail:

daniel.hackmann@jku.at

1

ﬁnance and economics [2, 5, 13] and other areas. For interesting recent theses on the KLT from three
diﬀerent points of view see also [10] (probability and time series), [15, Chapter 7] (stochastic partial
diﬀerential equations), and [27] (statistics).

(cid:90) b

Deriving the Karhunen–L`oeve expansion (KLE) of the type (1.1) for a square integrable stochastic pro-
cess X on [a, b] requires two steps: ﬁrst, one must solve a Fredholm integral equation to obtain the basis
functions {ek}k≥1 (c.f. the sine functions in Equation 1.1). Second, one must identify the distribution of
the stochastic coeﬃcients

Zk :=

Xtek(t)dt,

k ∈ N.

(1.2)

a

In general, obtaining both the basis functions and the distribution of the stochastic coeﬃcients is not
an easy task, and we have full knowledge in only a few speciﬁc cases. Besides the Wiener process, the
Brownian Bridge process, the Anderson–Darling process, and spherical fractional Brownian Motion (see
[9] for the latter) are some examples. For further examples with derivation see [10, Chapter 1]. Non-
Gaussian processes pose an additional challenge and the problem of deriving the KLE is usually left to
numerical means (see e.g., [19]).

In this paper we derive all the elements of the KLE for a square integrable L´evy process on the in-
terval [0, T ]. The result is timely since in many of the ﬁelds mentioned above, especially in ﬁnance, but
recently also in the area of image/signal processing (see e.g., [25]), L´evy models are becoming increas-
ingly popular. In Section 3 we show that the basis functions are sine functions, identical to those in
(1.1), and that the ﬁrst d stochastic coeﬃcients are jointly distributed like an inﬁnitely divisible (ID)
random vector. We identify the generating triple of this vector from which it follows that the coeﬃcients
are independent only when the process has no jumps, i.e., when the process is a scaled Wiener process
with drift. Although simulating dependent multivariate random variables from a characteristic function
is generally diﬃcult, in Section 4 we derive a shot-noise (series) representation for

Z (d) := (Z1, Z2, . . . , Zd)T,

d ∈ N,

(1.3)

for those processes which admit a strictly positive L´evy density. This result, in theory, allows us to
simulate the truncated KLE for a large class of L´evy models. We conclude by generating some paths of
a d-term KLE approximation of a variance gamma process.

To begin, we recall the necessary facts from the theory of L´evy processes and ID random vectors.

2 Facts from the theory of L´evy processes

The L´evy-Khintchine theorem states that every d-dimensional ID random vector ξ has a Fourier transform
of the form

where

Ψ(z) =

1
2

zTQz − ı(cid:104)a, z(cid:105) −

eı(cid:104)z,x(cid:105) − 1 − ı(cid:104)z, x(cid:105)h(x)ν(dx),

(2.1)

E[eı(cid:104)z,ξ(cid:105)] = e−Ψ(z),

z ∈ Rd,

(cid:90)

Rd\{0}

2

and where a ∈ Rd, Q is a positive semi-deﬁnite matrix, and ν(dx) is a measure on Rd\{0} satisfying

min(1,|x|2)ν(dx) < ∞.

(2.2)

(cid:90)

Rd\{0}

The function h is known as the cut-oﬀ function; in general, we need such a function to ensure convergence
of the integral. An important fact is that up to a choice of h, the generating triple (a, Q, ν) uniquely
identiﬁes the distribution of ξ. The L´evy-Khintchine theorem for L´evy processes gives us an analogously
powerful result, speciﬁcally, for any d-dimensional L´evy process X we have

E[ei(cid:104)z,Xt(cid:105)] = e−tΨ(z),

z ∈ Rd, t ≥ 0,

where Ψ is as in (2.1) and X is uniquely determined, up to identity in distribution, by the triple (a, Q, ν).
Following convention, we will refer to the function Ψ as the characteristic exponent of ξ (resp. X) and will
write Ψξ (resp. ΨX) if there is the potential for ambiguity. In one dimension we will write (a, σ2, ν) for the
generating triple; the measure ν will always be referred to as the L´evy measure. When ν(dx) = π(x)dx
for some density function π, we will write (a, σ2, π) and refer to π as the L´evy density. If we wish to
be speciﬁc regarding the cut-oﬀ function we will write (a, Q, ν)h≡· or (a, σ2, ν)h≡· for the generating triples.

In this article we will work primarily with one dimensional L´evy processes having zero mean and ﬁ-
t ] < ∞ for every t ≥ 0. We will denote
nite second moment; by this we mean that E[Xt] = 0 and E[X 2
the set of all such L´evy processes by K. One may show that the later condition implies that Ψ is twice
diﬀerentiable. Thus, when we work with a process X ∈ K, we can express the variance of Xt as

Var(Xt) = E[X 2

t ] = Ψ(cid:48)(cid:48)(0)t,

and the covariance of Xt and Xs as

For notational convenience we will set α := Ψ(cid:48)(cid:48)(0).

Cov(Xs, Xt) = E[XsXt] = Ψ(cid:48)(cid:48)(0) min(s, t).

The existence of moments for both L´evy processes and ID random vectors can be equivalently expressed
in terms of the L´evy measure. An ID random vector ξ or L´evy process X with associated L´evy measure
ν has a ﬁnite second moment (meaning the component-wise moments) if, and only if,

(Condition A)
We will denote the class of ID random vectors with zero ﬁrst moment and ﬁnite second moment by C.
The subset of C which also satisﬁes

|x|>1

|x|2ν(dx) < ∞.

(Condition B)
will be denoted CB and KB will denote the analogous subset of K. We remark that any ξ ∈ C (resp.
X ∈ K) necessarily has a representation of the form (0, Q, ν)h≡1 (resp. (0, σ2, ν)h≡1). Additionally, any
d-dimensional ξ ∈ CB necessarily has representation (a, Q, ν)h≡0 where a has entries

|x|≤1

|x|ν(dx) < ∞.

(cid:90)

(cid:90)

and Pk is the projection onto the k-th component. Analogously, if X ∈ KB then we have representation

(a, σ2, ν)h≡0 where a = −(cid:82)

R\{0} xν(dx).

Pk(x)ν(dx),

k ∈ {1, 2, . . . d}

(cid:90)

−

Rd\{0}

3

3 The Karhunen–Lo`eve theorem

Given a real valued continuous time stochastic process X deﬁned on an interval [a, b] and an orthonormal
basis {φk}k≥1 for L2([a, b]) we might try to express X as a generalized Fourier series

∞(cid:88)

(cid:90) b

Xt =

Ykφk(t),

where

Yk :=

Xtφk(t)dt.

(3.1)

k=1

a

In this section, our chosen basis will be derived from the eigenfunctions corresponding to the non-zero
eigenvalues {λk}k≥1 of the integral operator K : L2([a, b]) → L2([a, b]),

(cid:90) b

(Kf )(s) :=

Cov(Xs, Xt)f (t)dt.

a

When the covariance satisﬁes a continuity condition it is known (see for example [8] Section 2.3.3) that
the normalized set of eigenfunctions {ek}k≥1 of K is countable and forms a basis for L2([a, b]). When we
choose this basis in (3.1) we adopt the special notation {Zk}d≥1 for the stochastic coeﬃcients. In this
case, the expansion is optimal in a number of ways. Speciﬁcally, we have:

Theorem 1 (The Karhunen-Lo`eve Theorem). Let X be a real valued continuous time stochastic process
on [a, b] such that 0 ≤ a ≤ b < ∞ and let E[Xt] = 0 and E[X 2
t ] < ∞ for each t ∈ [a, b]. Further, suppose
Cov(Xs, Xt) is continuous on [a, b] × [a, b].

(i) Then,

E

(cid:32)

Xt − d(cid:88)

k=1

(cid:33)2 → 0,

Zkek(t)

as

d → ∞

uniformly for t ∈ [a, b]. Additionally, the random variables {Zk}k≥1 are uncorrelated and satisfy
E[Zk] = 0 and E[Z 2

k] = λk.

(ii) For any other basis {φk}k≥1 of L2([a, b]), with corresponding stochastic coeﬃcients {Yk}k≥1, and

any d ∈ N, we have

E(cid:2)(εd(t))2(cid:3) dt ≤
where εd and ˜εd are the remainders εd(t) :=(cid:80)∞

(cid:90) b

E(cid:2)(˜εd(t))2(cid:3) dt,

(cid:90) b
d+1 Zkek(t) and ˜εd(t) :=(cid:80)∞

a

a

d+1 Ykφk(t).

Going forward we assume the order of the eigenvalues, eigenfunctions, and the stochastic coeﬃcients is
determined according to λ1 ≥ λ2 ≥ λ3, . . ..

According to Ghanem and Spanos [8] the Karhunen-Lo`eve theorem was proposed independently by
Karhunen [12], Lo`eve [14], and Kac and Siegert [11]. Modern proofs of the ﬁrst part of the theorem can
be found in [1] and [8] and the second part – the optimality of the truncated approximation – is also
proven in [8]. A concise and readable overview of this theory is given in [15, Chapter 7.1].

We see that although the KLT is quite general, it is best applied in practice when can determine the

4

three components necessary for a Karhunen-Lo´eve expansion: the eigenfunctions {ek}k≥1; the eigenval-
ues {λk}k≥1; and the distribution of the stochastic coeﬃcients {Zk}k≥1.
If we wish to use the KLE
for simulation then we need even more: We also need to know how to simulate the random vector
Z (d) = (Z1, Z2, . . . , Zd) which, in general, has uncorrelated but not necessarily independent components.
For Gaussian processes, the second obstacle is removed, since one can show that the {Zk}k≥1 are again
Gaussian, and therefore independent. There are, of course, many ways to simulate a vector of indepen-
dent Gaussian random variables. For a process X ∈ K, the matter is slightly more complicated as we
establish in Theorem 2. However, since the covariance function of a process X ∈ K diﬀers from that of
a Wiener process only by the scaling factor α, the method for determining the eigenfunctions and the
eigenvalues for a L´evy process is identical to that employed for a Wiener process. Therefore, we omit the
proof of the following proposition, and direct the reader to [1, pg. 41] where the proof for the Wiener
process is given.

Proposition 1. The eigenvalues and associated eigenfunctions of the operator K deﬁned on L2([0, T ])
with respect to X ∈ K are given by

(cid:19)

(cid:19)

t

,

(cid:114) 2

T

(cid:18)

(cid:18) π

T

2

sin

and

αT 2

λk =

ek(t) =

(cid:1)2 ,

k − 1
2

π2(cid:0)k − 1
variance v(T ) :=(cid:82) T
0 Var(Xt)dt =(cid:82) T
straightforward to show that the total variance satisﬁes v(T ) =(cid:80)
explained by a d-term approximation is(cid:80)d
(cid:1)2 .
(cid:0)k − 1

k=1 λk
v(T )

d(cid:88)

2
π2

=

1

k=1

2

A nice consequence of Proposition 1 and Theorem 1 is that it allows us to estimate the amount of total
t ]dt = αT 2/2 we capture when we represent our process by a
truncated KLE. Using the orthogonality of the {ek}k≥1, and the fact that E[Z 2
k] = λk for each k, it is
k≥1 λk. Therefore, the total variance

E[X 2

0

k ∈ N, t ∈ [0, T ].

(3.2)

By simply computing the quantity on the right we ﬁnd that the ﬁrst 2, 5 and 21 terms already explain
90%, 95%, and 99% of the total variance of the process. Additionally, we see that this estimate holds
for all X ∈ K independently of α or T .

The following lemma is the important ﬁrst step in identifying the joint distribution of the stochastic
coeﬃcients of the KLE for X ∈ K. The reader should note, however, that the lemma applies to more
general L´evy processes, and is not just restricted to the set K.
Lemma 1. Let X be a L´evy process and let {fk}d
Then the vector ξ consisting of elements

k=1 be a collection of functions which are in L1([0, T ]).

ξk =

Xtfk(s)ds,

k ∈ {1, 2, . . . , d},

has an ID distribution with characteristic exponent

where u : [0, T ] → Rd is the function with k-th component uk(t) :=(cid:82) T

Ψξ(z) =

ΨX ((cid:104)z, u(t)(cid:105)) dt,

z ∈ Rd,

0

t fk(s)ds, k ∈ {1, 2, . . . , d}.

(3.3)

(cid:90) T

0

(cid:90) T

5

Remark 1. A similar identity to (3.3) is known, see pg. 128 in [3]. In the proof of Lemma 1, we borrow
some ideas from there. Since the proof is rather lengthy we relegate it to the Appendix.

With Lemma 1 and Proposition 1 in hand, we come to our ﬁrst main result. In the following theorem we
identify the generating triple of the vector Z (d) containing the ﬁrst d stochastic coeﬃcients of the KLE
for a process X ∈ K. Although it follows that Z (d) has dependent entries (see Corollary 2), Theorem 2,
and in particular the form of the L´evy measure Π, will also be the key to simulating Z (d). Going forward
we use the notation BS for the Borel sigma algebra on the topological space S.
Theorem 2. If X ∈ K with generating triple (0, σ2, ν)h≡1 then Z (d) ∈ C with generating triple
(0,Q, Π)h≡1 where Q is a diagonal d × d matrix with entries

π2(cid:0)k − 1

T 2

2

(cid:1)2 ,

σ2
2

k ∈ {1, 2, . . . , d},

and Π is the measure,

qk,k :=

R\{0}×[0,T ]

(cid:90)
cos(cid:0) π
(cid:0)1 − 1
(cid:0)1 − 1
(cid:1)

T

2

2

Π(B) :=

(cid:32)

√

2T x
π

(x, t) (cid:55)→

I(f (v) ∈ B)(ν × λ)(dv), B ∈ BRd\{0},

(cid:1) t(cid:1)

,

(cid:0)2 − 1
cos(cid:0) π
(cid:0)2 − 1
(cid:1)

T

2

2

(cid:1) t(cid:1)

, . . . ,

(cid:0)d − 1
cos(cid:0) π
(cid:0)d − 1
(cid:1)

T

2

2

where λ is the Lebesgue measure on [0, T ] and f : R × [0, T ] → Rd is the function

(3.4)

(3.5)

(cid:33)T

(cid:1) t(cid:1)

.

(3.6)

Proof. We substitute the formula for the characteristic exponent (Formula 2.1 with a = 0 and h ≡ 1)
and the eigenfunctions (Formula 3.2) into (3.3) and carry out the integration. Then (3.4) follows from
the fact that

(cid:114) 2

(cid:90) T

T

t

(cid:19)

(cid:19)

(cid:18)

(cid:18) π

T

k − 1
2

√
2T

s

ds =

cos(cid:0) π

(cid:0)k − 1

(cid:1) t(cid:1)

T

2
π(k − 1
2)

uk(t) =

ek(s)ds =

sin

(cid:90) T

t

k ∈ N

,

and that the {uk}k≥1 are therefore also orthogonal on [0, T ].

Next we note that f is a continuous function from R\{0}×[0, T ] to Rd and is therefore(cid:0)BR\{0}×[0,T ],BRd\{0}(cid:1)

measurable. Therefore, Π is nothing other than the push forward measure obtained from (ν × λ) and f ;
in particular, it is a well-deﬁned measure on BRd\{0}. It is also a L´evy measure that satisﬁes Condition
A since

|x|2Π(dx) ≤

|x|2Π(dx) =

2T
π2

Rd\{0}

|x|>1

u2
k(t)

dt

R\{0}

x2ν(dx) < ∞,

(3.7)

where the ﬁnal inequality follows from the fact that X ∈ K. Applying Fubini’s theorem and a change of
variables, i.e.,

(cid:33)

(cid:90)

(cid:90)

eıx(cid:104)z,u(t)(cid:105) − 1 − ıx(cid:104)z, u(t)(cid:105)ν(dx)dt =

=

eı(cid:104)z,f (v)(cid:105) − 1 − ı(cid:104)z, f (v)(cid:105)(ν × λ)(dv)

eı(cid:104)z,x(cid:105) − 1 − ı(cid:104)z, x(cid:105)Π(dx),

(cid:90)

(cid:90) T

(cid:90)

0

R\{0}

(cid:90) T

(cid:32) d(cid:88)

0

k=1

(cid:90)
(cid:90)

R\{0}×[0,T ]

Rd\{0}

6

concludes the proof of inﬁnite divisibility. Finally, noting that

(cid:20)(cid:90) T

(cid:21)

(cid:90) T

E[Zk] = E

shows that Z (d) ∈ C.

Xtek(t)dt

=

E[Xt]ek(t)dt = 0,

k ∈ {1, 2, . . . , d},

0

0

(cid:117)(cid:116)

Remark 2. Note, that if we set σ = 1, ν ≡ 0, and T = 1 we may easily recover the KLE of the Wiener
process, i.e., (1.1), from Theorem 2.

We gather some fairly obvious but important consequences of Theorem 2 in the following corollary.
Corollary 1. Suppose X ∈ K, then:
(i) X ∈ KB with generating triple (a, σ2, ν)h≡0 if, and only if, Z (d) ∈ CB with generating triple

(a,Q, Π)h≡0, where Q and Π are as deﬁned in (3.4) and (3.5) and a is the vector with entries

√

(−1)k+1

π2(cid:0)k − 1

(cid:1)2

2T 3

2

2

ak := a

k ∈ {1, 2, . . . , d}.

,

(3.8)

(ii) X has ﬁnite L´evy measure ν if, and only if, Z (d) has ﬁnite L´evy measure Π.

and Condition A is satisﬁed by both ν and Π it follows that Condition B is satisﬁed for ν if, and only if,
it is satisﬁed for Π. Formula 3.8 then follows from the fact that

|x|ν(dx)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) d(cid:88)
(cid:90) T

k=1

0

R\{0}

uk(t)

(cid:90)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) dt
cos(cid:0) π
(cid:0)k − 1
(cid:0)k − 1
(cid:1)

T

2

2

(cid:1) t(cid:1)

√
2T
π

√
2T 3

(−1)k+1

π2(cid:0)k − 1

(cid:1)2

2

2

dt = a

.

(cid:117)(cid:116)

Proof.
(i) Since

(cid:90)

Rd\{0}

|x|Π(dx) =

(cid:90) T

0

√

2T
π

(cid:90)

−

(cid:90)

Pk(x)Π(dx) = −

Rd\{0}

xν(dx)

R\{0}

(ii) Straightforward from the deﬁnition of Π in Theorem 2.

Also intuitively obvious, but slightly more diﬃcult to establish rigorously, is the fact that the entries of
Z (d) are dependent unless ν ≡ 0.
Corollary 2. If X ∈ K then Z (d) has independent entries if, and only if, ν is the zero measure.
To prove Corollary 2 we use the fact that a d-dimensional ID random vector with generating triple
(a, Q, ν) has independent entries if, and only if, ν is supported on the union of the coordinate axes and Q
is diagonal (see E 12.10 on page 67 in [23]). For this purpose we deﬁne, for a vector x = (x1, x2, . . . , xd)T ∈
Rd such that xk > 0, k ∈ {1, 2, . . . , d}, the sets
k=1(xk,∞),

and I−(x) := Πd

k=1(−∞,−xk),

I +(x) := Πd

where we caution the reader that the symbol Π indicates the Cartesian product and not the L´evy mea-
sure of Z (d).

In the proof below, and throughout the remainder of the paper, f will always refer to the function
deﬁned in (3.6), and fk to the k-th coordinate of f .

7

Proof of Corollary 2.
(⇐) The assumption ν ≡ 0 implies our process is a scaled Wiener process in which case it is well estab-
lished that Z (d) has independent entries. Alternatively, this follows directly the fact that the matrix Q
in Theorem 2 is diagonal.
(⇒) We assume that ν is not identically zero and show that there exists x such that either Π(I +(x)) or
Π(I−(x)) is strictly greater than zero.
Since ν(R\{0}) > 0 there must exist δ > 0 such that one of ν((−∞,−δ)) and ν((δ,∞)) is strictly
greater than zero; we will initially assume the latter. We observe that for d ∈ N, d ≥ 2, the zeros of the
function hd : [0, T ] → R deﬁned by

(cid:0)d − 1
t (cid:55)→ cos(cid:0) π
(cid:1)
(cid:0)d − 1

T

2

2

(cid:1) t(cid:1)

,

occur at points {nT /(2d − 1)}2d−1
that the cosine function is positive and decreasing on [0, π/2] we may conclude that

n=1 , and therefore the smallest zero is td := T /(2d − 1). From the fact

where  = hd(td/2) > 0. Now, let x be the vector with entries xk = δ

2T /π for k ∈ {1, 2, . . . , d}. Then,

2

> ,

T

2

k ∈ {1, 2, . . . , d},

cos(cid:0) π
(cid:0)k − 1
(cid:1) t(cid:1)
(cid:1)
(cid:0)k − 1
(δ,∞) × [0, td/2] ⊂ f−1(cid:0)I + (x)(cid:1) ,
cos(cid:0) π
(cid:0)k − 1
(cid:0)k − 1
(cid:1)

√
2T
π

√
2T
π

(cid:1) t(cid:1)

= xk,

T

2

> δ

fk(x, t) =

x

2

since for (x, t) ∈ (δ,∞) × [0, td/2] we have

t ∈ [0, td/2],
√

k ∈ {1, 2, . . . , d}.

But then,

Π(I +(x)) ≥ ν((δ,∞)) × λ([0, td/2]) > 0.

(3.9)
If we had initially assumed that ν((−∞,−δ)) > 0 we would have reached the same conclusion by using
the interval (−∞,−δ) and I−(x). We conclude that Π is not supported on the union of the coordinate
(cid:117)(cid:116)
axes, and so Z (d) does not have independent entries.

4 Shot-noise representation of Z (d)

Although we have characterized the distribution of our stochastic coeﬃcients Z (d) we are faced with the
problem of simulating a random vector with dependent entries with only the knowledge of the charac-
teristic function. In general, this seems to be a diﬃcult problem, even generating random variables from
the characteristic function is not straightforward (see for example [6]). In our case, thanks to Theorem
2 we know that Z (d) is inﬁnitely divisible and that the L´evy measure Π has a special disintegrated form.
This will help us build the connection with the so-called shot-noise representation of our vector Z (d).

8

The goal is to represent Z (d) as an almost surely convergent series of random vectors.

To explain this theory – nicely developed and explained in [20, 21] – we assume that we have two
random sequences {Vi}i≥1 and {Γi}i≥1 which are independent of each other and deﬁned on a common
probability space. We assume that each Γi is distributed like a sum of i independent exponential random
variables with mean 1, and that the {Vi}i≥1 take values in a measurable space D, and are i.i.d. with
common distribution F . Further, we assume we have a measurable function H : (0,∞)× D → Rd which
we use to deﬁne the random sum

n(cid:88)

Sn :=

H(Γi, Vi), n ∈ N,

i=1

I(H(r, v) ∈ B)F (dv)dr, B ∈ BRd\{0}.

and the measure

µ(B) :=

The function C : (0,∞) → Rd is deﬁned by

(cid:90)
(cid:90) ∞
(cid:90)
(cid:90) s

0

D

(4.1)

(4.2)

(4.3)

Ck(s) :=

Pk(H(r, v))F (dv)dr,

k ∈ {1, 2, . . . , d},

0

D

where, as before, Pk is the projection onto the k-th component. The connection between (4.1) and ID
random vectors is then explained in the following theorem whose results can be obtained by restricting
Theorems 3.1, 3.2, and 3.4 in [20] from a general Banach space setting to Rd.

Theorem 3 (Theorems 3.1, 3.2, and 3.4 in [20]). Suppose µ is a L´evy measure, then:

(i) If Condition B holds then Sn converges almost surely to an ID random vector with generating triple

(0, 0, µ)h≡0 as n → ∞.

(ii) If Condition A holds, and for each v ∈ S the function r → |H(r, v)| is non increasing, then

Mn := Sn − C(n), n ∈ N

(4.4)

converges almost surely to an ID random vector with generating triple (0, 0, µ)h≡1.

The name “shot-noise representation” comes from the idea that |H| can be interpreted as a model for
If |H| is non increasing in the
the volume of the noise of a shot Vi that occurred Γi seconds ago.
ﬁrst variable, as we assume in case (ii) in Theorem 3, then the volume decreases as the elapsed time
grows. The series limn→∞ Sn can be interpreted as the total noise at the present time of all previous shots.
The goal is to show that for any process in K whose L´evy measure admits a strictly positive den-
sity π, the vector Z (d) has a shot-noise representation of the form (4.1) or (4.4). To simplify notation we
make some elementary but necessary observations/assumptions: First, we assume that X has no Gaus-
sian component σ2. There is no loss of generality to this assumption, since if X does have a Gaussian
component then Z (d) changes by the addition of a vector of independent Gaussian random variables.
This poses no issue from a simulation standpoint. Second, from (2.1) we see that any L´evy process X
with representation (0, 0, π)h≡j, j ∈ {0, 1} can be decomposed into the diﬀerence of two independent

9

(cid:90)
(cid:90) ∞

R\{0}

ΨX(z) = −

= −

L´evy processes, each having only positive jumps. Indeed, splitting the integral and making a change of
variable x (cid:55)→ −x gives

eızx − 1 − ızxjπ(x)dx

eızx − 1 − ızxjπ(x)dx −

(cid:90) ∞

0

eız(−x) − 1 − ız(−x)jπ(−x)dx

(4.5)

0

= ΨX +(z) + Ψ−X−(z)

where X + (resp. X−) has L´evy density π(·) (resp. π(−·)) restricted to (0,∞).
In light of this ob-
servation, the results of Theorem 4 are limited to L´evy processes with positive jumps.
It should be
understood that for a general process we can obtain Z (d) by simulating Z (d)
+ and Z (d)− – corresponding
to X + and X− respectively – and then subtracting the second from the ﬁrst to obtain a realization of Z (d).

Last, for a L´evy process with positive jumps and strictly positive L´evy density π, we deﬁne the function

g(x) :=

π(s)ds.

(4.6)

which is just the tail integral of the L´evy measure. We see that g is strictly monotonically decreasing to
zero, and so admits a strictly monotonically decreasing inverse g−1 on the domain (0, g(0)).
Theorem 4. Let π be a strictly positive L´evy density on (0,∞) and identically zero elsewhere.
(i) If X ∈ KB with generating triple (a, 0, π)h≡0, then Z (d) has a shot noise representation

Z (d) d= a +

H(Γi, Ui)

(4.7)

where f and a are deﬁned in (3.6) and (3.8) respectively, {Ui}i≥1 is an i.i.d. sequence of uniform
random variables on [0, 1], and

H(r, v) := f (g−1(r/T )I(0 < r < g(0)), T v).

(ii) If X ∈ K with generating triple (0, 0, π)h≡1, then Z (d) has a shot noise representation

(cid:90) ∞

x

(cid:88)

i≥1

n(cid:88)

i=1

(4.8)

(4.9)

Z (d) d= lim
n→∞

H(Γi, Ui) − C(n),

where H and {Ui}i≥1 are as in Part (i) and C is deﬁned as in (4.3).

Proof. Rewriting (3.5) to suit our assumptions and making a change of variables t = T v gives, for any
B ∈ BRd\{0}

Π(B) =

I(f (x, t) ∈ B)π(x)dxdt =

I(f (x, T v) ∈ B)T π(x)dxdv.

(cid:90) T

(cid:90) ∞

0

0

(cid:90) 1

(cid:90) ∞

0

0

Making a further change of variables r = T g(x) gives

Π(B) =

I(f (g−1(r/T ), T v) ∈ B)drdv.

(cid:90) 1

(cid:90) g(0)

0

0

10

Since 0 /∈ B, so that I(0 ∈ B) = 0, we may conclude that

(cid:90) ∞

(cid:90) 1

Π(B) =

I(f (g−1(r/T )I(0 < r < g(0)), T v) ∈ B)dvdr.

From the deﬁnition of the function f (Formula 3.6), and that of g−1, it is clear that

0

0

(r, v) (cid:55)→ f (g−1(r/T )I(0 < r < g(0)), T v)

(4.10)

is measurable and non increasing in absolute value for any ﬁxed v. Therefore, we can identify (4.10)
with the function H, the uniform distribution on [0, 1] with F , and Π with µ. The results then follow
(cid:117)(cid:116)
by applying the results of Theorems 2 and 3 and Corollary 1.

Going forward we will write simply H(r, v) = f (g−1(r/T ), T v) where it is understood that g−1 vanishes
outside the interval (0, g(0)).

Discussion

There are two fairly obvious diﬃculties with the series representations of Theorem 4. The ﬁrst – this
a common problem for all series representations of ID random variables when the L´evy measure is not
ﬁnite – is that we have to truncate the series when g(0) = ∞ (equivalently ν(R\{0}) = ∞). Besides
the fact that in these cases our method fails to be exact, computation time may become an issue if the
series converge too slowly. The second issue is that g−1 is generally not known in closed form. Thus,
in order to apply the method we will need a function g that is amenable to accurate and fast numerical
inversion. In the survey [20] Rosi´nski reviews several methods, which depend on various properties of
the L´evy measure (for example, absolute continuity with respect to a probability distribution), that
avoid this inversion. In a subsequent paper [22] he develops special methods for the family of tempered
α-stable distributions that also do not require inversion of the tail of the L´evy measure. We have made
no attempt to adapt these techniques here, as the fall outside of the scope of this paper. However, this
seems to be a promising area for further research.
A nice feature of simulating a d-dimensional KLE of a L´evy process X ∈ K via Theorem 4 is that
we may increase the dimension incrementally. That is, having simulated a path of the d-term KLE
approximation of X,

d(cid:88)

k=1

S(d)
t

:=

Zkek(t),

t ∈ [0, T ],

(4.11)

we may derive a path of S(d+1) directly from S(d) as opposed to starting a fresh simulation. We observe
that a realization zk of Zk can be computed individually once we have the realizations {γi, ui}i≥1 of
{Γi, Ui}i≥1. Speciﬁcally,

(cid:88)

√
2T g−1(γi/T )

zk = ak +

i≥1

π

cos(cid:0)π(cid:0)k − 1
(cid:0)k − 1
(cid:1)

2

2

(cid:1)

(cid:1) ui

,

when Condition B holds, with an analogous expression when it does not. Thus, if s(d)
of S(d)

t we get a realization of S(d+1)

t + zd+1ed+1(t).

t

via s(d+1)

= s(d)

t

t

is our realization

11

It is also worthwhile to compare the series representations for L´evy processes found in [20] and the
proposed method. As an example, suppose we have a subordinator X with a strictly positive L´evy
density π. Then, it is also true that

(cid:41)

(cid:40)(cid:88)

i≥1

{Xt : t ∈ [0, T ]} d=

g−1(Γi/T )I(T Ui < t) : t ∈ [0, T ]

.

(4.12)

The key diﬀerence between the approaches, is that the series in (4.12) depends on t, whereas the series
representation of Z (d) is independent of t. Therefore, in (4.12) we have to recalculate the series for each
t, adding those summands for which UiT < t. Of course, the random variables {Γi, Ui}i≥1 need to be
generated only once. On the other hand, while we have to simulate Z (d) only once for all t, each summand
requires the evaluation of d cosine functions, and for each t we have to evaluate d sine functions when
we form the KLE. However, since there is no more randomness once we have generated Z (d) the second
computation can be done in advance.

Example

Consider the Variance Gamma (VG) process which was ﬁrst introduced in [17] and has since become a
popular model in ﬁnance. The process can be constructed as the diﬀerence of two independent Gamma
processes, i.e., processes with L´evy measures of the form

ν(dx) = c

e−ρx
x

dx,

x > 0,

(4.13)

where c, ρ > 0. For this example we use a Gamma process X + with parameters c = 1 and ρ = 1 and
subtract a Gamma process X− with parameters c = 1 and ρ = 2 to yield a VG process X. Assuming
no Gaussian component or additional linear drift, it can be shown (see Proposition 4.2 in [24]) that the
characteristic exponent of X is then
e−x
x
We observe that X +, X− /∈ K since

= log (1 − ız) + log

(cid:18)(cid:90) ∞

ΨX(z) = −

(e−ızx − 1)

(eızx − 1)

(cid:90) ∞

1 +

.

e−2x
x

dx

(cid:17)

ız
2

dx +

0

(cid:19)

(cid:16)

0

E[X +

t ] = ıtΨ(cid:48)

X +(0) = t (cid:54)= 0

and E[X−

t ] = ıtΨ(cid:48)

X−(0) =

(cid:54)= 0.

t
2

However, this is not a problem, since we can always construct processes ˜X +, ˜X− ∈ K by subtracting t
and t/2 from X + and X− respectively. We then generate the KLE of ˜X + and add back t to the result,
and apply the analogous procedure for X−. This is true generally as well, i.e., for a square integrable
X(0) (cid:54)= 0 we can always construct a process ˜X ∈ K by simply
L´evy process with expectation E[Xt] = ıtΨ(cid:48)
subtracting the expectation ıtΨ(cid:48)
(cid:90) ∞

From (4.13) we see that the function g will have the form

X(0).

g(x) = c

ds = cE1(ρx),

x

e−ρs
s

12

where E1(x) :=(cid:82) ∞

x s−1e−sds is the exponential integral function. Therefore,

(cid:16) r

(cid:17)

.

T c

g−1(T −1r) =

E−1

1

1
ρ

There are many routines available to compute E1; we choose a Fortran implementation to create a lookup
1 with domain [6.226× 10−22, 45.47]. We discretize this domain into 200000 unevenly spaced
table for E−1
points, such that the distance between two adjacent points is no more than 0.00231. Then we use poly-
nomial interpolation between points.

+ we truncate the series (4.7) when (T c)−1Γi > 45.47; at this point we have
When simulating Z (d)
g−1(T −1Γi) < ρ−110−19. Using the fact that the {Γi}i≥1 are nothing other than the waiting times of
a Poisson process with intensity one, we estimate that we need to generate on average 45T c random
variables to simulate Z (d)
+ and similarly for Z (d)− . We remark that for the chosen process both the decay
and computation of g−1 are manageable.
We simulate sample paths of S(d) for d ∈ {5, 10, 15, 20, 25, 100, 3000} using the described approach.
We also compute a Monte Carlo (MC) approximation of the expectation of X by averaging over 106
sample paths of the d-term approximation. Some sample paths and the results of the MC simulation are
depicted in Figure 1, where the colors black, grey, red, green, blue, cyan, and magenta correspond to d
equal to 5, 10, 15, 20, 25, 100, and 3000 respectively.

In Figure 1a we show the sample paths resulting from a simulation of S(d). We notice that the nu-
merical results correspond with the discussion of Section 3: the large movements of the sample path
are already captured by the 5-term approximation. We also notice peaks resulting from rapid oscilla-
tions before the bigger “jumps” in the higher term approximations. This behaviour is magniﬁed for
the 3000-term approximation in Figure 1b. In classical Fourier analysis this is referred to as the Gibbs
phenomenon; the solution in that setting is to replace the partial sums by Ces`aro sums. We can employ
the same technique here, replacing S(d) with C (d), which is deﬁned by

d(cid:88)

k=1

C (d)

t

:=

1
d

S(k)
t

.

It is relatively straightforward to show that C (d) converges to X in the same manner as S(d) (as described
in Theorem 1 (i)). In Figure 1c we show the eﬀect of replacing S(d) with C (d) on all sample paths, and
in Figure 1d we show the C (3000) approximation – now the Gibbs phenomenon is no longer apparent.

In Figure 1e we show the MC simulation of E[S(5)
] (black +) plotted together with E[Xt] = t/2 (green
◦). We see the 5-term KLE already gives a very good approximation. In Figure 1f we also show the
] − E[Xt] for d = 5 (black +), d = 25 (blue ◦), and d = 3000 (magenta (cid:3)). Again we
errors E[S(d)
have agreement with the discussion in Section 3: little is gained in our MC approximation of E[Xt] by
choosing a KLE with more than 25 terms. Recall that a KLE with 25 terms already captures more than
99% of the total variance of the given process.

t

t

13

(a)

(c)

(e)

(b)

(d)

(f)

Figure 1: (a) KLE sample paths (b) Example of Gibbs phenomenon (c) KLE with Ces`aro sums
(d) Mitigated Gibbs phen. (e) E[Xt] = t/2 and MC sim. of E[S(5)

] (f) MC Err. E[S(d)

t

] − t/2

t

14

Author’s acknowledgements

My work is supported by the Austrian Science Fund (FWF) under the project F5508-N26, which is part
of the Special Research Program “Quasi-Monte Carlo Methods: Theory and Applications”.
I would
like to thank Jean Bertoin for explaining his results in [3] to me. This helped me extend identity (3.3)
of Lemma 1 from C 1 functions to L1 functions. Further I would like to thank Alexey Kuznetsov and
Gunther Leobacher for reading a draft of this paper and oﬀering helpful suggestions.

15

Appendix A Additional proof
Proof of Lemma 1. We give a proof for continuously diﬀerentiable {fk}d
k=1 ﬁrst and then prove the
general case. Accordingly, we ﬁx z ∈ Rd, a collection of continuously diﬀerentiable {fk}d
k=1 deﬁned on
[0, T ], and a L´evy process X with state space R. Instead of proving identity (3.3) directly for X we will
prove that

(cid:90) T

(cid:90) T

Ψξ(b)(z) =

ΨX (b) ((cid:104)z, u(t)(cid:105)) dt = b

0

0

ΨX ((cid:104)z, u(t)(cid:105)) dt,

z ∈ Rd,

b > 0,

(A.1)

where X (b) is the process deﬁned by X (b)

t

(cid:90) T

0

ξ(b)
k

:=

:= Xbt and ξ(b) is the vector with entries

Xbtfk(t)dt,

k ∈ {1, 2, . . . , k}.

It is clear that X (b) is a L´evy process, that ΨX (b) = bΨX, and that (3.3) corresponds to the special case
b = 1. We focus on this more general result because it will lead directly to a proof of inﬁnite divisibility.
We begin by deﬁning

X b(n+1)T

,

N

k ∈ {1, 2, . . . , k}, N ∈ N,

(A.2)

which are N -point, right-endpoint Riemann sum approximations of the random variables ξ(b)
usual telescoping sum technique for L´evy processes we can write

k . By the

(cid:17)

(cid:16)

(cid:17)

X b(n+1)T

=

N

X b(n+1)T

N

− X bnT

N

+

X bnT

N

− X b(n−1)T

N

+ . . . +

X b2T

N

− X bT

N

+ X bT

N

(cid:16)

d= X (1) + X (2) + . . . + X (n+1),

where the random variables X (i) are independent and each distributed like XbT /N . This allows us to
rearrange the sum R(k)
N according to the random variables X (i), gathering together those with the same
index. Therefore, we have

N−1(cid:88)

n=0

(cid:32)

N−1(cid:88)

j=n

T
N

fk

(cid:18)(j + 1)T

(cid:19)(cid:33)

.

N

R(k)
N

d=

X (n+1)

We notice that the term in brackets on the right-hand side is a (N − n)-point, right-endpoint Riemann
sum approximation for the integral of fk over the interval [nT /N, T ]. Let us therefore deﬁne

(cid:18)(n + 1)T

(cid:19)

N−1(cid:88)

n=0

fk

R(k)

N :=

T
N

N

(cid:17)

(cid:16)

,

and

s(k)
n,N :=

fk(s)ds,

(A.3)

(cid:18)(j + 1)T

(cid:19)

N

N−1(cid:88)

j=n

fk

(cid:32)N−1(cid:88)

t(k)
n,N :=

T
N

(cid:34)

(cid:33)(cid:35)

ıX (n+1)(cid:104)z, tn,N(cid:105)

n=0

16

(cid:90) T

nT
N

(cid:32)

N−1(cid:88)

n=0

as well as the d-dimensional vectors tn,N and sn,N consisting of entries t(k)
observe that

n,N and s(k)

n,N respectively. We

E[exp(ı(cid:104)z, ξ(b)(cid:105)] = lim
N→∞

E

exp

= lim

N→∞ exp

−bT
N

ΨX((cid:104)z, tn, N )(cid:105)

,

(A.4)

(cid:33)

(cid:32)

N−1(cid:88)
(cid:40)

n=0

(cid:33)

(cid:41)
k(x)|
|f(cid:48)

where we have used the dominated convergence theorem to obtain the ﬁrst equality, and the independence
of the X (i) to obtain the ﬁnal equality. Further, we get

(cid:18)

(cid:90) T

0

exp

−

(cid:19)
ΨX (b) ((cid:104)z, u(t)(cid:105)) dt

= lim

N→∞ exp

−bT
N

ΨX((cid:104)z, sn, N )(cid:105)

,

by using the left-endpoint Riemann sums. We note that |(cid:104)z, tn, N(cid:105) − (cid:104)z, sn, N(cid:105)| → 0 uniformly in n since

|(cid:104)z, tn, N(cid:105) − (cid:104)z, sn, N(cid:105)| ≤ d(cid:88)

|zk|(cid:12)(cid:12)(cid:12)t(k)

n,N − s(k)

n,N

(cid:12)(cid:12)(cid:12) ≤ dT 2

N

k=1

max
1≤k≤d

|zk| sup
x∈[0,T ]

,

(A.5)

where the last estimate follows from the well-known error bound ((c − a)2 supx∈[a,c] |g(cid:48)(x)|)/N for the
absolute diﬀerence between an N -point, right end-point Riemann sum and the integral of a C 1 function
g over [a, c]. Then, by the continuity of ΨX, for any  > 0 we may choose an appropriately large N such
that

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) 1

N

N−1(cid:88)

n=0

N−1(cid:88)

n=0

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≤ 1

N

N−1(cid:88)

n=0

ψX((cid:104)z, tn, N(cid:105)) − 1
N

ψX((cid:104)z, sn, N(cid:105))

|ψX((cid:104)z, tn, N(cid:105)) − ψX((cid:104)z, tn, N(cid:105))| ≤ .

This proves (A.1) and therefore also (3.3) for C 1 functions.

To establish the inﬁnite divisibility of ξ we note that (A.1) shows that Ψξ(b) = bΨξ(1) = bΨξ and that
e−bΨξ is therefore a positive deﬁnite function for every b since it is the characteristic function of the
random vector ξ(b). Positive deﬁniteness follows from Bochner’s Theorem (see for example Theorem 2.13
in [4]). Also, we clearly have Ψξ(0) = 0 since ΨX(0) = 0. By Theorem 2.15 in [4] these two points
combined show that Ψξ is the characteristic exponent of an ID probability distribution, and hence ξ is
an ID random vector.
Now one can extend the lemma to L1 functions {fk}d
k=1 by exploiting the density of C 1([0, T ]) in L1([0, T ]).
In particular, for each k we can ﬁnd a sequence of C 1 functions {fn,k}n≥1 which converges in L1 to fk.
Then,

|uk(t) − un,k(t)| =

fk(t)dt −

fn,k(t)dt

|fk(t) − fn,k(t)| dt

showing that un,k → uk uniformly in t. This shows that for each z the functions {ΨX((cid:104)z, un(·)(cid:105))}n≥1, with
un := (un,1,··· , un,d)T, are uniformly bounded on [0, T ], so that the dominated convergence theorem
applies and we have

lim
n→∞ exp

−

ΨX ((cid:104)z, un(t)(cid:105)) dt

= exp

ΨX ((cid:104)z, u(t)(cid:105)) dt

.

(A.6)

(cid:19)

On the other hand, X is a.s. bounded on [0, T ], so that

n→∞|ξk − ξn,k| = lim
n→∞

lim

Xtfk(t)dt −

(cid:90) T

Xtfn,k(t)dt

0

17

(cid:33)

|Xt|

(cid:90) T

0

lim
n→∞

|fk(t) − fn,k(t)|dt = 0,

sup
t∈[0,T ]

(cid:12)(cid:12)(cid:12)(cid:12)(cid:90) T

t

(cid:90) T

0

(cid:18)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:90) T

0

(cid:90) T

t

(cid:19)

(cid:90) T

0

(cid:12)(cid:12)(cid:12)(cid:12) ≤
(cid:90) T

−

0

(cid:18)
(cid:32)

(cid:12)(cid:12)(cid:12)(cid:12) ≤

a.s.. Therefore Ξn := (ξn,1,··· , ξn,d)T converges a.s. and consequently also in distribution to ξ. Together
with (A.6), this implies that for each z

E[ei(cid:104)z,Ξn(cid:105)] = E[ei(cid:104)z,ξ(cid:105)] = exp

lim
n→∞

ΨX ((cid:104)z, u(t)(cid:105)) dt

.

(A.7)

(cid:18)

−

(cid:90) T

0

(cid:19)

Therefore, (3.3) is also proven for functions in L1. Since each Ξn has an ID distribution Lemma 3.1.6 in
(cid:117)(cid:116)
[18] guarantees that ξ is also an ID random vector.

References

[1] R.B. Ash and M.F. Gardner. Topics in stochastic process. Academic Press, New York–San Francisco–

London, 1975.

[2] M. Benko. Functional data analysis with applications in ﬁnance. PhD thesis, Humbolt–Universist¨at

zu Berlin, 2006.

[3] J. Bertoin. Some elements on L´evy processes. In C.R. Rao and D.N. Shanghag, editors, Stochastic

Processes: Theory and Methods. Elsevier Science B.V., Amsterdam, The Netherlands, 2001.

[4] B. B¨ottcher, J. Wang, and R. Schilling. L´evy matters III. L´evy-type processes: Construction, approx-
imation and sample path properties. Springer, Berlin–Heidelberg–New York–London–Paris–Tokyo–
Hong Kong–Barcelona–Budapest, 2013.

[5] R. Cont and J. Da Fonseca. Dynamics of implied volatility surfaces. Quantitative ﬁnance, 2(1):45–60,

2002.

[6] L. Devroye. An automated method for generating random variates with a given characteristic

function. Siam J. Appl. Math., 46(4):698–719, 1986.

[7] R.D. Dony. Karhunen–Lo`eve Transform. In K.R. Rao and P.C. Yip, editors, The transform and

data compression handbook. CRC Press., Boca Raton, U.S.A., 2001.

[8] R. G. Ghanem and P.D. Spanos. Stochastic ﬁnite elements: A spectral approach. Springer–Verlag,

New York–Berlin–Heidelberg–London–Paris–Tokyo–Hong Kong–Barcelona, 1991.

[9] J. Istas. KarhunenLo`eve expansion of spherical fractional brownian motions. Statistics and Proba-

bility Letters, 76(14):1578 – 1583, 2006.

[10] S. Jin. Gaussian processes: Karhunen–Lo`eve expansion, small ball estimates and applications in

time series modes. PhD thesis, University of Delaware, 2014.

[11] M. Kac and A.J.F. Siegert. An explicit representation of a stationary Gaussian process. Ann. Math.

Stat., 18:438–442, 1947.

[12] K. Karhunen. ¨Uber lineare Methoden in der Wahrscheinlichkeitsrechnung. Amer. Acad. Sc. Fenni-

cade, Ser. A, I, 37:3–79, 1947.

18

[13] G. Leobacher. Stratiﬁed sampling and quasi-Monte Carlo simulation of L´evy processes. Monte-Carlo

methods and applications, 12(3–4):231–238, 2006.

[14] M. Lo´eve. Fonctions aleatoires du second ordre. In P. L´evy, editor, Processus stochastic et mouvement

Brownien. Gauthier Villars, Paris, 1948.

[15] W. Luo. Wiener chaos expansion and numerical solutions of stochastic partial diﬀerential equations.

PhD thesis, California Institute of Technology, 2006.

[16] C. Maccone. Deep space ﬂight and communications. Springer–Praxis, Berlin–Chichester, 2009.

[17] D.B. Madan and E. Seneta. The variance gamma (v.g.) model for share market returns. The Journal

of Business, 63(4):511–524, 1990.

[18] M. M. Meerschaert and H. Scheﬄer. Limit distributions for sums of independent random vectors:

Heavy tails in theory and practice. John Wiley & Sons, Inc., New York, 2001.

[19] K.K. Phoon, Huang H.W., and S.T. Quek. Simulation of strongly non-Gaussian processes using

Karhunen–Loeve expansion. Probabilistic Engineering Mechanics, 20:188–198, 2005.

[20] J. Rosi´nski. On series representations of inﬁnitely divisible random vectors. The Annals of Proba-

bility, 18(1):405–430, 1990.

[21] J. Rosi´nski. Series representations of L´evy processes from the perspective of point processes. In O.E.
Barndorﬀ-Nielsen, T. Mikosh, and S. Resnick, editors, L´evy processes: Theory and Applications.
Birkh¨auser, Boston–Basel–Berlin, 2001.

[22] J. Rosi´nski. Tempering stable processes. Stochastic processes and their applications, 117(6):677–707,

2007.

[23] K. Sato.

L´evy procseses and inﬁnitely divisible distributions. Cambridge University Press,

Cambridge–New York–Melbourne–Cape Town–Singapore–S˜ao Paulo, 1999.

[24] P. Tankov and R. Cont. Financial Modelling with Jump Processes. Chapman and Hall/CRC, Boca

Raton–London–New York–Washington,D.C., 2004.

[25] M. Unser and P.D. Tafti. An introduction to sparse stochastic processes. Cambridge University

Press, Cambridge, 2014.

[26] M.L. Unser. Wavelets, ﬁlterbanks, and the Karhunen-Loeve transform. In Signal Processing Con-

ference (EUSIPCO 1998), 9th European, pages 1–4. IEEE, 1998.

[27] L. Wang. Karhunen–Lo`eve expansions and their applications. PhD thesis, The London School of

Economics, 2008.

19

