High Dimensional Tests for Functional Networks of Brain Anatomic

Regions

Jichun Xie and Jian Kang

March 22, 2016

Author’s Footnote:

Jichun Xie is Assistant Professor, Department of Biostatistics and Bioinformatics, Duke Univer-

sity School of Medicine, Durham, NC 27705.

(Email:

jichun.xie@duke.edu). Jian Kang is As-

sistant Professor, Department of Biostatistics, University of Michigan, Ann Arbor, MI 48109.

(Email:

jiankang@umich.edu). Jian Kang’s research was partially supported by the NIH grant

R01 MH105561. The authors thank the autism brain imaging data exchange (ABIDE) study

(Di Martino et al., 2013) shares the resting-state fMRI data.

6
1
0
2

 
r
a

 

M
9
1

 
 
]
E
M

.
t
a
t
s
[
 
 

1
v
8
3
1
6
0

.

3
0
6
1
:
v
i
X
r
a

1

Abstract

There has been increasing interests in learning resting-state brain functional connectivity

of autism disorders using functional magnetic resonance imaging (fMRI) data. The data in a

standard brain template consist of over 200,000 voxel speciﬁc time series for each single subject.

Such an ultra-high dimensionality of data makes the voxel-level functional connectivity analysis

(involving four billion voxel pairs) lack of power and extremely ineﬃcient.

In this work, we

introduce a new framework to identify functional brain network at brain anatomic region-level

for each individual. We propose two pairwise tests to detect region dependence, and one multiple

testing procedure to identify global structures of the network. The limiting null distributions of

the test statistics are derived. It is also shown that the tests are rate optimal when the alternative

networks are sparse. The numerical studies show the proposed tests are valid and powerful. We

apply our method to a resting-state fMRI study on autism and identify patient-unique and

control-unique hub regions. These ﬁndings are consistent with autism clinical symptoms.

Keywords: High dimensionality; Hypothesis testing; Brain network; Sparsity; fMRI study

1.

INTRODUCTION

The functional brain network refers to the coherence of the brain activities among multiple

spatially distinct brain regions. It plays an important role in information processing and mental

representations (Bullmore and Sporns, 2009; Sporns et al., 2004), and could be altered by one’s

disease status. Supekar et al. (2008); Koshino et al. (2005); Cherkassky et al. (2006) showed that

patients with neurodegenerative diseases (such as the Alzheimer’s disease and the Autism Spectrum

Disorder) have diﬀerent function network compared with controls. As a result, the inference on

functional brain network will beneﬁt the study of these diseases. Our research goal is to infer the

whole functional networks of the brain regions.

Recent advances in the neuroimaging technologies provide great opportunities for researchers

to study functional brain network based on massive nueroimaging data, which are generated us-

ing various imaging modalities such as positron emission tomography (PET), functional magnetic

resonance imaging (fMRI), and electroencephalography (EEG). In a neuroimaging experiment, the

scanner records the brain signals over multiple times at each location (or voxel) in the three-

2

dimensional brain, leading to a four-dimensional imaging data structure. In a typical fMRI study,

the number of voxels can be up to 200,000 and the number of imaging scans over time is round 100–

200. In light of the brain function and the neuroanatomy, the human brain can be partitioned to

100-200 anatomical regions and each region contains 200 to 4,000 voxels. Such high dimensionality

and complexity of the data imposes great challenges on the inference of the whole brain network.

Due to the ultra-high dimensionality of voxel numbers (up to 200,000), direct inference on the

network of voxels is extremely computationally expensive. More importantly, the network of interest

is the network of brain regions, not voxels. To this end, Andrews-Hanna et al. (2007) examines

the functional connectivity of a particular brain region, called seed region, by correlating the seed

region brain signals against the brain signals from all other regions. Although this method yields

a clear view of the functional connectivities between one region of interest (the seed region) and

other regions (Biswal et al., 1995; Cordes et al., 2000), it fails to examine the functional network

on a whole brain scale. Alternatively, Velioglu et al. (2014) proposed to form meshes around a

seed voxel by regressing p functionally nearest neighbor voxels on the seed voxel, where number

of regressors p is determined by minimizing the Akaike’s ﬁnal prediction error (Akaike, 1969).

Then two voxels are considered as functionally connected if one serves as a functional predictor

as the other. The number of all connected voxel pairs between two anatomic regions are treated

as the dependence level between these two regions. Although this method successfully provides

a functional network among anatomic regions, no inference results are provided on what level of

connectivities should be regarded as signiﬁcant. Another commonly used method (Huang et al.,

2009, 2010) is to summarize one statistic (such as the largest principal component of voxel signals)

in each region and then study the dependence between these statistics. Commonly used measures

of dependence include covariance matrix or Gaussian Graphical model. See Supekar et al. (2008);

Weiss and Freeman (2001); Huang et al. (2009); Marrelec et al. (2006). Since only one statistic is

summarized in each region, the dependence among these summarized statistics sometimes fail to

represent the dependence among the regions.

In this article, we propose a new method to estimate the region-level functional connectivity

for each individual. Instead of summarizing one statistic in each region, we summarize multiple

statistics so that information of the region can be adequately captured. These statistics can be

3

viewed as functional components of the region. The correlation matrix between the components in

two regions are used to measure the dependence between two regions. We assume that two regions

are functionally connected if and only if at least one pair of components are correlated between

these two regions.

We then concatenate these functional components region by region. No region-level functional

connectivity implies that the covariance matrix (or equivalently its inverse) of the concatenated

components has a block-diagonal structure. This is a reasonable assumption and has been used

in many existing literatures.

(See Rubinov and Sporns (2010); Bowman et al. (2012); Huang

et al. (2009).) Thus, to construct a functional network of brain anatomic regions, we check if the

correlation matrix of two regions has a block diagonal structure.

Previous literatures for testing high dimensional covariance/correlation matrix include testing

whether the covariance matrix is proportional to the identity matrix (Ledoit and Wolf, 2002; Birke

and Holder, 2005; Schott, 2007; Chen et al., 2010; Cai and Ma, 2013; Li and Qin, 2014), and

testing whether two covariance matrices are equal (Li and Chen, 2012; Cai et al., 2013; Li and Qin,

2014). To the best of our knowledge, no existing methods have been proposed to address whether a

rectangle block of a covariance matrix is zero. However, ideas in those literatures can be borrowed

to construct test statistics for our problem. There are mainly two types of existing test statistics:

one is chi-square type of statistic based on the sum square of sample covariances. and the other is

the extreme type of statistic based on the largest absolute self-standardized sample covariance. In

general, the chi-square type of statistics performs better when the alternative network is dense and

the extreme type of statistics performs better when the alternative network is sparse. In imaging

studies, the network of functional components is usually sparse. Therefore, we will use the extreme

type of statistics. Details will be discussed in Section 3.

The rest of the paper is organized as follows. In Section 2, we introduce the notations and deﬁne

the testing hypotheses of our interests. Section 3 presents two procedures to control type I error

of each hypothesis and a multiple testing procedure to control family-wise error rate. Theoretical

properties of the proposed procedures are discussed in Section 4, and their numerical performances

are shown in Section 6. We apply the proposed procedures on a resting-state fMRI data of sub-

jects with and without autism spectrum disorder (ASD), and compare the functional networks of

4

anatomic regions between cases and controls. The results match the clinical characteristics of ASD.

2. MODEL AND HYPOTHESES

In fMRI studies, blood-oxygen-level dependent (BOLD) signals are collected at a large number

of voxel locations for n scans. The standard preprocessing steps including motion correction,

slice-timing correction, normalization, de-trending and de-meaning procedures are applied to the

BOLD signals (Worsley et al., 2002; Friman and Westin, 2005; Lindquist, 2008), and then the

signals are clustered based on their voxel locations mapping to the existing anatomic regions.

After clustering, the signals are summarized into functional components to reduce the dimension

of voxels and eliminate the redundancy of high coherent signals. One way to summarize the

functional components is to perform principal component analysis (PCA) in region s to extract

the ﬁrst qs principal components. Alternatively, independent component analysis (ICA) can be

perform to extract qs independent components. The choice of summarizing method depends on the

distribution of the processed signals. See Anderson (2003); Richard and Yuan (2012).

For each patient, assume that qs functional components are summarized in region s. Each

functional component is of length n, containing replications of signals across n scans. After removing

the temporal-correlation between the scans, denote by Xk,s,i the k-th scan of the i-th component

in s-th brain region. Then these components can be treated as independent across scans.

Denote by X k,s = (Xk,s,1, . . . , Xk,s,qs)T the vector of functional components in region s of scan

k, and by

Υst = Cor(X k,s, X k,t)

the correlation matrix between region s and region t. To test whether region s and region t are

functionally connected, we set up the hypotheses:

H0,st : Υst = 0,

versus

H1,st : Υst (cid:54)= 0.

(1)

A rejection of H0,st implies that regions s and region t have signiﬁcant functional connectivity.

The goal is to test H0,st with controlled type I error, and also to perform multiple testing on H0,st

simultaneously to control family-wise error rate.

The diﬃculty of this testing problem lies in the large number of parameters and relatively small

number of replications. First, the number of summarized functional components in each region may

5

increase with the number of scans n. Second, the number of total region pairs p(p − 1)/2 usually

largely exceeds n. Therefore, we need to address the high dimensional challenges in testing each

hypothesis and testing a large number of them simultaneously.

3. TESTING PROCEDURES

To test H0,st, we propose two testing procedures to ﬁt diﬀerent distribution assumptions of

the functional components. Therefore, neither of them can universally outperform the other. We

further develop a multiple testing procedure to control the family-wise error (FWER) for testing
{ H0,st : 1 ≤ s < t ≤ p} simultaneously.

3.1 Test I: Marginal Dependence Testing

The ﬁrst procedure is based on the Pearson correlation between the components in two regions.

Denote by the pairwise correlation ρst,ij = Cor(Xk,s,i, Xk,t,j). Then the null hypothesis Hst,0 :
Υst = 0 is equivalent to Hst,0 : max1≤i≤qs,1≤j≤qt |ρst,ij| = 0. A straightforward approach is to check
whether the sample correlation between two regions is close to zero. Denote the Pearson correlation

between the i-th component in region s and the j-th component in region t by ˆρst,ij, i.e.,

where ¯Xs,i =(cid:80)n

(cid:80)n
k=1(Xk,s,i − ¯Xs,i)(Xk,t,j − ¯Xt,j) is
the sample covariance between the i-th component in region s and the j-th component in region

k=1 Xk,s,i/n, ¯Xt,j =(cid:80)n

ˆρst,ij = ˆσst,ij/ (ˆσss,iiˆσtt,jj)1/2 ,

k=1 Xk,t,j/n, ˆσst,ij = 1
n

t, and ˆσss,ii and ˆσtt,jj are sample variances deﬁning in the similar manner. The test statistic is

deﬁned as

st = n · max
T (1)

i,j

st,ij − 2 log(qsqt) + log log(qsqt).
ˆρ2

(2)

With mild conditions (details in Section 4), under H0,st, T (1)

st asymptotically follows the Gumbel

distribution

F (x) = exp{−π1/2 exp(−x/2)}.

(3)
exceeds the (1 − α)-th quantile of F (x),

To control type I error at level α, we reject H0,st if T (1)
st
i.e., T (1)

st > qα, with

qα = − log(π) − 2 log log{1/(1 − α)}.

(4)

6

3.2 Test II: Local Conditional Dependence Testing

The alternative testing procedure is based on the Pearson correlation between the residuals of

local neighborhood selection in two regions.

In region s, we regress on each component Xk,s,i the rest of components,

Xk,s,i = αs,i + X T

k,s,−iβs,i + εk,s,i,

(5)

where X k,s,−i is the vector of X k,s by removing the i-th component. In region t with t (cid:54)= s, we
build up similar regression model

Xk,t,j = αt,j + X T

k,t,−jβt,j + εk,t,l,

(6)

Let ρε,st,ij = Cor(εk,s,i, εk,t,j) be the correlation of the error terms in two models. Clearly, the null

hypothesis H0,st is equivalent to

H0,st : max

i,j

ρε,st,ij = 0.

We therefore develop a testing procedure to test if the correlations ρε,st,ij are all zero. If the

coeﬃcients βs,i and βt,j in model (5) and (6) were known, we would know the value of each
realization of the random error εk,s,i and εk,t,j, and center them as ˜εk,v,l = εk,v,l − ¯εv,l with ¯εv,l =
k=1 εk,v,l, (v, l) = (s, i) or (v, l) = (t, j). Based on model (5) and (6), the centered realization

(cid:80)n

1
n

of randome error ˜εk,v,l could be expressed as

˜εk,v,l = Xk,v,l − ¯Xv,l(X k,v,−l − ¯X v,−l)Tβv,l,

(v, l) = (s, i) or (v, l) = (t, j).

(7)

Consequently, the Pearson correlation between ˜εk,s,i and ˜εk,t,j would be

where ˜σε,st,ij = 1
n

k=1 ˜εk,s,i ˜εk,t,j, ˜σε,ss,ii = 1
n

k=1 ˜ε2

k,s,i, and ˜σε,tt,jj = 1
n

Unfortunately in practice, the coeﬃcients in (5) and (6) are unknown. However, the coeﬃcients

can be well estimated by existing methods, such as Lasso or Dantzig selector. Suppose “good”1

coeﬃcient estimators ˆβs,i and ˆβt,j exist. Then the centered error term ˜εk,v,l can be estimated by

ˆεk,v,l = Xk,v,l − ¯Xv,l − (X k,v,−l − ¯X v,−l)T ˆβv,l,

(v, l) = (s, i) or (v, l) = (t, j).

(8)

1We will discuss the criteria of “good” and how to obtain “good” coeﬃcient estimators in Section 4.

7

˜ρε,st,ij =

1
n

˜σε,st,ij/ (˜σε,ss,ii˜σtt,jj)1/2 ,

(cid:80)n

(cid:80)n

k=1 ˜ε2

k,t,j.

n(cid:88)

k=1

(cid:80)n

Consequently, we calculate Pearson correlation based on ˆεk,s,i and ˆεk,t,j,

(cid:80)n

ˆρε,st,ij = ˆσε,st,ij/ (ˆσε,ss,iiˆσtt,jj)1/2 ,

(cid:80)n

where ˆσε,st,ij = 1
n

k=1 ˆεk,s,i ˆεk,t,j, ˆσε,ss,ii = 1
n

k=1 ˆε2

k,s,i, and ˆσε,tt,jj = 1
n

(cid:80)n

k=1 ˆε2

k,t,j.

Similar as Test I, we obtain the test-statistics as follows.

st = n · max
T (2)

i,j

ε,st,ij − 2 log(qsqt) + log log(qsqt).
ˆρ2

Under certain condtions (discussed in Section 4) and H0,st, T (2)
in (3). Therefore, to control type I error at level α, we reject H0,st if T (2)
(1 − α)-th quantile of F (x).

st also follows the distribution F (x)

st > qα, where qα is the

3.3 Family-Wise Error Rate Control

Considering the standard space of the brain (Mazziotta et al., 1995, Montreal Neurological

Institute, MNI) and the commonly used brain atlas: the Automated Anatomical Labeling (Tzourio-

Mazoyer et al., 2002, AAL) regions, the number of region pairs in the whole brain is over 4,000, which

is much larger than the number of scans (typically a couple of hundreds). This motivates the needs

of correction for multiplicity when testing any two of them are connected, in order to detect the
functional connectivity of the whole brain. We propose procedure (9) to test { H0,st : 1 ≤ s < t ≤ p}
simultaneously and control the family-wise error rate (fwer). The procedure can involve either

st , depending on the structure assumption of the dependence structure of local voxels. It
turns out that to control fwer at level α, we only need to adopt a higher threshold. The adjusted

(cid:101)T (1)
st or (cid:101)T (2)

testing procedure is as follows:

Reject H0,st if and only if T (b)

st > 2 log{p(p − 1)/2} + qα

(1 ≤ s < t ≤ p),

(9)

for b = 1, 2. The threshold depends on the desired family-wise error rate α, and the total number
of region pairs p(p − 1)/2.

4. THEORY

In this section, we show the null distributions of the test statistics in procedures I and II, their

power, and the optimality properties of the proposed tests. Also, we prove that the multiple testing

procedure (9) is able to control family-wise error rate.

8

j=1 a2

a = (a1, . . . , ap)T ∈ Rp, denote by |a|2 = ((cid:80)p
Rp×q, deﬁne the spectral norm (cid:107)A(cid:107)2 =(cid:80)|x|2=1|Ax|2 and the Frobenius norm (cid:107)A(cid:107)F = ((cid:80)

For the rest of the paper, unless otherwise stated, we use the following notations. For a vector
j )1/2 its Euclidean norm. For a matrix A = (aij) ∈
ij)1/2.
For a ﬁnite set A = {a1, . . . , as}, Card(A) = s counts the number of elements in A. For two real
number sequences {an} and {bn}, write an = O(bn) if |an| ≤ C|bn| hold for a certain positive
constant C when n is suﬃciently large; write an = o(bn) if limn→∞ an/bn = 0; and write an (cid:16) bn if
c|bn| ≤ |an| ≤ C|bn|, for some positive constants c and C when n is suﬃciently large.

ij a2

Also assume the number of variables in all regions are comparable, i.e., q1 (cid:16) q2 . . . (cid:16) qp. Let
q0 = max(q1, . . . , qp). Assume X 1,v, . . . , X n,v are independently and identically distributed for each

region v.

4.1 Asymptotic Properties for Test I

Denote by Υvv = (ρvv,ij)qv×qv the correlation matrix of X k,v. For Xk,v,i, denote by r(1)

v,i the

number of other components in region v that at non-negligibly correlated with Xk,v,i,

v,i = Card{j : |ρvv,ij| ≥ (log q0)−1−α0, j (cid:54)= i},
r(1)

where α0 is a positive constant. For a positive constant ρ0 < 1, deﬁne

D(1)
v = {i : |ρvv,ij| > ρ0 for some j (cid:54)= i},

Thus, D(1)

v

region v.

contains index i such that Xk,v,i is highly correlated to at least one other component in

We need the following conditions:

(C1.1) For region v = s, t, there exists a subset Mv ⊂ {1, . . . , qv} with Card(Mv) = o(qv) and a
constant α0 > 0 such that for all γ > 0, maxi∈Mc
v ). Moreover, assume there exists a
v } = o(qv).
constant 0 ≤ ρ0 < 1 such that Card{D(1)

v,i = o(qγ

v r(1)

Condition (C1.1) constraints the sparsity level of non-neglegible and large signals. It speciﬁes
that for each region v, for almost all component i within the region, the count of non-neglible |ρvv,ij|
is of a smaller order of qγ
v . The condition is weaker than the commonly seen condition which imposes
a constant upper bound on the largest eigenvalue of Σvv. In fact, if λmax(Σvv) = o{qγ
v /(log q0)1+α0},
max1≤i≤qv r(1)
v ). In addition, (C1.1) also requires the number of components that are very

v,i = o(qγ

9

highly correlated with at least one other component to be small. This condition can be easily

satisﬁed if all the correlations ρvv,ij are bounded by ρ0.

(C1.2) Sub-Gaussian type tails: For region v = s, t, suppose that log(qv) = o(n1/5). There exist

some constants η > 0 and K > 0 such that

E(cid:2)exp{η(Xk,v,i − µv,i)2/σvv,ii}(cid:3) ≤ K.

max
1≤i≤qv

(C1.2*) Polynomial-type tails: For region v = s, t, suppose that for some γ1, c1 > 0, q0 ≤ c1nγ1+1/2,
and for some  > 0,

E|(Xk,v,i − µxi)/σ1/2

vv,ii|4γ1+4+ ≤ K.

max
1≤i≤qv

Conditions (C1.2) and C(1.2*) impose constraints on the tail of the distribution of Xk,v,i, and

the corresponding order of qv. They ﬁt a wide rage of distributions. For example, Gaussian

distribution satisfy Condition (C1.2), and Pareto distribution P areto(α) (a heavy tail distribution)

with α suﬃciently large satisfy Condition (C1.2*).
(C1.3) Let θst,ij = Var{(Xs,i − µs,i)(Xt,j − µt,j)}, with µs,i = EXs,i and µt,j = EXt,j. Suppose that
there exists κ1 > 0, such that

max

1≤i≤qs,1≤j≤qt

σss,iiσtt,jj

θst,ij

≤ κ1.

Condition (C1.3) holds immediately with κ1 = 1 under the null H0,st, and thus we only need

it for the power analysis. Under the alternative H1,st, it holds for a bunch of distributions. For

instance, it holds when the concatenated vector (X T
butions (Anderson, 2003). In particular, for multivariate Gaussian distributions, κ1 ≤ 2.

k,t)T follows elliptically contoured distri-

k,s, X T

We ﬁrst present the asymptotic null distribution of T (1)
st .

Theorem 1. Suppose that (C1.1) and (C1.2) (or (C1.2*)) hold. Then under H0,st, as n, q0 → ∞,
for all x ∈ R, the distribution T (1)

st converges to the Gumbel distribution F (x) deﬁned in (3).

When (C1.1) is not satisﬁed, i.e., the correlation matrices Υss and Υtt are arbitrary, it is diﬃcult

to derive the limiting null distribution of T (1)

st . However, Test I can still control the type I error.

Proposition 1. Under (C1.2) (or (C1.2*)) and the null H0,st, for 0 < α < 1,

P{T (1)

st ≥ qα} ≤ log{1/(1 − α)},

where qα is deﬁned in (4).

10

When the desired type I error α is small, log{1/(1− α)} ≈ α. Therefore, Test I can still control

error is desired for the test, we can deﬁne α(cid:48) = 1− exp(−α) and reject H0,st when (cid:101)T (1)

type I error close to the desired level. When there comes a rare circumstance that a larger type I
st ≥ qα(cid:48). Since
α = log{1/(1 − α(cid:48))}, Test I is always a asymptotically valid test, for arbitrary correlation matrices
Υss and Υtt. However, the power will be reduced when we threshold T (1)

st at the a higher level qα(cid:48).

We now turn to the power analysis of Test I. To test the correlation between region s and region

t, we deﬁne the following class of correlation matrix:

(cid:26)

(cid:27)

,

U (1)
st (c) =

Υst : n · max

i,j

st,ij ≥ c log dst
ρ2

It turns out that Test I distinguishes Υst in U (1)
approaching to one asymptotically.

st {4(1 + κ1)} from a zero matrix with a probability

Theorem 2. Suppose that (C1.2) (or (C1.2*)) and (C1.3) hold. Then as n and q0 both go to

inﬁnity,

Υst∈U (1)

inf
st {4(1+κ1)}

P{T (1)

st > qα} → 1.

To distinguishes the alternative from the null, Test I requires only one entry in the correlation

matrix Υst larger than (c log dst/n)1/2. The rate is optimal in terms of the following minimax
argument. Denote by F (1)
the collection of distributions satisfying (C1.2) or (C1.2*), and by T (1)
the collection of all α-level tests over F (1)

st,α

st

st , i.e.,

For all Φst,α ∈ T (1)

st,α, P{Φst,α = 1} ≤ α.

Theorem 3 shows that, if the maximum absolute correlation is less than (c0 log dst/n)1/2, for some

c0, no test can perfectly distinguish the alternative from the null. Thus, Theorems 2 and 3 together

indicate that Test I has certain rate optimality property.

Theorem 3. Suppose (C1.2) or (C1.2*) holds. Let α and β be any positive numbers with α+β < 1.

There exists a positive constant c0 such that for all large n and q0,

inf
Υst∈U (1)

st (c0)

sup

Tst,α∈T (1)

st,α

P(Tst,α = 1) ≤ 1 − β.

11

In Theorem 2 and 3, the diﬀerence between the null and the alternative is measured by the

maximal absolute value of the entries in Υst. Another commonly used measure is the Frobenius
norm (cid:107)Υst(cid:107)F . Denote by rst the count of the nonzero entries in Υst, i.e.,

qs(cid:88)

qt(cid:88)

rst =

I(ρst,ij (cid:54)= 0).

i=1

j=1

Consider the following class of matrices:

st (c) =(cid:8)Υst : (cid:107)Υst(cid:107)2

F ≥ crst log dst/n(cid:9) .

V (1)

We now show that Test I enjoys the rate optimality property measured by Frobenius norm too.

Corollary 1. Suppose that (C1.2) or (C1.2*) holds. Then for a suﬃciently large c, as n and q0

both go to inﬁnity,

P{T (1)

st > qα} → 1.

inf
Υst∈V (1)

st (c)

Theorem 4. Suppose that (C1.2) or (C1.2*) holds. Assume that rst ≤ qγ2
Let α, β be any positive number with α + β < 1. There exists a positive contant c0 such that for all

for some 0 < γ2 < 1/2.

0

large n and q0,

inf
Σst∈V (1)

st (c0)

sup

Tst,α∈T (1)

st,α

P(Φst,α = 1) ≤ 1 − β.

In Theorem 4, we assume that rst ≤ qγ2

0 . The assumption is quite reasonable for brain network,

because if the connections of the functional components exist between two brain regions, they are

usually sparse.

4.2 Asymptotic Properties for Test II

For Test II, the conditions required for achieving its asymptotic property are diﬀerent from

what required for Test I.

Recall that εk,s,i and εk,t,j are the error term of regressing all other components on one compo-

nent within the region, as deﬁned in (5) and (6), and σε,st,ij = Cov(εk,s,i, εk,t,j). Let Υε,st = (ρε,st,ij)

be the correlation matrix between εk,s = (εk,s,1, . . . , εk,s,qs)T and εk,t = (εk,t,1, . . . , εk,t,qt)T. Then

ρε,st,ij =

σε,st,ij

(σε,ss,iiσε,tt,jj)1/2

,

12

where σε,st,ij = Cov(εk,s,i, εk,t,j), σε,ss,ii = Var(εk,s,i) and σε,tt,jj = Var(εk,t,j).

For εk,s,i, denote by r(2)

v,i the number of other εk,s,j that are non-negligibly correlated (>

(log q0)−1−α0) with it,

v,i = Card{j :
r(2)

|ρε,vv,ij| ≥ (log q0)−1−α0, j (cid:54)= i}.

For a positive constant ρ0 < 1, deﬁne the following set that εk,v,i is highly correlated with at least

one εk,v,j as

D(2)
v = {i : |ρε,vv,ij| > ρ0 for some j (cid:54)= i}.

We need the following conditions:

(C2.1) For regions v = s, t, there exists a subset Mv ∈ {1, . . . , qv} with Card(Mv) = o(qv) and a
constant α0 > 0 such that all γ > 0, max1≤i≤p,i∈Mv r(2)
v ). Moreover, assume there exists a
constant 0 ≤ ρ0 < 1 such that Card{Dv} = o(q0).

v,i = o(qγ

Condition (C2.1) parallels with Condition (C1.1). It imposes conditions on the within region

correlation Υε,vv. Suppose X k,v follow multivariate Gaussian distribution with Ωvv = (ωvv,ij) to be

its inverse covariance matrix. Because ρε,vv,ij = ωvv,ij/(ωvv,iiωvv,jj)1/2 (Anderson, 2003), Condition

(C2.1) holds under many cases when inverse covariance matrix of the components are sparse and

bounded. See Honorio et al. (2009); Huang et al. (2010); Mazumder and Hastie (2012). Obviously,

the covariance matrix and inverse covariance matrix are diﬀerent, and consequently many data only

satisfy one of these two conditions, and then the corresponding procedure should be applied to the

data.
(C2.2) For region v = s, t, the variable X k,v ∼ N(µv, Σvv), with λmax(Σvv) ≤ c0, where λmax is
the maximum eigenvalue operator. Also assume log q0 = o(n1/5).

In general, the theoretical properties of Test II hold for many non-Gaussian distributions as

well. However, only under the Gaussian distribution assumption, ρε,vv,ij has an interpretation of

conditional dependence such that

ρε,vv,ij = 0 if and only if Xk,v,i ⊥⊥ Xk,v,j | {Xk,v,l, l (cid:54)= i, j}.

Condition (C2.2) makes Condition (C2.1) a natrual assumption on the conditional dependency.
Since σvv,ii ≤ λmax(Σvv) and σvv,iiωvv,ii ≥ 1, this condition also implies that Var(εk,s,i) = 1/ωvv,ii ≤
c0.

13

n(cid:88)

(C2.3) Recall the deﬁnition of ˜εk,v,l and ˆεk,v,l in (7) and (8). Under the cases (i) s (cid:54)= t and (ii)
s = t and i = j, with probability tending to one,

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) 1
(cid:80)n
(cid:80)n
Note that ˆεk,v,i is the centered residual and ˜εk,v,i is the centered random error. The term
k=1 ˜εk,s,i ˜εk,t,j| is determined by the diﬀerence between βv,i and its estimator
| 1
k=1 ˆεk,s,i ˆεk,t,j− 1
n
ˆβv,i. We will specify in Section 5 some estimation methods and corresponding suﬃcient conditions

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≤ C(log q0)−1−α0.

ˆεk,s,i ˆεk,t,j − 1
n

n(cid:88)

˜εk,s,i ˜εk,t,j

max

(10)

k=1

k=1

n

i,j

n

under which Condition (C2.3) will hold.

Theorem 5 speciﬁes the null distribution of T (2)
st .

Theorem 5. Suppose that (C2.1), (C2.2) and (C2.3) hold. Then under H0, as n, q0 → ∞, for all
v ∈ R, T (2)

st weakly converges to the Gumbel distribution F (x) in (3).

The derivation of the limiting null distribution of T (2)

st calls for Condition (C2.1); when it is not

satisﬁed, we can still control type I error based on the following proposition.

Proposition 2. Under (C2.2) and (C2.3) and the null H0,st,

P{T (2)

st ≥ qα} ≤ log {1/(1 − α)} ,

where qα = −log(π) − 2 log log{1/(1 − α)} is the (1 − α)-th quantile of F (x) deﬁned in (3).

The power analysis of Test II parallels to that of Procedure I. Let rε,st =(cid:80)qs

(cid:80)qt
j=1 I(ρε,st,ij (cid:54)=

i=1

0). Deﬁne the following two classes of matrices:

Υε,st :

U (2)
st (c) =
V (2)

st (c) =(cid:8)Υε,st : (cid:107)Υε,st(cid:107)2

ε,st,ij ≥ c log dst/n
ρ2

F ≥ crε,st log dst/n(cid:9) .

1≤i≤qs,1≤j≤qt

max

(cid:27)

;

(cid:26)

(cid:110)

st ≥ qα
T (2)

We have the following theorem.

Theorem 6. Suppose that (C2.2), and (C2.4) hold. Then

lim

n,q0→∞
for some c2 ≥ c1.

inf
Rst∈U (2)

st (c1)

P

(cid:110)

st ≥ qα
T (2)

(cid:111)

= 1,

(cid:111)

= 1,

and

lim
n,p→∞

inf
Rst∈V (2)

st (c2)

P

14

Similar as Test I, Test II enjoys certain rate optimality in its power. Denote by F (2)

the
st,α the collection of all α-level test over F (2)
st .

st

collection of distributions satisfying (C2.2), and by T (2)

Theorem 7. Suppose (C2.2) holds. Let α, β be any positive number with α + β < 1, There exists

a positive constant c3 such that for all large n and q0,

inf

Rε,st∈U (2)

st (c3)

sup

Φst,α∈T (2)

st,α

inf
Rε,st∈V (2)

st (c3)

sup

Φst,α∈T (2)

st,α

P(Φst,α = 1) ≤ 1 − β;

P(Φst,α = 1) ≤ 1 − β.

4.3 Asymptotic Properties for Multiple Testing Procedure

The properties of the the multiple testing procedure (9) are based on the limiting null distribu-

tion of each test statistic. Based on Theorems 1 and 5, we have the following results.

Theorem 8. Consider the multiple testing procedure (9). If (C1.1) and (C1.2) (or (C1.2*)) hold,

the procedure (9) with T (1)

st controls the family-wise error rate at level α. If (C2.1) and (C2.2) hold,

the procedure with T (2)

st controls the family-wise error rate at level α.

ESTIMATION OF (cid:98)βV,I

5.

Test II depends on the estimators of regression model. Estimating regression coeﬃcients has

been investigated extensively in the past several decades; methods include the Dantzig selector

(Candes and Tao, 2007), the Lasso (Tibshirani, 1996), the SCAD (Fan and Li, 2001), the adaptive

Lasso (Zou, 2006), the Scaled-Lasso (Sun and Zhang, 2012), the Square-root Lasso (Belloni et al.,

2011), etc.. In this paper, we focus on the Dantzig selector and Lasso, and discuss when they will

yield good estimators than can be used for our testing procedures. In particular, we will discuss

the necessary conditions for (C2.3) to hold.

(cid:80)n
Before we discuss the estimating methods, we introduce the following notations. For region v and
(cid:80)n
k=1(X k,v,−i − ¯X v,−i)T(Xk,v,i − ¯Xv,i) be the sample covariance between
component i, let bv,i = 1
n
k=1(X k,v,−i −
this components and other components in the region. Denote by ˆΣvv,−i,−i = 1
n
¯X v,−i)(X k,v,−j − ¯X v,−j)T the sample covariance matrix without component i, and let Dv,i =
diag( ˆΣvv,−i,−i). For the following methods, the tuning parameters are

λv,i(δ) = δ(ˆσvv,ii log qv/n)1/2.

15

Dantzig Selector. For v = 1, . . . , p and i = 1, . . . , qv, the Danztig selector estimators are

obtained by

ˆβv,i(δ) = arg min|α|1,

subject to |D

−1/2
v,i

ˆΣ−i,−iα − D

−1/2
v,i bv,i|∞ ≤ λv,i(δ).

Lasso. For v = 1, . . . , p and i = 1, . . . , qv, the Lasso estimators are obtained by

ˆβv,i(δ) = D

−1/2
v,i

ˆαv,i(δ),

where ˆαv,i(δ) = arg min
α∈Rp−1

(cid:34)

n(cid:88)

k=1

1
2n

(cid:110)

Xk,v,i − ¯Xv,i − (X k,v,−i − ¯X v,−i)D

(cid:111)2

−1/2
v,i α

+ λv,i(δ)|α|1

(11)

(cid:35)

.

(12)

We now demonstrate that under certain conditions, the methods yield good estimators that

satisfy the need to testing. Deﬁne by av,1 and av,2 the error bound

av,1 = max
1≤i≤qv

|ˆβv,i − βv,i|1,

av,2 = max
1≤i≤qv

|ˆβv,i − βv,i|2

(13)

Proposition 3. Suppose that (C2.2) holds. Consider the Dantzig selector estimator ˆβv,i(2) in

(11). Then if max1≤i≤qv|βv,i|0 = o(cid:8)n(log q0)−3−2α0[λmin(Σ)]2(cid:9), then Condition (C2.3) holds.
if max1≤i≤qv|βv,i|0 = o(cid:8)n(log q0)−3−2α0[λmin(Σ)]2(cid:9), Condition (C2.3) holds.

Proposition 4. Suppose that (C2.2) holds. Consider the Lasso estiamtor ˆβv,i(2.02) in (12). Then

In fact, Proposition 3 holds for any Dantzig selector estimator ˆβv,i(δ) with δ ≥ 2; and Propo-
sition 4 holds for any Lasso estimator ˆβv,i(δ) with δ > 2. For computational simplicity, we chose

δ = 2.02. In numerical studies, we found such choice work well in testing.

6.

SIMULATION STUDIES

In this section, we evaluate the performance of the our methods via two simulation studies: one

is focused on the size and power of the proposed tests for two regions, the other illustrates how to

identity the functional brain network using the proposed tests under family-wise error rate controls.

16

6.1 Size and Power

We simulate X k, for k = 1, . . . , n, from a normal distribution with mean zero and covariance

Σ11,22, i.e.

X k ∼ N(0q1+q2, Σ11,22) with Σ11,22 =

 Σ11 Σ12

ΣT

12 Σ22

 ,

where X k = (X T

k,1, X T

k,2)T and X k,s is of dimension qs for s = 1, 2. For comparisons, we also

consider a simple test for H0,12 in (1) based on the Person correlation coeﬃcient between the

principal component scores. Speciﬁcally, denote by Zs the ﬁrst principal component score of data

n,s)T. We compute the sample correlation between Z1 and Z2, denoted (cid:98)ρ12. The

1,s, . . . , X T

(X T

Fisher’s Z transformation is then taken to obtain the testing statistics T (3)

12 for this simple approach,

which is given by

T (3)
12 =

1
2

log

(cid:19)

(cid:18) 1 +(cid:98)ρ12
1 −(cid:98)ρ12

.

Using the results by Hotelling (1953), it is straightforward to show that

under H0,12 in (1). This implies that we reject H0,12 if

√

√

n − 3|T (3)

n − 3T (3)

12 → N (0, 1)
12 | > zα/2, where zα is the 1 − α

normal quantile. We refer to this testing procedure as test III.

To deﬁne diﬀerent model speciﬁcations on Σ11,22, we introduce a few auxiliary matrices. Let
Ad = (aij)d×d where aii = 1 and aij ∼ 0.5Bernoulli(0.5) for 10(k − 1) + 1 ≤ i (cid:54)= j ≤ 10k, where
k = 1, . . . , [d/10] and aij = 0 otherwise. Let Bd = (bij)d×d where bii = 1, bi,i+1 = bi−1,i = 0.5 and
bi,j = 0 for |i − j| > 3.

Let Λd = (λij)d×d with λii ∼ U(0.5, 2.5) and λij = 0 for i (cid:54)= j. Now, we deﬁne four diﬀerent

models for Σ11 and Σ22.

• Model 1 (Independent Cases): Σss = Λqs, for s = 1, 2.
• Model 2 (Block Sparse Covariance Matrices): Σss = Λ1/2

qs (Aqs + δiIqs)/(1 + δi)Λ1/2

qs , for

s = 1, 2, where δi = |λmin(Aqs)| + 0.05.

• Model 3 (Block Sparse Precision Matrices): Σss = Λ1/2

qs (A−1

qs +δ∗

i Iqs)/(1+δ∗

i )Λ1/2

qs , for s = 1, 2,

where δ∗

i = |λmin(A−1

qs )| + 0.05.

• Model 4 (Binded Sparse Covariance Matrices): Σss = Λ1/2

qs (Bqs + τsIqs)/(1 + τs)Λ1/2

qs , for

s = 1, 2, where τs = |λmin(Bqs)| + 0.05.

17

• Model 5 (Binded Sparse Precision Matrices): Σss = Λ1/2

s )Λ1/2

qs , for

qs (B−1

qs + τ∗

s Iqs)/(1 + τ∗

s = 1, 2, where τ∗

s = |λmin(B−1

qs )| + 0.05.

To simulate the empirical size, we assume Σ12 = 0q1×q2. To evaluate the empirical power, let

Σ12 = (σij)q1×q2 with σij ∼ sijBernoulli[5/(q1q2)] with sij ∼ N(4(cid:112)log(q1q2)/n, 0.5). The sample

size is taken to be n = 80 and 150, while the dimension (q1, q2) varies over (50, 50), (100, 150),

(200, 200) and (250, 300). The nominal signiﬁcant level for all the tests is set at α = 0.05. The

empirical sizes and powers for the ﬁve Models, reported in Tables 1 and 2, are estimated from 5,000

replications.

Obviously when the covariance matrix of each region is sparse, Test I controls the type I error

better; and when the precision matrix is sparse, Test II controls the type I error better. This

implies the essence of condition (C1.1) and (C2.1) when deriving the limiting null distribution. On

the other hand, the simulation also shows that without these two conditions, there is very little

inﬂation in the type I error. The power analysis shows the similar pattern. In general, Test I/II has

a larger power when the covariance/precision matrix is sparse. Both Tests I and II achieve a much

larger power than Test III (Person correlation test on the ﬁrst PC scores), although the empirical

sizes of Test III are comparable to the proposed tests.

6.2 Network Identiﬁcations

In this section, we perform the simulation studies to illustrate the performance of our proposed

testing procedure with the family-wise error rate control on the network identiﬁcations. We sim-

ulate a region-level brain network according to the Erd¨os-R´enyi model (Erd¨os and R´enyi, 1960).

We set the number of regions p = 90, and the probability of any two brain regions being func-

tional connected as 0.01. The simulated brain network is shown in Figure 1 in the supplementary

document.

For every two connected brain regions s and t on the simulated network, we consider four models

that we discussed in Section 6.1 for the speciﬁcations of Σss and Σtt. Similar to the simulation
studies for evaluating the empirical power, we set Σst = (σij)qs×qt with σij ∼ sijBernoulli(10/dst)

with sij ∼ N(4(cid:112)log(dst)/n, 1). We set sample size n = 150 and simulate the fMRI time series

18

Table 1: Empirical size of Tests I, II and III for diﬀerent sample sizes and models (×10−2)

Model Test

(q1, q2)

(30,30)

(50,50)

(100,150)

(200,200)

(300,250)

5.14

5.70

5.34

6.04

3.86

7.34

4.98

5.76

6.38

5.22

2.16

4.78

1.90

6.28

4.36

4.62

5.18

5.56

4.88

4.70

4.04

4.86

5.12

5.22

4.92

3.42

5.52

2.18

4.92

5.42

6.16

5.44

7.60

6.06

2.88

6.32

3.20

5.74

3.48

6.02

3.12

3.20

1.90

6.14

6.40

4.84

5.36

5.76

4.74

4.24

5.02

4.60

5.04

3.98

5.10

3.78

4.00

3.10

4.94

2.34

1

2

3

4

5

1

2

3

4

5

I

II

III

I

II

III

I

II

III

I

II

III

I

II

III

I

II

III

I

II

III

I

II

III

I

II

III

I

II

III

4.50

4.58

6.48

4.20

2.88

6.46

3.44

4.56

8.26

4.80

1.92

4.42

0.88

4.52

4.52

4.94

4.76

8.80

5.08

4.02

5.86

4.94

5.34

2.76

5.02

2.62

2.92

1.96

5.62

3.38

n = 80

4.54

4.70

3.38

4.52

4.08

8.88

4.50

5.02

7.40

5.12

3.04

6.56

1.06

4.32

5.38

n = 150

5.04

4.78

6.44

4.48

4.40

3.30

4.50

4.26

4.74

4.96

3.62

6.50

1.96

4.04

3.90

4.46

4.48

6.26

4.60

4.06

4.58

4.02

3.94

3.36

4.82

2.28

3.36

1.02

4.60

4.28

4.10

4.34

4.04

4.62

4.68

7.46

4.68

4.68

8.80

4.78

2.46

5.74

1.92

4.46

5.92

19

Table 2: Empirical power of Tests I, II and III for diﬀerent sample sizes and models (×10−2)

Model Test

(q1, q2)

(30,30)

(50,50)

(100,150)

(200,200)

(300,250)

55.44

55.84

8.66

55.08

44.72

5.72

44.40

54.94

4.50

56.08

42.18

5.96

35.00

49.90

6.68

89.24

87.78

7.52

87.62

55.58

5.02

54.88

84.80

5.84

74.92

58.32

6.56

44.96

55.24

3.62

54.74

54.04

6.18

55.10

43.94

7.28

44.36

55.90

3.96

64.32

42.84

8.64

34.78

44.96

7.60

85.22

85.04

9.48

84.46

55.18

3.64

55.48

79.94

3.04

85.42

59.26

6.08

45.40

53.32

2.34

1

2

3

4

5

1

2

3

4

5

I

II

III

I

II

III

I

II

III

I

II

III

I

II

III

I

II

III

I

II

III

I

II

III

I

II

III

I

II

III

88.58

88.46

11.32

88.04

69.72

6.46

69.88

87.46

3.84

90.24

56.82

8.02

80.82

89.94

8.12

98.82

98.96

13.82

99.14

86.98

8.10

90.06

94.58

3.80

95.26

85.40

9.34

84.74

95.10

7.94

n = 80

60.20

60.36

7.06

59.78

49.70

7.00

50.24

59.30

7.80

63.40

43.98

10.12

44.30

54.30

6.52

n = 150

96.66

96.98

8.82

97.02

73.30

6.26

76.38

92.48

4.26

88.68

64.48

9.24

56.00

78.44

5.26

85.00

85.46

6.26

80.20

64.10

4.00

65.50

80.40

3.36

95.42

59.16

8.52

75.14

85.36

5.30

98.08

98.04

4.04

97.86

75.92

11.48

87.74

94.70

9.26

92.56

67.54

10.14

79.74

89.96

9.08

20

based on a normal model, i.e. X k ∼ N(0, Σq×q), for k = 1, . . . , n, where q =(cid:80)p

s=1 qs and



Σq×q =

 .

Σ11 Σ12

. . . Σ1p

Σ21 Σ12

. . . Σ2p

. . .

. . .

. . .

. . .

Σp1 Σp2

. . . Σpp

Table 3 reports the accuracy of the network identiﬁcation and the performance for multiple

testing. Denote Est as the indicator of the true connectivity between region s and region t, and
ˆEa,st as the indicator of the estimated connectivity at the a-th iteration, 1 ≤ s < t ≤ p and
a = 1, . . . , 5000. The nettpr is deﬁned as the percentage of exactly identifying the correct network,

the fwer is the empirical familywise error rate which is the frequency of having one or mode false

discoveries of the functional connectivity over the brain network, and the fdr is the empirical false

discovery rate which is the proportion of falsely detecting the functional connectivities among the

entire detections. Mathematically,

nettpr =

fwer =

fdr =

a=1

1

1

5000

5000(cid:88)
5000(cid:88)
(cid:80)5000
(cid:80)
(cid:80)5000

5000

a=1

(cid:80)

I( ˆEa,st = Est, ∀ 1 ≤ s < t ≤ p),

I( ˆEa,st = 1, Est = 0, ∃ s < t),

a=1

1≤s<t≤p I( ˆEa,st = 1, Est = 0)

a=1

1≤s<t≤p I( ˆEa,st = 1)

.

Table 3 shows the similar pattern as Tables 1 and 2. When the covariance matrix is the identity

matrix, Test I performs better than Test II since the optimization step of Test II introduces extra

errors. In addition, Test I is computationally much faster than Test II. Therefore we recommend

Test I when the covariance matrix is the identity matrix or sparse, and Test II when the precision

matrix is sparse and its inverse is not sparse.

7. APPLICATION

In this section, we demonstrate our method via an analysis of the resting-state fMRI data

that are collected in the autism brain imaging data exchange (ABIDE) study (Di Martino et al.,

2013). The major goal of the ABIDE is to explore the association of brain activity with the autism

spectrum disorder (ASD), which is a widely recognized disease due to its high prevalence and

21

Test I

Test II

nettpr fwer fdr

nettpr fwer fdr

Model 1

Model 2

Model 3

Model 4

Model 5

0.72

0.64

0.24

0.66

0.18

0.02

0.08

0.02

0.04

0.10

0.06

0.04

0.02

0.12

0.07

0.60

0.56

0.68

0.36

0.70

0.02

0.08

0.08

0.02

0.04

0.12

0.16

0.08

0.02

0.06

Table 3: Accuracy of the network identiﬁcation for Tests I and II

substantial heterogeneity in children (Bauman and Kemper, 2005). The ABIDE study collected 20

resting-state fMRI data sets from 17 diﬀerent sites consists of 1,112 individuals with 539 ASDs and

573 age-matched typical controls (TCs). The resting-state fMRI is a popular non-invasive imaging

technique that measures the blood oxygen level to reﬂect the resting brain activity. For each subject,

the fMRI signal was recorded for each voxel in the brain over multiple time points (multiple scans).

The diﬀerent sites in the ABIDE consortium produced diﬀerent number of fMRI scans ranging

from 72 to 310. Several regular imaging preprocessing steps (Di Martino et al., 2013; Huettel et al.,

2004), e.g., motion corrections, slice-timing correction, spatial smoothing, have been applied to the
fMRI data, which were registered into the MNI space (image size: 91× 109× 91(2mm3)) consisting

of 228,483 voxels. We concentrate on the network identiﬁcation over 90 regions in the brain, with

regions deﬁned according to the AAL system.

We take a whitening transformation of original fMRI signals using the AR(1) model (Worsley

et al., 2002) to remove the temporal correlations. The de-trending and de-meaning procedures are

also applied for original fMRI signals. We perform the principal component analysis (PCA) to

summarize the voxel-level fMRI time series into a relatively small number of principal component

signals within each region. The number of signals is chosen according to the criterion of the cumu-

lative variance contribution being larger than 90%. The mean number of the principal components

over 90 regions is 18 ranging from 6 to 36. We apply the proposed methods to identify the resting

state brain network for each subject. The network for a group of subjects is deﬁned by including the

connections for regions i and j if they are connected over 85% of subject-level networks. The ASD

22

patient and control network include 445 connections and the 502 connections respectively, where

numbers of unique connections are 31 and 88. The number of connections shared by both groups is

441. The control network is denser than the ASD patient network. Figure 1 shows the unique con-

nections for the ASD patient network and the health control network. In the ASD patient network,

there are two “hub” brain regions that have at least 4 unique connections to other regions in the

brain. They are the medial part of the superior frontal gyrus (SFGmed-R) and Gyrus rectus (REC).

These regions were demonstrated in the previous references (Baron-Cohen et al., 1999; Tsatsanis

et al., 2003; Hardan et al., 2006; Oblak et al., 2011) to be strongly associated with Autism. Our

results suggest that Autism patients have active region-level functional connectivity to these three

regions, while the controls does not have those network. On the other hand, in the health control

network, there are three “hub” regions that have at least 7 connections. They are the dorsolateral

part of right superior frontal gyrus (SFGdor-R), the left middle frontal gyrus (MFG-L) and the

right middle frontal gyrus (MFG-R). Our results suggest that the Autism patients break the most

of the connections to these three regions. The brain functions of these regions are consistent with

the Autism clinical symptom. For example, the superior fontal gyrus is known for being involved

in self-awareness, in coordination with the action of the sensory system (Goldberg et al., 2006).

8. DISCUSSION

In additional to this, the novel contributions of our work include: 1) we propose a new framework

to identify the functional brain network using formal statistical testing procedures, which make full

use of the massive voxel-level brain signals and incorporate the brain anatomy into the analysis,

producing neurologically more meaningful interpretations. 2) we establish the statistical theory of

the proposed testing procedures, which provides the solid foundation for making valid inference on

the functional brain network. 3) the proposed method is computationally very eﬃcient and can be

paralleled to achieve fast computing performance. 4) Although the development of our proposed

approach is motivated by the analysis of brain imaging data, it is a general method for network

construction and can be readily applied to other problems, such as identiﬁcation of gene networks

and social networks.

23

ASD Patient Brain Network

Health Control Brain Network

Figure 1: Identiﬁed region-level resting state brain networks for ASD patient group and health

control group

24

ACKNOWLEDGEMENT

Jian Kang’s research was partially supported by the National Center for Advancing Trans-

lational Sciences of the National Institutes of Health under Award Number UL1TR000454 and

NIH grant 1R01MH105561. We thank the autism brain imaging data exchange (ABIDE) study

(Di Martino et al., 2013) shares the resting-state fMRI data.

SUPPLEMENTARY MATERIAL

The supplementary material includes the proof of and all technical lemmas, and the simulated

network (Figure 1) on 90 regions using Er¨dos-R´enyi model discussed in Section 6.2.

PROOF OF MAIN THEOREMS

Without loss of generality, in this section, we assume E(Xk,s,i) = E(Xk,t,j) = 0, and Var(Xk,s,i) =

Var(Xk,t,j) = 1 unless otherwise stated. Due to the space limit, we list the proofs of some theorems

(Theorem 2, Theorem 4, Theorem 5, Proposition 3 and Proposition 4) here. Theorem 6 follows

similar arguments of Theorem 2, and Theorem 7 follows that of Theorem 4. The proof of Theorem 1

is relatively long and the main techniques follows the proof of Theorem 1 in Cai et al. (2013), and

thus is placed in the supplementary material.

In addition, to simplify the notation in the proof, we denote by dst = qsqt the total number of
entries in the covariance matrix Υst. And also deﬁne c(dst, α) = 2 log(dst)− log log(dst) + qα, where
qα is the (1 − α)th quantile of null distribution F (x).

To prove Theorem 2, we need Lemma 1 and Lemma 2.

Lemma 1. Recall that θ1,st,ij = σss,iiσtt,jj and ˆθ1,st,ij = ˆσss,iiˆσtt,jj. Under the conditions of (C1.2)
or (C1.2*) and the null H0,st, there exists some constant C > 0, such that as n, q0 → ∞,

(cid:40)

P

max

i,j

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)1 − ˆθ1,st,ij

θ1,st,ij

(cid:41)

1

(log q0)2

= O(q−1

0 + n−/4).

(A.1)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≥ C
(cid:27)

Lemma 2. Recall that θst,ij = Var{(Xk,s,i − µs,i)(Xk,t,j − µt,j)}. Under the conditions of (C1.2)
or (C1.2*), we have for some constant C > 0 that

(cid:26)

P

max
(i,j)∈A

(˜σst,ij − σst,ij)2

θst,ij/n

≥ x2

≤ C|A|(1 − Φ(x)) + O(q−M

0 + n−/8)

(A.2)

25

uniformly for 0 ≤ x ≤ (8 log q0)1/2 and A ⊆ {(i, j) : 1 ≤ i ≤ qs, 1 ≤ j ≤ qt}. Under H0,st, (A.2)
also holds when substituting θst,ij to θ1,st,ij.

Proof of Theorem 2. Deﬁne

nˆσ2
st,ij
Tst,2 = max
θ1,st,ij
n(ˆσst,ij − σst,ij)2

i,j

θ1,st,ij

Tst,4 = max

ij

, Tst,3 = max

i,j

, Tst,5 = max

ij

,

nσ2
st,ij
θ1,st,ij
n(ˆσst,ij − σst,ij)2

θst,ij

.

By Lemma 1,

P(T 1

st > qα) ≥ P{Tst,2 ≥ c(dst, α)(1 + o(1))}.

Since Tst,3 ≤ 2Tst,4 + 2Tst,2 and Tst,3 ≥ 4(1 + κ1) log dst,

P{Tst,2 ≥ c(dst, α)(1 + o(1))}

≥ P{Tst,3 − 2Tst,4 ≥ 2c(dst, α)(1 + o(1))}
= P{Tst,4 ≤ Tst,3/2 − c(dst, α)(1 + o(1))}
= P{Tst,4 ≤ (2κ1 log dst + log logdst −qα)(1 − o(1))}.

By Condition (1.3), Tst,5 ≥ Tst,4/κ1. It follows that

P{Tst,4 ≤ (2κ1 log dst + log logdst −qα)(1 + o(1))}

≥ P{Tst,5 ≤ (2 log dst + (1/κ1) log log dst − (1/κ1)qα)(1 − o(1))}.

By Lemma 2,

P{Tst,5 ≤ (2 log dst + (1/κ1) log log dst − (1/κ1)qα)(1 − o(1))} → 1.

Proof of Theorem 4. It suﬃces to show the results for normal distribution which satisﬁes (C2) and
(C2*). Denote min(qs, qt) = q∗(s, t). Let M(s, t) = {S : S ⊆ {1, . . . , q∗}, Card(S) = rst} denote
the set of all the subsets of {1, . . . , q∗} with cardinality rst. Let ˆm be a random subset of {1, . . . , q∗},
which is uniformly distributed on M. Consider such covariance matrix of (X s, X t)T:

Iqs×qs Σ∗

st, ˆm
Σ∗T
st, ˆm Iqt×qt

 ,

Σ∗
ˆm =

and Σ∗

st, ˆm = (σst,ij)qs×qt,

26

with

σst,i1i1 = ρ = c(log dst/n)1/2, σst,i2i2 = σst,ij = 0

(i1 ∈ M(s, t), i2 ∈ M(s, t)c, j (cid:54)= i).

Here c is a positive constant which will be speciﬁed later. Without loss of generality, suppose qs ≤ qt.
Let’s reorder the variables X = (Xs,1, Xt,1, . . . , Xs,qs, Xt,qs, . . . , Xt,qt)T. Then the covariance matrix

of X is Σ ˆm = diag(A(i), . . . , A(i), Iqt−qs), with

A(i) =

and A(i) = I2 if i ∈ ˆmc.

ρ 1

 if i ∈ ˆm;
1 ρ
 if i ∈ ˆm;
 1 −ρ

−ρ

1

It is easy to see that the precision matrix is Ω ˆm = diag(B(i), . . . , B(i), Iqt−qs), with

A(i) =

1

1 − ρ2

and A(i) = I2 if i ∈ ˆmc.

We construct a class of Σ: Q = {Σ ˆm, ˆm ∈ M(s, t)}. Let Σ0 = I, and Σ1 be uniformly distributed
F = rstρ2}. Let
on Q. Let µρ be the distribution of Σ1. It is a measure on {∆ ∈ S(rst, s, t) : (cid:107)∆(cid:107)2
dPa(X) be the likelihood function given Σa, a = 0, 1. Deﬁne

(cid:26) dP1(X)

(cid:27)

dP0(X)

,

Lµρ(X) = Eµρ

where Eµρ is the expectation on Σ ˆm. By the arguments in Section 7.1 in Baraud (2002), it suﬃces

to show that E0(L2

µρ) ≤ 1 + o(1).

We have

k=1

(cid:34) n(cid:89)
(cid:40) n(cid:89)
(cid:34) n(cid:89)

k=1

E0

Lµρ = E ˆm

(cid:1) (cid:88)
(cid:88)

m∈M

(cid:34)
1(cid:0) q∗
1(cid:0) q∗
(cid:1)2

rst

rst

=

E0(L2

µρ) = E0

(cid:26)

− 1
2

1

|Σ ˆm|1/2

exp

k (Ω ˆm − I)X k

X T

(cid:18)

(cid:27)(cid:35)
(cid:19)(cid:41)(cid:35)2

Let E0 be the expectation on X k with N(0, I) distribution. Then

1

|Σm|1/2

exp

− 1
2

X T

k (Ωm − I)X k
(cid:26)

(cid:27)(cid:35)

m,m(cid:48)∈M

k=1

1

|Σm|1/2

1

|Σm(cid:48)|1/2

exp

− 1
2

k (Ωm + Ωm(cid:48) − 2I)X k

X T

Set Ωm + Ωm(cid:48) − 2I = (as1,s2,i,j), s1, s2 ∈ {s, t}, i = 1, . . . , qs1, and j = 1, . . . , qs2. If i ∈ m ∩ m(cid:48),
ass,ii = att,ii = 2ρ2/(1 − ρ2), ast,ii = −2ρ/(1 − ρ2). If i ∈ m∆m(cid:48), ass,ii = att,ii = 1/(1 − ρ2) − 1,

27

ast,ii = −ρ/(1 − ρ2). Otherwise, as1,s2,i,j = 0. Now let t = |m ∩ m(cid:48)|. By simple calculations, we
have

E0(L2

µρ) =

1tn(1 − ρ2)(2rst−t)n/2

rst

(cid:19)(cid:18)rst
(cid:18) q∗
rst(cid:88)
(cid:1)2 (1 − ρ2)−nrst
1(cid:0) q∗
(cid:19)
(cid:19)(cid:18)q∗ − rst
(cid:18) q∗
(cid:19)−1 rst(cid:88)
(cid:18)rst
(cid:19)(cid:18) s
(cid:18)rst
rst(cid:88)
≤ q∗rst (q∗ − rst)!
(cid:18)

rst − t

q∗!

q∗

rst

rst

t=0

t=1

t=0

=

t

t

rst

= (1 + o(1))

1 +

q∗(1 − ρ2)n/2

(cid:19)(cid:18)q∗ − rst

(cid:19)

rst − t

t

(cid:19)tn/2

(1 − ρ2)−tn/2

(cid:19)t(cid:18) 1
(cid:19)rst

1 − ρ2

≤ exp{rst log(1 + rstq∗c2−1)}(1 + o(1))
≤ exp(r2

stq∗c2−1)(1 + o(1))

For suﬃciently small c2, E0(L2

µρ) = 1 + o(1), and the theorem is proved.

Proof of Theorem 5. Deﬁne

Tst = n max

ρε,st,
n(˜σε,st,ij − σε,st,ij)2

ij

,

θε,st,ij

˜Tst = max
ij

ˆTst = max
i,j

˘Tst = max
i,j

n(ˆσε,st,ij − σε,st,ij)2

θε,st,ij

n(˘σε,st,ij − σε,st,ij)2

θε,st,ij

,

where

n(cid:88)

k=1

ˆσε,st,ij =

ˆεk,s,i ˆεk,t,j/n,

˜σε,st,ij =

n(cid:88)

k=1

˜εk,s,i ˜εk,t,j/n,

˘σε,st,,ij =

n(cid:88)

k=1

εk,s,iεk,t,j/n.

By Condition (2.3) and maxi|˜σε,ss,ii − σε,ss,ii| = OP{(log q0)−1−α0},

|ˆθε,st,ij − θε,st,ij| ≤ |ˆσε,ss,iiˆσε,tt,jj − σε,ss,iiσε,tt,jj|

≤ OP {max(|ˆσε,ss,ii − σε,ss,ii|,|ˆσε,tt,jj − σε,tt,jj|)} = OP{(log q0)−1−α0}.

By (C2.2), θε,st,ij ≥ 1/c2

0. Thus with proability tending to one,

|Tst − ˆTst| ≤ C ˆTst(log q0)−1−α0
| ˆTst − ˜Tst| ≤ C(log q0)−1−α0
| ˘Tst − ˜Tst| ≤ Cn( max
1≤i≤qs

¯ε4
s,i + max
1≤j≤qt

t,j) + Cn1/2 ˘T 1/2
¯ε4

st ( max
1≤i≤qs

¯ε2
s,i + max
1≤j≤qt

¯ε2
t,j).

28

The second inequality above is by Condition (C2.3). Note that

|¯εs,i| + max
1≤t≤qt
Thus, it suﬃces to show that for any x ∈ R,

max
1≤i≤qs

|¯εt,j| = OP ((log q0/n)1/2),

P{ ˘Tst ≤ 2 log dst − 2 log log(dst) + x} → exp

The rest of the proof is similar to the proof of Theorem 1.

(cid:26)

(cid:17)(cid:27)

.

(cid:16)− x

2

− 1
π1/2

exp

Proof of Proposition 3. We ﬁrst decompose ˆσε,st,ij as follows:

n(cid:88)

k=1

1
n

where

n(cid:88)

k=1

ˆεk,s,i ˆεk,t,j =

1
n

˜εk,s,i ˜εk,t,j − A1,s,t,i,j − A2,s,t,i,j + A3,s,t,i,j,

n(cid:88)
n(cid:88)

k=1

k=1

1
n

1
n

A1,s,t,i,j =

A2,s,t,i,j =

˜εk,s,i(X k,t,−j − ¯X t,−j)T(ˆβt,j − βt,j)

˜εk,t,j(X k,s,−i − ¯X s,−i)T(ˆβs,i − βs,i)

A3,s,t,i,j = (ˆβs,i − βs,i)T ˆΣst,−i,−j(ˆβt,j − βt,j)

We bound each term in order.

Note that for all s, t ∈ {1, . . . , p},

|A1,s,t,i,j| ≤

˜k,s,i(X k,t,−j − ¯X k,t,−j) − Cov(˜εk,s,i, X k,t,−j)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) 1
n(cid:88)
+(cid:12)(cid:12)Cov(˜εk,s,i, X T

k=1

n

k,s,−j)(cid:0) ˆβt,j − βt,j)|.

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)∞

(cid:12)(cid:12)(cid:12)ˆβt,j − βt,j

(cid:12)(cid:12)(cid:12)1

(A.3)

(cid:41)

= O(q−M

0

).

(cid:40)

P

max

1≤i≤qs,1≤j≤qt

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) 1

n

n(cid:88)

k=1

And also for any M > 0, there exists suﬃciently large C > 0 such that

˜εk,s,i(Xk,t,−j − ¯Xt,−j) − Cov(˜εk,s,i, X k,t,−j)

≥ C(log dst/n)1/2

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)∞
as,1(log qs/n)1/2(cid:111)
(cid:110)

.

Recall the deﬁnition of av,1 and av,2 in (13).

When s = t and i = j, Cov(˜εk,s,i, X k,s,−i) = 0. Therefore

|A1,s,s,i,i| = OP

max
1≤i≤qs

29

When s (cid:54)= t, under H0,st, Cov(˜εk,s,i, X k,t,−j) = 0. Therefore

max

1≤i≤qs,1≤j≤qt

|A1,s,t,i,j| = OP

(cid:110)
at,1(log dst/n)1/2(cid:111)
k,s,−j)(cid:0) ˆβt,j − βt,j)| ≤ {Var(˜εk,s,i)}1/2(cid:110)

.

When s (cid:54)= t and under H1,st,

(cid:12)(cid:12)Cov(˜εk,s,i, X T

(ˆβt,j − βt,j)TΣtt,−j,−j(ˆβt,j − βt,j)

(cid:111)1/2

≤ c0at,2

Therefore,

max

1≤i≤qs,1≤j≤qt

|A1,s,t,i,j| = OP

(cid:104)

at,1(log dst/n)1/2 + at,2

(cid:105)

We can show bounds for A2,s,t,i,j similarly.

Next, we bound A3,s,t,i,j.

A3,s,t,i,j =(ˆβk,s,i − βk,s,i)T( ˆΣst,−i,−j − Σst,−i,−j)(ˆβk,t,j − βk,t,j)

It is easy to show that for any M > 0, there exists suﬃciently large C > 0 such that

+ (ˆβk,s,i − βk,s,i)TΣst,−i,−j(ˆβk,t,j − βk,t,j)
(cid:27)

|ˆσst,ij − σst,ij| ≥ C(log dst/n)1/2

(cid:26)

P

max

1≤i≤qs,1≤j≤qt

= O(q−M

0

).

When s (cid:54)= t, under H0,st, Σst,−i,−j = 0; and under H1,st, (cid:107)Σst,−i,−j(cid:107)2 ≤ c0. By the inequality

|ˆβk,s,i − βk,s,i|1|ˆβk,t,j − βk,t,j|1,

(A.4)

(cid:12)(cid:12)(cid:12)(ˆβk,s,i − βk,s,i)T( ˆΣst,−i,−j − Σst,−i,−j)(ˆβk,t,j − βk,t,j)
(cid:12)(cid:12)(cid:12)
≤(cid:12)(cid:12)(cid:12) ˆΣst,−i,−j − Σst,−i,−j
(cid:12)(cid:12)(cid:12)∞
(cid:110)
as,1at,1(log dst/n)1/2(cid:111)

|A3,s,t,i,j| = OP

max

we have under H0,st,

;

1≤i≤qs,1≤j≤qt

and under H1,st,

max

1≤i≤qs,1≤j≤qt

|A3,s,t,i,j| = OP

(cid:110)

as,1at,1(log dst/n)1/2 + as,2at,2

(cid:111)

.

When s = t, we can show by similar argument that under H0,st,

(cid:110)

s,1(log qs/n)1/2(cid:111)

a2

;

max

1≤i≤qs,1≤j≤qt

|A3,s,s,i,j| = OP

30

ˆεk,s,i ˆεk,t,j =

˜εk,s,i ˜εk,t,j+OP

(as,1at,1 + as,1 + at,1)

+ (as,2at,2 + as,2 + at,2)

.

(cid:40)

(cid:110)

(cid:40)

.

(cid:111)
(cid:18) log dst
(cid:19)1/2

n

(cid:18) log dst

n

(cid:19)1/2(cid:41)

;

(A.5)

(cid:41)

(cid:19)1/2(cid:41)
(cid:18) log qs
(cid:18) log qs
(cid:19)1/2

n

+ a2
s,2

n

(A.6)

;

(A.7)

(cid:41)

.

(A.8)

|A3,s,s,i,j| = OP

a2
s,1(log qs/n)1/2 + a2
s,2

˜εk,s,i ˜εk,t,j + OP

(as,1at,1 + as,1 + at,1)

and under H1,st,

max

1≤i≤qs,1≤j≤qt

Therefore, when s (cid:54)= t, under H0,st

n(cid:88)

k=1

1
n

n(cid:88)

ˆεk,s,i ˆεk,t,j =

1
n

k=1

(cid:40)

and under H1,st,

n(cid:88)

k=1

1
n

When s = t and i = j, under H0,st,

n(cid:88)

k=1

1
n

1
n

n(cid:88)
n(cid:88)

k=1

ˆε2
k,s,i =

(a2

s,1 + as,1)

1
n

n(cid:88)
n(cid:88)

k=1

˜ε2
k,s,i + OP

(cid:40)

and under H1,st,

1
n

ˆε2
k,s,i =

1
n

k=1

k=1

˜ε2
k,s,i + OP

(a2

s,1 + as,1)

It then suﬃces to show that for v = 1, . . . , p, av,2 = OP{(log q0)−1−α0} and av,1 = OP{n(log q0)−2−α0}.
By the proof of Proposition 4.1 in Liu (2013), page 2975, with probability tending to 1,

|D

−1/2
v,i

ˆΣvv,−i,−i ˆβv,i − D

−1/2
v,i bv,i|∞ ≤ λv,i(2).

|D

−1/2
v,i

ˆΣvv,−i,−i(ˆβv,i − βv,i)|∞ ≤ 2λv,i(2).

λmin(Σ)(n/ log q0)1/2(cid:111)
(cid:110)

|βv,i|0 = o

max
1≤i≤qv

And it follows that

And also by

and the inequality

δT ˆΣvv,−i,−iδ ≥ λmin(Σ−i,−i)|δ|2

2 − OP{(log q0/n)1/2}|δ|1,

we can see that the restricted eigenvalue assumption RE(s, s, 1) in Bickel et al. (2009), page 1711,
holds with κ(s, s, 1) ≥ cλmin(Σ)1/2. And by the proof of Theorem 7.1 in Bickel et al. (2009),

(cid:34)(cid:26)

(cid:27)1/2 {λmin(Σ)}−1

(cid:35)

|βv,i|0(log qn/n)

av,1 = OP

max
1≤i≤qv

|βv,i|0(log qv/n)1/2

(cid:26)

(cid:27)

,

av,2 = OP

max
1≤i≤qv

31

Proof of Proposition 4. By Proof of Proposition 4.2 in Liu (2013), we have with probability tending

to one,

|D

−1/2
v,i

ˆΣvv,−i,−iD

−1/2
v,i

( ˆαv,i − D1/2

v,i βv,i)|∞ ≤ 2λv,i(δ).

Then by (A.5), (A.6), (A.7), (A.8), and the proof of Theorem 7.2 in Bickel et al. (2009), we get

Condition (2.3) holds for βv,i(δ) with δ > 2.

32

REFERENCES

Akaike, H. (1969), “Fitting autoregressive models for prediction,” Annals of the Institute of Statis-

tics Mathematics, 21-1, 243–247.

Anderson, T. W. (2003), An introduction to multivariate statistical analysis, Wiley-Interscience.

Andrews-Hanna, J. R., Snyder, A. Z., Vincent, J. L., Lustig, C., Head, D., Raichle, M. E., and

Buckner, R. L. (2007), “Disruption of large-scale brain systems in advanced aging,” Neuron, 56,

924–935.

Baraud, Y. (2002), “Non asymptotic minimax rates of testing,” Bernoulli, 8, 577–606.

Baron-Cohen, S., Ring, H. A., Wheelwright, S., Bullmore, E. T., Brammer, M. J., Simmons, A.,

and Williams, S. C. (1999), “Social intelligence in the normal and autistic brain: an fMRI study,”

European Journal of Neuroscience, 11, 1891–1898.

Bauman, M. L. and Kemper, T. L. (2005), The neurobiology of autism, JHU Press.

Belloni, A., Chernozhukov, V., and Wang, L. (2011), “Square-root Lasso: Pivotal recovery of sparse

signals via conic programming,” Biometrika, 98, 791–806.

Bickel, P., Ritov, Y., and Tsybakov, A. (2009), “Simultaneous analysis of Lasso and Dantzig

selector,” The Annals of Statistics, 37-4, 1705–1732.

Birke, M. and Holder, D. (2005), “A note on testing the covariance matrix for large dimension,”

Statistics and Probaiblity Letters, 74-3, 281–289.

Biswal, B., Zerrin Yetkin, F., Haughton, V. M., and Hyde, J. S. (1995), “Functional connectivity in

the motor cortex of resting human brain using echo-planar mri,” Magnetic resonance in medicine,

34, 537–541.

Bowman, F. D., Zhang, L., Derado, G., and Chen, S. (2012), “Determining functional connectivity

using fMRI data with diﬀusion-based anatomical weighting,” NeuroImage, 62, 1769–1779.

Bullmore, E. and Sporns, O. (2009), “Complex brain networks: graph theoretical analysis of struc-

tural and functional systems,” Nature Reviews Neuroscience, 10, 186–198.

33

Cai, T., Liu, W., and Xia, Y. (2013), “Two-sample covariance matrix testing and support recovery

in high-dimensional and sparse settings,” Journal of American Statistical Association, 108, 265–

277.

Cai, T. and Ma, Z. (2013), “Optimal hypothesis testing for high dimensional covariance matrices,”

Bernoulli, 19, 2359–2388.

Candes, E. and Tao, T. (2007), “The Dantzig selector: Statistical estimation when p is much larger

than n,” Annals of Statistics, 35, 2313–2351.

Chen, S., Zhang, L., and Zhong, P. (2010), “Tests for high-dimensional covariance matrices,”

Journal of the Americal Statistical Association, 105-490, 810–819.

Cherkassky, V. L., Kana, R. K., Keller, T. A., and Just, M. A. (2006), “Functional connectivity in

a baseline resting-state network in autism,” Neuroreport, 17, 1687–1690.

Cordes, D., Haughton, V. M., Arfanakis, K., Wendt, G. J., Turski, P. A., Moritz, C. H., Quigley,

M. A., and Meyerand, M. E. (2000), “Mapping functionally related regions of brain with func-

tional connectivity MR imaging,” American Journal of Neuroradiology, 21, 1636–1644.

Di Martino, A., Yan, C., Li, Q., Denio, E., Castellanos, F., Alaerts, K., Anderson, J., Assaf, M.,

Bookheimer, S., Dapretto, M., et al. (2013), “The autism brain imaging data exchange: towards

a large-scale evaluation of the intrinsic brain architecture in autism,” Molecular psychiatry.

Erd¨os, P. and R´enyi, A. (1960), “On the evolution of random graphs,” Publications of the Mathe-

matical Institute of Hungarian Academy of Sciences, 5, 17–61.

Fan, J. and Li, R. (2001), “Variable selection via nonconcave penalized likelihood and its oracle

properties,” Journal of the American Statistical Association, 96-456, 1348–1360.

Friman, O. and Westin, C.-F. (2005), “Resampling fMRI time series,” NeuroImage, 25, 859–867.

Goldberg, I. I., Harel, M., and Malach, R. (2006), “When the brain loses its self: prefrontal

inactivation during sensorimotor processing,” Neuron, 50, 329–339.

34

Hardan, A. Y., Girgis, R. R., Adams, J., Gilbert, A. R., Keshavan, M. S., and Minshew, N. J. (2006),

“Abnormal brain size eﬀect on the thalamus in autism,” Psychiatry Research: Neuroimaging, 147,

145–151.

Honorio, J., Samaras, D., Paragios, N., Goldstein, R., and Ortiz, L. (2009), “Sparse and locally

constant Gaussian graphical models,” Advances in Neural Information Processing Systems, 745–

753.

Hotelling, H. (1953), “New light on the correlation coeﬃcient and its transforms,” Journal of the

Royal Statistical Society. Series B (Methodological), 15, 193–232.

Huang, S., Li, J., Sun, L., Fieisher, A., T., W., K., C., and Reiman, E. (2010), “Learning brain

connectivity of Alzheimer’s disease by sparse inverse covariance estimation,” Neuroimage, 50-3,

935–949.

Huang, S., Li, J., Sun, L., Liu, J., Wu, T., Chen, K., Fleisher, A., Reiman, E., and Ye, J. (2009),

“Learning Brain Connectivity of Alzheimer’s Disease from Neuroimaging Data.” in NIPS, vol. 22,

pp. 808–816.

Huettel, S. A., Song, A. W., and McCarthy, G. (2004), Functional magnetic resonance imaging,

vol. 1, Sinauer Associates Sunderland, MA.

Jing, B., Shao, Q., and Wang, Q. (2003), “Self-normalized Cram´er-type large deviations for inde-

pendent random variables,” The Annals of Probability, 31, 2167–2215.

Koshino, H., Carpenter, P. A., Minshew, N. J., Cherkassky, V. L., Keller, T. A., and Just, M. A.

(2005), “Functional connectivity in an fMRI working memory task in high-functioning autism,”

Neuroimage, 24, 810–821.

Ledoit, O. and Wolf, M. (2002), “Some hypothesis test for the covariance matrix when the dimension

is large compared to the sample size,” The Annals of Statistics, 30-4, 1081–1102.

Li, J. and Chen, S. (2012), “Two sample tests for high-dimensional covariance matrices,” Annals

of Statistics, 40, 908–940.

35

Li, M. and Qin, Y. (2014), “Hypothesis testing for high-dimensional covariance matrices,” JOurnal

of Multivariate Analysis, 128, 108–119.

Lindquist, M. (2008), “The statistical analysis of fMRI data,” Statistical Science, 23-4, 439–463.

Liu, W. (2013), “Gaussian graphical model estimation with false discovery rate control,” Annals of

Statistics, 41-6, 2948–2978.

Marrelec, G., Krainik, A., Duﬀau, H., P´el´egrini-Issac, M., Leh´ericy, S., Doyon, J., and Benali, H.

(2006), “Partial correlation for functional brain interactivity investigation in functional MRI,”

Neuroimage, 32, 228–237.

Mazumder, R. and Hastie, T. (2012), “The graphical lasso: New insights and alternatives,” Elec-

tronic Journal of Statistics, 6, 2125–2149.

Mazziotta, J. C., Toga, A. W., Evans, A., Fox, P., and Lancaster, J. (1995), “A probabilistic atlas

of the human brain: theory and rationale for its development the international consortium for

brain mapping (ICBM),” Neuroimage, 2, 89–101.

Oblak, A. L., Gibbs, T. T., and Blatt, G. J. (2011), “Reduced GABA receptors and benzodiazepine

binding sites in the posterior cingulate cortex and fusiform gyrus in autism,” Brain research, 1380,

218–228.

Richard, J. and Yuan, M. (2012), “Independent component analysis via nonparametric maximum

likelihood estimation,” Annals of Statistics, 40-6, 2973–3002.

Rubinov, M. and Sporns, O. (2010), “Complex network measures of brain connectivity: uses and

interpretations,” Neuroimage, 52, 1059–1069.

Schott, J. (2007), “A test for the equality of covariance matrices when the dimention is large relative

to the sample sizes,” Computational Statistics and Data Analysis, 51, 6535–6542.

Sporns, O., Chialvo, D. R., Kaiser, M., and Hilgetag, C. C. (2004), “Organization, development

and function of complex brain networks,” Trends in cognitive sciences, 8, 418–425.

Sun, T. and Zhang, C. (2012), “Scaled sparse linear regression,” Biometrika, 99, 879–898.

36

Supekar, K., Menon, V., Rubin, D., Musen, M., and Greicius, M. D. (2008), “Network analysis

of intrinsic functional brain connectivity in Alzheimer’s disease,” PLoS computational biology, 4,

e1000100.

Tibshirani, R. (1996), “Regression shrinkage and selection via the lasso,” Journal of the Royal

Statistical Society Series B, 58, 267–288.

Tsatsanis, K. D., Rourke, B. P., Klin, A., Volkmar, F. R., Cicchetti, D., and Schultz, R. T. (2003),

“Reduced thalamic volume in high-functioning individuals with autism,” Biological psychiatry,

53, 121–129.

Tzourio-Mazoyer, N., Landeau, B., Papathanassiou, D., Crivello, F., Etard, O., Delcroix, N., Ma-

zoyer, B., and Joliot, M. (2002), “Automated anatomical labeling of activations in SPM using

a macroscopic anatomical parcellation of the MNI MRI single-subject brain,” Neuroimage, 15,

273–289.

Velioglu, B., Aksan, E., Onal, I., Firat, O., Ozay, M., and Yarman Vural, F. (2014), “Functional

networks of anatomic brain regions,” 2014 IEEE 13th International Conference on Cognitive

Informatics and Cognitive Computing.

Weiss, Y. and Freeman, W. T. (2001), “Correctness of belief propagation in Gaussian graphical

models of arbitrary topology,” Neural computation, 13, 2173–2200.

Worsley, K. J., Liao, C., Aston, J., Petre, V., Duncan, G., Morales, F., and Evans, A. (2002), “A

general statistical analysis for fMRI data,” Neuroimage, 15, 1–15.

Za¨ıtsev, A.Y. (1987), “On the Gaussian approximation of convolutions under multidimensional

analogues of S.N. Bernstein’s inequality conditions,” Probility Theory and Related Fields, 74,

535–566.

Zou, H. (2006), “The adaptive Lasso and its oracle properties,” Journal of the American Statistical

Association, 101-476.

37

Supplementary Material for “High Dimensional Tests for

Functional Brain Networks”

S.1.

PROOF OF OTHER THEOREMS

Lemma 3. For any ﬁxed integer D ≥ 1 and real number x ∈ R,

(cid:16)|ND|min ≥ y(dst, x)1/2 ± n(log q0)−1/2(cid:17)

(cid:88)

P

(cid:26) 1√

π

(cid:17)(cid:27)D

(cid:16)− x

2

exp

(1 + o(1)).

=

1
D!

1≤k1<...<kD≤K

Proof of Theorem 1. Without loss of generality, we assume that µs,i = µt,j = 0, σss,ii = σtt,jj = 1,
for i = 1, . . . , qs, and j = 1, . . . , qt. To simplify notation, let T = n · maxij ˆρst,ij.

Deﬁne

ˆT = max
i,j

(ˆσst,ij − σst,ij)2

θst,ij/n

,

By Lemma 1, with probability at least 1 − O(q−1

and ˜T = max
i,j
0 + n−/8),

(˜σst,ij − σst,ij)2

θst,ij/n

|T − ˆT| ≤ C ˆT

| ˆT − ˜T| ≤ max

ij

≤ max

ij

(log q0)2

1

(cid:12)(cid:12)(cid:12)(cid:12) (ˆσst,ij − ˜σst,ij)(ˆσst,ij + ˜σst,ij − 2σst,ij)
(cid:12)(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ( ¯Xs,i ¯Xt,j)(cid:0)2˜σst,ij − 2σst,ij − ¯Xs,i ¯Xt,j
(cid:1)
(cid:18)
(cid:18)

θst,ij/n

θst,ij/n

(cid:19)

max

i

¯X 2

s,i + max

¯X 2
t,j

j

+ 2n

≤ n1/2 ˜T 1/2

(cid:19)

i

j

¯X 4

¯X 4
t,j

max

s,i + max

(cid:8)(log q0/n)1/2(cid:9). Set y(dst, x) =
(cid:17)(cid:27)
(cid:16)− x

.

(cid:26)

By similar arguments as (9) and (11), maxi| ¯Xs,i| + maxj| ¯Xt,j| = OP
2 log dst − log log dst + x. By Lemma 2, it suﬃces to show that for any x ∈ R,

P{ ˜T ≤ y(dst, x)} → exp

− 1
π1/2

exp

2

as n and d → ∞.

Let

Let

Ost = {(i, j) : 1 ≤ i ≤ qs, 1 ≤ j ≤ qt},
Ast = {(i, j) : i (cid:54)∈ Ms, i (cid:54)∈ D(1)

s , j (cid:54)∈ Mt, j (cid:54)∈ D(1)
t }.

˜TAst = max
(i,j)∈Ast

n˜σ2
st,ij
θst,ij

,

˜TOst\Ast =

max

(i,j)∈Ost\Ast

n˜σ2
st,ij
θst,ij

.

1

Then

|P{ ˜T ≥ y(dst, x)} − P{ ˜TAst ≥ y(dst, x)}| ≤ P{ ˜TOst\Ast ≥ y(dst, x)}.

Note that Card(Ost \ Ast) = o(dst). Then by Lemma 2,

(cid:110) ˜TOst\Ast ≥ y(dst, x)

(cid:111) ≤ o(dst) · Cd−1

P

st + o(1) = o(1).

It suﬃes to show that for any x ∈ R,

P{ ˜TAst ≤ y(dst, x)} → exp

(cid:110)−π−1/2 exp (−x/2)
(cid:111)

.

as n and q0 → ∞.

We arrange the indices {(i, j) : (i, j) ∈ Ast} in any ordering and set them as {(im, jm) : 1 ≤

m ≤ d1}, with d1 (cid:16) dst. Let θst,l = θst,iljl. For k = 1, . . . , n, deﬁne

Zk,l = Xk,s,ilXk,t,jl − σst,iljl,
ˆZk,l = Zk,lI(|Zk,l| ≤ τn) − E{Zk,lI(|Zk,l| ≤ τn)},
˜Zk,l = Zk,l − ˆZk,l,

k=1

n(cid:88)
n(cid:88)
n(cid:88)

k=1

Vl =

ˆVl =

˜Vl =

Zk,l/(nθl)1/2,

ˆZk,l/(nθl)1/2,

˜Zk,l/(nθl)1/2,

where τn = 8η−1 log(dst + n) if (C1.2) holds, and τn = n1/2/(log dst)2 if (C1.2*) holds. Note that

k=1

under the null, σst,i1j1 = 0. By Markov inequality, under (C1.2),

P(Zk,l > τn) ≤ K2

1 exp(−η/2τn) ≤ (dst + n)−4,

and under (C1.2*),

P(Zk,l > τn) ≤ τ−4−4γ1−

n

2 ≤ C
K2

(log dst)8+8γ1+2

n2+2γ1+/2

.

The later inequality uses the independence between Xk,s,il and Xk,t,jl under H0,st.

2

(cid:18)

Therefore,

P

|Vl − ˆVl| ≥ (log dst + n)−M

max
1≤l≤d1

(cid:19)

= P

≤ P

(cid:26)
(cid:18)

max
1≤l≤d1

max
1≤l≤d1

| ˜Vl| ≥ (log dst + n)−M

(cid:19)

| ˜Zkl| > 0

max
1≤k≤n

(cid:27)

= ndst · P(|Zkl| > τn)
≤ O(d−1

st + n−/4).

(S1)

(S2)

(S3)

P

max
1≤l≤d1

(cid:18)
(cid:12)(cid:12)(cid:12)(cid:12) ≤ 2 max

By Bernstein’s inequality,

(cid:19)

| ˆV 2
l | ≥ (log dst + n)2

≤ O(d−1

(cid:12)(cid:12)(cid:12)(cid:12) max

It is easy to see that with probability larger than 1 − O(d−1

l − max
V 2
1≤l≤d1

|Vl − ˆVl| + max
1≤l≤d1
1≤l≤d1
It suﬃces to prove that for any ﬁxed x ∈ R, as n, d → ∞,

| ˆVl| max
1≤l≤d1

1≤l≤d1

ˆV 2
l

(cid:26)

P

max
1≤l≤d1

(cid:27)
l ≤ y(dst, x)
ˆV 2
(cid:26)

 ≤ P

Elj

 d(cid:92)

(−1)d−1 (cid:88)
2m(cid:88)

d=1

1≤l1<...<ld≤d1

j=1

By Bonferroni inequality, for any integer m with o < m < K/2,

(cid:27)
l ≥ y(dst, x)
ˆV 2

st + n−)
st + n−/4),
|Vl − ˆVl|2 ≤ (log dst + n)−M .

(cid:110)−π−1/2 exp(−x/2)
(cid:111)

→ exp

.

(S4)

where Elj = { ˆV 2
|a|min = min1≤i≤d|ai| for any vector a ∈ Rd. Then,

lj

P

P

d=1

Elj

max
1≤l≤d1

1≤l1<...<ld≤d1

 d(cid:92)

≤ 2m−1(cid:88)

(−1)d−1 (cid:88)

 ,
≥ y(dst, x)}. Let Wk,d = ( ˆZk,l1/(cid:112)θl1, . . . , ˆZk,ld/(cid:112)θld), for 1 ≤ k ≤ n. Deﬁne
 = P
 d(cid:92)
(cid:12)(cid:12)min ≥ y(dst, x)1/2

(cid:16)|Nd|min ≥ y(dst, x) − n(log dst)−1/2(cid:17)

(cid:32)(cid:12)(cid:12)n−1/2
(cid:33)

(cid:12)(cid:12)min ≥ y(dst, x)1/2

n(cid:88)

(cid:33)

≤ P

Wk,d

(S5)

k=1

P

Elj

j=1

j=1

By Theorem 1 in Za¨ıtsev, A.Y. (1987), we have

(cid:32)(cid:12)(cid:12)n−1/2

n(cid:88)

P

Wk,d

k=1

+ c1d5/2 exp

3

(cid:32)

−

n1/2n

c2d5/2τn(log dst)1/2

(cid:33)

,

with c1, c2 > 0 are constants, n → 0 suﬃciently slow, and Nd is a d−dimensional normal vector
with zero mean and Cov(Nd) = Cov(W1,d). Since d is a ﬁxed integer, log q0 (cid:16) log dst = o(n1/5) and
n → 0 suﬃciently slow such that

(cid:32)

(cid:33)

c1d5/2 exp

−

n1/2n

c2d5/2τn(log dst)1/2

= O(q−M

0

).

(cid:26)

(cid:26)

Thus

P

max
1≤l≤d1

and similarly

P

max
1≤l≤d1

d=1

1≤l1<...<ld≤dst

(cid:27)
l ≥ y(dst, x)
ˆV 2
(−1)d−1 (cid:88)
≤ 2m−1(cid:88)
(cid:27)
l ≥ y(dst, x)
ˆV 2
(−1)d−1 (cid:88)
≥ 2m(cid:88)
(cid:19)
(cid:18)
≤ 2m(cid:88)
(cid:19)
(cid:18)
≥ 2m−1(cid:88)

1≤l1<...<ld≤dst

n,q0→∞ P
lim sup

max
1≤l≤d1

ˆV 2
l

d=1

d=1

n,q0→∞ P
lim inf

max
1≤l≤d1

ˆV 2
l

d=1

By Lemma 3, we get

(cid:110)|Nd|min ≥ y(dst, x) − n(log dst)−1/2(cid:111)

+ o(1),

(cid:110)|Nd|min ≥ y(dst, x) + n(log dst)−1/2(cid:111) − o(1),

P

P

(cid:26) 1
(cid:26) 1

π1/2

(−1)d−1 1
d!

exp

(−1)d−1 1
d!

exp

π1/2

(cid:17)(cid:27)d
(cid:16)− x
(cid:17)(cid:27)d
(cid:16)− x

2

2

for any integer m. Let m → ∞, we prove the theorem.

Without loss of generality, in this section, we assume E(Xk,s,i) = E(Xk,t,j) = 0, and Var(Xk,s,i) =

Var(Xk,t,j) = 1 unless otherwise stated.

Proof of Proposition 1. Deﬁne T (1)

st,ij = nˆρst,ij. By the proof of Theorem 1, under (C2) (or (C2*)),

we have

st,ij > qα + 2 log dst − log log dst}

P H0{T (1)
=(1 + o(1))P(|N1| ≥ qα + 2 log dst − log log dst)

=(1 + o(1))

1
dst

log

(cid:18) 1

(cid:19)

1 − α

.

4

Note that T (1)

st = maxi,j T (1)

st,ij − 2 log(dst) + log log(dst). Then

P H0{T (1)

st > q(α)} ≤ dst · P H0{T (1)

st,ij ≥ c(dst, α)} ≤ log

(cid:18) 1

1 − α

(cid:19)

.

Proof of Lemma 1. Under H0,st, θst,ij = σss,iiσtt,jj and ˆθst,ij = ˆσss,iiˆσtt,jj. Thus

|ˆθst,ij − θst,ij|
σss,iiσtt,jj

≤

It suﬃces to show that

(cid:26)

P

max

i

(cid:12)(cid:12)(cid:12)(cid:12) ˆσss,ii

σss,ii

− 1

(cid:12)(cid:12)(cid:12)(cid:12) ·

− 1

(cid:12)(cid:12)(cid:12)(cid:12) ˆσtt,jj
(cid:27)

σtt,jj

σss,ii

(cid:12)(cid:12)(cid:12)(cid:12) ˆσss,ii
(cid:12)(cid:12)(cid:12)(cid:12) ≥ C

3

(cid:12)(cid:12)(cid:12)(cid:12) +

(cid:12)(cid:12)(cid:12)(cid:12) ˆσtt,jj

σtt,jj

(cid:12)(cid:12)(cid:12)(cid:12)

− 1

1

(log q0)2

= O(q−1

0 + n−/8),

(S6)

and the same holds for ˆσtt,jj.

Without loss of generality, we assume that µs,i = µt,j = 0, σss,ii = σtt,jj = 1, for i = 1, . . . , qs,

k,s,i)(cid:9) − ( ¯X s,i)2

We ﬁrst prove the results under (C1.2). Deﬁne Yk,s,i = X 2

k,s,i − E(X 2

k,s,i). Then

and j = 1, . . . , qt. We have

ˆσss,ii
σss,ii

− 1 =

1
n

n(cid:88)

k=1

(cid:8)X 2
k,s,i − E(X 2
(cid:27)
(cid:41)
(cid:41)

(cid:26)

+ P

1

max

(cid:40)

i

3

(cid:12)(cid:12)(cid:12)(cid:12) ≥ C
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≥ C
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≥ C

6

6

1

(log q0)2

(log q0)2

1

(log q0)2

(cid:26)
(cid:40)

P

≤P

max

− 1

max

Yk,s,i

i

σss,ii

(cid:12)(cid:12)(cid:12)(cid:12) ˆσss,ii
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) 1
n(cid:88)
(cid:40)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) 1
n(cid:88)

k=1

n

n

k=1

i

≤q0 · P

Yk,s,i

(cid:27)
(cid:19)1/2(cid:41)

( ¯X s,i)2 ≥ C
6

1

(log q0)2

(cid:18) C

+ q0 · P

¯X s,i ≥

εn

6

(log q0)2

5

Let t1 = η(log q0)1/2/(2n1/2). Then we have

(cid:41)(cid:35)

t1|Yk,s,i|

1

6

n

P

k=1

Yk,s,i

(cid:41)

(log q0)2

n(cid:88)

(cid:40) n(cid:88)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≥ C

≤ exp{−Ct1n/(6(log q0)2)} · E

(cid:40)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) 1
(cid:34)
≤ exp{−Ct1n/(6(log q0)2)} · n(cid:89)
≤ exp{−Ct1nεn/(6 log q0)} · n(cid:89)
(cid:2)1 + E(cid:8)t2
n(cid:88)
E(cid:8)t2

−Ct1n/(6(log q0)2) +

≤ exp

(cid:34)

exp

k=1

k=1

k=1

1Y 2

E{exp(t1|Yk,s,i|)}

1Y 2

k,s,i exp(t1|Yk,s,i|)(cid:9)(cid:3)
k,s,i exp(t1|Yk,s,i|)(cid:9)(cid:35)

k=1

≤ exp(−Cη log q0/12 + cη log q0)
≤Cq−M

,

0

where cη is a positive number only depends on η. Similarly,

(cid:40)

¯X s,i ≥

P

(cid:40)

≤ exp

− η
2

(cid:18) C
(cid:18) Cn

6

1

(log q0)2

(cid:19)1/2(cid:41)
(cid:19)1/2

6(log q0)2

+ cη log q0

(cid:41)

(S7)

(S8)

(S9)

≤Cq−M

0

It remains to prove the lemma under (C1.2*). Deﬁne

ˆYk,s,i = Yk,s,iI(cid:8)|Yk,s,i| ≤ n/(log q0)5(cid:9) − E(cid:2)Yk,s,iI(cid:8)|Yk,s,i| ≤ n/(log q0)5(cid:9)(cid:3) .

Then,

i

P

max

Yk,s,i

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) n(cid:88)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) n(cid:88)

(cid:40)
(cid:41)
(cid:40)
(cid:41)
≤Cq0 exp(cid:8)−C(log q0)2(cid:9) + Cn−/4.

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≥ C
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≥ C

(log q0)2

(log q0)2

ˆYk,s,i

≤P

max

k=1

k=1

1

6

1

6

i

(cid:26)

+ P

|Yk,s,i| ≥

max
i,k

n

(log q0)5

(S10)

(cid:27)

The last inequality is by Bernstein’s inequality and condition (C1.2*). Deﬁne

ˆXk,s,i = Xk,s,iI(|Xk,s,i − ¯Xs,i| ≤ n/(log q0)5) − E(cid:8)Xk,s,iI(|Xk,s,i − ¯Xs,i| ≤ n/(log q0)5)(cid:9) .

6

Then, following the similar argument, we have

1

P

max

¯X s,i ≥

(cid:19)1/2(cid:41)

(cid:40)
(cid:40)
(cid:19)1/2(cid:41)
≤Cq0 exp(cid:8)−C(log q0)4(cid:9) + Cn−2−2γ1−/2.

(cid:18) C
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≥ n

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) n(cid:88)

6(log q0)2

(log q0)2

ˆXk,s,i

(cid:18)

≤P

max

k=1

C

i

i

6

(cid:26)

+ P

|Xk,s,i| ≥ n/(log q0)5

max
i,k

(S11)

(cid:27)

Proof of Lemma 2. Set Yk,st,ij = Xk,s,iXk,t,j − σst,ij. Deﬁne ˜θst,ij = 1
estimator of θst,ij = Var(Xk,s,iXk,t,j). By the proof of Lemma 4 in Cai et al. (2013), it follows that

k,st,ij as an oracle

k=1 Y 2

n

k,st,ij − θst,ij
Y 2

= O(q−M

0 + n−/8),

(S12)

(cid:32)

P

max

ij

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) 1

n

n(cid:88)

k=1

(cid:80)n

(cid:33)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≥ Cεn/ log q0
((cid:80)n
(cid:80)n

k=1 Yk,st,ij)2
k=1 Y 2

k,st,ij

·

(cid:80)n

k=1 Y 2
θst,ij/n

k,st,ij

.

where εn = max{(log q0)1/6/n1/2, (log q0)−1}. We can write

(˜σst,ij − σst,ij)2

θst,ij/n

=

By Theorem 1 in Jing et al. (2003), we have

(cid:40)

((cid:80)n
(cid:80)n

max

i,j

P

k=1 Yk,st,ij)2
k=1 Y 2

k,st,ij

(cid:41)

≥ x2

≤ C(1 − Φ(x)).

Together with (12), we have the conclusion. Note that under the null, θ1,st,ij = θst,ij. So (12) also

holds for θst,ij under H0,st.

Proof of Lemma 3. When d = 1, it is easy to get

(cid:16)|N1|min ≥ y(dst, x)1/2 ± n(log dst)−1/2(cid:17)

P

=

1

dstπ1/2

exp(−x/2)(1 + o(1)).

We now prove the lemma for d ≥ 2. Note that for any 1 ≤ i, j ≤ qs and 1 ≤ k, l ≤ qt, under H0,st,
we have

Cov(Xs,iXt,k, Xs,jXt,l) = σss,ijσtt,kl.

To simplify notation, denote Xs,i by Xim1

graph Gim1 jm1 im2 jm2

= (Vim1 jm1 im2 jm2

is the set of vertices and Eim1 jm1 im2 jm2

, Eim1 jm1 im2 jm2

Xs,j by Xim2

, Xt,k by Xjm1

. Deﬁne
= {im1, jm1, im2, jm2}
is the set of edges. There is an edge between a (cid:54)= b ∈

), where Vim1 jm1 im2 jm2

, and Xt,l by Xjm2

7

{im1, jm1, im2, jm2} if and only if |ρss,ij| = |ρim1 im2
(log q0)−1−α0, for all a, b ∈ {im1, jm1, im2, jm2}. Gim1 jm1 im2 jm2
ber of diﬀerent vertices in Vim1 jm1 im2 jm2

| ≥ (log q0)−1−α0 or |ρtt,kl| = |ρjm1 jm2

| ≥

is a v vertices graph (v-G) if the num-

is v. It is a e edges graph (e-E) if Card(Eim1 jm1 im2 jm2

) = e.

A vertex in Gim1 jm1 im2 jm2
for any 1 ≤ m1 (cid:54)= m2 ≤ d, Gim1 jm1 im2 jm2
G = Gim1 jm1 im2 jm2

satisﬁes the weak correlation condition (13) if

is said to be isolated if there is no edge connected to it. Note that

could only be 3G/4G, and 0E/1E/2E. We say a graph

G is a 3G0E, 4G0E or 4G1E.

(S13)

For any Gim1 jm1 im2 jm2

satisfying Condition (13)

|Cov(Xim1

Xjm1, Xim2

Xjm2

)| = O{(log d)−1−α0}.

We now deﬁne the following set

I ={1 ≤ k1 < . . . < kd ≤ dst} ,
I0 ={1 ≤ k1 < . . . < kd ≤ dst : for some m1, m2 ∈ {k1, . . . , kd} with m1 . . . m2

does not satisfy Condition (13)(cid:9) ,
satisﬁes Condition (13)(cid:9) ,

Gim1 jm1 im2 jm2

Gim1 jm1 im2 jm2

I c
0 ={1 ≤ k1 < . . . < kd ≤ dst : for any m1, m2 ∈ {k1, . . . , kd} with m1 . . . m2

(cid:83)I c

0. For any subset S of {k1, . . . , kd}, we say that S satisﬁes (14) if
For any m1 (cid:54)= m2 ∈ S, Gim1 jm1 im2 jm2

satisﬁes (13).

(S14)

Obviously, I = I0

For 2 ≤ l ≤ d, let

I0l ={1 ≤ k1 < . . . < kd ≤ dst : the cardinality of the largest subset S is l, where

S is a subset of {k1, . . . , kd} satisﬁes (14)}

I01 ={1 ≤ k1 < . . . < kd ≤ dst : For any m1, m2 ∈ {k1, . . . , kd} with m1 (cid:54)= m2

Gim1 jm1 im2 jm2

does not satisfy (13)}

8

It is easy to show that Card(I0l) ≤ dl+γ(d−l)

and

Obviously, I c
Card(I c

l=1 I0l.

0 = I0d and I0 = (cid:83)dst−1
(cid:1). It suﬃces to prove
0) ≤(cid:0)dst
(cid:88)
(cid:16)|N|min ≥ y(dst, qα)1/2 ± n(log q0)−1/2(cid:17)
(cid:88)

Ic

P

d

0

I0

= o(1)

(cid:110)

1
d!

P(|N|min ≥ y(dst, qα)1/2 ± n(log q0)−1/2) = (1 + o(1))

π−1/2 exp(−x/2)

st

(cid:111)d

(S15)

(S16)

We ﬁrst prove (16). Further divide I0l as follows. Let (k1, . . . , kd) ∈ I0l and let S∗ ⊆ (k1, . . . , kd)

be the largest cardinality subset satisfying (14). Deﬁne

I0l1 = {(k1, . . . , kd) ∈ I0l : there exists an a (cid:54)∈ S∗ such that for some b1, b2 ∈ S∗

with b1 (cid:54)= b2, both Giajaib1 jb1

and Giajaib2 jb2

is 3G1E or 4G2E.}

I0l2 = I0l \ I0l1.

It is easy to see that I0l1 = ∅ and I0l2 = I0l. Recall that d is ﬁxed and l ≤ d − 1. We can
. Let S∗ = {b1, . . . , bl} and

and Card(I0l2) ≤ Cddl+γ(d−l)

st

show that Card(I0l1) ≤ Cddl−1+γ(d−l+1)
x(dst) = y(dst, x)1/2 ± n(log dst)−1/2.

st

For any (k1, . . . , kd) ∈ I0l, let Ul be the covariance matrix of (Nb1, . . . , Nbl). By (13), (cid:107)Ul −

Il(cid:107)2 ≤ O{(log q0)−1−α0}. Let |y|max = max1≤i≤l|yi| for y = (y1, . . . , yl). Then

P{|Nd|min ≥ x(dst)} ≤ P{|Nb1| ≥ x(dst), . . . ,|Nbl| ≥ x(dst)}
exp(− 1
2

(2π)l/2|Ul|1/2

|y|min≥x(dst)

=

1

yTU−1

l y) dy

(cid:90)
(cid:90)

|y|min≥x(dst),|y|max≤(log q0)1/2+α0/4

exp(− 1
2

yTU−1

l y) dy

1

(2π)l/2|Ul|1/2
+ O[exp{−(log q0)1+α0/2/4}]
1 + O{(log q0)−α0/2}

(cid:90)

=

=

(2π)l/2

|y|min≥x(dst),|y|max≤(log q0)1/2+α0/4

+ O[exp{−(log q0)1+α0/2/4}]

= O(d−l
st )

Thus,

(cid:88)

I0l1

P(|N|min ≥ x(dst)) ≤ Cdd

−1+γ(d−l+1)
st

= o(1).

9

exp(− 1
2

yTy) dy

(S17)

For (k1, . . . , kd) ∈ I0l2, let a1 = min{a : a ∈ (k1, . . . , kd), a (cid:54)∈ S∗}. WLOG, assume Gia1 ja1 ib1 jb1

is 3G1E or 4G2E. Because (k1, . . . , kd) ∈ I0l2, by deﬁnition of I0l2,

Cov(Na1, Nbj ) = O((log q0)−1−α0),
Cov(Nbi, Nbj ) = O((log q0)−1−α0),

j = 2, . . . , l
i, j = 1, . . . , l, i (cid:54)= j.

Let Vl be the covariance matrix of (Na1, Nb1, . . . , Nbl). It follows that (cid:107)Vl− ˆVl(cid:107)2 = O((log q0)−1−α0),
where ˆVl = diag(D, Il−1) with D to be the covariance matrix of (Na1, Nb1).

By the conditions, for all a1 and b1,

|EXia1
Y 2
ja1

Xib1
Yja1
)1/2(EX 2
ib1

|
Yjb1
Y 2
jb1

(EX 2
ia1

= ρss,ia1 ib1

ρtt,ja1 jb1

)1/2

≤ (ρ0 + 1)/2.

Using the similar argument as (17), we can show that

(cid:88)
P{|Na1| ≥ x(dst),|Nb1| ≥ x(dst), . . . ,|Nbl| ≥ x(dst)}
(cid:104)
(cid:88)
(cid:104)
(cid:88)

P{|Na1| ≥ x(dst),|Nb1| ≥ x(dst)} × d

−(l−1)
st

I0l2

I0l2
≤C

−1−(1−ρ0)/(3+ρ0)
d
st

× d

−(l−1)
st

+ exp(−(log q0)1+α0/2/4)

+ exp{−(log q0)1+α0/2/4}(cid:105)

(cid:105)

≤C

I0l2
− 1−ρ0
3+ρ0
st

≤Cd

+γ(d−l)

+ q−M

0 = o(1)

Thus (16) is proved. Following the same argument as (17) and Card(I c

0) = (1 + o(1))(cid:0)dst

d

(cid:1), we can

prove (15).

S.2.

SIMULATED NETWORK IN SECTION 5.2

10

Figure S1: Simulated network on 90 regions using the Er¨dos-R´enyi model

11

llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990