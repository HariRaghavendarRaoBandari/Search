6
1
0
2

 
r
a

M
9

 

 
 
]

V
C
.
s
c
[
 
 

1
v
4
1
8
2
0

.

3
0
6
1
:
v
i
X
r
a

MANUSCRIPT, 2016

1

Image Captioning and Visual Question
Answering Based on Attributes and Their

Related External Knowledge

Qi Wu, Chunhua Shen, Anton van den Hengel, Peng Wang, Anthony Dick

Abstract—Much recent progress in Vision-to-Language problems has been achieved through a combination of Convolutional Neural
Networks (CNNs) and Recurrent Neural Networks (RNNs). This approach does not explicitly represent high-level semantic concepts,
but rather seeks to progress directly from image features to text. In this paper we ﬁrst propose a method of incorporating high-level
concepts into the successful CNN-RNN approach, and show that it achieves a signiﬁcant improvement on the state-of-the-art in both
image captioning and visual question answering. We further show that the same mechanism can be used to incorporate external
knowledge, which is critically important for answering high level visual questions. Speciﬁcally, we design a visual question answering
model that combines an internal representation of the content of an image with information extracted from a general knowledge base to
answer a broad range of image-based questions. It particularly allows questions to be asked about the contents of an image, even
when the image itself does not contain a complete answer. Our ﬁnal model achieves the best reported results on both image captioning
and visual question answering on several benchmark datasets.

Index Terms—Image Captioning, Visual Question Answering, Concepts Learning, Recurrent Neural Networks, LSTM.

!

1 INTRODUCTION

V Sion-to-Language problems present a particular chal-

lenge in Computer Vision because they require trans-
lation between two different forms of information. In this
sense the problem is similar to that of machine translation
between languages. In machine language translation there
have been a series of results showing that good performance
can be achieved without developing a higher-level model
of the state of the world. In [1], [2], [3], for instance, a
source sentence is transformed into a ﬁxed-length vector
representation by an ‘encoder’ RNN, which in turn is used
as the initial hidden state of a ‘decoder’ RNN that generates
the target sentence.

Despite the supposed equivalence between an image
and a thousand words, the manner in which information
is represented in each data form could hardly be more
different. Human language is designed speciﬁcally so as to
communicate information between humans, whereas even
the most carefully composed image is the culmination of a
complex set of physical processes over which humans have
little control. Given the differences between these two forms
of information, it seems surprising that methods inspired
by machine language translation have been so successful.
These RNN-based methods which translate directly from
image features to text, without developing a high-level
model of the state of the world, represent the current state
of the art for key Vision-to-Language (V2L) problems, such
as image captioning and visual question answering.

• The authors are with the Australian Centre for Visual Technologies;
and School of Computer Science, The University of Adelaide, Aus-
tralia. E-mail: ({qi.wu01, chunhua.shen, anton.vandenhengel, p.wang,
anthony.dick}@adelaide.edu.au).

Fig. 1: An example from our V2L system. Attributes are predicted
by our CNN-based attributes prediction model. Image captions are
generated by our attribute-based captioning generation model. All the
predicted attributes and generated captions combined with the mined
external knowledge from a large-scale knowledge base are fed to an
LSTM to produce the answer to the asked question. Underlined words
indicate the information required to answer the question.

This approach is reﬂected in many recent successful
works on image captioning, such as [4], [5], [6], [7], [8],
[9], [10]. Current state-of-the-art captioning methods use a
CNN as an image ‘encoder’ to produce a ﬁxed-length vector
representation [11], [12], [13], [14], which is then fed into the
‘decoder’ RNN to generate a caption.

Image Caption: Agroupofpeopleenjoyingasunnydayatthebeachwithumbrellasinthesand.External Knowledge: Anumbrellaisacanopydesignedtoprotectagainstrainorsunlight.Largerumbrellasareoftenusedaspointsofshadeonasunnybeach.Abeachisalandformalongthecoastofanocean.Itusuallyconsistsoflooseparticles,suchassand….QuestionAnswering:Q:Whydotheyhaveumbrellas?A:Shade.Attributes:umbrellabeachsunnydaypeoplesandlayingbluegreenmountainMANUSCRIPT, 2016

2

Visual Question Answering (VQA) is a more recent
challenge than image captioning. It is distinct from many
problems in Computer Vision because the question to be
answered is not determined until run time [15]. In more
traditional problems such as segmentation or detection, the
single question to be answered by an algorithm is prede-
termined, and only the image changes. In visual question
answering, in contrast, the form that the question will take
is unknown, as is the set of operations required to answer it.
In this sense it more closely reﬂects the challenge of general
image interpretation. In this V2L problem an image and a
free-form, open-ended question about the image are pre-
sented to the method which is required to produce a suitable
answer [15]. As in image captioning, current state of the art
in VQA [16], [17], [18] relies on passing CNN features to an
RNN language model. However, visual question answering
is a signiﬁcantly more complex problem than image caption-
ing, not least because it requires accessing information not
present in the image. This may be common sense, or speciﬁc
knowledge about the image subject. For example, given
an image, such as Figure 1, showing ‘a group of people
enjoying a sunny day at the beach with umbrellas’, if one
asks a question ‘why do they have umbrellas?’, to answer
this question, the machine must not only detect the scene
‘beach’, but must know that ‘umbrellas are often used as
points of shade on a sunny beach’. Recently, Antol et al. [15]
also have suggested that VQA is a more “AI-complete” task
since it requires multimodal knowledge beyond a single
sub-domain.

Contributions of this paper are two-fold. First, we pro-
pose a fully trainable attribute-based neural network upon
the CNN+RNN architecture, that can be applied to multiple
V2L problems. We do this by inserting an explicit repre-
sentation of attributes of the scene which are meaningful
to humans. Each semantic attribute corresponds to a word
mined from the training image descriptions, and represents
higher-level knowledge about the content of the image.
A CNN-based classiﬁer is trained for each attribute, and
the set of attribute likelihoods for an image form a high-
level representation of image content. An RNN is then
trained to generate captions, or question answers, on the
basis of the likelihoods. Our attributes based model yields
signiﬁcantly better performance than current state-of-the-art
approaches in the task of image captioning. For example, in
the Microsoft COCO Captioning Challenge, we produce a
BLEU-1 score of 0.73, which is the state of the art at the time
of writing this paper.

Based on the proposed attribute-based V2L model, more
impotantly, our second contribution is to introduce external
commonsense and knowledge for the visual question an-
swering. In this work, we fuse the automatically generated
description of an image with information extracted from an
external knowledge base (KB) to provide an answer to a
general question about the image (See Figure 5). The image
description takes the form of a set of captions, and the
external knowledge is text-based information mined from a
Knowledge Base. Speciﬁcally, for each of the top-k attributes
detected in the image we generate a query which may be
applied to a Resource Description Framework (RDF) KB,
such as DBpedia. RDF is the standard format for large KBs,
of which there are many. The queries are speciﬁed using

Semantic Protocol And RDF Query Language (SPARQL).
We encode the paragraphs extracted from the KB using
Doc2Vec [19], which maps paragraphs into a ﬁxed-length
feature representation. The encoded attributes, captions,
and KB information are then input to an LSTM which is
trained so as to maximise the likelihood of the ground truth
answers in a training set. We further propose a question-
guided knowledge selection scheme to improve the quality
of the extracted KB information. Those knowledge that is
not related to the question is ﬁltered out. The approach that
we propose here combines the generality of information that
using a KB allows with the generality of questions that the
LSTM allows. In addition, it achieves an accuracy of 70.98%
on the Toronto COCO-QA [18], while the latest state of the
art is 61.60%. On the VQA [15] evaluation server (which
does not publish ground truth answers for its test set), we
also produce the state-of-the-art result, which is 59.44%.
Preliminary results of this paper appeared in [20], [21].

2 RELATED WORK
2.1 Attribute-based Representation
Using attribute-based models as a high-level representation
has shown potential in many computer vision tasks such
as object recognition, image annotation and image retrieval.
Farhadi et al. [22] are among the ﬁrst to propose to use
a set of visual semantic attributes to identify familiar ob-
jects, and to describe unfamiliar objects. Lampert et al. [23]
showed that semantic attributes can be used to recognize
object classes in the absence of training images, known as
zero-short learning. Vectors of visual attributes which are
predicted using corresponding attribute classiﬁers were also
used to describe faces by Kumar et al. [24]. In addition
to describing objects semantically, there are several works
describing the whole image using semantic features. Vogel
and Schiele [25] used visual attributes describing scenes
to characterize image regions and combined these local
semantics into a global image description. Su et al. [26]
deﬁned six groups of attributes to build intermediate level
features for image classiﬁcation. Li et al. [27], [28] introduced
the concept of an ‘object bank’ which enables objects to be
used as attributes for scene representation.

2.2 Image Captioning
The problem of annotating images with natural language
at the scene level has long been studied in both computer
vision and natural language processing. Hodosh et al. [29]
proposed to frame sentence-based image annotation as the
task of ranking a given pool of captions. Similarly, [30], [31],
[32] posed the task as a retrieval problem, but based on co-
embedding of images and text in the same space. Recently,
Socher et al. [33] used neural networks to co-embed image
and sentences together and Karpathy et al. [6] co-embedded
image crops and sub-sentences. Neither attempted to gener-
ate novel captions.

Attributes have been used in many image captioning
methods to ﬁll the gaps in predetermined caption templates.
Farhadi et al. [34], for instance, used detections to infer a
triplet of scene elements which is converted to text using a
template. Li et al. [35] composed image descriptions given

MANUSCRIPT, 2016

3

computer vision based inputs such as detected objects,
modiﬁers and locations using web-scale n-grams. Zhu et
al. [36] converted image parsing results into a semantic
representation in the form of Web Ontology Language,
which is converted to human readable text. A more sophisti-
cated CRF-based method use of attribute detections beyond
triplets was proposed by Kulkarni et al [37]. The advantage
of template-based methods is that the resulting captions are
more likely to be grammatically correct. The drawback is
that they still rely on hard-coded visual concepts and suffer
the implied limits on the variety of the output. Instead of us-
ing ﬁxed templates, more powerful language models based
on language parsing have been developed, such as [38], [39],
[40], [41].

Fang et al. [42] won the 2015 COCO Captioning Chal-
lenge with an approach that is similar to ours in as much
as it applies a visual concept (i.e., attribute) detection pro-
cess before generating sentences. They ﬁrst learned 1000
independent detectors for visual words based on a multi-
instance learning framework and then used a maximum
entropy language model conditioned on the set of visually
detected words directly to generate captions. Differently,
our visual attributes act as a high-level semantic representa-
tion for image content which is fed into an LSTM which
generates target sentences based on a much larger word
vocabulary. More importantly, the success of their model
relies on a re-scoring process from a joint image-text em-
bedding space. To what extent the high-level concepts help
in image captioning (and other V2L tasks) is not discussed in
their work. Instead, our work employ several well-designed
experiments (Sec 5) prove the value of explicit high-level
concept in multiple V2L applications.

In contrast to the aforementioned two-stage methods,
the recent dominant trend in V2L is to use an architecture
which connects a CNN to an RNN to learn the mapping
from images to sentences directly. Mao et al. [7], for instance,
proposed a multimodal RNN (m-RNN) to estimate the prob-
ability distribution of the next word given previous words
and the deep CNN feature of an image at each time step.
Similarly, Kiros et al. [43] constructed a joint multimodal em-
bedding space using a powerful deep CNN model and an
LSTM that encodes text. Karpathy and Li [44] also proposed
a multimodal RNN generative model, but in contrast to [7],
their RNN is conditioned on the image information only at
the ﬁrst time step. Vinyals et al. [8] combined deep CNNs for
image classiﬁcation with an LSTM for sequence modeling,
to create a single network that generates descriptions of im-
ages. Chen et al. [4] learn a bi-directional mapping between
images and their sentence-based descriptions, which allows
to reconstruct visual features given an image description.
Xu et al. [45] proposed a model based on visual attention.
Jia et al. [46] applied additional retrieved sentences to guide
the LSTM in generating captions.

Interestingly, this end-to-end CNN-RNN approach ig-
nores the image-to-word mapping which was an essential
step in many of the previous image captioning systems
detailed above [34], [35], [37], [47]. The CNN-RNN approach
has the advantage that it is able to generate a wider variety
of captions, can be trained end-to-end, and outperforms
the previous approach on the benchmarks. It is not clear,
however, what the impact of bypassing the intermediate

high-level representation is, and particularly to what extent
the RNN language model might be compensating. Donahue
et al. [5] described an experiment, for example, using tags
and CRF models as a mid-layer representation for video
to generate descriptions, but it was designed to prove that
LSTM outperforms an SMT-based approach [48]. It remains
unclear whether the mid-layer representation or the LSTM
leads to the success. Our paper provides several well-
designed experiments to answer this question.

We thus here show not only a method for introducing
a high-level representation into the CNN-RNN framework,
and that doing so improves performance, but we also inves-
tigate the value of high-level information more broadly in
V2L tasks. This is of critical importance at this time because
V2L has a long way to go, particularly in the generality of
the images and text it is applicable to.

2.3 Visual Question Answering
Malinowski et al. [49] may be the ﬁrst to study the VQA
problem. They proposed a method that combines semantic
parsing and image segmentation with a Bayesian approach
to sampling from nearest neighbors in the training set.
This approach requires human deﬁned predicates, which
are inevitably dataset-speciﬁc. This approach is also very
dependent on the accuracy of the image segmentation algo-
rithm and on the estimated image depth information. Tu et
al. [50] built a query answering system based on a joint parse
graph from text and videos. Geman et al. [51] proposed an
automatic ‘query generator’ that is trained on annotated im-
ages and produces a sequence of binary questions from any
given test image. Each of these approaches places signiﬁcant
limitations on the form of question that can be answered.

Most recently,

inspired by the signiﬁcant progress
achieved using deep neural network models in both com-
puter vision and natural language processing, an architec-
ture which combines a CNN and RNN to learn the mapping
from images to sentences has become the dominant trend.
Both Gao et al. [16] and Malinowski et al. [17] used RNNs
to encode the question and output the answer. Whereas
Gao et al. [16] used two networks, a separate encoder and
decoder, Malinowski et al. [17] used a single network for
both encoding and decoding. Ren et al. [18] focused on
questions with a single-word answer and formulated the
task as a classiﬁcation problem using an LSTM. A single-
word answer dataset COCO-QA was published with [18].
Ma et al. [52] used CNNs to both extract image features
and sentence features, and fuse the features together with
another multimodal CNN. Antol et al. [15] proposed a large-
scale open-ended VQA dataset based on COCO, which is
called VQA. They also provided a baseline for this dataset
using a CNN+BOW method, which encodes the image
with CNN features and questions with BOW representation.
Inspired by Xu et al. [45] who encode visual attention in the
Image Captioning, [53], [54], [55], [56], [57], [58] propose to
use the spatial attention to help answering visual questions.
[54], [58], [59] formulate the VQA as a classiﬁcation problem
and restrict the answer only can be drawn from a ﬁxed
answer space. In other words, they can not generate open-
ended answers. Zhu et al. [60] investigate the video question
answering problem using the question form of ‘ﬁll-in-the-
blank’.

MANUSCRIPT, 2016

4

Fig. 2: Our attribute-based image captioning framework. The image analysis module learns a mapping between an image and the semantic
attributes through a CNN. The language module learns a mapping from the attributes vector to a sequence of words using an LSTM.

Our framework also exploits both CNN and RNNs, but
in contrast to preceding approaches which use only image
features extracted from a CNN in answering a question, we
employ multiple sources, including image content, gener-
ated image captions and mined external knowledge, to feed
to an RNN to answer questions. Large-scale Knowledge
Bases (KBs), such as Freebase [61] and DBpedia [62], have
been used successfully in several natural language Question
Answering (QA) systems [63], [64]. However, VQA systems
exploiting KBs are still relatively rare.

The quality of the information in the KB is one of the
primary issues in this approach to VQA. The problem is
that KBs constructed by analysing Wikipedia and similar are
patchy and inconsistent at best, and hand-curated KBs are
inevitably very topic speciﬁc. Using visually-sourced infor-
mation is a promising approach to solve this problem [65],
[66], but has a way to go before it might be usefully applied
within our approach. Thus, although our SPARQL and
RDF driven approach can incorporate any information that
might be extracted from a KB, the limitations of the existing
available KBs mean that the text descriptions of the detected
attributes is all that can be usefully extracted. Zhu et al. [67],
in contrast used a hand-crafted KB primarily containing
image-related information such as category labels, attribute
labels and affordance labels, but also some quantities relat-
ing to their speciﬁc question format such as GPS coordinates
and similar. The questions in that system are phrased in
the DBMS query language, and are thus tightly coupled
to the nature of the hand-crafted KB. This represents a
signiﬁcant restriction on the form of question that might
be asked, but has the signiﬁcant advantage that the DBMS
is able to respond decisively as to whether it has the infor-
mation required to answer the question.Instead of building
a problem-speciﬁc KB, we use a pre-built large-scale KB
(DBpedia [62]) from which we extract information using a
standard RDF query language. DBpedia has been created
by extracting structured information from Wikipedia, and
is thus signiﬁcantly larger and more general than a hand-
crafted KB. Rather than having a user pose their question
in a formal query language, our VQA system is able to
encode questions written in natural language automatically.
This is achieved without manually speciﬁed formalization,
but rather depends on processing a suitable training set.
The result is a model which is very general in the forms of
question that it will accept.

3 IMAGE CAPTIONING USING ATTRIBUTES
Our image captioning model is summarized in Figure 2.
The model includes an image analysis part and a captioning
generation part. In the image analysis part, we ﬁrst use
supervised learning to predict a set of attributes, based on
words commonly found in image captions 1. We solve this as
a multi-label classiﬁcation problem and train a correspond-
ing deep CNN by minimizing an element-wise logistic loss
function. Secondly, a ﬁxed length vector Vatt(I) is created
for each image I, whose length is the size of the attribute
set. Each dimension of the vector contains the prediction
probability for a particular attribute. In the captioning gen-
eration part, we apply an LSTM-based sentence generator.
In the baseline model, as in [8], [16], [18] we use a pre-
trained CNN to extract image features CNN(I) which are
fed into the LSTM directly. For the sake of completeness a
ﬁne-tuned version of this approach is also implemented.

3.1 Attribute-based Image Representation
Our ﬁrst task is to describe the image content in terms
of a set of attributes. An attributes vocabulary is ﬁrst
constructed. Unlike [37], [47], that use a vocabulary from
separate hand-labeled training data, our semantic attributes
are extracted from training captions and can be any part of
speech, including object names (nouns), motions (verbs) or
properties (adjectives). The direct use of captions guarantees
that the most salient attributes for an image set are extracted.
We use the c (c = 256) most common words in the training
captions to determine the attribute vocabulary Vatt. Similar
to [42], the top 15 most frequent closed-class words such as
‘a’,‘on’,‘of’ are removed since they are in nearly every
caption. In contrast to [42], our vocabulary is not tense or
plurality sensitive, for instance, ‘ride’ and ‘riding’ are
classiﬁed as the same semantic attribute, similarly ‘bag’
and ‘bags’. This signiﬁcantly decreases the size of our
attribute vocabulary. Our attributes represent a set of high-
level semantic constructs, the totality of which the LSTM
then attempts to represent in sentence form. Generating
a sentence from a vector of attribute likelihoods exploits
a much larger set of candidate words which are learned
separately, allowing for greater ﬂexibility in the generated
text.

1. Please note that we use image captions to build our attributes

vocabulary regardless of the ﬁnal (i.e.captioning, VQA) tasks.

peopletablepizzabagcardogeatinggroup...winezebrarunningred-6.8-7.2-4.50.91.1...3.6-0.6-6.2-0.42.11.0-7.8AttributesPre-trained Single-label CNNPrediction LayerVision Understanding PartParameter TransferringCNNFine-tuned  Multi-labelLSTM      log  (  )LSTM           ℎ  log  (  )LSTM           ℎ . . .LSTM           log    (    )ℎ       ( )MANUSCRIPT, 2016

5

rates of ‘fc6’ and ‘fc7’ of the VggNet are initialized as
0.001 and the last fully connected layer is initialized as 0.01.
All the other layers are ﬁxed during training. We executed
40 epochs in total and decreased the learning rate to one
tenth of the current rate for each layer after 10 epochs. The
momentum is set to 0.9. The dropout rate is set to 0.5.

To predict attributes based on regions, we ﬁrst extract
hundreds of proposal windows from an image. However,
considering the computational inefﬁciency of deep CNNs,
the number of proposals processed needs to be small. Sim-
ilar to [69], we ﬁrst apply the normalized cut algorithm to
group the proposal bounding boxes into m clusters based
on the IoU scores matrix. The top k hypotheses in terms of
the predictive scores reported by the proposal generation
algorithm are kept and fed into the shared CNN. In contrast
to [69], we also include the whole image in the hypothesis
group. As a result, there are mk + 1 hypotheses for each
image. We set m = 10, k = 5 in all experiments. We
use Multiscale Combinatorial Grouping (MCG) [73] for the
proposal generation. Finally, a cross hypothesis max-pooling
is applied to integrate the outputs into a single prediction
vector Vatt(I).

L(cid:88)

3.2 Caption Generation Model
Similar to [7], [8], [44], we propose to train a caption gen-
eration model by maximizing the probability of the correct
description given the image. However, rather than using
image features directly as in typically the case, we use
the semantic attribute prediction value Vatt(I) from the
previous section as the input. Suppose that {S1,...,SL} is
a sequence of words. The log-likelihood of the words given
their context words and the corresponding image can be
written as:

log p(S|Vatt(I)) =

log p(St|S1:t−1,Vatt(I))

(2)
where p(St|S1:t−1,Vatt(I)) is the probability of generating
the word St given attribute vector Vatt(I) and previous
words S1:t−1. We employ the LSTM [74], a particular form
of RNN, to model this.

t=1

The LSTM is a memory cell encoding knowledge at
every time step for what inputs have been observed up to
this step. We follow the model used in [75]. Letting σ be
the sigmoid nonlinearity, the LSTM updates for time step t
given inputs xt, ht−1, ct−1 are:

it = σ(Wxixt + Whiht−1 + bi)
ft = σ(Wxf xt + Whf ht−1 + bf )
ot = σ(Wxoxt + Whoht−1 + bo)
gt = tanh(Wxcxt + Whcht−1 + bc)
ct = ft (cid:12) ct−1 + it (cid:12) gt
ht = ot (cid:12) tanh(ct)
pt+1 = softmax(ht)

(3)
(4)
(5)
(6)
(7)
(8)
(9)

Here, it, ft, ct, ot are the input, forget, memory, output
state of the LSTM. The various W matrices are trained
parameters and (cid:12) represents the product with a gate value.
ht is the hidden state at time step t and is fed to a Softmax,

Fig. 3: Attribute prediction CNN: the model is initialized from Vg-
gNet [13] pre-trained on ImageNet. The model is then ﬁne-tuned on the
target multi-label dataset. Given a test image, a set of proposal regions
are selected and passed to the shared CNN, and ﬁnally the CNN
outputs from different proposals are aggregated with max pooling to
produce the ﬁnal multi-label prediction, which gives us the high-level
image representation, Vatt(I)

Given this attribute vocabulary, we can associate each
image with a set of attributes according to its captions.
We then wish to predict the attributes given a test image.
Because we do not have ground truth bounding boxes for
attributes, we cannot train a detector for each using the
standard approach. Fang et al. [42] solved a similar problem
using a Multiple Instance Learning framework [68] to detect
visual words from images. Motivated by the relatively small
number of times that each word appears in a caption, we
instead treat this as a multi-label classiﬁcation problem. To
address the concern that some attributes may only apply to
image sub-regions, we follow Wei et al. [69] in designing a
region-based multi-label classiﬁcation framework that takes
an arbitrary number of sub-region proposals as input, then a
shared CNN is associated with each proposal, and the CNN
output results from different proposals are aggregated with
max pooling to produce the ﬁnal prediction.

Figure 3 summarizes the attribute prediction network. In
contrast to [69], which uses AlexNet [11] as the initialization
of the shared CNN, we use the more powerful VggNet
[13] pre-trained on ImageNet [70]. This model has been
widely used in image captioning tasks [7], [42], [44], [71].
The shared CNN is then ﬁne-tuned on the target multi-label
dataset (our image-attribute training data). In this step, the
output of the last fully-connected layer is fed into a c-way
softmax over the c class labels. The c here represents the
attributes vocabulary size. In contrast to [69] who employs
the squared loss, we ﬁnd that element-wise logistic loss
function performs better. Suppose that there are N training
examples and yi = [yi1, yi2,..., yic] is the label vector of
the ith image, where yij = 1 if the image is annotated
with attribute j, and yij = 0 otherwise. If the predictive
probability vector is pi = [pi1, pi2,..., pic], then the cost
function to be minimized is

N(cid:88)

c(cid:88)

i=1

j=1

J =

1
N

log(1 + exp(−yijpij))

(1)

During the ﬁne-tuning process, the parameters of the last
fully connected layer (i.e. the attribute prediction layer) are
initialized with a Xavier initialization [72]. The learning

Parameter TransferringPre-trained Single-label CNNImageNetMS COCOParameter TransferringRegionsProposeMaxPoolingFine-tuned Multi-labelCNN    ( )Single-labelImagesMulti-labelImagesSingle-labelLossesMulti-labelLossesMANUSCRIPT, 2016

Fig. 4: Examples of predicted attributes and generated captions.

which will produce a probability distribution pt+1 over all
words and indicate the word at time step t + 1.
Training details: The LSTM model for image captioning
is trained in an unrolled form. More formally, the LSTM
takes the attributes vector Vatt(I) and a sequence of words
S = (S0,...,SL,SL+1), where S0 is a special start word
and SL+1 is a special END token. Each word has been
represented as a one-hot vector St of dimension equal to
the size of words dictionary. The words dictionaries are
built based on words that occur at least 5 times in the
training set, which lead to 2538, 7414, and 8791 words on
Flickr8k, Flickr30k and MS COCO datasets separately. Note
it is different from the semantic attributes vocabulary Vatt.
The training procedure is as following: At time step t = −1,
we set x−1 = WeaVatt(I) and hinitial = (cid:126)0, where Wea is the
learnable attributes embedding weights. This gives us an
initial LSTM hidden state h−1 which can be used in the next
time step. From t = 0 to t = L, we set xt = WesSt and the
hidden state ht−1 is given by the previous step, where Wes
is the learnable word embedding weights. The probability
distribution pt+1 over all words is then computed by the
LSTM feed-forward process. Finally, on the last step when
SL+1 represents the last word, the target label is set to the
END token.

Our training objective is to learn parameters Wea, Wes
and all parameters in LSTM by minimizing the following
cost function:

N(cid:88)
N(cid:88)

i=1

C = − 1
N

= − 1
N

log p(S(i)|Vatt(I (i))) + λθ · ||θ||2
L(i)+1(cid:88)

t ) + λθ · ||θ||2

log pt(S(i)

2

2 (10)

(11)

i=1

t=1

6

pt(S(i)
t ) corresponds to the activation of the Softmax layer
in the LSTM model for the i-th input and θ represents
model parameters, λθ · ||θ||2
2 is a regularization term. We
use SGD with mini-batches of 100 image-sentence pairs.
The attributes embedding size, word embedding size and
hidden state size are all set to 256 in all the experiments.
The learning rate is set to 0.001 and clip gradients is 5. The
dropout rate is set to 0.5.

To infer the sentence given an input image, we use the
Beam Search, i.e., we iteratively consider the set of b best
sentences up to time t as candidates to generate sentences at
time t + 1, and only keep the best b results. We set the b as
5. Figure 4 shows some examples of the predicted attributes
and generated captions. More results can be found in the
supplementary material.

4 A VQA MODEL WITH EXTERNAL KNOWLEDGE
The key differentiator of our VQA model is that it is able
to usefully combine image information with that extracted
from a Knowledge Base, within the LSTM framework. The
novelty lies in the fact that this is achieved by representing
both of these disparate forms of information as text before
combining them. Figure 5 summarises how this is achieved:
given an image, an attribute-based representation Vatt(I)
(in Section 3.1) is ﬁrst generated and it will used as one of
input sources of our VQA-LSTM model. The second input
source are those captions generated in section 3.2. Rather
than inputing the generated words directly, the hidden
state vector of the caption-LSTM after it has generated the
last word in each caption is used to represent its content.
Average-pooling is applied over the 5 hidden-state vectors,
to obtain a vector representation Vcap(I) for the image I.
The third input source is the textual knowledge which is
mined from a large-scale knowledge base, the DBpedia.
More details are shown in the following section.

4.1 Relating to the Knowledge Base
The external data source that we use here is DBpeida [62]
as a source of general background information, although
any such KB could equally be applied. DBpeida is a struc-
tured database of information extracted from Wikipedia.
The whole DBpedia dataset describes 4.58 million entities,
of which 4.22 million are classiﬁed in a consistent ontology.
The data can be accessed using an SQL-like query language
for RDF called SPARQL. Given an image and its predicted
attributes, we use the top-ﬁve2 most strongly predicted
attributes to generate DBpedia queries. There are a range
of problems with DBpedia and similar, however, including
the sparsity of the information, and the inconsistency of
its representation. Inspecting the database shows that the
‘comment’ ﬁeld is the most generally informative about an
attribute, as it contains a general text description of it. We
therefore retrieve the comment text for each query term.
The KB+SPARQL combination is very general, however,
and could be applied problem speciﬁc KBs, or a database
of common sense information, and can even perform basic

where N is the number of training examples and L(i) is
the length of the sentence for the i-th training example.

2. We only use top-5 attributes to query the KB because, based on
observation of training data, an image typically contains 5-8 attributes.
We also tested with top-10, but no improvements were observed.

Top5Attributes:players,catch,bat,baseball,swingGeneratedCaptions:Abaseballplayerswingabatataball.Abaseballplayerholdingabatonafield.Abaseballplayerswingingabatonafield.Abaseballplayerisswingingabatataball.Abattercatcherandumpireduringabaseballgame.Top5Attributes:field,two,tree,grass,giraffeGeneratedCaptions:Twogiraffesarestandinginagrassyfield.Acoupleofgiraffestandingnexttoeachother.Twogiraffesstandingnexttoeachotherinafield.Acoupleofgiraffestandingnexttoeachotheronalushgreenfield.Top5Attributes:pizza,bottle,sitting,table,beerGeneratedCaptions:Alargepizzasittingontopofatable.Apizzasittingontopofawhiteplate.Apizzasittingontopofatablenexttoabeer.Apizzasittingontopofatablenexttoabottleofbeer.MANUSCRIPT, 2016

7

Fig. 5: Our proposed framework: given an image, a CNN is ﬁrst applied to produce the attribute-based representation Vatt(I). The internal textual
representation is made up of image captions generated based on the image-attributes. The hidden state of the caption-LSTM after it has generated
the last word in each caption is used as its vector representation. These vectors are then aggregated as Vcap(I) with average-pooling. The external
knowledge is mined from the KB (in this case DBpedia) and the responses encoded by Doc2Vec, which produces a vector Vknow(I). The 3 vectors
V are combined into a single representation of scene content, which is input to the VQA LSTM model which interprets the question and generates
an answer.

if the question is asking about the ‘dog’ in the image, it does
not make sense to input a piece of ‘bird’ knowledge into the
model, although the image does have a ‘bird’ inside.

Given a question Q and mined n knowledge paragraphs
using above KB+SPARQL combination, we ﬁrst use our
pre-trained Doc2Vec model to extract the semantic feature
V (Q) of the question and the feature V (Ki) for each single
knowledge paragraph, where i ∈ n. Then, we ﬁnd the k
most closest knowledge paragraph to the question based on
the cosine similarity between the V (Q) and V (Ki). Finally,
we combine the k selected knowledge paragraph in to a
single one and use the Doc2Vec model to extract its semantic
feature. In our experiments, we set n = 10, k = 5.

4.3 An Answer Generation Model with Multiple Inputs
We propose to train a VQA model by maximizing the prob-
ability of the correct answer given the image and question.
We want our VQA model to be able to generate multiple
word answers, so we formulate the answering process as a
word sequence generation procedure. Let Q = {q1,...,qn}
represent the sequence of words in a question, and A =
{a1,...,al} the answer sequence, where n and l are the length
of question and answer, respectively. The log-likelihood of
the generated answer can be written as:

l(cid:88)

log p(A|I,Q) =

log p(at|a1:t−1,I,Q)

(12)

t=1

where p(at|a1:t−1,I,Q) is the probability of generating at
given image information I, question Q and previous words
a1:t−1. We employ an encoder LSTM [74] to take the se-
mantic information from image I and the question Q, while
using a decoder LSTM to generate the answer. Weights are
shared between the encoder and decoder LSTM.
In the training phase, the question Q and answer A
are concatenated as {q1,...,qn,a1,...,al,al+1}, where al+1 is
a special END token. Each word is represented as a one-hot
vector of dimension equal to the size of the word dictionary.

Fig. 6: An example of SPARQL query language for the attribute ‘dog’.
The mined text-based knowledge are shown below.

inference over RDF. Figure 6 shows an example of the query
language and returned text.

Since the text returned by the SPARQL query is typically
much longer than the captions generated in the section 3.2,
we turn to Doc2Vec [19] to extract the semantic meanings3.
Doc2Vec, also known as Paragraph Vector, is an unsuper-
vised algorithm that learns ﬁxed-length feature representa-
tions from variable-length pieces of texts, such as sentences,
paragraphs, and documents. Le et al. [19] proved that it can
capture the semantics of paragraphs. A Doc2Vec model is
trained to predict words in the document given the context
words. We collect 100,000 documents from DBpedia to train
a model with vector size 500. To obtain the knowledge
vector Vknow(I) for image I, we combine the 5 returned
paragraphs in to a single large paragraph, before semantic
features using our pre-trained Doc2Vec model.
4.2 Question-guided Knowledge Selection
We incrementally implemented a question-guided knowl-
edge selection scheme to rule out the noise information,
since we observed that some mined knowledge are not
necessary for answering the given question. For example,

3. We investigated to use an LSTM to encode the mined paragraphs,
but we observed little performance improvement, despite the addi-
tional training overhead.

    ( )RegionsProposalsMulti-labelCNNCap1:adoglayingonthefloorwithabirdnexttoitandacatbehindthem,ontheothersideofaslidingglassdoor.Cap2:abrownandblackdoglayingonafloornexttoabird.Cap3:thedog,cat,andbirdareallonthefloorintheroom.…..CaptionInternal Representation…    ( )AveragePoolingTop 5AttributesSPARQLDBpediaThedogisafurry,carnivorousmemberofthecanidaefamily,mammalclass.Thecatisasmall,usuallyfurry,domesticated,andcarnivorousmammal.Birds,Avesclass,areagroupofendothermicvertebrates,characterisedbyfeathers,abeakwithnoteeth.Plants,alsocalledgreenplants,aremulticellulareukaryotesofthe….Doc2Vec     ( )         External KnowledgeLSTM              . . .       log  (  )             ℎ log  (  )             ℎ                log    (    )ℎ     . . .ℎ LSTMLSTMLSTMLSTMLSTMLSTMlog  (  )  ℎ       Howmany?. . .ThereTherearearetwomammalsEND. . .. . . =−1   log  (   )+  ∙     ( )         minimizing cost functionGenerationℎ  ℎ  ℎ  Thedomesticdogisafurry,carnivorousmemberofthecanidaefamily,mammalclass.Domesticdogsarecommonlyknownas"man'sbestfriend".Thedogwasthefirstdomesticatedanimalandhasbeenwidelykeptasaworking,hunting,andpetcompanion.Itisestimatedtherearebetween700millionandonebilliondomesticdogs,makingthemthemostabundantmemberoforderCarnivora.PREFIXrdfs:<http://www.w3.org/2000/01/rdf-schema#>sparqlSELECT DISTINCT ?comment WHERE{?entry rdfs:label “Dog"@en.?entry rdfs: comment ?comment.}MANUSCRIPT, 2016

The training procedure is as follows: at time step t = 0, we
set the LSTM input:

xinitial = [WeaVatt(I), WecVcap(I), WekVknow(I)]

(13)

where Wea, Wec, Wek are learnable embedding weights for
the vector representation of attributes, captions and exter-
nal knowledge, respectively. Given the randomly initialized
hidden state, the encoder LSTM feeds forward to produce
hidden state h0 which encodes all of the input information.
From t = 1 to t = n, we set xt = Wesqt and the hidden
state ht−1 is given by the previous step, where Wes is
the learnable word embedding weights. The decoder LSTM
runs from time step n + 1 to l + 1. Speciﬁcally, at time step
t = n + 1, the LSTM layer takes the input xn+1 = Wesa1
and the hidden state hn corresponding to the last word
of the question, where a1 is the start word of the answer.
The hidden state hn thus encodes all available information
about the image and the question. The probability distribu-
tion pt+1 over all answer words in the vocabulary is then
computed by the LSTM feed-forward process. Finally, for
the ﬁnal step, when al+1 represents the last word of the
answer, the target label is set to the END token.

Our training objective is to learn parameters Wea, Wec,
Wek, Wes and all the parameters in the LSTM by minimizing
the following cost function:

N(cid:88)
N(cid:88)

i=1

i=1

j=1

C = − 1
N

= − 1
N

log p(A(i)|I,Q) + λθ · ||θ||2
l(i)+1(cid:88)

j ) + λθ · ||θ||2

log pj(a(i)

2

2

(14)

(15)

State-of-art-Flickr8k

Karpathy & Li (NeuralTalk) [44]
Chen & Zintick (Mind’s Eye) [4]

Google(NIC) [8]

Mao et al. (m-Rnn-AlexNet) [7]
Xu et al. (Hard-Attention) [45]

Flickr8k
B-1
0.58

-

0.66
0.57
0.67

B-2
0.38

-

0.42
0.39
0.46

B-3
0.25

-

0.27
0.26
0.31

Baseline - CNN(I)

VggNet+LSTM

VggNet-PCA+LSTM
GoogLeNet+LSTM
VggNet+ft+LSTM

Ours - Vatt(I)

Attributes-GT+LSTM‡
Attributes-SVM+LSTM
Attributes-CNN+LSTM

0.56
0.56
0.56
0.64

0.76
0.73
0.74

0.37
0.38
0.38
0.43

0.57
0.53
0.54

0.24
0.25
0.24
0.30

0.41
0.38
0.38

State-of-art-Flickr30k

Karpathy & Li (NeuralTalk) [44]
Chen & Zintick (Mind’s Eye) [4]

Google(NIC) [8]

Donahue et al. (LRCN) [5]

Mao et al. (m-Rnn-AlexNet) [7]
Mao et al. (m-Rnn-VggNet) [7]
Xu et al. (Hard-Attention) [45]

Flickr30k
B-1
0.57

-

0.66
0.59
0.54
0.60
0.67

B-2
0.37

-
-

0.39
0.36
0.41
0.44

B-3
0.24

-
-

0.25
0.23
0.28
0.30

8

PPL

15.10

24.39

-

-

-

15.71
16.07
15.71
14.69

12.52
12.63
12.60

PPL

19.10

35.11
20.72

-

-
-

-

B-4
0.16
0.14
0.18
0.17
0.21

0.16
0.16
0.16
0.20

0.29
0.26
0.27

B-4
0.16
0.13

-

0.17
0.15
0.19
0.20

Baseline - CNN(I)

VggNet+LSTM

VggNet-PCA+LSTM
GoogLeNet+LSTM
VggNet+ft+LSTM

Ours - Vatt(I)

0.57
0.59
0.58
0.67

0.38
0.40
0.39
0.47

0.25
0.26
0.26
0.31

0.17
0.17
0.17
0.21

18.83
18.92
18.77
16.62

0.78
0.68
0.73

0.57
0.49
0.55

Attributes-GT+LSTM‡
Attributes-SVM+LSTM
Attributes-CNN+LSTM

14.88
16.01
15.96
TABLE 1: BLEU-1,2,3,4 and PPL metrics compared to other state-of-
the-art methods and our baseline on Flickr8k and Flickr30K dataset.
‡ indicates ground truth attributes labels are used, which (in gray )
will not participate in rankings. Our PPLs are based on Flickr8k and
Flickr30k word dictionaries of size 2538 and 7414, respectively.

0.42
0.33
0.40

0.30
0.23
0.28

where N is the number of training examples, and n(i) and
l(i) are the length of question and answer respectively for
the i-th training example. Let pt(a(i)
t ) correspond to the
activation of the Softmax layer in the LSTM model for
the i-th input and θ represent the model parameters. Note
that λθ · ||θ||2
2 is a regularization term. We use Stochastic
gradient Descent (SGD) with mini-batches of 100 image-
QA pairs. The attributes, internal textual representation,
external knowledge embedding size, word embedding size
and hidden state size are all 256 in all experiments. The
learning rate is set to 0.001 and clip gradients is 5. The
dropout rate is set to 0.5.

5 EXPERIMENTS
We evaluate our image captioning model and visual ques-
tion answering model separately in the following sections.

5.1 Evaluation on Image Captioning
5.1.1 Dataset
There are several datasets which consist of images and
sentences in English describing these images. We report
results on the popular Flickr8k [29], Flickr30k [76] and
Microsoft COCO dataset [77]. These datasets contain 8,000,
31,000 and 123,287 images respectively, and each image is
annotated with 5 sentences. In our reported results, we use
pre-deﬁned splits for Flickr8k, 1000 for validation, 1000 for
testing and the rest for training. Because most of previous

works in image captioning [5], [7], [8], [42], [44], [45] are not
evaluated on the ofﬁcial split for Flickr30k and MS COCO,
for fair comparison, we report results with the widely used
publicly available splits in the work of [44], which use 1000
images for validation, 1000 for testing for Flickr30k, and
5000 images for both validation and testing in MS COCO.
We further tested on the actually MS COCO test set (ofﬁcial
split) consisting of 40775 images (human captions for this
split are not available publicly), and evaluated them on the
COCO evaluation server.

5.1.2 Evaluation
Metrics: We report results with the frequently used BLEU
metric and sentence perplexity (PPL). BLEU [78] scores
are originally designed for automatic machine translation
where they measure the fraction of n-grams (up to 4-gram)
that are in common between a hypothesis and a reference
or set of references. Here we compare against 5 references.
Perplexity (PPL) is a standard measure for evaluating lan-
guage models, which measures how many bits on average
would be needed to encode each word given the language
model, so a low PPL means a better language model.
For MS COCO dataset, we additionally evaluate our model
based on the metrics of METEOR [79] and CIDEr [80]. All
scores (except PPL) are computed with the coco-caption
code [81].
Baselines: To verify the effectiveness of our high-level
attributes representation, we provide a baseline method.

MANUSCRIPT, 2016

The baseline framework is same as the one proposed in
section 3.2, except that the attributes vector Vatt(I) is re-
placed by the last hidden layer of CNN directly. Various
CNN architectures are applied in the baseline method to
extract image features, such as VggNet [13] and GoogLeNet
[14]. For the VggNet+LSTM, we use the second fully
connected layer (fc7) as the image features, which has
4096 dimensions. In VggNet-PCA+LSTM, PCA is applied
to decrease the feature dimension from 4096 to 1000. For
the GoogLeNet+LSTM, we use the model provided in the
Caffe Model Zoo [82] and the last average pooling layer
is employed, which is a 1024-d vector. VggNet+ft+LSTM
applies a VggNet that has been ﬁne-tuned on the target
dataset, based on the task of image-attributes classiﬁcation.
Our Approaches: We evaluate several variants of our ap-
proach: Att-GT+LSTM models use ground-truth attributes
as the input while Att-CNN+LSTM uses the attributes vec-
tor Vatt(I) predicted by the attributes prediction network in
section 3.1. We also evaluate an approach Att-SVM+LSTM
with linear SVM predicted attributes vector. SVM classiﬁers
are trained to divide positive attributes from those negatives
given an image-attributes correspondence. We use the sec-
ond fully connected layer of the ﬁne-tuned VggNet to feed
the SVM.
Results: Table 1 and 2 report image captioning results on
Flickr8k, Flickr30k and Microsoft COCO dataset. It is not
surprising that Att-GT+LSTM model performs best, since
ground truth attributes labels are used. We report these
results here just to show the advances of adding an inter-
mediate image-to-word mapping stage. Ideally, if we are
able to train a strong attributes predictor which gives us
a good enough estimation of attributes, we could obtain an
outstanding improvement comparing with both baselines
and state-of-the-arts. Indeed, apart from using ground truth
attributes, our Attributes-CNN+LSTM models generate the
best results on all the three datasets over all evaluation
metrics. Especially comparing with baselines, which do
not contain an attributes prediction layer, our ﬁnal models
bring signiﬁcant improvements, nearly 15% for B-1 and 30%
for CIDEr on average. VggNet+ft+LSTM models perform
better than other baselines because of the ﬁne-tuning on
the target dataset. However, they do not perform as good
as our attributes-based models. Attributes-SVM+LSTM
under-perform Attributes-CNN+LSTM means our region-
based attributes prediction network performs better than
the whole image classiﬁcation. Our ﬁnal model also out-
performs current state-of-the-arts listed in tables. We also
evaluate an approach (not shown in table) that combines
CNN features and attributes vector together as the input
of the LSTM, but we ﬁnd this approach is not as good
as using attributes vector only in the same setting. In any
case, above experiments show that an intermediate image-
to-words stage (i.e. attributes prediction layer) bring us
signiﬁcant improvements.

We further generated captions for the images in the
COCO test set containing 40,775 images and evaluated them
on the COCO evaluation server. These results are shown
in Table 3. We achieve 0.73 on B-1, and surpass human
performances on 13 of the 14 metrics reported. Other state-
of-the-art methods are also shown for comparison.

State-of-art

NeuralTalk [44]
Mind’s Eye [4]

NIC [8]
LRCN [5]

Mao et al. [7]
Jia et al. [46]

MSR [42]

Xu et al. [45]
Jin et al. [83]

Baseline-CNN(I)

VNet+LSTM

0.61
VNet-PCA+LSTM 0.62
0.60
0.68

GNet+LSTM
VNet+ft+LSTM
Ours-Vatt(I)
Att-GT+LSTM‡
0.80
Att-SVM+LSTM
0.69
Att-CNN+LSTM 0.74

B-1
0.63

-
-

-

0.67
0.67
0.67

0.72
0.70

B-2
0.45

-
-

-

0.49
0.49
0.49

0.50
0.52

B-3
0.32

-
-

-

0.35
0.34
0.36

0.36
0.38

0.42
0.43
0.40
0.50

0.28
0.29
0.26
0.37

M
0.20
0.20
0.24

-
-

0.23
0.24
0.23
0.24

C
0.66

0.86

0.81

0.84

-

-
-

-
-

9

P
-

-
-

-

-
-

11.60

13.60

18.10

0.19
0.20
0.19
0.22

0.56
0.60
0.55
0.73

13.58
13.02
14.01
13.29

B-4
0.23
0.19
0.28
0.25
0.24
0.26
0.26
0.25
0.28

0.19
0.19
0.17
0.25

0.50
0.38
0.42

0.40
0.28
0.31

0.64
0.52
0.56

9.60
12.62
10.49
TABLE 2: BLEU-1,2,3,4, METEOR, CIDEr and PPL metrics compared
to other state-of-the-art methods and our baseline on MS COCO
dataset. ‡ indicates ground truth attributes labels are used, which (in
gray ) will not participate in rankings. Our PPLs are based on MS
COCO word dictionaries of size 8791.

0.28
0.23
0.26

1.07
0.82
0.94

COCO-TEST

5-Refs
Ours
Human
MSR [42]
m-RNN [7]
LRCN [5]
40-Refs
Ours
Human
MSR [42]
m-RNN [7]
LRCN [5]

B-1

0.73
0.66
0.70
0.68
0.70

0.89
0.88
0.88
0.87
0.87

B-2

0.56
0.47
0.53
0.51
0.53

0.80
0.74
0.79
0.76
0.77

B-3

0.41
0.32
0.39
0.37
0.38

0.69
0.63
0.68
0.64
0.65

B-4

0.31
0.22
0.29
0.27
0.28

0.58
0.47
0.57
0.53
0.53

M

0.25
0.25
0.25
0.23
0.24

0.33
0.34
0.33
0.30
0.32

R

CIDEr

0.53
0.48
0.52
0.50
0.52

0.67
0.63
0.66
0.64
0.66

0.92
0.85
0.91
0.79
0.87

0.93
0.91
0.93
0.79
0.89

TABLE 3: COCO evaluation server results. M and R stands for ME-
TEOR and ROUGE-L. Results using 5 references and 40 references
captions are both shown. We only list the comparison results that have
been ofﬁcially published in the corresponding references.

Table 4 summarizes some properties of recurrent layers
employed in some recent RNN-based methods. We achieve
state-of-the-art using a relatively small dimensional visual
input feature and recurrent layer. Lower dimension of visual
input and RNN normally means less parameters in the RNN
training stage, as well as lower computation cost.

VIS Input Dim

RNN Dim

Ours NIC [8]
256
256

1000
512

LRCN [5] m-RNN [7] NeuralTalk [44]
1000
1000×4

4096
256

300-600

4096

TABLE 4: Visual feature input dimension and properties of RNN. Our
visual features has been encoded as a 256-d attributes score vector
while other models need higher dimensional features to feed to RNN.
According to the unit size of RNN, we achieve state-of-the-art using a
relatively small dimensional recurrent layer.

5.2 Evaluation on Visual Question Answering
We evaluate our model on four recent publicly available
visual question answering datasets, two toy size and two
large size. DAQURA-ALL is proposed in [84]. There are
7,795 training questions and 5,673 test questions. These
questions are generated on 795 and 654 images respectively.
The questions are categorized into three types including Ob-
ject, Color and Number. Most of the answers are single word.
DAQURA-REDUCED is a reduced version of DAQURA-
ALL. There are 3,876 training questions and only 297 test
questions. This dataset is constrained to 37 object categories
and uses only 25 test images. Two large-scale VQA data are

MANUSCRIPT, 2016

# Images
# Questions
# Question Types
# Ans per Que
# Words per Ans

DAQURA

All
1,449
12,468

3
1
1+

DAQURA
Reduced

1,423
4,173

3
1
1+

Toronto

COCO-QA

117,684
117,684

4
1
1

VQA
204,721
614,163

10
1+

more than 20

TABLE 5: Some statistics about the DAQURA, Toronto COCO-QA
Dataset [18] and MS COCO-VQA dataset [15].
constructed both based on MS COCO images. The Toronto
COCO-QA Dataset [18] contains 78,736 training and 38,948
testing examples, which are generated from 117,684 images.
There are four types of questions, relating to the object,
number, color and location, all constructed so as to have
a single-word answer. All of the question-answer pairs
in this dataset are automatically converted from human-
sourced image descriptions. Another benchmarked dataset
is VQA [15], which is a much larger dataset and contains
614,163 questions and 6,141,630 answers based on 204,721
MS COCO images. This dataset provides a surprising vari-
ety of question types, including “What is...’, “How Many”
and even “Why...”. The ground truth answers were gen-
erated by 10 human subjects and can be single word or
sentences. The data train/val split follows the COCO ofﬁcial
split, which contains 82,783 training images and 40,504
validation images, each has 3 questions and 10 answers.
We randomly choose 5000 images from the validation set as
our val set, with the remainder testing. The human ground
truth answers for the actual VQA test split are not available
publicly and only can be evaluated via the VQA evaluation
server. Hence, we also apply our ﬁnal model on a test
split and report the overall accuracy. Table 5 displays some
dataset statistics.

5.2.1 Results on DAQURA
Metrics: Following [18], [52], the accuracy value (the pro-
portion of correctly answered test questions), and the Wu-
Palmer similarity (WUPS) [85] are used to measure perfor-
mance. The WUPS calculates the similarity between two
words based on the similarity between their common subse-
quence in the taxonomy tree. If the similarity between two
words is greater than a threshold then the candidate answer
is considered to be right. We report on thresholds 0.9 and
0.0, following [18], [52].
Evaluations: To illustrate the effectiveness of our model,
we provide two baseline models and several state-of-the-
art results in table 6 and 7. The Baseline method is im-
plemented simply by connecting a CNN to an LSTM. The
CNN is a pre-trained (on ImageNet) VggNet model from
which we extract the coefﬁcients of the last fully connected
layer. We also implement a baseline model VggNet+ft-
LSTM, which applies a vggNet that has been ﬁne-tuned
on the COCO dataset, based on the task of image-attributes
classiﬁcation. We also present results from a series of cut
down versions of our approach for comparison. Att-LSTM
uses only the semantic level attribute representation Vatt
as the LSTM input. To evaluate the contribution of the
internal textual representation and external knowledge for
the question answering, we feed the image caption represen-
tation Vcap and knowledge representation Vknow with the
Vatt separately, producing two models, Att+Cap-LSTM and
Att+Know-LSTM. We also tested the Cap+Know-LSTM,

DAQURA-All
Askneuron [17]
Ma et al. [52]
Yang et al. [58]
Noh et al. [59]
Baseline
VggNet-LSTM
VggNet+ft-LSTM
Our-Proposal
Att-LSTM
Att+Cap-LSTM
Att+Know-LSTM
Cap+Know-LSTM
Att+Cap+Know-LSTM
A+C+Selected-K-LSTM

10

Acc(%) WUPS@0.9 WUPS@0.0
19.43
23.40
29.30
28.98

62.00
62.95
68.60
67.81

25.28
29.59
35.10
34.80

23.13
23.75

24.27
27.04
24.89
23.91
29.16
29.23

30.01
30.22

30.41
33.40
31.27
30.64
35.30
35.37

63.61
63.66

62.29
67.65
66.11
65.01
68.66
68.72

TABLE 6: Accuracy, WUPS metrics compared to other state-of-the-art
methods and our baseline on DAQURA-All.

DAQURA-Reduced
GUESS [18]
VIS+BOW [18]
VIS+LSTM [18]
2-VIS+BLSTM [18]
Askneuron [17]
Ma et al. [52]
Xu et al. [54]
Yang et al. [58]
Noh et al. [59]
Baseline
VggNet-LSTM
VggNet+ft-LSTM
Our-Proposal
Att-LSTM
Att+Cap-LSTM
Att+Know-LSTM
Cap+Know-LSTM
Att+Cap+Know-LSTM
A+C+Selected-K-LSTM

29.65
44.99
46.05
46.83
40.76
44.86

Acc(%) WUPS@0.9 WUPS@0.0
18.24
34.17
34.41
35.78
34.68
39.66
40.07
45.50
44.48

77.59
81.48
82.23
82.15
79.54
83.06

50.20
49.56

-

-

83.60
83.95

38.72
39.13

40.07
44.78
41.08
40.81
45.79
46.13

43.97
44.03

45.43
50.07
46.04
45.04
51.53
51.83

83.01
83.33

82.67
83.85
82.39
82.01
83.91
83.95

TABLE 7: Accuracy, WUPS metrics compared to other state-of-the-art
methods and our baseline on DAQURA-Reduced.

for the experiment completeness. Att+Cap+Know-LSTM
combines all the available information. Our ﬁnal model is
the A+C+Selected-K-LSTM, which uses the selected knowl-
edge information (see section 4.2) as the input. GUESS
[18] simply selects the modal answer from the training
set for each of 4 question types. VIS+BOW [18] performs
multinomial logistic regression based on image features and
a BOW vector obtained by summing all the word vectors
of the question. VIS+LSTM [18] has one LSTM to encode
the image and question, while 2-VIS+BLSTM [18] has two
image feature inputs, at the start and the end. Malinowskiet
al. [17] propose a neural-based approach and Ma et al. [52]
encodes both images and questions with a CNN. Yang et
al. [58] use a stacked attention networks to infer the answer
progressively.

All of our proposed models outperform the Base-
line method. And our ﬁnal model A+C+Selected-K-
LSTM achieves the best state-of-the-art on the DAQURA-
Reduced set. Att+Cap+Know-LSTM performs not as good
as A+C+Selected-K-LSTM, which shows the effectiveness
of our question-based knowledge selection scheme.

5.2.2 Results on Toronto COCO-QA
Evaluations: Table 8 reports the results on Toronto COCO-
QA. All of our proposed models outperform the Baseline
and all of the comparator state-of-the-art methods. Our ﬁnal

MANUSCRIPT, 2016

Toronto COCO-QA
GUESS [18]
VIS+BOW [18]
VIS+LSTM [18]
2-VIS+BLSTM [18]
Ma et al. [52]
Chen et al. [55]
Yang et al. [58]
Noh et al. [59]
Baseline
VggNet-LSTM
VggNet+ft-LSTM
Our-Proposal
Att-LSTM
Att+Cap-LSTM
Att+Know-LSTM
Cap+Know-LSTM
Att+Cap+Know-LSTM
A+C+Selected-K-LSTM

Acc(%) WUPS@0.9 WUPS@0.0

73.44
88.99
88.25
88.64
88.58
89.85
90.90
90.61

6.65
55.92
53.31
55.09
54.95
58.10
61.60
61.19

17.42
66.78
63.91
65.34
65.36
68.44
71.60
70.84

50.73
58.34

61.38
69.02
63.07
64.31
69.73
70.98

60.37
67.32

71.15
76.20
72.22
73.31
77.14
78.35

87.48
89.13

91.58
92.38
90.84
90.01
92.50
92.87

TABLE 8: Accuracy, WUPS metrics compared to other state-of-the-art
methods and our baseline on Toronto COCO-QA dataset.

Toronto COCO-QA
GUESS [18]
VIS+BOW [18]
VIS+LSTM [18]
2-VIS+BLSTM [18]
Chen et al. [55]
Yang et al. [58]
Baseline
VggNet-LSTM
VggNet+ft-LSTM
Our-Proposal
Att-LSTM
Att+Cap-LSTM
Att+Know-LSTM
Cap+Know-LSTM
Att+Cap+Know-LSTM
A+C+Selected-K-LSTM

Object
2.11
58.66
56.53
58.17
62.46
64.50

53.71
61.67

63.92
71.30
64.57
65.61
71.45
73.66

Number

35.84
44.10
46.10
44.79
45.70
48.60

45.37
50.04

51.83
69.98
54.37
55.13
75.33
72.20

Color
13.87
51.96
45.87
49.53
46.81
57.90

36.23
52.16

57.29
61.50
62.79
62.02
64.09
62.97

Location

8.93
49.39
45.52
47.34
53.67
54.00

46.37
54.40

54.84
60.98
56.98
57.28
60.98
61.18

TABLE 9: Toronto COCO-QA accuracy (%) per category.

model A+C+Selected-K-LSTM achieves the best results. It
surpasses the baseline by nearly 20% and outperforms the
previous state-of-the-art methods around 10%. Att+Cap-
LSTM clearly improves the results over the Att-LSTM
model. This proves that internal textual representation plays
a signiﬁcant role in the VQA task. The Att+Know-LSTM
model does not perform as well as Att+Cap-LSTM , which
suggests that the information extracted from captions is
more valuable than that extracted from the KB. Cap+Know-
LSTM also performs better than Att+Know-LSTM. This
is not surprising because the Toronto COCO-QA questions
were generated automatically from the MS COCO captions,
and thus the fact that they can be answered by training on
the captions is to be expected. This generation process also
leads to questions which require little external information
to answer. The comparison on the Toronto COCO-QA thus
provides an important benchmark against related methods,
but does not really test the ability of our method to in-
corporate extra information. It is thus interesting that the
additional external information provides any beneﬁt at all.
Table 9 shows the per-category accuracy for different
models. Surprisingly, the counting ability (see question
type ‘Number’) increases when both captions and exter-
nal knowledge are included. This may be because some
‘counting’ questions are not framed in terms of the labels
used in the MS COCO captions. Ren et al.also observed
similar cases. In [18] they mentioned that “there was some

11

observable counting ability in very clean images with a
single object type but the ability was fairly weak when
different object types are present”. We also ﬁnd there is a
slight increase for the ‘color’ questions when the KB is used.
Indeed, some questions like ‘What is the color of the stop
sign?’ can be answered directly from the KB, without the
visual cue.

3

5.2.3 Results on VQA
Antol et al. [15] provide the VQA dataset which is intended
to support “free-form and open-ended Visual Question An-
swering”. They also provide a metric for measuring perfor-
,1} thus 100% means that at
mance: min{ # humans that said answer
least 3 of the 10 humans who answered the question gave
the same answer. We have used the provided evaluation
code4 to produce the results.
Evaluation: There are several splits for VQA dataset, such
as the validation set, test-develop and test-standard set. We
ﬁrst tested several aspects of our models on the validation
set (we randomly choose 5000 images from the validation
set as our val set, with the remainder testing).

Inspecting Table 10, results the on VQA validation set,
we see that the attribute-based Att-LSTM is a signiﬁcant
improvement over our VggNet+LSTM baseline. We also
evaluate another baseline, the VggNet+ft+LSTM, which
uses the penultimate layer of the attributes prediction CNN
(in Section 3.1) as the input to the LSTM. Its overall accuracy
on the VQA is 50.01, which is still lower than our proposed
models (detailed results of different question types are not
shown in Table 10 due to the limited space.) Adding either
image captions or external knowledge further improves the
result. Our ﬁnal model A+C+S-K-LSTM produces the best
results, outperforming the baseline VggNet-LSTM by 11%
overall. Some other stat-of-the-art methods such as [54],
[55], [56] produce the overall accuracy 54.69%, 48.38% and
50.48%, respectively, on the validation set (on the different
splits). The performance comparison across categories is of
particular interest here because answering different classes
of questions requires different amounts of external knowl-
edge. The ’Where’ questions, for instance, require knowl-
edge of potential locations, and ’Why’ questions typically re-
quire general knowledge about people’s motivation. ’Num-
ber’ and ’Color’ questions, in contrast, can be answered
directly. The results show that for ’Why’ questions, adding
the KB improves performance by more than 50% (Att-LSTM
achieves 7.77% while Att+Know-LSTM achieves 11.88%),
and that the combined A+C+K-LSTM achieves 13.53%. We
further improve it to 13.76% by using the question-guided
knowledge selected model A+C+S-K-LSTM.

We have also tested on the VQA test-dev and test-
standard5 consisting of 60,864 and 244,302 questions (for
which ground truth answers are not published) using our
ﬁnal A+C+S-K-LSTM model, and evaluated them on the
VQA evaluation server6. Table 11 and 12 shows the server
reported results.

Antol et al. [15] provide several results for this dataset. In
each case they encode the image with the ﬁnal hidden layer

4. https://github.com/VT-vision-lab/VQA
5. http://www.visualqa.org/challenge.html
6. https://www.codalab.org/competitions/6961

Our Proposal

Att+Cap Att+Know A+C+K A+C+S-K

MANUSCRIPT, 2016

Question

Type

what is
what colour
what kind
what are
what type
is the
is this
how many
are
does
where
is there
why
which
do
what does
what time
who
what sport
what animal
what brand
others
Overall

Our-Baseline

VggNet

+

LSTM
21.41
29.96
24.15
23.05
26.36
71.49
73.00
34.42
73.51
76.51
10.54
86.66
3.04
31.28
76.44
15.45
13.11
17.07
65.65
27.77
26.73
44.37
44.93

Att
+

LSTM
34.63
39.07
41.22
38.87
41.71
73.22
75.26
39.14
75.14
76.71
21.42
87.10
7.77
36.60
75.76
19.33
15.34
22.56
91.02
61.39
32.25
50.23
51.60

+

LSTM
42.21
48.65
47.93
47.13
47.98
74.63
76.08
46.61
76.01
78.07
25.92
86.82
9.63
39.55
78.18
21.80
15.44
25.71
93.96
70.65
33.78
53.29
55.04

+

LSTM
37.11
39.68
46.16
41.13
44.91
74.40
76.56
39.78
75.75
76.55
24.13
85.87
11.88
37.71
75.25
19.50
15.47
21.23
90.86
63.91
32.44
52.11
53.79

+

LSTM
42.52
48.86
48.05
47.21
48.11
74.70
76.14
47.38
76.14
78.11
26.00
87.01
13.53
38.70
78.42
22.16
15.34
25.74
94.20
71.70
34.60
53.45
55.96

+

LSTM
42.51
48.89
48.02
47.27
48.14
74.70
76.17
47.38
76.15
78.11
25.96
87.33
13.76
38.83
78.44
22.71
15.17
25.97
94.18
72.33
35.68
53.53
56.17

TABLE 10: Results on the open-answer task for various question types
on VQA validation set. All results are in terms of the evaluation
metric from the VQA evaluation tools. The overall accuracy for the
model of VggNet+ft+LSTM and Cap+Know+LSTM is 50.01 and 52.31
respectively. Detailed results of different question types for these two
models are not shown in the table due to the limited space.

from VggNet, and questions and captions are encoded using
a BOW representation. A softmax neural network classiﬁer
with 2 hidden layers and 1000 hidden units (dropout 0.5)
in each layer with tanh non-linearity is then trained, the
output space of which is the 1000 most frequent answers in
the training set. They also provide an LSTM model followed
by a softmax layer to generate the answer. Two version of
this approach are used, one which is given only the question
and the image, and one which is given only the question
(see [15] for details). Our ﬁnal model outperforms all the
listed approaches according to the overall accuracy. Table 13
provides some indicative results. More results can be found
in the supplementary material.

6 CONCLUSIONS
In this paper, we ﬁrst examined the importance of intro-
ducing an intermediate attribute prediction layer into the
predominant CNN-LSTM framework, which was neglected
by almost all previous work. We implemented an attribute-
based model which can be applied to the task of image
captioning. We have shown that an explicit representation
of image content improves V2L performance, in all cases.
Indeed, at the time of submitting this paper, our image
captioning model outperforms the state-of-the-art on several
captioning datasets.

Secondly, in this paper we have shown that it is possible
to extend the state-of-the-art RNN-based VQA approach so
as to incorporate the large volumes of information required
to answer general, open-ended, questions about images.
The knowledge bases which are currently available do not
contain much of the information which would be beneﬁcial
to this process, but nonetheless can still be used to signiﬁ-
cantly improve performance on questions requiring external
knowledge (such as ’Why’ questions). The approach that
we propose is very general, however, and will be applicable
to more informative knowledge bases should they become
available. We further implement a knowledge selection
scheme which reﬂects both of the content of the question

Answer Type

Number

VQA

Test-dev
Question [15]
Image [15]
Q+I [15]
LSTM Q [15]
LSTM Q+I [15]
Jiang et al. [56]
Andreas et al. [57]
Yang et al. [58]
Noh et al. [59]
Ours

Yes/No
75.66
64.01
75.55
78.20
78.94
78.33
77.70
79.30
80.71
81.02

Other
27.14
3.77
37.37
26.59
36.42
34.46
39.30
46.10
41.69
45.30

36.70
0.42
33.67
35.68
35.24
35.93
37.20
36.60
37.24
38.47

12

Overall

40.09
28.13
52.64
48.76
53.74
52.62
54.80
58.70
57.22
59.22

TABLE 11: VQA Open-Ended evaluation server results. Accuracies for
different answer types and overall performances on the test-dev.

VQA

Test-standard

LSTM Q [15]
LSTM Q+I [15]
Andreas et al. [57]
Yang et al. [58]
Noh et al. [59]
Ours

Yes/No
78.12
79.01

-
-

80.28
81.10

Answer Type

Other
26.99
36.80

Number

34.94
35.55

-
-

42.24
45.90

-
-

36.92
37.18

Overall

48.89
54.06
55.10
58.90
57.36
59.50

TABLE 12: VQA Open-Ended evaluation server results. Accuracies for
different answer types and overall performances on the test-standard.

and the image, in order to extract more speciﬁcally related
information. Currently our system is the state-of-the-art on
three VQA datasets and produces the best results on the
VQA evaluation server.

Further work includes generating knowledge-base
queries which reﬂect the content of the question and the
image, in order to extract more speciﬁcally related infor-
mation. The Knowledge Base itself also can be improved.
For instance, Open-IE provides more general common-sense
knowledge such as ‘cats eat ﬁsh’. Such knowledge will help
answer high-level questions.

ACKNOWLEDGEMENTS
This research was in part supported by the Data to Decisions
Cooperative Research Centre. Correspondence should be
addressed to C. Shen.

REFERENCES
[1] D. Bahdanau, K. Cho, and Y. Bengio, “Neural machine trans-
lation by jointly learning to align and translate,” arXiv preprint
arXiv:1409.0473, 2014.

[2] K. Cho, B. van Merrienboer, C. Gulcehre, F. Bougares, H. Schwenk,
and Y. Bengio, “Learning phrase representations using rnn
encoder-decoder for statistical machine translation,” in Proc. Conf.
Empirical Methods on Natural Language Processing, 2014.
I. Sutskever, O. Vinyals, and Q. V. Le, “Sequence to sequence
learning with neural networks,” in Proc. Advances in Neural Inf.
Process. Syst., 2014.

[3]

[5]

[4] X. Chen and C. Lawrence Zitnick, “Mind’s eye: A recurrent visual
representation for image caption generation,” in Proc. IEEE Conf.
Comp. Vis. Patt. Recogn., June 2015.
J. Donahue, L. A. Hendricks, S. Guadarrama, M. Rohrbach,
S. Venugopalan, K. Saenko, and T. Darrell, “Long-term recurrent
convolutional networks for visual recognition and description,” in
Proc. IEEE Conf. Comp. Vis. Patt. Recogn., 2015.

[6] A. Karpathy, A. Joulin, and F. F. Li, “Deep fragment embeddings
for bidirectional image sentence mapping,” in Proc. Advances in
Neural Inf. Process. Syst., 2014.
J. Mao, W. Xu, Y. Yang, J. Wang, and A. Yuille, “Deep Captioning
with Multimodal Recurrent Neural Networks (m-RNN),” in Proc.
Int. Conf. Learn. Representations, 2015.

[7]

MANUSCRIPT, 2016

13

What color is the tablecloth?

Ours:
Vgg+LSTM:
Ground Truth:

white
red
white

Why are his hands outstretched?

Ours:
Vgg+LSTM:
Ground Truth:

balance

play

balance

How many people in the photo?

2
1
2

What is the red fruit?

apple
banana
apple

What are these people doing?

eating
playing
eating

Why are the zebras in water?

drinking

water

drinking

Is the dog standing or laying down?

laying down

sitting

laying down

Which sport is this?

baseball
tennis
baseball

TABLE 13: Some example cases where our ﬁnal model gives the correct answer while the base line model VggNet-LSTM generates the wrong
answer. All results are from MS COCO-VQA. More results can be found in the supplementary material.

[8] O. Vinyals, A. Toshev, S. Bengio, and D. Erhan, “Show and tell:
A neural image caption generator,” in Proc. IEEE Conf. Comp. Vis.
Patt. Recogn., 2014.

[9] L. Yao, A. Torabi, K. Cho, N. Ballas, C. Pal, H. Larochelle, and
A. Courville, “Describing videos by exploiting temporal struc-
ture,” arXiv preprint arXiv:1502.08029, 2015.

[10] J. Devlin, H. Cheng, H. Fang, S. Gupta, L. Deng, X. He, G. Zweig,
and M. Mitchell, “Language models for image captioning: The
quirks and what works,” arXiv preprint arXiv:1505.01809, 2015.

[11] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classiﬁca-
tion with deep convolutional neural networks,” in Proc. Advances
in Neural Inf. Process. Syst., 2012.

[12] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-based
learning applied to document recognition,” Proceedings of the IEEE,
vol. 86, no. 11, pp. 2278–2324, 1998.

[13] K. Simonyan and A. Zisserman, “Very deep convolutional
large-scale image recognition,” arXiv preprint

networks for
arXiv:1409.1556, 2014.

[14] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov,
D. Erhan, V. Vanhoucke, and A. Rabinovich, “Going deeper with
convolutions,” in Proc. IEEE Conf. Comp. Vis. Patt. Recogn., 2015.

[15] S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. L. Zitnick,
and D. Parikh, “Vqa: Visual question answering - version 2,” arXiv
preprint arXiv:1505.00468v2, 2015.

[16] H. Gao, J. Mao, J. Zhou, Z. Huang, L. Wang, and W. Xu, “Are You
Talking to a Machine? Dataset and Methods for Multilingual Im-
age Question Answering,” in Proc. Advances in Neural Inf. Process.
Syst., 2015.

[17] M. Malinowski, M. Rohrbach, and M. Fritz, “Ask Your Neurons: A
Neural-based Approach to Answering Questions about Images,”
in Proc. IEEE Int. Conf. Comp. Vis., 2015.

[18] M. Ren, R. Kiros, and R. Zemel, “Image Question Answering: A
Visual Semantic Embedding Model and a New Dataset,” in Proc.
Advances in Neural Inf. Process. Syst., 2015.

[19] Q. V. Le and T. Mikolov, “Distributed representations of sentences

and documents,” arXiv preprint arXiv:1405.4053, 2014.

[20] Q. Wu, C. Shen, A. van den Hengel, L. Liu, and A. Dick, “What
value high level concepts in vision to language problems?” in Proc.
IEEE Conf. Comp. Vis. Patt. Recogn., 2016.

[21] Q. Wu, P. Wang, C. Shen, A. van den Hengel, and A. Dick,
“Ask me anything: free-form visual question answering based on
knowledge from external sources,” in Proc. IEEE Conf. Comp. Vis.
Patt. Recogn., 2016.

[22] A. Farhadi, I. Endres, D. Hoiem, and D. Forsyth, “Describing
objects by their attributes,” in Proc. IEEE Conf. Comp. Vis. Patt.
Recogn., 2009.

[23] C. H. Lampert, H. Nickisch, and S. Harmeling, “Learning to detect
unseen object classes by between-class attribute transfer,” in Proc.
IEEE Conf. Comp. Vis. Patt. Recogn., 2009.

[24] N. Kumar, A. C. Berg, P. N. Belhumeur, and S. K. Nayar, “Attribute
and simile classiﬁers for face veriﬁcation,” in Proc. IEEE Int. Conf.
Comp. Vis.

IEEE, 2009, pp. 365–372.

[25] J. Vogel and B. Schiele, “Semantic modeling of natural scenes for
content-based image retrieval,” Int. J. Comput. Vision, vol. 72, no. 2,
pp. 133–157, 2007.

[26] Y. Su and F. Jurie, “Improving image classiﬁcation using semantic

attributes,” IJCV, vol. 100, no. 1, pp. 59–77, 2012.

[27] L.-J. Li, H. Su, L. Fei-Fei, and E. P. Xing, “Object bank: A high-level
image representation for scene classiﬁcation & semantic feature
sparsiﬁcation,” in Proc. Advances in Neural Inf. Process. Syst., 2010,
pp. 1378–1386.

[28] L.-J. Li, H. Su, Y. Lim, and L. Fei-Fei, “Objects as attributes
for scene classiﬁcation,” in Trends and Topics in Computer Vision.
Springer, 2012, pp. 57–69.

[29] M. Hodosh, P. Young, and J. Hockenmaier, “Framing image de-
scription as a ranking task: Data, models and evaluation metrics,”
J. Aritiﬁcial Intelligence Research, pp. 853–899, 2013.

[30] Y. Gong, L. Wang, M. Hodosh, J. Hockenmaier, and S. Lazebnik,
“Improving image-sentence embeddings using large weakly an-
notated photo collections,” in Proc. Eur. Conf. Comp. Vis., 2014.

[31] Y. Jia, M. Salzmann, and T. Darrell, “Learning cross-modality
similarity for multinomial data,” in Proc. IEEE Int. Conf. Comp.
Vis., 2011.

[32] V. Ordonez, G. Kulkarni, and T. L. Berg, “Im2text: Describing
images using 1 million captioned photographs,” in Proc. Advances
in Neural Inf. Process. Syst., 2011.

[33] R. Socher, A. Karpathy, Q. V. Le, C. D. Manning, and A. Y. Ng,
“Grounded compositional semantics for ﬁnding and describing
images with sentences,” Proc. Annual meeting of the Association for
Computational Linguistics, 2014.

[34] A. Farhadi, M. Hejrati, M. A. Sadeghi, P. Young, C. Rashtchian,
J. Hockenmaier, and D. Forsyth, “Every picture tells a story:
Generating sentences from images,” in Proc. Eur. Conf. Comp. Vis.,
2010.

[35] S. Li, G. Kulkarni, T. L. Berg, A. C. Berg, and Y. Choi, “Composing
simple image descriptions using web-scale n-grams,” in CoNLL:
Conference on Natural Language Learning, 2011.

[36] B. Z. Yao, X. Yang, L. Lin, M. W. Lee, and S.-C. Zhu, “I2t: Image
parsing to text description,” Proceedings of the IEEE, vol. 98, no. 8,
pp. 1485–1508, 2010.

[37] G. Kulkarni, V. Premraj, V. Ordonez, S. Dhar, S. Li, Y. Choi, A. C.
Berg, and T. L. Berg, “Babytalk: Understanding and generating

MANUSCRIPT, 2016

14

simple image descriptions,” IEEE Trans. Pattern Anal. Mach. Intell.,
vol. 35, no. 12, pp. 2891–2903, 2013.

[38] A. Aker and R. Gaizauskas, “Generating image descriptions using
dependency relational patterns,” in Proc. Annual meeting of the
Association for Computational Linguistics, 2010.

[39] P. Kuznetsova, V. Ordonez, A. C. Berg, T. L. Berg, and Y. Choi,
“Collective generation of natural image descriptions,” in Proc.
Annual meeting of the Association for Computational Linguistics, 2012.
[40] P. Kuznetsova, V. Ordonez, T. L. Berg, and Y. Choi, “Treetalk:
Composition and compression of trees for image descriptions,”
Proc. Annual meeting of the Association for Computational Linguistics,
2014.

[41] M. Mitchell, X. Han, J. Dodge, A. Mensch, A. Goyal, A. Berg,
K. Yamaguchi, T. Berg, K. Stratos, and H. Daum´e III, “Midge:
Generating image descriptions from computer vision detections,”
in Proc. Conf. European Chapter of the Association for Computational
Linguistics, 2012.

[42] H. Fang, S. Gupta, F. Iandola, R. Srivastava, L. Deng, P. Doll´ar,
J. Gao, X. He, M. Mitchell, J. Platt et al., “From captions to visual
concepts and back,” in Proc. IEEE Conf. Comp. Vis. Patt. Recogn.,
2015.

[43] R. Kiros, R. Salakhutdinov, and R. S. Zemel, “Unifying visual-
semantic embeddings with multimodal neural language models,”
in Trans. Association for Computational Linguistics, 2015.

[44] A. Karpathy and L. Fei-Fei, “Deep visual-semantic alignments for
generating image descriptions,” in Proc. IEEE Conf. Comp. Vis. Patt.
Recogn., 2015.

[45] K. Xu, J. Ba, R. Kiros, A. Courville, R. Salakhutdinov, R. Zemel,
and Y. Bengio, “Show, Attend and Tell: Neural Image Caption
Generation with Visual Attention,” in ICML, 2015.

[46] X. Jia, E. Gavves, B. Fernando, and T. Tuytelaars, “Guiding Long-
Short Term Memory for Image Caption Generation,” in Proc. IEEE
Int. Conf. Comp. Vis., 2015.

[47] Y. Yang, C. L. Teo, H. Daum´e III, and Y. Aloimonos, “Corpus-
guided sentence generation of natural images,” in Proc. Conf.
Empirical Methods on Natural Language Processing, 2011.

[48] M. Rohrbach, W. Qiu, I. Titov, S. Thater, M. Pinkal, and B. Schiele,
“Translating video content to natural language descriptions,” in
Proc. IEEE Int. Conf. Comp. Vis., 2013.

[49] M. Malinowski and M. Fritz, “A multi-world approach to question
answering about real-world scenes based on uncertain input,” in
Proc. Advances in Neural Inf. Process. Syst., 2014, pp. 1682–1690.

[50] K. Tu, M. Meng, M. W. Lee, T. E. Choe, and S.-C. Zhu, “Joint
video and text parsing for understanding events and answering
queries,” MultiMedia, vol. 21, no. 2, pp. 42–70, 2014.

[51] D. Geman, S. Geman, N. Hallonquist, and L. Younes, “Visual Tur-
ing test for computer vision systems,” Proceedings of the National
Academy of Sciences, vol. 112, no. 12, pp. 3618–3623, 2015.

[52] L. Ma, Z. Lu, and H. Li, “Learning to Answer Questions From
Image using Convolutional Neural Network,” arXiv preprint
arXiv:1506.00333, 2015.

[53] Y. Zhu, O. Groth, M. Bernstein, and L. Fei-Fei, “Visual7W:
Grounded Question Answering in Images,” arXiv preprint
arXiv:1511.03416, 2015.

[54] H. Xu and K. Saenko, “Ask, Attend and Answer: Exploring
Question-Guided Spatial Attention for Visual Question Answer-
ing,” arXiv preprint arXiv:1511.05234, 2015.

[55] K. Chen, J. Wang, L.-C. Chen, H. Gao, W. Xu, and R. Nevatia,
“Abc-cnn: An attention based convolutional neural network for
visual question answering,” arXiv preprint arXiv:1511.05960, 2015.
[56] A. Jiang, F. Wang, F. Porikli, and Y. Li, “Compositional memory for
visual question answering,” arXiv preprint arXiv:1511.05676, 2015.
[57] J. Andreas, M. Rohrbach, T. Darrell, and D. Klein, “Deep compo-
sitional question answering with neural module networks,” arXiv
preprint arXiv:1511.02799, 2015.

[58] Z. Yang, X. He, J. Gao, L. Deng, and A. Smola, “Stacked At-
tention Networks for Image Question Answering,” arXiv preprint
arXiv:1511.02274, 2015.

[59] H. Noh, P. H. Seo, and B. Han, “Image question answering using
convolutional neural network with dynamic parameter predic-
tion,” arXiv preprint arXiv:1511.05756, 2015.

[60] L. Zhu, Z. Xu, Y. Yang, and A. G. Hauptmann, “Uncovering
Temporal Context for Video Question and Answering,” arXiv
preprint arXiv:1511.04670, 2015.

[61] K. Bollacker, C. Evans, P. Paritosh, T. Sturge, and J. Taylor, “Free-
base: a collaboratively created graph database for structuring
human knowledge,” in Proceedings of the 2008 ACM SIGMOD

international conference on Management of data. ACM, 2008, pp.
1247–1250.

[62] S. Auer, C. Bizer, G. Kobilarov, J. Lehmann, R. Cyganiak, and

Z. Ives, Dbpedia: A nucleus for a web of open data. Springer, 2007.

[63] J. Berant, A. Chou, R. Frostig, and P. Liang, “Semantic parsing
on freebase from question-answer pairs,” in Proc. Conf. Empirical
Methods on Natural Language Processing, 2013, pp. 1533–1544.

[64] D. Ferrucci, E. Brown, J. Chu-Carroll, J. Fan, D. Gondek, A. A.
Kalyanpur, A. Lally, J. W. Murdock, E. Nyberg, J. Prager et al.,
“Building Watson: An overview of the DeepQA project,” AI maga-
zine, vol. 31, no. 3, pp. 59–79, 2010.

[65] X. Lin and D. Parikh, “Don’t just listen, use your imagination:
Leveraging visual common sense for non-visual tasks,” in Proc.
IEEE Conf. Comp. Vis. Patt. Recogn., June 2015.

[66] F. Sadeghi, S. K. Kumar Divvala, and A. Farhadi, “VisKE: Visual
knowledge extraction and question answering by visual veriﬁ-
cation of relation phrases,” in Proc. IEEE Conf. Comp. Vis. Patt.
Recogn., June 2015.

[67] Y. Zhu, C. Zhang, C. R´e, and L. Fei-Fei, “Building a Large-scale
Multimodal Knowledge Base for Visual Question Answering,”
arXiv preprint arXiv:1507.05670, 2015.

[68] C. Zhang, J. C. Platt, and P. A. Viola, “Multiple instance boosting
for object detection,” in Proc. Advances in Neural Inf. Process. Syst.,
2005.

[69] Y. Wei, W. Xia, J. Huang, B. Ni, J. Dong, Y. Zhao, and S. Yan, “CNN:

Single-label to multi-label,” arXiv preprint arXiv:1406.5726, 2014.

[70] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Im-
agenet: A large-scale hierarchical image database,” in Proc. IEEE
Conf. Comp. Vis. Patt. Recogn., 2009.

[71] X. Chen and C. L. Zitnick, “Learning a Recurrent Visual Represen-
tation for Image Caption Generation,” in Proc. IEEE Conf. Comp.
Vis. Patt. Recogn., 2015.

[72] X. Glorot and Y. Bengio, “Understanding the difﬁculty of training
deep feedforward neural networks,” in Proc. Int. Conf. Artiﬁcial
Intelligence and Statistics, 2010, pp. 249–256.

[73] J. Pont-Tuset, P. Arbel´aez, J. Barron, F. Marques, and J. Malik,
“Multiscale combinatorial grouping for image segmentation and
object proposal generation,” in arXiv preprint arXiv:1503.00848,
March 2015.

[74] S. Hochreiter and J. Schmidhuber, “Long short-term memory,”

Neural Computation, vol. 9, no. 8, pp. 1735–1780, 1997.
to

Sutskever,

“Learning

[75] W. Zaremba

and I.

execute,”

arXiv:1410.4615, 2014.

[76] P. Young, A. Lai, M. Hodosh, and J. Hockenmaier, “From image
descriptions to visual denotations: New similarity metrics for
semantic inference over event descriptions,” Trans. Association for
Computational Linguistics, vol. 2, 2014.

[77] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,
P. Doll´ar, and C. L. Zitnick, “Microsoft COCO: Common objects in
context,” in Proc. Eur. Conf. Comp. Vis., 2014.

[78] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, “BLEU: a method
for automatic evaluation of machine translation,” in Proc. Annual
meeting of the Association for Computational Linguistics, 2002.

[79] S. Banerjee and A. Lavie, “METEOR: An automatic metric for MT
evaluation with improved correlation with human judgments,” in
Proc. the ACL workshop on intrinsic and extrinsic evaluation measures
for machine translation and/or summarization, 2005.

[80] R. Vedantam, C. L. Zitnick, and D. Parikh, “CIDEr: Consensus-
based Image Description Evaluation,” in Proc. IEEE Conf. Comp.
Vis. Patt. Recogn., 2015.

[81] X. Chen, H. Fang, T.-Y. Lin, R. Vedantam, S. Gupta, P. Dollar,
and C. L. Zitnick, “Microsoft COCO captions: Data collection and
evaluation server,” arXiv preprint arXiv:1504.00325, 2015.

[82] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick,
S. Guadarrama, and T. Darrell, “Caffe: Convolutional Architecture
for Fast Feature Embedding,” arXiv preprint arXiv:1408.5093, 2014.
[83] J. Jin, K. Fu, R. Cui, F. Sha, and C. Zhang, “Aligning where to see
and what to tell: image caption with region-based attention and
scene factorization,” arXiv preprint arXiv:1506.06272, 2015.

[84] M. Malinowski and M. Fritz, “Towards a Visual Turing Chal-

lenge,” arXiv preprint arXiv:1410.8027, 2014.

[85] Z. Wu and M. Palmer, “Verbs semantics and lexical selection,” in
Proc. Annual meeting of the Association for Computational Linguistics,
1994.

MANUSCRIPT, 2016

15

Qi Wu is a postdoctoral researcher in Australia Centre for Visual Technologies, University of Adelaide. His research interests include cross-depiction
object detection and classiﬁcation, attributes learning, neural networks and image captioning and so on. He received the Bachelor of mathematical
science degree from China Jiliang University, Master’s degree in computer science, and the PhD degree in computer vision from University of Bath,
United Kingdom in 2012 and 2015, respectively.

Chunhua Shen is a Professor of Computer Science at the University of Adelaide. He was with the computer vision program at NICTA (National ICT
Australia) Canberra for about six years before he moved back to Adelaide. He studied at Nanjing University, at Australian National University, and
received his PhD degree from the University of Adelaide. In 2012, he was awarded the Australian Research Council Future Fellowship.

Anton van den Hengel is a Professor and the Founding Director of the Australian Centre for Visual Technologies, at the University of Adelaide,
focusing on innovation in the production and analysis of visual digital media. He received the Bachelor of mathematical science degree, Bachelor
of laws degree, Master’s degree in computer science, and the PhD degree in computer vision from The University of Adelaide in 1991, 1993, 1994,
and 2000, respectively.

Peng Wang received the B.S. degree in electrical engineering and automation, and the PhD degree in control science and engineering from
Beihang University, China, in 2004 and 2011, respectively. He is now a post-doctoral researcher at the University of Adelaide.

Anthony Dick received the PhD degree in 2002 from the University of Cambridge, UK, where he worked on problems in 3D reconstruction of
architecture from images. He is currently an Associate Professor at the University of Adelaide, Australia. His research interests include image
based modeling, automated video surveillance, and image search.

MANUSCRIPT, 2016
7 ADDITIONAL RESULTS

16

Why is she wearing a crown?

Ours:
Vgg+LSTM:
Ground Truth:

birthday

to eat

birthday

Why is he smiling?

happy

unknown

happy

Why is the zebra on the ground?

resting

eat

resting

Why is a man sitting under an umbrella? Why are there animals pinned to the wall? Why do they have umbrellas?
Ours:
Vgg+LSTM:
Ground Truth:

shade
safety
shade

decoration

decoration

raining

raining

teddy

yes

Why do they have umbrellas?

shade
raining
shade

Why is he swinging backhand?

to hit ball
tennis ball
to hit ball

Why are there so many pillows on the couch? Why is there water on the ground? Why are the people wearing wetsuits? Why are the men wearing helmets?
Ours:
Vgg+LSTM:
Ground Truth:

surﬁng
safety
surﬁng

safety
yes
safety

decoration

decoration

drinking

to rest

rain

rain

Why do these sheep have paint on them? Why is his arm outﬂung? Why are the animals laying here? Why are all the giraffes gathered together?
Ours:
Vgg+LSTM:
Ground Truth:

balance
to play
balance

eating
to play
eating

identiﬁcation

identiﬁcation

resting

resting

to eat

no

Why are they wearing such bright colors? Why are the men wearing orange?
Ours:
Vgg+LSTM:
Ground Truth:

safety
yes
safety

team

team

to

Why is the man jumping?

skateboarding

unknown

skateboarding

Why is this room warm?

ﬁreplace
to sleep
ﬁreplace

TABLE 14: Some examples that our ﬁnal model gives the right answer while the base line model VggNet-LSTM generates the wrong answer. All
questions are start with ‘why’ and some of them only can be answered with common sense knowledge.

MANUSCRIPT, 2016

17

Why is this person wet?

Ours:
Vgg+LSTM:
Ground Truth:

surﬁng
beach
surﬁng

Why is the baby wearing a snowsuit? Why does the boy have his arms Why are two of the giraffes so much

cold
safety
cold

in that position?

balance
to catch
balance

shorter than the other three?

they are babies

yes

babies

Why is he at the beach in long pants?

Why is this ground white? Why is there sand around the orange object? Why is the man wearing black there?

Ours:
Vgg+LSTM:
Ground Truth:

surﬁng
to water
surﬁng

snow
cold
snow

safety

to balance

safety

umpire
safety
umpire

Why is she wearing a potholder on her arm?
Ours:
Vgg+LSTM:
Ground Truth:

cooking
drinking
cooking

Why is the road closed?

train
stop
train

Why are there no leaves on the trees? Why does he have glasses on?

winter

unknown

winter

to see
to be
to see

Why is the ground wet?

Ours:
Vgg+LSTM:
Ground Truth:

rain
cold
rain

Why is he squatting?

ﬂying kite

resting

ﬂying kite

Why is the man running?

playing frisbee

running

playing frisbee

Why is the cat sitting on the bench?

resting
to sleep
resting

Why are her hands in the air?
Ours:
Vgg+LSTM:
Ground Truth:

ﬂying kite

ﬂying kite

surﬁng

Why is the man standing?

playing tennis

tennis ball

playing tennis

ﬂying kite

playing frisbee

ﬂying kite

zoo
to eat
zoo

Why is the child running? Why is there a giraffe in this setting?

TABLE 15: Some examples that our ﬁnal model gives the right answer while the base line model VggNet-LSTM generates the wrong answer. All
questions are start with ‘why’ and some of them only can be answered with common sense knowledge.

MANUSCRIPT, 2016

18

Fig. 7: Some qualitative results of our attributes prediction, image captions and question answering. Ground truth answers are in parentheses.
Blue indicates we give the right answer, red means we are wrong.

Top -5 Attributes-top, cake, fruits, table, platesQuestion AnsweringQ1: What are the orange sticks?A1:carrots(carrots)Q2: How many carrots are in the bowls?A2: 2(over 10)Q3: Is this set up for a party?A3: yes(yes)GeneratedCaption-atabletoppedwithplatesoffood.Top -5 Attributes-jumping, watching, skate, board, peopleQuestion AnsweringQ1: What is the guy doing?A1:skateboarding(skateboarding)Q2: Are both of these skateboarders upside down?A2: yes(no)Q3: Why are there signs on the wall?A3:to keep clean (advertising)GeneratedCaption-amandoingatrickonaskateboard.Top -5 Attributes-skate, road, board, people, hillQuestion AnsweringQ1: What is the man standing on?A1: skateboard(skateboard)Q2: Is the man facing the camera?A2: no(no)Q3: Is this man worshiping the local mountains?A3: no(no)Generated Caption-ayoungmanridingaskateboarddownthesideofaroad.Top -5 Attributes-hotdog, people, eating, young, redQuestion AnsweringQ1: What coloris the woman's jacket?A1: red(red)Q2: Is the food good?A2: yes(yes)Q3: What condiments did this woman put on the hot dog?A3: ketchup(ketchup and mustard)Generated Caption-a man is holding a hot dog in his hand.Top -5 Attributes-shelf, small, book, room, televisionQuestion AnsweringQ1: What pattern is on the curtain?A1: floral (leaves)Q2: What sport is being displayed on the television?A2: football(football)Q3: Is there a bookcase nearby?A3: yes(yes)GeneratedCaption-alivingroomwithacouchandatelevision.Top -5 Attributes-giraffe, standing, tree, tall, zooQuestion AnsweringQ1: Is it warm or cold in this picture?A1: warm(warm)Q2: What type of animal is this?A2:  giraffe(giraffe)Q3: Do you find a stone wall?A3: yes(yes)Generated Caption-a giraffe standing next to a tree in a zoo enclosure.Top -5 Attributes-table, vegetables, broccoli, carrots, onionsQuestion AnsweringQ1: Do all the vegetables have roots?A1: no(no)Q2: Wouldn't you like to participate in a CSA with veggies like these?A2: yes(yes)Generated Captions-a living room with a couch and a coffee table.Top -5 Attributes-room, couch, pillows, table, coffeeQuestion AnsweringQ1: What room is this?A1: living room (living room)Q2: What shape is the table without any lamps on it?A2: round(round)Generated Captions -a bunch of green vegetables on a table along with some literature.MANUSCRIPT, 2016

19

Fig. 8: Some qualitative results of our attributes prediction, image captions and question answering. Ground truth answers are in parentheses.
Blue indicates we give the right answer, red means we are wrong.

Top -5 Attributes-zebra, standing, ground, two, zooQuestion AnsweringQ1: Where is this picture taken?A1:zoo (zoo)Q2: How many zebras?A2:2(2)Q3: Is the zebra eating cake?A3:yes(no)Generated Caption-two zebras standing next to each other in a zoo enclosure.Top -5 Attributes-wine, table, meat, white, vegetablesQuestion AnsweringQ1: What is this drink?A1: wine(wine)Q2: How many slices of meat is here?A2: 2(6)Q3: What brand of wine is that?A3: Daisies(Bock)Generated Captions-a plate of food sitting on a table with a glass of wine.Top -5 Attributes-baseball, bat, swinging, red, peopleQuestion AnsweringQ1: What brand of cleats is the athlete wearing?A1: Nike(Nike)Q2: What type of hat is the better wearing?A2: baseball(helmet)Generated Caption-a baseball player is swinging a bat at a ball.Top -5 Attributes -people, holds, cellphone, air, racketQuestion AnsweringQ1: Is he holding a camera?A1: yes (yes)Q2: What is on the man's back?A2: backpack(backpack)Q3: Is he bald?A3: yes(yes)Generated Caption-a man is holding a cell phone in his hand.Generated Caption-a small bathroom with a toilet and a sink.Top -5 attributes-bathroom, door, small, wall, sinkQuestion AnsweringQ1: What type of room is this?A1: bathroom(bathroom)Q2: Is there medicine in the medicine cabinet?A2: no(no)Q3: What is the wall made of?A3: brick(stone)Generated Caption-a bunch of bananas hanging from a tree.Top -5 Attributes-bananas, tree, large, green, groundQuestion AnsweringQ1: Is this a fruit or vegetable?A1: fruit(fruit)Q2: Are these bananas ripe?A2: no(no)Q3: Does this tree have large leaves?A3: no(yes)Generated Caption-a herd of sheep standing on top of a lush green field.Top -5 Attributes-sheep, field, grass, standing, greenQuestion AnsweringQ1: Which animals are these?A1: sheep(sheep)Q2: Will the sheep taste good?A2: yes(yes)Q3: What type of ecosystem was this picture taken in?A3: sheep(farm)Generated Caption-a man sitting on a bench in front of a building.Top -5 Attributes-bench, park, people, sitting, whiteQuestion AnsweringQ1: What colorare the slats on the bench?A1: green(green)Q2: What's the statue holding?A2: umbrella(newspaper)Q3: What coloris the statue?A3: white(white)MANUSCRIPT, 2016

20

Fig. 9: Some qualitative results of our attributes prediction, image captions and question answering. Ground truth answers are in parentheses.
Blue indicates we give the right answer, red means we are wrong.

Top -5 Attributes-bathroom, sink, wall, two, yellowQuestion AnsweringQ1: What kind of room is this?A1:bathroom(bathroom)Q2: How many sinks are there?A2:2(2)Q3: Is the door across from the sinks?A3:yes(yes)Generated Caption-a bathroom with a toilet sink and a mirror.Top -5 Attributes-snow, people, snowboard, air, ridingQuestion AnsweringQ1: What sport is the man engaging in?A1: snowboarding(snowboarding )Q2: Is the man touching the ground?A2: no(no)Q3: What is on the man's hands?A3: gloves(gloves)Generated Caption-a person riding a snow board in the air.Top -5 Attributes-horse, field, brown, grass, standingQuestion AnsweringQ1: What is the animal eating?A1: grass(grass)Q2: Is there a fence?A2: yes(yes)Q3: Is there a house in the background?A3: yes(yes)Generated Caption-a brown horse standing on top of a grass covered field.Top -5 Attributes-car, traffic, road, tree, peopleQuestion AnsweringQ1: Is the street crowded?A1: yes(yes)Q2: Can you see the body of ocean in the back?A2: yes(yes)Q3: How many red trucks are there?A3: 2(8)Generated Caption-a busy city street filled with lots of cars.Top -5 Attributes-skate, board, people, road, ridingQuestion AnsweringQ1: Is the skateboarder casting a shadow?A1: yes(yes)Q2: Is the boy airborne?A2: yes(yes)Generated Caption-a man riding a skateboard down a sidewalk.Top -5 Attributes-boat, young, playing, water, childrenQuestion AnsweringQ1: Is it a chilly day?A1: yes(yes)Q2: Are the children fishing?A2: no(no)Q3: Is this a recent photo?A3: no(no)Generated Caption-a group of young men playing a game of frisbeeon a beach.Top -5 Attributes-people, tennis, racket, ball, hittingQuestion AnsweringQ1: What is the sport the man is playing?A1: tennis(tennis)Q2: Did the man hit the ball?A2: yes(yes)Q3: What car advertisement is in the background?A3: Mercedes-benz(Mercedes-benz)Generated Caption-a man swinging a tennis racket at a tennis ball.Top -5 Attributes-motorcycle, riding, people, office, helmetQuestion AnsweringQ1: Is this a police officer?A1: yes(yes)Q2: Is the police officer happy?A2:yes(yes)Q3: What type of vehicle is the policeman driving?A3: motorcycle(motorcycle )Generated Caption-a police officer riding a motorcycle on a city street.Top -5 Attributes-room, furniture, large, windows, couchQuestion AnsweringQ1: Is it a sunny day?A1: yes(yes)Q2: Is there a big window?A2: yes(yes)Q3: What room is this?A3: living room (living room)Generated Caption-a living room filled with furniture and a large window.Generated Caption-a woman is playing a video game in a living room.Top -5 Attributes-people, playing, wii, room, glassQuestion AnsweringQ1: What gaming system is the woman playing?A1: Wii(Wii)Q2: Do you see pillows on the couch?A2: yes(yes)Q3: What coloris the game controller?A3: white(white)MANUSCRIPT, 2016

21

What kind of weather it is?
Ours:
Vgg+LSTM:
Ground Truth:

sunny
cloudy
sunny

What is in the cup?

coffee
wine
coffee

What game is being played on the beach?
Ours:
Vgg+LSTM:
Ground Truth:

volleyball

volleyball

soccer

How many busses are there?

1
2
1

What kind of room is this?

bedroom

living

bedroom

Where did the water come from?

ocean
beach
ocean

Is the person wearing a shirt? What is the colorful object in the middle of the image?

no
yes
no

kite

frisbee

kite

What are the children holding?
teddy bears

Ours:
Vgg+LSTM:
Ground Truth:

wii

teddy bears

Where is this picture?

market
on left
market

What kind of cheese is this?

mozzarella

chicken

mozzarella

What style of cooking is this?
Ours:
Vgg+LSTM:
Ground Truth:

chinese
pizza
chinese

men’s
hotel
men’s

Is this a men’s room or a women’s room? What is on the top of the animals’ heads?

What room is this?

bathroom
kitchen
bathroom

Is this a vegetable?

yes
no
yes

How many airplanes?

4
1
4

horns
rocks
horns

stop
new
stop

What are the cats sleeping on?

Is it safe for the pedestrians to cross the street? What other word is written for this sign?

Ours:
Vgg+LSTM:
Ground Truth:

car
table
car

no
yes
no

TABLE 16: Some examples that our ﬁnal model gives the right answer while the base line model VggNet-LSTM generates the wrong answer.
Various question types are shown.

MANUSCRIPT, 2016

22

What is he looking at?

toothbrush

camera

toothbrush

Ours:
Vgg+LSTM:
Ground Truth:

Is this inside?

Ours:
Vgg+LSTM:
Ground Truth:

no
yes
no

What kind of meat is on this?

bacon
chicken
bacon

Which game is being played?

soccer
tennis
soccer

What brand is the bat bag?

nike
wilson
nike

What season does it look like?

winter

fall

fall

Is this a healthy breakfast?

no
yes
no

Is this meal healthy?

yes
no
yes

The green item on the pizza, what is it called?
Ours:
Vgg+LSTM:
Ground Truth:

broccoli
carrots
broccoli

Does this animal have fur?

no
yes
no

yes
no
yes

Is this a home ofﬁce? What letter is inside the blue circle?

p
b
p

What type of food is this person eating?
Ours:
Vgg+LSTM:
Ground Truth:

donut
pizza
donut

In what type of establishment is this taken?

zoo
zebra
zoo

Is that meat on the plate?

yes
no
yes

What is being celebrated?

birthday

pizza

birthday

What kind of building is this?

Ours:
Vgg+LSTM:
Ground Truth:

barn
church
barn

Is the weather cold or warm?

cold
sunny
cold

Is that normal a banana on a record?

no
yes
no

What color is the snow?

white
blue
white

TABLE 17: Some examples that our ﬁnal model gives the right answer while the base line model VggNet-LSTM generates the wrong answer.
Various question types are shown.

MANUSCRIPT, 2016

23

What season is it?
fall

Ours:
Ground Truth: winter

What shoe company is advertised?

vans
nike

Is this guy going to jump high?

no
yes

What is on the keyboard?

mouse

cat

What color is the front of the tow truck? What kind of wood is the table made of?
Ours:
Ground Truth:

red
white

oak

cherry

Where is the telephone?

on desk

on nightstand

How deep is water?

shallow
10 feet

Who ate some of the cake?

Ours:
Ground Truth:

man
person

What city is this?

new york
las vegas

What utensils are shown?

fork and knife

fork

What is she holding in her hand? What creature is this?
Ours:
Ground Truth:

ski poles
ski pole

horse
pegasus

What are the colors of the court?

blue

blue and green

What’s on the ﬂoor?

Ours:
Ground Truth:

scissors

tape

Who took this photo?

photographer

christopher brown

What food is this, really?

chicken

cake

TABLE 18: Some fail cases produced by our ﬁnal model

What is the building facade made from?

brick
stone

What is on their hand?

hot dog
glove

What kind of weather is this?

rainy
cloudy

