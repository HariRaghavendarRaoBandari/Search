6
1
0
2

 
r
a

M
2

 

 
 
]
S
D
h
t
a
m

.

[
 
 

1
v
4
5
8
0
0

.

3
0
6
1
:
v
i
X
r
a

Continuous products of matrices

Alexander Vladimirova

aInstitute for Information Transmission Problems

Russian Academy of Sciences

Abstract

We answer the question if the continuous product of square matrices M (t) over t ∈ [0, 1] can be correctly
deﬁned. The case where all M (t) are taken from a ﬁnite set Σ is studied. We ﬁnd necessary and suﬃcient
conditions on Σ that ensure the convergence of products M (t0 = 0)M (t1) . . . M (tN = 1) as the partition
0 < t1 < · · · < 1 reﬁnes. These conditions are properties LCP (left convergent product) and RCP (right
convergent product) of the set Σ. That is, it suﬃces to require the convergence of all ﬁnite products
M1M2 . . . MK and MK . . . M2M1 as K → ∞, where Mi ∈ Σ. The theory of joint spectral radius is heavily
used.

Keywords: Sweeping process, oblique projection, left convergent product, joint spectral radius.

1. Introduction

Directional derivatives of polyhedral sweeping processes [8–11] are closely related to ﬁnite sets of projec-
tion matrices and their inﬁnite products taken in special order. Here we develop the theory of continuous
products of matrices keeping in mind its possible applications to sweeping process and similar hysteresis
systems.

Suppose we want to deﬁne correctly the “multiplicative integral” of a matrix-valued function M (t) ∈
Σ ⊆ Mn(R) (or Mn(C)) over the time interval [0, 1]. That is, the question is if there exists a limit of ﬁnite
products M (t1) . . . M (tK) as the partitions 0 ≤ t1 < · · · < tK ≤ 1 of [0, 1] reﬁne in the sense of inclusion.

An obvious case where the answer is positive is that of a ﬁnite family Σ such that ρ(Σ) < 1, where ρ(Σ)
is the joint spectral radius, see [1, 3, 4, 6, 7, 12]. It is not surprising that each function M : [0, 1] → Σ has
the zero continuous product over [0, 1] (or over any other inﬁnite linearly ordered set).

There are less obvious cases where the limit exists, again, for each map M : [0, 1] → Σ (no continuity or
any other regularity is needed) and may be diﬀerent from zero. As we prove in this paper, for ﬁnite families
Σ, this happens exactly if Σ is both of left convergent products (LCP) and right convergent products (RCP)
type.

Additionally, we demonstrate that LCP and RCP properties together imply transversality (TR) of the
family Σ and, moreover, that LCP and TR imply RCP and that RCP and TR imply LCP. It happens
also that LCP and RCP imply the convergence of any inﬁnite sequence of matrix products where each
subsequent product is obtained by insertion of an arbitrary matrix A ∈ Σ in an arbitrary position of the
previous product (CP property).

2. Deﬁnitions

Let a ﬁnite set Σ = {Aj : j ∈ J = {1, . . . , k}} of real n×n-matrices be given. Denote by Σm, m = 1, 2, . . . ,
the set of all products Aj1 . . . Ajm , Aji ∈ Σ for i = 1, . . . , m. Denote also Σ0 = {I}, where I is the identity

1The research was carried out at the IITP RAS at the expense of the Russian Foundation for Sciences (project 14-50-00150).
Email address: vladim@iitp.ru (Alexander Vladimirov)

Preprint submitted to Elsevier

n×n-matrix. Let

L(Σ) = ∪m=0,1,...Σm.

The set Σ is said to be product bounded if there exists a C > 0 such that kAk < C, A ∈ L(Σ).

Deﬁnition 2.1. A set Σ is called LCP (left convergent products) if, for any sequence S = {ji ∈ J :
1, 2, . . . }, there exists a limit matrix LS such that

i =

lim
m→∞

kLm

S − LSk = 0,

where

The set Σ is called RCP if, for any sequence S = {ji ∈ J : i = 1, 2, . . . }, there exists a limit matrix RS

Lm

S = Ajm . . . Aj1 , m = 1, . . . .

(2.1)

such that

where

lim
m→∞

kRm

S − RSk = 0,

These properties are not the same as the following simple example demonstrates. Let Σ = {A1, A2} in

R2, where

Rm

S = Aj1 . . . Ajm , m = 1, . . . .

(2.2)

A1 = (cid:18) 1

0

0

0 (cid:19) , A2 = (cid:18) 1 1

0 0 (cid:19) .

Since A1A2 = A2 and A2A1 = A1, we get Aj1 . . . Ajm = Ajm for any sequence ji ∈ {1, 2}, i = 1, . . . , m, and
the set Σ is LCP and not RCP.

Both properties LCP and RCP are stronger than the product boundedness [1]. Transposing the matrix

equality (2.1) we get the following assertion.

Proposition 2.2. A set Σ = {Aj : j ∈ J} is LCP if and only if the dual set Σ∗ = {A∗

j : j ∈ J} is RCP.

By discrete linear inclusion DLI(Σ) we will understand the set of all inﬁnite sequences {xi}, i = 0, 1, . . . ,

of vectors in Rn such that

xi = Aji xi−1,

ji ∈ J,

i = 1, 2, . . . .

(2.3)

These sequences will be called paths of Σ. The LCP property is equivalent to convergence of any path {xi}
of Σ, see [1].

There are equivalent deﬁnitions of LCP and RCP for ﬁnite sets Σ, see, for instance, [13]. The family Σ
is LCP if and only if all its paths have bounded variation. It is RCP if and only if each family of aﬃne maps

Ψ = {A + (A − E)hA : A ∈ Σ, hA ∈ Rn}

generates a bounded semigroup. They are contraction semigroups if Σ is irreducible. For aﬃne semigroups
see [14] and the bibliography within.

For any subset J ′ ⊆ J of indices, let us deﬁne two subspaces of Rn:

NJ ′ = \j∈J ′

N (I − Aj), RJ ′ = spanj∈J ′ {R(I − Aj)},

where N (A) = {x ∈ Rn : Ax = 0} is the nullspace of A and R(A) = {Ax : x ∈ Rn} is the range of A.

Both NJ ′ and RJ ′ are invariant subspaces for paths of the subset Σ′ = {Aj ∈ Σ : j ∈ J ′}. Moreover,
any aﬃne subspace of the form x + RJ ′ , x ∈ Rn, is also invariant for these paths because xi − xi−1 ∈ RJ ′
whenever {xi} is a path of Σ′.

it is Rn-transversal if
Deﬁnition 2.3. The set Σ is 0-transversal if NJ ′ ∩ RJ ′ = 0 for each J ′ ⊆ J;
NJ ′ + RJ ′ = Rn for each J ′ ⊆ J. The set Σ is transversal if NJ ′ ⊕ RJ ′ = Rn for each J ′ ⊆ J, that is, if it is
both 0-transversal and Rn-transversal.

2

Obviously, the properties of 0-transversality and Rn-transversality are dual to each other: any one of
them holds for Σ if and only if the other one holds for Σ∗ = {A∗ : A ∈ Σ}. Let us prove a simple auxiliary
assertion.

Lemma 2.4. If Σ is LCP then Σ is Rn-transversal.

6= Rn for some J ′ ⊆ J. Then there exists an x0 ∈ Rn
Proof. Suppose the contrary, that is, NJ ′ + RJ ′
such that (x0 + RJ ′ ) ∩ NJ ′ = ∅. Consider a path {x0, x1, . . . } of Σ where each matrix Aj, j ∈ J ′ is used
inﬁnitely many times. Then the limit point x∗ = limi→∞ xi belongs to NJ ′. Since xi ∈ x0 + RJ ′ , we also
have x∗ ∈ x0 + RJ ′ , which is a contradiction.

Hence, the dual assertion is also true:

Lemma 2.5. If Σ is RCP then Σ is 0-transversal.

Thus, we get the following result.

Proposition 2.6. If Σ is both LCP and RCP then it is transversal.

3. Relations between LCP and RCP

We will need the following assertion from [5].

Theorem 3.1. Suppose Σ = {Aj : j ∈ J} is LCP and NJ = {0}. Then there exists a norm k · kΣ in Rn
and a constant 0 ≤ q < 1 such that

kAjkΣ ≤ 1, j ∈ J,

and

kAj1 . . . Ajm kΣ ≤ q

(3.1)

(3.2)

for any ﬁnite product containing each Aj from Σ.

Theorem 3.2. If Σ = {Aj : j ∈ J} is LCP and 0-transversal then it is also RCP.

Proof. Let us use induction on the cardinality k of Σ. For k = 1, there is no diﬀerence between LCP and
RCP by deﬁnition.

Suppose the assertion is true for all matrix sets of cardinality 1, . . . , k − 1 and prove it for Σ = {Aj :
j ∈ J = {1, . . . , k}}. Let us ﬁrst assume NJ = {0} Consider a right-inﬁnite product of matrices Aj1 Aj2 . . . .
Suppose that any j ∈ J occurs in the sequence {j1, j2, . . . } inﬁnitely many times. Then we can represent
each ﬁnite product Aj1 . . . Ajm as B1 . . . Bs(m), where each Bp, p = 1, . . . , s(m), is a ﬁnite product of matrices
Aji containing all Aj, j ∈ J, and s(m) → ∞ as m → ∞. Then, according to Theorem 3.1, the sequence of
products Aj1 . . . Ajm converges to the zero matrix as m → ∞.

Let us consider the remaining cases, that is, suppose that for some j ∈ J, there exists an m > 0 such
that j 6∈ {jm, jm+1, . . . }. By the induction hypothesis, the product Ajm Ajm+1 . . . converges to some matrix
A and, hence, the product Aj1 . . . Ajm Ajm+1 . . . converges to the matrix Aj1 . . . Ajm−1 A, and the required
assertion is proved.

It remains to consider the case NJ 6= {0}. Because of the 0-transversality assumption and Lemma 2.4
we have Rn = NJ ⊕ RJ . Since the subspaces RJ and NJ are invariant to all Aj ∈ Σ, we can reduce all
matrices Aj to the form

Aj = (cid:18) I

0

0

˜Aj (cid:19)

by the same nonsingular linear change of variables corresponding to the decomposition Rn = NJ ⊕ RJ .
Obviously, both the LCP and RCP properties of Σ are equivalent to those of ˜Σ = { ˜Aj : j ∈ J}, and for ˜Σ
we have ˜NJ = {0}.

Transposition of matrices Aj gives us the dual assertion:

3

Theorem 3.3. If Σ = {Aj : j ∈ J} is RCP and Rn-transversal then it is also LCP.

Consider the three properties LCP, RCP and transversality (TR) of sets of matrices. Proposition 2.6

and Theorems 3.2, and 3.3 yield the following result.

Theorem 3.4. For a ﬁnite set of matrices, any two of the properties LCP, RCP, and TR imply the third
one.

Let us say that the set Σ is CP if it is LCP, RCP, and transversal. As an example of a CP set, consider any
ﬁnite set Σ of orthogonal projections Pj onto subspaces Lj ⊆ Rn of arbitrary dimensions. The transversality
of Σ is immediate. The LCP property of Σ is an easy consequence of the following result [13].

Proposition 3.5. If, for any path {xi} of a ﬁnite set Σ, the relation

lim
i→∞

kxi+1 − xik = 0

(3.3)

holds, then Σ is LCP.

Indeed, for any j ∈ J, we have

kPjxk = qkxk2 − kPjx − xk2

and, hence, (3.3) must hold for any path {xi} of Σ.

4. Continuous products of matrices

In all what follows Σ is a ﬁnite CP set. Let R be an arbitrary linearly ordered set and let A be a map from
R to Σ (a matrix-valued function on R ranging in Σ). For any ﬁnite subset S = {s1 < s2 < · · · < sk} ⊂ R,
the product

MS = A(s1) . . . A(sk)

is deﬁned. Since Σ is CP, this deﬁnition is extended in a natural way to any countable subset S ⊂ R of the
form S = {s1 < s2 < . . . } or S = {s1 > s2 > . . . }. We will deﬁne the product

MS = Yr∈S

A(r)

(4.1)

for each subset S of R. It suﬃces to do this for R itself because each subset of a linearly ordered set is again
a linearly ordered set.

Let us use an iterative procedure on k = #J for this deﬁnition (by # we denote the cardinality of the
set). Suppose that, for all J ′ such that #J ′ < K, the products (4.1) are deﬁned. Deﬁne IR(A) as the
maximal number of disjoint intervals Si ⊆ R such that A(Si) = Σ (let us call them complete intervals). By
an interval we understand a set S ⊂ R without holes, that is, conditions a, b ∈ S, g ∈ R, a < g < b should
imply g ∈ S. Note that we can assume without loss of generality that ∪iSi = R, that is, intervals Si are
adjacent. For any incomplete interval S, the product M (S) is already deﬁned.

Now, we need an auxiliary result.

Lemma 4.1. If IR(A) < ∞, there exists a partition of R into no more than 3IR(A) adjacent disjoint
incomplete intervals.

Proof. First, let IR(A) = 1. Let us say that the interval S is a left interval of R if

S = ∪r∈S{x ∈ R : x ≤ r}.

Let us show that there exists an incomplete left interval of R. Suppose the contrary. Then R has no minimal
element. Let us choose a subset P = {r1, . . . , rk} ⊆ R, r1 < r2 < · · · < rk such that A(P ) = Σ. The interval
[r1, r2] is complete, and the interval Q = {x ∈ R : x < r1} is also complete, which is a contradiction.

4

Now, let S1 be the union of all incomplete left intervals. (it is, obviously, incomplete itself). Denote
R1 = R\S1 and repeat the same argument to construct the interval S2 as the maximal incomplete left
interval of R1. If R2\(S1 ∪ S2) is nonempty, we repeat the same construction, and so on. Suppose that R3
is nonempty. Then choose some r′ ∈ S2 and r′′ ∈ R3. Obviously, the intervals

I1 = {x ∈ R : x ≤ r′} and I2 = S3 ∪ {x ∈ R3 : x ≤ r′′}

are complete and disjoint. The contradiction ﬁnishes the proof for IR(A) = 1. Finally, in the general case
m = IR(A) < ∞, the set R can be partitioned into m disjoint adjacent complete intervals Qi such that
IQi (A) = 1, i = 1, . . . , m, and then each one of these intervals is partitioned into no more than 3 disjoint
incomplete intervals.

Let us now ﬁnish the deﬁnition of product (4.1). If IR(A) < ∞, we deﬁne this product by partitioning R
into a ﬁnite number of adjacent disjoint incomplete intervals; this is possible because of Lemma 4.1. If, on
the contrary, IR(A) = ∞, then we deﬁne M as the projection PJ on NJ along RJ . Because of transversality
of Σ, the projection PJ (x) is well deﬁned as a unique element y ∈ NJ satisfying x − y ∈ RJ .

Let us study basic properties of the map M (S) : 2R → Σ. First, for ﬁnite sets S, we have M (S) =

Ar1 . . . Arm, where

in the sense of the order on R.

S = {r1, . . . , rm}

and r1 < r2 < · · · < rm

Lemma 4.2. Let S1, S2 ⊆ R and S1 < S2, that is, r1 < r2 for each pair r1 ∈ S1, r2 ∈ S2. Then
M (S1 ∪ S2) = M (S1)M (S2).

Proof. Let us consider four possible cases:
(i) IS1 (A) < ∞ and IS2 (A) < ∞,
(ii) IS1 (A) = ∞ and IS2 (A) < ∞,
(iii) IS1 (A) < ∞ and IS2(A) = ∞,
(iv) IS1 (A) = ∞ and IS2 (A) = ∞.

For instance, if IS1(A) < ∞ and IS2(A) = ∞, we have IS(A) = ∞ and M (S) = PJ . Then the required

statement follows from the easy fact PJ Aj = PJ for any Aj ∈ Σ. The remaining cases are also obvious.

More generally, suppose F is a monotone map from R to another linearly ordered set Q, that is, r2 ≥ r1

implies F (r2) ≥ F (r1). Deﬁne M ′(q) = M (F −1(Q)) for each q ∈ Q.

Theorem 4.3. For each subset S ′ ⊆ Q, we have

M ′(S ′) = M (F −1(S ′)).

Proof. Let us use induction on k = #J. It suﬃces to consider the cases IF −1(S ′)(A) = ∞ and IF −1(S ′)(A) <
∞. In each case, the required assertion follows directly from the deﬁnitions.

The following assertion, again, follows from elementary induction considerations.

Theorem 4.4. Any M (S) can be represented as a limit of matrices Mi ∈ L(Σ) as i → ∞.

Theorem 4.5. The matrix-valued function M (Si(r)) on R has bounded variation for i = 1, . . . , 4, where
S1(r) = {p ∈ R : p ≤ r}, S2(r) = {p ∈ R : p < r}, S3(r) = {p ∈ R : p ≥ r}, and S4(r) = {p ∈ R : p > r}.

Proof. As is known [13], there exists an upper bound V on the variation

V ({Ai}) =

m−1

Xi=1

kMi − Mi+1k, where Mi = A1 . . . Ai,

of any ﬁnite product A1 . . . Am, Ai ∈ Σ, i = 1, . . . , m. Let us consider a ﬁnite partition of R into intervals
Si, i = 1, . . . , m, and prove that the same bound is valid for the product M (S1) . . . M (Sm). Indeed, by

5

Theorem 4.4, any M (S) can be approximated by ﬁnite products from L(Σ) with arbitrary precision, and,
hence

m−1

kM (Si+1) − M (Si)k ≤ V.

Xi=1

Theorem 4.6. The matrix M (R) can be found as an inductive limit of all ﬁnite products M (G), G ⊆ R,
that is, For each ε > 0, there exists a ﬁnite set F ⊆ R such that

kM (F ′) − M (R)k < ε

for all ﬁnite F ′ ⊇ F.

Proof. If IR(A) = ∞, this is obvious. If IR(A) < ∞, we use the induction on #J again.

Hence, the product M (R) can also be deﬁned by means of the following formal constructions. Let F be
the family of all ﬁnite subsets F ⊆ R. Considering F as a directed set with respect to the order relation
F ≤ F ′ ⇔ F ⊆ F ′, we can deﬁne M (R) as the limit of the net {M (F ) : F ∈ F}.

5. Insertions

The LCP property means that if, at each discrete time instant, a matrix from Σ is added to the current
product at the left, then the resulting procedure converges. For RCP left should be replaced by right. It
is also easy to see that, for a CP set Σ, one can add matrices alternately at the left and at the right, in
any sequence, and the resulting product still converges. Now, we are going to prove that, for CP sets, this
procedure can be generalized so that any subsequent matrix can be inserted at any place of the current
product, the front and the rear positions included.

Theorem 5.1. Let a sequence Mi ∈ Σi, i = 1, . . . , possess the following property. For each i, there exists
an mi, 0 ≤ mi ≤ i such that

Mi = M−M+ and Mi+1 = M−Aji M+,

where M− ∈ Σmi, M+ ∈ Σi−mi, and Aji ∈ Σ. Then Mi converges to some n×n-matrix as i → ∞.

Proof. Let us introduce a linear order relation ≻ on the set of indices 1, 2, . . . as follows. We will write
i′ ≻ i′′ if the matrix Aji′ takes position to the right of Aji′′ in the product Mi, where i = max{i′, i′′}.

Let us choose a countable number of reals xi such that xi > xj if and only if i ≻ j. This kind of choice
is possible because, if at step i the set {xj : j ≻ i} satisﬁes this requirement, the next point xi+1 can also
be chosen in a way that the requirement still holds for the set {xj : j ≻ i + 1}. The assertion of the theorem
follows now from Theorem 4.6, where R = {xi : i = 1, 2, . . . } equipped with the order induced by the natural
order on R, and A(xi) = Aji .

Moreover, the following stronger assertion holds.

Theorem 5.2. There exists a uniform upper bound V on the variation of any sequence Mi from the hy-
pothesis of Theorem 5.1.

Proof. Let us again use induction on #J and suppose that the assertion is proved for all proper subsets of
Σ (denote the corresponding upper bound by B0). Let R be the set deﬁned in the proof of Theorem 5.1
and A(xi) = Aji . Denote by Rj the subset {xi : i = 1, . . . , j} of R. Denote also k(j) = IRj (A), j = 1, 2, . . . .
The sequence k(j) is nondecreasing and k(1) = 0 if #J > 1. Now, denote by lm the maximal index j such
that k(j) = m (if it exists).

Let us ﬁrst ﬁnd an upper bound on the variation of the ﬁnite sequence M1, . . . Ml1. By Lemma 4.1, the
set Rl1 can be partitioned into 3 incomplete intervals. At each step i < l1, the matrix Aji is inserted into

6

one of these subintervals. By the induction assumption, variation of the product matrix for each subinterval
does not exceed B0 and, hence, because of (3.1), the variation of {M1, . . . , Ml1} does not exceed 3B0.

Now, let us ﬁnd an upper bound for the variation of

Mm = {Mlm−1+1, . . . , Mlm}

for an arbitrary m. The set Rlm can be divided into m disjoint adjacent intervals Si such that ISi (A) = 1,
i = 1, . . . , m. Whenever a matrix is insereted into one of these intervals, the variation of the whole product
at this step does not exceed qm−1V ′, where V ′ is the variation of the current product, at which the insertion
occurs (q < 1 is the constant from Theorem 3.1). Thus, the variation of Mm is bounded from the above by

sm = 3mqm−1. But the sum Pm=1,... sm is bounded since q < 1, hence, the theorem is proved.

Let us formulate a generalization of Theorem 5.2. The proof is completely analogous to that of Theorem

5.2, so we leave it to the reader.

Theorem 5.3. Let R be a linearly ordered set and let A be a map from R to a CP set Σ. Suppose Q
is another linearly ordered set and B is a monotone map from Q to 2R, that is, B(q2) ⊇ B(q1) whenever
q2 ≥ q1. Then the map M ′(q) = M (B(q)) possesses the bounded variation property in the sense of Theorem
4.5.

6. Linear control systems

In this section we assume that R has the minimal element r− and the maximal element r+. Suppose

f (r) is a bounded function from R to Rn. Let us deﬁne the integral

ZR

f (r)dM (r), where M (r) = M ({s ∈ R : s ≤ r}),

(6.1)

as follows. For each ﬁnite subset F ⊆ R of the form

F = {r1, . . . , rm},

r1 = r− < r2 < · · · < rm−1 < rm = r+,

deﬁne

Then deﬁne

m−1

H(F ) =

Xi=1

(M (ri+1) − M (ri))f (ri).

ZR

f (r)dM (r) = lim
F →R

H(F ),

(6.2)

where the limit for the net of all ﬁnite subsets of R containing r− and r+ is considered in the right-hand
side. The expression (6.2) is well deﬁned because of Theorem 4.5.

The integral (6.2) is an anlogue of the standard Lebesgue–Stiltjes integral. Finally, let us deﬁne formally

ZR

M (r)df (r) = M (r+)f (r+) − M (r−)f (r−) −ZR

f (r)dM (r).

(6.3)

The variable integral x(r) = RRr

system with the input f (r), r ∈ R.

M (s)df (s), r ∈ R, can be interpreted as the output of a linear control

7

7. Recognizing CP property

Let us say a few words on the general structure of CP families and on the hardness of their recognition.
First of all, the ﬁniteness property holds for CP families that have ρ(Σ) = 1. Moreover, if ρ(Σ) = 1, then
there exists a matrix A ∈ Σ such that ρ(A) = 1.

Indeed, if this is not the case, there exists a sequence Ai ∈ Σ that contains inﬁnite number of entries of
at least two diﬀerent matrices A and B and such that limk→∞ Ak . . . A1 6= 0. Then any non-zero column
of the limit matrix is invariant for A and B which is impossible by assumption. For each A ∈ Σ the limit
limm→∞ Am exists.

Thus we have two options for Σ. If ρ(Σ) < 1, it is a CP family. The hardness of recognition of this case
is still an open problem as far as we know. It is conjectured that the problem is algorithmically unsolvable
for matrices with rational entries, the same way as it happens for a similar problem ρ(Σ) ≤ 1, see [2].

The second case is ρ(Σ) = 1 and then the hardness of recognition is the same as in the ﬁrst case. Indeed,

let Σ = {A, B, C}, where

A = 


E 0
0
M1
M1
0

0
0
0


 , B = 


0 M2
0
0 E 0
0
0

0


 , C = 


0
0
0

0 M3
0
0
0 E




and Mi are square n/3×n/3-matrices.

Then Σ is a CP family if and only if the set Ψ of two matrices M2M1 and M3M1 is asymptotically stable,
that is, if ρ(Σ) < 1 (otherwise the variation of paths is not upper bounded). These are arbitrary matrices
of size n/3×n/3, hence the hardness of recognition is the same as in the ﬁrst case, but in a space of lower
dimension.

8. Inﬁnite bounded families

If Σ is an inﬁnite bounded subset of Mn(R), we conjecture that, again, CP is equivalent to LCP and RCP
together. Note that even if we restrict matrices to orthogonal projections in R2 (they are self-conjugated,
hence, LCP=RCP), inﬁnite families can be either CP or not.

The following assertion can be extended to higher dimensions.

Theorem 8.1. A family Σ of orthogonal projections on straight lines Rh, h ∈ H ⊆ S1 in R2 is CP if and
only if, for any sector K with non-empty interior in R2 there exists a sector K′ ⊆ K with non-empty interior
such that K′ ∩ H = ∅.

The crucial point here is that there are no non-trivial limit cycles for paths of Σ.

References

[1] M. Berger and Y. Wang. Bounded semigroups of matrices. Linear Algebra Appl., 166:21–27, 1992.
[2] V. Blondel and J. N. Tsitsiklis. The boundedness of all products of a pair of matrices is undecidable. Systems Control

Lett., 41:135–140, 2000.

[3] Vincent D. Blondel. The birth of the joint spectral radius: An interview with Gilbert Strang. Linear Algebra and its

Applications, 428:2261 – 2264, 2008.

[4] I. Daubechies and J. C. Lagarias. Sets of matrices all inﬁnite products of which converge. Linear Algebra Appl., 161:227–

263, 1992.

[5] L. Elsner and S. Friedland. Norm conditions for convergence of inﬁnite products. Linear Algebra Appl., 250:133–142,

1997.

[6] R. Jungers. The joint spectral radius. Theory and applications, volume 385 of Lecture Notes in Control and Information

Sciences. Springer-Verlag, London, 2009.

[7] V. Kozyakin. An explicit Lipschitz constant for the joint spectral radius. Linear Algebra and its Applications, 433(1):12–18,

2010.

[8] P. Krejci and A. Vladimirov. Polyhedral sweeping processes with oblique reﬂection in the space of regulated functions.

Set-Valued Anal., 11:91–110, 2003.

8

[9] M. Kunze and M.D.P. Monteiro Marques. An introduction to Moreau’s sweeping process.

In Impacts in Mechanical
Systems - Analysis and Modelling, volume 551 of Lecture Notes in Physics, pages 1–60. Springer, Berlin-New York, 2000.
[10] A. Mandelbaum and K. Ramanan. Directional derivatives of oblique reﬂection maps. Mathematics of Operations Research,

35(3):527–558, 2010.

[11] J. J. Moreau. Evolution problem associated with a moving convex set in a Hilbert space. Journ. of Dif. Eq., 20:347–374,

1977.

[12] G.-C. Rota and W. G. Strang. A note on the joint spectral radius. Indag. Math., 22:379–381, 1960.
[13] A. A. Vladimirov, L. Elsner, and W.-J. Beyn. Stability and paracontractivity of discrete linear inclusions. Linear Algebra

Appl., 312:125–134, 2000.

[14] A. S. Voynov and V. Yu. Protasov. Compact noncontraction semigroups of aﬃne operators. Sbornik: Mathematics,

206(7):921, 2015.

9

