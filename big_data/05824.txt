6
1
0
2

 
r
a

 

M
8
1

 
 
]
E
N
.
s
c
[
 
 

1
v
4
2
8
5
0

.

3
0
6
1
:
v
i
X
r
a

ComparingTimeandFrequencyDomainforAudioEventRecognitionUsingDeepLearningLarsHertel∗,HuyPhan∗†andAlfredMertins∗∗InstituteforSignalProcessing,UniversityofLuebeck,Germany†GraduateSchoolforComputinginMedicineandLifeSciences,UniversityofLuebeck,GermanyEmail:{hertel,phan,mertins}@isip.uni-luebeck.deAbstract—Recognizingacousticeventsisanintricateproblemforamachineandanemergingﬁeldofresearch.Deepneuralnetworksachieveconvincingresultsandarecurrentlythestate-of-the-artapproachformanytasks.Oneadvantageistheirimplicitfeaturelearning,oppositetoanexplicitfeatureextractionoftheinputsignal.Inthiswork,weanalyzedwhethermorediscriminativefeaturescanbelearnedfromeitherthetime-domainorthefrequency-domainrepresentationoftheaudiosignal.Forthispurpose,wetrainedmultipledeepnetworkswithdifferentarchitecturesontheFreiburg-106andESC-10datasets.Ourresultsshowthatfeaturelearningfromthefrequencydomainissuperiortothetimedomain.Moreover,additionallyusingconvolutionandpoolinglayers,toexplorelocalstructuresoftheaudiosignal,signiﬁcantlyimprovestherecognitionperformanceandachievesstate-of-the-artresults.I.INTRODUCTIONRecognizingacousticeventsinnaturalenvironments,likegunshotsorpolicesirens,isanintricatetaskforamachine.Theeffortlessnessofthehumanearandbraindeceivesthecomplexunderlyingprocess.However,havingamachinethatunderstandsitsenvironment,e.g.throughacousticevents,isimportantformanyapplicationssuchassecuritysurveillanceandambientassistedliving,especiallyinanagingpopulation.Thisisonereasonwhymachinehearingisbecomingamoreandmoreemergingﬁeldofresearch[1].Sofar,mostoftheaudioeventrecognitionsystemshaveusedhand-craftedfeatures,extractedfromthefrequencydo-mainoftheaudiosignal.Theyaremainlyborrowedfromtheﬁeldofspeechrecognition,suchasmel-scaleﬁlterbanks[2],log-frequencyﬁlterbanks[3]andtime-frequencyﬁlters[4].However,withtherapidadvanceincomputingpower,featurelearningisbecomingmorecommon[5]–[7].Inthiswork,weusedeepneuralnetworksingeneralandconvolutionalnetworksinparticularforcombinedfeaturelearningandclassiﬁcation.Theyhavebeensuccesfullyap-pliedtomanydifferentpatternrecognitiontasks[8]–[11],includingaudioeventrecognition[5],[6],[12],[13].Aschematicrepresentationofaone-dimensionalconvolutionalneuralnetworkisshowninFigure1.Thegivennetworkcom-prisesﬁvedifferentlayers,i.e.input,convolution,pooling,fullyconnected,andoutputlayers.Givenaninputsignalintheinputlayer,multipleﬁltersarelearnedandconvolvedwiththeinputsignalintheconvolutionlayer,resultinginvariousconvolvedsignals.Multiplevaluesofthosesignalsarethenpooledtogetherinthepoolinglayer.ThisintroducesInputSignalConvolutionPoolingFullyConnectedOutputFeatureExtractionClassiﬁcationFig.1.Schematicdiagramofaone-dimensionalconvolutionalneuralnetworkforaudioeventrecognition.Thenetworkcomprisesﬁvedifferentlayers.Bothfeatureextractionandclassiﬁcationarelearnedduringtraining.aninvariancetosmalltranslationsoftheinputsignal.Bothconvolutionandpoolinglayersareusuallyappliedmultipletimes.Afterwards,theextractedfeaturesareweightedandcombinedinthefully-connectedlayerandoutputintheoutputlayer.Theretypicallyexistsoneoutputneuronforeachaudioeventcategoryintheoutputlayer.Themotivationalquestionwewanttoanswerinthispaperiswhethermorediscriminativefeaturescanbelearnedfromthetime-domainorthefrequency-domainrepresentationoftheaudioinputsignal.Forthispurpose,wetrainvariousdeepneuralnetworkswithdifferentarchitecturesonmultipledatasetsbothintimeandfrequencydomainandcomparetheirachievedrecognitionresults.II.DATASETSTotrainandevaluateourdeepnetworks,weusedtwodifferentdatasets,namelyFreiburg-106andESC-10.Bothdatasetscontainshortsoundclipsofisolatedenvironmentalaudioevents.Notethattheaudioeventsarenotoverlapping.Thereisonlyasingleeventpresentineachsoundﬁle.Inthefollowing,wewillbrieﬂyintroducebothdatasets.AnoverviewofsomestatisticsofthetwodatasetsbeforeandafterpreprocessingisgiveninTableI.A.Freiburg-106TheFreiburg-106[14]datasetcontains1,479audio-basedhumanactivitiesof22categorieswithatotaldurationof48min.Itwascollectedusingaconsumer-leveldynamiccar-dioidmicrophone.Theaudiosignalswerepreampliﬁedandsampledat44100Hz.SeveralsourcesofstationaryambientTABLEISTATISTICSOFTHEUSEDDATASETS.DurationSamplesDatasetClassesTotal(min)Average(s)TrainingTestFreiburg-10622481.90763755AudioFrames129,320133,043ESC-1010335.0032080AudioFrames142,10135,606noisewerepresent.Theaveragedurationofarecordingis1.9s.Wesplitthedatasetintoatrainingandtestsetofequalsize,i.e.everyotherrecordingwasusedfortesting1.B.ESC-10TheESC-10[15]datasetcontains400environmentalrecordingsof10classeswithatotaldurationof33min.Therecordingsareuniformlydistributed,i.e.40recordingsforeachclass.Theyweresearched,downloaded,veriﬁedandannotatedbyPiczak[15]fromthepubliclyavailablefreesound2database.Afterwards,shortsoundclipsof5swereextracted,resampledto44100Hzandstoredwithabitrateof192kbit/susingOggVorbiscompression.Thedatasetissplitintoﬁvepartsforaﬁve-foldcrossvalidation.Theaveragehumanclassiﬁcationaccuracyis95.7%[15].C.PreprocessingBeforebeingabletotrainournetworks,wehadtoprepro-cessallaudioﬁlestoauniﬁedformat.First,weconvertedallstereoaudioﬁlestomonobyaveragingthetwochannels.Thiswasnecessary,sincesomeaudioﬁleswereonlymonorecordings.Secondly,toreducetheamountofdatawhilemaintainingmostoftheimportantfrequencies,weresampledtheaudioﬁlestoasamplingfrequencyof16000Hz.Thirdly,wechangedtheaudiobitdepthfromtheiroriginalformatsto32bitﬂoatingpointsandscaledtheamplitudestotherangeof[−1,1].Fourthly,weappliedarectangularslidingwindowtoeachaudioﬁlewithawindowsizeof150msandastepsizeof5ms.Thus,audioframeswithaﬁxedsizeof2,400sampleswereextracted.Thewindowsizewasdeterminedviaavalidationset.Applyingaslidingwindowwasnecessarysincedeepneuralnetworksinsistonaﬁxedinputsize.Whenwetrainedournetworksinthefrequencydomain,weusedaHammingwindowinsteadofarectangularone,calculatedtheFouriertransformandconcatenatedtheﬁrsthalfofboththesymmetricmagnitudeandphaseoftheFouriertransform.Thereby,thenetworkinputsinbothtimeandfrequencydomainwereequallysizedwithaﬁxedlengthof2,400samples.NotethatbycalculatingtheFouriertransform,wedonotloseanyinformation,sincetheoriginalaudiosignalcanberecoveredwiththeinverseFouriertransform.1ThisisbasedonunofﬁcialcommunicationwithStorketal.[14]2http://www.freesound.orgTABLEIIARCHITECTUREOFOURIMPLEMENTEDDEEPNETWORKS.No.LayerDimensionProbabilityParameters0Input2,400--1Dropout2,4000.2-2FullyConnected384-921,9843Dropout3840.5-4FullyConnected384-147,8405Dropout3840.5-6FullyConnected384-147,8407Dropout3840.5-8FullyConnected384-147,8409Dropout3840.5-10FullyConnected384-147,84011Dropout3840.5-12FullyConnectedx-x13Softmaxx--III.METHODSWethentrainedbothastandarddeepneuralnetworkandaconvolutionalnetworkonFreiburg-106andESC-10inbothtimeandfrequencydomainoftheaudioevents.Consequently,wetrainedeightdeepnetworksintotal.A.DeepNetworkThearchitectureforthestandarddeepnetworkisshowninTableII.Thenetworkcomprises14layerswithmorethan1.5milliontrainableweights.Theinputlayer0expectsasignalwith2,400values,correspondingtoasingleaudioframe.Thenumberofneuronsfortheoutputlayer15dependsonthenumberofclasses,i.e.22forFreiburg-106and10forESC-10.Toobtainaprobabilitydistributionofnoutputvaluesx,weemployedthesoftmaxfunctioninlayer15:softmax(x)i=exp(xi)Pnj=1exp(xj)fori=1,...,n.(1)Betweeninputandoutputlayerweusedﬁvefullyconnectedhiddenlayers.Wechosetherectiﬁedlinearunit(relu)asanonlinearactivationfunctionofanoutputvaluex:relu(x)=max(0,x).(2)Glorotetal.[16]showeditsadvantagesoverthesigmoidandhyperbolictangentasnonlinearactivationfunctions.Topreventthenetworkfromoverﬁtting,weregularizeditbyusingdropout[17]aftereachlayer.Theprobabilitytorandomlydropaunitinthenetworkis20%fortheinputlayerand50%forallthehiddenlayers.Moreover,weusedamaximumnormconstraintkwk2<1foranyweightwinthenetwork,assuggestedbyHinton[18].Thisformofregularizationboundsthevalueoftheweightswhilenotdrivingthemtobenearzero,ase.g.inweightdecay.B.ConvolutionalNetworkThearchitectureforourconvolutionalnetworkisshowninTableIII.Thenetworkcomprises16layerswithnearlyTABLEIIIARCHITECTUREOFOURIMPLEMENTEDCONVOLUTIONALNETWORKS.No.LayerDimensionSizeStrideParametersRowsColumns0Input12,400---1Dropout12,400---2Convolution482,392914803Pooling4859844-4Convolution965909141,5685Pooling9614744-6Convolution19213991166,0807Pooling1923444-8Convolution3842691663,9369Pooling3846---10FullyConnected1384--885,12011Dropout1384---12FullyConnected1384--147,84013Dropout1384---14FullyConnected1x--x15Softmax1x---2milliontrainableparameters.Theinputandoutputlayerareidenticaltothestandarddeepnetwork.However,inbetweenweadditionallyhaveconvolutionandpoolinglayers.Intheconvolutionlayer,theinputsignalisconvolvedwithmultiplelearnedﬁltersofaﬁxedsizewithaﬁxedstrideusingsharedweights.Weusedaﬁltersizeof9,analogousto3×3ﬁltersthatareoftenusedincomputervision.Thenumberoflearnedkernelsare48,96,192,and384,respectively.Notethataftertheﬁrstconvolutionourone-dimensionalinputsignaldoesnotbecomeatwo-dimensionalimage,butmultipleone-dimensionalsignals(c.f.Figure1).Hence,weonlyappliedone-dimensionalconvolutions.Thepoolinglayerthenreducesthesizeofthesignalwhiletryingtomaintainthecontainedinformationandintroducinganinvariancetosmalltranslationsoftheinputsignal.Thepoolingsizeandstridewassetto4,analogousto2×2poolingthatisagainoftenusedincomputervision.Weusedmaximumpoolingforallpoolinglayers.Asanonlinearactivationfunction,weagainsettledfortherectiﬁedlinearunit,justasinstandarddeepnetworks.Afterwards,theextractedfeaturesfromtheinputsignalwerecombinedusingthreefullyconnectedlayers.Toregularizeournetwork,weagainuseddropoutlayers.Thistime,however,dropoutwasonlyusedaftertheinputlayerwithaprobabilityof20%andaftereachfullyconnectedlayerwithaprobabilityof50%.WeusedthePythonlibraryTheano[19],[20]andtheNVIDIACUDADeepNeuralNetwork3(cuDNNv3)librarytotrainourdeepnetworks.ThelibraryallowedustoemploytheGPU4ofourcomputerforfastertraining.Thisresultedinaspeedupofapproximatelyten,comparedtotrainingon3https://developer.nvidia.com/cudnn4GeForceGT640with2GBofmemorytheCPU5.Thestandarddeepneuralnetworksweretrainedfor100epochs.Anepochmeansacompletetrainingcycleoverallaudioframesofthetrainingset.Onesingleepochtooknearly30s.Westartedwithaﬁxedlearningrateof0.05anddecreaseditbyafactoroftwoafter20epochs.Furthermore,weselectedabatchsizeof256framesandamomentumof0.9.Inconstrast,theconvolutionalnetworks,weretrainedfor20epochs.Asingleepochtooknearly11min.Weagainstartedwithaﬁxedlearningrateof0.05anddecreaseditbyafactoroftwoafterﬁveepochs.Batchsizeandmomentumremainedthesameasforstandarddeepnetworks.TopredicttheclasslabelofanentireaudioﬁleXofourtestset,weﬁrstpredictedeachofthenaudioframesindividually.Duetothesoftmaxoutputlayerofournetworkweobtainedaprobabilitydistributionamongthemclasslabels.Afterwards,weperformedaprobabilityvotingbyaddingthepredictedprobabilitiesforeachframetogetherandtakingtheclasslabelwiththemaximumprobability:vote(X)=argmaxj=1,...,m nXi=1xij!.(3)Toevaluateourpredictedclasslabels,weusedthef-scoremetric:f-score=2·precision·recallprecision+recall,(4)whichconsidersbothprecisionandrecallvaluesandcanbeinterpretedastheweightedaverageoftheprecisionandrecall.IV.RESULTSOurresultsaregiveninFigure2,TableIVandTableV.Forcomparison,thestate-of-the-artresultsare98.3%[21]forFreiburg-106andapproximately80%6[15]forESC-10.ThehumanaccuracyforESC-10is95.7%[15].Figure2displaystheaveragef-scoreinpercentforthestandarddeepneuralnetworksonthevalidationtestset.ThesolidlinesrepresenttraininginthefrequencydomainandthedashedlinesrepresenttraininginthetimedomainforbothFreiburg-106andESC-10,respectively.Notethattheshownf-scorewascalculatedandaveragedforasingleaudioframe,notanentireaudioﬁle.Thus,novotinghadbeenperformedyet.Clearly,audioeventsinFreiburg-106areeasiertorecognizethaninESC-10.Moreover,forbothdatasets,networkstrainedinthefrequencydomainsachievedahigherf-scorethannetworkstrainedinthetimedomain.MoredetailedresultsforFreiburg-106aregiveninTa-bleIV.Itshowsthef-scoreforeachindividualaudioeventcategoryandtheaveragef-scorevalue,obtainedwithproba-bilityvoting.Standarddeepneuralnetworksreachanaveragef-scoreof75.9%inthetimedomainand97.6%inthefrequencydomain.Convolutionalnetworks,however,reachanoverallaccuracyof91.0%intimedomainand98.3%inthefrequencydomain.Theimprovementinthetimedomainistherefore15.1%and0.7%inthefrequencydomain.The5IntelCorei7-3770Kwitheightcores6Therecognitionresultsareonlygiveninformofaboxplot.020406080100020406080epochf-score(%)ESC-10(time)ESC-10(freq.)Freiburg-106(time)Freiburg-106(freq.)Fig.2.Comparingthevalidationf-scoreofmultiplestandarddeepneuralnetworksontwodatasets.Thenetworksweretrainedfor100epochs.Thesolidlinesrepresenttraininginthefrequencydomainandthedashedlinesrepresenttraininginthetimedomain,respectively.TABLEIVRECOGNITIONRESULTSFORTHEFREIBURGDATASET(F-SCOREIN%).DeepNetworkConvolutionalNetworkNo.ClassTimeFrequencyTimeFrequency0Background32.378.045.875.01Bag77.598.895.0100.02Blender95.1100.0100.0100.03CornﬂakesBowl75.9100.072.2100.04CornﬂakesEating86.4100.095.2100.05Cup14.495.790.9100.06DishWasher93.797.8100.0100.07ElectricRazor96.397.6100.0100.08FlatwareSorting46.797.650.0100.09FoodProcessor86.7100.094.1100.010HairDryer90.4100.0100.0100.011Microwave98.9100.0100.0100.012MicrowaveBell95.7100.091.6100.013MicrowaveDoor33.397.765.191.314PlatesSorting59.198.586.6100.015StirringCup89.798.3100.0100.016ToiletFlush70.095.888.796.817Toothbrush64.696.385.7100.018VacuumCleaner90.9100.0100.0100.019WashingMachine92.498.597.0100.020WaterBoiler94.0100.096.9100.021WaterTap85.296.696.3100.0Average75.997.691.098.3backgroundclasswasmostdifﬁculttorecognizebythenetworks,whilenearlyallaudioeventsoftheMicrowavecategorywerecorrectlyrecognizedbyallthedifferentnet-works.AsfortherecognitionresultsfortheESC-10datasetinTableV,standarddeepneuralnetworksreachanaveragef-scoreof70.3%withtraininginthetimedomainand77.1%inthefrequencydomain.Convolutionalnetworksimprovetheseresultsby13.4%to83.7%inthetimedomainandby12.8%to89.9%inthefrequencydomain,respectively.NearlyallaudioeventsofthedogbarkclasswerecorrectlyTABLEVRECOGNITIONRESULTSFORTHEESC-10DATASET(F-SCOREIN%).DeepNetworkConvolutionalNetworkNo.ClassTimeFrequencyTimeFrequency0BabyCry62.576.293.3100.01Chainsaw80.071.475.071.42ClockTick66.680.084.280.03DogBark87.5100.0100.0100.04FireCrackling54.540.085.780.05Helicopter94.188.994.1100.06PersonSneeze50.066.771.480.07Rain61.585.766.794.18Rooster76.985.7100.0100.09SeaWaves69.676.266.793.3Average70.377.183.789.9recognizedbyallthedifferentnetworks,whilerecognizingachainsawwasmostdifﬁcultinthefrequencydomainandseawavesmostdifﬁcultinthetimedomain,respectively.V.DISCUSSIONDeepconvolutionalnetworksarethestate-of-the-artap-proachformanypatternrecognitiontasks,includingaudioeventrecognition.Onereasonistheimplicitfeaturelearninginsteadofanexplicitfeatureextractionoftheinputsignal.Inthiswork,weanalyzedwhethermoresuitablefeaturescanbelearnedfromeitherthetimedomainorthefrequencydomain.OurresultsshowthatlearningfromthefrequencydomainisconsistentlysuperiortolearningfromthetimedomainonbothdatasetsFreiburg-106andESC-10.Ourtraineddeepneuralnetworksachievedstate-of-the-artresults.Accord-ingly,morediscriminativefeaturescouldbelearnedinthefrequencydomain.Moreover,additionallyaddingconvolutionandpoolinglayerstothedeepneuralnetworkcouldmostofthetimesigniﬁcantlyimprovetheachievedf-score.OneexceptionisforlearninginthefrequencydomainonFreiburg-106,whereastandarddeepnetworkalonealreadyreachedcomparablestate-of-the-artresults.Thus,exploringlocalstructuresoftheinputsignalbothintimeandfrequencydomainseemsreasonable.Whentrainingdeepnetworksforaudioeventrecognition,weexperiencedheavyoverﬁttingofthenetworks,especiallywhentrainedinthetimedomain.Therefore,wehadtointensivelyregularizethenetworkbyemployingdropoutineachlayer.Additionally,weconstrainedthenormofeachweight,assuggestedbyHinton[18].Itsmainadvantageoverotherregularizationmethods,likeweightdecayforexample,isthatitdoesnotdrivetheweightstobenearzero.Thispartlypreventedthenetworksfromoverﬁtting.However,overﬁttingtoasmallextentwasstillnoticeable.Weexperiencedthatsomeclasseswereextraordinarilydifﬁculttorecognize,e.g.thebackgroundclassinFreiburg-106.Whenlisteningtotheaudioﬁlesofthoseclasses,wenoticedthatmostofthetimeeitheralongsilencewaspresentintheseﬁlesornogenericpatternwasrecognizable.Acarefulﬁlteringoftheseﬁlescouldimprovetheoverallrecognitionaccuracyandshouldbeconsidered.Asalreadyindicated,wedeterminedthewindowsizeof150msbyemployingavalidationsetthatwassplitfromthetrainingdata.Wenoticedthatatoosmallwindowsize,i.e.below50ms,couldnotgrasptheimportantinformationcontainedintheaudiosignal.Atoolargewindow,however,requiredmanyparametersintheﬁrstfullyconnectedlayerofourstandarddeepneuralnetworks,thusresultinginalongtrainingtime.Awindowsizeof150mswasareasonablecompromisebetweenaccuracyandtrainingtime.Whentrainingournetworksinthefrequencydomain,weusedboththemagnitudeandphaseinformationoftheFouriertransform.Themainreasonforthiswastomaintainthesamenumberofinputsamplesthatwereusedforthetimedomainsignal.Consequently,wewereabletousethesamenetworkarchitectureinbothtimeandfrequencydomain.Nottoosurprisingly,whenweremovedthephaseinformation,therecognitionresultsofournetworksremainedthesame.Incontrast,whentrainingwiththephaseinformationonly,thenetworkskeptguessingrandomly.Insteadofusingarectiﬁedlinearunit(2)asanonlinearactivationfunction,wealsotestedmaxoutnetworks[22]withapoolingsizeof5.Wedidnotnoticeanydifferencesinourobtainedrecognitionresults,however.Sincemaxoutnetworksarecomputationallymoreexpensivethanrectiﬁedlinearunits,wesettledforthelatter.Furthermore,besidesusingprobabilityvoting(3),wealsotriedmajorityvoting.Forthispurpose,wepredictedtheindividualclasslabelforeachaudioframeandassignedthemostfrequentlypredictedclasslabeltotheaudioﬁle.Ourresults,however,indicatedthatprobabilityvotingismoreappropriateforaudioeventrecognitionthanmajorityvoting.VI.CONCLUSIONSDeeplearningissuitableforaudioeventrecognitioninboththetimedomainandthefrequencydomainoftheaudiosignal.However,morediscriminativefeaturesarelearnedbythenetworkinthefrequencydomain,achievingsuperiorresults.Exploringthelocalstructureofaudiosignalsbyem-ployingconvolutionandpoolinglayersadditionallyimprovestherecognitionperformanceofthenetworks,whichthenachievestate-of-the-artresults.Furtherresearchwillfocusonvisualizingandunderstandingwhatourdeepnetworkshavelearnedbothfromthetime-domainandfrequency-domainrepresentation.REFERENCES[1]R.Lyon,“Machinehearing:Anemergingﬁeld,”IEEESignalProcess-ingMagazine,vol.27,no.5,pp.131–139,2010.[2]D.ReynoldsandR.Rose,“Robusttext-independentspeakeridentiﬁ-cationusinggaussianmixturespeakermodels,”IEEETrans.SpeechAudioProcess.,vol.3,no.1,pp.72–83,1995.[3]C.Nadeu,D.Macho,andJ.Hernando,“Timeandfrequencyﬁlteringofﬁlter-bankenergiesforrobustHMMspeechrecognition,”SpeechCommunications,vol.34,pp.93–114,2001.[4]S.Chu,S.Narayanan,andC.Kuo,“Environmentalsoundrecognitionwithtime-frequencyaudiofeatures,”IEEETrans.Audio,SpeechandLanguageProcess.,vol.17,no.6,pp.1142–1158,2009.[5]I.McLoughlin,H.Zhang,Z.Xie,Y.Song,andW.Xiao,“Robustsoundeventclassiﬁcationusingdeepneuralnetworks,”IEEE/ACMTrans.Audio,SpeechandLanguageProcess.TASLP,vol.23,no.3,pp.540–552,2015.[6]K.Piczak,“Environmentalsoundclassiﬁcationwithconvolutionalneuralnetworks,”inInt.WorkshopMach.LearningforSignalProcess.MLSP,2015.[7]A.Plinge,R.Grzeszick,andG.Fink,“ABag-of-Featuresapproachtoacousticeventdetection,”inIEEEInt.Conf.Acoust.,SpeechandSignalProcess.(ICASSP),2014,pp.3704–3708.[8]A.Krizhevsky,I.Sutskever,andG.E.Hinton,“ImageNetclassiﬁcationwithdeepconvolutionalneuralnetworks,”inAdvancesinNeuralInformationProcessingSystems(NIPS)25,2012,pp.1097–1105.[9]L.Hertel,E.Barth,T.Käster,andT.Martinetz,“Deepconvolutionalneuralnetworksasgenericfeatureextractors,”inInt.JointConf.NeuralNetworksIJCNN,2015.[10]D.Ciresan,U.Meier,andJ.Schmidhuber,“Multi-columndeepneuralnetworksforimageclassiﬁcation,”inIEEEConf.Comput.VisionandPatternRecognition(CVPR),2012,pp.3642–3649.[11]C.Szegedy,W.Liu,Y.Jia,P.Sermanet,S.Reed,D.Anguelov,D.Erhan,V.Vanhoucke,andA.Rabinovich,“Goingdeeperwithconvolutions,”presentedattheWorkshopImageNetLargeScaleVisualRecognitionChallenge(ILSVRC),2014.[12]E.Cakir,T.Heittola,H.Huttunen,andT.Virtanen,“Polyphonicsoundeventdetectionusingmultilabeldeepneuralnetworks,”inInt.JointConf.NeuralNetworksIJCNN,2015.[13]——,“Multi-labelvs.combinedsingle-labelsoundeventdetectionwithdeepneuralnetworks,”inEuropeanSignalProcess.Conf.EU-SIPCO,2015.[14]J.Stork,L.Spinello,J.Silva,andK.Arras,“Audio-basedhumanactivityrecognitionusingnon-Markovianensemblevoting,”inIEEEInt.Symp.RobotandHumanInteractiveCommunication(RO-MAN),2012,pp.509–514.[15]K.Piczak,“ESC:Datasetforenvironmentalsoundclassiﬁcation,”inProc.ACMInt.Conf.Multimedia(ACMMM),2015.[16]X.Glorot,A.Bordes,andY.Bengio,“Deepsparserectiﬁerneuralnetworks,”inProc.14thInt.Conf.Artif.Intell.andStat.(AISTATS),vol.15,2011,pp.315–323.[17]N.Srivastava,G.Hinton,A.Krizhevsky,I.Sutskever,andR.Salakhut-dinov,“Dropout:Asimplewaytopreventneuralnetworksfromoverﬁtting,”JournalofMachineLearningResearch,vol.15,no.1,pp.1929–1958,2014.[18]G.Hinton,N.Srivastava,A.Krizhevsky,I.Sutskever,andR.Salakhut-dinov,“Improvingneuralnetworksbypreventingco-adaptationoffeaturedetectors,”arXivpreprintarXiv:1207.0580,2012.[19]J.Bergstra,O.Breuleux,F.Bastien,P.Lamblin,R.Pascanu,G.Des-jardins,J.Turian,D.Warde-Farley,andY.Bengio,“Theano:ACPUandGPUmathcompilerinPython,”inProc.PythonSci.Comput.Conf.SciPy,2010.[20]F.Bastien,P.Lamblin,R.Pascanu,J.Bergstra,I.Goodfellow,A.Berg-eron,N.Bouchard,D.Warde-Farley,andY.Bengio,“Theano:Newfeaturesandspeedimprovements,”inNeuralInformationProcessingSystems(NIPS)DeepLearningWorkshop,2012.[21]H.Phan,L.Hertel,M.Maass,R.Mazur,andA.Mertins,“Audiophrasesforaudioeventrecognition,”inEuropeanSignalProcess.Conf.EUSIPCO,2015.[22]I.Goodfellow,D.Warde-Farley,M.Mirza,A.Courville,andY.Ben-gio,“Maxoutnetworks,”inJournalofMachineLearningResearchJMLRWorshopandConf.Proc.,2013,pp.1319–1327.