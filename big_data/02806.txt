IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, 2016

1

Optimized Kernel Entropy Components

Emma Izquierdo-Verdiguier, Valero Laparra,
Robert Jenssen, Senior Member, IEEE,

Luis G´omez-Chova, Senior Member, IEEE, and
Gustau Camps-Valls, Senior Member, IEEE

Abstract—This work addresses two main issues of the standard Kernel
Entropy Component Analysis (KECA) algorithm: the optimization of
the kernel decomposition and the optimization of the Gaussian kernel
parameter. KECA roughly reduces to a sorting of the importance of
kernel eigenvectors by entropy instead of by variance as in Kernel
Principal Components Analysis. In this work, we propose an extension
of the KECA method, named Optimized KECA (OKECA), that directly
extracts the optimal features retaining most of the data entropy by
means of compacting the information in very few features (often in
just one or two). The proposed method produces features which have
higher expressive power. In particular, it is based on the Independent
Component Analysis (ICA) framework, and introduces an extra rotation
to the eigen-decomposition, which is optimized via gradient ascent search.
This maximum entropy preservation suggests that OKECA features are
more efﬁcient than KECA features for density estimation. In addition, a
critical issue in both methods is the selection of the kernel parameter since
it critically affects the resulting performance. Here we analyze the most
common kernel length-scale selection criteria. Results of both methods
are illustrated in different synthetic and real problems. Results show that
1) OKECA returns projections with more expressive power than KECA,
2) the most successful rule for estimating the kernel parameter is based
on maximum likelihood, and 3) OKECA is more robust to the selection
of the length-scale parameter in kernel density estimation.

Index Terms—Density estimation, feature extraction, entropy compo-

nent analysis, kernel methods

I. INTRODUCTION

Kernel entropy component analysis (KECA) [1], [2] was recently
proposed as a general information-theoretic method for feature ex-
traction and dimensionality reduction in pattern analysis and machine
intelligence. It has proven useful in different applications, e.g. remote
sensing data analysis [3], [4], [5], face recognition [6], chemical pro-
cesses modelling [7], high-dimensional celestial spectra reduction [8]
and audio processing [9]. Several extensions have been proposed
for feature selection [10], class-dependent feature extraction [11]
and semisupervised learning as well [12]. The KECA algorithm is
fundamentally different from, but still intimately related to, the vastly
successful spectral kernel multivariate signal processing methods
such as kernel principal components analysis (KPCA) [13], kernel
canonical correlation analysis (KCCA) [14] and kernel partial least
squares (KPLS) [15], just to name a few [16].

One distinguishing feature of KECA is that the method originates
from kernel density estimation (KDE) [17], [18], [19], as do e.g.

6
1
0
2

 
r
a

M
9

 

 
 
]
L
M

.
t
a
t
s
[
 
 

1
v
6
0
8
2
0

.

3
0
6
1
:
v
i
X
r
a

EIV, VL, LGC and GCV are with the Image Processing Laboratory (IPL),

Universitat de Val`encia, Spain.
{emma.izquierdo,valero.laparra,luis.gomez-chova,gustau.camps}@uv.es,
http://isp.uv.es.
RJ is with the University of Tromsø, Norway.
http://ansatte.uit.no/robert.jenssen/

robert.jenssen@uit.no,

This work has been supported by the Spanish Ministry of Economy and
Competitiveness (MINECO) under project TIN2012-38102-C03-01 (LIFE-
VISION), the Generalitat Valenciana under project GV/2013/079, and the
ERC consolidator grant SEDAL-647423. The work has also been supported by
the Research Council of Norway over FRIPRO/IKTPLUSS grant no. 234498.
is permitted.
Permission from IEEE must be obtained for all other users,
including
reprinting/republishing this material for advertising or promotional purposes,
creating new collective works for resale or redistribution to servers or lists,
or reuse of any copyrighted components of this work in other works. DOI:
10.1109/TNNLS.2016.2530403.

Copyright (c) 2011 IEEE. Personal use of this material

principal curves estimation [20] and the family of information theo-
retic learning methods [21]. In KDE, the key is the kernel function,
locally approximating the underlying probability density function
(PDF). This in turn enables estimation of entropy, a quantity that
describes the shape of the PDF [22]. The KDE kernel must be a non-
negative function that integrates to one (i.e. a density) but need not
be positive semi-deﬁnite (PSD). The KDE kernel is versatile in that
sense. However, many KDE kernels are PSD, well-known examples
include the Gaussian kernel, the Student kernel and the Laplacian
kernel [23] functions.

If the KDE kernel used in KECA is PSD, then there are close
relations to the aforementioned kernel signal processing methods, in
the sense that the kernel computes an inner-product in a reproducing
kernel Hilbert space (RKHS). In this situation, KECA, KPCA, KCCA
and KPLS are based on RKHS learning algorithms to maximize e.g.
the feature space variance, correlation or alignment with the output
variables. PSD KECA hence bridges KDE, information theoretic
learning and RKHS learning.

Although both KDE and RKHS kernel methods have experienced
great success, all kernel-based methods, including the one in this
work, are sensitive to the kernel function used. For instance, many
kernel methods depend heavily on a bandwidth, or length-scale,
parameter. In addition, all
the aforementioned spectral methods
may need a considerable number of components (eigenvalues and
eigenvectors) in order to properly describe the data. This may be
undesirable e.g. in compression and data visualization contexts.

In this work, we take advantage of the KDE foundation of
KECA (see also [18] for further details), and introduce an opti-
mization procedure aiming at compressing the entropy information
into optimal directions in feature space. The proposed approach is
the ﬁrst kernel-based unsupervised feature extraction method in an
information theoretical sense. The approach introduces a rotation
procedure that resembles the one in Fast Independent Component
Analysis (ICA) [24]. Extracted OKECA components presents two
major advantages:

1) OKECA shows great robustness to the kernel bandwidth pa-
rameter. This is important, as there are no universally accepted
kernel size selection procedure for unsupervised KDE-based
kernel methods.

2) We use OKECA in order to improve the KDE. This is achieved

with far fewer components compared to KECA.

The rest of the paper is organized as follows. Section II presents the
OKECA formulation and proposes a density estimation that exploits
kernel feature characteristics. Section III is devoted to the analysis
of the results. We use OKECA as a feature extraction method and
analyze the retained entropy, show the estimated PDF, and perform
data classiﬁcation. We conclude the paper in Section IV.

II. OPTIMIZED KERNEL ENTROPY COMPONENTS (OKECA)
KECA relies on the eigen-decomposition of the uncentered kernel
matrix (see below) and sorts the eigenvectors according to the so-
called entropy values of the projections. This is tightly related
to information-theoretic concepts and KDE. The entropy-relevant
dimensionality reduction transforms the dataset in a way that reveals
cluster structure and hence information about the underlying classes
in the data [25], [2].

A. Kernel Entropy Components (KECA)

To be more precise, the measure of information used in [1] is the

H = − log

p2(x)dx,

(1)

Renyi’s second order entropy, given by

(cid:90)

IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, 2016

2

where p(x) is the PDF generating the data. Given a dataset D =
{x1, . . . , xn} of dimensionality d, the entropy may be estimated
through KDE [17] (see Sec. II-D) as − log ν, where ν is the so-
called information potential [21]:
(cid:62)
1
n2 1
n K1n

ν =

(2)

where Kij = k(xi, xj) is any valid KDE kernel comprising the
(n × n) kernel matrix and 1n is a n-dimensional vector of ones.
Using the kernel decomposition introduced in [1]:
(cid:62)

(cid:62)

K = AA

= (ED

1
2 )(D

1
2 E

),

(cid:19)2

.

(3)

(4)

we may write

ν(Nc) =

Nc(cid:88)

(cid:32) n(cid:88)

(cid:33)2

j=1

i=1

(cid:18)

Nc(cid:88)

j=1

Aij

=

1
2

λ

j 1

(cid:62)
n ej

1
2

j 1(cid:62)

In this expression, E contains the eigenvectors in columns, E =
[e1, e2, . . . , en], and D is a diagonal matrix containing the eigen-
values of K, i.e. Dii = λi, and Nc ≤ n is the number of retained
components. The terms (λ

n ej)2 are denoted entropy values.

As mentioned earlier, if the KDE kernel is PSD, then there is a
close connection between KECA and un-centered KPCA since the
kernel function in that case reproduces the dot product between two
samples mapped to a RKHS H via φ(·), i.e. Kij = k(xi, xj) =
φ(xi)(cid:62)φ(xj). Note that centering of the kernel matrix K makes no
sense in the KDE and entropy context of Eq. (2), as this would corre-
2 E(cid:62) is the uncentered
spond to ν = 0, i.e. inﬁnite entropy. Hence, D
projection of the feature space data DH = {φ(x1), . . . , φ(xn)}
onto all the principal axes in the feature space as [1], [2]. These
projections may be sorted according to their contribution to the input
space entropy as measured by the information potential (the entropy
values in Eq. (4)), constituting the KECA procedure.

However, the projections and their entropy content are fully depen-
dent on the quality of the KDE performed via the kernel function.
Moreover, using the eigen-decomposition procedure may not be
optimal to ﬁnd the best projections from an entropy perspective.

1

B. Proposed Optimized KECA (OKECA)

The novel approach proposed in this work searches for a basis that
maximizes the information potential in as few components as pos-
sible. Towards that end, we propose a new optimal (in information-
theoretic sense) feature extraction and unsupervised dimensionality
reduction method. The procedure corresponds to optimally capturing
in these components the high information potential part of the data
(low entropy), which typically corresponds to the structure of the data
in terms of class or cluster information.

Unlike the KECA method which only applies a different sorting
of KPCA features, the proposed method is motivated by the clas-
sical Independent Component Analysis (ICA) formulation [26] in
2 E(cid:62)), there is an extra
which, after the whitening step (applying D
rotation (applying W(cid:62)) that maximizes the independence between
components. Note that W is an orthonormal linear transformation,
i.e. WW(cid:62) = I. Similar ideas have been applied in kernel-based
components analysis (see for instance [27]). Following the ICA
rationale, we now aim at a new kernel matrix decomposition:

1

K = BB

(cid:62)

= (ED

1
2 W)(W

(cid:62)

1
2 E

D

(cid:62)

).

(5)

Note that the kernel matrix does not change, but the modiﬁcation
allows us to directly ﬁnd the basis that maximize the information po-
tential with respect to the number of retained components. Therefore,
for each column vector wk in W = [w1, . . . , wn], we maximize:

(cid:16)

(cid:17)2

L =

(cid:62)
n ED

1

1
2 wk

,

(6)

Algorithm 1 OKECA feature extraction
input K
output B, W
1: [E, D] ← eig of K
2: Initialize τ, W
3: for t iterations do
dJ ← 2(1(cid:62)
4:
n ED
n ED
5: wk(t + 1) ← wk(t) + τ dJ
6: E ← (ED
7: end for
8: B ← ED
j(λ
10: B ← sortν B, W ← sortν W

9: ν ←(cid:80)

1
2 W
j 1(cid:62)ej)2

1
2 ) wk(t + 1)

2 wk)(1(cid:62)

1
2

1

1

2 )(cid:62)

where each wk is restricted to be normal (cid:107)wk(cid:107)2 = 1 and to be
orthonormal to the previous wl, ∀l < k. This deﬂationary procedure
ensures that the obtained solution retains more (or equal) information
potential than the one obtained by the standard KECA in fewer
components.

In order to solve the OKECA optimization problem in Eq. (6), a

gradient-ascent approach can be followed:

wk(t + 1) = wk(t) + τ

∂L

∂wk(t)

,

where τ is the step size and the gradient is:

∂L
∂wk

= 2(1

(cid:62)
n ED

1
2 wk)(1

(cid:62)
n ED

1
2 )

(cid:62)

.

(7)

(8)

In this paper, we adopted a simple scheme for early stopping
that ensures that the gradient achieves a region where additional
iterations did not modify the cost function signiﬁcantly. A pseudocode
summary of the OKECA feature extraction procedure is given in
Algorithm 1. A Matlab implementation of the algorithm is available
at http://isp.uv.es/code/okeca.htm for the interested
reader. While other more sophisticated optimization algorithms could
be deployed here, in our experiments we observed that this simple
gradient-ascent strategy performed consistently even in the presence
of noise.

C. Computational cost

The proposed method is particularly promising for dimensionality
reduction since it compacts the information in very few features with
higher expressive power. When using this method as the ﬁrst step in a
data processing pipeline its properties will help to reduce the burden
and time computing in the next processing steps. Application of the
OKECA method requires just of a kernel generation and a matrix
multiplication, as in KPCA and KECA. That means that once it is
trained its application to new samples is not demanding. However
the training step is more computationally demanding than KECA.
While KECA method is based on an eigen-decomposition of a kernel
matrix, OKECA additionally requires the gradient ascent procedure
in order to reﬁne the obtained features. In particular, the KECA
method requires O(n3), while OKECA spends O(n3 + 4tn2), where
n is the number of samples and t is the number of iterations of the
gradient ascent process. Note that the number of iterations to converge
t depends on the particular problem at hand, but importantly, this
just has to be computed once. After the kernel is optimized entropic
decomposition is learned, the application is straightforward and as
demanding as the KPCA and KECA counterparts.

IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, 2016

3

σM L

σSilv

σd1

σd2

Fig. 1. Cumulative estimated information potential (ν(Nc)) versus the number of dimension components for different toy datasets, using KECA and OKECA
and different σ estimation approaches.

D. Kernel decomposition in density estimation

This section illustrates the beneﬁts of using the proposed decom-
position for KDE [28]. KDE is a classical method for estimating a
PDF in a non-parametric way. Essentially, KDE deﬁnes the PDF as
a sum of kernel functions, k(·, xi), deﬁned over the training samples
xi of the dataset D as follows:
1
n

n(cid:88)

k(x∗, xi).

ˆp(x∗) =

(9)

i=1

As mentioned before, KDE kernel functions need not in general
be PSD but have to be nonnegative and integrate to one to ensure
that ˆp is a valid probability density function. A classical example
of such a kernel function is the Gaussian distribution, k(x∗, xi) =
(2πσd)−1/2 exp(−(cid:107)x∗ − xi(cid:107)2/(2σ2)), but as mentioned, other
choices exist. Then, the corresponding kernel matrix can be used
for KDE

ˆp(x∗) =

k(x∗, xi) =

(10)

n(cid:88)

i=1

1
n

(cid:62)
n k∗,
1

1
n

where k∗ is the vector of kernel evaluations between the point of
interest x∗ and all samples in the training dataset D. As explained in
[18], if the decomposition of the un-centered kernel matrix follows
the form K = EDE(cid:62), where E is orthonormal and D is a diagonal
matrix, then the kernel-based density estimation may be expressed as

ˆp(x∗) = 1

(cid:62)
n ErE

(cid:62)
r k∗,

(11)

where Er is the reduced version of E by keeping columns for r <
n. Note that when using E instead of Er, Eq. (11) reduces to Eq.
(10). This shows that retained KECA components may be used for
KDE [18], by selecting the dimensions that maximize the information
potential in Eq. (4).

A novel aspect of this paper is to use the OKECA components
for KDE in a similar manner. Note that the KECA decomposition
in Eq. (3) is not exactly the same as the proposed OKECA in

it

Eq. (5). Nevertheless,
same decomposition form,

i.e. (cid:101)E(cid:101)D = B, where (cid:101)E is the B
matrix with normalized column vectors and (cid:101)D is diagonal matrix

is easy to ﬁnd a basis that fulﬁlls the

containing the norms of each column in B. Therefore, the eq. (11) is
ˆp(x∗) = 1(cid:62)
r k∗. In Sec. III-C, we will empirically show that,
even though the OKECA basis is not orthonormal in principle, it is
possible to obtain an accurate PDF estimation.

n(cid:101)Er(cid:101)E(cid:62)

E. Model estimation

In this work, we consider the Gaussian RBF kernel since it is the
most common in both RKHS kernel methods and KDE [28]. This
kernel induces a probabilistic Gaussian mixture model, and it only
introduces one scalar free parameter, σ. Note that more complicated
models could be taken into account in both frameworks. However, a
recurrent and unsolved problem in both approaches is the estimation
of the length-scale parameter σ.

A plethora of heuristics and rules for estimating the length-scale
have been proposed in the machine learning and statistics litera-
tures. Roughly speaking, one ﬁnds two main approximations. The
ﬁrst approach considers maximizing a particular objective function
through a cross-validation procedure. The objective function may be
optimized using unsupervised (e.g. maximum likelihood [19], de-
noted by σM L in the experiments) or supervised (e.g. a classiﬁcation
accuracy score, denoted by σclass in the experiments) approaches.
The second approach resorts to empirical rules of performance or
theoretical bounds. Good examples of this second approach, which
are considered in this paper, are: 1) the Silverman’s rule [17], which
is the classical rule of thumb in KDE, σSilv in the experiments; 2) the
mean distance between training points, which is a common approach
in kernel methods for classiﬁcation, σd1 in the experiments; and 3)
the 15% of the median distance between points, which is the classical
employed in KECA, σd2 in the experiments.

0246810101102103OKECAKECA024681010110210302468101011021030246810102.76441102.76441102.7644102468101011021030246810101102103OKECAKECA024681010110210302468101011021030246810102.296102.2961102.2962102.2963102.296402468101011021030246810101102103OKECAKECA024681010110210302468101011021030246810102.7998102.79990246810101102103IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, 2016

4

III. EXPERIMENTS

PDF Nc

1

2

3

4

5

We compare the performance of the standard KECA and the
proposed OKECA for both density estimation and data classiﬁcation.
We analyze the methods in terms of the retained information potential
as a function of the extracted features, the impact of the model
selection criteria, and the classiﬁcation accuracies in synthetic and
real datasets.

A. OKECA for optimally entropic representations

The ﬁrst experiment considers three well-known 2D toy examples
for analyzing the methods: a ring-shaped distribution consisting of
one class only, and the binary two-moons and pinwheel datasets.
In this section, we illustrate the ability of the proposed method to
obtain projections that maximize the information potential, hence
minimizing the squared R´enyi entropy. In the results, we used 80,
20, and 45 training samples for each problem, respectively.
Figure 1 shows the original data distributions and the estimated
cumulative information potential (ν and L deﬁned in (4) and (6),
respectively) attained by KECA and OKECA as a function of the 10
top components and all the considered kernel length-scale selection
criteria. For all datasets and for all σ values, OKECA reaches almost
the maximum entropy value with just one feature; whereas KECA
cumulative entropy values need ﬁve or more components to saturate.
This behaviour is almost independent of the chosen criterion to set
the σ parameter. The higher information content may translate into
more informative features potentially useful for density estimation
and classiﬁcation as we illustrate in the next sections.

B. OKECA for PDF estimation

Figure 2 illustrates the ability of KECA and OKECA for density
estimation in the ring dataset (see ﬁrst example in Fig. 1 to analyze
the ring dataset distribution). We merely applied Eq. (11) for different
number of components r in Ar. Note that, for the proposed OKECA,
the ﬁrst projection concentrates most of the entropy information. This
agrees with the fact that just one dimension is needed to obtain a good
PDF estimation. On the contrary, KECA cannot estimate correctly the
PDF using only the ﬁrst component and actually needs at least ﬁve
components. This issue is even more dramatic when using σM L (see
Fig. 2). It is worth noting that σM L and σd2 give rise to the best
PDF estimates.

C. OKECA components for data classiﬁcation

We here illustrate the capabilities of OKECA for data classiﬁcation.
A great many feature extraction methods exist, both linear like PCA
[29], ICA [24] or AWICA [30] and nonlinear such as the family of
kernel multivariate analysis [16]. In this paper, however, for the sake
of a fare comparison, we restrict to compare our OKECA proposal
to the original KECA counterpart, which are the only existing
unsupervised feature extraction kernel methods based on the same
principle of entropy maximization. The experiments are conducted
on a wide range of synthetic and real problems: 1) The two moons
and the pinwheel datasets considered previously in Sec. III-A; 2) Six
real datasets from the University California Irvine (UCI) Machine
Learning Repository1; and 3) A real satellite multispectral image
classiﬁcation problem. In order to evaluate the data classiﬁcation, we
have used the overall classiﬁcation accuracy (OA) which is obtained
as the average of samples correctly predicted in percentage terms.
While one could classify on top of the extracted features, we here rely
intentionally on the class-dependent estimated densities and perform
maximum a posteriori (MAP) classiﬁcation.

1http://archive.ics.uci.edu/ml/datasets.html

A
C
E
σM L K

A
C
E
K
O

A
C
E
σSilv K

A
C
E
K
O

A
C
E
σd1 K

A
C
E
K
O

A
C
E
σd2 K

A
C
E
K
O

Fig. 2. Density estimation for the ring dataset by KECA and OKECA using
different number of extracted features Nc and approaches to estimate the
kernel length-scale parameter σ. Black color represents low PDF values and
yellow color high PDF values.

1) Synthetic datasets: Figure 3 shows the test OA obtained with
different σ values and different number of retained dimensions with
KECA and the proposed OKECA on the two-moons and pinwheel
datasets. The ﬁve bars for every number of retained features are,
from left to right: σd1, σd2, σSilv, σM L and σclass. The value
of σclass has been optimized for classiﬁcation using all features
in a 5-fold cross-validation scheme. We used 20 samples and 45
samples per class for training two-moons and pinwheel respectively,
and 500 per class for testing the models and computing the test
OA in both datasets. Note that the OKECA method achieves better
classiﬁcation results than KECA for all σ values, conﬁrming that to
seek for optimally entropic data descriptors may beneﬁt classiﬁcation.
Smaller differences between methods are observed as the number of
components increases. When all n features are used, OKECA and
KECA are trivially equivalent.

In the following, we discuss the capabilities of OKECA in the pres-
ence of distorted distributions. The question raised is how sensitive
is the optimization algorithm to the presence of noise. To this end,
a toy example of the KECA and OKECA projections in presence
of noise is analyzed. We used 50 samples of two-moons dataset for
training and 500 samples to test the classiﬁer. Gaussian noise was
added to the original data distributions by varying the dimension-wise
standard deviation of the Gaussian noise σn from 0.001 to 0.091.
Numerical results are shown in Figure 4 [left]. Note that one main aim
of feature extraction and dimensional reduction methods is achieved
using the proposed method: KECA needs at least 12 components to
obtain similar results to the ones obtained by OKECA with just one

IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, 2016

5

σM L

σSilv

Fig. 3. Overall Accuracy obtained with two-moons (left) and pinwheel (right)
datasets. The ﬁve bars for every number of retained features are, from left to
right: σd1, σd2, σSilv, σM L and σclass.

σd1

σd2

Fig. 4.
Overall Accuracy obtained by KECA and OKECA methods using
different values of noise (left) and UCI database (right) with different number
of dimensions. The bars for every number of retained features are different
noise values (left) and different databases (right), respectively.

UCI DATABASE DESCRIPTION (d: NUMBER OF DIMENSIONS, nc: NUMBER

OF CLASS, Ntrain: NUMBER OF TRAINING SAMPLES, AND Ntest:

TABLE I

NUMBER OF TEST SAMPLES.

Database
Ionosphere
Letter
Pendigits
Pima-Indians
Vowel
wdbc

m
351

20000
10992

768
990
569

d
33
16
16
8
12
30

nc Ntrain Ntest
2
172
3874
26
3498
9
330
2
330
10
2
344

60
780
450
180
100
60

component, even in high-noise regime.

2) UCI benchmark datasets: We used six datasets from the UCI
machine learning repository of different sizes and dimensionality:
the Ionosphere dataset is a binary classiﬁcation problem of the
quality of the radar signal returned from the ionosphere; the goal for
the Letter dataset is to detect each of a large number of black-
and-white rectangular pixel displays as one of the 26 capital letters
in the English alphabet; the Pendigits problem deals with the
recognition of pen-based handwritten digits; the Pima-Indians
dataset constitutes a classical problem of diabetes diagnosis in
patients from clinical variables; the Vowel dataset deals with the
vowels detection problem in japanese and contains data from a large
number of time series of cepstrum coefﬁcients taken from speakers;
and ﬁnally wdbc is another clinical problem for diagnosis of breast
cancer in malignant/benign classes. The datasets were intentionally
selected either because of the observed high collinearity between
input features or the diversity in number of classes. Table I gives
details on the dimensionality, number of classes and training and test
samples used in the experiments that follow.

We run KECA and OKECA for all datasets for different numbers
of extracted components. The average of the OA for the ten ﬁrst

Fig. 5.
The cumulative information potential for the multispectral image
dataset using KECA and OKECA and different σ estimation approaches.
Results for KECA and OKECA in σd1 are equal (appear overlapped).

dimensions is shown in Figure 4 [right]. In this case we restrict
ourselves to σM L because of the good performance in the previous
experiments and for the sake of simplicity. In general, the OKECA
method outperforms the KECA method and, as observed before,
OKECA saturates its performance with just the ﬁrst extracted di-
mension.

3) Satellite image classiﬁcation:

In this experiment, we apply
KECA and OKECA to the segmentation of remotely-sensed multi-
spectral images. Nowadays, sensors mounted on satellite or airborne
platforms may acquire the reﬂected energy by the Earth with high
spatial detail and in several wavelengths or spectral channels [31].
This allows an improved detection and classiﬁcation of the pixels
in the scene. We consider a real multispectral image acquired over
a residential neighbourhood of the city of Z¨urich by the QuickBird
satellite in 2002. The analyzed image has 329×347 pixels. Additional
spatial information was added by means of morphological operators,
so the dataset has 22 input features. The images contain nine classes
of interest: water, meadows, trees, asphalt, brick roofs, bitumen,
parking lots, bare soil and shadows. The classes of training samples
have been labeled by photointerpretation. The considered data is not
only high-dimensional but also shows high collinearity since spectral
and spatial features are stacked together at a pixel level. The problem
may be quite challenging for classiﬁcation and feature extraction.

The KECA and OKECA cumulative information potential values
follow similar trends to the toy examples, see Fig. 5. OKECA
reaches the maximum with just one feature, while KECA needs much
more components to achieve similar informative content, especially
noticeable for the σM L and σd2 criteria. Such dependence with the
criterion is not shared by OKECA. These results suggest that the
sharpness in the component selection made by OKECA is relevant
in cases of high feature redundancy as well.

Figure 6 shows the classiﬁcation results obtained using different σ
values and different number of retained dimensions. In this case, we
use 22 and 200 samples per class for training and testing the models,
respectively. Both methods achieve the best results using the σM L
and σclass criteria. Finally, note that σd1, which is a common choice
in unsupervised kernel methods, provides very poor results for both
methods. Figure 7 shows the classiﬁcation maps obtained using three

135791113151719556065707580859095100OA [%]135796065707580859095100OA [%]KECAOKECA246810135791113151719All(n)5060708090100OA [%]NcKECAOKECA12345678910All(n)30405060708090100NcOA [%]KECAOKECAionosletterpendpimavowelwdbc02468101011021030246810101102103OKECAKECA02468101021030246810102103IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, 2016

6

the proposed method reduces the number of features required clearly
improving the results. OKECA estimates correctly the PDF using
one dimension concentrating most of the entropy information better
than the KECA method, and it is more robust to the selection of the
kernel parameter. Furthermore, the experiments show an improvement
of the classiﬁcation results even in the presence of noise. In some
situations, using just one OKECA feature is enough to achieve the
best overall performance, extracting more components does not add
new complementary information and classiﬁcation results do not
change signiﬁcantly.

IV. CONCLUSIONS

We proposed a highly efﬁcient modiﬁcation of the KECA algo-
rithm for optimal extraction of entropic kernel components. While
KECA reduces to sort the kernel eigenvectors by entropy, OKECA
explicitly searches for the features that retain most
informative
content. We have illustrated the ability of OKECA to retain more
information in PDF estimation and classiﬁcation on both synthetic
and real examples. Results consistently showed that OKECA outper-
forms KECA in terms of information content and robustness. In fact,
in many experiments, just one or two OKECA components retain
almost all the relevant information for data description. Accounting
for optimal entropic features allows us to improve the description of
the density shape, which is in turn the core for PDF estimation and
PDF-based classiﬁcation. Furthermore we have analyzed the effect
of using different unsupervised rules to ﬁt the RBF kernel length-
scale parameter on KECA and OKECA performances. In general,
the maximum likelihood approach showed the best performance.

REFERENCES

[1] R. Jenssen, “Kernel Entropy Component Analysis,” IEEE Transactions

on Pattern Analysis and Machine Intelligence, vol. 31, Issue 9, 2010.

[2] ——, “Entropy-relevant dimensions in the kernel feature space: Cluster-
capturing dimensionality reduction,” Signal Processing Magazine, IEEE,
vol. 30, no. 4, pp. 30–39, 2013.

[3] L. G´omez-Chova, R. Jenssen, and G. Camps-Valls, “Kernel entropy
component analysis for remote sensing image clustering,” Geoscience
and Remote Sensing Letters, IEEE, vol. 9, no. 2, pp. 312–316, 2012.

[4] X. Q. Luo and X. J. Wu, “Fusing Remote Sensing Images using a
Statistical Model,” Applied Mechanics and Materials, vol. 263-266,
2012.

[5] X. Q. Luo, X. J. Wu, and Z. Zhang, “Regional and Entropy Component
Analysis based Remote Sensing Images Fusion,” Journal of Intelligent
and Fuzzy Systems, 2013.

[6] B. H. Shekar, M. S. Kumari, L. M. Mestetskiy, and N. F. Dyshkant,
“Face Recognition using Kernel Entropy Component Analysis,” Neuro-
computing, vol. 74, no. 6, 2011.

[7] Q. Jiang, X. Yan, Z. Lv, and M. Guo, “Fault Detection in Nonlinear
Chemical Processes based on Kernel Entropy Component Analysis and
an Angular Structure,” Korean Journal of Chemical Engineering, vol. 30,
no. 6, 2013.

[8] Y. D. Hu, J. C. Pan, and X. Tan, “High-Dimensional Data Dimension
Reduction based on KECA,” Applied Mechanics and Materials, vol.
303-306, 2013.

[9] Z. Xie and L. Guan, “Multimodal information fusion of audio emotion
recognition based on kernel entropy component analysis,” in IEEE
International Symposium on Multimedia, 2012.

[10] X. Q. Luo, X. J. Wu, and Z. Zhang, “Kernel Entropy-Based Unsu-
pervised Spectral Feature Selection,” International Journal of Pattern
Recognition and Artiﬁcial Intelligence, vol. 26, no. 5, 2012.

[11] M. Cheng, C.-M. Pun, and Y. Y. Tang, “Nonnegative Class-Speciﬁc
Entropy Component Analysis with Adaptive Step Search Criterion,”
Pattern Analysis and Applications, 2011.

[12] J. Myhre and R. Jenssen, “Mixture Weight Inﬂuence on Kernel Entropy
Component Analysis and Semi-Supervised Learning using the LASSO,”
in IEEE International Workshop in Machine Learning for Signal Pro-
cessing, Santander, Spain, 2012.

[13] B. Sch¨olkopf, A. Smola, and K. M¨uller, “Nonlinear component analysis
as a kernel eigenvalue problem,” Neural Computation, vol. 10, no. 5,
1998.

Fig. 6. Classiﬁcation results for the Z¨urich QuickBird satellite image for
different σ values and numbers of retained dimensions by KECA and OKECA.
The ﬁve bars for every number of retained features are, from left to right:
σd1, σd2, σSilv, σM L and σclass.

RGB composite

Groundtruth map

KECA (84.92%)

OKECA (90.33%)

Classiﬁcation maps for the Z¨urich QuickBird satellite image
Fig. 7.
using three features and σclass in KECA and OKECA. Top-left: RBG
version of the original image; top-right: ground truth classiﬁcation map (each
color represents a different land-cover class); bottom-left: classiﬁcation map
obtained with KECA; bottom-right: classiﬁcation map obtained with OKECA.

retained features and σclass for both methods. Note how OKECA
outperforms KECA in general for all the classes.

D. Discussion

The OKECA potential has been shown in the different experiments.
In all of them, the proposed method presents an extraordinary advan-
tage: the information is compacted in very few features (often in just
one or two) with higher expressive power optimizing the information
potential. That is demonstrated from experimental viewpoint, not only
in PDF estimation but also in classiﬁcation tasks. As we have shown,

2468101214161820505560657075808590OA [%]KECAOKECAIEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, 2016

7

[14] P. Lai and C. Fyfe, “Kernel and non-linear canonical correlation analy-

sis,” Intl. Journal of Neural Systems, vol. 10, pp. 365–377, 2000.

[15] R. Rosipal and L. J. Trejo, “Kernel partial least squares regression
in reproducing kernel hilbert space,” Journal of Machine Learning
Research, vol. 2, pp. 97–123, 2001.

[16] J. Arenas-Garc´ıa, K. Petersen, G. Camps-Valls, and L. Hansen, “Kernel
multivariate analysis framework for supervised subspace learning: A
tutorial on linear and kernel multivariate methods,” Signal Processing
Magazine, IEEE, vol. 30, no. 4, pp. 16–29, 2013.

[17] B. W. Silverman, Density Estimation for Statistics and Data Analysis.

London: Chapman & Hall, 1986.

[18] M. Girolami, “Orthogonal series density estimation and the kernel
eigenvalue problem,” Neural Computation, vol. 14, no. 3, pp. 669–688,
2002.

[19] R. P. W. Duin, “On the choice of smoothing parameters for Parzen esti-
mators of probability density functions,” Computers, IEEE Transactions
on, vol. C-25, no. 11, pp. 1175–1179, 1976.

[20] U. Ozertem and D. Erdogmus, “Locally deﬁned principal curves and
surfaces,” J. Mach. Learn. Res., vol. 12, pp. 1249–1286, Jul. 2011.
[Online]. Available: http://dl.acm.org/citation.cfm?id=1953048.2021041
[21] J. C. Principe, Information Theoretic Learning: Renyi Entropy and

Kernel Perspectives. Springer, 2010.

[22] T. M. Cover and J. A. Thomas, Entropy, Relative Entropy, and Mutual

Information.

John Wiley & Sons, Inc., 2005.

[23] J. Kim and C. D. Scott, “Robust kernel density estimation,” J. Mach.
Learn. Res., vol. 13, no. 1, pp. 2529–2565, Sep. 2012. [Online].
Available: http://dl.acm.org/citation.cfm?id=2503308.2503323

[24] A. Hyv¨arinen, J. Karhunen, and E. Oja, Independent Component Anal-

ysis. New York, USA: John Wiley & Sons, 2001.

[25] R. Jenssen, “Information theoretic learning and kernel methods,” in
Information Theory and Statistical Learning, F. Emmert-Streib and
M. Dehmer, Eds. Springer US, 2009, pp. 209–230. [Online]. Available:
http://dx.doi.org/10.1007/978-0-387-84816-7 9

[26] A. Hyv¨arinen and E. Oja, “Independent Component Analysis: A Tuto-

rial,” Neural Networks, vol. 13, no. 4-5, pp. 411–430, 2000.

[27] S. J. Pan and Q. Yang, “Domain adaptation via transfer component

analysis,” IEEE Trans. Neural Networks, vol. 22, pp. 199–210, 2011.

[28] E. Parzen, “On estimation of a probability density function and mode,”
The Annals of Mathematical Statistics, vol. 33, no. 3, pp. 1065–1076,
1962.

[29] I. T. Jollife, Principal Component Analysis. Springer, 1986.
[30] N. Mammone, F. La Foresta, and F. Morabito, “Automatic artifact
rejection from multichannel scalp eeg by wavelet ica,” Sensors Journal,
IEEE, vol. 12, no. 3, pp. 533–542, March 2012.

[31] G. Camps-Valls, D. Tuia, L. G´omez-Chova, S. Jimenez, and J. Malo,
Remote Sensing Image Processing, ser. Synthesis Lectures on Image,
Video, and Multimedia Processing. Morgan and Claypool, 2011.

