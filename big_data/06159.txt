6
1
0
2

 
r
a

 

M
9
1

 
 
]

.

C
O
h
t
a
m

[
 
 

1
v
9
5
1
6
0

.

3
0
6
1
:
v
i
X
r
a

Fast Incremental Method for Nonconvex Optimization

Sashank J. Reddi

sjakkamr@cs.cmu.edu

Suvrit Sra

suvrit@mit.edu

Carnegie Mellon University

Massachusetts Institute of Technology

Barnab´as P´ocz´os

bapoczos@cs.cmu.edu

Alex Smola

alex@smola.org

Carnegie Mellon University

Carnegie Mellon University

(cid:80)

Abstract

We analyze a fast incremental aggregated gradient method for optimizing nonconvex prob-
i fi(x). Speciﬁcally, we analyze the Saga algorithm within an Incre-
lems of the form minx
mental First-order Oracle framework, and show that it converges to a stationary point provably
faster than both gradient descent and stochastic gradient descent. We also discuss a Polyak’s
special class of nonconvex problems for which Saga converges at a linear rate to the global opti-
mum. Finally, we analyze the practically valuable regularized and minibatch variants of Saga.
To our knowledge, this paper presents the ﬁrst analysis of fast convergence for an incremental
aggregated gradient method for nonconvex problems.

1 Introduction

We study the ﬁnite-sum optimization problem

min
x∈Rd

f (x) :=

n(cid:88)

i=1

1
n

fi(x),

(1)

where each fi (i ∈ {1, . . . , n} (cid:44) [n]) can be nonconvex; our only assumption is that the gradients of
fi exist and are Lipschitz continuous. We denote the class of such instances of (1) by Fn.

Problems of the form (1) are of central importance in machine learning where they occur as
instances of empirical risk minimization; well-known examples include logistic regression (con-
vex) (James et al., 2013) and deep neural networks (nonconvex) (Deng & Yu, 2014). Consequently,
such problems have been intensively studied. A basic approach for solving (1) is gradient descent
(GradDescent), described by the following update rule:

xt+1 = xt − ηt∇f (xt), where ηt > 0.

(2)

However, iteration (2) is prohibitively expensive in large-scale settings where n is very large. For
such settings, stochastic and incremental methods are typical (Bertsekas, 2011; Defazio et al., 2014).
These methods use cheap noisy estimates of the gradient at each iteration of (2) instead of ∇f (xt).
A particularly popular approach, stochastic gradient descent (Sgd) uses ∇fit , where it in chosen
uniformly randomly from {1, . . . , n}. While the cost of each iteration is now greatly reduced, it is
not without any drawbacks. Due to the noise (also known as variance in stochastic methods) in the

1

gradients, one has to typically use decreasing step-sizes ηt to ensure convergence, and consequently,
the convergence rate gets adversely aﬀected.

Therefore, it is of great interest to overcome the slowdown of Sgd without giving up its scalability.
Towards this end, for convex instances of (1), remarkable progress has been made recently. The key
realization is that if we make multiple passes through the data, we can store information that
allows us to reduce variance of the stochastic gradients. As a result, we can use constant stepsizes
(rather than diminishing scalars) and obtain convergence faster than Sgd, both in theory and
practice (Johnson & Zhang, 2013; Schmidt et al., 2013; Defazio et al., 2014).

Nonconvex instances of (1) are also known to enjoy similar speedups (Johnson & Zhang, 2013),
but existing analysis does not explain this success as it relies heavily on convexity to control variance.
Since Sgd dominates large-scale nonconvex optimization (including neural network training), it is
of great value to develop and analyze faster nonconvex stochastic methods.

With the above motivation, we analyze Saga (Defazio et al., 2014), an incremental aggregated
gradient algorithm that extends the seminal Sag method of (Schmidt et al., 2013), and has been
shown to work well for convex ﬁnite-sum problems (Defazio et al., 2014). Speciﬁcally, we analyze
Saga for the class Fn using the incremental ﬁrst-order oracle (IFO) framework (Agarwal & Bottou,
2014). For f ∈ Fn, an IFO is a subroutine that takes an index i ∈ [n] and a point x ∈ Rd, and
returns the pair (fi(x),∇fi(x)).

To our knowledge, this paper presents the ﬁrst analysis of fast convergence for an incremental
aggregated gradient method for nonconvex problems. The attained rates improve over both Sgd
and GradDescent, a beneﬁt that is also corroborated by experiments. Before presenting our new
analysis, let us brieﬂy recall some items of related work.

1.1 Related work

A concise survey of incremental gradient methods is (Bertsekas, 2011). An accessible analysis of
stochastic convex optimization (min Ez[F (x, z)]) is (Nemirovski et al., 2009). Classically, Sgd stems
from the seminal work (Robbins & Monro, 1951), and has since witnessed many developments
(Kushner & Clark, 2012), including parallel and distributed variants (Bertsekas & Tsitsiklis, 1989;
Agarwal & Duchi, 2011; Recht et al., 2011), though non-asymptotic convergence analysis is limited
to convex setups. Faster rates for convex problems in Fn are attained by variance reduced stochastic
methods, e.g., (Defazio et al., 2014; Johnson & Zhang, 2013; Schmidt et al., 2013; Shalev-Shwartz
& Zhang, 2013; Reddi et al., 2015). Linear convergence of stochastic dual coordinate ascent when
fi (i ∈ [n]) may be nonconvex but f is strongly convex is studied in (Shalev-Shwartz, 2015). Lower
bounds for convex ﬁnite-sum problems are studied in (Agarwal & Bottou, 2014).

For nonconvex nonsmooth problems the ﬁrst incremental proximal-splitting methods is in (Sra,
2012), though only asymptotic convergence is studied. Hong (Hong, 2014) studies convergence of
a distributed nonconvex incremental ADMM algorithm. The ﬁrst work to present non-asymptotic
convergence rates for Sgd is (Ghadimi & Lan, 2013); this work presents an O(1/2) iteration bound
for Sgd to satisfy approximate stationarity (cid:107)∇f (x)(cid:107)2 ≤ , and their convergence criterion is moti-
vated by the gradient descent analysis of Nesterov (Nesterov, 2003). The ﬁrst analysis for nonconvex
variance reduced stochastic gradient is due to (Shamir, 2014), who apply it to the speciﬁc problem
of principal component analysis (PCA).

2 Preliminaries

In this section, we further explain our assumptions and goals. We say f is L-smooth if there is a
constant L such that

(cid:107)∇f (x) − ∇f (y)(cid:107) ≤ L(cid:107)x − y(cid:107),

∀ x, y ∈ Rd.

2

f (x) − f (x∗) ≤ τ(cid:107)∇f (x)(cid:107)2,

Throughout, we assume that all fi in (1) are L-smooth, i.e., (cid:107)∇fi(x) − ∇fi(y)(cid:107) ≤ L(cid:107)x − y(cid:107) for all
i ∈ [n]. Such an assumption is common in the analysis of ﬁrst-order methods. For ease of exposition,
we assume the Lipschitz constant L to be independent of n. For our analysis, we also discuss the
class of τ -gradient dominated (Polyak, 1963; Nesterov & Polyak, 2006) functions, namely functions
for which

(3)
where x∗ is a global minimizer of f . This class of functions was originally introduced by Polyak
in (Polyak, 1963). Observe that such functions need not be convex. Also notice that gradient
dominance (3) is a restriction on the overall function f , but not on the individual functions fi
(i ∈ [n]).
Following (Nesterov, 2003; Ghadimi & Lan, 2013) we use (cid:107)∇f (x)(cid:107)2 ≤  to judge approximate
stationarity of x. Contrast this with Sgd for convex f , where one uses [f (x) − f (x∗)] or (cid:107)x − x∗(cid:107)2
as criteria for convergence analysis. Such criteria cannot be used for nonconvex functions due to the
intractability of the problem.
While the quantities (cid:107)∇f (x)(cid:107)2, [f (x)−f (x∗)], or (cid:107)x−x∗(cid:107)2 are not comparable in general, they are
typically assumed to be of similar magnitude (see (Ghadimi & Lan, 2013)). We note that our analysis
does not assume n to be a constant, so we report dependence on it in our results. Furthermore, while
stating our complexity results, we assume that the initial point of the algorithms is constant, i.e.,
f (x0) − f (x∗) and (cid:107)x0 − x∗(cid:107) are constants. For our analysis, we will need the following deﬁnition.
Deﬁnition 1. A point x is called -accurate if (cid:107)∇f (x)(cid:107)2 ≤ . A stochastic iterative algorithm is
said to achieve -accuracy in t iterations if E[(cid:107)∇f (xt)(cid:107)2] ≤ , where the expectation is taken over
its stochasticity.

We start our discussion of algorithms by recalling Sgd, which performs the following update in

its tth iteration:

xt+1 = xt − ηt∇fit(x),

(4)
where it is a random index chosen from [n], and the gradient ∇fit(xt) approximates the gradient
of f at xt, ∇f (xt). It can be seen that the update is unbiased, i.e., E[∇fit(xt)] = ∇f (xt) since
the index it is chosen uniformly at random. Though the Sgd update is unbiased, it suﬀers from
variance due to the aforementioned stochasticity in the chosen index. To control the variance one
has to decrease the step size ηt in (4), which in turn leads to slow convergence. The following is a
well-known result on Sgd in the context of nonconvex optimization (Ghadimi & Lan, 2013).
Theorem 1. Suppose (cid:107)∇fi(cid:107) ≤ σ i.e., gradient of function fi is bounded for all i ∈ [n], then the
IFO complexity of Sgd to obtain an -accurate solution is O(1/2).

It is instructive to compare the above result with the convergence rate of GradDescent. The
IFO complexity of GradDescent is O(n/). Thus, while Sgd eliminates the dependence on n,
the convergence rate worsens to O(1/2) from O(1/) in GradDescent. In the next section, we
investigate an incremental method with faster convergence.

3 Algorithm

We describe below the Saga algorithm and prove its fast convergence for nonconvex optimization.
Saga is a popular incremental method in machine learning and optimization communities. It is
very eﬀective in reducing the variance introduced due to stochasticity in Sgd. Algorithm 1 presents
pseudocode for Saga. Note that the update vt (Line 5) is unbiased, i.e., E[vt] = ∇f (xt). This is
due to the uniform random selection of index it. It can be seen in Algorithm 1 that Saga maintains
gradients at αi for i ∈ [n]. This additional set is critical to reducing the variance of the update vt. At

3

Algorithm 1 SAGA(cid:0)x0, T, η(cid:1)

(cid:80)n
1: Input: x0 ∈ Rd, α0
i = x0 for i ∈ [n], number of iterations T , step size η > 0
i=1 ∇fi(α0
2: g0 = 1
i )
3: for t = 0 to T − 1 do
n
4:
5:
6:
7:

Uniformly randomly pick it, jt from [n]
vt = ∇fit (xt) − ∇fit (αt
xt+1 = xt − ηvt
αt+1
gt+1 = gt − 1

it ) + gt
j for j (cid:54)= jt

= xt and αt+1

jt ) − ∇fjt (αt+1

8:
9: end for
10: Output: Iterate xa chosen uniformly random from {xt}T−1
t=0 .

jt

))

j = αt

n (∇fjt (αt

jt

each iteration of the algorithm, one of the αi is updated to the current iterate. An implementation
of Saga requires storage and updating of gradients ∇fi(αi); the storage cost of the algorithm is
nd. While this leads to higher storage in comparison to Sgd, this cost is often reasonable for many
applications. Furthermore, this cost can be reduced in the case of speciﬁc models; refer to (Defazio
et al., 2014) for more details.

For ease of exposition, we introduce the following quantity:

Γt =(cid:0)η − ct+1η

β − η2L − 2ct+1η2(cid:1),

(5)

where the parameters ct+1, β and η will be deﬁned shortly. We start with the following set of key
results that provide convergence rate of Algorithm 1.

Lemma 1. For ct, ct+1, β > 0, suppose we have

ct = ct+1(1 − 1

n + ηβ + 2η2L2) + η2L3.

Also let η, β and ct+1 be chosen such that Γt > 0. Then, the iterates {xt} of Algorithm 1 satisfy the
bound

where Rt := E[f (xt) + (ct/n)(cid:80)n

i=1 (cid:107)xt − αt

i(cid:107)2].

E[(cid:107)∇f (xt)(cid:107)2] ≤ Rt − Rt+1

Γt

,

The proof of this lemma is given in Section 9. Using this lemma we prove the following result on

the iterates of Saga.
Theorem 2. Let f ∈ Fn. Let cT = 0, β > 0, and ct = ct+1(1 − 1
n + ηβ + 2η2L2) + η2L3 be such
that Γt > 0 for 0 ≤ t ≤ T − 1. Deﬁne the quantity γn := min0≤t≤T−1 Γt. Then the output xa of
Algorithm 1 satisﬁes the bound

E[(cid:107)∇f (xa)(cid:107)2] ≤ f (x0) − f (x∗)

T γn

,

where x∗ is an optimal solution to (1).

Proof. We apply telescoping sums to the result of Lemma 1 to obtain

(cid:88)T−1

t=0

γn

E[(cid:107)∇f (xt)(cid:107)2] ≤(cid:88)T−1

t=0

≤ R0 − RT .

ΓtE[(cid:107)∇f (xt)(cid:107)2]

4

The ﬁrst inequality follows from the deﬁnition of γn. This inequality in turn implies the bound

(cid:88)T−1

t=0

E[(cid:107)∇f (xt)(cid:107)2] ≤ E[f (x0) − f (xT )]

γn

,

(6)

where we used that RT = E[f (xT )] (since cT = 0), and that R0 = E[f (x0)] (since α0
i = x0 for
i ∈ [n]). Using inequality (6), the optimality of x∗, and the deﬁnition of xa in Algorithm 1, we
obtain the desired result.

Note that the notation γn involves n, since this quantity can depend on n. To obtain an explicit
dependence on n, we have to use an appropriate choice of β and η. This is made precise by the
following main result of the paper.
Theorem 3. Suppose f ∈ Fn. Let η = 1/(3Ln2/3) and β = L/n1/3. Then, γn ≥
have the bound

12Ln2/3 and we

1

E[(cid:107)∇f (xa)(cid:107)2] ≤ 12Ln2/3[f (x0) − f (x∗)]

T

,

where x∗ is an optimal solution to the problem in (1) and xa is the output of Algorithm 1.

Proof. With the values of η and β, let us ﬁrst establish an upper bound on ct. Let θ denote
n − ηβ − 2η2L2. Observe that θ < 1 and θ ≥ 4/(9n). This is due to the speciﬁc values of η and β
1
stated in the theorem. Also, we have ct = ct+1(1 − θ) + η2L3. Using this relationship, it is easy to
see that ct = η2L3 1−(1−θ)T −t

. Therefore, we obtain the bound

θ

(7)
for all 0 ≤ t ≤ T , where the inequality follows from the deﬁnition of η and the fact that θ ≥ 4/(9n).
Using the above upper bound on ct we can conclude that

,

θ

ct = η2L3 1−(1−θ)T −t

≤ η2L3
θ

≤ L
4n1/3

(cid:0)η − ct+1η

β − η2L − 2ct+1η2(cid:1) ≥

γn = min

t

1

12Ln2/3

,

upon using the following inequalities: (i) ct+1η/β ≤ η/4, (ii) η2L ≤ η/3 and (iii) 2ct+1η2 ≤ η/6,
which hold due to the upper bound on ct in (7). Substituting this bound on γn in Theorem 2, we
obtain the desired result.

A more general result with step size η < 1/(3Ln2/3) can be proved, but it will only lead to a
theoretically suboptimal convergence result. Recall that each iteration of Algorithm 1 requires O(1)
IFO calls. Using this fact, we can rewrite Theorem 3 in terms of its IFO complexity as follows.
Corollary 1. If f ∈ Fn, then the IFO complexity of Algorithm 1 (with parameters from Theorem 3)
to obtain an -accurate solution is O(n + n2/3/).

This corollary follows from the O(1) per iteration cost of Algorithm 1 and because n IFO calls
are required to calculate g0 at the start of the algorithm. In special cases, the initial O(n) IFO calls
can be avoided (refer to (Schmidt et al., 2013; Defazio et al., 2014) for details). By comparing the
IFO complexity of Saga (O(n + n2/3/)) with that of GradDescent (O(n/)), we see that Saga
is faster than GradDescent by a factor of n1/3.

5

4 Finite Sums with Regularization

In this section, we study the problem of ﬁnite-sum problems with additional regularization. More
speciﬁcally, we consider problems of the form

min
x∈Rd

f (x) :=

1
n

n(cid:88)

i=1

fi(x) + r(x),

(8)

where r : Rd → R is an L-smooth (possibly nonconvex) function. Problems of this nature arise in
machine learning where the functions fi are loss functions and r is a regularizer. Since we assumed
r to be smooth, (8) can be reformulated as (1) by simply encapsulating r into the functions fi.
However, as we will see, it is beneﬁcial to handle the regularization separately. We call the variant
of Saga with the additional regularization as Reg-Saga. The key diﬀerence between Reg-Saga
and Saga lies in Line 6 of Algorithm 1. In particular, for Reg-Saga, Line 6 of Algorithm 1 is
replaced with the following update:

xt+1 = xt − η(vt + ∇r(xt)).

(9)

Note that the primary diﬀerence is that the part of gradient based on function r is updated at each
iteration of the algorithm. The convergence of Reg-Saga can be proved along the lines of our
analysis of Saga. Hence, we omit the details for brevity and directly state the following key result
stating the IFO complexity of Reg-Saga.

Theorem 4. If function f is of the form in (8), then the IFO complexity of Reg-Saga to obtain
an -accurate solution is O(n + n2/3/).

The proof essentially follows along the lines of the proof of Theorem 3. The diﬀerence, however,
being that the update corresponding to function r(x) is handled explicitly at each iteration. Note
that the above IFO complexity is not diﬀerent from that in Corollary 1. However, its main beneﬁt
comes in the form of storage eﬃciency in problems with more structure. To understand this, consider
the problems of form

min
x∈Rd

f (x) :=

1
n

l(x(cid:62)zi) + r(x),

(10)

where zi ∈ Rd for i ∈ [n] while l : R → R≥0 is a diﬀerentiable loss function. Here, l and r can be in
general nonconvex. Such problems are popularly known as (regularized) empirical risk minimization
in machine learning literature. We can directly apply Saga to (10) by casting it in the form (1).
However, recall that the storage cost of Saga is O(nd) due to the cost of storing the gradient at αt
i.
This storage cost can be avoided in Reg-Saga by handling the function r separately. Indeed, for
Reg-Saga we need to store just ∇l(x(cid:62)zi) for all i ∈ [n] (as ∇r(x) is updated at each iteration).
By observing that ∇l(x(cid:62)zi) = l(cid:48)(x(cid:62)zi)zi, where l(cid:48) represents the derivative of l, it is apparent that
we need to store only the scalars l(cid:48)(x(cid:62)zi) for Reg-Saga. This reduces the storage O(n) instead of
O(nd) in Saga.

n(cid:88)

i=1

5 Linear convergence rates for gradient dominated functions

Until now the only assumption we used was Lipschitz continuity of gradients. An immediate question
is whether the IFO complexity can be further improved under stronger assumptions. We provide
an aﬃrmative answer to this question by showing that for gradient dominated functions, a variant
of Saga attains linear convergence rate. Recall that a function f is called τ -gradient dominated if
around an optimal point x∗, f satisﬁes the following growth condition:
∀x ∈ Rd.

f (x) − f (x∗) ≤ τ(cid:107)∇f (x)(cid:107)2,

6

Algorithm 2 GD-SAGA(cid:0)x0, K, T, η(cid:1)

Input: x0 ∈ Rd, K, epoch length m, step sizes η > 0
for k = 0 to K do

xk = SAGA(xk−1, T, η)

end for
Output: xK

For such functions, we use the variant of Saga shown in Algorithm 2. Observe that Algorithm 2
uses Saga as a subroutine. Alternatively, one can rewrite Algorithm 2 in the form of KT iterations
of Algorithm 1 where one updates {αi} after every T iterations and then selects a random iterate
amongst the last T iterates. For this variant of Saga, we can prove the following linear convergence
result.
Theorem 5. If f is τ -gradient dominated, then with η = 1/(3Ln2/3) and T = (cid:100)24Lτ n2/3(cid:101), the
iterates of Algorithm 2 satisfy E[(cid:107)f (xk)(cid:107)2] ≤ 2−k(cid:107)f (x0)(cid:107)2, where x∗ is an optimal solution of (1).

Proof. The iterates of Algorithm 2 satisfy the bound

E[(cid:107)∇f (xk)(cid:107)2] ≤ E[f (xk−1) − f (x∗)]

2τ

,

(11)

which holds due to Theorem 3 given the choices of η and T assumed in the statement. However,
f is τ -gradient dominated, so E[(cid:107)∇f (xk−1)(cid:107)2] ≥ E[f (xk−1) − f (x∗)]/τ , which combined with (11)
concludes the proof.

An immediate consequence of this theorem is the following.

Corollary 2. If f is τ -gradient dominated, the IFO complexity of Algorithm 2 (with parameters
from Theorem 5) to compute an -accurate solution is O((n + τ n2/3) log(1/)).

While we state the result in terms of (cid:107)∇f (x)(cid:107)2, it is not hard to see that for gradient dominated

functions a similar result holds for the convergence criterion being [f (x) − f (x∗)].
Theorem 6. If f is τ -gradient dominated, with η = 1/(3Ln2/3) and T = (cid:100)24Lτ n2/3(cid:101), the iterates
of Algorithm 2 satisfy

E[f (xk) − f (x∗)] ≤ 2−k[f (x0) − f (x∗)],

where x∗ is an optimal solution to (1).

A noteworthy aspect of the above result is the linear convergence rate to a global optimum.
Therefore, the above result is stronger than Theorem 3. Note that throughout our analysis of
gradient dominated functions, no assumptions other than Lipschitz smoothness are placed on the
individual set of functions fi. We emphasize here that these results can be further improved with
additional assumptions (e.g., strong convexity) on the individual functions fi and on f . Also note
that GradDescent can achieve linear convergence rate for gradient dominated functions (Polyak,
1963). However, the IFO complexity of GradDescent is O(τ n log(1/)), which is strictly worse
than IFO complexity of GD-Saga (see Corollary 2).

6 Minibatch Variant

A common variant of incremental methods is to sample a set of indices It instead of single index it
when approximating the gradient. Such a variant is generally referred to as a “minibatch” version of
the algorithm. Minibatch variants are of great practical signiﬁcance since they reduce the variance

7

Algorithm 3 Minibatch-SAGA(cid:0)x0, b, T, η(cid:1)

(cid:80)n
1: Input: x0 ∈ Rd, α0
i = x0 for i ∈ [n], minibatch size b, number of iterations T , step size η > 0
i=1 ∇fi(α0
2: g0 = 1
i )
3: for t = 0 to T − 1 do
(cid:80)
n
4:
5:

Uniformly randomly pick (with replacement) indices sets It, Jt of size b from [n]
vt = 1|It|
i∈It
xt+1 = xt − ηvt
j = xt for j ∈ Jt and αt+1
αt+1
(∇fj(αt
gt+1 = gt − 1

(∇fi(xt) − ∇fi(αt

it )) + gt
j for j /∈ Jt
))

j = αt
j) − ∇fj(αt+1

(cid:80)

6:
7:

8:
9: end for
10: Output: Iterate xa chosen uniformly random from {xt}T−1
t=0 .

j∈Jt

n

j

of incremental methods and promote parallelism. Algorithm 3 lists the pseudocode for a minibatch
variant of Saga. Algorithm 3 uses a set It of size |It| = b for calculating the update vt instead
of a single index it used in Algorithm 1. By using a larger b, one can reduce the variance due to
the stochasticity in the algorithm. Such a procedure is also beneﬁcial in parallel settings since the
calculation of the update vt can be parallelized. For this algorithm, we can prove the following
convergence result.
Theorem 7. Suppose f ∈ Fn. Let η = b/(3Ln2/3) and β = L/n1/3. Then for the output xa of
Algorithm 3 (with b < n2/3) we have γn ≥

b

12Ln2/3 and

E[(cid:107)∇f (xa)(cid:107)2] ≤ 12Ln2/3[f (x0) − f (x∗)]

bT

,

where x∗ is an optimal solution to (1).

We omit the details of the proof since it is similar to the proof of Theorem 3. Note that the
key diﬀerence in comparison to Theorem 1 is that we can now use a more aggressive step size
η = b/(3Ln2/3) due to a larger minibatch size b. An interesting aspect of the result is the O(1/b)
dependence on the minibatch size b. As long as this size is not large (b < n2/3), one can signiﬁcantly
improve the convergence rate to a stationary point. A restatement of aforementioned result in terms
of IFO complexity is provided below.
Corollary 3. If f ∈ Fn, then the IFO complexity of Algorithm 3 (with parameters from Theorem 7
and minibatch size b < n2/3) to obtain an -accurate solution is O(n + n2/3/).

By comparing the above result with Corollary 1, we can see that the IFO complexity of minibatch-
Saga is the same Saga. However, since the b gradients can be computed in parallel, one can achieve
(theoretical) b times speedup in multicore and distributed settings. In contrast, the performance
√
Sgd degrades with minibatch size b since the improvement in convergence rate for Sgd is typically
b) but b IFO calls are required at each iteration of minibatch-Sgd. Thus, Saga has a much
O(1/
more eﬃcient minibatch version in comparison to Sgd.

Discussion of Convergence Rates

Before ending our discussion on convergence rates, it is important to compare and contrast diﬀerent
convergence rates obtained in the paper. For general smooth nonconvex problems, we observed that
Saga has a low IFO complexity of O(n+n2/3/) in comparison to Sgd (O(1/2)) and GradDescent
(O(n/)). This diﬀerence in the convergence is especially signiﬁcant if one requires a medium to
high accuracy solution, i.e.,  is small.

8

Figure 1: Results for nonconvex regularized generalized linear models (see Equation (12)). The
ﬁrst and last two ﬁgures correspond to rcv1 and realsim datasets respectively. The results compare
the performance of Reg-Saga and Sgd algorithms. Here ˆx corresponds to the solution obtained
by running GradDescent for a very long time and using multiple restarts. As seen in the ﬁgure,
Reg-Saga converges much faster than Sgd in terms of objective function value and the stationarity
gap (cid:107)∇f (x)(cid:107)2.

Furthermore, for gradient dominated functions, where Sgd obtains a sublinear convergence rate
of O(1/2)1 as opposed to fast linear convergence rate of a variant of Saga (see Theorem 2). It is
an interesting future work to explore other setups where we can achieve stronger convergence rates.
From our analysis of minibatch-Saga in Section 6, we observe that Saga proﬁts from mini-
batching much more than Sgd. In particular, one can achieve a (theoretical) linear speedup using
mini-batching in Saga in parallel settings. On the other hand, the performance of Sgd typically
In summary, Saga enjoys all the beneﬁts of GradDescent like
degrades with minibatching.
constant step size, eﬃcient minibatching with much weaker dependence on n.

Notably, Saga, unlike Sgd, does not use any additional assumption of bounded gradients (see
Theorem 1 and Corollary 1). Moreover, if one uses a constant step size for Sgd, we need to have
an advance knowledge of the total number of iterations T in order to obtain the convergence rate
mentioned in Theorem 1.

7 Experiments

We present our empirical results in this section. For our experiments, we study the problem of
binary classiﬁcation using nonconvex regularizers. The input consists of tuples {(zi, yi)}n
i=1 where
zi ∈ Rd (commonly referred to as features) and yi ∈ {−1, 1} (class labels). We are interested in the
empirical loss minimization setup described in Section 4. Recall that problem of ﬁnite sum with
regularization takes the form

n(cid:88)

i=1

min
x∈Rd

f (x) :=

1
n

fi(x) + r(x).

(12)

gradients. The function r(x) = λ(cid:80)d

For our experiments, we use logistic function for fi, i.e., fi(x) = log(1 + exp(−yix(cid:62)zi)) for all i ∈ [n].
All zi are normalized such that (cid:107)zi(cid:107) = 1. We observe that the loss function has Lipschitz continuous
i ) is chosen as the regularizer (see (Antoniadis
et al., 2009)). Note that the regularizer is nonconvex and smooth. In our experiments, we use the
parameter settings of λ = 0.001 and α = 1 for all the datasets.

i=1 αx2

i /(1 + αx2

We compare the performance of Sgd (the de facto incremental method for nonconvex optimiza-
tion) with nonconvex Reg-Saga in our experiments. The comparison is based on the following
criteria: (i) the objective function value (also called training loss in this context), which is the main
goal of the paper; and (ii) the stationarity gap (cid:107)∇f (x)(cid:107)2, the criteria used for our theoretical anal-
ysis. For the step size of Sgd, we use the popular t−inverse schedule ηt = η0(1 + η(cid:48)(cid:98)t/n(cid:99))−1, where

1For Sgd, we are not aware of any better convergence rates for gradient dominated functions.

9

# grad/n020406080100f(xt)!f(^x)10-510-410-310-210-1REG-SAGASGD# grad/n020406080100krf(xt)k210-1010-810-610-4REG-SAGASGD# grad/n020406080100f(xt)!f(^x)10-410-310-210-1REG-SAGASGD# grad/n020406080100krf(xt)k210-1010-810-610-4REG-SAGASGDη0 and η(cid:48) are tuned so that Sgd gives the best performance on the training loss. In our experiments,
we also use η(cid:48) = 0; this results in a ﬁxed step size for Sgd. For Reg-Saga, a ﬁxed step size is chosen
(as suggested by our analysis) so that it gives the best performance on the objective function value,
i.e., training loss. Note that due to the structure of the problem in (12), as discussed in Section 4,
the storage cost of Reg-Saga is just O(n).

Initialization is critical to many of the incremental methods like Reg-Saga. This is due to the
stronger dependence of the convergence on the initial point (see Theorem 3). Furthermore, one has
to obtain ∇fi(α0
i ) for all i ∈ [n] in Reg-Saga algorithm (see Algorithm 1). For initialization of both
Sgd and Reg-Saga, we make one pass (without replacement) through the dataset and perform the
updates of Sgd during this pass. Doing so not only allows us to also obtain a good initial point
i ) for i ∈ [n]. Note that this choice results in a
x0 but also to compute the initial values of ∇f (α0
variant of Reg-Saga where α0
i are diﬀerent for various i (unlike the pseudocode in Algorithm 1).
The convergence rates of this variant can be shown to be similar to that of Algorithm 1.

Figure 1 shows the results of our experiments. The results are on two standard UCI datasets,
‘rcv1’ and ‘realsim’2. The plots compare the criteria mentioned earlier against the number of IFO
calls made by the corresponding algorithm. For the objective function, we look at the diﬀerence
between the objective function (f (xt)) and the best objective function value obtained by running
GradDescent for a very large number of iterations using multiple initializations (denoted by
f (ˆx)). As shown in the ﬁgure, Reg-Saga converges much faster than Sgd in terms of objective
value. Furthermore, as supported by the theory, the stationarity gap for Reg-Saga is very small
in comparison to Sgd. Also, in our experiments, the selection of step size was much easier for
Reg-Saga when compared to Sgd. Overall the empirical results for nonconvex regularized problems
are promising. It will be interesting to apply the approach for other smooth nonconvex problems.

8 Conclusion

In this paper, we investigated a fast incremental method (Saga) for nonconvex optimization. Our
main contribution in this paper to show that Saga can provably perform better than both Sgd
and GradDescent in the context of nonconvex optimization. We also showed that with additional
assumptions on function f in (1) like gradient dominance, Saga has linear convergence to the global
minimum as opposed to sublinear convergence of Sgd. Furthermore, for large scale parallel settings,
we proposed a minibatch variant of Saga with stronger theoretical convergence rates than Sgd by
attaining linear speedups in the size of the minibatches. One of the biggest advantages of Saga is
the ability to use ﬁxed step size. Such a property is important in practice since selection of step size
(learning rate) for Sgd is typically diﬃcult and is one of its biggest drawbacks.

References

Agarwal, Alekh and Bottou, Leon.

A lower bound for the optimization of ﬁnite sums.

arXiv:1410.0723, 2014.

Agarwal, Alekh and Duchi, John C. Distributed delayed stochastic optimization. In Advances in

Neural Information Processing Systems, pp. 873–881, 2011.

Antoniadis, Anestis, Gijbels, Ir`ene, and Nikolova, Mila. Penalized likelihood regression for general-
ized linear models with non-quadratic penalties. Annals of the Institute of Statistical Mathematics,
63(3):585–615, June 2009.

2The datasets can be downloaded from https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.

html.

10

Bertsekas, D. and Tsitsiklis, J. Parallel and Distributed Computation: Numerical Methods. Prentice-

Hall, 1989.

Bertsekas, Dimitri P. Incremental gradient, subgradient, and proximal methods for convex optimiza-
tion: A survey. In S. Sra, S. Nowozin, S. Wright (ed.), Optimization for Machine Learning. MIT
Press, 2011.

Defazio, Aaron, Bach, Francis, and Lacoste-Julien, Simon. SAGA: A fast incremental gradient
method with support for non-strongly convex composite objectives. In NIPS 27, pp. 1646–1654,
2014.

Deng, Li and Yu, Dong. Deep learning: Methods and applications. Foundations and Trends Signal

Processing, 7:197–387, 2014.

Ghadimi, Saeed and Lan, Guanghui. Stochastic ﬁrst- and zeroth-order methods for nonconvex
stochastic programming. SIAM Journal on Optimization, 23(4):2341–2368, 2013. doi: 10.1137/
120880811.

Hong, Mingyi. A distributed, asynchronous and incremental algorithm for nonconvex optimization:

An admm based approach. arXiv preprint arXiv:1412.6058, 2014.

James, G., Witten, D., Hastie, T., and Tibshirani, R. An Introduction to Statistical Learning: with

Applications in R. Springer Texts in Statistics. Springer, 2013.

Johnson, Rie and Zhang, Tong. Accelerating stochastic gradient descent using predictive variance

reduction. In NIPS 26, pp. 315–323, 2013.

Kushner, Harold Joseph and Clark, Dean S. Stochastic approximation methods for constrained and

unconstrained systems, volume 26. Springer Science & Business Media, 2012.

Nemirovski, A., Juditsky, A., Lan, G., and Shapiro, A. Robust stochastic approximation approach

to stochastic programming. SIAM Journal on Optimization, 19(4):1574–1609, 2009.

Nesterov, Yurii. Introductory Lectures On Convex Optimization: A Basic Course. Springer, 2003.

Nesterov, Yurii and Polyak, Boris T. Cubic regularization of newton method and its global perfor-

mance. Mathematical Programming, 108(1):177–205, 2006.

Polyak, B.T. Gradient methods for the minimisation of functionals. USSR Computational Mathe-

matics and Mathematical Physics, 3(4):864–878, January 1963.

Recht, Benjamin, Re, Christopher, Wright, Stephen, and Niu, Feng. Hogwild!: A Lock-Free Ap-

proach to Parallelizing Stochastic Gradient Descent. In NIPS 24, pp. 693–701, 2011.

Reddi, Sashank, Hefny, Ahmed, Sra, Suvrit, Poczos, Barnabas, and Smola, Alex J. On variance
reduction in stochastic gradient descent and its asynchronous variants. In NIPS 28, pp. 2629–
2637, 2015.

Robbins, H. and Monro, S. A stochastic approximation method. Annals of Mathematical Statistics,

22:400–407, 1951.

Schmidt, Mark W., Roux, Nicolas Le, and Bach, Francis R. Minimizing Finite Sums with the

Stochastic Average Gradient. arXiv:1309.2388, 2013.

Shalev-Shwartz, Shai. SDCA without duality. CoRR, abs/1502.06177, 2015.

Shalev-Shwartz, Shai and Zhang, Tong. Stochastic dual coordinate ascent methods for regularized

loss. The Journal of Machine Learning Research, 14(1):567–599, 2013.

11

Shamir, Ohad. A stochastic PCA and SVD algorithm with an exponential convergence rate.

arXiv:1409.2848, 2014.

Sra, Suvrit. Scalable nonconvex inexact proximal splitting. In NIPS, pp. 530–538, 2012.

Appendix

9 Proof of Lemma 1

Proof. Since f is L-smooth, from Lemma 3, we have

E[f (xt+1)] ≤ E[f (xt) + (cid:104)∇f (xt), xt+1 − xt(cid:105)
2 (cid:107)xt+1 − xt(cid:107)2].

+ L

We ﬁrst note that the update in Algorithm 1 is unbiased i.e., E[vt] = ∇f (xt). By using this property
of the update on the right hand side of the inequality above, we get the following:

(13)
Here we used the fact that xt+1−xt = −ηvt (see Algorithm 1). Consider now the Lyapunov function

E[f (xt+1)] ≤ E[f (xt) − ηt(cid:107)∇f (xt)(cid:107)2 + Lη2

2 (cid:107)vt(cid:107)2].

Rt := E[f (xt) + ct

n

(cid:107)xt − αt

i(cid:107)2].

For bounding Rt+1 we need the following:

n(cid:88)

i=1

(cid:107)2]

E[(cid:107)xt+1 − αt+1

i

n(cid:88)
n(cid:88)

i=1

1
n

i=1

1
n

=

 1

n

E(cid:107)xt+1 − xt(cid:107)2 +

n − 1
n

 .
(cid:125)
i(cid:107)2
E(cid:107)xt+1 − αt

(cid:123)(cid:122)

(cid:124)

T1

(14)

The above equality from the deﬁnition of αt+1
rithm 1. The term T1 in (14) can be bounded as follows

i

and the uniform randomness of index jt in Algo-

i(cid:107)2]
T1 = E[(cid:107)xt+1 − xt + xt − αt
i(cid:105)]
i(cid:107)2 + 2(cid:104)xt+1 − xt, xt − αt
= E[(cid:107)xt+1 − xt(cid:107)2 + (cid:107)xt − αt
= E[(cid:107)xt+1 − xt(cid:107)2 + (cid:107)xt − αt
i(cid:107)2] − 2ηE[(cid:104)∇f (xt), xt − αt
i(cid:105)]
i(cid:107)2]
≤ E[(cid:107)xt+1 − xt(cid:107)2 + (cid:107)xt − αt

+ 2ηE(cid:104) 1

i(cid:107)2(cid:105)

2β(cid:107)∇f (xt)(cid:107)2 + 1

2 β(cid:107)xt − αt

.

(15)

The second equality again follows from the unbiasedness of the update of Saga. The last inequality
follows from a simple application of Cauchy-Schwarz and Young’s inequality. Plugging (13) and (15)

12

into Rt+1, we obtain the following bound:

n(cid:88)
2 (cid:107)vt(cid:107)2]

i=1

(cid:107)xt − αt

i(cid:107)2]

i(cid:107)2(cid:105)

2β(cid:107)∇f (xt)(cid:107)2 + 1

2 β(cid:107)xt − αt

+

n(cid:88)

2(n − 1)ct+1η

Rt+1 ≤ E[f (xt) − η(cid:107)∇f (xt)(cid:107)2 + Lη2
n − 1
+ E[ct+1(cid:107)xt+1 − xt(cid:107)2 + ct+1
n2

E(cid:104) 1
≤ E[f (xt) −(cid:16)
(cid:17)(cid:107)∇f (xt)(cid:107)2
(cid:16) Lη2
2 + ct+1η2(cid:17) E[(cid:107)vt(cid:107)2]
(cid:19) 1
(cid:18) n − 1
n(cid:88)

η − ct+1η

n2

i=1

+

β

ct+1 + ct+1ηβ

+

n

n

i=1

E(cid:2)(cid:107)xt − αt
i(cid:107)2(cid:3) .

(16)

To further bound the quantity in (16), we use Lemma 2 to bound E[(cid:107)vt(cid:107)2], so that upon substituting
it into (16), we obtain

η − ct+1η

Rt+1 ≤ E[f (xt)]

−(cid:16)
β − η2L − 2ct+1η2(cid:17) E[(cid:107)∇f (xt)(cid:107)2]
n(cid:88)
n + ηβ + 2η2L2(cid:1) + η2L3(cid:3) 1
+(cid:2)ct+1
(cid:0)1 − 1
≤ Rt −(cid:0)η − ct+1η
β − η2L − 2ct+1η2(cid:1)E[(cid:107)∇f (xt)(cid:107)2].

i=1

n

E(cid:2)(cid:107)xt − αt
i(cid:107)2(cid:3)

The second inequality follows from the deﬁnition of ct i.e., ct = ct+1(1 − 1
and Rt speciﬁed in the statement, thus concluding the proof.

n + ηβ + 2η2L2) + η2L3

10 Other Lemmas

The following lemma provides a bound on the variance of the update used in Saga algorithm. More
speciﬁcally, it bounds the quantity E[(cid:107)vt(cid:107)2]. A more general result for bounding the variance of the
minibatch scheme in Algorithm 3 can be proved along similar lines.

Lemma 2. Let vt be computed by Algorithm 1. Then,

Proof. For ease of exposition, we use the notation

Using the convexity of (cid:107)·(cid:107)2 and the deﬁnition of vt we get

E[(cid:107)vt(cid:107)2] ≤ 2E[(cid:107)∇f (xt)(cid:107)2] +

E[(cid:107)xt − αt

i(cid:107)2].

n(cid:88)
ζ t =(cid:0)∇fit(xt) − ∇fit(αt

2L2
n

i=1

)(cid:1)

it

E[(cid:107)vt(cid:107)2] = E[(cid:107)ζ t + 1

n

∇f (αt

i)(cid:107)2]

n(cid:88)

i) − ∇f (xt) + ∇f (xt)(cid:107)2]

n

∇f (αt

= E[(cid:107)ζ t + 1
≤ 2E[(cid:107)∇f (xt)(cid:107)2] + 2E[(cid:107)ζ t − E[ζ t](cid:107)2]
≤ 2E[(cid:107)∇f (xt)(cid:107)2] + 2E[(cid:107)ζ t(cid:107)2].

i=1

n(cid:88)

i=1

13

(cid:80)n
i=1 ∇f (αt

The ﬁrst inequality follows from the fact that (cid:107)a + b(cid:107)2 ≤ 2((cid:107)a(cid:107)2 + (cid:107)b(cid:107)2) and that E[ζ t] = ∇f (xt) −
i). The second inequality is obtained by noting that for a random variable ζ, E[(cid:107)ζ −
E[ζ](cid:107)2] ≤ E[(cid:107)ζ(cid:107)2]. Using Jensen’s inequality in the inequality above, we get

1
n

E[(cid:107)vt(cid:107)2]
≤ 2E[(cid:107)∇f (xt)(cid:107)2] +

2
n

≤ 2E[(cid:107)∇f (xt)(cid:107)2] +

n(cid:88)
E[(cid:107)∇fit(xt) − ∇fit(αt
n(cid:88)

E[(cid:107)xt − αt

i(cid:107)2].

i)(cid:107)2]

i=1

2L2
n

i=1

The last inequality follows from L-smoothness of fit, thus concluding the proof.

The following result provides a bound on the function value of functions with Lipschitz continuous

gradients.
Lemma 3. Suppose the function f : Rd → R is L-smooth, then the following holds

f (x) ≤ f (y) + (cid:104)∇f (y), x − y(cid:105) +

(cid:107)x − y(cid:107)2,

L
2

for all x, y ∈ Rd.

14

