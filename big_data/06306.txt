Distributed Semi-Stochastic Optimization with Quantization Reﬁnement

Neil McGlohon and Stacy Patterson

6
1
0
2

 
r
a

 

M
1
2

 
 
]

.

C
O
h
t
a
m

[
 
 

1
v
6
0
3
6
0

.

3
0
6
1
:
v
i
X
r
a

Abstract— We consider the problem of regularized regression
in a network of communication-constrained devices. Each
node has local data and objectives, and the goal is for the
nodes to optimize a global objective. We develop a distributed
optimization algorithm that is based on recent work on semi-
stochastic proximal gradient methods. Our algorithm employs
iteratively reﬁned quantization to limit message size. We present
theoretical analysis and conditions for the algorithm to achieve
a linear convergence rate. Finally, we demonstrate the perfor-
mance of our algorithm through numerical simulations.

I. INTRODUCTION

We consider the problem of distributed optimization in a
network where communication is constrained, for example a
wireless sensor network. In particular, we focus on problems
where each node has local data and objectives, and the goal
is for the nodes to learn a global objective that includes this
local information. Such problems arise in networked systems
problems such as estimation, prediction, resource allocation,
and control.

Recent works have proposed distributed optimization
methods that reduce communication by using quantization.
For example, in [1], the authors propose a distributed algo-
rithm to solve unconstrained problems based on a centralized
inexact proximal gradient method [2]. In [3], the authors
extend their work to constrained optimization problems. In
these algorithms, the nodes compute a full gradient step in
each iteration, requiring quantized communication between
every pair of neighboring nodes. Quantization has been
applied in distributed consensus algorithms [4], [5], [6] and
distributed subgradient methods [7].

In this work, we address the speciﬁc problem of distributed
regression with regularization over the variables across all
nodes. Applications of our approach include distributed
compressed sensing, LASSO, group LASSO, and regression
with Elastic Net regularization, among others. Our approach
is inspired by [1], [3]. We seek to further reduce per-iteration
communication by using an approach based on a stochastic
proximal gradient algorithm. This approach only requires
communication between a small subset of nodes in each
iteration. In general, stochastic gradients may suffer from
slow convergence. Thus any per-iteration communication
savings could be counter-acted by an extended number of
iterations. Recently, however, several works have proposed
semi-stochastic gradient methods [8], [9], [10]. To reduce the
variance of the iterates generated by a stochastic approach,
these algorithms periodically incorporate a full gradient

*This work was funded in part by NSF grants 1553340 and 1527287.
N. McGlohon and S. Patterson are with the Department of Com-
puter Science, Rensselaer Polytechnic Institute, Troy, NY 12180, USA
mcglon@rpi.edu, sep@cs.rpi.edu

computation. It has been shown that these algorithms achieve
a linear rate of convergence to the optimal solution.

We propose a distributed algorithm for regularized regres-
sion based on the centralized semi-stochastic proximal gra-
dient of [10]. In most iterations, only a subset of nodes need
communicate. We further reduce communication overhead
by employing quantized messaging. Our approach reduces
both the length of messages sent between nodes as well
as the number of messages sent in total to converge to the
optimal solution. The detailed contributions of our work are
as follows:

• We extend the centralized semi-stochastic proximal
gradient algorithm to include errors in the gradient
computations and show the convergence rate of this
inexact algorithm.

• We propose a distributed optimization algorithm based
on this centralized algorithm that uses iteratively reﬁned
quantization to limit message size.

• We show that our distributed algorithm is equivalent to
the centralized algorithm, where the errors introduced
by quantization can be interpreted as inexact gradient
computations. We further design quantizers that guar-
antees a linear convergence rate to the optimal solution.
• We demonstrate the performance of the proposed algo-

rithm in numerical simulations.

The remainder of this paper is organized as follows.
In Section II, we present the centralized inexact proximal
gradient algorithm and give background on quantization.
In Section III, we give the system model and problem
formulation. Section IV details our distributed algorithm.
Section V provides theoretical analysis of our proposed
algorithm. Section VI presents our simulation results, and
we conclude in Section VII.

II. PRELIMINARIES

A. Inexact Semi-Stochastic Proximal Gradient Algorithm
We consider an optimization problem over the form:

G(x) = F (x) + R(x),

(1)

minimize

x∈RP

(cid:80)N

i=1 fi(x), and the following assump-

where F (x) = 1
N
tions are satisﬁed.
Assumption 1: Each fi(x) is differentiable, and its gradi-
ent ∇fi(x) is Lipschitz continuous with constant Li, i.e., for
all x, y ∈ RP ,

(cid:107)∇fi(x) − ∇fi(y)(cid:107) ≤ Li(cid:107)x − y(cid:107).

(2)

Algorithm 1 Inexact Prox-SVRG.

Initialize: ˜x(s) = 0
for s = 0, 1, 2, . . . do
˜g(s) = ∇F (˜x(s))
x(s0) = ˜x(s)
for t = 0, 1, 2, . . . , T − 1 do
Choose (cid:96) uniformly at random from {1, . . . , N}.
v(st) = ∇f(cid:96)(x(st)) − ∇f(cid:96)(˜x(s)) + ˜g(s) + e(st)
x(st+1) = proxηR(˜x(st) − ηv(st))
end for
˜x(s+1) = 1
T

(cid:80)T

t=1 ˜x(st)

end for

Assumption 2: The function R(x) is lower semicontin-
uous, convex, and its effective domain, dom(R) := {x ∈
RP | R(x) < +∞}, is closed.
Assumption 3: The function G(x) is strongly convex with
parameter µ > 0, i.e., for all x, y ∈ dom(R) and for all
ξ ∈ ∂G(x),

G(x) − G(y) − 1

2 µ(cid:107)x − y(cid:107)2 ≥ ξT(x − y),

(3)

where ∂G(x) is the subdifferential of G at x. This strong
convexity may come from either F (x) or R(x) (or both).
Problem (1) can be solved using a stochastic proximal
gradient algorithm [11] where, in each iteration, a single ∇f(cid:96)
is computed for a randomly chosen (cid:96) ∈ {1, . . . , N}, and the
iterate is updated accordingly as,

x(t+1) = proxηR(x(t) − η(t)∇f(cid:96)(x(t))).

Here, proxηR(·) is the proximal operator

proxηR(v) = arg min

y∈Rp

1
2

(cid:107)y − v(cid:107)2 + ηR(y).

While stochastic methods offer the beneﬁt of reduced per-
iteration computation over standard gradient methods, the
iterates may have high variance. These methods typically use
a decreasing step-size η(t) to compensate for this variance,
resulting in slow convergence. Recently, Xiao and Zhang pro-
posed a semi-stochastic proximal gradient algorithm, Prox-
SVRG that reduces the variance by periodically incorporat-
ing a full gradient computation [10]. This modiﬁcation allows
Prox-SVRG to use a constant step size, and thus, Prox-SVRG
achieves a linear convergence rate.

We extend Prox-SVRG to include a zero-mean error in the
gradient computation. Our resulting algorithm, Inexact Prox-
SVRG, is given in Algorithm 1. The algorithm consists of
an outer loop where the full gradient is computed and an
inner loop where the iterate is updated based on both the
stochastic and full gradients.

Algorithm 1.

The following theorem states the convergence behavior of
Theorem 1: Let {˜x(s)}s≥0 be the sequence generated by
, where L = maxi Li.
Algorithm 1, with 0 < η < 1
4L
Assume that the functions R, G, and fi, i = 1, . . . , N,
satisfy Assumptions 1, 2, and 3, and that the errors e(st)
are zero-mean and uncorrelated with the iterates x(st) and

their gradients ∇fi(x(st)). Let x(cid:63) = arg minx G(x), and let
T be such that,

1

α =

µη(1 − 4Lη)T

+

4Lη(T + 1)
(1 − 4Lη)T

< 1.

Then,

(cid:104)

E

(cid:105)

G(˜x(s)) − G(x(cid:63))

(cid:32)

≤ αs

(cid:33)

s(cid:88)
t=0 E(cid:107)e(it)(cid:107)2.

i=1

α−iΓ(i)

G(˜x(0)) − G(x(cid:63)) + β

and Γ(i) =(cid:80)T−1

where β =
The proof is given in the appendix.

T (1−4Lη)

η

From this theorem, we can derive conditions for the
algorithm to converge to the optimal x(cid:63). Let the sequence
{Γ(s)}s≥0 decrease linearly at a rate κ. Then

1) If κ < α, then E(cid:2)G(˜x(s)) − G(x(cid:63))(cid:3) converges linearly
2) If α < κ < 1, then E(cid:2)G(˜x(s)) − G(x(cid:63))(cid:3) converges
3) If κ = α, then E(cid:2)G(˜x(s)) − G(x(cid:63))(cid:3) converges linearly

linearly with a rate of κ.

with a rate of α.

with a rate in O(sαs).

B. Subtractively Dithered Quantization

We employ a subtractively dithered quantizer to quantize
values before transmission. We use a substractively dithered
quantizer rather than non-subtractively dithered quantizer
because the quantization error of the subtractively dithered
quantizer is not correlated with its input. We brieﬂy summa-
rize the quantizer and its key properties below.

Let z be real number to be quantized into n bits. The
quantizer is parameterized by an interval size U and a
midpoint value z ∈ R. Thus the quantization interval
is [z − U/2, z + U/2], and the quantization step-size is
∆ = U

2n−1. We ﬁrst deﬁne the uniform quantizer,
1
2

q(z) (cid:44) z + sgn(z − z) · ∆ ·

∆

+

(cid:22)|z − z|

.

(4)

(cid:23)

In subtractively dithered quantization, a dither ν is added to
z, the resulting value is quantized using a uniform quantizer,
and then transmitted. The recipient then subtracts ν from
this value. The subtractively dithered quantized value of z,
denoted ˆz, is thus

ˆz = Q(z) (cid:44) q(z + ν) − ν.

(5)

Note that this quantizer requires both the sender and recipient
to use the same value for ν, for example, by using the same
pseudorandom number generator.

The following theorem describes the statistical properties
of the quantization error.
Theorem 2 (See [12]): Let z ∈ [z − U/2, z + U/2] and
ˆz = Q(z), for Q(·) in (5). Further, let ν is a real number
drawn uniformly at random from the interval (−∆/2, ∆/2).
The quantization error ε(z) (cid:44) z − ˆz satisﬁes the following:

2) E(cid:2)ε(z)2(cid:3) = E(cid:2)ν2(cid:3) = ∆2

1) E [ε(z)] = E [ν] = 0.

12

3) E [zε(z)] = E [z] E [ε(z)] = 0
4) For z1 and z2 in the interval

E [ε(z1)ε(z2)] = E [ε(z1)] E [ε(z2)] = 0.

[z − U/2, z + U/2],

With some abuse of notation, we also write Q(v) where v
is a vector. In this case, the quantization operator is applied
to each component of v independently, using a vector-valued
midpoint and the same scalar-valued interval bounds.

III. PROBLEM FORMULATION

We consider a similar system model to that in [1]. The
network is a connected graph of N nodes where inter-node
communication is limited to the local neighborhood of each
node. The neighbor set Ni consists of node i’s neighbors and
itself. The neighborhoods exist corresponding to the ﬁxed
undirected graph G = (V,E). We denote D as the maximum
degree of the graph G.

1 xT

2 . . . xT

Each node i has a state vector xi with dimension mi. The
N ]T. We let xNi be the
state of the system is x = [xT
vector consisting of the concatenation of states of all nodes in
Ni. For ease of exposition, we deﬁne the selecting matrices
Ai, i = 1, . . . , N, where xNi = Aix and the matrices Bij,
i, j = 1, . . . , N where xj = BijxNi. These matrices each
have (cid:96)2-norm of 1.
in Ni. The distributed optimization problem is thus,

Every node i has a local objective function over the states

G(x) = F (x) + R(x),

(6)

i=1 fi(xNi). We assume that Assump-
where F (x) = 1
N
tions 1 and 3 are satisﬁed. Further, we require the following
assumptions hold.
Assumption 4: For all i, ∇fi(xNi) is linear or constant.
This implies that, for a zero-mean random variable ν,
E [∇fi(xNi + ν)] = ∇fi(xNi).

Assumption 5: The proximal operation proxR(x) can be

performed by each node locally, i.e.,

proxR(x) = [proxR(x1)T proxR(x2)T . . . proxR(xN )T]T.
We note that Assumption 5 holds for standard regularization
functions used in LASSO ((cid:107)x(cid:107)1), group LASSO where each
xi its own group, and Elastic Net regularization (λ1(cid:107)x(cid:107)1 +
2 (cid:107)x(cid:107)2
2).
In the next section, we present our distributed implemen-
tation of Prox-SVRG to solve Problem (6).

λ2

IV. ALGORITHM

i

i

and sends it

Our distributed algorithm is given in Algorithm 2. In
each outer iteration s, node i quantizes its iterate ˜x(s)
and
the gradient ∇f (s)
to all of its neighbors.
These values are quantized using two subtractively dithered
quantizers, Q(s)
b,i , whereby the sender (node i) sends
an n bit representation and the recipient reconstructs the
value from this representation and subtracts the dither. The
midpoints for Q(s)
b,i are set to be the quantized values
from the previous iteration. Thus,
the recipients already
know these midpoints. The quantized values (after the dither

a,i and Q(s)

a,i and Q(s)

minimize

x∈RP

(cid:80)N

i

i

i

,

= 0

i = 0, ˆ˜x(−1)

a,i = ˆ˜x(s−1)
5:
b,i = ˆ∇f (s−1)
6: Quantize local variable and send to all j ∈ Ni:
i + a(s)
) = ˜x(s)
)

Algorithm 2 Inexact Semi-stochastic Gradient Descent as
executed by node i
1: Parameters: inner loop size T , step size η
= 0, ˆ∇f (−1)
2: Initialize: ˜x(0)
3: for s = 0, 1, . . . do
4: Update quantizer parameters:
U (s)
a,i = Caκ(s+1)/2, x(s)
b,i = Cbκ(s+1)/2, ∇f (s)
U (s)
i = Q(s)
a,i(˜x(s)
ˆ˜x(s)
Compute: ∇f (s)
i = ∇fi(ˆ˜x(s)Ni
(cid:80)
b,i (∇f (s)
ˆ∇f (s)
i = Q(s)
i = 1
9:
j∈Ni
N
ij = −Bij ˆ∇f (s)
10:
11: Update quantizer parameters:
c,i = Ccκ(s+1)/2, x(s)
U (s)
d,i = Cdκ(s+1)/2, ∇f (s)
U (s)
= ˜x(s)

7:
8: Quantize gradient and send to all j ∈ Ni:

c,i = ˆ˜x(s)
d,i = ˆ∇f (s)

Compute: ˜h(s)
Compute: v(s)

for all j ∈ Ni

) = ∇f (s)

Bij ˆ∇f (s)

j + ˜h(s)

i + b(s)

12:

,

j

i

i

i

i

i

i

i

i

i

x(s0)
for t = 0, 1, . . . , T − 1 do
i
Randomly pick (cid:96) ∈ {1, 2, 3, . . . , N}
if i ∈ N(cid:96) then
Quantize local variable and send to (cid:96):
) = x(st)

i

i + c(st)

ˆx(st)
i = Q(st)
if i = (cid:96) then
Compute: ∇f (st)
)
Quantize gradient and send to all j ∈ Ni:
) = ∇f (st)

c,i (x(st)
= ∇fi(ˆx(st)Ni
d,i (∇f (st)

ˆ∇f (st)

= Q(st)

i

i

i

i

end if
Update local variable:

i + d(st)

i

x

(st+1)
i

= proxηR(x(st)

i − η(Bi(cid:96)

ˆ∇f (st)

(cid:96) + v(s)

i(cid:96) ))

else

Update local variable:

13:
14:
15:
16:
17:

18:
19:
20:

21:
22:
23:

24:
25:
26:

x

(st+1)
i

= proxηR(x(st) − η˜h(s)

i

)

(cid:80)T

= 1
T

t=1 x(st)

end if
27:
end for
28:
˜x(s+1)
29:
i
30: end for

is subtracted) are denoted by ˆ˜x(s)
i
quantization errors are a(s)
and b(s)

i

i

and ˆ∇f (s)
, respectively.

i

, and the

i

and gradients ∇f (st)

For every iteration s of the outer loop of the algorithm,
there is an inner loop of T iterations. In each inner iteration,
a single node (cid:96), chosen at random, computes its gradient.
To do this, node (cid:96) and its neighbors exchange their states
x(st)
. These values are quantized using
i
two subtractively dithered quantizers, Q(st)
d,i . The
c,i
midpoints for these quantizers are ˆ˜x(s)
. Each
node sends these values to their neighbors before the inner
loop, so all nodes are aware of the midpoints. The quantized
values (after the dither is subtracted) are denoted by ˆx(st)
and ˆ∇f (st)
and d(st)
,
respectively. The quantization interval bounds U (s)
a,i , U (s)
b,i ,
U (s)
c,i , and U (s)
d,i , are initialized to Ca, Cb, Cc, and Cd,
respectively, and each iteration, the bounds are multiplied
by κ1/2. Thus the quantizers are reﬁned in each iteration.

, and their quantization errors are c(st)

and Q(st)
and ˆ∇f (s)

i

i

i

i

i

The quantizers limit the length of a single variable trans-
mission to n bits. In the outer loop of the algorithm, each

node i sends its local variable, consisting of mi quantized
components, to every neighbor. It also sends its gradient,
consisting of |Ni|mi quantized components to every neigh-
bor. Thus the number of bits exchanged by all nodes is
i=1 |Ni|mi + |Ni|2mi bits. In each inner iteration, only
nodes j ∈ N(cid:96) exchange messages. Each node j quantizes
mj state variables and sends them to node (cid:96). This yields a
mj bits in total. In turn, node (cid:96)
quantizes its gradient and sends it to all of its neighbors,
which is n|N(cid:96)|2m(cid:96) total bits. Thus, in each inner iteration
mj) bits are transmitted. The total
number of bits transmitted in a single outer iteration is
therefore,

n(cid:80)N
transmission of n(cid:80)
n(|N(cid:96)|2m(cid:96) + (cid:80)
 N(cid:88)

|N(cid:96)|2m(cid:96) +

(|Ni|mi(1 + |Ni|)) +

T−1(cid:88)

(cid:88)

j∈N(cid:96)

j∈N(cid:96)

mj

n

 .

i=1

t=0

j∈N(cid:96)

Let D = maxi |Ni| and m = maxi mi. An upper bound
on the number bits transmitted by the algorithm in each outer
iteration is nm(N + T )(D + D2).

V. ALGORITHM ANALYSIS

We now present our analysis of Algorithm 2. First we
show that the algorithm is equivalent to Algorithm 1, where
the quantization errors are encapsulated in the error term
e(st). We also give an explicit expression for this error term.

Lemma 1: Algorithm 2 is equivalent to the Inexact Prox-

j∈N(cid:96)
+ E(cid:107)d(st)

E(cid:107)b(s)

i (cid:107)2.

T ˆ∇f (s)

i

(cid:17)

(cid:96)

(cid:96)

2
N 2

e(st) = A(cid:96)

(cid:107)2 + 2E(cid:107)b(s)

Proof: The error e(st) is:
ˆ∇f (s)
) − A(cid:96)

−(cid:16)A(cid:96)
(cid:16)∇f(cid:96)(ˆx(st)N(cid:96)
(cid:16)∇f(cid:96)(ˆ˜x(s)N(cid:96)
(cid:80)N
i=1 AT

(cid:96) (cid:107)2 +
(cid:80)N
i=1 Ai
T ˆ∇f (st)
(cid:96) − AT
(cid:96) + 1
N
(cid:80)N
T∇f(cid:96)(˜x(s)N(cid:96)
T∇f(cid:96)(x(st)N(cid:96)
)
(cid:17)
i=1 Ai
+ 1
N
(cid:17) − AT
) − ∇f(cid:96)(x(st)N(cid:96)
)
(cid:16)∇fi(ˆ˜x(s)Ni
) − ∇f(cid:96)(˜x(s)N(cid:96)

T∇fi(˜x(s)Ni
+ AT
(cid:96) d(st)
(cid:17)
(cid:96) b(s)
random variable δ. Therefore, E(cid:2)e(st)(cid:3) = 0.

) − ∇fi(˜x(s)Ni

= AT
− AT

+ 1
N

)

(cid:96)

(cid:96)

(cid:96)

(cid:96)

i

)

i b(s)
We note that all quantization errors are zero-mean. Further,
by Assumption 5, E [∇fi(x + δ)] = ∇fi(x), for a zero-mean

+ 1
N

)

i

.

(cid:80)N
i=1 AT

(cid:17)
(cid:17) − AT

)

SVG method in Algorithm 1, with
e(st) = AT
) − ∇f(cid:96)(x(st)N(cid:96)

− AT

(cid:96)

) − ∇f(cid:96)(˜x(s)N(cid:96)

)

(cid:96)

(cid:16)∇f(cid:96)(ˆx(st)N(cid:96)
(cid:16)∇f(cid:96)(ˆ˜x(s)N(cid:96)
(cid:16)∇fi(ˆ˜x(s)Ni
(cid:80)N
i=1 AT
2 (cid:88)

i

+ 1
N

) − ∇fi(˜x(s)Ni
Further, E(cid:107)e(st)(cid:107)2 is upper-bounded by,
(cid:107)2 + 2L
E(cid:107)e(st)(cid:107)2 ≤ 2L

E(cid:107)c(st)

j

(cid:80)N
i=1 AT

i b(s)

i

.

(cid:96) d(st)

(cid:96)

(cid:96)

)

+ AT
(cid:96) b(s)

+ 1
N

(cid:17)
2 (cid:88)
N(cid:88)

j∈N(cid:96)

i=1

E(cid:107)a(s)
j (cid:107)2

i

(cid:96)

(cid:96)

, and b(s)

We now show that e(st) is is uncorrelated with x(st) and
the gradients ∇f(cid:96)(x(st)N(cid:96)
), (cid:96) = 1, . . . , N. Clearly, x(st) and
∇f(cid:96)(x(st)N(cid:96)
) are uncorrelated with the terms of e(st) contain-
ing d(st)
, b(s)
. In accordance with Assumption 5,
the gradients ∇f(cid:96) and ∇fi are either linear or constant.
If they are constant, then ∇f(cid:96)(ˆx(st)N(cid:96)
) = 0
and ∇fi(ˆ˜x(s)Ni
) = 0. Thus, the terms in e(st)
containing these differences are also 0. If they are linear,
e.g., ∇f(cid:96)(z) = Hz + h, for an appropriately sized, matrix
H and vector h (possibly 0). Then,
∇f(cid:96)(ˆx(st)N(cid:96)

) − ∇f(cid:96)(x(st)N(cid:96)

) − ∇fi(˜x(s)Ni

)
) + h) − (Hx(st) + h) = Hc(st)

) − ∇f(cid:96)(x(st)N(cid:96)
+ c(st)

.

= (H(x(st)N(cid:96)

i

i
is uncorrelated with x(st). It is clearly
). Similar arguments can be
) are uncorrelated with

i

By Theorem 2, c(st)
also uncorrelated with ∇f(cid:96)(x(st)N(cid:96)
used to show that x(st) and ∇f(cid:96)(x(st)N(cid:96)
the remaining terms in e(st).
With respect to E(cid:107)e(st)(cid:107)2, we have
E(cid:107)e(st)(cid:107)2 = E(cid:107)AT
− AT

(cid:16)∇f(cid:96)(ˆx(st)N(cid:96)
(cid:16)∇f(cid:96)(ˆ˜x(s)N(cid:96)
(cid:17)
) − ∇f(cid:96)(x(st)N(cid:96)
(cid:16)∇fi(ˆ˜x(s)Ni
(cid:80)N
) − ∇f(cid:96)(˜x(s)N(cid:96)
(cid:80)N
i=1 AT
i=1 AT
(cid:96) d(st)
(cid:96) + 1
N

+ 1
N
+ E(cid:107)AT

)

(cid:96)

(cid:96)

i

(cid:17)

)

) − ∇fi(˜x(s)Ni
)
i − AT
(cid:96) (cid:107)2.
(cid:96) b(s)
i b(s)

(cid:17)(cid:107)2

The ﬁrst term on the right hand side can be bounded using
the fact that (cid:107)a + b(cid:107)2 ≤ 2(cid:107)a(cid:107)2 + 2(cid:107)b(cid:107)2, as

(cid:17)(cid:107)2
(cid:17)
) − ∇fi(˜x(s)Ni

)

(cid:17)(cid:107)2.

)

≤ 2E(cid:107)AT

) − ∇f(cid:96)(x(st)N(cid:96)

)

(cid:96)

(cid:96)

+ 2E(cid:107)AT

(cid:16)∇f(cid:96)(ˆx(st)N(cid:96)
(cid:16)∇f(cid:96)(ˆ˜x(s)N(cid:96)
(cid:80)N
i=1 AT
(cid:16)∇f(cid:96)(ˆx(st)N(cid:96)

+ 1
N

i

) − ∇f(cid:96)(˜x(s)N(cid:96)

(cid:16)∇fi(ˆ˜x(s)Ni
(cid:17)(cid:107)2

)

We now bound the ﬁrst term in this expression,

2E(cid:107)AT

(cid:96)

) − ∇f(cid:96)(x(st)N(cid:96)
− x(st)N(cid:96)

(cid:107)2) ≤ 2L

≤ 2E(L2

i(cid:107)ˆx(st)N(cid:96)

2 (cid:88)

j∈N(cid:96)

E(cid:107)c(st)

j

(cid:107)2,

where the ﬁrst inequality follows from Assumptions 1 and 5
and the fact that (cid:107)A(cid:96)(cid:107) = 1. The second inequality follows
from the independence of quantization errors (Theorem 2).
Next we bound the second term,

2E(cid:107)AT

(cid:96)

i

)

) − ∇f(cid:96)(˜x(s)N(cid:96)

(cid:17)
) − ∇fi(˜x(s)Ni

(cid:16)∇f(cid:96)(ˆ˜x(s)N(cid:96)
(cid:16)∇fi(, ˆ˜x(s)Ni
(cid:80)N
(cid:16)∇f(cid:96)(ˆ˜x(s)N(cid:96)
i=1 AT
(cid:16)∇f(cid:96)(ˆ˜x(s)N(cid:96)
(cid:104)AT
(cid:16)∇f(cid:96)(ˆ˜x(s)N(cid:96)
2(cid:80)
i(cid:107)(ˆ˜x(s)N(cid:96)
j∈N(cid:96)

(cid:17)(cid:107)2
(cid:17)
(cid:17)(cid:105)(cid:107)2
) − ∇f(cid:96)(˜x(s)N(cid:96)
(cid:17)(cid:107)2
) − ∇f(cid:96)(˜x(s)N(cid:96)
) − ∇f(cid:96)(˜x(s)N(cid:96)
− ˜x(s)N(cid:96)
(cid:107)2)
j (cid:107)2,
E(cid:107)a(s)

)

)

)

)

(cid:96)

(cid:96)

+ 1
N
= 2E(cid:107)AT

− E

(cid:96)

≤ 2E(cid:107)AT
≤ 2E(L2
≤ 2L

where the ﬁrst inequality uses the fact that for a random
variable υ, E(cid:107)υ − Eυ(cid:107)2 = E(cid:107)υ(cid:107)2 − (cid:107)Eυ(cid:107)2 ≤ E(cid:107)υ(cid:107)2. The
remaining inequalities follow from Assumptions 1 and 5, the
fact that (cid:107)A(cid:96)(cid:107) = 1, and the independence of the quantization
errors.

Finally, again from the independence of the quantization

errors, we have,
E(cid:107)AT
(cid:96) d(st)
≤ E(cid:107)AT
≤ E(cid:107)d(st)

(cid:96) + 1
N
(cid:96) d(st)

(cid:96)

N

i b(s)

(cid:80)N
(cid:80)N
i − AT
i=1 AT
(cid:96) (cid:107)2
(cid:96) b(s)
(cid:80)N
i (cid:107)2 − AT
i=1 AT
(cid:96) (cid:107)2
(cid:107)2 + E(cid:107) 1
i b(s)
(cid:96) b(s)
(cid:96) (cid:107)2 + 2
i (cid:107)2.
(cid:107)2 + 2E(cid:107)b(s)
i=1 E(cid:107)b(s)
2 (cid:88)
N(cid:88)

2 (cid:88)

E(cid:107)c(st)

j∈N(cid:96)

N 2

(cid:107)2 + 2E(cid:107)b(s)

(cid:96) (cid:107)2 +

j∈N(cid:96)
+ E(cid:107)d(st)

(cid:96)

Combining these bounds, we obtain the desired result,
E(cid:107)e(st)(cid:107)2 ≤ 2L
E(cid:107)a(s)
j (cid:107)2

(cid:107)2 + 2L

j

E(cid:107)b(s)

i (cid:107)2.

(cid:96)

2
N 2

i=1

We next show that, if all of the values fall within their
respective quantization intervals, then the error term Γ(s) de-
creases linearly with rate κ, and thus the algorithm converges
to the optimal solution linearly with rate κ.
Theorem 3: Given p, if for all 1 ≤ s ≤ (p − 1), the
fall inside of the
c,i , and Q(st)
d,i ,

values of ˜x(s)
respective quantization intervals Q(s)
then Γ(k) ≤ Cκk, where,

, x(st), and ∇f (st)
a,i, Q(s)

b,i , Q(st)

, ∇f (s)

i

i

i

DT m

12(2(cid:96) − 1)2

C =
with D = maxi |Ni| and m = maxi mi.

(Ca + Cb) + 2( N +1

2
2L

It follows that, for α < κ < 1,

N )Cb + Cd

,

(cid:17)

(cid:16)

(cid:105)

(cid:18)

G(˜x(s)) − G(x(cid:63))
≤ κs

G(˜x(0)) − G(x(cid:63)) + βC

(cid:19)(cid:19)

(cid:18) 1

1 − α

κ

Proof: First we note that, by Theorem 2 and the update

rule for the quantization intervals, we have:

(cid:104)

E

12(2(cid:96)−1)2 Cbκs

12(2(cid:96)−1)2 Caκs

12(2(cid:96)−1)2 Ccκs

b,i

c,i

a,i

m

m

2(cid:96)−1

2(cid:96)−1

2(cid:96)−1

(cid:18) U (s)
(cid:19)2 ≤
(cid:19)2 ≤ Dm
(cid:18) U (s)
(cid:18) U (s)
(cid:19)2 ≤
(cid:18) U (s)
(cid:19)2 ≤ Dm
12(2(cid:96)−1)2 Ccκs(cid:17)
(cid:16) Dm
12(2(cid:96)−1)2 Cbκs(cid:17)

12(2(cid:96)−1)2 Cdκs

+ Dm

+ 2
N

2(cid:96)−1

d,i

12(2(cid:96)−1)2 Cdκs.

E(cid:107)a(s)

E(cid:107)b(s)

12

i (cid:107)2 ≤ m
i (cid:107)2 ≤ Dm
(cid:107)2 ≤ m

12

12

E(cid:107)c(st)

i

E(cid:107)e(st)(cid:107)2 ≤ 2L

E(cid:107)d(st)

(cid:107)2 ≤ Dm

i

12

2

m

D

(cid:16)
(cid:16)
12(2(cid:96)−1)2 Caκs(cid:17)
(cid:16) Dm
12(2(cid:96)−1)2 Cbκs(cid:17)
(cid:16)

D m

L

2

+ 2

+ 2

= Dm

12(2(cid:96)−1)2

2
2L

(Ca + Cc) + 2( N +1

N )Cb + Cd

We use these inequalities to bound (cid:107)e(st)(cid:107)2,

(cid:16)

T−1(cid:88)

t=0

(cid:105)

Summing over t = 0, . . . , T − 1, we obtain,

Γ(s) =

E(cid:107)e(st)(cid:107)2 ≤ Cκs,

(cid:17)

(cid:33)

where

(cid:104)

C = DT m

12(2(cid:96)−1)2

2
2L

(Ca + Cc) + 2( N +1

N )Cb + Cd

Applying Theorem 1, with κ > α, we have

G(˜x(s)) − G(x(cid:63))

E

≤ αs(cid:16)
(cid:32)
≤ κs(cid:16)
≤ κs(cid:16)

≤ κs

(cid:17)

i=1

s(cid:88)
s(cid:88)
(cid:16) 1

G(˜x(0)) − G(x(cid:63))

+ β

αs−iCκi

κ−(s−i)αs−i

G(˜x(0)) − G(x(cid:63)) + Cβ
G(˜x(0)) − G(x(cid:63)) + Cβ 1−( α
κ )s
1− α
G(˜x(0)) − G(x(cid:63)) + Cβ

i=1

κ

(cid:17)
(cid:17)(cid:17)

1− α

κ

.

While we do not yet have theoretical guarantees that
all values will fall within their quantization intervals, our
simulations indicate that is always possible to ﬁnd parameters
Ca, Cb, Cc, and Cd, for which all values lie within their
quantization intervals for all iterations. Thus, in practice, our
algorithm achieves a linear convergence rate. We anticipate
that
is possible to develop a programmatic approach,
similar to that in [1], to identify values for Ca, Cb, Cc, and
Cd that guarantee linear convergence. This is a subject of
current work.

it

VI. NUMERICAL EXAMPLE

This section illustrates the performance of Algorithm 2 by
solving a distributed linear regression problem with elastic
net regularization.
We randomly generate a d-regular graph with N = 40
and uniform degree of 8, i.e., ∀i |Ni| = 9. We set each
subsystem size, mi, to be 10. Each node has a local function
fi(xNi) = (cid:107)HixNi − hi(cid:107)2 where Hi is a 80 × 90 random
matrix. We generate hi by ﬁrst generating a random vector x
and then computing hi = Hix. The global objective function
is:

G(x) =

1
N

fi(xNi) + λ1(cid:107)x(cid:107)2 +

(cid:107)x(cid:107)1.

λ2
2

N(cid:88)

i

This simulation was implemented in Matlab and the op-
timal value x(cid:63) was computed using CVX. We set the total
number of inner iterations to be T = 2N and use the step
size η = 0.1/L. With these values, α < 1, as required by
Theorem 1. We set κ = 0.97, which ensures that κ > α. We
use the quantization parameters Ca = 50, Cb = 300, Cc =
50, Cd = 400. With these parameters, the algorithms values
always fell within their quantization intervals.

Fig. 1 shows the performance of the algorithm where the
number of bits n is 11, 13, and 15, as well as the performance
of the algorithm without quantization. In these results, x(s)

(cid:17)

κs.

convex, then µF (µR) is its strong convexity parameter; if
F (x) (R(x)) is only convex, then µF (µR) is 0. For any
x ∈ dom(R) and any v ∈ RP , deﬁne,

x+ = proxηR(x − ηv)
η (x − x+)
h = 1
∆ = v − ∇F (x),

where 0 < η < 1

L. Then, for any y ∈ RP ,

G(y) ≥ G(x+) + hT(y − x) + η

2(cid:107)h(cid:107)2 + µF
2 (cid:107)y − x+(cid:107)2 + ∆T(x+ − y).

+ µR

2 (cid:107)y − x(cid:107)2

We now proceed to prove Theorem 1. For brevity, we
omit some details that are identical to those in the proof
of Theorem 3.1 in [10]. We have indicated these omissions
below.

Proof: First, we deﬁne

h(st) = 1
= 1

η (x(st−1) − x(st))
η (x(st−1) − proxηR(x(st−1) − ηv(st−1))),

where v(st−1) is as deﬁned in Algorithm 1.

We analyze the change in the distance between x(st) and

x(cid:63) in a single inner iteration,
(cid:107)x(st) − x(cid:63)(cid:107)2 = (cid:107)x(st−1) − ηh(st) − x(cid:63)(cid:107)2
= (cid:107)x(st−1)x(cid:63)(cid:107)2 − 2ηh(st)T

(x(st−1) − x(cid:63)) + η2(cid:107)h(st)(cid:107)2.
We next apply Lemma 3, with x = x(st−1), x+ = x(st),
h = h(st), v = v(st−1), and y = x(cid:63), to obtain,

− h(st)T

2(cid:107)h(st)(cid:107)2
(x(st−1) − x(cid:63)) + η
≤ G(x(cid:63)) − G(x(st)) − µF

− µR

2 (cid:107)x(st) − x(cid:63)(cid:107)2 − ∆(st)T

2 (cid:107)x(st−1) − x(cid:63)(cid:107)2
(x(st) − x(cid:63)),

where ∆(st) = v(st−1) −∇F (x(st−1)) = w(st−1) + e(st−1) −
∇F (x(st−1)). This implies,
(cid:107)x(st) − x(cid:63)(cid:107)2 ≤ (cid:107)x(st−1) − x(cid:63)(cid:107)2 − 2η(G(x(st)) − G(x(cid:63)))

− 2η∆(st)T

(x(st) − x(cid:63)).

We follow the same reasoning as in the proof of Theorem
3.1 in [10] to obtain the following expression, which is
conditioned on x(st−1) and takes expectation with respect
to (cid:96),

E(cid:107)x(st) − x(cid:63)(cid:107)2 ≤ (cid:107)x(st−1) − x(cid:63)(cid:107)2

− 2ηE(G(x(st)) − G(x(cid:63))) + 2η2E(cid:107)∆(st)(cid:107)2
− 2ηE

(x(st) − x(cid:63))

∆(st)T

,

(cid:104)

(cid:105)

where

x(st) = proxηR(x(st−1) − η∇F (x(st−1))).

Since (cid:96) and e(st−1) are independent of x(st) and x(cid:63), and

(cid:104)

since e(st−1) is zero-mean,
(x(st) − x(cid:63))

∆(st)T

E

(cid:105)

= (E∆(st))T(x(st) − x(cid:63)) = 0.

Fig. 1: Comparison of the performance of Algorithm 2
with differing quantized message lengths and that with no
quantization applied.

is the concatenation of the ˜x(s)
vectors, for i = 1 . . . N. It
is important to note the rate of convergence of the algorithm
in all four cases is linear, and, performance improves as the
number of bits increases.

i

VII. CONCLUSION

We have presented a distributed algorithm for regular-
ized regression in communication-constrained networks. This
algorithm is based on recently proposed semi-stochastic
proximal gradient methods. Our algorithm reduces commu-
nication requirements by (1) using a stochastic approach
where only a subset of nodes communicate in each iteration
and (2) quantizing all messages. We have shown that this
distributed algorithm is equivalent to a centralized version
with inexact gradient computations, and we have used this
equivalence to analyze the convergence rate of the distributed
method. Finally, we have demonstrated the performance of
our algorithm in numerical simulations.

In future work, we plan to extend our theoretical analysis
to develop a programmatic way to identify initial quantiza-
tion intervals. We also plan to explore the integration of more
complex regularization functions.

Proof of Theorem 1

APPENDIX

We ﬁrst restate some useful results from [10].
Lemma 2: Let

w(st) = ∇f(cid:96)(x(st−1)) − ∇f(cid:96)(˜x(s)) + ∇F (˜x(s)).

Then, conditioned on x(st−1), E(cid:2)w(st)(cid:3) = ∇F (x(st−1)) and

E(cid:107)w(st) − ∇F (x(st−1))(cid:107)2

≤ 4L(G(x(st−1)) − G(x(cid:63)) + G(˜x(s)) − G(x(cid:63))),

Lemma 3: Let G(x) = F (x) + R(x), where G is strongly
convex, ∇F (x) is Lipschitz continuous with parameter L.
Further let F (x) and R(x) have convexity parameters µF and
µR, respectively. In other words, if F (x) (R(x)) is strongly

Iterations050100150200250300jjx(s)!x?jj10!1510!1010!5100105n=11n=13n=15NoErrorsince e(st−1)

Further,
∇F (x(st−1)),
E(cid:107)∆(st)(cid:107)2 = E(cid:107)w(st−1) − ∇F (x(st−1))(cid:107)2 + E(cid:107)e(st−1)(cid:107)2

independent of w(st−1) and

is

Applying Lemma 2, we obtain,
E(cid:107)x(st) − x(cid:63)(cid:107)2 ≤ (cid:107)x(st−1) − x(cid:63)(cid:107)2 − 2ηE(G(x(st)) − G(x(cid:63)))

+ 8Lη2(G(x(st−1)) − G(x(cid:63)) + G(˜x(s)) − G(x(cid:63)))
+ 2η2E(cid:107)e(st−1)(cid:107)2

(cid:80)T

We consider a single execution of the inner iteration of
the algorithm, so x(s0) = ˜x(s) and ˜x(s+1) = 1
t=1 x(st).
T
Summing over t = 1, . . . , T on both sides gives and taking
expectation over (cid:96), for t = 1, . . . , T gives us,

E(cid:107)x(sT ) − x(cid:63)(cid:107)2 + 2ηE(G(x(sT )) − G(x(cid:63)))
+ 2η(1 − 4Lη)
E(G(x(st)) − G(x(cid:63)))

T−1(cid:88)

t=1

≤ (cid:107)x(s0) − x(cid:63)(cid:107)2 + 8Lη2(G(x(s0)) − G(x(cid:63))

+ T (G(˜x(s)) − G(x(cid:63))) + 2η2Γ(s).

Following the same reasoning as in [10], we obtain,
E(G(˜x(s+1)) − G(x(cid:63))) ≤ αE

G(˜x(s) − G(x(cid:63))

+ βΓ(s).

(cid:104)

(cid:105)

Applying this bound recursively, we obtain the expression in
our theorem.

REFERENCES

[1] Y. Pu, M. N. Zeilinger, and C. N. Jones, “Quantization design for
unconstrained distributed optimization,” in American Control Confer-
ence, July 2015, pp. 1229–1234.

[2] M. Schmidt, N. L. Roux, and F. R. Bach, “Convergence rates of inexact
proximal-gradient methods for convex optimization,” in Advances in
Neural Information Processing Systems, J. Shawe-Taylor, R. Zemel,
P. Bartlett, F. Pereira, and K. Weinberger, Eds., 2011, pp. 1458–1466.
[3] Y. Pu, M. N. Zeilinger, and C. N. Jones, “Quantization design for

distributed optimization,” arXiv preprint arXiv:1504.02317, 2015.

[4] A. Kashyap, T. Bas¸ar, and R. Srikant, “Quantized consensus,” Auto-

matica, vol. 43, no. 7, pp. 1192–1203, 2007.

[5] D. Thanou, E. Kokiopoulou, Y. Pu, and P. Frossard, “Distributed
average consensus with quantization reﬁnement,” IEEE Transactions
on Signal Processing, vol. 61, no. 1, pp. 194–205, 2013.

[6] R. Carli, F. Fagnani, P. Frasca, T. Taylor, and S. Zampieri, “Average
consensus on networks with transmission noise or quantization,” in
Proceedings of European Control Conference, 2007, pp. 1852–1857.
[7] A. Nedi´c, A. Olshevsky, A. Ozdaglar, and J. N. Tsitsiklis, “Distributed
subgradient methods and quantization effects,” in 47th IEEE Confer-
ence on Decision and Control, 2008, pp. 4177–4184.

[8] A. Nitanda, “Stochastic proximal gradient descent with acceleration
techniques,” in Advances in Neural Information Processing Systems,
2014, pp. 1574–1582.

[9] R. Johnson and T. Zhang, “Accelerating stochastic gradient descent us-
ing predictive variance reduction,” in Advances in Neural Information
Processing Systems, 2013, pp. 315–323.

[10] L. Xiao and T. Zhang, “A proximal stochastic gradient method
with progressive variance reduction,” SIAM Journal on Optimization,
vol. 24, no. 4, pp. 2057–2075, 2014.

[11] J. Duchi and Y. Singer, “Efﬁcient online and batch learning using
forward backward splitting,” J. Mach. Learn. Res., vol. 10, pp. 2873–
2898, 2009.

[12] S. P. Lipshitz, R. A. Wannamaker, and J. Vanderkooy, “Quantization
and dither: A theoretical survey,” Journal of the Audio Engineering
Society, vol. 40, no. 5, pp. 355–375, 1992.

