6
1
0
2

 
r
a

 

M
9
1

 
 
]
L
C
.
s
c
[
 
 

1
v
5
5
1
6
0

.

3
0
6
1
:
v
i
X
r
a

A Persona-Based Neural Conversation Model

Jiwei Li 1∗ Michel Galley 2

Chris Brockett 2

Jianfeng Gao 2

Bill Dolan 2

1Stanford University, Stanford, CA, USA

jiweil@stanford.edu

2Microsoft Research, Redmond, WA, USA

{mgalley,chrisbkt,jfgao,billdol}@microsoft.com

Abstract

We present persona-based models for handling
the issue of speaker consistency in neural re-
sponse generation. A speaker model encodes
personas in distributed embeddings that cap-
ture individual characteristics such as back-
ground information and speaking style. A
dyadic speaker-addressee model captures prop-
erties of interactions between two interlocutors.
Our models yield qualitative performance im-
provements in both perplexity and BLEU scores
over baseline sequence-to-sequence models,
with similar gain in speaker consistency as mea-
sured by human judges.

1

Introduction

As conversational agents gain traction as user inter-
faces, there has been growing research interest in
training naturalistic conversation systems from large
volumes of human-to-human interactions (Ritter et
al., 2011; Sordoni et al., 2015; Vinyals and Le, 2015;
Li et al., 2015). One major issue for these data-driven
systems is their propensity to select the response with
greatest likelihood—in effect a consensus response of
the humans represented in the trainng data. Outputs
are frequently vague or non-committal as discussed
in (Li et al., 2015), and when not, they can be wildly
inconsistent, as illustrated in Table 1.

In this paper, we address the challenge of consis-
tency and how to endow data-driven systems with the
coherent “persona” needed to model human-like be-
havior, whether as personal assistants, personalized

* The entirety of this work was conducted at Microsoft.

message Where do you live now?
response I live in Los Angeles.
message In which city do you live now?
response I live in Madrid.
message In which country do you live now?
response England, you?
message Where were you born?
response I was born in Canada.
message Where are you from?
response England, you?
message Where did you grow up?
response I grew up in Texas.
message How old are you?
response 16 and you?
message What’s your age?
response 18.
message What is your major?
response I’m majoring in psychology
message What did you study in college?
response English lit.

Table 1: Inconsistent responses generated by a 4-layer
SEQ2SEQ model trained on 25 million Twitter conversa-
tion snippets.

avatar-like agents, or game characters.1 For present
purposes, we will deﬁne PERSONA as the character
that an artiﬁcial agent, as actor, plays or performs
during conversational interactions. A persona can be
viewed as a composite of elements of identity (back-
ground facts or user proﬁle), language behavior, and
interaction style. A persona is also adaptive, since an
agent may need to present different facets to different
human interlocutors depending on the demands of

1(Vinyals and Le, 2015) suggest that the lack of a coherent
personality makes it impossible for current systems to pass the
Turing test.

the interaction.

Fortunately, sequence-to-sequence (SEQ2SEQ)
models of conversation generation (Vinyals and Le,
2015; Li et al., 2015) provide a straightforward mech-
anism for incorporating personas as embeddings.
We therefore explore two persona models, a single-
speaker SPEAKER MODEL and a dyadic SPEAKER-
ADDRESSEE MODEL, within the SEQ2SEQ frame-
work. The Speaker Model integrates a speaker-
level vector representation into the target part of
the SEQ2SEQ model. Analogously, the Speaker-
Addressee model encodes the interaction patterns
of two interlocutors by constructing an interaction
representation from their individual embeddings and
incorporating it into the SEQ2SEQ model. These
persona vectors are trained on human-human conver-
sation data and used at test time to generate personal-
ized responses. Our experiments on an open-domain
corpus of Twitter conversations and dialog datasets
comprising TV series scripts show that leveraging
persona vectors can improve relative performance up
to 20% in BLEU score and 12% in perplexity, with
a commensurate gain in consistency as judged by
human annotators.

2 Related Work

This work follows the line of investigation initiated
by Ritter et al. (2011) who treat generation of con-
versational dialog as a statistical machine translation
(SMT) problem. Ritter et al. (2011) represents a
break with previous and contemporaneous dialog
work that relies extensively on hand-coded rules,
typically either building statistical models on top
of heuristic rules or templates (Levin et al., 2000;
Young et al., 2010; Walker et al., 2003; Pieraccini
et al., 2009; Wang et al., 2011) or learning genera-
tion rules from a minimal set of authored rules or
labels (Oh and Rudnicky, 2000; Ratnaparkhi, 2002;
Banchs and Li, 2012; Ameixa et al., 2014; Nio et
al., 2014; Chen et al., 2013). More recently (Wen
et al., 2015) have used a Long Short-Term Memory
(LSTM) (Hochreiter and Schmidhuber, 1997) to learn
from unaligned data in order to reduce the heuristic
space of sentence planning and surface realization.
The SMT model proposed by Ritter et al., on the
other hand, is end-to-end, purely data-driven, and
contains no explicit model of dialog structure; the
model learns to converse from human-to-human con-

versational corpora. Progress in SMT stemming from
the use of neural language models (Sutskever et al.,
2014; Gao et al., 2014; Bahdanau et al., 2015; Luong
et al., 2015) has inspired efforts to extend these neu-
ral techniques to SMT-based conversational response
generation. Sordoni et al. (2015) augments Ritter et
al. (2011) by rescoring outputs using a SEQ2SEQ
model conditioned on conversation history. Other
researchers have recently used SEQ2SEQ to directly
generate responses in an end-to-end fashion without
relying on SMT phrase tables (Serban et al., 2015;
Shang et al., 2015; Vinyals and Le, 2015). Serban
et al. (2015) propose a hierarchical neural model
aimed at capturing dependencies over an extended
conversation history. Recent work by Li et al. (2015)
measures mutual information between message and
response in order to reduce the proportion of generic
responses typical of SEQ2SEQ systems.

Modeling of users and speakers has been exten-
sively studied within the standard dialog modeling
framework (e.g., (Wahlster and Kobsa, 1989; Kobsa,
1990; Schatztnann et al., 2005; Lin and Walker,
2011)). Since generating meaningful responses in
a open-domain scenario is intrinsically difﬁcult in
conventional dialog systems, existing models often
focus on generalizing character style on the basis of
qualitative statistical analysis (Walker et al., 2012;
Walker et al., 2011). The present work, by contrast,
is in the vein of the SEQ2SEQ models of Vinyals
and Le (2015) and Li et al. (2015), enriching these
models by training persona vectors directly from con-
versational data and relevant side-information, and
incorporating these directly into the decoder.

3 Sequence-to-Sequence Models

Given a sequence of inputs X = {x1, x2, ..., xnX},
an LSTM associates each time step with an input
gate, a memory gate and an output gate, respectively
denoted as it, ft and ot. We distinguish e and h
where et denotes the vector for an individual text unit
(for example, a word or sentence) at time step t while
ht denotes the vector computed by the LSTM model
at time t by combining et and ht−1. ct is the cell state
vector at time t, and σ denotes the sigmoid function.
Then, the vector representation ht for each time step

t is given by: it

ft
ot
lt

 =

 σ

σ
σ
tanh

 W ·

(cid:21)

(cid:20) ht−1

es
t

(1)

ct = ft · ct−1 + it · lt
t = ot · tanh(ct)
hs

(2)
(3)
where Wi, Wf , Wo, Wl ∈ RK×2K. In SEQ2SEQ
generation tasks, each input X is paired with a se-
quence of outputs to predict: Y = {y1, y2, ..., ynY }.
The LSTM deﬁnes a distribution over outputs and se-
quentially predicts tokens using a softmax function:
p(yt|x1, x2, ..., xt, y1, y2, ..., yt−1)
p(Y |X) =

ny(cid:89)
ny(cid:89)

t=1

t=1

(cid:80)

=

exp(f (ht−1, eyt))
y(cid:48) exp(f (ht−1, ey(cid:48)))

where f (ht−1, eyt) denotes the activation function
between ht−1 and eyt. Each sentence terminates with
a special end-of-sentence symbol EOS. In keeping
with common practices, inputs and outputs use dif-
ferent LSTMs with separate parameters to capture
different compositional patterns.

During decoding, the algorithm terminates when
an EOS token is predicted. At each time step, either
a greedy approach or beam search can be adopted for
word prediction.
4 Personalized Response Generation
Our work introduces two persona-based models: the
Speaker Model, which models the personality of the
respondent, and the Speaker-Addressee Model which
models the way the respondent adapts their speech
to a given addressee — a linguistic phenomenon
known as lexical entrainment (Deutsch and Pech-
mann, 1982).

the response generation task,

4.1 Notation
For
let M de-
note the input word sequence (message) M =
{m1, m2, ..., mI}. R denotes the word sequence in
response to M, where R = {r1, r2, ..., rJ , EOS} and
J is the length of the response (terminated by an
EOS token). rt denotes a word token that is associ-
ated with a K dimensional distinct word embedding
et. V is the vocabulary size.

4.2 Speaker Model
Our ﬁrst model is the Speaker Model, which models
the respondent alone. This model represents each
individual speaker as a vector or embedding, which
encodes speaker-speciﬁc information (e.g., dialect,
register, age, gender, personal information) that inﬂu-
ences the content and style of her responses.2
Figure 1 gives a brief illustration of the Speaker
Model. Each speaker i ∈ [1, N ] is associated with a
user-level representation vi ∈ RK×1. As in standard
SEQ2SEQ models, we ﬁrst encode message S into
a vector representation hS using the source LSTM.
Then for each step in the target side, hidden units are
obtained by combining the representation produced
by the target LSTM at the previous time step, the
word representations at the current time step, and the
speaker embedding vi:

 it

ft
ot
lt

 =

 σ

σ
σ
tanh

 W ·

 ht−1

es
t
vi



(4)

ct = ft · ct−1 + it · lt
t = ot · tanh(ct)
hs

(5)
(6)
where W ∈ R4K×3K. In this way, speaker informa-
tion is encoded and injected into the hidden layer at
each time step and thus helps predict personalized
responses throughout the generation process. The
Speaker embedding {vi} is shared across all con-
versations that involve speaker i. {vi} are learned
by back propagating word prediction errors to each
neural component during training.

Another helpful property of this model is that it
helps infer answers to questions even if the evidence
is not readily present in the training set. This is im-
portant as the training data does not contain explicit
information about every attribute of each user (e.g.,
gender, age, country of residence). The model learns
speaker representations based on conversational con-
tent produced by different speakers, and speakers
producing similar responses tend to have similar em-
beddings, occupying nearby positions in the vector
space. This way, the training data of speakers nearby

2Note that these attributes are not explicitly annotated, which
would be tremendously expensive for our datasets. Instead, our
model manages to cluster users along some of these traits (e.g.,
age, country of residence) based on responses alone.

Figure 1: Illustrative example of the Speaker Model introduced in this work. Speaker IDs close in embedding space
tend to respond in the same manner. These speaker embeddins are learned jointly with word embeddings and all other
parameters of the neural model via backpropagation. In this example, say Rob is a speaker clustered with people who
often mention England in the training data, then the generation of the token ‘england’ at time t = 2 would be much
more likely than that of ‘u.s.’. A non-persona model would prefer generating in the u.s. if ‘u.s.’ is more represented in
the training data across all speakers.

in vector space help increase the generalization capa-
bility of the speaker model. For example, consider
two speakers i and j who sound distinctly British, and
who are therefore close in speaker embedding space.
Now, suppose that, in the training data, speaker i was
asked Where do you live? and responded in the UK.
Even if speaker j was never asked the same question,
this answer can help inﬂuence a good response from
speaker j, and this without any explicitly labeled
geo-location information.

4.3 Speaker-Addressee Model
A natural extension of the Speaker Model is a model
that is sensitive to speaker-addressee interaction pat-
terns within the conversation. Indeed, speaking style,
register, and content does not only vary with the iden-
tity of the speaker, but also with that of the addressee.
For example, in scripts for the TV series Friends used
in some of our experiments, the character Ross often
talks differently to his sister Monica than to Rachel,
with whom he is engaged in a on-again off-again
relationship throughout the series.

The proposed Speaker-Addressee Model operates
as follows: We wish to predict how speaker i would
respond to a message produced by speaker j. Simi-
larly to the Speaker model, we associate each speaker
with a K dimensional speaker-level representation,

namely vi for user i and vj for user j. We obtain an
interactive representation Vi,j ∈ RK×1 by linearly
combining user vectors vi and vj in an attempt to
model the interactive style of user i towards user j,

Vi,j = tanh(W1 · vi + W2 · v2)

(7)
where W1, W2 ∈ RK×K. Vi,j is then linearly incor-
porated into LSTM models at each step in the target:

 it

ft
ot
lt

 =

 σ

σ
σ
tanh

 W ·

 ht−1

es
t
Vi,j



(8)

(9)

(10)

ct = ft · ct−1 + it · lt
t = ot · tanh(ct)
hs

Vi,j depends on both speaker and addressee and the
same speaker will thus respond differently to a mes-
sage from different interlocutors. One potential issue
with Speaker-Addressee modelling is the difﬁculty
involved in collecting a large-scale training dataset in
which each speaker is involved in conversation with a
wide variety of people. Like the Speaker Model, how-
ever, the Speaker-Addressee Model derives general-
ization capabilities from speaker embeddings. Even
if the two speakers at test time (i and j) were never

EOSRobWord embeddings(50k)englandlondonu.s.greatgoodstayliveokaymondaytuesdaySpeaker embeddings(70k)Rob_712wheredoyouliveininRobenglandRobengland.Rob.EOSSourceTargetskinnyoflynny2TomcoatezKush_322D_Gomes25Dreamswallskierongillen5TheCharlieZThe_Football_BarThis_Is_ArtfulDigitalDan285Jinnmeow3Bob_Kelly2involved in the same conversation in the training data,
two speakers i(cid:48) and j(cid:48) who are respectively close in
embeddings may have been, and this can help mod-
elling how i should respond to j.

The dataset extracted using responses by these “con-
versationalists” contained 24,725,711 3-turn sliding-
window (context-message-response) conversational
sequences.

4.4 Decoding and Reranking
For decoding, the N-best lists are generated using the
decoder with beam size B = 200. We set a maximum
length of 20 for the generated candidates. Decoding
operates as follows: At each time step, we ﬁrst ex-
amine all B × B possible next-word candidates, and
add all hypothesis ending with an EOS token to the
N-best list. We then preserve the top-B unﬁnished
hypotheses and move to the next word position.

To deal with the issue that SEQ2SEQ models tend
to generate generic and commonplace responses such
as I don’t know, we follow Li et al. (2015) by rerank-
ing the generated N-best list using a scoring function
that linearly combines a length penalty and the log
likelihood of source given target:

log p(R|M, v) + λ log p(M|R) + γ|R|

(11)
where p(R|M, v) denotes the probability of the gen-
erated response given the message M and the re-
spondent’s speaker ID. |R| denotes the length of the
target and γ denotes the associated penalty weight.
We optimize γ and λ on N-best lists of response
candidates generated from the development set us-
ing MERT (Och, 2003) by optimizing BLEU. To
compute p(M|R), we train an inverse SEQ2SEQ
model by swapping messages and responses. We
trained standard SEQ2SEQ models for p(M|R) with
no speaker information considered.

5 Datasets

5.1 Twitter Persona Dataset
Data Collection Training data for the Speaker
Model was extracted from the Twitter FireHose for
the six-month period beginning January 1, 2012. We
limited the sequences to those where the responders
had engaged in at least 60 (and at most 300) con-
versational interactions during the period, in other
words, users who reasonably frequently engaged in
conversation. This yielded a set of 74,003 users who
took part in a minimum of 60 and a maximum of 164
conversational turns (average: 92.24, median: 90).

In addition, we sampled 12000 3-turn conversa-
tions from the same user set from the Twitter Fire-
Hose for the three-month period beginning July 1,
2012, and set these aside as development, valida-
tion, and test sets (4000 conversational sequences
each). Note that development, validation, and test
sets for this data are single-reference, which is by by
design. Multiple reference responses would typically
require acquiring responses from different people,
which would confound different personas.
Training
four-layer
SEQ2SEQ models on the Twitter corpus following
the approach of (Sutskever et al., 2014). Details are
as follows:

Protocols We

trained

for each layer.

• 4 layer LSTM models with 1,000 hidden cells
• Batch size is set to 128.
• Learning rate is set to 1.0.
• Parameters are initialized by sampling from the
• Gradients are clipped to avoid gradient explo-
sion with a threshold of 5.
• Vocabulary size is limited to 50,000.
• Dropout rate is set to 0.2.

uniform distribution [−0.1, 0.1].

Source and target LSTMs use different sets of param-
eters. We ran 14 epochs, and training took roughly a
month to ﬁnish on a Tesla K40 GPU machine.

As only speaker IDs of responses were speciﬁed
when compiling the Twitter dataset, experiments on
this dataset were limited to the Speaker Model.

5.2 Twitter Sordoni Dataset
The Twitter Persona Dataset was collected for this
paper for experiments with speaker ID information.
To obtain a point of comparison with prior state-of-
the-art work (Sordoni et al., 2015; Li et al., 2015),
we measure our baseline (non-persona) LSTM model
against prior work on the dataset of (Sordoni et al.,
2015), which we call the Twitter Sordoni Dataset. We
only use its test-set portion, which contains responses
for 2114 context and messages. It is important to note
that the Sordoni dataset offers up to 10 references per
message, while the Twitter Persona dataset has only

1 reference per message. Thus BLEU scores cannot
be compared across the two Twitter datasets (BLEU
scores on 10 references are generally much higher
than with 1 reference). Details of this dataset are in
(Sordoni et al., 2015).

System
MT baseline (Ritter et al., 2011)
Standard LSTM MMI (Li et al., 2015)
Standard LSTM MMI (our system)
Human

BLEU
3.60%
5.26%
5.82%
6.08%

5.3 Television Series Transcripts

the

Data Collection For
Speaker-
dyadic
Addressee Model we used scripts
from the
American television comedies Friends3 and The
Big Bang Theory,4 available from from Internet
Movie Script Database (IMSDb).5 We collected
13 main characters from the two series in a corpus
of 69,565 turns. We split the corpus into train-
ing/development/testing sets, with development and
testing sets each of about 2,000 turns.

Training Since the relatively small size of the
dataset does not allow for training an open domain
dialog model, we adopted a domain adaption strategy
where we ﬁrst trained a standard SEQ2SEQ models
using a much larger OpenSubtitles (OSDb) dataset
(Tiedemann, 2009), and then adapting the pre-trained
model to the TV series dataset.

The OSDb dataset is a large, noisy, open-domain
dataset containing roughly 60M-70M scripted lines
spoken by movie characters. This dataset does not
specify which character speaks each subtitle line,
which prevents us from inferring speaker turns. Fol-
lowing Vinyals et al. (2015), we make the simplify-
ing assumption that each line of subtitle constitutes
a full speaker turn.6 We trained standard SEQ2SEQ
models on OSDb dataset, following the protocols al-
ready described in Section 5.1. We run 10 iterations
over the training set.

We initialize word embeddings and LSTM parame-
ters in the Speaker Model and the Speaker-Addressee
model using parameters learned from OpenSubtitles
datasets. User embeddings are randomly initialized
from [−0.1, 0.1]. We then ran 5 additional epochs un-
til the perplexity on the development set stabilized.

3https://en.wikipedia.org/wiki/Friends
4https://en.wikipedia.org/wiki/The_Big_

Bang_Theory

5http://www.imsdb.com
6This introduces a degree of noise as consecutive lines are
not necessarily from the same scene or two different speakers.

Table 2: BLEU on the Twitter Sordoni dataset (10 refer-
ences). We contrast our baseline against an SMT baseline
(Ritter et al., 2011), and the best result (Li et al., 2015) on
the established dataset of (Sordoni et al., 2015). The last
result is for a human oracle, but it is not directly compa-
rable as the oracle BLEU is computed in a leave-one-out
fashion, having one less reference available. We never-
theless provide this result to give a sense that these BLEU
scores of 5-6% are not unreasonable.

Model

Perplexity

Standard LSTM Speaker Model
42.2 (−10.6%)

47.2

Table 3: Perplexity for standard SEQ2SEQ and the Speaker
model on the development set of the Twitter Persona
dataset.

6 Experiments

6.1 Evaluation

Following (Sordoni et al., 2015; Li et al., 2015) we
used BLEU (Papineni et al., 2002) for parameter tun-
ing and evaluation. BLEU has been shown to cor-
relate well with human judgment on the response
generation task, as demonstrated in (Galley et al.,
2015). Besides BLEU scores, we also report perplex-
ity, which has been widely adopted as an indicator of
model capability.

6.2 Baseline

Since our main experiments are with a new dataset
(the Twitter Persona Dataset), we ﬁrst show that our
LSTM baseline is competitive with the state-of-the-
art (Li et al., 2015) on an established dataset, the
Twitter Sordoni Dataset (Sordoni et al., 2015). Our
baseline is simply our implementation of the LSTM-
MMI of (Li et al., 2015), so results should be rela-
tively close to their reported results. Table 2 summa-
rizes our results against prior work. We see that our
system actually does better than (Li et al., 2015), and
we attribute the improvement to a larger training cor-
pus, the use of dropout during training, and possibly
to the “conversationalist” nature of our corpus.

Model

Perplexity

27.3

Standard LSTM Speaker Model Speaker-Addressee Model

25.4 (−7.0%)

25.0 (−8.4%)

Table 5: Perplexity for standard SEQ2SEQ and persona models on the TV series dataset.

Model Standard LSTM Speaker Model
1.82% (+13.7%)
MLE
MMI
1.90% (+10.6%)

1.60%
1.70%

Speaker-Addressee Model

1.83% (+14.3%)
1.88% (+10.9%)

Table 6: BLEU on the TV series dataset (1 reference), for the standard SEQ2SEQ and persona models.

Model
Standard LSTM MLE
Speaker Model MLE
Standard LSTM MMI
Speaker Model MMI

Objective BLEU
0.92%
1.12% (+21.7%)
1.41%
1.66% (+11.7%)

Table 4: BLEU on the Twitter Persona dataset (1 refer-
ence), for the standard SEQ2SEQ model and the Speaker
model using as objective either maximum likelihood
(MLE) or maximum mutual information (MMI).

6.3 Results
We ﬁrst report performance on the Twitter Persona
dataset. Perplexity is reported in Table 3. We observe
about a 10% decrease in perplexity for the Speaker
model over the standard SEQ2SEQ model. In terms
of BLEU scores (Table 4), a signiﬁcant performance
boost is observed for the Speaker model over the
standard SEQ2SEQ model, yielding an increase of
21% in the maximum likelihood (MLE) setting and
11.7% for mutual information setting (MMI). In line
with ﬁndings in (Li et al., 2015), we observe a con-
sistent performance boost introduced by the MMI
objective function over a standard SEQ2SEQ model
based on the MLE objective function. It is worth
noting that our persona models are more beneﬁcial to
the MLE models than to the MMI models. This result
is intuitive as the persona models help make Standard
LSTM MLE outputs more informative and less bland,
and thus make the use of MMI less critical.

For the TV Series dataset, perplexity and BLEU
scores are respectively reported in Table 5 and Ta-
ble 6. As can be seen, the Speaker and Speaker-
Addressee models respectively achieve perplexity
values of 25.4 and 25.0 on the TV-series dataset,
7.0% and 8.4% percent lower than the correspondent
standard SEQ2SEQ models. In terms of BLEU score,
we observe a similar performance boost as on the

Twitter dataset, in which the Speaker model and the
Speaker-Addressee model outperform the standard
SEQ2SEQ model by 13.7% and 10.6%. By compar-
ing the Speaker-Addressee model against the Speaker
model on the TV Series dataset, we do not observe
a signiﬁcant difference. We suspect that this is pri-
marily due to the relatively small size of the dataset
where the interactive patterns might not be fully cap-
tured. Smaller values of perplexity are observed for
the Television Series dataset than the Twitter dataset,
the perplexity of which is over 40, presumably due
to the more noisy nature of Twitter dialogues.

6.4 Qualitative Analysis
Diverse Responses by Different Speakers Table
7 represents responses generated by persona models
in response to three different input questions. We ran-
domly selected 10 speakers (without cherry-picking)
from the original twitter dataset. We collected their
user level representations from a speaker look-up ta-
ble and integrated them into the decoding models.
We can see that the model tends to generate speciﬁc
responses for different people in response to the fac-
tual questions.7

Human Evaluation We conducted a human eval-
uation of outputs from the Speaker Model, using
a crowdsourcing service. Since we cannot expect
crowdsourced human judges to know or attempt to
learn the ground truth of Twitter users who are not
well-known public ﬁgures, we designed our experi-
ment to evaluate the consistency of outputs associ-
ated with the speaker IDs. To this end, we collected
24 pairs of questions for which we would expect
responses to be consistent if the persona model is co-
herent. For example, responses to the questions What

7There appears to be a population bias in the training set that

favors British users.

message Where is your hometown?
baseline I was born in Canada.

user1 I’m from England.
user2 I’m from Manchester.
user3 I’m from Liverpool.
user4 England. You?
user5 My hometown.
user6 I’m from Texas.
user7 I’m from LA.
user8 England.
user9 I was born here.
user10 I was born in the us.

message What company do you work for?
baseline I work for a company.

user1 I don’t have a job.
user2 I’m a manager.
user3 I’m working. #
user4 I work for a company.
user5 I’m working. #
user6 Customer service.
user7 I work in retail.
user8 I work in a cafe.
user9 I work at a gas station.
user10 I don’t know #

message What did you have for dinner?
baseline I had ﬁsh and chips.

user1 I had spag bol.
user2 Chicken and chips.
user3 Chicken and rice.
user4 Fish and chips.
user5 I had spag bol.
user6 I had Mexican food.
user7 Salad...
user8 I had chicken and chips.
user9 I had spag bol.
user10 Pizza.

Table 7: Responses generated by the baseline (LSTM-
MMI) and the Speaker Model for ten randomly selected
users, without cherry picking. # indicates poor-quality
responses produced by the system.

country do you live in? and What city do you live
in? would be considered consistent if the answers
were England and London respectively, but not if
they were UK and Chicago. Similarly, the responses
to Are you vegan or vegetarian? and Do you eat beef?
are consistent if the answers generated are vegan and
absolutely not, but not if they are vegan and I love
beef. We collected the top 20 pairs of outputs pro-
vided by the Speaker Model for each question pair
(480 response pairs total). We also obtained the cor-

responding outputs from the baseline MMI-enhanced
SEQ2SEQ system.

Since our purpose is to measure the gain in con-
sistency over the baseline system, we presented the
pairs of answers system-pairwise, i.e., 4 responses, 2
from each system, displayed on the screen, and asked
judges to decide which of the two systems was more
consistent. The position in which the system pairs
were presented on the screen was randomized. Five
judges rated each pair. A system was assigned a score
1.0 if it was judged much more consistent than the
other, 0.5 if mostly more consistent, zero otherwise.
Ties (where the two systems are equally consistent or
inconsistent) were discarded. A maximum score of
5.0 was possible. After reweighting for frequency of
response pairs, the mean relative consistency rating
for the Speaker Model was 1.33 compared to 0.99
for the baseline model, representing a gain in relative
consistency of 34.68%. It should be stressed that the
latter is a strong baseline, since it represents the con-
sensus of all 70K Twitter users in the dataset. (I’m
not pregnant is an excellent consensus answer to the
question Are you pregnant?, while I’m pregnant is
consistent as a response only in the case of someone
who also answers the question Are you a guy or a
girl? with something in the vein of I’m a girl.)

Speaker Consistency Table 8 illustrates how con-
sistency is an emergent property of two arbitrarily
selected users. The model is capable of discovering
the relations between different categories of location
such as London and the UK, Jakarta and Indone-
sia. However, the model also makes inconsistent
response decisions, generating different answers in
the second example in response to questions asking
about age or major. Our proposed persona models
integrate user embeddings into the LSTM, and thus
can be viewed as encapsulating a trade-off between
a persona-speciﬁc generation model and a general
conversational model.

7 Conclusions

We have presented two persona-based response gen-
eration models for open-domain conversation gen-
eration. There are many other aspects of speaker
behavior, such as mood and emotion, that we have
not attempted to examine here, but these are beyond
the scope of the current paper and must be left to

User1

message Where is your hometown?
response I’m from England.
message Where are you from?
response I’m from England.
message In which city do you live now?
response I live in London.
message In which country do you live now?
response I live in England.
message What is your major?
response Business. You?
message What did you study in college?
response I did business studies.
message How old are you?
response I’m 18.
message What is your age?
response I’m 18.

User2

message Where is your hometown?
response I live in Indonesia.
message Where are you from?
response England, you?
message In which city do you live now?
response I live in Jakarta.
message In which country do you live now?
response I live in Indonesia.
message What is your major?
response Business, you?
message What did you study in college?
response Psychology, you?
message How old are you?
response I’m 18.
message What is your age?
response I’m 16.

Table 8: Examples of speaker consistency and inconsis-
tency generated by the Speaker Model

future work.

Although the gains presented by our new models
are not spectacular, the systems nevertheless outper-
form our baseline SEQ2SEQ systems in terms of
BLEU, perplexity, and human judgments of speaker
consistency. We have demonstrated that by encod-
ing personas into distributed representations, we are
able to capture certain personal characteristics such
as speaking style and background information. In the
Speaker-Addressee model, moreover, the evidence
suggests that there is beneﬁt in capturing dyadic in-
teractions.

Our ultimate goal is to be able to take the proﬁle of

an arbitrary individual whose identity is not known in
advance, and generate conversations that accurately
emulate that individual’s persona in terms of linguis-
tic response behavior and other salient characteristics.
Such a capability will dramatically change the ways
in which we interact with dialog agents of all kinds,
opening up rich new possibilities for user interfaces.
Given a sufﬁciently large training corpus in which
a sufﬁciently rich variety of speakers is represented,
this objective does not seem too far-fetched.
Acknowledgments
We thank Dan Jurafsky, Pushmeet Kohli, Chris Quirk,
Alan Ritter, and George Spithourakis for helpful dis-
cussions about this work.

References
David Ameixa, Luisa Coheur, Pedro Fialho, and Paulo
Quaresma. 2014. Luke, I am your father: dealing with
out-of-domain requests by using movies subtitles. In
Intelligent Virtual Agents, pages 13–21. Springer.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.
2015. Neural machine translation by jointly learning
In Proc. of the International
to align and translate.
Conference on Learning Representations (ICLR).

Rafael E Banchs and Haizhou Li. 2012. IRIS: a chat-
oriented dialogue system based on the vector space
model. In Proc. of the ACL 2012 System Demonstra-
tions, pages 37–42.

Yun-Nung Chen, Wei Yu Wang, and Alexander Rudnicky.
2013. An empirical investigation of sparse log-linear
models for improved dialogue act classiﬁcation.
In
Acoustics, Speech and Signal Processing (ICASSP),
2013 IEEE International Conference on, pages 8317–
8321. IEEE.

Werner Deutsch and Thomas Pechmann. 1982. Social
interaction and the development of deﬁnite descriptions.
Cognition, 11:159–184.

Michel Galley, Chris Brockett, Alessandro Sordoni,
Yangfeng Ji, Michael Auli, Chris Quirk, Margaret
Mitchell, Jianfeng Gao, and Bill Dolan. 2015. ∆BLEU:
A discriminative metric for generation tasks with intrin-
sically diverse targets. In Proc. of ACL-IJCNLP, pages
445–450, Beijing, China, July.

Jianfeng Gao, Xiaodong He, Wen-tau Yih, and Li Deng.
2014. Learning continuous phrase representations for
translation modeling. In Proc. of ACL, pages 699–709,
Baltimore, Maryland.

Sepp Hochreiter and J¨urgen Schmidhuber. 1997. Long
short-term memory. Neural computation, 9(8):1735–
1780.

Alfred Kobsa. 1990. User modeling in dialog systems:

Potentials and hazards. AI & society, 4(3):214–231.

Esther Levin, Roberto Pieraccini, and Wieland Eckert.
2000. A stochastic model of human-machine interac-
tion for learning dialog strategies. IEEE Transactions
on Speech and Audio Processing, 8(1):11–23.

Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,
and Bill Dolan. 2015. A diversity-promoting objective
function for neural conversation models. arXiv preprint
arXiv:1510.03055.

Grace I Lin and Marilyn A Walker. 2011. All the world’s
a stage: Learning character models from ﬁlm. In Pro-
ceedings of the Seventh AAAI Conference on Artiﬁcial
Intelligence and Interactive Digital Entertainment (AI-
IDE).

Thang Luong, Ilya Sutskever, Quoc Le, Oriol Vinyals, and
Wojciech Zaremba. 2015. Addressing the rare word
problem in neural machine translation. In Proc. of ACL,
pages 11–19, Beijing, China, July.

Lasguido Nio, Sakriani Sakti, Graham Neubig, Tomoki
Toda, Mirna Adriani, and Satoshi Nakamura. 2014.
Developing non-goal dialog system based on examples
of drama television. In Natural Interaction with Robots,
Knowbots and Smartphones, pages 355–361. Springer.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting of the Association for Computa-
tional Linguistics, pages 160–167, Sapporo, Japan, July.
Association for Computational Linguistics.

Alice H Oh and Alexander I Rudnicky. 2000. Stochastic
language generation for spoken dialogue systems. In
Proceedings of the 2000 ANLP/NAACL Workshop on
Conversational systems-Volume 3, pages 27–32.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proc. of ACL, pages
311–318.

Roberto Pieraccini, David Suendermann, Krishna
Dayanidhi, and Jackson Liscombe. 2009. Are we there
yet? research in commercial spoken dialog systems. In
Text, Speech and Dialogue, pages 3–13. Springer.

Adwait Ratnaparkhi. 2002. Trainable approaches to sur-
face natural language generation and their application
to conversational dialog systems. Computer Speech &
Language, 16(3):435–455.

Alan Ritter, Colin Cherry, and William B Dolan. 2011.
Data-driven response generation in social media. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing, pages 583–593.

Jost Schatztnann, Matthew N Stuttle, Karl Weilhammer,
and Steve Young. 2005. Effects of the user model on
simulation-based learning of dialogue strategies. In Au-
tomatic Speech Recognition and Understanding, 2005
IEEE Workshop on, pages 220–225.

Iulian V Serban, Alessandro Sordoni, Yoshua Bengio,
Aaron Courville, and Joelle Pineau. 2015. Building
end-to-end dialogue systems using generative hierarchi-
cal neural network models. In Proc. of AAAI.

Lifeng Shang, Zhengdong Lu, and Hang Li. 2015. Neural
In

responding machine for short-text conversation.
ACL-IJCNLP, pages 1577–1586.

Alessandro Sordoni, Michel Galley, Michael Auli, Chris
Brockett, Yangfeng Ji, Meg Mitchell, Jian-Yun Nie,
Jianfeng Gao, and Bill Dolan. 2015. A neural network
approach to context-sensitive generation of conversa-
tional responses. In Proc. of NAACL-HLT.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural networks.
In Advances in neural information processing systems
(NIPS), pages 3104–3112.

J¨org Tiedemann. 2009. News from OPUS – a collec-
tion of multilingual parallel corpora with tools and
In Recent advances in natural language
interfaces.
processing, volume 5, pages 237–248.

Oriol Vinyals and Quoc Le. 2015. A neural conversational

model. In Proc. of ICML Deep Learning Workshop.

Wolfgang Wahlster and Alfred Kobsa. 1989. User models

in dialog systems. Springer.

Marilyn A Walker, Rashmi Prasad, and Amanda Stent.
2003. A trainable generator for recommendations in
multimodal dialog. In INTERSPEECH.

Marilyn A Walker, Ricky Grant, Jennifer Sawyer, Grace I
Lin, Noah Wardrip-Fruin, and Michael Buell. 2011.
Perceived or not perceived: Film character models for
expressive nlg. In Interactive Storytelling, pages 109–
121. Springer.

Marilyn A Walker, Grace I Lin, and Jennifer Sawyer.
2012. An annotated corpus of ﬁlm dialogue for learn-
ing and characterizing character style. In LREC, pages
1373–1378.

William Yang Wang, Ron Artstein, Anton Leuski, and
David Traum. 2011. Improving spoken dialogue un-
derstanding using phonetic mixture models. In FLAIRS
Conference.

Tsung-Hsien Wen, Milica Gasic, Nikola Mrkˇsi´c, Pei-Hao
Su, David Vandyke, and Steve Young. 2015. Semanti-
cally conditioned LSTM-based natural language gener-
ation for spoken dialogue systems. In Proc. of EMNLP,
pages 1711–1721, Lisbon, Portugal, September. Asso-
ciation for Computational Linguistics.

Steve Young, Milica Gaˇsi´c, Simon Keizer, Franc¸ois
Mairesse, Jost Schatzmann, Blaise Thomson, and Kai
Yu. 2010. The hidden information state model: A prac-
tical framework for pomdp-based spoken dialogue man-
agement. Computer Speech & Language, 24(2):150–
174.

