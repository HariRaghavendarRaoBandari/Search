Wireless Content Caching for Small Cell and D2D

Networks

Maria Gregori, Jesús Gómez-Vilardebó, Javier Matamoros and Deniz Gündüz

1

6
1
0
2

 
r
a

M
 
4
1

 
 
]
T
I
.
s
c
[
 
 

1
v
1
4
3
4
0

.

3
0
6
1
:
v
i
X
r
a

Abstract—The ﬁfth generation wireless networks must provide
fast and reliable connectivity while coping with the ongoing
trafﬁc growth. It is of paramount importance that the required
resources, such as energy and bandwidth, do not scale with
trafﬁc. While the aggregate network trafﬁc is growing at an
unprecedented rate, users tend to request the same popular
contents at different time instants. Therefore, caching the most
popular contents at the network edge is a promising solution
to reduce the trafﬁc and the energy consumption over the
backhaul
links. In this paper, two scenarios are considered,
where caching is performed either at a small base station, or
directly at the user terminals, which communicate using Device-
to-Device (D2D) communications. In both scenarios, joint design
of the transmission and caching policies is studied when the user
demands are known in advance. This joint design offers two
different caching gains, namely, the pre-downloading and local
caching gains. It is shown that the ﬁnite cache capacity limits
the attainable gains, and creates an inherent tradeoff between the
two types of gains. In this context, a continuous time optimization
problem is formulated to determine the optimal transmission and
caching policies that minimize a generic cost function, such as
energy, bandwidth, or throughput. The jointly optimal solution
is obtained by demonstrating that caching ﬁles at a constant rate
is optimal, which allows to reformulate the problem as a ﬁnite-
dimensional convex program. The numerical results show that
the proposed joint transmission and caching policy dramatically
reduces the total cost, which is particularised to the total energy
consumption at the Macro Base Station (MBS), as well as to the
total economical cost for the service provider, when users demand
economical incentives for delivering content to other users over
the D2D links.

Index Terms—Proactive caching, 5G, wireless backhaul, small

cells, energy-efﬁciency, device-to-device.

I. INTRODUCTION

Wireless trafﬁc has experienced a tremendous growth in the
last years due to the wide spread use of hand-held devices
connected to the Internet, e.g., mobile phones, tablets, etc. This
trafﬁc increase is expected to continue steadily in the coming
years; for example, more than 127 exabytes of worldwide
mobile trafﬁc is forecasted for the year 2020 [1]. Video trafﬁc
is the major data source due to the growing success of on-
demand video streaming services [1]. Trafﬁc resulting from
video on-demand services exhibits the asynchronous content
reuse property [2], according to which a few popular ﬁles,
requested by users at different times (as opposed to television
broadcasting services), account for most of the data trafﬁc.

Maria Gregori,

and

Jesús Gómez-Vilardebó,

Javier Matamoros
are with the Centre Tecnològic de Telecomunicacions de Catalunya
(CTTC), 08860 Barcelona, Spain (e-mails: {maria.gregori,
jesus.gomez,
javier.matamoros}@cttc.cat). Deniz Gündüz is with the Imperial College of
London, UK (e-mail: d.gunduz@imperial.ac.uk).

This work is partially supported by the EC-funded project NEWCOM#
(n.318306), by the Spanish Government through the projects INTENSYV
(TEC2013-44591-P) and E-CROPs (PCIN-2013-027) in the framework of the
ERA-NET CHIST-ERA program, and by the Catalan Government (2014 SGR
1567).

To cope with this growing trafﬁc requirements,

lots of
efforts have been devoted towards the deﬁnition of the ﬁfth
generation of cellular communication systems (5G), which is
expected to be operative by 2020. The 5G system must provide
fast, ﬂexible, reliable, and sustainable wireless connectivity,
while supporting the growing mobile trafﬁc. Device-to-Device
(D2D) communications, small cell densiﬁcation, millimeter
wave, and massive MIMO are currently investigated as main
enabling technologies for its success.

Small cell densiﬁcation refers to the deployment of a large
number of Small Base Stations (SBSs) with different cell sizes
(micro, pico, and femtocells) allowing a larger spatial reuse of
the resources. The major drawback of cell densiﬁcation is that
the trafﬁc that can be served by an SBS is limited by the
capacity of the backhaul link, which provides connection to
the core network. This link is preferably wireless for various
reasons such as, rapid deployment, self-conﬁguration, and
cost. However, wireless backhaul connections entail limited
capacity and signiﬁcant energy consumption (due to its rela-
tively long range).

Caching the most popular contents at the network edge
has been proposed in [3] to increase connectivity, in [4]
to reduce the delay, and in [5] to alleviate the backhaul
link congestion and to reduce its energy consumption. Video
trafﬁc (e.g., popular Youtube videos) is especially suitable
to be cached since it requires high data rates and exhibits
the aforementioned asynchronous content reuse property. The
contents can be cached either at SBSs equipped with a cache
memory (also coined as “femtocaching”) [5]–[11], or directly
at the users’ devices [2], [12], [13]. The users can exchange
the cached content through D2D communications [14], which
allows direct communication between nearby mobile users. In
practice, due to limited cache and energy resources, users are
unwilling to serve data over the D2D links unless they obtain
incentives (e.g., economical) from the operator [15].

In a popular approach to wireless caching, [7], [11], [16],
the system design is performed in two separated phases. First,
in the content placement phase, each cache is ﬁlled with
appropriate data, exploiting periods of time in which the
network is not congested. Then, in the delivery phase, the
non-cached contents are transmitted when requested by users.
In this setup, two types of caching gains have been identiﬁed,
namely, the local and global caching gains [11]. On the one
hand, the local caching gain is obtained when a requested ﬁle
is locally available in the cache (either at the SBS or at the
users) by serving this ﬁle from the cache without connecting
to the Macro Base Station (MBS). This reduces the trafﬁc
in the wireless backhaul link [7] and improves the quality of
experience [11]. On the other hand, the global caching gain
is obtained by multicasting network-coded information in the
delivery phase [11], [16]. However, this underlying separation

between the caching (content placement) and transmission
(delivery) phases has two limiting assumptions: i) the content
placement phase is cost-free (e.g.,
in terms of energy or
bandwidth); and ii) cache content is never updated during the
delivery phase. As a result, the beneﬁts of proactive caching
are inherently limited.

In this work, we consider a different approach to wireless
caching. In particular, we consider that the cache is initially
empty, and it is dynamically ﬁlled with contents, i.e., we
combine the content placement and delivery phases. This
approach still allows to pre-download data over low-trafﬁc
periods; however, we now account for the cost of downloading
these contents. As a result, an additional caching gain is
obtained, which we call pre-downloading gain. Essentially, the
pre-downloading caching gain is obtained when the cache is
used to pre-download data, which can be beneﬁcial to avoid
non-favourable channel conditions, and to equalize the rate
in the backhaul link, improving its energy efﬁciency, and
reducing its peak load. In this context, the authors of [17] and
[18] derive caching and transmission policies that minimize the
bandwidth and energy consumption, respectively. These works
assume that the cache is solely used to pre-download content
for a single user; thus, content is removed from the cache
as soon as it is consumed by the user, ignoring any possible
future requests. Consequently, the policies in [17] and [18]
only exploit the pre-downloading caching gain. To the best
of our knowledge, this is the ﬁrst work that proposes jointly
optimal transmission and caching strategies by accounting for
both the local and pre-downloading caching gains.

To fully exploit the aforementioned gains, efﬁcient cache
management policies must be designed by taking into account,
among others, the stochastic but predictable nature of users’
demands. The cache management policies can be classiﬁed
into two groups according to the prior knowledge of the
different system parameters (users’ requests, channel state
information, etc.): (i) ofﬂine caching policies that assume non-
causal and complete knowledge of these parameters, e.g., [17]–
[19]; and (ii) online caching policies that consider only causal
or probabilistic knowledge of these parameters, e.g., [6]–[10],
[16], [20]. The characterization of the optimal ofﬂine policy
is extremely useful because: i) it serves as a theoretical bound
on the performance achievable by any online policy; and ii) it
can be instrumental in designing low-complexity near-optimal
online policies. Finding the optimal online policy is extremely
challenging since the cache management problem is usually a
hard combinatorial problem. As a result several works have
resorted to heuristic algorithms [6], [7].

In contrast to previous literature, the aim of this paper is
to study the jointly optimal transmission and caching policies
by taking into account both the local and pre-downloading
caching gains under two different scenarios. The ﬁrst scenario
considers a caching SBS that serves the demands from mul-
tiple users. When the users’ demand is not locally available
at the SBS, the SBS downloads the content from an MBS
through a wireless backhaul link. We addressed this scenario in
[21] assuming that the SBS serves the users in a time division
fashion; in this paper, we allow the SBS to serve multiple users
simultaneously. In the second scenario, we consider that the

2

MBS directly serves demands from users, which can cache the
received data proactively, and later cooperate with other users
through D2D communications. The key difference between the
two scenarios is that, in the former, the cache is centralized
at the SBS, whereas in the later, it is distributed across users.
The main contributions of the paper are summarized next:

• For the two scenarios mentioned above, we study the joint
design of the optimal transmission and caching policies
by formulating a continuous time optimization problem
aimed at minimizing a generic cost function (e.g., the
energy, throughput, or bandwidth requirement).

• For the ﬁrst scenario, where caching is performed at the
SBS: (i) we show that, within each time slot, it is optimal
to cache data at a constant rate, which permits reformu-
lating the problem as a convex program; (ii) we solve
this convex problem by means of dual decomposition and
propose a subgradient algorithm to obtain the optimal
dual variables; and (iii) we derive the structure of the
optimal transmission power at the MBS and the caching
policy at the SBS.

• For the second scenario, where information is cached at
the users and shared through D2D communications: (i)
we show that, within each time slot, each user should
cache data at a constant rate; (ii) we show that each user
should transmit the ﬁles in the D2D links at a constant
rate; and (iii) we reformulate the problem as a convex
optimization problem.

• Finally, the two scenarios are compared through numer-
ical simulations. Fist, we compare the performance of a
centralized cache with a distributed one and assess the
impacts of pre-downloading and local caching gains in
each scenario. Second, we evaluate how the cost of the
MBS increases with the economical incentives requested
by users for transmitting data over the D2D links.

The remainder of the paper is structured as follows. Section
II focuses on the ﬁrst scenario, where caching is performed
at an SBS. In particular, the system model is presented in
Section II-A; the optimal transmission strategy is derived for
a ﬁxed caching policy in Section II-B; and the problem is
solved in Section II-C. Section III is devoted to the second
scenario where caching is performed at the user terminals.
The system model for this scenario is introduced in Section
III-A and the resulting problem is solved in Section III-B.
Section IV presents the numerical results. Finally, the paper
is concluded in Section V.

Notation: Vectors and vector valued functions are denoted
by lower case boldface letters, i.e., v and ρ(v), respectively.
u=1 deﬁnes a column vector obtained by stacking the col-
(vu)U
umn vectors v1, . . . , vU and [v]k returns the k-th element of
the vector v. Symbol (cid:22) denotes the component-wise “smaller
than or equal to” inequality. Finally, [x]+ , max{0, x}.

II. SBS CACHING FOR 5G NETWORKS

A. System model and problem formulation

As depicted in Fig. 1, we consider U users served by an
SBS. The SBS has a ﬁnite cache memory of capacity C units,
and is connected through a wireless backhaul channel to an

SBS

d1(t) = s1(t)

3

l2
T s

l0 = 0

l3
T s

Ts

2Ts

3Ts

T

(a)

t

f0 = ∅

f3

f2

l4
T s

l2
T s

l1
T s

f1

0

d2(t)

l0 = 0

0

0

s2(t)

Cache of capacity C

d1(t)

User 1

r(t)

MBS

R
e
c
e
i
v
e
r

rM (t)

Pre-downloading

buffer

+

α

s(t)

Local caching

buffer

ℓ(t)

c(t)

d2(t)

dU (t)

T
r
a
n
s
m

i
t
t
e
r

User 2

User U

Fig. 1. System model when caching is performed at an SBS that serves U
users.

MBS, which has access to the core network. We assume that
the MBS and the SBS operate in different frequency bands;
thus, no interference is produced between the two. We deﬁne
T as the optimization time horizon consisting of N time slots
of duration Ts each. In each time slot, each user, u, u =
1, . . . , U , requests one ﬁle from the set of all possible ﬁles
F = {f0, . . . , fF }. We deﬁne lj as the length (in data units) of
ﬁle fj, j = 0, . . . , F . File f0 has length l0 = 0 and represents
slots without requests. Similarly to [19], we ﬁx the duration
of each ﬁle in the set F to the duration of one time slot, Ts.1
File fj is consumed at a constant rate lj/Ts by the users.

ated to user u. The downloaded data, r(t) = PU

As shown in Fig. 1, data is transmitted by the MBS at a
rate r(t). The SBS receives this information and separates the
streams associated to different user requests, obtaining the rate
vector rM (t) = (ru(t))U
u=1 with ru(t) being the rate associ-
u=1 ru(t), is
then stored at the SBS cache until it is served to the users
(which, without loss of generality, can happen immediately).
The SBS has a demand rate denoted by s(t) to satisfy the
users’ demand rates, du(t), ∀u. As it will be explained later,
the SBS demand rate is obtained as the sum of the users’
demand rates, du(t), after removing multiple demands for the
same ﬁle within the same slot. When serving a content to
a user, the SBS either deletes or locally caches it. This is
dictated by the local caching rate c(t). Notice that the cache
is represented with two different virtual buffers, namely, the
pre-downloading and local caching buffers. This representation
with virtual buffers allows us to distinguish between the
cached data that is downloaded in advance from the MBS
from the locally cached data that is used to reduce future
requests from the MBS. In this context, we deﬁne the vector
u=1 whose u-th component ℓu(t) denotes the
ℓ(t) , (ℓu(t))U
rate at which data is removed from the local caching buffer
to reduce the demand at time t from the MBS associated to
u-th user request2. In the sequel, we provide formal deﬁnitions
for du(t), s(t), and c(t). As in [17], [18], we assume a
known demand proﬁle (i.e, ofﬂine approach, see Section I);
accordingly, we assume that the demand variables du(t) and
s(t) are known for the period [0, T ].

We deﬁne δu(j, n) as the user request indicator variable

1Note that any generic ﬁle can be partitioned into smaller ﬁles to meet the

requirement of having the same duration Ts.

2 In Fig. 1 we represent the locally cached data as a feedback link from the
output to the input. Note that the data removed from the local caching buffer,
ℓ(t), can be instantaneously cached again if dictated by the local caching rate
c(t) (implying that in practice the content is not removed from the cache).

l3
T s

f3

T

Ts

2Ts

3Ts

f4

l4
T s

f0 = ∅

l0 = 0

f2

l2
T s

s42 = 0

Ts

2Ts

3Ts

T

f0 = ∅

f4

f2

f3

(b)

(c)

t

t

Fig. 2.
(a) and (b) denote the demand rates of users one and two, respectively.
The values above the curves represent the value of dnu. User 1 requests the
ﬁles f1, f2, f0, and f3, and user 2 requests f0, f4, f2, and f3 in this
order. The user request indicator variable for user 1 takes values δ1(1, 1) =
δ1(2, 2) = δ1(0, 3) = δ1(3, 4) = 1, and 0, otherwise; similarly, for user
2, δ2(0, 1) = δ2(4, 2) = δ2(2, 3) = δ2(3, 4) = 1, and it is 0, otherwise.
(c) shows the required SBS demand rate for user 2 (for user 1, we have
s1(t) = d1(t) as shown in (a)). The values above the curves represent the
value of sn2. Note that sn2 = dn2 for all the slots except the fourth one,
where we have s42 = 0, because the two users request ﬁle f3 in the fourth
slot; thus, we have σ2(3, 4) = 0.

that takes value 1 when user u requests ﬁle fj in slot n, n =
1, . . . , N , and 0, otherwise. Since each user requests one ﬁle

j=0 δu(j, n) = 1, ∀u, n.

from F per slot, we have PF
data from the SBS, i.e., du(t) , PN
user at the n-th time slot, i.e., dnu =PF

Deﬁnition 1 (User demand rate). The demand rate of user u,
du(t) ≥ 0, t ∈ [0, T ], is the rate at which user u requests
n=1 dnu rect((t − (n −
1/2)Ts)/Ts), where dnu denotes the demand rate of the u-th
j=0 δu(j, n)lj/Ts; and
rect((t − a)/b) stands for the rectangular function centered at
a with duration b.

Figs. 2(a)-(b) depict users’ demand rates when two users are
served from the SBS. Note that if a certain ﬁle is requested
by multiple users at the same time slot (as in the fourth time
slot in Fig. 2), these requests can be simultaneously handled
by the SBS without the need of downloading the same ﬁle
multiple times from the MBS. Therefore, in order to determine
the minimal demand rate of the SBS, we must account only
once for simultaneous requests of the same ﬁle within one
slot. Without loss of generality, we account for the request of
the user with the smallest index u. Accordingly, we deﬁne the
SBS request indicator variable σu(j, n) that takes value 1 for
the user with the smallest index u requesting ﬁle fj in time
slot n (i.e., σu(j, n) = 1 if δu(j, n) = 1 and u < u′, ∀u′ 6=
u : δu′ (j, n) = 1), and 0, otherwise.

Deﬁnition 2 (SBS demand rate). The demand rate of the SBS
is denoted by the vector s(t) = (su(t))U
u=1, t ∈ [0, T ]. The
u-th component of this vector, su(t) ≥ 0, identiﬁes the rate
at which the data corresponding to the u-th user request must
be available at the SBS to fulﬁll this request. Thus, we have
n=1 snu rect((t − (n − 1/2)Ts)/Ts), where snu
denotes the SBS demand rate at the n-th slot for the u-th user

su(t) , PN

request, snu =PF

j=0 σu(j, n)lj/Ts.

Given the users’ demands in Figs. 2(a)-(b), the associated
SBS demand rates are shown in Figs. 2(a) and 2(c), respec-
tively. Note that if the SBS demand, s(t), is satisﬁed, then the
SBS can serve all the user requests, du(t), ∀u. In the remainder
of this section, unless it is stated otherwise, by a user request
we refer to the request seen by the SBS, su(t), instead of the
request on the user side, du(t).
Deﬁnition 3 (Local caching rate). The local caching rate is
represented by the vector c(t) , (c(t, u))U
u=1 whose u-th
component, c(t, u), denotes the rate at which the SBS caches
the content associated to user u at time t. Thus, we have
0 ≤ c(t, u) ≤ su(t), t ∈ [0, T ].

Remark 1. The variables in the system model must be able to
indicate which portions of each ﬁle are cached at time t. Given
a certain user request u and time instant t, we can identify
the ﬁle being requested (through the SBS indicator variables).
Then, the cached portions of a ﬁle can be identiﬁed with the
tuple {rM (t), c(t), s(t)}. Note that we could have deﬁned the
vectors rM (t), c(t), and s(t) in terms of the ﬁles instead of
user requests (i.e., with dimensions F × 1 instead of U × 1),
which would simplify the identiﬁcation of the cached ﬁles;
however, this would dramatically increase the computational
complexity of the algorithms proposed in the remainder of the
paper as, in general, the number of available ﬁles, F , is several
orders of magnitude larger than the number of users connected
to the SBS, U .

MBS, r , {r(t)}T
SBS, c , {c(t)}T

in the backhaul link, R T

Our aim is to jointly design the transmission policy at the
t=0, and the local caching policy at the
t=0, to minimize a generic cost function
0 g(r(τ ))dτ, where g(r(t)) denotes
the instantaneous cost, which depends on the instantaneous
transmission rate at the MBS. As in [22], we assume that
the instantaneous cost function g(·) is time invariant, convex,
increasing, continuously differentiable, and g(0) = 0. In the
following, we give four examples of cost functions that satisfy
these conditions:
1) Energy consumption minimization: If the objective is
to minimize the total network energy consumption, then the
instantaneous cost is given by the instantaneous total power
consumption, p(t). In the case of Gaussian signaling, we
have p(t) = g(r(t)) = (exp(r(t)) − 1)/h + Pc + pS(t),
where h denotes the channel gain, Pc stands for the static
circuitry consumption at the MBS, and pS(t) is the SBS power
consumption, which is known as it can be computed from the
power-rate function at the SBS and the users’ demands.
2) Energy cost minimization: The instantaneous power con-
sumption above has to be multiplied by the energy cost, ξMBS,
paid by the network operator to the electricity utility. Thus, the
instantaneous cost function is g(r(t)) = ξMBS · ((exp(r(t)) −
1)/h + Pc + pS(t)).
3) Bandwidth minimization: In this case, the cost function
is given by the bandwidth-rate function, w(t) = g(r(t)) =
f −1(r(t)), obtained as the inverse of the rate-bandwidth
function, r(t) = f (w(t)). Again, in the case of Gaussian
signaling, we have r(t) = f (w(t)) = w(t)log(1 + P h/w(t)),

4

where P denotes the constant transmission power and h stands
for the channel gain.
4) Trafﬁc minimization: To minimize the data transmitted by
the MBS, we obtain g(r(t)) = r(t).

As argued in the introduction, the cache offers two different
gains to reduce the cost in the backhaul link, namely, pre-
downloading and local caching gains. As shown in Fig. 1,
the cache has two inputs: (i) the pre-downloaded data from
the MBS, which is controlled by the transmission policy at
the MBS, r, and contributes to the pre-downloading caching
gain; and (ii) the locally cached data, which is controlled by
the local caching policy at the SBS, c, and contributes to
the local caching gain. The design of r and c is constrained
by the cache size and the required demand rate at the SBS.
In the following, we deﬁne these constraints in terms of the
cumulative transmitted data [22].

Deﬁnition 4 (Data departure curve). The data departure curve,
D(t, r), is the amount of total data served by the MBS by time
t ≥ 0, and can be obtained from the transmission policy, r,

as D(t, r) ,R t

0 r(τ )dτ.

Due to the ﬁnite cache capacity, an upper bound on D(t, r)
must be imposed to avoid data overﬂows from the SBS cache.
This upper bound is imposed by the maximum data departure
curve that, as deﬁned next, increases as data is removed from
the SBS cache. The rate at which data is removed from the

u=1 su(t) − c(t, u).

cache at time t is obtained as PU

Deﬁnition 5 (Maximum data departure curve). The maximum
data departure curve, B(t, c), limits the maximum amount of
total data that can be transmitted by the MBS by time t ≥ 0
such that no data overﬂow at the cache memory is generated.
u=1 su(τ ) − c(τ, u)dτ

Thus, it is given by B(t, c) , C +R t

and depends on the caching policy c.

0 PU

The lower bound on the data departure curve is given by
the minimum amount of total data that must be downloaded
from the MBS to satisfy the SBS demand rate. The net SBS
demand rate from the MBS (the demand rate at point α in
Fig. 1) is the requested data that is not available in the local
caching buffer. Consider that, at a certain time instant t, user
u requests a ﬁle that had been previously requested by user
u′ at time t′, t′ < t. Then, the net SBS demand rate at time
t of the u-th user request is given by su(t) − ℓu(t), where,
as mentioned earlier, ℓu(t) denotes the rate at which data is
removed from the local caching buffer to reduce the demand
at time t from the MBS. Note that the rate ℓu(t) must be equal
to the caching rate adopted during the previous request of the
ﬁle requested by user u at time t, i.e., ℓu(t) = c(t′, u′) (as
otherwise data is unnecessarily downloaded from the MBS).
To compute the net SBS demand rate for any user and time
instant, we deﬁne the vector function [t′, u′] = ρ(t, u). This
function returns the time instant t′ and the index of the user,
u′, which last requested the ﬁle requested by user u at time
t. If the ﬁle being requested at time t by user u has not been
requested previously, we set ρ(t, u) = [−1, −1], and deﬁne
c(−1, −1) , 0 (since the ﬁles that have not been requested are
not yet available at the SBS). The function ρ(t, u) is depicted
in Fig. 3 for the demand proﬁle in Fig. 2. Using the function

[ρ(t, 1)]1

f1

f2

f0 = ∅

f3

Data

B(t, ˆc)

s11

1

2

s

2

2

s

+

s 3 2

s

4 1

l2

l1 +l2 +l4

A(t, ˆc)

C

0

Ts

Data

B(t, ˜c)

s

2 2

s11

C

0

Ts

3Ts

2Ts

(a)

f2 is cached

s 3 2

s 4 1

l1 +l2 +l4

A(t, ˜c)

3Ts

2Ts

(b)

Ts

0
-1

[ρ(t, 1)]2

Ts

−1

[ρ(t, 2)]1

f0 = ∅

f4

2Ts

3Ts

T

(a)

2

f2

−1

f3

2Ts

Ts

0
-1

[ρ(t, 2)]2

Ts

−1

2Ts

3Ts

T

1

−1

(b)

t

t

Fig. 3. Representation of the functions ρ(t, u) that maps a certain ﬁle request
in the SBS to its previous occurrence in time (t′ = [ρ(t, u)]1) and user index
(u′ = [ρ(t, u)]2). (a) corresponds to the ﬁrst user and (b) to the second one.

ρ(t, u), the net SBS demand rate at time t associated with the
u-th user request is given by su(t) − c(ρ(t, u)). Since non-
causal knowledge of the user demands is available (ofﬂine
approach), the function ρ(t, u) is known, ∀t, u. Next we deﬁne
the minimum data departure curve to satisfy the SBS demand.

Deﬁnition 6 (Minimum data departure curve). The minimum
data departure curve, A(t, c), is the minimum amount of total
data that must be transmitted by the MBS by time t ≥ 0 to
satisfy the SBS demand, and depends on the caching policy, c,
0 su(τ ) −

0 su(τ ) − ℓu(τ )dτ =PU

u=1R t

i.e., A(t, c) ,PU

c(ρ(τ, u))dτ.

u=1R t

Bearing all the above in mind, the problem is mathemati-

cally formulated as follows:

min

{r(t),c(t)}t∈[0,T ]

0 g(r(τ ))dτ

R T

s. t. D(t, r) ≤ B(t, c),

D(t, r) ≥ A(t, c),

r(t) ≥ 0,

0 (cid:22) c(t) (cid:22) s(t),

(1a)

∀t ∈ [0, T ], (1b)
∀t ∈ [0, T ], (1c)
∀t ∈ [0, T ], (1d)
∀t ∈ [0, T ], (1e)

where the constraint (1b) prevents cache overﬂows, and (1c)
imposes the fulﬁllment of the users’ demands. The constraints
(1d) and (1e) guarantee feasible transmission and local caching
rates. Note that a feasible caching policy, c, must satisfy
B(t, c) ≥ A(t, c) for all t ∈ [0, T ], and any feasible data
departure curve must lie within the tunnel between B(t, c)
and A(t, c).
Remark 2. In the problem formulation, we have assumed that
cached data can only be removed from the cache during the
subsequent requests of the same data. As a result, by caching
data the net SBS demand rate will be reduced. We remark here
that this assumption is without loss of optimality. Contrarily,
consider a policy that caches a certain data content at time t1,
its subsequent request occurs at t2, but the content is deleted
at t3 ∈ (t1, t2). As this content has to be downloaded again at

5

l3

t

T

l3

t

T

Fig. 4. Representation of the problem for two different caching policies. In
this example, we have set C = l2.

t2, this policy is unnecessarily using cache space in (t1, t3).
Remark 3. In realistic 5G scenarios, several SBSs will be
served by the same MBS. This work considers that the MBS
assigns orthogonal resources to each SBS and that the SBSs
have non overlapping coverage areas. As a result, a problem
of the form of (1) is obtained for each SBS. Further gains can
be achieved by multicasting information to different SBSs, or
by cooperation among SBSs with overlapping coverage areas
[5], [23]. However, this will inherently couple the design of
the SBSs’ caching policies, and is out of the scope of this
work.

B. Optimal transmission strategy for a ﬁxed caching policy

In this section, we derive the optimal transmission strategy
for a ﬁxed caching policy. Interestingly, when the local caching
policy, c, is given, the problem in (1) accepts an intuitive
graphical representation. For example, under the SBS demand
rate in Figs. 2(a) and 2(c), the problem is represented in Fig.
4 for two different caching policies:

Policy 1: The policy ˆc shown in Fig. 4(a) removes the data
from the cache as soon as it is served to a user, ignoring any
possible future requests for the same ﬁle, i.e., ˆc(t) = 0, ∀t.
Consequently, it only exploits the pre-downloading caching
gain. This caching policy was proposed in [18]. Observe that
if ˆc(t) = 0, ∀t, then there is a constant gap of C between
the lower and upper bounds, i.e., B(t, ˆc) = C + A(t, ˆc) (c.f.
Deﬁnitions 5 and 6). The optimal data departure curve exploits
this gap by pre-downloading data.

Policy 2: The policy ˜c shown in Fig. 4(b) caches the ﬁle
f2, when requested by user 1 in the second time slot, thus
anticipating the next request in the third slot by user 2, i.e.,
˜c(t, 1) = s21 if t ∈ [Ts, 2Ts] and ˜c(t, u) = 0, otherwise. As
a result, no data needs to be transmitted by the MBS in the
third slot.

Lemma 1 (Constant rate transmission is optimal [22]). Given
a feasible caching policy c, the optimal data departure curve
can be obtained as the tightest string whose ends are tied to the
origin and the point (T, A(T, c)), which is represented in Fig.
4 with the dashed lines. In particular, if the instantaneous cost,

g(·), is strictly convex, then this is the unique optimal data
departure curve; contrarily, if g(·) is linear multiple optimal
departure curves exist.

The free memory space in the cache can be obtained as
B(t, c) − D(t, r), ∀t. Focusing on Policy 1 (see Fig. 4(a)), the
cache is full at t = Ts, and all the data in the cache belongs
to f2 and/or f4, which have been pre-downloaded to equalize
the rates in the ﬁrst and second time slots. As for Policy 2
(see Fig. 4(b)), the cache is full at t = 2Ts, and exclusively
contains f2. Note that by caching f2 in the second slot the
upper bound is tightened (the net cache capacity is reduced)
while the lower bound is relaxed (the demand at the third slot
is reduced).

From the previous discussion, two questions arise: i) “which
of the two caching policies achieves the lowest MBS cost?”,
and ii) “is any of these policies the optimal one?”. One might
be tempted to think that the caching policy ˜c has a lower
cost since fewer data has to be transmitted; however, this does
not necessarily hold true since the caching policy ˆc might
achieve a lower cost by equalizing the rate across time slots.
In practice, the jointly optimal transmission and local caching
policies must be obtained by solving (1), which turns out to be
challenging since this problem belongs to the class of inﬁnite-
dimensional optimization problems [24].
C. Jointly optimal caching and transmission policies

To solve the inﬁnite-dimensional problem in (1), we ﬁrst
derive some structural properties of the optimal strategy. Then,
leveraging on these properties, we will formulate (1) as a
ﬁnite-dimensional convex program of affordable complexity.
As shown next, the optimization variables of the resulting
problem are the amount of data to be cached in each slot
for each request, qnu, ∀n, u, and the transmission rate of the
MBS at each slot, rn, ∀n.

As illustrated in Fig. 4, the caching policy changes the shape
of the upper and lower bounds on the data departure curve. For
example, in Fig. 4(b), we have observed that the data locally
cached in the second slot reduces the demand in the third slot.
Since the caching rate can have continuous variations over
time, we can potentially have arbitrary non-decreasing curves
as the upper and lower bounds, B(t, c) and A(t, c). However,
these curves are coupled through the caching policy c. In other
words, the caching rate of a certain request determines the
reduction in the demand rate of the subsequent request. The
following lemma shows that (within a time slot) caching data
at a constant rate turns out to be optimal.

Lemma 2 (Constant rate caching is optimal). The (not nec-
essarily unique) optimal local caching rate is a step-wise
function that can be written as c⋆(t) = (c⋆(t, u))U
u=1, where
nu/Ts) rect((t−(n−1/2)Ts)/Ts), and q⋆
nu
denotes the optimal amount of cached data for the request of
the u-th user at slot n.

c⋆(t, u) =PN

n=1(q⋆

Proof: See the Appendix.

Since su(t) and c⋆(t, u) are step-wise functions whose value
can only change at slot transitions, we know that A(t, c⋆)
and B(t, c⋆) are piece-wise linear functions (c.f. Deﬁnitions
5 and 6) whose slopes can only change at slot transitions.

Consequently, we can obtain the following properties of the
optimal transmission strategy.

6

Lemma 3. The (not necessarily unique) optimal data depar-
ture curve, D⋆(t, r⋆), can be written as a piece-wise linear
function, whose rate (or, equivalently, the slope of D⋆(t, r⋆))
may only change at time instants n · Ts, n = 1, . . . , N − 1,
n rect((t − (n − 1/2)Ts)/Ts), where r⋆
n
denotes the optimal transmission rate of the MBS at the n-
th slot. Additionally, if the rate increases at the n-th slot
transition (r⋆
n+1), then D⋆(nTs, r⋆) = B(nTs, c⋆); and
if the rate decreases at the n-th slot transition (r⋆
n+1),
then D⋆(nTs, r⋆) = A(nTs, c⋆).

i.e., r⋆(t) = PN

n > r⋆

n < r⋆

n=1 r⋆

Proof: The proof follows similarly to [25, Lemmas 5 and
max(ℓm), and

6] by identifying nTs as ℓm, B(nTs, c⋆) as D(m)
A(nTs, c⋆) as D(m)

min(ℓm).

From Lemmas 2 and 3, we can equivalently rewrite the
original problem in (1) as a function of the MBS rates at each
slot, r , (rn)N
n=1, and cached data units at the SBS for each
user request and time slot, q , ((qnu)U

u=1)N

n=1:

N

Tsg(rn)

(2a)

n

U

Tsrℓ ≤ C +

Tssℓu − qℓu,

∀n,

(2b)

Xu=1

Tsrℓ ≥

Tssℓu − q¯ρ(ℓ,u),

min
r,q

s. t.

n

Xn=1
Xℓ=1
Xℓ=1

n

U

Xℓ=1
Xu=1

n

Xℓ=1

rn ≥ 0,
0 ≤ qnu ≤ Tssnu,

∀n,

(2c)

∀n,
∀n, u,

(2d)
(2e)

where the constraints (2b)-(2e) correspond to the discrete
versions of the constraints in (1b)-(1e), respectively. The
function (n′, u′) = ¯ρ(n, u) returns the slot, n′, and user, u′, of
the previous request of the ﬁle associated to (n, u), or returns
(−1, −1) if it is the ﬁrst request of the ﬁle. Note that ¯ρ is the
discrete version of the function ρ; and as before, we deﬁne
q−1−1 , 0.
Remark 4. In (2), we have considered that the amount of
cached data, qnu, is a nonnegative real number. Note that if we
introduce an integer constraint to enforce data unit granularity
(e.g., bit), then the problem in (2) becomes an integer program
with its inherent complexity. In practice, as the data unit
granularity (bit) is sufﬁciently small in comparison to the ﬁles
sizes (of several Mbits) and cache capacity (of several Gbits),
the integer constraint can be relaxed without jeopardizing the
performance. Consequently, (2) is a convex program (since the
objective function is convex and the constraints are afﬁne),
and, thus, can be solved efﬁciently.

By studying the Karush Kuhn Tucker conditions of the
primal problem in (2), it is difﬁcult to derive the structure
of the optimal solution {r⋆, q⋆} due to the constraints in (2b)
and (2c) that couple the optimization variables. However, the
structure of the optimal primal variables can be obtained by
resorting to dual decomposition. From convex optimization
theory [26], the solution of the dual problem, maxλ,µ δ(λ, µ),

Algorithm 1 Projected subgradient
Initialization:

Set k := 0 and initialize λ(0) and µ(0) to any value such that λ(0) (cid:23) 0,
µ(0) (cid:23) 0.
Step 1: If a termination condition is met, the algorithm stops.
Step 2: Compute r(k), q(k) as the solution to the problem in (3) given the
current multipliers, λ(k) and µ(k).
Step 3: Update the dual variables
[λ(k+1)]n = λ(k+1)

and [µ(k+1)]n = µ(k+1)

following the subgradient,

, ∀n, with

i.e.,

n

n

λ(k+1)

n

µ(k+1)
n

= "λ(k)
= "µ(k)

n + ǫ(k) −C +
n − ǫ(k)  n
Xℓ=1

Step 4: Set k := k + 1 and go to Step 1.

Tsr(k)

ℓ −

n

Xℓ=1

U

Xu=1

Tsr(k)

ℓ −

Tssℓu − q(k)

U

Xu=1

Tssℓu − q(k)

+

ℓu !#
¯ρ(ℓ,u)!#

+

.

n=1 and µ = (µn)N

provides a lower bound on the primal problem in (2). We
have deﬁned λ = (λn)N
n=1, where λn and
µn are the Lagrange multipliers associated to the n-th cache
capacity and demand constraints, respectively. The function
δ(λ, µ) stands for the dual function that is deﬁned as follows
[26]:

δ(λ, µ) = min
r,q
s. t.

L(r, q, λ, µ)

(3)

rn ≥ 0, ∀n,

0 ≤ qnu ≤ Tssnu, ∀n, u,

i.e.,
ℓ=1 Tsrℓ −

where L(r, q, λ, µ)

denotes

the

Lagrangian,

u=1 Tssℓu −q¯ρ(ℓ,u)(cid:1).

L(r, q, λ, µ) = PN
u=1 Tssℓu −qℓu(cid:1)−µn(cid:0)Pn
PU

n=1 Tsg(rn) + λn(cid:0) − C + Pn

ℓ=1 Tsrℓ −PU

Since the primal problem in (2) is convex and the Slater
constraint qualiﬁcation holds, the duality gap (difference be-
tween the optimal values of the primal and dual problems) is
zero [26]. To solve the dual problem, we have implemented
the projected subgradient method, presented in Algorithm 1,
that guarantees convergence to the optimal dual variables, λ⋆
and µ⋆, if the updating step size ǫ(k) is correctly chosen [27].
Step 2 of Algorithm 1 requires to solve the problem
in (3), where the optimization variables are r, q, and the
Lagrange multipliers (λ and µ) are ﬁxed. To do so, we
ﬁrst rewrite the Lagrangian by reordering the sums over
n and ℓ, which allows us to separate the terms associated
n=1 Tsg(rn) −
ℓ=n λℓ −
−
u=1 Tssnu). The function ψ(n, u)
returns the slot index of the subsequent request of the ﬁle
being served at slot n to user u. Now, the problem in (3) is
decoupled in the optimization variables (r and q) and can be
easily solved by decomposing it into the following simpler
subproblems:

to each rn and qnu, i.e., L(r, q, λ, µ) = PN
rnTs(cid:0)PN
u=1 qnu(cid:0)PN
n=1PU
PN
ℓ=ψ(n,u) µℓ(cid:1)
ℓ=1 λℓ(C + Pℓ
PN

ℓ=n µℓ − λℓ(cid:1) + PN

n=1PU
ℓ=1 µℓ(Pℓ

+ PN
n=1PU

u=1 Tssnu)

min
rn≥0

min

0≤qnu≤Tssnu

Tsg(rn) − rnTs  N
µℓ − λℓ! ,
Xℓ=n
µℓ
Xℓ=ψ(n,u)
 , ∀n, u.

qnu


Xℓ=n

λℓ −

∀n,

N

N

(4)

(5)

7

Let ¯rn be the solution to the equation dg(rn)/drn =
ℓ=n µℓ − λℓ. If ¯rn is real and positive, the optimal solution

to (4) is r⋆

n(λ, µ) = ¯rn; otherwise, it is r⋆

n(λ, µ) = 0.

PN

Corollary 1. When the objective is the minimization of
the energy consumption over the backhaul link (g(rn) =
(exp(rn) − 1)/h), the optimal solution to (4) is found as
r⋆
ℓ=n µℓ − λℓ) > 1
and r⋆

ℓ=n µℓ − λℓ)) if h(PN

The solution to (5) is

n(λ, µ) = 0, otherwise.

n(λ, µ) = log(h(PN
nu(λ, µ) =

ℓ=n λℓ − PN
with Wnu , PN
n(λ(k), µ(k))(cid:1)N
and q(k) = (cid:0)(cid:0)q⋆
(cid:0)r⋆

0
Tssnu
¯qnu ∈ [0, Tssnu]

ℓ=ψ(n,u) µℓ. Accordingly, the
primal variables at the q-th iteration of the subgradient, which
are necessary in Step 2 of Algorithm 1, are given by r(k) =
,
n=1
where λ(k) and µ(k) denote the Lagrange multipliers at the
q-th iteration of the subgradient.

nu(λ(k), µ(k))(cid:1)U

if Wnu > 0,
if Wnu < 0,
if Wnu = 0,

(6)

u=1(cid:1)N

q⋆

n=1

When the subgradient algorithm converges to the optimal
Lagrange multipliers, {λ⋆, µ⋆}, the duality gap is zero, i.e., the
optimal solution of the dual and primal problems are the same.
Note that given the optimal dual variables, {λ⋆, µ⋆}, there
might be multiple minimizers of the problem in (3). Precisely,
nu(λ⋆, µ⋆) can take multiple values when Wnu = 0 (see
q⋆
(6)). Then, the optimal primal variables {r⋆, q⋆} are within the
set of minimizers of δ(λ⋆, µ⋆) in (3); in particular, {r⋆, q⋆}
are the minimizers that are feasible in the primal problem
(2) and satisfy the slackness conditions [27]. In practice, to
avoid waiting until the exact convergence to {λ⋆, µ⋆}, the
average across iterations of the primal iterates can be used as
an approximate solution to the problem in (2) [28].

Interestingly, it turns out that the parameter Wnu, which
only depends on the Lagrange multipliers, characterizes the
caching policy: if Wnu is positive, the associated ﬁle is not
cached; while, if Wnu is negative the ﬁle is completely cached;
and, ﬁnally, if Wnu = 0 the SBS caches a portion of the ﬁle
(the exact amount of cached data units must be obtained as
mentioned in the previous paragraph). Additionally, from the
expression of Wnu, we observe that the SBS caching policy
prioritizes the ﬁles that are requested again in the near future.3

III. CACHING AT USER DEVICES

A. System model and problem formulation

In this section, as depicted in Fig. 5, we consider a region
in space covered by an MBS that must serve the demand of U
users. The MBS allocates an orthogonal channel to each user,
whose transmission rate is denoted by ru(t), u = 1, . . . , U .
We assume that users cooperate with the network operator
(possibly in exchange of incentives) by acting as an SBS for
the rest of the users through dedicated D2D links. We consider

3 From the KKT optimality conditions, we have µn > 0 if the n-th demand
constraint in (2c) is satisﬁed with equality (and zero otherwise). Consider
that users u and u′ request different ﬁles and that the subsequent request of
these ﬁles appears ﬁrst for user u, i.e., ψ(n, u) < ψ(n, u′). Then, from the
expression of Wnu in (6), we have Wnu ≤ Wnu′ and, as a result, the SBS
prioritizes caching the ﬁle requested by user u.

MBS

( t )

r

1

r2(t)

rU(t)

User (SBS) 1

ˆr1(t, U )

ˆrU (t, 1)

ˆr

1(t,2)

User (SBS) 2

User (SBS) U

ˆr 2 ( t , U )

Fig. 5. System model when caching is performed at the user devices, which
act as an SBS for the other users through D2D communications.

MBS

ru(t)

SBSs

User

Module

Cache of capacity Cu

Pre-downloading

buffer

Local caching

buffer

+

α

+

ℓu(t)

cu(t, u′), ∀u′ 6= u

SBS Module

cu(t, u)

ˆru′ (t, u), ∀u′ 6= u

Aplication

du(t) =
du(t, u)

D2D

du(t, u′) =
ˆru(t, u′), ∀u′ 6= u

Fig. 6. Block diagram of the u-th user terminal. The solid lines correspond
to data streams associated to trafﬁc of the u-th user, and the dashed ones
represent trafﬁc served to other users, u′ 6= u, that is transmitted through the
D2D links.

that users are closely located; thus, a certain user u can act
as an SBS for any other user u′ 6= u. The rate in the D2D
link from SBS (user) u to user u′ at time t is denoted by
ˆru(t, u′), u′ 6= u = 1, . . . , U . We assume that the D2D links
operate over different frequency resources than those used by
the MBS. A user is allowed to download only its own trafﬁc
from the MBS; that is, users do not download content that is
of no interest to them, solely to serve another user. However,
downloaded ﬁles can be locally cached to later serve other
users through the D2D links.
As represented in Fig. 6,

the D2D user terminals are
composed of two main modules: the user module and the SBS
module. The user module acts exactly as a user terminal in
the previous scenario, i.e., it receives data from the SBSs (now
from the SBS modules of other users) and feeds it directly to
the application layer. For fair comparison with the previous
scenario, we do not allow data to be cached within the user
module. As a result, if user u caches data at time t to reduce
the demand from the MBS of another user u′ at time t′, t′ > t,
then user u must send this data over the D2D link at time t′,
and not earlier.

The SBS module at the user terminal essentially acts as the
SBS terminal in the previous scenario; the main difference is
that the SBS module in the u-th user terminal is allowed to
download only contents corresponding to its own demand from
the MBS, i.e., du(t, u) , du(t). This module contains a cache
memory of capacity Cu, represented with two different virtual
buffers to ease interpretation. The ﬁrst virtual buffer is used to

8

represent pre-downloaded contents from the MBS associated
to the u-th user’s demand. The second virtual buffer represents
locally cached data from previous demands. When the u-th
user serves its demand, at a rate du(t, u), to the application
layer, data can be cached at a rate cu(t, u) ≤ du(t, u). This
cached data can be later requested by other users; in particular,
du(t, u′) denotes the demand of user u from user u′ at time t,
which is served through the D2D links; accordingly, we have
du(t, u′) = ˆru(t, u′). The caching rate associated to demand
du(t, u′) is denoted by cu(t, u′). Finally, ℓu(t) denotes the rate
at which data is removed from the local cache of user u to
reduce its own demand, which is used if a ﬁle is requested
twice by the user. To simplify the notation, in the remainder
of the paper we refer to variable ℓu(t) as ˆru(t, u). However,
note that this data stream does not require D2D resources.

u′=1)U

As before,

the aim is to jointly design the transmis-
u=1, ˆr(t) ,
sion and caching rates, rM (t) , (ru(t))U
u=1, and c(t) , ((cu(t, u′))U
u=1, that
((ˆru(t, u′))U
u′=1)U
minimize a general cost function on the rates of the MBS,
rM (t), and D2D links, ˆr(t). This cost function accounts for:
(i) the cost of transmissions from the MBS to different users,
u=1 gu(ru(t)), with gu(ru(t)) standing for the instanta-
neous cost of the u-th link; and (ii) the cost of transmis-

PU
sions over the D2D links,PU

u=1Pu′6=u ˆguu′ (ˆru(t, u′)), where

ˆguu′(ˆru(t, u′)) denotes the cost of the D2D link from user
u to user u′. Again, we assume that the functions gu(·),
ˆguu′(·), ∀u, u′
6= u, are time invariant, convex, increasing,
continuously differentiable, and gu(0) = 0, ˆguu′ (0) = 0,
6= u.4 As in Section II, different objective functions
∀u, u′
can be modeled by appropriately selecting the cost functions
gu(·) and ˆguu′(·) (e.g., energy consumption, energy cost,
bandwidth, or trafﬁc minimization). As mentioned earlier,
the operator must give incentives to users that transmit over
the D2D links [15]. In this context, ˆguu′ (·) can represent
the economical incentive, ξU ≥ 0, paid by the operator to
the users for the data transmitted over the D2D links, i.e.,
ˆguu′(ˆru(t, u′)) = ξU ˆru(t, u′). The total incentive of user u

operator is the sum of the cost of energy used by the MBS,

is R T
t=0Pu′6=u ξU ˆru(t, u′). The total economical cost of the
0 ξMBS(cid:16)PU
u=1 W/U (exp(ru(τ )U/W ) − 1)(cid:17) dτ, and the
R T
incentives paid to the users, R T
u=1Pu′6=u ˆru(τ, u′)dτ.

Observe that, if this cost function is adopted, no channel state
information of the D2D links is required at the MBS.

0 ξUPU

For problem tractability, we forbid users to simultane-
ously cache the same data by including the constraint
∀u. The implications of this

u′=1 cu′ (t, u) ≤ du(t),

assumption are later discussed in Remark 5.

PU
Du(t, ru) = R t

Since we have one dedicated MBS link per user, we need
a data departure curve from the MBS to each user u, i.e.,
0 ru(τ )dτ. The u-th data departure curve is
constrained from above by the u-th user cache capacity, Cu
(through the maximum data departure curve Bu), and from
below by its net demand from the MBS (through the minimum
data departure curve Au).

Next, we derive the expression of the minimum data de-

4 Notice that by considering a different cost function for each link, it is

possible to model, among others, different channel gains.

parture curve for the transmission from the MBS to the u-th
user, to fulﬁll the user’s demand du(t). Since from the previous
assumption the users’ cache contents are non-overlapping, the
net demand of user u from the MBS (i.e., the demand in point
α in Fig. 6) is obtained by subtracting from du(t) the sum of
the rates in the D2D links to user u and the locally cached
data at user u, ℓu(t) = ˆru(τ, u). Thus, the lower bound on

The maximum data departure curve at user u, Bu(t, c, ˆr), can
be obtained by adding Cu to the minimum data departure
curve, Au(t, ˆr), and subtracting the locally cached data, i.e.,

0 du(τ ) −P∀u′ ˆru′ (τ, u)dτ.

Du(t, ru) reads as Au(t, ˆr) , R t
Bu(t, c, ˆr) , Cu+Au(t, ˆr)−R t

where the locally cached data is computed as the integral up
to time t of the difference between the data rates entering and
leaving the local caching buffer.

0 P∀u′ (cu(τ, u′)−ˆru(τ, u′))dτ,

Bearing all

the above in mind,

the problem with opti-
mization variables {rM (t), ˆr(t), c(t)}t∈[0,T ] is mathematically
formulated as follows:

minZ T

0

U

Xu=1


gu(ru(τ )) + Xu′6=u

ˆguu′ (ˆru(τ, u′))


s. t. Du(t, ru) ≤ Bu(t, c, ˆr),

dτ (7a)

∀u, t ∈ [0, T ], (7b)
∀u, t ∈ [0, T ], (7c)

Du(t, ru) ≥ Au(t, ˆr),

U

cu′ (t, u) ≤ du(t),

∀u, t ∈ [0, T ], (7d)

Xu′=1

∀u, u′ 6= u, t ∈ [0, T ], (7e)
cu(t, u′) ≤ cu(ρ(t, u′)),
ˆru(t, u′) ≤ cu(ρ(t, u′)),
∀u, u′, t ∈ [0, T ], (7f)
cu(t, u′) ≥ 0, ru(t) ≥ 0, ˆru(t, u′) ≥ 0, ∀u, u′, t ∈ [0, T ],
(7g)

where the constraints in (7b) prevent cache overﬂows at the
users, and those in (7c) impose the fulﬁllment of the users’
demands. The constraints in (7d) impose that the same data
cannot be simultaneously cached by different users, and those
in (7e) and (7f) restrict the maximum caching rate and D2D
transmission rate at user u when serving the requests of other
users u′ 6= u to the rate locally available in the cache of user
u, cu(ρ(t, u′)), respectively. The function ρ(t, u) is deﬁned in
the previous section.5 Finally, the constraints in (7g) impose
nonnegative caching and transmission rates, respectively.
Remark 5. For problem tractability, we have included the
constraint (7d) of caching non-overlapping data at the users.
This assumption is without loss of optimality when the D2D
link costs are equal and linear, i.e., ˆguu′ (x) = ξU x, ∀u, u′ 6= u,
which, as mentioned earlier, characterizes the incentives paid
by the operator to users. Then, there exists an optimal solution
where users do not cache overlapping contents. Further gains
can be achieved by caching overlapping contents in the general
case of unequal or non-linear D2D cost functions. However,

5 The function ρ(t, u) as deﬁned in the previous section is inherently
enforcing that if two users u and u′ request the same ﬁle at a given time slot,
then both users have to download the data that is not cached at other users
from the MBS (i.e, they cannot help each other in the current slot for this
ﬁle). However, we could redeﬁne the function ρ(t, u) to allow instantaneous
transmissions over the D2D links, by making the request of one user to point
the other one, i.e., ρ(t, u′) = [t, u]; thus, only user u downloads the data
from the MBS, which is instantaneously sent to user u′ through the D2D link.

9

due to the corresponding combinatorial structure, the optimal
solution considering overlapping caching at users is elusive,
and is left as an open problem for future work.

B. Jointly optimal strategy

As in Section II, we ﬁrst derive some structural properties
of the caching policy c,
the D2D transmission policy ˆr,
and the data departure curves, Du(t, ru), which allow us to
reformulate the problem with a ﬁnite number of optimization
variables. In the following lemma we show that, within each
slot, it is optimal that the users cache and transmit data at
constant rate.

n=1(q⋆

u(n, u′)/Ts) rect((t − (n − 1/2)Ts)/Ts)

Lemma 4. The (not necessarily unique) optimal caching
rate and D2D transmission rates in problem (7) can be
functions c⋆(t, u′) =
written as a piece-wise constant
and
u(n, u′)/Ts) rect((t − (n − 1/2)Ts)/Ts),
u(n, u′) is the optimal amount of cached data at user
where q⋆
u(n, u′) is the
u for the request of user u′ at slot n, and b⋆
optimal amount of transmitted data from user u to user u′
within slot n.

PN
ˆru(t, u′) =PN

n=1(b⋆

Proof: The proof follows similarly to the proof of Lemma

2 and is omitted for brevity.

Since the optimal local caching rate and D2D transmission
rates are piece-wise constant, we know that the constraints
in (7b) and (7c) are piece-wise linear, and the slopes of the
constraints can only change at some slot transition. From this,
and similarly to Lemma 3, we can prove that the optimal data
departure curve for each user can be written as a piece-wise
linear function. Thus, the associated transmission rate from
the MBS to each user reads as r⋆
nu rect(t −
(n − 1/2)Ts/Ts), where r⋆
nu is the optimal transmission rate
from the MBS to user u at the n-th slot.

u(t) = PN

n=1 r⋆

The problem in (7) can be equivalently rewritten in terms
of cached data qu(n, u′), the transmitted data at the D2D links
bu(n, u′), and the transmission rates from the MBS to each
user at each slot, rnu, as

min

s. t.

Ts
gu(rnu) + Xu′6=u
Xℓ=1(cid:16)dℓuTs+

n

Tsrℓu ≤ Cu +

Ts (cid:19)
ˆguu′(cid:18) bu(n, u′)


(8a)

U

N

n

U

Xn=1

Xu=1
Xℓ=1
Xu′=1
Xℓ=1
Xu′=1

U

n

bu(ℓ, u′) − bu′ (ℓ, u) − qu(ℓ, u′)(cid:17), ∀u, n, (8b)

n

U

dℓuTs −

Xℓ=1

Xu′=1

bu′(ℓ, u),

∀u, n,

(8c)

Tsrℓu ≥

qu′ (n, u) ≤ Tsdnu,

∀n, u, (8d)

qu(n, u′) ≤ qu(¯ρ(n, u′)),
bu(n, u′) ≤ qu(¯ρ(n, u′)),
qu(n, u′) ≥ 0, bu(n, u′) ≥ 0, rn ≥ 0,

∀n, u, u′ 6= u, (8e)
(8f)
(8g)

∀n, u, u′,
∀n, u, u′,

where the constraints in (8b)-(8g) stand for the discrete ver-
sions of the constraints in (7b)-(7g), respectively. The function
¯ρ is deﬁned as in Section II-C.

The problem in (8) is a convex program because the
objective function is convex and the constraints are linear.
Accordingly, it can be efﬁciently solved by, e.g., interior point
methods.

IV. NUMERICAL RESULTS

follows the Zipf distribution, i.e., θj = j−γ/(P|F|

In this section, we assess the performance of the proposed
caching and transmission policies in both scenarios, namely,
the SBS scenario in Section II and the D2D scenario in
Section III. We consider N = 20 time slots of duration 10
seconds each, and F = 2000 video ﬁles. The ﬁle lengths
are uniformly distributed in the interval
[0.3, 150] Mnats
with mean ﬁle length E[ℓj] = 75.15 Mnats. We assume
that the probability of requesting ﬁle fj, θj, is independent
and identically distributed across time slots and users, and
q=1 q−γ).
Parameter γ models the skewness of the ﬁle popularity; when
γ = 0, popularity is uniform and it becomes more skewed
as γ grows [9]. We consider the Shannon power-rate function
g(r(t)) = W (exp(r(t)/W ) − 1), where W is the channel
bandwidth. In the SBS scenario, the MBS allocates the whole
bandwidth W = 10 MHz to communicate with the SBS;
while in the D2D scenario, the MBS splits evenly the total
bandwidth across the U users; as a result the bandwidth in
each subchannel is W = 10/U MHz. The cache capacity C
is alternatively expressed by means of the percentage over the
average requested data per user, i.e., ˆC = (100C)/(N E{ℓj}).
For the D2D scenario,
the total cache capacity is evenly
distributed across users, Cu = C/U . Unless otherwise stated,
we set the number of users to U = 3, the Zipf distribution
parameter to γ = 1, and the total cache capacity to ˆC = 10
(C = 15.03Mnats).

We compare the proposed jointly optimal transmission and
caching strategies, obtained by solving (2) and (8), with four
sub-optimal strategies: the No caching strategy that serves as
a benchmark for comparison with traditional systems with-
out cache memory; the Least Recently Used (LRU) caching
algorithm that always keeps in the cache the most recently
requested ﬁles [29]; the Pre-Downloading Caching Algorithm
(PDCA) that uses the cache only to pre-download data (see
Policy 1 in Section II-A and Fig. 4(a)) [18]; and the Local
Caching Algorithm (LCA) that exploits only the local caching
gain (i.e, the MBS transmits at the net demand rate without
allowing pre-downloading). The optimal policy, the PDCA,
and the LCA are ofﬂine policies as non-causal knowledge of
the ﬁle demand is required; while the No caching and LRU
strategies are online policies that may depend only on the
previous ﬁle requests.

First, in Figs. 7-8, we focus on the energy consumption
at the MBS considering that the users cooperate altruistically
with the MBS (i.e., ˆguu′ = 0, ∀u, u 6= u). This allows
us to fairly compare the SBS and D2D scenarios, and to
evaluate the effect of having a centralized or distributed
cache. Afterwards, in Fig. 9, we consider the cost of the

10

No caching
LRU
LCA
PDCA
Optimal solution

No caching
LRU
LCA
PDCA
Optimal solution

)
z
H
/
J
k
(

z
t
r
e
H

r
e
p

n
o
i
t
p
m
u
s
n
o
c

y
g
r
e
n
E

)
z
H
/
J
k
(

z
t
r
e
H

r
e
p

n
o
i
t
p
m
u
s
n
o
c

y
g
r
e
n
E

2

1.5

1

4

3

2

(a) Caching at the SBS

10

ˆC

20

(b) Caching at the user devices

10

ˆC

20

Fig. 7.
Energy consumption of the MBS with respect to cache capacity
(γ = 1, U = 3). In (a), the ﬁles are cached at the SBS as explained in
Section II. In (b), the ﬁles are cached at the users as explained in Section III.

it

MBS when users cooperate in exchange of an economical
incentive. Figs. 7(a) and 7(b) evaluate the energy consumption
of the MBS for different sizes of the cache capacity in the
SBS and D2D scenarios, respectively. It is observed that in
both scenarios the MBS energy consumption decreases with
the cache capacity for all the caching strategies. When the
jointly optimal transmission and caching strategy is compared
with traditional non-caching solutions,
is observed that
the optimal policy reduces the MBS energy consumption by
53.59% in the SBS scenario, and by 61.78% in the D2D
scenario. This reduction is obtained when the cache capacity is
25% of the average user trafﬁc, i.e., ˆC = 25; however, further
energy savings can be achieved by increasing the total cache
capacity. The performance of the online LRU caching policy
is far from the optimal; however, this was expected as it does
not exploit any information about the future requests. Next,
we assess the performance of policies that exploit only one of
the caching gains, namely, pre-downloading (PDCA) or local
caching gains (LCA). Interestingly, it is observed in the SBS
scenario that LCA achieves more energy savings than PDCA
(see Fig. 7(a)); while in the D2D scenario, PDCA requires
a lower energy consumption than LCA (see Fig. 7(b)). This
different behavior is later argued in the following paragraph as
it depends on the value of γ. Finally, if we globally compare
the two scenarios, we observe that the deployment of SBSs
leads to more reduction in the MBS energy consumption
compared to the D2D scenario. The rationale behind this is
two-fold. First, the expressions for energy consumption in
the objective functions of problems (2) and (8) are different;

)
z
H
/
J
k
(
z
t
r
e
H

r
e
p

n
o
i
t
p
m
u
s
n
o
c

y
g
r
e
n
E

)
z
H
/
J
k
(

z
t
r
e
H

r
e
p

n
o
i
t
p
m
u
s
n
o
c

y
g
r
e
n
E

2

1

0

0

4

3

2

1

0

0

(a) Caching at the SBS

1

γ

2

(b) Caching at the user devices

1

γ

2

No caching
LRU
LCA
PDCA
Optimal solution

No caching
LRU
LCA
PDCA
Optimal solution

Fig. 8.
Energy consumption of the MBS for different values of the Zipf
distribution parameter γ ( ˆC = 10, U = 3). In (a) the ﬁles are cached at
the SBS as explained in Section II. In (b) the ﬁles are cached at the users as
explained in Section III.

speciﬁcally, the energy consumption in the SBS scenario is
always smaller (or equal) than the consumption in the D2D
scenario, which can be proved by using Jensen’s inequality.
Second, in the D2D scenario the total storage capacity is
distributed across users instead of being centralized and, as
a result, the energy consumption increases.

Figs. 8(a) and 8(b) evaluate the impact of the ﬁle popularity
distribution, which is controlled by γ, on the energy consump-
tion of the MBS for the scenarios and algorithms mentioned
above. We observe that the energy consumption is dramatically
reduced when γ increases (and thus, the popularity distribution
becomes skewed) as more ﬁle repetitions are encountered. By
focusing on the policies No caching and PDCA, we observe
that both follow a similar trend when γ increases; the energy
consumption of these policies decreases with γ because it is
more likely that two users request the same ﬁle within the
same time slot. Indeed, for the case of a single user connected
to the SBS, U = 1,
it can be observed that the energy
consumption of these policies is not affected by γ. Under a
uniform popularity distribution of the ﬁles (γ = 0), PDCA
outperforms LRU and LCA as ﬁle repetitions are unlikely;
however, there is a crossing point between the performances
of these policies. Interestingly, this crossing point occurs later
in the D2D scenario, and it moves to larger values of γ as
the number of users increases. We believe that this is because
the more distributed the cache is, the more the local caching
gain is penalized. In contrast, the pre-downloading gain is not
affected much by distributing the cache across users. Note

11

C
E
I

60

40

20

)
$
(

s
e
v
i
t
n
e
c
n
i

’
s
r
e
s
U

Full
D2D

0

0
0

)
$
(

S
B
M

t
s
o
c

l
a
n
o
i
t
a
r
e
p
O

1,700

No
D2D

1,600

1,500

1
1

2
2

3
3

4
4

ξU ($/Mnat)

Fig. 9. Economical cost at the MBS for different economical incentives per
transmitted data in the D2D links (U = 3, γ = 1, ˆC = 10, and ξM BS =
0.3$/KW h).

that the optimal policy outperforms all the other policies by
adapting to the best available gain for any value of γ.

E = R T
I = R T

0 ξMBS(cid:16)PU
0 ξUPU

In the previous simulations we have considered that users
altruistically cooperate with the MBS. Next, we consider
that users transmit over the D2D links in exchange of an
incentive of ξU dollars per transmitted data.
economical
In this context, we minimize the economical cost at
the
MBS deﬁned as the sum of the electricity bill, E, and
i.e., C = E + I, where
the incentives paid to users I,

u=1 W/U (exp(ru(τ )U/W ) − 1)(cid:17) dτ and
u=1Pu′6=u ˆru(τ, u′)dτ. We have set ξMBS =

0.3 $/KWh, which is a typical electricity price. Fig. 9 depicts
the costs obtained under the optimal transmission and caching
policy as ξU varies in the x-axis. The plot I refers to the left
y-axis, whereas the plots C and E refer to the right y-axis.
The D2D links are used at its greatest possible extent when
ξU = 0 as D2D transmissions do not incur any cost to the
MBS. Then, the users incentives ﬁrst increases with ξU until
it starts decreasing as the usage of the D2D links is reduced.
Finally, for very large values of ξU , the MBS serves all the
trafﬁc and no data is transmitted over the D2D links.

V. CONCLUSIONS

This paper has investigated the opportunities that caching
offers to reduce a generic cost function of the transmission
rates, e.g., the required energy, bandwidth, or trafﬁc to serve
the users. Two different scenarios have been considered where
the information is either cached at an SBS, or directly at
the user terminals, which then use D2D communications to
share the cached contents. It has been shown that, when the
transmission and caching policies are jointly designed, the
cache offers two possible gains, namely, the pre-downloading
and local caching gains. In both scenarios, the jointly optimal
transmission and caching policy has been obtained by demon-
strating that constant rate caching within a time slot is optimal,
which allows to reformulate the inﬁnite-dimensional optimiza-
tion problem as a solvable convex program. The numerical
results have focused on minimizing the energy consumption
at the MBS. It has been shown that the proposed solutions
achieve substantial energy savings. Speciﬁcally, when the
cache capacity is only 25% of the average trafﬁc of a single
user, energy savings of more than 53% have been obtained.
It has been observed that the pre-downloading gain is greater

than the local caching gain when the ﬁle popularity distribution
is uniform, and vice versa when the ﬁle popularity is skewed.
The proposed optimal ofﬂine transmission and caching policies
can be used as a lower bound to evaluate the cost of any
online policy. In particular, it has been observed that, in the
considered wireless setting, the popular LRU online algorithm
performs far from the optimal strategy as it does not exploit
the pre-downloading caching gain. To conclude, our results
motivate the design of novel online algorithms that can better
approach the performance of the optimal ofﬂine solution,
which is left for future work. These algorithms can be designed
by exploiting the partial knowledge of some of the subsequent
ﬁle requests (e.g., when some users are watching long video
content that span several time slots), or by learning users’ daily
behaviors.

APPENDIX

In the statement of Lemma 2, we assume that we know the
optimal number of data units to be cached for each request,
nu. Thus, we also know the optimal value of the maximum
q⋆
and minimum data departure curves at slot transitions, i.e.,
nu and an ,
¯ρ(n,u) (c.f. Deﬁnitions 5
and 6). ¯ρ(n, u) is deﬁned after the problem in (2). However,
we do not know the actual values of B(t, c⋆) and A(t, c⋆) for
t 6= nTs since it depends on the shape of the optimal caching
policy.

bn , B(nTs, c⋆) = C +Pn
A(nTs, c⋆) =Pn

ℓ=1PU

ℓ=1PU

u=1 Tssnu − q⋆

u=1 Tssnu − q⋆

We ﬁrst relax the problem in (1) by considering the con-
straints in (1b) and (1c) only at slot transitions (t = nTs, ∀n).
Thus, we consider the following relaxed problem:

{r(t),c(t)}t∈[0,T ]Z T

min

0

s. t.

g(r(τ ))dτ

an ≤ D(nT s, r) ≤ bn,
r(t) ≥ 0,

∀n,
∀t ∈ [0, T ],

0 (cid:22) c(t) (cid:22) s(t),
∀t ∈ [0, T ],
B(nTs, c) = bn, A(nTs, c) = an, ∀n,

(9a)

(9b)
(9c)
(9d)
(9e)

where the values of an and bn are known ∀n as argued above.
Note that any caching policy, c, satisfying (9d)-(9e) is

optimal to the relaxed problem.

This problem is represented in Fig. 10. Deﬁne ¯r⋆(t) as the
optimal transmission rate to the relaxed problem in (9). The
optimal data departure curve, D(t, ¯r⋆), is a piece-wise linear
function that can be obtained as the tightest string whose ends
are tied to (0, 0) and (0, aN ). This statement is proved in [22]
by using the integral version of the Jensen’s inequality and
the convexity of the cost function g(·). Accordingly, for any
feasible caching policy c, we know that the transmission rate
of this relaxed problem might only change at slot transitions.
Due to this, the optimal data departure curve to the problem
in (9), D(t, ¯r⋆), satisﬁes ¯A(t) ≤ D(t, ¯r⋆) ≤ ¯B(t), ∀t ∈ [0, T ],
where ¯A(t) is the piece-wise linear curve obtained by joining
the points (nTs, an) for n = 0, . . . , N ; and ¯B(t) is the piece-
wise linear curve obtained by joining the points (nTs, bn) for
n = 0, . . . , N (see Fig. 10).

Next consider the caching policy that caches each ﬁle at a
constant rate, c⋆, as deﬁned in Lemma 2. First note that c⋆

b4

a4

b3

a3

D(t, ¯r⋆)

b2

a2

¯A(t)

b1

¯B(t)

b0

a0

0

a1
Ts

T
Fig. 10. Representation of the relaxed problem in (9).

3Ts

2Ts

12

t

satisﬁes (9d)-(9e); and thus, it is an optimal caching policy
of the relaxed problem in (9). Additionally, the maximum
and minimum data departure curves associated to the caching
policy c⋆ are piece-wise linear and satisfy

A(t, c⋆) = ¯A(t) ≤ D(t, ¯r⋆) ≤ ¯B(t) = B(t, c⋆), ∀t ∈ [0, T ].
(10)
Thus, {¯r⋆, c⋆} is the optimal solution to the relaxed problem
in (9).

Note that, from (10), the pair {¯r⋆, c⋆} satisﬁes the con-
straints that had been relaxed in the original problem in
(1). Accordingly, since {¯r⋆, c⋆} is a feasible solution to the
original problem, and the objective function is the same in
both problems, it is also an optimal solution.

REFERENCES

[1] “Mobile trafﬁc forecasts 2010- 2020,” Tech. Rep. UMTS Forum, MSU-

CSE-00-2, Jan. 2011.

[2] M. Ji, G. Caire, and A. F. Molisch, “Wireless device-to-device caching
networks: Basic principles and system performance,” IEEE J. Sel. Areas
Commun., vol. 34, no. 1, pp. 176–189, Jan. 2016.

[3] S. Goebbels and R. Jennen, “Enhancements in wireless broadband
networks using smart caching an analytical evaluation,” in Proceedings
of the IEEE Int’l Symp on Personal, Indoor and Mobile Radio Commun.,
Sep. 2008, pp. 1–5.

[4] M. Dehghan, A. Seetharam, B. Jiang, T. He, T. Salonidis, J. Kurose,
D. Towsley, and R. Sitaraman, “On the complexity of optimal routing
and content caching in heterogeneous networks,” in Proceedings of the
IEEE Conf. on Computer Commun., Apr. 2015, pp. 936–944.

[5] N. Golrezaei, A. F. Molisch, A. G. Dimakis, and G. Caire, “Fem-
tocaching and device-to-device collaboration: A new architecture for
wireless video distribution,” IEEE Commun. Mag., vol. 51, no. 4, pp.
142–149, Apr. 2013.

[6] E. Bastug, J.-L. Guenego, and M. Debbah, “Proactive small cell net-
works,” in Proceedings of the IEEE Int’l Conf on Telecom, Casablanca,
Morocco, May 2013, pp. 1–5.

[7] K. Poularakis, G. Iosiﬁdis, V. Sourlas, and L. Tassiulas, “Multicast-aware
caching for small cell networks,” in Proceedings of the IEEE Wireless
Commun. and Netw. Conf., Apr. 2014, pp. 2300–2305.

[8] K. Poularakis, G. Iosiﬁdis, and L. Tassiulas, “Approximation algorithms
for mobile data caching in small cell networks,” IEEE Trans. Commun.,
vol. 62, no. 10, pp. 3665–3677, Oct. 2014.

[9] P. Blasco and D. Günd"uz, “Multi-armed bandit optimization of
cache content in wireless infostation networks,” in Proceedings of the
IEEE Int’l Symp. on Inf. Theory, Honolulu, HI, USA, Jun. 2014, pp.
51–55.

[10] F. Pantisano, M. Bennis, W. Saad, and M. Debbah, “Cache-aware user
association in backhaul-constrained small cell networks,” in Proceedings
of the IEEE Modeling and Optimization in Mobile, Ad Hoc, and Wireless
Net., 2014, pp. 37–42.

[11] M. Maddah-Ali and U. Niesen, “Fundamental limits of caching,” IEEE

Trans. Inf. Theory, vol. 60, no. 5, pp. 2856–2867, May 2014.

[12] N. Golrezaei, P. Mansourifard, A. Molisch, and A. Dimakis, “Base-
station assisted device-to-device communications for high-throughput
wireless video networks,” IEEE Trans. Wireless Commun., vol. 13, no. 7,
pp. 3665–3676, Jul. 2014.

[13] M. Ji, A. M. Tulino, J. Llorca, and G. Caire, “Order-optimal rate of
caching and coded multicasting with random demands,” arXiv preprint
arXiv:1502.03124, 2015.

13

[14] M. Ji, G. Caire, and A. F. Molisch, “Fundamental limits of caching in
wireless d2d networks,” IEEE Trans. Inf. Theory, vol. 62, no. 2, pp.
849–869, Feb. 2016.

[15] P. Li and S. Guo, “Incentive mechanisms for device-to-device commu-

nications,” IEEE Netw., vol. 29, no. 4, pp. 75–79, Jul. 2015.

[16] P. Ostovari, A. Khreishah, and J. Wu, “Cache content placement us-
ing triangular network coding,” in Proceedings of the IEEE Wireless
Commun. and Netw. Conf., Apr. 2013, pp. 1375–1380.

[17] S. Sadr and S. Valentin, “Anticipatory buffer control and resource

allocation for wireless video streaming,” arXiv: 1304.3056, 2013.

[18] A. Güngör and D. Gündüz, “Proactive wireless caching at mobile user
devices for energy efﬁciency,” in Proceedings of the IEEE Int’l Symp.
on Wireless Comm. Systems (ISWCS), Brussels, Belgium, Aug. 2015.

[19] M. Dräxler, J. Blobel, P. Dreimann, S. Valentin, and H. Karl, “Anticipa-
tory buffer control and quality selection for wireless video streaming,”
in Int. Conf. on Networked Systems, Mar. 2015.

[20] R. Pedarsani, M. A. Maddah-Ali, and U. Niesen, “Online coded

caching,” IEEE/ACM Trans. Netw., vol. PP, no. 99, pp. 1–10, 2015.

[21] M. Gregori, J. Gómez-Vilardebó, J. Matamoros, and D. Gündüz, “Joint
transmission and caching policy design for energy minimization in the
wireless backhaul link,” in Proceedings of the IEEE Int’l Symp on Inform
Theory, 2015, pp. 1004–1008.

[22] M. Zafer and E. Modiano, “A calculus approach to energy-efﬁcient
data transmission with quality-of-service constraints,” IEEE/ACM Trans.
Netw., vol. 17, no. 3, pp. 898–911, Jun. 2009.

[23] F. Pantisano, M. Bennis, W. Saad, and M. Debbah, “In-network caching
and content placement in cooperative small cell networks,” in Proceed-
ings of the IEEE Int’l Conf. on 5G for Ubiquitous Connectivity, 2014,
pp. 128–133.

[24] H. O. Fattorini, Inﬁnite Dimensional Optimization and Control Theory.

Cambridge University Press, 1999, vol. 54.

[25] M. Gregori and M. Payaró, “Energy-efﬁcient transmission for wireless
energy harvesting nodes,” IEEE Trans. Wireless Commun., vol. 12, no. 3,
pp. 1244–1254, Mar. 2013.

[26] S. Boyd and L. Vandenberghe, Convex Optimization. Cambridge Univ

Press, 2004.

[27] D. Bertsekas, A. Nedi´c,

and A. Ozdaglar, Convex Analysis
and Optimization,
and
Computation Series. Athena Scientiﬁc, 2003. [Online]. Available:
http://books.google.es/books?id=DaOFQgAACAAJ

Scientiﬁc Optimization

ser. Athena

[28] A. Nedic and A. Ozdaglar, “Approximate primal solutions and rate
analysis for dual subgradient methods,” SIAM Journal on Optimization,
vol. 19, no. 4, pp. 1757–1780, Feb. 2009.

[29] L. Rizzo and L. Vicisano, “Replacement policies for a proxy cache,”

IEEE/ACM Trans. Netw., vol. 8, no. 2, pp. 158–170, Apr. 2000.

