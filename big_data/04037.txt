Pose for Action ‚Äì Action for Pose

Umar Iqbal, Martin Garbade, Juergen Gall

Computer Vision Group

University of Bonn, Germay

{uiqbal, garbade, gall}@iai.uni-bonn.de

6
1
0
2

 
r
a

 

M
3
1

 
 
]

V
C
.
s
c
[
 
 

1
v
7
3
0
4
0

.

3
0
6
1
:
v
i
X
r
a

Abstract

In this work we propose to utilize information about
human actions to improve pose estimation in monocular
videos. To this end, we present a pictorial structure model
that exploits high-level information about activities to in-
corporate higher-order part dependencies by modeling ac-
tion speciÔ¨Åc appearance models and pose priors. However,
instead of using an additional expensive action recognition
framework, the action priors are efÔ¨Åciently estimated by our
pose estimation framework. This is achieved by starting
with a uniform action prior and updating the action prior
during pose estimation. We also show that learning the
right amount of appearance sharing among action classes
improves the pose estimation. Our proposed model achieves
state-of-the-art performance on two challenging datasets
for pose estimation and action recognition with over 80,000
test images. 1
1. Introduction

Human pose estimation from RGB images or videos is a
challenging problem in computer vision, especially for re-
alistic and unconstrained data taken from the Internet. Pop-
ular approaches for pose estimation [1, 2, 3, 4, 5, 6, 7, 8, 9]
adopt the pictorial structure (PS) model, which resembles
the human skeleton and allows for efÔ¨Åcient inference in case
of tree structures [10, 11]. Even if they are trained discrim-
inatively, PS models struggle to cope with the large varia-
tion of human pose and appearance. This problem can be
addressed by conditioning the PS model on additional ob-
servations from the image. For instance, [4] and [12] detect
poselets, which are examples of consistent appearance and
body part conÔ¨Ågurations, and condition the PS model on
these.

Instead of conditioning the PS model on predicted con-
Ô¨Ågurations of body parts from an image, we propose to con-
dition the PS model on high-level information like activity.
Intuitively, the information about the activity of a person

1The models and source code will be released publicly on author‚Äôs web

page.

can provide a strong cue about the pose and vice versa the
activity can be estimated from pose. There have been only
few works [13, 14, 15] that couple action recognition and
pose estimation to improve pose estimation.
In [13], ac-
tion class conÔ¨Ådences are used to initialize an optimization
scheme for estimating the parameters of a subject-speciÔ¨Åc
3D human model in indoor multi-view scenarios. In [14], a
database of 3D poses is used to learn a cross-modality re-
gression forest that predicts the 3D poses from a sequence
of 2D poses, which are estimated by [6]. In addition, the
action is detected and the 3D poses corresponding to the
predicted action are used to reÔ¨Åne the pose. However, both
approaches cannot be applied to unconstrained monocular
videos. While [13] requires a subject-speciÔ¨Åc model and
several views, [14] requires 3D pose data for training. More
recently, [15] proposed an approach to jointly estimate ac-
tion classes and reÔ¨Åne human poses. The approach decom-
poses the human poses estimated at each video frame into
sub-parts and tracks these sub-parts across time according
to the parameters learned for each action. The action class
and joint locations corresponding to the best part-tracks are
selected as estimates for the action class and poses. The
estimation of activities, however, comes at high computa-
tional cost since the videos are pre-processed by several ap-
proaches, one for pose estimation [16] and two for extract-
ing action related features [17, 18].

In this work, we present a framework for pose estimation
that infers and integrates activities with a very small com-
putational overhead compared to an approach that estimates
the pose only. This is achieved by an action conditioned pic-
torial structure (ACPS) model for 2D human pose estima-
tion that incorporates priors over activities. The framework
of the approach is illustrated in Figure 1. We Ô¨Årst infer the
poses for each frame with a uniform distribution over ac-
tions. While the binaries of the ACPS are modeled by Gaus-
sian mixture models, which depend on the prior distribution
over the action classes, the unaries of the ACPS model are
estimated by action conditioned regression forests. To this
end, we modify the approach [7], which consists of two lay-
ers of random forests, on two counts. Firstly, we replace the

1

Figure 1: Overview of the proposed framework. We propose an action conditioned pictorial structure model for human
pose estimation (2). Both the unaries œÜ and the binaries œà of the model are conditioned on the distribution of action classes
p(a). While the pairwise terms are modeled by Gaussians conditioned on p(a), the unaries are learned by a regression forest
conditioned on p(a) (1). Given an input video, we do not have any prior knowledge about the action and use a uniform prior
p(a). We then predict the pose for each frame independently (3). Based on the estimated poses, the probabilities of the action
classes p(a) are estimated for the entire video (4). Pose estimation is repeated with the updated action prior p(a) to obtain
better pose estimates (5).

Ô¨Årst layer by a convolutional network and use convolutional
channel features to train the second layer, which consists of
regression forests. Secondly, we condition the regression
forests on a distribution over actions and learn the sharing
among action classes.
In our experiments, we show that
these modiÔ¨Åcations increase the pose estimation accuracy
by more than 40% compared to [7]. After the poses are in-
ferred with a uniform distribution over actions, we update
the action prior and the ACPS model based on the inferred
poses to obtain the Ô¨Ånal pose estimates. Since the update
procedure is very efÔ¨Åcient, we avoid the computational ex-
pensive overhead of [15].

We evaluate our approach on the challenging J-HMDB
[19] and Penn-Action [20] datasets, which consist of videos
collected from the Internet and contain large amount of
scale and appearance variations.
In our experiments, we
provide a detailed analysis of the impact of conditioning
unaries and binaries on a distribution over actions and the
beneÔ¨Åt of appearance sharing among action classes. Our
approach achieves state-of-the-art pose estimation and ac-
tion recognition performance on both datasets. Compared
to [15], the pose estimation accuracy is improved by over
30%.

2. Related Work

State-of-the-art approaches for pose estimation are
mostly based on neural networks [21, 22, 23, 24, 25, 26]
or on the pictorial structure framework [27, 28, 6, 4, 29, 7].
A few works also combine both concepts [30, 9]. In this
work, we focus on PS models with a tree structure due to
their exact and efÔ¨Åcient inference [10, 11].

Several approaches have been proposed to improve the
accuracy of PS models for human pose estimation. For

instance, joint dependencies can be modeled not only by
the PS model, but also by a mid-level image representation
such as poselets [4, 5, 12], exemplars [31] or data dependent
probabilities learned by a neural network [30]. Pose estima-
tion in videos can be improved by taking temporal informa-
tion or motion cues into account [32, 8, 16, 33, 34, 35].

In [16, 33] several pose hypotheses are generated for
each video frame and a smooth conÔ¨Åguration of poses over
time is selected from all hypotheses. Instead of complete
articulated pose, [34] and [8] track individual body parts
and regularize the trajectories of the body parts through
the location of neighboring parts. A similar approach is
adopted in [32] where poses at each frame are tracked
across multiple frames, and subsequently reÔ¨Åned by intro-
ducing spatio-temporal smoothing constraints. Similar in
spirit, the approach in [36] jointly tracks symmetric body
parts in order to better incorporate spatio-temporal con-
straints, and also to avoid double-counting. Optical Ô¨Çow
information has also been used to enhance detected poses
at each video frame by analyzing body motion in adjacent
frames [37, 38, 35, 23, 39]. In contrast to these approaches,
we utilize the detected poses in each video frame to extract
high-level information about the activity and use it to reÔ¨Åne
the poses.

Action recognition based on 3D human poses has been
investigated in many works [40]. With the progress in the
area of 2D human pose estimation in recent years, 2D poses
have also gained an increased attention for action recogni-
tion [19, 41, 42, 43, 3, 44, 45]. However, utilizing action
recognition to aid human pose estimation is not well stud-
ied, in particular not in the context of 2D human pose esti-
mation. There are only a few works [13, 14, 46, 47, 15] that
showed the beneÔ¨Åt of it. The approaches in [13, 14] rely

Input VideoConditional Regression ForestwithConvolutional Channel FeaturesAction Conditioned Pictorial StructureEstimated PosesRefined PosesAction Classifier4p(ùëé)ùúìùëóùëù(ùê±ùëó,ùê±ùëù|p(ùëé))3215ùúôùëó(ùê±ùëó|p(ùëé),ùêà)on strong assumption. The approach [13] assumes that a
person-speciÔ¨Åc 3D model is given and considers pose es-
timation in the context of multiple synchronized camera
views. The approach [14] focuses on 3D pose estimation
from monocular videos and assumes that 3D pose data is
available for all actions. The approach [46] adopts a mix-
ture of PS models, and learns a model for each action class.
For a given image, each model is weighted by the conÔ¨Å-
dence scores of an additional action recognition system and
the pose with the maximum weight is taken. A similar ap-
proach is adopted in [47] to model object-pose relations.
These approaches, however, do not scale with the number
of action classes since each model needs to be evaluated.

The closest to our work is the recent approach of [15]
that jointly estimates the action classes and reÔ¨Ånes human
poses. The approach Ô¨Årst estimates human poses at each
video frame and decomposes them into sub-parts. These
sub-parts are then tracked across video frames based on ac-
tion speciÔ¨Åc spatio-temporal constraints. Finally, the action
labels and joint locations are inferred from the part tracks
that maximize a deÔ¨Åned objective function. While the ap-
proach shows promising results, it does not re-estimate the
parts but only re-combines them over frames i.e., only the
temporal constraints are inÔ¨Çuenced by an activity. More-
over, it relies on two additional activity recognition ap-
proaches based on optical Ô¨Çow and appearance features to
obtain good action recognition accuracy that results in a
very large computational overhead as compared to an ap-
proach that estimates activities using only the pose infor-
mation. In this work, we show that additional action recog-
nition approaches are not required, but predict the activities
directly from a sequence of poses. In contrast to [15], we
condition the pose model itself on activities and re-estimate
the entire pose per frame.
3. Overview

Our method exploits the fact that the information about
the activity of a subject provides a cue about pose and ap-
pearance of the subject, and vice versa.
In this work we
utilize the high-level information about a person‚Äôs activ-
ity to leverage the performance of pose estimation, where
the activity information is obtained from previously inferred
poses. To this end, we propose an action conditioned picto-
rial structure (PS) that incorporates action speciÔ¨Åc appear-
ance and kinematic models. If we have only a uniform prior
over the action classes, the model is a standard PS model,
which we will brieÔ¨Çy discuss in Section 4. Figure 1 depicts
an overview of the proposed framework.

4. Pictorial Structure

body joints. The structure of a human body is represented
by a kinematic tree with nodes of the tree being the joints j
and edges E being the kinematic constraints between a joint
j and its unique parent joint p as illustrated in Figure 1.
The pose conÔ¨Åguration in a single image is then inferred by
maximizing the following posterior distribution:

p(X|I) ‚àù (cid:89)

j‚ààJ

(cid:89)

j,p‚ààE

œÜj(xj|I)

œàjp(xj, xp)

(1)

where the unary potentials œÜj(xj|I) represent the likeli-
hood of the jth joint at location xj. The binary potentials
œàjp(xj, xp) deÔ¨Åne the deformation cost for the joint-parent
pair (j, p), and are often modeled by Gaussian distributions
for an exact and efÔ¨Åcient solution using a distance trans-
form [10]. We describe the unary and binary terms in Sec-
tion 4.1 and Section 4.2, respectively. In Section 5.1, we
then discuss how these can be adapted to build an action
conditioned PS model.

4.1. Unary Potentials

Random regression forests have been proven to be robust
for the task of human pose estimation [48, 49, 50, 7]. A re-
gression forest F consists of a set of randomized regression
trees, where each tree T is composed of split and leaf nodes.
Each split node represents a weak classiÔ¨Åer which passes an
input image patch P to a subsequent left or right node until
a leaf node LT is reached. As in [7], we use a separate re-
gression forest for each body joint. Each tree is trained with
a set of randomly sampled images from the training data.
The patches around the annotated joint locations are con-
sidered as foreground and all others as background. Each
patch consists of a joint label c ‚àà {0, j}, a set of image fea-
tures FP , and its 2D offset dP from the joint center. During
training, a splitting function is learned for each split node by
randomly selecting and maximizing a goodness measure for
regression or classiÔ¨Åcation. At the leaves the class probabil-
ities p(c|LT ) and the distribution of offset vectors p(d|LT )
are stored.

During testing, patches are densely extracted from the
input image I and are passed through the trained trees. Each
patch centered at location y ends in a leaf node LT (P (y))
for each tree T ‚àà F. The unary potentials œÜj for the joint j
at location xj are then given by

(cid:88)

y‚àà‚Ñ¶

1
|F|

(cid:88)

(cid:110)

T‚ààF

œÜj(xj|I) =

p(c = j|LT (P (y)))

¬∑ p(x ‚àí y|LT (P (y))

(cid:111)

.

(2)

We adopt the joint representation [7] of the PS model
[10], where the vector xj ‚àà X represents the 2D location of
the jth joint in image I, and X = {xj}j‚ààJ is the set of all

In [7] a two layer approach is proposed. The Ô¨Årst layer
consists of classiÔ¨Åcation forests that classify image patches
according to the body parts using a combination of color

jp and covariance Œ£k

with mean ¬µk
The weights wk
p(k|j, p)Œ± with a normalization constant Œ± = 0.1 [7].

jp, where djp = (xj‚àíxp).
jp are set according to the cluster frequency

For inference, we select the best cluster k for each joint
by computing the max-marginals for the root node and
backtrack the best pose conÔ¨Åguration from the maximum
of the max-marginals.

5. Action Conditioned Pose Estimation

As illustrated in Figure 1, our goal is to estimate the
pose X conditioned by the distribution p(a) for a set of
action classes a ‚àà A. To this end, we introduce in Sec-
tion 5.1 a pictorial structure model that is conditioned on
p(a). Since we do not assume any prior knowledge of the
action, we estimate the pose Ô¨Årst with the uniform distri-
bution p(a) = 1/|A|. The estimated poses for N frames
are then used to estimate the probabilities of the action
classes p(a|Xn=1...N ) as described in Section 5.2. Fi-
nally, the poses Xn are updated based on the distribution
p(a|Xn=1...N ).
5.1. Action Conditioned Pictorial Structure

In order to integrate the distribution p(a) of the action
classes obtained from the action classiÔ¨Åer into (1), we make
the unaries and binaries dependent on p(a):

p(X|p(a), I) ‚àù (cid:89)

œÜj(xj|p(a), I)¬∑(cid:89)

œàjp(xj, xp|p(a)).

j‚ààJ

j,p‚ààE

(4)
While the unary terms are discussed in Section 5.1.1, the
binaries œàjp(xj, xp|p(a)) are represented by Gaussians as
in (3). However, instead of computing mean and covariance
from all training poses with equal weights, we weight each
training pose based on its action class label and p(a). In our
experiments, we will also investigate the case where p(a) is
simpliÔ¨Åed to

(cid:40)

p(a) =

if a = argmaxa(cid:48) p(a(cid:48)|Xn=1...N )
otherwise.

1
0

(5)

5.1.1 Conditional Joint Regressors

Figure 3 shows examples of patches of the wrist extracted
from images of different action classes. We can see a large
amount of appearance variation across different classes re-
gardless of the fact that all patches belong to the same body
joint. However, it can also be seen that within individual
activities this variation is relatively small. We exploit this
observation and propose action speciÔ¨Åc unary potentials for
each joint j. To this end we adopt conditional regression
forests [54, 50] that have been proven to be robust for facial
landmark detection in [54] and 3D human pose estimation

Figure 2: Example of convolutional channel features ex-
tracted using VGG-16 net [52].

features, HOG features, and the output of a skin color de-
tector. The second layer consists of regression forests that
predict the joint locations using the features of the Ô¨Årst layer
and the output of the Ô¨Årst layer as features. For both layers,
the split nodes compare feature values at different pixel lo-
cations within a patch of size 24 √ó 24 pixels.

We propose to replace the Ô¨Årst layer by a convolu-
tional network and extract convolutional channel features
(CCF) [51] from the intermediate layers of the network to
train the regression forests of the second layer.
In [51]
several pre-trained network architectures have been eval-
uated for pedestrian detection using boosting as classiÔ¨Åer.
The study shows that the ‚Äúconv3-3‚Äù layer of the VGG-16
net [52] trained on the ImageNet (ILSVRC-2012) dataset
performs very well even without Ô¨Åne tuning, but it is indi-
cated that the optimal layer depends on the task. Instead
of pre-selecting a layer, we use regression forests to se-
lect the features based on the layers ‚Äúconv2-2‚Äù, ‚Äúconv3-3‚Äù,
‚Äúconv4-3‚Äù, and ‚Äúconv5-3‚Äù. An example of the CCF ex-
tracted from an image is shown in Figure 2. Since these
layers are of lower dimensions than the original image, we
upsample them using linear interpolation to make their di-
mensions equivalent to the input image. This results in a
1408 (128+256+512+512) dimensional feature representa-
tion for each pixel. As split nodes in the regression forests,
we use axis-aligned split functions. For an efÔ¨Åcient feature
extraction at multiple image scales, we use patchwork as
proposed in [53] to perform the forward pass of the convo-
lutional network only once.
4.2. Binary Potentials

Binary potentials œàjp(xj, xp) are modeled as a Gaussian
mixture model for each joint j with respect to its parent
joint p in the kinematic tree. As in [7], we obtain the rela-
tive offsets between child and parent joints from the train-
ing data and cluster them into k = 1, . . . , K clusters us-
ing k-means clustering. Each cluster k takes the form of a
weighted Gaussian distribution as

jp¬∑
œàjp(xj, xp) = wk

(cid:0)djp ‚àí ¬µk

jp

(cid:1)T(cid:0)Œ£k

jp

jp)(cid:1)(cid:19)
(cid:1)‚àí1(cid:0)djp ‚àí ¬µk

(3)

(cid:18)

exp

‚àí 1
2

Input ImageConvolutional Channel Featuresconv2-2conv3-3conv4-3conv5-3128                              256                      512                    512after an update of p(a).

5.1.2 Appearance Sharing Across Actions

Different actions sometimes share body pose conÔ¨Ågurations
and appearance of parts as shown in Figure 3. We therefore
propose to learn the sharing among action classes within a
conditional regression forest. To this end, we replace the
term œÜj(xj|a, I) in (6) by a weighted combination of the
action classes:

œÜsharing
j

(xj|a, I) =

Œ≥a(a(cid:48))œÜj(xj|a(cid:48), I)

(7)

(cid:88)

a(cid:48)‚ààA

where the weights Œ≥a(a(cid:48)) represent the amount of sharing
between action class a and a(cid:48). To learn the weights Œ≥a for
each class a ‚àà A, we apply the trained conditional regres-
sion forests to a set of validation images scaled to a constant
body size and maximize the response of (7) at the true joint
locations and minimize it at non-joint locations. Formally,
this can be stated as

(cid:40) (cid:88)

(cid:88)

j

Œ≥(a(cid:48))œÜ‚àó

a(cid:48)‚ààA
j (x|a(cid:48), Ina )

(cid:0)xgt
(cid:33)(cid:41)

Œ≥(a(cid:48))œÜ‚àó

j

|a(cid:48), Ina

j,na

(cid:1)

‚àí Œª(cid:107)Œ≥(cid:107)2

(8)

(cid:88)
(cid:32)(cid:88)

na

a(cid:48)‚ààA

Œ≥a = argmax

Œ≥

‚àí max
x‚ààXneg

subject to(cid:80)

j,na

a(cid:48)‚ààA Œ≥(a(cid:48)) = 1 and Œ≥(a(cid:48)) ‚â• 0. Ina denotes the
nth scaled validation image of action class a, xgt
is the
annotated joint position for joint j in image Ina, and Xneg
j,na
is a set of image locations which are more than 5 pixels
away from xgt
. The set of negative samples is obtained by
j (x|a(cid:48), Ina ) and taking the 10 strongest modes,
computing œÜ‚àó
which do not correspond to xgt
, for each image. For op-
timization, we use the smoothed unaries

j,na

j,na

j (x|a, I) =
œÜ‚àó

exp

œÜj(x|a, I)

(9)

j,na

(cid:32)
‚àí(cid:107)x ‚àí y(cid:107)2

(cid:33)

œÉ2

(cid:88)

y‚àà‚Ñ¶

with œÉ = 3 and replace max by the softmax function to
make (8) differentiable. The last term in (8) is a regularizer
that prefers sharing, i.e., (cid:107)Œ≥(cid:107)2 attains its minimum value
for uniform weights. In our experiments, we use Œª = 0.4
as weight for the regularizer. We optimize the objective
function by constrained local optimization using uniform
weights for initialization Œ≥(a(cid:48)) = 1/|A|.
In our experi-
ments, we observed that similar weights are obtained when
the optimization is initialized by Œ≥(a) = 1 and Œ≥(a(cid:48)) = 0
(cid:54)= a, indicating that the results are not sensitive to
for a(cid:48)
the initialization. In (8), we learn the weights Œ≥a for each
action class but we could also optimize for each joint inde-
pendently. In our experiments, however, we observed that
this resulted in over-Ô¨Åtting.

Figure 3: Example patches centered at the wrist of the left
hand side. We can see a large amount of appearance varia-
tion for a single body part. However, for several activities,
in particular sports such as golf and pull-up, this variation
is relatively small within the action classes. Nonetheless, a
few classes also share appearance with each other e.g., golf
and baseball or general activities such as run and walk. This
clearly shows the importance of class speciÔ¨Åc appearance
models with a right amount of appearance sharing across
action classes for efÔ¨Åcient human pose estimation.

in [50]. While [54] trains a separate regression forest for
each head pose and selects a speciÔ¨Åc regression forest con-
ditioned on the output of a face pose detector, [50] proposes
partially conditioned regression forests, where a forest is
jointly trained for a set of discrete states of a human attribute
like human orientation or height and the conditioning only
happens at the leaf nodes. Since the continuous attributes
are discretized, interpolation between the discrete states is
achieved by sharing the votes.

In this work we resort to partially conditional forests due
to its signiÔ¨Åcantly reduced training time and memory re-
quirements. During training we augment each patch P with
its action class label a. Instead of p(c|LT ) and p(d|LT ), the
leaf nodes model the conditional probabilities p(c|a, LT )
and p(d|a, LT ). Given the distribution over action classes
p(a), we obtain the conditional unary potentials:

œÜj(xj|p(a), I) =

(cid:88)

(cid:88)

(cid:110)

T‚ààF

a‚ààA

1
|F|

p(a)

(cid:88)
(cid:88)

y‚àà‚Ñ¶

a‚ààA

(cid:111)

¬∑ p(c = j|a, LT (P (y))).p(x ‚àí y|a, LT (P (y))
(6)

œÜj(xj|a, I)p(a).

=

Since the terms œÜj(xj|a, I) need to be computed only once
for an image I, œÜj(xj|p(a), I) can be efÔ¨Åciently computed

Pull-upGolfSwing baseballWalkRunKick ballLeft  Wrists5.2. Action ClassiÔ¨Åcation

For pose-based action recognition, we use the bag-of-
word approach proposed in [19]. From the estimated joint
positions Xn=1...N , a set of features called NTraj+ is com-
puted that encodes spatial and directional joint informa-
tion. Additionally, differences between successive frames
are computed to encode the dynamics of the joint move-
ments. Since we use a body model with 13 joints, we com-
pute the locations of missing joints (neck and belly) in or-
der to obtain the same 15 joints as in [19]. We approximate
the neck position as the mean of the face and the center of
shoulder joints. The belly position is approximated by the
mean of the shoulder and hip joints.

For each of the 3, 223 descriptor types, a codebook is
generated by running k-means 8 times on all training sam-
ples and choosing the codebook with minimum compact-
ness. These codebooks are used to extract a histogram for
each descriptor type and video. For classiÔ¨Åcation, we use
an SVM classiÔ¨Åer in a multi-channel setup. To this end, for
each descriptor type t, we compute a distance matrix Dt that
contains the œá2-distance between the histograms (ht
j) of
all video pairs (vi, vj). We then obtain the kernel matrix
that we use for the multi-class classiÔ¨Åcation as follows

i, ht

(cid:33)

(cid:32)

L(cid:88)

t=1

K(vi, vj) = exp

‚àí 1
L

Dt(ht
i, ht
j)
¬µt

(10)

where L is the number of descriptor types and ¬µt is the
mean of the distance matrix Dt. For classiÔ¨Åcation we use a
one-vs-all approach with C = 100 for the SVM.
6. Experiments

In order to evaluate the proposed method, we fol-
low the same protocol as proposed in [15].
In particu-
lar, we evaluate the proposed method on two challenging
datasets, namely sub-J-HMDB [19] and the Penn-Action
dataset [20]. Both datasets provide annotated 2D poses
and activity labels for each video. They consist of videos
collected from the Internet and contain large amount of
scale and appearance variations, low resolution frames, oc-
clusions and foreshortened body poses. This makes them
very challenging for human pose estimation. While sub-
J-HMDB [19] comprises videos from 12 action categories
with fully visible persons, the Penn-Action dataset com-
prises videos from 15 action categories with a large amount
of body part truncations. As in [15], we discard the activ-
ity class ‚Äúplaying guitar‚Äù since it does not contain any fully
visible person. For testing on sub-J-HMDB, we follow the
3-fold cross validation protocol proposed by [19]. On av-
erage for each split, this includes 229 videos for training
and 87 videos for testing with 8, 124 and 3, 076 frames,
respectively. The Penn-Action dataset consists of 1, 212
videos for training and 1, 020 for testing with 85, 325 and

Features

HOG, Color, Skin [7]

36.7

CCF
51.5

Table 1: Comparison of the features used in [7] with the
proposed convolutional channel features (CCF). APK with
threshold 0.1 on split-1 of sub-J-HMDB.

74, 308 frames, respectively. To evaluate the performance
of pose estimation, we use the APK (Average Precision of
Keypoints) metric [6, 15].
6.1. Implementation Details

For the Penn-Action dataset, we split the training images
half and half into a training set and a validation set. Since
the dataset sub-J-HMDB is smaller, we create a validation
set by mirroring the training images. The training images
are scaled such that the mean upper body size is 40 pixels.
Each forest consists of 20 trees, where 10 trees are trained
on the training and 10 on the validation set, with a maxi-
mum depth of 20 and a minimum of 20 patches per leaf.
We train each tree with 50, 000 positive and 50, 000 neg-
ative patches extracted from 5, 000 randomly selected im-
ages and generate 40, 000 split functions at each node. For
the binary potentials, we use k = 24 mixtures per part.

For

learning the appearance sharing among action
classes (Section 5.1.2) and training the action classiÔ¨Åer
(Section 5.2), we use the 10 trees trained on the training set
and apply them to the validation set. The action classiÔ¨Åer
and the sharing are then learned on the validation set.

For pose estimation, we create an image pyramid and
perform inference at each scale independently. We then se-
lect the Ô¨Ånal pose from the scale with the highest posterior
(4). In our experiments, we use 4 scales with scale factor
0.8. The evaluation of 260 trees (20 trees for each of the
13 joints) including feature extraction takes roughly 15 sec-
onds on average.2
Inference with the PS model for all 4
scales takes around 1 second. The action recognition with
feature computation takes only 0.18 seconds per image and
it does not increase the time for pose estimation substan-
tially.
6.2. Pose Estimation

We Ô¨Årst evaluate the impact of the convolutional chan-
nel features (CCF) for pose estimation on split-1 of sub-J-
HMDB. The results in Table 1 show that the CCF outper-
form the combination of color features, HOG features, and
the output of a skin color detector, which is used in [7].

In Table 2a we evaluate the proposed ACPS model un-
der different settings on split-1 of sub-J-HMDB when using
CCF features for joint regressors. We start with the Ô¨Årst step
of our framework where neither the unaries nor the binaries
2Measured on a 3.4GHz Intel processor using only one core with
NVidia GeForce GTX 780 GPU. The image size for all videos in sub-J-
HMDB is 320 √ó 240 pixels.

Binary

aaaaaaaaa

Unary
Indep. + CCF
Cond. (5) + CCF
Cond. (5) + AS + CCF
Cond. (p(a)) + CCF
Cond. (p(a)) + AS + CCF

Cond.

(5)

53.8
49.9
55.3
53.1
55.1

Cond.
(p(a))

51.0
48.4
52.9
52.0
52.5

Indep.

51.5
48.9
53.8
52.3
53.4

(a)

aaaaaaaaa

Binary

Unary
Indep.
Cond. (5)
Cond. (5) + AS
Cond. (p(a))
Cond. (p(a)) + AS

Cond.

(5)

38.5
32.5
39.6
39.0
39.5

Cond.
(p(a))

36.7
29.7
37.2
36.8
37.3

Indep.

36.7
29.3
38.0
37.0
38.0

(b)

Table 2: Analysis of the proposed framework under different settings. Cond. (5) denotes if the action class probabilities p(a)
are replaced by (5). (a) using CCF features. (b) using features from [7]. (APK threshold: 0.1)

Method

Cond.(5)+AS U. & Cond.(5) B.+CCF
Cond. (p(a))+AS U. & Cond.(5) B.+CCF
Indep. U. & Indep. B.+CCF

Head Shoulder Elbow Wrist Hip Knee Ankle Average Average
thr.=0.1
51.6
90.3
51.2
90.1
88.1
48.7

55.0
54.7
49.2

76.4
76.2
75.4

73.0
72.9
71.7

59.3
59.2
57.0

thr.=0.2

73.8
73.6
71.8

76.9
76.7
76.3

85.9
85.6
85.0

State-of-the-art approaches

Indep. U. & Indep. B. [7]
Yang & Ramanan [6]
Park & Ramanan [16]
Cherian et al. [8]
Nie et al. [15]
Chen & Yuille [30]

65.6
73.8
79.0
47.4
80.3
78.7

56.4
57.5
60.3
18.2
63.5
68.4

39.1
30.7
28.7
0.08
32.5
48.3

65.2
69.9
74.8

62.8
31.1
58.2
22.1
16.0
59.2
0.07 ‚Äî ‚Äî
62.7
21.6
39.7
66.3

76.3
76.3

60.9
48.9
49.3
‚Äî
53.1
60.3

54.4
51.6
52.5
16.4
55.7
62.6

34.4
‚Äî
‚Äî
‚Äî
‚Äî
42.2

Table 3: Comparison with the state-of-the-art on sub-J-HMDB using APK threshold 0.2. In the last column, the average
accuracy for the threshold 0.1 is given.

depend on the action classes. This is equivalent to the stan-
dard PS model described in Section 4, which achieves an
average joint estimation accuracy of 51.5%. Given the es-
timated poses, the pose-based action recognition approach
described in Section 5.2 achieves an action recognition ac-
curacy of 66.3% for split-1.

Having estimated the action priors p(a), we Ô¨Årst evalu-
ate action conditioned binary potentials while keeping the
unary potentials as in the standard PS model. As described
in Section 5.1, we can use in our model the probabilities
p(a) or replace them by the distribution (5), which consid-
ers only the classiÔ¨Åed action class. The Ô¨Årst setting is de-
noted by ‚ÄúCond. (p(a))‚Äù and the second by ‚ÄúCond. (5)‚Äù. It
can be seen that the conditional binaries based on (5) al-
ready outperform the baseline by improving the accuracy
from 51.5% to 53.8%. However, taking the priors from
all classes slightly decreases the performance. This shows
that conditioning the binary potentials on the most probable
class is a better choice than using priors from all classes.

Secondly, we analyze how action conditioned unary po-
tentials affect pose estimation. For the unaries, we have
the same options ‚ÄúCond. (p(a))‚Äù and ‚ÄúCond. (5)‚Äù as for the
binaries. In addition, we can use appearance sharing as de-
scribed in Section 5.1.2, which is denoted by ‚ÄúAS‚Äù. For all
three binaries, the conditional unaries based on (5) decrease
the performance. Since the conditional unaries based on (5)

are speciÔ¨Åcally designed for each action class, they do not
generalize well in case of a misclassiÔ¨Åed action class. How-
ever, adding appearance sharing to the conditional unaries
boost the performance for both conditioned on (5) and p(a).
Adding appearance sharing outperforms all other unaries
without appearance sharing, i.e., conditional unaries, inde-
pendent unaries and the unaries conditioned on p(a). For
all unaries, the binaries conditioned on (5) outperform the
other binaries. This shows that appearance sharing and bi-
naries conditioned on the most probable class performs best,
which gives an improvement of the baseline from 51.5% to
55.1%.

In Table 2b, we also evaluate the proposed ACPS when
using the weaker features from [7]. Although the accura-
cies as compared to CCF features are lower, the beneÔ¨Åt of
the proposed method remains the same. For the rest of this
paper, we will use CCF for all our experiments.

In Table 3 we compare the proposed action conditioned
PS model with other state-of-the-art approaches on all three
splits of sub-J-HMDB. In particular, we provide a compar-
ison with [7, 6, 15, 16, 8, 30]. The accuracies for the ap-
proaches [6, 15, 16, 8] are taken from [15] where the APK
threshold 0.2 is used. We also evaluated the convolutional
network based approach [30] using the publicly available
source code trained on sub-J-HMDB. Our approach outper-
froms the other methods by a margin, and notably improves

Method

Cond.(5)+AS U. & Cond.(5) B.+CCF
Indep. U. & Indep. B.+CCF

Yang & Ramanan [6]
Park & Ramanan [16]
Nie et al. [15]

57.9
62.8
64.2

73.0
62.6
State-of-the-art approaches
21.4
23.3
24.4

30.1
32.3
33.8

51.3
52.0
55.4

Head Shoulder Elbow Wrist Hip Knee Ankle Average Average
thr.=0.1
64.8
89.1
84.5
57.3

86.4
81.3

73.9
66.2

80.3
76.5

85.3
82.4

79.9
75.1

thr.=0.2

81.1
75.5

52.6
53.3
56.4

49.7
50.2
54.1

46.2
43.0
48.0

44.2
45.3
48.0

‚Äî
‚Äî
‚Äî

Table 4: Comparison with the state-of-the-art in terms of joint localization error on Penn Action dataset.

Method

sub-J-HMDB
Appearance features only

Penn-Action

Dense [19]
IDT-FV [55]

Pose [19]
Pose (Ours)

46.0%
60.9%
Pose features only
54.1%
61.5%

Pose + Appearance features

MST [18]

Pose+Dense [19]

AOG [15]
P-CNN [45]

Pose (Ours)+IDT-FV

45.3%
52.9%
61.2%
66.8%
74.6%

‚Äî

92.0%

‚Äî

79.0%

74.0%

‚Äî

85.5%

‚Äî

92.9%

Table 5: Comparison of action recognition accuracy with
the state-of-the-art approaches on sub-J-HMDB and Penn-
Action datasets.

wrist localization by more than 5% as compared to the base-
line.

Table 4 compares the proposed ACPS with the state-of-
the-art on the Penn-Action dataset. The accuracies for the
approaches [6, 15, 16] are taken from [15]. We can see
that the proposed method improves the baseline from 75.5%
to 81.1%, while improving the elbow and wrist localiza-
tion accuracy by more than 7% and 10%, respectively. The
proposed method also signiÔ¨Åcantly outperforms other ap-
proaches.

6.3. Action Recognition

In Table 5, we compare the action recognition accuracy
obtained by our approach with state-of-the-approaches for
action recognition. On sub-J-HMDB, the obtained accu-
racy using only pose as feature is comparable to the other
approaches. Only the recent work [45] which combines
pose, CNN, and motion features achieves a better action
recognition accuracy. However, if we combine our pose-
based action recognition with Fisher vector encoding of im-
proved dense trajectories [55] using late fusion, we outper-
form other methods that also combine pose and appearance.
On the Penn-Action dataset, the results are similar. Al-
though the approach [55] already achieves an accuracy of
92%, combining it with our pose-based approach increases
the accuracy to 92.9%.

In Table 6, we report the effect of different action recog-

nition approaches on pose estimation. We report the pose
estimation accuracy for split-1 of sub-J-HMDB when the
action classes are not inferred by our framework, but esti-
mated using improved dense trajectories with Fisher vec-
tor encoding (IDT-FV) [55] or the fusion of our pose-based
method and IDT-FV. Although the action recognition rate is
higher when pose and IDT-FV are combined, the pose esti-
mation accuracy is not improved. If the action classes are
not predicted but are provided (GT), the accuracy improves
slightly for sub-J-HMDB and from 64.8% to 68.1% for the
Penn-Action dataset. We also experimented with several it-
erations in our framework, but the improvements compared
to the achieved accuracy of 73.8% on all three splits of sub-
J-HMDB with one iteration were minor. A few qualitative
results are shown in Figure 4 along with some typical failure
cases in Figure 5.

7. Conclusion

In this paper, we have demonstrated that action recog-
nition can be efÔ¨Åciently utilized to improve human pose
estimation on realistic data. To this end, we presented a
pictorial structure model that incorporates high-level activ-
ity information by conditioning the unaries and binaries on
a prior distribution over action labels. Although the ac-
tion priors can be estimated by an accurate, but expensive
action recognition system, we have shown that the action
priors can also be efÔ¨Åciently estimated during pose estima-
tion without substantially increasing the computational time
of the pose estimation. In our experiments, we thoroughly
analyzed various combinations of unaries and binaries and
showed that learning the right amount of appearance shar-
ing among action classes improves the pose estimation ac-
curacy. On two challenging datasets for pose estimation
and action recognition, our proposed model achieves state-
of-the-art performance. We expect that combining the pro-
posed ACPS with the approach of [15] can lead to further
improvements and leave this for the future work.

References

[1] F. Wang and Y. Li, ‚ÄúBeyond physical connections: Tree mod-

els in human pose estimation,‚Äù in CVPR, 2013. 1

Indep. U. +

Indep. B. + CCF

sub-J-HMDB
Penn-Action

51.5
57.3

Cond. (5)+AS U. & Cond. (5) B. + CCF

Pose

55.3 (56.2%)
64.8 (79.0%)

IDT-FV [55]
52.6 (66.3%)

Pose+IDT-FV
55.3 (76.4%)

‚Äî

‚Äî

GT

55.9 (100%)
68.1 (100%)

Table 6: Analysis of pose estimation accuracy with respect to action recognition accuracy. The values in the parentheses are
the corresponding action recognition accuracies. (APK threshold: 0.1)

[2] M. Eichner and V. Ferrari, ‚ÄúAppearance sharing for collec-

tive human pose estimation,‚Äù in ACCV, 2012. 1

[3] C. Desai and D. Ramanan, ‚ÄúDetecting actions, poses, and

objects with relational phraselets,‚Äù in ECCV, 2012. 1, 2

[4] L. Pishchulin, M. Andriluka, P. Gehler, and B. Schiele,
‚ÄúPoselet conditioned pictorial structures,‚Äù in CVPR, 2013. 1,
2

[5] L. Pishchulin, M. Andriluka, P. Gehler, and B. Schiele,
‚ÄúStrong appearance and expressive spatial models for human
pose estimation,‚Äù in ICCV, 2013. 1, 2

[6] Y. Yang and D. Ramanan, ‚ÄúArticulated human detection with

Ô¨Çexible mixtures of parts,‚Äù TPAMI, 2013. 1, 2, 6, 7, 8

[7] M. Dantone, C. Leistner, J. Gall, and L. Van Gool, ‚ÄúBody
parts dependent joint regressors for human pose estimation
in still images,‚Äù TPAMI, 2014. 1, 2, 3, 4, 6, 7

[8] A. Cherian, J. Mairal, K. Alahari, and C. Schmid, ‚ÄúMix-
ing Body-Part Sequences for Human Pose Estimation,‚Äù in
CVPR, 2014. 1, 2, 7

[9] J. Tompson, A. Jain, Y. LeCun, and C. Bregler, ‚ÄúJoint train-
ing of a convolutional network and a graphical model for
human pose estimation,‚Äù in NIPS, 2014. 1, 2

[10] P. F. Felzenszwalb and D. P. Huttenlocher, ‚ÄúPictorial struc-

tures for object recognition,‚Äù IJCV, 2005. 1, 2, 3

[11] P. F. Felzenszwalb, R. B. Girshick, D. McAllester, and D. Ra-
manan, ‚ÄúObject detection with discriminatively trained part-
based models,‚Äù TPAMI, 2010. 1, 2

[12] A. Hernandez Vela, S. Escalera, and S. Sclaroff, ‚ÄúContextual
rescoring for human pose estimation,‚Äù in BMVC, 2014. 1, 2
[13] A. Yao, J. Gall, and L. Van Gool, ‚ÄúCoupled action recogni-
tion and pose estimation from multiple views,‚Äù IJCV, 2012.
1, 2

[14] T.-H. Yu, T.-K. Kim, and R. Cipolla, ‚ÄúUnconstrained monoc-
ular 3d human pose estimation by action detection and cross-
modality regression forest,‚Äù in CVPR, 2013. 1, 2, 3

[15] B. X. Nie, C. Xiong, and S.-C. Zhu, ‚ÄúJoint action recognition
and pose estimation from video,‚Äù in CVPR, 2015. 1, 2, 3, 6,
7, 8

[16] D. Park and D. Ramanan, ‚ÄúN-best maximal decoders for part

models,‚Äù in ICCV, 2011. 1, 2, 7, 8

[17] H. Wang, A. Kl¬®aser, C. Schmid, and C.-L. Liu, ‚ÄúAction

Recognition by Dense Trajectories,‚Äù in CVPR, 2011. 1

[18] J. Wang, X. Nie, Y. Xia, Y. Wu, and S.-C. Zhu, ‚ÄúCross-view
action modeling, learning, and recognition,‚Äù in CVPR, 2014.
1, 8

[19] H. Jhuang, J. Gall, S. ZufÔ¨Å, C. Schmid, and M. Black, ‚ÄúTo-
wards understanding action recognition,‚Äù in ICCV, 2013. 2,
6, 8

[20] W. Zhang, M. Zhu, and K. G. Derpanis, ‚ÄúFrom actemes to
action: A strongly-supervised representation for detailed ac-
tion understanding,‚Äù in ICCV, 2013. 2, 6

[21] A. Jain, J. Tompson, M. Andriluka, G. W. Taylor, and C. Bre-
gler, ‚ÄúLearning human pose estimation features with convo-
lutional networks,‚Äù in ICLR, 2014. 2

[22] A. Toshev and C. Szegedy, ‚ÄúDeeppose: Human pose estima-

tion via deep neural networks,‚Äù in CVPR, 2014. 2

[23] A. Jain, J. Tompson, Y. LeCun, and C. Bregler, ‚ÄúModeep:
A deep learning framework using motion features for human
pose estimation,‚Äù in ACCV, 2014. 2

[24] J. Tompson, R. Goroshin, A. Jain, Y. LeCun, and C. Bregler,
‚ÄúEfÔ¨Åcient object localization using convolutional networks,‚Äù
in CVPR, 2015. 2

[25] X. Fan, K. Zheng, Y. Lin, and S. Wang, ‚ÄúCombining local
appearance and holistic view: Dual-source deep neural net-
works for human pose estimation,‚Äù in CVPR, 2015. 2

[26] W. Ouyang, X. Chu, and X. Wang, ‚ÄúMulti-source deep learn-

ing for human pose estimation,‚Äù in CVPR, 2014. 2

[27] D. Tran and D. Forsyth, ‚ÄúImproved human parsing with a full

relational model,‚Äù in ECCV, 2010. 2

[28] M. Eichner, M. Marin-Jimenez, A. Zisserman, and V. Fer-
rari, ‚Äú2d articulated human pose estimation and retrieval in
(almost) unconstrained still images,‚Äù IJCV, 2012. 2

[29] M. Andriluka, S. Roth, and B. Schiele, ‚ÄúDiscriminative ap-

pearance models for pictorial structures,‚Äù IJCV, 2012. 2

[30] X. Chen and A. L. Yuille, ‚ÄúArticulated pose estimation by a
graphical model with image dependent pairwise relations,‚Äù
in NIPS, 2014. 2, 7

[31] B. Sapp, C. Jordan, and B. Taskar, ‚ÄúAdaptive pose priors for

pictorial structures,‚Äù in CVPR, 2010. 2

[32] H. Shen, S.-I. Yu, Y. Yang, D. Meng, and A. Hauptmann,
‚ÄúUnsupervised video adaptation for parsing human motion,‚Äù
in ECCV, 2014. 2

[33] D. Batra, P. Yadollahpour, A. Guzman-Rivera,

and
G. Shakhnarovich, ‚ÄúDiverse m-best solutions in markov ran-
dom Ô¨Åelds,‚Äù in ECCV, 2012. 2

[34] V. Ramakrishna, T. Kanade, and Y. Sheikh, ‚ÄúTracking human

pose by tracking symmetric parts,‚Äù in CVPR, 2013. 2

Figure 4: Qualitative results on some frames of the sub-J-HMDB (rows 1-3) and Penn-Action (rows 4-8) dataset as compared
to our baseline with CCF. The left part of the images corresponds to the baseline while the right part shows improved poses
obtained by the proposed ACPS.

[35] S. ZufÔ¨Å, J. Romero, C. Schmid, and M. J. Black, ‚ÄúEstimating

[38] K. Fragkiadaki, H. Hu, and J. Shi, ‚ÄúPose from Ô¨Çow and Ô¨Çow

human pose with Ô¨Çowing puppets,‚Äù in ICCV, 2013. 2

from pose,‚Äù in CVPR, 2013. 2

[36] D. Zhang and M. Shah, ‚ÄúHuman pose estimation in videos,‚Äù

in ICCV, 2015. 2

[37] B. Sapp, D. Weiss, and B. Taskar, ‚ÄúParsing human motion

with stretchable models,‚Äù in CVPR, 2011. 2

[39] T. PÔ¨Åster, J. Charles, and A. Zisserman, ‚ÄúFlowing convnets
for human pose estimation in videos,‚Äù in IEEE International
Conference on Computer Vision, 2015. 2

[40] M. Ye, Q. Zhang, L. Wang, J. Zhu, R. Yang, and J. Gall,

x     baseline               ours     baseline               ours     baseline               oursFigure 5: Few typical failure cases in the sub-J-HMDB (row 1) and PennAction (row 2) dataset due to large scale variations,
rare poses with motion blur, large amount of body part occlusions and truncations, multiple persons, and bad illumination
conditions.

[53] F. Iandola, M. Moskewicz, S. Karayev, R. Girshick, T. Dar-
rell, and K. Keutzer, ‚ÄúDensenet: Implementing efÔ¨Åcient con-
vnet descriptor pyramids,‚Äù arXiv preprint arXiv:1404.1869,
2014. 4

[54] M. Dantone, J. Gall, G. Fanelli, and L. Van Gool, ‚ÄúReal-time
facial feature detection using conditional regression forests,‚Äù
in CVPR, 2012. 4

[55] H. Wang and C. Schmid, ‚ÄúAction recognition with improved

trajectories,‚Äù in ICCV, 2013. 8, 9

‚ÄúA survey on human motion analysis from depth data,‚Äù in
Time-of-Flight and Depth Imaging. Sensors, Algorithms, and
Applications, 2013. 2

[41] L. Pishchulin, M. Andriluka, and B. Schiele, ‚ÄúFine-grained
activity recognition with holistic and pose based features,‚Äù in
GCPR, 2014. 2

[42] K. N. Tran, I. A. Kakadiaris, and S. K. Shah, ‚ÄúModeling mo-
tion of body parts for action recognition.,‚Äù in BMVC, 2011.
2

[43] V. K. Singh and R. Nevatia, ‚ÄúAction recognition in cluttered
dynamic scenes using pose-speciÔ¨Åc part models,‚Äù in ICCV,
2011. 2

[44] W. Yang, Y. Wang, and G. Mori, ‚ÄúRecognizing human ac-
tions from still images with latent poses,‚Äù in CVPR, 2010.
2

[45] G. Ch¬¥eron, I. Laptev, and C. Schmid, ‚ÄúP-cnn: Pose-based

cnn features for action recognition,‚Äù in CVPR, 2015. 2, 8

[46] N. Ukita, ‚ÄúIterative action and pose recognition using global-
and-pose features and action-speciÔ¨Åc models,‚Äù in ICCV
Workshop, 2013. 2, 3

[47] B. Yao and L. Fei-Fei, ‚ÄúRecognizing human-object interac-
tions in still images by modeling the mutual context of ob-
jects and human poses,‚Äù TPAMI, 2012. 2, 3

[48] J. Shotton, A. Fitzgibbon, M. Cook, T. Sharp, M. Finocchio,
R. Moore, A. Kipman, and A. Blake, ‚ÄúReal-time human pose
recognition in parts from single depth images,‚Äù in CVPR,
2011. 3

[49] R. Girshick,

J. Shotton, P. Kohli, A. Criminisi, and
A. Fitzgibbon, ‚ÄúEfÔ¨Åcient regression of general-activity hu-
man poses from depth images,‚Äù in ICCV, 2011. 3

[50] M. Sun, P. Kohli, and J. Shotton, ‚ÄúConditional regression

forests for human pose estimation,‚Äù in CVPR, 2012. 3, 4

[51] B. Yang, J. Yan, Z. Lei, and S. Z. Li, ‚ÄúConvolutional channel
features for pedestrian, face and edge detection,‚Äù in ICCV,
2015. 4

[52] K. Simonyan and A. Zisserman, ‚ÄúVery deep convolu-
tional networks for large-scale image recognition,‚Äù CoRR,
vol. abs/1409.1556, 2014. 4

