6
1
0
2

 
r
a

 

M
5
1

 
 
]
L
C
.
s
c
[
 
 

1
v
7
6
7
4
0

.

3
0
6
1
:
v
i
X
r
a

Evaluating the word-expert approach for

Named-Entity Disambiguation
Valentin I. Spitkovsky†
Angel X. Chang†
Eneko Agirre‡
Christopher D. Manning†

†Computer Science Department, Stanford University, Stanford, CA, USA
‡IXA Group, University of the Basque Country, Donostia, Basque Country

{angelx,vals,manning}@cs.stanford.edu, e.agirre@ehu.eus

Abstract

Named Entity Disambiguation (NED) is the task of linking a named-entity men-
tion to an instance in a knowledge-base, typically Wikipedia. This task is closely
related to word-sense disambiguation (WSD), where the supervised word-expert
approach has prevailed. In this work we present, for the ﬁrst time, the results of
the word-expert approach to NED, where one classiﬁer is built for each target en-
tity mention string. The resources necessary to build the system, a dictionary and
a set of training instances, have been automatically derived from Wikipedia. We
provide empirical evidence of the value of this approach, as well as a study of the
differences between WSD and NED, including ambiguity and synonymy statistics.

1 Introduction

Construction of formal representations from snippets of free-form text is a long-sought-
after goal in natural language processing (NLP). Grounding written language with re-
spect to background knowledge about real-life entities and world events is important
for building such representations. It also has many applications in its own right: text
mining, information retrieval and the semantic web [Weikum and Theobald, 2010].
Wikipedia and related repositories of structured data (e.g. WikiData or DBpedia) al-
ready provide extensive inventories of named entities, including people, organizations
and geo-political entities.

An individual named-entity string may refer to multiple entities and the process
of resolving the appropriate meaning in context is called entity linking (EL) or named
entity disambiguation (NED). The former terminology (EL) stresses the importance
of linking a mention to an actual instance in the given knowledge-base [McNamee
and Dang, 2009]. We prefer the latter term (NED), which focuses on the potential
ambiguity among several possible instances. It highlights the connection to the closely

1

Figure 1: Wikipedia disambiguation page for Jonathan Edwards.

related problem of word-sense disambiguation (WSD), and was used in some of the
earliest works [Bunescu and Pasca, 2006, Cucerzan, 2007].

As our ﬁrst example of NED, consider the following sentence: “Champion triple
jumper Jonathan Edwards has spoken of the impact losing his faith has had on his
life.” Jonathan Edwards may refer to several people — Wikipedia lists more than ten,1
including the intended athlete2 — as well as a residential college at Yale and a music
record. Figure 1 shows a disambiguation page for the string, and Figure 2 an excerpt
from the athlete’s Wikipedia article.

In addition to disambiguation pages that list possible entities to which a canonical
string like “Jonathan Edwards” may refer, many naturally-occurring entity mentions
in regular Wikipedia articles are also cross-referenced. For instance, the ﬁrst sentence
of the entry for Jonathan Edwards (the athlete) includes hyperlinks (shown in blue) to
“CBE,” “British” and “triple jumper.” The ﬁrst link references an article on the Order of
the British Empire.3 Anchor-text (words in blue) often exposes alternate ways of refer-
ring to entities: e.g., a member of the Order of the British Empire can be called “CBE.”
Figure 3 shows ﬁve sentences that link to Jonathan Edwards (the athlete), and Fig-
ure 4 shows three sentences with hyperlinks to other people called Jonathan Edwards.
Anchors present a rich source of disambiguation information. Aggregating over all
occurrences of hyper-text “Jonathan Edwards,” we can compute that, most of the time,
it refers to the theologian. Yet by analyzing the context of each named entity’s occur-
rence, we could conclude that a span like “. . . triple jumper Jonathan Edwards . . . ” is
more similar to mentions of the athlete. These linked spans from Wikipedia can also be

1http://en.wikipedia.org/wiki/Jonathan_Edwards_(disambiguation)
2http://en.wikipedia.org/wiki/Jonathan_Edwards_(athlete)
3http://en.wikipedia.org/wiki/Order_of_the_British_Empire

2

Figure 2: Wikipedia article for Jonathan Edwards, the athlete.

used to obtain alternative terms that refer to the same entity. Figure 5 shows a variety
of ways in which Wikipedia refers to Bill Clinton.

As online encyclopedias grow in size, entities and ideas that are of interest to even
small communities of users may get their own Wikipedia pages with relevant descrip-
tions.4 Thanks to the hyper-linked nature of the web, many online mentions have
already been annotated with pointers to corresponding articles, both within Wikipedia
and from external sites. As a result, large quantities of freely available information
— suitable for supervised machine learning algorithms — already exist, obviating the
need for costly manual annotations that are typically associated with the training of
traditional NLP systems in general and WSD in particular.

NED is a disambiguation task that is closely related to WSD, where the goal is
to disambiguate open-class words (i.e., common nouns, adjectives, verbs and adverbs)
with respect to a sense inventory, such as WordNet [Fellbaum, 1998]. The extensive
WSD literature [Agirre and Edmonds, 2006, Navigli, 2009] has shown that building a
supervised classiﬁer for each target lemma — the so-called word-expert approach —
outperforms other techniques [Zhong and Ng, 2010].

In this work we propose an architecture for NED following the word-expert ap-
proach, where we build a classiﬁer for each named-entity mention, with two main
modules: (1) A candidate generation module which, given a string, lists all potentially
relevant entities. This module is based on a static dictionary, which also lists the pop-
ularity of each of the entities, and can thus serve as a standalone context-independent

4http://en.wikipedia.org/wiki/Wikipedia:Size_of_Wikipedia

3

Wikipedia article

Triple Jump

Ilfracombe

Example sentence
The current male and female world record holders are Jonathan Edwards of Great Britain,
with a jump of 18.29 meters, and Inessa Kravets of Ukraine, with a jump of 15.50 m.
During the lead up to the 2012 Olympics a number of people carried the torch through
Ilfracombe, amongst these was Jonathan Edwards, the triple jump world record holder, who
had the privilege to carry the torch past his former home.

Great North Museum Athlete Jonathan Edwards is the patron of the ’Be Part of It’ campaign.

Christian Olsson

Test the Nation

Olsson ﬁrst became interested in triple jump after watching Jonathan Edwards set the world
record at the World Championships in his hometown Gothenburg.
They were pitted against a group of 10 celebrities including EastEnders star Adam Woodyatt,
pop music presenter Fearne Cotton, Olympic athlete Jonathan Edwards and former Sunday
Times editor and broadcaster Andrew Neil.

Figure 3:
Jonathan_Edwards_(athlete).

Example sentences from Wikipedia whose anchor-texts point

to

Wikipedia article
Mission (Christian)

One Day Closer

Buildings of Jesus College, Oxford

Example sentence
In North America, missionaries
to the native Americans included
Jonathan Edwards, the well known
preacher of the Great Awakening ,
who in his later years retired from
the very public life of his early ca-
reer.
One Day Closer is the ninth studio
album (eleventh
album)
songwriter
released
features
Jonathan Edwards
many ballads and love songs.
Jonathan Edwards (principal
from
1686 to 1712) is reported to have
spent £1,000 during his lifetime on
the interior of the chapel.

singer

by

total

and

Target Wikipedia article

Jonathan Edwards (theologian)

Jonathan Edwards (musician)

Jonathan Edwards (academic)

Figure 4: Example sentences from Wikipedia where the referent of “Jonathan Ed-
wards” is not the athlete.

disambiguation module. (2) A context-sensitive supervised classiﬁer that selects the
entity which is most suited for the context of the mention. The classiﬁers use the kind
of features routinely used in a WSD classiﬁer. To our knowledge, this is the ﬁrst time
a NED system following the word-expert approach is reported, although early work on
Wikiﬁcation already hinted at its usefulness [Csomai and Mihalcea, 2008].

Our system is based on two main resources: (i) a dictionary listing all candidate
entities for each surface string, together with their frequencies;5 and (ii) a set of training
instances for each target mention. The bulk of the information comes from Wikipedia
but we have further complemented the dictionary with web counts from a subset of a
2011 Google crawl. The dictionary’s release was documented in a short conference
paper [Spitkovsky and Chang, 2012], which we extend with additional explanations
and analyses here.

5http://www-nlp.stanford.edu/pubs/crosswikis-data.tar.bz2

4

Wikipedia article

Arthur Levitt

Asa Hutchinson

William Jefferson Blythe, Jr.

HMMT-164

Joan Jett Blakk

Example sentence
Levitt was appointed to his ﬁrst ﬁve-year term as Chairman of the SEC by
President Clinton in July 1993 and reappointed in May 1998.
Hutchinson, who had at ﬁrst decided to run for an open seat in the Arkansas House
of Representatives from Sebastian County, defeated Ann Henry, a long-time friend
of Bill and Hillary Clinton.
Three months later, Virginia gave birth to their son, William Jefferson Blythe III,
the future President.
In
the
President of the United States as he visited the ﬂood-ravaged areas around
Portland, Oregon.
Smith also ran for president in 1996 with the slogan “Lick Slick Willie in ’96!”

1996, HMM-164 was

February

called

upon

to

support

Figure 5: Example sentences from Wikipedia that use different names to refer to the
same entity (Bill Clinton).

We present a detailed analysis of the performance of the components and varia-
tions of our NED system, as applied to the entity linking task of the NIST Text Anal-
ysis Conference’s (TAC)6 knowledge-base population (KBP) track [Ji and Grishman,
2011]. The task focuses on several target entity mentions, which makes it well suited
for our word-expert approach. In the future we would like to explore other datasets
which include all mentions in full documents [Hoffart et al., 2011]. Our ﬁnal results
are quite strong despite the simplicity of the techniques used, with the dictionary’s raw
frequencies already performing extremely well. We focus on the candidate generation
and disambiguation modules, leaving aside mention detection and the task of NIL de-
tection, where mentions which refer to entities not listed in the knowledge-base have
to be detected [Durrett and Klein, 2014].

In addition, we will study differences between the closely related worlds of WSD
and NED. In WSD, an exhaustive dictionary is provided, while in NED, one has to
generate all candidate entities for a target string — a step that has been shown to be
critical to success [Hachey et al., 2012]. In WSD very few occurrences correspond
to senses missing in the dictionary, but in NED this problem is quite prevalent. We
will also show that ambiguity is larger for NED. On the bright side, there is a lot of
potential training data for NED, for instance, in the form of human-generated anchor-
texts. This article shows that an architecture based on WSD methods can work well
for NED, and that it is feasible to model candidate generation with a static dictionary.
We will also compare ambiguity, synonymy and inter-annotator agreement statistics of
both problems.

Note that the authors participated in the TAC-KBP Entity Linking tracks with pre-
liminary versions of the system reported in this article. Those systems, alongside all
participant systems7, were reported in publicly available proceedings [Agirre et al.,
2009, Chang et al., 2010, 2011].

The paper is organized as follows: we ﬁrst present related work, followed by the
architecture of our system (Section 3). The dictionary for candidate generation is pre-

6http://www.nist.gov/tac/
7There was no peer-review, and all papers were accepted.

5

sented next (Section 4), followed by methods that produce the training data and build
the supervised classiﬁer (Section 5). Section 6 introduces the TAC-KBP dataset used
for evaluation, followed by the adaptation of our system (Section 7). Section 8 explores
several alternatives to our approach and analyzes their performance on development
data, as well as the ﬁnal results and discussion. We then compare our system to related
work, draw conclusions and propose future directions.

2 Related Work

We will now review several NLP problems that are closely related to NED, includ-
ing wikiﬁcation and WSD, as well as previous work on NED. For clearer exposition,
we group the previous work into three sections: ﬁrst, the earlier and more inﬂuential
contributions, followed by work on wikiﬁcation, and ﬁnally NED systems. We brieﬂy
touch on state-of-the-art techniques used in NED, speciﬁcally, handling of candidate
generation (i.e., our dictionary), and disambiguation. A full comparison of our results
to those of the latest state-of-the-art systems can be found in Section 8.7.

2.1 Related Problems

NED is related to several problems in NLP. For instance, it presupposes that mentions
of named entities have already been identiﬁed in text, building up from the named entity
recognition (NER) task [Marsh and Perzanowski, 1998, Tjong Kim Sang and De Meul-
der, 2003]. Each mention may be further labeled with a broad semantic category —
such as names of persons, organizations or locations — via named entity classiﬁca-
tion (NEC). This last task is often performed by using gazetteers to cover many known
entities, in addition to training a single supervised classiﬁer that outputs category types
for input mentions given their speciﬁc contexts. Overall, we view NED as a speciﬁc
instantiation of the record linkage problem, in which the task is to ﬁnd records referring
to the same entity across different data sources, such as data ﬁles, databases, books or
websites.

The term record linkage was ﬁrst used by Dunn Halbert [1946], in reference to
resolving person names across ofﬁcial records held by a government. More recently,
Bagga and Baldwin [1998] focused on cross-document coreference of people by ﬁrst
identifying coreference chains within documents and then comparing the found chains’
contexts across documents. Similar problems arise in citation databases, where it is
necessary to decide which mentions of authors in bibliographic records refer to the
same person [Bhattacharya and Getoor, 2007]. Other typical applications that lack a
predeﬁned inventory of entities include resolving names in e-mails and web people
search (WePS). In the WePS task, starting from a set of web-pages that mention a
name (e.g., John Doe), the goal is to decide how many John Does there are, and who
is mentioned where [Artiles et al., 2008, 2009]. The e-mails task can be tackled by as-
suming that each address corresponds to a distinct person and that people’s identifying
information can be deduced from what they write [Elsayed et al., 2008].

Three important aspects differentiate NED from record linkage and other cross-

6

document entity coreference tasks, such as the exercise studied at ACE 20088 and clus-
tering of documents that mention the same entity [Mann and Yarowsky, 2003, Gooi and
Allan, 2004]. These differences hinge on the existence of (1) a knowledge-base (e.g.,
Wikipedia) that lists gold-standard entities; (2) rich textual information describing each
entity (i.e., its Wikipedia page); and (3) many explicitly disambiguated mentions of en-
tities (i.e., incoming hyper-links furnished by Wikipedia’s contributors and other, exter-
nal web publishers). A more closely related task is wikiﬁcation [Mihalcea and Csomai,
2007, Csomai and Mihalcea, 2008, Milne and Witten, 2008, Kulkarni et al., 2009],
which involves ﬁrst deciding which keywords or concepts are relevant in a given text
and then disambiguating them by linking to the correct Wikipedia article. Although the
overall thrust of that task is different, since wikiﬁcation systems target common nouns
as well as named entities, its disambiguation techniques are relevant to NED (see Sec-
tion 2.4).

2.2 Word Sense Disambiguation
In WSD, the task is to determine which sense of an open class content word — i.e.,
an adverb, verb, adjective or noun that isn’t a named entity — applies to a particular
occurrence of that word [Agirre and Edmonds, 2006, Navigli, 2009]. Typically, the
sense inventory is taken from a dictionary, such as WordNet [Fellbaum, 1998], and
ﬁxed in advance. Dictionaries are comprehensive, covering nearly all uses of a word.
Consequently, WSD systems tend to return a sense for every occurrence. For instance,
the Senseval-3 lexical sample dataset [Mihalcea et al., 2004] contains 2,945 manu-
ally annotated occurrences, 98.3% of which have been assigned a dictionary sense:
only 1.7% are problematic and have senses not found in the dictionary. By contrast,
in NED, a signiﬁcant portion of mentions are out-of-inventory (around 50% in our
dataset). However, in WSD target senses are often open to interpretation, as reﬂected
by low inter-annotator agreement, e.g., 72.5% in the Senseval-3 all-words task [Sny-
der and Palmer, 2004]. We will show that NED poses a better-deﬁned problem, with
less dispute about what constitutes a correct target entity, as indicated by signiﬁcantly
higher inter-annotator agreement.

WSD and NED differ also in two other key properties: (i) polysemy, the expected
number of senses a word might have; and (ii) synonymy, the expected number of dif-
ferent words that may be used to lexicalize a given concept. These statistics can be
used to characterize the difﬁculty of an evaluation set. We compared average polysemy
and synonymy values for WSD and NED, computed using gold standards (by counting
senses listed in dictionaries versus those actually occurring in test data). NED scored
substantially higher on both metrics, relative to WSD (see Section 6.4).

The best-performing WSD systems are currently based on supervised machine
learning, judging by public evaluation exercises [Snyder and Palmer, 2004, Pradhan
et al., 2007]. Typically, the problem is modeled using multi-class classiﬁers [Marquez
the word expert
et al., 2006], with one classiﬁer trained for each target word (a.k.a.
approach). Training examples are represented by feature vectors (see Section 5) and
labeled with gold senses. At test time, inputs are processed and represented in the same

8http://www.itl.nist.gov/iad/mig//tests/ace/2008/doc/

7

way, as vectors of features, with appropriate classiﬁers predicting intended senses. Our
proposed system architecture for NED follows this design (see Section 3).

WSD systems can perform well when training data are plentiful. On the Senseval-
3 lexical sample task [Mihalcea et al., 2004], accuracies can reach as high as 72.9%
with ﬁne-grained senses and 79.3% with coarser-grained senses . Both performance
numbers are well above the most-frequent sense (MFS) baselines (55.2 and 64.5%, re-
spectively). The test set used there comprised 57 target words, each backed by at least
100 manually-annotated examples. But many words found in running text lack sufﬁ-
cient training instances, leading to lower performance in evaluations when all words
are considered. The best accuracy reported for the Senseval-3 all words task, testing
all open-class words occurring in three texts (editorial, news and ﬁction), was 65.1%,
only slightly higher than the MFS baseline’s 62.4% [Snyder and Palmer, 2004]. Unfor-
tunately, it is expensive to produce thorough hand-tagged training data for WSD and
people do not typically annotate their words with the exact sense they meant.

In contrast with WSD, where training data is scarce, the upside for NED is that
labeled instances for many mentions are already available in large numbers, annotated
by Wikipedia volunteers, since the contexts and targets of hyper-links could be used as
supervision. In other respects, we ﬁnd many similarities between WSD and NED: For
each mention that could be used to refer to a named entity, the knowledge base may
list all possible targets, as the dictionary does for WSD. E.g., for a string like “John
Edwards” a disambiguation page may list all relevant entities (see Figure 1). Naturally,
there are also differences: A big downside for NED is that the inventory of meanings
is not explicit and, for most strings, will be badly incomplete, unlike dictionaries used
in WSD.

In Sections 6 and 8 we will explore the relation between WSD and NED further,
showing that (1) ambiguity, synonymy and incidence of dictionary misses are all higher
for NED than for WSD; (2) the NED task appears better-deﬁned, as signaled by higher
inter-annotator agreement than in WSD; (3) the skew of frequencies is more extreme
for NED, with MFS consequently presenting an even stronger baseline there than in
WSD; (4) the high number of training instances available to NED enables better super-
vised training, allowing NED systems to follow the same architecture as WSD systems,
using analogous preprocessing, feature sets, and classiﬁers; and (5) the high ambiguity
of mentions encountered by NED makes a typical word expert approach more com-
putationally expensive, but still feasible. Lastly, (6) we will discuss the feasibility of
constructing a comprehensive dictionary for NED.

2.3 Early Work on NED
The earliest work on NED using Wikipedia is by Bunescu and Pasca [2006], who used
article titles, redirects and disambiguation pages to generate candidate entities. Simi-
larity between a mention’s context and article text provided the rankings, according to
tf-idf and cosine similarity. Each article’s term vector was further enriched using words
from other articles in the same category. Disambiguation of mentions was local, i.e.,
performed separately for each one.

Cucerzan [2007] followed an overall similar design, but using context vectors that
consisted of key words and short phrases extracted from Wikipedia. He disambiguated

8

all named entities in text simultaneously, adding a global constraint that required target
Wikipedia articles to come from the same category. Candidate lists were augmented
with link information whenever a given anchor-text mentioned the same target entity
from at least two different Wikipedia pages. His approach was later reimplemented (see
Section 8.7) by Hachey et al. [2012], who also reimplemented the earlier system of
Bunescu and Pasca [2006].

Fader et al. [2009] also generated candidates as did Cucerzan [2007]. They intro-
duced prior probabilities — estimated from the numbers of anchors that refer to entities
— in addition to considering the overlap between the context of a mention and the text
of the target articles.

2.4 Wikiﬁcation
Most research on wikiﬁcation obviates candidate generation and focuses on disam-
biguation. In seminal work [Mihalcea and Csomai, 2007, Csomai and Mihalcea, 2008],
the authors used mentions in anchors to train a supervised (na¨ıve Bayes) classiﬁer.
This is the work most similar to ours. However, they did not address the problems
of building a dictionary or different methods to collect training data (cf. Section 5.2).
Wikiﬁcation work continued with Milne and Witten [2008], who combined popularity
and relatedness (computed as the number of inlinks shared between the context and
target articles), using several machine learning algorithms. They were the ﬁrst to use
the link structure of Wikipedia. Kulkarni et al. [2009] later proposed a method that col-
lectively wikiﬁed an entire document, by solving a global optimization problem, using
ideas from both Milne and Witten [2008] and Cucerzan [2007], but now applied in the
context of wikiﬁcation.

Zhou et al. [2010] also built on ideas of Milne and Witten [2008] and included, for
the ﬁrst time, relatedness between entities (based on search engine log analysis). They
tested two classiﬁers (binary and learning-to-rank), with mixed results. Ratinov et al.
[2011] utilized link structure — again, following Milne and Witten [2008] — to arrive
at coherent sets of disambiguations for input documents. They used a ranker to select
best-ﬁtting entities. Although anchor context was used to compute similarity between
article texts, it was not tapped for features during classiﬁcation. Evaluation against
their own dataset showed that improvements over local disambiguation were small.
Guo et al. [2011] use direct hyperlinks between the target entity and the mentions in
the context, using directly the number of such links.

2.5 Current NED Systems
Given the popularity of NED, we focus this review on systems that evaluate against
TAC-KBP 2009 and 2010 data — two of the most cited NED datasets — to which we
will compare our own results (see Section 8.7).

In 2009, Varma et al. [2009] obtained the best results, using anchors from Wikipedia
to train one classiﬁer per target string, combined with querying of mention contexts
against an online search engine. For candidate generation they used a complex mixture
of redirect links, disambiguation pages and bold text in leading paragraphs of articles,
metaphones to capture possible spelling variations and errors, and a separate module

9

that tried to ﬁnd acronym expansions. In addition, they used a dynamic algorithm that
returned entities matching some (but not all) tokens of a target string.

In 2010, Lehmann et al. [2010] did best. They used titles, redirects, anchors and
disambiguation pages, as well as Google searches, dynamic generation of acronym
expansions, longer mentions and inexact matching. Their system pulled in features
from both McNamee [2010] and Milne and Witten [2008] to train a binary classiﬁer.
McNamee [2010] had the highest-scoring submission among systems that did not use
the text in target Wikipedia articles, focusing on the provided KB alone for building
the list of candidate entities. A closely related system by Dredze et al. [2010] included
ﬁnite-state transducers that had been trained to recognize common abbreviations, such
as “J Miller” for “Jennifer Miller.”

Zheng et al. [2010] also use a number of features that resemble those of McNamee
[2010]. They evaluated several learning-to-rank systems, with ListNet yielding best
results. Unfortunately their systems were developed, trained and tested on the same
corpus, making it unclear whether their results are comparable to those of other sys-
tems evaluated in TAC-KBP 2009. A similar system [Zhang et al., 2010] made use of
a synthetic corpus, with unambiguous occurrences of strings replaced by ambiguous
synonyms. Manufactured training data was then combined with Wikipedia. Unfor-
tunately, since the overall system was developed and evaluated on the same (2009)
corpus, it may have been overﬁtted.

Han and Sun [2011] proposed a generative model with three components: popular-
ities of entities, probabilities of strings lexicalizing an entity, and probabilities of entire
documents given a seed entity, estimated from anchor data. They included a transla-
tion model, learning string-entity pairs from thousands of training examples. Although
their results are highest for TAC-KBP 2009 published to date, they may be overstated,
since some parameters were tuned using cross-validation over test data.

Recently, Hachey et al. [2012] reimplemented three well-known NED systems [Bunescu

and Pasca, 2006, Cucerzan, 2007, Varma et al., 2009], combined them, and carefully
analyzed the performance of candidate generation and disambiguation components for
each. They studied contributions from a variety of available candidate sources, includ-
ing titles (of articles, redirects and disambiguation pages) and link anchors, as well as
two additional heuristics: bold in ﬁrst paragraph and hatnote templates from popular
entities to corresponding disambiguation pages.

None of the published NED systems, except Csomai and Mihalcea [2008], uses
the context of anchors in Wikipedia to train a classiﬁer for each mention, as we do.
In fact, among other relevant systems which have been tested against data sets other
than TAC-KBP, all use other kinds of techniques. Given that those other systems have
been evaluated on other datasets, we will skip their description here. As mentioned
in the introduction, most of the papers on NED describe complex systems with many
components and data sources but few ablation studies that might help understand the
contribution of each.9 In contrast, our system relies solely on static dictionary look-ups
and supervised classiﬁcation of mentions.

9Again, with the notable exception of work by Hachey et al. [2012], which we will discuss in detail (see

Section 8.7).

10

Figure 6: System architecture.

3 System Architecture

Figure 6 summarizes our approach to NED, which is inspired by the word-expert tech-
nique, popular in mainstream WSD systems. We take a surface text form (mention),
with the context of its occurrence, and determine an appropriate entity (meaning) to
which the input string may refer. In this example, our system disambiguates the men-
tion “Abbott” in the sentence “The voice was provided by Candy Candido, who brieﬂy
became Abbott’s partner after Costello had died.” First, a context-independent compo-
nent (the dictionary) expands the string to a set of potentially-referent candidate enti-
ties, ordered by popularity; next, a context-sensitive component (supervised classiﬁer)
selects a candidate that seems most appropriate for the context.

The context-independent dictionary maps strings to ordered lists of possible Wikipedia

titles; it also provides scores — which are indicative of conditional probabilities of
each article given the string — that determine the ranking. This (untuned) component
is constructed, primarily, using anchor-texts — of (internal) hyper-links between En-
glish Wikipedia articles and (external) web-links into the greater Wikipedia’s pages,
covering many languages (see Section 4 for a detailed description of the dictionary and
its variants). According to our dictionary, the most probable entity for the mention
“Abbott” — ignoring context — is Abbott_Laboratories.

From the set of candidate Wikipedia titles, a classiﬁer can select more appropriate
entities, using not only the mention but also surrounding text, in the relevant document.
This (optimized) component is trained on contexts around inter-Wikipedia links, i.e.,
pointers at entities mentioned in Wikipedia articles. We train a separate classiﬁer for

11

Context Independent ComponentContext Dependent DisambiguationWikipedia titles0.14 Abbott_Laboratories0.07 John_Abbott0.05 Bud_Abbott0.05 Abbott,_Texas…DictionarySupervised ClassifierMentionAbbottWikipedia titles4.52 Bud_Abbott-0.10 Abbott,_Texas-0.39 John_Abbott…Top Choice SelectorWikipediaContext… became Abbott’s partner after Costello…Bud_AbbottContextMentionevery string: its job is to return the entity which best ﬁts the shared mention’s con-
text (this and other variants of our supervised classifers are described in Section 5). In
light of context, our system prefers the entity Bud_Abbott.

One could view these two components as performing (i) candidate generation, list-
ing all possible entities for a mention; and (ii) candidate selection [Hachey et al., 2012],
although our generation module also scores and ranks entities (by popularity). Follow-
ing the WSD convention, we will refer to these subsystems as (i) the dictionary; and
(ii) the disambiguation component.

4 The Dictionary

The dictionary is a cornerstone component of our system, serving two objectives. Its
primary goal is to provide a short list of candidate referent Wikipedia articles for any
string that could name an entity. In addition, it provides a score that quantiﬁes the
string’s afﬁnity for each candidate entity.

If the dictionary fails to recognize that a given string could refer to a particular
entity, then our system will not be able to return that entity. Thus, the dictionary intro-
duces a performance bottleneck. We will use this fact to measure an upper bound on
our system’s performance, by means of an oracle whose choices are restricted to just
the entities proposed by the dictionary.

The dictionary represents a set of weighted pairs — an exhaustive enumeration of
all possible string-entity combinations, with corresponding strengths of association.
We constructed this resource from all English Wikipedia pages (as of the March 6th,
2009 data dump) and many references to Wikipedia from the greater web (based on a
subset of a 2011 Google crawl). Individual string-entity co-occurrences were mined
from several sources:

• Article titles that name entities, e.g., “Robert Redford” from http://en.
wikipedia.org/wiki/Robert_Redford.10 Many title strings had to be
processed, e.g., separating trailing parentheticals, like “(athlete)” in Figure 2,
and underscores, from names proper.

• Titles of pages that redirect to other Wikipedia pages, e.g., “Stanford” for the

article Stanford_University, redirected from the page Stanford.

• Titles of disambiguation pages that fan out to many similarly-named articles,
e.g., linking “Stanford” to Stanford_University, Aaron_Stanford or
Stanford,_Bedfordshire, among other possibilities, via Stanford_
(disambiguation).

• Anchor text, e.g, we interpret the fact that Wikipedia editors linked the two

strings “Charles Robert Redford” and “Robert Redford Jr.” to the article Robert_

10In the remainder of this article, we will use the following conventions: “string” for a string that can
be used to name an entity (e.g., “Robert Redford”) and suﬁxes of English Wikipedia URLs, without the
preﬁx http://en.wikipedia.org/wiki/, as short-hand for corresponding articles (e.g., Robert_
Redford).

12

Route_102_(Virginia_pre-1933)
State_Route_102_(Virginia_1928-1933)
State_Route_63_(Virginia_1933)
State_Route_63_(Virginia_1940)
State_Route_758_(Lee_County,_Virginia)

State_Route_102_(Virginia_1928)
State_Route_102_(Virginia_pre-1933)
State_Route_63_(Virginia_1933-1946)
State_Route_63_(Virginia_pre-1946)
Virginia_State_Route_758_(Lee_County)

Figure 7: A connected component of URLs that redirect to Virginia_State_
Route_758_(Lee_County), which itself does not redirect further and is there-
fore selected as the canonical article for the entire cluster.

Redford as a strong indication that both could refer to “Robert Redford.” We
use the number of links connecting a particular string with a speciﬁc entity as a
measure of the strength of association.

Note that our dictionary spans not just named entities but also many general topics
for which there are Wikipedia articles. Further, it transcends Wikipedia by including
anchors (i) from the greater web; and (ii) to Wikipedia pages that may not (yet) ex-
ist. For the purposes of NED, it could make sense to discard all but the articles that
correspond to named entities. We keep everything, however, since not all articles have
a known entity type, and because we would like to construct a resource that is gen-
erally useful for disambiguating concepts. Our dictionary can disambiguate mentions
directly, simply by returning the highest-scoring entry for a given string. Next, we
describe the method used to create several different variants of the dictionary.

4.1 Redirects and Canonical Pages

One of many hassles involved in building a Wikipedia-based dictionary stems from
redirects, as it is important to separate articles that are actual entries in Wikipedia from
other place-holder pages that redirect to them. We will use the term URL in this context
to refer indiscriminately to any article page (redirect or not), e.g., both http://en.
wikipedia.org/wiki/Stanford and http://en.wikipedia.org/wiki/Stanford_
University, where it is forwarded.

To collect all titles and URLs that refer to the same article, we ﬁrst map all such
strings to URLs using Wikipedia’s canonicalization algorithm.11 We then connect any
two URLs that either appear together as an ofﬁcial redirection (in a Wikipedia dump)
or get redirected at crawl-time (via HTTP status codes 3xx). For each title and URL,
we then extract a connected component, represented by a single canonical article,
with preference given to (1) non-redirect pages from the Wikipedia dump; followed
by (2) ofﬁcial redirect pages; and ﬁnally (3) pages that did not appear in any ofﬁcial
dump.12 Within each preference category, we resolved ties lexicographically. Figure 7
shows a simple example clustering of URLs, with a canonical article.

11http://en.wikipedia.org/wiki/Wikipedia:Canonicalization
12This process can be particularly complicated when reconciling snapshots of Wikipedia taken at different
times (i.e., an ofﬁcial dump and a web crawl), since sources and targets of redirects can switch roles over
time, yielding cycles.

13

0.9976
0.0012
0.0006
0.0006
0
0
0
0

Hank_Williams
Your_Cheatin’_Heart
Hank_Williams_(Clickradio_CEO)
Hank_Williams_(basketball)
Hank_Williams,_Jr.
Hank_Williams_(disambiguation)
Hank_Williams_First_Nation
Hank_Williams_III

W:936/938
2/938
W:

w:756/758

w:
w:

1/758
1/758

8:

all

list-
dictionary,
Figure
“Hank Williams.”
ing
Final column(s) report counts from Wikipedia (w:x/y) and the web crawl (W:u/v),
where available.

Sample
and

EXCT
string

articles

from

scores

for

the

the

4.2 The Core and Other Dictionaries
We created a core dictionary by extracting strings from titles of canonical articles,
redirects and disambiguation pages, as well as the referring anchor text encountered in
both Wikipedia and the Google crawl. In all cases, we paired a string with the canonical
article of the respective cluster.

Our core dictionary maps strings to sorted lists of Wikipedia articles, with asso-
ciated scores. These scores are computed from the occurrence frequencies of the an-
chor texts. For a given string-article pair, where the string has been observed as the
anchor-text of a total of y inter-Wikipedia — and v external — links, of which x (and,
respectively, u) pointed to a page that is represented by the canonical article in the pair,
we set the pair’s score to be (x + u)/(y + v).

We call this dictionary exact (EXCT), as it matches precisely the raw strings found
using the methods outlined above. For example, Figure 8 shows all eight articles that
have been referred to by the string “Hank Williams.” Note that this dictionary does no
ﬁltering: removal of undesired target Wikipedia pages (such as disambiguations) will
be done at a later stage (see Section 4.3).

4.2.1 Aggregating String Variants

In addition to exact string lookups, we also adopt two less strict views of the core
dictionary. They capture string variants whose lower-cased normalized forms are either
the same (the LNRM dictionary) or close (the FUZZ dictionary) by Levenshtein edit-
distance to that of the string being queried. In both cases, an incoming string now
matches a set of keys (strings) in the dictionary, whose lists of scored articles are then
merged, as follows: given n articles, with scores ai/bi, their aggregate score is also a

ratio, ((cid:80)n

i=1 ai) / ((cid:80)n

i=1 bi).

We form the lower-cased normalized variant l(s) of a string s by canonicalizing
Unicode characters, eliminating diacritics, lower-casing and discarding any resulting
ASCII-range characters that are not alpha-numeric. If what remains is the empty string,
then s maps to no keys; otherwise, s matches all keys k such that l(s) = l(k), with the
exception of k = s, to exclude the original key (which is already covered by EXCT).
Figure 9 shows a subset of the LNRM dictionary, with combined contributions from

14

0.9524
0.0476
0
0
0

Hank_Williams
I’m_So_Lonesome_I_Could_Cry
Hank_Williams_(Clickradio_CEO)
Hank_Williams_(basketball)
Hank_Williams_(disambiguation)

W:20/21
W: 1/21

Figure 9: Sample from the LNRM dictionary (note that contributions already in
EXCT are excluded), for strings with forms like l(Hank Williams) = hankwilliams =
l(HANK WILLIAMS) = l(hank williams), etc.

0.6316
0.3158
0.0526

Tank_Williams
Hank_Williams
Your_Cheatin’_Heart

w:12/12

W:6/7
W:1/7

Figure 10: Sample from the FUZZ dictionary (note that contributions already in EXCT
or LNRM are excluded).

strings k that are similar to (but different from) s.
We deﬁne the FUZZ dictionary via a metric, d(s, s(cid:48)): the byte-level UTF-8 Leven-
shtein edit-distance between strings s and s(cid:48). If l(s) is empty, then s maps to no keys
once again; otherwise, s matches all keys k, whose l(k) is also not empty, that mini-
mize d(l(s), l(k)) > 0. This approach excludes not only k = s but also l(k) = l(s),
already covered by LNRM. For example, for the string Hank Williams, there exist keys
whose signature is exactly one byte away, including Tank Williams, Hanks Williams,
hankwilliam, and so forth. Figure 10 shows the three articles — two of them already
discovered by both EXCT and LNRM dictionaries — found via this fuzzy match.

4.2.2 Querying Search Engines

In addition to constructing our own custom dictionaries, we also experimented with
emulating a lookup by (twice) querying the Google search engine,13 which we call
the GOOG dictionary, on the ﬂy. Our query pairs consisted of the raw target string,
<str> — and also a separately-issued phrase-query, “<str>” — combined with
a restriction, site:en.wikipedia.org. We kept only the returned URLs that
begin with http://en.wikipedia.org/wiki/, using their (sums of) inverse
ranks as scores. Although this method cannot be used effectively to ﬁnd all strings that
may name an entity, given any string, the dictionary portion for that speciﬁc string can
be ﬁlled in dynamically, on demand.

4.3 Disambiguating with Dictionaries
For any input string, one could simply use a dictionary to select a highest-scoring en-
tity. Our core dictionary’s scores already capture the popularity of each entity given a
string, approximated by the frequency of their pairing in anchor texts. The Google dic-
tionary may, in addition, reﬂect relatedness to the text of the corresponding Wikipedia

13http://www.google.com/

15

article, according to more sophisticated ranking algorithms. Such approaches would be
closely related to the MFS heuristic in WSD and could also be viewed as context-free
disambiguation, since they do not take into account the context of the mention, always
returning the same article, regardless of the surrounding text.

A set of dictionaries, like ours, can be abstracted away behind a single dictionary
interface, to be used by downstream components.
Internally, dictionaries could be
combined using a variety of strategies. We suggest a backoff approach that checks the
dictionaries in order of precision, and also a heuristic that discards very unlikely can-
didates. Other strategies, such as using decision trees or a weighted combination of the
dictionaries, are also possible. But many of these strategies would involve parameter
tuning (e.g., learning the weights). A thorough exploration of more optimal techniques
for combining dictionaries lies outside of the present paper’s scope.

Our ﬁrst strategy is a cascade of dictionaries: for a given string, it provides a
ranked list of entities by consulting the dictionaries in order of their precision. It starts
from EXCT, which is most precise, backing off to the rest. We consider two speciﬁc
cascades (in remaining sections, all references to the LNRM dictionary correspond to
the LNRM cascade, and similarly for FUZZ):
• the LNRM cascade ﬁrst checks the EXCT dictionary, returning its associated
entities and scores if the string has an entry there, and defaulting to LNRM
dictionary’s results if not;
• the FUZZ cascade also ﬁrst checks the EXCT dictionary, backs off to the LNRM
dictionary in case of a miss, but then ﬁnally defaults to the contents from the
FUZZ dictionary.

Our second strategy is a heuristic combination (HEUR): it combines suggestions
from the original (EXCT, LNRM, and FUZZ) dictionaries while also ﬁltering out
some of the noise. Its goal is to retain good suggestions without drowning in obviously
bad choices: since many titles suggested by the FUZZ dictionary are noisy, we only
include those for which there is additional evidence of relevance (for instance, if the
suggestion is an acronym for the string). Similarly, if a string has been used to link to
an article only a few times, the connection between them may not be reliable, calling
for more evidence before the entity could be accepted as a possible referent for the
string. Naturally, we also discard articles that are clearly not real entities — such as
disambiguation pages, “list of” pages, and pages of dates — using additional features
collected with the core dictionary. Table 1 summarizes our complete list of heuristics,
which was ﬁnalized by consulting the news subset of the 2010 TAC-KBP development
dataset (see Section 6.1).

Heuristic combinations can yield dictionaries that are considerably smaller than full
cascades. For example, for the string ABC, the EXCT dictionary offers 191 options,
the LNRM cascade 253, and the FUZZ cascade 3,527. But with the above-mentioned
ﬁlters, the number of candidate Wikipedia titles can be reduced to just 110. Although
our heuristics could be applied to any dictionary or cascade, in the remainder of the
paper we will use the short-hand HEUR to refer to the larger FUZZ cascade, followed
by the application of these heuristic ﬁltering rules.
Inevitably, HEUR will sometimes discard a correct mapping in favor of inferior
choices. For instance, the feasible suggestion Angela Dorothea Kasner → Angela_
Merkel gets dropped, but the undesirable association Angela Merkel → German_

16

1
2
3
4

5

Rule
Discard disambiguation pages.
Discard date pages.
Discard list-of pages.
Discard pages only suggested by FUZZ, unless:
· string and title could be an acronym pair;14
· string is a substring of the title;
· string is very similar to the title.15
Discard articles supported by few links,16unless:
· article may disambiguate the string;
· string is the title of the page.

Example
Discard:
Discard:
Discard:
Discard:

* → Hank Williams (disambiguation)
* → 2000
* → List of cheeses
MND → MNW
Keep: NDMC → National Defense Medical Center
DeLorean Motor → DeLorean Motor Company
Keep:
Chunghua Telecom → Chunghwa Telecom
Keep:
Washington → Tacoma, Washington
Discard:
CNS → Szekler National Council
Keep:
Chunghwa Telecom → Chunghwa Telecom
Keep:

Table 1: Rules of the heuristic combination dictionary (HEUR).

federal_election,_2005 is kept. Nevertheless, with a reduced set of candi-
dates, it will be easier for supervised classiﬁers to narrow down remaining options
based on a document’s context, down the road (see Section 5).

5 Supervised Disambiguation

The large number of naturally-occurring links pointing at entities in Wikipedia makes it
possible to gather richly-annotated data automatically, to augment context-free disam-
biguation provided by the raw frequency information the dictionaries. In this respect,
NED differs from many other applications in natural language processing and informa-
tion retrieval, where most available input data is not already pre-labeled, and substantial
resources are devoted to manual annotation efforts.

5.1 Core Method
We followed a mainstream WSD approach (word-expert), training supervised classi-
ﬁers for all target strings, as follows: For every string in the dictionary, we ﬁrst identify
the entities to which it may refer. We then gather all example spans from Wikipedia
articles that contain links to any of these entities. To ensure that our training data is nat-
ural language (and not, e.g., lists or tables), we only include text marked as paragraphs
(i.e., enclosed between HTML tags <P> and </P>). The relevant training subset for a
target string then consists of example contexts with anchor texts containing the string.17

14I.e., either (a) the string is an acronym for the title; or (b) the title is an acronym for the string.
15I.e., either (a) the strings are the same; or (b) both strings have length less than or equal to six, with an
edit distance exactly equal to one; or (c) the ratio between edit distance and string length is less than or equal
to 0.1.

16I.e., if (a) the number of total links to the page (both inside Wikipedia and from the external web) is no
more than ten; or (b) the number of times the string links to the page is no more than one; or (c) the score is
no more than 0.001.

17The target string is a substring of the anchor text after case normalization.

17

On February 27, 2004, SuperFerry 14 was bombed by the Abu Sayyaf terrorists
killing 116 people .

It was considered as the worst terrorist attack ...

anchor text

lemmas in the span

lemma for N/V/A
in a 4 token window
around the anchor text

lemma and word for N/V/A
before the anchor text

lemma and word for N/V/A
after the anchor text

bigrams around anchor text

trigrams around anchor text

noun (lemma)
noun (word)
verb (lemma)
verb (word)

adjective (lemma)
adjective (word)
noun (lemma)
noun (word)
verb (lemma)
verb (word)

lemma before
lemma after
POS before
POS after
word before
word after

lemma before
lemma around
lemma after
POS before
POS around
POS after
word before
word around
word after

Abu Sayyaf

terrorist

kill
...

be

bomb
kill

people

terrorist

SuperFerry
SuperFerry

bomb

bombed

bad

worst

terrorist
terrorists

kill

killing

the Abu Sayyaf

Abu Sayyaf terrorist

DT J
J N2

the Abu Sayyaf

Abu Sayyaf terrorist

by the Abu Sayyaf

the Abu Sayyaf terrorist
Abu Sayyaf terrorist kill

P-ACP DT J

J N2 VVG
DT J N2

by the Abu Sayyaf

the Abu Sayyaf terrorists

Abu Sayyaf terrorists killing

Figure 11: Example training context and features extracted from Wikipedia’s article
for SuperFerry.

18

We take spans of up to 100 tokens to the left — and another 100 to the right — of a
link to be the contexts. Figure 11 shows one such sample training instance in detail.

Since the EXCT dictionary often provides too few examples, we used the LNRM
cascade as our source dictionary (with remapped canonical articles — see Section 4.1).
Given this training data, we applied standard machine learning techniques to perform
supervised disambiguation of entities. We trained a multi-class classiﬁer for each target
string. Then, given a mention of the target string in the test data, we applied its classi-
ﬁer to the context of the mention, and returned the corresponding article. We did not
construct classiﬁers for strings whose training data maps to a unique entity. Instead, in
those cases, a default classiﬁer falls back to LNRM cascade’s output.

From each context, we extracted features (see Figure 11) commonly used for su-
pervised classiﬁcation in the WSD setting [Agirre and Lopez de Lacalle, 2007, Zhong
and Ng, 2010]:

• the anchor text;
• the unordered set of lemmas in the span;
• lemma for noun/verb/adjective in a four-token window around the anchor text;
• lemma/word for noun/verb/adjective before and after the anchor text;
• word/lemma/part-of-speech bigram and trigrams including the anchor text.

5.2 Variations
Over the course of developing our system, we tested several variations of the core algo-
rithm:
Classiﬁer: We tried maximum entropy models (MAXENT) and support vector ma-
chines
(SVM).
Dictionary: A dictionary inﬂuences supervised classiﬁcation in two places. First,
when building the training data, to ﬁlter example spans selected for training. And
second, as a backup ranker, for cases when a classiﬁer is not trained, due to a lack of
examples. In both the ﬁltering stage and the back-off stage, we compared using the
HEUR dictionary in place of the LNRM cascade.
Span: In addition to training with contexts of (up to) 100 tokens to the left and right
of a string, we also tried single-sentence and full-paragraph spans (the 100, SENT and
PARA
variants).
Match: When gathering examples for a target string, we made sure that the anchor
text contains this string (the LEX default). Alternatively, we could allow additional
examples, ignoring anchor text mismatch (the SENSE variant): given the entities that
a dictionary lists for the target string, we include as training examples all contexts that
apply to these entities, regardless of their anchor text. In this variant, the target string
is simply treated as another feature by the classiﬁer. If a test example’s string does
not match any of the anchor text seen in training, then features that include the target
string (i.e., its unigram, bigram, and trigram features) will not ﬁre. Classiﬁcation will
then depend on features describing the rest of the context: a classiﬁer could still give a
high score, but only if surrounding words of a span carry a strong enough signal for an
entity. This approach may allow us to classify aliases for which there isn’t exact train-
ing data, provided that our ﬁltering dictionary yields a precise list of potential entities
corresponding to a target string.

19

<entity wiki_title="Mike_Quigley_(footballer)" type="PER"

id="E0000001" name="Mike Quigley (footballer)">

<facts class="Infobox Football biography">
<fact name="playername">Mike Quigley</fact>
<fact name="fullname">Michael Anthony Joseph Quigley</fact>
<fact name="dateofbirth">October 2, 1970 (1970-10-02) (age_38)</fact>
<fact name="cityofbirth"><link entity_id="E0467057">Manchester</link></fact>
<fact name="countryofbirth"><link entity_id="E0145816">England</link></fact>
<fact name="position"><link>Midfielder</link></fact>
</facts>
<wiki_text><![CDATA[Mike Quigley (footballer)
Mike Quigley (born 2 October 1970) is an English football midfielder.
]]></wiki_text>
</entity>

Figure 12: Example of a KB person entity from the TAC-KBP dataset.

6 Datasets for NED

Evaluation of NED systems requires manually annotated data. Although many corpora
have been introduced in various papers, we decided to focus on the earlier datasets
developped for the entity linking task of the knowledge-base population (KBP) track
at Text Analysis Conferences (TAC),18 which have been running annually each year
since 2009 [McNamee and Dang, 2009, Ji et al., 2010, Ji and Grishman, 2011].

The TAC-KBP evaluation focuses on three main types of named entities: per-
sons (PER), organizations (ORG) and geo-political entities (GPE). Given a set of hand-
selected mentions of entities — and documents containing these strings — the task is
to determine which knowledge-base instance, if any, corresponds to each named entity
string. The knowledge-base (KB) is derived from a subset of Wikipedia. Mentions
are chosen among occurrences in a collection of 1,286,609 newswire documents and
490,596 web-pages. The tasks’ organizers have released substantial amounts of devel-
opment and test data, as well as standardized evaluation software.

There are several reasons why we chose to evaluate against TAC-KBP data: (i) it
consists of named entity mentions from two genres (news articles and pages crawled
off the web); (ii) it focuses on several taget entity mentions, making it well suited to
our word-expert approach; and (iii) its high number of participating systems — and
subsequent publications — provide an informative setting for comparing state-of-the-
art techniques. Among TAC-KBP datasets, those from 2009 and 2010 attract the largest
number of papers [Varma et al., 2009, McNamee, 2010, Lehmann et al., 2010, Zheng
et al., 2010, Dredze et al., 2010, Zhang et al., 2010, Han and Sun, 2011, Ploch, 2011,
Chen and Ji, 2011, Gottipati and Jiang, 2011, Hachey et al., 2012, Han and Sun, 2012].
In the future we would like to extend our work to other datasets which include all
mentions in full documents [Hoffart et al., 2011].

20

<query id="EL55">
<name>ABC</name>
<docid>AFP_ENG_20070104.0533.LDC2009T13</docid>
</query>

Figure 13: Example of a query with document ID and entity name from the TAC-KBP
dataset.

6.1 The TAC-KBP Dataset

The TAC-KBP exercise provides an inventory of target named entities, based on a
subset of Wikipedia articles that had info-boxes in October of 2008. This KB con-
tains 818,741 entities (a.k.a. KB instances), each marked with (i) its name (a string);
(ii) the assigned entity type (one of PER, ORG or GPE); (iii) a KB instance ID (a
unique identiﬁer, such as E001); (iv) the set of info-box slot names and values from
the corresponding Wikipedia page; and (v) the text of that Wikipedia page. Figure 12
shows a sample KB entry for a person, whose entity derives from the Wikipedia article
identiﬁed by the URL Mike_Quigley_(footballer).

Given a query that consists of a string and a document ID (see Figure 13), the task
is to determine the knowledge-base entity to which that document’s string refers (or
to establish that the entity is not present in the reference KB). The document provides
context which may be useful in disambiguating the string. A referent entity will gen-
erally occur in multiple TAC-KBP queries, under different surface name variants and
in different documents. Because of possible auto-correlations, ofﬁcial rules stipulate
that queries must be processed independently of one another. As expected, some en-
tities share confusable names (e.g., the string Stanford refers to a university, its town
and founder, among many other possibilities). For each query, a system must return a
KB ID (or NIL when there is no corresponding KB-instance). All of the queries had
been tagged by a team of annotators. Inter-annotator agreement is high for organiza-
tions (ORG: 92.98%) and people (PER: 91.53%), and somewhat lower for remaining
entities (GPE: 87.5%).

TAC-KBP has been running and releasing development and test data each year
since 2009. It attracted 13 teams in the ﬁrst year and 16 teams in 2010. We use the
same data split as in the 2010 task. For the dev-set, all news examples come from
the 2009 test-set, and web examples are exclusively from the 2010 training set; the
2010 test-set contains both types. Table 2 gives a break-down of our development and
evaluation sets by the three entity types (PER, ORG, GPE). Note that news samples in
the dev-set are especially skewed toward ORG, with approximately ﬁve times as many
examples as either PER or GPE; the rest of the data is perfectly balanced.

Since the inventory of entities used in TAC-KBP is incomplete, it is possible for
queries to refer to unlisted entities. In such cases, human annotators tagged the men-
tions as NILs. Note that dthe development and test datasets a large number of NIL
mentions, 49% and 55%, respectively. We evaluate on non-NIL mentions.

18http://www.nist.gov/tac/

21

Development
Evaluation

Total
1,500
3,904
5,404
2,250
1,500
750

PER
500
627
1,127
750
500
250

ORG
500
2,710
3,210
750
500
250

GPE
500 web (2010 train)
news (2009 test)
567
1,067
750
news (2010 test)
500
250 web (2010 test)

Table 2: Number of examples in the development and evaluation datasets, broken down
by genre and entity type. We explicitly mention the relation to the 2009 and 2010 TAC
KBP datasets.

Development
Evaluation

Unique Strings
1,162
752

No Entities
(NILs)
462
366

Single Entity
(Monosemous)
488
317

Multiple Entities

(Polysemous)
112
69

Ambiguity
2.34
2.17

Table 3: Ambiguity of target strings in the development and evaluation datasets, ac-
cording to the gold standard.

6.2 Ambiguity

Some simple measurements can give an idea as to the difﬁculty of a problem or dataset.
In our case, one such quantity is a mention’s ambiguity: a lower bound on the number
of different KB-instances to which its string can refer. In WSD, an equivalent notion
would be the concept of polysemy. In our dictionary, for example, the string ICNC
covers four distinct entities in Wikipedia.

We estimated ambiguity as follows: For each target string in a dataset, we counted
the number of different KB-instances associated to it by human annotators. Table 3
reports ambiguities in both development and evaluation sets. The section of the dev-
set, for instance, comprises 1,162 unique target strings (types). Of those, 462 were
tagged with NIL, as no referent entity was deemed appropriate in the KB; 488 strings
had all their mentions tagged with the same entity; and only 112 had been tagged with
multiple KB-instances — an average of 2.34 entities per ambiguous string. Of course,
this calculation grossly underestimates actual polysemy, as the number of potential
articles to which a string could refer may far exceed those found in our collection by
annotators. As mentioned in the previous section, populating a complete list of entities
that could have been invoked is part of the task, and as such, still an open question.
However, many target strings tend to be highly skewed, in the sense that they usually
refer to one speciﬁc entity. This fact is reﬂected by the large number of strings which
refer to a single entity in the gold standard sample.

We will show that actual polysemy is much higher (e.g., according to our dictio-
naries — see Section 6.4), since many of the possible entities do not appear in standard
datasets. This fact further differentiates NED from typical WSD settings, where most
senses can be found in a gold standard. Table 4 shows polysemy as average number

22

Words

(Polysemous)
20
32
5
57

Ambiguity

GS Dictionary
5.80
5.05
6.31
4.56
6.20
10.20
6.47
4.88

Nouns
Verbs
Adjectives
Total

Table 4: Ambiguity for Senseval-3 lexical sample (a popular WSD dataset), including
all senses covered in the gold standard (GS), as well as any senses attested by the
dictionary; note that all of the words are polysemous.

Development
Evaluation

Unique Entities
1,239
871

1,053
853

Single String Multiple Strings
(15%)
(2%)

(85%)
(98%)

186
18

Average Synonymy
2.49
2.06

Table 5: Synonymy of target entities in the development and evaluation datasets, ac-
cording to the gold standard.

of senses for a WSD set-up (Senseval-3), with respect to both gold-standard data and a
dictionary, which happen to be fairly similar. Yet another point of contrast is that many
mentions in NED refer to only a single entity, whereas in WSD all 57 target words are
polysemous (with multiple senses attested by the gold standard).

6.3 Synonymy
Another quantity that sheds light on the complexity of a disambiguation task, in a man-
ner that is complementary to ambiguity, is the number of different strings that can be
used to name a particular entity: its synonymy, in WSD terms. As before, tallying all
unique strings that may refer to a given entity is an open problem. Table 5 tabulates the
gold standard’s synonymy (i.e., the number of strings found in the document collection
by the annotators) for each entity in the KB. Most entities are associated with only a
single string, especially in the evaluation sets. This could be an artifact of how the
organizers constructed the test data, since their procedure was to ﬁrst choose ambigu-
ous strings and then ﬁnd documents containing different meanings (entities) for these
mentions — as opposed to ﬁrst choosing entities and then querying string variants of
names. The annotators thus did not search for alternative lexicalizations (synonyms)
to be used as query strings. As was the case with ambiguity, actual synonymy is much
higher (see Section 6.4, in which we compute similar ﬁgures to analyze some of the
dictionaries used by our systems).

Table 6 shows average synonymy for a WSD dataset (Senseval-3 lexical sample).
The gold standard contains 278 senses (for 57 target words), of which 85 have a unique
lexicalization (and 193 have multiple), with 3.66 synonyms on average. Average syn-
onymy in this WSD setting is slightly higher than that of TAC-KBP’s gold standard,
similarly to polysemy (though Section 6.4 will show that, in practice, synonymy for

23

Nouns
Verbs
Adjectives
Total

Unique Synsets
101
146
31
278

Single string Multiple Strings Average Synonymy
3.34
3.84
3.87
3.66

71
107
15
193

30
39
16
85

Table 6: Synonymy for Senseval-3 lexical sample, as attested by the gold standard.

LNRM
HEUR
LEX + HEUR
SENSE + HEUR

Unique
Strings
1,162
1,162
1,162
1,162

No Entities
(NILs)
111
94
193
136

Single Entity
(Monosemous)
186
331
372
344

Multiple Entities

(Polysemous)
765
737
597
682

Ambiguity
86.14
22.26
15.44
19.01

Table 7: Ambiguity, according to dictionaries, of target strings in the development
dataset: top six rows show ﬁgures for the LNRM and HEUR dictionaries; last two
rows focus on HEUR only, after discarding entities that lack training examples, ac-
cording to the LEX and SENSE methods (see Section 5).

NED can be much higher, according to a dictionary ).

To summarize, the number of cases where annotators could not assign an entity
(NILs) is signiﬁcantly higher in NED than in WSD (around 50% compared to just
1.7%). And ambiguity and synonymy, according to gold standards, are substantially
lower than in WSD (with average ambiguities of around 2 vs. 5, and average poly-
semies of 2 vs. 4). These statistics are misleading, however, since TAC-KBP’s actual
ambiguity and synonymy are more extreme, according to our dictionaries (discussed
in Section 6.4). Finally, inter-annotator agreement for TAC-KBP is higher — ranging
between 87% and 93%, depending on the entity type — compared to 72% in WSD.

6.4 Ambiguity and Synonymy in Dictionaries
We now consider polysemy and synonymy in our dictionaries. Table 7 reports am-
biguities for the LNRM and HEUR dictionaries on development data. For instance,
the dev-set comprises 1,162 target strings, of which LNRM has no suggestions for
111 (94 with HEUR). Among the remaining strings, 331 yield a unique suggestion
from Wikipedia (331 with HEUR). The average ambiguity for the 765 strings with
multiple candidates is 86.14 (only 22.26 with HEUR). The distribution of the ambigu-
ity between news and web datasets is distinct, 50.90 and 103.24, respectively. Thus,
HEUR covers more strings and has fewer monosemous entries yet lower polysemy
than LNRM. Thus, HEUR retains more references from Wikipedia and simultane-
ously substantially reduces ambiguity. Although we don’t tabulate ﬁgures for other
dictionaries, we note here that FUZZ has even higher polysemy than LNRM. The two
lower rows in the the table also shows ambiguities faced by our supervised classiﬁers,

24

Unique
Entities
1,239
1,239

No

Strings

279 (23%)
296 (24%)

Single
String
0
0

Multiple
Strings

960 (77%)
943 (76%)

Average
Synonymy
210.38
46.37

EXCT
EXCT-HEUR

Table 8: Synonymy of target entities in the development dataset for the EXCT dic-
tionary, with and without ﬁltering heuristics; since some of these entities are not in
Wikipedia, they cannot be suggested by our dictionaries.

Nouns
Verbs
Adjectives
Total

Unique Synsets
114
202
53
369

Single String Multiple Strings Average Synonymy
3.35
3.76
4.04
3.66

34
58
30
122

80
144
23
247

Table 9: Synonymy for Senseval-3 lexical sample, as attested by the dictionary.

which discard dictionary suggestions for which there is no training data (e.g., entities
that do not occur as targets of an anchor link — see Section 5). The lower half of
the table focuses on the HEUR dictionary in combination with the LEX and SENSE
strategies for gathering training instances. As expected, resulting ambiguities are lower
than for full dictionaries, with more cases of zero or one candidates and fewer strings
mapping to multiple entities (also with lower ambiguities); the decrease is smaller for
the more conservative ﬁltering strategy (SENSE), which is again consistent with our
expectations.

Ambiguity in dictionaries is much higher than according to the gold standard (com-
pare to Table 3). This comes as no surprise, since the gold standard severely underesti-
mates true ambiguity by focusing exclusively on entities that are mentioned in a target
dataset. Since many target strings can, in fact, refer to dozens of possible external
entities, our estimates of ambiguity are also much higher than in WSD settings (see
Table 4).
In contrast to WSD, NED’s true ambiguity usually remains unknown, as
its determination would require a laborious inspection of all strings and entities, and
an exhaustive search for examples of usage in actual text. Although our dictionaries
have good quality and come close to covering all entities in the gold standard (see Sec-
tion 8.6), a manual inspection showed that they also contain incorrect entries. There-
fore, we suspect that actual ambiguity maybe be slightly lower than our estimate ob-
tained with the HEUR dictionary.

Table 8 shows synonymy ﬁgures for both the raw EXCT dictionary and also after
applying heuristic rules (EXCT-HEUR). Entities corresponding to NILs were not cov-
ered by the dictionary (tallied under the No Strings heading), and all of the remaining
entities were lexicalized by a large number of strings (none by just one). The EXCT
dictionary had, on average, 210 strings, which is reduced to 46 with heuristics. Al-
though high estimates reﬂect the comprehesive coverage afforded by our dictionaries,
they do not reveal true levels of synonymy, which would require hand-checking all en-

25

Figure 14: Architecture for our system adapted to TAC-KBP.

tries, as before. Nevertheless, these ﬁgures illustrate, at a high level, another important
difference between NED and WSD, where synonymy tends to be much lower (e.g.,
around 3.5, according to one popular dictionary — see Table 9). Extending the table
to include LNRM and FUZZ dictionaries would require performing more complicated
calculations, but the resulting synonymy estimates would only be higher for these omit-
ted cascades.

7 A NED System for TAC-KBP

Evaluating our system on the TAC-KBP exercise required several adaptations. Fig-
ure 14 shows the updated architecture. In it, a new module looks up top-ranked articles
in the KB. If a top-scorer has a corresponding entry in the KB, the module returns its
KB ID. Since all KB-instances came from Wikipedia, there is a direct mapping from
titles to KB IDs. To guarantee matches, we altered our dictionary construction process
slightly, making sure to include all Wikipedia titles explicitly referenced by the ofﬁcial
KB (see Figure 12). When deciding which pages are canonical (see Section 4.1), we
preferred entries listed in the KB over all others (from a superset of Wikipedia data that
included the TAC-KBP dump of October 2008).

26

Context Independent ComponentContext Dependent DisambiguationWikipedia titles0.14 Abbott_Laboratories0.07 John_Abbott0.05 Bud_Abbott0.05 Abbott,_Texas…DictionarySupervised ClassifierMentionAbbottWikipedia titles4.52 Bud_Abbott-0.10 Abbott,_Texas-0.39 John_Abbott…TAC KBP mappingContext… became Abbott’s partner after Costello…WikipediaKB IDE0064214KBContextMentionWikipedia counts only web counts only
0.6758

0.6513

EXCT 0.6937
LNRM 0.6949
FUZZ 0.7134
HEUR 0.7212
GOOG 0.6955

Table 10: Performance of dictionaries on the news subset of the development dataset
— as micro-averages — for two individual dictionaries (EXCT and GOOG), the two
dictionary cascades (LNRM and FUZZ) and our heuristic dictionary (HEUR). For
LNRM, we show also results with partitions of counts (i.e., just the popularity within
Wikipedia versus on the rest of the web).

8 Experimental Results and Performance Analyses

We now evaluate different variants of dictionaries and supervised classiﬁers on devel-
opment data. These experiments allow us to tune settings, to be used with the blind test
set (see Section 8.7).

8.1 Evaluation Setup for TAC-KBP
We follow a standard evaluation procedure, using scripts from the 2009–10 TAC-KBP
exercise. The metric is (micro-averaged) accuracy: given a reference set with N queries
— and a corresponding set of guesses, where C are correctly disambiguated (i.e., a
system’s output label equals the gold standard’s entity ID string) — the score is simply
C/N. Since our main goal is to disambiguate among entities, we will focus on eval-
uation of the dictionaries and supervised classiﬁers restricted to entities present in the
KB (i.e., ignoring gold standard examples tagged with NILs).

8.2 Performance of Top Dictionary Entries
Table 10 shows performance on the news subset of development data for the EXCT
and GOOG dictionaries (69.4 and 69.6%), the LNMR and FUZZ cascades (69.5
and 71.3%), and the heuristic combination, HEUR (72.1%).
It conﬁrms our intu-
ition (see Section 4): HEUR improves over the FUZZ cascade, offering cleaner sug-
gestions (since it yields many fewer candidates). Results from GOOG are competitive
with cascades and may be a good alternative in situations when full dictionaries are
unavailable.19 The table also shows that using counts from Wikipedia alone is worse
than relying on counts from the rest of the web — and that merging the two sets of
counts (see Section 4) works best — for the LNRM cascade; we use merged counts in
all remaining experiments. Hovering at 70%, MFS heuristics have higher accuracies
here than even in typical WSD settings, where they are known to be strong (e.g., around

19However, this may require querying the search engine just-in-time, using Google’s API, with consequent

limitations.

27

Figure 15: Precision and recall @k curves for a random subset of the news development
dataset (without NILs).

55% — see Section 2.2), which shows that choosing most popular entries makes for
powerful methods in NED as well.

8.3 Precision/Recall Curves for Dictionaries

Although performance of top candidates is indicative of a dictionary’s overall quality, it
tells us little about the less popular choices. We use dictionaries to expand queries into
pools of possible candidates, to be disambiguated by a supervised system. Therefore, it
is important to understand how close a dictionary might come to capturing all — even
low-ranking — entities that may be relevant to a gold standard. Figure 15 shows pre-
cision/recall curves that evaluate our dictionaries beyond just top-ranking entries (the
left-most points of each curve, corresponding to Table 10).20

These curves show that the FUZZ cascade generates more entities than other dic-
tionaries, with higher recall at similar levels of precision; unfortunately it draws on
far too many candidates. The LNRM and EXCT dictionaries perform similarly at all
recall levels; HEUR performs better at high precision, slightly better at high recall,
and worse in-between. The GOOG dictionary performs worse everywhere except at
the highest recall, where it dominates (at close to 99%); its generally-low performance
indicates that although GOOG manages to generate candidates for nearly all strings,
its ranking function is less-well suited to the NED task than are our methods.

20 Precision is the number of correct entities divided by the total number of entities returned; recall is the
number of correct entities divided by the total number of gold standard entities). The P/R curve is obtained
taking the top K candidates with highest probability.

28

Classiﬁer
MAXENT
multi-class SVM
one-versus-all SVMs

-
-
-
-
-

Dictionary

Filtering Back-Off
HEUR
LNRM

-
-

LNRM

-
-
-
-

-
-
-

HEUR

-
-
-

Span Match
100
LEX
-
-
-
-

-
-
-
-
-
-

SENT
PARA

-

SENSE

news
0.7707
0.7063
0.7463
0.7528
0.7827
0.7582
0.7582
0.8090

∆
-0.0644
-0.0244
-0.0179
+0.0120
-0.0125
-0.0125
+0.0283

Table 11: Performance of the supervised classiﬁer on the news subset of development
data, as micro-averages, for our default supervised classiﬁer (top row) and several vari-
ants, each of which differs from the default in exactly one parameter setting, with the
rest indicated by dashes (-).

8.4 Performance of Supervised Classiﬁers

Table 11 shows performance on the news subset of development data for several vari-
ants of our supervised classiﬁer (described in Section 5). The ﬁrst row corresponds
to default parameters; the rest represent a greedy exploration of the space of alterna-
tives. Each additional row speciﬁes a setting that differs from the ﬁrst row; dashes (-)
indicate that all other parameters are the same.
Classiﬁer: The second and third rows correspond to the accuracies of a multi-class
classiﬁer, based on SVMmulticlass [Tsochantaridis et al., 2004], and a one-versus-
all approach, using SVMlight binary classiﬁers [Joachims, 1999]. Both alternatives
perform worse than our default classiﬁcation algorithm (maximum entropy with (cid:96)2-
regularization), MAXENT [Manning and Klein, 2003]. We did not tune any of the
available parameters, since we were interested in out-of-the-box performance of all
methods (for SVMs, we used a linear kernel with a cost of 0.01).
Dictionary: The fourth row shows that the LNRM cascade generates worse training
examples than our default dictionary combination, HEUR. We do not show results for
other dictionaries, which yield too many candidates (without improving precision). The
ﬁfth row shows that HEUR also performs better than LNRM when used as a back-off
dictionary, improving over the default.
Span: Rows six and seven show that supervised classiﬁcation performs equally well
using either sentences or paragraphs (but that the best results are obtained using left
and right 100 tokens). One reason for similar performance is that most paragraphs are
not marked correctly, in Wikipedia, often comprising a single sentence. A ﬁxed span
of tokens to each side of a mention may extend beyond sentence boundaries, providing
more context to help with disambiguation.
Match: The last row shows that using all examples that refer to an entity (SENSE)
improves over the default approach, which uses only a subset of examples that contain
the target string (LEX).

29

8.5 Extending Analyses to Web Data
The best results for the news portion of the development data were obtained with the
MAXENT classiﬁer, the HEUR dictionary for both ﬁltering training examples and
backing off, 100-token spans, and the SENSE match strategy. On web data, our dic-
tionary and supervised components yield somewhat different results, most of them
congruent with conclusions based on news data.

Table 12 shows a subset of the variants with qualitatively different outcomes. For
instance, the LNRM dictionary, our default for back-off, is the better option with web
data. As for plain dictionary look-ups, HEUR is still the best overall choice, although
GOOG performs slightly better on web data. This disparity may stem from search en-
gines being developed primarily with web-pages in mind, whereas our heuristics came
about from analyzing examples of news articles in the development set; LNRM may
be the more robust back-off dictionary for similar reasons. Furthermore, our classiﬁers
fall through to a back-off dictionary only when there aren’t enough training instances,
which tends to be the case for rare entities. Since LNRM has higher recall than HEUR,
it may be generally more useful for obscure references. Overall, performance differ-
ences between dictionaries are smaller for web data, and average accuracies are sub-
stantially higher than with the news portion. Evaluated against the combined news+web
development data, ranks of dictionaries remain the same as for news alone.

All variations of the supervised system with good performance on news worked
better still for web data,21 scoring well above the dictionaries. The reduction in error
going from news to web is remarkable, with accuracies well over 90%. Manual inspec-
tion showed that mentions in web data are more heavily skewed towards most popular
entities, compared to news data, which may explain the much higher accuraces of both
supervised classiﬁers and the underlying dictionaries. In fact, web data referenced, for
the most part, extremely well-known entities (e.g., European Union), along with some
that are relatively unknown (e.g., CCC — the Cincinnati Cycle Club). Entities in news
data, on the other hand, tended to be of substantially more ambiguous nature.

The second (and ﬁnal) variant that does better on news but did not pan out with web
data is the SENSE strategy for gathering training instances. Although it has higher cov-
erage (see Section 6.4), additional matches tend to be less precise, as these examples
aren’t required to contain the target string. Since preliminary results indicate that rela-
tive performance is sensitive to the type of data, we decided to stick with the simpler,
more conservative strategy, LEX. We conclude that our default settings may already be
optimal for unseen data, and will thus evaluate this set-up against the TAC-KBP 2010
test data, to compare our system with the state-of-the-art.

8.6 Upper Bounds for Supervised Classiﬁcation with Our Dictio-

naries

An important function of the ﬁltering dictionary is to provide sets of plausible entities,
which determine construction of (distantly) supervised training data. Given an ideal

21Table 12 omits results for variations that did worse than the default on news, as they also do worse with

web data.

30

news
Dictionaries
EXCT
0.6937
LNRM 0.6949
0.7134
FUZZ
HEUR
0.7212
GOOG 0.6955

Span Match
100
LEX
-
-

SENSE

-

news
0.7707
0.7827
0.8090

web
0.8799
0.8799
0.8808
0.8845
0.8873

web
0.9376
0.9376
0.9209

Classiﬁer
MAXENT

-
-

Dictionary

Filtering Back-Off
HEUR
LNRM
HEUR

-
-

-

Table 12: Performance on both web and news subsets of development data, as micro-
averages, for dictionaries and three of the supervised classiﬁcation system variants.

LNRM dictionary
FUZZ dictionary
HEUR dictionary
SUPERVISED (LEX + HEUR)
SUPERVISED (SENSE + HEUR)

news

web

realized
0.6949
0.7134
0.7212
0.7707
0.8090

oracle
0.9158
0.9415
0.9188
0.8955
0.9140

realized
0.8799
0.8808
0.8845
0.9376
0.9209

oracle
0.9842
0.9851
0.9832
0.9814
0.9832

Table 13: Realized performance and oracle skylines — on both news and web portions
of development data — as micro-averages, for the LNRM, FUZZ and HEUR dictio-
naries, as well as for LEX and SENSE supervised classiﬁer variants (both also based
on HEUR).

classiﬁer, we would prefer to use comprehensive dictionaries that might contain the cor-
rect entity for all strings in the evaluation set, even if that meant dragging in many incor-
rect candidates too. Table 13 shows the skyline results that could be attained by an ora-
cle, choosing the best possible entity available to each system. For LNRM and HEUR
cascades, gold standard entities are among the dictionaries’ suggestions about 92% of
the time (98% for web data). Supervised classiﬁcation with the LEX strategy lowers
these bounds slightly, because in some cases there are no training examples available
for the gold entity, which precludes our classiﬁers from returning the correct result; by
expanding the pool of training instances, the SENSE strategy restores this bound to
nearly what it was for HEUR. Naturally, higher bounds aren’t necessarily superior: for
example, FUZZ yields the overall largest number of possibilities, but has lower real-
HEUR.
ized

performance

than

31

HEUR dictionary
SUPERVISED (LEX+HEUR)

news
0.6984
0.8125

web
0.8149
0.8668

full

0.7490
0.8448

Table 14: Performance on the 2010 test data, for our best dictionary and supervised
classiﬁer, broken down also for the news and web subsets.

[Dredze et al., 2010]
[Hachey et al., 2012]
[Varma et al., 2009]
[Zhang et al., 2010]
[Han and Sun, 2011]
[Guo et al., 2011]
HEUR dictionary
SUPERVISED (LEX+HEUR)
[McNamee, 2010]
[Varma et al., 2009]
[Guo et al., 2011]
[Hachey et al., 2012]
[Lehmann et al., 2010]
HEUR dictionary
SUPERVISED (LEX+HEUR)

System KB-only
0.7063
∗0.723
0.7654 Best submission to TAC 2009.
∗0.792
∗0.79
0.74
∗0.7212
∗0.7707
0.6500 Best “no context” system at TAC.
0.705
Reported by Hachey et al. [2012].
0.741
∗0.784
0.8059 Best system at TAC 2010.
0.7490
0.8448

2009
2010

Table 15: The state-of-the-art for 2009/2010 TAC-KBP test data, including perfor-
mance of our dictionary (HEUR) and classiﬁer (SUPERVISED); results from non-
blind evaluation set-ups (see Section 2.5) are starred (∗).

8.7 Final Results and Comparison to the State-of-the-Art

Following the development phase, we tested our best dictionary and supervised classi-
ﬁer on the hidden (2010 test) dataset. Table 14 shows both results, also broken down
by data type. This ﬁnal evaluation conﬁrmed that the heuristic dictionary already per-
forms quite well (at 75%) overall, and that trained classiﬁers can tap into further im-
provements (scoring close to 85%). The improvement is larger on news, which is again
more challenging that the web dataset. The results are similar to those of development,
except for web, where results have dropped around 5 points.

Table 15 shows performance of recent NED systems that report results for either of
the 2009/2010 TAC-KBP test sets (see Section 2 for their descriptions), including our
dictionary and classiﬁer.

The dictionary performs well on the KB-only subset of the 2009 test set, beating
some of the more complex systems. Since we used the 2009 data to choose bests of
several system variants, evaluation on this test set is not blind and may overstate our
results, as well as those of many other published systems that did the same [Zhang

32

et al., 2010, Han and Sun, 2011, Hachey et al., 2012]; our supervised classiﬁer scores
77% on the KB-only subset, behind two systems [Zhang et al., 2010, Han and Sun,
2011] scoring 79%, both of which were also developed using 2009 test data.

For the 2010 test data, our evaluation was completely blind. Here the dictionary
performs better than the next best “no context” system [McNamee, 2010] by a large
margin: 75% vs. 65%. The supervised classiﬁer also scores highest — 85% vs. 81%
for the next best system [Lehmann et al., 2010]. The entry for Hachey et al. [2012]
represents the best of several systems chosen on the test set; their best variant according
to development data was, in fact, the system of Varma et al. [2009], which scores 7%
lower22

9 Discussion

We have shown that it is possible to construct an effective dictionary for NED, covering
between 92% and 98% of the manually annotated string-entity pairs (for development
data). Although the ambiguity in such dictionaries varies, it tends to be higher than
for the gold standard in NED and also for a typical WSD dataset. Since a similar
phenomenon is also observed with synonymy, one might expect NED to pose a more
difﬁcult problem than WSD; nevertheless, we observed the opposite effect in practice.
Our popularity-based dictionary heuristic performs even more strongly than the MFS
baseline in WSD (75% in our blind NED evaluation, compared to 55%); supervised
system variants also score much higher (84% vs. 73%). Of course, comparing eval-
uation numbers across tasks requires extreme caution. Nonetheless, we suspect that
qualitatively large differences in performance here indicate that NED has larger num-
bers of training data, compared to WSD.

High ambiguity and synonymy, together with the large volumes of text data, make
NED computationally more demanding than WSD, both in the scope of regular mem-
ory and disk storage capacities, as well as speed and efﬁcient processing requirements.
Our approach in particular could invoke training of potentially millions of classiﬁers,
requiring signiﬁcant engineering effort. But since each classiﬁer can be trained inde-
pendently, parallelization is simple and easy.

We found that more comprehensive dictionaries that provide better coverage aren’t
always optimal when it comes to training supervised classiﬁers, since it is also im-
portant to have enough examples for each candidate entity being suggested. Instead,
a dictionary that contains the most precise and common choices may work better, as
demonstrated by our cascades of dictionaries.

All in all, our systems fare well, compared to the state-of-the-art. The dictionary
beats all systems not using context in the 2010 TAC-KBP task by a large margin. And
the supervised system, despite its simplicity, outperforms other systems as well. More
detailed comparisons are difﬁcult, because many recent papers lack ablative analyses:
even when performance of individual components is reported, interactions with NILs

22Note that, since Hachey et al. [2012] reimplemented three well-known systems [Bunescu and Pasca,

2006, Cucerzan, 2007, Varma et al., 2009], our results also compare favorably to the other two.

33

make proper comparisons challenging.23,24

As mentioned previously, our dictionaries could be expanded by drawing on addi-
tional sources of data. This fact should not be overlooked, since Hachey et al. [2012]
found that candidate generation accounted for most of the performance variation in the
systems that they reimplemented; in particular, acronym handling, using coreference
resolution to ﬁnd longer mentions, led to substantial improvements. We could aug-
ment our dictionaries via various techniques for mining acronyms [Varma et al., 2009,
McNamee, 2010, Lehmann et al., 2010], metaphones [Varma et al., 2009], as well as
bolded words in ﬁrst paragraphs and hatnote templates [Hachey et al., 2012]. Mov-
ing beyond the static dictionary model, it is also possible to exploit dynamic meth-
ods for proposing candidates. Online techniques can make use of partial matches
between tokens in query strings and entities [Varma et al., 2009, McNamee, 2010,
Lehmann et al., 2010], ﬁnite-state transducers [Dredze et al., 2010], matches with
longer mentions [Lehmann et al., 2010], automatic spelling correction [Zheng et al.,
2010], Wikipedia search engines and the “Did you mean...” functionality [Zhang et al.,
2010]. Storing as much information as possible in static dictionaries will make it eas-
ier to debug, replicate and share resources. But some dynamic lookups could also be
batched to enrich a dictionary just-in-time. A thorough study of the overlap and con-
tribution of various enhanced candidate generation methods may make for a fruitful
research direction.

With respect to disambiguation, our approach closely followed that of typical su-
pervised WSD systems, which train a classiﬁer for each target string. We used anchor
texts in Wikipedia articles to train logistic regressions, showing that this method is also
appropriate and computationally feasible for NED. Given their popularity in WSD re-
search, we expected straight-forward classiﬁcation techniques to be more prevalent in
NED, at least as baselines. Methods such as ours may also prove to be useful in com-
bination with other disambiguation systems which tap on different knowledge sources,
e.g. those using the hyperlink structure [Milne and Witten, 2008, Moro et al., 2014].

In summary, our NED system can be easily replicated because it uses an ofﬂine
dictionary. Its simplicity allows for a clearer understanding and ablative analyses, com-
pared with systems that rely on dynamic candidate generation methods. As a result, it
may make a good platform for testing and incorporating various modular extensions to
NED, such as NIL classiﬁers, candidate generators, similarity-based techniques, global
coherence or coreference resolution. We designed the system to work with the entire
space of Wikipedia articles and strings: no thresholds or other kinds of parameters
were ﬁne-tuned to the test data. Given the limited number, scope and complexity of
decisions made even in development, we expect our system to be robust. Naturally,

23E.g., Han and Sun [2011] report end-to-end performance of their popularity and name model (roughly
equivalent to our dictionary), combined with a NIL detection system, excluding KB- or NIL-only results.
Their score on the 2009 test set is far below ours (50% vs. 72%), but it is not possible to identify the main
reason behind this discrepancy.

24An exception, Hachey et al. [2012] provide precision and recall of their candidate generation component
for KB queries (56.3 and 87.8, respectively) on the 2009 test data (the news subset of our development set),
which can be compared to oracle and realized performance of HEUR (91.88 and 72.12); they also report
an ambiguity of 7.2, obtained by dividing the number of candidate entities by the total number of query
strings (our corresponding ﬁgure for HEUR would be 9.08, if calculated in the same fashion, i.e., differently
from the numbers listed in Table 3).

34

performance could be further optimized to ﬁt a target corpus or genre, if desired.

10 Conclusions and Future Work

We presented a system for NED based on word-experts, which is a well-understood
technique from the WSD literature. Our system comprises two components: (1) a
context-independent module, based on frequencies of entities, which returns most pop-
ular candidates; and (2) a context-sensitive classiﬁer for each named-entity string that
selects entities that are best suited to a mention’s surrounding text. We show that such a
word-expert provides results which are competitive to the state-of-the-art, as measured
on the 2009 and 2010 TAC datasets.

In the future we would like to extend our work to other datasets, which will pro-
vide further points of comparison to the state of the art. We would also like to include
our classiﬁers in more complex NED systems, where complementary information like
link structure [Moro et al., 2014], similarities between article texts and mention con-
texts [Hoffart et al., 2012], as well as global optimization techniques [Ratinov et al.,
2011] could further improve the results.

We highlighted many connections between WSD and NED. In WSD settings, dic-
tionaries are provided, but NED involves constructing possible mappings from strings
to entities — a step that Hachey et al. [2012] showed to be key to success, which we
also conﬁrmed experimentally. The resulting dictionaries exhibit very high synonymy
and ambiguity (polysemy) yet still do not cover many occurrences that ought to be
tagged by a NED system, making the task appear more difﬁcult, in theory, compared
to WSD. But in practice, the opposite seems to be the case, due to actual mentions
being more heavily skewed towards popular entities than in WSD, a plethora of avail-
able training data in the form of human-entered anchor-texts of hyperlinks on the web,
and higher inter-annotator agreement, which indicates more crisp differences between
possible shades of meanings than in WSD. As a result, both popularity-based dictio-
nary lookups (MFS heuristics) and supervised classiﬁers, which are traditional WSD
architectures, perform better for NED than for WSD. In the future, we would like to
extend our study using datasets which include all mentions in full documents [Hoffart
et al., 2011].

Acknowledgements

We thank Oier Lopez de Lacalle and David Martinez, for the script to extract features,
as well as Daniel Jurafsky and Eric Yeh, for their contributions to our earliest partici-
pation in TAC-KBP.

Parts of this work were carried out while Eneko Agirre was visiting Stanford Uni-
versity, with a grant from the Ministry of Science; Angel X. Chang has been supported
by a SAP Stanford Graduate Fellowship; Valentin I. Spitkovsky has been partially sup-
ported by NSF grants IIS-0811974 and IIS-1216875 and by the Fannie & John Hertz
Foundation Fellowship. We gratefully acknowledge the support of Defense Advanced
Research Projects Agency (DARPA) Machine Reading Program under Air Force Re-

35

search Laboratory (AFRL) prime contract no. FA8750-09-C-0181. Any opinions, ﬁnd-
ings, and conclusion or recommendations expressed in this material are those of the
author(s) and do not necessarily reﬂect the view of the DARPA, AFRL, or the US
government.

References

Eneko Agirre and Philip Edmonds, editors. Word Sense Disambiguation: Algorithms
and Applications, volume 33 of Text, Speech and Language Technology. Springer,
2006.

Eneko Agirre and Oier Lopez de Lacalle. UBC-ALM: Combining k-NN with SVD for

WSD. In SemEval, 2007.

Eneko Agirre, Angel X. Chang, Dan S. Jurafsky, Christopher D. Manning, Valentin I.
Spitkovsky, and Eric Yeh. Stanford-UBC at TAC-KBP. In Proceedings of the Text
Analysis Conference, 2009.

Javier Artiles, Satoshi Sekine, and Julio Gonzalo. Web people search: results of the

ﬁrst evaluation and the plan for the second. In WWW, 2008.

Javier Artiles, Julio Gonzalo, and Satoshi Sekine. WePS 2 evaluation campaign:

overview of the web people search clustering task. In WePS, 2009.

Amit Bagga and Breck Baldwin. Entity-based cross-document coreferencing using the

vector space model. In COLING-ACL, 1998.

Indrajit Bhattacharya and Lise Getoor. Collective entity resolution in relational data.

ACM TKDD, 1, 2007.

Razvan C. Bunescu and Marius Pasca. Using encyclopedic knowledge for named entity

disambiguation. In EACL, 2006.

Angel X. Chang, Valentin I. Spitkovsky, Christopher D. Manning, and Eneko Agirre.
Stanford-UBC entity linking at TAC-KBP. In Proceedings of the Text Analysis Con-
ference, 2010.

Angel X. Chang, Valentin I. Spitkovsky, Christopher D. Manning, and Eneko Agirre.
Stanford-UBC entity linking at TAC-KBP, again. In Proceedings of the Text Analysis
Conference, 2011.

Zheng Chen and Heng Ji. Collaborative ranking: A case study on entity linking. In

EMNLP, 2011.

Andras Csomai and Rada Mihalcea. Linking documents to encyclopedic knowledge.

IEEE Intelligent Systems, 23, 2008.

Silviu Cucerzan. Large-scale named entity disambiguation based on Wikipedia data.

In EMNLP-CoNLL, 2007.

36

Mark Dredze, Paul McNamee, Delip Rao, Adam Gerber, and Tim Finin. Entity disam-

biguation for knowledge base population. In COLING, 2010.

L. Dunn Halbert. Record linkage. AJPH, 36, 1946.

Greg Durrett and Dan Klein. A joint model for entity analysis: Coreference, typing,
and linking. In Proceedings of the Transactions of the Association for Computational
Linguistics, 2014.

Tamer Elsayed, Doug Oard, and Galileo Mark Namata. Resolving personal names in

email using context expansion. In ACL, 2008.

Anthony Fader, Stephen Soderland, and Oren Etzioni. Scaling Wikipedia-based named

entity disambiguation to arbitrary web text. In WikiAI, 2009.

Christiane Fellbaum, editor. WordNet: An Electronic Database. MIT Press, 1998.

Chung Heong Gooi and James Allan. Cross-document coreference on a large scale
corpus. In Daniel Marcu Susan Dumais and Salim Roukos, editors, HLT-NAACL,
2004.

Swapna Gottipati and Jing Jiang. Linking entities to a knowledge base with query

expansion. In EMNLP, 2011.

Y. Guo, W. Che, T. Liu, and S. Li. A graph-based method for entity linking. In Pro-
ceedings of 5th International Joint Conference on Natural Language Processing,
page 10101018, Chiang Mai, Thailand, November 2011. Asian Federation of Nat-
ural Language Processing. URL http://www.aclweb.org/anthology/
I11-1113.

B. Hachey, W. Radford, J. Nothman, M. Honnibal, and J.R. Curran. Evaluating Entity
Linking with Wikipedia. Artiﬁcial Intelligence, 194:130–150, January 2012. ISSN
0004-3702. doi: 10.1016/j.artint.2012.04.005. URL http://dx.doi.org/10.
1016/j.artint.2012.04.005.

Xianpei Han and Le Sun. A generative entity-mention model for linking entities with

knowledge base. In ACL HLT, 2011.

Xianpei Han and Le Sun. An entity-topic model for entity linking. In EMNLP-CoNLL,

2012.

Johannes Hoffart, Mohamed Amir Yosef, Ilaria Bordino, Hagen F¨urstenau, Manfred
Pinkal, Marc Spaniol, Bilyana Taneva, Stefan Thater, and Gerhard Weikum. Robust
disambiguation of named entities in text. In EMNLP, 2011.

Johannes Hoffart, Stephan Seufert, Dat Ba Nguyen, Martin Theobald, and Gerhard
Weikum. Kore: Keyphrase overlap relatedness for entity disambiguation. In Pro-
ceedings of the 21st ACM international conference on Information and knowledge
management, page 545554, 2012. URL http://dl.acm.org/citation.
cfm?id=2396832.

37

Heng Ji and Ralph Grishman. Knowledge base population: Successful approaches and

challenges. In ACL HLT, 2011.

Heng Ji, Ralph Grishman, Hoa Trang Dang, Kira Grifﬁtt, and Joe Ellis. Overview of

the TAC 2010 Knowledge Base Population track. In TAC, 2010.

Thorsten Joachims. Making large-scale SVM learning practical.

In Bernhard
Sch¨olkopf, Christopher J.C. Burges, and A. Smola, editors, Advances in Kernel
Methods - Support Vector Learning. MIT Press, 1999.

Sayali Kulkarni, Amit Singh, Ganesh Ramakrishnan, and Soumen Chakrabarti. Col-

lective annotation of Wikipedia entities in web text. In KDD, 2009.

John Lehmann, Sean Monahan, Luke Nezda, Arnold Jung, and Ying Shi. LCC ap-

proaches to knowledge base population at TAC 2010. In TAC, 2010.

Gideon S. Mann and David Yarowsky. Unsupervised personal name disambiguation.

In CoNLL, 2003.

Christopher D. Manning and Dan Klein. Optimization, maxent models, and conditional

estimation without magic. In HLT-NAACL, 2003.

Lluis Marquez, Gerard Escudero, David Martinez, and German Rigau. Supervised
corpus-based methods for WSD. In E. Agirre and P. Edmonds, editors, Word Sense
Disambiguation: Algorithms and Applications, volume 33 of Text, Speech and Lan-
guage Technology. Springer, 2006.

Elaine Marsh and Dennis Perzanowski. MUC-7 evaluation of IE technology: Overview

of results. In MUC, 1998.

Paul McNamee. HLTCOE efforts in entity linking at TAC KBP 2010. In TAC, 2010.

Paul McNamee and Hoa Dang. Overview of the TAC 2009 Knowledge Base Population

track. In TAC, 2009.

Rada Mihalcea and Andras Csomai. Wikify!:

knowledge. In CIKM, 2007.

linking documents to encyclopedic

Rada Mihalcea, Timothy Chklovski, and Adam Kilgarriff. The Senseval-3 English

lexical sample task. In Senseval, 2004.

David Milne and Ian H. Witten. Learning to link with Wikipedia. In CIKM, 2008.

Andrea Moro, Alessandro Raganato, and Roberto Navigli. Entity linking meets word
sense disambiguation: a unied approach. Transactions of the Association of Compu-
tational Linguistics, 2:231–244, May 2014.

Roberto Navigli. Word sense disambiguation: A survey. ACM Computing Surveys, 41,

2009.

Danuta Ploch. Exploring entity relations for named entity disambiguation.

HLT: Student Session, 2011.

In ACL

38

Sameer S. Pradhan, Edward Loper, Dmitriy Dligach, and Martha Palmer. SemEval-

2007 Task-17: English lexical sample SRL and all words. In SemEval, 2007.

Lev-Arie Ratinov, Dan Roth, Doug Downey, and Mike Anderson. Local and global

algorithms for disambiguation to Wikipedia. In ACL, 2011.

Benjamin Snyder and Martha Palmer. The English all-words task. In Senseval, 2004.

Valentin I. Spitkovsky and Angel X. Chang. A cross-lingual dictionary for English

Wikipedia concepts. In LREC, 2012.

Erik F. Tjong Kim Sang and Fien De Meulder. Introduction to the CoNLL-2003 shared

task: language-independent named entity recognition. In CoNLL, 2003.

Ioannis Tsochantaridis, Thomas Hofmann, Thorsten Joachims, and Yasemin Altun.
Support vector machine learning for interdependent and structured output spaces. In
ICML, 2004.

Vasudeva Varma, Vijay Bharath Reddy, Sudheer Kovelamudi, Praveen Bysani, San-
thosh Gsk, Kiran Kumar, Kranthi Reddy, Karuna Kumar, and Nithin Maganti. IIIT
Hyderabad at TAC 2009. In TAC, 2009.

Gerhard Weikum and Martin Theobald. From information to knowledge: Harvesting

entities and relationships from web sources. In PODS, 2010.

Wei Zhang, Jian Su, Chew Lim Tan, and Wen Ting Wang. Entity linking leveraging

automatically generated annotation. In COLING, 2010.

Zhicheng Zheng, Fangtao Li, Minlie Huang, and Xiaoyan Zhu. Learning to link entities

with knowledge base. In NAACL HLT, 2010.

Zhi Zhong and Hwee Tou Ng. It makes sense: A wide-coverage word sense disam-

biguation system for free text. In ACL: System Demonstrations, 2010.

Yiping Zhou, Lan Nie, Omid Rouhani-Kalleh, Flavian Vasile, and Scott Gaffney. Re-

solving surface forms to Wikipedia topics. In COLING, 2010.

39

