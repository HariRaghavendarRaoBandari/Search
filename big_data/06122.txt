6
1
0
2

 
r
a

 

M
9
1

 
 
]
T
N
h
t
a
m

.

[
 
 

1
v
2
2
1
6
0

.

3
0
6
1
:
v
i
X
r
a

INTEGER COMPLEXITY: REPRESENTING NUMBERS OF

BOUNDED DEFECT

HARRY ALTMAN

Abstract. Deﬁne knk to be the complexity of n, the smallest number of ones
needed to write n using an arbitrary combination of addition and multiplica-
tion. John Selfridge showed that knk ≥ 3 log3 n for all n. Based on this, this
author and Zelinsky deﬁned [4] the “defect” of n, δ(n) := knk − 3 log3 n, and
this author showed that the set of all defects is a well-ordered subset of the real
numbers [1]. This was accomplished by showing that for a ﬁxed real number r,
there is a ﬁnite set S of polynomials called “low-defect polynomials” such that
for any n with δ(n) < r, n has the form f (3k1 , . . . , 3kr )3k
r+1 for some f ∈ S.
However, using the polynomials produced by this method, many extraneous
n with δ(n) ≥ r would also be represented.
In this paper we show how to
remedy this and modify S so as to represent precisely the n with δ(n) < r and
remove anything extraneous. Since the same polynomial can represent both n
with δ(n) < r and n with δ(n) ≥ r, this is not a matter of simply excising the
appropriate polynomials, but requires “truncating” the polynomials to form
new ones.

1. Introduction

The complexity of a natural number n is the least number of 1’s needed to
write it using any combination of addition and multiplication, with the order of the
operations speciﬁed using parentheses grouped in any legal nesting. For instance,
n = 11 has a complexity of 8, since it can be written using 8 ones as

11 = (1 + 1 + 1)(1 + 1 + 1) + 1 + 1,

but not with any fewer than 8. This notion was implicitly introduced in 1953 by
Kurt Mahler and Jan Popken [13]; they actually considered an inverse function,
the size of the largest number representable using k copies of the number 1. (More
generally, they considered the same question for representations using k copies of a
positive real number x.) Integer complexity was explicitly studied by John Selfridge,
and was later popularized by Richard Guy [9, 10]. Following J. Arias de Reyna [5]
we will denote the complexity of n by knk.

Integer complexity is approximately logarithmic; it satisﬁes the bounds

3 log3 n =

3

log 3

log n ≤ knk ≤

3

log 2

log n,

n > 1.

The lower bound can be deduced from the result of Mahler and Popken, and was
explicitly proved by John Selfridge [9]. It is attained with equality for n = 3k for
all k ≥ 1. The upper bound can be obtained by writing n in binary and ﬁnding a
3
log 2 can
representation using Horner’s algorithm. It is not sharp, and the constant
be improved for large n [17].

Date: March 6, 2016.

1

2

HARRY ALTMAN

Based on the above, this author and Zelinsky deﬁned the defect of n:

Deﬁnition 1.1. The defect of n, denoted δ(n) is deﬁned by

δ(n) := knk − 3 log3 n.

The defect has proven to be a useful tool in the study of integer complexity. For
instance, one outstanding question regarding integer complexity, raised by Guy [9],
is that of the complexity of 3-smooth numbers; is k2n3kk always equal to 2n + 3k,
whenever n and k are not both zero? This author and Zelinsky used the study of
the defect to show in [4] that this holds true whenever n ≤ 21.

This was accomplished by means of a method for, given a real number s, deter-
mining restrictions on what natural numbers n could have δ(n) < s. They deﬁned:

Deﬁnition 1.2. For a real number s ≥ 0, the set As is the set of all natural
numbers with defect less than s.

The method worked by ﬁrst choosing a “step size” α ∈ (0, 1), and then recursively
building up coverings for the sets Aα, A2α, A3α, . . .; obviously, any As can be reached
this way. Using this method, one can show:

Theorem 1.3 (Covering theorem). For any real s ≥ 0, there exists a ﬁnite set Ss
of multilinear polynomials such that for any natural number n satisfying δ(n) < s,
there is some f ∈ Ss and some nonnegative integers k1, . . . , kr+1 such that n =
f (3k1, . . . , 3kr )3kr+1. In other words, given s one can ﬁnd Ss such that

{n : δ(n) < s} ⊆ [

f ∈Ss

{f (3k1, . . . , 3kr )3kr+1 : k1, . . . , kr+1 ≥ 0}.

It actually proved more: in particular, the polynomials in Theorem 1.3 are not
arbitrary multilinear polynomials, but are of a speciﬁc form, for which [1] introduced
the term low-defect polynomials. In particular, low-defect polynomials are in fact
read-once polynomials, as considered in [16] for instance. See Sections 2 and 3 for
more on these polynomials.

This sort of theorem is more powerful than it may appear; for instance, one can

use it to show that the defect has unusual order-theoretic properties [1]:

Theorem 1.4. (Defect well-ordering theorem) The set {δ(n) : n ∈ N}, considered
as a subset of the real numbers, is well-ordered and has order type ωω.

But while this theorem gave a way of representing a covering of As, this covering
could include extraneous numbers not actually in As. In this paper we remedy this
deﬁciency, and show that the sets As themselves can be described by low-defect
polynomials, rather than low-defect polynomials merely describing a covering for
each As.

In order to establish this result, we introduce a way of “truncating” a low-defect
polynomial f to a given defect s, though this replaces one polynomial f by a ﬁnite
set of low-defect polynomials {g1, . . . , gk}. If we truncate every polynomial in the
set Ss to the defect s, we obtain a set Ts of low-defect polynomials so that for any
natural number n, δ(n) < s if and only if n = f (3k1, . . . , 3kr )3kr+1 for some f ∈ Ts
and some k1, . . . , kr+1. So as stated above we are no longer merely covering the set
Ar, but representing it exactly. Our main result is as follows.

Theorem 1.5 (Representation theorem). For any real s ≥ 0, there exists a ﬁnite
set Ts of multilinear polynomials such that a natural number n satisﬁes δ(n) < s if

INTEGER COMPLEXITY: REPRESENTING NUMBERS OF BOUNDED DEFECT

3

and only if there is some f ∈ Ts and some nonnegative integers k1, . . . , kr+1 such
that n = f (3k1, . . . , 3kr )3kr+1 . In other words, given s one can ﬁnd Ts such that

{n : δ(n) < s} = [

f ∈Ts

{f (3k1, . . . , 3kr )3kr+1 : k1, . . . , kr+1 ≥ 0}.

This theorem is a special case of a stronger result; see Theorem 4.9.
Note that it is possible that, for a given s, there will be more than one set Ts
In particular, it’s not clear if the Ts

satisfying the conclusions of Theorem 1.5.
generated by the methods of this paper will be minimal in size. We ask:

Question 1.6. For a given s, what is the smallest size g(s) of a set Ts as above?

We can also ask what can be said about the function g(s) as s varies. It is not
monotonic in s; for instance, let us consider what happens as s approaches 1 from
below. We use the classiﬁcation of numbers of defect less than 1 from [4]. For any
s < 1, there’s a ﬁnite set of numbers m such that any n with δ(n) < 1 can be written
as m = n3k for some k. Or in other words, Ts necessarily consists of a ﬁnite set of
constants. As s approaches 1 from below, the required number of these constants
approaches inﬁnity, i.e., lims→1− = ∞. However, g(1) is certainly ﬁnite, since all
but ﬁnitely many of the constants in the Ss for s < 1 can be grouped together into
a single inﬁnite family, 3-represented by the single low-defect polynomial 3x + 1.
The g(s) was only required to balloon to inﬁnity as s approached 1 from below as
for s < 1, Ss could not contain this short summary, needing to list each possibility
separately.

This lack of monotonicity poses an obstacle for attempts to answer this question
simply. However, one could still possibly obtain a simpler (and potentially mono-
tonic) answer if one were to restrict the domain of s; for instance, if one required s
to be integral.

1.1. Truncation procedure: An example. The main new idea of this paper
is the truncation procedure. We illustrate the truncation procedure by example,
demonstrating it. For the more general version, see Section 4.

If we want to describe the set As, we can ﬁrst use Theorem 1.3 to obtain a
description of a covering set S for As. This is the “building-up” step. Then we
apply the truncation procedure to each element of S; this is the“ﬁltering-down”
step. As an example, we will consider truncating the polynomial

f (x1, x2) = (2x1 + 1)x2 + 1

to the defect value s = 1.92.

Observe that for any k1, k2, and k3, one has

kf (3k1, 3k2)3k3 k ≤ 4 + 3k1 + 3k2 + 3k3,

as illustrated by Figure 1.1. Let us take this as the “supposed complexity” of this
number. Then the “supposed defect”, obtained by subtracting 3 log3(f (3k1, 3k2)3k3 )
from the “supposed complexity”, is equal to

4 − 3 log3(2 + 3−k1 + 3−k1−k2 ).

Now, in reality the actual defect may be less than the “supposed defect”; but we
will ignore this for now and just work with the “supposed defect”, which we know
how to compute. As it will turn out, using the “supposed defect” will still yield the

4

HARRY ALTMAN

correct result, and we do not need to determine the actual defect; see remark (2)
below.

Figure 1. A tree for building the number ((2 · 3k1 + 1)3k2 + 1)3k3;
note that 3ki has complexity 3ki.

×
❜
✧

❜❜
3k3

✧✧
+
✧
❜

✧✧
×
✧
❜

❜❜
1

❜
3k2

✧
+
◗
✑

✑
×

◗
1

❝❝★★
3k1
+
❙❙✓✓
1
1

So if we were to truncate f to a defect of 3, there would be nothing to do; the
defect of any numbers coming from f would already be less than 3, since they would
be at most 4 − 3 log3 2 ≈ 2.1. But here we are truncating to 1.92, and f can indeed
yield defects greater than 1.92 (for instance, choose k1 = k2 = 2, yielding a defect
of approximately 1.94), so it will not be as simple as that.

In the above expression for the “supposed defect”, k1 has a larger eﬀect on the
numerical value than k2 does. We will determine possible values for k1.
It can
easily be checked that for k1 ∈ {0, 1}, all the defects produced this way fall below
1.92; for k1 = 2, some are above and some are below; and for k1 ≥ 3, all of them
are above 1.92. So we will exclude k1 ≥ 3 from consideration, and mark down
the possible values of k1 as being 0, 1, and 2; then we will substitute these in to f
(noting that xi = 3ki) to yield new polynomials: 3x2 + 1, 7x2 + 1, and 19x2 + 1.
(We ignore k3 since it has no eﬀect on the “supposed defect”.)

We now apply the procedure recursively, truncating each of our new polynomials
to 1.92. As noted above, 3x2 + 1 and 7x2 + 1 only produce defects less than 1.92, so
we do not need to do anything further to truncate them. For 19x2 +1, or 19·3k2 +1,
we observe that k2 = 0 and k2 = 1 yield a supposed defect below 1.92, while k2 ≥ 2
yields supposed defects above 1.92. So once again, we substitute in k = 0 and k = 1
to 19·3k2 +1 to yield the constant polynomials 20 and 58, along with our polynomials
3x2 + 1 and 7x2 + 1 from earlier. At this point we have reached polynomials of
degree 0, so our ﬁnal set of polynomials is {3x2 + 1, 7x2 + 1, 20, 58}. If instead the
polynomials had more variables, we would have to continue this recursion further.
(1) The general version is more complicated, because in general
there is not necessarily a linear order on which variables have the most eﬀect on
the defect. Nonetheless, the general idea of ﬁxing values for variables, progressing
from variables of largest eﬀect to variables of smallest eﬀect, is retained.

Remarks.

(2) Truncating a polynomial f to a defect r replaces it with a ﬁnite set of poly-
nomials that only produce numbers of defect less than r. This leaves the question

INTEGER COMPLEXITY: REPRESENTING NUMBERS OF BOUNDED DEFECT

5

of why applying this procedure to an appropriate set of polynomials should yield
all numbers with defect less than the chosen cutoﬀ, rather than only some of them.
This is where a stronger version of Theorem 1.3 – Theorem 2.19 below, taken from
[1] – is needed. This theorem ensures that, for any given r, not only is there a
ﬁnite set Sr of polynomials which represent all numbers with defect less than r by
substituting in powers of 3; but in fact that it represents all such numbers “with
the correct complexity”. Hence, if we truncate all of them to a defect of r, those
with defect at least r will be ﬁltered out simply by the nature of the procedure;
and those with defect less than r will be kept, since each will be represented “with
the correct complexity” by some polynomial, and thus kept when that particular
polynomial is truncated. See Sections 2 and 4 for more detail.

1.2. Comparison to addition chains. It is worth discussing here some work
analogous to this paper in the study of addition chains. An addition chain for
n is deﬁned to be a sequence (a0, a1, . . . , ar) such that a0 = 1, ar = n, and, for
any 1 ≤ k ≤ r, there exist 0 ≤ i, j < k such that ak = ai + aj; the number
r is called the length of the addition chain. The shortest length among addition
chains for n, called the addition chain length of n, is denoted ℓ(n). Addition chains
were introduced in 1894 by H. Dellac [7] and reintroduced in 1937 by A. Scholz
[14]; extensive surveys on the topic can be found in Knuth [12, Section 4.6.3] and
Subbarao [15].

The notion of addition chain length has obvious similarities to that of integer
complexity; each is a measure of the resources required to build up the number n
starting from 1. Both allow the use of addition, but integer complexity supplements
this by allowing the use of multiplication, while addition chain length supplements
this by allowing the reuse of any number at no additional cost once it has been con-
structed. Furthermore, both measures are approximately logarithmic; the function
ℓ(n) satisﬁes

log2 n ≤ ℓ(n) ≤ 2 log2 n.

A diﬀerence worth noting is that ℓ(n) is actually known to be asymptotic to
log2 n, as was proved by Brauer[6], but the function knk is not known to be asymp-
knk
totic to 3 log3 n; the value of the quantity lim supn→∞
log n remains unknown. As
mentioned above, Guy [9] has asked whether k2kk = 2k for k ≥ 1; if true, it would
2
make this quantity at least
log 2 . J. Iraids et. al. [11] have checked that this is true
for k ≤ 39.

Another diﬀerence worth noting is that unlike integer complexity, there is no
known way to compute addition chain length via dynamic programming. Specif-
ically, to compute integer complexity this way, one may use the fact that for any
n > 1,

knk =

min

a,b<n∈N

a+b=n or ab=n

kak + kbk.

By contrast, addition chain length seems to be harder to compute. Suppose
we have a shortest addition chain (a0, . . . , ar−1, ar) for n; one might hope that
(a0, . . . , ar−1) is a shortest addition chain for ar−1, but this need not be the case.
An example is provided by the addition chain (1, 2, 3, 4, 7); this is a shortest addition
chain for 7, but (1, 2, 3, 4) is not a shortest addition chain for 4, as (1, 2, 4) is shorter.
Moreover, there is no way to assign to each natural number n a shortest addition

6

HARRY ALTMAN

chain (a0, . . . , ar) for n such that (a0, . . . , ar−1) is the addition chain assigned to
ar−1 [12]. This can be an obstacle both to computing addition chain length and
proving statements about addition chains.

Nevertheless, there are important similarities between integer complexity and
addition chains. As mentioned above, the set of all integer complexity defects is a
well-ordered subset of the real numbers, with order type ωω. We might also deﬁne
the notion of addition chain defect, deﬁned by

δℓ(n) := ℓ(n) − log2 n;

for as shown [2] by this author, Theorem 1.4 has an analogue for addition chains:

Theorem 1.7 (Addition chain well-ordering theorem). The set {δℓ(n) : n ∈ N},
considered as a subset of the real numbers, is well-ordered and has order type ωω.

Theorem 1.5 seems to have a partial analogue for addition chains in the work
of A. Flammenkamp [8]. As mentioned above, this author introduced the addition
chain defect δℓ(n), but a closely related quantity, the number of small steps of n,
was introduced by Knuth [12]. The number of small steps of n is deﬁned by

s(n) := ℓ(n) − ⌊log2 n⌋;

clearly, this is related to δℓ(n) by s(n) = ⌈δℓ(n)⌉.

In 1991, A. Flammenkamp [8] determined a method for producing descriptions
of all numbers n with s(n) ≤ k for a given integer k, and produced such descriptions
for k ≤ 3. Note that for k an integer, s(n) ≤ k if and only if δℓ(n) ≤ k, so this is
the same as determining all n with δℓ(n) ≤ k, restricted to the case where k is an
integer. Part of what Flammenkamp proved may be summarized as the following:

Theorem 1.8 (Flammenkamp). For any integer k ≥ 0, there exists a ﬁnite set
Sk of polynomials (in any number of variables, with nonnegative integer coeﬃ-
cients) such that for any n, one has s(n) ≤ k if and only if one can write n =
f (2m1, . . . , 2mr )2mr+1 for some f ∈ Sk and some integers m1, . . . , mr+1 ≥ 0.

Note that this only allows integer k, as opposed to Theorem 1.5, which allows
arbitrary real s. Also, the polynomials used in Flammenkamp’s method are more
complicated than those produced by Theorem 1.5; for instance, they cannot always
be taken to be multilinear.

2. The defect, stability, and low-defect polynomials

In this section we review the results of [4] and [1] regarding the defect δ(n), the
stable complexity knkst and stable defect δst(n) described below, and low-defect
polynomials.

2.1. The defect and stability. First, some basic facts about the defect:

Theorem 2.1. We have:

(1) For all n, δ(n) ≥ 0.
(2) For k ≥ 0, δ(3kn) ≤ δ(n), with equality if and only if k3knk = 3k + knk.

The diﬀerence δ(n) − δ(3kn) is a nonnegative integer.

(3) A number n is stable if and only if for any k ≥ 0, δ(3kn) = δ(n).
(4) If the diﬀerence δ(n) − δ(m) is rational, then n = m3k for some integer k

(and so δ(n) − δ(m) ∈ Z).

(5) Given any n, there exists k such that 3kn is stable.

INTEGER COMPLEXITY: REPRESENTING NUMBERS OF BOUNDED DEFECT

7

(6) For a given defect α, the set {m : δ(m) = α} has either the form {n3k :
0 ≤ k ≤ L} for some n and L, or the form {n3k : 0 ≤ k} for some n. This
latter occurs if and only if α is the smallest defect among δ(3kn) for k ∈ Z.

(7) If δ(n) = δ(m), then knk = kmk (mod 3).
(8) δ(1) = 1, and for k ≥ 1, δ(3k) = 0. No other integers occur as δ(n) for any

n.

(9) If δ(n) = δ(m) and n is stable, then so is m.

Proof. Parts (1) through (8), excepting part (3), are just Theorem 2.1 from [1].
Part (3) is Proposition 12 from [4], and part (9) is Proposition 3.1 from [1].
(cid:3)

Also, although it will not be a focus of this paper, we will sometimes want to

consider the set of all defects:

Deﬁnition 2.2. We deﬁne the defect set D to be {δ(n) : n ∈ N}, the set of all
defects.

The paper [1] also deﬁned the notion of a stable defect :

Deﬁnition 2.3. We deﬁne a stable defect to be the defect of a stable number.

Because of part (9) of Theorem 2.1, this deﬁnition makes sense; a stable defect
α is not just one that is the defect of some stable number, but one for which any n
with δ(n) = α is stable. Stable defects can also be characterized by the following
proposition from [1]:

Proposition 2.4. A defect α is stable if and only if it is the smallest β ∈ D such
that β ≡ α (mod 1).

We can also deﬁne the stable defect of a given number, which we denote δst(n).

Deﬁnition 2.5. For a positive integer n, deﬁne the stable defect of n, denoted
δst(n), to be δ(3kn) for any k such that 3kn is stable. (This is well-deﬁned as if
3kn and 3ℓn are stable, then k ≥ ℓ implies δ(3kn) = δ(3ℓn), and so does ℓ ≥ k.)

Note that the statement “α is a stable defect”, which earlier we were thinking
of as “α = δ(n) for some stable n”, can also be read as the equivalent statement
“α = δst(n) for some n”.

We then have the following facts relating the notions of knk, δ(n), knkst, and

δst(n):

Proposition 2.6. We have:

(1) δst(n) = mink≥0 δ(3kn)
(2) δst(n) is the smallest α ∈ D such that α ≡ δ(n) (mod 1).
(3) knkst = mink≥0(k3knk − 3k)
(4) δst(n) = knkst − 3 log3 n
(5) δst(n) ≤ δ(n), with equality if and only if n is stable.
(6) knkst ≤ knk, with equality if and only if n is stable.

Proof. These are just Propositions 3.5, 3.7, and 3.8 from [1].

(cid:3)

2.2. Low-defect polynomials and low-defect pairs. As has been mentioned in
Section 1, we are going to represent the set Ar by substituting in powers of 3 into
certain multilinear polynomials we call low-defect polynomials. We will associate
with each one a “base complexity” to from a low-defect pair. In this section we will
review the basic properties of these polynomials. First, their deﬁnition:

8

HARRY ALTMAN

Deﬁnition 2.7. We deﬁne the set P of low-defect pairs as the smallest subset of
Z[x1, x2, . . .] × N such that:

(1) For any constant polynomial k ∈ N ⊆ Z[x1, x2, . . .] and any C ≥ kkk, we

have (k, C) ∈ P.

(2) Given (f1, C1) and (f2, C2) in P, we have (f1 ⊗ f2, C1 + C2) ∈ P, where,

if f1 is in r1 variables and f2 is in r2 variables,

(f1 ⊗ f2)(x1, . . . , xr1+r2) := f1(x1, . . . , xr1 )f2(xr1+1, . . . , xr1+r2).

(3) Given (f, C) ∈ P, c ∈ N, and D ≥ kck, we have (f ⊗ x1 + c, C + D) ∈ P

where ⊗ is as above.

The polynomials obtained this way will be referred to as low-defect polynomials.
If (f, C) is a low-defect pair, C will be called its base complexity. If f is a low-
defect polynomial, we will deﬁne its absolute base complexity, denoted kf k, to be
the smallest C such that (f, C) is a low-defect pair. We will also associate to a
low-defect polynomial f the augmented low-defect polynomial

ˆf = f ⊗ x1

Note that the degree of a low-defect polynomial is also equal to the number
of variables it uses; see Proposition 2.8. We will often refer to the “degree” of a
low-defect pair (f, C); this refers to the degree of f . Also note that augmented
low-defect polynomials are never low-defect polynomials; as we will see in a mo-
ment (Proposition 2.8), low-defect polynomials always have nonzero constant term,
whereas augmented low-defect polynomials always have zero constant term. We can
also observe, as mentioned above, that low-defect polynomials are in fact read-once
polynomials.

Note that we do not really care about what variables a low-defect polynomial (or
pair) is in – if we permute the variables of a low-defect polynomial or replace them
with others, we will still regard the result as a low-defect polynomial. From this
perspective, the meaning of f ⊗ g could be simply regarded as “relabel the variables
of f and g so that they do not share any, then multiply f and g”. Helpfully, the ⊗
operator is associative not only with this more abstract way of thinking about it,
but also in the concrete way it was deﬁned above.

In [1] were proved the following propositions about low-defect pairs:

Proposition 2.8. Suppose f is a low-defect polynomial of degree r. Then f is a
polynomial in the variables x1, . . . , xr, and it is a multilinear polynomial, i.e., it has
degree 1 in each of its variables. The coeﬃcients are non-negative integers. The
constant term is nonzero, and so is the coeﬃcient of x1 . . . xr, which we will call
the leading coeﬃcient of f .

Proposition 2.9. If (f, C) is a low-defect pair of degree r, then

kf (3n1, . . . , 3nr )k ≤ C + 3(n1 + . . . + nr).

and

k ˆf (3n1, . . . , 3nr+1)k ≤ C + 3(n1 + . . . + nr+1).

Proof. This is a combination of Proposition 4.5 and Corollary 4.12 from [1].

(cid:3)

Because of this, it makes sense to deﬁne:

INTEGER COMPLEXITY: REPRESENTING NUMBERS OF BOUNDED DEFECT

9

Deﬁnition 2.10. Given a low-defect pair (f, C) (say of degree r) and a number N ,
we will say that (f, C) eﬃciently 3-represents N if there exist nonnegative integers
n1, . . . , nr such that

N = f (3n1, . . . , 3nr ) and kN k = C + 3(n1 + . . . + nr).

We will say ( ˆf , C) eﬃciently 3-represents N if there exist n1, . . . , nr+1 such that

N = ˆf (3n1 , . . . , 3nr+1) and kN k = C + 3(n1 + . . . + nr+1).

More generally, we will also say f 3-represents N if there exist nonnegative integers
n1, . . . , nr such that N = f (3n1, . . . , 3nr ). and similarly with ˆf .

Note that if (f, C) (or ( ˆf , C)) eﬃciently 3-represents some N , then (f, kf k) (re-
spectively, ( ˆf , kf k) eﬃciently 3-represents N , which means that in order for (f, C)
(or ( ˆf , C) to 3-represent anything eﬃciently at all, we must have C = kf k. However
it is still worth using low-defect pairs rather than just low-defect polynomials since
we may not always know kf k. In our applications here, where we want to compute
things, taking the time to compute kf k, rather than just making do with an upper
bound, may not be desirable.

For this reason it makes sense to use “f eﬃciently 3-represents N ” to mean “some
(f, C) eﬃciently 3-represents N ” or equivalently “(f, kf k) eﬃciently 3-reperesents
N ”. Similarly with ˆf .

In keeping with the name, numbers 3-represented by low-defect polynomials, or
their augmented versions, have bounded defect. Let us make some deﬁnitions ﬁrst:

Deﬁnition 2.11. Given a low-defect pair (f, C), we deﬁne δ(f, C), the defect of
(f, C), to be C − 3 log3 a, where a is the leading coeﬃcient of f . When we are
not concerned with keeping track of base complexities, we will use δ(f ) to mean
δ(f, kf k).

Deﬁnition 2.12. Given a low-defect pair (f, C) of degree r, we deﬁne

δf,C (n1, . . . , nr) = C + 3(n1 + . . . + nr) − 3 log3 f (3n1, . . . , 3nr ).

Then we have:

Proposition 2.13. Let (f, C) be a low-defect pair of degree r, and let n1, . . . , nr+1
be nonnegative integers.

(1) We have

δ( ˆf (3n1, . . . , 3nr+1)) ≤ δf,C (n1, . . . , nr)

and the diﬀerence is an integer.

(2) We have

and if r ≥ 1, this inequality is strict.

δf,C(n1, . . . , nr) ≤ δ(f, C)

Proof. This is a combination of Proposition 4.9 and Corollary 4.14 from [1].

(cid:3)

In fact, not only is δ(f, C) an upper bound on the values of δf,C , it is the least

upper bound:

Proposition 2.14. Let (f, C) be a low-defect pair, say of degree r. Then δf,C is a
strictly increasing function in each variable, and

δ(f, C) = sup

k1,...,kr

δf,C (k1, . . . , kr).

10

HARRY ALTMAN

Proof. We can deﬁne g, the reverse polynomial of f :

g(x1, . . . , xr) = x1 . . . xrf (x−1

1 , . . . , x−1

r ).

So g is a multilinear polynomial in x1, . . . , xr, with the coeﬃcient of Qi∈S xi in
g being the coeﬃcient of Qi /∈S xi in f . By Proposition 2.8, f has nonnegative
coeﬃcients, so so does g; since the constant term of f does not vanish, the x1 . . . xr
term of g does not vanish. Hence g is strictly increasing in each variable.

Then

δf,C(k1, . . . , kr) = C + 3(k1 + . . . + kr) − 3 log3 f (3k1, . . . , 3kr )

= C − 3 log3

f (3k1, . . . , 3kr )

3k1+...+kr

= C − 3 log3 g(3−k1, . . . , 3−kr )

which is strictly increasing in each variable, as claimed. Furthermore, if a is the
leading coeﬃcient of f , then it is also the constant term of g, and so

Thus

inf

k1,...,kr

g(3−k1, . . . , 3−kr ) = a.

sup

k1,...,kr

δf,C(k1, . . . , kr) = C − 3 log3 a = δ(f, C).

(cid:3)

With this, we have the basic properties of low-defect polynomials.

2.3. Describing numbers of small defect. Now we will brieﬂy discuss the
“building-up” method from [4] and [1] that restricts what numbers may lie in Ar.
The new “ﬁltering-down” half, truncation, will have to wait for Section 4.

First, we will need the idea of a leader :

Deﬁnition 2.15. A natural number n is called a leader if it is the smallest number
with a given defect. By part (6) of Theorem 2.1, this is equivalent to saying that
either 3 ∤ n, or, if 3 | n, then δ(n) < δ(n/3), i.e., knk < 3 + kn/3k.

Let us also deﬁne:

Deﬁnition 2.16. For any real r ≥ 0, deﬁne the set of r-defect numbers Ar to be

Deﬁne the set of r-defect leaders Br to be

Ar := {n ∈ N : δ(n) < r}.

Br := {n ∈ Ar : n is a leader}.

These sets are related by the following proposition from [1]:

Proposition 2.17. For every n ∈ Ar, there exists a unique m ∈ Br and k ≥ 0
such that n = 3km and δ(n) = δ(m); then knk = kmk + 3k.

Because of this, we can focus on describing Br, and derive Ar from it.
The paper [4] showed how to inductively build up coverings of the sets Br. It

provided the base case [4] in the form of the following theorem:

Theorem 2.18. For every α with 0 < α < 1, the set of leaders Bα is a ﬁnite set.

It then showed how to inductively build up coverings for the sets Bα, B2α, B3α,
again subject to the restriction that α < 1. By applying this with α arbitrarily
close to 1, the paper [1] obtained the following result:

INTEGER COMPLEXITY: REPRESENTING NUMBERS OF BOUNDED DEFECT

11

Theorem 2.19. For any real r ≥ 0, there exists a ﬁnite covering set Sr for Br.
Furthermore, we can choose Sr such that each (f, C) ∈ Sr has degree at most ⌊r⌋.

In this paper, we will actually not need the bound on the degree of the polynomi-
als; the truncation operation will always output low-defect pairs with appropriately
bounded degree, regardless of the degrees of the inputs. This allows the possibil-
ity of building up a covering Sr by a diﬀerent method, rather than the particular
method of [1]. This includes the possibility of using the same method, but with a
smaller step size α.

3. Low-defect expressions, the nesting ordering, and structure of

low-defect polynomials

In this section we will go further into the structure of low-defect polynomials. In
order to do this, we will investigate the expressions that give rise to them. That is
to say, if we have a low-defect polynomial f , it was constructed according to rules
(1)–(3) in Deﬁnition 2.7; each of these rules though gives a way not just of building
up a polynomial, but an expression. For instance, we can build up the polynomial
4x + 2 by using rule (1) to make 2, then using rule (3) to make 2x + 1, then using
rule (2) to make 2(2x+1) = 4x+2. The polynomial 4x+2 itself does not remember
its history, of course; but perhaps we want to remember its history – in which we
do not want to consider the polynomial 4x + 2, but rather the expression 2(2x + 1),
which is diﬀerent from the expression 4x + 2, which has a diﬀerent history.

Strictly speaking, it is possible to prove many of the theorems about low-defect
polynomials in this and the next section purely by structural induction, using just
the rules (1)–(3) in Deﬁnition 2.7. But introducing low-defect expressions is more
enlightening; it makes it clear why, for instance, the nesting ordering (see Deﬁni-
tion 3.11) takes the form of a forest.

So, with that, we deﬁne:

Deﬁnition 3.1. A low defect expression is deﬁned to be a an expression in positive
integer constants, +, ·, and some number of variables, constructed according to the
following rules:

(1) Any positive integer constant by itself forms a low-defect expression.
(2) Given two low-defect expressions using disjoint sets of variables, their prod-
uct is a low-defect expression. If E1 and E2 are low-defect expressions, we
will use E1 ⊗ E2 to denote the low-defect expression obtained by ﬁrst rela-
beling their variables to disjoint and then multiplying them.

(3) Given a low-defect expression E, a positive integer constant c, and a variable
x not used in E, the expression E · x + c is a low-defect expression. (We
can write E ⊗ x + c if we do not know in advance that x is not used in E.)

And, naturally, we also deﬁne:

Deﬁnition 3.2. We deﬁne an augmented low-defect expression to be an expression
of the form E·x, where E is a low-defect expression and x is a variable not appearing
in E.
If E is a low-defect expression, we also denote the augmented low-defect
expression E ⊗ x by ˆE.

It is clear from the deﬁnitions that evaluating a low-defect expression yields
a low-defect polynomial, and that evaluating an augmented low-defect expression
yields an augmented low-defect polynomial. Note also that low-defect expressions

12

HARRY ALTMAN

Figure 2. Low-defect tree for the expression 2((73(3x1 + 1)x2 +
6)(2x3 + 1)x4 + 1).

2

1

1

1

2

6

73

1

3

are read-once expressions, so, as mentioned earlier, low-defect polynomials are read-
once polynomials.

3.1. Equivalence and the tree representation. We can helpfully represent a
low-defect expression by a rooted tree, with the vertices and edges both labeled
by positive integers. Note, some information is lost in this representation – but,
as it happens, nothing we will care about; it turns out that while knowing some
of the history of a low defect polynomial is helpful, knowing the full expression it
originated from is more than is necessary. The tree representation is frequently more
convenient to work with than an expression, as it does away with such problems
as, for instance, 4 and 2 · 2 being separate expressions. In addition, trees can be
treated more easily combinatorially; in a sequel paper[3], we will take advantage of
this to estimate how many elements of Ar lie below a given bound x. So we deﬁne:

Deﬁnition 3.3. Given a low-defect expression E, we deﬁne a corresponding low-
defect tree T , which is a rooted tree where both edges and vertices are labeled with
positive integers. We build this tree as follows:

(1) If E is a constant n, T consists of a single vertex labeled with n.
(2) If E = E′ · x + c, with T ′ the tree for E′, T consists of T ′ with a new root
attached to the root of T ′. The new root is labeled with a 1, and the new
edge is labeled with c.

(3) If E = E1 · E2, with T1 and T2 the trees for E1 and E2 respectively, we
construct E by “merging” the roots of E1 and E2 – that is to say, we remove
the roots of E1 and E2 and add a new root, with edges to all the vertices
adjacent to either of the old roots; the new edge labels are equal to the old
edge labels. The label of the new root is equal to the product of the labels
of the old roots.

See Figure 2 for an example illustrating this construction.
We can use these trees to deﬁne a notion of equivalence for expressions:

Deﬁnition 3.4. Two low-defect expressions are said to be equivalent if their cor-
responding trees are isomorphic. (Here isomorphism must preserve both the root
and all labels.)

INTEGER COMPLEXITY: REPRESENTING NUMBERS OF BOUNDED DEFECT

13

Furthermore, every such tree occurs in this way:

Proposition 3.5. Every rooted tree, with vertices and edges labeled by positive
integers, occurs (up to isomorphism) as the tree for some low-defect expression.

Proof. Call the tree T . We prove this by induction on the number of vertices. If
T has only one vertex, the root, labeled n, it occurs as the tree for the low-defect
expression n. Otherwise, the tree has more than one vertex, i.e., the root has at
least one child.

If the root has only one child, let T ′ be the tree obtained by deleting the root of
T , and let E′ be a low-defect expression that yields it. If the root is labeled n and
the unique edge oﬀ of it is labeled c, and x is a variable not appearing in E′, then
the expression n(E′ · x + c) is a low-defect expression that yields T . (If n = 1, we
may omit the multiplication by n.)

Finally, the root could have more than one child; call its children v1, . . . , vr, and
call its label n. Then for 1 ≤ i ≤ r, let Ti be the tree obtained by removing all
vertices except the root and the descendants of vi, and relabeling the root to have
a label of 1. Then for each i we can pick a low-defect expression Ei that yields Ti;
then the expression n · E1 · · · Er is a low-defect expression that yields T . (Again, if
n = 1, we may omit the multiplication by n.)
(cid:3)

Because of Proposition 3.5, we can use the term “low-defect tree” to simply refer
to a rooted tree with vertices and edges labeled by positive integers. Also, among
the various expressions in an equivalence class (i.e., that yield the same tree), the
one constructed by Proposition 3.5 is one we’d like to pick out:

Deﬁnition 3.6. Given a low-defect tree T , a low-defect expression for it generated
by the method of Proposition 3.5 (with multiplications by 1 omitted) will be called
a reduced low-defect expression for T .

As mentioned above, passing from an expression E to its tree T loses a little bit
of information, but not very much. We can, in fact, completely characterize when
two expressions will yield the same tree:

Proposition 3.7. Two low-defect expressions E and E′ are equivalent if and only
if one can get from E to the E′ by applying the following transformations to subex-
pressions:

(1) For low-defect expressions E1 and E2, one may replace E1 · E2 by E2 · E1.
(2) For low-defect expressions E1, E2, and E3, one may replace (E1 · E2) · E3

by E1 · (E2 · E3), and vice versa.

(3) For integer constants n and m, one may replace n · m by the constant nm;
and for an integer constant k with k = mn, one may replace k by m · n.
This latter rule may only be applied if k does not appear as an addend in a
larger expression.

(4) For a low-defect expression E1, one may replace 1 · E1 by E1, and vice

versa.

(5) One may rename all the variables in E, so long as distinct variables remain
distinct. (This transformation can only be applied to E as a whole, not
subexpressions.)

Proof. It’s clear that all these moves do not change the tree. The problem is proving
that all equivalences come about this way.

14

HARRY ALTMAN

Suppose T is the tree for E, T ′ is the tree for E′, and φ : T → T ′ is an
isomorphism. We induct on the number of vertices of T , the label of the root, and
the structure of E and E′.

First we consider the case where either E or E′ is a product.

In this case,
we decompose E and E′ until we have written each as a product of low-defect
expressions which themselves are not products. Each of these factors can either
be written as F · x + c for some low-defect expression F , some x not appearing in
F , and some c; or as a natural number constant. Say E = E1 · · · Er · n1 · · · ns and
E′ = E′
i have the former form and the ni are
constants. (Due to rules (1) and (2), we do not need to worry about parenthesization
or the order of the factors.) Note that by assumption, r + s, r′ + s′ ≥ 1, and at
least one of them is at least 2.

s′ , where the Ei and E′

1 · · · n′

1 · · · E′

r′ · n′

i denote the tree of E′

Let Ti denote the tree of Ei and T ′

i. Then we can conclude
that the root of T has r children, and that Ti can be formed from T by removing,
along with all their descendants, all the children of the root except child i, and
changing the label of the root to 1. Similarly with T ′
i and the r′ children of its
root. Similarly, if we let N denote the product of the ni, and N ′ the product of
i, we see that N is the label of the root of T , and N ′ the label of the root of
the n′
T ′. Since T and T ′ are isomorphic, then, we have N = N ′, r = r′, and φ maps
the children of the root of T to the children of the root of T ′. This allows us to
construct isomorphisms φi : Ti → T ′
σ(i), where σ is a ﬁxed permutation in the group
Sr. By the inductive hypothesis, then, each Ei can be turned into E′
σ(i) by use of
moves of type (1)-(5); we can then use rules (1) and (2) to put these back in the
original order. (Note that rule (5) should be applied all at once, at the end, so as
to ensure that no two distinct variables are ever turned into the same variable.)

Meanwhile, the product n1 · · · ns may be turned into the product N by moves
of type (3) and (4) (type (4) is necessary if s = 0; note that in this case we cannot
have r = 0). But N = N ′, which can be turned back into the product n′
s′ by
moves of type (3) and (4) as well. This concludes the case where either E or E′ is
a product.

1 · · · n′

In the case where neither E nor E′ is a product, E can either be an integer
constant n, or it can be of the form F · x + c, where F is a low-defect expression, x
is a variable not appearing in F , and c is an integer constant. In the former case, T
has no non-root vertices, so neither does T ′; since we assumed E′ is not a product,
this means it too is an integer constant n′. However, n is the label of the unique
vertex of T , and n′ that of T ′, and since T ∼= T ′, this implies n = n′. Thus E and
E′ are simply equal, and no moves need be applied.

Finally, we have the case where E = F · x + c as above. In this case, we must
also be able to similarly write E′ = F ′ · x′ + c′, as if E′ were a constant, E would
be as well by the above argument. Let U and U ′ denote the trees of F and F ′,
respectively. Then T consists of U together with a new root adjoined with a label
of 1, with the unique edge oﬀ of it labeled c; and the relation between T ′, U ′, and
c′ is the same. Then since T ∼= T ′, we conclude that c = c′ and U ∼= U ′. By the
inductive hypothesis, then, U may be transformed into U ′ by moves of type (1)-(5);
this transforms T from F · x + c to F ′ · y + c, where y is some variable not appearing
in F ′. (Since when applying rule (5), one may have to rename x if one changes one
of the variables of F to x.) One may then apply rule (5) again to replace y by x′,
completing the transformation into E′. This proves the proposition.
(cid:3)

INTEGER COMPLEXITY: REPRESENTING NUMBERS OF BOUNDED DEFECT

15

Figure 3. Two diﬀerent trees yielding the polynomial 4x + 2

1

2

2

2

1

4

This tells us also:

Corollary 3.8. If E1 and E2 are equivalent low-defect expressions that both yield
the tree T , they also yield the same low-defect polynomial f , up to renaming of the
variables. That is to say, up to renaming of the variables, it is possible to determine
f from T .

Proof. With the exception of renaming the variables, all of the moves allowed in
Proposition 3.7 consist of replacing subexpressions with other subexpressions that
evaluate to the same thing. This proves the claim.
(cid:3)

Note that inequivalent expressions (distinct trees) can also give rise to the same
polynomial; for instance, 2(2x + 1) and 4x + 2 are inequivalent expressions both
yielding the polynomial 4x + 2 (see Figure 3). However we will see in Section 3.2
that from the polynomial f we can recover at least the “shape” of T , i.e., the
isomorphism class of the rooted but unlabeled tree underlying T .

Now, the non-root vertices of the tree correspond to the variables of the original

expression:

Deﬁnition 3.9. Let E be a low-defect expression and T the corresponding tree. We
recursively deﬁne a bijection between the variables of E and the non-root vertices
of T as follows:

(1) If E is an integer constant n, then it has no variables, and T has no non-root

vertices, and the bijection is the trivial one.

(2) If E = E′ · x + c, with T ′ the tree for E′, then we use the correspondence
between variables of E′ and the non-root vertices of T ′ to associate variables
of E′ with vertices of T ′ ⊆ T ; and we assign the root of T ′ to correspond
to the variable x.

(3) If E = E1 · E2, with T1 and T2 the trees for E1 and E2 respectively, then
we use the correspondence between variables of E1 and non-root vertices of
T1 to associate variables of E1 with vertices of T1 ⊆ T ; and we do similarly
with E2 and T2.

See Figure 4 for an illustration of this bijection.
Equivalently, each variable can be thought of as corresponding to an edge rather
than to a non-root vertex; if the variable x corresponds to the vertex v, we can
instead think of it as corresponding to the edge between v and the parent of v. If
we think of variables as corresponding to vertices, however, then we can imagine the
root as corresponding to the extra variable in the augmented low-defect expression
ˆE, although this analogy is not perfect.

This bijection, placing the variables of E on the tree, shows us that the variables
of a low-defect expression do not all play the same role.
In Section 4, we will
make extensive use of the variables corresponding to leaves. See also Remark 3.19

16

HARRY ALTMAN

Figure 4. Low-defect tree for the expression 2((73(3x1 + 1)x2 +
6)(2x3 + 1)x4 + 1); non-root vertices have been marked with cor-
responding variables in addition to their labels.

2

1

1, x4

6

1

73, x2

2, x3

1

3, x1

regarding the variables corresponding to the children of the root. In the following
subsection, we will begin to lay out the details of how this works.

3.2. The nesting order, keys, and anti-keys. Given a low-defect expression,
we will deﬁne a partial order, the nesting order, on its set of variables. First, let us
make the following observation:

Proposition 3.10. Let E be a low-defect expression. Each variable of E appears
exactly once in E, and there is a smallest low-defect subexpression of E that contains
it.

Proof. By deﬁnition, a variable of E appears in E. A variable of E cannot appear
twice in E, as no rule of constructing low-defect expressions allows this; rule (2)
only allows multiplying two low-defect expressions if their variables are disjoint,
and rule (3) can only introduce a new variable diﬀerent from the ones already in
E.

For the second part, observe that rule (3) is the only rule that introduces new
variables; so say x is some variable of E, it must have been introduced via rule (3).
This means that it occurs in a subexpression of E of the form E′ · x + c, where
E′ is a low-defect expression and c is a positive integer constant. Since x itself is
not a low-defect expression, and neither is E′ · x, the next-smallest subexpression
containing x, i.e., E′ · x + c, is the smallest low-defect subexpression of E that
contains x.
(cid:3)

Because of this, it makes sense to deﬁne:

Deﬁnition 3.11. Let E be a low-defect expression. Let x and y be variables
appearing in E. We say that x (cid:22) y under the nesting ordering for E if x appears
in the smallest low-defect subexpression of E that contains y.

This is, in fact, a partial order:

Proposition 3.12. The nesting ordering for a low-defect expression E is a partial
order.

INTEGER COMPLEXITY: REPRESENTING NUMBERS OF BOUNDED DEFECT

17

Proof. We have x (cid:22) x as x appears in any expression containing x. If x (cid:22) y and
y (cid:22) z, then the smallest low-defect expression containing z also contains y, and
hence contains the smallest low-defect expression containing y, and hence contains
x. And if x (cid:22) y and y (cid:22) x, then the smallest low-defect expression containing
each is contained in the other, i.e., the smallest low-defect expression containing x
is the smallest low-defect expression containing y. Since the former has the form
E1 · x + c1, and the latter has the form E1 · y + c2, we must have x = y.
(cid:3)

In fact, it’s not just any partial order – it’s a partial order that we’ve already
sort of seen; it’s the partial order coming from the bijection between variables of a
low-defect expression E and non-root vertices of its tree T .

Proposition 3.13. Let E be a low-defect expression, and let T be the correspond-
ing tree. Then x (cid:22) y under the nesting ordering if and only if the vertex in T
corresponding to x is a descendant of the vertex in T corresponding to y.

Proof. We prove this by structural induction on E. If E is an integer constant,
then there are no variables and the statement is trivial.

In the case where E = E′ · x + c, say T ′ is the tree corresponding to E′. Suppose
x1 and x2 are variables of E. If x1 and x2 are both variables of E′, then by the
inductive hypothesis, x1 (cid:22) x2 in the nesting ordering in E′ if and only if the vertex
corresponding to x1 in T ′ is a descendant of that corresponding to x2. However,
it is clear that x1 (cid:22) x2 in the nesting ordering of E′ if and only if x1 (cid:22) x2 in the
nesting ordering of E, since the smallest low-defect subexpression of E′ containing
x2 is necessarily also the smallest low-defect subexpression of E containing x2; and
similarly with the corresponding vertices. Hence the proposition is proved in this
case. Otherwise, we must have that one of the variables is x itself; say the variables
are x and x′. But in that case we automatically have that x′ (cid:22) x, and the vertex
for x′ is a descendant of that of x.

This leaves the case where E = E1 ·E2; say T1 and T2 are the trees corresponding
to E1 and E2. If x1 and x2 are both variables of E1, then by the inductive hypoth-
esis, x1 (cid:22) x2 in the nesting ordering in E1 if and only if the vertex corresponding
to x1 in T1 is a descendant of that corresponding to x2; but as above, it does not
matter if we consider this in E1 and T1 or E and T . Similarly the statement holds
if x1 and x2 are both variables of E2. Finally, if x1 is a variable of E1 and x2 is
a variable of E2, then x1 and x2 are incomparable in the nesting ordering, as the
smallest low-defect subexpression containing x1 is contained in E1 and hence does
not contain x2, and vice versa; and, correspondingly, the corresponding vertices are
incomparable in T .
(cid:3)

Now, we’ve already seen (Corollary 3.8) that it is possible to determine the low-
defect polynomial f for a low-defect expression E from its tree T . In fact, not only
is it possible to do so, but we can write down an explicit description of the terms
of f in terms of T . Speciﬁcally:

Proposition 3.14. Let T be a low-defect tree (say with root v0) and f the cor-
responding low-defect polynomials after assigning variables to the non-root vertices
of T ; let xv denote the variable corresponding to the vertex v. Then for a subset
S of V (T ) \ {v0}, the monomial Qv∈S xv appears in f in and only if the subgraph

18

HARRY ALTMAN

induced by S ∪ v0 is a subtree of T . Furthermore, its coeﬃcient is given by


 Y

v∈S∪{v0}

w(v)





Y

e has exactly one
vertex in S ∪ {v0}



w(e)

.

The constant term corresponds to the subtree {v0}, and the leading term is the term
corresponding to all of T .

Proof. Let E be a low-defect expression giving rise to T ; we use structual induction
on E. If E is an integer constant n, then T consists of just a root labeled with n.
So the only rooted subtree of T is T itself, containing no non-root vertices; and,
correspondingly, f has a unique term, containing no variables, and with coeﬃcient
n, which matches the formula given.

If E = E′ · x + c, say T ′ and f ′ are the tree and the polynomial arising from E′.
Let vx be the vertex of T corresponding to x, which is also the root of T ′. Then a
rooted subtree of T consists of either just v0, or v0 together with a rooted subtree
of T ′. Correspondingly, since f = xf ′ + c, a term of f is either x times a term of
f , or just c. The subtree {v0} contains no non-root vertices and so corresponds to
c; since the root is labeled with a 1 and the sole edge out of it is labeled with a
c, the formula for the coeﬃcient is correct. Any other rooted subtree X consists
of v0 together with a rooted subtree X ′ or T ′; X ′ corresponds to some term m′
of f ′. Then we have a term xm′ in f , which corresponds to X, since the old root
of T ′ is also the vertex vx. Furthermore, the coeﬃcient matches that given by the
formula, changing X ′ to X just means adding in the vertex v0 and the edge {v0, vx};
however, v0 has a label of 1, not changing the product, and the label of the edge
{v0, vx} is irrelevant as both vertices are in X. (Moreover, no edges drop out of
the product, as the only new vertex is v0, and its only edge is {v0, vx}.) And since
every term of f is either c or of the form xm′ for some term m′ of f ′, every term
arises in this way.

This leaves the case where E = E1 · E2; say each Ei gives rise to a trees Ti and a
polynomial fi, and let vi denote the root of Ti. Then a rooted subtree of T consists
of {v0} together with subsets X1 ⊆ T1 and X2 ⊆ T2 such Xi∪{vi} is a rooted subtree
of Ti. Correspondingly, f = f1f2, so each term of f is the product of a term of f1
and a term of f2; since f1 and f2 have no variables in common, terms m1m2 are
determined uniquely by the pair (m1, m2), which by the inductive hypothesis are
in bijection with sets (X1, X2) as described above. It remains to check that the
coeﬃcients match. Say X1 and X2 are subsets as described above, with each Xi
corresponding to a term mi of fi, so that the subtree X1 ∪ X2 ∪ {v0} corresponds
to the term m1m2. Then the product of the labels of vertices in X1 ∪ X2 ∪ {v0}
is the product of the labels of vertices in X1 ∪ X2 times w(v0), the latter of which
is equal to w(v1)w(v2), so this is the same as the product of the labels of vertices
in X1 ∪ {v1} times the product of the labels of vertices in X2 ∪ {v2}. Meanwhile,
the product over the edges is also the product of both the previous ones, as the
only edges that could change are those that connected X1 to v1 or X2 to v2, all
of which were previously not in the product due to having both vertices in one
of the Xi ∪ {vi}; but these now connect X1 and X2 to v0, with both vertices in
X1 ∪ X2 ∪ {v0}, so they still are not in the product.

INTEGER COMPLEXITY: REPRESENTING NUMBERS OF BOUNDED DEFECT

19

Finally, the leading term corresponds to all of T as it contains all the variables,
and the constant term corresponds to {v0} as it contains none of the variables. (cid:3)

This yields the following corollary, which will be useful in Section 4:

Corollary 3.15. Let E be a low-defect expression and f the corresponding low-
defect polynomial. Any term of f other than the leading term must exclude at least
one minimal variable.

Proof. Consider the low-defect tree corresponding to E. Any subtree other than
the whole tree must exclude at least one leaf, i.e., the corresponding term of f must
exclude at least one minimal variable.
(cid:3)

It also, in particular, tells us the leading coeﬃcient of f in terms of the T , which

we will use in Section 3.3:

Corollary 3.16. Let T be a low-defect tree, and f be the corresponding low-defect
polynomial. Then the leading coeﬃcient of f is the product of the vertex labels of
T .

Proof. The leading term corresponds to the subtree consisting of all of T . This
includes all the vertices; and no edge has exactly one vertex in it, as all edges have
both vertices in it.
(cid:3)

Now, as we’ve already noted above, we cannot go backwards from f to determine
T ; the map from trees to polynomials is not one-to-one. However, we can go part of
the way back – we can determine the “shape” of T , that is to say, the isomorphism
class of the rooted but unlabeled tree underlying T ; it is only the labels we cannot
determine with certainty.

To do this, for a low-defect polynomial f , consider the set of monomials that
appear in f , without their associated coeﬃcients; ignoring the nesting ordering for
a moment, these monomials can be partially ordered by divisibility. But we can, in
fact, recover the nesting ordering (and thus the shape of T , without labels) from
this partial ordering. First, a deﬁnition:

Deﬁnition 3.17. Let E be a low-defect expression yielding a low-defect tree T
and a low-defect polynomial f ; let x be a variable in E and vx the corresponding
vertex in T . We deﬁne the key of x in E to be the term of f corresponding to the
subtree consisting of all ancestors of vx. We deﬁne the anti-key of x in E to be
the term of f corresponding to the subtree consisting of all non-descendants of vx.
So the key of x is the smallest term of f containing x (under divisibility ignoring
coeﬃcients), and the anti-key of x is the largest term not containing x.

Both these operations, key and anti-key, are order-reversing:

Proposition 3.18. Let E be a low-defect expression, and let x and y be variables
appearing in E. Then x (cid:22) y under the nesting ordering if and only if the key of
y divides the key of x (ignoring coeﬃcients), which also occurs if and only if the
anti-key of y divides the anti-key of x.

Proof. Let T be the low-defect tree determined by E, and let vx and vy be the
vertices corresponding to x and y. By Proposition 3.13, x (cid:22) y if and only if, vx
is a descendant of vy. But if vx is a descendant of vy, then every ancestor of vy
is an ancestor of vx, and so (ignoring coeﬃcients), the key of y divides the key of

20

HARRY ALTMAN

x. Conversely, if the key of y divides the key of x, then y divides the key of x,
and so vy is an ancestor of vx. Similarly, if vx is a descendant of vy, then every
non-descendant of vy is a non-descendant of vx, and so the anti-key of y divides
the anti-key of x (ignoring coeﬃcients). Convesely, if the anti-key of y divides
the anti-key of x, then every non-descendant of vy is a non-descendant of vx, i.e.,
every descendant of vx is a descendant of vy, i.e., vx is a descendant of vy and so
x (cid:22) y.
(cid:3)

Thus, from f alone, the nesting ordering on the variables can be recovered; for
as we saw above, it is possible from f alone to determine the key and the anti-key of
some variable in f (so we can speak simply of “the key of x in f ”, or “the anti-key
of x in f ”). But by Proposition 3.18, if x and y are variables in f , and we know
their keys or anti-keys, we can determine whether or not x (cid:22) y, without needing to
know the tree or expression that f came from; it does not depend on those things.
Thus it makes sense to simply talk about the nesting ordering on the variables of
f . Furthermore this means we can also recover the shape of T from f alone; the
vertex corresponding to x is a child of the vertex corresponding to y if and only if
x (cid:22) y and there are no other variables inbetween, and the vertex corresponding to
x is a child of the root if and only if x is maximal in the nesting ordering.

Indeed, we can, given f , determine all trees T that yield it. By above, we know
the shape, and which variables correspond to which vertices, and Proposition 3.14
constrains the vertex and edge labels – indeed, it not only constrains them, it
bounds them (as every label divides at least one coeﬃcient of f ), making it possible
to determine all T that yield f (and thus to determine kf k via brute-force search.
(One can also use this procedure to determine if f is a low-defect polynomial at
all, if one does not already know.) But this is rather more involved than what is
needed to compute the complexity of a low-defect expression or tree!

Remark 3.19. It is the minimal variables of f will turn out to be quite important
in Section 4, but it’s worth noting that the maximal variables have a use too – in
[1], the proposition was proved (Lemma 4.3) that if f is a low-defect polynomial of
degree at least 1, there exists a variable x, low-defect polynomials g and h, and a
positive integer c such that f = h · (g · x + c). With this framework – if we allow
for the use of commutativity and associativity – we can easily see that these x are
precisely the maximal variables of f .

3.3. A lower bound on the complexity of a low-defect polynomial. In this
section, we will discuss the notion of the complexity of a low-defect expression,
tree, or polynomial, and use this to prove a lower bound on the complexity of a
low-defect polynomial (Corollary 3.24). This lower bound is what allows us to show
that truncation will keep the degrees of our polynomials low despite our use of small
step sizes (see discussion in Section 2.3).

A low-defect expression has an associated base complexity:

Deﬁnition 3.20. We deﬁne the complexity of a low-defect expression E, denoted
kEk, as follows:

(1) If E is a positive integer constant n, we deﬁne kEk = knk.
(2) If E is of the form E1 · E2, where E1 and E2 are low-defect expressions, we

deﬁne kEk = kE1k + kE2k.

(3) If E is of the form E′ · x + c, where E′ is a low-defect expression, x is a
variable, and c is a positive integer constant, we deﬁne kEk = kE′k + kck.

INTEGER COMPLEXITY: REPRESENTING NUMBERS OF BOUNDED DEFECT

21

In Section 2 we deﬁned kf k, for a low-defect polynomial f , to be the smallest C
such that (f, C) is a low-defect pair. Above, we also deﬁned the notion of kEk for
E a low-defect expression. These are compatible as follows:

Proposition 3.21. Let f be a low-defect polynomial. Then kf k is the smallest
value of kEk among low-defect expressions E that evaluate to f .

Proof. The rules for building up a low-defect pair (f, C) are exactly the same as the
rules for building a low-defect expression E, and what these rules do to the base
complexity C is exactly the same as what they do to the complexity kEk (except
that they allow for increasing C further). So each low-defect pair (f, C) comes
from some low-defect expression E yielding f with kEk ≤ C, and any low-defect
expression E yielding f yields a low-defect pair (f, kEk). So the lowest possible
value of C and of kEk are the same.
(cid:3)

Indeed, though we will not use this formalism here, it may make sense to consider
“low-defect expression pairs”, pairs (E, C) where E is a low-defect expression and
C ≥ kEk. After all, the deﬁnition of kEk assumes one knows the complexities
of the integer constants appearing in kEk, but one may not know these exactly,
but only have an upper bound on them. For instance, one might not be using
low-defect expressions as we deﬁned them here, but rather ones where, instead of
integer constants, one has representations of integers in terms of 1, +, and ·. That
is to say, perhaps one is not using expressions such as 2(2x + 1), but rather such as
(1 + 1)((1 + 1)x + 1). In this example, the expressions used for the integer constants
were most-eﬃcient, but this may not be the case in general. In this case, it would
make sense to consider the complexity of the expression to be simply the number
of 1’s used, which would be an upper bound on the complexity of the low-defect
expression it yields. This sort of only having an upper bound is, after all, the
reason we consider pairs (f, C), and it may make sense in other contexts to do with
expressions as we do here with polynomials.

Since we like to encode low-defect expressions as trees, it makes sense to deﬁne

the complexity of these:

Deﬁnition 3.22. The complexity of a low-defect tree, kT k, is deﬁned to be the
smallest kEk among all low-defect expressions yielding T .

Note that it follows from this deﬁnition that for a low-defect polynomial f , kf k
can be equivalently characterized as the smallest kT k among all trees T yielding f .
Again, it may make sense in other contexts to consider pairs (T, C) with C ≥ kT k,
for the same reasons discussed above. If, however, we do know the complexity of
arbitrary natural numbers, then the complexities of expressions and of trees can be
computed as follows:

Proposition 3.23. We have:

(1) Let E be a low-defect expression. Then kEk is equal to the sum of the

complexities of all the integer constants occurring in E.

(2) Let T be a low-defect tree. Then
kw(e)k + X

kT k = X

e an edge

v a leaf

kw(v)k +

X

v a non-leaf vertex

w(v)>1

kw(v)k,

where w denotes the label of the given vertex or edge.

22

HARRY ALTMAN

Proof. The ﬁrst statement is a straightforward structural induction.
If E is a
If E = E1 · E2, its
constant, its complexity is the complexity of that constant.
complexity is kE1k + kE2k, which by the inductive hypothesis is the sum of the
complexities of all the constants used in either. And if E = E′ · x + c, its complexity
is kE′k + kck, which by the inductive hypothesis is the sum of the complexities of
the constants used in E′ plus that of the new constant introduced.

For the second statement, consider a reduced low-defect expression E giving rise
to T . Then the edge and vertex labels correspond exactly to the constants used in
E, with the exception of labels of 1 on non-leaf vertices. As kT k ≤ kEk, this shows
that the formula above is an upper bound on kT k. For the lower bound, note that
by Proposition 3.7, any other low-defect expression for T can be obtained by E by
the listed moves. Moves of the form (1), (2), and (5) do not alter the complexity
of an expression at all.

This leaves moves of type (3) and (4). Suppose (3) or (4) is going to be applied
to a subexpression E′; consider E′ as a product (possibly of one thing) and consider
the largest product P containing the factors of E′ as factors. That is to say, let
P be the largest subexpression of the form E1 · . . . · Ek (where due to (1) and (2),
we do not need to worry about parenthesization or order) where the Ei cannot be
written as products, and the factors of E′ are among the Ei. Since (3) and (4),
applied to factors of P , only alter things within P , and do not alter the internals of
any Ei which can be written as a sum, we see that the least complexity is obtained
by minimizing the complexity of each individual product P . But this is clearly
done by multiplying together all constants and eliminating 1’s where possible. This
leaves us with an expression which is the same as E up to moves of the form (1),
(2), and (5). Hence E has the lowest complexity among expressions for T , and so
kT k = kEk, which as noted, is given by the formula.

(cid:3)

With this, we now obtain our lower bound:

Proposition 3.24. Let (f, C) be a low-defect pair of degree k, and suppose that a
is the leading coeﬃcient of f . Then C ≥ kak + k. Equivalently, if f is a low-defect
polynomial of degree k with leading coeﬃcient a, then kf k ≥ kak + k.

Proof. Let T be a low-defect tree giving rise to f with C ≥ kT k. Then

kT k ≥ X

e an edge

kw(e)k + X

v a vertex

w(v)>1

kw(v)k

w(v)

.

Y

v a vertex

w(v)>1

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

≥ 
 X

e an edge

1
 +

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

That is, applying Corollary 3.16, it is at least the number of edges plus kak. Since
the number of edges is one less than the number of vertices, the number of edges
is k. So C ≥ kak + k.

The second statement then follows as kf k is by deﬁnition the smallest C among
(cid:3)

low-defect pairs (f, C).

In particular, the degree of a polynomial is bounded by its defect:

INTEGER COMPLEXITY: REPRESENTING NUMBERS OF BOUNDED DEFECT

23

Corollary 3.25. Let (f, C) be a low-defect pair of degree k, and suppose that
a is the leading coeﬃcient of f . Then δ(f, C) ≥ δ(a) + k ≥ k. Equivalently,
δ(f ) ≥ δ(a) + k ≥ k.

Proof. By deﬁnition, δ(f, C) = C − 3 log3 a. So

δ(f, C) = C − 3 log3 a ≥ kak + k − 3 log3 a = δ(a) + k,

and δ(a) + k ≥ k. The second statement then follows as δ(f ) is just the smallest
value of δ(f, C) among low-defect pairs (f, C).
(cid:3)

4. The truncation operation

Now, ﬁnally, we can describe the operation of truncating a low-defect polynomial
(or expression, or tree) to a given defect – the “ﬁltering-down” half of our method.
The results here will be phrased in terms of low-defect pairs, but the analogues for
low-defect expressions are clear.

4.1. Truncations and their properties. First we just describe truncating a low-
defect polynomial in general:

Proposition 4.1. Let (f, C) be a low-defect pair, say of degree r, and suppose xi
is a variable of f which is minimal with respect to the nesting ordering. Let k ≥ 0
be an integer, and deﬁne

g(x1, . . . , xi−1, xi+1, . . . , xr) := f (x1, . . . , xi−1, 3k, xi+1, . . . , xr).

Then:

(1) The polynomial g is a low-defect polynomial, and (g, C + 3k) is a low-defect

pair.

(2) If a is the leading coeﬃcient of f , then the leading coeﬃcient of g is strictly

greater than a3k, and so δ(g, C + 3k) < δ(f, C).

(3) The nesting order on the variables of g is the restriction of the nesting order

on the variables of f to {x1, . . . , xi−1, xi+1, . . . , xr}.

(4) For any k1, . . . , ki−1, ki+1, . . . , kr, we have

δg,C+3k(k1, . . . , ki−1, ki+1, . . . , kr) = δf,C (k1, . . . , ki−1, k, ki+1, . . . , kr).

Proof. Let E be a low-defect expression of complexity at most C giving rise to f ;
we apply structural induction to prove parts (1), (2), and (3). Note that E cannot
be an integer constant as then it would have no variables.

If E = E′ · x + c, there are two cases; either E′ has degree 0, or it has positive
degree. In the former case, x is the unique minimal variable, so xi = x; say E′
evaluates to the constant n and has complexity at most C′ = C − kck. Then g
is equal to the constant n3k + c, which can be given by a low-defect expression.
Furthermore, the complexity of this low-defect expression is at most C′ +3k +kck =
C + 3k, so (g, C + 3k) is a low-defect pair. And whereas the leading coeﬃcient of
f was n, the leading coeﬃcient of g is n3k + c > n3k. Finally, g has no variables,
so part (3) is trivially true.

Otherwise, if E′ has positive degree, then x is not minimal, and the minimal
variables in E are precisely the minimal variables in E′. Assume without loss of
generality that x = xr. Say E′ has complexity at most C′ = C − kck. Let f ′ be
the polynomial coming from E′, and

g′(x1, . . . , xi−1, xi+1, . . . , xr−1) := f ′(x1, . . . , xi−1, 3k, xi+1, . . . , xr−1).

24

HARRY ALTMAN

Then by the inductive hypothesis, g′ is a low-defect polynomial, coming from some
low-defect expression E′′ with complexity at most C′ +3k. So g is a low-defect poly-
nomial as it comes from the low-defect expression E′′ · x + c, which has complexity
at most

C′ + 3k + kck = C + 3k.

And if a is the leading coeﬃcient of f , then it is also the leading coeﬃcient of f ′,
and so by the inductive hypothesis the leading coeﬃcient of g′ is greater than a3k,
but the leading coeﬃcient of g is the same as that of g′. Finally, by the inductive
hypothesis, the nesting order on the variables of g′ is the restriction of the nesting
order of the variables of f ′, and the nesting order on the variables of g is the same
as that on the variables of g′, but with xr added as a new maximum element; since
the same relation holds between the nesting order for f and the nesting order for
f ′, part (3) is true in this case.

This leaves the case where E = E1 · E2. In this case, a minimal variable of E is
either a minimal variable of E1 or a minimal variable of E2. Suppose without loss
of generality that

E(x1, . . . , xr) = E1(x1, . . . , xs)E2(xs+1, . . . , xr)

and i ≤ s. Say E1 and E2 give rise to polynomials f ′ and h, and E1 has complexity
at most C′ = C − kE2k. Then if we deﬁne

g′(x1, . . . , xi−1, xi+1, . . . , xs) := f ′(x1, . . . , xi−1, 3k, xi+1, . . . , xs),

by the inductive hypothesis, g′ is a low-defect polynomial, coming from some low-
defect expression E′ with complexity at most C′ + 3k. So g is a low-defect poly-
nomial as it comes from the low-defect expression E′ · E2, which has complexity at
most C′ + 3k + kE2k = C + 3k. And if a1 is the leading coeﬃcient of f ′ and a2 is
the leading coeﬃcient of h, then the leading coeﬃcient of f = f ′ ·h is a1a2, while by
the inductive hypothesis, the leading coeﬃcient of g is strictly greater than 3ka1,
and so the leading coeﬃcient of g = g′ · h is strictly greater than 3ka1a2. Finally,
the nesting order on the variables of g is just the disjoint union of the nesting order
on the variables of g′ and the nesting order on the variables of h, and the same
relation holds between the nesting order for f and the nesting order for f ′. By the
inductive hypothesis, the nesting order for g′ is just the restriction of that for f ′,
so the same relation holds between g and f .

To prove the second statement in part (2), we note that if a is the leading

coeﬃcient of f and b is the leading coeﬃcient of g, since b > a3k,

δ(g, C + 3k) = C + 3k − 3 log3(b) = C − 3 log3(b3−k) < C − 3 log3(a) = δ(f, C).

Finally, part (4) follows as

δg,C+3k(k1, . . . , ki−1, ki+1, . . . , kr) =

C + 3k + 3(k1 + . . . + ki−1 + ki+1 + . . . + kr) − 3 log3 g(3k1 , . . . , 3ki−1, 3ki+1 , . . . , 3kr ) =
C +3(k1+. . .+ki−1+k+ki+1+. . .+kr)−3 log3 f (3k1, . . . , 3ki−1, 3k, 3ki+1 , . . . , 3kr ) =

δf,C (k1, . . . , ki−1, k, ki+1, . . . , kr).

(cid:3)

INTEGER COMPLEXITY: REPRESENTING NUMBERS OF BOUNDED DEFECT

25

Deﬁnition 4.2. Let (f, C) be a low-defect pair, and let (g, D) be obtained from
it as in Proposition 4.1; we will call (g, D) a direct truncation of f , and g an direct
truncation of f .

Furthermore, we will deﬁne (g, D) to be a truncation of (f, C) if there are low-
defect pairs (f, C) = (f0, C0), (f1, C1), . . . , (fk, Ck) = (g, D) with (fi+1, Ci+1) a
direct truncation of (fi, Ci). Similarly in this case we say g is a truncation of f .

Immediately we get:

Proposition 4.3. Say (f, C) is a low defect pair and (g, D) is a truncation of it.
Then:

(1) δ(g, D) < δ(f, C).
(2) The nesting order on the variables of g is the restriction of the nesting order

on the variables of f .

Proof. This follows immediately from iterating parts (2) and (3) of Proposition 4.1.
(cid:3)

So, when we truncate f , we are substituting powers of 3 into some of the vari-
ables, and leaving the other variables free. Say f has degree r, and consider the
function

(k1, . . . , kr) 7→ f (3k1, . . . , 3kr )

from Zr
a sense, we are restricting f to a subset of Zr
each Si is either a single point or all of Z≥0.

≥0 to N; when we truncate f , we are ﬁxing the values of some of the ki. In
≥0 fo the form S1 × . . . × Sr, where

As such we will want a way of talking about such sets; we will represent them by
elements of (Z≥0 ∪ {∗})r, where here ∗ is just an abstract symbol which is distinct
from any whole number; it represents “this position can be any number”, or the
set Z≥0, where putting in an actual number n would represent “this position must
be n”, or the set {n}. Let us formally deﬁne our way of getting a set from such an
object, how we can substitute these objects into low-defect polynomials:

Deﬁnitions 4.4. Given (k1, . . . , kr) ∈ (Z≥0 ∪ {∗})r, we deﬁne S(k1, . . . , kr) to be
the set

{(ℓ1, . . . , ℓr) ∈ Zr

≥0 : ℓi = ki for ki 6= ∗}.

Furthermore, given f ∈ Z[x1, . . . , xr], we deﬁne the 3-substitution of (k1, . . . , kr)
into f to be the polynomial obtained by substituting 3ki for xi whenever ki 6= ∗. If
(f, C) is a low-defect pair, we deﬁne the 3-substitution of (k1, . . . , kr) to be (g, D)
where g is the 3-substitution of (k1, . . . , kr) into f , and D = C + 3Pki6=∗ ki.

Be warned that in general, 3-substituting into a low-defect pair may not yield a
low-defect pair, if one substitutes into the wrong variables. For instance, if (f, C) =
((3x1 + 1)x2 + 1, 5), then 3-substituting in (∗, 1) yields (9x + 4, 8), which is not a
low-defect pair. And if (f, C) = ((3x1 + 1)(3x2 + 1)x3 + 1, 9), and one 3-substitutes
in (∗, ∗, 0), then one obtains (9x1x2 + 3x1 + 3x2 + 2, 9), the ﬁrst element of which
is not a low-defect polynomial at all.

However, in what follows, we will only be using this notion in cases where it
does, in fact, turn out to be a low-defect pair. Speciﬁcally, in the following cases:

Proposition 4.5. Let (f, C) be a low-defect pair, and let (k1, . . . , kr) ∈ (Z≥0∪{∗})r
be such that the set of i for which ki 6= ∗ corresponds to a downward-closed subset

26

HARRY ALTMAN

of the variables of f . Let (g, D) denote the 3-substitution of (k1, . . . , kr) into (f, C).
Then:

(1) The pair (g, D) is a truncation of (f, C) (and hence a low-defect pair).
(2) Let t be the number of i such that ki = ∗, and let ι be the map from Zt

≥0
≥0 given by inserting the arguments (ℓ1, . . . , ℓt) into the coordinates of

to Zr
(k1, . . . , kr) where ki = ∗. Then δg,D = δf,C ◦ ι.
Furthermore, all truncations of (f, C) arise in this way.

6= ∗,
Proof. We ﬁrst prove part (1). Let i1, . . . , is be the indices for which ki
enumerated in an order such that if xij (cid:22) xij′ then ij ≤ ij ′ . Let (f0, C0) = (f, C).
Now, for 1 ≤ j ≤ s, given (fj−1, Cj−1), we will take (fj, Cj ) to be the direct
truncation of (fj−1, Cj−1) where 3kij is substituted into xij . Of course, in order for
this to be a direct truncation, xij must be minimal in fj−1. But this follows due
to the order we have enumerated the elements; by assumption, each xij is minimal
in {xij , . . . , xis }, and since {xi1 , . . . , xis } is downwardly closed in {x1, . . . , xr}, we
have that {xij , . . . , xis } is downwardly closed in {x1, . . . , xr} \ {xi1 , . . . , xij−1 }, and
so xij is minimal in {x1, . . . , xr} \ {xi1 , . . . , xij−1 }. And by Proposition 4.3, this
last set is precisely the set of variables of fj, with the same nesting order. Thus
this is indeed a truncation.

Part (2) follows by simply iterating part (4) of Proposition 4.1 in the above.
Finally, we can see that every truncation arises in this way by inducting on the
number of steps in the truncation.
If there are no steps, then this is true with
(k1, . . . , kr) = (∗, . . . , ∗). Otherwise, say that (fs, Cs) is an s-step truncation of
(f, C) and that (fs+1, Cs+1) is a direct truncation of it; we assume by induc-
tion that (fs, Cs) is the 3-substitution into (f, C) of some tuple (k1, . . . , kr) ∈
(Z≥0 ∪ {∗})r. Then (fs+1, Cs+1) is the 3-substitution into (fs, Cs) of some tu-
ple (∗, . . . , ∗, ℓj, ∗, . . . , ∗) ∈ (Z≥0 ∪ {∗})r−s (here ℓj 6= ∗). This makes it the 3-
substitution into (f, C) of some tuple (k′
i = ki when ki 6= ∗, and
ki = ℓj for one particular i with ki = ∗.
(cid:3)

r), where k′

1, . . . , k′

So, in fact, we’ll only be using 3-substitution in cases where it yields a truncation;

or, really, we’ll just be using it as another way of thinking about truncation.

4.2. Truncating a polynomial to a given defect. Having discussed truncation
in general, we can now discuss how to truncate a low-defect polynomial to a given
defect. Earlier, in Proposition 2.14, we showed that for a low-defect pair (f, C), the
number δ(f, C) is the least upper bound of the values of δf,C. Now we show that
something stronger is true:

Proposition 4.6. Let (f, C) be a low-defect pair of degree r. Say xij , for 1 ≤ j ≤ s,
are the minimal variables of f . Then

lim

ki1 ,...,kis →∞

δf,C (k1, . . . , kr) = δ(f, C)

(where the other ki remain ﬁxed).

Proof. Consider once again g, the reverse polynomial of f :
1 , . . . , x−1

g(x1, . . . , xr) = x1 . . . xrf (x−1

r ).

So g is a multilinear polynomial in x1, . . . , xr, with the coeﬃcient of Qi∈S xi in
g being the coeﬃcient of Qi /∈S xi in f . Let a denote the leading coeﬃcient of f ,
which is also the constant term of g.

INTEGER COMPLEXITY: REPRESENTING NUMBERS OF BOUNDED DEFECT

27

By Corollary 3.15, every non-leading term of f excludes some minimal variable.
Hence every non-constant term of g includes some minimal variable. So if we once
again write

δf,C(k1, . . . , kr) = C − 3 log3 g(3−k1, . . . , 3−kr ),

we see that as the minimal variables approach inﬁnity, then each non-constant term
of g(3−k1, . . . , 3−kr ) approaches 0, and so g(3−k1, . . . , 3−kr ) approaches a. So once
again we have

lim

ki1 ,...,kis →∞

δf,C (k1, . . . , kr) = C − 3 log3 a = δ(f, C).

(cid:3)

One can obtain numerical versions of this proposition, but we do not bother to

state them here.

We can restate this proposition as follows:

Corollary 4.7. Let (f, C) be a low-defect pair, say of degree r > 0, let 0 ≤
s < δ(f, C) be a real number. Then there exists a number K such that, when-
ever δf,C (k1, . . . , kr) < s, then ki ≤ K for some i such that xi is minimal in the
nesting ordering for f .

Proof. By Proposition 4.6, since s < δ(f, C), we can choose some K such that
δf,C(k1, . . . , kr) ≥ s, where ki = K + 1 if xi is minimal in the nesting ordering and
ki = 0 otherwise. Then if for some ℓ1, . . . , ℓr we have δf,C (ℓ1, . . . , ℓr) < s, then
since δf,C is increasing in all variables, there is some i such that ℓi ≤ K.
(cid:3)

With this, we can now ﬁnally describe truncating a low-defect pair to a speciﬁed

defect:

Theorem 4.8. Let (f, C) be a low-defect pair, say of degree r, let s ≥ 0 be a real
number, and let S = {(k1, . . . , kr) : δf,C (k1, . . . , kr) < s}. Then there exists a ﬁnite
set T ⊆ (Z≥0 ∪ {∗})r such that:
(1) We have S = Sp∈T S(p).
(2) For each p in T , the set of i for which ki 6= ∗ corresponds to a subset of
the variables of f which is downward closed (under the nesting ordering);
hence if (g, D) denotes the 3-substitution of p into (f, C), then (g, D) is a
truncation of (f, C). Furthermore, we have δ(g, D) ≤ s, and hence deg g ≤
⌊s⌋; and if g has degree 0, the former inequality is strict.

Proof. We prove the statement by induction on r.

Suppose r = 0, that is to say, f is a constant n. If s > δ(f, C), then we may
take T = {()}, where here () indicates the unique element of (Z≥0 ∪ {∗})0. For
S() = {()}, and S = {()} as well, for δf,C () = C − 3 log3 n = δ(f, C) < s. So the
ﬁrst condition is satisﬁed. For the second condition, the set of indices used is the
empty set, we have (g, D) = (f, C) (hence (g, D) is trivially a truncation), and so
δ(g, D) = δ(f, C) < s.

Otherwise, if s ≤ δ(f, C), we take T = ∅, so Sp∈T S(p) = ∅. Since, as was
noted above, δf,C () = δ(f, C), we have δf,C () ≥ s, and hence S = ∅; thus the ﬁrst
condition is satisﬁed. The second condition is satisﬁed trivially.

Now suppose that r > 0. Once again, we have two cases. If s ≥ δ(f, C), then
≥0,

we may take T = {(∗, . . . , ∗)}. By Proposition 2.13, for any (k1, . . . , kr) ∈ Zr

28

HARRY ALTMAN

we have δf,C (k1, . . . , kr) < δ(f, C) ≤ s, i.e. S = Zr
≥0 = S(∗, . . . , ∗), satisfying
the ﬁrst condition. For the second condition, we once again have that the set of
indices used is the null set, so (g, D) = (f, C), and so is trivially a truncation, and
δ(g, D) = δ(f, C) ≤ s.

This leaves the case where r > 0 and s < δ(f, C). In this case, we may apply
Corollary 4.7, and choose a K such that whenever δf,C (k1, . . . , kr) < s, then ki ≤ K
for some i which is minimal in the nesting ordering. That is to say, if we deﬁne

T0 := {(∗, . . . , ∗, ki, ∗, . . . , ∗) : xi minimal in nesting ordering, ki ≤ K},

then S ⊆ Sp∈T0
S(p), and for each p ∈ T , the 3-substitution of p into (f, C)
is a direct truncation of (f, C). However, we still do not necessarily have that
δ(g, D) ≤ s, nor do we necessarily have equality in the ﬁrst condition. This is
where we apply the inductive hypothesis.

For each p ∈ T0, let (gp, Dp) be the 3-substitution of p into (f, C); this is a
direct truncation of (f, C). Apply the inductive hypothesis to each (gp, Dp) to
obtain Tp ⊆ (Z≥0 ∪ {∗})r−1. We can then pull this back to T ′
p ⊆ (Z≥0 ∪ {∗})r;
since p = (∗, . . . , ∗, ki, ∗, . . .) for some position i and some number ki, we can pull
back q = (ℓ1, . . . , ℓi−1, ℓi+1, . . . , ℓr) ∈ Tp (where here we may have ℓj = ∗) to
q′ := (ℓ1, . . . , ℓi−1, ki, ℓi+1, . . . , ℓr). Finally we can take T = Sp∈T0
It remains to show that T has the desired properties. Say we have an element
of T ; it is an element of some T ′
p, i.e., with the notation above, it has the form q′
for some q ∈ Tp. Say p = (∗, . . . , ∗, ki, ∗, . . .). The indices used in q correspond to
some downward closed subset of the variables of (gp, Dp), i.e. to a downward closed
subset of {x1, . . . , xi−1, xi+1, . . . , xr}. Since xi is minimal in {x1, . . . , xr}, adding
it in again results in a downward closed set.

T ′
p.

Now we check that S ⊆ Sp∈T S(p). Say δf,C (k1, . . . , kr) < s; then there is some
i with xi minimal and ki ≤ K. Let p be the corresponding element of T0 and
(gp, Dp) as above. Then by Proposition 4.1, δgp,Dp (k1, . . . , ki−1, ki+1, . . . , kr) =
δf,C(k1, . . . , kr), and so (k1, . . . , ki−1, ki+1, . . . , kr) ∈ Tp, and so (k1, . . . , kr) ∈ T ′
p ⊆
T , as needed.

Suppose now that we take an element of T ; write it as q′ ∈ T ′

p for some p and
some q ∈ Tp, using the notation above. Then (gq′ , Dq′ ) can also be obtained by
3-susbtituting q into (gp, Dp); hence by the inductive hypothesis, δ(gq′ , Dq′ ) ≤ s,
and this is strict if deg gq′ = 0. This then proves as well that S ⊇ Sp∈T S(p); say
q′ ∈ T , write q′ = (k1, . . . , kr), and let i1, . . . , is be the indices for which ki = ∗.
Then for (ℓ1, . . . , ℓr) ∈ S(q′), we may write δf,C (ℓ1, . . . , ℓr) = δgq′ ,Dq′ (ℓi1 , . . . , ℓis),
and this latter is less than s, since it is at most δ(gq′ , Dq′ ), and strictly less than it
if deg gq′ > 0. This proves the theorem.
(cid:3)

And if we can truncate one low-defect polynomial to a given defect, we can
truncate many low-defect polynomials to that same defect. Here, at last, is the
result of taking the “building-up” Theorem 2.19, and applying our new “ﬁltering-
down” step:

Theorem 4.9. For any real s ≥ 0, there exists a ﬁnite set Ss of low-defect pairs
satisfying the following conditions:

(1) For any n ∈ Bs, there is some low-defect pair in Ss that eﬃciently 3-

represents n.

INTEGER COMPLEXITY: REPRESENTING NUMBERS OF BOUNDED DEFECT

29

(2) Each pair (f, C) ∈ Ss satisﬁes δ(f, C) ≤ s, and hence deg f ≤ ⌊s⌋; and if

f has degree 0, the former inequality is strict.

Proof. By Theorem 2.19, there exists a ﬁnite set Ts of low-defect pairs such that
for any n ∈ Bs, there is some low-defect pair in Ts that eﬃciently 3-represents s.
(Indeed, by Theorem 2.19, we may even choose Ts to only consist of polynomials
of degree at most ⌊s⌋, but this is not needed.)

Now for each (f, C) ∈ Ts, take Tf,C as provided by Theorem 4.8; deﬁne Tf,C to

be the set

{(g, D) : (g, D) is a 3-substitution of p into (f, C), p ∈ Tf,C};

this is a set of low-defect pairs by condition (2) of Theorem 4.8. We can then deﬁne
Ss to be the union of the Tf,C . We see immediately that S satisﬁes condition (2)
of the theorem, as this follows from condition (2) of Theorem 4.8.

To verify condition (1), say n ∈ Bs. Then there is some (f, C) ∈ Ts that
eﬃciently 3-represents n; say n = f (3ℓ1, . . . , 3ℓr ) with knk = C + 3(ℓ1 + . . . + ℓr),
so δf,C (ℓ1, . . . , ℓr) = δ(n) < s. Then (ℓ1, . . . , ℓr) ∈ S(p) for some p ∈ Tf,C . Say p =
(k1, . . . , kr), and let i1, . . . , is be the indices for which ki = ∗. Then if we let (g, D)
be the 3-substitution of p into (f, C), then n = f (3ℓ1, . . . , 3ℓr ) = g(3ℓi1 , . . . , 3ℓis ),
and knk = C +3(ℓ1 +. . .+ℓr) = D +3(ℓi1 +. . .+ℓis), so n is eﬃciently 3-represented
by (g, D) ∈ Tf,C ⊆ Ss.
(cid:3)

Note that although such a covering of Br cannot produce extraneous numbers
in the sense of 3-representing numbers whose defects are too high, it can still 3-
represent numbers that are not leaders.

We then obtain Theorem 1.5 as a corollary:

Proof of Theorem 1.5. Given s, we may consider a set Ss of low-defect pairs as
described in Theorem 4.9. We may then deﬁne Ts to be the set of low-defect
polynomials used in these pairs. Then if δ(n) < s, n is 3-represented by ˆf for some
f ∈ Ss. Conversely, if n is 3-represented by ˆf for some f ∈ Ss, then either deg f > 0,
in which case δ(n) < δ(f ) ≤ s, or deg f = 0, in which case δ(n) ≤ δ(f ) < s.
(cid:3)

Acknowledgements. The author is grateful to J. Arias de Reyna and E. H. Brooks
for helpful discussion. He thanks his advisor J. C. Lagarias for help with editing and
further discussion. Work of the author was supported by NSF grants DMS-0943832
and DMS-1101373.

Appendix A. Representing closed intervals

It’s worth noting that the theorems above about Ar and Br, and how to build up
coverings for them, etc., are formulated in terms of Ar and Br, which are deﬁned
by the strict inequality δ(n) < r. In many contexts, however, it is more natural to
consider the nonstrict inequality δ(n) ≤ r. So let us deﬁne:

Deﬁnition A.1. For a real number r ≥ 0, the set Ar is the set {n ∈ N : δ(n) ≤ r}.
The set Br is the set of all elements of Ar which are leaders.

We can then also deﬁne:

Deﬁnition A.2. A ﬁnite set S of low-defect pairs will be called a covering set
for Br if, for every n ∈ Br, there is some low-defect pair in S that eﬃciently
3-represents it.

30

HARRY ALTMAN

One can then write down theorems about Ar and Br similar to those above and
in [4] and [1] about Ar and Br. We will state them here without proof, as the
proofs are the same except for the strictnesses of some of the inequalities.

Theorem A.3. For any real 0 ≤ α < 1, Bα is a ﬁnite set.

Theorem A.4. Suppose that 0 < α < 1 and that k ≥ 1. Then any n ∈ B(k+1)α
can be most-eﬃciently represented in (at least) one of the following forms:

(1) For k = 1, there is either a good factorization n = u · v where u, v ∈ Bα,

or a good factorization n = u · v · w with u, v, w ∈ Bα;
For k ≥ 2, there is a good factorization n = u · v where u ∈ Biα, v ∈ Bjα
with i + j = k + 2 and 2 ≤ i, j ≤ k.

(2) n = a + b with knk = kak + kbk, a ∈ Akα, b ≤ a a solid number and

δ(a) + kbk ≤ (k + 1)α + 3 log3 2.

(3) There is a good factorization n = (a + b)v with v ∈ Bα, a + b being a
most-eﬃcient representation, and a and b satisfying the conditions in the
case (2) above.

(4) n ∈ Tα, where Tα is as deﬁned in [4] (and thus in particular either n = 1

or knk = kn − 1k + 1.)

(5) There is a good factorization n = u · v with u ∈ Tα and v ∈ Bα.

(Note here that we can use the same Tα from [4] with no alterations.)

Theorem A.5. For any real r ≥ 0, there exists a ﬁnite covering set Sr for Br.
Furthermore, we can choose Sr such that each (f, C) ∈ Sr has degree at most ⌊r⌋.

Theorem A.6. Let (f, C) be a low-defect pair, say of degree r, let s ≥ 0 be a real
number, and let S = {(k1, . . . , kr) : δf,C (k1, . . . , kr) ≤ s}. Then there exists a ﬁnite
set T ⊆ (Z≥0 ∪ {∗})r such that:
(1) We have S = Sp∈T S(p).
(2) For each p in T , the set of i for which ki 6= ∗ corresponds to a subset of
the variables of f which is downward closed (under the nesting ordering);
hence if (g, D) denotes the 3-substitution of p into (f, C), then (g, D) is a
truncation of (f, C). Furthermore, we have δ(g, D) ≤ s, and hence deg g ≤
⌊s⌋.

Theorem A.7. For any real s ≥ 0, there exists a ﬁnite set Ss of low-defect pairs
satisfying the following conditions:

(1) For any n ∈ Bs, there is some low-defect pair in Ss that eﬃciently 3-

represents n.

(2) Each pair (f, C) ∈ Ss satisﬁes δ(f, C) ≤ s, and hence deg f ≤ ⌊s⌋.

References

[1] H. Altman, Integer Complexity and Well-Ordering, Michigan Mathematical Journal 64

(2015), no. 3, 509–538.

[2] H. Altman, Internal Structure of Addition Chains: Well-Ordering, arXiv:1409.1627, 2014
[3] H. Altman, Reﬁned Estimates for Counting Numbers of Low Defect, in preparation.
[4] H. Altman and J. Zelinsky, Numbers with Integer Complexity Close to the Lower Bound,

Integers 12 (2012), no. 6, 1093–1125.

[5] J. Arias de Reyna, Complejidad de los n´umeros naturales, Gac. R. Soc. Mat. Esp. 3 (2000),

230–250.

INTEGER COMPLEXITY: REPRESENTING NUMBERS OF BOUNDED DEFECT

31

[6] A. Brauer, On Addition Chains, Bull. Amer. Math. Soc., 45 (1939), 736–739.
[7] H. Dellac, Interm´ed. Math. 1 (1894), 162–164.
[8] A. Flammenkamp, Drei Beitr¨age zur diskreten Mathematik: Additionsketten, No-Three-in-
Line-Problem, Sociable Numbers, Diplomarbeit in Mathematics (Bieleﬁeld University, 1991),
pp. 3–118.

[9] R. K. Guy, Some suspiciously simple sequences, Amer. Math. Monthly, 93 (1986), 186–190;

and see 94 (1987), 965 & 96 (1989), 905.

[10] R. K. Guy, Unsolved Problems in Number Theory, Third Edition, Springer-Verlag, New York,

2004, pp. 399–400.

[11] J. Iraids, K. Balodis, J. ˇCer¸nenoks, M. Opmanis, R. Opmanis, K. Podnieks. Integer Com-

plexity: Experimental and Analytical results, arXiv:1203.6462, 2012

[12] D. E. Knuth, The Art of Computer Programming, Vol. 2, Third Edition, Addison-Wesley,

Reading, Massachusetts, pp. 461–485

[13] K. Mahler and J. Popken, On a maximum problem in arithmetic (Dutch), Nieuw Arch.

Wiskunde, (3) 1 (1953), 1–15; MR 14, 852e.

[14] A. Scholz, Aufgabe 253, Jahresbericht der Deutschen Mathematikervereinigung, Vol. 47, Teil

II, B. G. Teubner, Leipzig and Berlin, 1937, pp. 41–42.

[15] M. V. Subbarao, Addition Chains – Some Results and Problems, Number Theory and Ap-
plications, Editor R. A. Mollin, NATO Advanced Science Series: Series C, V. 265, Kluwer
Academic Publisher Group, 1989, pp. 555–574.

[16] I. Volkovich, Characterizing Arithmetic Read-Once Formulae, arXiv:1408.1995, 2014
[17] J. Zelinsky, An Upper Bound on Integer Complexity, in preparation

