6
1
0
2

 
r
a

 

M
1
2

 
 
]

G
L
.
s
c
[
 
 

1
v
2
5
3
6
0

.

3
0
6
1
:
v
i
X
r
a

Online Learning with Low Rank Experts

Elad Hazan˚

Tomer Koren:

Roi Livni;

Yishay Mansour§

March 22, 2016

Abstract

We consider the problem of prediction with expert advice when the losses of the experts
have low-dimensional structure: they are restricted to an unknown d-dimensional subspace.
We devise algorithms with regret bounds that are independent of the number of experts and

depends only on the rank d. For the stochastic model we show a tight bound of Θp?dTq,
an upper bound of Opd?Tq and a lower bound of Ωp?dTq.

and extend it to a setting of an approximate d subspace. For the adversarial model we show

1

Introduction

Arguably the most well known problem in online learning theory is the so called prediction with
experts advice problem. In its simplest form, a learner wishes to make an educated decision and
at each round chooses to take the advice of one of N experts. The learner then suﬀers a loss
between ´1 and 1.
for the learner will incur Θp?T log Nq regret (Cesa-Bianchi and Lugosi, 2006). However, it

It is a standard result in online learning that, without further assumptions, the best strategy

is natural to assume that while experts are abundant, their decisions are based on common
paradigms and that their decision making is based on few degrees of freedom – for example, if
experts are indeed experts, their political bias, social background or school of thought largely
dominates their decision making. Experts can also be assets on which the learner wishes to
distribute her wealth. In this setting, weather, market condition and interests are dominant
factors.

It is also sensible to assume that one can exploit this structure to achieve better regret
bounds, potentially independent of actual number of experts while still maintaining a strategy
of picking an expert’s advice at each round. Our main result is of this ﬂavor and we show how
a learner can exploit hidden structure in the problem in an online setting.

We model the problem as follows: We assume that each expert corresponds to a vector ui
in Rd space where d is potentially small. Then at each round the experts loss corresponds to a
scalar product with a vector vt chosen arbitrarily, and possibly in an adversarial manner. The
learner does not observe the chosen embedding of the experts in Euclidean space nor the vectors
vt, and can only observe the loss of each expert.

To further motivate our setting, let us consider the low rank expert model in the stochastic

case. It is well known that for linear predictors in d-dimensional space the regret will be Op?dTq,
on an approximate rank – formally we show that one can improve on the Op?T log Nq regret

independent of number of experts. Indeed, we show that a simple follow the leader algorithm
will achieve this regret bound. In fact, one novelty of this paper is a regret bound that depends

˚Princeton University; email: ehazan@cs.princeton.edu. Parts of this work were done while at Microsoft

Research, Herzliya.

:Technion—Israel Institute of Technology and Microsoft Research, Herzliya; email: tomerk@technion.ac.il.
;The Hebrew University of Jerusalem and Microsoft Research, Herzliya; email: roi.livni@mail.huji.ac.il.
§Microsoft Research, Herzliya and Tel Aviv University; email: mansour.yishay@gmail.com.

1

bound and derive bounds that depend on the approximate rank rather than the number of
experts.

The non-stochastic setting is more challenging.

dimension one can achieve Op?dTq regret bound even in the non-stochastic case. But the

It is true that for linear predictors in d-

result assumes that learner has access to the geometric structure of the problem, namely, the
embedding of the experts in the Euclidean space. Given the embedding one can apply a Follow
the Regularized Leader approach with proper regularization to derive the desired regret bound.

Our main result is a regret minimization algorithm that achieves an Opd?Tq regret in this

low d-rank setting, when the learner does not have access to the experts’ embedding in Euclidean
space. Our algorithm does not need to know the value of the rank d, and adaptively adapts
to it. Thus we demonstrate a regret bound that is independent of number of experts. We

accompany this upper bound with an Ωp?dTq lower bound.

Our results are part of a larger agenda in online learning. A working premise in Online
Learning is that in most cases the stochastic case is the hardest case. Indeed, the literature
is ﬁlled with generalization bounds and their analogue regret bounds. However, a striking
diﬀerence is that the statistical bounds are often achieved using simple ERM algorithms, that are
oblivious to any structure in the problem, even if the structure is required for the generalization
bounds to be valid. In contrast, to achieve the analogue regret bound, one has to work harder.
For ﬁnite hypothesis class the log N factor is achieved by a sophisticated algorithm, and for
more general convex problems in Euclidean space a problem-speciﬁc regularization needs to be
invoked in order to achieve optimality. Thus, a key diﬀerence is that online algorithms need to
be tailored to the structure of the problem. This lead to the disappointing fact that to achieve
optimal regret bounds, it is not enough for the problem to be structured but the learner needs
to actively understand the structure.

Our current research is an attempt to better understand this key diﬀerence: we wish to
understand whether an online linear predictor can somehow exploit the geometry of the problem
in an implicit manner, similar to an ERM algorithm, and how. For this, we invoke a setting
where the learner must choose its predictor without the a-priori ability to devise a regularizer.
Our ﬁnding so far indeed demonstrate that even without access to the structure the learner can
indeed overcome her dependence on the irrelevant parameter N .

Technically, one should compare our regret bound of Opd?Tq to the standard regret bound
of Op?T log Nq. For our bound to be superior one needs that d “ op?log Nq; while this can

indeed be the case in various settings, our result can be better seen as a ﬁrst step in a more
general research direction. We aim to understand how can online algorithms take advantage of
structural assumptions in the losses, without given any explicit information about it.

1.1 Related Work

Low rank assumptions are ubiquitous in the Machine Learning literature. It has been success-
fully applied to various problems, most notably to matrix completion (Cand`es and Recht, 2009;
Foygel and Srebro, 2011; Srebro et al., 2004) but also in the context of classiﬁcation with missing
data (Goldberg et al., 2010; Hazan et al., 2015) and large scale optimization (Shalev-Shwartz et al.,
2011).

A similar problem that was studied in the literature is the Branching Experts Problem (Gofer et al.,

2013). In the branching expert problem N potential experts are eﬀectively only k distinct ex-
perts, but the clustering of the experts to the k clusters is unknown a-priori. This case can
be considered as a special instance of our setting as indeed we can embed each expert as a k-

dimensional vector. Gofer et al. (2013) proved a sharp Θp?kTq regret (the bound is tight only
when k ă c log N for some constant c ą 0). It is perhaps worth noting that when eﬀectively
only k experts appear, the stochastic bound is Op?T log kq, thus showing that in this similar
problem, it is not true that the stochastic case is the hardest case.

2

Complexity measures for online learning. We are not the ﬁrst to try and understand
what is the proper analogue for ERM in the online setting. Notions like the VC-dimension and
Rademacher complexities have been extended to notions of Littlestone-Dimension (Littlestone,
1988; Shalev-Shwartz, 2011), and Sequential Rademacher Complexity (Rakhlin et al., 2010)
respectively.

The SOA algorithm suggested by Ben-David et al. (2009) is a general framework for regret
minimization the depends solely on the Littlestone dimension. However, the SOA algorithm is
conceptually distinct from an ERM algorithm within our framework: to implement the SOA
algorithm, one has to have access to the structure of the class (speciﬁcally, one needs to compute
the Littlestone dimension of subclasses within the algorithm).

Sequential Rademacher seems like a powerful tool in improving our bounds and answering
some of our open problems. There are also advances in constructing eﬀective algorithms within
this framework (Rakhlin et al., 2012). However, as the branching expert example shows, there
is no general argument that show that structure in the problem leads to stochastic–analogue
bound on the complexity.

In another line of research, which is similar in spirit to ours,
Learning from easy data.
several authors attempt to go beyond worst-case analysis in online learning, and provide al-
gorithms and bounds that can exploit deﬁciencies in the data. While most of the work in
this direction has been focused on worst-case robust online algorithms that can also adapt
to stochastic i.i.d. data (e.g., Hazan and Kale, 2009; Rakhlin et al., 2013; De Rooij et al., 2014;
Sani et al., 2014), several papers have explored various structural assumptions that can be lever-
aged for obtaining improved regret guarantees (e.g., Cesa-Bianchi et al., 2007; Hazan and Kale,
2010, 2011; Chiang et al., 2012; Rakhlin and Sridharan, 2013). However, to the best of our
knowledge, low rank assumptions in online learning have not been explored in this context.

Adaptive online algorithms. Online adaptive learning methods have recently been the
topic of extensive study and are eﬀective for large scale stochastic optimization in practice.
One of the earliest and most widely used methods in this family is the AdaGrad algorithm
(Duchi et al., 2011), a subgradient descent method that dynamically incorporate knowledge of
the geometry of the data from earlier iterations. Our problem can be cast into an online linear
optimization problem and subgradient descent methods are indeed applicable. It might seem at
ﬁrst sight that adapting the regularization via AdaGrad can lead to desired results. However,

as we show in Appendix B, in our setting the regret bound for AdaGrad is Ωpmint?N , Tuq,

hence depends on N .

2 Problem Setup and Main Results

We recall the standard adversarial online experts model for T rounds with N experts. At each
round t “ 1, . . . , T , the learner chooses a probability vector xt P ∆N , where ∆N denotes the
N -simplex, namely the set of all possible distributions over N experts,

An adversary replies by choosing a loss vector ℓt P r´1, 1sN , and the learner suﬀers a loss
xtpℓtq “ xt¨ ℓt. The objective of the learner is to minimize her regret, which is deﬁned as follows,

xpiq “ 1) .

∆N “!x P RN : @i, xpiq ą 0 and ÿN

i“1

RegretT “

T

ÿt“1

xt ¨ ℓt ´ min
iPrNs

T

ÿt“1

ℓtpiq.

3

In the stochastic online experts model, the adversary selects a distribution D over the loss

vectors in r´1, 1sN , and at time t a random ℓt P r´1, 1sN is selected from D. The regret is.

RegretT “

T

ÿt“1

xt ¨ Erℓts ´ min
iPrNs

T

ÿt“1

Erℓtpiqs ,

where the expectations are taken over the random loss vectors selected from D.

In our setting, we wish to assume that there is a structure over the experts which implies
that the loss vectors are structured, and are derived from a low rank subspace. Therefore we
will add the following constraint over the adversary: let L P MNˆT be the loss matrix obtained
in hindsight (i.e., the t-th column of L is ℓt). We restrict the feasible strategies for the adversary
to only such that satisfy:

rankpLq “ d .

An equivalent formulation of our model is as follows: An adversary chooses at the beginning
of the game a matrix U P MNˆd, where each row corresponds to an expert. At round t the
adversary chooses a vector vt, and the learner gets to observe ℓt where ℓt “ U vt. The objective
of the learner remains the same: to choose at each round a probability distribution xt that
minimizes the regret. We stress that the learner observes only the loss vectors ℓt, and does not
have access to either U or the vectors vt.

2.1 Main Results

We next state the main results of this paper:

Theorem 1. The T -round regret of Algorithm 2 (described in Section 4 below) is at most

Opd?Tq, where d “ rankpLq.

We remark that a regret upper bound of Op?T mintd,?log Nuq is attainable by combin-

ing the standard multiplicative-updates algorithm with our algorithm.1 Our upper bound is
accompanied by the following lower bound.

Theorem 2. For any online learning algorithm, T and d ď log2 N , there exists a sequence of
loss vectors ℓ1, . . . ℓT such that

RegretT “

T

ÿt“1

xt ¨ ℓt ´ min
iPrNs

ℓtpiq ě c dT

8

,

T

ÿt“1

and rankpLq “ d.

3 Preliminaries

3.1 Notation

Let In be the n ˆ n identity matrix. Let 1n be a vector of length n with all 1 entries. The
columns of a matrix U are denoted by u1, u2, . . .. The i’th coordinate of a vector x is denoted
by xpiq. For a matrix M , we denote by M: the Moore-Penrose pseudo-inverse of M . For a
?xTHx, and its
positive deﬁnite matrix H ą 0 we will denote its corresponding norm }x}H “
?xTH´1x. Given a positive semi-deﬁnite matrix M ľ 0 its corresponding
dual norm }x}˚H “
Ellipsoid is deﬁned as:

EpMq “ tx : xTM:x ď 1u .

1A standard way to accomplish this is by running the two online algorithms in parallel, and choosing between

their predictions by treating them as two meta-experts in another multiplicative-weights algorithm.

4

3.2 Ellipsoidal Approximation of Convex Bodies

A main tool in our algorithm is an Ellipsoid approximation of convex bodies. Recall John’s
theorem for symmetric zero-centered convex bodies (see, for example, Ball, 1997):

Theorem 3 (John’s Theorem). Let K be a convex body in Rd that is symmetric around zero
(i.e., K “ ´K). Let E be an ellipsoid with minimum volume enclosing K. Then:

1
?d

E Ď K Ď E.

While computing the minimum volume enclosed ellipsoid is computationally hard, for sym-
metric convex bodies it can be approximated to within 1 ` ǫ factor in polynomial time. Specif-
ically, given as input a matrix A P MNˆd, consider the polytope PA “ tx : }Ax}8 ď 1u. We
have the following.

Theorem 4 (Gr¨otschel et al., 2012, Theorem 4.6.5). There exists a poly-time procedure MVEEpAq
that receives as input a matrix A P MNˆd and returns a matrix M such that

Ep 1

2d Mq Ď PA Ď EpMq.

(1)

3.3 Online Mirror Descent Algorithm

Another main tool in our analysis is a well known online convex optimization algorithm Online
Mirror Descent. The Online mirror descent is a subgradient descent method for optimization
over a convex set in Rd that implies a regularization factor, chosen a-priori. In Algorithm 1 we
describe the algorithm for the special case where the convex set is ∆N and the regularization
function is chosen to be } ¨ }2
Algorithm 1 OMD: Online Mirror Descent
1: input: H ą 0, tηtuT
2: for t “ 1 to T do

H for some input matrix H ą 0:

t“1, x1 P ∆N .

3:

4:

5:

Play xt
Suﬀer cost xt ¨ ℓt and observe ℓt
Update

6: end for

xt`1 “ arg min
xP∆N

ℓt ¨ x ` η´1

t }x ´ xt}2
H .

The regret bound of the algorithm is dependent on the choice of regularization and is given

as follows (see, for example, Hazan, 2015):

Lemma 5. The T -round regret of the OMD algorithm (Algorithm 1) is bounded as follows:

T

ÿt“1

ℓt ¨ xt ´

T

ÿt“1

ℓt ¨ x˚ ď

1
ηT }x1 ´ x˚}2

H `

1
2

T

ÿt“1

ηtp}ℓt}˚Hq2 .

3.4 Rademacher Complexity

Our tool to analyze the stochastic case will be the Rademacher Complexity, speciﬁcally we will
use it to bound the regret of a “Follow The Leader” algorithm (FTL). Recall that the FTL
algorithm selection rule is deﬁned as follows:

xt “ arg min
xP∆N

t´1

ÿi“1

ℓi ¨ x.

5

One way to bound the regret of the FTL algorithm in the stochastic case is by bounding
the Rademacher complexity of the feasible samples. Recall that the Rademacher Complexity
of a class of target function F over a sample St “ tℓ1, . . . , ℓtu is deﬁned as follows

RpF , Stq “ Eσ«sup

fPF

1
t

σifpℓiqﬀ ,

t

ÿi“1

where σ P t´1, 1ut are i.i.d. Rademacher distributed random variables. The following bound is
standard and well known, and for completeness we provide a proof in Appendix A.1.2

Lemma 6. Let K be a symmetric convex set centered around zero in Rd. Recall that the dual
set K˚ is deﬁned as follows:

K˚ “ tx : sup

yPK |y ¨ x| ď 1u.

Let St “ tℓ1, . . . , ℓtu Ď K and let F Ď αK˚ be a subclass of linear functions, then:

RpF , Stq ď αc d

t

.

Another standard bound that can be found is the case F is l1 norm bounded Kakade et al.

(2009):
Lemma 7. Let St “ tˆℓ1, . . . , ˆℓtu P RN and let F1 be a subclass of linear functions such that
supt}f}1 : f P Fu ď 1, then:

RpF1, Stq ď max

i

} ˆℓi}8c 2 log N

t

.

The Rademacher complexity is a powerful tool in statistical learning theory and it allows
us to bound the generalization error of an FTL algorithm. Namely, for every sample St “
tℓ1, . . . , ℓtu denote:

t

Then we have the following bound for every f˚ P F (for i.i.d. loss vectors; see for example
Shalev-Shwartz and Ben-David, 2014):

fS “ arg min
fPF

fpℓiq.

ÿi“1

E

St„D

E

ℓ„DrfStpℓq ´ f˚pℓqs ď 2 E

St„DrRpF , Stqs .

Applying this to FTL in the experts setting we have, in terms of regret, that for any x˚:

E« T
ÿt“1

T

ℓtxt ´ ℓt ¨ x˚ﬀ “
ÿt“1
ÿt“1
ď 2

E

ℓ1,...ℓt´1„D
T

E

ℓt„Drℓt ¨ xt ´ ℓt ¨ x˚s

E

St´1„DrRp∆N , St´1qs .

(2)

2Surprisingly, we could not ﬁnd any speciﬁc reference that precisely derives it.

6

4 Upper Bounds

4.1 Stochastic Online Experts

We begin by analyzing the simpler stochastic model, where the loss vectors ℓt’s are chosen

i.i.d. In this case we can achieve a regret bound of Op?dTq using the “Follow The Leader”

(FTL) algorithm. We will in fact show an even stronger result for the stochastic case, that
an approximate rank is enough to bound the complexity. Recall that the approximate rank,
rankǫpLq, of a matrix is deﬁned as follows (see Alon et al., 2013):

rankǫpLq “ mintrankpL1q : }L1 ´ L}8 ă ǫu.

The following statement is the main result for this section:
Theorem 8. Assume that an adversary chooses her losses tℓtu i.i.d. from some distribution D
supported on r0, 1sN . Then the T -round regret of an FTL algorithm is bounded by:

RegretT ď 8E”aT ¨ rankǫpLqı ` ǫaT log N ,

for every 0 ď ǫ ă 1. In particular, if rankpLq ď d almost surely, then RegretT “ Op?dTq.
Proof. Our proof relies on Eq. (2) and a bound for Rp∆N , Stq. Fix a sequence ST “ tℓ1, . . . , ℓTu
and let d “ rankǫpLq and let U be N ˆ d matrix such that

where maxi,j | ˆLi,j| ă ǫ. We will denotes the columns of ˆL by ˆℓ1, . . . ˆℓN . We deﬁne a symmetric
convex set centered around zero in Rd as follows:

L “ U V ` ˆL ,

K “ tv : supi |ui ¨ v| ď 2u .

Note that for every vt we have that vt P K if ǫ ď 1. By deﬁnition of the set we have:
ui P 2K˚ for every i. One can verify that K˚ is convex, hence if we let F “ convpu1, . . . , uNq
we have that F Ď 2K˚. We can think of F as a linear function space, where fupvq “ u ¨ v. It
follows by Lemma 6 that RpF , Stq ďa2d{t. Finally,
ÿi“1

Rp∆N , Stq “ E« sup

σix ¨ U viﬀ ` E« sup

σix ¨ ℓiﬀ ď E« sup

σix ¨ ˆℓﬀ .

ÿi“1

ÿi“1

xP∆N

xP∆N

xP∆N

1
t

1
t

1
t

t

t

t

Next, we have:

E« sup

xP∆N

1
t

t

ÿi“1

and by Lemma 7,

t

t

1
t

xP∆N

σipUTxq ¨ viﬀ
σix ¨ U viﬀ “ E« sup
ÿi“1
E« 1
σifpviqﬀ
ÿi“1
“ sup
“ RpF , Stq ă 2c d
σix ¨ ˆℓiﬀ ď ǫc 2 log N

fPconvpuiq

t

t

t

.

.

t

E sup

xP∆N« 1

t

ÿi“1

Taking Eq. (3) and Eq. (4) together, we have:

Rp∆N , Stq ď 2c rankǫ L

t

` ǫc 2 log N

t

.

The statement now follows from Eq. (2).

7

(3)

(4)

4.2 Adversarial Online Experts

H “ xTHx as a regularizer.

We now describe our algorithm for the adversarial model, given in Algorithm 2. The algorithm
is a version of Online Mirror Descent with adaptive regularization.
It maintains a positive-
deﬁnite matrix H, which is being updated whenever the newly observed loss vector ℓt is not in
the span of previously appeared losses. In all other time steps (i.e., when ℓt leaves the previous
span), the algorithm preforms an Online Mirror Descent type update (see Algorithm 1), with
the function }x}2
The algorithm updates the regularization matrix H so as to adapt to the low-dimensional
geometry of the set of feasible loss vectors. Indeed, as our analysis below reveals, H is an ellip-
soidal approximation of a certain low-dimensional convex set in RN to which the loss vectors ℓt
can be localized. This low-dimensional set is the intersection of the unit cube in N dimensions—
in which the loss vectors ℓt reside by deﬁnition—and the low dimensional subspace spanned by
previously observed loss vectors, given by spanpUq. Whenever the latter subspace changes,
namely, once a newly observed loss vector leaves the span of previous vectors, the ellipsoidal
approximation is recomputed and the matrix H is updated accordingly.

Algorithm 2 OLRE: Online Low Rank Expert
1: Initialize: x1 “ 1
N 1N , τ “ 0, k “ 0, U “ tu
2: for t “ 1 to T do
3: Observe ℓt, suﬀer cost xt ¨ ℓt.

if ℓt R spanpUq then

4:

5:

6:

7:

8:

end if

Add ℓt as a new column of U , reset τ “ 0, and set k Ð k ` 1.
Compute M “ MVEEpUTq and H “ In ` UTM U .
let τ Ð τ ` 1 and ηt “ 4ak{τ , and set:
xt`1 “ arg min
xP∆N

t }x ´ xt}2
H .

ℓt ¨ x ` η´1

9: end for

To derive Theorem 1 we begin with a simple case where there learner is aware of the subspace
from which losses are derived. Speciﬁcally, assume that at the beginning of the rounds, the
learner is equipped with a matrix U such that for all losses ℓ1, ℓ2, . . . P spanpUq where we denote
by spanpUq the span of the columns of the matrix U .
In this simpliﬁed setting, we can obtain a regret bound of Op?dTq via John’s theorem
(Theorem 3).3 As discussed above, the loss vectors ℓ1, . . . , ℓT can be localized to the intersection
of the unit cube in N dimensions with the d-dimensional subspace spanned by the columns of
U . Then, John’s theorem asserts that the minimal-volume enclosing ellipsoid of the intersection
is a ?d-approximation to the set of feasible loss vectors.

Theorem 9. Run Algorithm 1 with Input H, tηtu and x1 deﬁned as follows: (i) H “ In `
UTM U , where M “ MVEEpUTq, (ii) ηt “ 4ad{t, where d “ rankpUq, and (iii) x1 P ∆ is
arbitrary. If ℓ1, . . . , ℓT P spanpUq , then the expected T -round regret of the algorithm is at most
8?dT .

Proof. Consider the d-dimensional polytope

3We remark that for the simpliﬁed setting, the Op?dTq regret bound is in fact tight, as our Ωp?dTq lower

bound (given in Section 5) applies in a setting where the subspace of the loss vectors is known a-priori to the
learner.

P “ tv P Rd : }UTv}8 ď 1u.

8

Then by John’s Theorem (Theorem 3), we have,

Ep 1

2d Mq Ď P Ď EpMq .

(5)

In order to apply Lemma 5, we need to bound both }ℓt}˚H and }x1 ´ x˚}2
H. We ﬁrst bound the
norms }ℓt}˚H . Notice that for each loss vector ℓt there exists vt P P such that ℓt “ UTvt (as
ℓt P spanpUq and }ℓt}8 ď 1). Thus, we can write,

p}ℓt}˚Hq2 “ ℓT

t H´1ℓt “ vT

t UpIn ` UTM Uq´1UTvt ď vT

t UpUTM Uq:UTvt “ vT

t M´1vt ,

where we have used Lemma 11 (see Appendix A). Now, since vt P P and EpMq is enclosing P ,
we obtain vT

t M´1vt ď 1. This proves that

p}ℓt}˚Hq2 ď 1.

Next we bound }x1 ´ x˚}H ď 2 Since }x1 ´ x˚}H ď 2 maxxP∆n }x}H, it suﬃces to bounbd
H “
H 1. Given a convex set in

maxxP∆n }x}H. Hence, our goal is to show that }x}H ď 2?d for all x P ∆n. Since }x}2
1` 2d}x}2
Rd, recall that the dual set is given by

2d UTM U , it is enough to bound the norm }x}2

H 1 with H1 “ 1

P ˚ “ tx : sup

pPP |x ¨ p| ď 1u.

The dual of an ellipsoid EpMq is given by pEpMqq˚ “ EpM´1q and it is standard to show that
Eq. (5) implies in the dual:

Taken together we obtain that P ˚ Ď Ep2dM´1q. Note that by deﬁnition the columns of U are
in P ˚, hence, for every ui,

pEpMqq˚ Ď P ˚ Ď pEp 1

2d Mqq˚.

}ui}2

M ď 2d.

Since x P ∆M :

}x}2

H 1 “ 1

2d}U x}2

M ď 1

2d max

i

}ui}2

M ď 1

Equipped with the bounds }x}H ď ?1 ` 2d ď 2?d for all x P ∆n and }ℓt}˚H ď 1 for all t,

we are now ready to analyze the expected regret of the algorithm, which via Lemma 5 can be
bounded as follows:

RegretT “

ď

ď

ď

T

1
2

ℓt ¨ x˚
ÿt“1
ÿt“1
ηtp}ℓt}˚Hq2

ηtp}ℓt}˚Hq2

1
2

T

T

T

ℓt ¨ xt ´

ÿt“1
ÿt“1
1
ηT }x1 ´ x˚}2
H `

4
ηT

max

xP∆n }x}2
H `
ÿt“1

1
2

ηt

T

16d
ηT `

/* Lemma 5*/

/* }x1 ´ x˚}H ď 4 max

xP∆n }x}H */

/* max

xP∆n }x}2

H ď 4d,}ℓt}˚H ď 1 */ .
t“1 1{?t ď 2?T , gives the theorem.

A choice of ηt “ 4ad{t, together with the inequality řT

The d-low rank setting does not assume that the learner has access to the subspace U , and
potentially an adversary may adapt her choice of subspace to the learner’s strategy. However,
the learner can still obtain regret bounds that are independent of number of experts. We are
now ready to prove Theorem 1.

9

Proof of Theorem 1. Let t0 “ 1, td`1 “ T and for all 1 ď k ď d let tk be the round where U
is added the k’th column. Also, let Tk “ tk`1 ´ tk the length of the k’th epoch. Notice that
between rounds tk and tk`1 the algorithm’s execution is identical to Algorithm 1 with input
depicted in Theorem 9. Therefore its regret in this time period is at most 8?kTk. The total
regret is then bounded by

8

d

ÿk“0akTk ď 8gffe

d

ÿk“0

k ¨gffe

d

ÿk“0

Tk ď 8d?T ,

and the theorem follows.

5 Lower Bound

We now prove Theorem 2. For our proof we will rely on lower bounds for online learning of
hypotheses classes w.r.t to the Littlestone–dimension of the class LdimpHq (see Shalev-Shwartz,
2011). For a class H of target functions h : X Ñ t0, 1u, the Littlestone dimension LdimpHq
measure the complexity, or online learnability, of the class.
To deﬁne LdimpHq one considers trees whose internal nodes are labeled by instances. Any
branch in such a tree can be described as a sequence of examples px1, y1q, . . . ,pxd, ydq where xi
is the instance associated with the ith node in the path, and yi is 1 if xi`1 is the right child of
the i–th node, and yi “ 0 otherwise. LdimpHq is then deﬁned as the depth of the largest binary
tree that is shattered by H. An instance-labeled tree is said to be shattered by a class H if for
any root-to-leaf path px1, y1q, . . . ,pxd, ydq there is some h P H such that hpxiq “ yi.

The following results has been proved in Ben-David et al. (2009).

Lemma 10. Let H be any hypothesis class with ﬁnite LdimpHq, where Ldim is the Littlestone-
dimension of a class H. For any (possibly randomized) algorithm, exists a sequence of labeled
instances pv1, y1q, . . . ,pvT , yTq with yt P t0, 1u such that

ÿt“1 |ˆyt ´ yt|ﬀ ´ min
E« T

hPH

T

ÿi“1|hpxtq ´ yt| ěc LdimpHqT

8

.

where ˆyt is the algorithm’s output at iteration t.

Proof of Theorem 2. We let H be the 2d vertices of the d-dimensional hypercube. We deﬁne a
function class F over the domain X “ te1, . . . , edu of standard basis vectors. A function fu P F,
is labeled by u P H, and deﬁne over the set of basis vector ei, as follows,

fupeiq “#0

1

if upiq “ ´1,
if upiq “ 1.

One can verify that LdimpFq “ d. For each ui P H and y P t0, 1u,we can write

|fuipejq ´ y| “

1 ´ p2y ´ 1q ¨ ui ¨ ej

2

.

By Lemma 10, we deduce that for for each algorithm, there exists a sequence ¯y1v1, . . . ¯yT vT of
standard basis vectors and ¯y1, . . . ¯yT P t´1, 1u such that:

We now consider an adversary that chooses U as his expert matrix, and at round t the learner
observes ℓt “ Up¯ytvtq. The bound follows from Eq. (6). The fact that rankpLq “ d follows from
the fact that our experts are embedded in Rd.

u ¨ p¯ytvtq ě 2c dT

8

.

(6)

ÿi“1

T

T

xtpiqui ¨ p¯ytvtq ´ min

u

ÿt“1ÿi

10

6 Discussion and Open Problems

We considered the problem of experts with a hidden low rank structure. Our ﬁndings are that
in the non-stochastic case, similar to the stochastic case, the regret bounds are independent of
number of experts. The most natural question is than to bridge the gap between upper and
lower bound:

Open Problem 1. Is there an algorithm that can achieve regret Op?dTq for any sequence
ℓ1, . . . , ℓT such that rankpLq “ d? Alternatively, can one prove a lower bound of Ωpd?Tq?

As discussed, our agenda is more general than the low-rank setting. Our aim is to construct
new online algorithms that can exploit structure in the data, without explicit information on
the structure. Other settings can also be considered within our framework.

Another interesting setting, that avoids dependence in dimension, is to assume that experts
are embedded in a Hilbert space. By isomorphisms of Hilbert spaces this is equivalent to an
adversary that chooses an expert embedding matrix U P MNˆN such that for every ui we have
}ui}2 ď 1 and correspondingly at each time point we receive a vector vt such that }vt}2 ď 1 as
a result we have a factorization:

L “ U V T,

}U}2,8,}V }2,8 ď 1,

where }X}2,8 “ sup}y}ď1 }Xy}8. Recall the deﬁnition of the max-norm, also called the γ2-norm
(Srebro and Shraibman, 2005):

}L}max “ min

U V T“L}U}2,8 ¨ }V }2,8.

Thus, similar to the low rank setting we can deﬁne this setting as follows: At each round a
learner chooses a xt P ∆N , an adversary replies by choosing a loss vector ℓt, and the learner
incurs the corresponding loss. The adversary is restricted to strategies such that

}L}max ď 1.

The importance of this setting is that the proper generalization bound for this case is dimension
independent (e.g., Kakade et al., 2009). Hence, we ask the following question:

Open Problem 2. Is there an algorithm that can achieve regret Op?Tq for any sequence
ℓ1, . . . , ℓT such that }L}max ď 1?

We can also generalize this setting to any pair of norms, } ¨ } and its dual } ¨ }˚, where
the description of the game remains the same. The adversary chooses an embedding U of the
experts with bounded } ¨ } norm. Then, at each round he chooses a set of vectors vt with } ¨ }˚
bounded norm.
Finally, a diﬀerent interesting direction to pursue in future work is to extend the noisy result

to the adversarial setting. Namely,

Open Problem 3. Is there an algorithm that can achieve regret Op?dT ` ǫ?T log Nq for any
sequence ℓ1, . . . , ℓT such that rankǫpLq ď d?

References

N. Alon, T. Lee, A. Shraibman, and S. Vempala. The approximate rank of a matrix and its
algorithmic applications: approximate rank. In Proceedings of the forty-ﬁfth annual ACM
symposium on Theory of computing, pages 675–684. ACM, 2013.

11

K. Ball. An elementary introduction to modern convex geometry. Flavors of geometry, 31:1–58,

1997.

S. Ben-David, D. P´al, and S. Shalev-Shwartz. Agnostic online learning. In COLT. Citeseer,

2009.

E. J. Cand`es and B. Recht. Exact matrix completion via convex optimization. Foundations of

Computational mathematics, 9(6):717–772, 2009.

N. Cesa-Bianchi and G. Lugosi. Prediction, Learning, and Games. 2006.

N. Cesa-Bianchi, Y. Mansour, and G. Stoltz. Improved second-order bounds for prediction with

expert advice. Machine Learning, 66(2-3):321–352, 2007.

C.-K. Chiang, T. Yang, C.-J. Lee, M. Mahdavi, C.-J. Lu, R. Jin, and S. Zhu. Online optimization

with gradual variations. In COLT, pages 6–1, 2012.

S. De Rooij, T. Van Erven, P. D. Gr¨unwald, and W. M. Koolen. Follow the leader if you can,

hedge if you must. The Journal of Machine Learning Research, 15(1):1281–1316, 2014.

J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and

stochastic optimization. The Journal of Machine Learning Research, 12:2121–2159, 2011.

R. Foygel and N. Srebro. Concentration-based guarantees for low-rank matrix reconstruction.

arXiv preprint arXiv:1102.3923, 2011.

E. Gofer, N. Cesa-Bianchi, C. Gentile, and Y. Mansour. Regret minimization for branching

experts. In Conference on Learning Theory, pages 618–638, 2013.

A. Goldberg, B. Recht, J. Xu, R. Nowak, and X. Zhu. Transduction with matrix completion:
In Advances in neural information processing systems, pages

Three birds with one stone.
757–765, 2010.

M. Gr¨otschel, L. Lov´asz, and A. Schrijver. Geometric algorithms and combinatorial optimiza-

tion, volume 2. Springer Science & Business Media, 2012.

E. Hazan. Introduction to Onlne Convex Optimization, Draft. now Publishers Inc., 2015.

E. Hazan and S. Kale. On stochastic and worst-case models for investing. In Y. Bengio, D. Schu-
urmans, J. Laﬀerty, C. Williams, and A. Culotta, editors, Advances in Neural Information
Processing Systems 22, pages 709–717. Curran Associates, Inc., 2009.

E. Hazan and S. Kale. Extracting certainty from uncertainty: Regret bounded by variation in

costs. Machine learning, 80(2-3):165–188, 2010.

E. Hazan and S. Kale. Better algorithms for benign bandits. The Journal of Machine Learning

Research, 12:1287–1311, 2011.

E. Hazan, R. Livni, and Y. Mansour. Classiﬁcation with low rank and missing data.

In
Proceedings of The 32nd International Conference on Machine Learning, pages 257–266, 2015.

S. M. Kakade, K. Sridharan, and A. Tewari. On the complexity of linear prediction: Risk
In Advances in neural information processing

bounds, margin bounds, and regularization.
systems, pages 793–800, 2009.

N. Littlestone. Learning quickly when irrelevant attributes abound: A new linear-threshold

algorithm. Machine learning, 2(4):285–318, 1988.

12

A. Rakhlin and K. Sridharan. Online learning with predictable sequences. In Conference on

Learning Theory, pages 993–1019, 2013.

A. Rakhlin, K. Sridharan, and A. Tewari. Online learning: Random averages, combinatorial
parameters, and learnability. In Advances in Neural Information Processing Systems, pages
1984–1992, 2010.

A. Rakhlin, O. Shamir, and K. Sridharan. Localization and adaptation in online learning. In
Proceedings of the Sixteenth International Conference on Artiﬁcial Intelligence and Statistics,
pages 516–526, 2013.

S. Rakhlin, O. Shamir, and K. Sridharan. Relax and randomize: From value to algorithms. In

Advances in Neural Information Processing Systems, pages 2141–2149, 2012.

A. Sani, G. Neu, and A. Lazaric. Exploiting easy data in online optimization. In Advances in

Neural Information Processing Systems, pages 810–818, 2014.

S. Shalev-Shwartz. Online learning and online convex optimization. Foundations and Trends

in Machine Learning, 4(2):107–194, 2011.

S. Shalev-Shwartz and S. Ben-David. Understanding machine learning: From theory to algo-

rithms. Cambridge University Press, 2014.

S. Shalev-Shwartz, A. Gonen, and O. Shamir. Large-scale convex minimization with a low-rank

constraint. arXiv preprint arXiv:1106.1622, 2011.

N. Srebro and A. Shraibman. Rank, trace-norm and max-norm. In Learning Theory, pages

545–560. Springer, 2005.

N. Srebro, J. Rennie, and T. S. Jaakkola. Maximum-margin matrix factorization. In Advances

in neural information processing systems, pages 1329–1336, 2004.

A Technical Proofs

Lemma 11. Let M P Rdˆd, U P Rdˆn such that M ą 0 and U . Then

UpUTM Uq:UT “ M´1.

Proof. Let N “ M 1{2U . Then, we have NpNTNq:NT “ Id. To see this, write the SVD
decomposition N “ OΣV T with diagonal non-singular Σ P Rdˆd and OOT “ OTO “ V TV “ Id.
Then,

NpNTNq:NT “ OΣV TpV Σ2V Tq:V ΣOT “ OΣV TpV Σ´2V TqV ΣOT “ Id.

Expanding the deﬁnition of N , we get M 1{2UpUTM Uq:UTM 1{2 “ Id, and since M 1{2 is non-
singular, we can multiply by M´1{2 on both sides and obtain the lemma.

A.1 Proof of Lemma 6

The proof relies on the following corollary of John’s Theorem:

Lemma 12. Let K be a symmetric convex set centered around zero in Rd. There exists a
positive semi-deﬁnite matrix Σ such that for every x P K:

xTΣx ď sup

fPK ˚ |fpxq|2 ď dpxTΣxq .

13

Proof. (of Lemma 6). wlog we assume α “ 1, the general case follows since RpαF , Sq “
αRpF , Sq. We have
RpF , Sq “ Eσ«sup

σifpℓiqﬀ “ Eσ«sup

σipℓiqqﬀ.

f 2p

fp

1
t

1
t

1
t

t

t

t

σiℓiqﬀ ďgffeEσ«sup

fPF

ÿi“1

Next, we take Σ whose existence follows from Lemma 12. Note that Σ deﬁnes a scalar product.
Speciﬁcally let us denote xℓi, ℓjy “ ℓT

i Σℓi. Then we have

ÿi“1

ÿi“1

fPF

fPF

gffeEσ« sup

fPK ˚

f 2˜ 1

t

t

ÿi“1

σiℓi¸ﬀ ďgfffe

t

t

2

2

1

i Σℓj, and also we let }ℓi}2
2 “ ℓT
ﬁ
ﬂ

d Eσ»
t2 ›››››
ÿi“1
–
“gffed Eσ« 1
ÿi,j“1
“gffed Eσ« 1
ÿi“1
σ2}ℓi}2
2 ďd d
ďc d
}ℓi}2

σiℓi›››››
σiσjxℓi, ℓjyﬀ
2ﬀ “gffe

max

max

t2

t2

t

t

t

i

i

sup
fPK ˚

d
t2

t

2

ÿi“1}ℓi}2
f 2pℓiq ďc d

t

,

as claimed.

B Lower Bounds for the AdaGrad Algorithm

AdaGrad (see Algorithm 3) is an algorithm that adapts the regularization matrix with respect
to prior losses. Our aim in this section is to show that this learning scheme of the regularization
cannot lead to a regret bound that is independent of number of experts. Our strategy is as
follows: since the AdaGrad algorithm depends on a learning rate parameter η we consider two
cases: either η scales with N and becomes smaller, but than we show that for some sequence the
algorithm’s update is “too slow”. On the other hand, we show that if η does not scale with N ,
the algorithm becomes less stable, and we can again inﬂict damage. Taken together we prove
the following statement:
Theorem 13. Consider Algorithm 3. For concreteness we assume that x1 “ 1
ciently large N , if T ă ?N{6 then there exist a sequence tℓ1, . . . , ℓTu such that RegretT ě T{2
and rankpLq “ 1.
Lemma 14. Consider Algorithm 3 with arbitrary η and δ. For concreteness we assume that
x1 “ 1
sequence tℓ1, . . . , ℓTu such that RegretT ě T{2 and rankpLq “ 1.
Proof. We prove each bound separately.

N 1. For suﬃciently large N , if T ď max´p 1

6η , η2N ´ δ¯ then there exist a

N 1. For suﬃ-

36η2 ` 2

?δ

Case 1: T ă 1
we have that

36η2 ` 2

?δ
6η . We let ℓt “ e “ p´1,

1

N´1 ,

1

N´1 ,

1

N´1 , . . .

1

N´1q for all t. For every t

and

Gt “ateeT ` δI
?t ` δ}e}
t ℓt “

η

ηG´1

e

14

Algorithm 3 AdaGrad
1: Input: η, δ, x1 P ∆N .
2: Initialize: S0 “ G0 “ δI
3: for t “ 1 to T do
4: Observe ℓt, suﬀer cost xt´1 ¨ ℓt.

5:

set

St “ St´1 ` ℓtℓT

t , Gt “ S

1{2
t

yt “ xt ´ ηG´1
t ℓt

xt`1 “ arg min

xP∆N }yt`1 ´ x}2

Gt

6: end for

Next we use the inequality:

For T ď´ 1

3η ` ?δ¯2

T

1

ÿt“1
´ δ “ 1

?t ` δ ďż T
36η2 ` 2

0

?δ
6η , we have that:

1

?t ` δ

dt “ 2´?T ` δ ´

?δ¯

´?T ` δ ´
?t ` δ ď

1
N `

1

1
6η

?δ¯ ă
}e}´?T ` δ ´

2η

?δ¯ ď

1
2

.

1
N `

η
}e}

T

ÿi“1

Where last inequality follows since }e} ą 1 and we assume N ą 3. One can observe that our
update rule does not take yt out of the simplex ∆N and we have

xt “

1
N

1 ´

η

}e}ÿ 1
?t ` δ

e,

and further, xtp1q ă 1
loss. Hence the algorithm’s regret is at least

2 . In hindsight xtp1q suﬀers loss ´T while all other experts suﬀer positive

RegretT ě

T
2

.

Case 2: T ă η2N ´ δ. We now choose e “ p`1,`1,`1,`1,`1,`1
loooooooooooooomoooooooooooooon

N{2times

and let

q.
,´1,´1,´1,´1,´1,´1
loooooooooooooomoooooooooooooon

N{2times

ℓt “ p´1qt`1e.

As before note that

We claim that for ?t ` δ ă η?N and t ą 1 we have that:

ηG´1

t ℓt “ p´1qt`1η
?t ` δ}e}

e “ p´1qt`1η
aNpt ` δq

e.

xt “

2
N

N{2times

p1, 1, 1, 1, 1, 1
loooooomoooooon
p0, 0, 0, 0, 0, 0
loooooomoooooon

N{2times

t is even

t is odd

q

q

(7)

, 0, 0, 0, 0, 0, 0

, 1, 1, 1, 1, 1, 1

N{2times

loooooomoooooon
loooooomoooooon

N{2times

$’’’&
’’’%

15

Hence xtℓt “ 1 and since the cumulative loss of each expert is at most 1 we have that:

RegretT ě

T
2

To see that Eq. (7) holds, we will show the statement for x2 other cases are easier and follow

the same proof: y1 “ 1 1
yt “ pa, a, a, a, a, a
loooooomoooooon

follows from Lemma 15.

N ` αe, where |α| ą 1?N
,´b,´b,´b,´b,´b
loooooooooomoooooooooon

N{2times

N{2times

, hence it has the form

q where a ´ b “ 2{N and a, b ą 0. The statement now

Proof of Theorem 13. By Lemma 14 we need to show that

min

η,δ ˜max˜p

1
9η2 ` 2

?δ
3η

, η2N ´ δ¸¸ ą

?N
6

.

To prove this, we note that since both terms in the max are monotone in both variables, the
minimum is attained when there is equality, i.e., the minimal η, δ satisfy:

1
36η2 ` 2

?δ
6η “ η2N ´ δ

Hence: Since

1

36η2 ` 2

and we have that:

?δ

6η ` δ “´ 1

6η ` ?δ¯2
?N “

, we get:

1

η ˆ 1
6η `

?δ˙

?N
6 “

?2
36η2 `

?δ
6η ă

?2
36η2 ` 2

?δ
6η

q where a, b ě 0 and assume that pa´bq “

N{2times

N{2times

2{N . Let Gt “

,´b,´b,´b,´b,´b
loooooooooomoooooooooon
for some α ą 0 where

Lemma 15. Let y “ pa, a, a, a, a, a
loooooomoooooon
?δI ` αeeT´1{2
e “ p`1,`1,`1,`1,`1,`1
loooooooooooooomoooooooooooooon
1
2}y ´ x}2

N{2times

Gt “

then

min
xP∆N

q

N{2times

,´1,´1,´1,´1,´1,´1
loooooooooooooomoooooooooooooon
q

, 0, 0, 0, 0, 0

2
N p1, 1, 1, 1, 1, 1
loooooomoooooon

N{2times

loooomoooon

N{2times

Proof. Considering the Lagrangian and KKT conditions, we observe that x minimizes the dis-
tance iﬀ the following hold:

1. x P ∆N (primal feasibility)
2. λ ą 0 and θp1q “ θp2q “ ¨¨¨ θpNq. (dual feasibility)
3. x “ y ` G´1
4. xpiq ‰ 0 ñ λpiq “ 0 and λpiq ‰ 0 ñ xpiq “ 0. (complementary slackness)

t pλ ` θq (stationarity)

16

Next note that e is an eigenvector of Gt and we have for some c ă 0 that

G´1
t ce “ p´b,´b,´b,´b,´b,´b
looooooooooooomooooooooooooon

N{2times

,`b,`b,`b,`b,`b
loooooooooomoooooooooon

N{2times

q

Now we can write

q
ce “ p0, 0, . . . , 0, 0
,´2c,´2c, . . . ,´2c,´2c
loooooomoooooon
loooooooooooooomoooooooooooooon
loooooooooooooooooooooooomoooooooooooooooooooooooon

N{2times

N{2times

λ

that concludes the proof.

, c, c, c, c, c, c

`pc, c, c, c, c, c
q
looooomooooon
looooomooooon
N{2times
looooooooooooooomooooooooooooooon

N{2times

θ

17

