The Pennsylvania State University

The Graduate School

DISCRIMINATIVE MODELS FOR ROBUST IMAGE CLASSIFICATION

6
1
0
2

 
r
a

M
8

 

 
 
]
L
M

.
t
a
t
s
[
 
 

1
v
6
3
7
2
0

.

3
0
6
1
:
v
i
X
r
a

A Dissertation in

Electrical Engineering

by

Umamahesh Srinivas

c(cid:13) 2013 Umamahesh Srinivas

Submitted in Partial Fulﬁllment

of the Requirements

for the Degree of

Doctor of Philosophy

August 2013

The dissertation of Umamahesh Srinivas was reviewed and approved∗ by the following:

Vishal Monga
Monkowski Assistant Professor of Electrical Engineering
Dissertation Advisor, Chair of Committee

William E. Higgins
Distinguished Professor of Electrical Engineering

Ram M. Narayanan
Professor of Electrical Engineering

Robert T. Collins
Associate Professor of Computer Science and Engineering

Kultegin Aydin
Professor of Electrical Engineering
Head of the Department of Electrical Engineering

∗Signatures are on ﬁle in the Graduate School.

Abstract

A variety of real-world tasks involve the classiﬁcation of images into pre-determined cat-
egories. Designing image classiﬁcation algorithms that exhibit robustness to acquisition
noise and image distortions, particularly when the available training data are insuﬃcient
to learn accurate models, is a signiﬁcant challenge. This dissertation explores the devel-
opment of discriminative models for robust image classiﬁcation that exploit underlying
signal structure, via probabilistic graphical models and sparse signal representations.

Probabilistic graphical models are widely used in many applications to approximate
high-dimensional data in a reduced complexity set-up. Learning graphical structures
to approximate probability distributions is an area of active research. Recent work has
focused on learning graphs in a discriminative manner with the goal of minimizing clas-
siﬁcation error. In the ﬁrst part of the dissertation, we develop a discriminative learning
framework that exploits the complementary yet correlated information oﬀered by mul-
tiple representations (or projections) of a given signal/image. Speciﬁcally, we propose a
discriminative tree-based scheme for feature fusion by explicitly learning the conditional
correlations among such multiple projections in an iterative manner. Experiments reveal
the robustness of the resulting graphical model classiﬁer to training insuﬃciency.

The next part of this dissertation leverages the discriminative power of sparse signal
representations. The value of parsimony in signal representation has been recognized for
a long time, most recently in the emergence of compressive sensing. A recent signiﬁcant
contribution to image classiﬁcation has incorporated the analytical underpinnings of
compressive sensing for classiﬁcation tasks via class-speciﬁc dictionaries. In continuation
of our theme of exploiting information from multiple signal representations, we propose a
discriminative sparsity model for image classiﬁcation applicable to a general multi-sensor
fusion scenario. As a speciﬁc instance, we develop a color image classiﬁcation framework
that combines the complementary merits of the the red, green and blue channels of
color images. Here signal structure manifests itself in the form of block-sparse coeﬃcient
matrices, leading to the formulation and solution of new optimization problems.

As a logical consummation of these ideas, we explore the possibility of learning dis-
criminative graphical models on sparse signal representations. Our eﬀorts are inspired by

iii

exciting ongoing work towards uncovering fundamental relationships between graphical
models and sparse signals. First, we show the eﬀectiveness of the graph-based feature
fusion framework wherein the trees are learned on multiple sparse representations ob-
tained from a collection of training images. A caveat for the success of sparse classiﬁca-
tion methods is the requirement of abundant training information. On the other hand,
many practical situations suﬀer from the limitation of limited available training. So
next, we revisit the sparse representation-based classiﬁcation problem from a Bayesian
perspective. We show that using class-speciﬁc priors in conjunction with class-speciﬁc
dictionaries leads to better discrimination. We employ spike-and-slab graphical priors
to simultaneously capture the class-speciﬁc structure and sparsity inherent in the signal
coeﬃcients. We demonstrate that using graphical priors in a Bayesian set-up alleviates
the burden on training set size for sparsity-based classiﬁcation methods.

An important goal of this dissertation is to demonstrate the wide applicability of
these algorithmic tools for practical applications. To that end, we consider important
problems in the areas of:

1. Remote sensing: automatic target recognition using synthetic aperture radar im-

ages, hyperspectral target detection and classiﬁcation

2. Biometrics for security: human face recognition

3. Medical imaging: histopathological images acquired from mammalian tissues.

iv

Table of Contents

List of Figures

List of Tables

Acknowledgments

Chapter 1

Introduction
1.1 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.2 Overview of Dissertation Contributions
. . . . . . . . . . . . . . . . . .
1.2.1 Discriminative Graphical Models . . . . . . . . . . . . . . . . . .
1.2.2 Discriminative Sparse Representations . . . . . . . . . . . . . . .
1.3 Organization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

viii

x

xii

1
1
4
4
5
7

Chapter 2

Feature Fusion for Classiﬁcation via Discriminative Graphical Models 11
11
2.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
12
2.2 Background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.2.1 Probabilistic Graphical Models . . . . . . . . . . . . . . . . . . .
12
14
2.2.2 Boosting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
15
2.3 Discriminative Graphical Models for Robust Image Classiﬁcation . . . .
2.3.1 Feature Extraction . . . . . . . . . . . . . . . . . . . . . . . . . .
15
16
2.3.2
Initial Disjoint Tree Graphs . . . . . . . . . . . . . . . . . . . . .
18
2.3.3 Feature Fusion via Boosting on Disjoint Graphs . . . . . . . . . .
19
2.3.4 Multi-class Image Classiﬁcation . . . . . . . . . . . . . . . . . . .
2.4 Application: Automatic Target Recognition . . . . . . . . . . . . . . . .
20
20
2.4.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
20
2.4.2 ATR Algorithms: Prior Art . . . . . . . . . . . . . . . . . . . . .
2.5 Experiments and Results . . . . . . . . . . . . . . . . . . . . . . . . . . .
22

v

2.5.1 Experimental Set-up . . . . . . . . . . . . . . . . . . . . . . . . .
2.5.2 Recognition Accuracy . . . . . . . . . . . . . . . . . . . . . . . .
2.5.2.1
Standard Operating Condition (SOC) . . . . . . . . . .
2.5.2.2 Extended Operating Conditions (EOC) . . . . . . . . .
2.5.3 ROC Curves: Outlier Rejection Performance . . . . . . . . . . .
2.5.4 Classiﬁcation Performance as Function of Training Size
. . . . .
Summary of Results . . . . . . . . . . . . . . . . . . . . . . . . .
2.5.5
2.6 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

Chapter 3

Application: Learning Graphical Models on Sparse Features
3.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.2 Sparsity in Signal Processing . . . . . . . . . . . . . . . . . . . . . . . .
3.2.1 Compressive Sensing . . . . . . . . . . . . . . . . . . . . . . . . .
3.2.2
Sparse Representation-based Classiﬁcation . . . . . . . . . . . . .
3.3 Hyperspectral Imaging . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.3.1
3.3.2 Motivation and Contribution . . . . . . . . . . . . . . . . . . . .
3.4 Experimental Results and Discussion . . . . . . . . . . . . . . . . . . . .
3.4.1 Hyperspectral Target Detection . . . . . . . . . . . . . . . . . . .
3.4.2 Hyperspectral Target Classiﬁcation . . . . . . . . . . . . . . . . .
3.4.2.1 AVIRIS Data Set: Indian Pines
. . . . . . . . . . . . .
3.4.2.2 ROSIS Urban Data Over Pavia, Italy . . . . . . . . . .
3.5 Robust Face Recognition . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.5.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.5.2 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.6 Face Recognition Via Local Decisions From Locally Adaptive Sparse Fea-
tures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.7 Experiments and Discussion . . . . . . . . . . . . . . . . . . . . . . . . .
3.7.1 Presence of Registration Errors . . . . . . . . . . . . . . . . . . .
3.7.2 Recognition Under Random Pixel Corruption . . . . . . . . . . .
3.7.3 Outlier Rejection . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.8 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

Chapter 4

Structured Sparse Representations for Robust Image Classiﬁcation
4.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.2 Histopathological Image Classiﬁcation: Overview . . . . . . . . . . . . .
4.2.1 Prior Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.2.2 Motivation and Challenges
. . . . . . . . . . . . . . . . . . . . .
4.2.3 Overview of Contributions . . . . . . . . . . . . . . . . . . . . . .
4.3 A Simultaneous Sparsity Model for Histopathological Image Representa-
tion and Classiﬁcation . . . . . . . . . . . . . . . . . . . . . . . . . . . .

23
25
25
25
26
26
27
28

38
38
39
39
40
42
42
43
44
44
49
49
50
51
51
51

53
53
54
56
56
57

59
59
60
61
61
62

63

vi

67
69
69
73
74
77
77
77
78
79
79
80
81
85
86
86
89

90
90
92
92
92
92

94

97

98

4.4 LA-SHIRC: Locally Adaptive SHIRC . . . . . . . . . . . . . . . . . . . .
4.5 Validation and Experimental Results . . . . . . . . . . . . . . . . . . . .
4.5.1 Experimental Set-Up: Image Data Sets
. . . . . . . . . . . . . .
4.5.2 Validation of Central Idea: Overall Classiﬁcation Accuracy . . .
4.5.3 Detailed Results: Confusion Matrices and ROC Curves
. . . . .
4.5.4 Performance as Function of Training Set Size . . . . . . . . . . .
4.6 Structured Sparse Priors for Image Classiﬁcation . . . . . . . . . . . . .
4.6.1 Related Work in Model-based Compressive Sensing . . . . . . . .
4.6.2 Overview of Contribution . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . .
Set-theoretic Interpretation . . . . . . . . . . . . . . . . . . . . .
4.7.1
4.7.2
Spike-and-slab Priors . . . . . . . . . . . . . . . . . . . . . . . . .
4.7.3 Analytical Development . . . . . . . . . . . . . . . . . . . . . . .
4.7.4
. . . . . . . . . . . . . . . . . . . . .
4.7.5 Parameter Learning . . . . . . . . . . . . . . . . . . . . . . . . .
4.8 Experimental Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.9 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4.7 Design of Discriminative Spike-and-slab Priors

SSPIC: Some Observations

Chapter 5

Conclusions and Future Directions
5.1 Summary of Main Contributions
. . . . . . . . . . . . . . . . . . . . . .
5.2 Suggestions for Future Research . . . . . . . . . . . . . . . . . . . . . . .
5.2.1 Discriminative Graph Learning . . . . . . . . . . . . . . . . . . .
5.2.2
. . . . . . . . . . . . . . . . . . . . . . . .
5.2.3 Design of Class-speciﬁc Priors . . . . . . . . . . . . . . . . . . . .

Joint Sparsity Models

Appendix A

A Greedy Pursuit Approach to Multi-task Classiﬁcation

Appendix B

Parameter Learning for SSPIC

Bibliography

vii

List of Figures

1.1 Schematic of ﬁngerprint veriﬁcation . . . . . . . . . . . . . . . . . . . .

Illustration of (2.4)-(2.5) . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.1
2.2 Two-stage framework for designing discriminative graphical models . . .
2.3 Receiver operating characteristic curves
. . . . . . . . . . . . . . . . . .
2.4 Classiﬁcation error vs. training sample size: SOC . . . . . . . . . . . . .
2.5 Classiﬁcation error vs. training sample size: EOC . . . . . . . . . . . . .

1

14
16
35
36
37

44
45
46

samples

3.1 Hyperspectral image classiﬁcation using discriminative graphical models
3.2 Target detection: ROC for FR-I . . . . . . . . . . . . . . . . . . . . . . .
3.3 Diﬀerence maps for AVIRIS Indian Pine data set . . . . . . . . . . . . .
3.4 Performance of diﬀerent approaches as a function of number of training
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

48
3.5 Representation of a block in the test image from a locally adaptive dictionary 52
3.6 Proposed framework for face recognition . . . . . . . . . . . . . . . . . .
54
55
3.7 An example of rotated test images
. . . . . . . . . . . . . . . . . . . . .
55
3.8 Recognition rate for rotated test images . . . . . . . . . . . . . . . . . .
56
3.9 An example of scaled test images . . . . . . . . . . . . . . . . . . . . . .
3.10 ROC curves for outlier rejection . . . . . . . . . . . . . . . . . . . . . . .
58

4.1 Color channel-speciﬁc dictionary design for SHIRC . . . . . . . . . . . .
4.2
Illustration to motivate LA-SHIRC . . . . . . . . . . . . . . . . . . . . .
4.3 Local cell regions selected from the IBL data set
. . . . . . . . . . . . .
4.4 LA-SHIRC: Locally adaptive variant of SHIRC . . . . . . . . . . . . . .
4.5 Sample images from the ADL data set . . . . . . . . . . . . . . . . . . .
4.6 Sample breast lesion images from the IBL data set . . . . . . . . . . . .
4.7 Bar graphs indicating the overall classiﬁcation accuracies of the competing
methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.8 Receiver operating characteristic curves for diﬀerent organs . . . . . . .
4.9 Overall classiﬁcation accuracy as a function of training set size for three
diﬀerent scenarios: low, limited and adequate training . . . . . . . . . .
4.10 SSPIC: Set-theoretic comparison . . . . . . . . . . . . . . . . . . . . . .
4.11 Probability density function of a spike-and-slab prior.
. . . . . . . . . .

65
67
67
68
70
71

72
75

76
79
80

viii

4.12 Structured spike-and-slab priors for sparse representation-based classiﬁ-
cation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.13 Error maps for AVIRIS Indian Pine data set . . . . . . . . . . . . . . . .
4.14 Sample lung tissue images . . . . . . . . . . . . . . . . . . . . . . . . . .
4.15 Performance as a function of training ratio . . . . . . . . . . . . . . . . .

81
87
88
88

ix

List of Tables

2.1 Training images from the MSTAR data set used for experiments
. . . .
2.2 Test images from the MSTAR data set used for experiments . . . . . . .
2.3 Test images used for the version variant testing set EOC-1 . . . . . . . .
2.4 Confusion matrix for SOC: EMACH correlation ﬁlter . . . . . . . . . . .
2.5 Confusion matrix for SOC: SVM classiﬁer with pose estimation . . . . .
2.6 Confusion matrix for SOC: Conditional Gaussian model with pose esti-
mation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.7 Confusion matrix for SOC: AdaBoost with pose estimation . . . . . . .
2.8 Confusion matrix for SOC: Iterative Graph Thickening (IGT) with pose
estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.9 Confusion matrix for SOC: SVM classiﬁer without pose estimation . . .
2.10 Confusion matrix for SOC: Conditional Gaussian model without pose
estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.11 Confusion matrix for SOC: AdaBoost without pose estimation . . . . .
2.12 Confusion matrix for SOC: Iterative Graph Thickening (IGT) without
pose estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.13 Confusion matrix for EOC-1: EMACH . . . . . . . . . . . . . . . . . . .
2.14 Confusion matrix for EOC-1: SVM classiﬁer . . . . . . . . . . . . . . . .
2.15 Confusion matrix for EOC-1: Conditional Gaussian model . . . . . . . .
2.16 Confusion matrix for EOC-1: AdaBoost . . . . . . . . . . . . . . . . . .
2.17 Confusion matrix for EOC-1: IGT . . . . . . . . . . . . . . . . . . . . .
2.18 Confusion matrix for EOC-2: EMACH . . . . . . . . . . . . . . . . . . .
2.19 Confusion matrix for EOC-2: SVM classiﬁer . . . . . . . . . . . . . . . .
2.20 Confusion matrix for EOC-2: Conditional Gaussian model . . . . . . . .
2.21 Confusion matrix for EOC-2: AdaBoost . . . . . . . . . . . . . . . . . .
2.22 Confusion matrix for EOC-2: IGT . . . . . . . . . . . . . . . . . . . . .
2.23 Average classiﬁcation accuracy . . . . . . . . . . . . . . . . . . . . . . .

3.1 Target detection: Confusion matrix for the FR-I hyperspectral image.
Four diﬀerent methods are compared. (Nt = 18 and Nb = 216.) . . . . .
3.2 Classiﬁcation rates for the AVIRIS Indian Pines test set. LSGM z-score
= −2.13 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

22
22
23
29
29

29
30

30
30

31
31

31
32
32
32
33
33
33
33
34
34
34
34

45

46

x

various scaling factors

3.3 Classiﬁcation rates for the University of Pavia test set. LSGM z-score =
−2.01 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
47
3.4 Classiﬁcation rates for the Center of Pavia test set. LSGM z-score = −2.17 47
3.5 Recognition rate (in percentage) for scaled test images using SRC under
56
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.6 Recognition rate (in percentage) for scaled test images using proposed
block-based approach under various SF . . . . . . . . . . . . . . . . . .
3.7 Overall recognition rate (as a percentage) for the scenario of scaling by
horizontal and vertical factors of 1.214 and 1.063 respectively . . . . . .
3.8 Overall recognition rate (as a percentage) for the scenario where test
images are scaled and subjected to random pixel corruption . . . . . . .

57

57

58

4.1 Confusion matrix: Lung . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.2 Confusion matrix: Kidney . . . . . . . . . . . . . . . . . . . . . . . . . .
4.3 Confusion matrix: Spleen . . . . . . . . . . . . . . . . . . . . . . . . . .
4.4 Confusion matrix: Intraductal breast lesions . . . . . . . . . . . . . . . .
4.5 False alarm probability for ﬁxed detection rate . . . . . . . . . . . . . .
4.6 AVIRIS Indian Pines hyperspectral image . . . . . . . . . . . . . . . . .
4.7 Confusion matrix: Lung . . . . . . . . . . . . . . . . . . . . . . . . . . .

73
73
74
74
75
86
87

xi

Acknowledgments

It is with the deepest sense of gratitude that I acknowledge the role of my advisor,
Professor Vishal Monga, in shaping this dissertation. His zeal for research has been
a constant source of inspiration.
I am indebted to him for his patient and relentless
eﬀorts towards nurturing me into what I am today. His inﬂuence has enriched every
aspect - adherence to rigorous standards of quality, clarity in technical writing, eﬀective
presentation and pedagogy - of my development as a researcher. I will fondly remember
his insightful observations about everything during our many conversations, and I look
forward to many more years of fruitful association.

I would be remiss not to recall the contributions of the late Professor Nirmal Bose,
who advised my MS thesis. I thank him for inculcating in me his philosophy of “good
theory for good practice.”

Words are inadequate to express my gratitude to my parents Srilatha and Srini-
vas, my grandparents Vishalakshi and Krishnamurthy, my sister Srividya, and extended
family for their unconditional love and unfailing support; I owe them everything. This
dissertation is my humble dedication to the memory of my late grandfather, whose ide-
als have profoundly inﬂuenced my upbringing and will continue to guide me. A special
thank-you to Sandhya for making the last few months enjoyable and the next many years
worth looking forward to.

I am thankful to the members of my committee, Professors Bob Collins, Bill Higgins,

and Ram Narayanan, for their feedback to improve the quality of this dissertation.

I consider myself fortunate to have been mentored by many wonderful collaborators
whose contributions have permeated my research: Professor Trac Tran of The Johns
Hopkins University; Dr. Raghu Raj, U.S. Naval Research Laboratory; Dr. Nasser
Nasrabadi, U.S. Army Research Laboratory; Dr. Jennifer Gille and Dr. Manu Par-
mar, Qualcomm MEMS Technologies; and Dr. Arthur Hattel and Dr. Bhushan Jayarao,
Animal Diagnostics Laboratory, Penn State. I thank them for their words of wisdom
and encouragement.

The Information Processing and Algorithms Laboratory (iPAL) has been my aca-
demic home during the last four years. I thank my wonderful lab mates Mu Li, Xuan
Mo, Bosung Kang, and Hojjat Mousavi for the many stimulating discussions on a vari-
ety of topics. I have also thoroughly enjoyed and beneﬁted from my collaboration with

xii

Professor Tran’s students at JHU: Yi Chen, Yuanming Suo, and Minh Dao.

Over the past six years, many friends have helped make my stay in State College
memorable. Special thanks to my roommate Harisha for being there always for me.
To Abhijith, Ashwath, Bhaskar, Gautham, Mahesh, Manjula, Niveditha, Shubha, and
Vikas, I will remain grateful for the wonderful company and friendship. I will cherish
our umpteen discussions - invariably accompanied by great food! - about things casual
and serious. To my friends from AID and the Raaga group, thank you all.

xiii

Dedication

To my parents, my grandparents, and all my teachers.

xiv

Chapter 1

Introduction

1.1 Motivation

Figure 1.1. Schematic of ﬁngerprint veriﬁcation.

Let us consider a scenario where a select group of individuals has access to a secure

facility, and their identity is authenticated using their ﬁngerprints. Fig. 1.1 describes the

steps involved in this process. A person places his/her ﬁnger on a scanning device, which

compares it with images of ﬁngerprints from a stored labeled database and decides to

either allow or deny access. This is just one example of image classiﬁcation, a commonly

encountered task in many domains of image processing today. Typically, in what is

referred to as a supervised setting, we have access to a collection of labeled training

images belonging to two or more perceptual classes or categories. The challenge lies

in utilizing the training data to learn an eﬀective automatic scheme of assigning a new

image to its correct category. Image classiﬁcation is employed in real-world applications

as diverse as disease identiﬁcation in medical imaging, video tracking, optical character

recognition, document classiﬁcation, biometric applications for security (ﬁngerprints or

faces), automatic target recognition in remote sensing, and many more.

Image classiﬁcation can be considered to be a two-stage process in general. First,

2

features that encapsulate image information are extracted from training data. The pro-

cess of feature extraction can be interpreted as a projection from the image space to a

feature space. It is desirable for such features to exhibit two characteristics:

1. Low dimensionality: High-dimensional data are commonly encountered in real-

world image classiﬁcation problems.

In the interest of computational eﬃciency,

feature extraction techniques condense image information into feature vectors of

much smaller dimension by exploiting the redundancy in image pixel intensity

information. For example, 2-D wavelet features oﬀer a compact multi-resolution

representation of images based on the observation that image content is mostly

sparse in the high spatial frequency regions.

2. Discriminability: The features are designed to capture information about a class

of images suﬃcient to distinguish it from images of other classes. This requires

a good understanding of the domain of application and the underlying imaging

physics. Diﬀerent sensing mechanisms and applications lend themselves to the

design of diﬀerent types of features. For example, the arches, loops and whorls

that characterize ﬁngerprints necessitate geometry-based image features that can

capture the speciﬁc curve patterns; eigenfaces derived from a principal component

analysis of the training image matrix have proved to be eﬀective for face recognition;

morphological image features have seen success in medical imaging problems.

In the second stage of image classiﬁcation, a decision engine (or equivalently, a classiﬁer)

is learned using training features from all classes. The feature vector corresponding to

a test image, whose class association is unknown, is evaluated using the classiﬁer rule

and the image is assigned to a particular class. A desirable property of such classiﬁers

is generalization, i.e. the ability to exhibit good classiﬁcation accuracy over a large set

of unknown test images.

From a detection-theoretic perspective, classiﬁcation can be cast as a hypothesis test-

ing problem. For simplicity of exposition, let us consider binary classiﬁcation, wherein

an image has to be assigned to one of two possible classes. Classical binary hypothesis

testing is the cornerstone of a variety of signal classiﬁcation problems. Based on the
observation of a single random vector xxx ∈ X n, we consider the two simple hypotheses

H0

H1

: xxx ∼ f (xxx|H0)
: xxx ∼ f (xxx|H1).

(1.1)

3

Traditionally, H0 and H1 are referred to as the null and alternative hypotheses respec-

tively. For continuous-valued variables, X ≡ R, while in the discrete-valued case, X is

a countable collection of indexed values. In the Bayesian testing scenario, the optimal

test compares the likelihood ratio L(xxx) to a threshold τ ,

L(xxx) :=

f (xxx|H1)
f (xxx|H0)

H1≷

H0

τ.

(1.2)

Classical hypothesis testing mandates that the true conditional densities f (xxx|·) are known
exactly. In most real-world classiﬁcation problems however, this assumption rarely holds.

One reason for this is the prevalence of high-dimensional data as discussed earlier. The

vectorized version of an image of size 200× 200 lies in R40000. Even for the case of binary
images, where each pixel assumes the value 0 or 1, there are over 1012000 possible diﬀerent
images. In practice, we usually have access only to a much smaller set of labeled training
images {(xxx1, y1), (xxx2, y2), . . . , (xxxT , yT )}, where each xxxi ∈ X n and each yi is a binary label;
for example, yi ∈ {−1, +1}. Class decisions are made using (often inaccurate) empirical
estimates of the true densities learned from available training. This problem persists even

when low-dimensional image features are considered instead of entire images. Training

insuﬃciency is thus an important concern in image classiﬁcation.

Further, the acquired images are often corrupted by various types of noise native

to the sensing mechanism. Examples include optical limitations in digital cameras and

speckle in ultrasound sensors. Some unexpected sources of noise or image distortions

are not always incorporated into the classiﬁcation framework. The minutiae features in

ﬁngerprints can get distorted with aging or injury. As a diﬀerent example, the exemplar

images used for training in face recognition are usually captured under good illumination

and are well-aligned. A test image however could include the face of the same individual

wearing sunglasses or a hat, or the test image could be captured in poor lighting. These

illustrations highlight the need for classiﬁcation schemes which incorporate a notion of

robustness to various types of uncertainty for eﬀective application in practical problems.

In this dissertation, I present new theoretical ideas and experimental validation to

support the following thesis:

Discriminative models that exploit signal structure can oﬀer beneﬁts of robustness to

noise and training insuﬃciency in image classiﬁcation problems.

4

1.2 Overview of Dissertation Contributions

This dissertation explores the design of discriminative models for robust image classiﬁca-

tion. The diﬀerent models proposed here are uniﬁed by the common goal of identifying

and leveraging the discriminative structure inherent in the images or feature represen-

tations thereof. By structure, we refer to the relationships (deterministic or stochastic)

between the variables and, more importantly, the existence of discriminative information

in such variables that is most relevant for classiﬁcation tasks. Speciﬁcally, we develop two

diﬀerent families of discriminative models for image classiﬁcation using ideas founded in

probabilistic graphical models and the theory of sparse signal representation.

How does discriminative structure manifest itself in real-world image classiﬁcation

tasks? In spite of a proliferation of techniques for feature extraction and classiﬁer de-

sign, consensus has evolved that no single feature set or decision engine is uniformly

superior for all image classiﬁcation tasks. In parallel, ongoing research in decision fusion

and feature fusion has unearthed the potential of such schemes to infuse robustness into

classiﬁcation by mining the diversity of available information. Motivated by this, a recur-

ring theme in this dissertation is to develop robust discriminative models by exploiting

structure in the complementary yet correlated information from diﬀerent feature sets.

1.2.1 Discriminative Graphical Models

In the ﬁrst part of this dissertation, we pose the following question: Given a collection

of multiple feature sets pertaining to the same observed images, (how) can we learn

their class-conditional dependencies to improve classiﬁcation performance over schemes

that use a subset of these features? In response, we propose a framework for feature fu-

sion using discriminative graphical models. Graphical models oﬀer a tractable means of

learning models for high-dimensional data eﬃciently by capturing dependencies crucial
to classiﬁcation in a reduced complexity set-up. A graph G = (V,E) is deﬁned by a set of

nodes V = {v1, . . . , vn} and a set of (undirected) edges E ⊂(cid:0)V

pairs of nodes. A probabilistic graphical model is obtained by deﬁning a random vec-
tor on G such that each node represents one (or more) random variables and the edges
reveal conditional dependencies. This marriage of graph theory and probability theory

2(cid:1), i.e. the set of unordered

oﬀers an intuitive visualization of a probability distribution from which conditional de-

pendence relations can be easily identiﬁed. While we focus only on undirected graphs in

this dissertation, directed graphs also are an active area of research. The use of graph-

ical models also enables us to draw upon the rich resource of eﬃcient graph-theoretic

5

algorithms to learn complex models and perform inference. Graphical models have thus

found application in a variety of tasks, such as speech recognition, computer vision, sen-

sor networks, biological modeling, artiﬁcial intelligence, and combinatorial optimization.

A more elaborate treatment of the topic is available in [1, 2].

Our contribution builds on recent work in discriminative graph learning [3]. First,

for each type of features, we learn pairs of tree-structured graphs in a discriminative

manner. Now we have a collection of unconnected trees for each hypothesis, equivalent

to the scenario of na¨ıve Bayes classiﬁcation which assumes statistical independence of

the feature sets. Next, we iteratively learn multiple trees on the larger graphs formed

by concatenating all nodes from each hypothesis. Crucially, the new edges learned in

each iteration capture conditional dependencies across feature sets. By learning simple

tree graphs in each iteration and accumulating all edges in the ﬁnal graphical structure,

we learn a dense edge graphical structure which encodes discriminative information in a

tractable manner. It must be noted that this framework makes minimal assumptions on

the feature extraction process - speciﬁcally, that the feature sets must be correlated. An

important consequence of mining the conditional dependencies using discriminative trees

is the relative robustness of classiﬁcation performance as the size of the available training

set reduces. As we shall demonstrate through experimental validation, this addresses an

important practical concern.

1.2.2 Discriminative Sparse Representations

It is well-known that a large class of signals, including audio and images, can be expressed

naturally in a compact manner with respect to well-chosen basis representations. Among

the most widely applicable of such basis representations are the Fourier and wavelet basis.

This has inspired a proliferation of applications that involve sparse signal representations

for acquisition [4], compression [5], and modeling [6]. The central problem in compressive

sensing (CS) is to recover a signal xxx ∈ Rn given a vector of linear measurements yyy ∈ Rm
of the form yyy = AAAxxx, where m (cid:28) n. Assuming xxx is compressible, it can be recovered from
this underdetermined system of equations by solving the following problem [4]:

xxx (cid:107)xxx(cid:107)0 subject to yyy = AAAxxx,
min

(1.3)

where (cid:107)xxx(cid:107)0 is the l0-“norm” that counts the number of non-zero entries in xxx. Under
certain conditions, the l0-norm can be relaxed to the l1-norm, leading to convex opti-

mization formulations.

6

Although the concept of sparsity was introduced to solve inverse reconstructive prob-

lems, where it acts as a strong prior to the abbreviated ill-posed nature of the problems,

recent work [7, 8] has demonstrated the eﬀectiveness of sparse representation in classiﬁ-

cation applications too. The crucial observation is that a test image can be reasonably

approximated as a linear combination of training images belonging to the same class,

with (ideally) no contributions from training images of other classes. Therefore, with

AAA := [AAA1 . . . AAAK] and AAAi representing the matrix of vectorized training images from
the i-th class, the corresponding coeﬃcient vector xxx := [xxxT
K]T is sparse and nat-
urally encodes discriminative information. In other words, the semantic information of

1 . . . xxxT

the signal of interest is often captured in the sparse representation. Albeit simplistic in

formulation, this linear sparse representation model is rooted in well-conditioned opti-

mization theory. The sparse representations exhibit robustness to a variety of real-world

image distortions, leading to their widespread use in applications such as face recognition,

remote sensing and medical image classiﬁcation for disease diagnosis.

In pursuit of our goal of robust classiﬁcation, a natural progression of thought is

to inquire if this sparse representation-based classiﬁcation (SRC) framework can be

extended to the scenario of multiple correlated observations of a given test image.

Eﬀorts have already been directed towards collaborative (or alternately discrimina-

tive/group/simultaneous) sparse models for sensing and recovery as well as classiﬁcation.

In practice, the measurements could come from homogeneous sensors - for example, mul-

tiple camera views or remote sensing arrays used in hyperspectral or radar imaging. An

emerging area of research interest is multi-sensor fusion, where the data/features are ac-

cumulated using heterogeneous sensing modalities. To illustrate, multi-modal biometrics

for security applications could combine ﬁngerprint veriﬁcation as well as face recogni-

tion. While the observations are still correlated since they pertain to the same image,

new notions of signal structure and correlation emerge.

Accordingly, the second part of this dissertation concerns itself with innovative

ways of mining structural dependencies among discriminative sparse signal represen-

tations for classiﬁcation. First, we develop a multi-task multi-variate model for sparse

representation-based classiﬁcation that is applicable for a variety of multi-modal fusion

applications. As a speciﬁc instantiation of the framework, we consider the problem of

categorizing histopathological (medical tissue) images as either healthy or diseased (in-

ﬂammatory). The tissue staining process unique to histopathology leads to digitized

images that encode class information in the red and blue color channels. So we develop

a simultaneous sparsity model for color images which exploits the color channel correla-

7

tions. The sparse coeﬃcient matrices have a block-diagonal structure due to meaningful

constraints imposed by imaging physics. We propose a variation of a well-known greedy

algorithm to recover this new sparse structure.

A caveat for the success of sparse classiﬁcation methods is the requirement of abun-

dant training information; the linear combination model will not hold well otherwise. On

the other hand, many practical situations suﬀer from the limitation of limited training.

So, we revisit the SRC framework from a Bayesian standpoint. It is well known that

a priori knowledge about the structure of signals often leads to signiﬁcant performance

improvements in many signal analysis and processing applications. Such information

typically manifests itself in the form of priors, constraints or regularizers in analyti-

cal formulations of the problems. In fact, the l1-norm variant of (3.1) is equivalent to
enforcing a sparsity-inducing Laplacian prior on the coeﬃcients xxx. Taking this idea fur-

ther, we investigate new optimization problems resulting from the enforcement of other

sparsity-inducing distributions such as the spike-and-slab prior. Signiﬁcantly, we look

for discriminative graphical priors that can simultaneously encode signal structure and

sparsity. Using graphical priors in a Bayesian set-up alleviates the burden on training set

size for sparsity-based classiﬁcation methods. Our eﬀorts in this direction are inspired by

ongoing work towards uncovering fundamental relationships between graphical models

and sparse representations [9].

1.3 Organization

A snapshot of the main contributions of this dissertation is presented next. Publications

related to the contribution in each chapter are also listed where applicable.

In Chapter 2, the primary contribution is the proposal of probabilistic graphical

models as a tool for low-level feature fusion and classiﬁcation. The algorithm can be ap-

plied broadly to any fusion-based classiﬁcation problem and is described in all generality.

We also show a speciﬁc application to the problem of automatic target recognition (ATR)

using synthetic aperture radar (SAR) images. Model-based algorithmic approaches to

target classiﬁcation [10, 11] attempt to learn the underlying class-speciﬁc statistics for

each class of target vehicles. This is clearly not an easy problem since real-world dis-

tortions lead to signiﬁcant deviations from the assumed ideal model. Also the eﬀects of

limited training are particularly pronounced when working with high-dimensional data

typical of ATR problems. We leverage a recent advance in discriminative graphical model

learning [3] to overcome these issues. The novelty of our contribution is in building a

8

feature fusion framework for ATR, which explicitly learns class-conditional statistical

dependencies between distinct feature sets. To the best of our knowledge, this is the ﬁrst

such application of probabilistic graphical models in ATR.

As for target image representations or feature sets, we employ well-known wavelet

LL, LH, and HL sub-bands (H = high, L = low). Usually the LL sub-band wavelet

coeﬃcients are used as features, since they capture most of the low-frequency content

in the images. Although largely ignored in ATR problems, the LH and HL coeﬃcients

carry high frequency discriminative information by capturing scene edges in diﬀerent

orientations. We utilize this discriminative aspect of the LH and HL sub-bands together

with the approximate (coarse) information from the LL sub-band as the sources of com-

plementary yet correlated information about the target scene.

A signiﬁcant experimental contribution of this work is the comparison of classiﬁca-

tion performance as a function of the size of the training set. Traditionally classiﬁcation

performance is reported in terms of confusion matrices and receiver operating character-

istic (ROC) curves. We believe that a more relevant and necessary comparison is with

the number of training samples available per class, which is often a serious practical

concern in ATR systems. In comparison with state-of-the-art alternatives, we show that

our graphical model framework exhibits a more graceful decay in performance under

reduced training, thereby highlighting its superior robustness.

This material was presented at the 2011 IEEE International Conference on Image

Processing [12] and will appear shortly in the IEEE Transactions on Aerospace and

Electronic Systems [13].

Chapter 3 serves as an introduction to sparse signal representations and their role in

discriminative tasks. In joint sparsity models for classiﬁcation, one assumption is that all

test vectors obey the exact same linear representation model, leading to coeﬃcient vec-

tors with identical sparse structure but possibly diﬀerent weights. Motivation for such an

assumption is often derived from the underlying imaging physics. Since sparse represen-

tations are known to be inherently discriminative, these sparsity model-based approaches

perform class assignment using the class-speciﬁc reconstruction error, although the orig-

inal task is one of classiﬁcation. We investigate the performance beneﬁts of using truly

discriminative classiﬁers on sparse features in this chapter. Speciﬁcally, in continuation

of the graph-based framework from Chapter 2, we present two instantiations of learning

such graphical models directly on sparse image features. These experimental demon-

strations oﬀer validation of the eﬀectiveness of training graphs on sparse features. This

work was done in collaboration with Prof. Trac Tran at the Johns Hopkins University,

9

Baltimore, MD.

The ﬁrst application is to hyperspectral target image detection and classiﬁcation.

Spatio-spectral information is fused for robust classiﬁcation by exploiting the correlations

among sparse representations learned from local pixel neighborhoods. This material

was presented at the 2012 IEEE International Symposium on Geoscience and Remote

Sensing [14] and was published in the IEEE Geoscience and Remote Sensing Letters [15]

in May 2013.

The second application is the well-known problem of face recognition. Recognizing

the potential of local image features to encode discriminative information more robustly

than global image features, we extract sparse features separately from the eyes, nose,

and mouth of human subjects in training images and combine this information via dis-

criminative trees. This material was presented at the 2011 IEEE Asilomar Conference

on Signals, Systems and Computers [16].

On a related note, we employed the graph-based framework for transient acoustic

signal classiﬁcation in collaboration with the U.S. Army Research Laboratory, Adelphi,

MD. This work is however not included in this dissertation. The material was presented

at the 2013 IEEE International Conference on Acoustics, Speech, and Signal Processing

[17] and submitted to the IEEE Transactions on Cybernetics [18] in April 2013.

Chapter 4 discusses innovative ways of learning discriminative models on sparse

signal representations. First, we develop a simultaneous sparsity model for color medical

image classiﬁcation. The motivation for this work originated from discussions with vet-

erinary pathologists at the Animal Diagnostic Laboratory (ADL), Pennsylvania State

University. Digitized tissue images from mammalian tissue - renal, splenic and pul-

monary - were provided along with the results of classiﬁcation by human observers as a

baseline. The multi-channel nature of digital histopathological images presents an oppor-

tunity to exploit the correlated color channel information for better image modeling. We

develop a new simultaneous Sparsity model for multi-channel Histopathological Image

Representation and Classiﬁcation (SHIRC). Essentially, we represent a histopathological

image as a sparse linear combination of training examples under suitable channel-wise

constraints. Classiﬁcation is performed by solving a newly formulated simultaneous

sparsity-based optimization problem. A practical challenge is the correspondence of im-

age objects (cellular and nuclear structures) at diﬀerent spatial locations in the image.

We propose a robust locally adaptive variant of SHIRC (LA-SHIRC) to tackle this issue.

This material was presented at the 2013 IEEE International Symposium on Biomedical

Imaging [19] and has been submitted to the IEEE Transactions on Image Processing [20]

10

in April 2013.

Next, we present the results of our recent and ongoing explorations into the design of

structured graphical priors for sparsity-based classiﬁcation. Model-based CS exploits the

structure inherent in sparse signals for the design of better signal recovery algorithms.

Analogously, our contribution is a logical extension of the sparse representation-based

classiﬁcation idea to discriminative models for structured sparsity. We incorporate class-

speciﬁc dictionaries in conjunction with discriminative class-speciﬁc priors. Speciﬁcally,

we use the spike-and-slab prior that has widely been acknowledged to be the gold stan-

dard for sparse signal modeling. Theoretical analysis reveals similarities to well-known

regularization-based formulations such as the lasso and the elastic net. Crucially, our

Bayesian approach alleviates the requirement of abundant training necessary for the

success of sparsity-based schemes. We also show that the framework organically evolves

in complexity for multi-task classiﬁcation while maintaining parallels with collaborative

hierarchical schemes. The material was presented at the 2013 IEEE International Con-

ference on Acoustic, Speech, and Signal Processing [21], will be presented at the 2013

IEEE International Conference on Image Processing [22], and has been submitted to the

IEEE Transactions on Image Processing [23] in July 2013.

In Chapter 5, the main contributions of this dissertation are summarized. We

also outline interesting directions for future research that can combine the robustness

beneﬁts of graphical models and sparse representations in a synergistic manner for image

classiﬁcation applications.

Chapter 2

Feature Fusion for Classiﬁcation via
Discriminative Graphical Models

2.1

Introduction

The primary contribution of this chapter is the proposal of probabilistic graphical models

as a tool for low-level feature fusion in image classiﬁcation problems. We are interested

in scenarios where we have access to multiple sets of features corresponding to the same

observed images. We argue that explicitly capturing statistical dependencies between

distinct low-level image feature sets can improve classiﬁcation performance compared to

existing fusion techniques. Accordingly, we develop a two-stage framework to directly

model dependencies between diﬀerent feature representations of a given collection of

images. The ﬁrst stage involves the generation of multiple image representations (or fea-

ture sets) that carry complementary beneﬁts for image classiﬁcation. In the second stage,

we perform inference (class assignment) that can exploit class-conditional correlations

among the aforementioned feature sets. This is a reasonably hard task as images (or

representative features thereof) are typically high-dimensional and the number of train-

ing images corresponding to a target class is limited. We address this problem by using

discriminative graphical models. Our graphical model-based classiﬁers are designed by

ﬁrst learning discriminative tree graphs on each distinct feature set. These initial disjoint

trees are then thickened, i.e. augmented with more edges to capture feature correlations,

via boosting on disjoint graphs.

The proposed framework is quite general in its applicability to a variety of real-

world fusion problems.

In this chapter, we demonstrate the experimental beneﬁts of

our proposed approach for the problem of automatic target recognition using synthetic

12

aperture radar images.

2.2 Background

2.2.1 Probabilistic Graphical Models

Probabilistic graphical models present a tractable means of learning models for high-

dimensional data eﬃciently by capturing dependencies crucial to the application task
in a reduced complexity set-up. A graph G = (V,E) is deﬁned by a set of nodes V =

{v1, . . . , vn} and a set of (undirected) edges E ⊂(cid:0)V

2(cid:1), i.e. the set of unordered pairs of

nodes. Graphs vary in structural complexity from sparse tree graphs to fully-connected

dense graphs. A probabilistic graphical model is obtained by deﬁning a random vector
on G such that each node represents one (or more) random variables and the edges
reveal conditional dependencies. The graph structure thus deﬁnes a particular way

of factorizing the joint probability distribution. Graphical models oﬀer an intuitive

visualization of a probability distribution from which conditional dependence relations

can be easily identiﬁed. The use of graphical models also enables us to draw upon the

rich resource of eﬃcient graph-theoretic algorithms to learn complex models and perform

inference. Graphical models have thus found application in a variety of modeling and

inference tasks such as speech recognition, computer vision, sensor networks, biological

modeling, and artiﬁcial intelligence. The interested reader is referred to [1, 2] for a more

elaborate treatment of the topic.

In the ensuing discussion, we assume a binary classiﬁcation problem for simplicity

of exposition. The proposed algorithm naturally extends to the multi-class classiﬁcation

scenario by designing discriminative graphs in a one-versus-all manner (discussed in

Section 2.3.4). The history of graph-based learning can be traced to Chow and Liu’s [24]

seminal idea of learning the optimal tree approximation ˆp of a multivariate distribution

p using ﬁrst- and second-order statistics:

ˆp = arg min
pt∈Tp

D(p||pt),

(2.1)

where D(p||pt) = Ep[log(p/pt)] denotes the Kullback-Leibler (KL) divergence and Tp is
the set of all tree-structured approximations to p. This optimization problem is shown

to be equivalent to a maximum-weight spanning tree (MWST) problem, which can be

solved using Kruskal’s algorithm [25] for example. The mutual information between a

13

pair of random variables is chosen to be the weight assigned to the edge connecting

those random variables in the graph. This idea exempliﬁes generative learning, wherein

a graph is learned to approximate a given distribution by minimizing a measure of

approximation error. This approach has been extended to a classiﬁcation framework by

independently learning two graphs to respectively approximate the two class empirical

estimates, and then performing inference. A decidedly better approach is to jointly learn

a pair of graphs from the pair of empirical estimates by minimizing classiﬁcation error

in a discriminative learning paradigm. An example of such an approach is [26].

Recently, Tan et al. [3] proposed a graph-based discriminative learning framework,

based on maximizing an approximation to the J-divergence. Given two probability
distributions p and q, the J-divergence is deﬁned as: J(p, q) = D(p||q) + D(q||p). This is
a symmetric extension of the KL-divergence. The tree-approximate J-divergence is then

deﬁned [3] as

ˆJ(ˆp, ˆq; p, q) =(cid:90) (p(x) − q(x)) log(cid:20) ˆp(x)

ˆq(x)(cid:21) dx,

(2.2)

and it measures the “separation” between tree-structured approximations ˆp (∈ Tp) and ˆq
(∈ Tq). Using the key observation that maximizing the J-divergence minimizes the upper
bound on probability of classiﬁcation error, the discriminative tree learning problem is

stated (in terms of empirical estimates ˜p and ˜q) as

(ˆp, ˆq) = arg max

ˆp∈T(cid:101)p,ˆq∈T(cid:101)q

ˆJ(ˆp, ˆq; ˜p, ˜q).

(2.3)

It is shown in [3] that this optimization further decouples into two MWST problems (see

Fig. 2.1):

ˆp = arg min

ˆq = arg min

ˆp∈T(cid:101)p
ˆq∈T(cid:101)q

D(˜p||ˆp) − D(˜q||ˆp)
D(˜q||ˆq) − D(˜p||ˆq).

(2.4)

(2.5)

Learning thicker graphical models: The optimal tree graphs in the sense of min-

imizing classiﬁcation error are obtained as solutions to (2.4)-(2.5). Such tree graphs

have limited ability to model general distributions; learning arbitrarily complex graphs

with more involved edge structure is known to be NP-hard [27], [28]. In practice, one

way of addressing this inherent tradeoﬀ between complexity and performance is to boost

simpler graphs [3, 29, 30] into richer structures. In this chapter, we employ the boosting-

based algorithm from [3], wherein diﬀerent pairs of discriminative graphs are learned -

14

Figure 2.1.

Illustration of (2.4)-(2.5) (ﬁgure reproduced from [3]). T(cid:101)p is the subset of tree
(cid:98)pCL, is the projection of (cid:101)p onto T(cid:98)p, according to (2.1). The discriminatively-learned distribution
distributions marginally consistent with(cid:101)p. The generatively-learned distribution (via Chow-Liu)
ˆpDT, is the solution of (2.4) which is “further” from (cid:101)q in the KL-divergence sense.

by solving (2.4)-(2.5) repeatedly - over the same sets of nodes (but weighted diﬀerently)

in diﬀerent iterations via boosting. This process results in a “thicker” graph formed by

augmenting the original tree with the newly-learned edges from each iteration.

2.2.2 Boosting

Boosting [31], which traces its roots to the probably approximately correct (PAC) learn-

ing model, iteratively improves the performance of weak learners which are marginally

better than random guessing into a classiﬁcation algorithm having arbitrarily accurate

performance. A distribution of weights is maintained over the training set.

In each

iteration, a weak learner ht minimizes the weighted training error to determine class

conﬁdence values. Weights on incorrectly classiﬁed samples are increased so that the

weak learners are penalized for the harder examples. The ﬁnal boosted classiﬁer makes

a class decision based on a weighted average of the individual conﬁdence values. In the

Real-AdaBoost version, each ht determines class conﬁdence values instead of discrete

class decisions; it is described in Algorithm 1. S is the set of training features and N is

the number of available training samples.

Boosting has been used in a variety of practical applications as a means of enhancing

the performance of weak classiﬁers. In [32] for example, radial basis function (RBF) nets

are used as the weak learners for boosting. Boosting has also been deployed on classiﬁers

derived from decision trees [29]. In the sequel, we use boosting as a tool to combine

initially disjoint discriminative tree graphs, learned from distinct feature representations

of a given image, into a thicker graphical model which captures correlation among the

diﬀerent target image representations.

Algorithm 1 AdaBoost learning algorithm
1: Input data (xi, yi), i = 1, 2, . . . , N , where xi ∈ S, yi ∈ {−1, +1}
2: Initialize D1(i) = 1
3: For t = 1, 2, . . . , T :

N , i = 1, 2, . . . , N

15

• Train weak learner using distribution Dt
• Determine weak hypothesis ht : S (cid:55)→ R with error t
• Choose βt = 1
• Dt+1(i) = 1

2 ln(cid:16) 1−t
t (cid:17)
4: Output decision H(x) = sign(cid:104)(cid:80)T

Zt {Dt(i) exp(−βtyiht(xi))}, where Zt is a normalization factor

t=1 βtht(x)(cid:105).

2.3 Discriminative Graphical Models for Robust Image

Classiﬁcation

The schematic of our framework is shown in Fig. 2.2. The idea is ﬁrst illustrated for

binary classiﬁcation. Extension to the more general multi-class scenario is discussed in

2.3.4. The graphical model-based algorithm is summarized in Algorithm 2, and it consists

of an oﬄine stage to learn the discriminative graphs (Steps 1-4) followed by an online

stage (Steps 5-6) where a new test image is classiﬁed. The oﬄine stage involves extrac-

tion of features from training images from which approximate probability distribution

functions (pdfs) for each class are learned after the graph thickening procedure.

2.3.1 Feature Extraction
The (vectorized) images are assumed to belong to Rn. The extraction of diﬀerent sets of
features from training images is performed in Stage 1. Each such image representation

may be viewed as a dimensionality reduction via projection Pi : Rn (cid:55)→ Rmi to some lower
dimensional space Rmi, mi < n. In our framework, we consider M distinct projections
Pi, i = 1, 2, . . . , M . For every input n-dimensional image, M diﬀerent features yyyi ∈
Rmi, i = 1, 2, . . . , M , are obtained. Fig. 2.2(b) depicts this process for the particular
case M = 3. For notational simplicity, in the ensuing description we assume m1 = m2 =

. . . = mM = m. The framework only requires that the diﬀerent projections lead to low-

level features that have complementary yet correlated information. For the application to

target recognition described in the latter half of this chapter, wavelet sub-bands obtained

after a multi-level wavelet decomposition are used as features.

16

Figure 2.2. Proposed two-stage framework for designing discriminative graphical models Gp
and Gq illustrated for the case when M = 3. (a) Sample target image, (b) Feature extraction
via a projection P, (c) Initial sparse graph, (d) Final thickened graph: newly learned edges
represented by dashed lines, (e) Graph-based inference. In (c)-(e), the corresponding graphs for
distribution q are not shown but are learned analogously.

2.3.2

Initial Disjoint Tree Graphs

Figs. 2.2(c)-(e) represent Stage 2 of the framework. Consistent with notation in [3], we
denote the two diﬀerent class distributions by p and q respectively. Let {yyyp
i}tr represent
the set of training feature vectors corresponding to projection Pi and class p; similarly
deﬁne {yyyq
i}tr. Further, let the class distributions corresponding to p and q for the i-th
set of features be denoted by f p(yyyi) and f q(yyyi) respectively.

Algorithm 2 Discriminative graphical models (Steps 1-4 oﬄine)
1: Feature extraction (training): Obtain vectors yyyi, i = 1, . . . , M in Rm using pro-

jections Pi, i = 1, . . . , M on target image in Rn

2: Initial disjoint graphs:

17

For i = 1, . . . , M :

i and Gq

i from the vectors {yyyi}

3: Concatenate nodes of the graphs Gp
i , i = 1, . . . , M to generate initial graph Gp,0;
4: Boosting on disjoint graphs: Iteratively thicken Gp,0 and Gq,0 via boosting to

Discriminatively learn m-node tree graphs Gp
likewise for Gq,0
obtain ﬁnal graphs Gp and Gq
{Online process}
image

5: Feature extraction (test): Obtain vectors yyyi, i = 1, . . . , M in Rm from test target

6: Inference: Classify based on output of the resulting strong classiﬁer using (2.7).

i and Gq

i

1 ,Gp

2 and Gp

3 . (The corresponding graphs Gq

A pair of m-node discriminative tree graphs Gp

is learned for each feature
projection Pi, i = 1, 2, . . . , M , by solving (2.4)-(2.5). Fig. 2.2(c) shows a toy example of
three 4-node tree graphs Gp
i are not shown
in the ﬁgure.) By concatenating the nodes of the graphs Gp
i , i = 1, . . . , M, we have one
initial sparse graph structure Gp,0 with M m nodes (Fig. 2.2(c)). Similarly, we obtain the
initial graph Gq,0 by concatenating the nodes of the graphs Gq
i , i = 1, . . . , M . The joint
probability distribution corresponding to Gp,0 is the product of the individual probability
distributions corresponding to Gp
i , i = 1, . . . , M ; likewise for Gq,0. Inference based on the
graphs Gp,0 and Gq,0 can thus be interpreted as feature fusion under the na¨ıve Bayes
assumption, i.e. statistical independence of the individual target image representations.
We have now learned (graphical) pdfs ˆf p(yyyi) and ˆf q(yyyi) (i = 1, . . . , M ).

Our ﬁnal goal is to learn probabilistic graphs for classiﬁcation purposes which do

not just model the individual feature projections but also capture their mutual class-

conditional correlations. This is tantamount to discovering new edges that connect

features from the aforementioned three individual tree graphs on the distinct feature

sets. Thicker graphs or newer edges can be learned with the same training sets by

boosting several simple graphs [3, 29–31]. In particular, we employ boosting of multiple

discriminative tree graphs as described in [3]. Crucially, as the initial graph for boosting,

we choose the forest of disjoint graphs over the individual feature sets. Such initialization

has two beneﬁts:

1. When the number of training samples is limited, each individual graph in the forest

is still well-learned over lower-dimensional feature sets and the forest graph oﬀers

a good initialization for the ﬁnal discriminative graph to be learned iteratively.

18

2. This naturally oﬀers a relatively general way of low-level feature fusion, where the

initial disjoint graphs could be determined from domain knowledge or learned using

any eﬀective graph learning technique (not necessarily tree-structured).

2.3.3 Feature Fusion via Boosting on Disjoint Graphs

We next iteratively thicken the graphs Gp,0 and Gq,0 using the boosting-based method
in [3]. In each iteration, the classiﬁer ht is a likelihood test using the tree-based approx-
imation of the conditional pdfs. We begin with the set of training features {yyyp
i}tr and
{yyyq
i}tr, i = 1, . . . , M , and obtain the empirical error t for t = 1 as the fraction of training
samples that are misclassiﬁed. We then compute the parameter β1, which varies directly

as 1. Finally, we identify the misclassiﬁed samples and re-sample the training set in

a way that the re-sampled (or re-weighted) training set now contains a higher number

of such samples. This is captured by the distribution Dt+1 in Algorithm 1. After each

such iteration (indexed by t in Algorithm 1), a new pair of discriminatively-learned tree
graphs Gp,t and Gq,t is obtained by modifying the weights on the existing training sets.
The ﬁnal graph Gp after T iterations is obtained by augmenting the initial graph Gp,0
with all the newly learned edges in the T iterations (Fig. 2.2(d)). Graph Gq is obtained
in a similar manner. A simple stopping criterion is devised to decide the number of

iterations based on the value of the approximate J-divergence at successive iterations.

This process of learning new edges is tantamount to discovering new conditional

correlations between distinct sets of image features, as illustrated by the dashed edges
in Fig. 2.2(d). The thickened graphs ˆf p(yyy) and ˆf q(yyy) are therefore estimates of the
true (but unknown) class-conditional pdfs over the concatenated feature vector yyy. If E p,t
represents the set of edges learned for distribution p in iteration t and E p represents the
edge set of Gp, then

E p =

E p,t;

E q =

E q,t.

(2.6)

T(cid:91)t=0

T(cid:91)t=0

Inference: The graph learning procedure described so far is performed oﬄine. The

actual classiﬁcation of a new test image is performed in an online process, via the likeli-

hood ratio test using the estimates of the class-conditional pdfs obtained above, and for

a suitably chosen threshold τ ,

ˆf q(yyy)(cid:33) p
log(cid:32) ˆf p(yyy)

≷

q

τ.

(2.7)

A note on inference complexity: The initial graph has M (m − 1) edges, and each
iteration of boosting adds at most mM−1 new edges. Making inferences from the learned

19

graph after T boosting iterations requires the multiplication of about 2mM T conditional

probability densities corresponding to the edges in the two graphs. This is comparable to

the cost of making inferences using classiﬁers such as support vector machines (SVMs).
In comparison, inference performed from likelihood ratios by assuming a Gaussian mM×
mM covariance matrix requires O(m3M 3) computations due to matrix inversion.

2.3.4 Multi-class Image Classiﬁcation

The proposed framework can be extended to a multi-class scenario in a one-versus-all

k-th binary classiﬁcation problem is then concerned with classifying a query image (or

manner as follows. Let Ck, k = 1, 2, . . . , K, denote the k-th class of images, and let (cid:101)Ck
denote the class of images complementary to class Ck, i.e., (cid:101)Ck =(cid:83)l=1,...,K,l(cid:54)=k Cl. The
corresponding feature) into Ck or (cid:101)Ck (k = 1, . . . , K).

k (yyy) and
ˆf q
k (yyy) as described previously. This process is done in parallel and oﬄine. The image
feature vector corresponding to a new test image is assigned to the class k∗ according to
the following decision rule:

For each such binary problem, we learn graphical estimates of the p.d.fs ˆf p

∗

k

= arg max

k∈{1,...,K} log(cid:32) ˆf p

k (yyy)(cid:33) .

k (yyy)
ˆf q

(2.8)

Inclusion of new image class: Of practical interest is the ﬂexibility of the algorithm

to incorporate training data from a new target class. The inclusion of the (K +1)-th class

will require just one more pair of graphs to be learned - corresponding to the problem of

the test phase. It must be noted that the features corresponding to the (K + 1)-th class

oﬄine training phase itself, thereby incurring minimal additional computation during

classifying a query into either CK+1 or (cid:101)CK+1. Crucially, these graphs are learned in the
are not used for training the complementary classes (cid:101)Ck in the Ck-versus-(cid:101)Ck problems for

k = 1, 2, . . . , K. Incorporating these features into the training process of all the (K + 1)

binary problems can lead to better discrimination but it will require a re-learning (oﬄine)

of all K pairs of graphs.

20

2.4 Application: Automatic Target Recognition

2.4.1

Introduction

The classiﬁcation of real-world empirical targets using sensed imagery into diﬀerent per-

ceptual classes is one of the most challenging algorithmic components of radar systems.

This problem, popularly known as automatic target recognition (ATR), exploits im-

agery from diverse sensing sources such as synthetic aperture radar (SAR), inverse SAR

(ISAR), and forward-looking infra-red (FLIR) for automatic identiﬁcation of targets. A

review of ATR can be found in [33].

SAR imaging oﬀers the advantages of day-night operation, reduced sensitivity to

weather conditions, penetration capability through obstacles, etc. Some of the earlier

approaches to SAR ATR can be found in [34–38]. A discussion of SAR ATR theory

and algorithms is provided in [39]. The Moving and Stationary Target Acquisition

and Recognition (MSTAR) data set [40] is widely used as a benchmark for SAR ATR

experimental validation and comparison. Robustness to real-world distortions is a highly

desirable characteristic of ATR systems, since targets are often classiﬁed in the presence

of clutter, occlusion and shadow eﬀects, diﬀerent capture orientations, confuser vehicles,

and in some cases, diﬀerent serial number variants of the same target vehicle. Typically

the performance of ATR algorithms is tested under a variety of operating conditions as

discussed in [39].

2.4.2 ATR Algorithms: Prior Art

Over two decades’ worth of investigations have provided a rich family of algorithmic tools

for target classiﬁcation. Early research in ATR mainly concerned itself with the task of

uncovering novel feature sets. Choices for features have been inspired by image processing

techniques in other application domains. Popular spatial features are computer vision-

based geometric descriptors such as robust edges and corners [41], and template classes

[35, 36, 42]. Selective retention of transform domain coeﬃcients based on wavelets [43]

and Karhunen-Loeve Transform [44], and estimation-theoretic templates [45] have also

been proposed. Likewise a wealth of decision engines has been proposed for target

classiﬁcation. Approaches range from model-based classiﬁers such as linear, quadratic

and kernel discriminant analysis [46], neural networks [47], to machine learning-based

classiﬁers such as SVM [34] and its variations [37] and boosting-based classiﬁers [32].

Here we use three distinct target image representations derived from wavelet basis

functions, which have been used widely in ATR applications [43, 48]. In particular, the

21

LL, LH and HL sub-bands obtained after a multi-level wavelet decomposition using 2-

D reverse biorthogonal wavelets [43] comprise the three sets of complementary features
{yyyi}, i = 1, 2, 3. The spectra of natural images are known to have a power law fall-oﬀ,
with most of the energy concentrated at low frequencies. The LL sub-band encapsulates

this low frequency information. On the other hand, the HL and LH sub-bands encode

discriminatory high frequency information. Our assumption about the complementary

nature of target image representations is still valid for this choice of basis functions.

The design of composite classiﬁers for ATR is an area of active research interest.

Paul et al. [49] combine outputs from eigen-template based matched ﬁlters and hidden

Markov models (HMM)-based clustering using a product-of-classiﬁcation-probabilities

rule. More recently, Gomes et al. [50] proposed simple voting combinations of individual

classiﬁer decisions. Invariably, the aforementioned methods use educated heuristics in

combining decisions from multiple decision engines while advocating the choice of a ﬁxed

set of features. In [51], a comprehensive review of popular classiﬁcation techniques is

provided in addition to useful theoretical justiﬁcations for the improved performance of

such ensemble classiﬁers. In [32], the best set of features is adaptively learned from a

collection of two diﬀerent types of features. We have also proposed a two-stage meta-

classiﬁcation framework [52], wherein the vector of ‘soft’ outputs from multiple classiﬁers

is interpreted as a meta-feature vector and fed to a second classiﬁcation stage to obtain

the ﬁnal class decision.

Related research in multi-sensor ATR [53–55] has investigated the idea of combining

diﬀerent “looks” of the same scene for improved detection and classiﬁcation. The avail-

ability of accurately geo-located, multi-sensor data has created new opportunities for

the exploration of multi-sensor target detection and classiﬁcation algorithms. Perlovsky

et al. [56] exploit multi-sensor ATR information to distinguish between friend and foe,

using a neural network-based classiﬁer. In [57], a simple non-linear kernel-based fusion

technique is applied to SAR and hyper-spectral data obtained from the same spatial

footprint for target detection using anomaly detectors. Papson et al. [54] focus on image

fusion of SAR and ISAR imagery for enhanced characterization of targets.

While such intuitively motivated fusion methods have yielded beneﬁts in ATR, a fun-

damental understanding of the relationships between heterogeneous and complementary

sources of data has not been satisfactorily achieved yet. Our approach is an attempt

towards gaining more insight into these inter-feature relationships with the aim of im-

proving classiﬁcation performance.

Table 2.1. Training images used for experiments. All images are taken from the MSTAR data
set.

22

Number of images Depression angle

Table 2.2. Test images used for experiments. All images are taken from the MSTAR data set.

Number of images Depression angle

Target
BMP-2
BTR-70

T-72

BTR-60

2S1

BRDM-2

D7
T62

ZIL131
ZSU234

Vehicles

9563, 9566, c21

c71

132, 812, s7
k10yt7532

b01
E-71

92v13015

A51
E12
d08

Target
BMP-2
BTR-70

T-72

BTR-60

2S1

BRDM-2

D7
T62

ZIL131
ZSU234

Vehicles

9563, 9566, c21

c71

132, 812, s7
k10yt7532

b01
E-71

92v13015

A51
E12
d08

299
697
298
256
233
299
299
691
299
299

587
196
582
195
274
263
274
582
274
274

17◦
17◦
17◦
17◦
17◦
17◦
17◦
17◦
17◦
17◦

15◦
15◦
15◦
15◦
15◦
15◦
15◦
15◦
15◦
15◦

2.5 Experiments and Results

We test the proposed framework on the benchmark MSTAR data set under a variety

of operating conditions. It is well-known that pose estimation can signiﬁcantly improve

classiﬁcation performance. Accordingly, we use the pose estimation technique from [32]

in our framework as a pre-processing step. The overall performance of our proposed

approach is better than that of well-known competing techniques in the literature [10,

32, 34]. We also compare our approach (in the absence of pose estimation) with the

template-based correlation ﬁlter method in [58] which is designed to be approximately

invariant to pose. Another important merit of our algorithm - robustness to limited

Table 2.3. Test images used for the version variant testing set EOC-1 in Section 2.5.2.

23

Serial number Depression angle Number of images

Target
T-72
T-72
T-72
T-72
T-72

s7
A32
A62
A63
A64

15◦, 17◦
15◦, 17◦
15◦, 17◦
15◦, 17◦
15◦, 17◦

419
572
573
573
573

training - is revealed by observing classiﬁcation performance as a function of training

set size, a comparison not usually performed for ATR problems although it has high

practical signiﬁcance.

2.5.1 Experimental Set-up

Our experiments are performed on magnitude SAR images obtained from the benchmark

MSTAR program [40], which has released a large set of SAR images in the public domain.

These consist of one-foot resolution X-band SAR images of ten diﬀerent vehicle classes as

well as separate clutter and confuser images. Target images are captured under varying

operating conditions including diﬀerent depression angles, aspect angles, serial numbers,

and articulations. This collection of images, referred to as the MSTAR data set, is

suitable for testing the performance of ATR algorithms since the number of images is

statistically signiﬁcant, ground truth data for acquisition conditions are available and a

variety of targets have been imaged. Standard experimental conditions and performance

benchmarks to compare classiﬁcation experiments using the MSTAR database have been

provided in [35].

The ten target classes from the database used for our experiments are: T-72, BMP-2,

BTR-70, BTR-60, 2S1, BRDM-2, D7, T62, ZIL131, and ZSU234. Tables 2.1 and 2.2 list

the various target classes with vehicle variant descriptions, number of images per class

available for training and testing, as well as the depression angle. Each target chip - the
processed image input to the classiﬁcation algorithm - is normalized to a 64 × 64 region
of interest.
Images for training corresponding to all the ten vehicle classes are acquired at 17◦
depression angle. We consider three diﬀerent test scenarios in our experiments. Under

standard operating conditions (SOC) [35], we test the algorithms with images from all ten

classes as listed in Table 2.2. These images are of vehicles with the same serial numbers
as those in the training set, captured at depression angles of 15◦. The other two test

24

scenarios pertain to extended operating conditions (EOC). Speciﬁcally, we ﬁrst consider

a four-class problem (denoted by EOC-1) with training images chosen from BMP-2,

BTR-70, BRDM-2, and T-72 as listed in Table 2.1, while the test set comprises ﬁve

version variants of T-72 as described in Table 2.3. This is consistent with the version

variant testing scenario in [10]. We also compare algorithm performance for another

test set (EOC-2) comprising four vehicles (2S1, BRDM2, T72, ZSU234) with the same
serial numbers as the training group but acquired at 30◦ depression angle. This EOC
is consistent with the Test Group 3 in [58]. It is well-known that test images acquired

with depression angle diﬀerent from the training set are harder to classify [39].

Pose estimation: The target images are acquired with pose angles randomly varying
between 0◦ and 360◦. Eliminating variations in pose can lead to signiﬁcant improvement
in overall classiﬁcation performance. Many pose estimation approaches have been pro-

posed for the ATR problem ( [59] for example). A few other approaches [10, 32] have

incorporated a pose estimator within the target recognition framework. On the other

hand, template-based approaches like [42, 58] are designed to be invariant to pose vari-

ations. Here, we use the pose estimation technique proposed in [32]. The pre-processed

chip is ﬁrst ﬁltered using a Sobel edge detector to identify target edges, followed by an ex-

haustive target-pose search over diﬀerent pose angles. The details of the pose estimation

process are available in [32].

We compare our proposed Iterative Graph Thickening (IGT) approach with four

widely cited methods in ATR literature:

1. EMACH: the extended maximum average correlation height ﬁlter in [58]

2. SVM: support vector machine classiﬁer in [34]

3. CondGauss: conditional Gaussian model in [10]

4. AdaBoost: feature fusion via boosting on RBF net classiﬁers [32].

In the subsequent sections, we provide a variety of experimental results to demon-

strate the merits of our IGT approach compared to existing methods. First, in Section

2.5.2, we present confusion matrices, shown in Tables 2.4-2.22, for the SOC and EOC

scenarios. The confusion matrix is commonly used in ATR literature to represent clas-

siﬁcation performance. Each element of the confusion matrix gives the probability of

classiﬁcation into one of the target classes. Each row corresponds to the true class of

the target image, and each column corresponds to the class chosen by the classiﬁer. In

Section 2.5.3, we test the outlier rejection performance of our proposed approach via

25

ROC plots. Finally, we evaluate the performance of the ﬁve approaches as a function of

training set size in Section 2.5.4.

2.5.2 Recognition Accuracy

2.5.2.1 Standard Operating Condition (SOC)

Tables 2.4-2.12 show results for the SOC scenario. As discussed earlier, estimating the

pose of the target image can lead to improvements in classiﬁcation performance. The

SVM, CondGauss, and AdaBoost approaches chosen for comparison in our experiments

incorporate pose estimation as a pre-processing step before classiﬁcation. However, the

EMACH ﬁlter [58] is designed to be inherently invariant to pose. Accordingly, we con-

sider two speciﬁc experimental cases:

With pose estimation: In this scenario, pose estimation is performed in all approaches

other than EMACH. For our IGT framework, we use the same pose estimator from [32].

The confusion matrices are presented in Tables 2.4-2.8. The proposed approach results

in better overall classiﬁcation performance in comparison to the existing approaches.

The classiﬁcation rates in Tables 2.4-2.8 are consistent with values reported in litera-

ture [10, 32, 34, 58].

No pose estimation For fairness of comparison with EMACH, we now compare perfor-

mance in the absence of explicit pose estimation. The confusion matrices are presented

in Tables 2.9-2.12. Comparing these results with the confusion matrix in Table 2.4,

we observe that the EMACH ﬁlter, unsurprisingly, gives the best overall performance

among the existing approaches, thereby establishing its robustness to pose invariance.

Signiﬁcantly, the classiﬁcation accuracy of the IGT framework (without pose estimation)

is comparable to EMACH.

2.5.2.2 Extended Operating Conditions (EOC)

We now compare algorithm performance using more diﬃcult test scenarios. Here, we

do not provide separate results with and without pose estimation, and each existing

approach being compared is chosen with its best settings (i.e. methods other than

EMACH incorporate pose estimation). It must be mentioned however that the overall

trends in the absence of pose estimation are similar to those observed for the SOC.

For the EOC-1 test set, the confusion matrices are presented in Tables 2.13-2.17. The

26

corresponding confusion matrices for the EOC-2 test set are shown in Tables 2.18-2.22.

In each test scenario, we see that IGT consistently outperforms the existing techniques.

The average classiﬁcation accuracies of the ﬁve methods for the diﬀerent experimental

scenarios are compared in Table 2.23.

2.5.3 ROC Curves: Outlier Rejection Performance

Perhaps the ﬁrst work to address the problem of clutter and confuser rejection in SAR

ATR is [60].

In this work, the outlier rejection capability of the EMACH ﬁlter [58]

is demonstrated for a subset of the MSTAR data with three target classes - BMP2,

BTR70, and T72. The D7 and 2S1 classes are treated as confusers. Extensions of

this work include experiments on all ten MSTAR classes with Gaussian kernel SVM

as classiﬁer [61], and similar comparisons using the Minace ﬁlter [62].

In each case,

no clutter or confuser images are used in the training phase. Our proposed framework

can also be used to detect outliers in the data set using the decision rule in (2.8).

The likelihood ratios from each of the K individual problems are compared against an

experimentally determined threshold τout. If all the K likelihood values are lower than

this threshold, the corresponding target image is identiﬁed as an outlier (confuser vehicle

or clutter).

We use the SOC test set and include new confuser images - provided in the MSTAR

database - in the test set. We consider a binary classiﬁcation problem with the two classes

being target and confuser. We also test the ability of the competing approaches to reject

clutter by considering another experiment where we use clutter images from MSTAR

instead of confuser vehicles. This experimental scenario is consistent with the ROC

curves provided in [58]. We compute the probability of detection (Pd) and probability

of false alarm (Pf a) using the threshold described in Section 2.3. Fig. 2.3(a) shows the

ROCs for the ﬁve methods for the target vs. confuser problem. The corresponding ROCs

for target vs. clutter are shown in Fig. 2.3(b). In both cases, the improved performance

of IGT is readily apparent. A visual comparison also shows the improvements in IGT

over the results in [61], [62].

2.5.4 Classiﬁcation Performance as Function of Training Size

Unlike some other real-world classiﬁcation problems, ATR suﬀers from the drawback

that the training images corresponding to one or more target classes may be limited. To
illustrate, the typical dimension of a SAR image in the MSTAR data set is 128 × 128 =
16384. Even after cropping and normalization to 64×64 the data size is 4096 coeﬃcients.

27

In comparison, the number of training SAR images per class is in the 50-250 range (see

Table 2.2). The Hughes phenomenon [63] highlights the diﬃculty of learning models

for high-dimensional data under limited training. So in order to test the robustness of

various ATR algorithms in the literature to low training, we revisit the SOC and EOC

experiments from Section 2.5.2, and plot the overall classiﬁcation accuracy as a function

of training set size. The corresponding plots are shown in Figs. 2.4 and 2.5.

Figs. 2.4(a)-2.4(b) show the variation in classiﬁcation error as a function of training

sample size for the SOC, both with and without pose estimation. For each value of train-

ing size, the experiment was performed using ten diﬀerent random subsets of training

vectors and the average error probability values are reported. The IGT algorithm con-

sistently oﬀers the lowest error rates in all regimes of training size. For large training (an

unrealistic scenario for the ATR problem) as expected all techniques begin to converge.

We also observe that our proposed algorithm exhibits a more graceful degradation with

a decrease in training size vs. competing approaches. Similar trends can be inferred

from the plots for the EOCs, shown in Figs. 2.5(a)-2.5(b). Recognition performance as

a function of training size in SAR ATR is a very signiﬁcant practical issue and in this

aspect, the use of probabilistic graphical models as decision engines oﬀers appreciable

beneﬁts over existing alternatives based on SVMs [34], neural networks [43] etc. The

superior performance of discriminative graphs in the low-training regime is attributed

to the ability of the graphical structure to capture dominant conditional dependencies

between features which are crucial to the classiﬁcation task [2, 3, 64].

2.5.5 Summary of Results

Here, we summarize the key messages from the various experiments described in Section

2.5.2 to 2.5.4. First, we test the proposed IGT approach against competing approaches

on the MSTAR SOC test set. We provide two types of results, with and without pose

estimation. When pose estimation is explicitly included as a pre-processing step, the Ad-

aBoost, CondGauss, and SVM methods perform better overall compared to the EMACH

ﬁlter, although the EMACH ﬁlter gives better results for speciﬁc vehicle classes. Overall,

the performance of the IGT method is better than all the competing approaches. Since

the EMACH ﬁlter is designed to be invariant to pose, we also provide confusion matrices

for the scenario where pose is not estimated prior to classiﬁcation using the competing

approaches. Here, we observe that the EMACH ﬁlter and IGT perform best overall,

while the other approaches suﬀer a signiﬁcant degradation in performance. These two

experiments demonstrate that IGT exhibits an inherent invariance to pose, to some ex-

28

tent. This can be explained due to the choice of wavelet features which are known to be

less sensitive to image rotations, as well as the fact that the graphical models learn better

from the available training image data compared to the other approaches. Similar trends

hold for the harder EOC test scenarios too. It must be noted that the classiﬁcation rates

reported in the confusion matrices for the various approaches are consistent with values

reported in literature [10, 32, 34, 58].

Next, we introduce a comparison of classiﬁcation performance as a function of training

set size. While all methods perform appreciably when provided with large amount of

training (representative of asymptotic scenario), the proposed IGT method exhibits a

much more graceful decay in performance as the number of training samples per class

is reduced. As Figs. 2.4(a) and 2.4(b) reveal, this is true for both the cases of with

and without pose estimation. This is the ﬁrst such explicit comparison in SAR ATR

to the best of our knowledge. Finally, we test the outlier rejection performance of the

approaches by plotting the ROCs. Here too, the overall improved performance of IGT

is apparent.

2.6 Conclusion

The value of complementary feature representations and decision engines is well appre-

ciated by the ATR community as conﬁrmed by a variety of fusion approaches, which

use high-level features, or equivalently, the outputs of individual classiﬁers. We lever-

age a recent advance in discriminative graph learning to explicitly capture dependencies

between diﬀerent competing sets of low-level features for the SAR target classiﬁcation

problem. Our algorithm learns tree-structured graphs on the LL, LH and HL wavelet

sub-band coeﬃcients extracted from SAR images and thickens them via boosting. The

proposed framework readily generalizes to any suitable choice of feature sets that oﬀer

complementary beneﬁts. Our algorithm is particularly eﬀective in the challenging regime

of low training and high dimensional data, a serious practical concern for ATR systems.

Experiments on SAR images from the benchmark MSTAR data set demonstrate the

success of our approach over well-known target classiﬁcation algorithms.

Looking ahead, multi-sensor ATR [56] oﬀers fertile ground for the application of

our feature fusion framework. Of related signiﬁcance is the problem of feature selec-

tion [65, 66]. Several hypotheses could be developed and the relative statistical signiﬁ-

cance of inter-relationships between learned target features can be determined via feature

selection techniques.

29

Table 2.4. Confusion matrix for SOC: EMACH correlation ﬁlter [58].

Class
BMP-2
BTR-70

T-72

BTR-60

2S1

BRDM-2

D7
T62

ZIL131
ZSU234

BMP-2

BTR-70

0.90
0.02
0.02

0

0.05
0.03
0.02
0.01
0.02
0.01

0.02
0.93

0

0.01
0.06
0.06
0.03
0.01

0
0

T-72
0.04
0.01
0.96

0

0.04
0.03
0.02
0.01
0.01
0.04

BTR-60

0.01

0
0

0.95
0.02

0

0.01
0.01
0.02
0.02

2S1
0.01
0.01
0.01
0.01
0.74
0.01

0

0.04

0
0

BRDM-2

0

0.01

0
0

0.03
0.84

0
0
0
0

D7
0
0
0

0.03
0.01
0.02
0.85

0
0
0

T62
0.01
0.02

0
0

0.02

0

0.03
0.86
0.04
0.01

ZIL131

ZSU234

0
0

0.01

0

0.01

0

0.02
0.04
0.88

0

0.01

0
0
0

0.02
0.01
0.02
0.02
0.03
0.92

Table 2.5. Confusion matrix for SOC: SVM classiﬁer [34] with pose estimation.

Class
BMP-2
BTR-70

T-72

BTR-60

2S1

BRDM-2

D7
T62

ZIL131
ZSU234

BMP-2

BTR-70

0.90
0.03
0.02
0.02
0.05
0.06

0

0.01
0.02

0

0.02
0.90
0.01
0.02
0.03
0.08

0
0

0.01
0.01

T-72
0.03
0.03
0.93
0.01
0.02
0.02

0
0
0
0

BTR-60

0.01

0

0.03
0.92

0

0.01

0

0.01

0

0.03

2S1
0.01

0
0
0

0.81

0

0.01

0
0
0

BRDM-2

0.02
0.02

0
0

0.03
0.79

0
0
0
0

D7
0
0
0

0.03
0.02

0

0.98

0
0

0.01

T62

ZIL131

ZSU234

0
0
0
0

0.03
0.03

0

0.91

0
0

0.01

0

0.01

0
0
0
0

0.04
0.95
0.03

0

0.02

0
0

0.01
0.01
0.01
0.03
0.02
0.92

Table 2.6. Confusion matrix for SOC: Conditional Gaussian model [10] with pose estimation.

Class
BMP-2
BTR-70

T-72

BTR-60

2S1

BRDM-2

D7
T62

ZIL131
ZSU234

BMP-2

BTR-70

0.93
0.03
0.02
0.02
0.03
0.04

0

0.01
0.02
0.01

0.01
0.91
0.01

0

0.04
0.03

0
0

0.02
0.02

T-72
0.02
0.02
0.95
0.01
0.01
0.01

0
0
0
0

BTR-60

0
0

0.01
0.95

0
0
0
0
0

0.01

2S1
0.01

0
0
0

0.87

0

0.01

0
0
0

BRDM-2

0.02
0.02

0
0

0.02
0.89

0
0
0

0.01

D7
0
0
0

0.02

0

0.03
0.98

0
0
0

T62

ZIL131

ZSU234

0
0
0
0

0.02

0
0

0.92

0
0

0.01

0

0.01

0
0
0
0

0.05
0.95
0.02

0

0.02

0
0

0.01

0

0.01
0.02
0.01
0.93

30

Table 2.7. Confusion matrix for SOC: AdaBoost [32] with pose estimation.

Class
BMP-2
BTR-70

T-72

BTR-60

2S1

BRDM-2

D7
T62

ZIL131
ZSU234

BMP-2

BTR-70

0.92
0.03
0.02
0.02
0.03
0.05

0

0.01
0.02
0.01

0.02
0.93
0.01

0

0.04
0.03

0
0

0.02

0

T-72
0.02

0

0.96
0.02
0.01
0.02

0
0
0
0

BTR-60

0
0

0.01
0.93

0
0
0
0
0

0.01

2S1
0.01

0
0
0

0.87

0

0.01

0
0
0

BRDM-2

0.02
0.02

0
0

0.02
0.85

0
0
0
0

D7
0
0
0

0.03

0

0.05
0.98

0
0
0

T62

ZIL131

ZSU234

0
0
0
0

0.02

0
0

0.93

0
0

0.01

0
0
0
0
0
0

0.03
0.94
0.02

0

0.02

0
0

0.01

0

0.01
0.03
0.02
0.96

Table 2.8. Confusion matrix for SOC: Iterative Graph Thickening (IGT) with pose estimation.

Class
BMP-2
BTR-70

T-72

BTR-60

2S1

BRDM-2

D7
T62

ZIL131
ZSU234

BMP-2

BTR-70

0.95
0.02
0.02
0.01
0.03
0.02

0

0.01
0.02
0.01

0.01
0.94
0.01

0

0.04
0.01

0
0

0.02

0

T-72
0.01

0

0.96
0.01
0.01
0.04

0
0

0.01

0

BTR-60

0
0

0.01
0.97

0
0
0
0
0

0.01

2S1
0.01

0
0
0

0.89

0

0.01

0
0
0

BRDM-2

0.01
0.02

0
0
0

0.90

0
0
0
0

D7
0
0
0

0.01

0

0.02
0.99

0
0
0

T62

ZIL131

ZSU234

0
0
0
0

0.01

0
0

0.95

0
0

0.01

0
0
0
0
0
0

0.03
0.95
0.02

0

0.02

0
0

0.02
0.01

0

0.01

0

0.96

Table 2.9. Confusion matrix for SOC: SVM classiﬁer [34] without pose estimation.

Class
BMP-2
BTR-70

T-72

BTR-60

2S1

BRDM-2

D7
T62

ZIL131
ZSU234

BMP-2

BTR-70

0.84
0.02
0.03
0.04
0.06
0.08
0.03
0.04
0.02
0.01

0.05
0.83
0.05
0.04
0.04
0.02
0.02
0.03
0.03

0

T-72
0.04

0

0.87

0

0.03
0.03

0

0.05
0.03

0

BTR-60

0.01
0.05

0

0.86
0.04
0.02

0

0.01

0

0.02

2S1

0

0.03

0

0.03
0.75

0

0.02

0
0
0

BRDM-2

0.02
0.02
0.03

0

0.05
0.74

0
0
0
0

D7
0

0.01
0.01
0.03
0.01

0

0.91

0
0

0.08

T62
0.03
0.01

0
0

0.01
0.09
0.02
0.85

0

0.04

ZIL131

ZSU234

0
0
0
0

0.01
0.01

0

0.02
0.89

0

0.01
0.03
0.01

0
0

0.01

0
0

0.03
0.85

31

Table 2.10. Confusion matrix for SOC: Conditional Gaussian model [10] without pose estima-
tion.

Class
BMP-2
BTR-70

T-72

BTR-60

2S1

BRDM-2

D7
T62

ZIL131
ZSU234

BMP-2

BTR-70

0.89
0.02
0.03
0.02
0.03
0.08
0.03
0.03
0.03
0.01

0.03
0.88

0
0

0.03
0.02
0.02
0.03

0
0

T-72
0.02

0

0.92
0.03
0.04
0.05

0

0.01
0.03
0.02

BTR-60

2S1

BRDM-2

0.01
0.05

0

0.90
0.01
0.01

0
0
0
0

0
0
0
0

0.82

0
0
0
0

0.03

0.01

0
0
0
0

0.75

0

0.05
0.05

0

D7
0

0.03
0.03
0.05

0

0.01
0.89

0

0.01

0

T62
0.03
0.01

0
0

0.03
0.05
0.04
0.84
0.01
0.04

ZIL131

ZSU234

0
0

0.02

0

0.03

0
0

0.03
0.85
0.03

0.01
0.01

0
0

0.01
0.03
0.02
0.01
0.02
0.87

Table 2.11. Confusion matrix for SOC: AdaBoost [32] without pose estimation.

Class
BMP-2
BTR-70

T-72

BTR-60

2S1

BRDM-2

D7
T62

ZIL131
ZSU234

BMP-2

BTR-70

0.88
0.02
0.02
0.04
0.04
0.05
0.03
0.02
0.03
0.05

0.03
0.90
0.02
0.03
0.02
0.02
0.02
0.03

0

0.02

T-72
0.02
0.04
0.91
0.01
0.04

0
0
0

0.01
0.01

BTR-60

2S1

BRDM-2

0.01

0

0.02
0.89
0.02
0.03

0

0.01
0.02

0

0
0
0
0

0.84

0
0

0.02

0
0

0.01
0.03

0
0

0.01
0.81

0
0

0.07
0.01

D7
0
0
0
0
0

0.03
0.91

0
0

0.03

T62
0.04

0

0.01
0.01
0.03

0

0.04
0.86

0

0.01

ZIL131

ZSU234

0
0

0.01

0
0

0.05

0

0.05
0.86
0.01

0.01
0.01
0.01
0.02

0

0.01

0

0.01
0.01
0.86

Table 2.12. Confusion matrix for SOC: Iterative Graph Thickening (IGT) without pose esti-
mation.

Class
BMP-2
BTR-70

T-72

BTR-60

2S1

BRDM-2

D7
T62

ZIL131
ZSU234

BMP-2

BTR-70

0.89
0.01
0.02
0.03
0.04
0.01
0.01

0

0.04
0.02

0.03
0.91
0.01
0.02
0.02
0.03
0.02

0
0
0

T-72
0.02
0.04
0.93
0.01
0.04

0
0

0.04
0.03
0.03

BTR-60

2S1

BRDM-2

0.01

0

0.02
0.92
0.02
0.03
0.01
0.05

0
0

0
0
0
0

0.77

0
0
0
0
0

0.01
0.03

0
0

0.05
0.88

0
0

0.02
0.03

D7
0
0
0
0
0

0.02
0.89

0
0
0

T62
0.01

0

0.01
0.01
0.03

0

0.02
0.88
0.02
0.01

ZIL131

ZSU234

0.01

0

0.01

0
0

0.02
0.05
0.01
0.88

0

0.01
0.01

0

0.01
0.03
0.01

0

0.02
0.01
0.91

32

Table 2.13. Confusion matrix for EOC-1: EMACH [58].

Class
T-72 s7
T-72 s7
T-72 62
T-72 63
T-72 64

BMP-2 BTR-70 BRDM-2 T-72
0.82
0.81
0.83
0.70
0.68

0.06
0.05
0.03
0.11
0.12

0.04
0.09
0.08
0.13
0.16

0.08
0.05
0.06
0.06
0.04

Table 2.14. Confusion matrix for EOC-1: SVM classiﬁer [34].

Class
T-72 s7
T-72 s7
T-72 62
T-72 63
T-72 64

0.05
0.07
0.05
0.15
0.09

BMP-2 BTR-70 BRDM-2 T-72
0.87
0.86
0.84
0.76
0.73

0.04
0.05
0.05
0.06
0.05

0.04
0.02
0.06
0.03
0.13

Table 2.15. Confusion matrix for EOC-1: Conditional Gaussian model [10].

Class
T-72 s7
T-72 s7
T-72 62
T-72 63
T-72 64

BMP-2 BTR-70 BRDM-2 T-72
0.87
0.84
0.81
0.75
0.73

0.09
0.05
0.07
0.08
0.10

0.02
0.05
0.06
0.05
0.08

0.02
0.06
0.06
0.12
0.09

Table 2.16. Confusion matrix for EOC-1: AdaBoost [32].

33

Class
T72 s7
T-72 s7
T-72 62
T-72 63
T-72 64

BMP-2 BTR-70 BRDM-2 T-72
0.88
0.84
0.85
0.76
0.76

0.02
0.03
0.04
0.04
0.09

0.06
0.08
0.05
0.09
0.05

0.04
0.05
0.06
0.11
0.10

Table 2.17. Confusion matrix for EOC-1: IGT.

Class
T-72 s7
T-72 s7
T-72 62
T-72 63
T-72 64

BMP-2 BTR-70 BRDM-2 T-72
0.88
0.89
0.87
0.81
0.79

0.03
0.02
0.04
0.07
0.06

0.05
0.06
0.05
0.07
0.11

0.04
0.03
0.04
0.05
0.04

Table 2.18. Confusion matrix for EOC-2: EMACH [58].

Class
2S1

BRDM-2

T-72

ZSU234

2S1 BRDM-2 T-72 ZSU234
0.67
0.17
0.07
0.10

0.15
0.57
0.09
0.07

0.12
0.19
0.66
0.02

0.06
0.07
0.18
0.81

Table 2.19. Confusion matrix for EOC-2: SVM classiﬁer [34].

Class
2S1

BRDM-2

T-72

ZSU234

2S1 BRDM-2 T-72 ZSU234
0.74
0.12
0.17
0.07

0.09
0.09
0.73
0.03

0.08
0.66
0.06
0.05

0.09
0.13
0.04
0.85

Table 2.20. Confusion matrix for EOC-2: Conditional Gaussian model [10].

34

Class
2S1

BRDM-2

T-72

ZSU234

2S1 BRDM-2 T-72 ZSU234
0.75
0.14
0.16
0.05

0.08
0.11
0.72
0.04

0.09
0.69
0.05
0.05

0.08
0.06
0.07
0.86

Table 2.21. Confusion matrix for EOC-2: AdaBoost [32].

Class
2S1

BRDM-2

T-72

ZSU234

2S1 BRDM-2 T-72 ZSU234
0.77
0.15
0.11
0.04

0.11
0.05
0.75
0.02

0.05
0.73
0.09
0.06

0.07
0.07
0.05
0.88

Table 2.22. Confusion matrix for EOC-2: IGT.

Class
2S1

BRDM-2

T-72

ZSU234

2S1 BRDM-2 T-72 ZSU234
0.78
0.15
0.10
0.05

0.09
0.06
0.78
0.02

0.06
0.76
0.09
0.05

0.07
0.03
0.03
0.88

Table 2.23. Average classiﬁcation accuracy.

Class

SOC (pose)

SOC (no pose) EOC-1 EOC-2

EMACH

SVM

CondGauss
AdaBoost

IGT

0.88
0.90
0.92
0.92
0.95

-

0.84
0.86
0.87
0.89

0.77
0.81
0.80
0.82
0.85

0.68
0.75
0.76
0.78
0.80

35

(a) Target vs. confuser.

(b) Target vs. clutter.

Figure 2.3. Receiver operating characteristic curves. The proposed IGT method is compared
with the EMACH ﬁlter [58], SVM [34], conditionally Gaussian model [10], and the AdaBoost
method [32].

36

(a) SOC with pose estimation.

(b) SOC with no pose estimation.

Figure 2.4. Classiﬁcation error vs. training sample size. The proposed IGT method is compared
with the EMACH ﬁlter [58], SVM [34], conditionally Gaussian model [10], and the AdaBoost
method [32].

37

(a) EOC-1.

(b) EOC-2.

Figure 2.5. Classiﬁcation error vs. training sample size. The proposed IGT method is compared
with the EMACH ﬁlter [58], SVM [34], conditionally Gaussian model [10], and the AdaBoost
method [32].

Chapter 3

Application: Learning Graphical
Models on Sparse Features

3.1

Introduction

The goals of this chapter are two-fold. First, the role of sparsity in signal processing

applications is reviewed. Starting with a brief description of the compressive sensing

(CS) problem, we present a recent seminal contribution that exploits the underlying

analytical framework of CS for classiﬁcation tasks via class-speciﬁc dictionaries. Such

sparse representations have been shown to be discriminative and robust to a variety of

real-world imaging distortions. We also brieﬂy review an extension of this framework that

can handle the scenario of multiple measurements through simultaneous/joint sparsity

models. This discussion naturally serves as the background for the contributions in

Chapter 4, which deal with discriminative structured models on sparse coeﬃcients.

The second goal of this chapter is to oﬀer preliminary validation of the links be-

tween graphical models and sparse features [9]. Speciﬁcally, we use our graphical model

framework from Chapter 2 to learn discriminative trees on collections of sparse features

that have been jointly extracted from images in a speciﬁc manner. We demonstrate this

for two diﬀerent practical applications: (i) hyperspectral target detection and classiﬁ-

cation, and (ii) robust face recognition. In hyperspectral imaging, we exploit the fact

that hyperspectral scenes are spatially homogeneous to enforce identical sparse repre-

sentation structure on a local neighborhood of pixels. In face recognition, we exploit the

correlations among sparse features extracted from diﬀerent parts of the face that carry

discriminative information - the eyes, nose, and mouth. In each application, we see that

39

our graphical approach oﬀers robustness in the scenario of limited training.

3.2 Sparsity in Signal Processing

The value of parsimony in signal representation has been recognized for a long time now.

It is well-known that a large class of signals, including audio and images, can be expressed

naturally in a compact manner with respect to well-chosen basis representations. Among

the most widely applicable of such basis representations are the Fourier and wavelet basis.

Sparse signal decomposition and representation have emerged as the cornerstone of high-

performing signal processing algorithms. A sparse representation can not only provide

better signal compression for bandwidth eﬃciency, but also lead to faster processing

algorithms. Sparse signal representation allows us to capture the simple structure often

hidden in visual data, and thus minimizes the undesirable eﬀects of noise in practical

settings. This has inspired a proliferation of applications that involve sparse signal

representations for acquisition [4], compression [5], and modeling [6].

The pre-eminence of vision among all our sensory systems has led to our enduring

fascination with the workings of the human visual system [67]. The seminal work of

Olshausen and Field [68] established that the receptive ﬁelds of simple cells in mammalian

primary visual cortex (V1) can be characterized as being selective to a variety of speciﬁc

stimuli such as color, texture, orientation, and scale. Interpreted diﬀerently, the ﬁring of

neurons with respect to a particular input scene is typically highly sparse if we assume

that the neurons form an overcomplete dictionary of base signals [68–70].

The Sparseland model [71] has emerged as a powerful method to describe signals

based on the sparsity and redundancy of their representations. This linear representation

model is simplistic albeit quite powerful in its ability to capture redundancy given a good

basis for representation. The quest for economical signal representations in practice

has fueled the development of very eﬃcient compression algorithms for digital image

and video signals [5, 72]. Compressive sensing [4] has formalized this notion of sparse

representations by seeking the vector with minimum number of non-zero entries that

minimizes reconstruction error (w.r.t. a sparsifying basis representation AAA).

3.2.1 Compressive Sensing

Analytically, the challenge in CS is to recover a signal xxx ∈ Rn given a vector of linear
measurements yyy ∈ Rm of the form yyy = AAAxxx, where m (cid:28) n. Assuming xxx is compressible, it

can be recovered from this underdetermined system of equations by solving the following

problem [4]:

(P0) min

xxx (cid:107)xxx(cid:107)0 subject to yyy = AAAxxx,

40

(3.1)

where (cid:107)xxx(cid:107)0 is the l0-“norm” that counts the number of non-zero entries in xxx. This is an
NP-hard problem and it has been shown [73] that if xxx is suﬃciently sparse, it can be

exactly recovered by solving the convex program

(P1) min

xxx (cid:107)xxx(cid:107)1 subject to yyy = AAAxxx.

(3.2)

The l1-norm problem has the additional interpretation of enforcing an i.i.d. Laplacian
prior on xxx. In fact, this is a speciﬁc example of the broader Bayesian perspective that

prior information can improve signal comprehension. In practice, the presence of noise in

real signals is accounted for by relaxing the equality constraint yyy = AAAxxx to the inequality
constraint (cid:107)yyy − AAAxxx(cid:107)2 <  for some ﬁxed noise level ,

(P2) min

xxx (cid:107)xxx(cid:107)1 subject to (cid:107)yyy − AAAxxx(cid:107)2 < .

(3.3)

Approaches to solve the problem (P2) are well-known, e.g. lasso in statistics [74].

With the goal of achieving better signal recovery, research in CS has primarily focused

on two aspects: (i) the design of optimal “sparsifying” projections AAA [75], and (ii) eﬃcient

algorithms to solve (P0) [76].

3.2.2 Sparse Representation-based Classiﬁcation

A seminal contribution to the development of algorithms for signal classiﬁcation and

decision-making is a recent sparse representation-based classiﬁcation (SRC) framework

[7]. In this work, Wright et al. explicitly mined the discriminative capability of sparse

representations for image classiﬁcation by combining two ideas: (i) the analytical frame-

work underlying CS, and (ii) the development of models for human vision based on

overcomplete dictionaries [69]. Given a suﬃciently diverse collection of training images

from each class, a linear representation model is assumed, whereby an image from a

speciﬁc class can be expressed approximately as a linear combination of training images

from the same class. So, if a basis matrix or dictionary is built from training images of

all classes, any test image has a sparse representation with respect to such a dictionary.

Here, sparsity manifests itself due to the class-speciﬁc design of dictionaries as well as

the assumption of the linear representation model. This model alleviates the challenge

of designing sophisticated task-speciﬁc features.

Suppose that there are K diﬀerent image classes, labeled C1, . . . , CK. Let there be
Ni training samples (each in Rn) corresponding to class Ci, i = 1, . . . , K. It is under-
stood that each sample is the vectorized version of the corresponding grayscale (or single

41

channel) image. The training samples corresponding to class Ci can be collected in a

matrix DDDi ∈ Rn×Ni, and the collection of all training samples is expressed using the

matrix

DDD = [DDD1 DDD2 . . . DDDK],

(3.4)

where DDD ∈ Rn×T , with T =(cid:80)K

sparse linear combination of the training samples,

k=1 Nk. A new test sample yyy ∈ Rn can be expressed as a

yyy (cid:39) DDD1ααα1 + . . . + DDDKαααK = DDDα,

(3.5)

where ααα is ideally expected to be a sparse vector (i.e., only a few entries in ααα are nonzero).

The classiﬁer seeks the sparsest representation by solving the following problem:

(P3)

ˆααα = arg min(cid:107)ααα(cid:107)0

subject to (cid:107)yyy − DDDααα(cid:107)2 ≤ ,

(3.6)

where (cid:107)·(cid:107)0 denotes the number of nonzero entries in the vector and ε is a suitably
chosen reconstruction error tolerance. Essentially, this is a modiﬁcation of (P2) with
DDD := [DDD1, DDD2 . . . , DDDK] and the l0-norm. The problem in (3.6) can be solved by greedy
pursuit algorithms [77,78]. Once the sparse vector is recovered, the identity of yyy is given

by the minimal class-speciﬁc reconstruction residual,

Class(yyy) = arg min

i (cid:107)yyy − DDDδi(ˆααα)(cid:107) ,

(3.7)

where δi(ααα) is a vector whose only nonzero entries are the same as those in ααα which are

associated with class Ci.

Rooted in optimization theory, the robustness of the sparse feature vector to real-

world image distortions like noise and occlusion has led to its widespread application

in practical classiﬁcation tasks. Modiﬁcations to (3.6) include relaxing the non-convex

l0-term to the l1-norm [73] and introducing regularization terms to capture physically

meaningful constraints [79]. This sparsity-based algorithm has been shown [7,8] to yield

markedly improved performance over traditional eﬀorts in the practical problem of face

recognition under various distortion scenarios, including illumination, disguise, occlusion,

and random pixel corruption. Sparse representation-based classiﬁcation has also been

applied successfully to other discriminative applications such as subspace clustering [80],

42

iris recognition [81], and classiﬁcation of hyperspectral imagery [82].

Modiﬁcations to the original SRC problem have considered regularizers that exploit

the correlated behavior of groups of coeﬃcients corresponding to the same training sub-

dictionary. The simplest extension minimizes the sum of l2-norms of the sub-vectors xxxi,
giving rise to a l1 − l2 group sparse regularizer [79]. This is similar to the idea of the
group Lasso [83]. This method does not explicitly enforce sparsity within each group, an

issue which has subsequently been addressed using the hierarchical Lasso in [84]. Other

types of group regularizers have been proposed in [85].

In many scenarios, we have access to multiple sets of measurements that capture

information about the same image. The SRC model is extended to incorporate this

additional information by enforcing a common support set of training images for the T

correlated test images yyy1, . . . , yyyT :

··· DDDαααT(cid:105)

(3.8)

YYY =(cid:104)yyy1 yyy2
= DDD(cid:104)ααα1 ααα2
(cid:124)

SSS

··· yyyT(cid:105) =(cid:104)DDDααα1 DDDααα2
··· αααT(cid:105)
(cid:125)
(cid:123)(cid:122)

= DDDSSS.

The vectors αααi, i = 1, . . . , T , all have non-zero entries at the same locations, albeit with
diﬀerent weights, leading to the recovery of a sparse matrix SSS with only a few nonzero

rows,

ˆSSS = arg min(cid:107)YYY − DDDSSS(cid:107)F

subject to

(cid:107)SSS(cid:107)row,0 ≤ K0,

(3.9)

where (cid:107)SSS(cid:107)row,0 denotes the number of non-zero rows of SSS and (cid:107)·(cid:107)F is the Frobenius
norm. The greedy Simultaneous Orthogonal Matching Pursuit (SOMP) [86] algorithm

and convex relaxations of the row-sparsity norm [87] have been proposed to solve the

non-convex problem (3.9).

3.3 Hyperspectral Imaging

3.3.1

Introduction

Hyperspectral imaging (HSI) sensors acquire digital images in hundreds of continuous

narrow spectral bands spanning the visible to infrared spectrum [88]. A pixel in hy-

perspectral images is typically a high-dimensional vector of intensities as a function of

wavelength. The high spectral resolution of the HSI pixels facilitates superior discrimi-

nation of object types.

An important research problem in HSI [88] is hyperspectral target detection, which

43

can be viewed as a binary classiﬁcation problem where hyperspectral pixels are labeled

as either target or background based on their spectral characteristics. Many statistical

hypothesis testing techniques [89] have been proposed for hyperspectral target detec-

tion, including the spectral matched ﬁlter (SMF), matched subspace detector (MSD)

and adaptive subspace detector (ASD). Advances in machine learning theory have con-

tributed to the popularity of SVMs [90] as a powerful tool to classify hyperspectral

data [91]. Variants such as SVM with composite kernels, which incorporates spatial

information directly in the kernels [92], have led to improved performance. HSI classiﬁ-

cation is the generalization of the binary problem to multiple classes.

Recent work has highlighted the relevance of incorporating contextual information

during HSI classiﬁcation to improve performance [92–95], particularly because HSI pixels

in a local neighborhood generally correspond to the same material and have similar

spectral characteristics. Many approaches have exploited this aspect, for example by

including post-processing of individually-labeled samples [93, 94] and Markov random

ﬁelds in Bayesian approaches [95]. The composite kernel approach [92] combines the

spectral and spatial information from each HSI pixel via kernel composition.

A signiﬁcant recent advance exploits sparsity for HSI classiﬁcation [82] based on the

observation that spectral signatures of the same material lie in a subspace of reduced

dimensionality compared to the number of spectral bands. An unknown pixel is then ex-

pressed as a sparse linear combination of a few training samples from a given dictionary

and the underlying sparse representation vector encodes the class information. Further,

to exploit spatial correlation, a joint sparsity model is employed in [82], wherein neigh-

boring pixels are assumed to be represented by linear combinations of a few common

training samples to enforce smoothness across these pixels.

3.3.2 Motivation and Contribution

The technique in [82] performs classiﬁcation by using (spectral) reconstruction error

computed over the pixel neighborhood. The sparse representations corresponding to

diﬀerent pixels in a local neighborhood are statistically correlated, and this correlation is

captured intuitively by the joint sparsity model. A challenging open problem, therefore,

is to mine the class-conditional correlations among these distinct feature representations

in a discriminative manner for detection and classiﬁcation.

Recent work [9] in model-based compressed sensing has shown the beneﬁts of using

probabilistic graphical models as priors on sparse coeﬃcients for signal (e.g. image) re-

construction problems. Inspired by this, we use probabilistic graphical models to enforce

44

Figure 3.1. Hyperspectral image classiﬁcation using discriminative graphical models on sparse
feature representations obtained from local pixel neighborhoods.

class-speciﬁc structure on sparse coeﬃcients, wherein our designed graphs represent class

conditional densities. Fig. 3.1 shows an illustration of the overall framework.

First, multiple sparse representations (corresponding to each pixel in a spatial neigh-

borhood) are extracted using the joint sparsity model [82]. We claim that these sparse

representations oﬀer complementary yet correlated information that is useful for classi-

ﬁcation. Our graphical model-based framework introduced in Chapter 2 then exploits

these class conditional correlations into building a powerful classiﬁer. Speciﬁcally, a pair

of discriminative tree graphs [3] is ﬁrst learned for each distinct set of features, i.e. the

sparse representation vectors of each pixel in the local spatial neighborhood of a central

pixel. These initially disjoint graphs are then thickened (by introducing new edges) into

a richer graphical structure via boosting [3, 30, 31]. The training phase of our graphical

model learning uses sparse coeﬃcients from all HSI classes. Consequently we learn a dis-

criminative graph-based classiﬁer that captures inter-class information, which is ignored

by the reconstruction residual-based scheme in [82].

3.4 Experimental Results and Discussion

In this section, we present separate sets of experimental results for the detection and

classiﬁcation problems. Our proposed algorithm is termed as Local-Sparse-GM (LSGM).

3.4.1 Hyperspectral Target Detection

Hyperspectral images from the HYDICE forest radiance I data collection (FR-I) [98] are

used for the experiment. The HYDICE sensor generates 210 bands across the whole

spectral range from 0.4 to 2.5 µm, spanning the visible and short-wave infrared bands

Table 3.1. Target detection: Confusion matrix for the FR-I hyperspectral image. Four diﬀerent
methods are compared. (Nt = 18 and Nb = 216.)

45

Class
Target

Background

Target Background Method
0.6512
0.9493
0.9556
0.9612
0.0239
0.0090
0.0097
0.0086

0.3488
0.0507
0.0444
0.0388
0.9761
0.9910
0.9903
0.9914

SOMP
LSGM

SOMP
LSGM

MSD

MSD

SVM-CK

SVM-CK

Figure 3.2. Target detection. ROC for FR-I comparing the four approaches: (i) MSD [96], (ii)
SVM-CK [92], (iii) SOMP [97], and (iv) the proposed algorithm (LSGM).

and including 14 targets. Only 150 of the 210 available bands are retained by removing

the absorption and low-SNR bands. The target sub-dictionary DDDt comprises 18 training

spectra chosen from the leftmost target in the scene, while the background sub-dictionary

DDDb has 216 training spectra chosen using the dual window technique described in [99].

Four diﬀerent methods are compared:

1. Classical matched subspace detector (MSD) which operates on each pixel indepen-

dently [96]

Table 3.2. Classiﬁcation rates for the AVIRIS Indian Pines test set. LSGM z-score = −2.13.

46

Class type
Alfalfa
Corn-notill
Corn-min
Corn
Grass/pasture
Grass/trees
Pasture-mowed
Hay-windrowed
Oats
Soybeans-notill
Soybeans-min
Soybeans-clean
Wheat
Woods
Building-trees
Stone-steel
Overall

Training Test
48
1290
750
210
447
672
23
440
18
871
2221
552
190
1164
342
85
9323

6
144
84
24
50
75
3
49
2
97
247
62
22
130
38
10
1043

87.50
94.80
94.53
93.33
89.71
98.51
91.30
99.32

0

SVM SVM-CK SOMP LSGM
89.58
83.33
95.50
88.06
94.80
72.40
60.48
94.76
90.82
92.39
99.55
96.72
47.82
91.30
99.55
98.41
44.44
50.00
90.93
72.91
85.14
97.39
92.39
86.23
100
99.47
99.65
93.73
63.45
99.71
98.82
87.05
85.11
96.18

95.83
96.82
91.20
87.62
93.74
97.62
73.91
98.86
55.56
94.26
94.73
93.84
99.47
99.05
88.01
100
95.15

89.44
97.03
88.94
100
99.57
98.83
97.65
95.31

(a)

(b)

(c)

(d)

Figure 3.3. Diﬀerence maps for AVIRIS Indian Pine data set, for the ground truth map in (a).
(b) SVM-CK [92]. (c) SOMP [82]. (d) Proposed LSGM approach.

2. Composite kernel support vector machines (SVM-CK) which considers a weighted

sum of spectral and spatial information [92]

3. Simultaneous orthogonal matching pursuit (SOMP) which involves solving Eq.

(3.9) with a 3 × 3 local window [82]

4. Proposed local-sparsity-graphical-model (LSGM) approach with the same 3 × 3

window to generate the sparse features.

Table 3.1 shows the confusion matrix in which detection and error rates are provided

with each row representing the true class of the test pixels and each column representing

the output of the speciﬁed classiﬁer. All four approaches are compared, and the pro-

Table 3.3. Classiﬁcation rates for the University of Pavia test set. LSGM z-score = −2.01.

Training

47

Class type
Asphalt
Meadows
Gravel
Trees
Metal sheets
Bare soil
Bitumen
Bricks
Shadows
Overall

548
540
392
524
265
532
375
514
231
3921

Test
6304
18146
1815
2912
1113
4572
981
3364
795
40002

SVM SVM-CK SOMP LSGM
66.55
84.01
86.10
67.50
67.49
86.72
96.94
97.32
98.83
99.28
94.62
92.65
89.70
99.18
94.44
92.24
96.10
96.73
79.24
86.38

59.49
78.31
84.13
96.30
87.78
77.45
98.67
89.00
91.70
78.75

80.20
84.99
82.37
96.33
99.82
93.35
90.21
92.95
95.85
87.33

Table 3.4. Classiﬁcation rates for the Center of Pavia test set. LSGM z-score = −2.17.

Class type Training
Water
Trees
Meadow
Brick
Soil
Asphalt
Bitumen
Tile
Shadow
Overall

745
785
797
485
820
678
808
223
195
5536

Test
64533
5722
2094
1667
5729
6847
6479
2899
1970
97940

SVM SVM-CK SOMP LSGM
99.19
99.44
92.99
77.74
96.99
86.72
87.28
40.37
97.52
97.64
94.54
94.77
96.99
74.37
99.90
98.94
100
99.34
98.20
94.63

99.38
91.98
95.89
86.44
96.75
93.79
95.06
99.83
98.48
97.82

97.61
92.99
97.37
79.60
98.65
94.37
97.53
99.86
99.89
96.97

posed LSGM methods oﬀers better target detection performance. Improvements over

SOMP can be attributed to the use of an explicit discriminative classiﬁer in LSGM. All

approaches identify the background class with a reasonably high degree of accuracy.

Fig. 3.2 shows the ROC curve for the detection problem. The ROC curve describes

the probability of detection (PD) as a function of the probability of false alarms (PFA). To

calculate the ROC curve, a large number of thresholds are chosen between the minimum

and maximum of the detector output, and class labels for all test pixels are determined

at each threshold. The PFA is calculated as the ratio of the number of false alarms

(background pixels determined as target) to the total number of pixels in the test region,

while the PD is the ratio of the number of hits (target pixels correctly determined as

target) to the total number of true target pixels. It can be seen that the proposed LSGM

approach oﬀers the best overall detection performance.

48

(a)

(b)

(c)

Figure 3.4. Performance of diﬀerent approaches as a function of number of training samples
provided. (a) AVIRIS image, (b) University of Pavia image, (c) Center of Pavia image. For
each image, the density function of the classiﬁcation rates obtained for ten diﬀerent random
realizations of training is plotted on the right.

49

3.4.2 Hyperspectral Target Classiﬁcation

Hyperspectral target classiﬁcation is a generalization of the binary detection problem to

multiple classes. Here, we compare our proposed LSGM approach with three competi-

tive methods: (i) spectral features-based SVM classiﬁer [91, 100], (ii) composite kernel

support vector machines (SVM-CK) [92], and (iii) joint sparsity model (SOMP) [82]. In

SVM-CK, two types of kernels are used: a spectral kernel Kω for the spectral (pixel)
features (in R200) and a spatial kernel Ks for spatial features (in R400) which are formed
by the mean and standard deviation of pixels in a neighborhood per spectral channel.

A polynomial kernel (order d = 7) is used for spectral features, while the RBF kernel

is used for the spatial features. The σ parameter for the RBF kernel and SVM regular-

ization parameter C are selected by cross-validation. The weighted summation kernel,
K = µKs + (1 − µ)Kω, eﬀectively captures spectral and contextual spatial information,
with the optimal choice µ = 0.4 determined by cross-validation. A 5 × 5 window is used
for the neighborhood kernels. Parameters for SOMP are chosen as described in [82].
The proposed LSGM approach uses a local window of dimension 3 × 3. For fairness of
comparison, results for SOMP are also presented for the same window dimension.

We perform experiments using 3 distinct HSI data sets. Note that two ﬂavors of the

results are reported:

1. Tables 3.2-3.4 show classiﬁcation rates for carefully selected or good training sam-

ples which amount to about 10% of available data (typical of training choices

in [82, 92])

2. Figs. 3.4(a)-(c) where performance is plotted as a function of training set size and

results averaged from multiple (10) random training runs. In each sub-ﬁgure, the

plot on the right-hand side characterizes the distribution of the classiﬁcation rates

(modeled as a random variable whose value emerges as an outcome of a given run,

and ﬁt to a Gaussian). Further, we establish the statistical signiﬁcance of our

results by computing LSGM z-scores for each data set.

3.4.2.1 AVIRIS Data Set: Indian Pines

The ﬁrst hyperspectral image in our experiments is the Airborne Visible/Infrared Imag-

ing Spectrometer (AVIRIS) Indian Pines image [101]. The AVIRIS sensor generates 220

bands across the spectral range from 0.2 to 2.4 µm, of which only 200 bands are consid-

ered by removing 20 water absorption bands [100]. This image has spatial resolution of

50

20m per pixel and spatial dimension 145 × 145. For well-chosen training samples, dif-
ference maps obtained using the diﬀerent approaches are shown in Figs. 3.3(b)-(d), and

classiﬁcation rates for each class as well as overall accuracy are shown in Table 3.2. The

improvement over SOMP indicates the beneﬁts of using a discriminative classiﬁer instead

of reconstruction residuals for class assignment, while still retaining the advantages of

exploiting spatio-spectral information.

Fig. 3.4(a) compares algorithm performance as a function of training set size. Our

LSGM approach outperforms the competing approaches, and the diﬀerence is particu-

larly signiﬁcant in the low training regime. As expected, overall classiﬁcation accuracy

decreases when number of training samples is reduced. That said, LSGM oﬀers a more

graceful degradation in comparison to other approaches. From the density function plot,

we see that average classiﬁcation rate is the highest for LSGM, consistent with the plot on

the left-hand side in Fig. 3.4(a). Further, variance is the lowest for LSGM, underlining

its improved robustness against particular choice of training samples.

3.4.2.2 ROSIS Urban Data Over Pavia, Italy

The next two hyperspectral images, University of Pavia and Center of Pavia, are urban

images acquired by the Reﬂective Optics System Imaging Spectrometer (ROSIS). The

ROSIS sensor generates 115 spectral bands ranging from 0.43 to 0.86 m and has a spatial
resolution of 1.3 m per pixel. The University of Pavia image consists of 610× 340 pixels,
each having 103 bands with the 12 noisiest bands removed. The Center of Pavia image
consists of 1096 × 492 pixels, each having 102 spectral bands after 13 noisy bands are
removed. For these two images, we repeat the experimental scenarios tested in Section

3.4.2.1.

Classiﬁcation rates for the two ROSIS images are provided in Tables 3.3 and 3.4

respectively, for the scenario of well-chosen training samples. In Table 3.3, the SVM-CK

technique performs marginally better than LSGM in the sense of overall classiﬁcation

accuracy. However, for most individual classes LSGM does better and particularly in

cases where training sample size is smaller. In Table 3.4, LSGM performs better than

SOMP as well as SVM-CK. From Figs. 3.4(b)-(c), we observe that the LSGM improves

upon the performance of SOMP and SVM-CK by about 4%, while the improvements

over the baseline SVM classiﬁer are even more pronounced.

The z-score for LSGM on the AVIRIS image is −2.13, which indicates that with a
high probability (= 0.983), any random selection of training samples will give results

similar to the values in Table 3.2. For the University of Pavia and Center of Pavia

51

images, z-scores are −2.01 and −2.17 respectively. The negative sign merely indicates
that the experimental value is lesser than the the most likely value (Gaussian mean).

3.5 Robust Face Recognition

3.5.1

Introduction

The problem of automatic face recognition has witnessed considerable research activity

in the image processing and computer vision community over the past two decades. Its

signiﬁcance can be gauged by the variety of practical applications that employ face recog-

nition, like video surveillance systems, biometrics to control access to secure facilities,

and online face search in social networking applications. The diversity of facial image

captures, due to varying illumination conditions, pose, facial expressions, occlusion and

disguise, oﬀers a major challenge to the success of any automatic human face recognition

system. A comprehensive survey of face recognition methods in literature is provided

in [102].

One of the most popular dimensionality-reduction techniques used in computer vision

is principal component analysis (PCA). In face recognition, PCA-based approaches have

led to the use of eigenpictures [103] and eigenfaces [104] as features. Other approaches

have used local facial features [105] like the eyes, nose and mouth, or incorporated geo-

metrical constraints on features through structural matching. An important observation

is that diﬀerent (photographic) versions of the same face approximately lie in a linear

subspace of the original image space [106–109]. A variety of classiﬁers have been pro-

posed for face recognition, ranging from template correlation to nearest neighbor and

nearest subspace classiﬁers, neural networks and support vector machines [110].

3.5.2 Motivation

Recently, the merits of exploiting parsimony in signal representation and classiﬁcation

have been demonstrated in [7, 81, 111]. The sparsity-based face recognition algorithm [7]

yields markedly improved recognition performance over traditional eﬀorts in face recog-

nition under various conditions, including illumination, disguise, occlusion, and random

pixel corruption. In many real world scenarios, test images for identiﬁcation obtained by

face detection algorithms are not perfectly registered with the training samples in the

databases. The sparse subspace assumption in [7], however, requires the test face image

to be well aligned to the training data prior to classiﬁcation. Recent approaches have at-

tempted to address this misalignment issue in sparsity-based face recognition [8,112,113],

52

(a)

(b)

Figure 3.5. Representation of a block in the test image from a locally adaptive dictionary.
(a) The blocks in the test and training images (only one training sample is displayed). (b) Sparse
representation yyyij = DDDijαααij.

usually by jointly optimizing the registration parameters and sparse coeﬃcients and thus

leading to more complex systems.

In order to overcome this constraint for application to practical face recognition sys-

tems, Chen et al. [114] proposed a locally adaptive sparse representation-based approach,

inspired by the inter-frame sparsity model and the observation that local image features

are often more beneﬁcial than global features in image processing applications. This is

illustrated in Fig. 3.5. Accordingly, a (vectorized) local block yyyij (indexed by its top-left

pixel location (i, j)) in a new test image is represented by a sparse linear combination of

similar blocks from the training images located within the same spatial neighborhood,

yyyij = DDDijαααij,

(3.10)

canbesparselyrepresentedbyfewneighboringblocksinrefer-enceframes.Fig.1(a)illustratestheproposedmethodofrepre-sentingablockinthetestfaceimageYYYfromalocallyadaptivedictionaryconsistingofneighboringblocksinthetrainingimages{XXXt}t=1,...,Tinthesamephysicalarea,whereT=(cid:229)Kk=1Nkisthetotalnumberoftrainingsamples(onlyonetrainingimageisshowninFig.1).Tobemorespeciﬁc,letyyyijbeanMN-dimensionalvec-torrepresentingthevectorizedM×Nblockinthetestimagewiththeupperleftpixellocatedat(i,j).DeﬁnethesearchregionSSStijtobethe(M+2△M)×(N+2△N)blockinthetthtrainingimageXXXtas:SSStij=xti−△M,j−△N···xti−△M,j+N−1+△N.........xti+M−1+△M,j−△N···xti+M−1+△M,j+N−1+△N.FromthesearchregionsofallTtrainingimages,wecanconstructthedictionaryDDDijfortheblockyyyijasDDDij=hDDD1ijDDD2ij···DDDTiji,whereeachDDDtij=(cid:2)dddti−△M,j−△Ndddti−△M,j−△N+1···dddti+△M,j+△N(cid:3)isan(MN)×(cid:0)(2△M+1)(2△N+1)(cid:1)matrixwhosecolumnsarethevectorizedblocksinthetthtrainingimagedeﬁnedinthesamewayasyyyij.ThedictionaryDDDijislocallyadaptiveandchangesfromblocktoblock.Thesizeofthedictionarydependsonthenon-stationarybehaviorofthedataaswellasthelevelofcomputationalcomplexitywecanafford.Inthepresenceofregistrationerror,thetestimageYYYmaynolongerlieinthesubspacespannedbythetrainingsamples{XXXt}t.Attheblocklevel,however,yyyijcanstillbeapproximatebytheblocksinthetrainingsamplesndddtijot,i,j.Com-paredtotheoriginalapproach,thedictionaryDDDijbettercapturesthelocalcharacteristics.Notethatourapproachisquitediffer-entfrompatch-baseddictionarylearning[10]fromseveralangles:(i)weemphasizethelocaladaptivityofthedictionaries;and(ii)dictionariesinourapproacharedirectlyobtainedfromthedatawithoutanycomplicatedlearningprocess.WeproposethattheblockyyyijinthemisalignedimageYYYcanbesparselyapproximatedbyalinearcombinationofafewatomsinthedictionaryDDDij:yyyij=DDDijaaaij,(3)whereaaaijissparsevector,asillustratedinFig.1(b).Thesparsevectorcanberecoveredbysolvingtheminimalℓ0-normproblemˆaaaij=argmin(cid:13)(cid:13)aaaij(cid:13)(cid:13)0subjecttoDDDijaaaij=yyyij.(4)Sinceoursparserecoveryisperformedonasmallblockofdatawithamodestsizedictionary,theresultingcomplexityoftheover-allalgorithmismanageable.Afterthesparsevectorˆaaaijisob-tained,theidentityofthetestblockcanbedeterminedbytheerrorresidualsbyidentity(yyyij)=argmink=1,...,K(cid:13)(cid:13)yyyij−DDDijdddk(cid:0)ˆaaaij(cid:1)(cid:13)(cid:13)2,(5)wheredddk(cid:0)ˆaaaij(cid:1)isasdeﬁnedin(2).Toimprovetherobustness,weproposetoemploymultipleblocks,classifyeachblockindividually,andthencombinetheclassiﬁcationresults.Theblocksmaybechosencompletelyatrandom,ormanuallyinthemorerepresentativeareas(suchastheregionaroundeyes)orareaswithhighSNR,orexhaustivelyintheentiretestimage(non-overlappedoroverlapped).Notethatsinceeachblockishandledindependently,theycanbeprocessedinparallel.Also,sinceblockscanbeoverlapped,ourproposedalgorithmiscomputationallyscalable-morecomputationdeliversbetterrecognitionresult.(i,j)NMtestimageYYYblockyyyijtrainingimageXXXt△N△M···...searchrangecandidateblockdddtij(a)yyyij=···dddtij···DDDij...αααijzerononzero(b)Fig.1.Representationofablockinthetestimagefromalocallyadaptivedictionary.(a)Theblocksinthetestandtrainingimages(onlyonetrainingsampleisdisplayed).(b)Sparserepresentationyyyij=DDDijaaaij.Oncetherecognitionresultsareobtainedforallblocks,theycanbecombinedbymajorityvoting.LetLbethenumberofblocksinthetestimageYYY,and{yyyl}l=1,...,LbetheLblocks.Then,bymajorityvotingidentity(YYY)=maxk=1,...,K|{l=1,...,L:identity(yyyl)=k}|,where|S|denotesthecardinalityofasetSandidentity(yyyl)isde-terminedby(5).Maximumlikelihoodisanalternativewaytofusetheclassiﬁ-cationresultsfrommultipleblocks.Forablockyyyl,itssparserep-resentationˆaaalobtainedbysolving(4),andthelocaldictionaryDDDl,wedeﬁnetheprobabilityofyyylbelongingtothekthclasstobein-verselyproportionaltotheresidualassociatedwiththedictionaryatomsinthekthclass:pkl=P(identity(yyyl)=k)=1/rkl(cid:229)Kk=1(cid:0)1/rkl(cid:1),(6)whererkl=kyyyl−DDDldddk(ˆaaal)k2istheresidualassociatedwiththekthclassandthevectordddk(ˆaaal)isasdeﬁnedin(5).Then,theidentityofthetestimageYYYisgivenbyidentity(YYY)=argmaxk=1,...,Klog L(cid:213)l=1pkl!.(7)Themaximumlikelihoodapproachcanalsobeusedasameasuretorejectoutliers,asforanoutliertheprobabilityofitbelongingtosomeclasstendstobeuniformlydistributedamongallclassesinthetrainingdata.Fig.2illustratesanexampleoftheproposedapproachwithmultipleblocks.ThetestandtrainingimagesaretakenfromtheExtendedYaleBDatabase[11]whichconsistsoffaceimagesofcanbesparselyrepresentedbyfewneighboringblocksinrefer-enceframes.Fig.1(a)illustratestheproposedmethodofrepre-sentingablockinthetestfaceimageYYYfromalocallyadaptivedictionaryconsistingofneighboringblocksinthetrainingimages{XXXt}t=1,...,Tinthesamephysicalarea,whereT=(cid:229)Kk=1Nkisthetotalnumberoftrainingsamples(onlyonetrainingimageisshowninFig.1).Tobemorespeciﬁc,letyyyijbeanMN-dimensionalvec-torrepresentingthevectorizedM×Nblockinthetestimagewiththeupperleftpixellocatedat(i,j).DeﬁnethesearchregionSSStijtobethe(M+2△M)×(N+2△N)blockinthetthtrainingimageXXXtas:SSStij=xti−△M,j−△N···xti−△M,j+N−1+△N.........xti+M−1+△M,j−△N···xti+M−1+△M,j+N−1+△N.FromthesearchregionsofallTtrainingimages,wecanconstructthedictionaryDDDijfortheblockyyyijasDDDij=hDDD1ijDDD2ij···DDDTiji,whereeachDDDtij=(cid:2)dddti−△M,j−△Ndddti−△M,j−△N+1···dddti+△M,j+△N(cid:3)isan(MN)×(cid:0)(2△M+1)(2△N+1)(cid:1)matrixwhosecolumnsarethevectorizedblocksinthetthtrainingimagedeﬁnedinthesamewayasyyyij.ThedictionaryDDDijislocallyadaptiveandchangesfromblocktoblock.Thesizeofthedictionarydependsonthenon-stationarybehaviorofthedataaswellasthelevelofcomputationalcomplexitywecanafford.Inthepresenceofregistrationerror,thetestimageYYYmaynolongerlieinthesubspacespannedbythetrainingsamples{XXXt}t.Attheblocklevel,however,yyyijcanstillbeapproximatebytheblocksinthetrainingsamplesndddtijot,i,j.Com-paredtotheoriginalapproach,thedictionaryDDDijbettercapturesthelocalcharacteristics.Notethatourapproachisquitediffer-entfrompatch-baseddictionarylearning[10]fromseveralangles:(i)weemphasizethelocaladaptivityofthedictionaries;and(ii)dictionariesinourapproacharedirectlyobtainedfromthedatawithoutanycomplicatedlearningprocess.WeproposethattheblockyyyijinthemisalignedimageYYYcanbesparselyapproximatedbyalinearcombinationofafewatomsinthedictionaryDDDij:yyyij=DDDijaaaij,(3)whereaaaijissparsevector,asillustratedinFig.1(b).Thesparsevectorcanberecoveredbysolvingtheminimalℓ0-normproblemˆaaaij=argmin(cid:13)(cid:13)aaaij(cid:13)(cid:13)0subjecttoDDDijaaaij=yyyij.(4)Sinceoursparserecoveryisperformedonasmallblockofdatawithamodestsizedictionary,theresultingcomplexityoftheover-allalgorithmismanageable.Afterthesparsevectorˆaaaijisob-tained,theidentityofthetestblockcanbedeterminedbytheerrorresidualsbyidentity(yyyij)=argmink=1,...,K(cid:13)(cid:13)yyyij−DDDijdddk(cid:0)ˆaaaij(cid:1)(cid:13)(cid:13)2,(5)wheredddk(cid:0)ˆaaaij(cid:1)isasdeﬁnedin(2).Toimprovetherobustness,weproposetoemploymultipleblocks,classifyeachblockindividually,andthencombinetheclassiﬁcationresults.Theblocksmaybechosencompletelyatrandom,ormanuallyinthemorerepresentativeareas(suchastheregionaroundeyes)orareaswithhighSNR,orexhaustivelyintheentiretestimage(non-overlappedoroverlapped).Notethatsinceeachblockishandledindependently,theycanbeprocessedinparallel.Also,sinceblockscanbeoverlapped,ourproposedalgorithmiscomputationallyscalable-morecomputationdeliversbetterrecognitionresult.(i,j)NMtestimageYYYblockyyyijtrainingimageXXXt△N△M···...searchrangecandidateblockdddtij(a)yyyij=···dddtij···DDDij...αααijzerononzero(b)Fig.1.Representationofablockinthetestimagefromalocallyadaptivedictionary.(a)Theblocksinthetestandtrainingimages(onlyonetrainingsampleisdisplayed).(b)Sparserepresentationyyyij=DDDijaaaij.Oncetherecognitionresultsareobtainedforallblocks,theycanbecombinedbymajorityvoting.LetLbethenumberofblocksinthetestimageYYY,and{yyyl}l=1,...,LbetheLblocks.Then,bymajorityvotingidentity(YYY)=maxk=1,...,K|{l=1,...,L:identity(yyyl)=k}|,where|S|denotesthecardinalityofasetSandidentity(yyyl)isde-terminedby(5).Maximumlikelihoodisanalternativewaytofusetheclassiﬁ-cationresultsfrommultipleblocks.Forablockyyyl,itssparserep-resentationˆaaalobtainedbysolving(4),andthelocaldictionaryDDDl,wedeﬁnetheprobabilityofyyylbelongingtothekthclasstobein-verselyproportionaltotheresidualassociatedwiththedictionaryatomsinthekthclass:pkl=P(identity(yyyl)=k)=1/rkl(cid:229)Kk=1(cid:0)1/rkl(cid:1),(6)whererkl=kyyyl−DDDldddk(ˆaaal)k2istheresidualassociatedwiththekthclassandthevectordddk(ˆaaal)isasdeﬁnedin(5).Then,theidentityofthetestimageYYYisgivenbyidentity(YYY)=argmaxk=1,...,Klog L(cid:213)l=1pkl!.(7)Themaximumlikelihoodapproachcanalsobeusedasameasuretorejectoutliers,asforanoutliertheprobabilityofitbelongingtosomeclasstendstobeuniformlydistributedamongallclassesinthetrainingdata.Fig.2illustratesanexampleoftheproposedapproachwithmultipleblocks.ThetestandtrainingimagesaretakenfromtheExtendedYaleBDatabase[11]whichconsistsoffaceimagesof53

where αααij is a sparse vector and DDDij is an adaptive dictionary. The sparse vector is

recovered by solving the following optimization problem:

ˆαααij = arg min(cid:107)αααij(cid:107)0 subject to (cid:107)yyyij − DDDijαααij(cid:107)2 < ,

(3.11)

and the class label is determined using the reconstruction residual, similar to the global

sparsity approach. To enhance robustness to distortions, multiple local blocks are chosen

and the sparse recovery problem is solved for each block individually. Each local block

is assigned the label of the class with minimum residual error, and class assignments

from all such local blocks are combined either by majority voting or heuristic maximum

likelihood-type approaches.

Our contribution is the development of a discriminative graphical model classiﬁer to

combine the statistically correlated local sparse features from informative local regions

of the face - eyes, nose and mouth.

3.6 Face Recognition Via Local Decisions From Locally

Adaptive Sparse Features

Fig. 3.6 shows the overall discriminative graphical model framework for face recognition.

The feature extraction process in Stage 1 is designed diﬀerently for this application. We

build local dictionaries using the idea described in 3.5.2 and separately extract sparse

representations corresponding to the eyes, nose and mouth. Since these features are

conditionally correlated (they correspond to the same face), they form a suitable set

of multiple feature representations which can be fused using our discriminative graph-

ical model classiﬁer. The procedure in Stage 2 is identical to the procedure described

previously in Chapter 2 and in the application to hyperspectral imaging earlier in this

chapter. To handle multiple classes, the graphs are learned in a one-versus-all manner

and the test image is identiﬁed with the class that maximizes the likelihood ratio.

3.7 Experiments and Discussion

We test the proposed algorithm on the Extended Yale B database [115], which consists
of 2414 perfectly-aligned frontal face images of size 192× 168 of 38 individuals, 64 images
per individual, under various conditions of illumination. In our experiments, for each

subject we randomly choose 32 images in Subsets 1 and 2, which were taken under

less extreme lighting conditions, as the training data. The remaining images are used

54

Figure 3.6. Proposed framework for face recognition: (a) Target face image, (b) Local regions
for extracting sparse features, (c) Initial pairs of tree graphs for each feature set, (d) Initial
sparse graph formed by tree concatenation, (e) Final pair of thickened graphs; newly learned
edges represented by dashed lines, (f) Graph-based inference. In (c)-(e), the graphs on the left
and right correspond to distributions p (class Ci) and q (class ˜Ci) respectively.

as test data, after introducing some misalignment. All training and test samples are
downsampled to size 32 × 28.

We compare our LSGM technique against ﬁve popular face recognition algorithms:

(i) sparse representation-based classiﬁcation (SRC) [7], (ii) Eigenfaces [104] as features

with nearest subspace [116] classiﬁer (Eigen-NS), (iii) Eigenfaces with support vector ma-

chine [90] classiﬁer (Eigen-SVM), (iv) Fisherfaces [107] as features with nearest subspace

classiﬁer (Fisher-NS), and (v) Fisherfaces with SVM classiﬁer (Fisher-SVM).

3.7.1 Presence of Registration Errors

First, we show experimental results for test images under rotation and scaling operations.

Test images are randomly rotated by an angle between -20 and 20 degrees, as illustrated

55

Figure 3.7. An example of rotated test images. (a) Original image and (b) the image rotated
by 20 degrees clockwise.

(a)

(b)

Figure 3.8. Recognition rate for rotated test images.

by the example in Fig. 3.7. Fig. 3.8 shows the recognition rate (y-axis) for each rotation

degree (x-axis). We see that LSGM outperforms the SRC approach by a signiﬁcant

margin for the case of severe misalignment.

For the second set of experiments, the test images are stretched in both directions by

scaling factors up to 1.313 vertically and 1.357 horizontally. An example of an aligned

image in the database and its distorted version to be tested are shown in Fig. 3.9.

The beneﬁts of LSGM over SRC are apparent from Tables 3.5 and 3.6 which show the

percentage of correct identiﬁcation with various scaling factors. Finally, we compare the

performance of our LSGM approach with ﬁve other algorithms: SRC, Eigen-NS, Eigen-

SVM, Fisher-NS and Fisher-SVM, for the scenario where the test images are scaled by

a horizontal factor of 1.214 and a vertical factor of 1.063. The overall recognition rates

are shown in Table 3.7.

56

Figure 3.9. An example of scaled test images. (a) Original image and (b) the image scaled by
1.313 vertically and 1.357 horizontally.

(a)

(b)

Table 3.5. Recognition rate (in percentage) for scaled test images using SRC [7] under various
scaling factors (SF).

SF
1
1.063
1.125
1.188
1.25
1.313

1
100
99.7
83.8
54.5
36.1
31.5

1.071
100
96.5
70.2
43.7
27.2
24.3

1.143
98.0
86.1
49.8
26.8
20.9
16.7

1.214
88.2
68.5
33.6
20.0
16.6
13.9

1.286
76.5
50.3
26.2
18.0
12.3
10.6

1.357
58.8
37.6
17.9
12.6
11.3
9.8

3.7.2 Recognition Under Random Pixel Corruption

We randomly corrupt 50% of the image pixels in each test image. In addition, each test

image is scaled by a horizontal factor of 1.071 and a vertical factor of 1.063. Local sparse

features are extracted using the robust form of the (cid:96)1-minimization similar to the ap-

proach in [7]. The overall recognition rates are shown in Table 3.8. These results reveal

that under the mild scaling distortion scenario, our LSGM approach retains the robust-

ness characteristic of the global sparsity approach (SRC), while the other competitive

algorithms suﬀer drastic degradation in performance.

3.7.3 Outlier Rejection

In this experiment, samples from 19 of the 38 classes in the Yale database are included in

the training set, and faces from the other 19 classes are considered outliers. For training,
15 samples per class from Subsets 1 and 2 are used (19 × 15 = 285 samples in total),
while 500 samples are randomly chosen for testing, among which 250 are inliers and the

other 250 are outliers. All test samples are rotated by ﬁve degrees.

The ﬁve diﬀerent competing approaches are compared with our proposed LSGM

method. For the LSGM approach, we use a minimum threshold δ in the decision rule.

If the maximum value of the log-likelihood ratio does not exceed δ, the corresponding

test sample is labeled an outlier.

In the SRC approach, the Sparsity Concentration

Table 3.6. Recognition rate (in percentage) for scaled test images using proposed block-based
approach under various SF.

57

SF
1
1.063
1.125
1.188
1.25
1.313

1
98.8
97.5
97.4
94.9
94.9
90.7

1.071
98.2
96.7
96.5
92.9
93.0
90.4

1.143
98.5
96.0
96.2
91.6
92.2
84.1

1.214
97.5
96.0
95.2
89.4
87.9
81.0

1.286
97.5
93.5
93.2
87.1
82.0
75.5

1.357
97.2
93.4
91.1
83.3
77.8
64.2

Table 3.7. Overall recognition rate (as a percentage) for the scenario of scaling by horizontal
and vertical factors of 1.214 and 1.063 respectively.

Method
LSGM
SRC

Eigen-NS
Eigen-SVM
Fisher-NS
Fisher-SVM

Recognition rate (%)

89.4
60.8
55.5
56.7
54.1
57.1

Index is used as the criterion for outlier rejection. For the other approaches under

comparison which use the nearest subspace and SVM classiﬁers, reconstruction residuals

are compared to a threshold to decide outlier rejection. The ROC curves for all the

approaches are shown in Fig. 3.10. LSGM oﬀers the best performance, while some of

the approaches are actually worse than random guessing.

3.8 Conclusion

In this chapter, we have demonstrated two additional applications of our graph-based

feature fusion framework ﬁrst introduced in Chapter 2. The novelty of our contribution

is in the design of multiple sets of sparse representations that can then be fused via

discriminative trees. The consistently superior performance of our approach in a variety

of experimental scenarios oﬀers proof of the wide ﬂexibility of the fusion framework.

The two central ideas in this dissertation are those of probabilistic graphical models and

the theory of sparse signal representations. The contributions in this chapter constitute

our ﬁrst attempt to bring the discriminative beneﬁts of these two approaches together

for robust image classiﬁcation. Looking ahead, the latter half of the following chapter

formalizes this relationship by learning graphical spike-and-slab priors directly for sparse

features in a Bayesian set-up.

58

Table 3.8. Overall recognition rate (as a percentage) for the scenario where test images are
scaled and subjected to random pixel corruption.

Method
LSGM
SRC

Eigen-NS
Eigen-SVM
Fisher-NS
Fisher-SVM

Recognition rate (%)

96.3
93.2
54.3
58.5
56.2
59.9

Figure 3.10. ROC curves for outlier rejection.

Chapter 4

Structured Sparse Representations
for Robust Image Classiﬁcation

4.1

Introduction

This chapter primarily concerns itself with learning discriminative models on sparse

signal representations. As outlined in Chapter 1, our goal is to understand the discrim-

inative structure in multiple feature representations from the standpoint of improving

robustness in image classiﬁcation. Here, we learn models on sparse feature representa-

tions in interesting ways.

First, we introduce a simultaneous sparsity model for classiﬁcation scenarios that are

multi-modal in nature. Example real-world manifestations of this scenario occur in the

form of spatially local pixels for hyperspectral target classiﬁcation [82], multiple feature

sets for automatic image annotation [117], kernel features for object categorization, or

as query images for video-based face recognition [118]. In the latter two examples, each

event is in fact represented using multiple heterogeneous sources, resulting in multi-task

versions of SRC [7]. We consider a speciﬁc instantiation of the multi-task framework for

the classiﬁcation of color medical images acquired by histopathology. Digital histopatho-

logical images are comprised of three color channels - red, green and blue. The tissue

staining process central to histopathology imbues the images with prominent red and

blue hues. In fact, color information is used as a crucial visual cue by pathologists to

identify whether a tissue is healthy or diseased. Our simultaneous sparsity model builds

color-speciﬁc dictionaries and solves for sparse coeﬃcients with constraints guided by

the color channel correlations.

60

The success of SRC comes with a caveat. The validity of the linear representation

model rests on the supposition that if enough diversity (in imaging conditions, for exam-

ple) is captured by the set of training images, any new test image can be approximated

very well using a small number of training images. In practice however, many applica-

tions have the limitation that rich training is not available a priori. Examples include

automatic target recognition using radar images and hyperspectral target classiﬁcation.

The linear representation model assumption is violated in the regime of low training.

This important issue concerning sparsity-based classiﬁcation methods has not been in-

vestigated thoroughly so far.

Accordingly, the second half of this chapter explores another way of learning discrim-

inative models on sparse coeﬃcients, this time in the form of class-speciﬁc probabilistic

priors. We learn class-speciﬁc parameters for spike-and-slab priors, which have been suc-

cessful in modeling sparse signals. The main advantage of working in a Bayesian set-up is

the robustness to training insuﬃciency. We also show that hierarchical extensions of the

spike-and-slab prior lead to new optimization formulations that capture group sparsity

structure for multi-task scenarios.

4.2 Histopathological Image Classiﬁcation: Overview

The advent of digital pathology [119] has ushered in an era of computer-assisted di-

agnosis and treatment of medical conditions based on the analysis of medical images.

Of active research interest is the development of quantitative image analysis tools to

complement the eﬀorts of radiologists and pathologists towards better disease diagnosis

and prognosis. This research thrust has been fueled by a variety of factors, including

the availability of large volumes of patient-related medical data, dramatic improvements

in computational resources (both hardware and software), and algorithmic advances in

image processing, computer vision and machine learning theory. An important emerging

sub-class of problems in medical imaging pertains to the analysis and classiﬁcation of

histopathological images [120–123]. Whole slide digital scanners process tissue slides to

generate these digital images. Examples of histopathological images are shown in Figs.

4.5 and 4.6. It is evident that these images carry rich structural information, making

them invaluable for the diagnosis of many diseases including cancer [124–126].

61

4.2.1 Prior Work

Pathologists often look for visual cues at the nuclear and cellular level in order to cat-

egorize a tissue image as either healthy or diseased. Motivated by this, a variety of

low-level image features have been developed based on texture, morphometric character-

istics (shape and spatial arrangement) and image statistics. The gray level co-occurrence

matrix by Haralick et al. [127] estimates the texture of an image in terms of the dis-

tribution of co-occurring pixel intensities at speciﬁed oﬀset positions. Morphological

image features [128] have been used in medical image segmentation for detection of

vessel-like patterns [129]. Image histograms are a popular choice of features for med-

ical imaging [130]. Wavelet features have been deployed for prostate cancer diagnosis

in [131]. Esgiar et al. [132] have captured the self-similarity in colon tissue images using

fractal-based features. Tabesh et al. [133] have combined color, texture and morpho-

metric features for prostate cancer diagnosis. Doyle et al. [134] introduced graph-based

features using Delaunay triangulation and minimum spanning trees to exploit spatial

structure. Orlov et al. [135, 136] have recently proposed a multi-purpose feature set

that aggregates transform domain coeﬃcients, image statistics and texture information.

Experimental success in many diﬀerent classiﬁcation problems has demonstrated the ver-

satility of this feature set. It must be mentioned that all the features discussed above are

applicable broadly for image analysis and have been particularly successful in medical

imaging. For classiﬁcation, these features are combined with powerful classiﬁers such

as SVMs [90, 130] and boosting [31, 137]. A comprehensive discussion of features and

classiﬁers for histopathological analysis is provided in [122].

4.2.2 Motivation and Challenges

While histopathology shares some commonalities with other popular imaging modali-

ties such as cytology and radiology, it also exhibits two principally diﬀerent character-

istics [138] that pose challenges to image analysis. First, histopathological images are

invariably multi-channel in nature (commonly using three color channels - red, green and

blue (RGB)). Key geometric information is spread across the color channels. It is well

known that color information in the hematoxylin-eosin (H&E) stained slides is essential

to identify the discriminative image signatures of healthy and diseased tissue [139, 140].

Speciﬁcally, the nuclei assume a bluish tinge due to hematoxylin, while the cytoplasmic

structures and connective tissue appear red due to eosin. As seen from Fig. 4.5, there is a

higher density of nuclei in diseased tissue. Typically in histopathological image analysis,

62

features are extracted from each color channel of the images [133], and the classiﬁer de-

cisions based on the individual feature sets are then fused for classiﬁcation. Alternately,

only the luminance channel information - image edges resulting mainly from illumination

variations - is considered [140]. The former approach ignores the inherent correlations

among the RGB channels while the latter strategy fails to exploit chrominance channel

geometry, i.e. the edges and image textures caused by objects with diﬀerent chrominance.

The second challenge posed by histopathology is the relative diﬃculty in obtaining

good features for classiﬁcation due to the geometric richness of tissue images. Tissues

from diﬀerent organs have structural diversity and often, the objects of interest occur at

diﬀerent scales and sizes [122]. As a result, features are usually customized for speciﬁc

classiﬁcation problems, most commonly cancer of the breast and prostate.

In this chapter, we address both these challenges through a novel simultaneous spar-

sity model inspired by recent work using sparse representations for image classiﬁca-

tion [7]. SRC has been proposed earlier for single-channel medical images, in cervigram

segmentation [79, 141] and colorectal polyp and lung nodule detection [142]. To the

best of our knowledge, ours is the ﬁrst discriminative sparsity model for multi-channel

histopathological images.

4.2.3 Overview of Contributions

The relevance of color information for discriminative tasks has been identiﬁed previ-

ously [143, 144]. We propose a new simultaneous Sparsity model for multi-channel

Histopathological Image Representation and Classiﬁcation (SHIRC). Essentially, our

model recognizes the diversity of information in multiple color channels and extends

the standard SRC approach [7, 8, 79, 82, 141, 142, 145] by designing three color dictionar-

ies, corresponding to the RGB channels. Each multi-channel histopathological image is

represented as a sparse linear combination of training examples under suitable channel-

wise constraints which capture color correlation information. The constraints agree with

intuition since a sparse linear model for a color image necessitates identical models for

each of its constituent color channels with no cross-channel contributions.

Our approach considers a multi-task scenario that is qualitatively similar to the

visual classiﬁcation problem addressed very recently by Yuan et al. [118]. In [118], three

types of image features - containing texture, shape and color information - are extracted

from images and a joint sparsity model is proposed to classify the images. The joint

(simultaneous) sparsity model employed in [118] and the one we develop are however

signiﬁcantly diﬀerent. First, [118] does not consider the problem of multi-channel or

63

color image modeling. Second and more crucially, the cost function in [118] is a sum

of reconstruction error terms from each of the feature dictionaries which results in the

commonly seen row sparsity structure on the sparse coeﬃcient matrix. The resulting

optimization problem is solved using the popular Accelerated Proximal Gradient (APG)

method [146]. In our work however, to conform to imaging physics, we introduce color

channel-speciﬁc constraints on the structure of the sparse coeﬃcients, which do not

directly conform to row sparsity, leading to a new optimization problem. This in turn

requires a modiﬁed greedy matching pursuit approach to solve the problem.

As discussed earlier, feature design in medical imaging is guided by the object-based

perception of pathologists. Depending on the type of tissue, the objects could be nuclei,

cells, glands, lymphocytes, etc. Crucially it is the presence or absence of these local

objects in an image that matters to a pathologist; their absolute spatial location mat-

ters much less. As a result, the object of interest may be present in the test image as

well as the representative training images, albeit at diﬀerent spatial locations, causing a

seeming breakdown of the image-level SHIRC. This scenario can occur in practice if the

acquisition process is not carefully calibrated. So we infuse the SHIRC with a robust

locally adaptive ﬂavor by developing a Locally Adaptive SHIRC (LA-SHIRC). We rely

on the pathologist’s insight to carefully select multiple local regions that contain these

objects of interest from training as well as test images and use them in our linear sparsity

model instead of the entire images. Local image features often possess better discrimi-

native power than image-level features [147]. LA-SHIRC is a well-founded instantiation

of this idea to resolve the issue of correspondence between objects at diﬀerent spatial

locations. LA-SHIRC oﬀers ﬂexibility in the number of local blocks chosen per image

and the size of each such block (tunable to the size of the object). Experimentally, a

beneﬁcial consequence of LA-SHIRC is the reduced burden on the number of training

images required.

4.3 A Simultaneous Sparsity Model for Histopathological

Image Representation and Classiﬁcation

Section 3.2.2 has identiﬁed the central analytical formulation underlying the simultaneous

sparsity methods in literature. Typically a single dictionary DDD is used, as in [82]. In other

cases, each event is in fact characterized by multiple heterogeneous sources [117, 118],

resulting in multi-task versions of SRC. Although diﬀerent dictionaries are used for the

diﬀerent sources, the issue of correlation among diﬀerent representations of the same

image is not thoroughly investigated. Our contribution is an example of multi-task clas-

siﬁcation, with separate dictionaries designed from the RGB channels of histopathological

images.

For ease of exposition, we consider the binary classiﬁcation problem of classifying

64

images as either healthy or diseased. DDDh and DDDd indicate the training dictionaries of
healthy and diseased images respectively. A total of N training images are chosen. We

r, g, b correspond to the RGB color channels respectively. The dictionary DDD is redeﬁned

represent a color image as a matrix YYY := [yyyr yyyg yyyb] ∈ Rn×3, where the superscripts
as the concatenation of three color-speciﬁc dictionaries, DDD := (cid:2)DDDr DDDg DDDb(cid:3) ∈ Rn×3N .
Each color dictionary DDDc, c ∈ {r, g, b}, is the concatenation of sub-dictionaries from both
classes belonging to the c-th color channel,

DDDc := [DDDc

h DDDc

d], c ∈ {r, g, b}.
h DDDg

d DDDg

h DDDr

d DDDb

h DDDb
d].

⇒ DDD := [DDDr DDDg DDDb] = [DDDr

The color dictionaries are designed to obey column correspondence, i.e., the i-th column
from each of the color dictionaries DDDc taken together correspond to the i-th training im-
age. Fig. 4.1 shows the arrangement of training images into channel-speciﬁc dictionaries.

A test image YYY can now be represented as a linear combination of training samples:

(4.1)

(4.2)

(4.3)

YYY = DDDSSS =(cid:104)DDDr

h DDDr

d DDDg

h DDDg

d DDDb

h DDDb

d(cid:105)(cid:104)αααr αααg αααb(cid:105) ,

where the coeﬃcient vectors αααc ∈ R3N , c ∈ {r, g, b}, and SSS =(cid:2)αααr αααg αααb(cid:3) ∈ R3N×3.

A closer investigation of SSS reveals interesting characteristics:

1. It is reasonable to assume that the c-th channel of the test image (i.e. yyyc) can be
represented by the linear span of the training samples belonging to the c-th channel
alone (i.e. only those training samples in DDDc). So the columns of SSS ideally have
the following structure:

αααr =

αααr
h
αααr
d
000

000

000

000





, αααg =

000

000
αααg
h
αααg
d
000

000





, αb =



000

000

000

000
αααb
h
αααb
d

,



65

Figure 4.1. Color channel-speciﬁc dictionary design for SHIRC. The constituent RGB channels
of the i-th sample training image occupy the i-th columns of the dictionaries DDDr, DDDg and DDDb
respectively. Coeﬃcient vectors αααr, αααg and αααb are color-coded to indicate the dictionary corre-
sponding to each coeﬃcient. Filled-in blocks indicate non-zero coeﬃcients. The ﬁlling pattern
illustrates that identical
linear representation models hold for each color channel of the test
image, with possibly diﬀerent weights in the coeﬃcient vector.

where 000 denotes the conformal zero vector.

In other words, SSS exhibits block-

diagonal structure.

2. Each color channel representation yyyc of the test image is in fact a sparse linear
combination of the training samples in DDDc. Suppose the image belongs to class h
(healthy); then only those coeﬃcients in αααc that correspond to DDDc
h are expected to
be non-zero.

3. The locations of non-zero weights of color training samples in the linear combina-

tion exhibit one-to-one correspondence across channels. If the j-th training sample
in DDDr has a non-zero contribution to yyyr, then for c ∈ {g, b}, yyyc has non-zero con-
tribution from the j-th training sample in DDDc.

This immediately suggests a joint sparsity model similar to (3.9). However, the row

sparsity constraint leading to the SOMP solution is not obvious from this formulation.
Instead, we introduce a new matrix SSS(cid:48)
redundant zero coeﬃcients removed,

∈ RN×3 as the transformation of SSS with the

(cid:48)

SSS

=(cid:34) αααr

h αααg
d αααg
αααr

d (cid:35) .

h αααb
h
d αααb

(4.4)

This is possible by ﬁrst deﬁning HHH ∈ R3N×3 and JJJ ∈ RN×3N ,

HHH =

111N
000

000

000

111N
000

000

000

111N

 , JJJ = [III N III N III N ] ,

66

(4.5)

where 111N ∈ RN is the vector of all ones, and III N denotes the N -dimensional identity

matrix. Now,

(cid:48)

SSS

= JJJ (HHH ◦ SSS) ,

(4.6)

where ◦ denotes the Hadamard product, (HHH ◦ SSS)ij (cid:44) hijsij ∀ i, j. Finally, we formulate
a sparsity-enforcing optimization problem that is novel to the best of our knowledge:

(cid:48)

ˆSSS

= arg min(cid:13)(cid:13)SSS

(cid:48)(cid:13)(cid:13)row,0

subject to (cid:107)YYY − DDDSSS(cid:107)F ≤ .

(4.7)

Solving the problem in (4.7) presents a challenge in that a straightforward application

of SOMP [86] is not possible due to the non-invertibility of the Hadamard operator. We

have developed a greedy algorithmic modiﬁcation of SOMP that fares well in practice.

The analytical details of the algorithm are presented in Appendix A.

The ﬁnal classiﬁcation decision is made by comparing the class-speciﬁc reconstruction

errors to a threshold τ ,

R(YYY ) =(cid:13)(cid:13)(cid:13)YYY − DDDh ˆSSSh(cid:13)(cid:13)(cid:13)2 −(cid:13)(cid:13)(cid:13)YYY − DDDd ˆSSSd(cid:13)(cid:13)(cid:13)2

h

≷

d

τ,

(4.8)

where ˆSSSh and ˆSSSd are matrices whose entries are those in ˆSSS associated with DDDh and DDDd
respectively. Our approach extends to the K-class scenario in a straightforward manner

by incorporating additional class-speciﬁc dictionaries in DDD. The classiﬁcation rule is then

modiﬁed as follows:

Class(YYY ) = arg min

,

(4.9)

k=1,...,K(cid:13)(cid:13)(cid:13)YYY − DDDδk(ˆSSS)(cid:13)(cid:13)(cid:13)F

where δk(ˆSSS) is the matrix whose only non-zero entries are the same as those in ˆSSS asso-
ciated with class Ck.

67

Figure 4.2. Illustration to motivate LA-SHIRC. Shown here are four images from the IBL data
set. The test image cannot be represented accurately as a linear combination of the training
images. However, the four local regions marked in yellow (individual cells in the tissue) have
structural similarities, although they are located at diﬀerent spatial locations in the image.

Figure 4.3. Local cell regions selected from the IBL data set. Top row: DCIS (actionable),
bottom row: UDH (benign). Structural diﬀerences between the two classes are readily apparent.

4.4 LA-SHIRC: Locally Adaptive SHIRC

Some histopathological image collections present a unique practical challenge in that

the presence or absence of desired objects (e.g. cells, nuclei) in images is more crucial -

compared to their actual locations - for pathologists for disease diagnosis. Consequently,

if these discriminative objects (sub-images) are not in spatial correspondence in the test

and training images, it would seem that SHIRC cannot handle this scenario. Fig. 4.2

illustrates this for sample images from the IBL data set.

This issue can be handled practically to some extent by careful pre-processing that

manually segments out the objects of interest for further processing [126]. However this

approach causes a loss in contextual information that is also crucial for pathologists to

make class decisions. We propose a robust algorithmic modiﬁcation of SHIRC, known

as Locally Adaptive SHIRC (LA-SHIRC), to address this concern.

It is well known that local image features are often more useful than global features

from a discriminative standpoint [147]. This also conforms with the intuition behind

pathologists’ decisions. Accordingly, we modify SHIRC to classify local image objects

instead of the global image. Several local objects of the same dimension (vectorized

68

Figure 4.4. LA-SHIRC: Locally adaptive variant of SHIRC. The black boxes indicate local
objects of interest such as cells and nuclei. The new dictionary ¯DDD is built using multiple local
blocks from each training image. In every test image, the local objects are classiﬁed using the
simultaneous sparsity model and their decisions are fused for overall image-level classiﬁcation.

versions lie in Rm, m (cid:28) n) are identiﬁed in each training image based on the recommen-

dation of the pathologist. Fig. 4.3 shows individual cells from the IBL data set. In fact,

these obvious diﬀerences in cell structure have been exploited for classiﬁcation [126] by

designing morphological features such as cell perimeter and ratio of major-to-minor axis

of the best-ﬁtting ellipse.

The dictionary DDD in SHIRC is now replaced by a new dictionary ¯DDD that comprises the
local blocks. In Fig. 4.4, the yellow boxes indicate the local regions. Assuming N full-size

training images, the selection of B local blocks per image results in a training dictionary

of size N B. Note that even for ﬁxed N , the dictionary ¯DDD ∈ Rm×N B has more samples
(or equivalently, leads to a richer representation) than DDD ∈ Rn×N . Therefore, a test

image block is expressed as a sparse linear combination of all local image blocks from

the training images. B blocks are identiﬁed in every test image, and a class decision
is obtained for each such block by solving (4.7) but with the dictionary ¯DDD. Finally
the identity of the overall image is decided by combining the local decisions from its

constituent blocks. The adaptive term in the name LA-SHIRC indicates the ﬂexibility

oﬀered by the algorithm in terms of choosing the number of blocks B and their dimension

m. Objects of interest in histopathological image analysis exist at diﬀerent scales [122]

and the tunability of LA-SHIRC makes it amenable to a variety of histopathological

classiﬁcation problems.

69

LA-SHIRC satisfactorily handles the issue of spatial correspondence of objects. Ad-

ditionally, as will be demonstrated through experiments in Section 4.5.4, it ensures high

classiﬁcation accuracy even with a small number of (global) training images. This has

high practical relevance since generous number of training histopathological images per

condition (healthy/diseased) may not always be available. The improved performance

however comes at the cost of increased computational complexity since B optimization
problems need to be solved and the dictionary ¯DDD has considerably more columns than DDD.

Decision Fusion: A natural way of combining local class decisions is majority voting.

Suppose YYY i, i = 1, . . . , B, represent the B local blocks from image YYY . Then,

Class (YYY ) = max

k=1,...,K |{i : Class (YYY i) = k}| , i = 1, . . . , B,

(4.10)

where | · | denotes set cardinality and Class (YYY i) is determined by (4.9). We use a more
intuitive approach to fuse individual decisions based on a heuristic version of maximum
likelihood. Let ˆSSSi be the recovered sparse representation matrix of the block YYY i. The
probability of YYY i belonging to the k-th class is deﬁned to be inversely proportional to

the residual associated with the dictionary atoms in the k-th class:

pk
i = P (Class (YYY i) = k) =

(4.11)

1/Rk
i

i(cid:1) ,
k=1(cid:0)1/Rk
(cid:80)K
k=1,...,K(cid:32) B(cid:89)i=1

i(cid:33) .

where Rk

i =(cid:13)(cid:13)(cid:13)YYY i − ¯DDDδδδk(cid:16)ˆSSSi(cid:17)(cid:13)(cid:13)(cid:13)2

. The identity of the test image YYY is then given by:

Class (YYY ) = arg max

pk

(4.12)

4.5 Validation and Experimental Results

4.5.1 Experimental Set-Up: Image Data Sets

We compare the performance of SHIRC and LA-SHIRC against state-of-the-art alterna-

tives for two challenging real-world histopathological image data sets.

ADL data set: These images are provided by pathologists at the Animal Diagnostics

Lab, Pennsylvania State University. The tissue images have been acquired from three

diﬀerent mammalian organs - kidney, lung, and spleen. For each organ, images belonging

to two categories - healthy or inﬂammatory - are provided. The H&E-stained tissues

70

(a) Healthy lung.

(b) Healthy lung.

(c) Inﬂamed lung.

(d) Inﬂamed lung.

(e) Healthy kidney.

(f) Healthy kidney.

(g) Inﬂamed kidney.

(h) Inﬂamed kidney.

(i) Healthy spleen.

(j) Healthy spleen.

(k) Inﬂamed spleen.

(l) Inﬂamed spleen.

Figure 4.5. Sample images from the ADL data set. Each row corresponds to tissues from one
mammalian organ.

are scanned using a whole slide digital scanner at 40x optical magniﬁcation, to obtain
digital images of pixel size 4000 × 3000. All images are downsampled to 100 × 75 pixels
in an aliasing-free manner for the purpose of computational speed-up. The algorithm
in fact works at any image resolution. Example images1 are shown in Fig. 4.5. There
are a total of 120 images for each organ, of which 40 images are used for training and

the rest for testing. The ground truth labels for healthy and inﬂammatory tissue are

assigned following manual detection and segmentation performed by ADL pathologists.

We present classiﬁcation results separately for each organ.

It is worthwhile to brieﬂy understand the biological mechanisms underlying the dif-

ferent conditions in these images. Inﬂammatory cell tissue in cattle is often a sign of a

contagious disease, and its cause and duration are indicated by the presence of speciﬁc

types of white blood cells. Inﬂammation due to allergic reactions, bacteria, or parasites

is indicated by the presence of eosinophils. Acute infections are identiﬁed by the presence

of neutrophils, while macrophages and lymphocytes indicate a chronic infection. In Fig.

4.5, we observe that a healthy lung is characterized by large clear openings of the alveoli,

1While the entire data set cannot be made publicly available, sample full-resolution images can be

viewed at: http://signal.ee.psu.edu/histimg.html.

71

Figure 4.6. Sample breast lesion images from the IBL data set. Top row: healthy (UDH)
lesions, bottom row: cancerous (DCIS) lesions.

while in the inﬂamed lung, the alveoli are ﬁlled with bluish-purple inﬂammatory cells.

Similar clusters of dark blue nuclei indicate the onset of inﬂammation in the other organs.

IBL data set: The second data set comprises images of human intraductal breast lesions

[126]. It has been provided by the Clarian Pathology Lab and Computer and Information

Science Dept., Indiana University-Purdue University Indianapolis. The images belong

to either of two well-deﬁned categories: usual ductal hyperplasia (UDH) and ductal

carcinoma in situ (DCIS). UDH is considered benign and the patients are advised follow-

up check-ups, while DCIS is actionable and the patients require surgical intervention.

Ground truth class labels for the images are assigned manually by the pathologists.

A total of 40 patient cases - 20 well-deﬁned DCIS and 20 UDH - are identiﬁed for

experiments in the manner described in [126]. Each case contains a number of Regions

of Interest (RoIs), and we have chosen a total of 120 images (RoIs), consisting of a

randomly selected set of 60 images for training and the remaining 60 RoIs for test.

Each RoI represents a full-size image for our experiments. Smaller local regions are

chosen carefully within each such RoI for LA-SHIRC as described in 4.4, using a classical

morphology-based blob detection technique [128].

We compare the performance of SHIRC and LA-SHIRC with two competing ap-

proaches:

1. SVM: this method combines state-of-the-art feature extraction and classiﬁcation.

We use the collection of features from WND-CHARM [135, 136] which is known

72

(a) ADL data set.

(b) IBL data set.

Figure 4.7. Bar graphs indicating the overall classiﬁcation accuracies of the competing methods.

to be a powerful toolkit of features for medical images. A support vector machine

is used for decisions unlike weighted nearest neighbor in [135] to further enhance

classiﬁcation. We pick the most relevant features for histopathology [122], including

but not limited to (color channel-wise) histogram information, image statistics,

morphological features and wavelet coeﬃcients from each color channel. The source

code for WND-CHARM is made available by the National Institutes of Health

online at: http://ome.grc.nia.nih.gov/wnd-charm/.

2. SRC: the single-channel sparse representation-based classiﬁcation approach re-

viewed in Section 3.2.2. Speciﬁcally, we employ SRC directly on the luminance

channel (obtained as a function of the RGB channels) of the histopathological

images, as proposed initially for face recognition and applied widely thereafter.

For results on the IBL data set, we also directly report corresponding numbers from [126]

- the multiple-instance learning (MIL) algorithm - which is a full image analysis and

classiﬁcation system customized for the IBL data set.

In supervised classiﬁcation, it is likely that some particularly well-chosen training sets

can lead to high classiﬁcation accuracy. In order to mitigate this issue of selection bias,

we perform 10 diﬀerent trials of each experiment. In each trial, we randomly select a set

of training images – all results reported are the average of the classiﬁcation accuracies

from the individual trials.

73

Table 4.1. Confusion matrix: Lung.

Class

Healthy

Inﬂammatory

Healthy
0.8875
0.7250
0.7500
0.3762
0.2417
0.1500

Inﬂammatory Method

0.1125
0.2750
0.2500
0.6238
0.7583
0.8500

SVM
SRC

SHIRC
SVM
SRC

SHIRC

Table 4.2. Confusion matrix: Kidney.

Class

Healthy

Inﬂammatory

Healthy
0.6925
0.8750
0.8250
0.2812
0.2500
0.1667

Inﬂammatory Method

0.3075
0.1250
0.1750
0.7188
0.7500
0.8333

SVM
SRC

SHIRC
SVM
SRC

SHIRC

4.5.2 Validation of Central Idea: Overall Classiﬁcation Accuracy

First, we provide experimental validation of our central hypothesis: that exploiting color

information in a principled manner through simultaneous sparsity models leads to better

classiﬁcation performance over existing techniques for histopathological image classiﬁca-

tion. To this end, we present overall classiﬁcation accuracy for the three organs from

the ADL data set, in the form of bar graphs in Fig. 4.7(a). SHIRC outperforms SVM

and SRC in each of the three organs, thereby conﬁrming the merit of utilizing color

correlation information. The selection of application-speciﬁc features coupled with the

inclusion of features from the RGB channels ensures that the SVM classiﬁer performs

competitively, particularly for the lung.

A similar experiment using the full-size images from the IBL data set illustrates the

variability in histopathological imagery. Each image in the data set contains multiple

cells at diﬀerent spatial locations, as seen in Fig. 4.6. SHIRC is not designed to handle

this practical challenge. The bar graph in Fig. 4.7(b) shows that the SVM classiﬁer and

the systemic MIL approach in [126] oﬀer the best classiﬁcation accuracy. This is not

surprising because MIL [126] incorporates elaborate segmentation and pre-processing

followed by feature extraction strategies customized to the acquired set of images. This

experimental scenario occurs frequently enough in practice and serves as our motivation

to develop LA-SHIRC.

74

Table 4.3. Confusion matrix: Spleen.

Class

Healthy

Inﬂammatory

Healthy
0.5112
0.7083
0.6500
0.1275
0.2083
0.1167

Inﬂammatory Method

0.4888
0.2917
0.3500
0.8725
0.7917
0.8833

SVM
SRC

SHIRC
SVM
SRC

SHIRC

Table 4.4. Confusion matrix: Intraductal breast lesions.

Class
UDH
UDH 0.8636
0.6800
0.6818
0.9333
0.0909
0.4400
0.3600
0.1000

DCIS

DCIS
0.1364
0.3200
0.3182
0.0667
0.9091
0.5600
0.6400
0.9000

Method

SVM
SRC

SHIRC

LA-SHIRC

SVM
SRC

SHIRC

LA-SHIRC

4.5.3 Detailed Results: Confusion Matrices and ROC Curves

Next, we present a more elaborate interpretation of classiﬁcation performance in the

form of confusion matrices and ROC curves. Each row of a confusion matrix refers to

the actual class identity of test images and each column indicates the classiﬁer output.

Tables 4.1-4.3 show the mean confusion matrices for the ADL data set. In continu-

ation of trends from Fig. 4.7(a), SHIRC oﬀers the best disease detection accuracy - a

quantitative metric of high relevance to pathologists - for each organ, while maintaining

high classiﬁcation accuracy for healthy images too. An interesting observation can be

made from Table 4.3. The SVM classiﬁer reveals a tendency to classify the diseased tissue

images much more accurately than the healthy tissues. In other words, there is a high

false alarm rate (healthy image mistakenly classiﬁed as inﬂammatory) associated with

the SVM classiﬁer. SHIRC however oﬀers a more consistent class-speciﬁc performance,

resulting in the best overall performance. The corresponding results using LA-SHIRC

are identical to SHIRC and hence not shown, since a single block (i.e. the entire image)

was deemed by pathologists to have suﬃcient discriminative information.

Table 4.4 shows the mean confusion matrix for the IBL data set. SHIRC provides

an average classiﬁcation accuracy of 66.09%, in comparison with about 87.9% using

the MIL approach [126]. However, LA-SHIRC results in a signiﬁcant improvement in

75

(a) Lung (ADL).

(b) Kidney (ADL).

(c) Spleen (ADL).

(d) IBL.

Figure 4.8. Receiver operating characteristic (ROC) curves for diﬀerent organs.

Table 4.5. False alarm probability for ﬁxed detection rate.

Images

Lung (ADL)
Kidney (ADL)
Spleen (ADL)

IBL

Fixed rate
of detection

0.15
0.15
0.15
0.10

False alarm rate

SVM SRC SHIRC LA-SHIRC
0.71
0.50
0.45
0.17

0.26
0.22
0.33
0.10

0.42
0.27
0.40
0.69

0.26
0.22
0.33
0.65

performance, even better than the rates reported using SVM, MIL or SRC. For LA-

SHIRC, we identify 9 local objects per image corresponding to individual cells.

It is

noteworthy that a pre-processing stage involving careful image segmentation is performed

prior to feature extraction in MIL [122], implying that MIL is representative of state-of-

the-art classiﬁcation techniques using local image information.

Typically in medical image classiﬁcation problems, pathologists desire algorithms

76

(a) Kidney (ADL).

(b) IBL.

Figure 4.9. Overall classiﬁcation accuracy as a function of training set size for three diﬀerent
scenarios: low, limited and adequate training.

that reduce the probability of miss (classifying diseased image as healthy) while also

ensuring that the false alarm rate remains low. However, there is a trade-oﬀ between

these two quantities, conveniently described using a receiver operating characteristic

(ROC) curve. Fig. 4.8 shows the ROC curves for the ADL and IBL data sets. The

lowest curve (closest to the origin) has the best overall performance and the optimal

operating point minimizes the sum of the miss and false alarm probabilities. In Figs.

4.8(a)-(c), the curves for LA-SHIRC are not shown since they are identical to SHIRC

(only one global image block is suﬃcient). In each case, SHIRC oﬀers the best trade-oﬀ.

In Fig. 4.8(d), the LA-SHIRC outperforms SVM, and both methods are much better
than SRC and SHIRC2.

Depending on the inherent degree of diﬃculty in classifying a particular image set
and the severity of the penalty for misclassifying a diseased image3, a pathologist can
choose an acceptable probability of miss and corresponding false alarm rate for each

method. Table 4.5 shows that for each organ in the ADL data set, a higher false alarm

must be tolerated with the SVM method, compared to SRC and SHIRC, in order to

maintain a ﬁxed rate of miss. For the IBL data set, the LA-SHIRC incurs the lowest

false alarm rate to achieve a miss rate of 10%.

2Note that ROCs for MIL [126] could not be reported because the image analysis and classiﬁcation
system in [126] has a variety of pre-processing, segmentation and other image processing and classiﬁcation
steps which makes exact reproduction impossible in the absence of publicly available code.

3For example, DCIS requires immediate surgical attention, while a mild viral infection may only

prolong for a few more days if not diagnosed early.

77

4.5.4 Performance as Function of Training Set Size

This experiment oﬀers new insight into the practical performance of our algorithms.

Real-world classiﬁcation tasks often suﬀer from the lack of availability of large training

sets. We present a comparison of overall classiﬁcation accuracy as a function of the

training set size for the diﬀerent methods.

We identify three scenarios of interest: (i) low training, (ii) limited training, and (iii)

adequate training. In Fig. 4.9(a), overall classiﬁcation accuracy is reported for ADL data

set (kidney) corresponding to the three training scenarios: 20 (low), 30 (limited) and 40

(adequate) training images respectively. As before, comparison is made against the single

channel SRC and state-of-the-art feature extraction plus SVM classiﬁer. Unsurprisingly,

all three methods suﬀer in performance as training is reduced.

Fig. 4.9(b) reports analogous results for the IBL data set. Here the regime of low,

limited and adequate training images are deﬁned by 20, 40 and 60 images respectively.

Analyzing the results in Fig. 4.9(b) for the IBL data set, a more interesting trend reveals

itself. As discussed before in Section 4.4, LA-SHIRC can lead to richer dictionaries made

out of local image blocks even as the number of training images is not increased. This

allows LA-SHIRC to perform extremely well even under low training - oﬀering about

90% accuracy - as is evident from Fig. 4.9(b). This beneﬁt however comes at the cost of

increased computational complexity at the time of inference because the dictionary size

(number of columns) is signiﬁcantly increased in LA-SHIRC vs. SHIRC.

4.6 Structured Sparse Priors for Image Classiﬁcation

4.6.1 Related Work in Model-based Compressive Sensing

We introduced sparse signal representations in Chapter 3 by discussing the compressive

problem and the sparse representation-based classiﬁcation framework. For the sake of

clarity, it is worthwhile to reproduce here the key equations from Section 3.2:

(P0) min

(P1) min

xxx (cid:107)xxx(cid:107)0 subject to yyy = AAAxxx.
xxx (cid:107)xxx(cid:107)1 subject to yyy = AAAxxx.

(P2) min

xxx (cid:107)xxx(cid:107)1 subject to (cid:107)yyy − AAAxxx(cid:107)2 < .

(4.13)

(4.14)

(4.15)

The optimization problem in (P2) can be interpreted as maximizing the probability
of observing xxx given yyy under the assumption that the coeﬃcients of xxx are modeled

78

as i.i.d. Laplacians. Thus, sparsity can be interpreted as a prior for signal recovery.

This is a particular example of the broader Bayesian perspective: signal comprehension

can be enhanced by incorporating contextual information (in this case, sparsity) as

priors. Estimating the sparse coeﬃcients in a Bayesian framework has the beneﬁts of

probabilistic predictions and automatic estimation of model parameters. The relevance

vector machine [148] has inspired more sophisticated substitutes for the Laplacian via

hierarchical priors [149, 150].

Sparsity is in fact a ﬁrst-order description of structure in signals. However, often

there is a priori structure inherent to the sparse signals that is exploited for better

representation, compression or modeling. As an illustration, a connected tree structure

can be enforced on wavelet coeﬃcients to capture the multi-scale dependence [151]. Other

such structured prior models have also been integrated into the CS framework [152–158].

The wavelet-based Bayesian approach in [152] employs a “spike-and-slab” prior [159–

162], which is a mixture model of two components representing the zero and nonzero

coeﬃcients, and dependencies are encouraged in the mixing weights across resolution

scales. Structure on sparse coeﬃcients can also be enforced via probabilistic prior models,

as demonstrated in [9, 163] by solving the following optimization problem:

(P4) max

xxx

f (xxx) subject to (cid:107)yyy − AAAxxx(cid:107)2 < .

(4.16)

The pdf f simultaneously captures the sparsity and structure (joint distributions of

coeﬃcients) of xxx.

In comparison, the standard CS recovery (P2) captures only the

sparse nature of xxx.

4.6.2 Overview of Contribution

Consider a binary classiﬁcation problem (classes C0 and C1) with the two class condi-
tional pdfs represented by fC0 and fC1 respectively. We choose a ﬁxed dictionary matrix
AAA = [AAA0, AAA1]. Suppose that we have access to T labeled training vectors {yyyi,t}T
t=1 and
corresponding sparse features {xxxi,t}T
t=1 from each class, where the index i ∈ {0, 1} refers
to C0 and C1. Given a test vector yyy, we solve a constrained posterior maximization
problem separately for each class:

(P5)

ˆxˆxˆx(i) = arg max

xxx

fCi(xxx) s.t. (cid:107)yyy − AAAxxx(cid:107)2 < ,

i = 0, 1.

(4.17)

79

Figure 4.10. Set-theoretic comparison: (a) traditional CS recovery, and (b)-(c) our proposed
framework - structured sparsity using class-speciﬁc priors. In (b), the test vector yyyC0 is actually
from class C0, while in (c), the test vector yyyC1 is from C1.

The two spike-and-slab priors fCi are learned separately, in a class-speciﬁc manner, from
training samples of the two classes. Class assignment is performed as follows:

Class(yyy) = arg max

i∈{0,1} fCi(ˆxˆxˆx(i)).

(4.18)

The evolution of this formulation can be traced organically through (P0)-(P5), and

it represents a consummation of ideas developed for model-based CS into a general

framework for sparse model-based classiﬁcation. Owing to its proven success in mod-

eling sparsity, the spike-and-slab prior is an excellent initial choice for the fCi in this
framework. This is validated next by theoretical analysis and experiments.

4.7 Design of Discriminative Spike-and-slab Priors

4.7.1 Set-theoretic Interpretation

Fig. 4.10 oﬀers a set-theoretic viewpoint to illustrate the central idea of our SSPIC
framework. Fig. 4.10(a) represents the traditional CS recovery problem. Srec is the
sub-level set of all vectors that lead to reconstruction error less than a speciﬁc tolerance
. Ssparse is the set of all vectors with only a few non-zero coeﬃcients. The vectors that
lie in the intersection of these two sets (shaded region in Fig. 4.10(a)) are exactly the

80

Figure 4.11. Probability density function of a spike-and-slab prior.

set of solutions to (3.1).

Figs. 4.10(b) and 4.10(c) describe our idea for binary classiﬁcation. Now we have two
sub-level sets Srec,1 and Srec,2, which correspond to vectors xxx leading to reconstruction
error not greater than 1 and 2 respectively (1 > 2).

Our contribution is the introduction of the two sets S Ci

struct, i = 0, 1. These sets
enforce additional class-speciﬁc structure on the sparse coeﬃcients. Let us ﬁrst consider
Fig. 4.10(b), where the sample test vector yyyC0 is in fact from class C0. The two sets
S C0
struct and S C1
struct are deﬁned by priors fC0 and fC1 respectively, which simultaneously
encode sparsity and class-speciﬁc structure. For a relaxed reconstruction error tolerance
1, both these sets have non-zero intersection with S C0
rec,1. As a result, both the class-
speciﬁc optimization problems in (4.17) are feasible and this increases the possibility of

the test vector being misclassiﬁed. However, as the error bound is tightened to 2, we
see that only S C0
rec,2, and the solution to (4.17) correctly identiﬁes
the class of the test vector as C0.

struct intersects with S C0

C1. As the reconstruction error tolerance is reduced, only S C1

An analogous argument holds in Fig. 4.10(c), where the test vector yyyC1 is now from
rec,2.
The proposed framework extends to multi-class classiﬁcation in a natural manner,

struct intersects with S C1

by deﬁning multiple such structured priors, one per class, and solving the corresponding

optimization problems in parallel. In terms of computational complexity, it is similar to

the requirements of SRC.

4.7.2 Spike-and-slab Priors

What priors do we choose per class? We remind ourselves that the priors fCi should be
chosen to simultaneously capture the sparse nature of and structure inherent to xxx. A

81

Figure 4.12. Structured spike-and-slab priors for sparse representation-based classiﬁcation.

particularly well-suited example of probabilistic sparsity prior is the spike-and-slab prior

which is widely used in Bayesian regression models [159–162]. In fact, the spike-and-slab

prior is acknowledged to be the gold standard for sparse inference in the Bayesian set-

up [164]. Each individual coeﬃcient xi of xxx is modeled as a mixture of two components:

xi ∼ (1 − γi)δ0 + γif (xi),

(4.19)

where δ0 is a point mass concentrated at zero (the “spike”), and fi (the “slab”) is any

suitable distribution on the non-zero coeﬃcient (e.g. a Gaussian). Fig. 4.11 shows an

illustration of a spike-and-slab prior. Structural sparsity is encoded by the parameter
γi ∈ [0, 1]. For example, the slab term (fi) is expected to dominate for a non-zero
coeﬃcient and this can be enforced by choosing γi closer to 1; likewise γi is chosen closer

to 0 to encourage a zero coeﬃcient.

Fig. 4.12 illustrates a binary classiﬁcation problem for the task of face recognition.

AAA1 and AAA2 are built using training samples from the two respective classes. For yyy from
class 1, the ﬁrst half of coeﬃcients in ααα are expected to be active. For our choice of

spike-and-slab priors, this leads to weights γ = 0 for the corresponding nodes in the

graph. Similarly, the inactive coeﬃcients are identiﬁed with weights γ = 1.

4.7.3 Analytical Development

Inspired by a recent maximum a posteriori (MAP) estimation approach to variable selec-

tion using spike-and-slab priors [165], we develop the corresponding Bayesian framework

82

for classiﬁcation. Another model is developed analogously for the other class, albeit

with diﬀerent parameters; considering the model for a single class keeps notation simpler

without an additional class index term. We consider the following linear representation

model:

yyy = AAAxxx + nnn,

(4.20)

where yyy ∈ Rm, xxx ∈ Rn, AAA ∈ Rm×n, and nnn ∈ Rm models Gaussian noise. We deﬁne
γi := I(xi (cid:54)= 0), i = 1, . . . , n, i.e. γi is binary-valued. It is an indictor variable which
takes the value one if the corresponding coeﬃcient xi (cid:54)= 0 in xxx, and zero otherwise. We
set up the Bayesian formulation as follows:

yyy|AAA, xxx, γγγ, σ2 ∼ N(cid:0)AAAxxx, σ2III(cid:1)
xi|σ2, γi, λ ∼ γiN (0, σ2λ
−1(τ1, τ2)
σ2|τ1, τ2 ∼ Γ

−1) + (1 − γi)I(xi = 0)

γi|κi ∼ Bernoulli(κi), i = 1, . . . , n.

(4.21a)

(4.21b)

(4.21c)

(4.21d)

Here, N (·) represents the Gaussian distribution. It can be seen that (4.21b) represents
the i.i.d. spike-and-slab prior on each coeﬃcient of xxx. The slab term is a zero-mean
Gaussian with variance σ2λ−1. The role of λ as a regularization parameter will become
In all generality, we select diﬀerent κi to parameterize each coeﬃcient
clear shortly.

γi, i = 1, . . . , n. It can be seen that (4.21b) represents the spike-and-slab prior on xi.

Exploiting the mixture form of the prior, we can write the pdf for xi in (4.21b) as:

xi ∼ (cid:0)N (0, σ2λ

−1)(cid:1)γi . (I(xi = 0))1−γi

The joint posterior density is then given by:

f (xxx, γγγ, σ2|AAA, yyy, λ, τ1, τ2, κκκ) ∝ f (yyy|AAA, xxx, γγγ, σ2)f (xxx|γγγ, σ2, λ)

f (σ2|τ1, τ2)f (γγγ|κκκ).

The optimal xxx∗, γγγ∗, σ2∗ are obtained by MAP estimation as:

∗

∗

, σ2∗

, γγγ

(xxx

) = arg min

xxx,γγγ,σ2(cid:8)−2 log f (xxx, γγγ, σ2|AAA, yyy, λ, τ1, τ2, κκκ)(cid:9) .

(4.22)

(4.23)

(4.24)

We now evaluate each of the terms separately on the right hand side of Eq. (4.23).

83

f (yyy|AAA, xxx, γγγ, σ2) =

1

(2π)m/2σm exp(cid:26)−

1

2σ2 (yyy − AAAxxx)T (yyy − AAAxxx)(cid:27)

⇒ −2 log f (yyy|AAA, xxx, γγγ, σ2) =

1
σ2 (yyy − AAAxxx)T (yyy − AAAxxx)
+m log σ2 + m log(2π).

f (xxx|γγγ, σ2, λ) =

1

(cid:112)2πσ2/λ(cid:33)γi
n(cid:89)i=1(cid:40)(cid:32)
n(cid:89)i=1
I(xi = 0)1−γi
(cid:80)n
λ (cid:19)− 1
exp(cid:26)−
= (cid:18) 2πσ2
n(cid:89)i=1

I(xi = 0)1−γi

i=1 γi

2

exp(cid:18)−

γix2
i

2σ2λ−1(cid:19)(cid:41)

1

2σ2λ−1 xxxT xxx(cid:27)

(4.25)

(4.26)

(4.27)

(4.28)

⇒ −2 log f (xxx|γγγ, σ2, λ) =

xxxT xxx

σ2λ−1 +(cid:32) n(cid:88)i=1

γi(cid:33) log(cid:18) 2πσ2

λ (cid:19) − 2

n(cid:88)i=1

(1 − γi) log I(xi = 0).

(4.29)

The ﬁnal term on the right hand side evaluates to zero, since I(xi = 0) = 1 ⇒ log I(xi =
0) = 0, and I(xi = 0) = 0 ⇒ xi (cid:54)= 0 ⇒ γi = 1 ⇒ (1 − γi) = 0. Therefore,
γi(cid:33) log(cid:18) 2πσ2
λ (cid:19) .

− 2 log f (xxx|γγγ, σ2, λ) =

(4.30)

xxxT xxx

f (σ2|τ1, τ2) =
⇒ −2 log f (σ2|τ1, τ2) =

Finally,

τ2

τ τ1
2
Γ(τ1)
2τ2
σ2 + 2(τ1 + 1) log σ2 − 2τ1 log τ2 + 2 log Γ(τ1). (4.32)

(4.31)

σ2(cid:17)

f (γγγ|κκκ) =

i (1 − κi)1−γi
κγi

n(cid:89)i=1

(4.33a)

σ2λ−1 +(cid:32) n(cid:88)i=1
σ2(−τ1−1) exp(cid:16)−

⇒ −2 log f (γγγ|κκκ) =

γi log(cid:18) (1 − κi)2

κ2
i

(cid:19) −

n(cid:88)i=1

n(cid:88)i=1

log(1 − κi)2.

Collecting all these expressions together:

84

(4.33b)

∗

∗

, σ2∗

, γγγ

(xxx

) = arg min

+

xxxT xxx

σ2λ−1 +(cid:32) n(cid:88)i=1

1
σ2 (yyy − AAAxxx)T (yyy − AAAxxx) + m log σ2
2τ2
σ2

γi(cid:33) log(cid:18) 2πσ2
λ (cid:19) +
γi log(cid:18) (1 − κi)2
n(cid:88)i=1

+2(τ1 + 1) log σ2 +

κ2
i

(cid:19) .

(4.34)

Note that in the analytical development so far, we have introduced the formulation

for a single class. The formulations for the other classes in a general multi-class scenario

are obtained in exactly the same manner, but with diﬀerent sets of model parameters

per class.

Tractability of optimization problem: We observe that (4.34) comprises a collection

of terms that: (i) lacks direct interpretation in terms of modeling sparsity, and (ii) leads

to a diﬃcult optimization problem. So, we introduce the simplifying assumption of

choosing a single scalar κ per class. With this simpliﬁcation, we now have:

(4.35a)

(4.35b)

(4.36)

(cid:80)n
i=1 γi(1 − κ)n−(cid:80)n

i=1 γi

f (γγγ|κ) =

n(cid:89)i=1
κγi(1 − κ)1−γi = κ
γi log(cid:18) (1 − κ)2
n(cid:88)i=1

κ2 (cid:19) − 2n log(1 − κ).

⇒ −2 log f (γγγ|κ) =

Plugging this back into (4.34):

∗

∗

, σ2∗

, γγγ

(xxx

) = arg min
xxx,γγγ,σ2
xxxT xxx

1
σ2 (yyy − AAAxxx)T (yyy − AAAxxx) + m log σ2

+

σ2λ−1 +(cid:32) n(cid:88)i=1

γi(cid:33) log(cid:18) 2πσ2
λ (cid:19) +
γi log(cid:18) (1 − κ)2
κ2 (cid:19) .
n(cid:88)i=1
Now we observe another interesting aspect of this formulation. The term(cid:80)n

+2(τ1 + 1) log σ2 +

2τ2
σ2

the number of γi which are equal to 1, since γi is a binary variable.
equivalent to counting the number of entries of xxx that are non-zero, i.e. the l0-norm of

i=1 γi counts
In turn, this is

xxx. So,

L(xxx, γγγ, σ2) =

where ρσ2,λ,κ := σ2 log(cid:16) 2πσ2(1−κ)2

λκ2

1

2 + λ(cid:107)xxx(cid:107)2

2 + ρσ2,λ,κ(cid:107)xxx(cid:107)0

σ2(cid:8)(cid:107)yyy − AAAxxx(cid:107)2
+(m + 2τ1 + 2)σ2 log σ2 + 2τ2(cid:9) ,
(cid:17). For ﬁxed σ2, the cost function reduces to:

L(xxx; σ2) = (cid:107)yyy − AAAxxx(cid:107)2

2 + λ(cid:107)xxx(cid:107)2

2 + ρσ2,λ,κ(cid:107)xxx(cid:107)0.

85

(4.37)

(4.38)

In fact, we obtain multiple such cost functions L0(xxx, γγγ, σ2) and L1(xxx, γγγ, σ2), corre-
sponding to to each class. Diﬀerent sets of data-dependent parameters ρσ2,λ,κ and λ are
learned from the training images of each class. The general form of the classiﬁcation rule

for multiple (K) classes is as follows:

Class(yyy) = arg max

i∈{1,...,K} fCi(ˆxˆxˆx(i)).

4.7.4 SSPIC: Some Observations

For ﬁxed σ2, the cost function reduces to:

(P5) L(xxx; σ2) = (cid:107)yyy − AAAxxx(cid:107)2

2 + λ(cid:107)xxx(cid:107)2

2 + ρσ2,λ,κ(cid:107)xxx(cid:107)0.

(4.39)

(4.40)

To summarize our analytical contribution, we initially choose a sparsity-inducing spike-

and-slab prior per class and perform MAP estimation. With reasonable simpliﬁcations

on model structure, we obtain the ﬁnal formulation (4.38) which explicitly captures

sparsity in the form an l0-norm minimization term. This is intuitively satisfying, since
we are looking for sparse vectors xxx. Thereby, we oﬀer a Bayesian perspective to SRC.

We can immediately draw parallels to SRC. However, our framework has two key
diﬀerences when compared to SRC. One, the (cid:107)xxx(cid:107)2-term, which enforces smoothness in
xxx, is absent in SRC. Extensions to SRC have considered the addition of regularizers such
as the (cid:107)xxx(cid:107)2-term to induce group sparsity [79]. However these have largely been heuristic
yet meaningful choices to improve classiﬁcation accuracy over SRC. On the other hand,

our SSPIC framework handles the classiﬁcation problem in a completely Bayesian setting.

Secondly, the regularization parameter in SRC is chosen uniformly to be the same for

images from all classes. However, our framework learns diﬀerent parameters per class,

leading to the solution of multiple optimization problems in parallel. On a related note,

(P5) is identical to the elastic net proposed for statistical regression problems [166] if the

86

Table 4.6. AVIRIS Indian Pines hyperspectral image.

Class type
Corn-notill
Grass/trees
Soybeans-notill
Soybeans-min
Overall

Training Test
1290
672
871
2221
5054

144
75
97
247
563

SVM SRC SSPIC
91.82
88.06
97.85
96.72
72.91
85.26
89.73
82.14
84.12
90.57

89.32
97.64
79.64
88.32
88.34

l0-term is relaxed to its l1-counterpart. Of course, it must be mentioned that elastic net

was proposed in the context of sparse signal modeling and not for classiﬁcation tasks.

What beneﬁt does the Bayesian approach buy? A limitation of SRC is its requirement

of abundant training (highly overcomplete dictionary AAA). However in many real-world

problems such as hyperspectral image classiﬁcation, there is a scarcity of available train-

ing. The use of carefully selected class-speciﬁc priors alleviates the burden on the number

of training images required, as we shall see in Section 4.8.

4.7.5 Parameter Learning

Diﬀerent sets of parameters ρσ2,λ,κ and λ are learned for diﬀerent classes. The classi-
ﬁcation accuracy is tied to the accuracy of estimating these parameters which encode

discriminative information. A common way of selecting these parameters is by cross-

validation on the training samples. An alternate approach involves learning the poste-

rior distributions in the parameters in a Markov chain Monte Carlo (MCMC) setting,

an example of which is described in [152]. In our framework, the inverse-Gamma prior is

chosen speciﬁcally because it is conjugate to the Gaussian distribution, leading to closed

form expressions for the posterior. Similarly, κ can be sampled from a Beta distribution,

since the Beta-Bernoulli is a well-known conjugate pair of distributions. With these

assumptions and good initialization of hyperparameters, the MCMC method will lead

to posterior estimates for all parameters (see Algorithm 4 in Appendix B). In fact, a

signiﬁcant advantage of this approach is that it can be completely training-free [152] if

hyperparameters are carefully selected.

4.8 Experimental Results

We perform experiments on two diﬀerent data sets. We solve an l1-relaxation of (P5)

per class using software from the SPArse Modeling Software (SPAMS) toolbox [167].

Hyperspectral image classiﬁcation: The ﬁrst set is the AVIRIS Indian Pines hy-

87

Ground

(b) SVM.

(c) SRC.

(d) SSPIC.

(a)
truth

Figure 4.13. Error maps for AVIRIS Indian Pine data set.

Table 4.7. Confusion matrix: Lung.

Class

Healthy

Inﬂammatory

Healthy

Inﬂammatory Method

0.734
0.706
0.868
0.333
0.366
0.294

0.266
0.294
0.132
0.667
0.634
0.706

SVM
SRC
SSPIC
SVM
SRC
SSPIC

perspectral image [101]. The AVIRIS sensor generates 220 bands across the spectral

range from 0.2 to 2.4 µm, of which only 200 bands are considered by removing 20 water

absorption bands. This image has spatial resolution of 20m per pixel and spatial dimen-
sion 145 × 145. Following [168], we consider a subset scene of size 86 × 68 consisting of
pixels [27 − 94] × [31 − 116]. Each pixel is classiﬁed into one of four classes: corn-notill,
grass/trees, soybeans-notill, and soybeans-min. Table 4.6 shows the class-wise classiﬁ-

cation rates for a speciﬁc training-test combination, comparing three approaches: state

of the art support vector machine (SVM) [168], SRC, and SSPIC. Fig. 4.13 shows the

error maps.

Histopathological image classiﬁcation: We classify bovine lung tissue samples as

either healthy or inﬂammatory. The images are acquired by pathologists at the Animal

Diagnostic Laboratory, Pennsylvania State University [169]. The H&E-stained tissues

are scanned using a whole slide digital scanner at 40x optical magniﬁcation. All images
are downsampled to 100 × 75 pixels (aliasing-free). Sample healthy and inﬂamed tissue
images are shown in Fig. 4.5(a)-(b). 40 images per condition are used for training,

and performance is evaluated over a set of 10 images from each class. The ground

truth labels for healthy and inﬂammatory tissue are obtained via manual detection and

segmentation by ADL pathologists. In order to mitigate selection bias, the experiment

is repeated over 1000 trials of randomly selected training. Table 4.7 reports the mean

classiﬁcation accuracy as a confusion matrix, where rows refer to the actual identity of

88

(a) Healthy lung.

(b) Inﬂamed lung.

Figure 4.14. Sample lung tissue images.

Figure 4.15. Performance as a function of training ratio. Training ratio is the actual fraction
of 40 training images per class used in the experiment.

test images and columns refer to the classiﬁer output. We compare the performance of

SSPIC with SRC using the luminance channels of the color images and a state of the art

SVM classiﬁer [170].

For each data set, the classiﬁcation rates for SSPIC are better than those for SVM

and SRC. In Fig. 4.15, the average (of healthy and inﬂamed tissue) classiﬁcation rate

is plotted as a function of training set size. Given adequate training, SSPIC oﬀers

signiﬁcant beneﬁts over SRC. Crucially, as training ratio is reduced, SSPIC suﬀers a

graceful decay in performance. This is enabled by the use of class-speciﬁc priors that

oﬀer additional discriminability over class-speciﬁc dictionaries.

89

4.9 Conclusion

In this chapter, we have proposed discriminative models on sparse signal representations

in two diﬀerent ways. First, we develop a simultaneous sparsity model for histopathologi-

cal image representation and classiﬁcation. The central idea of our approach is to exploit

the correlations among the red, green and blue channels of the color images in a sparse

linear model setting with attendant color channel constraints. We formulate and solve

a new sparsity-based optimization problem. We also introduce a robust locally adaptive

version of the simultaneous sparsity model to address the issue of correspondence of local

image objects located at diﬀerent spatial locations. This modiﬁcation results in beneﬁts

that have signiﬁcant practical relevance: we demonstrate that the sparsity model for

classiﬁcation can work even under limited training if local blocks are chosen carefully.

In the second half of this chapter, discriminative structure in sparse representations

is revealed by an appropriate choice of sparsity-inducing priors. We use class-speciﬁc

dictionaries in conjunction with discriminative class-speciﬁc priors, speciﬁcally the spike-

and-slab prior widely applied in Bayesian regression. Signiﬁcantly, the proposed frame-

work takes the burden oﬀ the demand for abundant training necessary for the success of

sparsity-based classiﬁcation schemes.

Chapter 5

Conclusions and Future Directions

5.1 Summary of Main Contributions

The overarching theme in this research is the design of discriminative models for robust

image classiﬁcation. Two diﬀerent families of discriminative models have been explored,

based on: probabilistic graphical models and sparse signal representations. We have

primarily considered multi-task classiﬁcation scenarios where the diﬀerent image repre-

sentations exhibit discriminative structure - deterministic or probabilistic - that can be

leveraged for robustness beneﬁts.

In Chapters 2 and 3, we explore the ability of graphical models to fuse multiple

feature sets for classiﬁcation tasks. In particular, we base our contributions on a recent

approach to discriminative learning of tree-structured graphs [3]. By learning simple trees

iteratively on the larger graphs formed by concatenating all nodes from each hypothesis

and accumulating all edges, we obtain a ﬁnal graphical model with dense edge structure

that explicitly captured statistical correlations - which encode discriminative structure -

across feature sets. The framework makes minimal demands on the choice of feature sets

- it merely requires a collection of features that capture complementary yet correlated

class information. In order to verify that the framework is indeed applicable in a variety

of scenarios and for diﬀerent choices of feature sets, we consider three important practical

applications:

1. Automatic target recognition: Here, wavelet sub-band LL, LH and HL features

are chosen as the feature sets, owing to their popularity as feature choices for the

ATR problem. On a related note, it is well-known that wavelet coeﬃcients can be

modeled using tree-structured graphs [171].

91

2. Hyperspectral target detection and classiﬁcation: Hyperspectral imaging exhibits

some unique properties, such as the presence of joint spatio-spectral information

and the observation of spatial homogeneity of pixels. These ideas are exploited

for robust classiﬁcation by extracting multiple sparse features from pixels in local

neighborhoods and fusing them via discriminative graphs. Our contribution di-

rectly builds upon very recent work [82] which proposed a joint sparsity model to

enforce identical sparse representation structure on neighboring pixels.

3. Face recognition: The robustness of local features, in comparison with global image

features, for classiﬁcation tasks is well-known. We leverage this idea for robust

face recognition by extracting sparse features from local informative regions such

as the eyes, nose and mouth. Analogous to a single global dictionary in SRC, we

design locally adaptive dictionaries borrowing from recent work [114] that encode

robustness to minor geometric and photometric distortions. Discriminative graphs

are then learned on these distinct feature representations akin to the procedure

described for the above two applications.

In each problem, we observe that our graphical model framework exhibits robustness

to distortion scenarios intrinsic to the task. For example, pixel corruption and regis-

tration errors can cause signiﬁcant degradation in face recognition performance, while

the extended operating scenarios in ATR are acknowledged to be particularly severe

conditions to test algorithm performance. In addition, our framework exhibits a more

graceful decay in classiﬁcation performance with reduction in the size of the training set.

We consider this to be a signiﬁcant experimental contribution, since the issue of limited

training has not been as thoroughly investigated before despite being a concern in many

practical problems.

In the second half of this dissertation, we propose discriminative models for sparse

signal representations. Our ﬁrst contribution is a simultaneous sparsity model which

exploits correlations among the diﬀerent color channels of medical images for disease

identiﬁcation. While this problem has similarities with many group/simultaneous models

proposed recently in literature, the novelty of our contribution is in designing constraints

based on an understanding of the imaging physics, leading to the formulation and solu-

tion of new optimization problems. The sparse coeﬃcient matrices in our formulation

exhibit a unique block-sparse structure that is not amenable to popular row sparsity-

norm techniques such as the SOMP [86]. As the ﬁnal contribution of this dissertation,

we revisit the SRC framework from a Bayesian perspective. We design class-speciﬁc

92

spike-and-slab priors in conjunction with class-speciﬁc dictionaries to obtain improved

classiﬁcation performance, even under the regime of limited training.

5.2 Suggestions for Future Research

The contributions in the previous chapters naturally point towards various directions for

future research. We mention some of the possible extensions in this section.

5.2.1 Discriminative Graph Learning

Learning arbitrary graph structures is known to be an NP-hard problem [27]. While

tree graphs are easy to learn, they are also limited in their modeling capacity. However,

extensions of trees, such as junction trees and block trees, can still be learned in a

tractable manner. Block trees have the interesting qualitative interpretation of clustering

groups of graph nodes based on similarity. An immediate extension of our graphical

model framework could therefore be to learn such block trees on the multiple feature

representations and identify correlations across feature sets, or equivalently, graph node

clusters.

5.2.2 Joint Sparsity Models

Sparse representation-based image classiﬁcation is an area of ongoing research interest,

and here we identify some connections to our work in published literature. Our frame-

work can be generalized for any multi-variate/multi-task classiﬁcation problem [172]

by simply including training from those tasks as new sub-dictionaries. Recent work in

multi-task classiﬁcation has explored the idea of sparse models on image features [118].

Admittedly, the sparse linear model may not be justiﬁable for all types of image clas-

siﬁcation problems. However, one way of incorporating non-linear sparse models is to

consider the sparse model in a feature space induced by a kernel function [173]. Recent

work has focused attention on solving the costly sparsity optimization problem more

eﬀectively [76, 174, 175]. Our solution to the optimization problem in (4.7) is a modiﬁca-

tion of the greedy SOMP algorithm. We believe a deeper investigation towards eﬃcient

solutions to our modiﬁed optimization problem is a worthwhile research pursuit.

5.2.3 Design of Class-speciﬁc Priors

This aspect of our research opens the doors to many interesting theoretical extensions for

robust image classiﬁcation. We have demonstrated in Chapter 4 that sparsity-inducing

93

priors can introduce robustness to limited training. This analytical development has

been demonstrated for the single task classiﬁcation case. In our very recent work [21],

we have shown that the spike-and-slab prior can be extended in a hierarchical manner

to account for collaborative representation problems. The formulations have parallels to

the hierarchical lasso and the collaborative hierarchical lasso [84], with the additional

improvement that our formulation explicitly captures sparsity via an l0-norm minimiza-

tion (compared to l1-relaxations in [84]). These extensions readily carry forward to

classiﬁcation tasks by learning the priors in a class-speciﬁc manner.

We obtain multiple optimization problems, one for each class. These are distinguished

by the diﬀerent values of regularization parameters, chosen in a class-speciﬁc manner. So

an important research challenge is the accurate selection of regularization parameters.

One of the beneﬁts of Bayesian learning is that a full posterior estimate of parameter

distributions is obtained, instead of a point estimate used in the optimization-based ap-

proaches. Other approaches to learn these parameters, such as MCMC sampling, can

also be explored. We have primarily considered the spike-and-slab priors on account

of their wide acceptability in modeling sparse signals. An interesting research direction

would be the investigation of other families of priors that can simultaneously capture

sparsity and discriminative structure, the resulting optimization problems and their so-

lutions. Finally, this framework can be applied to a variety of multi-modal classiﬁcation

problems where robustness is an important concern.

Appendix A

A Greedy Pursuit Approach to
Multi-task Classiﬁcation

Notation: Let yyyi ∈ Rn, i = 1, . . . , T be T diﬀerent representations of the same physical
event, which is to be classiﬁed into one of K diﬀerent classes. Let YYY := [yyy1 . . . yyyT ] ∈
Rm×T . Assuming n training samples/events in total, we design T dictionaries DDDi ∈
Rm×n, i = 1, . . . , T , corresponding to the T representations. We deﬁne a new composite
dictionary DDD := [DDD1 . . . DDDT ] ∈ Rm×nT . Further, each dictionary DDDi is represented

as the concatenation of the sub-dictionaries from all classes corresponding to the i-th

representation of the event:

DDDi := [DDD1

i DDD2
i

. . . DDDK

i ],

(A.1)

where DDDj
to the j-th class. So, we have:

i represents the collection of training samples for representation i that belong

DDD := [DDD1 . . . DDDT ] = [DDD1

1 DDD2

1 . . . DDDK
1

. . . DDD1

T DDD2

T . . . DDDK
T ].

(A.2)

A test event YYY can now be represented as a linear combination of training samples

as follows:

YYY = [yyy1

. . . yyyT ] = DDDSSS

T(cid:3) [ααα1
where the coeﬃcient vectors αααi ∈ RnT , i = 1, . . . , T , and SSS = [ααα1

= (cid:2)DDD1

T . . . DDDK

1 DDD2

1 . . . DDDK
1

Since SSS obeys column correspondence, we introduce a new matrix SSS(cid:48)

. . . DDD1

T DDD2

. . . αααT ] ,

. . . αααT ] ∈ RnT×T .

∈ Rn×T as the

Algorithm 3 SOMP for multi-task multivariate sparse representation-based classiﬁca-
tion
Require: Dictionary DDD, signal matrix YYY , number of iterations K

95

Initialization: residual RRR0 = YYY , index set Λ0 = φ, iteration counter k = 1
while k ≤ K do
(1) Find the index of the atom that best approximates all residuals:
λi,k = arg max

j=1,...,n(cid:80)T

q=1 wq(cid:13)(cid:13)RRRt

k−1dddq,j(cid:13)(cid:13)p , p ≥ 1

(2) Update the index set Λi,k = Λi,k−1(cid:83){λi,k} , i = 1, . . . , T
(3) Compute the orthogonal projector pppi,k = (cid:16)DDDt
1, . . . , T , where DDDΛi,k ∈ Rn×k consists of the k atoms in DDDi indexed in Λi,k
(4) Update the residual matrix RRRk = YYY −(cid:2)DDDΛ1,kppp1,k
(5) Increment k: k ← k + 1

DDDΛi,k(cid:17)−1
. . . DDDΛT,kpppT,k(cid:3)

DDDt

yyyi,

Λi,k

Λi,k

end while

(cid:48)

Ensure: Index set Λi = Λi,K, i = 1, . . . , T ; sparse representation ˆSSS

whose non-zero rows
indexed for each representation by Λi, i, i = 1, . . . , T , are the K rows of the matrix

for i =

(cid:16)DDDt

Λi,K

DDDΛi,K(cid:17)−1

DDDt

Λi,K

YYY .

transformation of SSS with the zero coeﬃcients removed,

(cid:48)

SSS

=

ααα1
1
...
αααK
1

. . . ααα1
i
...
...
. . . αααK
i

ααα1
T
...
...
. . . αααK
T

 ,

where αααj
i refers to the sub-vector extracted from αααi that corresponds to coeﬃcients from
the j-th class. Note that, in the i-th column of SSS(cid:48), only the coeﬃcients corresponding to
DDDi are retained (for i = 1, . . . , T ).

We can now apply row-sparsity constraints similar to the approach in [86]. Our

modiﬁed optimization problem becomes:

(cid:48)

ˆSSS

= arg min

SSS(cid:48) (cid:13)(cid:13)SSS

(cid:48)(cid:13)(cid:13)row,0

subject to (cid:107)YYY − DDDSSS(cid:107)F ≤ ,

(A.3)

for some tolerance  > 0. We minimize the number of non-zero rows, while the constraint

guarantees a good approximation.

The matrix SSS can be transformed into SSS(cid:48) by introducing matrices HHH ∈ RnT×T and

JJJ ∈ Rn×nT ,

HHH = diag [111 111 . . . 111] , JJJ = [III n III n . . . III n] ,

where 111 ∈ Rn is the vector of all ones, and III n denotes the n-dimensional identity matrix.

96

Finally, we obtain SSS(cid:48) = JJJ (HHH ◦ SSS), where ◦ denotes the Hadamard product, (HHH ◦ SSS)ij (cid:44)
hijsij for all i, j. Eq. (A.3) represents a hard optimization problem due to presence of
the non-invertible transformation from SSS to SSS(cid:48). We bypass this diﬃculty by proposing
a modiﬁed version of the SOMP algorithm for the multi-task multivariate case.

Recall that the original SOMP algorithm gives K distinct atoms (assuming K itera-

tions) from a dictionary D that best represent the data matrix Y . In every iteration k,

SOMP measures the residual for each atom in D and creates an orthogonal projection

with maximal correlation. Extending this to the multi-task setting, for every representa-

tion i, i = 1, . . . , T , we can identify the index set that gives the highest correlation with

the residual at the k-th iteration as follows:

λi,k = arg max
j=1,...,n

T(cid:88)q=1

wq(cid:13)(cid:13)RRRt

k−1dddq,j(cid:13)(cid:13)p , p ≥ 1,

where wq denotes the weight (conﬁdence) assigned to the q-th representation, dddq,j repre-
sents the j-th column of DDDq, q = 1, . . . , T , and the superscript (·)t indicates the matrix
transcript operator. After ﬁnding λi,k, we modify the index set to:

Λi,k = Λi,k−1(cid:91) λi,k, i = 1, . . . , T.

Thus, by ﬁnding the index set for the T distinct representations, we can create an

orthogonal projection with each of the atoms in their corresponding representations.

The algorithm is summarized in Algorithm 3.

Appendix B

Parameter Learning for SSPIC

Algorithm 4 Parameter learning
Require: AAA

Initialization: iteration counter k = 1
while k ≤ K do

(1) Update spike-and-slab parameters (i = 1, . . . , n):

xjaaaj

i aaai(cid:1)−1

ˆyyyi ← yyy −(cid:88)j(cid:54)=i
Σi = σ2(cid:0)λ + aaaT
1
σ2 ΣiaaaT
i ˆyyyi
γi ← (cid:18)1 +

µi =

γi exp(−1/2Σiµ2
(xi|−) ∼ (1 − γi)δ0 + γiN (µi, Σi)

1 − γi

i )(cid:19)−1

(2) Update signal noise variance:

(σ2|−) ∼ Γ

−1(cid:0)e0 + 0.5m, f0 + 1/2(cid:107)yyy − AAAxxx(cid:107)2
2(cid:1)

(3) Increment k: k ← k + 1

end while

Ensure: σ2,{γi}

Bibliography

[1] S. L. Lauritzen, Graphical Models. Oxford University Press, USA, 1996.

[2] M. J. Wainwright and M. I. Jordan, “Graphical models, exponential families and
variational inference,” Foundations and Trends in Machine Learning, vol. 1, no.
1-2, pp. 1–305, 2008.

[3] V. Y. F. Tan, S. Sanghavi, J. W. Fisher, and A. S. Willsky, “Learning graphical
models for hypothesis testing and classiﬁcation,” IEEE Trans. Signal Process.,
vol. 58, no. 11, pp. 5481–5495, Nov 2010.

[4] E. Cand`es, J. Romberg, and T. Tao, “Robust uncertainty principles: Exact sig-
nal reconstruction from highly incomplete frequency information,” IEEE Trans.
Inform. Theory, vol. 52, no. 2, pp. 489–509, Feb. 2006.

[5] D. S. Taubman and M. W. Marcellin, JPEG 2000: Image Compression Funda-

mentals, Standards and Practice. Kluwer Academic, Norwell, MA, 2001.

[6] M. Lustig, D. L. Donoho, and J. L. Pauly, “Sparse MRI: The application of com-
pressed sensing for rapid MR imaging,” Magnetic Resonance in Medicine, vol. 58,
pp. 1182–1195, 2007.

[7] J. Wright, A. Y. Yang, A. Ganesh, S. Sastry, and Y. Ma, “Robust face recognition
via sparse representation,” IEEE Trans. Pattern Anal. Machine Intell., vol. 31,
no. 2, pp. 210–227, Feb. 2009.

[8] A. Wagner, J. Wright, A. Ganesh, Z. Zhou, H. Mobahi, and Y. Ma, “Towards a
practical face recognition system: Robust alignment and illumination by sparse
representation,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 34, no. 2, pp. 372–
386, Feb. 2012.

[9] V. Cevher, P. Indyk, L. Carin, and R. G. Baraniuk, “Sparse signal recovery and
acquisition with graphical models,” IEEE Signal Process. Mag., vol. 27, no. 6, pp.
92–103, 2010.

99

[10] J. A. O’Sullivan, M. D. DeVore, V. Kedia, and M. I. Miller, “SAR ATR per-
formance using a conditionally Gaussian model,” IEEE Trans. Aerosp. Electron.
Syst., vol. 37, no. 1, pp. 91–108, 2001.

[11] M. D. DeVore and J. A. O’Sullivan, “Performance complexity study of several ap-
proaches to automatic target recognition from SAR images,” IEEE Trans. Aerosp.
Electron. Syst., vol. 38, no. 2, pp. 632–648, 2002.

[12] U. Srinivas, V. Monga, and R. G. Raj, “Automatic target recognition using using
discriminative graphical models,” in Proc. IEEE Int. Conf. Image Process., 2011,
pp. 33–36.

[13] ——, “SAR automatic target recognition using discriminative graphical models,”

IEEE Trans. Aerosp. Electron. Syst., vol. 50, no. 1, pp. 591–606, Jan. 2014.

[14] U. Srinivas, Y. Chen, V. Monga, N. M. Nasrabadi, and T. D. Tran, “Discriminative
graphical models for sparsity-based hyperspectral target detection,” in Proc. IEEE
Int. Geosci. Remote Sens. Symp., 2012, pp. 1489–1492.

[15] ——, “Exploiting sparsity in hyperspectral image classiﬁcation via graphical mod-

els,” IEEE Geosci. Remote Sens. Lett., vol. 10, no. 3, pp. 505–509, May 2013.

[16] U. Srinivas, V. Monga, Y. Chen, and T. D. Tran, “Sparsity-based face recognition
using discriminative graphical models,” in Proc. IEEE Asilomar Conf. on Signals,
Systems and Computers, 2011, pp. 1204–1208.

[17] U. Srinivas, N. M. Nasrabadi, and V. Monga, “Graph-based multi-sensor fusion
for acoustic signal classiﬁcation,” in Proc. IEEE Int. Conf. Acoust., Speech, Signal
Process., 2013, pp. 261–265.

[18] ——, “Graph-based sensor fusion for classiﬁcation of transient acoustic signals,”

IEEE Trans. Cybern., vol. 45, no. 3, pp. 562–573, Mar. 2015.

[19] U. Srinivas, H. Mousavi, C. Jeon, V. Monga, A. Hattel, and B. Jayarao, “SHIRC:
A simultaneous Sparsity model for Histopathological Image Representation and
Classiﬁcation,” in Proc. IEEE Int. Symp. Biomed. Imag., 2013, pp. 1106–1109.

[20] U. Srinivas, H. Mousavi, V. Monga, A. Hattel, and B. Jayarao, “Simultaneous
sparsity model for histopathological image representation and classiﬁcation,” IEEE
Trans. Med. Imag., vol. 33, no. 5, pp. 1163–1179, May 2014.

[21] Y. Suo, M. Dao, T. D. Tran, U. Srinivas, and V. Monga, “Hierarchical sparse
modeling using spike and slab priors,” in Proc. IEEE Int. Conf. Acoust., Speech,
Signal Process., 2013, pp. 3103–3107.

[22] U. Srinivas, Y. Suo, M. Dao, V. Monga, and T. D. Tran, “Structured sparse priors
for image classiﬁcation,” in Proc. IEEE Int. Conf. Image Process., 2013, pp. 3211–
3215.

100

[23] ——, “Structured sparse priors for image classiﬁcation,” IEEE. Trans. Image Pro-

cess., vol. 24, no. 6, pp. 1763–1776, June 2015.

[24] C. K. Chow and C. N. Liu, “Approximating discrete probability distributions with

dependence trees,” IEEE Trans. Inf. Theory, vol. 14, no. 3, pp. 462–467, 1968.

[25] J. B. Kruskal, “On the shortest spanning subtree of a graph and the traveling

salesman problem,” Proc. Amer. Math. Soc., vol. 7, no. 1, pp. 48–50, Feb. 1956.

[26] N. Friedman, D. Geiger, and M. Goldszmidt, “Bayesian network classiﬁers,” Ma-

chine Learning, vol. 29, pp. 131–163, 1997.

[27] G. F. Cooper, “The computational complexity of probabilistic inference using

Bayesian belief networks,” Artiﬁcial Intell., vol. 42, pp. 393–405, March 1990.

[28] D. Karger and N. Srebro, “Learning Markov networks: maximum bounded tree-

width graphs,” in Symposium on Discrete Algorithms, 2001, pp. 392–401.

[29] H. Drucker and C. Cortes, “Boosting decision trees,” in Neural Information Pro-

cessing Systems, 1996, pp. 479–485.

[30] T. Downs and A. Tang, “Boosting the tree augmented naive Bayes classiﬁer,” in
Intelligent Data Engineering and Automated Learning, Lecture Notes in Computer
Science, vol. 3177, 2004, pp. 708–713.

[31] Y. Freund and R. E. Shapire, “A short introduction to boosting,” J. Jap. Soc.

Artiﬁcial Intell., vol. 14, no. 5, pp. 771–780, Sep. 1999.

[32] Y. Sun, Z. Liu, S. Todorovic, and J. Li, “Adaptive boosting for SAR automatic
target recognition,” IEEE Trans. Aerosp. Electron. Syst., vol. 43, no. 1, pp. 112–
125, Jan. 2007.

[33] B. Bhanu and T. L. Jones, “Image understanding research for automatic target

recognition,” IEEE Aerosp. Electron. Syst. Mag, pp. 15–22, Oct. 1993.

[34] Q. Zhao and J. Principe, “Support vector machines for SAR automatic target
recognition,” IEEE Trans. Aerosp. Electron. Syst., vol. 37, no. 2, pp. 643–654,
April 2001.

[35] T. Ross, S. Worrell, V. Velten, J. Mossing, and M. Bryant, “Standard SAR ATR
evaluation experiments using the MSTAR public release data set,” in Proc. SPIE,
Algorithms for Synthetic Aperture Radar Imagery V, vol. 3370, April 1998, pp.
566–573.

[36] V. Bhatnagar, A. Shaw, and R. W. Williams, “Improved automatic target recogni-
tion using singular value decomposition,” IEEE Int. Conf. Acoust., Speech, Signal
Process., vol. 5, pp. 2717–2720, 1998.

[37] D. Casasent and Y.-C. Wang, “A new hierarchical classiﬁer using new support
vector machines for automatic target recognition,” Neural Networks, vol. 18, pp.
541–548, 2005.

101

[38] C. Tison, N. Pourthie, and J. C. Souyris, “Target recognition in SAR images with
support vector machines (SVM),” in IEEE Int. Geosci. Remote Sens. Symp., 2007,
pp. 456–459.

[39] J. C. Mossing, T. D. Ross, and J. Bradley, “An evaluation of SAR ATR algorithm
performance sensitivity to MSTAR extended operating conditions,” in Proc. SPIE,
Algorithms for Synthetic Aperture Radar Imagery V, vol. 3370, 1998, pp. 554–565.

[40] “The Airforce Moving and Stationary Target Recognition Database.” [Online].

Available: https://www.sdms.afrl.af.mil/datasets/mstar/

[41] C. F. Olson and D. P. Huttenlocher, “Automatic target recognition by matching
oriented edge pixels,” IEEE Trans. Image Process., vol. 6, no. 1, pp. 103–113, Jan.
1997.

[42] A. Mahalanobis, D. W. Carlson, and B. V. Kumar, “Evaluation of MACH and
DCCF corelation ﬁlters for SAR ATR using MSTAR public data base,” in Proc.
SPIE, Algorithms for Synthetic Aperture Radar Imagery V, vol. 3370, 1998, pp.
460–468.

[43] N. M. Sandirasegaram, “Spot SAR ATR using wavelet features and neural network

classiﬁer,” Technical Memorandum, DRDC Ottawa TM 2005-154, Oct. 2005.

[44] S. Suvorova and J. Schroeder, “Automated target recognition using the Karhunen-
Loeve transform with invariance,” Digital Signal Processing, vol. 12, no. 2-3, pp.
295–306, 2002.

[45] U. Grenander, M. I. Miller, and A. Srivastava, “Hilbert-Schimdt lower bounds
for estimators on matrix lie groups for ATR,” IEEE Trans. Pattern Anal. Mach.
Intell., vol. 20, no. 8, pp. 790–802, Aug. 1998.

[46] X. Yu, X. Wang, and B. Liu, “Radar target recognition using a modiﬁed kernel
direct discriminant analysis algorithm,” IEEE Int. Conf. Commun., Circuits Syst.,
pp. 942–946, 2007.

[47] C. E. Daniell, D. H. Kemsley, W. P. Lincoln, W. A. Tackett, and G. A.
Baraghimian, “Artiﬁcial neural networks for automatic target recognition,” Op-
tical Engineering, vol. 31, no. 12, pp. 2521–2531, 1992.

[48] M. Simard, S. Saatchi, and G. DeGrandi, “Classiﬁcation of the Gabon SAR mosaic
using a wavelet based rule classiﬁer,” in IEEE Int. Geosci. Remote Sens. Symp.,
vol. 5, 1999, pp. 2768–2770.

[49] A. S. Paul, A. K. Shaw, K. Das, and A. K. Mitra, “Improved HRR-ATR using
hybridization of HMM and eigen-template-matched ﬁltering,” in IEEE Int. Conf.
Acoust., Speech, Signal Process., 2003, pp. 397–400.

[50] J. P. P. Gomes, J. F. B. Brancalion, and D. Fernandes, “Automatic target recogni-
tion in synthetic aperture radar image using multiresolution analysis and classiﬁers
combination,” in IEEE Radar Conf., 2008, pp. 1–5.

102

[51] J. Kittler, M. Hatef, R. P. W. Duin, and J. Matas, “On combining classiﬁers,”
IEEE Trans. Pattern Anal. Mach. Intell., vol. 20, no. 3, pp. 226–239, March 1998.

[52] U. Srinivas, V. Monga, and R. G. Raj, “Meta-classiﬁers for exploiting feature
dependencies in automatic target recognition,” in Proc. IEEE Radar Conference,
May 2011, pp. 147–151.

[53] S. A. Rizvi and N. M. Nasrabadi, “Fusion techniques for automatic target recog-
nition,” Proc. 32nd Applied Imagery Pattern Recognition Workshop, pp. 27–32,
2003.

[54] S. Papson and R. Narayanan, “Multiple location SAR/iSAR image fusion for en-
hanced characterization of targets,” Proc. SPIE, Radar Sensor Technology IX, vol.
5788, pp. 128–139, 2005.

[55] H. Kwon and N. M. Nasrabadi, “Kernel matched subspace detectors for hyper-
spectral target detection,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 28, no. 2,
pp. 178–194, Feb. 2006.

[56] L. I. Perlovsky, J. A. Chernick, and W. H. Schoendor, “Multi-sensor ATR and
identiﬁcation of friend or foe using MLANS,” Neural Networks, Special Issue,
vol. 8, pp. 1185–1200, 1995.

[57] N. M. Nasrabadi, “A nonlinear kernel-based joint fusion/detection of anamolies
using hyperspectral and SAR imagery,” IEEE Int. Conf. Image Process., pp. 1864–
1867, Oct. 2008.

[58] R. Singh and B. V. Kumar, “Performance of the extended maximum average cor-
relation height (EMACH) ﬁlter and the polynomial distance classiﬁer correlation
ﬁlter (PDCCF) for multi-class SAR detection and classiﬁcation,” in Proc. SPIE,
Algorithms for Synthetic Aperture Radar Imagery IX, vol. 4727, 2002, pp. 265–276.

[59] Q. Zhao, D. X. Xu, and J. Principe, “Pose estimation of SAR automatic recogni-

tion,” in Proc. Image Understanding Workshop, 1998, pp. 827–832.

[60] D. Casasent and A. Nehemiah, “Confuser rejection performance of EMACH ﬁlters
for MSTAR ATR,” in Proc. SPIE, Optical Pattern Recognition XVII, vol. 6245,
2006, pp. 62 450D–1–62 450D–12.

[61] C. Yuan and D. Casasent, “MSTAR 10-Class classiﬁcation and confuser and clutter
rejection using SVRDM,” in Proc. SPIE, Optical Pattern Recognition XVII, vol.
6245, 2006, pp. 624 501–1–624 501–13.

[62] R. Patnaik and D. Casasent, “SAR classiﬁcation and confuser and clutter rejection
tests on MSTAR ten-class data using Minace ﬁlters,” in Proc. SPIE, Automatic
Target Recognition XVI, vol. 6574, 2007, pp. 657 402–1–657 402–15.

[63] G. F. Hughes, “On the mean accuracy of statistical pattern recognizers,” IEEE

Trans. Inf. Theory, vol. 14, no. 1, pp. 55–63, Jan. 1968.

103

[64] C. M. Bishop, Pattern Recognition and Machine Learning. Springer, 2006.

[65] I. Guyon and A. Elisseeﬀ, “An introduction to variable and feature selection,” J.

Mach. Learn. Res., vol. 3, pp. 1157–1182, 2003.

[66] H. Liu and L. Yu, “Toward integrating feature selection algorithms for classiﬁcation
and clustering,” IEEE Trans. Knowl. Data Eng., vol. 17, no. 4, pp. 491–502, Apr.
2005.

[67] D. Marr, Vision: A Computational Investigation into the Human Representation

and Processing of Visual Information. Henry Holt, NY, 1982.

[68] B. A. Olshausen and D. J. Field, “Emergence of simple-cell receptive ﬁeld proper-
ties by learning a sparse code for natural images,” Nature, no. 381, pp. 607–609,
Jul. 1996.

[69] B. Olshausen and D. Field, “Sparse coding with an overcomplete basis set: A

strategy employed by V1?” Vision Research, vol. 37, pp. 3311–3325, 1997.

[70] T. Serre, “Learning a dictionary of shape-components in visual cortex: Comparison

with neurons, humans and machines,” Ph.D. Thesis, MIT, 2006.

[71] M. Aharon, M. Elad, and A. M. Bruckstein, “K-SVD: An algorithm for designing
overcomplete dictionaries for sparse representation,” IEEE Trans. Signal Process.,
vol. 54, no. 11, pp. 4311–4322, Nov. 2006.

[72] J. Watkinson, The MPEG Handbook. Elsevier, 2001.

[73] D. L. Donoho, “For most large underdetermined systems of linear equations the
minimal l1-norm solution is also the sparsest solution,” Comm. Pure Appl. Math.,
vol. 59, no. 6, pp. 797–829, 2006.

[74] R. Tibshirani, “Regression shrinkage and selection via the LASSO,” J. Royal Sta-

tistical Soc. B, vol. 58, no. 1, pp. 267–288, 1996.

[75] R. Rubinstein, A. M. Bruckstein, and M. Elad, “Dictionaries for sparse represen-

tation modeling,” Proc. IEEE, vol. 98, no. 6, pp. 1045–1057, Jun. 2010.

[76] A. M. Bruckstein, D. L. Donoho, and M. Elad, “From sparse solutions of systems
of equations to sparse modeling of signals and images,” SIAM Review, vol. 51,
no. 1, pp. 34–81, 2009.

[77] J. Tropp and A. Gilbert, “Signal recovery from random measurements via orthog-
onal matching pursuit,” IEEE Trans. Inf. Theory, vol. 53, no. 12, pp. 4655–4666,
2005.

[78] W. Dai and O. Milenkovic, “Subspace pursuit for compressive sensing signal re-
construction,” IEEE Trans. Inform. Theory, vol. 55, no. 5, pp. 2230–2249, May
2009.

104

[79] Y. Yu, J. Huang, S. Zhang, C. Restif, X. Huang, and D. Metaxas, “Group spar-
sity based classiﬁcation for cervigram segmentation,” in Proc. IEEE Int. Symp.
Biomed. Imag., 2011, pp. 1425–1429.

[80] E. Elhamifar and R. Vidal, “Sparse subspace clustering,” in Proc. IEEE Conf.

Comput. Vision Pattern Recognition, Jun. 2009, pp. 2790–2797.

[81] J. K. Pillai, V. M. Patel, R. Chellappa, and N. K. Ratha, “Secure and robust iris
recognition using random projections and sparse representations,” IEEE Trans.
Pattern Anal. Machine Intell., vol. 33, no. 9, pp. 1877–1893, Sep. 2011.

[82] Y. Chen, N. Nasrabadi, and T. D. Tran, “Hyperspectral image clasiﬁcation us-
ing dictionary-based sparse representation,” IEEE Trans. Geosci. Remote Sens.,
vol. 49, no. 10, pp. 3973–3985, Oct. 2011.

[83] M. Yuan and Y. Lin, “Model selection and estimation in regression with grouped

variables,” J. Royal Stat. Soc. B, vol. 68, no. 1, pp. 49–67, 2007.

[84] P. Sprechmann, I. Ramirez, G. Sapiro, and Y. C. Eldar, “C-HiLasso: A collabora-
tive hierarchical sparse modeling framework,” IEEE Trans. Signal Process., vol. 59,
no. 9, pp. 4183–4198, Sep. 2011.

[85] A. Majumdar and R. K. Ward, “Classiﬁcation via group sparsity promoting reg-
ularization,” in Proc. IEEE Int. Conf. Acoust., Speech, Signal Process., 2009, pp.
861–864.

[86] J. A. Tropp, A. C. Gilbert, and M. J. Strauss, “Algorithms for simultaneous sparse
approximation. Part I: Greedy pursuit,” Signal Processing, vol. 86, pp. 572–588,
Apr. 2006.

[87] J. A. Tropp, “Algorithms for simultaneous sparse approximation. Part II: Convex

relaxation,” Signal Processing, vol. 86, pp. 589–602, Apr. 2006.

[88] M. Borengasser, W. S. Hungate, and R. Watkins, Hyperspectral Remote Sensing -

Principles and Applications. CRC Press, 2008.

[89] D. Manolakis and G. Shaw, “Detection algorithms for hyperspectral imaging ap-

plications,” IEEE Signal Process. Mag., vol. 19, no. 1, pp. 29–43, Jan. 2002.

[90] V. N. Vapnik, The nature of statistical learning theory. New York, USA: Springer,

1995.

[91] F. Melgani and L. Bruzzone, “Classiﬁcation of hyperspectral remote sensing images
with support vector machines,” IEEE Trans. Geosci. Remote Sens., vol. 42, no. 8,
pp. 1778–1790, Aug. 2004.

[92] G. Camps-Valls, L. Gomez-Chova, J. Mu˜noz-Mar´ı, J. Vila-Franc´es, and J. Calpe-
image classiﬁcation,” IEEE

Maravilla, “Composite kernels for hyperspectral
Geosci. Remote Sens. Lett., vol. 3, no. 1, pp. 93–97, Jan. 2006.

105

[93] F. Bovolo, L. Bruzzone, and M. Marconcini, “A novel context-sensitive SVM for
classiﬁcation of remote sensing images,” in Proc. IEEE Int. Geosci. Remote Sens.
Symp., Denver, Jul. 2006, pp. 2498–2501.

[94] Y. Tarabalka, J. A. Benediktsson, and J. Chanussot, “Spectral-spatial classiﬁcation
of hyperspectral imagery based on partitional clustering techniques,” IEEE Trans.
Geosci. Remote Sens., vol. 47, no. 8, pp. 2973–2987, Aug. 2009.

[95] J. Li, J. M. Bioucas-Dias, and A. Plaza, “Spectral-spatial hyperspectral image
segmentation using subspace multinomial logistic regression and Markov random
ﬁelds,” IEEE Trans. Geosci. Remote Sens., to appear.

[96] L. L. Scharf and B. Friedlander, “Matched subspace detectors,” IEEE Trans. Signal

Process., vol. 42, no. 8, pp. 2146–2157, Aug. 1994.

[97] Y. Chen, N. M. Nasrabadi, and T. D. Tran, “Simultaneous joint sparsity model
for target detection in hyperspectral imagery,” IEEE Geosci. Remote Sens. Lett.,
vol. 8, no. 4, pp. 676–680, Jul. 2011.

[98] R. W. Basedow, D. C. Carmer, and M. E. Anderson, “HYDICE system: Implemen-
tation and performance,” in Proc. SPIE Conf. Algorithms Technol. Multispectral,
Hyperspectral, Ultraspectral Imagery XV, 1995, pp. 258–267.

[99] Y. Chen, N. M. Nasrabadi, and T. D. Tran, “Sparse representation for target
detection in hyperspectral imagery,” IEEE J. Sel. Topics Signal Process., vol. 5,
no. 3, pp. 629–640, Jun. 2011.

[100] J. A. Gualtieri and R. F. Cromp, “Support vector machines for hyperspectral

remote sensing classiﬁcation,” Proc. SPIE, vol. 3584, pp. 221–232, Jan. 1998.

[101] “AVIRIS NW Indiana’s Indian Pines 1992 Data Set.” [Online]. Available:

http://cobweb.ecn.purdue.edu/biehl/MultiSpec/documentation.html

[102] W. Zhao, R. Chellappa, P. J. Phillips, and A. Rosenfeld, “Face recognition: A

literature survey,” ACM Comput. Surv., vol. 35, pp. 399–458, 2003.

[103] L. Sirovich and M. Kirby, “Low-dimensional procedure for the characterization of

human faces,” J. Optical Soc. of Am. A, vol. 4, no. 3, pp. 519–524, Mar. 1987.

[104] M. Turk and A. Pentland, “Eigenfaces for recognition,” J. Cogn. Neurosci., vol. 3,

no. 1, pp. 71–86, 1991.

[105] J. Zou, Q. Ji, and G. Nagy, “A comparative study of local matching approach
for face recognition,” IEEE Trans. Image Process., vol. 16, no. 10, pp. 2617–2628,
2007.

[106] A. Shashua, “Geometry and photometry in 3D visual recognition,” Ph.D. disser-

tation, MIT, 1992.

106

[107] P. N. Belhumeur, J. P. Hespanha, and D. J. Kriegman, “Eigenfaces vs. ﬁsher-
faces: Recognition using class speciﬁc linear projection,” IEEE Trans. Pattern
Anal. Mach. Intell., vol. 19, no. 7, pp. 711–720, 1997.

[108] C. Liu and H. Wechsler, “A shape- and texture-based enhanced ﬁsher classiﬁer for

face recognition,” IEEE Trans. Image Process., vol. 10, pp. 598–608, 2001.

[109] R. Basri and D. W. Jacobs, “Lambertian reﬂectance and linear subspaces,” IEEE

Trans. Pattern Anal. Mach. Intell., vol. 25, no. 2, pp. 218–233, Feb. 2003.

[110] W. Zhao, R. Chellappa, P. J. Phillips, and A. Rosenfeld, “Face recognition: A
literature survey,” ACM Computing Surveys, vol. 35, no. 4, pp. 399–458, Dec.
2003.

[111] X. Hang and F.-X. Wu, “Sparse representation for classiﬁcation of tumors using
gene expression data,” Journal of Biomedicine and Biotechnology, vol. 2009, 2009,
doi:10.1155/2009/403689.

[112] J. Huang, X. Huang, and D. Metaxas, “Simultaneous image transformation and
sparse representation recovery,” in Proc. IEEE Conf. Comput. Vision Pattern
Recognition, Jun. 2008, pp. 1–8.

[113] A. Wagner, J. Wright, A. Ganesh, Z. Zhou, and Y. Ma, “Towards a practical
face recognition system: Robust registration and illumination by sparse represen-
tation,” in Proc. IEEE Conf. Comput. Vision Pattern Recognition, Jun. 2009, pp.
597–604.

[114] Y. Chen, T. T. Do, and T. D. Tran, “Robust face recognition using locally adaptive
sparse representation,” in Proc. IEEE Int. Conf. Image Processing, 2010, pp. 1657–
1660.

[115] A. S. Georghiades, P. N. Belhumeur, and D. J. Kriegman, ““From few to many:
Illumination cone models for face recognition under variable lighting and pose”,”
IEEE Trans. Pattern Anal. Mach. Intell., vol. 23, no. 6, pp. 643–660, Jun 2001.

[116] J. Ho, M. Yang, J. Lim, K. Lee, and D. Kriegman, “Clustering appearances of ob-
jects under varying illumination conditions,” in Proc. IEEE Conf. Comput. Vision
Pattern Recognition, 2003, pp. 11–18.

[117] S. Zhang, J. Huang, H. Li, and D. N. Metaxas, “Automatic image annotation and
retrieval using group sparsity,” IEEE Trans. Syst., Man, Cybern. B, vol. 42, no. 32,
pp. 838–849, Jun. 2012.

[118] X.-T. Yuan, X. Liu, and S. Yan, “Visual classiﬁcation with multitask joint sparse
representation,” IEEE Trans. Image Process., vol. 21, no. 10, pp. 4349–4360, Oct.
2012.

[119] A. J. Mendez, P. G. Tahoces, M. J. Lado, M. Souto, and J. J. Vidal, “Computer-
aided diagnosis: Automatic detection of malignant masses in digitized mammo-
grams,” Med Phys., vol. 25, pp. 957–964, Jun. 1998.

107

[120] L. E. Boucheron, “Object- and spatial-level quantitative analysis of multispectral
histopathology images for detection and characterization of cancer,” Ph.D. disser-
tation, Dept. of ECE, University of California, Santa Barbara, 2008.

[121] A. Madabhushi, “Digital pathology image analysis: Opportunities and challenges,”

Imaging Med., vol. 1, no. 1, pp. 7–10, 2009.

[122] M. N. Gurcan, L. E. Boucheron, A. Can, A. Madabhushi, N. M. Rajpoot, and
B. Yener, “Histopathological image analysis: A review,” IEEE Rev. Biomed. Eng.,
vol. 2, pp. 147–171, 2009.

[123] H. Fox, “Is H&E morphology coming to an end?” J. Clin. Pathol., vol. 53, pp.

38–40, 2000.

[124] G. Alexe, J. Monaco, S. Doyle, A. Basavanhally, A. Reddy, M. Seiler, S. Ganesan,
G. Bhanot, and A. Madabhushi, “Towards improved cancer diagnosis and prognosis
using analysis of gene expression data and computer aided imaging,” Exp. Biol.
Med., vol. 234, pp. 860–879, 2009.

[125] A. Basavanhally, S. Ganesan, S. Agner, J. P. Monaco, M. D. Feldman, J. E.
Tomaszewski, G. Bhanot, and A. Madabhushi, “Computerized image-based detec-
tion and grading of lymphocytic inﬁltration in HER2+ breast cancer histopathol-
ogy,” IEEE Trans. Biomed. Eng., vol. 57, no. 3, pp. 642–653, 2010.

[126] M. M. Dundar, S. Badve, G. Bilgin, V. Raykar, R. Jain, O. Sertel, and M. N. Gur-
can, “Computerized classiﬁcation of intraductal breast lesions using histopatholog-
ical images,” IEEE Trans. Biomed. Eng., vol. 58, no. 7, pp. 1977–1984, 2011.

[127] R. M. Haralick, K. Shanmugam, and I. Dinstein, “Textural features for image
classiﬁcation,” IEEE Trans. Syst., Man, Cybern., vol. SMC-3, no. 6, pp. 610–621,
1973.

[128] J. Serra, Image Analysis and Mathematical Morphology. Academic Press, 1982.

[129] F. Zana and J.-C. Klein, “Segmentation of vessel-like patterns using mathematical
morphology and curvature evaluation,” IEEE Trans. Image Process., vol. 10, no. 7,
pp. 1010–1019, 2001.

[130] O. Chapelle, P. Haﬀner, and V. N. Vapnik, “Support vector machines for
histogram-based image classiﬁcation,” IEEE Trans. Neural Netw., vol. 10, no. 5,
pp. 1055–1064, 1999.

[131] A. Wetzel, R. Crowley, S. Kim, R. Dawson, L. Zheng, Y. Joo, Y. Yagi, J. Gilbert-
son, C. Gadd, D. Deerﬁeld, and M. Becich, “Evaluation of prostate tumor grades
by content based image retrieval,” in Proc. SPIE, vol. 3584, 1999, pp. 244–252.

[132] A. Esgiar, R. Naguib, B. Sharif, M. Bennett, and A. Murray, “Fractal analysis in
the detection of colonic cancer images,” IEEE Trans. Inf. Technol. Biomed., vol. 6,
no. 1, pp. 54–58, 2002.

108

[133] A. Tabesh, M. Teverovskiy, H. Pang, V. Kumar, D. Verbel, A. Kotsianti, and
O. Saidi, “Multifeature prostate cancer diagnosis and gleason grading of histolog-
ical images,” IEEE Trans. Med. Imag., vol. 26, no. 10, pp. 1366–1378, 2007.

[134] S. Doyle, S. Agner, A. Madabhushi, M. Feldman, and J. Tomaszewski, “Automated
grading of breast cancer histopathology using spectral clustering with textural and
architectural image features,” in Proc. IEEE Int. Symp. Biomed. Imag., 2008, pp.
496–499.

[135] N. Orlov, L. Shamir, T. Macura, J. Johnston, D. M. Eckley, and I. G. Goldberg,
“WND-CHARM: Multi-purpose image classiﬁcation using compound image trans-
forms,” Pattern Recogn. Lett., vol. 29, no. 11, pp. 1684–1693, 2008.

[136] L. Shamir, N. Orlov, D. M. Eckley, T. Macura, J. Johnston, and I. G. Goldberg,
“Wndchrm - an open source utility for biological image analysis,” Source Code for
Biology and Medicine, vol. 3, no. 13, 2008.

[137] A. Basavanhally, S. Ganesan, N. Shih, C. Mies, M. Feldman, J. Tomaszewski, and
A. Madabhushi, “A boosted classiﬁer for integrating multiple ﬁelds of view: Breast
cancer grading in histopathology,” in Proc. IEEE Int. Symp. Biomed. Imag., 2011,
pp. 125–128.

[138] J. D. Hipp, A. Fernandez, C. C. Compton, and U. J. Balis, “Why a pathology
image should not be considered as a radiology image,” J. Pathol. Inform., 2011,
2:26.

[139] S. Naik, S. Doyle, S. Agner, A. Madabhushi, J. Tomaszeweski, and M. Feldman,
“Automated gland and nuclei segmentation for grading of prostate and breast
cancer histopathology,” in ISBI Workshop Comput. Histopathology, 2008, pp. 284–
287.

[140] O. Sertel, J. Kong, U. Catalyurek, G. Lozanski, J. Saltz, and M. Gurcan,
“Histopathological image analysis using model-based intermediate representations
and color texture: Follicular lymphoma grading,” J. Signal Processing Syst.,
vol. 55, no. 1-3, pp. 169–183, 2009.

[141] S. Zhang, J. Huang, D. Metaxas, W. Wang, and X. Huang, “Discriminative sparse
representations for cervigram image segmentation,” in Proc. IEEE Int. Symp.
Biomed. Imag., 2010, pp. 133–136.

[142] M. Liu, L. Lu, X. Ye, S. Yu, and M. Salganicoﬀ, “Sparse classiﬁcation for com-
puter aided diagnosis using learned dictionaries,” in Proc. Med. Image Comput.
Computer Assisted Intervention, vol. 6893, 2011, pp. 41–48.

[143] Z. Liu, J. Yang, and C. Liu, “Extracting multiple features in discriminant color
space for face recognition,” IEEE Trans. Image Process., vol. 19, no. 9, pp. 2502–
2509, 2010.

109

[144] S.-J. Wang, J. Yang, N. Zhang, and C.-G. Zhou, “Tensor discriminant color space
for face recognition,” IEEE Trans. Image Process., vol. 20, no. 9, pp. 2490–2501,
2011.

[145] H. Zhang, N. M. Nasrabadi, Y. Zhang, and T. S. Huang, “Multi-view automatic
target recognition using joint sparse representation,” IEEE Trans. Aerosp. Elec-
tron. Syst., vol. 48, no. 3, pp. 2481–2497, Jul. 2012.

[146] Y. Nesterov, “Gradient methods for minimizing composite objective function,”
CORE, Catholic Univ. Louvain, Louvain-la-Neuve, Belgium, Tech. Rep. 2007/076,
2007.

[147] J. Zou, Q. Ji, and G. Nagy, “A comparative study of local matching approach
for face recognition,” IEEE Trans. Image Process., vol. 16, no. 10, pp. 2617–2628,
2007.

[148] M. Tipping, “Sparse Bayesian learning and the relevance vector machine,” J. Mach.

Learn. Res., pp. 211–244, 2001.

[149] S. Ji, Y. Xue, and L. Carin, “Bayesian compressive sensing,” IEEE Trans. Signal

Process., vol. 56, no. 6, pp. 2346–2356, Jun. 2008.

[150] S. D. Babacan, R. Molina, and A. K. Katsaggelos, “Bayesian compressive sensing
using Laplace priors,” IEEE Trans. Image Process., vol. 19, no. 1, pp. 53–63, Jan.
2010.

[151] R. G. Baraniuk, V. Cevher, M. F. Duarte, and C. Hegde, “Model-based compres-
sive sensing,” IEEE Trans. Inform. Theory, vol. 56, no. 4, pp. 1982–2001, Apr.
2010.

[152] L. He and L. Carin, “Exploiting structure in wavelet-based Bayesian compressive

sensing,” IEEE Trans. Signal Process., vol. 57, no. 9, pp. 3488–3497, Sep. 2009.

[153] T. Blumensath and M. E. Davies, “Sampling theorems for signals from the union
of ﬁnite-dimensional linear subspaces,” IEEE Trans. Inform. Theory, vol. 55, pp.
1872–1882, 2009.

[154] Y. C. Eldar and M. Mishali, “Robust recovery of signals from a structured union
of subspaces,” IEEE Trans. Inform. Theory, vol. 55, no. 11, pp. 5302–5316, Nov.
2009.

[155] S. Ji, Y. Xue, and L. Carin, “Bayesian compressive sensing,” IEEE Trans. Signal

Process., vol. 56, no. 6, pp. 2346–2356, Jun. 2008.

[156] M. Stojnic, F. Parvaresh, and B. Hassibi, “On the reconstruction of block-sparse
signals with an optimal number of measurements,” IEEE Trans. Signal Process.,
vol. 57, no. 8, pp. 3075–3085, Aug. 2009.

[157] D. Baron, M. B. Wakin, M. F. Duarte, S. Sarvotham, and R. G. Baraniuk, “Dis-

tributed compressed sensing,” Tech. Rep., 2005.

110

[158] M. Duarte and Y. C. Eldar, “Structured compressed sensing: From theory to
applications,” IEEE Trans. Signal Process., vol. 59, no. 9, pp. 4053–4085, Sep.
2011.

[159] H. Ishwaran and J. S. Rao, “Spike and slab variable selection: Frequentist and

Bayesian strategies,” Ann. Stat., vol. 33, pp. 730–773, 2005.

[160] E. I. George and R. E. McCulloch, “Variable selection via Gibbs sampling,” J.

Amer. Statist. Assoc., vol. 88, pp. 881–889, 1993.

[161] H. Chipman, “Bayesian variable selection with related predictors,” Canad. J.

Statist., vol. 24, pp. 17–36, 1996.

[162] C. Carvalho, J. Chang, J. Lucas, Q. Wang, J. Nevins, and M. West, “High-
dimensional sparse factor modelling: Applications in gene expression genomics,”
J. Amer. Statist. Assoc., vol. 103, pp. 1438–1456, 2008.

[163] V. Cevher, “Learning with compressible priors,” in Proc. Neural Information Pro-

cessing Systems, Dec. 2009, pp. 261–269.

[164] M. K. Titsias and M. L´azaro-Gredilla, “Spike and slab variational inference for
multi-task and multiple kernel learning,” in Adv. Neural Inform. Process. Syst. 25,
2011.

[165] T.-J. Yen, “A majorization-minimization approach to variable selection using spike

and slab priors,” Ann. Stat., vol. 39, no. 3, pp. 1748–1775, 2011.

[166] H. Zou and T. Hastie, “Regularization and variable selection via the elastic net,”

J. R. Statist. Soc. B, vol. 67, no. 2, pp. 301–320, 2005.

[167] F. Bach, R. Jenatton, J. Mairal, and G. Obozinski, “Optimization with sparsity-
inducing penalties,” Foundations and Trends in Machine Learning, vol. 4, no. 1,
pp. 1–106, 2012.

[168] J. A. Gualtieri and R. F. Cromp, “Support vector machines for hyperspectral

remote sensing classiﬁcation,” in Proc. SPIE, vol. 3584, Jan. 1998, pp. 221–232.

[169] “ADL Histopathological Image Data Set.” [Online]. Available: http://signal.ee.

psu.edu/adl.htm

[170] M. N. Gurcan, L. E. Boucheron, A. Can, A. Madabhushi, N. M. Rajpoot, and
B. Yener, “Histopathological image analysis: a review,” IEEE Rev. Biomed. Eng.,
vol. 2, pp. 147–171, 2009.

[171] A. Willsky, “Multiresolution Markov models for signal and image processing,”

Proc. IEEE, vol. 90, pp. 1396–1458, 2002.

[172] G. Obozinski, B. Taskar, and M. I. Jordan, “Joint covariate selection and joint
subspace selection for multiple classiﬁcation problems,” Stat. Comput., vol. 20,
no. 2, pp. 231–252, 2009.

111

[173] Y. Chen, N. M. Nasrabadi, and T. D. Tran, “Hyperspectral image classiﬁcation via
kernel sparse representation,” IEEE Trans. Geosci. Remote Sens., vol. 51, no. 1,
pp. 217–231, Jan. 2013.

[174] A. Y. Yang, A. Ganesh, S. S. Sastry, and Y. Ma, “Fast l1-minimization algorithms
and an application in robust face recognition: A review,” in Proc. IEEE Int. Conf.
Image Processing, 2010, pp. 1849–1852.

[175] V. Shia, A. Y. Yang, S. S. Sastry, A. Wagner, and Y. Ma, “Fast l1-minimization
and parallelization for face recognition,” in Proc. IEEE Asilomar Conf., 2011, pp.
1199–1203.

Vita

Umamahesh Srinivas

Umamahesh Srinivas received his B.Tech degree in electronics and communication
engineering from the National Institute of Technology Karnataka (NITK), Surathkal,
India in May 2007 and his M.S. degree in electrical engineering from The Pennsylvania
State University in December 2009. During the summer of 2010, he was a summer
research intern at Qualcomm MEMS Technologies, San Jose, CA, where he worked on
halftoning algorithms for color image displays. In the fall of 2012, he was a research
intern at the U.S. Army Research Laboratory, Adelphi, MD.

Mr. Srinivas received the 2012 IEEE Mikio Takagi Prize for the Best Student Paper
at the IEEE International Geoscience and Remote Sensing Symposium held in Munich,
Germany in July 2012. He also received the Best Automatic Target Recognition Student
Paper Award at the SPIE Defense, Security and Sensing Symposium held in Baltimore,
MD in April 2012. He is a student member of IEEE, SPIE and SIAM.

