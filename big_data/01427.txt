6
1
0
2

 
r
a

M
4

 

 
 
]

.

A
F
h
t
a
m

[
 
 

1
v
7
2
4
1
0

.

3
0
6
1
:
v
i
X
r
a

Splines are universal solutions of linear inverse problems with

generalized-TV regularization

Michael Unser∗,

Julien Fageot∗,

John Paul Ward∗†

March 7, 2016

Abstract

Splines come in a variety of ﬂavors that can be characterized in terms of some diﬀerential
operator L. The simplest piecewise-constant model corresponds to the derivative operator.
Likewise, one can extend the traditional notion of total variation by considering more general
operators than the derivative. This leads us to the deﬁnition of the generalized Beppo-Levi
space, ML = {f : (cid:107)Lf(cid:107)TV < ∞}, which is further identiﬁed as the direct sum of two
Banach spaces. We then prove that the minimization of the generalized total variation
(gTV) over ML, subject to some arbitrary (convex) consistency constraints on the linear
measurements of the signal, admits nonuniform L-spline solutions with fewer knots than
the number of measurements. This shows that non-uniform splines are universal solutions
of continuous-domain linear inverse problems with LASSO, L1, or TV-like regularization
constraints. Remarkably, the spline-type is fully determined by the choice of L and does not
depend on the actual nature of the measurements.

Keywords. Sparsity, total variation, splines, inverse problems, compressed sensing

AMS subject classiﬁcations. 41A15, 47A52, 94A20, 46E27, 46N20, 47F05, 34A08, 26A33

1

Introduction

Imposing sparsity constraints is a powerful paradigm for solving ill-posed inverse problems
and/or for reconstructing signals at super-resolution [2]. The concept is central to the theory of
compressed sensing [10, 3] and is currently driving the development of a new generation of al-
gorithms for the reconstruction of biomedical images [22]. There are essentially three strategies
to recover sparse signals. The ﬁrst is the variational approach, where the solution is constrained
by imposing a sparsity-promoting regularization such as a bound on the total variation of the
reconstruction [25, 15, 7, 18]. The second is the ﬁnite-rate-of-innovation (FRI) model [37] that
imposes a parametric form of the solution with a restricted number of degrees of freedom—the
prototypical example is the superposition of a few Dirac impulses with adaptive weights and
locations. The third is the dictionary-based reconstruction where the signal is reconstructed
from a small subset of atoms within a fairly large dictionary of basis functions [11, 24, 13].
It is well known that the ﬁrst and third approaches are equivalent for signal denoising when
the sparsifying transform (or dictionary) is orthogonal and when the regularization functional is
∗Biomedical Imaging Group, ´Ecole polytechnique f´ed´erale de Lausanne (EPFL), CH-1015 Lausanne, Switzer-

land

†University of Central Florida, Orlando, USA

1

chosen to be the (cid:96)1-norm of the expansion coeﬃcients—this property is exploited by the popular
LASSO estimator [31].

Two factors have contributed to make sparsity a remarkably popular research topic during
the past decade. The ﬁrst is the increasing evidence of the superiority of the sparsity-promoting
schemes over the classical linear reconstruction (including the Tikhonov (cid:96)2 regularization) in a
variety of imaging modalities. The second is the theoretical guarantee of perfect recovery—under
strict conditions—that is provided by the theory of compressed sensing [10, 5, 4], although the
current formulation is restricted to the ﬁnite-dimensional setting.

The contribution of this paper is to formulate in the continuous domain the recovery of sparse
signals and to derive the general form of the solution. As a pleasing outcome, we also achieve a
uniﬁcation of point of views: While our formulation of the problem is variational (minimization
of gTV), we obtain a parametric description of the solution—as a nonuniform L-spline—that
agrees with the concept of a FRI signal. Alternatively, one can interpret the solution as the
best N -term representation of the signal within an inﬁnite-dimensional dictionary that consists
of polynomials plus a continuum of shifted Green’s functions (i.e., {ρL(· − τ )}τ∈Rd) of the
regularization operator L. Observe that the described sparsifying eﬀect is much more dramatic
than what can be achieved in the ﬁnite-dimensional setting since one is collapsing a continuum
(integral representation) into a ﬁnite sum. In particular, our theoretical analysis explains why the
solutions of 1D linear inverse problems with a classical total-variation regularization tend to be
piecewise-constant, which is well documented in the literature. Our theorems also extend some
older results on spline interpolation with minimum L1-norms, including the adaptive regression
splines of Mammen and van de Geer [23] and the functional analytic characterization of Fisher
and Jerome [16]. There is a connection as well with the work of Steidl et al. on splines and higher-
order TV [30], although their formulation is strictly discrete and restricted to the denoising
problem.

The paper is organized as follows: First, we revisit the classical topic of splines by providing
an extended operator-based formulation. The cornerstone of our approach is the existence of
a suitable inverse, as stated in Theorem 1. This leads to the identiﬁcation of the generalized
total-variation criterion (gTV) together with its corresponding generalized Beppo-Levi space
(Theorem 2). The central part of the paper is devoted to the characterization of the solution of
a generic inverse problem involving the minimization of gTV (Theorem 4). We conclude with a
discussion of the result and a review of applications. The proofs of the theorems are provided
in the appendix.

2 Notations and preliminaries
S(cid:48)(Rd) denotes the space of tempered distributions: These are deﬁned as continuous linear
functionals µ : ϕ (cid:55)→ (cid:104)µ, ϕ(cid:105) on Schwartz’ space S(Rd) of smooth and rapidly decaying test
functions on Rd [17, 19]. We shall primarily work with the space of real-valued, countably
additive Borel measures on Rd,

sup

ϕ∈S(Rd):(cid:107)ϕ(cid:107)∞=1

(cid:104)w, ϕ(cid:105) < ∞},

M(Rd) = {w ∈ S(cid:48)

(Rd) : (cid:107)w(cid:107)TV =

where w : ϕ (cid:55)→ (cid:104)w, ϕ(cid:105) = (cid:82)

Rd ϕ(x)dw(x). M(Rd) is a Banach space equipped with the “total
variation” norm (cid:107)·(cid:107)TV. The above deﬁnition is compatible with the Riesz-Markov theorem that
states that M(Rd) is the continuous dual of C0(Rd), which is the Banach space of continuous
functions on Rd that vanish at inﬁnity equipped with the supremum norm (cid:107) · (cid:107)∞ [27, Chap. 6].
Concretely, this means that we are allowed to extend the domain of the test functions ϕ from

2

S(Rd) to C0(Rd).

Two key observations in relation to our goal are:

1. the compatibility of the L1 and total-variation norms with the former being stronger than

the latter. Indeed, (cid:107)f(cid:107)L1(Rd) = (cid:107)f(cid:107)TV for all f ∈ L1(Rd);

2. the inclusion of Dirac impulses in M(Rd), but not in L1(Rd). Speciﬁcally, δ(· − x0) ∈

M(Rd) for any ﬁxed oﬀset x0 ∈ Rd with (cid:104)δ(· − x0), ϕ(cid:105) = ϕ(x0) for all ϕ ∈ C0(Rd).

We shall also quantify the algebraic rate of decay/growth of (ordinary) functions of the variable
x ∈ Rd using the spaces

L∞,n0(Rd) = {f : Rd → R : sup
x∈Rd

(|f (x)|(1 + (cid:107)x(cid:107))n0) < +∞}

with n0 ∈ Z. For instance, xm = xm1

1

··· xmd

d ∈ L∞,−n0(Rd) with n0 = |m| = m1 + ··· + md.

3 Splines and operators

There is a powerful association between splines and operators, the idea being that the selection
of an admissible operator L speciﬁes a corresponding type of splines [28, 29][36, Chapter 6].
Deﬁnition 1 (Admissible operator). A linear operator L : X → Y, where X ⊃ S(Rd) and Y
are appropriate subspaces of S(cid:48)(Rd), is called spline-admissible if

1. it is linear shift-invariant (LSI); that is, L{s(·− x0)} = L{s}(·− x0) for any signal s ∈ X ;
2. its null space

NL = {p ∈ X : L{p} = 0} ⊂ L∞,−n0(Rd)

is ﬁnite-dimensional with dimension N0 ≥ 0 and maximal order of polynomial growth
n0 ∈ {0, . . . , N0 − 1};

3. there exists a function ρL : Rd → R of slow growth (the Green’s function of L) such that

L{ρL} = δ where δ is the Dirac distribution.

response (cid:98)L(ω) at ω = ω0. Another standard property is that ρL cannot grow faster than the
The null space of a LSI operator can only include exponential polynomial components of
the form xmej(cid:104)ω0,x(cid:105) which correspond to a zero of multiplicity at least |m| + 1 of the frequency
components in the null space of the operator so that ρL ∈ L∞,−n0(Rd).

The three types of operators that are of interest to us with increasing level of complexity are:
(i) the ordinary diﬀerential operators, which are polynomials of the derivative operator D = d
dx
[28, 6, 34], (ii) the partial diﬀerential operators such as the Laplacian ∆ (or some polynomial
2 with γ ∈ R+ whose Fourier
thereof), and (iii) the fractional derivatives such as Dγ or (−∆)
symbols are (jω)γ and (cid:107)ω(cid:107)γ, respectively [33, 35, 12]. It can be shown that all LSI operators of
type (i) and all elliptic operators of type (ii) are spline-admissible. The same holds true for the
fractional derivatives, although the conditions need to be checked on a case-by-case basis.
Deﬁnition 2 (Nonuniform L-spline). A function s : Rd → R of slow growth (i.e., s ∈ L∞,−n0(Rd)
with n0 ≥ 0) is said to be a nonuniform L-spline if

γ

where (an) is a sequence of weights and the Dirac impulses are located at the spline knots {xn}.
The generalized function L{s} = wδ is called the spline innovation.

L{s} =

anδ(· − xn) = wδ,

(cid:88)

n∈N

3

Figure 1: Interpretation of a piecewise-constant signal as a D-spline. The application of the
derivative operator uncovers the spline innovation wδ: The Dirac impulses are located at the
points of discontinuity (knots), while their height (weight) encodes the magnitude of the corre-
sponding jump.

The concept is illustrated in Figure 1 for the simple case of a polynomial spline of degree
zero. The deﬁning operator there is D = d
dx whose the null space with (N0, n0) = (1, 0) is
ND = span{p1}, where p1(x) = 1 is the constant signal. The Green’s function of D is the unit
step (or Heaviside function) 1+(x), which is the impulse response of the integrator D−1. This
suggests that one can reconstruct the piecewise-constant signal by integrating its innovation,
which yields the explicit representation

s(x) = b +

an1+(x − xn),

(cid:88)

n∈N

where b = b × p1(x) is some integration constant that lies in the null space of the operator.

Remarkably, the synthesis mechanism that has just been described is generic and applicable
to the complete family of L-splines. The only delicate part is the proper handling of the “inte-
gration constants” (the part of the solution that lies in the null space of the operator), which is
achieved through the speciﬁcation of boundary conditions. We shall now show that these can be
−1
incorporated in the deﬁnition of the inverse operator L
φ whose kernel (or generalized impulse
response) is denoted by gφ(x, y) = L
Theorem 1 (Stable right-inverse operator). Let p = (p1, . . . , pN0) be a basis of NL ⊂ L∞,−n0(Rd)
and φ = (φ1, . . . , φN0) an admissible1 set of biorthogonal analysis distributions such that (cid:104)φm, pn(cid:105) =
δm−n. Then, the kernel

−1
φ {δ(· − y)}(x).

gφ(x, y) = ρL(x − y) − N0(cid:88)

pn(x)qn(y),

(1)

1φn ∈ S(cid:48)(Rd) should have suﬃciently decay and a limited degree of singularity so that the required duality

products are well-deﬁned.

n=1

4

a1x1s(x)w (x)=ddxs(x)xxwith ρL such that L{ρL} = δ and qn(y) = (cid:104)ρL(·− y), φn(cid:105), speciﬁes an operator L

−1
φ that satisﬁes

−1
• Right-inverse property: LL
φ w = w, for all w ∈ M(Rd)
−1
• Boundary conditions: (cid:104)φ, L
φ w(cid:105) = 0 for all w ∈ M(Rd)

−1
φ continuously maps M(Rd) → L∞,−n0(Rd) if and only if gφ(·, y0) ∈ L∞,−n0(Rd)

Moreover, L
for any ﬁxed y0 ∈ Rd.

The proof of this result, which is fundamental to our formulation, is given in Appendix A.
Since the choice of the N0 linear boundary conditions (cid:104)φ, s(cid:105) = 0 is somewhat arbitrary,
there is some ﬂexibility in deﬁning admissible inverse operators. The important ingredient for
our formulation is the existence of such operators, which is substantiated by the fact that we are
able to provide speciﬁc examples for any known brand of splines. For instance, for L = DN0, we

have that ρDN0 (x) =
basis is biorthogononal to φ with φn(x) = δ(n−1)(x).

xN0−1
(N0−1)! and pn(x) = xn−1

+

(n−1)! for n = 1, . . . , N0 where the latter polynomial

4 Generalized Beppo-Levi spaces

The proposed methodology to solve ill-posed inverse problems is to impose a regularization by
searching for a consistent solution that minimizes the gTV semi-norm (cid:107)Lf(cid:107)TV. We therefore
specify our search space as

ML(Rd) = {f ∈ X : Lf ∈ M(Rd)},

(2)
where L is a spline-admissible operator. The crucial point for our purpose is that ML(Rd) is a
complete normed (or Banach) space when equipped with the proper direct-sum topology. Since
the principle is similar to the characterization of the classical Beppo-Levi spaces2, we shall refer
to ML(Rd) as a generalized Beppo-Levi space.

Theorem 2. Let L be a spline-admissible operator that admits a stable right-inverse L
form speciﬁed by Theorem 1. Then, any f ∈ ML(Rd) has a unique representation as

where w = L{f} ∈ M(Rd) and p = (cid:80)N0

f = L

−1
φ w + p,

n=1(cid:104)f, φn(cid:105)pn ∈ NL with φn ∈ (cid:0)ML(Rd)(cid:1)(cid:48)

ML(Rd) ⊆ L∞,−n0(Rd) and is a Banach space equipped with the norm

(cid:107)f(cid:107)ML,φ = (cid:107)Lf(cid:107)TV + (cid:107)(cid:104)f, φ(cid:105)(cid:107)2.

−1
φ of the

. Moreover,

(3)

The proof of Theorem 2 can be found in Appendix B.
The connection with the L-spline s of Deﬁnition 2 is that s ∈ ML(Zd) if and only if the (cid:96)1-
(cid:80)N
norm of its spline weights a = (a1, . . . , aN ) is ﬁnite. Indeed, we have that (cid:107)Ls(cid:107)TV = (cid:107)wδ(cid:107)TV =
n=1 |an| = (cid:107)a(cid:107)(cid:96)1, owing to the property that (cid:107)δ(· − xn)(cid:107)TV = 1.
We note that the choice of gTV is essential here since the simpler (and a priori only slightly
more restrictive) L1-norm regularization (cid:107)Ls(cid:107)L1 would exclude the spline solutions that are of
interest to us because δ /∈ L1(Rd).

2The classical Beppo-Levi spaces of order n ∈ N and exponent p ≥ 1 are deﬁned as Bp,n(Rd) = {f ∈ S(cid:48)(Rd) :
∂mf ∈ Lp(Rd) for all multi-indices |m| = n} [8, 20]. Hence, in 1D, the proposed deﬁnition of MDn (R) is a slight
extension of B1,n(R). In higher dimensions, it can be shown that Bp,2n(Rd) = {f ∈ S(cid:48)(Rd) : (−∆)nf ∈ Lp(Rd)},
where ∆ is the Laplace operator, so that there also exists a close connection between B1,2n(Rd) and M(−∆)n (Rd).

5

5

gTV-optimality of splines

We shall start with a preparatory result that generalizes an earlier theorem by Fisher and
Jerome [16]. The ﬁrst and most fundamental extension is that we are considering functions
deﬁned over Rd. This is a substantial departure from the compact Hausdorﬀ framework (i.e.,
bounded domain) of Fisher and Jerome. It is essential for covering non-local operators such
as fractional derivatives, and for deploying Fourier-domain/signal-processing techniques. The
second extension is that we are generalizing the extremal problem by replacing simple scalar
intervals by an arbitrary convex set C in measurement space. The third reﬁnement is that we have
simpliﬁed and modernized Fisher and Jerome’s hypotheses concerning the linear independence
and continuity of the linear map F , by replacing them by the two stability bounds (4) and (5),
which have the advantage of being completely explicit.
Let H be the direct sum of M(Rd) and a ﬁnite-dimensional space N equipped with some
norm (cid:107) · (cid:107)N . The generic element of H is f = (w, p) with (cid:107)f(cid:107)H = (cid:107)w(cid:107)TV + (cid:107)p(cid:107)N .
Theorem 3 (Generalized Fisher-Jerome theorem). Let F : H → RM with M ≥ N0 = dim(N )
be a linear map such that

(cid:107)F (w, p)(cid:107) ≤ A ((cid:107)w(cid:107)TV + (cid:107)p(cid:107)N )

B(cid:107)p(cid:107)N ≤(cid:107)F (0, p)(cid:107)

(5)
for some constants A, B > 0 and every (w, p) ∈ H. Let C be a convex compact subset of RM
such that U = F −1(C) = {(w, p) ∈ H : F (w, p) ∈ C} is nonempty (feasibility hypothesis). Then,

(4)

(6)

V = arg min
(w,p)∈U

(cid:107)w(cid:107)TV

N(cid:88)

wδ =

anδ(· − xn)

is a nonempty, convex, weak∗-compact subset of H with extremal points (wδ, p) of the form

with N ≤ M and xn ∈ Rd for n = 1, . . . , N , and min(w,p)∈U (cid:107)w(cid:107)TV =(cid:80)N

n=1

n=1 |an|.

Theorem 3 is the most technical component of our formulation as it involves the weak-*
topology. The details of the proof are laid out in Appendix C together with a precise deﬁnition
of the underlying concepts.

We shall now deduce our primary result on the optimality of splines as a corollary of Theo-
rem 3. Although we are considering a ﬁnite number of measurements, the powerful aspect of the
formulation is that the optimization problem is stated in the continuous domain by searching
for the best inﬁnite-dimensional solution in ML(Rd): the largest class of functions for which the
gTV criterion that we are minimizing is well-deﬁned. Other than that, the technical conditions
in the theorem ensure that the measurements (linear functionals) are well-deﬁned.

Theorem 4 (gTV optimality of splines for linear inverse problems). Let L be a spline-admissible
operator (see Deﬁnition 1) with corresponding generalized Beppo-Levi space

ML(Rd) = {f ∈ S(cid:48)

(Rd) : (cid:107)Lf(cid:107)TV < ∞}

6

whose Banach structure is speciﬁed in Theorem 2. We consider the linear map ν from ML(Rd) →

RM : f (cid:55)→ ν(f ) =(cid:0)(cid:104)ν1, f(cid:105), . . . ,(cid:104)νM , f(cid:105)(cid:1) subject to the boundedness constraints

∀f ∈ ML :
(cid:107)ν(f )(cid:107) ≤ A(cid:107)f(cid:107)ML,φ,
∀p ∈ NL : B(cid:107)p(cid:107)ML,φ ≤ (cid:107)ν(p)(cid:107)

(7)

(8)
for some constants A, B > 0, where the underlying norm (cid:107) · (cid:107)ML,φ is speciﬁed by (3). Then, the
extremal points of the general constrained minimization problem
(cid:107)Lf(cid:107)TV s.t. ν(f ) ∈ C,

β = min

(9)

f∈ML(Rd)

N(cid:88)

N0(cid:88)

where C is any (feasible) convex compact subset of RM , are necessarily nonuniform L-splines of
the form

s(x) =

anρL(x − xn) +

bkpk(x)

(10)

n=1

k=1

k=1 is a basis of NL and L{ρL} = δ so that β = (cid:107)Ls(cid:107)TV =(cid:80)N

n=1 with xn ∈ Rd, a = (a1, . . . , aN ) ∈ RN , and b = (b1, . . . , bN0) ∈
n=1 |an| = (cid:107)a(cid:107)1.

with parameters N ≤ M , {xn}N
RN0. Here, {pk}N0
The full solution set of (9) is the convex hull of those extremal points.
−1
Proof. By Theorem 2, any function f ∈ ML(Rd) has a unique decomposition as f = L
φ w + p
with w = Lf ∈ M(Rd) and p ∈ NL. This allows us to interpret the measurement process
f (cid:55)→ ν(f ) = (cid:104)ν, f(cid:105) as the linear map F : H → RM such that

(cid:104)ν, f(cid:105) = (cid:104)ν, L

−1w(cid:105) + (cid:104)ν, p(cid:105)

= (cid:104)L

−1∗ν, w(cid:105) + (cid:104)ν, p(cid:105) = F (w, p),

while the admissibility condition translates into

(cid:107)ν(f )(cid:107) = (cid:107)F (w, p)(cid:107) ≤ A(cid:107)f(cid:107)ML,φ

B(cid:107)p(cid:107)ML,φ ≤(cid:107)ν(p)(cid:107) = (cid:107)F (0, p)(cid:107).

form (p, wδ) with wδ = (cid:80)N

With this new representation, the constrained minimization problem is equivalent to the one
considered in Theorem 3, which ensures that all extreme points of the solution set are of the
n=1 anδ(· − xn), N ≤ M , and xn ∈ Rd. Upon application of the
−1
φ wδ + p, where p is a suitable component

(stable) right-inverse operator, this maps into s = L
that is in the null space of the operator.

Theorem 4 is a powerful existence result that points towards the universality of nonuniform
L-spline solutions. Although the extremal problem is deﬁned over a continuum, the remarkable
outcome is that the problem admits solutions that are intrinsically sparse, with the level of
sparsity being measured by the minimum number N of required spline knots. In particular, this
explains why the solution of a problem with a TV/L1-type constraint on the derivative (resp.,
the second derivative) is piecewise-constant (resp., piecewise linear when L = D2). The other
pleasing aspect is the direct connection between the functional concept of generalized TV and
the (cid:96)1-norm of the expansion coeﬃcients a.

We observe that the solution is made up of two components: an adaptive one that is speciﬁed
by (xn) and a, and a linear regression term (with expansion coeﬃcients b) that describes

7

the component in the null space of the operator. Since b does not contribute to (cid:107)Ls(cid:107)TV,
the optimization tends to maximize the contribution of the null-space component. The main
diﬃculty in ﬁnding the optimal solution is that N and (xn) are problem-dependent and unknown
a priori.

The statement in Theorem 4 is remarkably general.

In particular, it covers the generic

regularized least-squares problem

fλ = arg min

f∈ML(Rd)

(cid:32) M(cid:88)

m=1

(cid:33)

|ym − (cid:104)νm, f(cid:105)|2 + λ(cid:107)Lf(cid:107)TV

(11)

that is commonly used to formulate linear inverse/compressed-sensing problems [2, 10, 3, 15, 13].
The connection is obtained by taking C = C(y) = {z ∈ RM : (cid:107)y − z(cid:107)2 ≤ 2} = B(y; ), which
is a ball of diameter  centered on the measurement vector y = (y1, . . . , yM ). Indeed, since the
problem is convex, the optimal solution s of (9) saturates the inequality such that (cid:107)y−ν(s)(cid:107)2 =
2 and has minimum gTV α = α() = (cid:107)Ls(cid:107)TV. In the Lagrange/LASSO form (11), the selection
of a ﬁxed λ ∈ R+ results in a particular value of the data error (cid:107)y − ν(fλ)(cid:107)2 = (cid:48)(λ) with the
optimal solution fλ = s(cid:48) having the same total variation as if we were looking at the primary
problem (9) with C = B(y; (cid:48)).
To get further insight on the optimization problem (11), we can look at two limit cases.
When λ → ∞, the solution must be of the form f∞ = p ∈ NL so that (cid:107)Lf∞(cid:107)TV = 0.
It
then follows that (cid:107)y − ν(f∞)(cid:107)2 ≤ (cid:107)y(cid:107)2 < ∞. On the contrary, when λ → 0, the minimization
will force the data term (cid:107)y − ν(f0)(cid:107)2 to vanish. Theorem 4 then ensures the existence of a
nonuniform “interpolating” L spline f0(x) with ν(f0) = y and minimum gTV norm.

6 Application areas

Before listing speciﬁc examples, let us brieﬂy comment on the admissibility conditions (7) and
(8) and show that the practical constraints are light. The ﬁrst requirement is that the set
of analysis functions (νm)M
m=1 be suﬃciently discriminative to fully determine the null-space
component of the solution. Mathematically, this null-space completeness property is expressed
by

B(cid:107)p(cid:107)ML,φ ≤ (cid:107)ν(p)(cid:107) ≤ A(cid:107)p(cid:107)ML,φ,∀p ∈ NL.

It is very similar to the inequality that deﬁnes a frame [1]. In particular, it implies that M ≥ N0.
To be on the safe side for the upper bound, we can impose that νm ∈ L1,n0(Rd), where

L1,n0(Rd) = {ϕ : Rd → C s.t.

|ϕ(x)|(1 + (cid:107)x(cid:107))n0dx < ∞}.

(cid:90)

Rd

This is a mild algebraic decay requirement that is met by the impulse responses of most physical
devices. Since L1,n0(Rd) ⊂ (ML(Rd))(cid:48) (as a consequence of the inclusion ML(Rd) ⊂ L∞,−n0(Rd),
see Theorem 2), this condition guarantees that the upper bound in (7) is satisﬁed for all f ∈
ML(Rd).
We shall now discuss examples of signal recovery that are covered by Theorem 4. The
standard setting is that one is given a set of noisy measurements y = (cid:104)ν, s(cid:105) + “noise” of an
unknown signal s and that one is trying to recover s from y based on the solution of (11),
or some variant of the problem involving some other (convex) data term—the most favorable
choice being the log likelihood of the measurement noise.

8

6.1 Ideal sampling

The task here is to reconstruct a continuous-domain signal from its (possibly, noisy) nonuni-
form samples {s(xm)}M
m=1, which is achieved by searching for the function s(x) that ﬁts the
samples while minimizing (cid:107)L{s}(cid:107)TV. This corresponds to the problem setting in Theorem 4
with νm = δ(· − xm) and C = B(y; ), where y denotes the measurement vector. Hence, the
admissibility condition (7) is equivalent to L−1∗{δ(· − xm)} = g(·, xm) ∈ C0(Rd), where the
boundedness is ensured by the stability condition in Theorem 1. The more technical continuity
requirement is achieved when ρL is continuous (H¨older exponent r0 > 0). This happens when the
order of the diﬀerential operator is greater than one, which seems to exclude3 simple operators
such as D (piecewise-constant approximation). This limitation notwithstanding, our theoretical
results are directly applicable to the problems of adaptive regression splines [23] with L = DN ,
the construction of shape-preserving splines [21], as well as a whole range of variations including
TV denoising.

6.2 Generalized sampling

The setting is analogous to the previous one, except that the samples are now observed through
a sampling aperture φ ∈ L1,n0(Rd) so that νm = φ(· − xm) [32, 14]. The function φ may, for
example, correspond to the point-spread function of a microscope. Then, the recovery problem
is equivalent to a deconvolution [9]. Since the measurements are obtained by integration of s
against an ordinary function νm ∈ L1,n0(Rd), there is no requirement for the continuity of ρL
because of the implicit smoothing eﬀect of φ. This means that essentially no restrictions apply.

6.3 Compressed sensing

The result of Theorem 4 is highly relevant to compressed sensing too, especially since the
underlying L1 signal-recovery problem is formulated in the continuous domain. We like to
view (10) as the prototypical form of a piecewise-smooth signal that is intrinsically sparse with
sparsity N = (cid:107)a(cid:107)0. The model also conforms with the notion of ﬁnite rate of innovation [37].
If we know that the unknown signal s has such a form, then Theorem 4 suggests that we
can attempt to recover it from a M -dimensional linear measurement y = (cid:104)ν, s(cid:105) by solving the
optimization problem (9) with C = B(y; ), which is agreement with the predominant paradigm
in the ﬁeld. While the theorem states that M ≥ N , common sense dictates that we should
take M > Nfreedom, where Nfreedom = 2N + N0 is the number of degrees of freedom of the
underlying model. The diﬃculty, of course, is that a subset of those parameters (the spline
knots xk) induce a model dependency that is highly nonlinear. By building on the analogy with
the discrete theory of compressed sensing, this also raises the fundamental theoretical question:
−1∗
Is it possible to provide conditions on ν or, more generically, on L
φ ν, such that such a recovery
is possible? This is an open topic that calls for further investigation.
The other interesting aspect is that the reconstruction problem is formulated in analysis
form (i.e., minimization of (cid:107)Ls(cid:107)TV), while the generic solution is given in synthesis form, with
the unusual twist that the underlying dictionary {ρL(· − τ )}τ∈Rd of basis functions is inﬁnite-
dimensional and not even countable. Yet, Theorem 4 also suggest a potential discretization
approach which is to select a ﬁnite subset of functions {ρL(·− τ k)}K
k=1, typically equally spaced,
with K (cid:29) M and to rely on linear programming for  = 0, or quadratic programming for 2 > 0,
or some other convex optimization technique to numerically solve the underlying (cid:96)1-minimization
problem.

3We can bypass this somewhat artiﬁcial limitation by replacing the ideal sampler by a quasi-ideal sampling

device that involves a molliﬁed version of a Dirac impulse.

9

A Proof of Theorem 1
−1
φ = G. Schwartz’ kernel theorem states that any continuous

To simplify the notation, we set L
linear operator G : S(Rd) → S(cid:48)(Rd) admits an “integral” representation as

G{ϕ}(x) =

g(x, y)ϕ(y)dy

(12)

(cid:90)

Rd

In essence, g(·,·) is the
for all ϕ ∈ S(Rd) with g(x, y) = G{δ(· − y)}(x) ∈ S(cid:48)(Rd × Rd).
continuous-domain analog of the matrix that speciﬁes a ﬁnite-dimensional linear operator. We
then invoke Theorem 5 to show that G admits a continuous extension M(Rd) → L∞,−n0(Rd) if
and only if supx,y∈Rd |g(x, y)| (1 + (cid:107)x(cid:107))−n0 < ∞, which is equivalent to the last statement in
Theorem 1.
The right-inverse property is obvious from the explicit representation of G since L{ρL} = δ
and L{pm} = 0 for m = 1,··· , N0. Next, we invoke the biorthogonality property (cid:104)φm, pn(cid:105) =
δm−n to evaluate the inner product of (12) with φm as

(cid:104)φm, G{ϕ}(cid:105) = (cid:104)φm, ρL ∗ ϕ(cid:105) − N0(cid:88)

(cid:104)φm, pn(cid:105)(cid:104)qn, ϕ(cid:105)

n=1

= (cid:104)φm, ρL ∗ ϕ(cid:105) − (cid:104)qm, ϕ(cid:105)
= (cid:104)φm, ρL ∗ ϕ(cid:105) − (cid:104)φm, ρL ∗ ϕ(cid:105) = 0,

which shows that the boundary conditions are satisﬁed.

Theorem 5. The generic linear operator G : S(Rd) → S(cid:48)(Rd) : ϕ (cid:55)→ f (x) =(cid:82)

Rd g(x, y)ϕ(y)dy
admits a continuous extension G : M(Rd) → L∞,−n0(Rd) with n0 ∈ Z if and only if g(·,·) is
measurable and

(cid:3)

|g(x, y)| (1 + (cid:107)x(cid:107))

−n0 < ∞.

sup
x,y∈Rd

(13)

Proof. Since M(Rd) ⊃ S(Rd) and L∞,−n0(Rd) ⊂ S(cid:48)(Rd) are two Banach spaces, we only

need to show that the induced operator norm

(cid:107)G(cid:107) = (cid:107)G(cid:107)M(Rd)→L∞,−n0 (Rd) = sup
w(cid:54)=0

(cid:107)G{w}(cid:107)∞,−n0

(cid:107)w(cid:107)TV

(cid:107)G{w}(cid:107)∞,−n0

= sup

(cid:107)w(cid:107)TV≤1

suﬃciency of (13) by considering the signal f (x) = G{w}(x) = (cid:82)

is ﬁnite (by the Hahn-Banach theorem). Having set the context, we can now establish the
Rd g(x, y)dw(y), where w ∈
M(Rd) and by constructing the estimate

(cid:12)(cid:12)(cid:12)(cid:12)(cid:90)

(cid:12)(cid:12)(cid:12)(cid:12)

|f (x)|(1 + (cid:107)x(cid:107))

−n0 = (1 + (cid:107)x(cid:107))
≤ (1 + (cid:107)x(cid:107))

−n0
Rd
−n0 sup
y∈Rd

g(x, y)dw(y)
|g(x, y)| (cid:107)w(cid:107)TV,

which implies that

(cid:107)f(cid:107)∞,−n0 = (cid:107)G{w}(cid:107)∞,−n0 ≤

(cid:32)

(cid:33)

(cid:107)w(cid:107)TV,

|g(x, y)| (1 + (cid:107)x(cid:107))

−n0

sup
x∈Rd

sup
y∈Rd

10

for all w ∈ M(Rd). In doing so, we have shown that

(cid:107)G(cid:107) ≤ sup
x,y∈Rd

|g(x, y)| (1 + (cid:107)x(cid:107))

−n0 < ∞.

To prove necessity, we use the property that g(x, y) = G{δ(· − y)}(x), where the shifted Dirac
impulse δ(· − y) is included in M(Rd) with (cid:107)δ(· − y)(cid:107)TV = 1. We then observe that, for each
y ∈ Rd,

(cid:107)G{δ(· − y)}(cid:107)∞,−n0 = sup
x∈Rd

(1 + (cid:107)x(cid:107))

−n0|g(x, y)|.

Moreover, G being bounded, we have that

(cid:107)G{δ(· − y)}(cid:107)∞,−n0 ≤ (cid:107)δ(· − y)(cid:107)TV(cid:107)G(cid:107) = (cid:107)G(cid:107),

which means that

(1 + (cid:107)x(cid:107))

−n0|g(x, y)| ≤ (cid:107)G(cid:107) < ∞.

sup
x∈Rd

As we already know that the inequality holds in the other direction as well, so that

(cid:107)G(cid:107) = sup
x,y∈Rd

|g(x, y)| (1 + (cid:107)x(cid:107))

−n0,

which concludes the proof.

(cid:3)

B Proof of Theorem 2
Let f, h ∈ ML(Rd) be such that Lf = Lh = w. By deﬁnition of the null space, this is equivalent
−1
to (f − h) = p ∈ NL. Now, let h = L
φ satisﬁes the stability condition in Theorem
−1
φ : M(Rd) → L∞,−n0(Rd) is bounded (by Theorem 5), we have that (cid:107)h(cid:107)∞,−n0 ≤
1. Since L
C(cid:107)w(cid:107)TV, which implies that h ∈ S(cid:48)(Rd), while the condition (cid:107)Lh(cid:107)TV = (cid:107)w(cid:107)TV < ∞ ensures
that h ∈ ML(Rd). This allows us to deduce that ML(Rd) is the sum of NL = span{pn}N0
n=1 and

−1
φ w, where L

ML,0(Rd) = {f = L

−1
φ w : w ∈ M(Rd)}

= {f ∈ ML(Rd) such that (cid:104)f, φn(cid:105) = 0, n = 1, . . . , N0}.

ML,0(Rd) is a Banach space equipped with the norm (cid:107)L{·}(cid:107)TV. Its completeness simply follows
−1
from the Banach-space property of M(Rd) and the fact that the inverse operator L
φ performs an
−1
isometric mapping M(Rd) → ML,0(Rd). Moreover, since (cid:104)φ, L
φ w(cid:105) = 0 (boundary conditions)
and the φm are biorthogonal to the pn, we ﬁnd that the null-space component p is given by

N0(cid:88)

p =

pn(cid:104)f, φn(cid:105) = pT(cid:104)φ, f(cid:105).

n=1

It is therefore speciﬁed by its expansion coeﬃcients (cid:104)φ, f(cid:105) = ((cid:104)φ1, f(cid:105), . . . ,(cid:104)φN0, f(cid:105)) whose (cid:96)2-
−1
norm is (cid:107)(cid:104)φ, f(cid:105)(cid:107)2. This shows that the decomposition f = L
φ w+p, where w = Lf ∈ M(Rd) and
p ∈ NL, is unique, which also translates into ML(Rd) = ML,0(Rd)⊕NL because ML,0(Rd)∩NL =
{0}. Since ML,0(Rd) and NL are two Banach spaces, we can equip their direct sum ML(Rd)
with the composite norm (cid:107)f(cid:107)ML,φ = (cid:107)w(cid:107)TV + (cid:107)(cid:104)φ, f(cid:105)(cid:107)2 with the guarantee that the Banach-
space property is preserved. Finally, Theorem 5 implies that ML,0(Rd) ⊂ L∞,−n0(Rd), which
allows us to conclude that ML(Rd) ⊂ L∞,−n0(Rd).
(cid:3)

11

C Proof of theorem 3

The proof follows the same steps as the original one of Fisher and Jerome [16, Theorem 1]. Yet,
it diﬀers in the assumptions and technicalities (e.g., consideration of the non-compact domain
Rd and use of bounds). We have done our best to make it self-contained.

As preparation, we recall that the weak*-topology on M(Rd) = (cid:0)C0(Rd)(cid:1)(cid:48)
convex topology associated with the family of semi-norms pf (w) = |(cid:82)
and only if(cid:82)

is the locally
Rd f dw| for f ∈ C0(Rd).
In particular, a sequence of elements wn ∈ M(Rd) converges to 0 for the weak*-topology if
Rd f dwn → 0 for every f ∈ C0(Rd). A subset of M(Rd) is said to be weak*-close
(weak*-compact, respectively) if it is closed (compact, respectively) for the weak*-topology. We
shall use the following results, which are consequences of the Banach-Alaoglu theorem and its
variations [26, p.68].
Proposition 1. Compactness in the weak*-topology of M(Rd).

• For every α > 0, the set Bα = {w ∈ M(Rd), (cid:107)w(cid:107)TV ≤ α} is weak*-compact in M(Rd).
• If (wn) is a sequence on M(Rd), bounded for the TV-norm, then we can extract a subse-

quence that converges in M(Rd) for the weak*-topology.

These properties also carry over to the Banach space H = M(Rd) ⊕ N =(cid:0)C0(Rd) ⊕ N (cid:48)(cid:1)(cid:48)

,
which is endowed with the corresponding weak*-topology: A sequence (wn, pn) in H vanishes for
the weak*-topology if and only if wn vanishes for the weak*-topology of M(Rd) and (cid:107)pn(cid:107)N → 0.

Proposition 2. Compactness in the weak*-topology of H.

• For every α1, α2 > 0, the set Bα1,α2 = {(w, p) ∈ H, (cid:107)w(cid:107)TV ≤ α1, (cid:107)p(cid:107)N ≤ α2} is weak*-

compact in H.

• If (wn, pn) is a sequence in H such that (cid:107)wn(cid:107)TV + (cid:107)pn(cid:107)N is bounded, then we can extract

a subsequence that converges in H for the weak*-topology.

The proof is divided in two parts. First, we show that V is a nonempty, convex, and weak*-
compact subspace of H. This allows us to specify V by means of its extremal points via the
Krein-Milman theorem. Second, we show that the extremal points have the announced form.
We set β = inf (w,p)∈U(cid:107)w(cid:107)TV.
Part I: V is nonempty, convex, and weak*-compact Let us consider a sequence (wn, pn)n∈N
in U such that (cid:107)wn(cid:107)TV decreases to β. In particular, (cid:107)wn(cid:107)TV is bounded by (cid:107)w0(cid:107)TV. We set
M = maxx∈C(cid:107)x(cid:107). Using respectively (8), (7), and (cid:107)F (wn, pn)(cid:107) ≤ M (since (wn, pn) ∈ U), we
deduce the inequalities

(cid:107)F (wn, pn) − F (wn, 0)(cid:107)

(cid:107)pn(cid:107)N ≤ 1
B
≤ 1
B
≤ 1
B

1
B

(cid:107)F (0, pn)(cid:107) =
((cid:107)F (wn, pn)(cid:107) + (cid:107)F (wn, 0)(cid:107))
(M + A(cid:107)wn(cid:107)TV) ≤ 1
B

(M + A(cid:107)w0(cid:107)TV),

(14)

which shows that pn is bounded. We can then extract a sequence (wϕn, pϕn) from (wn, pn) that
converges to (w∞, p∞) for the weak*-topology (by Proposition 2). Moreover, for any ﬁxed  > 0,

12

we can pick n large enough so that (cid:107)wn(cid:107) ≤ β +  which implies that (cid:107)w∞(cid:107)TV ≤ β + . Since
this works for every , (cid:107)w∞(cid:107)TV ≤ β.
On the other hand, F (w∞, p∞) = lim F (wϕn, pϕn) by the continuity of F . Since the set C is
closed and F (wn, pn) ∈ C for every n, we deduce that (w∞, p∞) ∈ C, so that (cid:107)w∞(cid:107)TV ≥ β. In
light of the previous inequality, this yields (cid:107)w∞(cid:107)TV = β, which proves that V is not empty.

by a weak*-continuous function F , it is also weak*-closed. Now, V = U(cid:84){(w, p), (cid:107)w(cid:107)TV ≤ β} is

Since C is convex and F linear, the set U = F −1(C) is convex. As the preimage of a closed set

convex and weak*-closed since it is the intersection of two convex and weak*-closed sets. Finally,
for (w, p) ∈ V, we show that (cid:107)p(cid:107)N ≤ M +A(cid:107)w(cid:107)TV
B = γ based on the same inequalities as
in (14). Therefore, we have

= M +Aβ

B

V ⊂ {(w, p) ∈ H, (cid:107)w(cid:107)TV ≤ β, (cid:107)p(cid:107)N ≤ γ},

where the set on the right-hand side is weak*-compact, due to Proposition 2. Since any closed
set included in a compact set is necessarily compact, this shows that V is weak*-compact.

We are now in the position to apply the Krein-Milman theorem (see [26, p.75]) to the convex
weak*-compact set V ⊂ H, which tells us that “V is the closed convex hull of its extreme points
in H endowed with the weak*-topology”. This leads us to ﬁnal part of the proof, which is the
characterization of those extreme points.
Part II: The extreme points of V are of the form (6) We shall prove that a necessary
condition for (w, p) to be an extreme point of V is that there are no disjoint Borelian sets
E1, . . . , EM +1 ⊂ Rd such that w(Em) (cid:54)= 0 for m = 1, . . . , M + 1. The only elements of M(Rd)
satisfying this condition are precisely those described by (6).

We denote wm = w1Em, E = Rd\(cid:83)
dependent, there exists (cm)1≤m≤M +1 (cid:54)= 0 such that(cid:80)M +1
Let µ = (cid:80)M +1

We shall proceed by contradiction and assume that there exist disjoint sets E1, . . . , EM +1
such that w(Em) (cid:54)= 0 for all m.
m Em, and ¯w = w1E with (cid:107)w(cid:107)TV = β. For m =
1, . . . , M + 1, we set ym = F (wm, p). Since any collection of (M + 1) vectors in RM is linearly
m=1 cmwm ∈ M(Rd) and  ∈ (−max, max) with max = 1/ maxm |cm|, so that
1 + cm > 0 and 1 − cm > 0 for all m. The choice of the cm implies that Fm(w + µ, p) =
Fm(w, p) = Fm(w − µ, p) for all m. Hence,

m=1 cmym = 0.

while w + µ = ¯w +(cid:80)M +1

(w + µ, p), (w − µ, p) ∈ U,

m=1 (1 + cm)wm. The hypothesis that the measures ¯w, w1, . . . , wM +1

13

have disjoint supports then gives

(cid:107)w + µ(cid:107)TV = (cid:107) ¯w(cid:107)TV +

= (cid:107) ¯w(cid:107)TV +

m=1

M +1(cid:88)
M +1(cid:88)
M +1(cid:88)

m=1

(1 + cm)(cid:107)wm(cid:107)TV

M +1(cid:88)

(cid:107)wm(cid:107)TV + 

cm(cid:107)wm(cid:107)TV

m=1

cm(cid:107)wm(cid:107)TV

= (cid:107)w(cid:107)TV + 

M +1(cid:88)
Likewise, we get that (cid:107)w − µ(cid:107)TV = β − (cid:80)M +1

= β + 

m=1

m=1

cm(cid:107)wm(cid:107)TV.

m=1 cm(cid:107)wm(cid:107)TV. If(cid:80)M +1

m=1 cm(cid:107)wm(cid:107)TV (cid:54)= 0, then we
either have (cid:107)w + µ(cid:107)TV < β or (cid:107)w − µ(cid:107)TV < β, which is impossible since the minimum over U
is β. Hence,

(15)

M +1(cid:88)

cm(cid:107)wm(cid:107)TV = 0,

and (cid:107)w + µ(cid:107)TV = (cid:107)w − µ(cid:107)TV = β, which translates into (w + µ, p) and (w − µ, p) being
included in V. This, in turn, implies that (w, p) = 1
2 (w − µ, p) is not an extreme
point of V.
(cid:3)

2 (w + µ, p) + 1

m=1

Acknowledgments

The research was partially supported by the Swiss National Science Foundation under Grant
200020-162343 and the Center for Biomedical Imaging (CIBM) of the Geneva-Lausanne Univer-
sities and EPFL.

The authors are thankful to H. Gupta for helpful discussions.

References

[1] A. Aldroubi. Portraits of frames. Proceedings of the American Mathematical Society,

123(6):1661–1668, 1995.

[2] A. M. Bruckstein, D. L. Donoho, and M. Elad. From sparse solutions of systems of equations

to sparse modeling of signals and images. SIAM Review, 51(1):34–81, 2009.

[3] E. J. Cand`es and J. Romberg. Sparsity and incoherence in compressive sampling. Inverse

Problems, 23(3):969–985, 2007.

[4] Emmanuel J Cand`es. The restricted isometry property and its implications for compressed

sensing. C. R. Acad. Sci., 346(9):589–592, 2008.

[5] Emmanuel J Cand`es, Justin K Romberg, and Terence Tao. Stable signal recovery from
incomplete and inaccurate measurements. Communications on Pure and Applied Mathe-
matics, 59(8):1207–1223, 2006.

14

[6] W. Dahmen and C.A. Micchelli. On theory and application of exponential splines. In C.K.
Chui, L.L. Shumaker, and F.I. Utreras, editors, Topics in Multivariate Approximation,
pages 37–46. Academic Press, New York, 1987.

[7] I. Daubechies, M. Defrise, and C. De Mol. An iterative thresholding algorithm for lin-
ear inverse problems with a sparsity constraint. Communications on Pure and Applied
Mathematics, 57(11):1413–1457, 2004.

[8] Jacques Deny and Jacques-Louis Lions. Les espaces du type de Beppo Levi. Annales de

l’institut Fourier, 5:305–370, 1954.

[9] N. Dey, L. Blanc-F´eraud, C. Zimmer, P. Roux, Z. Kam, J.-C. Olivo-Marin, and J. Zerubia.
Richardson-Lucy algorithm with total variation regularization for 3D confocal microscope
deconvolution. Microscopy Research and Technique, 69(4):260–266, 2006.

[10] D. L. Donoho. Compressed sensing. IEEE Transactions on Information Theory, 52(4):1289–

1306, 2006.

[11] David L Donoho and Michael Elad. Optimally sparse representation in general (nonorthog-
onal) dictionaries via (cid:96)1 minimization. Proceedings of the National Academy of Sciences,
100(5):2197–2202, 2003.

[12] J. Duchon.

Splines minimizing rotation-invariant semi-norms in Sobolev spaces.

In
W. Schempp and K. Zeller, editors, Constructive Theory of Functions of Several Variables,
pages 85–100. Springer-Verlag, Berlin, 1977.

[13] Michael Elad. Sparse and Redundant Representations. From Theory to Applications in

Signal and Image Processing. Springer, 2010.

[14] Yonina Eldar. Sampling Theory: Beyond Bandlimited Systems. Cambridge University

Press, 2015.

[15] M. A. T. Figueiredo and R. D. Nowak. An EM algorithm for wavelet-based image restora-

tion. IEEE Transactions on Image Processing, 12(8):906–916, 2003.

[16] S.D. Fisher and J.W. Jerome. Spline solutions to L1 extremal problems in one and several

variables. Journal of Approximation Theory, 13(1):73–83, 1975.

[17] I. M. Gelfand and G. Shilov. Generalized Functions. Vol. 1. Properties and Operations.

Academic press, New York, USA, 1964.

[18] Tom Goldstein and Stanley Osher. The split Bregman method for L1-regularized problems.

SIAM Journal on Imaging Sciences, 2(2):323–343, 2009.

[19] Lars H¨ormander. The analysis of linear partial diﬀerential operators.I. Distribution Theory

and Fourier Analysis, volume 256. Springer-Verlag, Berlin, 2nd edition, 1990.

[20] Takahide Kurokawa. Riesz potentials, higher Riesz transforms and Beppo Levi spaces.

Hiroshima Math. J, 18(3):541–597, 1988.

[21] John E Lavery. Shape-preserving, multiscale ﬁtting of univariate data by cubic L1 smooth-

ing splines. Computer Aided Geometric Design, 17(7):715–727, 2000.

15

[22] M. Lustig, D. L. Donoho, and J. M. Pauly. Sparse MRI: The application of compressed

sensing for rapid MR imaging. Magnetic Resonance in Medicine, 58(6):1182–1195, 2007.

[23] E. Mammen and S. van de Geer. Locally adaptive regression splines. Annals of Statistics,

25(1):387–413, 1997.

[24] Holger Rauhut, Karin Schnass, and Pierre Vandergheynst. Compressed sensing and redun-

dant dictionaries. IEEE Transactions on Information Theory, 54(5):2210–2219, 2008.

[25] L. I. Rudin, S. Osher, and E. Fatemi. Nonlinear total variation based noise removal algo-

rithms. Physica D, 60(1-4):259–268, 1992.

[26] W. Rudin. Functional Analysis. International Series in Pure and Applied Mathematics,

McGraw-Hill, Inc., New York, 1991.

[27] Walter Rudin. Real and Complex Analysis. McGraw-Hill, New York, 3rd edition, 1987.

[28] M. H. Schultz and R. S. Varga. L-splines. Numerische Mathematik, 10(4):345–369, 1967.

[29] L. L. Schumaker. Spline functions: Basic theory. Cambridge University Pressity Press,

Cambridge, 3rd edition edition, 2007.

[30] Gabriele Steidl, Stephan Didas, and Julia Neumann. Splines in higher order tv regulariza-

tion. International journal of computer vision, 70(3):241–255, 2006.

[31] R. Tibshirani. Regression shrinkage and selection via the Lasso. Journal of the Royal

Statistical Society. Series B, 58(1):265–288, 1996.

[32] M. Unser. Sampling—50 years after Shannon. Proceedings of the IEEE, 88(4):569–587,

April 2000.

[33] M. Unser and T. Blu. Fractional splines and wavelets. SIAM Review, 42(1):43–67, March

2000.

[34] M. Unser and T. Blu. Cardinal exponential splines: Part I—Theory and ﬁltering algorithms.

IEEE Transactions on Signal Processing, 53(4):1425–1449, April 2005.

[35] M. Unser and T. Blu. Self-similarity: Part I—Splines and operators. IEEE Transactions

on Signal Processing, 55(4):1352–1363, April 2007.

[36] M. Unser and P. D. Tafti. An Introduction to Sparse Stochastic Processes. Cambridge

University Press, 2014.

[37] M. Vetterli, P. Marziliano, and T. Blu. Sampling signals with ﬁnite rate of innovation.

IEEE Transactions on Signal Processing, 50(6):1417–1428, June 2002.

16

