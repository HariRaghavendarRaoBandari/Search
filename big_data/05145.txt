6
1
0
2

 
r
a

 

M
6
1

 
 
]

V
C
.
s
c
[
 
 

1
v
5
4
1
5
0

.

3
0
6
1
:
v
i
X
r
a

Suppressing the Unusual: towards Robust CNNs using Symmetric Activation

Functions

Qiyang Zhao
Beihang University, 37 Xueyuan Rd., Beijing 100191, China
Lewis D Grifﬁn
University College London, Gower St., London, WC1E 6BT, UK

ZHAOQY@BUAA.EDU.CN

L.GRIFFIN@CS.UCL.AC.UK

Abstract

Many deep Convolutional Neural Networks
(CNN) make incorrect predictions on adversar-
ial samples obtained by imperceptible perturba-
tions of clean samples. We hypothesize that
this is caused by a failure to suppress unusual
signals within network layers. As remedy we
propose the use of Symmetric Activation Func-
tions (SAF) in non-linear signal transducer units.
These units suppress signals of exceptional mag-
nitude. We prove that SAF networks can per-
form classiﬁcation tasks to arbitrary precision in
a simpliﬁed situation. In practice, rather than use
SAFs alone, we add them into CNNs to improve
their robustness. The modiﬁed CNNs can be eas-
ily trained using popular strategies with the mod-
erate training load. Our experiments on MNIST
and CIFAR-10 show that the modiﬁed CNNs per-
form similarly to plain ones on clean samples,
and are remarkably more robust against adver-
sarial and nonsense samples.

2014). In (Goodfellow et al., 2015), a simpler method, fast
gradient sign (FGS), is proposed to compute adversarial
samples effectively. Recently, a more complicated method
is proposed to construct an adversarial sample with both the
prediction and internal CNN features being similar to an-
other arbitrary sample (Sabour et al., 2016). Nonsense sam-
ples (called rubbish samples in (Goodfellow et al., 2015))
are another challenge to the robustness of CNNs. These are
generated from noise or arbitrary images using FGS-like
methods (Goodfellow et al., 2015)(Nguyen et al., 2015).
Nonsense samples are totally unrecognizable but popular
CNNs typically predict with high conﬁdence that they be-
long to some category. All these references argue that ad-
versarial and nonsense samples arise from some unknown
ﬂaw of popular deep CNNs, either in their structure or
training methods (Szegedy et al., 2014)(Goodfellow et al.,
2015)(Sabour et al., 2016).

1. Introduction
Although deep CNNs have delivered state-of-the-art per-
formance on several major computer vision challenges
(Krizhevsky et al., 2012)(He et al., 2015), they have some
pooly understood aspects.
In particular, it was shown in
(Szegedy et al., 2014) that one can easily construct an
adversarial sample by imperceptible perturbation of any
clean sample using the box-constrained limited-memory
BFGS method (Andrew & Gao, 2007). These samples are
perceptually similar to the original ones, but are predicted
to be of different categories, and even generalize well
across different CNN models and train sets (Szegedy et al.,

Copyright 2016 by the author(s).

(a)

(c)

(b)

(d)

Figure 1. Symmetric activation functions and their derivatives .
(a)(b) 1-D RBF and its derivative; (c)(d) mReLU and its deriva-
tive. Please see Sec. 3 for details.

A proposed solution to the problem is adversarial train-
ing (Szegedy et al., 2014)(Goodfellow et al., 2015): ad-

−3−2−10123−0.500.511.5xe−x2−3−2−10123−1.5−1−0.500.511.5xddxe−x2−3−2−10123−0.500.511.5xmReLU(x)−3−2−10123−1.5−1−0.500.511.5xddxmReLU(x)Towards Robust CNNs Using SAFs

In this paper we propose a solution to the adversary prob-
lem that does not increase much training load, nor increase
the complexity of the network. The solution is to use sym-
metric activation function (SAF) units. These are non-
linear transduction units that suppress exceptional signals.
We incorporate SAFs into CNNs by addition or replace-
ment of layers of standard models. The resulting CNNs are
easily trained using standard approaches and give low error
rates on clean and perturbed samples.

2. Motivation
Unlike CNNs and linear classiﬁcation models, local mod-
els (Moody & Darken, 1989)(Jacobs et al., 1991) based on
radial basis functions (RBF) (Broomhead & Lowe, 1988)
are intrinsically immune to abnormal samples which are
far from high-density regions: they will always output low
conﬁdence scores for such. Furthermore, the RBFs are ro-
bust against tiny perturbations of the input. This suggests
the possibility of improving the robustness of CNNs by us-
ing RBF units.
RBFs have been used in several ways within networks, e.g.
as a hidden layer in mixture of experts (Jacobs et al., 1991),
and as the top layer units in LeNet-5 (LeCun et al., 1998).
Only a few implementations attempt multiple RBF layers
(Craddock & Warwick, 1996)(Bohte et al., 2002)(Mr´azov´a
& Kukaˇcka, 1996). The reason for avoiding multiple lay-
ers is the high-dimensionality of the parameter space of
each RBF unit, together with the need for many RBF units.
This high-dimensionality makes it hard to train deep net-
works involving RBF units to achieve acceptable accura-
cies (Goodfellow et al., 2015). Some have argued that
more powerful optimization methods are needed to make
this work (Goodfellow et al., 2015).
We have considered whether it is feasible to use 1-D RBFs
instead of high-dimension ones. To assess this, we run a
LeNet-5 model (LeCun et al., 1998) (see Sec. 5.1 for its
details) on the MNIST test set, and show the histograms
and joint distributions of features from the second convolu-
tional layer in Fig. 3. The empirical distributions of these
features are approximately Gaussian. In our experiments,
we ﬁnd it is effective to distinguish abnormal samples from
normal ones by exploiting this observation. In the second
row of Fig. 3, we show the feature pair values of nonsense
samples of pure Gaussian noise in red squares. Although
the LeNet-5 model incorrectly assigns nonsense samples
to meaningful categories with high conﬁdence (∼ 99%),
as the ﬁgure shows, their features usually deviate from the
high-density regions of clean samples. The higher the layer
the more distinct the deviation.
We also observe that features are not strongly correlated,
as shown in Fig. 4. Therefore it is viable to use 1-D

(a)

(b)

(c)

Figure 2. Building robust CNNs using SAF units. (a) the basic
block of a SAF unit. (b) replacing ReLU/sigmoid layers with SAF
layers. (c) inserting SAF layers into a CNN.

versarial samples are constructed according to the current
model, then utilized in subsequent training epochs. CNNs
so trained will be vulnerable to new adversarial samples;
but repeated rounds of adversarial training eventually lead
to improved robustness, at the price of an increased train-
ing load. Unfortunately, the ﬁnal error rates on the newest
adversarial samples, after this scheme, remain large (Good-
fellow et al., 2015). It is interesting that adversarial train-
ing can be understood as a new regularization method for
training CNNs; for example in (Miyato et al., 2016), vir-
tual adversarial samples are constructed to force the model
distributions of a supervised learning process to be smooth.
Adversarial samples can also be used to establish new un-
supervised and/or semi-supervised representation learning
methods (Radford et al., 2016) (Springenberg, 2016) or au-
toencoders (Makhzani et al., 2015).
A different approach to the adversary problem is to mod-
ify the structure of the CNN. Autoencoders can be added
to denoise input samples and improve the local stability of
feature spaces in individual layers (Gu & Rigazio, 2015),
where the contractive penalty terms are designed to pun-
ish unexpected feature changes caused by small perturba-
tions. The idea is similar to (Miyato et al., 2016). However
this method cannot achieve good robustness on moderately
large perturbations while keeping high accuracies on clean
samples (Gu & Rigazio, 2015). Another interesting idea,
proposed in (Chalupka et al., 2015), is to identify features
that are causally related, rather than merely correlated, with
the categories. It is thus possible to predict the semantic
categories according to causal features, so to improve the
robustness. However it would be a large burden to integrate
this idea with popular CNN models.

SAFConvolutionalx(cid:20)(cid:3)(cid:3)x(cid:21)(cid:3)(cid:3)(cid:3)(cid:3)(cid:171)(cid:171)(cid:3)(cid:3)(cid:3)(cid:3)xnyzConvolutionalx(cid:20)(cid:3)(cid:3)x(cid:21)(cid:3)(cid:3)(cid:3)(cid:3)(cid:171)(cid:171)(cid:3)(cid:3)(cid:3)(cid:3)xnyReLU/SigmoidzOutputFully ConnectedPoolingReLU/SigmoidConvolutionalInputSAFTowards Robust CNNs Using SAFs

Gaussian distributions instead of high-dimension ones to
model the feature statistics of clean samples. Such 1-D
RBFs can be used within networks, as multi-dimensional
RBFs have been. The potential advantage is obvious:
it
may be possible to use much fewer 1-D RBF units than
high-dimensional ones, and therefore with fewer parame-
ters to train. It should be pointed out that we do not aim to
approximate a traditional RBF network of only a single hid-
den layer using 1-D RBFs. Instead, we propose to integrate
1-D RBF units with popular CNN models. The integration
is neither a pure ‘local’ model such as RBF networks nor a
pure ‘distributed’ model such as CNNs. It models diverse
features ‘locally’ with 1-D RBFs but combines them in a
‘distributed’ way to give the ﬁnal representation.

Figure 3. Statistics of four feature channels from the 2nd convo-
lutional layer in LeNet-5 on MNIST. Top two rows: histograms
of feature values. Bottom two rows: joint distributions of feature
values. Note: all feature values are from the center entry (4, 4);
red squares are from nonsense samples of pure Gaussian noise.

3. Symmetric Activation Functions
Compared with commonly-used activation functions such
as Rectiﬁed Linear Units (ReLU) (Nair & Hinton, 1996)
and sigmoid, it is well known the RBFs have a localiz-
ing property. 1-D RBFs suppress signals that deviate from
the normal in either direction while their counterparts sup-
press signals only unilaterally. By suppressing the signals
of abnormal samples, we ensure that no strong prediction

conv-1

conv-2

fc-1

fc-2

Figure 4. Visualization of the correlation matrices of features (on
speciﬁed entries) from the convolutional/fully-connected layers in
LeNet-5 on the MNIST test set.

conﬁdence will be produced at the top layer. Inspired by
the ReLU, we propose the mirrored rectiﬁed linear unit
(mReLU). In the paper, we call both the 1-D RBF and the
mReLU symmetric activation functions (SAF) since they
have the reﬂectional symmetry.

3.1. 1-D Radial Basis Function
The 1-D RBF adopted in the paper is σ(x) = e−x2, and
its derivative is −2xσ(x) (Fig. 1.a-b). Although it is
parameter-free, any needed ﬂexibility can be achieved by
argument rescaling and shifting in the preceding convolu-
tional layer.

3.2. Mirrored Rectiﬁed Linear Unit (mReLU)

The 1-D RBF requires exponential and square calculations.
These will add considerable load in both the training and
test stages. Inspired by the simplicity of the ReLU, we pro-
pose the mReLU composed of only basic arithmetic and
logic calculations (Fig. 1.c-d):

 1 + x,

1 − x,
0,

0 > x > −1,
+1 > x > 0,
otherwise,

(1)

mReLU(x) =

or equivalently

mReLU(x) = min (ReLU(1 − x), ReLU(1 + x)) .

(2)

Its derivative is

d
dx

mReLU =

 +1,

−1,
0,

0 > x > −1;
+1 > x > 0;
otherwise.

(3)

−2−101202000400060008000100001200014000feature values of the 6th channelamounts−2−1012302000400060008000100001200014000feature values of the 14th channelamounts−2−10123020004000600080001000012000feature values of the 37th channelamounts−3−2−1012020004000600080001000012000feature values of the 48th channelamounts−2−1012−1.5−1−0.500.511.522.56th channel14th channel−2−1012−1.5−1−0.500.511.522.56th channel37th channel−2−10123−1.5−1−0.500.511.522.514th channel37th channel−2−10123−3−2−101214th channel48th channel  −1−0.8−0.6−0.4−0.200.20.40.60.81  −1−0.8−0.6−0.4−0.200.20.40.60.81  −1−0.8−0.6−0.4−0.200.20.40.60.81  −1−0.8−0.6−0.4−0.200.20.40.60.81Towards Robust CNNs Using SAFs

3.3. The Capacity of SAF Networks

In the limit of using an inﬁnite number of RBF units,
a network of high-dimension RBFs can approximate any
function with arbitrary precision (Park & Sandberg, 1991).
We can use the products of 1-D RBFs to exactly mimic
any high-dimensional RBF, and therefore construct a 1-
D RBF network for function approximation. However,
the product-of-units scheme is not compatible with popu-
lar CNNs, and it does not work for the mReLUs. Below
we establish a property which is weaker than function ap-
proximation but with closer ties to our problem: sample
classiﬁcation.
We express the problem in geometric form. Suppose we
have N template samples Z1, Z2, ..., ZN in a uniform n-D
data space, and a ﬁdelity threshold r. If an input sample X
has (cid:107)X − Zi(cid:107) ≤ r for a certain i ∈ {1, 2, ..., N}, we clas-
sify X into the same category as Zi; otherwise we regard
it as a nonsense sample. We assume the problem is self-
consistent, i.e., there are no template samples from differ-
ent categories with overlapping radius-r hyperspheres. It
is sufﬁcient to ﬁnd a method to judge whether a sample is
in the radius-r hypersphere centered at a given sample Zi.
First, we have
Lemma 1. For a given hypersphere, we can build a net-
work of two SAF layers to judge whether a given sample is
in the hypersphere with arbitrary precision.

Proof. We present the proof for 1-D RBFs, as it is straight-
forward to be extended to the mReLU. We build a ba-
sic block of ﬁve layers as shown in Fig. 5.a. Let X =
[x1, x2, ..., xn]T , Zi = [zi,1, zi,2, ..., zi,n]T . We calculate

rk(X) = λ(xk − zi,k), sk(X) = e−r2
k ,
y(X) = e−t2
t(X) =

sk − n,

(4)

(cid:88)

k

where λ is a carefully chosen parameter, the sk’s are the
output of the ﬁrst SAF layer and the single y is the output
of the second SAF layer. To achieve a given error rate , it
is sufﬁcient to ﬁnd a threshold τ which holds V (Ω∩Ω(cid:48))
V (Ω∪Ω(cid:48)) ≥
1 − . Here V (·) is the volume of a closed subspace, Ω =
{X : (cid:107)X − Zi(cid:107) ≤ r} and Ω(cid:48) = {X : y(X) ≥ τ}. We let

(cid:21)T(cid:33)

.

r√
n

f1(r) = y

[zi,1 + r, zi,2, ..., zi,n]T(cid:17)
(cid:16)
(cid:32)(cid:20)

,

r√
n

r√
n

f2(r) = y

zi,1 +

, zi,2 +

, ..., zi,n +

(5)
It is easy to prove that f1(r) ≤ y(X) ≤ f2(r) for any X
which holds (cid:107)X − Zi(cid:107) = r.
We set τ = f1(r) and r0 = f−1
2 (τ ). It is easy to see that
r0 ≤ r. So we have Ω(cid:48)(cid:48) ⊂ Ω(cid:48) ⊂ Ω where Ω(cid:48)(cid:48) = {X :

(cid:0) r0

(cid:107)X − Zi(cid:107) ≤ r0}. Since we have V (Ω) = c · rn and
V (Ω(cid:48)(cid:48)) = c · rn
0 for a certain constant c, it is required that

(cid:1)n ≥ 1 − , or equivalently

r

f−1
2 (τ )
f−1
1 (τ )

≥ (1 − )

1

n .

(6)

−1
f
2 (τ )
−1
f
1 (τ )

Since lim
= 1, there must exist a τ0 satisfying eq.
τ→1
6, and so does λ. In other words, we can judge whether a
sample X is in the hypersphere of Zi with a error rate less
than  by comparing y(X) and τ0.

(a)

(b)

Figure 5. Using SAF units to build classiﬁcation networks. (a) a
basic block using two SAF layers to judge whether an input is in
a n-D hypersphere approximately. (b) a network using the basic
blocks in (a) to perform the classiﬁcation task.

For example, we can approximate the unit disk with three
1-D RBF units in Fig. 6.a. Then we use the basic block in
the proof of Lemma 1 to build the classiﬁcation network.
As shown in Fig. 5.b, we lay a basic block for each tem-
plate sample, then choose the maximum ymax across all
basic blocks. It ymax is larger than the threshold τ0 in the
proof of Lemma 1, we output the category of the chosen
basic block, otherwise we give a nonsense prediction. In-
terestingly, if we are allowed to use more 1-D RBF units in
the basic block, the output y will be increasingly similar to
a high-dimension RBF, as shown in Fig. 6.b-c.
Although the network in Fig. 5.b is compatible with the
popular CNNs in structure, the network is infeasible in
practice as we would require a huge number of SAF units.

(a)

(b)

(c)

Figure 6. 2-D approximations using 1-D RBFs. (a) using the basic
block to approximate a unit disk with different values of λ: 0.1
(bordered in blue), 0.4 (green), 0.7 (red) and 1.0 (dark). (b) the
approximation in the x1-x2-y space using ﬁve 1-D RBF units and
λ = 0.4. (c) a 2-D RBF.

SAF(cid:520) (cid:171)(cid:171)SAF(cid:520) SAF(cid:520) x(cid:20)x(cid:21)xn(cid:520) SAFr(cid:20)r(cid:21)rns(cid:20)s(cid:21)snty(cid:171)(cid:171)BasicBlock 1BasicBlock 2BasicBlockN(cid:171)(cid:171)MaxPrediction(cid:171)(cid:171)x(cid:20)x(cid:21)xn−1−0.500.51−1−0.500.51x1x2Towards Robust CNNs Using SAFs

Instead in Sec. 4, we exploit the advantages of deep CNNs,
which generate representations distributedly, to build effec-
tive classiﬁers with moderate number of SAFs.

4. Building Robust CNNs
In popular CNNs, convolutional layers are cascaded to
learn increasingly abstract features (Zeiler & Fergus,
2014). We ﬁnd the empirical distributions of these features
are fairly compact and symmetric (Fig. 3). Thus it is feasi-
ble to suppress unusual signals using SAFs which are simi-
larly shaped. We insert additional SAF layers immediately
after the convolutional layers to suppress unusual signals,
as shown in Fig. 2. Any ReLU activation layers can be dis-
carded since the SAF outputs are non-negative. We do the
same thing to sigmoid activation layers. We also add a 1-
D RBF layer after the fully-connected layer as a new ﬁnal
layer, but do not insert SAF layers between fully-connected
layers. In the remainder, we call the modiﬁed models ro-
bust CNNs and the original ones plain CNNs.

4.1. The Hybrid Loss function

i yi

(cid:19)

We design a special hybrid loss function for the robust
CNN that combines the negative logarithm of the normal-
ized value, and the weighted p-order errors (p > 1) accord-
ing to:
Lh (X, (cid:96), θ) = −α1·ln

+α2·(1 − y(cid:96))p+α3·(cid:88)

(cid:18) y(cid:96)(cid:80)

i(cid:54)=(cid:96)
(7)
where (cid:96) is the correct label, Y = {yi} is the prediction
on X by the robust CNN with the parameter θ, L is the
amount of categories, and α1, α2, α3 are weighting param-
eters. Lh is designed to prefer y(cid:96) to be large both relatively
and absolutely. It is necessary to involve the p-order errors
to get sufﬁciently large y(cid:96), thus to recognize nonsense sam-
ples which would be of small y(cid:96)’s. We set p = 2 in all our
experiments.

yp
i

4.2. Robustness Analysis

Figure 7. A simpliﬁed depiction of a robust CNN. X is the input,
and Y is the prediction. Blue circles: linear layers. Yellow circles:
SAF layers.

In the simpliﬁed robust CNN in Fig. 7, fi’s are convolu-
tional or pooling modules, and gi’s are the SAF modules
following convolutional layers. We have

Y = gn(fn(gn−1(fn−1(...g1(f1(X))...))))

(8)

where Y = [y1, y2, .., yL]T is the conﬁdence vector of

Figure 8. Perturbing samples for the plain and robust CNNs. X is
a clean sample, and y(cid:96) is the conﬁdence score for the (cid:96)th category.
Xadv−robust and Xadv−plain are the adversarial samples for the
robust and plain CNNs respectively.

length L, the number of categories. The partial derivative
of y(cid:96) with respect to xk is

· (cid:88)

(cid:18) ∂fn,(cid:96)

(cid:19)

=

(9)

in−1,...,i1

dgn,(cid:96)
dfn,(cid:96)

∂gn−1,in−1

· ... · ∂f1,i1
∂xk

∂y(cid:96)
∂xk
where i1 → i2 →, ...,→ in−1 → (cid:96) is a path from xk to y(cid:96)
crossing all layers, in other words, they are the indices of
cells whose receptive ﬁeld contain xk. For unusual signals
which are away from the high-density regions, the g’s in
eq. 9 will be considerably small. Consequently, according
to the derivatives of SAFs shown in Fig. 1, the ∂g(.,.)
’s
∂f(.,.)
are also small or nearly zero, implying ∂y(cid:96)
is unlikely to
be large. If the norm of the gradient ∇y(cid:96) with respect to
∂xk
X is small, we have to make a large perturbation ∆X to
achieve a noticeable change of y(cid:96), as shown in Fig. 8. In
conclusion, to achieve an adversarial sample successfully,
the perturbation for the robust CNN would be larger than
that for the plain CNN.

4.3. Training Issues

An adversarial training requires repeated generations of ad-
versarial samples and so increases the training load signiﬁ-
cantly. In this paper, we also use training data additional to
the clean samples; but instead of adversarial samples, we
use random perturbation of the mean of training data. We
ﬁnd the mean sample itself might be easily mis-classiﬁed
(it seldom belongs to any meaningful category), and we can
generate nonsense samples by perturbing it slightly. There-
fore we use it as a typical hard instance, and train robust
CNNs to produce high conﬁdence for nonsense on it. How-
ever, there is only one mean sample and thus little effect on
the training process. So we perturb it with the Gaussian
noise and get a certain number of noisy copies. We call
this trick mean training for short.
It is suggested in (Goodfellow et al., 2015) that randomly
perturbed samples are of little signiﬁcance. Interestingly,
we do ﬁnd them effective for our network, especially when
adopted together with mean training. We generate per-

Xh(cid:20)g(cid:20)gnY(cid:397)hn(cid:171)(cid:171)Xg(cid:20)f(cid:20)fnYgn(cid:171)(cid:171)XXadv-plainXadv-robustyl(cid:507)X(cid:507)Y(cid:507)X(cid:507)YThreshold lineTowards Robust CNNs Using SAFs

turbed samples by adding perturbations randomly drawn
from [−δ, +δ]w×h×c (w: width, h: height, c: channel)
to the original samples. We perturb a fraction of the train
set and leave others untouched. We call this trick random
training for short.
We use the standard stochastic gradient descent method
to train the robust CNNs. It is unnecessary to use a pre-
training stage to ﬁnd RBF centers because 1-D RBFs are
parameter-free. The batch normalization trick (Ioffe &
Szegedy, 2015) is necessary to accelerate training robust
CNNs and prevent the gradient from vanishing.

5. Experiments
We use two well-studied datasets, MNIST (LeCun et al.,
1998) and CIFAR-10 (CIFAR, accessed in Feb 2016), in
our experiments. In the following ﬁgures and tables, we use
plain, RBF and mReLU to refer respectively to plain CNNs,
robust CNNs using 1-D RBF, and robust CNNs using mRe-
LUs. We use the ﬂags -a, -r and -m to indicate the use of ad-
versarial, random and mean training. The plain CNNs are
typical models implemented in (Vedaldi, accessed in Feb
2016). We use all default settings in (Vedaldi, accessed in
Feb 2016) to train them, except for whitening and normal-
ization. We plan to release our source code and data upon
acceptance.
In related literature, CNNs are expected to be robust against
three kinds of sample: adversarial, nonsense and noisy. If
our proposals about SAFs are correct, then robust CNNs
will be as accurate as plain CNNs; on adversarial/noisy
samples with tiny perturbations, the classiﬁcation accuracy
should not drop remarkably; while on severely perturbed
samples or pure noise images, they should be classiﬁed as
nonsense. The nonsense case is a little different for models
without an explicit nonsense category: we consider they
predict a sample as nonsense if all conﬁdence scores of
meaningful categories are below a threshold, e.g. 0.5.
We follow (Goodfellow et al., 2015) to generate adversarial
samples and nonsense samples by:

Xadv = X + 255 · β · sign (∇Lh(X, (cid:96)max, θ))

(10)
where ∇Lh(X, (cid:96), θ) is the gradient towards the smaller
loss for a certain incorrect category (cid:96), β is the strength
parameter, and (cid:96)max = arg max(cid:96) (cid:107)∇Lh(X, (cid:96), θ)(cid:107)1. X
and Xadv are the original and adversarial/nonsense sam-
ples respectively. Nonsense samples are generated from the
stationary Gaussian noise, shifted and scaled to cover the
range 0∼255. Noisy samples are generated by perturbing
clean samples with stationary Gaussian noise. We collect
classiﬁcation accuracies on varying perturbation strength
β, rather than at a single strength value, to better under-
stand the robustness of CNNs. Meanwhile, we present the

well-adopted Peak-Signal-Noise-Ratios (PSNR) to assess
the image quality easily.

5.1. MNIST

We use LeNet-5 (LeCun et al., 1998) as the plain CNN
for MNIST, please see Table 1 for its structure. There are
70,000 clean samples and we follow (Vedaldi, accessed in
Feb 2016) to use 60,000 as the train set and 10,000 as the
test set. There are 10,000 groups of nonsense samples. All
CNNs are trained with 20 epochs. We set α1 = 1, α2 =
1, α3 = 0 for the hybrid loss functions of robust CNNs.
We present accuracies and error rates in Fig. 9 and Table
2.
It is clear that CNNs using SAFs are much more ro-
bust than the plain CNN. The best one, mReLU-r-m, makes
marginally more errors than the plain CNN for clean sam-
ples, adversarial samples with perturbation of up to strength
β = 0.01, and noisy samples of up to β = 0.10; but
has greatly superior performance for adversarial and noisy
samples of large perturbation, and all nonsense samples.
Particularly, the accuracy of mReLU-r-m under adversarial
perturbations of the strength β = 0.25 is only 0.073, which
is much lower than 0.179 of Maxout (Goodfellow et al.,
2013) trained using adversarial training (Goodfellow et al.,
2015).
Random training is very effective in improving the robust-
ness of SAF CNNs against both adversarial and noisy sam-
ples.
In our opinion, random perturbations of the train
samples make the large-output/non-zero widths of SAFs ef-
fectively broader, which improving robustness to perturba-
tions of moderate strength. Using all training tricks, we can
improve all CNNs to be considerably robust against non-
sense and noisy samples, even the plain CNN. Adversarial
training, which is recommended in previous literatures, is
much less effective improving the robustness against both
adversarial and nonsense samples. It is probably due to our
insufﬁcient number of adversarial training rounds. So in
Sec. 5.2, we train all CNNs with only the combination of
random and mean training.
We show some examples of mReLU-r-m classiﬁcations in
Fig. 10. The robust CNN gives correct predictions on even
severe distorted samples.
It does make incorrect predic-
tions for samples bounded by color boxes, but the images
are hard even for a human being. The case of green boxes is
a bit different: the predictions are wrong, but it is sensible
to classify these samples into the nonsense category.

5.2. CIFAR-10

The plain CNN for CIFAR-10 is a deeper LeNet model;
see Table 3 for its structure. There are 60,000 clean sam-
ples and we follow (Vedaldi, accessed in Feb 2016) to use
50,000 as the train set and 10,000 as the test set. The

Towards Robust CNNs Using SAFs

Table 1. Structures of the plain and robust CNNs for MNIST. Parameters of convolutional layers: cv1-(5, 5, 20), cv2-(5, 5, 50) (in
the height-width-channel order). The number of hidden units in fully-connected layers are 500 (fc1) and 10 (fc2). max-max pooling.
sloss-softmaxloss. hloss-hrbridloss.

Models
cv1
Plain
RBF
cv1
mReLU cv1

-

max
1-D RBF max
mReLU
max

cv2
cv2
cv2

-

Layers
max
1-D RBF max
mReLU
max

fc1
fc1
fc1

ReLU fc2
ReLU fc2
ReLU fc2

-

1-D RBF
1-D RBF

sloss
hloss
hloss

#Layers

8
11
11

(a)

(b)

(c)

Figure 9. Accuracies of the CNNs on the MNIST test set. (a), (b) and (c) are the accuracies on adversarial, nonsense and noisy samples
respectively. The horizontal axes are the perturbation strength β’s.

-

clean

β
PSNR
On adversarial samples:
0.009
plain
mReLU-r-m 0.015
On nonsense samples:
plain
0.979
mReLU-r-m 0.000
On noisy samples:
0.009
plain
mReLU-r-m 0.015

0.013
0.016

0.982
0.000

0.008
0.016

Table 2. Error rates on the MNIST test set. The best values are shown in bold.
0.25
0.01
40.00
12.04

0.03
30.46

0.02
33.98

0.04
27.96

0.05
26.02

0.15
16.48

0.20
13.98

0.10
20.00

0.30
10.46

0.40
7.96

0.50
6.02

0.023
0.017

0.041
0.019

0.071
0.020

0.118
0.022

0.537
0.030

0.836
0.039

0.914
0.051

0.934
0.073

0.943
0.110

0.948
0.252

0.944
0.463

0.981
0.000

0.008
0.016

0.974
0.000

0.009
0.016

0.958
0.000

0.009
0.016

0.937
0.000

0.010
0.016

0.979
0.000

0.013
0.017

0.993
0.000

0.991
0.000

0.991
0.002

0.993
0.017

0.994
0.146

0.997
0.270

0.027
0.018

0.109
0.020

0.234
0.023

0.372
0.027

0.686
0.039

0.817
0.059

00.10.20.30.40.500.10.20.30.40.50.60.70.80.91classification accuracyβ00.10.20.30.40.500.10.20.30.40.50.60.70.80.91classification accuracyβ00.10.20.30.40.500.10.20.30.40.50.60.70.80.91classification accuracyβ  plainplain−aplain−rplain−r−mRBFRBF−rRBF−r−mmReLUmReLU−rmReLU−r−mTowards Robust CNNs Using SAFs

0.01

0.02

0.03

0.04

0.05

0.10

0.15

0.20

0.25

0.30

40.00

33.98

30.46

27.96

26.02

20.00

16.48

13.98

12.04

10.46

Strengths β
clean
PSNR

-

0.40

7.96

0.50

6.02

Figure 10. Examples of mReLU-r-m classiﬁcations on the MNIST test set. The prediction and conﬁdence are on the top of individual
samples. ns-nonsense. Rows 1-3: adversarial samples. 4th row: nonsense samples. 5th row: noisy samples.

training set is augmented by randomly reﬂecting samples
about their vertical mid-line. There are 10,000 groups of
nonsense samples. The robust CNNs are trained with 90
epochs, twice of that for the plain CNN. We set α1 =
9 for the hybrid loss functions of robust
1, α2 = 1, α3 = 1
CNNs.
We present the accuracies and error rates in Fig. 11 and
Table 4. Since the CIFAR-10 images are colored, the per-
turbation is much easier to perceive. Therefore we use per-
turbations of smaller magnitude than those for the MNIST.
As with the MNIST, the robust CNNs are much more ro-
bust than the plain CNN. The best one, mReLU-r-m, makes
marginally more errors than the plain CNN for clean sam-
ples, and noisy samples of up to β = 0.015; but has greatly
superior performance for noisy samples of large pertur-
bation, all adversarial samples and all nonsense samples.
However, the improvement on CIFAR-10 is not as large as
that on the MNIST. The reason is that there are not sufﬁ-
cient training samples in the CIFAR-10 to train a perfect
classiﬁer, thus lots of samples are close to the decision
boundaries and easy to perturb to be an adversary. The
robustness of plain-r-m against nonsense samples is not as
good as with the MNIST, and some curves in Fig. 11.b
seem a bit odd. We leave it to the future work.
Interestingly, the accuracy of mReLU CNN (without ran-
dom and mean training) on clean samples is 0.194 which
is marginally better than that of the plain CNN, 0.217. Al-

though we believe the plain CNN can perform better with
advanced training tricks, we hypothesize that there is a con-
nection between the robustness and generalization capabil-
ity. Thus the CNN using mReLUs can generalize slightly
better on the test set.
We show some examples of mReLU-r-m classiﬁcations in
Fig. 12. The model makes correct predictions on most
samples, but makes incorrect predictions for the samples in
the red squares which are of the severest perturbation.

6. Conclusions and Discussion
Our experiments show it is effective to use SAFs to improve
the robustness of CNNs. Without substantial changes to
the accuracies on clean samples, we obtain remarkably bet-
ter robustness against adversarial, nonsense and noisy sam-
ples. That supports our proposal that SAFs can improve the
robustness of CNNs by suppressing unusual signals.
Since we change only the activation functions of CNNs,
while leaving their structure, training strategies and opti-
mization methods untouched, it implies there are no se-
vere weaknesses in existing CNN frameworks. Neverthe-
less, in contrast with popular activation functions including
sigmoid and ReLU, the SAFs have less documented sup-
port from neuroscience research. Furthermore, it remains a
question how to accommodate SAFs better by ﬁne-tuning
the CNN structure and training strategies to get the better

Towards Robust CNNs Using SAFs

Table 3. Structures of the plain and robust CNNs for CIFAR-10. Parameters of convolutional layers: cv1-(5, 5, 32), cv2-(5, 5, 32), cv3-
(5, 5, 64) (in the height-width-channel order). The number of hidden units in fully-connected layers are 64 (fc1) and 10 (fc2). max-max
pooling. avg-average pooling. sloss-softmaxloss. hloss-hrbridloss.

Models
Plain
RBF
mReLU

cv1
cv1
cv1

-

1-D RBF
mReLU

max
max
max

cv2
cv2
cv2

-

1-D RBF
mReLU

avg
avg
avg

cv3
cv3
cv3

Layers

-

1-D RBF
mReLU

avg
avg
avg

fc1
fc1
fc1

ReLU
ReLU
ReLU

fc2
fc2
fc2

-

1-D RBF
1-D RBF

sloss
hloss
hloss

#Layers

10
14
14

(a)

(b)

(c)

Figure 11. Accuracies of the CNNs on the CIFAR-10 test set. (a), (b) and (c) are the accuracies on adversarial, nonsense and noisy
samples respectively. The horizontal axes are the perturbation strength β’s.

-

clean

β
PSNR
On adversarial samples:
0.217
plain
mReLU-r-m 0.226
On nonsense samples:
plain
0.937
mReLU-r-m 0.000
On noisy samples:
0.217
plain
mReLU-r-m 0.226

0.591
0.255

0.679
0.000

0.218
0.226

Table 4. Error rates on the CIFAR-10 test set. The best values are shown in bold.
0.010
40.00

0.020
33.98

0.015
36.48

0.025
32.04

0.030
30.46

0.035
29.12

0.040
27.96

0.045
26.94

0.050
26.02

0.075
22.50

0.100
20.00

0.150
16.48

0.719
0.278

0.800
0.309

0.850
0.344

0.885
0.388

0.905
0.430

0.918
0.468

0.928
0.506

0.933
0.541

0.941
0.677

0.938
0.751

0.922
0.816

0.742
0.000

0.221
0.225

0.779
0.000

0.804
0.000

0.821
0.000

0.835
0.000

0.843
0.000

0.849
0.000

0.857
0.000

0.880
0.008

0.889
0.111

0.916
0.281

0.227
0.225

0.231
0.225

0.234
0.225

0.241
0.225

0.253
0.224

0.266
0.223

0.280
0.223

0.357
0.220

0.461
0.230

0.656
0.295

00.050.10.1500.10.20.30.40.50.60.70.8classification accuracyβ00.050.10.1500.10.20.30.40.50.60.70.80.91classification accuracyβ00.050.10.1500.10.20.30.40.50.60.70.8classification accuracyβ  plainplain−r−mRBFRBF−r−mmReLUmReLU−r−mTowards Robust CNNs Using SAFs

0.010

0.015

0.020

0.025

0.030

0.035

0.040

0.045

0.050

0.075

0.100

0.150

40.00

36.48

33.98

32.04

30.46

29.12

27.96

26.94

26.02

22.50

20.00

16.48

Strengths β

-

clean

PSNR

Figure 12. Examples of mReLU-r-m classiﬁcations on the CIFAR-10 test set. The prediction and conﬁdence are on the top of individual
samples. sp-ship. bd-bird. fr-frog. ap-airplane. ns-nonsense. Rows 1-3: adversarial samples. 4th row: nonsense samples. 5th row:
noisy samples.

performance. In that sense, we believe the attempt in this
paper is just a start and far from the end.

References
Andrew, G. and Gao, J. Scalable training of l1-regularized

log-linear models. In ICML, 2007.

Bohte, S. M., Poutr´e, H. L., and Kok, J. N. Unsupervised
clustering with spiking neurons by sparse temporal cod-
ing and multilayer rbf networks. IEEE TNN, 2002.

Broomhead, D. S. and Lowe, D. Multivariable fuctional
interpolation and adaptive networks. Complex systems,
1988.

Chalupka, K., Perona, P., and Eberhardt, F. Visual causal

feature learning. In UAI, 2015.

CIFAR. Cifar-10. https://www.cs.toronto.edu/∼kriz/cifar.html,

accessed in Feb 2016.

Craddock, R. J. and Warwick, K. Multi-layer radial basis
function networks. an extension to the radial basis func-
tion. In ICNN, 1996.

Goodfellow, I. J., Warde-Farley, D., Mirza, M., Courville,

A., and Bengio, Y. Maxout networks. In ICML, 2013.

Goodfellow, I. J., Shlens, J., and Szegedy, C. Explaining

and harnessing adversarial examples. In ICLR, 2015.

Gu, S. and Rigazio, L. Towards deep neural network archi-
tectures robust to adversarial examples. In ICLR work-
shop, 2015.

He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn-

ing for image recognition. arXiv:1512.03385, 2015.

Ioffe, S. and Szegedy, C. Batch normalization: accelerat-
ing deep network training by reducing internal covariate
shift. In ICML, 2015.

Jacobs, R. A., Jordan, M. I., Nowlan, S. J., and Hinton,
G. E. Adaptive mixtures of local experts. Neural com-
putations, 1991.

Krizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet
classiﬁcation with deep convolutional neural networks.
In NIPS, 2012.

LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. Gradient-
based learning applied to document recognition. Pro-
ceedings of the IEEE, 1998.

Makhzani, A., Shlens, J., Jaitly, N., and Goodfellow, I. Ad-

versarial autoencoders. arXiv:1511.05644, 2015.

Miyato, T., Maeda, S. I., Koyama, M., Nakae, K., and Ishii,
S. Distributional smoothing by virtual adversarial exam-
ples. In ICLR, 2016.

Towards Robust CNNs Using SAFs

Moody, J. and Darken, C.

Fast learning in networks
of locally-tuned processing units. Neural computation,
1989.

Mr´azov´a, I. and Kukaˇcka, M. Multi-layer radial basis func-
tion networks. an extension to the radial basis function.
In ICNN, 1996.

Nair, V. and Hinton, G. Rectiﬁed linear units improve re-

stricted boltzmann machines. In ICML, 1996.

Nguyen, A., Yosinski, J., and Clune, J. Deep neural net-
works are easily fooled: High conﬁdence predictions for
unrecognizable images. In CVPR, 2015.

Park, J. and Sandberg, I. W. Universal approximation us-
ing radial-basis-function networks. Neural computation,
1991.

Radford, A., Metz, L., and Chintala, S. Unsupervised rep-
resentation learning with deep convolutional generative
adversarial networks. In ICLR, 2016.

Sabour, S., Cao, Y., Faghri, F., and Fleet, D. J. Adversarial

manipulation of deep representations. In ICLR, 2016.

Springenberg, J. T. Unsupervised and semi-supervised
learning with categorical generative adversarial net-
works. In ICLR, 2016.

Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan,
D., Goodfellow, I., and Fergus, R. Intriguing properties
of neural networks. In ICLR, 2014.

Vedaldi, A. Matconvnet. http://www.vlfeat.org/matconvnet,

accessed in Feb 2016.

Zeiler, M. D. and Fergus, R. Visualizing and understanding

convolutional networks. In ECCV, 2014.

