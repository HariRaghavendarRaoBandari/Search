6
1
0
2

 
r
a

 

M
5
1

 
 
]

V
C
.
s
c
[
 
 

2
v
2
3
1
4
0

.

3
0
6
1
:
v
i
X
r
a

A Novel Method for Extrinsic Calibration of a 2-D

Laser-Rangeﬁnder and a Camera

Wenbo Dong, Student, and Volkan Isler, Member, IEEE

1

Abstract—We present a novel solution for extrinsically calibrating a camera and a Laser Rangeﬁnder (LRF) by computing the
transformation between the camera frame and the LRF frame. Our method is applicable for LRFs which measure only a single plane.
It does not rely on observing the laser plane in the camera image. Instead, we show that point-to-plane constraints from a single
observation of a V-shaped calibration pattern composed of two non-coplanar triangles sufﬁce to uniquely constrain the transformation.
Next, we present a method to obtain a solution using point-to-plane constraints from single or multiple observations. Along the way, we
also show that previous solutions, in contrast to our method, have inherent ambiguities and therefore must rely on a good initial estimate.
Real and synthetic experiments validate our method and show that it achieves better accuracy than previous methods.

Index Terms—Extrinsic Calibration, 2D Laser Rangeﬁnder

!

1 INTRODUCTION

A Fundamental task in computer vision is to compute envi-

ronment geometry [1]. In many computer vision systems,
cameras are coupled with additional sensors to acquire dense
depth information in real-time. In indoor environments, RGB-D
cameras such as Microsoft Kinect are now commercial grade [2].
While commonly used, RGB-D cameras have their own limi-
tations. They have limited range and usually get saturated in
bright outdoor environments. Therefore, many outdoors systems
combine a camera with a Laser Rangeﬁnder (LRF) [3], [4]. On
the high-end of the spectrum, there are 3D LIDAR devices which
can measure multiple depth planes and can have hundreds of
meters range [5]. These devices can be heavy, power-hungry and
expensive. As such, smaller 2D LRFs which measure only a
single depth plane are commonly used to complement cameras
in autonomous navigation and mobile robotics applications [6].

Taking advantage of measurements from an LRF or a LIDAR
combined with a camera, however, requires precise knowledge of
the relative pose (orientation and position) between them. This
is a classical extrinsic calibration problem where the objective is
to determine the transformation between two coordinate frames.
If we can establish correspondences between the laser points
and their images, the extrinsic calibration problem becomes a
standard PnP (Perspective-n-Point) computation [7]. Establishing
correspondences is easier for 3D LIDARs since distinct features
can be identiﬁed both among laser points and in the camera image.
Existing methods include 3D LIDAR-camera calibration by using
a circle-based calibration target [8] and an arbitrary trihedron [9].
An analytical method for computing a precise initial estimate
for both the LIDAR’s intrinsic parameters and the 3D LIDAR-
camera transformation is presented in [10]. Further, the authors
in [11] present a mutual information based algorithm for automatic
extrinsic calibration of a 3D LIDAR-camera system without the
need for any speciﬁc calibration target.

• W. Dong and V. Isler are with the Department of Computer Science and

Engineering, University of Minnesota, Twin Cities, MN, 55455, USA.
E-mail: {dong, isler}@cs.umn.edu

Figure 1. The calibration system incorporating a calibration target and a
capture rig; Left: The calibration target formed by two triangular boards
with a checkerboard on each triangle; Right: The capture rig consisting
of a 2D LRF and stereo cameras.

Extrinsic calibration of a 2D LRF is more challenging. The
main challenge is that a 2D LRF produces only a single scanning
plane for each pose, which makes it difﬁcult to ﬁnd correspon-
dences. For example, suppose we observe a triangular board with a
camera and a 2D LRF (a.k.a. laser). First, consider the intersection
between the laser plane and the board. By ﬁnding discontinuities
in the laser image, we can ﬁnd the endpoints of this intersection.
However, we cannot tell where these two points project to in the
camera image. Therefore, additional constraints must be used. We
study this extrinsic calibration problem and make the following
contributions:

• We show that a single observation of two non-coplanar
triangles sharing a common side (Fig. 1) sufﬁces to unam-
biguously solve the calibration problem.

• Even though planar, triangular or V-shaped rectangular
patterns have already been proposed to solve the cali-
bration problem, we show that previous methods do not
sufﬁciently constrain the calibration problem to allow for
a unique solution. Therefore, they rely on a large number

of measurements and a good initial estimate.

• We present a robust method to solve the calibration from

•

a single observation in the presence of noise.
For additional accuracy, we show that by using only a few
additional observations, our method achieves signiﬁcantly
smaller error than existing methods.

The rest of the paper is structured as follows. In Section 2,
we present a survey of existing methods and point out the spatial
ambiguity in each of them. The calibration setup and features
used in our method are described in Section 3. We prove the
sufﬁciency of our geometry constraints to obtain a unique solution
in Section 4. In Section 5, we present a solution technique for
obtaining a unique solution from a single observation of the
calibration target, and extend it to the case of using multiple
observations. Further details of the setup and how to extract
features are explained in Section 6. In Section 7, our method is
validated through simulation and real experiments. We conclude
the paper in Section 8.

2 RELATED WORK
The basic approach for extrinsic calibration between a camera
and an LRF is to recognize the laser points from the images.
For example, an IR camera can observe the laser beams and
directly search for correspondences between it and the LRF.
Based on the correspondences, the calibration can be solved as
a PnP problem [12], [7]. However,
the light beams of most
LRFs are invisible for regular cameras, which is why additional
constraints are needed. This kind of extrinsic calibration problem
is commonly solved by establishing geometric constraints from
the the association of different features (e.g. points, lines and
planes) between the measurements observed simultaneously from
both sensors.

One of the earliest methods presented in [13] is based on
the constraints between different views of a planar calibration
pattern from the camera and the LRF. Speciﬁcally, the authors
ﬁrst estimate the pose (orientation and position) between the
camera and the pattern for each observation. Then, they impose
the geometry constraint between the laser points and the pattern
plane (that laser points must lie on the plane of the calibration
pattern) to obtain a linear least-squares solution which is served as
the initial value in the further non-linear optimization for rotation
and translation parameters. The weakness of the approach is that
the constraint is not sufﬁcient enough to ﬁnd the correct solution
(Fig. 2). The cost function focuses on the distance between the
laser points and the corresponding plane. So it constrains only
two out of six degrees of freedom for the relative pose between
camera and LRF, and the remaining four degrees have ambiguity.
Therefore, the cost value at the obtained solution may not be the
optimum since many local minima exist. Consequently, it lets
the cost function converge into an incorrect solution frequently.
Another reason for why the solution of this method could be sub-
optimal is that there is no guarantee that the initial value satisﬁes
the rotation matrix constraints, R ∈ SO(3). Further, projecting the
rotation part of the obtained solution onto the nearest orthogonal
matrix could lead to a local minimum. The authors in [14] also
address these disadvantages. They present a minimal solution to
this problem reformulated as a linear perspective PnP problem
and also reﬁne it with non-linear optimization. But it still suffers
from limited accuracy and robustness because the constrains
from each observation have ambiguities in relative pose between

camera and LRF, and a large number (typically more than 30) of
different observations are needed for an accurate result (rotation
and translation error within 1◦ and 10 mm).

2

Figure 2. Existing methods do not sufﬁciently constrain the problem.
(a): In Zhang’s approach [13], the rectangular calibration board can be
moved horizontally and vertically, and also can be respectively rotated
along two different axes without changing their solution; (b): In Li’s
approach [15], the triangular calibration board can be rotated respec-
tively along two different axes, and each red laser edge point can be
moved along the green plane through the camera center and one board
edge; (c): In methods [16] and [17], the V-shaped calibration target
can be moved vertically, and each red laser edge point can be moved
long its corresponding green plane through the camera center and one
board edge; (d): In approaches [19] and [18] using single or double line
features but with same principle, the rectangular calibration board can
be moved vertically, and each red laser feature point can be moved long
the green plane through the camera center and one feature line.

The approaches proposed in [15] and [16] use constraints
that laser points must lie at intensity edges or lines obtained by
corner detection from the image, and performs the optimization
minimizing the projection error from image. The approach in [15]
uses a black triangular board and aims at minimizing the distance
between the projected side lines and the intersected laser points
on the corresponding lines. The approach in [16] also uses a V-
shaped calibration target with the constraint that the intersected
laser point with maximum curvature must lie on the center line
of the calibration target from the image. The results from these
two methods are not accurate due to sparse sampling of laser
points,
therefore a large number (usually more than 100) of
images are needed to compensate for the lack of constraints for
each observation. Using triangular board [15] turns out to be
one way to improve the constraints above-mentioned in [13]: the
constraint “points on the border lines” removes the ambiguity
of the horizontal translation and “triangular plane” removes the
ambiguity of the vertical translation for the relative camera-LRF
pose. However, there are still two degrees of freedom remained
as ambiguities for the triangular board (See Fig. 2). Essentially,
the drawback is that the constraints are imposed on image: there
must be a total of four degrees of freedom from views of depth
and orientation (2 linear geometry constraints plus 3 nonlinear
constraints for rotation matrix to solve nine unknowns and more
details are explained in Section 4).

Based on the ideas in [15] and [16], the authors in [17]

propose a method to minimize the projection distance on the
image between intersected laser points and the feature lines of
a V-shaped calibration target. This method increases the laser
points’ sampling for each observation by introducing virtual end-
points, but the same drawback still exists. Therefore, they need
a large amount (around 50) of different observations to achieve
a reasonable result (back-projection laser points matches objects
from images). Besides, the convergence of further optimization
requires good initial estimate, which is not guaranteed in these
methods. From Fig. 2, the same weakness still exists when using
V-shaped rectangular board as in [16] and [17].

There exist methods using other kinds of calibration targets to
deal with this calibration problem. The method in [18] provides
an analytical solution using a white board with a black band in
the middle. It needs only six different observations. Similarly, the
authors in [19] gives an analytical solution to this problem using a
white board with a black line in the middle. Compared with [18], it
further computes the optimal least-squares solution to improve the
robustness to noise. However, both of these two methods cannot
avoid using a large number of different observations for accuracy
because of the insufﬁciency of constraints for each observation.
The ambiguities in these two methods are shown in Fig. 2. The
disadvantage for insufﬁcient constraints is that it reduces the
robustness of implementation since a relative large number of
observations from different valid views are needed in order to
deal with the ambiguities for an accurate result.

The work described in [20], presents an approach only re-
quiring the observation of a scene corner (orthogonal trihedron)
commonly found in any human-made environment. Moreover, this
method builds line-to-plane and point-to-plane constraints, which
are enough for each observation. However, its assumption of three
orthogonal planes highly depends on the accuracy of three right
angles which are difﬁcult to be made exactly 90◦ in practice.

In contrast to previous methods, we propose a novel method
for camera-LRF extrinsic calibration using a triangular V-shaped
calibration target with checkerboards on it, which can be used
for camera calibration. Our method builds sufﬁcient constraints,
which guarantee the uniqueness of the solution for each observa-
tion. In theory, we can use only one observation to calibrate the
camera-LRF rig. In practice, an accurate result can be achieved
with only a few images (previous methods require 30 or more).
Further, the angle between two triangular boards of our calibration
target is arbitrary which makes it convenient.

3 SETUP AND FEATURES
Our proposed calibration method is based on observing a V-shaped
calibration target which is formed by two triangular boards with
a checkerboard on each triangle (Fig. 3). The four corners of the
target are P, Q, R and O. For a given rigidly connected camera and
LRF pair, {C} and {L} are respectively the camera’s and LRF’s
frames of reference. We deﬁne the triangles as T1 = (cid:52)PQC and
T2 = (cid:52)PRC, and let T3 = (cid:52)PQO and T4 = (cid:52)PRO. For simplicity,
the notations we use are the same ones to refer the planes they lie
in. In contrast to the methods proposed in [16] and [17], the angle
between T3 and T4 is not required to be a ﬁxed value. It could be
an arbitrary obtuse angle. For each observation, the scanning plane
of LRF intersects with the three sides PQ, PR and PO at points
Lp1, Lp2 and Lp3 respectively in the LRF frame. Moreover, the
camera and LRF should be either synchronized or held stationary
during data collection. Each observation of the calibration target

3

consists of an image acquired from the camera and a single scan
obtained from the LRF. The features to be extracted from a single
observation are: three laser points Lp1, Lp2 and Lp3 in LRF frame;
four unit normal vectors Cn1, Cn2, Cn3 and Cn4 of respective
planes T1, T2, T3 and T4 in camera frame; and two distances d1 and
d2 from camera to planes T3 and T4 respectively. Further details of
feature extraction are described in Section 6.

Figure 3. A single pair of camera-LRF observations of the calibration
target with deﬁnition of parameters for geometry constraints.

Problem Deﬁnition: Next, we formally deﬁne the problem
of camera-LRF calibration. The objective is to obtain the relative
pose between these two sensors: in our case, the orientation C
LR
and position CtL of LRF with respect to camera’s frame. The LRF
uses a bundle of laser beams to measure the distance to objects
within its scanning plane. Without loss of generality, the laser
scanning plane is deﬁned as the plane YL = 0. Thus, a 3D laser
point represented as Lp = [xL,0,zL](cid:62) can be transformed to the
point Cp = [xC,yC,zC](cid:62) in the camera frame by

Cp = C

LR· Lp + CtL,

(1)
LR is a 3× 3 orthonormal rotation matrix and CtL is a 3× 1

where C
translation vector.
The camera is modeled by the standard pinhole model. A 3D
point Cp in the camera frame is projected to a pixel ip = [u,v](cid:62) in
the image coordinate using the following relation

i ˜p ∼ K· Cp,

(2)
where i ˜p = [ip(cid:62),1](cid:62) and K is the 3×3 camera matrix. In practice,
images can exhibit signiﬁcant lens distortion, which is usually de-
scribed as a 5×1 vector consisting of radial and tangent distortion
coefﬁcients. In the rest of this paper, we ignore these distortions
and assume that the images have already been undistorted using
the functions from MATLAB Camera Calibration Toolbox [21].

4 UNIQUENESS OF THE SOLUTION
In this section, we show that the features (three laser points Lp1,
Lp2 and Lp3; four normal vectors Cn1, Cn2, Cn3 and Cn4; and
two distances d1 and d2) from a single observation constrain the
calibration problem to an unique solution.

4.1 Constraints
A single laser scan consists of a depth and angle at which the
depth was sensed. In LRF frame, we assume that the sensor is
at its origin L. Let us express the feature points Lpi as [xi,0,zi](cid:62),

(cid:0)C
LR· Lpi + CtL
(cid:0)C
LR· Lp j + CtL
(cid:0)C
LR· Lpk + CtL

(cid:1) = 0,
(cid:1) = d1,
(cid:1) = d2.

where i = {1,2,3} are the indices of the feature points. Let the
feature norm vectors ni be [a j,b j,c j](cid:62), where j = {1,2,3,4} are
the indices of the norm vectors of planes Ti. We now have a
correspondence between a 3D point in LRF frame and a plane
in camera frame. Thus, our constraint is that the laser point,
transformed to the camera frame, must lie on the corresponding
plane, which can be divided into three parts.
First, laser points Lp1 and Lp2 must respectively lie on the
planes T1 and T2. Then, for i = {1,2}, the ﬁrst two constraints
have the form

(3)
LR ∈ SO(3) and CtL are the unknowns. Second, for laser
where C
points Lp1 and Lp3, they must both lie on the plane T3. Then, we
have other two constraints

Cn(cid:62)
i

Cn(cid:62)
3

Cn(cid:62)
4

(4)
for j = {1,3}. Similarly, laser points Lp2 and Lp3 must both lie on
the plane T4. This gives two more constraints for k = {2,3}:

(5)
Let r1, r2 and r3 be the three columns of C
LR. Since we set up the
LRF coordinate system in a way that the second coordinate of any
laser point Lp is zero, we do not have an explicit dependence on
r2. Once we solve for r1 and r3, r2 can be obtained by

(6)
LR is an orthonormal matrix, we have three further con-

r2 = r3 × r1.

r(cid:62)
3 r3 = 1,

r(cid:62)
1 r1 = 1,

(7)
To summarize, we have nine unknowns (in r1, r3 and CtL) and
a system of six linear and three nonlinear equations. In the next
section, we show that these constraints are independent and hence
sufﬁcient to obtain a solution.

r(cid:62)
3 r1 = 0.

Since C
straints

4.2 Proof of Uniqueness
For a single observation of the calibration target, our method
builds up a system of Equations (3)-(7). In order to prove
the proposed method does not induce any ambiguity, the nine
equations must be independent. We show that the ﬁrst six linear
equations are linearly independent. Since the other three nonlinear
equations have no relationship with geometry constraints, they are
independent from the ﬁrst six linear equations.

From the constraints formulation, it is obvious to show that the

six linear equations can be expressed as the following form

(cid:62),r(cid:62)

A X = B,

(8)
3 ](cid:62) is the vector of unknowns with CtL =
where X = [CtL
[t1,t2,t3](cid:62), r1 = [r11,r21,r31](cid:62) and r3 = [r13,r23,r33](cid:62), B is the
distance vector denoted as B = [0,0,d1,d1,d2,d2](cid:62), and A is the
coefﬁcient matrix deﬁned as follows:

1 ,r(cid:62)

b1
b2
b3
b3
b4
b4

c1
c2
c3
c3
c4
c4

a1x1
a2x2
a3x1
a3x3
a4x2
a4x3

b1x1
b2x2
b3x1
b3x3
b4x2
b4x3

c1x1
c2x2
c3x1
c3x3
c4x2
c4x3

a1z1
a2z2
a3z1
a3z3
a4z2
a4z3

b1z1
b2z2
b3z1
b3z3
b4z2
b4z3

 ,

c1z1
c2z2
c3z1
c3z3
c4z2
c4z3



a1
a2
a3
a3
a4
a4

A =

(9)
in which the unit norm vectors Cni for i = {1,2,3,4} and laser
points Lp j for j = {1,2,3} are replaced with their corresponding

4

components as deﬁned in Section 4.1. As proved in Appendix A,
three unit vectors Cn1, Cn2 and Cn3 are linearly independent,
which means they can span the whole 3D space such that unit
vector Cn4 can be expressed as the combination of ﬁrst three unit
vectors Cn4 = u·C n1 + v·C n2 + w·C n3, where u, v and w are the
coefﬁcients. This allows us to perform Gaussian elimination to A :
• Keep Row1 ← row1, Row2 ← row2 and let Row3 ← row4;
• Let Row4 ← row3 − row4 and Row5 ← row6 − row5;
• Let Row6 ← row5 − (u· row1 + v· row2 + w· row3),

where row1,..., row6 are the six rows of A while Row1,..., Row6
are the six rows of the matrix after Gaussian elimination arranging
from both sides of Equation (8). Here, the transformed matrix Λ
becomes

with sub-matrices Rα, Rβ , Lα and Lβ where

Lα
03×3 Rβ|Lβ

(cid:20) Rα
 , Rβ =

(cid:21)
a3

Λ =

b1
b2
b3

c1
c2
c3

b3
b4
b1

a4
a1

b1x1
b2x2
b3x3

c1x1
c2x2
c3x3

a1z1
a2z2
a3z3

b1z1
b2z2
b3z3

b3kγ
b4kβ

c3
c4
c1

 ,
 ,
 ,

c1z1
c2z2
c3z3
c3kγ
c4kβ

Rα =

a2
a3

a1
a1x1


a2x2
a3x3

Lα =

Lβ =

(10)

(11)

(12)

a3kγ
a4kβ

a1kα + a3kδ
z1 − z2
x1 − x2

b1kα + b3kδ
z2 − z3
x2 − x3

c1kα + c3kδ
z3 − z1
x3 − x1

(cid:104) w

(cid:105)
· a3(kα − kγ )

u

, kγ =

, kβ =

with kα =

and kδ =
. Since laser features Lp1, Lp2 and Lp3 are
extracted from two distinct line segments, their XL coordinates
cannot be equal otherwise these three points are on a same plane
from an invalid observation. Therefore, kα, kβ and kγ can be
calculated. After Gaussian elimination, the distance vector B is
transformed into a new vector denoted as Π = [0,0,d1,0,0, ˆd](cid:62),
where ˆd =

.

d2 − wd1
u(x2 − x1)

Now, we have reduced the coefﬁcient matrix A to matrix Λ.
First, let us take a close look at the structure of Λ. Since we
know that unit vectors Cn1, Cn2 and Cn3 are linearly independent
(Appendix A), matrix Rα is non-singular such that we can
reduce it to an upper triangular matrix. Thus, the ﬁrst three linear
equations are independent. Next, the unit vectors Cn1, Cn3 and
Cn4 are also linearly independent as proved in Appendix A. Then,
matrix Rβ is also non-singular and can be reduced into an upper
triangular matrix, which means the last three linear equations are
also independent. From the procedure above, we just reduce the Λ
to a matrix which has a lower triangular corner with zero elements,
just shown as the following

(cid:20)(cid:53)3×3 (cid:3)3×3 (cid:3)3×3

(cid:21)

,

03×3 (cid:53)3×3 (cid:3)3×3

(13)
where (cid:53) represents a 3 × 3 upper triangular matrix and (cid:3)
represents a 3 × 3 square matrix. So from the matrix structure
in Equation (13), we can conclude that the six linear equations
for geometry constraints are linearly independent, which means
plus the other three nonlinear equations we can solve for nine
unknown components in r1, r3 and CtL respectively. Then, r2 can
be retrieved by cross product of r3 and r1. Hence, we prove that

Λ →

there is no ambiguity in our proposed method since a solution
is obtained just from a single observation of the calibration
target, which means the relative pose between camera and LRF
(cid:4)
is determined.

5 SOLUTION
In this section, we ﬁrst present how to obtain the solution for the
extrinsic calibration of the camera-LRF system from just a single
observation of the calibration target. Then, we show the solution
from multiple observations which is needed to reduce the effect
of noise based on weighted parametric optimization. Note that a
closed-from solution can be obtained in our constraints system as
derived in [18]. However, since a real solution is not guaranteed
in practice due to noise, we propose a new strategy for solving
the constraints system. The advantage of our method is that it
reduces the total number of observations to obtain accurate results
as veriﬁed in Section 7.2.

5.1 From a Single Observation
Now we present the solution to the polynomial system (Equa-
tions (3)-(7)). Since nonlinear constrains are quadratic, it is not
surprising that there are eight possible solutions. So wrong solu-
tions need to be eliminated. First, we brieﬂy outline the steps to
eliminate six variables. We then solve a set of quadratic equations
and then back substitute:

2.

1. Transform the six linear equations into the objective
function for a nonlinear optimization problem over CtL
to symbolically solve for the translation vector, and
substitute it into linear equations;
Symbolically solve the six linear equations for r1 and
substitute it into remaining three nonlinear equations;
Solve three quadratic equations for r3;
Substitute r3 into the symbolic solution for r1;

3.
4.
5. Back-substitute r1 and r3 into the symbolic solution for

CtL and calculate r2.

Now let us explain the solution in detail. In practice, the
geometry constraints (3), (4) and (5) for a single observation
are affected by noise such that they do not hold exactly. Thus,
there are residuals between the two sides of equations, whose
solution can be treated as a nonlinear optimization problem in
which we choose the optimal solution for C
LR and CtL to minimize
the sum of the residuals from the six linear geometry constraints.
For convenience, we ﬁrst redeﬁne the parameters as shown below

Cni = Cni,

Cn j = Cn3,
Cnk = Cn4,

i = 1,2
j = 3,4
k = 5,6

,

(14)

.

(15)

di = 0,

d j = d1,
dk = d2,

i = 1,2
j = 3,4
k = 5,6

,

i = 1,3
j = 2,5
k = 3,6

Lp j = Lp2,
Lpk = Lp3,

Lpi = Lp1,
(cid:1) = di,
(cid:0)C
LR· Lpi + CtL
(cid:16)Cn(cid:62)
(cid:0)C
LR· Lpi + CtL
det(cid:0)C
LR(cid:1) = 1

LR = I,

∑

i=1

N

i

arg min
C
L R,CtL
s. t. C

J =
LR(cid:62) · C

Then the geometry constraints are reformulated as the following
(16)
STEP 1: The problem is reformulated in the view of nonlinear
optimization as shown below

i = 1,2, ...,6.

Cn(cid:62)
i

(cid:17)2

(cid:1)− di

,

(17)

5

(cid:17)

,

(18)

where N = 6 and the constraints for rotation matrix are equivalent
to Equation (7).

From the reformulated problem (17), it is obvious that CtL
is not involved in the rotation matrix constraints, and the cost
function is quadratic over CtL. Thus, CtL can be expressed in terms
LR(cid:62). Speciﬁcally, the optimal solution for CtL is obtained by
of C
minimizing the cost function J in problem (17) as shown below

(cid:104)Cn(cid:62)

i

(cid:0)C
LR·L pi + CtL
(cid:33)−1
(cid:16)

· N
∑

N

2

∑

(cid:32) N

i=1

=

∂ J
∂CtL
⇒ CtL =

(cid:1)− di

(cid:105)Cni = 0

∑

Cni

Cn(cid:62)
i

Cni − Cni

Cn(cid:62)
i

di

C
LRLpi

i=1

Cni

Cn(cid:62)
i .

i=1
where N0 = ∑N
i=1
Lemma 1. The matrix N0 is non-singular such that it is invertible.
The proof of Lemma 1 is presented in Appendix B. Since for
a laser point Lpi = [xi,0,zi](cid:62), we arrange the expression of CtL
in (18) to the form

CtL = N −1
i=1 di

(DN − N1r1 − N3r3)
Cn(cid:62)
i xi

0
Cni, N1 = ∑N
i=1

Cni

where DN = ∑N
∑N
i=1
STEP 2: We ﬁrst deﬁne a series of matrices for simplicity

Cn(cid:62)

i zi.

Cni

 , Nz =

x1

xN

Cn(cid:62)
1
...
Cn(cid:62)
N

 , N =

z1

zN

Cn(cid:62)
1
...
Cn(cid:62)
N

 , D =

Cn(cid:62)

...
Cn(cid:62)
N

1

Nx =

(19)

and N3 =

 .

d1

...
dN

(20)

Then we substitute (18) in constraints (16) and obtain

Gxr1 + Gzr3 = Gd,

(21)

where Gx = Nx − N N −1
D − N N −1
rows greater than columns, we further express r1 in terms of r3

N3 and Gd =
DN. Since Gx is a thin matrix with the number of

N1, Gz = Nz − N N −1

0

0

0

where H = −(cid:0)G (cid:62)

x Gx

(cid:1)−1 G (cid:62)

r1 = H r3 + K ,

x Gz and K =(cid:0)G (cid:62)

x Gx

(cid:1)−1 G (cid:62)

x Gd.

STEP 3: Now we can eliminate r1 by substituting (22) in the three
remaining second order constraints (7). After full expansion, we
have the following

(22)

(23)

(24)

e11r2

13 + e12r13r23 + e13r2

23 + e14r13r33 + e15r23r33

+ e16r2

33 + e17r13 + e18r23 + e19r33 + m = 0,

e21r2

13 + e22r13r23 + e23r2
+ e26r2
33 − 1 = 0,

23 + e24r13r33 + e25r23r33
33 + e27r13 + e28r23 + e29r33 = 0,

23 + r2

r2
13 + r2
(25)
where for i = {1,2} and j = {1,2, ...,9}, the coefﬁcients ei j and
the constant m are computed in a closed form in terms of the
components of H and K ; and ri3 for i = {1,2,3} are the three
elements of r3.

We now focus on the last three nonlinear equations which are
used for solving for r3. In the literature, there are two methods
for solving this system in closed-form. The ﬁrst one is to obtain
a uni-variate polynomial in r33 using the Macaulay resultant [22]
of these three equations. Naroditsky et al. presented a solution
using this technique in [18]. They also noted that there could be
no real solution due to noise in feature extraction. The second

23 +r2

13 +r2

23 +r2

13 + r2

23 + r2

method, presented in [19], is based on an analytical least-squares
solution [19] using eigenvalue decomposition. Either one of these
methods can be used to obtain an analytical solution. For com-
pleteness, in Appendix C, we show how to transform our problem
to the form in [19]. However, since both approaches can fail to
deliver a real solution in the presence of noise, we also give a prac-
tical method based on a non-linear solver. Speciﬁcally, we used
the MATLAB function f mincon to solve for r3 in these quadratic
33 − 1 in Equation (25) is a quadratic
equations. Since r2
33−1 using
function over r13, r23 and r33, we minimize r2
sequential quadratic programming method [23] subject to two
equality constraints (23) and (24) and one inequality constraint
33 ≥ 1. The initial value for r3 is chosen to be [0,0,1](cid:62),
r2
13 +r2
since in practice the camera and LRF are close connected to each
and their z axes both face toward the same direction. It means the
dot product of the unit vectors in their own z direction is larger than
zero and nearly one. Without loss of generality, the initial value
for r3 is straightforward to be set based on the practical setting of
X, Y , or Z axes between camera and LRF. Since our method does
not induce any spatial ambiguity, the initial value here is just a
direction reference for the nonlinear solver to avoid obtaining the
mirror solution (e.g. the LRF is behind the calibration target) and
no further speciﬁc requirement for the initial value is needed.
STEP 4: After obtaining r3, r1 can be calculated from Equa-
tion (22) and r2 can be retrieved from Equation (6), which means
C
LR is determined.
STEP 5: Then CtL can be obtained using Equation (18).

N

M

(cid:16)Cn(cid:62)

(cid:0)C
LR· Lpi j + CtL

5.2 From Multiple Observations
In order to suppress the effect of noise during measurements, it
is reasonable to use multiple observations of the calibration target
from different views to obtain C
LR and CtL. Different from (17), the
problem for using multiple observations is formulated as following
LR ∈ SO(3),
(26)
where N = 6 and M is the total number of different multiple ob-
servations. The parameters in every six of constraints are deﬁned
as in (14) and (15). Thus, the procedure of obtaining the solution
from multiple observations does not change, just as from (18) to
(25).

(cid:1)− di j

arg min
C
L R,CtL

(cid:17)2

s. t. C

∑

∑

j=1

i=1

i j

LP3 and line LP3

In practice, some laser feature points are extracted more
accurately and thus more reliable than the others. So the laser
LP2 can be used in constraints
points from line LP1
for each observation (e.g. points from LP1
LP3 must lie on the
plane T3). Further, different weights are applied to the residual of
each linear constraint: we apply more weight on smaller residual.
LR and CtL are optimized by
Then, the extrinsic parameters C
minimizing the weighted distance between the laser points and
their corresponding planes. The optimization problem based on
weighted feature data is formulated as follows:

(cid:32) N
∑
(cid:0)C
LR ∈ SO(3)
LR· Lpi j + CtL
where Di j = Cn(cid:62)
LP3 and D(cid:48)
points of line LP1
the distance residuals from these points Lp(cid:48)

(cid:33)
(cid:1)2
(cid:1)− di j; N(cid:48) is the number of laser
(cid:1) − d1i is

(cid:0)C
LR· Lp(cid:48)

arg min
C
L R,CtL
s. t. C

wi j (Di j)2 +

ik = Cn(cid:62)

(cid:0)D(cid:48)(cid:48)

ik to plane T3 for ith

(cid:0)D(cid:48)

ik + CtL

(cid:1)2

N(cid:48)(cid:48)
∑

N(cid:48)
∑

w(cid:48)
ik

w(cid:48)(cid:48)
il

(27)

∑

k=1

l=1

i=1

j=1

ik

+

il

,

M

3i

i j

il + CtL

6
LP2

observation; N(cid:48)(cid:48) is the number of laser points of line LP3
il = Cn(cid:62)
and D(cid:48)(cid:48)
4i
these points Lp(cid:48)(cid:48)

(cid:0)C
LR· Lp(cid:48)(cid:48)
il to plane T4for ith observation.

(cid:1)−d2i is the distance residuals from

Since the constraints for laser edge points and the ones for
laser line points are treated equally here, the weights in (27) are
calculated based on the following strategy: we ﬁrst calculate the
sum of the residuals Di j in constraints for laser edge points S =
i=1 ∑N
∑M
j=1 Di j and apply the weight wi j = 1/ (Di jS) to each of
them; then we calculate the sum of the residuals D(cid:48)
il in
l=1 D(cid:48)(cid:48)
constraints for laser line points S(cid:48) = ∑M
il
and respectively apply the weights w(cid:48)
il =
il. The optimization problem in (27) is
solved using Levenberg-Marquardt (LM) method [24] [25] with
the accurate initial value from our solution in (26) which is more
robust than simply using least square solution in [13] for start-up.

ikS(cid:48)(cid:1) and w(cid:48)(cid:48)

ik = 1/(cid:0)D(cid:48)

ilS(cid:48)(cid:1) to D(cid:48)

ik and D(cid:48)(cid:48)
i=1 ∑N(cid:48)(cid:48)

1/(cid:0)D(cid:48)(cid:48)

ik and D(cid:48)(cid:48)

i=1 ∑N(cid:48)

k=1 D(cid:48)

ik +∑M

6 DETAILS OF THE CALIBRATION APPROACH
In this section, we explain how to extract the features required for
our method. These are: four normal vectors Cni with i = {1,2,3,4}
and two distances d j with j = {1,2}) from the camera, and three
laser edge points Lpk with k = {1,2,3}) from the LRF. We also
apply a detection strategy to each sensor for additional accuracy.

6.1 Data from the Camera
From the camera, we need to calculate the unit normal vectors of
the planes (T1, T2, T3 and T4) and two distances from the camera
to the corresponding planes (T3 and T4). We ﬁrst perform camera
calibration using MATLAB Camera Calibration Toolbox [21].
Then three corners P, Q and R of the target need to be detected
from the image. While the corners can be detected directly, for
accuracy we place a small unit of checker board (4 squares) on
each corner and let the center of the 4-squares unit be coincident
with the corner. Then three corners are accurately detected in sub-
pixel level using Harris corner detector [26], as shown in Fig. 4.

As is well-known, image-based point features are correspond-
ing lines in 3D camera frame with origin C; and image-based line
features are planes in 3D camera frame (Fig. 3). Since the intrinsic
−→
matrix K is known, the unit vectors of line directions
CQ and
−→
CR in camera frame are obtained as nCP, nCQ and nCR with:

−→
CP,

nCI =

vCI(cid:107)vCI(cid:107) ,

vCI ∼ K−1 · i ˜pI,

(28)

−→
where, for I = {P,Q,R}, vCI is the vector of line direction
CI, and
i ˜pI denotes the homogeneous pixel coordinate of point I from the
undistorted image, as in Equation (2). Then, two normal vectors
Cn1 and Cn2 of the planes T1 and T2 are calculated respectively as
following

Cn1 =

vCP × vCQ
(cid:107)vCP × vCQ(cid:107) ,

Cn2 =

vCR × vCP
(cid:107)vCR × vCP(cid:107) .

(29)

The other two normal vectors Cn3 and Cn4 of planes T3 and T4
can be obtained directly from the calibration toolbox. Speciﬁcally,
for each observation of each plane (T3 or T4) during the camera
calibration, we assume that the checker board is on the plane
Zw = 0 in the world frame. The normal vector for the plane of
checkerboard is just the 3 × 1 vector Cni in camera frame for
i = {3,4}, and it points back to the camera side. Then, it is obvious
that the normal vector Cni is minus the 3rd column of rotation
matrix which represents the orientation of the checkerboard with
respect to (w.r.t) to camera. This rotation matrix can be obtained

from the parameter Rc j of the calibration result, where j is
the image index during camera calibration process. Next, two
distances d1 and d2 from the camera origin C to the planes T3
and T4 respectively are calculated as following

d1 = Cn(cid:62)

3 · Ct1,

d2 = Cn(cid:62)

4 · Ct2,

(30)
where, for i = {1,2}, Cti is the 3 × 3 translation vector which
represents the position of the corresponding checkerboard w.r.t to
the camera (see Fig. 4). It can be obtained from the parameter
Tc j of the calibration result, where j is the image index for
camera calibration. Fig. 3 shows the directions of these four
feature vectors extracted from camera.

Figure 4. Calibration methodology in practice. Upper Left: The strategy
in laser edge point detection using a planar strip on each side for
additional accuracy; Upper Right: The strategy in corner detection suing
unit checker board (4 squares) for additional accuracy; Lower Left: The
geometry illustration for calculation of the distance d1 from the camera
center to the plane T3; Lower Right: The strategy in laser edge point
detection using virtual edge point for additional accuracy.

6.2 Data from the LRF
The feature points extracted from LRF are the left and right edge
points Lp1, Lp2 and the center point Lp3, which are intersections
of the scanning plane with the edges PQ, PR and PO, respectively
(See Fig. 3). The intersection between laser scanning plane and the
calibration target are two line segments. Since laser beams cannot
be seen from camera, the edge ending points of the laser scan on
the left and right sides (Lp(cid:48)
2) can be detected based on depth
discontinuity. Next, the laser intersection can be segmented into
two lines (e.g. with the Ramer-Douglas-Peucker algorithm (RDP)
algorithm [27]). For each segment, we ﬁt a line to it using the total
least squares method [28]. Then the intersection of these two lines
is calculated as the center point Lp3.

1, Lp(cid:48)

To accurately estimate the location of the edge points, a
median ﬁlter is used to process the laser measurements. For
additional accuracy, the calibration target can be modiﬁed to
include two planar strips which are respectively attached to left
and right borders, and faces towards outside. Then, for each side,
the edge point can be obtained as the intersection of two adjacent
lines, as shown in Fig. 4. For some views in which the planar strip
cannot show up, we use a different strategy stated in [17]. Since the
laser scanning beams can be sparse especially at a long distance

7

from the LRF, the left and right edge points may stay far away
from the true edges of the calibration target. Therefore, we use
the virtual edge points placed at the expected location of the true
edges of the target. It assumes that the true edges are uniformly
distributed between the last laser point and the next consecutive
one. Then, based on the location of points Lp(cid:48)
2, the left and
right edge points Lp1 and Lp2 for use are computed respectively
by selecting the intersection point of the resolution-angle-bisector
(θ /2) and the corresponding ﬁtted line.

1 and Lp(cid:48)

7 EXPERIMENT
In this section, we ﬁrst validate the correctness and numerical
stability of our solution and explore its sensitivity to noise using
synthetic data from a single observation of the calibration target.
We also present the result of noise reduction for our solution
using multiple different observations. Finally, a series of real
experiments are performed to further validate our method in
comparison with other two existing methods.

7.1 Synthetic Experiment
For the simulation setting, the obtuse angle between two triangular
boards of the calibration target is set to 150◦. Based on the
practical application as explained in Section 5.1, we assume that
the camera and LRF are connected close to each other and their
Z axes face towards the calibration target, which means we can
choose [0,0,1](cid:62) to be the initial value for solving Equations (23)-
(25). Then, we uniformly and randomly generate, roll, pitch and
yaw in the range of ±45◦ for the orientation of LRF w.r.t the
camera, and the components of position vector from 5 to 30 cm.
For each instance of the ground truth, we randomly generate the
orientation and position of the calibration target within the range of
±45◦ and 50 to 150 cm. Speciﬁcally, the valid poses of the target
are chosen such that it ﬁrst has three intersection points with laser
scanning plane, which respectively lie on the corresponding lines
PQ, PR and PO. Besides that, the checkerboard of calibration
target must face to the camera-LRF pair, which closely models
what happens in practice to avoid degenerate cases. Then, three
feature points Lp1, Lp2 and Lp3 are calculated from the intersection
between the laser plane and three lines of the calibration target;
the normal vector and four planes (T1, T2, T3 and T4) are deﬁned
according to the known corners of the target; and two distances
(d1 and d2) are obtained from known orientation and position of
the checkerboards w.r.t the camera.

The ﬁrst simulation aims to validate the numerical stability of
our proposed solution, in which 104 Monte-Carlo trials performed.
For each trial, only one observation of the calibration target is
needed in the case of noise-free measurements. The histogram of
errors for such 104 tials is shown in Fig. 5. And the error metric
here is the following

error = (cid:107)[Rgt|tgt ]− [Rcp|tcp](cid:107)F ,

(31)
where Rgt and tgt are the ground truth for the orientation and
position of LRF w.r.t camera, Rcp and tcp are corresponding ones
computed by our solution, and (cid:107)·(cid:107)F is the Frobenius norm. Due to
the randomness of the orientation and position between calibration
target and camera-LRF pair, the computational error varies but the
accuracy is still in a high level (around 10−8). Fig. 5 demonstrates
that our solution correctly solves the problem for the camera-LRF
calibration, which further validates the constraints we built are
sufﬁcient for a single observation.

8

Figure 6. Errors in estimated rotation and translation for simulated
camera-LRF rigs as a function of laser depth noise with different levels
of image noise. Each point represents the median error for 1000 Monte-
Carlo trials. Each line corresponds to a different level of image noise
versus laser depth noise changed by their own standard deviation.

Figure 7. Mean and Standard deviation of errors for estimated rotation
and translation versus the number of observations of the calibration
target in 1000 Monte-Carlo trials with laser depth noise changed by its
standard deviation. The standard deviation for image noise is set to 3
pixels.

The last simulation is designed for testing the noise reduction
of our solution when using multiple observations of the calibration
target. Fig. 7 shows the effect of the number of observations for
noise reduction, where the standard deviation of camera noise is
set to 3 pixels while the standard deviation of LRF noise varies
from 1 mm to 10 mm. For each noise setting, the mean and
standard deviation of 1000 Monte-Carlo trials are represented by a
single point. We observe that as number of observations increases,
the mean and standard deviation of errors for both rotation
and translation decrease asymptotically. Moreover, with a small
number of observations, our method can achieve a highly accurate
initial value for further weighted optimization. Speciﬁcally, the
errors for rotation and translation are respectively around 0.5◦ and
5 mm for only 5 different observations.

Figure 5. The histogram of computational errors for computed rotation
and translation in 104 random and noise-free Monte-Carlo trials.

The next simulation tests our solution in terms of the sensitiv-
ity to measurement noise in feature data. We assume that camera
calibration has been performed accurately using our calibration
target such that two normal vectors Cn3 and Cn4, and two distances
d1 and d2 are known exactly. Then two sources of error are taken
into account: depth uncertainty in laser points along the beams
direction and pixel uncertainty in detection of the calibration
target’s corners from image. Both of these lead to the feature laser
points being some distance off their corresponding planes (from
T1 to T4). For the LRF depth error, the Hokuyo device speciﬁes the
standard deviation of 10 mm for ranges less than 1 m, while up to
1% of the range from 1 m to 4 m. Hence, for laser measurements,
we set the noise standard deviation to vary from 0 mm to 10
mm along the directions of laser beams. For corner detection, the
accuracy for image processing is in pixels from images, each of
which has 640× 480 pixels. Since corner detection error mainly
depends on the pose of the calibration target in practice, the noise
standard deviation is set within the range of 0 to 3 pixels. In order
to combine the noise information from both sensors in one plot,
we induce a variable factor proportionality kσ from 0 to 1, which
is multiplied by the standard deviation of each sensor, i.e. σC = 1
pixel for camera and σL = 10 mm for LRF. The noise standard
deviation for each sensor is shown as follows

ˆσI = kσ σI,

(32)
where ˆσI varies for use as kσ changes with I = {C,L} for the
camera and laser, respectively. The number of Monte-Carlo trials
is set to be 1000, each of which only needs one observation of the
calibration target. The error metrics for rotation and translation
employed here are the following

(cid:18)(cid:107)Rcp − Rgt(cid:107)F

(cid:19)

eR = 2arcsin

√
2

2

et = (cid:107)tcp − tgt(cid:107)2 ,

,

(33)

where eR represents the angular error between two rotations in
radians and et is the Euclidean distance between two translations
in meters. Fig. 6 shows the rotation and translation errors for
different levels, and demonstrates that from a single observation
our solution is not sensitive to the image noise but has a greater
sensitivity to laser depth noise.

Laser Depth Noise std. dev. (mm)012345678910Med. Rotation Error (deg)00.511.522.50 pixel1 pixel2 pixel3 pixelLaser Depth Noise std. dev. (mm)012345678910Med. Translation Error (mm)0510152025300 pixel1 pixel2 pixel3 pixelNumber of Observations012345678910Mean & Std Error (deg)-1012345Mean and Standard Deviation of Rotation ErrorSigma = 1 mmSigma = 4 mmSigma = 7 mmSigma = 10mmNumber of Observations012345678910Mean & Std Error (mm)-100102030405060Mean and Standard Deviation of Translation ErrorSigma = 1 mmSigma = 4 mmSigma = 7 mmSigma = 10mm(cid:20) Cr
(cid:20) Rerr

(cid:21)
(cid:21)

7.2 Real Experiment
We performed experiments with real data to further evaluate
our algorithm. In our experiment, a LRF Hokuyo URG-04LX is
rigidly mounted on a stereo rig which consists a pair of Point
Grey Chameleon CMLN-13S2C cameras (see Fig. 1). The LRF
has 180◦ horizontal ﬁeld of view, with an angular resolution of
0.36◦ and a line scanning frequency set to 10 Hz. Its scanning
accuracy is ±1 cm within a range from 2 cm to 100 cm, and has
1% error for a range from 100 cm to 400 cm. The cameras have a
resolution of 640×480 pixels, and are calibrated using the method
in [29]. The images prior to feature extraction are warped to get
rid off the radial and tangent distortions. Based on time stamp, the
sensors are synchronized such that each laser corresponds to a pair
of stereo images.

In order to objectively evaluate our method, we compare it
to two state-of-the-art algorithms [13] and [17] using the ground
truth from the baseline of stereo rig. Speciﬁcally, for each method,
the LRF is ﬁrst calibrated w.r.t
left and right cameras such
that we can compute the relative pose (baseline) between stereo
cameras and compare it with the result from the camera calibration
toolbox [21], which is considered as the ground truth. Let Cr
Cl T
represent the transformation (rotation and translation) from left
camera to right camera obtained from the calibration toolbox, and
similarly let Cl
L T respectively represent the transformation
from LRF to left and right cameras, as shown below
L R CitL
0 0 0
1

Cl R CrtCl
1
0 0 0

L T and Cr

Cr
Cl T =

Ci
L T =

(34)

,

where i = {c,l}. Then, the error metric is deﬁned as follow

(cid:20) Ci
(cid:21)
L T · (cid:0)Cr
L T(cid:1)−1

,

terr
1

Cl T · Cl

,

= Cr

Terr =

0 0 0

(35)
where Rerr would be close to identity and (cid:107)terr(cid:107)2 close to zero
if the given camera-LRF calibration method is ideally accurate.
Here, the stereo cameras calibration are performed for 10 times,
and each time 20 image pairs are randomly chosen for use
from a pool of 40 pairs. The rotation is represented as a 3 × 1
vector by Rodrigues’ formula. The mean values of the rotation
and translation are respectively [0.0061,0.0118,−0.0032](cid:62) and
[−96.0505,−0.3035,−0.1326](cid:62) in mm, which means the rotation
angle is 0.0137◦ and the distance of baseline is 96.0511 mm.
The standard deviations are 0.0018◦ and 0.2742 mm respectively.
Since the standard deviations are low (0.01◦ and 1 mm), we treat
the mean values of rotation and translation as ground truth. The
distances between the LRF and the stereo cameras in our rig are
approximately 100 mm and 150 mm.

In the ﬁrst experiment, 30 best observations from each method
are obtained from a RANSAC framework which selects inliers
based on 5 observations at each iteration from 50 in total. We
randomly select subsets of 1, 5, 10,15 and 20 in 30 observations
and perform the calibration between LRF and stereo cameras
using the feature data extracted from each subset. This process
is repeated 10 times for each method. From (33), the rotation error
and translation error in real experiment are respectively calculated
by comparing Rerr with identity I3×3 and comparing terr with
03×1. As shown in Fig. 8, our method has the smallest mean
and standard deviation for both rotation and translation error as
the number of observations increases. Especially when using 20
observations, the mean errors are respectively 0.3◦ and 3.4 mm,
which are almost three times lower than Zhang’s (1.3◦ and 12.0
mm) and Kwak’s (1.0◦ and 12.6 mm). Moreover, our method can

9

obtain a reasonable result even using only one observation, which
is impossible for the other two methods. Thus, we can conclude
from the results that our method builds sufﬁcient constraints for
each observation such that it outperforms the previous methods
using the same number of observations. In other words, our
method can achieve an accuracy within the same level but using
the smallest number of observations.

Figure 9. The back-projection of the laser points (red) on the calibration
target to the images from both left and right cameras.

Next, we perform another experiment in which we observe
the back-projection of laser scanning line on the calibration
target from both left and right images. This experiment aims to
respectively test the calibration results from stereo cameras (LRF
to left camera and LRF to right camera) obtained using our method
(see Fig. 9). Here, the calibration results are calculated by using
the data from 20 observations randomly chosen from 30 different
observations, and then are tested by the stereo images of one
observations randomly chosen from the other 10 observations. We
can observe that the laser scanning line reasonably matches the
calibration target from both left and right cameras, which validates
the correctness of our calibration result for each camera-LRF pair.

8 CONCLUSION

In this paper, we presented a novel method for calibrating the
extrinsic parameters of a system of a camera and a 2D LRF.
The proposed method is based on observing a special V-shaped
calibration target formed by two non-coplanar triangles sharing
a common side. In contrast to existing methods, our coplanarity
constraints for feature data sufﬁces to unambiguously determine
the relative pose between these two sensors even from a single
observation. It means that the number of observations can be
reduced for an accurate result. Our solution technique can also be
extended to the case of multiple observations due to noise effect
and serve as an accurate initial for further weighted optimization.
We demonstrated the accuracy and robustness of our solution
using synthetic data. The real experiment further veriﬁed that our
method achieves more accurate and stable result compared with
two typical previous methods.

10

Figure 8. Comparisons with Zhangs’ method [13] and Kwak’s method [17]. Upper: Mean errors of estimated rotation and translation for the three
methods as the number of different observations of the calibration target increases; Lower: Errors standard deviation of estimated rotation and
translation errors for the three methods as the number of different observations of the calibration target increases. The experiment for each method
is performed for 10 times using its own best 30 observations using RANSAC.

APPENDIX A
PROOF OF INDEPENDENCY AMONG NORMAL VEC-
TORS ASOCCIATED WITH THE CALIBRATION TAR-
GET

From Fig. 3, Cni is the normal vector of plane Ti for i = 1,2,3,4
and we claim that these normal vectors in any cardinality three
subset of {Cn1,Cn2,Cn3,Cn4} are linearly independent. It is obvi-
ous that there are totally four subsets: I. Cn1, Cn2 and Cn3; II. Cn1,
Cn2 and Cn4; III. Cn1, Cn3 and Cn4; IV. Cn2, Cn3 and Cn4. We will
prove separately for each subset and show that subsets I and II are
symmetric arguments, and subsets III and IV are also symmetric
arguments.

According to the geometry setting in Fig. 3, we notice that
for each subset three different planes have a common intersection
point P, which means there is no parallelism between them. We let
l12, l13, l23 and l34 respectively denote the directional unit vectors
of the lines PC, PQ, PR and PO w.r.t the camera frame.

So let us ﬁrst prove the claim for subset I: Cn1, Cn2 and Cn3.
We assume that these three normal vectors are linearly dependent,
which means there exists three nonzero coefﬁcients α, β and γ
such that

otherwise two of three planes would have parallelism (e.g. α = 0
such that βCn2 = −γCn3) or one plane of them reduces to
nonexistence (e.g. α = β = 0 such that γCn3 = 0). Thus, Cn3
can be represented as the combination of n1 and n2. Since the
intersecting line of T1 and T2 is PC, we have the following

⇒ l(cid:62)

12 · Cn3 = 0.

(37)

(cid:40)l(cid:62)

12 · Cn1 = 0
12 · Cn2 = 0
l(cid:62)

It means that the line PC is on the plane T3 given the fact that
they share a common point P. It is a contradiction unless camera
center C is also on the plane T3, which is a useless case since
camera cannot capture the checkerboard on T3. Thus, the normal
vectors Cn1, Cn2 and Cn3 are linearly independent. It is similar for
subset II that we would have a contradiction that the line PC is on
12 · Cn4 = 0 if Cn1, Cn2 and Cn4 are linear
the plane T4 based on l(cid:62)
dependent. So we can conclude that these vectors in both subset I
and II are linearly independent.

Next let us focus on the claim for subset III: Cn1, Cn3 and Cn4.
We assume that these three normal vectors are linearly dependent,
which means there exists three nonzero coefﬁcients α, β and γ as
explained in subset I such that

αCn1 + βCn2 + γCn3 = 0,

(36)

αCn1 + βCn3 + γCn4 = 0.

(38)

ZhangKwakOur MethodMean of Rotation Error (deg)00.511.522.533.544.5 1 Obsrv 5 Obsrv10 Obsrv15 Obsrv20 ObsrvZhangKwakOur MethodMean of Translation Error (mm)051015202530354045 1 Obsrv 5 Obsrv10 Obsrv15 Obsrv20 ObsrvZhangKwakOur MethodStd. of Rotation Error (deg)00.511.522.5 1 Obsrv 5 Obsrv10 Obsrv15 Obsrv20 ObsrvZhangKwakOur MethodStd. of Translation Error (mm)05101520253035 1 Obsrv 5 Obsrv10 Obsrv15 Obsrv20 ObsrvThus, Cn1 can be represented as the combination of n3 and n4.
Since the intersecting line of T3 and T4 is PO, we have the
following

(cid:34)

⇒ l(cid:62)

34 · Cn1 = 0.

J =

(39)

N

∑

i=1

Cn(cid:62)
i

C
LRLpi

(cid:40)l(cid:62)

34 · Cn3 = 0
34 · Cn4 = 0
l(cid:62)

11
We substitute CtL in (18) into the cost function J in (17), then

the cost function becomes:

(cid:16)

d j

Cn j − Cn j

Cn(cid:62)
j

(cid:34)

Cn(cid:62)
i

N

∑

i=1

+ Cn(cid:62)

0

i N −1

(cid:32) N

N

∑

j=1
Cn(cid:62)

∑

j=1

C

LRLpi −

i N −1

Cn j

Cn(cid:62)
j

C
LRLp j

+

Cn(cid:62)

i N −1
0 d j

Cn j

(cid:32)

Cn(cid:62)
i

C
LRLpi +

N

∑

i=1

N

∑

j=1

g(cid:62)
i j

C
LRLpi + fi

,

0

(cid:32) N

∑

j=1

(cid:33)2

(cid:16)

C
LRLp j

(cid:33)

(cid:35)2

(cid:17)− di
(cid:35)2
(cid:33)

− di

(43)

(cid:17)− di

Cni, fi =

j N −1
where gi j = −Cn j
Cn(cid:62)
Cn(cid:62)
and N0 = ∑N
Cni
i=1
i
problem becomes to minimize J with respect to C
quaternion representation C
simplify the cost function J.

0
is nonsingular and symmetric. Then the
LR. We use the
LR to further

Lq for the rotation matrix C

i N −1
0 d j

Cn(cid:62)

Cn j

∑N

j=1

Before that, we need to introduce some properties about
quaternion representation. Given any two quarternions q1 and q2,
the quaternion multiplication ⊗ is deﬁned as follows:

q1 ⊗ q2 = L (q1)q2 = R (q2)q1,

(44)

with L (·) and R(·) deﬁned below

 q4 −q3
 q4

q2
q4 −q1
q3
−q2
q1
q4
−q1 −q2 −q3
q3 −q2
−q3
q1
q4
q2 −q1
q4
−q1 −q2 −q3




,

q1
q2
q3
q4
q1
q2
q3
q4

(45)

L (q) =

R (q) =

where q = [q1,q2,q3,q4](cid:62) and it satisﬁes q(cid:62)q = 1. We further
have

L(cid:0)q−1(cid:1) = L (cid:62) (q)

R(cid:0)q−1(cid:1) = R(cid:62) (q) ,

(46)

where q−1 = [−q1,−q2,−q3,q4](cid:62). Then a product pr = Rp, a
rotation matrix R with quaternion representation q times a vector
p, can be rewritten as

¯pr = q⊗ ¯p⊗ q−1,

(47)

where ¯pr and ¯p are respectively quaternion form of pr and p,
¯pr = [p(cid:62)

r ,0](cid:62) and ¯p = [p(cid:62),0](cid:62).

Based on the quaternion properties, the rotation matrix and
the vectors in the cost function J are represented using quaternion

It means that the line PO is on the plane T1 given the fact that they
share a common point P. It is a contradiction unless the corner O
of the calibration target is also on the plane T1, which is a useless
case since camera cannot capture the checkerboard on T3. Thus,
the normal vectors Cn1, Cn3 and Cn4 are linearly independent. It is
similar for subset IV that we would have a contradiction that the
34 · Cn2 = 0 if Cn2, Cn3 and
line PO is on the plane T2 based on l(cid:62)
Cn4 are linear dependent. So we can conclude that these vectors
in both subset III and IV are also linearly independent.

Above all, it is proved that three normal vectors in each subset
(cid:4)

from I, II, III and IV are linearly independent.

=

=

APPENDIX B
PROOF OF LEMMA 1

From Appendix A, we know that any three of normal vectors Cn1,
Cn2, Cn3 and Cn4 can span the whole 3D space. Based on (14)
and (15), the matrix in (18) is

N

i=1

Cni

∑

Cn(cid:62)

Cn(cid:62)

N0 =

i = Cn1

Cn(cid:62)
4 ,
(40)
which is a symmetric matrix. We now show that this matrix is
non-singular.

2 + 2Cn3

3 + 2Cn4

1 + Cn2

Cn(cid:62)

Cn(cid:62)

As is known, the eigenvalues of a positive deﬁnite matrix A are
all positive [30]. Further, we know that a positive deﬁnite matrix
is always invertible [30]. From the properties above, we just need
to prove that N0 is positive deﬁnite. Let v (cid:54)= 0 be an arbitrary
non-zero vector. Then we calculate the quadratic form

(cid:16)Cn(cid:62)

(cid:17)2

(cid:16)Cn(cid:62)

(cid:17)2

(cid:16)Cn(cid:62)

(cid:17)2

(cid:16)Cn(cid:62)

(cid:17)2 ≥ 0.

+

4 v

1 v

2 v

3 v

+ 2

v(cid:62)N0v =
+ 2
(41)
We assume that v(cid:62)N0v = 0, which means Cn(cid:62)
i v = 0 for i =
{1,2,3,4}. However, since any three of these four normal vectors
are linearly independent, we get a contradiction that, for example,
[Cn1,Cn2,Cn3](cid:62)v = 0 if and only if v = 0. Thus, we can conclude
that
v(cid:62)N0v =
> 0.
(42)
Therefore, matrix N0 is positive deﬁnite and always invertible. (cid:4)

(cid:16)Cn(cid:62)

(cid:16)Cn(cid:62)

(cid:16)Cn(cid:62)

(cid:16)Cn(cid:62)

(cid:17)2

(cid:17)2

(cid:17)2

(cid:17)2

+ 2

+ 2

3 v

2 v

1 v

4 v

+

APPENDIX C
TRANSFORMATION TO AN ANALYTICAL LEAST-
SQUARES SOLUTION

We show how to transform the problem in (17) starting from (18)
to a problem for solving an analytical least-squares solution as
stated in [19].

(cid:35)2

+ fi

(cid:35)2

¯Lp j + fi

(cid:35)2

+ fi

(cid:35)2

+ fi

(cid:34)

+

=

+

+

j=1

¯gi j

j=1

¯gi j

j=1

Lq

N

=

=

N

i=1

N

i=1

N

i=1

J =

∑

i=1

Lq−1(cid:17)(cid:33)
Lq(cid:1)(cid:33)
Lq(cid:1) L(cid:0)C
(cid:17)(cid:33)
Lq(cid:1) ¯Lp j
(cid:17)(cid:33)

Lq⊗ ¯Lp j

∑

∑

∑

∑

∑

∑

¯Cni

¯Cni

(cid:34)

Lq⊗ ¯Lpi

(cid:62)(cid:16)C

(cid:62)(cid:16)C

Lq⊗ ¯Lpi ⊗ C

Lq⊗ ¯Lp j ⊗ C

form and J is further processed as follows:

Lq−1(cid:17)
(cid:32) N
Lq(cid:1) L(cid:0)C
Lq(cid:1) ¯Lpi
(cid:62)R(cid:62)(cid:0)C
(cid:32) N
(cid:62)R(cid:62)(cid:0)C
(cid:34)(cid:16)R(cid:0)C
(cid:17)
(cid:17)(cid:62)(cid:16)L(cid:0)C
Lq(cid:1) ¯Lpi
Lq(cid:1) ¯Cni
(cid:32) N
(cid:1)(cid:62)(cid:16)L(cid:0)C
Lq(cid:1) ¯gi j
(cid:0)R(cid:0)C
(cid:34)(cid:16) ¯Cni ⊗ C
(cid:17)
(cid:17)(cid:62)(cid:16)C
(cid:32) N
Lq(cid:1)(cid:62)(cid:16)C
(cid:0) ¯gi j ⊗ C
(cid:34)(cid:16)L(cid:16) ¯Cni
(cid:17)
(cid:17)C
(cid:17)(cid:62)(cid:16)R(cid:16) ¯Lpi
(cid:32) N
Lq(cid:1)(cid:62)(cid:16)R(cid:16) ¯Lp j
(cid:1)C
(cid:0)L(cid:0) ¯gi j
(cid:34)
(cid:17)C
Lq(cid:62)L (cid:62)(cid:16) ¯Cni
(cid:17) R(cid:16) ¯Lpi
(cid:33)
(cid:32) N
(cid:17)C
(cid:1) R(cid:16) ¯Lp j
Lq(cid:62)L (cid:62)(cid:0) ¯gi j
(cid:105)2
(cid:104)C
(cid:17) R(cid:16) ¯Lpi
where Mi = L (cid:62)(cid:16) ¯Cni
(cid:40)
Lq(cid:1)(cid:0)Mi + M(cid:62)
(cid:0)C
Lq(cid:62)Mi
∑N
i=1
Lq− 1 = 0
Lq(cid:62)C
C

Lq(cid:62) · Mi · C
Lq

C
Lq + λC

(cid:17)C

(cid:17)

(cid:16)

j=1 L (cid:62)(cid:0) ¯gi j
Lq(cid:1) = 0

∑

i=1

∑

i=1

∑

j=1

∑

j=1

∑

j=1

N

∑

i=1

∑N

Lq

N

=

N

=

Lq

Lq

=

C

+

C

+

+

,

C

i

+
fiI. Now, the cost function is reduced to a quadratic form with
four variables, which becomes relatively easy to solve by using
the Karush-Kuhn Tucker (KKT) conditions:

+

(cid:17)(cid:33)

(cid:17)C

Lq

+ fi

(cid:35)2

(cid:35)2

Lq

Lq(cid:62) (fiI)C
+ C
Lq

(cid:1) R(cid:16) ¯Lp j

(cid:17)(cid:17)

(48)

,

(49)

with the Lagrange multiplier λ . The problem in (49) consists ﬁve
polynomials with ﬁve unknowns, whose solution can be computed
using the eigenvalue decomposition of the so-called multiplication
matrix. Further steps are explained in [19].

ACKNOWLEDGMENTS
This work is supported in part by NSF Award 1317788, USDA
Award MIN-98-G02 and the MnDrive initiative.

REFERENCES
[1] R. A. Newcombe and A. J. Davison, “Live dense reconstruction with a
single moving camera,” IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), 2010.

12

[2] R. Newcombe, S. Izadi, O. Hilliges, D. Molyneaux, D. Kim, A. Davison,
P. Kohli, J. Shotton, S. Hodges, and A. Fitzgibbon, “KinectFusion: Real-
time dense surface mapping and tracking,” 10th IEEE Intl. Symposium on
Mixed and augmented reality (ISMAR), 2011.

[3] D. M. Cole and P. M. Newman, “Using laser range data for 3D SLAM in
outdoor environments,” IEEE International Conference on Robotics and
Automation (ICRA), 2006.

[4] Y. Bok, Y. Jeong, D. Choi, and I. Kweon, “Capturing village-level her-
itages with a hand-held camera-laser fusion sensor,” International Journal
of Computer Vision, vol. 94, no. 1, pp. 36-53, 2011.

[5] F. Moosmann and C. Stiller, “Velodyne SLAM,” in IEEE Intelligent

Vehicles Symposium, pp. 393-398, 2011.

[6] J. Zhang and S. Singh, “LOAM: Lidar odometry and mapping in realtime,”

in Robotics: Science and Systems Conference (RSS), July 2014.

[7] V. Lepetit, F. Moreno-Noguer, and P. Fua, “Epnp: An accurate o(n)
solution to the pnp problem,” International Journal of Computer Vision,
vol. 81, pp. 155-166, February 2009.

[8] S. A. Rodriguez, V. Fremont, and P. Bonnifait, “Extrinsic calibration be-
tween a multi-layer lidar and a camera,” in IEEE Int. Conf. on Multisensor
Fusion and Integration for Intelligent Systems, vol. 1, pp. 214-219, Seoul,
Korea, 2008.

[9] X. Gong, Y. Lin, and J. Liu, “3D LIDAR-camera extrinsic calibration
using an arbitrary trihedron,” Sensors, vol. 13, no. 2, pp. 1902-1918, 2013.
[10] F. M. Mirzaei, D. G. Kottas, and S. I. Roumeliotis, “3D LIDARcamera in-
trinsic and extrinsic calibration: Identiﬁability and analytical least-squares-
based initialization,” The International Journal of Robotics Research, vol.
31, no. 4, pp. 452-467, April, 2012.

[11] G. Pandey, J. McBride, S. Savarese, and R. Eustice, “Automatic targetless
extrinsic calibration of a 3D lidar and camera by maximizing mutual
information,” in Twenty-Sixth AAAI Conference on Artiﬁcial Intelligence,
2012.

[12] R. M. Haralick, C.-N. Lee, K. Ottenberg, and M. N¨alle, “Review
and analysis of solutions of the three point perspective pose estimation
problem,” International Journal of Computer Vision, vol. 13, pp. 331-356,
December 1994.

[13] Q. Zhang and R. Pless, “Extrinsic calibration of a camera and laser
range ﬁnder (improves camera calibration),” in IEEE/RSJ International
Conference on Intelligent Robots and Systems (IROS04), vol. 3, pp. 2301-
2306, 2004.

[14] F. Vasconcelos, J. P. Barreto, and U. Nunes, “A minimal solution for
the extrinsic calibration of a camera and a laser-rangeﬁnder,” IEEE
Transactions on Pattern Analysis and Machine Intelligence (PAMI), vol.
34, no. 11, pp. 2097-2107, 2012.

[15] G. Li, Y. Liu, L. Dong, X. Cai, and D. Zhou, “An algorithm for extrinsic
parameters calibration of a camera and a laser range ﬁnder using line
features,” in Proceedings of the IEEE/RSJ International Conference on
Intelligent Robots and Systems (IROS), pp. 3854-3859, 2007.

[16] S. Wasielewski and O. Strauss, “Calibration of a multi-sensor system
laser rangeﬁnder/camera,” in IEEE Proc. of the Intelligent Vehicles 95
Symposium, pp. 472-477, 1995.

[17] K. Kwak, D. F. Huber, H. Badino, and T. Kanade, “Extrinsic calibration
of a single line scanning lidar and a camera,” IEEE Proceedings of
International Conference on Intelligent Robots and Systems (IROS), pp.
3283-3289, 2011.

[18] O. Naroditsky, A. Patterson, and K. Daniilidis, “Automatic alignment of
a camera with a line scan lidar system,” in Proc. of the IEEE International
Conference on Robotics and Automation (ICRA), pp. 3429-3434, 2011.

[19] C. X. Guo and S. I. Roumeliotis, “An analytical least-squares solution
to the line scan LIDAR-camera extrinsic calibration problem,” in Proc. of
the IEEE International Conference on Robotics and Automation (ICRA),
pp. 2943-2948, Karlsruhe, Germany, 2013.

[20] R. Gomez-Ojeda, J. Briales, E. Fern´andez-Moral, and J. Gonzalez-
Jimenez, “Extrinsic Calibration of a 2D Laser-Rangeﬁnder and a Camera
based on Scene Corners,” in IEEE International Conference on Robotics
and Automation (ICRA), Seattle, USA, 2015.

[21] J.-Y. Bouguet, “Camera calibration toolbox for matlab,” http://www.

vision.caltech.edu/bouguetj/calib doc/.

[22] F. S. Macaulay, “Some formulae in elimination,” Proceedings of the

London Mathematical Society, vol. 1, no. 1, pp. 3-27, 1902.

[23] P. T. Boggs, and J. W. Tolle, “Sequential quadratic programming,” Acta

Numerica, vol. 4, pp. 1-51, 1995.

[24] K. Levenberg, “A method for the solution of certain non-linear problems
in least squares,” Quarterly of Applied Mathematics, vol. 2, pp. 164-168,
1944.

[25] D. W. Marquardt, “An algorithm for least-squares estimation of nonlinear
parameters,” Journal of the society for Industrial and Applied Mathemat-
ics, vol. 11, no. 2, pp. 431-441, 1963.

[26] c. Harris and M. Stephens, “A combined corner and edge detector,” in

Alvey Vision Conference, pp. 147-151, 1998.

13

[27] D. H. Douglas, and T. K. Peucker, “Algorithms for the reduction of the
number of points required to represent a digitized line or its caricature,”
Cartographica: The International Journal for Geographic Information
and Geovisualization, vol. 10, no. 2, pp. 112-122, 1973.

[28] G. H. Golub and C. F. Van Loan, “An analysis of the total least squares
problem,” SIAM Journal on Numerical Analysis, vol. 17, pp. 883-893,
1980.

[29] Z. Zhang, “A ﬂexible new technique for camera calibration,” IEEE Trans.
Pattern Analysis and Machine Intelligence, vol. 22, no. 11, pp. 1330-1334,
2000.

[30] C. R. Johnson, “Positive deﬁnite matrices,” The American Mathematical

Monthly, vol. 77, no. 3, pp. 259-264, 1970.

Wenbo Dong is a Ph.D student in the Depart-
ment of Computer Science at the University of
Minnesota, Minneapolis.

Volkan Isler is an Associate Professor at the
University of Minnesota. His research interests
are in robotics, sensing and geometric algo-
rithms.

PLACE
PHOTO
HERE

PLACE
PHOTO
HERE

