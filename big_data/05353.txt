6
1
0
2

 
r
a

 

M
7
1

 
 
]
I

N
.
s
c
[
 
 

1
v
3
5
3
5
0

.

3
0
6
1
:
v
i
X
r
a

OpenFunction: Data Plane Abstraction for Software-Deﬁned Middleboxes

Chen Tian†
Yangming Zhao†
†State Key Laboratory for Novel Software Technology, Nanjing University, China

Alex X. Liu‡

‡Department of Computer Science and Engineering, Michigan State University, USA

Ali Munir‡

Jie Yang†

Email: alexandretian@gmail.com, {alexliu, munirali}@cse.msu.edu

Abstract

The state-of-the-art OpenFlow technology only partially
realized SDN vision of abstraction and centralization for
packet forwarding in switches. OpenFlow/P4 falls short
in implementing middlebox functionalities due to the
fundamental limitation in its match-action abstraction. In
this paper, we advocate the vision of Software-Deﬁned
Middleboxes (SDM) to realize abstraction and central-
ization for middleboxes. We further propose OpenFunc-
tion, an SDM reference architecture and a network func-
tion abstraction layer. Our SDM architecture and Open-
Function abstraction are complementary to existing SDN
and Network Function Virtualization (NFV) technolo-
gies. SDM complements SDN as SDM realizes abstrac-
tion and centralization for middleboxes, whereas SDN
realizes those for switches. OpenFunction complements
OpenFlow as OpenFunction addresses network functions
whereas OpenFlow addresses packet forwarding. SDM
also complements NFV in that SDM gives NFV the abil-
ity to use heterogenous hardware platforms with various
hardware acceleration technologies.

1 Introduction

The vision of Software-Deﬁned Networking (SDN) is
two-fold: abstraction and centralization. Abstraction
means that heterogenous network devices of different
vendors/architectures have common programming inter-
faces for implementing the functionalities needed by the
control plane. Centralization means to use a logically
centralized controller in the control plane to obtain a
global view of a network and to manage network re-
sources and functionalities in an orchestrated and ﬂex-
ible manner. Abstraction and centralization help SDN
to achieve its two goals: breaking device vendor lock-
in and supporting network innovation. Abstraction helps
to break device vendor lock-in by allowing third-party
software to run on all SDN compliant network devices.
In contrast, traditional network devices are black boxes

where both hardware and software are tightly coupled as
they are typically from the same vendor, which leads to
both network ossiﬁcation and vendor lock-in. Centraliza-
tion helps to support network innovation by allowing net-
work resources and functionalities to be dynamically and
ﬂexibly organized and managed. In contrast, on tradi-
tional networks, packet forwarding and middlebox func-
tionalities are statically deployed, which makes adopting
and experimenting new network services and protocols
extremely difﬁcult. SDN has been increasingly gaining
market acceptance because of reduced cost (by abstrac-
tion) and new functionalities (by centralization).

Currently this SDN vision has only been realized for
switches by OpenFlow [1, 2] and its advanced version
P4 [3], but not for middleboxes. A data communica-
tion network has two types of devices, switches and mid-
dleboxes. Switches (including routers in the broader
sense) provide packet forwarding. Middleboxes pro-
vide a wide variety of networking and security func-
tionalities such as Network Address Translation (NAT),
Load Balancers (LB), ﬁrewalls (FW), WAN optimizers,
proxies, IPSec gateways (VPN), and network Intrusion
Detection/Prevention Systems (IDS/IPS). For a packet,
switches decide which path the packet traverses and mid-
dleboxes decide what processing the packet receives.

Network Function Virtualization (NFV) attempts to
address the issues of tight hardware/software coupling
and hardware vendor lock-in for middleboxes by imple-
menting middlebox functions purely in software running
on commodity servers [4, 5]. Unfortunately, NFV mid-
dleboxes are software, not software-deﬁned. The key
weakness of NFV middleboxes is low performance be-
cause commodity servers are designed for general com-
puting purposes. The functions and requirements of
packet processing are signiﬁcantly different from those
of general computing. Such differences warrant a wide
variety of specialized hardware acceleration technolo-
gies such as ASIC chips and Network Processing Units
(NPU), which typically have a 10-50 times performance

improvement over commodity server based solutions [6].
This partially explains that market has been in favor of
the closed-but-fast model, which uses proprietary hard-
ware platform with fast packet processing capability,
over the open-but-slow model, which uses commodity
servers with slow packet processing capability.

The match-action abstractions in OpenFlow/P4 has
too limited expression power to be applicable to mid-
dleboxes. First, for the action abstraction, OpenFlow/P4
limit themselves to a ﬁxed number of actions. However,
middleboxes, such as IPSec VPN, often need to perform
complex payload actions such as encryption and decryp-
tion. To address this limitation, the OpenFlow standard
has kept adding new actions. This is not a sustainable
solution as packet processing actions can never be ex-
haustively standardized. Furthermore, frequent standard-
ization of new action abstraction would lead to frequent
redevelopment of software/hardware, which results in
vertically integrated hardware-software that must be re-
placed all together in network upgrading [7]. Second,
for the match abstraction, OpenFlow and P4 limit them-
selves to matching packet headers. OpenFlow can only
specify matching conditions over a ﬁxed number of pre-
deﬁned packet header ﬁelds. P4 improves OpenFlow
by allowing users to extract customized packet header
ﬁelds. However, many middleboxes, such as applica-
tion ﬁrewalls and IPSes, need to match packet payload
against signatures [8]. Third, the table abstraction in
OpenFlow/P4 is fundamentally incapable of modeling
the function of scheduling. Scheduling is essential to im-
plement Quality-of-Service (QoS), which is further es-
sential for many middlebox functionalities. This inabil-
ity is because the table abstraction performs splitting an
aggregate ﬂow into multiple ﬂows whereas scheduling
performs merging multiple ﬂows into an aggregate ﬂow.
In some sense, a table and a scheduler perform totally
opposite functionalities, and one cannot model the other.
In this paper, we advocate the vision of Software-
Deﬁned Middleboxes (SDM) to realize abstraction and
centralization for middleboxes. For abstraction, an SDM
architecture should have a network function abstrac-
tion layer that is platform independent and fully pro-
grammable. Platform independence means to decou-
ple the network function semantics and the underlying
hardware that realizes the function. This allows any
third-party SDM program to execute at any SDM boxes
(i.e., SDM compliant middleboxes) with the same se-
mantics but different performance depending on hard-
ware adequacy. This abstraction layer should be hard-
ware optimizable, which means that the performance of
an SDM box can be optimized by various hardware ac-
celeration technologies. An SDM programmer needs
not to be aware of the underlying hardware features of
SDM boxes. However, an SDM program can be trans-

lated to a platform dependent program that fully lever-
ages the hardware features of the underlying SDM box.
Full programmability means that any middlebox func-
tionality can be implemented by an SDM program. For
centralization, an SDM architecture should have a logi-
cally centralized controller and a suite of communication
protocols that allows the controller to dynamically pro-
gram the functionality of an SDM box over the network.
To realize the vision of SDM, we propose OpenFunc-
tion, an SDM reference architecture and a network func-
tion abstraction layer, complementing what OpenFlow
achieves for switches. Our OpenFunction architecture
consists of a logically centralized OpenFunction con-
troller and a number of OpenFunction boxes distributed
around a network, as illustrated in Figure 2. Each net-
work function, such as NAT, LB, or FW, is accomplished
by a Control Plane (CP) process running on the Open-
Function controller and a group of Data Plane (DP) pro-
cesses running on different OpenFunction boxes. An DP
process has a local view of the network, and its main
role is to process packets in the data plane. Based on
the processed trafﬁc, it sends predeﬁned events to its
CP process, and receives commands from its CP pro-
cess. The CP process has a global view of the network,
and its main role is to make well-informed decisions for
the group of DP processes that it manages. It receives
events from DP processes, performs analyses, makes de-
cisions, and sends commands to DP processes to enforce
its decisions. The OpenFunction controller performs the
management and scheduling of network resources, both
CP and DP processes, and service chains. The CP pro-
cesses interact with the OpenFunction controller through
its northbound API. An OpenFunction box has a mid-
dleware layer called an OpenFunction shim layer, which
manages all DP processes running on the device, such
as installs a DP, starts a DP process, and terminates a
DP process. The OpenFunction shim layer interacts with
the controller through its southbound API. A DP process
interacts with the hardware resources in an OpenFunc-
tion box through an OpenFunction abstraction layer,
which provides a suite of atomic actions (i.e., APIs) for
packet/ﬂow/trafﬁc processing. Each DP process is im-
plemented using these atomic operations.

OpenFunction has ﬁve categories of predeﬁned ac-
tions: (1) starting action, i.e., packet sending to NICs,
(2) one-to-one actions such as packet encapsulation or
TTL decreasing, (3) one-to-many actions such as ﬂow
classiﬁcation based on exact match or pattern-match,
(4) many-to-one actions such as ﬂow scheduling based
on strict priority and weighted round robin, (5) end-
ing action, i.e., packet receiving from hardware NICs.
For specifying customized actions beyond those prede-
ﬁned ones, OpenFunction has a platform-independent
pseudo language. An OpenFunction pseudo program can

2

FW CP

NAT CP

LB CP

Management

OpenFunction

Controller

Scheduling

OpenFunction Shim Layer

OpenFunction Shim Layer

FW DP 1

NAT DP 1

FW DP 2

LB DP 1

OpenFunction Abstraction Layer

OpenFunction Abstraction Layer

SDM Box 1 Hardware Resources

SDM Box 2 Hardware Resources

Figure 1: Software-Deﬁned Middlebox Architecture

be compiled to platform-dependent modules for better
performance. For specifying packet processing proce-
dures using predeﬁned and customized actions, Open-
Function has a platform-independent scripting language.
An OpenFunction script implements a DP whereas an
OpenFunction pseudo program implements an action.
Through OpenFunction abstraction, the programmer de-
ﬁnes what to do and the device decides how to do.

The similarity between OpenFunction and Click mod-
ular router [9] is on the abstraction of actions. The con-
cept of element in Click and the concept of action in
OpenFunction are similar in that they are both abstrac-
tions of packet processing operations. The difference
between OpenFunction and Click modular router [9] is
two fold. First, in Click elements, both data plane opera-
tions and control plane operations are mixed together; in
contrast, OpenFunction actions contains only data plane
operations. Fundamentally, Click was not designed for
SDN whereas OpenFunction is designed for SDN. Click
elements cannot be used as a data plane network func-
tion abstraction, although we do use Click elements as
references in designing OpenFunction. Second, Click is
platform dependent whereas OpenFunction is platform
independent. In Click, each element is implemented by
C/C++ and a program using Click library needs to be
compiled before running. In OpenFunction, pseudo pro-
grams and scripts do not need to be compiled before run-
ning; they can be distributed by the controller to Open-
Function boxes and executed at run time.

OpenFunction realizes the abstraction vision of SDN
for middleboxes by the network function abstraction
layer in OpenFunction boxes. Through abstraction, for
middleboxes, we decouple their hardware and software
as well as their control plane and data plane. This ab-
straction layer eliminates hardware vendor lock-in be-
cause the underlying hardware architecture and vendor
are transparent to DP processes. It also eliminates soft-
ware vendor lock-in because any DP software can run
on any OpenFunction boxes. This abstraction layer
supports hardware/software heterogeneity and promotes
hardware/software innovation as the abstraction layer
can be implemented in a variety of hardware/software

technologies such as ASIC, multi-core, FPGA, and GPU.
This lowers the middlebox market barrier because as
long as a hardware device supports the abstraction layer,
it can immediately support DP processes and thus can be
immediately deployed on existing networks to work with
existing devices. This abstraction layer also signiﬁcantly
improves the programmability of middlebox functionali-
ties because a DP developer does not need to implement
those common operations in packet processing. From the
software engineering perspective, this abstraction layer
promotes software reuse. This will signiﬁcantly shorten
the time-to-market cycle.

OpenFunction realizes the centralization vision of
SDN for middleboxes by the logically centralized Open-
Function controller. As it is well understood that cen-
tralization in OpenFlow brings a long list of beneﬁts to
packet forwarding such as ﬂexible path selection, bet-
ter load balancing, ﬁner-grained network control, less
conﬁguration errors, higher manageability, and increased
reliability and security, centralization in OpenFunction
brings many beneﬁts to network functions. For example,
the global view that a CP process obtains from the events
generated by the group of DP processes that it manages
helps the CP to make better informed decisions, e.g., an
IDS CP process can perform correlation analysis on the
events from its DP processes to better identify attack ac-
tivities. For another example, the ability that a CP pro-
cess can issue commands to its DP processes allows the
CP to orchestrate the DP processes to collaboratively ac-
complish some tasks.

OpenFunction, together with OpenFlow, makes ser-
vice chain construction and scheduling much easier than
the current NFV architecture. A service chain is a se-
quence of middlebox functions that the network opera-
tor wants certain trafﬁc to traverse in order. To construct
and schedule service chains, the OpenFunction controller
performs resource allocation according to a given op-
timization policy (e.g., load balancing) in collaboration
with the forwarding plane authority (e.g., an OpenFlow
controller). For example, suppose the network operator
wants to allocate 10G bandwidth between two networks
and also all trafﬁc between the two networks to pass
through an IDS”, ﬁrst, the OpenFlow controller ﬁnds an
appropriate path between the two networks where each
switch on the path has at least 10G bandwidth and the
path contains at least one OpenFunction box, second, the
OpenFunction controller starts a new IDS DP process on
that OpenFunction box. In this SDM+SDN and Open-
Function+OpenFlow paradigm, one technical challenge
is service chain scheduling: given a service chain that a
ﬂow needs to traverse, the controller chooses a sequence
of middlebox data plane processes (running on one or
more OpenFunction boxes) to form the service chain.
The scheduling needs to satisfy the three requirements

3

of service chain constraints, device resource constraints,
and ﬂow conservation. Furthermore, it needs to be done
in a load balancing way to minimize the maximum uti-
lization of the resources in candidate devices. To address
this challenge, in this paper, we formulate service chain
scheduling as a NP-Hard problem, and proposed both of-
ﬂine and online scheduling algorithms.

OpenFunction, together with OpenFlow, helps to sup-
port the vision of service oriented network management.
While OpenFlow makes switches easier to manage, cur-
rently middleboxes are still notoriously difﬁcult to man-
age because the hardware and software of those mid-
dleboxes are often tightly coupled and different middle-
boxes of different vendors often have different manage-
ment interfaces. Service oriented network management
means to automate the translation from high-level service
oriented network management requirement to low-level
network device conﬁgurations. For example, a high-level
service oriented network management requirement can
be to “allocate 10G bandwidth between two networks
and to ensure that all trafﬁc between the two networks to
pass through an IDS”. While translating this high-level
requirement to network device conﬁgurations in today
networks is a complex, laborious, and error prone pro-
cess, it can be easily done in our proposed OpenFunc-
tion+OpenFlow architecture as a service chain.

Our SDM architecture and OpenFunction abstraction
are complementary to existing SDN and NFV technolo-
gies. SDM realizes abstraction and centralization for
middleboxes, whereas SDN realizes those for switches.
SDM breaks device vendor lock-in and supports network
innovation for middleboxes, whereas SDN does those
for switches. OpenFunction addresses network functions
whereas OpenFlow addresses packet forwarding. SDM
complements NFV in that SDM gives NFV the ability to
use heterogenous hardware platforms with various hard-
ware acceleration technologies.

We implemented a working SDM system including
one OpenFunction controller and three OpenFunction
boxes based on Netmap, DPDK and FPGA respectively.
We also develop two stateless network functions (i.e.,
NAT and IPsec), two stateful network functions (state-
ful ﬁrewall and IPS). We draw the following conclusions
from our experimental results. First, middlebox func-
tions implemented using OpenFunction abstraction can
achieve high performance. For example, our FPGA plat-
form can achieve near 10G bps throughput for NAT and
IPSEC middleboxes. Second, the performance of Open-
Function scripts is platform dependent. Third, for ser-
vice chain scheduling, our ofﬂine algorithm can achieve
near to optimal load balancing whereas our online algo-
rithm is fast but the performance is suboptimal. Fourth,
the overhead of enforcing service chain both across and
inside OpenFunction boxes is negligible.

2 Background and Related Work

Router Abstraction: Some prior work is on design-
ing an abstraction layer to support portability across het-
erogeneous platforms. Handley et al. built extensible
routers on top of commodity platforms in XORP, focus-
ing on breaking the complex control plane of routing pro-
tocols [10, 11]. Mogul et al. advocated an abstraction
layer called Orphal to support portability of third-party
software in a position paper [12]. XORP and Orphal sig-
niﬁcantly differ from OpenFunction. First, XORP and
Orphal are not designed for SDN whereas OpenFunc-
tion is. Second, XORP and Orphal are not platform
independent whereas OpenFunction is. Furthermore,
XORP focuses on routing whereas OpenFunction fo-
cuses middlebox functionalities, and Orphal is based on
router speciﬁc hardware resources (such as TCAM and
DPI engines) whereas OpenFunction is based on mid-
dlebox functionalities and is hardware agnostic. Song
proposed a hardware abstraction called POF, which fo-
cuses on protocol-oblivious forwarding [13]. It extends
the OpenFlow instructions with protocol-oblivious oper-
ations such as AddField and SetFieldFromValue. Answer
et al. propose to decouple the processing at the con-
troller and at middleboxes while providing an interface
by which they can communicate [14]. This vision is sim-
ilar to us; however, neither design nor implementation
details were given in the two-page position paper [14].

Data Plane Programmability: Several systems focus
on the data plane throughput of extensible software-
based routers. These systems are orthogonal to Open-
Function and they are useful to build OpenFunction
boxes. RouteBricks exploits parallelism both across mul-
tiple servers and across multiple cores inside each single
server
[15]. PacketShader exploits the massive paral-
lelism of GPU for batch processing [16]. GASPP focuses
on stateful packet processing, and its GPU-accelerated
framework can achieve multi-gigabit processing [17].
Some systems focus on data plane extensibility of new
network forwarding protocols. The main direction is
to pair the high-throughput ASIC processing path with
a fully programmable co-processer to enhance the pro-
grammability. ServerSwitch uses x86 CPU as the co-
processor [18, 19]. SSDP instead uses an NPU-driven
subsystem [20]. SwitchBlade allows rapid prototyping
in FPGA [21]. OpenDataPlane is an open-source cross-
platform set of APIs for programming network proces-
sors [22]. OpenDataPlane abstraction is at a lower level
than OpenFunction as OpenDataPlane speciﬁes how to
do by directly manipulating device resources whereas
OpenFunction only speciﬁes what to do and let each de-
vice decide how to implement each action.

4

(cid:894)(cid:258)(cid:895)(cid:3)(cid:47)(cid:374)(cid:296)(cid:396)(cid:437)(cid:400)(cid:410)(cid:396)(cid:437)(cid:272)(cid:410)(cid:437)(cid:396)(cid:286)(cid:3)(cid:115)(cid:349)(cid:286)(cid:449)

(cid:894)(cid:271)(cid:895)(cid:3)(cid:68)(cid:349)(cid:282)(cid:282)(cid:367)(cid:286)(cid:271)(cid:381)(cid:454)(cid:3)(cid:115)(cid:349)(cid:286)(cid:449)

(cid:894)(cid:272)(cid:895)(cid:3)(cid:94)(cid:286)(cid:396)(cid:448)(cid:349)(cid:272)(cid:286)(cid:3)(cid:18)(cid:346)(cid:258)(cid:349)(cid:374)(cid:3)(cid:115)(cid:349)(cid:286)(cid:449)

(cid:18)(cid:381)(cid:374)(cid:410)(cid:396)(cid:381)(cid:367)(cid:3)(cid:87)(cid:367)(cid:258)(cid:374)(cid:286)
(cid:24)(cid:258)(cid:410)(cid:258)(cid:3)(cid:87)(cid:367)(cid:258)(cid:374)(cid:286)

(cid:17)(cid:381)(cid:454)(cid:3)

(cid:87)(cid:396)(cid:286)(cid:282)(cid:286)(cid:296)(cid:349)(cid:374)(cid:286)(cid:282)(cid:3)
(cid:4)(cid:272)(cid:410)(cid:349)(cid:381)(cid:374)(cid:3)(cid:1005)
(cid:104)(cid:400)(cid:286)(cid:396)(cid:24)(cid:286)(cid:296)(cid:349)(cid:374)(cid:286)(cid:3)
(cid:4)(cid:272)(cid:410)(cid:349)(cid:381)(cid:374)(cid:3)(cid:1005)

(cid:87)(cid:396)(cid:286)(cid:282)(cid:286)(cid:296)(cid:349)(cid:374)(cid:286)(cid:282)(cid:3)
(cid:4)(cid:272)(cid:410)(cid:349)(cid:381)(cid:374)(cid:3)(cid:1006)
(cid:104)(cid:400)(cid:286)(cid:396)(cid:24)(cid:286)(cid:296)(cid:349)(cid:374)(cid:286)(cid:3)
(cid:4)(cid:272)(cid:410)(cid:349)(cid:381)(cid:374)(cid:3)(cid:1006)

(cid:75)(cid:393)(cid:286)(cid:374)(cid:38)(cid:437)(cid:272)(cid:410)(cid:349)(cid:381)(cid:374)(cid:3)(cid:4)(cid:271)(cid:400)(cid:410)(cid:396)(cid:258)(cid:272)(cid:410)(cid:349)(cid:381)(cid:374)

(cid:18)(cid:87)(cid:104)

(cid:38)(cid:87)(cid:39)(cid:4)

(cid:69)(cid:87)

(cid:4)(cid:94)(cid:47)(cid:18)

(cid:68)(cid:286)(cid:373)(cid:381)(cid:396)(cid:455)

(cid:18)(cid:346)(cid:286)(cid:272)(cid:364)(cid:400)(cid:437)(cid:373)
(cid:75)(cid:296)(cid:296)(cid:367)(cid:381)(cid:258)(cid:282)

(cid:87)(cid:346)(cid:455)(cid:400)(cid:349)(cid:272)(cid:258)(cid:367)
(cid:89)(cid:437)(cid:286)(cid:437)(cid:286)

(cid:94)(cid:90)(cid:882)(cid:47)(cid:75)(cid:115)

(cid:90)(cid:94)(cid:94)

(cid:100)(cid:94)(cid:75)

(cid:94)(cid:24)(cid:68)(cid:882)(cid:1005)(cid:3)(cid:18)(cid:87)

(cid:94)(cid:24)(cid:68)(cid:882)(cid:1005)(cid:3)(cid:24)(cid:87)(cid:882)(cid:1005)

(cid:4)(cid:272)(cid:410)(cid:349)(cid:381)(cid:374)

(cid:4)(cid:272)(cid:410)(cid:349)(cid:381)(cid:374)

(cid:4)(cid:272)(cid:410)(cid:349)(cid:381)(cid:374)

(cid:94)(cid:24)(cid:68)(cid:882)(cid:1005)(cid:3)(cid:24)(cid:87)(cid:882)(cid:1006)

(cid:4)(cid:272)(cid:410)(cid:349)(cid:381)(cid:374)

(cid:4)(cid:272)(cid:410)(cid:349)(cid:381)(cid:374)

(cid:94)(cid:24)(cid:68)(cid:882)(cid:1006)(cid:3)(cid:18)(cid:87)

(cid:18)(cid:381)(cid:374)(cid:410)(cid:396)(cid:381)(cid:367)(cid:3)(cid:87)(cid:367)(cid:258)(cid:374)(cid:286)
(cid:24)(cid:258)(cid:410)(cid:258)(cid:3)(cid:87)(cid:367)(cid:258)(cid:374)(cid:286)

(cid:17)(cid:381)(cid:454)(cid:3)(cid:4)

(cid:94)(cid:24)(cid:68)(cid:882)(cid:1006)(cid:3)(cid:24)(cid:87)

(cid:4)(cid:272)(cid:410)(cid:349)(cid:381)(cid:374)

(cid:4)(cid:272)(cid:410)(cid:349)(cid:381)(cid:374)

(cid:17)(cid:381)(cid:454)(cid:3)(cid:17)

(cid:100)(cid:396)(cid:258)(cid:296)(cid:296)(cid:349)(cid:272)

(cid:18)(cid:381)(cid:374)(cid:410)(cid:396)(cid:381)(cid:367)

(cid:94)(cid:24)(cid:68)(cid:882)(cid:1005)(cid:3)(cid:18)(cid:87)

(cid:94)(cid:24)(cid:68)(cid:882)(cid:1006)(cid:3)(cid:18)(cid:87)

(cid:94)(cid:449)(cid:349)(cid:410)(cid:272)(cid:346)(cid:3)(cid:1005)

(cid:75)(cid:393)(cid:286)(cid:374)(cid:38)(cid:437)(cid:272)(cid:374)(cid:410)(cid:349)(cid:381)(cid:374)(cid:3)

(cid:18)(cid:381)(cid:374)(cid:410)(cid:396)(cid:381)(cid:367)(cid:367)(cid:286)(cid:396)

(cid:75)(cid:393)(cid:286)(cid:374)(cid:296)(cid:367)(cid:381)(cid:449)(cid:3)(cid:18)(cid:381)(cid:374)(cid:410)(cid:396)(cid:381)(cid:367)(cid:367)(cid:286)(cid:396)

(cid:94)(cid:449)(cid:349)(cid:410)(cid:272)(cid:346)(cid:3)(cid:1006)

(cid:396)
(cid:286)
(cid:455)
(cid:258)

(cid:367)
(cid:3)

(cid:349)

(cid:373)
(cid:346)
(cid:94)

(cid:396)
(cid:286)
(cid:455)
(cid:258)

(cid:367)
(cid:3)

(cid:349)

(cid:373)
(cid:346)
(cid:94)

(cid:17)(cid:381)(cid:454)(cid:3)(cid:4)

(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:94)(cid:24)(cid:68)(cid:882)(cid:1005)(cid:3)(cid:24)(cid:87)(cid:882)(cid:1005)

(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:94)(cid:24)(cid:68)(cid:882)(cid:1006)(cid:3)(cid:24)(cid:87)(cid:882)(cid:1005)

(cid:17)(cid:381)(cid:454)(cid:3)(cid:17)

(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:94)(cid:24)(cid:68)(cid:882)(cid:1006)(cid:3)(cid:24)(cid:87)(cid:882)(cid:1006)

(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:94)(cid:24)(cid:68)(cid:882)(cid:1007)(cid:3)(cid:24)(cid:87)(cid:882)(cid:1005)

Figure 2: OpenFunction Architecture Overview

3 OpenFunction Architecture Overview

OpenFunction architecture consists of a logically cen-
tralized OpenFunction controller and a number of Open-
Function boxes distributed around a network where each
box has a network function abstraction layer. Here we
give an overview of OpenFunction from three perspec-
tives: the data plane, middleboxes, and service chains.
Data Plane View: OpenFunction exposes an extensible
set of actions to the control plane as shown in Figure 2
(a). The semantics of an action is to take a packet as
its input, perform some operations, and either push the
packet to the next action or wait for the next action to pull
the packet. OpenFunction is action oriented, similar to
the object oriented programming paradigm. Each action
object is an instance of an action class. An action class
is a self-contained and functionally independent packet
processing unit, such as decreasing the TTL ﬁeld and cal-
culating the TCP checksum. Each action class consists
of some data members, which we call attributes. For ex-
ample, a PacketCounter action may need to update
its counter attribute whenever a packet passes through
this action. An action may also send an event to the
control plane. For example, a TCPSyncNotifier ac-
tion may send a TCP sync event whenever a TCP sync
packet passes through it. There are two kinds of actions:
pre-deﬁned actions and user-deﬁned actions. Pre-deﬁned
actions are those supported by OpenFunction compliant
boxes and user-deﬁned actions are those written by users
using a platform-independent pseudo language. We al-
low multiple DP processes running on the same Open-
Function box, similar to router visualization, where in
the same box, different DP processes belongs to differ-
ent CP processes for different middlebox functions.
Middlebox View: In OpenFunction, a network function
(such as NAT, LB, or FW) is implemented by a Control
Plane (CP) process and a set of Data Plane (DP) pro-
cesses running on OpenFunction boxes, as shown in Fig-
ure 2 (b). A DP is modeled as a directed acyclic graph
where each node is an action. A DP process has a lo-

cal view of the network and its main role is to process
data plane packets. Based on the processed trafﬁc, it
may sends events to its CP process, and receives com-
mands from its CP process. The CP process has a global
view (via the controller) and its main role is to make
well-informed decisions for the group of DP processes
that it manages, according to its middlebox functional-
ity. It receives events from DP processes, performs anal-
yses, makes decisions, and sends commands to DP pro-
cesses to enforce its decisions. We call a CP process to-
gether with its group of DP processes a Software-Deﬁned
Middlebox (SDM). With OpenFunction, implementing a
middlebox functionality on the data plane becomes much
simpler as it mostly involves composing a graph of pre-
deﬁned actions, possibly with a few user-deﬁned actions.
Thus, SDM developers mostly focus on designing CP
programs, which are often the most innovative part of
their implementation.
Service Chain View: A service chain is a series of mid-
dlebox functions that a packet needs to go through, as
shown in Figure 2 (c). The OpenFunction schedules
and enforces service chains. The controller monitors re-
source (such as CPU, memory, or a speciﬁc acceleration
card) utilization and executes an service chain scheduling
algorithm; it then coordinates with the forwarding plane
authority (i.e., an OpenFlow controller) to enforce the
chain. Each OpenFunction box has a middleware layer
called an OpenFunction shim layer, which manages all
DP processes running on the device, such as installs a
DP, starts a DP process, and terminates a DP process.
The shim layer is fully controlled by the controller and
the controller monitors the resource unitization through
it. Note that service chains needs to be enforced both
across and inside boxes. An Openﬂow controller still
controls the forwarding between boxes. The OpenFunc-
toon controller sends ﬂow steering instructions to the
Openﬂow controller. A dedicated IO process of the
shim layer steers ﬂows among functions according to
their service chains. Figure 2 (c) shows a service chain
SDM 1 → SDM 2 → SDM 3.

5

4 Abstraction Design

4.1 Abstraction Speciﬁcation

OpenFunction uses a script for dataplane speciﬁcation:
an example of IPSEC middlebox data plane is shown in
Algorithm 1. Hereby is the brief description of Open-
Function script: each action instance deﬁnition line starts
with a “@”; each line deﬁnes a connection between an
egress port of one action and an ingress port of another
action; action deﬁnitions and connection deﬁnitions to-
gether construct the action graph of a DP process.

To carefully tradeoff composability and optimizabil-
ity, we analyze a number of middleboxes such as NAT,
IPSec, and IDS, and design a suite of atomic actions that
we classify into the following ﬁve categories:

• starting action: These actions send packets to ei-
ther hardware/virtualized NICs, or speciﬁc memory
locations (e.g., FromDevice in the example ).

• one-to-many action: This category consists of ac-
tions based on the following four matches: exact-
match (e.g., used in the example), longest-preﬁx-
match (e.g., used in FIB), ﬁrst-match (e.g., used in
ACL), and pattern-match (e.g., used in DPI).

• many-to-one action: A ﬂow scheduling action takes
multiple ﬂows as input, and outputs one combined
stream of trafﬁc. Example ﬂow scheduling opera-
tions include strict priority (SP) and weighted round
robin (WRR).

• one-to-ones action: These actions perform a single
packet processing such as changing the length of a
packet, either at the head (i.e., Encap/Decap) or the
tail (i.e., Pad/Unpad), and modifying a ﬁeld or the
metadata of a packet (i.e., decreasing the IP TTL
value and setting the ECN bit are two actions).

• ending action: These actions receive packets from
either hardware/virtualized NICs, or speciﬁc mem-
ory locations (e.g., ToDevice in the example).

The attributes of an action can be set either at the conﬁgu-
ration phase, or at run-time. Stick to the IPSEC example,
for a given IPSEC tunnel, the IP SRC/IP DST attributes
of ChangeSrcIP/ChangeDstIP actions are set at the con-
ﬁguration phase as parameters. While for the ESPEncap
action, its security index SPI and replay parameter RPL
should be negotiated by the control plain at run-time; the
IPSEC DP process leaves the two attributes unset at ﬁrst,
and lets the CP process performs negotiation before set-
ting the attribute value.

As the target of action operations, a packet class con-
sists of three types of data: ﬁelds, metadata, and prop-
erties. The metadata is a piece of memory associated

with a packet and is used to store modiﬁable informa-
tion about the packet, such as the ID of the physical port
from which the packet is received. The properties means
some non-modiﬁable information about the packet, such
as the packet size. Although some properties of a packet
may change after certain actions, e.g., the length of a
packet changes after Encap/Decap, the properties of a
packet cannot be written or modiﬁed by action functions
directly. Table 1 summaries the accessibility of packet
data.

Targets

Read
Write

Action

Property Attributes

Packet
Field Metadata
Yes
Yes
Yes
Yes
Table 1: Summary of data access

Yes
Yes

Yes
No

Event
No
Yes

To prevent the exact fate of OpenFlow’s vertical inte-
gration, we do not enforce any standard action. Instead,
a set of actions are recommended; and for each such
action, a default implementation written in OpenFunc-
tion pseudo language (next part) is provided together, An
OpenFunction box can ﬂexibly choose the action to op-
timize by exploiting their underlying hardware accelera-
tion capabilities. For the actions whose optimized imple-
mentation is not available at the box, the default pseudo
code can be used, like user-deﬁned actionS.

Furthermore, we allow OpenFunction boxes to pro-
vide multiple implementations of the same action: for
example, one version is CPU based and another version
is GPU based. The controller can dynamically determine
which version to load/unload based on the availability
of network resources and the processing requirements of
ﬂows at run time. Later in Section 5, we consider this
ﬂexibility in formulating and solving the service chain
scheduling problem.

4.2 Pseudo Language

We propose a platform-independent pseudo language
that allows SDM developers to design arbitrary new mid-
dlebox data plane action, and supported by any Open-
Function enabled device. The vendor of an OpenFunc-
tion box needs to provide a source-to-source translator,
which takes a pseudo C program as its input and out-
puts a native program that exploits the hardware acceler-
ation strength of the box. The output platform-dependent
program is further compiled into executable components
using the box’s native compiler. As a proof-of-concept,
shown in Section 6, in our prototype system an action
class can be translated either to a platform-dependent
C/C++ program such as a Click element, or a FPGA
block (in Verilog).

We now introduce the packet ﬁeld abstraction that the
pseudo programs operate on. The whole packet is as-

6

sumed to be stored in a continuous memory location as
shown in Figure 3. There is large headroom/tailroom be-
fore/after the packet data. Such spaces are reserved for
actions that change packet length (i.e., Encap/Decap and
Pad/Unpad). We also assume the existence of a manda-
tory preprocessing step, which marks the start offset of
link/network/tranport/app layers (if they exist). Each
ﬁeld is a continuous range of bytes in a packet mem-
ory range, deﬁned by a tuple (header, offset,
type) and can be attached with a name. For example,
IP checksum can be deﬁned as a ﬁeld starts at the 10th
bytes offset of the network header by the FIELD key-
word. The length is automatically determined by its data
type UNIT16. Note that a special type is DATA, which
represents a raw array of bytes.

Link 

Header

Transport 

Header

Packet

Headroom

Tailroom

Network 
Header

Application 

Header

A field

Figure 3: Abstraction of packet ﬁeld

The metadata of a packet is accessed as a named ob-
ject (with keyword META). In OpenFunction, a (type,
length) tuple deﬁnes a speciﬁc metadata object. The
length conﬁguration is required only when the type
is DATA. This design provides the system with the free-
dom to arrange a meta object in its metadata mem-
ory. Packet properties (with keyword PROP) and ac-
tion attributes (with keyword ATTR) are also accessed
as named objects. Note that packet properties are limited
to a small set of intrinsic packet characteristics and read-
only to program. The additional feature of an attribute is
that it can be read/written by another action or the con-
trol plane. Each event object is a continuous raw array
of bytes. It is up to the action developer to decide the
content of the event.

A key design decision is to separate packet access (Ta-
ble 1) codes and normal codes. More speciﬁcally, we use
character ”@” at the start of a line to denote a data ac-
cess line. Following the ”@” letter, there should either be
a packet variable deﬁnition of FIELD, META, PROP or
ATTR, or LOAD/STORE to access the data. Currently, we
do not support direct algorithmic operations over packet
data variables; they are used to denote the physical lo-
cations in the packet.
Instead, we ﬁrst load them into
local variables, perform calculation, and then store them
back. The beneﬁt is that all non-data-access codes can
be written as normal C codes.

Algorithm 5 shows an example pseudo program. This
user-deﬁned SetIPChecksum action accepts an incoming

IP packet and calculates and sets its IP checksum ﬁeld;
here the action assumes that the incoming packets are
with MAC headers. As an example, the iphlen is deﬁned
ﬁrst (Line 1), and loaded into a local variable hlen before
used (Line 5).

5 Service Chain Scheduling

One challenging task for OpenFunction controllers is to
perform service chain scheduling: given a ﬂow and a
sequence of SDM functions that the ﬂow needs to pass
through, the controller needs to choose a sequence of DP
processes of that SDM so that the ﬂow can go through
this sequence of DP processes. Note that if a speciﬁc ac-
tion in the program has multiple implementations (e.g.,
both CPU-based and GPU-based), then the compiled
SDM data plane also has multiple implementations. The
controller can load any desired implementation accord-
ing to the resource scheduling and ﬂow requirements.
For example, if a middlebox’s data plane graph has 3
actions and each action has 2 implementations; in total
there could be 23 = 8 implementations stored for this
middlebox at the maximum. Due to the cheap price
of storage, we consider this exponential increase in the
number of implementations a trivial concern.

There are some related work in service chain schedul-
ing and enforcement. CoMb exploits consolidation op-
portunities in NFV deployment to save cost [23]. It also
provides solutions to resource management, trafﬁc redi-
rection, and coordination among different data plane pro-
cessing elements. However, it requires that the middle-
box processes of the whole service chain pertaining to a
given session ﬂow run on the same device. This is not op-
timal in many OpenFunction scenarios: image that one
hardware platform has special optimization for IPSEC,
and another platform has special optimization for cache;
force the whole service chain in a single node compro-
mises the beneﬁts of global resource optimization. Some
schemes have been proposed to solve the problem of
steering trafﬁc among physical middleboxes to follow
the network policy (i.e., service chain). As a pioneer
work, the pswitches (i.e., policy-aware switches) scheme
adds a new layer-2 for data centers, which can steer the
network trafﬁc through unmodiﬁed middleboxes [24].
Using SDN, SIMPLE can enforce ﬂow-level policy even
if middleboxes modify the packets or change the ses-
sion level semantics [25]. FlowTag makes a further step
by attaching tags to ﬂow packets to enable ﬂow track-
ing. These existing schemes can be used in OpenFunc-
tion to control the service chain among middlebox plat-
forms [26].

Table 2 summarizes the symbols used in this section.
For a given ﬂow, the OpenFunction controller needs the

7

following three types of inputs to perform service chain
scheduling.

The main constraint is the ﬂow conservation for each

ﬂow crossing the SDMs

• Chaining Requirement: Each ﬂow has a classiﬁca-
tion rule that uniquely identiﬁes the ﬂow and es-
timated bandwidth. For the given ﬂow, we use
SDM1,SDM2, · · · ,SDMK to denote the sequence of
K SDMs that it needs to traverse. This ﬂow speciﬁ-
cation is typically speciﬁed by network operators.

• Implementation Availability: For each SDMk, let Vk
denote the total number of different implementa-
tions of SDMk. Each implementation has its spe-
ciﬁc resource requirement, which is usually multi-
dimensional as different actions in an implementa-
tion require different resources.

• Box Capability: Let N denote the set of OpenFunc-
tion boxes. Let W
n,r denote the capabilities of re-
source r in each box n. Also, the existing utiliza-
tion of resource r in OpenFunction box n is de-
noted as G n,r (in percentage). The OpenFunction
controller obtains box capabilities by performing re-
source monitoring of each OpenFunction box in real
time.

As a practical constraints, all packets in a given ﬂow
should traverse the same DP process for a given SDM
in the chain.

k,K
n,N
r,R
N f
kl

Dk,i,r

G n,r
n,r

A f
Zl, j,m, f
k,i,n

Xk,i,n

Table 2: Symbols used in this paper
SDM k, 1 ≤ k ≤ K.
OpenFunction box n, 1 ≤ n ≤ N.
resource r, 1 ≤ r ≤ R.
Input binary constant of ﬂow f ’s service
chain to indicate if SDM l is the next SDM
of k
The amount of resource r is required to sup-
port a unit of ﬂow for SDM k with implemen-
tation i.
the existing utilization of resource r in box n.
the capabilities of resource r in box n.
The amount of ﬂow f
Binary variable to indicate if ﬂow f is tra-
versed from the SDM k with implementation
i on box n to SDM l with implementation j
on box m.
The amount of ﬂows traversing node n for
SDM k with implementation i.

ILP Formulation The optimization objective in service
chain scheduling is load balancing across all resources of
all nodes.

minimize

max

k (cid:229)

i Xk,i,nDk,i,r

n,r

+ G n,r

(1)

8

k,i,n − (cid:229)
Zl, j,m, f

l, j,m≤n

l, j,m≥n

l, j,m =
Zk,i,n, f


1
if k = 0,n = 0
−1 if k = K,n = N
0

otherwise

(2)
In addition, a ﬂow may traverse from SDM k to SDM l
only if SDM k is followed by SDM l on ﬂow f ’s service
chain,

N f
kl ≥ Zl, j,m, f

k,i,n

(3)

Based on this ﬂow conservation constraint, the trafﬁc can
be calculated by

Xk,i,n = (cid:229)

A f Zk,i,n, f
l, j,m

f

l, j,m

(4)

To derive an ILP formualation, we can introduce an aux-
iliary variable z to present the objective function and z
should satisfy

z ≥

k (cid:229)

i Xk,i,nDk,i,r

n,r

+ G n,r

and the objective becomes

minimize

z

(5)

(6)

k,i,n

Therefore, the Integer Linear Programming (ILP) formu-
lation of this problem can be summarized as the resource
utilization optimization problem (RUOP). Obviously, the
ILP model formulated above is intractable in large size
network. Speciﬁcally, due to the large dimension of bi-
nary variable Zl, j,m, f
, there will be more than 107 binary
variables in the model even if each index has only 10 pos-
sible values. Accordingly, we need to design an efﬁcient
heuristic to solve this problem.
Ofﬂine Algorithm We ﬁrst consider the ofﬂine schedul-
ing, where a number of ﬂows are given and can be op-
timized at one time. Since the most critical element
which makes our problem intractable is the large number
of binary variables, we consider relaxation and round-
ing method to solve this problem. For clear presentation,
we call the enforced service chain (i.e., the ordered SDM
realized by which box and with which implementation,
as the path of this ﬂow. Following the relaxation and
rounding idea, we ﬁrst relax the binary constraint of the
variable Zl, j,m, f
, i.e., treat it as a real variable in the range
[0,1], and solve the derived Linear Programming model.
With this solution, each ﬂow may be split among multi-
ple paths which are determined by the value of Zl, j,m, f
.
To get unsplitable path for each ﬂow, a simple method
is to directly round each ﬂow to the path which carries
the largest fraction of this ﬂow. However, this method
cannot obtain a good result especially when there are a

k,i,n

k,i,n

W
(cid:229)
W
(cid:229)
(cid:229)
(cid:229)
W
lot of boxes and many implementation versions for each
SDM. In this case, each path may carry only a little frac-
tion of the ﬂow; directly rounding each ﬂow to the path
carrying maximum fraction of ﬂow may deviate too far
away from the ILP optimum. To solve this problem, we
propose a progressive method for the rounding phase to
get unsplitable paths.

k,i,n

Respect to progressive rounding, the main idea is to it-
eratively forbid some links that carrying very little frac-
tion for each ﬂow, till there is only one path for each
ﬂow or there is a path carrying most of the fraction of
ﬂow. Based on above discussions, we design Algorithm
4 to solve the RUOP formulation. In this algorithm, there
is one step that has not been discussed. In Line 4, we
should ﬁnd a threshold e to determine if a Zl, j,m, f
should
be set to 0 for the rounding purpose. This epsilon can
be set based on the tradeoff between the algorithm per-
formance and the algorithm running time. With large
epsilon, more Zl, j,m, f
k,i,n will be set to 0 in each loop and
hence there are fewer loops executed in the algorithm.
With smaller epsilone, we can do derive a better solu-
tion. On the other hand, we should update the epsilon in
each loop since the Zl, j,m, f
should become larger in each
loop as there are fewer paths can be used by the ﬂows
with the algorithm execution. Accordingly, without up-
dating the rounding threshold, there will be endless loop
in the algorithm.
From Ofﬂine to Online We can also extend the algo-
rithm to online version of this problem, because in real-
ity service chain requests may arrive randomly, not all at
once. The ﬁrst scheme is to hold the ﬂows for a while
to get a bulk of ﬂows and then trigger the ofﬂine algo-
rithm to reserve SDM resources for these ﬂows. If the
ﬂow arriving rate is very small, hold ﬂows to wait for the
later ﬂows may be a large overhead for the ﬂow arriving
earlier. In this case, we can treat each ﬂow as a bulk to
trigger Algorithm 4 to set up SDMs for each single ﬂow.

k,i,n

6 Prototype Implementation

6.1 Platform

We build a proof-of-concept system to verify the Open-
Function abstraction and get an estimation of achievable
performance.
Data Plane We develop two x64 platform OpenFunc-
tion boxes based on DPDK and Netmap, and one hard-
ware box based on FPGA. Netmap is a tool designed
for high speed packet I/O in commodity servers.
Im-
plemented as either a modiﬁed NIC driver or a kernel
module, Netmap also enables high-speed processing in
user space. Supported by Intel, Data Plane Development
Kit (DPDK) is a combination of data plane libraries and
NIC drivers for fast packet processing in user space. Poll

9

mode drivers (PMD) are designed to enable direct pack-
ets exchange between a user space process and a NIC. A
low overhead run-to-completion model is used to achieve
fast data plane performance.
In our testbed, there are
16 Dell R320 machines, each has a 10G Intel X520-
SR1 ethernet NIC and is both DPDK and Netmap en-
abled. We also develop a prototype box in a FPGA card
via Xilinx Vivado High-Level Synthesis tool, which sup-
ports C/C++ to describe designs and translates them into
hardware description languages (HDL) such as VHDL
or Verilog HDL [33]. The platform is a programmable
FPGA card with a Xilinx Kintex 7 chip, two 10 Gbps
NIC and four 1 Gbps NIC.

For each prototype box, we need to provide three
support: OpenFunction script, OpenFunction language
and in-box chaining. Regards the third requirement,
the device needs to support dynamic DP function load-
ing/unloading: consider the scenario initially ﬂow 1 only
passes through SDM 1 and SDM 2, later an operator de-
cides to add an additional action to be performed on that
ﬂow and therefore SDM 3 is inserted into the service
chain. There can be many scenarios for such a need. For
example, a company can perform DPI, to block instant
message packets (such as gtalk and Skype) only during
the ofﬁce hour. Such temporary insertion/deletion of data
plane instances should be seamless as no packet loss is
allowed. A simple solution could be to temporarily hold
the packets of the ﬂow, construct a new service chain,
and switch to the new chain. However, this approach
adds signiﬁcant delay to packets processing during the
transition. Instead, we prefer a bufferless solution.
Control Plane OpenFunction controller sends ﬂow
steering instructions to the Openﬂow controller. We use
OpenDaylight (ODL) [34] as the OpenFlow controller
that manages OpenFlow capable switches and routers.
OpenFunction controller runs as an OpenDaylight appli-
cation. It uses many base functions provided by ODL to
learn about the network and control network elements.

In our testbed, we use OpenvSwitch [35] as a switch-
ing element to steer trafﬁc. We use a quad core 3.0 GHz
AMD Phenom(tm) II X4 945 Processor system to host
the OpenFunction controller, the OpenFlow controller
and all SDM CP processes.

6.2 Script Support

Due to space limitation, we only explain how Netmap-
Click and FPGA platforms works. Netmap tool does not
provide enough native packet libraries; therefore, we use
Click as the packet processing pipeline.

The translated Click script of the IPSEC data plane
speciﬁcation (Algorithm 1) is shown in Algorithm 2.
One major challenge is the abstract parameters of ac-
tions in the speciﬁcation: we either replace them with

real parameters in Click format, or implement them as
action attributes, and let the IPSEC CP process sets them
in runtime. Another challenge is that we need to add ad-
ditional MarkMACHeader and MarkIPHeader elements,
after the input device action, for Click speciﬁc require-
ments.

In FPGA box, both OpenFunction script and pseudo
language are translated to the special C/C++ code: the
speciﬁcation is translated to the top level block (com-
posed by a call graph among low level blocks), while
each pseudo language based action deﬁnition is trans-
lated to a low level block. The translation of data plane
IPSEC speciﬁcation (Algorithm 1) as shown in Algo-
rithm 6, requires (1) deﬁning the inter-link between func-
tional actions (i.e., 5-20); and (2) using direct function
calls to represent actions (e.g., Line 21-29). Note that in
FPGA platform, we implement all parameters as action
attributes, which are controlled by its corresponding CP
process.

6.3 Language Support

In Netmap platform, we translate each action pseudo pro-
gram to an element class deﬁnition in Click. Mostly, the
generated codes are in the simple action function. Each
special line in the pseudo language ﬁle is processed. For
example, Algorithm 3 is the translated code of generat-
ing a pointer to IP header length ﬁeld (Algorithm 5 Line
1). Other codes are input from the pseudo program as is.
OpenFunction language translation in Xilinx FPGA is
similar to the operations of that in Netmap. The major
difference is that: an additional C++ encapsulation class
is required for each action class, to express the state tran-
sition semantics in FPGA hardware. Due to space lim-
itation, we omit the details of both FPGA and DPDK
platforms.

IO -> SDM 1 

IO -> SDM 1 

IO 

Process 

SDM 1 -> IO 

IO -> SDM 3 

SDM 3 -> IO 

IO -> SDM 2 

SDM 1 

Data Plane 

SDM 2 

Data Plane 

IO 

Process 

SDM 1 -> IO 

IO -> SDM 3 

SDM 3 -> IO 

IO -> SDM 2 

SDM 1 

Data Plane 

SDM 3 

Data Plane 

SDM 2 

Data Plane 

SDM 2 -> IO 

SDM 2 -> IO 

Figure 4: IO based service chain

pairs it needs to traverse in order, and an index pointer
keeps the value of the next pair. Considering the above-
mentioned dynamic example, initially only SDM 1 and
SDM 2 are traversed by a ﬂow;
later when the new
SDM 3 data plane process starts, all subsequent pack-
ets’ metadata change to the new service chain SDM 1 →
SDM 3 → SDM 2; and as a result, packets traverse
SDM 3 between SDM 1 and SDM 2. Similarly, to unload
a service chain, when SDM 3 is removed from the service
chain, the metadata of all subsequent packets change to
the new service chain SDM 1 → SDM 2 again; the con-
troller waits for a while, before it cleans up the SDM 3
process from the system. For performance consideration,
we assume all processes of a group is running on a sin-
gle CPU socket (possibly with multiple cores). Note that
multiple process group is supported, if we start different
process groups in different sockets.

We also support this feature in the FPGA platform.
Unlike software, loading/uploading new data plane func-
tions in programmable hardware needs direct support
from hardware. Partial Reconﬁguration is a feature of
modern FPGAs, which allows a subset of the logic fab-
ric of a FPGA to be dynamically reconﬁgured while the
remaining logic continues to operate [36].

6.4 In-Box Service Chain

6.5 Developed SDMs

In this part, we use DPDK to illustrate the provisioning
of in-box service chain. We use process group to realize
the service chain and treat each SDM data plane instance
as a separate process. As shown in Figure 4, a single IO
process acts as a central process: it sets up all memory
structures for use and handles packet reception and trans-
mission. A pair of rings are setup between the IO process
and a SDM process for packets exchange. When a new
SDM DP process starts, it discovers both the rings and
packet memory locations via DPDK library support. The
IO process sends a packet’s descriptor through the ring;
a SDM data plane processes the packet and returns it via
another ring.

The pair of rings is the key to realize dynamic
load/unload. Each packet’s metadata carry the service
chain information: an array records the indexes of ring

We implement NAT and IPSec gateway as two stateless
SDMs on all Netmap and FPGA platforms. The NAT
middlebox performs simple address translation; the map-
ping table is stored in its CP process and the mapping
item is send to a DP process when the ﬁrst packet of
a ﬂow comes in. The IPSec gateway uses ESP tunnel
model with AES-EBC encryption. The control plain ne-
gotiates with the remote peer, and sends command to DP
processes after security negotiation.

With Netmap, we also implement a simple stateful
ﬁrewall and a stateful IPS. The stateful ﬁrewall moni-
tors all path-through TCP connections; it removes the
allow rules in both directions whenever a RESET ﬂag
is detected. The stateful IPS monitors FTP connections:
some FTP commands are allowed only after the user au-
thentication; otherwise an alert is raised.

10

7 Evaluation

7.1 Abstraction

First, we test the OpenFunction speciﬁcation perfor-
mance for the data plane abstraction of each SDM. For
each SDM, we use same set of OpenFunction speciﬁca-
tions. We test with both small size packets (60 bytes)
and large size packets (1400 bytes). Table 3 shows the
throughput performance.

For non-stateful SDMs: NAT and IPSEC, DPDK box
achieves higher throughput than Netmap-based box in al-
most all the scenarios. This demonstrates that, even with
the same OpenFunction abstraction implementation and
the same hardware, the performance actually relies on
underlying systems.
In turn, the FPGA box is always
better than DPDK.
Key Takeaway 1 Our OpenFunction abstraction ac-
commodates vendor differentiation with the same code
across heterogeneous systems, which is beneﬁcial for the
NFV community.

Table 3: Throughput of OpenFunction middleboxes

NAT Small
NAT Large
IPSEC Small
IPSEC Large

FW Small
FW Large
IPS Small
IPS Large

Netmap
0.14 Gbps
0.20 Gbps
0.02 Gbps
0.17 Gbps
0.03 Gbps
0.18 Gbps
0.02 Gbps
0.19 Gbps

DPDK
1.84
9.96
1.2
2.8
N/A
N/A
N/A
N/A

FPGA

1.64 Gbps
9.34 Gbps
2.61 Gbps
9.35 Gbps

FPGA-C
0.24 Gbps
0.53 Gbps
0.04 Gbps
0.10 Gbps

N/A
N/A
N/A
N/A

N/A
N/A
N/A
N/A

Next,

we

the

replace

SetIPChecksum and
SetTCPChecksum actions in the speciﬁcation with
user deﬁned versions (written in OpenFunction language
and translated). The performance overhead caused by
user deﬁned versions is negligible: for these software-
based boxes, without special acceleration treatment, the
C compiler performs most of the optimization work.
Also, for such an action, the efﬁciency of a translated
version is similar to the implemented version.

For FPGA platform, the performance gap between
optimized implementation and OpenFunction language
generated version (i.e., FPGA-C) is signiﬁcant (10x in
this case)Table 3. The reason is that FPGA enables a
programmer to introduce several optimizations. For ex-
ample, for NAT function, the programmer may choose to
calculate the checksum at line rate, without the need to
receive complete packet.
Key Takeaway 2 The performance of SDM data plane
actions, if generated from OpenFunction pseudo lan-
guage, is dominated by the nature of system.

11

Offline Algorithm
Online Algorithm
LP solution

 

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

n
o
i
t
a
z
i
l
i
t
u

 
e
c
r
u
o
s
e
r
 

m
u
m
i
x
a
M

0
 
0

200

400

Flow Number

600

800

1000

Figure 5: Performance impacted by ﬂow number.

Similarly, for two stateful SDMs: FW and IPS, the

performance is comparable to non-stateful SDMs.
Key Takeaway 3 OpenFunction abstraction has the po-
tential to support various stateful ﬁrewalls.

7.2 Centralization

In this section, we evaluate our algorithm by leveraging
the trafﬁc data collected from a production datacenter
hosted by IBM global services. To this end, we conduct
performance evaluation from two perspectives. First, we
evaluate the impact of varying number of ﬂows in the
system on algorithm performance, and then the impact
of threshold selected at line 4 in Algorithm 4 on algo-
rithm performance.

k,i,n

Performance impact by varying the number of ﬂows:
In this section, we ﬁx the box number to be 10, there are
6 SDMs and the required SDM number of each ﬂow is
evenly distributed in the range [1,5]. Each SDM has 2
or 3 versions implemented in the system.
In addition,
we set the forbidden threshold to be 0.2, i.e. 20% of
the Zl, j,m, f
and the fraction value according to the LP
solution will be set to 0 in each iteration. To see how
the algorithm performance changes with the number of
ﬂows, we change the ﬂow number from 100 to 1000, and
the simulation results are shown in Fig. 5. We evaluate
the performance of proposed online and ofﬂine algorithm
and compare it to the LP solution, which treats all the
ﬂows as splittable, and hence the LP solution is a lower
bound of our problem. From Figure 5, we make two ob-
servations: i) the online algorithm achieves performance
very close to LP solution, and ii) its performance margin
from ofﬂine approach improves with increasing number
of ﬂows in the system.

A single ﬂow only needs very little system resource
compared to the available system resources, and there-

 

Maximum resource utilization

0.7

0.65

0.6

0.55

0.5

0.45

0.4

0.35

0.3

0.25

n
o
i
t
a
z
i
l
i
t
u

 
e
c
r
u
o
s
e
r
 

m
u
m
i
x
a
M

0.2
 
0

0.2

0.4

0.6

Percentage

0.8

1

Figure 6: Performance impacted by ﬂow number.

fore, we can treat all the ﬂows as splittable in the sys-
tem’s point of view. As a result, the performance of of-
ﬂine algorithm is very close to the LP solution.

The performance of online algorithm deviates from the
optimal solution as the number of ﬂows increase in the
system. This is for the reason that the online algorithm is
a myopic algorithm and optimizes the ﬂows one by one,
whenever a ﬂow enters the system, it optimizes the max-
imum resource utilization based on the current system
utilization but does not consider future ﬂow arrivals.

The impact of threshold e : In this section, we study
how the forbidden parameter impacts the ofﬂine algo-
rithm performance. For this purpose, we inject 400 ﬂows
into the system and change the value of forbidden thresh-
old from 0.1 to 0.95.

The simulation results, Fig. 6, show that

for smaller
thresholds, the ofﬂine algorithm performance degrades
very slowly with increasing the value of forbidden thresh-
old. However, when the threshold value exceeds a certain
point, the algorithm performance degrades quickly. The
reason is that in the LP solution, there are many Zl, j,m, f
with very little fraction, and they can be set to 0 at one
time without impacting the performance signiﬁcantly.

k,i,n

In addition, as the value of the forbidden threshold
continues to increase, the performance degradation slows
down. This happens because if we set too many Zl, j,m, f
as zero, the optimization space for the ﬂows reduces sig-
niﬁcantly. Therefore, forbidding more Zl, j,m, f
cannot de-
grade the algorithm performance if the forbidden param-
eter is relatively large.
Key Takeaway 4 OpenFunction abstraction has the po-
tential to support various stateful ﬁrewalls.

k,i,n

k,i,n

Lastly, we consider two scenarios to evaluate Open-
Function data plane overhead:
takes to
start/stop a data plane process in the service chain; and

(1) time it

12

(2) additional path delays due to our chain enforcement
inside a box. In the ﬁrst case, we are interested in un-
derstanding how much delay is incurred when a new ser-
vice chain or SDM DP is added or removed from the
network. To evaluate, we use a small topology with 5
DPDK middleboxes and generate TCP ﬂows between
source and destination. We have two DPDK boxes on
path 1 and one DPDK box on path 2. We initiate ﬂows
on path 1 and path 2 which initiates ﬂow rule installation
in the switches along the datapath and calculate datap-
ath setup times. Our evaluation shows that it takes less
than 100 msec to add a new datapath or service chain in
the network.
In the second case, we are interested in
understanding the additional delays caused by enforcing
service chains in the MB’s. Our evaluations shows that it
adds less than 20 usec per SDM DP in the service chain.
In this case, we are interested in knowing the respon-
siveness of the controller to adapt to dynamic network
conditions and we measure the time required to recon-
ﬁgure the network in case of SDM DP failure or traf-
ﬁc overload in some of the service chains. We consider
same topology as before, with DPDK middleboxes, and
use Iperf [37] to generate TCP ﬂows between source and
destination. We ﬁrst initiate ﬂows on path1 and path2
by install rules in the switches along the datapath and
distribute load evenly across the two paths. Next, we
bring down the switch in path2 and observe how quickly
network converges to the stable state and diverts trafﬁc
to path1. Our evaluation shows that network takes less
than 200 msec to converge to a stable state and reconﬁg-
ure ﬂows to use other paths. We also dynamically change
the datapaths, to replicate the SDM insertion and deletion
scenarios, in our topology to steer network trafﬁc through
different paths and our evaluation shows that the network
quickly adapts to these changes without any drop in the
ﬂow throughput.
Key Takeaway 5 OpenFunction abstraction is ﬂexible
and has light overhead.

8 Conclusions

In this paper, we make three main contributions. First,
we propose the concept of Software-Deﬁned Middle-
boxes to realize abstraction and centralization for mid-
dleboxes, which are complement existing SDN effort.
Second, we propose OpenFunction as an SDM reference
architecture and a network function abstraction layer.
Third, we implemented a working OpenFunction sys-
tem including one OpenFunction controller, three Open-
Function boxes, and four network functions including
both stateful and stateless ones. Our experimental results
show that the middlebox functions implemented using
our OpenFunction abstraction can achieve high perfor-
mance and platform independence.

References

[1] Nick McKeown, Tom Anderson, Hari Balakrishnan,
Guru Parulkar, Larry Peterson, Jennifer Rexford, Scott
Shenker, and Jonathan Turner. Openﬂow: Enabling inno-
vation in campus networks. SIGCOMM Comput. Com-
mun. Rev., 38(2):69–74, March 2008.

[2] ONF.

OpenFlow

Switch

Version

iﬁcation
https://www.opennetworking.org/images/stories/downloads/sdn-
resources/onf-speciﬁcations/openﬂow/openﬂow-switch-
v1.5.0.noipr.pdf.

1.5.0,

Spec-
2015.

[3] Pat Bosshart, Dan Daly, Glen Gibb, Martin Izzard,
Nick McKeown, Jennifer Rexford, Cole Schlesinger,
Dan Talayco, Amin Vahdat, George Varghese, et al.
P4: Programming protocol-independent packet proces-
sors. ACM SIGCOMM Computer Communication Re-
view, 44(3):87–95, 2014.

[4] Network functions virtualisation – an introduction, ben-
eﬁts, enablers, challenges & call for action. In Network
Functions Industry Speciﬁcation Group (NFV ISG) under
the auspices of the European Telecommunications Stan-
dards Institute (ETSI), 2012.

[5] Network functions virtualisation – network operator per-
spectives on industry progress. In Network Functions In-
dustry Speciﬁcation Group (NFV ISG) under the auspices
of the European Telecommunications Standards Institute
(ETSI), 2013.

[6] John Larkins. Recent trends in virtual network func-
tions acceleration for carrier clouds. In Architectures for
Networking and Communications Systems (ANCS), 2015
ACM/IEEE Symposium on, pages 3–3. 2015.

[7] Bilal Anwer, Theophilus Benson, Nick Feamster, Dave
Levin, and Jennifer Rexford. A slick control plane for
network middleboxes.
In Proc. 2nd ACM SIGCOMM
workshop on Hot topics in software deﬁned networking.
2013.

[8] Alex X. Liu, Chad Meiners, Eric Norige, and Eric Torng.
High-speed application protocol parsing and extraction
for deep ﬂow inspection. IEEE Journal on Selected Areas
in Communications (JSAC), 2014.

[9] Eddie Kohler, Robert Morris, Benjie Chen, John Jannotti,
and M Frans Kaashoek. The click modular router. ACM
Transactions on Computer Systems (TOCS), 18(3):263–
297, 2000.

[10] Mark Handley, Orion Hodson, and Eddie Kohler. Xorp:
An open platform for network research. ACM SIGCOMM
Computer Communication Review, 33(1):53–57, 2003.

[11] Mark Handley, Eddie Kohler, Atanu Ghosh, Orion Hod-
son, and Pavlin Radoslavov. Designing extensible ip
router software. In Proc. 2nd conference on Symposium
on Networked Systems Design & Implementation-Volume
2, pages 189–202. USENIX Association, 2005.

13

[12] Jeffrey C Mogul, Praveen Yalagandula, Jean Tourrilhes,
Rick McGeer, Sujata Banerjee, Tim Connors, and Puneet
Sharma. API design challenges for open router platforms
on proprietary hardware. In HotNets, pages 7–12, 2008.

[13] Haoyu Song. Protocol-oblivious forwarding: Unleash the
power of sdn through a future-proof forwarding plane. In
Proc. second ACM SIGCOMM workshop on Hot topics
in software deﬁned networking, pages 127–132. 2013.

[14] Bilal Anwer, Theophilus Benson, Nick Feamster, Dave
Levin, and Jennifer Rexford. A slick control plane for
network middleboxes. In Proc. second ACM SIGCOMM
workshop on Hot topics in software deﬁned networking,
pages 147–148. 2013.

[15] Mihai Dobrescu, Norbert Egi, Katerina Argyraki, Byung-
Gon Chun, Kevin Fall, Gianluca Iannaccone, Allan
Knies, Maziar Manesh, and Sylvia Ratnasamy. Route-
bricks: exploiting parallelism to scale software routers.
In Proc. ACM SIGOPS 22nd symposium on Operating
systems principles, pages 15–28. 2009.

[16] Sangjin Han, Keon Jang, KyoungSoo Park, and Sue
Moon. Packetshader: a gpu-accelerated software router.
In Proc. 2010 ACM conference on SIGCOMM. 2010.

[17] Giorgos Vasiliadis, Lazaros Koromilas, Michalis Poly-
Gaspp: A gpu-
In

chronakis, and Sotiris Ioannidis.
accelerated stateful packet processing framework.
USENIX Annual Technical Conference, 2014.

[18] Guohan Lu, Chuanxiong Guo, Yulong Li, Zhiqiang
Zhou, Tong Yuan, Haitao Wu, Yongqiang Xiong, Rui
Gao, and Yongguang Zhang.
Serverswitch: A pro-
grammable and high performance platform for data cen-
ter networks. In Proc. 2011 USENIX NSDI, 2011.

[19] Guohan Lu, Rui Miao, Yongqiang Xiong, and Chuanx-
iong Guo. Using cpu as a trafﬁc co-processing unit in
commodity switches. In Proc. 1st workshop on Hot top-
ics in software deﬁned networks. 2012.

[20] Rajesh Narayanan, Saikrishna Kotha, Geng Lin, Aimal
Khan, Sajjad Rizvi, Wajeeha Javed, Hassan Khan, and
Syed Ali Khayam. Macroﬂows and microﬂows: En-
abling rapid network innovation through a split sdn data
plane. In European Workshop on Software Deﬁned Net-
working, 2012.

[21] Muhammad Bilal Anwer, Murtaza Motiwala, Mukar-
ram bin Tariq, and Nick Feamster. Switchblade: a plat-
form for rapid deployment of network protocols on pro-
grammable hardware. In Proc. 2010 ACM conference on
SIGCOMM. 2010.

[22] Opendataplane. http://www.opendataplane.org/.

[23] Vyas Sekar, Norbert Egi, Sylvia Ratnasamy, Michael K
Reiter, and Guangyu Shi. Design and implementation of
a consolidated middlebox architecture. In Proc. USENIX
NSDI, 2012.

Appendix

Algorithm 1: IPSEC Script (one way)
1 @ FromDevice(1) fromdevice ;
2 @ ExactMatch(PROTO IP,-) match ;
3 @ DecapHeader(14) decap;
4 @ ESPEncap espencap;
5 @ Aes(EBC) aes;
6 @ IPsecEncap ipencap;
7 @ ChangeSrcIP(IP SRC) srcip;
8 @ ChangeDstIP(IP DST) dstip;
9 @ SetIPChecksum checksum;
10 @ EncapHeader(MAC DEST, MAC SRC,

PROTO IP) encap;

11 @ ToDevice(2) todevice;
12 fromdevice 0 0 match; match 0 0 decap ;
13 decap 0 0 espencap; espencap 0 0 aes ;
14 aes 0 0 ipencap; ipencap 0 0 srcip ;
15 srcip 0 0 dstip; dstip 0 0 checksum ;
16 checksum 0 0 encap; encap 0 0 todevice ;
17 match 1 0 discard;

Algorithm 2: Translated Click IPSEC script
1 in1 :: FromDevice(netmap:eth1, PROMISC true) ;
2 out2 :: Queue(100)→ToDevice(netmap:eth2) ;
3 elementclass ExactMatch Classiﬁer ;
4 EM::ExactMatch(12/0800,-) ;
5 in1→EM→DecapHeader(14)→MyIPsecESPEncap

→MyAes(1)→IPsecEncap→SetIPChecksum
→MarkIPHeader(OFFSET 0)→ChangeSrcIP
→ChangeDstIP→EncapHeader→out2 ;

6 EM[1]->Discard ;

Algorithm 3: Translated Click source
1 nh data = p→network header();
2 unsigned char* pch = NULL ;
3 pch = nh data ;
4 pch = pch + 0 ;
5 unsigned char* iphlen = pch ;

[24] Dilip A Joseph, Arsalan Tavakoli, and Ion Stoica. A
policy-aware switching layer for data centers. In Proc.
2008 ACM conference on SIGCOMM. 2008.

[25] Zafar Ayyub Qazi, Cheng-Chun Tu, Luis Chiang, Rui
Miao, Vyas Sekar, and Minlan Yu. Simple-fying middle-
box policy enforcement using sdn. In Proc. ACM SIG-
COMM 2013 conference on SIGCOMM, pages 27–38.
2013.

[26] Seyed Kaveh Fayazbakhsh, Luis Chiang, Vyas Sekar,
Minlan Yu, and Jeffrey C Mogul. Enforcing network-
wide policies in the presence of dynamic middlebox ac-
tions using ﬂowtags. In Proc. 2014 USENIX NSDI, 2014.

[27] Dominik Scholz. A look at intels dataplane development

kit. Network, 115, 2014.

[28] Luigi Rizzo. netmap: A novel framework for fast packet
In USENIX Annual Technical Conference, pages

i/o.
101–112, 2012.

[29] Tudor Marian, Ki Suh Lee, and Hakim Weatherspoon.
Netslices: scalable multi-core packet processing in user-
space.
In Proc. eighth ACM/IEEE symposium on Ar-
chitectures for networking and communications systems,
pages 27–38. 2012.

[30] Jinho Hwang, KK Ramakrishnan, and Timothy Wood.
Netvm: high performance and ﬂexible networking using
virtualization on commodity platforms. In Proc. USENIX
NSDI, 2014.

[31] Joao Martins, Mohamed Ahmed, Costin Raiciu,
Vladimir Olteanu, Michio Honda, Roberto Bifulco, and
Felipe Huici. Clickos and the art of network function
virtualization. In Proc. 2014 USENIX NSDI, 2014.

[32] James W Anderson, Ryan Braud, Rishi Kapoor, George
xOMB: Extensible open
In Proc. . 8th

Porter, and Amin Vahdat.
middleboxes with commodity servers.
ACM/IEEE ANCS. 2012.

[33] Wim Meeus, Kristof Van Beeck, Toon Goedem´e, Jan
Meel, and Dirk Stroobandt. An overview of todays high-
level synthesis tools. Design Automation for Embedded
Systems, 16(3):31–51, 2012.

[34] Sixto Ortiz. Software-deﬁned networking: On the verge
of a breakthrough? IEEE Computer, 46(7):10–12, 2013.

[35] Openvswitch. http://openvswitch.org/.

[36] Brandon Blodget, Christophe Bobda, Michael H¨ubner,
and Adronis Niyonkuru. Partial and dynamically re-
conﬁguration of xilinx virtex-ii fpgas.
In Field Pro-
grammable Logic and Application, pages 801–810.
Springer, 2004.

[37] Iperf 2.0-the tcp/udp bandwidth measurement

tool.

https://iperf.fr.

14

Algorithm 4: Ofﬂine Resource Utilization Opti-
mization
Require: Flow volume and service chain requirement,
resource requirement for each SDM implementation

Ensure: The SDM path of each ﬂow

1: Formulate RUOP model according to the input
2: while not all Zl, j,m, f
3:
4:
5:

k,i,n = 0or1 do
Solve RUOP model
Get a forbidden threshold e
for all Zl, j,m, f

do

6:

7:

k,i,n
if Zl, j,m, f

k,i,n < e then
Add one more constraint Zl, j,m, f
RUOP model

k,i,n = 0 into

end if
8:
end for
9:
10: end while
11: return {xk
i }

Algorithm 5: SetIPChecksum (User Deﬁned)
Input: Packet P
Output: P with IP Checksum ﬁeld set

1 @ FIELD iphlen NETWORK 0 UINT8 ;
2 @ FIELD ipcsum NETWORK 10 UINT8 ;
3 @ FIELD ipheader NETWORK 0 DATA ;
4 unsigned char hlen;
5 @ LOAD iphlen hlen ;
6 hlen = hlen & 0x0f;
7 hlen = hlen ≪ 2;
8 @ STORE ipcsum 0 ;
9 unsigned int sum=0;
10 unsigned short hoffset=0;
11 unsigned short tempshort=0;
12 while hoffset < hlen do
13

{ @ LOAD ipheader tempshort
hoffset UINT16 ;
hoffset = hoffset + 2;
sum = sum + tempshort;
}

14
15
16

17 unsigned int tempint = 0;
18 tempint = sum & 0xffff0000;
19 while tempint != 0 do
20
21
22
23
24

{ sum = sum & 0xffff;
tempint = tempint ≫ 16;
sum = sum + tempint;
tempint = sum & 0xffff0000;
}

25 sum = ∼sum;
26 @ STORE ipcsum sum ;

15

Algorithm 6: Translated Xilinx speciﬁcation
1 void ipsec(stream<axiWord> &inData,

stream<axiWord> &outData) {

2 #pragma HLS dataﬂow interval=1
3 #pragma HLS INTERFACE port=inData axis
4 #pragma HLS INTERFACE port=outData axis
5 static stream<axiWord> ippacket(”ippacket”);
6 static stream<axiWord>

decap2espencap(”decap2espencap”);

7 static stream<axiWord>

espencap2aes(”espencap2aes”);

8 static stream<axiWord>

aes2ipencap(”aes2ipencap”);

9 static stream<axiWord>

ipencap2changeSrcIP(”ipencap2changeSrcIP”);

10 static stream<axiWord> changeSr-

cIP2changeDstIP(”changeSrcIP2changeDstIP”);

11 static stream<axiWord> changeD-

stIP2ipchecksum(”changeDstIP2ipchecksum”);

12 static stream<axiWord>

ipchecksum2macencap(”ipchecksum2macencap”);

13 #pragma HLS STREAM variable=ippacket

depth=16

14 #pragma HLS STREAM variable=decap2espencap

depth=16

15 #pragma HLS STREAM variable=espencap2aes

depth=16

16 #pragma HLS STREAM variable=aes2ipencap

depth=16

17 #pragma HLS STREAM

variable=ipencap2changeSrcIP depth=16

18 #pragma HLS STREAM

variable=changeSrcIP2changeDstIP depth=16

19 #pragma HLS STREAM

variable=changeDstIP2ipchecksum depth=16

20 #pragma HLS STREAM

variable=ipchecksum2macencap depth=16

21 parser(inData, ippacket);
22 decap(ippacket, decap2espencap);
23 espencap(decap2espencap, espencap2aes);
24 aes(espencap2aes,aes2ipencap);
25 ipencap(aes2ipencap, ipencap2encap);
26 encap(decap2encap, encap2changeSrcIP);
27 changeSrcIP(encap2changeSrcIP,

changeSrcIP2changeDstIP);

28 changeDstIP(changeSrcIP2changeDstIP,

changeDstIP2ipchecksum);

29 ipchecksum(changeDstIP2ipchecksum, outData);
30 }

