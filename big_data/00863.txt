Optimization via Chebyshev Polynomials

Kareem T. Elgindy∗

Mathematics Department, Faculty of Science, Assiut University, Assiut 71516, Egypt

SUMMARY

A robust line-search method is proposed and analyzed. The method takes on an adaptive search procedure, and combines the
superior accuracy of Chebyshev pseudospectral (PS) approximations with the high-order approximations obtained through
Chebyshev PS differentiation matrices (CPSDMs). In addition, the method exhibits quadratic convergence rate by enforcing an
adaptive Newton search iterative scheme. A rigorous error analysis of the proposed method is presented along with a detailed
set of pseudocodes for the established computational algorithms. Several numerical experiments are conducted on one- and
multi-dimensional optimization test problems to illustrate the advantages of the proposed strategy.

KEY WORDS: Chebyshev polynomials; Differentiation matrix; Line search; One-dimensional optimization; Pseudospectral

method.

6
1
0
2

 
r
a

M
4

 

 
 
]

.

C
O
h
t
a
m

[
 
 

2
v
3
6
8
0
0

.

3
0
6
1
:
v
i
X
r
a

1. INTRODUCTION

The area of optimization received enormous attention in recent years due to the rapid progress in computer technology,
development of user-friendly software, the advances in scientiﬁc computing provided, and most of all, the remarkable
interference of mathematical programming in crucial decision-making problems. One-dimensional optimization or
simply line search optimization is a branch of optimization that is most indispensable, as it forms the backbone
of nonlinear programming algorithms. In particular, it is typical to perform line search optimization in each stage
of multivariate algorithms to determine the best length along a certain search direction; thus, the efﬁciency of
multivariate algorithms largely depends on it. Even in constructing high-order numerical quadratures, line search
optimization emerges in minimizing their truncation errors; thus boosting their accuracy and allowing to obtain very
accurate solutions to intricate boundary-value problems, integral equations, integro-differential equations, and optimal
control problems in short times via stable and efﬁcient numerical schemes; cf. [Elgindy and Smith-Miles (2013b),
Elgindy et al. (2012), Elgindy and Smith-Miles (2013a), Elgindy and Smith-Miles (2013), Elgindy (2016)].

Many line search methods were presented in the literature in the past decades. Some of the most popular line search
methods include interpolation methods, Fibonacci’s method, golden section search method, secant method, Newton’s
method, to mention a few; cf. [Pedregal (2006), Chong and Zak (2013), Nocedal and Wright (2006)]. Perhaps Brent’s
method is considered one of the most popular and widely used line search methods nowadays. It is a robust version
of the inverse parabolic interpolation method that makes the best use of both techniques, the inverse parabolic
interpolation method and the golden section search, and can be implemented in MATLAB Software, for instance,
using the ‘fminbnd’ optimization solver. The method is a robust optimization algorithm that does not require
derivatives; yet it lacks the rapid convergence rate manifested in the derivative methods when they generally converge.
In 2008, [Elgindy and Hedar (2008)] gave a new approach for constructing a line search method using Chebyshev
polynomials that have become increasingly important in scientiﬁc computing, from both theoretical and practical
points of view. They introduced a fast line search method that is an adaptive version of Newton’s method. In
particular, their method forces Newton’s iterative scheme to progress only in descent directions. Moreover, the method
replaces the classical ﬁnite-difference formulas for approximating the derivatives of the objective function by the more

∗Correspondence to: Mathematics Department, Faculty of Science, Assiut University, Assiut 71516, Egypt

This is a preprint copy that has been submitted for publication.

2

KAREEM T. ELGINDY

accurate Chebyshev pseudospectral (PS) differentiation matrices (CPSDMs); thus getting rid of the dependency of
the iterative scheme on the choice of the step-size, which can signiﬁcantly affect the quality of calculated derivatives
approximations. Although the method worked quite well on some test functions, where classical Newton’s method
fail to converge, the method may still suffer from some drawbacks that we discuss thoroughly later in the next section.
In this article, the question of how to construct a line search method based on PS methods is re-investigated and
explored. In particular, we propose a novel line search method based on two global strategies: (i) Approximating
the objective function by an accurate fourth-order Chebyshev interpolant based on Chebyshev-Gauss-Lobatto (CGL)
points; (ii) approximating the derivatives of the function using CPSDMs. The ﬁrst strategy actually plays a signiﬁcant
role in capturing a close proﬁle to the objective function from which we can determine a close initial guess to the
local minimum, since Chebyshev polynomials as basis functions can represent smooth functions to arbitrarily high
accuracy by retaining a ﬁnite number of terms. This approach also improves the performance of the adaptive Newton’s
iterative scheme by starting from a sufﬁciently close estimate; thus moving in a quadratic convergence rate. The
second strategy yields accurate search directions by maintaining very accurate approximations to the derivatives of
the function. The proposed method is also adaptive in the sense of searching only along descent directions, and avoids
the raised drawbacks pertaining to the [Elgindy and Hedar (2008)] method.

The rest of the article is organized as follows: In the next section, we revisit the [Elgindy and Hedar (2008)] method
highlighting its strengths aspects and weaknesses. In Section 3, we provide a new explicit expression for higher-order
CPSDMs based on the successive differentiation of the PS expansion of the Chebyshev interpolant. We present our
novel line search strategy in Section 4 using ﬁrst-order and/or second-order information, and discuss its integration
with multivariate nonlinear optimization algorithms in Section 4.3. Furthermore, we provide a rigorous error and
sensitivity analysis in Sections 5 and 6, respectively. Section 7 veriﬁes the accuracy and efﬁciency of the proposed
method through an extensive sets of one- and multi-dimensional optimization test examples followed by some
conclusions given in Section 8. Some useful background in addition to a necessary pseudocode for calculating the
Chebyshev coefﬁcients of the derivative of Chebyshev interpolant are presented in Appendices A and B, respectively.

2. THE ELGINDY AND HEDAR (2008) OPTIMIZATION METHOD REVISITED

The [Elgindy and Hedar (2008)] line search method is an adaptive method that is considered an improvement over
the standard Newton’s method and the secant method by forcing the iterative scheme to move in descent directions.
The developed algorithm is considered unique as it exploits the peculiar convergence properties of spectral methods,
the robustness of orthogonal polynomials, and the concept of PS differentiation matrices for the ﬁrst time in a one-
dimensional search optimization. In addition, the method avoids the use of classical ﬁnite difference formulas for
approximating the derivatives of the objective function, which are often sensitive to the values of the step-size.

The success in carrying out the above technique lies very much in the linearity property of the interpolation,
differentiation, and evaluation operations. Indeed, since all of these operations are linear, the process of obtaining
approximations to the values of the derivative of a function at the interpolation points can be expressed as a matrix-
vector multiplication. In particular, if we approximate the derivatives of a function f (x) by interpolating the function
with an nth-degree polynomial Pn(x) at, say, the CGL points deﬁned by Eq. (A.7), then the values of the derivative
n(x) at the same (n + 1) points can be expressed as a ﬁxed linear combination of the given function values, and the
P ′
whole relationship may be written in the following matrix form:

P ′

n(x0)
...
n(xn)

P ′




.

(2.1)




=


d(1)
00
...
d(1)
n0

. . .
.. .
···

d(1)
0n
...
d(1)
nn

f (x0)
...

f (xn)










n(x0), P ′

n(x1), . . . , P ′

n(xn)]T , as the values of the derivative at the CGL points, and D(1) = (d(1)

Setting F = [f (x0), f (x1), . . . , f (xn)]T , as the vector consisting of values of f (x) at the (n + 1) interpolation points,
Ψ = [P ′
ij ) , 0 ≤ i, j ≤
n, as the ﬁrst-order differentiation matrix mapping F → Ψ, then Formula (2.1) can be written in the following simple
form,
(2.2)
Eq. (2.2) is generally known as the PS differentiation rule, and it generally delivers better accuracy than standard
ﬁnite-difference rules. We show later in the next section some novel formulas for calculating the elements of

Ψ = D(1)F .

OPTIMIZATION VIA CHEBYSHEV POLYNOMIALS

3

a general qth-order CPSDM, D(q) ∀q ≥ 1. Further information on classical explicit expressions of the entries of
differentiation matrices can be found in [Weideman and Reddy (2000), Baltensperger (2000), Costa and Don (2000),
Elbarbary and El-Sayed (2005)].

A schematic ﬁgure showing the framework of [Elgindy and Hedar (2008)] line search method is shown in Figure
1. As can be observed from the ﬁgure, the method starts by transforming the uncertainty interval [a, b] into [−1, 1]
to exploit the rapid convergence properties provided by Chebyshev polynomials. The method then takes on a global
i=0, using CPSDMs. If the optimality
approach in calculating the derivatives of the function at the CGL points, {xi}n
conditions are not satisﬁed at any of these candidate points, the method then endeavors to locate the best point xm
that minimizes the function, and decides whether to keep or expand the uncertainty interval. An update ˜xm is then
calculated using the descent direction property followed by updating row m in both matrices D(1) and D(2). The
iterative scheme proceeds repeatedly until the stopping criterion is satisﬁed.

Although the method possesses many useful features over classical Newton’s method and the secant method; cf.
[Elgindy and Hedar (2008), Section 8], it may still suffer from the following drawbacks: (i) In spite of the fact that,
D(1) and D(2) are constant matrices, the method requires the calculation of their entire elements beforehand. We show
later that we can establish rapid convergence rates using only one row from each matrix in each iterate. (ii) The method
attempts to calculate the ﬁrst and second derivatives of the function at each point xi ∈ [−1, 1], i = 0, . . . , n. This
could be expensive for large values of n, especially if the local minimum t∗ is located outside the initial uncertainty
interval [a, b]; cf. [Elgindy and Hedar (2008), Table 6], for instance. (iii) The method takes on a global approach for
approximating the derivatives of the objective function using CPSDMs instead of the usual ﬁnite-difference formulas
that are highly sensitive to the values of the step-size. However, the method locates a starting approximation by
distributing the CGL points along the interval [−1, 1], and ﬁnding the point that best minimizes the value of the
function among all other candidate points. We show later in Section 4 that we can signiﬁcantly speed up this process
by adopting another global approach based on approximating the function via a fourth-order Chebyshev interpolant,
and determining the starting approximation through ﬁnding the best root of the interpolant derivative using exact
formulas. (iv) During the implementation of the method, the new approximation to the local minimum, ˜xm, may lie
outside the interval [−1, 1] in some unpleasant occasions; thus the updated differentiation matrices may produce false
approximations to the derivatives of the objective function. (v) To maintain the adaptivity of the method, the authors
proposed to ﬂip the sign of the second derivative f ′′ whenever f ′′ < 0, at some point followed by its multiplication
with a random positive number β. This operation may not be convenient in practice; therefore, we need a more
efﬁcient approach to overcome this difﬁculty. (vi) Even if f ′′ > 0, at some point, the magnitudes of f ′ and f ′′ may
be too small slowing down the convergence rate of the method. This could happen for instance if the function has a
multiple local minimum, or has a nearly ﬂat proﬁle about t∗. (vii) Suppose that t∗ belongs to one side of the real line
while the initial search interval [a, b] is on the other side. According to the presented method, the search interval must
shrink until it converges to the point zero, and the search procedure halts. Such drawbacks motivate us to develop a
more robust and efﬁcient line search method.

3. HIGH-ORDER CPSDMS

In the sequel, we derive a new explicit expression for higher-order CPSDMs based on the successive differentiation of
the PS expansion of the Chebyshev interpolant. The higher derivatives of Chebyshev polynomials are expressed here
in standard polynomial form rather than as Chebyshev polynomial series expansion. The relation between Chebyshev
polynomials and trigonometric functions is used to simplify the expressions obtained, and the periodicity of the cosine
function is used to express it in terms of existing nodes.

The following theorem gives rise to a new useful form for evaluating the mth-derivative of Chebyshev polynomials.

4

KAREEM T. ELGINDY

Figure 1. An illustrative ﬁgure showing the framework of the line search method introduced by [Elgindy and Hedar (2008)] for
minimizing a single-variable function f : [a, b] → R, where {xi}n
i=0 are the CGL points, ti = ((b − a)xi + a + b) /2 ∀i, are
the corresponding points in [a, b], fi = f (ti) ∀i, ε is a relatively small positive number, µ > 1, is a parameter preferably chosen

as 1.618k, where ρ = 1.618 is the golden ratio, and k = 1, 2, . . ., is the iteration counter.

Theorem 3.1
The mth-derivative of the Chebyshev polynomials is given by,

T (m)
k

(x) =




where,

Xl=0

1, m = k = 0,

⌊k/2⌋

l,k c(k)
γ(m)

l xk−2l−m,

cos(cid:16) π

2 (cid:16)k − δ m+1
β(m)
k
2k−1m!, m = k ≥ 1,
0, m > k,

2

0 ≤ m < k ∧ x 6= 0,
2 ⌋(cid:17)(cid:17) ,
,⌊ m+1

0 ≤ m < k ∧ x = 0,

(k − 2l − m + 1) (k − 2l − m + 2)m−1, m ≥ 1,
l = k = 0,

γ(m)

1,
2k−1,

l,k =(cid:26)1, m = 0,
l =


−

c(k)

l = 0 ∧ k ≥ 1,

(k − 2l + 1) (k − 2l + 2)

4l (k − l)

c(k)
l−1,

l = 1, . . . ,⌊k/2⌋ ∧ k ≥ 1,

(3.1a)

(3.1b)

(3.1c)

(3.1d)
(3.1e)

(3.2a)
(3.2b)

(3.3a)
(3.3b)

(3.3c)

OPTIMIZATION VIA CHEBYSHEV POLYNOMIALS

1, m = 0,

(−4)⌊ m−1
2 ⌋(−1)
(cid:18) 1
2(cid:16)k − δ m+1

2

δ m+1

2

,⌊ m+1

2 ⌋+⌊ m+1
2 ⌋k
2 ⌋ + 2(cid:17)(cid:19)⌊ m−1

2 ⌋

,⌊ m+1

, m ≥ 1,

δ m
2

2 ⌋+1(cid:18) 1
,⌊ m

2(cid:16)−k − δ m+1

2

,⌊ m+1

2 ⌋ + 2(cid:17)(cid:19)⌊ m−1

2 ⌋

5

(3.4a)

(3.4b)

×

β(m)
k =




⌊x⌋ is the ﬂoor function of a real number x; (x)l = x(x + 1) . . . (x + l − 1), l ∈ Z+, is the Pochhammer symbol.
Proof
The proof of Eqs. (3.1a), (3.1d), and (3.1e) is straightforward using Eqs. (A.1)-(A.3). We prove Eqs. (3.1b) and (3.1c)
by mathematical induction. So consider the case where 0 ≤ m < k ∧ x 6= 0. For m = 0, and with a bit of manipulation
we can rewrite Eq. (A.2) in the form

For m = 1, we have

Tk(x) =

c(k)
l xk−2l.

⌊k/2⌋

Xl=0

(3.5)

γ(1)
l,k c(k)

l xk−2l−1 =

⌊k/2⌋

Xl=0

⌊k/2⌋

Xl=0

(k − 2l)c(k)

l xk−2l−1 =

d
dx

⌊k/2⌋

Xl=0

c(k)
l xk−2l = T ′

k(x),

so the theorem is true for m = 0 and 1. Now, assume that the theorem is true for m = n, then for m = n + 1, we have

⌊k/2⌋

⌊k/2⌋

γ(n+1)
l,k

c(k)
l xk−2l−n−1 =

Xl=0
Hence Eq. (3.1b) is true for every positive integer m. Now consider the case 0 ≤ m < k ∧ x = 0. The proof of Eq.
(3.1c) for m = 0 is trivial, so consider the case m = 1, where the proof is derived as follows:

(k − 2l − n) γ(n)

l xk−2l−n = T (n+1)

l xk−2l−n−1 =

γ(n)
l,k c(k)

Xl=0

Xl=0

l,k c(k)

(x).

k

⌊k/2⌋

d
dx

β(1)

k cos(cid:16) π

2

(k − 1)(cid:17) = (−1)2k cos(cid:16) π

2

(k − 1)(cid:17) = k sin(cid:18) kπ

2 (cid:19) = T ′

k(0).

Now assume that Eq. (3.1c) is true for m = n. We show that it is also true for m = n + 1. Since

T (n+1)
k

(x) =

dn
dxn T ′

k(x),

then substituting Eq. (A.8) in Eq. (3.6) yields

T (n+1)
k

(x) =

k
2

dn
dxn (φ(x) ψ(x)),

where φ(x) = Tk−1(x) − Tk+1(x) ; ψ(x) = 1/(1 − x2). Since

ψ(n)(0) =(cid:26) 0, n is odd,

n is even,

n!,

then by the general Leibniz rule,

(φ(x) · ψ(x))(n) =

n

Xk=0(cid:18) n

k (cid:19) φ(n−k)(x) ψ(k)(x),

(3.6)

(3.7)

(3.8)

(3.9)

6

KAREEM T. ELGINDY

Eq. (3.7) at x = 0 is reduced to

T (n+1)
k

(0) =

=

k
2

1
2

n

n

1
2

l is even(cid:18) n
l (cid:19) φ(n−l)(0) ψ(l)(0) =
Xl=0
k−1 cos(cid:16) π
(n − l)!(cid:16)β(n−l)
Xl=0
k · n!
cos(cid:16) π
2 (cid:16)k − δ n+2
2 ](cid:17)(cid:17) ,

2 ,[ n+2

l is even

1

= β(n+1)

k

which completes the proof.

k · n!

n

Xl=0

l is even

φ(n−l)(0)
(n − l)!

2 (cid:16)k − 1 − δ n−l+1

2

,⌊ n−l+1

2

k−1 (0) − T (n−l)

n

1

1
2

=

l is even

Xl=0
k · n!
k+1 cos(cid:16) π
⌋(cid:17)(cid:17) − β(n−l)

(n − l)!(cid:16)T (n−l)
2 (cid:16)k + 1 − δ n−l+1

2

k+1 (0)(cid:17)
⌋(cid:17)(cid:17)(cid:17)

,⌊ n−l+1

2

Introducing the parameters,

replaces formulas (A.9) and (A.10) with

θj =(cid:26) 1/2,

1,

j = 0, n,

j = 1, . . . , n − 1,

n

Pn(x) =

θkakTk(x);

ak =

2
n

θjfjTk(xj ),

n

Xk=0
Xj=0

where fj = f (xj) ∀j. Substituting Eq. (3.12) into Eq. (3.11) yields,

Pn(x) =

2
n

n

n

Xk=0

Xj=0

θjθkfjTk(xj )Tk(x).

The derivatives of the Chebyshev interpolant Pn(x) of any order m are then computed at the CGL points by

differentiating (3.13) such that

P (m)

n

(xi) =

2
n

n

n

Xj=0

Xk=0

θjθkfjTk(xj )T (m)

k

(xi) =

n

Xj=0

d(m)
ij fj, m ≥ 0,

(3.10)

(3.11)

(3.12)

(3.13)

(3.14)

(3.15)

where

d(m)
ij =

2θj
n

θkTk(xj)T (m)

k

(xi),

n

Xk=0

are the elements of the mth-order CPSDM. With the aid of Theorem 3.1, we can now calculate the elements of the
mth-order CPSDM using the following useful formula:

d(m)
ij =

2θj
n

n

Xk=0

θkTk(xj)

=

2θj
n

n

Xk=0

θkTk(xj)













2

i

⌊k/2⌋

c(k)
l

cos(cid:16) π

γ(m)
l,k xk−2l−m

1, m = 0 ∧ k = 0
Pl=0
2 (cid:16)k − δ m+1
β(m)
k
2k−1m!, m = k ∧ k ≥ 1
1, m = 0 ∧ k = 0
Pl=0
2 (cid:16)k − δ m+1
β(m)
k
2k−1m!, m = k ∧ k ≥ 1

γ(m)
l,k xk−2l−m

cos(cid:16) π

c(k)
l

⌊k/2⌋

i

2

, m ≥ 0 ∧ m < k ∧ xi 6= 0,
2 ⌋(cid:17)(cid:17) , m ≥ 0 ∧ m < k ∧ xi = 0,

,⌊ m+1

, m ≥ 0 ∧ m < k ∧ i 6= n
2 ,
2 ⌋(cid:17)(cid:17) , m ≥ 0 ∧ m < k ∧ i = n

,⌊ m+1

2 ,

(3.16a)

(3.16b)








.

OPTIMIZATION VIA CHEBYSHEV POLYNOMIALS

Using the periodic property of the cosine function,

we can further rewrite Eqs. (3.16b) as follows:

cos (πx) = (−1)⌊x⌋ cos (π (x − ⌊x⌋)) ∀x ∈ R,

7

(3.17)

d(m)
ij =

2θj
n

n

Xk=0

θk(−1)⌊jk/n⌋xjk−n⌊ jk
n ⌋

⌊k/2⌋

1, m = 0 ∧ k = 0,
c(k)
l γ(m)
l,k xk−2l−m
Pl=0
(−1)⌊σ(m)
2k−1m!, m = k ∧ k ≥ 1

⌋β(m)
k x

n(cid:16)σ(m)

i

k

, m ≥ 0 ∧ m < k ∧ i 6= n
2 ,

k −⌊σ(m)

k

⌋(cid:17), m ≥ 0 ∧ m < k ∧ i = n
2 ,

,





(3.18)






where σ(m)
computing all the off-diagonal elements then applying the formula,

2 ⌋(cid:17) /2. To improve the accuracy of Eqs. (3.18), we can use the negative sum trick,
,⌊ 1+m

k =(cid:16)k − δ 1+m

2

d(m)
ii = −

n

Xj=0

j6=i

d(m)
ij ∀m ≥ 1,

(3.19)

to compute the diagonal elements. Applying the last trick gives the formula,

d(m)
ij =

2θj
n

n

Xk=0

θk(−1)⌊jk/n⌋xjk−n⌊ jk
n ⌋

, m ≥ 0 ∧ m < k ∧ i 6= n

2 ∧ i 6= j,
⌋(cid:17), m ≥ 0 ∧ m < k ∧ i = n

k

2 ∧ i 6= j,

Hence, the elements of the ﬁrst- and second-order CPSDMs are given by,

d(1)
ij =

2θj
n

n

Xk=1

θk(−1)⌊jk/n⌋xjk−n⌊ jk
n ⌋

k > 1 ∧ i 6= n
k > 1 ∧ i = n

2 ∧ i 6= j,
2 ∧ i 6= j,














k

i

⌊k/2⌋

s=0
s6=i

i = j

d(m)
is ,

n(cid:16)σ(m)

1,
⌊k/2⌋

k −⌊σ(m)

⌋β(m)
k x

l,k xk−2l−m

1, m = 0 ∧ k = 0 ∧ i 6= j,
c(k)
l γ(m)
Pl=0
(−1)⌊σ(m)
2k−1m!, m = k ∧ k ≥ 1 ∧ i 6= j,
−Pn


Pl=0
−(−1)⌊ k
−Pn

Pl=0
(−1)⌊ k−1
−Pn
k = 2 ∧ i 6= j,
(k − 2l − 1) (k − 2l) c(k)

k = 1 ∧ i 6= j,
(k − 2l) c(k)
2 ⌋k xn( k−1

2 ⌋k2 xn( k
d(2)
is ,

2 −⌊ k−1
i = j

l xk−2l−1

2 −⌊ k
i = j

4,
⌊k/2⌋

2⌋),

2 ⌋),

d(1)
is ,

s=0
s6=i

s=0
s6=i

i

,

.





(3.20)

,

(3.21)





,





(3.22)

d(2)
ij =

2θj
n

n

Xk=2

θk(−1)⌊jk/n⌋xjk−n⌊ jk
n ⌋

respectively.

i

,

l xk−2l−2
k > 2 ∧ i = n

k > 2 ∧ i 6= n
2 ∧ i 6= j,

2 ∧ i 6= j,

4. PROPOSED LINE SEARCH METHOD

In this section we present a novel line search method that we shall call the Chebyshev PS line search method
(CPSLSM). The key idea behind our new approach is ﬁve-fold:

8

KAREEM T. ELGINDY

1. express the function as a linear combination of Chebyshev polynomials,
2. ﬁnd its derivative,
3. ﬁnd the derivative roots,
4. determine a local minimum in [−1, 1];
5. ﬁnally reverse the change of variables to obtain the approximate local minimum of the original function.

So to begin our illustration for the method, let us denote by Pn the space of polynomials of degree at most n, and
suppose that we want to ﬁnd a local minimum t∗ of a twice-continuously differentiable single-variable function f (t)
on a ﬁxed interval [a, b] to within a certain accuracy ε. Using the change of variable,

(4.1)
we transform the interval [a, b] into [−1, 1]. Now let I4f ∈ P4, be the fourth-degree Chebyshev interpolant of f at the
CGL points such that,

x = (2t − a − b)/(b − a),

4

I4f (x; a, b) =

˜fk Tk(x).

Xk=0

k=0 via the discrete Chebyshev transform,

We can determine the Chebyshev coefﬁcients { ˜fk}4
cos(cid:18) k j π
n (cid:19) b
Xj=0
n ck " 1
2(cid:16)b
afn(cid:17) +
af0 + (−1)k b

˜fk =

1
cj

n ck

afj

=

2

2

4

1
cj

3

Xj=1

cos(cid:18) k j π

n (cid:19) b

afj# ,

where b

afj = f (xj; a, b), j = 0, . . . , 4, and,

We approximate the derivative of f by the derivative of its interpolant I4f (x; a, b),

ck =(cid:26) 2,

1,

k = 0, n,
k = 1, . . . , n − 1.

I ′

4f (x; a, b) =

˜f (1)
k Tk(x),

4

Xk=0

where the coefﬁcients { ˜f (1)
k }4
[Kopriva (2009)],

k=0 are akin to the coefﬁcients of the original function, ˜fk, by the following recursion

˜ck ˜f (1)

k = ˜f (1)

k+2 + 2 (k + 1) ˜fk+1,

k ≥ 0;

˜ck = (cid:26) ck,

1,

k = 0, . . . , n − 1,
k = n.

We can calculate { ˜f (1)
k }4
in Eq. (4.5) to get the cubic algebraic equation,

k=0 efﬁciently using Algorithm 2. Now we collect the terms involving the same powers of x

where,

I ′

4f (x; a, b) = A1 x3 + A2 x2 + A3 x + A4,

(4.2)

(4.3)

(4.4)

(4.5)

(4.6)

(4.7)

(4.8)

(4.9a)

(4.9b)

(4.9c)

(4.9d)

A1 = 4 ˜f (1)
3 ,
A2 = 2 ˜f (1)
2 ,
1 − 3 ˜f (1)
A3 = ˜f (1)
3 ;
A4 = ˜f (1)
0 − ˜f (1)
2 .

Let εc and εmach denote a relatively small positive number and the machine precision that is approximately equals
2.2204 × 10−16 in double precision arithmetic, respectively. To ﬁnd a local minimum of f, we consider the following
three cases:

OPTIMIZATION VIA CHEBYSHEV POLYNOMIALS

9

Case 1: If |A1| < εc ∧ A2 < εc, then I ′

4f (x; a, b) is either linear (or nearly linear) or quadratic (or nearly quadratic)
so that the second derivative of the interpolant is negative and its graph is concave down. Therefore, an
approximate local minimum is located at one of the endpoints of the interval [a, b]. Hence, if f (a) < f (b),
we set t∗ ≈ a, and stop the search procedure; otherwise we set t∗ ≈ b, and stop.

Case 2: If |A1| < εc ∧ A2 ≥ εc, then I ′

4f (x; a, b) is quadratic such that the second derivative of the interpolant is
positive and its graph is concave up (simply convex). We carry out one iteration of the golden section search
method on the interval [a, b] to determine a smaller interval [a1, b1] with candidate local minimum ˜t. If the length
of the new interval is below ε, we set t∗ ≈ ˜t, and stop. Otherwise, we calculate the the ﬁrst- and second-order
derivatives of f at the point ˜x1 in the interval [−1, 1] deﬁned by,

˜x1 = (2 ˜t − a1 − b1)/(b1 − a1).

(4.10)

To this end, we construct the row CPSDMs D(1) =(cid:16)d(1)
m ∈ Z+ using the following formulas:

j (cid:17) and D(2) =(cid:16)d(2)

d(1)
j =

2θj
m

m

Xk=1

θk(−1)⌊jk/m⌋xjk−m⌊ jk
m⌋

j (cid:17) of length (m + 1), for some


k > 1 ∧ ˜x1 6= 0 ∧ j 6= m,

k > 1 ∧ ˜x1 = 0 ∧ j 6= m,

,



(4.11)











,

1

2 ⌋),

1,
⌊k/2⌋

s=0 d(1)
s ,

l ˜xk−2l−1

2 −⌊ k−1
j = m

k = 1 ∧ j 6= m,
(k − 2l) c(k)
2 ⌋k xm( k−1

Pl=0
(−1)⌊ k−1
−Pm−1
k = 2 ∧ j 6= m,
(k − 2l − 1) (k − 2l) c(k)
Pl=0
−(−1)⌊ k
−Pm−1

2 ⌋k2 xm( k

2 −⌊ k
j = m

s=0 d(2)
s ,

4,
⌊k/2⌋

2⌋),

F ′ ≈ D(1)F ,
F ′′ ≈ D(2)F ,
a1 f ′′

d(2)
j =

2θj
m

m

Xk=2

θk(−1)⌊jk/m⌋xjk−m⌊ jk
m⌋

l ˜xk−2l−2

1

,

k > 2 ∧ ˜x1 6= 0 ∧ j 6= m,

k > 2 ∧ ˜x1 = 0 ∧ j 6= m,

(4.12)
respectively. The computation of the derivatives can be carried out easily by multiplying D(1) and D(2) with
the vector of function values; that is,

,





(4.13a)
(4.13b)

where F ′ = [b1
we update ˜x1 according to the following formula,

m]T ; F ′′ = [b1

0, . . . , b1

0 , . . . , b1

a1 f ′′

a1f ′

a1f ′

m]T . Following the [Elgindy and Hedar (2008)] approach,

At this stage, we check the stopping criterion,

˜x2 = ˜x1 −

D(1) F

D(2) F

.

If it is fulﬁlled, we set

|˜x2 − ˜x1| ≤ εx =

2ε

b1 − a1

.

(4.14)

(4.15)

t∗ ≈ ((b1 − a1) ∗ ˜x2 + a1 + b1)/2,

and stop. Otherwise, if |˜x2| > 1, we repeat the procedure again starting from the construction of a fourth-degree
Chebyshev interpolant of f (x; a1, b1) using the CGL points. The third scenario appears when the magnitudes
of both D(1)F and D(2)F are too small, which may appear as we mentioned earlier when the proﬁle of the
function f is too ﬂat near the current point, or if the function has a multiple local minimum. In this case, the
convergence rate of the Chebyshev-Newton iterative scheme (4.14) is no longer quadratic, but rather linear. We
therefore suggest here to apply Brent’s method. To reduce the length of the search interval though, we consider
the following two cases:

10

KAREEM T. ELGINDY

• If ˜x2 > ˜x1, then we carry out Brent’s method on the interval [((b1 − a1)˜x1 + a1 + b1)/2, b1]. Notice
that the direction from ˜x1 into ˜x2 is a descent direction, since the second derivative of the interpolant,
I ′′

4f (x; a1, b1) ≈ f ′′(x; a1, b1) is positive for all x ∈ [−1, 1], as shown by [Elgindy and Hedar (2008)].

• If ˜x2 < ˜x1, then we carry out Brent’s method on the interval [a1, ((b1 − a1)˜x1 + a1 + b1)/2].

If none of the above three scenarios appear, we compute the ﬁrst- and second-order derivatives of the interpolant
at ˜x2 as discussed before, set ˜x1 := ˜x2, and repeat the iterative formula (4.14).
Remark 4.1
The tolerance εx is chosen to satisfy the stopping criterion with respect to the variable t, since

where,

|˜x2 − ˜x1| ≤ εx ⇒(cid:12)(cid:12)

˜t2 − ˜t1(cid:12)(cid:12) ≤ ε,

˜ti = ((b1 − a1) ∗ ˜xi + a1 + b1)/2,

i = 1, 2.

Remark 4.2
To reduce the round-off errors in the calculation of ˜x2 through Eq. (4.14), we prefer to scale the vector
of function values F if any of its elements is large. That is, we choose a maximum value Fmax, and set
a1 fj(cid:12)(cid:12) > Fmax. This procedure does not alter the value of ˜x2, since
F := F /max0≤j≤m(cid:12)(cid:12)
the scaling of F is canceled out through division.
Case 3: If |A1| ≥ εc, then the derivative of the Chebyshev interpolant is cubic. To avoid overﬂows, we scale
4f (x; a, b) by dividing each coefﬁcient with the coefﬁcient of largest magnitude if the

the coefﬁcients of I ′
magnitude of any of the coefﬁcients is larger than unity. This procedure ensures that,

if max0≤j≤m(cid:12)(cid:12)

a1fj(cid:12)(cid:12)

b1

b1

(4.16)

max
1≤j≤4|Aj| ≤ 1.

(4.17)

The next step is divided into two subcases:
Subcase I: If any of the three roots {¯xi}3

i=1 of the cubic polynomial I ′

4f (x; a, b), is a complex number, or
the magnitude of any of them is larger than unity, we perform one iteration of the golden section search
method on the interval [a, b] to determine a smaller interval [a1, b1] with candidate local minimum ˜t. Again
and as we showed before for the quadratic case, if the length of the new interval is below ε, we set t∗ ≈ ˜t,
and stop. Otherwise, we calculate the point ˜x1 using Eq. (4.10).
Subcase II: If all of the roots are real, distinct, and lie within the interval [−1, 1], we ﬁnd the root that

minimizes the value of f among all three roots; that is, we calculate,

˜x1 = arg min
1≤i≤3

f (¯xi; a, b) .

(4.18)

j (cid:17) using Eqs. (4.11) and (4.12), and
We then update both rows of the CPSDMs D(1) =(cid:16)d(1)
calculate the ﬁrst- and second- derivatives of the Chebyshev interpolant using Eqs. (4.13). If D(2)F > εmach,
then Newton’s direction is a descent direction, and we follow the same procedure presented in Case 2 except
when |˜x2| > 1. To update the uncertainty interval [a, b], we determine the second best root among all three
roots; that is, we determine,
(4.19)

j (cid:17) and D(2) =(cid:16)d(2)

˜x2 = arg min
1≤i≤3

f (¯xi; a, b) : ˜x2 6= ˜x1.

Now if ˜x1 > ˜x2, we replace a with ((b − a)˜x2 + a + b)/2. Otherwise, we replace b with ((b − a)˜x2 + a + b)/2.
The method then proceeds repeatedly until it converges to the local minimum t∗, or the number of iterations
exceeds a preassigned value, say kmax.
It is noteworthy to mention that the three roots of the cubic polynomial, I ′
the trigonometric method due to Franc¸ois Vi`ete; cf. [Nickalls (2006)]. In particular, let

4f (x), can be exactly calculated using

p =

q =

,

1

A1(cid:19)2
3(cid:18) A2
A3
A1 −
A1(cid:19)3
27(cid:18) A2
2
−

A2A3
3A2
1

+

A4
A1

,

(4.20)

(4.21)

OPTIMIZATION VIA CHEBYSHEV POLYNOMIALS

and deﬁne,

C(p, q) = 2r−

p
3

cos(cid:18) 1

3

cos−1(cid:18) 3q

2pr−3

p (cid:19)(cid:19) .

Then the three roots can be easily computed using the following useful formulas,

where,

¯xi = ¯ti −

A2
3A1

,

i = 1, 2, 3,

¯t1 = C(p, q),
¯t3 = −C(p,−q);
¯t2 = −¯t1 − ¯t3.

11

(4.22)

(4.23)

(4.24a)
(4.24b)
(4.24c)

If the three roots {¯xi}3
¯x1 > ¯x2 > ¯x3.

i=1 are real and distinct, then it can be shown that they satisfy the inequalities

Remark 4.3
The derivative of the Chebyshev interpolant, I ′
all its zeros in (−1, 1); cf. [Peherstorfer (1995)].

4f (x), has all three simple zeros in (−1, 1) ifP3

˜f (1)
k xk, has

k=0

The CPSLSM can be implemented efﬁciently using Algorithms 1–5.

4.1. Locating an Uncertainty Interval

It is important here to mention that the proposed method can easily work if the uncertainty interval is not known a
priori. In this case the user inputs any initial interval, say [˜a, ˜b]. We can then divide the interval into some l uniform
subintervals using (l + 1) equally-spaced nodes {ti}l
i=0. We then evaluate the function f at those points and ﬁnd the
point tj that minimizes f such that,

Now we have the following three cases:

tj = arg min
0≤i≤l

f (ti).

• If 0 < j < l, then we set ˜a = tj−1 and ˜b = tj+1, and return.
• If

j = 0,

then if ˜a > 0 we divide ˜a by ρk, where k is

shown by
[Elgindy and Hedar (2008)]. However, to avoid drawback (vii) in Section 2, we replace the calculated ˜a with
−1/˜a if ˜a < 1. Otherwise, we multiply ˜a by ρk. In both cases we set ˜b = t1, and repeat the search procedure.
• If j = l, then if ˜b > 0 we multiply ˜b by ρk. Otherwise, we divide ˜b by ρk and replace the calculated ˜b with −1/˜b
if ˜b > −1. In both cases we set ˜a = tl−1, and repeat the search procedure.

the iteration number as

The above procedure should proceed repeatedly until an uncertainty interval [a, b] is located, or the number of
iterations exceeds kmax.

4.2. The Line Search Method Using First-Order Information Only

Suppose that we want to ﬁnd a local minimum t∗ of a differentiable single-variable function f (t) on a ﬁxed interval
[a, b] to within a certain accuracy ε. Moreover, suppose that the second-order information is not available. We can
slightly modify the method presented in Section 4 to work in this case. In particular, in Case 2, we carry out one
iteration of the golden section search method on the interval [a, b] to determine a smaller interval [a1, b1] with two
candidate local minima ˜t1 and ˜t2: f (˜t2) < f (˜t1). If the length of the new interval is below ε, we set t∗ ≈ ˜t2, and stop.
Otherwise, we calculate the ﬁrst-order derivatives of f at the two points ˜x1 and ˜x2 deﬁned by Eqs. (4.16). We then
calculate,

s1 =

(˜x2 − ˜x1)
F − D

(1)
1

D

(1)
2

,

F

(4.25)

12

KAREEM T. ELGINDY

(1)
where D
1
[Elgindy and Hedar (2008)] approach, we calculate the secant search direction,

are the ﬁrst-order CPSDMs corresponding to the points ˜x1 and ˜x2, respectively. Following

(1)
and D
2

and update ˜x2 according to the following formula,

s2 = −s1 · D

(1)
2

F ,

We check the stopping criterion,

If it is fulﬁlled, we set

˜x3 = ˜x2 + s2.

|˜x3 − ˜x2| ≤ εx.

(4.26)

(4.27)

and stop. Otherwise, if |˜x3| > 1, we repeat the procedure again starting from the construction of a fourth-degree
Chebyshev interpolant of f (x; a1, b1) at the CGL points. The third scenario appears when the magnitudes of both s2
and 1/s1 are too small. We therefore suggest here to apply Brent’s method as we consider the following two cases:

t∗ ≈ ((b1 − a1) ∗ ˜x3 + a1 + b1)/2,

• If ˜x3 > ˜x2, then we carry out Brent’s method on the interval [((b1 − a1)˜x2 + a1 + b1)/2, b1].
• If ˜x3 < ˜x2, then we carry out Brent’s method on the interval [a1, ((b1 − a1)˜x2 + a1 + b1)/2].

i=1, set ˜x2 := ˜x3, and repeat the iterative formula (4.27).

(1)
If none of the above three scenarios appear, we replace D
1
interpolant at ˜x3, update {si}2
In Case 3 (Subcase I), we apply one iteration of the golden section search method on the interval [a, b] to determine
a smaller interval [a1, b1] with candidate local minima ˜t1 and ˜t2 : f (˜t2) < f (˜t1). If the length of the new interval is
below ε, we set t∗ ≈ ˜t2, and stop. Otherwise, we calculate the two points ˜x1 and ˜x2 using Eqs. (4.16). For Subcase
II, we ﬁnd the best root, ˜x2, that minimizes the value of f among all three roots. Then suppose that ˜x2 = ¯xJ, for
some J = 1, 2, 3. To calculate the secant search direction, we need another point ˜x1 within the interval [−1, 1]. This
can be easily resolved by making use of the useful inequalities ¯x1 > ¯x2 > ¯x3. In particular, we proceed as follows:

F, calculate the ﬁrst-order derivative of the

(1)
F by D
2

• If J = 1, then xJ > x2 > x3, and we set ˜x1 = ˜x2 − (˜x2 − ¯x2)/ρ2.
• If J = 2, then x1 > xJ > x3, and we set ˜x1 = ˜x2 − (˜x2 − ¯x3)/ρ2.
• If J = 3, then x1 > x2 > xJ , and we set ˜x1 = ˜x2 + (¯x2 − ˜x2)/ρ2.

(1)
2 , and calculating the ﬁrst-
This procedure is then followed by updating both rows of the CPSDMs, D
i=1. We then update s1 using Eq. (4.25). If
order derivatives of the Chebyshev interpolant at the two points, {˜xi}2
s1 > εmach, then the secant direction is a descent direction, and we follow the same procedure discussed before. The
method then proceeds repeatedly until it converges to the local minimum t∗, or the number of iterations exceeds
the preassigned value, kmax. Notice that the convergence rate of the CPSLSM using ﬁrst-order information only is
expected to be slower than its partner using second-order information, since it performs the secant iterative formula
as one of its ingredients rather than Newton’s iterative scheme; thus the convergence rate degenerates from quadratic
to superlinear.

and D

(1)
1

Remark 4.4
It is inadvisable to assign the value of ˜x1 to the second best root that minimizes the value of f, since the function
proﬁle could be changing rapidly near ˜t2; thus 1/s1 yields a poor approximation to the second derivative of f. In fact,
applying this procedure on the rapidly varying function f7 (see Section 7) near t = 0 using the CPSLSM gives the
poor approximate solution t∗ ≈ 5.4 with f7(t∗) ≈ −0.03.
4.3. Integration With Multivariate Nonlinear Optimization Algorithms

The proposed CPSLSM can be integrated easily with multivariate nonlinear optimization algorithms. Consider, for
instance, the most popular quasi-Newton algorithm for solving unconstrained nonlinear optimization problems widely
known as Broyden-Fletcher-Goldfarb-Shanno (BFGS) algorithm. Algorithm 6 implements a modiﬁed BFGS method
endowed with a line search method, where a scaling of the search direction vector, pk, is applied at each iteration
whenever its size exceeds a prescribed value pmax. This step is required to avoid multiplication with large numbers;
thus maintaining the stability of the numerical optimization scheme. Practically, the initial approximate Hessian

OPTIMIZATION VIA CHEBYSHEV POLYNOMIALS

13

matrix B0 can be initialized with the identity matrix I, so that the ﬁrst step is equivalent to a gradient descent,
but further steps are more and more reﬁned by Bk, k = 1, 2, . . .. To update the search direction at each iterate, we
can easily calculate the approximate inverse Hessian matrix, B−1
k , for each k = 1, 2, . . ., by applying the Sherman-
Morrison formula [Sherman and Morrison (1949)] giving,

B−1

k+1 = B−1

k yk + yT
k

k + (cid:0)sT

B−1

k yk(cid:1)(cid:0)sksT
k(cid:1)
k yk(cid:1)2
(cid:0)sT

B−1

k + skyT
k yksT
k
sT
k yk

−

B−1

k

,

where,

sk = αkpk,

instead of typically solving the linear system,

yk = ∇f(cid:16)x(k+1)(cid:17) − ∇f(cid:0)xk(cid:1) ,

(4.28)

(4.29)

(4.30)

(4.31)

Bkpk = −∇f(cid:16)x(k+1)(cid:17) ,

k = 1, 2, . . . .

Since pk is a descent direction at each iteration, we need to adjust the CPSLSM to search for an approximate local
minimum αk in R+. To this end, we assign a relatively small positive number to the left endpoint of the initial
uncertainty interval [a, b], and allow the interval to expand as discussed before, but only rightward the real line of
numbers.

the coefﬁcient A2 obtained by the CPSLSM. In particular, the value of the second derivative of the function f could
actually be above the prescribed value εc whereas the computed value of the second derivative of the interpolant could

During the implementation of a line search method within a multivariate optimization algorithm, an approximate
this procedure involves the
multiplication of αk by pk, and the addition of the result to the current state vector x(k). Due to round-off error

optimal step length αk is determined by minimizing f(cid:0)x(k) + αkpk(cid:1). Clearly,
propagation, one would expect a perturbation in the value of f(cid:0)x(k) + αk pk(cid:1) that may slightly affect the value of
be below εc. So if |A1| < εc; i.e. f(cid:0)x(k) + αkpk(cid:1) happens to be a linear/quadratic function in αk, and f (a) < f (b),
the CPSLSM approximates αk by a, and the modiﬁed BFGS algorithm may halt prematurely due to the expected
slight change in the state vector (could be smaller than the user prescribed tolerance). To avoid such a difﬁculty, we
can easily perform Algorithm 3 assuming always A2 ≥ εc, as a safety measure. Notice here that if the true value of
A2 is less than εc using exact arithmetic, the CPSLSM can still locate an approximate local minimum αk in one step,
as its foundation is based on a descent Newton strategy.

Remark 4.5
Our practical experience shows that Algorithm 3 can be carried out safely for a single-variable function f.
The assumption that A2 ≥ εc is only recommended when integrating the CPSLSM with a multivariate nonlinear
optimization algorithm, where we seek to minimize a multivariate function f.

5. ERROR ANALYSIS

A major step in implementing the proposed CPSLSM lies in the interpolation of the objective function f using
the CGL points. In fact, Eq. (4.2) is exact for all polynomials hn(t) ∈ P4 allowing faster convergence rates than
the standard quadratic and cubic interpolation methods. Moreover, expressing a function in terms of Chebyshev
polynomials is better conditioned than expressing it by its monomial coefﬁcients. In particular, the transformation
between a polynomial of degree n in [−1, 1] and its expansion coefﬁcients with respect to the monomials

has O(cid:16)(cid:0)1 + √2(cid:1)n+1(cid:17) condition number with respect to maximum norms over [−1, 1]. On the contrary, the

transformation has O(n) condition number with respect to Chebyshev polynomials; cf. [Day and Romero (2005)].
From another point of view, the CGL points have a number of pleasing advantages as one of the most commonly
used node distribution in spectral methods and numerical discretizations. They include the two endpoints, −1 and 1,
so they cover the whole search interval [−1, 1]. Moreover, it is well known that the Lebesgue constant gives an idea
of how good the interpolant of a function is in comparison with the best polynomial approximation of the function.

14

KAREEM T. ELGINDY

Using Theorem 3.4 in [Hesthaven (1998)], we can easily deduce that the Lebesgue constant, ΛCGL
using the CGL set {xi}4
the optimal canonical nodal set. In particular, ΛCGL

, for interpolation
i=0, is uniformly bounded by those obtained using the Gauss nodal set that is close to that of

is bounded by,

4

4

ΛCGL

4

<

γ log(100) + log(1024/π2)

π log(10)

+ α3 ≈ 1.00918 + α3,

(5.1)

where, γ = 0.57721566 . . ., represents Euler’s constant, and 0 < α3 < π/1152.

5.1. Rounding Error Analysis for the Calculation of the CPSDMs

In this section we address the effect of round-off errors encountered in the calculation of the elements d(1)
given by,

01 and d(2)

01

n ⌋ +

θk (−1)⌊k/n⌋xk−n ⌊ k

n ⌋ +

xk−n ⌊ k
n ⌋

⌊k/2⌋

Xl=0

n ⌋ +

θk(−1)⌊k/n⌋xk−n⌊ k

d(1)
0,1 =

=

d(2)
0,1 =

=

2

2

n 
(−1)⌊1/n⌋x1−n⌊ 1
n 
(−1)⌊1/n⌋x1−n⌊ 1
n 
4(−1)⌊2/n⌋x2−n⌊ 2
n
4(−1)⌊2/n⌋x2−n⌊ 2

2

2

n

n−1

Xk=2
Xk=2
Xk=3
Xk=3

n−1

n

n ⌋

⌊k/2⌋

l 
(k − 2l) C(k)
Xl=0
 ,
l 
(k − 2l) C(k)
 − n, n ≥ 2
l 
(k − 2l − 1) (k − 2l) C(k)
 ,

Xl=0

⌊k/2⌋

n ⌋

l 
(k − 2l − 1) (k − 2l) C(k)
 −

1
3

(5.2)

(5.4)

(5.5)

n ⌋ +

⌊k/2⌋

xk−n⌊ k
n ⌋

Xl=0

n(cid:0)n2 − 1(cid:1) , n ≥ 3

(5.3)

, respectively, since they are the major elements with regard to their values. Accordingly, they bear the major error
responsibility comparing to other elements. So let δ ≈ 1.11 × 10−16, be the round-off unity in the double-precision
k=0 are the computed values, and
ﬂoating-point system, and assume that {x∗
{δk}n

k=0, are the corresponding round-off errors such that,

k=0 are the exact CGL points, {xk}n

k}n

x∗
k = xk + δk ∀k,

with |δk| ≤ δ ∀k. If we denote the exact elements of D(1) and D(2) by d(1)∗

i,j and d(2)∗

i,j ∀i, j, respectively, then

∗

d(1)
01

− d(1)

01 =

≤

=

δk−n ⌊ k
n ⌋

n−1

n ⌋ +

Xk=2
l 
(k − 2l) C(k)


2

n−1

2 δ

⌊k/2⌋

n 
(−1)⌊1/n⌋δ1−n⌊ 1
n 
Xl=0
1 +
n  1 +
k2! =

Xk=2
Xk=2

2 δ

n−1

δ
3

(n − 1)(2 n − 1) = O(cid:0)n2δ(cid:1) .

⌊k/2⌋

Xl=0

l 
(k − 2l) C(k)


OPTIMIZATION VIA CHEBYSHEV POLYNOMIALS

15

Moreover,

∗

d(2)
01

− d(2)

01 =

≤

=

n−1

⌊k/2⌋

n ⌋ +

δk−n⌊ k
n ⌋

l 
(k − 2l − 1) (k − 2l) C(k)
Xl=0

l 
(k − 2l − 1) (k − 2l) C(k)


2

n−1

2 δ

⌊k/2⌋

Xk=3

n
4(−1)⌊2/n⌋δ2−n⌊ 2
n 
Xk=3
Xl=0
4 +
Xk=3(cid:0)k4 − k2(cid:1)! =
n  4 +

1
3

2 δ

n−1

δ

15(cid:0)−2 + 5n − 5n3 + 2n4(cid:1) = O(n4δ).

(5.6)

Remark 5.1
It is noteworthy to mention here that the round-off error in the calculation of d(1)
01 from the classical Chebyshev
differentiation matrix is of order O(n4δ); cf. [Canuto et al. (1988), Baltensperger and Trummer (2003)]. Hence,
Formulas (3.21) are better numerically. Moreover, the upper bounds (5.5) and (5.6) are in agreement with those
obtained by [Elbarbary and El-Sayed (2005)].

6. SENSITIVITY ANALYSIS

The following theorem highlights the conditioning of a given root ¯xi, i = 1, 2, 3, with respect to a given coefﬁcient
Aj, j = 1, 2, 3, 4, using the Chebyshev basis polynomials.

4f (x) be a cubic polynomial, and suppose that ¯xi is a simple nonzero root of I ′

Theorem 6.1
Let I ′
4f (x), for some i = 1, 2; 3.
Moreover, assume that Aj 6= 0, for some = 1, 2, 3, 4, then the relative condition number, κij, of ¯xi with respect to Aj
is given by,

κij =(cid:12)(cid:12)(cid:12)(cid:12)

Aj Tj(¯xi)

j ¯xi Uj−1(¯xi)(cid:12)(cid:12)(cid:12)(cid:12)

,

(6.1)

where Uj(x) is the jth-degree Chebyshev polynomial of the second kind. Moreover, κij is bounded by the following
inequality,

Proof
Using [Tsai (2014), Theorem 2], we can easily show that,

κij ≤

1

j |¯xi Uj−1(¯xi)|

.

Eq. (6.1) and Inequality (6.2) follow directly via the identity,

κij =(cid:12)(cid:12)(cid:12)(cid:12)

Aj Tj(¯xi)
¯xi T ′

j(¯xi)(cid:12)(cid:12)(cid:12)(cid:12)

.

and Condition (4.17).

T ′
j(x) = j Uj−1(x) ∀ − 1 ≤ x ≤ 1,

7. NUMERICAL EXPERIMENTS

(6.2)

(6.3)

(6.4)

In the following two sections we show our numerical experiments for solving two sets of one- and multi-dimensional
optimization test problems. All numerical experiments were conducted on a personal laptop equipped with an
Intel(R) Core(TM) i7-2670QM CPU with 2.20GHz speed running on a Windows 10 64-bit operating system, and
the numerical results were obtained using MATLAB Software V. R2014b (8.4.0.150421).

16

KAREEM T. ELGINDY

7.1. One-Dimensional Optimization Test Problems
In this section, we ﬁrst apply the CPSLSM using second-order information on the seven test functions, {fi}7
considered earlier by [Elgindy and Hedar (2008)], in addition to the following test function,

i=1,

f8(t) = (t − 3)12 + 3t4.

The plots of the test functions are shown in Figure 2. The exact local minima and their corresponding optimal
function values obtained using MATHEMATICA 9 Software accurate to 15 digits precision are shown in Table
I. All of the results are presented against the widely used MATLAB ‘fminbnd’ optimization solver to assess the
accuracy and efﬁciency of the current work. We present the number of correct digits cdn := −log10(cid:12)(cid:12)fi(t∗) − fi(˜t∗)(cid:12)(cid:12)
,
obtained for each test function, where ˜t∗ is the approximate solution obtained using the competing line search
solvers, the CPSLSM and fminbnd solver. The CPSLSM was carried out using m = 12,Fmax = 100, εc = 10−3, ε =
10−10, kmax = 100, and the magnitudes of both D(1)F and D(2)F were considered too small if their values
fall below 10−1. The fminbnd solver was implemented with the termination tolerance ‘TolX’ set at 10−10. The
j=1, for the considered test functions are listed in respective order as follows:
starting uncertainty intervals {Ij}8
I1 = [0, 10], I2 = [0, 20], I3 = [1, 5], I4 = [0, 5], I5 = [1, 20], I6 = [0.5, 5], I7 = [−10, 10]; I8 = [8, 10].

Function

t∗

f1(t) = t4 − 8.5 t3 − 31.0625 t2 − 7.5 t + 45
f2(t) = (t + 2)2(t + 4)(t + 5)(t + 8)(t − 16)

f3(t) = et − 3 t2,
t > 0.5
f4(t) = cos(t) + (t − 2)2

f5(t) = 3774.522/t + 2.27 t − 181.529,
t > 0

f6(t) = 10.2/t + 6.2 t3,

t > 0

f7(t) = −1/(1 + t2)
f8(t) = (t − 3)12 + 3 t4

8.27846234384512
12.6791200596419 −4.36333999223710× 106
2.83314789204934
2.35424275822278
40.7772610902992
0.860541475570675

Optimal function value
−2271.58168119200
−7.08129358237484
−0.580237420623167
3.59976534995851
15.8040029284830

0
8

−1

2.44152913000000× 108

Table I. The one-dimensional test functions together with their corresponding local minima and optimal values.

Figure 3 shows the cdn obtained using the CPSLSM and fminbnd solver for each test function. Clearly, the
CPSLSM establishes more accuracy than the fminbnd solver in general with the ability to exceed the required
precision in the majority of the tests. Moreover, the CPSLSM was able to ﬁnd the exact local minima for f7 and
f8. The experiment conducted on the test function f5 is even more interesting, because it manifests the adaptivity
of the CPSLSM to locate a search interval bracketing the solution when the latter does not lie within the starting
uncertainty interval. On the other hand, the fminbnd solver was stuck in the starting interval, and failed to locate
the solution. For test function f6, we observe a gain in accuracy in favor of the fminbnd solver. Notice though that
the approximate solution ˜t∗ = 0.86053413225062, obtained using the CPSLSM in this case yields the function value
15.8040029302092 that is accurate to 9 signiﬁcant digits.

Figure 4 further shows the number of iterations, k, required by both methods to locate the approximate minima
given the stated tolerance ε. The ﬁgure conspicuously shows the power of the novel optimization scheme observed in
the rapid convergence rate, as the CPSLSM requires about half the iterations number required by the fminbnd solver
in the majority of the test functions. The gap is even much wider for test functions f5, f7; f8.

Figures 5 and 6 show the cdn and the number of iterations required by the CPSLSM using only ﬁrst-order
information versus fminbnd solver. Here we notice that the obtained cdn values using the CPSLSM are almost
identical with the values obtained using second-order information with a slight increase in the number of iterations
required for some test functions as expected.

7.2. Multi-Dimensional Optimization Test Problems

The functions listed below are some of the common functions and data sets used for testing multi-dimensional
unconstrained optimization algorithms:

OPTIMIZATION VIA CHEBYSHEV POLYNOMIALS

17

×106

15

10

5

0

)
t
(
2
f

0

5
t

10

-5

0

10 20
t

)
t
(
6
f

800

600

400

200

0

)
t
(
1
f

0

-500

-1000

-1500

-2000

-2500

)
t
(
5
f

700
600
500
400
300
200
100
0

60
50
40
30
20
10
0
-10

0

-0.2

-0.4

-0.6

-0.8

)
t
(
3
f

)
t
(
7
f

2

4

t

)
t
(
4
f

8

6

4

2

0

)
t
(
8
f

14
12
10
8
6
4
2
0

5

0

t
×109

20 40

t

2

4

t

-10

0
t

10

8

10

9
t

Figure 2. The plots of the test functions.

n
d
c

15

14

13

12

11

10

9

8

7

6

5

4

3

2

1

0

CPSLSM
fminbnd

f1

f2

f3

f4

f5

f6

f7

f8

Figure 3. The cdn for the CPSLSM and fminbnd solver.

• Sphere Function:

f1(x) =

d

Xi=1

x2
i ,

d ∈ Z+.

The global minimum function value is f (x∗) = 0, obtained at x∗ = [0, . . . , 0]T .

18

KAREEM T. ELGINDY

40

35

30

25

k

20

15

10

5

0

CPSLSM
fminbnd

f1

f2

f3

f4

f5

f6

f7

f8

Figure 4. The number of iterations required by the CPSLSM and fminbnd solver.

n
d
c

15
14
13
12
11
10
9
8
7
6
5
4
3
2
1
0

CPSLSM
fminbnd

f1

f2

f3

f4

f5

f6

f7

f8

Figure 5. The cdn for the CPSLSM using ﬁrst-order information and fminbnd solver.

• Bohachevsky Function:

f2(x) = x2

1 + 2 x2

2 − 0.3 cos(3 πx1) − 0.4 cos(4 πx2) + 0.7.

The global minimum function value is f (x∗) = 0, obtained at x∗ = [0, 0]T .

• Booth Function:

The global minimum function value is f (x∗) = 0, obtained at x∗ = [1, 3]T .

f3(x) = (x1 + 2 x2 − 7)2 + (2 x1 + x2 − 5)2.

• Three-Hump Camel Function:

f4(x) = 2 x2

1 − 1.05 x4

1 + x6

1/6 + x1 x2 + x2
2.

OPTIMIZATION VIA CHEBYSHEV POLYNOMIALS

19

40

35

30

25

k

20

15

10

5

0

CPSLSM
fminbnd

f1

f2

f3

f4

f5

f6

f7

f8

Figure 6. The number of iterations required by the CPSLSM using ﬁrst-order information and fminbnd solver.

The global minimum function value is f (x∗) = 0, obtained at x∗ = [0, 0]T .

• Powell Function:

d/4

f5(x) =

Xi=1(cid:16)(x4i−3 + 10 x4i−2)2 + 5 (x4i−1 − x4i)2 + (x4i−2 − 2 x4i−1)4 + 10 (x4i−3 − x4i)4(cid:17),

d ∈ Z+.

The global minimum function value is f (x∗) = 0, obtained at x∗ = [0, . . . , 0]T .

• Goldstein-Price Function:

f6(x) =(cid:16)1 + (x1 + x2 + 1)2 (cid:0)19 − 14 x1 + 3 x1
×(cid:16)30 + (2 x1 − 3 x2)2 (cid:0)18 − 32 x1 + 12 x2

2(cid:1)(cid:17)
2 − 14 x2 + 6 x1 x2 + 3 x2
2(cid:1)(cid:17) .
1 + 48 x2 − 36 x1 x2 + 27 x2

The global minimum function value is f (x∗) = 3, obtained at x∗ = [0,−1]T .

• Styblinski-Tang Function:

f7(x) =

1
2

d

Xi=1(cid:0)x4

i − 16 x2

i + 5 xi(cid:1),

d ∈ Z+.

The global minimum function value is f (x∗) = −39.16599 d, obtained at x∗ = [−2.903534, . . . ,−2.903534]T .

• Easom Function:

f8(x) = − cos(x1) cos(x2) e−(x1−π)2−(x2−π)2
The global minimum function value is f (x∗) = −1, obtained at x∗ = [π, π]T .

.

Table II shows a comparison between the modiﬁed BFGS method endowed with MATLAB “fminbnd” line
search solver (MBFGSFMINBND) and the modiﬁed BFGS method integrated with the present CPSLSM
(MBFGSCPSLSM) for the multi-dimensional test functions fi, i = 1, . . . , 8. The modiﬁed BFGS method was
performed using Algorithm 6 with B0 = I, kmax = 104, and pmax = 10. The gradients of the objective functions
were approximated using central difference approximations with step-size 10−4. Both fminbnd method and CPSLSM
were initiated using the uncertainty interval [3 ε, 10]. The maximum number of iterations allowed for each line search
method was 100. Each line search method was considered successful at each iterate k if the change in the approximate
step length αk is below 10−6. The CPSLSM was carried out using m = 6, εc = εmach, εD = 10−6, and Fmax = 100.

20

KAREEM T. ELGINDY

Moreover, both MBFGSFMINBND and MBFGSCPSLSM were stopped whenever,

or,

< 10−12,

< 10−12.

(cid:13)(cid:13)(cid:13)∇f(cid:16)x(k)(cid:17)(cid:13)(cid:13)(cid:13)2
x(k+1) − x(k)(cid:13)(cid:13)(cid:13)2
(cid:13)(cid:13)(cid:13)

Table II clearly manifests the power of the CPSLSM, where the running time and computational cost of the modiﬁed
BFGS method can be signiﬁcantly reduced.

Function

x(0)

f1(x) (d = 4)
[50, 1, 4,−100]T
f1(x) (d = 100)

[50, 1, 4, 2.5, . . . , 2.5,−100]T

f2(x)
[10, 20]T

f3(x)
[2, 2]T

f4(x)
[−0.5, 1]T
f5(x) (d = 4)

[2, 3, 1, 1]T

f6(x)
[−0.5, 1]T
f7(x) (d = 4)
[−4,−4, 5, 5]T
f7(x) (d = 12)

[3,−0.5, 1.278, 1, . . . 1, 0.111, 4.5]T

f8(x)
[1, 1]T

MBFGSFMINBND

NI/fval

x∗

MBFGSCPSLSM

NI/fval

x∗

[−3.0928e − 16,−6.186e − 18,−2.4723e − 17, 6.1814e − 16]T

13/4.784e − 31

[2.6019e − 15, 5.2024e − 17, 2.0815e − 16,−5.2038e − 15]T

2/3.3895e − 29

13/3.8088e − 31

Omitted

412/0.88281

[0.61861,−0.46953]T

1/6.3109e − 30

[1, 3]T

NI exceeded 10000

2/7.3153e − 30

Omitted

16/0.46988

[6.001e − 09, 0.46953]T

1/0
[1, 3]T

5/1.8396e − 32

[−9.7238e − 17, 5.6191e − 18]T

[2.3123e − 06,−2.3123e − 07, 1.0163e − 06, 1.0163e − 06]T

29/5.4474e − 23

[3.7073e − 07,−3.7073e − 08, 1.6667e − 07, 1.6667e − 07]T

28/3.6165e − 26

NI exceeded 10000

576/ − 128.39

[−2.9035,−2.9035, 2.7468, 2.7468]T

308/ − 342.76

Omitted

NI exceeded 10000

53/3

[5.3617e − 09,−1]T

11/ − 128.39

[−2.9035,−2.9035, 2.7468, 2.7468]T

35/ − 342.76

Omitted

3/ − 1

[3.1416, 3.1416]T

Table II. A comparison between the BFGSFMINBND and the BFGSCPSLSM for the multi-dimensional test functions
fi, i = 1, . . . , 8. “NI” denotes the number of iterations required by the modiﬁed BFGS algorithm, and “fval” denotes the

approximate minimum function value.

8. CONCLUSION

In this article, we presented a novel CPSLSM that can be efﬁciently integrated with the current state of the art
multivariate optimization methods. The accurate approximation to the objective function, quadratic convergence rate
to the local minimum, and the relatively inexpensive computation of the derivatives of the function using CPSDMs
are some of the many useful features possessed by the method. Numerical comparisons with the popular rival Brent’s
method verify further the effectiveness of the proposed method. The CPSLSM is a robust method that adds more
power to the arsenal of line search methods by signiﬁcantly reducing the running time and computational cost of
multivariate optimization algorithms.

OPTIMIZATION VIA CHEBYSHEV POLYNOMIALS

21

A. PRELIMINARIES

In this section, we present some useful results from approximation theory. The ﬁrst kind Chebyshev polynomial (or
simply the Chebyshev polynomial) of degree n, Tn(x), is given by the explicit formula,

using trigonometric functions, or in the following explicit polynomial form [Snyder (1966)]:

Tn(x) = cos(cid:0)n cos−1(x)(cid:1) ∀x ∈ [−1, 1],

where,

T k
n−2k xn−2k,

⌊n/2⌋

1
2

Tn(x) =

Xk=0
m = 2m(−1)k(cid:26) m + 2 k

T k

m + k (cid:27)(cid:18) m + k

k

(cid:19) , m, k ≥ 0.

The Chebyshev polynomials can be generated by the three-term recurrence relation,

(A.4)
Tn+1(x) = 2xTn(x) − Tn−1(x),
starting with T0(x) = 1 and T1(x) = x. They are orthogonal in the interval [−1, 1] with respect to the weight function
w(x) =(cid:0)1 − x2(cid:1)−1/2, and their orthogonality relation is given by,

n ≥ 1,

π
2
where c0 = 2, cn = 1, n ≥ 1, and δnm is the Kronecker delta function deﬁned by,

hTn, Tmiw =Z 1

2 dx =

−1

cnδnm,

Tn(x) Tm(x)(cid:0)1 − x2(cid:1)− 1
δnm =(cid:26) 1, n = m,
0, n 6= m.

The roots (aka Chebyshev-Gauss points) of Tn(x) are given by,

and the extrema (aka CGL points) are deﬁned by,

xk = cos(cid:18) 2k − 1

2n

π(cid:19) ,

k = 1, . . . , n,

xk = cos(cid:18) kπ
n (cid:19) ,

k = 0, 1, . . . , n.

(A.6)

(A.7)

derivative

The
[Mason and Handscomb (2003)]:

Tn(x)

of

can

be

obtained

in

terms

of Chebyshev

polynomials

as

follows

d
dx

Tn(x) =

n
2

Tn−1(x) − Tn+1(x)

1 − x2

,

|x| 6= 1.

(A.8)

[Clenshaw and Curtis (1960)] showed that a continuous function f (x) with bounded variation on [−1, 1] can be

approximated by the truncated series,

(A.1)

(A.2)

(A.3)

(A.5)

(A.9)

(A.10)

where,

(Pnf )(x) =

′′

ak Tk(x),

n

Xk=0

ak =

2
n

n

Xj=0

′′

fj Tk(xj), n > 0,

fj = f (xj)∀j, and the summation symbol with double primes denotes a sum with both the ﬁrst and last terms halved.
For a smooth function f, the Chebyshev series (A.9) exhibits exponential convergence faster than any ﬁnite power of
1/n [Gottlieb and Orszag (1977)].

22

KAREEM T. ELGINDY

B. PSEUDOCODES OF DEVELOPED COMPUTATIONAL ALGORITHMS

REFERENCES

[Baltensperger (2000)]. Baltensperger, R., 2000. Improving the accuracy of the matrix differentiation method for arbitrary collocation points.

[Baltensperger and Trummer (2003)]. Baltensperger, R., Trummer, M. R., 2003. Spectral differencing with a twist. SIAM Journal on Scientiﬁc

[Canuto et al. (1988)]. Canuto, C., Hussaini, M. Y., Quarteroni, A., Zang, T. A., 1988. Spectral Methods in Fluid Dynamics. Springer series

Applied Numerical Mathematics 33 (1), 143–149.

Computing 24 (5), 1465–1487.

in computational physics. Springer-Verlag.

[Chong and Zak (2013)]. Chong, E. K., Zak, S. H., 2013. An introduction to optimization. Vol. 76. John Wiley & Sons.
[Clenshaw and Curtis (1960)]. Clenshaw, C. W., Curtis, A. R., 1960. A method for numerical integration on an automatic computer.

[Costa and Don (2000)]. Costa, B., Don, W. S., 2000. On the computation of high order pseudospectral derivatives. Applied Numerical

Numerische Mathematik 2, 197–205.

Mathematics 33 (1), 151–159.

numerical analysis 43 (5), 1969–1987.

Numerical Mathematics 55 (4), 425–438.

[Day and Romero (2005)]. Day, D., Romero, L., 2005. Roots of polynomials expressed in terms of orthogonal polynomials. SIAM journal on

[Elbarbary and El-Sayed (2005)]. Elbarbary, E. M., El-Sayed, S. M., 2005. Higher order pseudospectral differentiation matrices. Applied

[Elgindy (2016)]. Elgindy, K. T., 2016. High-order numerical solution of second-order one-dimensional hyperbolic telegraph equation using

a shifted Gegenbauer pseudospectral method. Numerical Methods for Partial Differential Equations 32 (1), 307–349.

[Elgindy and Hedar (2008)]. Elgindy, K. T., Hedar, A., 15 December 2008. A new robust line search technique based on Chebyshev

polynomials. Applied Mathematics and Computation 206 (2), 853–866.

[Elgindy and Smith-Miles (2013a)]. Elgindy, K. T., Smith-Miles, K. A., 2013a. Fast, accurate, and small-scale direct trajectory optimization

using a Gegenbauer transcription method. Journal of Computational and Applied Mathematics 251 (0), 93–116.

[Elgindy and Smith-Miles (2013b)]. Elgindy, K. T., Smith-Miles, K. A., 2013b. Optimal Gegenbauer quadrature over arbitrary integration

nodes. Journal of Computational and Applied Mathematics 242 (0), 82 – 106.

[Elgindy and Smith-Miles (2013)]. Elgindy, K. T., Smith-Miles, K. A., 2013. Solving boundary value problems, integral, and integro-

differential equations using Gegenbauer integration matrices. Journal of Computational and Applied Mathematics 237 (1), 307–325.

[Elgindy et al. (2012)]. Elgindy, K. T., Smith-Miles, K. A., Miller, B., 2012. Solving optimal control problems using a Gegenbauer
transcription method. Accepted for publication in the proceedings of 2012 Australian Control Conference, AUCC 2012, to be held
in University of New South Wales, Sydney, Australia, on November 15–16.

[Gottlieb and Orszag (1977)]. Gottlieb, D., Orszag, S. A., 1977. Numerical Analysis of Spectral Methods: Theory and Applications. No. 26

in CBMS-NSF Regional Conference Series in Applied Mathematics. SIAM, Philadelphia.

[Hesthaven (1998)]. Hesthaven, J. S., 1998. From electrostatics to almost optimal nodal sets for polynomial interpolation in a simplex. SIAM

Journal on Numerical Analysis 35 (2), 655–676.

Engineers. Springer, Berlin.

[Kopriva (2009)]. Kopriva, D. A., 2009. Implementing Spectral Methods for Partial Differential Equations: Algorithms for Scientists and

[Mason and Handscomb (2003)]. Mason, J. C., Handscomb, D. C., 2003. Chebyshev Polynomials. Chapman & Hall/CRC, Boca Raton.
[Nickalls (2006)]. Nickalls, R., 2006. Vi`ete, Descartes and the cubic equation. The Mathematical Gazette, 203–208.
[Nocedal and Wright (2006)]. Nocedal, J., Wright, S., 2006. Numerical optimization. Springer Science & Business Media.
[Pedregal (2006)]. Pedregal, P., 2006. Introduction to optimization. Vol. 46. Springer Science & Business Media.
[Peherstorfer (1995)]. Peherstorfer, F., 1995. Zeros of linear combinations of orthogonal polynomials. In: Mathematical Proceedings of the

Cambridge Philosophical Society. Vol. 117. Cambridge Univ Press, pp. 533–544.

[Sherman and Morrison (1949)]. Sherman, J., Morrison, W. J., 1949. Adjustment of an inverse matrix corresponding to changes in the
elements of a given column or a given row of the original matrix. In: Annals of Mathematical Statistics. Vol. 20. INST MATHEMATICAL
STATISTICS IMS BUSINESS OFFICE-SUITE 7, 3401 INVESTMENT BLVD, HAYWARD, CA 94545, pp. 621–621.

[Snyder (1966)]. Snyder, M. A., 1966. Chebyshev methods in numerical approximation.
[Tsai (2014)]. Tsai, E., 2014. A method for reducing ill-conditioning of polynomial root ﬁnding using a change of basis.
[Weideman and Reddy (2000)]. Weideman, J. A. C., Reddy, S. C., 2000. A MATLAB differentiation matrix suite. ACM Transactions of

Mathematical Software 26 (4), 465–519.

OPTIMIZATION VIA CHEBYSHEV POLYNOMIALS

23

Algorithm 1 The CPSLSM Algorithm Using Second-Order Information
Input: Positive integer number m; objective function f; maximum function value Fmax; uncertainty interval
Ensure: The local minimum t∗ ∈ [a, b].

endpoints a, b; relatively small positive numbers εc, εD, ε; maximum number of iterations kmax.

ρ1 ← 1.618033988749895; ρ2 ← 2.618033988749895; e+ = a + b; e− = b − a; k ← 0; xj ← cos (jπ/4) ,
0, . . . , 4.
Calculate c(k)

, l = 0, . . . ,⌊k/2⌋ , k = 0, . . . , 4 using Eqs. (3.3); {θk, ˜ck}4

k=0 using Eqs. (3.10) & (4.7), respectively.

j =

l

j = 0, . . . , m.

if m 6= 4 then
else

xD,j ← cos (jπ/m) ,
xD,j ← xj ,

j = 0, . . . , 4.

end if
while k ≤ kmax do
ﬂag ← 0; F x ← {f ((e−xj + e+) /2)}4
if Fx,max > Fmax then
F x ← F x/Fx,max.
end if
k o4
Calculaten ˜fk, ˜f (1)
(4.9a) & (4.9b), respectively.
if |A1| < εc then
else

Call Algorithm 3

k=0

j=0 ;Fx,max ← kF xk∞.

, using Eqs. (4.3) & Algorithm 2, respectively, & the coefﬁcients {Aj}2

j=1 using Eqs.

j=3 using Eqs. (4.9c) & (4.9d); Amax ← max

1≤j≤4 |Aj|.

Calculate {Aj}4
if Amax > 1 then
Aj ← Aj/Amax,

j = 1, . . . , 4.

i=1.

end if
Calculate the roots {¯xi}3
if ¯xi is complex or |¯xi| > 1 for any i = 1, 2, 3 then
else

k ← k + 1; Call Algorithm 4; e+ ← a + b; ˜x1 ← (2 ˜t1 − e+)/e−; ﬂag ← 1.
Determine ˜x1 using Eq. (4.18).

end if
Compute F ′ & F ′′ at ˜x1.
if F ′′ > εmach then
Call Algorithm 5.

end if
if ﬂag = 1 then

continue

else

Compute ˜x2 using Eq. (4.19).
if ˜x1 > ˜x2 then

else

a ← (e− ˜x2 + e+)/2.
b ← (e− ˜x2 + e+)/2.

end if

end if
e+ ← a + b; e− ← b − a; k ← k + 1.

end if

end while
Output(‘Maximum number of iterations exceeded.’).
return t∗.

24

KAREEM T. ELGINDY

Algorithm 2 Calculating the Chebyshev Coefﬁcients of the Derivative of a Polynomial Interpolant
Input: The Chebyshev coefﬁcients { ˜fk}4

k=0.

˜f (1)
4 ← 0.
˜f (1)
3 ← 8 ˜f4.
˜f (1)
k ← 2 (k + 1) ˜fk+1 + ˜f (1)
k+2,
0 ← ˜f1 + ˜f (1)
˜f (1)
2 /2.
return { ˜f (1)
k=0.
k }4

k = 2; 1.

Algorithm 3 Linear/Quadratic Case
Input: m; f ; a; b; e−; ρ1; ρ2; A2; k; kmax;{xD,j}m

if A2 < εc then

j=0;Fmax; εc; εD; ε.

if f (a) < f (b) then

else

t∗ ← a; Output(t∗); Stop.
t∗ ← b; Output(t∗); Stop.

end if

else

k ← k + 1; Call Algorithm 4;
e+ ← a + b; ˜x1 ← (2˜t1 − e+)/e−; F ← {f ((e−xD,j + e+) /2)}m
if F1,max > Fmax then
F ← F /F1,max.
end if
Compute F ′ & F ′′ at ˜x1 using Eqs. (4.11)–(4.13); Call Algorithm 5.
continue

j=0; F1,max ← kFk∞.

end if

Algorithm 4 One-Step Golden Section Search Algorithm
Input: f ; a; b; e−; ρ1; ρ2; ε.

t1 ← a + e−/ρ2; t2 ← a + e−/ρ1.
if f (t1) < f (t2) then

else

b ← t2; t2 ← t1; e− ← b − a; t1 ← a + e−/ρ2.
a ← t1; t1 ← t2; e− ← b − a; t2 ← a + e−/ρ1.

end if
if f (t1) < f (t2) then

else

˜t1 ← t1; b ← t2.
˜t1 ← t2; a ← t1.

end if
e− ← b − a.
if e− ≤ ε then
else

t∗ ← ˜t1; Output(t∗); Stop.
return.

end if
return ˜t1, a, b, e−.

OPTIMIZATION VIA CHEBYSHEV POLYNOMIALS

25

Algorithm 5 The Chebyshev-Newton Algorithm
Input: m; f ; a; b; e−; e+; ˜x1; k; kmax; εD; ε; F; F ′; F ′′.

while k ≤ kmax do

Determine ˜x2 using Eq. (4.14); k ← k + 1.
if Condition (4.15) is satisﬁed then

t∗ ← (e− ˜x2 + e+)/2.
Output(t∗); Stop.
else if |˜x2| > 1 then
F ′(cid:12)(cid:12) < εD and(cid:12)(cid:12)
else if(cid:12)(cid:12)

if ˜x2 > ˜x1 then

break

else

F ′′(cid:12)(cid:12) < εD then

Apply Brent’s method on the interval [(e− ˜x1 + e+)/2, b]; Output(t∗); Stop.

Apply Brent’s method on the interval [a, (e− ˜x1 + e+)/2]; Output(t∗); Stop.

end if

else

Calculate F ′&F ′′ at ˜x2 using Eqs. (4.13); ˜x1 ← ˜x2.

end if

end while
return k, ˜x1.

Algorithm 6 Modiﬁed BFGS Algorithm With a Line Search Strategy
Input: Objective function f; initial guess x(0); an approximate Hessian matrix B0; maximum number of iterations

kmax; maximum direction size pmax.
k ← 0; calculate B−1
x∗ ← x(k) if the convergence criterion is satisﬁed.
pk ← −∇f(cid:0)x(k)(cid:1) ; pnorm ← kpkk2.
if pnorm > pmax then
pk ← pk/pnorm. {Scaling}

k , and the gradient vector ∇f(cid:0)x(k)(cid:1).

end if
while k ≤ kmax do

Perform a line search to ﬁnd an acceptable step-size αk in the direction pk.
sk ← αkpk.
x(k+1) ← x(k) + αkpk. {Update the state vector}
Calculate ∇f(cid:0)x(k+1)(cid:1), and set x∗ ← x(k+1) if the convergence criterion is satisﬁed.
yk ← ∇f(cid:0)x(k+1)(cid:1) − ∇f(cid:0)x(k)(cid:1) ; t ← sT
B−1
k+1 ← B−1
k yk(cid:1)(cid:0)sksT
pk+1 ← −B−1
if pnorm > pmax then

k +(cid:0)t + yT
k+1∇f(cid:0)x(k+1)(cid:1) ; pnorm ← kpk+1k2.

k yk; T1 ← yksT
k(cid:1)/t2 −(cid:0)T2 + TT

k ; T2 ← B−1
2(cid:1) /t.

T1.

k

B−1

k

pk+1 ← pk+1/pnorm. {Scaling}

end if
k ← k + 1.

end while
Output(‘Maximum number of iterations exceeded.’).
return x∗; f (x∗).

