6
1
0
2

 
r
a

 

M
8
1

 
 
]
T
I
.
s
c
[
 
 

1
v
6
3
7
5
0

.

3
0
6
1
:
v
i
X
r
a

Construction of polar codes for arbitrary discrete

memoryless channels

Talha Cihad Gulcu

Min Ye

Alexander Barg

1

Abstract

It is known that polar codes can be efﬁciently constructed for binary-input channels. At the same time, existing
algorithms for general input alphabets are less practical because of high complexity. We address the construction problem
for the general case, and analyze an algorithm that is based on successive reduction of the output alphabet size of the
subchannels in each recursion step. For this procedure we estimate the approximation error as O(µ−1/(q−1)), where µ
is the “quantization parameter,” i.e., the maximum size of the subchannel output alphabet allowed by the algorithm. The
complexity of the code construction scales as O(N µ4), where N is the length of the code.

We also show that if the polarizing operation relies on modulo-q addition, it is possible to merge subsets of output
symbols without any loss in subchannel capacity. Performing this procedure before each approximation step results in a
further speed-up of the code construction, and the resulting codes have smaller gap to capacity. We show that a similar
acceleration can be attained for polar codes over ﬁnite ﬁeld alphabets.

Experimentation shows that the suggested construction algorithms can be used to construct long polar codes for alphabets

of size q = 16 and more with acceptable loss of the code rate for a variety of polarizing transforms.

Index terms: Channel degrading, Greedy symbol merging, Polarizing transforms.

I. INTRODUCTION

Arıkan’s polar codes [1] form the ﬁrst explicit family of binary codes that achieve the capacity of binary-input channels.
Polar codes rely on a remarkable phenomenon called channel polarization. After their introduction, both polar codes and
the channel polarization concept have been used in a vast range of problems in information theory [2]–[11]. A drawback
of the original proposal of [1] is that the construction of codes is not efﬁcient because the alphabet of bit subchannels
grows exponentially as a function of the number of iterations of the polarization procedure, resulting in an exponential
complexity of construction.

The difﬁculty of selecting subchannels for information transmission with polar codes was recognized early on in a
number of papers. According to an observation made in [12], the construction procedure of polar codes for binary-input
channels relies on essentially the same density evolution procedure that plays a key role in the analysis of low-density
parity-check codes. It was soon realized that the proposal of [12] requires increasing precision of the computations, but
this paper paved way for later research on the construction problem.

An important step was taken in [13] which suggested to approximate each bit-channel after each evolution step by
its degraded or upgraded version whose output alphabet size is constrained by a speciﬁed threshold µ that serves as
a parameter of the procedure. As a result, [13] put forward an approximation procedure that results in a code not
too far removed from the ideal choice of the bit-channels of [1]. This code construction scheme has a complexity of
O(N µ2 log µ), where N = 2n is the code length. For the channel degradation method described in [13], an error analysis
and approximation guarantees are provided in [14].

Another approximation scheme for the construction of binary codes was considered in [15]. It is based on degrading
each bit-channel after each evolution step, performed by merging several output symbols into one symbol based on
quantizing the curve pX|Y (0|y) vs h(pX|Y (0|y)), where pX|Y is the conditional distribution of the “reverse channel”
that corresponds to the bit-channel in question. Symbols of the output alphabet that share the same range of quantization
are merged into a single symbol of the approximating channel. Yet another algorithm based on bit-channel upgrading
was described in [16], in which the authors argue that it is possible to obtain a channel which is arbitrarily close to the
bit-channel of interest in terms of the capacity. However, no error or complexity analysis is provided in this work.

Moving to general input alphabets, let us mention a code construction algorithm based on degrading the subchannels
in each evolution step designed in [17]. This algorithm involves a merging procedure of output symbols similarly to [15].
However, as noted by the authors, their construction scheme is practical only for small values of input alphabet size q,
The authors are with Dept. of ECE and ISR, University of Maryland, College Park, MD 20742, USA. Emails: {tcgulcu,yeemmi}@gmail.com,
abarg@umd.edu. A. Barg is also with Inst. Probl. Inform. Trans. (IITP), Moscow, Russia. Research supported in part by NSF grants CCF1217245 and
CCF1422955.

2

its efﬁciency constrained by the complexity of order O(µq). Paper [18] proposed to perform the upgrading instead of
degrading of the subchannels, but did not manage to reduce the implementation complexity. In [19], the authors consider
another channel upgrading method for nonbinary-input channels, but stop short of providing an explicit construction
scheme or error analysis.

Papers [20], [21], [22] addressed the construction problem of polar codes for AWGN channels. These works are based
on Gaussian approximation of the intermediate likelihood ratios and do not analyze the error guarantees or rate loss of the
obtained codes. A comparative study of various polar code constructions for AWGN channel is presented in [23]. Some
other heuristic constructions for binary-input channels similar to the cited results for the Gaussian channel appear in
[24], [25], [26]. Note also constructions of polar codes for some particular channels [27], [28], for various transformation
kernels [29], [30], [31], and concatenated codes [32], [33].

In this paper we present a construction method of polar codes for input alphabets of arbitrary size, together with explicit
analysis of approximation error and construction complexity. In particular, the complexity estimate of our procedure grows
as O(N µ4) as opposed to O(N µq) in earlier works. Our algorithm can be viewed as a generalization of the channel
degradation method in [13] to nonbinary input channels. Although the approach and the proof methods here are rather
different from earlier works, the estimate of the approximation error that we derive generalizes the error bound given by
[14] for the binary case. Another interesting connection with the literature concerns a very recent result of [34] which
derives a lower bound on the alphabet size µ that is necessary to restrict the capacity loss by at most a given value .
This bound is valid for any approximation procedure that is based only on the degrading of the subchannels in each
evolution step. The construction scheme presented here relies on the value µ that is not too far from this theoretical limit
(see Proposition 3 for more details). We stress that we aim at approximating symmetric capacity of the channels, and
do not attempt to construct or implement polar codes that attain Shannon capacity, which is greater than the symmetric
one for non-symmetric channels.

Our paper is organized as follows. In Section II we give a brief overview of polar codes including various polarizing
transformations for nonbinary alphabets. The rate loss estimate in the code construction based on merging pairs of
output symbols in a greedy way is derived in Section III. in Section IV we argue that output symbols whose posterior
probability vectors are cyclic shifts of each other can be merged with no rate loss. This observation enables us to
formulate an improved version of the construction algorithm that further reduces the construction complexity. We have
also implemented our algorithms and constructed polar codes for various nonbinary alphabets. These results are presented
in Section V. For relatively small q we can construct rather long polar codes (for instance, going to length 106 for q = 5
takes several hours). For larger q such as 16 we can reach lengths of tens of thousands within reasonable time and with
low rate loss. Even in this case, by increasing the gap to capacity of the resulting codes, we can reach lengths in the
range of hundreds of thousands to a million without putting an effort in optimizing our software.

II. PRELIMINARIES ON POLAR CODING

We begin with a brief overview of binary polar codes. Let W be a channel with the output alphabet Y, input alphabet
X = {0, 1}, and the conditional probability distribution WY |X (·|·). Throughout the paper we denote the capacity and
the symmetric capacity of W by C(W ) and I(W ), respectively. We say W is symmetric if WY |X (y|1), y ∈ Y can be
obtained from WY |X (y|0), y ∈ Y through a permutation π : Y → Y such that π2 = id. Note that if W is symmetric then
(cid:16) 1 0
I(W ) = C(W ).
For N = 2n and n ∈ N, deﬁne the polarizing matrix (or the Arıkan transform matrix) as GN = BN F ⊗n, where
, ⊗ is the Kronecker product of matrices, and BN is a “bit reversal” permutation matrix [1]. In [1], Arıkan
F =
showed that given a symmetric and binary input channel W , an appropriate subset of the rows of GN can be used as a
generator matrix of a linear code that achieves the capacity of W as N → ∞.
Given a binary-input channel W , deﬁne the channel W N with input alphabet {0, 1}N and output alphabet YN by the

(cid:17)

1 1

conditional distribution

where W (.|.) is the conditional distribution that deﬁnes W . Deﬁne a combined channel(cid:102)W by the conditional distribution

i=1

N(cid:89)

W N (yN|xN ) =

W (yi|xi)

(cid:102)W (yN|uN ) = W N (yN|uN GN ).

3

(1)

(2)

(6)

(7)

(8)

(9)

Based on this equality, information can be transmitted over the good-bit channels while the remaining bits are ﬁxed to
some values known in advance to the receiver (in polar coding literature they are called frozen bits). The transmission
scheme can be described as follows: A message of k = |GN (W, β)| bits is written in the bits ui, i ∈ GN (W, β). The
remaining N − k bits are set to 0. This determines the sequence uN which is transformed into xN = uN GN , and the
vector xN is sent over the channel. Denote by yN the sequence received on the output. The decoder ﬁnds an estimate
of uN by computing the values ˆui, i = 1, . . . , N as follows:

(cid:40)
argmaxu∈{0,1}Wi(yN , ˆui−1
0,

1

ˆui =

|u),

if i ∈ GN (W, β),
if i ∈ BN (W, β).

The results of [1], [35] imply the following upper bound on the error probability Pe = Pr(ˆuN (cid:54)= uN ) :

Pe ≤ (cid:88)

i∈GN (W,β)

Z(Wi) ≤ N 2−N β

2 − , and  > 0 is arbitrarily small. This describes the basic construction of polar codes [1] which attains
where β = 1
symmetric capacity I(W ) of the channel W with a low error rate. At the same time, (1), (6) highlight the main obstacle
in the way of efﬁciently constructing polar codes: the size of the output alphabet of the channels Wi is of the order 22N ,
so it scales exponentially with the code length. For this reason, ﬁnding a practical code construction scheme of polar
codes represents a nontrivial problem.

Concluding the introduction, let us mention that the code construction technique presented below can be applied to
any polarizing transform based on combining pairs of subchannels. There has been a great deal of research on properties
of polarizing operations in general. In particular, it was shown in [36] that (7) holds true whenever the input alphabet
size q of the channel W is a prime number, and W − and W + are deﬁned as

In terms of (cid:102)W , the channel seen by the i-th bit Ui, i = 1, . . . , N (also known as the bit-channel of the i-th bit) can be

written as

Wi(yN , ui−1

1

|ui) =

1
2n−1

(cid:88)
(cid:101)u∈{0,1}n−i

(cid:102)W (yN|(ui−1

1

, ui,(cid:101)u)).

(cid:88)

We see that Wi is the conditional distribution of (Y N , U i−1
) given Ui provided that the channel inputs Xi are uniformly
distributed for all i = 1, . . . , N. Moreover, it is the case that [1] the bit-channels Wi can be constructed recursively using
the channel transformations W − and W +, which are deﬁned by the equations

1

W −(y1, y2|u1) (cid:44) 1
2
W +(y1, y2, u1|u2) (cid:44) 1
2

W (y1|u1 + u2)W (y2|u2)

u2∈{0,1}
W (y1|u1 + u2)W (y2|u2).

The Bhattacharyya parameter Z(W ) of a binary-input channel W is deﬁned as Z(W ) =(cid:80)

(cid:112)WY |X (y|0)WY |X (y|1).

(3)

y∈Y

The bit-channels deﬁned in (2)-(3) are partitioned into good channels GN (W, β) and bad channels BN (W, β) based on
the values of Z(Wi). More precisely, we have

(4)
where [N ] = {1, 2, . . . , N} and δN > 0 is a small number. As shown in [35], for any binary-input channel W and any
constant β < 1/2,

GN (W, β) = {i ∈ [N ] : Z(Wi) ≤ δN}
BN (W, β) = {i ∈ [N ] : Z(Wi) > 1 − δN},

|GN (W, β)|
|BN (W, β)|

N

N

lim
N→∞

lim
N→∞

= I(W )
= 1 − I(W ).

(5)

(cid:88)

W −(y1, y2|u1) (cid:44) 1
q
W +(y1, y2, u1|u2) (cid:44) 1
q

W (y1|u1 + u2)W (y2|u2)

u2∈{0,1,...,q−1}
W (y1|u1 + u2)W (y2|u2),

meaning that Arıkan’s transform F =
is polarizing for prime alphabets. For the case when q is a power of a
prime, it was proved in [7] that there exist binary linear transforms different from F that support the estimate in (7) for
some exponent β that depends on F . For example, [7] shows that the transform

1 1

(cid:16) 1 0

(cid:17)

(cid:32)

(cid:33)

Gγ =

1
γ

0
1

4

(10)

is polarizing whenever γ is a primitive element of the ﬁeld Fq. Paper [9] considered the use of Arıkan’s transform for
the channels with input alphabet of size q = 2r, showing that the symmetric capacities of the subchannels converge to
one of r + 1 integer values in the set {0, 1, . . . , r}.

Even more generally, necessary and sufﬁcient conditions for a binary operation f : X 2 → X 2 given by

u1 = f (x1, x2),
u2 = x2.

(11)

to be a polarizing mapping were identiﬁed in [37], [38]. A simple set of sufﬁcient conditions for the same was given
in [39], which also gave a concrete example of a polarizing mapping for an alphabet of arbitrary size q. According to
[39], in (11) one can take f in the form f (x1, x2) = x1 + π(x2), where π : X → X is the following permutation:

(cid:98)q/2(cid:99),

x − 1,
x,

π(x) =

if x = 0,
if 1 ≤ x ≤ (cid:98)q/2(cid:99),
otherwise.

(12)

We include experimental results for code construction using the transforms (10) and (12) in Sect. V.
Finally recall that it is possible to attain polarization based on transforms that combine l > 2 subchannels. In particular,
polarization results for transformation kernels of size l × l with l > 2 for binary-input channels were studied in [10].
Apart from that, [7] derived estimates of the error probability of polar codes for nonbinary channels based on transforms
deﬁned by generator matrices of Reed-Solomon codes. However, below we will restrict our attention to binary combining
operations of the form discussed above.

III. CHANNEL DEGRADATION AND THE CODE CONSTRUCTION SCHEME

In the algorithm that we deﬁne, the subchannels are constructed recursively, and after each evolution step the resultant
channel is replaced by its degraded version which has an output alphabet size less than a given threshold µ. In general
terms, this procedure is described in more detail is described as follows.

Algorithm 1 Degrading of subchannels
input: DMC W , bound on the output size µ, code length N = 2n, channel index i with binary representation
i = (cid:104)b1, b2, . . . bn(cid:105)2.
output: A DMC obtained from the subchannel Wi.

T ← degrade(W, µ)
for j = 1, 2, . . . , n do

if bj = 0 then

T ← T −
else
T ← T +

end if
T ← degrade(T, µ)

end for
return T

Before proceeding further we note that T − and T + appearing in Algorithm 1 can be any transformations that produce
combined channels for the polarization procedure. The possibilities range from Arıkan’s transform to the schemes
discussed in the end of Section II.

The next step is to deﬁne the function degrade in such a way that it can be applied for general discrete channels.
Ideally, the degrading-merge operation should optimize the degraded channel by attaining the smallest rate loss over all
T (cid:48) :

I(W ) − I(T (cid:48))

inf

T (cid:48): T (cid:48)≺W
|out(T (cid:48))|≤µ

5

(13)

Equation (13) deﬁnes a convex maximization problem, which is difﬁcult to solve with reasonable complexity. To reduce
the computational load, [13] proposed the following approximation to (13): replace y, y(cid:48) ∈ Y by a single symbol if the
pair y, y(cid:48) gives the minimum loss of capacity among all pairs of output symbols, and repeat this as many times as needed
until the number of the remaining output symbols is equal to or less than µ (see Algorithm C in [13]). In [14], [15]
this procedure was called greedy mass merging. In the binary case this procedure can be implemented with complexity
O(N µ2 log µ) because one can check only those pairs of symbols (y1, y2) which are closest to each other in terms of the
likelihood ratios (see Theorem 8 in [13]). This simpliﬁcation does not generalize to the channels with nonbinary inputs,
meaning that we need to inspect all pairs of symbols. Since the total number of pairs is O(µ4) after each evolution step,
the overall complexity of the greedy mass merging algorithm for nonbinary input alphabets becomes O(N µ4 log µ).

For a channel W : X → Y deﬁne

PY (y) =

1
q

for all x ∈ X and y ∈ Y. For a subset A ⊆ Y, deﬁne

PY (A) =

PW (x|y) =

(cid:80)

,

W (y|x)
(cid:88)
x0∈X W (y|x0)
W (y|x0)
(cid:88)

x0∈X

PY (y).

y∈A

In the following lemma we establish an upper bound on the rate loss of the greedy mass merging algorithm for

nonbinary input alphabets.
Lemma 1. Let W : X → Y be a discrete memoryless channel and let y1, y2 ∈ Y be two output symbols. Let ˜W : X →
Y\{y1, y2} ∪ {ymerge} be the channel obtained from by W by merging y1 and y2 which has the transition probabilities

(cid:40)

˜W (y|x) =

W (y|x),
W (y1|x) + W (y2|x),

y ∈ Y\{y1, y2}
y = ymerge

.

|PW (x|y1) − PW (x|y2)|.

(14)

if
if

(cid:88)

x∈X

Then

0 ≤ I(W ) − I( ˜W ) ≤ PY (y1) + PY (y2)

ln 2

Proof: Since ˜W is degraded with respect to W , we clearly have that I(W ) ≥ I( ˜W ), where I(·) is the symmetric
capacity. To prove the upper bound for I(W ) − I( ˜W ) in (14) let X be the random variable uniformly distributed on X,
and let Y be the random output of W . Then we have

(cid:16)

H(X) −(cid:88)

(cid:17)

I(W ) − I( ˜W ) =

H(X|Y = y)PY (y)

−(cid:16)

H(X)−H(X|Y ∈ {y1, y2})(PY (y1) + PY (y2)) − (cid:88)

y∈Y

(cid:17)

H(X|Y = y)PY (y)

y∈Y\{y1,y2}

= H(X|Y ∈ {y1, y2})(PY (y1) + PY (y2))

− H(X|Y = y1)PY (y1) − H(X|Y = y2)PY (y2).

(15)

Next we have

Pr(X = x|Y ∈ {y1, y2}) =

1|X| (W (y1|x) + W (y2|x))

PY (y1) + PY (y2)

1|X| W (y1|x)

PY (y1) + PY (y2)

1|X| W (y2|x)

PY (y1) + PY (y2)

+

=

PY (y1)

PY (y1) + PY (y2)

=
= α12PW (x|y1) + (1 − α12)PW (x|y2)

PW (x|y1) +

PY (y2)

PY (y1) + PY (y2)

where α12 (cid:44)

PY (y1)

PY (y1)+PY (y2). Hence, it follows from (15) that

I(W ) − I( ˜W ) =(PY (y1) + PY (y2))

[α12PW (x|y1) + (1 − α12)PW (x|y2)]

(cid:88)

x∈X

6

PW (x|y2)

α12PW (x|y1) + (1 − α12)PW (x|y2)

× log2
− PY (y1)

1

(cid:88)
PW (x|y1) log2
(cid:88)

x∈X

1

PW (x|y1)

− PY (y2)

(cid:88)

x∈X

PW (x|y2) log2

1

PW (x|y2)

.

x∈X

PW (x|y1) log2
(cid:88)

PW (x|y2) log2

PW (x|y1)

α12PW (x|y1) + (1 − α12)PW (x|y2)

PW (x|y2)

α12PW (x|y1) + (1 − α12)PW (x|y2)

.

Rearranging the terms, we obtain

I(W ) − I( ˜W ) =PY (y1)

+ PY (y2)
Next use the inequality ln x ≤ x − 1 to write

x∈X

which simpliﬁes to

I(W ) − I( ˜W ) ≤ PY (y1)

(cid:18)

(cid:18)

ln 2

x∈X

x∈X

+ PY (y2)

PW (x|y1)

ln 2
PW (x|y2)

(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:12)(cid:12)(cid:12)(cid:12) (1 − α12)(PW (x|y1) − PW (x|y2))

α12PW (x|y1) + (1 − α12)PW (x|y2)

I(W ) − I( ˜W ) ≤ PY (y1)
ln 2

PY (y2)

x∈X

x∈X

ln 2

+

PW (x|y1)

PW (x|y2)

Bound the ﬁrst term in (16) using the inequality

and do the same for the second term. We obtain the estimate
1 − α12
α12

I(W ) − I( ˜W ) ≤ PY (y1)
ln 2

α12PW (x|y1) + (1 − α12)PW (x|y2)

PW (x|y1)

PW (x|y2)

α12PW (x|y1) + (1 − α12)PW (x|y2)

(cid:19)

− 1

(cid:19)

− 1

(1 − α12)(PW (x|y1) − PW (x|y2))
α12PW (x|y1) + (1 − α12)PW (x|y2)
α12(PW (x|y2) − PW (x|y1))

α12PW (x|y1) + (1 − α12)PW (x|y2)

α12PW (x|y1)

(cid:12)(cid:12)(cid:12)(cid:12) ≤ (1 − α12)|PW (x|y1) − PW (x|y2)|
(cid:88)
(cid:88)

|PW (x|y1) − PW (x|y2)|

|PW (x|y1) − PW (x|y2)|

x∈X

x∈X
||PW (.|y1) − PW (.|y2)||1.

+

PY (y2)

α12
1 − α12
PY (y1) + PY (y2)

ln 2

=

ln 2

.

(16)

This completes the proof of (14).

The bound (14) brings in metric properties of the probability vectors. Leveraging them, we can use simple volume

arguments to bound the rate loss due to approximation.
Lemma 2. Let the input and output alphabet sizes of W be q and M, respectively. Then, there exists a pair of output
symbols (y1, y2) such that

(cid:16) 1

(cid:17)

PY (y1) = O
||PW (.|y1) − PW (.|y2)||1 = O

M

, PY (y2) = O

,

(cid:17)
(cid:16) 1
(cid:16)(cid:16) 1
(cid:17) 1
q−1(cid:17)
(cid:17) q
q−1(cid:17)
(cid:16)(cid:16) 1

M

M

M

which implies the estimate

0 ≤ I(W ) − I( ˜W ) = O

Proof: Consider the subset of output symbols AM (Y) = {y : PY (y) ≤ 2/M}. Noticing that |(AM (Y))c| ≤ M/2,

we conclude that

Keeping in mind the bound (14), let us estimate the maximum value of the quantity

For each y ∈ Y, the vector PW (.|y) is an element of the probability simplex

Let R > 0 be a number less than the quantity in (21). Clearly, for any y1, y2 ∈ AM (Y)) the q-dimensional (cid:96)1-balls of
radius R/2 centered at PW (·|yi), i = 1, 2 are disjoint, and therefore, so are their intersections with Sq. Let vol(Sq) be
q/(q − 1)!, but in this proof we will stay with
the (q − 1)-dimensional volume of Sq. It is easily seen that vol(Sq) =
crude bounds (a more precise calculation is performed in the remark below). Clearly for any y ∈ Y

√

7

(17)

(18)

(19)

(20)

(21)

(22)

|AM (Y)| ≥ M
2

.

min

y1,y2∈AM (Y)

(cid:107)PW (·|y1) − PW (·|y2)(cid:107)1.

Sq =

(s1, . . . , sq) ∈ Rq

si = 1

.

(cid:41)

(cid:12)(cid:12)(cid:12)(cid:12)si ≥ 0,

q(cid:88)

i=1

(cid:40)

(cid:17)q−1

vol(cid:8)BR/2(PW (·|yi)) ∩ Sq

(cid:16) R
(cid:9) = O
(cid:16)(cid:16) R
(cid:17)q−1(cid:17) ≤ vol(Sq)
(cid:16)(cid:16) 1
(cid:17)1/(q−1)(cid:17)

2

2

R = O

M

On account of (20) we obtain that

whence

M
2

O

for all R less than the quantity in (21). Hence, we see that there exist two output symbols y1, y2 ∈ AM (Y) such that
the conditions (17), (18) hold simultaneously. So if these symbols are merged in the algorithm discussed, the rate loss
is bounded above as in (19).
This lemma leads to an important conclusion for the code construction: to degrade the subchannels we should merge
the symbols y1, y2 with small PY (yi) and such that the reverse channel conditional PMFs PW (·|yi), i = 1, 2 are (cid:96)1-
close. Performing this step several times in succession, we obtain the operation called degrade in the description of
Algorithm 1. The properties of this operation are stated in the following proposition.
Proposition 3. Let W be a DMC with input of size q.
(a) There exists a function degrade(W, µ) such that its output channel T satisﬁes

0 ≤ I(W ) − I(T ) ≤ O

.

(23)

(cid:16)(cid:16) 1

q−1(cid:17)
(cid:17) 1

µ

(b) For a given block length, let W (i)
T (i)
N denote the its approximation returned by Algorithm 1. Then
N ) − I(T (i)

(cid:88)

(I(W (i)

0 ≤ 1
N

0≤i≤N

N )) ≤ n O

(cid:16)(cid:16) 1

(cid:17) 1
q−1(cid:17)

µ

N be the i-th subchannel after n evolution steps of the polarization recursion. Let

.

(24)

8

Proof: Let M be the cardinality of the output alphabet of W . Performing M − µ merging steps of the output
symbols in succession, we obtain a channel with an output alphabet of size µ. If the pairs of symbols to be merged are
chosen based on Lemma 2, then (18) implies that

0 ≤ I(W ) − I(T ) ≤ C(q)

≤ C(q)

(cid:18) 1

(cid:19) q

q−1

i

M(cid:88)
(cid:90) M

i=µ+1

µ

(x − 1)

−( q

q−1 )dx = O

(cid:16)(cid:16) 1

(cid:17) 1
q−1(cid:17)

µ

where C(q) is a constant which depends on the input alphabet size q but not on the number n of recursion steps. This
proves (23), and (24) follows immediately.
Remark III.1. This result provides a generalization to the nonbinary case of a result in [14] which analyzed the a merging
(degrading) algorithm of [13]. For the case of binary-input channels, Lemma 1 of [14] gave an estimate O(1/µ) of the
approximation error. Substituting q = 2 in (23), we note that this result is a generalization of [14] to channels with
arbitrary ﬁnite-size input.
Remark III.2. Upper bounds similar to (23) are derived in [17, Lemma 6] and [18, Lemma 8]. The output symbol merging
policy in [17] makes it possible to have I(W )−I( ˜W ) = O((1/µ)1/q). On the other hand, the channel upgrading technique
introduced in [18] gives the same bound as (23). Recall that the code construction schemes considered in those two
works have complexity O(µq). It is interesting to observe that merging a pair of output symbols at each step as we do
here is as good as the algorithms based on binning of output symbols which requires a higher complexity.
Remark III.3. A very recent result of [34] states that any construction procedure of polar codes construction based on
degrading after each polarization step, that guarantees the rate loss bounded as I(W ) − I(T ) ≤ , necessarily has the
q−1
2 ). Proposition 3 implies that the alphabet size of the algorithm that we propose
output alphabet of size µ = Ω((1/)
√
scales as the square of this bound, meaning that the proposed procedure is not too far from being optimal, namely for
any channel, our degradation scheme satisﬁes µ ≤ (1/)q−1, and there exists a channel for which µ ≥ (1/
)q−1 holds
true even for the optimal degradation scheme.

A HEURISTIC CALCULATION RELATED TO (22): Some of the implicit constants in the calculation that leads to (18) in
the proof of Lemma 2 can be removed using the following (heuristic) geometric argument. Let

(cid:40)

q(cid:88)

i=1

Sq =

x ∈ Rq,

si ≤ 1, si ≥ 0, i = 1, . . . , q

(cid:41)

√

be the regular q-dimensional simplex whose “outer” face is Sq. Consider the intersection of the q-dimensional balls with
Sq rather than Sq. The volume of this intersection is the smallest when the center of the ball is located at a vertex of
q (R) be the volume of the (cid:96)p-ball of radius R in q dimensions. Computing a crude estimate for the number
Sq. Let V p
of simplices that share a common vertex, note that they all ﬁt in the (cid:96)2 sphere of radius
2, so their number is at most
2)/ vol(Sq). Assuming that the volume of the (cid:96)1-ball around the vertex is shared equally between these simplices,
V 2
q (
we estimate the volume of the intersection to be
vol(Sq)
√
V 2
2)
q (

(Γ(2)R)q
Γ(q + 1)

V 1
q (R/2)

√

=

Using a packing argument similar to (22), we obtain

√

(cid:0)2

Γ(cid:0) q
2 + 1(cid:1)
2 Γ(3/2)(cid:1)q vol(Sq).
Γ(cid:0) q
2 + 1(cid:1)
2 Γ(3/2)(cid:1)q ≤ 2
(cid:16) 1
(cid:17)1/q

M

√

R ≤ C

M

(cid:0)2

Rq
q!

which gives

2 Γ(cid:0) 3

(cid:1)(cid:0)

√
where C = 2
but contains no implicit constants.

2q!
2 +1)

Γ( q

2

(cid:1)1/q ≤ √

2πq. This calculation results in a bound slightly weaker than the one in (23),

IV. NO-LOSS ALPHABET REDUCTION

9

Throughout this section we will use the transformation (8)-(9), in which the “+” is addition modulo q. We discuss a
way to further reduce the complexity of the code construction algorithm using the additive structure on X. As shown
in (14), the symmetric capacity loss is small if the posterior distributions induced by the merged symbols are (cid:96)1-close.
Here we argue that if these vectors are related through cyclic shifts, the output symbols can be merged at no cost to
code performance.
Consider the construction of q-ary polar codes for channels with input alphabet q ≥ 2. Since I(W ) = log q−H(X|Y ),
to construct polar codes it sufﬁces to track the values of H(X|Y ) for the transformed channels. Keeping in mind that
H(X|Y ) = E(− log PX|Y (X|Y )), let us write the polarizing transformation in terms of the reverse channel PX|Y :

P −
Y − (yi, yj) = PY (yi)PY (yj),

u2∈X

PX|Y (x + u2|yi)PX|Y (u2|yj)

(cid:88)
(cid:16)(cid:88)
(cid:80)
x∈X
x0∈X PX|Y (u + x0|yi)PX|Y (x0|yj)

PX|Y (u + x|yi)PX|Y (x|yj)
PX|Y (u + x|yi)PX|Y (x|yj)

(cid:17)

X|Y − (x|yi, yj) =
P −

P +
Y +(u, yi, yj) =

X|Y +(x|u, yi, yj) =
P +
X and P −



PY (yi)PY (yj),

(25)

If PX is uniform, both P +
X are also uniform. Consequently, the transformation (25) is the same as (8)-(9) under
the uniform prior distributions. Throughout this section we will calculate the transformation of probability distributions
using (25) instead of (8)-(9) since we rely on the posterior distributions to merge symbols.
Deﬁnition IV.1. Given a distribution PXY on X × Y, deﬁne an equivalence relation on Y as follows: y1
P∼ y2 if for
every x ∈ X there exists x1 ∈ X such that PX|Y (x + x1|y1) = PX|Y (x|y2). This deﬁnes a partition of Y into a set of
equivalence classes Y = {A1, A2, . . . , A|Y|}.

P∼ y2, then we can merge y1 and y2 into one alphabet symbol without changing H(X|Y ) for all
We show that if y1
XY , s ∈ {−, +}n and all n ≥ 1. As a consequence, it is possible to assign one symbol to each equivalence class, i.e.,
P s
the effective output alphabet of W for the purposes of code construction is formed by the set Y.

To formalize this intuition, we need the following deﬁnitions.

Deﬁnition IV.2. Consider a pair of distributions PXY1, QXY2. We say that two subsets of output alphabets A ⊆ Y1, B ⊆ Y2
are in correspondence, denoted A (cid:39) B, if

(1) PY1(A) = PY2(B);
(2) For every y1 ∈ A and y2 ∈ B and every x ∈ X there exists x1 ∈ X such that PX|Y1(x + x1|y1) = QX|Y2(x|y2)

(the value of x1 may depend on y1 and y2).

Note that condition (2) in this deﬁnition implies that all the elements in A are in the same equivalence class, and all

the elements in B are also in the same equivalence class.
Deﬁnition IV.3. We call the distributions PXY1, QXY2 equivalent, denoted PXY1 ≡ QXY2, if there is a bijection φ :
Y1 → Y2 such that A (cid:39) φ(A) for every equivalence class A ∈ Y1.
Note that two equivalent distributions have the same H(X|Y ).
The following proposition underlies the proposed speedup of the polar code construction. Its proof is computational

in nature and is given in the Appendix.
Proposition 4. Let PXY1, QXY2 be two distributions. If PXY1 ≡ QXY2 then for all s ∈ {−, +}n, n ≥ 1 we have
P s

(and therefore HP s (X|Y1) = HQs(X|Y2)).

XY s
1
The next proposition provides a systematic way to merge output symbols of the synthesized channels obtained by the

≡ Qs

XY s
2

‘+’ transformation.
Proposition 5. Let distribution PXY on X × Y, and let P −
X × Y2 we have

XY − and P +
(v, y1, y2) P +∼ (−v, y2, y1),

where if y1 = y2 then v (cid:54)= 0.

XY + be deﬁned as in (25). For every (v, y1, y2) ∈

(26)

10

Proof: For every y1, y2 ∈ Y and any u1, u ∈ X, we have

X|Y +(u|(u1, y1, y2)) =
P +

=

PX|Y (u1 + u|y1)PX|Y (u|y2)

(cid:80)
x0∈X PX|Y (u1 + x0|y1)PX|Y (x0|y2)
(cid:80)
x0∈X PX|Y (−u1 + (u1 + x0)|y2)PX|Y (u1 + x0|y1)
X|Y +(u + u1|(−u1, y2, y1)).

P (−u1 + (u + u1)|y2)P (u1 + u|y1)

= P +

This proves (26).

No-loss cyclic merging algorithm

Using the above considerations, we can reduce the time needed to construct a polar code. The informal description
of the algorithm is as follows. Given a DMC W : X → Y, we calculate a joint distribution PXY on X × Y by assuming
a uniform prior on X. We then use (25) to recursively calculate P s
XY s , and after each step of the recursion we reduce
the output alphabet size by assigning one symbol to the whole equivalence class. Namely, for each equivalence class A
X|Y s(x|A) = P s
X|Y s(x|y∗) for an arbitrarily chosen
in the output alphabet Ys, we set P s
y∗ ∈ A. Note that y∗ can be chosen arbitrarily because the vectors P s
X|Y s(·|y), y ∈ A are cyclic shifts of each other.
By Prop. 4, we have I(W s) = log q − HP s(X|Y ), i.e., the alphabet reduction entails no approximation of the capacity
values.
output alphabet. Let W be a q-ary symmetric channel (qSC) W : X → Y, |X| = |Y| = q

Let us give an example, which shows that this simple proposal can result in a signiﬁcant reduction of the size of the

Y s (A) =(cid:80)

Y s(y) and P s

y∈A P s

W (y|x) = (1 − )δx,y +



(1 − δx,y),

(27)
and let us take q = 4. Consider the channels W s, s ∈ {+,−}n obtained by several applications of the recursion (2)-(3).
The actual output alphabet size of the channels W +, W ++ and W +++ is 43, 47, and 415, respectively. At the same
time, the effective output alphabet size of W +, W ++ and W +++ obtained upon merging the equivalence classes in Y
is no more than 3, 24, and 1200. In particular, the effective output alphabet size of W +++ is less than a 106-th fraction
of its actual output alphabet size. Let n ≥ 3 and s ∈ {+,−}n. If s starts with + + +, then the effective output alphabet
size of W s is less than a (106×2n−3

)-th fraction of its actual alphabet size.

q − 1

Improved greedy mass merging algorithm
Now we are ready to describe the improved code construction scheme. Prop. 4 implies that if the vectors PX|Y (·|yi), i =
1, 2 are cyclic shifts of each other, merging them into one symbol ˜y incurs no rate loss. Extending this intuition, we
assume that performing greedy mass merging using all the cyclic shifts of these vectors improves the accuracy of the
approximation.
Given a DMC W : X → Y, we calculate a joint distribution PXY on X × Y by assuming the uniform prior on X
XY s and after each step of

and taking W as the conditional probability. We then use (25) to recursively calculate P s
transformation:
(−u1, y2, y1) for all u1, y1, y2, then use the degrade function below on P s
XY s .
Q ˜Y (y) = QY (y), QX| ˜Y (x|y) = QX|Y (x|y) for all x ∈ X and y ∈ ˜Y\{˜y} and

(1) If the last step in s is +: First use the merge_pair function below to merge the symbols (u1, y1, y2) and
(2) If the last step in s is −, use the degrade function below on P s
The function merge_pair(Q, (y1, y2, u)) is deﬁned as follows: Form the alphabet ˜Y = Y\{y1, y2} ∪ {˜y}, putting

XY s.

Q ˜Y (˜y) = QY (y1) + QY (y2),

QY (y1)QX|Y (x|y1) + QY (y2)QX|Y (x + u|y2)

.

(28)

QX| ˜Y (x|˜y) =

Q ˜Y (˜y)

Remark IV.1. Due to the concavity of the entropy function [40, Thm. 2.7.3], H(X|Y ) can only increase after calling
the merge_pair function.

11

Algorithm 2 The degrade function
input: distribution PX,Y0 over X × Y0, the target output alphabet size µ.
output: distribution QX,Y over X × Y, where |Y| ≤ µ.

(y1, y2, u) ← choose(Q)
Q ← merge_pair(Q, (y1, y2, u))
(cid:96) ← (cid:96) − 1

Q ← P
(cid:96) ← |Y|
while (cid:96) > µ do

end while
return Q

The function choose(Q) is deﬁned as follows. Find the triple y1, y2 and u ∈ X such that the change of conditional

entropy HQ(X|Y ) incurred by the merge (y1, y2) → ˜y using merge_pair(Q, (y1, y2, u))

∆(H) (cid:44) Q ˜Y (˜y)H(X| ˜Y = ˜y) − 2(cid:88)

QY (yi)H(X|Y = yi)

i=1

is the smallest among all the triples (yi, yj, u) ∈ Y2 × X.
Remark IV.2. The main difference between Algorithm 2 and the ordinary greedy mass merging algorithm discussed
in Sect. III (e.g., Algorithm C in [13]) can be described as follows. In order to select a pair of symbols that induces
the smallest increase of H(X|Y ), Algorithm 2 considers all the cyclic shifts of the posterior distributions of pairs of
symbols, while the “ordinary” greedy mass merging algorithm examines only the distributions themselves. As argued
above, this is the reason that Algorithm 2 leads to a smaller rate loss than Algorithm 1.

Note that to perform the ‘+’ transformation, we ﬁrst use (26) to merge pairs of symbols with cyclically shifted posterior
vectors and then switch to greedy mass merging. In doing so, we incur a smaller rate loss because the number of steps
of approximation performed for Algorithm 2 is only half the number of steps performed in Algorithm 1. Moreover, since
(26) provides a systematic way of merging symbols with cyclically shifted distributions, (in other words, we do not need
to search all the pairs in order to ﬁnd them,) the running time of Algorithm 2 is also reduced from that of Algorithm 1.
This intuition is conﬁrmed in our experiments which show that the overall gap to capacity of the constructed codes is
smaller than the one attained by using the basic greedy mass merging, while the time taken by the algorithm is reduced
from greedy mass merging alone. More details about the experiments are given in Sect. V.

The ﬁnite ﬁeld transformation of [7]
We also note that Prop. 4 remains to be valid when the alphabet is a ﬁnite ﬁeld Fq and Arıkan’s transform F =

is replaced by the transform given by (10). This fact is stated in the proposition below. Its proof is similar to Prop. 4
and will be omitted.
Proposition 6. Let X = Fq and let PXY1, QXY2 be two distributions. If PXY1 ≡ QXY2 then for all s ∈ {−, +}n, n ≥ 1
we have P s

(and therefore HP s(X|Y1) = HQs (X|Y2)).

≡ Qs

XY s
1

XY s
2

(cid:17)

(cid:16) 1 0

1 1

V. EXPERIMENTAL RESULTS

There are several options of implementing the alphabet reduction procedures discussed above. The overall idea is to
perform cyclic merging (with no rate loss) and then greedy mass merging for every subchannel in every step n ≥ 1 of
the recursion.
Greedy mass merging (the function degrade of Algorithm 1) calls for ﬁnding a pair of symbols y1, y2 whose merging
minimizes the rate loss ˜∆, which can be done in time O(M 2 log M ), M := |Y|. In practice this may be too slow, so
instead of optimizing we can merge the ﬁrst pair of symbols for which the rate loss is below some chosen threshold C.
It is also possible to merge pairs of symbols based on the proximity of probabilities on the RHS of (14).

Note also that greedy mass merging can be applied to any binary polarizing operation including those described
in Sect. II. We performed a number of experiments using addition modulo q, the ﬁnite ﬁeld polarization Gγ, and a
polarizing operation from [39]. A selection of results appears in Fig. 1. In Examples 1-3 we construct polar codes for

12

(a) Example 1: qSC with q = 5;  =
0.2, I(W ) = 1.2, n = 16, ∆(I(W )) =
0.097

(b) Example 2: 16 QAM, SNR=10dB,
I(W ) = 2.82, n = 12, ∆(I(W )) = 0.2

(c) Example 3: qSC with q = 16;
 = 0.15, I(W ) = 2.804, n = 12,
∆(I(W )) = 0.23

(e) Example 5: The same channel as
in Example 4, polarizing transform (12),
n = 10, ∆(I(W )) = 0.185

(d) Example 4: OEC with 0 = 0.3, 1 =
0.2, 2 = 0.3, 3 = 0.2, I(W ) = 1.6,
n = 12, ∆(I(W )) = 0 (in this case there
is no approximation loss)
Fig. 1: Construction of nonbinary polar codes. In Fig. (a)-(c) we plot the capacity distribution of subchannels for channels with
q = 5 and 16 (in these examples qSC is a q-ary symmetric channel deﬁned in (27)). In Examples 4-6 we apply different polarizing
transforms, showing convergence to different number of extremal conﬁgurations for the same channel (here OEC is the ordered erasure
channel, see (29))

(f) Example 6: The same channel as
in Example 4, polarizing transform (10),
n = 10, ∆(I(W )) = 0.216

the q-ary symmetric channel (27) and the 16 QAM channel, showing the distribution of capacities of the subchannels.
In Examples 4-6 we apply different polarizing transforms to a channel W with inputs X = {0, 1}3 and outputs Y =
{0, 1}3 ∪ {? ∗ ∗, ?∗, ???}, where ∗ can be 0 or 1. The transitions are given by

W (x1x2x3|x1x2x3) = 0.3, W (?x2x3|x1x2x3) = 0.2
W (??x3|x1x2x3) = 0.3, W (???|x1x2x3) = 0.2

(29)
for all x1, x2 ∈ {0, 1}. Following [9], we call W an ordered erasure channel. One can observe that under the addition
modulo-q transform (8)-(9) the channel polarizes to several extremal conﬁgurations, while under the transforms given
in (10), (12) it converges to only two levels. This behavior, predicted by the general results cited in Section II, supports
the claim that the basic algorithm of Sect. III does not depend on (is unaware of) the underlying polarizing transform.
More details about the experiments are provided in the captions to Fig. 1.

It is interesting to observe that the q-ary symmetric channel for q = 16 polarizes to two levels under Arıkan’s transform.
In principle there could be 5 different extremal conﬁgurations, and it is a priori unclear that no intermediate levels arise
in the limit. An attempt to prove this fact was previously made in [36], but no complete proof is known to this date.

Next we give some simulation results to support the conclusions drawn for Algorithm 2. We construct polar codes of
several block lengths for qSC W with q = 4 and  = 0.15, setting the threshold µ = 256. The capacity of the channel
equals I(W ) = 1.15242.

N
128
256
512
1024

t1
404
1038
2256
4378

t2
177
490
1088
2164

∆I1
0.041
0.048
0.055
0.061

∆I2
0.026
0.033
0.038
0.042

t1
t2
2.3
2.1
2.1
2.0

∆I1
∆I2
1.6
1.5
1.5
1.5

������������������������������������������������������������(��)������������������������������������(��)������������������������������������(��)010002000300040000.00.51.01.52.02.53.0ChannelindexiIWi020040060080010000.00.51.01.52.02.53.0ChannelindexiIWi020040060080010000.00.51.01.52.02.53.0ChannelindexiIWi13

In this table N is the code length, t1 is the running time of greedy mass merging and t2 is the running time of
Algorithm 2 (our algorithm) in seconds. The quantities ∆I1 and ∆I2 represent the rate loss (the gap between I(W ) and
the average capacity of the subchannels) in greedy mass merging and our algorithm, respectively.
Binary codes: Here our results imply the following speedup of Algorithm A in [13]. Denote LR(y) = W (y|1)/W (y|0).
The cyclic merging means that we merge any two symbols (y1, y2) → ˜y if LR(y1) = LR(y2)±1, so we can only record
the symbols y ∈ ˜Y with LR(y) ≥ 1. This implies that the threshold µ in [13] can be reduced to µ/2. Overall the
alphabet after the + or − step is reduced by a factor of about 8 while the code constructed is exactly the same as in
[13]. In the following table we use the threshold values µ = 32 for [13] and µ = 16 for our algorithm. The codes are
constructed for the BSC channel with  = 0.11.

N
512
2048

tA
3.6
14.7

t2
0.5
2.3

tA
t2
7.2
6.4

N
1024
4096

tA
7.3
29.2

t2
1.1
4.6

tA
t2
6.6
6.3

In the second table N is the code length, tA is the running time of Algorithm A in [13], and t2 is the running time
of our algorithm in seconds. Our algorithm indeed is about 7 times faster, and the codes constructed in both cases are
exactly the same.

VI. CONCLUSION

We considered the problem of constructing polar codes for nonbinary alphabets. Constructing polar codes has been
a difﬁcult open question since the introduction of the binary polar codes in [1]. Ideally, one would like to obtain an
explicit description of the polar codes for a given block length, but this seems to be beyond reach at this point. As an
alternative, one could attempt to construct the code by approximating each step of the recursion process. For binary
codes, this has been done in [13],[14], but extending this line of work to the nonbinary case was an open problem
despite several attempts in the literature. We take this question one step closer to the solution by designing an algorithm
that approximates the construction for moderately-sized input alphabets such as q = 16. The algorithm we implement
works for both binary and non-binary channels with complexity O(N µ4), where N is the blocklength and µ is the
parameter that limits the output alphabet size. Furthermore, the error estimate the we derive generalizes the estimate of
[14] to the case of nonbinary input alphabets (but relies on a different proof method). It is also interesting to note that
the error is rather close to a lower bound for this type of construction algorithms, derived recently in [34]. Apart from
presenting a theoretical advance, this algorithm provides a useful tool in the analysis of properties of various polarizing
transforms applied to nonbinary codes over alphabets of different structure. The proposed construction algorithm also
brings nonbinary codes closer to practical applications, which is another promising direction to be explored in the future.

We will show that if PXY1 ≡ QXY2 , then P −

≡ Q−

−
XY
1

−
XY
2

and P +

XY +
1

≡ Q+

XY +
2

APPENDIX: PROOF OF PROP. 4

induction on n.

, which will imply the full claim by

and Q−

2, respectively. In
, we need to show that for every A1, B1 ∈ Y1, we have A1 × B1 (cid:39) φ(A1)× φ(B1).

are deﬁned on the sets X × Y2

1 and X × Y2

−
XY
1

−
XY
2

(a) (The ‘−’ case) The distributions P −

order to prove that P −
Indeed,

−
XY
1

≡ Q−

−
XY
2

(cid:88)

P −
−
1

Y

((y1, y2)) =

(y1,y2)∈A1×B1

Similarly,

(cid:88)

(y1,y2)∈φ(A1)×φ(B1)

Q−

Y

−
2

=

=

((y1, y2)) =

(cid:88)
(cid:88)

y2∈B1

y2∈B1

y1∈A1

(cid:88)
(cid:88)
(cid:16) (cid:88)
(cid:16) (cid:88)

y1∈A1

y1∈A1

y1∈φ(A1)

P −
−
1

Y

((y1, y2))

PY1(y1)PY1 (y2)

(cid:17)(cid:16) (cid:88)
(cid:17)(cid:16) (cid:88)

y2∈B1

PY1(y1)

PY1(y2)

.

QY2(y1)

QY2(y2)

y2∈φ(B1)

(cid:17)

(cid:17)

.

Since A1 (cid:39) φ(A1) and B1 (cid:39) φ(B1), we have PY1 (A) = QY2 (φ(A)1) and PY1(B) = QY2(φ(B)1). Therefore,

(cid:88)

(cid:88)

P −
−
1

Y

((y1, y2)) =

(y1,y2)∈A1×B1

(y1,y2)∈φ(A1)×φ(B1)

Q−

Y

−
2

((y1, y2)).

Thus A1 × B1 and φ(A1) × φ(B1) satisfy condition (1) in Def. IV.2.
and x2 such that PX|Y1(x + x1|y1) = QX|Y2 (x|y3) and PX|Y1 (x + x2|y2) = QX|Y2(x|y4) for all x ∈ X. Thus

To prove condition (2), choose y1 ∈ A1, y2 ∈ B1, and let y3 ∈ φ(A1) and y4 ∈ φ(B1). By Def. IV.2, there exist x1

14

Q−
X|Y

−
2

(x|(y3, y4)) =

=

(cid:88)
(cid:88)
(cid:88)

u2∈X

u2∈X

=

u2∈X
= P −
X|Y

−
1

QX|Y2 (x + u2|y3)QX|Y2 (u2|y4)
PX|Y1 (x + u2 + x1|y1)PX|Y1 (u2 + x2|y2)
PX|Y1 ((x + z) + u2|y1)PX|Y1(u2|y2)
(x + z|(y1, y2)),

where z = x1 + (−x2). Therefore, A1 × B1 (cid:39) φ(A1) × φ(B1), and PXY1 ≡ QXY2.

(b). (The ‘+’ case) The distribution P +

2) respectively. Similarly
to case (a) above, we will show that for every A1, B1 ∈ Y1 there exist permutations πy1,y2 and πy3,y4 on X such that
for every u ∈ X

and Q+

XY +
2

XY +
1

are over X× (X× Y2

1) and X× (X× Y2

{(πy1,y2(u), y1, y2) : y1 ∈ A1, y2 ∈ B1} (cid:39) {(πy3,y4 (u), y3, y4) : y3 ∈ φ(A1), y4 ∈ φ(B1)}

To show this, ﬁx A1, B1 ∈ Y1 and choose some z1 ∈ A1, z2 ∈ B1, y1 ∈ A1, y2 ∈ B1, y3 ∈ φ(A1) and y4 ∈ φ(B1). By
Def. IV.2, for every x ∈ X there exist x1, x2, x3 and x4 such that

PX|Y1(x + x1|z1) = PX|Y1(x|y1), PX|Y1(x + x2|z2) = PX|Y1(x|y2)
PX|Y1 (x + x3|z1) = QX|Y2 (x|y3), PX|Y1 (x + x4|z2) = QX|Y2 (x|y4).

For x ∈ X deﬁne permutations πy1,y2, πy3,y4 as πy1,y2 (x) = −x1 + x + x2 and πy3,y4 (x) = −x3 + x + x4. We compute

P +

X|Y +

1

Similarly,

1

X|Y +

(x|(−x1 + x2 + u, y1, y2))
PX|Y1(−x1 + x2 + x + u|y1)PX|Y1 (x|y2)

(x|(πy1,y2 (u), y1, y2)) = P +
(cid:80)
x0∈X PX|Y1 (−x1 + x2 + x0 + u|y1)PX|Y1(x0|y2)
(cid:80)
x0∈X PX|Y1 (x0 + x2 + u|y1)PX|Y1 (x0 + x2|y2)
X|Y +

PX|Y1 (x + x2 + u|z1)PX|Y1(x + x2|z2)

(x + x2|(u, z1, z2)).

= P +

=

=

1

Q+

X|Y +

2

(x|(πy3,y4(u), y3, y4)) = P +

X|Y +

1

(x + x4|(u, z1, z2)).

The last two equations imply that

P +

X|Y +

1

(x|(πy3,y4 (u), y3, y4)),

X|Y +

2

which veriﬁes condition (2) in Def. IV.2. Let us check that condition (1) is satisﬁed as well. We have

({(πy1,y2 (u), y1, y2) : y1 ∈ A1, y2 ∈ B1}) =

P +
Y +
1

P +
Y +
1

((πy1,y2(u), y1, y2))

(−x2 + x4 + x|(πy1,y2 (u), y1, y2)) = Q+
(cid:88)
(cid:88)
(cid:88)

(cid:88)
(cid:88)

PY1(y1)PY1(y2)

y1∈A1,y2∈B1

y1∈A1,y2∈B1

x∈X

=

PY1(y1)PY1(y2)

=

y1∈A1,y2∈B1

x∈X

PX|Y1 (−x1 + x2 + x + u|y1)PX|Y1(x|y2)
PX|Y1 (u + x2 + x|z1)PX|Y1(x + x2|z2)

(cid:16)(cid:88)

x∈X

=

PX|Y1(u + x|z1)PX|Y1(x|z2)

(cid:17)(cid:16) (cid:88)

y1∈A1

(cid:17)(cid:16) (cid:88)

y2∈B1

(cid:17)

PY1(y2)

PY1(y1)

15

and

Q+
Y +
2

({(πy3,y4(u), y3, y4) : y3 ∈ φ(A1), y4 ∈ φ(B1)})

=

PX|Y1(u + x|z1)PX|Y1(x|z2)

(cid:16)(cid:88)

x∈X

(cid:17)(cid:16) (cid:88)

y3∈φ(A1)

(cid:17)(cid:16) (cid:88)

y4∈φ(B1)

(cid:17)

.

QY2(y4)

QY2(y3)

By assumption PY1(A1) = QY2(φ(A1)) and PY1 (B1) = QY2(φ(B1)), so this proves that

({(πy1,y2(u), y1, y2) : y1 ∈ A1, y2 ∈ B1}) = Q+

({(πy3,y4 (u), y3, y4) : y3 ∈ φ(A1), y4 ∈ φ(B1)}).

Y +
2

P +
Y +
1

Thus for every u ∈ X

{(πy1,y2(u), y1, y2) : y1 ∈ A1, y2 ∈ B1} (cid:39) {(πy3,y4(u), y3, y4) : y3 ∈ φ(A1), y4 ∈ φ(B1)}

The proof is complete.

REFERENCES

[1] E. Arıkan, Channel polarization: a method for constructing capacity-achieving codes for symmetric binary-input memoryless channels, IEEE

Trans. Inform. Theory 55 (2009), no. 7, 3051–3073.

[2] E. Abbe, and E. Telatar, Polar codes for the m-user multiple access channel. IEEE Trans. Inform. Theory 58 (2012), no. 8, 5437–5448.
[3] H. Mahdavifar and A. Vardy, Achieving the secrecy capacity of wiretap channels using polar codes, IEEE Trans. Inform. Theory, 57 (2011),

no. 10, 6428–6443.

[4] E. Sasoglu, E. Telatar, E. M. Yeh. Polar codes for the two-user multiple-access channel, IEEE Trans. Inform. Theory 59 (2013), no. 10, 6583–6592.
[5] E. Arıkan, Source polarization, Proc. IEEE Int. Symposium on Information Theory, Austin, TX, June 2010, 899–903.
[6] S. B. Korada and R. Urbanke, Polar codes are optimal for lossy source coding, IEEE Trans. Inform. Theory, 56 (2010), no. 4, 1751–1768.
[7] R. Mori and T. Tanaka, Source and channel polarization over ﬁnite ﬁelds and Reed-Solomon matrices, IEEE Trans. Inform. Theory, 60(2014),

[8] A. G. Sahebi and S. S. Pradhan, Multilevel channel polarization for arbitrary discrete memoryless channels, IEEE Trans. Inform. Theory,

[9] W. Park and A. Barg, Polar codes for q-ary channels, q = 2r, IEEE Trans. Inform. Theory, 59(2013), no. 2, 955–969.
[10] S.B. Korada, E. Sasoglu, R. Urbanke, Polar codes: Characterization of exponent, bounds, and constructions, IEEE Trans. Inform. Theory,

no. 5, 2720–2736.

59(2013), no. 12, 7839–7857.

56(2010), no. 12, 6253–6264.

[11] S.H. Hassani and R. Urbanke, Universal polar codes, arXiv:1307.7223, 2013.
[12] R. Mori and T. Tanaka, Performance and construction of polar codes on symmetric binary-input memoryless channels, Proc. IEEE Int. Sympos.

Inform. Theory, Seoul, Korea, 2009, 1496–1500.

[13] I. Tal and A. Vardy, How to construct polar codes, IEEE Trans. Inform. Theory, 59(2013), no. 10, 6562–6582.
[14] R. Pedarsani, S.H Hassani, I. Tal, E. Telatar, On the construction of polar codes, Proc. IEEE Int. Sympos. Inform. Theory, St. Peterburg, Russia,

[15] E. Sasoglu, Polarization and Polar Codes, Foundations and Trends in Communications and Information Theory. vol. 8, Now Publishers, 2012,
[16] A. Ghayoori and T.A. Gulliver, Constructing polar codes using iterative bit-channel upgrading, arXiv:1302.5153, 2013.
[17] I. Tal, A. Sharov, A. Vardy, Constructing polar codes for non-binary alphabets and MACs, Proc. IEEE Int. Sympos. Inform. Theory, Boston,

2011, 11–15.

MA, 2012, 2132–2136.

2014, 411–415.

[18] U. Pereg and I. Tal, Channel upgradation for non-binary input alphabets and MACs, Proc. IEEE Int. Sympos. Inform. Theory, Honolulu, HI,

[19] A. Ghayoori and T.A. Gulliver, Upgraded approximation of non-binary alphabets for polar code construction, arXiv:1304.1790, 2013.
[20] P. Trifonov, Efﬁcient design and decoding of polar codes, IEEE Trans. on Comm., 60(2012), no. 11, 3221–3227.
[21] H. Li and J. Yuan, A practical construction method for polar codes in AWGN channels, in TENCON Spring Conference, Sydney, NSW, 2013,

[22] D. Wu, Y. Li, Y. Sun, Construction and block error rate analysis of polar codes over AWGN channel based on Gaussian approximation, IEEE

Comm. Letters, 18(2014), no. 7, 1099–1102.

[23] H. Vangala, E. Viterbo, Y. Hong, A comparative study of polar code constructions for the AWGN channel, arXiv:1501.02473, 2015.
[24] D. Kern, S. Vorkoper, V. Kuhn, A new code construction for polar codes using min-sum density, in ISTC, 8th, Bremen, Germany, 2014, 228–232.
[25] G. Bonik, S. Goreinov, N. Zamarashkin, Construction and analysis of polar and concatenated polar codes: practical approach, arXiv:1207.4343,

223–226.

2012.

[26] S. Zhao, P. Shi, B. Wang, Designs of Bhattacharyya parameter in the construction of polar codes, in WiCOM, 7th, Wuhan, China, 2011, 1–4.
[27] A. Bravo-Santos, Polar codes for the Rayleigh fading channel, IEEE Comm. Letters, 17(2013), no. 12, 2352–2355.
[28] K. Chen, K. Niu, J.R. Lin, Practical polar code construction over parallel channels, IET Comm., 7(2013), no. 7, 620–627.
[29] L. Zhang, Z. Zhang, X. Wang, Polar code with blocklength N = 3n, in WCSP, Huangshan, China, 2012, 1–6.
[30] V. Miloslavskaya and P. Trifonov, Design of binary polar codes with arbitrary kernel, in ITW, Lausanne, Switzerland, 2012, 119–123.
[31] B. Serbetci and A.E. Pusane, Practical polar code construction using generalised generator matrices, IET Comm., 8(2014), no. 4, 419–426.
[32] P. Trifonov and P. Semenov, Generalized concatenated codes based on polar codes, in ISWCS, 2011, 442–446.
[33] H. Mahdavifar, M. El-Khamy, J. Lee, I. Kang, On the construction and decoding of concatenated polar codes, Proc. IEEE Int. Sympos. Inform.

Theory, Istanbul, Turkey, 2013, 952–956.

[34] I. Tal, On the construction of polar codes for channels with moderate input alphabet sizes, arXiv:1506.08370, 2015.
[35] E. Arıkan and E. Telatar, On the rate of channel polarization, Proc. IEEE Int. Sympos. Inform. Theory, Seoul, Korea, 2009, 1493–1495.

[36] E. Sasoglu, E. Telatar, E. Arikan, Polarization for arbitrary discrete memoryless channels, in ITW, Taormina, Italy, 2009, 144–148.
[37] R. Nasser, Ergodic theory meets polarization. I: An ergodic theory for binary operations, arXiv:1406.2943, 2014.
[38] R. Nasser, Ergodic theory meets polarization. II: A foundation of polarization theory, arXiv: 1406.2949, 2014.
[39] E. Sasoglu, Polar codes for discrete alphabets, Proc. IEEE Int. Sympos. Inform. Theory, Boston, MA, 2012, 2137–2141.
[40] T. Cover and J. Thomas, Elements of Information Theory, Wiley-Interscience, 1991.

16

