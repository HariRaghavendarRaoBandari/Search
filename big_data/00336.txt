6
1
0
2

 
r
a

M
1

 

 
 
]

.

A
N
h
t
a
m

[
 
 

1
v
6
3
3
0
0

.

3
0
6
1
:
v
i
X
r
a

Projection based model order reduction methods for the

estimation of vector-valued variables of interest∗
Marie BILLAUD-FRIESS†, Anthony NOUY†‡, Olivier ZAHM§

March 2, 2016

Abstract

We propose and compare goal-oriented projection based model order reduction
methods for the estimation of vector-valued functionals of the solution of parameter-
dependent equations. The ﬁrst projection method is a generalization of the classical
primal-dual method to the case of vector-valued variables of interest. We highlight
the role played by three reduced spaces: the approximation space and the test space
associated to the primal variable, and the approximation space associated to the dual
variable. Then we propose a Petrov-Galerkin projection method based on a saddle
point problem involving an approximation space for the primal variable and an ap-
proximation space for an auxiliary variable. A goal-oriented choice of the latter space,
deﬁned as the sum of two spaces, allows us to improve the approximation of the vari-
able of interest compared to a primal-dual method using the same reduced spaces.
Then, for both approaches, we derive computable error estimates for the approxima-
tions of the variable of interest and we propose greedy algorithms for the goal-oriented
construction of reduced spaces. The performance of the algorithms are illustrated on
numerical examples and compared to standard (non goal-oriented) algorithms.

†Ecole Centrale de Nantes, GeM, UMR CNRS 6183, France.
‡Corresponding author (anthony.nouy@ec-nantes.fr).
∗This work was supported by the French National Research Agency (Grant ANR CHORUS MONU-0005)
†Ecole Centrale de Nantes, GeM, UMR CNRS 6183, France.
‡Corresponding author anthony.nouy@ec-nantes.fr.
§Department of Aeronautics & Astronautics, Massachusetts Institute of Technology, Cambridge, MA

02139, USA.

1

1

Introduction

This paper is concerned with the numerical solution of linear equations of the form

A(ξ)u(ξ) = b(ξ),

(1)

where the operator A(ξ) and right-hand side b(ξ) depend on a parameter ξ which takes
values in some parameter set Ξ. Such equations arise in many contexts such as uncertainty
quantiﬁcation, optimization or control, where the solution of (1) have to be evaluated with
many instances of the parameters (multi-query context). For large systems of equations
(e.g. arising from a ﬁne discretization of a parameter-dependent partial diﬀerential equation),
solving (1) for one instance of the parameter can be very expensive, which leads to intractable
computations in a multi-query context. Model order reduction methods aim at constructing
an approximation of the solution map u : Ξ → V whose evaluation for a certain value of ξ is
cheaper than solving (1). Standard approaches rely on Galerkin-type projections of u(ξ) on a
low-dimensional subspace Vr of the solution space V , a so-called reduced space. The reduced
space can be generated from evaluations (snapshots) of the solution u(ξ) at some selected
(or randomly chosen) values of the parameter ξ, see [1, 10, 12, 14]. The Proper Orthogonal
Decomposition method aims at constructing an optimal subspace for the approximation of
the set of solutions M = {u(ξ) : ξ ∈ Ξ} in a mean-square sense (see [10]). Reduced Basis
(RB) methods (see [7] for a survey) aim at controlling the approximation uniformly over
the parameter set.
In this context, reduced spaces are usually constructed using greedy
algorithms.

In many applications one is not interested in the solution u(ξ) itself, but only in a vari-
able of interest s(ξ) which is a function of u(ξ). Here we assume that s(ξ) depends linearly
on u(ξ). Eﬃcient goal-oriented methods have been proposed for the estimation of a scalar-
valued variable of interest s(ξ). A standard method consists in computing an approximation
of the solution of the so-called dual problem associated to (1) which is used to correct the
estimation of s(ξ). We refer to [11] for a general survey on primal-dual methods and to
[2, 6, 7, 12] for the application in the context of RB methods.

In this paper, we propose projection based model order reduction methods for the esti-
mation of a variable of interest s(ξ) taking values in a vector space (ﬁnite or inﬁnite dimen-
sional). For example, for boundary value problems, this allows us to consider a variable of
interest which is the restriction of the solution to a subset of the domain or its boundary,
or to consider simultaneously multiple scalar variables of interest. The paper is restricted to
the case where

s(ξ) = L(ξ)u(ξ),

2

with L(ξ) a linear (possibly parameter-dependent) operator.

In a ﬁrst part, we introduce and analyze diﬀerent methods for computing projections
of the solution and approximations of the variable of interest. We ﬁrst present a non goal-
oriented Petrov-Galerkin approach to compute an approximation of u(ξ) from which an esti-
mation of s(ξ) is deduced. Then, we introduce a generalization of the standard primal-dual
method to the case of a vector-valued variable of interest, which relies on the approximation
of the primal variable u(ξ) and of the solution Q(ξ) of the dual problem

A(ξ)∗Q(ξ) = L(ξ)∗,

where A(ξ)∗ and L(ξ)∗ are the adjoints of operators A(ξ) and L(ξ) respectively. We show
that the error on the variable of interest depends on three reduced spaces: the approximation
space Vr for the primal variable u(ξ), the test space Wr which is used for the Petrov-Galerkin
projection of u(ξ), and an approximation space W Q
for the dual variable Q(ξ) which is
k
projected on the space of W Q
k -valued linear operators. Finally, we present a Petrov-Galerkin
method where the projection is obtained by solving a saddle point problem which involves
an approximation space Vr for u(ξ) and an approximation space Tp for an auxiliary variable.
We show that if Tp is chosen under the form Tp = Wr + W Q
k , then error bounds for both the
projection of the primal variable on Vr and the approximation of the variable of interest can
be improved compared to error bounds of a primal-dual approach using the same spaces Vr,
Wr and W Q
k . The proposed approach is a goal-oriented extension of the method proposed
in [4].

In a second part, we derive (for both approaches) computable error estimates for the ap-
proximation of the variable of interest. Then, we propose greedy algorithms based on these
error estimates for the construction of the reduced spaces Vr and W Q
k . We discuss diﬀerent
choices for the reduced space Wr. In particular, we introduce a parameter-dependent space
depending on a preconditioner obtained by means of an interpolation of the inverse of the
operator A(ξ) proposed in [17].

This paper is organized as follows. In Section 2, we introduce and analyze the diﬀerent
projection methods for the estimation of vector-valued variables of interest for general linear
equations of the form (1) formulated in a Hilbert setting. Then, in Section 3, we derive
error estimates for the approximation of the variable of interest and we propose practical
greedy algorithms for the construction of reduced spaces. Finally, in Section 4, numerical
experiments illustrate the properties of the projection methods and of the greedy algorithms.
In particular, we provide a simpliﬁed complexity analysis for the so-called oﬄine phase (i.e.
the construction of the reduced spaces) and for the online phase (i.e. the evaluation of s(ξ)
for a particular instance of ξ).

3

2 Projection methods for the estimation of a variable

of interest

Let V , W and Z be three Hilbert spaces. For a Hilbert space H equipped with a norm (cid:107)·(cid:107)H,
we denote by H(cid:48) the continuous dual space of H. We consider the linear equation

Au = b

(2)

with A ∈ L(V, W (cid:48)) and b ∈ W (cid:48), and a variable of interest

s = Lu,

where L ∈ L(V, Z). We assume that A is a norm-isomorphism1 such that for all u ∈ V ,

where

α(cid:107)u(cid:107)V ≤ (cid:107)Au(cid:107)W (cid:48) ≤ β(cid:107)u(cid:107)V ,

inf
0(cid:54)=v∈V

sup
0(cid:54)=w∈W

sup
0(cid:54)=v∈V

sup
0(cid:54)=w∈W

(cid:104)Av, w(cid:105)
(cid:107)v(cid:107)V (cid:107)w(cid:107)W
(cid:104)Av, w(cid:105)
(cid:107)v(cid:107)V (cid:107)w(cid:107)W

:= α > 0

:= β < ∞,

(3a)

(3b)

which ensures the well-posedness of (2). In this section, we present diﬀerent methods for

constructing an approximation(cid:101)s of s. First, in Section 2.1, we present a standard approach

which consists in estimating the variable of interest from a Petrov-Galerkin projection of u.
In Section 2.2, we present an extension of the primal-dual approach to the case of vector-
valued variables of interest, where the variable of interest is estimated from a standard
Petrov-Galerkin projection of the primal variable and a projection of the solution of a dual
problem. Finally, in Section 2.3, we introduce a goal-oriented projection method based on a
saddle-point formulation.

Before going further, let us introduce some additional notations. For a Hilbert space
H, we denote by RH ∈ L(H, H(cid:48)) the Riesz map such that (cid:107)v(cid:107)2
H = (cid:104)RHv, v(cid:105), where (cid:104)·,·(cid:105)
denotes the duality pairing. The dual norm (cid:107) · (cid:107)H(cid:48) on H(cid:48) is such that RH(cid:48) = R−1
H . Then
(cid:107)v(cid:107)H = (cid:107)RHv(cid:107)H(cid:48) and |(cid:104)v, w(cid:105)| ≤ (cid:107)v(cid:107)H(cid:107)w(cid:107)H(cid:48) hold for any v ∈ H and w ∈ H(cid:48). For any
operator C ∈ L(H1, H(cid:48)
1) denotes the
adjoint of C, such that (cid:104)Cv1, v2(cid:105) = (cid:104)v1, C∗v2(cid:105) for any v1 ∈ H1 and v2 ∈ H2.

2), with H1 and H2 two Hilbert spaces, C∗ ∈ L(H2, H(cid:48)

1A is a norm-isomorphism if it is a continuous and weakly coercive operator satisfying the assumptions

of the Neˇcas’ theorem [5, Chapter 2].

4

2.1 Petrov-Galerkin projection
Suppose that we are given a subspace Vr ⊂ V of ﬁnite dimension r in which we seek an
r = arg minv∈Vr (cid:107)u−
approximation of u. The orthogonal projection u∗
v(cid:107)V , is characterized by

r of u on Vr, given by u∗

(4)
In practice, an approximation ur ∈ Vr can be deﬁned as a Petrov-Galerkin projection of u
characterized by

(cid:104)u − u∗

r, RV v(cid:105) = 0,

∀v ∈ Vr.

(cid:104)Aur − b, y(cid:105) = 0,

∀y ∈ Wr,

where Wr ⊂ W is a test space of dimension r. Under the assumption that

αVr,Wr = inf

0(cid:54)=v∈Vr

sup
0(cid:54)=y∈Wr

(cid:104)Av, y(cid:105)
(cid:107)v(cid:107)V (cid:107)y(cid:107)W

> 0,

the following proposition provides a quasi-optimality result for ur and gives an error bound
for the approximation of the variable of interest.

Proposition 2.1. Under assumption (6), the solution ur of equation (5) satisﬁes

(cid:107)u − ur(cid:107)V ≤

1(cid:112)1 − (δVr,Wr)2

(cid:107)u − v(cid:107)V .

min
v∈Vr

(5)

(6)

(7)

(8)

(9)

δVr,Wr = max
0(cid:54)=v∈Vr

min
y∈Wr

(cid:107)v − R−1
V A∗y(cid:107)V
(cid:107)v(cid:107)V

< 1.

(cid:107)s − Lur(cid:107)Z ≤

(cid:112)1 − (δVr,Wr)2

δL
Wr

(cid:107)u − v(cid:107)V ,

min
v∈Vr

where

Furthermore,

with

Proof. With u∗
have

δL
Wr = sup
0(cid:54)=z(cid:48)∈Z(cid:48)

(10)
r the orthogonal projection of u on Vr, for any v ∈ Vr \ {0} and y ∈ Wr, we

(cid:107)z(cid:48)(cid:107)Z(cid:48)

min
y∈Wr

.

(cid:107)L∗z(cid:48) − A∗y(cid:107)V (cid:48)

r − ur, RV v(cid:105) (4)
(cid:104)u∗

(5)

= (cid:104)u − ur, RV v(cid:105) = (cid:104)b − Aur, A−∗RV v(cid:105)
= (cid:104)b − Aur, A−∗RV v − y(cid:105) = (cid:104)u − ur, RV v − A∗y(cid:105)
≤ (cid:107)u − ur(cid:107)V (cid:107)RV v − A∗y(cid:107)V (cid:48).

Taking the minimum over y ∈ Wr, dividing by (cid:107)v(cid:107)V and taking the supremum over v ∈
r − ur(cid:107)V ≤ δVr,Wr(cid:107)u − ur(cid:107)V , where δVr,Wr is deﬁned by (8). Thanks to
Vr \ {0}, we obtain (cid:107)u∗

5

the orthogonality condition (4) we have (cid:107)u − ur(cid:107)2
r(cid:107)2
V = (cid:107)u − u∗
V + (cid:107)u∗
V , from which
r(cid:107)2
V ≤ (cid:107)u − u∗
we deduce that (1 − δ2
V . To prove (7), it remains to prove that
δVr,Wr < 1. Noting that

Vr,Wr)(cid:107)u − ur(cid:107)2

r − ur(cid:107)2

(cid:107)v − R−1

V A∗y(cid:107)2

V = min

0(cid:54)=y∈Wr,λ∈R(cid:107)v − λR−1

V A∗y(cid:107)2

V = min
0(cid:54)=y∈Wr

(cid:107)v(cid:107)2 − (cid:104)v, A∗y(cid:105)2
(cid:107)A∗y(cid:107)2

V

,

min
y∈Wr

we obtain

(cid:104)Av, y(cid:105)2
V (cid:107)A∗y(cid:107)2
V (cid:48)
Then, using (cid:107)A∗y(cid:107)V (cid:48) ≤ β(cid:107)y(cid:107)W and assumption (6), we obtain

Vr,Wr = 1 − min
δ2
0(cid:54)=v∈Vr

max
0(cid:54)=y∈Wr

(cid:107)v(cid:107)2

Vr,Wr ≤ 1 − α2
δ2

Vr,Wr

β2 < 1.

.

(11)

(12)

Furthermore for any z(cid:48) ∈ Z(cid:48) \ {0} and y ∈ Wr, we have

(cid:104)s − Lur, z(cid:48)(cid:105) = (cid:104)b − Aur, A−∗L∗z(cid:48)(cid:105) (5)

= (cid:104)b − Aur, A−∗L∗z(cid:48) − y(cid:105)

≤ (cid:107)u − ur(cid:107)V (cid:107)L∗z(cid:48) − A∗y(cid:107)V (cid:48).

Taking the minimum over y ∈ Wr, dividing by (cid:107)z(cid:48)(cid:107)Z(cid:48) and taking the supremum over
z(cid:48) ∈ Z(cid:48) \ {0}, we obtain (9) thanks to (7).

The error bound (9) for the approximation of the variable of interest s is the product of

three terms:
(a) minv∈Vr (cid:107)u − v(cid:107)V , which suggests that the approximation space Vr should be deﬁned

such that u can be well approximated in Vr,

(b) (1 − (δVr,Wr)2)−1/2, which suggests that the test space Wr should be chosen such that

any element of Vr can be well approximated by an element of R−1
Wr, which suggests that any element of range(L∗) should be well approximated by an
element of A∗Wr.

V A∗Wr, and

(c) δL

As already noticed in [14, Section 11.1], Wr plays a double role: a test space for the deﬁnition
of ur (point (b)) and an approximation space for the range of A−∗L∗ (point (c)).

6

Remark 2.2 (Comparison with the C´ea’s Lemma). Under assumption (6), the classical
C´ea’s lemma states that

(cid:107)u − ur(cid:107)V ≤ (1 +

β

αVr,Wr

) min
v∈Vr

(cid:107)u − v(cid:107)V .

From (12), we obtain that

1(cid:112)1 − (δVr,Wr)2

≤ β

αVr,Wr

,

which means that the inequality (7) is sharper than

(cid:107)u − ur(cid:107)V ≤ β

αVr,Wr

(cid:107)u − v(cid:107)V ,

min
v∈Vr

(13)

(14)

which is itself sharper than (13) (a similar result has been obtained in [16]).

Remark 2.3 (Symmetric coercive case and compliant case). We suppose that A is a sym-
metric coercive operator, with V = W and (cid:107) · (cid:107)V = (cid:107) · (cid:107)W the norm induced by the operator
A such that RV = A. Then δVr,Wr deﬁned by (8) admits the following simple expression

δVr,Wr = sup
0(cid:54)=v∈Vr

min
y∈Wr

(cid:107)v − y(cid:107)V
(cid:107)v(cid:107)V

.

If the test space Wr is deﬁned by Wr = Vr, we obtain δVr,Wr = 0, and from (7), we obtain
ur = u∗
In other words, the standard Galerkin projection coincides with the orthogonal
r.
projection.

In the case where the variable of interest s is scalar-valued, we have Z = R and L(V, Z) =
V (cid:48). The so-called compliant case corresponds to Lv = (cid:104)b, v(cid:105) for any v ∈ V . Then, by
deﬁnition (10), we have

δL
Wr = min
v∈Vr

(cid:107)b − Av(cid:107)V (cid:48) = min
v∈Vr

(cid:107)u − v(cid:107)V = (cid:107)u − ur(cid:107)V ,

and thanks to (9), we recover the so-called “squared eﬀect”

(cid:107)s − Lur(cid:107)Z = |s − Lur| ≤ (cid:107)u − ur(cid:107)2
V .

2.2 Primal-dual approach

We now extend the classical primal-dual approach [11] for the estimation of a vector-valued
variable of interest.

Let us introduce the dual variable Q ∈ L(Z(cid:48), W ) deﬁned by A∗Q = L∗. The relation

s = Lu = Q∗Au = Q∗b

7

shows that the variable of interest can be exactly determined if either the primal variable u
or the dual variable Q is known.

Now, for given approximations(cid:101)u of u and (cid:101)Q of Q, we deﬁne the approximation ˜s of s by
where L(cid:101)u is the standard estimation of the variable of interest and where (cid:101)Q∗(b − A(cid:101)u) is a

(cid:101)s = L(cid:101)u + (cid:101)Q∗(b − A(cid:101)u),

(15)

correction using the approximation of the dual variable. The following proposition provides
an error bound on the variable of interest, which is a generalization of the classical error
bound for scalar-valued variables of interest (see [11]) to vector-valued variables of interest.

Proposition 2.4. The approximation ˜s of s deﬁned by (15) satisﬁes

(cid:107)s −(cid:101)s(cid:107)Z ≤ (cid:107)u −(cid:101)u(cid:107)V (cid:107)L∗ − A∗(cid:101)Q(cid:107)Z(cid:48)→V (cid:48),
(cid:107)(L∗ − A∗(cid:101)Q)z(cid:48)(cid:107)V (cid:48)
(cid:107)L∗ − A∗(cid:101)Q(cid:107)Z(cid:48)→V (cid:48) = sup

0(cid:54)=z(cid:48)∈Z(cid:48)

(cid:107)z(cid:48)(cid:107)Z(cid:48)

(16)

(17)

.

where

Proof. For any z(cid:48) ∈ Z(cid:48), we have

(cid:104)s −(cid:101)s, z(cid:48)(cid:105) = (cid:104)Lu − L(cid:101)u − (cid:101)Q∗(b − A(cid:101)u), z(cid:48)(cid:105) = (cid:104)(L − (cid:101)Q∗A)(u −(cid:101)u), z(cid:48)(cid:105)
= (cid:104)u −(cid:101)u, (L∗ − A∗(cid:101)Q)z(cid:48)(cid:105) ≤ (cid:107)u −(cid:101)u(cid:107)V (cid:107)(L∗ − A∗(cid:101)Q)z(cid:48)(cid:107)V (cid:48).

Dividing by (cid:107)z(cid:48)(cid:107)Z(cid:48) and taking the supremum over z(cid:48) ∈ Z(cid:48) \ {0}, we obtain (16).

In practice, the approximation (cid:101)u can be deﬁned as the Petrov-Galerkin projection ur of u
approximation (cid:101)Q of Q ∈ L(Z(cid:48), W ), the bound (16) suggests that (cid:107)L∗ − A∗(cid:101)Q(cid:107)Z(cid:48)→V (cid:48) should
be small. We then propose to choose (cid:101)Q as a solution of

on a given approximation space Vr with a given test space Wr, see equation (5). For the

(cid:107)L∗ − A∗(cid:101)Q(cid:107)Z(cid:48)→V (cid:48),

(cid:101)Q∈L(Z(cid:48),W Q

min

k )

(18)

k ⊂ W is a given approximation space (diﬀerent from Wr). The next proposition

where W Q
shows how to construct a solution of (18).

8

Proposition 2.5. The operator Qk : Z(cid:48) → W Q

k deﬁned for z(cid:48) ∈ Z(cid:48) by
(cid:107)L∗z(cid:48) − A∗yk(cid:107)V (cid:48)

Qkz(cid:48) = arg min
yk∈W Q

k

is linear and is a solution of (18). Moreover Qkz(cid:48) ∈ W Q

k is characterized by

(cid:104)L∗z(cid:48) − A∗Qkz(cid:48), R−1

V A∗yk(cid:105) = 0,

∀yk ∈ W Q
k .

(19)

(20)

(cid:107)z(cid:48)(cid:107)Z(cid:48)

Proof. We easily prove that the optimization problem (19) admits a unique solution which
depends linearly and continuously on z(cid:48), so that Qk deﬁned by (19) is a linear operator in
L(Z(cid:48), W Q
k ). Equation (20) is the Euler equation associated to the minimization problem

(19). Furthermore for any (cid:101)Q ∈ L(Z(cid:48), W Q

k ) and z(cid:48) ∈ Z(cid:48) \ {0}, we have

(cid:107)L∗z(cid:48) − A∗Qkz(cid:48)(cid:107)V (cid:48)

≤ (cid:107)L∗ − A∗(cid:101)Q(cid:107)Z(cid:48)→V (cid:48).
Taking the supremum over z(cid:48) ∈ Z(cid:48)\{0} and then the inﬁmum over (cid:101)Q ∈ L(Z(cid:48), W Q
that (cid:107)L∗−A∗Qk(cid:107)Z(cid:48)→V (cid:48) ≤ inf(cid:101)Q∈L(Z(cid:48),W Q

(19)≤ (cid:107)L∗z(cid:48) − A∗(cid:101)Qz(cid:48)(cid:107)V (cid:48)
k ) (cid:107)L∗−A∗(cid:101)Q(cid:107)Z(cid:48)→V (cid:48), which means that Qk ∈ L(Z(cid:48), W Q
In practice, for computing the approximation of the variable of interest (15) with (cid:101)Q = Qk, we
k(b − Aur). The following lemma shows how this can be performed

k ), we obtain
k )

is a solution of (18).

(cid:107)z(cid:48)(cid:107)Z(cid:48)

only need to compute Q∗
without computing the operator Qk.

Lemma 2.6. Let Qk be deﬁned by (19). Then for r = b − Aur ∈ W (cid:48),

kr = LR−1
Q∗

V A∗y∗
k,

where y∗

k ∈ W Q

k is deﬁned by

(cid:104)AR−1

V A∗y∗

k, yk(cid:105) = (cid:104)r, yk(cid:105),

∀yk ∈ W Q
k .

Proof. For any z(cid:48) ∈ Z(cid:48), since Qkz(cid:48) ∈ W Q
(cid:104)Qkz(cid:48), AR−1

k , we have
k(cid:105) (22)
V A∗y∗

= (cid:104)Qkz(cid:48), r(cid:105).

Furthermore, by deﬁnition of Qk we have
V A∗y∗

(cid:104)Qkz(cid:48), AR−1

k(cid:105) (20)

= (cid:104)L∗z(cid:48), R−1

k(cid:105).
V A∗y∗

9

(21)

(22)

(23)

(24)

Combining (23) and (24), we obtain (cid:104)z(cid:48), Q∗
cludes the proof.

kr(cid:105) = (cid:104)z(cid:48), LR−1

V A∗y∗

k(cid:105) for all z(cid:48) ∈ Z(cid:48), which con-

We give now a sharper bound of the error on the variable of interest taking the advantage
of the property (20).

Proposition 2.7. Let(cid:101)u = ur be the Petrov-Galerkin projection deﬁned by (5) and let (cid:101)Q = Qk
be deﬁned by (20). Then the approximation(cid:101)s deﬁned by (15) satisﬁes
V A∗yk(cid:107)V ,

(cid:107)s −(cid:101)s(cid:107)Z ≤ δL

(cid:107)u − ur − R−1

(25)

W Q
k

min
yk∈W Q

k

where

Moreover,

δL
W Q
k

= sup
0(cid:54)=z(cid:48)∈Z(cid:48)

min
y∈W Q

k

(cid:107)L∗z(cid:48) − A∗y(cid:107)V (cid:48)

(cid:107)z(cid:48)(cid:107)Z(cid:48)

.

(26)

(27)

(28)

Proof. For any z(cid:48) ∈ Z(cid:48), and for any yk ∈ W Q

δL
W Q
k

(cid:107)s −(cid:101)s(cid:107)Z ≤

(cid:112)1 − (δVr,Wr)2
(cid:104)s −(cid:101)s, z(cid:48)(cid:105) = (cid:104)u − ur, (L∗ − A∗Qk)z(cid:48)(cid:105)

k we have

min
v∈Vr

(cid:107)u − v(cid:107)V .

(20)

= (cid:104)u − ur − R−1
≤ (cid:107)u − ur − R−1

V A∗yk, (L∗ − A∗Qk)z(cid:48)(cid:105)
V A∗yk(cid:107)V (cid:107)(L∗ − A∗Qk)z(cid:48)(cid:107)V (cid:48).

From (19), we have

(cid:107)(L∗ − A∗Qk)z(cid:48)(cid:107)V (cid:48) = min
yk∈W Q

k

(cid:107)L∗z(cid:48) − A∗yk(cid:107)V (cid:48).

Dividing by (cid:107)z(cid:48)(cid:107)Z and taking the supremum over z(cid:48) ∈ Z(cid:48) \ {0} in (28), we obtain

(cid:107)s −(cid:101)s(cid:107)Z ≤ δL

W Q
k

(cid:107)u − ur − R−1

V A∗yk(cid:107)V

Then, taking the minimum over yk ∈ W Q
obtain (27) from (7).

k , we obtain (25). Finally, taking yk = 0 in (25), we

10

2.3 Projection based on a saddle point problem

In this section we extend the method proposed in [4] for the approximation of (vector-valued)
variables of interest. The idea is to deﬁne the projection of u on the reduced space Vr by
means of a saddle point problem. We ﬁrst deﬁne and analyze this saddle point problem.
Then we use the solution of this problem for the estimation of the variable of interest.

Let us equip W with a norm (cid:107) · (cid:107)W such that the relation (cid:107)y(cid:107)W = (cid:107)A∗y(cid:107)V (cid:48) holds for any
y ∈ W , which is equivalent to the following relation between the Riesz maps RW and RV :

RW = AR−1

V A∗.

(29)

The orthogonal projection u∗

(cid:107)u − u∗

r(cid:107)V = min
v∈Vr

r of u on Vr satisﬁes
(cid:107)u − v(cid:107)V = min
v∈Vr

max
0(cid:54)=w∈V

(cid:104)Av − b, A−∗RV w(cid:105)

(cid:104)u − v, RV w(cid:105)

(cid:107)w(cid:107)V

= min
v∈Vr

max
0(cid:54)=w∈V

= min
v∈Vr

max
0(cid:54)=y∈W

(cid:107)w(cid:107)V
(cid:104)Av − b, y(cid:105)

(cid:107)y(cid:107)W

.

= min
v∈Vr

max
0(cid:54)=y∈W

(cid:104)Av − b, y(cid:105)
(cid:107)R−1
V A∗y(cid:107)V

Starting from this observation, we introduce a subspace Tp ⊂ W of dimension p and we
deﬁne the projection ur,p in Vr as the solution of the saddle point problem

min
v∈Vr

(cid:104)Av − b, w(cid:105).

max
w∈Tp
(cid:107)w(cid:107)W =1

(30)

In the following proposition, we prove the well-posedness of (30) under the condition (discrete
inf-sup condition)

(cid:104)Av, y(cid:105)
(cid:107)v(cid:107)V (cid:107)y(cid:107)W
and we provide a practical characterization of ur,p.
Proposition 2.8. Under assumption (31), there exists a unique solution (ur,p, yr,p) ∈ Vr×Tp
to

:= αVr,Tp > 0,

(31)

inf

0(cid:54)=v∈Vr

sup
0(cid:54)=y∈Tp

(cid:104)RW yr,p, y(cid:105) + (cid:104)Aur,p, y(cid:105) = (cid:104)b, y(cid:105) ∀y ∈ Tp,
∀v ∈ Vr,

(cid:104)A∗yr,p, v(cid:105) = 0

(32a)

(32b)

and (ur,p,

yr,p

(cid:107)yr,p(cid:107)W

) is the unique solution of (30).

11

Proof. Since the Riesz map RW deﬁned by (29) is coercive and under the discrete inf-sup
condition (31) on operator A, Theorem 2.34 of [5] gives that (32) is a well-posed problem
whose solution (ur,p, yr,p) is the unique solution of the saddle-point problem

min
v∈Vr

max
y∈Tp

−1
2

(cid:104)RW y, y(cid:105) + (cid:104)b, y(cid:105) − (cid:104)Av, y(cid:105).

Denoting y = λw with (cid:107)w(cid:107)W = 1, this saddle point problem is equivalent to

min
v∈Vr

max
w∈Tp
(cid:107)w(cid:107)W =1

λ∈R −1

max

2

λ2 + λ(cid:104)b − Av, w(cid:105) = min
v∈Vr

(cid:104)Av − b, w(cid:105)2,

1
2

max
w∈Tp
(cid:107)w(cid:107)W =1

which coincides with problem (30).

The following proposition provides a quasi-optimality result for the projection ur,p ∈ Vr

of u onto Vr.

Proposition 2.9. Under assumption (31), the solution ur,p of (32) is such that

(cid:107)u − ur,p(cid:107)V ≤

1(cid:113)

1 − δ2

Vr,Tp

(cid:107)u − v(cid:107)V ,

min
v∈Vr

where

is such that

δVr,Tp = max
0(cid:54)=v∈Vr

min
y∈Tp

(cid:107)v − R−1
V A∗y(cid:107)V
(cid:107)v(cid:107)V

Vr,Tp = 1 − α2
δ2

Vr,Tp < 1.

Proof. Let (ur,p, yr,p) be the solution of (32). For any v ∈ Vr and y ∈ Tp, we have

(cid:104)u∗
r − ur,p, RV v(cid:105) (4)

(32a)

= (cid:104)u − ur,p, RV v(cid:105) = (cid:104)b − Aur,p, A−∗RV v(cid:105)
= (cid:104)b − Aur,p, A−∗RV v − y(cid:105) + (cid:104)RW yr,p, y(cid:105)
= (cid:104)b − Aur,p, A−∗RV v − y(cid:105) − (cid:104)RW yr,p, A−∗RV v − y(cid:105)
= (cid:104)u − ur,p − R−1
≤ (cid:107)u − ur,p − R−1

V A∗yr,p, RV v − A∗y(cid:105)
V A∗yr,p(cid:107)V (cid:107)RV v − A∗y(cid:107)V (cid:48).

(32b)

(33)

(34)

(35)

(36)
W (b −

Equation (32a) implies that yr,p = arg miny∈W (cid:107)R−1
Aur,p) − yr,p(cid:107)W ≤ (cid:107)R−1

W (b − Aur,p)(cid:107)W . Using (29), it comes

W (b − Aur,p) − y(cid:107)W , so that (cid:107)R−1

(cid:107)u − ur,p − R−1

V A∗yr,p(cid:107)V ≤ (cid:107)u − ur,p(cid:107)V ,

(37)

12

Using (37) in (36), taking the minimum over y ∈ Tp, dividing by (cid:107)v(cid:107)V and taking the
maximum over v ∈ Vr \ {0}, we obtain

(cid:107)u∗

r − ur,p(cid:107)V ≤ δVr,Tp(cid:107)u − ur,p(cid:107).

From (11) (with Wr replaced by Tp) and (29) (which implies (cid:107)A∗y(cid:107)V (cid:48) = (cid:107)y(cid:107)W ), we obtain
(35). Then, from the deﬁnition of u∗

r, we have

(cid:107)u − ur,p(cid:107)2 = (cid:107)u − u∗

r(cid:107)2 + (cid:107)u∗

r − ur,p(cid:107)2 ≤ (cid:107)u − u∗

r(cid:107)2 + δ2

Vr,Tp(cid:107)u − ur,p(cid:107)2,

from which we deduce (33).

From the deﬁnition (34) of δVr,Tp, we easily deduce the following corollary.
Corollary 2.10. If Tp is such that R−1
δVr,Tp = 0 and ur,p coincides with the best approximation u∗

W AVr ⊂ Tp (or equivalently Vr ⊂ R−1

r of u in Vr.

V A∗Tp), then

Remark 2.11. Note that (33) and (35) give
(cid:107)u − ur,p(cid:107)V ≤ 1
αVr,Tp

(cid:107)u − v(cid:107)V ,

min
v∈Vr

which is sharper than the classical error bound obtained by the Cea’s lemma

(cid:107)u − ur,p(cid:107)V ≤ (1 +

Now, we consider the approximation(cid:101)s of s deﬁned by

αVr,Tp

) min
v∈Vr

1

(cid:107)u − v(cid:107)V .

(cid:101)s = Lur,p + LR−1

V A∗yr,p,

(38)
where (ur,p, yr,p) ∈ Vr × Tp is the solution of the saddle point problem (32). The following
proposition provides an error bound for the approximation of the variable of interest.

Proposition 2.12. The approximation(cid:101)s deﬁned by (38) satisﬁes
V A∗yr,p(cid:107)V ,

(cid:107)s −(cid:101)s(cid:107)Z ≤ δL

Tp(cid:107)u − ur,p − R−1

with

and

δL
Tp = sup
0(cid:54)=z(cid:48)∈Z(cid:48)

min
y∈Tp

(cid:107)L∗z(cid:48) − A∗y(cid:107)V (cid:48)

(cid:107)z(cid:48)(cid:107)Z(cid:48)

,

(cid:107)s −(cid:101)s(cid:107)Z ≤

(cid:112)1 − (δVr,Tp)2

δL
Tp

(cid:107)u − v(cid:107)V .

min
v∈Vr

13

(39)

(40)

(41)

Proof. For any z(cid:48) ∈ Z(cid:48) and y ∈ Tp, we have

(cid:104)s −(cid:101)s, z(cid:48)(cid:105) = (cid:104)u − ur,p − R−1

(32a)

= (cid:104)u − ur,p − R−1
≤ (cid:107)u − ur,p − R−1

V A∗yr,p, L∗z(cid:48)(cid:105)
V A∗yr,p, L∗z(cid:48) − A∗y(cid:105)
V A∗yr,p(cid:107)V (cid:107)L∗z(cid:48) − A∗y(cid:107)V (cid:48).

Taking the minimum over y ∈ Tp, dividing by (cid:107)z(cid:48)(cid:107)Z(cid:48) and taking the supremum over z(cid:48) ∈
Z(cid:48) \ {0}, we obtain (39). Finally, thanks to (39), (37) and (33), we obtain (41).

We observe that Tp impacts both the quality of the projection of u (via the constant
δVr,Tp in (33)) and the quality of the approximation of the variable of interest (via constants
δVr,Tp and δL

Tp in (41)). Then, we will consider for Tp spaces of the form

Tp = Wr + W Q
k ,

(42)

with dim(Wr) = r. This implies

Tp ≤ δL
δL

W Q
k

and δVr,Tp ≤ δVr,Wr,

so that the error bound (41) for the variable of interest is better than the error bound (27) of
the primal-dual method with primal approximation space Vr, primal test space Wr and dual
approximation space W Q
k . Therefore, we expect the approximation ur,p to be closer to the
solution u than the Petrov-Galerkin projection ur. Also, the approximation of the quantity
of interest is expected to be improved.

Remark 2.13 (Symmetric coercive case). Let us consider the case where A is symmetric
and coercive, RV = RW = A and Wr = Vr. The choice (42) implies that Vr ⊂ Tp, so that
Tp admits the orthogonal decomposition Tp = Vr ⊕ (Tp ∩ V ⊥
r ). Equation (32b) implies that
r . Let tr,p = yr,p + ur,p ∈ Tp. Equation (32a) gives (cid:104)RV tr,p, y(cid:105) = (cid:104)RV u, y(cid:105) for
yr,p ∈ Tp ∩ V ⊥
all y ∈ Tp, which implies that tr,p is the orthogonal projection of u on Tp, where ur,p and
approximation of the variable of interest (38) is given by (cid:101)s = Ltr,p. We conclude that in
yr,p are the orthogonal projections of u on Vr and Tp ∩ V ⊥
respectively. Furthermore, the

this particular setting, the saddle point approach can be simply interpreted as an orthogonal
projection of u on the enriched space Tp = Vr + W Q
k , followed by a standard estimation of
the variable of interest.

r

14

3 Goal-oriented projections for parameter-dependent

equations

We now consider a parameter-dependent equation A(ξ)u(ξ) = b(ξ) where ξ denotes a pa-
rameter taking values in a set Ξ ⊂ Rd, A(ξ) ∈ L(V, W (cid:48)) and b(ξ) ∈ W (cid:48). The variable of
interest is deﬁned by s(ξ) = L(ξ)u(ξ), with L(ξ) ∈ L(V, Z).

In Section 2, we have presented diﬀerent projection methods for the estimation of the
variable of interest which rely on the introduction of three spaces: the primal approxima-
tion space Vr, the primal test space Wr and the dual approximation space W Q
k . We recall
that for the saddle point approach, we introduce the space Tp = Wr + W Q
k . We adopt an
oﬄine/online strategy. Reduced (low-dimensional) spaces Vr, Wr and W Q
k are constructed
during the oﬄine phase. Then, the projections on these reduced spaces and the evaluations
of the variable of interest are rapidly computed for any parameter value ξ ∈ Ξ during the
online phase.

In section 3.1, we will ﬁrst consider the construction of the test space Wr. For scalar-
valued variables of interest, reduced spaces Vr and W Q
k are classically deﬁned as the span of
snapshots of the primal and dual solutions u(ξ) and Q(ξ). These snapshots can be selected
at random, using samples drawn according a certain probability measure over Ξ, see e.g.
[12]. Another popular method is to select the snapshots in a greedy way [3, 7, 14], with

a uniform control of the error (cid:107)s(ξ) −(cid:101)s(ξ)(cid:107)Z over Ξ. This method requires an estimation

of the error on the variable of interest. In the same lines, we introduce error estimates for
vector-valued variables of interest in Section 3.2, and we propose greedy algorithms for the
construction of Vr and W Q

k in Section 3.3.

3.1 Construction of the test space Wr

Assuming that the primal approximation space Vr is given, we know from the previous
section that Wr should be chosen such that δVr,Wr is as close to zero as possible (see Propo-
sitions 2.1, 2.7 and 2.9). In the literature, Wr = Vr is a common choice (standard Galerkin
projection). When the operator A(ξ) is symmetric and coercive, we can choose Wr = Vr
which is the optimal test space with respect to the norm induced by A(ξ) (see remark
2.3). However, this choice may lead to an inaccurate projection of the primal variable
α (cid:29) 1). In the case of non coercive operators, a
when the operator is ill-conditioned (i.e. β
parameter-dependent test space is generally deﬁned by Wr = Wr(ξ) = R−1
W A(ξ)Vr, where
R−1
W A(ξ) is called the “supremizer operator” (see e.g. [15]). This approach is no more than

15

a minimal residual method since the resulting Petrov-Galerkin projection deﬁned by (5) is
ur(ξ) = arg minvr∈Vr (cid:107)A(ξ)vr−b(ξ)(cid:107)W (cid:48). In Section 2.1, we have seen that the Petrov-Galerkin
projection with an ideal test space

Wr(ξ) = A(ξ)−∗RV Vr

(43)

coincides with the best approximation. Having a basis v1, . . . , vr of Vr, the computation
of this ideal parameter-dependent test space would require the computation of A−∗(ξ)RV vi
for all 1 ≤ i ≤ r for each parameter’s value ξ, which is unfeasible in practice. Up to
our knowledge, the only attempt to construct quasi-optimal test spaces for non symmetric
and weakly coercive operators can be found in [4], where the authors proposed a greedy
algorithm for the construction of a (parameter independent) test space which ensures the
quasi-optimality constant to be uniformly bounded by an arbitrarily small constant. Here,
we adopt an alternative approach where the (parameter-dependent) test space is deﬁned by

Wr(ξ) = Pm(ξ)∗RV Vr,

(44)

where Pm(ξ) is an interpolation of the inverse of A(ξ) using m interpolation points in the
parameter set Ξ. In practice, when A(ξ) is a matrix, algorithms developed in [17] can be
used. This will be detailed later on. The underlying idea is to obtain a test space as close as
possible to the ideal test space A−∗(ξ)RV Vr deﬁned in (43). For m = 0, with P0(ξ) = R−1
by convention, we have Wr = Vr, which yields the standard Galerkin projection.

V

3.2 Error estimates for vector-valued variables of interest

In this section, we propose practical error estimates for the variable of interest, ﬁrst for the
primal-dual approach and then for the saddle point method.

3.2.1 Primal-dual approach

Given approximations(cid:101)u and (cid:101)Q of the primal solution u and the dual solution Q respectively,

a standard approach is to start from the error bound

(cid:107)s(ξ) −(cid:101)s(ξ)(cid:107)Z ≤ (cid:107)u(ξ) −(cid:101)u(ξ)(cid:107)V (cid:107)L(ξ)∗ − A(ξ)∗(cid:101)Q(ξ)(cid:107)Z(cid:48)→V (cid:48),

which is provided by Proposition 2.4. This suggests to measure the norm of the residuals
associated to the primal and dual variables. In practice, we distinguish two cases.

In the case where the operator A(ξ) is symmetric and coercive, it is natural to choose the
parameter-dependent norm (cid:107) · (cid:107)V as the one induced by the operator, i.e. RV = RV (ξ) =

16

A(ξ). However, neither the primal error (cid:107)u(ξ) −(cid:101)u(ξ)(cid:107)V = (cid:104)b(ξ) − A(ξ)(cid:101)u(ξ), u(ξ) −(cid:101)u(ξ)(cid:105)
nor the dual residual norm (cid:107)L(ξ)∗ − A(ξ)∗(cid:101)Q(ξ)(cid:107)Z(cid:48)→V (cid:48) = sup(cid:107)z(cid:107)Z =1 (cid:107)Q(ξ)z − (cid:101)Q(ξ)z(cid:107)Z can be

computed without computing the primal and dual solutions u(ξ) and Q(ξ). The classical
way to circumvent this issue is to introduce a parameter-independent norm (cid:107) · (cid:107)V0, which is
in general the “natural” norm associated to the space V , and to measure residuals with the
associated dual norm (cid:107) · (cid:107)V (cid:48)

. Here we assume that the operator A(ξ) satisﬁes

0

for all v ∈ V , where α(ξ) > 0. By deﬁnition of the norm (cid:107) · (cid:107)V , we can write

α(ξ)(cid:107)v(cid:107)V0 ≤ (cid:107)A(ξ)v(cid:107)V (cid:48)

0

(45)

(cid:107)v(cid:107)2

V = (cid:104)A(ξ)v, v(cid:105) ≤ (cid:107)A(ξ)v(cid:107)V (cid:48)

(cid:107)v(cid:107)V0 ≤ α(ξ)−1(cid:107)A(ξ)v(cid:107)2
V (cid:48)
Then we have (cid:107)u(ξ)−(cid:101)u(ξ)(cid:107)V ≤ α(ξ)−1/2(cid:107)A(ξ)(cid:101)u(ξ)− b(ξ)(cid:107)V (cid:48)
that (cid:107)L(ξ)∗ − A(ξ)∗(cid:101)Q(ξ)(cid:107)Z(cid:48)→V (cid:48) ≤ α(ξ)−1/2(cid:107)L(ξ)∗ − A(ξ)∗(cid:101)Q(ξ)(cid:107)Z(cid:48)→V (cid:48)
(cid:107)L(ξ)∗ − A(ξ)∗(cid:101)Q(ξ)(cid:107)Z(cid:48)→V (cid:48)

(cid:107)s(ξ) −(cid:101)s(ξ)(cid:107)Z ≤ (cid:107)A(ξ)(cid:101)u(ξ) − b(ξ)(cid:107)V (cid:48)

0

0

0

0

0

α(ξ)

∀v ∈ V.

. In the same way, we can prove

. Finally, we obtain

0

:= ∆(ξ),

(46)

where ∆(ξ) is a certiﬁed error bound for the variable of interest, which involves computable
primal and dual residual norms.

In the general case, we consider for (cid:107) · (cid:107)V the natural norm on V , i.e. (cid:107) · (cid:107)V = (cid:107) · (cid:107)V0.
error (cid:107)u(ξ) −(cid:101)u(ξ)(cid:107)V0 requires the primal solution u(ξ) which is not available in practice.
As a consequence, the norm of the dual residual is computable, but the computation of the
(cid:107)u(ξ) −(cid:101)u(ξ)(cid:107)V0 ≤ α(ξ)−1(cid:107)A(ξ)(cid:101)u(ξ) − b(ξ)(cid:107)V (cid:48)

Once again, we assume that the operator satisﬁes the property (45) so that we can write
. Then we end up with the same error bound

0

(46) for the variable of interest.

3.2.2 Saddle point method

We now derive new error bounds in the case where the approximation (cid:101)s(ξ) is obtained by

the saddle point method introduced in Section 2.3. Let us start from the error bound

(cid:107)s(ξ) −(cid:101)s(ξ)(cid:107)Z

≤ sup
0(cid:54)=z(cid:48)∈Z(cid:48)

min
y∈Tp

(cid:107)L(ξ)∗z(cid:48) − A(ξ)∗y(cid:107)V (cid:48)

(cid:107)z(cid:48)(cid:107)Z(cid:48)

(cid:107)u(ξ) − ur,p(ξ) − R−1

V A(ξ)∗yr,p(ξ)(cid:107)V

provided by Proposition 2.9. Once again, we distinguish two cases.

17

For the case where the operator A(ξ) is symmetric and coercive, we consider for (cid:107)·(cid:107)V the
norm induced by the operator, i.e. RV = A. According to Remark 2.13, the quantity tr,p(ξ) =
ur,p(ξ) − R−1
V (ξ)A(ξ)∗yr,p(ξ) = ur,p(ξ) + yr,p(ξ) is nothing but the orthogonal projection of
u(ξ) onto Tp = Wr + W Q

k , with Wr = Vr. Then for any(cid:101)tr,p ∈ Tp we have
V ≤ α(ξ)−1(cid:107)b(ξ) − A(ξ)(cid:101)tr,p(cid:107)2

V ≤ (cid:107)u(ξ) −(cid:101)tr,p(cid:107)2

(cid:107)u(ξ) − tr,p(ξ)(cid:107)2

,

V (cid:48)

0

where the norm (cid:107) · (cid:107)V0 is the natural norm on V such that (45) holds. Then, taking the

minimum over(cid:101)tr,p ∈(cid:102)Wp we obtain

(cid:107)u(ξ) − tr,p(ξ)(cid:107)V ≤ α(ξ)−1/2 min(cid:101)tr,p∈Tp

(cid:107)A(ξ)(cid:101)tr,p − b(ξ)(cid:107)V (cid:48)

0

.

Finally, we obtain that

(cid:107)s(ξ) −(cid:101)s(ξ)(cid:107)Z

≤ 1
α(ξ)

sup
0(cid:54)=z(cid:48)∈Z(cid:48)

min
y∈Tp

(cid:107)L(ξ)∗z(cid:48) − A(ξ)∗y(cid:107)V (cid:48)

0

(cid:107)z(cid:48)(cid:107)Z(cid:48)

(cid:107)A(ξ)(cid:101)tr,p − b(ξ)(cid:107)V (cid:48)

0

min(cid:101)tr,p∈Tp

:= ∆(ξ).

(47)

Note that the main diﬀerence between this error estimate and the previous one (46) is the
minimization problem over Tp in both primal and dual residuals. This leads to additional
computational costs, but a sharper error bound will be obtained, as illustrated by the nu-
merical examples in the next section.

For the general case, we consider (cid:107) · (cid:107)V = (cid:107) · (cid:107)V0. Once again, using the relation (45), we

obtain the following error estimate

(cid:107)s(ξ) −(cid:101)s(ξ)(cid:107)Z

≤ 1
α(ξ)

min
y∈Tp
where tr,p(ξ) = ur,p(ξ) + R−1

sup
0(cid:54)=z(cid:48)∈Z(cid:48)

V0

A(ξ)∗yr,p(ξ).

(cid:107)L(ξ)∗z(cid:48) − A(ξ)∗y(cid:107)V (cid:48)

0

(cid:107)z(cid:48)(cid:107)Z(cid:48)

(cid:107)A(ξ)tr,p(ξ) − b(ξ)(cid:107)V (cid:48)

0

:= ∆(ξ),

(48)

Remark 3.1. All the proposed error estimates rely on the knowledge of α(ξ). In the case
where α(ξ) can not be easily computed, we can replace it by a lower bound αLB(ξ) ≤ α(ξ),
e.g. provided by a SCM procedure [9]. This option will not be considered here. Another
option is to remove α(ξ) from the deﬁnitions of ∆(ξ), therefore leading to error estimates
which are no more certiﬁed error bounds.

18

3.3 Greedy construction of the reduced spaces

Here, we propose greedy algorithms for the construction of the reduced spaces Vr and W Q
k .
At each iteration, we search for a parameter’s value ξ∗ ∈ Ξ where the error estimate ∆(ξ) is
maximum, i.e.

ξ∗ ∈ arg max
ξ∈Ξ

∆(ξ).

Then we can either simultaneously enrich the primal approximation space

Vr+1 = Vr + span(u(ξ∗))

and the dual approximation space

W Q

k+l = W Q

k + range(Q(ξ∗)),

(49)

(50)

(51)

or alternatively enrich W Q

k and Vr.

Remark 3.2. In the literature, and for scalar-valued variables of interest, the classical ap-
proaches are either a separated construction of Vr and W Q
k (using two independent greedy
algorithms, see for e.g. [6, 14]), or a simultaneous construction (see e.g. [13]). The latter
option can take advantage of a single factorization of the operator A(ξ∗) to compute both
the primal and dual variables. The alternate construction is not usual. This possibility is
mentioned in remark 2.47 of the tutorial [7].

For vector-valued variables of interest (dim(Z) > 1), the enrichment strategy (51) makes
sense only if dim(range(Q(ξ∗)) < ∞, in which case l = dim(W Q
l ) < ∞. How-
ever, if dim(range(Q(ξ∗)) is ﬁnite but very high, the enrichment strategy (51) may lead
to a rapid increase of the dimension of the dual approximation space. Therefore, when
dim(range(Q(ξ∗))) is inﬁnite or very high, we propose to replace the enrichment strategy
(51) by

k+l) − dim(W Q

W Q

k+1 = W Q

k + span(Q(ξ∗)z(cid:48)),

(52)

where the space W Q

k is enriched with a single vector Q(ξ∗)z(cid:48), with z(cid:48) ∈ Z(cid:48) such that

(cid:107)(L(ξ∗)∗ − A(ξ∗)∗(cid:101)Q(ξ))(cid:101)z(cid:48)(cid:107)V (cid:48)
(cid:107)L(ξ∗)∗(cid:101)z(cid:48) − A(ξ∗)∗y(cid:107)V (cid:48)

(cid:107)(cid:101)z(cid:48)(cid:107)Z(cid:48)
(cid:107)(cid:101)z(cid:48)(cid:107)Z(cid:48)

k

0

0

z(cid:48) ∈ arg max(cid:101)z(cid:48)∈Z(cid:48)
z(cid:48) ∈ arg max(cid:101)z(cid:48)∈Z(cid:48) min

y∈W Q

(for primal-dual method), or

(53)

(for saddle point method).

(54)

Contrarily to the full enrichment (51), this partial enrichment does not necessarily lead to a
zero error at the point ξ∗ for the next iterations. Then we expect that (52) will deteriorate
the convergence properties of the algorithm, but for dim(Z) (cid:29) 1, the space W Q
k+1 deﬁned

19

by (52) will have a much lower dimension than the space W Q
k+l deﬁned by (51). It is worth
mentioning that in [4], the authors propose a similar partial enrichment strategy for the test
space Tp but not in a goal-oriented framework.

The deﬁnition (44) of the test space Wr requires the deﬁnition of a preconditioner Pm(ξ)
which is here constructed by interpolation of the inverse of A(ξ). Following the idea of
[17], the interpolation points for the preconditioner are chosen as the points where solutions
(primal and dual) have already been computed, i.e. the points given by (49). The resulting
algorithms are summarized in Algorithm 1 and Algorithm 2 respectively for the simultaneous
and the alternate constructions of Vr and W Q
k .

Algorithm 1 Simultaneous construction of Vr and W Q
k
Input: Error estimator ∆(·), a samples set Ξ, maximum iteration I
1: Initialize r, k = 0 and the spaces Vr = {0} and W Q
2: for i = 1 to I do

k = {0}

Find ξi ∈ arg maxξ∈Ξ ∆(ξ)
Compute a factorization of A(ξi) and update the preconditioner if needed
Solve u(ξi) = A(ξi)−1b(ξi)
Update Vr+1 = Vr + span(u(ξi)), and r ← r + 1
if Full dual enrichment then
Solve Q(ξi) = A(ξi)−∗L(ξi)∗
Update W Q

k + range(Q(ξi)), and k ← k + l

k+l = W Q

else if Partial dual enrichment then

3:

4:

5:

6:

7:

8:

9:

10:

11:

12:

13:

Find z(cid:48) according to (54) or (53)
Solve y(ξi) = A(ξi)−∗(L(ξi)∗z(cid:48))
Update W Q

k+1 = W Q

k + span(y(ξi)), and k ← k + 1

end if
14:
15: end for

4 Numerical results

In this section, we present numerical applications of the methods proposed in Sections 2
and 3. We ﬁrst describe the applications in Section 4.1. Then we compare the projection
methods for the estimation of a variable of interest in Section 4.2. Finally, we study the
behavior of the proposed greedy algorithms for the construction of the reduced spaces in
Section 4.3.

20

Algorithm 2 Alternate construction of Vr and W Q
k
Input: Error estimator ∆(·), a samples set Ξ, maximum iteration I
1: Initialize r, k = 0 and the spaces Vr = {0} and W Q
2: for i = 1 to I do

k = {0}

Find ξi ∈ arg maxξ∈Ξ ∆(ξ)
Compute a factorization of A(ξi) and update the preconditioner if needed
if i is even then

3:

4:

5:

6:

7:

8:

9:

10:

11:

12:

13:

14:

15:

16:

Solve u(ξi) = A(ξi)−1b(ξi)
Update Vr+1 = Vr + span(u(ξi)), and r ← r + 1

else if i is odd then

if Full dual enrichment then
Solve Q(ξi) = A(ξi)−∗L(ξi)∗
Update W Q

k+l = W Q

k + range(Q(ξi)), and k ← k + l

else if Partial dual enrichment then

Find z(cid:48) according to (54) or (53)
Solve y(ξi) = A(ξi)−∗(L(ξi)∗z(cid:48))
Update W Q

k+1 = W Q

k + span(y(ξi)), and k ← k + 1

end if

end if
17:
18: end for

4.1 Applications

4.1.1 Application 1 : a symmetric problem

We consider a linear elasticity problem2 div(K(ξ) : ε(u(ξ))) = 0 over a domain Ω (represented
on Figure 1(a)), where u(ξ) : Ω → R3 is the displacement ﬁeld and ε(u) = 1
2(∇u + ∇uT ) ∈
R3×3 is the strain tensor associated to the displacement ﬁeld u. The Hooke tensor K(ξ) is
such that

K(ξ) : ε(u(ξ)) =

ε(u(ξ)) +

trace(ε(u(ξ)))I3

,

ν

1 − 2ν

(cid:16)

E(ξ)
1 + ν

(cid:17)

1Ω0 +(cid:80)6

where ν = 0.3 is the Poisson coeﬃcient and E(ξ) is the Young modulus deﬁned by E(ξ) =
i=1 ξi1Ωi, 1Ωi being the indicator function of the subdomain Ωi, see Figure 1(b). The
components of ξ = (ξ1, . . . , ξ6) are independent and log-uniformly distributed over [10−1, 10].
We impose homogeneous Dirichlet boundary condition u(ξ) = 0 on ΓD (red lines), a unit
vertical surface load on ΓL (green square), and a zero surface load on the complementary part
of the boundary (see Figure 1(a)). We consider the Galerkin approximation uh(ξ) of u(ξ)

2The authors thank Mathilde Chevreuil for having proposed this benchmark problem.

21

n, with

and bi = (cid:82)

6(cid:88)

k=1

(cid:90)

Ωk

i=1 ⊂ {v ∈ H 1(Ω)3 : v|ΓD = 0} of
on a P1 ﬁnite element approximation space Vh = span(φi)n
dimension n = 8916 associated to the mesh plotted on Figure 1(b). The vector u(ξ) ∈ V =
i=1 ui(ξ)φi is the solution of the linear system A(ξ)u(ξ) = b of size

Rn such that uh(ξ) =(cid:80)n

A(ξ) = A(0) +

ξiA(k) , A(k)

i,j =

∇φi : K0 : ∇φj dΩ,

(55)

ΓL

−e3 · φi dΓ, where K0 denotes the Hooke tensor with the Young modulus
E = 1. The norm (cid:107) · (cid:107)V on the space V is chosen such that (cid:107)v(cid:107)2
V = (cid:104)A(ξ)v, v(cid:105), that
means RV = A(ξ). We also consider the parameter-independent norm (cid:107) · (cid:107)V0 deﬁned by
V0 = (cid:104)A(ξ0)v, v(cid:105) with ξ0 = (1, . . . , 1). It corresponds to the norm induced by the operator
(cid:107)v(cid:107)2
associated with the Hooke tensor K0 instead of K(ξ).

approximation on the blue line Γ, see Figure 1(a). We can write sh(ξ) =(cid:80)l

Let us consider sh(ξ) = uh|Γ(ξ) · e3 which is the vertical displacement of the Galerkin
j=1 sj(ξ)ψj where
j=1 is a basis of the space {vh|Γ · e3 : vh ∈ Vh} of dimension l = 44. Then there exists

{ψj}l
L ∈ Rl×n such that

where s(ξ) = (s1(ξ), . . . , sl(ξ)) ∈ Z = Rl is the variable of interest. The norm (cid:107)·(cid:107)Z is deﬁned
as the canonical norm of Rl.

s(ξ) = Lu(ξ),

4.1.2 Application 2: a non symmetric problem

We consider the benchmark problem of the cooling of electronic components proposed in the
OPUS project3. The equation to solve is an advection-diﬀusion equation over the domain
Ω ⊂ R2

− ∇ · (κ(ξ)∇T (ξ)) + c(ξ) · ∇T (ξ) = f,

(56)
whose solution T (ξ) : Ω → R is the temperature ﬁeld. Here κ and c denote respectively the
diﬀusion coeﬃcient and the advection ﬁeld, which are parameter-dependent coeﬃcients of
the operator. The full description of this problem is given in [17]. Here, we only focus on
the resulting algebraic parameter-dependent equation coming from stabilized ﬁnite element
discretization of (56), that is A(ξ)u(ξ) = b(ξ), where u(ξ) ∈ Rn are the coeﬃcients of
i=1 uiϕi of T , and where ξ = (ξ1, . . . , ξ4) is a
4-dimensional random vector. The space V = Rn with n = 2.8 × 104 is endowed with

the ﬁnite element approximation T h = (cid:80)n

3See http://www.opus-project.fr

22

(a) Geometry, boundary condition and variable of interest.

(b) Realization of a solution and mesh of the domain Ω. The colors corre-
sponds to the diﬀerent sub-domains Ωi for i = 0, . . . , 6.

Figure 1: Application 1: schematic representation of the problem and a realization of the
solution.
the norm (cid:107) · (cid:107)V = (cid:107) · (cid:107)V0 which corresponds to the H 1(Ω)-norm4. The variable of interest
s(ξ) = (s1(ξ), s2(ξ)) is the mean temperature of both electronic components, with

(cid:90)

(cid:90)

ΩIC2

T h(ξ)dΩ,

(57)

s1(ξ) =

1

|ΩIC1|

T h(ξ)dΩ ,

s2(ξ) =

1

|ΩIC2|

ΩIC1

where ΩICi (i = 1, 2) are two subdomains of Ω ⊂ R2 (see [17, Fig.7]). Then we can write
s(ξ) = Lu(ξ) for an appropriate L ∈ Rl×n, with l = 2. Here we have Z = R2, which we
4 It means that (cid:107)v(cid:107)V0 = (cid:107)vh(cid:107)H 1(Ω) for all v ∈ V , where vh =(cid:80)n
equip with the canonical norm on R2.

i=1 viϕi.

23

4.2 Comparison of the projections methods

The goal of this section is to compare the projection methods proposed in Section 2 for the
estimation of s(ξ). Here the approximation spaces Vr, W Q
k and the test space Wr are given.
We denote by Vr, WQ
k and Wr the matrices containing the basis vectors of the corresponding
subspaces.

4.2.1 Application 1

k

k .

We consider a samples set Ξt ⊂ Ξ of size t = 104. For each ξ ∈ Ξt we compute the exact

We ﬁrst detail how we build Vr, WQ
k and Wr. The matrix Vr contains r = 20 snapshots
of the solution: Vr = (u(ξ1), . . . , u(ξ20)). The test space is Wr = Vr, which corresponds to
a standard Galerkin projection method. The matrix WQ
k contains 2 snapshots of the dual
variable Q(ξ) = A(ξ)−1L∗ ∈ Rn×l. Then k = 2l = 88. Finally, according to (42) the matrix

Tp =(cid:0)Wr, WQ
quantity of interest s(ξ) and the approximation(cid:101)s(ξ) by the following methods.
• Primal only: solve the linear system(cid:0)VT
(cid:1)Ur(ξ).
(cid:101)s(ξ) =(cid:0)LVr
• Dual only: solve the linear system(cid:0)(WQ
compute(cid:101)s(ξ) =(cid:0)LWQ
(cid:1)Yk(ξ).5
(cid:0)(WQ
(cid:1)Yk(ξ) =(cid:0)(WQ
k )T b(cid:1) −(cid:0)(WQ
(cid:101)s(ξ) =(cid:0)LVr
(cid:1)Ur(ξ) +(cid:0)LWQ
(cid:1)Yk(ξ).
(cid:0)TT

(cid:1) is the concatenation of the matrices Wr and WQ
r b(cid:1) of size r and compute
(cid:1)Ur(ξ) =(cid:0)VT
k )T b(cid:1) of size k and
(cid:1)Yk(ξ) =(cid:0)(WQ
(cid:1)Ur(ξ) of size k and compute
(cid:1)Yp(ξ) =(cid:0)TT
p b(cid:1)
(cid:1)Yp(ξ).

of size p = k + r, and compute(cid:101)s(ξ) =(cid:0)LTp

p A(ξ)Tp

• Primal-dual : solve the linear system of the Primal only method, solve the linear system

• Saddle point: According to Remark 2.13, solve the linear system

r A(ξ)Vr

k )T A(ξ)WQ

k

k )T A(ξ)WQ

k

k )T A(ξ)Vr

k

k

The aﬃne decomposition (55) of matrix A(ξ) allows for a rapid solution of the reduced

systems for any parameter ξ.

Figure 2 gives the probability density function (PDF), the L∞ norm and L2 norm of the

error (cid:107)s(ξ)−(cid:101)s(ξ)(cid:107)Z estimated over the samples set Ξt. We see that the primal-dual method

5The dual only method corresponds to the primal-dual method where we consider a zero primal approx-

imation, i.e. Vr = Wr = {0}.

24

provides errors for the quantity of interest which correspond to the product of the errors
of the primal only and dual only methods. This reﬂects the “squared eﬀect”. Moreover
the saddle point method provides errors that are almost 10 times lower than the primal-
dual method. This impressive improvement can be explained by the fact that the proposed
problem is “almost compliant”, in the sense that the primal and dual solutions are similar:
the primal solution is associated to a vertical force on the green square of Figure 1(a), and
the dual solution is associated to a vertical loading on Γ. To illustrate this, let us consider
a “less compliant” application where the variable of interest is deﬁned as the horizontal
displacement (in the direction e2, see ﬁgure 1(a)) of the solution on the blue line Γ, i.e.
sh(ξ) = u|Γ(ξ) · e2 (instead of sh(ξ) = u|Γ(ξ) · e3). The results are given on Figure 3. For
this new setting, we can draw similar conclusions but the saddle point method provides a
solution which is “only” 2 times better (instead of 10 times) than the primal-dual method.

(a) PDF of the error.

(b) L∞ and L2 norm of the error.

Figure 2: Application 1: Probability density function, L∞ norm and L2 norm of the error

(cid:107)s(ξ) −(cid:101)s(ξ)(cid:107)Z estimated on a samples set of size 104.
Now we consider the eﬀectivity index η(ξ) = ∆(ξ)/(cid:107)s(ξ) −(cid:101)s(ξ)(cid:107)Z associated to the

primal-dual error estimate deﬁned by (46) and to the saddle-point error estimate deﬁned by
(47). For the considered application, the coercivity constant α(ξ) can be obtained by the
min-theta method [7, Proposition 2.35]. Figure 4 presents statistical information on η(ξ):
the PDF, the mean, the max-min ratio and the normalized standard deviation estimated on
a samples set of size 104. We ﬁrst observe on Figure 4(a) that the eﬀectivity index is always
greater than 1: this illustrates the fact that the error estimates are certiﬁed. Moreover, the
error estimate of the saddle point method is much better than the one of the primal-dual
method. The max-min ratio and the standard deviation of the corresponding eﬀectivity index
are much smaller and the mean value is much closer to one for the saddle point method.

25

10−410−310−210−110010110210−410−310−210−1100101102PrimalonlyDualonlyPrimal-DualSaddlepointL∞-normL2-normPrimalonly8.16×1009.72×10−1Dualonly6.73×1011.22×101Primal-dual2.60×1002.47×10−1Saddlepoint3.56×10−14.69×10−2(a) PDF of the error.

(b) L∞ and L2 norm of the error.

Figure 3: Application 1 with a diﬀerent variable of interest (“less compliant” case): Prob-

ability density function, L∞ norm and L2 norm of the error (cid:107)s(ξ) −(cid:101)s(ξ)(cid:107)Z estimated on a

samples set of size 104.

(a) PDF of η(ξ) for the primal-dual method and
the saddle point method.

(b) Statistics of the eﬀectivity index η(ξ)
for the primal-dual method and saddle
point method.

Figure 4: Application 1: Probability density function, mean, min-max ratio and normalized

standard deviation of the eﬀectivity index η(ξ) = ∆(ξ)/(cid:107)s(ξ)−(cid:101)s(ξ)(cid:107)Z estimated on a samples

set of size 104. Here, ∆(ξ) is deﬁned by (46) for the primal-dual method and by (47) for the
saddle point method.

4.2.2 Application 2

For this second application, Vr = (u(ξ1), . . . , u(ξ50)) contains 50 snapshots of the primal
solution (r = 50), and WQ
k = (Q(ξ1) . . . Q(ξ25)) contains 25 snapshots of the dual solution
so that the dimension of W Q
is k = 25l = 50. The test space Wr is deﬁned according to
k
(44), where Pm(ξ) is an interpolation of A(ξ)−1 using m interpolation points selected by a

26

10−410−310−210−110010−210−1100101102103PrimalonlyDualonlyPrimal-DualSaddlepointL∞-normL2-normPrimalonly3.90×10−19.03×10−2Dualonly7.09×10−12.87×10−1Primal-dual9.61×10−21.34×10−2Saddlepoint4.28×10−27.07×10−310010110210310−510−410−310−210−1100Primal-dualSaddlepointPrimal-dualSaddlepointE(η(ξ))26.94.42maxη(ξ)minη(ξ)366.751.8Var(η(ξ))1/2E(η(ξ))1.340.808m(ξ)RV Vr.

greedy procedure based on the residual (cid:107)I − Pm(ξ)A(ξ)(cid:107)F (where (cid:107) · (cid:107)F denotes the matrix
Frobenius norm), see [17]. The interpolation is deﬁned by a Frobenius semi-norm projection
(with positivity constraint) using a P-SRHT matrix with 400 columns. The matrix associated
to the test space is given by Wr(ξ) = P T

Once again, we consider a samples set Ξt of size t = 104. For any ξ ∈ Ξt we compute the

exact quantity of interest s(ξ) and the approximation(cid:101)s(ξ) by the following methods.
• Primal only: solve the linear system (cid:0)WT
(cid:1)Ur(ξ).
compute(cid:101)s(ξ) =(cid:0)LVr
(cid:0)(WQ

(cid:1)Ur(ξ) = Wr(ξ)T b of size r and

• Dual only: solve the linear system

r (ξ)A(ξ)Vr

k )T b

of size k and compute(cid:101)s(ξ) =(cid:0)LR−1

k )T A(ξ)R−1

• Primal-dual : solve the linear system of the Primal only method, solve the linear system

k

k

V A(ξ)∗WQ
V A(ξ)∗WQ

(cid:1)Yk(ξ) = (WQ
(cid:1)Yk(ξ).
(cid:1)Yk(ξ) =(cid:0)(WQ
k )T b(cid:1) −(cid:0)(WQ
(cid:1)Ur(ξ) +(cid:0)LR−1
(cid:33)(cid:32)

V A(ξ)∗WQ

k

p (ξ)A(ξ)Vr

(cid:1)Ur(ξ)

(cid:33)

k )T A(ξ)Vr

(cid:1)Yk(ξ).
(cid:32)
(cid:33)

=

Yr,p(ξ)
Ur,p(ξ)

Tp(ξ)T b

0

(cid:0)(WQ

k

(cid:32)

• Saddle point: solve the linear system of size p + r

V A(ξ)∗WQ

k )T A(ξ)R−1

of size k, and compute(cid:101)s(ξ) =(cid:0)LVr
(cid:1)T
(cid:1), and compute
with Tp(ξ) =(cid:0)Wr(ξ), WQ
(cid:101)s(ξ) =(cid:0)LVr
(cid:1)Ur,p(ξ) +(cid:0)LR−1

V A(ξ)∗Tp(ξ) TT

p (ξ)A(ξ)R−1
TT

(cid:0)TT

p (ξ)A(ξ)Vr

0

k

V A(ξ)∗Tp(ξ)(cid:1)Yr,p(ξ).

The numerical results are given in Figure 5. Once again, the saddle point method leads to the
lowest error on the variable of interest. Also, we see that a good preconditioner (for example
with m = 30) improves the accuracy for the saddle point method, the primal only method
and the primal-dual method. However, this improvement is not really signiﬁcant for the
considered application: the errors are barely divided by 2 compared to the non preconditioned
Galerkin projection (m = 0). In fact, the preconditioner improves the quality of the test
space, and the choice Wr = Vr (yielding the standard Galerkin projection) is suﬃciently
accurate for this example and for the chosen norm on V .

27

(a) PDF of the error. Three diﬀerent precondi-
tioners Pm(ξ) are used: m = 0 (dotted lines),
m = 10 (dashed lines) and m = 30 (continuous
lines).

(b) L∞ and L2 norm of the error.

Figure 5: Application 2: Probability density function, L∞ norm and L2 norm of the error

(cid:107)s(ξ) −(cid:101)s(ξ)(cid:107)Z estimated from a samples set of size 104.

We discuss now the quality of the error estimate ∆(ξ) for the variable of interest. Since
in this application the constant α(ξ) can not be easily computed, we consider surrogates for
(46) and (48) using a preconditoner Pm(ξ). We consider

∆(ξ) = (cid:107)Pm(ξ)(A(ξ)ur(ξ) − b(ξ))(cid:107)V0(cid:107)L(ξ)∗ − A(ξ)∗Qk(ξ)(cid:107)Z(cid:48)→V (cid:48)

0

for the primal-dual method, and

∆(ξ) = (cid:107)Pm(ξ)(A(ξ)tr,p(ξ) − b(ξ))(cid:107)V0

sup
0(cid:54)=z(cid:48)∈Z(cid:48)

min
y∈Tp

(cid:107)L(ξ)∗z(cid:48) − A(ξ)∗y(cid:107)V (cid:48)

0

(cid:107)z(cid:48)(cid:107)Z(cid:48)

(58)

(59)

for the saddle point method. Figure 6 shows statistics of the eﬀectivity index η(ξ) =

∆(ξ)/(cid:107)s(ξ) −(cid:101)s(ξ)(cid:107)Z for diﬀerent numbers m of interpolation points for the preconditioner.

We see that the max-min ratio and the normalized standard deviation are decreasing with
m: this indicates an improvement of the error estimate. Furthermore, the mean value of
η(ξ) seems to converge (with m) to 19.5 for the primal-dual method, and to 13.8 for the
saddle point method. In fact, with a good preconditioner, (cid:107)Pm(ξ)(A(ξ)ur(ξ) − b(ξ))(cid:107)V0 (or
(cid:107)Pm(ξ)(A(ξ)tr,p(ξ) − b(ξ))(cid:107)V0) is expected to be a good approximation of the primal error
(cid:107)u(ξ) − ur(ξ)(cid:107)V0 (or (cid:107)u(ξ) − tr,p(ξ)(cid:107)V0), but this does not ensure that the eﬀectivity index
η(ξ) will converge to 1.

28

10−610−510−410−310−210−110010110−310−210−1100101102103104PrimalonlyDualonlyPrimal-DualSaddlepointPrimalonlyL∞-normL2-normm=01.284×1001.245×10−1m=51.203×1009.637×10−2m=101.458×1001.064×10−1m=201.068×1008.386×10−2m=301.066×1007.955×10−2Primal-dualL∞-normL2-normm=02.751×10−11.085×10−2m=51.308×10−15.708×10−3m=101.333×10−15.807×10−3m=201.232×10−15.465×10−3m=301.224×10−15.408×10−3SaddlepointL∞-normL2-normm=01.023×10−14.347×10−3m=59.715×10−23.389×10−3m=109.573×10−23.867×10−3m=206.022×10−22.996×10−3m=305.705×10−22.896×10−3(a) PDF of η(ξ) for the primal-dual methods and
the saddle point methods. Three diﬀerent precon-
ditioners Pm(ξ) are used: m = 0 (dotted lines),
m = 10 (dashed lines) and m = 30 (continuous
lines)

(b) Statistics of the eﬀectivity index η(ξ)
for the primal-dual method and the saddle
point method.

Figure 6: Application 2: PDF, mean, max-min ratio and normalized standard deviation

of the eﬀectivity index η(ξ) = ∆(ξ)/(cid:107)s(ξ) −(cid:101)s(ξ)(cid:107)Z. Here, ∆(ξ) is deﬁned by (58) for the

primal-dual method and by (59) for the saddle point method.

4.2.3 Partial conclusions and remarks

In both numerical examples, the saddle point method provides the most accurate estimation
for the variable of interest. Let us note that the saddle point problem requires the solution of
a dense linear system of size (r + k) for the symmetric and coercive case, and of size (2r + k)
for the general case. When using Gauss elimination method for the solution of those systems,
the complexity is either in C(r + k)3 or C(2r + k)3 (with C = 2/3), which is larger than the
complexity of the primal-dual method C(r3 + k3). However, in the case where the primal
and dual approximation spaces have the same dimension r = k, the saddle point method is
only 4 times (in the symmetric and coercive case) or 13.5 times (in the general case) more
expensive.

Furthermore, we showed that the preconditioner slightly improves the quality of the

estimation(cid:101)s(ξ), and of the error estimate ∆(ξ). Since the construction of the preconditioner

yields a signiﬁcant increase in computational and memory costs (see [17]), preconditioning
is clearly not needed for the present applications.

4.3 Greedy construction of the reduced spaces

We now consider the greedy construction of the reduced spaces by Algorithms 1 or 2. For the
two considered applications, we show the convergence of the error estimate with respect to the

29

10−110010110210310−610−510−410−310−210−1Primal-dualSaddlepointE(η(ξ))maxη(ξ)minη(ξ)Var(η(ξ))1/2E(η(ξ))Primal-dualm=05.5453.52×1032.246m=516.038.68×1021.920m=1018.691.01×1031.925m=2019.205.77×1021.504m=3019.593.95×1021.615Saddlepointm=04.7266.93×1033.597m=512.611.80×1021.429m=1013.271.72×1021.160m=2013.971.89×1021.090m=3013.842.17×1021.113complexity of the oﬄine and of the online phase. For the sake of simplicity, we measure the
complexity of the oﬄine phase with the number of operator factorizations (this corresponds
to the number of iterations I of Algorithms 1 and 2). Of course exact estimation of the
oﬄine complexity should take into account many other steps (for example, the computation
of ∆(ξ), of the preconditioner, etc), but the operator factorization is considered, for large
scale applications, as the main source of computation cost. For the online complexity, we
only consider the computation cost for the solution of one reduced system, see Section 4.2.3.
Here we do not take into account the complexity for assembling the reduced systems although
it may be a signiﬁcant part of the complexity for “not so reduced” systems of equations.

4.3.1 Application 1

Figure 7 shows the convergence of maxξ ∆(ξ) with respect to the oﬄine and online complex-
ities (as deﬁned above). On Figure 7(a), we see that the saddle point method (dashed lines)
always provides lower values for the error estimate compared to the primal-dual method
(continuous lines). However, as already mentioned, the saddle point method requires the so-
lution of larger reduced systems during the online phase. Therefore, the primal-dual method
can sometimes provide lower error estimates (see the blue and red curves of Figure 7(b)) for
the same online complexity.

The simultaneous construction of Vr and W Q

k with full dual enrichment (51) (green curves)
yields a very fast convergence of the error estimate during the oﬄine phase, see Figure 7(a)).
But the rapid increase of dim(W Q
k ) leads to high online complexity, so that this strategy
becomes non competitive during the online phase, see Figure 7(b).

We compare now the alternate and the simultaneous construction of Vr and W Q

k with
partial dual enrichment (52) (red and blue curves on Figure 7). The initial idea of the alter-
nate construction is to build reduced spaces of better quality. Indeed, since the evaluation
points of the primal solution are diﬀerent from the one of the dual solution, the reduced
spaces are expected to contain “complementary information” for the approximation of the
variable of interest. In practice, we observe on Figure 7(a) that the alternate construction is
(two times) more expensive during the oﬄine phase, but the resulting error estimate behaves
very similarly to the simultaneous strategy, see Figure 7(b). We conclude that the alternate
strategy is not relevant for this application.

Furthermore, let us note that after iteration 50 of the greedy algorithm, the rate of con-
vergence of the dashed red curve of Figure 7(a) (i.e. the simultaneous construction with
partial dual enrichment using the saddle point method) rapidly increases. A possible expla-
nation is that the dimension of the dual approximation space is large enough to reproduce
correctly the dual variable, which requires a dimension higher than l = 44. The same obser-
vation can be done for the alternative strategy (the dashed blue curve) after iteration 100

30

(which corresponds to dim(W Q
present this behavior.

k ) ≥ 50). Also, we note that the primal-dual method does not

(a) Maximum value of the error estimate ∆(ξ)
with respect to the number of operator factoriza-
tions.

(b) Maximum value of the error estimate ∆(ξ)
with respect to the complexity of solving one re-
duced system.

Figure 7: Application 1: error estimate maxξ ∆(ξ) with respect to the oﬄine complexity
(Figure 7(a)) and the online complexity (Figure 7(b)). The continuous lines correspond to
the primal-dual method, and the dashed lines correspond to the saddle point method. The
primal only curves serve as reference.

4.3.2 Application 2

For the application 2, we ﬁrst test Algorithms 1 and 2 with the use of a preconditioner
(deﬁned in Section 4.2.2). The interpolation points for the preconditioner are the ones
where the solutions (primal and dual) have been computed, see Algorithms 1 and 2. The
preconditioner is used for the deﬁnition of the test space Wr(ξ), see equation (44), and
for the error estimate ∆(ξ), see equation (58) for the primal-dual method and (59) for the
saddle point method. The numerical results are given on Figure 8. We can draw the same
conclusions as for application 1.

• In the oﬄine phase, the saddle point method provides lower errors (Figure 8(a)). How-
ever, the corresponding reduced systems are larger, and we see that the primal-dual
method provides lower errors for the same online complexity, see Figure 8(b). For
this test case, the beneﬁts (in term of accuracy) of the saddle point method does not
compensate the additional online computational costs.

• The full dual enrichment yields a fast convergence during the oﬄine phase, but the
is disadvantageous regarding the online complexity. However,

rapid increase of W Q
k

31

2040608010012014016010−410−310−210−1100101102103maxξ∆(ξ)PrimalOnlyAlternately(partialdual)Simultaneously(partialdual)Simultaneously(fulldual)10010110210310410510610710−410−310−210−1100101102103maxξ∆(ξ)PrimalOnlyAlternately(partialdual)Simultaneously(partialdual)Simultaneously(fulldual)since the dimension of the variable of interest is “only” l = 2, the full dual enrichment
is still an acceptable strategy (compared to the previous application).

• Here, the alternate strategy (blue curves) seems to yield slightly better reduced spaces
compared to the simultaneous strategy, see Figure 8(b). But this leads to higher oﬄine
costs, see Figure 8(a).

We also run numerical tests without using the preconditioner. In that case, we replace
Pm(ξ) by R−1
V . Figure 9 shows numerical results which are very similar to those of Figure
8. To illustrate the beneﬁts of using the preconditioner, let us consider the eﬀectivity index

η(ξ) = ∆(ξ)/(cid:107)s(ξ) −(cid:101)s(ξ)(cid:107)Z associated to the error estimate for the variable of interest.

Figure 10 shows the conﬁdence interval I(p) of probability p for η(ξ) deﬁned as the smallest
interval which satisﬁes

P(ξ ∈ Ξt : η(ξ) ∈ I(p)) ≥ p,

where P(A) = #A/#Ξt for A ⊂ Ξt. When using the preconditioner, we see on Figure
10 that the eﬀectivity index is improved during the greedy iterations in the sense that the
conﬁdence intervals are getting smaller and smaller. Also, we note that after the iteration
15, the eﬀectivity index is always above 1: this indicates that the error estimate tends to
be certiﬁed. Furthermore, after iteration 20 we do not observe any further improvement, so
that is seems not useful to continue enriching the preconditioner.

Let us ﬁnally note that the use of the preconditioner yields signiﬁcant computational
costs. Indeed, we have to store operator factorizations (in our current implementation of the
method), and the computation of the interpolation of the inverse operator requires additional
problems to solve (see [17]). For the present application, even if the eﬀectivity index of the
error estimate is improved, the beneﬁts of using the preconditioner remains questionable.

5 Conclusion

We have proposed and analyzed projection based methods for the estimation of vector-
valued variables of interest in the context of parameter-dependent equations. This includes
a generalization of the classical primal-dual method to the case of vector-valued variables of
interest, and also a Petrov-Galerkin method based on a saddle point problem. Numerical
results showed that the saddle point method always improves the quality of the approxi-
mation compared to the primal-dual method using the same reduced spaces. We have also
derived computable error estimates and greedy algorithms for the goal-oriented construction
of the reduced spaces. The performances of these approaches have been compared on numer-
ical examples, with an analysis of both the oﬄine complexity (construction of the reduced

32

(a) Maximum value of the error estimate ∆(ξ)
with respect to the number of operator factoriza-
tions.

(b) Maximum value of the error estimate ∆(ξ)
with respect to the complexity for solving one re-
duced system.

Figure 8: Application 2 with preconditioner: error estimate maxξ ∆(ξ) with respect to the
oﬄine complexity (Figure 8(a)) and the online complexity (Figure 8(b)). The continuous
lines correspond to the primal-dual method, and the dashed lines correspond to the saddle
point method. The primal only curves serve as references.

(a) Maximum value of the error estimate ∆(ξ)
with respect to the number of operator factoriza-
tion.

(b) Maximum value of the error estimate ∆(ξ)
with respect to the complexity for solving one re-
duced system.

Figure 9: Application 2 without preconditioner: error estimate maxξ ∆(ξ) with respect to
the oﬄine complexity 9(a) and the online complexity 9(b). The continuous lines correspond
to the primal-dual method, and the dashed lines correspond to the saddle point method.
The primal only curves serve as references.

spaces) and the online complexity (evaluation of the reduced order model and estimation of
the variable of interest for one instance of the parameter). This complexity analysis revealed

33

102030405060708010−310−210−1100101102maxξ∆(ξ)PrimalOnlyAlternately(partialdual)Simultaneously(partialdual)Simultaneously(fulldual)10010110210310410510610710−310−210−1100101102maxξ∆(ξ)PrimalOnlyAlternately(partialdual)Simultaneously(partialdual)Simultaneously(fulldual)102030405060708010−310−210−1100101102maxξ∆(ξ)PrimalOnlyAlternately(partialdual)Simultaneously(partialdual)Simultaneously(fulldual)10010110210310410510610710−310−210−1100101102maxξ∆(ξ)PrimalOnlyAlternately(partialdual)Simultaneously(partialdual)Simultaneously(fulldual)(a) Without preconditioner.

(b) With preconditioner.

Figure 10: Application 2: evolution with respect to the greedy iterations of the conﬁdence

interval I(p) for the eﬀectivity index η(ξ) = ∆(ξ)/(cid:107)s(ξ) −(cid:101)s(ξ)(cid:107)Z for saddle point method.

that the saddle point method is preferable to the primal-dual method regarding the oﬄine
costs. However, in the situation where the reduction of the online costs matter more than
the reduction of oﬄine costs, then the primal-dual method seems to be a better option (at
least for the considered applications).
In the considered applications, whereas the use of
preconditioners allows the construction of better reduced test spaces and better error es-
timates, it introduces additional computational costs and yields a loss of eﬃciency of the
proposed methods. The proposed error estimates, which involve the use of Cauchy-Schwarz
inequalities, are clearly not optimal. Better error estimates could be obtained by extending
to the case of vector-valued variables of interest the probabilistic error bounds proposed in
[8].

References

[1] G. Berkooz, P. Holmes and J. Lumley, The proper orthogonal decomposition in

the analysis of turbulent ﬂows. Ann. Rev. Fluid Mech., 25:539–575, 1993.

[2] Y. Chen, J. S. Hesthaven, Y. Maday, J. Rodrguez, Certiﬁed Reduced Basis Meth-
ods and Output Bounds for the Harmonic Maxwell’s Equations. SIAM J. Sci. Comput.,
32(2):970–996, 2010.

[3] N. Cuong, K. Veroy, A.T. Patera, Certiﬁed real-time solution of parametrized

partial diﬀerential equations. Handbook of Materials Modeling, 1523–1558, 2005.

34

51015202530354010−210−1100101102103104I(50%)I(75%)I(90%)I(99%)I(100%)51015202530354010−210−1100101102103104I(50%)I(75%)I(90%)I(99%)I(100%)[4] W. Dahmen, C. Plesken, and G. Welper, Double greedy algorithms: Reduced
basis methods for transport dominated problems. ESAIM Math. Model. Numer. Anal.,
48(03):623–663, May 2013.

[5] A. Ern, and J.-L. Guermond Theory and practice of ﬁnite elements. Vol. 159 of

Appl. Math. Sci., 2004.

[6] M. A. Grepl and A. T. Patera, A posteriori error bounds for reduced-basis approx-
imations of parametrized parabolic partial diﬀerential equations. ESAIM Math. Model.
Numer. Anal., 39(1):157–181, 2005.

[7] B. Haasdonk, Reduced Basis Methods for Parametrized PDEs – A Tutorial Introduction
for Stationary and Instationary Problems. Chapter to appear in P. Benner, A. Cohen, M.
Ohlberger and K. Willcox: ”Model Reduction and Approximation for Complex Systems”,
Springer, 2014.

[8] A. Janon, M. Nodet, C. Prieur, Goal-oriented error estimation for the reduced basis
method, with application to sensitivity analysis. Journal of Scientiﬁc Computing, 1–15,
2015.

[9] D. B P Huynh, G. Rozza, S. Sen, and A. T. Patera, A successive constraint
linear optimization method for lower bounds of parametric coercivity and inf-sup stability
constants. Comptes Rendus Math., 345(8):473–478, October 2007.

[10] K. Kahlbacher and S. Volkwein, Galerkin proper orthogonal decomposition meth-
ods for parameter dependent elliptic systems. Discuss. Math. Diﬀerential Incl., 27:95–117,
2007.

[11] N. A. Pierce and M. B. Giles, Adjoint Recovery of Superconvergent Functionals

from PDE Approximations. SIAM Rev., 42(2):247–264, 2000.

[12] C. Prud’homme, D. V. Rovas, K. Veroy, L. Machiels, Y. Maday, a. T. Pat-
era, and G. Turinici, Reliable Real-Time Solution of Parametrized Partial Diﬀerential
Equations: Reduced-Basis Output Bound Methods. J. Fluids Eng., 124(1):70, 2002.

[13] A. Quarteroni, G. Rozza, and A. Manzoni, Certiﬁed reduced basis approximation
for parametrized partial diﬀerential equations and applications. J. Math. Ind., 1(1):3,
2011.

[14] G. Rozza, D. B. P. Huynh, and A. T. Patera, Reduced basis approximation and
a posteriori error estimation for aﬃnely parametrized elliptic coercive partial diﬀerential

35

equations: Application to transport and continuum mechanics. Arch. Comput. Methods
Eng., 15(3):229–275, May 2008.

[15] G. Rozza and K. Veroy, On the stability of the reduced basis method for Stokes
equations in parametrized domains. Comput. Methods Appl. Mech. Eng., 196(7):1244–
1260, 2007.

[16] J. Xu, and L. Zikatanov Some observations on Babuska and Brezzi theories. Nu-

merische Mathematik, 94(1):195–202, 2003

[17] O. Zahm and A. Nouy, Interpolation of

inverse operators for preconditioning

parameter-dependent equations. SIAM J. Sci. Comput., 2016.

36

