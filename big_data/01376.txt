Lasso Estimation for GEFCom2014 Probabilistic Electric Load Forecasting

Florian Ziel∗

Europa-Universit¨at Viadrina, Frankfurt (Oder), Germany

Bidong Liu

University of North Carolina at Charlotte, Charlotte, North Carolina, USA

6
1
0
2

 
r
a

M
4

 

 
 
]
P
A

.
t
a
t
s
[
 
 

1
v
6
7
3
1
0

.

3
0
6
1
:
v
i
X
r
a

Abstract

We present a lasso (least absolute shrinkage and selection operator) estimation based methodology for proba-
bilistic load forecasting. The considered model can be regarded as a bivariate time-varying threshold autoregres-
sive(AR) process for the hourly electric load and temperature. The joint modeling approach directly incorporates
the temperature eﬀects and reﬂects daily, weekly, and annual seasonal patterns and public holiday eﬀects. We pro-
vide two empirical studies, one based on the probabilistic load forecasting track of the Global Energy Forecasting
Competition 2014 (GEFCom2014-L), and the other based on another recent probabilistic load forecasting compe-
tition that follows the similar setup as GEFCom2014-L. In both empirical case studies, the proposed methodology
outperforms two multiple linear regression based benchmarks from a top 8 entry of GEFCom2014-L.

Keywords: Probabilistic forecasting, Threshold AR, Time-varying eﬀects

1. Introduction

We present a lasso (least absolute shrinkage and selection operator) estimation based methodology for proba-
bilistic load forecasting. The lasso estimator introduced by Tibshirani (1996) has the properties of automatically
shrinking parameters and selecting variables. It thus enables us to estimate high-dimensional parameterizations.
The procedure learns from the data in the sense that the parameters of less important variables will automatically
get minor or even zero value. The considered time series model is a bivariate time-varying threshold autoregressive
(AR) model for hourly load and temperature. The model is speciﬁed so that it captures several stylized facts in
load forecasting, such as the underlying daily, weekly, and annual seasonal patterns, the non-linear relationship
between load and temperature, and holiday and long term eﬀects.

In this paper, we illustrate the proposed methodology using two case studies from two recent forecasting com-
petitions. The ﬁrst one was from the probabilistic load forecasting track of Global Energy Forecasting Competition
2014, denoted as GEFCom2014-L. The topic of GEFCom2014-L is month-ahead hourly probabilistic load fore-
casting with hourly temperature from 25 weather stations. More details about GEFCom2014-L such as rules and
data can be found in Hong et al. (2015a). When implementing the proposed methodology, we create a new virtual
temperature time series by averaging the temperature of stations 3 and 9. These stations are chosen, as they give
the best in-sample ﬁt with a cubic regression of the load against the temperature.

The second one was from the year-ahead probabilistic load forecasting competition organized by Tao Hong
from UNC Charlotte in fall 2015, which was an extended version of GEFCom2014-L. In this paper we refer this
competition as GEFCom2014-E. The competition included 5 tasks.
In each task, the participants were asked
to forecast the next year of hourly load and submit the forecasts in 99 quantiles. 6 years (2004-2009) of hourly
temperature and 4 years (2006-2009) of hourly load data was provided as the historical data for the ﬁrst task. In
each of the remaining 4 tasks, an additional year of hourly load and temperature for the forecasted period of the
previous task was provided. The data for GEFCom2014-E can also be found in Hong et al. (2015a). Florian Ziel
joined this competition with the proposed methodology, ranking top 2 out of 16 participating teams.

The structure of this paper is as follows:

in section 2, we introduce the time series model; in section 3, we
discuss the lasso estimation algorithm; in section 4, we describe two benchmarks developed from the methodology
used by Bidong Liu to win a top 8 place in GEFCom2014-L; and in section 5, we present the empirical results.
The paper is concluded in section 6.

.

∗Corresponding author
Email addresses: ziel@europa-uni.de (Florian Ziel ), bliu8@uncc.edu (Bidong Liu )

Preprint submitted to International Journal of Forecasting

March 7, 2016

2 Time Series Model

2. Time Series Model

2

Let (Y t)t∈Z with Y t = (YL,t, YT ,t)(cid:48) be the d = 2-dimensional time series model of interest and denote D =

{L,T }. So that YL,t is the electric load and YT ,t the temperature at time point t.

The considered joint multivariate time-varying threshold AR model (VAR) for (Y t)t∈Z is given by

(cid:88)

(cid:88)

(cid:88)

j∈D

c∈Ci,j

k∈Ii,j,c

Yi,t = φi,0(t) +

φi,j,c,k(t) max{Yj,t−k, c} + εi,t

(1)

for i ∈ D where φi,0 are the time-varying intercepts and φi,j,k,c are time-varying autoregressive coeﬃcients.
Moreover, Ci,j are the sets of all considered thresholds, Ii,j,c are the index sets of the corresponding lags and εi,t
is the error term. We assume that the error process is uncorrelated with zero mean and constant variance.

Furthermore, it is important that we are using the whole dataset with all hours to model hourly load and
temperature, instead of using dataset sliced by hour to model load with a speciﬁc hour as often done in literature.
Forecasting algorithms applied on the whole dataset can learn better about those events since the full dataset is
more informative than each small hourly dataset.

The modeling process has three crucial components: The choice of the thresholds sets Ci,j, the choice of the
lag sets Ii,j,k and the time-varying structure of the coeﬃcient. We describe these issues in the following three
subsections.

2.1. Choice of the threshold sets
that if we choose Ci,j = {−∞} model (1) will turn into a standard multivariate time-varying AR process.

The choice of the thresholds sets Ci,j will characterize the potential non-linear impacts in the model. Note

For load data there is typically a non-linear eﬀect of the temperature to the electric load. Figure 1 shows
the temperature of every day at 00:00 in the sample against the corresponding load. We observe in general a

(a) GEFCom2014-L data

(b) GEFCom2014-E data

Figure 1: Temperature against load for all days at 00:00 with ﬁtted values of model (2) for both data sets.

decreasing relationship in the lower temperature area and an increasing one for larger degrees. To emphasize the
non-linear relationship, we added the ﬁtted line of the toy example regression

YL,t = c0 + c1YT ,t + c2 max{YT ,t, 50} + c3 max{YT ,t, 60} + t.

(2)

This is a simple threshold model with thresholds at 50◦F and 60◦F.

In Figure 1 we see that the threshold model (2) captures the relationship by piecewise linear functions. Even
though this is just an illustrative example, we see that this type of model is able to approximate all non-linear
relationships between load and temperature.

We can also introduce many other thresholds in the model to obtain more ﬂexibility. However, it will enlarge
the parameter space, which brings with longer computation time and raises the concern of over-ﬁtting. The lasso
estimation algorithm can help to ease these two concerns. Even better, it will only keep signiﬁcant non-linear
impacts.
For both data sets we choose the threshold sets manually. For the GEFCom2014-L data, we consider CL,T =
{−∞, 20, 30, 40, 45, 50, 55, 60, 65, 70, 80} for thresholds of the temperature to electric load impact and CL,L =
{−∞, 100, 125, 150, 175, 200, 225} for the load to load eﬀects. Remember that the thresholds corresponding with
−∞ model the linear eﬀects. For the other sets we assume no non-linear eﬀects, so CT ,L = CT ,T = {−∞}. For the
GEFCom2014-E data we are using diﬀerent thresholds, as the data is on a diﬀerent scale. In detail we use CL,T =
{−∞, 10, 20, 30, 40, 45, 50, 60, 70, 80}, CL,L = {−∞, 2500, 3000, 3500, 4000, 4500} and CT ,L = CT ,T = {−∞} for
the thresholds sets. Note that in general a data driven threshold set selection is plausible as well, e.g. by a set of
selected quantiles.

2040608050100150200250300Temperature in °FLoad in MWhJanFebMarAprMayJunJulAugSepOctNovDec02040608020002500300035004000Temperature in °FLoad in MWJanFebMarAprMayJunJulAugSepOctNovDec2 Time Series Model

3

2.2. Choice of the relevant lag sets

The lag sets Ii,j,c are essential for a good model as they characterize the causal structure of the processes and
the potential memory of the process. The lags in Ii,j,c describe a potential lagged impact of regressor j at threshold
c to the process i. It is widely known that the load at time t is related to both its past and the temperature.
Therefore we choose IL,L,c and IL,T ,c non-empty for all c. For the temperature the situation is slightly diﬀerent.
Here, we assume that the temperature depends on its past, so IT ,T ,−∞ is non-empty as well. But, it is clear that
the electric load does not eﬀect the temperature, so IT ,L,−∞ is empty.

The selected index sets are given in Table 1. Here, similarly as for the threshold sets, larger sets increase the

Table 1: Considered lags of the required index sets.

Index sets
IL,L,−∞
IL,L,c (with c (cid:54)= −∞), IL,T ,c
IT ,T ,−∞
IT ,L,−∞

Contained Lags
1, . . . , 1200
1, . . . , 200
1, . . . , 360
-

parameter space and consequently result in computational burden. Still they have to be chosen large enough to
capture the relevant information. IL,L,−∞ contains all lags up to 1200, so the maximal memory is the preceding
1200 hours, slightly more than 7 weeks. The most essential part is that the important lags of orders such as 1, 24,
48 and 168 are included. A detailed discussion for the choice of the index sets can be found in Ziel et al. (2015).

2.3. The time-varying coeﬃcients

The assumed structure of the time-varying coeﬃcients is substantial as well. They have big impacts not only
on seasonality and public holiday eﬀects, but also on the long term trend behavior. Still, we keep most of the
coeﬃcients constant, allowing only the important ones to vary over time. The intercepts φi,0 in equation (1) are
important and are allowed to vary over time for both the load and the temperature. For the load we additionally
allow for φL,L,−∞,k with k ∈ {1, 2, 24, 25} to vary over time, and for the temperature φT ,T ,−∞,k with k ∈ {1, 2}.
So in total the 2 intercepts, 4 autoregressive load, and 2 autoregressive temperature coeﬃcients are allowed to vary
over time. Obviously, this choice can be modiﬁed based on knowledge about the important parameters. And again,
it holds true that the more parameters vary over time, the larger the parameter space. Thus, the computation
time increases and the limited over-ﬁtting risk as well.

For the time varying coeﬃcients we assume a similar structure as in Ziel et al. (2015). We assume for a

time-varying parameter of interest ξ (e.g. φi,0 or φi,j,c,k) that

ξ(t) = ξ0 + ξ

(cid:48)

Bξ(t) = ξ0 +

Nξ(cid:88)

l=1

ξlBξ

l (t)

(3)

where ξ = (ξ1, . . . , ξNξ )(cid:48) is the vector of coeﬃcients that applies to the basis functions Bξ = (Bξ
Obviously, the sum in (3) is empty for constant parameters.

1, . . . , Bξ
Nξ

)(cid:48).

The basis functions of the time-varying coeﬃcients have to be chosen accurately. The selection is modular.
Several eﬀects can be added and merged easily. We consider a selection of several groups of regressors as listed in
Table 2.

Table 2: List of all considered groups G1, . . . ,G8 of basis functions

Group Description
G1
G2
G3
G4
G5
G6
G7
G8

hourly impacts on the seasonal daily pattern
hourly impacts on the seasonal weekly pattern
daily impacts on the seasonal annual pattern
smooth annual impacts
long term trend eﬀects
ﬁxed date public holidays eﬀects
varying date public holidays eﬀects
interaction eﬀects between G1 and G4

Below we explain the groups G1, . . . ,G8 one by one. The daily and the weekly mean electric load of the
GEFCom2014-L data is given in Figure 2. In 2a we see the clear distinct seasonal daily pattern, with low values
during night and high values during the day. The group G1 will cover this eﬀect. Obviously, this requires 24
parameters. However, in 2b we observe that the Saturdays and Sundays show diﬀerent behaviors to the typical
working days from Monday to Friday, which exhibit basically the same behavior every day. Nevertheless, there is

2 Time Series Model

4

(a) Hourly mean load during a day

(b) Hourly mean load during a week

Figure 2: Hourly mean load during a day (2a) and week (2b) of the GEFCom2014-L data

a transition eﬀect on Monday morning and Friday evening towards and from the weekend. G2 will cover the full
weekly structure and 168 parameters are required. As mentioned, there is redundancy in the pattern, e.g. the
Tuesdays, Wednesdays and Thursdays generally exhibit similar behaviors. This structure is automatically taken
into account when using the regressors G1 and G2 in combination with the lasso estimation technique. The basis
function of group G1 and G2 are deﬁned by

G1
k (t) =

B

, k ≤ HoD(t)
, otherwise

and B

G2
k (t) =

, k ≤ HoW(t)
, otherwise

(4)

(cid:40)

1
0

(cid:40)

1
0

where HoD(t) and HoW(t) gives the hour-of-the-day (1, 2, . . . , 24) and the hour-of-the-week (1, 2, . . . , 168, start
counting at Sunday 0:00) of time point t. Note that in (4) the parametrization is done by cumulative components.
Therefore the ”≤” relation is used instead of the commonly used ”=” relation. As an example, B
G1
2 models the
G1
additional impact of hour 1:00 to hour 0:00, which is modeled by B
1 ; instead of modeling the direct impact of
hour 1:00 which would be associated with the ”=” relation in (4). In other words, we are modeling the changes of
the impacts associated with an hour, instead of the absolute eﬀects. Our estimation method will make a parameter
included in the model only if the corresponding change is signiﬁcant.

Similarly to the daily and weekly pattern, there is an annual seasonal pattern. To capture this we introduce

(cid:40)

1
0

G3
k (t) =

B

, k ≤ DoY(t)
, otherwise

(5)

where DoY(t) gives the day-of-the-year (1, 2, . . . , 365) of time point t in a common year with 365 days. In a leap
year DoY(t) also takes values from (1, 2, . . . , 365), but the 29th February has the same value (namely 59) as the
28th February. Similarly as above, we model the changes in the annual pattern, not the direct impact.

G3
The next group of basis functions concerns smooth annual impacts. This will capture similar eﬀects as in B
k

but more in a smooth manner. We consider periodic B-splines which results in a local modeling approach. In
detail, we use cubic B-splines with a periodicity of 8765.76 = 24 × 365.24 on an equidistant grid with 6 basis
functions. In graph 3a we see these basis functions on a time range of three years. We clearly observe the local
impact. So e.g. the dashed yellowish function (k = 2) covers only eﬀects in the summer, but has no impact in the
winter.

The most tricky basis function group concerns the long term eﬀects. The challenging part is the distinction
between spurious eﬀects and real long term changes in the load behavior. The spurious eﬀect problem is crucial
for long term forecasting, whereas for shorter time horizons it is negligible. To make the problem clear, suppose
the available time series ends in 31th December. Suppose the last two months, November and December, had
low load values for some unknown reason. Now the question is, if this was a random eﬀect (just a realization of
rare or outlier events) or a structural change in the load level (induced e.g. by better energy eﬃciency which is
not captured by external regressors). The conservative way of statistical modeling would suggest a random eﬀect,
unless the structural change is signiﬁcant enough to be detected by the modeling approach.

We model long term eﬀects by monotonically increasing basis functions. They are constant in the past, then
strictly monotonically increasing in a certain time range where the long term transition eﬀect might have taken
place, then constant after this possible transition. The time range where the basis function is monotonically
increasing should be larger than a year to reduce the probability to include spurious eﬀects. Furthermore, the
distance between these basis functions should be relatively large as well. We consider a distance of one year
between the basis functions with a support of two years for the transition eﬀect. In detail, we use cumulative

llllllllllllllllllllllll120140160HourLoad in MW0246810121416182022llllllllllllllllllllllllllllllllllllllllllllllll120140160HourLoad in MW0246810121416182022SunMonTueWedThuFriSat2 Time Series Model

5

(a) Periodic cubic B-spline basis within 3 years

(b) Cumulated quadratic B-spline basis on 12 years

Figure 3: Illustration of basis functions for G4 and G5

quadratic B-splines as basis function for the long term eﬀects. We consider only basis functions where the in-
sample basis functions take a smallest value of at least 10% of the overall maximum and at most 90% of the overall
maximum. This will reduce the danger of modeling a spurious eﬀect. We end up with only a few basis functions.
An illustrative example for an in-sample period of 12 years (2001 to 2012) with the out-of-sample year 2013 is
given in Figure 3b. Note that the number of the long term basis functions in group G5 depends on the data range.
The next two groups G6 and G7 contain the public holiday information. In general electric load exhibits a
special behavior at public holidays, that eventually disturbs the standard weekly pattern. For modeling purpose,
we group the public holidays into two classes: with ﬁxed date such as New Year’s Day (Jan. 1) and with a ﬂexible
date such as Thanksgiving Day (fourth Thursday in Nov.). We consider all United States federal public holidays.
We denote the sets of public holidays with ﬁxed and ﬂexible date by Fix and Flex.

As days in Flex are always at a speciﬁc weekday, we can expect the same behavior every year at these public
holidays. If there is a week with a public holiday, then the typical weekly structure in Figure (2b) changes. Not
only the structure of the public holiday is aﬀected, but also the hours before and after the public holiday, due to
transition eﬀects. Therefore we deﬁne for each ﬂexible public holiday F ∈ Flex a basis of 6 + 24 + 6 = 36 hours (6
hours before F , 24 hours at F , and 6 hours after F ). In detail, it is given by

BF

k (t) =

, k ≤ HoF (t)
, otherwise

1
0

(cid:40)

(cid:40)

where HoF (t) gives the hours from 1, 2, . . . , 36 at time point t around the public holiday starting counting from
18:00.

The impact of the days in Fix is complex, because it depends on the weekday of incidence. Some research found
it is usually similar to that of a Sunday (see e.g. Ziel et al. (2015)). We will introduce an eﬀective coeﬃcients C(t)
for each hour of the week. With C(t) we can deﬁne the basis functions for H ∈ Fix

BH

k (t) =

C(t)
0

, k ≤ HoH(t)
, otherwise

,

where HoH(t) gives the hours from 1, 2, . . . , 36 at time point t around the public holidays starting counting from
18:00. The coeﬃcients C(t) are deﬁned as follows: If the public holiday is on a Sunday, then the eﬀective coeﬃcient
is 0, assuming that there is no additional impact of the public holiday on a Sunday. Thus, we call these 24 hourly
mean load values as low level load target.
If such a public holiday occurs during the core working days such
as Tuesday, Wednesday or Thursday, we expect a full impact with the eﬀective coeﬃcient of 1. We call the 24
hourly mean load values of these three days as high level load target. If the holiday happens on Monday, Friday or
Saturday, the impact then should be between above two situations and the eﬀective coeﬃcient is usually between
0 and 1. If we denote the hourly mean load of the week from Figure 2b by actual load target, then we deﬁne the
coeﬃcients by C(t) = max{1 − high level load target(t)−actual load target(t)

high level load target(t)−low level load target(t) , 1}.

The last group of basis functions focuses on interaction eﬀects, which is important for the temperature modeling.
As the length of the night is changing over the year, the daily seasonal pattern change over the year as well. We
create the interaction group by multiplying each basis function of one group with the basis function of another
group. Thus, the interaction groups tend to require many parameters. For that reason we consider for the last
group G8 only the multiplication of the daily seasonal component G1 with the smooth annual basis functions G4.
In detail, G8 contains the basis functions B

j (t) for i ∈ {1, . . . , 24} and j ∈ {1, . . . , 6}.
G4

G8
24(j−1)+i(t) = B

G1
i (t)B

lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll20042005200620070.00.20.40.6timePeriodic Cubic B−spline Basis Functionsk = 1k = 2k = 3k = 4k = 5k = 6lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll2002200420062008201020120.00.20.40.60.81.01.2timeCumulative Quadratic B−spline Basis Functionsk = 1k = 2k = 3k = 4k = 5k = 6k = 7k = 8k = 9k = 103 Estimation and Forecasting Method

6

With all basis function groups, we can deﬁne the full basis function vector Bξ for a parameter ξ. Hence, the basis
functions for a time-varying parameter ξL associated with the load is given by BξL = (BG1, BG2, BG3, . . . , BG8 )
G4
where BG1 = (B
6 ), . . . deﬁne
the vectors of the basis functions. For the time-varying parameters ξT of the temperature modeling process,
we deﬁne BξT = (BG1, BG4, BG8 ). Thus, only daily, smooth annual, and their interaction eﬀects are allowed.
Especially, we do not include any weekly, public holiday or long term eﬀects for modeling the temperature.

G2
168), BG3 = (B

G3
365), BG4 = (B

G1
24 ), BG2 = (B

G1
1 , . . . , B

G2
1 , . . . , B

G3
1 , . . . , B

G4
1 , . . . , B

3. Estimation and Forecasting Method

In the introduction we mention that we use a lasso estimation technique which is a penalized ordinary least

square regression estimator. The ordinary least square (OLS) representation of (1) is given by

Yi = Xiβi + Ei.

2 + λi(cid:107)β(cid:107)1

(6)
Here we denote Yi = (Yi,1, . . . , Yi,n)(cid:48), Xi the n×pi-dimensional regressor matrix that corresponds to (1), βi the full
parameter vector of length pi, Ei = (εi,1, . . . , εi,n)(cid:48) the residual vector, and n as number of observations. However,
we do not perform a lasso estimation for (6) directly, but for its standardized version. Therefore we standardize
(6) so that the regressors and the regressand have all variance 1 and mean 0. Thus we receive the standardized
version of (6):

We can easily compute βi by rescaling, if (cid:101)βi is determined. The lasso optimization problem of (7) is given by

(7)

(cid:101)Yi = (cid:101)Xi(cid:101)βi +(cid:101)Ei.
(cid:98)(cid:101)βi = arg minβ∈Rpi (cid:107)(cid:101)Yi − (cid:101)Xiβ(cid:107)2

(8)
with tuning parameters λi, and (cid:107) · (cid:107)1 and (cid:107) · (cid:107)2 as (cid:96)1- and (cid:96)2-norm. For λi = 0, (8) is the standard OLS problem.

For huge λi values, we have a huge penalty on the parameters and receive the estimator (cid:98)(cid:101)βi = 0 = (0, . . . , 0)(cid:48), so

no parameter is included in the model. In a moderate range of λi values, we get diﬀerent solutions. It holds that
the larger λi, the less parameters are included in the estimated model.

To better understand this feature, we consider a simple lasso problem given by

(cid:107)Yi − Xβ(cid:107)2

2 + λ(cid:107)β(cid:107)1,

(9)
where X is the regressor matrix that contains the 24 basis functions of G1 and the 168 basis functions of G2. We
remember that the OLS solution of this problem corresponds to Figure 2b and requires 168 parameters to full
fully capture all eﬀects. In Figure 4 we plot the ﬁtted values of solution of (9) for four diﬀerent λ values. As
mentioned, we see that the smaller λ, the more parameters are included in the model. Thus, the closer the solution
gets to Figure in 2b. For example, in Figure 4c we observe a pattern where the diﬀerence to Figure 2b is not easy
to observe by eye-balling, even though only 80 parameters are required to capture the structure instead of 168.
In contrast 4a with only 28 parameters does not cover the pattern well, so e.g. the seasonal pattern for all days
except the Sunday is the same during the morning and noon hours. This indicates that the 28 parametric solution
includes not enough parameters for an appropriate modeling.

Note that not only the selection property of the lasso is relevant, but also the shrinkage property. For example,
if we have the lasso solution in 2b with 80 non-zero parameters then this is diﬀerent from the OLS solution of the
corresponding 80 regressors. In general the lasso solution tends to have smaller estimated parameters (in terms of
absolute values) than the OLS solution, due to the shrinkage towards 0. In detail, the in-sample residual sums of
square (RSS) is always larger for the lasso solution than for the OLS solution. Thus, even though there might be
many non-zero parameters in the ﬁnal estimated model, the contribution of many of the non-zero parameters to
the model is small. This shrinkage property reduces the parameter uncertainty and might give better out-of-sample
performance.

In general, the tuning parameters λi should be chosen by a selection algorithm. Usually the optimal λi will be
chosen from a given grid Λi by minimizing an information criterion. We select the tuning parameter with minimal
Bayesian information criterion (BIC). The BIC is a conservative information criterion that avoids over-ﬁtting. For
the grid Λi we choose an exponential grid as suggested by Friedman et al. (2010).

As computation algorithm, we consider the fast coordinate descent algorithm and the corresponding R package
functions of the glmnet package, see e.g. Friedman et al. (2010) for more details. The asymptotic computational
complexity of the coordinate descent algorithm is only O(npi). This is optimal, as npi is the number of elements
in the regression matrix. Thus, we can estimate the model eﬃciently and can easily carry out the model selection.
Another positive feature is that we do not require a division into training and test data set, as we can tune the
model based on statistical theory (like the BIC).

For each forecasting task we use all available data for the lasso estimation procedure. Given the estimated
model, we can use residual based bootstrap to simulate future scenario sample paths as in Ziel et al. (2015). We
consider in total N = 10000 sample paths here. The corresponding empirical percentiles are used as estimates for
the target quantiles.

4 Benchmarks

7

(a) λ = 0.25 with 28 non-zero parameters

(b) λ = 0.125 with 43 non-zero parameters

(c) λ = 0.0625 with 80 non-zero parameters

(d) λ = 0.03125 with 101 non-zero parameters

Figure 4: Fitted model for model (9) for selected λ values with corresponding number of non-zero parameters.

4. Benchmarks

The scenario-based probabilistic forecasting methodology proposed by Hong et al. (2014b) was used by two top
8 teams (Jingrui Xie, top 3; Bidong Liu, top 8) in GEFCom2014-L. In this paper, we develop two benchmarks using
this method with two underlying models. The ﬁrst one is Tao's Vanilla Benchmark model used in GEFCom2012
(Hong et al., 2014a), abbreviated as Vanilla in this paper. The second one is a recency eﬀect model proposed by
Wang et al. (2015), abbreviated as Recency in this paper. In the GEFCom2014-L case study, instead of performing
weather station selection as discussed in Hong et al. (2015b), we create a temperature series by averaging the 25
weather stations to keep the benchmarks simple and easily reproducible. Note that this is diﬀerent from how the
temperature series is created when implementing the lasso based methodology as discussed in section 1.

4.1. Vanilla model

The Vanilla model for the load YL,t is given as:

YL,t = β0 + β1MoY(t) + β2DoW(t) + β3HoD(t) + β4DoW(t)HoD(t) + f (YT ,t) + t,

(10)

where βi are the regression coeﬃcients, MoY(t) gives the month-of-the-year (1, . . . , 12) of time t, DoW(t) gives the
day-of-the-week (1, . . . , 7 with Sunday = 1, Monday = 2, . . .) of time t, HoD(t) gives the hour-of-the-day (1, . . . , 24)
of time t as for equation (4) and

f (YT ,t) = β5YT ,t + β6Y 2T ,t + β7Y 3T ,t + β8YT ,tMoY(t) + β9Y 2T ,tMoY(t)

+ β10Y 3T ,tMoY(t) + β11YT ,tHoD(t) + β12Y 2T ,tHoD(t) + β13Y 3T ,tHoD(t).

(11)

Here for task 1 we are using the model speciﬁed in (10) as the underlying model, of which the parameters are
estimated using the most recent 24 months (from 01/2009 to 12/2010) of hourly load and temperature. The
10 years (2001-2010) of weather history is used to generate 10 weather scenarios.
In total, we are getting 10
load forecasts for each hour in 01/2011. We compute the required 99 quantiles based on these 10 forecasts using
the empirical distribution function. Similarly, we generate the 99 quantiles for the other 11 months of 2011.
For instance, when forecasting the load of 05/2011, the 24 months from 05/2009 to 04/2011 of hourly load and
temperature is used for parameter estimation.

llllllllllllllllllllllll120140160HourLoad in MW0246810121416182022SunMonTueWedThuFriSatllllllllllllllllllllllll120140160HourLoad in MW0246810121416182022SunMonTueWedThuFriSatllllllllllllllllllllllll120140160HourLoad in MW0246810121416182022SunMonTueWedThuFriSatllllllllllllllllllllllll120140160HourLoad in MW0246810121416182022SunMonTueWedThuFriSat5 Empirical Results and Discussion

4.2. Recency model

The underlying model for the second benchmark is given as:

YL,t = β0 + β1MoY(t) + β2DoW(t) + β3HoD(t) + β4DoW(t)HoD(t)

where f is as in (11) and the daily moving average temperature of the j-th day (cid:101)YT ,t,j is deﬁned through

k∈K

+ f (YT ,t) +

f (YT ,t−k) + t,

f ((cid:101)YT ,t,j) +

(cid:88)

(cid:88)

j∈J

(cid:101)YT ,t,j =

1
24

24j(cid:88)

(cid:101)YT ,t−h.

h=24j−23

8

(12)

(13)

The sets J and K in equation (12) are given by J = {1, . . . , J} and K = {1, . . . , K} for J > 0 and K > 0; they
are empty if J = 0 and K = 0. Note that for (J, K) = (0, 0) we receive the Vanilla in (10). The ’average-lag’ pair
(J, K) needs to be identiﬁed before the Recency model could be applied to generate forecast for the target month.
Since the load pattern against temperature varies each year, the optimal pair selected correspondingly changes
every year. To identify the optimal pair for the year i, we use the data of year (i − 3) and (i − 2) as training,
the data of year (i − 1) as validation. The pair resulting in the lowest mean absolute percentage error (MAPE)
in validation period will be selected and then the corresponding Recency model will be applied to forecast the
year i. We search for the optimal (J, K) on the grid {0, . . . , 7} × {0, . . . , 48}. With this method, the optimal pair
identiﬁed for the year of 2011 is (2, 10) for the GEFCom2014-L data.

In the GEFCom2014-E case study, the target years are from 2010 to 2014. The optimal pairs identiﬁed are
listed in Table 3. After identifying the optimal pairs of (J, K), we follow the same steps as for the ﬁrst benchmark
discussed in Section 4.1, including two years of hourly loads and temperatures for parameter estimation and an
empirical distribution function for extrapolating the 99 quantiles. But we use a Recency model as the underlying
model to do forecasting, instead of the vanilla model. When creating weather scenarios, we use 6 years (2004-
2009) weather data for the target year of 2010, 7 years (2004-2010) for 2011, 8 years (2004-2011) for 2012, 9 years
(2004-2012) for 2013 and 10 years (2004-2013) for 2014.

Table 3: The optimal pairs of (J, K) for the years from 2010 to 2014 in GEFCom2014-E

Year
J
K

2010
1
9

2011
1
0

2012
1
8

2013
1
13

2014
0
13

To keep the benchmarks simple and easy to reproduce, neither underlying models incorperate any other special
treatments such as weather station selection, data cleansing, weekend and holiday eﬀect modeling, or forecast
combination.

5. Empirical Results and Discussion

We evaluate the forecasting performance by the overall mean pinball loss function of the 99 percentiles. For
more details on the pinball loss function and evaluation methods used in GEFCom2014-L, see Hong et al. (2015a).

5.1. GEFCom2014-L results

As an illustrative example, the predicted 99 quantiles for the April 2011 task are given in Figure 5. We observe
that the daily and weekly seasonal behaviors are well captured. Furthermore, the prediction intervals get wider
with increasing forecasting horizon as expected.

The pinball scores of the proposed model (Lasso) and the two benchmarks are given in Table 4. We also
list Bidong Liu’s original GEFCom2014-L scores in the last column under BL. The main factors resulting in the
diﬀerence between the two benchmarks and BL include the length of training data and the extrapolation method.
In GEFCom2014-L, Bidong Liu implemented the scenario based method as described in section 4 for months 2 to
12, but not month 1. For parameter estimation, Bidong Liu used 5 years of historical data for most of the tasks
during GEFCom2014-L. In addition, the required quantiles were generated by linear extrapolation. For illustration
purpose, we also list the pinball scores from the Vanilla benchmark estimated using 5 years of data in Table 4
under Vanilla-5Y.

We observe that the proposed lasso estimation method outperforms the two benchmarks, i.e.Vanilla and
Recency in 9 and 8 months out of 12. The reductions on the 12-month average pinball score are 6.4% and 7.6%
comparing with the Recency and Vanilla, respectively. Although BL ranked top 8 in GEFCom2014-L, its average
pinball score is higher than all the other four methods. The average pinball score of Vanilla-5Y(8.32) is high than
Vanilla(8.05), which reveals the necessity of selecting the right length of the training data.

5 Empirical Results and Discussion

9

Figure 5: April forecast of the GEFCom2014-L data with corresponding legend and observed values (black line).

Table 4: Overall pinball scores for the GEFCom2014-L data

Month
1
2
3
4
5
6
7
8
9
10
11
12
Average

Lasso Vanilla Recency
12.13
9.88
10.57
9.54
8.38
7.97
4.89
4.80
7.11
5.96
7.35
5.86
9.38
7.66
10.70
11.30
5.65
6.28
3.40
5.20
5.93
6.38
8.99
9.45
7.95
7.44

11.94
10.95
8.57
5.05
7.37
6.75
9.60
11.21
5.81
3.53
6.06
9.74
8.05

BL Vanilla-5Y
11.78
11.24
8.70
5.67
7.98
6.48
9.08
11.36
6.19
4.53
6.50
10.29
8.32

16.42
11.87
9.37
5.62
7.74
6.55
9.14
11.35
6.51
4.80
6.97
10.89
8.94

5.2. GEFCom2014-E results

The pinball scores of the proposed method (Lasso) and the two benchmarks in GEFCom2014-E case study
are given in Tables 5. We also provide the original scores of Florian Ziel (FZ) in the GEFCom2014-E. The FZ
scores slightly diﬀer from the Lasso, because the long term trend components (G5) were added to the time-varying
parameters of Lasso. For FZ, no long term modeling was considered, but for the years 2012 and 2013 a manual
long-term eﬀect adjustment was done. Additionally, the list of considered holidays was extended by some bridging
holidays, such as Christmas Eve (24 Dec), Boxing Day (26 Dec) and New Years Eve (31 Dec).

Similarly to the GEFCom2014-L results, the lasso outperforms the two benchmarks in 4 out of 5 years. The
average reductions of the pinball score in comparison with the Recency and the Vanilla are 11.9% and 15.6%,
respectively.

Table 5: Overall pinball scores for the GEFCom2014-E data

Year
2010
2011
2012
2013
2014
Average

Lasso
59.01
49.74
47.08
62.53
55.00
54.69

FZ Vanilla Recency
80.76
56.77
55.37
60.62
56.82
62.07

85.03
59.54
57.58
62.59
59.16
64.78

58.02
54.50
46.51
63.71
52.25
55.00

5.3. Discussion

Even though the proposed methodology outperforms two credible benchmarks, we may further improve it from
several aspects. One model assumption is the homoscedasticity of the residuals, but the residuals are heteroscedas-
tic in practice. Usually we observe lower variation in night and during low load seasons. The heteroscedasticity
of residuals should be taken into account when designing the model. Ziel et al. (2015) and Ziel (2015) suggest

50100150200TimeLoad in MW01Apr03Apr05Apr07Apr09Apr11Apr13Apr15Apr17Apr19Apr21Apr23Apr25Apr27Apr29AprFriSatSunMonTueWedThuFriSatSunMonTueWedThuFriSatSunMonTueWedThuFriSatSunMonTueWedThuFriSat1%10%20%30%40%50%60%70%80%90%99%References

10

an iteratively reweighted lasso approach incorporating the volatility of the residuals. Their results suggest a sig-
niﬁcant improvement of the forecasting results. It might help as well to apply normality assumption with group
analysis as discussed by Xie et al. (2015) or a block bootstrap method as used by Fan and Hyndman (2012), to
incorporate the remaining dependency structure in the residuals. Another issue is the tuning of the lasso itself.
We simply considered the Bayesian information criterion, but other special cases of the generalized information
criterion (GIC) might yield better forecasting performance. Lastly, for the GEFCom2014-L data, the treatment of
the available temperature information might be improved. For instance, the weather station selection methodology
as proposed by Hong et al. (2015b) might yield a better incorporation of the temperature data.

6. Summary and Conclusion

We introduce a lasso estimation based methodology that can estimate parameters for a large pool of candidate
variables to capture several distinct and well-known stylized facts in load forecasting. The proposed methodology
ranked top 2 in GEFCom2014-E. Two empirical studies based on two recent probabilistic load forecasting com-
petitions (GEFCom2014-L and GEFCom2013-E) demonstrate the superior competence of the proposed method
over two credible benchmarks.

7. References

References

Fan, S. and Hyndman, R. J. (2012). Short-term load forecasting based on a semi-parametric additive model.

IEEE

Transactions on Power Systems, 27(1):134–141.

Friedman, J., Hastie, T., and Tibshirani, R. (2010). Regularization paths for generalized linear models via coordinate

descent. Journal of statistical software, 33(1):1.

Hong, T., Pinson, P., and Fan, S. (2014a). Global energy forecasting competition 2012. International Journal of Forecasting,

30(2):357–363.

Hong, T., Pinson, P., Fan, S., Zareipour, H., Troccoli, A., and Hyndman, R. J. (2015a). Probabilistic energy forecasting:

state-of-the-art 2015. International Journal of Forecasting (to appear).

Hong, T., Wang, P., and White, L. (2015b). Weather station selection for electric load forecasting. International Journal

of Forecasting, 31(2):286–295.

Hong, T., Wilson, J., and Xie, J. (2014b). Long term probabilistic load forecasting and normalization with hourly infor-

mation. IEEE Transactions on Smart Grid, 5(1):456–462.

Tibshirani, R. (1996). Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society. Series B

(Methodological), pages 267–288.

Wang, P., Liu, B., and Hong, T. (2015). Electric load forecasting with recency eﬀect: a big data approach. International

Journal of Forecasting (accepted).

Xie, J., Hong, T., Laing, D. T., and Kang, C. (2015). On normality assumption in residual simulation for probabilistic load

forecasting. IEEE Transactions on Smart Grid (in press, DOI: 10.1109/TSG.2015.2447007).

Ziel, F. (2015). Iteratively reweighted adaptive lasso for conditional heteroscedastic time series with applications to ar-arch

type processes. Computational Statistics and Data Analysis (accepted, DOI: 10.1016/j.csda.2015.11.016).

Ziel, F., Steinert, R., and Husmann, S. (2015). Eﬃcient modeling and forecasting of electricity spot prices. Energy

Economics, 47:98–111.

