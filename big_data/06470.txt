Frankenstein: Learning Deep Face Representations using Small Data

Guosheng Hu1, Xiaojiang Peng1, Yongxin Yang2, Timothy Hospedales2, Jakob Verbeek1

LEAR team, Inria Grenoble Rhone-Alpes, France1

Electronic Engineering and Computer Science, Queen Mary University of London, UK2

{guosheng.hu,xiaojiang.peng,jokob.verbeek}@inria.fr, {yongxin.yang, t.hospedales}@qmul.ac.uk

6
1
0
2

 
r
a

 

M
1
2

 
 
]

V
C
.
s
c
[
 
 

1
v
0
7
4
6
0

.

3
0
6
1
:
v
i
X
r
a

Abstract

Deep convolutional neural networks have recently
proven extremely effective for difﬁcult
face recognition
problems in uncontrolled settings. To train such networks
very large training sets are needed with millions of labeled
images. For some applications, such as near-infrared (NIR)
face recognition, such larger training datasets are, however,
not publicly available and very difﬁcult to collect. We pro-
pose a method to generate very large training datasets of
synthetic images by compositing real face images in a given
dataset. We show that this method enables to learn models
from as few as 10,000 training images, which perform on
par with models trained from 500,000 images. Using our
approach we also improve the state-of-the-art results on the
CASIA NIR-VIS heterogeneous face recognition dataset.

1. Introduction

In recent years, deep learning methods, and in particular
convolutional neural networks (CNNs), have achieved con-
siderable success in a range of computer vision applications
including object recognition [23], object detection [10], se-
mantic segmentation [34], action recognition [44], and face
recognition [38]. The recent success of CNNs stems from
the following facts: (i) big annotated training datasets are
currently available for a variety of recognition problems to
learn rich models with millions of free parameters; (ii) mas-
sively parallel GPU implementations greatly improve the
training efﬁciency of CNNs; and (iii) new effective CNN ar-
chitectures are being proposed, such as the very deep VGG
network [45] and Google’s Inception network [54].

Good features are essential for object recognition, in-
cluding face recognition. Conventional features include
linear functions of the raw pixel values, including Eigen-
face (Principal Component Analysis) [56], Fisherface (Lin-
ear Discriminant Analysis)
[3], and Laplacianface (Lo-
cality Preserving Projection) [12]. Such linear features
were later replaced by hand-crafted local non-linear fea-
tures, such as Local Binary Patterns [1], Local Phase Quan-

tisation (LPQ) [2], and Fisher vectors computed over dense
SIFT descriptors [42]. Note that the latter is an exam-
ple of a feature that also involves unsupervised learning.
These traditional features achieve promising face recogni-
tion rates in constrained environments, as e.g. represented
in the CMU PIE dataset [41]. However, using these fea-
tures face recognition performance may degrade dramati-
cally in uncontrolled environments, as e.g. represented in
the Labeled Faces in the Wild (LFW) benchmark [15]. To
improve the performance in such challenging settings, met-
ric learning can be used, see e.g. [4, 11, 57]. Metric learning
methods learn a (linear) transformation of the features that
pulls the objects that have the same label closer together,
while pushing the objects that have different labels apart.

Although hand-crafted features and metric learning
achieve promising performance for uncontrolled face recog-
nition, it remains cumbersome to improve the design of
hand-crafted local features (such as SIFT [26]) and their
aggregation mechanisms (such as Fisher vectors [37]). This
is because the experimental evaluation results of the fea-
tures cannot be automatically fed back to improve the ro-
bustness w.r.t. to nuisance factors such as pose, illumina-
tion and expression. The major advantage of CNNs is that
all processing layers, starting from the raw pixel-level in-
put, have conﬁgurable parameters that can be learned from
data. This obviates the need for manual feature design,
and replaces it with supervised data-driven feature learn-
ing. Learning the large number of parameters in CNN mod-
els (millions of parameters are rather a rule than an excep-
tion) requires very large training datasets. For example, the
CNNs, which achieve state-of-the-art performance on the
LFW benchmark, are trained using datasets with millions
of labeled faces: Facebook’s DeepFace [55] and Google’s
FaceNet [39] were trained using 4 million and 200 million
training samples, respectively.

For some recognition problems large supervised train-
ing datasets can be collected relatively easily. For exam-
ple the CASIA Webface dataset [60] of 500,000 face im-
ages was collected semi-automatically from IMDb. How-
ever, in many other cases collection of large datasets may

be costly, and possibly problematic due to privacy regula-
tion. For example, thermal infrared imaging is ideal for
low-light nighttime and covert face recognition applications
[22], but it is not possible to collect millions of labeled train-
ing images from the internet for the thermal IR domain. The
lack of large training datasets is an important bottleneck that
prevents the use of deep learning methods in such cases,
as the models will overﬁt dramatically when using small
datasets [14].

To address this issue, the use of big synthetic train-
ing datasets has been explored by a number of authors
[17, 29, 35]. There are two important advantages of using
synthetic data (i) one can generate as many training sam-
ples as desired, and (ii) it allows explicit control over the
nuisance factors, e.g. we can synthesize face images of all
desired viewpoints, whereas data collected from the inter-
net might be mostly limited to near frontal views. The syn-
thetic data has successfully been applied to diverse recogni-
tion problems, including text recognition [17], scene under-
standing [29], and object detection [35]. To the best of our
knowledge, synthesizing large face datasets to train CNNs
has not been investigated before. One reason for this is the
focus on visible spectrum face recognition for which large
training datasets are easily collected from the internet.

Data augmentation is another technique that is com-
monly used to reduce the data scarcity problem, see e.g.
[32, 45]. This is similar to data synthesis, but more limited
in that existing training images are transformed without af-
fecting the semantic class label, e.g. by applying cropping,
rotation, scaling, etc.

The main contribution of this paper is a data synthesize
technique to expand limited face datasets to very large ones
that are suitable to train powerful deep CNNs. We synthe-
size images of a “virtual” subject c by compositing auto-
matically detected face parts (eyes, nose, mouth) of two
existing subjects a and b in the dataset in a ﬁxed pattern.
Images for the new subject are e.g. generated by collaging
e.g. a nose from an image of subject a with a mouth of an
image of subject b. This is motivated by the observation
that face recognition consists in ﬁnding the differences in
the appearance and constellation of face parts among peo-
ple. For a dataset with an equal number of faces per person,
this method can increase a dataset of n images to one with
n2 images when using only 2 face parts (we use 5 parts in
practice). A dataset like LFW can thus be expanded from a
little over 10,000 images to a dataset of 100 million images.
the synthesized
large training datasets indeed signiﬁcantly improve the gen-
eralization capacity of CNNs. In our experiments, we gen-
erate a training set of 1.5 million images using an initial
labeled dataset of only 10,000 images. We improve the face
veriﬁcation rates from 78.97% to 95.77% on LFW using
CNNs trained on 10K images and 1.5 million synthetic im-

We experimentally demonstrate that

ages respectively. In addition, the proposed face synthesize
is also used for NIR-VIS heterogeneous face recognition
and improve the rank-1 face identiﬁcation rate from 17.41%
to 83.86%. With the synthetic data, we achieve state-of-the-
art performance on both (1) LFW under the “unrestricted,
label-free outside data” protocol and (2) CASIA NIR-VIS
2.0 database under rank-1 face identiﬁcation protocol.
2. Related work

Our work relates to three research areas that we brieﬂy
review below: face recognition using deep learning meth-
ods (Section 2.1), face data collection (Section 2.2), and
data augmentation and synthesis methods (Section 2.3).
2.1. Face recognition using deep learning

Here we brieﬂy review recent state-of-the-art face recog-

nition methods based on deep learning.

Since face recognition is a special case of object recogni-
tion, good architectures for general object recognition may
carry over to face recognition. Schroff et al. [38] explored
networks that are based on that of Zeiler & Fergus [62] and
Inception networks [54]. DeepID3 [48] uses variants of
both Inception networks [53] and the very deep VGG net-
work [45]. Parkhi et al. [30] use the same architecture as
the very deep VGG network [45], while Yi et al. [60] also
use 3 × 3 ﬁlters but less layers.

DeepFace [55] combines 3D face alignment and CNN
for face recognition. Speciﬁcally, a 3D model is used for
pose normalization, by which all the faces are rotated to the
frontal pose. In this way, pose variations are removed from
the faces. Then an 8-layer CNN is trained using four million
pose-normalized images.

DeepID [51], DeepID2 [47], DeepID2+ [52] all train an
ensemble of small CNNs. The input of one small CNN is an
image patch cropped around a facial part (face, nose, mouth,
etc.). The same idea is also used in [25]. DeepID uses only
a classiﬁcation-based loss to train the CNN, while DeepID2
includes an additional veriﬁcation-based loss function. To
further improve the performance, DeepID2+ adds the loss
functions to all the convolutional layers rather than the top-
most layer only.

All the above methods train CNNs using large train-
ing datasets (500,000 faces or more). To the best of our
knowledge, only [13] uses small datasets to train CNNs
(only around 10,000 LFW images) and achieves signiﬁ-
cantly worse performance on the LFW benchmark: 87%
vs 97% or higher in [38, 52, 55]. Clearly, sufﬁciently large
training datasets are extremely important for learning deep
face representations.
2.2. Face dataset collection

Since big data is important for learning a deep face rep-
resentation, several research groups have collected large

datasets with 90,000 up to 2.6 million labeled face im-
ages [4, 28, 30, 49, 60]. To achieve this, they collect face
images from the internet, by querying for speciﬁc websites
such as IMDb or general search engines for celebrity names.
This data collection process is detailed in [30, 60].

There are, however, two main weaknesses of the existing
face data collection methods. First, and most importantly,
internet-based collection of large face datasets is limited to
visible spectrum images, and is not applicable to collect e.g.
infrared face images. Second, the existing collection meth-
ods are expensive and time-consuming. It results from the
fact that automatically collected face images are noisy, and
manual ﬁltering has to be performed to remove incorrectly
labeled images [31].

The difﬁculty to collect large datasets in some domains,
e.g. for infrared imaging, motivates the work presented in
this paper. To address this issue we propose a data augmen-
tation method that we describe in the next section.

2.3. Data augmentation and synthesis

The availability of large supervised datasets is the key
for machine learning to succeed, and this is true in partic-
ular for very powerful deep CNN models with millions of
parameters. To alleviate data scarcity in visual recognition
tasks, data augmentation has been used to add more exam-
ples by applying simple image transformations that do not
affect the semantic-level image label, see e.g. [7]. Examples
of such transformations are horizontal mirroring, cropping,
small rotations, etc. Since it is not always clear in advance
which (combinations of) transformations are the most ef-
fective to generate examples that improve the learning the
most, Paulin et al. [33] proposed to learn which transforma-
tions to exploit.

Data augmentation, however, it limited to relatively sim-
ple image transformations. Out-of-plane rotations, for ex-
ample, are hard to accomplish since they would require
some degree of 3D scene understanding from a single im-
age. Pose variations of articulated objects are another ex-
ample of transformations that are non-trivial to obtain, and
generally not used in data augmentation methods.

Training models from synthetic data can overcome such
difﬁculties, provided that sufﬁciently accurate object mod-
els are available. Recent examples where visual recogni-
tion systems have been trained from synthetic data include
the following. Shotton et al. [40] train randomized deci-
sion forests for human pose estimation from synthesized
3D depth data. Jaderberg et al. [18] use synthetic data to
train CNN models for natural scene text recognition. Su
et al. [46] use synthetic images of objects to learn a CNN
for viewpoint estimation. Papon and Schoeler [29] train a
multi-output CNN that predicts class, pose, and location of
objects from realistic cluttered room scenes that are syn-
thesized on the ﬂy. Weinmann et al. [58] synthesize mate-

Figure 1. Schematic illustration of the face synthesis process using
ﬁve parts: rest, left-eye, right-eye, nose, mouth. Parent images P0
and P1 (top) are mixed by using the eyes of P1 and the other parts
of P0 (middle) to form the synthetic image (bottom).

rial images under different viewing and lighting conditions
based on detailed surface geometry measurements, and use
these to train a recognition system using a SIFT-VLAD rep-
resentation [19]. Feng et al. [9] use a 3D morphable face
model to synthesize face images in arbitrary poses to im-
prove the accuracy of facial landmark detection. Ronzant-
sev et al. [36] use rough 3D models to synthesize new views
of real object category instances. They show that this out-
performs more basic data augmentation using crops, ﬂips,
rotations, etc. Our work is most related to that of Ronzant-
sev et al. [36], in that our approach also synthesizes new
images from a dataset of real images.

3. Synthetic data engine

Human faces are well structured in the sense that they
are composed of parts (eyes, nose, mouth, etc.) which are
organized in a relatively rigid constellation. Face recogni-
tion is conducted implicitly by ﬁnding the differences of one
or more facial parts and possibly their constellation among
people. Motivated by this, our synthetic face images are
generated by swapping one or more facial parts among ex-
isting “parent” images. In our work we use ﬁve face parts:
right eye (RE), left eye (LE), nose (N), mouth (M) and the
rest (R). See Figure 2 for an illustration. For simplicity, we
only consider the synthesis using only two parent images
in this work. Our synthesis method can easily be extended,
however, to the scenario of more than two parent images.
Suppose that we have an original dataset and let S denote
the set subjects in the dataset, and let ni denote the number

P0P1selectioncode01100of images of subject i ∈ S. To synthesize an image, we
select a tuple (i, j, c, s, t) where i ∈ S, j ∈ S correspond
to two subjects that will be mixed, and s ∈ {1, . . . , ni} and
t ∈ {1, . . . , nj} are indices of images of i and j that will
be used. The bitcode c ∈ {0, 1}5 deﬁnes which parts will
be taken from each subject. A zero at a certain position
in b means that the corresponding part will be taken from i,
otherwise it will be taken from j. There are only 25−2 = 30
valid options for b, since the codes 00000 and 11111 would
correspond to the original images of s and t respectively,
instead of synthetic ones.

To synthesize a new image, we distinguish the two parent
images as the “base” image from which we use the R (the
rest) part, and the “injection” image from which one or more
parts will be pasted on this base image. Since the size of the
facial parts of the two parent images are in general different,
we re-size the facial parts of the injection image to that of
the base image. The main challenge to implement the pro-
posed synthesis method is to accurately locate the positions
of the facial parts. Recently, many efﬁcient and accurate
landmark detectors have been proposed. We use four land-
marks detected by the method of Zhang et al. [63] to deﬁne
the rectangular region that corresponds to each face part.
We refer to each choice of (i, j, c) with i (cid:54)= j as a “virtual
subject” which consists of a mix of two existing subjects in
the dataset. In total we can generate 30|S|(|S|−1)/2 differ-
ent virtual subjects, and for each of these we can generate
ni × nj samples. Note that if we set i = j we can in the
same manner synthesize 30ni(ni − 1)/2 new images for an
existing subject. We will explore the relative merit of both
types of synthetic images in our experiments.

4. Face recognition pipeline

In this section we describe the different elements of our

pipeline for face identiﬁcation and veriﬁcation in detail.

4.1. CNN architectures

Face recognition in the wild is a challenging task. As
described in Section 2.1, the existing deep learning meth-
ods highly depend on big training data. Very little research
investigates training CNNs using small data. Recently, Hu
[14] evaluated CNNs trained using small datasets. Due to
the limited training samples, they found the performance
of CNNs to be worse than handcrafted features such as
high-dimensionality features [6] (0.8763 vs 0.9318). In this
work, we use a limited training set of around 10,000 images
to synthesize a much larger one of around 1.5 million im-
ages for CNN training. The synthesized training data cap-
tures various deformable facial patterns that is important to
improve the generalization capacity of CNNs.

We use two CNN architectures. The ﬁrst one introduced
in [14] has fewer ﬁlters and is referred as CNN-S, and the

Table 1. The architectures of the two CNN models we used in our
experiments.

CNN-L

CNN-S

conv1

conv2

conv3

conv4

conv5

32×3×3, st.1
64 × 3 × 3, st.1
x2 maxpool, st.2
64×3×3, st.1
128 × 3 × 3, st.1
x2 maxpool, st.2
96×3×3, st.1
192 × 3 × 3, st.1
x2 maxpool, st.2
128×3×3, st.1
256 × 3 × 3, st.1
x2 maxpool, st.2
160×3×3, st.1
320 × 3 × 3, st.1
x7 avgpool, st.1

16×3×3, st.1
16 × 3 × 3, st.1
x2 maxpool, st.2

32×3×3, st.1
x2 maxpool, st.2

48×3×3, st.1
x2 maxpool, st.2

-

-

fully connected

Softmax-5000

FC-160

Softmax-5000

other introduced in [61] is much larger and therefore re-
ferred as CNN-L. These two architectures are detailed in Ta-
ble 1. Using the CNN-L model we achieve state-of-the-art
performance on the LFW dataset [15] under ‘unrestricted,
label-free outside data’ protocol.

4.2. NIR-VIS heterogeneous face recognition

NIR-VIS (near-infrared to visual) face recognition is im-
portant in applications where probe images are captured by
NIR cameras that use active lighting which is invisible to
the human eye. Gallery images are, however, generally only
available in the visible spectrum. The existing methods for
NIR-VIS face recognition include three steps: (i) illumina-
tion pre-processing, (ii) feature extraction, and (iii) metric
learning. First, the NIR-VIS illumination differences cause
the main difﬁculty of NIR-VIS face recognition. There-
fore, illumination normalization methods are usually used
to reduce these differences. Second, to reduce the hetero-
geneities of NIR and VIS images, illumination-robust fea-
tures such as LBP are usually extracted. Third, metric learn-
ing is widely utilized, aiming at removing the differences of
modalities and meanwhile keeping the discriminative infor-
mation of the extracted features.

In this work, we also follow these three steps that are
detailed in Section 5.2.3. Unlike the existing work that ex-

tracts handcrafted features, we learn face representations
using the introduced two CNN architectures introduced in
Section 4.1. To our knowledge, we are the ﬁrst to use deep
CNNs for NIR-VIS face recognition. The main difﬁculty
of training CNNs results from the lack of training NIR im-
ages which are not available from internet. To solve this,
we synthesize big data for CNN training.
4.3. Network Fusion

Fusion of multiple networks is a widely used strategy
to improve the performance of deep CNN models. For ex-
ample, in [45], an ensemble of seven networks is used to
improve the object recognition performance due to comple-
mentarity of the models trained at different scales. Network
fusion is also successfully applied to learn face representa-
tions. DeepID and its variants [47, 51, 52] train multiple
CNNs using image patches extracted from different facial
parts.

The heterogeneity of NIR and VIS images is intrinsically
caused by the different spectral bands from which they are
acquired. The images in both modalities, however, are re-
ﬂective in nature and affected by illumination variations. Il-
lumination normalization can be used to reduce such vari-
ability, at the risk of loosing identity-speciﬁc characteris-
tics. In this work, we fuse two networks that are trained us-
ing the original and illumination-normalized images respec-
tively. This network fusion signiﬁcantly boosts the recogni-
tion rate.
4.4. Metric Learning

The goal of metric learning is to make different classes
more separated, and instances in the same class closer. Most
approaches learn a Mahalanobis metric

A(xi, xj) = (xi − xj)T A(xi − xj)
d2

(1)

which maximizes inter-class discrepancy, while minimiz-
ing intra-class discrepancy. Some methods, instead, learn a
generalized dot-product of the form

d2
B(xi, xj) = xT

i Bxj

(2)

Metric learning methods are widely used for face iden-
tiﬁcation and veriﬁcation. Because identiﬁcation and ver-
iﬁcation are two different tasks, different loss functions
should be optimized to learn the metric. Joint Bayesian met-
ric learning (JB) [5] and Fisher linear discriminant analysis
(LDA) are probably the two most widely used metric learn-
ing methods for face veriﬁcation and identiﬁcation respec-
tively. In particular, LDA can be seen as a method to learn a
metric of the form of Eq. (1), while JB learns a veriﬁcation
function that can be written as a weighted sum of Eq. (1)
and (2). In our work we use JB and LDA to improve the
performance of face veriﬁcation and identiﬁcation respec-
tively.

5. Experiments
5.1. Facial data synthesis

Given some face images and their IDs, we deﬁne three
Inter-Synthesis, Intra-Synthesis, and
synthetic strategies:
Self-Synthesis. Inter-Synthesis synthesizes a new image us-
ing two parents from different IDs as shown in Fig. 1. The
facial components of an Intra-Synthesized face are from dif-
ferent images with the same ID. Self-synthesis is a special
cause of Intra-Synthesis. Speciﬁcally, one given image syn-
thesizes new images by swapping facial components of it-
self and its mirrored images. By virtue of Self-Synthesis,
one input image can become maximum 32 images which
have complementary information. The fusion of features
extracted from these 32 images has stronger face represen-
tation capacity which is validated in Section 5.2.2. In the
view of cross modality such as NIR-VIS image pairs, we
also deﬁne ‘cross-modality synthesis’ which uses images
from different modalities to synthesize a new one. Some
synthetic images from the CASIA NIR-VIS 2.0 dataset with
LSSF [59] illumination normalization are shown in Fig. 2.
The reasons of using LSSF illumination normalization is
detailed in Section 5.2.3. As shown in Fig. 2, the results of
Intra-Synthesis method are usually more natural than Inter-
Synthesis method since the Intra-Synthesis method uses the
same ID. However, as shown in the right of Fig. 2, some
samples from Intra-Synthesis can also be very strange due
to large pose variations caused by inaccurate detected land-
marks.

5.2. Face recognition
5.2.1

Implementation details

Our implementation is based on the Caffe open source deep
learning toolbox. Before face synthesis, all the raw images
are aligned and cropped to size 100×100 as in [61] on both
datasets. We train our models using images only from LFW
and CASIA NIR-VIS2.0 databases. For the CNN-S model
on both datasets, we set the learning rate as 0.001, and de-
crease it by 10 times every 4000 iterations, and stop training
after 10K iterations. We practically ﬁnd dropout is not help-
ful for small network, therefore, we train the CNN-S model
without dropout. For the CNN-L model on the NIR-VIS
dataset, we set the learning rate as 0.01, and decrease it by
10 times every 8000 iterations, and stop training after 20K
iterations. We train networks on the combination of NIR
and VIS data. For the CNN-L model on the LFW dataset,
we set the learning rate as 0.01, and decrease it by 10 times
every 120K iterations, and stop training after 200K itera-
tions. We use dropout 0.4 for the pool5 layer of the CNN-L
model. For both CNN-S and CNN-L models, the batch size
is 128, momentum is 0.9, and decay is 0.0005. The features
used in our experiments of CNN-S and CNN-L are FC-160

Figure 2. Left: Inter-Synthesis. Middle: cross-modality Intra-Synthesis. Right: Intra-Synthesis.

(160D) and Pool5 (320D), respectively.

5.2.2 Face recognition in the wild
Database and protocol Labeled Faces in the Wild (LFW)
[16] is a public available dataset for unconstrained face
recognition study. It contains 5,749 unique identities and
13,233 face photographs. The training and test sets are pre-
deﬁned in [16]. For evaluation, the full dataset is divided
into ten splits, and each time nine of them are used for train-
ing and the left one for testing. Our work falls in the pro-
tocol of ”Unrestricted, Label-Free Outside Data” as we use
the identity information to train the neural network (soft-
max loss). Meanwhile, all face images are aligned using a
model trained on unlabeled outside data. As a benchmark
for comparison, we report the mean and standard deviation
of classiﬁcation accuracy.

Synthetic data generation Under LFW protocol,
the
training set in each fold is different. Therefore, the size
of synthetic data and the original raw LFW data in Table 2
is averaged over 10 folds. We generate 1.5 million training
images including 1 million ‘Inter-Syn’ ones and 0.5 million
‘Intra-Syn’ ones. ‘Inter-Syn’ and ‘Intra-Syn’ are deﬁned in
Section 5.1.

Table 2. Training data synthesized from LFW

Synthetic

Intra-Syn
Inter-Syn

total

Raw

IDs
5K
5K
10K
5K

Images
500K
1M
1.5M
10K

Images/ID

100
200
150
2

Impact of synthetic data Table 3 analyzes the impor-
tance of using the synthetic data. First, CNN-S trained us-

ing synthetic data (‘Intra-Syn’ and ‘Inter-Syn’) outperforms
greatly that trained using raw LFW images, showing the
importance of data synthesize. Second, ‘Inter-Syn’ works
slightly better than ‘Intra-Syn’ because ‘Inter-Syn’ can cap-
ture richer facial variations. Third, combining ‘Inter-Syn’
and ‘Intra-Syn’ works better than either of them because
both inter- and intra-personal variations can be captured.
Fourth, averaging the features of 32 ‘Self-Syn’ (‘32-Avg’
in Table 3 and deﬁned in Section 5.1 ) images works con-
sistently better than that of one single test image (‘sin-
gle’ in Table 3 ). Fifth, CNN-L works consistently better
than CNN-S using either raw LFW or synthetic images be-
cause deeper architecture has stronger generalization capac-
ity. Last but not least, the metric learning (JB) can further
enhance the face recognition performance.

Comparison with the state-of-the-art Table 4 compares
our method with state-of-the-art methods. All methods
listed in table 4 except ours use hand-crafted features such
as LBP, and this again indicates the hardness of training
deep CNNs with small data. In fact, the best deep learn-
ing solution [50] recorded in ofﬁcial benchmark achieves
91.75%, and ours is 4% better. Besides, most of state-of-
the-art solutions rely on an extremely high dimensional fea-
ture vector because they fundamentally employ dense sam-
pling on the face image, in contrast, we just use a 320-
dimensional feature vector, which is much more compact
than others.

5.2.3 NIR-VIS face recognition
Database and protocol The largest face database across
NIR and VIS spectrum so far is the CASIA NIR-VIS 2.0
face database (CASIA NIR-VIS2.0)
It contains
17,580 images of 725 subjects which exhibit intra-personal
variations such as pose and expression. This database in-

[24].

Table 3. Comparison of synthetic data methods on LFW under ‘unrestricted, label-free outside data’

Architecture

Metric
learning

CNN-S

-

CNN-L

-

-

JB [5]

JB [5]

Raw

Training data

Intra-Syn+Raw
Inter-Syn+Raw

single (%)
78.97 ± 0.78
83.03 ± 0.56
83.18 ± 0.74
Intra-Syn+Inter-Syn+Raw 85.61 ± 0.71
85.03 ± 0.98
87.03 ± 0.69
Intra-Syn+Inter-Syn+Raw 94.88 ± 0.66
Intra-Syn+Inter-Syn+Raw 95.32 ± 0.38

Raw
Raw

32-Avg (%)

-

83.93 ± 0.49
84.35 ± 0.65
86.98 ± 0.57

-
-

95.13 ± 0.53
95.77 ± 0.38

Table 4. Comparison with state-of-the-art methods on LFW under
‘unrestricted, label-free outside data’

Methods

High-dim LBP [6]

Fisher vector faces [43]

HPEN [64]

MDML-DCPs [8]

the proposed

Accuracy (%)
93.18 ± 1.07
93.03 ± 1.05
95.25 ± 0.36
95.58 ± 0.34
95.77 ± 0.38

cludes two views: view 1 for parameter tuning and view 2
including 10 folds for performance evaluation. During test,
the gallery and probe images are VIS and NIR images re-
spectively, simulating the scenario of face recognition in the
dark environment. The rank 1 identiﬁcation rate including
the mean accuracy and standard deviation of 10 folds are
reported.

Synthetic data generation We synthesize training sam-
ples using the existing images in CASIA NIR-VIS2.0. The
size of synthesized data is detailed in Table 5.

rates at different training iterations using different input im-
ages. In Fig. 3 and 4, three IN methods outperform ‘GRAY’
which does not use IN method, showing the effectiveness
of IN. Note that LSSF achieves the best performance due to
its strong capacity of removing illumination but keep iden-
tity information. In addition, same as Section 5.2.2 CNN-L
works better than CNN-S.

The use of illumination-robust features can effectively
reduce the gap between NIR and VIS images. The most
commonly used hand-crafted feature is LBP. Since LSSF
achieves the best performance, we extract LBP features
from LSSF-normalized images and achieve 12.48 ± 3.1 in
comparison with 17.41 ± 3.76 by CNN-L learned feature.
It shows the superior performance of CNN learned features.

Table 5. Training data synthesized from CASIA NIR-VIS2.0

Synthetic

Intra-Syn
Inter-Syn

total

Raw

IDs
357
1K
1.4K
357

Images
90K
150K
240K
8.5K

Images/ID

250
150
170
23

Illumination normalization and feature extraction Il-
lumination Normalization (IN) methods are usually used to
reduce the gab between NIR and VIS images. To inves-
tigate the impact of IN methods, we preprocessed images
using three popular IN methods:
illumination normaliza-
tion based on large-and small-scale features (LSSF) [59],
DoG ﬁltering-based normalization (DOG) and single-scale
retinex (SSR) [20]. We train CNN-S and CNN-L using illu-
mination normalized and non-normalized images. For sim-
plicity, only the images from CASIA NIR-VIS2.0 excluding
synthetic ones are used. Fig. 3 shows the face recognition

Figure 3. Comparison of illumination normalization methods us-
ing CNN-S

Effects of synthetic data To evaluate the effects of syn-
thetic data on a common ground, LSSF is used to prepro-
cess the illumination and CNN-L is applied to learn face
representations for all (raw and/or synthetic) input images.
In the practice, we ﬁnd two problems of the synthetic data
generated from CASIA NIR-VIS2.0 database: (1) it can-
not capture enough facial variations because it only has 357
subjects as shown in Table 5. (2) there are much fewer VIS
images than NIR ones. To solve these two problems, we

Table 6. Comparison of CNN-L variants and State-of-the-art on CASIA NIR-VIS2.0 Database

Method

Accuracy (%)

CNN-L

State-of-the-art

Training Data
Network Fusion
Metric Learning LDA (Original+LSSF)

Original+LSSF

Original
LSSF

C-CBFD [27]

C-CBFD+LDA [27]

Dictionary Learning [21]

67.99 ± 1.37
66.37 ± 1.45
78.67 ± 1.38
83.86 ± 0.82
56.6 ± 2.4
81.8 ± 2.3
78.46 ± 1.67

Comparison with the state-of-the-art The CNN-Ls in
Table 6 are all trained using synthetic LFW data. First,
LSSF-normalized and Original LFW synthetic data achieve
very comparable performance: 67.99% vs 66.37%. How-
ever, the fusion (averaging) of these 2 features can signif-
icantly improve the face recognition rates.
It shows the
fusion can keep the discriminative facial information but
remove the illumination effects. Second, not surprisingly,
metric learning can further improve the performance. Last,
Table 6 compares the proposed method against the state-
of-the-art solutions [27, 21]. [27] uses a designed descriptor
that performs better in this dataset compared with other gen-
eral hand-crafted feature, and LDA can further improve the
accuracy. Our method signiﬁcantly outperforms [27] in the
case that LDA is not employed, while it earns 2% advantage
with the help of LDA. [21] tries to solve the domain shift
between two data sources by a cross-modality metric learn-
ing: it assumes that a pair of NIR and VIS images shares the
same sparse representation under two jointly learned dictio-
naries. Our method beats [21] with a 5% margin without
such extra step of dictionary learning.

6. Conclusion

Recently, convolutional neural networks have attracted a
lot of attention in the ﬁeld of face recognition. However,
deep learning methods highly depend on big training data,
which is not always available. To solve this problem in the
ﬁeld of face recognition, we propose a new face synthe-
size method which swaps the facial components of different
face images to generate a new face. With this technique,
we achieve state-of-the-art face recognition performance on
LFW and CASIA NIR-VIS2.0 face databases.
In the fu-
ture, we will apply this technique to more applications of
face analysis.

Figure 4. Comparison of illumination normalization methods us-
ing CNN-L

also use the synthetic data generated from LFW images de-
ﬁned in Table 2. Table 7 compares the results achieved
by these two sources of synthetic data. First, the accu-
racy achieved by using the synthetic data generated from
CASIA NIR-VIS2.0 database is 34.13 ± 2.13, in compari-
son with 17.41 ± 3.76 without synthetic data. The signiﬁ-
cant improvement shows the effectiveness of data synthesis.
Second, the model trained using raw and synthetic LSSF-
normalized LFW images greatly outperforms those syn-
thetic CASIA NIR-VIS2.0 images even NIR images are un-
seen at all during training The reasons are 2-fold: (1) LFW
images contains more subjects which can capture more fa-
cial variations as analyzed above. (2) LSSF can properly
reduce the gap between NIR and VIS, therefore, LSSF-
normalized LFW synthetic images can generalize well to
LSSF-normalized NIR images.

Table 7. Evaluation of the impact of synthetic data on CASIA NIR-
VIS2.0 Database

Baseline

Synthetic

Data

Raw

Raw+Syn

-
-

Training Data

CASIA

NIR-VIS2.0

LFW

-
-

Raw

Raw+Syn

Accuracy(%)

17.41 ± 3.76
34.13 ± 2.13
38.45 ± 2.08
66.37 ± 1.45

References
[1] T. Ahonen, A. Hadid, and M. Pietik¨ainen. Face recognition
In Computer vision-eccv 2004,

with local binary patterns.
pages 469–481. Springer, 2004.

[2] T. Ahonen, E. Rahtu, V. Ojansivu, and J. Heikkila. Recog-
In

nition of blurred faces using local phase quantization.
International Conference on Pattern Recognition, 2008.

[3] P. Belhumeur, J. Hespanha, and D. Kriegman. Eigenfaces
vs. Fisherfaces: Recognition using class speciﬁc linear pro-
jection. PAMI, 19(7):711–720, 1997.

[4] D. Chen, X. Cao, L. Wang, F. Wen, and J. Sun. Bayesian

face revisited: a joint formulation. In ECCV, 2012.

[5] D. Chen, X. Cao, L. Wang, F. Wen, and J. Sun. Bayesian face
revisited: A joint formulation. In Computer Vision–ECCV
2012, pages 566–579. Springer, 2012.

[6] D. Chen, X. Cao, F. Wen, and J. Sun. Blessing of dimension-
ality: High-dimensional feature and its efﬁcient compression
for face veriﬁcation. In Computer Vision and Pattern Recog-
nition (CVPR), 2013 IEEE Conference on, pages 3025–3032.
IEEE, 2013.

[7] D. Decoste and B. Sch¨olkopf. Training invariant support vec-

tor machines. Machine Learning, 46:161–190, 2002.

[8] C. Ding, J. Choi, D. Tao, and L. S. Davis. Multi-directional
multi-level dual-cross patterns for robust face recognition.
arXiv preprint arXiv:1401.5311, 2014.

[9] Z. Feng, G. Hu, J. Kittler, W. Christmas, and X. Wu. Cas-
caded collaborative regression for robust facial landmark de-
tection trained using a mixture of synthetic and real images
with dynamic weighting. 24(11):3425–3440, 2015.

[10] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich fea-
ture hierarchies for accurate object detection and semantic
segmentation. In CVPR, 2014.

[11] M. Guillaumin, J. Verbeek, and C. Schmid.

Is that you?
Metric learning approaches for face identiﬁcation. In ICCV,
2009.

[12] X. He, S. Yan, Y. Hu, P. Niyogi, and H.-J. Zhang. Face recog-

nition using Laplacianfaces. PAMI, 27(3):328–340, 2005.

[13] G. Hu, Y. Yang, D. Yi, J. Kittler, W. Christmas, S. Li, and
T. Hospedales. When face recognition meets with deep
learning: an evaluation of convolutional neural networks for
face recognition. In ICCV Chalearn Looking at People Work-
shop, 2015.

[14] G. Hu, Y. Yang, D. Yi, J. Kittler, W. J. Christmas, S. Z. Li,
and T. M. Hospedales. When face recognition meets with
deep learning: an evaluation of convolutional neural net-
works for face recognition. CoRR, abs/1504.02351, 2015.

[15] G. Huang, M. Ramesh, T. Berg, and E. Learned-Miller. La-
beled faces in the wild: a database for studying face recogni-
tion in unconstrained environments. Technical Report 07-49,
University of Massachusetts, Amherst, 2007.

[16] G. B. Huang, M. Ramesh, T. Berg, and E. Learned-Miller.
Labeled faces in the wild: A database for studying face
recognition in unconstrained environments. Technical Re-
port 07-49, University of Massachusetts, Amherst, October
2007.

[17] M. Jaderberg, K. Simonyan, A. Vedaldi, and A. Zisserman.
Synthetic data and artiﬁcial neural networks for natural scene
text recognition. arXiv preprint arXiv:1406.2227, 2014.

[18] M. Jaderberg, K. Simonyan, A. Vedaldi, and A. Zisserman.
Synthetic data and artiﬁcial neural networks for natural scene
text recognition. In NIPS Deep learning workshop, 2014.

[19] H. J´egou, M. Douze, C. Schmid, and P. P´erez. Aggregating
In

local descriptors into a compact image representation.
CVPR, 2010.

[20] D. J. Jobson, Z.-U. Rahman, and G. A. Woodell. Properties
and performance of a center/surround retinex. IEEE Trans.
Image Processing, 6(3):451–462, 1997.

[21] F. Juefei-Xu, D. Pal, and M. Savvides. Nir-vis heterogeneous
face recognition via cross-spectral joint dictionary learning
and reconstruction. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition Workshops,
pages 141–150, 2015.

[22] S. Kong, J. Heo, B. Abidi, J. Paik, and M. Abidi. Recent
advances in visual and infrared face recognition – a review.
CVIU, 97(1):103 – 135, 2005.

[23] A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet clas-
siﬁcation with deep convolutional neural networks. In NIPS,
2012.

[24] S. Z. Li, D. Yi, Z. Lei, and S. Liao. The casia nir-vis 2.0 face
database. In Computer Vision and Pattern Recognition Work-
shops (CVPRW), 2013 IEEE Conference on, pages 348–353.
IEEE, 2013.

[25] J. Liu, Y. Deng, and C. Huang. Targeting ultimate accu-
racy: Face recognition via deep embedding. arXiv preprint
arXiv:1506.07310, 2015.

[26] D. Lowe. Distinctive image features from scale-invariant

keypoints. IJCV, 60(2):91–110, 2004.

[27] J. Lu, V. E. Liong, X. Zhou, and J. Zhou. Learning compact
binary face descriptor for face recognition. Pattern Analysis
and Machine Intelligence, IEEE Transactions on, 2015.

[28] D. Miller, E. Brossard, S. M. Seitz, and I. Kemelmacher-
Shlizerman. Megaface: A million faces for recognition at
scale. arXiv preprint arXiv:1505.02108, 2015.

[29] J. Papon and M. Schoeler.

networks trained on synthetic rgb-d.
arXiv:1508.00835, 2015.

Semantic pose using deep
arXiv preprint

[30] O. Parkhi, A. Vedaldi, and A. Zisserman. Deep face recog-

nition. In BMVC, 2015.

[31] O. M. Parkhi, A. Vedaldi, and A. Zisserman. Deep face
In British Machine Vision Conference, 2015.

recognition.

[32] M. Paulin, J. Revaud, Z. Harchaoui, F. Perronnin, and
C. Schmid. Transformation pursuit for image classiﬁcation.
In CVPR, 2014.

[33] M. Paulin, J. Revaud, Z. Harchaoui, F. Perronnin, and
C. Schmid. Transformation pursuit for image classiﬁcation.
In Computer Vision and Pattern Recognition (CVPR), 2014
IEEE Conference on, pages 3646–3653. IEEE, 2014.

[34] P. Pinheiro and R. Collobert. From image-level to pixel-level

labeling with convolutional networks. In CVPR, 2015.

[35] A. Rozantsev, V. Lepetit, and P. Fua. On rendering synthetic
images for training an object detector. Computer Vision and
Image Understanding, 2015.

[54] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed,
D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.
Going deeper with convolutions. In CVPR, 2015.

[55] Y. Taigman, M. Yang, M. Ranzato, and L. Wolf. DeepFace:
Closing the gap to human-level performance in face veriﬁca-
tion. In CVPR, 2014.

[56] M. A. Turk and A. P. Pentland. Face recognition using eigen-
faces. In Computer Vision and Pattern Recognition, pages
586–591, 1991.

[57] K. Weinberger and L. Saul. Distance metric learning for
large margin nearest neighbor classiﬁcation. JMLR, 10:207–
244, 2009.

[58] M. Weinmann, J. Gall, and R. Klein. Material classiﬁcation
based on training data synthesized using a BTF database. In
ECCV, 2014.

[59] X. Xie, W.-S. Zheng, J. Lai, P. C. Yuen, and C. Y. Suen.
Normalization of face illumination based on large-and small-
scale features. IEEE Trans. Image Processing, 20(7):1807–
1821, 2011.

[60] D. Yi, Z. Lei, S. Liao, and S. Li. Learning face representation

from scratch. In Arxiv preprint, 2014.

[61] D. Yi, Z. Lei, S. Liao, and S. Z. Li. Learning face represen-
tation from scratch. arXiv preprint arXiv:1411.7923, 2014.

[62] M. Zeiler and R. Fergus. Visualizing and understanding con-

volutional networks. In ECCV, 2014.

[63] Z. Zhang, P. Luo, C. Loy, and X. Tang. Facial landmark

detection by deep multi-task learning. In ECCV, 2014.

[64] X. Zhu, Z. Lei, J. Yan, D. Yi, and S. Z. Li. High-ﬁdelity
pose and expression normalization for face recognition in the
wild. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pages 787–796, 2015.

[36] A. Rozantsev, V. Lepetit, and P. Fua. On rendering synthetic
images for training an object detector. CVIU, 137:24 – 37,
2015.

[37] J. S´anchez, F. Perronnin, T. Mensink, and J. Verbeek. Image
classiﬁcation with the Fisher vector: Theory and practice.
IJCV, 105(3):222–245, 2013.

[38] F. Schroff, D. Kalenichenko, and J. Philbin. Facenet: A uni-
ﬁed embedding for face recognition and clustering. In CVPR,
2015.

[39] F. Schroff, D. Kalenichenko, and J. Philbin. Facenet: A uni-
ﬁed embedding for face recognition and clustering. arXiv
preprint arXiv:1503.03832, 2015.

[40] J. Shotton, T. Sharp, A. Kipman, A. Fitzgibbon, M. Finoc-
chio, A. Blake, M. Cook, and R. Moore. Real-time human
pose recognition in parts from single depth images. Commu-
nications of the ACM, 56(1):116–124, 2013.

[41] T. Sim, S. Baker, and M. Bsat. The CMU Pose, Illumination,
and Expression (PIE) database. In IEEE International Con-
ference on Automatic Face and Gesture Recognition, 2002.

[42] K. Simonyan, O. Parkhi, A. Vedaldi, and A. Zisserman.

Fisher vector faces in the wild. In BMVC, 2013.

[43] K. Simonyan, O. M. Parkhi, A. Vedaldi, and A. Zisserman.
Fisher Vector Faces in the Wild. In British Machine Vision
Conference, 2013.

[44] K. Simonyan and A. Zisserman. Two-stream convolutional

networks for action recognition in videos. In NIPS, 2014.

[45] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. In ICLR, 2015.
[46] H. Su, C. Qi, Y. Li, and L. Guibas. Render for cnn: View-
point estimation in images using cnns trained with rendered
3d model views. In ICCV, 2015.

[47] Y. Sun, Y. Chen, X. Wang, and X. Tang. Deep learning
face representation by joint identiﬁcation-veriﬁcation.
In
Advances in Neural Information Processing Systems, pages
1988–1996, 2014.

[48] Y. Sun, D. Liang, X. Wang, and X. Tang. Deepid3: Face
recognition with very deep neural networks. arXiv preprint
arXiv:1502.00873, 2015.

[49] Y. Sun, X. Wang, and X. Tang. Hybrid deep learning for face

veriﬁcation. In ICCV, 2013.

[50] Y. Sun, X. Wang, and X. Tang. Hybrid deep learning for
In Computer Vision (ICCV), 2013 IEEE
face veriﬁcation.
International Conference on, pages 1489–1496. IEEE, 2013.

[51] Y. Sun, X. Wang, and X. Tang. Deep learning face represen-
In Computer Vision
tation from predicting 10,000 classes.
and Pattern Recognition (CVPR), 2014 IEEE Conference on,
pages 1891–1898. IEEE, 2014.

[52] Y. Sun, X. Wang, and X. Tang. Deeply learned face repre-
sentations are sparse, selective, and robust. arXiv preprint
arXiv:1412.1265, 2014.

[53] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed,
D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabi-
novich. Going deeper with convolutions. arXiv preprint
arXiv:1409.4842, 2014.

