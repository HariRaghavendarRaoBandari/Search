You Really Need A Good Ruler to Measure Caching

Performance in Information-Centric Networks

Liang Wang, Jussi Kangasharju∗, Arjuna Sathiaseelan, Jon Crowcroft

University of Cambridge, UK

University of Helsinki, Finland∗

6
1
0
2

 
r
a

 

M
8
1

 
 
]
I

N
.
s
c
[
 
 

1
v
3
6
9
5
0

.

3
0
6
1
:
v
i
X
r
a

ABSTRACT
Information-centric networks are an interesting new paradigm
for distributing content on the Internet. They bring up
many research challenges, such as addressing content by
name, securing content, and wide-spread caching of content.
Caching has caught a lot of attention in the research commu-
nity, but a lot of the work suﬀers from a poor understanding
of the diﬀerent metrics with which caching performance can
be measured. In this paper we not only present a compre-
hensive overview of diﬀerent caching metrics that have been
proposed for information-centric networks, but also propose
the coupling factor as a new metric to capture the relation-
ship between content popularity and network topology. As
we show, many commonly used metrics have several failure
modes which are largely ignored in literature. We identify
these problems and propose remedies and new metrics to
address these failures. Our work highlights the fundamen-
tal diﬀerences between information-centric caches and “tra-
ditional” cache networks and we demonstrate the need for
a systematic understanding of the metrics for information-
centric caching. We also discuss how experimental work
should be done when evaluating networks of caches.

1.

INTRODUCTION

Information-centric networking (ICN) provides a new paradigm

for addressing and accessing content on the Internet. The
current Internet was developed as a host-centric network,
where the main focus was on interconnecting computers,
or hosts, but the modern usage of Internet is very much
information-centric, i.e., users do not care from where the
information they want to access comes from; they simply
are interested in getting the information. The web is in
its essence a host-centric system, although content delivery
networks (CDN) and technologies do break the dependence
on (particular) hosts serving speciﬁc content to some ex-
tent. However, fundamentally the web is still a host-centric
system and its diﬀerent components, such as naming and
security, are tied to this host-centric world.

ICN puts the focus on the content as opposed to the hosts
to address the architectural issues preventing the web from
becoming a full-blown information-centric system. There
are several independent proposals around ICN [1–4]. They
each present a diﬀerent solution to try to re-construct the
current Internet, and build a new architecture around the
notion of content. While the details in the proposal diﬀer, we
can identify three common components that are fundamen-
tal to information-centric networking: addressing content by
name, securing content, and wide-spread caching.

Of these three, the last one, wide-spread caching, seems
to have attracted the most attention in the research world
lately. Our focus in this paper is on measuring the eﬀec-
tiveness of caching, but ﬁrst we outline the main issues in
naming and security, highlighting in particular how they im-
pact caching.

Addressing content by name is an important change to
how content is addressed in the web. Although URLs are
“names” of content, they have internal structure which indi-
cates the server hosting that content as well as a local “path”
on the server to the content. While at ﬁrst sight similar,
names in ICN may have structure, but the structure does
not identify a particular server in the network that would
need to be accessed to retrieve the content.

Content discovery is a big challenge in ICN and two dif-
ferent choices seem to emerge from the ICN proposals. One
possibility is to use an indexing service [3] which keeps track
of copies of objects; however it is not clear if this will scale
up to a global scale. The other possibility, used in most
of the other ICN proposals, is to route requests based on
some components in the content name, with the hope that
this routing converges on a copy of the object. Especially
in the latter case (sometimes combined with well controlled
scoped-ﬂooding [14]), en-route caching becomes an attrac-
tive option to speed up discovery and spread the load on con-
tent distribution. In this paper, we mainly follow this kind
of a model and assume that content requests are routed in
the same way from all over the network and that the routing
converges towards existing copies of the content. Caching
is assumed to happen en-route and cached copies are not
tracked in any way.

Security on the web is essentially based on identifying the
server providing the content via SSL and its associated cer-
tiﬁcates. Since content no longer has a single origin in ICN,
this approach does not work anymore. Instead, the ICN ap-
proaches all focus on securing the content, by ensuring via
signatures and public keys that the content has not been
tampered with in the network. This also allows caching to
take place since any piece of content, no matter from which
server it is served from, can be authenticated to be the same
content that the originator put in the network. Obviously,
this does not tie the content to a real world entity, but this
can be achieved in a similar way to how it is done on the
web.

Finally, wide-spread caching is used to store the content
in the network and allow for faster delivery. Caching also
reduces traﬃc in the network and is therefore attractive for
network operators since it has the potential to reduce their

costs.

Although all three above factors are fundamental to ICN,
caching seems to have attracted the most attention in recent
research. Caching is a topic that has been researched in
many diﬀerent contexts and it is attractive in the sense that
it can be measured quantitatively with relative ease, whereas
eﬀectiveness of naming schemes or security solutions tend
towards more qualitative measurements. However, a lot of
the work on caching in ICN uses the “old” caching metrics
that are known from processors or web caches. As we discuss
in this paper, ICN is a network of caches and there are
fundamental diﬀerences between ICN caching and, e.g., web
caching. Web caches can also be organised in networks,
however they work in a fundamentally diﬀerent way from
the network of caches in ICN.

Our key contributions in this paper are to highlight the
fundamental diﬀerences between diﬀerent caching metrics
and show how they impact the metrics that should be used
to measure the eﬀectiveness of caching. We speciﬁcally pro-
pose using the coupling factor as a new ICN metric to cap-
ture the relationship between content popularity and net-
work topology. We present several metrics and show how
they vary in their complexity and expressiveness by using a
dimensionality notion to illustrate where hits occur in a net-
work. The goal of this paper is to demonstrate that caching
in ICN is a novel area of research and that existing measure-
ment solutions have only limited applicability in this ﬁeld.

2. SYSTEM VIEW

In this paper we focus on CCN-like [1] ICN where re-
quests for (pieces of) content are forwarded via routers and
these routers are equipped with a cache where they can store
content. We focus on the case of a single ISP, as shown in
Figure 1, which depicts several clients, one server, and a net-
work of routers. Some of the routers are connected towards
clients and some towards servers. We do not distinguish
whether these are actual clients or other ISPs that are con-
nected to the clients; they represent the incoming requests.
Likewise, the servers represent sources of content and do not
necessarily need to be connected to this particular ISP. From
the ISP’s point of view, the traﬃc reduction that caching
brings can have two goals. Firstly, it reduces traﬃc towards
the servers (inter-ISP traﬃc) which typically has a (direct)
ﬁnancial cost for the ISP. Secondly, it reduces traﬃc within
the ISP’s own network (intra-ISP traﬃc). Intra-ISP traﬃc
has no direct connection to the ISP’s costs, however lower
intra-ISP traﬃc means that the ISP is able to serve more
customers with the same infrastructure, which does have a
positive ﬁnancial eﬀect.

The fundamental diﬀerence between ICN caching and, for
example, web caching is that in ICN caching we have a
network of caches that can work together to optimise the
performance of the whole network. Although hierarchical
web caching implemented similar networks of caches, each
cache was operated by a diﬀerent real-world entity and at-
tempted to optimise its own performance. This led to issues
like the ﬁltering eﬀect [5, 6] where ﬁrst caches in the hierar-
chy capture the most popular objects because they attempt
to optimise their own performance. This in turn leads to
the following caches to see a request stream with less lo-
cality, making their performance suﬀer.
In the context of
web caching where every cache is operated by a diﬀerent
entity, this is reasonable, but in the context of ICN, a single

Figure 1: Model of the network

entity controls multiple caches and is able to make them co-
operate to optimise the overall performance. Results in [9]
show that even a simple randomization of where to cache a
particular piece of content has a signiﬁcant boost of over-
all performance because it mitigates the ﬁltering eﬀect. As
example, consider Figure 1. In the web caching model, the
routers next to the clients would cache the most popular
content, but in ICN, it is feasible to have some other routers
do that, for example the green routers marked with X. In
other words, [9] shows that caches being less greedy in op-
timising their own performance is beneﬁcial to the whole
system. This implies that when evaluating networks of ICN
caches, we need to look at the performance of the whole net-
work instead of optimising performance of individual caches.

3. METRICS

We now present the main contribution of the paper and
outline diﬀerent metrics that can be used to measure per-
formance of a network of caches. We consider three metrics
that have been used in literature and present a new metric
called coupling factor.
3.1

(Byte) Hit Rate

Typically cache performance has been measured via hit
rate, which captures the ratio between cache hits (requests
found in the cache) to the total number of requests seen by
the cache. Byte hit rate is its natural extension where every
hit is weighted by the size of the object, hence byte hit rate
measures the reduction in outgoing traﬃc from the cache.
As our focus is on traﬃc reduction, we use byte hit rate in
the following. When we apply byte hit rate as the perfor-
mance metric, we eﬀectively aggregate the whole network of
caches as a single cache and look at its performance. (Note
that since multiple caches may hold a copy of the same ob-
ject in ICN, such an “aggregate” cache has less storage than
the individual caches together; this does not inﬂuence the
metric.) Byte hit rate is location-agnostic since it only cares
whether there was a hit in any cache; it does not provide
any information about where the hit happened.

Byte hit rate is an often-used metric, partly because caches
have traditionally been measured by hit rate, partly because
it is easy to compute, and partly because it translates di-
rectly to savings in inter-ISP traﬃc, i.e., ﬁnancial savings.
Reducing duplicate copies of objects in the network is the
most eﬀective way of improving byte hit rate; however an ef-
ﬁcient reduction in number of copies requires an eﬃcient co-
operation method between the caches to discover the cached

ServerClient AClient BClient CXX(a) 1-dimension metric, BHR only tells
you that how many hits happen in the
whole network. The whole network is
treated as a single entity.

(b) 2-dimension metric, FPR tells you
both how many hits and where they
happen along a path (source ↔ desti-
nation) on average in the network.

(c) 3-dimension metric, CPF tells you
how many hits and where they happen
on average in the network (e.g., net-
work edge or network core).

Figure 2: An illustration of the information contained in three measurement metrics: (Byte) Hit Rate (BHR), Footprint
Reduction (FPR), and Coupling Factor (CPF). The calculation becomes more complicated as the information increases.

copies. Typical examples are various distribute key-value
stores [7, 8] which is able to combine distributed caches into
a big logical cache via collaboration.

While byte hit rate is easy to compute, it treats the net-
work of ICN caches as a black box since it does not take into
account where the hit happens. Another argument against
byte hit rate as a metric stems from the current trends of
content distribution in the Internet. Large content delivery
networks or content providers, who host the most popular
content, install their servers in or close to the ISPs where
the users are. Although the servers are in the ISP’s network,
the normal way of calculating byte hit rate would consider
them external, thus traﬃc to them would be counted the
same way as any outgoing traﬃc; yet there is typically no
cost to the ISP for traﬃc to them. Hence, most of the pop-
ular content actually comes from inside the ISP, and only
the savings in the less popular content are relevant for the
ISP. Byte hit rate is not able to capture this and therefore
we recommend that it should not be used as a general met-
ric; in speciﬁc situations it may be appropriate, but it is not
appropriate as a general metric for all situations.
3.2 Average Hops

Measuring the number of hops a request needs to traverse
in order to ﬁnd the content is also a metric that has been
recently used [10]. It is appealing in the sense that it aug-
ments byte hit rate by taking into account where the hit
happens, however it does not provide any meaningful way
of estimating savings in outgoing traﬃc. In addition, as is
done in [10], average hops is sometimes used as a proxy for
download latency.

Our previous work [9] shows that average hops as a metric
does not discriminate well, i.e., while the qualitative rank-
ing of caching solutions is correct, the quantitative diﬀer-
ences between them are very small, which can easily lead
to an impression that the performance diﬀerences would be
small [10]. Other metrics we consider in this paper do not
share this weakness. The reason behind this is that average
hops measures absolute values and because many networks
are scale-free, the number of hops is typically small. Hence,
diﬀerences between caching strategies will appear small, but
this is actually an artifact of the metric, not an indicator
that the strategies would be close to each other according
to other metrics.

Another diﬃculty in using average hops as a metric relates

to what value to assign to content retrieved from outside the
ISP, i.e., a miss. Assigning a high value puts emphasis on
avoiding misses, i.e., the metric becomes similar to hit rate.
Assigning a low value emphasizes the location of content in
the ISP’s network, i.e., it gives an impression of intra-ISP
traﬃc. However, as the amount of data is not part of the
metric, it does a poor job in capturing something useful and
a better metric, like footprint reduction described below, is
needed.
3.3 Footprint Reduction

Traﬃc footprint in traﬃc engineering is deﬁned as a prod-
uct of traﬃc volume and the distance it travels within the
network. The distance is usually measured in terms of hops.
To calculate the footprint, we need to sum up all the prod-
ucts of every data packet sizes and their travel distances.
Footprint reduction is the fraction of reduction in footprint
when caching is enabled (comparing to “no caching” case).
Although we choose “no caching” as the baseline to calculate
the footprint reduction, we must point out that the rank-
ing of diﬀerent strategies in an evaluation is irrelevant to the
choice of the baseline strategy. The corresponding proof can
be found in Appendix.

Compared to byte hit rate, footprint reduction takes into
account where the hit happens, since the number of hops is
counted in the metric. However, since footprint reduction
uses the size of the content, it gives more accurate informa-
tion about traﬃc reduction than simply using the average
hops. Also, it measures relative change and gives therefore
a better picture of the diﬀerences between caching strate-
gies. Note that footprint reduction measures only reduction
of intra-ISP traﬃc and does not give any indication about
possible reductions in inter-ISP traﬃc.

Byte hit rate and footprint reduction are the two key met-
rics in evaluating performance of networks of ICN caches,
but they must be used in conjunction; using only one of
them leads to biased results (using only average hops will
lead to even more bias). For example, consider two caching
strategies which achieve the same byte hit rate, but diﬀerent
footprint reductions. Higher footprint reduction indicates
that the hits happen closer to the clients, thus less intra-
ISP traﬃc and generally better performance for the users.
We have identiﬁed and quantiﬁed the tradeoﬀ between byte
hit rate and footprint reduction [11] and will brieﬂy outline
this tradeoﬀ below.

(cid:38)(cid:51)(cid:41)(cid:41)(cid:51)(cid:53)(cid:37)(cid:43)(cid:53)(cid:38)(cid:51)(cid:41)(cid:41)(cid:51)(cid:53)(cid:37)(cid:43)(cid:53)(cid:38)(cid:51)(cid:41)(cid:41)(cid:51)(cid:53)(cid:37)(cid:43)(cid:53)A naive solution for improving footprint reduction is to
place the popular content as close to the clients as possible,
i.e, edge caching [10]. However, this leads to large redun-
dancy in cached content and leads to (much) lower byte hit
rate. This tradeoﬀ between the two metrics is mediated by a
cooperation policy which enables richer cooperation between
the clients than a simple en-route caching allows. As dis-
cussed in [11], the tradeoﬀ can be mediated by adjusting
the number of copies for a content item and the range of
how widely we search for the content in the network in case
of a miss. The search range covers all possible cases from
en-route caching to searching the whole network (obviously
with a cost that would need to be accounted for). Adjust-
ing the number of copies is harder to do exactly, but simple
mechanisms like Cachedbit [9] are likely to be suﬃcient in
many cases. For more details, we refer the reader to [9, 11].
3.4 Coupling Factor

We propose a new metric in this paper: the coupling fac-
tor, to capture the eﬀects of the network topology on the
performance of caching. We achieve this by identifying the
“position” in the network where the hit happens. Recall that
byte hit rate does not give any information about where the
hit happens, and footprint reduction is limited to ﬁnding
content only along the routing path. A cooperation policy
that searches wider in the network is able to ﬁnd content
in other locations as well. In this case, the position has a
direct impact on the calculation of the metrics.

We deﬁne the coupling factor as a function of content pop-
ularity and network topology and it measures the impact
of topology on content placement, and thus the impact on
metrics like byte hit rate and footprint reduction. Content
popularity is easy to obtain, but for characterising topology,
we have many more options, such as degree centrality, be-
tweenness centrality, closeness centrality and so on. There-
fore, coupling factor can have several forms depending on
which metrics are used in calculating the correlation, but
the general idea is the same: we need a way of showing
the relationship between popularity and topology. For ex-
ample, [11] speciﬁcally chooses betweenness centrality and
Pearson correlation in the calculation.

Figure 3 shows how diﬀerent degrees of coupling aﬀect
the placement of the most popular content. The red dots
represent the most popular content and the concentric cir-
cles group nodes according to their betweenness centrality.
Strong coupling means that the most popular content is
placed in the nodes with high betweenness, i.e., the network
core. Weak coupling means the opposite, i.e., the popular
content is placed at the network edge. (Strictly speaking, if
using correlation between popularity and node degree as a
metric, strong coupling is indicative of strong positive cor-
relation and weak coupling implies strong negative correla-
tion.) By adjusting the range of the search and the number
of copies, we can inﬂuence the placement of content in the
system, i.e., adjust the degree of coupling. When the popu-
lar content is in the core, we improve byte hit rate and when
the popular content is near the edge, we favor footprint re-
duction. This means that the two parameters, search range
and number of copies, can be used to adjust the tradeoﬀ
between the two metrics.

4. OTHER USEFUL METRICS

The metrics we presented in Section 3 are by no means

the only metrics that can be used for measuring cache per-
formance.

As is well known, the popularity distribution signiﬁcantly
inﬂuences caching performance in all kinds of caches. How-
ever, new content is constantly added and popularity of con-
tent changes which may inﬂuence the metrics that are being
used to measure the performance of caches. Typically, some
kind of aging is used to rid the system of old popularity
information and test a caching strategy’s ability to adapt
to changes in content popularity. Since popularity typically
follows a power law and is characterised by a (single) pa-
rameter, it is a common technique to measure the eﬀects of
changes in that parameter on other system metrics.

Another interesting measure would be to ﬁnd a way to
quantify the ﬁltering eﬀects. Filtering eﬀect has been no-
ticed in hierarchical cache systems [6], and it refers to the
phenomenon that the popularity of the most popular part in
a miss stream is “ﬂattened” because the downstream router
ﬁlters out the most popular content. Filtering eﬀect de-
grades the caching performance of upstream routers. We do
not yet have a good metric to quantify the ﬁltering eﬀect,
however results in [9] seem to indicate that simple solutions
might be able to ﬁght the ﬁltering eﬀect, thus obviating the
need for its measurement. However, this is a question for
future research.

As mentioned, content popularity changes dynamically,
but also the network topology changes, be it due to failures
or simply an evolution of the infrastructure. This question
needs more research in order to evaluate how often these
kinds of topology changes aﬀect an information-centric cache
network and how large the impact would be.

5. BEYOND THE METRICS: HOW TO RUN

EXPERIMENTS

Choosing the right metrics is the ﬁrst step, but it is not
enough. Designing experiments is as critical as selecting the
right measurements, since it directly inﬂuences the ability
to extract the right information from the experiment, as
needed by the metrics that are to be computed. A poorly
designed experiment will not allow the correct information
to be extracted, leading to possibly erroneous conclusions.
In terms of ICN experiments, there are three key elements
in experiment design: content, topology, and traﬃc model.

Content popularity is a key factor aﬀecting performance of
caching systems. In the absence of publicly available request
traces from recent years, many researchers are forced to use
synthetic request traces. For synthetic traces, it is impor-
tant that its characteristics match those of realistic traces as
closely as possible; obviously it will not be an exact match,
but basic statistical characteristics should match real traces.
Research has shown that real world traces can often be mod-
eled by either Zipf of Weibull distributions [12]. A further
aspect of modeling the content is the size of the content set,
i.e., how many objects are included in the synthetic trace;
a real trace has its own ﬁxed number of objects. The rea-
son why the content set size is important is the “heavy tail”
nature of these popularity models which drags down per-
formance is the content set is too large (for the amount of
cache storage in the system).

Topology of the network is also of high importance in
a correct experimental setting. Research has showed that
most realistic networks,
including the Internet are scale-

(a) Strong coupling

(b) Moderate coupling

(c) Weak coupling

Figure 3: Coupling between content popularity and network topology. The nodes are grouped with three concentric circles
according to their betweenness centrality values CB. Red color represents where the most popular content reside.

free, so a Barab´asi-Albert model could be used to gener-
ate synthetic network topologies. However, as work in [13]
shows, real ISP router topologies do not always conform to
the scale-free model, so it is also important to experiment
with real topologies. As opposed to request traces, network
topologies, both at ISP and AS level, are readily available,
so unless the experiment setting requires a network much
larger than exists in traces, using a real network topology
is preferable to synthetic topologies. Obviously, work needs
to consider many diﬀerent topologies in order to eliminate
speciﬁcs of particular topologies from the results.

The third element, traﬃc, has two components: how much
traﬃc and between which points it goes? In the real world,
requests from users at the network edge and traﬃc volume is
typically proportional to the population of an edge routers
geographical location. These observations have led to the
so-called “gravity model” which has been used in many pa-
pers. Typically servers are connected to routers with high
degree, i.e., near the core of the network and clients are
connected to edge routers. The question then becomes how
many clients should the network have? If all routers are as-
signed as servers or clients, there is no “intermediate layer”
of caches and this may lead to a stronger ﬁltering eﬀect
which degrades the performance of the system. Based on our
experience, we have observed that placing a small number
of servers according to the gravity model and then assigning
20–30% of the edge routers as clients works well. In addi-
tion, the placement of clients should be randomized, varied
from one experiment run to another, and we should perform
a suﬃcient number of repetitions to guarantee statistically
meaningful results, using for example conﬁdence intervals to
determine the number of repetitions.

6. LESSONS LEARNED

We now summarize the key ﬁndings and observations in

the paper.

• We need to evaluate the performance of the network
as a whole as opposed to optimising individual caches.
• Byte hit rate measures only reduction in inter-ISP traf-
ﬁc and due to content providers installing their servers

close to clients may not reﬂect a true reduction in out-
going (costly) traﬃc.

• Using average hops as a metric is not a good idea since
it does not discriminate well and has issues related to
assigning the number of hops for cache misses.

• Footprint reduction measures reduction in intra-ISP
traﬃc and gives more information about where the
hits happen. It is to be preferred over byte hit rate
and average hops.

• A more reﬁned metric, like the coupling factor, can
help characterise the performance by looking at sev-
eral aspects (content popularity and topology in this
case) and provide insight into the inherent behavior of
a network of caches.

• Experiment setup needs careful thinking and control

in order to get meaningful results.

7. CONCLUSION

In this article, we have argued that measuring the per-
formance of caching in information-centric networks is fun-
damentally diﬀerent from previous networks of caches, like
web caching hierarchies. We have discussed diﬀerent metrics
and showed common mistakes in use of metrics in caching
in information-centric networks. In addition, we have high-
lighted some lesser-known metrics which we consider more
appropriate, and have proposed new metrics that capture
more ﬁne-grained information about the performance of an
ICN caching network. Finally, we have considered how ICN
caching experiments should be set up and discussed key el-
ements of experimental work.

APPENDIX

Theorem 1. In an evaluation, the ranking of caching
strategies using footprint reduction metric is irrelevant to
the choice of baseline strategy.

Proof. By deﬁnition, “no caching” strategy is selected
as a baseline in calculating footprint reduction metric. In

0.2<CB≤1.00.1<CB≤0.20.0<CB≤0.10.2<CB≤1.00.1<CB≤0.20.0<CB≤0.10.2<CB≤1.00.1<CB≤0.20.0<CB≤0.10.2<CB≤1.00.1<CB≤0.20.0<CB≤0.10.2<CB≤1.00.1<CB≤0.20.0<CB≤0.10.2<CB≤1.00.1<CB≤0.20.0<CB≤0.1[2] T. Koponen, M. Chawla, B.-G. Chun, A. Ermolinskiy,
K. H. Kim, S. Shenker, and I. Stoica, “A data-oriented
(and beyond) network architecture,” SIGCOMM
Comput. Commun. Rev., vol. 37, no. 4, pp. 181–192,
2007.

[3] C. Dannewitz, “Netinf: An information-centric design

for the future internet,” in Proc. 3rd GI/ITG KuVS
Workshop on The Future Internet, 2009.

[4] Publish/Subscribe Internet Routing Paradigm,

“Conceptual architecture of psirp including
subcomponent descriptions. Deliverable d2.2, PSIRP
project,” , August 2008.

[5] A. Wolman, M. Voelker, N. Sharma, N. Cardwell,

A. Karlin, and H. M. Levy, “On the scale and
performance of cooperative web proxy caching,” in
Proceedings of ACM SOSP, 1999.

[6] C. Williamson, “On ﬁlter eﬀects in web caching

hierarchies,” ACM Trans. Internet Technol., vol. 2,
no. 1, pp. 47–77, Feb. 2002.

[7] S. Ratnasamy, P. Francis, M. Handley, R. Karp, and

S. Shenker. ”A scalable content-addressable network”.
In SIGCOMM ’01. ACM, 2001.

[8] P. Maymounkov, and D. Mazieres. ”Kademlia: A
peer-to-peer information system based on the xor
metric”. In Peer-to-Peer Systems. Springer Berlin
Heidelberg, 2002.

[9] W. Wong, L. Wang, and J. Kangasharju,

“Neighborhood search and admission control in
cooperative caching networks,” IEEE GLOBECOM,
2012.

[10] S. K. Fayazbakhsh, A. T. Yin Lin, T. K. Ali Ghodsi,

V. S. Bruce M. Maggs, K. C. Ng, and S. Shenker,
“Less pain, most of the gain: Incrementally deployable
icn,” in Proceedings of ACM SIGCOMM, 2013.

[11] L. Wang, S. Bayhan, and J. Kangasharju, “Eﬀects of

cooperation policy and network topology on
performance of in-network caching,” submitted, 2013.
Available: http://www.cs.helsinki.ﬁ/u/jakangas/
Papers/cooperation.pdf

[12] M. Cha, H. Kwak, P. Rodriguez, Y.-Y. Ahn, and

S. Moon, “I tube, you tube, everybody tubes:
analyzing the world’s largest user generated content
video system,” in Proceedings of ACM IMC, 2007.

[13] N. Spring, R. Mahajan, and D. Wetherall, “Measuring

ISP topologies with rocketfuel,” in Proceedings of
ACM SIGCOMM, 2002.

[14] L. Wang, S. Bayhan, J. Ott, J. Kangasharju,
A. Sathiaseelan, J. Crowcroft, “Pro-Diluvian:
Understanding Scoped-Flooding for Content Discovery
in Information-Centric Networking” in ICN, 2015.

other words, it represents the traﬃc saving comparing to
a special caching strategy in which all the caches are dis-
abled. Doubts have been raised that if we change to another
caching strategy as baseline, whether the results (or obser-
vation) will remain valid. Below, we give a simple proof to
show it actually does not matter which strategy we choose
as a baseline as the theorem above states, the results are
aﬃne.

Let xα, xβ and xθ denote the traﬃc footprint for caching
strategy α, caching strategy β, and no caching θ respectively,
and yα, yβ, and yθ are the corresponding footprint reduc-
tion. Our current way of calculating footprint reduction is
deﬁned as:

yα = 1 − xα
xθ
yβ = 1 − xβ
xθ
yθ = 1 − xθ
xθ

(1)

(2)

(3)

= 0

(baseline)

As we can see, xθ is just the baseline we are comparing
to. Obviously, the footprint reduction yθ is zero when com-
paring against itself. We can of course change the baseline
to the caching strategy β’s footprint xβ, then we have the
corresponding new metrics y(cid:48)
θ calculated as be-
low:

β and y(cid:48)

α, y(cid:48)

= 0

(baseline)

(4)

(5)

α = 1 − xα
(cid:48)
y
xβ
β = 1 − xβ
(cid:48)
y
xβ
θ = 1 − xθ
(cid:48)
y
xβ

(6)
From Eq (2), we have xβ = xθ(1−yβ). If we let a = 1−yβ
, then Eq (4), (5) and (6) can be rewritten

and b = 1− 1
1−yβ
as follows

α = 1 − xα
(cid:48)
y
xβ
1 − yβ

=

1

= 1 −

(1 − xα
xθ

xα

xθ(1 − yβ)
) + 1 − 1

1 − yβ

(7)

= ayα + b

(8)

y

β = 1 − xβ
(cid:48)
xβ
1 − yβ

=

1

= 1 −

(1 − xβ
xθ

xβ

xθ(1 − yβ)
) + 1 − 1

1 − yβ

(9)

= ayβ + b

(10)

y

θ = 1 − xθ
(cid:48)
xβ
1 − yβ

=

1

= 1 −

(1 − xθ
xθ

xθ

xθ(1 − yβ)
) + 1 − 1

1 − yβ

(11)

(12)

= ayθ + b
α, y(cid:48)

β and y(cid:48)

So we can see that the new metrics (y(cid:48)
θ) are
simply aﬃne transformation of the old ones (yα, yβ and
yθ). It means the footprint reduction is independent of the
choice on which caching strategy as baseline, since it will
not change the “ranking” of the results.
In some sense,
geometrically, it simply means where we want to set our
“origin” point, namely “0” point.
A. REFERENCES
[1] V. Jacobson, D. K. Smetters, J. D. Thornton, M. F.

Plass, N. H. Briggs, and R. L. Braynard, “Networking
named content,” in Proceedings of ACM Conext, 2009.

