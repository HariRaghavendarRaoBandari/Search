6
1
0
2

 
r
a

 

M
9
2

 
 
]

.

C
O
h
t
a
m

[
 
 

2
v
8
9
3
5
0

.

3
0
6
1
:
v
i
X
r
a

A GENERIC LINEAR RATE ACCELERATION OF OPTIMIZATION

ALGORITHMS VIA RELAXATION AND INERTIA

F. IUTZELER AND J. M. HENDRICKX ∗

Abstract. Optimization algorithms can often be seen as ﬁxed-points iterations of some oper-
ators. We develop a general analysis of the linear convergence rate of such iterations and of the
methods to accelerate them.

Acceleration methods using previous iterations can be separated in two main types: relaxation
(simple combination of current and previous iterate) and inertia (slightly more involved modiﬁcation
made popular by Nesterov’s acceleration and heavy balls). These methods have been celebrated for
accelerating linearly and sub-linearly converging algorithms such as gradient methods, proximal
gradient (FISTA), or ADMM (Fast ADMM).

In this paper, we build upon generic contraction properties and aﬃne approximations to i) provide
a coherent joint vision of these methods; ii) analyze the reach and limitations of these modiﬁcations;
and iii) propose generic auto-tuned acceleration methods. The performance of these methods is then
evaluated for various optimization algorithms.

Key words. Applied Optimization Methods, Relaxation, Inertia, Acceleration.

AMS subject classiﬁcations. 47H05, 65K10, 65B99

1. Introduction. A large class of optimization algorithms can be cast as ﬁxed-
point iterations in the sense that they consist in applying the same operation suc-
cessively in order to converge to a ﬁxed point of this operation. For the gradient
algorithm on a diﬀerentiable function f , the operation consists in applying the iden-
tity minus the gradient of f , and the ﬁxed point reached nulls the gradient of f . The
convergence of such ﬁxed-points iterations can thus be proven by ﬁnding a suitable
contraction property, for which the monotone operators provide an attractive frame-
work. They also provide an elegant framework to derive splitting algorithms such
as the Alternating Direction Method of Multipliers (ADMM) [24], or, more recently,
primal-dual algorithms [11], and randomized or distributed optimization algorithms
[17, 39, 34, 18, 8].

In order to accelerate the convergence of ﬁxed point algorithms and in particular
optimization methods, there exists a variety of modiﬁcations based on the construction
of the next iterate by combining the output of the operation with former outputs or
iterates. We focus here on the two main modiﬁcation schemes: relaxation and inertia.

(cid:4) Relaxation combines the output of the operation with the former iterate as

xk+1 = ηT(xk) + (1 − η)xk

where η is some positive parameter. This modiﬁcation notably appears in Richardon’s
method for solving linear systems [32], and in Krasnoselski˘ı–Mann monotone opera-
tors convergence theorem. For the gradient algorithm, relaxation amounts to modi-
fying the step-size. For ADMM, the beneﬁts of relaxation are often reduced to the
phrase “experiments [...] suggest that over-relaxation with η ∈ [1.5, 1.8] can improve
convergence.” (see [12] and [9, Chap. 3.4.3]) except in speciﬁc cases [14].

(cid:4) Inertia on the other side is performed by combining the output of the operation

∗F.I. is with LJK, Universit´e Grenoble Alpes, Grenoble, France. J.H. is with ICTEAM, Universit´e
Catholique de Louvain, Louvain-la-Neuve, Belgium. This project was conducted while F.I. was a
post-doctoral researcher at UCL and is supported by the Belgian Network DYSCO, funded by the
Belgian government and the Concerted Research Action (ARC) of the French Community of Belgium.

1

with the former output. An inertial iteration for operator T writes

(cid:26) xk+1 = T(yk)

yk+1 = xk + γ(xk − xk−1)

⇔ xk+1 = T (xk + γ(xk − xk−1))

where γ is some positive parameter. This modiﬁcation was made immensely pop-
ular by Nesterov’s accelerated gradient algorithm [28]. More recently extensions of
this method to proximal gradient (FISTA [7]) and ADMM (Fast ADMM [15]) were
proposed and quite popular themselves.

However, despite the popularity of these methods, proving the convergence of the
iterates sequence (xk) is still an issue in many situations (see e.g. [10, 4] for the case of
FISTA) and additional restart mechanisms may have to be implemented to improve
the convergence properties [30]. Finally, the key problem when using these methods
is tuning eﬃciently their parameters.
Indeed, “good”, if not optimal, parameters
depend on a variety of elements including the algorithm itself (an optimal parameter
for the gradient may make the ADMM divergent for instance) or function parameters
in the case of optimization (often through the strong convexity constant which may
be hard to estimate [23] or maladjusted to local analysis [36]).

In this paper, our aims are i) to establish and characterize the convergence of
these modiﬁcations and ii) to propose online acceleration methods for a general class
of ﬁxed point algorithms that encompasses the aforementioned optimization methods.
The idea of generic acceleration using inertia was investigated in the sub-linear case in
[22] by sequentially solving well-chosen strongly convex approximations of the original
problem or in [23] which is based on line-search. In order to be as generic as possible,
our approach is based on the monotone operators framework and more precisely on
the averaging contraction property, veriﬁed by a large class of algorithms. We begin
by considering the particular case of aﬃne operators (T(x) = Rx + d where R is
a matrix and d a vector) and study how this property translates into a property
on the spectrum. Finally, this spectral characterization of the convergence speed
makes possible the derivation of optimal parameters and gives us useful guidelines
in the general case. Going back and forth between approximating the investigated
algorithm by an aﬃne operator and using the derived optimal parameters is at the
core of our online acceleration methods.

The contributions of the paper are as follows.
• In Section 3, we provide a general operator formulation for Relaxation, In-
ertia, and Alternated Inertia (applying inertia every other iteration) which
captures the proximity between relaxation and alternated inertia. We also
present general suﬃcient conditions for the convergence of the iterates for the
mentioned schemes.

• In Section 4, we analyze the linear convergence rates of aﬃne operators.
• In Section 5, based on the previous analysis, we derive and prove the con-
vergence of online acceleration methods, for each studied modiﬁcation, that
automatically tunes its parameters online. These algorithms are based on the
operator framework and thus are applicable to a large variety of optimization
algorithms.
• In Section 6, we illustrate the relevance and performance of the algorithms
derived compared to the literature and theory (when available) in the case
of Linear Systems, Gradient algorithms, ADMM, proximal gradient, and a
primal-dual algorithm.

2

2. Fixed-point Algorithm. Let T be a mapping1 on RN . T is said monotone

if ∀x, y ∈ RN , (cid:104)x − y; T(x) − T(y)(cid:105) ≥ 0. For α ∈]0, 1[, T is said α-averaged iﬀ

∀x, y ∈ RN , (cid:107)T(x) − T(y)(cid:107)2 +

1 − α
α

(cid:107)(I − T)(x) − (I − T)(y)(cid:107)2 ≤ (cid:107)x − y(cid:107)2

and T is said to be Firmly Non-Expansive (FNE) if it is 1/2-averaged. The set of the
ﬁxed points of T will be denoted by ﬁxT = {¯x : ¯x = T¯x}. Let us state the convergence
theorem for ﬁxed point iterations of α-averaged operators.

Lemma 1 (Krasnoselskii–Mann algorithm). [6, Prop. 5.15] Let α ∈]0, 1[. Let T
be an α-averaged operator such that ﬁxT (cid:54)= ∅. Then, the sequence (xk)k>0 generated
by x0 ∈ RN and the iterations

converges to a point in ﬁxT.

xk+1 = T(xk)

Remark 1. The iterations produced by averaged operators give Fej´er monotone

iterates sequences (xk)k>0: for any ﬁxed point ¯x and iteration k,

(cid:107)xk+1 − ¯x(cid:107) ≤ (cid:107)xk − ¯x(cid:107) ;

we will investigate this attractive property for the modiﬁcations considered.

If f is convex function, then its subgradient ∂f is monotonous. J = (I + ∂f )−1
is FNE. Furthermore, if its gradient ∇f is L-Lipschitz continuous, G = I − 1/L∇f
is also FNE. Both the ﬁxed points of J and G coincide with the points where ∂f
is null, thus the associated sequences converge to a minimizer of f . Other famous
examples of optimization algorithms coming directly from averaged operators include
the proximal gradient, the ADMM, etc.

3. Relaxation and Inertia. In this section, we describe Relaxation, Inertia,
and Alternated Inertia as modiﬁcations on the classical ﬁxed-point iterations pre-
sented above and exhibit their relations. Notably, we give convergence results for the
iterates, and exhibit the diﬀerences in monotonicity between inertia and relaxation.

3.1. Relaxation. For a positive sequence (ηk), the relaxed iterations follow

(1)

xk+1 = ηkT(xk) + (1 − ηk)xk = T(xk) + (ηk − 1)(T(xk) − xk).

As mentioned in the introduction, this modiﬁcation is present since Richardson’s
iterations and Krasnosel’ski˘ı–Mann algorithm, and over-relaxation (η > 1) is still
investigated to improve convergence speed, notably for ADMM, but remains mostly
experimental. The following convergence result is well known.

Lemma 2. Let α ∈]0, 1[ and let the sequence (ηk) verify 0 < η ≤ ηk ≤ η < 1/α
for all k > 0. Let T be an α-averaged operator such that ﬁxT (cid:54)= ∅. Then, the sequence
(xk)k>0 generated by x0 ∈ RN and the iterations

xk+1 = ηkT(xk) + (1 − ηk)xk

converges to a point in ﬁxT.

The proof is based on the fact that if α ∈]0, 1[ and η ∈]0, 1/α[, then Tη is ηα-
averaged [6, Prop. 4.28]. The convergence thus directly follows from Lemma 1 and
the produced iterates are monotonous in the light of Remark 1.

1For the sake of clarity, we only discuss single-valued mappings in ﬁnite dimensional spaces;

further results on monotone operators theory can be found in [6].

3

3.2. Inertia. Stemming from popular inertial methods [31, 28, 29], acceleration
techniques based on the use of the memory of the previous outputs are very popular
both from a theoretical and a practical point of view (see [38] and references therein
for an overview of these methods). Formally, with T an operator, the core of these
methods consist in performing the following iterations.

(2)

yk+1 = xk+1 + γ(xk+1 − xk)

⇔ xk+1 = T (xk + γ(xk − xk−1))

(cid:26) xk+1 = T(yk)

A careful choice of the sequence (γk)k>0 is known to accelerate the theoretical
functional convergence rate from O(1/k) to O(1/k2) for a large class of algorithms
(see [38, 10, 20] for details) and is very popular in practice.

However, contrary to the relaxation, this modiﬁcation of the algorithm deeply
changes the algorithm behavior as the error between the iterates and some ﬁxed
point is not monotonously decreasing anymore which can cause stability or domain
problems for the iterates. The next lemma provides a general set of conditions for
iterates convergence encompassing several results of the literature [3, 2, 26, 25, 22].
Lemma 3. Let α ∈]0, 1[. Let T be an α-averaged operator such that ﬁxT (cid:54)= ∅.

Assume one the following:

i) ∃γ, 0 ≤ γk ≤ ¯γ < 1 and(cid:80)∞

ii) ∃¯γ < 1, (γk)k>0 is non-decreasing sequence in [0, ¯γ) such that ∀k > 1

k=1 γk(cid:107)xk − xk−1(cid:107)2 < ∞.

1 − γk−1 − (1 − γk)γk − α
1 − α

γk(1 + γk) ≥ m > 0.

iii) as a particular case of ii), when γk = γ for all ∀k > 1, (1−γ)2 > α
Then, the sequence (xk)k>0 generated by x−1 = x0 ∈ RN and the iterations

1−α γ(1+γ).

xk+1 = T(xk + γk(xk − xk−1))

converges to a point in ﬁxT.

3.3. Alternated Inertia. In order to improve the convergence properties of the
iterates of Eq. (2), it was suggested in [27] to apply inertia every other iteration. This
variant is a lot less popular than vanilla inertia. However, its rather good convergence
properties and performances, along with its remarkable closeness with relaxation make
it worthy of careful attention. The iterations of alternated inertia are:

(cid:26) xk+1 = T(xk)

(3)

xk+1 = T(xk + γk(xk − xk−1))

if k is even
if k is odd

Interestingly, using inertia every other iteration can make the error monotonously

decreasing again which will reveal to be interesting numerically.

Lemma 4. Let α ∈]0, 1[. Let T be an α-averaged operator such that ﬁxT (cid:54)= ∅.
for all k > 0. Then, the

Assume that the sequence (γk) veriﬁes 0 ≤ γk ≤ 1−α
sequence (xk)k>0 generated by x0 ∈ RN and the iterations

α

(cid:26) xk+1 = T(xk)

xk+1 = T(xk + γk(xk − xk−1))

if k is even
if k is odd

converges to a point in ﬁxT.

The proof, which generalizes [27] to α-averaged operators, can be found in Apx. A.

4

3.4. Relations between Relaxation and Inertia. Relaxation and inertia are
not often presented altogether. We now provide a general formulation encompassing
the three above mentioned schemes, exhibiting the tight relations between them.

Let us consider the following procedure for all k > 0, k odd:

(cid:26) xk+1 = T1 (xk + νk(xk − xk−1))

xk+2 = T2 (xk+1 + νk+1(xk+1 − xk))

.

The three modiﬁcations presented above can be derived from the above iterations
by choosing carefully the values of the sequence (νk) and operators T1 and T2 as
shown in the table. We notice that, unexpectedly, relaxation and alternated iner-
tia are quite close; this reﬂects both theoretically and practically, notably because
both beneﬁt from monotonous decrease of the error. However, it remains open if
these modiﬁcations could be seen as particular cases of a a general scheme for which
convergence could be proven.

Relaxation

Inertia

Alternated Inertia

T1 = T
T2 = I
T1 = T
T2 = T
T1 = T
T2 = T

νk = 0

νk+1 = ηk/2 − 1

νk = γk

νk+1 = γk+1

νk = 0

νk+1 = γk+1

4. Linear Convergence of Aﬃne Operators. We now give a precise charac-
terization of the spectral signiﬁcation of the averaging property for an aﬃne operator.
This will be useful to investigate the eﬀect of the investigated modiﬁcations and will
lead to our online algorithms.

Results of the literature include analysis of matrices with subdominant eigenval-
ues and applications to alternating projections and Douglas-Rachford splitting [5] or
spectral analysis in the case of the FISTA algorithm [36]. The novelties in this result
include i) proof that the algebraic and geometric multiplicities of eigenvalue 1 coin-
cide which allows the deﬁnition of a proper projection onto the ﬁxed points space (see
Apx. B); ii) the characterization of the practical linear convergence rate based on the
greatest eigenvalue in magnitude, 1 excluded (Theo. 5); and iii) the derivation of the
position of the eigenvalues under the averaging property (Lemma 6). For the sake of
clarity, all the proof details are reported in Apx. B.

T is an aﬃne operator denoted by T = R · +d if it can be written

where R is an N × N real matrix and d is a size-N real vector.

Let us deﬁne the eigenspace of R linked to eigenvalue 1: N (cid:44)(cid:8)x ∈ RN : Rx = x(cid:9) .

T(x) = Rx + d

Importantly, as shown in Apx. B, the averaging property implies that one can deﬁne
a projection ΠN onto N ; and thus the complementary projection ΠN .

Theorem 5. Let α ∈]0, 1[. Let T = R·+d be an α-averaged operator and suppose

that ﬁxT (cid:54)= ∅. Then, the sequence (xk)k>0 generated by x0 ∈ RN and

xk+1 = T(xk)

converges linearly to a point in ﬁxT at a rate

ν (cid:44) max{|λi| : λi (cid:54)= 1 is an eigenvalue of R} < 1

in the sense that ∃¯x ∈ ﬁxT, lim supk

log (cid:107)ΠN (xk−¯x)(cid:107)

k
5

≤ log ν.

i

0

i

1

FNE

0
2
3 -averaged

1

i

0

1

1.5 relaxation

FNE

(a) 1/2-averaged

(b) 2/3-averaged

(c) 1.5 relaxation

Fig. 1: Eigenvalues disks of some α-averaged linear operators

In the following, we will call any eigenvalue λ such that |λ| = ν a dominant eigenvalue
and we will approximate it online vk = (cid:107)xk+1 − xk(cid:107)/(cid:107)xk − xk−1(cid:107) (see Apx. B).

Remark 2. This deﬁnition of the convergence rate diﬀers from [5] (notably Ex-
ample 2.11) as taking the log enables to retrieve directly the principal eigenvalue and
not some ν + ε or knνk; this choice was made in order to match practical rates and
justiﬁes our next analysis.

Lemma 6. Let α ∈]0, 1[ and T (cid:44) R · +d be an α-averaged aﬃne operator. Then,
every eigenvalue λi of R satisﬁes |λi − (1 − α)| ≤ α. Furthermore, |λi| ≤ 1 with
equality iﬀ λi = 1, so ν < 1.

This lemma shows, if T is α-averaged, the eigenvalues of R are contained in a disk

of center 1 − α and radius α as illustrated by Fig. 1-a,b.

5. Online Acceleration of Linear Rates using Relaxation and Inertia.
In this Section, we provide practical acceleration algorithms for ﬁxed point iterations
of general averaged operators using relaxation and inertia.

For the three considered modiﬁcations, we will proceed in the same fashion:

1) we derive optimal parameters and rates in the case of real eigenvalues;
2) most importantly, we propose practical online methods for automatically tuning
relaxation/inertia coeﬃcients.

The fact that the parameters of the online methods are based on the real eigen-
value case may appear very limiting at ﬁrst but i) in practice, linear approximation
of averaged operators often have dominant eigenvalues close the real line (real eigen-
values are linked to the cyclic monotonicity property which appears when considering
(sub)-gradients, see [35] and [6, Theo. 22.14]; ii) similar reasoning have been used in
recent proofs of inertial algorithms [13]; iii) we prove the iterates convergence in the
general averaged operator case (not just aﬃne let alone with real eigenvalues) and iv)
our methods work very well in practice as demonstrated in Section 6.

5.1. Online Relaxation.
5.1.1. Optimal parameters for real eigenvalues. Let T = R · +d be an α-
averaged linear operator. Suppose that R has real eigenvalues λi ∈ [1 − 2α, λ] ∪ {1}.
The eigenvalues of Rη = ηR + (1 − η)I have the form µi = ηλi + (1 − η) . The eﬀect
of over-relaxation (for η > 1) is thus the combination of an inﬂation and a translation
as seen in Figure 1-c.
i) When η > 0 is small enough, the dominant eigenvalue of Rη is ηλ + (1 − η) > 0; so

6

that the convergence rate ν will decrease when η increases.
ii) When η < 1/α is big enough, the dominant eigenvalue of Rη will be η(1 − 2α) +
(1 − η) = 1 − 2αη < 0; so that the convergence rate ν will increase when η increases.
Finally, The optimal parameter η(cid:63), which minimizes the rate, corresponds to the case
where the dominant eigenvalues in the two cases are the opposite one of each other:

η(cid:63) =

2

2α + 1 − λ

and optimal rate ν(cid:63) =

2α − 1 + λ
2α + 1 − λ

.

5.1.2. ORM: Online Relaxation Method. Building on this analysis, we wish

to estimate η(cid:63) without having access to the spectrum of R.
i) To do so, we estimate the current convergence rate as2
vk = (ηk(cid:107)xk+1 − xk(cid:107))/(ηk+1(cid:107)xk − xk−1(cid:107)).
ii) Using this vk, the current relaxation ηk, and the above formula for ν(cid:63), we can
compute an estimate for dominant eigenvalue λ: λk = (vk + ηk − 1)/ηk.
iii) Using λk and optimal η(cid:63), we take our next relaxation parameter as

ηk+1 =

2

2α + 1 − λk

=

2ηk

2αηk + 1 − vk

.

This gives the intuition for our Online Relaxation Method (ORM). In order to

prevent convergence issues, we add an ε > 0 in chosen places.

Online Relaxation Method (ORM) for α-averaged operator T :
Initialization: ε ∈]0, 2 min(α; 1 − α)], x0, x1 = Tx0 , η0 = η1 = 1.
At each iteration k ≥ 1:

(2 − ε)ηk

ηk+1 =
2αηk + 1 − ηk−1(cid:107)xk−xk−1(cid:107)
ηk(cid:107)xk−1−xk−2(cid:107)
xk+1 = ηk+1Txk + (1 − ηk+1)xk

+

ε
4α

Obviously, ε should be taken small for better performance. The following result
provides convergence guarantees for this method in the general framework of averaged
operators. We will show in Section 6 that ORM practically acceleration a variety of
algorithms.

Theorem 7. Let α ∈]0, 1[. Let T be an α-averaged operator such that ﬁxT (cid:54)= ∅.
Then, the sequence (xk)k>0 generated by the Online Relaxation Method converges to
a point in ﬁxT.

Proof. In order to use Lemma 2 to prove the convergence, let us prove by induction
that for all k ≥ 1, ηk ∈ [ ε
4α ]. It is obviously true for η1 = 1. Let us assume
that ηk ∈ [ ε
First, as T is α-averaged, it writes T = αR + (1− α)I with R a non-expansive operator
and I the identity. Tηk then writes Tηk = αηkR + (1 − αηk)I, thus

α − ε
4α ].

4α , 1

α − ε

4α , 1

(cid:107)xk − xk−1(cid:107) = αηk(cid:107)R(xk−1) − xk−1(cid:107)
= αηk(cid:107)R(xk−1) − R(xk−2) + (1 − αηk−1)(R(xk−2) − xk−2)(cid:107)
≤ αηk(cid:107)xk−1 − xk−2(cid:107) + αηk(1 − αηk−1)(cid:107)R(xk−2) − xk−2(cid:107)
= αηk(cid:107)xk−1 − xk−2(cid:107) + αηk
(cid:107)xk−1 − xk−2(cid:107) =

1 − αηk−1
αηk−1

(cid:107)xk−1 − xk−2(cid:107)

ηk
ηk−1

2Note the extra factor ηk/ηk+1 compared to vk in Sec. 4 to deal with Rηk (cid:54)= Rηk+1 .

7

Thus, we have vk ≤ 1 which makes ηk+1 ≥ ε

4α . Now,

ηk+1 ≤ (2 − ε)ηk

2αηk

+

ε
4α

=

1
α

− ε
2α

+

ε
4α

=

1
α

− ε
4α

thus we have that ηk+1 ∈ [ ε
lies in [ ε

α − ε

4α , 1

α − ε

4α , 1

4α ] and thus veriﬁes the conditions of Lemma 2 for convergence.

4α ]. This means the generated sequence (ηk)k>0

5.2. Online Inertia.

5.2.1. Optimal parameter for real eigenvalues. Let us deﬁne Tγ, the oper-
ator generating (xk+1, xk) from (xk, xk−1) where xk+1 = T(xk + γ(xk − xk−1)). When
T = R · +d, we have

(cid:21)(cid:19)

(cid:18)(cid:20) z1

z2

=

Tγ

(cid:20) (1 + γ)Rz1 − γRz2 + d

(cid:21)

z1

(cid:20) (1 + γ)R −γR
(cid:124)

(cid:123)(cid:122)

0

I

Rγ

(cid:20) z1

z2

(cid:21)
(cid:125)

(cid:21)

+

(cid:20) d
(cid:21)
(cid:124) (cid:123)(cid:122) (cid:125)

0

˜d

.

=

As for relaxation, the eigenvalues of Rγ can be derived from those of from R. However,
one eigenvalue λi of R leads to two eigenvalues for Rγ; they are the roots of

pi(µ) = µ2 − (1 + γ)λiµ + γλi.

The main results are:

i) for negatives eigenvalues λi < 0, the magnitude of µi is

(1 + γ)|λi| +(cid:112)(1 + γ)2λ2

2

i + 4γ|λi|

≥ (1 + γ)|λi|

thus inertia has a negative eﬀect on the negative side of the spectrum. For the sake
of clarity, we will focus on the non-negative eigenvalue case in the following.
ii) for non-negative eigenvalues λi ∈ [0, λ] ∪ {1}, optimal parameter and rate are

(1 − √

1 − λ)2
λ

γ(cid:63) =

and ν(cid:63) = 1 − √

1 − λ.

Notably, we have ν(cid:63) ≥ λ/2 which means the rate with inertia can be up to twice as
good as without.

5.2.2. OIM: Online Inertia Method. An online inertia method can be pro-
posed based on the same principles as ORM. For simplicity, we will consider α-
averaged operators with α ∈ (0, 1/2] to deal only with non-negative eigenvalues.
i) We estimate the current convergence rate related to operator Tγk as

(cid:113) (cid:107)xk−xk−1(cid:107)2+(cid:107)xk−1−xk−2(cid:107)2

(cid:107)xk−1−xk−2(cid:107)2+(cid:107)xk−2−xk−3(cid:107)2 .

vk =
ii) Using vk, current inertia γk, we estimate λ: λk = (vk)2/(γkvk − γk + vk).
as γk+1 = (1 − √
iii) Using λk and the formula for optimal γ(cid:63), we take our next relaxation parameter

1 − λk)2/λk.

These steps are at the core of our Online Relaxation Method (ORM). However,
to the diﬀerence of ORM but similarly to other inertia-based accelerations [15], a
restart mechanism has to be introduced to make sure the algorithm converges. Indeed,
this scheme, which is rather aggressive, often overpasses the theoretical limits of the
convergence results. Thus, in order to maintain convergence, the algorithm must

8

either i) suﬃciently decrease the error (cid:107)xk − yk(cid:107); or ii) set inertial parameter γk to 0
so that classical convergence results apply.

Online Inertia Method (OIM) for α-averaged operator T with α ∈ (0, 1/2] :
Initialization: x0, x1 = Tx0, γ1 = 0, ε > 0.
At each time k ≥ 1:
if k = 0 mod 2 :

(cid:107)xk−1−yk−1(cid:107) ;

(cid:107)xk−1−yk−1(cid:107)
(cid:107)xk−2−yk−2(cid:107)

(cid:17)
(cid:16) (cid:107)xk−yk(cid:107)
(cid:113) (cid:107)xk−xk−1(cid:107)2+(cid:107)xk−1−xk−2(cid:107)2
(cid:17)
(cid:16)
(cid:17)

(cid:107)xk−1−xk−2(cid:107)2+(cid:107)xk−2−xk−3(cid:107)2

(cid:16)

(vk)2

γkvk−γk+vk
0; (1−√

; 1 − ε
1−λk)2
λk

[Acceleration]

ck = max
if ck ≤ 1 − ε

vk =

λk = min

γk+1 = max

(xr, xr−1, yr−1, γr) = (xk, xk−1, yk−1, γk+1)

elseif γr > 0
γr = 0

[Restart]

(xk, xk−1, yk−1, γk+1) = (xr, xr−1, yr−1, 0)

elseif γr = 0

γk+1 = 0

else γk+1 = γk

[No Acceleration]

yk+1 = xk + γk+1(xk − xk−1)
xk+1 = T(yk+1)

Note that even though the computation for a new parameter is done every other
iteration (for better precision), the computational step (the application of T) is per-
formed exactly once per time k.

Theorem 8. Let α ∈]0, 1/2]. Let T be an α-averaged operator such that ﬁxT (cid:54)= ∅.
Then, the sequence (yk)k>0 generated by the Online Inertia Method converges in the
sense that (cid:107)T(yk) − yk(cid:107) → 0. Furthermore, if ﬁxT is reduced to a single point x(cid:63),
xk → x(cid:63).

Proof. The proof follows the same reasoning as [15, Theo. 3]. At each iteration,
one of the following situation happens:
i) the last iteration was beneﬁcial: ck ≤ 1−ε so that (cid:107)xk−yk(cid:107) ≤ (1−ε)(cid:107)xk−1−yk−1(cid:107)
and (cid:107)xk−1 − yk−1(cid:107) ≤ (1 − ε)(cid:107)xk−2 − yk−2(cid:107) ;
ii) a restart is made so that the iterates xk and xk−1 by their previous values (cid:107)xk −
yk(cid:107) = (cid:107)xk−2 − yk−2(cid:107) and (cid:107)xk−1 − yk−1(cid:107) = (cid:107)xk−3 − yk−3(cid:107);
iii) there is no acceleration and non expansiveness gives (cid:107)xk − yk(cid:107) ≤ (cid:107)xk−1 − yk−1(cid:107) ≤
(cid:107)xk−2 − yk−2(cid:107).
To conclude the proof, one has to notice that for all k > 0, (cid:107)xk − yk(cid:107) ≤ (cid:107)xk−1 − yk−1(cid:107)
and (cid:107)xk − yk(cid:107) ≤ (1 − ε)(cid:107)xk−1 − yk−1(cid:107) if i) happens. Now, if there is a ﬁnite number
of beneﬁcial iterations (when i) happens), then after the last one, the algorithm goes
back to the unaccelerated iterations and convergence is ensured by Lemma 1.
If
there is a inﬁnite number of beneﬁcial iterations, introducing variable ιk as ιk = 1 if

9

∞(cid:88)

∞(cid:88)

k(cid:89)

iteration k is beneﬁcial and 0 elsewhere; we have

ιk(cid:107)T(yk) − yk(cid:107) ≤ (cid:107)T(x0) − x0(cid:107)

k=1

k=1

(cid:96)=1

(1 − ε)ι(cid:96) ≤ (cid:107)T(x0) − x0(cid:107)

< ∞

ε

and thus (cid:107)T(yk) − yk(cid:107) → 0. This means that the accumulation points of (yk) are in
ﬁxT. In addition, if it is reduced to a single point, then (yk) converges to it and as
(cid:107)xk − yk(cid:107) → 0, so does (xk).

Remark 3. When the convergence is sublinear, the restart condition based on a
constant ε may be too harsh. Following the convergence proof, one can easily deduce
that ε can be taken as a sequence (ε(cid:96)) provided that 1/ε(cid:96) = o((cid:96)) where (cid:96) is the number
of accelerations. For instance, a typical setting is to keep track of the number of
accelerations (cid:96) and take ε(cid:96) = ε0/

√

(cid:96).

5.3. Online Alternated Inertia.

5.3.1. Optimal parameter for real eigenvalues. Let us deﬁne T.,γ the op-

erator generating xk+2 from xk for k even. When T = R · +d, one has

xk+2 = T (T(xk) + γ(T(xk) − xk)) =(cid:2)(1 + γ)R2 − γR(cid:3)
(cid:125)

(cid:124)

xk + (1 + γ)Rd + d

(cid:123)(cid:122)
i − γλi with λi an eigenvalue of R.

R.,γ

so that the eigenvalues of R.,γ are µi = (1 + γ)λ2

The main results are:

i) for negatives eigenvalues λi < 0, the magnitude of µi is

(1 + γ)|λi|2 + γ|λi|

thus alternated inertia has a negative eﬀect on negative eigenvalues λi ≤ (γ−1)/(γ+1).
ii) for non-negative eigenvalues λi ∈ [0, λ] ∪ {1}, the largest µi in magnitude is linked
to the original dominant eigenvalue λ and intermediate eigenvalues. Taking the worst
case scenario over the unknown (and hard to estimate) intermediate eigenvalues, the
optimal parameter and rate are

√

2 − 1)λ

γ(cid:63) =

2(λ)2 + (

2λ(1 − λ) + 1

2

and ν(cid:63) =

(γ(cid:63))2

4(1 + γ(cid:63))

.

5.3.2. OAIM: Online Alternated Inertia Method. Using the same reason-
ing as for OIM, we are able to obtain a similar algorithm. As before, we omit here
the case where negative eigenvalues appear (α > 1/2) and use a restart mechanism.

Online Alternated Inertia Method (OAIM) for an α-averaged operator T
with α ∈ (0, 1/2] :
Initialization: x0, x1 = Tx0, γ1 = 0, ε > 0.
At each time k ≥ 1:
if t = 0 mod 4 :
ck = max
if ck ≤ 1 − ε

(cid:107)xk−2−xk−3(cid:107)
(cid:107)xk−4−xk−5(cid:107)

(cid:107)xk−2−xk−3(cid:107) ;

[Acceleration]

(cid:17)

(cid:16) (cid:107)xk−xk−1(cid:107)
(cid:18)

(cid:107)xk−xk−2(cid:107)
(cid:107)xk−2−xk−4(cid:107)
√

vk =

λk = min

(cid:19)

γk+

(γk)2+4γkvk+4vk

; 1 − ε

2(γk+1)

10

√
γk+1 = 2(λk)2+(
(x[r:r−4], γr) = (x[k:k−4], γk+1)

2λk(1−λk)+ 1

2−1)λk

2

elseif γr > 0

[Restart]

γr = 0
(x[k:k−4], γk+1) = (x[r:r−4], 0)

[No Acceleration]

elseif γr = 0

γk+1 = 0

if t = 0 mod 2 :

yk+1 = xk + γk+1(xk − xk−1)

else

yk+1 = xk

xk+1 = T(yk+1)

Theorem 9. Let α ∈]0, 1/2]. Let T be an α-averaged operator such that ﬁxT (cid:54)=
∅. Then, the sequence (xk)k>0 generated by the Online Alternated Inertia Method
converges in the sense that (cid:107)T(x2k) − x2k(cid:107) → 0. Furthermore, if ﬁxT is reduced to a
single point x(cid:63), xk → x(cid:63).

Proof. The proof follow the same steps as the proof of Theo. 8.

6. Applications. We now particularize the operator T to diﬀerent values cor-
responding to popular algorithms of the literature. We illustrate the interest of the
modiﬁcations studied and, most importantly, we demonstrate the acceleration pro-
vided by our online methods.

6.1. Aﬃne Iterations. Let us begin by considering the simple aﬃne iteration

xk+1 = Rxk + d

where R is an n × n matrix with real eigenvalues λi such that −1 < λmin ≤ λi ≤
λmax < 1 and d is a size n vector. In addition to its theoretical interest, this structure
can be encountered in methods for solving linear systems [33].
In this setup, relaxation has received a lot of attention and, the optimal relaxation
lies in the intersection between η (cid:55)→ |ηλmin + 1 − η| and η (cid:55)→ |ηλmax + 1 − η| leading
to an optimal parameter (see Fig. 2a for an illustration)

η(cid:63) =

1

1 − λmin+λmax

2

and rate ν(cid:63) =

λmax − λmin

2 − (λmin + λmax)

.

Relaxation actually boils down to Richardson’s iterations when solving linear
systems Ax = b by simply taking R = I − A and d = b [33, Example 4.1]; we observe
that optimal parameters and rates match.

In general, comparison between relaxation and inertia depends on both limit
eigenvalues λmin, λmax (and intermediates ones for alternate inertia). Fig. 2b displays
a 3D plot of the optimal rate obtained by numerical simulations (the lower the better)
when λmin and λmax vary between −1 and 1 along with the modiﬁcation scheme
attaining it. We observe that relaxation is generally better with negative eigenvalues
while inertia is preferable with eigenvalues close to 1 (slow convergence). In the next
Section, we will investigate the particular aﬃne iterations of the gradient algorithm on
quadratic functions; this will demonstrate the need for online algorithms as we propose
and their performance compared to theoretically computable optimal parameters.

11

ν

e
t
a
R

1

λmax
ν(cid:63)

0

0

ν

|ηλmin + 1 − η|
|ηλmax + 1 − η|

1 η(cid:63)

2

Relaxation parameter η

(a) Convergence rate versus the relaxation
parameter along with tradeoﬀ enabling to de-
rive optimal relaxation. (for illustration pur-
poses, we assumed |λmax| ≥ |λmin|)

1

0.5

−1
0

e
t
a
r

l
a
m

i
t
p
O

1

0

1 −1

λmax

0

λmin

(b) Best rate obtained with one of the three
modiﬁcations when the eigenvalues interval
[λmin, λmax] varies. If the best rate is attained
by relaxation, it is displayed in black; by in-
ertia, in red; by alt. inertia, in magenta.

Fig. 2: Eﬀect of studied modiﬁcation on linear iterations

6.2. Gradient algorithms. For a diﬀerentiable convex function f with an L-

Lipschitz gradient ∇f , the standard gradient algorithm writes

(4)
and the related operator is T = I − 1

L∇f .

xk+1 = xk − 1
L

∇f (xk)

Then, Baillon-Haddad theorem tells us that T is 1/2-averaged operator and estab-
lishes close connections between such operators and the averaging property itself (see
[6, Cor. 18.16 and Prop. 4.2]). However, while any operator of the form of T is FNE;
any FNE operator is not a gradient step on some function. To be able to identify an
FNE operator to a gradient descent, one has to call an additional property known as
cyclic monotonicity [6, Theo. 22.14] which translates, for aﬃne operators, to having
real eigenvalues (see [35]). This explains why the following optimal parameters are
tight and in general out of the scope of the convergence theorems of Section 3.

For this illustration, we take quadratic f (x) = 1

2(cid:107)Ax − b(cid:107)2. It is thus L-smooth

with L = λmax(ATA) and µ-strongly convex with µ = λmin(ATA).
In this case,
the iterations are aﬃne and the spectrum of the linear part of T is comprised in the
interval [0, 1 − µ/L]. While L can often be known or upper bounded, µ is in general
unknown so that the optimal parameters cannot be computed hence the need for
automatically tuned scheme such as ours.

6.2.1. Relaxation. The relaxed iteration for T writes

xk+1 = xk − ηk+1
L

∇f (xk)

and thus simply consists in adjusting the step size for the gradient algorithm.
Following Section 5.1, we have the following optimal relaxation parameter

η(cid:63) =

2

1 + µ/L

and rate ν(cid:63) =

1 − µ/L
1 + µ/L

leading to an optimal stepsize of 2/(µ + L) which matches the asymptotic optimal
stepsize in the sense of [37, Sec. 4.1.2].

12

One can also notice that when the gradient algorithm converges sub-linearly, the

ORM makes the stepsize go to 2/L which is also optimal as in [37, Sec. 4.1.1]3.

6.2.2. Inertia. The inertial iteration of T writes

Following Section 5.2, we have the following optimal inertia parameter

(cid:26)
1 −(cid:112)µ/L
1 +(cid:112)µ/L

γ(cid:63) =

yk = xk + γk(xk − xk−1)

xk+1 = yk − 1

L∇f (yk)

and rate ν(cid:63) = 1 −(cid:112)µ/L.

Once again, the obtained parameter and rate matches practical and theoretical

optimal situations as summarized in [30].

Finally, one also notices that if the algorithm is converging sub-linearly, the OIM

makes the acceleration parameter go to 1 like Nesterov’s optimal method [28].

6.2.3. Alternated Inertia. The alternated inertial iteration of T writes

 yk = xk + γk(xk − xk−1)

xk+1 = yk − 1
xk+2 = xk+1 − 1

L∇f (yk)

L∇f (xk+1)

Following Section 5.3, we have the following optimal inertia parameter

√

2(µ/L)2 − (3 +

2)µ/L + 1 +

−2(µ/L)2 + 2µ/L + 1

2

√

2

and rate ν(cid:63) =

(γ(cid:63))2

4(1 + γ(cid:63))

.

γ(cid:63) =

6.2.4. Comparison of the optimal rates. One can notice that the optimal
speed with inertia (alternated or classical) is always faster than with relaxation, the
equality case being when µ/L = 1. Between alternated and classical inertia, the
alternated version is faster for well enough conditioned problems, more precisely when
1 ≥ µ/L ≥ 4/(9 + 4
Also, the optimal parameter for inertia γ(cid:63) is greater than theoretical limit 1/3
as soon as µ/L ≤ 1/4. Similarly, for alternated inertia, optimal γ(cid:63) is greater than 1
when µ/L ≤ (3 − √

2) ≈ 0.273. This can be observed in Fig. 2b.

2)/4 ≈ 0.396.

√

6.2.5. Numerical Illustrations. We performed our simulation on 10× 10 ma-
trices A taken from the matrix normal distribution with mean identity and covariance
Σ = 0.08I, and vector b from the zero-mean unit-variance normal distribution. We
chose to present a realization where the condition number µ/L ≈ 0.33.

In Fig. 3, we compare i) the standard gradient descent (Eq. (4)); ii) the three
modiﬁed iterations above with optimal parameters (in plain lines); iii) our three cor-
responding online methods (in emptied marks); and iv) the algorithm obtained by
applying both relaxation and inertia with the respective optimal parameters (which
fails to converge). We plot a) the iterates error in log scale; b) the simulated and
theoretical convergence rates 1/k log((cid:107)xk − x(cid:63)(cid:107)); and c) the parameters values.

We observe that the studied modiﬁcations allow substantial acceleration as pre-
dicted by the analysis, and that the online methods show remarkable performance
for their computational cost. Interestingly, the online methods exhibit quite stable

3The optimal stepsize when doing K iterations goes to 2/L, staying strictly below, when K → ∞.

13

(cid:107)

(cid:63)
x
−
k
x
(cid:107)

r
o
r
r
e

s
e
t
a
r
e
t
I

)
a

101

10−2

10−5

10−8

10−11

10−14

10−17

0

Original Gradient
Optimal Relaxation
Optimal Inertia
Optimal Alt. Iner.
Opt. Relax. + Opt. Iner.

ORM
OIM
OAIM

5

10

15

20

25

30

35

40

)
(cid:107)

(cid:63)
x
−
k
x
(cid:107)
(
g
o
l

1k

e
t
a
r

e
c
n
e
g
r
e
v
n
o
C
)
b

0

−0.2

−0.4

−0.6

−0.8

−1

0

Original Gradient
Optimal Relaxation
Optimal Inertia
Optimal Alt. Iner.

Theo. rate
Theo. rate
Theo. rate
Theo. rate

ORM
OIM
OAIM

5

10

15

20

25

30

35

40

s
e
u
l
a
v

s
r
e
t
e
m
a
r
a
P
)
c

2

1

0

0

Optimal Relax.
ORM

Optimal Iner.
OIM

Optimal Alt. Iner.
OAIM

5

10

15

20

25

30

35

40

Number of iterations

Fig. 3: Optimal and Online acceleration with relaxation and inertia for the gradient
algorithm on a quadratic function.

parameters which justiﬁes the proposed choice for speed estimation. Finally, one can
also notice that applying optimal relaxation along with optimal inertia leads to a
non-converging algorithm illustrating the fact that the studied modiﬁcations do not
directly add up.

6.3. Alternating Direction Method of Multipliers. Consider the following

optimization problem:

(5)

min
x∈RN

f (x) + g(M x)

with f, g two convex lower semi-continuous functions and M a linear operator. The
Alternating Direction Method of Multipliers (ADMM) addresses this problem by per-
forming the following iterations with free parameter ρ > 0.
ADMM

(cid:40)
(cid:40)

(cid:13)(cid:13)(cid:13)(cid:13)M w − zk +
(cid:13)(cid:13)(cid:13)(cid:13)M xk+1 − w +

λk
ρ

(cid:13)(cid:13)(cid:13)(cid:13)2(cid:41)
(cid:13)(cid:13)(cid:13)(cid:13)2(cid:41)

λk
ρ

ρ
2

ρ
2

xk+1 = argmin

w

f (w) +

g(w) +

zk+1 = argmin
λk+1 = λk + ρ(M xk+1 − zk+1)

w

This algorithm is very popular for a variety of applications from power networks

[1] to machine learning [9].

From an operator point of view, the iterations of ADMM can be seen as updates
on the meta-variable ζk = λk + ρzk of an 1/2-averaged operator TADMM (see [12]
and references therein for details). This meta-variable is central as it aﬀects the way
relaxation and inertia translates for this algorithm.

6.3.1. Relaxation. While it is fairly evident to see that the relaxed version of
the operation writes ζk+1 = ηTADMM (ζk) + (1 − η)ζk, it is slightly more complex to
derive the eﬀect of relaxation on the algorithm variables (xk, zk, λk). Indeed, these
variables are computed by a representation of the meta-variable that is non-linear.

14

Let us call Jz the operation giving zk from ζk, then

(6)

zk+1 = Jz(ζk+1) = Jz(ηTADMM (ζk) + (1 − η)ζk) (cid:54)= ηJz(TADMM (ζk)) + (1 − η)Jz(ζk).

This means that, in general4 relaxation cannot be added directly on top of ADMM
in the sense performing the standard ADMM update then adding a step of the form
zk+1 ← ηzk+1 + (1 − η)zk and λk+1 ← ηλk+1 + (1 − η)λk.

Following the operator vision, the canonical relaxation on the ADMM leads to

the following iterations (derivations can be found in [12]).
Relaxed ADMM

xk+1 = argmin

w

f (w) +

ρ
2

(cid:40)
(cid:40)

(cid:13)(cid:13)(cid:13)(cid:13)2(cid:41)

(cid:13)(cid:13)(cid:13)(cid:13)M w − zk +
(cid:13)(cid:13)(cid:13)(cid:13)ηM xk+1 + (1 − η)zk − w +

λk
ρ

ρ
2

(cid:13)(cid:13)(cid:13)(cid:13)2(cid:41)

λk
ρ

g(w) +

zk+1 = argmin
λk+1 = λk + ρ(ηM xk+1 + (1 − η)zk − zk+1)

w

6.3.2. Inertia. As previously, inertial ADMM cannot be derived simply by
adding inertia on top of the above iterations. Following the operator vision, the
canonical inertial version of the ADMM leads to the following iterations.
Inertial ADMM

(cid:19)(cid:13)(cid:13)(cid:13)(cid:13)2(cid:41)

λk − λk−1

ρ

(cid:40)
(cid:40)

xk+1 = argmin

w

f (w) +

zk+1 = argmin

g(w) +

w

(cid:13)(cid:13)(cid:13)(cid:13)2(cid:41)

(cid:13)(cid:13)(cid:13)(cid:13)M w − zk +
(cid:13)(cid:13)(cid:13)(cid:13)M xk+1 − w +
(cid:18)

λk
ρ

ρ
2

ρ
2

(cid:18)

λk+1 = λk + ρ(M xk+1 − zk+1) + γρ

M (xk+1 − xk) +

λk
ρ

+ γ

M (xk+1 − xk) +

(cid:19)

λk − λk−1

ρ

Remark 4. As for relaxation, if g is either i) the indicator function of a linear
space, or ii) quadratic; inertia can be performed as an outer modiﬁcation. Note
that ADMM + outer inertia with Nesterov-like parameter sequence corresponds to
the algorithm named Fast ADMM studied in [15]. However, this algorithm is not
convergent in the general case, unless a restart scheme is added. Interestingly, for the
convergence proof of Fast ADMM in the strongly convex case, g is assumed quadratic.

6.3.3. Alternated Inertia. Alternated Inertial simply consists in alternating
an iteration of ADMM with an iteration of Inertia ADMM. To the best of our knowl-
edge, this algorithm has never been considered before.

6.3.4. Numerical illustrations. In this section, we compare and demonstrate

the performance of the proposed algorithms on the lasso problem:

min
x∈Rn

1
2

(cid:107)Ax − b(cid:107)2

2 + λ(cid:107)x(cid:107)1.

In our setup, A has m = 600 examples and n = 500 observations taken from
the normal distribution with zero mean and unit variance, the columns of A are then

4If either i) g is the indicator function of a linear space, or ii) when g is quadratic; then the
representation operation Jz of Eq. (6) becomes linear and relaxation can be performed as an outer
modiﬁcation.

15

Original ADMM ORM
OIM
OAIM
Fast ADMM

r
o
r
r
e

l
a
n
o
i
t
c
n
u
F

)
a

10−1

10−3

10−5

10−7

10−9

0

10

20

30

40

50

60

70

80

90

s
e
u
l
a
v

s
r
e
t
e
m
a
r
a
P
)
b

4

3

2

1

0

0

ORM
Fast ADMM

OIM OAIM

10

20

30

40

50

60

70

80

90

Number of iterations

Number of iterations

Fig. 4: Relaxation and inertia on ADMM

scaled to have unit norm. b is generated by i) drawing a sparse vector p ∈ Rn with
250 non-zeros entries taken from the normal distribution with zero mean and unit
variance; ii) then creating b as b = Ap + e where e is a small white noise taken from
the normal distribution with zero mean and standard deviation σ = 0.001.

We implement the ADMM with M = I and f and g respectively equal to the two
terms of the problem. We set the regularization parameter λ to 0.1 and the ADMM
parameter ρ to 0.1. For the online algorithms, the convergence-ensuring ε is set to
10−4 and for Fast ADMM η is set to 1−10−4. In this case, the iterations are non-linear
and no optimal convergence rate or modiﬁcation parameters can be derived.

In Fig. 4, we compare i) the standard ADMM; ii) our three proposed online
methods; and iii) Fast ADMM with restart [15]. We plot a) the functional error
1/2(cid:107)Azk−b(cid:107)2
2 +λ(cid:107)z(cid:63)(cid:107)1) where optimal point z(cid:63) is obtained
by package l1 ls [21]; b) the parameters values.

2 +λ(cid:107)zk(cid:107)1−(1/2(cid:107)Az(cid:63)−b(cid:107)2

We observe that, once again, the proposed online methods show remarkable per-
formance for their computational cost. OIM performs best; however, ORM and
OAIM, contrary to OIM and Fast ADMM show steady parameter sequences, this
can be seen as more monotonous behaviors. Finally, ORM oﬀers a better alternative
to arbitrarily ﬁxed relaxation.

6.4. Other Optimization Algorithms. Our analysis and methods are very
general, they directly adapt to a vast range of optimization methods. Notably, many
primal-dual algorithms were derived using the monotone operators framework [11].

6.4.1. Proximal Gradient Algorithm.

Proximal Gradient algorithm for minx f (x) + g(x), f diﬀ.

(cid:40)

(cid:13)(cid:13)(cid:13)(cid:13)w − xk +

L
2

(cid:13)(cid:13)(cid:13)(cid:13)2(cid:41)

∇f (xk)

1
L

xk+1 = argmin

g(w) +

w

In Fig. 5, we plot the functional error and the parameters for i) classical ISTA;
ii) popular FISTA (ISTA + Nesterov’s acceleration) [7]; iii) our three online methods.
We observe that all proposed algorithm show good behaviors. Inertia-based methods
perform very well: OIM outperforms FISTA and OAIM performs similarly to OIM
without the use of restart; this illustrates the strong potential of alternated inertia.

6.4.2. a Primal Dual Algorithm. We investigate the primal-dual algorithm
3.1 from [11] with F = 0. We are able to implement the algorithm so that, contrary
to the ADMM, no matrix inversion is performed, with M = A, g(·) = 1/2(cid:107)·−b(cid:107)2 and
f (·) = λ(cid:107) · (cid:107)1. We chose τ = 0.5 and σ = 1/(τ(cid:107)A(cid:107)2) as prescribed.

16

1
(cid:107)

k
z
(cid:107)
λ
+
22
(cid:107)
b
−
k
z
A
(cid:107)

12

r
o
r
r
e

l
a
n
o
i
t
c
n
u
F

100

10−3

10−6

10−9

10−12

ISTA FISTA
ORM OIM
OAIM

0

20

40

60

80

100

120

140

160

s
e
u
l
a
v

s
r
e
t
e
m
a
r
a
P

4

3

2

1

0

0

FISTA
ORM

OIM OAIM

20

40

60

80

100

120

140

160

Number of iterations

Number of iterations

Fig. 5: Proximal Gradient

1
(cid:107)

k
z
(cid:107)
λ
+
22
(cid:107)
b
−
k
z
A
(cid:107)

12

r
o
r
r
e

l
a
n
o
i
t
c
n
u
F

10−1

10−3

10−5

10−7

10−9

10−11

Primal-Dual ORM
OIM
OAIM

0

10

20

30

40

50

60

70

80

90

100

s
e
u
l
a
v

s
r
e
t
e
m
a
r
a
P

4

3

2

1

0

0

ORM OIM OAIM

10

20

30

40

50

60

70

80

90

100

Number of iterations

Number of iterations

Fig. 6: a Primal-Dual algorithm

a Primal-Dual algorithm [11, Alg. 3.1] for minx f (x) + g(M x)

(cid:13)(cid:13)(cid:13)w − xk + τ M Tyk
(cid:26)

(cid:13)(cid:13)(cid:13)2(cid:27)

(cid:13)(cid:13)(cid:13)w − yk

σ

σ
2

− M (2xk+1 − xk)

(cid:13)(cid:13)(cid:13)2(cid:27)

(cid:26)

xk+1 = argmin

f (w) +

w

1
2τ

yk+1 = yk + σM (2xk+1 − xk) − σ argmin

h(w) +

w

In Fig. 6, we plot the functional error and the parameters for the original algorithm
and our three online methods. The formulation of all algorithms are again quite simple
and we obtain signiﬁcant speed improvements.

7. Conclusion. In this paper, we investigated the theoretical and practical in-
terests of relaxation and inertia on averaged operators. Notably, we established the
expression for optimal parameters and rate when possible and built upon it to pro-
pose novel online methods. Numerical illustrations have demonstrated the behavioral
diﬀerences between relaxation and inertia and showed the remarkable performance of
the proposed online methods.

REFERENCES

[1] A. Abboud, F. Iutzeler, R. Couillet, M. Debbah, and H. Siguerdidjane, Distributed
production-sharing optimization and application to power grid networks, IEEE Transac-
tions on Signal and Information Processing over Networks, (2016).

[2] F. Alvarez, Weak convergence of a relaxed and inertial hybrid projection-proximal point algo-
rithm for maximal monotone operators in hilbert space, SIAM Journal on Optimization,
14 (2004), pp. 773–782.

[3] F. Alvarez and H. Attouch, An inertial proximal method for maximal monotone operators
via discretization of a nonlinear oscillator with damping, Set-Valued Analysis, 9 (2001),
pp. 3–11.
backward method is actually o(k−2), arXiv preprint arXiv:1510.08740, (2015).

[4] H. Attouch and J. Peypouquet, The rate of convergence of nesterov’s accelerated forward-

17

[5] H.H. Bauschke, J.Y. Bello Cruz, T.T.A. Nghia, H.M. Phan, and X. Wang, Optimal rates
of linear convergence of relaxed alternating projections and generalized douglas-rachford
methods for two subspaces, Numerical Algorithms, pp. 1–44, in press.

[6] H. H. Bauschke and P. L. Combettes, Convex analysis and monotone operator theory
in Hilbert spaces, CMS Books in Mathematics/Ouvrages de Math´ematiques de la SMC,
Springer, New York, 2011.

[7] A. Beck and M. Teboulle, A fast iterative shrinkage-thresholding algorithm for linear inverse

problems, SIAM journal on imaging sciences, 2 (2009), pp. 183–202.

[8] P. Bianchi, W. Hachem, and F. Iutzeler, A coordinate descent primal-dual algorithm
and application to distributed asynchronous optimization, arXiv preprint arXiv:1407.0898,
(2014).

[9] S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein, Distributed optimization and
statistical learning via the alternating direction method of multipliers, Foundations and
Trends in Machine Learning, 3 (2011), pp. 1–122.

[10] A. Chambolle and C. Dossal, On the convergence of the iterates of “ﬁsta”., Preprint hal-

01060130, September, (2014).

[11] L. Condat, A primal–dual splitting method for convex optimization involving lipschitzian,
proximable and linear composite terms, Journal of Optimization Theory and Applications,
158 (2013), pp. 460–479.

[12] J. Eckstein and D. P. Bertsekas, On the Douglas-Rachford splitting method and the proximal
point algorithm for maximal monotone operators, Mathematical Programming, 55 (1992),
pp. 293–318.

[13] N. Flammarion and F. Bach, From averaging to acceleration, there is only a step-size, arXiv

preprint arXiv:1504.01577, (2015).

[14] E. Ghadimi, A. Teixeira, I. Shames, and M. Johansson, Optimal parameter selection for the
alternating direction method of multipliers (admm): Quadratic problems, arXiv preprint
arXiv:1306.2454, (2013).

[15] T. Goldstein, B. O’Donoghue, S. Setzer, and R. Baraniuk, Fast alternating direction

optimization methods, SIAM Journal on Imaging Sciences, 7 (2014), pp. 1588–1623.

[16] R. A. Horn and C. R. Johnson, Matrix analysis, Cambridge University Press, 2007.
[17] F. Iutzeler, P. Bianchi, P. Ciblat, and W. Hachem, Asynchronous distributed optimization
using a randomized Alternating Direction Method of Multipliers, in Proc. IEEE Conf.
Decision and Control (CDC), Florence, Italy, Dec. 2013.

[18] F. Iutzeler, P. Bianchi, P. Ciblat, and W. Hachem, Explicit convergence rate of a
distributed alternating direction method of multipliers, arXiv preprint arXiv:1312.1085,
(2013).

[19] F. Iutzeler, P. Ciblat, and W. Hachem, Analysis of sum-weight-like algorithms for averaging
in wireless sensor networks, IEEE Transactions on Signal Processing, 61 (2013), pp. 2802–
2814.

[20] P. Johnstone and P. Moulin, A lyapunov analysis of ﬁsta with local linear convergence for

sparse optimization, arXiv preprint arXiv:1502.02281, (2015).

[21] K. Koh, S. Kim, and S. Boyd, l1 ls: Simple matlab solver for l1-regularized least squares

problems. https://stanford.edu/∼boyd/l1 ls/, 2008.

[22] H. Lin, J. Mairal, and Z. Harchaoui, A Universal Catalyst for First-Order Optimization,

ArXiv e-prints, (2015), arXiv:1506.02186.

[23] Q. Lin and L. Xiao, An adaptive accelerated proximal gradient method and its homotopy
continuation for sparse optimization, Computational Optimization and Applications, 60
(2014), pp. 633–674.

[24] P.-L. Lions and B. Mercier, Splitting algorithms for the sum of two nonlinear operators,

SIAM Journal on Numerical Analysis, 16 (1979), pp. 964–979.

[25] D. Lorenz and T. Pock, An inertial forward-backward algorithm for monotone inclusions,

Journal of Mathematical Imaging and Vision, 51 (2014), pp. 311–325.

[26] P.-E. Maing´e, Convergence theorems for inertial km-type algorithms, Journal of Computa-

tional and Applied Mathematics, 219 (2008), pp. 223–236.

[27] Z. Mu and Y. Peng, A note on the inertial proximal point method, Statistics, Optimization

& Information Computing, 3 (2015), pp. 241–248.

[28] Y. Nesterov, A method of solving a convex programming problem with convergence rate o

(1/k2), Soviet Mathematics Doklady, 27 (1983), pp. 372–376.

[29] Y. Nesterov, Smooth minimization of non-smooth functions, Mathematical programming, 103

(2005), pp. 127–152.

[30] B. O’Donoghue and E. Candes, Adaptive restart for accelerated gradient schemes, Founda-

tions of computational mathematics, 15 (2013), pp. 715–732.

18

[31] B. Polyak, Some methods of speeding up the convergence of iteration methods, USSR Com-

putational Mathematics and Mathematical Physics, 4 (1964), pp. 1–17.

[32] L. F. Richardson, The approximate arithmetical solution by ﬁnite diﬀerences of physical
problems involving diﬀerential equations, with an application to the stresses in a masonry
dam, Philosophical Transactions of the Royal Society of London, (1911), pp. 307–357.

[33] Y. Saad, Iterative methods for sparse linear systems, Siam, 2003.
[34] W. Shi, Q. Ling, K. Yuan, G. Wu, and W. Yin, On the Linear Convergence of the ADMM

in Decentralized Consensus Optimization, ArXiv e-prints, (2013), arXiv:1307.5561.

[35] E. Shiu, Cyclically monotone linear operators, Proceedings of the American Mathematical

Society, 59 (1976), pp. 127–132.

[36] S. Tao, D. Boley, and S. Zhang, Local linear convergence of ista and ﬁsta on the lasso

problem, arXiv preprint arXiv:1501.02888, (2015).

[37] A. Taylor, J. Hendrickx, and F. Glineur, Smooth strongly convex interpolation and exact

worst-case performance of ﬁrst-order methods, arXiv preprint arXiv:1502.05666, (2015).

[38] P. Tseng, On accelerated proximal gradient methods for convex-concave optimization, submit-

ted to SIAM Journal on Optimization, (2008).

[39] E. Wei and A. Ozdaglar, On the O(1/k) convergence of asynchronous distributed Alternating

Direction Method of Multipliers, arXiv preprint arXiv:1307.8254, (2013).

Appendix A. Proof of Lemma 4.
Let ¯x ∈ ﬁxT, and take k even, then xk+2 = T (T(xk) + γk+1(T(xk) − xk)).

(cid:107)xk+2 − ¯x(cid:107)2 = (cid:107)T (T(xk) + γk+1(T(xk) − xk)) − T(¯x)(cid:107)2
≤ (cid:107)T(xk) + γk+1(T(xk) − xk) − ¯x(cid:107)2 − 1 − α
= (1 + γk+1)(cid:107)T(xk) − ¯x(cid:107)2 − γk+1 (cid:107)xk − ¯x(cid:107)2 + (1 + γk+1)γk+1 (cid:107)T(xk) − xk(cid:107)2

(cid:107)T(xk) + γk+1(T(xk) − xk) − xk+2(cid:107)2

α

− 1 − α

α

(cid:107)T(xk) + γk+1(T(xk) − xk) − xk+2(cid:107)2

≤ (1 + γk+1)(cid:107)xk − ¯x(cid:107)2 − γk+1 (cid:107)xk − ¯x(cid:107)2 − (1 + γk+1)

(cid:107)T(xk) − xk(cid:107)2

+ (1 + γk+1)γk+1 (cid:107)T(xk) − xk(cid:107)2 − 1 − α
(cid:19)
α
− γk+1

= (cid:107)xk − ¯x(cid:107)2 − (1 + γk+1)

(cid:18) 1 − α

(cid:107)T(xk) + γk+1(T(xk) − xk) − xk+2(cid:107)2
(cid:107)T(xk) − xk(cid:107)2

1 − α
α

− 1 − α

α

α

(cid:107)T(xk) + γk+1(T(xk) − xk) − xk+2(cid:107)2

i) the fact that T is α-averaged; ii) the equality of [6,
where we used successively:
Cor. 2.14]; iii) a second time that T is α-averaged. The assumption on the sequence
(γk) makes the second term negative or null hence it can be dropped.
α (cid:107)T(xk) + γk+1(T(xk) − xk) − xk+2(cid:107)2
We notice that (cid:107)xk+2 − ¯x(cid:107)2 ≤ (cid:107)xk − ¯x(cid:107)2 − 1−α
implies that the sequence of the even ((cid:107)x2k − ¯x(cid:107)2)k>0 is decreasing and non-negative,
it is thus convergent and the (x2k)k>0 are bounded. Furthermore,

(cid:13)(cid:13)(cid:13)T(x2k) + γ2k+1(T(x2k) − x2k) − x2(k+1)(cid:13)(cid:13)(cid:13)2

< ∞

∞(cid:88)

k=0

implies that any limit point of the sequence (x2k)k>0 belongs to ﬁxT.
Let us now take x(cid:63), a limit point of (x2k)k>0, then ((cid:107)x2k − x(cid:63)(cid:107)2)k>0 converges
and its limit is limk→∞ (cid:107)x2k − x(cid:63)(cid:107)2 = 0 which means that x(cid:63) is unique. Finally, using
non-expansivity, we get that the odd sequence also converges to the same point x(cid:63).

Appendix B. Proof of the linear behavior of aﬃne averaged operators.

19

We consider the ﬁxed point iterations xk+1 = T(xk) = Rxk + d with T = R · +d
a aﬃne α-averaged operator. We assume that ﬁxT (cid:54)= ∅ that is, d lives in the column
space of I − R.
point of T can be expressed as one particular ﬁxed point plus a vector in N .

Let us denote by N the nullspace of I − R: N (cid:44)(cid:8)x ∈ RN : Rx = x(cid:9). Any ﬁxed
of Lemma 1 (see [6, Prop. 5.15]) tells that(cid:80)+∞
+∞(cid:88)

Consider the Jordan decomposition of matrix R: R = W ΛW −1 with W a non-
singular matrix and Λ the Jordan block-diagonal for R (see [16, Chap. 3]). The proof

(cid:13)(cid:13)Rk(R − I)x0 + Rkd(cid:13)(cid:13)2

k=0 (cid:107)xk − T(xk)(cid:107)2 < ∞ so

(cid:13)(cid:13)W(cid:0)Λk(Λ − I)W −1x0 + ΛkW −1d(cid:1)(cid:13)(cid:13)2

+∞(cid:88)

< ∞.

=

k=0

k=0

From the last line, we can deduce that:

i) the eigenvalues of R are smaller than 1 in magnitude and 1 is the only one

with this magnitude;

ii) the algebraic and geometric multiplicities of eigenvalue 1 coincide as the Jor-

dan form of R does not have block of the form J1 =

. Indeed,

1

1
. . .

. . .
. . .

1
1


(cid:21)
(cid:20) I

(cid:20)
(cid:21)

(cid:21)(cid:20) I

∗
∗

˜Λ

W2

(cid:21)(cid:20) W1
(cid:21)
=
∗ + W2 ˜ΛW2

∗W2
∗W2

if it had one could take x0 so that the terms in the sum are bounded away
from zero (e.g. take J = [1 1; 0 1], then Jk(J − I) = [0 1; 0 0]).

Thus, one can write R =
• ˜Λ is the block diagonal matrix of the Jordan blocks corresponding to the

W1 W2

where:

eigenvalues of R with magnitude strictly smaller than 1;

(cid:20) W1

∗
∗

(cid:21)(cid:20)

• and

W1 W2

=

(cid:20) W1

∗W1 W1
∗W1 W2

(cid:21)

.

0
I

W2

W2
∗ deﬁnes a projection onto N .

∗ where conveniently ΠN (cid:44)
From the previous result, R = W1W1
Deﬁne ΠN (cid:44) I − ΠN the complementary projection. Let ¯x ∈ ﬁxT and deﬁne

W1W1
∆k (cid:44) ΠN (xk − ¯x) for all k > 0. We have

0

∆k+1 (cid:44) ΠN (xk+1 − ¯x) = ΠN R(xk − ¯x)

(7)

= W2 ˜ΛW2

∗ ΠN (xk − ¯x)

= W2 ˜ΛkW2

∗∆0

(cid:124)

(cid:123)(cid:122)

∆k

(cid:125)

Thus (∆k)k>0 vanishes exponentially as a consequence of [16, Cor. 5.6.14] on
Eq. (7) which states that there is constant C ∈ R+ such that
(cid:107)∆k(cid:107) ≤ Cknρ(W2 ˜ΛW2
∗)k where the factor kn stems from the k-th power of the
Jordan decomposition of R which introduces terms of the form νkk(cid:96). Using the ∞
norm and taking the log gives the stated result (see [19, Sec. III-C] or [16, Chap 3.2.5]).
Recalling that ˜Λ contains the Jordan blocks associated to the non-unit eigenvalues of
R, which are all strictly smaller than 1 in magnitude, ρ(W2 ˜ΛW2

∗) = ν < 1.

Finally, we can notice that ΠN (xk+1 − xk) = 0, and thus

(cid:107)xk+1 − xk(cid:107)
(cid:107)xk − xk−1(cid:107) =

(cid:107)ΠN (xk+1 − xk)(cid:107)
(cid:107)ΠN (xk − xk−1)(cid:107) =

vk =

(cid:107)ΠN (xk − xk−1)(cid:107)
where the inequality tends to be sharper as k grows and ν ≤ (cid:107)˜Λ(cid:107) ≤ 1.

(cid:107)W2 ˜ΛW2

∗ΠN (xk − xk−1)(cid:107)

≤ (cid:107)˜Λ(cid:107)

20

