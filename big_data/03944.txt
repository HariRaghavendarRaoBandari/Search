6
1
0
2

 
r
a

 

M
2
1
 
 
]

.

C
A
h
t
a
m

[
 
 

1
v
4
4
9
3
0

.

3
0
6
1
:
v
i
X
r
a

Prony’s method in several variables: symbolic

solutions by universal interpolation

Tomas Sauer∗

March 15, 2016

Abstract

The paper considers a symbolic approach to Prony’s method in several vari-
ables and its close connection to multivariate polynomial interpolation. Based on
the concept of universal interpolation that can be seen as a weak generalization of
univariate Chebychev systems, we can give estimates on the minimal number of
evaluations needed to solve Prony’s problem.

1 Introduction

Formulated in several variables, Prony’s problem consists of reconstructing a function
f : Rs → C of the form

f (x) =Xω∈Ω

fωeωT x,

fω ∈ C \ {0}, ω ∈ Ω ⊂ (R + iT)s ,

(1)

from discrete samples, where, as usual, T stands for the torus T := R/2πZ. The re-
striction of the imaginary part of the frequencies is needed to avoid ambiguities in the
solution. In one variable, this problem and its solution date back to Prony [21] in 1795
and since then various numerical methods have been devised to solve the problem, in
particular the ESPRIT and MUSIC algorithms [22, 29] from multi source radar detec-
tion. One should also consider [20] for recent improvements and [18] for a survey on
Prony’s method and its extensions and generalizations.

More recently, also the multivariate case has been considered, by projection methods
as in [5, 19], as well as by direct multivariate approaches [10]. This paper is an extension

∗Lehrstuhl f¨ur Mathematik mit Schwerpunkt Digitale Bildverarbeitung & FORWISS, Universit¨at Pas-

sau, Innstr. 43, D-94053 Passau, Germany. Email: Tomas.Sauer@uni-passau.de

1

of [28], where the algebraic nature of the multivariate problem has been pointed out,
resulting in a fast numerical method based on orthogonal H–bases.

Let us begin with a slightly informal presentation of the algebraic structure under-
lying Prony’s problem in several variables: the approach consist of considering ﬁnite
parts of the inﬁnite Hankel matrix

(2)

(3)

F := [f (α + β) : α, β ∈ Ns

0] ,

for example

Fn := [f (α + β) : α, β ∈ Γn] ,

Γn :=(cid:8)α ∈ Nd

0 : |α| ≤ n(cid:9) ,

with the standard length |α| = α1 + · · · + αs of a multiindex α ∈ Ns
observation is that the Prony ideal IΩ, the set of all polynomials vanishing on

0. The crucial

XΩ = eΩ = {xω = eω = (eω1, . . . , eωs) : ω ∈ Ω} ,

is in one-to-one correspondence with the kernels of the matrices Fn. More precisely,
if we denote by Πn the vector space of all polynomials of total degree at most n, and
identify an polynomial

Πn ∋ p(x) = X|α|≤n

pα xα

with its coefﬁcient vector p = [pα : α ∈ Γn] ∈ CΓn, then the following result from [28]
holds true.

Theorem 1.1 For sufﬁciently large n we have that p ∈ IΩ ∩ Πn if and only if Fnp = 0.

The “sufﬁciently large” in Theorem 1.1 can be made concrete: it is the degree of some
degree reducing interpolation space for XΩ and since all degree reducing interpolation
spaces have the same degree, this is indeed a geometric quantity depending on XΩ only.
The algorithm developed in [28] relies on the a priori knowledge of such a sufﬁciently
large n and then successively builds the matrices

Fn,k :=(cid:20)f (α + β) :

α ∈ Γn

α ∈ Γk (cid:21) ,

k = 0, . . . , n,

for which the following observation has been made in [28].

Theorem 1.2 For sufﬁciently large n, we have that

1. ker Fn,k ≃ IΩ ∩ Πk, k = 0, . . . , n,

2. k 7→ rank Fn,k computes the afﬁne Hilbert function for the ideal IΩ,

2

3. there exists a number 0 ≤ m ≤ n such that

rank Fn,0 < · · · < rank Fn,m−1 = rank Fn,m = · · · = Fn,n = Fn

and this m is the minimal choice for n.

Based on these structural observations, an algorithm can be derived that computes an
orthogonal H–basis in the sense of [25] as well as a graded homogeneous basis for the
interpolation space Π/IΩ entirely by application of standard techniques from Numerical
Linear Algebra, namely singular valued decompositions and QR factorizations. This
allows for fast and accurate solutions of even high dimensional problems.

Once the H–basis and the interpolation space are determined, it is a fairly standard
approach, see [9, 30] to determine the multiplication tables, a set of s commuting ma-
trices of size #Ω × #Ω whose eigenvalues are the components of the xω that can be
related by the respective eigenvectors, see [17].

This paper takes a somewhat different approach to Prony’s problem by considering
interpolation spaces spanned by a minimal number of monomials, see [3, 23]. While
orthogonal H–bases are more favorable from a numerical point of view, this yields a
symbolic approach that will provide us with a minimal sampling set for Prony’s method
and tell us what an (asymptotically) minimal number of evaluations of f needed for the
reconstruction.

The paper is organized as follows. In Section 2 the notation will be ﬁxed and Prony’s
problem will be expressed in terms of degree reducing interpolation. In Section 3 we
study the fundamental algebraic tool, namely uniform interpolation. This means the
identiﬁcation of spaces that permit interpolation at any subset of Cs of a given cardinal-
ity. Based on this concept, Section 4 points out how to solve Prony’s problem with a
minimal number of evaluations. Detailed symbolic algorithms for that purpose are de-
veloped in Section 5. Finally, Section 6 brieﬂy points out the connection to sparse poly-
nomials and how those can be determined symbolically and an appendix in Section 7
provides two valuable tools from computational ideal theory together with proofs.

All results presented in this paper are of algebraic nature. Of course, the numerical
stability of the methods and of the reconstruction in general depends on the conditioning
of Vandermonde matrices. This important and valuable question, however, is not in the
scope of this paper.

2 Prony’s problem revisited

Let Π = C[x1, . . . , xs] denote the algebra of polynomials in s variables with complex
coefﬁcients. We consider the nonnegative integer grid Γ = Ns
0. For a ﬁnite A ⊂ Γ we
deﬁne

ΠA :=(p(x) =Xα∈A

pα xα : pα ∈ C)

3

as the space of polynomials supported on A, a ﬁnite dimensional subspace of Π of
dimension #A. Recall that the total degree of a polynomial p ∈ Π is deﬁned as

deg p := max{|α| : pα 6= 0}.

In the important case A = Γn = {α : |α| ≤ n}, we use the common abbreviation
Πn := ΠΓn for the vector space of all polynomials of total degree (at most) n. In an
analogous way, we deﬁne Γ0
n = {α : |α| = n} as the set of homogeneous multiindices
of length n and Π0

n as the homogeneous polynomials of degree (exactly) n.

The coefﬁcients pα of p ∈ ΠA can be conveniently arranged into a vector p = (pα :
α ∈ A) ∈ CA; we use the same symbol for the polynomial and the vector, the respective
meaning will be clear from the context. The next notion is standard in (polynomial)
interpolation theory.

n := ΠΓ0

Deﬁnition 2.1 For ﬁnite sets A ⊂ Γ and X ⊂ Cs we deﬁne the Vandermonde matrix
V (X, A) as

V (X, A) :=(cid:20)xα :

x ∈ X

α ∈ A (cid:21) .

Vandermonde matrices play a fundamental role in Prony’s method as the following sim-
ple computation shows: for A, B ⊂ Γ we consider

FA,B :=(cid:20)f (α + β) :

α ∈ A

β ∈ B (cid:21) ,

hence,

(FA,B)α,β = Xω∈Ω
= Xω∈Ω

fω eωT (α+β) =Xω∈Ω

fω eωT αeωT β

eT
αV (XΩ, A)T eω fω eT

ω V (XΩ, B)eβ

which yields the well–known factorization

FA,B = V (XΩ, A)T FΩ V (XΩ, B),

FΩ := diag (fω : ω ∈ Ω),

(4)

already used in the univariate ESPRIT method [22]. Since, by assumption (1), fω 6= 0,
ω ∈ Ω, the rank of FA,B is at most #Ω with equality if and only if

rank V (XΩ, A) = rank V (XΩ, B) = #Ω.

(5)

The meaning of (5) is well–known: ΠA and ΠB have to be interpolation spaces for XΩ.

Deﬁnition 2.2 A subspace P of Π is called an interpolation space for X if for any
y ∈ CX there exists (at least one) p ∈ P such that p(X) = y.

4

Despite of its simple derivation, (5) has an immediate important consequence for Prony’s
problem and the reconstruction of Ω from FA,B

Theorem 2.3 The coefﬁcients fω can be reconstructed from FA,B if and only if ΠA and
ΠB are interpolation spaces with respect to XΩ.

Proof: Suppose that ΠA and ΠB are interpolation spaces with respect to XΩ, then #A ≥
#Ω, and there exist coefﬁcient vectors pω = (pω,α : α ∈ A) such that for ω′ ∈ Ω

pω,α xα
ω′,

δω,ω′ = pω(xω′) =Xα∈A
V (XΩ, A)(cid:20)pω,α :

α ∈ A

ω ∈ Ω (cid:21) = I#Ω.

hence

In other words, this matrix is a right inverse of V (XΩ, A) which we call V (XΩ, A)−1
and since the same holds for V (XΩ, B), it follows that

V (XΩ, A)−T FA,BV (XΩ, B)−1 = FΩ,

which reconstructs the coefﬁcients under the assumption that ΠA and ΠB are interpola-
tion spaces for XΩ.

Conversely, if rank V (XΩ, B) < #Ω, then there exists a nonzero diagonal matrix

F ∈ CΩ×Ω such that F V (XΩ, B) = 0 and therefore

FA,B = V (XΩ, A)T (FΩ + F ) V (XΩ, B)

so that FΩ cannot be reconstructed from FA,B. An analogous argument can also be used
in the case that rank V (XΩ, A) < #Ω.
(cid:3)

Corollary 2.4 Any sampling sets A, B for Prony’s method must be chosen such that ΠA
and ΠB are interpolation spaces for XΩ.

Deﬁnition 2.5 A subspace P of Π is called a degree reducing interpolation space for
a ﬁnite set X ⊂ Cs if for any q ∈ Π there exist a unique polynomial p ∈ P such that
p(X) = q(X) and deg p(X) ≤ deg q(X).

Degree reducing interpolation spaces for some set X ⊂ Cs of sites have the advantage
that they give the ideal IX = {p ∈ Π : p(X) = 0} almost for free. To be more
concrete, let A ⊂ Γ be such that ΠA is a degree reducing interpolation space for X and
let LA : Π → ΠA denote the interpolation operator deﬁned by

(LAp)(X) = p(X)

(6)

5

q = Xα∈Ac

which is well deﬁned because degree reducing interpolation is unique by deﬁnition. It
can then be shown that the polynomials

hα := (·)α − LA(·)α,

α ∈ Ac := Γ \ A,

(7)

form an H–basis of IX = {q : q(X) = 0}, that is, any polynomial q ∈ IX can be written
as

qα hα,

deg qα ≤ deg q − deg hα,

(8)

where the sum in (8) is ﬁnite and uses the convention that deg p < 0 iff p = 0. This fact,
that is some folklore in ideal interpolation will be (re-)proved in even stronger form in
Lemma 7.1 of the abstract.

By Corollary 2.4 we can reconstruct FΩ from the symmetric sampling matrix FA,A
if and only if ΠA is an interpolation space. A can be chosen minimally or at least
of minimal cardinality by requesting ΠA to be a minimal degree interpolation space.
In other words, computing a solution to Prony’s problem with a minimal number of
evaluations leads to determining such a space for the unknown point set XΩ from FA,A.
The following two examples illustrate what can happen.

Example 2.6 (Generic case) Suppose for simplicity that

s (cid:19).
N := #Ω = rn = dim Πn =(cid:18)n + s

In this situation the set of all point conﬁgurations X such that Πn is the degree reducing
interpolation space is open and dense in (Cs)#Ω, hence,

det V (XΩ, Γn) 6= 0

with probability 1. Hence, Fω can be reconstructed from Fn and the kernel of

Fn,n+1 =(cid:20)f (α + β) :

α ∈ Γn

β ∈ Γn+1 (cid:21)

determines an H–basis of IΩ by Theorem 1.1. Hence, Prony’s problem can be solved
based on the knowledge of f on the grid

{α + β : α ∈ Γn, β ∈ Γn+1} = Γ2n+1

since any multiindex of length 2n + 1 can be written as the sum of two multiindices, one
of length n, one of length n + 1. Hence the number of samples is r2n+1 and since

r2n+1

N

s

(cid:1)
= (cid:0)2n+1+s
s (cid:1) =
(cid:0)n+s

(2n + 1 + s) · · · (2n + 2)

(n + s) · · · (n + 1)

=

sYj=1(cid:18)1 +

n + 1

n + j(cid:19) ≤ 2s,

with 2s being the smallest bound independent of n, it follows that the generic case needs
≈ 2s#Ω samples of f without any curse of dimensionality.

6

Unfortunately, not every conﬁguration of the frequencies Ω and therefore of the points
XΩ is generic and even if any conﬁguration could be made generic by an arbitrarily
small perturbation, relying on the generic situation leads to numerical and structural
problems. The second example shows that linear complexity cannot be expected in
general.

Example 2.7 (Hyperbola) Let Ω ⊂ C2 consist of 2N + 1 distinct frequencies of the
form (ωj, −ωj) with ωj ∈ R, j = 0, . . . , 2N + 1. Then the points xj = (eωj , e−ωj ) ∈ R2
all lie on the hyperbola x1x2 = 1, hence q(x) = x1x2 − 1 ∈ IΩ, and the (unique)
degree reducing interpolation space is spanned by 1, x1, . . . , xN
2 which is a
subset of ΠN . Hence, A = {0, ǫ1, . . . , Nǫ1, ǫ2, . . . , Nǫ2} where ǫj stands for the unit
multiindex. With A′ := A ∪ {(N + 1)ǫ1, (N + 1)ǫ2} the minimal sampling set consists
of

1 , x2, . . . , xN

A + A′ = {α + β : α ∈ A, β ∈ A′} ⊃ {α ∈ Γ : kαk∞ ≤ N},

which has > (N + 1)2 elements and therefore at least O(N 2) sampling points have to
be used in this case.

Nevertheless, since #(A + A) ≤ (#A)2 we could always have a chance to reconstruct
f from O(N 2) samples, independent of the dimension, provided we could ﬁnd a set
A ⊂ Γ such that ΠA is a degree reducing interpolant. Why “degree reducing” is so
important will become clear in Section 5 where we use this property to identify the
ideal.

3 Universal interpolation spaces

The observations from the preceding section naturally suggest the following question.

Problem 3.1 For any N ≥ 0 determine a minimal set ΥN ⊂ Γ such that for any set X
with #X = N we ﬁnd a subset A ⊂ ΥN such that ΠA is a degree reducing interpolation
space for X.

In one variable, we know that ΥN = ΠN −1 solves Problem 3.1 since the polynomials
of degree at most N − 1 form a Chebychev system of order N or span a Haar space
of dimension N. Both means the same: any interpolation problem at N sites has a
unique solution. Since Haar spaces only exist in one variable according to Mairhuber’s
theorem, cf. [11], a solution of Problem 3.1 must contain more than N elements.

Remark 3.2 Problem 3.1 is a simpler version of the classical problem of ﬁnding for any
N a polynomial space of minimal dimension that allows for interpolation at arbitrary
N points. To my knowledge, these spaces are only known for some very special cases.

7

In the following deﬁnition of universal interpolation spaces we deﬁne the data to be
interpolated by means of polynomials. Since ΠN is always a universal interpolation
space of order N + 1, cf. [24], this is no restriction, but more consistent when degree
reducing interpolation is concerned.

Deﬁnition 3.3 A subspace P is called a universal interpolation space or generalized
Haar space of order N if for any X ⊂ Cs with #X ≤ N and any q ∈ Π there exists
p ∈ P such that p(X) = q(X). P is called a degree reducing universal interpolation
space if the interpolant p ∈ P can be chosen such that deg p ≤ deg q.

Deﬁnition 3.4 A degree reducing universal interpolation P space of order N is called
redundant if there exists a proper subspace Q ⊂ P that is also a degree reducing
universal interpolation space.

Lemma 3.5 Any non-redundant degree reducing universal interpolation space P of
order N + 1 is a subspace of ΠN .

Proof: If P is not a subspace of ΠN and not redundant, there must be a conﬁguration
X ⊂ Cs of N sites such that at least one of the polynomials ℓx deﬁned by ℓx(x′) = δx,x′,
x, x′ ∈ X, has degree > N as otherwise P could be chosen as a subspace of ΠN .
Let ℓx denote this polynomial and x the respective element of X. On the other hand,
since ΠN is also a universal interpolation space there exists ˜ℓx ∈ ΠN with the same
interpolation property ˜ℓx(x′) = δx,x′, x′ ∈ X, so that ℓx is the interpolant to ˜ℓx. But then
deg ℓx > N ≥ deg ˜ℓx contradicts the assumption that P is degree reducing.
(cid:3)
Returning to Problem 3.1, we now give an explicit non–redundant and therefore minimal
monomial degree reducing interpolation space that is spanned by monomials, hence is
of the form ΠA for some set A ⊂ Γ. This can be formalized as follows.

Deﬁnition 3.6 A set A ⊂ Γ is called a monomial degree reducing universal interpola-
tion set of order N if for any X ⊂ Cs with #X ≤ N there exists a subset B ⊆ A of
cardinality #X such that ΠB is a degree reducing universal interpolation space.

To identify monomial degree reducing universal interpolation spaces, we need another
fundamental concept.

Deﬁnition 3.7 For two multiindices α, β ∈ Γ we write α ≤ β if αj ≤ βj, j = 1, . . . , s.
We call A ⊂ Γ a lower set if α ∈ A implies {β ∈ Γ : β ≤ α} ⊂ A. By L(Γ) we denote
the set of all lower sets in Γ and

Lj(A) = {B ∈ L(Γ) : #B = j},

j ∈ N,

stands for all lower sets of cardinality j.

8

Deﬁnition 3.8 For a ﬁnite set A ⊂ Γ we deﬁne its border as

and its corona as

∂A :=  s[j=1

(A + ǫj)! \ A

⌈A⌉ := A ∪ ∂A.

(9)

(10)

We can now describe degree reducing sets of monomials in terms of lower sets.

Theorem 3.9 If A ⊂ Γ is a monomial degree reducing universal interpolation set of
order N, then

A ⊇

N[j=1 [B∈Lj (Γ)

B.

(11)

Proof: Let B be any lower set with #B ≤ N and consider interpolation on the grid

XB = {β : β ∈ B} .

Since A is a monomial degree reducing set, there exists a B′ ⊆ A such that ΠB ′ is a
degree reducing interpolation space for XB where uniqueness of interpolation implies
that #B′ = #B. Since ΠB ′ is degree reducing, the polynomials

(·)β − LB ′(·)β,

β ∈ ∂B′,

B

, see Lemma 7.1, and ΠB ′ is the normal form or reduced space
form an H–Basis of IX ′
for XB with respect to this H–basis and an appropriate inner product, see Lemma 7.2.
The same holds true for B and the standard inner product, hence ΠB is also a normal
form space. It has been proved in [26] that for interpolation grids based on lower sets the
normal form interpolation space is unique independently of grading and inner product,
hence ΠB = ΠB ′ and since both spaces are spanned by #B monomials, it follows that
B = B′.
(cid:3)

Proposition 3.10 Any set A that satisﬁes (11) is a monomial degree reducing universal
interpolation set of order N.

Proof: Given X ⊂ Cs, #X ≤ N, let G be any reduced graded Gr¨obner basis for
IX and B the index set for the quotient space ΠB ≃ Π/IX. Since the complement of
B consists of the upper set leading terms of the ideal, cf. [4, p. 230], it follows that
B is a lower set and unique interpolation requests that #B = #X. Therefore, any
interpolation problem with ≤ N sites can be solved by an appropriate lower set B of
the same cardinality.
(cid:3)
Combining Theorem 3.9 with Proposition 3.10, we immediately get the following result.

9

Corollary 3.11 The index set

A∗

N :=

N[j=1 [B∈Lj (Γ)

B

(12)

consisting of the union of all lower sets of cardinality at most N is a non–redundant,
hence minimal, monomial degree reducing universal interpolation set.

The index set A∗ deﬁned in (12) can easily be described in a different way.

Lemma 3.12 For any N ≥ 1 we have

α ∈ A∗
N

⇔

π(α) :=

(αj + 1) ≤ N.

sYj=1

(13)

Proof: Since the set {β ∈ Γ : β ≤ α} has cardinality (α1 + 1) · · · (αs + 1), any
N of
multiindex satisfying the right hand side of (13) determines a lower subset of A∗
N. If, on the other hand π(α) > N then
cardinality ≤ N, and therefore must belong to A∗
any lower set containing α contains > N elements and cannot be a subset of A∗
N, hence
N.
α 6∈ A∗
(cid:3)

Deﬁnition 3.13 The set ΥN := {α ∈ Γ : π(α) ≤ N} is called the (positive octant of
the) hyperbolic cross of order N.

In the sequel we will use “hyperbolic cross” for the positive octant since we only con-
sider subsets of Γ. Hyperbolic crosses play an important role in the context of FFT
methods [6], but also in general multivariate Approximation Theory, cf. the recent sur-
vey [7]. In terms of interpolation, we can summarize the above results as follows.

Corollary 3.14 ΥN forms a minimal monomial degree reducing universal interpolation
set of order N.

Since #ΥN ≤ N logs−1 N, cf. [12, Lemma 1.4, p. 71], the universal interpolation space
based on ΠΥN allows for degree reducing interpolation at arbitrary N sites in Cs which
can be seen as some “Haar space without uniqueness” where the number of elements
only exceeds the number of points by a very moderate logarithmic factor.

Remark 3.15 The hyperbolic crosses ΥN are only minimal monomial interpolation
spaces, but not general ones. This can be easily seen in the case N = 3 in s = 2
where {1, x, y, x2 + y2} forms a universal interpolation of minimal dimension 4 while
Υ2 = {(0, 0), (1, 0), (2, 0), (0, 1), (0, 2)} already consists of 5 elements and therefore
has dimension 5.

10

4 Minimal recovery

By the results of the preceding section, a simple method can be devised to solve Prony’s
problem, provided that N = #Ω is given:

1. Set up the matrix

F =(cid:20)f (α + β) :

α ∈ ΥN

β ∈ ⌈ΥN ⌉ (cid:21) .

2. Compute the kernel of this matrix and therefore the Prony ideal as the ideal gen-

erated by the kernel vectors, interpreted as polynomial coefﬁcients.

3. Compute a graded Gr¨obner basis or an H–basis as in [15] and the normal form

space, cf. [27], for this ideal.

4. Determine the multiplication tables and their eigenvalues and therefore XΩ as

described in [28].

5. Determine the coefﬁcients by solving the Vandermonde system.

This procedure is an evaluation efﬁcient way of solving Prony’s problem where the num-
ber of function evaluations needed depends only in a very mild way on the dimension
s.

Theorem 4.1 Prony’s problem can be solved for N frequencies in Cs on the basis of at
most

(s + 1)N 2 log2s−2 N

point evaluations on the grid Γ.

Proof: The theorem relies on two simple facts: any kernel element of F belongs the
ideal IΩ by Theorem 4.4 and by Lemma 7.1 from the appendix, the corona of ΥN con-
tains even an H–basis of the ideal IΩ, hence the ideal and the quotient space can be deter-
mined from F . This allows for the reconstruction of frequencies by eigenvalues methods
and coefﬁcients by solving a simple linear system. Since the corona of a set contains at
most s + 1 times as many elements as the set, the estimate #ΥN ≤ N logs−1 N leads to
the claim.
(cid:3)

Remark 4.2 The order N 2 is optimal up to constants and logarithmic factors for re-
construction from Hankel matrices of samples from Γ. Indeed, we saw in Theorem 2.3
that even the coefﬁcients can only be reconstructed from a matrix FA,B provided that
A, B are interpolation sets for XΩ. Without a priori information on Ω, these sets have
to be universal and ΥN is the minimal universal set, at least when degree reduction and
monomiality are requested. Moreover, Example 2.7 showed that a complexity of N 2 is
unavoidable for Hankel matrices already for s = 2.

11

Remark 4.3 In the generic case that happens with probability one, the complexity is
even lower, namely ∼ 2sN, as pointed out in Example 2.6. Consequently, with proba-
bility one the number of variables even enters only as a constant.

Theorem 4.4 Suppose that N = #Ω and A ⊂ Γ. Then a vector p ∈ CA belongs to the
kernel of FΥN ,A if and only if the associated polynomial fulﬁlls p ∈ IΩ ∩ ΠA.

Proof: The standard “Prony trick” yields that for any β ∈ ΥN we have

(FΥN ,A p)β =Xα∈A

f (α + β)pα =Xα∈AXω∈Ω

fωeωT (α+β) pα =Xω∈Ω

which we can rewrite in vector form as

fω eωT β p(xω)

(14)

FΥN ,A p = V (XΩ, ΥN )T FΩ [p(ω) : ω ∈ Ω] ,

and since rank V (XΩ, ΥN )T = rank FΩ = #Ω, this vector is zero if and only if p
vanishes on XΩ.
(cid:3)
A slightly closer inspection of the proof shows that we can reformulate Theorem 4.4 in
even stronger form.

Corollary 4.5 The equivalence

p ∈ ker FA,B

⇔

p ∈ IΩ ∩ ΠB

holds if and only if ΠA is an interpolation space for XΩ.

By Theorem 4.4, the algorithm from [28] could immediately be restated for the matrices

Fn,k :=(cid:20)f (α + β) :

α ∈ ΥN

|β| ≤ k (cid:21) ,

but since this approach is based on orthogonal decompositions it requires square roots
which makes it inappropriate for a symbolic environment. Therefore, the next section
provides an algorithm that works in a symbolic and more “monomial” environment.

5 Symbolic algorithms

Now we are in position to turn the observations obtained so far into detailed symbolic
algorithms for the reconstruction of f. The ﬁrst one will be called Sparse Monomial
Interpolation with Least Elements (SMILE), in contrast to the Sparse Homogeneous In-
terpolation Technique from [28], which, for some unclear reasons, has not been awarded
an acronym. Both methods have in common that for k = 0, 1, . . . they successively

12

compute ΠA ∩ Πk and an H–basis of IΩ ∩ Πk at the same time by appropriate update
rules. This is more efﬁcient than ﬁrst determining some basis of the ideal, then a “good”
basis (G¨obner or H–basis) and afterwards the quotient space.

We return to the function of the form (1) and assume that N := #Ω is known.

During the process, we will consider matrices of the form

Fk :=(cid:20)f (α + β) :

α ∈ ΥN

β ∈ Ak (cid:21) ,

Ak ⊆ Γk,

(15)

with nested sets A0 ⊆ A1 ⊆ · · · to be determined during the reconstruction process
which will eventually terminate for some n with An being a monomial degree reducing
set for interpolation at XΩ.

The initialization is A0 = {0} which leaves us with

F0 = [f (α) : α ∈ ΥN ] ∈ C#ΥN ×1.

which is 6= 0 since F0 = V (XΩ, ΥN )T FΩ1N with rank V (XΩ, ΥN ) = #Ω = N and
FΩ 6= 0. Hence, rank F0 = 1 = #A0. Moreover, we deﬁne I0 = B0 := ∅ as a subset of
Γ0 and note that Γ0 = A0 ∪ I0 ∪ B0.

To advance from k → k + 1 we assume that rank Fk = #Ak and Γk = Ak ∪ Bk ∪ Ik

and deﬁne the sets

and extend Fk into

k ∩ Γ0

B :=

s[j=1(cid:0)Ac
eFk+1 :=(cid:20)f (α + β) :

k+1,

k(cid:1) + ǫj ⊆ Γ0
β ∈ eAk+1 (cid:21) = [Fk | G] ,

α ∈ ΥN

with additional columns. Next, we compute

eAk+1 := Ak ∪ (Γ0

k+1 \ B)

G ∈ CΥN ×( eAk+1\Ak)

and obtain that

and arrange a basis of Y into a matrix Y ∈ C
Ak). We write

ker eFn+1 =ny ∈ C
Y ′ (cid:21) ,
Y =(cid:20) Yk
0 = eFk+1Y = [Fk | G](cid:20) Yk

Yk ∈ CAk×d,

13

eAk+1 \ {0} : eFn+1y = 0o
eAk+1×d with d := dim ker eFn+1 ≤ #(eAk+1\

Y ′ ∈ C

eAk+1\Ak×d,

Y ′ (cid:21) = FkYk + GY ′.

(16)

Since Fk is of maximal rank by assumption, it has a left inverse, for example the pseu-
doinverse F +
k GY ′ by (16). This implies the Schur complement
relation

k , giving Yk = −F +

k )GY ′.

(17)

0 = eFk+1(cid:20) −F +

k G
I

(cid:21) Y ′ = [Fk | G](cid:20) −F +

k G
I

(cid:21) Y ′ = (I − FkF +

Still, rank Y ′ = d, hence there exist d linear independent rows of Y ′ or a permutation
P such that

Z := [Id×d | 0] P Y ′ ∈ Rd×d

is invertible, so that

P Y ′Z −1 =(cid:20) I
∗ (cid:21) .
After replacing Y ′ by Y ′Z −1 and ordering the elements of eAk+1 \ Ak according to the
permutation P , we can thus assume that Y ′ =(cid:20) Id×d
∗ (cid:21) and determine Yk = −F ∗

k GY ′
which of course also requires a compatible ordering of the columns of G. Now we set

Ak+1

Ik+1

Bk+1

:= Ak ∪(cid:16)eAk+1 \ Ak(cid:17)(cid:16)d + 1 : #(eAk+1 \ Ak)(cid:17) ,
:= Ik ∪(cid:16)eAk+1 \ Ak(cid:17) (1 : d),

:= Bk ∪ B,

(18)

(19)

(20)

the respective columns of Y .

where the components of the vectors are indexed in a Matlab–like way. It follows di-
rectly from (18), (19), (20) and the assumption on Ak and Ik that Ak+1 ∪ Ik+1 = Γk+1.
whose coefﬁcients are

For α ∈(cid:16)eAk+1 \ Ak(cid:17) (1 : d) we deﬁne polynomials qα ∈ Π eAk

Having determined Ak+1, we can build the matrix Fk+1 according to (15). If Ak+1
is a proper superset of Ak, the matrix enlarges Fk by adding further columns which
immediately yields that rank Ak+1 ≥ rank Ak. This construction also advances the
rank hypothesis from k to k + 1.

Lemma 5.1 The matrix

Fk+1 = FΥN ,Ak+1 = V (ΥN , XΩ)T FΩV (Ak+1, XΩ)

(21)

has maximal rank #Ak+1.

Proof: The claim is the induction hypothesis if Ak+1 = Ak and therefore trivial in this
case. If #Ak+1 > #Ak we ﬁrst note that, since rank Fk = #Ak there exists a matrix

Z =(cid:20)

∗

0#( eAk+1\Ak)×#Ak (cid:21) ∈ C# eAk+1×#Ak

14

such that rankeFk+1Z = #Ak. If we extend the matrix Y from above as
∗  ∈ R#( eAk+1\Ak)×( eAk+1\Ak)

Z ′ = [Y | ˆY ] =

∗
Id×d 0

∗

∗

implies that

into a matrix of rank eAk+1 \ Ak, the fact that Y exactly contains the kernel of eFk+1
rank Fk+1[Z |bY ] = rankeFk+1[Z | Y |bY ] = rankeFk+1[Z |bY ] = #eAk+1 − d = #Ak+1

and therefore rank Fk+1 = #Ak+1.

(cid:3)

This algorithm is repeated until Ak+1 = Ak and it solves Prony’s problem at termi-

nation. Let us summarize the algorithm formally.

Algorithm 5.2 (Prony’s method, symbolic decomposition)
Given: function f : Γ → C and N ≥ 0.

1. Initialization: A0 := {0}, I0 := ∅, B0 := ∅ and F0 := [f (α) : α ∈ ΥN ].

2. For k = 0, 1, . . . repeat

(a) Compute

set b = #(Γ0

k+1 \ B) and compute

B :=

s[j=1(cid:0)Ac
G :=(cid:20)f (α + β) :

k ∩ Γ0

k(cid:1) + ǫj
k+1 \ B (cid:21) .

α ∈ ΥN

β ∈ Γ0

(b) Determine the kernel of (I−FkF +

k )G and write it as a matrix Y =(cid:20) Id×d
∗ (cid:21),

where d is the dimension of the kernel. This deﬁnes an ordering of Γ0

k+1 \ B.

(c) Set

Ak+1 = Ak ∪ (Γ0
Ik+1 = Ik ∪ (Γ0
Bk+1 = Bk ∪ B.

k+1 \ B)(d + 1 : b),
k+1 \ B)(1 : d),

(d) Set

Fk+1 :=(cid:20)f (α + β) :

α ∈ ΥN

β ∈ Ak+1 (cid:21) =(cid:20)Fk (cid:12)(cid:12)(cid:12)(cid:12)f (α + β) :

15

α ∈ ΥN

β ∈ Ak+1 \ Ak (cid:21) .

(e) Deﬁne polynomials qα, α ∈ Ik+1 \ Ik, by taking the αth column of the matrix

(cid:20) −F +

k GY
Y

(cid:21) as coefﬁcient vectors.

until Ak+1 = Ak.

Results: Monomial degree reducing interpolation space ΠAk for XΩ and H–basis H =
{hα : α ∈ Ik+1} for IΩ.

Remark 5.3 The intuitive meaning of (18), (19) and (20) is to split the multiindices
from Γ0
k+1 into three groups: Ak+1 collects those which are used for the interpolation
space, Ik+1 those which are needed for the H–basis and Bk+1 those which also belong
to the ideal due to an H–basis element of lower degree.

Theorem 5.4 If N ≥ #Ω Algorithm 5.2

1. terminates at some level k = n ≤ #Ω,

2. determines a degree reducing interpolation set An for XΩ,

3. determines an H–basis H := {hα : α ∈ In+1} for IΩ.

Proof: We will ﬁrst verify that the polynomial set

Hk :=(cid:8)(·)βhα : β ∈ Πk−|α|, α ∈ Ik(cid:9) ,

k ∈ N0,

(22)

forms a vector space basis for IΩ ∩ Πk. This is trivially true as long as Ik = ∅. Since,
for any α ∈ Ik, we have deg hα = |α| and since the coefﬁcient vector of any hα belongs
to ker F|α|−1, Theorem 4.4 ensures that hα ∈ IΩ ∩ Π|α|. Thus, Hk ⊆ IΩ ∩ Πk. For the
converse inclusion inclusion, we assume that q ∈ Πk ∩ IΩ with deg q = k. Again by
Theorem 4.4, the coefﬁcient vector of q has to belong to ker Fk. By subtracting a proper
element of Hk we can, like in the standard Gr¨obner basis division algorithm, eliminate
all monomials from Γk \ Ak from q and obtain another q′ ∈ IΩ ∩ Πk since we only
subtracted ideal elements. Moreover, q′ ∈ ker Fk, but since, according to Lemma 5.1,
Fk has rank #Ak, the only polynomial q′ ∈ ΠAk ∩ IΩ is q′ = 0. Hence, q ∈ Hk and
therefore span Hk ⊇ IΩ ∩ Πk as well, yielding span Hk = IΩ ∩ Πk.

Since Π is a Noetherian ring, cf. [4], there exists some n ∈ N0 such that the increas-
ing chain hHki, k ∈ N0, of ideals from (22) stabilizes, and since the strict inclusion
Ik+1 ⊃ Ik implies Hk+1 ⊃ Hk in the strict sense, it follows that also In = In+1 = · · · .
By (19) this means that either d = 0, i.e., all columns of Y correspond to ideal el-
n+1 and
ements, or B = Γ0
An = An+1 = · · · . Since Πn = An ⊕ Hn, it follows that ΠAn is a degree reducing
interpolation space, see [27] and that

In both cases we have that Λ(Hn+1) ∩ Π0

n+1 = Π0

n+1.

H := {hα : α ∈ In}

16

is an H–basis for IΩ.
The algorithm can also be formulated in a “term-by-term” way which gives an im-
plicit version of the M¨oller–Buchberger algorithm from [13]. To that end, we recall the
classical graded lexicographic ordering “(cid:22)” where α ≺ β provided that |α| < |β| or
|α| = |β| and there exists some k ∈ {1, . . . , s} such that αj = βj, j = 1, . . . , k − 1, and
αk < βk, cf. [4]. “(cid:22)” is a total ordering on Γ and therefore also induces a total order on
the monomials or terms (·)α, α ∈ Γ. The algorithm now proceeds as follows.

(cid:3)

Algorithm 5.5 (Prony’s method, Gr¨obner decomposition)
Given: function f : Γ → C and N ≥ #Ω.

1. Initialization: B = ΓN +1, I := ∅, A = ∅, F = [] ∈ C#ΥN ×0.

2. While B 6= ∅

(a) β := min(cid:22) B.
(b) Expand the matrix by one column:

(c) If rankeF = rank F then determine 0 6= qα ∈ ker eF and set

I

eF = [F | f (α + β) : α ∈ ΥN ] .
B := B \(cid:0)β + ΓN −|β|(cid:1) ,

:= I ∪ {β}.

If rankeF > rank F then set F := eF and

B := B \ {β},
A := A ∪ {β}.

Results: Gr¨obner basis {qα : α ∈ O} and monomial quotient space ΠA ≃ Π/IΩ.

The proof of the validity of this algorithm works like the proof of the preceding theorem,
one only has to keep in mind that whenever the rank increases, a new term for the
quotient space has been found which guarantees that always rank F = #A. If, on the
other hand, the rank does not increase after adding the column, there must be a nontrivial
kernel element, unique up to normalization with nonzero value in its βth component
which becomes a member of the ideal basis.

Theorem 5.6 Algorithm 5.5 computes the decomposition using at most

evaluations of f if N = #Ω.

s N 2 logs−1 N

17

Proof: Each column added needs at most #ΥN ≤ N logs−1 N evaluations of f. The
number of columns added during the algorithm is

#A + #I ≤ #A + #∂A ≤ #A + (s − 1)#A = s #A

since I ⊆ ∂A.
Once the set An and the H–basis H are determined, the points XΩ can be determined by
means of multiplication tables as described in [1, 16] and efﬁciently determined by the
methods from [17]. For reduction we can again use the inner product from Lemma 7.2.
Once XΩ and thus Ω are determined, the coefﬁcients fω, ω ∈ Ω, are determined by
solving a linear system, for details see [28].

(cid:3)

6 Sparse polynomials

A problem, closely related to Prony’s problem is the reconstruction of sparse polynomi-
als, i.e., of polynomials of the form

f (x) =Xκ∈K

fκ xκ,

fκ ∈ C \ {0},

κ ∈ K,

where sparsity means that #K is (very) small relative to (cid:0)deg K+s

requirement is quite easy to achieve in several variables.

s

(cid:1) = Πdeg K. This

The “classical” method to reconstruct f from samples on Γ is the one from [2] and
uses a univariate Prony method together with divisibility aspects of relatively prime
numbers. A variant with unit roots and the Chinese remainder theorem can be found in
[8].

As shown in [28], it is easy to reduce this problem to Prony’s problem: let Θ ∈ Zs×s

be any nonsingular matrix, then

fκ eωT

κ α,

ωκ := log 2 Θκ,

which is Prony’s problem with Ω = {ωκ : κ ∈ A} which can be solved by considering
the Hankel matrices

f(cid:0)2Θα(cid:1) =Xκ∈K

fκ elog 2 (Θκ)T α =Xκ∈K
FA,B :=(cid:20)f(cid:0)2Θ(α+β)(cid:1) :

α ∈ A

β ∈ B (cid:21) .

If the coefﬁcients fκ of f belong to the Gaussian integers Z + iZ, which is the normal
assumption in symbolic computations, the evaluations in FA,B are rational numbers and
therefore also the ideal basis computed in the preceding section consists of symbolic
polynomials with coefﬁcients in Q + iQ. The same holds true for the multiplication

18

tables and only the joint eigenvalues have to be computed in numerical precision giving
the frequencies ωκ from which the exponents can be computed as

κ = rd(cid:18) 1

log 2

Θ−1ωκ(cid:19)

by rounding to the next integer.

7 Appendix

This section gives a detailed exposition of some of the algebraic results used in the pre-
ceding ones. We begin by pointing out that any monomial degree reducing interpolation
space automatically deﬁnes a natural H–basis.

Lemma 7.1 If A ⊂ Γ is a ﬁnite set such that ΠA is a degree reducing interpolation
space for X then the polynomials qα := (·)α − LA(·)α, β ∈ ∂A, form an H–basis of IX.

Proof: Deﬁne

qα := (·)α − LA(·)α,

α ∈ Γ,

(23)

and note that qα = 0 for α ∈ A and deg qα = |α| since A is degree reducing as well
as ∂αqβ
∂xα (0) = α!δα,β, α, β ∈ Ac := Γ \ A. Therefore the polynomials qα, α ∈ Γn \ A,
and (·)α, α ∈ An := A ∩ Γn form a basis of Πn for any n ∈ N. Consequently, any

polynomial p =P pα (·)α ∈ Π can be written as

p(x) = LAp(x) + p(x) − LAp(x) = LAp(x) + Xα∈Ac

pα qα(x)

and

p ∈ IX

⇔

p(x) = Xα∈Ac

pα qα(x).

(24)

The representation on the right hand side of (24) is an H–representation [14], hence the
polynomials {qα : α ∈ Ac}, form an inﬁnite H–basis, and qα : α ∈ Ac
n+1 :=
Ac ∩ Γn+1, where n := deg A = max{|α| : α ∈ A}, is a ﬁnite H–basis of IX.

n+1}, Ac

19

Next, we ﬁx j ∈ {1, . . . , s} and α ∈ Ac, and write p(x) := LA(·)α(x) ∈ ΠA as

qα+ǫj (x) − xjqα(x) = xα+ǫj − LA(·)α+ǫj (x) − xα+ǫj + xj LA(·)α(x)

pβ xβ+ǫj − LA(·)α+ǫj (x)

p(x) =P pβ xβ. Then,
= xj LA(·)α(x) − LA(·)α+ǫj (x) =Xβ∈A
= Xβ∈∂A
= Xβ∈∂A
= Xβ∈∂A

pβ−ǫj xβ + Xβ∈A∩(A+ǫj)
pβ−ǫj qβ(x) + Xβ∈∂A

pβ−ǫj qβ(x) + ˜p(x)

pβ−ǫj xβ − LA(·)α+ǫj(x)

pβ−ǫj LA(·)β(x) + Xβ∈A∩(A+ǫj)

pβ−ǫj xβ − LA(·)α+ǫj (x)

with some ˜p ∈ ΠA. The polynomial on the left hand side belongs to the ideal and
vanishes on X as do the qβ in the sum on the right hand side, hence ˜p(X) = 0 and
therefore, taking account on the lengths of the β appearing in the above decomposition,

qα+ǫj (x) − xjqα(x) = Xβ∈∂A∩Γn+1

cα,β qβ(x),

cα,β ∈ C.

(25)

Therefore, any qα with α ∈ Ac such that α−ǫj ∈ Ac has a H–representation by (·)jqα−ǫj
and qβ, α ∈ ∂A ∩ Γn. If, on the other hand, α − ǫj 6∈ Ac, j = 1, . . . , s, and α 6= 0,
then there must be some some j such that α − ǫj ∈ A and therefore α ∈ ∂A. Since any
nontrivial degree reducing set must contain 0, it follows that 0 6∈ Ac and therefore an
inductive application of the above process shows that any qα must eventually be written
as a linear combination of qβ, β ∈ ∂A ∩ Γ|α|.
(cid:3)
We recall the notion of a reduced polynomial. Given an inner product (·, ·) on Π, we
call a polynomial p reduced if each homogeneous term

pj(x) := X|α|=j

pαxα,

j = 0, . . . , deg p,

of p is perpendicular to the homogeneous leading forms in Λ(IX) ∩ Π0
pdeg p ∈ Π0
any polynomial p ∈ Π, a decomposition

j , where Λ(p) :=
deg p. As shown in [25] that whenever H is an H–basis for IX there exists, for

qh h + r,

deg qh + deg h ≤ deg p,

(26)

p =Xh∈H

such that r is reduced and depends only on IX and (·, ·) and is zero if and only if p ∈ IX.
Therefore, r can be seen as a well deﬁned mapping r : Π → Π. Also note that (26) is
the multivariate analog of euclidean division or division with remainder and that r is the
natural interpolant of p.

20

Lemma 7.2 If A ⊂ Γ is a ﬁnite set such that ΠA is a degree reducing interpolation
space for X then there exists an inner product (·, ·) such that ΠA = r(Π).

Proof: We use the H–basis qα, α ∈ Ac, deﬁned in (23) and deﬁne the inner product
separately on Π0
n, n = 0, 1, . . . If n < min{|α| : α ∈ Ac} and n > deg A, we
simply use the inner product of the coefﬁcients,

n × Π0

(p, p′)n := X|α|=n

pαp′
α,

p, p′ ∈ Π0
n.

For other values of n we ﬁrst observe that the polynomials xα, α ∈ A ∩ Γ0
leading forms Λ(qα), α ∈ Ac ∩ Γ0
n×r0
a nonsingular matrix Y ∈ Cr0
Gramian G := Y Y H is hermitian and positive deﬁnite. Deﬁning

n and the
n. We arrange the coefﬁcient vectors into
n := dim Π0

n, span Π0
n where r0

n = (cid:0)n+s−1

s−1 (cid:1) and note that the

(p, p′)n = pHG−1p = X|α|=|β|=n

(G−1)α,βpαp′
β,

p, p′ ∈ Π0
n,

we get that

(Y, Y )n = Y HG−1Y = Y H(Y Y H)−1Y = I

which means that the vectors eα, α ∈ A ∩ Γ0
of Λ(qα), α ∈ Ac ∩ Γ0

n. Consequently, the inner product

n are perpendicular to the coefﬁcient vectors

(p, p′) = Xj∈N0

(pj, p′

j)j,

p, p′ ∈ Π,

has the property that a polynomial is reduced if and only if it belongs to ΠA, that is,
ΠA = r(Π) as claimed.
(cid:3)

References

[1] W. Auzinger and H. J. Stetter, An elimination algorithm for the computation of all
zeros of a system of multivariate polynomial equations, Numerical mathematics,
Singapore 1988, Internat. Schriftenreihe Numer. Math., vol. 86, Birkh¨auser, Basel,
1988, pp. 11–30.

[2] M. Ben-Or and P. Tiwari, A deterministic algorithm for sparse multivariate poly-
nomial interpolation, Proc. Twentieth Annual ACM Symp. Theory Comput., ACM
Press, New York, 1988, pp. 301–309.

[3] C. de Boor, Interpolation from spaces spanned by monomials, Advances Comput.

Math. 26 (2007), 63–70.

21

[4] D. Cox, J. Little, and D. O’Shea, Ideals, varieties and algorithms, 2. ed., Under-

graduate Texts in Mathematics, Springer–Verlag, 1996.

[5] B. Diederichs and A. Iske, Parameter estimation for bivariate exponential
sums, IEEE International Conference Sampling Theorey and Applications, 2015,
pp. 493–497.

[6] M. D¨ohler, S. Kunis, and D. Potts, Nonequispaced hyperbolic cross fast Fourier

transform, SIAM J. Numer. Anal 47 (2009), 4415–4428.

[7] D. D˜ung, V. N. Temlyakov, and T. Ullrich, Hyperbolic cross approximation,

(2015), arXiv:1601.03978.

[8] M. Giesbrecht, G. Labahn, and W. Lee, Symbolic–numeric sparse interpolation of

multivariate polynomials, J. Symbolic Comput. 44 (2009), 943–959.

[9] L. Gonzales-Vega, F. Rouillier, M.-F. Roy, and G. Trujillo, Symbolic recipes for
real solution, Some Tapas in Computer Algebra (A. M. Cohen, H. Cuypers, and
M. Sterk, eds.), Algorithms and Computations in Mathematics, vol. 4, Springer,
1999, pp. 121–162.

[10] S. Kunis, Th. Peter, T. R¨omer, and U. von der Ohe, A multivariate generalization

of Prony’s method, (2015), arXiv:1506.00450v1.

[11] G. G. Lorentz, Approximation of functions, Chelsea Publishing Company, 1966.

[12] Ch. Lubich, From quantum to classical molecular dynamics: Reduced models and

numerical analysis, European Mathematical Society, 2008.

[13] H. M. M¨oller and B. Buchberger, The construction of multivariate polynomials
with preassigned zeros, Computer Algebra, EUROCAM ’82, European Computer
Algebra Conference (G. Goos and J. Hartmanis, eds.), Lecture Notes in Computer
Science, vol. 144, Springer Verlag, 1982, pp. 24–31.

[14] H. M. M¨oller and T. Sauer, H–bases for polynomial interpolation and system solv-

ing, Advances Comput. Math. 12 (2000), no. 4, 335–362, to appear.

[15]

, H–bases I: The foundation, Curve and Surface ﬁtting: Saint–Malo 1999
(A. Cohen, C. Rabut, and L. L. Schumaker, eds.), Vanderbilt University Press,
2000, pp. 325–332.

[16] H. M. M¨oller and H. J. Stetter, Multivariate polynomial equations with multiple

zeros solved by matrix eigenproblems, Numer. Math. 70 (1995), 311–329.

22

[17] H. M. M¨oller and R. Tenberg, Multivariate polynomial system solving using inter-

sections of eigenspaces, J. Symbolic Comput. 32 (2001), 513–531.

[18] G. Plonka and M. Tasche, Prony methods for recovery of structured functions,

GAMM–Mitt. 37 (2014), 239–258.

[19] D. Potts and M. Tasche, Parameter estimation for multivariate exponential sums,

Electron. Trans. Numer. Anal. 40 (2013), 204–224.

[20]

, Fast ESPRIT algorithms based on partial singular value decompositions,

Appl. Numer. Math. 88 (2015), 31–45.

[21] C. Prony, Essai exp´erimental et analytique sur les lois de la dilabilit´e des ﬂuides
´elastiques, et sur celles de la force expansive de la vapeur de l’eau et de la vapeur
de l’alkool, `a diff´erentes temp´eratures, J. de l’ ´Ecole polytechnique 2 (1795), 24–
77.

[22] R. Roy and Th. Kailath, ESPRIT – estimation of signal parameters via rotational
invariance techniques, IEEE Trans. Acoustics, Speech and Signal Processing 37
(1989), 984–995.

[23] T. Sauer, Polynomial interpolation of minimal degree, Numer. Math. 78 (1997),

no. 1, 59–85.

[24]

[25]

[26]

[27]

, Polynomial interpolation of minimal degree and Gr¨obner bases, Groeb-
ner Bases and Applications (Proc. of the Conf. 33 Years of Groebner Bases)
(B. Buchberger and F. Winkler, eds.), London Math. Soc. Lecture Notes, vol. 251,
Cambridge University Press, 1998, pp. 483–494.

, Gr¨obner bases, H–bases and interpolation, Trans. Amer. Math. Soc. 353

(2001), 2293–2308.

, Lagrange interpolation on subgrids of tensor product grids, Math. Comp.

73 (2004), 181–190.

, Polynomial interpolation in several variables: Lattices, differences, and
ideals, Multivariate Approximation and Interpolation (M. Buhmann, W. Haus-
mann, K. Jetter, W. Schaback, and J. St¨ockler, eds.), Elsevier, 2006, pp. 189–228.

[28]

, Prony’s method in several variables, Submitted for publication (2015),

arXiv:1602.02352.

[29] R. Schmidt, Multiple emitter location and signal parameter estimation, IEEE

Transactions on Antennas and Propagation 34 (1986), 276–280.

23

[30] H. J. Stetter, Matrix eigenproblems at the heart of polynomial system solving,

SIGSAM Bull. 30 (1995), no. 4, 22–25.

24

