6
1
0
2

 
r
a

M
2

 

 
 
]

O
R
.
s
c
[
 
 

1
v
2
5
6
0
0

.

3
0
6
1
:
v
i
X
r
a

Physics-Based Damage-Aware

Manipulation Strategy Planning

Using Scene Dynamics Anticipation

Tobias Fromm and Andreas Birk ∗

March 3, 2016

Abstract

We present a damage-aware planning approach which determines the best sequence to
manipulate a number of objects in a scene. This works on task-planning level, abstracts
from motion planning and anticipates the dynamics of the scene using a physics simulation.
Instead of avoiding interaction with the environment, we take unintended motion of other
objects into account and plan manipulation sequences which minimize the potential damage.
Our method can also be used as a validation measure to judge planned motions for their
feasibility in terms of damage avoidance. We evaluate our approach on one industrial scenario
(autonomous container unloading) and one retail scenario (shelf replenishment).

1 Introduction

Since no robot is perfect, just like the humans building them, autonomous manipulation can be
destructive if fragile goods have to be handled. Many compliant grippers, sophisticated perception
systems and manipulation routines have been developed recently, mostly relying on consequent
obstacle avoidance. This works to a certain degree, but why not take alteration of the environment
into account for planning instead of bluntly avoiding interaction with it?

We present a damage-aware manipulation strategy planning approach which validates and
selects the best sequence to remove or unload a number of objects from a scene. The idea relies
on anticipating the dynamics of the scene using a physics simulation which contains a copy of the
physical scene, including the robot, the environment and a number of objects to manipulate.

The possibilities are obvious: One the one hand, motion planning is facilitated because of a less
constrained search space in contrast to regarding all other objects as obstacles. In some scenes,
it may be hard or even impossible to perform manipulation if no collisions are permitted. On the
other hand, we want to avoid damage to the possibly heavy or fragile goods as well as the robot
itself which may occur if an object is shifted or dropped unintendedly.

Hence, the goal of our method is to optimize a robot’s autonomous behavior according to the

anticipated dynamics of the real-life scene.

We evaluate our approach on two everyday scenarios shown in Figure 1: logistics, in the context
of the EU project “Cognitive Robot for Automation of Logistics Processes” (RobLog) [1] where
containers are unloaded autonomously, and in a supermarket for shelf replenishment [2].

The contribution of our work is:
• a method to plan strategies for the manipulation of a sequence of objects
• minimizing damage while dynamically considering the physics of all objects
• including an industrial and a retail application scenario which show that our approach can

be integrated into arbitrary scenarios.

∗The authors are with the Robotics Group, Computer Science & Electrical Engineering, Jacobs University
Bremen, Germany; t.fromm@jacobs-university.de. The research leading to the presented results has received funding
from the European Union’s Seventh Framework Programme (EU FP7 ICT-2) within the project “Cognitive Robot
for the Automation of Logistic Processes (RobLog)”.

1

(a) Logistics (container unloading) [1]

(b) Supermarket (shelf replenishment) [2]

Figure 1: Example application scenarios for autonomous manipulation using Scene Dynamics
Anticipation

2 Foundations and Related Work

2.1 Physics-Based Motion Planning

Motion planning based on physics simulation has been investigated in several publications, amongst
these, Zickler and Veloso’s work [3].
Instead of conventionally planning motions for a robot’s
physical degrees of freedom, they group low-level motions and use them as a single point in action
selection sampling space. As for the environment representation, Zickler and Veloso state that it
is necessary to deﬁne domain-appropriate distance functions between states of the scene in order
to be able to compare them. For their usage domain of robot soccer and minigolf, the Euclidean
distance serves as a suﬃcient measure in most cases.

Weitnauer et al. [4] present an approach which works in a similar way, though only on plain

objects and using only the Euclidean distance as a distance function.

Another usage example of physics-based planning can be found in [5] and [6] where the authors
present methods of planning grasps through clutter, taking shifting objects into account and ex-
plicitly manipulating them. This is rather similar to our approach, though not with the intention
to ﬁnd an eﬃcient manipulation order, but to clear manipulation paths from obstructing objects.
Before using one of these approaches, the object to manipulate would have to be selected
by some algorithm or a human operator. On the contrary, our method automates exactly this
scenario where not only physics-based manipulation planning takes place, but also autonomous
object selection to plan manipulation sequences on a high level.

In addition, in the scenarios intended to be tackled with our approach, intentionally pushing
objects aside with the gripper poses an additional risk of damaging the goods and even the gripper.
For this reason, we want to limit the movement of any other than the object to manipulate as much
as possible. Our approach takes care of this by penalizing any movement of a passive object, i.e.
an object which is not subject to intended manipulation at the moment. Therefore, intentional as
well as unintentional damage-prone manipulation of passive objects is avoided.

2

2.2 Manipulation Strategy Planning

In contrast to the methods described so far, our approach does not aim on planning motions, but
high-level behaviors with the help of a physics simulation. This means that, instead of giving
an explicit trajectory, our method will a) validate whether an ongoing manipulation is likely to
be successful or b) plan a sequence of manipulation actions to perform in the desired scenario.
Therefore, our possible application scenarios are separated into Manipulation Outcome Prediction
and Manipulation Sequence Planning which will be introduced in the following.

Task Outcome Prediction Covered by this term, several methods have shown diﬀerent ideas
and scenarios. Unfortunately, most of the existing work focuses on predicting the outcome of
motions instead of behaviors, like Pastor et al.’s work [7] where robot parameters like joint positions,
velocities, forces etc. are continuously monitored and veriﬁed whether they lie within a learned
envelope. Haidu et al. [8] use a similar technique, also to learn motoric skills.

In contrast to these works, Rockel et al. [9] present an interesting idea of predicting task success
on the basis of motion of the manipulated objects. Concretely, their application includes a robot
which maximizes its speed moving around in a scene, but without losing the balance of an object
which sits on its base. This works on the basis of semantic predicates, but with hand-crafted
features which describe if the object shakes or topples and which cannot easily be employed on a
diﬀerent scenario.

Akhtar et al. [10] also use semantic predicates to reason about success of a manipulation action.
They present a Machine Learning algorithm which is able to predict whether an object released on
top of another will behave as expected. This detection of external faults can predict the behavior
of a concrete action based on a simulated training set, but again uses hand-crafted predicates
which only cover this particular action. Our approach, on the contrary, aims on covering implicit
manipulation of passive objects as well, independent of which exact manipulation is performed on
the scene.

Manipulation Sequence Planning In order to manipulate objects which are placed currently
unreachable behind others, Stilman et al. [11] use a sampling-based planner to move away the
blocking objects.

Okada et al. [12] present a similar simple, but eﬀective example for strategy planning for a
humanoid robot where obstacles are considered as planning goals and subsequently moved away
from the desired walking path. Their goal is to enable robots to implicitly perform necessary
manipulation actions, even if these actions were not part of the original task plan. This is similar
to our recent work evaluated in a diﬀerent scenario [2], but diﬀerent to what we present in this
work. Here we use a dynamics simulation to evaluate the feasibility of a manipulation action,
taking unintended side eﬀects into account in addition to static spatial knowledge.

3 Prerequisites

Simulating the behavior of robots and objects requires both high accuracy and some abstractions
since not every detail in robot and object properties can easily be modeled. Since our approach
is intended to work in complex scenarios based on perception systems for object recognition and
real robots for motion execution, we need to be sure that these work error-free and take care of
the respective noise where it occurs. Therefore, we assume object models and environment to be
given as well as the state of the robot.

3.1 Simulation Scene Composition

In order to build the scene conﬁguration used in the physics simulation, we ﬁrst recognize and
localize all objects in the scene using our perception system [1] [13]. This system includes a
pipeline of several segmentation and ﬁltering steps before it executes diverse recognition modules,
including a feature-based textured object recognition module [14] and a graph-based shape model
recognition module [15].

3

3.2 Motion Planning, Grasp Planning and Execution

After all objects have been perceived, the respective grasps need to be planned, evaluated and
ﬁnally executed by the robot. The way of determining grasping conﬁgurations strongly depends
on the objects and gripper used in the scenario since diﬀerent object sizes and structure demand
for diﬀerent gripper sizes and amounts of dexterity. Since the focus in this work does not lie on
grasp and motion planning and our method is supposed to be re-used in diﬀerent scenarios, we use
simple grasping conﬁgurations around the principal object axes.

The physics simulation uses a kinematics and dynamics model of the robot which has been
developed together with the respective simulated controllers. For the supermarket scenario, we
use the standard PR2 simulation model. The eventual generation of the motion trajectories can
be performed, for example, by using MoveIt!.

4 Manipulation Cost Functions

4.1 Motivation

The usage of cost functions to ﬁnd the most suitable and reasonable solution to a planning problem
has been common practice for a long time (e.g. in [12], [16]) to be able to evaluate the eﬀects of a
particular action. In order to apply this method to a speciﬁc planning problem, domain knowledge
has to come into play which accounts for the optimization target of the respective problem.

Since our approach aims on reducing unintended motions of passive objects, i.e. objects which
are not subject to intended manipulation at the moment, we need to consider cost functions which
cover spatial modiﬁcations in the scene.

4.2 Terminology
For all of the following deﬁnitions, we use O as the set of objects present in the scene, α ∈ O as
the active object (which will be manipulated), Φ = O \ α as the set of passive objects (which will
not be manipulated) and φ ∈ Φ as one member of this set.

4.3 Pose-Based Cost Functions

The following two simple cost functions are based on object poses and allow for a quick validation
without a lot of processing, but may not be predictive-eﬃcient enough.

Maximum Pose Shift cp describes the maximum Euclidean distance between object poses:

(1)
where pt(φ) ∈ R6 is the pose of passive object φ before (at time t) and pt+1(φ) after running the
simulation step (at time t + 1). This is simple and easy to compute, but smoothes out non-straight
motion paths and thus does not describe changes in an object’s direction of movement.

cp = max
φ∈Φ

(cid:107)pt+1(φ) − pt(φ)(cid:107)

Maximum Path Length cl
during simulation:

is the maximum length of the path any object’s centroid traveled

n(cid:88)

i=1

cl = max
φ∈Φ

(cid:107)pi(φ) − pi−1(φ)(cid:107)

(2)

where pi(φ) ∈ R6 is a pose of passive object φ during the simulation step (= between time steps t
and t+1) and n is the number of poses covered during simulation. This takes into account diﬀerent
directions of movement, but not a possible spin the object may have been exposed to.

4.4 Volume-Based Cost Functions

The following cost functions are based on the volume of the objects instead of their pose, thus
considering complex motion paths as well as changes of movement direction and spin.

4

(a) initial conﬁguration

(b) ﬁnal conﬁguration

(c) Pose Shift (dashed line), Path Length (dotted line) and
Swept Convex Volume (yellow)

Figure 2: Visualization of the Swept Convex Volume compared to the Pose Shift and Path Length
covered by a passive object during manipulation. When the right box was pulled out by the robot,
the cylindrical container fell down and rolled away, causing costs like shown.

Maximum Swept Convex Volume cv Swept Volume estimations can be used for collision
detection [17] as well as space occupancy calculation for automation and production purposes [18].
For a mathematical formulation see the survey of Abdel-Malek et al. [19].

The object’s surface mesh is used as the generator which follows a trajectory (a set of poses
covered while moving during simulation runtime) and creates the swept volume (the outer boundary
of the object during its motion). This can be considered a voxel-based variant of [19] which is widely
used in applications where generator and trajectory are discrete.

In most applications, this volume basically represents a concave hull around all points in space
which the object ever touched. However, this takes a lot of eﬀort to compute [18] and, since the
concave hull of a number of points is not generally well-deﬁned, may raise ambiguities for diﬀerent
kinds of objects. As a remedy, we simpliﬁed the original Swept Volume approach by replacing the
concave with the convex hull which is easy to compute and well-deﬁned, and call the result the
Swept Convex Volume. Figure 2 shows a visualization of the Swept Convex Volume of a moving
object compared to its Pose Shift and Path Length.

Since we relate all objects in a scene, we take the Maximum Swept Convex Volume cv which is

the maximum over all Swept Convex Volumes of the scene objects. It is determined as

where V (φ) is the Swept Convex Volume of the respective object φ ∈ Φ as computed in Algorithm 1.

cv = max
φ∈Φ

V (φ)

(3)

Maximum Weighted Swept Convex Volume cw Since the desired usage of our method puts
special focus on fragile and vulnerable goods, we propose an additional cost function based on
cv, but with additional weights for each of the the 3-D axes. These weights can be adapted to
the domain, e.g. to punish vertically dropping objects like when they fall oﬀ a shelf or out of a
container.

As for the implementation of such weights, in Algorithm 1 we need to replace pi(φ) in Line 4

with the weighted pw

i (φ):

i (φ) = p0(φ) + (pi(φ) − p0(φ)) · w
pw

(4)

5

Algorithm 1 Swept Convex Volume calculation
1: input: object mesh M(φ), object poses p0..n(φ) covered during simulation
2: create point cloud C(φ) from M(φ) at p0(φ)
3: for all pi(φ) do
4:
5:
6: end for
7: compute convex hull H(φ) of C(φ)
8: V (φ) ← volume of H(φ)
9: output: Swept Convex Volume V (φ)

Ci(φ) ← C transformed from p0(φ) to pi(φ)
C(φ) ← C(φ) ∪ Ci(φ)

where w ∈ R3 = (cid:2)wx wy wz

(cid:3)(cid:124)

are domain-dependent weights for the x, y, and z direction,

respectively.

5 Manipulation Outcome Prediction

Scene Dynamics Anticipation as a validation method for planned actions can be used in runtime-
restricted scenarios and scenarios where another planner has already determined which objects to
manipulate in which order. One example for this is the planner presented by Mojtahedzadeh et
al. [20] which uses static equilibrium calculations to ﬁnd objects that support each other, hence it
will eventually select objects ﬁrst which do not support any other object.

Implicitly, our approach will usually determine similar manipulation sequences, but in addition
considers dynamic events occuring during the manipulation for which Mojtahedzadeh et al.’s ap-
proach is not equipped. Consequently, using our method for validating plans on dynamic scenarios
can be seen as an addition and enrichment to another planner.

Algorithm 2 shows how we perform Outcome Prediction using Scene Dynamics Anticipation

with the deﬁnitions and cost functions given in the previous sections.

Algorithm 2 Manipulation Outcome Prediction
1: input: scene objects O, active object α ∈ O
2: spawn all objects O in simulation
3: plan approach T α
1 and extract T α
4: move simulated robot on T α
1
5: grasp active object α
6: move simulated robot on T α
2
7: release active object α
8: c ← manipulation costs (→ Section 4)
9: output: manipulation costs c

2 trajectory

One drawback of using Scene Dynamics Anticipation as a validation method, however, is the
fact that it outputs the manipulation costs for the given scene conﬁguration, but classiﬁcation
between positively and negatively validated actions has to be performed manually via a threshold.
This is a common problem for Outcome Prediction methods in general, also for Rockel et al.’s
approach [9] where thresholds are deﬁned manually for which an object is considered to be toppling.
Another example is [7] where the authors perform statistical tests on whether a planned motion
lies within a learned envelope. Whenever such an approach is deployed on a new scenario, these
parameters have to be set accordingly.

We empirically determined the manipulation cost threshold based on satisfactory operation of
our method. For the logistics scenario, for instance, where the container is low above ground and
provides lots of operation space, a threshold of cw = 2.0 gives satisfactory results. The implicit
lower limit for this value is cw = 1.0 which equals the volume of the object. Since jitter can always
occur in simulation and small pushes usually do not cause any damage, setting this value too close
to 1.0 will prevent the algorithm from positively evaluating any conﬁguration. On the other hand,
too loose settings should be avoided.

6

Level 4
(1 node)

Level 3
(4 nodes)

Level 2

(12 nodes)

Level 1

(24 nodes)

Figure 3: Example search tree (other branches cropped for better visibility) – each node shows its
initial conﬁguration, i.e. before manipulation

6 Manipulation Sequence Planning

In addition to Manipulation Outcome Prediction, we now want to introduce a way to include Scene
Dynamics Anticipation into planning a sequence of manipulation actions.

Instead of serving as a validation tool only, our method is able to plan a manipulation sequence
which considers side-eﬀects of moving objects.
In addition, obstacle-avoiding motion planning
may fail in certain scenarios. But since we explicitly allow for moving passive objects during
manipulation, though trying to minimize it, our high-level planner may still succeed in ﬁnding a
viable manipulation sequence.

In our previous work [1], we have sketched the diﬃculties and pitfalls of motion planning
in heavily conﬁned spaces, i.e. when all passive objects are considered as obstacles. This may
even lead to a dead end in manipulating any remaining object if the constraints are too tight.
Manipulation Sequence Planning using Scene Dynamics Anticipation, on the other hand, allows
for passive objects to move and takes their motion into account as a feature, thus getting stuck in
such a case is less likely.

6.1 Planning Procedure

Our method uses a search tree built from all possible objects conﬁgurations which can occur while
manipulating all objects in the scene, see the example in Figure 3. The initial conﬁguration is
shown as the tree root, on the top, while further down the tree one object has been unloaded in
each node. The images show the respective initial conﬁguration for these nodes, before performing
the manipulation step.

While traversing the tree further down, unintended motion and high costs are caused for the
pink container which falls oﬀ the blue box when this one is manipulated. The leaves merely contain
one remaining object which can directly be manipulated disregarding the scene physics since no
passive objects are left.

Algorithm 3 shows how to generate the search tree for a scene given the initial object con-
ﬁguration. After the tree has been ﬁlled, traversing it works like shown in Algorithm 4, using a
depth-ﬁrst search-like technique.

As for the cost functions used for calculating the node costs, all those introduced in Section 4
can in principle be used interchangeably here. Since one of the main topics of this paper is damage
avoidance, however, we prefer the Maximum Weighted Swept Convex Volume cw because it takes
into account a) complex motion paths and b) change of movement direction and spin. Both of

7

Algorithm 3 Search Tree Generation for Sequence Planning
1: initialize search tree S0 ← ∅
2: initialize set of objects O0 with current scene
3: procedure createNode(Si, Oi)

if |Oi| ≤ 2, i.e. this is a leaf node then

else

return
create new tree node Ni containing object set Oi
Si+1 ← Si ∪ Ni
determine new active object αi ∈ Oi
Oi+1 ← Oi \ αi
return createNode(Si+1, Oi+1)

8:
9:
10:
11:
12:
13: end procedure
14: output: ﬁlled search tree S

end if

4:
5:
6:
7:

3:
4:
5:
6:
7:
8:

Algorithm 4 Manipulation Sequence Planning
1: input: scene objects O, search tree S generated from O (→ Algorithm 3)
2: for all nodes Ni ∈ S do

1 and extract T αi

spawn all objects Oi in simulation
determine active object αi ∈ Oi
plan approach T αi
move simulated robot on T αi
1
grasp active object αi
move simulated robot on T αi
2
release active object αi
ci ← manipulation costs (→ Section 4)

2

trajectory

9:
10:
11: end for
12: for all leaf nodes N leaf
cj ← summed-up costs of N leaf
13:
cmin ← min(cmin, cj)
14:
15: end for
16: output: node Nj with lowest manipulation costs cmin

∈ S do

’s parents

j

j

these factors are a crucial ingredient to avoid objects toppling or dropping under all circumstances.
Using standard, unchanged A∗ search or something alike with a heuristic evaluation function
f (n) = c(n)+h(n) where c(n) are the costs accumulated so far for a node n, h(n) being the heuristic
cost estimation until the target, does not work here because we cannot deﬁne a reasonable h(n)
for our problem which gives the distance (in costs) to the target conﬁguration. For A∗, this cost
estimation is often related to a distance in a metric space for goal paths of diﬀerent length. This
is not applicable here because our goal paths always have the same length, that is, manipulating
all objects present in the scene. Additionally, no reasonable and, in terms of its mean, correctly
estimating heuristic can be given for objects which may fall oﬀ a shelf or roll away when pushed
accidently.

6.2 Eﬃciency Considerations
In contrast to common search methods like A∗, discretized Rapidly-Exploring Random Trees (RRT)
[21] or Rapidly-Exploring Random Leafy Trees (RRLT) [21], our scenario causes more of an eﬃ-
ciency issue in estimating the node costs instead of the search itself. Searching the tree is rather
trivial because the number of nodes which are handled in our search trees can be processed rapidly
on modern machines. Thus, the eﬃcient search problem is overshaded by a heuristic cost estimation
problem which presents itself much more prominent and crucial to solve in minimal time.

The worst-case tree size for a number of objects n, constructed from all possible permutations

8

of the object set like in Algorithm 3, is

n(cid:88)

i=1

n!
i!

= n! +

n!
2

+ ... +

n!

(n − 1)!

+ 1

(5)

where n!
i!
level (i.e. leaves), and i = n representing the top level (i.e. root); see the example in Figure 3.

is the number of objects for the respective tree level i, i = 1 representing the bottom

This number grows fast with a rising number of objects, taking along the cost estimation
problem with the same speed. A possible remedy for this is to partition the scene into piles of 4-5
objects ﬁrst and plan a manipulation sequence for one these piles, then advance to the next pile.
However, so far, how to optimize these partitions still remains an open research question.

In addition to not using more than 4-5 objects at a time, we cut oﬀ as many branches of the
search tree as possible in which ultimately there is no possibility to present the optimal solution.
In the following, we will present several ideas to reduce the tree size as far as possible. In Section 7
we show that the tree size in a typical scenario can be reduced to as low as 48.7% using these
measures.

6.2.1

Implicit Search Tree Pruning

As we will show by an empiric consideration in the Results section, in every scene there is a number
of conﬁgurations which implicitly cannot be simulated. Usual reasons for this are that approaching
an object may push a passive object which, in turn, moves the active object away by a signiﬁcant
distance.
In this case, the active object will end up unreachable for the planned manipulation
action.

Additionally, sometimes no feasible grasping conﬁguration can be found without the robot
colliding with the environment (container, shelf, etc.). Since motion planning is out of scope of
our work, we have to skip simulating the respective conﬁguration in this case and impose inﬁnite
costs.

6.2.2 Explicit Search Tree Pruning

In addition to the implicit reasons given above, since we use depth-ﬁrst search, there is another
possibility to crop tree branches. After simulating a conﬁguration, if the accumulated costs in the
current branch exceed the total costs of any already computed goal sequence (i.e. a tree leaf), we
will stop exploring the current branch.

6.2.3 Re-Use of Similar Conﬁgurations

Another way to reduce computational load, in addition to pruning tree branches, is to re-use similar
conﬁgurations which occured before somewhere else in the tree. Before running the simulation, we
compare the current conﬁguration to each one which was simulated before, anywhere in the tree,
for similarity of objects and their poses. If there is a similar conﬁguration, we re-use this node and
the whole child tree of the node (if any) without having to simulate any of them.

6.3 Open-Loop vs. Closed-Loop Planning

The method we propose basically allows for open-loop planning, where the manipulation sequence
is planned once and then executed without further anticipation of scene dynamics, or closed-loop,
where the whole process is repeated from the beginning after manipulation of any object. Re-
planning the entire remaining manipulation sequence after each perception/manipulation cycle
in a closed loop has the advantage of covering object movement which stems from to inaccurate
physics simulation or other, external disturbances in the scene. However, time constraints may
not allow for running Manipulation Sequence Planning after each manipulation cycle, so using the
results only as an initial plan is a viable alternative.

9

(a) Logistics

(b) Supermarket

Figure 4: Typical scenes encountered in our application scenarios

(a) Scene 1

(b) Scene 2

(c) Scene 3

(d) Scene 4

Figure 5: Experimental scenes used for evaluating our method

7 Applications and Experimental Results

In this section, we want to evaluate our method applied on the two practical scenarios of Figure 1
for which typical scenes are shown in Figure 4.

As for the ﬁrst scenario, logistics is a ﬁeld where many diﬀerent goods have to be handled, some
of which are heavy, bulky, fragile or otherwise damage-prone. Our previous work on unloading
shipping containers [1] provides an ideal application in this respect, so we have modeled the robot,
container and objects from this scenario as our ﬁrst example.

Secondly, domestic and retail robotics provide another playground for manipulation strategy

planning, thus the supermarket scenario of [2] will serve as the second example.

7.1 Prerequisites

For evaluating our method, we take several modalities as granted and thus abstract from them
because they fall out of the scope of this work.

First of all, since the perception and grasping accuracy of our previous work have been validated

10

y
c
n
e
u
q
e
r
F

15

10

5

0

y
c
n
e
u
q
e
r
F

15

10

5

0

y
c
n
e
u
q
e
r
F

15

10

5

0

y
c
n
e
u
q
e
r
F

15

10

5

0

1
-
0
-
2
-
3

1
-
0
-
3
-
2

1
-
2
-
0
-
3

0
-
1
-
3
-
2

0
-
2
-
3
-
1

3
-
2
-
0
-
1

2
-
3
-
0
-
1

2
-
3
-
1
-
0

3
-
0
-
2
-
1

3
-
0
-
1
-
2

3
-
2
-
0
-
1

2
-
3
-
0
-
1

2
-
0
-
3
-
1

2
-
0
-
1
-
3

3
-
0
-
2
-
1

1
-
3
-
0
-
2

0
-
1
-
2
-
3

3
-
1
-
0
-
2

3
-
0
-
1
-
2

1
-
3
-
2
-
0

3
-
2
-
0
-
1

3
-
0
-
2
-
1

1
-
3
-
0
-
2

2
-
1
-
0
-
3

2
-
0
-
1
-
3

0
-
2
-
1
-
3

(a) Scene 1

(b) Scene 2

(c) Scene 3

(d) Scene 4

Figure 6: Frequencies of ﬁrst-ranked manipulation sequences selected by our method

extensively already in [1], [13], [14] and [15], we are not going into detail on these measures anymore.
Abstracting from details in the object models is necessary as well because we would like to
show the general concept of our method in this work. Although it is desirable to obtain simulation
models as close as possible to the real objects, this is not an easy task and constitutes an own
branch of research. Because of this, we basically use the rigid objects from our real scenarios in
Figure 4, but with a simpliﬁed representation in the simulation like shown in Figure 5. For the
same reason, we do not employ complex grasping techniques here, but instead generate simple
grasping conﬁgurations around the principal axes.

Although we present a general approach which is not limited to scenarios where damage-
awareness is the main focus, we want to emphasize the importance of this factor in our applications.
Thus, we use the Maximum Weighted Swept Convex Volume cw as a cost function which explicitly
penalizes covered space. The weights w for cw we set on

w =(cid:2)1.0

2.0(cid:3)(cid:124)

1.0

which puts increased importance on damage-prone vertical motion. This is desired especially in
scenarios like a supermarket where objects would break if dropped from a shelf.

7.2 Example Scenes

We want to show how our method works by running it many times on diﬀerent scenes and com-
paring the ﬁnal manipulation sequences. Additionally, we show what happens to the search tree
which can be heavily pruned during traversal.

Logistics Scenes 1 and 2 show typical scenarios encountered in RobLog [1] where Scene 2 is the
more challenging one due to the goods supporting each other. Hence, once any of the objects is
manipulated it is likely that one of the others will move as well.

Supermarket The tall shelf, along with the cans which may roll away if dropped, is a hostile area
for any damage-prone product and thus a good example for our method. The tipped-over items
in Scene 4 occur frequently in a supermarket where customers leave the shelf like this, products
coming to rest partly on top of each other.

7.3 Manipulation Sequence Planning Results

We ran our method 25 times each on the two scenes per application, resulting in a total of 100
runs. Figure 6 shows a histogram of how often a particular manipulation sequence was selected
for a particular scene of Figure 5.

Numerical results are shown in Table 1 with their means (µ) and standard deviations (σ) for

the following criteria:

11

• mean ﬁrst-ranked costs per node: mean over 25 runs of the mean costs per object imposed

on the manipulation sequence selected by Algorithm 4

• mean second-ranked costs per node: mean over 25 runs of the mean costs per object imposed

on the second-best manipulation sequence

• pruned tree nodes: percentage of nodes pruned from the tree, resulting in a similar reduction

in runtime, and composed of the following sub-criteria:

– known subtree: a node shares its object conﬁguration and poses with a previously pro-

cessed node, thus the costs were copied without re-simulating

– costs exceed existing sequence: a solution is existing already which has lower costs than

the costs accumulated so far in this branch

– active object moved : the active object was pushed away during approach, ending up

unreachable

– object out of workspace: an object fell out of the workspace (container/shelf), causing

maximum damage and ending up unreachable for the robot

– planning failure: no motion plan was found for the active object, e.g. because it was

pushed away too far in a parent node

• nodes with signiﬁcant movement: percentage of nodes which were not pruned from the tree

and reported signiﬁcant costs higher than a manually deﬁned threshold

7.4 Discussion

The results in Figure 6 show that, over many runs, of the possible number of sequences only a
small selection is considered as best. The fact that not the same sequence is reported for each run,
but there is obviously some noise in the selection stems mostly from the used motion planning
techniques. These rely on random trees and thus deliver noisy results for similar inputs. This
way, diﬀerent grasping conﬁgurations are selected for the same object conﬁguration, resulting in
diﬀerent motions even if the scene looks similar.

Nevertheless, the resulting manipulation sequences are valid and reasonable in a way that they
comply with the intuitive sequence a human would use to clear the container/shelf: unloading the
objects ﬁrst which do not physically support others. So, even if there is some distribution of the
ﬁnally selected manipulation sequence due to reasons that cannot be inﬂuenced by our planning
approach, our method succeeds in selecting reasonable solutions for all scenes.

The results show that generally a major ratio of pruned nodes results from comparing the
current costs to the already planned sequences. Therefore, this is one of the most useful tricks we
use to reduce computational load.

In Scene 3, items falling oﬀ the shelf are the major reason for tree pruning: 31.9% of nodes
featured an object which moved out of the robot’s workspace. The robot used in the logistics
scenario has a big workspace in all directions, additionally, the container provides physical limits
on three sides, thus this did not happen for Scenes 1 and 2.

The high values for the standard deviations of pruned nodes result from all the children of such
a node automatically being cut oﬀ. This way, any disturbance propagates down through the tree
and may aﬀect many nodes at the same time when it happens in a high-level node.

8 Conclusion

We presented a method to plan manipulation sequences for a number of objects, depending on
their physical behavior. This enables autonomous robots to anticipate an optimal manipulation
order to avoid potential damage by taking into account unintended movement of other objects.

We evaluated our approach on two everyday scenarios showing good results for scenes of diﬀerent
physical complexity and diﬀerent scenarios. Since the planning technique is abstract, given the
scene was perceived correctly, it is universally usable on any other autonomous robot scenario.

12

l
a
t
o
T

σ

5
3
4
.
0

8
7
8
.
3

%
1
.
4

%
0
.
6
1

%
2
.
3
1

%
9
.
5
1

%
6
.
8

µ

2
8
3
.
1

1
5
0
.
2

%
3
.
1
5

%
8
.
2

%
8
.
3
2

%
9
.
0
1

%
6
.
9

%
2
.
4

σ

1
2
3
.
0

3
9
3
.
7

%
0
.
0

%
1
.
8
1

%
2
.
9

%
6
.
9

%
9
.
0
1

4

µ

5
7
9
.
1

4
6
7
.
3

%
9
.
1
6

%
0
.
0

%
0
.
1
4

%
1
.
8

%
0
.
7

%
8
.
5

σ

9
5
3
.
0

8
2
7
.
1

%
2
.
2

%
7
.
4
1

%
6
.
1
1

%
7
.
5
1

%
1
.
3

3

µ

9
4
3
.
1

0
0
7
.
1

%
3
.
8
5

%
2
.
1

%
3
.
9
1

%
2
.
4

%
7
.
1

%
9
.
1
3

s
t
l

u
s
e
R

2

l
a
c
i
r
e
m
u
N

:
1

e
l

b
a
T

σ

4
7
0
.
0

6
2
3
.
0

%
5
.
2

%
3
.
5

%
0
.
0

%
4
.
8

%
5
.
3
1

µ

8
0
2
.
1

9
2
4
.
1

%
4
.
0
5

%
6
.
2

%
3
.
8
1

%
2
.
5
2

%
0
.
0

%
3
.
4

σ

8
0
0
.
0

8
6
8
.
0

%
8
.
4

%
3
.
8

%
6
.
5

%
0
.
0

%
6
.
9

1

µ

8
0
0
.
1

7
3
3
.
1

%
8
.
4
3

%
5
.
7

%
5
.
6
1

%
8
.
5

%
0
.
0

%
0
.
5

%
0
.
5

%
9
.
4
1

%
1
.
5

%
9
.
8
1

%
3
.
4

%
8
.
4
1

%
9
.
4

%
9
.
1
1

%
0
.
3

%
8
.
3
1

e
d
o
n

r
e
p

s
t
s
o
c

d
e
k
n
a
r
-
d
n
o
c
e
s

n
a
e
m

e
d
o
n

r
e
p

s
t
s
o
c

d
e
k
n
a
r
-
t
s
r
ﬁ

n
a
e
m

s
e
d
o
n

e
e
r
t

d
e
n
u
r
p

e
c
n
e
u
q
e
s

g
n
i
t
s
i
x
e

d
e
e
c
x
e

s
t
s
o
c

e
e
r
t
b
u
s

n
w
o
n
k

e
c
a
p
s
k
r
o
w

f
o

t
u
o

t
c
e
j
b
o

d
e
v
o
m

t
c
e
j
b
o

e
v
i
t
c
a

e
r
u
l
i
a
f

g
n
i
n
n
a
l
p

-

-

-

-

-

h
t
i
w
s
e
d
o
n

d
e
n
u
r
p
-
n
o
n

:
e
c
n
e
r
e
f
e
r

r
o
f

)
0
.
2
>
w
c
(

t
n
e
m
e
v
o
m

t
n
a
c
ﬁ
i
n
g
i
s

e
n
e
c
S

13

References

[1] T. Stoyanov, N. Vaskevicius, C. A. Mueller, T. Fromm, R. Krug, V. Tincani, R. Mojta-
hedzadeh, S. Kunaschk, R. Mortensen Ernits, D. Ricao Canelhas, M. Bonilla, S. Schwertfeger,
M. Bonini, H. Halfar, K. Pathak, M. Rohde, G. Fantoni, A. Bicchi, A. Birk, A. Lilienthal,
and W. Echelmeyer, “No More Heavy Lifting: Robotic Solutions to the Container Unloading
Problem,” Robotics and Automation Magazine, 2016, to appear.

[2] J. Winkler, F. Balint-Benczedi, T. Wiedemeyer, M. Beetz, N. Vaskevicius, C. A. Mueller,
T. Fromm, and A. Birk, “Knowledge-Enabled Robotic Agents for Shelf Replenishment in
Cluttered Retail Environments,” in International Conference on Autonomous Agents and
Multiagent Systems, 2016, to appear.

[3] S. Zickler and M. Veloso, “Eﬃcient Physics-Based Planning: Sampling Search Via Non-
Deterministic Tactics and Skills,” in International Conference on Autonomous Agents and
Multiagent Systems, 2009.

[4] E. Weitnauer, R. Haschke, and H. Ritter, “Evaluating a Physics Engine as an Ingredient for
Physical Reasoning,” in International Conference on Simulation, Modeling, and Programming
for Autonomous Robots, 2010.

[5] M. Dogar, K. Hsiao, M. Ciocarlie, and S. Srinivasa, “Physics-Based Grasp Planning Through

Clutter,” in Robotics: Science and Systems, 2012.

[6] N. Kitaev, I. Mordatch, S. Patil, and P. Abbeel, “Physics-Based Trajectory Optimization for
Grasping in Cluttered Environments,” in International Conference of Robotics and Automa-
tion, 2015.

[7] P. Pastor, M. Kalakrishnan, S. Chitta, E. Theodorou, and S. Schaal, “Skill learning and task
outcome prediction for manipulation,” in International Conference on Robotics and Automa-
tion, 2011.

[8] A. Haidu, D. Kohlsdorf, and M. Beetz, “Learning Action Failure Models from Interactive
Physics-based Simulations,” in International Conference on Intelligent Robots and Systems,
2015.

[9] S. Rockel, S. Konecny, S. Stock, J. Hertzberg, F. Pecora, and J. Zhang, “Integrating Physics-
Based Prediction with Semantic Plan Execution Monitoring,” in International Conference on
Intelligent Robots and Systems, 2015.

[10] N. Akhtar, A. Kuestenmacher, P. Ploeger, and G. Lakemeyer, “Simulation-based approach

for avoiding external faults,” in International Conference on Advanced Robotics, 2013.

[11] M. Stilman, J.-U. Schamburek, J. Kuﬀner, and T. Asfour, “Manipulation Planning Among

Movable Obstacles,” in International Conference on Robotics and Automation, 2007.

[12] K. Okada, A. Haneda, H. Nakai, M. Inaba, and H. Inoue, “Environment Manipulation Planner
for Humanoid Robots Using Task Graph That Generates Action Sequence,” in International
Conference on Intelligent Robots and Systems, 2004.

[13] N. Vaskevicius, C. Mueller, M. Bonilla, V. Tincani, T. Stoyanov, G. Fantoni, K. Pathak,
A. Lilienthal, A. Bicchi, and A. Birk, “Object Recognition and Localization for Robust Grasp-
ing with a Dexterous Gripper in the Context of Container Unloading,” in International Con-
ference on Automation Science and Engineering, 2014.

[14] N. Vaskevicius, K. Pathak, A. Ichim, and A. Birk, “The Jacobs Robotics approach to object
recognition and localization in the context of the ICRA’11 Solutions in Perception Challenge,”
in International Conference on Robotics and Automation, 2012.

14

[15] C. Mueller, K. Pathak, and A. Birk, “Object Shape Categorization in RGBD Images using
Hierarchical Graph Constellation Models based on Unsupervisedly Learned Shape Parts de-
scribed by a Set of Shape Speciﬁcity Levels,” in International Conference on Intelligent Robots
and Systems, 2014.

[16] K. Goldberg, “Stochastic Plans for Robotic Manipulation,” Ph.D. dissertation, 1990.

[17] H. Taeubig, B. Baeuml, and U. Frese, “Real-time Swept Volume and Distance Computation
for Self Collision Detection,” in International Conference on Intelligent Robots and Systems,
2011.

[18] A. von Dziegielewski, M. Hemmer, and E. Schoemer, “High Precision Conservative Surface
Mesh Generation for Swept Volumes,” Transactions on Automation Science and Engineering,
vol. 12, no. 1, pp. 764–769, 2015.

[19] K. Abdel-Malek, J. Yang, D. Blackmore, and K. Joy, “Swept Volumes: Fundation [sic], Per-

spectives, and Applications,” International Journal of Shape Modeling, vol. 12, no. 1, 2006.

[20] R. Mojtahedzadeh, A. Bouguerra, E. Schaﬀernicht, and A. Lilienthal, “Support relation anal-
ysis and decision making for safe robotic manipulation tasks,” Robotics and Autonomous
Systems, vol. 71, no. July 2015, pp. 99–117, 2015.

[21] S. Morgan and M. Branicky, “Sampling-Based Planning for Discrete Spaces,” in International

Conference on Intelligent Robots and Systems, 2004.

15

