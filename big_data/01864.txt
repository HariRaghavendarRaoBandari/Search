General Participative Media Single Image Restoration

Joel D.O. Gaya ∗, Felipe Codevilla ∗, Amanda C. Duarte and Silvia Botelho

Universidade Federal do Rio Grande (FURG)

Rio Grande - Brazil

6
1
0
2

 
r
a

M
6

 

 
 
]

V
C
.
s
c
[
 
 

1
v
4
6
8
1
0

.

3
0
6
1
:
v
i
X
r
a

felipe.codevilla@furg.br

Abstract

This paper describes a method to restore degraded im-
ages captured in general participative media — fog, turbid
water, sand storm, etc. To obtain generality, we, ﬁrst, pro-
pose a novel interpretation of the participative media image
formation by considering the color variation of the media.
Second, we introduce that joining different image priors is
an effective alternative for image restoration. The proposed
method contains a Composite Prior supported by statistics
collected on both haze-free and degraded participative en-
vironment images. The key of the method is joining two
complementary measures — local contrast and color. The
results presented for a variety of underwater and haze im-
ages demonstrate the power of the method. Moreover, we
showed the potential of our method using a special dataset
for which a reference haze-free image is available for com-
parison.

1. Introduction

A participative media is deﬁned as any kind of media
where the particles in suspension in the media affect the
image formation, e.g. underwater media, fog, sand storm,
etc. (See Fig. 3 for samples). Images taken in these types
of environments are degraded by these particles. This hap-
pens since light is scattered and absorbed, culminating into
scene information loss. Also, the particles from outside the
scene scatter over the image producing a characteristic veil
which reduces the image contrast.

Most of the image restoration methods attempt to recover
degraded images relying on a physical model of the image
formation. The used physical model describes a linear su-
perposition between the signal and the veil. To recover the
degraded images, the methods have to estimate (i) the color
and radiance properties of the veil, also called veiling light,
and (ii) the amount of veil on each image patch, i.e.
the
transmission. This is an oddly-posed problem, since there

∗Both authors had equal contribution on the design of this paper.

is plenty of information ambiguity in a single image patch.
Thus, many methods in the literature use multiple images,
polarization [22] or special hardware [12]. To estimate (i)
and (ii) from a single image we need to deﬁne some image
priors which contain information that indicates the trans-
mission and the veiling light.

Most of the single image restoration algorithms are made
to speciﬁc kinds of participative media, and sometimes lead
to impressive results, e.g. He et al. [13] and Fattal [9] for
images with haze around them, or Ancuti et al. for underwa-
ter images [1]. However, a simple change in the lighting, or
sometimes in the structure of the imaged scene, can make a
speciﬁc method fail. We believe that a more general indica-
tion of turbidity is needed in order for an image restoration
method to be more robust to environment changes.

In this context, we propose an automatic single image
restoration method designed to work in general participative
media. Generality is obtained through the contribution to-
ward a new physical model simpliﬁcation that assumes that
the veiling light is the same as the illumination of the ob-
jects, i.e. the ambient light. We are then able to estimate the
veiling light using robust color constancy algorithms [10].
Further, in order to obtain robustness we propose an im-
age prior integration scheme. We integrate two different,
yet complementary, transmission indicators. The ﬁrst prior,
the Veil Difference Prior, assumes that the difference be-
tween the channel with the maximum difference to the am-
bient light in a haze-free image is equal to one. We assume
that the visibility (transmission) is proportional to the dif-
ference to the ambient light. The second prior, the Contrast
Prior, assumes that non-turbid images have high contrasts
in certain patches. Thus, we infer that the transmission is
proportional to a contrast measure. Finally, the priors are
joined by assuming that a higher indication of transmission
is less likely to be wrong.

With the proposed transmission indication, we are able
to successfully restore both haze and underwater images
without any parameter change. We compare the results
qualitatively, by image examination, and also quantitatively,
by a direct comparison with a ground truth.

4321

2. Image Formation Model

When the light propagates in a participative environment
it is scattered and absorbed by the suspended particles. The
scattering and absorption cause effects on the image forma-
tion. The image signal, i.e. the imaged scene, suffers from
attenuation, such that just part of the information reaches
the camera. Further, the scattering degrades the image for-
mation. Forward scattering happens when the light rays
coming from the scene are scattered in small angles creat-
ing a blurry effect on the image. This effect, however, has a
small contribution to the total image degradation and is fre-
quently neglected [22]. Another effect, the backscattering,
happens when the information from other sources scatters
over the camera plane creating a characteristic veil on the
image which reduces the contrast and further attenuates the
signal information. We deﬁne an image captured in a par-
ticipative environment, for each color channel λ ∈ {r, g, b},
as

Iλ(x) = Edλ(x) + Ebsλ(x),

(1)

where Edλ(x) is the direct component (signal) and Ebsλ(x)
is the backscattering component. The rest of this section
explains each of the components of Eq. (1).

Figure 1: The image formation model for participative en-
vironments.

2.1. Direct Component

The direct component, Edλ(x), is deﬁned as

Edλ(x) = Jλ(x) e−cd(x),

(2)

where Jλ(x) is the signal with no degradation, which is at-
tenuated by e−cd(x), named as transmission t(x). We pro-
pose that, considering Jλ(x) as a general image taken from
a Lambertian Surface, Jλ(x) can also be described by the
color constancy image formation model [26]

Jλ(x) = Lλ(x) Mλ(x) Cλ(x),

(3)

where Lλ(x) is the light source, Mλ(x) is the reﬂectivity
of the imaged object, and Cλ(x) are the camera parameters.
Omitting the camera parameters and considering the light
source as constant, we have that

Jλ(x) = Lλ Mλ(x).

(4)

In participative environment images, the natural light comes
from a limited cone above the scene, as portrayed in Fig.
1 as ROIo optical manhole cone [5]. For this reason, we
assume that the light source is related to the cone size and
also inﬂuenced by the environment.

2.2. Backscattering Component

Following the Jaffe-McGlamery [15] and [19], and the
respective simpliﬁcations [22] and [4], the backscattering
component, Ebsλ(x), can be deﬁned as

Ebsλ(x) = AD

λ · (1 − t(x)),

(5)

where AD
λ is the veiling light, here also called ambient light
constant, that represents the color and radiance character-
istics of the media. This constant is related to the ROIp,
analogous to ROIo, placed above the LOS (Line of Sight).
Also, this constant is altered by the depth and inﬂuenced
by the environment. The (1 − t(x)) portion weights the ef-
fect of the backscattering as a function of the distance, d(x),
from the object to the camera. The higher the distance, the
higher the chance that AD

λ scatters over the scene.

2.3. Final Model

By considering low depth variations in the same captured
scene we can consider that the LOS and the objects are all
illuminated by the same light source. The main consider-
ation of this section is that the ambient light, AD
λ , and the
constant light source, Lλ, are actually the same. Thus, by
considering Lλ = AD
λ in Eqs. (2) and (4) the ﬁnal equation
is deﬁned as

Iλ(x) = AD

λ · M (x) · t(x) + AD

λ · (1 − t(x)).

(6)

Note that this equation presents a generalization from the
Koschmieder’s equation [16] that is commonly used by de-
λ ≈ [1, 1, 1]
hazing methods, e.g. [13] and [8]. When AD
(approximately white) Eq. (6) turns into the Koschmieder’s
equation. We show in Section 6 that, by using Eq. (6) in
restoration, we can jointly obtain color recovering and haze
removal by solving a single equation.

3. Related Works

In order to estimate the true object color reﬂectivity, both
the transmission and the ambient light must be estimated.

4322

Toward that end, one has to assume some properties a haze-
free image should have, i.e., an image without ambient in-
terference. These properties are usually image priors, or as-
sumptions that can be used to ﬁnd indicators of the amount
of turbidity a certain image patch has.

Fattal [8] has assumed that there is no covariance be-
tween the reﬂectance and the illumination, so that the trans-
mission can be deﬁned as the source of covariance. How-
ever, it has been shown that the assumption works only for
low degradation conditions [9].

Fattal also proposed a method to estimate the transmis-
sion that uses a color line assumption [20].
It infers the
transmission by ﬁnding the intersection point between the
color line and the vector with the orientation of the veiling
light. A robust method is obtained. However, it depends on
ﬁnding patches were some model properties exist.

One of the main methods developed is based on the Dark
Channel Prior [13], where the minimum value of the image
channels in a patch gives an indication of the transmission,
t(x). This is a robust idea, but it is developed solely for
white colored haze and does not work well for underwa-
ter environments [6]. The same method has been adapted
several times for underwater environments, e.g.
[3], [4],
[11], and [18]. However, all adaptations lacked to consider
the large range of colors that exist underwater by assuming
some speciﬁc conditions such as the Red Channel Absorp-
tion [11]. Our modeling (Section 2) do not consider the par-
ticipative media as having single color properties. We show
later that this consideration is helpful in achieving accurate
image restoration from any type of environment.

There are also approaches that directly manipulate some
of the image properties, e.g. contrast, blur, and noise, in or-
der to try to improve them. Many general image enhancing
methods can be used to recover the visibility through turbid
media, e.g. CLAHE [14], Bilateral Filters [25], and Color
Constancy. There are examples of enhancement method fu-
sions in the literature, e.g. [1] and [2]. The direct manip-
ulation of the image properties reduces the haze at the cost
of also degrading some of the image properties. Moreover,
enhancement methods do not usually consider the spatial
variation that exists in participative media degraded images.

4. Composite Prior Transmission Estimation

The use of a single indicator, such as the color [4] and
[13], or the contrast [24], is decisive but not sufﬁcient. For
instance, it is not possible to know if a signal corresponds
to an object reﬂectance of a given color, or if it has a certain
color due to the ambient light. The same happens to the
image structure, i.e.
it is not possible to know if a patch
has a certain weak structure or if this structure is already
attenuated by the turbidity.

In contrast, we believe that, when a pixel has a high
transmission, this indication is usually related to the infor-

mation of the signal, and unrelated to the veiling. Thus,
we propose a simple combination of transmission estima-
tors using the maximum between each of them. This com-
bination is constrained by using indicators that output over
distinct types of patches.

In this work we propose the Composite Transmission by
joining two transmission indicators. We denote the trans-
mission of a pixel x as being

t(x) = max(tv(x), tc(x)),

(7)
where tv(x) is the transmission computed with a color-
based Veil Difference Transmission and tc(x) is the trans-
mission computed with the Contrast Transmission.1
4.1. Veil Difference Transmission

Considering the ambient light, we propose the follow-
ing assumption: the image tends to be closer to the ambient
light when the image is affected by turbidity. Fig. 2 shows
the histogram of a single scene that is captured under dif-
ferent turbidity levels (Fig. 8). It can be seen that the in-
tensities of the pixels tend to be closer to the ambient light
(represented by dashed lines) as the amount of turbidity in-
creases.

Figure 2: Histograms of the same scene captured under dif-
ferent levels of turbidity. T0 is an image free of turbid-
ity. T5, T11 and T20 have different turbidity levels created
through the addition of milk [7]. Fig. 8 shows the images
used.

Given that we assume all pixels from a patch Ω(x), cen-
tered at x, to have the same distance to the camera, we de-
ﬁne the transmission as

tv(x) =

max( max(|Iλ(y) − AD
λ∈{r,g,b} y∈Ω(x)
max( max(|Jλ(y) − AD
λ∈{r,g,b} y∈Ω(x)

λ |))
λ |))

,

(8)

1To compute the transmission, we assume AD

λ as given.

4323

05010015020025000.0020.0040.0060.0080.010.012T005010015020025000.0020.0040.0060.0080.010.0120.0140.0160.0180.02T505010015020025000.0050.010.0150.020.0250.030.0350.040.0450.05T1105010015020025000.050.10.150.20.25T20where the transmission, tv(x), can be interpreted as the loss
of distinction to AD
λ between the haze-free image, Jλ, and
the turbid image Iλ. We take pixel of the channel with the
highest difference since it is the one with more informa-
tion. The problem is that the haze-free image is unknown.
However, when looking at haze-free images, we perceive
that they tend to have a signiﬁcantly difference to the light
source on at least one of its color channels. Formally, for an
image Jλ, we deﬁne the Veil Difference Prior as
= max( max(1 − AD

(9)

max( max(|Jλ(y) − AD
λ∈{r,g,b} y∈Ω(x)

λ , AD

λ ))
.

λ |))

λ∈{r,g,b}

This prior can be further understood as a generalization
of the dark channel prior from He et al. [13]. Considering
the ambient light color as pure white, i.e. AD
λ = [1, 1, 1],
Eq. (9) turns into max(1 − Jλ(x)) = 1 for λ ∈ {r, g, b},
which is analogous to the dark channel prior. By replacing
Eq. (9) into Eq. (8) we deﬁne the veil difference transmis-
sion computed as

tv(x) =

max( max(|Iλ(y) − AD
λ∈{r,g,b} y∈Ω(x)
max( max(1 − AD
λ∈{r,g,b}

λ , AD

λ |))

λ ))

.

(10)

We show the resultant veil difference transmission on Fig.
5b. We can perceive that this transmission captures the dis-
tance variation existent on the image.
4.2. Contrast Transmission

We can observe on Fig. 2 that, besides the color ap-
proximation to veiling light, we have shrinking into the his-
togram shape, dramatically reducing the global image con-
trast. Thus, we assume the following local indicator: for
a given patch Iλ, the contrast of this patch tends to reduce
proportionally to an increase at the turbidity.

We selected a contrast function that is a valid indicator,

with this, we deﬁne the contrast transmission

tc(x) =

max( max(Iλ(y)) − min(Iλ(y)))
λ∈{r,g,b} y∈Ω(x)
max( max(Jλ(y)) − min(Jλ(y)))
λ∈{r,g,b} y∈Ω(x)

,

(11)

this ratio, analogously to Eq. (8), represents the lost of con-
trast information caused by the turbidity. However, we do
not know the contrast of the haze free version of image. So,
we assume that this haze free image patch has the maximum
possible range deﬁning the Contrast Prior as
max( max(Jλ(y)) − min(Jλ(y)))
λ∈{r,g,b} y∈Ω(x)

(12)

= 1.

This is true for some samples of the image. Since we are
not using only this indicator, it is reasonable to make this
assumption. Thus, we have the contrast transmission com-
puted as

tc(x) = max( max(Iλ(y)) − min(Iλ(y)))
.

λ∈{r,g,b} y∈Ω(x)

(13)

4324

We show the resultant contrast transmission on Fig. 5c. We
can perceive that this transmission is sparse, but has a clear
representation of the distance variation.

We also show the contribution from each of the image
priors after applying Eq. (7) on Fig. 5d. We perceive that
the intermediate range transmissions are dominated by the
contrast transmission.2

4.3. Priors Validation

To evaluate the proposed priors, we compute the average
histogram for different priors on 2,000 images for a haze-
free and a participative media dataset. The images were
resized into a maximum side of 1,024 and the priors were
computed over patches of 15x15 pixels.

For haze-free dataset we used images from the test set
of the popular ImageNet [21]. For participative media,
we built a dataset by collecting turbid images over the
web. Some samples of these participative media images are
shown on Fig. 3. The dataset was made to be diverse and
contain samples of several kinds of media, e.g., fog, oceanic
water, coastal water, sand storm etc.

On these datasets we test the Veil Difference Prior (VDP)
(Eq. (9)) and the Composite Prior (CP) that takes the max-
imum between Eq. 10 and Eq. (13). We also compare the
histograms with the Dark Channel Prior (DCP) [13] , and
the UDCP prior [6].

Figure 3: The participative media dataset used to test the
priors on a more general setting of turbid places. We col-
lected around 2,000 images over the web containing all
kinds of participative media.

Fig. 4 shows the generated histograms. We can conﬁrm
that the DCP of the images is approximately one (Fig. 4a),
however, it tends to have a similar behaviour for participa-
tive media images, which demonstrates non-sensibility to
general participative media. The same, in a lesser extent,
happens for the UDCP prior (Figs. 4c and 4d).

2Please refer to the supplementary material for further comparisons on

the transmission estimations.

For the VDP (Figs. 4e and 4f), it showed around 40%
of the bins equal to one and a tendency to have higher val-
ues. This result shows that the prior is clearly not as strong
as the DCP. However, the average for the prior on degraded
participative media showed a very high range of values, in-
dicating a sensitivity to the presence of turbidity.

Finally, we show the histograms for the Composite Prior,
arguably to be the most reliable behaviour. For haze-free
images (Fig. 4g) there is a clearer tendency of assuming the
value one than just the veil difference prior. Yet, it showed
less range of results on participative media images (Fig. 4h),
mainly due to the tendency on still ﬁnding structures. How-
ever, it combines the best of two worlds, having high range
of values for participative media images, and a solid high
response for haze-free images. This corroborates about the
generality of the proposed priors.

t(x) must be reﬁned. We choose to use the soft matting
algorithm on t(x) [17]. The mentioned algorithm can be
applied since there is some relation between the Eq. (6) and
the deﬁnition of the matting problem given by the equation:

I = F α + B(1 − α),

(14)

Thus, α for the matting problem is the same as the transmis-
sion for the haze removal problem. The results of a reﬁned
transmission are shown on Fig. 5e.

5. Image Restoration

(a) DCP haze-free

(b) DCP participative media

(a) Initial

(b) tv(x)

(c) UDCP haze-free

(d) UDCP participative media

(e) VDP haze-free

(f) VDP participative media

(c) tc(x)

(d) Contributions

(g) CP haze-free

(h) CP participative media

(e) Final With Reﬁnement

(f) Restored Image

Figure 4: The histograms computed for, on the left, haze
free images (Image Net), on the right, general participative
media images (Fig. 3). For a better understanding of results,
we show sampling of the histograms on every ﬁve bins. For
an easier comparison we ploted 1 − DCP and also 1 −
U DCP .

Figure 5: The results from the proposed method and the
transmission maps. (a) The input fog image. (b) veil differ-
ence transmission tv(x). (c) Contrast transmission tc(x).
(d) The contribution of each transmission after the max op-
erator, where green is tc(x) and blue is tv(x). (e) The image
after the reﬁnement with the soft matting algorithm. (f) The
restored image.

4.4. Reﬁning Transmission

When the transmission is computed over a patch, there
may be considerable intensity variations that do not agree
with a single transmission. For that, the transmission map

In this section, ﬁrst we show the estimation of the ambi-
ent light constant AD
λ . Then we also show a way to reduce
image noise and, ﬁnally, the restoration process that uses
our novel model derivation from Sec. 2.

4325

05010015020025000.10.20.30.40.50.60.70.80.91ProbabilityIntensity05010015020025000.10.20.30.40.50.60.70.80.91ProbabilityIntensity05010015020025000.10.20.30.40.50.60.70.80.91ProbabilityIntensity05010015020025000.10.20.30.40.50.60.70.80.91ProbabilityIntensity05010015020025000.10.20.30.40.50.60.70.80.91ProbabilityIntensity05010015020025000.10.20.30.40.50.60.70.80.91ProbabilityIntensity05010015020025000.10.20.30.40.50.60.70.80.91ProbabilityIntensity05010015020025000.10.20.30.40.50.60.70.80.91ProbabilityIntensity5.1. Estimating the Ambient Light

The ambient or veiling light is associated with the light
color and intensity that is scattered to the scene, and as
stated on Sec. 2. As stated by [23], a correct estimation
of the ambient light is fundamental, since all the calculated
transmission uses it as reference.
Initially, many authors
estimated it as being the brightest pixels in the image [24]
[8]. This estimation presents some downsides, specially if
the ambient light is not present on the image. Sulami et
al. proposed a method [23], that divides the estimation into
orientation and magnitude and do not need the presence of
an ambient light pixel on the image, however it depends on
ﬁnding patches that obeys certain properties.

As we stated on Section 2, the ambient light is associ-
ated with the light source on the scene. With this, it is
reasonable to use color constancy techniques, commonly
used to ﬁnd the light source color on general images. Tech-
niques such as gray edge [26], the gray world, the max-rgb
or the shades-of-gray [10] can be used. The problem with
gray edge is that the edges are normally blurry and the gray
world technique fails when there are too many close ob-
jects. Also, max-rgb is dependent on actually having white
patches fully reﬂecting the light information. Preliminary
tests shows that using the shades-of-gray algorithm encap-
sulates the best behaviour in our case. The algorithm does
so by doing an estimation in between the average of the
scene and the reﬂectivity of a white patch.
5.2. Ensuring Model Integrity

As we discussed previously, the images captured in par-
ticipative media are intensively affected by the backscatter-
ing effect. Besides that, the process of photometry in par-
ticipative media is very prone to noise and blur. In order to
effectively estimate the transmission related to the effects of
backscattering it is necessary to minimize other effects that
may occur.

The bilateral ﬁlter [25] is the function we choose to im-
prove the results. It tends to eliminate the high frequency
isolated points but keeps the edges, where it is likely to have
greater variation of transmission. This reduces the blurring
effect and the noise conditions but keep the backscattering
for estimation. With this, we choose to apply this function
before computing the veil difference transmission.
5.3. Restoration

The image with restored visibility (see Fig. 5f) is ob-
tained by isolating the object reﬂectivity M (x) on equation
6:

Iλ(x) − AD

λ + AD

λ t(x)

,

(15)

Mλ(x) =

max(tλ0, AD

λ t(x))

where the tλ0 parameter is the minimum transmission. This
parameter is useful when there are no information behind

4326

the veil, avoiding to restore noise. Here we use three min-
imum parameters, tr0, tg0 and tb0, where, normally, set-
ting them between 0.1 and 0.2. However, for some images,
where for instance, there is low red channel information, it
is reasonable to set tr0 higher to avoid saturation.
6. Evaluation

All our results were obtained by C++ implementations
using OpenCV for matrices manipulation. All the parame-
ters are kept as explained on Section 5. We ﬁrst compare
the results by directly comparing them visually with previ-
ous state-of-the-art results. For most of the cases, we used
the provided images by the authors in order to reproduce the
results. To produce the Red Channel [11] and He [13] trans-
missions, we made a C++ implementation. For all UDCP
[6] results, we used an implementation provided by the au-
thors.
6.1. Qualitative Evaluation

Fig. 6 shows the results obtained for the underwater en-
vironment. First of all, when comparing the transmission
maps (Fig. 6 ﬁrst and third row), the proposed method tends
to not overestimate transmission. This happens mainly due
to the observation that the ambient light color is not neces-
sarily present on the image. Further, we perceive that the
use of a transmission that also contains structure, helps on
not underestimating the transmission on structured regions.
The Red Channel assumption works quite well for Fig. 6
with a little overestimation. However, for the scuba-divers
image (Fig. 6i), this method clearly has problems by overes-
timating highly red objects but with high amount of degra-
dation.

Also, when comparing the results with [1] (Figs. 6e and
6m) there is a clear tendency of our method to have more
contrasted colors and less noise, but with slightly less con-
trast.

We proposed a model that consider color variation on
participative media. With this idea, our method is capable
of recovering color properties without further use of white
balances and compensations. This can be seen when com-
paring the proposed method with Drews et al.
[13] [6],
where a good restoration was obtained (Fig 6f) but with a
not satisfactory color correction.

As stated earlier, our method is designed to restore any
kind of participative media without any parameter adjust-
ment. On Fig. 7 we show the results comparing with other
state-of-the-art methods. Once more, the fact that we sup-
pose that the ambient light is not a pixel from the scene
helps not to overestimate transmission on lower distances,
where we obtain a much clearer result. However, for the
same reason, on longer distances, our method culminates
into overestimating the transmission. Finally, we also see
a better color correction when compared to Fattal [9] and

(a) Input

(b) Drews et al.

(c) Galdran et al.

(d) Our trans.

(e) Ancuti

(f) Drews et al.

(g) Galdran et al.

(h) Our

(i) Input

(j) Drews et al.

(k) Galdran et al.

(l) Our trans.

(m) Ancuti et al.

(n) Drews et al.

(o) Galdran et al.

(p) Our

Figure 6: Results showing the transmission estimation and restoration for underwater images. The transmissions are show
using a red to blue scale, where red indicates a higher transmission. The obtained results are compared with Galdran et al.
[11], Drews et al. [6] and Ancuti et al. [1].

He et al. [13]. We explain this color enhancement due to
the fact that we do not consider fog as being approximately
white.

6.2. Quantitative Evaluation

Usually methods are compared subjectively by the per-
ception of image quality. However in many cases, such as
when comparing Figs. 6e and 6h, it is hard to access which
of the methods obtained the best haze removal.

An effective way to access dehazing algorithms quality
is by comparing them with a ground truth, a version of the
same image without turbidity. For this, we used the TUR-
BID dataset proposed on [7]. They reproduced a scene of
an underwater environment where multiple images are cap-
tured with an increasing turbidity by addition of milk. Fig.

8 shows four image samples of the TURBID dataset.

We plot on Fig. 9 the mean square error in function of
turbidity (proportional to the amount of milk). The error is
measured between the restored image and the clean image
(no milk). We compare our result with the Red Channel
Prior (RCP) [11], the Dark Channel Prior (DCP) [13] and
also the TURBID images. On the TURBID images (Blue
Line) we compare the mean square error of the turbid im-
ages without any kind of restoration. The other lines shows
the error after the restoration. Considering this, when error
is below the TURBID images lines, it shows that the method
performed an effective restoration. This is true since the im-
age became closer to the reference image.

On Fig. 9a, we measured the results of each method by
setting a ﬁxed ambient light estimation. We can perceive

4327

(a) Input

(b) He et al. trans.

(c) He et al.

(d) Fattal trans.

(e) Fattal

(f) Our trans.

(g) Our

Figure 7: Results for a fog image dehazing. We compare the proposed method with some state-of-the-art methods, Fattal [9]
and He et al. [13].

(a) Clean Image (T0)

(b) 20ml of Milk (T5)

(c) 58ml of Milk (T11)

(d) 110ml of Milk (T20)

Figure 8: Some samples of the TURBID dataset [7] used to
evaluate the quality of image restoration.

that the proposed method got the lowest average error, just
having a higher error than DCP on very turbid images.

On Fig. 9b, we measured the results of each method by
using the own ambient light estimation of the method. The
proposed method was considerably better, since the ambient
light estimation for this dataset not accurate for the DCP or
the RCP considering it is not present on the image.3

7. Conclusions

In this paper we proposed a novel automatic image
restoration method to restore images captured in participa-
tive media.

We contributed to the theoretical modeling by proposing
a new simpliﬁcation to the image formation model that con-
siders color changes in the participative media. We also pro-
posed a new way to estimate image transmission by jointly
using different novel image priors. These priors were veri-

3To get all the restored images, refer to the supplementary material.

(a) AD

λ ﬁxed

(b) AD

λ estimated

Figure 9: Objective evaluation of the proposed method us-
ing the TURBID dataset. The curves represents the MSE
in function of turbidity (Simulated by Milk). Each turbid
image is compared with the clear image (Fig. 8a).

ﬁed using different datasets.

This image transmission was reﬁned by soft matting and
better estimated by ensuring the model integrity with the
bilateral ﬁlter. We proposed that the ambient light, or veil-
ing light can be successfully estimated by using color con-
stancy techniques, specially the shades-of-gray [10]. All of
this contributions leaded into a restoration method capable

4328

02040608010012000.020.040.060.080.10.120.14Milliliters of milkMean square error  Original ImageDCPRed ChannelOur Method02040608010012000.020.040.060.080.10.120.140.160.18Milliliters of milkMean square error  Original ImageDCPRed ChannelOur Method[14] R. Hummel. Image enhancement by histogram transforma-
tion. Computer graphics and image processing, 6(2):184–
195, 1977.

[15] J. S. Jaffe. Computer modeling and the design of optimal
underwater imaging systems. Oceanic Engineering, IEEE
Journal of, 15(2):101–111, 1990.

[16] H. Koschmeider. Theorie der horizontalen sichtweite. Beitr.

Phys. Freien Atm., pages 12:171–181, 1990.

[17] A. Levin, D. Lischinski, and Y. Weiss. A closed-form solu-
tion to natural image matting. Pattern Analysis and Machine
Intelligence, IEEE Transactions on, 30(2):228–242, 2008.

[18] H. Lu, Y. Li, L. Zhang, and S. Serikawa. Contrast enhance-
ment for images in turbid water. JOSA A, 32(5):886–893,
2015.

[19] B. McGlamery. A computer model for underwater camera
systems. In Ocean Optics VI, pages 221–231. International
Society for Optics and Photonics, 1980.

[20] I. Omer and M. Werman. Color lines: Image speciﬁc color
representation. In Computer Vision and Pattern Recognition,
2004. CVPR 2004. Proceedings of the 2004 IEEE Computer
Society Conference on, volume 2, pages II–946. IEEE.

[21] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,
S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,
A. C. Berg, and L. Fei-Fei.
ImageNet Large Scale Visual
Recognition Challenge. International Journal of Computer
Vision (IJCV), pages 1–42, April 2015.

[22] Y. Schechner and N. Karpel. Recovery of underwater visi-
bility and structure by polarization analysis. Oceanic Engi-
neering, IEEE Journal of, 30(3):570–587, July 2005.

[23] M. Sulami, I. Geltzer, R. Fattal, and M. Werman. Auto-
matic recovery of the atmospheric light in hazy images. In
IEEE International Conference on Computational Photogra-
phy (ICCP), 2014.

[24] R. T. Tan. Visibility in bad weather from a single image.
In Computer Vision and Pattern Recognition, 2008. CVPR
2008. IEEE Conference on, pages 1–8. IEEE, 2008.

[25] C. Tomasi and R. Manduchi. Bilateral ﬁltering for gray and
color images. In Computer Vision, 1998. Sixth International
Conference on, pages 839–846. IEEE, 1998.

[26] J. Van De Weijer, T. Gevers, and A. Gijsenij. Edge-based
color constancy. Image Processing, IEEE Transactions on,
16(9):2207–2214, 2007.

of restoring colors independent of the type of participative
media used.

We tested the proposed method with different kinds
of participative media obtaining state-of-the-art results on
most of them. Finally, we also tested the method objectively
by using the TURBID dataset [7] to be able to compare re-
stored images with a ground truth.

As a future work, we would like to study the behaviours

of the fusion with other transmission estimators.

References
[1] C. Ancuti, C. O. Ancuti, T. Haber, and P. Bekaert. Enhancing
underwater images and videos by fusion. In Computer Vision
and Pattern Recognition (CVPR), 2012 IEEE Conference on,
pages 81–88. IEEE, 2012.

[2] S. Bazeille, I. Quidu, L. Jaulin, and J.-P. Malkasse. Au-
In CMM’06,

tomatic underwater image pre-processing.
page xx, 2006.

[3] N. Carlevaris-Bianco, A. Mohan, and R. M. Eustice. Initial
In OCEANS

results in underwater single image dehazing.
2010, pages 1–8. IEEE, 2010.

[4] J. Y. Chiang and Y.-C. Chen. Underwater image enhance-
ment by wavelength compensation and dehazing. Image Pro-
cessing, IEEE Transactions on, 21(4):1756–1769, 2012.

[5] T. W. Cronin and N. Shashar.

The linearly polarized
spatial and
light ﬁeld in clear,
temporal variation of light intensity, degree of polariza-
tion and e-vector angle. Journal of Experimental Biology,
204(14):2461–2467, 2001.

tropical marine waters:

[6] P. Drews, E. do Nascimento, F. Moraes, S. Botelho, and
M. Campos. Transmission estimation in underwater sin-
gle images. In Computer Vision Workshops (ICCVW), 2013
IEEE International Conference on, pages 825–830. IEEE,
2013.

[7] N. D. S. B. F. Codevilla, J. Gaya. Achieving turbidity robust-
ness on underwater images local feature detection. BMVC
2015, 26:157, 2015.

[8] R. Fattal. Single image dehazing. In ACM Transactions on

Graphics (TOG), volume 27, page 72. ACM, 2008.

[9] R. Fattal. Dehazing using color-lines. ACM Transactions on

Graphics (TOG), 34(1):13, 2014.

[10] G. D. Finlayson and E. Trezzi. Shades of gray and colour
In Color Imaging Conference, pages 37–41,

constancy.
2004.

[11] A. Galdran, D. Pardo, A. Pic´on, and A. Alvarez-Gila. Auto-
matic red-channel underwater image restoration. Journal of
Visual Communication and Image Representation, 26:132–
145, 2015.

[12] D.-M. He and G. G. Seet. Underwater lidar imaging in highly
turbid waters. In International Symposium on Optical Sci-
ence and Technology, pages 71–81. International Society for
Optics and Photonics, 2002.

[13] K. He, J. Sun, and X. Tang. Single image haze removal us-
ing dark channel prior. Pattern Analysis and Machine Intel-
ligence, IEEE Transactions on, 33(12):2341–2353, 2011.

4329

