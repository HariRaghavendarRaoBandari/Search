One-Shot Generalization in Deep Generative Models

6
1
0
2

 
r
a

 

M
6
1

 
 
]
L
M

.
t
a
t
s
[
 
 

1
v
6
0
1
5
0

.

3
0
6
1
:
v
i
X
r
a

Danilo J. Rezende*
Shakir Mohamed*
Ivo Danihelka
Karol Gregor
Daan Wierstra
Google DeepMind, London

Abstract

Humans have an impressive ability to reason
about new concepts and experiences from just a
single example.
In particular, humans have an
ability for one-shot generalization: an ability to
encounter a new concept, understand its struc-
ture, and then be able to generate compelling
alternative variations of the concept. We de-
velop machine learning systems with this im-
portant capacity by developing new deep gen-
erative models, models that combine the repre-
sentational power of deep learning with the in-
ferential power of Bayesian reasoning. We de-
velop a class of sequential generative models that
are built on the principles of feedback and atten-
tion. These two characteristics lead to genera-
tive models that are among the state-of-the art
in density estimation and image generation. We
demonstrate the one-shot generalization ability
of our models using three tasks: unconditional
sampling, generating new exemplars of a given
concept, and generating new exemplars of a fam-
ily of concepts. In all cases our models are able
to generate compelling and diverse samples—
having seen new examples just once—providing
an important class of general-purpose models for
one-shot machine learning.

1. Introduction

Figure 1. Given the ﬁrst
row, our model gener-
ates new exemplars.

Consider the images in the red
box to the left in ﬁgure 1. We see
each of these new concepts just
once, understand their structure,
and are then able to imagine and
generate compelling alternative
variations of each concept, sim-
ilar to those drawn in the rows
beneath the red box. This is an

*Equal contributions. Copyright 2016 by the author(s).

DANILOR@GOOGLE.COM
SHAKIR@GOOGLE.COM
DANIHELKA@GOOGLE.COM
KAROLG@GOOGLE.COM
WIERSTRA@GOOGLE.COM

ability that humans have for one-shot generalization: an
ability to generalize to new concepts given just one or a few
examples. In this paper, we develop new models that pos-
sess this capacity for one-shot generalization—models that
allow for one-shot reasoning from the data streams we are
likely to encounter in practice, that use only limited forms
of domain-speciﬁc knowledge, and that can be applied to
diverse sets of problems.
There are few approaches for one-shot generalization.
Salakhutdinov et al. (2013) developed a probabilistic model
that combines a deep Boltzmann machine with a hierarchi-
cal Dirichlet process to learn hierarchies of concept cate-
gories as well as provide a powerful generative model. Re-
cently, Lake et al. (2015) presented a compelling demon-
stration of the ability of probabilistic models to perform
one-shot generalization, using Bayesian program learning,
which is able to learn a hierarchical, non-parametric gen-
erative model of handwritten characters. Their approach
incorporates speciﬁc knowledge of how strokes are formed
and the ways in which they are combined to produce char-
acters of different types, exploiting similar strategies used
by humans to perform this task. Lake et al. (2015) see
the capacity for one-shot generalization demonstrated by
Bayesian programming learning ‘as a challenge for neu-
ral models’. By combining the representational power of
deep neural networks embedded within hierarchical latent
variable models, with the inferential power of approximate
Bayesian reasoning, we show that this is a challenge that
can be overcome. The resulting deep generative models are
general-purpose image models that are accurate and scal-
able, among the state-of-the-art, and possess the important
capacity for one-shot generalization.
Deep generative models are a rich class of models for den-
sity estimation that specify a generative process for ob-
served data using a hierarchy of latent variables. Mod-
els that are directed graphical models have recently be-
come popular in machine learning and include discrete la-
tent variable models such as sigmoid belief networks and
deep auto-regressive networks (Saul et al., 1996; Gregor
et al., 2014), or continuous latent variable models such as
non-linear Gaussian belief networks and deep latent Gaus-

One-Shot Generalization in Deep Generative Models

sian models (Rezende et al., 2014; Kingma & Welling,
2014). These models use deep networks in the speciﬁcation
of their conditional probability distributions to allow rich
non-linear structure to be learned. Such models have been
shown to have a number of desirable properties: inference
of the latent variables allows us to provide a causal expla-
nation for the data that can be used to explore its underlying
factors of variation, and for exploratory analysis; analogi-
cal reasoning between two related concepts, e.g., styles and
identities of images, is naturally possible; any missing data
can be imputed by treating them as additional latent vari-
ables, capturing the the full range of correlation between
missing entries under any missingness mechanism; these
models embody minimum description length principles and
can be used for compression; these models can be used to
learn environment-simulators enabling a wide range of ap-
proaches for simulation-based planning.
Two principles are central to our approach: feedback and
attention. These principles allow the models we develop to
reﬂect the principles of analysis-by-synthesis, in which the
analysis of observed information is continually integrated
with constructed interpretations of it (Yuille & Kersten,
2006; Erdogan et al., 2015; Nair et al., 2008). Analysis
is realized by attentional mechanisms that allow us to se-
lectively process and route information from the observed
data into the model. Interpretations of the data are then ob-
tained by sets of latent variables that are inferred sequen-
tially to evaluate the probability of the data. The aim of
such a construction is to introduce internal feedback into
the model that allows for a ‘thinking time’ during which
information can be extracted from each data point more
effectively, leading to improved inference, generation and
generalization. We shall refer to such models as sequential
generative models. Models such as DRAW (Gregor et al.,
2015) and composited variational auto-encoders (Huang &
Murphy, 2015) are existing models in this class, and we
will develop a general class of sequential generative mod-
els that incorporates these and other latent variable models
and variational auto-encoders. Our contributions are:
• We develop sequential generative models that provide a
generalization of existing approaches, allowing for se-
quential generation and inference, multi-modal posterior
approximations, and a rich new class of deep generative
models.

• We demonstrate the clear improvement that the combi-
nation of attentional mechanisms in more powerful mod-
els and inference has in advancing the state-of-the-art in
deep generative models.

• Importantly, we show that our generative models have
the ability to perform one-shot generalization. We ex-
plore three generalization tasks and show that our mod-
els can imagine and generate compelling alternative vari-
ations of images after having seen them just once.

2. Varieties of Attention
Attending to parts of a scene, ignoring others, analyzing
the parts that we focus on, and sequentially building up an
interpretation and understanding of a scene: these are nat-
ural parts of human cognition. This is so successful a strat-
egy for reasoning that it is now also an important part of
many machine learning systems, and this repeated process
of attention and interpretation, analysis and synthesis, is an
important component of the generative models we develop.
In its most general form, any mechanism that allows us to
selectively route information from one part of our model
to another can be regarded as an attentional mechanism.
Attention allows for a wide range of invariances to be in-
corporated, with few additional parameters and low com-
putational cost. Attention has been most widely used for
classiﬁcation tasks, having been shown to improve both
scalability and generalization (Larochelle & Hinton, 2010;
Chikkerur et al., 2010; Xu et al., 2015; Jaderberg et al.,
2015; Mnih et al., 2014; Ba et al., 2015). The attention
used in discriminative tasks is a ‘reading’ attention, where
we work, not on the image itself, but on a transformation
of the image (into a canonical coordinate space that is typ-
ically lower dimensional), with the parameters controlling
the attention being learned by gradient descent.
Attention in unsupervised learning is much more recent
(Tang et al., 2014; Gregor et al., 2015). In latent variable
models, we have two processes, inference and generation,
that can both use attention, though in slightly different
ways. The generative process makes use of a ‘writing’
or generative attention, which implements a selective up-
dating of the output variables, e.g., updating only a small
part of the image. The inference process makes use of
reading attention, like that used in classiﬁcation. Although
conceptually different, both these forms of attention can be
implemented with the same computational tools. We focus
on image modelling and make use of spatial attention. Two
other types of attention, randomized and error-based, are
discussed in appendix B.

Spatially-transformed attention.
Rather than simply
selecting a patch of an image (taking glimpses) as other
methods do, a more powerful approach is to use a mecha-
nism that provides invariance to shape and size of objects
in the images (general afﬁne transformations).
Tang
et al. (2014) take such an approach and use 2D similarity
transforms to provide basic afﬁne invariance.
Spatial
transformers (Jaderberg et al., 2015) are a more general
method for providing such invariance, and is our preferred
attentional mechanism. Spatial transformers (ST) process
an input image x, using parameters h, and generate an
output y(x, h):

y(x, h) = [κh(h) ⊗ κw(h)] ∗ x,

One-Shot Generalization in Deep Generative Models

where κh and κw are 1-dimensional kernels, ⊗ indicates
the tensor outer-product of the two kernels and ∗ indicates
a convolution. Huang & Murphy (2015) develop occlusion-
aware generative models that make use of spatial trans-
formers in this way. When used for reading attention, spa-
tial transformers allow the model to observe the input im-
age in a canonical form, providing the desired invariance.
When used for writing attention, it allows the generative
model to independently handle position, scale and rotation
of parts of the generated image, as well as their content.
An simple extension is to use multiple attention windows
simultaneously (see appendix).
3. Iterative and Attentive Generative Models
3.1. Latent Variable Models and Variational Inference
Generative models with latent variables describe the proba-
bilistic process by which an observed data point can be gen-
erated. The simplest formulations such as PCA and factor
analysis use Gaussian latent variables z that are combined
linearly to generate Gaussian distributed data points x. In
more complex models, the probabilistic description con-
sists of a hierarchy of L layers of latent variables, where
each layer depends on the layer above in a non-linear way
(Rezende et al., 2014) —for deep generative models, we
specify this non-linear dependency using deep neural net-
works. To compute the marginal probability of the data, we
must integrate over any unobserved variables:

(cid:90)

p(x) =

pθ(x|z)p(z)dz

(1)
In deep latent Gaussian models,
the prior distribution
p(z) is a Gaussian distribution and the likelihood function
pθ(x|z) is any distribution that is appropriate for the ob-
served data, such as a Gaussian, Bernoulli, categorical or
other distribution, and that is dependent in a non-linear way
on the latent variables. For most models, the marginal like-
lihood (1) is intractable and we must instead approximate
it. One popular approximation technique is based on vari-
ational inference (Jordan et al., 1999), which transforms
the difﬁcult integration into an optimization problem that
is typically more scalable and easier to solve. Using vari-
ational inference we can approximate the marginal likeli-
hood by a lower bound, which is the objective function we
use for optimization:

F = E

q(z|x)[log pθ(x|z)] − KL[qφ(z|x)(cid:107)p(z)]

(2)

The objective function (2) is the negative free energy,
which allows us to trade-off the reconstruction ability of the
model (ﬁrst term) against the complexity of the posterior
distribution (second term). Variational inference approxi-
mates the true posterior distribution by a known family of
approximating posteriors qφ(z|x) with variational param-
eters φ. Learning now involves optimization of the varia-
tional parameters φ and model parameters θ.
Instead of optimization by the variational EM algorithm,

we take an amortized inference approach and represent the
distribution q(z|x) as a recognition or inference model,
which we also parameterize using a deep neural network.
Inference models amortize the cost of posterior inference
and makes it more efﬁcient by allowing for generalization
across the inference computations using a set of global vari-
ational parameters φ. In this framework, we can think of
the generative model as a decoder of the latent variables,
and the inference network as its inverse, an encoder of the
observed data into the latent description. As a result, this
speciﬁc combination of deep latent variable model (typ-
ically latent Gaussian) with variational inference that is
implemented using an inference network is referred to as
a variational auto-encoder (VAE). VAEs allow for a sin-
gle computational graph to be constructed and straightfor-
ward gradient computations: when the latent variables are
continuous, gradient estimators based on pathwise deriva-
tive estimators are used (Rezende et al., 2014; Kingma &
Welling, 2014; Burda et al., 2015) and when they are dis-
crete, score function estimators are used (Mnih & Gregor,
2014; Ranganath et al., 2014; Mansimov et al., 2015).
3.2. Sequential Generative Models
The generative models as we have described them thus
far can be characterized as single-step models, since they
are models of i.i.d data that evaluate their likelihood func-
tions by transforming the latent variables using a non-
linear, feed-forward transformation. A sequential genera-
tive model is a natural extension of the latent variable mod-
els used in VAEs. They combine both stochastic and deter-
ministic computations to form a multi-step generative pro-
cess that uses recursive transformations of the latent vari-
ables, i.e. uses an internal state-space model.
3.2.1. GENERATIVE MODEL

In their most general form, sequential generative models
describe the observed data over T time steps using a set of
latent variables zt at each step. The generative model is
shown in the stochastic computational graph of ﬁgure 2(a),
and described by:

Latent variables

zt ∼ N (zt|0, 1) t = 1, . . . , T (3)
(4)
(5)
(6)

Hidden state ht = fh(ht−1, zt; θh)
ct = fc(ct−1, ht; θc)
Observation x ∼ p(x|fo(cT ; θo))

Hidden Canvas

Each step generates an independent set of K-dimensional
latent variables zt (equation (3)). A deterministic transi-
tion function fh introduces the sequential dependency be-
tween each of the latent variables (equation (4)). This al-
lows any transition mechanism to be used and our transition
is speciﬁed as a long short-term memory network (LSTM,
Hochreiter & Schmidhuber (1997). We explicitly represent
the creation of a set of hidden variables ct that represents
a hidden canvas of the model equation (5). The canvas

One-Shot Generalization in Deep Generative Models

(a) Unconditional generative model.

(b) One-step of the conditional generative model.

Figure 2. Stochastic computational graph showing conditional probabilities and computational steps for sequential generative models.
Additive Canvas. As the name implies, an additive canvas
function fc allows for many different transformations, and
it is here where generative (writing) attention is used; we
updates the canvas by simply adding a transformation of the
describe a number of choices for this function in section
hidden state fw(ht; θc) to the previous canvas state ct−1.
3.2.3. The condition (6) is computed using an observation
This is a simple, yet effective (see results) update rule:
function fo(c; θo) that maps the last hidden canvas cT to
the parameters of the observation model, e.g., probability
of being on or off for a Bernoulli distribution. We de-
note the set of all parameters of this generative model as
θ = {θh, θc, θo}.
3.2.2. FREE ENERGY OBJECTIVE

(8)
Gated Recurrent Canvas. The canvas function can be up-
dated using a convolutional gated recurrent unit (CGRU)
architecture (Kaiser & Sutskever, 2015), which provides a
non-linear and recursive updating mechanism for the can-
vas and are simpliﬁed versions of convolutional LSTMs
(further details of the CGRU are given in appendix B). The
canvas update is:

fc(ct−1, ht; θc) = ct−1 + fw(ht; θc),

Given the probabilistic model (3)-(6) we can obtain an ob-
jective function for inference and parameter learning using
variational inference. By applying the variational principle,
we obtain the free energy objective:

log p(x) = log(cid:82) p(x|z1,...,T )p(z1...T )dz1...T ≥ F

(cid:80)T
F = E
q(z1,...,T )[log pθ(x|z1,...,T )]
t=1 KL[qφ(zt|z<tx)(cid:107)p(zt)],
−

(7)

where z<t indicates the collection of all latent variables
from iteration 1 to t − 1. We can now optimize this ob-
jective function for the variational parameters φ and the
model parameters θ, by stochastic gradient descent using
a mini-batch of data. As with other VAEs, we use a sin-
gle sample of the latent variables generated from qφ(z|x)
when computing the Monte Carlo gradient. To complete
our speciﬁcation, we now describe speciﬁc forms of the
hidden-canvas functions fc and the approximate posterior
distribution qφ(zt).

3.2.3. HIDDEN CANVAS FUNCTIONS
The canvas transition function fc(ct−1, ht; θc) (5) updates
the hidden canvas by ﬁrst non-linearly transforming the
current hidden state of the LSTM ht (using a function fw)
and fuses the result with the existing canvas ct−1. In this
work we use hidden canvases that have the same size as
the original images, though they could be either larger or
smaller in size and can have any number of channels (four
in this paper). We consider two ways with which to update
the hidden canvas:

fc(ct−1, ht; θc) = CGRU(ct−1 + fw(ht; θc))

(9)
In both cases, the function fw(ht; θw) is a writing func-
tion that is used by the canvas function to transform the
LSTM hidden state into the coordinate system of the hid-
den canvas. This can be any operation, such as a fully- or
locally-connected mapping. We use a writing or generative
attention function. The writing attention will be ﬁxed to
use a spatial transformer throughout.
The ﬁnal phase of the generative process transforms the
hidden canvas at the last time step cT into the parameters of
the likelihood function using the output function fo(c; θo).
Since we use a hidden canvas that is the same size as the
original images but that have a different number of ﬁlters,
we implement the output function as a 1 × 1 convolution.
When the hidden canvas has a different size, a convolu-
tional network is used instead.

3.2.4. DEPENDENT POSTERIOR INFERENCE

We use a structured posterior approximation that has an
auto-regressive form, i.e. q(zt|z<t, x). We implement this
distribution as an inference network parameterized by a
deep network. The speciﬁc form we use is:
rt = fr(x, ht−1; φr)

Sprite
(10)
Sample zt∼N (zt|µ(st,ht−1;φµ),σ(rt,ht−1; φσ)) (11)
At every step of computation, we form a low-dimensional
representation rt of the input image using a non-linear

xct 1zt 1ht 1A……fwfczt 1AfwfohTcTGenerative modelAht 1xztfrInference modelxzt 1AfwfohTcTx’hT 1AGenerative modelAht 1xfrx’AztInference modelOne-Shot Generalization in Deep Generative Models

transformation fr of the input image and the hidden state
of the model. This function is a reading function and is
the counterpart to the writing function from the previous
section. The reading function allows the input image to
be transformed into a new coordinate space that allows
for easier inference computations. Like the writing func-
tion, reading can be implemented as a fully- or locally-
connected network (and we compare to a fully-connected
function). Better inference is obtained using a reading or
recognition attention. The result of reading is a sprite rt
that is then combined with the previous state ht−1 through
a further non-linear function to produce the mean µt and
variance σt of a K-dimensional diagonal Gaussian distri-
bution. We denote all the parameters of the inference model
by φ = {φr, φµ, φσ}. Although the conditional distribu-
tions q(zt|z<t) are Gaussian, the joint posterior posterior
is non-Gaussian and can be multi-modal, and thus provides
more accurate inference.

3.2.5. MODEL PROPERTIES AND COMPLEXITY

The above sequential generative model and inference is
a generalization of existing models such as DRAW (Gre-
gor et al., 2015) and composited VAEs (Huang & Murphy,
2015). This generalization has a number of differences and
important properties. One of the largest deviations is the
introduction of the hidden canvas into the generative model
that provides an important richness to the model, since it
allows a pre-image to be constructed in a hidden space be-
fore a ﬁnal corrective transformation, using the function fo,
is used. The generative process has an important property
that allows the model be sampled without feeding-back the
results of the canvas ct to the hidden state ht—such a con-
nection is not needed and provides more efﬁciency by re-
ducing the number of model parameters. The inference net-
work in our framework is also similarly simpliﬁed. We do
not use a separate recurrent function within the inference
network (like DRAW), but instead share parameters of the
LSTM from the prior—the removal of this additional re-
cursive function has no effect on performance.
Another important difference between our framework and
existing frameworks is the type of attention that is used.
Gregor et al. (2015) use a generative attention based on
Gaussian convolutions parameterized by a location and
scale, and Tang et al. (2014) use 2D similarity transforma-
tions. We use a much more powerful and general attention
mechanism based on spatial transformers (Jaderberg et al.,
2015; Huang & Murphy, 2015).
The overall complexity of the algorithm described matches
the typical complexity of widely-used methods in deep
learning. For images of size I × I, the spatial transformer
has a complexity that is linear in the number of pixels
of the attention window. For a J × J attention window,
with J ≤ I, the spatial transformer has a complexity of

O(N T J 2), for T sequential steps and N data points. All
other components have the standard quadratic complexity
in the layer size, hence for L layers with average size D,
this gives a complexity of O(N LD2).
4. Image Generation and Analysis
We ﬁrst show that our models are state-of-the-art, obtain-
ing highly competitive likelihoods, and are able to generate
high-quality samples across a wide range of data sets with
different characteristics.
For all our experiments, our data consists of binary images
and we use use a Bernoulli likelihood to model the proba-
bility of the pixels. In all models we use 400 LSTM hidden
units. We use 12 × 12 kernels for the spatial transformer,
whether used for recognition or generative attention. The
latent variable zt are 4-dimensional Gaussian distributions
and we use a number of steps that vary from 20-80. The
hidden canvas has dimensions that are the size of the im-
ages with four channels. We present the main results here
and any additional results in Appendix A. All the mod-
els were trained for approximatively 800K iterations with
mini-batches of size 24. The reported likelihood bounds
for the training set are computed by averaging the last 1K
iterations during training. The reported likelihood bounds
for the test set were computed by averaging the bound for
24000 random samples (sampled with replacement) and the
reported error bars are the standard-deviations of the mean.

4.1. MNIST and Multi-MNIST
We highlight the behaviour of the models using two data
sets based on the MNIST benchmark. The ﬁrst experi-
ment uses the binarized MNIST data set of Salakhutdinov
& Murray (2008), that consists of 28 × 28 binary images
with 50,000 training and 10,000 test images. Table 1 com-
pares the log-likelihoods obtained on MNIST using exist-
ing models, as well as the models discussed here (with
variances of our estimates in parentheses). The sequential
generative model that uses the spatially-transformed atten-
tion with the CGRU hidden canvas provides the best per-
formance among existing work on this data set. We show
samples from the model in ﬁgure 3.
We form a multi-MNIST data set of 64 × 64 images that
consists of two MNIST digits placed at random locations in
the image (having adapted the cluttered MNIST generator
from Mnih et al. (2014) to procedurally generate the data).
We compare the performance in table 2 and show samples
from this model in ﬁgure 3. This data set is much harder
than MNIST to learn, with much slower convergence. The
additive canvas with spatially-transformed attention pro-
vides a reliable way to learn from this data.
Importance of each step
These results also indicate that longer sequences can lead
to better performance. Every step taken by the model adds

One-Shot Generalization in Deep Generative Models

Table 1. Test set negative log-likelihood on MNIST.

Model

Test NLL

From Gregor et al. (2015) and Burda et al. (2015)

DBM 2hl
DBN 2hl
NADE
DLGM-VAE
VAE + HVI/Norm Flow
DARN
DRAW (40 steps, no attention)
DRAW (40 steps, Gaussian attention)
IWAE (2 layers; 50 particles )

Sequential generative models

Attention
Spatial tr.
Spatial tr.
Spatial tr.
Spatial tr.
Fully conn. CGRU

Canvas
CGRU
Additive
CGRU
Additive

Steps Train
80
80
30
30
80

≈84.62
≈84.55
88.33
≈ 86.60
≈ 85.10
≈ 84.13
≤ 87.40
≤ 80.97
≈ 82.90
Test NLL
78.5 ≤80.5(0.3)
80.1 ≤81.6(0.4)
80.1 ≤81.5(0.4)
79.1 ≤82.6(0.5)
80.0 ≤98.7(0.8)

Figure 3. Generated samples for MNIST. For a video of the gen-
eration process, see https://youtu.be/ptLdYd8FXRA

a term to the objective function (2) corresponding to the
KL-divergence between the prior distribution and the con-
tribution to the approximate posterior distribution at that
step. Figure 4 shows the KL-divergence for each itera-
tion for two models on MNIST up to 20 steps. The KL-
divergence decays towards the end of the sequence, indi-
cating that the latent variables zt have diminishing contri-
bution to the model as the number of steps grow. Unlike
VAEs where we often ﬁnd that there are many dimensions
which contribute little to the likelihood bound, the sequen-
tial property allows us to more efﬁciently allocate and de-
cide on the number of latent variables to use and means of
deciding when to terminate the sequential computation.
4.2. Omniglot
Unlike MNIST, which has a small number of classes with
many images of each class and a large amount of data, the
omniglot data set (Lake et al., 2015) consists of 105 × 105
binary images across 1628 classes with just 20 images per
class. This data set allows us to demonstrate that attentional
mechanisms and better generative models allow us to per-
form well even in regimes with larger images and limited
amounts of data.
There are two versions of the omniglot data that have been
previously used for the evaluation of generative models.
One data set used by Burda et al. (2015) consists of 28× 28

Figure 4. Per-step KL contri-
bution on MNIST.

Figure 5. Gap between train
and test bound on omniglot.

Table 2. Train and test NLL bounds on 64 × 64 Multi-MNIST.

Att
Multi-ST
Spatial tr.
Spatial tr.
Fully conn.

CT

Additive
Additive
CGRU
CGRU

Steps Train
177.2
80
80
183.0
196.0
80
80
272.0

Test

176.9(0.5)
182.0(0.6)
194.9(0.5)
270.3(0.8)

images, but is different to that of Lake et al. (2015). We
compare the available methods on the dataset from Burda
et al. (2015) in table 3 and ﬁnd that the sequential models
perform better than all competing approaches, further es-
tablishing the effectiveness of these models. Our second
evaluation uses the dataset of Lake et al. (2015), which we
downsampled to 52 × 52 using a 2 × 2 max-pooling. We
compare different sequential models in table 4 and again
ﬁnd that spatially-transformed attention is a powerful gen-
eral purpose attention and that the additive hidden canvas
performs best.
4.3. Multi-PIE
The Multi-PIE dataset (Gross et al., 2010) consists of
48 × 48 RGB face images from various viewpoints. We
have converted the images to grayscale and trained our
model on a subset comprising of all 15-viewpoints but only
3 out of the 19 illumination conditions. Our simpliﬁcation
results in 93, 130 training samples and 10, 000 test sam-
ples. Samples from this model are shown in ﬁgure 7 and
are highly compelling, showing faces in different orienta-
tions, different genders and are representative of the data.
The model was trained using the logit-normal density for
the pixels as in Rezende & Mohamed (2015).
5. One-Shot Generalization
Lake et al. (2015) introduce three tasks to evaluate one-shot
generalization, testing weaker to stronger forms of gener-
alization. The three tasks are: (1) unconditional (free) gen-
eration, (2) generation of novel variations of a given ex-
emplar, and (3) generation of representative samples from
a novel alphabet. Lake et al. (2015) conduct human eval-
uations as part of their assessment, which is important in
contrasting the performance of models against the cogni-
tive ability of humans; we do not conduct human bench-
marks in this paper (human evaluation will form part of our
follow-up work). Our focus is on the machine learning of
one-shot generalization and the computational challenges

llllllllll12345101520StepsKLD (nats)llST+CGRUST+Additive12513013514014515015530x2040x1045x5Data splitBound (nats)traintestOne-Shot Generalization in Deep Generative Models

Table 4. Train and test NLL bounds on 52 × 52 omniglot
Att
Test
Multi-ST
Spatial tr.
Spatial tr.
Spatial tr.
Fully conn.

Steps Train
120.6
80
128.7
40
80
134.6
141.6
80
80
170.0

134.1(0.5)
136.1(0.4)
141.5(0.5)
144.5(0.4)
351.5(1.2)

CT

CGRU
Additive
Additive
CGRU
CGRU

Figure 6. Generated samples for multi-MNIST. For a video
of the generation process, see https://www.youtube.com/watch?v=
HkDxmnIfWIM

Table 3. NLL on the 28 × 28 omniglot data.

Model

Test NLL

From Burda et al. (2015)

VAE (2 layer, 5 samples)
IWAE (2 layer, 50 samples)
RBM (500 hidden)
Seq Gen Model (20 steps, ST, additive)
Seq Gen Model (80 steps, ST, additive)

106.31
103.38
100.46
≤96.5
≤95.5

associated with this task.
1. Unconditional Generation.
This is the same generation task reported for the data sets
in the previous section. Figure 8 shows samples that reﬂect
the characteristics of the omniglot data, showing a variety
of styles including rounded patterns, line segments, thick
and thin strokes that are representative of the data set. The
likelihoods reported in tables 3 and 4 quantitatively estab-
lish this model as among the state-of-the-art.
2. Novel variations of a given exemplar.
This task corresponds to ﬁgure 5 in Lake et al. (2015)). At
test time, the model is presented with a character of a type
it has never seen before (was not part of its training set),
and asked to generate novel variations of this character.
To do this, a simple modiﬁcation of the model is made,
and shown in ﬁgure 2(b), in which it is conditioned on an
external context. The context is the image that we wish
the model to generate new exemplars of. To expose the
boundaries of our approach, we test this under weak and
strong one-shot generalization tests:
a) We use a data set whose training data consists of all
available alphabets, but for which three character types
from each alphabet have been removed to form the test
set (3000 characters). This is a weak one-shot general-
ization test where, although the model has never seen
the test set characters, it has seen related characters
from the same alphabet and is expected to transfer that
knowledge to this generation task.

b) We use exactly the data split used by Lake et al. (2015),
which consists of 30 alphabets as the training set and
the remaining 20 alphabets as the test set. This is a
strong one-shot generalization test, since the model has
seen neither the test character nor any alphabets from
its family. This is a hard test for our model, since this
split provides limited training data, making overﬁtting

Figure 7. Generated samples for Multi-PIE using the model with
Spatial Transformer + additive canvas (32 steps). For a video of
the generation process including the boundaries of the writing at-
tention grid, see https://www.youtube.com/watch?v=6S6Tx_OtvnA

easier, and hence generalization harder.

c) We use two alternative training-test split of the data, a
40-10 and 45-5 split. We can examine the spectrum of
difﬁculty of the previous one-shot generalization task
by considering these alternative splits.

We show the model’s performance on the weak generaliza-
tion test in ﬁgure 9, where the ﬁrst row shows the exemplar
image, and the subsequent rows show the variations of that
image generated by the model. We show generations for
the strong generalization test in ﬁgure 10. Our model also
generates visually similar and reasonable variations of the
image in this case. Unlike the model of Lake et al. (2015),
which uses human stroke information and a model struc-
tured around the way in which humans draw images, our
model is applicable to any image data, with the only do-
main speciﬁc information that is used being that the data
is spatially arranged (which is exploited by the convolution
and attention). This test also exposes the difﬁculty that the
model has in coping with small amounts of data. We com-
pare the difference between train and test log-likelihoods
for the various data splits in ﬁgure 5. We see that there is
a small gap between the training and test likelihoods in the
regime where we have more data (45-5 split) indicating no
overﬁtting. There is a large gap for the other splits, hence
a greater tendency for overﬁtting in the low data regime.
An interesting observation is that even for the cases where
there is a large gap between train and test ( ﬁgure 5 ) like-
lihood bounds, the examples generated by the model (ﬁg-
ure 10, left and middle) still generalize to unseen character
classes. Data-efﬁciency is an important challenge for the
large parametric models that we use and one we hope to
address in future.

One-Shot Generalization in Deep Generative Models

30-20

40-10

45-5

Figure 8. Unconditional samples for 52 × 52 omniglot (task 1).
For a video of the generation process, see https://www.youtube.com/
watch?v=HQEI2xfTgm4

Figure 10. Generating new examplars of a given character for the
strong generalization test (task 2b,c), with models trained with
different amounts of data. Left: Samples from model trained on
30-20 train-test split; Middle: 40-10 split; Right: 45-5 split (right)

Figure 9. Generating new examplars of a given character for the
wea k generalization test (task 2a). First row shows the test im-
ages and the next 10 are one-shot samples from the model.

3. Representative samples from a novel alphabet.
This task corresponds to ﬁgure 7 in Lake et al. (2015), and
conditions the model on anywhere between 1 to 10 samples
of a novel alphabet and asks the model to generate new
characters consistent with this novel alphabet. We show
here the hardest form of this test, using only 1 context im-
age. This test is highly subjective, but the model genera-
tions in ﬁgure 11 show that it is able to pick up common
features and use them in the generations.
We have emphasized the usefulness of deep generative
models as scalable, general-purpose tools for probabilistic
reasoning that have the important property of one-shot gen-
eralization. But, these models do have limitations. We have
already pointed to the need for reasonable amounts of data.
Another important consideration is that, while our models
can perform one-shot generalization, they do not perform
one-shot learning. One-shot learning requires that a model
is updated after the presentation of each new input, e.g.,
like the non-parametric models used by Lake et al. (2015)
or Salakhutdinov et al. (2013). Parametric models such as
ours require a gradient update of the parameters, which we
do not do. Instead, our model performs a type of one-shot
inference that during test time can perform inferential tasks
on new data points, such as missing data completion, new
exemplar generation, or analogical sampling, but does not
learn from these points. This distinction between one-shot
learning and inference is important and affects how such
models can be used. We aim to extend our approach to the
online and one-shot learning setting in future.

Figure 11. Generating new exemplars from a novel alphabet (task
3). The ﬁrst row shows the test images, and the next 10 rows are
one-shot samples generated by the model.
6. Conclusion
We have developed a new class of general-purpose mod-
els that have the ability to perform one-shot generalization,
emulating an important characteristic of human cognition.
Sequential generative models are natural extensions of vari-
ational auto-encoders and provide state-of-the-art models
for deep density estimation and image generation. The
models specify a sequential process over groups of latent
variables that allows it to compute the probability of data
points over a number of steps, using the principles of feed-
back and attention. The use of spatial attention mechanisms
substantially improves the ability of the model to general-
ize. The spatial transformer is a highly ﬂexible attention
mechanism for both reading and writing, and is now our
default mechanism for attention in generative models. We
highlighted the one-shot generalization ability of the model
over a range of tasks that showed that the model is able to
generate compelling and diverse samples, having seen new
examples just once. However there are limitations of this
approach, e.g. still needing a reasonable amount of data to
avoid overﬁtting, which we hope to address in future work.

One-Shot Generalization in Deep Generative Models

Acknowledgements
We thank Brenden Lake and Josh Tenenbaum for insight-
ful discussions. We are grateful to Theophane Weber, Ali
Eslami, Peter Battaglia and David Barrett for their valuable
feedback.

References
Ba, Jimmy, Salakhutdinov, Ruslan R, Grosse, Roger B, and
Frey, Brendan J. Learning wake-sleep recurrent attention
models. In Advances in Neural Information Processing
Systems, pp. 2575–2583, 2015.

Burda, Yuri, Grosse, Roger, and Salakhutdinov, Rus-
lan. Importance weighted autoencoders. arXiv preprint
arXiv:1509.00519, 2015.

Chikkerur, Sharat, Serre, Thomas, Tan, Cheston, and Pog-
gio, Tomaso. What and where: A bayesian inference
theory of attention. Vision research, 50(22):2233–2247,
2010.

Erdogan, G., Yildirim, I., and Jacobs, R. A. An analysis-
by-synthesis approach to multisensory object shape per-
ception. In NIPS, 2015.

Gregor, Karol, Danihelka, Ivo, Mnih, Andriy, Blundell,
Charles, and Wierstra, Daan. Deep autoregressive net-
works. In ICML, 2014.

Gregor, Karol, Danihelka,

Ivo, Graves, Alex,
Jimenez Rezende, Danilo, and Wierstra, Daan. Draw:
A recurrent neural network for image generation.
In
ICML, 2015.

Gross, Ralph, Matthews, Iain, Cohn, Jeffrey, Kanade,
Takeo, and Baker, Simon. Multi-pie. Image and Vision
Computing, 28(5):807–813, 2010.

Hochreiter, Sepp and Schmidhuber, J¨urgen. Long short-
term memory. Neural computation, 9(8):1735–1780,
1997.

Huang, Jonathan and Murphy, Kevin. Efﬁcient inference
in occlusion-aware generative models of images. arXiv
preprint arXiv:1511.06362, 2015.

Jaderberg, Max, Simonyan, Karen, Zisserman, Andrew,
In Advances in
et al. Spatial transformer networks.
Neural Information Processing Systems, pp. 2008–2016,
2015.

Jordan, Michael

I, Ghahramani, Zoubin,

Jaakkola,
Tommi S, and Saul, Lawrence K. An introduction
to variational methods for graphical models. Machine
learning, 37(2):183–233, 1999.

Kaiser, Łukasz and Sutskever, Ilya. Neural gpus learn al-

gorithms. arXiv preprint arXiv:1511.08228, 2015.

Kingma, D. P. and Welling, M. Auto-encoding variational

Bayes. In ICLR, 2014.

Lake, Brenden M, Salakhutdinov, Ruslan, and Tenenbaum,
Joshua B. Human-level concept learning through prob-
abilistic program induction. Science, 350(6266):1332–
1338, 2015.

Larochelle, Hugo and Hinton, Geoffrey E. Learning to
combine foveal glimpses with a third-order boltzmann
machine. In Advances in neural information processing
systems, pp. 1243–1251, 2010.

Mansimov, Elman, Parisotto, Emilio, Ba, Jimmy Lei, and
Salakhutdinov, Ruslan. Generating images from cap-
tions with attention. arXiv preprint arXiv:1511.02793,
2015.

Mnih, A. and Gregor, K. Neural variational inference and

learning in belief networks. In ICML, 2014.

Mnih, Volodymyr, Heess, Nicolas, Graves, Alex, et al. Re-
current models of visual attention. In Advances in Neural
Information Processing Systems, pp. 2204–2212, 2014.
Nair, Vinod, Susskind, Josh, and Hinton, Geoffrey E.
Analysis-by-synthesis by learning to invert generative
black boxes. In Artiﬁcial Neural Networks-ICANN 2008,
pp. 971–981. Springer, 2008.

Netzer, Yuval, Wang, Tao, Coates, Adam, Bissacco,
Alessandro, Wu, Bo, and Ng, Andrew Y. Reading dig-
its in natural images with unsupervised feature learning.
In NIPS workshop on deep learning and unsupervised
feature learning, volume 2011, pp. 5. Granada, Spain,
2011.

Ranganath, Rajesh, Gerrish, Sean, and Blei, David M.

Black box variational inference. In AISTATS, 2014.

Rezende, D. J., Mohamed, S., and Wierstra, D. Stochas-
tic backpropagation and approximate inference in deep
generative models. In ICML, 2014.

Rezende, Danilo Jimenez and Mohamed, Shakir. Varia-
tional inference with normalizing ﬂows. arXiv preprint
arXiv:1505.05770, 2015.

Salakhutdinov, Ruslan and Murray, Iain. On the quantita-
tive analysis of deep belief networks. In Proceedings of
the 25th international conference on Machine learning,
pp. 872–879, 2008.

Salakhutdinov, Ruslan, Tenenbaum, Joshua B, and Tor-
ralba, Antonio. Learning with hierarchical-deep models.
Pattern Analysis and Machine Intelligence, IEEE Trans-
actions on, 35(8):1958–1971, 2013.

Saul, Lawrence K,

Jaakkola, Tommi,

and Jordan,
Michael I. Mean ﬁeld theory for sigmoid belief net-
works. Journal of artiﬁcial intelligence research, 4(1):
61–76, 1996.

Tang, Yichuan, Srivastava, Nitish, and Salakhutdinov, Rus-
lan R. Learning generative models with visual attention.
In Advances in Neural Information Processing Systems,
pp. 1808–1816, 2014.

Xu, Kelvin, Ba, Jimmy, Kiros, Ryan, Courville, Aaron,
Salakhutdinov, Ruslan, Zemel, Richard, and Bengio,
Yoshua. Show, attend and tell: Neural image caption
generation with visual attention. In ICML, 2015.

Yuille, Alan and Kersten, Daniel. Vision as bayesian in-
ference: analysis by synthesis? Trends in cognitive sci-
ences, 10(7):301–308, 2006.

One-Shot Generalization in Deep Generative Models

model with this attention mechanism. In this type of hard-
attention, a policy does not need to be learned, since a new
one is obtained after every step based on the reconstruction
error and effectively allows every step to work more efﬁ-
ciently towards reducing the reconstruction error. It also
overcomes the problem of limited gradient information in
large, sparse images, since this form of attention will have
a saccadic behaviour since it will be able to jump to any
part of the image that has high error.
Multiple spatial attention. A simple generalization of us-
ing a single spatial transformer is to have multiple STs that
are additively combined:

K(cid:88)

i=1

y(v) =

[κh(hi(v)) ⊗ κw(hi(v))] ∗ xi(v),

where v is a context that conditions all STs. This mod-
ule allows the generative model to write or read at multiple
locations simultaneously.

C. Other model details
The CGRU of Kaiser & Sutskever (2015) has the following
form:

fc(ct−1, ht; θc) = CGRU(ct−1 + fw(ht; θc)),
(12)
CGRU(c) = u (cid:12) c + (1 − u) (cid:12) tanh(U ∗ (r (cid:12) c) + B),
u = σ(U(cid:48) ∗ c + B(cid:48)),
where the symbols (cid:12) indicates the element-wise product,
∗ a size-preserving convolution with stride of 1 × 1, and
σ(·) is the sigmoid function. The matrices U, U(cid:48) and U(cid:48)(cid:48)
are 3× 3 kernels. The number of ﬁlters used for the hidden
canvas c is speciﬁed on section 4.

r = σ(U(cid:48)(cid:48) ∗ c + B(cid:48)(cid:48))

Figure 12. Generated samples for SVHN using the model with
Spatial Transformer + Identity (80 steps). For a video of the
generation process, see this video https://www.youtube.com/watch?
v=281wqqkmAuw

A. Additional Results
A.1. SVHN
The SVHN dataset (Netzer et al., 2011) consists of 32× 32
RGB images from house numbers.

B. Other types of attention
Randomized attention. The simplest attention randomly
selects patches from the input image, which is the sim-
plest way of implementing a sparse selection mechanism.
Applying dropout regularisation to the input layer of deep
models would effectively implement this type of attention
(a hard attention that has no learning).
In data sets like
MNIST this attention allows for competitive learning of
the generative model if the model is allowed to attend to a
large number of patches; see this video https://www.youtube.
com/watch?v=W0R394wEUqQ.
Error-based attention. One of the difﬁculties with at-
tention mechanisms is that for large and sparse images,
there can be little gradient information available, which can
cause the attentional selection to become stuck. To address
this issue, previous approaches have used particle methods
(Tang et al., 2014) and exploration techniques from rein-
forcement learning (Mnih et al., 2014) to infer the latent
variables that control the attentional, and allow it to jump
more easily to relevant parts of the input. A simple way of
realizing this, is to decide where to attend to by jumping o
places where the model has made the largest reconstruction
errors. To do this, we convert the element-wise reconstruc-
tion error at every step into a probability map of locations
to attend to at the next iteration:

(cid:18)

(cid:19)

p(at = k|x, ˆxt−1) ∝ exp

k − ¯
κ |

−β|

where k = xk − ˆxt−1,k is the reconstruction error of the
kth pixel, ˆxt−1 is the reconstructed image at iteration t− 1,
x is the current target image, ¯ is the spatial average of k,
and κ is the spatial standard deviation of k. This atten-
tion is suited to models of sparse images ; see this video
https://www.youtube.com/watch?v=qb2-73OHuWA for an example of a

