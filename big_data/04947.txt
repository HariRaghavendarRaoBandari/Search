6
1
0
2

 
r
a

 

M
6
1

 
 
]

G
L
.
s
c
[
 
 

1
v
7
4
9
4
0

.

3
0
6
1
:
v
i
X
r
a

On the Complexity of One-class SVM for Multiple

Instance Learning

Zhen Hu a,∗ Zhuyin Xue a

aSci. & Technol. on Inf. Syst. Eng. Lab., Nanjing 210000, China

Abstract

In traditional multiple instance learning (MIL), both positive and negative bags are required to learn
a prediction function. However, a high human cost is needed to know the label of each bag—positive
or negative. Only positive bags contain our focus (positive instances) while negative bags consist of
noise or background (negative instances). So we do not expect to spend too much to label the negative
bags. Contrary to our expectation, nearly all existing MIL methods require enough negative bags
besides positive ones. In this paper we propose an algorithm called “Positive Multiple Instance” (PMI),
which learns a classiﬁer given only a set of positive bags. So the annotation of negative bags becomes
unnecessary in our method. PMI is constructed based on the assumption that the unknown positive
instances in positive bags be similar each other and constitute one compact cluster in feature space
and the negative instances locate outside this cluster. The experimental results demonstrate that PMI
achieves the performances close to or a little worse than those of the traditional MIL algorithms on
benchmark and real data sets. However, the number of training bags in PMI is reduced signiﬁcantly
compared with traditional MIL algorithms.

Keywords: Multiple Instance Learning; One-class

1

Introduction

Multiple instance learning (MIL) is introduced by [9] to solve the drug activity prediction prob-
lem. During the past years, MIL approaches have been applied successfully to many real appli-
cations such as image categorization [5, 6, 19], image retrieval [24, 22], tracking [3], web mining
[21], gene expression [15] and medical diagnosis [10].
In traditional supervised learning, each
instance(feature vector) of training set corresponds to one given label. By contrast, every set of
instances (not one instance) is associated with a given label in MIL. Each instance set is called
a “bag”. If a bag contains at least one positive instance, it is labeled as positive and negative
otherwise. It is unknown which instance is positive in each bag. In other words, only bag level
label is available while instance level label is not in positive bags. The task of MIL is to learn a
concept to predict the label of an unseen bag.

(cid:63)Project supported by the National Nature Science Foundation of China (No. ***).
∗Corresponding author.
Email address: 49859211@qq.com (Zhen Hu).

1

In nearly all existing MIL algorithms, both positive and negative bags are required during the
training phase. But only the positive instances are our focus generally and the negative instances
are unrelated to our interest. For example, if we attempts to learn a concept of face, the face
patches in an image are positive instances and other non-face images are labeled as negative bags.
According to this preference, we are willing to concentrate on labeling face images and ignore
non-face images as much as possible. Non-face images are not our interest. However, negative
bags are not less and even much more than positive in many real data sets such as Corel [7].
The availability of negative bags requires a high cost. So labeling on negative bags brings great
inconvenience in real applications. In addition, it takes more time to label one negative bag than
positive in that every instance must be conﬁrmed to be negative. In contrast, remaining instances
in one positive bag can be ignored if only one positive instance is found. The labeling on negative
bags increases label cost signiﬁcantly. So it is necessary to design a method using only positive
bags to learn.

One related work is [22]. His method is simply to solve a query problem, in which an extra
positive instance must be provided by user. The most similar instance in every positive bag to this
provided positive instance is considered positive. This solution has the following disadvantages.
Firstly, the positive instance is diﬃcult to provide or unavailable in some applications. So [22]
can be applied to a limited range of settings. Secondly, some bags contain more than one positive
instances.
[22] cannot select multiple positive instances accurately in one bag. Thirdly, the
performance of this algorithm depends mostly upon the positive instance given by user. It causes
the prediction accuracy of [22] is too sensitive to the provided positive instance. In contrast, our
proposed algorithm in this paper requires no additional positive instance in most cases. Therefore,
our proposed algorithm can solve a wider range of problems.

In this paper, a new algorithm called PMI(Positive Multiple Instance) is proposed, which learns
a concept from only positive training bags. PMI is designed based on the assumption that the
positive instances constitute one compact cluster in feature space and most negative instances
locate outside this cluster. PMI works with two major steps—training and query. The training
step is to convert the problem of learning with only positive bags into a one-class classiﬁcation
problem [18] to get a classiﬁer f . If the instance level label is not available, PMI terminates and
outputs f as the concept. Otherwise, go to the query step. The query step is to select an instance
to query its label from instances positively labeled by the classiﬁer f in the previous training
step. If the queried instance is negative, remove the instances positively labeled by the classiﬁer
f from the training bags and return to the training step. Otherwise, PMI outputs f as the desired
concept and terminates. We provide the maximum number of queried instances theoretically. The
queried instance number in real applications usually is far smaller than the theoretical result. The
experimental results demonstrate that PMI achieves close performances to those of the traditional
MIL algorithms on most data sets. Our contribution is that the training bags number can be
reduced greatly at a little or no worse on accuracy. Negative bags are unnecessary for training in
our method.

The remainder of this paper is organized as follows. We introduce PMI algorithm in Section
2 in detail. Section 3 illustrates PMI with experiments. We conclude on our work in Section 4
ﬁnally.

2

2 Our Proposed Algorithm

In this section, we propose PMI algorithm. One-class SVM [18] is involved in PMI, so a brief
review on one-class SVM is presented here.

2.1 Review on One-Class SVM

The formal deﬁnition of one-class SVM is described as following. N d-dimensional instances
x1, x2, ..., xN in feature space Rd are given, where R denotes real number ﬁeld. The task is to
learn a prediction function takes value +1 while capturing most given instances in a small region
of feature space and -1 otherwise. One-class SVM maximizes the margin between the training
instances and origin in feature space to obtain a decision hyperplane by the following formulation:

(cid:88)

ξi − ρ

1
νN

(cid:107)w(cid:107)2 +

1
2

min
w,ρ,ξ,ν
s. t. (w · Φ(xi)) ≥ ρ − ξi,
ξi ≥ 0, w ∈ Rd, ρ ∈ R,

i

(1)

(cid:88)

ξi, w and

i

where ν ∈ (0, 1) is a parameter to balance the regularization (cid:107)w(cid:107)2 and the hinge loss

ρ are the weight coeﬃcient and the bias in linear decision function respectively, ξ, ξT = [ξ1, ..., ξn]
denotes a slack variable vector, ξi is the i-th element of ξ, and Φ(xi) is a nonlinear map function
to xi to deal with nonlinear boundary of training instances. A kernel function ker(x, y) is deﬁned
to replace Φ(x)·Φ(y). Model 1 is a quadratic programming problem, which can be solved through
its dual form:

(cid:88)

i,j

min

α

1
2

αiαjker(xi, xj)

s. t. 0 ≤ αi ≤ 1
νN

,

(cid:88)

i

αi = 1,

(2)

where αi is a Lagrange multiplying factor of xi. The value of αi is summarized into three categories
according to where xi locates:

1. αi = 0 → xi locates inside positive instance region;
2.

νN > αi > 0→ xi lies on the prediction function boundary;

1

3. αi = 1

νN→ xi falls outside the positive instance region.

The label of an unseen instance x is predicted by the following function:

f (x) = sign(

αiker(x, xi) − ρ),

(3)

where the function sign(y) outputs +1 if y ≥ 0 and -1 otherwise. If f (x) ≥ 0, we assume that x
should be similar to the training instances with a high likelihood. Otherwise, x is an outlier. ρ
in Eq. (3) is computed:

ρ =

αiker(xi, xj),

(4)

(cid:88)

i

(cid:88)

i

3

where xj is a support vector, which implies that xj locates on the boundary of decision function
f (x) and f (x) = 0.

Various complex separation hyperplanes can be described by diﬀerent types of kernel functions,

such as polynomial, RBF, sigmod or self-deﬁned ones. RBF kernel is given by:

ker(x, y) = eγ(cid:107)x−y(cid:107)2,

(5)

where γ denotes a parameter to control how similar x is to y. In general, we assume that samples
of interest locate inside closed regions in the feature space. RBF function is one of the most
widely used kernel for its ﬂexibility. We will use RBF as the default kernel function in the rest of
this paper.

2.2 Our Proposed Algorithm

Our proposed algorithm PMI is introduced in detail in this subsection. At the beginning, we
provide the formal deﬁnition of the problem PMI solves. A collection of bags {B1, B2, ..., BN} are
given as the training set. The i-th bag in the training set Bi contains Ni d-dimensional instances
Bi = [Bi1, ..., BiNi] ∈ Rd×Ni and Bij ∈ Rd is a d-dimensional instance, j = 1, ..., Ni. We deﬁne an
d × n matrix stacking all instances together B = [B1, ..., BN ], where n denotes the total number
of all instances in N bags. At least one positive instance resides in each bag of training set, but
it is unknown which instance is positive. The same as the label rule in previous MIL approaches,
a bag is positive if it contains at least one positive instance and is negative otherwise. Our goal
is to learn a function to predict the label of an unseen bag. PMI works in two steps: training
and query. If instance level label is available, PMI algorithm alternates between training and
query step until the desired concept is obtained. Otherwise, PMI runs training step for once.
The remainder describes the two steps explicitly.

2.2.1 Training Step

As is mentioned in Section Introduction, PMI assumes that positive instances be similar to each
other in feature space. We explain this assumption in Figure 1. Take face identiﬁcation as an
example, the task is to learn a function to predict whether an image contains face or not. So the
patches containing face are positive instances we are focus on in Figure 1 and the other non-face
patches are negative instances. It can be easily found that face patches (red rectangles in Figure 1)
look similar to each other while non-face patches are dissimilar. In summary, positive instances
refer to one concept leading to the high similarity between positive and negative instances usually
locate outside the region occupied by positive instances in feature space.

According to the previous explanation and illustration in Figure 1, we assume that most positive
instances gather in a compact cluster and negative instances locate outside this cluster. At least
one instance in most training bags resides in this positive cluster (the cluster composed of positive
instances). Now we needs a function to describe the positive cluster. This function takes the value
+1 in the positive cluster region and takes -1 elsewhere. We can apply this function on every
instance in one bag to identify whether this bag is positive or not. But since we actually do not
which instance is positive, the positive concept cannot directly be learned from positive instances.

As is stated in [2, 10], the selection of instances will be formulated as a combinatorial optimiza-
tion problem, which is non-convex and diﬃcult to solve especially when a global optimal solution

4

Fig. 1: It is illustrated positive instances are clustered in a small region and negative instances locate
outside. The red solid points are positive and the green circle is negative.

is required. We tend to approximate one virtual instance bi with a high likelihood to be positive
for each training bag Bi. The desired concept can be learned from these approximated positive
instances. To make our method eﬃcient and easy to solve, a linear combination coeﬃcient vector
λi is used to convert each bag Bi to a virtual positive instance bi in feature space:

Ni(cid:88)

bi = Biλi =

Bijλij,
λij = 1, λij ≥ 0,

j=1

s.t.

(cid:88)

j

i = 1, ..., N,

(6)

where λi = [λi1, ..., λiNi](cid:62). Let λ denote the vector concatenating all λi, λ = [λ1; λ2; ...; λN ].
Assume that bi is positive, bi should be highly close to the positive instances in Bi. Furthermore,
if Bij is positive, λij will be assigned a larger value and small otherwise. According to the
assumption that the positive instances get similar to each other, all bi should get close to each
other in feature space. To keep the distances between bi as small as possible, the variance between
all bi is minimized to obtain λ:

N(cid:88)
Ni(cid:88)

i=1

min

s. t.

(Biλi − mi)T (Biλi − mi)

λij = 1, λij ≥ 0,

(7)

j=1

N(cid:88)

Biλi.

where mi =

1
N

i=1

We deﬁne matrix Zi with n× n size, i = 1, ..., N . The diagnose elements in Zi are 1 at location

N(cid:88)

i=1

Zi = I, where I is an

j = Ni−1 + 1, ..., Ni in Zi and 0 elsewhere. It satisﬁes that

(j, j),
identity matrix with n × n size.

5

Eq. (7) is rewritten as a standard quadratic programming formulation:

min λT (

[BZi − 1
N

B]T [BZi − 1
N

B])λ

s. t.

λij = 1, λij ≥ 0,

B]T [BZi − 1
N

where [BZi − 1
N
B]T [BZi − 1
1
N
N
is equal to Biλi. The formulation Eq. (8) is a convex optimization problem.

B] is really symmetric and semi-positively deﬁnite, so (

B]) is also real symmetric and semi-positive deﬁnite. The non-zero part of BZiλ

To tackle nonlinear separable data, the kernel trick is applied to Eq. (8). We reformulate the
B(cid:62)B as the kernel matrix K, whose element Kij at i-th row and j-th column is the kernel function
value ker(xi, xj), i, j = 1, ..., n. Eq. (8) is reformulated:

N(cid:88)
Ni(cid:88)

i=1

j=1

N(cid:88)
Ni(cid:88)

i=1

j=1

min λT (

[KZi − 2
N

KZi +

1
N 2 K])λ

s. t.

λij = 1, λij ≥ 0.

Ni(cid:88)

Nj(cid:88)
(cid:88)

r=1

k,r

(cid:88)

k=1

αiαj

ker(bi, bj) =

λikλjrker(BikBjr).

1
2

min

α

s. t.

λikλjrker(Bik, Bjr)

i,j

0 ≤ αi ≤ 1
νN

,

αi = 1.

(cid:88)

i

f (Bi) = sign(cid:0) max
Ni(cid:88)

N(cid:88)

r=1,...,Ni

l(Bir) =

αj

l(Bir)(cid:1),

λjkker(Bir, Bjk) − ρ,

j=1

k=1

6

(8)

N(cid:88)

i=1

[BZi −

(9)

(10)

(11)

(12)

As λ is known by Eq. (9), we can learn the concept of positive instance from {bi, ..., bN} using
one-class SVM method. The kernel function value between two virtual positive instances is
computed:

Eq. (2) is reformulated as the following:

As is described previously, αi in three diﬀerent ranges implies where bi locates in feature space.
According to the value of αi, bag Bi can be summarized as two kinds:

Deﬁnition 1 (Support Bag) For a bag Bi, if 0 < αi < 1

νN , then Bi is a “Support Bag”.

Deﬁnition 2 (Outlier Bag) For a bag Bi, if αi = 1

νN , then Bi is an “Outlier Bag”.

By solving Eq. (11), the prediction function is:

where Bi is the predicted bag. ρ is computed:

N(cid:88)

Ni(cid:88)

Nj(cid:88)

ρ =

αi

λikλjrker(Bik, Bjr),

i=1

k=1

r=1

Bj satisﬁes: 0 < αj <

1
νN

.

If it holds that max

j

f (Bij) = f (bi),∀i = 1, ..., N , ν satisﬁes the following theorem:

(13)

Theorem 1 If ρ((cid:54)= 0) is the solution of Eq. (11) and max
following holds: ν is the upper bound on the fraction of outlier bags.

j

f (Bij) = f (bi)(∀i = 1, ..., N ), the

Proof Each bag is converted to one virtual instance by Eq. (6). Theorem 1 is easy to be validated
from proposition 4 in [18][17]. 2

If f (Bi) = f (bi) = 1 , at least one instance in Bi must locate on or inside the prediction
function boundary(Eq. (12)). However, f (Bi) = f (bi) = 1 cannot be guaranteed completely. In
other words, when the virtual instance bi = Biλi falls inside or on the function boundary, all
instances in Bi may fall outside. It can be expressed as follows:

f (bi) = +1 (cid:54)= max

j=1,...,Ni

f (Bij) = −1.

(14)

j

So the assumption max

f (Bij) = f (bi) in Eq. (1) does not always hold in the proof of Theorem 1.
It causes the upper bound on outlier bags fraction is larger than ν sometimes. To keep the
upper bound of outlier bags fraction on ν, our solution is to select the instance Bisi that is
closest to function boundary in each bag Bi and add it to the training set. We learn a new
prediction function with the updated training set. Now the training set consists of both the
selected instances {Bisi} and the virtual instances {bi}. The parameter to control the number of
outlier bag becomes ν
2 since the number of training instances is 2N here. Bisi is determined by
the following criterion:

si = arg max

j=1,...,Ni

l(Bij).

(15)

The new decision function will replace the result of Eq. (11) by solving the following quadratic
programming problem:

αp = 1

(16)

(cid:88)

1
2

(cid:88)

min

α

s. t.

ker(xp, xq)

p,q

p,q

αpαq

(cid:88)
xp, xq ∈ {bi} ∪ {Bisi},

0 ≤ αp ≤ 1
ν2N

,

p

i = 1, ..., N,

p, q = 1, ..., 2N.

7

Fig. 2: The distribution of positive (red) and negative (blue) instances is illustrated. The positive
instances are similar to each other and gathered compactly. By comparison, the negative are dissimilar
and scattered.

The prediction function from Eq. (16) is:

l(Bir)(cid:1)

r=1,...,Ni

αpker(Bir, xp) − ρ,

f (Bi) = sign(cid:0) max
2N(cid:88)

2N(cid:88)

l(Bir) =

p=1

p=1

ρ =

αpker(xp, xq), xq satisﬁes: 0 < αq <

(17)

1

ν2N

.

Now Eq. (17) is the desired prediction function instead of Eq. (12) if ∃i, f (Bi) (cid:54)= f (bi).

In Eq. (16), the upper bound on outlier bags fraction satisﬁes the following theorem:

Theorem 2 If ρ((cid:54)= 0) is the solution of Eq. (16), the following holds: ν is the upper bound on
the fraction of outlier bags.

Proof If bi falls outside and Bisi locates inside the prediction function boundary, it holds max
f (Bisi) = 1). In the worst case, there are ν
the prediction function boundary. This implies that the largest number of outlier bags is νN . So
in the worst case the upper bound on outlier bag fraction reaches ν in Eq. (17). 2

2 × 2N = νN instances in the set {Bisi} falls outside

j

f (Bij =

2.2.2 Query Step

As is described previously, the positive instances should gather compactly in feature space. On
the other hand, the distribution of negative instances is unknown. If the negative instances are
scattered and dissimilar to each other greatly (Figure 2), we can get the prediction function
to enclose most positive instances in feature space easily. However, negative instances are also

8

−15−10−5051015−15−10−5051015xyNegative instances are scattered.  positive instancenegative instanceqeury its label:+1Fig. 3: The distribution of positive (red) and negative (blue) instances is illustrated. Both the positive
and negative instances are clustered. The number of positive instances (red) is equal to that of negative,
but negative occupy a smaller circle region. So the negative cluster looks more compact than positive.
The query step is needed. The black elliptical circle is positive cluster’s boundary. The queried instances
(black and green) locate closest to the centers of positive and negative cluster respectively.

clustered sometimes (Figure 3) and even share a higher similarity than positive. Take face iden-
tiﬁcation as an example, some face images are used to learn a prediction function and each image
contains the same background (for example, tree, sky and so on). If there is a higher similarity
between background patches than face ones, the variance of negative instances get smaller than
that of positive. The result of training step is to enclose the negative instances with a higher
similarity instead of the desired positive cluster. To avoid this result, our method requires to
conﬁrm whether the cluster enclosed by prediction function boundary is composed of positive or
negative instances. We select an instance Bqr characterizes this cluster best to query its label.
The queried instance shares the same label with the most member of this cluster. If the queried
instance is positive, this cluster is positive with a high likelihood and negative otherwise.

The function l(x) is a good measure on the membership of x to the resulted cluster. When x
locates outside the cluster, l(x) < 0 and l(x) ≥ 0 otherwise. The larger l(x) is, the closer x gets
to the center of the cluster. So the instance Bqr with maximum l(x) characterizes this cluster
best and is selected to query its label:

Bqr = arg max

i,j

l(Bij)
l(Bij) ≥ 0

s.t.

(18)

If prediction function boundary is like a circle, the queried instance usually locates closest to
the circle center than others (Figure 3). If Bqr is negative, this cluster should be also negative.
Otherwise, this cluster should be positive. Figure 3 illustrates the queried instance location when
there are more than one cluster in feature space.

If the queried instance is negative, we update the current training bags by removing the in-
stances satisfying {Bij|l(Bij) ≥ 0} and go back to the training step for a second instance label
query. PMI works in such a recycle between training and query steps. PMI terminates when
the queried instance is positive or there is one empty bag in training set. PMI is summarized in
Algorithm 1.

9

−15−10−5051015−15−10−5051015xyNegative instances are compact.  positive instancenegative instancequery its label:+1query its label:−1In Algorithm 1, the number of queried instances satisﬁes the following:

Theorem 3 In Eq. (11), if the parameter ν <

, the maximum number of queried instances is

1
N

Ni − 1 . Otherwise, the maximum number of queried instance is (cid:100)

min
i=1,...,N

n

(1 − ν)N

(cid:101) − 1.

Proof According to Theorem 1, ν < 1
N implies there is no outlier bag in the current training set.
The step 4 in Algorithm 1 guarantees that there is at least one instance Bij satisfying f (Bij) = 1
for every bag Bi,
i = 1, ..., N . Thus, At least one instance in each bag must be removed in every
instance removal operation. If one training bag gets empty after removal, PMI terminates at the
step 6 in Algorithm 1. Therefore, the maximum number of queried instances is mini=1,...,N Ni − 1.
N , at least (1 − ν)N instances will be removed at the step 8 according to Theorem 1.
(1−ν)N(cid:101) − 1, where (cid:100)x(cid:101) denotes the minimum

So the maximum number of queried instances is (cid:100)
integer number not smaller than x. 2

If ν ≥ 1

n

The step 6 in Algorithm 1 needs some further explanations.

If all members of one bag are
labeled positive, these instances belongs to the positive class in that each training bag contains
at least one positive instance.

However, the instance label information is not always available. When it is diﬃcult to query
instance label, our solution is to assume that the dissimilarities between negative instances are
larger than positive ones. Thus, Algorithm 1 terminates at the step 4 and outputs the prediction
function f (x) without running the query step.
In most real applications, negative instances
cover a diverse range of backgrounds or noise. Therefore, it is almost impossible that negative
instances share a higher similarity than positive. It is high likely to achieve the concept on positive
instance after only one training step. The experimental results in the next section illustrate that
our hypothesis is applicable for most of data sets. It also implies that the maximum query number
in Theorem 3 is not a tight bound. The number of queried instance label usually is far smaller
than the result in Theorem 3 and close to 1 mostly. It suggests that the cost of instance label
query is very limited, which is much lower than that of labeling a large number of negative bags.

Algorithm 1 PMI
Input: the training bags B = {Bi,
kernel function parameters;
ν;

i = 1, ..., N};

1: Solve Eq. (9) to get λ;
2: Solve Eq. (11) to get a prediction function f0(Eq. (12)). Set f = f0;
3: If there exists one bag Bi satisfying both αi < 1

instance Bisi by Eq. (18) from every training bag Bi, i = 1, ..., N ;

νN and maxj∈1,...,Ni f (Bij) = −1, select an

4: Solve Eq. (16) to get a prediction function f1(Eq. (17)). Set f = f1;
5: If no instance label information is available, PMI terminates;
6: If there is one bag Bi, i = 1, ..., N satisfying that ∀j ∈ 1, ..., Ni f (Bij) = 1, PMI terminates;
7: Select the most certain instance by Eq. (18) to query its label;
8: If the queried label is positive, PMI ends. Otherwise, update the training set B by removing

the instances ∀i, j Bij satisfying f (Bij) = 1. Go back to the step 1;

Output: prediction function f ;

10

2.3 Time Complexity Analysis

The time complexity analysis of PMI involves Eq. (9), Eq. (11), Eq. (16) and the number of
the queried instances. These three objective functions Eq. (9), Eq. (11) and Eq. (16) are three
quadratic programming problems with the time complexities O(n3), O(N 3), O(N 3) respectively.
Since n > N in general, the term O(n3) dominates the time complexity of PMI. According to

).
Theorem 3, the number of queried instances is related to
But in real applications, the queried instance number is much smaller than the theoretical result
in Theorem 3 and can be approximated to a constant. The time complexity of PMI is close to
O(n3).

. So the time complexity is O(

n
N

n4
N

3 Experiments

This section presents the experimental results on ﬁve benchmark and one face image data sets.
We evaluate PMI compared the traditional MIL algorithms. To solve the quadratic programming
problem in Eq. (2), the optimization toolbox “Mosek” [16] is used.

3.1 Benchmark Data Sets

Five benchmark data sets are used in our experiment. They are Musk1, Musk2, Elephant, Fox
and Tiger, which were used frequently to test new MIL algorithms in previous studies[9, 10, 2].
Both Musk1 and Musk2 come from UCI data set web site [20]. And the other three derive from
Corel image set [7]. The details of ﬁve data sets are described in Table 1.

Table 1: ﬁve datasets details

data set

bag number (positive) mean bag size dimensionality

Musk1

Musk2

Elephant

Fox

Tiger

92(47)

102(36)

200(100)

200(100)

200(100)

5.17

64.69

6.96

6.90

6.10

166

166

230

230

230

The accuracies of PMI on ﬁve benchmark data sets are recorded in Table 3. To compare with
PMI, we also provide the performances [10, 2] of several other traditional MIL algorithms . These
traditional MIL algorithms are mi-SVM [2], MI-SVM[2], EM-DD[23] and MICA[10].

The same with previous MIL studies, 10-fold cross validation strategy is used to get the ac-
curacies of PMI. Both positive and negative bags are divided into 10 folds randomly and one
fold positive and negative bags are selected as testing set. The remaining 9 fold positive bags
(without negative ones) are the training set. The diﬀerence between that of PMI and 10-fold
cross validation used in traditional MIL studies is that the training set of PMI does not contain
9 fold negative bags.

11

Table 2: The numbers of positive bags used for training in PMI and other MIL algorithms are listed.

Musk1 Musk2 Elephant

PMI

mi-SVM

MI-SVM

MICA

EM-DD

47

92

92

92

92

36

102

102

102

102

100

200

200

200

200

Fox

100

200

200

200

200

Tiger

100

200

200

200

200

Table 3: The classiﬁcation accuracies of ﬁve algorithms on the ﬁve data sets are presented. The results
are percentage of correctly predicted bags. These accuracies are average results of 10 runs.

Musk1
79.1±1.1

Musk2
85.1±1.2

Elephant
78.3±0.7

Fox

57.9±1.5

Tiger
52.5±1.0

87.4

77.9

84.4

84.8

83.6

84.3

90.5

84.9

80.0

73.1

82.5

78.3

57.9

58.8

62.0

56.1

78.9

66.6

82.0

72.1

PMI

mi-SVM

MI-SVM

MICA

EM-DD

Table 2 shows the numbers of positive bags used for training in PMI and other traditional
MIL algorithms. Table 3 reports the accuracies of ﬁve approaches on ﬁve benchmark data sets.
These accuracies are the average of 10 runs. PMI terminates at the step 5 of Algorithm 1 and
returns f (x) due to no available instance label of these ﬁve data sets. PMI achieves close results
to those of the other traditional MIL methods mostly. The result on Musk2 is better than mi-
SVM, MI-SVM, EM-DD, but worse than MICA. Musk2 has only 39 positive bags, which are
obviously smaller than the negative bag number 63. So it is exciting that PMI achieves the
similar performance using much fewer training bags (only positive bags) to those of other MIL
approaches. PMI achieves a little lower accuracy on Musk1 than mi-SVM, MICA and EM-DD,
but a little larger than MI-SVM. The results of PMI are close to those of the other MIL methods
on Elephant and Fox.

According to the performance comparison in Table 3, if the traditional MIL methods achieve a
high accuracy on one data set, PMI can also obtain similar result. But when the performance of
traditional MIL method becomes very low(only about 60% accuracy on Fox data set), PMI will
get a worse result than those of traditional MIL methods. We explain the phenomenon as follows.
If there is a enough large margin and no overlap between positive and negative instances, the
traditional MIL methods with positive and negative training bags usually get a perfect accuracy.
Due to no overlap between positive and negative instances, PMI with only positive training bags
can describe the positive instance distribution accurately and also achieves satisfying performance.
However, if there exists a big overlap between positive and negative, we will get a poor result
though both positive and negative bags are used as the training set. So the result will degrade
furthermore with only positive training bags.

12

Fig. 4: The face images from three persons in this data set are shown. Each row corresponds one person’s
images.

3.2 Face Identiﬁcation

This experiment shows result of PMI on face identiﬁcation. The image set [1] is used to evaluate
PMI. The task of face identiﬁcation is predict whether an image contains face(s) or not. Each of
these face images contains one person’s face and the similar green background. This image set
collects 37 persons’ faces from various views under diﬀerent light conditions. Three face samples
for each person are recorded. So this face image set includes totally 111 face images. Every
person has various expressions. These 37 persons include various types: male and female, young
and old. Each image is 640 × 480 size with JPEG ﬁle format. No non-face image is provided in
this image set. Figure 4 illustrates some examples of this image set. Much more non-face images
than face are needed in previous MIL studies. But no non-face image is provided in this data set.

The feature extraction method in [5] is used in our experiments. Only the brief introduction
is provided. Please read [6] for more details. The ﬁrst is to split each image into 4 × 4 blocks
and compute L, U, V mean values of each block in LUV color space. Secondly, 4-order wavelet
transformation is applied to L value on each block to compute mean values of LH, HL, HH bands.
Each block is transformed into a 6-dimension (L,U,V,LH,HL,HH) feature vector and scaled into
[0, 1]. We use the toolkit “JSEG”[8] to segment each image into regions. Each region is represented
by the mean feature vector of the blocks in this region. So one region corresponds to an instance.
An image is associated with a bag. The average instance number of positive bags is 7.8.

Table 4: The numbers of positive bags used for training in PMI and other MIL algorithms are listed.

algorithm

training bag number

PMI

111

mi-SVM MI-SVM

211

211

DD

211

EM-DD

211

Table 5: The classiﬁcation accuracies of both traditional MIL methods and PMI.

algorithm

accuracy

PMI
90.4±1.9

mi-SVM MI-SVM
89.6±1.3
86.5±1.8

DD

EM-DD
85.2±2.4 97.2±1.6

We select 100 background images(for the balance between positive and negative bags) randomly
from [4, 14] as negative bags for testing and training in other traditional MIL methods. These

13

100 negative images contain rich contents and backgrounds. The average instance number of
negative bags is about 19. The feature extraction method is the same as that of the face images.
The numbers of training bags in PMI and other MIL algorithms are reported in Table 4. The
5-fold cross validation accuracies of both the traditional MIL algorithms and PMI are reported in
Table 5. Each accuracy is the average of 10 runs. PMI achieves close performances to the other
MIL algorithms. The accuracy of mi-SVM keeps very close to that of PMI. In contrast, MI-SVM
and DD fall a little behind the other three methods.

Table 6: The number of queried instances under diﬀerent ν and γ combinations

HHHHHH

γ

ν

60

70

80

90

100

0.01

0.05

0.1

0.2

0.3

0.5

1

1

1

1

1

1

1

1

1

1

1

1

1

1

1

1

1

1

1

1

1

1

1

1

1

1

1

1

1

1

To validate Theorem 3, we use all face images as the training set. Table 6 reports the number
of queried instances under diﬀerent parameter combinations of γ and ν. According to Theorem 3,
the maximum number of queried instances is larger than (cid:100)7.81(cid:101) − 1 = 7 when ν ≥ 1
111. 7.81
is the average instance number of positive bags. The largest number in Table 6 is much smaller
than 7, which demonstrates Theorem 3 is correct.

N = 1

4 Conclusion

This paper proposes a new algorithm PMI, which learns a prediction function with only positive
bags. Our proposed algorithm is to reduce the label cost signiﬁcantly compared with previous
MIL algorithms without much accuracy loss. A detailed theoretic analysis is also provided. In
most real applications, the queried instance number is much smaller than the theoretical result in
Theorem 3 and can be approximated to a constant. The time complexity of PMI is approximated
as O(n3). Experimental results illustrate that PMI usually achieves close performance to those of
traditional MIL methods mostly especially when traditional MIL algorithms can predict bag label
with a high accuracy. In the future work, we try to apply some dimensional reduction approaches
[12, 13, 11] on our proposed method to improve the eﬃciency.

Acknowledgement

Acknowledge here.

14

Appendix

Appendix here.

References

[1] AAM. Active Appearance Model. http://www.imm.dtu.dk/~aam/.
[2] S. Andrews, I. Tsochantaridis, and T. Hofmann. Support vector machines for multiple-instance
learning. In Advances in Neural Information Processing Systems, pages 577–584. The MIT press,
2003.

[3] B. Babenko, M.-H. Yang, and S. Belongie. Robust object tracking with online multiple instance
learning. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 33(8):1619–1632,
2011.

[4] Caltech. Caltech 101 Objects Image Dataset. http://www.vision.caltech.edu/Image_Datasets/

Caltech101/Caltech101.html.

[5] Y. Chen, J. Bi, and J. Z. Wang. Miles: Multiple-instance learning via embedded instance selection.

Pattern Analysis and Machine Intelligence, IEEE Transactions on, 28(12):1931–1947, 2006.

[6] Y. Chen and J. Z. Wang.

Image categorization by learning and reasoning with regions. The

Journal of Machine Learning Research, 5:913–939, 2004.

[7] Corel. Corel Image Feature Set. http://archive.ics.uci.edu/ml/datasets/Corel+Image+

Features.

[8] Y. Deng and B. S. Manjunath. Unsupervised segmentation of color-texture regions in images and
video. Pattern Analysis and Machine Intelligence, IEEE Transactions on (PAMI), 23:800–810,
2001.

[9] T. Dietterich, R. Lathrop, and T. Lozano-P´erez. Solving the multiple instance problem with

axis-parallel rectangles. Artiﬁcial Intelligence, 89(1-2):31–71, 1997.

[10] G. Fung, M. Dundar, B. Krishnapuram, and R. B. Rao. Multiple instance learning for computer
aided diagnosis. In Advances in neural information processing systems, pages 425–433. MIT, 2007.

[11] C. Hou, F. Nie, C. Zhang, D. Yi, and Y. Wu. Multiple rank multi-linear svm for matrix data

classiﬁcation. Pattern Recognition, 47(1):454–469, 2014.

[12] C. Hou, C. Zhang, Y. Wu, and Y. Jiao. Stable local dimensionality reduction approaches. Pattern

Recognition, 42(9):2054–2066, 2009.

[13] C. Hou, C. Zhang, Y. Wu, and F. Nie. Multiple view semi-supervised dimensionality reduction.

Pattern Recognition, 43(3):720–730, 2010.

[14] R. F. L. Fei-Fei and P. Perona. Learning generative visual models from few training examples:
An incremental bayesian approach tested on 101 object categories.
In Computer Vision and
Pattern Recognition, 2004 IEEE International Conference on, Workshop on Generative-Model
Based Vision (CVPR). IEEE, 2004.

[15] Y.-X. Li, S. Ji, S. Kumar, J. Ye, and Z.-H. Zhou. Drosophila gene expression pattern anno-
tation through multi-instance multi-label learning. Computational Biology and Bioinformatics,
IEEE/ACM Transactions on, 9(1):98–112, 2012.

[16] Mosek. Mosek Optimization Toolbox. http://www.mosek.com.

[17] B. Sch¨olkopf, J. Platt, J. Shawe-Taylor, A. Smola, and R. Williamson. Estimating the support of

a high-dimensional distribution. Neural Computation, 13(7):1443–1471, 2001.

15

[18] B. Sch¨olkopf, R. Williamson, A. Smola, J. Shawe-Taylor, and J. Platt. Support vector method
for novelty detection. In Advances in Neural Information Processing Systems, volume 12, pages
582–588. MIT press, 2000.

[19] J. Tang, H. Li, G.-J. Qi, and T.-S. Chua. Image annotation by graph-based inference with in-
tegrated multiple/single instance representations. Multimedia, IEEE Transactions on, 12(2):131–
141, 2010.

[20] UCI. Machine Learning Repository. http://archive.ics.uci.edu/ml/.
[21] A. Zafra, E. L. Gibaja, and S. Ventura. Multiple instance learning with multiple objective genetic

programming for web mining. Applied Soft Computing, 11(1):93–102, 2011.

[22] C. Zhang, X. Chen, M. Chen, S. Chen, and M. Shyu. A multiple instance learning approach for
content based image retrieval using one-class support vector machine. In Multimedia and Expo,
2005. ICME 2005. IEEE International Conference on, pages 1142–1145. IEEE, 2005.

[23] Q. Zhang and S. Goldman. EM-DD: An improved multiple-instance learning technique. In Ad-

vances in neural information processing systems, volume 2, pages 1073–1080. MIT Press, 2002.

[24] Q. Zhang, S. A. Goldman, W. Yu, and J. E. Fritts. Content-Based Image Retrieval Using Multiple-
In International Conference on Machine Learning (ICML), pages 682–689,

Instance Learning.
2002.

16

