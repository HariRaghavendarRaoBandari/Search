6
1
0
2

 
r
a

 

M
5
1

 
 
]

.

C
O
h
t
a
m

[
 
 

1
v
3
8
6
4
0

.

3
0
6
1
:
v
i
X
r
a

Kullback-Leibler Divergence Approach to

Partitioned Update Kalman Filter

Matti Raitoharju, Ángel F. García-Fernández, and Robert Piché

1

Abstract—Kalman ﬁltering is a widely used framework for
Bayesian estimation. The partitioned update Kalman ﬁlter ap-
plies a Kalman ﬁlter update in parts so that the most linear parts
of measurements are applied ﬁrst. In this paper, we generalize
partitioned update Kalman ﬁlter, which requires the use oft the
second order extended Kalman ﬁlter, so that it can be used with
any Kalman ﬁlter extension. To do so, we use a Kullback-Leibler
divergence approach to measure the nonlinearity of the measure-
ment, which is theoretically more sound than the nonlinearity
measure used in the original partitioned update Kalman ﬁlter.
Results show that the use of the proposed partitioned update
ﬁlter improves the estimation accuracy.

Index Terms—Bayesian estimation; nonlinear; estimation;

Kalman ﬁlters; Kullback-Leibler divergence

I. INTRODUCTION

In Bayesian ﬁltering we are interested in calculating the
probability density function (PDF) of a dynamic state based
on a sequence of measurements. It is a recursive process in
which a prior distribution is updated using a measurement to
obtain a posterior distribution. This distribution then evolves in
time to become a new prior distribution. Bayesian estimation
has a wide range of applications from positioning [1], tracking
[2], and quality control [3], to brain imaging [4] and modeling
spread of infectious diseases [5]. In general, a Bayesian
estimate cannot be computed in closed form. Under certain
conditions, which include that measurements and state transi-
tion function have to be linear and associated noises Gaussian,
Bayesian estimates can be computed in closed form using
the algorithm known as the [6]. For nonlinear measurements
various Kalman ﬁlter extensions have been developed. When
the nonlinearity is small, the Kalman ﬁlter (KF) extensions
produce accurate estimates, but large nonlinearity can cause
serious inaccuracies.

Several methods have been proposed to quantify the amount
of nonlinearity in order to monitor the performance of a KF
extension [7]–[13]. An algorithm based on second order Ex-
tended Kalman ﬁlter (EKF2) was presented in [7, p. 349] and
it was extended for multidimensional correlated measurements
in [8]. Although it was found to be a good indicator of the
accuracy of a KF extension compared to methods that are
computationally feasible presented in [9]–[11], its evaluation
requires computation of second order derivatives of the mea-
surement function or computation of the measurement function
values in a number of points that increases quadratically.

The effect of the nonlinearity can also be gauged by
comparing the higher order moments of the posterior to those
corresponding to the Gaussian assumption. This is considered
for example in [12].

If the prior covariance is sufﬁciently small, any nonlinear
function is well approximated as a linear function in the prior’s
high-probability region. Therefore, if a nonlinearity measure
indicates that a measurement is highly nonlinear, the prior can
be split into a sum of small-covariance parts that can be used
as the components in a Gaussian mixture ﬁlter (GMF) [14].
As the computational burden increases with every split, the
number of new components needed to bring the nonlinearity
measure below a certain threshold should be minimized [15].
In [16] another use of the EKF2-based nonlinearity measure
in [8] was presented. The algorithm, called the partitioned up-
date Kalman ﬁlter (PUKF), transforms a measurement vector
with a linear transformation so that the nonlinearity of the
least nonlinear measurement element is minimized. Only the
elements with nonlinearity below a threshold are applied ﬁrst
and then the linearization and evaluation of the nonlinearity is
redone. This way the estimates become more accurate.

In [13], it was shown that the moments computed in a
general Gaussian ﬁlter (GGF) [17] can be used to compute
the Kullback-Leibler divergence (KLD) [18] of the GGF ap-
proximation to the joint PDF of the state and the measurement
from the true joint PDF. Various KF extensions can be seen
as approximations of the GGF and, thus, they can be used
to approximate this KLD, which can be also interpreted as
a nonlinearity measure. This KLD has been used in Kalman
optimization in [19].

In this paper, we present an algorithm that is similar to
PUKF, but uses the KLD based nonlinearity measure. The
beneﬁts of using the KLD measure is that it is mathematically
sound and it can be used in combination with any GGF
approximation that can be used to approximate the KLD. If
the proposed algorithm is used with EKF2 or a numerical
approximation of it,
the results are the same as with the
original PUKF.

The rest of this paper is organized as follows: Section II
gives the background work. The new algorithm is developed
in Section III. Section IV presents examples of the use and
accuracy of the proposed algorithm. Section V concludes the
paper.

II. BACKGROUND WORK

it

is convenient

In this paper, we derive an improved version of the PUKF.
To do so,
to introduce some background
material. In Section II-A, we ﬁrst present the GGF update
and a measure of its performance based on the KLD. In
Section II-B, we revisit the PUKF.

A. GGF

p(x) = N(x|µ−, P −).

In this paper, we consider the Bayesian update step of a
state. We assume that state x ∈ Rn has a Gaussian prior PDF
(1)
where N is the PDF of a normal distribution, µ− is the prior
mean, and P − is the prior covariance. This state is observed
through a measurement that is modeled with a measurement
model of form

y = h(x) + ε,

(2)

where y is the d-dimensional measurement value and ε is a
zero-mean Gaussian measurement noise with covariance R.

The objective is to compute the posterior

p(x|y) ∝ p(x)p(y|x),

(3)
where ∝ stands for proportionality and p(y|x) = N(y|h(x), R)
is the density of the measurement given the state. If function
h(·) is linear, the posterior can be computed exactly using the
KF. But when h(·) is not linear, an approximation has to be
used.

A general way to formulate a Kalman ﬁlter type approxi-
mation is to use the GGF formulation for additive noise [17].
In the update step, we approximate the posterior as a Gaussian
with mean µ+ and covariance P + by

µ+ = µ− + K(y − ˆy)
P + = P − − KSK T ,

h(x)p(x)dx

(cid:90)
(cid:90) (cid:0)x − µ−(cid:1) (h(x) − ˆy)T p(x)dx
(cid:90)

(h(x) − ˆy) (h(x) − ˆy)T p(x)dx

where

ˆy =

Ψ =

Φ =

S = Φ + R
K = ΨS−1.

(4)
(5)

(6)

(7)

(8)

(9)
(10)

The GGF is implicitly deﬁning a Gaussian approximation
of the joint density of state x and measurement y. The
approximation, denoted q(x, y) is not the exact joint density
p(x, y). The approximation error can be measured using KLD
[18]

p(x, y)
q(x, y)

p(x, y) log

(cid:90) (cid:90)
log(cid:12)(cid:12)I + R−1Υ(cid:12)(cid:12) ,
Υ = Φ − ΨT(cid:0)P −(cid:1)−1

Ψ.

η = KLD (p, q) =

In [13], it was shown that
1
2

η =

where

dxdy

(11)

(12)

(13)

Different KF extensions, such as the unscented Kalman ﬁlter
(UKF) [20] and the cubature Kalman ﬁlter (CKF) [21], can be
interpreted as approximations of the GGF and they can also be
used to compute approximations of the integrals (6)-(8) and,
thus, approximate the joint KLD. Some KF extensions, such

2

as the extended Kalman ﬁlter (EKF), make linearizations such
that the moments (7) and (8) can be written in the form

Ψ = P −AT
Φ = AP −AT ,

(14)
(15)
where A is a d×n matrix. For these ﬁlters Υ in (13) is always
0 and they cannot be used to approximate the KLD.

B. PUKF

For conditionally independent measurements y1, . . . , yd

given the state we can write

p(y1:d|x) =

p(yi|x).

(16)

The prior can be updated using the measurements sequentially:

d(cid:89)

i=1

d(cid:89)

d(cid:89)

p(x|y1:n) ∝ p(x)p(y1|x)

p(yi|x)

∝ p(x|y1)

i=2

p(yi|x).

(17)

i=2

When the measurement model is of the form (2) and the noise
covariance R is diagonal, the measurements are conditionally
independent. Thus, in a linear Gaussian measurement model,
the KF update can be applied one measurement element at
a time [22, p. 119] and the posterior distribution does not
change. However, when the updates are approximate, the ﬁnal
posterior PDF approximation changes.

In general,

the d measurement elements are not condi-
tionally independent given the state so (16) does not hold.
The main idea behind the PUKF [16] is to apply a linear
transformation to the measurement model so that measurement
elements are conditionally independent and the measurements
are applied in an order that aims to minimize the approx-
imation error. A general description of PUKF is given in
Algorithm 1.

The PUKF in [16] can only be used with EKF2 [7, pp.
345-347], which is based on second order Taylor expansion
of the measurement function, or with a central difference ﬁlter
[17] that is a numerical approximation of EKF2. We call this
numerical approximation the numerical second order Extended

Algorithm 1: A general description of the PUKF algo-
rithm
1 Evaluate the nonlinearity of the measurement elements.
2 Minimize the nonlinearity of part of the measurement by

applying a linear transformation to the measurement.

3 Update the state using the part of the measurement
whose nonlinearity is smaller than a set threshold.

4 If the whole measurement is not applied, use the partially

updated prior as a new prior and the unused
measurements as a new measurement and return to 1.

5 Return posterior.

3

III. KULLBACK-LEIBLER PARTITIONED UPDATE KALMAN

FILTER (KLPUKF)

In this paper we develop KLPUKF that uses (12) as the
nonlinearity measure. Thus, algorithm is not limited to EKF2
based ﬁlters and any ﬁlter, such as UKF [20] or CKF [21], that
is a good enough approximation of the GGF can be used. As
noted earlier, some algorithms, such as EKF, produce always
0 nonlinearity. Thus, they are not good enough approximations
of the GGF. We assume that
the KF extension does the
moment approximations at the prior. Iterative Kalman ﬁlter
extensions, such as iterated extended Kalman ﬁlter (IEKF) [7,
pp. 349-351] and iterated posterior linearization ﬁlter (IPLF)
[23], are not considered in this paper to be used with the
KLPUKF.

We recall that the steps of the PUKF algorithm are given
in Algorithm 1. Therefore, in this section we specify how to
compute the nonlinearity and the linear transformation. The
linear transformation of the measurement model can be written
as

˜y = Dy = Dh(x) + Dε = ˜h(x) + ˜ε,

(22)

where D is a nonsingular square matrix. In the appendix we
show that the linear transformation (22) does not change the
posterior if all measurements are applied at once (Proof A)
and does not have an effect on the nonlinearity measure (12)
(Proof A).

However, if an update is split into a sequence of independent
measurement updates, the posterior may change. We use a
transformation such that the transformed noise covariance ˜R
is

˜R = cov ˜ε = DRDT = I.

(23)

We show in Proof A in the Appendix that the nonlinearity (12)
can be written now as

(cid:12)(cid:12)(cid:12)I + ˜Υ
(cid:12)(cid:12)(cid:12) ,

η =

1
2

log

where

˜Υ = DΥDT .

(24)

(25)

Fig. 1. Transforming second order polynomial measurements to minimize
nonlinearity of ˆy1 and posterior comparison of PUKF and EKF2

Kalman ﬁlter (nEKF2). The EKF2-speciﬁc nonlinearity mea-
sure is

d(cid:88)

d(cid:88)

(cid:0)R−1(cid:1)

i=1

j=1

ˆη =

[i,j] tr P HiP Hj,

(18)

where Hi is the Hessian of ith element of the measurement
function h(·) or its numerical approximation and subscript
[i, j] means the matrix element in the ith row and jth column.
Example 1: We proceed to illustrate how the PUKF works
as it is the foundation of the methods we propose in this
paper. We consider an example from [16]. Here the prior
is one dimensional with PDF N(x|1, 1) and the nonlinear
measurement equation is

y =

(19)
where ε has PDF N(ε|0, I). The measurement can be trans-
formed with the linear transformation

+ ε,

2

−x2 + 3

(cid:20)x2 − 2x − 4
(cid:21)
(cid:20)1
(cid:20) −x − 5

1
1 −1

1√
2

(cid:21)
(cid:21)

y

ˆy =

√

into a linear term and a polynomial term:

4

4

2

ˆy =

+ ˆε,

x2 − x − 11

(21)
where ˆε ∼ N(0, I). In PUKF the linear measurement function
is applied ﬁrst and the partially updated state has mean − 1
2 and
3. The polynomial measurement function is applied
covariance 1
using this partially updated state and EKF2-based update. In
Figure 1 EKF2 is used as a reference. EKF2 applies both
measurements at once and the posterior estimate is the same
for the original and transformed measurement models. When
compared to the true posterior, which is computed using a
dense grid, the posterior estimate of PUKF is signiﬁcantly
more accurate than the EKF2 posterior estimate.

We further want ˜Υ to be diagonal. Now the nonlinearity
measure of the ith measurement element can be deﬁned as

(20)

(cid:16)

˜ηi =

1
2

log

(cid:17)

1 + ˜Υ[i,i]

.

(26)

When ˜R = I and ˜Υ is diagonal the nonlinearity of the least
nonlinear measurement element is minimized. This is shown
in Proof A in the Appendix. Thus, this gives the best choice
for a single element update.

Rather than choosing ˜R = I, we could have chosen any
other diagonal matrix. However, this would increase the value
of the corresponding diagonal values of ˜Υ and the KLD values
of individual measurements of (53) would not change. Thus,
choosing ˜R = I is an arbitrary, but natural choice.

To compute the matrix D we ﬁrst introduce the notation for

matrix square root

√

√

R

T

R

= R,

(27)

x-4-3-2-101234h(x)  -6-4-20246Original measurement functionsy[1]y[2]x-4-3-2-101234h(x)  -6-4-20246Transformed measurement functionsˆy[1]ˆy[2]x-4-3-2-101234p(x)00.511.52pdfspriortrueposteriorEKF2PUKFafterupdatewithˆy1FinalPUKFposteriorwhich can be computed with, e.g., Cholesky decomposition.
The transformation D that makes ˜Υ diagonal and ˜R = I is
[16]

√

−1

R

,

D = U T

where U is computed using an eigendecomposition

√

√
Υ

−1

R

−T

,

R

U ΛU T =

(28)

(29)

where U is orthogonal and the eigenvalues in the diagonal
matrix Λ are sorted in ascending order. We can see that
this transformation fulﬁlls our requirements of having identity
transformed measurement noise covariance (23) from

√

−1

√

−T

˜R = U T

(30)
and the diagonality of matrix ˜R (25) can be seen from
orthogonality and (29)

U = I

R

R

R

√

√

˜Υ = U T

−1

R

Υ

−T

R

U = Λ.

(31)

To save computational resources, rather than using only one
measurement in each update, we perform the update using all
measurement elements that have ˜ηi below a limit ηlimit or if
this set is empty then with the measurement element with
smallest ˜ηi. We also note that after a measurement update the
moments (6)-(8) are recomputed. Because the nonlinearities
of the measurements change, the transformation matrix D is
recomputed between every update.

The KLPUKF algorithm is given in Algorithm 2. When
using EKF2 or a numerical approximation of it as basis of the
ﬁlter and setting ηlimit properly, the algorithm produces the
same results as with the original PUKF.

IV. SIMULATION EXAMPLES

In this section, we present examples of how the KLPUKF
improves estimation accuracy. First, we evaluate the accu-
racy enhancements for a one-dimensional state that is ob-
served through measurements containing trigonometric func-
tions. Then we present examples with range measurements to
beacons. In these examples we also test ﬁltering and evaluate
the effect of the nonlinearity limit ηlimit. In our examples, we
use an approximation of GGF computed in a dense grid, UKF
[20], nEKF2 [17], and smart sampling Kalman ﬁlter (S2KF)
[24].

Example 2: 1D example
First we consider an example with a 1-dimensional state and
standard normal prior. The prior is updated with a 3-element
measurement of form

 x + 4 sin(x) + 7

−x + 4 sin(x) − 4
−2 cos(x) − 8

 + ε,

y =

(32)

where PDF of ε is N(ε|0, I). The top-left plot of Figure 2
shows these measurement functions. The dashed lines show
the statistical linearizations of the functions obtained from
moments (6)-(8) [23]. The dashed black line is the prior mean
and dotted lines show the 2σ limits; i.e. 95% of the probability
is within these limits. The numbers in the legend are the KLDs
(53) of each element of the measurement function computed

4

Algorithm 2: Measurement update step in KLPUKF
input
: Prior state: µ0 – mean P0 – covariance
Measurement model: y0 – value, h0(·) –
function, R0 – covariance

output

√

: Updated state: µ+ – mean, P + –
covariance
R (27)

1 Compute
2 d ← initial measurement dimension
3 i ← 0 // Iteration counter
4 while d > 0 do
5

i

Ri

Compute approximations (ˆyi, Ψi, and Φi) of
moments (6)-(8) for the d-dimensional measurement
hi(·) using a KF extension and prior N(x|µi, Pi)
−1 Ψ
Υ ← Φ − ΨT (Pi)
Compute Ui and Λi from
−1
Di ← U T
Choose largest k so that log(1 + Λi,[k,k]) ≤ ηlimit. If
no such k exists, set k ← 1
// Compute partial update

R and Υ using (29)

√

√

i Di

T
[1:k,:]

[1:k,:] + I
˜S−1

˜ˆyi ← Di[1:k,:] ˆyi
˜Si ← Di[1:k,:]ΦDT
˜Ki ← ΨT
µi+1 ← µi + ˜Ki(Di[1:k,:]yi − ˜ˆy)
Pi+1 ← Pi − ˜Ki ˜Si ˜K T
yi+1 ← Di[k+1:d,:]y
hi+1(x) ← Di[k+1:d,:]hi(x)
√
Ri+1 ← I // Updated measurement noise
covariance is an identity matrix due
to decorrelation

// Update remaining measurement

i

i

6
7
8
9

10
11
12
13
14

15
16
17

18

d ← d − k // Updated measurement
i ← i + 1 // Increase counter

dimension

19
20 end
21 µ+ ← µi // Posterior mean
22 P + ← Pi // Posterior covariance

using (53). Because Υ is not diagonal for initial functions,
their KLDs does not sum to the total KLD that is 0.8533 for
the initial measurements. The sum of KLDs of transformed
functions is the same as the total KLD.

Before transformation, the function with the cosine term
(blue) has the smallest KLD. For these measurements and prior
the transformation matrix is

D =

2

2

0
− 1√

− 1√
 −√

1√
2
0
− 1√

2

0
1
0

 ,
 + ε.

which causes the sin terms in the new ﬁrst component to cancel
out and the new measurements are
2x − 11√
−2 cos x − 4
√
−4
2 sin(x) 3√
2

(34)

y =

2

(33)

5

Fig. 3. Example updates with using all measurements at once and with using
KLPUKF with GGF. Ellipses contain 50% of the prior probability. Intensity
of the color of the likelihoods represents its value.

We can see that

the third measurement of both non-
transformed and transformed measurements has a large un-
certainty when linearized using the original prior. When using
the partially updated prior the third measurement has much
smaller uncertainty.

Figure 4 shows the same example computed using the
UKF linearizations. Contours containing 50% of the posterior
probability computed using GGF, UKF, nEKF2 and their
KLPUKF counterparts are shown in Figure 5. The contour
for true posterior is computed in a grid. Figure shows how the
GGF produces a larger covariance than the true covariance
and how the use of KLPUKF makes the estimate closer to the
true estimate. The UKF estimate has a bad shape: it is too
narrow horizontally and too long vertically. Using KLPUKF
with UKF reduces the covariance in the too long direction and
the resulting posterior estimate is closer to the true posterior.

Example 4: Filtering example
In the ﬁltering example we consider the same measurement
model as in the previous example. We use a 4-dimensional
state

x =(cid:2)r1

(cid:3)T

r2

v1

v2

,

(36)

where r1 and r2 are position variables and v1 and v2 velocity
variables. The state transition model is

xt+1 = F xt + εQ,

(37)

Fig. 2. Example updates using all measurements at once and using KLPUKF.
Red corresponds to y1, green to y2, and blue to y3.

The top-right plot shows these transformed measurements and
corresponding KLDs. The linear measurement naturally has
a zero KLD and is applied ﬁrst. The second row shows the
remaining components of the transformed functions and the
new linearizations that are made in the new prior. The last
row shows the last measurement and the posterior PDFs. The
curve labeled “partitioned” is the posterior computed with
the proposed algorithm. “Full update” is the update using all
measurements simultaneously and “nontransformed update” is
the update made in parts without applying transformation D
i.e. the measurement with the cosine element is applied ﬁrst
because it has the smallest KLD. This plot shows that the
posterior computed with the proposed method is the most
accurate one.

Example 3: Range measurements
In this example, we consider a more realistic situation with
2-dimensional state with prior PDF N(x|0, 12I) updated with
3 range measurements. The measurement model is

(cid:112)(x1 − 2)2 + (x2 − 2)2

(cid:112)(x1 + 6)2 + (x2 − 6)2
(cid:112)(x1 + 2)2 + (x2 − 1)2

 + ε,

where ε has PDF N(ε|0, I) and the measurement values are

h(x) =

(cid:2)5 11.5 3.5(cid:3)T .

(35)

Figure 3 shows an example of the update with KLPUKF.
The top row in Figure 3 shows the likelihoods of
the
range measurements (blue), prior (red) and the measurement
likelihoods when linearized using the GGF (orange). The
second row shows the transformed measurements and their
linearizations within the original prior. The last row shows
the linearizations within the prior and how the prior and
linearizations change after partial updates. The integrals for
GGF were computed using a dense grid.

-202-15-10-5051015Initial functions0.350.350.29-202-15-10-5051015Transformed functions0.000.290.56-202-15-10-5051015Functions after 1st update0.050.23-202-15-10-5051015Transformed functions0.020.25-202-15-10-5051015Function after 2nd measurement0.07-20200.511.52PosteriorPartitionedFull updateNontransformed updateTrue posteriorh1(x)=p(x1−2)2+(x2−2)2-20020-20-1001020Original measurementsh2(x)=p(x1+6)2+(x2−6)2-20020-20-1001020h3(x)=p(x1+2)2+(x2+1)2-20020-20-10010200.25h1(x)+0.80h2(x)−0.55h3(x)-20020-20-1001020Transformed measurements0.71h1(x)−0.54h2(x)−0.45h3(x)-20020-20-10010200.66h1(x)+0.28h2(x)+0.70h3(x)-20020-20-10010200.25h1(x)+0.80h2(x)−0.55h3(x)-20020-20-1001020Linearizations after parial updates0.71h1(x)+0.55h2(x)−0.43h3(x)-20020-20-10010200.66h1(x)−0.30h2(x)+0.69h3(x)-20020-20-1001020Measurement likelihoodLikelihood of linearizationPriorMEAN OF FILTERING ERRORS AT THE FIRST TIME STEP

TABLE I

EKF
nEKF2
UKF
S2KF

All
2.35
1.87
1.87
1.82

Sequential

KLPUKF

1.95
1.72
1.69
1.69

-16.7%
-8.0%
1.48
-10.0% 1.54
-7.5%
1.46

-20.9%
-17.9%
-20.0%

MEAN OF FILTERING ERRORS AT THE LAST TIME STEP

TABLE II

EKF
nEKF2
UKF
S2KF

All
2.17
1.59
1.67
1.59

Sequential

1.99
1.64
1.70
1.62

-8.1%
3.1%
1.5%
1.5%

The prior is normal with covariance

12

0
0
0

0
12
0
0

0
0
1
0

P0 =

KLPUKF

-5.6%
-6.2%
-6.5%

1.50
1.57
1.49

 .

0
0
0
1

6

(40)

Fig. 4. Example updates with using all measurements at once and with using
KLPUKF and UKF. Ellipses contain 50% of the prior probability. Intensity
of the color of the likelihoods represent its value.

Fig. 5. Contours containing 50% of the posterior probability obtained with
different algorithms

where

F =

p(εQ) = N

0 1 0
0 0 1
0 0 0

1 0 1
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) 0,
εQ

0
1
0
1


0

0
0
0

(38)

(39)


 .

0
0
0

0.04

0
0

0
0
0 0.04
0

0

To have variation in the initial linearizations we sample the
initial mean µ0 from the normal distribution that has a zero
mean and covariance P0. In this test we used Algorithm 2
with KLD threshold ηlimit = 0, so that each measurement
is processed separately at every time step. We tested the
KLPUKF with moments computed using nEKF2, UKF and
S2KF [24]. S2KF allows to select
the number of sigma-
points. We used in our tests 64 sigma-points for S2KF while
the UKF used 9 sigma-points and nEKF2 used 16 sigma-
points. The nonlinearity threshold is set to ηlimit = 0 so each
measurement element is applied separately. We also tested how
the estimation accuracy changes if the measurement elements
are applied sequentially in a random order. Estimates are also
computed with EKF. Because EKF produces always a zero
nonlinearity the KLPUKF cannot improve the result from the
applying the measurements in a random order. The routes were
simulated 10000 times.

Table I shows the mean of errors after the ﬁrst update. The
“all” column uses all measurements at once, the “sequential”
column uses the measurements in random order and the third
column uses KLPUKF. The table shows how the use of
KLPUKF instead of the standard ﬁlter improves the estimate
accuracy more than 15% at the ﬁrst time step. Also we can see
that the application of the measurements sequentially improves
the estimation accuracy.

Table II shows the results of the last time step of the routes.
In these results we can see that the sequential application
of measurements improves only the estimation accuracy of
EKF; other ﬁlters have worse accuracy, but the KLPUKF
improves the accuracy. The smaller improvement gained by the
KLPUKF can be explained with the observation that the route
moved usually outside the sources of the range measurements
and the measurement geometry is worse i.e. there is less
information in the actual measurements, while they are more
linear than in the ﬁrst step.

This example showed that the use of KLPUKF gives accu-
racy improvements and that the use of independent measure-
ments sequentially does not necessarily improve the accuracy.

h1(x)=p(x1−2)2+(x2−2)2-20020-20-1001020Original measurementsh2(x)=p(x1+6)2+(x2−6)2-20020-20-1001020h3(x)=p(x1+2)2+(x2+1)2-20020-20-10010200.79h1(x)−0.23h2(x)−0.57h3(x)-20020-20-1001020Transformed measurements−0.03h1(x)−0.94h2(x)+0.33h3(x)-20020-20-10010200.61h1(x)+0.24h2(x)+0.75h3(x)-20020-20-10010200.79h1(x)−0.23h2(x)−0.57h3(x)-20020-20-1001020Linearizations after parial updates−0.03h1(x)+0.99h2(x)+0.12h3(x)-20020-20-10010200.61h1(x)−0.07h2(x)+0.79h3(x)-20020-20-1001020Measurement likelihoodLikelihood of linearizationPrior-3-2-10123-6-5-4-3-2-10TrueKLPUKF (GGF)GGFKLPUKF (UKF)UKFKLPUKF (nEKF2)nEKF27

[3] M. Zhai and S. Fu, “Applying target maneuver onset detection algo-
rithms to defects detection in aluminum foil,” Signal Processing, vol. 90,
no. 7, pp. 2319 – 2326, 2010, doi:10.1016/j.sigpro.2010.02.012.

[4] P. Hiltunen, S. Särkkä, I. Nissilä, A. Lajunen, and J. Lampinen, “State
space regularization in the nonstationary inverse problem for diffuse
optical tomography,” Inverse Problems, vol. 27, no. 2, p. 025009, 2011.
[Online]. Available: http://stacks.iop.org/0266-5611/27/i=2/a=025009

[5] M. J. Keeling and P. Rohani, Modeling infectious diseases in humans
and animals. Princeton: Princeton University Press, 2008. [Online].
Available: http://opac.inria.fr/record=b1124487

[6] Y. Ho and R. Lee, “A Bayesian approach to problems in stochastic
estimation and control,” IEEE Transactions on Automatic Control, vol. 9,
no. 4, pp. 333 – 339, October 1964, doi:10.1109/TAC.1964.1105763.

[8] M. Raitoharju, “Linear models and approximations

[7] A. H. Jazwinski, Stochastic Processes and Filtering Theory, ser. Math-
ematics in Science and Engineering. Academic Press, 1970, vol. 64.
in personal
positioning,” Ph.D. dissertation, Tampere University of Technology,
November
2014.
http://URN.ﬁ/URN:ISBN:
978-952-15-3421-8

[Online]. Available:

[9] F. Faubel,

J. McDonough,

and
merge unscented Gaussian mixture ﬁlter,” IEEE Signal Process-
ing Letters, vol. 16, no. 9, pp. 786 –789, September 2009,
doi:10.1109/LSP.2009.2024859.

and D. Klakow,

“The

split

[10] F. Havlak and M. Campbell, “Discrete and continuous, proba-
bilistic anticipation for autonomous robots in urban environments,”
IEEE Transactions on Robotics, vol. PP, no. 99, pp. 1–14, 2013,
doi:10.1109/TRO.2013.2291620.

[11] M. Huber, “Adaptive Gaussian mixture ﬁlter based on statistical
linearization,” in Proceedings of the 14th International Conference on
Information Fusion (FUSION), July 2011, pp. 1–8. [Online]. Available:
http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=5977694

[12] O. Straka, J. Dunik, and M. Simandl, “Measures of non-Gaussianity
in unscented Kalman ﬁlter framework,” in Proceedings of the 17th
International Conference on Information Fusion (FUSION), July 2014,
pp. 1–8. [Online]. Available: http://ieeexplore.ieee.org/xpl/articleDetails.
jsp?arnumber=6916118

[13] M. Morelande and A. García-Fernández, “Analysis of Kalman ﬁl-
ter approximations for nonlinear measurements,” IEEE Transactions
on Signal Processing, vol. 61, no. 22, pp. 5477–5484, Nov 2013,
doi:10.1109/TSP.2013.2279367.

[14] H. W. Sorenson and D. L. Alspach, “Recursive Bayesian estimation
using Gaussian sums,” Automatica, vol. 7, no. 4, pp. 465–479, 1971,
doi:10.1016/0005-1098(71)90097-5.

[15] M. Raitoharju, S. Ali-Loytty, and R. Piche, “Binomial Gaussian mixture
ﬁlter,” EURASIP Journal on Advances in Signal Processing, vol. 2015,
no. 1, p. 36, 2015, doi:10.1186/s13634-015-0221-2.

[16] M. Raitoharju, R. Piché, J. Ala-Luhtala, and S. Ali-Löytty, “Partitioned
update Kalman ﬁlter,” ISIF Journal of Advances in Information Fusion,
in press. [Online]. Available: http://arxiv.org/abs/1503.02857

[17] K. Ito and K. Xiong, “Gaussian ﬁlters for nonlinear ﬁltering problems,”
IEEE Transactions on Automatic Control, vol. 45, no. 5, pp. 910–927,
May 2000, doi:10.1109/9.855552.

[18] S. Kullback and R. A. Leibler, “On information and sufﬁciency,” The
Annals of Mathematical Statistics, vol. 22, no. 1, pp. 79–86, 1951.
[Online]. Available: http://www.jstor.org/stable/2236703

[19] Á. F. García-Fernández and L. Svensson, “Gaussian MAP ﬁltering using
Kalman optimization,” IEEE Transactions on Automatic Control, vol. 60,
no. 5, pp. 1336–1349, May 2015, doi:10.1109/TAC.2014.2372909.

[20] E. Wan and R. Van der Merwe, “The unscented Kalman ﬁlter for
nonlinear estimation,” in Proceedings of the Adaptive Systems for Signal
Processing, Communications, and Control Symposium. AS-SPCC., 2000,
pp. 153–158, doi:10.1109/ASSPCC.2000.882463.

[21] I. Arasaratnam and S. Haykin, “Cubature Kalman ﬁlters,” IEEE Trans-
actions on Automatic Control, vol. 54, no. 6, pp. 1254–1269, June 2009,
doi:10.1109/TAC.2009.2019800.

[22] P. S. Maybeck, Stochastic models, estimation, and control, ser. Mathe-

matics in Science and Engineering. Academic Press, 1979, vol. 1.

[23] A. García-Fernández, L. Svensson, M. Morelande, and S. Särkka,
“Posterior linearization ﬁlter: Principles and implementation using sigma
points,” IEEE Transactions on Signal Processing, vol. 63, no. 20, pp.
5561–5573, Oct 2015, doi:10.1109/TSP.2015.2454485.

[24] J. Steinbring and U. D. Hanebeck, “S2KF: The smart sampling
Kalman ﬁlter,” in Proceedings of the 16th International Conference
of Information Fusion (FUSION).
Istanbul: IEEE, 2013, pp. 2089–
2096. [Online]. Available: http://ieeexplore.ieee.org/xpl/articleDetails.
jsp?arnumber=6641263

Fig. 6. Mean errors as a function of ηlimit

To get an insight of how to choose ηlimit we evaluated the
previous example with the S2KF, which we assume to be the
best approximation of the GGF of used ﬁlters, with different
values for ηlimit. Values were set to range from 0 to 2 with
interval 0.1 and with ηlimit = ∞, which corresponds to the
standard S2KF.

Figure 6 shows how the mean error varies with ηlimit. First
of all we can see that in this test setup all measurements had
η < 1.5 so having ηlimit ≥ 1.5 does not change the outcome
at all. At the last time step the saturation point of error is
already achieved at ηlimit ≥ 1.5. At the ﬁrst and last time step
having ηlimit = 0.2 would not increase the error much. At the
beginning of the track ηlimit = 0.5 has only a small effect
compared to having ηlimit = 0.5 at the end of the track. This
behavior may be caused by the accumulation of the errors
during the track.

V. CONCLUSIONS

We have presented KLPUKF, an algorithm to perform
Bayesian updates with nonlinear multi-dimensional measure-
ments. The key idea is to perform the update in several steps
such that the measurement elements with lower nonlinearity
are processed ﬁrst. The measurement elements’ processing
order is selected so that the joint KLD of the state and the
measurement is minimized.

The proposed algorithm can be used with GGF approxima-
tions, such as sigma-point ﬁlters, to improve the estimation
accuracy as the nonlinearity measure (KLD) can be com-
puted using variables that are already computed within the
ﬁlter. Simulation results have demonstrated better accuracy of
KLPUKF compared to several existing ﬁlters.

ACKNOWLEDGEMENT

M. Raitoharju works in OpenKin project that is funded by

the Academy of Finland.

REFERENCES

[1] P. Müller, H. Wymeersch, and R. Piché, “UWB positioning with general-
ized Gaussian mixture ﬁlters,” IEEE Transactions on Mobile Computing,
vol. 13, no. 10, pp. 2406–2414, Oct 2014.

[2] S. Sadhu, S. Mondal, M. Srinivasan, and T. Ghoshal, “Sigma point
Kalman ﬁlter for bearing only tracking,” Signal Processing, Special
Section: Multimodal Human-Computer Interfaces, vol. 86, no. 12, pp.
3769 – 3777, 2006, doi:10.1016/j.sigpro.2006.03.006.

ηlimit00.511.52∞mean error1.41.51.61.71.81.9Step 1Step 10APPENDIX

Proof: GGF estimate does not change when a linear

transformation is applied to measurements

tions are

Substituting (22) to the integrals (6)-(8) the new expecta-

Dh(x)p(x)dx = Dˆy

(cid:90)
(cid:90) (cid:0)x − µ−(cid:1) (Dh(x) − Dˆy)T p(x)dx = ΨDT
(cid:90)

(Dh(x) − Dˆy) (Dh(x) − Dˆy)T p(x)dx = DΦD

˜ˆy =

˜Ψ =

˜Φ =

(43)
and the measurement noise covariance of the transformed
measurement is

˜R = DRDT .

(44)

Substituting these into (4), (5), (9), and (10)

˜S = D(Φ + R)DT
˜K = ΨDT D−T S−1D−1 = KD−1
˜µ+ = µ− + KD−1(Dy − Dˆy) = µ+
˜P + = P − − KD−1DSDT D−T DT K T = P +

(45)
(46)
(47)
(48)
we see that the posterior computed using the transformed
measurement does not change.

Proof: Total KLD nonlinearity does not change under

linear transformation

Using (42) and (43) in (13) we get

By substituting this into (12) with transformed measurement
noise covariance (44)

(49)

= DΥDT

˜Υ = ˜Φ − ˜ΨT(cid:0)P −(cid:1)−1 ˜Ψ
= D(Φ − ΨT(cid:0)P −(cid:1)−1 ˜Ψ)DT
log(cid:12)(cid:12)I + (DRDT )−1DΥDT(cid:12)(cid:12)
log(cid:12)(cid:12)D−T DT + D−T R−1ΥD−T(cid:12)(cid:12)
log(cid:0)(cid:12)(cid:12)D−T(cid:12)(cid:12)(cid:12)(cid:12)I + R−1Υ(cid:12)(cid:12)(cid:12)(cid:12)DT(cid:12)(cid:12)(cid:1)
log(cid:12)(cid:12)I + R−1Υ(cid:12)(cid:12)

˜η =

=

1
2
1
2
1
2
1
2
= η

=

=

1
2

d(cid:88)

i=1

1
2

8

we see that the total nonlinearity does not change.

Proof: Nonlinearity of an element is minimized

The proof is very similar to one in the Appendix B in [16].

When ˜R = I and ˜Υ are diagonal the total nonlinearity is

(41)

(42)

˜η =

=

log |I + ˜Υ|

1
2

log(1 + ˜Υ[i,i])

(51)

(52)

and the nonlinearity corresponding to ith measurement ele-
ment is

˜ηi =

log(1 + ˜Υ[i,i])

(53)
We will show that the smallest diagonal element of ˜Υ is as
small as possible under a linear transformation that preserves
R = I and further that the second smallest diagonal element
is as small as possible, when the next smallest is as small
as possible etc. If the measurement model is transformed by
multiplying it with matrix V , the transformed variables are
ˆΥ = V ˜ΥV T and ˆR = V IV T = V V T . Because we want to
have R = I, V has to be unitary. The ith diagonal element of
the transformed matrix ˜Υ is vT
˜Υ[j,j], where
i
j=1 v2
i,[j] =
1 and the ith diagonal element of the transformed matrix ˆΥ is

vi is the ith column of V . Because V is unitary(cid:80)d
d(cid:88)

˜Υvi =(cid:80)d

˜Υ[j,j] ≥ d(cid:88)

{ ˜Υ[j,j]} = min

{Υ[j,j]}. (54)

j=1 v2

v2
i,[j] min

v2
i,[j]

i,[j]

j

j=1

j=1

Thus, the new diagonal element cannot be smaller than the
smallest diagonal element of ˜Υ.

If the smallest element is the ﬁrst element of the diagonal
the possible transformation for the second smallest element is

(cid:20)1

(cid:20)1

(cid:21)

ˆΥ =

0T
0 V

˜Υ

0T
0 V T

.

(55)

j

(cid:21)

With the same reasoning as given already the second diagonal
has to be already the smallest possible. Inductively this applies
to all diagonal elements. One could also show that now the
measurement element corresponding to the maximal element
of ˜Υ has the largest possible KLD.

(50)

