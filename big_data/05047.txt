Benchmarking Concurrent Priority Queues:

Performance of k-LSM and Related Data Structures

[Brief Announcement]

Jakob Gruber
TU Wien, Austria

gruber@par.tuwien.ac.at

Jesper Larsson Träff
traff@par.tuwien.ac.at

TU Wien, Austria

Martin Wimmer

Google

wimmerm@google.com

6
1
0
2

 
r
a

 

M
6
1

 
 
]
S
D
.
s
c
[
 
 

1
v
7
4
0
5
0

.

3
0
6
1
:
v
i
X
r
a

Keywords
Priority Queues, Concurrency, Relaxation, Benchmarking

ABSTRACT
A number of concurrent, relaxed priority queues have re-
cently been proposed and implemented. Results are com-
monly reported for a throughput benchmark that uses a uni-
form distribution of keys drawn from a large integer range,
and mostly for single systems. We have conducted more ex-
tensive benchmarking of three recent, relaxed priority queues
on four diﬀerent types of systems with diﬀerent key ranges
and distributions. While we can show superior throughput
and scalability for our own k-LSM priority queue for the
uniform key distribution, the picture changes drastically for
other distributions, both with respect to achieved through-
put and relative merit of the priority queues. The through-
put benchmark alone is thus not suﬃcient to characterize
the performance of concurrent priority queues. Our bench-
mark code and k-LSM priority queue are publicly available
to foster future comparison.

1. CONCURRENT PRIORITY QUEUES

Due to the increasing number of processors in modern
computer systems, there is signiﬁcant interest in concur-
rent data structures with scalable performance that goes be-
yond a few dozen processor-cores. However, data structures
(e.g., priority queues) with strict sequential semantics often
present (inherent) bottlenecks (e.g., the delete_min opera-
tion) to scalability, which motivates weaker correctness con-
ditions or data structures with relaxed semantics (e.g., one
of the smallest k items for some k allowed to be deleted).
Applications can often accomodate such relaxations, and in
many such cases (discrete event simulation, shortest path
algorithms, branch-and-bound), the priority queue is a key
data structure.

Many lock-free designs have been based on Skiplists [5, 7,
8]. In contrast, the recently proposed, relaxed k-LSM pri-
ority queue [9] is based on a deterministic Log-Structured

Merge-Tree (LSM), and combines an eﬃcient thread-local
variant for scalability with a shared, relaxed variant for
semantic guarantees. The k-LSM priority queue is lock-
free, linearizable, and provides conﬁgurable guarantees of
delete_min returning one of the kP smallest items, where
k is a conﬁguration parameter and P the number of cores
(threads). The SprayList [1] uses a lock-free Skiplist, and
allows delete_min to remove a random element from the
O(P log3 P ) items at the head of the list. MultiQueues [6]
randomly spread both insertions and deletions over cP local
priority queues, each protected by a lock, with tuning pa-
rameter c, but gives no obvious guarantees on the order of
deleted elements.

2. A CONFIGURABLE BENCHMARK

Priority queue performance is often measured by counting
the number of insert and delete_min operations that can
be performed in a given amount of time, i.e., the through-
put, which would ideally increase linearly with the number
of threads. Like recent studies [1, 5, 7, 8, 9], we also measure
throughput, but additionally we experiment with diﬀerent
workloads: (a) uniform, where each thread performs 50%
insertions and 50% deletions, randomly chosen, (b) split,
where half the threads perform only insertions, and the other
half only deletions; and (integer) key distributions: (a) uni-
form, where keys are drawn uniformly at random from the
range of 32-bit, 16-bit, or 8-bit integers, and (b) ascending
(descending), where keys are drawn from a 10-bit integer
range which is shifted upwards (downwards) at each opera-
tion (plus/minus one).

Queues are preﬁlled with 106 elements with keys taken
from the chosen distribution. This benchmark provides more
scope for investigating locality (split workload), distribution
and range sensitivity. The benchmark could be parameter-
ized further [2] to provide for wider synthetic workloads, e.g.,
sorting as in [4]. To some extent, our ascending/descending
distributions correspond to the hold model advocated in [3].
For relaxed priority queues, it is as important to charac-
terize the deviation from strict priority queue behavior, also
for verifying whether claimed relaxation bounds hold. We
have implemented a rank error benchmark as in [6], where
the rank of an item is its position within the priority queue
as it is deleted.

3. EXPERIMENTAL RESULTS

We have benchmarked variants of the k-LSM priority que-
ue [9] with diﬀerent relaxation settings (klsm128, klsm256,
klsm4096) against a Skiplist based queue [5] (linden), the

Figure 1: mars: Uniform workload, uniform keys.

Figure 3: mars: Uniform workload, 8-bit restricted keys.

20 threads

40 threads

80 threads

Mean

St.D. Mean

St.D. Mean

St.D.

klsm128
klsm256
klsm4096
multiq

32
42
422
1163

29
42
729
3607

57
68
1124
2296

48
57
1287
7881

298
635
13469
3753

288
464
13980
12856

Table 1: mars: Rank error, uniform workload and keys.

posium on Principles and Practice of Parallel Pro-
gramming. 2015, pp. 11–20.

[2] V. Gramoli. “More than you ever wanted to know
about synchronization: synchrobench, measuring the
impact of the synchronization on concurrent algo-
rithms”. In: Proceedings of the 20th ACM Symposium
on Principles and Practice of Parallel Programming.
2015, pp. 1–10.

[3] D. W. Jones. “An Empirical Comparison of Priority-
Queue and Event-Set Implementations”. In: Commu-
nications of the ACM 29.4 (1986), pp. 300–311.

[4] D. H. Larkin, S. Sen, and R. E. Tarjan. “A Back-to-
Basics Empirical Study of Priority Queues”. In: Pro-
ceedings of the 16th Workshop on Algorithm Engineer-
ing and Experiments. 2014, pp. 61–72.

[5] J. Lind´en and B. Jonsson. “A Skiplist-Based Con-
current Priority Queue with Minimal Memory Con-
tention”.
In: Principles of Distributed Systems.
Vol. 8304. Lecture Notes in Computer Science. 2013,
pp. 206–220.

[6] H. Rihani, P. Sanders, and R. Dementiev. “Brief An-
nouncement: MultiQueues: Simple Relaxed Concur-
rent Priority Queues”. In: Proceedings of the 27th ACM
Symposium on Parallelism in Algorithms and Archi-
tectures. 2015, pp. 80–82.

[7] N. Shavit and I. Lotan. “Skiplist-Based Concurrent
Priority Queues”. In: Proceedings of the 14th Interna-
tional Parallel and Distributed Processing Symposium
(IPDPS). 2000, pp. 263–268.

[8] H. Sundell and P. Tsigas. “Fast and lock-free con-
current priority queues for multi-thread systems”. In:
Journal of Parallel and Distributed Computing 65.5
(2005), pp. 609–627.

[9] M. Wimmer et al. “The Lock-free k-LSM Relaxed Pri-
ority Queue”. In: 20th ACM Symposium on Principles
and Practice of Parallel Programming. 2015.

Figure 2: mars: Split workload, ascending keys.

MultiQueue [6] (multiq) and the SprayList [1] (spray). As
a baseline we have used a sequential heap protected by a
lock (globallock). The benchmarks ran on four diﬀer-
ent machines, but we give only results from an 80-core In-
tel Xeon E7-8850 2 GHz system (mars) here (without hy-
perthreading); see the appendix for full details and results
on all machines. Each benchmark is executed 30 times,
and we report on the mean values and conﬁdence intervals.
Our benchmark code can be found at https://github.com/
klsmpq/klsm.

Figure 1 compares the seven priority queue variants un-
der uniform workload, uniform keys. The k-LSM variant
with k = 4096 exhibits superior scalability and throughput
of more than 300 million operations per second (MOps/s),
and vastly outperforms the other priority queues. Changing
to a split workload and ascending keys, this picture changes
dramatically, as shown in Figure 2 where the throughput
drops by a factor of 10. Here multiq performs best, also
in terms of scalability, surprisingly closely followed by lin-
den. Restricting the key range likewise dramatically reduces
the throughput, but the k-LSM performs better in this case,
(Figure 3). In the latter two benchmark conﬁgurations, the
SprayList code was not stable and it was not possible to
gather results. Similar behavior and sensitivity can be ob-
served for the other three machines. Hyperthreading only in
rare cases leads to a throughput increase. Overall, multiq
delivers the most consistent performance.

The rank error results in Table 1 for the uniform workload,
uniform key situation show that delete_min for all queues
return keys that are not far from the minimum, much better
than the the worst-case analyses predict.
References
[1] D. Alistarh et al. “The SprayList: a scalable relaxed
priority queue”. In: Proceedings of the 20th ACM Sym-

llllllllllllllllllllllllllllllll0100200300020406080Number of threadsThroughput in Mops/skernellgloballockklsm128klsm256klsm4096lindenmultiqsprayllllllllllllllllllllllllllllllll0102030020406080Number of threadsThroughput in Mops/skernellgloballockklsm128klsm256klsm4096lindenmultiqllllllllllllllllllllllllllllllll0102030020406080Number of threadsThroughput in Mops/skernellgloballockklsm128klsm256klsm4096lindenmultiqAPPENDIX
This appendix contains the full set of experimental results
and details on the experimental setup. We brieﬂy recapit-
ulate the main three priority queue implementations, and
describe the benchmarks in more detail. The remainder are
our current results from the four available machines.

A. CONCURRENT, RELAXED PRIORITY

QUEUES

The priority queues considered here support two opera-

tions on key-value pairs, namely:

• insert, which inserts a key-value pair into the priority

queue, and

• delete_min, which removes a key-value pair with a
smallest key and copies the corresponding value into a
given location.

Concurrent priority queues so far neither support operations
on speciﬁc key-value pairs, like for instance decrease_key
as needed for asymptotically eﬃcient single-source shortest
path algorithms, nor operations on the queues as a whole
like meld.

A linearizable, strict priority queue imposes a real time or-
der on priority queue operations, and in such an order each
delete_min must return a least key-value pair. Relaxed
consistency conditions like quasi-linearizability allow some
non-determinism by accepting runs that are some bounded
distance away from a strict, linear order [1] as correct. Re-
cently, an even weaker conditions called local linearizability
was proposed [8], but this may be too weak to be useful for
priority queues. On the other hand, relaxed priority queue
semantics relax the sequential priority queue semantics to
allow that one of the k smallest keys is returned, for some
value k, at the delete_min operation [25, 26]. Such relaxed
priority queues are considered here. Presumably, the better
and the more precisely k can be controlled, the better for
the applications.

B. THE K-LSM PRIORITY QUEUE

The k-LSM [7, 26] is a lock-free, linearizable, relaxed pri-
ority queue consisting of a global component called the Shared
LSM (SLSM), and a thread-local component called the Dist-
ributed LSM (DLSM). As their names imply, both the SLSM
and DLSM are based on the LSM (Log-Structured Merge-
Tree) [18] data structure. The LSM was ﬁrst introduced
to the database community in 1996 and later reinvented in-
dependently by Wimmer driven by the requirements of re-
laxed, concurrent priority queues [25]. Both the SLSM and
the DLSM may be used as standalone priority queues, but
have complementary advantages and disadvantages which
can be balanced against each other by their composition.

The LSM consists of a logarithmic number of sorted arrays
(called blocks) storing key-value containers (items). Blocks
have capacities C = 2i and capacities within the LSM are
distinct. A block with capacity C must contain more than C
2
and at most C items. Insertions initially add a new single-
ton block to the LSM, and then merge blocks with identical
capacities until all block capacities within the LSM are once
again distinct. Deletions simply return the smallest of all
blocks’ minimal item. It is easy to see that both insertions

and deletions can be supported in O(log n) operations where
n is the number of items in the LSM.

The DLSM is a distributed data structure containing a
single thread-local LSM per thread. Operations on the
DLSM are essentially embarassingly parallel, since inter-
thread communication occurs only when a deletion ﬁnds
the local LSM empty, and then attempts to copy another
thread’s items. Items returned by delete_min are guaran-
teed to be minimal on the current thread.

The SLSM consists of a single global, centralized LSM, to-
gether with a corresponding range of items called the pivot
range. The SLSM’s pivot range depicts a subset of the k + 1
smallest items (where k is the relaxation parameter). Dele-
tions randomly choose an item from this range, and thus are
allowed to skip at most k items.

Finally, the k-LSM itself is a very simple data structure:
it contains a DLSM, limited to a maximum capacity of k per
thread; and a SLSM with a pivot range containing at most
k+1 of its smallest items. Items are initially inserted into the
local DLSM. When its capacity overﬂows, its largest block is
batch-inserted into the SLSM. Deletions simply peek at both
the DLSM and SLSM, and return the smaller item. Since
deletions from the DLSM skip at most k(P −1) items (where
P is the number of threads) and deletions from the SLSM
skip at most k items, k-LSM deletions skip a maximum of
kP items in total.

We implemented the k-LSM using the C++11 memory
model. A memory model determines the order in which
changes to memory locations by one thread become visible
to other threads; for instance, usage of the the std::atomic
type together with its load() and store() operations ensures
portable multithreaded behavior across diﬀerent architec-
tures. It is possible to vary the strictness of provided guar-
antees between sequential consistency (on the strict end)
and relaxed behavior (guaranteeing only atomicity).

In our implementation, we extensively use the previously
mentioned std::atomic type together with its load, store,
fetch_add, and compare_exchange_strong operations. When
possible, we explicitly use relaxed memory ordering as it is
the potentially most eﬃcient (and weakest) of all memory
ordering types, requiring only atomicity.

Our implementation, consisting of a standalone k-LSM as
well as our parameterizable benchmark, is publicly available
at https://github.com/klsmpq/klsm, and desribed in detail
in [7].

C. ALGORITHMS

Our benchmarks compare the following algorithms:
Globallock (globallock). A simple, standardized se-
quential priority queue implementation protected by a global
lock is used to establish a baseline for acceptable perfor-
mance. We use the simple priority queue implementation
(std::priority_queue) provided by the C++ Standard Li-
brary (STL) [11].

Linden (linden). The Lind´en and Jonsson priority que-
ue [15] is currently one of the most eﬃcient Skiplist-based
designs, improving upon the performance of previous similar
data structures [9, 22, 24] by up to a factor of two. It is lock-
free and linearizable, but has strict semantics, i.e., deletions
must return the minimal item in some real-time order.

SprayList (spray). This relaxed priority queue is based
on the lock-free Fraser Skiplist [5]. Deletions use a random-
walk method in order to return one of the O(P log3 P ) small-

est items, where P is the number of threads [2].

MultiQueue (multiq). The MultiQueue is a recent de-
sign by Rihani, Sanders, and Dementiev [20] and consists
of cP arbitrary priority queues, where c is a tuning param-
eter (set to 4 in our benchmarks) and P is the number of
threads; our benchmark again uses the simple sequential pri-
ority queue (std::priority_queue) provided by the STL [11],
each protected by a lock. Items are inserted into a random
priority queue, while deletions return the minimal item of
two randomly selected queues. So far, no complete analysis
of its semantic bounds exists.

k-LSM (klsm128, klsm256, klsm4096). We evaluate sev-
eral instantiations of the k-LSM with varying degrees of re-
laxation, ranging from medium (k ∈ {128, 256}) to high
relaxation (k = 4096). Results for low relaxation (k = 16)
are not shown since its behavior closely mimics the Lind´en
and Jonsson priority queue.

Unfortunately, we were not able to measure every algo-
rithm on each machine. The linden and spray priority
queues require libraries not present on ceres and pluto.
The SprayList implementation also turned out to be unsta-
ble in our experiments, crashing under most circumstances
outside the uniform workload, uniform key distribution bench-
mark.

D. OTHER PRIORITY QUEUES

The Hunt et al. priority queue [10] is an early concurrent
design.
It is based on a Heap structure and attempts to
minimize lock contention between threads by a) adding per-
node locks, b) spreading subsequent insertions through a bit-
reversal technique, and c) letting insertions traverse bottom-
up in order to minimize conﬂicts with top-down deletions. It
has been shown to perform well compared to other eﬀorts of
the time; however, it is easily outperformed by more modern
designs.

Shavit and Lotan were the ﬁrst to propose the use of
Skiplists for priority queues [15]. Their initial locking im-
plementation [22] builds on Pugh’s concurrent Skiplist [19],
which uses one lock per node per level. Herlihy and Shavit [9]
later described and implemented a lock-free, quiescently con-
sistent version of this idea in Java.

Sundell and Tsigas invented the ﬁrst lock-free concurrent
priority queue in 2003 [24]. Benchmarks show their queue
performing noticeably better than both locking queues by
Shavit and Lotan and Hunt et al., and slightly better than a
priority queue consisting of a Skiplist protected by a single
global lock.

Mounds [16, 17] is a recent concurrent priority queue de-
sign based on a tree of sorted lists. Liu and Spear provide
two variants of their data structure; one of them is lock-
based, while the other is lock-free and relies on the Double-
Compare-And-Swap (DCAS) instruction, which is not avail-
able natively on most current processors.

One of the latest strict priority queues of interest, called
the Chunk-Based Priority Queue (CBPQ), was presented
recently in the dissertation of Braginsky [3]. It is primarily
based on two main ideas: the chunk linked list [4] replaces
Skiplists and heaps as the backing data structure, and use
of the more eﬃcient Fetch-And-Add (FAA) instruction is
preferred over the Compare-And-Swap (CAS) instruction.
Benchmarks compare the CBPQ against the Lind´en and
Jonsson queue and lock-free as well as lock-based versions of
the Mound priority queue [16] for diﬀerent workloads. The

CBPQ clearly outperforms the other queues in mixed work-
loads (50% insertions, 50% insertions) and deletion work-
loads, and exhibits similar behavior as the Lind´en and Jon-
sson queue in insertion workloads, where Mounds are dom-
inant.

E. MACHINES

The benchmarks were executed on four machines:
• mars, an 80-core (8x10 cores) Intel Xeon E7-8850 at 2
GHz with 1 TB of RAM main memory, and 32 KB L1,
256 KB L2, 24 MB L3 cache, respectively. mars has
2-way hardware hyperthreading.

• saturn, a 48-core machine with 4 AMD Opteron 6168
processors with 12 cores each, clocked at 1.9 GHz, and
125 GB RAM main memory, and 64 KB of L1, 512 KB
of L2, and 5 MB of L3 cache, respectively. The AMD
processor does not support hyperthreading.

• ceres, a 64-core SPARCv9-based machine with 4 pro-
cessors of 16 cores each. Cores are clocked at 3.6 GHz
and have 8-way hardware hyperthreading. Main mem-
ory is 1 TB RAM, and cache is 16 KB L1, 128 KB L2,
and 8 MB L3 , respectively.

• pluto, a 61-core Intel Xeon Phi processor clocked at
1.2 GHz with 4-way hardware hyperthreading. Main
memory is 15 GB RAM, and cache 32 KB L1, 512 KB
L2, respectively.

All applications are compiled using gcc, when possible:
version 5.2.1 on mars and saturn, and version 4.8.2 on
ceres. We use optimization level of -O3 and enable link-time
optimizations using -flto. Cross-compilation for the Intel
Xeon Phi on pluto is done using Intel’s icc 14.0.2. No
further optimizations were performed, in particular vector-
ization was entirely delegated to the compiler, which prob-
ably leaves the Xeon Phi pluto at a disadvantage. On the
other hand, all implementations are treated similarly.

F. BENCHMARKS

Our performance benchmarks are (currently) based on
throughput, i.e., how many operations (insertions and dele-
tions combined) complete within a certain timeframe. We
preﬁll priority queues with 106 elements prior the bench-
mark, and then measure throughput for 10 seconds, ﬁnally
reporting on the number of operations performed per sec-
ond. This metric and a roughly similar setup is used in
much recent work [2, 15, 22, 24, 26]. Alternatively, a num-
ber of queue operations could be prescribed, and the time
(latency) for this number and mix of operations measured.
The behavior of our throughput benchmark is controlled
by the two parameters workload and key distribution. The
workload may be

• uniform, meaning that each thread executes a roughly

equal amount of insertions and deletions,

• split, meaning that half the threads insert, while the

other half delete, or

• alternating, in which each thread strictly alternates be-

tween insertions and deletions.

The key distribution controls how keys are generated for
inserted key-value pairs, and my may be either

• uniform with keys chosen uniformly at random from
some range of integers (we have used 32-, 16-, and 8-bit
ranges), or

• ascending or descending, meaning that a uniformly
chosen key from a 10-bit integer range ascends or de-
scends over time by adding or subtracting the chosen
key to the operation number.

We would like to supply a parameterized benchmark sim-
ilar to the Synchrobench framework of Gramoli [6] with the
following orthogonal parameters:

• Key type:

integer, ﬂoating point, possibly complex
type from some ordered set (here, we have experi-
mented with integers only).

• Key base range, which is the range from which the
random component of the next key is chosen (here, we
have used 32-, 16-, 10- and 8-bit ranges).

• Key distribution, the distribution of keys within their
base range (here, we have used only uniform distribu-
tions, but others, e.g., as in [12], are also possible).

• Key dependency switch (none, ascending, descending),
which determines whether the next key for a thread is
dependent on the key of the last deleted element by
the thread. A dependent key is formed by adding or
subtracting the randomly generated base key to the
key of the last deleted item (we have experimented
with dependent keys where the next key is formed by
adding to or subtracting from the operation number).

• Operation distribution:

insertions and deletions are
chosen randomly with a prescribed probability of an
operation being an insert (we have experimented with
50% insertions so that the queues remain in a steady
state).

• Alternatively, an operation batch size can be set to al-
ternate between batches of insertions and deletions (we
have experimented with strictly alternating insertions
and deletions).

• Workload determines the fraction of threads that per-
form insertions and the fraction of threads that per-
form deletions (we have experimented with uniform
and split workloads, where in the latter half the threads
perform the insertions and the other half the dele-
tions).

• Preﬁll determines the number of items put in the queue
before the time measurement starts; preﬁlling is done
according to the workload and key distribution.

• Throughput/latency switch, where for throughput a
duration (time limit) is speciﬁed and for latency the
total number of operations.

• Repetition count and other statistic requirements.

For instance, giving an operation batch size of one with
an insert following delete with dependent keys under a spe-
ciﬁc distribution would correspond to the hold model pro-
posed in [12] and used in early studies of concurrent priority
queues [13, 21]. Choosing large batches would correspond to
the sorting benchmark used in [14].

In addition, as in [20] we also evaluated the semantic qual-
ity produced by multiq and the k-LSM with several diﬀerent
relaxations by measuring the rank of items (i.e., their posi-
tion within the priority queue) returned by delete_min. The
quality benchmark initially records all inserted and deleted
items together with their timestamp in a log; this log is then
used to reconstruct a global, linear sequence of all opera-
tions. A specialized sequential priority queue is then used
to replay this sequence and eﬃciently determine the rank
of all deleted items. Our quality benchmark is pessimistic,
i.e., it may return artiﬁcially inﬂated ranks when items with
duplicate keys are encountered.

G. FURTHER EXPERIMENTAL RESULTS
Each benchmark is executed 30 times, and we report on

the mean values and conﬁdence intervals.

Figure 4 shows throughput results for mars. Up to 80
threads, each thread is pinned to a separate physical core,
while hyperthreading is used at higher thread counts.

Under uniform workload and uniform key distribution,
the k-LSM has very high throughput, reaching over 300
MOps/s at 80 cores with a relaxation factor of k = 4096.
For the remaining data structures, multiq shows the best
performance, reaching around 40 MOps/s at 160 threads
(a decrease over the k-LSM by around a factor 7.5). The
SprayList reaches a maximum of around 11 MOps/s at 140
threads, while the Lind´en and Jonsson queue and global-
lock peak at around 6.7 MOps/s (10 threads) and 7.5 MOp-
s/s (1 thread), respectively.

Varying the key distribution has a strong inﬂuence on the
behavior on the k-LSM. Ascending keys (Figure 4b) result
in a signiﬁcant performance drop for all k-LSM variants,
all of which behave similarly: throughput oscillates between
around 5 and 15 MOps/s until 80 threads, and then slowly
increases to a local maximum at around 140 threads. On the
other hand, descending keys (Figure 4c) cause a performance
increase for the klsm4096, which reaches a new maximum
of around 400 MOps/s. Behavior of multiq, linden, and
globallock remain more or less stable in both cases.

Under split workloads (Figures 4d through 4f), the k-
LSM’s throughput is very low and never exceeds the through-
put of our sequential baseline globallock at a single thread.
Interestingly, the Lind´en and Jonsson priority queue has
drastically improved scalability when using a combination
of split workload and ascending key distribution. We as-
sume that this is due to improved cache-locality: inserting
threads access only the tail end of the list, with deleting
threads accessing only the list head. linden was unstable at
higher thread counts under split workload and descending
keys, and we omit these results.

A key domain restricted to 8-bit integers (Figure 4g) re-
sults in many duplicate key values within the priority queue.
This also causes decreased throughput of the k-LSM: medium
relaxations do not appear to scale at all, while the klsm4096
does seem to scale well — but only to a maximum through-
put of just over 30 MOps/s. The larger 16-bit domain of
Figure 4h produces very similar results to the uniform key

benchmark with a 32-bit range.

Hyperthreading is beneﬁcial in only a few cases. For in-
stance, multiq makes further modest gains beyond 80 threads
with uniform workloads (e.g., Figures 4b and 4g). However,
in general, most algorithms do not appear to beneﬁt from
hyperthreading.

Table 2 contains our quality results for mars, showing both
the rank mean and its standard deviation for 20, 40, and 80
threads. In general, the k-LSM produces an average quality
signiﬁcantly better than its theoretic upper bound of a rank
of kP + 1. For example, the klsm128 has an average rank of
32 under the uniform workload, uniform key benchmark at
20 threads (Table 2a), compared to the maximally allowed
rank error of 2561. Relaxation of multiq appears to be
somewhat comparable to klsm4096, and it seems to grow
linearly with the thread count. The uniform 8-bit restricted
key benchmark (Table 2g) has artiﬁcially inﬂated ranks due
to the way our quality benchmark handles key duplicates.

Figure 5 shows results for our AMD Opteron machine
called saturn. Here, MultiQueue has fairly disappointing
throughput, barely achieving the sequential performance of
globallock, and only substantially exceeding it under split
workload and ascending key distribution (Figure 5e). Sur-
prisingly, with keys restricted to the 8-bit range (Figure 5g),
the linden queue has a higher throughput than all other
data structures. Quality trends in Table 3 are consistent
with those on mars.

Figure 6 and Table 4 display throughput and quality re-
sults for ceres. On this machine, we display results for up to
4-way hyperthreading. As previously on mars, throughput
of tested algorithms does not beneﬁt from hyperthreading in
general. Only multiq appears to consistently gain further
(small) increases at thread counts over 64. Split workload
combined with uniform key distribution (Figure 6d) causes
a local maximum of around 30 MOps/s for the klsm4096.

Figure 7 shows throughput results for our Xeon Phi ma-
chine pluto (there is no corresponding quality table, since
we did not run our quality benchmark on this machine). On
pluto, the k-LSM does not match the trend for very high
throughput as previously exhibited for the uniform workload
and uniform key benchmark (Figure 7a). While scalability
is decent up to the physical core count of 61, its absolute
performance is exceeded by the multiq at higher thread
counts. Only in descending key distribution (Figure 7c)
does the k-LSM reach previous heights. Note that this is
also the only benchmark in which throughput on pluto ex-
ceeds roughly 15 MOps/s. Unfortunately, the k-LSM was
unstable at higher thread counts under split workloads and
we omit all data where it is not completely reliable. Again,
hyperthreading results in modest gains for multiq, and in
stagnant performance for all other data structures in the
best case.

Finally, Figures 8 and 9 show results for alternating work-
loads on all of our machines. Although the alternating work-
load appears to be similar to uniform workloads (both per-
form 50% insertions and 50% deletions, and are distinguished
only by the fact that operations are strictly alternating in
the alternating workload), there are signiﬁcant diﬀerences in
the resulting throughput. Uniform keys on mars (Figure 8a)
show increases for the k-LSM in both throughput (to al-
most 400 MOps/s) and scalability, with all k-LSM variants
(k ∈ {128, 256, 4096}) scaling almost equally well until 80
threads. Likewise, descending keys (Figure 8c) sees all k-

LSM variants reaching a new throughput peak of around
600 MOps/s. Behavior on saturn is similar, in which uni-
form and descending keys show improved throughput and
scalability for the k-LSM, while results for ascending keys
remain unchanged from the uniform workload benchmark.
On ceres, only scalability seems to improve while through-
put is again roughly unchanged compared to uniform work-
load. Finally, on pluto, the k-LSM surprisingly does not
perform well in any case, not even under descending keys
(which led to good results when using uniform workload).
However, multiq throughput increases by almost a factor of
8, reaching over 80 MOps/s in all cases.

In general, the k-LSM priority queue seems superior to the
other priority queues in speciﬁc scenarios: in uniform work-
load combined with uniformly chosen 32- or 16-bit keys, and
with descending key distribution, throughput is almost 10
times that of other priority queues. However, in most other
benchmarks its performance is disappointing. This appears
to be due to the diﬀering loads placed on its component
data structures: whenever the extremely scalable DLSM is
highly utilized, throughput increases; and when the load
shifts towards the SLSM, throughput drops. The fact that
the k-LSM is composed of two priority queue designs seems
to cause it to be highly sensitive towards changes in its en-
vironment.

The MultiQueue does not seem to have the same poten-
tial for raw performance as the k-LSM at its peak. However,
in the majority of cases it still outperforms all other tested
priority queues by a good margin. And most signiﬁcantly,
its behavior is extremely stable across all of our benchmark
types. Quality results show that relaxation of the Multi-
Queue is fairly high, but it appears to grow linearly with
the thread count.

The linden queue generally only scales as long as partic-
ipating processors are located on the same physical socket;
however, a split workload combined with ascending key gen-
eration is the exception to this rule, in which the linden
queue is often able to scale well until the maximal thread
count.

References
[1] Y. Afek, G. Korland, and E. Yanovsky. “Quasi-
linearizability: Relaxed consistency for improved con-
In: Principles of Distributed Systems.
currency”.
Vol. 6490. Lecture Notes
in Computer Science.
Springer, 2010, pp. 395–410.

[2] D. Alistarh et al. “The SprayList: a scalable relaxed
priority queue”. In: Proceedings of the 20th ACM Sym-
posium on Principles and Practice of Parallel Pro-
gramming. 2015, pp. 11–20.

[3] A. Braginsky. “Multi-Threaded Coordination Methods
for Constructing Non-blocking Data Structures”. PhD
thesis. 2015.

[4] A. Braginsky and E. Petrank. “Locality-conscious
lock-free linked lists”. In: Distributed Computing and
Networking. Vol. 6522. Lecture Notes in Computer Sci-
ence. Springer, 2011, pp. 107–118.

[5] K. Fraser. “Practical lock-freedom”. PhD thesis. PhD
thesis, Cambridge University Computer Laboratory,
2003. Also available as Technical Report UCAM-CL-
TR-579, 2004.

[22] N. Shavit and I. Lotan. “Skiplist-Based Concurrent
Priority Queues”. In: Proceedings of the 14th Interna-
tional Parallel and Distributed Processing Symposium
(IPDPS). 2000, pp. 263–268.

[23] H. Sundell and P. Tsigas. “Fast and lock-free concur-
rent priority queues for multi-thread systems”. In: Pro-
ceedings of the 17th International Symposium on Par-
allel and Distributed Processing. IEEE. 2003, 11–pp.

[24] H. Sundell and P. Tsigas. “Fast and lock-free con-
current priority queues for multi-thread systems”. In:
Journal of Parallel and Distributed Computing 65.5
(2005), pp. 609–627.

[25] M. Wimmer. “Variations on Task Scheduling for
Shared Memory Systems”. PhD thesis. Vienna Uni-
versity of Technology (TU Wien), June 2014.

[26] M. Wimmer et al. “The Lock-free k-LSM Relaxed Pri-
ority Queue”. In: 20th ACM Symposium on Principles
and Practice of Parallel Programming. 2015.

[6] V. Gramoli. “More than you ever wanted to know
about synchronization: synchrobench, measuring the
impact of the synchronization on concurrent algo-
rithms”. In: Proceedings of the 20th ACM Symposium
on Principles and Practice of Parallel Programming.
2015, pp. 1–10.

[7] J. Gruber. “KLSM: A Relaxed Lock-Free Priority
Queue”. MA thesis. Vienna University of Technology
(TU Wien), Jan. 2016.

[8] A. Haas et al. “Local Linearizability”. In: CoRR
abs/1502.07118 (2015). url: http : / / arxiv . org / abs /
1502.07118.

[9] M. Herlihy and N. Shavit. The Art of Multiprocessor

Programming, Revised Reprint. Elsevier, 2012.

[10] G. C. Hunt et al. “An eﬃcient algorithm for concur-
rent priority queue heaps”. In: Information Processing
Letters 60.3 (1996), pp. 151–157.

[11]

ISO/IEC.
14882:2014(E)
Geneva, Switzerland, 2014.

ISO International Standard ISO/IEC
– Programming Language C++.

[12] D. W. Jones. “An Empirical Comparison of Priority-
Queue and Event-Set Implementations”. In: Commu-
nications of the ACM 29.4 (1986), pp. 300–311.

[13] D. W. Jones. “Concurrent Operations on Priority
Queues”. In: Communications of the ACM 32.1 (1989),
pp. 132–137.

[14] D. H. Larkin, S. Sen, and R. E. Tarjan. “A Back-to-
Basics Empirical Study of Priority Queues”. In: Pro-
ceedings of the 16th Workshop on Algorithm Engineer-
ing and Experiments. 2014, pp. 61–72.

[15] J. Lind´en and B. Jonsson. “A Skiplist-Based Con-
current Priority Queue with Minimal Memory Con-
tention”.
In: Principles of Distributed Systems.
Vol. 8304. Lecture Notes in Computer Science. 2013,
pp. 206–220.

[16] Y. Liu and M. F. Spear. “A lock-free, array-based pri-
ority queue”. In: Proceedings of the 17th ACM Sympo-
sium on Principles and Practice of Parallel Program-
ming. 2012, pp. 323–324.

[17] Y. Liu and M. F. Spear. “Mounds: Array-Based Con-
current Priority Queues”. In: 41st International Con-
ference on Parallel Processing. 2012, pp. 1–10.

[18] P. O’Neil et al. “The Log-Structured Merge-Tree
33.4 (1996),

In: Acta Informatica

(LSM-tree)”.
pp. 351–385.

[19] W. Pugh. Concurrent maintenance of skip lists. Tech.

rep. 1998.

[20] H. Rihani, P. Sanders, and R. Dementiev. “Brief An-
nouncement: MultiQueues: Simple Relaxed Concur-
rent Priority Queues”. In: Proceedings of the 27th ACM
Symposium on Parallelism in Algorithms and Archi-
tectures. 2015, pp. 80–82.

[21] R. R¨onngren and R. Ayani. “A Comparative Study of
Parallel and Sequential Priority Queue Algorithms”.
In: ACM Transactions on Modeling and Simulation 7.2
(1997).

(a) Uniform workload, uniform keys (32 bits).

(b) Uniform workload, ascending keys.

(c) Uniform workload, descending keys.

(d) Split workload, uniform keys (32 bits).

(e) Split workload, ascending keys.

(f) Split workload, descending keys.

(g) Uniform workload, uniform keys (8 bits).

(h) Uniform workload, uniform keys (16 bits).

Figure 4: Throughput on mars.

lllllllllllllllllllllllllllllllllllllllllllllllllll0100200300050100150Number of threadsThroughput in Mops/skernellgloballockklsm128klsm256klsm4096lindenmultiqspraylllllllllllllllllllllllllllllllllllllllllllllllllll010203040050100150Number of threadsThroughput in Mops/skernellgloballockklsm128klsm256klsm4096lindenmultiqlllllllllllllllllllllllllllllllllllllllllllllllllll0100200300400050100150Number of threadsThroughput in Mops/skernellgloballockklsm128klsm256klsm4096lindenmultiqlllllllllllllllllllllllllllllllllllllllllllllllllll0102030050100150Number of threadsThroughput in Mops/skernellgloballockklsm128klsm256klsm4096lindenmultiqlllllllllllllllllllllllllllllllllllllllllllllllllll0102030050100150Number of threadsThroughput in Mops/skernellgloballockklsm128klsm256klsm4096lindenmultiqlllllllllllllllllllllllllllllllllllllllllllllllllll0102030050100150Number of threadsThroughput in Mops/skernellgloballockklsm128klsm256klsm4096lindenmultiqlllllllllllllllllllllllllllllllllllllllllllllllllll010203040050100150Number of threadsThroughput in Mops/skernellgloballockklsm128klsm256klsm4096lindenmultiqlllllllllllllllllllllllllllllllllllllllllllllllllll0100200050100150Number of threadsThroughput in Mops/skernellgloballockklsm128klsm256klsm4096lindenmultiq20 threads

40 threads

80 threads

20 threads

40 threads

80 threads

Mean

St.D. Mean

St.D. Mean

St.D.

Mean

St.D. Mean

St.D. Mean

St.D.

klsm128
klsm256
klsm4096
multiq

32
42
422
1163

29
42
729
3607

57
68
1124
2296

48
57
1287
7881

298
635
13469
3753

288
464
13980
12856

klsm128
klsm256
klsm4096
multiq

21
38
499
101

18
33
469
120

22
38
479
202

19
33
451
239

26
42
496
451

22
37
467
566

(a) Uniform workload, uniform key.

(b) Uniform workload, ascending keys.

20 threads

40 threads

80 threads

20 threads

40 threads

80 threads

Mean

St.D. Mean

St.D. Mean

St.D.

Mean

St.D. Mean

St.D. Mean

St.D.

klsm128
klsm256
klsm4096
multiq

241
472
2261
329

175
341
2601
1708

490
942
2954
674

340
667
2728
3641

880
1765
3913
1277

649
1234
3709
5985

klsm128
klsm256
klsm4096
multiq

129
217
4497
198

70
133
2495
617

294
557
11999
506

176
340
9176
1530

916
1762
41466
2528

520
1174
25387
7492

(c) Uniform workload, descending keys.

(d) Split workload, uniform keys (32 bits).

20 threads

40 threads

80 threads

20 threads

40 threads

80 threads

Mean

St.D. Mean

St.D. Mean

St.D.

Mean

St.D. Mean

St.D. Mean

St.D.

klsm128
klsm256
klsm4096
multiq

20
34
439
163

16
29
400
634

27
44
894
514

62
97
2772
1308

44
78
1530
1031

152
290
4027
2107

klsm128
klsm256
klsm4096
multiq

561
1098
13226
1252

203
426
7884
6376

1138
2385
34502
6504

340
541
12221
24567

2251
4555
56529
1453

702
1286
29092
5442

(e) Split workload, ascending keys.

(f) Split workload, descending keys.

20 threads

40 threads

80 threads

20 threads

40 threads

80 threads

Mean

St.D. Mean

St.D. Mean

St.D.

Mean

St.D. Mean

St.D. Mean

St.D.

klsm128
klsm256
klsm4096
multiq

992
1001
1091
1675

1252
1384
2274
4263

1006
1022
1320
2620

1192
1399
2480
8243

1059
1174
12654
3812

1111
1306
13036
12166

klsm128
klsm256
klsm4096
multiq

36
43
268
1173

34
44
470
3579

58
64
481
2398

49
56
854
8092

267
551
14060
3744

165
392
14217
12367

(g) Uniform workload, uniform keys (8 bits).

(h) Uniform workload, uniform keys (16 bits).

Table 2: Rank error on mars.

(a) Uniform workload, uniform keys (32 bits).

(b) Uniform workload, ascending keys.

(c) Uniform workload, descending keys.

(d) Split workload, uniform keys (32 bits).

(e) Split workload, ascending keys.

(f) Split workload, descending keys.

(g) Uniform workload, uniform keys (8 bits).

(h) Uniform workload, uniform keys (16 bits).

Figure 5: Throughput on saturn.

llllllllllllllllllll030609001020304050Number of threadsThroughput in Mops/skernellgloballockklsm128klsm256klsm4096lindenmultiqllllllllllllllllllll0.02.55.07.510.001020304050Number of threadsThroughput in Mops/skernellgloballockklsm128klsm256klsm4096lindenmultiqllllllllllllllllllll05010015001020304050Number of threadsThroughput in Mops/skernellgloballockklsm128klsm256klsm4096lindenmultiqllllllllllllllllllll03691201020304050Number of threadsThroughput in Mops/skernellgloballockklsm128klsm256klsm4096lindenmultiqllllllllllllllllllll05101501020304050Number of threadsThroughput in Mops/skernellgloballockklsm128klsm256klsm4096lindenmultiqllllllllllllllllllll0246801020304050Number of threadsThroughput in Mops/skernellgloballockklsm128klsm256klsm4096lindenmultiqllllllllllllllllllll036901020304050Number of threadsThroughput in Mops/skernellgloballockklsm128klsm256klsm4096lindenmultiqllllllllllllllllllll025507510001020304050Number of threadsThroughput in Mops/skernellgloballockklsm128klsm256klsm4096lindenmultiq12 threads

24 threads

48 threads

12 threads

24 threads

48 threads

Mean

St.D. Mean

St.D. Mean

St.D.

Mean

St.D. Mean

St.D. Mean

St.D.

klsm128
klsm256
klsm4096
multiq

21
31
310
277

19
32
452
625

32
43
2412
899

27
39
2096
2499

74
120
3319
2298

77
136
3006
6544

klsm128
klsm256
klsm4096
multiq

21
38
515
60

17
33
484
71

21
38
486
121

18
33
466
143

23
39
748
243

20
35
1130
287

(a) Uniform workload, uniform keys (32 bits).

(b) Uniform workload, ascending keys.

12 threads

24 threads

48 threads

12 threads

24 threads

48 threads

Mean

St.D. Mean

St.D. Mean

St.D.

Mean

St.D. Mean

St.D. Mean

St.D.

klsm128
klsm256
klsm4096
multiq

143
262
1406
193

113
215
1697
964

284
532
2720
381

213
419
3141
1853

582
1092
3948
754

447
797
4034
3418

klsm128
klsm256
klsm4096
multiq

61
198
1982
81

31
114
1218
140

219
220
2832
172

118
113
1523
332

257
950
5482
548

140
524
3825
1476

(c) Uniform workload, descending keys.

(d) Split workload, uniform keys (32 bits).

12 threads

24 threads

48 threads

12 threads

24 threads

48 threads

Mean

St.D. Mean

St.D. Mean

St.D.

Mean

St.D. Mean

St.D. Mean

St.D.

klsm128
klsm256
klsm4096
multiq

19
34
446
62

15
29
411
93

19
34
422
118

16
29
403
150

20
35
405
219

17
30
376
267

klsm128
klsm256
klsm4096
multiq

192
362
6471
337

159
290
4024
1780

431
759
12923
329

284
620
8545
1557

835
1559
24879
372

786
1564
17970
1115

(e) Split workload, ascending keys.

(f) Split workload, descending keys.

12 threads

24 threads

48 threads

12 threads

24 threads

48 threads

Mean

St.D. Mean

St.D. Mean

St.D.

Mean

St.D. Mean

St.D. Mean

St.D.

klsm128
klsm256
klsm4096
multiq

986
989
1233
1160

1129
1253
1858
2039

992
1002
1398
1478

1119
1241
1973
3094

1015
1039
2537
2669

1108
1234
3540
7097

klsm128
klsm256
klsm4096
multiq

26
39
355
342

24
43
553
843

37
52
698
898

32
54
1097
2478

82
156
2793
2314

76
194
3934
6898

(g) Uniform workload, uniform keys (8 bits).

(h) Uniform workload, uniform keys (16 bits).

Table 3: Rank error on saturn.

(a) Uniform workload, uniform keys (32 bits).

(b) Uniform workload, ascending keys.

(c) Uniform workload, descending keys.

(d) Split workload, uniform keys (32 bits).

(e) Split workload, ascending keys.

(f) Split workload, descending keys.

(g) Uniform workload, uniform keys (8 bits).

(h) Uniform workload, uniform keys (16 bits).

Figure 6: Throughput on ceres.

llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll0501001500100200Number of threadsThroughput in Mops/skernellgloballockklsm128klsm256klsm4096multiqllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll0102030400100200Number of threadsThroughput in Mops/skernellgloballockklsm128klsm256klsm4096multiqllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll0501001502000100200Number of threadsThroughput in Mops/skernellgloballockklsm128klsm256klsm4096multiqllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll0102030400100200Number of threadsThroughput in Mops/skernellgloballockklsm128klsm256klsm4096multiqllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll0102030400100200Number of threadsThroughput in Mops/skernellgloballockklsm128klsm256klsm4096multiqllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll0102030400100200Number of threadsThroughput in Mops/skernellgloballockklsm128klsm256klsm4096multiqllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll0102030400100200Number of threadsThroughput in Mops/skernellgloballockklsm128klsm256klsm4096multiqllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll0501000100200Number of threadsThroughput in Mops/skernellgloballockklsm128klsm256klsm4096multiq16 threads

32 threads

64 threads

16 threads

32 threads

64 threads

Mean

St.D. Mean

St.D. Mean

St.D.

Mean

St.D. Mean

St.D. Mean

St.D.

klsm128
klsm256
klsm4096
multiq

30
43
289
1258

26
44
488
4657

58
79
594
1995

49
72
1032
6603

147
231
5831
3315

130
231
7604
11720

klsm128
klsm256
klsm4096
multiq

20
37
513
81

17
32
477
95

23
39
493
163

19
33
460
192

25
43
528
500

22
37
493
669

(a) Uniform workload, uniform keys (32 bits).

(b) Uniform workload, ascending keys.

16 threads

32 threads

64 threads

16 threads

32 threads

64 threads

Mean

St.D. Mean

St.D. Mean

St.D.

Mean

St.D. Mean

St.D. Mean

St.D.

klsm128
klsm256
klsm4096
multiq

215
400
1097
295

153
292
961
1912

427
796
4080
535

310
556
3970
2766

1064
2057
7742
1085

848
1562
6931
5348

klsm128
klsm256
klsm4096
multiq

126
218
4231
376

76
127
2614
1195

250
486
13006
491

144
287
7627
1290

871
1723
16781
969

548
1027
13269
2417

(c) Uniform workload, descending keys.

(d) Split workload, uniform keys (32 bits).

16 threads

32 threads

64 threads

16 threads

32 threads

64 threads

Mean

St.D. Mean

St.D. Mean

St.D.

Mean

St.D. Mean

St.D. Mean

St.D.

klsm128
klsm256
klsm4096
multiq

19
34
435
2342

15
28
400
10594

21
36
453
479

18
30
411
1160

25
41
465
5077

22
35
417
8644

klsm128
klsm256
klsm4096
multiq

381
779
10584
3690

195
389
6209
18381

696
1009
14480
257

376
919
11416
702

1115
1584
27109
1363

626
1211
22042
4570

(e) Split workload, ascending keys.

(f) Split workload, descending keys.

16 threads

32 threads

64 threads

16 threads

32 threads

64 threads

Mean

St.D. Mean

St.D. Mean

St.D.

Mean

St.D. Mean

St.D. Mean

St.D.

klsm128
klsm256
klsm4096
multiq

997
1007
1102
1838

1404
1579
2241
6381

1006
1028
1491
2249

1454
1610
2693
7262

1108
1212
8001
3582

1552
1721
9726
12676

klsm128
klsm256
klsm4096
multiq

33
45
306
1280

32
47
515
5041

63
77
638
1768

55
73
1201
5602

142
242
4137
3421

119
275
5347
12338

(g) Uniform workload, uniform keys (8 bits).

(h) Uniform workload, uniform keys (16 bits).

Table 4: Rank error on ceres.

(a) Uniform workload, uniform keys (32 bits).

(b) Uniform workload, ascending keys.

(c) Uniform workload, descending keys.

(d) Split workload, uniform keys (32 bits).

(e) Split workload, ascending keys.

(f) Split workload, descending keys.

(g) Uniform workload, uniform keys (8 bits).

(h) Uniform workload, uniform keys (16 bits).

Figure 7: Throughput on pluto.

llllllllllllllllllllllllllllllllllllllllllllllllllllll0510050100150200250Number of threadsThroughput in Mops/skernellgloballockklsm128klsm256klsm4096multiqllllllllllllllllllllllllllllllllllllllllllllllllllllll0510050100150200250Number of threadsThroughput in Mops/skernellgloballockklsm128klsm256klsm4096multiqllllllllllllllllllllllllllllllllllllllllllllllllllllllll0204060050100150200Number of threadsThroughput in Mops/skernellgloballockklsm128klsm256klsm4096multiqlllllllllllllllllllllllllllllllllllllllllll0.02.55.07.510.012.5050100150200250Number of threadsThroughput in Mops/skernellgloballockklsm128klsm256klsm4096multiqllllllllllllllllllllllll051015050100150200Number of threadsThroughput in Mops/skernellgloballockklsm128klsm256klsm4096multiqlllllllllllllllll0510050100150200Number of threadsThroughput in Mops/skernellgloballockklsm128klsm256klsm4096multiqllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll04812050100150200250Number of threadsThroughput in Mops/skernellgloballockklsm128klsm256klsm4096multiqllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll04812050100150200250Number of threadsThroughput in Mops/skernellgloballockklsm128klsm256klsm4096multiq(a) mars, uniform keys (32 bits).

(b) mars, ascending keys.

(c) mars, descending keys.

(d) saturn, uniform keys (32 bits).

(e) saturn, ascending keys.

(f) saturn, descending keys.

Figure 8: Throughput with alternating workload.

lllllllllllllllllllllllllllllllllllllllllllllllllll0100200300400050100150Number of threadsThroughput in Mops/skernellgloballockklsm128klsm256klsm4096lindenmultiqspraylllllllllllllllllllllllllllllllllllllllllllllllllll010203040050100150Number of threadsThroughput in Mops/skernellgloballockklsm128klsm256klsm4096lindenmultiqlllllllllllllllllllllllllllllllllllllllllllllllllll0200400600050100150Number of threadsThroughput in Mops/skernellgloballockklsm128klsm256klsm4096lindenmultiqllllllllllllllllllll05010001020304050Number of threadsThroughput in Mops/skernellgloballockklsm128klsm256klsm4096lindenmultiqsprayllllllllllllllllllll0.02.55.07.510.001020304050Number of threadsThroughput in Mops/skernellgloballockklsm128klsm256klsm4096lindenmultiqllllllllllllllllllll05010015020025001020304050Number of threadsThroughput in Mops/skernellgloballockklsm128klsm256klsm4096lindenmultiq(a) ceres, uniform keys (32 bits).

(b) ceres, ascending keys.

(c) ceres, descending keys.

(d) pluto, uniform keys (32 bits).

(e) pluto, ascending keys.

(f) pluto, descending keys.

Figure 9: Throughput with alternating workload.

llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll0501000100200Number of threadsThroughput in Mops/skernellgloballockklsm128klsm256klsm4096multiqllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll0102030400100200Number of threadsThroughput in Mops/skernellgloballockklsm128klsm256klsm4096multiqllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll01002000100200Number of threadsThroughput in Mops/skernellgloballockklsm128klsm256klsm4096multiqllllllllllllllllllllllllllllllllllllllll020406080050100150200Number of threadsThroughput in Mops/skernellgloballockklsm128klsm256klsm4096multiqllllllllllllllllllllllllllllllllllllllll020406080050100150200Number of threadsThroughput in Mops/skernellgloballockklsm128klsm256klsm4096multiqllllllllllllllllllllllllllllllllllllllll020406080050100150200Number of threadsThroughput in Mops/skernellgloballockklsm128klsm256klsm4096multiq20 threads

40 threads

80 threads

20 threads

40 threads

80 threads

Mean

St.D. Mean

St.D. Mean

St.D.

Mean

St.D. Mean

St.D. Mean

St.D.

klsm128
klsm256
klsm4096
multiq

41
25
457
1084

39
30
726
3243

26
43
1628
2422

21
50
1615
8278

443
1377
9486
2890

277
885
13502
7927

klsm128
klsm256
klsm4096
multiq

21
38
495
100

18
33
463
118

22
39
490
202

19
34
464
238

26
42
484
413

22
37
449
490

(a) mars, uniform keys (32 bits).

(b) mars, ascending keys.

20 threads

40 threads

80 threads

12 threads

24 threads

48 threads

Mean

St.D. Mean

St.D. Mean

St.D.

Mean

St.D. Mean

St.D. Mean

St.D.

klsm128
klsm256
klsm4096
multiq

10
11
8
334

7
7
6
1634

24
23
20
674

13
13
15
3640

40
41
41
1194

30
28
36
4935

klsm128
klsm256
klsm4096
multiq

17
31
379
340

20
39
599
839

22
34
396
815

23
39
591
2178

65
115
915
2216

72
165
900
6763

(c) mars, descending keys.

(d) saturn, uniform keys (32 bits).

12 threads

24 threads

48 threads

12 threads

24 threads

48 threads

Mean

St.D. Mean

St.D. Mean

St.D.

Mean

St.D. Mean

St.D. Mean

St.D.

klsm128
klsm256
klsm4096
multiq

20
37
509
60

17
32
478
70

21
38
492
120

18
33
467
142

23
40
849
244

20
35
1402
287

klsm128
klsm256
klsm4096
multiq

6
6
5
182

5
4
4
858

11
11
14
364

8
7
10
1612

24
24
22
766

16
17
19
3619

(e) saturn, ascending keys.

(f) saturn, descending keys.

16 threads

32 threads

64 threads

16 threads

32 threads

64 threads

Mean

St.D. Mean

St.D. Mean

St.D.

Mean

St.D. Mean

St.D. Mean

St.D.

klsm128
klsm256
klsm4096
multiq

19
30
355
1268

21
36
576
4727

30
50
958
1944

32
71
1317
6454

99
151
3602
3433

129
195
6421
12385

klsm128
klsm256
klsm4096
multiq

20
36
521
80

17
31
485
95

22
39
491
163

19
33
461
192

25
42
532
1049

22
37
491
1993

(g) ceres, uniform keys (32 bits).

(h) ceres, ascending keys.

16 threads

32 threads

64 threads

Mean

St.D. Mean

St.D. Mean

St.D.

klsm128
klsm256
klsm4096
multiq

10
9
9
280

6
6
7
1706

16
17
19
551

11
11
15
3118

33
35
37
1071

23
25
29
5497

(i) ceres, descending keys.

Table 5: Rank error with alternating workload.

