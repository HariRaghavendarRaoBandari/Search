Image Co-localization by Mimicking a Good

Detector’s Conﬁdence Score Distribution

Yao Li1,2, Lingqiao Liu1, Chunhua Shen1,3, Anton van den Hengel1,3

1The University of Adelaide

2NICTA

3Australian Centre for Robotic Vision

Abstract. Given a set of images containing objects from the same cat-
egory, the task of image co-localization is to identify and localize each
instance. This paper shows that this problem can be solved by a simple
but intriguing idea, that is, a common object detector can be learnt by
making its detection conﬁdence scores distributed like those of a strongly
supervised detector. More speciﬁcally, we observe that given a set of ob-
ject proposals extracted from an image that contains the object of in-
terest, an accurate strongly supervised object detector should give high
scores to only a small minority of proposals, and low scores to most
of them. Thus, we devise an entropy-based objective function to enforce
the above property when learning the common object detector. Once the
detector is learnt, we resort to a segmentation approach to reﬁne the lo-
calization. We show that despite its simplicity, our approach outperforms
state-of-the-art methods.

Keywords: Image co-localization, detection conﬁdence score distribu-
tion

6
1
0
2

 
r
a

 

M
5
1

 
 
]

V
C
.
s
c
[
 
 

1
v
9
1
6
4
0

.

3
0
6
1
:
v
i
X
r
a

1 Introduction

There has been an explosion of images available on the Internet in recent years,
largely due to the popularity of photo sharing sites like Facebook and Flicker.
However, most of these images are either unlabeled or weakly-labeled. One way of
accessing these images is ﬁnding images depicting the same object, for instance,
Google Image Search will return the images contains the common object de-
scribed by the user input keyword. In this paper, we aim to localize the common
object in the scenario like this, i.e., without using any other forms of supervision
(manually-labeled bounding boxes or negative images). This task is known as
the image co-localization task [30,17,4] in literature.

Image co-localization is a particularly challenging task, and thus there exist
a limited number of comparable methods [30,17,4]. These methods address this
problem from various perspectives. The work in [30] introduces binary latent
variables to indicate the presence of the common object and formulates the co-
localization via latent variable inference. The work of [4], in contrast, localizes
the common object by matching common object parts. Our work diﬀers from
previous approaches in that it directly learns the common object detector by

2

authors running

modeling its detection conﬁdence score distribution on each image, and achieves
the localization with the learned detector.

The key insight in our method is that although we do not have suﬃcient
supervision to learn a strongly supervised object detector, it is still possible to
learn a detector by enforcing its detection conﬁdence scores distributed as those
of a strongly supervised detector. For a strongly supervised object detector,
we have made the following observation: when an accurate strongly supervised
object detector is applied to an image contains the object of interest, only a
small minority of proposals are given high detection conﬁdence scores while
most of them are associated with low scores. Motivated by the key insight and
the above observation, in this paper we design a novel Shannon-entropy-based
objective function to promote the scarcity of high detection conﬁdence scores
within an image while avoiding the trivial solution of producing low scores for
all proposals. In other words, by optimizing the proposed objective, our approach
will encourage the existence of a few high response proposals in each image as the
common object while suppressing responses in the remainder proposals which
will be deemed as background.

To generate the ﬁnal co-localization result, we have also devised a method
for improving the bounding box estimate. Inspired by detection-by-segmentation
approaches (e.g.,[22]), we use the ﬁnal detection heat map

and color information to deﬁne a CRF-based segmentation algorithm, the

output of which indicates the instances of the common object.

Through an extensive evaluation on several benchmark datasets, including
the PASCAL VOC 2007 and 2012 [8], and also some subsets of the ImageNet [6],
we demonstrate that our approach not only outperforms the state-of-the-art in
image co-localization, but is also on par with some weakly supervised object
localization approaches.

2 Related Work

Image co-localization shares some similarities with image co-segmentation [16,26,3]
in the sense that both problems require a set of images of objects from a common
category as input. Instead of generating a precise segmentation of the related
objects in each image, co-localization algorithms [30,17,4] aim to draw a tight
bounding box around the object. Image co-localization is also related to works
on weakly supervised object localization (WSOL) [29,7,5,28,32,1,33,24] as both
try to localize objects of the same type within an image set, the key diﬀerence is
WSOL requires manually-labeled negative images whereas co-localization does
not.

Tang et al. [30] formulate co-localization as a boolean constrained quadratic
program which can be relaxed to a convex problem, which is further accelerated
by the Frank-Wolfe Algorithm [17]. Recently, Cho et al. [4] propose a Prob-
abilistic Hough Matching algorithm to match object proposals across images
and then dominant objects are localized by selecting proposals based on match-
ing scores. There are also approaches address the problem of co-localization in

title running

3

Fig. 1. An overview of our image co-localization framework. (a) The input of our
system is a set of images contains a common object category (here, aeroplane). (b) The
common object detector is learnt by modeling the distribution of detection conﬁdence
scores. (c) Detection heat maps generated by the learnt detector are used as the unary
potential for graph-cuts segmentation. (d) The output for each image is the smallest
rectangle which covers the corresponding segmentation.

video [23,17,21]. Notably, Prest et al. [23] select spatio-temporal tubes which
are likely to contain the common object, and Joulin et al. [17], in contrast,
extend [30] by incorporating temporal consistency.

However, in this paper, we tackle the co-localization problem from a new per-
spective, that is, learning the common object detector by modeling its detection
conﬁdence score distribution, and thus get rid of the need of manually-labeled
negative images. An advantage of the proposed approach for learning common
object detectors is that it provides an explicit mechanism by which to exploit the
relationship between localization and detection. The beneﬁts of exploiting this
relationship have been identiﬁed before in WSOL. In [7], objects are localized
by minimizing a Conditional Random Field (CRF) energy function which incor-
porates class-speciﬁc information, and the class-speciﬁc information is learned
from the localized objects. Cinbis et al. [5] propose a multi-fold training proce-
dure for Multiple Instance Learning whereby, at each iteration, positive instances
in each fold are localized by a detector trained from other folds in the previous
iteration. The approach that we propose here, however, is the ﬁrst to system-
atically leverage the idea of jointly performing object detection and localization
for co-localizing common objects in images.

3 Approach

We give an overview of our image co-localization framework in Fig. 1. The input
to our framework is a set of N images I = {I1, I2, . . . , IN} contains one common
object (e.g., aeroplane), and we aim to annotate the location of common object

initial detection heat mapﬁnal detection heat map (b) learning object detectors without negative images   (d) output(c) segmentation   (a) input4

authors running

instances in each image. Inspired the behaviour of an accurate strongly super-
vised object detector (Sec. 3.1), the core of our framework is the procedure of
learning the common object detector by modeling its detection conﬁdence score
distribution (Sec. 3.2). We further formulate object localization as a segmenta-
tion problem (Sec. 3.3), which involves using the detection heat map to deﬁne
unary potentials of a binary energy function and solve it eﬃciently by standard
graph-cuts.

conﬁdence

Fig. 2. (a) Predicted ob-
jects by Fast R-CNN [10].
(b) Normalized detec-
tion
score
histogram of object pro-
posals in (a). We observe
the same statistics
for
most images.

(a)

(b)

3.1 The behaviour of an accurate strongly supervised detector

Object proposals [31,34], which are image regions that are likely to contain ob-
jects, have been widely used in state-of-the-art object detection approaches [11,12,10].
In this section we are interested in the statistics of proposal detection conﬁdence
scores on an image generated by a strongly supervised detector. The observation
here motivates our formulation for learning common object detectors in Sec. 3.2.
More speciﬁcally, we apply one state-of-the-art strongly supervised object
detector Fast R-CNN [10] (trained on PASCAL VOC 2007 trainval set [8]) to
a PASCAL VOC 2007 test image which contains the object of interest (Fig. 2
(a)). After obtaining the detection conﬁdence scores of the more than 2000 object
proposals [31] extracted from this image, we calculate the normalized histogram
of detection conﬁdence scores of all proposals (Fig. 2 (b)).

From Fig. 2 (b) it is clear that, although there are multiple instances of
the object of interest (“car” in this case), more than 90% of object proposals
have a very low detection conﬁdence score (less the 0.05), which indicates that a
dominantly large portion of proposals are likely to cover image regions that do
not cover the object of interest tightly. This is understandable as object proposal
generation is a pre-processing step in object detection systems, where recall rate
much more important than precision (not missing any objects of interest is more
important than generating less false positives).

3.2 Learning detectors by modeling detection conﬁdence score

distribution

In the setting of image co-localization, although all we know is that there exists a
common object category across images, we still aim to learn the common object

detection score00.20.40.60.81percentage of proposals00.20.40.60.81title running

5

detector. This is possible by modeling the distribution of proposals detection
conﬁdence scores. More speciﬁcally, in our method the common object detector
will be learned by enforcing its the distribution of detection conﬁdence scores to
mimic that of an accurate strongly supervised detector (Sec. 3.1).
Formally, for each image Ii ∈ I, we ﬁrst extract a set of object proposals
Bi = {Bi,1, Bi,2, . . . , Bi,Mi} using EdgeBox [34], the performance of which has
been illustrated in a recent review [14]. Let φ(Bi,j) ∈ RK denote the feature
representation of proposal Bi,j ∈ Bi. The particular detection conﬁdence scores
that we use are formulated as follows

si,j = f (wT φ(Bi,j) + b),

(1)
where w ∈ RK, b ∈ R1 denote weight and bias terms of the detector respectively,
and f (·) is the softplus function which has the form f (x) = ln(1 + exp(x)).
Irrespective of the form of the detector, we can construct the set of detec-
tion conﬁdence scores Si = {si,1, si,2, . . . , si,Mi} over all the proposals Bi of
(cid:80)
si,j +
j (si,j +) , where the parameter 
image Ii, and normalized them as pi,j =
is a small constant. If the detector in Eq. (1) is trained with strong supervi-
sion, according to the observation in Sec. 3.1, most of its detection conﬁdence
scores in Si should have near-zero values which means that the score vector
si = [si,1, si,2,··· , si,Mi]T and its normalized version pi = [pi,1, pi,2,··· , pi,Mi]T
should be sparse vectors. Note that when all proposals have zero detection conﬁ-
dence scores, si will be sparse but pi will be dense due to the eﬀect of the constant
. Thus, our method will be based on pi because enforcing its sparsity will be
equivalent to requiring the detector to have few high detection conﬁdence scores
and many low (zero) detection conﬁdence scores, in other words, the detection
conﬁdence score distribution will mimic that of an accurate strongly supervised
detector.

Objective function. To measure the sparsity of the normalized detection con-
ﬁdence score vector pi, we utilize the Shannon entropy as a sparsity indicator,
that is,

pi,j log pi,j,

(2)

L(pi) = − Mi(cid:88)
N(cid:88)

j=1

min
w,b

1
N

N(cid:88)

M(cid:88)

i=1

j=1

and the objective for learning the common object detector is formulated as
follows:

L(pi) + λ||w||2
2,

i=1

(3)

where we use the square of the L2-norm of w as a regularizer on the weight
vector.

So the optimal value of the weight and bias of the detector is given by:

w∗, b∗ = argmin

w,b

− 1
N

pi,j log pi,j + λ||w||2
2.

(4)

6

authors running

Fig. 3. Our detectors ﬁre at diﬀerent common visual patterns (denoted by red and
green bounding boxes) by minimizing Eq. (4) with diﬀerent random initializations.
Although these common visual patterns may not be suitable for the co-localization
task, they may be useful for other computer vision tasks, such as discovering common
object parts for ﬁne-grained image classiﬁcation [18].

Note that Eq. (4) does not involve a set of manually-labeled negative images,
but rather describes the desired form of the detection conﬁdence score distribu-
tion. The learning process also implicitly takes advantage of the chicken-and-egg
relationship between object localization and detection: precisely localized object
instances are critical for training a good object detector, and objects can be
localized more precisely by a well-trained detector.

Optimization. As our objective function in Eq. (4) is non-convex, we mini-
mize it using stochastic gradient descent (SGD). Similar to the approach used
in training a Convolutional Neural Network [19], we divide all data (i.e., object
proposals) into mini-batches. We initialize the weight vector w from a zero-mean
Gaussian distribution, while the bias term b is set to zero initially. During train-
ing we divide the learning rate (which is set to 0.1 initially) by 10 after each 10
epochs. We stop learning after 20 epochs when the objective function converges.

Modiﬁcation. After minimizing Eq. (4), when we visualize the proposal with
the maximal detection conﬁdence score for each image (Fig. 3), it is interesting
to note that the learnt detector may not ﬁre at the common object but some
common visual patterns (e.g., common object parts, common object with some
context) instead. Also, the discovered common visual patterns can be very diﬀer-
ent if the initialization of our detector varies (diﬀerent local minimums). However
in this work, as we aim to co-localize the common object, we reformulate Eq.
(1) by incorporating the “objectness” score oi,j (outputs of Edgebox) of each
proposal Bi,j as a weight to favour proposals with high objectness score (which
more likely to cover a whole object tightly)

si,j = oi,jf (wT φ(Bi,j) + b).

(5)

We experimentally ﬁnd that minimizing Eq. (4) using si,j deﬁned in Eq. (5)
gives a stable solution regardless of initialization.

title running

7

Localizing the common object. The optimal w and b, inserted into Eq. (1),
lead to a mechanism for determining the detection conﬁdence scores for all object
proposals. The nature of the co-localization problem means that the maximal
score for each image indicates the desired detection. This method is used as a
baseline in the Experiments section (Sec. 4.1).

Discussion. Theoretically, other sparsity measures could be employed to replace
the Shannon entropy. Note that the commonly used L1 norm cannot be applied
here because (cid:107)pi(cid:107)1 = 1. One possible way to use L1 norm is to redeﬁne the
normalization score pi,j =

si,j +

√(cid:80)

j (si,j +)2 .

3.3 Reﬁning the bounding box estimate

The quality of the detections generated through the above described process
depends entirely on the quality of object proposals. To overcome this dependency,
and enable better ﬁnal bounding box estimates to be achieved, we have developed
a bounding box reﬁnement process as follows.

Given the optimal w∗ and b∗ identiﬁed by minimizing Eq. (4), we generate
the detection heat map as follows. For each pixel in the image, we add up the
weighted detection conﬁdence score si,j from Eq. (5) for all proposals Bi,j that
cover this pixel (zero for pixels not covered by any proposals). The values are
then normalized to the interval [0, 1]. This gives rise to a set of detection heat
maps H = {H1, H2, . . . , Hn}. Some examples are illustrated in Fig. 4.
Given the set of detection heat maps H, we aim to produce a segmentation
of the entire object. This approach is inspired by previous work which casts
localization as a segmentation problem (e.g., [22]).
Formally, we formulate the segmentation problem as a standard graph-cut
problem. We ﬁrst extract superpixels [9] to construct the vertex set {m} and
aim to label each superpixel as foreground (ym = 1) or background (ym = 0).
Mathematically, the energy function is given by

(cid:88)

m

(cid:88)

(m,n)∈E

E(y) =

um(ym) +

vmn(ym, yn),

(6)

where um and vmn are the unary and pairwise potential respectively. E is the
set of edges connecting superpixels1.

Unary potential um. Inspired by [20], the unary potential is the novel part of
our segmentation framework, which carries information from the detection heat
map H:

up(ym) = − log Am(ym),

(7)

1 In our case two superpixels are connected if the distance between their centroids is

smaller than the sum of their major axis length.

8

authors running

Fig. 4. Examples of our co-localization process. From top to bottom: input images
(predicted boxes in red), detection heat maps, segmentation results.

where Am is the prior information from the detection heat map H:

Ap(ym = 1) = H(m),
Ap(ym = 0) = 1 − H(m),

where H(m) is the mean of values inside superpixel m on map H.

Pairwise potential vmn. Our pairwise potential is deﬁned as follows.

vmn(ym, yn) = [ym (cid:54)= yn]e−β||C(m)−C(n)||2
2,

(8)

(9)

where C(m) is the color histogram feature. As in [25,20], this potential penalizes
superpixels with diﬀerent colors taking the same label.

As our pairwise potential in Eq. (9) is submodular, the optimal label y∗ can
be found eﬃciently by the graph-cuts [2]. As shown in Fig. 4, the segmentation
derived through this approach is accurate. The ﬁnal bounding box estimate is
then calculated as the smallest rectangle which covers the segmentation.

4 Experiments

Datasets. We evaluate our approach on three datasets, including PASCAL VOC
2007 and 2012 [8] datasets, six subsets of the ImageNet dataset [6] which have
not been used in the ILSVRC [27]2. For PASCAL VOC datasets, following previ-
ous works in co-localization and weakly supervised object localization [5,4,1,33],
we use all images on the trainval set discarding images that only contain object
instances marked as “diﬃcult” or “truncate”. For ImageNet subsets, we ﬁlter
images with very large ground-truth bounding boxes.

2 The six categories are chipmunk, rhino, stoat, racoon, rake and wheelchair. Bounding

box annotations are available for these categories.

title running

9

Fig. 5. CorLoc scores of our approaches, and baselines, on the PASCAL VOC 2007
dataset.

Evaluation metric. We use two metrics to evaluate our approach. Firstly, for
comparison with state-of-the-art approaches, we use the CorLoc metric [7], which
is deﬁned as the percentage of images that are correctly localized. An image is
considered as correctly localized if the IoU score between the predicted bounding
box and any ground-truth bounding boxes of the object of interest exceeds 50%.
Instead of using a ﬁxed threshold (50%) for CorLoc metric, we also compute
percentages of correctly localized images under a wide range of thresholds from
0 to 1 which results in a CorLoc curve.

Implementation details. We use Edgebox [34] to extract object proposals
with a maximum of 2000 proposals extracted from each image. We represent
each Edgebox proposal as a 4096-dimensional CNN feature from the f c6 layer
(after ReLU) from the BVLC Reference CaﬀeNet model [15]. We use a ﬁxed
value of 1 for λ in Eq. (4) which controls the tradeoﬀ between the loss function
and regularizer. The value of β in Eq.(9) is set to 10.

4.1 Ablation study

Baselines. To investigate the impact of the various elements of the proposed
approach, we consider the following two baseline methods:

– “obj-sel”: the predicted bounding box for an image is simply the proposal

with maximum objectness score.

– “obj-seg”: for each image, objectness scores of all proposals are treated as
detection conﬁdence scores to generate a fake detection heat map, which is
then sent to our segmentation model in Sec. 3.3.

The two methods proposed in our work are:

– “our-sel”: given the learnt detector 3.2, we simply select the object pro-
i =

posal which has the maximum detection conﬁdence score pi,j i.e., B∗
argmaxBi,j∈Bi si,j.

– “our-seg”: combination of detector training 3.2 and the segmentation reﬁne-

ment 3.3.

00.20.40.60.8aeroplanebicyclebirdboatbottlebuscarcatchaircowdiningtabledoghorsemotorbikepersonpottedplantsheepsofatraintvmonitoraverageobj-selobj-segour-selour-seg10

authors running

Corloc scores for the above four methods on the PASCAL VOC 2007 dataset
are illustrated in Fig. 5.

As shown in Fig. 5, the simplest baseline “obj-sel” does not work well (19.8%
CorLoc). This is because the objectness measure of Edgebox [34] is heuristically
deﬁned based on only edge information. Therefore, taking the proposal with
maximum objectness score is certainly not the optimal method.

However, the “obj-seg” baseline in which we use objectness scores to generate
a detection heat map for each image, performs quite well, with CorLoc increasing
to 24.7%. Surprisingly, this performance is on the par with one state-of-the-art
image co-localization approach [17] (24.7% vs.24.6%), even though there is no
common object assumption. This phenomenon indicates that our segmentation
model is quite eﬀective.

Thanks to the proposed common object detector learning procedure in Sec. 3.2,

“our-sel” achieves a performance of 36.5%, outperforming “obj-sel” and “obj-
seg” by over 16% and 11% respectively. This veriﬁes the eﬀectiveness of this pro-
cedure, and particularly that, although we do not have annotated image labels
nor bounding boxes, the detector still captures the appearance of the common
object, which improves co-localization signiﬁcantly.

Combing the advantages of the common object detector learning procedure
(Sec. 3.2) and segmentation reﬁnement (Sec. 3.3), we observe another 3.5% boost
in the case of “our-seg”, reaching 40.0% Corloc. Thus we use “our-seg” to com-
pare with state-of-the-art approaches.

To further demonstrate the performance of the proposed approaches, we plot
the CorLoc curves in which the IoU threshold ranges from 0 to 1. Fig. 6 illustrates
CorLoc curves of the four methods. Obviously, the proposed approach “our-seg”
bypasses all other baselines in a wide range of IoU thresholds.

Fig. 6. CorLoc curves for the pro-
posed approaches, and baselines, on
the PASCAL VOC 2007 dataset.

Number of candidate proposals. To test the robustness of our method under
diﬀerent number of candidate object proposals, we select three settings—500,
1000 and 2000. As shown in Table. 1, our method is quite insensitive to the
changes in number of candidate proposals.

IoU threshold00.20.40.60.81CorLoc00.20.40.60.81obj-selobj-segour-selour-segTable 1. CorLoc for diﬀerent number of object proposals on PASCAL VOC 2007.

title running

11

# proposals 500 1000 2000
39.2 39.6 40.0
CorLoc

Fig. 7. An illustra-
tion of error types for
our method on the
PASCAL VOC 2007
dataset.

4.2 Diagnosing the localization error

In order to better understand the localization errors, following [13,5], each pre-
dicted bounding box predicted by the proposed approach is categorized into
the following ﬁve cases: (1) correct: IoU score exceeds 50%, (2) g.t. in hypothe-
sis: ground-truth completely inside prediction, (3) hypothesis in g.t.: prediction
completely inside ground-truth, (4) no overlap: IoU score equals zero, (5) low
overlap: none of the above four cases. In Fig. 7 we show the error modes of our
approach across all categories on the PASCAL VOC 2007 dataset.

As shown in Fig. 7, the fraction of “no overlap” cases is quite small (3.5%)
across all categories, which means our approach can localize common objects to
some extent in most cases. Comparing “g.t. in hypothesis” to its “hypothesis
in g.t.”, it is clear that the former appears more frequently (19.9% v.s. 3.1%),
which means our approach tends to localize objects with some context details.
In terms of correct localization, the three categories with lowest CorLoc values
are bottle (6.8%), chair (6.2%) and pottedplant (12.8%). Images of objects in
these categories are always in very clustered environments with occlusion (e.g.,
chair is often occluded by table) and large appearance changes.

4.3 Comparison to state-of-the-art approaches

Comparison to image co-localization approaches. We now compare the
results of the proposed approach to the state-of-the-art image co-localization
approaches of Joulin et al. [17] and Cho et al. [4] on the PASCAL VOC 2007
dataset (Table 2). The performance of the proposed method exceeds that of
Joulin et al. [17] signiﬁcantly in most categories, with an improvement of over
15% in mean CorLoc. The recent method of Cho et al. [4] relies on matching
object parts by Hongh Transform with the predicted bounding box is selected by
a heuristic standout score. Candidate regions are object proposals represented by

00.20.40.60.81aeroplanebicyclebirdboatbottlebuscarcatchaircowdiningtabledoghorsemotorbikepersonpottedplantsheepsofatraintvmonitoraveragecorrectg.t. in hypothesishypothesis in g.t.low overlapno overlap12

authors running

Table 2. Comparison to image co-localization approaches on the PASCAL VOC 2007
dataset in terms of CorLoc metric [7].

cat chair cow table dog horse mbike person plant sheep sofa train tv mean
VOC aero bike bird boat bottle bus
24.6
[17] 32.8 17.3 20.9 18.2
30.9 34.0 61.6 31.5 36.6
[4]
50.3 42.8 30.0 18.5
Ours 73.1 45.0 43.4 27.7 6.8 53.3 58.3 45.0 6.2 48.0 14.3 47.3 69.4 66.8 24.3 12.8 51.5 25.5 65.2 16.8 40.0

26.9 32.7 41.0 5.8 29.1 34.5 31.6 26.1
4.5
4.0 62.3 64.5 42.5 8.6 49.0 12.2 44.0 64.1

11.8 25.0 27.5 35.6 12.1
9.4

40.4
57.2

17.9
15.3

car

Table 3. Comparison to image co-localization approaches on the PASCAL VOC 2012
dataset in terms of CorLoc metric [7].

VOC aero bike bird boat bottle bus
cat chair cow table dog horse mbike person plant sheep sofa train tv mean
11.7 51.4 45.3 64.6 39.2 41.8
[4]
Ours 65.7 57.8 47.9 28.9 6.0 74.9 48.4 48.4 14.6 54.4 23.9 50.2 69.9 68.4 24.0 14.2 52.7 30.9 72.4 21.6 43.8

5.0 81.1 54.6 50.9 18.2 54.0 31.2 44.9 61.8

57.0 41.2 36.0 26.9

48.0

car

13.0

whitened HOG features. However, we found that this whitening process, whose
mean vector and covariance matrix are estimated from the random sampled
images from the same dataset (inevitably using images from other categories),
is crucial for the performance of their algorithm. Our performance bypasses that
of [4] by a reasonable margin of 3.4%.

To further verify the eﬀectiveness of the proposed approach, we now present
an evaluation on the PASCAL VOC 2012 dataset [8] which has twice the num-
ber of images of PASCAL VOC 2007. Table 3 shows our performance along with
that of Cho et al. [4] which we evaluated using their publicly available code. It
is clear that on average our method outperforms that of Cho et al. [4] by 2%.

Table 4. Comparison to weakly supervised object localization approaches on the PAS-
CAL VOC 2007 dataset in terms of CorLoc metric [7]. Note that these comparators
require access to a negative image set, whereas our method does not.

car

VOC aero bike bird boat bottle bus
[29]
[28]
[5]
[33]
[1]
[24]
[32]
Ours 73.1 45.0 43.4 27.7 6.8

40.9 73.2 44.8 5.4 30.5 19.0 34.0 48.8
42.4 46.5 18.2 8.8
16.7 32.3 54.8 5.5
9.4
46.6 60.7 68.9 2.5 32.4 16.2 58.9 51.5
67.3 54.4 34.3 17.8
3.1
20.9 34.7 63.4 5.9
54.9 69.1 20.8 9.2 50.5 10.2 29.0 58.0
56.6 58.3 28.4 20.7
18.7 56.5 13.2 54.9 59.4
48.4 70.0 63.7 9.0 54.2 33.3 37.4 61.6
31.7 32.4 52.8 49.0 27.8
37.7 58.8 39.0 4.7
66.4 59.3 42.7 20.4 21.3 63.4 74.3 59.6 21.1 58.2 14.0 38.5 49.5
39.2 41.7 30.1 50.2 44.1
79.2 56.9 46.0 12.2 15.7 58.4 71.4 48.6 7.2 69.9 16.7 47.4 44.2 75.5 41.2 39.6 47.4 32.2 49.8 18.6
80.1 63.9 51.5 14.9 21.0 55.7 74.2 43.5 26.2 53.4 16.3 56.7 58.3

cat chair cow table dog horse mbike person plant sheep sofa train tv mean
30.4
36.2
38.8
40.2
43.7
43.9
38.3 58.8 47.2 49.1 60.9 48.5
12.8 51.5 25.5 65.2 16.8
40.0

69.5
53.3 58.3 45.0 6.2 48.0 14.3 47.3 69.4 66.8

65.3
64.6
64.9
57.6
60.0

8.2
18.2
36.7
30.1
19.8

2.9
1.3
6.8
4.0

14.1
24.3

Comparison to weakly supervised object localization approaches. We
also compare the proposed approach with some state-of-the-art approaches on
weakly supervised object localization. Table 4 illustrates the comparison of sev-
eral recent works and our approach on PASCAL VOC 2007 dataset. In particular,
our performance (40.0%) is comparable to that of a very recent work [33](40.2%)
which also uses CNN features and Edgebox proposals. Please note that in our
framework we do not have any negative images whereas WSL approaches do,
which means the we are addressing a more challenging problem. As shown in
Table 4, though we do not have any negative images, we still outperforms WSOL
approaches on 3 of 20 categories.

title running

13

Fig. 8. Examples of successful co-localization results for the PSACAL VOC 2007
dataset. For each category, the top row depicts predicted bounding boxes on the original
image, the bottom row shows corresponding detection heat maps.

Table 5. Comparison to image co-localization approaches on the ImageNet subsets
in terms of CorLoc metric [7]. Note that these categories have not been used for pre-
training the CNN model, which is used as a feature extractor in this work.

ImageNet
Cho et al. [4]
Ours

26.6
44.9

30.1

chipmunk rhino stoat racoon rake wheelchair mean
37.7
48.3

81.8 44.2
8.3
81.8 67.3 41.8 14.5

35.3
39.3

Visualization. In Fig. 8, we provide a set of successful co-localization results
along with the corresponding detection heat maps for some categories of the
PASCAL VOC 2007 dataset. It demonstrates that detection heat maps success-
fully predict the correct location of the common object regardless of changes
in scale, appearance and viewpoint. This provides a strong indication that, al-
though trained without annotated positive or negative examples, our method is
able to discriminate the common object from other objects in the scene.

4.4

ImageNet subsets

We note that the CNN model used for extracting features is pre-trained in the
ILSVRC [27], whose training set may have some overlapping categories with the
PASCAL VOC datasets. In order to justify the proposed method is insensitive to
the category of the object used, we select six subsets of the ImageNet dataset [6]
which have not been used in the ILSVRC (thus “unseen” by the CNN model)
for evaluation.

14

authors running

Fig. 9. Examples of successful co-localization results for the ImageNet subsets.

Fig. 10. Examples of failure cases (IoU< 0.5) of rake and wheelchair (ground truth in
megenta and incorrectly predicted boxes in yellow).

Table 5 shows our co-localization result along with that of the current state-
of-the-art work of Cho et al. [4]. Clearly, the proposed approach outperforms [4]
by a reasonable margin on all categories except the rhino category, whose images
tend to have relatively large common instances and less cluttered background.
Some successfully co-localization samples are depicted in Fig. 9.

We also visualize some failure cases of the two categories our approach per-
formed worst—rake and wheelchair (See Fig. 10). Interestingly, these failure
cases are quite understandable. For example, a large portion of images in the
rake category have the scenario in which people are holding a rake, thus our co-
localization approach tends to capture this scenario as the “common object”. A
similar phenomenon is also observed in the wheelchair category in which people
tend to sit on the wheelchair.

5 Conclusion

We have addressed the image co-localization problem by directly learning a com-
mon object detector. The key discovery made in this paper is that this detector
can be learned with the objective of making its detection score distribution mimic
an accurate strongly supervised object detector. Also, we have illustrated that
it is proﬁtable to use a CRF model to reﬁne the co-localization result, which has
not been explored in recent works on co-localization.

title running

15

References

1. Bilen, H., Pedersoli, M., Tuytelaars, T.: Weakly supervised object detection with
convex clustering. In: Proc. IEEE Conf. Comp. Vis. Patt. Recogn. pp. 1081–1089
(2015)

2. Boykov, Y., Veksler, O., Zabih, R.: Fast approximate energy minimization via

graph cuts. IEEE Trans. Pattern Anal. Mach. Intell. 23(11), 1222–1239 (2001)

3. Chen, X., Shrivastava, A., Gupta, A.: Enriching visual knowledge bases via object
discovery and segmentation. In: Proc. IEEE Conf. Comp. Vis. Patt. Recogn. pp.
2035–2042 (2014)

4. Cho, M., Kwak, S., Schmid, C., Ponce, J.: Unsupervised object discovery and
localization in the wild: Part-based matching with bottom-up region proposals. In:
Proc. IEEE Conf. Comp. Vis. Patt. Recogn. pp. 1201–1210 (2015)

5. Cinbis, R.G., Verbeek, J.J., Schmid, C.: Multi-fold MIL training for weakly su-
pervised object localization. In: Proc. IEEE Conf. Comp. Vis. Patt. Recogn. pp.
2409–2416 (2014)

6. Deng, J., Dong, W., Socher, R., Li, L., Li, K., Li, F.: Imagenet: A large-scale
hierarchical image database. In: Proc. IEEE Conf. Comp. Vis. Patt. Recogn. pp.
248–255 (2009)

7. Deselaers, T., Alexe, B., Ferrari, V.: Weakly supervised localization and learning

with generic knowledge. Int. J. Comp. Vis. 100(3), 275–293 (2012)

8. Everingham, M., Eslami, S.M.A., Gool, L.V., Williams, C.K.I., Winn, J.M., Zisser-
man, A.: The pascal visual object classes challenge: A retrospective. Int. J. Comp.
Vis. 111(1), 98–136 (2015)

9. Felzenszwalb, P.F., Huttenlocher, D.P.: Eﬃcient graph-based image segmentation.

Int. J. Comp. Vis. 59(2), 167–181 (2004)

10. Girshick, R.: Fast r-cnn. In: Proc. IEEE Int. Conf. Comp. Vis. pp. 1440–1448

(2015)

11. Girshick, R., Donahue, J., Darrell, T., Malik, J.: Region-based convolutional net-
works for accurate object detection and segmentation. IEEE Trans. Pattern Anal.
Mach. Intell. (2015)

12. He, K., Zhang, X., Ren, S., Sun, J.: Spatial pyramid pooling in deep convolutional
networks for visual recognition. IEEE Trans. Pattern Anal. Mach. Intell. 37(9),
1904–1916 (2015)

13. Hoiem, D., Chodpathumwan, Y., Dai, Q.: Diagnosing error in object detectors. In:

Proc. Eur. Conf. Comp. Vis. pp. 340–353 (2012)

14. Hosang, J.H., Benenson, R., Doll´ar, P., Schiele, B.: What makes for eﬀective de-
tection proposals? IEEE Trans. Pattern Anal. Mach. Intell. 38(4), 814–830 (2016)
15. Jia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J., Girshick, R., Guadar-
rama, S., Darrell, T.: Caﬀe: Convolutional architecture for fast feature embedding.
arXiv preprint arXiv:1408.5093 (2014)

16. Joulin, A., Bach, F.R., Ponce, J.: Discriminative clustering for image co-
segmentation. In: Proc. IEEE Conf. Comp. Vis. Patt. Recogn. pp. 1943–1950 (2010)
17. Joulin, A., Tang, K., Li, F.: Eﬃcient image and video co-localization with frank-

wolfe algorithm. In: Proc. Eur. Conf. Comp. Vis. pp. 253–268 (2014)

18. Krause, J., Jin, H., Yang, J., Li, F.: Fine-grained recognition without part anno-

tations. In: Proc. IEEE Conf. Comp. Vis. Patt. Recogn. pp. 5546–5555 (2015)

19. Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classiﬁcation with deep con-
volutional neural networks. In: Proc. Adv. Neural Inf. Process. Syst. pp. 1106–1114
(2012)

16

authors running

20. K¨uttel, D., Ferrari, V.: Figure-ground segmentation by transferring window masks.

In: Proc. IEEE Conf. Comp. Vis. Patt. Recogn. pp. 558–565 (2012)

21. Kwak, S., Cho, M., Ponce, J., Schmid, C., Laptev, I.: Unsupervised object discovery
and tracking in video collections. In: Proc. IEEE Int. Conf. Comp. Vis. pp. 3173–
3181 (2015)

22. Parkhi, O.M., Vedaldi, A., Jawahar, C.V., Zisserman, A.: The truth about cats

and dogs. In: Proc. IEEE Int. Conf. Comp. Vis. pp. 1427–1434 (2011)

23. Prest, A., Leistner, C., Civera, J., Schmid, C., Ferrari, V.: Learning object class
detectors from weakly annotated video. In: Proc. IEEE Conf. Comp. Vis. Patt.
Recogn. pp. 3282–3289 (2012)

24. Ren, W., Huang, K., Tao, D., Tan, T.: Weakly supervised large scale object lo-
calization with multiple instance learning and bag splitting. IEEE Trans. Pattern
Anal. Mach. Intell. 38(2), 405–416 (2016)

25. Rother, C., Kolmogorov, V., Blake, A.: Grabcut: interactive foreground extraction

using iterated graph cuts. ACM Trans. Graph. 23(3), 309–314 (2004)

26. Rubinstein, M., Joulin, A., Kopf, J., Liu, C.: Unsupervised joint object discov-
ery and segmentation in internet images. In: Proc. IEEE Conf. Comp. Vis. Patt.
Recogn. pp. 1939–1946 (2013)

27. Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z.,
Karpathy, A., Khosla, A., Bernstein, M.S., Berg, A.C., Li, F.: Imagenet large scale
visual recognition challenge. Int. J. Comp. Vis. 115(3), 211–252 (2015)

28. Shi, Z., Hospedales, T.M., Xiang, T.: Bayesian joint topic modelling for weakly
supervised object localisation. In: Proc. IEEE Int. Conf. Comp. Vis. pp. 2984–
2991 (2013)

29. Siva, P., Xiang, T.: Weakly supervised object detector learning with model drift

detection. In: Proc. IEEE Int. Conf. Comp. Vis. pp. 343–350 (2011)

30. Tang, K., Joulin, A., Li, L., Li, F.: Co-localization in real-world images. In: Proc.

IEEE Conf. Comp. Vis. Patt. Recogn. pp. 1464–1471 (2014)

31. Uijlings, J.R.R., van de Sande, K.E.A., Gevers, T., Smeulders, A.W.M.: Selective

search for object recognition. Int. J. Comp. Vis. 104(2), 154–171 (2013)

32. Wang, C., Ren, W., Huang, K., Tan, T.: Weakly supervised object localization
with latent category learning. In: Proc. Eur. Conf. Comp. Vis. pp. 431–445 (2014)
33. Wang, X., Zhu, Z., Yao, C., Bai, X.: Relaxed multiple-instance svm with application

to object discovery. In: Proc. IEEE Int. Conf. Comp. Vis. pp. 1224–1232 (2015)

34. Zitnick, C.L., Doll´ar, P.: Edge boxes: Locating object proposals from edges. In:

Proc. Eur. Conf. Comp. Vis. pp. 391–405 (2014)

