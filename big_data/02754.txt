XGBoost: A Scalable Tree Boosting System

Tianqi Chen

University of Washington

tqchen@cs.washington.edu

Carlos Guestrin

University of Washington

guestrin@cs.washington.edu

6
1
0
2

 
r
a

M
9

 

 
 
]

G
L
.
s
c
[
 
 

1
v
4
5
7
2
0

.

3
0
6
1
:
v
i
X
r
a

ABSTRACT
Tree boosting is a highly eﬀective and widely used machine
learning method. In this paper, we describe a scalable end-
to-end tree boosting system called XGBoost, which is used
widely by data scientists to achieve state-of-the-art results
on many machine learning challenges. We propose a novel
sparsity-aware algorithm for sparse data and weighted quan-
tile sketch for approximate tree learning. More importantly,
we provide insights on cache access patterns, data compres-
sion and sharding to build a scalable tree boosting system.
By combining these insights, XGBoost scales beyond billions
of examples using far fewer resources than existing systems.

CCS Concepts
•Methodologies → Machine learning; •Information
systems → Data mining;

Keywords
Large-scale Machine Learning

1.

INTRODUCTION

Machine learning and data-driven approaches are becom-
ing very important in many areas. Smart spam classiﬁers
protect our email by learning from massive amounts of spam
data and user feedback; advertising systems learn to match
the right ads with the right context; fraud detection systems
protect banks from malicious attackers; anomaly event de-
tection systems help experimental physicists to ﬁnd events
that lead to new physics. There are two important factors
that drive these successful applications: usage of eﬀective
(statistical) models that capture the complex data depen-
dencies and scalable learning systems that learn the model
of interest from large datasets.

Among the machine learning methods used in practice,
gradient tree boosting [9]1 is one technique that shines in

1Gradient tree boosting is also known as gradient boosting
machine (GBM) or gradient boosted regression tree (GBRT)

Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).

c(cid:13) 2016 Copyright held by the owner/author(s).
ACM ISBN .
DOI:

many applications. Tree boosting has been shown to give
state-of-the-art results on many standard classiﬁcation bench-
marks [14]. LambdaMART [4], a variant of tree boosting for
ranking, achieves state-of-the-art result for ranking prob-
lems. Besides being used as a stand-alone predictor, it is
also incorporated into real-world production pipelines for ad
click through rate prediction [13]. Finally, it is the de-facto
choice of ensemble method and is used in challenges such as
the Netﬂix prize [2].

In this paper, we describe XGBoost, a scalable machine
learning system for tree boosting. The system is available as
an open source package2. The impact of the system has been
widely recognized in a number of machine learning and data
mining challenges. Take the challenges hosted by the ma-
chine learning competition site Kaggle for example. Among
the 29 challenge winning solutions 3 published at Kaggle’s
blog during 2015, 17 solutions used XGBoost. Among these
solutions, eight solely used XGBoost to train the model,
while most others combined XGBoost with neural nets in en-
sembles. For comparison, the second most popular method,
deep neural nets, was used in 11 solutions. The success
of the system was also witnessed in KDDCup 2015, where
XGBoost was used by every winning team in the top-10.
Moreover, the winning teams reported that ensemble meth-
ods outperform a well-conﬁgured XGBoost by only a small
amount [1].

These results demonstrate that our system gives state-of-
the-art results on a wide range of problems. Examples of
the problems in these winning solutions include: store sales
prediction; high energy physics event classiﬁcation; web text
classiﬁcation; customer behavior prediction; motion detec-
tion; ad click through rate prediction; malware classiﬁcation;
product categorization; hazard risk prediction; massive on-
line course dropout rate prediction. While domain depen-
dent data analysis and feature engineering play an important
role in these solutions, the fact that XGBoost is the consen-
sus choice of learner shows the impact and importance of
our system and tree boosting.

The most important factor behind the success of XGBoost
is its scalability in all scenarios. The system runs more than
ten times faster than existing popular solutions on a single
machine and scales to billions of examples in distributed or
memory-limited settings. The scalability of XGBoost is due
to several important systems and algorithmic optimizations.
These innovations include: a novel tree learning algorithm

2https://github.com/dmlc/xgboost
3These solutions come from of top-3 teams of each compe-
titions.

predict the output.

ˆyi = φ(xi) =

K(cid:88)

k=1

fk(xi), fk ∈ F ,

(1)

where F = {f (x) = wq(x)}(q : Rm → T, w ∈ RT ) is the
space of regression trees (also known as CART). Here q rep-
resents the structure of each tree that maps an example to
the corresponding leaf index. T is the number of leaves in the
tree. Each fk corresponds to an independent tree structure
q and leaf weights w. Unlike decision trees, each regression
tree contains a continuous score on each of the leaf, we use
wi to represent score on i-th leaf. For a given example, we
will use the decision rules in the trees (given by q) to classify
it into the leaves and calculate the ﬁnal prediction by sum-
ming up the score in the corresponding leaves (given by w).
To learn the set of functions used in the model, we minimize
the following regularized objective.

(cid:88)

L(φ) =

(cid:88)

l(ˆyi, yi) +

Ω(fk)

i

where Ω(f ) = γT +

k

λ(cid:107)w(cid:107)2

1
2

(2)

Here l is a diﬀerentiable convex loss function that measures
the diﬀerence between the prediction ˆyi and the target yi.
The second term Ω penalizes the complexity of the model
(i.e., the regression tree functions). The additional regular-
ization term helps to smooth the ﬁnal learnt weights to avoid
over-ﬁtting. Intuitively, the regularized objective will tend
to select a model employing simple and predictive functions.
A similar regularization technique has been used in Regu-
larized greedy forest (RGF) [22] model. Our objective and
the corresponding learning algorithm is simpler than RGF
and easier to parallelize. When the regularization parame-
ter is set to zero, the objective falls back to the traditional
gradient tree boosting.
2.2 Gradient Tree Boosting

The tree ensemble model in Eq. (2) includes functions as
parameters and cannot be optimized using traditional opti-
mization methods in Euclidean space. Instead, the model
is trained in an additive manner. Formally, let ˆy(t)
be the
prediction of the i-th instance at the t-th iteration, we will
need to add ft to minimize the following objective.

i

L(t) =

l(yi, ˆyi

(t−1) + ft(xi)) + Ω(ft)

L(t) (cid:39) n(cid:88)

i=1

This means we greedily add the ft that most improves our
model according to Eq. (2).p To quickly optimize the ob-
jective in the general setting, we approximate it using the
second order Taylor expansion.

[l(yi, ˆy(t−1)) + gift(xi) +

1
2

hif 2

t (xi)] + Ω(ft)

ˆy(t−1) l(yi, ˆy(t−1))
where gi = ∂ˆy(t−1) l(yi, ˆy(t−1)) and hi = ∂2
are ﬁrst and second order gradient statistics on the loss func-
tion. We can remove the constant terms to obtain the fol-
lowing simpliﬁed objective at step t.

n(cid:88)

i=1

n(cid:88)

i=1

˜L(t) =

[gift(xi) +

1
2

hif 2

t (xi)] + Ω(ft)

(3)

Figure 1: Tree Ensemble Model. The ﬁnal predic-
tion for a given example is the sum of predictions
from each tree.

is for handling sparse data; a theoretically justiﬁed weighted
quantile sketch procedure enables handling instance weights
in approximate tree learning. Parallel and distributed com-
puting makes learning faster which enables quicker model ex-
ploration. More importantly, XGBoost exploits out-of-core
computation and enables data scientists to process hundred
millions of examples on a desktop. Finally, it is even more
exciting to combine these techniques to make an end-to-end
system that scales to even larger data with the least amount
of cluster resources. The major contributions of this paper
is listed as follows:

• We design and build a highly scalable end-to-end tree

boosting system.

• We introduce a novel sparsity-aware algorithm for par-

allel tree learning.

• We propose a theoretically justiﬁed weighted quantile

sketch for eﬃcient proposal calculation.

• We propose an eﬀective cache-aware block structure

for out-of-core tree learning.

While there are some existing works on parallel tree boost-
ing [19, 20, 16], the directions such as out-of-core compu-
tation, cache-aware and sparsity-aware learning have not
been explored. More importantly, an end-to-end system
that combines all of these aspects gives a novel solution for
real-world use-cases. This enables data scientists as well
as researchers to build powerful variants of tree boosting
algorithms [6, 7]. Besides these major contributions, we
also make additional improvements in proposing a regular-
ized learning objective and supporting column sub-sampling,
which we will include for completeness.

The remainder of the paper is organized as follows. We
will ﬁrst review tree boosting and introduce a general regu-
larized objective in Sec. 2. We then describe the split ﬁnding
algorithms in Sec. 3 as well as the system design in Sec. 4, in-
cluding experimental results when relevant to provide quan-
titative support for each optimization we describe. Related
work is discussed in Sec. 5. Detailed end-to-end evaluations
of the system are included in Sec. 6. Finally we conclude
the paper in Sec. 7.

2. TREE BOOSTING IN A NUTSHELL
2.1 Regularized Learning Objective
For a given data set with n examples and m features
D = {(xi, yi)} (|D| = n, xi ∈ Rm, yi ∈ R), a tree ensem-
ble model (shown in Fig. 1) uses K additive functions to

Figure 2: Structure Score Calculation. We only
need to sum up the gradient and second order gra-
dient statistics on each leaf, then apply the scoring
formula to get the quality score.

Deﬁne Ij = {i|q(xi) = j} as the instance set of leaf j. We
can rewrite Eq (3) by expanding the regularization term Ω
as follows

n(cid:88)
T(cid:88)

i=1

j=1

(cid:88)

i∈Ij

[(

˜L(t) =

=

T(cid:88)

1
2

λ

j=1

(4)

[gift(xi) +

hif 2

t (xi)] + γT +

w2
j

gi)wj +

1
2

(

hi + λ)w2

j ] + γT

Algorithm 1: Exact Greedy Algorithm for Split Finding
Input: I, instance set of current node
Input: d, feature dimension
gain ← 0

G ←(cid:80)

i∈I gi, H ←(cid:80)

i∈I hi

for k = 1 to m do

GL ← 0, HL ← 0
for j in sorted(I, by xjk) do

GL ← GL + gj, HL ← HL + hj
GR ← G − GL, HR ← H − HL
score ← max(score, G2
HL+λ + G2

L

HR+λ − G2
H+λ )

R

end

end
Output: Split with max score

Algorithm 2: Approximate Algorithm for Split Finding

for k = 1 to m do

Propose Sk = {sk1, sk2,··· skl} by percentiles on feature k.
Proposal can be done per tree (global), or per split(local).

end
for k = 1 to m do

Gkv ←=(cid:80)
Hkv ←=(cid:80)

j∈{j|sk,v≥xjk>sk,v−1} gj
j∈{j|sk,v≥xjk>sk,v−1} hj

1
2

(cid:88)

i∈Ij

(cid:80)
(cid:80)

j = −
∗

w

i∈Ij

gi

i∈Ij

hi + λ

For a ﬁxed structure q(x), we can compute the optimal

weight w∗

j of leaf j by

,

(5)

end
Follow same step as in previous section to ﬁnd max
score only among proposed splits.

and calculate the corresponding optimal objective function
value by

((cid:80)
(cid:80)

T(cid:88)

j=1

i∈Ij

gi)2
hi + λ

i∈Ij

˜L(t)(q) = − 1
2

+ γT.

(6)

(cid:34) ((cid:80)
(cid:80)

Eq (6) can be used as a scoring function to measure the
quality of a tree structure q. This score is like the impurity
score for evaluating decision trees, except that it is derived
for a wider range of objective functions. Fig. 2 illustrates
how this score can be calculated.

Normally it is impossible to enumerate all the possible
tree structures q. A greedy algorithm that starts from a
single leaf and iteratively adds branches to the tree is used
instead. Assume that IL and IR are the instance sets of left
and right nodes after the split. Lettting I = IL ∪ IR, then
the loss reduction after the split is given by

((cid:80)
(cid:80)

− ((cid:80)
(cid:80)

(cid:35)

Lsplit =

1
2

i∈IL

gi)2
hi + λ

i∈IL

+

i∈IR

gi)2
hi + λ

i∈IR

i∈I gi)2
i∈I hi + λ

−γ

(7)
This formula is usually used in practice for evaluating the
split candidates.
2.3 Shrinkage and Column Subsampling

Besides the regularized objective mentioned in Sec. 2.1,
two additional techniques are used to further prevent over-
ﬁtting. The ﬁrst technique is shrinkage introduced by Fried-
man [10]. Shrinkage scales newly added weights by a factor
η after each step of tree boosting. Similar to a learning rate
in stochastic optimization, shrinkage reduces the inﬂuence
of each individual tree and leaves space for future trees to
improve the model.

The second technique is column (feature) subsampling.
This technique is commonly used in RandomForest [3], but
has not been applied to tree boosting before. According
to user feedback, using column sub-sampling prevents over-
ﬁtting even more so than the traditional row sub-sampling
(which is also supported). The usage of column sub-samples
also speeds up computations of the parallel algorithm de-
scribed later part of the paper.

3. SPLIT FINDING ALGORITHMS
3.1 Basic Exact Greedy Algorithm

One of the key problems in tree learning is to ﬁnd the
best split as indicated by Eq (7). In order to do so, a split
ﬁnding algorithm enumerates over all the possible splits on
all the features. We call this the exact greedy algorithm.
Most existing single machine tree boosting implementations,
such as scikit-learn [17], R’s gbm [18] as well as the single
machine version of XGBoost support the exact greedy algo-
rithm. The exact greedy algorithm is shown in Alg. 1. It
is computationally demanding to enumerate all the possible
splits for continuous features. In order to do so eﬃciently,
the algorithm must ﬁrst sort the data according to feature
values and visit the data in sorted order to accumulate the
gradient statistics for the structure score in Eq (7).
3.2 Approximate Algorithm

The exact greedy algorithm is very powerful since it enu-
merates over all possible splitting points greedily. However,
it is impossible to eﬃciently do so when the data does not ﬁt
entirely into memory. Same problem also arises in the dis-
tributed setting. To support eﬀective gradient tree boosting

Figure 3: Comparison of test AUC convergence on
Higgs 10M dataset. The eps parameter corresponds
to the accuracy of the approximate sketch. This
roughly translates to 1 / eps buckets in the proposal.
We ﬁnd that local proposals require fewer buckets,
because it reﬁne split candidates.

in these two settings, an approximate algorithm needs to
be used. An approximate framework for tree boosting is
given in Alg. 2. To summarize, the algorithm ﬁrst proposes
candidate splitting points according to percentiles of fea-
ture distribution (speciﬁc criteria will be given in Sec. 3.3).
The algorithm then maps the continuous features into buck-
ets split by these candidate points, aggregates the statistics
and ﬁnds the best solution among proposals based on the
aggregated statistics.

There are two variants of the algorithm, depending on
when the proposal is given. The global variant proposes all
the candidate splits during the initial phase of tree construc-
tion, and uses the same proposals for split ﬁnding at all lev-
els. The local variant re-proposes after each split. The global
method requires less proposal steps than the local method.
However, usually more candidate points are needed for the
global proposal because candidates are not reﬁned after each
split. The local proposal reﬁnes the candidates after splits,
and can potentially be more appropriate for deeper trees. A
comparison of diﬀerent algorithms on a Higgs boson dataset
is given by Fig. 3. We ﬁnd that the local proposal indeed
requires fewer candidates. The global proposal can be as
accurate as the local one given enough candidates.

Most existing approximate algorithms for distributed tree
learning also follow this framework. Notably, it is also possi-
ble to directly construct approximate histograms of gradient
statistics [19]. Our system eﬃciently supports exact greedy
for the single machine setting, as well as approximate al-
gorithm with both local and global proposal methods for
all settings. Users can freely choose between the methods
according to their needs.

3.3 Weighted Quantile Sketch

One important step in the approximate algorithm is to
propose candidate split points. Usually percentiles of a fea-
ture are used to make candidates distribute evenly on the
data. Formally, let multi-set Dk = {(x1k, h1), (x2k, h2)··· (xnk, hn)}
represent the k-th feature values and second order gradient
statistics of each training instances. We can deﬁne a rank

Figure 4: Tree structure with default directions. An
example will be classiﬁed into the default direction
when the feature needed for the split is missing.
functions rk : R → [0, +∞) as

1(cid:80)

rk(z) =

(cid:88)

(x,h)∈Dk

h

(x,h)∈Dk,x<z

h,

(8)

which represents the proportion of instances whose feature
value k is smaller than z. The goal is to ﬁnd candidate split
points {sk1, sk2,··· skl}, such that
|rk(sk,j) − rk(sk,j+1)| < , sk1 = min

xik, skl = max

xik.

i

i

(9)
Here  is an approximation factor. Intuitively, this means
that there is roughly 1/ candidate points. Here each data
point is weighted by hi. To see why hi represents the weight,
we can rewrite Eq (3) as

hi(ft(xi) − gi/hi)2 + Ω(ft) + constant,

1
2

n(cid:88)

i=1

which is exactly weighted squared loss with labels gi/hi
and weights hi. For large datasets, it is non-trivial to ﬁnd
candidate splits that satisfy the criteria. When every in-
stance has equal weights, an existing algorithm called quan-
tile sketch [12, 21] solves the problem. However, there is no
existing quantile sketch for the weighted datasets. There-
fore, most existing approximate tree boosting algorithms ei-
ther resorted to sorting on a random subset of data which
have a chance of failure or heuristics that do not have theo-
retical guarantee.

To solve this problem, we introduced a novel distributed
weighted quantile sketch algorithm that can handle weighted
data with a provable theoretical guarantee. The general idea
is to propose a data structure that supports merge and prune
operations, with each operation proven to maintain a certain
accuracy level. A detailed description of the algorithm as
well as proofs are given in the appendix.
3.4 Sparsity-aware Split Finding

In many real-world problems, it is quite common for the
input x to be sparse. There are multiple possible causes
for sparsity: 1) presence of missing values in the data; 2)
frequent zero entries in the statistics; and, 3) artifacts of
feature engineering such as one-hot encoding. It is impor-
tant to make the algorithm aware of the sparsity pattern in
the data.
In order to do so, we propose to add a default
direction in each tree node, which is shown in Fig. 4. When
a value is missing in the sparse matrix x, the instance is
classiﬁed into the default direction.

There are two choices of default direction in each branch.
The optimal default directions are learnt from the data. The
algorithm is shown in Alg. 3. The key improvement is to only
visit the non-missing entries Ik. The presented algorithm

0102030405060708090Number of Iterations0.750.760.770.780.790.800.810.820.83Test AUCexact greedyglobal eps=0.3local eps=0.3global eps=0.05Algorithm 3: Sparsity-aware Split Finding
Input: I, instance set of current node
Input: Ik = {i ∈ I|xik (cid:54)= missing}
Input: d, feature dimension
Also applies to the approximate setting, only collect
statistics of non-missing entries into buckets
gain ← 0

G ←(cid:80)

i∈I , gi,H ←(cid:80)

i∈I hi

for k = 1 to m do

// enumerate missing value goto right
GL ← 0, HL ← 0
for j in sorted(Ik, ascent order by xjk) do

GL ← GL + gj, HL ← HL + hj
GR ← G − GL, HR ← H − HL
score ← max(score, G2
HL+λ + G2

L

HR+λ − G2
H+λ )

R

end
// enumerate missing value goto left
GR ← 0, HR ← 0
for j in sorted(Ik, descent order by xjk) do

GR ← GR + gj, HR ← HR + hj
GL ← G − GR, HL ← H − HR
score ← max(score, G2
HL+λ + G2

L

HR+λ − G2
H+λ )

R

end

end
Output: Split and default directions with max gain

treats the non-presence as a missing value and learns the
best direction to handle missing values. The same algorithm
can also be applied when the non-presence corresponds to
a user speciﬁed value by limiting the enumeration only to
consistent solutions.

To the best of our knowledge, most existing tree learn-
ing algorithms are either only optimized for dense data, or
need speciﬁc procedures to handle limited cases such as cat-
egorical encoding. XGBoost handles all sparsity patterns in
a uniﬁed way. More importantly, our method exploits the
sparsity to make computation complexity linear to number
of non-missing entries in the input. Fig. 5 shows the com-
parison of sparsity aware and a naive implementation on an
Allstate-10K dataset (description of dataset given in Sec. 6).
We ﬁnd that the sparsity apware algorithm runs 50 times
faster than the naive version. This conﬁrms the importance
of the sparsity aware algorithm.

4. SYSTEM DESIGN
4.1 Column Block for Parallel Learning

The most time consuming part of tree learning is to get
the data into sorted order. In order to reduce the cost of
sorting, we propose to store the data in in-memory units,
which we called block. Data in each block is stored in the
compressed column (CSC) format, with each column sorted
by the corresponding feature value. This input data layout
only needs to be computed once before training, and can be
reused in later iterations.

In the exact greedy algorithm, we store the entire dataset
in a single block and run the split search algorithm by lin-
early scanning over the pre-sorted entries. We do the split
ﬁnding of all leaves collectively, so one scan over the block
will collect the statistics of the split candidates in all leaf

Figure 5: Impact of the sparsity aware algorithm
on Allstate-10K. The dataset is sparse mainly due
to one-hot encoding. The sparsity aware algorithm
is more than 50 times faster than the naive version
that does not take sparsity into consideration.

branches. Fig. 6 shows how we transform a dataset into
the block-based format and ﬁnd the optimal split using the
block structure.

The block structure also helps when using the approxi-
mate algorithms. Multiple blocks can be used in this case,
with each block corresponding to subset of rows in the dataset.
Diﬀerent blocks can be distributed across machines, or stored
on disk in the out-of-core setting. Using the sorted struc-
ture, the quantile ﬁnding step becomes a linear scan over
the sorted columns. This is especially valuable for local pro-
posal algorithms, where candidates are generated frequently
at each branch. The binary search in histogram aggregation
also becomes a linear time merge style algorithm.

Collecting statistics for each column can be parallelized,
giving us a parallel algorithm for split ﬁnding. Importantly,
the column block structure also supports column subsam-
pling, as it is easy to select a subset of columns in a block.
Time Complexity Analysis Let d be the maximum depth
of the tree and K be total number of trees. For the ex-
act greedy algorithm, the time complexity of original spase
aware algorithm is O(Kd(cid:107)x(cid:107)0 log n). Here we use (cid:107)x(cid:107)0 to
denote number of non-missing entries in the training data.
On the other hand, tree boosting on the block structure only
cost O(Kd(cid:107)x(cid:107)0 + (cid:107)x(cid:107)0 log n). Here O((cid:107)x(cid:107)0 log n) is the one
time preprocessing cost that can be amortized. This analysis
shows that the block structure helps to save an additional
log n factor, which is signiﬁcant when n is large. For the
approximate algorithm, the time complexity of original al-
gorithm with binary search is O(Kd(cid:107)x(cid:107)0 log q). Here q is
the number of proposal candidates in the dataset. While q
is usually between 32 and 100, the log factor still introduces
overhead. Using the block structure, we can reduce the time
to O(Kd(cid:107)x(cid:107)0 +(cid:107)x(cid:107)0 log B), where B is the maximum num-
ber of rows in each block. Again we can save the additional
log q factor in computation.
4.2 Cache-aware Access

While the proposed block structure helps optimize the
computation complexity of split ﬁnding, the new algorithm
requires indirect fetches of gradient statistics by row index,
since these values are accessed in order of feature. This is
a non-continuous memory access. A naive implementation
of split enumeration introduces immediate read/write de-

124816Number of Threads0.031250.06250.1250.250.512481632Time per Tree(sec)Sparsity aware algorithmBasic algorithmFigure 6: Block structure for parallel learning. Each column in a block is sorted by the corresponding feature
value. A linear scan over one column in the block is suﬃcient to enumerate all the split points.

(a) Allstate 10M

(b) Higgs 10M

(c) Allstate 1M

(d) Higgs 1M

Figure 7: Impact of cache-aware prefetching in exact greedy algorithm. We ﬁnd that the cache-miss eﬀect
impacts the performance on the large datasets (10 million instances). Using cache aware prefetching improves
the performance by factor of two when the dataset is large.

Figure 8:
that can cause stall due to cache miss.

Short range data dependency pattern

pendency between the accumulation and the non-continuous
memory fetch operation (see Fig. 8). This slows down split
ﬁnding when the gradient statistics do not ﬁt into CPU cache
and cache miss occur.

For the exact greedy algorithm, we can alleviate the prob-
lem by a cache-aware prefetching algorithm. Speciﬁcally,
we allocate an internal buﬀer in each thread, fetch the gra-
dient statistics into it, and then perform accumulation in
a mini-batch manner. This prefetching changes the direct
read/write dependency to a longer dependency and helps to
reduce the runtime overhead when number of rows in the
is large. Figure 7 gives the comparison of cache-aware vs.
non cache-aware algorithm on the the Higgs and the All-
state dataset. We ﬁnd that cache-aware implementation of
the exact greedy algorithm runs twice as fast as the naive
version when the dataset is large.

For approximate algorithms, we solve the problem by choos-

ing a correct block size. We deﬁne the block size to be max-
imum number of examples in contained in a block, as this
reﬂects the cache storage cost of gradient statistics. Choos-
ing an overly small block size results in small workload for
each thread and leads to ineﬃcient parallelization. On the
other hand, overly large blocks result in cache misses, as the

gradient statistics do not ﬁt into the CPU cache. A good
choice of block size balances these two factors. We compared
various choices of block size on two data sets. The results
are given in Fig. 9. This result validates our discussion and
shows that choosing 216 examples per block balances the
cache property and parallelization.

4.3 Blocks for Out-of-core Computation

One goal of our system is to fully utilize a machine’s re-
sources to achieve scalable learning. Besides processors and
memory, it is important to utilize disk space to handle data
that does not ﬁt into main memory. To enable out-of-core
computation, we divide the data into multiple blocks and
store each block on disk.

During computation, it is important to use an indepen-
dent thread to pre-fetch the block into a main memory buﬀer,
so computation can happen in concurrence with disk read-
ing. However, this does not entirely solve the problem since
the disk reading takes most of the computation time. It is
important to reduce the overhead and increase the through-
put of disk IO. We mainly use two techniques to improve
the out-of-core computation.
Block Compression The ﬁrst technique we use is block
compression. The block is compressed by columns, and de-
compressed on the ﬂy by an independent thread when load-
ing into main memory. This helps to trade some of the
computation in decompression with the disk reading cost.
We use a general purpose compression algorithm for com-
pressing the features values. For the row index, we substract
the row index by the begining index of the block and use a
16bit integer to store each oﬀset. This requires 216 examples
per block, which ﬁs conﬁrmed to be a good setting. In most
of the dataset we tested, we achieve roughly a 26% to 29%
compression ratio.

124816Number of Threads8163264128Time per Tree(sec)Basic algorithmCache-aware algorithm124816Number of Threads8163264128256Time per Tree(sec)Basic algorithmCache-aware algorithm124816Number of Threads0.250.51248Time per Tree(sec)Basic algorithmCache-aware algorithm124816Number of Threads0.250.51248Time per Tree(sec)Basic algorithmCache-aware algorithmTable 1: Comparison of major tree boosting systems.

System

XGBoost
pGBRT
Spark MLLib
H2O
scikit-learn
R GBM

exact
greedy
yes
no
no
no
yes
yes

approximate
global
yes
no
yes
yes
no
no

approximate
local
yes
yes
no
no
no
no

out-of-core

yes
no
no
no
no
no

sparsity
aware
yes
no
partially
partially
no
partially

parallel

yes
yes
yes
yes
no
no

overﬁtting. This this resembles previous work on regularized
greedy forest [22], but simpliﬁes the objective and algorithm
for parallelization. Column sampling is a simple but eﬀec-
tive technique that we borrow from RandomForest [3] and
use for tree boosting. While sparsity-aware learning is es-
sential in other types of models such as linear models [8], few
works on tree learning have considered this topic in a prin-
cipled way. The algorithm proposed in this paper is the ﬁrst
uniﬁed approach to handle all kinds of sparsity patterns.

There are several existing works on parallelizing tree learn-
ing [19, 16]. Most of these algorithms fall into the ap-
proximate framework described in this paper. Notably, it
is also possible to partition data by columns [20] and ap-
ply the exact greedy algorithm. This is also supported in
our framework, and the techniques such as cache-aware pre-
fecthing can be used to beneﬁt this type of algorithm. While
most existing works focus on the algorithmic aspect of par-
allelization, our work improves in two unexplored system di-
rections: out-of-core computation and cache-aware learning.
This gives us insights on how the system and the algorithm
can be jointly optimized and provides an end-to-end sys-
tem that can handle large scale problems with very limited
computing resources. We also summarize the comparison
between our system and existing ones in Table 1.

Quantile summary (without weights) is a classical prob-
lem in the database community [12, 21]. However, the ap-
proximate tree boosting algorithm reveals a more general
problem – ﬁnding quantiles on weighted data. To the best
of our knowledge, the weighted quantile sketch proposed in
this paper is the ﬁrst method to solve this problem. The
weighted quantile summary is also not speciﬁc to the tree
learning and can beneﬁt other applications in data science
and machine learning in the future.

6. END TO END EVALUATIONS

In this section, we present end-to-end evaluations of our
system in various settings. Dataset and system conﬁgura-
tions for all experiments are summarized in Sec. 6.2.
6.1 System Implementation

We implemented XGBoost as an open source package4.
The package is portable and reusable. It is available in pop-
ular languages such as python, R, Julia and integrates nat-
urally with language native data science pipelines such as
scikit-learn. The distributed version is built on top of the
rabit library5 for allreduce. The portability of XGBoost
makes it available in many ecosystems, instead of only be-
ing tied to a speciﬁc platform. The distributed XGBoost

4https://github.com/dmlc/xgboost
5https://github.com/dmlc/rabit

(a) Allstate 10M

(b) Higgs 10M

Figure 9: The impact of block size in the approxi-
mate algorithm. We ﬁnd that overly small blocks re-
sults in ineﬃcient parallelization, while overly large
blocks also slows down training due to cache misses.

Block Sharding The second technique is to shard the data
onto multiple disks in an alternative manner. A pre-fetcher
thread is assigned to each disk and fetches the data into an
in-memory buﬀer. The training thread then alternatively
reads the data from each buﬀer. This helps to increase the
throughput of disk reading when multiple disks are available.

5. RELATED WORKS

Our system implements gradient boosting [9], which per-
forms additive optimization in functional space. Gradient
tree boosting has been successfully used in classiﬁcation [11],
learning to rank [4], structured prediction [7] as well as other
ﬁelds. XGBoost incorporates a regularized model to prevent

124816Number of Threads48163264128Time per Tree(sec)block size=2^12block size=2^16block size=2^20block size=2^24124816Number of Threads48163264128256512Time per Tree(sec)block size=2^12block size=2^16block size=2^20block size=2^24Table 2: Dataset used in the Experiments

n

m

Dataset
10 M 4227
Allstate
Higgs Boson
10 M 28
Yahoo LTRC 473K 700
Criteo
67

1.7 B

Task
Insurance claim classiﬁcation
Event classiﬁcation
Learning to Rank
Click through rate prediction

runs natively on Hadoop, MPI Sun Grid engine. Recently,
we also enable distributed XGBoost on jvm bigdata stacks
such as Flink and Spark. The distributed version has also
been integrated into cloud platform Tianchi6 of Alibaba. We
believe that there will be more integrations in the future.
6.2 Dataset and Setup

We used four datasets in our experiments. A summary
of these datasets is given in Table 2.
In some of the ex-
periments, we will use a subset of the data either due to
slow baselines or to demonstrate the performance of the al-
gorithm with varying dataset size. We use a suﬃx to denote
the size of the dataset in these cases. For example Allstate-
10K means a subset of the Allstate dataset with 10K in-
stances.

The ﬁrst dataset we use is the Allstate insurance claim
dataset7. The task is to predict the likelihood and cost of
an insurance claim given diﬀerent risk factors. In the exper-
iment, we simpliﬁed the task to only predict the likelihood
of an insurance claim. This dataset is used to evaluate the
impact of sparsity-aware algorithm in Sec. 3.4. Most of the
sparse features in this data come from one-hot encoding.

The second dataset is the Higgs boson dataset8 from high
energy physics. The data was produced using Monte Carlo
simulations of physics events. It contains 21 kinematic prop-
erties measured by the particle detectors in the accelerator.
It also contains seven additional derived physics quantities
of the particles. The task is to classify whether an event
corresponds to the Higgs boson.

The third dataset is the Yahoo! learning to rank challenge
dataset [5], which is one of the most commonly used bench-
marks in learning to rank algorithms. The dataset contains
20K web search queries, with each query corresponding to a
list of around 22 documents. The task is to rank the docu-
ments according to relevance of the query.

The last dataset is the criteo terabyte click log dataset9.
We use this dataset to evaluate the scaling property of the
system in the out-of-core and the distributed settings. The
data contains 13 integer features and 26 ID features of user,
item and advertiser information. Since a tree based model
is better at handling continuous features, we preprocess the
data by calculating the statistics of average CTR and count
of ID features on the ﬁrst ten days, replacing the ID fea-
tures by the corresponding count statistics during the next
ten days for training. The training set after preprocessing
contains 1.7 billion instances with 67 features (13 integer, 26
average CTR statistics and 26 counts). The entire dataset
is more than one terabyte in LibSVM format.

We use the ﬁrst three datasets for the single machine par-

6https://tianchi.aliyun.com
7https://www.kaggle.com/c/ClaimPredictionChallenge
8https://archive.ics.uci.edu/ml/datasets/HIGGS
9http://labs.criteo.com/downloads/download-terabyte-
click-logs/

Table 3: Comparison of Exact Greedy Methods with
500 trees on Higgs-1M data

Method
XGBoost
XGBoost (colsample=0.5)
scikit-learn
R.gbm

Time per Tree (sec) Test AUC

0.6841
0.6401
28.51
1.032

0.8304
0.8245
0.8302
0.6224

Figure 10: Comparison between XGBoost and pG-
BRT on Yahoo LTRC dataset.

Table 4: Comparison of Learning to Rank with 500
trees on Yahoo! LTRC Dataset

Time per Tree (sec) NDCG@10

Method
XGBoost
XGBoost (colsample=0.5)
pGBRT [19]

0.826
0.506
2.576

0.7892
0.7913
0.7915

allel setting, and the last dataset for the distributed and
out-of-core settings. All the single machine experiments are
conducted on a Dell PowerEdge R420 with two eight-core
Intel Xeon (E5-2470) (2.3GHz) and 64GB of memory.
If
not speciﬁed, all the experiments are run using all the avail-
able cores in the machine. The machine settings of the dis-
tributed and the out-of-core experiments will be described in
the corresponding section. In all the experiments, we boost
trees with a common setting of maximum depth equals 8,
shrinkage equals 0.1 and no column subsampling unless ex-
plicitly speciﬁed. We can ﬁnd similar results when we use
other settings of maximum depth.
6.3 Classiﬁcation

In this section, we evaluate the performance of XGBoost
on a single machine using the exact greedy algorithm on
Higgs-1M data, by comparing it against two other commonly
used exact greedy tree boosting implementations. Since
scikit-learn only handles non-sparse input, we choose the
dense Higgs dataset for a fair comparison. Among the meth-
ods in comparison, R’s GBM uses a greedy approach that
only expands one branch of a tree, which makes it faster
but can result in lower accuracy, while both scikit-learn and
XGBoost learn a full tree. The results are shown in Table 3.
Both XGBoost and scikit-learn give better performance than
R’s GBM, while XGBoost runs more than 10x faster than
scikit-learn. In this experiment, we also ﬁnd column sub-
samples gives slightly worse performance than using all the

124816Number of Threads0.512481632Time per Tree(sec)XGBoostpGBRTFigure 11: Comparison of out-of-core methods on
diﬀerent subsets of criteo data. The missing data
points are due to out of disk space. We can ﬁnd
that basic algorithm can only handle 200M exam-
ples. Adding compression gives 3x speedup, and
sharding into two disks gives another 2x speedup.
The system runs out of ﬁle cache start from 400M
examples. The algorithm really has to rely on disk
after this point. The compression+shard method
has a less dramatic slowdown when running out of
ﬁle cache, and exhibits a linear trend afterwards.

features. This could due to the fact that there are few im-
portant features in this dataset and we can beneﬁt from
greedily select from all the features.
6.4 Learning to Rank

We next evaluate the performance of XGBoost on the
learning to rank problem. We compare against pGBRT [19],
the best previously pubished system on this task. XGBoost
runs exact greedy algorithm, while pGBRT only support an
approximate algorithm. The results are shown in Table 4
and Fig. 10. We ﬁnd that XGBoost runs faster. Interest-
ingly, subsampling columns not only reduces running time,
and but also gives a bit higher performance for this prob-
lem. This could due to the fact that the subsampling helps
prevent overﬁtting, which is observed by many of the users.
6.5 Out-of-core Experiment

We also evaluate our system in the out-of-core setting on
the criteo data. We conducted the experiment on one AWS
c3.8xlarge machine (32 vcores, two 320 GB SSD, 60 GB
RAM). The results are shown in Figure 11. We can ﬁnd
that compression helps to speed up computation by factor of
three, and sharding into two disks further gives 2x speedup.
For this type of experiment, it is important to use a very
large dataset to drain the system ﬁle cache for a real out-
of-core setting. This is indeed our setup. We can observe a
transition point when the system runs out of ﬁle cache. Note
that the transition in the ﬁnal method is less dramatic. This
is due to larger disk throughput and better utilization of
computation resources. Our ﬁnal method is able to process
1.7 billion examples on a single machine.
6.6 Distributed Experiment

(a) End-to-end time cost include data loading

(b) Per iteration cost exclude data loading

Figure 12: Comparison of diﬀerent distributed sys-
tems on 32 EC2 nodes for 10 iterations on diﬀerent
subset of criteo data. XGBoost runs more 10x than
spark per iteration and 2.2x as H2O’s optimized ver-
sion (However, H2O is slow in loading the data, get-
ting worse end-to-end time). Note that spark suﬀers
from drastic slow down when running out of mem-
ory. XGBoost runs faster and scales smoothly to
the full 1.7 billion examples with given resources by
utilizing out-of-core computation.

We set up a YARN cluster on EC2 with m3.2xlarge ma-
chines, which is a very common choice for clusters. Each
machine contains 8 virtual cores, 30GB of RAM and two
80GB SSD local disks. The dataset is stored on AWS S3
instead of HDFS to avoid purchasing persistent storage.

We ﬁrst compare our system against two production-level
distributed systems: Spark MLLib [15] and H2O 10. We use
32 m3.2xlarge machines and test the performance of the sys-
tems with various input size. Both of the baseline systems
are in-memory analytics frameworks that need to store the
data in RAM, while XGBoost can switch to out-of-core set-
ting when it runs out of memory. The results are shown
in Fig. 12. We can ﬁnd that XGBoost runs faster than the
baseline systems. More importantly, it is able to take ad-
vantage of out-of-core computing and smoothly scale to all
1.7 billion examples with the given limited computing re-
sources. The baseline systems are only able to handle sub-

Finally, we evaluate the system in the distributed setting.

10www.h2o.ai

12825651210242048Number of Training Examples (million)128256512102420484096Time per Tree(sec)Basic algorithmBlock compressionCompression+shardOut of system file cachestart from this point12825651210242048Number of Training Examples (million)12825651210242048409681921638432768Total Running Time (sec)Spark MLLibH2OXGBoost12825651210242048Number of Training Examples (million)8163264128256512102420484096Time per Iteration (sec)Spark MLLibH2OXGBoost[3] L. Breiman. Random forests. Maching Learning,

45(1):5–32, Oct. 2001.

[4] C. Burges. From ranknet to lambdarank to lambdamart:

An overview. Learning, 11:23–581, 2010.

[5] O. Chapelle and Y. Chang. Yahoo! Learning to Rank

Challenge Overview. Journal of Machine Learning
Research - W & CP, 14:1–24, 2011.

[6] T. Chen, H. Li, Q. Yang, and Y. Yu. General functional

matrix factorization using gradient boosting. In Proceeding
of 30th International Conference on Machine Learning
(ICML’13), volume 1, pages 436–444, 2013.

[7] T. Chen, S. Singh, B. Taskar, and C. Guestrin. Eﬃcient

second-order gradient boosting for conditional random
ﬁelds. In Proceeding of 18th Artiﬁcial Intelligence and
Statistics Conference (AISTATS’15), volume 1, 2015.

[8] R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and

C.-J. Lin. LIBLINEAR: A library for large linear
classiﬁcation. Journal of Machine Learning Research,
9:1871–1874, 2008.

[9] J. Friedman. Greedy function approximation: a gradient
boosting machine. Annals of Statistics, pages 1189–1232,
2001.

[10] J. Friedman. Stochastic gradient boosting. Computational

Statistics & Data Analysis, 38(4):367–378, 2002.

[11] J. Friedman, T. Hastie, and R. Tibshirani. Additive logistic

regression: a statistical view of boosting. The annals of
statistics, 28(2):337–407, 2000.

[12] M. Greenwald and S. Khanna. Space-eﬃcient online

computation of quantile summaries. In Proceedings of the
2001 ACM SIGMOD International Conference on
Management of Data, pages 58–66, 2001.

[13] X. He, J. Pan, O. Jin, T. Xu, B. Liu, T. Xu, Y. Shi,

A. Atallah, R. Herbrich, S. Bowers, and J. Q. n. Candela.
Practical lessons from predicting clicks on ads at facebook.
In Proceedings of the Eighth International Workshop on
Data Mining for Online Advertising, ADKDD’14, 2014.
[14] P. Li. Robust Logitboost and adaptive base class (ABC)

Logitboost. In Proceedings of the Twenty-Sixth Conference
Annual Conference on Uncertainty in Artiﬁcial Intelligence
(UAI’10), pages 302–311, 2010.

[15] X. Meng, J. Bradley, B. Yavuz, E. Sparks,

S. Venkataraman, D. Liu, J. Freeman, D. B. Tsai,
M. Amde, S. Owen, D. Xin, R. Xin, M. J. Franklin,
R. Zadeh, M. Zaharia, and A. Talwalkar. MLlib: Machine
Learning in Apache Spark, May 2015.

[16] B. Panda, J. S. Herbach, S. Basu, and R. J. Bayardo.

Planet: Massively parallel learning of tree ensembles with
mapreduce. Proceeding of VLDB Endowment,
2(2):1426–1437, Aug. 2009.

[17] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,

B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,
R. Weiss, V. Dubourg, J. Vanderplas, A. Passos,
D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay.
Scikit-learn: Machine learning in Python. Journal of
Machine Learning Research, 12:2825–2830, 2011.

[18] G. Ridgeway. Generalized Boosted Models: A guide to the

gbm package.

[19] S. Tyree, K. Weinberger, K. Agrawal, and J. Paykin.

Parallel boosted regression trees for web search ranking. In
Proceedings of the 20th international conference on World
wide web, pages 387–396. ACM, 2011.

[20] J. Ye, J.-H. Chow, J. Chen, and Z. Zheng. Stochastic

gradient boosted distributed decision trees. In Proceedings
of the 18th ACM Conference on Information and
Knowledge Management, CIKM ’09.

[21] Q. Zhang and W. Wang. A fast algorithm for approximate
quantiles in high speed data streams. In Proceedings of the
19th International Conference on Scientiﬁc and Statistical
Database Management, 2007.

[22] T. Zhang and R. Johnson. Learning nonlinear functions

using regularized greedy forest. IEEE Transactions on

Figure 13: Scaling of XGBoost with diﬀerent num-
ber of machines on criteo full 1.7 billion dataset.
Using more machines results in more ﬁle cache and
makes the system run faster, causing the trend
to be slightly super linear. XGBoost can process
the entire dataset using as little as four machines,
and scales smoothly by utilizing more available re-
sources.

set of the data with the given resources. This experiment
shows the advantage to bring all the system improvement
together and solve a real-world scale problem. We also eval-
uate the scaling property of XGBoost by varying the number
of machines. The results are shown in Fig. 13. We can ﬁnd
XGBoost’s performance scales linearly as we add more ma-
chines. Importantly, XGBoost is able to handle the entire
1.7 billion data with only four machines. This shows the
system’s potential to handle even larger data.

7. CONCLUSION

In this paper, we described the lessons we learnt when
building XGBoost, a scalable tree boosting system that is
widely used by data scientists and provides state-of-the-art
results on many problems. We proposed a novel sparsity
aware algorithm for handling sparse data and a theoretically
justiﬁed weighted quantile sketch for approximate learning.
Our experience shows that cache access patterns, data com-
pression and sharding are essential elements for building a
scalable end-to-end system for tree boosting. These lessons
can be applied to other machine learning systems as well.
By combining these insights, XGBoost is able to solve real-
world scale problems using a minimal amount of resources.

Acknowledgments
We would like to thank Tyler B. Johnson, Marco Tulio Ribeiro,
Sameer Singh for their valuable feedback. We also sincerely thank
Tong He, Bing Xu, Michael Benesty, Yuan Tang, Hongliang Liu,
Qiang Kou, Nan Zhu and all other contributors in the XGBoost
community. This work was supported in part by ONR (PECASE)
N000141010672, NSF IIS 1258741 and the TerraSwarm Research
Center sponsored by MARCO and DARPA.

8. REFERENCES
[1] R. Bekkerman. The present and the future of the kdd cup

competition: an outsider’s perspective.

[2] J. Bennett and S. Lanning. The netﬂix prize. In

Proceedings of the KDD Cup Workshop 2007, pages 3–6,
New York, Aug. 2007.

481632Number of Machines12825651210242048Time per Iteration (sec)Pattern Analysis and Machine Intelligence, 36(5), 2014.

APPENDIX
A. WEIGHTED QUANTILE SKETCH

In this section, we introduce the weighted quantile sketch algo-
rithm. Approximate answer of quantile queries is for many real-
world applications. One classical approach to this problem is GK
algorithm [12] and extensions based on the GK framework [21].
The main component of these algorithms is a data structure called
quantile summary, that is able to answer quantile queries with
relative accuracy of . Two operations are deﬁned for a quantile
summary:

• A merge operation that combines two summaries with ap-
proximation error 1 and 2 together and create a merged
summary with approximation error max(1, 2).
• A prune operation that reduces the number of elements in
the summary to b + 1 and changes approximation error from
 to  + 1
b .

A quantile summary with merge and prune operations forms basic
building blocks of the distributed and streaming quantile comput-
ing algorithms [21].

In order to use quantile computation for approximate tree boost-
ing, we need to ﬁnd quantiles on weighted data. This more gen-
eral problem is not supported by any of the existing algorithm. In
this section, we describe a non-trivial weighted quantile summary
structure to solve this problem. Importantly, the new algorithm
contains merge and prune operations with the same guarantee as
GK summary. This allows our summary to be plugged into all
the frameworks used GK summary as building block and answer
quantile queries over weighted data eﬃciently.
A.1 Formalization and Deﬁnitions
Given an input multi-set D = {(x1, w1), (x2, w2) · · · (xn, wn)}
such that wi ∈ [0, +∞), xi ∈ X . Each xi corresponds to a po-
sition of the point and wi is the weight of the point. Assume
we have a total order < deﬁned on X . Let us deﬁne two rank
functions r−

D, r+D : X → [0, +∞)

(cid:88)
(cid:88)

(x,w)∈D,x<y

(x,w)∈D,x≤y

r−
D(y) =

r+D(y) =

w

w

(10)

(11)

We should note that since D is deﬁned to be a multiset of the
points. It can contain multiple record with exactly same position
x and weight w. We also deﬁne another weight function ωD :
X → [0, +∞) as

(cid:88)

ωD(y) = r+D(y) − r−

D(y) =

w.

(12)

(x,w)∈D,x=y

Finally, we also deﬁne the weight of multi-set D to be the sum of
weights of all the points in the set

(cid:88)

(x,w)∈D

ω(D) =

w

(13)

Our task is given a series of input D, to estimate r+(y) and r−(y)
for y ∈ X as well as ﬁnding points with speciﬁc rank. Given these
notations, we deﬁne quantile summary of weighted examples as
follows:

Definition A.1. Quantile Summary of Weighted Data

A quantile summary for D is deﬁned to be tuple Q(D) = (S, ˜r+D, ˜r−
where S = {x1, x2, · · · , xk} is selected from the points in D (i.e.
xi ∈ {x|(x, w) ∈ D}) with the following properties:
imum point in D:

1) xi < xi+1 for all i, and x1 and xk are minimum and max-

D, ˜ωD),

x1 = min

(x,w)∈D x, xk = max

(x,w)∈D x

2) ˜r+D, ˜r−
˜r−
D(xi) ≤ r−

D and ˜ωD are functions in S → [0, +∞), that satisﬁes

D(xi), ˜r+D(xi) ≥ r+D(xi), ˜ωD(xi) ≤ ωD(xi),
the equality sign holds for maximum and minimum point ( ˜r−
r−
D(xi), ˜r+D(xi) = r+D(xi) and ˜ωD(xi) = ωD(xi) for i ∈ {1, k}).
Finally, the function value must also satisfy the following con-
straints
˜r−
D(xi) + ˜ωD(xi) ≤ ˜r−

D(xi+1), ˜r+D(xi) ≤ ˜r+D(xi+1) − ˜ωD(xi+1)

(14)

D(xi) =

(15)

Since these functions are only deﬁned on S, it is suﬃce to use 4k
record to store the summary. Speciﬁcally, we need to remember
each xi and the corresponding function values of each xi.

Definition A.2. Extension of Function Domains

Given a quantile summary Q(D) = (S, ˜r+D, ˜r−
D, ˜ωD) deﬁned in
Deﬁnition A.1, the domain of ˜r+D, ˜r−
D and ˜ωD were deﬁned only
in S. We extend the deﬁnition of these functions to X → [0, +∞)
as follows
When y < x1:

˜r−
D(y) = 0, ˜r+D(y) = 0, ˜ωD(y) = 0

When y > xk:

˜r−
D(y) = ˜r+D(xk), ˜r+D(y) = ˜r+D(xk), ˜ωD(y) = 0

When y ∈ (xi, xi+1) for some i:

D(xi) + ˜ωD(xi),

˜r−
D(y) = ˜r−
˜r+D(y) = ˜r+D(xi+1) − ˜ωD(xi+1),
˜ωD(y) = 0

(16)

(17)

(18)

Lemma A.1. Extended Constraint

The extended deﬁnition of ˜r−
constraints

D, ˜r+D, ˜ωD satisﬁes the following

˜r−
D(y) ≤ r−

D(y), ˜r+D(y) ≥ r+D(y), ˜ωD(y) ≤ ωD(y)

(19)

˜r−
D(y) + ˜ωD(y) ≤ ˜r−

D(x), ˜r+D(y) ≤ ˜r+D(x) − ˜ωD(x), for all y < x

(20)
Proof. The only non-trivial part is to prove the case when

y ∈ (xi, xi+1):

˜r−
D(y) = ˜r−

D(xi) + ˜ωD(xi) ≤ r−

D(xi) + ωD(xi) ≤ r−

D(y)

˜r+D(y) = ˜r+D(xi+1) − ˜ωD(xi+1) ≥ r+D(xi+1) − ωD(xi+1) ≥ r+D(y)

This proves Eq. (19). Furthermore, we can verify that

˜r+D(xi) ≤ ˜r+D(xi+1) − ˜ωD(xi+1) = ˜r+D(y) − ˜ωD(y)
˜r−
D(y) + ˜ωD(y) = ˜r−
D(xi+1)

D(xi) + ˜ωD(xi) + 0 ≤ ˜r−

˜r+D(y) = ˜r+D(xi+1) − ˜ωD(xi+1)

Using these facts and transitivity of < relation, we can prove
Eq. (20)

We should note that the extension is based on the ground case
deﬁned in S, and we do not require extra space to store the sum-
mary in order to use the extended deﬁnition. We are now ready
to introduce the deﬁnition of -approximate quantile summary.

Definition A.3. -Approximate Quantile Summary

Given a quantile summary Q(D) = (S, ˜r+D, ˜r−
-approximate summary if for any y ∈ X

D, ˜ωD), we call it is

˜r+D(y) − ˜r−

D(y) − ˜ωD(y) ≤ ω(D)

(21)

We use this deﬁnition since we know that r−(y) ∈ [˜r−
D(y), ˜r+D(y)−
˜ωD(y)] and r+(y) ∈ [˜r−
D(y) + ˜ωD(y), ˜r+D(y)]. Eq. (21) means the
we can get estimation of r+(y) and r−(y) by error of at most
ω(D).

Lemma A.2. Quantile summary Q(D) = (S, ˜r+D, ˜r−

D, ˜ωD) is an
-approximate summary if and only if the following two condition
holds

˜r+D(xi) − ˜r−

D(xi) − ˜ωD(xi) ≤ ω(D)

˜r+D(xi+1) − ˜r−

D(xi) − ˜ωD(xi+1) − ˜ωD(xi) ≤ ω(D)

(22)

(23)

Proof. The key is again consider y ∈ (xi, xi+1)

˜r+D(y)−˜r−

D(y)−˜ωD(y) = [˜r+D(xi+1)−˜ωD(xi+1)]−[˜r+D(xi)+˜ωD(xi)]−0

This means the condition in Eq. (23) plus Eq.(22) can give us
Eq. (21)

Property of Extended Function In this section, we have in-

troduced the extension of function ˜r+D, ˜r−

D, ˜ωD to X → [0, +∞).

The key theme discussed in this section is the relation of con-
straints on the original function and constraints on the extended
function. Lemma A.1 and A.2 show that the constraints on the
original function can lead to in more general constraints on the
extended function. This is a very useful property which will be
used in the proofs in later sections.
A.2 Construction of Initial Summary
we can construct initial summary Q(D) = {S, ˜r+D, ˜r−
to the set of all values in D (S = {x|(x, w) ∈ D}), and ˜r+D, ˜r−
D(x), ˜ωD(x) = ωD(x) for x ∈ S

Given a small multi-set D = {(x1, w1), (x2, w2), · · · , (xn, wn)},
D, ˜ωD}, with S
D, ˜ωD

˜r+D(x) = r+D(x), ˜r−

D(x) = r−

deﬁned to be

(24)
The constructed summary is 0-approximate summary, since it can
answer all the queries accurately. The constructed summary can
be feed into future operations described in the latter sections.
A.3 Merge Operation
In this section, we deﬁne how we can merge the two summaries
together. Assume we have Q(D1) = (S1, ˜r+D1
, ˜ωD1 ) and
Q(D2) = (S2, ˜r+D1
, ˜ωD2 ) quantile summary of two dataset
D1 and D2. Let D = D1 ∪ D2, and deﬁne the merged summary
Q(D) = (S, ˜r+D, ˜r−

D, ˜ωD) as follows.

, ˜r−
D1

, ˜r−
D2

S = {x1, x2 · · · , xk}, xi ∈ S1 or xi ∈ S2

The points in S are combination of points in S1 and S2. And the
function ˜r+D, ˜r−

D, ˜ωD are deﬁned to be

(25)

(26)

(27)

˜r−
D(xi) = ˜r−
D1
˜r+D(xi) = ˜r+D1
(xi)
˜ωD(xi) = ˜ωD1 (xi) + ˜ωD2 (xi)

(xi) + ˜r−
D2
(xi) + ˜r+D2

(xi)

(28)
Here we use functions deﬁned on S → [0, +∞) on the left sides of
equalities and use the extended function deﬁnitions on the right
sides.
Due to additive nature of r+, r− and ω, which can be formally

written as

D(y) =r−
r−
D1
r+D(y) =r+D1
(y),
ωD(y) =ωD1 (y) + ωD2 (y),

(y) + r−
D2
(y) + r+D2

(y),

(29)

and the extended constraint property in Lemma A.1, we can verify
that Q(D) satisﬁes all the constraints in Deﬁnition A.1. Therefore
it is a valid quantile summary.

Lemma A.3. The combined quantile summary satisﬁes

D(y) = ˜r−
˜r−
D1
˜r+D(y) = ˜r+D1
(y)
˜ωD(y) = ˜ωD1 (y) + ˜ωD2 (y)

(y) + ˜r−
D2
(y) + ˜r+D2

(y)

(30)

(31)

(32)

for all y ∈ X

Algorithm 4: Query Function g(Q, d)
Input: d: 0 ≤ d ≤ ω(D)
Input: Q(D) = (S, ˜r+D, ˜r
−
D, ˜ωD) where
S = x1, x2,··· , xk
−
D(x1) + ˜r+D(x1)] then return x1 if
if d < 1
2 [˜r
d ≥ 1
−
D(xk) + ˜r+D(xk)] then return xk Find i such
2 [˜r
D(xi) + ˜r+D(xi)] ≤ d < 1
−
−
D(xi+1) + ˜r+D(xi+1)]
that 1
2 [˜r
2 [˜r
D(xi) + ˜ωD(xi) + ˜r+D(xi+1) − ˜ωD(xi+1) then
−
if 2d < ˜r

return xi

else

return xi+1

end

This can be obtained by straight-forward application of Deﬁni-
tion A.2.

Theorem A.1. If Q(D1) is 1-approximate summary, and Q(D2)

is 2-approximate summary. Then the merged summary Q(D) is
max(1, 2)-approximate summary.
Proof. For any y ∈ X , we have
˜r+D(y) − ˜r−
=[˜r+D1
≤1ω(D1) + 2ω(D2) ≤ max(1, 2)ω(D1 ∪ D2)

D(y) − ˜ωD(y)
(y)] − [˜r−
D1

(y)] − [˜ωD1 (y) + ˜ωD2 (y)]

(y) + ˜r−
D2

(y) + ˜r+D2

Here the ﬁrst inequality is due to Lemma A.3.
A.4 Prune Operation

Before we start discussing the prune operation, we ﬁrst in-
troduce a query function g(Q, d). The deﬁnition of function is
shown in Algorithm 4. For a given rank d, the function returns
a x whose rank is close to d. This property is formally described
in the following Lemma.

Lemma A.4. For a given -approximate summary Q(D) =

D, ˜ωD), x∗ = g(Q, d) satisﬁes the following property

(S, ˜r+D, ˜r−

d ≥ ˜r+D(x∗) − ˜ωD(x∗) − 
d ≤ ˜r−
D(x∗) + ˜ωD(x∗) +

2

2

ω(D)
ω(D)

(33)

2 [˜r−

Proof. We need to discuss four possible cases
• d < 1

D(x1) + ˜r+D(x1)] and x∗ = x1. Note that the rank
information for x1 is accurate (˜ωD(x1) = ˜r+D(x1) = ω(x1),
˜r−
D(x1) = 0), we have

2

ω(D) = ˜r+D(x1) − ˜ωD(x1) − 

d ≥ 0 − 
2
[˜r−
D(x1) + ˜r+D(x1)]

1
d <
2
≤ ˜r−
D(x1) + ˜r+D(x1)
= ˜r−
D(x1) + ˜ω+D(x1)
D(xk) + ˜r+D(xk)] and x∗ = xk, then

• d ≥ 1

[˜r−
D(xk) + ˜r+D(xk)]

2 [˜r−
d ≥ 1
2
= ˜r+D(xk) − 1
= ˜r+D(xk) − 1
d < ω(D) +

2


2

[˜r+D(xk) − ˜r−

D(xk)]

˜ωD(xk)
2
ω(D) = ˜r−

D(xk) + ˜ωD(xk) +

ω(D)

ω(D)


2

• x∗ = xi in the general case, then

2d < ˜r−
= 2[˜r−
≤ 2[˜r−
2d ≥ ˜r−

D(xi) + ˜ωD(xi) + ˜r+D(xi+1) − ˜ωD(xi+1)
D(xi) + ˜ωD(xi)] + [˜r+D(xi+1) − ˜ωD(xi+1) − ˜r−
D(xi) + ˜ωD(xi)] + ω(D)
D(xi) + ˜r+D(xi)

= 2[˜r+D(xi) − ˜ωD(xi)] − [˜r+D(xi) − ˜ωD(xi) − ˜r−
≥ 2[˜r+D(xi) − ˜ωD(xi)] − ω(D) + 0

D(xi)] + ˜ωD(xi)

D(xi) − ˜ωD(xi)]

• x∗ = xi+1 in the general case

2d ≥ ˜r−

D(xi) + ˜ωD(xi) + ˜r+D(xi+1) − ˜ωD(xi+1)

= 2[˜r+D(xi+1) − ˜ωD(xi+1)]

− [˜r+D(xi+1) − ˜ωD(xi+1) − ˜r−

D(xi) − ˜ωD(xi)]

≥ 2[˜r+D(xi+1) + ˜ωD(xi+1)] − ω(D)

2d ≤ ˜r−
= 2[˜r−

D(xi+1) + ˜r+D(xi+1)

D(xi+1) + ˜ωD(xi+1)]

+ [˜r+D(xi+1) − ˜ωD(xi+1) − ˜r−

D(xi+1)] − ˜ωD(xi+1)

≤ 2[˜r−

D(xi+1) + ˜ωD(xi+1)] + ω(D) − 0

Now we are ready to introduce the prune operation. Given a

quantile summary Q(D) = (S, ˜r+D, ˜r−
elements, and a memory budget b. The prune operation creates
another summary Q(cid:48)(D) = (S(cid:48), ˜r+D, ˜r−
where x(cid:48)

D, ˜ωD) with S = {x1, x2, · · · , xk}
2, · · · , x(cid:48)
D, ˜ωD) with S(cid:48) = {x(cid:48)

i are selected by query the original summary such that

1, x(cid:48)

b+1},

(cid:18)

i − 1

(cid:19)

.

b

Q,

ω(D)

x(cid:48)
i = g
The deﬁnition of ˜r+D, ˜r−
D, ˜ωD in Q(cid:48) is copied from original sum-
mary Q, by restricting input domain from S to S(cid:48). There could
be duplicated entries in the S(cid:48). These duplicated entries can be
safely removed to further reduce the memory cost. Since all the
elements in Q(cid:48) comes from Q, we can verify that Q(cid:48) satisﬁes all
the constraints in Deﬁnition A.1 and is a valid quantile summary.
Theorem A.2. Let Q(cid:48)(D) be the summary pruned from an
-approximate quantile summary Q(D) with b memory budget.
Then Q(cid:48)(D) is a ( + 1

b )-approximate summary.

Proof. We only need to prove the property in Eq. (23) for Q(cid:48).

Using Lemma A.4, we have

i − 1

b

i − 1

b

ω(D) +


2
ω(D) − 
2

ω(D) ≥ ˜r+D(x(cid:48)
ω(D) ≤ ˜r−
D(x(cid:48)

i) − ˜ωD(x(cid:48)
i)
i) + ˜ωD(x(cid:48)
i)

Combining these inequalities gives

˜r+D(x(cid:48)
≤[

i
b

i+1) − ˜ωD(x(cid:48)

ω(D) +

ω(D)] − [


2

i+1) − ˜r−
i − 1

i) − ˜ωD(x(cid:48)
i)

D(x(cid:48)
ω(D) − 
2

b

ω(D)] = (

1
b

+ )ω(D)

