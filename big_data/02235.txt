6
1
0
2

 
r
a

M
7

 

 
 
]

.

R
P
h
t
a
m

[
 
 

1
v
5
3
2
2
0

.

3
0
6
1
:
v
i
X
r
a

Large deviations and Berry-Esseen bounds for hashing with linear

probing

T. Klein, A. Lagnoux, and P. Petit∗

Abstract

We study the asymptotic behavior of a sum of independent and identically distributed random variables
conditioned by a sum of independent and identically distributed integer-valued random variables. First,
we prove a large deviations result in the context of hashing with linear probing. By the way, we
establish a large deviations result for triangular arrays when the Laplace transform is not deﬁned in a
neighborhood of 0. Second, we prove a Berry-Esseen bound in a general setting.
Keywords: Berry-Esseen bound ; large deviations ; conditional distribution ; combinatorial problems
; hashing with linear probing.
AMS MSC 2010: 60F10; 60F05; 62E20; 60C05; 68W40.

1

Introduction

As pointed out by Svante Janson in his seminal work [13], in many random combinatorial problems, the in-
teresting statistic is the sum of independent and identically distributed (i.i.d.) random variables conditioned
on some exogenous integer-valued random variable. In general, the exogenous random variable is itself a sum
of integer-valued random variables. More precisely, we are interested in the law of N−1
)
conditioned on a speciﬁc value of X (n)

that is to say in the conditional distribution

1 + ··· + Y (n)

n (Y (n)

Nn

Ln := L(N−1

) | X (n)

1 + ··· + X (n)

Nn

= mn),

1 + ··· + X (n)
n (Y (n)

Nn

1 + ··· + Y (n)
, Y (n)

Nn

i

i

)n∈N∗,16i6Nn be i.i.d. copies of a pair (X (n), Y (n)) of random

where mn and Nn are integers and (X (n)
variables with X (n) integer-valued.
Hashing with linear probing was the motivating example for Janson’s work [13]. This model comes from
theoretical computer science, where it modelizes the time cost to store data in the memory. Then, it was
introduced in a mathematical framework by Knuth [17]. Due to its strong connection with parking functions,
the Airy distributions (i.e., the area under the brownian excursion) and the Lukasiewicz random walks [20],
this model was studied by many authors (see, e.g., Flajolet, Poblete and Viola [8], Janson [12, 14, 15],
Chassaing et al. [1, 2, 3], and Marckert [22]).
In his work, Janson proves a general central limit theorem (with convergence of all moments) for this kind of
conditional distribution under some reasonable assumptions and gives several applications in classical com-
binatorial problems: occupancy in urns, hashing with linear probing, random forests, branching processes,
etc. Following this work, at least two natural questions arise:

1. Is it possible to obtain a general large deviations result for these models?

2. Is it possible to obtain a general Berry-Esseen bound for these models?

∗Institut de Math´ematiques de Toulouse, France. firstname.lastname@math.univ-toulouse.fr

1

Partial answer to the ﬁrst question When the distribution of (X (n), Y (n)) does not depend on n, the
Gibbs conditioning principle ([30, 4, 5]) states that Ln converges weakly to the degenerated distribution
concentrated on a point depending on the conditioning value (see [9, Corollary 2.2]). Around the Gibbs
conditioning principle, general limit theorems yielding the asymptotic behavior of the conditioned sum are
given in [29, 11, 19] and asymptotic expansions are proved in [10, 28].
An extension to arrays has been proposed by Gamboa, Klein and Prieur [9]. They prove a large (and a
moderate) deviation principle under some strong assumptions. The most restricting assumption is that the
joint Laplace transform of (X (n), Y (n)) is ﬁnite at least in a neighborhood of (0, 0). This assumption is
satisﬁed by all the examples considered in [13] except for hashing with linear probing, which is the most
interesting one. Indeed, in this case, the joint Laplace transform is only deﬁned in (−∞, a] × (−∞, 0] for
some positive a.
More generally, it would be interesting to get large deviations results for a larger class of models the Laplace
transforms of which are not deﬁned. In [23, 24], Nagaev establishes large deviations results for sums of i.i.d.
random variables which are absolutely continuous with respect to the Lebesgue measure and the Laplace
transform of which is not deﬁned in a neighborhood of 0. Following this work, we extend his result and prove
a large deviations result (Theorem 3.3) for arrays. It is then natural to consider the asymptotic behavior of
conditioned sums and to extend the work of [9] to models the Laplace transforms of which are not deﬁned.
Proving a theorem for a general class of models seems to be a very diﬃcult task. That is why, we restrict
ourselves to the study of hashing with linear probing (Theorem 3.1).
Let us point out the main diﬀerences between the large deviations result of the present work and [9,
Theorem 2.1]. First, the proof in [9, Theorem 2.1] is based on a sharp control of a Fourier-Laplace transform

ΦX (n),Y (n) (t, u) := E(cid:0)exp[itX (n) + uY (n)](cid:1) of(cid:0)X (n), Y (n)(cid:1). The Fourier part allows to treat the conditioning

whereas the Laplace one allows to apply G¨artner-Ellis theorem. In the present paper, the proof follows ideas
borrowed from [23, 24]. Contrary to the case when the Laplace transform is deﬁned, the large deviations
of the sum of the random variables with heavy-tailed distributions is due to exceptional values taken by
few random variables. Second, unlike the classical speeds in Nn obtained either in Cram´er’s theorem or
in Theorem 2.1 of [9], the speed in this paper is √Nn. Third, oscillations of the tails are allowed (in a
controlled range) and may aﬀect the large deviation bounds. When the Laplace transform is deﬁned, the
tails are controlled (see Cram´er’s theorem or G¨artner-Ellis theorem in [5]) and the sum satisﬁes a large
deviation principle with the same lower and upper bounds.

Complete answer to the second question The ﬁrst Berry-Esseen theorem for conditional models is
given by Quine and Robinson [27]. In their work, the authors study the particular case of the occupancy
problem, i.e. the case when the random variables X (n) are Poisson distributed and Y (n) = 1{X (n)=0}. Up to
our knowledge, it is the only result in that direction for this kind of conditional distribution. In our work,
we prove a general Berry-Esseen bound (Theorem 4.1) that covers all the examples presented by Janson
[13].

In Section 2, we present the general
Organization of the paper The paper is organized as follows.
model and describe precisely the framework of hashing with linear probing. Section 3 is devoted to the large
deviations result for hashing. A Berry-Esseen bound (Theorem 4.1) is stated in Section 4, which applies to
the examples presented by Janson [13]. Finally, the last section is dedicated to the proofs.

2 The model

2.1 A general framework for conditional distributions
In the whole paper, N∗ = {1, 2, . . .} is the set of positive integers, N = N∗ ∪ {0}, and Z is the set of all
integers. For all n > 1, we consider a pair of random variables (cid:0)X (n), Y (n)(cid:1) such that X (n) is integer-
valued and Y (n) real-valued. Let Nn be a natural number such that Nn → ∞ as n goes to inﬁnity. Let

2

i

(cid:16)X (n)

, Y (n)

i (cid:17)16i6Nn

be an i.i.d. sample distributed as (cid:0)X (n), Y (n)(cid:1) and deﬁne

k

k

S(n)
k

:=

Xi=1

X (n)

i

and T (n)

k

:=

Y (n)
i

,

Xi=1

for k ∈ {1, . . . , Nn}. Let mn ∈ Z be such that P(S(n)

asymptotic behavior of the conditional distribution

Nn

= mn) > 0. The purpose of the paper is to derive the

Ln := L((Nn)−1T (n)

Nn |S(n)

Nn

= mn).

2.2 Classical examples

In this section, we give several examples.

2.2.1 Occupancy problem

In the classical occupancy problem (see [13] and the references therein for more details), m balls are dis-
tributed at random into N urns. The resulting numbers of balls (Z1, . . . , ZN ) have a multinomial distribu-
tion. It is well known that (Z1, . . . , ZN ) is also distributed as (X1,··· , XN ) conditioned on PN
i=1 Xi = m,
where X1, ..., XN are i.i.d. with Xi ∼ P(λ), for any arbitrary λ > 0. The classical occupancy problem
studies the number of empty urns which is distributed as PN
Let m = mn → ∞ and N = Nn → ∞ with λn := mn/Nn → λ. Following the work of Janson [13], we
will study the asymptotic behavior of T (n)
=
Nn
PNn

i=1 1{Xi=0} conditioned on PN
(Y (n) = 1{X (n)=0}) conditioned on S(n)

i = mn with X (n) ∼ P(λn).

= PNn

i=1 Xi = m.

i=1 X (n)

i=1 1{X (n)

i =0}

Nn

2.2.2 Bose-Einstein statistics

This example is borrowed from [11], see also [6]. Consider N urns. Put n indistinguishable balls in the urns
in such a way that each distinguishable outcome has the same probability

Let Zk be the number of balls in the kth urn.

It is well known that (Z1, . . . , ZN ) is distributed as

1/(cid:18)n + N − 1
(cid:19) .

n

(cid:0)X1,··· , XN(cid:1) conditioned on nPN
Assume m = mn = n → ∞, N = Nn → ∞ with Nn/n → p, and take X (n)
with parameter pn = Nn/n.

with any parameter p. The framework is similar to the one of Subsection 2.2.1 and we proceed analogously.
having geometric distribution

i=1 Xi = no, where X1,··· , XN are i.i.d. and geometrically distributed

i

2.2.3 Branching processes

Consider a Galton-Watson process, beginning with one individual, where the number of children of an
individual is given by a random variable X having ﬁnite moments. Assume further that E[X] = 1. We
number the individuals as they appear. Let Xi be the number of children of the ith individual. It is well
known (see [13, Example 3.4] and the references therein) that the total progeny is n > 1 if and only if

Sk :=

k

Xi=1

Xi > k for 0 6 k < n but Sn = n − 1 .

(1)

This type of conditioning is diﬀerent from the one studied in the present paper, but by [31, Corollary
2] and [13, Example 3.4], if we ignore the order of X1, . . . , Xn, it is proven that they have the same

3

distribution conditioned on (1) as conditioned on Sn = (n − 1). Hence our results apply to variables of
the kind Yi = f (Xi). For example, if Yi = 1{Xi=3}, the sum Pn
i=1 Yi is the number of families with
three children. The framework is similar to the one of Subsection 2.2.1 and we proceed analogously with
m = mn = n − 1 → ∞, N = Nn = n → ∞.

2.2.4 Random forests

Consider a uniformly distributed random labeled rooted forest with m vertices and N < m roots. Without
loss of generality, we may assume that the vertices are 1, . . . , m and, by symmetry, that the roots are the
ﬁrst N vertices. Following [13], this model can be realized as follows. The sizes of the N trees in the forest
i=1 Xi = m, where Xi are i.i.d. as the Borel distribution

are distributed as X1, . . . , XN conditioned on PN
with some arbitrary parameter λ ∈ (cid:0)0, e−1(cid:3) that is deﬁned in the following way

P(Xi = l) =

1

λlll−1

T (λ)

l!

,

where T is the tree function (see, e.g., [8] or [12] for more details). Then, tree number i is drawn uniformly
among the trees of size Xi.
A classical quantity of interest is the number of trees of size K in the forest (see, e.g., [18, 25, 26]). It means
that we consider Yi = 1{Xi=K}. Let us now assume that we condition on PN
i=1 Xi = m. The framework is
similar to the one of Subsection 2.2.1 and we proceed analogously. Assume m = mn → ∞, N = Nn → ∞
with mn/Nn → λ, and take X (n)
2.3 Hashing with linear probing

having Borel distribution with parameter λn = mn/Nn.

i

Hashing with linear probing is a classical model in theoretical computer science that appears in the 60’s. It
has been studied from a mathematical point of view ﬁrstly in [16] and then by several authors. For more
details on the model, we refer to [8, 12, 14, 2, 22]. The model describes the following experiment. One
throws n balls sequentially into m urns at random; the urns are arranged in a circle and numbered clockwise.
A ball that lands in an occupied urn is moved to the next empty urn, always moving clockwise. The length
of the move is called the displacement of the ball and we are interested in the sum of all displacements
which is a random variable denoted dm,n. We assume n < m.
In order to make things clear, let us give an example. Assume that n = 8, m = 10, and (6, 9, 1, 9, 9, 6, 2, 5)
are the addresses where the balls land. This sequence of addresses is called a hash sequence of length m
and size n. Let di be the displacement of ball i. Then d1 = d2 = d3 = 0. The ball number 4 should land
in the 9th urn which is occupied by the second ball; thus it is moved one step ahead and lands in 10th urn
so that d4 = 1. The ball number 5 should land in the 9th urn, which is occupied like the 10th and the
ﬁrst one, so that d5 = 3. And so on: d6 = 1, d7 = 1, d8 = 0. Here, the total displacement is equal to
1 + 3 + 1 + 1 = 6. After throwing all balls, there are N := m− n empty urns. These divide the occupied urns
into blocks of consecutive urns. For convenience, we consider the empty urn following a block as belonging
to this block. In our example, there are two blocks: the ﬁrst one containing urns 9, 10, 1, 2, 3 (occupied) and
urn 4 (empty), and the second one containing urns 5, 6, 7 (occupied) and urn 8 (empty).
Janson [12] prove that the lengths of the blocks (counting the empty urn) and the sums of displacements
i=1 Xi = m, where (Xi, Yi) are
i.i.d. copies of a pair (X, Y ) of random variables, X having the Borel distribution with arbitrary parameter

inside each block are distributed as (X1, Y1), . . . , (XN , YN ) conditioned onPN
λ ∈ (cid:0)0, e−1(cid:1) and Y given X = l being distributed as dl,l−1. For the ease of computation, we use the

parametrization λ = e−µµ to get an equivalent deﬁnition of the Borel distribution

P(X = l) = e−µl (µl)l−1

l!

, µ ∈ (0, 1)

(see section 5.3 for more details on Borel distribution and references therein). Notice that the conditional
distribution of Y given X does not depend on the parameter µ.

4

Hence, dm,n is distributed as PN

i=1 Yi conditioned on PN

results on the total displacement dm,n that will be useful in the proofs.

i=1 Xi = m. The following lemma states basic

Lemma 2.1.

1. The number of hash sequences of length m and size n is mn.

2. One has 0 6 dm,n 6

n(n−1)

2

.

3. The total displacement of any hash sequence (h1, . . . , hn) is invariant with respect to any permutation
of the h′is. More precisely for any permutation σ of {1, . . . , n}, the total displacement associated to
the hash sequence (h1, . . . , hn) is the same as the total displacement associated to the hash sequence
(hσ(1), . . . , hσ(n)).

The ﬁrst two points are obvious and the last one is a consequence of [12, Lemma 2.1].
From now on, we assume that m = mn → ∞ and N = Nn = mn − n → ∞ with µn := n/mn ∈ (0, 1) → µ ∈
(0, 1), as in Subsection 2.2.1. Let (X (n)
)16i6Nn be i.i.d. copies of (X (n), Y (n)), X (n) following Borel
distribution with parameter µn, and Y (n) given X (n) = l being distributed as dl,l−1. The total displacement
dmn,n is distributed as the conditional distribution of T (n)
Nn
Remark 2.2. The local limit theorem stated in Proposition 5.2 is crucial in the proofs of the large deviations
result (Theorem 3.1) and the one of the Berry-Esseen bound (Theorem 4.1) and requires

given S(n)
Nn

, Y (n)

= mn.

i

i

If one takes µn = µ (i.e. X (n) and Y (n) do not depend on n), the convergence µn → µ only gives

mn = NnE[X (n)] + O(pNn).

mn = Nn(cid:18) 1
1 − µ

+ o(1)(cid:19) = NnE[X (n)] + o(Nn).

So triangular arrays are needed. Therefore, one may choose µn = n/mn, so that mn = NnE[X (n)].
Also notice that, in the proof of the lower bound in Theorem 3.1, one has to establish

P(S(n)
Nn

= m′n) >

c

2πσX (n) N 1/2

n

with m′n 6= mn in Proposition 5.2.

3 Large deviations result for hashing with linear probing

In [9], the authors prove a classical large deviation principle for the conditional distribution Ln which ap-
plies to Subsections 2.2.1 to 2.2.4. The proof relies on G¨artner-Ellis theorem which requires the existence
of the Laplace transform in a neighborhood of the origin. In the context of hashing with linear probing,
using the results in [8, 13, 12], we can prove that the joint Laplace transform of (X, Y ) is only deﬁned on
[−∞, a] × [−∞, 0] for some positive a. Hence, [9, Theorem 2.1] does not apply. Consequently, one needs
a speciﬁc result in the case when the Laplace transform is not deﬁned. Working in a general framework
appears to be diﬃcult. Nevertheless, in the particular case of hashing with linear probing, we establish the
following theorem.

Theorem 3.1 (Large deviations result for hashing with linear probing). If n/mn → µ ∈ (0, 1), there exists
0 < α(µ) 6 β(µ) such that, for all y > 0,

−β(µ)√y 6 lim inf

n→∞

1
√n
1
√n

6 lim sup
n→∞

log P(dmn,n − E[dmn,n] > ny)
log P(dmn,n − E[dmn,n] > ny) 6 −α(µ)√y.

5

Remark 3.2. In the proof, we exhibit

α(µ) = (1 + log(µ) − µ)√2

and β(µ) = 4 + log(2) + 2 log(µ) − 2µ.

It is still an open question whether we can take α(µ) = β(µ).

Since Nn/n → (1 − µ)/µ, the theorem can equivalently be stated as follows:
= mn] > Nny|S(n)
= mn] > Nny|S(n)

Nn − E[T (n)
Nn − E[T (n)

−β(µ)√y 6 lim inf

Nn |S(n)
Nn |S(n)

1
√Nn
1
√Nn

log P(T (n)

log P(T (n)

Nn

Nn

n→∞

6 lim sup
n→∞

Nn

Nn

= mn)

= mn) 6 −α(µ)√y.

We will prove the result in the latter form.
The following proposition is a non conditioned version of Theorem 3.1 in a general framework. In fact, it is
a generalization to triangular arrays of [23, Theorem 3]. For the sake of simplicity, we focus on rough large
deviations results instead of precise ones.

Proposition 3.3. For all n > 1, let Y (n) be a real-valued random variable, Nn be an integer, (cid:0)Y (n)
be i.i.d. copies of Y (n), and zn be a positive number. Suppose that Nn → ∞ and that:
(H3.3.1) lim inf zn/Nn > 0;

i

(cid:1)16i6Nn

n (cid:17);
(H3.3.2) Var(Y (n)) = o(cid:16)N 1/2

(H3.3.3) the right tail of Y (n) satisﬁes: there exist 0 < α 6 β such that

lim inf
n→∞

1
√zn

log P(Y (n) > zn) > −β

lim sup
n→∞

sup
u>√zn

1
√u

log P(Y (n) > u) 6 −α.

(2)

(3)

and

Then,

−β 6 lim inf
n→∞

6 lim sup
n→∞

1
√zn
1
√zn

log P(T (n)

] > zn)

Nn

Nn − E[T (n)
Nn − E[T (n)

Nn

log P(T (n)

] > zn) 6 −α.

Proposition 3.4. Let Y (n) be the random variable appearing in the context of hashing with linear probing.
Then,

−β(µ) 6 lim inf
n→∞

6 lim sup
n→∞

1

√Nny
sup

u>√Nny

log P(Y (n) > Nny)

1
√u

log P(Y (n) > u) 6 −α(µ)

(4)

(5)

with

α(µ) = (1 + log(µ) − µ)√2

and β(µ) = 4 + log(2) + 2 log(µ) − 2µ.

6

4 Conditional Berry-Esseen bound

We come back to the general framework of Subsection 2.1. Let also Un be a random variable distributed as
T (n)
Nn

conditioned on S(n)
Nn

= mn.

Theorem 4.1. Suppose that there exist positive constants ˜c1, c1, c2, ˜c3, c3, c4, c5, and c6 such that:

(H4.1.1) ˜c1 6 σX (n) := Var(cid:0)X (n)(cid:1)1/2
(H4.1.2) ρX (n) := Eh(cid:12)(cid:12)X (n) − E(cid:2)X (n)(cid:3)(cid:12)(cid:12)

(H4.1.3) for Y

6 c1;

2σ3

X (n) ;

3i 6 c3

′(n) := Y (n) − X (n) Cov(X (n), Y (n))/σ2

and t ∈ [0, η0],

n );

′ (n))i(cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12)
Ehei(sX (n)+tY
(H4.1.4) mn = NnE(cid:2)X (n)(cid:3) + O(σX (n) N 1/2
(H4.1.5) ˜c3 6 σY (n) := Var(cid:0)Y (n)(cid:1)1/2
3i 6 c3
(H4.1.6) ρY (n) := Eh(cid:12)(cid:12)Y (n) − E(cid:2)Y (n)(cid:3)(cid:12)(cid:12)
(H4.1.7) the correlation rn := Cov(cid:0)X (n), Y (n)(cid:1) σ−1
X (n) σ−1
Y (n) (1 − r2

n := σ2
τ 2

Y (n);

6 c3;

4σ3

X (n) , there exists η0 > 0 such that, for all s ∈ [−π, π]
6 1 − c5(cid:0)σ2

Y ′(n) t2(cid:1);

X (n) s2 + σ2

Y (n) satisﬁes |rn| 6 c6 < 1, so that
n) > ˜c2

6) > 0.

2(1 − c2

Then the following conclusions hold.

4.1.a. There exists ˜c5 > 0 such that

P(S(n)
Nn

= mn) >

˜c5

2πσX (n) N 1/2

n

.

4.1.b. For Nn > N0 := max(3, c6

2, c6

4), the conditional distribution of

N−1/2

n

n (T (n)
τ−1

Nn − NnE[Y (n)] − rn

σY (n)
σX (n)

(mn − NnE[X (n)]))

on {S(n)

Nn

= mn} satisﬁes the Berry-Esseen inequality

sup

x (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
P  Un − NnE(cid:2)Y (n)(cid:3) − rnσY (n) σ−1

N 1/2

n τn

X (n) (mn − NnE(cid:2)X (n)(cid:3))

6 x! − Φ(x)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

where Φ denotes the standard normal cumulative distribution function and C is a positive constant
that only depends on ˜c1, c1, c2, ˜c3, c3, c4, c5, ˜c5, and c6.

6

C

N 1/2

n

,

(6)

4.1.c. Moreover, there exists two positive constants c7 and c8 only depending on ˜c1, c1, c2, ˜c3, c3, c4,

c5, ˜c5, and c6 such that

and

(cid:12)(cid:12)(cid:12)(cid:12)

E [Un] − NnE[Y (n)] − rn

σY (n)
σX (n)

(mn − NnE[X (n)])(cid:12)(cid:12)(cid:12)(cid:12)
n(cid:12)(cid:12) 6 c8N 1/2

n .

(cid:12)(cid:12)Var (Un) − Nnτ 2

7

6 c7

(7)

(8)

If Nn > ˜N0 := max(N0, 4c2

8/˜c2

3), we also have

sup

x (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
P  Un − E [Un]

Var (Un)1/2

6 x! − Φ(x)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

6

˜C

N 1/2

n

,

(9)

where ˜C is a constant that only depends on ˜c1, c1, c2, ˜c3, c3, c4, c5, ˜c5, and c6.

Remark 4.2.

1. The fact that Nn → ∞ is only required for the existence of the constant ˜c5 which relies on Lebesgue

dominated convergence theorem.

2. The set of hypotheses of Theorem 4.1 implies the one of the central limit theorem established in [13,

Theorem 2.1] which is clearly not surprising.

3. By Hypothesis (H4.1.4), the conditioning value is approximately equal to the mean as in the central

limit theorem given in [13, Theorem 2.3].

4. As a consequence of Lemma 5.5 below, ˜c1 can be chosen as c−3

2 /4.

5. Hypothesis (H4.1.7) is not very restricting and holds in the examples provided in Subsection 2.2.

6. One should note that 4.1.a is the analog of Equation (3.7) in [9, Lemma 3.2].

7. Following [13], we introduce Y

′(n) in order to work with a centered variable which is also uncorrelated

with X (n).

8. If (X, Y ′) is a pair of random variables such that the correlation r satisﬁes |r| < 1, then

(cid:12)(cid:12)(cid:12)

Ehei(sX+tY ′)i(cid:12)(cid:12)(cid:12)

= 1 −
6 1 −

1

2(cid:0)σ2
1 − |r|

2

X s2 + 2σX σY ′rst + σ2

Y ′ t2(cid:1) + o(s2 + t2)

(cid:0)σ2

X s2 + σ2

Y ′ t2(cid:1) + o(s2 + t2),

so Hypothesis (H4.1.3) is reasonable for i.i.d. sequences.

As in [13], the result simpliﬁes considerably in the special case when the pair (X (n), Y (n)) does not depend
on n, that is to say when we consider an i.i.d. sequence instead of a triangular array. This is a consequence
of the following corollary.

Corollary 4.3. Assume that (cid:0)X (n), Y (n)(cid:1) (d)

→ (X, Y ) as n → ∞ and that, for every ﬁxed r > 0,

lim sup
n→∞

Eh|X (n)|ri < +∞ and

lim sup
n→∞

Eh|Y (n)|ri < +∞.

Suppose further that the distribution of X has span 1 and that Y is not a.s. equal to an aﬃne function

c + dX of X. Let mn and Nn be integers such that E(cid:2)X (n)(cid:3) = mn/Nn and Nn → ∞. Then, all hypotheses

of Theorem 4.1 are satisﬁed and Theorem 4.1 holds.

Each example presented in Subsection 2.2, including hashing with linear probing, satisﬁes the assumptions
of Corollary 4.3, as shown in [13], leading to a Berry-Esseen bound for all of them.

8

5 Proofs

5.1 Technical results

The proofs of Theorems 3.1 and 4.1 intensively rely on the use of Fourier transforms. Deﬁne ϕn and ψn by

ϕn(s, t) := Ehexpnis(cid:16)X (n) − EhX (n)i(cid:17) + it(cid:16)Y (n) − EhY (n)i(cid:17)oi

= mn)Ehexpnit(cid:16)Un − NnEhY (n)i(cid:17)oi .

and ψn(t) := 2πP(S(n)
Nn

(10)

(11)

In this ﬁrst subsection, we establish some properties of these two functions. First notice that ϕn(s, 0) =

e−isE[X (n)]EheisX (n)i and ψn(0) = 2πP(S(n)

Nn

Lemma 5.1. One has

= mn).

ψn(t) =

1

σX (n) N 1/2

n

Z πσ

−πσ

X(n) N 1/2

n

X(n) N 1/2

n

e−isσ−1

X(n) N −1/2

n

(mn−NnE[X (n)])ϕNn

n  

s

σX (n) N 1/2

n

, t! ds.

(12)

Proof. Since

we have

Z π

−π

eis(S(n)

Nn−mn)ds = 2π1{S(n)

Nn

,

=mn}

ψn(t) = 2πP(S(n)
Nn

= mn)Ehexpnit(cid:16)Un − NnEhY (n)i(cid:17)oi
=mn}i

Nn − NnEhY (n)i(cid:17)o 1{S(n)
Nn − mn(cid:17) + it(cid:16)T (n)

Nn

Nn − NnEhY (n)i(cid:17)oi ds

= 2πEhexpnit(cid:16)T (n)
=Z π
Ehexpnis(cid:16)S(n)
=Z π

−π

−π

e−is(mn−NnE[X (n)])ϕNn

n (s, t)ds,

which leads to the result after the change of variable s′ = sσX (n) N 1/2
n .

Now we establish the local limit theorem (LLT) which is crucial both in the proofs of Theorem 3.1 and
Theorem 4.1.

Proposition 5.2 (LLT). We assume

2σ3

X (n) ;

2. for Y

3i 6 c3

′(n) := Y (n) − X (n) Cov(X (n), Y (n))/σ2

1. ρX (n) := Eh(cid:12)(cid:12)X (n) − E(cid:2)X (n)(cid:3)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)
Ehei(sX (n)+tY

t ∈ [0, η0],

′ (n))i(cid:12)(cid:12)(cid:12)

3. mn = NnE(cid:2)X (n)(cid:3) + O(σX (n) N 1/2

Then there exists c > 0 such that

X (n) , there exists η0 > 0 such that, for all s ∈ [−π, π] and
6 1 − c5(cid:0)σ2

Y ′(n) t2(cid:1);

X (n) s2 + σ2

n ) (remind that mn ∈ Z and P(S(n)

Nn

= mn) > 0);

c

2πσX (n) N 1/2

n

.

P(S(n)
Nn

= mn) >

9

Proof. Only consider the indices n for which σX (n) < +∞. Remember that ϕn(s, 0) = Eheis(X (n)−E[X (n)])i

and

ψn(0) = 2πP(S(n)
Nn

= mn) =

1

σX (n) N 1/2

n

Z πσ

−πσ

X(n) N 1/2

n

X(n) N 1/2

n

e−isvn ϕNn

n  

s

σX (n) N 1/2

n

, 0! ds

where vn =

mn−NnE[X (n)]

X(n) N 1/2
σ

n

, by Lemma 5.1. Let us prove that the sequence

(un)n =(cid:16)ψn(0)σX (n) N 1/2

n ev2

n/2(cid:17)

converges to √2π, from which the conclusion follows, since (vn)n is bounded by assumption 3. and P(S(n)
=
mn) > 0 for all n. Inequality (13) with l = 0 and t = 0 implies that the sequence (un)n is bounded. Let us
prove that √2π is the only accumulation point of (un)n. Let φ(n) such that (uφ(n))n converges. Even if it
means extracting more, we can suppose that (vφ(n))n converges. Let v = lim vφ(n). Using Taylor’s theorem,
one gets

Nn

s

σX (n) N 1/2

n

, 0! − 1 +

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
ϕn 

|s|3
X (n) N 3/2

n

6σ3

E(cid:20)(cid:12)(cid:12)(cid:12)

X (n) − EhX (n)i(cid:12)(cid:12)(cid:12)

Nn(cid:19)
3(cid:21) = o(cid:18) 1

where the last equality follows from assumption 1. Now,

e−isvφ(n) ϕ

Nφ(n)

φ(n)  

, 0! → e−isv−s2/2 = e−v2/2e−(s+iv)2/2

6

s2

2Nn(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
σX (φ(n))pNφ(n)

s

and, by Lebesgue dominated convergence theorem and the fact that σX (n) N 1/2

n → +∞ (see Lemma 5.5),

ψφ(n)(0)σX (φ(n))qNφ(n)ev2

φ(n)/2 →

√2π.

Now we give controls on the function ϕn and its second partial derivative.
Lemma 5.3. Under Hypothesis (H4.1.3), for any integer l > 0, |s| 6 πσX (n) N 1/2
6 e−(s2+t2)·c5·(Nn−l)/Nn .

ϕNn−l

s

t

,

n

(cid:18)

σX (n) N 1/2

n

σY (n)N 1/2

n , and |t| 6 η0σY (n) N 1/2
n ,

Proof. The proof is a mere consequence of the inequality 1 + x 6 ex that holds for any x ∈ R.
In the sequel, we also need diﬀerent controls on the ﬁrst partial derivative of ϕn with respect to the ﬁrst
variable.

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

n (cid:19)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

n !(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

Lemma 5.4. For any s and t, one has

and

∂ϕn

∂t  

s

σX (n) N 1/2

n

,

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

∂ϕn

s

∂t  
n !(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

t

σY (n) N 1/2

t

,

σX (n) N 1/2

n

σY (n) N 1/2

6

σY (n)
N 1/2

n

(|s| + |t|);

6

σY (n)
N 1/2

n

σY (n)

(|s|rn + |t|) +
X (n)(cid:19)1/3(cid:18) ρY (n)

Nn (cid:20) s2
Y (n)(cid:19)2/3

X (n)(cid:19)2/3(cid:18) ρY (n)
2 (cid:18) ρX (n)
2(cid:18) ρY (n)
Y (n)(cid:19)(cid:21).

σ3

σ3

σ3

σ3

t2

+

Y (n)(cid:19)1/3

+ |st|(cid:18) ρX (n)

σ3

10

(13)

(14)

(15)

Proof. We apply Taylor’s theorem to the function deﬁned by

(s, t) 7→ f (s, t) =

∂ϕn

∂t  

s

σX (n) N 1/2

n

,

t

n ! .

σY (n) N 1/2

We conclude to (14) using

|f (s, t) − f (0, 0)| 6 |s|

and to (15) using

|f (s, t) − f (0, 0)| 6 |s|(cid:12)(cid:12)(cid:12)(cid:12)

∂f
∂s

+ |st|

+ |t|

sup

∂f
∂s

θ,θ ′∈[0,1](cid:12)(cid:12)(cid:12)(cid:12)
(0, 0)(cid:12)(cid:12)(cid:12)(cid:12)
+ |t|(cid:12)(cid:12)(cid:12)(cid:12)
θ,θ ′∈[0,1](cid:12)(cid:12)(cid:12)(cid:12)

sup

(θs, θ′t)(cid:12)(cid:12)(cid:12)(cid:12)
(0, 0)(cid:12)(cid:12)(cid:12)(cid:12)
(θs, θ′t)(cid:12)(cid:12)(cid:12)(cid:12)

+

∂f
∂t
∂2f
∂t∂s

sup

∂f
∂t

θ,θ ′∈[0,1](cid:12)(cid:12)(cid:12)(cid:12)
θ,θ ′∈[0,1](cid:12)(cid:12)(cid:12)(cid:12)
θ,θ ′∈[0,1](cid:12)(cid:12)(cid:12)(cid:12)

t2
2

sup

sup

(θs, θ′t)(cid:12)(cid:12)(cid:12)(cid:12)
(θs, θ′t)(cid:12)(cid:12)(cid:12)(cid:12)
(θs, θ′t)(cid:12)(cid:12)(cid:12)(cid:12)

∂2f
∂2t

∂2f
∂2s

.

s2
2

+

Lemma 5.5. Under Hypothesis (H4.1.2), one has σX (n) > (4c3

2)−1.

Proof. The proof relies on the fact that, for any integer-valued random variable X (see [13, Lemma 4.1.]),

The conclusion follows, using Hypothesis (H4.1.2).

σ2

X 6 4Eh|X − E [X]|3i .

5.2 Proof of Proposition 3.3

Since Y (n) − E(cid:2)Y (n)(cid:3) also satisﬁes the hypotheses, we can assume that E(cid:2)Y (n)(cid:3) = 0. Write

P(T (n)
Nn

> zn) = P(T (n)
Nn

> zn,

∀i Y (n)

i < zn) + P(T (n)

Nn

> zn,

∃i Y (n)

i > zn)

:= PNn,0 + PNn,1.

If we prove that

−β 6 lim inf
n→∞

1
√zn

log(PNn,1) 6 lim sup
n→∞

1
√zn

log(PNn,1) 6 −α

1
√zn

lim sup
n→∞

log(PNn,0) 6 −α,

(16)

(17)

and

then,

−β 6 lim inf
n→∞

1
√zn

log(PNn,1) 6 lim inf
n→∞

1
√zn
1
√zn

log P(T (n)
Nn

> zn)

log P(T (n)
Nn

> zn) 6 −α

6 lim sup
n→∞

which establishes Proposition 3.3.

11

Proof of (16). First, using (3),

1
√zn

lim sup
n→∞

log(PNn,1) 6 lim sup
n→∞

1
√zn

log(cid:0)NnP(Y (n)

Nn

> zn)(cid:1) 6 −α.

Let us prove the converse inequality. Let ε > 0. We have

PNn,1 > P(T (n)
Nn
> P(T (n)

> zn,

Y (n)
1 > zn)

Nn−1 > −Nnε)P(Y (n) > zn + Nnε).

By Chebyshev’s inequality and Hypothesis (H3.3.2),

P(T (n)

Nn−1 > −Nnε) = 1 − P(cid:16)T (n)

Nn−1 < −Nnε(cid:17) > 1 −

σ2
Y (n)
Nnε2 → 1,

the random variables Y (n) being assumed centered. Finally, using (2) and (H3.3.1), and noting δ :=
lim inf
n→∞

, one gets

zn
Nn

lim inf
n→∞

1
√zn

log(PNn,1) > lim inf

n→∞ r zn + Nnε

zn

1

√zn + Nnε

log P(Y (n) > zn + Nnε) > −βr δ + ε

δ

.

Conclude by letting ε → 0.
Proof of 17. Let α′ ∈ (0, α) and sn = α′/√zn. The exponential Chebyshev’s inequality for T (n)
on {∀i, Y (n)

i < zn} yields

Nn

conditioned

If we prove that

then log(PNn,0) 6 −α′√zn + o(N 1/2

.

1Y (n)<zniNn
1Y (n)<zni = 1 + o(N−1/2

PNn,0 6 e−snzn EhesnY (n)
EhesnY (n)
n ) and the conclusion follows by letting α′ → α. Let η ∈]3/4, 1[. Write

),

n

EhesnY (n)
=Z

−∞

√zn

1Y (n)<zni
esnuP(Y (n) ∈ du) +Z zn−(zn)η

√zn

esnuP(Y (n) ∈ du) +Z zn

zn−(zn)η

esnuP(Y (n) ∈ du)

=: I1 + I2 + I3.

By a Taylor expansion of f (t) = et, (H3.3.2) and (H3.3.1), there exists

θ(u) 6 snu 6 sn√zn = α′

such that

√zn

−∞ (cid:18)1 + snu +
I1 6Z
6Z +∞
−∞ (cid:18)1 + snu +

s2
nu2
2
s2
nu2
2

eθ(u)(cid:19) P(Y (n) ∈ du)
eα′(cid:19) P(Y (n) ∈ du) = 1 + 0 +

α′2σ2
2zn

Y (n)

eα′

= 1 + o(N−1/2

n

).

12

Let n0 such that, for all n > n0 and u > √zn, log P(Y (n) > u) 6 −α′√u. Suppose n is larger than n0.

Integrating by part, we get

+ snZ zn−(zn)η

√zn

√zn

I2 = −hesnuP(Y (n) > u)izn−(zn)η
6 esn√zn P(Y (n) > √zn) + snZ zn−(zn)η
6 eα′(1−(zn)1/4) + snZ zn−(zn)η

√zn

√zn

exp(cid:18)α′(cid:18) u

esnu−α′√udu
√zn − √u(cid:19)(cid:19) du.

esnuP(Y (n) > u)du

Since, for all t ∈ [0, 1], √1 − t 6 1 − t/2, we get, for all u ∈ [√zn, zn − (zn)η] and n large enough to have
(zn)ν−1 < 1,

u

√zn − √u 6 √u(cid:16)p1 − (zn)η−1 − 1(cid:17) 6 −

(zn)η−3/4

2

.

Hence, I2 = o(N−1/2
Let α′′ ∈ (α′, α ∧ 2α′). Let n1 such that, for all n > n1 and u > zn − zη
Suppose n is larger than n1. Integrating by part, we get

).

n

n, log P(Y (n) > u) 6 −α′′√u.

I3 = −hesnuP(Y (n) > u)izn

n

zn−zη
n)P(Y (n) > zn − zη

6 esn(zn−zη

zn−zη

+ snZ zn
n) + snZ zn

n

zn−zη

n

esnuP(Y (n) > u)du

esnu−α′′√udu.

Now, since √t > t if t ∈ [0, 1],

esn(zn−zη

n)P(Y (n) > zn − zη

n) 6 exp(cid:16)√zn(cid:16)α′(cid:0)1 − zη−1
6 exp(cid:0)√zn(α′ − α′′)(1 − zη−1

(cid:1) − α′′(cid:0)1 − zη−1
)(cid:1) = o(cid:16)N−1/2

(cid:1)1/2(cid:17)(cid:17)
(cid:17) .

n

n

n

n

Finally, applying Taylor’s theorem to the function f (u) = snu − α′′√u around the point zn yields

f (u) =

α′u

√zn − α′′√u = (α′ − α′′)√zn +(cid:18) α′

√zn −

α′′

2√c(cid:19) (u − zn)

with c ∈ [u, zn]. Since α′′ < 2α′, we have

(cid:18) α′
√zn −

α′′

2√c(cid:19) (u − zn) 6  α′
√zn −

for n large enough and we conclude that I3 = o(N−1/2

n

).

5.3 Proof of Proposition 3.4

n! (u − zn) 6 0,

α′′

2pzn − zη

Remind that (X (n)
)16i6Nn are i.i.d. copies of (X (n), Y (n)), X (n) following the Borel distribution with
parameter µn = n/mn → µ ∈ (0, 1), and Y (n) given X (n) = l is distributed as dl,l−1. We start with
computing the asymptotic tail behavior of X (n). Remind that

, Y (n)

i

i

P(X (n) = xn) = e−µnxn (µnxn)xn−1

xn!

.

13

Lemma 5.6 (Tail of X (n)). If ln → ∞, then

log P(X (n) = ln) = −κln(1 + o(1))

log P(X (n) > ln) = −κln(1 + o(1))

and

with κ = µ − log(µ) − 1 ∈ (0,∞).
Proof. By Stirling’s formula,

log P(cid:0)X (n) = ln(cid:1) = log(cid:0)e−µnln (µnln)ln−1

ln!

Similar estimates give the second result.

(cid:1) ∼ ln(1 + log(µ) − µ).

Proof of (5). Let u > 0 and nu be the ceiling of the positive solution of 2u = (n − 1)(n − 2):

2' .
Since Y (n) conditioned on {X (n) = l} is distributed as dl,l−1, we get

nu =&r2u +

1
4

+

3

(18)

(19)

(20)

P(Y (n) > u) =

P(dl,l−1 > u)P(X (n) = l) 6

+∞

Xl=nu

P(X (n) = l) = P(X (n) > nu).

+∞

Xl=nu

By (19) and the fact that nu = √2u(1 + o(1)) for u > √Nny, we ﬁnally conclude that

lim sup
n→∞

sup

u>√Nny

1
√u

log P(Y (n) > u) 6 −κ√2.

Lemma 5.7. Let a > 0. Let l = 1 + ⌈√a⌉ and k = ⌊√a⌋. Then
(l − 1)!

.

P(dl,l−1 > a) >

1
ll−1

2k

Proof. Take the hash sequence

(1, 1, 2, 2, . . . k, k, k + 1, k + 2, . . . , l − 1 − k) .

(21)

Notice that 0 6 k 6 (l − 1)/2. On the one hand, it is decomposed into l − 1 − 2k single numbers and k
pairs leading to a hash sequence of size l − 1 as required. On the other hand, each pair (q, q) (q = 1, . . . ,
k) realizes a displacement equal to (q − 1) + q while each singleton q (q = k + 1, . . . , l − 1 − k) realizes a
displacement equal to k. The total displacement is then k(l − 1 − k) which is greater than a.
Moreover as mentioned in Lemma 2.1 the total displacement associated to any hash sequence does not
depend on the order of the hash sequence. One can consider all the permutations of the hash sequence
deﬁned in (21) the total number of which is given by

(cid:18)l − 1

1 (cid:19)(cid:18)l − 2

1 (cid:19) . . .(cid:18)2k + 1

1 (cid:19)(cid:18)2k

2(cid:19)(cid:18)2k − 2

2 (cid:19) . . .(cid:18)2

2(cid:19) =

(l − 1)!

2k

.

To conclude, it remains to use item 1. of Lemma 2.1.

14

Proof of (4). For any ln > 1, one has

P(Y (n) > Nny) =

+∞

P(dl,l−1 > Nny)P(X (n) = l)

Xl=1
> P(dln,ln−1 > Nny)P(X (n) = ln).

As a consequence, using Lemma 5.7 with a := Nny and Lemma 5.6,

lim inf
n→∞

1

√Nny

log P(Y (n) > Nny) > lim inf
n→∞
n→∞ −

> lim inf

(ln − 1)!

2kn

1

√Nny
ln + kn log(2) + κln

lln−1
n

log(cid:18) 1
√Nny

P(cid:0)X (n) = ln(cid:1)(cid:19)

where ln = 1 + ⌈√a⌉ and kn = ⌊√a⌋.

= −(4 + log(2) + 2 log(µ) − 2µ)

5.4 Proof of Theorem 3.1
Remind that the total displacement dmn,n is distributed as the conditional distribution of T (n)
Nn
mn. Notice that E[S(n)
Nn

] = NnE[X (n)] = mn. Now let

given S(n)
Nn

=

Pn := P(dmn,n − E[dmn,n] > Nny)

= P(T (n)
= P(T (n)

Nn |S(n)

Nn

= mn] > Nny|S(n)

Nn

= mn)

] > Nnyn, S(n)
Nn

= mn)/P(S(n)
Nn

= mn)

Nn

Nn − E[T (n)
Nn − E[T (n)
= mn] − E[T (n)
Nn(cid:12)(cid:12)(cid:12)
Eh T (n)

S(n)
Nn

Nn

where yn := y + 1
Nn

Lemma 5.8.

(E[T (n)

Nn |S(n)

Nn

]). The following lemma entails yn → y.
= mni = EhT (n)

Nni + o(Nn).

Proof. According to [12, Section 4], the hypotheses of Proposition 5.2 are satisﬁed by the variables (X (n), Y (n)).
Using (11), diﬀerentiating under the integral sign of (12) and using Proposition 5.2 yield

It remains to show that the integral converges to 0. Putting together (22) and (15), using the fact that
Var(Y (n)) is convergent and the control (13) with l = 1 and t = 0, one gets

ds.

(22)

s

σX (n) N 1/2

n

, 0!(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12)

6

n

n

s

Nn

∂ϕn

−πσ

S(n)
Nn

X(n) N 1/2

X(n) N 1/2

= mni(cid:12)(cid:12)(cid:12)

Nn − NnEhY (n)i(cid:12)(cid:12)(cid:12)
Eh T (n)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
∂t  
2πcZ πσ
Nn − NnEhY (n)i(cid:12)(cid:12)(cid:12)
Eh T (n)

σX (n) N 1/2

n

n

ϕNn−1

2πP(S(n)
Nn

−iψ′n(0)

= mn)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
 

=(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
, 0!(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
·(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
= mni = o(Nn).

S(n)
Nn

Remember that the assumptions of Theorem 4.1 are satisﬁed. By Lemma 5.8 (respectively Hypothesis
(H4.1.5) and Proposition 3.4), Hypothesis (H3.3.1) with zn = Nnyn (resp. Hypotheses (H3.3.2) and (H3.3.3))
holds.

15

Proof of the upper bound. We have

Pn 6

P(T (n)

Nn − E[T (n)

Nn
= mn)

P(S(n)
Nn

] > Nnyn)

.

The conclusion follows from the upper bound of Proposition 3.3, Proposition 5.2.

Proof ot the lower bound. We have

= mn)

Pn > P(T (n)

Nn

Nn − E[T (n)
Nn − E[T (n)
Nn−1 − E[T (n)

] > Nnyn, S(n)
Nn
] > Nnyn, S(n)
Nn
Nn−1] > −Nnε, S(n)

Nn

> P(cid:0)T (n)
> P(cid:0)T (n)

=: P1P2

= mn,

Nn − E[Y (n)
Y (n)
Nn−1 = mn − ln(cid:1)P(cid:0)Y (n)

Nn

] > Nn(yn + ε)(cid:1)
Nn − E[Y (n)

Nn

] > Nn(yn + ε), X (n)
Nn

= ln(cid:1)
where ln := 1 + ⌈√an⌉ and an := Nn(yn + ε) + E[Y (n)]. Applying Lemma 5.7 and Lemma 5.6, we get
log(cid:16)P(cid:0)dln,ln−1 > Nn(yn + ε) + E[Y (n)](cid:1)P(cid:0)X (n) = ln(cid:1)(cid:17)
log(cid:18) 1

1
√Nn
1
√Nn
ln + kn log(2) + (1 + log(µ) − µ)ln

P(cid:0)X (n) = ln(cid:1)(cid:19)

log(P2) = lim inf
n→∞

(ln − 1)!

lim inf
n→∞

1
√Nn

lln−1
n

2kn

> lim inf
n→∞
n→∞ −

> lim inf

= −(4 + log(2) + 2 log(µ) − 2µ)√y + ε.

√Nn

Letting ε → 0, we get

lim inf
n→∞

1
√Nn

log(P2) > −β(µ)√y

(23)

with β(µ) := 4 + log(2) + 2 log(µ) − 2µ.
Let us turn to the minoration of P1:

Nn−1] > −Nnε, S(n)

P1 = P(cid:0)T (n)
> P(cid:0)S(n)

Nn−1 − E[T (n)
Nn−1 = mn − ln(cid:1) − P(cid:0)T (n)

Nn−1 − E[T (n)

Nn−1 = mn − ln(cid:1)

Nn−1] < −Nnε(cid:1).

Since ln = O(N 1/2

n ), Proposition 5.2 provides a c > 0 such that

P(cid:0)S(n)

Nn−1 = mn − ln(cid:1) >

c

2πσX (n) N 1/2

n

> c′N−1/2

n

with c′ > 0, since σX (n) converges to the standard deviation of the Borel distribution of parameter µ.
Chebyshev’s inequality and the fact that Var(Y (n)) = o(N 1/2

n ) yield

Nn−1 − E[T (n)

P(cid:0)T (n)

Nn−1] < −Nnε(cid:1) 6

log(P1) = 0 that leads with (23) to

Var(Y (n))

Nnε2 = o(N−1/2

n

).

Eventually, limn→∞

1√Nn

lim inf
n→∞

1
√Nn

log(Pn) > −β(µ)y1/2.

16

5.5 Proof of Theorem 4.1
To lighten notation, we denote Sn := S(n)
. Remind that Un is distributed as Tn conditioned
Nn
on Sn. Part a) is Proposition 5.2 with ˜c5 = c. Now we follow the procedure of Janson [13] to uncorrelate
X (n) and Y (n) and center the variable Y (n). We replace Y (n) by the projection

and Tn := T (n)
Nn

Y

′(n) := Y (n) − E[Y (n)] −

Cov(X (n), Y (n))

σ2

X (n)

(cid:16)X (n) − E[X (n)](cid:17) .

′(n)] = 0 and Cov(X (n), Y

′(n)) = E[X (n)Y

′(n)] = 0. Besides, Hypotheses (H4.1.3) and (H4.1.7)

Then E[Y
are veriﬁed by Y

′(n). By Hypothesis (H4.1.7),
Y ′ (n) = σ2
σ2

Y (n) (1 − r2

n) ∈ [˜c2

3(1 − c2

6), c2
3],

′(n). Finally, by Minkowski Inequality, Hypotheses (H4.1.2) and (H4.1.6), and

so (H4.1.5) is satisﬁed by Y
the fact that |rn| 6 1,
Y

(cid:13)(cid:13)(cid:13)

′(n)(cid:13)(cid:13)(cid:13)3

6(cid:13)(cid:13)(cid:13)
Y (n) − E[Y (n)](cid:13)(cid:13)(cid:13)3

Y (n) + rnσY (n)

6 ρ1/3

ρ1/3
X (n)
σX (n)

+ |rn| σX (n) σY (n)

σ2

X (n)

(cid:13)(cid:13)(cid:13)
X (n) − E[X (n)](cid:13)(cid:13)(cid:13)3

6 σY (n) (c2 + c4).

′(n) satisﬁes Hypothesis (H4.1.6). Consequently, all conditions hold for the pair (X (n), Y

′(n)) too.

Hence Y
Finally,

T ′n :=

′(n)
Y
i

Nn

Xi=1

= Tn − NnEhY (n)i −

Cov(X (n), Y (n))

σ2

X (n)

(cid:16)Sn − NnEhX (n)i(cid:17) .

So, conditioned on Sn = mn, we have T ′n = Tn − NnE(cid:2)Y (n)(cid:3) − rn
conclusions for (cid:0)X (n), Y (n)(cid:1) and (cid:16)X (n), Y
(cid:16)X (n), Y

(mn − NnE[X (n)]). Hence the
′(n)(cid:17) are the same. Thus, it suﬃces to prove the theorem for
′(n)(cid:17); in other words, we may henceforth assume that E(cid:2)Y (n)(cid:3) = E(cid:2)X (n)Y (n)(cid:3) = 0. Note that in

that case τ 2

n = σ2

Y (n)
X(n)

Y (n) .

σ
σ

Proof of Theorem 4.1 - Part b). We follow the classical proof of Berry-Esseen (see e.g. [7]) combined with
the procedure of Quine and Robinson [27].
As shown in Lo`eve [21] (page 285) or Feller [7], the left hand side of (6) is dominated by

where η > 0 is such that

From Lemma 5.1 and a Taylor expansion,

du
u

+

24σ−1

n

Y (n) N−1/2
ηπ√2π

ψn(u/σY (n) N 1/2
n )

n

0

9

2

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

c3c3

Y (n) N 1/2

π Z ησ

ψn(u/σY (n) N 1/2
n )

2πP(Sn = mn) − e−u2/2(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
η := min(cid:18) 2
4, η0(cid:19).
2πP(Sn = mn) − e−u2/2(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
= u−1e−u2/2(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
#(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)t=θ
06θ6u(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
∂t" et2/2ψn(t/σY (n) N 1/2
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
n  
∂t"et2/2ϕNn
06θ6u(Z πσ

2πP(Sn = mn)

X(n) N 1/2

X(n) N 1/2

−πσ

n )

∂

∂

n

n

u−1(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

6 e−u2/2 sup

6 c−1

n e−u2/2 sup

17

eu2/2ψn(u/σY (n) N 1/2
n )

2πP(Sn = mn)

s

,

t

σX (n) N 1/2

n

σY (n) N 1/2

(24)

(25)

ds)

− 1(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
n !#(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)t=θ

where cn := 2πP(Sn = mn)σX (n) N 1/2
X(n) N 1/2
σ
of Proposition 5.2. Now we split the integration domain of s into

n > ˜c5 and vn =

n

mn−Nn E[X (n)]

has already been deﬁned in the proof

A1 :=ns :

|s| < εσX (n) N 1/2

n o and A2 :=ns : εσX (n) N 1/2

n o ,
n 6 |s| 6 πσX (n) N 1/2

where 0 < ε < π is such that

and decompose

where

I1(u, θ) = c−1

I2(u, θ) = c−1

and

If we prove that there exists positive constants C1, C2 and C3, such that

2, π(cid:19)

9

c1c3

ψn(u/σY (n) N 1/2
n )

ε := min(cid:18) 2
2πP(Sn = mn) − e−u2/2(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
u−1(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
e−(u2+s2)/2(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
n  
∂t"e(t2+s2)/2ϕNn
  ∂
n ZA1
n e−u2/2ZA2(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
  ∂
n  
∂t"et2/2ϕNn
Z ησ
Z ησ

sup
06θ6t

Y (n) N 1/2

Y (n) N 1/2

06θ6u

sup

s

0

0

n

n

I1(u, θ)du 6

C1
N 1/2

n

I2(u, θ)du 6 C2e−C3Nn,

6 sup

[I1(u, θ) + I2(u, θ)] ,

06θ6u

s

σX (n) N 1/2

n

,

t

,

t

n !#!t=θ(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

ds.

σY (n) N 1/2

n !#!t=θ(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

σX (n) N 1/2

n

σY (n) N 1/2

(26)

(27)

ds,

(28)

(29)

(30)

(31)

we conclude to part b) of Theorem 4.1 writing

C2e−C3Nn =

C2C−1/2

3

N 1/2

n

(C3Nn)1/2e−C3Nn 6

C2C−1/2

3

N 1/2

n

(1/2)1/2e−1/2,

since x1/2e−x is maximum in 1/2. The proofs of (30) and (31) are postponed after the present proof. So,

with

P  Un − NnE(cid:2)Y (n)(cid:3)

N 1/2

n τn

sup

x (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

C := C1 + C2C−1/2

3

(1/2)1/2e−1/2 +

C

N 1/2

n

6

6 x! − Φ(x)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
˜c3π√2π (cid:18)min(cid:18) 2

24

9

c3c3

4, η0(cid:19)(cid:19)−1

.

(32)

Now it remains to prove (30) and (31). To bound I1(u, θ), we use a result due to Quine and Robinson ([27,
Lemma 2]).

Lemma 5.9. [Lemma 2 in [27]] Deﬁne

l1,n := ρX (n) σ−3

X (n) N−1/2

n

and

l2,n := ρY (n) σ−3

Y (n) N−1/2

n

.

If l1,n 6 1 and l2,n 6 1, then, for all

(s, t) ∈ R :=(cid:26)(s, t) :

|s| <

2
9

l−1
1,n,|t| <

2
9

l−1

2,n(cid:27) ,

18

we have

with C0 := 98.

(cid:12)(cid:12)(cid:12)(cid:12)

s

∂

n !#(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
n  
∂the(s2+t2)/2 ϕNn
6 C0(|s| + |t| + 1)3(l1,n + l2,n) exp(cid:26) 11

σX (n) N 1/2

σY (n) N 1/2

t

n

,

24(cid:0)s2 + t2(cid:1)(cid:27)

(33)

Proof. We refer to the proof in the appendix of [27]. The condition l1,n < 12−3/2 and l2,n < 12−3/2
appearing in [27, Lemma 2] can be replaced by l1,n 6 (33/32)3/2 and l2,n 6 (33/32)3/2 since the factor 8/27
in (A4) of their proof can be replaced by a factor 1/27. Since we do not provide the best constants here,
we simply suppose l1,n 6 1 and l2,n 6 1. Finally, C0 has to be greater than 4 and

sup

(v,s)∈R2

27(|v| + 2 |s|)(|v|3 + |s|3)

(|v| + |s| + 1)3
√6r v2 + s2

12

6 108 ·

e−(v2+s2)/24 6 54 · (|v| + |s|)e−(v2+s2)/24

e−(v2+s2)/24 6

108 · √6

e

6 98.

By Hypotheses (H4.1.2) and (H4.1.1),

l1,n 6 c3

2N−1/2

n

6 c3

2c1σ−1

X (n) N−1/2

n

which implies that σX (n) N 1/2

n 6 c−3

2 c−1

1 l−1

1,n. Similarly,

l2,n 6 c3

4N−1/2

n

6 c3

4c3σ−1

Y (n) N−1/2

n

and σY (n)N 1/2

n 6 c−3

4 c−1

3 l−1
2,n.

Lemma 5.10. There exists a positive constant C1 such that

,

,

(34)

(35)

Y (n) N 1/2

n

Z ησ

0

sup

06θ6u

I1(u, θ)du 6

C1
N 1/2

n

.

Proof. Conditions (26) and (25) imply that, on A1,

|s| < εσX (n) N 1/2

n 6

2
9

l−1
1,n

and |θ| 6 |u| 6 ησY (n) N 1/2

n 6

2
9

l−1
2,n,

which ensures that (s, u) ∈ R as speciﬁed in Lemma 5.9. Moreover, since we have Nn > max(c6
4) (cf.
Hypothesis in 4.1.b), l1,n 6 1 and l2,n 6 1. Now applying Lemma 5.9 in (28) and using part 4.1.a, we get

2, c6

Y (n) N 1/2

n

Z ησ

0

sup

06θ6u

I1(u, θ)du

and the result follows with C1 = ˜c−1

(|s| + |u| + 1)3e−(s2+u2)/24dsdu

6 c−1

6 N−1/2

Y (n) N 1/2

n

0

n

ZA1

˜c−1
5 C0(c3

n C0(l1,n + l2,n)Z ησ
4)ZR2
(|s| + |u| + 1)3e−(s2+u2)/24dsdu
4)RR2 (|s| + |u| + 1)3e−(s2+u2)/24dsdu.

5 C0(c3

2 + c3

2 + c3

19

Now, we study the integral on A2.

Lemma 5.11. There exist positive constants C2 and C3, only depending on ˜c1, c1, c2, ˜c3, c3, c4, c5, ˜c5,
and c6, such that

Y (n) N 1/2

n

Z ησ

0

I2(u, θ)du 6 C2e−C3Nn.

sup
06θ6t

Proof. We use the controls (14), (13) with l = 1, and |ϕn| 6 1 to get

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
n  
∂t"et2/2ϕNn
  ∂
= eθ2/2(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
 

ϕNn−1

n

s

σX (n) N 1/2

n

,

σY (n) N 1/2

s

,

σX (n) N 1/2

n

σY (n) N 1/2

t

θ

n !#!t=θ(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
n !(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
·(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
θϕn 

+

s

,

n

σX (n) N 1/2
Nn
∂ϕn

θ

n !

σY (n) N 1/2

∂t  

s

σX (n) N 1/2

n

,

σY (n) N 1/2

n

6 eθ2/2e−(s2+θ2)·c5(Nn−1)/Nn (|s| + 2 |θ|).
Finally by (29) and for Nn > 2, we conclude that

θ

σY (n) N 1/2

n !(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

sup

06θ6u(cid:20)(s + 2θ) exp(cid:18) −

u2
2

+

θ2

2 (cid:18)1 − 2c5

Nn (cid:19)(cid:19)(cid:21)
Nn − 1

· e−s2·c5(Nn−1)/Nn dsdu

(s + 2t)e− min(1,c5)u2/2e−s2c5/2dsdt

+ 2˜c−1

5

2

min(1, c5)

e−Nnc5ε2σ2
X(n) /2
c5εσX (n) N 1/2

n

.

Y (n) N 1/2

n

Z ησ

0

sup

I2(u, θ)du

06θ6u

6 2c−1

n Z +∞

0

Z +∞

εσ

X(n) N 1/2

n

6 2˜c−1

5 Z +∞

0

Z +∞

εσ

X(n) N 1/2

n

6 2˜c−1

5

2
c5

e−Nnc5ε2σ2

X(n) /2

The conclusion follows with

√2π

2pmin(1, c5)
5 


√2π

pmin(1, c5)

C2 := 2˜c−1

5 c−1

+

2

min(1, c5) min(cid:18) 2

9 c1c3

2, π(cid:19)˜c1




(36)

and C3 := c5 min(cid:18) 2

9 c1c3

2, π(cid:19)2

˜c2
1/2.

Proof of Theorem 4.1 - Part c). We start proving (7). We adapt the proof given in [13]. Using (11) with
E[Y (n)] = 0, and diﬀerentiating under the integral sign of (12), we naturally have

−iψ′n(0)
|E [Un]| =(cid:12)(cid:12)(cid:12)(cid:12)
2πP(Sn = mn)(cid:12)(cid:12)(cid:12)(cid:12)
2πP(Sn = mn) Z πσ

X (n) N−1/2
σ−1

n Nn

−πσ

6

X(n) N 1/2

n

X(n) N 1/2

n

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

∂ϕn

∂t  

s

σX (n) N 1/2

n

20

ϕNn−1

n

 

s

σX (n) N 1/2

n

, 0!(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

·(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

, 0!(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

ds.

(37)

Using inequality (15) of Lemma 5.4 with rn = 0 and t = 0, Hypotheses (H4.1.1), (H4.1.2), and (H4.1.6), we
deduce

∂ϕn

∂t  

s

σX (n) N 1/2

n

6

s2
2

Y (n) ρ2/3
ρ1/3
X (n)
σ2
X (n) Nn

6

c2
2c3c4
2Nn

s2.

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

Then using inequality (13) with l = 1 and t = 0 and for Nn > 2,

, 0!(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

ϕNn−1

n

Z πσ

−πσ

X(n) N 1/2

n

X(n) N 1/2

n

∂ϕn

∂t  

s

σX (n) N 1/2

n

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

, 0!(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

·(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

 

s

σX (n) N 1/2

n

ds 6

, 0!(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

c2
2c3c4

2Nn Z +∞

−∞

s2e−c5s2/2ds.

So, (7) holds with c7 := c2

s2e−c5s2/2ds.

2c3c4

2˜c5 R +∞

−∞

To prove (8), since τn = σY (n) and E [Un] is bounded, it suﬃces to show that the quantity(cid:12)(cid:12)

is bounded by some c′8N 1/2

n . Proceeding as previously,

E(cid:2)U 2

Y (n)(cid:12)(cid:12)
n(cid:3) − Nnσ2

−ψ′′n(0)

2πP(Sn = mn)

E(cid:2)U 2
n(cid:3) =
n Nn(Nn − 1)Z πσ
= −c−1
n NnZ πσ
− c−1

−πσ
X(n) N 1/2

X(n) N 1/2

−πσ

n

n

X(n) N 1/2

n

X(n) N 1/2
∂2ϕn

∂t2  

n   ∂ϕn
∂t  

s

σX (n) N 1/2

n

s

σX (n) N 1/2

n

, 0! ϕNn−1

n

, 0!!2
 

ϕNn−2

n

s

σX (n) N 1/2

n

σX (n) N 1/2

n

s

 
, 0! ds.

, 0! ds

(38)

(39)

First, by inequality (15) with rn = 0 and t = 0, the control (13) with l = 1 and t = 0, and for Nn > 3, one
has

Z πσ

−πσ

X(n) N 1/2

n

∂ϕn

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
∂t  
n Z +∞

−∞

n

X(n) N 1/2
3c2
2c2
c4
4
4N 2

6

s

σX (n) N 1/2

n

s4e−c5s2/3ds,

, 0!(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

ϕNn−2

n

2(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

 

s

σX (n) N 1/2

n

dv

, 0!(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

and ﬁnally using 4.1.a, the term (38) is bounded by

c′′8 :=

2c2
c4

3c2
4

4˜c5 Z +∞

−∞

s4e−c5s2/3ds.

(40)

Second, we study the term (39). We want to show that

∆n := c−1

n Z πσ

−πσ

X(n) N 1/2

n

X(n) N 1/2

n

∂2ϕn

∂t2  

s

σX (n) N 1/2

n

, 0! ϕNn−1

n

 

s

σX (n) N 1/2

n

, 0! ds + σ2

Y (n)

is bounded by some c′′′8 /N 1/2

n . Recall that, by Lemma 5.1 and Hypothesis (H4.1.4),

Z πσ

−πσ

X(n) N 1/2

n

X(n) N 1/2

n

ϕNn

n  

s

σX (n) N 1/2

n

, 0! dv = 2πP(Sn = mn)σX (n) N 1/2

n = cn,

21

, 0! ds

, 0! ds.

so

∆n = c−1

n Z πσ

−πσ

X(n) N 1/2

n

n   ∂2ϕn
∂t2  

s

σX (n) N 1/2

n

, 0! + σ2

X(n) N 1/2

, 0!!

s

Y (n) ϕn 
· ϕNn−1

n

n

σX (n) N 1/2
s

 

σX (n) N 1/2

n

= c−1

n Z πσ

−πσ

X(n) N 1/2

n

X(n) N 1/2

n

E(cid:20)Y (n)2(cid:16) − eisσ−1

X(n) N −1/2

n

(X (n)−E[X (n)])

X(n) N −1/2

n

+ Eheisσ−1

(X (n)−E[X (n)])i(cid:17)(cid:21)
· ϕNn−1

n

 

s

σX (n) N 1/2

n

Applying Taylor’s theorem to the function

f (s) = −eisσ−1

X(n) N −1/2

n

yields

(X (n)−E[X (n)]) + Eheisσ−1

X(n) N −1/2

n

(X (n)−E[X (n)])i

|f (s)| 6 |s| sup

u∈[0,s](cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
n (cid:18)(cid:12)(cid:12)(cid:12)(cid:12)

X (n) − E[X (n)]

n

σX (n) N 1/2

−i
+E(cid:20)i
X (n) − E[X (n)]

X (n) − E[X (n)]

σX (n) N 1/2

n

σX (n)

(cid:12)(cid:12)(cid:12)(cid:12)

+ E(cid:20)(cid:12)(cid:12)(cid:12)(cid:12)

6 |s|
N 1/2

eiuσ−1

X(n) N −1/2

n

(X (n)−E[X (n)])

eiuσ−1

X(n) N −1/2

n

X (n) − E[X (n)]

σX (n)

(X (n)−E[X (n)])(cid:21)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)(cid:12)
(cid:21)(cid:19).

Thus, using H¨older’s inequality,

E[Y (n)2

(cid:12)(cid:12)(cid:12)

f (s)](cid:12)(cid:12)(cid:12)

6 |s|
N 1/2
n
σ2
Y (n) |s|
N 1/2

E(cid:20)Y (n)2(cid:18)(cid:12)(cid:12)(cid:12)(cid:12)
(cid:18) ρ2/3

Y (n)
σ2

Y (n)

6

n

X (n) − E[X (n)]

σX (n)

ρ1/3
X (n)
σX (n)

+ 1(cid:19)

(cid:12)(cid:12)(cid:12)(cid:12)

+ E(cid:20)(cid:12)(cid:12)(cid:12)(cid:12)

X (n) − E[X (n)]

σX (n)

(cid:21)(cid:19)(cid:21)

(cid:12)(cid:12)(cid:12)(cid:12)

and, using part 4.1.a, Hypotheses (H4.1.1), (H4.1.2), (H4.1.5), (H4.1.6), and the majoration (13) with t = 0,
we get

σY (n)
N 1/2

n cn(cid:18) ρ2/3

Y (n)
σ2

Y (n)

ρ1/3
X (n)
σX (n)

|s| e−s2c5(Nn−1)/Nn ds 6

c′′′8
N 1/2

n

c′′′8 := c3˜c−1

5 (1 + c2c2

|s| e−s2c5/2ds.

(41)

−∞

+ 1(cid:19)Z +∞
4)Z +∞

−∞

|∆n| 6

with

Finally,

with

c8 := c7 + c′′8 + c′′′8

(cid:12)(cid:12)Var(Un) − Nnτ 2
4˜c5 Z +∞

c4
2c2

−∞

3c2
4

=

c2
2c3c4

2˜c5 Z +∞

−∞

s2e−cs2/2ds +

n(cid:12)(cid:12) 6 c7 + c′′8 + c′′′8 N 1/2

n 6 c8N 1/2

n

s4e−c5s2/3ds + c3˜c−1

5 (1 + c2c2

4)Z +∞

−∞

|s| e−s2c5/2ds.

(42)

22

Now we turn to the proof of (9). Let us show that the previous estimates of E[Un] and Var(Un) make it

possible to apply (6). Remind that E(cid:2)Y (n)(cid:3) = 0. Write
6 x) =(

( Un − E[Un]

Var (Un)1/2

Un

N 1/2

n σY (n)

6 anx + bn) ,

where

an :=

Var(Un)1/2
N 1/2
n σY (n)

and bn :=

E[Un]
n σY (n)

N 1/2

.

The previous estimates of E[Un] and Var(Un) yield

Now,

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

P  Un − E[Un]

Var (Un)1/2

|an − 1| 6(cid:12)(cid:12)a2
6 x! − Φ(x)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

n − 1(cid:12)(cid:12) 6 c8˜c−1
6(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
P 

3 N−1/2

n

Un

N 1/2

n σY (n)

and bn 6 c7˜c−1

3 N−1/2

n

.

6 anx + bn! − Φ(anx + bn)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

+ |Φ(anx + bn) − Φ(x)|

6

C1
N 1/2

n

+ C2e−C3Nn + |Φ(anx + bn) − Φ(x)| .

For Nn > 4c2

8/˜c2

3, an > 1/2 and applying Taylor’s theorem to Φ yields

|Φ(anx + bn) − Φ(x)| 6 |(an − 1)x + bn| sup

6 N−1/2

n max(c8˜c−1

t

e−t2/2
√2π
3 )(|x| + 1)e−(|x|/2−c7˜c−1
3 , c7˜c−1

3 )2/2,

the supremum being over t between x and anx + bn. The last function in x being bounded, we get (9) with

˜C1 := max(c8˜c−1

3 , c7˜c−1

3 ) sup

x∈Rh(|x| + 1)e−(|x|/2−c7˜c−1

3 )2/2i.

References

[1] P. Chassaing and S. Janson. A Vervaat-like path transformation for the reﬂected Brownian bridge

conditioned on its local time at 0. Ann. Probab., 29(4):1755–1779, 2001.

[2] P. Chassaing and G. Louchard. Phase transition for parking blocks, Brownian excursion and coales-

cence. Random Structures Algorithms, 21(1):76–119, 2002.

[3] P. Chassaing and J.-F. Marckert. Parking functions, empirical processes, and the width of rooted

labeled trees. Electron. J. Combin., 8(1):Research Paper 14, 19, 2001.

[4] I. Csiszar. Sanov property, generalized i-projection and a conditional limit theorem. The Annals of

Probability, 12(3):768–793, 08 1984.

[5] A. Dembo and O. Zeitouni. Large deviations techniques and applications, volume 38 of Applications of

Mathematics (New York). Springer-Verlag, New York, second edition, 1998.

[6] W. Feller. An introduction to probability theory and its applications. Vol. I. Third edition. John Wiley

& Sons Inc., New York, 1968.

23

[7] W. Feller. An introduction to probability theory and its applications. Vol. II. Second edition. John

Wiley & Sons Inc., New York, 1971.

[8] P. Flajolet, P. Poblete, and A. Viola. On the analysis of linear probing hashing. Algorithmica, 22(4):490–

515, 1998. Average-case analysis of algorithms.

[9] F. Gamboa, T. Klein, and C. Prieur. Conditional large and moderate deviations for sums of discrete

random variables. Combinatoric applications. Bernoulli, 18(4):1341–1360, 2012.

[10] C. Hipp. Asymptotic expansions for conditional distributions: the lattice case. Probab. Math. Statist.,

4(2):207–219, 1984.

[11] L. Holst. Two conditional limit theorems with applications. Ann. Statist., 7(3):551–557, 1979.

[12] S. Janson. Asymptotic distribution for the cost of linear probing hashing. Random Structures Algo-

rithms, 19(3-4):438–471, 2001. Analysis of algorithms (Krynica Morska, 2000).

[13] S. Janson. Moment convergence in conditional limit theorems. J. Appl. Probab., 38(2):421–437, 2001.

[14] S. Janson. Individual displacements for linear probing hashing with diﬀerent insertion policies. ACM

Trans. Algorithms, 1(2):177–213, 2005.

[15] S. Janson.

Individual displacements in hashing with coalesced chains. Combin. Probab. Comput.,

17(6):799–814, 2008.

[16] D. E. Knuth. Computer science and its relation to mathematics. Amer. Math. Monthly, 81:323–343,

1974.

[17] D. E. Knuth. The art of computer programming. Vol. 3. Addison-Wesley, Reading, MA, 1998. Sorting

and searching, Second edition [of MR0445948].

[18] V. F. Kolchin. Random mappings. Translation Series in Mathematics and Engineering. Optimization
Software, Inc., Publications Division, New York, 1986. Translated from the Russian, With a foreword
by S. R. S. Varadhan.

[19] `E. M. Kudlaev. Conditional limit distributions of sums of random variables. Teor. Veroyatnost. i

Primenen., 29(4):743–752, 1984.

[20] J.-F. Le Gall. Random trees and applications. Probab. Surv., 2:245–311, 2005.

[21] M. Lo`eve. Probability theory. Foundations. Random sequences. D. Van Nostrand Company, Inc.,

Toronto-New York-London, 1955.

[22] J.-F. Marckert. Parking with density. Random Structures Algorithms, 18(4):364–380, 2001.

[23] A. Nagaev. Integral limit theorems taking large deviations into account when cram´er’s condition does

not hold. i. Theory of Probability and Its Applications, 14(1):51–64, 1969.

[24] A. Nagaev. Integral limit theorems taking large deviations into account when cram´er’s condition does

not hold. ii. Theory of Probability and Its Applications, 14(2):193–208, 1969.

[25] Y. L. Pavlov. Limit theorems for the number of trees of a given size in a random forest. Mat. Sb.

(N.S.), 103(145)(3):392–403, 464, 1977.

[26] Y. L. Pavlov. Random forests. In Probabilistic methods in discrete mathematics (Petrozavodsk, 1996),

pages 11–18. VSP, Utrecht, 1997.

[27] M. P. Quine and J. Robinson. A Berry-Esseen bound for an occupancy problem. Ann. Probab.,

10(3):663–671, 1982.

24

[28] J. Robinson, T. H¨oglund, L. Holst, and M. P. Quine. On approximating probabilities for small and

large deviations in Rd. Ann. Probab., 18(2):727–753, 1990.

[29] G. P. Steck. Limit theorems for conditional distributions. Univ. California Publ. Statist., 2:237–284,

1957.

[30] J. M. van Campenhout and T. M. Cover. Maximum entropy and conditional probability. IEEE Trans.

Inform. Theory, 27(4):483–489, 1981.

[31] J. G. Wendel. Left-continuous random walk and the Lagrange expansion. Amer. Math. Monthly,

82:494–499, 1975.

25

