Cascaded Subpatch Networks for Effective CNNs

Xiaoheng Jiang, Yanwei Pang, Manli Sun, and Xuelong Li

1

6
1
0
2

 
r
a

M
1

 

 
 
]

V
C
.
s
c
[
 
 

1
v
8
2
1
0
0

.

3
0
6
1
:
v
i
X
r
a

Abstract—Conventional Convolutional Neural Networks
(CNNs) use either a linear or non-linear ﬁlter to extract features
from an image patch (region) of spatial size H × W (Typically,
H is small and is equal to W , e.g., H is 5 or 7). Generally,
the size of the ﬁlter is equal to the size H × W of the input
patch. We argue that the representation ability of equal-size
strategy is not strong enough. To overcome the drawback,
we propose to use subpatch ﬁlter whose spatial size h × w is
smaller than H × W . The proposed subpatch ﬁlter consists
of two subsequent ﬁlters. The ﬁrst one is a linear ﬁlter of
spatial size h × w and is aimed at extracting features from
spatial domain. The second one is of spatial size 1 × 1 and is
used for strengthening the connection between different input
feature channels and for reducing the number of parameters.
The subpatch ﬁlter convolves with the input patch and the
resulting network is called a subpatch network. Taking the
output of one subpatch network as input, we further repeat
constructing subpatch networks until the output contains only
one neuron in spatial domain. These subpatch networks form a
new network called Cascaded Subpatch Network (CSNet). The
feature layer generated by CSNet is called csconv layer. For
the whole input image, we construct a deep neural network
by stacking a sequence of csconv layers. Experimental results
on four benchmark datasets demonstrate the effectiveness and
compactness of the proposed CSNet. For example, our CSNet
reaches a test error of 5.68% on the CIFAR10 dataset without
model averaging. To the best of our knowledge, this is the best
result ever obtained on the CIFAR10 dataset.

Index Terms—Convolutional neural network, feature extrac-

tion, subpatch ﬁlter, cascaded subpatch networks (CSNet)

I. INTRODUCTION

C ONVOLUTIONAL neural networks (CNNs) [11], [15]

have achieved a great success in the ﬁeld of computer
vision, including image classiﬁcation [9], [12], [22], [29]–[31]
and object detection [6], [7], [17], [18], [33]. The underlying
reason lies in the fact that CNN is able to learn a hierarchy of
features [1], [3], [4], [34] that can represent objects in different
levels. Low-level features denote some visual features such as
edges, dots, and textures, whereas high-level features represent
objects in a semantic way. Low-level features are shared by all
objects while high-level features are of high discriminability.
High-level features are learned progressively from low-level
features. All these features are in fact learned through a series
of linear and non-linear transformations which are the primary
elements of CNNs.

Typically, CNN consists of several computational building
blocks: convolution, activation, and pooling. They work to-

Y. Pang, X. Jiang, and M. Sun are with the School of Electronic Informa-
tion Enginnering, Tianjin University, Tianjin 300072, China. (E-mail: {pyw,
jiangxiaoheng, sml}@tju.edu.cn).

X. Li is with the Center for OPTical IMagery Analysis and Learning
(OPTIMAL), State Key Laboratory of Transient Optics and Photonics, Xi’an
Institute of Optics and Precision Mechanics, Chinese Academy of Sciences,
Xi’an 710119, Shaanxi, P. R. China. E-mail: xuelong li@opt.ac.cn

gether to fulﬁll the task of feature extraction and transforma-
tion. Convolution takes inner product of the linear ﬁlter and the
local region of input channel. Activation imposes a non-linear
transformation on the convolutional results. Pooling gathers
the responses of a given region. Among these three building
blocks, convolutional block plays the most important role in
CNN. It controls the number of feature maps (i.e., width of
CNN) and the number of layers (i.e., depth of CNN). The
width and depth determine the capacity of CNN. The size of
neural network is a double-edged sword. On the one side, large
size means large capacity. Large capacity makes it possible
for deep networks to learn rich features which are essentially
important for task of recognizing tens or even thousands of
object categories. On the other side, large size typically means
a larger number of parameters, which makes the enlarged
network more prone to over-ﬁtting especially when the number
of labelled samples in the training set is limited. What is
more, the main drawback of large network is the dramatically
increased consumption of computational resources.

To construct a compact and powerful network, we propose
a novel type of convolutional ﬁlters. Given a local patch,
traditional CNNs typically use a convolutional ﬁlter which
is the same size as the patch to extract features. We argue
that this level of abstraction is not strong enough to generate
robust features. Multi-Layer Perceptron (MLP) [14] can be
used to impose more complex transformation. However, it is
still not complex enough to represent the input data which
lies on a highly non-linear manifold. Therefore, in this paper,
we propose to use cascaded subpatch ﬁlters to bring in much
more complex structures to abstract the local patch within the
receptive ﬁeld. One subpatch ﬁlter contains two subsequent
convolutional ﬁlters. The ﬁrst one abstracts subpatches of the
input patch. The second one is to fully connect all the output
channels of the ﬁrst one. Taking the convolutional output of
previous subpatch ﬁlter as input, we repeat constructing new
subpatch ﬁlters until the ﬁnal output contains only one neuron
in spatial domain. This results in the Cascaded Subpatch
Network (CSNet). CSNet can be used to replace conventional
convolutional layer to extract more complex and more robust
features. We call the resulting layer a csconv layer. A deep
neural network can be obtained by stacking multiple csconv
layers. For clarity, in the rest of this paper, the overall deep
network containing multiple csconv layers is called a CSNet.
The goal of the proposed method is to construct a more
effective structure to abstract
the local patch. Instead of
designing a CNN that is too wide (i.e., too many feature
maps in one layer) or too lengthy (i.e., too many layers), we
present a novel neural network which is compact, yet powerful.
Speciﬁcally, the contributions and merits of this paper are
summarized as follows.

1) We gain new insight into the convolutional block of

CNN. When abstracting one local patch of size H × W ,
we propose to use cascaded subpatch ﬁlters to replace
conventional convolutional ﬁlter.
2) Subpatch ﬁlter consists of an h×w linear ﬁlter followed
by a 1 × 1 ﬁlter. Its purpose is to impose a complex
transformation on subpatches of the input patch while
reducing the number of parameters.

3) Cascaded subpatch ﬁlters contain a sequence of subpatch
ﬁlters and they together reach the goal of generating a
more complex and more robust abstraction of the local
patch. The cascaded subpatch ﬁlters can be regarded
as one new convolutional kernel structure called csconv
ﬁlter. Csconv ﬁlter abstracts local patch much better than
conventional ﬁlter.

4) Csconv ﬁlter is a ﬂexible structure which can be con-
structed using a group of different subpatch ﬁlters ac-
cording to size of the local region and the demanding
number of parameters.

5) We build several CSNets with different number of pa-
rameters to deal with different tasks. And our CSNets
achieve the state-of-the-art performance on four widely
used benchmark image classiﬁcation datasets.

This paper is organized as follows. Section II reviews the
related work. Section III presents the proposed CSNet method.
The experimental results are given in Section IV. Finally,
Section V concludes this paper.

II. RELATED WORK

Since the great success of AlexNet [12] on the ImageNet
Large Scale Visual Recognition Challenge (ILSCRC-2010), a
number of attempts have been made to improve the architec-
tures of CNN in order to achieve better accuracy. We divide
these methods into the following three categories.

(1) Parameter adjusting. Some researchers paid attention
to the parameters of CNN, such as the sizes of convolutional
ﬁlters, the strides of ﬁlters, the number of feature channels
in each layer, and the number of convolutional layers. They
tried to adjust the parameters to improve the performance
of CNN through exhaustive experiments. Zeiler and Fergus
[25] visualized the trained CNN model and found that large
ﬁlter size and large stride of the ﬁrst convolutional
layer
could cause aliasing artifacts. Therefore, they used smaller
receptive window size and smaller stride. Sermanet et al.
[21] utilized smaller strides in the ﬁrst convolution, larger
number of feature maps, and larger number of layers. They
achieved better results than the AlexNet. The VGG network
[22] pushes the depth of CNN to up to 19 convolutional
layers by using very small convolutional ﬁlters and gains a
signiﬁcant improvement. These above efforts can be viewed
as preliminary explorations on how to construct networks with
better performance.

(2) Structure designing. Another line of improvements
go further into the designing of new CNN structures. Net-
work in Network (NIN) [14] utilizes shallow Multi-Layer
Perception (MLP) to increase the representational power of
neural networks. MLP is a more complex structure which
consists of multiple fully connected layers. Conventional linear

2

convolution and MLP together result in a new convolutional
structure called mlpconv. Mlpconv can be easily implemented
by stacking additional 1 × 1 convolutional layers on conven-
tional convolutional layer. These 1 × 1 convolutions actually
enhance the connection between different feature channels.
Therefore, mlpconv is able to abstract local regions much
more effectively than conventional convolution. Szegedy et
al. [24] constructed a 22-layer GoogleNet by stacking dozens
of Inception modules. Each Inception module contains a
group of convolutional ﬁlters of different sizes which aim at
capturing information of multiple scales. However, such an
Inception module is too wide to be efﬁciently used in a very
deep network. To overcome the disaster of having too many
parameters, GoogleNet takes advantage of 1 × 1 convolutions
as dimension reduction modules to remove computational
bottlenecks. As a result, it allows for increasing not just the
depth but also the width of the GoogleNet without signiﬁcant
increasing in parameters.

(3) Deeper and wider networks. Since increasing the size
of CNN is the most straightforward way to improve their
performance, researchers went even further into designing
much deeper networks which are up to hundreds or even
thousands of layers. Highway Networks [23] make it possible
to train very deep networks even with hundreds of layers by
using adaptive gating units to regulate the information ﬂow.
More importantly, Highway Networks are able to train deeper
networks without sacriﬁcing generalization ability. The 32-
layer highway network presented in [23] achieved the state-of-
the-art performance on the CIFAR10 [37] dataset. Based on
Highway Networks, He et al. [32] recently presented a residual
learning framework to effectively train networks which are
substantially deeper than ever used. They constructed residual
nets (ResNest) with a depth of up to 152 layers and evaluated
them on the ImageNet dataset. They also presented ResNets
with 100 and 1000 layers and evaluated them on the CIFAR10
dataset. They argued that the depth of representations is of
great importance for many visual recognition tasks. In addition
to the depth of CNN, the width of CNN is also very important.
Shao et al. [2] pointed out that the combination of multicolumn
deep neural networks could enhance the robustness. Instead
of simply averaging the outputs of multicolumn predictions,
they learned a compact representation from multicolumn deep
neural networks by embedding the features of all the penulti-
mate layers into a multispectral space. The resulting features
are then used for classiﬁcation. Their multispectral neural
networks (MSNN) in fact make use of the complementary
information captured by different neural networks. Since the
MSNN has to use multiple networks that do not share param-
eters, the computation increases with the number of networks.
We agree that both width and depth of networks are impor-
tant for the tasks of visual recognition. However, larger capac-
ity does not guarantee higher accuracy. Given a dataset with
limited samples, when a network reaches its peak performance,
it is difﬁcult to further improve performance by simply adding
more feature maps or stacking more convolutional
layers.
This means that the discriminability of networks does not
increase inﬁnitely with the size of networks. The performance
comparison with ResNet110 [32] and ResNet1202 [32] on the

3

(a) A conventional ﬁlter.

(b) An n-stage csconv ﬁlter

Fig. 2. Comparison between conventional ﬁlter and the proposed csconv ﬁlter.
(a) A conventional convolutional ﬁlter Wk that is the same size as the input
patch of size H × W . (b) An n-stage csconv ﬁlter [(h1 × w1, 1 × 1), (h2 ×
w2, 1 × 1), ..., (hn × wn, 1 × 1)]. The input is a patch P of size H × W
and the ﬁnal output is of size 1 × 1.

To make the feature representation more effective, we
propose to utilize cascaded subpatch ﬁlters to transform the
patch from size H × W to 1 × 1. Let x ∈ R(h×w×D)×1
is subpatch of X with h < H and w < W . The number
of overlapping subpatches of size h × w in the patch of size
H×W is N = (H−h+1)×(W −w+1). A subpatch ﬁlter wk
consists of two subsequent ﬁlters with the size of the ﬁrst ﬁlter
k ∈ R(h×w×D)×1 being h × w and the size of the second
w(1)
k ∈ R(1×1×D)×1 being 1× 1. To explicitly show that
ﬁlter w(2)
a subpatch ﬁlter wk is composed of two basic ﬁlters w(1)
and
k
w(2)
k , w(2)
k , we denote the subpatch ﬁlter wk by wk = (w(1)
k )
and denote the size of subpatch ﬁlter by (h × w, 1 × 1). We
call the second ﬁlter w(2)
channel ﬁlter because its function is
k
to fully connect different channels. We call the ﬁrst ﬁlter w(1)
spatial ﬁlter because its size is larger than 1 × 1 and its role
k
is to extract features from both spatial and channel domain.
and the

The inner product between the spatial ﬁlter w(1)
k

subpatch x is

k = (w(1)
f (1)
k )

T x ∈ R1, k = 1, 2, ..., K1

(2)

where K1 is the number of output channels. Express the output
of the spatial ﬁlter as a K1-dimensional feature vector f(1) =
) ∈ RK1. Taking the feature vector as the
(f (1)
input of the channel ﬁlter w(2)
k , the second inner product is
obtained by

2 , ..., f (1)
K1

1 , f (1)

k = (w(2)
f (2)
k )

T f(1) ∈ R1, k = 1, 2, ..., K2

(3)

where K2 is the number of output channels.

k ) of size (h1×w1, 1×1) consists
Fig. 1. A subpatch ﬁlter wk = (w(1)
of a h1 × w1 ﬁlter w(1)
. The input is a patch P of
size H × W and the convolutional output is a patch P1 of size H1 × W1
with H1 = (H − h1 + 1) and W1 = (W − w1 + 1).

and a 1 × 1 ﬁlter w(2)

k , w(2)

k

k

CIFAR10 dataset supports our viewpoint. ResNet110 is a 110-
layer CNN with 1.7M parameters, and ResNet1202 is a 1202-
layer CNN with up to 19.4M parameters. However, ResNet110
achieves a test error of 6.43% whereas ResNet1202 achieves
a test error of 7.93%. That is, the classiﬁcation ability of
deep neural network may suffer from excessive parameters.
Therefore, it is important to explore new methods to learn
features in a more effective way.

III. PROPOSED METHOD

This paper is aimed at using subpatch ﬁlters to construct
compact and powerful CNNs. One of the characteristics of
the proposed method is that the size of subpatch ﬁlter is
smaller than that of the patch to be presented. In our method,
cascaded subpatch ﬁlters are used to represent a patch. We
take the cascaded subpatch ﬁlters as a whole and call
it
csconv ﬁlter. Applying the csconv ﬁlters layer by layer results
in a deep CNN which we call CSNet. In this section, we
ﬁrst introduce the subpatch ﬁlter. Next, we describe cascaded
subpatch ﬁlters. Then, the CSNet is presented. Finally, analysis
of the computational complex of CSNet is given.

A. Subpatch Filter

The task is to represent an input patch P ∈ RH×W×D
where H × W stands for the spatial size and D is the number
of channels. Throughout this paper, the spatial size is used
to express the patch size. By vectoring the three-order tensor,
P can be expressed as an H × W × D dimensional column
vector X ∈ R(H×W×D)×1. Conventional convolution uses a
linear ﬁlter Wk ∈ R(H×W×D)×1 whose size is the same as
the patch X. The conventional convolution can be computed
by inner product

fk = Wk

T X ∈ R1, k = 1, 2, ..., K

(1)

where K is the number of output channels. The convolution
converts the patch of spatial size H × W into a scalar fk. For
the sake of notation consistence, we use 1 × 1 to represent
the size of feature fk. Fig. 2(a) shows the conventional
convolution.

...HWPatch PDDsubpatch(1)1f(1)2f1(1)Kf1H1W2KPatch P1(2)kf......HWPatch PD1H1W1w2w1h2h1DD2W2H...Patch 1PPatch 2PPatch 1nP1nW1nH1nD...nhnw11Output 1w1h(1)1,1f(1)1,2f1,1(1)1,Kf(1)2,1f(1)2,1f2,1(1)2,Kf(1),1nf(1),2nf,1(1),nnKf1,2K2,2K1,2nK,2nK(2)1,kf(2),nkf(2)2,kfGenerated by Foxit PDF Creator © Foxit Softwarehttp://www.foxitsoftware.com   For evaluation only....HWPatch PDDsubpatch(1)1f(1)2f1(1)Kf1H1W2KPatch P1(2)kf1w1h11Output WDHPatch PK......HWPatch PD1H1W2W2H...Patch 1PPatch 2PPatch 1nP1nW1nH...nhnw11Output 1,2K2,2K1,2nK,2nK1h1w2w2h(1)1,1f(1)1,2f1,1(1)1,Kf(1)2,1f(1)2,2f(2)1,kf(2)2,kf(2),nkf(1),1nf(1),2nfsubpatchsubpatchsubpatch,1(1),nnKfkWkf2,1(1)2,KfGenerated by Foxit PDF Creator © Foxit Softwarehttp://www.foxitsoftware.com   For evaluation only....HWPatch PDDsubpatch(1)1f(1)2f1(1)Kf1H1W2KPatch P1(2)kf1w1h11Output WDHPatch PkfK......HWPatch PD1H1W2W2H...Patch 1PPatch 2PPatch 1nP1nW1nH...nhnw11Output 1,2K2,2K1,2nK,2nK1h1w2w2h(1)1,1f(1)1,2f1,1(1)1,Kf(1)2,1f(1)2,2f(2)1,kf(2)2,kf(2),nkf(1),1nf(1),2nfsubpatchsubpatchsubpatch,1(1),nnKf2,1(1)2,KfGenerated by Foxit PDF Creator © Foxit Softwarehttp://www.foxitsoftware.com   For evaluation only.4

i,k , w(2)

are several subpatch ﬁlters of different sizes, for the sake of
clarity, we denote the i-th subpatch ﬁlter of size (h1×w1, 1×1)
by wi = (w(1)
i,k ). Fig. 1 shows that convolving a subpatch
ﬁlter w1 of size (h1× w1, 1× 1) within the input patch of size
H×W results in an output patch P1 of size H1×W1 = (H−
h1+1)×(W−w1+1). But our goal is to output a 1×1 patch to
represent the input P. This goal can be arrived at by convolving
the output patch P1 with another subpatch ﬁlter w2 of size
(h2×w2, 1×1) with h2 ≤ h1 and w2 ≤ w1. The size H2×W2
of the output of w2 is (H1−h2 +1)×(W1−w2 +1). It can be
noted that H2 < H1 and W2 < W1. That is, once a subpatch
ﬁlter is used, the size of the output patch is decreased. The
subpatch ﬁlters are subsequently used until the output is of size
1× 1. Specially, th sizes of spatial ﬁlters can be expressed as:

H1 = H − h1 + 1, W1 = W − w1 + 1;
H2 = H1 − h2 + 1, W2 = W1 − w2 + 1;
...
Hn−1 = Hn−2−hn−1 +1, Wn−1 = Wn−2−wn−1 +1;
Hn = Hn−1−hn +1 = 1, Wn = Wn−1−wn +1 = 1.

(4)

It is noted that the size Hn−1 × Wn−1 of penultimate output
patch is the same as that hn × wn of the spatial ﬁlter of the
last subpatch ﬁlter.
Suppose that n subpatch ﬁlters are ﬁnally used to obtain a
1× 1 output patch. We denote the cascaded n subpatch ﬁlters
by [(h1 × w1, 1 × 1), (h2 × w2, 1 × 1), ..., (hn × wn, 1 × 1)].
If all the n subpatch ﬁlters have the same size h × w (i.e.,
h1 = h2 = ...h, w1 = w2 = ...w), then the cascaded ﬁlter
can be denoted by n ∗ (h × w, 1 × 1). We call the n cascaded
subpatch ﬁlters n-stage csconv ﬁlter. Fig. 2(b) demonstrates
an n-stage csconv ﬁlter.

Given a local patch, different ﬁlters can be used to deal with
it. An example of conventional ﬁlter, two-stage csconv ﬁlter,
and three-stage csconv ﬁlter is shown in Fig. 3. The input
patch is of size 7× 7 with N1 channels, the conventional ﬁlter
in Fig. 3(a) is of size 7 × 7, the two-stage csconv ﬁlter in
Fig. 3(b) is [(5 × 5, 1 × 1), (3 × 3, 1 × 1)], and the three-stage
csconv ﬁlter in Fig. 3(c) is 3 ∗ (3 × 3, 1 × 1). In Fig. 3(a),
the convolution directly generates an output of size 1× 1 with
N2 channels. In Fig. 3(b), feature channels of size 3 × 3 are
obtained by applying the (5 × 5, 1 × 1) subpatch ﬁlter, and
then an output of size 1 × 1 with N2 channels is obtained by
applying the (3×3, 1×1) subpatch ﬁlter on them. In Fig. 3(c),
feature channels of size 5 × 5 are ﬁrstly obtained by applying
the ﬁrst (3×3, 1×1) subpatch ﬁlter. And then feature channels
of size 3×3 are obtained by applying the second (3×3, 1×1)
subpatch ﬁlter on feature channels of size 5 × 5. Finally, an
output of size 1× 1 with N2 channels is obtained by applying
the third (3 × 3, 1 × 1) subpatch ﬁlter on feature channels
of size 3 × 3. It is seen that the conventional convolution is
the most simplest one and that the csconv convolution with a
three-stage csconv ﬁlter is the most complex one.

C. Form Cascaded Subpatch Network (CSNet) by Stacking
Csconv Layers

2 , 1×1), ..., (h(1)
n ×
Let F1 = [(h(1)
n , 1 × 1)] be a csconv ﬁlter. Applying F1 on an input
w(1)

1 , 1×1), (h(1)

2 ×w(1)

1 ×w(1)



(a) A conventional ﬁlter

(b) A two-stage csconv ﬁlter

(c) A three-stage csconv ﬁlter

Fig. 3. Abstrcating the input patch of size 7 × 7 using different ﬁlters. (a)
Convolution with a conventional ﬁlter of size 7 × 7, directly generating an
output of size 1×1. (b) Convolution with a two-stage csconv ﬁlter [(5×5, 1×
1), (3 × 3, 1 × 1)], generating one intermediate feature layer (consisting of a
number of feature channels) of size 3× 3. (c) Convolution with a three-stage
csconv ﬁlter 3 ∗ (3 × 3, 1 × 1), generating two intermediate feature layers of
sizes 5 × 5 and 3 × 3, respectively.

Eq. 2 and Eq. 3 are only involved in one subpatch. There
are N = (H − h + 1)× (W − w + 1) subpatches. So we apply
Eq. 2 and Eq. 3 on all the N subpatches. That is, the subpatch
ﬁlter wk of size (h× w, 1× 1) convolves with the input patch
P ∈ RH×W×D. The convolution is conducted without zero-
padding. Consequently, the output P1 of the convolution with
subpatch ﬁlter wk is a patch of size H1 × W1 where H1 is
(H − h + 1) and W1 is (W − w + 1). Fig. 1 demonstrates one
subpatch ﬁlter of size (h1 × w1, 1 × 1).

B. Generating a Csconv Filter by Cascaded Subpatch Filters
In the previous section, we explicitly denote a subpatch ﬁlter
k ) where k indexes channels. When there

by wk = (w(1)

k , w(2)

577......533（3×3,1×1）（3×3,1×1）（3×3,1×1）...1177...33（5×5,1×1）（3×3,1×1）...11333333Patch55subpatchsubpatchsubpatchPatchsubpatchsubpatchN1N2N1N211N277N1（7×7）PatchGenerated by Foxit PDF Creator © Foxit Softwarehttp://www.foxitsoftware.com   For evaluation only.577......533（3×3,1×1）（3×3,1×1）（3×3,1×1）...1177...33（5×5,1×1）（3×3,1×1）...11333333Patch55subpatchsubpatchsubpatchPatchsubpatchsubpatchN1N2N1N211N277N1（7×7）PatchGenerated by Foxit PDF Creator © Foxit Softwarehttp://www.foxitsoftware.com   For evaluation only.577......533（3×3,1×1）（3×3,1×1）（3×3,1×1）...1177...33（5×5,1×1）（3×3,1×1）...11333333Patch55subpatchsubpatchsubpatchPatchsubpatchsubpatchN1N2N1N211N277N1（7×7）PatchGenerated by Foxit PDF Creator © Foxit Softwarehttp://www.foxitsoftware.com   For evaluation only.5

Fig. 4. The overall structure of CSNet. The number of csconv ﬁlters and the number of subpatch ﬁlters in each csconv ﬁlter can be tuned to deal with
different tasks.

patch yields a 1 × 1 unit and convolving F1 over the whole
input channels yields a convolutional layer C1 (called csconv
layer) containing a number of 1 × 1 units. As the con-
ventional CNN, we can create a new CNN (called CSNet)
by stacking a number of csconv layers: C1, C2, ..., Cm with
2 ×
1 , 1× 1× M (i)
1 ), (h(i)
Ci = [(h(i)
ni ×
ni × w(i)
2 × N (i)
2 × Q(i)
w(i)
ni × M (i)
N (i)
ni )]. It is noted that we express
both spatial ﬁlter and channel ﬁlter as four-order tensors by
explicitly writing the number of input channels (N (i)
for
ni
spatial ﬁlter and M (i)
for channel ﬁlter) and the number of
ni
for spatial ﬁlter and Q(i)
output channels (M (i)
ni for channel
ni
ﬁlter).

1 × M (i)
1 × w(i)
1 × N (i)
2 , 1 × 1 × M (i)
2 × M (i)
ni × Q(i)
ni , 1× 1× M (i)

1 × Q(i)
2 ), ..., (h(i)

It is noted that the number of subpatch ﬁlters of a csconv
ﬁlter Ci is determined by the spatial size of the input patch
and the spatial size of each subpatch ﬁlter (see Eq. 4). It is
also noted that different csconv layers can have either different
or the same conﬁguration of csconv ﬁlters. For example, the
ﬁrst three csconv layers of one CSNet can have csconv ﬁlters
of [(5 × 5, 1 × 1), (3 × 3, 1 × 1)] , 2 ∗ (3 × 3, 1 × 1), and
3∗(3×3, 1×1), respectively. Fig. 4 shows the overall structure
of the proposed CSNet. The ﬁrst two csconv layers both have
a two-stage csconv ﬁlter. The number of csconv ﬁlters and
the number of subpatch ﬁlters in each csconv ﬁlter can be
tuned according to different tasks. In the proposed CSNet,
Rectiﬁed Linear Units (ReLUs) [16] follows the output of each
convolution of the subpatch ﬁlter. Sub-sampling layers can be
added in between the csconv layers as in CNN if necessary.

D. Computational Complex Analysis

Though the csconv convolution is much more complex than
conventional convolution, it does not mean that the parameters
of one deep CSNet have to be very huge. A comparison
of parameters consumed by conventional convolution and
the csconv convolution is shown in Tab. I. Suppose that a
conventional convolution has a ﬁlter of size 7 × 7 × N1 × N2
(see Fig. 3(a)), where 7 × 7 is the size of the convolutional
ﬁlter in spatial domain, N1 is the number of input feature
channels, and N2 is the number of output feature channels. The
corresponding three-stage csconv convolution (3∗(3×3, 1×1),
see Fig. 3(c)) can be implemented with different number of

input channels and output channels. Table I presents conﬁg-
urations of two common csconv layers denoted by csconv 1
and csconv 2. As shown in Table I, the parameters consumed
by conventional convolution, csconv 1 and csconv 2 are fc =
2 and fc2 = N1N2 + 29N1
2,
49N1N2 , fc1 = 9N1N2 +21N2
respectively. Therefore, the difference between conventional
convolution and csconv 1 is fc − fc1 = N2 × (40N1 − 21N2)
. If N2 ≤ 40
21 N1, then the number of parameters consumed by
csconv 1 is no larger than that of conventional convolution.
Similarly, the difference between conventional convolution and
csconv 2 is fc − fc2 = N1 × (48N2 − 29N1). If N2 ≥ 29
48 N1,
then the number of parameters consumed by csconv 2 is no
larger than that of conventional convolution. Especially, if
2.
N1 = N2, then fc = 49N1
It is obviously seen that the number of parameters consumed
by conventional convolution is lager than that of the proposed
csconv convolution.
In case that N1 and N2 do not satisfy the constraints above,
the 1× 1 convolution can be used as reduction layer to reduce
the number of intermediate output feature channels. This can
guarantee that the total number of parameters consumed by
csconv convolution is no larger than that of conventional
convolution. Since the parameters of each csconv layer are
no larger than those of the corresponding conventional convo-
lutional layer, the total parameters of one deep CSNet are also
no larger than those of the corresponding conventional neural
network.

2, and fc1 = 30N1

2, fc1 = 30N1

IV. EXPERIMENTAL RESULTS

We evaluate the proposed CSNet on four standard bench-
mark datasets: CIFAR10 [39], CIFAR100 [39], MNIST [5],
and SVHN [39]. We compare our CSNets with a dozen
well known networks that have achieved the state-of-the-art
performance on the four datasets. These networks include
Maxout (Maxout Networks) [8], NIN (Network in Network)
[14], NIN+LA (Networks with Learned Activation Func-
tions) [26], FitNet (Thin and Deep Networks) [19], DSN
(Deeply Supervised Networks) [13], DropConnect (Networks
using Dropconnect) [27], dasNet (Deep Attention Selective
Networks) [35], Highway (Networks Allowing Information
Flow on Information Highways) [23], ALL-CNN (ALL Con-
volutional Networks) [38], RCNN (Recurrent Convolutional

...............InputCSconv Layer 1CSconv Layer 2...CSconv Layer nOutputm-class softmax patchsubpatchpatchsubpatchsubpatchsubpatchGenerated by Foxit PDF Creator © Foxit Softwarehttp://www.foxitsoftware.com   For evaluation only.THE NUMBER OF PARAMETERS CONSUMED BY CONVENTIONAL

TABLE I

CONVOLUTION AND CSCONV CONVOLUTION

Method

Conventional

Structure

7×7×N1×N2

csconv 1

[(3×3×N1×N2,
1×1×N2×N2),
(3×3×N2×N2,
1×1×N2×N2),
(3×3×N2×N2,
1×1×N2×N2)]

csconv 2

[(3×3×N1×N1,
1×1×N1×N1),
(3×3×N1×N1,
1×1×N1×N1),
(3×3×N1×N1,
1×1×N1×N2)]

#params
N1 (cid:54)= N2
#params

N1 = N2

49N1N2

9N1N2 +21N2

2

N1N2 + 29N1

2

49N1

2

30N1

2

30N1

2

Neural Networks) [28], and ResNet (Deep Residual Networks)
[32].

A. Conﬁguration

We adopt the global average pooling scheme introduced in
[14] on the top layer of CSNet. We also incorporate dropout
layers with dropout rate of 0.5 [8] . In addition, we use Batch
Normalization (BN) [10] to accelerate the training stage. The
CSNet is implemented using the MatConvNet [40] toolbox
in the Matlab environment. We follow a common training
protocol [8] in all experiments. We use stochastic gradient
descent
technique with mini-batch of size 100 at a ﬁxed
constant momentum value of 0.9. Initial value for learning rate
and weight decay factor is determined based on the validation
set. The proposed CSNet is easy to converge and no particular
engineering tricks are adopted in all our experiments. All the
results are achieved without using the model averaging [12]
techniques which can help improve the performance.

To comprehensively evaluate the performance of the pro-
posed CSNet, we design three CSNets of different architec-
tures, each of which has different number of parameters. Our
small CSNet (CSNet-S), middle CSNet (CSNet-M) and large
CSNet (CSNet-L) have 0.96M, 1.6M and 3.5M parameters,
respectively. The conﬁgurations of CSNet-S, CSNet-M, and
CSNet-L are given in Table II. And the corresponding overall
structures are presented Fig.6. Though the three CSNets are
speciﬁcally designed for the CIFAR10 dataset, they are also
applied on the other three datasets with all the parameters
almost remaining the same. The only modiﬁcation is to change
the number of output feature channels of the last csconv layer
from 10 to 100 on CIFAR100 dataset.

As shown in Table II, the CSNet-S and the CSNet-M have
three csconv layers, and the CSNet-L has four csconv layers.
Since the input sample is small (32 × 32 or 28 × 28), the
receptive ﬁeld of the ﬁlters adopted by traditional methods
is typically of size 5 × 5. Therefore, our CSNets use csconv
ﬁlters of 2 ∗ (3 × 3, 1 × 1) to replace linear ﬁlters of size
5 × 5. Fig. 5 shows the two-stage csconv ﬁlter used in our
experiment. As shown in Fig. 6, max-pooling follows the ﬁrst
two csconv ﬁlters of each CSNet. Average-pooling is applied

6

Fig. 5. Abstracting one patch of size 5 × 5 with a two-stage csconv ﬁlter
2 ∗ (3 × 3, 1 × 1).

CONFIGURATIONS OF THREE DIFFERENT CSNETS. TWO-STAGE CSCONV

FILTERS ARE USED TO REPRESENT PATCHES OF SIZE 5 × 5

TABLE II

Patch size

C1

C2

C3

#params

Patch size

C1

C2

C3

#params

Patch size

C1

C2

C3

C4

CSNet-S

5x5

[(3×3×3×192, 1×1×192×96),
(3×3×96×192, 1×1×192×96)]
[(3×3×96×192, 1×1×192×96),
(3×3×96×192, 1×1×192×96)]
[(3×3×96×192, 1×1×192×96),
(3×3×96×192, 1×1×192×10)]

0.96M

CSNet-M

5x5

[(3×3×3×192, 1×1×192×192),
(3×3×192×192, 1×1×192×192)]
[(3×3×192×192, 1×1×192×192),
(3×3×192×192, 1×1×192×192)]
[(3×3×192×192, 1×1×192×192),
(3×3×192×192, 1×1×192×10)]

1.6M

CSNet-L

5x5

[(3×3×3×224, 1×1×224×224),
(3×3×224×224, 1×1×224×224)]
[(3×3×224×224, 1×1×224×224),
(3×3×224×224, 1×1×224×224)]
[(3×3×224×224, 1×1×224×224),
(3×3×224×224, 1×1×224×224)]
[(3×3×224×224, 1×1×224×224),
(3×3×224×224, 1×1×224×10)]

#params

3.5M

after the last csconv layer to assign one single score for each
class. Softmax classiﬁer is then used to recognize the objects.

B. Results on the CIFAR10 Dataset

CIFAR10 dataset [37] consists of 10 classes of images with
50K training images and 10K testing images. These images

......5533（3×3,1×1）（3×3,1×1）11Patch3333Generated by Foxit PDF Creator © Foxit Softwarehttp://www.foxitsoftware.com   For evaluation only.7

(a) CSNet-S

(b) CSNet-M

(c) CSNet-L

Fig. 6. Overall structures of three different CSNets. The three CSNets are designed for the CIFAR10 dataset. However, they are also applied on the other
three datasets with all the parameters almost remaining the same. (a) The CSNet-S with three csconv layers. (b) The CSNet-M with three csconv layers. (c)
The CSNet-L with four csconv layers.

are 32 × 32 color images including airplanes, automobiles,
ships, trucks, horses, dogs, cats, birds, deers and frogs. Before
training, we preprocess these images using global contrast
normalization and ZCA whitening. We carry on experiments
with and without data augmentation, respectively. For a fair
comparison, we obtain the augmented dataset by padding 4
pixels on each side, and then doing random cropping and
random ﬂipping on the ﬂy during training. The augmented data
is denoted by CIFAR10+. During testing, we only evaluate the
single view of the original 32 × 32 color image.

To have a quick overview of the performance of the CSNets,
we ﬁrstly compare CSNets with two well known neural
networks on this dataset. The ﬁrst one is the classic NIN
network which has 0.97M parameters. The second one is a
new network called ResNet [32] which is the champion of
the ILSVRC 2015 [20] classiﬁcation task. ResNet-110 [32]
is a really deep neural network which has up to 110 layers
and 1.7M parameters. ResNet-1202 [32] is even much deeper

QUICK OVERVIEW OF COMPARISON BETWEEN (SMALL, MIDDLE AND
LARGE) CSNETS AND THE CORRESPONDING COUNTERPARTS. THE

RESULTS ARE REPORTED ON CIFAR10 IN THE FORM OF CLASSIFICATION

TABLE III

ERROR (IN %)

Methods
NIN [14]
CSNet-S
ResNet-110 [32]
CSNet-M
ResNet-1202 [32]
CSNet-L

#layers

9
12
110
12
1202
16

#params
0.97M
0.96M
1.7M
1.6M
19.4M
3.5M

CIFAR10

CIFAR10+

10.41
8.33

-

8.15

-

7.74

8.81
6.98
6.43
6.38
7.93
5.68

and has 19M parameters. It can be seen that the CSNet-S
(0.96M) has a little fewer parameters than NIN, and that the
CSNet-M (1.6M) has 0.1M fewer parameters than ResNet110,

88......8832242242241033×31×13×31×1..................22432322243322422416162242242248822433322422422422433×33×33×33×31×11×1Max poolingMax pooling1×11×13×31×13×31×110-class softmax Average pooling1055555555......10-class softmax ............192323219233192192161619219219288192333192192192103Average pooling3×33×33×33×31×11×1Max poolingMax pooling1×11×13×31×13×31×18810555555......10-class softmax ............1923232963319296161619219296889633319296192103Average pooling3×33×33×33×31×11×1Max poolingMax pooling1×11×13×31×13×31×1881055555533333333333333333333Generated by Foxit PDF Creator © Foxit Softwarehttp://www.foxitsoftware.com   For evaluation only.88......8832242242241033×31×13×31×1..................22432322243322422416162242242248822433322422422422433×33×33×33×31×11×1Max poolingMax pooling1×11×13×31×13×31×110-class softmax Average pooling1055555555......10-class softmax ............192323219233192192161619219219288192333192192192103Average pooling3×33×33×33×31×11×1Max poolingMax pooling1×11×13×31×13×31×18810555555......10-class softmax ............1923232963319296161619219296889633319296192103Average pooling3×33×33×33×31×11×1Max poolingMax pooling1×11×13×31×13×31×1881055555533333333333333333333Generated by Foxit PDF Creator © Foxit Softwarehttp://www.foxitsoftware.com   For evaluation only.88......8832242242241033×31×13×31×1..................22432322243322422416162242242248822433322422422422433×33×33×33×31×11×1Max poolingMax pooling1×11×13×31×13×31×110-class softmax Average pooling1055555555......10-class softmax ............192323219233192192161619219219288192333192192192103Average pooling3×33×33×33×31×11×1Max poolingMax pooling1×11×13×31×13×31×18810555555......10-class softmax ............1923232963319296161619219296889633319296192103Average pooling3×33×33×33×31×11×1Max poolingMax pooling1×11×13×31×13×31×1881055555533333333333333333333Generated by Foxit PDF Creator © Foxit Softwarehttp://www.foxitsoftware.com   For evaluation only.CLASSIFICATION ERROR (IN %) FOR CIFAR10 USING VARIOUS METHODS.
A ‘-’ INDICATES THE CITED WORK DID NOT PRESENT RESULTS FOR THAT

TABLE IV

CLASSIFICATION ERROR (IN %) FOR CIFAR100 USING VARIOUS

TABLE V

METHODS

8

Methods
Maxoutt [8]
NIN [14]
NIN+LA [26]
FitNet [19]
DSN [13]
DropConnect [27]
dasNet [35]
Highway [23]
ALL-CNN [38]
RCNN-160 [28]
ResNet-110 [32]
ResNet-1202 [32]
CSNet-S
CSNet-M
CSNet-L

DATASET

CIFAR10

CIFAR10+

11.68
10.41
9.59

-

9.75
9.41
9.22

-

9.08
8.69

-
-

8.33
8.15
7.74

9.38
8.81
7.51
8.39
8.22

-
-

7.54
7.25
7.09
6.43
7.93
6.98
6.38
5.68

and that the CSNet-L (3.5M) has much fewer parameters than
ResNet1202.

The comparison results are presented in Tab. III. Compared
with NIN, the CSNet-S reduces the test error from 10.41%
to 8.33% (without data augmentation), which improves the
performance by more than two percent. The CSNet-M obtains
a test error of 6.38% which is a slightly lower than 6.44% of
ResNet110 (with data augmentation). However, the CSNet-M
has only 12 layers which are much fewer than the 110 layers
of ResNet-110. Therefore, it is much easier to train CSNet-
M than ResNet-110. Unlike ResNet-1020 which degrades
the performance due to the huge parameters, our CSNet-L
further reduces the test error to 5.68%. The above comparison
results demonstrate the superiority of the proposed CSNets. A
comprehensive comparison of various methods is presented in
Tab. IV. It can be seen that the CSNet-S is already among the
state-of -the-art results. The CSNet-M surpasses ResNet-100
by 0.05% and the CSNet-L surpasses ResNet-100 by 0.8%.

C. Results on the CIFAR100 Dataset

The CIFAR100 dataset [37] is just like the CIFAR10 dataset.
It has the same amount of training images and testing images
as the CIFAR10. However, CIFAR100 contains 100 classes
which are ten times of those of CIFAR10. Therefore, the
number of images in each class is only one tenth of CIFAR10.
The 100 classes in CIFAR100 are grouped into 20 super-
classes. Each image has two labels. One is the ”ﬁne” label
indicating the speciﬁc class and the other one is the ”coarse”
label indicating the super-class. Considering the number of
training images for each class, it is much more difﬁcult to
recognize the 100 classes of CIFAR100 than the 10 classes of
CIFAR10. There is no data augmentation for CIFAR100. We
use the same data preprocessing methods as in CIFAR00.

Methods
Maxout [8]
NIN [14]
NIN+LA [26]
FitNet [19]
DSN [13]
dasNet [35]
ALL-CNN [38]
Highway [23]
RCNN-160 [28]
CSNet-M

CIFAR100

38.57
35.68
34.40
35.04
34.57
33.78
33.71
32.24
31.75
30.24

CLASSIFICATION ERROR (IN %) FOR MNIST USING VARIOUS METHODS

TABLE VI

Methods
DropConnect [27]
FitNet [19]
NIN [14]
Maxout [8]
Highway [23]
DSN [13]
RCNN-96 [28]
CSNet-S

MNIST

0.57
0.51
0.47
0.45
0.45
0.39
0.31
0.31

Since there are 100 classes to be recognized, we adopt the
CSNet-M in this experiment. The only difference is that the
last convolutional layer of the third csconv layer outputs 100
feature channels, each of which is then averaged to generate
one score for one speciﬁc class. Details of performance
comparison are shown in Tab. V. It can be seen that CSNet-M
obtains a test error of 30.24% for CIFAR100, which surpasses
the second best performance (RCNN-160 with 31.75%) by
1.51 percent. It also should be noted that RCNN-160 has
1.87M parameters, which are about 0.27M larger than those
of CSNet-M.

D. Results on the MNIST Dataset

MNIST [5] is one of the most well known datasets in the
ﬁeld of machine learning. It consists of hand written digits
ranging from 0 to 9. There are 60000 training images and
10000 testing images which are 28 × 28 gray-scale images.
Only mean subtraction is used to preprocess the dataset.
Since MNIST is relatively a simpler dataset compared with
CIFAR10, CSNet-S is used in this experiment. The results of
performance comparison are shown in Tab. VI. It can be seen
that CSNet-S achieves the state-of-the-art performance with a
test error of 0.31%.

CLASSIFICATION ERROR (IN %) FOR SVHN USING VARIOUS METHODS

TABLE VII

Methods
Maxout [8]
FitNet [19]
NIN [14]
DropConnect [27]
DSN [13]
RCNN-160 [28]
CSNet-M

SVHN
2.47
2.42
2.35
1.94
1.92
1.80
1.90

E. Results on the SVHN Dataset

The SVHN (Street View House Numbers) [39] is a real-
world image dataset containing 10 classes representing digits
of 0 to 9. There are totally 630,420 32 × 32 color images
which are divided into three sets, 73,527 images in training
set, 26,032 images in testing set, and 531,131 images in extra
set. More than one digit may exist in an image, and the task
is to classify the digit located at the center. We followed the
training and testing procedure described by Goodfellow et
al. [8]. That is, 400 samples per class are randomly selected
from the training set, and 200 samples per class are randomly
selected from the extra set. These selected data together form
the validation set. The remaining images in the training set and
extra set compose the training set. The validation set is only
used for tuning hyper-parameter selection and not used during
training. Since there are large variations among one same kind
of digit in SVHN due to the changes of color and brightness,
it is much more difﬁcult to recognize digits in SVHN than
in MNIST. Therefore, local contrast normalization is used to
preprocess the samples. No data augmentation is used in this
experiment. To deal with the large variations of digits, we use
the CSNet-M in this experiment. The performance comparison
with other methods is shown in Tab. VII. It can be seen
that CSNet-M obtains a test error test error of 1.9%, which
already improves NIN (2.35% with 1.98M parameters) by
0.45 percent. CSNet-M achieves the second best performance
(1.9%) which is very close the best performance with a test
error of 1.8%.

V. CONCLUSION

In this paper, we have presented a novel CNN structure
called CSNet. The core of CSNet
is to represent a local
patch with one neuron which is obtained by using cascaded
subpatch ﬁlters. The subpatch ﬁlter has two characteristics:
(1) the spatial size of the subpatch ﬁlter is smaller than that
H × W of the input patch, (2) the subpatch ﬁlter consists
of an h × w (with h > 1 and w > 1) ﬁlter followed by
a 1 × 1 ﬁlter. The role of cascaded subpatch ﬁlters can be
considered as representing the input patch using a pyramid
with the resolution decreasing from H × W to 1 × 1 . Due to
the large ability of feature representation, the proposed method
achieves the state-of-the-art performance.

9

REFERENCES

[1] C. H. Chang, “Deep and shallow architecture of multilayer neural
networks,” IEEE Transactions on Neural Networks and Learning Systems,
2015, vol. 26, no. 10, pp. 2477-2486, 2015.

[2] L. Shao, D. Wu, and X. Li, “Learning deep and wide: a spectral method
for learning deep networks,” IEEE Transactions on Neural Networks and
Learning Systems, vol. 25, no. 12, pp. 2303-2308, 2014.

[3] Y. Bengio, A. Courville, and P. Vincent, “Representation learning: a
review and new perspectives,” IEEE Transactions Pattern Analysis and
Machine Intelligence, vol. 35, no. 8, pp. 17981828, Aug. 2013.

[4] M. Gong, J. Liu, H. Li, Q. Cai, and L. Su, “A Multi objective Sparse
Feature Learning Model for Deep Neural Networks,” IEEE Transactions
on Neural Networks and Learning Systems, vol. 26, no. 12, pp. 3263-
3277, 2015.

[5] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-based learning
applied to document recognition,” in Proceedings of the IEEE, vol. 86,
no. 11, pp. 2278-2324, 1998.

[6] R. Girshick, “Fast R-CNN,” in Proceedings of the IEEE International

Conference on Computer Vision, 2015, pp. 1440-1448.

[7] R. Girshick, J. Donahue, T. Darrell, and J. Malik, “Rich feature hi-
erarchies for accurate object detection and semantic segmentation,” in
Proceedings of IEEE Conference on Computer Vision and Pattern Recog-
nition, 2014, pp. 580-587.

[8] I. J. Goodfellow, D. Warde-Farley, M. Mirza, A. Courville, and Y. Bengio,

“Maxout networks,” CoRR, abs/1302.4389, 2013.

[9] K. He, X. Zhang, S. Ren, and J. Sun, “Spatial pyramid pooling in
deep convolutional networks for visual recognition,” in Proceedings of
European Conference on Computer Vision, 2014, pp. 346-361.

[10] S.

Ioffe

and C. Szegedy,

“Batch normalization: Accelerating
deep network training by reducing internal covariate shift,” CoRR,
abs/1502.03167, 2015.

[11] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S.
Guadarrama, and T. Darrell, “Caffe: convolutional architecture for fast
feature embedding,” CoRR, abs/1408.5093, 2014.

[12] A. Krizhevsky, I. Sutskever, and G. Hinton, “Imagenet classiﬁcation
with deep convolutional neural networks,” in Proceedings of Advances in
Neural Information Processing Systems, 2012, pp. 11061114.

[13] C.-Y. Lee, S. Xie, P. Gallagher, Z. Zhang, and Z. Tu, “Deeply supervised

nets,” CoRR, abs/1409.5185, 2014.

[14] M. Lin, Q. Chen, and S. Yan, “Network in network,” CoRR,

abs/1312.4400, 2013.

[15] J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional networks for
semantic segmentation,” in Proceedings of IEEE Conference on Computer
Vision and Pattern Recognition, 2015, pp. 3431-3440.

[16] V. Nair and G. E. Hinton, “Rectiﬁed linear units improve restricted
boltzmann machines,” in Proceedings of International Conference on
Machine Learning, 2010, pp. 807-814.

[17] S. Ren, K. He, R. Girshick, and J. Sun, “Faster R-CNN: towards real-
time object detection with region proposal networks,” in Proceedings of
Advances in Neural Information Processing Systems, 2015, pp. 91-99.

[18] Y. Pang, J. Cao, X. Li, “Learning Sampling Functions for Efﬁcient

Object Detection,” CoRR, abs/1508.05581, 2015.

[19] A. Romero, N. Ballas, S. E. Kahou, A. Chassang, C. Gatta, and Y.
Bengio, “Fitnets: Hints for thin deep nets,” CoRR, abs/1412.6550, 2014.
[20] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,
Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A.C. Berg, and F.
International
Li, “Imagenet large scale visual recognition challenge,”
Journal of Computer Vision, vol. 115, no. 3, pp. 211-252, Dec. 2015.

[21] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, and Y. Le-
Cun, “Overfeat: integrated recognition, localization and detection using
convolutional networks,” CoRR, abs/1312.6229, 2013.

[22] K. Simonyan and A. Zisserman, “Very deep convolutional networks for

large-scale image recognition,” CoRR, abs/1409.1556, 2014.

[23] R. K. Srivastava, K. Greff, and J. Schmidhuber, “Training very deep
networks,” in Proceedings of Advances in Neural Information Processing
Systems, 2015, pp. 2368-2376.

[24] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,
V. Vanhoucke, and A. Rabinovich, “Going deeper with convolutions,” in
Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition ,2015, pp. 1-9.

[25] M. D. Zeiler and R. Fergus, “Visualizing and understanding convo-
lutional neural networks,” in Proceedings of European Conference on
Computer Vision, 2014, pp. 818-833.

[26] F. Agostinelli, M. Hoffman, P. Sadowski, and P. Baldi, “Learning activa-
tion functions to improve deep neural networks, CoRR, abs/1412.6830,
2014.

10

[27] L. Wan, M. Zeiler, S. Zhang, Y. LeCun, and R. Fergus, “Regularization
of neural networks using dropconnect,” in Proceedings of International
Conference on Machine Learning, 2013, pp. 1058-1066.

[28] M. Liang and X. Hu, “Recurrent convolutional neural network for object
recognition,” in Proceedings of IEEE Conference on Computer Vision and
Pattern Recognition, 2015, p. 3367-3375.

[29] P. Agrawal, R. Girshick, and J. Malik, “Analyzing the Performance of
Multilayer Neural Networks for Object Recognition,” in Proceedings of
European Conference on Computer Vision, 2014, pp. 329-344.

[30] Z. Ji, Y. Pang, X. Li, “Relevance Preserving Projection and Ranking for
Web Image Search Reranking,” IEEE Transactions on Image Processing,
vol. 24, no. 11, pp. 4137-4147, 2015.

[31] Y. Pang, S.Wang, Y. Yuan, “Learning Regularized LDA by Clustering,”
IEEE Trans. Neural Netw. Learning Syst, vol. 25, no. 12, pp. 2191-2201,
2014.

[32] K. He, X. Zhang, S. Ren, and J. Sun, “Deep Residual Learning for

Image Recognition,” CoRR, abs/1512.03385 ,2015.

[33] X. Jiang, Y. Pang, X. Li, and J. Pan, “Speed up deep neural network
based pedestrian detection by sharing features across multi-scale models,”
Neurocomputing, doi:10.1016/j.neucom.2015.12.042, 2015.

[34] D. Wu and L. Shao, “Leveraging hierarchical parametric networks for
skeletal joints based action segmentation and recognition”, in Proceedings
of IEEE Conference on Computer Vision and Pattern Recognition, 2014,
pp. 724-731.

[35] M. Stollenga, J. Masci, F. J. Gomez, and J. Schmidhuber, “Deep
networks with internal selective attention through feedback connections,”
in Proceedings of Advances in Neural Information Processing Systems,
2014, pp. 3545-3553.

[36] Y. Netzer, T. Wang, A.Coates, A. Bissacco, B. Wu, and A. Y. Ng,
“Reading digits in natural images with unsupervised feature learning,”
Advances in Neural Information Processing Systems Workshop on Deep
Learning and Unsupervised Feature Learning, 2011.

[37] A. Krizhevsky and G. Hinton, “Learning multiple layers of features from
tiny images,” Masters thesis, Department of Computer Science, University
of Toronto, 2009.

[38] J. T. Springenberg, A. Dosovitskiy, T. Brox, and M. Riedmiller, “Striving

for Simplicity,” CoRR, abs/1412.6806v3, 2014.

[39] I.J. Goodfellow, Y. Bulatov, J. Ibarz, S. Arnoud, and V. Shet, “Multi-digit
number recognition from street view imagery using deep convolutional
neural networks,” CoRR, abs/1312.6082, 2013

[40] A. Vedaldi and K. Lenc, “MatConvNet: convolutional neural networks
for matlab,” in Proceedings of ACM Conference on Multimedia Confer-
ence, 2015, pp. 689-692.

