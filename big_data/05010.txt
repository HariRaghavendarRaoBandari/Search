6
1
0
2

 
r
a

 

M
6
1

 
 
]

.

A
N
h
t
a
m

[
 
 

1
v
0
1
0
5
0

.

3
0
6
1
:
v
i
X
r
a

Structure-preserving low multilinear rank approximation

of antisymmetric tensors

Erna Begovi´c Kovaˇc∗

Daniel Kressner†

March 17, 2016

Key words: tensor, low rank, singular value decomposition, Jacobi rotation.

AMS subject classiﬁcation. 15A69, 65F99

Abstract

This paper is concerned with low multilinear rank approximations to antisym-
metric tensors, that is, multivariate arrays for which the entries change sign when
permuting pairs of indices. We show which ranks can be attained by an antisymmet-
ric tensor and discuss the adaption of existing approximation algorithms to preserve
antisymmetry, most notably a Jacobi algorithm. Particular attention is paid to the
important special case when choosing the rank equal to the order of the tensor. It is
shown that this case can be addressed with an unstructured rank-1 approximation.
This allows for the straightforward application of the higher-order power method,
for which we discuss eﬀective initialization strategies.

1

Introduction

A tensor A ∈ Rn×···×n of order d ≥ 2 is called antisymmetric if its entries A(i1, i2, . . . , id)
change sign when permuting pairs of indices. For example, a tensor of order three with
entries is antisymmetric if

A(i1, i2, i3) = −A(i2, i1, i3) = −A(i3, i2, i1) = −A(i1, i3, i2),

i1, i2, i3 = 1, . . . , n.

For order two, the notion of antisymmetric tensors coincides of course with the notion
of skew-symmetric matrices.

Antisymmetric tensors play a major role in quantum chemistry, where the Pauli
exclusion principle implies that wave functions of fermions are antisymmetric under
permutations of variables. This antisymmetry needs to be taken into account when
solving the multiparticle Schr¨odinger equation determining such a wave function; see [11]
for a recent overview.

∗Faculty of Chemical Engineering and Technology, University of Zagreb, 10000 Zagreb, Croatia.
ebegovic@fkit.hr The author has been supported in part by Croatian Science Foundation under the
project 3670.

†MATHICSE-ANCHP, EPF Lausanne, 1015 Lausanne, Switzerland. daniel.kressner@epfl.ch

1

This paper is concerned with ﬁnding an approximation B to a given antisymmet-
ric tensor A such that B has a data-sparse representation and is again antisymmetric.
More speciﬁcally, we will consider an approximation of multilinear rank r in structure-
preserving Tucker decomposition

B = S ×1 U ×2 U ··· ×d U,

(1.1)

where S ∈ Rr×···×r for some r ≤ n is again antisymmetric and U ∈ Rn×r has orthonormal
columns. This choice is analogous to existing approaches for symmetric tensors, see,
e.g., [3, 4, 8].
In this paper, we demonstrate that some existing algorithms for the
symmetric case extend to the antisymmetric case. In particular, we study the extension
of the Jacobi algorithm by Ishteva, Absil, and Van Dooren [7].

Despite a number of similarities, there are pronounced diﬀerences between symmetric
and antisymmetric tensors. For example, every (multilinear) rank r can be attained
by a symmetric matrix or tensor.
In contrast, it is well known that skew-symmetric
matrices have even rank. Although this statement does not extend to d > 2, we will
see that there are still restrictions on the ranks that can be attained by anti-symmetric
tensors. In particular, the smallest possible nonzero rank is r = d. In this case, the
decomposition (1.1) simpliﬁes to

anti(cid:0)αu1 ⊗ u2 ⊗ ··· ⊗ ud(cid:1), α ∈ R,

with the antisymmetrizer A = anti(X ) deﬁned by

A(i1, . . . , id) :=

1

d! Xπ∈Sd

sign(π)X(cid:0)π(i1), π(i2), . . . , π(id)(cid:1),

(1.2)

(1.3)

where Sd denotes the symmetric group on {1, . . . , d}. This corresponds to the notion of
Slater determinants that feature prominently in the Hartree-Fock method from quantum
mechanics. The expression (1.2) suggests the more general decomposition anti(X ) for
(non-symmetric) tensor X of low tensor rank. This corresponds to a short sum of
Slater determinants used, e.g., in the Multi-Conﬁguration Self-Consistent Field method.
Such a low-rank model for antisymmetric tensors has been studied in the literature. In
particular, Beylkin, Mohlenkamp, and P´erez [2, 1] have developed an alternating least-
squares algorithm for approximating a given antisymmetric tensor A by anti(X ). The
algorithm employs L¨owdin’s rule to avoid having to deal with the exponentially many
terms in the sum (1.3). One contribution of this paper is a much simpler approach
for (1.2), that is, when X has rank 1: The best choice of X is given by a scalar multiple
of the best (non-symmetric) rank-1 approximation of A.
The rest of this paper is organized as follows. In Section 2, we study the multilinear
rank of an antisymmetric tensor and recall the higher-order singular value decomposition.
Section 3 is concerned with algorithms that aim at the antisymmetric low multilinear
rank approximations, the higher-order iterations method and a variant of the Jacobi
method. Section 4 is dedicated to the special case of rank-d approximation.

2

2 Multilinear rank of antisymmetric tensors

Let us ﬁrst recall some basic concepts related to the multilinear rank of a tensor; see [9] for
details. For any 1 ≤ µ ≤ d, the µth matricization of a general tensor X ∈ Rn1×n2×···×nd
is the nµ ×Qν6=µ nν matrix X(µ) deﬁned by

X(µ)(iµ, j) = X (i1, . . . , id),

j = j(i1, . . . , id) := 1 +

d

Xν=1

ν6=µ

ν−1

(iν − 1)

Yη=1

η6=µ

nη.

(2.1)

For an antisymmetric tensor, all matricizations are essentially the same.

The multilinear rank of X is the tuple (r1, r2, . . . , rd) deﬁned by rµ = rank(X(µ)). Note
that X(µ) is a matrix and hence rµ ≤ min{nµ,Qν6=µ nν}.
Lemma 2.1 Let A ∈ Rn×n×···×n be an antisymmetric tensor of order d. Then A
(−1)|µ−ν|A

(ν) holds for any 1 ≤ µ, ν ≤ d.

(µ) =

Proof. Without loss of generality, let µ ≤ ν. According to (2.1), X(µ)(iµ, j) =
X (i1, . . . , id) implies X(ν)(iµ, j) = X (i1, . . . , iµ−1, iµ+1, . . . , iν , iµ, iν+1, . . . , id). The re-
sult follows from the observation that the permutation (1, . . . , µ − 1, µ + 1, . . . , ν, µ, ν +
1, . . . , d) has sign (−1)|µ−ν|.
Lemma 2.1 implies that the multilinear rank of A always takes the form (r, . . . , r)
for some 1 ≤ r ≤ n. In the following, we will simply refer to r as the multilinear rank of
an antisymmetric tensor.

2.1 Restrictions on the multilinear rank

It is well known that skew-symmetric matrices have even rank. It turns out that this
property does not extend to antisymmetric tensors; it is simple to construct tensors of
higher order with odd multilinear ranks. However, the following theorem shows that
antisymmetry still imposes some (weaker) restrictions on the ranks of antisymmetric
tensors that are of small size n relative to d.
Theorem 2.2 Let A ∈ Rn×n×···×n be an antisymmetric tensor of order d ≥ 3. Then
the multilinear rank r of A satisﬁes

(i) r = 0 for n < d;

(ii) r ≤ d for n = d or n = d + 1;
(iii) r ≤ n for n ≥ d + 2.
There exist tensors A for which equality is attained in (i)–(iii).

Proof. We will make use of the fact that the entries of A satisfy

A(i1, i2, . . . , id) = 0,

if ip = iq, for some p 6= q, 1 ≤ p, q ≤ d.

(2.2)

3

(i) According to (2.2), all d indices i1, . . . , id ∈ [1, n] need to be mutually diﬀerent for
an entry A(i1, i2, . . . , id) to be nonzero. When n < d, this is clearly not possible
and hence A = 0.

(ii) For n = d, the condition r ≤ d follows from the size of the matricizations. To show
that equality is attained, consider the tensor A = d! anti(X ) where all entries of
X ∈ Rd×···×d are zero except for X (1, 2, . . . , d) = 1. For arbitrary 1 ≤ i ≤ d choose
the permutation p = (i, 1, . . . , i − 1, i + 1, . . . , d). By deﬁnition, A(p) = sign(p) =
(−1)i−1. By letting j = j(cid:0)p(2), . . . , p(d)(cid:1), it follows that the jth column of A(1)
equals (−1)i−1ei with the ith unit vector ei ∈ Rn. In particular, A(1) has d linearly
independent columns and is thus of rank d.
Now, let n = d + 1 and assume, without loss of generality, that A 6= 0. We
denote the rows of the matricization A(1) by A1,(1), . . . , Ad+1,(1) ∈ Rnd−1
. This
matricization has rank at most d if we can show that these rows are linearly
dependent. Let

αk := A(1, . . . , k − 1, k + 1, . . . , d + 1),

k = 1, . . . , d + 1.

Since A 6= 0, at least one αk is diﬀerent from zero. Let us now consider the column
of A(1) corresponding to a ﬁber A(:, i2, . . . , id) for some i2, . . . , id ∈ [1, d + 1]. We
may assume that i2, . . . , id are mutually distinct because otherwise this ﬁber is
zero. For the moment, we also assume that these indices are ordered, that is,
1 ≤ i2 < i3 < ··· < id ≤ d + 1. By the pigeon hole principle, there are two integers
1 ≤ k < ℓ ≤ d + 1 such that k, l 6∈ {i2, . . . , id}. The situation is now as follows:

i2
ik
= ··· =
1

···
···

···
k − 1 k k + 1 ···

ik+1
il−1
= ··· =

l − 1 l

···
il
id
= ··· =
d + 1
l + 1 ···

In particular, this implies

A(k, i2, . . . , id) = (−1)k−1A(i2, . . . , ik−1, k, ik+1, . . . , id) = (−1)k−1αℓ,
A(ℓ, i2, . . . , id) = (−1)ℓ−2A(i2, . . . , iℓ−1, ℓ, iℓ+1, . . . , id) = (−1)ℓ−2αk.

Using that A(i1, i2, . . . , id) is only nonzero for mutually distinct indices, we arrive
at the linear combination

d+1

Xi1=1

(−1)i1 αi1A(i1, i2, . . . , id) = (−1)kαkA(k, i2, . . . , id) + (−1)ℓαℓA(ℓ, i2, . . . , id)

= (−1)2k−1αkαℓ + (−1)2ℓ−2αkαℓ = −αkαℓ + αkαℓ = 0.

Since this relation is not aﬀected by a permutation of i2, . . . , id, it also holds if
these indices are not ordered. In summary, we have shown that

d+1

Xi1=1

(−1)i1 αi1 Ai1,(1) = 0

4

and thus the rank of A(1) is at most d.
For n = d + 1 equality is attained by the tensor used in the construction for n = d
bordered with zeros.

(iii) Let n ≥ d + 2. By the size of the matricization, r ≤ n. To show that r = n can
be attained, let us ﬁrst deﬁne the integer vector h = (1, 2, . . . , n, 1, . . . , d − 1). We
choose the tensor X ∈ Rn×n×···×n to be zero except for

X (hk, hk+1, hk+2, . . . , hk+d−1) = −d!,

k = 1, 2, . . . , n.

The corresponding sets σk = {hk, hk+1, hk+2, . . . , hk+d−1} ⊂ N all have cardinality
d for k = 1, 2, . . . , n. The set {1, 3, 4, . . . , d} = σ1 \ {2} is only contained in σ1. In
particular, n > d ≥ 3 implies that it is not contained in σn = {n, 1, . . . , d − 1}.
This shows A(:, 1, 3, 4, . . . , d) = e2. Analogously, A(:, 2, 4, 5, . . . , d + 1) = e3 and
A(:, 3, 5, 6, . . . , d + 2) = e4. This construction can be continued until we arrive at
the set {n − 1, 1, . . . , d − 2} = σn−1 \ {n}, which is not contained in σ1 because of
n − 1 > d, or in any other σk except σn−1. Hence, A(:, n − 1, 1, . . . , d − 2) = en.
Finally, we have A(:, n, 2, . . . , d − 2) = e1. In summary, we have found n linearly
independent columns of A(1) and, therefore, the multilinear rank of A is n.

2.2 HOSVD
Given a general tensor X ∈ Rn1×···×nd, the higher-order singular value decomposition
(HOSVD) introduced in [3] proceeds by computing the SVDs of the matricizations X(µ),
1 ≤ µ ≤ d, and letting Vµ ∈ Rnµ×nµ contain the left singular vectors. Setting T =
X ×1 V T

d yields the Tucker decomposition

1 ×2 V T

2 ×3 ··· ×d V T

X = T ×1 V1 ×2 V2 ··· ×d Vd.

S ×1 U1 ×2 U2 ··· ×d Ud,

The truncated HOSVD for a given multilinear rank (r1, . . . , rd) with rµ ≤ nµ is obtained
by setting
(2.3)
with Uµ = Vµ(:, 1 : rµ) and S = T (1 : r1, 1 : r2, . . . , 1 : rµ). This gives a quasi-best
approximation of X , in the sense that the approximation error in the Frobenius norm,
kX − S ×1 U1 ··· ×d Udk, is within a factor √d of the error of the best rank-(r1, . . . , rd)
approximation. In particular, if X happens to have multilinear rank (r1, . . . , rd) then
the decomposition (2.3) is exact.
We now apply the truncated HOSVD to obtain an approximation of multilinear rank
r to an antisymmetric tensor A. By Lemma 2.1, all matrices Uµ in (2.3) can be chosen
In turn S = A ×1 U T ··· ×d U T is again antisymmetric.
equal to a ﬁxed matrix U .
In summary, the truncated HOSVD described in Algorithm 1 automatically preserves
structure and produces a quasi-best antisymmetric approximation.

5

Algorithm 1 Truncated HOSVD of antisymmetric tensor

Compute matrix U ∈ Rn×r containing the leading r left singular vectors of A(1).
Set S = A ×1 U T ··· ×d U T .
Return approximation S ×1 U ··· ×d U .

Corollary 2.3 Let A be an antisymmetric tensor of order d. Then the multilinear rank
r of A satisﬁes r = 0 or r = d or d + 2 ≤ r ≤ n. Any of these ranks can be attained.

Proof. By the discussion above, an antisymmetric tensor of multilinear rank r can
be written as A = S ×1 U ···×d U , where the r ×···× r tensor S is again antisymmetric
and has multilinear rank r. The statement of the corollary now follows from applying
Theorem 2.2 to S.
order d takes the form

Let us inspect the case r = d more closely. Any antisymmetric d × ··· × d tensor of
(2.4)
for some α ∈ R; see also the construction in the proof of Theorem 2.2 (i). By letting
U = [u1, u2, . . . , ud], the truncated HOSVD implies that any antisymmetric tensor of
order d and multilinear rank d takes the form

S = anti(αe1 ⊗ e2 ⊗ ··· ⊗ ed),

A = anti(αe1 ⊗ e2 ⊗ ··· ⊗ ed) ×1 U ×2 U ··· ×d U = anti(αu1 ⊗ u2 ⊗ ··· ⊗ ud),

verifying the claim (1.2) from the introduction.

3 Low multilinear rank approximation

In this section, we discuss two iterative methods that aim to compute a best antisym-
metric multilinear rank-r approximation

min(cid:8)kA − S ×1 U ··· ×d Uk : S ∈ Rr×···×r antisymmetric, U ∈ Rn×r(cid:9),

starting, for example, from the truncated HOSVD of A. Both methods are based on the
fact that this minimization problem is equivalent to solving

max(cid:8)kA ×1 U T ··· ×d U Tk : U ∈ Rn×r with U T U = Ir(cid:9)

(3.1)

and setting S = A ×1 U T ··· ×d U T ; see, for example, [4].
Remark 3.1 For symmetric tensors, there is numerical evidence (see, e.g., [7]) that the
best (unstructured) approximation of multilinear rank r can usually be chosen symmetric.
For d = 2 and general r, this follows from the spectral decomposition. For general d and
r = 1, this property has recently been shown by Friedland [5]. For general d and r, this
question remains open.

For antisymmetric tensors, we will observe the analogous phenomenon below; it ap-
pears that the best (unstructured) approximation of multilinear rank r can usually be

6

chosen antisymmetric. For d = 2 and even r, this property follows from the real Schur
decomposition. For r = d and general d, we will see in Section 4 that it is actually
the unstructured rank-1 approximation that gives an antisymmetric multilinear rank-d
approximation.

To simplify the presentation, we will consider the case d = 3 for the rest of this
section; all developments extend in a relatively straightforward manner to general d > 3.

3.1 HOOI

The higher-order orthogonal iteration (HOOI) introduced in [10] is a popular approach
to the best low multilinear rank approximation of a general tensor. It consists of ap-
plying alternating least squares (ALS) to the unstructured variant of the maximization
problem (3.1):

max(cid:8)kA ×1 U T

1 ×2 U T

2 ×3 U T

3 k : Uµ ∈ Rn×r with U T

µ Uµ = Ir, µ = 1, 2, 3(cid:9).

One step of the method optimizes a single factor Uµ while keeping the other two factors
ﬁxed. The resulting optimization problem admits a straightforward solution by the SVD;
see Algorithm 2.

Algorithm 2 HOOI for multilinear rank-(r, r, r) approximation

Apply Algorithm 1 to choose initial factors U1 = U2 = U3 = U .
repeat

X = A ×2 U T
Compute matrix U1 ∈ Rn×r containing the leading r left singular vectors of X(1).
Y = A ×1 U T
Compute matrix U2 ∈ Rn×r containing the leading r left singular vectors of Y(2).
Z = A ×1 U T
Compute matrix U3 ∈ Rn×r containing the leading r left singular vectors of Z(3).

2 ×3 U T
1 ×3 U T
1 ×2 U T

3

3

2

until convergence
S = Z ×3 U T
Return approximation S ×1 U1 ×2 U2 ×3 U3.

3

Note that the iterates of Algorithm 2 are not antisymmetric. However, similarly as
in the symmetric case, we have observed that Algorithm 2 often converges towards an
antisymmetric approximation; see Section 3.3 below. To antisymmetrize the output of
Algorithm 2, one could set all factors equal to the factor Uµ that maximizes (3.1).

A simple antisymmetric variant of Algorithm 2 consists of setting all factors to the
factor that has been obtained from the SVD in one step. In the symmetric case, this
variant has been observed to suﬀer from convergence problems [7] and we observed
similar diﬃculties in the antisymmetric case.

3.2 Jacobi algorithm

In contrast to HOOI, the Jacobi algorithm proposed for symmetric tensors in [7] preserves
structure, that is, all iterates stay symmetric. In this section, we develop an extension

7

of this algorithm to antisymmetric tensors.

It will be convenient to rewrite the maximization problem (3.1) as

where

max(cid:8)f (Q) : Q ∈ Rn×n with QT Q = In(cid:9),

f (Q) = kA ×1 M QT ×2 M QT ×3 M QTk2,

M = (cid:18)Ir 0
0(cid:19) .

0

(3.2)

We will denote a Givens rotation acting on rows/columns i and j by

R(i, j, φ) =

i

j

cos φ

sin φ

I

− sin φ
cos φ

I





i

j

.





I

In the following, (i, j) will be called a pivot pair.

The main idea of the Jacobi algorithm is to repeatedly apply Givens rotations that
increase the norm of the (1 : r, 1 : r, 1 : r) subtensor. For this purpose, it will be suﬃcient
to consider rotations corresponding to the pivot pairs

(1, r + 1),
(2, r + 1),

(1, r + 2),
(2, r + 2),

. . .
. . .

(1, n)
(2, n)

...

...

...

(r, r + 1),

(r, r + 2),

. . .

(r, n).

(3.3)

In every iteration of the Jacobi algorithm, we choose a pivot pair that produces a

direction of suﬃciently strong descent. Letting

dij =

∂
∂φ

R(i, j, φ)(cid:12)(cid:12)(cid:12)φ=0

=

i

0

1

I

j

−1
0

i

j

,





I

I





we can always ﬁnd pivot pairs (i, j) among (3.3) such that

|hgradf (I), diji| ≥ ǫkgradf (I)k

(3.4)

holds, provided that 0 < ǫ < 2/n; see [7, Lemma 5.2].

Once a pivot pair (i, j) satisfying (3.4) is determined, we choose the rotation angle

φ that maximizes f , i.e., we solve

max(cid:8)f(cid:0)R(i, j, φ)(cid:1) : φ ∈ [0, π](cid:9),

8

(3.5)

Because of 1 ≤ i ≤ r < j ≤ n, the tensor B = A×1 R(i, j, φ)T ×2 R(i, j, φ)T ×3 R(i, j, φ)T
diﬀers from A within the subtensor (1 : r, 1 : r, 1 : r) only in the three slices (i, 1 :
r, 1 : r), (1 : r, i, 1 : r), and (1 : r, 1 : r, i). Because of antisymmetry, these slices have
identical norms and we can ignore their intersections. Hence, (3.5) becomes equivalent
to maximizing

kB(i, 1 : r, 1 : r)k2 =

=

r

Xp,q=1

r

Xp,q=1

p,q6=i

Let

B(i, p, q)2 =

r

Xp,q=1

p,q6=i

B(i, p, q)2

(cid:0) cos φA(i, p, q) + sin φA(j, p, q)(cid:1)2 =: ψ(φ).

(3.6)

α1 =

r

Xp,q=1

p,q6=i

A(i, p, q)2, α2 =

r

Xp,q=1

p,q6=i

A(i, p, q)A(j, p, q), α3 =

r

Xp,q=1

p,q6=i

A(j, p, q)2.

Then the derivative of (3.6) takes the form

ψ′(φ) = −2α1 cos φ sin φ + 2α2(cos2 φ − sin2 φ) + 2α3 cos φ sin φ.

In order to ﬁnd the zeros of this function, we divide it by cos2 φ and solve the resulting
quadratic equation in t = sin φ/ cos φ:

α2t2 + (α1 − α3)t − α2 = 0.

Among the two solutions to this equation, we choose the one that maximizes (3.6).
Algorithm 3 summarizes the described procedure.

Algorithm 3 Jacobi algorithm for antisymmetric multilinear rank-r approximation

Apply Algorithm 1 to choose initial factor U ∈ Rn×r.
Choose U⊥ such that Q = [U, U⊥] is orthogonal.
Set A1 = A ×1 QT ×2 QT ×3 QT .

repeat

Choose (i, j) according to (3.3) and (3.4).
Determine φ that maximizes (3.6).
Qk+1 = QkR(i, j, φ)
Ak+1 = Ak ×1 R(i, j, φ)T ×2 R(i, j, φ)T ×3 R(i, j, φ)T

until convergence
U = Qk(:, 1 : r)
Return approximation A ×1 U U T ×2 U U T ×3 U U T .

For choosing the pivot pair (i, j) in Algorithm 3, we traverse the list (3.3) cyclically.
For each pair, the condition (3.4) is checked. If (i, j) does not fulﬁll this condition, it is
skipped and the algorithm continues checking the next pair.

9

Although observed in practice, it cannot be guaranteed that Algorithm 3 produces
the minimum of the function in (3.2). The proof of a weaker convergence result for
symmetric tensors [7, Theorem 5.4] directly extends to antisymmetric tensors, resulting
in the following statement.

Theorem 3.2 Let (Qk) be the sequence of orthogonal matrices generated by Algorithm 3
applied to an antisymmetric tensor A ∈ Rn×n×n. Then every accumulation point of (Qk)
is a stationary point of the function f from (3.2).

3.3 Numerical Experiments

The algorithms described in this sections have been implemented and tested in Matlab
version 7.11.

In our ﬁrst set of experiments, we study the approximation error obtained by trun-
cated HOSVD, HOOI, and the Jacobi algorithm. The latter two algorithms are iterative;
they are considered converged when the norm of the gradient of the objective function
is 10−10 or below. We have chosen ǫ = 1/(10n) in the condition (3.4) of the Jacobi
algorithm. We tested the algorithms with random tensors generated by applying anti-
symmetrizer from (1.3) to tensors with uniformly distributed random entries from the
interval [0, 1]. Figure 1 shows that HOOI and the Jacobi algorithm always improve
upon the approximation obtained from the HOSVD. In many cases, HOOI and the Ja-
cobi algorithm result in the same (antisymmetric) approximation.
In cases when the
error of the Jacobi algorithm is smaller than the one of HOOI, it is observed that the
tensor produced by HOOI is not antisymmetric. On the other hand, when the error
of HOOI is smaller, the tensor produced by HOOI is antisymmetric. This leads us to
conjecture that the best (unstructured) approximation of multilinear rank (r, r, r) to a
generic antisymmetric tensor can always been chosen antisymmetric for r ≥ 3.

3.5

3

2.5

2
0

HOSVD
HOOI
Jacobi

20

40

60

80

100

3

2.8

2.6

2.4

2.2

2

1.8

1.6
0

HOSVD
HOOI
Jacobi

20

40

60

80

100

(a) Multilinear rank 3

(b) Multilinear rank 6

Figure 1: Approximation error of low multilinear rank approximation to 100 random
antisymmetric 10 × 10 × 10 tensors.

10

Figure 2 yields insights into the convergence behavior of HOOI and Jacobi algorithm
for a representative run with a random antisymmetric tensor. To emphasize the beneﬁts
from initializing with the truncated HOSVD we compare with using no initialization,

that is, instead of using Algorithm 1 we set U = h Ir

0 i and Q = In in Algorithms 2 and 3,

respectively. Apart from the approximation error we also show the norm of the gradient
of the objective function.

2.5

2.45

2.4

2.35

2.3

2.25

2.2
0

HOOI with identity initialization
HOOI with HOSVD initialization
Jacobi with identity initialization
Jacobi with HOSVD initialization

5

10

15

20

105

100

10−5

10−10
0

HOOI with identity initialization
HOOI with HOSVD initialization
Jacobi with identity initialization
Jacobi with HOSVD initialization

10

20

30

40

50

(a) Approximation error

(b) Norm of gradient of objective function

Figure 2: Convergence behavior of HOOI and Jacobi algorithm for multilinear rank-6
approximation of random antisymmetric 10 × 10 × 10 tensor.

We have also considered antisymmetric tensors for which the matrizations exhibit

rapid singular value decays. To construct such a tensor, consider the function

f (x, y, z) = exp(−px2 + 2y2 + 3z2)

on [0, 1]3. Then we let X contain its discretization:
X (i1, i2, i3) = f(cid:0)ξi1, ξi2, ξi3(cid:1),

iµ = 1, . . . , n,

where ξi = (i − 1)/(n − 1), and set A = anti(X ). Figure 3 shows the obtained results
for n = 20. It reveals that the HOSVD gives an excellent initial approximation. This is
also an example where the Jacobi algorithm with no initialization fails to converge to a
global optimum.

4 Multilinear rank-d approximation

Antisymmetric tensors of order d and multilinear rank d have the very particular struc-
ture (1.2). As we will discuss in this section, this simpliﬁes the approximation with such
tensors signiﬁcantly.

The following basic lemma plays a key role; it extends the well known fact that

uT Au = 0 always holds for a skew-symmetric matrix A.

11

HOOI with identity initialization
HOOI with HOSVD initialization
Jacobi with identity initialization
Jacobi with HOSVD initialization

x 10−3

8

7

6

5

4

3

2

1

0
0

5

10

15

20

105

100

10−5

10−10

10−15

10−20
0

HOOI with identity initialization
HOOI with HOSVD initialization
Jacobi with identity initialization
Jacobi with HOSVD initialization

5

10

15

20

(a) Approximation error

(b) Norm of gradient of objective function

Figure 3: Convergence behavior of HOOI and Jacobi algorithm for multilinear rank-7
approximation of function-related tensor.

Lemma 4.1 Let A ∈ Rn×···×n be an antisymmetric tensor of order d ≥ 2 and u ∈ Rn.
Then A ×µ u ×ν u = 0 for any 1 ≤ µ < ν ≤ d.

Proof. Without loss of generality, we may assume that µ = d − 1 and ν = d. Then

any entry of B = A ×µ u ×ν u satisﬁes

n

B(i1, . . . , id−2) =

A(i1, . . . , id−2, j, k)uj uk

Xj,k=1
Xj,k=1

n

= −

A(i1, . . . , id−2, k, j)uj uk = −B(i1, . . . , id−2),

which implies B = 0.
The following theorem establishes an equivalence between the best antisymmetric
multilinear rank-d approximation and the best unstructured rank-1 approximation of an
antisymmetric tensor.
Theorem 4.2 Let A ∈ Rn×···×n be an antisymmetric tensor of order d. Then

max(cid:8)kA ×1 U T ··· ×d U Tk : U ∈ Rn×d with U T U = Id(cid:9)
[u1, . . . , ud]T [u1, . . . , ud] = Id(cid:9)

d | :
d | : kv1k = ··· = kvdk = 1(cid:9).

= d! max(cid:8)|A ×1 uT
= d! max(cid:8)|A ×1 vT

1 ··· ×d uT
1 ··· ×d vT

Proof. Let α = |A ×1 uT

1 ··· ×d uT

d |. Using (2.4),

(4.1)

(4.2)

kA ×1 U T ··· ×d U Tk2 = k anti(αe1 ⊗ ··· ⊗ ed)k2 = α2k anti(e1 ⊗ ··· ⊗ ed)k2 = (αd!)2,
which shows (4.1).

12

Consider vectors v1, . . . , vd assuming the maximum in (4.2). By the QR decomposi-
tion, there is an upper triangular matrix R ∈ Rd×d with |rµµ| ≤ 1 for µ = 1, . . . , d and
a matrix [u1, . . . , ud] with orthonormal columns such that

Using Lemma 4.1,

[v1, . . . , vd] = [u1, . . . , ud]R.

A ×1 vT

1 ··· ×d vT

d = A ×1

d

Xµ1=1

rµ1,1uT

µ1 ··· ×d

d

Xµd=d

rµd,duT
µd

d

d

=

···

Xµ1=1

Xµd=d
A ×1 rµ1,1uT
= A ×1 r11uT
1 ··· ×d rdduT

µd

µ1 ··· ×d rµd,duT
d = r11 ··· rddA ×1 uT
d | ≤ |A ×1 uT

1 ··· ×d uT
d .
1 ··· ×d uT
Because of |rµµ| ≤ 1, this implies |A ×1 vT
d | and
hence (4.2) cannot be larger than (4.1). On the other hand, trivially, (4.1) cannot
be larger than (4.2). This shows the equality (4.2).

1 ··· ×d vT

4.1 HOPM

Algorithm 4 recalls the higher-order power method (HOPM) proposed in [4] for ﬁnding
a rank-1 approximation of a tensor A ∈ Rn×···×n.
Algorithm 4 HOPM for rank-1 approximation

Choose initial vectors u1, . . . , ud ∈ Rn.

repeat

2 ×3 uT
1 ×3 uT

3 ··· ×d uT
3 ··· ×d uT

d

d

v1 = A ×2 uT
u1 = v1/kv1k
v2 = A ×1 uT
u2 = v2/kv2k

...

vd = A ×1 uT
ud = vd/kvdk

1 ×2 uT

2 ··· ×d−1 uT

d−1

until convergence
α = A ×1 uT
Return approximation αu1 ⊗ u2 ⊗ ··· ⊗ ud.

1 ··· ×d uT

d

Assuming that Algorithm 4 converges to a best rank-1 approximation with mutually
orthogonal vectors uµ, Theorem 4.2 allows us to construct the best antisymmetric mul-
tilinear rank-d approximation anti(αu1 ⊗··· ⊗ ud). The following lemma assures mutual
orthogonality.
Lemma 4.3 Let A ∈ Rn×···×n be an antisymmetric tensor of order d ≤ n. Then the
vectors u1, . . . , ud returned by HOPM form an orthonormal basis, provided that HOPM
does not encounter a zero vector.

13

Proof. After the ﬁrst step of HOPM, Lemma 4.1 implies

hv1, uνi = A ×1 uT

ν ×2 uT

2 ×3 uT

3 ··· ×d uT

d = 0

for any ν 6= 1. Hence, each step of HOPM orthogonalizes one of the vectors uµ =
vµ/kvµk. In turn, the statement of the lemma holds after at least one sweep of HOPM,
even if the initial vectors are not orthogonal.

Remark 4.4 To ensure orthogonality numerically, we perform another orthogonaliza-
tion step after each step of Algorithm 4. In principle, this procedure can also be applied
to HOOI, yielding an (unstructured) multilinear rank (r1, . . . , rd) approximation with
mutually orthogonal basis matrices Uµ. This can then be turned into an antisymmetric
multilinear rank-(r1 + ··· + rd) approximation by setting U = [U1, . . . , Ud]. We have
tested this idea numerically and observed that this often yields a good approximation but
the approximation error is usually worse compared to the result of the Jacobi algorithm.

4.2

Initialization

It remains to discuss a proper initialization strategy for HOPM. For general d, we use
the truncated HOSVD from Section 3. For d = 4, we propose an antisymmetric variant
of the technique proposed by Koﬁdis and Regalia [8] for symmetric tensors. For this
purpose, we deﬁne the (1, 2)-matricization of a 4th-order tensor X ∈ Rn1×n2×n3×n4 to
be the n1n2 × n3n4 matrix X(1,2) with the entries

X(1,2)(k, ℓ) = X (i1, i2, i3, i4),

k = j(i1, i2),

ℓ = j(i3, i4),

(4.3)

where the function j(·) is deﬁned as in (2.1).
Lemma 4.5 Let A ∈ Rn×n×n×n be antisymmetric. Then the following statements hold:

1. A(1,2) is symmetric.
2. Let λ be a nonzero eigenvalue of A(1,2) with eigenvector v ∈ Rn2

cization V(1) ∈ Rn×n of v is skew-symmetric.

. Then the matri-

3. If A = anti(αu1⊗u2⊗u3⊗u4) such that α 6= 0 and [u1, u2, u3, u4] is an orthonormal
basis then A(1,2) has an eigenvalue α/12 of multiplicity 3, an eigenvalue −α/12
of multiplicity 3, and n2 − 6 zero eigenvalues. Any eigenvector v belonging to a
nonzero eigenvalue satisﬁes range(V(1)) = span{u1, u2, u3, u4}.
Proof. 1. This statement follows directly from the deﬁnition (4.3):

A(1,2)(k, ℓ) = A(i1, i2, i3, i4) = A(i3, i4, i1, i2) = A(1,2)(ℓ, k).

14

2. The relation A(1,2)v = λv implies

V(1)(i1, i2) = v(j(i1, i2)) =

1
λ

n

Xi3,i4=1

A(1,2)(j(i1, i2), j(i3, i4))v(j(i3, i4))

=

1
λ

= −

n

Xi3,i4=1
Xi3,i4=1

1
λ

n

A(i1, i2, i3, i4)v(j(i3, i4))

A(i2, i1, i3, i4)v(j(i3, i4)) = −V(1)(i2, i1),

which shows that V(1) is skew-symmetric.
3. By the deﬁnition of A, range(V(1)) ⊂ span{u1, u2, u3, u4} and, together with its
j − ujuT
skew-symmetry, this implies that V(1) is a linear combination of matrices uiuT
for all i 6= j. Let π ∈ Sd and set σ = sign(π). Using Lemma 4.1, we have

i

α

= vec(cid:0)A ×1 uπ(1) ×2 uπ(2) − A ×1 uπ(2) ×2 uπ(1)(cid:1)

A(1,2)(cid:0)uπ(1) ⊗ uπ(2) − uπ(2) ⊗ uπ(1)(cid:1)
24(cid:0)σuπ(3) ⊗ uπ(4) − σuπ(4) ⊗ uπ(3) + σuπ(3) ⊗ uπ(4) − σuπ(4) ⊗ uπ(3)(cid:1)
12 (cid:0)uπ(3) ⊗ uπ(4) − uπ(4) ⊗ uπ(3)(cid:1).

σα

=

=

In turn, there is an eigenspace of dimension three with orthonormal basis

(cid:0)u1uT
(cid:0)u1uT
(cid:0)u3uT

2 − u2uT
4 − u4uT
1 − u1uT

1 + u3uT
1 + u2uT
3 + u2uT

4 − u4uT
3 − u3uT
4 − u3uT

3(cid:1)/2,
2(cid:1)/2,
2(cid:1)/2,

(4.4)

belonging to the eigenvalue α/12. Due to the orthogonality of u1, u2, u3, u4 the range
of any linear combination of (4.4) equals span{u1, u2, u3, u4}. Analogously, there is an
eigenspace of dimension three belonging to the eigenvalue −α/12 with the same property.

Lemma 4.5.3 suggests the initialization strategy described in Algorithm 5.

Algorithm 5 HOPM initialization strategy for antisymmetric tensor of order 4

Compute eigenvector v ∈ Rn2
Form V(1) ∈ Rn×n and compute its SVD.
Return the four leading left singular vectors u1, u2, u3, u4.

belonging to eigenvalue of largest magnitude of A(1,2).

4.3 Numerical Experiments

To investigate the diﬀerence between the diﬀerent initializations, we focus our experi-
ments on antisymmetric tensors of order four. Figure 4 shows the approximation errors
returned by HOPM initialized with truncated HOSVD or Algorithm 5, using the random

15

antisymmetric tensors described in Section 3.3. HOPM is considered converged when
the norm of the gradient of the objective function reaches 10−10 or below. It can be
seen that both initialization strategy appear to work equally well in terms of the ﬁnal
approximation error.

5

4.8

4.6

4.4

4.2

4

3.8

3.6
0

HOSVD initialization
new initialization

20

40

60

80

100

Figure 4: Approximation error of multilinear rank-4 approximation produced by HOPM
for 100 random antisymmetric 10 × 10 × 10 × 10 tensors.

Figure 5 shows the convergence behavior for a typical run. It turns out that initializ-
ing with Algorithm 5 gives a signiﬁcant convergence beneﬁt both for the approximation
error and the norm of the gradient.

3.9

3.8

3.7

3.6

3.5

3.4
0

HOSVD initialization
new initialization

5

10

15

20

100

10−2

10−4

10−6
0

HOSVD initialization
new initialization

5

10

15

20

(a) Approximation error

(b) Norm of gradient of objective function

Figure 5: Convergence behavior of HOPM for multilinear rank-4 approximation of ran-
dom antisymmetric 10 × 10 × 10 × 10 tensor.

Finally, analogous to Section 3.3, Figure 6 shows results for the 10 × 10 × 10 × 10

tensor generated by the function

f (x, y, z, w) = exp(−px2 + 2y2 + 3z2 + 4w2).

16

In this case, both initialization methods yield excellent approximations.

HOSVD initialization
new initialization

x 10−5

8

7

6

5

4

3

2

1

0
0

5

10

15

20

10−10

10−15

10−20

10−25
0

HOSVD initialization
new initialization

5

10

15

20

(a) Approximation error

(b) Norm of gradient of objective function

Figure 6: Convergence behavior of HOPM algorithm for multilinear rank-4 approxima-
tion of function-related tensor

5 Conclusions

The multilinear rank of an antisymmetric tensor has been analyzed and new algorithms
for antisymmetric low multilinear rank approximation have been proposed. The Jacobi
algorithm initialized with truncated HOSVD preserves antisymmetry and appears to
enjoy excellent global convergence properties. We have shown that a best unstructured
rank-1 approximation can always be turned into a best antisymmetric multilinear rank-d
approximation. In such a scenario, HOPM initialized either with truncated HOSVD (for
d 6= 4) or Algorithm 5 (for d = 4) is certainly the method of choice. The algorithms
discussed in this paper could provide a building block in the design of low-rank tensor
algorithms [6] for eigenvalue problems with antisymmetric eigenvectors. In particular,
the simplicity of HOPM makes it well suited in the context of truncated iterations and
greedy strategies.

References

[1] G. Beylkin, M. J. Mohlenkamp, Algorithms for numerical analysis in high

dimensions, SIAM J. Sci. Comput., 26 (6) (2005), pp. 2133–2159.

[2] G. Beylkin, M. J. Mohlenkamp, F. P´erez, Approximating a wavefunction as
an unconstrained sum of Slater determinants, J. Math. Phys., 49 (3) (2008), pp.
032107, 28.

[3] L. De Lathauwer, B. De Moor, J. Vandewalle, A Multilinear Singular Value

Decomposition, SIAM J. Matrix Anal. Appl., 21 (4) (2000), pp. 1253–1278.

17

[4] L. De Lathauwer, B. De Moor, J. Vandewalle, On the best rank-1 and rank-
(R1, . . . , RN ) approximation of higher order tensors, SIAM J. Matrix Anal. Appl.,
21 (4) (2000), pp. 1324–1342.

[5] S. Friedland, Best rank one approximation of real symmetric tensors can be cho-

sen symmetric, Front. Math. China, 8 (1) (2013), pp. 19–40.

[6] L. Grasedyck, D. Kressner, C. Tobler, A literature survey of low-rank tensor

approximation techniques, GAMM-Mitt., 36 (2013), pp. 53–78.

[7] M. Ishteva, P.-A. Absil, P. Van Dooren, Jacobi algorithm for the best low
multilinear rank approximation of symmetric tensors, SIAM J. Matrix Anal. Appl.,
34 (2) (2013), pp. 651–672.

[8] E. Kofidis, P. A. Regalia, On the best rank-1 approximation of higher-order
supersymmetric tensors, SIAM J. Matrix Anal. Appl., 23 (3) (2002), pp. 863–884.

[9] T. G. Kolda, B. W. Bader, Tensor Decompositions and Applications, SIAM

Rev., 51 (3) (2009), pp. 455–500.

[10] B. N. Sheehan, Y. Saad, Higher Order Orthogonal Iteration of Tensors (HOOI)
and its Relation to PCA and GLRAM, Proceedings of the 2007 SIAM International
Conference on Data Mining, (2007), pp. 355–365.

[11] S. Szalay, M. Pfeffer, V. Murg, G. Barcza, F. Verstraete, R. Schnei-
der, ¨O. Legeza, Tensor product methods and entanglement optimization for ab
initio quantum chemistry, International Journal of Quantum Chemistry, 115 (19)
(2015), pp. 1342–1391.

18

