6
1
0
2

 
r
a

 

M
1
2

 
 
]

.

R
P
h
t
a
m

[
 
 

1
v
6
5
5
6
0

.

3
0
6
1
:
v
i
X
r
a

WEIGHTED SAMPLING WITHOUT REPLACEMENT

ANNA BEN-HAMOU, YUVAL PERES AND JUSTIN SALEZ

Abstract. Comparing concentration properties of uniform sampling with and
without replacement has a long history which can be traced back to the pioneer
work of Hoeﬀding [7]. The goal of this short note is to extend this comparison to
the case of non-uniform weights, using a coupling between samples drawn with
and without replacement. When the items’ weights are arranged in the same
order as their values, we show that the induced coupling for the cumulative
values is a submartingale coupling. As a consequence, the powerful Chernoﬀ-
type upper-tail estimates known for sampling with replacement automatically
transfer to the case of sampling without replacement. For general weights, we
use the same coupling to establish a sub-Gaussian concentration inequality. As
the sample size approaches the total number of items, the variance factor in this
inequality displays the same kind of sharpening as Serﬂing [15] identiﬁed in the
case of uniform weights. We also construct an other martingale coupling which
allows us to answer a question raised by Luh and Pippenger [10] on sampling in
Polya urns with diﬀerent replacement numbers.

1. Introduction

In a celebrated paper [7], Hoeﬀding ﬁrst singled out a fruitful comparison be-
tween sampling with and without replacement: any linear statistics induced by
uniform sampling without replacement in a ﬁnite population is less, in the convex
order, than the one induced by sampling with replacement. In particular, all the
Chernoﬀ-type tail estimates that apply to sampling with replacement (the sample
then being i.i.d.) automatically apply to sampling without replacement. As the
sample size increases, it is natural to expect that sampling without replacement
should concentrate even more, in the sense that, when the sample size approaches
the total number of items, the variance should not be of the order of the number
of sampled items, but of the number of unsampled items. This was veriﬁed by
Serﬂing in [15].

One natural question is to determine whether a similar comparison also holds
when the sampling procedure is no longer uniform and when diﬀerent items have
diﬀerent weights.

More precisely, consider a collection of N items 1 ≤ i ≤ N, each equipped with

a weight ω(i) > 0 and a value of interest ν(i) ∈ R. We assume that

N

Xi=1

ω(i) = 1 .

1

SAMPLING WITHOUT REPLACEMENT

2

Let X be the cumulative value of a sample of length n ≤ N drawn without
replacement and with probability proportional to weights, i.e.

where for each n−tuple (i1, . . . , in) of distinct indices in {1, . . . , N},

X := ν(I1) + · · · + ν(In),

P ((I1, . . . , In) = (i1, . . . , in)) =

n

Yk=1

ω(ik)

1 − ω(i1) − · · · − ω(ik−1)

.

A much simpler statistic is the one that arises when the sample is drawn with

replacement, namely

Y := ν(J1) + · · · + ν(Jn),
where now for each n−tuple (j1, . . . , jn) ∈ {1, . . . , N}n,
n

P ((J1, . . . , Jn) = (j1, . . . , jn)) =

ω(jk) .

Yk=1

One particular case is when weights and values are arranged in the same order,

i.e.

(1)

ω(i) > ω(j) =⇒ ν(i) ≥ ν(j) .

Theorem 1. Assume that condition (1) holds. Then X is less than Y in the
increasing convex order, i.e. for every non-decreasing, convex function f : R → R,

(2)

E [f (X)] ≤ E [f (Y )] .

Our second result is a sub-Gaussian concentration inequality for X in the case

of arbitrary weights (ω(i))N

i=1. Deﬁne

∆ := max
1≤i≤N

ν(i) − min
1≤i≤N

ν(i)

and

α =

min1≤i≤N ω(i)
max1≤i≤N ω(i)

.

The case α = 1 (uniform sampling) was analysed by Serﬂing [15].

Theorem 2. Assume α < 1. For all t > 0,

max {P (X − EX > t) , P (X − EX < −t)} ≤ exp −

t2

2v! ,

with

(3)

v = min 4∆2n ,

1 + 4α
α(1 − α)

α!
∆2N (cid:18)N − n
N (cid:19)

We also answer a question raised by [10]. The problem is to compare linear
statistics induced by sampling in Polya urns with replacement number d versus D,
for positive integers d, D with D > d ≥ 1.

Let C be a population of N items, labelled from 1 to N, each item i being
equipped with some value ν(i). Let d < D be two positive integers. For n ≥ 1,
let (K1, . . . , Kn) and (Ł1, . . . , Łn) be samples generated by sampling in Polya urns

SAMPLING WITHOUT REPLACEMENT

3

with initial composition C and replacement numbers d and D respectively, i.e.
each time an item is picked, it is replaced along with d − 1 (resp. D − 1) copies.
We say that (K1, . . . , Kn) (resp. (Ł1, . . . , Łn)) is a d-Polya (resp. D-Polya) sample.
Let

W = ν(K1) + · · · + ν(Kn) ,
Z = ν(Ł1) + · · · + ν(Łn) .

Theorem 3. The variable W is less than Z in the convex order, i.e.
convex function f : R → R,

for every

E [f (W )] ≤ E [f (Z)] .

Remark 1. [10] proved a similar result in the case where the ﬁrst sample is drawn
without replacement in C and the second is a D-Polya sample, for D ≥ 1.

2. Related work

Weighted sampling without replacement, also known as successive sampling,
appears in a variety of contexts (see [6, 8, 14, 19]). When n << N, it is natural
to expect Y to be a good approximation of X. For instance, the total-variation

distance between P(cid:16)In+1 ∈ · (cid:12)(cid:12)(cid:12)

is O(n/N) provided all the weights are O(1/N).

(Ik)n

k=1(cid:17) and P (J1 ∈ · ) is given by

n

ω(Ik), which

Xk=1

Under the monotonicity assumption (1), Theorem 1 establishes an exact strong
stochastic ordering between X and Y . Since J1, . . . , Jn are independent copies
of I1, the innumerable results on sums of independent and identically distributed
random variables apply to Y . In particular, Chernoﬀ’s bound

(4)

P (Y ≥ a) ≤ exp (nΛ(θ) − θa) ,

yields a variety of sharp concentration results based on eﬃcient controls on the
log-Laplace transform Λ(θ) = ln E[eθν(I1)]. This includes the celebrated Hoeﬀding
and Bernstein inequalities, see the book [5]. Theorem 1 implies in particular that
all upper-tail estimates derived from Chernoﬀ’s bound (4) apply to X without
modiﬁcation.

The condition (1) describes a sampling procedure which is sometimes referred
to as size-biased sampling without replacement. It arises in many situations, in-
cluding ecology, oil discovery models, in the construction of the Poisson-Dirichlet
distribution ([12, 13]), or in the conﬁguration model of random graphs ([3, 4]).

Stochastic orders provide powerful tools to compare distributions of random vari-
ables and processes, and they have been used in various applications [11, 16, 18].
As other stochastic relations, the increasing convex order is only concerned with
marginal distributions. One way of establishing (2) is thus to carefully construct
two random variables X and Y with the correct marginals on a common probability

SAMPLING WITHOUT REPLACEMENT

4

space, in such a way that

(5)

X ≤ E[Y |X]

holds almost-surely. The existence of such a submartingale coupling clearly implies
(2), thanks to Jensen’s inequality. Quite remarkably, the converse is also true, as
proved by Strassen [17]. Similarly, Theorem 3 is equivalent to the existence of a
martingale coupling (W, Z).

Remark 2 (The uniform case). When ω is constant, i.e. α = 1, the sequence
(I1, . . . , In) is exchangeable. In particular, E[X] = E[Y ], forcing equality in (5).
Thus, (2) automatically extends to arbitrary convex functions. This important
special case was established ﬁve decades ago by Hoeﬀding in his seminal paper
[7]. Since then, improvements have been found as n/N approaches 1 [2, 15]. An-
other remarkable feature of uniform sampling without replacement is the negative
association of the sequence (ν(I1), . . . , ν(In)) [9]. However, this result seems to
make crucial use of the exchangeability of (I1, . . . , In), and it is not clear whether
it can be extended to more general weights, e.g.
to monotone weights satisfying
(1). Non-uniform sampling without replacement can be more delicate and induce
counter-intuitive correlations, as highlighted by Alexander [1], who showed that for
two ﬁxed items, the indicators that each is in the sample can be positively corre-
lated.

Theorem 2 holds under the only assumption that α < 1, but the domain of
application that we have in mind is when α is bounded away from 0 and 1. In
this domain, when n ≤ qN, for some ﬁxed 0 < q < 1, equation (3) gives v =
O(∆2n), which corresponds to the order of the variance factor in the classical
Hoeﬀding inequality. When n/N →
1, then it can be improved up to v =

N→∞

O(cid:16)∆2n(cid:16) N −n

N (cid:17)α(cid:17). In the uniform case α = 1, Serﬂing [15] showed that X satisﬁes

a sub-Gaussian inequality with v = ∆2n N −n+1
4N , implying that the variance factor
has the order of the minimum between the number of sampled and unsampled
items.

Organization. Both Theorems 1 and 2 rely on a coupling between samples drawn
with and without replacement, which is constructed in Section 3. Then, Theorems
1, 2, 3 are proved respectively in Sections 4, 5 and 6.

3. The coupling

The proofs of Theorem 1 and 2 rely on a particular coupling of samples drawn
with and without replacement. This coupling is inspired by the one described in
[10] for the uniform case.

First generate an inﬁnite sequence (Jk)k≥1 by sampling with replacement and
i=1. Now, “screen” this sequence, starting

with probability proportional to (ω(i))N

SAMPLING WITHOUT REPLACEMENT

5

at J1 as follows: for 1 ≤ k ≤ N, set

Ik = JTk ,

where Tk is the random time when the kth distinct item appears in (Ji)i≥1.

The sequence (I1, . . . , In) is then distributed as a sample without replacement.

n

n

As above, we deﬁne X =

ν(Ik) and Y =

Xk=1

ν(Jk).

Xk=1

4. Proof of Theorem 1

Consider the coupling of X and Y described above (Section 3). Under the
monotonicity assumption (1), we show that (X, Y ) is a submartingale coupling in
the sense of (5). As the sequence (J1, . . . , Jn) is exchangeable and as permuting Ji
and Jj in this sequence does not aﬀect X, it is suﬃcient to show that E [ν(J1)|X] ≥
X/n.

Let {i1, . . ., in} ⊂ {1, . . . , N} be a set of cardinality n, and let A be the event

{I1, . . . , In} = {i1, . . ., in}.

Ai =

n

Xj=1

Ehν(J1)(cid:12)(cid:12)(cid:12)
Let us now show that, for all 1 ≤ k 6= ℓ ≤ n, if ν(ik) ≥ ν(iℓ), then P(cid:16)J1 = ik(cid:12)(cid:12)(cid:12)
A(cid:17)
is not smaller than P(cid:16)J1 = iℓ(cid:12)(cid:12)(cid:12)
A(cid:17). First, by (1), one has ω(ik) ≥ ω(iℓ). Letting

Sn be the set of permutations of n elements, one has

P(cid:16)J1 = ij(cid:12)(cid:12)(cid:12)

A(cid:17) ν(ij) .

P ({J1 = ik} ∩ A) =

Xπ∈Sn,π(1)=k

p(π) ,

where

p(π)

:= ω(iπ(1))

ω(iπ(2))

1 − ω(iπ(1))

· · ·

ω(iπ(n))

1 − ω(iπ(1)) − ω(iπ(2)) − ω(iπ(n−1))

Now, each permutation π with π(1) = k can be uniquely associated with a per-
mutation π⋆ such that π⋆(1) = ℓ, by performing the switch: π⋆(π−1(ℓ)) = k, and
letting π(j) = π⋆(j), for all j 6∈ {1, π−1(ℓ)}. Observe that p(π) ≥ p(π⋆). Thus

P(cid:16)J1 = ik(cid:12)(cid:12)(cid:12)

A(cid:17) − P(cid:16)J1 = iℓ(cid:12)(cid:12)(cid:12)

A(cid:17) =

1

P (A) Xπ∈Sn,π(1)=k

(p(π) − p(π⋆)) ≥ 0 .

SAMPLING WITHOUT REPLACEMENT

6

Consequently, by Chebyshev’s sum inequality,

Ehν(J1)(cid:12)(cid:12)(cid:12)

1
n

Ai = n
≥ n

= Pn

n

n

P(cid:16)J1 = ij(cid:12)(cid:12)(cid:12)
Xj=1
P(cid:16)J1 = ij(cid:12)(cid:12)(cid:12)
Xj=1

A(cid:17) ν(ij)
A(cid:17)




1
n

j=1 ν(ij)

,

n

and EhY(cid:12)(cid:12)(cid:12)

Xi ≥ X.

1
n

n

Xj=1

ν(ij)


(cid:3)

5. Proof of Theorem 2

We only need to show that the bound in Theorem 2 holds for P [X − EX > t].
Indeed, replacing X by −X (i.e. changing all the values to their opposite) does
not aﬀect the proof. Hence, the bound on P [X − EX < −t] will follow directly.

Theorem 2 is proved using the same coupling between sampling with and without

replacement as described in Section 3.

Note that, in this coupling, X is a function of the i.i.d. variables (Ji)i≥1:

(6)

X =

+∞

Xi=1

ν(Ji)1{Ji6∈{J1,...,Ji−1}}1{Tn≥i} .

As such, one may obtain concentration results for X by resorting to the various
methods designed for functions of independent variables.

The proof relies on the entropy method as described in Chapter 6 of [5]. We will

show that X is such that, for all λ > 0,

(7)

λEhX eλXi − Eh eλXi log Eh eλXi ≤

λ2v
2

Eh eλXi ,

for v as in (3). Then, a classical argument due to Herbst (see [5], Proposition 6.1)
ensures that, for all λ > 0,

and thus, for all t > 0,

log Eh eλ(X−EX)i ≤

λ2v
2

,

P (X − EX > t) ≤ exp −

t2

2v! ,

that is, the upper-tail of X is sub-Gaussian with variance factor v. Let us establish
inequality (7). For t ≥ 1, consider the truncated variable Xt deﬁned by summing

SAMPLING WITHOUT REPLACEMENT

7

only from 1 to t in (6), i.e.

t

Xt =

ν(Ji)1{Ji6∈{J1,...,Ji−1}}1{Tn≥i}

Xi=1

:= f (J1, . . . , Jt) .

Note that Xt converges to X almost surely as t → +∞. Then, for all 1 ≤ i ≤
t, consider the perturbed variable X i
t which is obtained by replacing Ji by an
independent copy J′

i, i.e.
X i

t = f (J1, . . . , Ji−1, J′

i, Ji+1, . . . , Jt) ,

and let X i be the almost sure limit of X i
that, for all λ > 0,

t , as t → +∞. Theorem 6.15 of [5] implies

(8)

λEhXt eλXti − Eh eλXti log Eh eλXti ≤

Ehλ2 eλXt(Xt − X i
+i .
t )2

We now show that this inequality still holds when we let t tend to +∞. Let
νmax = max
ν(j). For all t ≥ 1, the variable Xt is almost surely bounded by
1≤j≤N

nνmax. Hence, the left-hand side of (8) tends to the left-hand side of (7). As for
the right-hand side, we have that, for all 1 ≤ i ≤ t,

t

Xi=1

Ehλ2 eλXt(Xt − X i
+i ≤ λ2 eλnνmax∆2P(i ≤ Tn) ,
t )2

P[i ≤ Tn] = E[Tn] < +∞. Hence, by dominated convergence, the right-

and P+∞

i=1

hand side also converges, and we obtain

λEhX eλXi − Eh eλXi log Eh eλXi ≤

Ehλ2 eλX(X − X i)2
+i .

+∞

Xi=1

Recall that (I1, . . . , In) is the sequence of the ﬁrst n distinct items in (Ji)i≥1 and
that X is measurable with respect to σ(I1, . . . , In), so that

+∞

Xi=1

Ehλ2 eλX(X − X i)2

Thus, letting

(X − X i)2

+i = E"λ2 eλX E"+∞
Xi=1
I1, . . . , In# ,
+(cid:12)(cid:12)(cid:12)

(X − X i)2

V := E"+∞
Xi=1

I1, . . . , In## .
+(cid:12)(cid:12)(cid:12)

our task comes down to showing that

V ≤

v
2

a.s. .

Observe that for all i ≥ 1, we have (X − X i)2
i ≤ Tn and one of the following two events occurs:

+ ≤ ∆2 and that X = X i unless

i 6∈ {I1, . . . , In};

• J′
• the item Ji occurs only once before Tn+1.

SAMPLING WITHOUT REPLACEMENT

8

Let us deﬁne

and

A =

+∞

Xi=1

B =

i6∈{I1,...,In}}1i≤Tn(cid:12)(cid:12)(cid:12)
Eh1{J′
Eh1{∃! i<Tn+1, Ji=Ik}(cid:12)(cid:12)(cid:12)

n

Xk=1

I1, . . . , Ini ,

I1, . . . , Ini ,

so that V ≤ ∆2 (A + B). Since J′
σn := ω(I1) + . . . ω(In) is a measurable function of (I1, . . . , In), we have

i is independent of everything else and since

We use the following fact.

A = (1 − σn)EhTn(cid:12)(cid:12)(cid:12)

I1, . . . , Ini .

Lemma 3. For 1 ≤ k ≤ n, let τk = Tk − Tk−1. Conditionally on (I1, . . . , In),
the variables (τk)n
k=1 are independent and for all 1 ≤ k ≤ n, τk is distributed as a
Geometric random variables with parameters 1 − σk−1.

Proof. Let (i1, . . . , in) be an n-tuple of distinct elements of {1, . . . , N} and let
t1, . . . , tn ≥ 1. Let also (Gk)n
k=1 be independent Geometric random variables with
parameter (1 − ω(i1) − · · · − ω(ik−1)). We have

P ((τ1, . . . , τn) = (t1, . . . , tn), (I1, . . . , In) = (i1, . . . , in))

= 1{t1=1}ω(i1)

(ω(i1) + · · · + ω(ik−1))tk−1 ω(ik)

n

Yk=2

ω(ik)

=

n

Yk=1

1 − ω(i1) − · · · − ω(ik−1)

P (Gk = tk)

P (Gk = tk) ,

n

n

Yk=1
Yk=1

(cid:3)

= P ((I1, . . . , In) = (i1, . . . , in))

and we obtain the desired result.

Lemma 3 implies that

I1, . . . , Ini =

n

Xk=1

1

1 − σk−1

.

EhTn(cid:12)(cid:12)(cid:12)
Xk=1

n

1
α

In particular, A ≤ n. We also have

(9)

A ≤

N − n

N − k + 1

≤

1
α

(N − n) log(cid:18) N

N − n(cid:19) .

It remains to control B. Clearly B ≤ n, which shows that V ≤ 2∆2n. Moreover,
for 1 ≤ k ≤ n, we have

P(cid:16)∃! i < Tn+1, Ji = Ik(cid:12)(cid:12)(cid:12)

n

I1, . . . , In(cid:17) = E
Yj=k 1 −


ω(Ik)

σj !τj+1−1

I1, . . . , In
(cid:12)(cid:12)(cid:12)
 .

SAMPLING WITHOUT REPLACEMENT

9

Using Lemma 3 and the fact that the generating function of a geometric variable

G with parameter p is given by EhxGi =
1−(1−p)x , we obtain
Xk=1
Yj=k

1 + ω(Ik )
1−σj

B =

1

px

.

n

n

Thanks to the inequality the inequality log(1 + x) ≥ x − x2/2 for x ≥ 0,

B ≤

n

n

Xk=1

Yj=k

1

1 + α
N −j

≤

n

Xk=1

exp
−α

1

N − j

n

Xj=k

+

1
2

n

Xj=k

1

(N − j)2
 .

The second term in the exponent is always smaller than 1/2. Using Riemann sums,
we get

B ≤ 2

n

Xk=1

2

N − n !! = 2

exp −α log  N − k + 1
N (cid:18)N − n
N (cid:19)

α

,

≤

1 − α

α

n

Xk=1(cid:18) N − n

N − k + 1(cid:19)

Combined with (9), this yields

V ≤   1
≤  

α (cid:18)N − n
N (cid:19)

e−1

+

α(1 − α)

1−α

2

log(cid:18) N

1 − α! ∆2N (cid:18)N − n
N − n(cid:19) +
N (cid:19)
1 − α! ∆2N (cid:18)N − n
N (cid:19)

2

α

α

≤

1/2 + 2α
α(1 − α)

∆2N (cid:18)N − n
N (cid:19)

α

,

where the second inequality is due to the fact that log(x)/x1−α ≤ e−1/(1 − α) for
all x > 0.

(cid:3)

6. Proof of Theorem 3

The proof of Theorem 3 relies on the construction of a martingale coupling

Wi = W .

(W, Z), i.e. of a coupling of W and Z such that EhZ(cid:12)(cid:12)(cid:12)

Consider two urns, Ud and UD, each of them initially containing N balls, la-
belled from 1 to N. In each urn, arrange the balls from left to right by increasing
order of their label. Then arrange UD and Ud on top of one another. Each time
we will pick a ball in UD, we will pick the ball just below it in Ud. More precisely,
we perform an inﬁnite sequence of steps as follows: at step 1, we pick a ball B1
uniformly at random in UD and pick the ball just below it in Ud. They necessarily
have the same label, say j. We let K1 = Ł1 = j, and add, on the right part of UD,
D − 1 balls with label j, and, on the right part of Ud, d − 1 balls with label j and
D − d unlabelled balls. Note that, at the end of this step, the two urns still have

SAMPLING WITHOUT REPLACEMENT

10

the same number of balls, N + D − 1. The ﬁrst step is depicted in Figure 1. Then,
at each step t, we pick a ball Bt at random among the N + (t − 1)(D − 1) balls of
UD and choose the ball just below it in Ud. There are two diﬀerent possibilities:
• if the ball drawn in Ud is unlabelled and the one drawn in UD has label
j, we let Łt = j and add D − 1 balls with label j on the right part of UD,
and D − 1 unlabelled balls on the right part of Ud, .

• if both balls have label j, and if t corresponds to the ith time a labelled
ball is drawn in Ud, we let Łt = Ki = j and add D − 1 balls with label j
on the right part of UD, and d − 1 balls with label j and D − d unlabelled
balls on the right part of Ud;

Figure 1. The ball B1 has label 2 (N = 5, d = 3, D = 4).

B1

1 2 3 4 5 2 2 2

N

D − 1

unlabelled

1 2 3 4 5 2 2

d − 1

UD

Ud

The sequence (K1, . . . , Kn) records the labels of the ﬁrst n labelled balls picked
in Ud, and (Ł1, . . . , Łn) the labels of the ﬁrst n balls picked in UD. Observe
that (K1, . . . , Kn) (resp. (Ł1, . . . , Łn)) is distributed as a d-Polya (resp. D-Polya)
sample. Deﬁne

W = ν(K1) + · · · + ν(Kn) ,
Z = ν(Ł1) + · · · + ν(Łn) .

Let us show that 1 ≤ i ≤ n − 1, Ehν(Łi+1)(cid:12)(cid:12)(cid:12)

be a multiset of cardinality n of elements of {1, . . . , N}, and let A be the event
{K1, . . . , Kn} = {k1, . . ., kn} (accounting for the multiplicity of each label). Denote
by Ci the set of D − 1 balls added at step i. Observe that, if Bi+1 ∈ Ci, then
Łi+1 = Łi. Hence

Wi = Ehν(Łi)(cid:12)(cid:12)(cid:12)

Wi. Let {k1, . . ., kn}

We have

Ehν(Łi+1)(cid:12)(cid:12)(cid:12)
Ehν(Łi+1)1{Bi+16∈Ci}(cid:12)(cid:12)(cid:12)

N

N

P (Łi = ℓ, Łi+1 = k, Bi+1 6∈ Ci, A) .

Ai + Ehν(Łi+1)1{Bi+16∈Ci}(cid:12)(cid:12)(cid:12)
Ai = Ehν(Łi)1{Bi+1∈Ci}(cid:12)(cid:12)(cid:12)
Ai =
Xℓ=1

Xk=1

P(A)

ν(k)

1

Ai .

SAMPLING WITHOUT REPLACEMENT

11

Notice that, on the event Bi+1 6∈ Ci, the balls Bi and Bi+1 are exchangeable.
Hence P (Łi = ℓ, Łi+1 = k, Bi+1 6∈ Ci) = P (Łi = k, Łi+1 = ℓ, Bi+1 6∈ Ci). Moreover,
permuting Bi and Bi+1 can not aﬀect the multiset {K1, . . . , Kn}. Hence

and Ehν(Łi+1)(cid:12)(cid:12)(cid:12)

Ehν(Łi+1)1{Bi+16∈Ci}(cid:12)(cid:12)(cid:12)
Wi = Ehν(Łi)(cid:12)(cid:12)(cid:12)
Wi = Ehν(Ł1)(cid:12)(cid:12)(cid:12)
Ehν(Łi)(cid:12)(cid:12)(cid:12)

Ai = Ehν(Łi)1{Bi+16∈Ci}(cid:12)(cid:12)(cid:12)
Ai ,
Wi. We get that, for all 1 ≤ i ≤ n,
Wi = Ehν(K1)(cid:12)(cid:12)(cid:12)

Wi = W/n ,

where the last equality comes from the exchangeability of (K1, . . . , Kn).

(cid:3)

References

[1] K. S. Alexander et al. A counterexample to a correlation inequality in ﬁnite

sampling. The Annals of Statistics, 17(1):436–439, 1989.

[2] R. Bardenet and O.-A. Maillard. Concentration inequalities for sampling

without replacement. ArXiv e-prints, Sept. 2013.

[3] B. Bollobás. A probabilistic proof of an asymptotic formula for the number
of labelled regular graphs. European Journal of Combinatorics, 1(4):311–316,
1980.

[4] B. Bollobás. Random graphs. Springer, 1998.
[5] S. Boucheron, G. Lugosi, and P. Massart. Concentration inequalities. Oxford

University Press, Oxford, 2013.

[6] L. Gordon. Successive sampling in large ﬁnite populations. Ann. Statist.,

11(2):702–706, 1983.

[7] W. Hoeﬀding. Probability inequalities for sums of bounded random variables.

J. Amer. Statist. Assoc., 58:13–30, 1963.

[8] L. Holst. Some limit theorems with applications in sampling theory. Ann.

Statist., 1:644–658, 1973.

[9] K. Joag-Dev and F. Proschan. Negative association of random variables with

applications. The Annals of Statistics, pages 286–295, 1983.

[10] K. Luh and N. Pippenger. Large-deviation bounds for sampling without re-

placement. American Mathematical Monthly, 121(5):449–454, 2014.

[11] A. Müller and D. Stoyan. Comparison methods for stochastic models and
risks. Wiley Series in Probability and Statistics. John Wiley & Sons, Ltd.,
Chichester, 2002.

[12] J. Pitman and N. M. Tran. Size biased permutation of a ﬁnite sequence with

independent and identically distributed terms. ArXiv e-prints, Oct. 2012.

[13] J. Pitman and M. Yor. The two-parameter poisson-dirichlet distribution de-
rived from a stable subordinator. The Annals of Probability, pages 855–900,
1997.

SAMPLING WITHOUT REPLACEMENT

12

[14] B. Rosén. Asymptotic theory for successive sampling with varying proba-
bilities without replacement. I, II. Ann. Math. Statist., 43:373–397; ibid. 43
(1972), 748–776, 1972.

[15] R. J. Serﬂing. Probability inequalities for the sum in sampling without re-

placement. Ann. Statist., 2:39–48, 1974.

[16] M. Shaked and J. G. Shanthikumar. Stochastic orders. Springer Series in

Statistics. Springer, New York, 2007.

[17] V. Strassen. The existence of probability measures with given marginals. Ann.

Math. Statist., 36:423–439, 1965.

[18] R. Szekli. Stochastic ordering and dependence in applied probability, volume 97

of Lecture Notes in Statistics. Springer-Verlag, New York, 1995.

[19] Y. Yu. On the inclusion probabilities in some unequal probability sampling

plans without replacement. Bernoulli, 18(1):279–289, 2012.

