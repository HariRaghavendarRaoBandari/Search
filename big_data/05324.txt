Fast moment estimation for generalized latent

Dirichlet models

Shiwen Zhao∗1, Barbara E. Engelhardt†2, Sayan Mukherjee‡1, and David B. Dunson§1

1Department of Statistical Science, Duke University

2Department of Computer Science, Princeton University

March 24, 2016

Abstract

We develop a generalized method of moments (GMM) approach for fast parameter estimation in a
new class of Dirichlet latent variable models with mixed data types. Parameter estimation via GMM
has been demonstrated to have computational and statistical advantages over alternative methods,
such as expectation maximization, variational inference, and Markov chain Monte Carlo. The key
computational advantage of our method (MELD) is that parameter estimation does not require
instantiation of the latent variables. Moreover, a representational advantage of the GMM approach is
that the behavior of the model is agnostic to distributional assumptions of the observations. We derive
population moment conditions after marginalizing out the sample-speciﬁc Dirichlet latent variables.
The moment conditions only depend on component mean parameters. We illustrate the utility of our
approach on simulated data, comparing results from MELD to alternative methods, and we show the
promise of our approach through the application of MELD to several data sets.

Keywords: generalized method of moments; latent Dirichlet allocation; latent variables; mixed member-
ship model; mixed scale data; tensor factorization

6
1
0
2

 
r
a

 

M
3
2

 
 
]
T
S
h
t
a
m

.

[
 
 

2
v
4
2
3
5
0

.

3
0
6
1
:
v
i
X
r
a

∗sz63@duke.edu
†bee@princeton.edu
‡sayan@stat.duke.edu
§dunson@duke.edu

1

1 Introduction

Many modern statistical applications require the analysis of large-scale, heterogeneous data types in-

cluding continuous, categorical, and count variables. For example, in social science, survey data often

consist of collections of diﬀerent data types (e.g., height, gender, and age); in population genetics, re-

searchers are interested in analyzing genotype (integer-valued) and heterogeneous traits (e.g., blood pres-
sure, BMI, alcoholic drinks per day). Often data take the form of an n×p matrix Y = (y1, . . . , yn)T , with
yi = (yi1, . . . , yip)T a p dimensional vector of measurements of varying types for subject i, for i = 1, . . . , n

subjects.

This paper focuses on 1) developing a new class of generalized latent variable models for mixed data

types, and 2) performing fast and robust parameter estimation using the generalized method of moments

(GMM). Many models have been proposed to address the ﬁrst goal (reviewed below), but often these

models lack robustness both statistically and computationally. In addition, parameter estimation in these

models is often both ineﬃcient and unstable, which necessitates our second goal. Hence, there is a clear

need for new classes of models and corresponding robust and eﬃcient approaches for routine inference

under these models in general applications.

For modeling mixed scale data, there are two general strategies that have been most commonly

employed in the literature. The ﬁrst is to assume an underlying Gaussian model, and then to characterize

the dependence through a structured model of the Gaussian covariance (Muth´en, 1984). Such models

are routinely used in the social science literature, focusing almost entirely on the case in which data are

categorical or continuous, with the categorical variables arising via thresholding of Gaussian variables.

These models are related to Gaussian copula models, which have been developed for broad classes of mixed

scale data also including counts (Murray et al., 2013). The second approach is to deﬁne an exponential

family distribution for each of the p variables, inducing dependence between variables through generalized

linear models containing shared latent variables (Sammel et al., 1997; Moustaki and Knott, 2000; Dunson,

2003).

There are a number of disadvantages to the above approaches. The underlying Gaussian framework is

restrictive in forcing continuous variables to be Gaussian and non-continuous variables to be categorical.

The exponential family latent variable models are potentially more ﬂexible, but in practice suﬀer in

several respects. Most notably, there is a fundamental lack of robustness due to the model structure,

which arises because of the dual role of the latent variables in controlling sample dependence and the

shape of the marginal distributions. For example, if there are two count variables, yi1 and yi2, that are

highly correlated, then both counts must load strongly on a similar set of latent variables, which will

induce over-dispersion in the marginal distributions with the variance much larger than the mean. There

is no possibility of having highly correlated counts that do not have high over-dispersion. Copula models

solve this problem by imposing a restrictive Gaussian copula covariance structure. Another fundamental

issue is computation, which tends to rely on expectation-maximization (EM) or Markov chain Monte

Carlo (MCMC) algorithms that alternate between updating latent variables and population parameters,

intrinsically leading to slow convergence, ineﬃciency, and instability.

One promising approach to address the above issues is to rely on a generalized method of moments

(GMM) estimator, which relates to the second goal in this paper. The GMM estimator is robust to

misspeciﬁcation of the higher-order moments, while also leading to optimal eﬃciency among all estimators

2

using only information on the initial moments (Hansen, 1982). Applications of GMM to structural models

with latent variables have a long history, with early examples including Bentler (1983) and Anderson

and Gerbing (1988), among others. More recently, Gallant et al. (2013) applied GMM to a speciﬁc

class of latent variable model by deﬁning moment conditions based on the complete data, including the

latent variables. In contrast, Bollen et al. (2014) relied on a model-implied instrumental variable GMM

estimator. Generally, current GMM methods focus on latent variable models that satisfy restrictive

assumptions or require the instantiation of latent variables in a computationally intensive estimation

algorithm.

The focus of our work is on a broad new class of latent variable models for mixed scale data. This

eliminates some of the problems of current exponential family latent variable models, while also enabling

derivation of an eﬃcient GMM implementation that marginalizes out latent variables. With the ﬁrst goal

in mind, we focus on generalized mixed membership models, which incorporate Dirichlet latent variables.

Mixed membership models have a rich literature with applications ranging from inference of ancestral

populations in genomic data (Pritchard et al., 2000a,b) to topic modeling of documents (Blei et al.,

2003). Using Dirichlet latent variables to deﬁne cluster membership allows samples to partially belong to

each of k latent components. Our model class includes latent Dirichlet allocation (Blei et al., 2003) and

simplex factor models (Bhattacharya and Dunson, 2012) as special cases; however, we go beyond current

approaches by allowing mixed scale data.

For the second goal, we develop an eﬃcient moment tensor approach for parameter estimation in

mixed membership models with the Dirichlet latent variables marginalized out. This objective is related

to recent moment tensor methods developed for latent variable models including mixtures of Gaussians,

hidden Markov models, mixed membership models, and stochastic block models (Arora et al., 2012;

Anandkumar et al., 2012a,b; Hsu and Kakade, 2013; Anandkumar et al., 2014a,b). This recent work

ﬁnds moment tensors of these latent variable models that have a symmetric PARAFAC (parallel factors)

tensor decomposition (Kiers, 2000). Parameter estimation using a moment tensor approach is performed

as follows: 1) third order empirical moment tensors are transformed to an orthogonal decomposable form;

2) orthogonal decompositions are performed using tensor power methods or singular value decompositions;

3) parameter estimates are recovered from the decomposition of the moment tensors. The moment tensor

approach oﬀers substantial computational advantages over other approximation methods, as illustrated

in various applications (Tung and Smola, 2014; Anandkumar et al., 2014a; Colombo and Vlassis, 2015).

In this work, we develop a moment tensor approach for generalized mixed membership models. Here,

parameter estimates are performed in the GMM framework by minimizing quadratic forms of the mo-

ment functions, and asymptotic eﬃciencies are analyzed. Our approach, Moment Estimation for Latent

Dirichlet models (MELD), diﬀers from previous approaches in that the moment functions are deﬁned for

second and third order moments for variables with diﬀerent generative distributions. This goes beyond

previous moment tensor approaches that construct moment tensors for homogeneous data distributions.

In Section 2, we characterize the class of generalized Dirichlet latent variable models in MELD. In

Section 3, we brieﬂy review GMM and introduce the estimation procedure used in MELD. Asymptotic

properties of the GMM estimator are also discussed.

In Section 4, we evaluate the performance in a

simulation study. We apply our method to two examples in Section 5 and conclude with a discussion in

Section 6.

3

2 Generalized Dirichlet latent variable models

In this section, we specify a generalized Dirichlet latent variable model. Let yi = (yi1, . . . , yip)T denote a

vector of p measurements having heterogeneous data types (e.g., continuous, categorical, and counts) over

subjects i = 1, . . . , n. We are interested in ﬂexible models for joint distributions of the data. A simple

way to achieve this is to assume the elements of yi are conditionally independent given a component
index si ∈ {1, . . . , k}, with the densities for each variable in each component having a parametric form.
This is inﬂexible due to the assumption that each subject i belongs to exactly one component. We

instead allow subjects to partially belong to each of the k components. We assume that the elements of
yi are conditionally independent given a latent Dirichlet vector xi = (xi1, . . . , xik)T ∈ ∆k−1, with ∆k−1
denoting the (k − 1) probability simplex. In particular, we let

yij ∼ k(cid:88)

xihgj(φjh),

(1)

h=1

where gj(φjh) is the density of the jth variable speciﬁc to component h. The elements of xi are inter-

pretable as probability weights for subject i on each of k components; in this setting, pure subjects have

weight vectors with all zeros except for a single one.

The corresponding density gj(φjh) is absolutely continuous with respect to a dominating measure
(Ω,H, µ); in general, it can take any parametric form, and may be constrained to belong to the exponential
family. Our model in Equation (1) generalizes the traditional mixed membership model to allow mixed
data types. The parameters Φ = {φjh}1≤j≤p,1≤h≤k are mixture component parameters shared by all
subjects. A full likelihood speciﬁcation is completed by choosing a population distribution for the latent
variable vector, xi ∼ P , with P a distribution on the simplex ∆k−1. In this work, we put the Dirichlet
distribution on this latent variable vector xi ∼ Dir(α) with parameter α = (α1, . . . , αk)T .

Let mi = (mi1, . . . mip)T denote a membership vector for subject i, where mij ∈ {1, . . . , k} indicates
the component that feature j in subject i is generated from. We specify the following generative model

yij | mij = h ∼ gj(φjh), mij | xi ∼ Multi(xi), xi ∼ Dir(α).

(2)

When yi is a multivariate categorical vector and gj represents a multinomial distribution, Bhat-

tacharya and Dunson (2012) proposed a simplex factor model. Their model reduces to latent Dirichlet

allocation (Blei et al., 2003) in the special case in which yij is the number of occurrences of word j in

document i, and gj(φjh) is a Poisson distribution, with scalar φjh indicating the rate of occurrence of
word j in topic h. We also assume xi ∼ Dir(α), but allow a general form for the component speciﬁc
densities. This leads to the following likelihood after marginalizing out the latent variables:

n(cid:89)

(cid:20)(cid:90) p(cid:89)

(cid:18) k(cid:88)

(cid:19)

(cid:21)

p(Y | α, Φ) =

xihgj(yij | φjh)

dP (xi)

.

i=1

j=1

h=1

Categorical data: Let yij ∈ {1, . . . , dj}, for j = 1, . . . , p, so that we have multivariate categorical data
that can be organized as a p-way contingency table. Let c = (c1, . . . , cp)T with cj ∈ {1, . . . , dj} and
πc = Pr(yi = c) = Pr(yi1 = c1, . . . , yip = cp). Then π = {πc} is a probability tensor (Dunson and Xing,
c πc = 1. In this case, gj(φjh) d= Multi(φjh) is a multinomial distribution,
with φjh = (φjh1, . . . , φjhdj )T ∈ ∆dj−1 a probability vector. As shown by Bhattacharya and Dunson

2009) satisfying πc ≥ 0 and(cid:80)

4

(2012), marginalizing out the mixture proportion xi leads to a Tucker decomposition of π

k(cid:88)

k(cid:88)

p(cid:89)

πc =

. . .

gh1,...,hp

φjhj cj .

h1=1

hp=1

j=1

Mixed data types: When yi includes variables of mixed data types, gj(φjh) belongs to the exponential

family of distributions with mean parameter φjh. We are interested in estimating mean parameters

Φ = (Φ1; . . . ; Φp) with Φj = (φj1, . . . , φjk). For categorical variables yij, φjh is a probability vector

indicating the distribution of the categories in component h; for non-categorical variables yij, φjh is a

scalar term indicating the mean parameter of that feature for component h.

3 Generalized methods of moments estimation

In this section, we provide a procedure for parameter estimation in generalized Dirichlet latent variable

models based on a GMM framework. We ﬁrst state the moment functions and then propose a two-stage

procedure for parameter estimation.

3.1 Moment functions used in MELD

The idea behind method of moments (MM) algorithms is to derive a list of moment equations that
have expected value zero at the true parameter values. Given observations {yi}1≤i≤n, MM algorithms
specify (cid:96) moment functions and deﬁne a moment vector f (yi, θ) = (f1(yi, θ), . . . , f(cid:96)(yi, θ))T such that
E[f (yi, θ)] = 0 at the true parameter θ = θ0 ∈ Θ ⊆ Rp. The moment vector is then used to specify an
estimator by solving the following (cid:96) sample equations with p unknowns:

(cid:98)θ such that 0 =

(cid:104)

n(cid:88)

i=1

fn(θ) ≡ 1
n

f (yi, θ)

(cid:105)

at θ = (cid:98)θ,

(3)

where fn(θ) is the sample estimate of E[f (yi, θ)]. When f (yi, θ) is linear in θ (e.g., linear regression
with instrumental variables) with independent moment functions and (cid:96) = p, then θ can be uniquely

determined. When (cid:96) > p, the system in Equation (3) might be over-determined, in which case standard

MM cannot be applied.

Generalized method of moments (GMM) minimizes the following quadratic form rather than solving

a linear system:

(cid:98)θ ≡ arg min

θ

(cid:2)Qn(θ; An) = fn(θ)T An fn(θ)(cid:3),

with An the asymptotically optimal positive semideﬁnite matrix:

A−1

n = Sn = Var[n1/2fn(θ)].

(4)

In the over-determined setting, minimizing the quadratic form is still well deﬁned. Hansen’s two stage

procedure (Hansen, 1982) obtains an initial estimate of (cid:98)θ using a suboptimal weight matrix, such as the

identity. This initial estimate is used to calculate An, which is then plugged into Equation (4) to obtain

the ﬁnal estimate (Hall, 2005) which is consistent and asymptotically normal (Hansen, 1982).

Applying GMM to all of the parameters in the generalized Dirichlet latent variable model is not feasible

due to the dimensionality of the parameter space. Higher-order moment functions are complex and involve

5

large numbers of unknowns. We will make use of a tensor decomposition and deﬁne moment functions

that only depend on lower order moments. This extends recent moment tensor approaches to our proposed

broad class of latent variable models (Arora et al., 2012; Anandkumar et al., 2012a,b; Hsu and Kakade,

2013; Anandkumar et al., 2014a,b). One key innovation in our procedure is that we allow for lower order

interaction moments across variables, where each variable can have a diﬀerent distribution. The moment

functions will have rank one decompositions of the component mean parameters over the heterogeneous

variables. These heterogeneous low-order moment functions lead to fast and robust parameter estimation

in MELD, as we show in the simulation and application sections.

We now deﬁne the moment functions. Recall that yij is the jth variable for subject i, which may

include various data types, including continuous, categorical, or count data. Each categorical variable

yij is encoded as a vector bij of length dj containing all zeros except for a one in position yij. If yij

is a non-categorical variable, then bij = yij is a scalar value. In the following, we assume that there

are k latent components and that the latent variable xi follows a Dirichlet distribution with parameters

α = (α1, ..., αk)T and α0 =(cid:80)k

h=1 αh. Given an observation bij and hyperparameter α, we deﬁne moment

functions based on second and third moments:
jt (yi, Φ) = bij ◦ bit − α0
F (2)
α0 + 1
jst (yi, Φ) = bij ◦ bis ◦ bit
F (3)

(cid:18)

2α2
0

− α0

α0 + 2

+

(α0 + 1)(α0 + 2)
1 ≤ j, s, t ≤ p, j (cid:54)= s (cid:54)= t.

µj ◦ µt − ΦjΛ(2)ΦT
t ,

1 ≤ j, t ≤ p, j (cid:54)= t

(cid:19)

bij ◦ bis ◦ µt + µj ◦ bis ◦ bit + bij ◦ µs ◦ bit

µj ◦ µs ◦ µt − Λ(3) ×1 Φj ×2 Φs ×3 Φt,

(5)

(6)

We use ◦ to denote an outer product, and ×s indicates multiplication of a tensor with a matrix for mode
s. The means µj take the values µj = E(bij | Φ) = 1
Φjα for j = 1, ..., p. The second moment function
jst (yi, Φ) is a dj × ds × dt tensor. We
jt (yi, Φ) is a dj × dt matrix, and the third moment function F (3)
F (2)
set dj = 1 when variable j is non-categorical. The matrix Λ(2) is a k × k matrix with α/[α0(α0 + 1)]
along the diagonals and zero for all other entries. Λ(3) is a three-way tensor with the diagonal entry
λ(3)
h = 2αh/[α0(α0 + 1)(α0 + 2)] for h = 1, . . . , k and all the other entries are zero.

α0

The following theorem states that, at the true parameter value, the expectations of the moment

functions are zero. The proof can be found in Appendix A.

Theorem 1 (Moment conditions in MELD). The expectations of the second moment matrix F (2)
and third moment tensor F (3)

jt (yi, Φ)
jst (yi, Φ) deﬁned in Equations (5) and (6) are zero at true model parameter

values Φ0

E[F (2)

jt (yi, Φ0)] = 0, E[F (3)

jst (yi, Φ0)] = 0,

with the expectations taken with respect to yi.

3.2 Two stage optimal estimation

We will state two versions of Hansen’s two stage optimal GMM estimation procedure for parameter

inference in MELD, with the ﬁrst version using the second moment matrix in (5) and second version

using both the second moment matrix in (5) and the third moment tensor in (6). To ﬁx notation, we

re-state Hansen’s two stage GMM procedure as:

6

(cid:2)Qn(θ; I) = fn(θ)T fn(θ)(cid:3);
(cid:2)Qn(θ; An) = fn(θ)T Anfn(θ)(cid:3) as the ﬁnal parameter estimate.

n ;

(1) estimate (cid:98)θ = arg minθ
(2) given (cid:98)θ calculate Sn and set An = S−1
(3) compute (cid:98)θ = arg minθ
(cid:18)

We now deﬁne two moment vectors used in MELD. We deﬁne the ﬁrst version of moment vector

f (2)(yi, Φ) by stacking second moment matrices in Equation (5) as follows:

f (2)(yi, Φ) =

vec[F (2)

12 (yi, Φ)]T , . . . , vec[F (2)

1p (yi, Φ)]T ,

As the moment matrix is symmetric in the sense that F (2)

, we only consider

vec[F (2)

23 (yi, Φ)]T , . . . , vec[F (2)

2p (yi, Φ)]T , . . . , vec[F (2)

jt (yi, Φ) = (cid:2)F (2)

tj (yi, Φ)(cid:3)T

p−1,p(yi, Φ)]T

the case where j < t. Assuming d levels for each element of yi results in a moment vector of dimension
p(p− 1)d2/2. The second version of moment vector f (3)(yi, Φ) is deﬁned by stacking the second moment
matrices and third moment tensors in Equations (5) and (6)

(cid:19)T

.

(7)

(cid:18)

f (3)(yi, Φ) =

vec[F (2)

12 (yi, Φ)]T , . . . , vec[F (2)

p−1,p(yi, Φ)]T ,

vec[F (3)

123(yi, Φ)]T , . . . , vec[F (3)

12p(yi, Φ)]T ,

vec[F (3)

134(yi, Φ)]T , . . . , vec[F (3)

13p(yi, Φ)]T . . . , vec[F (3)

p−2,p−1,p(yi, Φ)]T

(cid:19)T

.

(8)

The moment matrices F (2)
F (3)

jt (yi, Φ) follow the same order as in f (2)(yi, Φ). The third moment tensors
jst (yi, Φ) are ordered such that the index on the right runs faster than the index on the left in the
subscript. The third moment tensor is symmetric with respect to any permutations of its indices, so we
include only the moment tensors with j < s < t. The dimension of this moment vector is p(p − 1)d2/2 +

(cid:2)p3 − 3p(p − 1) − p(cid:3) d3/6 with the assumption of d levels for each element of yi.

Based on the two moment vector versions, we deﬁne the following two quadratic functions used for

our GMM estimation:

Q(2)

n (Φ; A(2)
n (Φ; A(3)

n ) = f (2)
n ) = f (3)

n (Φ)T A(2)
n (Φ)T A(3)

n f (2)
n f (3)

Q(3)

n (Φ),

n (Φ),

(9)

(10)

where f (2)

n (Φ) and f (3)

n (Φ) are sample estimates of the expectations of the moment vectors

f (2)
n (Φ) =

1
n

f (2)(yi, Φ),

f (3)
n (Φ) =

1
n

f (3)(yi, Φ),

and A(2)

n and A(3)

the µj and µt in f (2)(yi, Φ) are replaced by their sample estimates(cid:98)µj = 1

n are two positive semideﬁnite matrices. We emphasize that when calculating f (2)

(cid:80)n
i=1 bij and(cid:98)µt = 1

(cid:80)n

n

n (Φ),

i=1 bit,
Φtα. In doing this the second moment matrix

n

instead of their parametric counterparts 1
α0
F (2)

Φjα and 1
α0

jt (yi, Φ) becomes a function of Φ only through ΦjΛ(2)ΦT

t , which is a rank one summation of φjh and
φth. This method facilitates development of a fast coordinate descent algorithm for parameter estimation.
Similar methods are used for calculating f (3)

n (Φ).

n to an identity weight matrix. Then the quadratic functions in Equations

n(cid:88)

i=1

n(cid:88)

i=1

In the ﬁrst stage, we set A(·)

(9) and (10) can be re-written as follows

p−1(cid:88)

p(cid:88)

j=1

t=j+1

Q(2)

n (Φ, I) =

||F (2)

n,jt(Φ)||2
F ,

7

p−1(cid:88)

p(cid:88)

Q(3)

n (Φ, I) =

p−2(cid:88)

p−1(cid:88)

p(cid:88)

||F (3)

n,jst(Φ)||2
F ,

||F (2)

n,jt(Φ)||2

F +

j=1

t=j+1

j=1

s=j+1

t=s+1

where F (2)

n,jt(Φ) and F (3)

n,jst(Φ) are deﬁned as

n(cid:88)

1
n

n(cid:88)

i=1

1
n

F (3)

jst (yi, Φ),

F (2)

n,jt(Φ) =

F (2)
jt (yi, Φ), F (3)

n,jst(Φ) =

with µj, µs and µt replaced by (cid:98)µj, (cid:98)µs and (cid:98)µt respectively in F (2)

i=1

jt (yi, Φ) and F (3)

jst (yi, Φ). ||·||2

F indicates

the Frobenius norm, the element-wise sum of squares.

We obtain a ﬁrst stage estimator of Φ by minimizing the quadratic forms using Newton-Raphson.

Note that after we substitute µj by its sample estimate, only the last term of F (2)
involves unknown parameter Φ. For simplicity we denote

n,jt(Φ) and F (3)

n,jst(Φ)

E(2)

n,jt = F (2)
n,jst = F (3)

n,jt(Φ) + ΦjΛ(2)ΦT
t ,
n,jst(Φ) + Λ(3) ×1 Φj ×2 Φs ×3 Φt.

E(3)

The two quantities E(2)
other mean parameters ﬁxed. After calculating the gradient and Hessian of Q(2)

n,jst can be computed directly from the samples. We optimize φjh with
n (φjh, I), the update rule

n,jt and E(3)

simply becomes

φs

jh =

p(cid:80)

t=1,t(cid:54)=j

(E

p(cid:80)

(2)
n,jtφth)T

,

φT

thφth

(λ(2)
h )

t=1,t(cid:54)=j

(11)

(cid:21)

,

(12)

where E

(2)

n,jt = E(2)

h(cid:48)(cid:54)=h λ(2)

is the hth diagonal entry of Λ(2).

The update rule for φjh with Q(3)

n (φjh, I) can be calculated as

h

h(cid:48) φjh(cid:48) ◦ φth(cid:48) and λ(2)
p(cid:80)

n,jtφth)T + λ(3)

(2)

h

(cid:20)
p(cid:80)

p(cid:80)

(cid:20)

s=1,s(cid:54)=j

t=1,t(cid:54)=s,t(cid:54)=j

thφth + (λ(3)
φT

h )2

s=1,s(cid:54)=j

t=1,t(cid:54)=s,t(cid:54)=j

(E

p(cid:80)

(3)

(cid:21)
n,jst ×2 φsh ×3 φth)T

(φT

shφsh)(φT

thφth)

n,jt −(cid:80)
p(cid:80)

λ(2)
h

φs

jh =

(E

p(cid:80)

t=1,t(cid:54)=j

t=1,t(cid:54)=j

(λ(2)

h )2

n,jst −(cid:80)

where E

(3)

n,jst = E(3)

h(cid:48)(cid:54)=h λ(3)

h(cid:48) φjh(cid:48) ◦ φsh(cid:48) ◦ φth(cid:48) and λ(3)

h

is the hth diagonal entry in Λ(3). The

derivations can be found in Appendix E. After updating φjh using the above equations, we retract φjh to

its probability simplex when yij is a categorical variable. We use the diﬀerence of the objective function

between two iterations divided by the dimension of the moment vector to determine convergence.
particular, we stop iterations when this value is smaller than 1 × 10−5.

In

After an initial consistent estimator of Φ is found, we calculate the asymptotic covariance matrix of
n and deﬁne a new weight matrix A(·)
n )−1 for a second
n can be derived analytically, and we provide the results in the

the two versions of moment vectors S(·)
stage GMM estimation. The form of S(·)
Supplement Materials. In our implementation, the calculation of A(·)
dense matrix S(·)
when including the oﬀ-diagonal terms in the weight matrix, the updating rules become intrinsically
n = 1/diag[(S(·)
n )]
in the second stage estimation. This approximation has been used in previous GMM implementations

In practice, we only extract the diagonal elements of S(·)

n with dimension scaling as O(p2d2) for f (2)

n requires the inversion of a full-rank

n (Φ) and O(p3d3) for f (3)

n and let A(·)

n (Φ). In addition,

n = (S(·)

complicated.

(J¨oreskog and S¨orbom, 1987). The gradient descent update equations can be found by slight modiﬁcation

of Equations (11) and (12) with weights included.

8

Note that the moment functions do not solve the identiﬁability problems with respect to Φ. When α

is a constant vector, any permutation τ of 1, . . . , k with Φj(τ ) = (φjτ (1), . . . , φjτ (k)) for all j = 1, . . . , p

satisﬁes the moment condition. This problem is inherited from the label switching problem in mixture

models. A similar situation occurs when there are ties in α and the permutation is restricted to each

tie. However, in real applications, a minimizer of the quadratic function is generally suﬃcient to ﬁnd a

parameter estimate that is close to a unique conﬁguration of the true parameter.

3.3 Properties of parameter estimates

We use GMM asymptotic theory to show that parameter estimates in MELD are consistent. We assume
the following regularity conditions on the two versions of moment vector f (·)(yi, Φ) and the parameter
space Θ.

Assumption 1 (Regularity conditions (Assumption 3.2, 3.9 and 3.10 (Hall, 2005))). 1) f (·)(yi, Φ) is
continuous on Θ for all yi ∈ Y; 2) E[f (·)(yi, Φ)] < ∞ and continuous for Φ ∈ Θ; 3) Θ is compact and
E[supΦ∈Θ||f (·)(yi, Φ)||] < ∞.

Remark 1. Conditions 1) and 2) are satisﬁed in MELD. Condition 3) is satisﬁed automatically for cat-
egorical variables, noting that φjh ∈ ∆dj−1 ⊂ Rdj is compact. For non-categorical variables, we can
restrict support to a large compact domain such as closed intervals in R without sacriﬁcing practical
performance.

With these conditions, we further assume that the weight matrix A(·)

n converges to a positive deﬁnite

matrix A(·) in probability. We deﬁne the population analogs of the quadratic functions as

0 (Φ; A(2)) = E[f (2)(yi, Φ)]T A(2)E[f (2)(yi, Φ)],
Q(2)
0 (Φ; A(3)) = E[f (3)(yi, Φ)]T A(3)E[f (3)(yi, Φ)].
Q(3)

(13)

(14)

We have the following lemma showing the uniform convergence of Q(·)

n (Φ; A(·)
n ).

Lemma 1 (Uniform convergence (Lemma 3.1 (Hall, 2005))). Under regularity conditions in Assumption

1,

|Q(2)

n (Φ; A(2)

n ) − Q(2)

0 (Φ; A(2))| p→ 0

|Q(3)

n (Φ; A(3)

n ) − Q(3)

0 (Φ; A(3))| p→ 0.

sup
Φ∈Θ

sup
Φ∈Θ

Theorem 2 (Consistency). Under the same conditions in Lemma 1, the estimator (cid:98)Φ(2) that minimizes
n ) converges to the true parameter Φ0 in probability. A similar result holds for (cid:98)Φ(3) that

n (Φ; A(2)

Q(2)
minimizes Q(3)

n (Φ; A(3)

n ).

The proof can be found in Appendix B.

The asymptotic normality of (cid:98)Φ(2) and (cid:98)Φ(3) can also be established by assuming the following condi-

tions on ∂f (·)(yi, Φ)/∂Φ.
Assumption 2 (Conditions on ∂f (·)(yi, Φ)/∂Φ (Assumptions 3.5, 3.12 and 3.13. (Hall, 2005))). 1) ∂f (·)(yi, Φ)/∂Φ
exists and is continuous on Θ for all yi ∈ Y; 2) Φ0 is an interior point of Θ; 3) E[∂f (·)(yi, Φ)/∂Φ] = G(·)
0 (Φ) < ∞;
4) G(·)
n (Φ) uniformly con-
verges to G(·)

0 (Φ) is continuous on some neighborhood N of Φ0; 5) the sample estimate G(·)

0 (Φ).

9

Remark 2. We derive ∂f (·)(yi, Φ)/∂Φ in Appendix D. Conditions 1, 3, and 4 are satisﬁed in MELD.
Condition 5 can be shown with continuousness of the derivative and the compactness of Θ.

Theorem 3 (Asymptotic normality). With Assumptions 1 and 2, we have

(cid:18)

vec((cid:98)Φ(·)) − vec(Φ0)

(cid:19) p→ N

(cid:18)

0, M (·)S(·)(M (·))T

(cid:19)

n1/2

with

M (·) = [(G(·)

0 )T A(·),
0 = E[∂f (·)(yi, Φ)/∂Φ]|Φ=Φ0 and S(·) = limn→∞ Var[n1/2f (·)

where G(·)
Appendix C. The optimal estimator can be obtained so that the weight matrix A(·)
(Hansen, 1982).

0 )T A(·)G(·)

0 ]−1(G(·)

n (Φ0)]. The proof can be found in
n → A(·) = (S(·))−1

3.4 Model selection using goodness of ﬁt tests

We use goodness of ﬁt tests to choose the number of latent components k. With the optimal weight matrix
A(·)
n → (S(·))−1, Wald-, Lagrange multiplier- (score) and likelihood ratio-type tests can be constructed.
We could construct a sequence of test statistics with increasing k to quantify the goodness of ﬁt in MELD.

However, this approach requires the calculation of the optimal weight matrix and large matrix inversions.

Instead, we quantify goodness of ﬁt using a ﬁtness index (FI) (Bentler, 1983). Practically, we show in an

extensive simulation study that the FI has low error in estimating k in MELD.

The FI is deﬁned based on the value of the objective function evaluated at the parameter esti-

mate (Bentler, 1983)

n ((cid:98)Φ(·), A(·)

FI = 1 − Q(·)
(e(·)

n )
n e(·)
n,jt) for j < t and e(3)

n )T A(·)

n

,

(15)

n is the concatenation of vec(E(2)

where e(2)
for j < t and vec(E(3)

n,jst) for j < s < t. This FI is for any weight matrix A(·)

n is the concatenation of both vec(E(2)

n,jt)
n . It can be viewed as a

normalized objective function; thus, the FI has values less than one. Larger values of FI indicate a better

ﬁt.

4 Simulation study

In this section, we evaluate the accuracy and run time of MELD in simulations with both categorical

and mixed data types. We use two stage estimations described in previous sections. In the ﬁrst stage an
identity weight matrix is used. In second stage we set A(·)
suppress the weight matrix A(·) and the subscript n in the objective functions Q(·)

n )]. For notation convenience we

n = 1/diag[(S(·)

n (Φ, A(·)).

4.1 Categorical data

For simulation with categorical data, we considered a low dimensional setting (p = 20) so that both

second and third order moment functions may be eﬃciently calculated. A simulation with a moderate

dimension (p = 100) was also studied and is summarized in the Supplementary Materials.

We assumed each of the 20 variables has d = 4 levels. We set the number of components to k = 3

and generated φjh from Dir(0.5, 0.5, 0.5, 0.5) with h = 1, . . . , k, and α was set to (0.1, 0.1, 0.1)T . We

10

drew n = {50, 100, 200, 500, 1, 000} samples from the generative model in Equation (1) with gj(φjh)
a multinomial distribution. For each value of n, we generated ten independent data sets. We ran
MELD for diﬀerent values of k = {1, . . . , 5}. The FI criterion consistently chose the correct number of
latent components k in ﬁrst stage estimation (Table 1). For second stage, FI did not perform well. The

trajectories of the objective functions under diﬀerent values of k are shown in Figure S1. The convergence

of parameter estimations on the ten simulated data sets under n = 1, 000 and k = 3 is shown in Figure

S2. MELD converged in about 25 iterations with Q(2)(Φ) and in about 10 iterations with Q(3)(Φ).

Table 1: Goodness of ﬁt tests using the ﬁtness index (FI) in categorical simulation. Larger

values of FI indicate better ﬁt, with the maximum at one. Results shown are based on ten simulated

data sets for each value of n. Standard deviations of FI are provided in parentheses.

n

50

100

200

500

1,000

k

1

2

3

4

5

1

2

3

4

5

1

2

3

4

5

1

2

3

4

5

1

2

3

4

5

Q(2)(Φ) 1st stage

Q(2)(Φ) 2nd stage

Q(3)(Φ) 1st stage

Q(3)(Φ) 2nd stage

0.824(0.020)

0.908(0.011)

0.930(0.004)

0.901(0.010)

0.795(0.033)

0.860(0.012)

0.930(0.011)

0.960(0.005)

0.942(0.012)

0.863(0.046)

0.869(0.012)

0.940(0.007)

0.980(0.001)

0.967(0.006)

0.911(0.017)

0.882(0.007)

0.948(0.006)

-1.865(4.950)

-1.285(2.630)

0.570(0.044)

0.588(0.032)

0.686(0.021)

-0.521(2.142)

0.282(0.644)

0.677(0.044)

0.691(0.042)

0.782(0.022)

0.679(0.060)

0.699(0.047)

0.761(0.061)

0.780(0.019)

0.824(0.008)

0.783(0.022)

0.799(0.019)

0.992(<0.001)

0.884(0.024)

0.966(0.001)

0.983(0.004)

0.937(0.014)

0.888(0.003)

0.951(0.003)

0.874(0.026)

0.894(0.006)

0.828(0.006)

0.855(0.009)

0.938(0.005)

0.921(0.007)

0.729(0.008)

0.881(0.008)

0.996(<0.001)

0.950(0.005)

0.982(0.001)

0.989(0.002)

0.953(0.010)

0.938(0.004)

0.932(0.006)

0.961(0.003)

0.951(0.006)

0.585(0.031)

-27.987(40.128)

0.745(0.032)

-27.240(54.415)

0.775(0.022)

-27.713(42.607)

0.735(0.023)

0.653(0.024)

0.651(0.021)

0.795(0.030)

0.254(0.090)

0.305(0.091)

-0.031(0.703)

-4.457(8.924)

0.851(0.009)

-4.868(11.889)

0.822(0.010)

0.768(0.044)

0.682(0.021)

0.838(0.014)

0.919(0.004)

0.891(0.008)

0.864(0.015)

0.713(0.012)

0.870(0.013)

0.225(0.019)

0.232(0.080)

0.298(0.070)

0.306(0.054)

0.278(0.049)

0.287(0.050)

0.286(0.042)

0.414(0.080)

0.427(0.080)

0.388(0.073)

0.365(0.065)

0.353(0.042)

0.571(0.017)

0.615(0.030)

0.609(0.031)

0.579(0.030)

0.550(0.034)

We compared MELD with the simplex factor model (SFM) (Bhattacharya and Dunson, 2012) and

latent Dirichlet allocation (LDA) (Blei et al., 2003). For SFM, we ran 10, 000 steps of MCMC with ﬁxed k

and a burn-in of 5, 000 iterations. Posterior thinned samples were collected by keeping one posterior draw

after every 50 steps. For the LDA model, we used the lda package in R (Chang, 2012) with collapsed

Gibbs sampling. We used the same number of MCMC iterations and burn-in iterations as with SFM.

The Dirichlet parameter for mixture proportions was set to α = 0.1 and the Dirichlet parameter for topic

distributions was set to β = 0.5. We calculated mean squared errors (MSE’s) of diﬀerent estimates as

follows. We ﬁrst recovered the membership variable mij and then calculated the parameter estimate of

yij. The MSE was calculated between the estimated parameters of yij’s and their true parameters (see

Supplementary Materials for details). MSE’s with diﬀerent values of k are shown in Figure 1 and the

running times of diﬀerent methods are reported in Table S1. MELD Q(2)(Φ) with ﬁrst stage estimation

had the most accurate parameter estimation and fastest running speed in most cases. The second stage

of MELD Q(3)(Φ) did not perform well with small values of n (i.e., n = 50), but estimation accuracy was

better when n was larger. SFM had comparable MSE’s when n was not large. However when n = 500

and 1, 000, MELD outperformed SFM.

We further evaluated performance in the presence of contamination. For each simulated data set,

11

we randomly replaced a proportion of observations (4% and 10%) with draws from a discrete uniform

distribution. The MSE’s under diﬀerent values of k are shown in Figure 1. With 4% contamination,

MELD had the most accurate parameter estimation in almost all cases. MELD Q(2)(Φ) with ﬁrst stage

estimation performed the best, followed by MELD Q(3)(Φ) with ﬁrst stage estimation. The MSE’s of

SFM increased. When we increased contamination to 10%, MELD had the most accurate MSE in all

cases. MELD Q(2)(Φ) with ﬁrst stage estimation performed best, followed by MELD Q(2)(Φ) with second

stage estimation. MELD Q(2)(Φ) consistently performed better than Q(3)(Φ), suggesting the robustness

of using lower order moments in parameter estimation.

Figure 1: Comparison of mean squared error (MSE) of estimated parameters in categorical

simulations. For SFM and LDA, posterior means of parameters are calculated using 100 posterior draws

on each of the ten simulated data sets. The values of the MSE’s and their standard deviations are in

Supplementary Table S2, S3 and S4.

4.2 Mixed data types

We considered a simulation setting mimicking applications in which DNA sequence variations inﬂuence

a quantitative trait. An additional simulation combining categorical, Gaussian, and Poisson variables
is summarized in the Supplementary Materials. We generated a sequence of nucleotides {A, C, G, T}
at 50 genetic loci along with a continuous or integer-valued trait, leading to p = 51 variables. We set

k = 2 latent components and simulated n = 1, 000 individuals, with the ﬁrst 500 from the ﬁrst compo-
nent and the last 500 from the second component. We chose eight loci J = {2, 4, 12, 14, 32, 34, 42, 44}
to be associated with the trait. Their multinomial parameters for each of the two components were

randomly drawn from Dir(0.5, 0.5, 0.5, 0.5). The distributions for nucleotides in other loci were set to
Multi(0.25, 0.25, 0.25, 0.25). Continuous traits were drawn from N (−3, 1) and N (3, 1), while count traits
were drawn from Poisson(5) and Poisson(10), respectively for the two components. Ten data sets were

simulated from the generative model in Equation (1). To assess robustness, we added contamination

(e.g., through genotyping errors) by randomly replacing 4%, 10% and 20% of the nucleotides with values
uniformly generated from {A, C, G, T}.

We ran MELD with ﬁrst stage estimation of Q(2)(Φ). We chose the number of components k =
{1, . . . , 5}. The ﬁtness test indicated that FI chose the correct value of k on all ten data sets (Table S8).

12

For each genomic locus, we calculated its marginal frequency according to the simulated data, and then

we computed the averaged KL distance between the estimated component distributions and the marginal

frequency as follows

aveKL(yij) =

k(cid:88)

dj(cid:88)

h=1

cj =1

1
k

Pr(yij = cj|mij = h) log

(cid:18) Pr(yij = cj|mij = h)

(cid:19)

Pr(yij = cj)

.

(16)

A smaller averaged KL distance suggests that the component distributions were closer to the marginal

distribution, implying that the locus frequency was not diﬀerentiated across components. The set J

corresponded exact to the eight loci with largest averaged KL distance (Table 2).

We compared MELD with the Bayesian copula factor model (Murray et al., 2013), which estimates a

correlation matrix C between variables. From this estimate, we computed partial correlations between

the response variable (trait) and each genetic locus (Hoﬀ, 2007). We ran MCMC for the Bayesian copula

factor model 10, 000 iterations with the correct value for k and a burn-in of 5, 000 iterations. Posterior

samples were collected every 50 iterations. We then selected genomic locations for which their 95%

posterior intervals of the partial correlation did not include zero. The resulting loci are shown in Table

2. The Bayesian copula factor model selected nucleotides that are not in J and missed locus 32 in most

cases.

Table 2: Quantitative trait association simulation with 50 nucleotides and one response.
Nucleotides not in J = {2, 4, 12, 14, 32, 34, 42, 44} are labeled by an underline and missing nucleotides are
crossed out. Results shown are for one of the ten simulated data sets. The complete results can be found

in Table S9 and S10.

Response

Contamination

Gaussian

Poisson

0%

4%

10%

20%

0%

4%

10%

20%

Q(2)(Φ) 1st stage

{2, 4, 12, 14, 32, 34, 42, 44}
{2, 4, 12, 14, 32, 34, 42, 44}
{2, 4, 12, 14, 32, 34, 42, 44}
{2, 4, 12, 14, 32, 34, 42, 44}
{2, 4, 12, 14, 32, 34, 42, 44}
{2, 4, 12, 14, 32, 34, 42, 44}
{2, 4, 12, 14, 32, 34, 42, 44}
{2, 4, 12, 14, 32, 34, 42, 44}

Bayesian copula factor model

{2, 4, 12, 14, 18, 27, 32 , 34, 42, 44}
{2, 4, 12, 14, 18, 27, 32 , 34, 42, 44, 45}
{2, 4, 12, 14 , 27, 32 , 34, 42, 44, 49, 50}
{2, 4, 9, 11, 12, 14 , 20, 32 , 34, 42, 44}

{2, 4, 7, 12, 14, 32 , 34, 42, 44}
{2, 4, 12, 14, 32 , 34, 42, 44}

{2, 4, 7, 12, 14, 16, 32, 34, 42, 44}
{2, 4, 7, 12, 14 , 32, 34, 35, 42, 44}

5 Applications of MELD

Promoter sequence analysis We applied MELD to gene transcription promoter data available in

the UCI machine learning repository (Lichman, 2013). The data include n = 106 nucleotide sequences
{A, C, G, T} of length 57. The ﬁrst 53 sequences are located in promoter regions, and the last 53 sequences
are located in non-promoter regions. The goal is binary classiﬁcation: using nucleotide sequence alone,

we would like to predict whether or not the sequence is in a promoter or a non-promoter region.

We included the promoter or non-promoter status of the sequences as an additional binary variable,

giving us p = 57 + 1 categorical variables. We applied MELD Q(2)(Φ) with ﬁrst stage estimation on the

full data and also on the subset of the sequences in the promoter region and the subset of sequences in
non-promoter regions separately. We set k = {1, . . . , 8}. For k = 2, MELD converged in 2.13 seconds,
compared with SFA, which took 41.6 seconds to perform 10, 000 MCMC iterations. We evaluated diﬀerent

13

values of k using the goodness of ﬁt test. FI selected two components for the full data, two components

for the promoter data, and one component for the non-promoter data (Table S13).

We choose k = 2 in the following analysis. For each nucleotide position, we calculated the averaged

KL distance between the estimated component distributions and its marginal distribution using Equa-

tion (16). An interpretation of the averaged KL distance is that it quantiﬁes the stratiﬁcation of each

nucleotide distribution across components: a larger value of the averaged KL distance indicates greater

stratiﬁcation across components, which suggests that the nucleotide is important in deﬁning and diﬀer-

entiating the components. For the full data set, we observed approximately two peaks of the averaged

KL distance, one around the 15th nucleotide and one around the 42nd nucleotide (Figure S4). The ﬁrst

peak corresponds to the start of the biologically conserved region for promoter sequences (Harley and

Reynolds, 1987). For MELD applied only to promoter sequences, this peak was reduced, suggesting

that, at approximately the 15th nucleotide, the components all included similarly well conserved distri-

butions of this nucleotide. The estimated component membership variables also showed the importance

of nucleotides around 15th nucleotide (Figure S5 and S6). For the peak around the 42nd nucleotide,

this phenomenon was reversed: the increased averaged KL distance remained in promoter sequences

but diminished in non-promoter sequences. One possible explanation is that this region was conserved

uniformly in the non-promoter region, but diﬀerentially conserved in the promoter region.

Political-economic risk data In a second application, we applied MELD to political-economic risk

data (Quinn, 2004), which include ﬁve proxy variables of mixed types measured for 62 countries. The

data set has been collected and analyzed to quantify a sense of political-economic risk, a latent quantity

associated with each of the 62 countries, using a mixed Gaussian and probit factor model (Quinn, 2004)

and a Bayesian copula factor model (Murray et al., 2013). The data are available in the MCMCpack

package. There are three categorical variables and two real valued variables (Table 3).

Table 3: Variables in the political-economic risk data

Variable

Type

Explanation

ind.jud

binary

of the national judiciary. This variable is equal to one if the

An indicator variable that measures the independence

judiciary is judged to be independent and equal to zero otherwise.

Black-market premium measurement. Original values are measured

blk.mkt

real

as the black-market exchange rate (local currency per dollar)

divided by the oﬃcial exchange rate minus one. Quinn (2004)

lack.exp.risk

ordinal

lack.corrup

ordinal

gdp.worker

real

transformed the original data to log scale.

Lack of appropriation risk measurement.

Six levels with coding 0 < 1 < 2 < 3 < 4 < 5.

Lack of corruption measurement.

Six levels with coding 0 < 1 < 2 < 3 < 4 < 5.

Real gross domestic product (GDP) per worker in 1985 international prices.

Recorded data are log transformed.

We applied MELD with k = {1, . . . , 5} using both Q(2)(Φ) and Q(3)(Φ) with ﬁrst stage estimation
to the data set. For Q(2)(Φ) with k = 3 MELD converged in 0.10s, and for Q(3)(Φ) with k = 3 MELD

converged in 0.45s. The Bayesian copula factor model took 0.91s to complete 10, 000 MCMC iterations.

14

The FI criterion for Q(2)(Φ) selected k = 4 and, for Q(3)(Φ), selected k = 3 (Table S14). We chose

results from Q(3)(Φ) with k = 3 for further analysis.

The estimated component parameters for the ﬁve variables showed distinct interpretations of the three

components (Figure S7). We might interpret the three components as low-risk, intermediate-risk, and

high-risk political-economic status respectively. The ﬁrst component had a high probability of indepen-

dence of the national judiciary (ind.jud being one) and a low measurement of black-market premium.

The ﬁrst component also had a high probability of observing 4th and 5th levels in lack.exp.risk and

3rd, 4th, and 5th levels in lack.corrup. The mean of the GDP per worker was highest among the three

components. The second component had a relatively high probability of being zero in ind.jud and a

large mean value of blk.mkt. Both of lack of appropriation risk measurement and lack of corruption

measurement put higher weights on lower category numbers (0, 1 and 2), indicating more risk and higher

levels of corruption; the GDP per worker was still high. We might interpret this component as a society

being relatively unstable while still having a good economic forecast, meaning that GDP per worker is

high, possibly through the black market. The last component had the least judicial independence as

quantiﬁed by the probability of ind.jud being zero. The black-market premium is also low, as is the

lack of risk level and lack of corruption level. The GDP per worker is by far the lowest among the three

components. We might interpret this component as society being the most unstable with the greatest

economic risk. We found although the three components had distinct stratiﬁcation, each country was a

mixture of the three components (Figure S8).

6 Discussion

In this paper, we developed a new class of latent variable models with Dirichlet-distributed latent vari-

ables for mixed data types. These generalized latent Dirichlet variable models extend previous mixed

membership models such as LDA (Blei et al., 2003) and simplex factor models (Bhattacharya and Dun-

son, 2012) to allow mixed data types. For this class of models, we developed a fast parameter estimation

procedure using generalized method of moments. Our procedure extends the moment tensor methods de-

veloped in recent work (Anandkumar et al., 2014b) to models with mixed data types. Our approach does

not require instantiation of latent variables. We derive population moment conditions after marginalizing

out the latent Dirichlet variables. We demonstrated the utility of our approach using simulations and

two applications. Our results show that MELD is a promising alternative to MCMC or EM methods for

parameter estimation, producing fast and robust parameter estimates. Since our method depends only

on certain forms of sample moments, parameter estimation does not scale with sample size n after the

moment statistics have been computed from observations. An online method to update moment statistics

when new samples arrive would allow re-estimation of the parameters to include new observations. One

limitation of our method is that the Newton-Raphson method is of order O(p2) using second moment

functions and order O(p3) using third moment functions. One possible approach to ensure tractability

of MELD when p is large is to use stochastic gradient methods to calculate an approximate gradient in

each step.

SUPPLEMENTARY MATERIALS

15

We provide Supplementary Materials that include a) Derivations of asymptotic optimal weight matri-

ces; b) Calculation of MSE and computational complexity analysis; c) Two additional simulations with

categorical variables and mixed data types; d) Supplementary Tables S1-S14; e) Supplementary Figures

S1-S8.

Appendix

A Proof of Theorem 1

Proof. We start with the case where yij is a categorical data with dj diﬀerent levels. The latent probability
vector xi = (xi1,··· , xik)T ∈ ∆k−1 deﬁnes the mixture proportion of individual i. We assume xi ∼

Dir(α1,··· , αk). Deﬁne α0 =(cid:80)

h αh and α = (α1,··· , αk)T .

We use the standard basis for encoding. We encode yij = cj as bij ∈ Rdj a binary (0/1) vector with
the cjth coordinate being 1 and all others being 0. Similarly, we encode the membership variable mij as
a k dimensional binary vector mij ∈ Rk. Consider the ﬁrst moment of bij.

µj = E(bij) = E[E(bij|mij)] = E(Φjmij) = E[E(Φjmij|xi)] = E(Φjxi) = Φj

α
α0

,

where Φj = (φj1,··· φjk).

We consider second order moment conditions. There are four types of second moments: same variable

same subject (type SS), same variable cross subject (type SC), cross variable same subject (type CS),

and cross variable cross subject (type CC). Of the four types, only the CS type is needed to prove the
theorem. The CS type second moment for bij and bit (j (cid:54)= t) can be written as
t = ΦjE(xi ◦ xi)ΦT
t .

E(bij ◦ bit) = ΦjE(mij ◦ mit)ΦT

For a Dirichlet distributed variable,

E(xi ◦ xi) = cov(xi) + E(xi) ◦ E(xi)

=

1

α0(α0 + 1)

diag(α) +

α0

α2

0(α0 + 1)

α ◦ α.

Then we have

E(bij ◦ bit) = ΦjE(xi ◦ xi)ΦT

t

k(cid:88)

1

=

α0(α0 + 1)

h=1

αhφjh ◦ φth +

α0

α0 + 1

µj ◦ µt.

(17)

We next consider third order moment conditions. There are eight diﬀerent types of third order moments

for bij. Only the moments with diﬀerent variables for the same subject are needed to prove the theorem.
We consider the third cross moment for bij, bit and bis with j (cid:54)= t (cid:54)= s for the same subject. First we

calculate E(mij ◦ mis ◦ mit).

E(mij ◦ mis ◦ mit) = E[E(mij ◦ mis ◦ mit|xi)]

= E(xi ◦ xi ◦ xi)

=

1

α0(α0 + 1)(α0 + 2)

(cid:18)

(α ◦ α ◦ α) +

k(cid:88)

h=1

αh(eh ◦ eh ◦ α)

16

k(cid:88)
k(cid:88)

h=1

(cid:18)

(cid:18)

+

αh(eh ◦ α ◦ eh) +

αh(α ◦ eh ◦ eh)

+ 2

αh(eh ◦ eh ◦ eh)

k(cid:88)
(cid:19)

h=1

.

Here eh is standard basis vector of length k with hth coordinate being one. The third order moment
tensor of bij ◦ bis ◦ bit can be derived as

h=1

E(bij ◦ bis ◦ bit) = E(mij ◦ mis ◦ mit) × {Φj, Φs, Φt}

k(cid:88)

(Φjα) ◦ (Φsα) ◦ (Φtα) +

αh[φjh ◦ φsh ◦ (Φtα)]

α0(α0 + 1)(α0 + 2)

+

αh[φjh ◦ (Φsα) ◦ φth] +

h=1

αh[(Φjα) ◦ φsh ◦ φth]

k(cid:88)

h=1

(cid:19)

1

k(cid:88)
k(cid:88)

h=1

=

=

+ 2

αhφjh ◦ φsh ◦ φth

h=1

1

α0(α0 + 1)(α0 + 2)

0µj ◦ µs ◦ µt
α3

+ α2

+ α2

+ α2

0(α0 + 1)E(bij ◦ bis ◦ µt) − α3
0(α0 + 1)E(µj ◦ bis ◦ bit) − α3
0(α0 + 1)E(bij ◦ µs ◦ bit) − α3
k(cid:88)

αhφjh ◦ φsh ◦ φth

(cid:19)

.

+ 2

0µj ◦ µs ◦ µt
0µj ◦ µs ◦ µt
0µj ◦ µs ◦ µt

(18)

The theorem follows Equations (17) and (18). For non-categorical data, we let bij ≡ yij and φjh is a

h=1

scalar mean parameter for yij. Equations 17 and 18 still hold.

n ) and A(·) in Q(·)

0 (Φ; A(·)). Lemma 1
0 (Φ0)| < /3] = 1

n (Φ0) − Q(·)

(19)

(20)

(21)

B Proof of Theorem 2
Proof. For notation simplicity we suppress A(·)
implies limn→∞ Pr[|Q(·)
for  > 0. This result also implies

n ((cid:98)Φ(·)) − Q(·)

n (Φ; A(·)

n in Q(·)

0 ((cid:98)Φ(·))| < /3] = 1 and limn→∞ Pr[|Q(·)
n ((cid:98)Φ(·)) + /3] = 1.

n→∞Pr[Q(·)
n→∞Pr[Q(·)
On the other hand, (cid:98)Φ(·) minimizes Q(·)

lim

lim

n (Φ), therefore

0 (Φ0) + /3] = 1.

0 ((cid:98)Φ(·)) < Q(·)
n (Φ0) < Q(·)
n ((cid:98)Φ(·)) < Q(·)
0 ((cid:98)Φ(·)) < Q(·)
0 ((cid:98)Φ(·)) < Q(·)

17

n→∞ Pr[Q(·)

lim

n→∞ Pr[Q(·)

lim

n (Φ0) + /3] = 1.

n (Φ0) + 2/3] = 1.

n→∞ Pr[Q(·)

lim

0 (Φ0) + ] = 1.

Equations (19) and (21) imply

Together with Equation (20), we get

Therefore

0 ((cid:98)Φ(·)) < ] = 1

lim

follows with Q(·)
Θ. Due to the compactness of Θ the neighborhood N C is also compact. The continuousness of Q(·)
implies the existence of inf Φ∈N C Q(·)

n→∞ Pr[0 ≤ Q(·)
0 ((cid:98)Φ(·)) ≥ 0. Next, we choose a neighborhood N , which contains Φ0 in
0 (Φ) and it is positive. Let  = inf Φ∈N C Q(·)

0 (Φ0) = 0 and Q(·)

0 (Φ), then we get

0 (Φ)

(22)

0 ((cid:98)Φ(·)) < inf

n→∞ Pr[0 ≤ Q(·)

lim

Q(·)
0 (Φ)] = 1.

Therefore limn→∞ Pr((cid:98)Φ(·) (cid:54)∈ N C) = 1, which suggests limn→∞ Pr((cid:98)Φ(·) ∈ N ) = 1. Shrinking the neigh-

Φ∈N C

(23)

borhood size of N we get

n→∞ Pr((cid:98)Φ(·) = Φ0) = 1.

lim

C Proof of Theorem 3
Proof. We approximate f (·)

n ((cid:98)Φ(·)) using ﬁrst order Taylor expansion
n (Φ0) + G(·)

n ((cid:98)Φ(·)) = f (·)

f (·)

Ignoring the high order term, we left multiply both sides by [G(·)

n (Φ0)[vec((cid:98)Φ(·)) − vec(Φ0)] + O{[vec((cid:98)Φ(·)) − vec(Φ0)]2}
n ((cid:98)Φ(·)) ≈ [G(·)
n ((cid:98)Φ(·))]T A(·)

n (Φ0)[vec((cid:98)Φ(·)) − vec(Φ0)].

n ((cid:98)Φ(·))]T A(·)
n f (·)

n ((cid:98)Φ(·))]T A(·)

n . Then we get

n G(·)

n (Φ0)

+ [G(·)

n ((cid:98)Φ(·))]T A(·)

[G(·)

n f (·)

The fact that estimator (cid:98)Φ(·) minimizes Q(·)
n1/2[vec((cid:98)Φ(·)) − vec(Φ0)] ≈ −{[G(·)

we get

n (Φ, A(·)

n ((cid:98)Φ(·))]T A(·)

n ) implies the left hand side equals to zero. Therefore

n G(·)

n (Φ0)}−1[G(·)

n n1/2f (·)

n (Φ0).

n ((cid:98)Φ(·))]T A(·)

The theorem follows with n1/2f (·)

n (Φ0)

p→ N (0, S(·)) and Assumptions 1 and 2.

D Derivatives of moment functions

D.1 Second moment matrix

The second moment matrix F (2)
F (2)

jt (yi, Φ) with respect to Φj and Φt can be written as

jt (yi, Φ) may be written as bij ◦ bit − ΦjE(xi ◦ xi)ΦT

t . The derivatives of

∂vec[F (2)

jt (yi, Φ)]

∂vec(Φj)

∂vec[F (2)

jt (yi, Φ)]

∂vec(Φt)

= − ∂{[ΦtE(xi ◦ xi)] ⊗ Idj}vec(Φj)
∂vec(Φj)
= −[ΦtE(xi ◦ xi)] ⊗ Idj ,
= −T

∂vec[ΦtE(xi ◦ xi)ΦT
j ]

∂vec(Φt)

∂{[ΦjE(xi ◦ xi)] ⊗ Idtvec(Φt)}

= −T
= −T{[ΦjE(xi ◦ xi)] ⊗ Idt},

∂vec(Φt)

18

where ⊗ indicates a Kronecker product and T is a dtk × dtk 0/1 matrix that satisﬁes

vec[ΦjE(xi ◦ xi)ΦT

t ] = T vec[ΦtE(xi ◦ xi)ΦT
j ].

Therefore E[∂f (2)(yi, Φ)/∂Φ] is a block matrix with block of −ΦT
sponding to vec(Φt) and rows corresponding to vec[F (2)

jt (yi, Φ)].

t

E(xi ◦ xi) ⊗ Idj on columns corre-

D.2 Third moment tensor

We next consider the third moment tensor. Write F (3)
Φs ×3 Φt. Then only the second term involves Φ.

jst (yi, Φ) as bij ◦ bij ◦ bij − E(xi ◦ xi ◦ xi) ×1 Φj ×2

The derivatives of F (3)

jst (yi, Φ) with respect to Φj can be written as

∂vec[F (3)

jst (yi, Φ)]

∂vec(Φj)

= − ∂vec{[E(xi ◦ xi ◦ xi) ×1 Φj ×2 Φs ×3 Φt](1)}
= − ∂vec{[ΦjE(xi ◦ xi ◦ xi)(1)(Φt ⊗ Φs)T}
= −(Φt ⊗ Φs)vec[E(xi ◦ xi ◦ xi)(1)]T ⊗ Idj ,

∂vec(Φj)

∂vec(Φj)

where subscript (1) indicates model-1 unfolding of a three way tensor.

The derivatives of F (3)

jst (yi, Φ) with respect to Φs and Φt can be calculated accordingly by introducing

0/1 transformation matrices T(2)(1) and T(3)(1) both with size djdsdt × djdsdt that satisfy

vec{[F (3)

jst (yi, Φ)](1)} = T(2)(1)vec{[F (3)
= T(3)(1)vec{[F (3)

jst (yi, Φ)](2)}
jst (yi, Φ)](3)}.

Then

∂vec[F (3)

jst (yi, Φ)]

∂vec(Φs)

= −T(2)(1)

∂vec{[E(xi ◦ xi ◦ xi) ×1 Φj ×2 Φs ×3 Φt](2)}

∂vec(Φs)

∂vec[F (3)

jst (yi, Φ)]

∂vec(Φt)

∂vec{[ΦsE(xi ◦ xi ◦ xi)(2)(Φt ⊗ Φj)T}

= −T(2)(1)
= −T(2)(1){(Φt ⊗ Φj)[E(xi ◦ xi ◦ xi)(2)]T ⊗ Ids},
= −T(3)(1)

∂vec{[E(xi ◦ xi ◦ xi) ×1 Φj ×2 Φs ×3 Φt](3)}

∂vec(Φs)

∂vec(Φt)

∂vec{[ΦtE(xi ◦ xi ◦ xi)(3)(Φs ⊗ Φj)T}

= −T(3)(1)
= −T(3)(1){(Φs ⊗ Φj)[E(xi ◦ xi ◦ xi)(3)]T ⊗ Idt}.

∂vec(Φt)

The conditions 1), 3) and 4) in Assumption 2 in main text follow after we calculating the derivatives

of moment functions.

E Derivation of Newton-Raphson update

We denote

E(2)

n,jt = F (2)
n,jst = F (3)

n,jt(Φ) + ΦjΛ(2)ΦT
t ,
n,jst(Φ) + Λ(3) ×1 Φj ×2 Φs ×3 Φt.

E(3)

19

With identity matrix, then the two objective functions can be written as

Q(2)

n (Φ) =

Q(3)

n (Φ) =

p−1(cid:88)
p−1(cid:88)

j=1

t=j+1

p(cid:88)
p(cid:88)
p−2(cid:88)

j=1

t=j+1

+

||E(2)

n,jt − ΦjΛ(2)ΦT

t ||2
F ,

t ||2

F

||E(2)

n,jt − ΦjΛ(2)ΦT
p(cid:88)

p−1(cid:88)

||E(3)

n,jst − Λ(3) ×1 Φj ×2 Φs ×3 Φt||2
F .

Here we suppress the weight matrix in the objective functions. We ﬁrst consider Q(2)

n (Φ). The terms

j=1

s=j+1

t=s+1

(cid:20)

− 2λ(2)

h (E

(2)

n,jtφth)T φjh + (λ(2)

h )2(φT

thφth)φT

jhφjh

(cid:21)

,

involve φjh are

(2)

n,jt = E(2)
where E
the fact that ||E(2)

p(cid:88)
n,jt −(cid:80)

t=1,t(cid:54)=j

is the hth diagonal element of Λ(2). Here we use

h

F = ||E(2)

h(cid:48) φjh(cid:48) ◦ φth(cid:48) and λ(2)
h(cid:48)(cid:54)=h λ(2)
n,jt − ΦjΛ(2)ΦT
t ||2
n,tj − ΦtΛ(2)ΦT
p(cid:88)
p(cid:88)

ξ(2) = −2λ(2)

t=1,t(cid:54)=j

h

j ||2

F . By letting

(E

(2)
n,jtφth),

γ(2) = (λ(2)

h )2

φT

thφth,

t=1,t(cid:54)=j

− 2(cid:104)E

(3)

n,jst, λ(3)

h φjh ◦ φsh ◦ φth(cid:105) + ||λ(3)

h φjh ◦ φsh ◦ φth||2

F

(cid:21)

,

the gradient ∇Q(2)

n (φjh) and Hessian ∇2Q(2)
∇Q(2)
∇2Q(2)

n (φjh) can be written as

n (φjh) = ξ(2) + 2γ(2)φjh,
n (φjh) = 2γ(2)I.

The update rule in (11) can be derived accordingly.

Then we consider Q(3)

n (Φ). The terms involve φjh are

h (E

n,jtφth)T φjh + (λ(2)

h )2(φT

thφth)φT

jhφjh

(cid:20)

p(cid:88)

t=1,t(cid:54)=j

+

(2)

p(cid:88)

(cid:20)

t=1,t(cid:54)=s,t(cid:54)=j

− 2λ(2)

p(cid:88)
n,jt −(cid:80)

s=1,s(cid:54)=j

(cid:21)

n,jst −(cid:80)

F and ||E(3)
(cid:21)

(cid:20)

p(cid:88)

t=1,t(cid:54)=j

+

By letting

(2)

n,jt = E(2)

where E
we use the symmetric property of ||E(2)
organizing the terms, we get

h(cid:48)(cid:54)=h λ(2)

h(cid:48) φjh(cid:48) ◦ φth(cid:48) and E
n,jt − ΦjΛ(2)ΦT

n,jst = E(3)
t ||2

(3)

h(cid:48)(cid:54)=h λ(3)

h(cid:48) φjh(cid:48) ◦ φsh(cid:48) ◦ φth(cid:48). Again
F . By

n,jst − Λ(3) ×1 Φj ×2 Φs ×3 Φt||2

− 2λ(2)

h (E

(2)

n,jtφth)T φjh + (λ(2)

h )2(φT

thφth)φT

jhφjh

p(cid:88)

p(cid:88)

(cid:20)

s=1,s(cid:54)=j

t=1,t(cid:54)=s,t(cid:54)=j

− 2λ(3)

h (E

(3)

n,jst ×2 φsh ×3 φth)T φjh + (λ(3)

h )2(φT

shφsh)(φT

thφth)(φT

jhφjh)

.

(cid:21)

ξ(3) = −2λ(2)

h

p(cid:88)

t=1,t(cid:54)=j

(E

(2)

n,jtφth) − 2λ(3)

h

(cid:20)

p(cid:88)

p(cid:88)

s=1,s(cid:54)=j

t=1,t(cid:54)=s,t(cid:54)=j

(E

20

(cid:21)
n,jst ×2 φsh ×3 φth)

(3)

,

p(cid:88)

t=1,t(cid:54)=j

γ(3) = (λ(2)

h )2

thφth + (λ(3)
φT

h )2

(cid:20)

p(cid:88)

p(cid:88)

s=1,s(cid:54)=j

t=1,t(cid:54)=s,t(cid:54)=j

(cid:21)

(φT

shφsh)(φT

thφth)

,

the gradient ∇Q(3)

n (φjh) and Hessian ∇2Q(3)
∇Q(3)
∇2Q(3)

n (φjh) can be written as

n (φjh) = ξ(3) + 2γ(3)φjh,
n (φjh) = 2γ(3)I.

The update rule in (12) follows directly.

References

Anandkumar, A., Ge, R., Hsu, D., and Kakade, S. M. (2014a). A tensor approach to learning mixed

membership community models. The Journal of Machine Learning Research, 15:2239–2312.

Anandkumar, A., Ge, R., Hsu, D., Kakade, S. M., and Telgarsky, M. (2014b). Tensor decompositions for

learning latent variable models. The Journal of Machine Learning Research, 15:2773–2832.

Anandkumar, A., Hsu, D., and Kakade, S. M. (2012a). A method of moments for mixture models and

hidden markov models. JMLR W&CP 23: COLT.

Anandkumar, A., Liu, Y., Hsu, D. J., Foster, D. P., and Kakade, S. M. (2012b). A spectral algorithm for

latent Dirichlet allocation. In Advances in Neural Information Processing Systems 25, pages 917–925.

Anderson, J. C. and Gerbing, D. W. (1988). Structural equation modeling in practice: A review and

recommended two-step approach. Psychological bulletin, 103(3):411.

Arora, S., Ge, R., and Moitra, A. (2012). Learning topic models - going beyond SVD. In Fifty-Third

IEEE Annual Symposium on Foundations of Computer Science, pages 1–10.

Bentler, P. M. (1983). Some contributions to eﬃcient statistics in structural models: Speciﬁcation and

estimation of moment structures. Psychometrika, 48(4):493–517.

Bhattacharya, A. and Dunson, D. B. (2012). Simplex factor models for multivariate unordered categorical

data. Journal of the American Statistical Association, 107(497):362–377.

Blei, D. M., Ng, A. Y., and Jordan, M. I. (2003). Latent Dirichlet allocation. The Journal of Machine

Learning Research, 3:993–1022.

Bollen, K. A., Kolenikov, S., and Bauldry, S. (2014). Model-implied instrumental variable-generalized

method of moments (MIIV-GMM) estimators for latent variable models. Psychometrika, 79(1):20–50.

Chang, J. (2012). lda: Collapsed Gibbs sampling methods for topic models.

Colombo, N. and Vlassis, N. (2015). FastMotif: Spectral sequence motif discovery. Bioinformatics, to

appear.

Dunson, D. B. (2003). Dynamic latent trait models for multidimensional longitudinal data. Journal of

the American Statistical Association, 98(463):555–563.

21

Dunson, D. B. and Xing, C. (2009). Nonparametric Bayes modeling of multivariate categorical data.

Journal of the American Statistical Association, 104(487):1042–1051.

Gallant, A. R., Giacomini, R., and Ragusa, G. (2013). Generalized Method of Moments with Latent

Variables. Centre for Economic Policy Research.

Hall, A. R. (2005). Generalized Method of Moments. Oxford University Press.

Hansen, L. P. (1982). Large sample properties of generalized method of moments estimators. Economet-

rica: Journal of the Econometric Society, 50(4):1029.

Harley, C. B. and Reynolds, R. P. (1987). Analysis of E. coli promoter sequences. Nucleic Acids Research,

15(5):2343–2361.

Hoﬀ, P. D. (2007). Extending the rank likelihood for semiparametric copula estimation. The Annals of

Applied Statistics, 1(1):265–283.

Hsu, D. and Kakade, S. M. (2013). Learning mixtures of spherical Gaussians: Moment methods and

spectral decompositions. In Proceedings of the 4th conference on Innovations in Theoretical Computer

Science, pages 11–20. ACM.

J¨oreskog, K. G. and S¨orbom, D. (1987). New developments in LISREL. Paper presented at the National

Symposium on Methodological Issues in Causal Modeling, University of Alabama, Tuscaloosa.

Kiers, H. A. (2000). Towards a standardized notation and terminology in multiway analysis. Journal of

chemometrics, 14(3):105–122.

Lichman, M. (2013). UCI machine learning repository.

Moustaki, I. and Knott, M. (2000). Generalized latent trait models. Psychometrika, 65(3):391–411.

Murray, J. S., Dunson, D. B., Carin, L., and Lucas, J. E. (2013). Bayesian Gaussian copula factor models

for mixed data. Journal of the American Statistical Association, 108(502):656–665.

Muth´en, B. (1984). A general structural equation model with dichotomous, ordered categorical, and

continuous latent variable indicators. Psychometrika, 49(1):115–132.

Pritchard, J. K., Stephens, M., and Donnelly, P. (2000a). Inference of population structure using multi-

locus genotype data. Genetics, 155(2):945–959.

Pritchard, J. K., Stephens, M., Rosenberg, N. A., and Donnelly, P. (2000b). Association mapping in

structured populations. The American Journal of Human Genetics, 67(1):170–181.

Quinn, K. M. (2004). Bayesian factor analysis for mixed ordinal and continuous responses. Political

Analysis, 12(4):338–353.

Sammel, M. D., Ryan, L. M., and Legler, J. M. (1997). Latent variable models for mixed discrete and

continuous outcomes. Journal of the Royal Statistical Society. Series B, 59(3):667–678.

Tung, H. F. and Smola, A. J. (2014). Spectral methods for Indian buﬀet process inference. In Advances

in Neural Information Processing Systems 27, pages 1484–1492.

22

