6
1
0
2

 
r
a

M
7

 

 
 
]
E
M

.
t
a
t
s
[
 
 

1
v
9
4
0
2
0

.

3
0
6
1
:
v
i
X
r
a

Prediction of functional ARMA processes with an
Taoran Wei‚àó
Johannes Klepsch‚àó

application to traÔ¨Éc data
Claudia Kl¬®uppelberg‚àó

March 8, 2016

We study the functional ARMA(p, q) and a corresponding approximating vector

Abstract

model, based on functional PCA. We investigate the structure of the multivariate
vector process and derive conditions for the existence of a stationary solution to both
the functional and the vector model equation. We then use the stationary vector
process to predict the functional process, and compare the resulting predictor to
the functional best linear predictor proposed by [3]. We derive bounds for the error
due to dimension reduction. We conclude by applying functional ARMA processes
for the modelling and prediction of highway traÔ¨Éc data.

AMS 2010 Subject ClassiÔ¨Åcations: primary: 62M20 secondary: 60G25
Keywords: FPCA, functional ARMA process, functional principal component analysis,
FTSA, functional time series analysis, prediction, projection, traÔ¨Éc data

Introduction

1
A macroscopic highway traÔ¨Éc model involves velocity, Ô¨Çow (number of vehicles passing a
reference point per unit of time) and density (number of vehicles on a given road segment).
The relation among these three variables can be depicted in diagrams of ‚Äúvelocity-Ô¨Çow
relation‚Äù and ‚ÄúÔ¨Çow-density relation‚Äù. The diagram of ‚ÄúÔ¨Çow-density relation‚Äù is also called
fundamental diagram of traÔ¨Éc Ô¨Çow and can be used to determine the capacity of a road
system and give guidance for inÔ¨Çow regulations or speed limits. Figures 1 and 2 depict
the ‚Äúvelocity-Ô¨Çow relation‚Äù and ‚ÄúÔ¨Çow-density relation‚Äù for traÔ¨Éc data provided by the
Autobahndirektion S¬®udbayern. At a critical traÔ¨Éc density the state of Ô¨Çow will change
from stable to unstable. In Figure 2, the critical traÔ¨Éc density for traÔ¨Éc on highway A92
in southern Bavaria is depicted.

In this paper we develop a statistical model for traÔ¨Éc data and apply it to the above
data. As can be seen from Figure 4 and 5 the data show a certain pattern over the day,
‚àóCenter for Mathematical Sciences, Technical University of Munich, 85748 Garching,

Boltzmannstrasse 3, Germany, e-mail: j.klepsch@tum.de , cklu@tum.de , taoran.wei@tum.de,
http://www.statistics.tum.de

1

Figure 1: Velocity-Ô¨Çow relation on highway A92 in Southern Bavaria. Depicted are average
velocities per 3 min versus number of vehicles within these 3 min during the period 01/01/2014
0:00 to 30/06/2014 23:59.

Figure 2: Flow-density relation for the data from Figure 1.

2

which we want to capture using tools from functional data analysis. The basic idea of
functional data analysis is to represent the very high-dimensional raw data by a random

function X(‚ãÖ), which in our case describes the traÔ¨Éc velocity over a day. In this paper

we do not focus on the transformation of discrete to functional data. For theoretical
considerations we work with data in functional form. For a sound introduction on the
transformation from vector observations to functions, we refer to [15]. We want to assess
temporal dependence between diÔ¨Äerent days; i.e., our goal is a realistic time series model
for functional data, which captures the day to day dependence. We hope that our analysis
may support short term traÔ¨Éc regulation realised in real-time by electronic devices during
the day, which could beneÔ¨Åt from a more precise day-to-day prediction.

tional ARMA(p, q) process for arbitrary orders p, q. In scalar and multivariate time series

From a statistical point of view we are mainly interested in the prediction of a func-

analysis there exist several prediction methods which can be easily implemented like the
Durbin-Levinson and the innovations algorithms (e.g see [6]). For functional time series,
[3] has proposed the functional best linear predictor for a general linear process. However,
implementation of the predictor is in general not feasible in practice, because explicit

formulas of the predictor can not be derived. The class of functional AR(p) models is
presented in [3], Chapter 8 and [14]. The AR(1) model has also been applied for the

an exception. Functional autoregressive model of Ô¨Ånite order are well studied (e.g. [3],
Chapter 3) and allow for a elaborate prediction theory. Two well known approaches are

prediction of traÔ¨Éc data in [2].

error are derived.

In [1] a prediction algorithm is proposed, which combines the idea of functional prin-
cipal component analysis (FPCA) and functional time series analysis. The basic idea is
to reduce the inÔ¨Ånite-dimensional functional data by FPCA to Ô¨Ånite-dimensional vector
data. Thus, the prediction of the functional time series is transformed to the prediction
of a multivariate time series. In [1] this algorithm is used to predict linear functional time

series, with a focus on the functional AR(1) process for which bounds for the prediction
In this paper we focus on functional ARMA(p, q) processes. In a Ô¨Årst step we obtain
tions such that the projected process follows a vector ARMA(p, q). If these assumptions
ARMA(p, q) process and assess the quality of the approximation. We then present condi-
solution. This opens the way for prediction of functional ARMA(p, q) processes and we
sense under stationarity of the functional and the vector ARMA(p, q) process. We derive

a multivariate vector process by projection of the functional process X on the linear span
of the d most important eigenfunctions of the covariance operator of X. We derive condi-

tions such that both functional and multivariate vector process have a unique stationary

discuss relevant methods. The prediction algorithm of [1] can be applied, and makes

do not hold, we show that the projected process can at least be approximated by a vector

bounds for the prediction error based on the multivariate vector process in comparison to
the functional best linear predictor derived by [3].

An extended simulation study can be found in [17], Chapter 5. It shows in particu-
lar that approximating the projection of functional ARMA processes by vector ARMA
processes is reasonable. This is seen by comparing the model Ô¨Åt based on AIC and BIC

3

criteria. The simulation study also yields a more detailed assessment of the quality of
the functional predictor obtained by an extension of the algorithm [1] for diÔ¨Äerent linear
processes.

Our paper is organised as follows. In Section 2 we introduce the necessary Hilbert
space theory and notation, which we use throughout. Here we present the Karhunen-
Lo¬¥eve Theorem and describe the FPCA based on the functional covariance operator. We
also introduce the CVP method, which is used for truncation of the functional data. In

Section 3 we turn to functional time series models with special emphasis on ARMA(p, q)
processes. Section 3.1 is devoted to stationarity conditions for the functional ARMA(p, q)

model. In Section 3.2 we study the multivariate vector process obtained by projection of
the functional process on the linear span of the d most important eigenfunctions of the
covariance operator of X. We investigate its stationarity and prove that the multivariate
vector ARMA process approximates the functional ARMA process in a natural way. Sec-

tion 4 investigates the prediction algorithm for functional ARMA(p, q) processes invoking

the multivariate vector process and compares it to the functional best linear predictor.
Finally, in Section 5 we apply our results to a traÔ¨Éc dataset of velocity measurements
from 01/01/2014 to 30/06/2014 (obtained from the Autobahndirektion S¬®udbayern) on a
highway in Southern Bavaria, Germany.

We shall often use Parseval‚Äôs equality, which ensures that for a countable orthonormal

x(t)y(t)dt,
x, eiei, y,

2 Methodology
We summarize some concepts we shall use throughout. For details and more background

see e.g. the monographs [3], [11] and [13]. Let H= L2([0, 1]) be the real separable Hilbert
space of square integrable functions x‚à∂[0, 1]‚Üí R with normx=(‚à´ 1
0 x2(s)ds)1~2 gener-
ated by the inner productx, y‚à∂=S 1
x, y‚àà L2([0, 1]) .
basis(ei)i‚ààN,
x, y‚àà H.
x, y= ‚àûQ
We denote byL the space of bounded linear operators acting on H. If not stated diÔ¨Äerently,
i=1
we take the standard operator norm deÔ¨Åned for a bounded operator Œ®‚ààL byŒ®L‚à∂=
supx‚â§1Œ®(x).
every orthonormal basis(ei)i‚ààN of H,‚àûQ
We denote byS the space of Hilbert-Schmidt operators acting on H, which is again a

A bounded linear operator Œ® is a Hilbert-Schmidt operator, if it is compact and for

separable Hilbert space equipped with the following inner product and corresponding

(2.1)

0

Œ®(ei)2<‚àû.

i=1

4

i=1

(2.2)

Hilbert-Schmidt norm,

and Œ®S‚à∂=Œ®, Œ®S=¬ø``(cid:192)‚àûQ

integrable random functions L2

Œ®(ei)2<‚àû.

Œ®1(ei), Œ®2(ei)

Œ®1, Œ®2S‚à∂= ‚àûQ
i=1
If Œ® is a Hilbert-Schmidt operator, thenŒ®L‚â§Œ®S.
LetBH be the Borel œÉ-algebra of subsets of H. All random functions are deÔ¨Åned on
some probability space(‚Ñ¶,A, P) and areA‚àíBH-measurable. Then the space of square
(‚Ñ¶,A, P) is a Hilbert space with inner product
= L2
EX, Y= E‚à´ 1
0 X(s)Y(s)ds for X, Y ‚àà L2
For X‚àà L2
t‚àà[0, 1].
¬µ(t)‚à∂= E[X(t)],
W.l.o.g. we will assume throughout that ¬µ‚â° 0.
x‚àà H.
CX‚à∂ x E[X, xX] ,
E[X(t)X(s)] x(s)ds
X(s)x(s)ds X(t)=S 1

DeÔ¨Ånition 2.1. The covariance operator CX of X acts on H and is deÔ¨Åned as

H. We call such X an H-valued random function.

CX(x)(t)= ES 1

H the functional mean of X is deÔ¨Åned as

More precisely,

(2.3)

H

H

(2.4)

(2.5)

(2.6)

0

0

resentation

x‚àà H,

CX(x)= ‚àûQ
j=1

the covariance operator CX. This is known as the Karhunen-Lo¬¥eve representation.

CX is a symmetric, non-negative deÔ¨Ånite Hilbert-Schmidt operator with spectral rep-

Œªjx, ŒΩjŒΩj,
for eigenpairs (Œªj, ŒΩj)j‚ààN, where (ŒΩj)j‚ààN is an orthonormal basis of H and (Œªj)j‚ààN is a
j=1 Œªj<‚àû. When considering eigendecom-
sequence of positive real numbers such that‚àë‚àû
positions, we will assume that the Œªj are ordered decreasingly, hence Œªi‚â• Œªk for i< k.
H can be represented as a linear combination of the eigenfunctions(ŒΩi)i‚ààZ of
Every X‚àà L2
Theorem 2.2 (Karhunen-Lo¬¥eve Theorem). Suppose X‚àà L2
where(ŒΩi)i‚ààZ are the orthonormal eigenfunctions of the covariance operator C deÔ¨Åned in
(2.5). The scalar products(X, ŒΩi)i‚ààZ have mean-zero, variance Œªi and are uncorrelated;
i.e., for all i, j‚àà Z, i‚â† j,
where(Œªi)i‚ààZ are the eigenvalues of the covariance operator CX.

EX, ŒΩi= 0, E(X, ŒΩiX, ŒΩj)= 0 and EX, ŒΩi2= Œªi,

H with EX= 0, then

X, ŒΩi ŒΩi,

X= ‚àûQ
i=1

(2.8)

(2.7)

5

The scalar products(X, ŒΩi)i‚ààZ deÔ¨Åned in (2.7) are called the scores of X. By the last
equation in (2.8), we have‚àûQ
j=1
Remark 2.3. [The CVP method] For any integer d‚àà N, we consider the largest d eigen-

Combining (2.8) and (2.9), every Œªj represents some proportion of the total variability of
X.

EX, ŒΩj2= EX2<‚àû, X‚àà L2

Œªj= ‚àûQ
j=1

values Œª1, . . . , Œªd of CX. The cumulative percentage of total variance CPV(d) is deÔ¨Åned
as

If we choose d ‚àà N such that the CPV(d) exceeds a predetermined high value, then

CPV(d)‚à∂= dQ
j=1

Œªj ‚àûQ
j=1

(2.10)

(2.9)

Œªj.

H.

X, ŒΩi ŒΩi,

denote by sp{ŒΩ1, . . . , ŒΩd}

Œª1, . . . , Œªd or the corresponding ŒΩ1, . . . , ŒΩd explain most of the variability of X. In this
context ŒΩ1, . . . , ŒΩd are called the functional principal components (FPC‚Äôs). If we project
the H-valued random function X on the Ô¨Ånite dimensional subspace of H, spanned by
the d most important eigenfunctions ŒΩ1, . . . , ŒΩd of the covariance operator of X, which we

Xd‚à∂= dQ
i=1
then it contains most of the variability of X.

3 Functional ARMA processes


In this section, we Ô¨Årst introduce the functional ARMA(p, q) equations and derive suf-
H. We approximate this Ô¨Ånite dimensional process by a suitable vector ARMA(p, q) pro-

Ô¨Åcient conditions for the equations to have a stationary and causal solution, which we
present. We then project the functional linear process on a Ô¨Ånite dimensional subspace of

(2.11)

cess, and give conditions for the stationarity of this multivariate approximation. We also
give conditions on the functional ARMA model such that the projection of the functional
process on a Ô¨Ånite dimensional space still follows an ARMA structure.

ables.

paper.

We start by deÔ¨Åning functional white noise, which will be needed throughout the

DeÔ¨Ånition 3.1. [[3], DeÔ¨Ånition 3.1] Let(Œµn)n‚ààZ be a sequence of H-valued random vari-
= CŒµ,
<‚àû, CŒµn
(i) (Œµn)n‚ààZ is H-white noise (WN) if for all n‚àà Z, EŒµn= 0, 0< EŒµn2= œÉ2
(ii)(Œµn)n‚ààZ is H-strong white noise (SWN), if for all n‚àà Z, EŒµn= 0, 0< EŒµn2= œÉ2
<‚àû
and(Œµn)n‚ààZ is i.i.d.

<‚àû. When
= œÉ2
We assume throughout that(Œµn)n‚ààZ is WN with zero mean and EŒµ2

(‚ãÖ)‚à∂= E[Œµm,‚ãÖ Œµn]= 0 for all n‚â† m.

and if CŒµn,Œµm

Œµ

Œµ

SWN is required, this will be speciÔ¨Åed.

n

Œµ

6

We will derive conditions such that (3.1) has a stationary solution. We begin with the

3.1 Stationary functional ARMA processes
Formally we can deÔ¨Åne a functional ARMA process of arbitrary order.

Theorem 3.4. Under Assumption 3.3 there exists a unique stationary and causal solution

DeÔ¨Ånition 3.2. Let(Œµn)n‚ààZ be WN as in DeÔ¨Ånition 3.1(i). Let furthermore œÜ1, . . . , œÜp,
Œ∏1, . . . , Œ∏q‚ààL. Then a solution of
Xn= pQ
œÜi(Xn‚àíi)+ qQ
Œ∏j(Œµn‚àíj)+ Œµn, n‚àà Z,
i=1
j=1
is called a functional ARMA(p, q) process.
functional ARMA(1, q) process, and need the following assumption.
Assumption 3.3. There exists some j0‚àà N such thatœÜj0L< 1.
to (3.1) for p= 1 given by
Xn= Œµn+(œÜ+ Œ∏1)Œµn‚àí1+(œÜ2+ œÜŒ∏1+ Œ∏2)Œµn‚àí2
++(œÜq‚àí1+ œÜq‚àí2Œ∏1++ Œ∏q‚àí1)Œµn‚àí(q‚àí1)
œÜj‚àíq(œÜq+ œÜq‚àí1Œ∏1++ Œ∏q)Œµn‚àíj
+ ‚àûQ
j=q
œÜq‚àíkŒ∏k)Œµt‚àíj,
œÜj‚àíq( qQ
œÜj‚àíkŒ∏k)Œµt‚àíj+ ‚àûQ
( jQ
= q‚àí1Q
j=q
j=0
k=0
k=0

where œÜ0= I denotes the identity operator in H. Furthermore, the series in (3.2) converges
Lemma 3.5 ([3], Lemma 3.1). For every œÜ‚ààL the following two conditions are equivalent:
(i) There exists an integer j0 such thatœÜj0L< 1.
(ii) There exist a> 0 and 0< b< 1 such that for every j‚â• 0,œÜjL< abj.
in [3]. First we prove the mean square convergence of the series in (3.2). Take m‚Ä≤> m‚â• q

Proof of Theorem 3.4. We follow the lines of the proof of Prop. 3.1.1 of [6] and Theorem 3.1

almost surely and in L2
H.

For the proof we need the following lemma.

and consider the truncated series

(3.1)



(3.2)

(3.3)

X

DeÔ¨Åne

(m)

n

‚à∂= Œµn+(œÜ+ Œ∏1)Œµn‚àí1+(œÜ2+ œÜŒ∏1+ Œ∏2)Œµn‚àí2
++(œÜq‚àí1+ œÜq‚àí2Œ∏1++ Œ∏q‚àí1)Œµn‚àí(q‚àí1)
+ mQ
œÜj‚àíq(œÜq+ œÜq‚àí1Œ∏1++ Œ∏q)Œµn‚àíj.
j=q
Œ≤(œÜ, Œ∏)‚à∂= œÜq+ œÜq‚àí1Œ∏1++ œÜŒ∏q‚àí1Œ∏q‚ààL.

7

Then for all m‚Ä≤> m‚â• q, using that(Œµn)n‚ààZ is WN,

(3.4)

‚àí X

(m)

n

(m‚Ä≤)

n

m‚Ä≤Q
j=m

EX

Using (3.4) we get

Since œÜ‚ààL satisÔ¨Åes Lemma 3.5(i), and equivalently (ii), we obtain

œÜj‚àíqŒ≤(œÜ, Œ∏)(Œµn‚àíj)2
2= E m‚Ä≤Q
j=m
EœÜj‚àíqŒ≤(œÜ, Œ∏)(Œµn‚àíj), œÜk‚àíq(Œ≤(œÜ, Œ∏)(Œµn‚àík)
= m‚Ä≤Q
m‚Ä≤Q
j=m
k=m
EœÜj‚àíqŒ≤(œÜ, Œ∏)(Œµn‚àíj)2
= m‚Ä≤Q
j=m
œÜj‚àíqŒ≤(œÜ, Œ∏)2L
‚â§ EŒµ02
m‚Ä≤Q
j=m
œÜj‚àíq2LŒ≤(œÜ, Œ∏)2L.
‚â§ œÉ2
m‚Ä≤Q
j=m
<‚àû.
œÜj2L< ‚àûQ
a2b2j= a2
‚àûQ
1‚àí b2
j=0
j=0
œÜj‚àíq2LŒ≤(œÜ, Œ∏)2L œÉ2
b2(j‚àíq)‚Üí 0,
‚â§Œ≤(œÜ, Œ∏)2L œÉ2
m‚Ä≤Q
j=m
œÜj‚àíqŒ≤(œÜ, Œ∏)(Œµn‚àíj)<‚àû a.s.
sure convergence we verify that‚àûQ
j=1
œÜj‚àíqLŒ≤(œÜ, Œ∏)L2
œÜj‚àíqŒ≤(œÜ, Œ∏)(Œµn‚àíj)2‚â§ ‚àûQ
EŒµ02= œÉ2
E ‚àûQ
Since
j=1
j=1
then by(3.4), we have
abj‚àíq2= œÉ2
Œ≤(œÜ, Œ∏)2L ‚àûQ
œÜj‚àíq2L2= œÉ2
Œ≤(œÜ, Œ∏)2L ‚àûQ
j=1
j=1
œÜj‚àíqŒ≤(œÜ, Œ∏)(Œµn‚àíj)2<‚àû.
E ‚àûQ
j=1
its second order stucture only depends on(Œµn)n‚ààZ, which is WN.
In order to prove that (3.2) is a solution of (3.1) with p= 1, we plug (3.2) into (3.1),
and obtain for n‚àà Z,
œÜq‚àíkŒ∏k)Œµn‚àíj

Thus we obtain a.s. convergence of the series in (3.2). Note that (3.2) is stationary, since

By the Cauchy criterion, the series in (3.2) converges in mean square. To prove almost

Œµ

Œ≤(œÜ, Œ∏)2L ‚àûQ
j=1
Œ≤(œÜ, Œ∏)2L
(1‚àí b)2

a2

œÜj‚àíq2L ,
<‚àû.

as m, m‚Ä≤‚Üí‚àû.

Xn‚àí œÜ(Xn‚àí1)= q‚àí1Q
( jQ
j=0
k=0

œÜj‚àíkŒ∏k)Œµn‚àíj+ ‚àûQ
j=q

œÜj‚àíq( qQ
k=0

œÉ2

Œµ

Hence

Œµ

Œµ

Œµ

Œµ

Œµ a2

8

(3.5)

œÜq‚àíkŒ∏k)Œµn‚àí1‚àíj.

Now notice that the second term of the right-hand side can be written as

‚àí œÜ q‚àí1Q
œÜj‚àíkŒ∏k)Œµn‚àí1‚àíj+ ‚àûQ
( jQ
œÜj‚àíq( qQ
j=0
j=q
k=0
k=0
œÜ q‚àí1Q
( jQ
œÜj‚àíkŒ∏k)Œµn‚àí1‚àíj+ ‚àûQ
œÜq‚àíkŒ∏k)Œµn‚àí1‚àíj
œÜj‚àíq( qQ
j=0
j=q
k=0
k=0
= q‚àí1Q
( jQ
œÜj+1‚àíkŒ∏k)Œµn‚àí1‚àíj+ ‚àûQ
œÜj+1‚àíq( qQ
œÜq‚àíkŒ∏k)Œµn‚àí1‚àíj
j=0
j=q
k=0
k=0
= qQ
(j‚Ä≤‚àí1Q
œÜj‚Ä≤‚àíkŒ∏k)Œµn‚àíj‚Ä≤+ ‚àûQ
œÜj‚Ä≤‚àíq( qQ
œÜq‚àíkŒ∏k)Œµn‚àíj‚Ä≤
j‚Ä≤=1
j‚Ä≤=q+1
k=0
k=0
= qQ
Œ∏j‚Ä≤)Œµn‚àíj‚Ä≤+ ‚àûQ
œÜj‚Ä≤‚àíkŒ∏k‚àí œÜj‚Ä≤‚àíj‚Ä≤
( j‚Ä≤Q
œÜq‚àíkŒ∏k)Œµn‚àíj‚Ä≤
œÜj‚Ä≤‚àíq( qQ
j‚Ä≤=1
j‚Ä≤=q+1
k=0
k=0
œÜq‚àíkŒ∏k)Œµn‚àíj‚Ä≤‚àí qQ
œÜj‚Ä≤‚àíq( qQ
œÜj‚Ä≤‚àíkŒ∏k)Œµn‚àíj‚Ä≤+ ‚àûQ
= qQ
( j‚Ä≤Q
Œ∏j‚Ä≤Œµn‚àíj‚Ä≤.
j‚Ä≤=1
j‚Ä≤=1
j‚Ä≤=q+1
k=0
k=0
œÜq‚àíkŒ∏kŒµn‚àíq+ qQ
Œ∏j‚Ä≤Œµn‚àíj‚Ä≤+ qQ
Xn‚àí œÜ(Xn‚àí1)= Œµn‚àí qQ
Hence, comparing the sums in (3.5), the only remaining terms are
j‚Ä≤=1
k=0
k=0
Œ∏j‚Ä≤Œµn‚àíj‚Ä≤, n‚àà Z,
= Œµn+ qQ
j‚Ä≤=1

œÜq‚àíkŒ∏kŒµn‚àíq

n of (3.1).

which shows that (3.2) is a solution of equation (3.1) with p= 1. Finally we prove the
uniqueness of the solution. Assume that there is another stationary solution X‚Ä≤
Iteration gives (cf. [16], eq. (4)) for all r> q,
œÜj‚àíkŒ∏k)Œµn‚àíj+ r‚àí1Q
= q‚àí1Q
( jQ
œÜj‚àíq( qQ
œÜq‚àíkŒ∏k)Œµn‚àíj
j=q
j=0
k=0
k=0
œÜr+j‚àíq( qQ
+ q‚àí1Q
œÜq‚àíkŒ∏k)Œµn‚àí(r+j)+ œÜrX‚Ä≤
n‚àír
j=0
k=j+1
Therefore, with X(r) as in (3.3), for r> q,
n 2= E q‚àí1Q
œÜq‚àíkŒ∏k)Œµn‚àí(r+j)+ œÜrX‚Ä≤
œÜr+j‚àíq( qQ
2
(r)
n‚àír
j=0
k=j+1
œÜq‚àíkŒ∏k)Œµn‚àí(r+j)2+ 2 EœÜr(X‚Ä≤
œÜr+j‚àíq( qQ
‚â§ 2E q‚àí1Q
n‚àír
j=0
k=j+1
Since(Œµn)n‚ààZ is WN, and using the linearity of the operators
œÜq‚àíkŒ∏kL2
œÜjL2( qQ
n 2‚â§ 2œÜr‚àíq2L q‚àí1Q
EX‚Ä≤
(r)
j=0
k=j+1

)2
EŒµn‚àí(r+j)2+ 2œÜr2LE(X‚Ä≤
n‚àír

EX‚Ä≤

‚àí X

‚àí X

)2

X‚Ä≤

n

n

n

9

+ qQ
j=0

p.

Xn

Yn

I

0

0

I

0

0

0

0

condition (ii) of Lemma 3.5,

EX‚Ä≤

n

‚àí X

n is equal in L2

H to the limit of X

He thus extended known results considerably.

Remark 3.6. Spangenberg [16] derived a strictly stationary, not necessarily causal so-

r‚Üí‚àû.

n 2‚Üí 0,
(r)
(r)
n , hence to Xn, which proves uniqueness.

where Œ∏0= I, and I and 0 in (3.6) denote the indentity and zero operators, respectively.

By stationarity of(Xn)n‚ààZ and(Œµn)n‚ààZ, the boundedness of œÜ and Œ∏j, j= 1, . . . , q and by

Thus X‚Ä≤
lution of a functional ARMA(p, q) equation in Banach spaces under minimal conditions.

For a functional ARMA(p, q) process we use the state space representation
√Ø√Ø√Ø√Ø√Ø
√Ø√Ø√Ø√Ø√Ø√Ø
√Ø√Ø√Ø√Ø√Ø ,
√Ø√Ø√Ø√Ø√Ø√Ø
Œµn‚àíj
Xn‚àí1‚ãÆ
0‚ãÆ
¬∑‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äö‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû¬∂
¬∑‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äö‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû¬∂
Xn‚àíp+1
0
Œ¥n‚àíj

œÜ1  œÜp‚àí1 œÜp
Œ∏j 0  0
√Ø√Ø√Ø√Ø√Ø
=√Ø√Ø√Ø√Ø√Ø√Ø
√Ø√Ø√Ø√Ø√Ø
√Ø√Ø√Ø√Ø√Ø√Ø
√Ø√Ø√Ø√Ø√Ø
√Ø√Ø√Ø√Ø√Ø√Ø
‚ãÆ
Xn‚àí1

‚ãÆ
Xn‚àí2‚ãÆ
‚ãÆ

¬∑‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äö‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû¬∂
¬∑‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äö‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû¬∂
¬∑‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äö‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû‚Äû¬∂
Xn‚àíp
ÃÉœÜ
ÃÉŒ∏j
Yn‚àí1
ÃÉŒ∏j(Œ¥n‚àí1), n‚àà Z.
Yn=ÃÉœÜ(Yn‚àí1)+ qQ
j=0
Hp‚à∂=(L2([0, 1]))p with inner product and norm given by
and xp‚à∂=x, y
We denote byL(H p) the space of bounded linear operators acting on H p, the operator
norm ofÃÉœÜ‚ààL(H p) is deÔ¨Åned byÃÉœÜL‚à∂= supxp‚â§1ÃÉœÜ(x)p.
(Œ¥n)n‚ààZ is WN in Hp. Let P1 be the projection of H p on H deÔ¨Åned as
Assumption 3.7. There exists some j0‚àà N such that ÃÉœÜ as in (3.6) satisÔ¨ÅesÃÉœÜj0L< 1.
representation of a functional ARMA(p, q) in H as a functional ARMA(1, q) in H p, we
to the functional ARMA(p, q) equations (3.1). The solution can be written as Xn= P1Yn,

Since the Xn and Œµn take values in H, Yn and Œ¥n take values in the product Hilbert space

Theorem 3.8. Under Assumption 3.7 there exists a unique stationary and causal solution

Since the proof of Theorem 3.4 works in arbitrary Hilbert spaces, using the state space

get the following theorem as a consequence of Theorem 3.4.

P1(x1, . . . , xn)= x1,

(x1, . . . , xn)‚àà H p.

x, y

p

‚à∂= pQ
j=1

xj, yj ,

(3.7)

(3.8)

We summarize this as

(3.6)

(3.9)

where Yn is the solution to the state space equation (3.7), given by

Yn= Œ¥n+(ÃÉœÜ+ÃÉŒ∏1)Œ¥n‚àí1+(ÃÉœÜ 2+ÃÉœÜÃÉŒ∏1+ÃÉŒ∏2)Œ¥n‚àí2

10

(3.10)

Furthermore, the series converges almost surely and in L2
H.

++(ÃÉœÜ q‚àí1+ÃÉœÜ q‚àí2ÃÉŒ∏1++ÃÉŒ∏q‚àí1)Œ¥n‚àí(q‚àí1)
ÃÉœÜ j‚àíq(ÃÉœÜ q+ÃÉœÜ q‚àí1ÃÉŒ∏1++ÃÉŒ∏j)Œ¥n‚àíj,
+ ‚àûQ
j=q
ÃÉœÜj‚àíkÃÉŒ∏k)Œ¥t‚àíj+ ‚àûQ
ÃÉœÜj‚àíq( qQ
ÃÉœÜq‚àíkÃÉŒ∏k)Œ¥t‚àíj
= q‚àí1Q
( jQ
j=0
j=q
k=0
k=0
where ÃÉœÜ 0 denotes the identity operator in H p and Yn, Œ¥n, ÃÉœÜ and ÃÉŒ∏j are deÔ¨Åned in (3.7).
3.2 The multivariate vector ARMA(p, q) process
We project the functional ARMA(p, q) process on a Ô¨Ånite dimensional subspace of H,
X, which we denote by sp{ŒΩ1, . . . , ŒΩd}. With the CPV-method from Remark 2.3 we choose
d‚àà N such that most of the variability of the stationary functional time series variables
of (2.11) we consider the projection on sp{ŒΩ1, . . . , ŒΩd}
Xn, ŒΩiŒΩi.
Xn,d= Psp{ŒΩ1,...,ŒΩd}Xn= dQ
i=1
Xn‚à∂=(Xn, ŒΩ1 , . . . ,Xn, ŒΩd)

can be explained by ŒΩ1, . . . , ŒΩd. Recalling the concept of functional principal components

spanned by the d most important eigenfunctions ŒΩ1, . . . , ŒΩd of the covariance operator of

In what follows, we are interested in

(3.11)

.

(3.12)

operators and their eigenelements in the case of dependent data we refer to [10].

Due to its Ô¨Ånite dimensionality Xn is isomorph to Xn,d.
Remark 3.9. We will in the following assume that the eigenfunctions of the covariance
operators are known. In practice, this is of course not the case, and the eigenfunctions
that show up in the following have to replaced by their empirical counterpart. Our results
remain unchanged, except that we need stronger assumptions on the innovation process

(Œµn)n‚ààZ to ensure consistency of the estimators. For details on the estimation of covariance

A Ô¨Årst result concerns the projection of the WN(Œµn)n‚ààZ on sp{ŒΩ1, . . . , ŒΩd}, which we
Lemma 3.10. Let(ei)i‚ààZ be an arbitrary orthonormal basis of H. For d‚àà N, we deÔ¨Åne
If(Œµn)n‚ààZ is WN as in DeÔ¨Ånition 3.1(i), then(Zn)n‚ààN is d-dimensional WN.
If(Œµn)n‚ààZ is SWN as in DeÔ¨Ånition 3.1(ii), then(Zn)n‚ààN is d-dimensional SWN.

Zn‚à∂=(Œµn, e1 , . . . ,Œµn, ed), n‚àà Z.

the d-dimensional vector process

will need throughout.

(i)
(ii)

(3.13)

11

(3.14)

Œ∏j(Œµn‚àíj), ŒΩl ,

Xn, ŒΩl=œÜ(Xn‚àí1), ŒΩl+ qQ
j=0

œÜ(ŒΩl‚Ä≤), ŒΩlXn‚àí1, ŒΩl‚Ä≤ ,
Œ∏j(ŒΩl‚Ä≤), ŒΩlŒµn‚àíj, ŒΩl‚Ä≤ .

Xn‚àí1, ŒΩl‚Ä≤ ŒΩl‚Ä≤, ŒΩl= ‚àûQ
l‚Ä≤=1
Œµn‚àíj, ŒΩl‚Ä≤ ŒΩl‚Ä≤, ŒΩl= ‚àûQ
l‚Ä≤=1

As in Section 3.1 we start with the functional ARMA(1, q) process for q‚àà N and are
interested in the dynamics of(Xn,d)n‚ààZ of (3.11) for Ô¨Åxed d‚àà N. For every l‚àà Z, using the
model equation (3.1) with p= 1, we get
l‚àà Z.
For every l we expandœÜ(Xn‚àí1), ŒΩl, using that(ŒΩl)l‚ààZ is a ONB of H
œÜ(Xn‚àí1), ŒΩl=œÜ ‚àûQ
l‚Ä≤=1
andŒ∏j(Œµn‚àíj), ŒΩl for j= 1, . . . , q as
Œ∏j(Œµn‚àíj), ŒΩl=Œ∏j ‚àûQ
l‚Ä≤=1
In order to derive the d-dimensional vector process(Xn)n‚ààZ, for notational ease, we restrict
a precise presentation to the ARMA(1, 1) model. The presentation of the ARMA(1, q)
For q= 1, with Œ∏0= I and Œ∏1= Œ∏, in matrix form (3.14) is given by
œÜ(ŒΩd+1) , ŒΩ1
Xn‚àí1, ŒΩ1
Xn, ŒΩ1

‚ãÆ
‚ãÆ
œÜ(ŒΩd+1) , ŒΩd
Xn‚àí1, ŒΩd
Xn, ŒΩd
Xn‚àí1, ŒΩd+1
Xn, ŒΩd+1
œÜ(ŒΩd+1) , ŒΩd+1
‚ãÆ
‚ãÆ

Œ∏(ŒΩd+1) , ŒΩ1
Œµn, ŒΩ1
Œµn‚àí1, ŒΩ1

‚ãÆ
‚ãÆ
Œ∏(ŒΩd+1) , ŒΩd
Œµn, ŒΩd
Œµn‚àí1, ŒΩd
Œµn, ŒΩd+1
Œµn‚àí1, ŒΩd+1
Œ∏(ŒΩd+1) , ŒΩd+1
‚ãÆ
‚ãÆ


œÜ(ŒΩ1) , ŒΩ1
‚ãÆ
œÜ(ŒΩ1) , ŒΩd
œÜ(ŒΩ1) , ŒΩd+1
‚ãÆ
Œ∏(ŒΩ1) , ŒΩ1
Œ∏(ŒΩ1) , ŒΩd
Œ∏(ŒΩ1) , ŒΩd+1

œÜ(ŒΩd) , ŒΩ1
‚ãÆ
œÜ(ŒΩd) , ŒΩd
œÜ(ŒΩd) , ŒΩd+1
‚ãÆ
Œ∏(ŒΩd) , ŒΩ1
Œ∏(ŒΩd) , ŒΩd
Œ∏(ŒΩd) , ŒΩd+1

√Ø√Ø√Ø√Ø√Ø√Ø√Ø√Ø
√Ø√Ø√Ø√Ø√Ø√Ø√Ø√Ø

model is an obvious extension.






√Ø√Ø√Ø√Ø√Ø√Ø√Ø√Ø
√Ø√Ø√Ø√Ø√Ø√Ø√Ø√Ø

√Ø√Ø√Ø√Ø√Ø√Ø√Ø√Ø√Ø
√Ø√Ø√Ø√Ø√Ø√Ø√Ø√Ø√Ø







√Ø√Ø√Ø√Ø√Ø√Ø√Ø√Ø√Ø

+

‚ãÆ
‚ãÆ
‚ãÆ
‚ãÆ

√Ø√Ø√Ø√Ø√Ø√Ø√Ø√Ø√Ø

. . .

. . .

. . .

. . .

. . .

. . .

. . .

. . .

. . .

. . .

. . .

. . .

=

+

‚ãÆ
‚ãÆ

We simplify the notation in (3.15) by summarizing vectors and matrices to

(3.15)

(3.16)

‚ãÆ
‚ãÆ

12

where

En‚àí1
E‚àû
n‚àí1

 ,

Xn
X‚àû

n

+ Œò Œò‚àû
‚ãÆ

‚ãÆ

Xn‚àí1
= Œ¶ Œ¶‚àû
+En
‚ãÆ
‚ãÆ
X‚àû
E‚àû
n‚àí1
En‚à∂=(Œµn, ŒΩ1 , . . . ,Œµn, ŒΩd)
‚à∂=(Xn, ŒΩd+1 , . . .)
X‚àû
‚à∂=(Œµn, ŒΩd+1 , . . .)
E‚àû

n

n

,

.

,

n

The operators Œ¶ and Œò in (3.16) are d√ó d matrices with entriesœÜ(ŒΩl‚Ä≤), ŒΩl andŒ∏(ŒΩl‚Ä≤), ŒΩl
in the l-th row and l‚Ä≤-th column, respectively. Œ¶‚àû and Œò‚àû are d√ó‚àû matrices with ll‚Ä≤-th
entriesœÜ(ŒΩl‚Ä≤+d), ŒΩl andŒ∏(ŒΩl‚Ä≤+d), ŒΩl, respectively.

By (3.16), we write the d-dimensional vector equation

Xn= Œ¶Xn‚àí1+ En+ ŒòEn‚àí1+ ‚àÜn‚àí1, n‚àà Z,

(3.17)

where

n‚ààZ is d-dimensional WN. Note that ‚àÜn‚àí1 in (3.18) is a d-dimensional
= ‚àûQ
vector with l-th component
l‚Ä≤=d+1

n‚àí1+ Œò‚àûE‚àû
‚àÜn‚àí1‚à∂= Œ¶‚àûX‚àû
n‚àí1.
œÜ(ŒΩl‚Ä≤), ŒΩlXn‚àí1, ŒΩl‚Ä≤+ ‚àûQ
Œ∏(ŒΩl‚Ä≤) , ŒΩlŒµn‚àí1, ŒΩl‚Ä≤ .
l‚Ä≤=d+1

By Lemma 3.10(En)
(‚àÜn‚àí1)

(3.18)

(3.19)

l

E‚àÜn‚àí12

Proof. Using (3.18) we obtain

arbitrarily small by increasing the dimension d.

Thus, the ‚Äúerror term‚Äù ‚àÜn‚àí1 depends on Xn‚àí1, and the vector process(Xn)n‚ààZ in (3.17)
is in general not a vector ARMA(1, 1) process with innovations(En)n‚ààZ. However, we can
use a vector ARMA model as an approximation to(Xn)n‚ààZ, where we can make ‚àÜn‚àí1
Lemma 3.11. Let‚ãÖ2 denote the Euclidean norm in Rd, and let the d-dimensional vector
‚àÜn‚àí1 be deÔ¨Åned as in (3.18). Then E‚àÜn‚àí12
2 is bounded and tends to 0 as d‚Üí‚àû.
2‚â§ 2EŒ¶‚àûX‚àû
2 .
n‚àí12
2+ EŒò‚àûE‚àû
n‚àí12
n‚àí12
n‚àí1+ Œò‚àûE‚àû
2= EŒ¶‚àûX‚àû
We estimate the two parts EŒ¶‚àûX‚àû
n‚àí12
2 and EŒò‚àûE‚àû
n‚àí12
(using Parseval‚Äôs identity in the third line),
n‚àí12
EŒ¶‚àûX‚àû
 ‚àûQ
2= E dQ
œÜ(ŒΩl‚Ä≤)Xn‚àí1, ŒΩl‚Ä≤, ŒΩl2
l=1
l‚Ä≤=d+1
œÜ(ŒΩl‚Ä≤)Xn‚àí1, ŒΩl‚Ä≤, ŒΩl2
 ‚àûQ
‚â§ E ‚àûQ
l‚Ä≤=d+1
l=1
Xn‚àí1, ŒΩl‚Ä≤œÜ(ŒΩl‚Ä≤)2
= E ‚àûQ
l‚Ä≤=d+1
= E ‚àûQ
Xn‚àí1, ŒΩlœÜ(ŒΩl),
Xn‚àí1, ŒΩl‚Ä≤œÜ(ŒΩl‚Ä≤).
‚àûQ
l=d+1
l‚Ä≤=d+1
Xn‚àí1, ŒΩl‚Ä≤2œÜ(ŒΩl‚Ä≤)2= ‚àûQ
n‚àí12
EŒ¶‚àûX‚àû
2‚â§ E ‚àûQ
l‚Ä≤=d+1
l‚Ä≤=d+1

By the Karhunen-Lo¬¥eve Theorem (Theorem 2.2) the scores(Xn‚àí1,l, ŒΩl)l‚ààZ are uncorre-

E(Xn‚àí1, ŒΩl‚Ä≤)2œÜ(ŒΩl‚Ä≤)2

2 separately. By (3.19), we have

lated. Thus,

(3.20)

.

13

2 is bounded and tends

(3.21)

(3.22)

2 can be obtained in exactly the same way, and we get

Œªl‚Ä≤œÜ2LŒΩl‚Ä≤2‚â§œÜ2L ‚àûQ
l‚Ä≤=d+1

where CŒµ is the covariance operator of the WN. As a covariance operator, it has Ô¨Ånite

Œªl‚Ä≤.
EŒµn‚àí1, ŒΩl‚Ä≤Œµn‚àí1, ŒΩl‚Ä≤

Since by (2.8) we have EXn‚àí1, ŒΩl‚Ä≤2= Œªl‚Ä≤, we get
E(Xn‚àí1, ŒΩl‚Ä≤)2œÜ(ŒΩl‚Ä≤)2= ‚àûQ
‚àûQ
l‚Ä≤=d+1
l‚Ä≤=d+1
The bound for EŒò‚àûE‚àû
n‚àí12
EŒò‚àûE‚àû
n‚àí12
2‚â§ ‚àûQ
EŒµn‚àí1, ŒΩl‚Ä≤2Œ∏(ŒΩl‚Ä≤)2‚â§Œ∏2L ‚àûQ
l‚Ä≤=d+1
l‚Ä≤=d+1
=Œ∏2L ‚àûQ
CŒµ(ŒΩl‚Ä≤), ŒΩl‚Ä≤,
l‚Ä≤=d+1
l‚Ä≤=d+1CŒµ(ŒΩl‚Ä≤), ŒΩl‚Ä≤‚Üí 0 for
l‚Ä≤=1CŒµ(ŒΩl‚Ä≤), ŒΩl‚Ä≤<‚àû. Hence,‚àë‚àû
nuclear operator normCŒµN ‚à∂=‚àë‚àû
d‚Üí‚àû. Combining (3.20), (3.21) and (3.22) we Ô¨Ånd that E‚àÜn‚àí12
to 0 as d‚Üí‚àû.
2 is analogous in the ARMA(1, q) case. We now sum-
The proof of bounding E‚àÜn‚àí12
marize our Ô¨Åndings in the case of a functional ARMA(1, q) process.
Theorem 3.12. Consider a functional ARMA(1, q) process such that Assumption 3.3
holds. For d‚àà N the vector process Xn‚à∂=(Xn, ŒΩ1 , . . . ,Xn, ŒΩd) has the representation
ŒòqEn‚àí1+ ‚àÜn‚àí1, n‚àà Z,
Xn= Œ¶Xn‚àí1+ En+ qQ
j=1
n‚àí1+ qQ
‚àÜn‚àí1‚à∂= Œ¶‚àûX‚àû
Œò‚àû
j En‚àíj
j=1
ŒòjEn‚àíj, n‚àà Z.
ÀáXn= Œ¶ ÀáXn‚àí1+ En+ qQ
j=1
2 is bounded and tends to 0 as d‚Üí‚àû.
œÜ(ŒΩd) , ŒΩ1
œÜ(ŒΩ1) , ŒΩ1
Œ¶=√Ø√Ø√Ø√Ø
√Ø√Ø√Ø .
œÜ(ŒΩ1) , ŒΩd
œÜ(ŒΩd) , ŒΩd

Then both the functional ARMA(1, q) process (Xn)n‚ààZ in (3.1) and the d-dimensional
vector process( ÀáXn)n‚ààZ in (3.24) have a unique stationary and causal solution.
Moreover E‚àÜn‚àí12
Proof. Recall that the d√ó d matrix Œ¶ of the vector process (3.24) (see (3.15) and (3.16))

and all quantities are deÔ¨Åned analogously to (3.12), (3.17), and (3.18). DeÔ¨Åne

In order to show that (3.24) has a stationary solution, by Theorem 11.3.1 of [6], it suÔ¨Éces

to prove that every eigenvalue Œªk of Œ¶ with corresponding eigenvector ak=(ak,1, . . . , ak,d),
k = 1, . . . , d of Œ¶ satisÔ¨Åes  Œªk < 1. Note that  Œªk < 1 is equivalent to  Œªj0
 < 1, for all

k

(3.23)

(3.24)

‚ãÆ

‚ãÆ



. . .

. . .

14

where

has representation

which Ô¨Ånishes the proof.

k,l

=ak2= 1 for all
2=‚àëd
l=1ak,ŒΩl
j0‚àà N. Let ak= ak,1ŒΩ1+‚ãÖ‚ãÖ‚ãÖ+ ak,dŒΩd, andak=‚àëd
l‚Ä≤=1œÜŒΩl‚Ä≤, ŒΩlak,l2 and,
l=1‚àëd
2 =‚àëd
1‚â§ k ‚â§ d. With the orthogonality of ŒΩ1, . . . , ŒΩd, Œ¶ak2
l=1 a2
deÔ¨Åning Ad={ŒΩ1, . . . , ŒΩd}, we calculate
PAdœÜPAdak2= dQ
œÜPAdak, ŒΩlŒΩl2
l=1
œÜ( dQ
ak,l‚Ä≤ŒΩl‚Ä≤), ŒΩl2ŒΩl2
= dQ
l=1
l‚Ä≤=1
 dQ
ak,l‚Ä≤œÜŒΩl‚Ä≤, ŒΩl2=Œ¶ak2
= dQ
l‚Ä≤=1
l=1
j0(ak)‚â§PAdœÜPAd
k ak2=Œ¶j0ak2=PAdœÜPAd

Hence, for j0 as in Assumption 3.3,

2

k

ÃÉŒ∏j(Œ¥n‚àíj), n‚àà Z,

Yn=ÃÉœÜ(Yn‚àí1)+ qQ
j=0

j0Lak‚â§œÜj0L< 1,
 Œªj0
 =Œªj0
In order to extend approximation (3.24) of a functional ARMA(1, q) process to a
functional ARMA(p, q) process we use again the state space representation (3.7) given by
where ÃÉŒ∏0 = I, Yn, ÃÉœÜ, ÃÉŒ∏ and Œ¥n are deÔ¨Åned as in Theorem 3.8 and take values in Hp =
(L2([0, 1]))p; see (3.8).
Theorem 3.13. Consider the functional ARMA(p, q) process as deÔ¨Åned in (3.1) such
that Assumption 3.7 holds. Then for d‚àà N the vector process
Xn‚à∂=(Xn, ŒΩ1 , . . . ,Xn, ŒΩd)
ŒòqEn‚àíj+ ‚àÜn‚àí1, n‚àà Z,
Œ¶iXn‚àíi+ En+ qQ
j=1
+ qQ
‚àÜn‚àí1‚à∂= pQ
Œò‚àû
i X‚àû
Œ¶‚àû
j En‚àíj
n‚àíi
j=1
i=1
ÀáXn= pQ
ŒòqEn‚àí1, n‚àà Z.
Œ¶i ÀáXn‚àíi+ En+ qQ
i=1
j=1
2 is bounded and tends to 0 as d‚Üí‚àû.

Then both the functional ARMA(p, q) process (Xn)n‚ààZ in (3.1) and the d-dimensional
vector process( ÀáXn)n‚ààZ in (3.27) have a unique stationary and causal solution.
Moreover E‚àÜn‚àí12

and all quantities are deÔ¨Åned analogously to (3.12), (3.17), and (3.18). DeÔ¨Åne

Xn= pQ
i=1

(3.25)

(3.26)

(3.27)

has representation

where

15

d

d

that

= PA‚ä•

d

Œ∏jPA‚ä•

d

= 0

d

d

(3.28)

d its orthogonal com-

d, the orthogonal com-

However, as we show next, assumptions on the moving average parameters are actually
not required. We start with a well known result that characterises vector MA processes.

We are now interested in conditions for(Xn)n‚ààZ actually following a vector ARMA(p, q)
model. A trivial condition is that the projection of œÜi and Œ∏j on A‚ä•
plement of Ad= sp{ŒΩ1, . . . , ŒΩd}, satisÔ¨Åes
PA‚ä•
œÜiPA‚ä•
for all i= 1, . . . , p and j= 1, . . . , q. Then the vector process ÀáXn‚â° Xn for all n‚àà Z.
Lemma 3.14 ([6], Proposition 3.2.1). If(Xn)n‚ààZ is a stationary vector process with au-
tocovariance function CXh,X0= E[XhX
0] with CXq,X0‚â† 0 and CXh,X0= 0 for h > q, then
(Xn)n‚ààZ is a vector MA(q).
Proposition 3.15. Denote again by Ad‚à∂= sp{ŒΩ1, . . . , ŒΩd}, and by A‚ä•
= 0 for all i= 1, . . . , p, then the d-dimensional process (Xn)n‚ààZ in
(3.26) is a vector ARMA(p, q) process.
plement. If PA‚ä•
œÜiPA‚ä•
Proof. Since œÜi for i= 1, . . . , p only acts on Ad, from (3.26) we get
Œ¶iXn‚àíi+ En+ qQ
Xn= pQ
j=1
i=1
Œ¶iXn‚àíi+ En+ qQ
= pQ
j=1
i=1
ŒòjEn‚àíj+ qQ
Yn‚à∂= En+ qQ
j=1
j=1

Hence, in order to show that(Xn)n‚ààZ follows an ARMA(p, q) process, we have to show
follows an vector MA(q) model. According to Lemma 3.14, it is suÔ¨Écient to show that
(Yn)n‚ààZ is stationary and has an appropriate autocovariance structure. DeÔ¨Åning (with
Œ∏0= I)
observe that Yn = (Yn, ŒΩ1, . . . ,Yn, ŒΩd) is isomorph to PAdYn = ‚àëd
j=1Yn, ŒΩjŒΩj for all
n‚àà Z. Hence, stationarity of(Yn)n‚ààZ immediately follows from the stationarity of(Yn)n‚ààZ.
= PAdCYh,Y0PAd.
But since(Yn)n‚ààZ is a functional MA(q) process, CYh,Y0= 0 for h > q. Due to the relation
between PAdYn and Yn, we also have CYh,Y0= 0 for h > q and, hence,(Yn)n‚ààZ is a vector
MA(q).

ŒòjEn‚àíj+ ‚àÜn‚àí1
ŒòjEn‚àíj+ qQ
n‚àíj, n‚àà Z.
j E‚àû
Œò‚àû
j=1
n‚àíj, n‚àà Z,
j E‚àû
Œò‚àû

E[PAdYhPAdY0,‚ãÖ]= PAdE[YhY0,‚ãÖ]PAd

Œ∏j(Œµn‚àíj), n‚àà Z,

Yn‚à∂= qQ
j=0

Furthermore,

16

4 Prediction of functional ARMA process

We derive the best linear predictor of a functional ARMA(p, q) process (Xn)n‚ààZ based

on X1, . . . , Xn, deÔ¨Åned as in (3.26). We then compare the vector best linear predictor to
the functional best linear predictor based on X1, . . . , Xn and show that, under regularity
conditions, the diÔ¨Äerence is bounded and tends to 0 as d tends to inÔ¨Ånity.

by

4.1 Prediction based on the vector process
In Ô¨Ånite dimensions, the concept of a best linear predictor is well studied. For a d-

dimensional stationary time series(Xn)n‚ààZ we denote the ‚Äúmatrix linear span‚Äù of X1, . . . , Xn

M1‚à∂= nQ
i=1

AniXi‚à∂ Ani are real d√ó d matrices, i= 1, . . . , n.

(4.1)
Then the vector best linear predictor ÀÜXn+1 of Xn+1 based on X1, . . . , Xn is deÔ¨Åned as the
projection of Xn+1 on M1; i.e.,

ÃÇXn+1‚à∂= PM1Xn+1.

(4.2)

(4.3)

sponding scalar product.

Its properties are given by the projection theorem (e.g. Theorem 2.3.1 of [6]) and are
summarized as follows.

Remark 4.1. Recall that‚ãÖ2 denotes the Euclidean norm in Rd and ,Rd the corre-
(i) EXn+1‚àí ÀÜXn+1, YRd= 0 for all Y‚àà M1.
2= infY‚ààM1 EYn+1‚àíY2
(ii) ÀÜXn+1 is the unique element in M1 such that EXn+1‚àí ÀÜXn+12

(iii) M1 is a linear subspace of Rd.
linear predictor of Xn+1 based on X1, . . . , Xn is the following:
Algorithm1:
(1) Select the number d of FPC‚Äôs by the CPV-method (Remark 2.3) such that most of

the data variability can be explained by ŒΩ1, . . . , ŒΩd. Compute the FPC scoresXk, ŒΩl
for l= 1, . . . , d and k= 1, . . . , n by projecting each Xk for k= 1, . . . , n on ŒΩ1, . . . , ŒΩd.

In analogy to the prediction algorithm suggested in [1], a method for Ô¨Ånding the best

2.

We summarize the scores in the vector

k= 1, . . . n.
best vector linear predictor of Xn+1 that we denote by
Xn+1, ŒΩd).

Xk‚à∂=(Xk, ŒΩ1 , . . . ,Xk, ŒΩd),
ÃÇXn+1=( Xn+1, ŒΩ1, . . . ,

(2) Now we consider the d-dimensional vectors X1, . . . , Xn. Using (4.2), we compute the

1The Ô¨Årst and the third step in the algorithm can be implemented in R with the package fda, and the

second step can be achieved with the R package mts.

17

(3) Finally, we re-transform the best vector linear predictor ÃÇXn+1 into a functional form
ÃÇXn+1 by the truncated Karhunen-Lo¬¥eve representation:

ÃÇXn+1‚à∂= Xn+1, ŒΩ1ŒΩ1+‚ãÖ‚ãÖ‚ãÖ+ Xn+1, ŒΩdŒΩd=(ŒΩ1, . . . , ŒΩd)ÃÇXn+1.

(4.4)

In [1] the resulting predictor (3) is compared to the functional best linear predictor for

functional AR(1) processes.
Our goal is to extend these results to functional ARMA(p, q) processes. However, when

moving away from autoregressive models, the best linear predictor is no longer directly
given by the process. Therefore, we start by recalling the notion of best linear predictors
in Hilbert spaces.

We introduce to our setting the functional best linear predictor ÃÇXn+1 of Xn+1 based on
4.2 Functional best linear predictor
X1, . . . , Xn proposed in [5], whose notation we also use. It is the projection of Xn+1 on a
L-closed subspaces as in DeÔ¨Ånition 1.1 in [3].
H containing X1, . . . , Xn. More formally, we use the concept of
large enough subspace of L2
DeÔ¨Ånition 4.2. Recall thatL denotes the space of bounded linear operators acting on
H. We call G anL-closed subspace (LCS) of L2

(2) If X‚àà G and g‚ààL, then gX‚àà G.

(1) G is a Hilbertian subspace of L2
H.

H, if

We deÔ¨Åne

X(n)‚à∂=(Xn, . . . , X1).

where

The functional best linear predictor ÀÜX G
G, which we write as

X(n)‚à∂=gn X(n)‚à∂ gn‚ààL(H n, H).
G‚Ä≤

By Theorem 1.8 of [3] the LCS G‚à∂= GX(n) generated by X(n) is the closure of G‚Ä≤
(4.5)
X(n),
(4.6)
n+1 of Xn+1 is deÔ¨Åned as the projection of Xn+1 on
n+1‚à∂= PGXn+1‚àà G.
n+1, Y= 0 for all Y ‚àà G.
n+1 is the unique element in G such that EXn+1‚àí ÀÜX G
‚à∂= EXn+1‚àí ÀÜX G
n+12.

(ii) ÀÜX G
(iii) The mean squared error of the functional best linear predictor ÀÜX G
by

(4.7)
Its properties are given by the projection theorem (e.g. Theorem 2.3.1 of [6]) and are

n+12= inf Y‚ààG EXn+1‚àí Y2.
n+1 will be denoted
(4.8)

summarized as follows.

Remark 4.3. (i) EXn+1‚àí ÀÜX G

ÀÜX G

œÉ2

n

18

Note that, since G‚Ä≤
n+1= gn X(n) for some gn‚ààL(H n, H), whereL(H n, H) denotes
n+1 is not
X(n) is not closed in general (cf. [5], Proposition 2.1), ÀÜX G
necessarily of the form ÀÜX G
the space of bounded linear operators from H n to H. However, the following Proposition
n+1 to be represented in terms of bounded
gives necessary and suÔ¨Écient conditions for ÀÜX G
linear operators.

n+1= sn X(n) for

We now formulate conditions for ÀÜX G

Proposition 4.5. The following statements are equivalent:

n+1 to have the representation ÀÜX G

Proposition 4.4 (Proposition 2.2, [5]). The following statements are equivalent:

(i) There exists some g0‚ààL(H n, H) such that CX(n),Xn+1
= g0CX(n).
(ii) PGXn+1= g0 X(n) for some g0‚ààL(H n, H).
some Hilbert-Schmidt operator sn from H n to H (sn‚ààS(H n, H)).
(i) There exists some s0‚ààS(H n, H) such that CX(n),Xn+1
= s0CX(n).
(ii) PGXn+1= s0 X(n) for some s0‚ààS(H n, H).
= s0CX(n).
s0‚ààS(H n, H), such that CX(n),Xn+1
Then, since CX(n),s0 X(n)= E[s0X(n)X(n),‚ãÖ]= s0CX(n), we have
CX(n),Xn+1‚àís0 X(n)= 0.
Therefore Xn+1‚àí s0(X(n))‚ä• X(n) and hence Xn+1‚àí s0(X(n))‚ä• G which gives (ii).

Proof. The proof is similar to the proof of Proposition 4.4. Assume there exists some

For the reverse, note that (ii) implies

CX(n),Xn+1‚àís0 X(n)= CX(n),Xn+1‚àíPGXn+1

= 0.
= CX(n),s0 X(n)= s0CX(n), which Ô¨Ånishes the proof.

that

applies.

We will proceed with examples of processes where Proposition 4.4 or Proposition 4.5

Hence CX(n),Xn+1
Example 4.6. Let(Xn)n‚ààZ be a stationary and invertible functional linear process, such
Xn= Œµn+ ‚àûQ
œÄj(Xn‚àíj), n‚àà Z,
j=1
j=1œÄjL<‚àû. Then there exists some l0‚ààL(H n, H)
where(Œµn)n‚ààZ is WN and œÄj‚ààL with‚àë‚àû
Proof. By Lemma 2.1 in [4] it suÔ¨Éces to show that there exists some Œ±> 0 such that
such that CX(n),Xn+1
CX(n),Xn+1
L‚â§ Œ±CX(n)L n‚àà Z.
L(H n, H), and on the left-hand side, it is the operator norm on L(H n, H n). To ease

In the above equation, the norm used for the right-hand side is the operator norm on

= l0CX(n).

the representation we use the same notation being conÔ¨Ådent that this does not lead to

19

H, by repeatedly applying the Cauchy-Schwarz inequality,

œÄjXn+1‚àíjX(n),‚ãÖ(cid:6)L
œÄjEXn+1‚àíjX(n),‚ãÖ(cid:6)L
=œÄ1, . . . , œÄnEX(n)X(n),‚ãÖ(cid:6)+Q
j>n
‚â§œÄ1, . . . , œÄnLCX(n)L+Q
œÄjLCX(n)1~2L CXn+1‚àíj
1~2L
j>n
‚â§œÄ1, . . . , œÄnLCX(n)L+Q
œÄjLCX(n)1~2L CX01~2L .
j>n

By stationarity CX0 = CXk for all k = 1, . . . , n. But CX0 is the projection of CX(n) ‚àà
L(H n, H n) on the Ô¨Årst component in the following sense: with P1 deÔ¨Åned as in (3.9),
CX0 = P1CX(n)P1. Hence, using Theorem 4.2.7 of [13], one can show that Œªj(CX0) ‚â§
Œªj(CX(n)) for j‚àà N, where Œªj(CX0) and Œªj(CX(n)) denote the j-th eigenvalues of CX0 and
CX(n), respectively. Since furthermore CX0L = Œª1(CX0) (e.g. [7], Theorem 4.9.8) and
CX(n)L= Œª1(CX(n)) we getCX0L‚â§CX(n)L, and
œÄjLCX(n)L.
EXn+1X(n),‚ãÖ(cid:6)‚â§œÄ1, . . . , œÄnL+Q
j>n
The invertibility of(Xn)n‚ààZ assures the boundedness of(œÄ1, . . . , œÄn)+‚àëj>n
Example 4.7. Let(Xn)n‚ààZ be a stationary functional AR(p) process with representation
œÜj(Xn‚àíj), n‚àà Z,
where(Œµn)n‚ààZ is WN and œÜj‚ààS are Hilbert Schmidt operators. Then for n‚â• p Proposi-
tion 4.5 applies, giving PGXn+1= s0 X(n) for some s0‚ààS.

Note that an obvious special case of Example 4.6 is a functional autoregressive process

of Ô¨Ånite order. In this case we can also apply Proposition 4.5 in an obvious way.

Xn= Œµn+ pQ
j=1

œÄj.

Proof. We immediately get

CX(n),Xn+1

(‚ãÖ)= EXn+1X(n),‚ãÖ(cid:6)= E pQ
j=1
= E(œÜ1, . . . , œÜp, 0, . . . , 0)X(n)X(n),‚ãÖ(cid:6)
= œÜCX(n)(‚ãÖ),

œÜjXn+1‚àíjX(n),‚ãÖ(cid:6)

misunderstandings. For Y, Z‚àà L2
CY,ZL‚â§CY1~2L CZ1~2L . Hence, we have
EXn+1X(n),‚ãÖ(cid:6)L=E ‚àûQ
j=1

where œÜ=(œÜ1, . . . , œÜp, 0, . . . , 0)‚ààL(H n, H). Now let(ei)i‚ààN be an orthonormal basis of H.
Then(fj)j‚ààN with f1=(e1, 0, . . . , 0), f2=(0, e1, 0, . . . , 0), . . . , fn=(0, . . . , 0, e1), fn+1=
(e2, 0, . . . , 0), fn+2 =(0, e2, 0, . . . , 0), . . . , f2n =(0, . . . , 0, e2), f2n+1 =(e3, 0, . . . , 0), . . . is
an orthonormal basis of H n and, by orthogonality of(ei)i‚ààN, we get
œÜj(ei)2= pQ
j=1
since œÜj‚ààS for every j= 1, . . . , p, which implies that œÜ‚ààS(H n, H).

œÜj(ei)2= pQ
j=1

œÜ(fj)2=Q
i‚ààN

œÜ1~2S =Q
j‚ààN

œÜj2L<‚àû,

pQ
j=1

Q
i‚ààN

20

represented as an autoregressive process of Ô¨Ånite order, where the operators in the inverse
representation are still Hilbert-Schmidt operators. Then the statement follows from the
arguments of the proof of Example 4.7.

Example 4.8. Let(Xn)n‚ààZ be a stationary functional MA(1) process
Xn= Œµn+ Œ∏(Œµn‚àí1) n‚àà Z,
where(Œµn)n‚ààZ is WN,Œ∏L< 1, Œ∏‚ààS and Œ∏ nilpotent, such thatŒ∏jL= 0 for j> j0 for
some j0‚àà N. Then for n> j0 Proposition 4.5 applies.
Proof. Since Œ∏L < 1, (Xn)n‚ààZ is invertible, and since Œ∏ is nilpotent, (Xn)n‚ààZ can be
Example 4.9. Let(Xn)n‚ààZ be a stationary functional MA(1) process
where(Œµn)n‚ààZ is WN, and denote by CŒµ the covariance operator of Œµ0. Assume thatŒ∏L<
1. If Œ∏ and CŒµ commute, then there exists an s0‚ààS such that CXn,Xn+1= s0CXn.
Proof. Stationarity of(Xn)n‚ààZ ensures that CXn,Xn+1= CX0,X1. Let Œ∏‚àó denote the adjoint
operator of Œ∏. Since Œ∏CŒµ= CŒµŒ∏, we have that CX1,X0= CX0,X1 which implies Œ∏CŒµ= CŒµŒ∏‚àó=
CŒµŒ∏. Hence, CŒµ= CX0‚àí Œ∏CŒµŒ∏‚àó= CX0‚àí Œ∏CŒµŒ∏= CX0‚àí Œ∏2CŒµ, and we get by iteration
CX1,X0= Œ∏CŒµ= Œ∏(CX0‚àí Œ∏CŒµŒ∏)=Q
(‚àíŒ∏2)j(Œ∏CX0)=(Id+ Œ∏2)‚àí1Œ∏CX0,
j‚â•0
where Id+ Œ∏2 is invertible, since Œ∏L < 1. Furthermore, since the space S of Hilbert-
operators and Œ∏‚ààS, also(Id+ Œ∏2)‚àí1Œ∏‚ààS.

Schmidt operators forms an ideal in the space of bounded linear (e.g. [8], Theorem VI.5.4.)

Xn= Œµn+ Œ∏(Œµn‚àí1) n‚àà Z,

4.3 Bounds for the error of the vector predictor
We are now ready to derive bounds for the prediction error caused by the dimension
reduction. More precisely, we want to compare the predictor

as deÔ¨Åned in (4.4), and based on the vector process, with the functional best linear pre-
dictor

Xk, ŒΩjŒΩj=(ŒΩ1, . . . , ŒΩd)ÃÇXn+1
n+1= PGXn+1,
as deÔ¨Åned in (4.7). We Ô¨Årst compare them on sp{ŒΩ1, . . . , ŒΩd} where the vector representa-
ÃÇXn+1=( Xn+1, ŒΩ1, . . . ,
and ÃÇXG
We give Assumptions under which for d‚Üí‚àû the mean squared distance between the
vector best linear predictor ÀÜXn+1 and the vector ÀÜXG

n+1, ŒΩd
n+1‚à∂= ÀÜX G
n+1 becomes arbitrarily small.

n+1, ŒΩ1 , . . . , ÀÜX G

tions are

.

(4.9)

ÃÇXn+1‚à∂= dQ
j=1

ÀÜX G

Xn+1, ŒΩd)

21

ÀÜXG

√Ø√Ø√Ø√Ø√Ø√Ø√Ø√Ø√Ø

ni

(4.10)

√Ø√Ø√Ø√Ø√Ø√Ø√Ø√Ø

(4.11)

(4.12)

‚ãÆ

. . .

. . .

i ,

such that

‚ãÆ

. . .

. . .

Using a vector representation, we can write

n+1 is given by
Xi, ŒΩl‚Ä≤ gni(ŒΩl‚Ä≤) , ŒΩl= nQ
‚àûQ
i=1
l‚Ä≤=1
gni(ŒΩd+1) , ŒΩ1
gni(ŒΩd+1) , ŒΩd

‚àûQ
l‚Ä≤=1
gni(ŒΩd) , ŒΩ1
gni(ŒΩd) , ŒΩd

‚ãÆ

‚ãÆ

gni(Xi), ŒΩl= nQ
i=1

For l= 1, . . . , d, the l-th component of ÃÇXG
 ÀÜX G
n+1, ŒΩl= nQ
Xi, ŒΩl‚Ä≤gni(ŒΩl‚Ä≤) , ŒΩl .
i=1
Xi, ŒΩ1


‚ãÆ
gni(ŒΩ1) , ŒΩ1
Xi, ŒΩd
n+1= nQ
‚ãÆ
gni(ŒΩ1) , ŒΩd
Xi, ŒΩd+1
i=1
‚ãÆ
=‚à∂ nQ
GniXi+ dQ
G‚àû
niX‚àû
i=1
i=1
where Gni is a d√ó d matrix with ll‚Ä≤-th componentgni(ŒΩl‚Ä≤), ŒΩl and G‚àû
ni is a d√ó‚àû matrix
with ll‚Ä≤-th componentgni(ŒΩd+l‚Ä≤), ŒΩl.
Moreover, for all Y ‚àà G there exist (possibly unbounded) linear operators tn1, . . . , tn,n
Similar to (4.10), we project Y ‚àà G on ŒΩ1, . . . , ŒΩd, which results in
tni(Xi), ŒΩd

tni(Xi).
Y = nQ
i=1
Y‚à∂=(Y, ŒΩ1 , . . . ,Y, ŒΩd)
= nQ
tni(Xi), ŒΩ1, . . . , nQ
i=1
i=1
‚à∂= nQ
TniXi+ nQ
T‚àû
niX‚àû
i=1
i=1
The d√ó d matrix Tni and the d√ó‚àû matrix T‚àû
Gni and G‚àû
M‚à∂=Y=(Y, ŒΩ1 , . . . ,Y, ŒΩd)‚à∂ Y ‚àà G
Observing that for all Y1‚àà M1 there exist d√ó d matrices An1, . . . , Ann such that Y1=
i=1 AniXi, one can Ô¨Ånd operators tni such that Tni= Ani, and T‚àû
= 0, which then gives
‚àën
Y1‚àà M. Hence M1‚äÜ M.
the mean squared distance E ÀÜXn+1‚àí ÀÜXG
n+12
Theorem 4.10. Suppose(Xn)n‚ààZ is a functional ARMA(p, q) process such that Assump-
n+1 be the functional best linear predictor of Xn+1 as deÔ¨Åned in (4.7)
tion 3.7 holds. Let ÀÜX G
n+1 be as deÔ¨Åned in (4.9). Let furthermore ÀÜXn+1 be the vector best linear predictor
‚àö
and let ÀÜXG
Œªl<‚àû, for all d‚àà N,
(i) In the framework of Proposition 4.4, and if‚àë‚àû
of Xn+1 based on X1, . . . , Xn as in (4.2).

l=1
E ÀÜXn+1‚àí ÀÜXG
n+12
gniL2 ‚àûQ
Œªl2<‚àû.
‚â§ 4 nQ
i=1
l=d+1
22

Now that we have introduced the notation and the setting, we are ready to compute

ni in (4.11). We denote by M the space of all Y:

ni in (4.12) are deÔ¨Åned in the same way as

(4.13)

(4.14)

i .

2.

2

(4.15)

(4.16)

(4.17)

 ‚àûQ
l=d+1

22 ‚àûQ
l=d+1

Œªl<‚àû.

We start with a technical lemma, which we shall need for the proof of the above

n+1, ŒΩlY, ŒΩj(cid:6)= 0,
EXn+1‚àí ÀÜX G
Y ‚àà G.
sl,j(x)=x, ŒΩl ŒΩj‚â§x‚â§ 1,

(ii) In the framework of Proposition 4.5, for all d‚àà N,
E ÀÜXn+1‚àí ÀÜXG
n+12
‚â§ 4 nQ
gni(ŒΩl)2 1
i=1
In both cases, E ÀÜXn+1‚àí ÀÜXG
n+12
2 tends to 0 as d‚Üí‚àû.
Lemma 4.11. Suppose(Xn)n‚ààZ is a stationary and causal functional ARMA(p, q) process
and(ŒΩl)l‚ààZ are the eigenfunctions of its covariance operator C. Then for all j, l‚àà Z
Proof. For all j, l‚àà Z we set sl,j(‚ãÖ)‚à∂=‚ãÖ, ŒΩl ŒΩj. We Ô¨Årst show that sl,j ‚ààL. Since for all
x‚àà H withx‚â§ 1,
hence sl,j‚ààL. Since G is anL-closed subspace, Y ‚àà G implies sl,j(Y)‚àà G and we get with
Remark 4.3(i) for all j, l‚àà Z,
n+1, ŒΩlY, ŒΩj(cid:6)= 0.
n+1, sl,j(Y)= EXn+1‚àí ÀÜX G
EXn+1‚àí ÀÜX G
Proof of Theorem 4.10. First, notice that both under (i) and (ii) there exist gni ‚ààL
i=1 gniXn+1‚àíi, and thatS ‚äÇL. Furthermore, recall that‚ãÖ2 denotes
n+1=‚àën
the Euclidean norm in Rd and , Rd the corresponding scalar product. Now, using the
EY, ŒΩjXn+1‚àí ÀÜX G
dQ
j=1

= EY, Xn+1‚àí nQ
niX‚àû
G‚àû
i=1
Y=(Y, ŒΩ1, . . . ,Y, ŒΩd)‚àà M.
GniXi‚àí nQ
EY1, Xn+1‚àí nQ
G‚àû
niX‚àû
i=1
i=1
= EY1,
GniXi
EY1, ÀÜXn+1‚àí nQ
nQ
Combining (4.20) and Remark 4.3(i), we have
i=1
i=1

Since (4.18) holds for all Y‚àà M and M1‚äÜ M, it especially holds for all Y1‚àà M1; i.e.,

= 0, Y1‚àà M1.
, Y1‚àà M1.


n+1,Y, ŒΩl ŒΩj= EXn+1‚àí ÀÜX G

n+1 in (4.11) and Lemma 4.11, we obtain
n+1, ŒΩj= EY, Xn+1‚àí ÀÜXG
n+1
= 0, Y ‚àà G,
GniXi‚àí nQ
i=1

Rd

Rd

i

matrix representation of ÀÜXG

G‚àû
niX‚àû

i

Rd

where we have set

(4.18)

(4.19)

(4.20)

such that ÀÜX G



i

Rd

Rd

23

2

Theorem.

(4.21)

and for the right hand side of (4.23) we get by the Cauchy-Schwarz inequality applied
twice,

Dividing the right hand side of (4.24) by the Ô¨Årst square root on the right hand side of
(4.25) we obtain

(4.22)



Rd

.

(4.23)

,

(4.24)

i

i

2

 1

2

2

2

. (4.25)

(4.26)

(4.27)

Since both ÀÜXn+1 and n‚àë
i=1

Rd

Rd

2

2



Rd

2

i

GniXi,

GniXi,

G‚àû
niX‚àû

i

GniXi are in M1, (4.21) especially holds, when

E ÀÜXn+1‚àí nQ
i=1

We plug Y1 as deÔ¨Åned in (4.22) in (4.21) and obtain

GniXi, ÀÜXn+1‚àí nQ
E ÀÜXn+1‚àí nQ
i=1
i=1
E ÀÜXn+1‚àí nQ
i=1
nQ
i=1

GniXi‚àà M.
Y1= ÀÜXn+1‚àí nQ
i=1
= E ÀÜXn+1‚àí nQ
GniXi
nQ
G‚àû
niX‚àû
i=1
i=1
GniXi2
= E ÀÜXn+1‚àí nQ
GniXi
GniXi, ÀÜXn+1‚àí nQ
From the left hand side of (4.23) we read oÔ¨Ä
i=1
i=1
GniXi
‚â§ E ÀÜXn+1‚àí nQ
i=1
GniXi2
‚â§E ÀÜXn+1‚àí nQ
i=1
‚â§ E nQ
GniXi2
E ÀÜXn+1‚àí nQ
G‚àû
niX‚àû
i=1
i=1
= E ÀÜXn+1‚àí nQ
GniXi‚àí nQ
E ÀÜXn+1‚àí ÀÜXG
n+12
Hence, for the mean squared distance we get
G‚àû
niX‚àû
i=1
i=1
‚â§ 2E ÀÜXn+1‚àí nQ
GniXi2
+ 2E nQ
i=1
i=1
‚â§ 4E nQ
2
G‚àû
niX‚àû
i=1
What remains to do is to bound n‚àë
G‚àû
niX‚àû
i=1
Xi, ŒΩl‚Ä≤gni(ŒΩ‚Ä≤
), ŒΩl= nQ
i=1
‚àûQ
l‚Ä≤=d+1

 nQ


G‚àû
niX‚àû
i=1
2E nQ
 1
G‚àû
niX‚àû
i=1
2

nQ
‚àûQ
with l-th component
i=1
l‚Ä≤=d+1
E nQ
i=1

calculate

‚àûQ
l‚Ä≤=d+1
xi,l‚Ä≤gni(ŒΩ‚Ä≤

2
G‚àû
niX‚àû

2

i

xi,l‚Ä≤gni(ŒΩ‚Ä≤

= E dQ
l=1

), ŒΩl2

G‚àû
niX‚àû

i

), ŒΩl.

2

2

2

2

i

.

2

i

.

2

i

2

2

 nQ
i=1
24

2

l

i , which, by (4.11), is a d-dimensional vector

(i) First we consider the framework of Proposition 4.4. We abbreviate xi,l‚Ä≤‚à∂=Xi, ŒΩ‚Ä≤

l

l

 and

l

by Parseval‚Äôs identity. Then we proceed using the linearity and orthogonality of ŒΩl and
the Cauchy-Schwarz inequality

(4.28)

(4.29)

‚àö
Œªl<
i=1 gniXn+1‚àíi.

gn,j(ŒΩl‚Ä≤)2 1
2

(4.30)

l

l

l

xi,l‚Ä≤gni(ŒΩ‚Ä≤
xi,l‚Ä≤gni(ŒΩ‚Ä≤
)2

l

), ŒΩlŒΩl2
), ŒΩlŒΩl2

 nQ
= E dQ
‚àûQ
i=1
l‚Ä≤=d+1
l=1
‚â§ E ‚àûQ
 nQ
‚àûQ
i=1
l=1
l‚Ä≤=d+1
=E nQ
xi,l‚Ä≤gni(ŒΩ‚Ä≤
‚àûQ
i=1
l‚Ä≤=d+1
= E nQ
xi,lgni(ŒΩl),
xj,l‚Ä≤gn,j(ŒΩ‚Ä≤
)
‚àûQ
‚àûQ
nQ
i=1
j=1
l=d+1
l‚Ä≤=d+1
= nQ
E(xi,lxj,l‚Ä≤)gni(ŒΩl), gn,j(ŒΩl‚Ä≤)
‚àûQ


i,j=1
l,l‚Ä≤=d+1
E(xj,l‚Ä≤)2gn,j(ŒΩl‚Ä≤)
E(xi,l)2gni(ŒΩl) nQ
‚â§ nQ
‚àûQ
‚àûQ


j=1
i=1
l‚Ä≤=d+1
l=d+1
Œªlgni(ŒΩl) nQ
Œªl‚Ä≤gn,j(ŒΩl‚Ä≤),
= nQ
‚àûQ
‚àûQ
j=1
i=1
l‚Ä≤=d+1
l=d+1
since EXi, ŒΩl2= Œªl by (2.8). Then using the linearity of the operators


ŒªlgniLŒΩl nQ
‚â§ nQ
Œªl‚Ä≤gniLŒΩl‚Ä≤
‚àûQ
‚àûQ


i=1
i=1
l‚Ä≤=d+1
l=d+1
Œªl‚Ä≤gniL
ŒªlgniL nQ
= nQ
‚àûQ
‚àûQ

i=1
i=1
l‚Ä≤=d+1
l=d+1
Œªl2
= nQ
gniL2 ‚àûQ
i=1
l=d+1
sinceŒΩl= 1. Now since gni‚ààL, we have‚àën
i=1gniL<‚àû for all n‚àà N and with‚àë‚àû
‚àû, the right hand side tends to 0 as d‚Üí‚àû.
l=1
n+1=‚àën
(ii) In the framework of Proposition 4.5 there exist gni‚ààS such that ÀÜX G


Œªl‚Ä≤gn,j(ŒΩl‚Ä≤)
Œªlgni(ŒΩl) nQ
E nQ
‚àûQ
‚àûQ
j=1
i=1
l=d+1
l‚Ä≤=d+1
Œªl 1
 ‚àûQ
Œªl‚Ä≤ 1
 ‚àûQ
2 nQ
gni(ŒΩl)2 1
2 ‚àûQ
2 ‚àûQ
j=1
l=d+1
l‚Ä≤=d+1
l=d+1
l‚Ä≤=d+1
22 ‚àûQ
gni(ŒΩl)2 1
 ‚àûQ
l=d+1
l=d+1
gni(ŒΩl)2‚â§gniS<‚àû.
‚àûQ
l=d+1
gni 1
22 ‚àûQ
gni(ŒΩl)2 1
2S2 ‚àûQ
l=d+1
l=d+1
25

 ‚àûQ
Thus, (4.30) is bounded by
l=d+1

Then, similarly as before, using the Cauchy-Schwarz inequality, we calculate

‚â§ nQ
i=1
‚â§ nQ
i=1
= nQ
i=1

Œªl‚â§ nQ
i=1

Œªl<‚àû,

G‚àû
niX‚àû

i

2

2

Now note that

 nQ
i=1

,

Œªl

n

(4.31)

(3.1). Then for œÉ2

n as deÔ¨Åned in (4.8),

Proof. First note that by orthogonality of ŒΩ1, . . . , ŒΩd,

+ Œ≥d;n.
‚àö
Œªl<‚àû, for all d‚àà N,
Œªl2+ ‚àûQ
l=d+1

Œªl.


such that (4.30) tends to 0 as d‚Üí‚àû.
We are now ready to provide bounds of the mean squared prediction error EXn+1‚àí ÀÜXn+12.
Theorem 4.12. Consider a stationary and causal functional ARMA(p, q) process as in
EXn+1‚àí ÀÜXn+12‚â§ œÉ2
(i) In the framework of Proposition 4.4, and if‚àë‚àû

l=1
Œ≥d;n= 4 nQ
gniL2 ‚àûQ
i=1
l=d+1
(ii) In the framework of Proposition 4.5, for all d‚àà N,
Œªl4 gn;d+ 1
Œ≥d;n= ‚àûQ
l=d+1
gni(ŒΩl)21~2‚â§ nQ
 ‚àûQ
gn;d= nQ
i=1
i=1
l=d+1
In both cases, EXn+1‚àí ÀÜXn+12
n as d‚Üí‚àû.
2 tends to œÉ2
EXn+1‚àí ÀÜXn+12= E dQ
Xn+1‚àí ÀÜXn+1, ŒΩlŒΩl+ ‚àûQ
Xn+1, ŒΩlŒΩl2
l=1
l=d+1
EXn+1‚àí ÀÜXn+1, ŒΩlŒΩl2+ ‚àûQ
= dQ
EXn+1, ŒΩlŒΩl2
l=1
l=d+1
= dQ
EXn+1‚àí ÀÜXn+1, ŒΩl2+ ‚àûQ
l=1
l=d+1
EXn+1‚àí ÀÜXn+1, ŒΩl2= EXn+1‚àí ÀÜXn+12
n+1, Y= 0 for all Y ‚àà G. Observing that ÀÜX G
EXn+1‚àí ÀÜX G
n+1‚àí ÀÜXn+1= 0,
n+1, ÀÜX G
n+1‚àí ÀÜXn+1, ŒΩl‚Ä≤= 0,
n+1, ŒΩl ÀÜX G
n+12
2= EXn+1‚àí ÀÜXG
2+ E ÀÜXG

by (2.8) and the fact thatŒΩl= 1 for all l‚àà N. Now recall that similarly as in the Ô¨Årst
Furthermore, by DeÔ¨Ånition 4.2 of L-closed subspaces and Remark 4.3(i) we know that
EXn+1‚àí ÀÜX G
n+1‚àí ÀÜXn+1‚àà G, we conclude that

EXn+1‚àí ÀÜX G
EXn+1‚àí ÀÜXn+12

l, l‚Ä≤‚àà N.
n+1‚àí ÀÜXn+12

2,

and, by Lemma 4.11,

Hence,

equation of (4.28)

dQ
l=1

gni2S.

2.

Œªl

(4.32)

26

(4.33)

n+12
EXn+1‚àí ÀÜXG
2= E

n+1, ŒΩl2‚â§ ‚àûQ
where for the Ô¨Årst term of the right hand side,
l=1

Xn+1‚àí ÀÜX G
dQ
l=1

n,
(4.34)
and the last equality holds by Remark 4.3(iii). For the second term of the right hand side
of (4.33) we use Theorem 4.10. We Ô¨Ånish the proof of both (i) and (ii) by plugging (4.33)
and (4.34) into (4.32).

Xn+1‚àí ÀÜX G

n+1, ŒΩl2= EXn+1‚àí ÀÜX G

n+12= œÉ2

Figure 3: Functional velocity data (black) and raw data (grey) on the last ten working days in
June 2014 (June 19th 2014 was a catholic holiday).

5 Real data analysis
In this section we apply the functional time series prediction theory to highway traÔ¨Éc
data provided by the Autobahndirektion S¬®udbayern, thus extending previous work by [2].
Our dataset consists of measurements at a Ô¨Åxed point on a highway (A92) in Southern
Bavaria, Germany. Recorded is the average velocity per minute from 1/1/2014 00:00 to
30/06/2014 23:59 on three lanes. After taking care of missing values and data outliers,
we merge the three lanes (using the weighted average velocity per minute). Finally, we
smooth the cleaned daily high-dimensional data, using a Fourier basis to obtain functional
data. In Figure 3 we depict the outcome on the working days of two weeks in June 2014.
For a precise description we refer to Wei [17], Chapter 6.

As can be seen in Figure 4, diÔ¨Äerent weekdays have diÔ¨Äerent mean velocity functions.
To account for the diÔ¨Äerence between weekdays, we subtract the empirical individual
daily mean from all daily data (Monday mean from Monday data, etc.). The eÔ¨Äect is
clearly visible in Figure 5. However, even after deduction of the daily mean, functional
stationarity tests [12] reject stationarity of time series. This is due to traÔ¨Éc Ô¨Çow on
weekends: Saturday and Sunday traÔ¨Éc show diÔ¨Äerent patterns than weekdays, even after
mean correction. Consequently, we restrict our investigation to working days (Monday-

Friday), resulting in a functional time series Xn for n= 1, . . . , N= 119.

27

40608010012006‚àí16(M)06‚àí17(Tu)06‚àí18(W)06‚àí20(F)06‚àí23(M)06‚àí24(Tu)06‚àí25(W)06‚àí26(Th)06‚àí27(F)06‚àí30(M)Velocity (km/h)raw datafunctional dataFigure 4: Empirical functional mean velocity on the 7 days of the week

Figure 5: Functional velocity data for 30 working days smoothed by a Fourier basis before and
after substracting the weekday mean

28

9095105115MTuWThFSaSu0:002:004:006:008:0010:0012:0014:0016:0018:0020:0022:0024:00Velocity(km/h).60801001200:002:004:006:008:0010:0012:0014:0016:0018:0020:0022:0024:00Velocity (km/h)‚àí60‚àí40‚àí200200:002:004:006:008:0010:0012:0014:0016:0018:0020:0022:0024:00Velocity (km/h)A Portmanteau test applied to Xn for n= 1, . . . , N with N = 119 working days (cf.
[9]) rejects (with a p-value as small as 10‚àí6) that the daily functional data are uncorre-
data on working days, hence the empirical version of E[(X(t)‚àí ¬µ(t))(X(s)‚àí ¬µ(s))], for
0‚â§ t, s‚â§ 1, based on 119 working days.

lated. Furthermore, the stationarity tests suggested in [12] do not reject the stationarity
assumption.

Figure 6 shows the empirical covariance kernel for the highway functional velocity

Figure 6: Empirical covariance kernel of functional velocity data on 119 working days.

As indicated by the arrows, the point(t, s)=(0, 0) is at the bottom right corner and

estimates the variance at midnight. The empirical variance over the day is represented
along the diagonal from the bottom right to the top left corner. The valleys and peaks
along the diagonal represent phases of low and high traÔ¨Éc density: for instance, the Ô¨Årst
peak represents the variance at around 05:00 a.m., where traÔ¨Éc becomes denser, since
commuting to work starts. Peaks away from the diagonal represent high dependencies
between diÔ¨Äerent time points during the day. For instance, high traÔ¨Éc density in the
early morning correlates with high traÔ¨Éc density in the late afternoon, again due to
commuting.
Remark 5.1. We want to emphasize that we have developed the prediction theory in its
natural framework of a Hilbert space, which follows from the projection theorem. This
requires only second order stationarity of all processes involved. Second order stationarity

follows from the fact that we used WN as driving process of the functional ARMA(p, q)
as driving process of the functional ARMA(p, q) equations.

equations. Consistency of the empirical estimators of e.g. the covariance operator, how-
ever, hold under strict stationarity (see e.g. [10]). Strict stationarity of our models (both
functional and vector models) follows immediately from using SWN (cf. DeÔ¨Ånition 3.1 (ii))

29

HoursHoursAll our results remain valid under this more restrictive condition of SWN driving process
with the obvious modiÔ¨Åcations.



Figure 7: Four empirical eigenfunctions of the N = 119 working days functional velocity data.

The criterion is 80%; i.e., ŒΩ1, ŒΩ2, ŒΩ3, ŒΩ4 explain together 80% of the total data variability.

Based on the empirical covariance operator with kernel represented in Figure 6 we
compute its empirical eigenvalues and eigenfunctions (cf. Figure 7) that we denote by Œªe
and ŒΩe
functional velocity data and implement the following steps.

j , for j= 1, . . . , N. We are then ready to apply the Algorithm of Section 4.1 to the
‚ÄúCPV(d) vs. d‚Äù plot we read oÔ¨Ä that d= 4 functional principal componens explain 80% of
the variability of the data. Now for each day n‚àà{1, . . . , N}, we use the Karhunen-Lo¬¥eve

(1) We apply the CPV method to the highway functional velocity data. From a

j

Theorem 2.2 and truncate the daily functional velocity curve Xn. This yields

In Figure 8 we show the (centered) functional velocity data and the corresponding trun-
cation.

 ŒΩe
j , n= 1, . . . , N= 119.
We store the d= 4 scores in the vector Xn,
1 , . . . ,Xn, ŒΩe
(2) We now Ô¨Åt diÔ¨Äerent vector ARMA(p, q) models to the multivariate vector data and

4), n= 1, . . . , N= 119.

Xn=(Xn, ŒΩe

Xn,d‚à∂= dQ
j=1

Xn, ŒΩe

j

compare the goodness of Ô¨Åt of the models by their prediction error. We summarize root
mean squared errors (RMSE) and mean absolute errors (MAE) for the diÔ¨Äerent models in
Table 5.1. To be able to evaluate the performance of the diÔ¨Äerent models, we use standard
non-parametric prediction methods from the literature in comparison. All of the linear
models signiÔ¨Åcantly outperform methods like exponential smoothing or naively predicting
with the mean of the time series. Again details are given in [17]. We Ô¨Ånd minimal errors

for VAR(2) and VMA(1) models, where both prediction errors are equal in case of the
MAE, and the RMSE for the VAR(2) model is slightly smaller than that for the VMA(1)
model. Since we opt for a parsimonious model, we choose the VMA(1) model, which we

Ô¨Åt to the data.

30

‚àí10120:002:004:006:008:0010:0012:0014:0016:0018:0020:0022:0024:00n1n2n3n4Figure 8: Functional velocity raw data on 5 consecutive working days (black) versus the truncated
data by the Karhunen-Lo¬¥eve representation (grey). The criterion is 80% and the resulting number
d of FPC‚Äôs is 4.

Using the model Ô¨Åt of the VMA(1) model, we compute the best linear predictor ÃÇXn+1

as in (4.2).

Model Ô¨Åt AR(1) AR(2) MA(1) MA(2) ARMA(1, 1)

RMSE
MAE

4.05
3.19

3.87
3.06

3.89
3.06

4.78
3.77

4.50
3.59

Table 5.1: Average prediction errors of the predictors for the last 10 observations for all working
days

(3) We re-transform the vector best linear predictorÃÇXn+1 into its functional form ÃÇXn+1,

which is depicted in Figure 9.

Figure 9: Functional velocity data in black and one-step functional predictor based on VMA(1)

in grey for the last working days in June 2014

Acknowledgement: We thank the Autobahndirektion S¬®udbayern and especially J.
Gr¬®otsch for their support und for providing the traÔ¨Éc data. J. Klepsch furthermore ac-
knowledges Ô¨Ånancial support from the Munich Center for Technology and Society based

31

‚àí20‚àí100102014‚àí04‚àí14(M)2014‚àí04‚àí15(Tu)2014‚àí04‚àí16(W)2014‚àí04‚àí17(Th)2014‚àí04‚àí18(F)2014‚àí04‚àí19(Sa)Velocity(km/h)functionaltruncated70809010012006‚àí16(M)06‚àí17(Tu)06‚àí18(W)06‚àí20(F)06‚àí23(M)06‚àí24(Tu)06‚àí25(W)06‚àí26(Th)06‚àí27(F)06‚àí30(M)Velocity (km/h)functional dataVMA(1) predictorproject ASHAD.

References
[1] A. Aue, D. Norinho, and S. H¬®ormann. On the prediction of stationary functional

time series. Journal of the American Statistical Association, 110:378‚Äì392, 2015.

[2] P. Besse and H. Cardot. Approximation spline de la pr¬¥evision d‚Äôun processus func-
tionnel autor¬¥egressive d‚Äôordre 1. Canadian Journal of Statistics, 24:467‚Äì487, 1996.

[3] D. Bosq. Linear Processes in Function Spaces: Theory and Applications. Springer

New York, 2000.

[4] D. Bosq. General linear processes in Hilbert spaces and prediction. Journal of

Statistical Planning and Inference, 137:879‚Äì894, 2007.

[5] D. Bosq. Computing the best linear predictor in a Hilbert space. Applications to

general ARMA processes. Journal of Multivariate Analysis, 124:436‚Äì450, 2014.

[6] P.J. Brockwell and R.A. Davis. Time Series: Theory and Methods (2nd Ed.).

Springer, New York, 1991.

[7] L. Debnath and P. Mikusi¬¥nski.

Academic Press, 1999.

Introduction to Hilbert Spaces with Applications.

[8] N. Dunford and J.T. Schwartz. Linear Operators Part 1 General Theory. Wiley,

1988.

[9] R. Gabrys and P. Kokoszka. Portmanteau test of independence for functional obser-

vations. Journal of the American Statistical Association, Vol. 102(No. 480), 2007.

[10] S. H¬®ormann and P. Kokoszka. Weakly dependent functional data. The Annals of

Statistics, 38(3):1845‚Äì1884, 2010.

[11] L. Horv¬¥ath and P. Kokoszka.

Springer, New York, 2012.

Inference for Functional Data with Applications.

[12] L. Horv¬¥ath, P. Kokoszka, and G. Rice. Testing stationarity of functional time series.

Journal of Econometrics, 2013.

[13] T. Hsing and R. Eubank. Theoretical Foundations of Functional Data Analysis, with

an Introduction to Linear Operators. Wiley, 2015.

[14] V. Kargin and A. Onatski. Curve forecasting by functional autoregression. Journal

of Multivariate Analysis, 99:2508‚Äì2526, 2008.

[15] J.O. Ramsay and B.W. Silverman. Functional Data Analysis. Springer, New York,

2005.

32

[16] F. Spangenberg. Strictly stationary solutions of ARMA equations in Banach spaces.

Journal of Multivariate Analysis, 121:127‚Äì138, 2013.

[17] T. Wei. Time series in functional data analysis. Master‚Äôs thesis, Technical University

of Munich, 12 2015.

33

