6
1
0
2

 
r
a

 

M
5
1

 
 
]

G
L
.
s
c
[
 
 

2
v
0
5
3
4
0

.

3
0
6
1
:
v
i
X
r
a

An optimal regret algorithm for bandit convex optimization

Elad Hazan ∗

Yuanzhi Li †

March 16, 2016

Abstract

We consider the problem of online convex optimization against an arbitrary adversary with bandit feed-
back, known as bandit convex optimization. We give the ﬁrst ˜O(
T )-regret algorithm for this setting based
on a novel application of the ellipsoid method to online learning. This bound is known to be tight up to
logarithmic factors. Our analysis introduces new tools in discrete convex geometry.

√

1 Introduction

In the setting of Bandit Convex Optimization (BCO), a learner repeatedly chooses a point in a convex decision
set. The learner then observes a loss which is equal to the value of an adversarially chosen convex loss function.
The only feedback available to the learner is the loss — a single real number. Her goal is to minimize the regret,
deﬁned to be the difference between the sum of losses incurred and the loss of the best ﬁxed decision (point in
the decision set) in hindsight.

This fundamental decision making setting is extremely general, and has been used to efﬁciently model on-
line prediction problems with limited feedback such as online routing, online ranking and ad placement, and
many others (see [8] and [17] chapter 6 for applications and a detailed survey of BCO). This generality and
importance is accompanied by signiﬁcant difﬁculties: BCO allows for an adversarially chosen cost functions,
and extremely limited information is available to the leaner in the form of a single scalar per iteration. The
extreme exploration-exploitation tradeoff common in bandit problems is accompanied by the additional chal-
lenge of polynomial time convex optimization to make this problem one of the most difﬁcult encountered in
learning theory.

As such, the setting of BCO has been extremely well studied in recent years and the state-of-the-art signif-
icantly advanced. For example, in case the adversarial cost functions are linear, efﬁcient algorithms are known
that guarantee near-optimal regret bounds [2, 9, 18]. A host of techniques have been developed to tackle the dif-
ﬁculties of partial information, exploration-exploitation and efﬁcient convex optimization. Indeed, most known
optimization and algorithmic techniques have been applied, including interior point methods [2], random walk
optimization [23], continuous multiplicative updates [13], random perturbation [6], iterative optimization meth-
ods [15] and many more.

Despite this impressive and the long lasting effort and progress, the main question of BCO remains unre-
solved: construct an efﬁcient and optimal regret algorithm for the full setting of BCO. Even the optimal regret
attainable is yet unresolved in the full adversarial setting.

√
A signiﬁcant breakthrough was recently made by [10], who show that in the oblivious setting and in the
special case of 1-dimensional BCO, O(
T ) regret is attainable. Their result is existential in nature, showing
√
that the minimax regret for the oblivious BCO setting (in which the adversary decides upon a distribution
over cost functions independently of the learners’ actions) behaves as ˜Θ(
T ). This result was very recently
extended to any dimension by [11], still with an existential bound rather than an explicit algorithm and in the
oblivious setting.

∗Princeton University, Email: ehazan@cs.princeton.edu
†Princeton University, Email: yuanzhil@cs.princeton.edu

1

In this paper we advance the state of the art in bandit convex optimization and show the following results:

√
1. We show that minimax regret for the full adversarial BCO setting is ˜Θ(

T ).

2. We give an explicit algorithm attaining this regret bound. Such an explicit algorithm was unknown

previously even for the oblivious setting.

√
3. The algorithm guarantees ˜Θ(

√
cally, the algorithm guarantees regret of ˜Θ(

T ) regret with high probability and exponentially decaying tails. Speciﬁ-

T log 1

δ ) with probability at least 1 − δ.

√

It is known that any algorithm for BCO must suffer regret Ω(

T ) in the worst case, even for oblivious
adversaries and linear cost functions. Thus, up to logarithmic factors, our results close the gap of the attainable
regret in terms of the number of iterations.

To obtain these results we introduce some new techniques into online learning, namely a novel online

variant of the ellipsoid algorithm, and deﬁne some new notions in discrete convex geometry.

What remains open? Our algorithms depend exponentially on the dimensionality of the decision set, both
in terms of regret bounds as well as in computational complexity. As of the time of writing, we do not know
whether this dependencies are tight or can be improved to be polynomial in terms of the dimension, and we
leave it as an open problem to resolve this question1.

1.1 Prior work
The best known upper bound in the regret attainable for adversarial BCO with general convex loss functions is
˜O(T 5/6) due to [15] and [21] 2. A lower bound of Ω(
T ) is folklore, even the easier full-information setting
of online convex optimization, see e.g. [17].

√

The special case of bandit linear optimization (BCO in case where the adversary is limited to using linear
losses) is signiﬁcantly simper. Informally, this is since the average of the function value on a sphere around a
center point equals the value of the function in the center, regardless of how large is the sphere. This allows for
√
very efﬁcient exploration, and was ﬁrst used by [13] to devise the Geometric Hedge algorithm that achieves an
optimal regret rate of ˜O(
T ). An efﬁcient algorithm inspired by interior point methods was later given by [2]
with the same optimal regret bound. Further improvements in terms of the dimension and other constants were
subsequently given in [9, 18].

The ﬁrst gradient-descent-based method for BCO was given by [15]. Their regret bound was subsequently
improved for various special cases of loss functions using ideas from [2]. For convex and smooth losses, [24]
attained an upper bound on the regret of of ˜O(T 2/3). This was recently improved to by [14] to ˜O(T 5/8). [3]
√
obtained a regret bound of ˜O(T 2/3) for strongly-convex losses. For the special case of strongly-convex and
smooth losses, [3] obtained a regret of ˜O(
T ) in the unconstrained case, and [19] obtain the same rate even in
the constrained cased. [25] gives a lower bound of Ω(
T ) for the setting of strongly-convex and smooth BCO.
A comprehensive survey by Bubeck and Cesa-Bianchi [8], provides a review of the bandit optimization

√

literature in both stochastic and online setting.
Another very relevant line of work is that on zero-order convex optimization. This is the setting of convex
optimization in which the only information available to the optimizer is a valuation oracle that given x ∈ K
for some convex set K ⊆ Rd, returns f (x) for some convex function f : K (cid:55)→ R (or a noisy estimate of this
number). This is considered one of the hardest areas in convex optimization (although strictly a special case of
BCO), and a signiﬁcant body of work has culminated in a polynomial time algorithm, see [12]. Recently, [4]
give a polynomial time algorithm for regret minimization in the stochastic setting of zero-order optimization,
greatly improving upon the known running times.

1In the oblivious setting [11] show that the regret behaves polynomially in the dimension. It is not clear if this result can be extended

to the adversarial setting.

2although not speciﬁed precisely to the adversarial setting, this result is implicit in these works.

2

1.2 Paper structure
In the next section we give some basic deﬁnitions and constructs that will be of use. In section 3 we survey a
natural approach, motivated by zero-order optimization, and explain why completely new tools are necessary
to apply it. We proceed to give the new mathematical constructions for discrete convex geometry in section 4.
This is followed by our main technical lemma, the discretization lemma, in section 5. We proceed to give the
new algorithm and the main result statement in section 6.

2 Preliminaries

The setting of bandit convex optimization (BCO) is a repeated game between an online learner and an adversary
(see e.g. [17] chapter 6). Iteratively, the learner makes a decision which is a point in a convex decision set,
which is a subset of Euclidean space xt ∈ K ⊆ Rd. Meanwhile, the adversary responds with an arbitrary
Lipschitz convex loss function ft : K (cid:55)→ R. The only feedback available to the learner is the loss, ft(xt) ∈ R,
and her goal is to minimize regret, deﬁned as
RT =

(cid:88)

(cid:88)

ft(x∗)

ft(xt) − min
x∗∈K

t

t

Let K ⊆ Rd be a convex compact and closed subset in Euclidean space. We denote by EK the minimal
volume enclosing ellipsoid (MVEE) in K, also known as the John ellipsoid [20, 7]. For simplicity, assume that
EK is centered at zero.

i ≤ 1}, we shall use the notation (cid:107)x(cid:107)E ≡ (cid:112)x(cid:62)(V V (cid:62))−1x

Given an ellipsoid E = {(cid:80)

to denote the (Minkowski) semi-norm deﬁned by the ellipsoid, where V is the matrix with the vectors vi’s as
columns.
John’s theorem says that if we shrink MVEE of K by a factor of 1/d, then it will be inside K. For con-
nivence, we denote by (cid:107) · (cid:107)K the norm according to 1
dEK, which is the matrix norm corresponding to the
(shrinked by factor 1/d) MVEE ellipsoid of K . To be speciﬁc, Let E be the MVEE of K,

: (cid:80)

i αivi

i α2

We use d(cid:107)x(cid:107)E inside of (cid:107)x(cid:107)E merely to insure ∀x /∈ K, (cid:107)x(cid:107)K ≥ 1, which simpliﬁes our expression.

(cid:107)x(cid:107)K = d(cid:107)x(cid:107)E = (cid:107)x(cid:107) 1
dE

Enclosing box. Denote by CK the bounding box of the ellipsoid EK, which is obtained by the box with axis
parallel to the eigenpoles of EK. The containing box CK can be computed by ﬁrst computing EK, then the
diagonal transformation of this ellipsoid into a ball, computing the minimal enclosing cube of this ball, and
performing the inverse diagonal transformation into a box.
Deﬁnition 2.1 (Minkowski Distance of a convex set). Given a convex set K ⊂ Rd and x ∈ Rd, the Minkowski
distance γ(x,K) is deﬁned as

γ(x,K) = ||x − x0||K−x0

Where x0 is the center of the MVEE of K. K− x0 denotes shifting K by −x0 (so its MVEE is centered at zero)
Deﬁnition 2.2 (Scaled set). For β > 0, deﬁne βK as the scaled set 3
βK = {y | γ(y,K) ≤ β}

Henceforth we will require a discrete representation of convex sets, which we call grids, as constructed in

Algorithm 1.
Claim 2.1. For every K ∈ Rd, grid = grid(K, α) contains at most (2dα)d many points

3According to our deﬁnition of γ, 1K ⊆ K ⊆ dK

3

Algorithm 1 construct grid
1: Input: convex set K ∈ Rd, resolution α.
dE(cid:48)
2: Compute the MVEE E(cid:48) of K. Let E = 1
3: Let A be the (unique) linear transformation such that A(E) = Bα(0) (unit ball of radius α centered at 0).
4: Let Z d = {(x1, ..., xd), xi ∈ Z} be d-dimensional integer lattice.
5: Output: grid = A−1(Z) ∩ K.

Figure 1: The property of the grid

Lemma 2.1 (Property of the grid). Let K(cid:48) ⊆ K ⊆ Rd 4 be two convex sets. For every β, γ such that β > γ > 1,
β > d, for every α ≥ 2(γ + 1)β2
d such that the following holds. Let grid = grid(βK(cid:48) ∩ K, α), then we
have:

√

1. For every x ∈ K(cid:48): ∃xg ∈ grid such that xg + γ(xg − x) ∈ 1
2. For every x /∈ K(cid:48), x ∈ K: ∃xg ∈ grid such that xg + γ

2βK(cid:48)

γ(x,K(cid:48)) (xg − x) ∈ 1

2βK(cid:48)

Proof of Lemma 2.1. Since β > d, by John’s theorem, K(cid:48) ⊂ βK(cid:48). Moreover, since we only interested in the
distance ratio, we can assume that the MVEE E(cid:48) of βK(cid:48) ∩ K is the ball centered at 0 of radius dα, and grid
dE(cid:48) = Bα(0), by John’s Theorem, we know that
are all the integer points intersected with βK(cid:48) ∩ K. Let E = 1
E ⊆ βK(cid:48) ∩ K ⊆ dE.
(0) ⊆ K(cid:48).
d, we can ﬁnd xg ∈ grid such that (cid:107)xg − z(cid:107)2 ≤ √
d.

γ+1 x. Since E = Bα(0) ⊆ βK(cid:48), we know that B α

(a). For every x ∈ K(cid:48), consider point z = γ

(z) ⊆ K(cid:48), which implies when α ≥ γβ

√

β

Therefore, B α
Therefore,

γβ

(cid:107)xg + γ(xg − x)(cid:107)2 = (cid:107) [z + γ(z − x)] + [xg − z + γ(xg − z)](cid:107)2

= ||xg − z + γ(xg − z)||2
= (γ + 1)(cid:107)xg − z(cid:107)2 ≤ (γ + 1)
√

since z + γ(z − x) = 0
√
by (cid:107)xg − z(cid:107) ≤ √
d
2β2 Bα(0) contains all points with norm α
2β2 , and in particular it contains xg +
2 βK(cid:48). With

γ(x,K(cid:48))+γ x. When β > 2γ, we know that z ∈ 1

d.

d

γ

2β2E = 1
Moreover, 1
γ(xg − x) when α ≥ 2(γ + 1)β2

2βK ⊇ 1

(b). For every x /∈ K(cid:48) but x ∈ K, take z =

same idea as (a), we can also conclude that

4We will apply the lemma to K(cid:48) being our working Ellipsoid and K being the original input convex set

B γ(x,K(cid:48) )
γ(x,K(cid:48) )+γ

α
β2

(z) ⊆ βK(cid:48) ∩ K

4

Since γ(x,K(cid:48)) ≥ 1 for x /∈ K(cid:48), we can ﬁnd xg ∈ grid be such that (cid:107)xg−z(cid:107)2 ≤ √

d when α ≥ (γ+1)β2

√

d.

Therefore,

(cid:13)(cid:13)(cid:13)(cid:13)2
(cid:21)

+

γ

γ

(cid:13)(cid:13)(cid:13)(cid:13)xg +
(cid:13)(cid:13)(cid:13)(cid:13)(cid:20)
(cid:13)(cid:13)(cid:13)(cid:13)xg − z +
(cid:18)

z +

(xg − x)

γ(x,K)

(z − x)

γ(x,K)
γ

γ(x,K)

(cid:19)

(xg − z)

=

=

(cid:21)(cid:13)(cid:13)(cid:13)(cid:13)2

(xg − z)

γ

γ(x,K)

since z + γ

(cid:19)√
γ(x,K) (z − x) = 0

xg − z +

(cid:20)
(cid:13)(cid:13)(cid:13)(cid:13)2
(cid:18) γ

√

γ
γ(x,K)
=
1 +
√
≤ (1 + γ)

d

(cid:107)xg − z(cid:107)2 ≤
since γ(x,K) ≥ 1

γ(x,K)

+ 1

d

As before, this implies that when α ≥ 2(γ + 1)β2

d, it holds that xg + γ

γ(x,K) (xg − x) ∈ 1

2βK.

2.1 Non-stochastic bandit algorithms
Deﬁne the following

(pt, vt, σt) ← A(S,{pt−1, f1:t−1})

pt: A probability distribution over the discrete set S
i=1 fi on S.
σt: Variance, such that for every x ∈ S, vt(x) − σt(x) ≤ Ft(x) ≤ vt(x) + σt(x).
For xt picking according to distribution pt, deﬁne the regret of A as:

vt: Estimation of the values of F t =(cid:80)t
(cid:88)

(cid:40)(cid:88)

(cid:41)

RT =

ft(xt) − min

x

t

ft(x)

t

The following theorem was essentially established in [5] (although the original version was stated for
gains instead of losses, and had known horizon parameter), for the algorithm called EXP3.P, which is given in
Appendix 8 for completeness:
Theorem 2.1 ([5]). Algorithm EXP3.P over N arms guarantees that with probability at least 1 − δ,

RT =

ft(xt) − min

x

ft(x)

≤ 8

T N log

T N

δ

(cid:41)

(cid:114)

(cid:40)(cid:88)

t

(cid:88)

t

3 The insufﬁciency of convex regression

Before proceeding to give the main technical contributions of this paper, we give some description of the
technical difﬁculties that are encountered and intuition as to how they are resolved.

A natural approach for BCO, and generally for online learning, is to borrow ideas from the less general
setting of stochastic zero-order optimization. Till recently, the only polynomial time algorithm for zero-order
optimization was based on the ellipsoid method [16]. Roughly speaking, the idea is to maintain a subset,
usually an ellipsoid, in space in which the minimum resides, and iteratively reduce the volume of this region
till it is ultimately found.

In order to reduce the volume of the ellipsoid one has to ﬁnd a hyperplane separating the minimum and a
large constant fraction of the current ellipsoid in terms of volume. In the stochastic case, such a hyperplane can
be found by sampling and estimating a sufﬁciently indicative region of space. A simple way to estimate the

5

underlying convex function in the stochastic setting is called convex regression (although much more time and
query-efﬁcient methods are known, e.g. [4]).
Formally, given noisy observations from a convex function f : K (cid:55)→ Rd, denoted {v(x1), ..., v(xn)}, such
that v(xi) is a random variable whose expectation is f (xi), the problem of convex regression is to create an
estimator of the value of f over the entire space which is consistent, i.e. approaches its expectation as the
number of observations increases n (cid:55)→ ∞. The methodology of convex regression proceeds by solving a
convex program to minimize the mean square error and ensuring convexity by adding gradient constraints,
formally,

n(cid:88)

i=1

min
yj ≥ yi + ∇(cid:62)

(v(xi) − yi)2
i (xj − xi)

In this convex program {∇i, yi} are variables, points xi are chosen by the algorithm designer to observe, and
v(xi) the observed values from sampling. Intuitively, there are nd + n degrees of freedom (n scalars and n
vectors in d dimensions) and O(n2) constraints, which ensures that this convex program has a unique solution
and generates a consistent estimator for the values of f w.h.p. (see [22] for more details).

The natural approach of iteratively applying convex regression to ﬁnd a separating hyperplane within an

ellipsoid algorithm fails for BCO because of the following difﬁculties:

1. The ellipsoid method was thus far not applied successfully in online learning, since the optimum is not
ﬁxed and can change in response to the algorithms’ behavior. Even within a particular ellipsoid, the
optimal strategy is not stationary.

2. Estimation using convex regression over a ﬁxed grid is insufﬁcient, since arbitrarily deep “valleys” can

hide between the grid points.

Our algorithm and analysis below indeed follows the general ellipsoidal scheme, and overcomes these

difﬁculties by:

1. The ellipsoid method is applied with an optional “restart button”. If the algorithm ﬁnds that the optimum
is not within the current ellipsoidal set, it restarts from scratch. We show that by the time this happens,
the algorithm has accumulated so much negative regret that it only helps the player. Further, inside each
ellipsoid we use the standard multiarmed bandit algorithm EXP3.P due to [5], to exploit and explore it.

2. A new estimation procedure is required to ensure that no valleys are missed. For this reason we develop
some new machinery in convex geometry and convex regression that we call the lower convex envelope of
a function. This is a convex lower bound on the original function that ensures there are no valleys missed,
and in addition needs only constant-precision grids for being consistent with the original function.
This contribution is the most technical part of the paper, as culminates in the ”discretization lemma”, and
can be skimmed at ﬁrst read.

4 Geometry of discrete convex function
4.1 Lower convex envelopes of continuous and discrete convex functions
Bandit algorithms generate a discrete set of evaluations, which we have to turn into convex functions. The
technical deﬁnitions that allow this are called lower convex envelopes (LCE), which we deﬁne below. First,
for continuous but non-convex function f, we can deﬁne the LCE denoted FLCE(f ) as the maximal convex
function that bounds f from below, or formally,

6

Deﬁnition 4.1 (Simple Lower Convex Envelope). Given a function f : K → R (not necessarily convex) where
K ⊂ Rd, the simple lower convex envelope FSLCE = SLCE(f ) : K → R is a convex function deﬁned as:

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ∃s ∈ N∗, y1, ..., ys ∈ K : ∃(λ1, ..., λs) ∈ ∆s, x =

(cid:88)

i

(cid:41)

λiyi

FSLCE(x) = min

λif (yi)

(cid:40) s(cid:88)

i=1

1

It can be seen that FSLCE is always convex, by showing for every x, y ∈ K that f ( 1

vex function any convex combination of points satisfy f ((cid:80)

2 f (x) +
2 f (y), which follows from the deﬁnition. Further, for a convex function, FSLCE(f ) = f, since for a con-
i λif (yi), and the minimum in the

i λiyi) ≤ (cid:80)

2 x + 1

2 y) ≤ 1

deﬁnition is realized at the point x itself.

For a discrete function, the SLCE is deﬁned to be the SLCE of the piecewise linear continuation.
We will henceforth need a signiﬁcant generalization of this notion, both for the setting above, and for the
setting in which the discrete function is given as a random variable - on each point in the grid we have a value
estimation and variance estimate. We ﬁrst deﬁne the minimal extension, and then the SLCE of this minimal
extension.
Deﬁnition 4.2 (Random Discrete Function). A Random Discrete Function (RDF), denoted (X, v, σ), is a
mapping f : X → R2 on a discrete domain X = {x1, ..., xk} ⊆ K ⊆ Rd, and range of values and variances
denoted {v(x), σ(x), x ∈ X} such that f (xi) = (v(xi), σ(xi)).
Deﬁnition 4.3 (Minimal Extension of a Random Discrete Function). Given a RDF (X, v, σ), we deﬁne ˜f i
K → R as

˜f i
min(x) =

h∈Rd:∀xj ,(cid:104)h,xj−xi(cid:105)≤v(xj )+σ(xj )−[v(xi)−σ(xi)]

min

{(cid:104)h, x − xi(cid:105) + [v(xi) − σ(xi)]}

The minimal extension ˜fmin(X, v, σ) is now deﬁned as

˜fmin(x) = max
i∈[k]

˜f i
min(x)

We can now deﬁne the LCE of a discrete random function

Deﬁnition 4.4 (Lower convex envelope of a random discrete function). Given a RDF (X, v, σ) over domain
X = grid ⊆ K ⊆ Rd, for the grid for K as constructed in Algorithm 1, its lower convex envelope is deﬁned to
be

FLCE(X, v, σ) = FSLCE( ˜fmin(X, v, σ))

We now address the question of computation of an LCE of a discrete function, or how to provide oracle
access to the LCE efﬁciently. The following theorem and algorithm establish the computational part of this
section, whose proof is deferred to the appendix.

min(X, v, σ) :

Algorithm 2 Fit-LCE
1: Input: RDF (X, v, σ), and a convex set K where X ⊆ K.
2: (minimal extension): Compute the minimal extension ˜fmin(X, v, σ) : CK → R (see Section 2 for deﬁnition

of the bounding box C)

3: (LCE) Compute and return FLCE = SLCE( ˜fmin).

Theorem 4.1 (LCE computation). Given a discrete random function over k points {x1, ..., xk} in a polytope
K ⊆ Rd deﬁned by N = poly(d) halfspaces, with conﬁdence intervals [v(xi) − σ(xi), v(xi) + σ(xi)] for each
point xi, then for every x ∈ K, the value FLCE(x) can be computed in time O

(cid:16)
kd2(cid:17)

To prove the running time of LCE computation, we need the following Lemma:

7

Figure 2: The minimal extension and LCE of a discrete function

Lemma 4.1 (LCE properties). The lower convex envelope (LCE) has the follow properties:

1. ˜fmin is a piece-wise linear function with kO(d2) different regions, each region is a polytope with d + 1
vertices. We denote all the vertices of all regions as v1, ..., vn where n = kO(d2), where each vi and its
value ˜fmin(vi) are computable in time kO(d2).

2.

FLCE(x) = min

λi

˜fmin(vi)

λivi = x, (λ1, ..., λn) ∈ ∆n

(cid:88)

i∈[n]

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:88)

i∈[n]



Proof. Recall the deﬁnition of ˜f i

min : K → R as

˜f i
min(x) =

h∈Rd:∀xj ,(cid:104)h,xj−xi(cid:105)≤v(xj )+σ(xj )−[v(xi)−σ(xi)]

min

{(cid:104)h, x − xi(cid:105) + [v(xi) − σ(xi)]}

d

The vector h in the above expression is the result of a linear program. Therefore, it belongs to the vertex set of
the polyhedral set given by the inequalities (cid:104)h, xj − xi(cid:105) ≤ v(xj) + σ(xj) − [v(xi) − σ(xi)], or the objective is
unbounded, a case which we can ignore since ˜fmin is ﬁnite. The number of vertices of a polyhedral set in Rd

deﬁned by k hyperplanes is bounded by(cid:0)k

(cid:1) ≤ kd.

Thus, ˜f i

min is the minimal of a ﬁnite set of linear functions at any point in space. This implies that it is
a piecewise linear function with at most kd regions. More generally, the minimum of s linear functions is a
piece-wise linear function of at most s regions, as we now prove:
Lemma 4.1. The minimum (or maximum) of s linear functions is a piecewise linear function with at most s
regions.
Proof. Let f (x) = mini∈[s] fi(x) for linear functions {fi}, the proof for maxi∈[s] fi(x) is analoguous. Con-
sider the sets Si = {x | f (x) = fi(x)}, inside which f = fi is linear. It sufﬁces to show that each Si is a
convex set, and thus each Si is a polyhedral region with at most s faces. Now suppose x1, x2 ∈ Si, we want to
(this is because fj is linear). If
argue that x3 = x1+x2
there is a j such that fj(x3) < fi(x3), then either fj(x1) < fi(x1) or fj(x2) < fi(x2), contradict to the fact
that x1, x2 ∈ Si.

∈ Si: Observe that for every j, fj(x3) = fj (x1)+fj (x2)

2

2

8

Next we consider

˜fmin(x) = max
i∈[k]

˜f i
min(x)

Recall that each ˜f i

min is piecewise linear with s = kd regions who are determined by at most s hyperplanes.
Consider regions in which all these functions are jointly linear, we would like to bound the number of such
regions. These regions are created by the hyperplanes that create the regions of the functions ˜f i
min, a total
of at most ks hyperplanes, plus N hyperplanes of the bounding polytope K. The number of regions these
hyperplanes create is at most (N + ks)2 [1]. In each such region, the functions ˜f i
min are linear, and according
to the previous lemma there are at most k sub-regions, giving a total of k× (N + ks)2 ≤ kN 2 + k3d polyhedral
regions within which the function ˜fmin is linear.

and solving a system of d equations, in overall time (N + ks)2d = kO(d2).

The vertices of these regions can be computed by taking all d intersections of the (N + ks)2 hyperplanes
2. By deﬁnition of FLCE, there exists points p1, ..., pm ∈ K and (λ1, ..., λm) ∈ ∆m such that

λipi = x

(1)

(cid:88)
vi1, ..., vid+1 such that there exists (λi1, ..., λid+1) ∈ ∆d+1 with(cid:80)

(cid:88)

˜fmin(pi),

i∈[m]

pi. Put it into Equation 1 we get the result.

FLCE(x) =

λi

i∈[m]

By part 1, ˜fmin is a piece-wise linear function, we know that for every i ∈ [m], there exists d + 1 vertices

˜fmin(vij ) = ˜fmin(pi),(cid:80)

j∈[d+1] λij

j∈[d+1] λij vij =

Having Lemma 4.1, we can calculate FLCE by ﬁrst ﬁnding vertices v1, ..., vn and then solve an LP on λi.

The algorithm runs in time kO(d2)

5 The discretization lemma

The tools for discrete convex geometry developed in the previous section, and in particular the lower convex
envelope, are culminated in the discretization lemma that shows consistency of the LCE for discrete random
functions which we prove in this section.

Informally, the discretization lemma asserts that for any value of a given RDF, the LCE has a point with
value at least as large not too far away. Convexity is crucial for this lemma to be true at all, as demonstrated in
Figure 3.

Figure 3: The LCE cannot capture global behavior for non-convex functions.

We now turn to a precise statement of this lemma and its proof:

9

LCE	. Let (X, v, σ) be a RDF on X = Z d ∩ K such that v, σ are non-negative,
Lemma 5.1 (Discretization).
moreover, for all x ∈ X, v(x) − (8d2 + 1)σ(x) ≥ 0. Assume further that there exists a convex function
F : Rd (cid:55)→ R such that for all x ∈ X, F (x) ∈ [v(x) − σ(x), v(x) + σ(x)]. Let K(cid:48) = CK be the enclosing
bounding box for K such that B
d2K(cid:48) ⊆ K ⊆ K(cid:48) 5. Deﬁne FLCE = LCE(X, v, σ) : K(cid:48) → R as in
Deﬁnition 4.4.
4K with Br(y) ⊆ K, there exists a point
y(cid:48) ∈ Br(y) with FLCE(y(cid:48)) ≥ 1

Then there exists a value r = 23d2 such that for every y ∈ 1

24d2 (0) ⊆ 4

2 F (y).

5.1 Proof intuition in one dimension
The discretization lemma is the main technical challenge of our result, and as such we ﬁrst give intuition for the
proof for one dimension, for readability purposes only, and for the special case that the input DRF is actually a
deterministic function (i.e. all variances are zero, and v(xi) = F (xi) for a convex function F . The full proof
is deferred to the appendix.

Proof. Please refer to Figure 4 for an illustration.

Figure 4: Discretization lemma in 1-d

Assume w.l.o.g. that y ∈ Z, otherwise take the nearest point. Assume w.l.o.g that f(cid:48)(y) > 0, and thus all
points x > y have value larger than F (y). Consider the discrete points {y = x−1, x0, x1, ....,}, and the value
of ˜fmin on these integer points, which by deﬁnition has to be equal to F , and thus larger than F (y).
Since F is increasing in the positive direction, we have that ˜fmin(x0) ≤ ˜fmin(x1), and by the deﬁnition of

˜fmin, the gradient from x0 to x1, implies that

∀z ≥ x1, ˜fmin(z) ≥ ˜fmin(x0)

In the open interval [x2,∞), the value of the LCE is by deﬁnition a convex combination of values ˜fmin(x)
only for points in the range x ∈ [x1,∞). Thus, the function FLCE obtains a value larger than ˜fmin(x1) ≥ F (y)
on all points within this range, which is within a distance of two from y.

The proof of the Discretization Lemma requires the following lemmas:

Lemma 5.2 (Convex cover). For every k ∈ N∗, r ∈ R∗, if k convex sets S1, ...,Sk covers a ball in Rd of radius
r, then there exists a set Si that contains a ball of radius
d3/2 K(cid:48) ⊆ K ⊆ K(cid:48) for any convex body K

5John’s theorem implies

kdd .

1

r

10

LCE	x0	x1	x2	y=x-1	fmin	gradient	lower	bound	Proof of Lemma 5.2. Consider the maximum volume contained Ellipsoid Ei of Si ∩ Br(0), we know that the
volume of Ei is at least 1/dd the volume of Si ∩ Br(0). Now, since S1, ...,Sk covers Br(0), there exists a set
Si ∩ Br(0) of volume at least 1/k fraction of the volume of Br(0). Which implies that Ei has volume at least
1/(kdd) of Br(0), note that Ei ⊆ Br(0), therefore, it contains a ball of radius
Lemma 5.3 (Approximation of polytope with integer points). Suppose a polytope Po = conv{v1, ..., vd+1} ⊆
Rd contains B4d8 (0), then there exists d + 1 integer points g1, ..., gd+1 ∈ 2

d2Po such that:

kdd .

r

i λivi = 0, then there exists (λ(cid:48)

1, ..., λ(cid:48)

d+1) ∈

1. Let (λ1, ..., λd+1) ∈ ∆d+1 be the coefﬁcient such that(cid:80)
∆d+1 such that(cid:80)
i ≤ λi ≤ 2λ(cid:48)
(cid:88)
j ∈ ∆d+1} such that λi

2. For every i ∈ [d + 1], there exists {λi

2 λ(cid:48)
igi = 0. Moreover, 1

i λ(cid:48)

i

gi = λi

ivi +

λi
jgj

j(cid:54)=i

i ≥ 1

2d2 and

Figure 5: Approximation Lemma

Proof of Lemma 5.3.
Property 1:

Let ui = 1

d2 vi. For every i ∈ [n], since B4d8(0) ⊆ conv{v1, ..., vd+1}, it holds that

Bd (ui) ⊆ conv{v1, ..., vd+1}

Therefore, we can ﬁnd integer points around ui in conv{v1, ..., vd+1}. Now, let gi be the closest integer

point to ui, which has distance at most d to ui, i.e. (cid:107)gi − ui(cid:107)2 ≤ d. Observe that

Bd6 (0) ⊆ conv{2u1, ..., 2ud+1}

2

d2Po

Which implies that for every i ∈ [d+1], Bd6/2 (ui) ⊆ conv{2u1, ..., 2ud+1}. Therefore, gi ∈ conv{2u1, ..., 2ud+1} =
Now we want to show that 0 ∈ conv{g1, ..., gd+1}.

Consider a function f : conv{u1, u2, ..., ud+1} → Rd deﬁned as: for x =(cid:80)

iui where (λ(cid:48)

2, ..., λ(cid:48)

d+1) ∈

1, λ(cid:48)

i λ(cid:48)

∆d+1:

Observe that

f (x) =

λ(cid:48)
igi

(cid:88)
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)d+1(cid:88)

i=1

i

11

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2

(cid:107)f (x) − x(cid:107)2 =

i (ui − gi)
λ(cid:48)

≤ d

Notice that for x1, x2 ∈ conv{u1, u2, ..., ud+1},

(cid:18) x1 + x2

(cid:19)

f

2

=

f (x1) + f (x2)

2

Which implies that f is a linear transformation. Moreover, Bd2 (0) ⊆ conv{u1, u2, ..., ud+1}. Therefore,
f (Bd2 (0)) = ∪x∈B
Now, we want to show that 0 ∈ f (Bd2 (0)). Suppose on the contrary 0 /∈ f (Bd2 (0)), then we know there
is a separating hyperplane going through 0 that separates 0 and f (Bd2(0)). Which implies that there is a point
g(cid:48) ∈ ∂Bd2 (0) such that

d2 (0){f (x)} is a convex set, since a linear transformation preserves convexity.

dist(g(cid:48), f (Bd2 (0))) = min
x∈f (B

d2 (0))

(cid:107)x − g(cid:48)(cid:107) ≥ d2

(cid:107)f (x) − x(cid:107)2 ≤ d. Therefore, 0 ∈ f (Bd2(0)) ⊆ conv{g1, ..., gd}.

In particular, since f (g(cid:48)) ∈ f (Bd2(0)), the above equality implies dist(g(cid:48), f (g(cid:48))) ≥ d2, in contradiction to
We proceed to argue about the coefﬁcients. Denote gi = ui +bi, and by the above (cid:107)bi(cid:107)2 ≤ d. By symmetry
Let {λ(cid:48)

2 λ(cid:48)
it sufﬁces to show that 1

i(ui + bi) = 0. Then

i λ(cid:48)
λ(cid:48)

1 ≤ λ1 ≤ 2λ(cid:48)
1.
i λ(cid:48)

i} ∈ ∆d+1 be such that(cid:80)

igi =(cid:80)
iui = −(cid:88)
(cid:88)
Since (cid:107)bi(cid:107)2 ≤ d, by the triangle inequality it holds that (cid:107)(cid:80)
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) ≤ d
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:88)
Now, (after rotation) Deﬁne b = (b1, ..., bd) = (cid:80)

λ(cid:48)
iui

i

i

i

λ(cid:48)
ibi

i λ(cid:48)

ibi(cid:107)2 ≤ d, which implies

Let H be the hyperplane going through u2, ..., ud+1. Without lost of generality, we can apply a proper
rotation (unitary transformation) to put H = {x1 = −a} for some value a > 0, where x1 denotes the ﬁrst axis.
iui and denote u1 = (a1, ..., ad). The point b is a
juj. In addition we know that c1 = −a. Thus, we can write
b1 − c1
(u1)1 − c1

convex combination of u1 and c := 1
1−λ(cid:48)
λ(cid:48)
1 as:

(cid:80)
j≥2 λ(cid:48)

b1 + a
a1 + a

λ(cid:48)
1 =

i λ(cid:48)

=

1

On the other hand, by(cid:80)

i λiui = 0, we know that

λ1 =

a

a1 + a

Note that (cid:107)b(cid:107)2 ≤ d, which implies |b1| < d. However, by assumption there is a ball centered at 0 of radius

4d6 in conv{u1, ..., ud+1}, which implies a ≥ 4d6 ≥ 4|b1|.

1 ≤ λ1 ≤ 2λ(cid:48)
1.

2 λ(cid:48)
Therefore 1
Property 2:
By symmetry, it sufﬁces to show for v1.
j=2 λ1

(cid:80)d+1
Consider a function f : conv{v1, u2, ..., ud+1} → Rd deﬁned as: for x = λ(cid:48)v1 +(cid:80)d+1

1 ≥ 1
d+1(cid:88)

j = 1 such that

there exists λ1

2d2 and λ1

g1 = λ1

1v1 +

λ1
j gj

j=2

j ≥ 0(j = 2, 3, ..., d + 1), λ1

1 +

j=2 λ(cid:48)

juj where

(λ(cid:48), λ(cid:48)

2, ..., λ(cid:48)

d+1) ∈ ∆d+1:

f (x) = λ(cid:48)vi +

λ(cid:48)
jgj

d+1(cid:88)

j=2

12

Observe that

(cid:107)f (x) − x(cid:107)2 =

Notice that for x1, x2 ∈ conv{v1, u2, ..., ud+1},

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)d+1(cid:88)
(cid:19)

j=2

=

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2

j (uj − gj)
λ(cid:48)

≤ d

f (x1) + f (x2)

2

(cid:18) x1 + x2

2

f

Which implies that f is a linear transformation. Moreover, Bd2(g1) ⊆ B2d2 (u1) ⊆ conv{v1, u2, ..., ud+1}.
d2 (g1){f (x)} is a convex set, since a linear transformation preserves convexity.
Therefore, f (Bd2(g1)) = ∪x∈B
Now, we want to show that g1 ∈ f (Bd2(g1)). Suppose on the contrary g1 /∈ f (Bd2 (g1)), then we know
there is a separating hyperplane going through g1 that separates g1 and f (Bd2(g1)). Which implies that there
is a point g(cid:48) ∈ Bd2 (g1) such that

dist(g(cid:48), f (Bd2(g1))) =

min

x∈f (B

d2 (g1))

(cid:107)x − g(cid:48)(cid:107) = d2

to (cid:107)f (x) − x(cid:107)2 ≤ d for all x ∈ conv{v1, u2, ..., ud+1}.

In particular, since f (g(cid:48)) ∈ f (Bd2(g1)), the above equality implies dist(g(cid:48), f (g(cid:48))) ≥ d2, in contradiction
Therefore, there is a point g ∈ Bd2 (g1) such that f (g) = g1, i.e. g1 can be written as

g1 = λ(cid:48)v1 +

λ(cid:48)
jgj

(λ(cid:48), λ(cid:48)

2, ..., λ(cid:48)

d+1) ∈ ∆d+1

We proceed to give a bound on the coefﬁcients. Since g1 = f (g), we know that

d+1(cid:88)

j=2

d+1(cid:88)

j=2

g = λ(cid:48)v1 +

λ(cid:48)
juj

On the other hand, observe that (since(cid:80)

j λjuj = 0 as deﬁned in Property 1)

(cid:18)

(cid:19) d+1(cid:88)

j=1

u1 =

1
d2 v1 +

1 − 1
d2

λjuj

By (cid:107)g − u1(cid:107)2 ≤ 2d2, using the same method as Property 1 we can obtain: λ(cid:48) ≥ 1
Which completes the proof.

2d2

Now we can prove the discretization Lemma. The proof goes by the following steps:

1. First, suppose the Lemma does not hold, then we can ﬁnd a large hypercube that is contained inside K(cid:48)

and has entirely small LCE compared to the value of the point y.

2. We proceed to identify the points whose ˜fmin value is associated with the LCE of the large hypercube,

these ˜fmin have small values (compare to F (y)) and span a large region.

3. We ﬁnd a simplex of d + 1 points that span a large region in which the same holds, i.e. ˜fmin value

compared to v(y).

4. Using the approximation Lemma, we ﬁnd an inner simplex of d + 1 discrete points inside the previous
simplex. These discrete points all have ˜fmin value larger than f (y) by the fact that they are inside the
ﬁrst large region.

13

5. We use the deﬁnition of ˜fmin to show that one of the vertices of the outer simplex has value of ˜fmin larger

than f (y), in contradiction to the previous observations.

Proof of Lemma 5.1.
Step 1:

Consider a point y ∈ P with Br(y) ∈ K. By convexity of F , there is a hyperplane H going y such that on
one side of the hyperplane, all the points have larger or equal F value than F (y). Therefore, there exists a point
such that for all integer points z ∈ Qr(cid:48)(y(cid:48)),
y(cid:48), a cube Qr(cid:48)(y(cid:48)) ⊂ Br(y) centered at y(cid:48) with radius r(cid:48) = r
√
F (z) ≥ F (y). Let v1, ..., v2d be the vertex of this cube.
2
can assume that for all i ∈ [2d], FLCE(vi) < 1

If there exists i ∈ [2d] such that FLCE(vi) ≥ 1

2 F (y), then we already conclude the proof. Therefore, we

2 F (y). Step 2:

By the deﬁnition of FLCE, we know that for every i ∈ [2d], there exists pi,1, ...., pi,m ∈ K(cid:48) such that

d

j λi,jpi,j, (λi,1, ..., λi,m) ∈ ∆m with

vi =(cid:80)

1
2
Moreover, by Carathodory’s theorem 6, we can make m = d + 1.
Now we get a set of (d + 1)2d many points Po = {pi,j}i∈[2d],j∈[d+1]. Consider a size d + 1 subset

λi,j ˜Fmin(pi,j) <

FLCE(vi) =

F (y)

J = {pi1,j1 , ..., pid+1,jd+1} of Po, deﬁne convex set

SJ =

x ∈ Qr(cid:48)(y(cid:48)) | ∃(λ1, ..., λd+1) ∈ ∆d+1 : x =

λs ˜Fmin(pis,js) <

1
2

F (y)

(cid:41)

(cid:40)

We claim that

This is because for every x ∈ Qr(cid:48)(y(cid:48)), there exists vi1, ..., vid+1 and λ1, ..., λd+1 ∈ ∆d+1 such that

J⊆P0,|J|=d+1

Moreover, for each vi, vi =(cid:80)

λsvis = x

s∈[d+1]

j λi,jpi,j. Therefore:

(cid:88)

(cid:91)
(cid:88)

(cid:88)

(cid:88)

(cid:88)

λspis,js,

s

s

SJ = Qr(cid:48)(y(cid:48))



(cid:88)
(cid:17)

j

make the sum only contains d + 1 such pis,j, which proves the claim.

On the other hand, (cid:80)
dkdd where k =(cid:0)(d+1)2d

d+1

s∈[d+1] λs

λs

x =

λis,jpis,j

s∈[d+1]

j λis,j ˜Fmin(pis,j)

(cid:16)(cid:80)
(cid:1) ≤ 22d2 and y(cid:48)(cid:48) is an integer point.

< 1

Step 3:
By lemma 5.2, we know that there exists J∗ such that SJ∗ contains a ball Br(cid:48)(cid:48) (y(cid:48)(cid:48)) inside Qr(cid:48)(y(cid:48)) of radius

For simplicity, we denote J∗ = {p1, ..., pd+1}. By the deﬁnition of SJ∗, there exists (λ(cid:48)(cid:48)

1 , ..., λ(cid:48)(cid:48)

d+1) ∈ ∆d+1

2 F (y). By Carathodory’s theorem, we can

r(cid:48)(cid:48) = r(cid:48)
√
2

such that

1.

(cid:88)

λ(cid:48)(cid:48)
i pi = y(cid:48)(cid:48)

6The original Carathedory’s theorem only states for convex combination of points, but the same proof can be extended to convex

functions by looking at the graph of the function

i

14

2.

(cid:88)

i

λ(cid:48)(cid:48)

i

˜Fmin(pi) <

1
2

F (y)

Step 4:Let P = conv{p1, ..., pd+1} with center y(cid:48)(cid:48). The above argument implies that Br(cid:48)(cid:48)(y(cid:48)(cid:48)) ⊆ conv{p1, ..., pd+1},

(2d)2d2 ≥ 4d8. By lemma 5.3, there exists integer points g1, ..., gd+1 ∈ 2

r

d2 P (where

when r(cid:48)(cid:48) = r(cid:48)
√
2
d2 P denotes shrink P of factor 2

dkdd ≥

2

1. y(cid:48)(cid:48) ∈ conv{g1, ..., gd+1}
2. For every i ∈ [d + 1], there exists {λi

d2 according to center y(cid:48)(cid:48)) with
(cid:88)
j ∈ ∆d+1} such that λi

gi = λi

ipi +

λi
jgj

i ≥ 1

2d2 and

d2 P ⊆ K. This
The conditions of the lemma assert that 2
implies that gi ∈ Z d ∩ K, over which the RDF is deﬁned, and we have values v(gi) and σ(gi) to construct
˜Fmin.

2K, P ⊆ K(cid:48), we know that 2

d2K(cid:48) ⊆ 1

j(cid:54)=i
2K, by y(cid:48)(cid:48) ∈ 1

Step 5: By the fact that gi = λipi

jgj and the deﬁnition of ˜Fmin, we know that

j(cid:54)=i λi

i +(cid:80)
v(gi) − σ(gi) −(cid:88)
−(cid:88)

1, ..., λ(cid:48)

(cid:88)

j(cid:54)=i

d+1) ∈ ∆d+1. By the fact that pi = 1
i pi, where λ(cid:48)(cid:48)
λ(cid:48)(cid:48)
i ≤ 2λ(cid:48)
i.

λ(cid:48)(cid:48)
j λj
λj
j

= λ(cid:48)

i
λi
i

j(cid:54)=i

i

i

λi
i

λi
j[v(gj) + σ(gj)]

y(cid:48)(cid:48) =

i


(gi −(cid:80)

Let us write y(cid:48)(cid:48) =(cid:80)

˜Fmin(pi) ≥ 1
λi
i

i λ(cid:48)

igi : (λ(cid:48)

calculate:

From Lemma 5.3 we also obtain that λ(cid:48)(cid:48)
Moreover, for the interpolation:

j(cid:54)=i λi

jgj) We can

i

i

=

(cid:88)
≥ (cid:88)
(cid:88)
(cid:88)
(cid:88)
≥ (cid:88)
(cid:88)

=

=

i

i

i

i

=

i

λ(cid:48)(cid:48)

λi
j[v(gj) + σ(gj)]

j(cid:54)=i

λ(cid:48)(cid:48)
i
λi
i

λ(cid:48)(cid:48)
i
λi
i

˜Fmin(pi)

v(gi) − σ(gi) −(cid:88)
v(gi) + σ(gi) −(cid:88)
 [v(gi) + σ(gi)]
 − 2
 λ(cid:48)(cid:48)
−(cid:88)
(cid:88)
i[v(gi) + σ(gi)] − 4d2(cid:88)

i[v(gi) + σ(gi)] − 2
λ(cid:48)
λ(cid:48)

λ(cid:48)(cid:48)
i
λi
i
λ(cid:48)
iσ(gi)

λ(cid:48)(cid:48)
j λj
λj
j

σ(gi)

i
λi
i

j(cid:54)=i

j(cid:54)=i

λi
j[v(gj) + σ(gj)]

i

i


 − 2
(cid:88)

λ(cid:48)(cid:48)
i
λi
i

i

(cid:88)

i

λ(cid:48)(cid:48)
i
λi
i

σ(gi)

σ(gi)

since λi

i ≥ 1

d2 ,λ(cid:48)(cid:48)

i ≤ 2λ(cid:48)

i

i[v(gi) − (4d2 − 1)σ(gi)]
λ(cid:48)

i

i

15

Figure 6: Depiction of the algorithm

By assumption, since gi is a integer point, we get v(gi) − (8d2 + 1)σ(gi) ≥ 0

v(gi) − (4d2 − 1)σ(gi) ≥ v(gi) + σ(gi) − 4d2σ(gi)
≥ F (gi) − 4d2σ(gi)
≥ F (gi) − v(gi) − σ(gi)
≥ 1
2

Note that by the convexity of F ,(cid:80)
(cid:88)

Thus,

F (gi)
i λ(cid:48)
λ(cid:48)(cid:48)

˜Fmin(pi) ≥(cid:88)

i

By contradiction we complete the proof.

i

i

1
2

iF (gi) ≥ 1
λ(cid:48)
2

F (y)

since by deﬁnition v(gi) + σ(gi) ≥ F (gi)

since (8d2 + 1)σ(gi) ≤ v(gi)

2
since by deﬁnition v(gi) − σ(gi) ≤ F (gi)

iF (gi) ≥ F (y(cid:48)(cid:48)) ≥ F (y) (last inequality is due to our choice of y(cid:48)(cid:48)).

6 Algorithm and statement of results
6.1 Algorithm statement and parameter setting

1. δ > 0 - an upper bound on the failure probability of the algorithm

(cid:16)

(cid:17)√

T

2. The desired regret bound (cid:96) =

3. resolution of the grid: α = 23d2

(log T )2d log 1
δ
√

2d4
log3 T ≥ γβ2

d.

4. Scaling factor β = 4096d4 log T .

5. Extension ratio: γ = 2048d4 log T .

6. Blow up factor η = 8d2 + 1.
7. Upper bound on the number of epoch τ ≤ 8d2 log T .

This algorithm calls upon two subroutines, FitLCE which was deﬁned in section 4, and ShrinkSet which

we now deﬁne.

16

Algorithm 3 Bandit Ellipsoid
1: Input: A convex set K ⊆ Rd, A: a high-probability low regret bandit algorithm on discrete set of points
2: Initialize: Epoch τ = 0, epoch set Γτ = ∅, Kτ = K, Grid gridτ = grid(βKτ ∩ K)
3: for t = 1 to T do
4:

Apply the low-regret algorithm on current grid:

(pt, vt, σt) ← A(gridτ ,{pi, xi, fi(xi) | i ∈ Γτ})

5:
6:

same notation for the new vt. Moreover, we can shift F τ =(cid:80)

where pt, vt, σt are deﬁned as in section 2.1.
Play a point xt ∈ gridτ from distribution pt(xt), observe value ft(xt). Set Γτ = Γτ ∪ {t}.
(Shift): Shift vt by a constant so that minx∈gridτ{vt(x) − ησt(x)} = 0, for simplicity we just keep the
fj by the same constant and assume
that adversary presents us the (shifted) fj. For simplicity we also keep the same notation for the new
F τ .
Compute F τ
if ∀x ∈ Kτ ,∃j ≤ τ, F j

LCE = FitLCE(βKτ ∩ K, [gridτ , vτ , στ ]).

j∈Γτ

LCE(x) > (cid:96)

4 then

7:
8:
9:
10:
11:
12:
13:
end if
14:
15: end for

RESTART (goto Initialize)
end if
if (DecideMove) ∃˜xτ ∈ Kτ

β such that F τ

LCE(˜xτ ) ≥ (cid:96) then

Kτ +1 = ShrinkSet(Kτ , ˜xτ , F τ
Set Γτ +1 = ∅, gridτ +1 = grid(βKτ +1 ∩ K, α), τ = τ + 1.

LCE, gridτ ,{vt, σt})

Algorithm 4 ShrinkSet
1: Input: Convex set Kτ , convex function F τ
2: Compute a separation hyperplane H(cid:48)
3: Let xτ be the center of the MVEE Eτ of Kτ .
4: (Amplify Distance). Let Hτ = {x | (cid:104)hτ , x(cid:105) = zτ} for some zτ ≥ 0 such that the following holds:

LCE, point ˜xτ ∈ Kτ , Grid gridτ , value estimation vt and variance
τ = {x |

estimation σt.
(cid:104)hτ , x(cid:105) = wτ} and {y | F τ

τ through ˜xτ between ˜xτ and {y | F τ

LCE(y) < (cid:96)} ⊆ {y | (cid:104)hτ , x(cid:105) ≤ wτ}

LCE(y) < (cid:96)}. Assume H(cid:48)

LCE(y) < (cid:96)} ⊆ {y | (cid:104)hτ , y(cid:105) ≤ zτ}

1. {y | F τ
2. dist(xτ , Hτ ) = 2dist(xτ , H(cid:48)
τ ).
3. (cid:104)hτ , xτ(cid:105) ≤ zτ .

5: Return Kτ +1 = (Kτ ∩ {y | (cid:104)hτ , y(cid:105) ≤ zτ})

(cid:16)(cid:80)

6.2 Statement of main theorem
Theorem 6.1 (Main, full algorithm). Suppose for all time t in all epoch τ, A outputs vt and σt such that for
all x ∈ gridτ ,

(cid:17) ∈ [vt(x) − σt(x), vt(x) + σt(x)]. Moreover, A achieves a value
(cid:88)

j∈Γτ ,j≤t fj(x)

(cid:96)

fj(xj) ≤ min
x∈gridτ

{vt(x) − ησt(x)} +

1024d3 log T

vτ (A) =

then Algorithm 3 satisﬁes

j∈Γτ ,j≤t

(cid:88)

t

ft(x∗) ≤ (cid:96)

(cid:88)

t

ft(xt) − min
x∗

17

Figure 7: Depiction of the ShrinkSet procedure

Corollary 6.1 (Exp3.P.). Algorithm 3 with A being Exp3.P satisﬁes the condition in Theorem 6.1 with proba-
bility 1 − δ for

(cid:19)√

T

(log T )2d log

1
δ

(cid:96) =

2d4

(cid:18)
(log T )poly(d)(cid:17)

(cid:16)

6.3 Running time

Our algorithm runs in time O
Exp3.P on K ≤ (2dα)d Experts

, which follows from Theorem 4.1 and the running time of

6.4 Analysis sketch
Before going to the details, we brieﬂy discuss the steps of the proof.

to get the regret bound, we can just focus on the minimal value of(cid:80)
Step 1: In the algorithm, we shift the input function so that the player can achieve a value ≤ √
Step 2: We follow the standard Ellipsoid argument, maintaining a shrinking set, which at epoch τ is denoted
Kτ . We show the volume of this set decreases by a factor of at least 1 − 1
d, and hence the number of epochs
between iterative RESTART operations can be bounded by O(d2 log T ) (when the diameter of Kτ along one
Step 3: We will show that inside each epoch τ, for every x ∈ Kτ ,(cid:80)
direction decreases below 1√
γ for (cid:96) ≈ √
− 2(cid:96)
(cid:80)
Step 4: We will show that when one epoch τ ends, for every point x cut off by the separation hyperplane,
t:t in epoch τ ft(x) is lower bounded by (cid:96)
Step 5: Putting the result of 3, 4 together, we know that for a point outside the current set Kτ , it must be cut
off by a separation hyperplane at some epoch j ≤ τ. Moreover, we can ﬁnd such j with γ(x,Kj) ≥ γ(x,Kτ )
.
Which implies that

T , γ ≥ 1. For point x outside the Kτ ,(cid:80)

t:t in epoch τ ft(x) is lower bounded by − 2(cid:96)

, we do not need to further discretizate along that direction).

t:t in epoch τ ft(x) is lower bounded by

γ γ(x,Kτ ).

2 γ(x,Kτ )

T . Therefore,

t ft.

d

(cid:88)

ft(x) ≥ − 2τ (cid:96)
γ

γ(x,Kτ ) +

(cid:96)γ(x,Kτ )

2d

√

T

≈

T

(cid:88)

ft(x) =

ft(x) +

t:t in epoch 1,2,...,j−1,j+1,...,τ

t:t in epoch j

By our choice of γ ≥ 8dτ. Therefore, when the adversary wants to move the optimal outside the current
set Kτ , the player has zero regret. Moreover, by the result of 3, inside current set Kτ , the regret is bounded by
γ ≈ √
τ 2(cid:96)

T .

18

(cid:88)

t

The crucial steps in our proof are Step 3 and Step 4. Here we brieﬂy discuss about the intuition to prove the
two steps.
Intuition of Step 3: For x ∈ Kτ , we use the grid property (Property of grid, 2.1) to ﬁnd a grid xg point
such that xc = xg + γ(xg − x) is close to the center of Kτ . Since xg is a grid point, by shifting we know that

(cid:88)

ft(xg) ≥ 0

t:t in epoch τ

γ , by convexity of ft, we know that(cid:80)

Therefore, if(cid:80)

t:t in epoch τ ft(x) < − 2(cid:96)

t:t in epoch τ ft(xc) ≥ 2(cid:96). Now,
c) ≥ (cid:96), by our
Intuition of Step 4: We use the fact that the algorithm does not RESTART, therefore, according to our
4. Observe that the separation hyperplane of our
LCE(x) ≥
t:t in epoch τ ft(x) ≥

apply discretization Lemma 5.1, we know that there is a point x(cid:48)
c near xc such that F τ
DecideMove condition, the epoch τ should end. Same argument can be applied to x /∈ Kτ .
condition, there is a point x0 ∈ Kτ with F τ
algorithm separates x0 with points whose F τ
2 γ(x,Kτ ). Apply the fact that F τ
(cid:96)
2 γ(x,Kτ ).
convex regression is not a lower bound on F , see section 3 for further discussion on this issue).

t:t in epoch τ ft we can conclude(cid:80)

Notice that here we use the convexity of FLCE, and also the fact that it is a lower bound on F (standard

LCE ≥ (cid:96). Using the convexity of FLCE, we can show that F τ

LCE(x0) ≤ (cid:96)

LCE is a lower bound of(cid:80)

LCE(x(cid:48)

(cid:96)

Now we can present the proof for general dimension d
To prove the main theorem we need the following lemma, starting from the following corollary of Lemma

5.1:

Corollary 6.2.
(1). For every epoch τ, ∀x ∈ βKτ ∩ K,

(cid:88)

LCE(x) ≤ F τ (x) =
F τ

fi(x)

.

(2). For every epoch τ, let xτ be the center of the MVEE of Kτ , then F τ (x) =(cid:80)
there exists x(cid:48) ∈(cid:16)

(cid:17) ⊆ Kτ

x + Kτ

i∈Γτ

LCE(x(cid:48)) ≥ 1

β such that F τ

2 F τ (x)

2β

Proof. (1) is just due to the deﬁnition of LCE. (2) is due to the Geometry Lemma on F τ : for every x ∈ K
2β ,

i∈Γ fi(x) ≤ 2(cid:96).

Lemma 6.1 (During an epoch). During every epoch τ the following holds:
x ∈ K ∩ Kτ
x ∈ K ∩ Kc

− 2γ(x,Kτ )(cid:96)

F τ (x) ≥

 − 2(cid:96)

γ

τ

γ

Lemma 6.2 (Number of epoch). There are at most 8d2 log T many epochs before RESTART.
Proof of 6.2. Let Eτ be the minimal volume enclosing Ellipsoid of Kτ , we will show that

(cid:18)

(cid:19)

vol(Eτ +1) ≤

1 − 1
8d

vol(Eτ )

First note that Kτ +1 = Kτ ∩ H for some half space H corresponding to the separating hyperplane going
through 1
Let E(cid:48)

βEτ , therefore, Kτ +1 ⊂ Eτ ∩ H.
τ +1 be the minimal volume enclosing Ellipsoid of Eτ ∩ H, we know that

vol(Eτ ) ≤ vol(E(cid:48)

τ +1)

19

Without lose of generality, we can assume that Eτ is centered at 0. Let A be a linear operator on Rd such

that A(Eτ ) is the unit ball B1(0), observe that

vol(E(cid:48)
τ +1)
vol(Eτ )

vol(AE(cid:48)
τ +1)
vol(AEτ )

=

Since AE(cid:48)

τ +1 is the MVEE of AEτ ∩ AH, where AH is the halfspace corresponding to the separating
(0). Without lose of generality, we can assume that H = {x ∈ Rd | x1 ≥ a} for
hyperplane going through B 1
β ≤ 1
some a such that |a| ≤ 1
d2 .
(cid:40)

Observe that

(cid:41)

β

AEτ ∩ AH ⊆

(cid:0)1 − 1
x ∈ Rd | (x1 − 1
4d )2

(cid:1)2 +

4d

x2
2

1 + 1
12d2

+ ... +

x2
d

1 + 1
12d2

≤ 1

= E

Therefore,

vol(AE(cid:48)

τ +1) ≤ vol(E) ≤ 1 − 1
8d

.

Now, observe that the algorithm will not cut through one eigenvector of the MVEE of Kτ if its length is
. Therefore,

, and the algorithm will stop when all its eigenvectors have length smaller than 1√

T

smaller than 1√
the algorithm will make at most

T

many epochs.
Lemma 6.3 (Beginning of each epoch). For every τ ≥ 0:

(cid:19)

T

(cid:18) 1√
 −τ 2(cid:96)

γ

d log1− 1

8d

= 8d2 log T

τ−1(cid:88)

F i(x) ≥

x ∈ K ∩ Kτ
x ∈ K ∩ Kc

Lemma 6.4 (Restart). (After shifting) If A obtains a value vj(A) = (cid:80)

γ(x,Kτ )(cid:96)

i=0

64d

τ

epoch j, then when the algorithm RESTART, Regret = 0.

ft(xt) ≤

(cid:96)

1024d3 log T for each

t∈Γj

6.5 Proof of main theorem
Now we can prove the regret bound assuming all the lemmas above, whose proof we defer to the next section.

Proof of Theorem 6.1. Using Lemma 6.4, we can only consider epochs between two RESTART. Now, for
epoch τ, we know that for x ∈ K ∩ Kc

τ , (cid:88)
(cid:88)

i∈Γτ

i∈Γ0∪...∪Γτ−1

fi(x) ≥ γ(x,Kτ )(cid:96)

64d

fi(x) ≥ − 2γ(x,Kτ )(cid:96)
(cid:18) 1

γ

fi(x) ≥ γ(x,Kτ )(cid:96)

− 2
γ

64d

(cid:19)

≥ 0

20

Therefore, for x ∈ K ∩ Kc

τ (cid:88)

i∈Γ0∪...∪Γτ

By our choice of γ = 2048d4 log T .

In the same manner, we know that for x ∈ K ∩ Kτ ,

(cid:88)
Which implies that for every x ∈ K,(cid:80)
Denote by vj(A) =(cid:80)

By τ ≤ 8d2 log T .

i∈Γ0∪...∪Γτ

The low-regret algorithm A guarantees that in each epoch:

i∈Γ0∪...∪Γτ

fi(x) ≥ − (cid:96)
2.

fi(x) ≥ − 2(τ + 1)(cid:96)

γ

≥ − (cid:96)
2

(cid:88)

vj(A) =

fi(xi)

i∈Γj ,i≤t fj(xj) the overall loss incurred by the algorithm in epoch j before time t.

{vt(x) − ησt(x)} +

(cid:96)

1024d3 log T

by shifting min
x∈gridτ

{vt(x) − ησt(x)} = 0

i∈Γτ ,i≤t

≤ min
x∈gridτ
(cid:96)

≤

1024d3 log T

(cid:88)

0≤j≤τ

1024d3 log T

Thus A obtains over all epochs a total value of at most

Therefore,

Regret =

= (τ + 1) ×

vj(A) − (cid:88)

(cid:96)

(cid:88)

0≤j≤τ

i∈Γ0∪...∪Γτ

(cid:96)

1024d3 log T

≤ (cid:96)
2

.

fi(x∗) ≤ (cid:96)

7 Analysis and proof of main lemmas
7.1 Proof of Lemma 6.1

Proof of 6.1. Part 1:
Consider any x ∈ Kτ . By Lemma 2.1 part 1, we know that there exists xg ∈ gridτ such that xc =
xg + γ(x − xg) ∈ Kτ
2β . Any convex function f satisﬁes for any two points y, z that f (γx + (1 − γ)y) ≤
γf (x) + (1 − γ)f (y). Applying this to the convex function F τ over the line on which the points x, xc, xg
reside and observe γ =

, we have

(cid:107)xc−xg(cid:107)2
(cid:107)xg−x(cid:107)2

F τ (xc) − F τ (xg) ≥ ||xc − xg||2
||xg − x||2

(F τ (xg) − F τ (x)) = γ(F τ (xg) − F τ (x))

Since xg ∈ grid and we shifted all losses on the grid to be nonnegative, F τ (xg) ≥ 0. Thus, we can simplify

the above to:

F τ (xc) ≥ −γF τ (x)

Since the epoch is ongoing, the conditions of DecideMove are not yet satisﬁed, and hence ∀x(cid:48) ∈ 1

βKτ , F τ
2βKτ it holds that F τ (x(cid:48)(cid:48)) ≤ 2(cid:96), in particular F τ (xc) ≤ 2(cid:96). The

(cid:96). By (2) of Lemma 6.2 for all points x(cid:48)(cid:48) in 1
above simpliﬁes to

LCE(x(cid:48)) ≤

Part 2:

F τ (x) ≥ − 1
γ

F τ (xc) ≥ − 2(cid:96)
γ

21

Figure 8: geometric intution for the proof

For x ∈ Kc

τ ∩ K By Lemma 2.1 part 2, we know that there exists xg ∈ gridτ such that xc = xg +

γ(x,Kτ ) (x − xg) ∈ Kτ

γ

β2 . Now, by the convexity of F τ , we know that

F τ (xc) − F τ (xg) ≥ ||xc − xg||2
||xg − x||2

(F τ (xg) − F τ (x)) =

γ

γ(x,Kτ )

(F τ (xg) − F τ (x))

Since xg ∈ grid and we shifted all losses on the grid to be nonnegative, F τ (xg) ≥ 0. Thus, we can simplify

the above to:

F τ (xc) ≥ −

γ

γ(x,Kτ )

F τ (x)

Since the epoch is ongoing, the conditions of DecideMove are not yet satisﬁed, and hence ∀x(cid:48) ∈ 1

βKτ , F τ
2βKτ it holds that F τ (x(cid:48)(cid:48)) ≤ 2(cid:96), in particular F τ (xc) ≤ 2(cid:96). The

(cid:96). By (2) of Lemma 6.2 for all points x(cid:48)(cid:48) in 1
above simpliﬁes to

LCE(x(cid:48)) ≤

F τ (x) ≥ − γ(x,Kτ )

γ

F τ (xc) ≥ − 2γ(x,Kτ )(cid:96)

γ

7.2 Proof of Lemma 6.3
Proof of Lemma 6.3. Part 1: For every x ∈ K ∩ Kτ , since Kτ ⊆ Kτ−1 ⊆ ... ⊆ K0 = K, we have x ∈ Kj for
every 0 ≤ j ≤ τ. Therefore, by Lemma 6.1 we get F j(x) ≥ − 2(cid:96)

γ . Summing over the epochs,

Part 2: Figure 8 illustrates the proof.

i=0

τ−1(cid:88)

F i(x) ≥ −τ

2(cid:96)
γ

22

For every x ∈ K ∩ Kc

τ , since the Algorithm does not RESTART, therefore, there must be a point x0 ∈ Kτ

∀τ(cid:48) ≤ τ, F τ(cid:48)

LCE(x0) ≤ (cid:96)
4

(2)
Let l be the line segment between x and x0. Since x /∈ Kτ , the line l intersects Kτ , and denote xm be the
intersection point between l and Kτ : {xm} = l ∩ Kτ . The corresponding boundary of Kτ was constructed in
an epoch j ≤ τ, and a hyperplane which separates the (cid:96)-level set of Kj, namely H = {xm | (cid:104)hj, xm(cid:105) = zj})
(See ShrinkSet for deﬁnition of hj, zj) such that H ∩ l = {xm}.

Now, by the deﬁnition of Minkowski Distance, we know that (Since Minkowski Distance is the distance
dEτ can be 1/d smaller than Kτ , and xm is the intersection point to

dEτ where Eτ is the MVEE of Kτ , 1

such that

ratio to 1
Kτ )

≥ γ(x,Kτ ) − 1

2d

||x − xm||2
||xm − x0||2
LCE)

We know that (by the convexity of F j
LCE(x) − F j
F j
LCE(xm) − F j
F j

LCE(xm)
LCE(x0)

≥ ||x − xm||2
||xm − x0||2

≥ γ(x,Kτ ) − 1

2d

where the denominator is non-negative, by equation (2), F j
hyperplane of the (cid:96)-level-set of F j

LCE(xm) ≥ (cid:96). This implies

LCE(x0) ≤ (cid:96)

LCE), F j
LCE(x) ≥ (γ(x,Kτ ) − 1) · 3

4 (cid:96)

F j

2d

+ (cid:96) ≥ γ(x,Kτ )(cid:96)

4d

4, and by the deﬁnition of H (separation

We consider the following two cases: (a). x ∈ βKj, (b). x /∈ βKj.
case (a): x ∈ βKj
The LCE is a lower bound of the original function only for x in the LCE ﬁtting domain, here LCE = F j

LCE,
original function F j : βKj ∩ K → R, so it is only true for x ∈ βKj ∩ K. Now, by (1) in Lemma 6.2, we know
that F j(x) ≥ F j

LCE(x) ≥ γ(x,Kτ )(cid:96)

4d

.

For other epoch i < τ, we can apply Lemma 6.1 and get F i(x) ≥ − 2γ(x,Ki)(cid:96)

... ⊆ K0, By John’s theorem, we can conclude that γ(x,Ki) ≤ 2dγ(x,Kτ )

γ

. Since the set Kτ ⊆ Kτ−1 ⊆

which implies

τ−1(cid:88)

F i(x) ≥ (cid:88)

i=0

i(cid:54)=j

F i(x) + F j(x)

≥ γ(x,Kτ )(cid:96)

− τ × 4dγ(x,Kτ )(cid:96)

≥ γ(x,Kτ )(cid:96)

32d

γ
by our choice of parameters τ ≤ 8d2 log T and γ = 2048d4 log T .

4d

case (b): x /∈ βKj, x ∈ K 7
This part of the proof consists of three steps. First, We ﬁnd a point xj in center of Kj that has low F j value.
LCE value, which implies by

Then we ﬁnd a point xp inside βKj, on the line between xj and x, with large F j
lemma 6.2 it has large F j value. Finally, we use both x0, xp to deduce the large value of F j(x).

Step1: Let xj be the center of MVEE Ej of Kj. By (2) in Lemma 6.2, we know that F j(xj) ≤ 2(cid:96).
Step 2: Deﬁne H(cid:48) = {y | (cid:104)y, hj(cid:105) = wj} to be the hyperplane parallel to H such that dist(xj, H(cid:48)) =
2dist(xj, H), and H(cid:48)(cid:48) = {y | (cid:104)y, hj(cid:105) = uj} to be the hyperplane parallel to H such that dist(x0, H(cid:48)(cid:48)) =
1
9dist(x0, H).

7In the follow proof, if not mentioned speciﬁcally, every points are in K

23

We can assume (cid:104)x0, hj(cid:105) < wj (x0, H are in different side of H(cid:48)), since we know that F j

4 by
deﬁnition, and the hyperplane H(cid:48) separates such that all points with (cid:104)x0, hj(cid:105) ≥ wj (See ShrinkSet for deﬁnition
of H, H(cid:48)) have value F j

LCE(x0) ≤ (cid:96)

LCE(x) ≥ (cid:96).

Note (cid:104)x0, hj(cid:105) < wj implies dist(x0, H) ≥ 1

2dist(xj, H) = dist(H, H(cid:48)) 8, which implies that

dist(xj, H(cid:48)(cid:48)) ≥ dist(H, H(cid:48)(cid:48)) = 8dist(x0, H) ≥ 4dist(xj, H).

Now, let xs = l ∩ H(cid:48)(cid:48) be the intersection point between H(cid:48)(cid:48) and l, we can get: xs = xm + 8(xm − x0).
s = l(cid:48) ∩ H(cid:48)(cid:48) be the intersection
Since x0, xm ∈ Kj , we can obtain xs ∈ β
point of H(cid:48)(cid:48) and the line segment l(cid:48) of x and xj. Let x1 be the intersection point of H(cid:48) and l: {x1} = H(cid:48) ∩ l.
Consider the plane deﬁned by x0, xj, x. Deﬁne xp to be the intersecting point of the ray shooting from xs
Note that (cid:107)xs − xp(cid:107) ≤ (cid:107)x1 − xj(cid:107), we have:

towards the interval [x, xj], that is parallel to the line from x1 to xj.

2Kj by our choice of β ≥ 64d2. Let x(cid:48)

xp = xs + (xp − xs) = xs + (xj − x1)

Moreover, we know that ||x(cid:48)
2(cid:107)x(cid:48)

We know that x1, xj ∈ Kj, xs ∈ β
m − x(cid:48)
We also note that ||x(cid:48)

1

s − xp||2 ≤ ||xp − x(cid:48)

s(cid:107)2 (last inequality by dist(xj, H(cid:48)(cid:48)) ≥ 4dist(xj, H)).

2Kj, therefore, xs + (xj − x1)

s − xp||2 ≤ ||xp − x(cid:48)

m||2 due to the fact that ||x(cid:48)

m||2 implies
dist(xp, H) ≥ 1
2

dist(x(cid:48)

s, H).

||xs − xp||
||x1 − xj||
||xs−xp||
||x1−xj|| ∈ βKj, which means xp ∈ βKj.
m − xj||2 ≤

s − xp||2 ≤ ||x(cid:48)

Now, let l(cid:48)(cid:48) be the line segment between xp and x0, let x(cid:48)(cid:48)
m}.
{x(cid:48)(cid:48)
Consider the value of F j(xp), by (1) in Lemma 6.2 and xp ∈ βKj, we know that F j(xp) ≥ F j

m be the intersection point of H and l(cid:48)(cid:48): H ∩ l(cid:48)(cid:48) =

LCE(xp). By

the convexity of F j

LCE, we obtain:

LCE(xp) − F j
F j
m) − F j
LCE(x(cid:48)(cid:48)
F j

LCE(x(cid:48)(cid:48)
m)
LCE(x0)

=

m||2
≥ ||xp − x(cid:48)(cid:48)
||x(cid:48)(cid:48)
m − x0||2
dist(xp, H)
dist(x0, H)
2dist(x(cid:48)
s, H)
dist(x0, H)
2dist(H(cid:48)(cid:48), H)
1
dist(x0, H)

≥ 1

=

= 4

Note that F j

LCE(x(cid:48)(cid:48)

m) ≥ (cid:96), F j

LCE(x0) ≤ (cid:96)

4, therefore, F j

LCE(xp) ≥ 3(cid:96). Which implies F j(xp) ≥ F j

LCE(xp) ≥

3(cid:96).

Step 3:
Due to x /∈ βKj and xm ∈ Kj, by our choice of xs and β, we know that ||x − xm||2 ≥ 8||xs − xm||2.
8H, H(cid:48), H(cid:48)(cid:48) are parallel to each other, so we can deﬁne distance between them

24

We ready to bound the value of F j(x): By the convexity of F j, we have:

F j(x) − F j(xp)
F j(xp) − F j(xj)

triangle similarity

by ||xs − xm||2 ≥ 8||xm − x1||

=

=

≥ ||x − xp||2
||x − xs||2
||xp − xj||2
||xs − x1||2
||x − xm||2 − ||xs − xm||2
||xm − x1||2 + ||xs − xm||2
||x − xm||2
≥
2||xs − xm||2
≥ ||x − xm||2
||xm − x0||2
≥ γ(x,Kτ ) − 1

× ||xm − x0||2
2||xs − xm||2

32d

= 1

The last inequality is due to ||xm−x0||2
||xs−xm||2
Putting together, we obtain (by F j(xj) ≤ 2(cid:96)):
F j(x) ≥ γ(x,Kτ ) − 1
τ−1(cid:88)

F i(x) ≥ (γ(x,Kτ ) − 1)(cid:96)

32d

Same as case (a) , we can sum over rest epoch to obtain:

8 and ||x−xm||2
||xm−x0||2

≥ γ(x,Kτ )−1

2d

(cid:0)F j(xp) − F j(xj)(cid:1) ≥ γ(x,Kτ ) − 1

32d

− τ × 4dγ(x,Kτ )(cid:96)

γ

≥ γ(x,Kτ )(cid:96)

64d

i=0

32d

by our choice of parameters τ ≤ 8d2 log T and γ = 2048d4 log T .

7.3 Proof of Lemma 6.4

Proof of Lemma 6.4. Suppose algorithm RESTART at epoch τ, then(cid:80)

need to show that for every x ∈ K,

(cid:88)

fi(x) ≥ (cid:96)
128d

i∈Γ0∪...∪Γτ

j≤τ vj(A) ≤ (cid:96)

128d. Therefore, we just

(a). Since the algorithm RESTART, by the RESTART condition, for every x ∈ Kτ , we know that ∃j ≤ τ
4. Using Lemma 6.1, we know that for every j(cid:48) ≤ τ, j(cid:48) (cid:54)= j:

fi(x) ≥ F j

LCE(x) > (cid:96)

i∈Γj

.

such that F j(x) =(cid:80)
(x) =(cid:80)

F j(cid:48)

Which implies that

i∈Γj(cid:48) fi(x) ≥ − 2(cid:96)
γ .

(b). For every x /∈ Kτ , by Lemma 6.3, we know that

(cid:88)
(cid:88)

fi(x) ≥ (cid:96)
4

− τ

2(cid:96)
γ

≥ (cid:96)
8

i∈Γ0∪...∪Γτ

fi(x) ≥ γ(x,Kτ )(cid:96)

64d

i∈Γ0∪...∪Γτ−1

Moreover, by Lemma 6.1, we know that(cid:88)

i∈Γτ

fi(x) ≥ − 2γ(x,Kτ )(cid:96)

γ

25

Putting together we have: (cid:88)

i∈Γ0∪...∪Γτ

fi(x) ≥ γ(x,Kτ )

(cid:19)

(cid:18) (cid:96)

64d

− 2(cid:96)
γ

≥ (cid:96)
128d

8 The EXP3 algorithm

For completeness, we give in this section the deﬁnition of the EXP3.P algorithm of [5], in slight modiﬁcation
which allows for unknown time horizon and output of the variances.

9 Acknowledgements

We would like to thank Aleksander Madry for very helpful conversations during the early stages of this work.

26

(cid:113) K ln K

Algorithm 5 Exp3.P
1: Initial: T = 1.
2: Input: K experts, unknown rounds. In round t the cost function is given by ft.
3: Let γ =
4: for j = 1, ..., K do
5:

(cid:113)
ln(cid:0) KT

(cid:1),

, α =

T

δ

set

(cid:32)

(cid:114)

(cid:33)

T
K

w1(j) = exp

ηαγ

6: end for
7: for t = T, ..., 2T − 1 do
8:
9:

for j = 1, ..., K do

pt(j) = (1 − γ)

wt(j)(cid:80)

j(cid:48) wt(j(cid:48))

+

γ
K

end for
pick jt at random according to pt(j), play expert jt and receive ft(jt)
for j = 1, ..., K do

10:
11:
12:
13:

Let

14:

And

15:
16:

end for
Update

17:

return

and

ˆft(j) =

ˆgt(j) =

if j = jt;
otherwise.

if j = jt;
otherwise.

wt+1(j) = wt(j) exp

ˆgt(j) +

(cid:19)(cid:19)

√
ηα
pt(j)

T K

0

pt(j)

pt(j)
0

(cid:40) ft(j)
(cid:40) 1−ft(j)
(cid:18)
(cid:18) γ
t(cid:88)
t(cid:88)

i=1

K

vt(j) =

ˆfi(j)

σt(j) =

√
α
pi(j)

T K

18: end for
19: Set T = 2T and Goto 3.

i=1

References
[1] Rediet Abebe. Counting regions in hyperplane arrangements. Harvard College Math Review, 5.

[2] Jacob Abernethy, Elad Hazan, and Alexander Rakhlin. Competing in the dark: An efﬁcient algorithm for

bandit linear optimization. In COLT, pages 263–274, 2008.

[3] Alekh Agarwal, Ofer Dekel, and Lin Xiao. Optimal algorithms for online convex optimization with

multi-point bandit feedback. In COLT, pages 28–40, 2010.

27

[4] Alekh Agarwal, Dean P. Foster, Daniel Hsu, Sham M. Kakade, and Alexander Rakhlin. Stochastic convex

optimization with bandit feedback. SIAM Journal on Optimization, 23(1):213–240, 2013.

[5] Peter Auer, Nicol`o Cesa-Bianchi, Yoav Freund, and Robert E. Schapire. The nonstochastic multiarmed

bandit problem. SIAM J. Comput., 32(1):48–77, January 2003.

[6] Baruch Awerbuch and Robert Kleinberg. Online linear optimization and adaptive routing. J. Comput.

Syst. Sci., 74(1):97–114, 2008.

[7] Keith Ball. An elementary introduction to modern convex geometry. In Flavors of Geometry, pages 1–58.

Univ. Press, 1997.

[8] S´ebastien Bubeck and Nicolo Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-armed

bandit problems. Foundations and Trends in Machine Learning, 5(1):1–122, 2012.

[9] S´ebastien Bubeck, Nicol`o Cesa-Bianchi, and Sham M. Kakade. Towards minimax policies for online
linear optimization with bandit feedback. Journal of Machine Learning Research - Proceedings Track,
23:41.1–41.14, 2012.

[10] S´ebastien Bubeck, Ofer Dekel, Tomer Koren, and Yuval Peres. Bandit convex optimization: \(\sqrt{T}\)
regret in one dimension. In Proceedings of The 28th Conference on Learning Theory, COLT 2015, Paris,
France, July 3-6, 2015, pages 266–278, 2015.

[11] S´ebastien Bubeck and Ronen Eldan. Multi-scale exploration of convex functions and bandit convex

optimization. CoRR, abs/1507.06580, 2015.

[12] Andrew R Conn, Katya Scheinberg, and Luis N Vicente. Introduction to Derivative-Free Optimization,

volume 8. Society for Industrial and Applied Mathematics, 2009.

[13] Varsha Dani, Thomas P. Hayes, and Sham Kakade. The price of bandit information for online optimiza-

tion. In NIPS, 2007.

[14] Ofer Dekel, Ronen Eldan, and Tomer Koren. Bandit smooth convex optimization: Improving the bias-
In Advances in Neural Information Processing Systems 28: Annual Conference on

variance tradeoff.
Neural Information Processing Systems 2015, 2015.

[15] Abraham Flaxman, Adam Tauman Kalai, and H. Brendan McMahan. Online convex optimization in the

bandit setting: gradient descent without a gradient. In SODA, pages 385–394, 2005.

[16] M. Gr¨otschel, L. Lov´asz, and A. Schrijver. Geometric algorithms and combinatorial optimization. Algo-

rithms and combinatorics. Springer-Verlag, 1993.

[17] Elad Hazan. DRAFT: Introduction to online convex optimimization. Foundations and Trends in Machine

Learning, XX(XX):1–168, 2015.

[18] Elad Hazan and Zohar Karnin. Hard-margin active linear regression. In 31st International Conference on

Machine Learning (ICML 2014), 2014.

[19] Elad Hazan and Kﬁr Y. Levy. Bandit convex optimization: Towards tight bounds. In Advances in Neural
Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014,
December 8-13 2014, Montreal, Quebec, Canada, pages 784–792, 2014.

[20] F. John. Extremum Problems with Inequalities as Subsidiary Conditions.

In K. O. Friedrichs, O. E.
Neugebauer, and J. J. Stoker, editors, Studies and Essays: Courant Anniversary Volume, pages 187–204.
Wiley-Interscience, New York, 1948.

[21] Robert D Kleinberg. Nearly tight bounds for the continuum-armed bandit problem. In NIPS, volume 17,

pages 697–704, 2004.

28

[22] Eunji Lim and Peter W. Glynn. Consistency of multidimensional convex regression. Oper. Res.,

60(1):196–208, January 2012.

[23] Hariharan Narayanan and Alexander Rakhlin. Random walk approach to regret minimization. In Ad-
vances in Neural Information Processing Systems 23: 24th Annual Conference on Neural Informa-
tion Processing Systems 2010. Proceedings of a meeting held 6-9 December 2010, Vancouver, British
Columbia, Canada., pages 1777–1785, 2010.

[24] Ankan Saha and Ambuj Tewari. Improved regret guarantees for online smooth convex optimization with

bandit feedback. In AISTATS, pages 636–642, 2011.

[25] Ohad Shamir. On the complexity of bandit and derivative-free stochastic convex optimization. In Confer-

ence on Learning Theory, pages 3–24, 2013.

29

