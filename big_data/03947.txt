6
1
0
2

 
r
a

 

M
2
1

 
 
]

D
S
.
s
c
[
 
 

1
v
7
4
9
3
0

.

3
0
6
1
:
v
i
X
r
a

Spooﬁng Detection Goes Noisy: An Analysis of Synthetic

Speech Detection in the Presence of Additive Noise

Cemal Hanil¸cia,b,∗, Tomi Kinnunena, Md Sahidullaha, Aleksandr Sizova

aSchool Of Computing, University of Eastern Finland, Joensuu, Finland.

bDepartment of Electrical-Electronic Engineering, Bursa Technical University, Bursa, Turkey.

Abstract

Speaker recognition technology is recently ﬁnding its way to commercial applications to
secure access to personal data or smart services. But similar to other biometric iden-
tiﬁers, speaker recognition is highly vulnerable to spooﬁng attacks where an attacker
masquerades as a speciﬁc target speaker via text-to-speech (TTS) or voice conversion
(VC) techniques in order to gain illegitimate access to the system. Existing studies
on spooﬁng detection, determining whether a speech signal is natural or generated by
a speech synthesis/voice conversion technique, mostly report their observations using
high-quality clean recordings while the accuracy of spooﬁng detection in the presence of
noise is largely unknown. To this end, we investigate the suitability of diﬀerent synthetic
speech detectors under additive noise. In order to ﬁnd the most appropriate counter-
measure, performances of various recently developed front-ends for spooﬁng detection
are systematically examined under noisy conditions. We evaluate two diﬀerent back-
ends: Gaussian mixture model (GMM) and modern i-vector probabilistic linear discrim-
inant analysis (PLDA) approach. Our experiments conducted on the recently released
ASVspoof 2015 corpus indicate that synthetic speech detection accuracy decreases dra-
matically in the presence of additive noise. In contrast to the case of speech and speaker
recognition system in noisy environment, we observe that standard speech enhancement
techniques are not helpful. In the experiments with diﬀerent front-ends, relative phase
shift (RPS) features are found to give the highest synthetic speech detection accuracy for
clean data on development set. However, Mel-frequency cepstral coeﬃcients (MFCCs)
and subband spectral centroid magnitude coeﬃcients (SCMCs) are the absolute winners
among the other features for matched and mismatched noise conditions, respectively.

Keywords:

speaker recognition, anti spooﬁng, countermeasures, additive noise

1. Introduction

Automatic speaker veriﬁcation (ASV) [1] is the task of authenticating users based on
their voices. Traditionally, ASV has mostly been applied in specialized surveillance and
forensics applications but recent methodological advances have greatly increased interest

∗Corresponding author

Preprint submitted to Elsevier

March 12, 2016

in mass-market adoption to secure personal data. For instance, in 2013 a smartphone
voice unlock feature was introduced to a Baidu-Lenovo phone1, and similar activities
are currently being pursed by Google to their Android devices2. Some of the favorable
points of ASV over other popular biometric identiﬁers are wide applicability (no other
sensors except microphone required), natural integration with face authentication in
smart-phones, as well as revocability: if a voice token is compromised or stolen, another
user pass-phrase can be selected.

A speech-based authentication system to control access to personal data or physi-
cal site will be useful only if it helps to improve the overall system security. A now
well-recognized security concern with any biometric modality — including ﬁngerprints,
face and speech — is that they are vulnerable to circumvention by spooﬁng attacks [2],
whereby an attacker attempts to gain unauthorized access to the system by masquerad-
ing herself as another user. Attacks can naturally be executed at any parts of the system
[3], including software, biometric templates or features. But direct attacks, involving
injection of forged biometric data to the sensor or the transmission point, are arguably
more accessible to even less technology-aware attackers. Consequently, direct spooﬁng
attacks are under active research across all the major biometric modalities. Speciﬁc to
ASV, four major types of direct attacks have been identiﬁed [4, 5]: (i) replay [6, 7, 8],
representation of a pre-recorded target speaker utterance; (ii) impersonation [9, 10],
human-based mimicry of a target voice; (iii) text-to-speech synthesis (TTS), artiﬁcially
generated target voice from an abitrary text input [11]; and (iv) voice conversion (VC),
modiﬁcation of source speech towards target speaker characteristics [12].

In this study, we focus on VC and TTS as they are arguably more ﬂexible and
consistent for spooﬁng both text-independent and -dependent ASV systems [5]. The ef-
fectiveness of VC and TTS spooﬁng attacks were ﬁrst demonstrated nearly two decades
ago in [13] and [14]. Further recent studies [15, 16, 17, 18, 19, 20, 21] aﬃrm that even
state-of-the-art ASV systems are highly vulnerable to modern VC and TTS attacks.
State-of-the-art VC and TTS can produce high quality target speech using relatively
small amount of training data [22, 23]. Anytime in near future one should expect ad-
vanced voice transformation tools to be readily available for consumers in smart-phones
or other portable devices, thereby greatly increasing the threats imposed by advanced
VC and TTS spooﬁng attacks.

Having recognized the vulnerability problem caused by spooﬁng attacks, a few ﬁrst
steps to develop various countermeasures (CMs) have been taken [5]. The most common
approach (for an exception, see [24]) is to equip an oﬀ-the-shelf ASV system with a
stand-alone spooﬁng attack detector. In our case, a classiﬁer that will assigns a human
or synthetic label (or a likelihood score) to a given utterance3. The novel contribution of
this work, which is further reﬁned and put to a wider context in Section 2, is a detailed
study on synthetic speech detection under acoustically degraded conditions, namely,
additive noise. Speciﬁcally, basing on the recent ASVspoof 2015 challenge data [25], we

1http://www.signalprocessingsociety.org/technical-committees/list/sl-tc/spl-nl/2013-02/

SpeakerVerificationMakesItsDebutinSmartphone

2http://thehackernews.com/2015/04/android-trusted-voice.html
3For brevity, we use “synthetic speech detection” to refer to detection of both VC and TTS attacks.
In the present context, such umbrella term is justiﬁed as TTS and VC systems often employ similar
methods for voice coding

2

address the joint eﬀect of varied VC and TTS attacks and additive noise. By focusing
to the key part of synthetic spooﬁng detectors, the feature extractor, our intention is to
gain improved understanding on which methods are expected to generalize and which
might have been over-optimized for a particular attack or corpus.

2. Related work and motivation

2.1. Methods for detecting synthetic speech

Synthetic speech detection is enabled by imperfections of the VC or TTS systems.
For instance, voice coders (vocoders) used for speech parametrization in VC and TTS
systems use greatly simpliﬁed models of human voice production, such as all-pole synthe-
sis ﬁlters driven by impulse train excitation [26]. Processing artifacts aﬀect the spectral,
temporal and prosody characteristics of synthetic speech. Similar to ASV, synthetic
speech detectors consists of front-end (feature extraction) and back-end (classiﬁer) com-
ponents. Most of the work on synthetic speech detection focuses on the former, including
speciﬁc/tailored features combined with a simple Gaussian mixture model (GMM) or
support vector machine (SVM) based detector back-end module. A radically diﬀerent
approach, using standard MFCC features but focusing on i-vectors and advanced back-
end modeling ideas, was carried out in [24] with promising results on voice-converted
version of NIST 2006 SRE data (though not performing well on ASVspoof 2015 [27],
possibly due to much shorter test utterances).

In [28], standard mel-frequency cepstral coeﬃcients (MFCCs), cosine phase and mod-
iﬁed group delay features were compared for the detection of Gaussian mixture model
(GMM) and unit selection based synthetic speech, cosine phase features leading to the
lowest error rates. In [29], MFCCs, modiﬁed group delay, phase and amplitude modu-
lation features were compared for detecting synthetic speech, the group delay features
yielding the highest accuracy. One of the most popular feature sets used for synthetic
speech detection are the so-called relative phase shift (RPS) features [30, 31, 32]. They
are calculated based on the phase shift of the harmonic components of the signal with
respect to fundamental frequency and have been reported very eﬀective in detecting
synthetic speech [31, 32]. But in [32] it was reported that RPS-based synthetic speech
detection might be sensitive to vocoder mismatch across training and test sets, leading
to degraded performance. More recently in [31], the RPS features were used to detect
synthetic speech signals provided by Blizzard Challenge. The authors found out that
RPS features outperformed MFCCs on detecting speech generated by statistical para-
metric speech synthesis whereas MFCCs yielded higher accuracy when synthetic signals
were generated by unit selection, diphone or hybrid methods. Similar, inconsistent ob-
servations were found in our recent study [33] where RPS features performed the best
out of 17 compared feature extraction techniques when vocoders between training and
test were matched, but yielding the highest error rates in the opposite case.

2.2. Towards varied spooﬁng attacks: SAS corpus and ASVspoof 2015 challenge

As the above review indicates, a large number of potentially useful methods to detect
synthetic speech have been investigated. But the user’s dilemma is that their relative
performances are either incomparable or under-representative of real-world deployment,
for a number of reasons. Firstly, there is no single study that compares the various

3

methods on a common set of data or using a uniﬁed objective evaluation metric, making
unbiased performance assessment challenging, if not impossible. Secondly, the studies
usually contain only a handful of attacks, making conclusions speciﬁc to a certain attack.
Thirdly, most studies involve a closed-world evaluation setting where the synthetic test
samples originate from the same methods, channels and environments as used in training.
This corresponds to a scenario where the ASV system administrator (defender) knows
in advance what spooﬁng technique the attacker will employ. While such an oracle eval-
uation scenario may provide experimental bounds to the highest performance achievable
using a speciﬁc attack detector, it is unlikely to be representative of an actual attack
scenario where the attacker may employ novel (presently unknown) attacks. Fourthly,
diﬀerently from the traditional NIST speaker veriﬁcation scenarios involving channel-
and condition-mismatched data, most of the datasets used for synthetic speech detection
have consisted of high-quality (wideband) noise-free signals. As a result, it is largely
unknown whether and how well the existing synthetic speech detectors generalize to
non-ideal conditions involving not only varied spooﬁng materials but external distortions
caused by the environment or channel, important factors in any real-world deployment
of ASV technology.

To address the ﬁrst three concerns — incomparability of results, limited set of at-
tacks and closed-world evaluation bias — a new speaker veriﬁcation spooﬁng and anti-
spooﬁng (SAS) corpus was introduced recently in [21] and used in ASVspoof 2015: Auto-
matic Speaker Veriﬁcation Spooﬁng and Countermeasures Challenge [25]4, that focused
on stand-alone synthetic speech detection involving both known and unknown attacks.
The ﬁndings of ASVspoof 2015 were disseminated at a special session of the latest edition
of Interspeech conference in Dresden, Germany5.

During the special session, several participating sites reported independently that
spectral phase-based features (such as cosine phase [28], modiﬁed group delay [28] and
RPS [31]) outperformed spectral magnitude-based features in synthetic speech detection
[34, 35, 36, 37]. GMM-based system [1] was used for modeling both natural and synthetic
speech class in most of the studies presented at the special session session [36, 35, 38].
Though some studies utilized more advanced support vector machines (SVM), deep neu-
ral networks (DNN) and probablistic linear discriminant analysis (PLDA) methods as
their back-ends, the performance of GMM systems was found generally better [39, 35, 34].
A similar observation was made in our preliminary study on ASVspoof 2015 data [27].
However, as in most of the studies, only a limited number of feature representation and
classiﬁers has taken into consideration, it is diﬃcult to make a conclusive remark about
the best combination of features and classiﬁers for synthetic speech detection.

2.3. Contribution of the present study: joint eﬀect of varied attacks and noise

In our two preliminary studies on ASVspoof 2015 data, we did extensive comparative
evaluation of several front-end [33] and back-end [27] synthetic speech detectors. In our
experiments, the simplest ideas tended to outperform more elaborate ones; for instance,
raw power spectrum features and maximum likelihood (ML) trained Gaussian mixture

4www.spoofingchallenge.org
5http://www.signalprocessingsociety.org/technical-committees/list/sl-tc/spl-nl/2015-11/

2015-11-ASVspoof/

4

models (GMMs) did a decent job both in detecting both unknown and known attacks,
while i-vector [40] based spooﬁng detection [24, 41] yielded much higher error rates.

The present study extends [33] and [27] towards an extended and self-contained com-
parative evaluation of synthetic speech detectors. Unlike [33] and [27], where we used
the original high-quality ASVspoof 2015 samples, in this study, we address the fourth
concern in existing spooﬁng countermeasures studies, robustness of spooﬁng detection
under imperfect conditions. In general, an acoustic signal reaching a recognizer can be
subjected to a number of extrinsic imperfections, induced by additive noise, transmis-
sion channel (including compression artefacts and low bandwidth), and reverberation,
to name a few. A limited number of earlier studies have executed spooﬁng experiments
on 8 kHz telephony data [42, 41], though under somewhat artiﬁcial scenario in which an
existing telephone-quality corpus has been processed through voice conversion attacks,
as opposed to the more likely case of spooﬁng attacks taking place before signal transmis-
sion. We argue that without an access to the original, undistorted signal it is diﬃcult,
if not impossible, to isolate the relative impact of spooﬁng artefacts and extrinsic dis-
tortions. Therefore, there is a clear call to examine spooﬁng attacks under controlled
extrinsic distortions to gain more insight as to what might be the important consider-
ations in developing practical countermeasures - information largely unknown to date.
A recent study [43] addressed the impact of bandwidth to synthetic speech detection
accuracy on the same ASVspoof 2015 corpus as used in the present study.

In contrast to the above prior studies, we focus solely on arguably one of the most
common and relevant sources of distortions, additive noise, that has received almost no
prior attention to the best of our knowledge6. Speciﬁcally, using the ASVspoof 2015
data, we provide a detailed performance assessment of several spooﬁng detectors under
additive noise contamination. We expect this to be a notoriously diﬃcult task that could
serve as a useful evaluation test-bench for developing new robust countermeasures more
relevant for end-user applications.

As state-of-the-art TTS and VC methods can produce high-quality speech, sometimes
close to or indistinguishable from authentic human speech (unit selection [45] is a good
example), we expect additive noise to mask further the already small diﬀerences between
human and synthetic speech. As a motivation, Fig. 1 displays spectrograms of natural
and synthetic speech signals of the same speaker and their noisy counterparts. While
diﬀerences of natural and synthetic speech apparent for the clean data, additive noise
makes it diﬃcult to tell the diﬀerence.

It is not obvious, for instance, whether standard speech enhancement techniques as
a pre-processing method will be helpful: as noise suppression is always traded-oﬀ with
speech distortion [46], processing artifacts due to speech enhancement could be confused
with artifacts due to synthesis vocoders. Similarly, as indicated above, the popular RPS
[32, 31] feature requires fundamental frequency tracking whose performance is aﬀected
by additive noise [47]. For these reasons, it is not obvious what type of front-end or back-
end modeling ideas will work comparatively better for synthetic speech detection under
noise. To answer these questions, we have selected state-of-the-art or otherwise popular

6An independent study, made publicly available almost in parallel to ours [44], considers the same
ASVspoof2015 database under additive noise contamination. Their noise contamination design is similar
to ours though spooﬁng detection features are mostly diﬀerent and our manuscript provides a more
thorough analysis.

5

Natural Speech

Synthetic Speech

)
z
H
k
(
 
y
c
n
e
u
q
e
r
F

8

6

4

2

0

1.5

2

2.5

3

)
z
H
k
(
 
y
c
n
e
u
q
e
r
F

8

6

4

2

0

1.5

2

2.5

3

Noisy Natural Speech (White Noise 0dB SNR)
8

Noisy Synthetic Speech (White Noise 0dB SNR)
8

)
z
H
k
(
 
y
c
n
e
u
q
e
r
F

6

4

2

0

1.5

2
Time (s)

2.5

3

)
z
H
k
(
 
y
c
n
e
u
q
e
r
F

6

4

2

0

1.5

2
Time (s)

2.5

3

Figure 1: Natural and synthetic speech signals of the same speaker and their noisy counterparts.

feature extraction methods based on our preliminary results in [33]. These features, de-
tailed below, include the standard MFCCs, inverted MFCCs (IMFCCs), subband spectral
centroid magnitude (SCMC) and recently introduced mean Hilbert envelope coeﬃcient
(MHEC) features that were reported to outperform MFCCs in speaker and language
recognition tasks [48]. From the classiﬁer side, we use GMMs trained via maximum like-
lihood (ML), reported as the best-performing one in [27], as well as the i-vector approach
[41, 24].

3. Spooﬁng Detection

Given a speech signal s, synthetic speech detection task is to decide whether s belongs
to a natural speech class — hypothesis H0, or a synthetic speech class — hypothesis H1.
The decision is based upon the log-likelihood ratio score, Λ:

Λ(s) = log p(s|H0) − log p(s|H1).

(1)

To estimate the probabilities p(s|H0) and p(s|H1) we need to train an acoustic model
for each hypothesis. In our recent anti-spooﬁng study on ASVspoof 2015 [27], we evalu-
ated a number of diﬀerent classiﬁcation techniques. Gaussian mixture models (GMM),
trained with maximum likelihood (ML) principle, was found the best choice.

GMM is a well-known probabilistic model that is extensively used for speaker recogni-
tion ever since it was introduced for the task [1]. We separately use natural and synthetic
training data to train two GMMs. Each GMM consists of a mixture weight wi, a mean
vector µi and a covariance matrix Σi for each mixture component i. We use expectation-
maximization (EM) algorithm to estimate the model parameters λ = {wi, µi, Σi}M
i=1,
where M is the number of mixture components.

6

After the two acoustical models are trained, the log-likelihood for each hypothesis
and a sequence of feature vectors X = {x1, . . . xT}, that represent the speech signal s,
takes the following form

log p(s|Hk) =

1
T

log p({x1, . . . xT}|λk) =

1
T

T

X

t=1

log (xt|λk).

Besides GMM, we also consider the i-vector paradigm [40], that became state-of-
the-art technique for text-independent speaker veriﬁcation. Recently, it was also used
to perform speaker veriﬁcation and anti-spooﬁng jointly in the i-vector space [24]. In
essence, i-vector w is a ﬁxed-sized low-dimensional vector per utterance that contains
both speaker- and channel-speciﬁc variability. To extract an i-vector, we factorize a
GMM mean supervector µ as µ = m + Tw, where T is a low-rank rectangular matrix,
m is a speaker-independent mean vector and w has a standard normal prior distribution.
Refer to [40] for more details.

We use two diﬀerent i-vector based classiﬁers to compute the ﬁnal score (1): cosine
similarity measure and probabilistic linear discriminant analysis (PLDA) [49]. Given
two i-vectors, extracted from target (wtgt) and test (wtst) utterances, we compute cosine
similarity between them as follows

cosine(wtgt, wtst) =

wT

tgtwtst

kwtgtkkwtstk

.

(2)

As the cosine similarity measure does not compute likelihoods, instead of Eq. (1) we

form the detection score as follows:

score = cosine( ˆwnat, wtst) − cosine( ˆwsynth, wtst),

(3)

where ˆwnat and ˆwsynth represent the average training i-vectors for natural and synthetic
speech classes, respectively.

Besides cosine scoring, we also consider the so-called simpliﬁed PLDA [50]. The idea
behind PLDA is to split total i-vector variability into speaker and channel components,
which allows eﬃcient inference during a test stage. To train the model, we grouped
together i-vectors from each synthesis method and from natural speech which gave us 6
classes (“speakers”). For more details on the data, refer to Section 6.1.

4. Natural v.s. Synthetic/Converted Speech

Before proceeding to recognition experiments, we ﬁrst wish to understand the acoustic
signal properties of the natural and synthetic speech signals. In order to analyze the
characteristics of natural and synthetic speech, long-term average spectra (LTAS) is
utilized. LTAS somewhat represents the physical characteristics of the speaker related
the vocal tract resonances [51] and is mostly used in audio forensics [52] and for measuring
the audibility of speech to compute speech intelligibility index [53]. LTAS is computed
by time averaging the short-term Fourier magnitude spectra of all frames:

LTAS(k) =

|St(k)|2,

T

X

t=1

1
T
7

(4)

)

B
d
(
 

m
u
r
t
c
e
p
S

 

 
r
e
w
o
P
e
g
a
r
e
v
A

100

80

60

40

20

0

−20

−40
 
0

1000

2000

3000

Frequency (Hz)

4000

5000

 

Human
S5
S4
S3
S2
S1

6000

7000

8000

Figure 2: Long-term average power spectra of synthetic and human speech signals (we used 2525 speech
ﬁles per each method to compute an average). The spectra have been shifted by 10 dB with respect to
each other.

where St(k) denotes the windowed discrete Fourier transform of tth speech frame of the
signal, s, at DFT bin k and T is the total number of speech frames after voice activity
detection (VAD). We compute the average LTAS of human and synthetic speech signals
using the training portion of the ASVspoof 2015 dataset for each synthesis/conversion
technique (S1-S5) to visualize their diﬀerences in frequency domain. Fig. 2 displays the
LTAS computed using synthetic and natural speech signals (average LTAS is computed
using 2525 speech ﬁles per method). Synthetic speech power is attenuated below 4 kHz
compared to natural speech. For f > 4 kHz, the opposite happens and the diﬀerence
between human and synthetic speech signals are larger. Especially for S3 and S4, hidden
Markov model (HMM)-based speech synthesis techniques, the relative diﬀerence between
human and synthetic speech are higher than for the other synthesis/conversion tech-
niques. Interestingly, when f > 7 kHz, larger diﬀerences occur between other conversion
techniques and natural speech.

It is well known that additive noise drastically reduces the speaker, language and
speech recognition performances. Several methods to cope with the adverse eﬀects of
additive noise contamination have been proposed. Speech enhancement techniques aim
to improve the quality of the signal corrupted by noise in the signal level. Cepstral
mean subtraction (CMS) [54], cepstral mean and variance normalization (CMVN) and
RASTA ﬁltering [55] are the popular feature level methods to suppress linear channal
bias in cepstral features, often yielding increased speaker recognition accuracy. Speaker,
language and speech recognition under additive noise and mismatched channel conditions
are well-studied and several techniques have been proposed to improve the performance.
However, since spooﬁng detection has only recently been drawn attention, its performance
under degradation and possible solutions for mismatched conditions are unknown. Thus,
a thorough analysis on the eﬀect of noise is necessary for the anti-spooﬁng research.

In this study, we consider three noise types: (i) white noise, (ii) babble noise and (iii)

8

0

−10

−20

−30

−40

−50

)

B
d
(
 

S
A
T
L

−60
 
0

1000

2000

 

White Noise
Babble Noise
Car Noise

6000

7000

8000

3000

Frequency (Hz)

4000

5000

Figure 3: Long-term average power spectra of diﬀerent noise types used in the experiments.

Table 1: Summary of the features and their parameters used in this study.

Features Frame length/shift

MFCC
IMFCC
SCMC
MHEC
RPS

20 ms/10 ms
20 ms/10 ms
20 ms/10 ms
20 ms/10 ms
20 ms/10 ms

# DFT

Filters

bins
512
512
512

-

512

Type

#
Triangular
32
32
Triangular
32 Rectangular
32 Gammatone
32
Triangular

Scale
Mel
Mel

Linear
ERB
Mel

Coeﬃcients

c0 − c31

c0 − c31

c0 − c31

c0 − c31

c0 − c31

car noise. The LTAS variations of each noise type are shown in Fig. 3.

5. Feature Extraction Methods

Speech features representing short-term spectral features, which are mostly used for
speech and speaker recognition, are also employed in speech-based spooﬁng detection. A
comparative evaluation of a large number of speech features for this task is available in
[33]. In this paper, we focus on the most promising (or otherwise popular) features for
noise-robust spooﬁng detection, namely, mel-frequency cepstral coeﬃcients (MFCCs),
inverted mel-frequency cepstral coeﬃcients (IMFCCs), spectral centroid magnitude coef-
ﬁcients (SCMCs) and relative phase shift (RPS) features. MFCC and IMFCC are based
on ﬁlter bank analysis, SCMC contains detailed information of subband while RPS carries
phase-related information. In addition, we also evaluate recently proposed mean Hilbert
envelope coeﬃcient (MHEC) feature used successfully for robust speaker and language
recognition [48]. The features and their parameters used in this study are summarized
in Table 1. In the following, we brieﬂy describe the features.

9

1

0.5

0
0

1

0.5

0
0

Mel−Filterbank

1000

2000

3000

4000

5000

6000

7000

8000

Inverted Mel−Filterbank

1000

2000

3000

4000

Frequency (Hz)

5000

6000

7000

8000

Figure 4: Triangular ﬁlters spaced on Mel and inverted-Mel scale.

5.1. Mel-frequency Cepstral Coeﬃcients (MFCC)

In short-term speech processing, the speech signal is ﬁrst divided into short overlap-
ping frames (here 20 ms frames with 10 ms overlap is used). Then, the power spectrum
of each Hamming windowed frame is computed using discrete Fourier transform (DFT)
by

|X[k]|2 =

N −1

X

n=0

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

x[n]e−j2πkn/N (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

2

0 ≤ k ≤ K − 1,

(5)

where, k is the DFT bin and x = [x[0], . . . , x[N − 1]] is a windowed speech frame
(assumed to be zero outside of the interval [0, N −1]). In standard ﬁlterbank based feature
extraction schemes, the power spectrum is processed using a set of overlapping band-pass
ﬁlters. Logarithmic ﬁlter bank outputs are then converted into cepstral coeﬃcients by
applying discrete Cosine transform (DCT). Generally, triangular ﬁlters spaced in mel-
scale are used as ﬁlterbank and the resulting features are the mel-frequency cepstral
coeﬃcients (MFCCs).

5.2. Inverted Mel-frequency Cepstral Coeﬃcients (IMFCC)

In MFCCs, ﬁlters have denser spacing in low-frequency region. Similarly, IMFCC
features are extracted using inverted-mel scale [56]. It is implemented by ﬂipping the
mel-scaled ﬁlter bank in frequency axis giving more emphasis on high-frequency region.
Fig. 4 shows an example of triangular ﬁlters spaced on Mel and inverted-Mel scale.
Otherwise, all the processing steps remain the same as in MFCC extraction.

5.3. Spectral Centroid Magnitude Coeﬃcients (SCMC)

Spectral centroid magnitude contains speech information similar to magnitude at the
formant frequencies [57]. We have found that this subband features representing this
information outperformed other related features in spooﬁng detection [33]. Spectral cen-
troid magnitude coeﬃcients (SCMCs) are computed as follows. First, spectral centroid

10

magnitude (SCM) for the ith subband of speech frame is computed as:

SCMi = PK/2

k=0 f [k] |X[k]| wi[k]
PK/2

k=0 f [k]wi[k]

,

(6)

where f [k] is the normalized frequency (0 ≤ f [k] ≤ 1) and wi[k] is a window function
in the frequency domain (here rectangular window is used) for computing the centroid
of the i-th subband. In the next step, the logarithm of SCM values are computed and
converted into feature coeﬃcients (SCMCs) by taking the DCT.

5.4. Relative-Phase Shift (RPS) Features

The relative phase shift (RPS) features [30, 32, 31] are based on harmonic modeling
of the speech signal. In harmonic modeling, each frame is approximated as the sum of
sinusoids in the form:

(7)

(8)

x[n] = X

Ak[n] cos(φk[n]),

where Ak[n] is the amplitude and

k

φk[n] = 2πkF0n + θk

is the instantaneous phase of the kth harmonic. F0 is the fundamental frequency and
θk is the initial phase of the kth harmonic. The instantaneous phase depends on the
time instant n and harmonic, k, whereas the initial phase, θk, is independent of the time
instant. The RPS value is the phase shift of the kth harmonic component with respect to
fundamental frequency [30, 32, 31]. It is calculated by solving for θk by equating the time
instants ni in (8) between the kth harmonic and the reference fundamental frequency,
assuming θ1 = 0:

θk = φk[ni] − kφ1[ni],

(9)

We used COVAREP tool [58] to compute the RPS values. COVAREP tool uses 100
ms frames with 10 ms frame shift for computing the F0. The RPS features are computed
from the RPS values by performing phase unwrapping and then diﬀerentiation followed
by mel-scale integration and DCT as in [30, 32]. Similar to other front-end conﬁgurations,
the 0th coeﬃcient is included.

5.5. Mean Hilbert Envelope Coeﬃcients (MHEC)

Gammatone ﬁlter bank based features are sometimes used in speech and speaker
recognition especially under mismatched and reverberated speech conditions [48, 59, 60].
In general, the speech signal is ﬁrst processed by a bank of Gammatone ﬁlters that are
equally spaced on the equivalent rectangular bandwith (ERB) scale between 100 and
8000 Hz (assuming the speech signal is sampled at 16 kHz). In this study we used the
Gammatone ﬁlterbank implementation provided by Auditory toolbox [61]. An example
of Gammatone ﬁlterbank consists of 16 ﬁlters is shown in Fig. 5.

Mean Hilbert envelope coeﬃcients (MHECs) were recently proposed for noise robust
speech, speaker and language recognition [48, 62]. It uses the output of each ﬁlter in the
ﬁlterbank. Calculation of MHEC features is performed through the following steps:

11

0

−5

−10

−15

−20

−25

−30

−35

)

B
d
(
 

e
s
n
o
p
s
e
R
 
y
c
n
e
u
q
e
r
F

−40

102

103

Frequency (Hz)

Figure 5: Gammatone ﬁlterbank, for MHEC features, consisting of 16 ﬁlters.

1. First, the speech signal is passed through a Gammatone ﬁlterbank consisting of 32
ﬁlters and for each Gammatone ﬁlter output, the temporal envelope, the squared
magnitude of the analytical signal is obtained using the Hilbert transform.

2. The envelope is smoothed by applying a low pass ﬁlter with cut-oﬀ frequency of

fc = 20 Hz.

3. Short-term energy is computed from each smoothed envelope by framing and win-

dowing.

4. MHECs are computed from the energies using logarithmic compression followed by

DCT.

6. Experimental Setup

6.1. Database

The experiments are conducted on the ASVspoof 2015 database [25] which consists
of speech data with no channel or background noise collected from 106 speakers (45 male
and 61 female) and three subsets with non-overlapping speakers:

• Training subset is used to train genuine and spoofed classes for spooﬁng detection.
It contains natural and ﬁve diﬀerent types of spoofed speech: three are generated
using voice conversion and the rest using speech synthesis. Voice conversion algo-
rithms are (i) frame-selection (S1), (ii) spectral slope shifting (S2) and (iii) Festvox
(S5) system7 whereas the speech synthesis spoofs are based on hidden Markov
model-based methods (S3 and S4).

• Development subset is used to optimize spooﬁng detectors. It contains the same

ﬁve spooﬁng methods (S1-S5) as used in the training subset.

• Evaluation subset is used for evaluating the ﬁnal performance of the system. It
contains ﬁve “known” algorithms seen in the training and development subsets
(S1-S5) as well as ﬁve “unknown” algorithms (S6-S10).

7http://www.festvox.org

12

Training Set

0.12

0.1

0.08

0.06

0.04

0.02

y
c
n
e
u
q
e
r
F
e
v
i
t

 

l

a
e
R

0
0

20

40

SNR (dB)

60

80

0.12

0.1

0.08

0.06

0.04

0.02

0
0

Development Set

20

40

SNR (dB)

60

80

0.12

0.1

0.08

0.06

0.04

0.02

0
0

Evaluation Set

20

40

SNR (dB)

60

80

Figure 6: Distributions of estimated SNR levels for each subset of ASVspoof 2015 dataset.

Table 2 summarizes speaker and utterance information for each subset.

Table 2: Statistics of the ASVspoof 2015 database, used in the experiments

Subset

Number of speakers
Male

Female

Number of utterances
Synthetic
Natural

Training
Development
Evaluation

10
15
20

15
20
26

3750
3497
9404

12625
49875
184000

In order to analyse the original ASVspoof 2015 data in terms of noise level and to
show the quality of recordings in database before interpreting the results, we computed
the SNR level of recordings. Fig. 6 shows the histograms of estimated SNR levels8 for
each subset of the original ASVspoof 2015 dataset. All the speech ﬁles from the training
set are used to plot histogram for this subset, whereas randomly selected 30000 speech
signals are used to generate histograms for Evaluation and Development subsets. A vast
majority of the signals have a relatively high SNR exceeding 20 dB. Interestingly, the
evaluation subset includes also signals with higher SNRs (approximately 8% of 30000
ﬁles have SNR > 50 dB).

We use Filtering and Noise Adding Tool (FaNT)9 to corrupt the original ASVspoof
2015 signals with noise for introducing controlled degradation. FaNT is an open-source
tool which follows the ITU recommendations for noise adding and ﬁltering. To be more
precise, it uses psychoacoustic speech level computation based on the ITU recommenda-
tion P.56 (objective measurement of active speech level ). We digitally add white, babble
and car noises from NOISEX-92 database [63]. For each noise type we consider 3 SNR
levels: 0, 10 and 20 dB. The reasons for selecting these types of noise are the following:
(i) white noise has a ﬂat spectral density and it masks all the frequency components uni-
formly. Although it rarely represents a real-case scenario, it is a commonly used control
noise in studying robust speech processing methods. (ii) Babble noise is one of the most
diﬃcult noise types in speech applications because it is a mixture of multiple speakers —

8SNREval Toolkit from http://labrosa.ee.columbia.edu/projects/snreval/ is used to estimate

the SNR levels.

9http://dnt.kr.hsnr.de/

13

a situation that occurs on a daily basis in any crowded place [64].
(iii) Car noise is
another noise type that may frequently occur in our daily life such as making a phone
call while driving.

In the experiments, both matched and mismatched conditions between training and
test are considered. Matched-noise condition refers to training natural and synthetic
models using the same noise type and SNR as will present in test. Mismatched condi-
tion, in turn, refers to a scenario where we train natural and synthetic classes using the
original clean training ﬁles but noisy ﬁles are used in test.

6.2. Classiﬁer and Features

We use 32 coeﬃcients (including c0) and 32 ﬁlters in ﬁlterbank for every method. This
is done to have comparable results for diﬀerent feature extraction methods. We apply
energy-based voice activity detection (VAD) [65, p. 24] on clean data to get speech/non-
speech labels. Using clean VAD labels allows us to focus merely on the eﬀect of noise
on synthetic speech detection rather than mixed eﬀects of VAD and feature set. These
labels are used to discard non-speech frames for both clean and noisy speech.

For GMM-based classiﬁcation, we use two models to represent natural and synthetic
speech classes (see details in Section 3). GMM for each class has 512 components and is
trained using 5 EM iterations (the performance diﬀerences for larger number of compo-
nents were negligible in our initial experiments).

For i-vector based classiﬁcation, we train a gender-independent universal background
model (UBM) consisting of 512 Gaussians using 9000 utterances from 150 male and 150
female speakers from the WSJ0 & 1 corpora [66]. To train the T-matrix, we select 8945
utterances from 178 male and 177 female speakers from the WSJ0 & 1 databases and
run EM-algorithm for 5 iterations. The extracted 600 dimensional i-vectors are further
processed by applying within-class covariance normalization (WCCN) [67], followed by
projection to the unit sphere [68]. The logic behind WCCN is not to normalize within-
speaker variation [40], like it is done for speaker recognition, but to normalize within-class
(natural or synthetic) variation. To this end, we separate the training data into natural
and synthetic classes and use them to compute WCCN transformation matrix B [40, p.
791]. For mismatched case, we train PLDA on original clean data. For matched case, we
use noisy data.

6.3. Performance Measure

Equal error rate (EER) is used as the performance criterion in the experiments. EER
corresponds to the threshold at which false acceptance (Pfa) and miss rate (Pmiss) are
equal. Pfa is the ratio of number of spoof trials detected as genuine speech to the total
number of spoof trials and Pmiss is the ratio of number of genuine trials detected as
spoofed to the total number of genuine trials. The EERs reported in this work were
computed using the bosaris toolkit10 which computes the EER on receiver operating
characteristics (ROC) convex hull (ROCCH) that is an interpolated version of standard
ROC.

10https://sites.google.com/site/bosaristoolkit/

14

White Noise

Babble Noise

Car Noise

50

40

)

%

(
 

R
E
E

30

20

10

0

50

40

30

20

10

0

)

%

(
 

R
E
E

Orig.

20
10
SNR (dB)

0

White Noise

Orig.

20
10
SNR (dB)

0

50

40

)

%

(
 

R
E
E

30

20

10

0

50

40

30

20

10

0

)

%

(
 

R
E
E

Orig.

20
10
SNR (dB)

0

Babble Noise

Orig.

20
10
SNR (dB)

0

50

40

)

%

(
 

R
E
E

30

20

10

0

50

40

30

20

10

0

 

)

%

(
 

R
E
E

Orig.

20
10
SNR (dB)

0

Car Noise

 

MFCCs
MFCCs+∆
MFCCs+∆+∆∆
MFCCs+∆+∆∆+CMS

Orig.

20
10
SNR (dB)

0

Figure 7: Eﬀects of ∆ and ∆∆ MFCC features and Cepstral Mean Subtraction on synthetic speech
detection. First row, mismatched condition: Clean Training-Noisy Test. Second row, matched condition:
Noisy Training-Noisy Test.

7. Results

We conduct the experiments separately on the development and evaluation parts of
ASVspoof 2015. The Development part is ﬁrst used for optimizing the system parameters
and conﬁgurations. The feature extraction method that yield the lowest EERs are then
selected for further experiments on the evaluation part.

7.1. Eﬀect of Feature Post-Processing

In our ﬁrst experiment on the development set, we study the eﬀect of feature post-
processing. Speciﬁcally, we study the appending ∆ and ∆∆ features and cepstral mean
subtraction (CMS). The results on MFCC features are shown in Fig. 7. The upper row
corresponds to the noise-mismatched and the lower row to the noise-matched condition.
For the original (clean) case, 2.24% EER is obtained using only the base MFCCs. Ap-
pending ∆ and ∆∆ features reduces the EER to 0.49%. Applying CMS slightly reduces
the performance for the clean case (0.84% EER).

For the noisy case, appending the ∆ and ∆∆ coeﬃcients considerably improves the
accuracy in most cases. For example, we see 78% relative improvement over the base
MFCCs for babble noise at 20 dB SNR under noise mismatch (EER 16.29% → 3.61%).
Similarly, applying CMS on top of the dynamic features improves performance especially
in the noise-mismatched case. For the matched noise case, in turn, CMS mostly increases
EERs slightly in comparison to MFCC+∆ + ∆∆ conﬁguration (red vs. yellow bars in

15

Table 3: Comparison (EER, %) of diﬀerent front-end features in noisy conditions on development set
using Gaussian Mixture Model classiﬁer. The results for clean original condition are presented as well
as the average results for all noisy sub-conditions.

Original Training - Noisy Test

Noise
type

SNR
(dB)

Original

White

Babble

Car

20
10
0
20
10
0
20
10
0

Average

Noise
type

SNR
(dB)

White

Babble

Car

20
10
0
20
10
0
20
10
0

Average

MFCC IMFCC SCMC MHEC RPS

0.84
15.75
24.13
31.42
7.23
15.32
31.05
3.51
7.48
16.44
16.92

0.91
34.17
44.56
48.86
5.66
15.4
37.73
1.94
4.69
14.27
23.03

0.38
21.91
32.19
39.86
2.71
9.36
30.09
0.87
2.48
8.74
14.85

3.92
12.08
22.2
33.37
11.06
25.58
40.87
8.96
19.47
33.12
21.06

0.15
37.64
41.37
43.61
5.26
20.04
39.90
0.74
5.75
24.03
21.84

Noisy Training - Noisy Test

MFCC IMFCC SCMC MHEC RPS

5.41
9.77
18.28
3.73
7.52
26.52
3.09
5.33
8.87
9.83

20.72
38.58
47.27
3.70
11.82
37.68
2.22
4.17
10.24
19.60

11.33
20.85
39.26
2.23
6.63
26.83
0.95
2.11
5.79
12.88

8.72
10.17
17.33
8.91
12.66
29.66
7.14
10.28
13.24
13.12

8.58
18.04
28.65
1.60
6.23
32.36
0.40
2.95
18.00
12.97

the second row of Fig. 7). For the noisiest case of white and car noises, CMS slightly
boosts the performance (e.g. EER 20.04% → 18.28% at 0 dB SNR for white noise).

The results in Fig. 7 are for the MFCC features. In general, the results were similar
for the other studied features, except RPS. Out from the 19 cases conditions evaluated
(3 SNRs x 3 noise types under noise-matched and noise-mismatched cases, plus the clean
data), the raw RPS features without deltas or feature normalization yielded the lowest
EERs in 14 cases. Thus, in all the remaining experiments, we will adopt the raw RPS
features. For all the rest of the features, we include deltas and feature normalization
(CMS).

7.2. Comparison of Features

The results on development set for diﬀerent features using GMM are summarized in
Table 3. For the clean (original) case, the RPS features yield the lowest EER. However,
under additive noise, especially for white noise and at low SNR levels of car and babble

16

White Noise

Babble Noise

Car Noise

 

 

 

)

 

%
n
i
(
 
y
t
i
l
i

b
a
b
o
r
p
 
s
s
M

i

  40  

  20  

  10  

  5   

  2   

 

No Enh. (EER = 33.0%)
Mag. Spect. Subt. (EER = 34.4%)
Power Spect. Subt. (EER = 37.1%)
Wiener Filt. (EER = 35.7%)

  2   

  5   

  10  

  20  

False Alarm probability (in %)

  40  

)

 

%
n
i
(
 
y
t
i
l
i

b
a
b
o
r
p
 
s
s
M

i

  40  

  20  

  10  

  5   

  2   

 

No Enh. (EER = 32.4%)
Mag. Spect. Subt. (EER = 35.0%)
Power Spect. Subt. (EER = 35.2%)
Wiener Filt. (EER = 35.5%)

  2   

  5   

  10  

  20  

False Alarm probability (in %)

  40  

)

 

%
n
i
(
 
y
t
i
l
i

b
a
b
o
r
p
 
s
s
M

i

  40  

  20  

  10  

  5   

  2   

 

No Enh. (EER = 18.9%)
Mag. Spect. Subt. (EER = 22.3%)
Power Spect. Subt. (EER = 21.7%)
Wiener Filt. (EER = 25.0%)

  2   

  5   

  10  

  20  

False Alarm probability (in %)

  40  

Figure 8: DET curves for diﬀerent speech enhancement techniques under additive noise (0 dB) for
mismatched condition (Clean Training-Noisy Test).

noises, the performance of RPS is relatively poor. This could be because RPS requires
estimated F0 values that are diﬃcult to compute from noisy data. For babble and
car noises at high SNRs (20 dB), RPS yields reasonable accuracy both under noise-
mismatched and matched conditions. The SCMC features perform well for the babble
and car noises, whereas for white noise, MHEC yields lower EERs. To sum up Table 3,
none of the feature sets is consistently superior to others. For the noise-mismatched case,
SCMC outperforms the other features in most cases, whereas MFCCs yield the lowest
average EERs in matched-noise case.

7.3. Eﬀect of Speech Enhancement

Next, we study the eﬀect of speech enhancement techniques. To this end, magnitude
and power spectral subtraction algorithms [69, 70] and Wiener ﬁltering [71] approaches
are adopted. Detection error trade-oﬀ (DET) curves plotted for diﬀerent speech enhance-
ment methods for each noise type, at 0 dB SNR and using MFCC features with deltas
and CMS, are shown in Fig. 8. Here, the DET curves are generated by pooling the scores
of all the individual attacks11. Fig. 8 indicates that the attempted speech enhancement
techniques do not yield any performance gains. Apart from these three popular meth-
ods, other methods including minimum mean square error (MMSE), logarithmic MMSE
(logMMSE) and iterative Wiener ﬁltering techniques (as available in the Appendix of
[72]) were studied, without success.

7.4. i-vector Countermeasures from Diﬀerent Features

Up to this point, we have utilized the computationally light GMM classiﬁer to study
diﬀerent feature conﬁgurations. In our last experiments with the development set, we
study an i-vector based countermeasure. To this end, i-vector extractors are trained from
scratch for all the ﬁve acoustic feature sets. The results are provided in Table 4. The
accuracy of the i-vector countermeasure is inferior to GMM classiﬁer independent of the

11Although in ASVspoof 2015 the evaluation metric is averaged EER over diﬀerent attacks, producing
a single DET curve that would coincidence with this operating point is not obvious. Thus, here the
scores are pooled to generate the DET plot and to compute the corresponding EERs in Fig. 8 legends.

17

Table 4: Comparison (EER, %) of diﬀerent front-end features in noisy conditions on development set
using Cosine/Probabilistic Linear Discriminant Analysis i-vector classiﬁers. The results for clean original
condition are presented as well as the average results for all noisy sub-conditions.

SNR (dB)
Original

MFCC

5.12 / 5.03

20
10
0
20
10
0
20
10
0
Average

26.48 / 25.32
36.35 / 34.87
43.47 / 43.67
20.94 / 20.65
33.59 / 33.13
45.71 / 45.82
24.00 / 24.35
33.67 / 32.50
39.62 / 38.01
30.89 / 30.33

Original Training - Noisy Test

IMFCC

3.24 / 4.05
45.51 / 44.64
47.60 / 47.74
48.26 / 48.58
28.07 / 28.03
40.54 / 39.62
48.15 / 48.23
13.56 / 14.53
26.28 / 26.05
40.39 / 39.07
34.16 / 34.05

SCMC

5.3 / 6.03

39.97 / 39.04
44.15 / 44.11
46.68 / 46.95
24.44 / 24.48
34.97 / 34.55
45.02 / 44.66
14.34 / 14.89
22.61 / 23.14
33.12 / 32.51
31.06 / 31.03

MHEC

12.31 / 12.7
26.05 / 28.23
30.71 / 33.20
39.20 / 40.49
25.58 / 26.07
33.54 / 33.66
43.65 / 43.53
22.53 / 22.77
27.76 / 28.27
34.76 / 35.58
29.60 / 30.45

SNR (dB)

MFCC

IMFCC

SCMC

Noisy Training - Noisy Test

20
10
0
20
10
0
20
10
0
Average

19.69 / 11.43
25.21 / 20.59
33.82 / 27.36
16.29 / 12.86
21.65 / 19.66
34.20 / 29.87
15.24 / 11.68
18.98 / 15.08
20.60 / 18.87
22.85 / 18.60

31.63 / 25.33
36.74 / 33.41
43.99 / 41.07
16.77 / 13.66
27.05 / 21.29
39.17 / 32.41
10.14 / 6.85
15.53 / 11.19
26.01 / 18.27
27.44 / 22.60

28.03 / 21.10
29.57 / 28.07
39.65 / 35.45
16.52 / 14.30
23.05 / 20.32
36.34 / 31.39
11.78 / 9.44
16.11 / 12.74
22.20 / 17.99
24.80 / 21.20

MHEC

21.46 / 18.02
25.89 / 21.36
35.05 / 30.30
22.55 / 20.12
29.46 / 26.30
39.65 / 35.34
19.74 / 18.56
25.00 / 21.63
28.93 / 25.92
27.52 / 24.17

e
t
i

h
W

e
l

b
b
a
B

r
a
C

e
t
i

h
W

e
l

b
b
a
B

r
a
C

RPS

5.18 / 5.03

39.97 / 43.09
45.24 / 46.77
47.60 / 47.44
19.10 / 23.15
31.03/ 35.02
43.73 / 45.42
11.88 / 13.79
22.14 / 26.11
38.34 / 42.19
30.42 / 32.80

RPS

20.18 / 18.54
23.71 /23.36
26.00 / 25.59
11.20 / 9.77
18.47 / 15.98
35.35 / 31.94

8.39 / 8.80

14.43 / 14.72
29.12 / 28.38
20.76 / 19.67

noise type and feature extraction methods. For example, the lowest EER obtained by the
i-vector system on the clean data is 3.24%, while GMM yields 0.15% EER (see Table 3).
This holds for the additive noise case as well. Similar results for GMM and i-vector
techniques were found in our recent classiﬁer comparison paper for synthetic speech
detection [27]. This could be because of the short duration of recordings (approximately
3 seconds) that ASVspoof 2015 consists of.

Similar to GMM experiments under additive noise (Table 3), none of the features are
systematically superior to others. For the mismatched conditions, the features that yield
the lowest EERs are diﬀerent for each noise type and SNR level. For the matched-noise
condition, in turn, MFCCs yield the highest performance for white and babble noises
whereas for the car noise, IMFCCs are superior to other features at high SNRs (20 and
10 dB). Concerning the two i-vector back-end variants, PLDA does not bring substantial
improvements in comparison to cosine scoring for mismatched condition. However, for
the matched-noise condition, PLDA improves the performance considerably for all the
features. In general, MFCCs yield the lowest EERs under additive noise contamination.
In the next experiments on Evaluation set, MFCC and SCMC features using GMM and
i-vector techniques will be considered.

18

7.5. Results on Evaluation Set

In the experiments with the evaluation portion of ASVspoof 2015, we ﬁrst study the
performance of each individual attack using clean data with MFCC and SCMC features.
The EERs obtained with GMM and i-vector techniques for each individual attack are
summarized in Table 5 for the MFCC and SCMC features. Similar to observations found
on the development set, GMM outperforms both i-vector scoring variants independent
of the attack type and the features. Independent of the classiﬁer and features, S10 —

Table 5: Comparion (EER, %) of Gaussian Mixture Model classiﬁer and two i-vector based classiﬁers:
Cosine scoring and Probabilistic Linear Discriminant Analysis. We consider individual attacks on clean
evaluation set using MFCC and SCMC features.

Features Classiﬁer

C
C
F
M
C
M
C
S

GMM
Cosine
PLDA
GMM
Cosine
PLDA

S1

0.00
2.89
3.26
0.00
4.24
5.29

S3

Known Attacks
S2
S4
3.54
9.26
9.67
1.22
12.31
12.72

0.00
2.67
2.16
0.05
2.08
2.61

S5
0.70
6.01
5.84
0.60
5.46
5.76

S6
1.10
8.07
8.23
0.46
7.64
8.33

Unknown Attacks
S7
0.80
3.64
3.55
0.07
3.03
3.76

S9
0.11
3.07
3.29
0.02
2.45
3.14

S8
0.53
5.03
6.97
0.31
2.73
4.72

S10

27.34
46.49
47.11
29.92
44.17
46.47

0.00
2.66
2.39
0.02
2.27
2.90

the speech synthesis algorithm that uses MARY text-to-speech system12— is the most
diﬃcult attack type to detect in comparison to the other unknown attacks (S6-S9). This
could be because S10 does not use any vocoder in generating the synthetic speech signals
whereas the popular STRAIGHT vocoder [73] is used in most of the remaining attacks.
Thus, spooﬁng detectors trained with a STRAIGHT vocoder but tested without it will
induce a mismatch between the training and the test samples [25], making detection of
S10 relatively more diﬃcult. In general, SCMC features yield lower EERs than MFCCs
with the GMM classiﬁer except for S10. For the two scoring variants of i-vector, in turn,
MFCCs outperform the SCMC features, except for S10. Overall, S10 yields extremely
high EERs while reasonable accuracies are obtained for the other attacks. Since GMM
outperformed i-vectors, only GMM results will be presented in the remaining experiments
on the Evaluation set.

The results for the noise-contaminated evaluation set obtained with GMM using
MFCC and SCMC features are given in Table 6. MFCCs yield lower EERs than SCMCs
under white noise for both known and unknown attacks. For the babble and car noises, in
turn, SCMCs outperform MFCCs for both noise-mismatched and matched cases. Similar
to results on Development Set (Table 4), considerable reduction in EERs is obtained using
SCMC features over MFCCs under car and babble noise cases.

To analyze the joint eﬀect of additive noise and unknown attacks, the EERs obtained
using MFCCs for babble noise are selected from Table 6 as an example and represented
in Table 7 in a diﬀerent format. The average EERs for the unknown attacks are always
higher than those of the known attacks regardless of the noise and the SNR level, as
expected. The noise mismatch has higher impact on known attacks than on unknown

12http://mary.dfki.de/

19

Table 6: Comparison (EER, %) of known and unknown attacks for Gaussian Mixture model classiﬁer
on evaluation set. In each row, the lowest EERs are bolded and underlined for the known and unknown
attacks, respectively.

Original Training - Noisy Test

Noise
type

SNR
(dB) Known Unknown Known Unknown

MFCC

SCMC

Original

White

Babble

Car

Noise
type

White

Babble

Car

20
10
0
20
10
0
20
10
0

0.85
16.43
25.45
35.07
7.48
15.59
33.54
3.57
7.31
17.33

5.98
21.43
31.81
41.30
12.50
18.88
32.63
8.31
11.31
20.21

0.38
19.92
33.36
43.73
2.15
8.32
29.74
0.79
2.16
8.59

6.16
20.63
35.36
43.82
8.90
13.31
29.98
6.82
8.49
13.84

Noisy Training - Noisy Test

SNR
(dB) Known Unknown Known Unknown

MFCC

SCMC

20
10
0
20
10
0
20
10
0

4.90
9.00
17.10
3.56
6.77
23.99
2.87
4.87
7.71

10.58
17.21
27.44
7.38
10.42
27.57
5.87
7.63
10.81

10.06
18.90
40.71
1.99
5.94
24.89
0.88
1.89
5.09

13.22
23.15
43.50
6.15
9.78
29.40
5.02
5.80
8.65

20

Table 7: Comparison (EER, %) of known and unknown attacks for diﬀerent levels of babble noise using
MFCC features and Gaussian Mixture Model classiﬁer.

Noise
Matched
Mismatched

Attack
Known
Known
Unknown Matched
Unknown Mismatched

Clean
0.85
—
5.98
—

20 dB 10 dB 0 dB
23.99
3.56
33.54
7.48
27.57
7.38
12.50
32.63

6.77
15.59
10.42
18.88

attacks. For 20 dB SNR, for example, 110% relative increase in the error rate is observed
for the known attacks (EER 3.56% → 7.48%) whereas for the unknown attacks, the
corresponding increment is 69% (EER 7.38% → 12.50%). For the attack type, in turn
(comparing the 1st row with 3rd row or 2nd row with 4th row), the performance reduction
in the presence of unknown type of attacks are higher in matched-noise case than that
of noise-mismatched case. Similar observations hold for the other noise types, too.

8. Conclusion

Existing studies on synthetic speech detection usually consider high-quality noise-
free microphone recordings to investigate diﬀerent feature extraction and classiﬁcation
techniques. In this study, we have addressed the robustness of the spooﬁng detection
systems in matched and mismatched noise conditions. We have analyzed ﬁve diﬀerent
feature extraction techniques, which eﬀectively detect synthetic speech in clean condi-
tions. We have also tried diﬀerent speech enhancement techniques. As a back-end, we
have evaluated two well-known classiﬁer approaches: Gaussian Mixture Models (GMM)
and i-vector.

Our results on ASVspoof 2015 dataset indicate that: (i) Additive noise contamina-
tion considerably complicates synthetic speech detection. The eﬀect is worst for white
noise, as might be expected. For example, in an experiment with development set, EER
increased from 0.84% to 31.42% with MFCC features and GMM back-end under 0 dB
SNR and white mismatched-noise condition. (ii) RPS features — which were successfully
used in many spooﬁng detection studies and outperform other features such as MFCC
in clean conditions — generally yield higher EERs than standard MFCC features in the
presence of additive noise. (iii) Standard speech enhancement techniques, such as mag-
nitude spectral subtraction, power spectral subtraction and Wiener ﬁltering, were not
found helpful. (iv) GMM classiﬁer trained using maximum likelihood criterion consider-
ably outperforms more sophisticated state-of-the-art i-vector PLDA system for clean as
well as all noisy conditions studied in this paper. For example, in presence of white noise,
the average EERs of synthetic speech detection are 16.92% and 30.33%, respectively, for
GMM and i-vector PLDA system. It is well-known that i-vector based recognizers do
not perform well on short data, as was the case for ASVspoof 2015.

The results on the evaluation section of ASVspoof 2015 also reveals that detecting
unknown attacks is more diﬃcult than detecting known attacks in both matched and
mismatched noisy condition. Moreover, from a detailed study on attack-speciﬁc perfor-
mances with clean speech data, we ﬁnd that the notable performance diﬀerence between

21

known and unknown attacks is mostly due to the technique S10 (i.e., MARY TTS)
which does not use any vocoder as other synthetic speech generation techniques used in
ASVspoof 2015. This was the general observation regarding diﬀerent systems submitted
to ASVspoof 2015.

In this work, we have given emphasis on analyzing the eﬀect of noise in synthetic
speech detection with various existing features and classiﬁers. Our results suggest that
synthetic speech detection becomes more challenging in noisy conditions similar to speaker
recognition in noisy environment. This study opens a few potential directions for future
work. Besides the development of robust approaches for both front-end and back-end
sides of spooﬁng detection systems, study of speaker recognition under the joint presence
of spooﬁng attack and noise is also interesting to investigate in the context of trustwor-
thiness of voice-biometric systems.

References

[1] D. A. Reynolds and R. C. Rose, “Robust text-independent speaker identiﬁcation using Gaussian
mixture speaker models,” IEEE Transactions on Speech and Audio Processing, vol. 3, no. 1, pp.
72–83, 1995.

[2] A. K. Jain, A. Ross, and S. Pankanti, “Biometrics: A tool for information security,” IEEE Trans-

actions on Information Forensics and Security, vol. 1, no. 2, pp. 125–143, 2006.

[3] N. K. Ratha, J. H. Connell, and R. M. Bolle, “Enhancing security and privacy in biometrics-based

authentication systems,” IBM Systems Journal, vol. 40, no. 3, pp. 614–634, 2001.

[4] N. W. D. Evans, T. Kinnunen, J. Yamagishi, Z. Wu, F. Alegre, and P. L. D. Leon, “Speaker
recognition anti-spooﬁng,” in Handbook of Biometric Anti-Spooﬁng - Trusted Biometrics under
Spooﬁng Attacks, 2014, pp. 125–146.

[5] Z. Wu, N. Evans, T. Kinnunen, J. Yamagishi, F. Alegre, and H. Li, “Spooﬁng and countermeasures

for speaker veriﬁcation: A survey,” Speech Communication, vol. 66, pp. 130–153, 2015.

[6] J. A. Villalba and E. Lleida, “Speaker veriﬁcation performance degradation against spooﬁng and

tampering attacks,” in Proc. FALA, 2010, pp. 131–134.

[7] S. K. Erg¨unay, E. Khoury, A. Lazaridis, and S. Marcel, “On the vulnerability of speaker veriﬁcation

to realistic voice spooﬁng,” in Proc. BTAS, 2015, pp. 1–6.

[8] J. Galka, M. Grzywacz, and R. Samborski, “Playback attack detection for text-dependent speaker

veriﬁcation over telephone channels,” Speech Communication, vol. 67, pp. 143–153, 2015.

[9] R. G. Hautam¨aki, T. Kinnunen, V. Hautam¨aki, T. Leino, and A. Laukkanen, “I-vectors meet
imitators: on vulnerability of speaker veriﬁcation systems against voice mimicry,” in Proc. INTER-
SPEECH, 2013, pp. 930–934.

[10] M. Farr´us, M. Wagner, J. Anguita, and J. Hernando, “How vulnerable are prosodic features to

professional imitators?” in Proc. Odyssey, 2008, p. 2.

[11] P. L. D. Leon, V. R. Apsingekar, M. Pucher, and J. Yamagishi, “Revisiting the security of speaker
veriﬁcation systems against imposture using synthetic speech,” in Proc. ICASSP, 2010, pp. 1798–
1801.

[12] Q. Jin, A. R. Toth, A. W. Black, and T. Schultz, “Is voice transformation a threat to speaker

identiﬁcation?” in Proc. ICASSP, 2008, pp. 4845–4848.

[13] B. L. Pellom and J. H. L. Hansen, “An experimental study of speaker veriﬁcation sensitivity to

computer voice-altered imposters,” in Proc. ICASSP, 1999, pp. 837–840.

[14] T. Masuko, T. Hitotsumatsu, K. Tokuda, and T. Kobyashi, “On the security of HMM-based speaker

veriﬁcation systems against imposture using synthetic speech,” in Proc. Eurospeech, 1999.

[15] D. Matrouf, J. Bonastre, and C. Fredouille, “Eﬀect of speech transformation on impostor accep-

tance,” in Proc. ICASSP, 2006, pp. 933–936.

[16] J. Bonastre, D. Matrouf, and C. Fredouille, “Artiﬁcial impostor voice transformation eﬀects on false

acceptance rates,” in Proc. INTERSPEECH, 2007, pp. 2053–2056.

[17] P. L. D. Leon, M. Pucher, and J. Yamagishi, “Evaluation of the vulnerability of speaker veriﬁcation

to synthetic speech,” in Proc. Odyssey, 2010, p. 28.

[18] F. Alegre, R. Vipperla, N. W. D. Evans, and B. G. B. Fauve, “On the vulnerability of automatic
speaker recognition to spooﬁng attacks with artiﬁcial signals,” in Proc. EUSIPCO, 2012, pp. 36–40.

22

[19] Z. Kons and H. Aronowitz, “Voice transformation-based spooﬁng of text-dependent speaker veriﬁ-

cation systems,” in Proc. INTERSPEECH, 2013, pp. 945–949.

[20] Z. Wu and H. Li, “Voice conversion versus speaker veriﬁcation: an overview,” APSIPA Transactions

on Audio, Signal and Information Processing, vol. 3, no. e17, 2014.

[21] Z. Wu, A. Khodabakhsh, C. Demiro˘glu, J. Yamagishi, D. Saito, T. Toda, and S. King, “SAS: A
speaker veriﬁcation spooﬁng database containing diverse attacks,” in Proc. ICASSP, April 2015,
pp. 4440–4444.

[22] J. Toda, Y. Ohtani, and K. Shikano, “Eigenvoice conversion based on gaussian mixture model,” in

Proc. INTERSPEECH, 2006.

[23] J. Yamagishi, T. Kobayashi, Y. Nakano, K. Ogata, and J. Isogai, “Analysis of speaker adapta-
tion algorithms for hmm-based speech synthesis and a constrained smaplr adaptation algorithm,”
Transactions on Audio, Speech and Language Processing, vol. 17, no. 1, pp. 66–83, 2009.

[24] A. Sizov, E. Khoury, T. Kinnunen, Z. Wu, and S. Marcel, “Joint speaker veriﬁcation and anti-
spooﬁng in the i-vector space,” IEEE Transactions on Information Forensics and Security, no. 99,
2015.

[25] Z. Wu, T. Kinnunen, N. Evans, J. Yamagishi, C. Hanil¸ci, M. Sahidullah, and A. Sizov, “ASVspoof
2015: The ﬁrst automatic speaker veriﬁcation spooﬁng and countermeasures challenge,” in Proc.
INTERSPEECH, 2015, 2037–2041.

[26] “SPTK: Speech signal processing toolkit,” 2014, version 3.8, http://sp-tk.sourceforge.net/.
[27] C. Hanil¸ci, T. Kinnunen, and M. Sahidullah, “Classiﬁers for synthetic speech detection: A compar-

ison,” in Proc. INTERSPEECH, 2015, pp. 2057–2061.

[28] Z. Wu, C. E. Siong, and H. Li, “Detecting converted speech and natural speech for anti-spooﬁng

attack in speaker recognition,” in Proc. INTERSPEECH, 2012.

[29] Z. Wu, X. Xiao, E. Chng, and H. Li, “Synthetic speech detection using temporal modulation

feature,” in Proc. ICASSP, 2013, pp. 7234–7238.

[30] P. L. D. Leon, I. Hernez, I. Saratxaga, M. Pucher, and J. Yamagishi, “Detection of synthetic speech

for the problem of imposture,” in Proc. ICASSP.

IEEE, 2011, pp. 4844–4847.

[31] J. S´anchez, I. Saratxaga, I. Hern´aez, E. Navas, D. Erro, and T. Raitio, “Toward a universal synthetic
speech spooﬁng detection using phase information,” IEEE Transactions on Information Forensics
and Security, vol. 10, no. 4, pp. 810–820, 2015.

[32] P. L. D. Leon, M. Pucher, J. Yamagishi, I. Hern´aez, and I. Saratxaga, “Evaluation of speaker
veriﬁcation security and detection of HMM-based synthetic speech,” IEEE Transactions on Audio,
Speech and Language Processing, vol. 20, no. 8, pp. 2280–2290, 2012.

[33] M. Sahidullah, T. Kinnunen, and C. Hanil¸ci, “A comparison of features for synthetic speech detec-

tion,” in Proc. INTERSPEECH, 2015, 2087–2091.

[34] S. Novoselov, A. Kozlov, G. Lavrentyeva, K. Simonchik, and V. Shchemelinin, “Stc anti-spooﬁng
systems for the asvspoof 2015 challenge,” http://arxiv.org/ftp/arxiv/papers/1507/1507.08074.pdf,
2015.

[35] J. A. Villalba, A. Miguel, A. Ortega, and E. Lleida, “Spooﬁng detection with dnn and one-class

svm for the asvspoof 2015 challenge,” in Proc. INTERSPEECH, 2015, pp. 2067–2071.

[36] L. Wang, Y. Yoshida, Y. Kawakami, and S. Nakagawa, “Relative phase information for detecting

human speech and spoofed speech,” in Proc. INTERSPEECH, 2015, pp. 2092–2096.

[37] X. Xiao, X. Tian, S. Du, H. Xu, E. S. Chng, and H. Li, “Spooﬁng speech detection using high
dimensional magnitude and phase features: The ntu approach for asvspoof 2015 challenge,” in
Proc. INTERSPEECH, 2015, pp. 2052–2056.

[38] J. Sanchez, I. Saratxaga, I. Hernaez, E. Navas, and D. Erro, “The aholab rps ssd spooﬁng challenge

2015 submission,” in Proc. INTERSPEECH, 2015, pp. 2042–2046.

[39] S. Weng, S. Chen, L. Yu, X. Wu, W. Cai, Z. Liu, and M. Li, “The sysu system for the interspeech
2015 automatic speaker veriﬁcation spooﬁng and countermeasures challenge,” http://arxiv.org/
pdf/1507.06711v2.pdf, 2015.

[40] N. Dehak, P. Kenny, R. Dehak, P. Dumouchel, and P. Ouellet, “Front-end factor analysis for speaker
veriﬁcation,” IEEE Transactions on Audio, Speech and Language Processing, vol. 19, no. 4, pp.
788–798, 2011.

[41] E. Khoury, T. Kinnunen, A. Sizov, Z. Wu, and S. Marcel, “Introducing i-vectors for joint anti-

spooﬁng and speaker veriﬁcation,” in Proc. INTERSPEECH, 2014, pp. 61–65.

[42] Z. Wu, T. Kinnunen, E. S. Chng, and H. Li, “A study on spooﬁng attack in state-of-the-art speaker

veriﬁcation: the telephone speech case,” in Proc. APSIPA ASC, 2012, pp. 1–5.

[43] M. Wester, Z. Wu, and J. Yamagishi, “Human vs machine spooﬁng detection on wideband and

narrowband data,” in Proc. INTERSPEECH, 2015, pp. 2047–2051.

23

[44] X. Tian, Z. Wu, X. Xiao, E. S. Chng, and H. Li, “Spooﬁng detection under noisy conditions: A

preliminary investigation and an initial database,” http://arxiv.org/pdf/1602.02950v1.pdf, 2016.

[45] D. S¨undermann, H. H¨oge, A. Bonafonte, H. Ney, A. W. Black, and S. S. Narayanan, “Text-

independent voice conversion based on unit selection,” in Proc. ICASSP, 2006, pp. 81–84.

[46] J. Benesty, M. M. Sondhi, and Y. Huang, Eds., Springer Handbook of Speech Processing. Berlin:

Springer, 2008.

[47] L. R. Rabiner, M. J. Cheng, A. E. Rosenberg, and C. A. McGonegal, “A comparative performance
study of several pitch detection algorithms,” IEEE Transactions on Acoustics, Speech and Signal
Processing, vol. 24, no. 5, pp. 399–418, 1976.

[48] S. O. Sadjadi and J. H. Hansen, “Mean Hilbert envelope coeﬃcients (MHEC) for robust speaker

and language identiﬁcation,” Speech Communication, vol. 72, pp. 138–148, 2015.

[49] S. J. D. Prince and J. H. Elder, “Probabilistic linear discriminant analysis for inferences about

identity,” in Proc. ICCV, 2007, pp. 1–8.

[50] P. Kenny, “Bayesian speaker veriﬁcation with heavy-tailed priors.” in Proc. Odyssey, 2010, p. 14.
[51] S. E. Linville and J. Rens, “Vocal tract resonance analysis of aging voice using long-term average

spectra.” Journal of Voice, vol. 15, no. 3, pp. 323–330, 2001.

[52] C. Grigoras, “Statistical tools for multimedia forensics,” in Proc. Audio Engineering Society Con-
ference: 39th International Conference: Audio Forensics: Practices and Challenges, Jun 2010, pp.
27–32.

[53] D. Byrne, H. Dillon, K. Tran, S. Arlinger, K. Wilbraham, R. Cox, B. Hagerman, R. Hetu, J. Kei,
C. Lui, J. Kiessling, N. M. Kotby, N. H. A. Nasser, Wafaa, Y. Nakanishi, H. Oyer, R. Powell,
D. Stephens, R. Meredith, T. Sirimanna, G. Tavartkiladze, G. I. Frolenkov, S. Westerman, and
C. Ludvigsen, “An international comparison of long-term average speech spectra,” The Journal of
the Acoustical Society of America, vol. 96, no. 4, pp. 2108–2120, 1994.

[54] B. S. Atal, “Eﬀectiveness of linear prediction characteristics of the speech wave for automatic
speaker identiﬁcation and veriﬁcation,” The Journal of the Acoustical Society of America, vol. 55,
no. 6, pp. 1304–1312, 1974.

[55] H. Hermansky and N. Morgan, “RASTA processing of speech,” IEEE Transactions on Speech and

Audio Processing, vol. 2, no. 4, pp. 578–589, 1994.

[56] S. Chakroborty, A. Roy, and G. Saha, “Improved closed set text-independent speaker identiﬁcation
by combining MFCC with evidence from ﬂipped ﬁlter banks,” International Journal of Signal
Processing, vol. 4, no. 2, pp. 114–122, 2007.

[57] J. M. K. Kua, T. Thiruvaran, M. Nosratighods, E. Ambikairajah, and J. Epps, “Investigation of
spectral centroid magnitude and frequency for speaker recognition,” in Proc. Odyssey, 2010, p. 7.
[58] G. Degottex, J. Kane, T. Drugman, T. Raitio, and S. Scherer, “COVAREP – A collaborative
voice analysis repository for speech technologies,” in Proc. ICASSP, Florence, Italy, May 2014, pp.
960–964.

[59] H. Yin, V. Hohmann, and C. Nadeu, “Acoustic features for speech recognition based on Gammatone
ﬁlterbank and instantaneous frequency,” Speech Communication, vol. 53, no. 5, pp. 707 – 715, 2011.
[60] V. Mitra, W. Wang, H. Franco, Y. Lei, C. Bartels, and M. Graciarena, “Evaluating robust features
on deep neural networks for speech recognition in noisy and channel mismatched conditions,” in
Proc. INTERSPEECH, 2014, pp. 895–899.

[61] M. Slaney, “Auditory Toolbox (version 2),” Interval Research Corporation Technical Report #1998-

10, 1998.

[62] S. O. Sadjadi, H. Boril, and J. H. L. Hansen, “A comparison of front-end compensation strategies
for robust LVCSR under room reverberation and increased vocal eﬀort,” in Proc. ICASSP, 2012,
pp. 4701–4704.

[63] A. Varga and H. J. M. Steeneken, “Assessment for automatic speech recognition ii: NOISEX-92: A
database and an experiment to study the eﬀect of additive noise on speech recognition systems,”
Speech Communication, vol. 12, no. 3, pp. 247–251, 1993.

[64] N. Krishnamurthy and J. H. L. Hansen, “Babble noise: Modeling, analysis, and applications,” IEEE
Transactions on Audio, Speech and Language Processing, vol. 17, no. 7, pp. 1394–1407, Sep. 2009.
[65] T. Kinnunen and H. Li, “An overview of text-independent speaker recognition: From features to

supervectors,” Speech Communication, vol. 52, no. 1, pp. 12–40, 2010.

[66] “Wall Street Journal Corpus,” [Online:] http://www.ldc.upenn.edu, 2015.
[67] A. O. Hatch, S. S. Kajarekar, and A. Stolcke, “Within-class covariance normalization for SVM-based

speaker recognition,” in Proc. ICSLP, 2006.

[68] D. Garcia-Romero and C. Y. Espy-Wilson, “Analysis of i-vector length normalization in speaker

recognition systems,” in Proc. INTERSPEECH, 2011, pp. 249–252.

24

[69] S. Boll, “Suppression of Acoustic Noise in Speech Using Spectral Subtraction,” IEEE Transactions

on Acoustics, Speech, and Signal Processing, vol. 27, pp. 113–120, 1979.

[70] N. Berouti, R. Schwartz, and J. Makhoul, “Enhancement of speech corrupted by acoustic noise,”

in Proc. ICASSP, 1979, pp. 208–211.

[71] J. S. Lim and A. V. Oppenheim, “Enhancement and bandwidth compression of noisy speech,” Proc.

IEEE, vol. 67, no. 12, pp. 1586–1604, 1979.

[72] P. C. Loizou, Speech Enhancement: Theory and Practice, 1st ed. CRC Press, Inc., 2007.
[73] H. Kawahara, I. Masuda-Katsuse, and A. de Cheveign´e, “Restructuring speech representations using
a pitch-adaptive time-frequency smoothing and an instantaneous-frequency-based F0 extraction:
Possible role of a repetitive structure in sounds,” Speech Communication, vol. 27, no. 3-4, pp.
187–207, 1999.

25

