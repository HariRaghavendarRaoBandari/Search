Privacy-preserving Analysis of Correlated Data

6
1
0
2

 
r
a

 

M
3
1

 
 
]

G
L
.
s
c
[
 
 

1
v
7
7
9
3
0

.

3
0
6
1
:
v
i
X
r
a

Yizhen Wang
UC San Diego, 9500 Gilman Drive, La Jolla CA 92093
Shuang Song
UC San Diego, 9500 Gilman Drive, La Jolla CA 92093
Kamalika Chaudhuri
UC San Diego, 9500 Gilman Drive, La Jolla CA 92093

Abstract

Many modern machine learning applications in-
volve sensitive correlated data, such private in-
formation on users connected together in a social
network, and measurements of physical activity
of a single user across time. However, the current
standard of privacy in machine learning, differ-
ential privacy, cannot adequately address privacy
issues in this kind of data.
This work looks at a recent generalization of dif-
ferential privacy, called Pufferﬁsh, that can be
used to address privacy in correlated data. The
main challenge in applying Pufferﬁsh to corre-
lated data problems is the lack of suitable mech-
anisms. In this paper, we provide a general mech-
anism, called the Wasserstein Mechanism, which
applies to any Pufferﬁsh framework. Since the
Wasserstein Mechanism may be computationally
inefﬁcient, we provide an additional mechanism,
called Markov Quilt Mechanism, that applies to
some practical cases such as physical activity
measurements across time, and is computation-
ally efﬁcient.

1. Introduction
Machine-learning is increasingly done on personal data,
and consequently, it is extremely important to design meth-
ods that can learn while still preserving privacy of individ-
uals in the sensitive datasets. For the past several years,
the de facto standard of privacy in machine learning has
been differential privacy (Dwork et al., 2006), and there
is a growing body of work on differentially private learn-
ing algorithms (Chaudhuri et al., 2011; Jain et al., 2012;
Kifer et al., 2012; Chaudhuri et al., 2012; Hardt and Roth,
2013; Wang et al., 2015; Duchi et al., 2013; Bassily et al.,
2014; Imtiaz and Sarwate, 2015). The typical setting in

YIW248@ENG.UCSD.EDU

SHS037@ENG.UCSD.EDU

KAMALIKA@ENG.UCSD.EDU

these works is that each (independently drawn) training ex-
ample corresponds to a single individual’s private value,
and the goal is to learn a supervised or unsupervised model
while adding enough noise to hide the evidence of the par-
ticipation of a single individual in the dataset.
Many machine learning applications however increasingly
involve a very different setting – correlated data – privacy
issues in which have been largely ignored by this literature.
An example is data on the ﬂu status of people connected
together in a social network; if a fairly large subset of these
people are children attending the same school, then their
ﬂu status are highly correlated. Another example is data
from measurements of physical activity of a single subject
across time; if these measurements are made at small in-
tervals, then they are highly correlated as human activities
change slowly over time. Differential privacy is usually in-
sufﬁcient to address privacy challenges in such data. For
example, in the ﬂu status case, differential privacy with pa-
rameter  = 1 would advocate adding noise with standard
deviation √2 to the number of people who have ﬂu. While
this hides the evidence of the ﬂu status of a single indepen-
dent individual, it will not hide the evidence of ﬂu of a child
in the school if the school size is big enough. Similarly, in
the physical activity example, a modiﬁed form of differ-
ential privacy, called entry-privacy, will advocate adding
noise with standard deviation 2√2 to the histogram of ac-
tivities; again, this will not hide evidence of the fact that the
individual was sitting at a speciﬁc time as the same activity
is likely to continue for several measurements. Thus to deal
with this kind of correlated data, we need a different notion
of privacy.
A generalized version of differential privacy called Puffer-
ﬁsh that captures some of these issues has been recently
proposed by (Kifer and Machanavajjhala, 2014). The main
idea is to specify what kind of privacy protection is required
through three components. These are S, a set of secrets
that represents what may need to be hidden, Q, a set of se-

Privacy of Correlated Data

cret pairs that represents pairs of secrets that the adversary
should not be able to distinguish between and ﬁnally Θ, a
class of distributions that can plausibly model correlation in
the data. For example, in the ﬂu status case, secrets would
be the ﬂu statuses of each person, and the secret pair set
would be the set of all pairs of the form (Alice has ﬂu, Al-
ice does not have ﬂu). The distribution class Θ could be the
class of all distributions that induce a certain amount of cor-
relation between the ﬂu statuses of people in the connected
components. In the physical activity example, the secrets
will be the activity at each time point t, secret pairs will be
pairs of the form (Activity a at time t, Activity b at time t),
for all activity pairs a and b and all t. Assuming that activ-
ities transition in a Markovian fashion, Θ will be the set of
all Markov Chains over activities. (Kifer and Machanava-
jjhala, 2014) has shown that Pufferﬁsh is a generalization
of differential privacy, and differential privacy essentially
is equivalent to Pufferﬁsh when we want to keep each in-
dividual’s private value secret and when Θ consists of all
product distributions over the individuals’ values. To ad-
dress privacy issues in correlated data, we adopt Pufferﬁsh
as our privacy deﬁnition.
The main challenge in directly applying Pufferﬁsh to corre-
lated data is the lack of suitable mechanisms. While mech-
anisms are known for speciﬁc Pufferﬁsh frameworks (Kifer
and Machanavajjhala, 2014; He et al., 2014), there is no
general mechanism, and moreover, there is no mechanism
in existing work that applies to correlated data. Our ﬁrst
contribution in this work is to provide a generic mechanism
that applies to any Pufferﬁsh framework and provides pri-
vacy. Our mechanism, called the Wasserstein Mechanism,
is an analogue of the popular global sensitivity mechanism
for differential privacy, and exploits a novel connection be-
tween privacy and a certain form of the Wasserstein dis-
tance between measures.
The Wasserstein Mechanism, due to its extreme generality,
is computationally inefﬁcient, and a natural question to ask
is whether we can come up with a more computationally ef-
ﬁcient mechanism, at least for some cases of practical inter-
est. To address this issue, we turn our attention to the case
when correlation between the variables can be described
by a Bayesian network, and the goal is to hide the private
value of each variable. We provide a second mechanism,
called Markov Quilt Mechanism, that can exploit proper-
ties of the Bayesian network to reduce the computational
complexity. As a case study, we apply our mechanism to
a Markov Chain, which models the physical activity mea-
surement problem, and we show that this mechanism runs
in O(n3) time in the size of the chain n.

2. The Privacy Model
2.1. Differential Privacy

We begin with deﬁning differential privacy, which was in-
troduced in (Dwork et al., 2006), and has now become a
gold standard for privacy in machine learning.
Deﬁnition 2.1. A (randomized) privacy mechanism M (·)
is said to be -differentially private if for any set of out-
comes S ⊆ Range(M ), and any datasets D and D(cid:48) that

differ in the private value of a single individual, we have:

Pr(M (D) ∈ S) ≤ e Pr(M (D(cid:48)) ∈ S).

(1)

The parameter  is called the privacy budget, and is usually
set to be a small constant. Differential privacy ensures that
the participation of a single individual in the dataset does
not alter the probability of any outcome by much. This,
in turn, means that if we have an adversary who has prior
information on the data and an individual Alice, then par-
ticipation of Alice in the data will not change the extra in-
formation he gains about Alice from observing the output
of a differentially-private algorithm on the data (Kifer and
Machanavajjhala, 2011; Dwork et al., 2006).
Finally, a variant of differential privacy, called entry-
privacy, enforces (1) for datasets D and D(cid:48) that differ in
a single entry or feature of an individual. This means that a
change in the value of a ﬁxed feature of any individual Al-
ice in the dataset (for example, Alice’s disease status) does
not change the probability of any outcome by more than a
factor of e when the values of all other features for Alice
remain the same.
Recent work has developed a number of generic mecha-
nisms to enforce differential privacy which apply under
different conditions; see (Sarwate and Chaudhuri, 2013;
Dwork and Roth, 2013) for surveys. The most popular
mechanism is the Global Sensitivity Mechanism.
Deﬁnition 2.2. (Global Sensitivity Mechanism) The global
sensitivity of a function F is deﬁned as:

GS(F ) = max

(D,D(cid:48))|F (D) − F (D(cid:48))|,

where D, D(cid:48) are two data sets that differ by a single indi-
vidual’s private value. Given a dataset D, a function F ,
and a privacy budget , the global sensitivity mechanism
outputs:

M (D) = F (D) + Z,

where Z ∼ Lap(GS(F )/).
Here Lap(α) is a Laplace random variable with mean 0
and scale parameter α. (Dwork et al., 2006) shows that this
mechanism is -differentially private.

Privacy of Correlated Data

2.2. Differential Privacy for Correlated Data

It was shown by (Kifer and Machanavajjhala, 2011) that
differential privacy will erase the evidence of a single in-
dividual’s private value only when individuals in the data
are independent. There is thus a potential for privacy leaks
when individuals’ private values are correlated.
Let us consider two concrete examples when this happens.
First, suppose we have a dataset X = {X1, . . . , Xn} where
Xi is an indicator variable for whether person i has ﬂu.
Our goal is to release (an approximation to) the number
of people in the dataset who have ﬂu, while an adversary
would like to determine if Alice in the dataset has ﬂu or
not.
As changing one person’s ﬂu status changes the number of
patients by 1, the Global Sensitivity Mechanism (Dwork
 to
et al., 2006) will add noise with standard deviation ≈ 1
the true output value. If the ﬂu status of the people are in-
dependent, then this measure will be sufﬁcient to preserve
privacy. However now suppose that a certain number of
people in the dataset work together, and Alice is in the con-
nected component. Moreover, ﬂu is highly contagious, and
passes on between interacting people with probability 0.5.
In this case, the noise added due to differential privacy is
not sufﬁcient to hide the evidence of Alice’s disease status,
and we need more noise.
As a second example, consider a time-series X =
{X1, X2, . . .} where Xt denotes a discrete physical activ-
ity (e.g, running, sleeping, sitting, etc) of a person at time
t. Our goal is to release (an approximate) histogram of ac-
tivities, while the adversary would like to determine what
the person was doing at a speciﬁc time t.
Looking at each Xt as an entry, then we can use entry-
differential privacy, which would propose adding noise
with standard deviation ≈ 1/ to each bin of the histogram.
However, time-series data is highly correlated across con-
secutive time periods. Thus, this amount of noise is not
sufﬁcient to hide the evidence of their physical activity sta-
tus, and we need to add more noise.

2.3. The Pufferﬁsh Privacy Framework

To account for correlated data, (Kifer and Machanavajjhala,
2014) introduce a novel generalization of the differential
privacy framework called Pufferﬁsh.
A Pufferﬁsh privacy framework has three parameters – a
set S of secrets, a set Q ⊆ S × S of secret pairs, and a
class of data distributions Θ.
S consists of possible facts about the data that we wish to
hide, and secrets could refer to a single individual’s private
data or part thereof. Q is the set of secret pairs that the pri-
vacy algorithm wishes to be indistinguishable. Finally, Θ

represents a set of distributions that plausibly generate the
data, and the set Θ thus controls the amount of allowable
correlation in the data. Each θ ∈ Θ represents a belief an
adversary may hold about the data, and our goal is to ensure
indistinguishability in the face of these beliefs.
How do we model the two examples we described above
in Pufferﬁsh? In the ﬂu example, the set of secrets S is
everyone’s disease status. A plausible set of secret pairs
Q is {(si,0, si,1) : i = 1, . . . , n} where si,j means person
i has ﬂu status j, j ∈ {0, 1}. If only some people in the
data are privacy-sensitive, Q can consist of pairs (si,0, si,1)
where i ranges over all the privacy-sensitive people. Θ is a
set of network models that model spread of the disease in
question. For example, Θ could be all models where con-
nections represent social interaction, and ﬂu spreads across
connections with probability between [0, 0.5]. Observe that
this example could also be modeled using group differen-
tial privacy – where we enforce (1) for datasets D and D(cid:48)
which differ in the disease status of an entire group of con-
nected individuals; however, Pufferﬁsh allows for a more
nuanced model that accounts for the diffusion rate as well
as the connected individuals.
In the physical activity example, the set of secrets S is
the activity status at all times t. Q is the set of all pairs
(st,a, st,b) where a, b lie in the set of activities, and where
st,a means that activity a happened at time t. Finally,
Θ would be a set of time series models, such as Markov
or semi-Markov models that capture how people switch
between activities. Observe that another plausible way
to model this example is via pan-privacy (Dwork et al.,
2010a), which models a streaming setting where a new in-
dividual arrives at each time period, and the goal is con-
tinually release a private statistic with time. However, this
setting does not capture this example as pan-privacy still
(implicitly) assumes that the arriving individuals are inde-
pendent of individuals existing in the data.
(Kifer and Machanavajjhala, 2014) shows that in general,
we cannot expect to have privacy against all possible dis-
tributions and still retain utility. As a result it is essential
to select Θ wisely; if Θ is too restrictive, then we may not
have privacy against legitimate prior knowledge on the part
of the adversary, and if Θ is too large, then the resulting
privacy mechanisms will have little utility.
Deﬁnition 2.3. A privacy mechanism M is said to be -
Pufferﬁsh private with Pufferﬁsh parameters (S,Q, Θ) if
for datasets X ∼ θ where θ ∈ Θ and for all secret pairs
(si, sj) ∈ Q, and for all w ∈ Range(M ), we have:

(2)

e− ≤

P (M (X) = w|si, θ)
P (M (X) = w|sj, θ) ≤ e

when si and sj are such that P (si|θ) (cid:54)= 0, P (sj|θ) (cid:54)= 0.
Observe that unlike (1), the probability in (2) is with re-

Privacy of Correlated Data

spect to the randomness in the mechanism and X ∼ θ; to
emphasize this, we use the notation X instead of D in the
deﬁnition; in general, we use D to denote a speciﬁc dataset.
An alternative interpretation of the deﬁnition is for X ∼ θ,
θ ∈ Θ, for all (si, sj) ∈ Q, and for all w ∈ Range(M ), we
have:

e− ≤

P (si|M (X) = w, θ)
P (sj|M (X) = w, θ)

(cid:46) P (si|θ)
P (sj|θ) ≤ e.

(3)

Deﬁnition 3.1 (p-Wasserstein Distance). Let (X , d) be a
Radon space, and µ, ν be two probability measures on X
with ﬁnite p-th moment. The p-th Wasserstein distance be-
(cid:19)1/p
tween µ and ν is deﬁned as:

(cid:90)

Wp(µ, ν) =

inf

γ∈Γ(µ,ν)

d(x, y)p dγ(x, y)

(cid:19)1/p

X×X
E[d(X, Y )p]

=

inf

γ∈Γ(µ,ν)

,

(4)

(cid:18)
(cid:18)

In other words, knowledge of the output M (X) does not
affect the ratio of likelihood of si and sj, compared to the
initial belief.
(Kifer and Machanavajjhala, 2014) shows that Differential
Privacy is a special case of Pufferﬁsh, where S is the set of
all facts of the form Person i has value x for i ∈ {1, . . . , n}
and x in a domain X , Q = S × S, and Θ is the set of all
distributions where each individuals private value is dis-
tributed independently.

3. A General Pufferﬁsh Mechanism
Recent work has developed a large number of general pri-
vacy mechanisms (Dwork et al., 2006; McSherry and Tal-
war, 2007; Chaudhuri et al., 2011; Nissim et al., 2007;
Dwork and Lei, 2009; Chaudhuri et al., 2014) that guar-
antee differential privacy under different conditions. How-
ever, while a number of Pufferﬁsh mechanisms for speciﬁc
frameworks are known (Kifer and Machanavajjhala, 2014;
He et al., 2014), there is no generic mechanism that ap-
plies to any Pufferﬁsh framework. Our ﬁrst contribution
is to provide such a mechanism, analogous to the popular
Global Sensitivity Mechanism for differential privacy, that
guarantees privacy in any Pufferﬁsh framework.
Speciﬁcally, given data represented by random variables X
(that may be scalar-valued or vector-valued), a Pufferﬁsh
privacy framework (S,Q, Θ), and a function F that maps
X into a number, our goal is to design a generic mechanism
M that satisﬁes -Pufferﬁsh privacy in the given framework
and approximates F (X).

3.1. The Mechanism

Our proposed mechanism is inspired by the global sensitiv-
ity mechanism in differential privacy; recall that this mech-
anism adds noise to the function F proportional to the sen-
sitivity, which is the worst case distance between F (D) and
F (D(cid:48)) where D and D(cid:48) are two datasets that differ in the
value of a single individual.
In Pufferﬁsh, the quantities
analogous to D and D(cid:48) are the measures p(F (X)|si, θ) and
p(F (X)|sj, θ) for a secret pair (si, sj), and therefore, the
added noise should be proportional to some distance be-
tween these two measures.
It turns out that the relevant
distance is a form of the Wasserstein distance.

where Γ(µ, ν) is the set of all couplings γ over µ and ν.

Intuitively, each γ ∈ Γ(µ, ν) can be regarded as a way
to shift probability mass between µ and ν;
the cost of
a shift γ is (E(X,Y )∼γ[d(X, Y )p])1/p, and the cost of
the min-cost shift is the Wasserstein distance.
In par-
ticular, we are interested in the ∞-Wasserstein distance
W∞(µ, ν) = inf γ∈Γ(µ,ν) max(x,y)∈A d(x, y), where A =
(cid:54)= 0}. We speciﬁcally consider the
{(x, y)|γ(x, y)
case when d(x, y) = |x − y|, and thus W∞(µ, ν) =
inf γ∈Γ(µ,ν) max(x,y)∈A |x − y|.‘
The Wasserstein Mechanism. Given a Pufferﬁsh frame-
work (S,Q, Θ) and a function F , the Wasserstein Mecha-
nism is as follows.

1. Inputs: Pufferﬁsh framework (S,Q, Θ), privacy pa-

rameter , function F , dataset D.

2. For any (si, sj) ∈ Q and any θ ∈ Θ such that
let µi,θ =
Pr(si|θ)
Pr(F (X) = ·|si, θ) be the distribution of F (X) con-
ditioned on si when X ∼ θ. Similarly, deﬁne µj,θ.

(cid:54)= 0 and Pr(sj|θ)

(cid:54)= 0,

3. Let W = sup(si,sj )∈Q,θ∈Θ W∞(µi,θ, µj,θ).
4. Output F (D) + Z, where Z ∼ Lap( W
 ).
3.2. Performance Guarantees

We ﬁrst establish that the Wasserstein Mechanism satisﬁes
-Pufferﬁsh privacy.
Theorem 3.2 (Privacy of the Wasserstein Mechanism).
The Wasserstein Mechanism satisﬁes -Pufferﬁsh privacy
in the framework (S,Q, Θ).
The proof is presented in Appendix A.1.
Recall that Pufferﬁsh reduces to differential privacy in the
special case when (a) X = {X1, . . . , Xn} with each Xi
being a single individual’s value, (b) Θ is the set of all prod-
uct distributions over the domains of each Xi, and (c) Q =
{(Xi is in the dataset with value x, Xi is not in the dataset) :
i ∈ [n], x ∈ X}. It can be shown that in this case, the
Wasserstein mechanism also reduces to the familiar global

Privacy of Correlated Data

sensitivity mechanism. Thus the Wasserstein mechanism
is a generalization of the global sensitivity method for
Pufferﬁsh.

4. Mechanisms for Bayesian Networks
The Wasserstein mechanism we provide in the last sec-
tion is very general and applies to any Pufferﬁsh frame-
work; however, perhaps because of its extreme generality,
it is extremely computationally expensive. Thus a natural
question to ask is whether we can come up with Pufferﬁsh
mechanisms which are less computationally challenging,
and which apply to some cases of practical interest. In this
section, we provide such a mechanism, called the Markov
Quilt Mechanism, when the dependence among the vari-
ables in X is described by a Bayesian network, Θ has some
special structure, and the goal is to keep the value of any in-
dividual node in the network private.
We illustrate our mechanism on a Markov Chain, and show
that it runs in time polynomial in the length of the chain.
This is an important case that models our physical activ-
ity measurement example in Section 2. We derive pri-
vacy mechanisms for this case, and provide simulations
that demonstrate the associated privacy-accuracy tradeoffs.

4.1. The Setting

This section considers a more restricted but more practical
setting than the fully general setting of Section 3. We as-
sume that data X can be written as X = {X1, . . . , Xn},
a de-
where each Xi lies in a bounded domain X . Let si
note the event that Xi takes value a. The set of secrets is
a : a ∈ X , i ∈ [n]}, and the set of secret pairs is
S = {si
b) : a, b ∈ X , i ∈ [n]}. In other words, we
a, si
Q = {(si
would like to hide the value of each individual variable Xi
from the adversary.
We assume that there is an underlying known Bayesian
network G = (X, E) connecting the variables X that is
known to us. Each θ ∈ Θ that describes the distribution
of the variables is based on this Bayesian network G, but
may have different model parameters. An application of
this setting is the physical activity monitoring example in
Section 2. See Section 4.3 for more details.

Notations. We use X with a lowercase subscript, for ex-
ample, Xi, to denote a single node in G, and X with an up-
percase subscript, for example, XA, to denote a set of nodes
in G. For a set of nodes XA we use the notation card(XA)
to denote the number of nodes contained in XA.
In this section, we will consider functions F that are 1-
Lipschitz, as in Deﬁnition 4.1. Since any Lipschitz func-
tion can be scaled to be 1-Lipschitz, this is not a signiﬁcant
restriction.

Deﬁnition 4.1 (1-Lipschitz). A function F (X1, . . . , Xn)
i, and for all
is said to be 1-Lipschitz if
pairs Xi and X(cid:48)
i, we have |F (X1, . . . , Xi, . . . , Xn) −
F (X1, . . . , X(cid:48)

for all

i, . . . , Xn)| ≤ 1.

4.2. The Markov Quilt Mechanism

The main insight behind the Markov Quilt mechanism is
that if nodes Xi and Xj are “far apart” in the graph G,
then, Xj is largely independent from Xi. Thus, to obscure
the effect of Xi on F , it is sufﬁcient to add noise that is
proportional to the number of nodes that are “local” to Xi
plus a small correction term that accounts for the effect of
the distant nodes. The rest of the section will explain how
to calculate this correction term, and how to determine the
number of local nodes.
We begin with some deﬁnitions. First, we quantify how
much changing the value of a variable Xi ∈ X can affect
a set of variables XR ⊂ X, where Xi /∈ XR. To this end,
we deﬁne the max-inﬂuence of a variable Xi on a set of
variables XR as follows.
Deﬁnition 4.2. The max-inﬂuence of a variable Xi on a
set of variables XR is deﬁned as

e(XR|Xi) =
a,b∈X sup
max
θ∈Θ

max

xR∈X card(XR )

log

Pr(XR = xR|Xi = a, θ)
Pr(XR = xR|Xi = b, θ)

.

Here X is the range of any Xj and Θ is the set of allow-
able distributions. The max-inﬂuence is thus the maximum
max-divergence (Dwork et al., 2010b) between the distri-
butions XR|Xi = a, θ and XR|Xi = b, θ where the maxi-
mum is taken over any pair (a, b) ∈ X ×X and any θ ∈ Θ.
If XR and Xi are independent, then the max-inﬂuence
of Xi on XR is 0, and a large max-inﬂuence means that
changing Xi can have a large impact on the distribution of
XR.
Our goal is to design a mechanism that protects the value of
each variable Xi while still adding a small amount of noise
to F ; to do so, we ﬁnd large sets of nodes XR such that Xi
has low max-inﬂuence on XR. The naive way to ﬁnd such
sets is brute force search, which takes time exponential in
the size of G. We now show how properties of the Bayesian
network G can be exploited to perform this search more
efﬁciently.
We next provide a second deﬁnition that generalizes the fa-
miliar notion of a Markov Blanket. Recall that the Markov
Blanket of a node u in a Bayesian network consists of its
parents, its children and the other parents of its children,
and the rest of the nodes in the network are independent
of u conditioned on its Markov Blanket. Its generalization,
the Markov Quilt, is deﬁned as follows.

Privacy of Correlated Data

(a) The Markov Blanket of Xi is XB =
{Xi−1, Xi+1}

(b) One Markov Quilt of Xi is XQ = {Xi−2, Xi+2} with corresponding
XN = {Xi−1, Xi, Xi+1}, XR = {. . . , Xi−3, Xi+3, . . .}

Figure 1. Illustration of Markov Blanket and Markov Quilt

Deﬁnition 4.3 (Markov Quilt). A set of nodes XQ, Q ⊂ [n]
in a Bayesian network G = (X, E) is a Markov Quilt for a
node Xi if the following conditions hold:

4.

1. Deleting XQ partitions G into parts XN and XR such
that (a). X = XN ∪ XQ ∪ XR (b). Xi ∈ XN and (c).
there is no edge in G that connects a node in XR to a
node in XN .

2. For all xR ∈ X card(XR), all xQ ∈ X card(XQ) and

for all a ∈ X , we have:

P (XR = xR|XQ = xQ, Xi = a)
=P (XR = xR|XQ = xQ)

In other words, XR is independent of Xi conditioned
on XQ.

Intuitively, XR is a set of “remote” nodes that are far from
Xi, and XN is the set of “nearby” nodes; XN and XR are
separated by the Markov Quilt XQ. Observe that unlike
Markov Blankets, a node can have many Markov Quilts.
Figure 1 shows an example.
Suppose our goal is to protect the private value of a node
Xi. It turns out that if we can ﬁnd an XQ such that (a).
the max-inﬂuence of Xi on XQ is at most δ and (b). XQ
is a Markov Quilt of Xi, deleting which splits up X into
XN and XR, then, adding Laplace noise of scale param-
eter card(XN )/( − δ) to F is sufﬁcient to preserve 
Pufferﬁsh privacy.
This motivates the following mechanism, which we call the
Markov Quilt Mechanism. To protect the private value of
a speciﬁc Xi, we search over a set SQ,i of Markov Quilts
XQ for Xi and pick the one which requires adding the least
amount of noise to guarantee privacy. We then iterate this
process over all Xi and add to F the maximum amount of
noise needed to protect any Xi; this ensures that the private
values of all nodes are protected.

(a) Suppose deleting XQ breaks up the underlying
Bayesian network into a “nearby” set XN and a
“remote” set XR with Xi ∈ XN .
score of XQ as: σ(XQ) = card(XN )
wise σ(XQ) = ∞.

(b) If the max-inﬂuence e(XQ|Xi) < , deﬁne the
−e(XQ|Xi); other-

(c) Let σi = minXQ∈SQ,i σ(XQ)

5. Let σmax = maxi σi.
6. Output: F (D) + σmax · Z, where Z ∼ Lap(1).
First, we note that the algorithm makes an underlying as-
sumption that the max-inﬂuence of a node Xi on a set of
nodes XQ may be computed relatively easily; this assump-
tion holds when Θ has some nice structure; see Section 4.3
for a concrete example. Second, the set SQ,i of Markov
Quilts that the mechanism searches over can be restricted
in speciﬁc ways to ensure computational efﬁciency; how-
ever, a potential cost is statistical efﬁciency – we may end
up adding more noise than necessary to preserve privacy
because we may exclude some Markov Quilts with very
low scores.

Performance Guarantees. The privacy of the Markov
Quilt mechanism follows from Theorem 4.4.
Theorem 4.4 (Privacy of Markov Quilt Mechanism). Let
F be a 1-Lipschitz function, and suppose data is repre-
sented by a random variable X = {X1, . . . , Xn} where
Xi ∈ X . Suppose we are given a Pufferﬁsh framework
(S,Q, Θ) where S = {si
a : a ∈ X , i ∈ [n]}, the set of
secret pairs Q = {(si
b) : a, b ∈ X , i ∈ [n]}, and each
a, si
θ ∈ Θ is based on a ﬁxed Bayesian network G = (X, E).
If each SQ,i is non-empty, then the Markov Quilt Mech-
anism preserves -Pufferﬁsh privacy in the framework
(S,Q, Θ).
The proof is presented in the Appendix A.2.

1. Inputs. Dataset D, function F , Pufferﬁsh framework

(S,Q, Θ), privacy parameter .

2. For all Xi:

3. Iterate over all Markov Quilts XQ for Xi in SQ,i:

4.3. Case Study: Markov Chain

We now show that the Markov Quilt mechanism can be
simpliﬁed even further when the Bayesian network under-
lying X is a Markov Chain, which models the physical ac-
tivity measurement example.

XNXi+1Xi+2XiXi−2XBXBXi−1Xi+1Xi+2Xi+3Xi+4XiXi−1Xi−2Xi−3XRXRXQXQXNPrivacy of Correlated Data

The Setting. We consider data described by a random
vector X = {X1, X2, . . . , XT} over time t ∈ [T ] where
each Xt represents a state that lies in a ﬁnite discrete set
X = [k].
In the physical activity monitoring example,
X is a set of activities, and each element in [k] repre-
sents a particular activity, such as Walking, Running, Sit-
ting and so on. We would like to hide from an adversary
the evidence that Xt takes a speciﬁc value; using st
a rep-
resents the event that Xt takes value a, our secret set is
a : a ∈ X , t ∈ [T ]}, and our set of secret pairs is
S = {st
a, st
Q = {(st
The underlying Bayesian network G is a Markov chain
X1 → X2 → . . . → XT , where for each t, Xt+1 is
independent of Xτ , τ < t, conditioned on Xt. Each θ ∈ Θ
corresponds to a k× k transition matrix Pθ for this Markov
chain. For simplicity, we focus on Markov Chains whose
transition matrices do not change with time; a similar anal-
ysis may also be applied to time-varying Markov Chains
with some additional complexity.

b) : a, b ∈ X , t ∈ [T ]}.

Properties. We begin by showing that this graphical
model has Markov Quilts with particularly simple struc-
tures.
Lemma 4.5. Let a, b and i be positive integers such that
i − a ≥ 1 and i + b ≤ T . Then, XQ = {Xi−a, Xi+b} is a
Markov Quilt for Xi with XN = {Xi−a+1, . . . , Xi+b−1},
and XR = {X1, . . . , Xi−a−1} ∪ {Xi+b+1, . . . , XT}.
Moreover, XQ = {Xi+b} is a Markov Quilt for Xi with
XN = {X1, . . . , Xi+b−1} and XR = {Xi+b+1, . . . , XT}.
Similarly, XQ = {Xi−a} is a Markov Quilt
for
Xi with XN = {Xi−a+1, . . . , XT} and XR =
{X1, . . . , Xi−a−1}.
We next quantify the max-inﬂuence e(XQ|Xi) of a Markov
Quilt XQ for Xi in terms of properties of Θ. In general,
this is a complex task, but it is considerably simpliﬁed for
the model class we consider. We ﬁrst deﬁne two pieces of
notation.
For any θ ∈ Θ, let πθ be the stationary distribution of the
Markov Chain whose transition matrix is Pθ. We deﬁne:

πΘ = min

x∈X ,θ∈Θ

πθ(x)

as the probability of the least probable state in the station-
ary distribution corresponding to any θ ∈ Θ. Additionally,
we deﬁne:

gΘ = min
θ∈Θ

min{1 − |λ| : Pθx = λx, λ < 1}

as the least eigengap of any Pθ.
We make the following assumption about Θ.

Condition 4.6. We assume that the Markov Chain induced
by each Pθ ∈ Θ is irreducible and aperiodic. This implies
that each Pθ has a unique stationary distribution πθ.
Additionally, we assume that each Pθ is a reversible
Markov Chain, πΘ > 0 and that gΘ > 0.

Condition 4.6 is quite reasonable; if Pθ does not have a
stationary distribution, then the inﬂuence of the initial state
may not decay over time, and we may need to add O(T )
noise for privacy. The second part ensures that the decay in
the inﬂuence of the initial state is fast enough, and may be
relaxed somewhat.
Provided Condition 4.6 holds, we can provide upper
bounds on the max-inﬂuence of a Markov Quilt XQ on a
node Xi.
Lemma 4.7. Let Xi be any node in the Markov Chain,
and suppose a and b are integers such that min(a, b) ≥
log(1/πΘ)
. If XQ = {Xi−a, Xi+b} is a Markov Quilt for
Xi, then:

gΘ

e(XQ|Xi) ≤ log

πΘ − exp(−gΘb)

(cid:18) πΘ + exp(−gΘb)
(cid:19)
(cid:19)
(cid:18) πΘ + exp(−gΘa)
(cid:19)
(cid:18) πΘ + exp(−gΘa)
(cid:18) πΘ + exp(−gΘb)
(cid:19)
and if XQ = {Xi+b} is a Markov Quilt for Xi, then:

πΘ − exp(−gΘa)

πΘ − exp(−gΘa)

e(XQ|Xi) ≤2 log

Moreover, if XQ = {Xi−a} is a Markov Quilt for Xi, then:

+2 log

.

.

.

e(XQ|Xi) ≤ log

πΘ − exp(−gΘb)

and if XQ = ∅ is a trivial Markov Quilt for Xi, then
e(XQ|Xi) = 0.
As a consequence of Lemmas 4.5 and 4.7, we can assign
SQ,i, the set of Markov Quilts for node Xi, to:

SQ,i =

(cid:110)
{Xi−a, Xi+b},{Xi−a},{Xi+b},∅
∀a ∈

(cid:110) log(1/πΘ)
(cid:110) log(1/πΘ)

(cid:111)
(cid:111)(cid:111)

, . . . , i − 1

gΘ

,

gΘ

, . . . , T − i

.

b ∈

When gΘ and πΘ are known, (an upper bound on) the score
of each XQ may be calculated quite easily in O(1) time
based on Lemma 4.7. Thus, the computational complexity
of the Markov Quilt mechanism in this case is O(T 3). A
more optimized version which runs in O(T 2) time in some
good cases is derived in Appendix B.2.

Privacy of Correlated Data

Generalization
to Vector-valued Functions. The
Markov Quilt Mechanism can be easily generalized to
vector-valued functions F . If F is 1-Lipschitz with respect
to L1 norm, then from Proposition 1 of (Dwork et al.,
2006), adding noise drawn from σmax · Lap(1) to each out-
put dimension of F guarantees -Pufferﬁsh privacy, where
σmax is the quantity in the Markov Quilt Mechanism.

when the remaining parameters are ﬁxed, utility improves
with larger  as well as larger πest and larger gest. An in-
teresting observation is that πest is less inﬂuential to utility
than gest; this agrees with the dependence of max-inﬂuence
on πest in Lemma A.2. Finally, Figure 2c shows that as
the total sequence length T gets larger, our Markov Quilt
Mechanism outputs more accurate results.

6. Related Work
There is a great deal of work on differential privacy (Dwork
et al., 2006) and differentially private mechanisms, espe-
cially for machine learning. For more details, see (Sarwate
and Chaudhuri, 2013; Dwork and Roth, 2013).
Our work uses Pufferﬁsh, a recent generalization of differ-
ential privacy introduced by (Kifer and Machanavajjhala,
2014). (Kifer and Machanavajjhala, 2014) provides a num-
ber of elegant examples of Pufferﬁsh with associated pri-
vacy mechanisms. (He et al., 2014) provides practical and
efﬁcient privacy algorithms for speciﬁc Pufferﬁsh frame-
works. However, none of these works provide a fully gen-
eral Pufferﬁsh mechanism, nor do they provide algorithms
for time series and social network applications. (Kessler
et al., 2015) apply Pufferﬁsh to smart-meter data; instead
of directly modeling correlation through Markov models,
they add noise to the wavelet coefﬁcients of the time se-
ries that correspond to different frequencies. Finally, (Yang
et al., 2015) consider privacy issues in correlated data with
privacy deﬁnition that is similar to Pufferﬁsh; unlike us,
they look at correlation that can be modeled by a Gaus-
sian Markov Random Field, and their privacy deﬁnition ac-
counts for the adversary’s prior knowledge more explicitly.
Finally, coupled-worlds privacy (Bassily et al., 2013) is an
elegant privacy framework alternative to Pufferﬁsh, which
is also capable of accounting for correlation in data. How to
model complex kinds of correlated data in this framework,
and how to design privacy mechanisms for these models is
left as an avenue for future work.

1, . . . , f(cid:48)

1, . . . , f(cid:48)

5. Simulations
We now illustrate the performance of the Markov Quilt
Mechanism through simulations. We consider the setting
in Section 4.3 where the underlying Bayesian network is an
aperiodic, irreducible and reversible discrete time Markov
chain with k states. The function F we use is the his-
togram of states over time t = 1, . . . , T . The output of
F is a k-dimensional vector (f1, . . . , fk) where fi is the
frequency of state i in times 1 to T , and the desired output
is noisy frequencies (f(cid:48)
k). This histogram function
is 2/T -Lipschitz with respect to the L1 norm, and thus can
be scaled to ﬁt our mechanism.
Synthetic data is generated as follows. Firstly, we choose
two generating parameters πgen ∈ (0, 1/k), ggen ∈ (0, 1)
and generate an aperiodic, irreducible and reversible tran-
sition matrix P such that πmin(P ) ≥ πgen and g(P ) ≥ ggen.
Then we generate a random vector q ∈ Rk uniformly from
the probability simplex as the initial distribution. A se-
quence X = {Xi}T
i=1, Xi ∈ [k] is then generated from
the Markov Chain induced by P and q as our dataset.
Notice that the transition matrix P or the generating param-
eters πgen, ggen are not known to the private mechanism; the
mechanism only knows conservative estimates of πgen and
ggen, denoted by πest and gest. We enforce that πest ≤ πgen
and gest ≤ ggen. Therefore, πest, gest together with  can
be viewed as a set of “privacy parameters”. As  gets
smaller, the mechanism guarantees more privacy; as πest or
gest gets smaller, the mechanism is able to guarantee pro-
tection against a larger set of distributions.
Utility of the mechanism is measured by the L1 distance
between the output (f(cid:48)
k) with (f1, . . . , fk), the ex-
act frequencies. A smaller distance implies higher utility.
In our simulations, we investigate the effect on utility of
the three privacy parameters, as well as T , the total length
of the sequence. In each simulation, we keep two of the
parameters {πest, gest, T} ﬁxed, and vary the remaining one
and . We plot L1 distance on the y-axis as a function of
log . P, q are generated with k = 5 and generating pa-
rameters πgen = 0.05, ggen = 0.1, and are ﬁxed through-
out the simulations. We repeat each simulation 100 times
and plot the averaged error in the ﬁgures. A new dataset
X = {Xi}T
Figure 2 shows the results. We observe that as expected,

i=1 is generated at each repeated run.

Privacy of Correlated Data

(a) Vary πest. (gest = 0.1, T = 5000).

(b) Vary gest. (πest = 0.05, T = 5000).

(c) Vary T . (gest = 0.1, πest = 0.05).

Figure 2. Averaged L1 distance between private and non-private histograms vs.
(a).πest (b).gest (c). T .

log(). Different lines represent different values of

Acknowledgements.

We thank Mani Srivastava for inspiring us to work on the
physical activity measurement problem and for early dis-
cussions, and Supriyo Chakravarty for helpful discussions.
We thank NSF for supporting this work under IIS 1253942
and NIH for partially supporting it under U54 HL108460.

References
R. Bassily, A. Groce, J. Katz, and A. Smith. Coupled-
worlds privacy: Exploiting adversarial uncertainty in sta-
tistical data privacy. In Foundations of Computer Science
(FOCS), 2013 IEEE 54th Annual Symposium on, pages
439–448. IEEE, 2013.

R. Bassily, A. Smith, and A. Thakurta. Private empirical

risk minimization, revisited. arXiv:1405.7085, 2014.

K. Chaudhuri, C. Monteleoni, and A. D. Sarwate. Differ-
entially private empirical risk minimization. Journal of
Machine Learning Research, 12:1069–1109, 2011.

K. Chaudhuri, A. D. Sarwate, and K. Sinha. Near-optimal
differentially private principal components. In Advances
in Neural Information Processing Systems, pages 998–
1006, 2012.

C. Dwork and A. Roth. The algorithmic foundations of
differential privacy. Theoretical Computer Science, 9(3-
4):211–407, 2013.

C. Dwork, F. McSherry, K. Nissim, and A. Smith. Cali-
In

brating noise to sensitivity in private data analysis.
Theory of Cryptography, 2006.

C. Dwork, M. Naor, T. Pitassi, and G. N. Rothblum. Differ-
ential privacy under continual observation. In Proceed-
ings of the forty-second ACM symposium on Theory of
computing, pages 715–724. ACM, 2010a.

C. Dwork, G. Rothblum, and S. Vadhan. Boosting and dif-

ferential privacy. In FOCS, 2010b.

M. Hardt and A. Roth. Beyond worst-case analysis in pri-

vate singular vector computation. In STOC, 2013.

X. He, A. Machanavajjhala, and B. Ding. Blowﬁsh privacy:
tuning privacy-utility trade-offs using policies. In Inter-
national Conference on Management of Data, SIGMOD
2014, Snowbird, UT, USA, June 22-27, 2014, pages
1447–1458, 2014.

H. Imtiaz and A. D. Sarwate. Symmetric matrix perturba-

tion for differentially private pca. In ICASSP, 2015.

P. Jain, P. Kothari, and A. Thakurta. Differentially private

online learning. In COLT, 2012.

K. Chaudhuri, D. Hsu, and S. Song. The large margin
In

mechanism for differentially private maximization.
NIPS, 2014.

S. Kessler, E. Buchmann, and K. B¨ohm. Deploying and
evaluating pufferﬁsh privacy for smart meter data. Karl-
sruhe Reports in Informatics, 1, 2015.

J. Duchi, M. J. Wainwright, and M. Jordan. Local privacy
and minimax bounds: Sharp rates for probability esti-
mation. In Advances in Neural Information Processing
Systems 26, pages 1529–1537, 2013.

D. Kifer and A. Machanavajjhala. No free lunch in data
In Proceedings of the ACM SIGMOD Inter-
privacy.
national Conference on Management of Data, SIGMOD
2011, Athens, Greece, June 12-16, 2011, pages 193–204,
2011.

C. Dwork and J. Lei. Differential privacy and robust statis-
tics. In Proceedings of the 41st annual ACM symposium
on Theory of computing, pages 371–380. ACM, 2009.

D. Kifer and A. Machanavajjhala. Pufferﬁsh: A frame-
work for mathematical privacy deﬁnitions. ACM Trans.
Database Syst., 39(1):3, 2014.

11.522.533.50.020.040.060.080.10.12log(ε)L1 error  0.050.040.030.020.01πest11.522.533.50.050.10.150.2log(ε)L1 error  0.10.090.080.070.060.05gest11.522.533.50.050.10.150.20.250.30.350.40.45log(ε)L1 error  10002000300040005000TPrivacy of Correlated Data

D. Kifer, A. Smith, and A. Thakurta. Private convex op-
timization for empirical risk minimization with applica-
tions to high-dimensional regression. In COLT, 2012.

D. Levin, Y. Peres, and E. Wilmer. Markov Chains and

Mixing Time. American Mathematical Society, 2009.

F. McSherry and K. Talwar. Mechanism design via differ-

ential privacy. In FOCS, 2007.

K. Nissim, S. Raskhodnikova, and A. Smith. Smooth sen-
In Pro-
sitivity and sampling in private data analysis.
ceedings of the Thirty-Ninth Annual ACM Symposium
on Theory of Computing (STOC ’07), pages 75–84, New
York, NY, USA, 2007. ACM. doi: 10.1145/1250790.
1250803. URL http://dx.doi.org/10.1145/
1250790.1250803.

A. Sarwate and K. Chaudhuri. Signal processing and ma-
chine learning with differential privacy: Algorithms and
challenges for continuous data. Signal Processing Mag-
azine, IEEE, 30(5):86–94, Sept 2013. ISSN 1053-5888.
doi: 10.1109/MSP.2013.2259911.

Y. Wang, S. E. Fienberg, and A. J. Smola. Privacy for free:
Posterior sampling and stochastic gradient monte carlo.
In Proceedings of the 32nd International Conference on
Machine Learning, ICML 2015, Lille, France, 6-11 July
2015, pages 2493–2502, 2015.

B. Yang, I. Sato, and H. Nakagawa. Bayesian differen-
In Proceedings of the
tial privacy on correlated data.
2015 ACM SIGMOD International Conference on Man-
agement of Data, pages 747–762. ACM, 2015.

Privacy of Correlated Data

A. Proofs
A.1. Proof to Theorem 3.2 (Privacy of the Wasserstein Mechanism)
Proof. Let (si, sj) be a pair of secrets in Q and let θ ∈ Θ such that p(si|θ) > 0 and p(sj|θ) > 0. Recall that µi,θ =
p(F (X) = ·|si, θ), and µj,θ is deﬁned similarly. Let γ∗ = γ∗(µi,θ, µj,θ) be the coupling between µi,θ and µj,θ that
achieves the ∞-Wasserstein distance.
Using M to denote the Wasserstein mechanism, we can write for any w,

(cid:82)
p(M (X) = w|si, θ)
(cid:82)
p(M (X) = w|sj, θ)
(cid:82)
t p(F (X) = t|si, θ)p(Z = w − t)dt
(cid:82)
s p(F (X) = s|sj, θ)p(Z = w − s)ds
t p(F (X) = t|si, θ)e−|w−t|/W dt
(cid:82)
(cid:82) t+W
s p(F (X) = s|sj, θ)e−|w−s|/W ds
(cid:82)
(cid:82) s+W
s=t−W γ∗(t, s)e−|w−t|/W dsdt
t=s−W γ∗(t, s)e−|w−s|/W dtds

s

t

=

=

=

(5)

Here the ﬁrst step follows from the deﬁnition of the Wasserstein mechanism, the second step from properties of the Laplace
distribution, and the third step from the following property of ∞-Wasserstein distance:

(cid:90) t+W
(cid:90) s+W

s=t−W

t=s−W

γ∗(t, s)ds,

γ∗(t, s)dt.

p(F (X) = t|si, θ) =

p(F (X) = s|sj, θ) =

Observe that in the last step of (5), |s − t| ≤ W in both the numerator and denominator integrals; therefore we have the
following inequality(cid:82)
(cid:82)

(cid:82) t+W
(cid:82) s+W
s=t−W γ∗(t, s)e−|w−t|/W dsdt
t=s−W γ∗(t, s)e−|w−s|/W dtds ≤ e

(cid:82) t+W
(cid:82) s+W
s=t−W γ∗(t, s)e−|w−s|/W dsdt
t=s−W γ∗(t, s)e−|w−s|/W dtds

(cid:82)
(cid:82)

.

s

t

t

s

By deﬁnition of ∞-Wasserstein distance, γ∗(t, s) = 0 when |s − t| > W . Therefore, the right hand side of the above
inequality is equal to

e

t

(cid:82)
(cid:82)
(cid:82)
(cid:82)
s γ∗(t, s)e−|w−s|/W dsdt
t γ∗(t, s)e−|w−s|/W dtds ≤ e.
p(M (X)=w|sj ,θ) ≥ e−. Combining these two facts concludes the proof of the

s

Similar argument can be used to show that p(M (X)=w|si,θ)
theorem.

A.2. Proof to Theorem 4.4 (Privacy of the Markov Quilt Mechanism)

Proof. Pick a speciﬁc secret pair (si
b) ∈ Q and a ﬁxed θ ∈ Θ. Let XQ be the Markov Quilt for Xi which has the
minimum score σ(XQ), and suppose that deleting XQ breaks up the underlying Bayesian network into XN and XR where
Xi ∈ XN .
For any w we have

a, si

p(F (X) + σmax · Z = w|Xi = a, θ)
p(F (X) + σmax · Z = w|Xi = b, θ)

(cid:82)
(cid:82)

=

xR∪Q

xR∪Q

p(F (X) + σmaxZ = w|Xi = a, XR∪Q = xR∪Q, θ)p(XR∪Q = xR∪Q|Xi = a, θ)dxR∪Q
p(F (X) + σmaxZ = w|Xi = b, XR∪Q = xR∪Q, θ)p(XR∪Q = xR∪Q|Xi = b, θ)dxR∪Q

.

(6)

Privacy of Correlated Data

Recall that for any value xR∪Q of XR∪Q,
p(XR∪Q = xR∪Q|Xi = a, θ)
p(XR∪Q = xR∪Q|Xi = b, θ)
Since XQ is a Markov Quilt for Xi and Xi /∈ XR, we have p(XR|XQ, Xi = a, θ) = p(XR|XQ, Xi = b, θ); moreover, by
deﬁnition of max-inﬂuence, p(XQ=xQ|Xi=a,θ)
Therefore, the right hand side of (6) is at most:

p(XR = xR|XQ = xQ, Xi = a, θ)p(XQ = xQ|Xi = a, θ)
p(XR = xR|XQ = xQ, Xi = b, θ)p(XQ = xQ|Xi = b, θ)

=

.

ee(XQ|Xi) ·

p(F (X) + σmaxZ = w|Xi = a, XR∪Q = xR∪Q, θ)dxR∪Q
p(F (X) + σmaxZ = w|Xi = b, XR∪Q = xR∪Q, θ)dxR∪Q

xR∪Q

.

(7)

p(XQ=xQ|Xi=b,θ) ≤ ee(XQ|Xi).
(cid:82)
(cid:82)

xR∪Q

Since F is 1-Lipschitz, for any ﬁxed value of XR∪Q, F (X) can vary by at most card(XN ) (potentially when all the
variables in XN change values). Since σmax ≥ card(XN )

−e(XQ|Xi) and Z ∼ Lap(1), for any xR∪Q, we have:

p(F (X) + σmaxZ = w|Xi = a, XR∪Q = xR∪Q, θ)
p(F (X) + σmaxZ = w|Xi = b, XR∪Q = xR∪Q, θ) ≤ e−e(XQ|Xi).

Combining this with (7), for any w and any secret pair (si

a, si

b) we have
p(F (X) + σmaxZ = w|θ, si
a)
b) ≤ e,
p(F (X) + σmaxZ = w|θ, si

which completes the proof of the theorem.

A.3. Proof of Lemma 4.5

Proof. It is easy to check that XR, XQ, XN partitions the node set X, thus meets the ﬁrst criterion of 4.3.
It remains to check that XR is conditional independent of Xi given XQ. This can be veriﬁed by the d-separation criteria.
If the any path from Xi to nodes in XR is d-separated by XQ, then the conditional independence claim will hold. In a
discrete Markov chain, there exists only one path between Xi and any node Xj ∈ XR. Since the path connects the nodes
in ascending index order, there must be a node in XQ that blocks the path. Hence Xi is conditionally independent of set
XR given XQ. The second criterion of 4.3 is met.
Therefore, the choice of XQ in Lemma 4.5 is indeed an Markov Quilt.

A.4. Proof of Lemma 4.7

The main ingredient in the proof of Lemma 4.7 is the following (fairly standard) result in Markov Chain theory:
Lemma A.1. Consider a k-state discrete time Markov Chain with a transition matrix P with eigengap g∗. Let π be the
stationary distribution of the chain and let πmin = minx π(x) be the minimum probability of any state in the stationary
distribution. If P t is the t-th power of P such that P t(x, y) = Pr(Xi+t = y|Xi = x), then,

(cid:12)(cid:12)(cid:12)(cid:12) P t(x, y)

π(y) − 1

(cid:12)(cid:12)(cid:12)(cid:12) ≤

exp(−tg∗)

πmin

.

Or equivalently,

(1 − ∆t)π(y) ≤ P t(x, y) ≤ (1 + ∆t)π(y), ∆t =
This lemma, along with some algebra, sufﬁces to show the following two facts:
Lemma A.2. Suppose t > log(1/πΘ)

, and ∆t = exp(−tgΘ)

gΘ

πΘ

. Then, for any j, any θ ∈ Θ, and any x, x(cid:48) and y,

exp(−tg∗)

πmin

.

1 − ∆t
1 + ∆t ≤

Pr(Xt+j = y|Xj = x, θ)
Pr(Xt+j = y|Xj = x(cid:48), θ) ≤

1 + ∆t
1 − ∆t

.

Privacy of Correlated Data

Lemma A.3. Suppose t > log(1/πΘ)

and ∆t = exp(−tgΘ)

πΘ

. Then, for any j > 0, any θ ∈ Θ, and any x, x(cid:48) and y,

(cid:19)2

gΘ

(cid:18) 1 − ∆t

1 + ∆t

≤

Pr(Xj = y|Xj+t = x, θ)
Pr(Xj = y|Xj+t = x(cid:48), θ) ≤

(cid:18) 1 + ∆t

(cid:19)2

.

1 − ∆t

Proof. (of Lemma 4.7) First, we can express e(XQ|Xi) as

e(XQ|Xi) = max

x,x(cid:48)∈X sup
θ∈Θ

max

xQ∈X |Q|

log

Pr(XQ = xQ|Xi = x, θ)
Pr(XQ = xQ|Xi = x(cid:48), θ)

.

For any x, x(cid:48) and any xQ, by conditional independence,

Pr(XQ = xQ|Xi = x, θ)
Pr(XQ = xQ|Xi = x(cid:48), θ)

=

=

Pr(Xi+b = xi+b, Xi−a = xi−a|Xi = x, θ)
Pr(Xi+b = xi+b, Xi−a = xi−a|Xi = x(cid:48), θ)
Pr(Xi+b = xi+b|Xi = x, θ)
Pr(Xi+b = xi+b|Xi = x(cid:48), θ)

Pr(Xi−a = xi−a|Xi = x, θ)
Pr(Xi−a = xi−a|Xi = x(cid:48), θ)

.

Taking log on both sides, we have

log

Pr(XQ = xQ|Xi = x, θ)
Pr(XQ = xQ|Xi = x(cid:48), θ)

= log

Pr(Xi+b = xi+b|Xi = x, θ)
Pr(Xi+b = xi+b|Xi = x(cid:48), θ)

+ log

Pr(Xi−a = xi−a|Xi = x, θ)
Pr(Xi−a = xi−a|Xi = x(cid:48), θ)

.

For any θ ∈ Θ, let P be the corresponding transition matrix, and π, g∗ be the stationary distribution and eigengap of P .
Let ∆θ

. Then by Lemma A.2 and Lemma A.3, we have

t = exp(−tg∗)

πmin

log

Pr(Xi+b = xi+b|Xi = x, θ)
Pr(Xi+b = xi+b|Xi = x(cid:48), θ) ≤ log
(cid:18) 1 + ∆θ

1 + ∆θ
t
t ≤ log
1 − ∆θ
(cid:19)2

1 + ∆t
1 − ∆t

(cid:18) 1 + ∆t

= log

(cid:19)2

Pr(Xi−a = xi−a|Xi = x, θ)
Pr(Xi−a = xi−a|Xi = x(cid:48), θ) ≤ log

t

1 − ∆θ

t

≤ log

1 − ∆t

and

log

(cid:19)
Combining the two inequalities together, when XQ = {Xi−a, Xi+b}, we have

(cid:18) πΘ + exp(−gΘb)

log

Pr(XQ = xQ|Xi = x, θ)
Pr(XQ = xQ|Xi = x(cid:48), θ) ≤ log

πΘ − exp(−gΘb)

+ 2 log

πΘ − exp(−gΘb)

(cid:18) πΘ + exp(−gΘb)
(cid:19)
(cid:18) πΘ + exp(−gΘa)
(cid:19)

(cid:18) πΘ + exp(−gΘa)

πΘ − exp(−gΘa)

.

πΘ − exp(−gΘa)

= 2 log

(cid:19)

.

log

Pr(Xi+b=xi+b|Xi=x(cid:48),θ) term will degenerate to 1, thus

Moreover, when XQ = {Xi−a}, the Pr(Xi+b=xi+b|Xi=x,θ)
Pr(XQ = xQ|Xi = x, θ)
Pr(XQ = xQ|Xi = x(cid:48), θ) ≤ 2 log
πΘ − exp(−gΘa)
Pr(Xi−a=xi−a|Xi=x(cid:48),θ) term will degenerate to 1, thus

Similarly, when XQ = {Xi+b}, the Pr(Xi−a=xi−a|Xi=x,θ)
Pr(XQ = xQ|Xi = x, θ)
Pr(XQ = xQ|Xi = x(cid:48), θ) ≤ log

(cid:18) πΘ + exp(−gΘa)
(cid:19)
(cid:18) πΘ + exp(−gΘb)

πΘ − exp(−gΘb)

log

(cid:19)

.

.

Since the above results hold for any x, x(cid:48), xQ and θ, we can conclude the three statements of Lemma 4.7.

A.5. Proof to Lemma A.1

The full standard proof can be found at (Levin et al., 2009).

Privacy of Correlated Data

Proof. Let P be the transition matrix. Construct a matrix A s.t. A(x, y) = π(x)1/2π(y)−1/2P (x, y). A is symmetric since
the chain is reversible, therefore A can be written as

, in which V is orthonormal.
Deﬁne another inner product

A = V ΛV −1 = V ΛV T

(cid:88)

x∈Ω

(cid:104)·,·(cid:105)π :=

f (x)g(x)π(x)

We call a matrix π-orthonormal if its basis are orthonormal with respect to (cid:104)·,·(cid:105)π.
Let D = diagπ. Since A = D1/2P D−1/2 by the construction of A and A = V ΛV T , we have

P [D−1/2V ] = D−1/2V Λ

Let U = D−1/2V and let ui, vi represent the i-th column vector of U and V respectively, we observe

(cid:88)
(cid:88)

x∈Ω

x∈Ω

(cid:104)ui, uj(cid:105)π =

=

ui(x)uj(x)π(x)

ui(x)π(x)1/2uj(x)π(x)1/2

=(cid:104)vi, vj(cid:105)

Thus P is π-orthonormal with {ui} as its basis.
(cid:104)ui, uj(cid:105)π = 1 when i = j and (cid:104)ui, uj(cid:105)π = 0 otherwise.
Then express P t in terms of U,

Therefore,

P t =D1/2AtD1/2

=D−1/2V ΛV T D1/2
=[D−1/2V ]Λ[V T D]D−1/2
=U ΛU T D

(cid:88)

i

P t(x, y) =

ui(y)λt

iui(x)π(y)

Since the chain is aperiodic and irreducible, P has a unique largest eigenvalue λ1 = 1, we can rewrite the expression above
as

|Ω|(cid:88)

i=2

P t(x, y) = 1 +

ui(y)λt

iui(x)π(y)

Privacy of Correlated Data

Apply Cauchy-Schwarz inequality, (cid:12)(cid:12)(cid:12)(cid:12) P t(x, y)

π(y) − 1

(cid:12)(cid:12)(cid:12)(cid:12) =

≤

ui(y)λt

iui(x)

ui(y)λt∗ui(x)

i=2

|Ω|(cid:88)
|Ω|(cid:88)
 |Ω|(cid:88)

i=2

|Ω|(cid:88)

1/2

ui(y)2

≤λt∗

ui(x)2

i=2

i=2

, where λ∗ is the eigenvalue of P whose absolute value is the second largest. Furthermore,

|Ω|(cid:88)
(cid:68)(cid:80)|Ω|
i=1 ui(x)π(x)ui,(cid:80)|Ω|
(cid:12)(cid:12)(cid:12)(cid:12) P t(x, y)

i=2

π(y) − 1

(cid:12)(cid:12)(cid:12)(cid:12) ≤

i=1

|Ω|(cid:88)
ui(x)2 ≤ π(x)−1
(cid:69)
= π(x)2(cid:80)|Ω|
λt∗(cid:112)π(x)π(y) ≤

e−(1−λ∗)t

πmin

ui(x)2 ≤

e−gt
πmin

=

because π(x) =

i=1 ui(x)π(x)ui

i=1 ui(x)2 We eventually get

, where g = 1 − λ∗ denotes the eigengap of P .
A.6. Proof to Lemma A.2
Proof. Consider any underlying distribution θ ∈ Θ with transition matrix P , and let the stationary distribution be π and
the eigengap of P be g∗. We have

Pr(Xt+j = y|Xj = x)
Pr(Xt+j = y|Xj = x(cid:48))

=

P t(x, y)
P t(x(cid:48), y)

=

P t(x, y)/π(y)
P t(x(cid:48), y)/π(y)

.

Based on Lemma A.1, for ∆θ

with πmin = minx π(x), we have

t = exp(−tg∗)

πmin

Recall that 0 ≤ ∆θ

1 − ∆θ

t ≤ P t(x, y)/π(y) ≤ 1 + ∆θ
t .
t ≤ ∆t ≤ 1, where ∆t is as deﬁned in Lemma A.2. Therefore
1 − ∆t ≤ P t(x, y)/π(y) ≤ 1 + ∆t

and the lemma follows.

A.7. Proof to Lemma A.3
Proof. Consider any underlying distribution θ ∈ Θ with transition matrix P , and let the stationary distribution be π and
the eigengap of P be g∗. By Bayes’ rule we have

Pr(Xj = y|Xt+j = x)
Pr(Xj = y|Xt+j = x(cid:48))
t = exp(−tg∗)

πmin

Let ∆θ

with πmin = minx π(x). Based on Lemma A.1, we have

=

Pr(Xt+j = x|Xj = y) Pr(Xj = y)/ Pr(Xt+j = x)
Pr(Xt+j = x(cid:48)|Xj = y) Pr(Xj = y)/ Pr(Xt+j = x(cid:48))

P t(y, x) Pr(Xt+j = x(cid:48))
P t(y, x(cid:48)) Pr(Xt+j = x)

.

=

(1 − ∆θ

t )π(x) ≤ P t(y, x) ≤ (1 + ∆θ

t )π(x).

We also have

and similarly,

Therefore

Privacy of Correlated Data

P (Xt+j = x|Xj = y)P (Xj = y)
P (Xt+j = x|Xj = y) = max P t(y, x)
t )π(x),

P (Xt+j = x) =

(cid:88)

y

y

≤ max
≤ (1 + ∆θ
(cid:88)

P (Xt+j = x) =

y

P (Xt+j = x|Xj = y)P (Xj = y)
P (Xt+j = x|Xj = y) = min

y

P t(y, x)

y

≥ min
≥ (1 − ∆θ

t )π(x).

(cid:19)2

(cid:18) 1 − ∆θ

t
1 + ∆θ
t

P t(y, x) Pr(Xt+j = x(cid:48))
P t(y, x(cid:48)) Pr(Xt+j = x) ≤

≤

(cid:18) 1 + ∆θ

t

(cid:19)2

1 − ∆θ

t

.

t ≤ ∆t ≤ 1, where ∆t is as deﬁned in Lemma A.2.

The lemma follows from the fact that 0 ≤ ∆θ
B. Implementing the Markov Quilt Mechanism for Markov Chains
In this section, we provide a detailed description of the Markov Quilt Mechanism for Markov chain, along with a more
computationally efﬁcient version of the algorithm. Throughout this section we assume that the Pufferﬁsh parameters
(S,Q, Θ) and the corresponding gΘ, πΘ are known.
B.1. The Markov Quilt Mechanism for Markov Chains

Algorithm 1 presents the Markov Quilt mechanism for a Markov Chain. Recall that in this case, the set SQ,i of plausible
Markov Quilts for node Xi that we search over is of the form:

(cid:110)
{Xi−a, Xi+b},{Xi−a},{Xi+b},∅,∀a ∈

(cid:110) log(1/πΘ)

SQ,i =

(cid:111)

(cid:110) log(1/πΘ)

(cid:111)(cid:111)

gΘ

, . . . , i − 1

, b ∈

gΘ

, . . . , T − i

.

(8)

Algorithm 1 enumerates over all Markov Quilts in SQ,i for all nodes Xi in the chain, computes the requisite scores, and
calculates σmax, which is (approximately) the minimum standard deviation of the amount of noise needed to keep the
values of all nodes private. Finally, it adds Laplace noise with standard deviation proportional to σmax to the exact value
of F (D) to ensure privacy.

Algorithm 1 BasicMarkovQuiltMechansim(, F , D)

Initialize σmax ← 0
for Xi ∈ X do
σi ← FindBestMarkovQuilt(i, )
σmax = max(σmax, σi)
end for
return F (D) + σmax · Lap(1)

Algorithm 2 shows the FindBestMarkovQuilt function that returns the minimal score of any Markov Quilt in SQ,i. This is
computed by searching over the set SQ,i of candidate Markov quilts for Xi.

Privacy of Correlated Data

Algorithm 2 FindBestMarkovQuilt(i, )
σi ← ∞
(a∗, b∗) ← (i, n − i + 1)
for (Xi−a, Xi+b), a, b ≥ log(1/πΘ)/gΘ that is a Markov Quilt for Xi do
if i − a = 0 and i + b = n + 1 then
(cid:17)
(cid:16) πΘ+exp(−gΘb)
e(a, b) ← 0
else if i − a = 0 then
(cid:17)
(cid:16) πΘ+exp(−gΘa)
e(a, b) ← log
else if i + a = n + 1 then
(cid:17)
(cid:16) πΘ+exp(−gΘb)

(cid:16) πΘ+exp(−gΘa)

πΘ−exp(−gΘa)

πΘ−exp(−gΘb)

(cid:17)

πΘ−exp(−gΘb)

+ 2 log

πΘ−exp(−gΘa)

else

e(a, b) ← 2 log
e(a, b) ← log

end if
if e(a, b) >  then

Pass

else
σi ← min( a+b−1
If σi = a+b−1
end if
end for
return σi, a∗, b∗

−e(a,b) , σi)

−e(a,b) then (a∗, b∗) ← (a, b).

B.2. A Faster Algorithm

BasicMarkovQuiltMechansim needs to enumerate over all nodes in the chain and compute the best Markov quilt for each
one. This can be improved in most cases. The intuition is that the best Markov quilt for a node near the center of the
chain should also be a privacy-preserving choice of quilt for a node near the end of the chain. For ease of presentation, we
introduce two imaginary nodes X0 and Xn+1 at either end of the chain. We prepend X0 to to X1 and append Xn+1 to Xn.
Quilt {X0, Xi+b} in the algorihm description corresponds to Quilt {Xi+b} in SQ,i, {Xi−a, Xn+1} corresponds to Quilt
{Xi−a}, and Quilt {X0, Xn+1} corresponds to the trivial quilt ∅.
We shall show that the noise parameter σ calculated from this quilt is enough to work for all nodes, that is, preserves
-Pufferﬁsh privacy for all nodes. Starting from a node in the middle of the chain offers better chance of ﬁnding such a
quilt than starting from the end. Algorithm 3 shows the algorithm.
Claim B.1. The standard deviation of the noise added to F (D) by BasicMarkovQuiltMechansim is larger than or equal
to that added by FastMarkovQuiltMechanism.

Proof. This is obvious. The σmax in BasicMarkovQuiltMechansim is the maximum of FindBestMarkovQuilt(i, ) for all
Xis in X , while FastMarkovQuiltMechanism’s σmax is the maximum of of FindBestMarkovQuilt(i, ) for only a subset of
X . Therefore, the amount of noise added to F (D) by BasicMarkovQuiltMechansim cannot be smaller.
Claim B.2. Suppose the best Markov Quilt found by FastMarkovQuiltMechanism is {Xi−a, Xi+b} where i − a ≥ 1 and
i + b ≤ n. Then, FastMarkovQuiltMechanism ensures -Pufferﬁsh privacy.

Proof. First let j (cid:54)= i be such that j − ai ≥ 1 and j + bi ≤ n. Then, {Xj−a, Xj+b} is a Markov Quilt for Xj with score
equal to σi, and thus σmax = σi ≥ σj. Thus the amount of noise added by FastMarkovQuiltMechanism is sufﬁcient to
protect the privacy of Xj.
Next consider a j (cid:54)= i such that j − ai ≥ 1 but j + bi > n. Then, {Xj−a} is a Markov Quilt for Xj with score less than
σi. Thus σmax = σi ≥ σj in this case as well. The ﬁnal case, where j − ai < 1 but j + bi ≤ n is analogous. Thus,
the amount of noise added by FastMarkovQuiltMechanism is sufﬁcient to protect the privacy of such Xj’s, and as such it
preserves -Pufferﬁsh privacy.

Algorithm 3 FastMarkovQuiltMechanism(, F , D)

Privacy of Correlated Data

Initialize σmax ← 0
n = |X|
i = (cid:98) n
2(cid:99)
while i has not being explored do

Mark i as explored
(ai, bi, σi) ← FindBestMarkovQuilt (i, )
σmax = max(σmax, σi)
if i − ai = 0 and i + bi = n + 1 then
break
else if i − ai = 0 then
else if i + bi = n + 1 then

i = i + 1

else

i = i − 1
break

end if

end while
return F (D) + σmax · Lap(1)

For the rest of the proof, observe that there are three cases:

1. FastMarkovQuiltMechanism ﬁnds a node Xi and a corresponding Markov Quilt {Xi−a, Xi+b} such that i − a ≥ 1

and i + b ≤ n. Claim B.2 shows that in this case it ensures -Pufferﬁsh privacy.

2. FastMarkovQuiltMechanism ﬁnds the Markov Quilt {X0, Xn+1}. In this case, it adds noise with parameter n/,

which trivially ensures -Pufferﬁsh privacy.

3. FastMarkovQuiltMechanism ﬁnds a Markov Quilt of the form {X0, Xm} where m ≤ n. This case will only happen
when (a) there exists some k such that the best Markov Quilts for Xk, . . . , Xn/2 are of the form {Xj−aj , Xn+1} and
(b) the best Markov Quilt for Xk−1 is {X0, Xm}. In this case, observe that any Xj for j ≥ k is protected as σj can
only be smaller than the maximum of σk, σk+1, . . . , σn/2. Moreover, any Xj for j ≤ k − 1 is also protected, as σj is
at most σk−1.

4. FastMarkovQuiltMechanism ﬁnd a Markov Quilt of the form {Xm, Xn+1} for m ≥ 1. This case is analogous to the

previous case.

