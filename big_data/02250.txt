6
1
0
2

 
r
a

M
7

 

 
 
]

G
L
.
s
c
[
 
 

1
v
0
5
2
2
0

.

3
0
6
1
:
v
i
X
r
a

Online Sparse Linear Regression

Dean Foster

Amazon

Satyen Kale

Yahoo Research

Howard Karloﬀ
Goldman Sachs

dean@foster.net

satyen@yahoo-inc.com

howard@cc.gatech.edu

Abstract

We consider the online sparse linear regression problem, which is the problem of
sequentially making predictions observing only a limited number of features in each
round, to minimize regret with respect to the best sparse linear regressor, where predic-
tion accuracy is measured by square loss. We give an ineﬃcient algorithm that obtains
regret bounded by ˜O(√T ) after T prediction rounds. We complement this result by
showing that no algorithm running in polynomial time per iteration can achieve regret
bounded by O(T 1−δ) for any constant δ > 0 unless NP ⊆ BPP. This computational
hardness result resolves an open problem presented in COLT 2014 (Kale, 2014) and
also posed by Zolghadr et al. (2013). This hardness result holds even if the algorithm is
allowed to access more features than the best sparse linear regressor up to a logarithmic
factor in the dimension.

1

Introduction

In various real-world scenarios, features for examples are constructed by running some com-
putationally expensive algorithms. With resource constraints, it is essential to be able to
make predictions with only a limited number of features computed per example. One ex-
ample of this scenario, from (Cesa-Bianchi et al., 2011), is medical diagnosis of a disease, in
which each feature corresponds to a medical test that the patient in question can undergo.
Evidently, it is undesirable to subject a patient to a battery of medical tests, for medical
as well as cost reasons. Another example from the same paper is a search engine, where
a ranking of web pages must be generated for each incoming user query and the limited
amount of time allowed to answer a query imposes restrictions on the number of attributes
that can be evaluated in the process. In both of these problems, predictions need to be made
sequentially as patients or search queries arrive online, learning a good model in the process.
In this paper, we model the problem of prediction with limited access to features in
the most natural and basic manner as an online sparse linear regression problem. In this
problem, an online learner makes real-valued predictions for the labels of examples arriving
sequentially over a number of rounds. Each example has d features that can be potentially
accessed by the learner. However, in each round, the learner is restricted to choosing an

1

arbitrary subset of features of size at most k, a budget parameter. The learner then acquires
the values of the subset of features, and then makes its prediction, at which point the true
label of the example is revealed to the learner. The learner suﬀers a loss for making an
incorrect prediction (for simplicity, we use square loss in this paper). The goal of the learner
is to make predictions with total loss comparable to the loss of the best sparse linear regressor
with a bounded norm, where the term sparse refers to the fact that the linear regressor has
nonzero weights on at most k features. To measure the performance of the online learner,
we use the standard notion of regret, which is the diﬀerence between the total loss of the
online learner and the total loss of the best sparse linear regressor.

While regret is the primary performance metric, we are also interested in eﬃciency of the
online learner. Ideally, we desire an online learning algorithm that minimizes regret while
making predictions eﬃciently, i.e., in polynomial time (as a function of d and T ). In this
paper, we prove that this goal is impossible unless there is a randomized polynomial-time
algorithm for deciding satisﬁability of 3CNF formulas, the canonical NP-hard problem. This
computational hardness result resolves open problems from (Kale, 2014) and (Zolghadr et al.,
2013). In fact, the computational hardness persists even if the online learner is given the
additional ﬂexibility of choosing k′ = D log(d)k features for any constant D > 0. In light of
this result, in this paper we also give an ineﬃcient algorithm for the problem which queries

k′ ≥ k + 2 features in each round, that runs in O((cid:0)d

regret bounded by O(

k(cid:1)k′) time per round, and that obtains

d2

(k′−k)2pk log(d)T ).

2 Related Work and Known Results

A related setting is attribute-eﬃcient learning (Cesa-Bianchi et al., 2011; Hazan and Koren,
2012; Kukliansky and Shamir, 2015). This is a batch learning problem in which the examples
are generated i.i.d., and the goal is to simply output a linear regressor using only a limited
number of features per example with bounded excess risk compared to the optimal linear
regressor, when given full access to the features at test time. While the aforementioned
papers give eﬃcient, near-optimal algorithms for this problem, these algorithms do not work
in the online sparse regression setting in which we are interested because here we are required
to make predictions only using a limited number of features.

In (Kale, 2014), a simple algorithm has been suggested, which is based on running a

bandit algorithm in which the actions correspond to selecting one of(cid:0)d

of size k at regular intervals, and within each interval, running an online regression algorithm
(such as the Online Newton-Step algorithm of Hazan et al. (2007)) over the k coordinates
chosen by the bandit algorithm. This algorithm, with the right choice of interval lengths,
has a regret bound of O(k2dk/3T 2/3 log(T /d)). The algorithm has exponential dependence on
k both in running time and the regret. Also, Kale (2014) sketches a diﬀerent algorithm with
performance guarantees similar to the algorithm presented in this paper; our work builds
upon that sketch and gives tighter regret bounds.

k(cid:1) subsets of coordinates

Zolghadr et al. (2013) consider a very closely related setting (called online probing) in
which features and labels may be obtained by the learner at some cost (which may be dif-

2

ferent for diﬀerent features), and this cost is factored into the loss of the learner. In the
special case of their setting corresponding to the problem considered here, they given an
algorithm, LQDExp3, which relies on discretizing all k-sparse weight vectors and running
an exponential-weights experts algorithm on the resulting set with stochastic loss estima-
tors, obtaining a O(√dT ) regret bound. However the running time of their algorithm is
prohibitive: O((dT )O(k)) time per iteration. In the same paper, they pose the open problem
of ﬁnding a computationally eﬃcient no-regret algorithm for the problem. The hardness
result in this paper resolves this open problem.

On the computational hardness side, it is known that it is NP-hard to compute the
optimal sparse linear regressor (Foster et al., 2015; Natarajan, 1995). The hardness result
in this paper is in fact inspired by the work of Foster et al. (2015), who proved that it is
computationally hard to ﬁnd even an approximately optimal sparse linear regressor for an
ordinary least squares regression problem given a batch of labeled data. While these results
imply that it is hard to properly1 solve the oﬄine problem, in the online setting we allow
improper learning, and hence these prior results don’t yield hardness results for the online
problem considered in this paper.

3 Notation and Setup

We use the notation [d] = {1, 2, . . . , d} to refer to the coordinates. All vectors in this paper
are in Rd, and all matrices in Rd×d. For a subset S of [d], and a vector x, we use the notation
x(S) to denote the projection of x on the coordinates indexed by S. We also use the notation
IS to denote the diagonal matrix which has ones in the coordinates indexed by S and zeros
elsewhere: this is the identity matrix on the subspace of Rd induced by the coordinates in
S, as well as the projection matrix for this subspace. We use the notation k·k to denote the
ℓ2 norm in Rd and k · k0 to denote the zero “norm,” i.e., the number of nonzero coordinates.
We consider a prediction problem in which the examples are vectors in Rd with ℓ2 norm
bounded by 1, and labels are in the range [−1, 1]. We use square loss to measure the accuracy
of a prediction: i.e., for a labeled example (x, y) ∈ Rd × [−1, 1], the loss of a prediction ˆy is
(ˆy−y)2. The learner’s task is to make predictions online as examples arrive one by one based
on observing only k out of d features of the learner’s choosing on any example (the learner is
allowed to choose diﬀerent subsets of features to observe in each round). The learner’s goal
is to minimize regret relative to the best k-sparse linear regressor whose ℓ2 norm is bounded
by 1.

Formally, for t = 1, 2, . . . , T , the learner:

1. selects a subset St ⊆ [d] of size at most k,
2. observes xt(St), i.e., the values of the features of xt restricted to the subset St,

3. makes a prediction ˆyt ∈ [−1, 1],
1Proper learning means ﬁnding the optimal sparse linear regressor, whereas improper learning means
ﬁnding an arbitrary predictor with performance comparable to that of the optimal sparse linear regressor.

3

4. observes the true label yt, and suﬀers loss (ˆyt − yt)2.

Deﬁne regret of the learner as

T

T

Regret :=

(w · xt − yt)2.
In case ˆyt is chosen using randomization, we consider expected regret instead.

Xt=1
(ˆyt − yt)2 −

w: kwk0≤k, kwk≤1

min

Xt=1

Given the NP-hardness of computing the optimal k-sparse linear regressor (Foster et al.,
2015; Natarajan, 1995), we also consider a variant of the problem which gives the learner
more ﬂexibility than the comparator: the learner is allowed to choose k′ ≥ k coordinates
to query in each round. The deﬁnition of the regret remains the same. We call this the
(k, k′, d)-online sparse regression problem.

We are interested in the following two goals2:
1. (No Regret) Make predictions ˆyt so that regret is bounded by poly(d)T 1−δ for some

δ > 0.

2. (Eﬃciency) Make these predictions eﬃciently, i.e., in poly(d, T ) time per iteration.

In this paper, we show it is possible to get an ineﬃcient no-regret algorithm for the online
sparse regression problem. Complementing this result, we also show that an eﬃcient no-
regret algorithm cannot exist, assuming the standard hardness assumption that NP 6⊆ BPP.

4 Upper bound

In this section we give an ineﬃcient algorithm for the (k, k′, d)-online sparse regression
problem which obtains an expected regret of O(
to be at least k + 2. It is ineﬃcient because it maintains statistics for every subset of [d] of

(k′−k)2pk log(d)T ). The algorithm needs k′

d2

size k, of which there are (cid:0)d
k(cid:1).

At a high level, the algorithm runs an experts algorithm (speciﬁcally, Hedge) treating
all such subsets as experts. Each expert internally runs stochastic gradient descent only on
the coordinates speciﬁed by the corresponding subset, ensuring low regret to any bounded
norm parameter vector that is nonzero only on those coordinates. The Hedge algorithm
ensures low regret to the best subset of coordinates, and thus the overall algorithm achieves
low regret with respect to any k-sparse parameter vector. The necessity of using k′ ≥ k + 2
features in the algorithm is that the algorithm uses the additional k′ − k features to generate
unbiased estimators for xtx⊤t and ytxt in each round, which are needed to generate stochastic
gradients for all the experts. These estimators have large variance unless k′ − k is large.
The pseudocode is given in Algorithm 1. In the algorithm, in round t, the algorithm
generates a distribution Dt over the subsets of [d] of size k; for any such subset S, we use
the notation Dt(S) to denote the probability of choosing the set S in this distribution. We
also deﬁne the function Π on Rd to be the projection onto the unit ball, i.e., for w ∈ Rd,
Π(w) = w if kwk ≤ 1, and Π(w) = 1
kwk

w otherwise.

2In this paper, we use the poly(·) notation to denote a polynomially-bounded function of its arguments.

4

Algorithm 1 Algorithm for Online Sparse Regression

d , q = (k′−k)(k′−k−1)

1: Deﬁne the parameters p = k′−k
2: Let D1 be the uniform distribution over all subsets of [d] of size k.
3: For every subset S of [d] of size k, let wS,1 = 0, the all-zeros vector in Rd.
4: for t = 1, 2, . . . , T do
5:

, ηHedge = qq ln(d)

Sample a subset ˆSt of [d] of size k from Dt, and a subset Rt of [d] of size k′ − k drawn
uniformly at random, independently of ˆSt.

T , and ηSGD = qq 1

d(d−1)

T .

6: Acquire xt(St) for St := ˆSt ∪ Rt.
7: Make the prediction ˆyt = w ˆSt,t · xt and obtain the true label yt.

Compute the matrix Xt ∈ Rd×d and the vector zt ∈ Rd deﬁned as follows:

8:

xt(i)2

q

p

0

xt(i)xt(j)

if i = j and i ∈ Rt
if i 6= j and i, j ∈ Rt
otherwise,

Xt(i, j) =

Dt+1(S) = Dt(S) exp(−ηHedge(w⊤S,tXtwS,t − 2z⊤t wS,t + y2

zt(i) =( ytxt(i)

and

0

p

9: Update the distribution over the subsets: for all subsets S of [d] of size k, let

if i ∈ Rt
otherwise,

t ))/Zt,

where Zt is the normalization factor to make Dt+1 a distribution.
For each subset S of [d] of size k, let

10:

wS,t+1 = Π(wS,t − 2ηSGDIS(XtwS,t − zt)).

11: end for

Theorem 1. There is an algorithm for the online sparse regression problem with any given

d2

O(

parameters (k, k′, d) such that k′ ≥ k + 2 running in O((cid:0)d
(k′−k)2pk log(d)T ) expected regret.

k(cid:1) · k′) time per iteration with

Proof. The algorithm is given in Algorithm 1. Since the algorithm maintains a parameter
vector in Rk for each subset of [d] of size k, the running time is dominated by the time to
sample from Dt and update it, and the time to update the parameter vectors. The updates

time.

can be implemented in O(k′) time, so overall each round can be implemented in O((cid:0)d
k(cid:1) · k′)
We now analyze the regret of the algorithm. Let Et[·] denote the expectation conditioned
on all the randomness prior to round t. Then, it is easy to check, using the fact that k′−k ≥ 2,
that the construction of Xt and zt in Step 8 of the algorithm has the following property:

Et[Xt] = xtx⊤t and Et[zt] = ytxt.

(1)

Next, notice that in Step 9, the algorithm runs the standard Hedge-algorithm update

(see, for example, Section 2.1 in (Arora et al., 2012)) on (cid:0)d

k(cid:1) experts, one for each subset of

5

[d] of size k, where, in round t, the cost of the expert corresponding to subset S is deﬁned
to be3

q ) = O(

w⊤S,tXtwS,t − 2z⊤t wS,t + y2
t .

algorithm here, the standard regret bound for Hedge (Arora et al., 2012, Theorem 2.3) with

(2)
It is easy to check, using the facts that kxtk ≤ 1, kwS,tk ≤ 1 and p ≥ q, that the cost (2)
is bounded deterministically in absolute value by O( 1
(k′−k)2 ). Let EDt[·] denote the
expectation over the random choice of ˆSt from the distribution Dt conditioned on all other
k(cid:1) ≤ dk experts in the Hedge
k(cid:1) ≤ k ln d,
(k′−k)2pk ln(d)T ).

randomness up to and including round t. Since there are (cid:0)d
the speciﬁed value of ηHedge implies that for any subset S of [d] of size k, using ln(cid:0)d
Xt=1

(3)
Next, we note, using (1) and the fact that conditioned on the randomness prior to round t,
wS,t is completely determined, that (for any S)

EDt[w ˆSt,tXtw ˆSt,t−2z⊤t w ˆSt,t+y2

(w⊤S,tXtwS,t−2z⊤t wS,t+y2

Xt=1

t )+O(

d2

we have

T

d2

T

t ] ≤

t = (wS,t · xt − yt)2. (4)
Et[w⊤S,tXtwS,t − 2z⊤t wS,t + y2
Taking expectations on both sides of (3) over all the randomness in the algorithm, and using
(4), we get that for any subset S of [d] of size k, we have

t ] = w⊤S,txtx⊤t wS,t − 2ytx⊤t wS,t + y2

T

T

d2

Xt=1

E[(wS,t · xt − yt)2] + O(

(k′−k)2pk log(d)T ).

Xt=1
E[(ˆyt − yt)2]. We now analyze the right-hand side.

E[(w ˆSt,t · xt − yt)2] ≤
The left-hand side of (5) equals PT
For any given subset S of [d] of size k, we claim that in Step 10 of the algorithm, the
parameter vector wS,t is updated using stochastic gradient descent with the loss function
ℓt(w) := (x⊤t ISw − yt)2 over the set over {w | ||w||2 ≤ 1}, only on the coordinates in S,
while the coordinates not in S are ﬁxed to 0. To prove this claim, ﬁrst, we note that the
premultiplication by IS in the update in Step 10 ensures that in the parameter vector wS,t+1
all coordinates that are not in S are set to 0, assuming that coordinates of wS,t not in S
were 0.

(5)

t=1

Next, at time t, consider the loss function ℓt(w) = (x⊤t ISw − yt)2. The gradient of this

loss function at wS,t is

∇ℓt(wS,t) = 2(x⊤t ISwS,t − yt)ISxt = 2IS(xtx⊤t wS,t − ytxt),

where we use the fact that ISwS,t = wS,t since wS,t has zeros in coordinates not in S. Now,
by (1), we have

Et[2IS(XtwS,t − zt)] = 2IS(xtx⊤t wS,t − ytxt),

3Recall that the costs in Hedge may be chosen adaptively.

6

and thus, Step 10 of the algorithm is a stochastic gradient descent update as claimed. Fur-
thermore, a calculation similar to the one for bounding the loss of the experts in the Hedge
algorithm shows that the norm of the stochastic gradient is bounded deterministically by
O( 1

q ), which is O(
Using a standard regret bound for stochastic gradient descent (see, for example, Lemma
3.1 in (Flaxman et al., 2005)) with the speciﬁed value of ηSGD, we conclude that for any
ﬁxed vector w of ℓ2 norm at most 1, we have,

d2

(k′−k)2 ).

T

Xt=1

E[(x⊤t ISwS,t − yt)2] ≤

(x⊤t ISw − yt)2 + O(

d2

(k′−k)2√T ).

T

Xt=1

Since ISwS,t = wS,t, the left hand side of the above inequality equalsPT
E[(wS,t· xt− yt)2].
Finally, let w be an arbitrary k-sparse vector of ℓ2 norm at most 1. Let S = {i | wi 6= 0}.
Note that |S| ≤ k, and IS(w) = w. Applying the above bound for this set S, we get

t=1

T

Xt=1

E[(wS,t · xt − yt)2] ≤

(w · xt − yt)2 + O(

d2

(k′−k)2√T ).

T

Xt=1

(6)

Combining the inequality (6) with inequality (5), we conclude that

T

Xt=1

E[(ˆyt − yt)2] ≤

T

Xt=1

(w · xt − yt)2 + O(

d2

(k′−k)2pk log(d)T ).

This gives us the required regret bound.

5 Computational lower bound

In this section we show that there cannot exist an eﬃcient no-regret algorithm for the online
sparse regression problem unless NP ⊆ BPP. This hardness result follows from the hardness
of approximating the Set Cover problem. We give a reduction showing that if there were an
eﬃcient no-regret algorithm AlgOSR for the online sparse regression problem, then we could
distinguish, in randomized polynomial time, between two instances of the Set Cover problem:
in one of which there is a small set cover, and in the other of which any set cover is large.
This task is known to be NP-hard for speciﬁc parameter values. Speciﬁcally, our reduction
has the following properties:

1. If there is a small set cover, then in the induced online sparse regression problem there
is a k-sparse parameter vector (of ℓ2 norm at most 1) giving 0 loss, and thus the
algorithm AlgOSR must have small total loss (equal to the regret) as well.

2. If there is no small set cover, then the prediction made by AlgOSR in any round has at
least a constant loss in expectation, which implies that its total (expected) loss must
be large, in fact, linear in T .

7

By measuring the total loss of the algorithm, we can distinguish between the the two instances
of the Set Cover problem mentioned above with probability at least 3/4, thus yielding a BPP
algorithm for an NP-hard problem.

The starting point for our reduction is the work of Dinur and Steurer (2014) who give a
polynomial-time reduction of deciding satisﬁability of 3CNF formulas to distinguishing in-
stances of Set Cover with certain useful combinatorial properties. We denote the satisﬁability
problem of 3CNF formulas by SAT.

Reduction 1. For any given constant D > 0, there is a constant cD ∈ (0, 1) and a poly(nD)-
time algorithm that takes a 3CNF formula φ of size n as input and constructs a Set Cover
instance over a ground set of size m = poly(nD) with d = poly(n) sets, with the following
properties:

1. if φ ∈ SAT, then there is a collection of k = O(dcD) sets, which covers each element

exactly once, and

2. if φ /∈ SAT, then no collection of k′ = ⌊D ln(d)k⌋ sets covers all elements; i.e., at least

one element is left uncovered.

The Set Cover instance generated from φ can be encoded as a binary matrix Mφ ∈ {0, 1}m×d
with the rows corresponding to the elements of the ground set, and the columns correspond
to the sets, such that each column is the indicator vector of the corresponding set.

Using this reduction, we now show how an eﬃcient, no-regret algorithm for online sparse

regression can be used to give a BPP algorithm for SAT.

Theorem 2. Let D > 0 be any given constant. Suppose there is an algorithm, AlgOSR, for
the (k, k′, d)-online sparse regression problem with k = O(dcD), where cD is the constant from
Reduction 1, and k′ = ⌊D ln(d)k⌋, that runs in poly(d, T ) time per iteration and has expected
regret bounded by poly(d)T 1−δ for some constant δ > 0. Then NP ⊆ BPP.
Proof. Since the expected regret of AlgOSR is bounded by p(d)T 1−δ (where p(d) is a polyno-
mial function of d), by Markov’s inequality we conclude that with probability at least 3/4,
the regret of AlgOSR is bounded by p(d)·T 1−δ. Figure 2 gives a randomized algorithm, AlgSAT,
for deciding satisﬁability of a given 3CNF formula φ using the algorithm AlgOSR. Note that
the feature vectors (i.e., the xt vectors) generated by AlgSAT are bounded in ℓ2 norm by 1,
as required. It is clear that AlgSAT is a polynomial-time algorithm since T is a polynomial
function of n (since m, k, d, p(d) are polynomial functions of n), and AlgOSR runs in poly(d, T )
time per iteration.

We now claim that this algorithm correctly decides satisﬁability of φ with probability at

least 3/4, and is hence a BPP algorithm for SAT.

To prove this, suppose φ ∈ SAT. Then, there are k sets in the Set Cover which cover
all elements with each element being covered exactly once. Consider the k-sparse parameter
vector w which has 1√k
in the positions corresponding to these k sets and 0 elsewhere. Note
that kwk ≤ 1, as required. Note that since this collection of k sets covers each element

8

Algorithm 2 Algorithm AlgSAT for deciding satisfiability of 3CNF formulas

Require: A constant D > 0, and an algorithm AlgOSR for the (k, k′, d)-online sparse re-
gression problem with k = O(dcD), where cD is the constant from Reduction 1, and
k′ = ⌊D ln(d)k⌋, that runs in poly(d, T ) time per iteration with regret bounded by
p(d) · T 1−δ with probability at least 3/4.

Require: A 3CNF formula φ.
1: Compute the matrix Mφ and the associated parameters k, k′, d, m from Reduction 1.
2: Run AlgOSR with the parameters k, k′, d for T := ⌈max{(2p(d)mdk)1/δ, 256m2d2k2}⌉ it-
3: for t = 1, 2, . . . , T do
4:
5:
6:
7: Obtain a set of coordinates St of size at most k′ by running AlgOSR, and provide it the

Sample a row of Mφ uniformly at random; call it ˆxt.
Sample σt ∈ {−1, 1} uniformly at random independently of ˆxt.
Set xt = σt√d

ˆxt and yt = σt√dk

erations.

.

coordinates xt(St).

8: Obtain the prediction ˆyt from AlgOSR, and provide it the true label yt.
9: end for

2mdk then

10: if PT

t=1(yt − ˆyt)2 ≤ T
11: Output “satisﬁable”.
12: else
13: Output “unsatisﬁable”.
14: end if

exactly once, we have Mφw = 1√k
row of Mφ, we have

1, where 1 is the all-1’s vector. In particular, since ˆxt is a

= yt.

σt√dk

w · xt = w ·(cid:18) σt√d
ˆxt(cid:19) =
Thus, (w· xt− yt)2 = 0 for all rounds t. Since algorithm AlgOSR has regret at most p(d)· T 1−δ
with probability at least 3/4, its total loss PT
t=1(ˆyt − yt)2 is bounded by p(d) · T 1−δ ≤ T
(since T ≥ (2p(d)mdk)1/δ) with probability at least 3/4. Thus, in this case algorithm AlgSAT
correctly outputs “satisﬁable” with probability at least 3/4.
Next, suppose φ /∈ SAT. Fix any round t and let St be the set of coordinates of size
at most k′ selected by algorithm AlgOSR to query. This set St corresponds to k′ sets in the
Set Cover instance. Since φ /∈ SAT, there is at least one element in the ground set that is
not covered by any set among these k′ sets. This implies that there is at least one row of
Mφ that is 0 in all the coordinates in St. Since ˆxt is a uniformly random row of Mφ chosen
independently of St, we have

2mdk

Pr[xt(St) = 0] = Pr[ˆxt(St) = 0] ≥

1
m

.

Here, 0 denotes the all-zeros vector of size k′.

9

Now, we claim that E[yt ˆyt | xt(St) = 0] = 0. This is because the condition that xt(St) = 0
is equivalent to the condition that ˆxt(St) = 0. Since yt is chosen from {− 1√dk
1√dk} uniformly
at random independently of ˆxt and ˆyt, the claim follows. The expected loss of the online
algorithm in round t can now be bounded as follows:

,

E[ (ˆyt − yt)2 (cid:12)(cid:12) xt(St) = 0] = E(cid:20) ˆy2

= E(cid:20) ˆy2

t +

1

dk (cid:12)(cid:12)(cid:12)(cid:12)

xt(St) = 0(cid:21)

1

t +

dk − 2ytˆyt (cid:12)(cid:12)(cid:12)(cid:12)
xt(St) = 0(cid:21) ≥

1
dk

,

and hence

E[(yt − ˆyt)2] ≥ E[(yt − ˆyt)2 | xt(St) = 0] · Pr[xt(St) = 0] ≥

1
dk ·

1
m

=

1

mdk

.

Let Et[·] denote expectation of a random variable conditioned on all randomness prior to
round t. Since the choices of xt and yt are independent of previous choices in each round, the
same argument also implies that Et[(yt − ˆyt)2] ≥ 1
mdk . Applying Azuma’s inequality (see
Theorem 7.2.1 in (Alon and Spencer, 1992)) to the martingale diﬀerence sequence Et[(yt −
ˆyt)2] − (yt − ˆyt)2 for t = 1, 2, . . . , T , since each term is bounded in absolute value by 4, we
get

Pr" T
Xt=1
Thus, with probability at least 3/4, the total lossPT
ˆyt)2] − 8√T ≥ 1
mdk T − 8√T ≥ T

Et[(yt − ˆyt)2] − (yt − ˆyt)2 ≥ 8√T# ≤ exp(cid:0)− 64T

Et[(yt−
2mdk (since T ≥ 256m2d2k2). Thus in this case the algorithm

t=1(ˆyt−yt)2 is greater thanPT

2·16T(cid:1) ≤

correctly outputs “unsatisﬁable” with probability at least 3/4.

1
4

.

t=1

Parameter settings for hard instances. Theorem 2 implies that for any given constant
D > 0, there is a constant cD such that the parameter settings k = O(dcD), and k′ =
⌊D ln(d)k⌋ yield hard instances for the online sparse regression problem. The reduction of
Dinur and Steurer (2014) can be “tweaked”4 so that the cD is arbitrarily close to 1 for any
constant D.

We can now extend the hardness results to the parameter settings k = O(dǫ) for any
ǫ ∈ (0, 1) and k′ = ⌊D ln(d)k⌋ either by tweaking the reduction of Dinur and Steurer (2014)
so it yields cD = ǫ if ǫ is close enough to 1, or if ǫ is small, by adding O(d1/ǫ) all-zeros columns
to the matrix Mφ. The two combinatorial properties of Mφ in Reduction 1 are clearly still
satisﬁed, and the proof of Theorem 2 goes through.

4This is accomplished by simply replacing the Label Cover instance they construct with polynomially

many disjoint copies of the same instance.

10

6 Conclusions

In this paper, we prove that minimizing regret in the online sparse regression problem is
computationally hard even if the learner is allowed access to many more features than the
comparator, a sparse linear regressor. We complement this result by giving an ineﬃcient
no-regret algorithm.

The main open question remaining from this work is what extra assumptions can one
make on the examples arriving online to make the problem tractable. Note that the sequence
of examples constructed in the lower bound proof is i.i.d., so clearly stronger assumptions
than that are necessary to obtain any eﬃcient algorithms.

References

Noga Alon and Joel Spencer. The Probabilistic Method. John Wiley, 1992. ISBN 0-471-

53588-5.

Sanjeev Arora, Elad Hazan, and Satyen Kale. The multiplicative weights update method: a

meta-algorithm and applications. Theory of Computing, 8(1):121–164, 2012.

Nicol`o Cesa-Bianchi, Shai Shalev-Shwartz, and Ohad Shamir. Eﬃcient learning with par-

tially observed attributes. Journal of Machine Learning Research, 12:2857–2878, 2011.

Irit Dinur and David Steurer. Analytical approach to parallel repetition. In STOC, pages

624–633, 2014.

Abraham Flaxman, Adam Tauman Kalai, and H. Brendan McMahan. Online convex op-
timization in the bandit setting: gradient descent without a gradient. In SODA, pages
385–394, 2005.

Dean Foster, Howard Karloﬀ, and Justin Thaler. Variable selection is hard. In COLT, pages

696–709, 2015.

Elad Hazan and Tomer Koren. Linear regression with limited observation. In ICML, 2012.

Elad Hazan, Amit Agarwal, and Satyen Kale. Logarithmic regret algorithms for online

convex optimization. Machine Learning, 69(2-3):169–192, 2007.

Satyen Kale. Open problem: Eﬃcient online sparse regression. In COLT, pages 1299–1301,

2014.

Doron Kukliansky and Ohad Shamir. Attribute eﬃcient linear regression with distribution-

dependent sampling. In ICML, pages 153–161, 2015.

B. K. Natarajan. Sparse approximate solutions to linear systems. SIAM J. Computing, 25

(2):227–234, 1995.

11

Navid Zolghadr, G´abor Bart´ok, Russell Greiner, Andr´as Gy¨orgy, and Csaba Szepesv´ari.

Online learning with costly features and labels. In NIPS, pages 1241–1249, 2013.

12

