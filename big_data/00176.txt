6
1
0
2

 
r
a

M
1

 

 
 
]

.

A
N
h
t
a
m

[
 
 

1
v
6
7
1
0
0

.

3
0
6
1
:
v
i
X
r
a

Analysis of the structure of the Krylov subspace in various

preconditioned CGS algorithms

Shoji Itoh∗ and Masaaki Sugihara†

Abstract

An improved preconditioned CGS (PCGS) algorithm has recently been proposed, and it
performs much better than the conventional PCGS algorithm.
In this paper, the improved
PCGS algorithm is veriﬁed as a coordinative to the left-preconditioned system; this is done
by comparing, analyzing, and executing numerical examinations of various PCGS algorithms,
including the most recently proposed one. We show that the direction of the preconditioned
system for the CGS method is determined by the operations of αk and βk in the PCGS algorithm.
By comparing the logical structures of these algorithms, we show that the direction can be
switched by the construction and setting of the initial shadow residual vector.

1 Introduction

The conjugate gradient squared (CGS) method [13] is one of various methods used to solve systems
of linear equations

(1.1)

Ax = b,

where the coeﬃcient matrix A of size n × n is usually nonsymmetric, x is the solution vector, and
b is the right-hand side (RHS) vector.

The CGS method is a bi-Lanczos method that belongs to the class of Krylov subspace methods.
Bi-Lanczos-type methods are derived from the biconjugate gradient (BiCG) method [4, 10], which
assumes the existence of a dual system ATx♯ = b♯ (we will refer to this as the “shadow system”). Bi-
Lanczos-type algorithms have the advantage of requiring less memory than Arnoldi-type algorithms,
which is another class of Krylov subspace methods. Furthermore, a variety of bi-Lanczos-type
algorithms, such as the biconjugate gradient stabilized (BiCGStab) [15] and the generalized product-
type method based on the BiCG (GPBiCG) [16], have been constructed by adopting the idea behind
the derivation of the CGS. Various iterative methods, including bi-Lanczos-type algorithms, are
often used following a preconditioning operation that is used to improve the properties of the linear
equations. Such algorithms are called preconditioned algorithms; for example, the preconditioned
CGS (PCGS). Therefore, it is very important to study the properties of the PCGS so that its
performance can be improved.

Generally, the degree k of the Krylov subspace generated by A and r0 is displayed as Kk (A, r0)
= span(cid:8)r0, Ar0, A2r0, · · · , Ak−1r0(cid:9), where r0 is the initial residual vector r0 = b − Ax0, where x0
is the initial guess at the solution. The Krylov subspace Kk (A, r0) generated by the k-th iteration
forms the structure of xk ∈ x0 + Kk (A, r0), where xk is the approximate solution vector (or simply
the “solution vector”). However, for a given preconditioned Krylov subspace method, there are
various diﬀerent algorithms that can be used for the preconditioning conversion. In such cases, the
structure of the approximate solution formed by the Krylov subspace is often diﬀerent for diﬀerent
algorithms, and the performance of these various algorithms can also diﬀer substantially [8].

An improved PCGS algorithm has been proposed [8]. Reference [8] illustrates that this improved
algorithm has many advantages over the conventional PCGS algorithms [1, 12, 15]. In this paper, a
variety of PCGS algorithms are discussed. We begin by considering two typical PCGS algorithms,
and we analyze the structure of the solution vector for each Krylov subspace. We then perform the

∗Division of Science, School of Science and Engineering, Tokyo Denki University. (itosho@acm.org).
†Department of Physics and Mathematics, College of Science and Engineering, Aoyama Gakuin University.

1

same analysis for two improved PCGS algorithms, one of which was mentioned above [8] and the
other is presented in the present paper.

In this paper, when we refer to a preconditioned algorithm, we mean one that uses a precondition-
ing operator M or a preconditioning matrix, and by preconditioned system, we mean one that has
been converted by some operator(s) based on M . These terms never indicate the algorithm for the
preconditioning operation itself, such as incomplete LU decomposition or by using the approximate
inverse. For example, under a preconditioned system, the original linear system (1.1) becomes

(1.2)
(1.3)

˜A˜x = ˜b,
˜A = M −1

L AM −1

R , ˜x = MRx, ˜b = M −1

L b,

with the preconditioner M = MLMR (M ≈ A). In this paper, the matrix and the vector under the
preconditioned system are denoted by the tilde (˜). However, the conversions in (1.2) and (1.3) are
not implemented directly; rather, we construct the preconditioned algorithm that is equivalent to
solving (1.2).

This paper is organized as follows. Section 2 provides various preconditioned CGS algorithms;
in particular, we consider right- and left-preconditioned systems for CGS algorithms. The improved
PCGS algorithms are shown to be coordinative to the left-preconditioned system. Section 3 discusses
the diﬀerence for PCGS algorithms between the direction of a preconditioning conversion and the
direction of a preconditioned system. We show that preconditioning conversions are congruent for
PCGS, and we provide some examples in which the direction of the preconditioned system for the
CGS is switched.
In section 4, we present some numerical results to illustrate the convergence
properties of the various PCGS algorithms discussed in section 2, and we illustrate the eﬀect of
switching the direction of the preconditioned system for the CGS algorithm in section 3. Finally,
our conclusions are presented in section 5.

2 Analyses of various PCGS algorithms

In this section, four kinds of PCGS algorithms are analyzed. These PCGS algorithms can be derived
as follows.

Algorithm 1. CGS method under preconditioned system:

˜x0 is the initial guess, ˜r0 = ˜b − ˜A˜x0, set βPCGS
(cid:16)˜r♯
For k = 0, 1, 2, · · · , until convergence, Do:

0, ˜r0(cid:17) 6= 0, e.g., ˜r♯

0 = ˜r0,

−1 = 0,

k−1 ˜qk−1,

k−1

˜uk = ˜rk + βPCGS
˜pk = ˜uk + βPCGS
0, ˜rk(cid:17)
(cid:16)˜r♯
0, ˜A˜pk(cid:17)
(cid:16)˜r♯

αPCGS

=

k

,

(cid:0)˜qk−1 + βPCGS

k−1 ˜pk−1(cid:1) ,

˜A˜pk,

k

k

˜qk = ˜uk − αPCGS
˜xk+1 = ˜xk + αPCGS
˜rk+1 = ˜rk − αPCGS
0, ˜rk+1(cid:17)
(cid:16)˜r♯
0, ˜rk(cid:17)
(cid:16)˜r♯

βPCGS
k

=

k

,

(˜uk + ˜qk) ,
˜A (˜uk + ˜qk) ,

End Do

Any preconditioned algorithm can be derived by substituting the matrix with the preconditioner
for the matrix with the tilde and the vectors with the preconditioner for the vectors with the tilde.
Obviously, Algorithm 1 without the preconditioning conversion is the same as the CGS method. If ˜A
is a symmetric positive deﬁnite matrix and ˜r♯
0 = ˜r0, then Algorithm 1 is mathematically equivalent
to the conjugate gradient (CG) method [6] under a preconditioned system.

2

The case of (1.3) is called two-sided preconditioning, the case in which ML = M and MR = I is
called left preconditioning, and the case in which ML = I and MR = M is called right preconditioning,
where I denotes the identity matrix. We now formally deﬁne these1.

Deﬁnition 1 For the system and solution
(1.2')

˜A˜x = ˜b,
˜A = M −1

(1.3')

L AM −1

R , ˜x = MRx, ˜b = M −1

L b,

we deﬁne the direction of a preconditioned system of linear equations as follows:

• The two-sided preconditioned system: Equation (1.3');

• The right-preconditioned system: ML = I and MR = M in (1.3');

• The left-preconditioned system: ML = M and MR = I in (1.3'),

where M is the preconditioner M = MLMR (M ≈ A), and I is the identity matrix.

Other vectors in the solving method are not preconditioned. The initial guess is given as x0, and

˜x0 = MRx0.

The two-sided preconditioned system may be impracticable, but it is of theoretical interest.
The preconditioned system is diﬀerent from the preconditioning conversion. There are various
ways of performing a preconditioning conversion, but the direction of the preconditioned system is
uniquely deﬁned. (For example, see the preconditioning conversions (2.2) and (2.5) in Algorithm 2,
Section 2.1.1).

Both the CGS and the PCGS extend the two-dimensional subspace in each iteration [2, 5],

therefore, the Krylov subspace K2k( ˜A, ˜r0) generated by the k-th iteration forms the structure of

(2.1)

˜xk ∈ ˜x0 + K2k( ˜A, ˜r0).

2.1 Two typical PCGS algorithms

In this subsection, we present two well-known and typical PCGS algorithms. One is a right-
preconditioned system, although this is not always recognized, and the other is a left-preconditioned
system. For both of these algorithms, we examine the structure of their Krylov subspace and the
solution vector.

2.1.1 Conventional right-preconditioned PCGS

This PCGS algorithm has been described in many manuscripts and numerical libraries, for example,
see [1, 12, 15]. It is usually derived by the following preconditioning conversion2:

(2.2)

R , ˜xk = MRxk, ˜b = M −1

L b,

˜A = M −1
˜rk = M −1

L AM −1
L rk, ˜r♯

0 = M T

L r♭

0, ˜pk = M −1

L pk, ˜uk = M −1

L uk, ˜qk = M −1

L qk.

Finally, Algorithm 2 is derived.

Algorithm 2. Conventional PCGS algorithm:

x0 is the initial guess, r0 = b − Ax0,
(cid:16)˜r♯
0 = r0,
For k = 0, 1, 2, · · · , until convergence, Do:

0, r0(cid:1) 6= 0, e.g., r♭

0, ˜r0(cid:17) = (cid:0)r♭

set β−1 = 0,

uk = rk + βk−1qk−1,
pk = uk + βk−1 (cid:0)qk−1 + βk−1pk−1(cid:1) ,

1 Here, we have oﬀered a general deﬁnition. However, for preconditioned bi-Lanczos-type algorithms, additional

restrictions are necessary[9].

2In this case, the initial shadow residual vector (ISRV) ˜r

displaying M T
will be discussed in section 3. The same applies to (2.5).

♯
0 in the notation of the algorithm. However, its internal structure is r

L r

♯
0 is converted to M T
L r

♭
0, but there is no problem with
♯
♭
0. The notation r
0

♭
0 ≡ M −T

r

3

(2.3)

End Do

The stopping criterion is

(2.4)

αk =

0, rk(cid:1)

(cid:0)r♭
0, AM −1pk(cid:1)

(cid:0)r♭

,

qk = uk − αkAM −1pk,
xk+1 = xk + αkM −1 (uk + qk) ,
rk+1 = rk − αkAM −1 (uk + qk) ,
βk = (cid:0)r♭
0, rk+1(cid:1)
(cid:0)r♭
0, rk(cid:1)

,

krk+1k

kbk

≤ ε.

The results of this algorithm can also be derived by the following conversion:

(2.5)

˜A = AM −1, ˜xk = M xk, ˜b = b,
˜rk = rk, ˜r♯

0 = r♭

0, ˜pk = pk, ˜uk = uk, ˜qk = qk.

This is the same as using ML = I and MR = M in (2.2). Furthermore, this is the same as

converting only ˜A, ˜xk, and ˜b, that is, the right-preconditioned system.

2.1.2 Left-preconditioned CGS

The following conversion can be used to derive another PCGS algorithm:

(2.6)

˜A = M −1A, ˜xk = xk, ˜b = M −1b,
˜rk = r+

0, ˜pk = p+

0 = r♯

k , ˜r♯

k , ˜uk = u+

k , ˜qk = q+
k .

This is the same as applying ML = M and MR = I to ˜A, ˜xk, and ˜b, that is, the left-

preconditioned system.

Algorithm 3. Left-preconditioned CGS algorithm (Left-PCGS):

x0 is the initial guess, r+
(cid:16)˜r♯
For k = 0, 1, 2, · · · , until convergence, Do:

0 (cid:17) 6= 0, e.g., r♯

0, ˜r0(cid:17) = (cid:16)r♯

0 = r+
0 ,

0 = M −1 (b − Ax0) ,

0, r+

set β−1 = 0,

k−1(cid:1) ,

,

αk =

u+
k = r+
p+
k = u+

k + βk−1q+
k−1,
k + βk−1 (cid:0)q+
k−1 + βk−1p+
(cid:16)r♯
k (cid:17)
0, r+
(cid:16)r♯
k (cid:17)
0, M −1Ap+
k − αkM −1Ap+
k = u+
q+
k ,
k + q+
xk+1 = xk + αk (cid:0)u+
k (cid:1) ,
r+
k − αkM −1A(cid:0)u+
k+1 = r+
k + q+
(cid:16)r♯
0, r+
(cid:16)r♯

k+1(cid:17)
k (cid:17)
0, r+

βk =

,

k (cid:1) ,

End Do

In this paper, r+

structure is r+

k denotes the residual vector under the left-preconditioned system3, its internal
k achieve

k ≡ M −1rk, and this is the deﬁnition of r+

k . Note that p+

k , and q+

k , u+

3The notation r

+

k will be discussed in sections 2.1.3, 2.3, and 3, but there is no problem with displaying rk in

the notation of the algorithm. Note that this is also true for p

+

+

k , u

k , and q

+

k .

4

the same purpose. Here, r+
rk = b − Axk, and here, the stopping criterion is

k in Algorithm 3 provides diﬀerent information to the residual vector

(2.7)

kr+
k+1k
kM −1bk

≤ ε.

Note that this also diﬀerent from (2.4), and this is an example of incomplete judging, because r+
never provides important information about b − Axk [7].

k+1

This algorithm can also be derived by the following conversion:

(2.8)

˜A = M −1
˜rk = MRr+

L AM −1
k , ˜r♯

R , ˜xk = MRxk, ˜b = M −1
0, ˜pk = MRp+

0 = M −T

R r♯

L b,
k , ˜uk = MRu+

k , ˜qk = MRq+
k .

If ML = M and MR = I are substituted into (2.8), then (2.6) is obtained.

2.1.3 Comparison between two typical PCGS algorithms

Here, we compare the conventional PCGS (Algorithm 2) with the left-PCGS (Algorithm 3); we will
focus on the structures of their Krylov subspaces and the solution vectors.

The conventional PCGS (Algorithm 2) is the right-preconditioned system4, that is, (cid:0)AM −1(cid:1) (M x) =

b, and rk = b −(cid:0)AM −1(cid:1) (M xk). The relation between the Krylov subspace and the solution vector
is

(2.9)

M xk ∈ M x0 + KR

2k (cid:0)AM −1, r0(cid:1) .

This means that the Krylov subspace KR
xk directly, but xk is calculated with corrections, as in (2.3) in Algorithm 2.

2k (cid:0)AM −1, r0(cid:1) generates the solution vector as M xk, not
k = M −1 (b − Axk). The relation between

The left-PCGS (Algorithm 3) is M −1Ax = M −1b, r+

its Krylov subspace and the solution vector is

(2.10)

xk ∈ x0 + KL

2k (cid:0)M −1A, r+

0 (cid:1) .

Therefore, the Krylov subspace KL
rithm 3).

2k (cid:0)M −1A, r+

0 (cid:1) generates the solution vector directly as xk (Algo-

These are summarized in Table 1.
It is important to note that the structures are diﬀerent for the two Krylov subspaces, KR

2k (cid:0)M −1A, r+
for the conventional PCGS (the right system) and KL
0 (cid:1) of the left-PCGS, because their
scalar parameters αk and βk are not equivalent [8, 9]. We summarize this here; for details, see [9].
The recurrences of the BiCG under the preconditioned system are

2k (cid:0)AM −1, r0(cid:1)

(2.11)

(2.12)

R0(˜λ) = 1, P0(˜λ) = 1,
Rk(˜λ) = Rk−1(˜λ) − αPBiCG
k−1
Pk(˜λ) = Rk(˜λ) + βPBiCG

k−1 Pk−1(˜λ).

˜λPk−1(˜λ),

Here, Rk(˜λ) is the degree k of the residual polynomial, and Pk(˜λ) is the degree k of the probing
direction polynomial, that is, ˜rk = Rk( ˜A)˜r0 and ˜pk = Pk( ˜A)˜r0. For example, in the left-PCGS,
(2.11) is shown as RL

k−1(˜λ), so ˜rk ∈ KL

k−1(˜λ) − αL

k (˜λ) = RL

˜λP L

k−1

k+1 (cid:16) ˜A, ˜r0(cid:17).

2.2 Improved preconditioned CGS algorithms

An improved PCGS algorithm has been proposed [8]. This algorithm retains some mathemati-
cal properties that are associated with the CGS derivation from the BiCG method under a non-
preconditioned system. The improved PCGS algorithm from [8] will be referred to as “Improved1.”
Another improved PCGS algorithm will be presented, and it will be referred to as “Improved2.” We
note that Improved2 is mathematically equivalent to Improved1. The stopping criterion for both
algorithms is (2.4).

4 The Krylov subspace of the right-preconditioned system is denoted with a superscript R, and that of the left-

PCGS is denoted by with a superscript L.

5

Table 1: Summary of two typical PCGS algorithms.

Structure of
residual vector

Structure of solution vector
for each Krylov subspace

Conventional (Alg.2) rk = b − (AM −1)(M xk) M xk ∈ M x0 + KR
Left-PCGS (Alg.3)

r+
k = M −1 (b − Axk)

xk ∈ x0 + KL

2k (cid:0)AM −1, r0(cid:1)

2k (cid:0)M −1A, r+
0 (cid:1)

2.2.1 The Improved1 PCGS algorithm (Improved1) [8]

Improved1 can be derived from the following conversion:

(2.13)

˜A = M −1
˜rk = M −1

L AM −1
L rk, ˜r♯

R , ˜xk = MRxk, ˜b = M −1
0, ˜pk = MRp+

0 = M −T

R r♯

L b,
k , ˜uk = MRu+

k , ˜qk = MRq+
k .

Algorithm 4. Improved PCGS algorithm (Improved1):

x0 is the initial guess, r0 = b − Ax0,
(cid:16)˜r♯
e.g., r♯
For k = 0, 1, 2, · · · , until convergence, Do:

0, M −1r0(cid:17) 6= 0,

0, ˜r0(cid:17) = (cid:16)r♯

set β−1 = 0,

0 = M −1r0,

k−1 + βk−1p+

k−1(cid:1) ,

,

k−1,

αk =

u+
k = M −1rk + βk−1q+
k + βk−1 (cid:0)q+
k = u+
p+
(cid:16)r♯
0, M −1rk(cid:17)
(cid:16)r♯
k (cid:17)
0, M −1Ap+
q+
k − αkM −1Ap+
k = u+
k ,
k + q+
xk+1 = xk + αk (cid:0)u+
k (cid:1) ,
rk+1 = rk − αkA(cid:0)u+
k + q+
k (cid:1) ,
0, M −1rk+1(cid:17)
(cid:16)r♯
0, M −1rk(cid:17)
(cid:16)r♯

βk =

,

End Do

2.2.2 Improved2 PCGS algorithm (Improved2)

Improved2 can be derived from the following conversion:

(2.14)

R , ˜xk = MRxk, ˜b = M −1

L b,

˜A = M −1
˜rk = M −1

L AM −1
L rk, ˜r♯

0 = M −T

R r♯

0, ˜pk = M −1

L pk, ˜uk = M −1

L uk, ˜qk = M −1

L qk.

Note that this conversion is diﬀerent than (2.13) for ˜pk, ˜uk, and ˜qk.

Algorithm 5. Another improved PCGS algorithm (Improved2):

x0 is the initial guess, r0 = b − Ax0,
(cid:16)˜r♯
e.g., r♯
For k = 0, 1, 2, · · · , until convergence, Do:

0, M −1r0(cid:17) 6= 0,

0, ˜r0(cid:17) = (cid:16)r♯

set β−1 = 0,

0 = M −1r0,

uk = rk + βk−1qk−1,
pk = uk + βk−1 (cid:0)qk−1 + βk−1pk−1(cid:1) ,

(cid:16)M −Tr♯

0, rk(cid:17)

αk =

,

(cid:16)M −Tr♯

0, AM −1pk(cid:17)

6

qk = uk − αkAM −1pk,
xk+1 = xk + αkM −1 (uk + qk) ,
rk+1 = rk − αkAM −1 (uk + qk) ,

βk =

(cid:16)M −Tr♯
(cid:16)M −Tr♯

0, rk+1(cid:17)
0, rk(cid:17)

,

End Do

2.3 Analysis of the four kinds of PCGS algorithms

We will now analyze the four PCGS algorithms presented above.

We split the residual vector of the left-PCGS (Algorithm 3) r+

k into

(2.15)

r+
k 7→ M −1rk, (k = 0, 1, 2, · · ·)

and give the necessary deformations; then the left-PCGS (Algorithm 3) is reduced to Improved1
(Algorithm 4). Alternatively, we can derive Algorithm 3 from Algorithm 4 by substituting M −1rk
for r+
k ≡ M −1rk. By this means, we can explain the relationships between the four
kinds of PCGS algorithms, as shown in Figure 1.

k , that is, r+

Figure 1: Relations between the four diﬀerent PCGS algorithms. 7→ : Splitting left vector to right
members (preconditioner and vector), ≡ : Substituting left vector for right members.

In addition, if we apply (2.15) to (2.10) for the structure of Krylov subspace of Algorithm 3, then

2k (cid:0)M −1A, r+
KL

0 (cid:1) 7→ KL

2k (cid:0)M −1A, M −1r0(cid:1) = M −1KL

2k (cid:0)AM −1, r0(cid:1) .

The structure of the solution vector for the Krylov subspace is then

(2.16)

xk ∈ x0 + KL

2k (cid:0)M −1A, r+

0 (cid:1) 7→ xk ∈ x0 + M −1KL

2k (cid:0)AM −1, r0(cid:1) .

Therefore, the system of Improved1 (Algorithm 4) is coordinative to that of the left-PCGS (Algo-
rithm 3), and Improved2 (Algorithm 5) is equivalent to Improved1 (Algorithm 4). Both algorithms
have important advantages over the left-PCGS, because their residual vector is rk, and their stopping
criterion is (2.4), not kr+

k+1k/kM −1bk.

Table 2 shows the structure of the residual vector and the structure of the solution vector for the

Krylov subspace for each of the four PCGS algorithms.

In this summary, we see that the structures of the Krylov subspaces diﬀer: KR

the conventional PCGS (the right-preconditioned system) is diﬀerent from KL

2k (cid:0)AM −1, r0(cid:1) of
2k (cid:0)AM −1, r0(cid:1) of both

7

Table 2: Summary of the four PCGS algorithms.

Structure of
residual vector

Structure of solution vector
for each Krylov subspace

Conventional (Alg. 2)

rk = b − (AM −1)(M xk) M xk ∈ M x0 + KR

k = M −1 (b − Axk)
r+

Left-PCGS (Alg. 3)
Improved1 (Alg. 4) M −1rk =
Improved2 (Alg. 5) M −1 (cid:0)b − (AM −1)(M xk)(cid:1)

xk ∈ x0 + KL

xk ∈ x0 + M −1KL

2k (cid:0)AM −1, r0(cid:1)
0 (cid:1)
2k (cid:0)M −1A, r+
2k (cid:0)AM −1, r0(cid:1)

improved PCGS (coordinative to the left-preconditioned systems), because the scalar parameters αk
and βk are not equivalent [8, 9].

Furthermore, there is superﬁcially the same recurrence relation for the solution vector for both the
conventional PCGS (Algorithm 2) and Improved2 (Algorithm 5): xk+1 = xk + αkM −1 (uk + qk).
However, each recurrence relation belongs to a diﬀerent system, because the components of the
conventional PCGS are αR

k , and those of Improved2 are αL

k , and qL
k .

k , uR

k , and qR

k , uL

The structure of the residual vector of Improved1 (Algorithm 4) and Improved2 (Algorithm 5)
is illustrated as M −1rk = M −1 (cid:0)b − (AM −1)(M xk)(cid:1) in Table 2, because they are both from the
left-PCGS and the structure of their Krylov subspace is M −1KL

2k (cid:0)AM −1, r0(cid:1).

3 Congruence of preconditioning conversion, and direction of

preconditioned system for the CGS

In a previous section, we deﬁned the general direction of a preconditioned CGS system (see Def-
inition 1). However, the direction of a preconditioned system is diﬀerent from the direction of a
preconditioning conversion. We will show that the direction of a preconditioned system is switched
by the construction of the ISRV.

3.1 Congruence of preconditioning conversion for PCGS

Here, we consider the congruence of a preconditioning conversion for PCGS in the following propo-
sition.

Proposition 1 (Congruency) There is congruence to a PCGS algorithm in the direction of the
preconditioning conversion.

Proof We have already shown instances of this. For example, Algorithm 2 can be derived by
the two-sided conversion (2.2), and if ML = I, MR = M , and the conversion (2.2) is reduced to
(2.5), then Algorithm 2 is derived. If ML = M and MR = I, then Algorithm 2 can be derived. The
other preconditioned algorithms (Algorithms 3, 4, and 5) and their corresponding preconditioning
conversions are also the same.
✷
Although this property has been repeatedly discussed in the literature, it should be considered

when evaluating the direction of a preconditioned system.

3.2 Direction of a preconditioned system and that of the PCGS

The direction of a preconditioned system is diﬀerent from the direction of a preconditioning conver-
sion.

Proposition 2 The direction of a preconditioned system is determined by the operations of αk and
βk in each PCGS algorithm. These intrinsic operations are based on biorthogonality and biconjugacy.

Proof. The operations of biorthogonality and biconjugacy in each PCGS algorithm and the
structure of the solution vector for each Krylov subspace are shown below. The underlined inner
products are the actual operators for each PCGS.

8

Only the conventional PCGS (Algorithm 2) algorithm has the ISRV in the form r♭

algorithms, it is r♯
coeﬃcient matrix for the biconjugacy is ﬁxed as AM −1, that is, the right-preconditioned system.

0 never splits as M −Tr♯

0. The ISRV r♭

0; in all other
0 in this algorithm, and the preconditioned

• Conventional (Algorithm 2) :

r♭
0 = r0,

0, ˜rk(cid:17) = (cid:16)M T
(cid:16)˜r♯
(cid:16)˜r♯
0, ˜A˜pk(cid:17) = (cid:16)M T

L r♭

0, M −1

L r♭

0, (M −1

L rk(cid:17) = (cid:16)r♭
L AM −1

0, rk(cid:17),
L pk)(cid:17) = (cid:16)r♭

R )(M −1

0, (AM −1)pk(cid:17),

M xk ∈ M x0 + KR

2k (cid:0)AM −1, r0(cid:1) .

• Left-PCGS (Algorithm 3) :

r♯
0 = r+
0 ,
0, ˜rk(cid:17) = (cid:16)r♯
(cid:16)˜r♯
0, r+
0, ˜A˜pk(cid:17) = (cid:16)r♯
(cid:16)˜r♯

k (cid:17),

0, (M −1A)p+

k (cid:17),

xk ∈ x0 + KL

2k (cid:0)M −1A, r+

0 (cid:1) .

• Improved1 (Algorithm 4) :

r♯
0 = M −1r0,
(cid:16)˜r♯
0, ˜rk(cid:17) = (cid:16)M −T
R r♯
(cid:16)˜r♯
0, ˜A˜pk(cid:17) = (cid:16)M −T

R r♯

xk ∈ x0 + M −1KL

0, M −1rk(cid:17),

0, M −1

0, (M −1

L rk(cid:17) = (cid:16)r♯
L AM −1
2k (cid:0)AM −1, r0(cid:1) .

R )(MRp+

k )(cid:17) = (cid:16)r♯

0, (M −1A)p+

k (cid:17),

• Improved2 (Algorithm 5) :

r♯
0 = M −1r0,
0, ˜rk(cid:17) = (cid:16)M −T
(cid:16)˜r♯
R r♯
0, ˜A˜pk(cid:17) = (cid:16)M −T
(cid:16)˜r♯

R r♯
= (cid:16)M −Tr♯

xk ∈ x0 + M −1KL

0, M −1

L rk(cid:17) = (cid:16)M −Tr♯
R )(M −1
L AM −1
0, (M −1
0, A(M −1pk)(cid:17) = (cid:16)r♯
2k (cid:0)AM −1, r0(cid:1) .

✷

0, rk(cid:17) = (cid:16)r♯

0, M −1rk(cid:17) ,

L pk)(cid:17)
0, (M −1A)(M −1pk)(cid:17) ,

We present the following proposition and corollary.

Proposition 3 On the structure of biorthogonality (˜r♯
0, ˜rk) in the iterated part of each PCGS algo-
rithm, there exists a single preconditioning operator between rk (basic form of the residual vector)
and r♯

0 (basic form of the ISRV) such that M −1 operates on rk or M −T operates on r♯
0.

Proof. We split r♭

0 7→ M −Tr♯

0 and r+

k 7→ M −1rk in Algorithms 2 to 5, and obtain

(cid:16)˜r♯
(cid:16)˜r♯
(cid:16)˜r♯
(cid:16)˜r♯

0, ˜rk(cid:17) = (cid:16)r♭
0, ˜rk(cid:17) = (cid:16)r♯
0, ˜rk(cid:17) = (cid:16)r♯
0, ˜rk(cid:17) = (cid:16)M −Tr♯

0, rk(cid:17) 7→ (cid:16)M −Tr♯
0, r+

0, rk(cid:17) ,
0, M −1rk(cid:17) ,

k (cid:17) 7→ (cid:16)r♯
0, M −1rk(cid:17),

0, rk(cid:17) = (cid:16)r♯

0, M −1rk(cid:17) .

9

The underlined inner products are the actual operators for each PCGS.
In addition, for the two-sided conversion, we obtain

(cid:16)˜r♯

0, ˜rk(cid:17) = (cid:16)M −T

R r♯

0, M −1

L rk(cid:17) = (cid:16)M −Tr♯

0, rk(cid:17) = (cid:16)r♯

0, M −1rk(cid:17) .

✷

Corollary 1 On the structure of biconjugacy (˜r♯
there exists a single preconditioning operator between A (coeﬃcient matrix) and r♯
the ISRV), such that M −1 operates on A or M −T operates on r♯
preconditioning operator between A and pk (basic form of probing direction vector).

0, ˜A˜pk) in the iterated part of each PCGS algorithm,
0 (basic form of
0. Furthermore, there exists a single

From Propositions 2 and 3 and Corollary 1, the intrinsic operations on the biorthogonality and
the biconjugacy for the four PCGS algorithms have the same matrix and vector structures, even
though the superﬁcial descriptions of these algorithms are diﬀerent.

3.3 ISRV switches the direction of the preconditioned system for the

CGS

Although the mathematical properties of the conventional PCGS (Algorithm 2) and Improved2
(Algorithm 5) are quite diﬀerent, the structures of these algorithms are very similar. This can be
seen by replacing M −Tr♯

0 in Algorithm 5, and in the initial part, we have

(3.1)

0 with r♭
0, ˜r0(cid:17) = (cid:16)M −T

R r♯

(cid:16)˜r♯

0, M −1

L r0(cid:17) = (cid:16)M −Tr♯

0, r0(cid:17)

≡ (cid:16)r♭

0, r0(cid:17) 6= 0, e.g., r♭

0 = r0;

then Algorithm 5 becomes Algorithm 2.

Theorem 1 The direction of a preconditioned system for the CGS method is switched by construc-
tion and setting of the ISRV.

Proof. Proposition 2 shows that the direction of a preconditioned system for the CGS algorithm
is determined by the structures of the biorthogonality and the biconjugacy. Here, we show that
their structures are switched by the ISRV. The underlined inner products are the actual operators
for each PCGS.
• ISRV1 : r♯

0 = M −1r0 (Based on left conversion)

(3.2)

(cid:16)˜r♯

0, ˜r0(cid:17) = (cid:16)M −T
R r♯
e.g., r♯

0 = M −1r0.

0, M −1

L r0(cid:17) = (cid:16)r♯

0, M −1r0(cid:17) 6= 0,

• ISRV2 : r♯

0 = M Tr0 (Based on right conversion)

(3.3)

(cid:16)˜r♯

0, ˜r0(cid:17) = (cid:16)M −T

R r♯

0, M −1

L r0(cid:17) = (cid:16)M −Tr♯
0 = r0 → r♯

0 = M Tr0.

0, r0(cid:17) 6= 0,

e.g., M −Tr♯

If we apply ISRV2 to Algorithm 5, then Algorithm 5 is equivalent to Algorithm 2 with r♭

0 = r0:

0, ˜rk(cid:17) = (cid:16)M −Tr♯
(cid:16)˜r♯
(cid:16)˜r♯
0, ˜A˜pk(cid:17) = (cid:16)M −Tr♯

0, rk(cid:17) = (r0, rk) ,
0, A(M −1pk)(cid:17) = (cid:0)r0, (AM −1)pk(cid:1) .

Alternatively, if we apply r♭

0 = M −TM −1r0 (we will call this ISRV9) to Algorithm 2, then

Algorithm 2 is equivalent to Algorithm 5 with ISRV1:

0, ˜rk(cid:17) = (cid:16)r♭
(cid:16)˜r♯
(cid:16)˜r♯
0, ˜A˜pk(cid:17) = (cid:16)r♭

0, rk(cid:17) = (cid:0)M −T(M −1r0), rk(cid:1) ,
0, (AM −1)pk(cid:17) = (cid:0)M −T(M −1r0), (AM −1)pk(cid:1) .

✷

If we change Improved2 (Algorithm 5) to Improved1 (Algorithm 4), then we will obtain the same

results.

In the next section, Theorem 1 is veriﬁed numerically.

10

4 Numerical experiments

Convergence of the four PCGS algorithms of section 2 is conﬁrmed in section 4.1 by evaluating
three cases. Furthermore, in section 4.2, the ability of the ISRV to switch the direction of the
preconditioned system (as discussed in section 3.3) and Theorem 1 are veriﬁed.

4.1 Comparison of the four PCGS algorithms

The test problems were generated by building real nonsymmetric matrices corresponding to linear
systems taken from the University of Florida Sparse Matrix Collection [3] and the Matrix Market
[11]. The RHS vector b of (1.1) was generated by setting all elements of the exact solution vector
xexact to 1.0 and substituting this into (1.1). The solution algorithm was implemented using the
sequential mode of the Lis numerical computation library (version 1.1.2 [14]) in double precision,
with the compiler options registered in the Lis “Makeﬁle.” Furthermore, we set the initial solution
to x0 = 0. The maximum number of iterations was set to 1000.

The numerical experiments were executed on a DELL Precision T7400 (Intel Xeon E5420, 2.5
GHz CPU, 16 GB RAM) running the Cent OS (kernel 2.6.18) and the Intel icc 10.1, ifort 10.1
compiler.

In all tests, ILU(0) was adopted as a preconditioning operation with each PCGS algorithm;
0 = r0 in the conventional
0 = M −1r0 in Improved1 and

here, the value “zero” means the ﬁll-in level. The ISRVs were set as r♭
PCGS (Algorithm 2), r♯
Improved2 (Algorithms 4 and 5, respectively).

0 = r+

0 in the left-PCGS (Algorithm 3), and r♯

We considered the following three cases:

(a) Evaluating the algorithm relative residual (see Figure 2, 5, and Table 3);

(b) Evaluating the true relative residual (see Figure 3, 6, and Table 4);

when we have prior knowledge of the exact solution (xexact);

(c) Evaluating the true relative error (see Figure 4, 7, and Table 5).

We adopted the following stopping criteria: For case (a), we adopted the 2-norm of (2.4) for
Algorithms 2, 4, and 5, and we adopted the 2-norm of (2.7) for Algorithm 3. For case (b), we adopted
kb − Axk+1k2/||b||2 ≤ ε for all algorithms. For case (c), we adopted kxk+1 − xexactk2/||xexact||2 ≤ ε
for all algorithms. We set ε = 10−12 for all cases.

11

Table 3: (a) Numerical evaluation using the relative residual of each algorithm. N is the problem
size, and NNZ is the number of nonzero elements. The three numbers in each row for the column
for each method are as follows: the leftmost number is the true relative residual log10 2-norm, the
number in parentheses is the number of iterations required to reach convergence, and the lower
number is the true relative error log10 2-norm.

Matrix

N

NNZ

Conventional
(Algorithm 2)

Left-PCGS

Improved1

Improved2

(Algorithm 3)

(Algorithm 4)

(Algorithm 5)

-12.17 (35)

-13.06 (37)

-12.04 (35)

-12.04 (35)

add32

4960

19848

-12.17

-12.96

-11.96

-11.96

bfwa782

jpwh 991

782

991

7514

6027

-9.36 (93)

-10.29

-12.37 (83)

-12.82 (78)

-12.17 (84)

-12.09

-12.48

-12.22

Breakdown

-11.83 (15)

-12.44 (16)

-12.44 (16)

-12.10

-12.53

-12.53

-0.18 (Stag.)

-12.79 (38)

-12.20 (34)

-12.21 (33)

olm5000

5000

19996

4.22

-10.64

-8.05

-8.00

-10.14 (122)

-12.93 (119)

-12.49 (123)

-11.79 (117)

poisson3Db

85623

2374949

-10.33

-13.31

-13.39

-12.07

-12.69 (34)

-11.68 (32)

-12.69 (33)

-12.69 (33)

sherman4

1104

3786

-13.83

-12.82

-13.82

-13.83

-12.22 (52)

-10.96 (55)

-12.69 (56)

-12.70 (56)

wang4

26068

177196

-10.14

-9.66

-9.71

-9.71

watt 1

1856

11360

-5.96

-12.63

-9.77

-9.77

-13.01 (27)

-15.48 (41)

-12.11 (35)

-12.11 (35)

Table 4: (b) Numerical evaluation using the true relative residual of each algorithm.

Matrix

N

NNZ

Conventional
(Algorithm 2)

Left-PCGS

Improved1

Improved2

(Algorithm 3)

(Algorithm 4)

(Algorithm 5)

-12.17 (35)

-12.04 (35)

-12.04 (35)

-12.04 (35)

add32

4960

19848

-12.17

-11.96

-11.96

-11.96

bfwa782

782

7514

-10.29

-12.09

-12.48

-12.22

-9.36 (Stag.)

-12.37 (83)

-12.82 (78)

-12.17 (84)

jpwh 991

991

6027

Breakdown

-12.44 (16)

-12.44 (16)

-12.44 (16)

-12.53

-12.53

-12.53

-0.18 (Stag.)

-12.49 (31)

-12.20 (34)

-12.21 (33)

olm5000

5000

19996

4.22

-8.28

-8.05

-8.00

-10.14 (Stag.)

-12.08 (113)

-12.49 (123)

-11.77 (Stag.)

poisson3Db

85623

2374949

-10.33

-12.95

-13.39

-12.06

sherman4

1104

3786

-13.83

-13.81

-13.82

-13.83

-12.69 (34)

-12.68 (33)

-12.69 (33)

-12.69 (33)

wang4

26068

177196

-10.14

-9.71

-9.71

-9.71

-12.22 (52)

-12.69 (56)

-12.69 (56)

-12.70 (56)

watt 1

1856

11360

-5.96

-9.77

-9.77

-9.77

-13.01 (27)

-12.11 (35)

-12.11 (35)

-12.11 (35)

12

Table 5: (c) Numerical evaluation using the true relative error of each algorithm.

Matrix

N

NNZ

Conventional
(Algorithm 2)

Left-PCGS

Improved1

Improved2

(Algorithm 3)

(Algorithm 4)

(Algorithm 5)

-12.17 (35)

-12.00 (36)

-12.00 (36)

-12.00 (36)

add32

4960

19848

-12.17

-12.29

-12.29

-12.29

bfwa782

jpwh 991

782

991

-9.36 (Stag.)

-12.37 (83)

-12.00 (77)

-12.17 (84)

7514

-10.29

-12.09

-12.42

-12.22

Breakdown

6027

-11.83 (15)

-11.83 (15)

-11.83 (15)

-12.10

-12.10

-12.10

-0.18 (Stag.)

-12.79 (Stag.)

-12.80 (49)

-12.59 (52)

olm5000

5000

19996

4.22

-11.23

-13.22

-13.09

-10.14 (Stag.)

-11.24 (111)

-11.61 (117)

-11.53 (116)

poisson3Db

85623

2374949

-10.33

-12.21

-12.57

-12.04

-12.69 (34)

-11.68 (32)

-11.68 (32)

-11.68 (32)

sherman4

1104

3786

-13.83

-12.82

-12.82

-12.82

wang4

26068

177196

-10.66

-12.20

-12.57

-12.17

-12.48 (Stag.)

-13.37 (68)

-12.80 (66)

-12.83 (66)

watt 1

1856

11360

-12.13

-12.02

-12.01

-12.05

-18.06 (41)

-14.28 (40)

-14.26 (40)

-14.26 (40)

We will ﬁrst focus on the results of the conventional PCGS (Algorithm 2), as shown in Tables
3 to 5. Breakdown occurs for jpwh 991, and stagnation occurs for olm5000 at pitifully insuﬃcient
accuracy5, although the other three algorithms (Algorithms 3 to 5) were able to solve them.

Next, it is very important to compare cases (a) and (b) (Tables 3 and 4) with case (c) (Table 5),
in order to determine the crucial ways in which they diﬀer. Because (a) and (b) can be evaluated
without knowing the exact solution but (c) requires the exact solution, it is important to examine
the results when the exact solution is known. Comparing the results for bfwa782, poisson3Db, and
watt 1 in cases (a) and (b) (Tables 3 and 4), the conventional PCGS (Algorithm 2) has results in
which the true relative residual or true relative error (or both) are much less accurate than those
obtained by the other algorithms, and only in the conventional PCGS does stagnation occur at
insuﬃcient accuracy6. In particular, the conventional PCGS is the fastest to converge for watt 1
in cases (a) and (b) (Tables 3 and 4), but this is undesirable, because when it converges too quickly,
evaluating by the relative residual and by the true relative residual to satisfy the accuracy. On the
other hand, evaluating by the true relative error in the case of (c) (Table 5), the conventional PCGS
converges after almost the same number of iterations as do the other methods.

Next, in contrast, the results of the conventional PCGS with wang4 gave the most accurate true
relative error for cases (a) and (b) (Tables 3 and 4), but the conventional PCGS stagnated with
wang4, and this resulted in the lowest accuracy for case (c) (Table 5).

From the graphs in Figures 2 to 7, we can see the following: in case (a), Improved1, Improved2,
and the left-PCGS show diﬀerent convergence behaviors, but in cases (b) and (c), they show similar
behaviors. These results correspond to the analysis in section 2.3. Therefore, Algorithms 4 and 5 are
coordinative to Algorithm 3 regarding the structures of the solution vector for the generated Krylov
subspace, in spite of the diﬀerence between the residual vectors: r+
k for the left-PCGS (Algorithm 3)
and rk for Improved1 and Improved2 (Algorithms 4 and 5, respectively). The conventional PCGS
had a convergence behavior that diﬀers from those of all of the other algorithms for all cases (a) to
(c).

These numerical results conform to the behavior expected from the discussion of the relation
between the structure of solution vector and the Krylov subspace. We compared the numerical

5 The row marked olm5000 in all tables contains the results after 1000 iterations; furthermore, olm5000 by Algo-

rithm 2 stagnated after 5000 iterations, due to the size of the matrix.

6 The results of poisson3Db with Improved2 in Table 4 and olm5000 with the left-PCGS in Table 5 can be considered
to be suﬃciently accurate, because they were almost convergence with being close to 10−12. We note that ε = 10−12
is a stringent value for the tolerance for the true relative residual and the true relative error.

13

sherman4

Conventional
Left-PCGS
Improved1
Improved2

 100

 1

 0.01

 0.0001

 1e-06

 1e-08

 1e-10

 1e-12

)
0
1
g
o
l
(
 

m
r
o
n
-
2

 
l

a
u
d
s
e
r
 

i

e
v
i
t

l

a
e
r
 

m
h

t
i
r
o
g
A

l

 1e-14

 0

 5

 10

 15
 20
Iteration number

 25

 30

 35

Figure 2: (a) Convergence histories of the algorithm relative residual 2-norm for each of the four
algorithms (sherman4).

sherman4

Conventional
Left-PCGS
Improved1
Improved2

 100

 1

 0.01

 0.0001

 1e-06

 1e-08

 1e-10

 1e-12

)
0
1
g
o
l
(
 

m
r
o
n
-
2

 
l

i

a
u
d
s
e
r
 
e
v
i
t

l

a
e
r
 
e
u
r
T

 1e-14

 0

 5

 10

 15
 20
Iteration number

 25

 30

 35

Figure 3: (b) Convergence histories of the true relative residual 2-norms (sherman4) for each of the
four algorithms.

sherman4

Conventional
Left-PCGS
Improved1
Improved2

 100

 1

 0.01

 0.0001

 1e-06

 1e-08

 1e-10

 1e-12

)
0
1
g
o
l
(
 

m
r
o
n
-
2
 
r
o
r
r
e
 
e
v
i
t
a
e
r
 
e
u
r
T

l

 1e-14

 0

 5

 10

 15
 20
Iteration number

 25

 30

 35

Figure 4: (c) Convergence histories of the true relative error 2-norm (sherman4) for each of the four
algorithms.

14

watt__1

Conventional
Left-PCGS
Improved1
Improved2

 10000

 100

 1

 0.01

 0.0001

 1e-06

 1e-08

 1e-10

 1e-12

)
0
1
g
o
l
(
 

m
r
o
n
-
2

 
l

a
u
d
s
e
r
 

i

e
v
i
t

l

a
e
r
 

m
h

t
i
r
o
g
A

l

 1e-14

 0

 5

 10

 15

 20

 25

Iteration number

 30

 35

 40

 45

Figure 5: (a) Convergence histories of the algorithm relative residual 2-norm (watt 1) for each of
the four algorithms.

watt__1

Conventional
Left-PCGS
Improved1
Improved2

 10000

 100

 1

 0.01

 0.0001

 1e-06

 1e-08

 1e-10

 1e-12

)
0
1
g
o
l
(
 

m
r
o
n
-
2

 
l

i

a
u
d
s
e
r
 
e
v
i
t

l

a
e
r
 
e
u
r
T

 1e-14

 0

 5

 10

 15

 20

 25

Iteration number

 30

 35

 40

 45

Figure 6: (b) Convergence histories of the true relative residual 2-norm (watt 1) for each of the four
algorithms.

watt__1

Conventional
Left-PCGS
Improved1
Improved2

 10000

 100

 1

 0.01

 0.0001

 1e-06

 1e-08

 1e-10

 1e-12

)
0
1
g
o
l
(
 

m
r
o
n
-
2
 
r
o
r
r
e
 
e
v
i
t
a
e
r
 
e
u
r
T

l

 1e-14

 0

 5

 10

 15

 20

 25

Iteration number

 30

 35

 40

 45

Figure 7: (c) Convergence histories of the true relative error 2-norm (watt 1) for each of the four
algorithms.

15

results with the theoretical results of sections 2.1.3 and 2.3, and these are summarized as follows:

1. For case (a), the diﬀerence between the residual vector r+

k of the left-PCGS and rk has been

veriﬁed.

2. For cases (b) and (c), we veriﬁed (2.16):

xk ∈ x0 + KL

2k(M −1A, r+

0 ) 7→ xk ∈ x0 + M −1KL

2k(AM −1, r0).

3. The diﬀerences between the conventional PCGS and the left-PCGS, Improved1, Improved2
have been conﬁrmed through their convergence behaviors. That is, the relation of the solution
vector and the Krylov subspace between the right system (the conventional PCGS) and the
left-PCGS, the coordinative PCGSs to the left-PCGS (Improved1 and Improved2).

4.2 Behavior of the PCGS when it is switched by the ISRV

In this subsection, the experimental environment was same as that described in section 4.1, except
that we used Matlab 7.8.0 (R2009a), and we gave diﬀerent ISRVs to the conventional PCGS and
Improved1.

We compared ﬁve diﬀerent PCGS algorithms, including using a diﬀerent ISRV. In the ﬁgures,
we use the following labels. “Conventional” means the conventional PCGS (Algorithm 2), for which
the ISRV is r♭
0 = r0; this is a right-preconditioned system. “Impr1-ISRV1” means Improved1 (Al-
gorithm 4) with ISRV1: r♯
0 = M Tr0.
“Left” means the left-PCGS (Algorithm 3), for which the ISRV is r♯
0 . “Conv ISRV9” means
the conventional PCGS with ISRV9: r♭

0 = M −1r0. “Impr1-ISRV2” means Improved1 with ISRV2: r♯

0 = M −TM −1r0.

0 = r+

The convergence histories of “Conventional,” “Impr1-ISRV1,” and “Left” in both ﬁgures are the

same as those of “Conventional,” “Improved1,” and “Left-PCGS,” respectively, in Figures 3 and 6.

In both ﬁgures, “Impr1-ISRV2” and “Conv ISRV9” were added to verify Theorem1. The conver-
gence history of “Impr1-ISRV2” is the same as that of “Conventional,” and those of “Impr1-ISRV1”
and “Conv ISRV9” are the same as that of “Left.”

We have numerically veriﬁed the discussion in section 3; in particular, we have veriﬁed Theorem1.

5 Conclusions

In this paper, an improved PCGS algorithm [8] has been analyzed by mathematically comparing
four diﬀerent PCGS algorithms, and we have focused on the structures of their Krylov subspace and
the solution vector. From our analysis and numerical results, we have veriﬁed two improved PCGS
algorithms. They are both coordinative to the left-preconditioned systems, although their residual
vector maintains the basic form rk, not r+
k . For both algorithms, the structures of their Krylov
subspace and the solution vector are xk ∈ x0 + M −1KL
2k (cid:0)AM −1, r0(cid:1). Further, the numerical results
of the improved PCGS with the ILU(0) preconditioner show many advantages, such as eﬀectiveness
and consistency across several preconditioners, have also been shown; see [8].

We presented a general deﬁnition of the direction of a preconditioned system of linear equations.
Furthermore, we have shown that the direction of a preconditioned system for CGS is switched
by the construction and setting of the ISRV. This is because the direction of the preconditioning
conversion is congruent. We have also shown that the direction of a preconditioned system for
CGS is determined by the operations of αk and βk, and these intrinsic operations are based on
biorthogonality and biconjugacy. However, the structures of these intrinsic operations are the same
in all four of the PCGS algorithms. Therefore, we have focused on the ability of the ISRV to switch
the direction of a preconditioned system, and such a mechanism may be unique to the bi-Lanczos-
type algorithms that are based on the BiCG method.

As we analyzed the four PCGS algorithms, we paid particular attention to the vectors. We note
that there exist preconditioned BiCG (PBiCG) algorithms that correspond to the preconditioning
conversion of each of the PCGS algorithms. The polynomial structure of the PBiCG can be minutely
analyzed by replacing the vectors of the PCGS. We have analyzed the four PBiCG algorithms in
parallel [9], and each PBiCG corresponds to one of the four PCGS algorithms in this paper. In [9],
using the ISRV to switch the direction of a preconditioned system was discussed in detail.

16

)
0
1
g
o
l
(
 

m
r
o
n
−
2

 
l

i

a
u
d
s
e
r
 
e
v
i
t

l

a
e
r
 
e
u
r
T

2

0

−2

−4

−6

−8

−10

−12

−14

 
0

sherman4

 

Conventional
Impr1_ISRV1
Impr1_ISRV2
Left
Conv_ISRV9

5

10

15

20

25

30

35

Iteration Number

Figure 8:
preconditioned PCGS, for each of the ﬁve PCGS algorithms with ISRV switching (sherman4).

Convergence histories of the true relative residual 2-norm of the right- and left-

)
0
1
g
o
l
(
 

m
r
o
n
−
2

 
l

i

a
u
d
s
e
r
 
e
v
i
t

l

a
e
r
 
e
u
r
T

2

0

−2

−4

−6

−8

−10

−12

−14

 
0

watt__1

 

Conventional
Impr1_ISRV1
Impr1_ISRV2
Left
Conv_ISRV9

5

10

15

20

25

30

35

Iteration Number

Figure 9: Convergence histories of true relative residual 2-norm of the right- and left-preconditioned
PCGS, for each of the ﬁve PCGS algorithms with ISRV switching (watt 1).

17

References

[1] Barrett, R. et al., Templates for the Solution of Linear Systems: Building Blocks for Iterative

Methods, SIAM, Philadelphia, PA, 1994.

[2] Bruaset, A. M., A Survey of Preconditioned Iterative Methods, Longman Scientiﬁc & Technical,

Harlow, Essex, UK, 1995.

[3] Davis, T. A., The University of Florida Sparse Matrix Collection,

http://www.cise.uﬂ.edu/research/sparse/matrices/

[4] Fletcher, R., Conjugate gradient methods for indeﬁnite systems, in Numerical Analysis: Pro-
ceedings of the Dundee Conference on Numerical Analysis, 1975, G. Watson, ed., Lecture Notes
in Math. 506, Springer, New York, 1976, pp. 73–89.

[5] Gutknecht, M. H., On Lanczos-type methods for Wilson Fermions, in Numerical Challenges in
Lattice Quantum Chromodynamics, A. Frommer, T. Lippert, B. Medeke, and K. Schilling, eds.,
Lecture Notes in Computational Science and Engineering 15, Springer, Berlin, 2000, pp. 48–65.

[6] Hestenes, M. R. and Stiefel, E., Methods of conjugate gradients for solving linear systems, J.

Res. Nat. Bur. Standards, 49 (1952), pp. 409–435.

[7] Itoh, S. and Sugihara, M., Systematic performance evaluation of linear solvers using quality
control techniques, in Software Automatic Tuning From Concepts to State-of-the-Art Results,
K. Naono, K. Teranishi, J. Cavazos, and R. Suda, eds., Springer, New York, 2010, pp. 135–152.

[8] Itoh, S. and Sugihara, M., Formulation of a preconditioned algorithm for the conjugate gradient
squared method in accordance with its logical structure, Appl. Math., 6 (2015), pp. 1389–1406.

[9] Itoh, S. and Sugihara, M., The structure of the polynomials in preconditioned BiCG algorithms

and the switching direction of preconditioned systems, arXiv, 2016.

[10] Lanczos, C., Solution of systems of linear equations by minimized iterations, J. Res. Nat. Bur.

of Standards, 49 (1952), pp. 33–53.

[11] Matrix Market, http://math.nist.gov/MatrixMarket/

[12] Meurant, G., Computer Solution of Large Linear Systems, Elsevier, New York, 2005.

[13] Sonneveld, P., CGS: A fast Lanczos-type solver for nonsymmetric linear systems, SIAM J. Sci.

Stat. Comput., 10 (1989), pp. 36–52.

[14] SSI project, Lis: Library of iterative solvers for linear systems, http://www.ssisc.org/lis/

[15] Van der Vorst, H. A., Bi-CGSTAB: A fast and smoothly converging variant of Bi-CG for the
solution of nonsymmetric linear systems, SIAM J. Sci. Stat. Comput., 13 (1992), pp. 631–644.

[16] Zhang, S.-L., GPBi-CG: Generalized product-type methods based on Bi-CG for solving nonsym-

metric linear systems, SIAM J. Sci. Comput., 18 (1997), pp. 537–551.

18

