6
1
0
2

 
r
a

 

M
2
1

 
 
]

V
C
.
s
c
[
 
 

1
v
1
1
9
3
0

.

3
0
6
1
:
v
i
X
r
a

Optical Flow with Semantic Segmentation and Localized Layers

Laura Sevilla-Lara1 Deqing Sun2,3 Varun Jampani1 Michael J. Black1

{laura.sevilla, varun.jampani, black}@tuebingen.mpg.de

1MPI for Intelligent Systems

2NVIDIA , 3Harvard University

deqings@nvidia.com

(b) Our segmentation

(d) Our optical ﬂow

(a) Initial segmentation [8]
Figure 1:
(a) Semantic segmentation breaks the image into regions with labels such as road, bike, person, sky, etc. (c)
Existing optical ﬂow algorithms do not have access to either the segmentations or the semantics of the classes. (d) Our
semantic optical ﬂow algorithm computes motion differently in different regions, depending on the semantic class label
resulting in more precise ﬂow, particularly at object boundaries. (b) The ﬂow also helps reﬁne the object segmentation of the
foreground objects.

(c) DiscreteFlow [33]

Abstract

Existing optical ﬂow methods make generic, spatially
homogeneous, assumptions about the spatial structure of
the ﬂow. In reality, optical ﬂow varies across an image de-
pending on object class. Simply put, different objects move
differently. Here we exploit recent advances in static seman-
tic scene segmentation to segment the image into objects of
different types. We deﬁne different models of image motion
in these regions depending on the type of object. For exam-
ple, we model the motion on roads with homographies, veg-
etation with spatially smooth ﬂow, and independently mov-
ing objects like cars and planes with afﬁne motion plus de-
viations. We then pose the ﬂow estimation problem using
a novel formulation of localized layers, which addresses
limitations of traditional layered models for dealing with
complex scene motion. Our semantic ﬂow method achieves
the lowest error of any published monocular method in the
KITTI-2015 ﬂow benchmark and produces qualitatively bet-
ter ﬂow and segmentation than recent top methods on a wide
range of natural videos.

1. Introduction

[7, 11]. However, even state-of-the-art optical ﬂow meth-
ods still perform poorly with fast motions, in areas of low
texture, and around object (occlusion) boundaries (Fig. 1
(c)). Here we address these issues and improve the estima-
tion of optical ﬂow by using semantic image segmentation.
Like ﬂow, the ﬁeld of semantic segmentation is also mak-
ing rapid progress, driven by convolutional neural networks
(CNNs) and large amounts of labeled data. Here we use a
state-of-the-art method [8] (Fig. 1 (a)) and ﬁnd that existing
semantic segmentation methods, while not perfect, are good
enough to signiﬁcantly improve ﬂow estimation.

We use semantic image segmentation in multiple ways.
First, it provides information about object boundaries. Sec-
ond, different objects move differently; roads are ﬂat, cars
move independently, and trees sway in the wind. This
means that our prior expectations about the image motion
should vary between regions with different class labels.
Third, the spatial relationships between objects provide in-
formation about the relative local depth ordering of regions.
Reasoning about depth order is typically challenging and
we use the semantics to simplify this, improving ﬂow es-
timates at occlusion boundaries. Fourth, object identities
are constant over time, providing a cue that we exploit to
encourage temporally consistency of the optical ﬂow.

The accuracy of optical ﬂow methods is improving
steadily, as evidenced by results on several recent datasets

To model complex scene motions and to deal well with
motion boundaries, we adopt a layered approach [3, 9, 14,

1

tic regions (Fig. 4).

We call the algorithm semantic optical ﬂow (SOF) be-
cause it exploits scene semantics to improve ﬂow estima-
tion. The approach achieves the lowest error on the KITTI-
2015 ﬂow dataset [32], when compared with all published
monocular ﬂow methods.1 We also test the method on a
challenging range of sequences from the Internet. There
are several reasons for the improvements. First our motion
models provide a form of long-range regularization in areas
like roads. Since these are well modeled by a homography,
accuracy improves. Second, this region-based regulariza-
tion helps ﬂow estimation in homogeneous regions, which
contain few motion cues. Third, the localized layer formu-
lation improves the segmentation and ﬂow around motion
boundaries. Key here is that the object segmentation gives a
good initialization for layered ﬂow segmentation and gives
a good hypothesis for which surface is in front and which is
behind; this improves occlusion estimation.

While we focus on improving optical ﬂow, we note that
motion can also help with scene segmentation. While cur-
rent semantic segmentation methods are good, they still
struggle to separate object boundaries from appearance
boundaries (Fig. 1 (a)). Layered optical ﬂow estimation seg-
ments the region and provides additional information about
object boundaries (Fig. 1 (b)). When computed over several
frames, this segmentation can be quite precise.

In summary, we make two contributions. First, we
present the ﬁrst optical ﬂow method that uses semantic in-
formation about scenes, objects, and their segmentation,
producing the lowest error among all monocular methods
on the KITTI ﬂow benchmark. Second, we show how lay-
ered optical ﬂow estimation can be extended to cope with
complex scenes. Our results conﬁrm that knowing what and
where things are helps the estimation of how they move.

2. Related Work

Motion estimation and segmentation. There is a long
history of simultaneously estimating optical ﬂow and its
segmentation [31, 35]. Many methods focus on segmen-
tation using motion information alone; we do not consider
these here. More relevant are methods that use image seg-
mentation to aid optical ﬂow. Previous work [6, 52] seg-
ments the scene into patches according to color or other
cues, and then ﬁts parametric ﬂow models within these.
Like us they vary the type of model in each region but we
go beyond this to use semantic information to determine the
appropriate model. Sun et al. [40] ﬁrst segment the scene
into superpixels and then reason about the occlusion rela-
tionships between neighboring superpixels (cf. [51]). These
methods are generic in the sense that they do not know any-

1The most accurate methods use stereo motion sequences and exploit

the stereo to estimate scene structure.

Figure 2: Localized layered model. An image is seg-
mented into semantic regions (color coded). Different re-
gions are assigned different motion models. Independently
moving objects are shown with a region around them. These
regions require reasoning about occlusion because such ob-
jects move in front of the background. Within each such re-
gion, we make the assumption that two motions are present
(the background and the foreground object). The formula-
tion is similar to previous layered models but here the spa-
tial extent of each layer may vary.

17, 20, 42, 47, 48, 49]. Layered models, however are typ-
ically global and cannot represent complex occlusion rela-
tionships. There have been attempts to formulate locally
layered models [21, 40], but these methods are still spa-
tially homogenous. Here we propose a new model of local-
ized layers in which the number of layers in the scene varies
spatially. Any pixel of the scene may belong to one or more
layers and these layers may have varying spatial extent. Lo-
cal layered models are used as needed to capture relevant
scene objects. In regions corresponding to objects that can
move, we may ﬁnd two motions – the foreground motion of
the object against a background motion. Here we use local
two-layer models. Rather than a small number of global lay-
ers, the result is a patchwork of smaller layered regions on
top of background regions as illustrated in Fig. 2. The ap-
proach keeps the complexity and optimization manageable
by using at most two layers within any patch. But, because
we can use as many patches as needed, the approach can
model complex motions. This adaptive, spatially inhomo-
geneous approach extends layered models to more complex
scenes and uses them where they are most valuable.

Each layer or region is represented by a motion model
and the type of model varies depending on the semantic la-
bel of the region. For regions that are likely to be planar we
model their motion with a homography; this includes roads,
sky, and water. For regions corresponding to independently
moving objects, we treat their motion as afﬁne but allow it
to deviate from this assumption; these classes include ob-
jects like cars, planes, boats, horses, bicycles, and people.
There are still other classes like vegetation and buildings
that are diverse in their 3D shape and motion and are con-
sequently not well modeled by a simple parametric motion.
Consequently, we model these classes with a classical spa-
tially varying dense ﬂow ﬁeld. The motion of the scene is
then described by composing the motions of all the seman-

2

thing about the objects being segmented but rather seek a
partitioning of the scene into coherently moving regions.

Combining ﬂow models. Here we use different ﬂow
models to represent the motion of different parts of the
scene. These are combined within our localized layer for-
mulation to deﬁne the ﬂow for the whole image. Previous
work has explored the combination of different ﬂow algo-
rithms [26, 29]. Irani and Anandan [15] develop a theory
for modeling motion in general scenes with varying levels
of complexity. The above methods, however, are generic
in the sense that they do not use any semantic information
about objects to select among the possible models.

Occlusion reasoning and ﬁgure-ground. One goal of
optical ﬂow estimation is the detection of motion disconti-
nuities that may signal the presence of an object (surface)
boundary (see [45] for an overview). Previous methods
focus on generic constraints without taking into account
object-speciﬁc information [5, 39, 43, 45]. In these cases
the goal is to detect boundaries that may be useful later for
object detection. We turn this around by performing object
detection and then using this to detect motion boundaries
more accurately.

Layered optical ﬂow. Layered ﬂow estimation has a
long history [3, 9, 14, 17, 20, 47, 48] and recent improve-
ments have made the approach more competitive on stan-
dard benchmarks [42] and more computationally tractable
[49]. The most recent work integrates image segmentation
cues with motion cues to produce an accurate segmenta-
tion at motion boundaries. In particular, we build on [42],
which uses a fully connected graphical model (cf. [22]) to
exploit long-range image cues for layer segmentation. Un-
like previous work, we apply the model locally within im-
age patches around segmented objects that can move.

Traditional layered models have limitations and are most
applicable to simple scenes with a small number of moving
objects. Occlusion relationships in the world are complex
and 2D motion layers are too restrictive to capture the 3D
spatial occlusion relationships in real scenes. Also, while
the depth order of layers is important, this may be ambigu-
ous in two frames [41]. Reasoning about layer depth order
is combinatorial (K! for K layers), which becomes infeasi-
ble in realistic scenarios. To address these issues, locally
layered models of motion have been proposed [21, 40].
These models, again, are generic and do not know about
objects. Here we ﬁnd the problem of depth order reasoning
is often simpliﬁed when we have semantic information. For
example, we assume that independently moving objects like
cars are in front of static objects like roads. When the as-
sumption holds, as it often does, this simpliﬁes layered ﬂow
estimation and produces accurate motion boundaries.

Several methods decompose scenes into layers corre-
sponding to objects [18, 20, 24, 46, 53]. What these meth-
ods mean by “object,” however, is a region of the image

that moves coherently and differently from the background;
there is no notion of what this object is. In contrast, Isola
and Liu [16] represent static images of scenes as a patch-
work of objects layered on top of each other but they do not
consider image motion.

Video segmentation. There is signiﬁcant and increasing
interest in the ﬁeld [10, 12, 27, 36, 37, 50] but the deﬁnition
of the problem varies between identifying coherent motions
or coherent objects regions. Like the approaches above,
these methods are generic in that they focus on bottom-up
analysis of regions and motion. They typically use opti-
cal ﬂow as a cue to track superpixels over time to establish
temporal coherence. They usually do not use high-level ob-
ject recognizers or try to improve optical ﬂow. Taylor et
al. [44] incorporate object detections and use temporal in-
formation to reason about occlusions to improve their seg-
mentation results, but do not compute optical ﬂow. Lalos et
al. [25] compute optical ﬂow for an object of interest using
a tracking-by-detection approach. Unlike us, they only esti-
mate object displacement (not full ﬂow), ignore background
motion, and do not take object identity into account.

3. Model and Methods

Using a semantic segmentation of the scene allows us to
model the motion of different regions of the image differ-
ently. We deﬁne the motion in the scene compositionally in
terms of the motion of the regions. Below we discuss how
we compute the motion for each segmented region and then
how we combine these into a coherent ﬂow ﬁeld.

Classes. We deﬁne three classes of objects (Things,
Planes, and Stuff) that exhibit different types of motion (see
Fig. 2). (1) Things [1, 13] correspond to objects with a de-
ﬁned spatial extent, that can move independently, are typi-
cally seen in the foreground and may be rigid or non-rigid.
Things include aeroplane, bicycle, bird, boat, bus, car, cat,
(2)
cow, dog, horse, motorbike, sheep, train and person.
Planes are regions like ‘roads’ that have a broad spatial ex-
tent, are roughly planar, and are typically in the background.
Other classes that we treat as planes are ‘sky’ and ‘water’.
Water is treated as a plane because the air/water boundary is
often planar. (3) Stuff [2] corresponds to classes that exhibit
textural motion or objects like ‘buildings’ and ‘vegetation’
that may have a complicated 3D shape, exhibit complex par-
allax, and for which we have no compact motion represen-
tation. Regions of unknown class are modeled as Stuff.
3.1. Preprocessing

Segmentation.
We used the Caffe framework [19] to train a seman-
tic segmentation model. Following the DeepLab architec-
ture [8], we modiﬁed the VGG network [38] substituting
all fully-connected layers with convolutional layers. We
modiﬁed the output layer to predict the 22 classes described

3

above. We used the atrous [30] algorithm to get denser pre-
dictions with a stride of 8.

We initialized the network with the VGG model and ﬁne-
tuned it with standard stochastic gradient descent using a
ﬁxed momentum of 0.9 and weight decay of 0.0005 during
200K iterations. The learning rate is 0.0001 for the ﬁrst
100K iterations and is reduced by 0.1 after every 50K steps.
We used patch-wise training [28] with a batch size of 10.

To get better performance [8], we used a densely con-
nected conditional random ﬁeld (Dense-CRF) [23] with the
CNN output as unaries and the pairwise potentials com-
prising of two kernels: one is the position kernel and the
other is a bilateral kernel with both position and RGB color.
This is a standard Dense-CRF model that is shown to be
useful for semantic segmentation [8, 23]. The standard de-
viation of the ﬁlter kernels and their relative weights are
cross-validated. The inference in the Dense-CRF model is
performed using 10 steps of meanﬁeld.

To train the network, we selected 22 of the 540 classes

from the PASCAL-CONTEXT dataset [34].

Thing tracking. Given the segmentation in each frame,
we compute connected components to obtain regions con-
taining putative objects (Things). We track across time each
Thing found in the ﬁrst frame. We use up to 5 frames at
a time if they are available, or 2 if they are not. Given
these tracks across time, we deﬁne local regions in which
to compute layered motion estimation. These regions fully
surround the object in all frames and serve to deﬁne the spa-
tial extent of the layered ﬂow estimation (Fig. 3). Figure 2
shows a few Thing regions in one frame. Note that if no
correspondence for a Thing region is found over the sub-
sequence, we change its type to Stuff.

Initial ﬂow. We also compute an initial dense ﬂow ﬁeld,
ˆu using the DiscreteFlow method [33]. We use this in sev-
eral ways as described below.

3.2. Motion Models

The motion of Planes. We model planar regions using
homographies. Given the initial ﬂow vectors ˆu(x), x ∈
Ri in region i, we use RANSAC to robustly estimate the
parameters, hi of the homography. The planar motion then
deﬁnes the ﬂow uPlane(x; hi) for every pixel x ∈ Ri.

The motion of Stuff. For Stuff we have no class-speciﬁc
motion model and set the ﬂow in every Stuff region i to be
the initial ﬂow; that is uStuff(x) = ˆu(x) for x ∈ Ri.

The motion of Things. In Thing regions we expect oc-
clusions and disocclusions, complex geometry, and defor-
mations. Thus, we assume the motion of a Thing can be
described as afﬁne plus a smooth deformation from afﬁne.
This may sound restrictive but we build on the work of
[42], where they show positive results applying this mo-
tion model to the entire scene. Our Thing regions are much
smaller than the entire scene, and the motion within this re-

Figure 3: The method in pictures. (a) Image with the seg-
mentation into road (blue), car (green), sky (yellow), grass
(grey), and “unknown” (clear) superimposed.
(b) Initial
dense ﬂow computed with Discrete ﬂow [33]. The follow-
ing images show intermediate results in the extracted car re-
gion. (c) Our ﬁnal Thing segmentation. (d) Our ﬁnal ﬂow.
(e) Estimated foreground motion. (f) Estimated background
motion. (g) Estimated ﬂow for the localized region. (h) Fi-
nal layer segmentation (blue is foreground).

gion is more likely to satisfy the assumptions. We allow the
motion of Things to deviate from afﬁne and the amount of
deviation depends on the object class. For example, cars are
more rigid than people and their motion is more afﬁne. Con-
sequently we assume that the motion of cars will be more
afﬁne and penalize deviations from this assumption more.
While we are interested in the motion of the Thing, be-
cause we assume Things are in front of backgrounds, it is
actually important to also consider the motion of the back-
ground. Speciﬁcally, estimating an accurate foreground
segmentation requires that we reason about the motion of
both foreground and background. We do this using the lay-
ered model of [42], but restrict the motion estimation to a
the estimated Thing region and assume that there are only
two motions present in the region.
Formally, given a sequence of images {It, 1 ≤ t ≤ T},
we want to jointly estimate the motion (utk, vtk) for every
pixel, in each layer, at every frame, as well as group pix-
els that move together into layers denoted by gtk, where
k ∈ {1, 2}. In the case of two layers that we consider here,
we only need to estimate the segmentation, gt1, for the fore-
ground layer as the background layer is constant. We for-
mulate the local layered energy term (Eq. 1) similar to Sun
et al. with some modiﬁcations and refer the reader to [42]
for further details. The method estimates the motion of both
layers and the segmentation of the foreground region.

The general formulation incorporates occlusion reason-
ing in the motion estimation using layered segmentation
(data term), enforces temporal consistency of layer segmen-
tation (time term) according to the motion, couples seman-
tic segmentation and layered segmentation (layer term), and
encourages spatial contiguity of layered segmentation using
a fully-connected CRF model (space term).

4

2(cid:88)

EThing(u,v,g, Θ;I) =

(cid:110) T−1(cid:88)

{Edata(utk,vtk,gtk;It:t+1)+λmotionEmotion(utk,vtk,gtk, Θtk)

(1)

k=1

t=1

+λtimeEtime(utk,vtk,gt:t+1,k)}+

{λlayerElayer(gtk)+λspaceEspace(gtk)}(cid:111)
T(cid:88)

.

The data term imposes appearance constancy when cor-
responding pixels are visible at the same layer, and a con-
stant penalty otherwise. It reasons about occlusions by com-
paring the layer assignment of corresponding pixels:

(cid:88)

Edata(utk,vtk,gtk; It:t+1) =
t+1)δ(gp

ρD(I p

t1 = gq

t+1,1)+

t −I q
t1(cid:54)= gq
tk, y + vp

t+1,1),

p
λDδ(gp
where q = (x + up

(2)
tk) denotes the corresponding
pixel according to the motion for pixel p, for every pixel
in the image, ρD is a robust penalty function, and λD is a
constant penalty for occluded pixels and pixels of different
objects. The indicator function δ(x) is 1 if the expression x
is true, and 0 otherwise.

The motion term encodes two assumptions.

First,
neighboring pixels should have similar motion if they be-
long to the same layer. Second, pixels from each individual
layer k should share a global motion model ¯u(Θtk), where
Θtk are parameters that change over time and depend on the
object class k:

(cid:88)
(cid:88)

r∈Np

(cid:88)

p

λaff

p

Emotion(utk,vtk,gtk, Θtk) =

tk = gr

tk)+

ρ(up

tk)δ(gp

tk−ur
tk− ¯up(Θtk))

ρaff(up

(3)

where the set Np contains the four nearest spatial neighbors
of the pixel p. The motion term for the vertical ﬂow ﬁeld vt
is deﬁned similarly.

The time term encourages corresponding pixels over

time to have the same layer label

Etime(utk,vtk,gt:t+1k) =

δ(gp

tk (cid:54)= gq

t+1k),

(4)

(cid:88)

p

where q is the corresponding pixel at the next frame for p
according to the motion (utk, vtk).

The space term encourages spatial contiguity of layer

segmentation:

Espace(gtk) =

(cid:88)

(cid:88)

r(cid:54)=p

p

wp

r δ(gp

tk (cid:54)= gr

tk),

(5)

5

(cid:88)

δ(cid:0)gp

tk (cid:54)= ˆgp

t

(cid:1),

t=1

r is the same as in Sun et al. [42]. This
where the weight wp
term fully connects each pixel with all other pixels in the
localized region. In our implementation, we modify the ap-
proach in [42] and apply this, not over the whole frame, but
over a detected object region.

The major difference from Sun et al. [42] is that we have
a semantic segmentation for the foreground and this seg-
mentation is usually reasonably good. Consequently we de-
ﬁne a new coupling term, Elayer, that enforces similarity
between the foreground layer segmentation and the seman-
tic segmentation:

Elayer(gtk) =

(6)

p

where ˆgt is the semantic segmentation mask of the fore-
ground Thing; i.e. ˆgp

t = δ(cp

t = 1).

Initialization and optimization. The layer method re-
quires an initialization of the foreground region g, an initial
ﬂow ˆu, and parametric motions of both layers ¯u(Θ).

The initial ﬂow is typically inaccurate at the boundaries
and we do not want this to corrupt the initialization. Con-
sequently we compute the initial afﬁne motion ignoring the
pixels close to the object boundary both in the background
and foreground. We then optimize Eq. 1 using the method
in [42]. This reﬁnes the ﬂow of each layer and the segmen-
tation (Fig. 3). The segmentation is quite accurate because
it uses backward and forward ﬂow and image evidence with
the fully connected model in the region (see [42]). The
method [42] uses heuristics to reason about depth ordering.
Here we use the class category to decide the depth ordering
and assume that Things are always foreground.

3.3. Composing the Flow Field

Each Plane and Stuff region gives exactly one ﬂow value
at each pixel in the region and, if this pixel is not occluded
by a localized layer, then this will be the ﬁnal ﬂow value.
The localized layers extend over the background regions
and each layer gives a foreground and background ﬂow.
There are several ways to composite this with the back-
ground and we use the following (Fig. 4). The foreground
ﬂow is directly pasted onto the ﬂow ﬁeld (blue region). The
background ﬂow is linearly interpolated with the original

Table 1: Results in the test set of KITTI 2015.

Method

Fl-all
(All px)

Fl-fg
(Nocc)
Discrete 22.38% 21.53% 26.68% 12.18% 9.96% 22.17%
SOF 16.81% 14.63% 27.73% 10.86% 8.11% 23.28%

Fl-bg
(All px)

Fl-fg
(All px)

Fl-all
(Nocc)

Fl-bg
(Nocc)

Figure 4: Compositing the ﬂow. The motion of Stuff,
Planes (yellow) and regions around Things (red and blue)
is composited to produce the ﬁnal ﬂow estimation.

ﬂow in the Stuff regions (dark red region), and left un-
touched in the planes (yellow region). We found this ap-
proach to be faster and perform better than FusionFlow [26].

4. Experiments
We test our Semantic Optical Flow (SFO) method in two
different datasets: natural Youtube sequences and KITTI
2015 [32]. Standard optical ﬂow benchmarks do not con-
tain the variety of objects that a semantic segmentation
method can recognize. Thus, we collected a suite of nat-
ural videos from YouTube, containing objects of the Pascal
VOC classes that move. Although there is no ground truth
to provide a quantitative analysis, the difference of qual-
ity can be observed qualitatively in planar regions and at
motion boundaries. All sequences will be made publicly
available.
In addition, we test our method on the KITTI
2015 dataset, where existing semantic segmentation meth-
ods perform reasonably well. We do not include results on
the Sintel dataset because semantic segmentation does not
produce reasonable results. This is probably due to the fact
that the statistics of synthetically generated images are dif-
ferent from those of natural images, like the ones in the
enriched Pascal VOC dataset. We tried training the same
network using the Sintel training set (manually annotated),
and we found that the network did not perform well, pre-
sumably due to a shortage of training data. In the Middle-
bury dataset [4] the semantic segmentation results produce
mostly the ‘unknown’ class, or they correspond to classes
without a speciﬁc motion model (i.e. building), or they are
very small regions and we do not consider them. Thus, on
Middlebury our results are identical to the initial ﬂow (Dis-
crete Flow) in all sequences but in one, where our accuracy
is 0.004 better.

4.1. KITTI 2015

For a numeric comparison with previous work we test
our method on the KITTI 2015 benchmark (Fig. 5). A nu-
merical comparison between the state-of-the-art Discrete-

6

Flow and our method is shown in Table 1. Our method has
a signiﬁcant reduction in the overall percentage of outliers
compared with DiscreteFlow (from 22.38% to 16.81%).
The improvements mainly come from 1) our reﬁned mo-
tion for the Planes; and 2) correctly interpolated motion for
the occluded background regions. Figure 6 shows several
examples where our method ﬁxes large errors of the fore-
ground cars in the initial DiscreteFlow results.

Our method has slightly higher percentage of outliers in
the foreground region, which reveals some limitation of our
approach. We only assume two major motions present in
the detected region and our method tends to fail when the
assumption breaks, as shown in Fig. 7.
In the ﬁgure the
issue arises because our segmentation method gives a class
segmentation, grouping multiple cars together. To address
this, we either need instance-level segmentation of Things
or a formulation that deals with more than two layers [41].
The execution time of our method depends on the size
of the image, the number of objects, and the size of these.
An upper bound for the total time is 6 minutes for a frame
of KITTI 2015. Speciﬁcally, the initial semantic segmen-
tation takes 10 seconds, the initial motion estimation from
DiscreteFlow takes 3 minutes, the motion of Planes takes 2
seconds, and the motion of Things depends on the size of
the object, but takes on average 1-2 minutes.
4.2. Natural Sequences.

Figure 8 shows examples on natural sequences down-
loaded from YouTube. Our method improves over the state-
of-the-art optical ﬂow estimation method.
It corrects er-
rors in large planar regions and produces more accurate
motion boundaries.
It is also able to reﬁne the semantic
segmentation, especially at object boundaries and in thin
regions. These results demonstrate the beneﬁts of our ap-
proach when reliable semantic segmentation is available.

5. Conclusion and Future work

We have deﬁned a method for using semantic segmenta-
tion to improve optical ﬂow estimation. Our semantic op-
tical ﬂow method uses object class labels to determine the
appropriate motion model to apply in each region. We clas-
sify a scene into Things, which move independently, Planes,
which are large roughly planar regions, and Stuff, which is
everything else. We focus on the estimation of Things us-
ing a localized layer model in which we only apply layered
optical ﬂow in constrained regions around objects of inter-

Composing the ﬁnal ﬂowLocal LayersFigure 5: Examples of Semantic Optical Flow on KITTI 2015. From left to right: Initial segmentation; Optical ﬂow esti-
mation from SOF; Comparison of outliers between DiscreteFlow and SOF (black pixels indicate neither algorithm produced
an outlier in that location, yellow pixels indicate both methods produced an outlier, green pixels indicate DiscreteFlow was
incorrect SOF was correct, and red pixels indicate DiscreteFlow was correct but SOF was not). Notice that much of the gain
from SOF is on the road, especially at occluded regions, and on the areas close to cars.

Figure 7: Failure case. From left to right: Initial segmen-
tation; SOF segmentation; Flow estimation from Discrete-
Flow; Flow estimation from SOF; Ground truth ﬂow. Our
layered method assumes two dominant motions in the re-
gion, failing if there are more than two motions.

mentations that resemble our semantic segmentation. A key
insight is that a detected object region is likely to contain at
most two motions and the object is likely to be in front. We
show that using motion we are able to improve the segmen-
tation, sometimes dramatically. We tested the method on
the KITTI-2015 ﬂow benchmark and have the lowest error
of any monocular method by a signiﬁcant margin at the time
of writing. We also tested on a wide range of other videos
containing more varied classes and see clear qualitative im-
provement in terms of ﬂow and segmentation.

This work conﬁrms the beneﬁt of using high quality seg-
mentation for optical ﬂow and for exploiting knowledge of
the class labels in estimating ﬂow. This opens several doors
for future work. In particular, it may be possible to formu-
late our localized layer model as a single objective function
and optimize it as such; this may improve results further.
Additionally it would be useful, but challenging, to inte-
grate ﬂow estimation with semantic segmentation. Flow in-
formation may even help with class recognition in addition
to segmentation.

Figure 6: Comparison of details recovered by Semantic
Optical Flow. From left to right: Initial segmentation; SOF
segmentation; Optical ﬂow estimation from DiscreteFlow;
Optical ﬂow estimation from SOF; Ground truth ﬂow.

est. We introduce a novel constraint to prefer layered seg-

7

Figure 8: Qualitative analysis of Semantic Optical Flow. We show a few representative examples from the YouTube
dataset. From left to right: Initial segmentation, SOF segmentation, optical ﬂow estimation from DiscreteFlow, optical ﬂow
estimation from SOF, ground truth ﬂow.

8

References
[1] E. Adelson and J. Bergen. The plenoptic function and the
elements of early vision. In M. Landy and J. Movshon, ed-
itors, Computational Models of Visual Processing, pages 1–
20. MIT Press, 1991.

[2] E. H. Adelson. On seeing stuff: the perception of materials
by humans and machines. In Photonics West 2001-Electronic
Imaging, pages 1–12. International Society for Optics and
Photonics, 2001.

[3] S. Ayer and H. S. Sawhney. Layered representation of motion
video using robust maximum-likelihood estimation of mix-
ture models and MDL encoding. In ICCV, pages 777–784,
Jun 1995.

[4] S. Baker, D. Scharstein, J. P. Lewis, S. Roth, M. J. Black,
and R. Szeliski. A database and evaluation methodology for
optical ﬂow. IJCV, 92(1):1–31, Mar. 2011.

[5] M. J. Black and D. J. Fleet. Probabilistic detection and track-
ing of motion boundaries. IJCV, 38(3):231–245, July 2000.
[6] M. J. Black and A. Jepson. Estimating optical ﬂow in seg-
mented images using variable-order parametric models with
local deformations. PAMI, 18(10):972–986, Oct. 1996.

[7] D. J. Butler, J. Wulff, G. B. Stanley, and M. J. Black. A
naturalistic open source movie for optical ﬂow evaluation.
In ECCV, IV, pages 611–625, 2012.

[8] L. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L.
Yuille. Semantic image segmentation with deep convolu-
tional nets and fully connected crfs. CoRR, abs/1412.7062,
2014.

[9] T. Darrell and A. Pentland. Robust estimation of a multi-
layered motion representation. In Workshop on Visual Mo-
tion, pages 173–178, 1991.

[10] F. Galasso, N. Nagaraja, T. Jimenez Cardenas, T. Brox, and
B. Schiele. A uniﬁed video segmentation benchmark: An-
notation, metrics and analysis. In ICCV, pages 3527–3534,
Dec 2013.

[11] A. Geiger, P. Lenz, C. Stiller, and R. Urtasun. Vision
International Journal

meets robotics: The KITTI dataset.
of Robotics Research, 32(11):1231 – 1237, Sept. 2013.

[12] M. Grundmann, V. Kwatra, M. Han, and I. Essa. Efﬁcient
hierarchical graph-based video segmentation. CVPR, pages
2141–2148, 2010.

[13] G. Heitz and D. Koller. Learning spatial context: Using stuff

to ﬁnd things. In ECCV, pages 30–43. Springer, 2008.

[14] S. Hsu, P. Anandan, and S. Peleg. Accurate computation
of optical ﬂow by using layered motion representations. In
ICPR, pages A:743–746, 1994.

[15] M. Irani and P. Anandan. A uniﬁed approach to moving ob-
ject detection in 2d and 3d scenes. PAMI, 20(6):577–589,
1998.

[16] P. Isola and C. Liu. Scene collaging: Analysis and synthe-
sis of natural images with semantic layers. In CVPR, pages
3048–3055, 2015.

[17] A. Jepson and M. J. Black. Mixture models for optical ﬂow

computation. In CVPR, pages 760–761, 1993.

[18] A. Jepson, D. Fleet, and M. Black. A layered motion rep-
resentation with occlusion and compact spatial support. In
ECCV, volume I, pages 692–706, 2002.

[19] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Gir-
shick, S. Guadarrama, and T. Darrell. Caffe: Convolu-
tional architecture for fast feature embedding. arXiv preprint
arXiv:1408.5093, 2014.

[20] N. Jojic and B. Frey. Learning ﬂexible sprites in video layers.

In CVPR, pages I:199–206, 2001.

[21] S. Ju, M. J. Black, and A. D. Jepson. Skin and bones:
Multi-layer, locally afﬁne, optical ﬂow and regularization
with transparency. In CVPR, pages 307–314, June 1996.

[22] P. Kr¨ahenb¨uhl and V. Koltun. Efﬁcient inference in fully con-
nected CRFs with gaussian edge potentials. In NIPS, pages
109–117, 2011.

[23] P. Kr¨ahenb¨uhl and V. Koltun. Efﬁcient nonlocal regulariza-

tion for optical ﬂow. In ECCV, pages I:356–369, 2012.

[24] M. Kumar, P. Torr, and A. Zisserman. Learning layered mo-
IJCV, 76(3):301–319, March

tion segmentations of video.
2008.

[25] C. Lalos, H. Grabner, L. Van Gool, and T. Varvarigou. Object
ﬂow: Learning object displacement. In ACCV, pages 133–
142, 2011.

[26] V. Lempitsky, S. Roth, and C. Rother. Fusionﬂow: Discrete-
In

continuous optimization for optical ﬂow estimation.
CVPR, pages 1–8, June 2008.

[27] B. Liu and X. He. Multiclass semantic video segmentation
In CVPR, pages 4286–

with object-level active inference.
4294, 2015.

[28] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional
networks for semantic segmentation. CVPR, pages 3431–
3440, Nov. 2015.

[29] O. Mac Aodha, A. Humayun, M. Pollefeys, and G. J. Bros-
tow. Learning a conﬁdence measure for optical ﬂow. PAMI,
35(5):1107–1120, May 2013.

[30] S. Mallat. A wavelet tour of signal processing: the sparse

way. Academic press, 2008.

[31] E. M´emin and P. P´erez. Dense estimation and object-based
segmentation of the optical ﬂow with robust techniques.
IEEE Trans. Im. Proc., 7(5):703–719, May 1988.

[32] M. Menze and A. Geiger. Object scene ﬂow for autonomous

vehicles. In CVPR, pages 3061–3070, 2015.

[33] M. Menze, C. Heipke, and A. Geiger. Discrete optimization
for optical ﬂow. In German Conference on Pattern Recogni-
tion (GCPR), volume 9358, pages 16–28. Springer Interna-
tional Publishing, 2015.

[34] R. Mottaghi, X. Chen, X. Liu, N.-G. Cho, S.-W. Lee, S. Fi-
dler, R. Urtasun, and A. Yuille. The role of context for object
detection and semantic segmentation in the wild. In CVPR,
2014.

[35] D. W. Murray and B. F. Buxton. Scene segmentation from
visual motion using global optimization. PAMI, 9(2):220–
228, March 1987.

[36] P. Ochs, J. Malik, and T. Brox. Segmentation of moving ob-
jects by long term video analysis. PAMI, 36(6):1187–1200,
June 2014.

[37] X. Ren, T. Han, and Z. He. Ensemble video object cut in
highly dynamic scenes. In CVPR, pages 1947–1954, 2013.
Very deep convolu-
tional networks for large-scale image recognition. CoRR,
abs/1409.1556, 2014.

[38] K. Simonyan and A. Zisserman.

9

[39] A. N. Stein and M. Hebert. Occlusion boundaries from mo-
IJCV,

tion: Low-level detection and mid-level reasoning.
82(3):325–357, May 2009.

[40] D. Sun, C. Liu, and H. Pﬁster. Local layering for joint motion
estimation and occlusion detection. In CVPR, pages 1098–
1105, 2014.

[41] D. Sun, E. B. Sudderth, and M. J. Black. Layered segmenta-
tion and optical ﬂow estimation over time. In CVPR, pages
1768–1775, 2012.

[42] D. Sun, J. Wulff, E. Sudderth, H. Pﬁster, and M. Black.
A fully-connected layered model of foreground and back-
ground ﬂow. In CVPR, pages 2451–2458, June 2013.

[43] P. Sundberg, T. Brox, M. Maire, P. Arbelaez, and J. Malik.
Occlusion boundary detection and ﬁgure/ground assignment
from optical ﬂow. In CVPR, pages 2233–2240, 2011.

[44] B. Taylor, A. Ayvaci, A. Ravichandran, and S. Soatto. Se-
mantic video segmentation from occlusion relations within
a convex optimization framework. In EMMCVPR, volume
8081, pages 195–208. Springer Berlin Heidelberg, 2013.

[45] W. B. Thompson. Exploiting discontinuities in optical ﬂow.

IJCV, 30(3):163–173, Dec. 1998.

[46] C. Wang, M. de La Gorce, and N. Paragios. Segmentation,
ordering and multi-object tracking using graphical models.
In ICCV, pages 747–754, 2009.

[47] J. Y. A. Wang and E. H. Adelson. Representing moving im-

ages with layers. IEEE Trans. IP, 3(5):625–638, 1994.

[48] Y. Weiss. Smoothness in layers: Motion segmentation using
nonparametric mixture estimation. In CVPR, pages 520–526,
1997.

[49] J. Wulff and M. J. Black. Efﬁcient sparse-to-dense optical
ﬂow estimation using a learned basis and layers. In CVPR,
pages 120–130, 2015.

[50] C. Xu, C. Xiong, and J. Corso. Streaming hierarchical
video segmentation. In ECCV, volume 7577, pages 626–639.
Springer Berlin Heidelberg, 2012.

[51] K. Yamaguchi, D. McAllester, and R. Urtasun. Robust
monocular epipolar ﬂow estimation. CVPR, 0:1862–1869,
2013.

[52] J. Yang and H. Li. Dense, accurate optical ﬂow estimation
In CVPR, pages 1019–

with piecewise parametric model.
1027, June 2015.

[53] Y. Zhou and H. Tao. A background layer model for ob-
ject tracking through occlusion. In ICCV, pages 1079–1085,
2003.

10

