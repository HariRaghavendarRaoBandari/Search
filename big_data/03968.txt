6
1
0
2

 
r
a

 

M
2
1

 
 
]

V
C
.
s
c
[
 
 

1
v
8
6
9
3
0

.

3
0
6
1
:
v
i
X
r
a

Temporally Robust Global Motion

Compensation by Keypoint-based Congealing

S. Morteza Safdarnejad, Yousef Atoum, Xiaoming Liu

Michigan State University

Abstract. Global motion compensation (GMC) removes the impact of
camera motion and creates a video in which the background appears
static over the progression of time. Various vision problems, such as hu-
man activity recognition, background reconstruction, and multi-object
tracking can beneﬁt from GMC. Existing GMC algorithms rely on se-
quentially processing consecutive frames, by estimating the transforma-
tion mapping the two frames, and obtaining a composite transformation
to a global motion compensated coordinate. Sequential GMC suﬀers from
temporal drift of frames from the accurate global coordinate, due to ei-
ther error accumulation or sporadic failures of motion estimation at a
few frames. We propose a temporally robust global motion compensation
(TRGMC) algorithm which performs accurate and stable GMC, despite
complicated and long-term camera motion. TRGMC densely connects
pairs of frames, by matching local keypoints of each frame. A joint align-
ment of these frames is formulated as a novel keypoint-based congealing
problem, where the transformation of each frame is updated iteratively,
such that the spatial coordinates for the start and end points of matched
keypoints are identical. Experimental results demonstrate that TRGMC
has superior performance in a wide range of scenarios.

Keywords: Global motion compensation, Congealing, Motion panorama

1 Introduction

Global motion compensation (GMC) removes the impact of intentional and
unwanted camera motion in the video, transfroming the video to have static
background with the only motion coming from foreground objects. As a related
problem, video stabilization removes unwanted camera motion, such as vibration,
and generates a video with a smooth camera motion. The term “global motion
compensation” is also used in video coding literature, where background motion
is estimated roughly to enhance the video compression performance [1, 2].

GMC is an essential module for processing videos from non-stationary cam-
eras, which are abundant due to emerging mobile sensors, e.g., wearable cameras,
smartphones, and camera drones. First, the resultant motion panorama [3], as
if virtually generated by a static camera, is by itself appealing for visual per-
ception. More importantly, many vision tasks beneﬁt from GMC. For instance,
dense trajectories [4] are shown to be superior when camera motion is compen-
sated [5]. Otherwise, camera motion interferes with human motion, rendering the

2

S. Morteza Safdarnejad et al.

Fig. 1. Schematic diagrams of proposed TRGMC and existing sequential GMC algo-
rithms, and resultant motion panorama for a video shot by panning the camera up and
down. Background continuity breaks easily in the case of the sequential GMC [10].

analysis problem very challenging. GMC allows reconstruction of a “stitched”
background [6], and subsequently segmentation of foreground [7, 8]. This helps
multi-object tracking by mitigating the unconstrained problem of tracking mul-
tiple in-the-wild objects, to tracking objects with a static background [9].

In existing GMC work [10–12], frames are transformed to a global motion-
compensated coordinate (GMCC), by sequentially processing input frames. For
a pair of consecutive frames, the mapping transformation is estimated, and by
accumulating the transformations, a composite global transformation of each
frame to the GMCC is obtained. However, the sequential processing scheme
causes frequent GMC failures for multiple reasons: 1) Sequential GMC is only
as strong as the weakest pair of consecutive frames. A single frame with high
blur or dominant foreground motion can cause the rest of the video to fail. 2)
Generally, multiple planes exist in the scene. The common assumption of a sin-
gle homography will accumulate residual errors into remarkable errors. 3) Even
if the error of consecutive frames is in a sub-pixel scale, due to the multiplica-
tion of several homography matrices, the error can be signiﬁcant over time [6].
These problems are especially severe when processing long videos and/or the
camera motion becomes more complicated. E.g., when the camera pans to left
and right repeatedly, or severe camera vibration exists, the GMC error is obvious
by exhibiting discontinuity on the background (see Fig. 1 for an example).

To address the issues of sequential GMC, we propose a temporally robust
global motion compensation (TRGMC) algorithm which by joint alignment of
input frames, estimates accurate and temporally consistent transformations to
GMCC. The result can be rendered as a motion panorama that maintains percep-
tual realism despite complicated camera motion (Fig. 1). TRGMC densely con-
nects pairs of frames, by matching local keypoints. Joint alignment (a.k.a. con-
gealing) of these frames is formulated as an optimization problem where the
transformation of each frame is updated iteratively, such that for each link in-
terconnecting a keypoint pair, the spatial coordinates of two end points are iden-

TRGMCSequential GMCtytyTemporally Robust Global Motion Compensation

3

tical. This novel keypoint-based congealing, built upon succinct keypoint coordi-
nates instead of high-dimensional appearance features, is the core of TRGMC.
Joint alignment not only leads to the temporal consistency of GMC, but also
improves GMC stability by using redundancy of the information. The improved
stability is crucial for GMC, especially in the presence of considerable fore-
ground motion, motion blur, non-rigid motion like water, or low-texture back-
ground. The joint alignment scheme also provides capabilities such as coarse-
to-ﬁne alignment, i.e., alignment of the keyframes followed by non-keyframes,
and appropriate weighting of keypoints matches, which cannot be naturally in-
tegrated in sequential GMC. Our quantitative experiments reveal that TRGMC
pushes the alignment error close to human performance.

In summary, this work makes the following contributions:

– An algorithm for joint alignment of video frames is proposed to produce a
globally motion compensated video where, despite the complicated camera
movement and considerable foreground motion, the background appears to
be static over the progression of time.

– A keypoint-based congealing algorithm aligns the spatial coordinates of key-
points for an image stack. It extends congealing applications from spatially
cropped objects (faces and letters) to complex motion-rich video frames.

– The capabilities and applications of TGRMC are demonstrated. Our col-
lected video dataset, manual labels, and the code will be publicly available.

2 Prior Work

TRGMC is related to many techniques in diﬀerent aspects. We ﬁrst review them
and then compare our work with existing GMC algorithms.

Firstly, homography estimation from keypoint matches is crucial to many
vision tasks, e.g., image stitching, registration, and GMC. Its main challenge is
the false matches due to appearance ambiguities. Methods are proposed to either
be robust to outliers, such as RANSAC [13–16] and reject false matches [17, 18],
or probabilistically combine appearance similarities and keypoint matches [10,
19]. All methods estimate a homography for a frame pair. In contrast, we jointly
estimate homographies of all frames to a global coordinate, which leverages the
redundant background matches over time to better handle outliers.

Image stitching (IS) and panoramic image mosaicing share similarity with
GMC. IS aims to minimize the distortions and ghosting artifact in the over-
lap region. Recent works focus on diﬀerent challenges, e.g., multi-plane scenes
[20–25], the parallax issue [26–28], and motion blur [29]. In these works, input
images have much less overlap than GMC. On the other hand, video mosaicing
takes in a video which raster scans a wide angle static scene, and produces a sin-
gle static panoramic image [30–32]. When the camera path forms a 2D scan [30]
or a 360◦ rotation [32], global reﬁnement is performed via bundle adjustment
(BA) [33], which ensures an artifact-free panoramic image. Although a byprod-
uct of TRGMC is a similar static reconstruction of the scene, TRGMC focuses on
eﬃcient generation of an appealing video, for a highly dynamic scene. While one
may use BA to estimate camera pose and then transformation between frames,

4

S. Morteza Safdarnejad et al.

our experiments reveal that BA is not reliable for videos with foreground motion
and is far less eﬃcient than TRGMC. Hence, image/video mosaicing and GMC
have diﬀerent application scenarios and challenges.

Another related topic is the panoramic video [34–38]. For instance, Perazzi et
al. [35] create a panoramic video from an array of stationary cameras by general-
izing parallax-tolerant image stitching to video stitching. While these works focus
on stitching multiple synchronized videos, GMC creates a motion panorama from
a single non-stationary camera. Unlike GMC, video panoramas do not require
the resultant video to have a stationary background.

Video stabilization (VS) is a closely related but diﬀerent problem. TRGMC
can be re-purposed for VS, but not vice versa, due to the accuracy requirement.
Given the accurate mapping to a global coordinate using TRGMC, VS would
mainly amount to cropping out a smooth sequence of frames and handling ren-
dering issues such as parallax. Among diﬀerent categories of VS, 2D VS methods
calculate consecutive warping between the frames and have similarities with se-
quential GMC, but any estimation error will not cause severe degradation in
VS as long as it is smoothed. While TRGMC targets long-term staticness of the
background, VS mainly cares about smoothing of camera motion, not removing
it. In other words, TRGMC imposes a stronger constraint on the result. This
strict requirement diﬀerentiates TRGMC also from Re-Cinematography [39].

Congealing aims to jointly align a stack of images from one object class,
e.g., faces and letters [40, 41]. Congealing iteratively updates the transforma-
tions of all images such that the entropy [40] or Sum of Squared Diﬀerences
(SSD) [42] of the images, is minimized. However, despite many extensions of con-
gealing [43–47], almost all prior work deﬁne the energy based on the appearance
features of two images. Our experiments on GMC show that appearance-based
congealing is ineﬃcient and sensitive to initialization and foreground motion.
Therefore, we propose a novel keypoint-based congealing algorithm minimizing
the SSD of corresponding keypoint coordinates. Further, most prior works apply
to a spatially cropped object such as faces, while we deal with complex video
frames with dynamic foreground and moving background, at a higher spatial-
temporal resolution. Note that [44] uses a heuristic local feature based algorithm
to rigidly align object class images. In contrast we formulate the joint alignment
of keypoints as an optimization problem and solve it in a principal way.

There are a few existing sequential GMC works, where the main problem is to
accurately estimate a homography transformation between consecutive frames,
given challenges such as appearance ambiguities, multi-plane scene, and dom-
inant foreground [3, 10, 12]. Bartoli et al. [11] ﬁrst estimate an approximate 4-
degree-of-freedom homography, and then reﬁne it. Sakamoto et al. [32] generate a
360◦ panorama from an image sequence. Assuming a 5-degree-of-freedom homog-
raphy, all the homographies are optimized jointly to prevent error accumulation.
In contrast, TRGMC employs an 8-degree-of-freedom homography. Although us-
ing homography in the case of considerable camera translation and large depth
variation results in parallax artifacts, using a higher degrees-of-freedom homog-
raphy than prior works allows TRGMC to better handle camera panning, zoom-

Temporally Robust Global Motion Compensation

5

Fig. 2. Flowchart of the TRGMC algorithm.

ing, and translation. Safdarnejad et al. [10] incorporate edge matching into a
probabilistic framework that scores candidate homographies. Although [10, 12]
improve the robustness to foreground, error accumulation and failure in a single
frame pair still deteriorate the overall performance. Thus, TRGMC targets ro-
bustness of the GMC in terms of both the presence of foreground and long-term
consistency by joint alignment of frames.

3 Proposed TRGMC Algorithm

The core of TRGMC is the novel keypoint-based congealing algorithm. Our
method relies on densely interconnecting the input frames, regardless of their
temporal oﬀset, by matching the detected SURF [48] keypoints at each frame.
We refer to these connections, shown in Fig. 2, as links. Frames are initialized to
their approximate spatial location by only 2D translation (Sec. 3.4). We rectify
the keypoints such that majority of the links have end points on the background
region. Then the congealing applies appropriate transformation to each frame
and the links connected to it, such that the spatial coordinates of the end-points
of each link are as similar as possible. In Fig. 2, this translates to having the
links as parallel to the t−axis as possible.

For eﬃciency and robustness, TRGMC processes an input video in two
stages. Stage one selects and jointly aligns a set of keyframes. The keyframes are
frozen, and then stage two aligns each remaining frame to its two encompassing
keyframes. The remainder of this section presents the details of the algorithm.

3.1 Formulation of keypoint-based congealing
Given a stack of N frames {I(i)}, with indices i ∈ K = {k1, ..., kN}, the keypoint-
based congealing is formulated as an optimization problem,

(cid:88)

i∈K

min{pi}  =

(cid:124)
[ei(pi)]

Ω(i)[ei(pi)],

(1)

where pi is the transformation parameter from frame i to GMCC, ei(pi) collects
the pair-wise alignment errors of frame i relative to all the other frames in the
stack, and Ω(i) is a weight matrix.

We deﬁne the alignment error of frame i as the SSD between the spatial
coordinates of the endpoints of all links connecting frame i to the other frames,
instead of the SSD of appearance [42]. Speciﬁcally, as shown in Fig. 3, we denote
coordinates of the start and the end point of each link k connecting frame i to
the frame d(i)
k ), respectively. For simplicity,
we omit the frame index i in pi. Thus, the error ei(p) is deﬁned as,

k ∈ K\{i} as (x(i)

k , v(i)

k , y(i)

k ) and (u(i)

. . . Input FramesInitializationIterative UpdatesDense InterconnectionAligned KeyframesIndependent Alignment of Non-keyframesPruning the Links. . . Average of framesAverage of framestSec. 3.5Sec. 3.4Sec. 3.2Sec. 3.66

S. Morteza Safdarnejad et al.

Fig. 3. The notation used in TRGMC.

(cid:124)
ei(p) = [∆xi(p)

(cid:124)
, ∆yi(p)

(cid:124)
]

(3)

(2)

where

k , y(i)

∆xi(p) = w(i)

,
x − u(i), ∆yi(p) = w(i)
y − v(i),
x = [Wx(x(i)
are the errors in x− and y− axes. The vectors w(i)
k ; p)] and
k ; p)] denote the x and y− coordinates of (x(i)
y = [Wy(x(i)
w(i)
k ) warped
k ] and v(i) = [v(i)
by the parameter p, respectively. The vectors u(i) = [u(i)
k ]
denote the coordinates of the end points. Similarly, the vectors x(i) = [x(i)
k ] and
y(i) = [y(i)
k ] denote the coordinates of the start points. If Ni links emanate from
frame i, ei is a 2Ni−dim vector. Ω(i) is a diagonal matrix of size 2Ni × 2Ni
which assigns a weight to each element in ei. The parameter p has 2, 6, or 8
elements for the cases of 2D translation, aﬃne transformation, or homography,
respectively. In this paper, we focus on homography transformation which is a
projective warp model, parameterized as,

k , y(i)
k , y(i)

Wx(x(i)

k , y(i)
Wy(x(i)
k , y(i)
1

k ; p)
k ; p)

 =

p

(cid:122)
(cid:125)(cid:124)
p1 p2 p3

p4 p5 p6
p7 p8 1

(cid:123)


 .

x(i)

k
y(i)
k
1

(4)

Although the homography model assumes the planar scene and this assump-
tion may be violated in real world [27], we identify the problem of temporal
robustness to be more fundamental for GMC than the inaccuracies due to a sin-
gle homography. Also, videos for GMC are generally swiped through the scene
with high overlap, thus the discontinuity resulted from this assumption is minor.

3.2 Optimization solution
Equation 1 is a non-linear optimization problem and diﬃcult to minimize. Fol-
lowing [42], we linearize this equation by taking the ﬁrst-order Taylor expansion
around p. Starting from an initial p, the goal is to estimate ∆p by,

argmin

∆p

[ei(p) +

∂ei(p)

∂p

(cid:124)
∆p]

Ω(i)[ei(p) +

∂ei(p)

∂p

∆p] + γ∆p

(cid:124)I∆p,

(5)

(cid:124)I∆p is a regularization term, with a positive constant γ setting
where ∆p
the trade-oﬀ. We observe that without this regularization, parameter estima-
tion may lead to distortion of the frames. The indicator matrix I is a diag-
onal matrix specifying which elements of ∆p need a constraint. We use I =
diag([1, 1, 0, 1, 1, 0, 1, 1]) to specify that there is no constraint on the translation
parameters of the homography, but the rest of parameters should remain small.

(cid:124)
∂ei(p)

∆p = H−1

R

HR =

∂ei(p)

∂p

Ω(i)ei(p),

+ γI.

(cid:124)

∂p
Ω(i) ∂ei(p)
∂p
∂W ∂W
∂p = ∂ei(p)
(cid:105)

(cid:104) 1Ni 0Ni

(6)

(7)

(cid:21)

Temporally Robust Global Motion Compensation

7

By setting the ﬁrst-order derivative of Eqn. 5 to zero, the solution for ∆p is,

Using the chain rule, we have ∂ei(p)

∂p . Knowing that the mapping
has two components as W = (Wx,Wy), and the ﬁrst half of ei only contains x
components and the rest only y components, we have,

(8)
where 1Ni and 0Ni are Ni−dim vectors with all element being 1 and 0, respec-
tively. For homography transformation, ∂W
∂(p1,p2,p3,p4,p5,p6,p7,p8) is given by,

∂(Wx,Wy)

∂p =

0Ni 1Ni

,

∂ei(p)
∂W =

x w(i)

y 1Ni 0Ni 0Ni 0Ni −u(i)w(i)
y 1Ni −v(i)w(i)

x w(i)

x −u(i)w(i)
x −v(i)w(i)

0Ni 0Ni 0Ni w(i)

y

y

.

(9)

(cid:20) w(i)

∂W
∂p

=

At each iteration, and for each frame i, ∆p is calculated and the start points
of all the links emanating from frame i are updated accordingly. Similarly, for
all links with end points on frame i, the end point coordinates are updated. 1

We use the SURF [48] algorithm for keypoint detection with a low detection
threshold, τs = 200, to ensure suﬃcient keypoints are detected even for low-
texture backgrounds. We use the nearest-neighbor ratio method [49] to match
keypoints and form links between each pair of keyframes.
Keyframe selection We select keyframes at a constant step of ∆f , i.e., from
every ∆f frames, only one is selected. Based on the experimental results, as a
trade-oﬀ between accuracy and eﬃciency, we use ∆f = 10 in TRGMC.

3.3 Weight assignment
We have deﬁned all parameters in the problem formulation, except the weights of
links, Ω(i). We consider two factors in setting Ω(i). Firstly, the keypoints detected
at larger scales are more likely to be from background matches, since they cover
coarser information and larger image patches. Thus, to be robust to foreground,
the early iterations should emphasize links from larger-scale keypoints, which
forms a coarse-to-ﬁne alignment. We normalize the scales of all keypoints such
that the maximum is 1, and denote the minimum of the normalized scales of the
two keypoints comprising the link k as sk. Then, Ω(i)
k,k is set proportional to sk.
Secondly, for each frame i, the links may be made either to all the previ-
ous frames, denoted as backward scheme, or both the previous and upcoming
frames, denoted as backward-forward scheme. The former is for potential real
time application, whereas the latter for oﬄine video processing. These schemes
are implemented by assigning diﬀerent weights to backward and forward links,

1

In algorithm implementation, it is important to store the original coordinates of
the detected keypoints and apply the composite transformations accumulated in all
the iterations to update the coordinates of the start and end points of the links.
Otherwise, accumulation of numerical errors will harm the performance.

8

S. Morteza Safdarnejad et al.

(a)

(b)

(a)

(b)

Fig. 4. Comparison of the ratios of
background-foreground matches for
(a) sequential GMC and (b) TRGMC.

Fig. 5. (a) The input frame, (b) the reliabil-
ity map, with the red color showing higher
reliability.

(cid:40)

Ω(i)

k,k =

(β.sk)rq
(α.sk)rq

; if d(i)
; if d(i)

k < i
k > i

(Backward links)

(Forward links)

(10)

where 0 < α, β < 1, q is the iteration index, and 0 < r < 1 is the rate of change
of the weights. Note that the alignment errors in x and y−axes have the same
weights, i.e., Ω(i)
k,k. After a few iterations, the weights of all the
links will be restored to 1. In the backward scheme, we set α = 0.

= Ω(i)

k+Ni,k+Ni

3.4

Initialization

Initialization speeds up the alignment and decreases the false keypoint matches.
The objective is to roughly place each frame at the appropriate coordinates in the
GMCC. For initialization, we align the frames based only on rough estimation
of translation without considering rotation, skew, or scale. We use the average of
the motion vectors in matching two consecutive frames as the translation. Using
this simple initializiation, even if the camera has in-plane rotation, estimated 2D
translations are zero, which is indeed correct and does not cause any problem
for TRGMC. Given the estimated translation, approximate overlap area of each
pair of frames is calculated, and only the keypoints inside the overlap area are
matched, reducing number of false matches due to appearance ambiguities.

3.5 Outlier handling

Links may become outliers for two reasons: (i) the keypoints reside on foreground
objects not consistent with camera motion; (ii) false links between diﬀerent phys-
ical locations are caused by the low detection threshold and similar appearances.
In order to prune the outliers, we assume that the motion vectors of back-
ground matches, i.e., background links, have consistent and smooth patterns,
caused by camera motion such as pan, zoom, tilt, whereas, the outlier links will
exhibit arbitrary pattern, inconsistent with the background pattern. Speciﬁcally,
we use Ma et al. [17] method to prune outlier links by imposing a smoothness
constraint on the motion vector ﬁeld2. This method outperforms RANSAC if the
set of keypoint matches contains a large proportion of outliers. Since keyframes
have larger relative time diﬀerence than consecutive frames, the foreground mo-
tion is accentuated and more distinguishable from camera motion. This helps
with better pruning of the foreground links. At each stage that the keypoints
from a pair of frames are matched to form the links, we perform the pruning.

2 We use the implementation provided by the authors and default parameters.

1-11-11-01-0.....▪ ▪ ▪ =4-210-3.....▪ ▪ ▪ Temporally Robust Global Motion Compensation

9

Congealing of an image stack also increases the proportion of background
matches over the outliers - another way to suppress outliers. The keypoints on
background are more likely to form longer range matches than the foreground

ones, due to non-rigid foreground motion. Hence, when(cid:0)N

(cid:1) combinatorial pairs

of frames are interconnected, there are a lot more background matches (Fig. 4).

2

3.6 Alignment of non-keyframes
The keyframes alignment provides a set of temporally consistent motion com-
pensated frames, which are the basis for aligning non-keyframes. We refer to
keyframes and non-keyframes with superscripts i and j, respectively. For a non-
keyframe j between the keyframes ki and ki+1, its alignment is a special case of
Eqn. 1, with indices K = {j}, and the destination of the links d(j)
k ∈ {ki, ki+1},
i.e., only pj of frame j is updated while the keyframes remain ﬁxed. Each non-
keyframe between keyframes ki and ki+1 is aligned independently.

However, given the small time oﬀset between j and d(j)

k , the observed fore-
ground motion may be hard to discern. Also, frame j is linked only to two
keyframes, thus there is no redundancy of background information to improve
robustness to foreground motion. Therefore, we need a diﬀerent means of outlier
handling. We handle this issue by assigning higher weights to links that are more
likely to be connected to the background.

For each keyframe i, we quantify how well the links emanating from frame
k =

i are aligned with other keyframes. If the alignment error is small, i.e., (i)

(cid:12)(cid:12) < τ , the link k is more likely

(cid:12)(cid:12) +(cid:12)(cid:12)Wy(x(i)

(cid:12)(cid:12)Wx(x(i)

k , y(i)

k ; p) − u(i)

k

k , y(i)

k ; p) − v(i)

k

on the background of frame i and thus, more reliable for aligning non-keyframes.
We create a reliability map for each keyframe i, denoted as R(i) (Fig. 5). For
each link k with (i)
k < τ , a Gaussian function with µk = (x(i)
k ) and σk = csk
is superposed on R(i), where the constant c is 20. We deﬁne,

k , y(i)

(cid:0)

−

e

(cid:38)(cid:22)(cid:88)

k∈Bi

R(i)

m,n =

(cid:1)2

(cid:0)

+
2σ2
k

(cid:1)2

(cid:23)

(cid:39)

m−x

(i)
k

n−y
(i)
k

,

1

η

(11)

where Bi = {k|(i)
k < τ}, η > 0 is a small constant (set to 0.1), (cid:100)x(cid:101)η = max(x, η)
and (cid:98)x(cid:99)1 = min(1, η). Now, we assign the weight of the links connecting frame
j to the keyframe d(j)
k ), as the reliability map of the
, where a = d(j)
keyframe at the endpoint, Ω(j)
k .

k,k =(cid:0)R(a)

k at the coordinate (u(j)

k , v(j)

(cid:1)rq

u(j)
k ,v(j)

k

We summarize the TRGMC algorithm in Algorithm 1.

4 Experimental Results and Applications

We now present qualitative and quantitative results of the TRGMC algorithm
and discuss how diﬀerent computer vision applications will beneﬁt from TRGMC.

4.1 Experiments and results
Baselines and details We choose three sequential GMC algorithms as the
baselines for comparison: MLESAC [15] and HEASK [19] both based on our own

10

S. Morteza Safdarnejad et al.

m=1

Algorithm 1: TRGMC Algorithm
Data: A set of input frames {I(m)}M
Result: A set of homography matrices {pm}M
/* Align keyframes (Sec. 3.2)
1 Specify K = {k1, ..., kN} and initialize (Sec. 3.4);
2 Match keypoints of all frames i ∈ K densely;
3 Prune links (Sec. 3.5) and set weights (Eqn. 10);
4 Store links’ start and end coordinates in (xi, yi) and (ui, vi);
5 repeat
6

forall the i ∈ K do

m=1

Compute ∆pi (Eqn. 6), update pi, xi and yi ;
Update (um, vm) according to pi for m ∈ K\{i};
Update weights (Eqn. 10);

*/

*/

7

8

9

14

15

16

17

18

19

20

21

10

q ← q + 1;

11 until q < T1 or(cid:0) 1

(cid:80)
i∈K ||∆pi||2 > τ1
/* Align non-keyframes (Sec. 3.6)
12 Compute reliability map R(i) for i ∈ K ;
13 for i = 1 : N − 1 do

(cid:1);

N

forall the j ∈ {ki + 1, ..., ki+1 − 1} do

Match keypoints in j with d(j) ∈ {ki, ki+1} ;
Prune links (Sec. 3.5) and set weights Ω(j)
k,k;
Store links’ coordinates in (xj, yj) and (uj, vj);
repeat

Compute ∆pj (Eqn. 6), update pj, xj and yj;
Update weights (Eqn. 10), q ← q + 1;

until q < T2 or(cid:0)||∆pj||2 > τ2

(cid:1);

implementation, and RGMC [10] based on the authors’ Matlab code available
online. We implement TRGMC in Matlab, and will publish the code. Denoting
the video frames of w × h pixels, we set the parameters as γ = 0.1wh, T1 = 300,
τ1 = 5 × 10−4, T2 = 50, τ2 = 10−4, r = 0.7, τ = 1, ∆f = 10, and β = 1. For the
backward-forward scheme we set α = 1 and for the backward scheme α = 0.
Datasets and metric Given there is no public dataset for quantitative GMC
evaluation, we form a dataset composed of 40 challenging videos from SVW [50]
and 15 videos from UCF101 [51], termed “quantitative dataset”. SVW is an
extremely unconstrained dataset including videos of amateurs practicing sports,
and is also captured by amateurs via smartphone. The min. and max. spatial
size of videos are 240 and 480 pixels, respectively. The average, min., and max.
length of the videos are 14, 3, and 45 seconds, captured at 25 or 30 FPS. In
addition, we form another “qualitative dataset” with 200 unlabeled videos from
SVW, in challenging categories of boxing, diving, and hockey.

To compare GMC over diﬀerent temporal distances of frames, for each video
of length M in the quantitative dataset, we manually align all 10 possible pairs
from the 5-frame set, F = {1, 0.25M, 0.5M, 0.75M, M}, as long as they are over-

Temporally Robust Global Motion Compensation

11

Algorithm

MLESAC HEASK RGMC

Setting

Avg. BRE

Eﬃciency (s/f)

–

0.116
0.17

–

0.110
7.47

–

0.097
3.47

TRGMC
B*

GT*

BF*
0.058 0.060 0.038
0.64

0.41

–

–

Table 1. Comparison of GMC algorithms on quantitative dataset (*GT: Ground truth,
BF: Backward-Forward, B: Backward).

Fig. 6. Average BRE of frame pairs versus the time diﬀerence between the two frames.

(a)

(b)

(c)

(d)

Fig. 7. Top view of the frames and links (a) before and (b) after TRGMC. The parallel
links in (b) show successful spatial alignment of keypoints. Average of frames (c) before
and (d) after TRGMC. For better visibility, we show up to 15 links emanated per frame.

lapping, and specify the background regions. For this, a GUI is developed for
a labeler to match 4 points on each frame pair, and ﬁne tune them up to a
half-pixel accuracy, until the background diﬀerence is minimized. Then, the la-
beler selects the foreground regions which subsequently identify the background
region. Similar to [10], we quantify the consistency of two warped frames I(i)(pi)
and I(j)(pj) (0 to 1 grayscale pixels) via the background region error (BRE),

1

(cid:107)|(I(i)(pi) − I(j)(pj))| (cid:12) MB(cid:107)1,

(cid:107)MB(cid:107)1

BRE(i, j) =

(12)
where (cid:12) is element-wise multiplication and MB is the background mask for the
intersection of two warped frames.
Quantitative evaluation Average of BRE over all the temporal frames pairs is
shown in Table 1. TRGMC outperform all the baseline methods with consider-
able margin. The backward-forward (BF) scheme has a slightly better accuracy
than the backward (B) scheme, and is also more stable based on our visual
observation. Thus, we use BF as the default scheme for TRGMC.

To illustrate how the accumulation of errors over time aﬀects the ﬁnal error,
Fig. 6 summarizes the average error versus the time diﬀerence between the frames
in F. This shows that TRGMC error is almost constant over a wide temporal
distance between the frames. Thus, even if a frame is not aligned accurately,
the error is not propagating to all the frames after that. However, in sequential
GMC, the error increases as the time diﬀerence increases.
Qualitative evaluation While quantitative results are comprehensive, the
number of videos is limited by the labeling cost. Thus, we further compare
TRGMC and the best performing baseline, RGMC, on the larger qualitative

020040060050100150200xt020040060050100150200xt12

S. Morteza Safdarnejad et al.

Fig. 8. Composite image formed by overlaying the frame n on frame 1 for several videos
after TRGMC. Left to right, n is equal to 144, 489, 912, 93, respectively. In the overlap
region the diﬀerence between the frames is shown.

Alg. \Perfromance Good Shaking Failed
3%
2%

64% 33%
93% 5%

RGMC
TRGMC

Fig. 9. Error and eﬃciency vs. the
keyframe selection step, ∆f .

Table 2. Comparison of GMC algorithms
on qualitative dataset.

dataset. The resultant motion panoramas were visually investigated and catego-
rized into three cases: good, shaking, and failed (i.e., considerable background
discontinuity). The comparison in Tab. 2 again shows the superiority of TRGMC.
Figure 7 shows the links of a sample video processed by TRGMC, and the
average frames, before and after processing. Initialization module is disable for
generating this ﬁgure to better illustrate how well the spatial coordinate of the
keypoints are aligned, resulting in links parallel to the t− axis. This video also
shows how GMC might be utilized for video stabilization. Figure 8 shows a
composite image formed by overlaying the last frame (or a far apart frame with
enough overlap) on frame 1 for several videos, after TRGMC. In the overlap
region, diﬀerence between the two frames is shown, to demonstrate how well the
background region matches for the frames with large temporal distance.
Computational eﬃciency Table 1 also presents the average time for process-
ing each frame for each method, on a PC with an Intel i5-3470@3.2GHz CPU,
and 8GB RAM. While obtaining considerably better accuracy than HEASK or
RGMC, TRGMC is on average 15 times faster than HEASK and 7 times faster
than RGMC. MLESAC is ∼3 times faster than TRGMC, but with twice the
error. For TRGMC, the backward scheme is 50% faster than forward-backward,
since it has approximately half the links of BF.
Accuracy vs. eﬃciency trade-oﬀ Fig. 9 presents the error and eﬃciency
results for a set of 5 videos versus the keyframe selection step, ∆f . For this
set, the ground truth error is 0.049. As a sweet spot in the error and eﬃciency
trade-oﬀ, we use ∆f = 10 for TRGMC. This ﬁgure also justiﬁes the two stage
processing scheme in TRGMC, as processing frames at a low selection step ∆f ,
is costly in terms of eﬃciency, but only improves the accuracy slightly.

4.2 TRGMC applications

Motion panorama By sequentially reading input frames, applying the trans-
formation found by TRGMC, and overlaying the warped frames on a suﬃciently
large canvas, a motion panorama is generated. Furthermore, it is possible to
reconstruct the background using the warped frames ﬁrst (as will be discussed
later), and overlay the frames on that, to create a more impressive panorama.

24681012141618200.511.5∆ fEfficiency(s/f)24681012141618200.0540.0550.0560.0570.058Avg. BRETemporally Robust Global Motion Compensation

13

Fig. 10. Temporal overlay of frames from diﬀerent videos processed by TRGMC. Tra-
jectory of the center of image plane over time is overlaid on each plot to show the
camera motion pattern, where color changes from blue to red with progression of time.

Fig. 11. Background reconstruction results. Compare the left image with Fig. 10, mid-
dle image with Fig. 7, and right image with Fig. 8.

(a)

(b)

(c)

Fig. 12. (a) Input frame, (b) reconstructed background, (c) diﬀerence of (a,b) on (a).

The last frame on the video generated such, can be referred to as a panoramic
mosaic [52]. Figure 10 shows a few exemplar panoramas along with the camera
2 (p1 + pM ))−1
motion pattern. For all the input videos of length M , we apply ( 1
to the transformations found by TRGMC to normalize the result and have a
better view of the scene in a smaller spatial area.
Raster scan of scenes/Image mosaic We may swipe the camera through a
large scene in a raster scan fashion and use TRGMC to reconstruct a big image
mosaic. Note that this scenario is non-trivial since the accumulated error can be
obvious when the raster scan comes back to the original camera position. The
long term robustness presented by TRGMC is crucial in this scenario.
Background reconstruction Background reconstruction is important for re-
moving occlusions, or detecting foreground [6]. To reconstruct the background,
a weighted average scheme is used to weight each frame by the reliability map,
R(i), which assigns higher weights to background. Since the minimum value of
R(i) is a positive constant η, if no reliable keyframe exists at a coordinate, all the
frames will have equal weights. Speciﬁcally, the background is reconstructed by
, where R(i)(pi) and Ii(pi) are the reliability map and
B =
the input frame warped using the transformation pi. Using our scheme, recon-
structed background in Fig. 11 is sharper and less impacted by the foreground.
Foreground segmentation The reliable background reconstruction result B
along with the GMC result of frame I(i), e.g., pi, can be easily used to segment
the foreground by thresholding the diﬀerence, |B − I(i)(pi)| (Fig. 12).
Human action recognition State of the art human action recognition heavily
relies on analysis of human motion. GMC helps to suppress camera motion and
magnify human motion, making the motion analysis more feasible, which is
clearly shown by the dense trajectories [4] in Fig. 13.

i∈K R(i)(pi)I(i)(pi)

i∈K R(i)(pi)

(cid:80)

(cid:80)

Frame = 7592004006008001000120014001600−200−1000100200Frame = 115100150200250300350400450500−50050100150200250300Frame = 165400600800100012001400−300−200−100010020030040050014

S. Morteza Safdarnejad et al.

(a)

(b)

Fig. 13. Dense trajectories of the (a) original video, and (b) TRGMC-processed video.

Fig. 14. Multi-player tracking using [55] for a football video with camera panning to
the right, before (left) and after processing by TRGMC (right).

Multi-object Tracking (MOT) When appearance cues for tracking are am-
biguous, e.g., tracking players in team sports like football, motion cues gain
extra signiﬁcance [53, 54]. MOT is comprised of two tasks, data association by
assigning each detection a label, and trajectory estimation – both highly aﬀected
by camera motion. TRGMC can be applied to remove camera motion and thus,
revive the power of tracking algorithms relying on motion cues. To verify the
impact of TRGMC, we manually label the locations of all players in 566 frames
of a football video (the one in Fig. 12) and use this ground truth detection re-
sults to study how MOT using [55] beneﬁts from TRGMC. Fig. 14 compares
the trajectories of players over time with and without applying TRGMC. Com-
paring number of label switches, this qualitatively demonstrates improvement
of a challenging MOT scenario using TRGMC. Also, the Multi-Object Tracking
Accuracy (MOTA) [56] achieved for the original video and the video processed
by TRGMC are 63.79% and 84.23%, respectively.

5 Conclusions and Discussions

We proposed a temporally robust global motion compensation (TRGMC) al-
gorithm by joint alignment (congealing) of frames, in contrast to the common
sequential scheme. Despite complicated camera motions, TRGMC can remove
the intentional camera motion, such as pan, as well as unwanted motion due
to vibration on handheld cameras. Experiments demonstrate that TRGMC out-
performs existing GMC methods, and applications of TRGMC.

The enabling assumption of TRGMC is that the camera motion in the direc-
tion of the optical axis is negligible. For instance, TRGMC will not work properly
on a video from a wearable camera of a pedestrian, since in the global coordi-
nate the upcoming frames grow in size and cause computational and rendering
problems. Similar to panorama images, the best results are achieved if the opti-
cal center of the camera has negligible movement during the capturing, making
a homography-based approximation of camera motion appropriate. However, if
the optical center moves in the perpendicular direction to the optical axis (e.g., a
camera following a swimmer), TRGMC still works well, but rendering the results
in the form of motion panorama will be degraded by parallax eﬀect.

02004006000100200300400500xt020040060080010001200140016001800200022000100200300400500xtTemporally Robust Global Motion Compensation

15

References

1. He, Y., Feng, B., Yang, S., Zhong, Y.: Fast global motion estimation for global
In: Proc. IEEE Int. Symp. Circuits and Systems

motion compensation coding.
(ISCAS). Volume 2., IEEE (2001) 233–236

2. Smoli´c, A., Vatis, Y., Schwarz, H., Wiegand, T.: Long-term global motion compen-
sation for advanced video coding. In: ITG-Fachtagung Dortmunder Fernsehsemi-
nar. (2003) 213–216

3. Bartoli, A., Dalal, N., Bose, B., Horaud, R.: From video sequences to motion
panoramas. In: Proc. Conf. Motion and Video Computing Workshops, IEEE (2002)
201–207

4. Wang, H., Klaser, A., Schmid, C., Liu, C.L.: Action recognition by dense trajec-
tories. In: Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR),
IEEE (2011) 3169–3176

5. Wang, H., Schmid, C.: Action recognition with improved trajectories. In: Proc.

Int. Conf. Computer Vision (ICCV), IEEE (2013) 3551–3558

6. Monari, E., Pollok, T.: A real-time image-to-panorama registration approach for
background subtraction using pan-tilt-cameras. In: Proc. IEEE Conf. Advanced
Video and Signal Based Surveillance (AVSS), IEEE (2011) 237–242

7. Sun, Y., Li, B., Yuan, B., Miao, Z., Wan, C.: Better foreground segmentation for
static cameras via new energy form and dynamic graph-cut. In: Proc. Int. Conf.
Pattern Recognition (ICPR). Volume 4., IEEE (2006) 49–52

8. Wan, C., Yuan, B., Miao, Z.: A new algorithm for static camera foreground seg-
mentation via active coutours and GMM. In: Proc. Int. Conf. Pattern Recognition
(ICPR), IEEE (2008) 1–4

9. Solera, F., Calderara, S., Cucchiara, R.: Learning to divide and conquer for online

multi-target tracking. arXiv preprint arXiv:1509.03956 (2015)

10. Safdarnejad, S.M., Liu, X., Udpa, L.: Robust global motion compensation in pres-
ence of predominant foreground. In: Proc. British Mach. Vision Conf. (BMVC).
(2015)

11. Bartoli, A., Dalal, N., Horaud, R.: Motion panoramas. Computer Animation and

Virtual Worlds 15(5) (2004) 501–517

12. D´eniz, O., Bueno, G., Bermejo, E., Sukthankar, R.: Fast and accurate global

motion compensation. Pattern Recognition 44(12) (2011) 2887–2901

13. Fischler, M.A., Bolles, R.C.: Random sample consensus: a paradigm for model
ﬁtting with applications to image analysis and automated cartography. ACM
Commun. 24(6) (1981) 381–395

14. Chum, O., Matas, J., Kittler, J.: Locally optimized RANSAC. Pattern Recognition

(2003) 236–243

15. Torr, P.H., Zisserman, A.: MLESAC: A new robust estimator with application
to estimating image geometry. Computer Vision and Image Understanding 78(1)
(2000) 138–156

16. Tordoﬀ, B.J., Murray, D.W.: Guided-MLESAC: Faster image transform estimation
by using matching priors. IEEE Trans. Pattern Anal. Mach. Intell. 27(10) (2005)
1523–1535

17. Ma, J., Zhao, J., Tian, J., Yuille, A.L., Tu, Z.: Robust point matching via vector

ﬁeld consensus. IEEE Trans. Image Processing 23(4) (2014) 1706–1721

18. Li, X., Hu, Z.: Rejecting mismatches by correspondence function. Int. J. Computer

Vision 89(1) (2010) 1–17

16

S. Morteza Safdarnejad et al.

19. Yan, Q., Xu, Y., Yang, X., Nguyen, T.: HEASK: Robust homography estimation
based on appearance similarity and keypoint correspondences. Pattern Recognition
47(1) (2014) 368–387

20. Szpak, Z.L., Chojnacki, W., van den Hengel, A.: Robust multiple homography
In: Proc. IEEE Conf. Computer Vision and

estimation: An ill-solved problem.
Pattern Recognition (CVPR), IEEE (2015) 2132–2141

21. Zuliani, M., Kenney, C.S., Manjunath, B.: The multiRANSAC algorithm and its
application to detect planar homographies. In: Proc. Int. Conf. Image Processing
(ICIP). Volume 3., IEEE (2005) III–153

22. Toldo, R., Fusiello, A.: Robust multiple structures estimation with j-linkage. In:

Proc. European Conf. Computer Vision (ECCV). Springer (2008) 537–547

23. Ma, J., Chen, J., Ming, D., Tian, J.: A mixture model for robust point matching

under multi-layer motion. PloS one 9(3) (2014) e92282

24. Uemura, H., Ishikawa, S., Mikolajczyk, K.: Feature tracking and motion compen-
sation for action recognition. In: Proc. British Mach. Vision Conf. (BMVC). (2008)
1–10

25. Gao, J., Kim, S.J., Brown, M.S.: Constructing image panoramas using dual-
homography warping. In: Proc. IEEE Conf. Computer Vision and Pattern Recog-
nition (CVPR), IEEE (2011) 49–56

26. Lin, W.Y., Liu, S., Matsushita, Y., Ng, T.T., Cheong, L.F.: Smoothly varying
aﬃne stitching. In: Proc. IEEE Conf. Computer Vision and Pattern Recognition
(CVPR), IEEE (2011) 345–352

27. Zaragoza, J., Chin, T.J., Tran, Q.H., Brown, M.S., Suter, D.: As-projective-as-
possible image stitching with moving dlt. IEEE Trans. Pattern Anal. Mach. Intell.
36(7) (2014) 1285–1298

28. Lin, C.C., Pankanti, S.U., Ramamurthy, K.N., Aravkin, A.Y.: Adaptive as-natural-
as-possible image stitching. In: Proc. IEEE Conf. Computer Vision and Pattern
Recognition (CVPR), IEEE (2015) 1155–1163

29. Li, Y., Kang, S.B., Joshi, N., Seitz, S.M., Huttenlocher, D.P.: Generating sharp
panoramas from motion-blurred videos. In: Proc. IEEE Conf. Computer Vision
and Pattern Recognition (CVPR), IEEE (2010) 2424–2431

30. Sawhney, H.S., Hsu, S., Kumar, R.: Robust video mosaicing through topology
inference and local to global alignment. In: Computer VisionECCV98. Springer
(1998) 103–119

31. Shum, H.Y., Szeliski, R.: Construction and reﬁnement of panoramic mosaics with
global and local alignment. In: Computer Vision, 1998. Sixth International Con-
ference on, IEEE (1998) 953–956

32. Sakamoto, M., Sugaya, Y., Kanatani, K.: Homography optimization for consis-
tent circular panorama generation. In: Advances in Image and Video Technology.
Springer (2006) 1195–1205

33. Triggs, B., McLauchlan, P.F., Hartley, R.I., Fitzgibbon, A.W.: Bundle adjustmenta
modern synthesis. In: Vision algorithms: theory and practice. Springer (1999) 298–
372

34. El-Saban, M., Izz, M., Kaheel, A., Refaat, M.: Improved optimal seam selection
blending for fast video stitching of videos captured from freely moving devices. In:
Proc. Int. Conf. Image Processing (ICIP), IEEE (2011) 1481–1484

35. Perazzi, F., Sorkine-Hornung, A., Zimmer, H., Kaufmann, P., Wang, O., Watson,
S., Gross, M.: Panoramic video from unstructured camera arrays. In: Computer
Graphics Forum. Volume 34., Wiley Online Library (2015) 57–68

36. Zeng, W., Zhang, H.: Depth adaptive video stitching.

In: Proc. IEEE Conf.

Computer and Information Science (ICIS), IEEE (2009) 1100–1105

Temporally Robust Global Motion Compensation

17

37. Jiang, W., Gu, J.: Video stitching with spatial-temporal content-preserving warp-
ing. In: Proc. IEEE Conf. Computer Vision and Pattern Recognition Workshops
(CVPRW), IEEE (2015) 42–48

38. Ibrahim, M.T., Haﬁz, R., Khan, M.M., Cho, Y., Cha, J.: Automatic reference
selection for parametric color correction schemes for panoramic video stitching.
In: Advances in Visual Computing. Springer (2012) 492–501

39. Gleicher, M.L., Liu, F.: Re-cinematography: Improving the camerawork of ca-
sual video. ACM Transactions on Multimedia Computing, Communications, and
Applications (TOMM) 5(1) (2008) 2

40. Learned-Miller, E.G.: Data driven image models through continuous joint align-

ment. IEEE Trans. Pattern Anal. Mach. Intell. 28(2) (2006) 236–250

41. Liu, X., Tong, Y., Wheeler, F.W.: Simultaneous alignment and clustering for an
image ensemble. In: Proc. Int. Conf. Computer Vision (ICCV), IEEE (2009) 1327–
1334

42. Cox, M., Sridharan, S., Lucey, S., Cohn, J.: Least squares congealing for unsuper-
vised alignment of images. In: Proc. IEEE Conf. Computer Vision and Pattern
Recognition (CVPR), IEEE (2008) 1–8

43. Huang, G., Mattar, M., Lee, H., Learned-Miller, E.G.: Learning to align from
scratch. In: Advances in Neural Information Processing Systems (NIPS). (2012)
764–772

44. Lankinen, J., K¨am¨ar¨ainen, J.K.: Local feature based unsupervised alignment of
In: Proc. British Mach. Vision Conf. (BMVC). Volume 1.

object class images.
(2011) 5

45. Lucey, S., Navarathna, R., Ashraf, A.B., Sridharan, S.: Fourier lucas-kanade algo-

rithm. IEEE Trans. Pattern Anal. Mach. Intell. 35(6) (2013) 1383–1396

46. Cox, M., Sridharan, S., Lucey, S., Cohn, J.: Least-squares congealing for large
numbers of images. In: Proc. Int. Conf. Computer Vision (ICCV), IEEE (2009)
1949–1956

47. Shokrollahi Yancheshmeh, F., Chen, K., Kamarainen, J.K.: Unsupervised visual
In: Proc. IEEE Conf. Computer Vision and

alignment with similarity graphs.
Pattern Recognition (CVPR), IEEE (2015) 2901–2908

48. Bay, H., Tuytelaars, T., Van Gool, L.: SURF: Speeded up robust features. In:

Proc. European Conf. Computer Vision (ECCV). Springer (2006) 404–417

49. Lowe, D.G.: Distinctive image features from scale-invariant keypoints.

Int. J.

Computer Vision 60(2) (2004) 91–110

50. Safdarnejad, S.M., Liu, X., Udpa, L., Andrus, B., Wood, J., Craven, D.: Sports
videos in the wild (SVW): A video dataset for sports analysis. In: Proc. Int. Conf.
Automatic Face and Gesture Recognition (FG), IEEE (2015) 1–7

51. Soomro, K., Zamir, A.R., Shah, M.: UCF101: A dataset of 101 human actions

classes from videos in the wild. arXiv preprint arXiv:1212.0402 (2012)

52. Steedly, D., Pal, C., Szeliski, R.: Eﬃciently registering video into panoramic mo-
In: Proc. Int. Conf. Computer Vision (ICCV). Volume 2., IEEE (2005)

saics.
1300–1307

53. Lezama, J., Alahari, K., Sivic, J., Laptev, I.: Track to the future: Spatio-temporal
video segmentation with long-range motion cues. In: Proc. IEEE Conf. Computer
Vision and Pattern Recognition (CVPR), IEEE (2011)

54. Dicle, C., Camps, O., Sznaier, M.: The way they move: tracking multiple targets
In: Proc. Int. Conf. Computer Vision (ICCV), IEEE

with similar appearance.
(2013) 2304–2311

18

S. Morteza Safdarnejad et al.

55. Andriyenko, A., Schindler, K., Roth, S.: Discrete-continuous optimization for
multi-target tracking. In: Proc. IEEE Conf. Computer Vision and Pattern Recog-
nition (CVPR), IEEE (2012) 1926–1933

56. Bernardin, K., Stiefelhagen, R.: Evaluating multiple object tracking performance:

the CLEAR MOT metrics. J. Image and Video Process. 2008 (2008) 1

