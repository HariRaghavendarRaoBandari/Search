6
1
0
2

 
r
a

 

M
2
1

 
 
]
T
S
h
t
a
m

.

[
 
 

1
v
0
3
8
3
0

.

3
0
6
1
:
v
i
X
r
a

HOMOSCEDASTICITY TESTS FOR BOTH LOW AND
HIGH-DIMENSIONAL FIXED DESIGN REGRESSIONS

ZHIDONG BAI

KLASMOE and School of Mathematics and Statistics, Northeast Normal University,

Changchun, P.R.C., 130024.

GUANGMING PAN

School of Physical and Mathematical Sciences, Nanyang Technological University,

Singapore, 637371

YANQING YIN

KLASMOE and School of Mathematics and Statistics, Northeast Normal University,

Changchun, P.R.C., 130024.

Abstract. This paper is to prove the asymptotic normality of a statistic for detecting the
existence of heteroscedasticity for linear regression models without assuming randomness of
covariates when the sample size n tends to inﬁnity and the number of covariates p is either
ﬁxed or tends to inﬁnity. Moreover our approach indicates that its asymptotic normality
holds even without homoscedasticity.

1.1. A brief review of homoscedasticity test. Consider the classical multivariate linear
regression model of p covariates

1. Introduction

(1.1)
where yi is the response variable, xi = (xi,1, xi,2,· · · , xi,p) is the p-dimensional covariates,
β = (β1, β2,· · · , βp)′ is the p dimensional regression coeﬃcient vector and εi is the indepen-
dent random errors obey the same distribution with zero mean and variance σ2
i . In most

i = 1, 2,· · · , n,

yi = xiβ + εi,

E-mail addresses: baizd@nenu.edu.cn, gmpan@ntu.edu.sg, (Corresponding

author)

yinyq799@nenu.edu.cn.

Key words and phrases. Breusch and Pagan test, White’s test, heteroscedasticity, homoscedasticity, high-

dimensional regression, design matrix.

Zhidong Bai is partially supported by a grant NSF China 11571067.
G. M. Pan was partially supported by a MOE Tier 2 grant 2014-T2-2-060 and by a MOE Tier 1 Grant

RG25/14 at the Nanyang Technological University, Singapore.

Yanqing Yin was partially supported by a project of China Scholarship Council.

1

2

HOMOSCEDASTICITY TEST FOR BOTH LOW AND HIGH-DIMENSIONAL REGRESSIONS

applications of the linear regression models the homoscedasticity is a very important as-
sumption. Without it, the loss in eﬃciency in using ordinary least squares (OLS) may be
substantial and even worse, the biases in estimated standard errors may lead to invalid in-
ferences. Thus, it is very important to examine the homoscedasticity. Formally, we need to
test the hypothesis

(1.2)
where σ2 is a positive constant.

H0 : σ2

1 = σ2

2 = · · · = σ2

n = σ2,

In the literature there are a lot of work considering this hypothesis test when the dimension
p is ﬁxed.
Indeed, many popular tests have been proposed. For example Breusch and
Pagan [3] and White [12] proposed statistics to investigate the relationship between the
estimated errors and the covariates in economics. While in statistics, Dette and Munk [6],
Glejser [7], Harrison and McCabe [8], Cook and Weisberg [4], Azzalini and Bowman [1]
proposed nonparametric statistics to conduct the hypothesis. One may refer to Li and Yao
[10] for more details in this regard.

The development of computer science makes it possible for people to collect and deal with
high-dimensional data. As a consequence, high-dimensional linear regression problems are
becoming more and more common due to widely available covariates. Note that the above
mentioned tests are all developed under the low-dimensional framework when the dimension
p is ﬁxed and the sample size n tends to inﬁnity.

In Li and Yao’s paper, they proposed two test statistics in the high dimensional setting
by using the regression residuals. The ﬁrst statistic uses the idea of likelihood ratio and
the second one uses the idea that “the departure of a sequence of numbers from a constant
can be eﬃciently assessed by its coeﬃcient of variation”, which is closely related to John’s
idea [9]. By assuming that the distribution of the covariates is N(0, Ip) and that the error
obey the normal distribution, the “coeﬃcient of variation” statistic turns out to be a function
of residuals. But its asymptotic distribution missed some part as indicated from the proof of
Lemma 1 in [10] even in the random design.

The aim of this paper is to establish central limit theorem for the “coeﬃcient of variation”
statistic without assuming randomness of the covariates by using the information in the
projection matrix (the hat matrix). This ensures that the test works when the design matrix
is both ﬁxed and random. More importantly we prove that the asymptotic normality of this
statistics holds even without homoscedasticity. That assures a high power of this test.

The structure of this paper is as follows. Section 2 is to give our main theorem and some
simulation results, as well as two real data analysis. Some calculations and the proof of the
asymptotic normality are presented in Section 3.

2. Main Theorem, Simulation Results and Real Data Analysis

2.1. The Main Theorem. Suppose that the parameter vector β is estimated by the OLS
estimator

ˆβ = (X′X)−1 X′Y.

Denote the residuals by

ˆε = ( ˆε1, ˆε2,· · · , ˆεn)′ = Y − X ˆβ = Pε,

with P = (pij)n×n = In − X(X′X)−1X′ and ε = (ε1, ε2,· · · , εn)′. Let D be an n × n
diagonal matrix with its i-th diagonal entry being σi, set A = (aij)n×n = PD and let
ξ = (ξ1, ξ2,· · · , ξn)′ stand for a standard n dimensional random vector whose entries obey

HOMOSCEDASTICITY TEST FOR BOTH LOW AND HIGH-DIMENSIONAL REGRESSIONS

3

(2.1)

Consider the following statistic

the same distribution with ε. It follows that the distribution of ˆε is the same as that of Aξ.
In the following, we use Diag (B) = (b1,1, b2,2,· · · , bn,n)′ to stand for the vector formed by the
diagonal entries of B and Diag′ (B) as its transpose, use DB stand for the diagonal matrix
of B, and use 1 stand for the vector (1, 1,· · · , 1)′.
i=1(cid:0) ˆεi
nPn
2 − 1
2(cid:1)2
n(cid:0)Pn

We below use A ◦ B to denote the Hadamard product of two matrices A and B and use A◦k
to denote the Hadamard product of k A.
Theorem 2.1. Under the condition that the distribution of ε1 is symmetric, E|ε1|8 ≤ ∞ and
p/n → y ∈ [0, 1) as n → ∞, we have

T = Pn

2(cid:1)2

i=1 ˆεi

i=1 ˆεi

.

1

where a, b are determined by n, p and A. Under H0, we further have

T − a
√b

d−→ N(0, 1)
a =  n (3tr (P ◦ P) + ν4tr(P ◦ P)2)

(cid:0)(n − p)2 + 2 (n − p) + ν4tr(P ◦ P)(cid:1) − 1! , b = ∆′Θ∆,

where

∆′ = (

and

where

(2.2)

(2.3)

(2.4)

n2 (3tr (P ◦ P) + ν4tr(P ◦ P)2)

(cid:0)(n − p)2 + 2 (n − p) + ν4tr(P ◦ P)(cid:1)2 )

n

(cid:0)(n − p)2 + 2 (n − p) + ν4tr(P ◦ P)(cid:1) ,−
Θ =(cid:18) Θ11 Θ12
Θ21 Θ22 (cid:19) ,
Θ11 =72Diag′(P) (P ◦ P) Diag(P) + 24tr (P ◦ P)2

+ ν2

+ ν4(cid:0)96trPDPPP◦3 + 72tr(P ◦ P)3 + 36Diag′(P) (P ◦ P)2 Diag(P)(cid:1)
4(cid:0)18tr(P ◦ P)4 + 16tr(P◦3P)2)(cid:1)
+ ν6(cid:0)12tr(cid:0)(PDPP) ◦(cid:0)P◦2P◦2(cid:1)(cid:1) + 16trPP◦3P◦3(cid:1) + ν81′(P◦4P◦4)1,

Θ22 =

8 (n − p)3 + 4ν4 (n − p)2 tr(P ◦ P)

n2

,

Θ12 = Θ21
(n − p)

=

n

(cid:0)24tr (P ◦ P) + 16ν4tr(PP◦3) + 12ν4tr ((PDpP) ◦ P) + 2ν6[Diag(P)′(P◦4)1](cid:1) ,

ν4 = M4 − 3 , ν6 = M6 − 15M4 + 30 and ν8 = M8 − 28M6 − 35M 2

corresponding cumulants of random variable ε1.

4 + 420M4 − 630 are the

4

HOMOSCEDASTICITY TEST FOR BOTH LOW AND HIGH-DIMENSIONAL REGRESSIONS

Remark 2.2. The existence of the 8-th moment is necessary because it determines the as-
ymptotic variance of the statistic.

Remark 2.3. The explicit expressions of a and b are given in Theorem 2.1 under H0. How-
ever the explicit expressions of a and b are quite complicated under H1. Nevertheless one
may obtain them from (3.10)-(3.39) and (3.12)-(3.37) below.

Remark 2.4. In Li and Yao’s paper, under the condition that the distribution of ε is nor-
mal, they also did some simulations when the design matrices are non-Gaussian. Speciﬁcally
speaking, they also investigated the test when the entries of design matrices are drawn from
gamma distribution G(2, 2) and uniform distribution U(0, 1) respectively. There is no sig-
niﬁcant diﬀerence in terms of size and power between these two non-normal designs and the
normal design. This seems that the proposed test is robust against the form of the distribution
of the design matrix. But according to our main theorem, it is not always the case. In our
main theorem, one can ﬁnd that when the error ε obey the normal distribution, under H0 and
given p and n, the expectation of the statistics is only determined by tr(P ◦ P). We conduct
some simulations to investigate the inﬂuence of the distribution of the design matrix on this
term when n = 1000 and p = 200. The simulation results are presented in table 1. It suggests

N(0, 1) G(2, 2) U(0, 1) F (1, 2) exp(N(5, 3))/100
640.3

640.7

640.2

712.5

708.3

tr(P ◦ P)

Table 1. The value of tr(P ◦ P) corresponding to diﬀerent design distributions

that even if the entries of the design matrix are drawn from some common distribution, the
expectation of the statistics may deviate far from that of the normal case. This will cause a
wrong test result. Moreover, even in the normal case, our result is more accurate since we
do not use any approximate value in the mean of the statistic T .

Remark 2.5. Let’s take an example to explain why this test works. For convenient, suppose
that ε1 obey the normal distribution. From the calculation in Section 3.2 we know that the
expectation of the statistic T deﬁned in (2.1) can be represented as

ET =

i=1 p2
i=1 piiσ2

iiσ4
i
i )2 − 1 + o(1).

3nPn
(Pn

Now assume that pii = n−p
n for all i = 1,· · · , n. Moreover, without loss of generality, suppose
that σ1 = · · · = σn = 1 under H0 so that we get ET → 2 as n → ∞. However, when
σ1 = · · · = σ[n/2] = 1 and σ[n/2]+1 = · · · = σn = 2, one may obtain ET → 3.08 as n → ∞.
Since Var(T) = O(n−1) this ensures a high power as long as n is large enough.

2.2. Some simulation results. We next conduct some simulation results to investigate
the performance of our test statistics. Firstly, we consider the condition when the random
error obey the normal distribution. Table 2 shows the empirical size compared with Li and
Yao’s result in [10] under four diﬀerent design distributions. We use “CVT” and “FCVT”
to represent their test and our test respectively. The entries of design matrices are i.i.d
random samples generated from N(0, 1), t(1) (t distribution with freedom degree 1), F (3, 2)
(F distribution with parameters 3 and 2) and logarithmic normal distribution respectively.
The sample size n is 512 and the dimension of covariates varies from 4 to 384. We also
follow [5] and consider the following two models:

HOMOSCEDASTICITY TEST FOR BOTH LOW AND HIGH-DIMENSIONAL REGRESSIONS

5

Model 1: yi = xiβ + εi(1 + xih),

where h = (1, 0(p−1)),

Model 2: yi = xiβ + εi(1 + xih),

where h = (1(p/2), 0(p/2)).

i = 1, 2,· · · , n,
i = 1, 2,· · · , n

Tables 3 and 4 show the empirical power compared with Li and Yao’s results under four
diﬀerent regressors distributions mentioned above.

Then, we consider the condition that the random error obey the two-point distribution.
Speciﬁcally speaking, we suppose P (ε1 = −1) = P (ε1 = 1) = 1/2. Since Li and Yao’s result
is unapplicable in this situation, Table 5 just shows the empirical size and empirical power
under Model 2 of our test under four diﬀerent regressors distributions mentioned above.

According to the simulation result, it is showed that when p/n → [0, 1) as n → ∞, our

test always has good size and power under all regressors distributions.

e(N (5,3))

t(1)

F (3, 2)

N(0,1)
FCVT CVT FCVT CVT FCVT CVT FCVT CVT
p
0.0594
0.0582 0.0531 0.0600 0.0603 0.0594 0.0597
4
0.0803
0.0621 0.0567 0.0585 0.0805 0.0585 0.0824
16
0.2348
64
0.0574 0.0515 0.0605 0.2245 0.0586 0.2312
128 0.0597 0.0551 0.0597 0.5586 0.0568 0.5779
0.5934
0.9933
256 0.0551 0.0515 0.0620 0.9868 0.0576 0.9908
384 0.0580 0.0556 0.0595 1.0000 0.0600 1.0000
1.0000

0.0590
0.0595
0.0578
0.0590
0.0595
0.0600
Table 2. empirical size under diﬀerent distributions

e(N (5,3))

t(1)

F (3, 2)

N(0,1)
p
FCVT CVT FCVT CVT FCVT CVT FCVT CVT
4
1.0000
1.0000 1.0000 1.0000 1.0000 1.0000 1.0000
1.0000
16
1.0000 1.0000 1.0000 1.0000 1.0000 1.0000
64
1.0000 1.0000 1.0000 1.0000 1.0000 1.0000
1.0000
1.0000
128 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000
1.0000
256 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000
384 0.8113 0.8072 0.9875 1.0000 0.9876 1.0000
1.0000

1.0000
1.0000
1.0000
1.0000
1.0000
0.9905

Table 3. empirical power under model 1

2.3. Two Real Rata Analysis.

2.3.1. The Death Rate Data Set. In [11], the authors ﬁtted a multiple linear regression of the
total age adjusted mortality rate on 15 other variables (the average annual precipitation, the
average January temperature, the average July temperature, the size of the population older
than 65, the number of members per household, the number of years of schooling for persons
over 22, the number of households with fully equipped kitchens, the population per square
mile, the size of the nonwhite population, the number of oﬃce workers, the number of families
with an income less than $3000, the hydrocarbon pollution index, the nitric oxide pollution in-
dex, the sulfur dioxide pollution index and the degree of atmospheric moisture). The number
of observations is 60. To investigate whether the homoscedasticity assumption in this models

6

HOMOSCEDASTICITY TEST FOR BOTH LOW AND HIGH-DIMENSIONAL REGRESSIONS

is justiﬁed, we applied our test and got a p-value of 0.4994, which strongly supported the
assumption of constant variability in this model since we use the one side test. The data set is
available at http://people.sc.fsu.edu/~jburkardt/datasets/regression/regression.html.

2.3.2. The 30-Year Conventional Mortgage Rate Data Set. The 30-Year Conventional Mort-
gage Rate data [13] contains the economic data information of USA from 01/04/1980 to
02/04/2000 on a weekly basis (1049 samples). The goal is to predict the 30-Year Conven-
tional Mortgage Rate by other 15 features . We used a multiple linear regression to ﬁt this
data set and got a good result. The adjusted R-squared is 0.9986, the P value of the overall
F-test is 0. Our homoscedasticity test reported a p-value 0.4439.

3. Proof Of The Main Theorem

This section is to prove the main theorem. The ﬁrst step is to establish the asymptotic
normality of T1, T2 and αT1 + βT2 with α2 + β2 6= 0 by the moment convergence theorem.
Next we will calculate the expectations, variances and covariance of the statistics T1 =
. The main theorem then follows by the delta method. Note

4 and T2 = 1

i=1 ˆεi

that without loss of generality, under H0, we can assume that σ = 1.

i=1 ˆεi

n(cid:0)Pn

2(cid:1)2

Pn

3.1. The asymptotic normality of the statistics. We start by giving a deﬁnition in
Graph Theory.

Deﬁnition 3.1. A graph G = (V, E, F) is called two-edge connected, if removing any one
edge from G, the resulting subgraph is still connected.

The next lemma is a fundamental theorem for Graph-Associated Multiple Matrices without

the proof. For the details of this theorem, one can refer to the section A.4.2 in [2].

e(N (5,3))

t(1)

F (3, 2)

N(0,1)
FCVT CVT FCVT CVT FCVT CVT FCVT CVT
p
1.0000
1.0000 1.0000 1.0000 1.0000 1.0000 1.0000
4
1.0000 1.0000 1.0000 1.0000 1.0000 1.0000
1.0000
16
1.0000
64
1.0000 1.0000 1.0000 1.0000 1.0000 1.0000
1.0000
128 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000
256 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000
1.0000
1.0000
384 0.9066 0.9034 0.9799 1.0000 0.9445 1.0000

1.0000
1.0000
1.0000
1.0000
1.0000
0.8883

Table 4. empirical power under model 2

F (3, 2)

N(0,1)

Size

t(1)
Size

Size

Power

Power

Power
p
0.0695 1.0000 0.0726 1.0000 0.0726 1.0000
4
0.0695 1.0000 0.0638 1.0000 0.0706 1.0000
16
64
0.0646 1.0000 0.0606 1.0000 0.0649 1.0000
128 0.0617 1.0000 0.0705 1.0000 0.0597 1.0000
256 0.0684 1.0000 0.0685 1.0000 0.0608 1.0000
384 0.0610 0.8529 0.0748 1.0000 0.0758 1.0000

e(N (5,3))

Size

0.0664
0.0556
0.0622
0.0630
0.0649
0.0742

Power
1.0000
1.0000
1.0000
1.0000
1.0000
1.0000

Table 5. empirical size and power under diﬀerent distributions

HOMOSCEDASTICITY TEST FOR BOTH LOW AND HIGH-DIMENSIONAL REGRESSIONS

7

Lemma 3.2. Suppose that G = (V, E, F) is a two-edge connected graph with t vertices
and k edges. Each vertex i corresponds to an integer mi ≥ 2 and each edge ej corre-

sponds to a matrix T(j) = (cid:16)t(j)

α,β(cid:17) , j = 1,· · · , k, with consistent dimensions, that is, if

F (ej) = (fi(ej), fe(ej)) = (g, h), then the matrix T(j) has dimensions mg × mh. Deﬁne
v = (v1, v2,· · · , vt) and

(3.1)

where the summationPv is taken for vi = 1, 2,· · · , mi, i = 1, 2,· · · , t. Then for any i ≤ t,

we have

T ′ =Xv

kYj=1

t(j)
vfi(ej ),vfe(ej )

,

|T ′| ≤ mi

kT(j)k.

kYj=1

Let T = (T(1),· · · , T(k)) and deﬁne G(T ) = (G,T ) as a Graph-Associated Multiple Ma-
trices. Write T ′ = sum(G(T )), which is referred to as the summation of the corresponding
Graph-Associated Multiple Matrices.

We also need the following truncation lemma

Lemma 3.3. Suppose that ξn = (ξ1,· · · , ξn) is an i.i.d sequence with E|ξ1|r ≤ ∞, then there
exists a sequence of positive numbers (η1,· · · , ηn) satisfy that as n → ∞, ηn → 0 and

where bξn =(cid:0)ξ1I(|ξ1| ≤ ηnn1/r),· · · , ξnI(|ξn| ≤ ηnn1/r)(cid:1) . And the convergence rate of ηn can

be slower than any preassigned rate.
Proof. E|ξ1|r ≤ ∞ indicated that for any ǫ > 0, we have

P (ξn 6=bξn, i.o.) = 0,

Then there exists a sequence of positive numbers ǫ = (ǫ1,· · · , ǫm) such that

∞Xm=1
∞Xm=1

22mP (|ξ1| ≥ ǫ22m/r) ≤ ∞.

22mP (|ξ1| ≥ ǫm22m/r) ≤ ∞,

and ǫm → 0 as m → 0. And the convergence rate of ǫm can be slower than any preassigned
rate.

Now, deﬁne δn = 21/rǫm for 22m−1 ≤ n ≤ 22m, we have as n → ∞

(3.2)

P (ξn 6=bξn, i.o.) ≤ lim

k→∞

≤ lim

k→∞

≤ lim

k→∞

∞Xm=k
∞Xm=k
∞Xm=k

P(cid:16) [22m−1≤n≤22m
P(cid:16) [22m−1≤n≤22m
P(cid:16) [22m−1≤n≤22m

n[i=1(cid:0)|ξi| ≥ ηnn1/r(cid:1)(cid:17)
22m[i=1(cid:16)|ξi| ≥ ǫm21/r2
22m[i=1(cid:0)|ξi| ≥ ǫm22m/r(cid:1)(cid:17)

(2m−1)

r (cid:17)(cid:17)

8

HOMOSCEDASTICITY TEST FOR BOTH LOW AND HIGH-DIMENSIONAL REGRESSIONS

= lim
k→∞

≤ lim

k→∞

∞Xm=k
∞Xm=k

P(cid:16) 22m[i=1(cid:0)|ξi| ≥ ǫm22m/r(cid:1)(cid:17)
22mP(cid:16)|ξ1| ≥ ǫm22m/r(cid:17) = 0.

(cid:3)

We note that the truncation will neither change the symmetry of the distribution of ξ1 nor

change the order of the variance of T.

Now, we come to the proof of the asymptotic normality of the statistics. We below give
the proof of the asymptotic normality of αT1 + βT2 , where α2 + β2 6= 0. The asymptotic
normality of either T1 or T2 is a result of setting α = 0 or β = 0 respectively.

Below is devote to calculating the moments of T0 = αT1+βT2−(αµ1+βµ2)

.
Note that by Lemma 3.3, we can assume that ξ1 is truncated at ηnn1/8. Then we have for

S

i=1 ˆε2

i )2 and S =pVar (αT1 + βT2).

= α(T1−µ1)+β(T2−µ2)

S

Denote µ1 = ET1 = EPn

i=1 ˆε4

large enough n and l > 4,

i , µ2 = ET2 = En−1 (Pn
M2l ≤ ηnM8√n

2l/4−1

.

We next construct two type of graphs for the last two sums.

represents the random variable ai,jtξjt and the graph G1(i, j) represents Q4

For given integers i, j1, j2, j3, j4 ∈ [1, n], draw a graph as follows: draw two parallel lines,
called the I-line and the J-line respectively; plot i on the I-line and j1, j2, j3 and j4 on the
J-line; ﬁnally, we draw four edges from i to jt, t = 1, 2, 3, 4 marked with 1(cid:13). Each edge (i, jt)
ρ=1 ai,jρξjρ. For
any given integer k1, we draw k1 such graphs between the I-line and the J-line denoted by
G1(τ ) = G1(iτ , jτ ), and write G(1,k1) = ∪τ G1(τ ).
For given integers u1, u2, v1, v2, v3, v4 ∈ [1, n], draw a graph as follows: plot u1 and u2 on the
I-line and v1, v2, v3 and v4 on the J-line; then, we draw two edges from u1 to v1 and v2 marked
with 2(cid:13) , draw two edges from u2 to v3 and v4 marked with 2(cid:13). Each edge (ul, vt) represents
the random variable aul,vtξvt and the graph G2(u, v) represents au1,v1au1,v2au2,v3au2,v4. For
any given integer k2, we draw k2 such graphs between the I-line and the J-line denoted by
G2(ψ) = G2(uψ, vψ), and write G(2,k2) = ∪ψG2(ψ), Gk = G(1,k1) ∪ G(2,k2). Then the k-th
order moment of T0 is

M ′

k =S−k Xk1+k2=k(cid:18) k

k1(cid:19)αk1βk2 X{i1,j1,··· ,ik1 ,jk1 }

{u1,v1,··· ,uk2 ,vk2 }

Let’s take a look at the random variable

(3.3)

αT1 + βT2 = α

aijξj!4

nXi=1  nXj=1

+ (n−1)β

nXi=1  nXj=1
ai,j1ai,j2ai,j3ai,j4ξj1ξj2ξj3ξj4 + (n−1)β Xi1,i2,j1,··· ,j4
ai,j1ai,j2ai,j3ai,j4ξj1ξj2ξj3ξj4 + (n−1)β Xu1,u2,v1,··· ,v4

2

aijξj!2

=α Xi,j1,··· ,j4
=α Xi,j1,··· ,j4

ai1,j1ai1,j2ai2,j3ai2,j4ξj1ξj2ξj3ξj4

au1,v1au1,v2au2,v3au2,v4ξv1ξv2ξv3ξv4.

HOMOSCEDASTICITY TEST FOR BOTH LOW AND HIGH-DIMENSIONAL REGRESSIONS

9

n−k2Eh k1Yτ =1

[G1(iτ , jτ ) − E(G1(iτ , jτ ))]

[G2(uψ, vψ) − E(G2(uψ, vψ))]i.
k2Yφ=1

We ﬁrst consider a graph Gk for the given set of integers k1, k2, i1, j1,· · · , ik1, jk1 and
u1, v1,· · · , uk2, vk2. We have the following simple observations: Firstly, if Gk contains a j
vertex of odd degree, then the term is zero because odd-ordered moments of random variable
ξj are 0. Secondly, if there is a subgraph G1(τ ) or G2(ψ) that does not have an j vertex
coinciding with any j vertices of other subgraphs, the term is also 0 because G1(τ ) or G2(ψ)
is independent of the remainder subgraphs.

Then, upon these two observations, we split the summation of non-zero terms in M ′

k into
a sum of partial sums in accordance of isomorphic classes (two graphs are called isomorphic
if one can be obtained from the other by a permutation of (1, 2,· · · , n), and all the graphs
are classiﬁed into isomorphic classes. For convenience, we shall choose one graph from an
isomorphic class as the canonical graph of that class). That is, we may write

where

M ′

k1(cid:19)αk1βk2n−k2XG′
k =S−k Xk1+k2=k(cid:18) k
= XGk∈G′

EGk.

MG′

k

k

k

MG′

k

,

Here G′
to G′
k.

k is a canonical graph andPGk∈G′

k

denotes the summation for all graphs Gk isomorphic

In that follows, we need the fact that the variances of T1 and T2 and their covariance are

all of order n. This will be proved in Section 3.3.

Since all of the vertices in the non-zero canonical graphs have even degrees, every connected
component of them is a circle, of course a two-edge connected graph. For a given isomorphic
class with canonical graph G′
the number of connected components of the
canonical graph G′
k. For every connected component G0 that has l non-coincident J-vertices
with degrees d1,· · · , dl, let d′ = max{d1 − 8,· · · , dl − 8, 0}, denote T = (A,· · · , A
) and

k, denote by cG′

k

k

t=1 Mdt(cid:17) sum(G(T )) =

deﬁne G0(T ) = (G0,T ) as a Graph-Associated Multiple Matrices. By Lemma 3.2 we then

conclude that the contribution of this canonical class is at most(cid:16)Ql
n n√nd′/4). Noticing that ηn → 0, if cG′

is less than k/2 + k2, then the contribution
k has a factor of n−k2.
k by the argument above and

O(ηd′
of this canonical class is negligible because Sk ≍ nk/2 and MG′
is at most [k/2] + k2 for every G′
However one can see that cG′
noticing that every G2(•) has two i vertices. Therefore, M ′
k when k = 2s. We shall say that the given set of in-
tegers i1, j1,· · · , ik1, jk1 and u1, v1,· · · , uk2, vk2 (or equivalent the graph Gk) satisﬁes the
condition c(s1, s2, s3) if in the graph Gk plotted by this set of integers there are 2s1 G1(•)
connected pairwisely, 2s2 G2(•) connected pairwisely and s3 G1(•) connected with s3 G2(•),
where 2s1 + s3 = k1, 2s2 + s3 = k2 and s1 + s2 + s3 = s, say G1(2τ − 1) connects G1(2τ ),
τ = 1, 2,· · · , s1, G2(2ψ − 1) connects G1(2ψ), ψ = 1, 2,· · · , s2 and G1(2s1 + ϕ) connects
G2(2s2 + ϕ), ϕ = 1, 2,· · · , s3, and there are no other connections between subgraphs. Then,

Now we consider the limit of M ′

k → 0 if k is odd.

in M ′

k

k

|

Pl

t=1 dt

{z

}

10

HOMOSCEDASTICITY TEST FOR BOTH LOW AND HIGH-DIMENSIONAL REGRESSIONS

for any Gk satisfying c(s1, s2, s3), we have

(3.4)

EGk =

s1Yτ =1
s2Yψ=1
s3Yϕ=1

E[(G1(2τ − 1) − E(G1(2τ − 1)))(G1(2τ ) − E(G1 (2τ )))]×

E[(G2(2ψ − 1) − E(G2(2ψ − 1)))(G2(2ψ) − E(G2 (2ψ)))]×

E[(G1(2s1 + ϕ) − E(G1(2s1 + ϕ)))(G2(2s2 + ϕ) − E(G2 (2s2 + ϕ)))].

Now, we compare

(3.5)

with

(3.6)

E[(G1(2τ − 1) − E(G1(2τ − 1)))(G1(2τ ) − E(G1 (2τ )))]×

E[(G1(2s1 + ϕ) − E(G1(2s1 + ϕ)))(G2(2s2 + ϕ) − E(G2 (2s2 + ϕ)))],

EGk

s1Yτ =1

E[(G2(2ψ − 1) − E(G2(2ψ − 1)))(G2(2ψ) − E(G2 (2ψ)))]×

n−k2 XGk∈c(s1,s2,s3)
=n−k2 XGk∈c(s1,s2,s3)
s2Yψ=1
s3Yϕ=1
(cid:0)E (T1 − µ1)2(cid:1)s1(cid:0)E (T2 − µ2)2(cid:1)s2 (E (T1 − µ1) (T2 − µ2))s3
=n−k2XGk
s2Yψ=1
s3Yϕ=1

E[(G2(2ψ − 1) − E(G2(2ψ − 1)))(G2(2ψ) − E(G2 (2ψ)))]×

s1Yτ =1

E[(G1(2τ − 1) − E(G1(2τ − 1)))(G1(2τ ) − E(G1 (2τ )))]×

E[(G1(2s1 + ϕ) − E(G1(2s1 + ϕ)))(G2(2s2 + ϕ) − E(G2 (2s2 + ϕ)))],

dition c(s1, s2, s3).

wherePGk∈c(s1,s2,s3) stands for the summation running over all graph Gk satisfying the con-

If Gk satisﬁes the two observations mentioned before, then EGk = 0, which does not
appear in both expressions; if Gk satisﬁes the condition c(s1, s2, s3), then the two expressions
both contain EGk. Therefore, the second expression contains more terms that Gk have more
connections among subgraphs than the condition c(s1, s2, s3). Therefore, by Lemma 3.2,
(3.7)

(cid:0)E (T1 − µ1)2(cid:1)s1(cid:0)E (T2 − µ2)2(cid:1)s2 (E (T1 − µ1) (T2 − µ2))s3 = n−k2 XGk∈c(s1,s2,s3)
s3(cid:1)(2s1 − 1)!!(2s2 − 1)!!s3! ways to pairing
s3(cid:1)(cid:0)k2
k2 = 2s2 + s3 and s1 + s2 + s3 = s, we have(cid:0)k1

= s+k2, for any nonnegative integers s1, s2, s3 satisfying k1 = 2s1 +s3,

If Gk ∈ G′

EGk + o(Sk).

k with cG′

k

=s+k2

k

=

It follows that

M ′

=s+k2

k

2s1 +s3=k1,2s2+s3=k2

EGk + o(1)

n−k2EGk + o(Sk)

XcG′
Xs1+s2+s3=s
k1(cid:19)αk1βk2n−k2 XcG′
k =S−k Xk1+k2=k(cid:18) k
s3(cid:19) (2s1 − 1)!! (2s2 − 1)!!s3!
s3(cid:19)(cid:18)k2
k1(cid:19)(cid:18)k1
min{k1,k2}Xs3=0 (cid:18)2s
=(cid:16)S−2s
2sXk1=0
(cid:0)α2V ar(T1)(cid:1)s1(cid:0)β2V ar(T2)(cid:1)s2 (αβCov(T1, T2))s3(cid:17) + o(1)
s3 (cid:19)(cid:18)2s2 + s3
2s1 + s3(cid:19)(cid:18)2s1 + s3
=(cid:16)S−2s Xs1+s2+s3=s(cid:18) 2s
(cid:0)α2V ar(T1)(cid:1)s1(cid:0)β2V ar(T2)(cid:1)s2 (αβCov(T1, T2))s3(cid:17) + o(1)
=(cid:16)S−2s Xs1+s2+s3=s
(cid:0)α2V ar(T1)(cid:1)s1(cid:0)β2V ar(T2)(cid:1)s2 (αβCov(T1, T2))s3(cid:17) + o(1)
=(cid:16)S−2s Xs1+s2+s3=s
(cid:0)α2V ar(T1)(cid:1)s1(cid:0)β2V ar(T2)(cid:1)s2 (2αβCov(T1, T2))s3(cid:17) + o(1),

(2s1 + s3)!(2s2 + s3)!s3!(2s1)!s3!(2s2)!

(2s)!(2s1 + s3)!(2s2 + s3)!

(2s − 1)!!

s!

s1!s2!s3!

s3 (cid:19) (2s1 − 1)!! (2s2 − 1)!!s3!

(2s1 − 1)!! (2s2 − 1)!!s3!

HOMOSCEDASTICITY TEST FOR BOTH LOW AND HIGH-DIMENSIONAL REGRESSIONS

11

the subgraphs satisfying the condition c(s1, s2, s3). By (3.7), we then have

s3(cid:19)(cid:18)k2
(cid:18)k1

s3(cid:19)(2s1 − 1)!!(2s2 − 1)!!s3!(V ar(T1))s1(V ar(T2))s2(Cov(T1, T2))s3

which implies that

Combining the arguments above and the moment convergence theorem we conclude that

M ′

2s → (2s − 1)!!.

T1 − ET1
√VarT1

d→ N (0, 1) ,

T2 − ET2
√VarT2

d→ N (0, 1) ,

where α2 + β2 6= 0. Let

Σ =(cid:18) Var(T1)

Cov(T1, T2)

(αT1 + βT2) − E (αT1 + βT2)

pVar (αT1 + βT2)
Var(T2) (cid:19) .

Cov(T1, T2)

d→ N (0, 1) ,

We conclude that Σ−1/2 (T1 − ET1, T2 − ET2)′ is asymptotic two dimensional gaussian vec-
tor.

12

HOMOSCEDASTICITY TEST FOR BOTH LOW AND HIGH-DIMENSIONAL REGRESSIONS

3.2. The expectation. In the following let B = AA′. Recall that

4 =

T1 =

nXi=1 bεi
T2 = n−1
nXi=1  nXj=1

nXi=1  nXj=1
ai,jξj!4
ai,jξj!2

nXi=1 Xj1,j2,j3,j4
= n−1Xi1,i2 Xj1,j2,j3,j4

=

2

ai,j1ai,j2ai,j3ai,j4ξj1ξj2ξj3ξj4,

ai1,j1ai1,j2ai2,j3ai2,j4ξj1ξj2ξj3ξj4.

Since all odd moments of ξ1,· · · , ξn are 0, we know that ET1 and ET2 are only aﬀected by
terms whose multiplicities of distinct values in the sequence (j1,· · · , j4) are all even.
1Tω
2 ). For simplifying notations particularly
in Section 3.3 we introduce the following notations

We need to evaluate the mixed moment E (Tγ

Ω(γ1,γ2,··· ,γt)
{ω1,ω2,··· ,ωs}[(φ1,1,· · · , φ1,s) , (φ2,1,· · · , φ2,s) ,· · · , (φt,1,· · · , φt,s)

]0

where i1,· · · , it and j1,· · · , js run over 1,· · · , n and are subject to the restrictions that
l=1 φl,k = θ.
Intuitively, t is the number of distinct i-indices and s that of distinct j’s; γτ is the multiplicity
l=1 φl,ρ that of jρ; φτ,ρ the multiplicity of the factor aiτ ,jρ; and

l=1 ωl = θ, and for any k = 1,· · · , s, Pt

Ω(γ1,γ2,··· ,γt)
{ω1,ω2,··· ,ωs}[(φ1,1,· · · , φ1,s) , (φ2,1,· · · , φ2,s) ,· · · , (φt,1,· · · , φt,s)

]

}

}

}

t groups

{z

aφτ,ρ
iτ ,jρ,

(3.8)

(3.9)

= Xi1,··· ,it,j16=···6=js Yτ =1,··· ,t Yρ=1,··· ,s
l=1 γl = Ps

θ = 4(γ + ω).

j1,· · · , js are distinct; Pt
of the index iτ and ωρ = Pt
|

Deﬁne

= Xi1,··· ,it,j1,··· ,js Yτ =1,··· ,t Yρ=1,··· ,s

aφτ,ρ
iτ ,jρ.

The deﬁnition above is similar to that of

t groups

{z

t groups

{z

|

|

Ω(γ1,γ2,··· ,γt)
{ω1,ω2,··· ,ωs}[(φ1,1,· · · , φ1,s) , (φ2,1,· · · , φ2,s) ,· · · , (φt,1,· · · , φt,s)

]0

without the restriction that the indices j1,· · · , js are distinct from each other. To help
understand these notations we demonstrate some examples as follows.
i1,j2a2

a2
i1,j1a2

i2,j3a2

Ω(4,4)

{2,2,2,2}[(2, 2, 0, 0), (0, 0, 2, 2)] = Xi1,i2,j1,··· ,j4

i2,j3,

Ω(4,4)

{2,2,2,2}[(2, 1, 1, 0), (0, 1, 1, 2)] = Xi1,i2,j1,··· ,j4
{2,2,2,2}[(2, 2, 0, 0), (0, 0, 2, 2)]0 = Xi1,i2,j16=···6=j4

Ω(4,4)

Ω(4,4)

{2,2,2,2}[(2, 1, 1, 0), (0, 1, 1, 2)]0 = Xi1,i2,j16=···6=j4

a2
i1,j1ai1,j2ai1,j3ai2,j2ai2,j3a2

i2,j4,

a2
i1,j1a2

i1,j2a2

i2,j3a2

i2,j3,

a2
i1,j1ai1,j2ai1,j3ai2,j2ai2,j3a2

i2,j4.

where ν4 = M4 − 3 and

(3.11)

=

C2
4
2!

{2,2}[(2, 0) , (0, 2)] + ν4Ω(4)
Ω(4)

=M4Ω(4)

{4} + M 2
C2
4

=M4Ω(4)

{4} +

2! (cid:16)Ω(4)

2 Ω(4)

{2,2}0

= M4Ω(4)

{4} +

Ω(4)

{2,2}[(2, 0) , (0, 2)]0

C2
4
2!

{4}(cid:17)
{2,2}[(2, 0) , (0, 2)] − Ω(4)
{4} = 3Xi  Xj

+ ν4Xij
a4
ij = 3tr (B ◦ B) + ν4tr(A ◦ A)′(A ◦ A),

i,j!2

a2

a4
ij

b2

2 Ω(2,2)

{4} + M 2

ai1,j1ai1,j2ai2,j3ai2,j4ξj1ξj2ξj3ξj4

i,i + ν4Xij

{2,2}[(2, 0) , (0, 2)]0 + 2Ω(2,2)
{2,2}[(2, 0) , (0, 2)] + 2Ω(2,2)

=3Xi
ET2 = n−1EXi1,i2 Xj1,j2,j3,j4
{2,2}0(cid:17)
=n−1(cid:16)M4Ω(2,2)
{4} +(cid:16)Ω(2,2)
=n−1(cid:16)M4Ω(2,2)
{2,2}[(1, 1) , (1, 1)]0(cid:17)(cid:17)
=n−1(cid:16)M4Ω(2,2)
{4} +(cid:16)Ω(2,2)
{2,2}[(1, 1) , (1, 1)](cid:17) − 3Ω(2,2)
{4} (cid:17)
i2j!
=n−1  Xi1,i2,j1,j2
ai1,j1ai1,j2ai2,j1ai2,j2 + ν4 Xi1,i2,j
=n−1 Xi,j
i2j
i,j!2
ai1,jai2,j!2
+ ν4 Xi1,i2,j
jj
=n−1 Xi,j
i,j!2
nXj=1
=n−1(cid:0)(trB)2 + 2trB2 + ν4tr(B ◦ B)(cid:1) .

i2,j2 + 2 Xi1,i2,j1,j2
+ 2Xi1,i2 Xj
+ 2Xi1,i2

b2
i1,i2 + ν4

a2
i1,j1a2

i1ja2
a2

a2

a2

a2
i1ja2

b2

HOMOSCEDASTICITY TEST FOR BOTH LOW AND HIGH-DIMENSIONAL REGRESSIONS

13

We further use Mk to denote the k-th order moment of the error random variable. We also

use Ck

n to denote the combinatorial number(cid:0)n

k(cid:1). We then obtain

(3.10)

ET1 = E

ai,j1ai,j2ai,j3ai,j4ξj1ξj2ξj3ξj4

nXi=1 Xj1,j2,j3,j4

3.3. The variances and covariance. We are now in the position to calculate the variances
of T1, T2 and their covariance.

First, we have

(3.12)

Var(T1) = E Xi bεi
= Xi1,i2,j1,··· ,j8

4 − E Xi bεi

4!!2

[EG(i1, j1)G(i2, j2) − EG(i1, j1)EG(i2, j2)]

14

HOMOSCEDASTICITY TEST FOR BOTH LOW AND HIGH-DIMENSIONAL REGRESSIONS

= Ω(4,4)

{8} + Ω(4,4)

{2,6}0

+ Ω(4,4)
{4,4}0

+ Ω(4,4)

{2,2,4}0

+ Ω(4,4)

{2,2,2,2}0!,

where the ﬁrst term comes from the graphs in which the 8 J-vertices coincide together; the
second term comes from the graphs in which there are 6 J-vertices coincident and another
two coincident and so on.

Because G(i1, j1) and G(i2, j2) have to connected each other, thus, we have

(3.13)

4C1

3C1

2Ω(4,4)

{2,2,2,2}[(1, 1, 1, 1), (1, 1, 1, 1)]0

{2,2,4}[(2, 1, 1), (0, 1, 3)]0 − Ω(4,4)

{2,2,4}[(2, 0, 2), (0, 2, 2)]0

{4,4}[(2, 2), (2, 2)]0

{2,2,2,2}[(1, 1, 1, 1), (1, 1, 1, 1)] − 6Ω(4,4)

{2,2,4}[(1, 1, 2), (1, 1, 2)]0 − 3Ω(4,4)

{4,4}[(2, 2), (2, 2)]0

2C1
2

Ω(4,4)

=

{2,2,2,2}[(2, 1, 1, 0), (0, 1, 1, 2)]0 + C1

Ω(4,4)
{2,2,2,2}0
4C2
4C1
C2
2!
=72(cid:16)Ω(4,4)
{2,2,2,2}[(2, 1, 1, 0), (0, 1, 1, 2)] − 4Ω(4,4)
− Ω(4,4)
{2,2,4}[(1, 1, 2), (1, 1, 2)]0 − 2Ω(4,4)
{4,4}[(3, 1), (1, 3)]0 − Ω(4,4)
{8} (cid:17)
{2,6}[(1, 3), (1, 3)]0 − 2Ω(4,4)
− 2Ω(4,4)
{2,6}[(2, 2), (0, 4)]0 − Ω(4,4)
+ 24(cid:16)Ω(4,4)
{8} (cid:17).
− 4Ω(4,4)
{2,6}[(1, 3), (1, 3)]0 − Ω(4,4)
=72(cid:16)Ω(4,4)
{2,2,2,2}[(2, 1, 1, 0), (0, 1, 1, 2)] − 4Ω(4,4)
− Ω(4,4)
{2,2,4}[(1, 1, 2), (1, 1, 2)] + 2Ω(4,4)
{2,6}[(1, 3), (1, 3)]0 + 4Ω(4,4)
+ 4Ω(4,4)
+ 24(cid:16)Ω(4,4)
=72(cid:16)Ω(4,4)
− Ω(4,4)
+ 4Ω(4,4)
+ 24(cid:16)Ω(4,4)

{2,6}[(1, 3), (1, 3)]0 + 5Ω(4,4)
{2,2,2,2}[(2, 1, 1, 0), (0, 1, 1, 2)] − 4Ω(4,4)
{2,2,4}[(1, 1, 2), (1, 1, 2)] + 2Ω(4,4)
{2,6}[(1, 3), (1, 3)] + 4Ω(4,4)

{2,2,2,2}[(1, 1, 1, 1), (1, 1, 1, 1)] − 6Ω(4,4)

{8} (cid:17)
{2,6}[(2, 2), (0, 4)] − 6Ω(4,4)
{8} (cid:17).
{2,6}[(1, 3), (1, 3)] − 6Ω(4,4)

{2,2,2,2}[(1, 1, 1, 1), (1, 1, 1, 1)] − 6Ω(4,4)

{2,6}[(2, 2), (0, 4)]0 + 5Ω(4,4)

{4,4}[(3, 1), (1, 3)]0 + Ω(4,4)

{8} (cid:17).

{8} (cid:17)

+ 8Ω(4,4)

+ 8Ω(4,4)

Likewise we have

(3.14)

Ω(4,4)

{2,2,4}0

{2,2,4}[(2, 1, 1), (0, 1, 3)] − Ω(4,4)

{2,2,4}[(2, 0, 2), (0, 2, 2)]

{4,4}[(2, 2), (2, 2)]0

{2,2,4}[(1, 1, 2), (1, 1, 2)] + 3Ω(4,4)

{4,4}[(2, 2), (2, 2)]0

{2,2,4}[(2, 1, 1), (0, 1, 3)] − Ω(4,4)
{4,4}[(2, 2), (2, 2)]

{4,4}[(3, 1), (1, 3)] + Ω(4,4)

{2,2,4}[(2, 0, 2), (0, 2, 2)]

{2,2,4}[(1, 1, 2), (1, 1, 2)] + 3Ω(4,4)

{4,4}[(2, 2), (2, 2)]

HOMOSCEDASTICITY TEST FOR BOTH LOW AND HIGH-DIMENSIONAL REGRESSIONS

15

{2,2,4}[(1, 2, 1), (1, 0, 3)]0

=C1

2C3
C2

3M4Ω(4,4)
2C1
2

4C1
4C1
4C2
4C1
2!
=96M4Ω(4,4)
{2,2,4}[(1, 2, 1), (1, 0, 3)]0

M4Ω(4,4)

+

{2,2,4}[(1, 1, 2), (1, 1, 2)]0 + C2

4C2

4(M4 − 1)Ω(4,4)

{2,2,4}[(2, 0, 2), (0, 2, 2)]0

+ 72M4Ω(4,4)

{2,2,4}[(1, 1, 2), (1, 1, 2)]0 + 36(M4 − 1)Ω(4,4)

{2,2,4}[(2, 0, 2), (0, 2, 2)]0,

=96M4Ω(4,4)

{2,2,4}[(1, 2, 1), (1, 0, 3)]

+ 72M4Ω(4,4)
− 96M4Ω(4,4)
− 240M4Ω(4,4)
=96M4Ω(4,4)

{2,2,4}[(1, 1, 2), (1, 1, 2)] + 36(M4 − 1)Ω(4,4)
{4,4}[(3, 1), (1, 3)]0 − (108M4 − 36)Ω(4,4)
{2,6}[(1, 3), (1, 3)]0 − (168M4 − 72)Ω(4,4)

{2,2,4}[(1, 2, 1), (1, 0, 3)]

+ 72M4Ω(4,4)
− 96M4Ω(4,4)
− 240M4Ω(4,4)

{2,2,4}[(1, 1, 2), (1, 1, 2)] + 36(M4 − 1)Ω(4,4)
{4,4}[(3, 1), (1, 3)] − (108M4 − 36)Ω(4,4)
{2,6}[(1, 3), (1, 3)] − (168M4 − 72)Ω(4,4)

(3.15)

Ω(4,4)
{4,4}0

=C1

=16M 2

2C1

4 Ω(4,4)

4C3
4M 2
4 Ω(4,4)
− (34M 2

{4,4}[(3, 1), (1, 3)]0 +
{4,4}[(3, 1), (1, 3)] + 18(M 2
4 − 18)Ω(4,4)
{8} ,

{2,2,4}[(2, 0, 2), (0, 2, 2)]

{4,4}[(2, 2), (2, 2)]0
{2,6}[(2, 2), (0, 4)]0 − (204M4 − 36)Ω(4,4)

{8}

{2,2,4}[(2, 0, 2), (0, 2, 2)]

{4,4}[(2, 2), (2, 2)]
{2,6}[(2, 2), (0, 4)] + (408M4 − 72)Ω(4,4)

{8}

4C2
C2
4
2!

(M 2
4 − 1)Ω(4,4)

4 − 1)Ω(4,4)
{4,4}[(2, 2), (2, 2)]

{4,4}[(2, 2), (2, 2)]0

(3.16)

Ω(4,4)
{2,6}0

2C2

4(M6 − M4)Ω(4,4)

=C1
=12(M6 − M4)Ω(4,4)
− (28M6 − 12M4)Ω(4,4)
{8} .

{2,6}[(2, 2), (0, 4)]0 + C1

4C1
{2,6}[(2, 2), (0, 4)] + 16M6Ω(4,4)

4M6Ω(4,4)

{2,6}[(1, 3), (1, 3)]0

{2,6}[(1, 3), (1, 3)]

and

(3.17)

Ω(4,4)
{8}0

=(M8 − M 2

4 )Ω(4,4)

{8} [(4), (4)].

Combining (3.12), (3.13), (3.14), (3.15), (3.16) and (3.17), we obtain

(3.18)
Var(T1) = 72Ω(4,4)
+ 96(M4 − 3)Ω(4,4)
+ 72(M4 − 3)Ω(4,4)
4 − 6M4 + 9)Ω(4,4)
+ 18(M 2
+ 12(M6 − 15M4 + 30)Ω(4,4)

{2,2,2,2}[(2, 1, 1, 0), (0, 1, 1, 2)] + 24Ω(4,4)
{2,2,4}[(2, 1, 1), (0, 1, 3)] + 36(M4 − 3)Ω(4,4)
{2,2,4}[(1, 1, 2), (1, 1, 2)] + 16(M 2

{2,2,4}[(2, 0, 2), (0, 2, 2)]

4 − 6M4 + 9)Ω(4,4)

{4,4}[(3, 1), (1, 3)]

{2,2,2,2}[(1, 1, 1, 1), (1, 1, 1, 1)]

{4,4}[(2, 2), (2, 2)] + 16(M6 − 15M4 + 30)Ω(4,4)
{2,6}[(2, 2), (0, 4)] + (M8 − 28M6 − 35M 2

{2,6}[(1, 3), (1, 3)]
4 + 420M4 − 630)Ω(4,4)

{8} [(4), (4)],

16

HOMOSCEDASTICITY TEST FOR BOTH LOW AND HIGH-DIMENSIONAL REGRESSIONS

i1,j1ai1,j2ai1,j3ai2,j2ai2,j3a2
a2

i2,j4

ai1,j1ai1,j2ai1,j3ai1,j4ai2,j1ai2,j2ai2,j3ai2,j4

i1,j1ai1,j2ai1,j3ai2,j2a3
a2

i2,j3 = trBDBAA′◦3,

i1,j1a2
a2

i1,j3a2

i2,j2a2

i2,j3

where

(3.19)

(3.20)

(3.21)

(3.22)

(3.23)

(3.24)

(3.25)

(3.26)

(3.27)

and

(3.28)

(3.29)

Ω(4,4)

Ω(4,4)

{2,2,2,2}[(2, 1, 1, 0), (0, 1, 1, 2)] = Xi1,··· ,i2,j1,··· ,j4
= Diag′(B) (B ◦ B) Diag(B),
{2,2,2,2}[(1, 1, 1, 1), (1, 1, 1, 1)] = Xi1,··· ,i2,j1,··· ,j4
= tr (B ◦ B)2 ,
{2,2,4}[(2, 1, 1), (0, 1, 3)] = Xi1,··· ,i2,j1,··· ,j4

Ω(4,4)

Ω(4,4)

Ω(4,4)

Ω(4,4)

Ω(4,4)

a2
i1,j1a2

{2,2,4}[(2, 0, 2), (0, 2, 2)] = Xi1,··· ,i2,j1,··· ,j4
= Diag′(B) (A ◦ A) (A ◦ A)′ Diag(B),
{2,2,4}[(1, 1, 2), (1, 1, 2)] = Xi1,··· ,i2,j1,··· ,j4
= tr(cid:0)(B ◦ B) (A ◦ A) (A ◦ A)′(cid:1) ,
{4,4}[(3, 1), (1, 3)] = Xi1,··· ,i2,j1,··· ,j4
{4,4}[(2, 2), (2, 2)] = Xi1,··· ,i2,j1,··· ,j4
{2,6}[(1, 3), (1, 3)] = Xi1,··· ,i2,j1,··· ,j4
{2,6}[(2, 2), (0, 4)] = Xi1,··· ,i2,j1,··· ,j4
{8} [(4), (4)] = Xi1,··· ,i2,j1,··· ,j4
− E2 Xi bεi

2!2

2!4

a4
i1,j12a4

a2
i1,j1a2

ai1,j1a3

i1,j2a4

i1,j2a2

Ω(4,4)

Ω(4,4)

Var(T2) = n−2E Xi bεi

Ω(4,4)

Using the same procedure, we have

ai1,j1ai1,j2a2

i1,j3ai2,j1ai2,j2a2

i2,j3

i1,j1ai1,j2ai2,j1a3
a3

i2,j1a2

i2,j2 = tr(cid:16)(cid:0)A◦3A′(cid:1)(cid:0)A◦3A′(cid:1)′(cid:17) ,
i2,j2 = tr(cid:0)(A ◦ A) (A ◦ A)′(cid:1)2
i2,j2 = tr(cid:0)BA◦3A′◦3(cid:1) ,
i2,j2 = tr(cid:0)(A′DBA) ◦(cid:0)A′◦2A◦2(cid:1)(cid:1) ,

i1,j2ai2,j1a3

,

i2,j1 = 1′A◦4A′◦41,

HOMOSCEDASTICITY TEST FOR BOTH LOW AND HIGH-DIMENSIONAL REGRESSIONS

17

=n−2 Xi1,··· ,i4,j1,··· ,j8

=n−2(P2,1 + P2,2) + O(1),

ai1,j1ai1,j2ai2,j3ai2,j4ai3,j5ai3,j6ai4,j7ai4,j8 E

8Yt=1

ξjt − E

ξjtE

4Yt=1

ξjt!

8Yt=5

where

(3.30)

(3.31)

(3.32)

Similarly, we have

2C1

2C1

2C1

P2,1 = C1

P2,2 = ν4C1

2 Xi1,··· ,i4,j1,··· ,j4
2 Xi1,··· ,i4,j1,j2,j3
Cov(T1, T2) = n−1E Xi bεi
=n−1 Xi1,··· ,i3,j1,··· ,j8

=n−1(P3.1 + P3,2 + P3,3 + P3,4) + O(1),

i1,j1ai2,j2ai3,j2ai2,j3ai3,j3a2
a2

i4,j4 = 8 (trB)2 trB2,

a2
i1,j1a2

i2,j2a2

i3,j2a2

i4,j3 = 4ν4tr(B′ ◦ B′) (trB)2 ,

4 − E Xi bεi
ai1,j1ai1,j2ai2,j3ai2,j4ai3,j5ai3,j6ai3,j7ai3,j8 E

2!2Xi bεi

2!2Xi bεi
8Yt=1

4

ξjt − E

ξjtE

4Yt=1

ξjt!

8Yt=5

(3.33)

where

(3.34)

(3.35)

(3.36)

(3.37)

P3,1 = C2

4C1

2C1

a2
i1,j1ai2,j2ai2,j3ai3,j2ai3,j3a2

i3,j4 = 24tr(cid:0)B2 ◦ B(cid:1) trB,

P3,2 = ν4C1

4C1

2C1

a2
i1,j1ai2,j2ai3,j2ai2,j3a3

i3,j3 = 16ν4tr(BAA′◦3)trB,

P3,3 = ν4C2

4C1

a2
i1,j1a2

i2,j2a2

i3,j2a2

i3,j3 = 12ν4tr ((A′DBA) ◦ (A′A)) trB,

P3,3 = ν6C1

a2
i1,j1a2

i2,j2a4

i3,j2 = 2ν6[Diag(A′A)′(A′◦4)1]trB,

2 Xi1,··· ,i3,j1,··· ,j4
2 Xi1,··· ,i3,j1,··· ,j3

2 Xi1,··· ,i3,j1,··· ,j3
2 Xi1,i2,i3,j1,j2

We would like to point out that we do not need the assumption that H0 holds up to now.

From now on, in order to simplify the above formulas we assume H0 holds.

Summarizing the calculations above, we obtain under H0

(3.38)

(3.39)

b2

i,i + ν4Xij

ET1 = 3Xi
a4
ij = 3tr (P ◦ P) + ν4tr(P ◦ P)2,
ET2 = n−1(cid:0)(n − p)2 + 2 (n − p) + ν4tr(P ◦ P)(cid:1) ,

18

HOMOSCEDASTICITY TEST FOR BOTH LOW AND HIGH-DIMENSIONAL REGRESSIONS

VarT1 =72Diag′(P) (P ◦ P) Diag(P) + 24tr (P ◦ P)2

+ ν2

+ ν4(cid:0)96trPDPPP◦3 + 72tr(P ◦ P)3 + 36Diag′(P) (P ◦ P)2 Diag(P)(cid:1)
4(cid:0)18tr(P ◦ P)4 + 16tr(P◦3P)2)(cid:1)
+ ν6(cid:0)12tr(cid:0)(PDPP) ◦(cid:0)P◦2P◦2(cid:1)(cid:1) + 16trPP◦3P◦3(cid:1) + ν81′(P◦4P◦4)1,

8 (n − p)3 + 4ν4 (n − p)2 tr(P ◦ P)

+ O(1),

Var(T2) =

n2

(3.40)

(3.41)

and
(3.42)

Cov(T1, T2)
(n − p)

=

n

(cid:0)24tr (P ◦ P) + 16ν4tr(PP◦3) + 12ν4tr ((PDpP) ◦ P) + 2ν6[Diag(P)′(P◦4)1](cid:1) .

3.4. The proof of the main theorem. Deﬁne a function f (x, y) = x
that fx(x, y) = 1
derivative. Since T = T1

y − 1. One may verify
y2 , where fx(x, y) and fy(x, y) are the ﬁrst order partial

y , fy(x, y) = − x

T2 − 1, using the delta method, we have under H0,

ET = f (ET1, ET2) =(cid:18)

3ntr (P ◦ P)

(n − p)2 + 2 (n − p) − 1(cid:19) ,

(3.43)

VarT = (fx(ET1, ET2), fy(ET1, ET2))Σ(fx(ET1, ET2), fy(ET1, ET2))′.

The proof of the main theorem is complete.

References

[1] Adelchi Azzalini and Adrian Bowman. On the use of nonparametric regression for checking linear rela-

tionships. Journal of the Royal Statistical Society. Series B (Methodological), pages 549–557, 1993.

[2] Zhidong Bai and Jack W Silverstein. Spectral analysis of large dimensional random matrices, volume 20.

Springer, 2010.

[3] Trevor S Breusch and Adrian R Pagan. A simple test for heteroscedasticity and random coeﬃcient

variation. Econometrica: Journal of the Econometric Society, pages 1287–1294, 1979.

[4] R Dennis Cook and Sanford Weisberg. Diagnostics for heteroscedasticity in regression. Biometrika,

70(1):1–10, 1983.

[5] Holger Dette and Axel Munk. Testing heteroscedasticity in nonparametric regression. Journal of the

Royal Statistical Society: Series B (Statistical Methodology), 60(4):693–708, 1998.

[6] Holger Dette, Axel Munk, and Thorsten Wagner. Estimating the variance in nonparametric regression-
what is a reasonable choice? Journal of the Royal Statistical Society: Series B (Statistical Methodology),
60(4):751–764, 1998.

[7] Herbert Glejser. A new test for heteroskedasticity. Journal of the American Statistical Association,

64(325):316–323, 1969.

[8] Michael J Harrison and Brendan PM McCabe. A test for heteroscedasticity based on ordinary least

squares residuals. Journal of the American Statistical Association, 74(366a):494–499, 1979.

[9] S John. Some optimal multivariate tests. Biometrika, 58(1):123–127, 1971.
[10] Zhaoyuan Li and Jianfeng Yao. Homoscedasticity tests valid in both low and high-dimensional regres-

sions. arXiv preprint arXiv:1510.00097, 2015.

[11] Gary C McDonald and Richard C Schwing. Instabilities of regression estimates relating air pollution to

mortality. Technometrics, 15(3):463–481, 1973.

HOMOSCEDASTICITY TEST FOR BOTH LOW AND HIGH-DIMENSIONAL REGRESSIONS

19

[12] Halbert White. A heteroskedasticity-consistent covariance matrix estimator and a direct test for het-

eroskedasticity. Econometrica: Journal of the Econometric Society, pages 817–838, 1980.

[13] H.Altay Guvenir

and

I.Uysal. Bilkent University Function Approximation Repository.

http://funapp.cs.bilkent.edu.tr , 2000

