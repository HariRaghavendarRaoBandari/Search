6
1
0
2

 
r
a

 

M
7
1

 
 
]
L
M

.
t
a
t
s
[
 
 

1
v
9
2
7
5
0

.

3
0
6
1
:
v
i
X
r
a

Convergence of Contrastive Divergence Algorithm

in Exponential Family

Tung-Yu Wu1, Bai Jiang1, Yifan Jin, Wing H. Wong2

Stanford University

This paper studies the contrastive divergence algorithm for approximate
Maximum Likelihood Estimate (MLE) in exponential family, by relating it to
Markov chain theory and stochastic stability literature. We prove that, with
asymptotically probability 1, the algorithm generates a sequence of parameter
guesses which converges to an invariant distribution concentrating on shrink-
ing balls centering at the MLE. The convergence rate is slightly slower than
O(n−1/2) and the number of steps in Markov Chain Monte Carlo will only
aﬀect the constant factor of convergence rate.

Keywords: Contrastive Divergence Algorithm, Exponential Family, Harris

Recurrent Chain, Convergence Rate

1

Introduction

Contrastive Divergence (CD) algorithm [1] has been widely used for parameter
inference of Markov Random Fields. This ﬁrst example of application is given by
Hinton [1] to train Restricted Boltzmann Machines, the essential building blocks
for Deep Belief Networks [2, 3, 4]. The key idea behind CD is to approximate
the computationally intractable term in the likelihood gradient by running a
small number (m) of steps of a Markov Chain Monte Carlo (MCMC) run. Thus
it is much faster than the conventional MCMC methods that run a large number
to reach equilibrium distributions.

Despite of CD’s empirical success, theoretical understanding of its behav-
ior is far less satisfactory. Both computer simulation and theoretical analysis
show that CD may fail to converge to the correct solution [5, 6, 7]. Studies
on theoretical convergence properties have thus been motivated. Yuille relates
the algorithm to the stochastic approximation literature and gives very restric-
tive convergence conditions [8]. Others show that for Restricted Boltzmann
Machines the CD update is not the gradient of any function [9], but that for
full-visible Boltzmann Machines the CD update can be viewed as the gradient
of pseudo-likelihood function if adopting a simple scheme of Gibbs sampling
[10]. In any case, the fundamental question of why CD with ﬁnite m can work
asymptotically in the limit of n → ∞ has not been answered.

This paper studies the convergence properties of CD algorithm in exponential
families and gives the convergence condition involving the number of steps of
Markov kernel transitions m, spectral gap of Markov kernels, concavity of the
log-likelihood function and learning rate η of CD updates (assumed ﬁxed in our

1Contributing equally to this paper
1Contributing equally to this paper
2Corresponding author. whwong at stanford.edu

1

analyses). This enables us to establish the convergence of CD with a ﬁxed m to
the true parameter as the sample size n increases.

Section 2 describes the CD algorithm for exponential family with parameter
θ and data X. Section 3 states our main result. Section 4 gives three inequalities
for data X = (X1, ..., Xn), which are shown to hold asymptotically with prob-
ability 1 in Appendix. Section 5 shows CD updates generate a Markov chain
{θt}t≥0 in the parameter space given any realization of data X = x satisfying
the three inequalities. Section 6 quantiﬁes the gradient approximation error of
CD algorithm and Section 7 establishes drift condition with the log-likelihood
gap u(θ) = l(ˆθ)− l(θ) as Lyapunov function. Section 8 follows to prove that the
chain {θt}t≥0 is positive Harris recurrent and thus processes a unique invariant
distribution πn. The invariant distribution πn is shown to concentrate around
the Maximum Likelihood Estimate in Section 9.

For convenience of the reader, we assume throughout Sections 3-9 that the
exponential family under study is a set of continuous probability distributions
and show in Section 10 how to get a similar conclusion for the case of dis-
crete probability distribution. We also provide two numerical experiments to
illustrate the theories in Section 11.

2 Contrastive Divergence in Exponential Family
Consider an exponential family over X ⊆ Rp with parameter θ ∈ Θ ⊆ Rd

pθ(x) = c(x)eθ·φ(x)−Λ(θ)

where c(x) is the carrier measure, φ(x) ⊆ Rd is the suﬃcient statistic and Λ(θ)
is the cumulant generating function

(cid:90)

X

Λ(θ) = log

c(x)eθ·φ(x)dx.

We assume φ(X ) is bounded, then Λ(θ) < ∞ for any θ ∈ Rd, i.e. the natural
parameter domain {θ ∈ Rd : Λ(θ) < ∞} = Rd. Λ(θ) is convex and diﬀerentiable
at any interior point of the natural parameter domain, and both the gradient
and Hessian matrix of cumulant generating function Λ(θ) exist

µ(θ) (cid:44) ∇Λ(θ) = Eθ[φ(X)]
Σ(θ) (cid:44) ∇2Λ(θ) = Covθ[φ(X)]

Given an i.i.d. sample X = (X1, ..., Xn) generated from a certain underlying

distribution pθ∗ , the log-likelihood function is
log c(Xi) + θ · 1
n

l(θ) =

1
n

n(cid:88)

i=1

n(cid:88)

i=1

φ(Xi) − Λ(θ),

and the gradient

g(θ) (cid:44) ∇l(θ) =

1
n

φ(Xi) − µ(θ).

n(cid:88)

i=1

2

µ(ˆθn) =

1
n

φ(Xi).

n(cid:88)
(cid:34)

i=1

n(cid:88)

i=1

1
n

(cid:35)

φ(Xi) − µ(θ)

(cid:80)n

Maximum likelihood learning can be done by gradient ascent

θnew = θ + ηg(θ) = θ + η

where learning rate η > 0.

When computing the gradient g(θ), the ﬁrst term 1
n

i=1 φ(Xi) is easy. But
it is usually diﬃcult to compute the second term µ(θ), which involves a com-
plicated integral over X . Markov Chain Monte Carlo (MCMC) methods may
generate a random sample from a Markov chain with the equilibrium distri-
bution pθ(x) and approximate µ(θ) by the sample average. However, Markov
chains take a large number of steps to reach the equilibrium distributions.

To address this problem, Hinton [1] proposed the Contrastive Divergence

(CD) method. The idea of CD is to replace µ(θ) and g(θ) with

(cid:16)

n(cid:88)

i=1

n(cid:88)

i=1

ˆµ(θ) (cid:44) 1
n

φ

X (m)

i

,

ˆg(θ) (cid:44) 1
n

φ(Xi) − ˆµ(θ)

Assuming positive deﬁniteness of Σ(θ), the Maximum Likelihood Estimate (MLE)
ˆθn uniquely exists and satisﬁes g(ˆθn) = 0 or equivalently

(cid:17)

(cid:34)

1
n

(cid:90)

X

respectively, where X (m)
is obtained by a samll number (m) of steps of an
MCMC run starting from the observed sample Xi. Formally, denote by kθ(x, y)
the Markov transition kernel with pθ(x) as equilibrium distribution. CD ﬁrst
run Markov chains for m steps

i

kθ−→ X (1)

i

kθ−→ X (2)

i

Xi

and makes update

θnew = θ + ηˆg(θ) = θ + η

kθ−→ . . . kθ−→ X (m)

i

, for i = 1, ..., n,

n(cid:88)

i=1

φ(Xi) − 1
n

(cid:16)

φ

n(cid:88)

i=1

X (m)

i

(cid:17)(cid:35)

.

Denote by Kθ the Markov operator associated with kθ(x, y), i.e.

(Kθf )(x) =

f (y)kθ(x, y)dy,

and by α(θ) the second largest absolute eigenvalue of Kθ. Markov kernel Kθ
is said to have L2-spectral gap 1 − α(θ) if α(θ) < 1. The convergence rate of
MCMC depends on L2-spectral gap [11].
θ pθ(cid:48)(·)
denotes the distribution of Markov chain after m-step transition starting from
initial distribution pθ(cid:48), and K m
θ denotes the m-step Markov operator of Kθ. We
also let (cid:107) · (cid:107) denote the l2-norm (cid:107) · (cid:107)2.

θ denotes the m-step transition kernel of kθ, km

Throughout the paper, km

3

3 Main Result

We base the convergence properties of CD algorithms for for exponential family
of continuous distributions on the assumptions (A1), (A2), (A3), (A4), (A5),
(A6). Theorem 3.1 states our main result, whose proof is presented in Sections
4-9. We later show in Section 10 a similar conclusion for the case of discrete
distribution.

(A1) φ(x) is bounded, i.e.

[−C, C]d for any x ∈ X .

there exists some constant C such that φ(x) ⊆

(A2) Θ ⊆ Rd is convex and compact, and the true parameter θ∗ is an interior

point of Θ.

(A3) For any θ ∈ Θ, Σ(θ) is positive deﬁnite. This assumption together with

continuity of Σ(θ) and (A2) immediately implies the existence of

κmin (cid:44) inf

Θ

κmin(θ) > 0,

κmax (cid:44) sup

Θ

κmax(θ) < ∞.

(A4) Deﬁne a metric ρ on the set of Markov operators {Kθ : θ ∈ Θ} as

ρ(Kθ, Kθ(cid:48)) (cid:44) sup
f(cid:54)=0

supx∈X |(Kθf )(x) − (Kθ(cid:48)f )(x)|

supx∈X |f (x)|

,

and assume the Lipchitz continuity of Kθ in sense that
ρ(Kθ, Kθ(cid:48)) ≤ ζ(cid:107)θ − θ(cid:48)(cid:107) for any θ, θ(cid:48) ∈ Θ.
(A5) Markov operators Kθ have L2-spectral gap 1 − α(θ) and

α (cid:44) sup

Θ

α(θ) < 1.

(A6) φ(X ) is a convex set. Using MCMC transition kernel kθ(x, y), φ(y) has a
conditional probability density function p(φ|θ, x) conditioning on θ and x.
For any x ∈ X , inf θ∈Θ inf φ∈φ(X ) p(φ|θ, x) > 0.

Theorem 3.1. Assume (A1), (A2), (A3), (A4), (A5), (A6). There exists some
constant L > 0 such that for any m and learning rate η satisfying

√

a (cid:44) λ2

min −

dCLαmλmax − ηλmax

λmax +

2

(cid:16)

dCLαm(cid:17)2

√

> 0,

CD algorithm generates a sequence {θt}t≥0 which converges to an invariant
distribution πn as t → ∞, conditioning on every observation X1, ..., Xn in an
event with probability approaching 1. Furthermore, for any γ1, γ2 > 0 such that
γ1 + γ2 < 1/2, deﬁne

(cid:16)

1 +

(cid:20)

√

dCLαm(cid:17)(cid:32)
(cid:16)

dC 2n−2γ1 +

1 +

√

1 + ηλmax +

η

dCLαm(cid:17)2(cid:21)

dCLαm

2

n−1+2γ1

(cid:33)

n−1/2+γ1

bn (cid:44) λmax

cn (cid:44) ηλmax

2

√

4

and a closed ball centering at centering at MLE ˆθn

(cid:40)
θ ∈ Θ : (cid:107)θ − ˆθn(cid:107)2 ≤ bn +(cid:112)b2

Bn =

n + 4acn
2a

(cid:41)

,

× (1 + nγ2)

the invariant distribution πn concentrates on the ball Bn as

while Bn is shrinking with radius

πn(Bc
n)
πn(Bn)

≤

bn +(cid:112)b2

4

Inequalities

1

(1 + nγ2)2 − 1

× b2

n + 4acn

4acn

= O(n−γ2)

n + 4acn
2a

× (1 + nγ2) = O(n−1/2+γ1+γ2 ).

In this section, we ﬁrst introduce three inequalities (4.1), (4.2), (4.3) for X =
(X1, ..., Xn) i.i.d.∼ pθ∗ .

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 1

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) < n−1/2+γ1
(cid:90)

φ(Xi) − µ(θ∗)

n
(cid:107)ˆθn(X1, ..., Xn) − θ∗(cid:107) < n−1/2+γ1
θ (Xi, y)dy −

n(cid:88)

φ(y)km

(cid:90)

sup
θ∈Θ

i=1

i=1

n(cid:88)
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 1

n

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) < n−1/2+γ1 .

(4.1)

(4.2)

(4.3)

φ(y)km

θ pθ∗ (y)dy

Lemma 4.1 shows the three inequalities hold asymptotically with probability 1.

Lemma 4.1. Assume (A1), (A2), (A3), (A4), and X1, ..., Xn
P (inequalities (4.1), (4.2), (4.3) hold) = 1

lim
n→∞
for any γ1 ∈ (0, 1/2).

i.i.d.∼ pθ∗ .

The result that (4.1) and (4.2) hold asymptotically with probability 1 follows
from standard theorems in large sample theory [12]. Therefore it suﬃces to
show inequality (4.3) holds asymptotically. To this end, we deﬁne fθ : x (cid:55)→

θ (x, y)dy and bound the tail of empirical process

(cid:82) φ(y)km

(cid:20)
(cid:20)(cid:90)

n(cid:88)
n(cid:88)

i=1

vn(fθ) (cid:44) n−1/2

= n−1/2

(cid:90)

fθ(Xi) −

fθ(x)pθ∗ (x)dx

φ(y)km

θ (Xi, y)dy −

φ(y)km

θ pθ∗ (y)dy

(cid:21)

(cid:90)

(cid:21)

i=1

by Theorem 2.14.9 in [13], which relates the tail of empirical process to covering
number of function class. Details of proof are provided in Appendix.

5

5 Markov Chain θt
In this section, we show that CD generates a Markov chain {θt}t≥0 in the
parameter space Θ given any realization of observation data x = (x1, ..., xn).
Denote by Px(·) the conditional probability measure given X = x, and by
X(m)
the m-step MCMC random sample to estimate
the gradient from θt−1 to θt in CD. The ﬁltration

t,2 , ..., X (m)

t,1 , X (m)

t =

X (m)

(cid:16)

(cid:17)

t,n

G0 (cid:44) σ-algebra (X, θ0)
Gt (cid:44) σ-algebra

(cid:16)

X, θ0, X(m)

1

, θ1, X(m)

2

, ..., θt−1, X(m)

t

, θt

for t ≥ 1

contains all historical information until tth step of CD. The CD update

(cid:16)

n(cid:88)

i=1

φ

X (m)
t,i

φ(xi) − 1
n

n(cid:88)

i=1

1
n

(cid:34)
(cid:17)

θt = θt−1 + η

(cid:16)

(cid:17)
(cid:17)(cid:35)

t,n

X (m)

t =

t,1 , X (m)

t,2 , ..., X (m)

is merely function of observation data x, current θt and m-step MCMC sample
X(m)
for the current step. Conditioning on observa-
tion data x and current θt, X (m)
Thus {θt} is indeed a homogeneous Gt-adapted Markov chain under Px.
t,i
Thereafter the remaining of the paper discusses the convergence property of
CD path {θt}t≥0 in the framework of Markov chain theory. From now on Px
θ (·)
denotes the conditional probability measure and the conditional expectation
θ (·) denote
given observation data x and parameter guess θ. And Ex
the expectation and covariance under Px

is independent to the history of CD updates.

θ (·) and Covx

θ (·).

6 Gradient Approximation Error

Our next question is how well CD method approximate the gradient given ob-
servation data x? In Lemma 6.1, we bound the bias and variance of ˆg(θ) under
Px
θ . Denote the approximation error by

∆g(θ) (cid:44) ˆg(θ) − g(θ) = µ(θ) − 1
n

φ

X (m)

i

.

(cid:17)

(cid:16)

n(cid:88)

i=1

Lemma 6.1 shows that the gradient approximation bias is O(n−1/2+γ1 )+O(αm(cid:107)θ−
ˆθn(cid:107)) depending on the L2 convergence rate of MCMC chains α, the MCMC step
number m, sample size n and the distance between θ and the MLE ˆθn, and that
the gradient approximation variance is O(1/n) depending on the sample size n.

Lemma 6.1. Assume (A1), (A2) and (A5) and observation data x satisﬁes
inequalities (4.2) and (4.3). Then

θ ∆g(θ)(cid:107) ≤(cid:16)

(cid:107)Ex

1 +

dCLαm(cid:17)

√

√

n−1/2+γ1 +

dCLαm(cid:107)θ − ˆθn(cid:107)

6

for some constant L > 0, where 1 − α is the uniform L2-spectral gap of MCMC
chains and γ1 is introduced by inequalities (4.2) and (4.3). And

trace [Covx

θ ∆g(θ)] ≤ dC 2
n

.

Proof. For simplicity of notations, we abbreviate ∆g(θ) as ∆g.

(cid:107)Ex

θ ∆g(cid:107) =

≤

n

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)µ(θ) − 1
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 1
(cid:90)
n(cid:88)
(cid:13)(cid:13)(cid:13)(cid:13)µ(θ) −

i=1

X

n

+

(cid:90)

n(cid:88)

i=1

(cid:90)

X

φ(y)km

θ (xi, y)dy

X
θ (xi, y)dy −

φ(y)km

φ(y)km

θ pθ∗ (y)dy

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

φ(y)km

θ pθ∗ (y)dy

The ﬁrst term is bounded by n−1/2+γ1 in inequality (4.3). For the second term,
consider each j = 1, ..., d,

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

(cid:90)
(cid:13)(cid:13)(cid:13)(cid:13)

X

(cid:12)(cid:12)(cid:12)(cid:12)

(cid:19)2

− 1

pθ(y)dy

(6.1)

(6.2)

φj(y)km

φj(y)km

θ pθ∗ (y)dy

(cid:90)

(cid:12)(cid:12)(cid:12)(cid:12)µj(θ) −
(cid:12)(cid:12)(cid:12)(cid:12)(cid:90)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:90)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:90)

K m

X

X

X

=

=

=

θ pθ∗ (y)dy

φj(y)km

X
θ pθ(y)dy −

(cid:18) pθ∗ (y)

(cid:12)(cid:12)(cid:12)(cid:12)

(cid:90)
(cid:19)
(cid:18) pθ∗ (y)

X
− 1

pθ(y)

pθ(y)

θ φj(y)
(cid:115)(cid:90)
θ (φj(y) − µj(θ))
K m
(cid:112)

X

≤ α(θ)m

(φj(y) − µj(θ))2 pθ(y)dy

e−2Λ(θ∗)+Λ(θ)+Λ(2θ∗−θ) − 1

≤ αmC
≤ CLαm(cid:107)θ − θ∗(cid:107).

pθ(y)dy

− 1

pθ(y)dy

(cid:12)(cid:12)(cid:12)(cid:12)
(cid:19)
(cid:115)(cid:90)

(cid:12)(cid:12)(cid:12)(cid:12)
(cid:18) pθ∗ (y)

X

pθ(y)

(φj(y) − µj(θ))2 pθ(y)dy ≤ C 2

(6.1) follows the facts that(cid:90)
(cid:18) pθ∗ (y)

and that

(cid:90)

X

(cid:19)2

X

pθ(y)

− 1

pθ(y)dy = e−2Λ(θ∗)+Λ(θ)+Λ(2θ∗−θ) − 1,

and (6.2) follows the continuously diﬀerentiability and thus Lipschitz continuity
of function f : θ ∈ Θ (cid:55)→

e−2Λ(θ∗)+Λ(θ)+Λ(2θ∗−θ) − 1 , i.e.

√

|f (θ)| = |f (θ) − 0| = |f (θ) − f (θ∗)| ≤ L(cid:107)θ − θ∗(cid:107)

7

with L being the Lipschitz constant. Putting inequalities (4.2), (4.3) and (6.2)
together yields

(cid:107)Ex

θ ∆g(cid:107) ≤ n−1/2+γ1 +
≤ n−1/2+γ1 +

dCLαm(cid:107)θ − θ∗(cid:107)
dCLαm(cid:107)ˆθn − θ∗(cid:107) +

√

dCLαm(cid:107)θ − ˆθn(cid:107)

√

n−1/2+γ1 +

dCLαm(cid:107)θ − ˆθn(cid:107)

√
√

≤(cid:16)

1 +

dCLαm(cid:17)

√

θ (xi,·) are conditional independent (but not
Also, noting that X (m)
identically distributed) since we run n Markov Chains independently starting
from diﬀerent xi, i = 1, ..., n, write

|x, θ ∼ km

i

(cid:19)2

φj(y) −

φj(z)km

θ (xi, z)dz

km
θ (xi, y)dy

trace [Covx

n(cid:88)

d(cid:88)

(cid:90)

θ ∆g]

(cid:18)

X

j=1

i=1

=

1
n2
≤ dC 2
n

.

(cid:90)

X

7 Drift Towards MLE

When studying the convergence of CD method, we hypothesize that starting at
some θ0 far away from ˆθn, the exact gradient g(θt) is large enough to dominate
the approximation error of m-step MCMC sampling, and bring a positive drift
in log-likelihood, until θt is close to ˆθn and g(θt) fails to suppress the MCMC
error. To precisely characterize this phenomenon, we give the deﬁnitions of drift
and establish the drift condition with Lyapunov function

u(θ) (cid:44) l(ˆθn) − l(θ)

being the log-likelihood gap at θ compared to the MLE ˆθn.
Deﬁnition 7.1. (drift) Let V : S → R+ be some function on the state space of
a Markov chain {Zt}. The one-step drift of V is deﬁned as

EzV (Z1) − V (z)

Deﬁnition 7.2. (drift condition) V satisﬁes the drift condition if

EzV (Z1) − V (z) ≤ −δ for z ∈ Bc

with some δ > 0 and some subset of the state space B. V is called a Lyapunov
function for the Markov chain {Zt}.

Lemma 7.1 shows that function u(θ) (cid:44) l(ˆθ) − l(θ) satisﬁes drift condition
outside of closed balls Bβ = {θ ∈ Θ : (cid:107)θ − ˆθ(cid:107) ≤ βrn} with β > 1 and rn =
O(n−1/2+γ1 ).

8

Lemma 7.1. Assume (A1), (A2), (A3), (A5). For any m and learning rate η
satisfying

√

a (cid:44) λ2

min −

dCLαmλmax − ηλmax

2

√

λmax +

(cid:16)

dCLαm(cid:17)2

> 0

and observation data x satisfying inequalities (4.2) and (4.3), the chain {θt}t≥0
has Lyapunov function u(θ) = l(ˆθn) − l(θ) which holds drift condition outside
closed ball

Bβ = {θ ∈ Θ : (cid:107)θ − ˆθn(cid:107) ≤ βrn} for any β > 1

bn +(cid:112)b2

with

where

δ ≥ η(β2 − 1)cn,

rn =

n + 4acn
2a

= O(n−1/2+γ1)

(cid:16)

√

dCLαm(cid:17)(cid:16)
(cid:16)

1 +
dC 2n−2γ1 +

(cid:20)

1 + ηλmax + η

√

dCLαm(cid:17)2(cid:21)

√

1 +

dCLαm(cid:17)

n−1+2γ1

n−1/2+γ1

bn = λmax

cn =

ηλmax

2

Proof. For simplicity of notations, we abbreviate g(θ) as g, ˆg(θ) as ˆg, and ∆g(θ)
as ∆g. The diﬀerence u(θ1)−u(θ) when moving from θ to θ1 = θ+ηˆg is bounded
from above as

u(θ1) − u(θ) = −η(cid:104)g, ˆg(cid:105) +
≤ −η(cid:104)g, ˆg(cid:105) +
= −η(cid:104)g, g(cid:105) − η(cid:104)g, ∆g(cid:105) +

η2 (cid:104)Σ(θ(cid:48))ˆg, ˆg(cid:105)
η2λmax (cid:104)ˆg, ˆg(cid:105)
1
2

1
2
1
2

η2λmax (cid:104)ˆg, ˆg(cid:105)

where θ(cid:48) is a convex combination of θ and θ1. The ﬁrst term (cid:107)g(cid:107)2 is constant.
Expectation of the second term is

θ [−(cid:104)g, ∆g(cid:105)] = −(cid:104)g, Ex
Ex

θ ∆g(cid:105) ≤ (cid:107)g(cid:107)(cid:107)Ex

θ ∆g(cid:107) ,

and expectation of the third term is

θ [(cid:104)ˆg, ˆg(cid:105)] = trace [Covx
Ex
≤ trace [Covx

θ ˆg(cid:107)2

θ ˆg] + (cid:107)Ex
θ ∆g] + ((cid:107)g(cid:107) + (cid:107)Ex

θ ∆g(cid:107))2 .

Since g(θ) = g(θ) − g(ˆθn) = µ(ˆθn) − µ(θ),

λmin(cid:107)θ − ˆθn(cid:107) ≤ (cid:107)g(θ)(cid:107) ≤ λmax(cid:107)θ − ˆθn(cid:107).

Putting (7.1), (7.2) and (7.3) with Lemma 6.1 together yields

θ [u(θ1) − u(θ)] ≤ −η(a(cid:107)θ − ˆθn(cid:107)2 − bn(cid:107)θ − ˆθn(cid:107) − cn).
Ex

9

(7.1)

(7.2)

(7.3)

(7.4)

The RHS has a quadratic form of (cid:107)θ − ˆθn(cid:107), and it is clear that a > 0 for
suﬃciently large m and suﬃciently small η. Then

(cid:107)θ − ˆθn(cid:107) ≥ bn +(cid:112)b2

n + 4acn
2a
θ [u(θ1) − u(θ)] ≤ 0.

:= rn = O(n−1/2+γ1)

can guarantee Ex
outside any closed ball Bβ centering at MLE of radius βrn with β > 1, i.e.

In particular, the drift condition holds

(cid:107)θ − ˆθn(cid:107) > βrn ⇒ Ex

θ [u(θ1) − u(θ)] ≤ −δ < 0

with

n − bnβrn − cn)

n − bn(β − 1)rn]
n − brn)

δ = η(aβ2r2
= η[a(β2 − 1)r2
≥ η(β2 − 1)(ar2
= η(β2 − 1)cn

Remark. A function h(·) is called harmonic for a transition probability p(x,·)

at x if(cid:82) h(y)p(x, dy) ≤ h(x). And it is called strong sup-harmonic if

(cid:90)

h(y)p(x, dy) ≤ h(x) − δ

for some positive δ. We actually prove in Lemma 7.1 that u(θ) is sup-harmonic
at Bc
β. We later see in Theorem 8.1 a nice connection between sup-harmonic
functions, positive recurrence of Markov chains, and sup-martingales.

8 Positive Harris Recurrence

Tweedie [15] connected the drift condition in Deﬁnition 7.2 to positive recur-
rence of sets in the state space by a Markov chain. We restate this result in
Theorem 8.1 and provide a proof based on sup-martingales and sup-harmonic
functions in Appendix. Next, Corollary 8.1 combines Lemma 7.1 with Theorem
8.1 and concludes that the closed balls Bβ centering at the MLE ˆθn of radius
βrn are positive recurrent by the chain {θt}.
Theorem 8.1. (Theorem 6.1 in [15]) Suppose a Markov chain {Zt} has a non-
negative function V on the state space satisfying the drift condition in Deﬁnition
7.2 with some δ > 0 and set B. Let T = min{t ≥ 1 : Zt ∈ B} be the ﬁrst hitting
time of B if starting from Z0 ∈ Bc or the ﬁrst returning time otherwise, then

(cid:40)

(cid:82)

V (z)/δ
1 + 1
δ

EzT ≤

Bc V (z1)p(z, dz1)

for z ∈ Bc
for z ∈ B

10

where p(z, dz1) is the transition probability of the chain. Thus, if

(cid:90)

V (z1)p(z, dz1) < ∞

sup
z∈B

Bc

also holds, then the set B is positive recurrent.

Lyapunov function is widely used in stochastic stability or optimal control
study [16]. As we have seen in Theorem 8.1, a suitably designed Lyapunov
function can determine the positive recurrence of sets of a Markov chain. We
proceed to apply Theorem 8.1 to the Markov chain {θt}, for which u(θ) satisﬁes
the drift condition outside of any closed ball Bβ in Lemma 7.1, and conclude in
Corollary 8.1 that closed balls Bβ centering at MLE are positive recurrent by
the chain {θt}.
Corollary 8.1. Following Theorem 8.1 and Lemma 7.1, Bβ for each β > 1 are
positive recurrent by the chain {θt}.
Proof. Let T (cid:44) min{t ≥ 1 : θt ∈ Bβ} be the ﬁrst hitting or returning time of Bβ
by the chain {θt}. Lemma 7.1 establishes the drift condition for the likelihood
gap function u(θ) outside of Bβ, i.e.

The compactness of Θ and continuity of u(θ) follow the boundedness of u(θ),
implying

θ u(θ1) − u(θ) ≤ −δ for θ ∈ Bc
Ex
β.
(cid:90)

u(θ1)p(θ, dθ1) < sup
Θ

u(θ1) < ∞.

sup
θ∈Bβ

Bc
β

Both conditions of Theorem 8.1 are satisﬁed, thus Bβ is positive recurrent.

Next we prove the positive Harris recurrence of the chain {θt}, which further

implies the distribution convergence of Markov chain in total variation.

Deﬁnition 8.1. An accessible set B is called a small set of a Markov chain Zt
if

Pz(Z1 ∈ ·) ≥ IB(z)q(·)

for some positive  > 0 and probability measure q(·) over the state space.

Deﬁnition 8.2. A Markov chain Zt is called Harris recurrent if there exists a
set B s.t.

(a) B is recurrent.

(b) B is a small set.

If B is positive recurrent in addition, then the chain is called positive Harris
recurrent.

11

Lemma 8.1. Assume (A1), (A2), (A3), (A4), (A5), (A6). For suﬃciently
large n and for any m and learning rate η satisfying

√

a (cid:44) λ2

min −

dCLαmλmax − ηλmax

2

(cid:16)

dCLαm(cid:17)2

√

λmax +

> 0,

observation data x satisfying inequalities (4.1), (4.2) and (4.3), the chain {θt}
generated by CD updates is positive Harris recurrent.

Proof. Since Lemma 8.1 ensures the positive recurrence of Bβ, it suﬃces to show
Bβ is a small set according to Deﬁnition 8.2. Since θ∗ is an interior point of Θ
and µ : Θ → φ(X ) a continuous mapping, µ(θ∗) is an interior point of φ(X ).
Denoting by ∂φ(X ) the boundary of φ(X ),

inf

(cid:107)µ(θ∗) − φ(cid:107) > 0.

φ∈∂φ(X )

(cid:80)n
i=1 φ(xi) − µ(θ∗)(cid:107) < n−1/2+γ1 holds,
n(cid:88)

(cid:107)µ(θ∗) − φ(cid:107) −

φ(xi) − φ

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

φ(xi) − µ(θ∗)

i=1

If inequality (4.1) (cid:107) 1

n

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 1

n

n(cid:88)

i=1

inf

φ∈∂φ(X )

for suﬃciently large n. Then for any θ ∈ Bβ

Bβ ⊆ θ + η

φ(xi) − φ(X )

Assumption (A6) implies 1
n
which is strictly bounded away from 0 for any θ, so does

i=1 φ

i

X (m)

has positive density over φ(X ),

over θ + η(cid:2) 1

θ1 = θ + η

i=1 φ(xi) − φ(X )(cid:3). Denote by p(θ, θ1) the transition kernel of
(cid:80)n

i=1

i=1

φ

i

X (m)

φ(xi) − 1
n

the chain {θt}, then

n

n(cid:88)

(cid:16)

(cid:17)(cid:35)

(cid:34)

1
n

n(cid:88)

(cid:35)(cid:41)

inf

p (θ, θ1) ≥ inf
θ∈Bβ

inf

p (θ, θ1) : θ1 ∈ θ + η

φ(xi) − φ(X )

> 0.

θ,θ1∈Bβ
Let q(·) be the uniform measure on Bβ. There exists some constant  > 0 such
that

i=1

p(θ, θ1)dθ1 ≥ IBβ (θ)q(A)

for any Borel set A ⊆ Θ, completing the proof.

(cid:40)

(cid:90)

A

n

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 1
(cid:35)

.

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) ≥ inf
(cid:34)

φ∈∂φ(X )
≥ 2βrn/η

(cid:80)n
(cid:34)
n(cid:88)

1
n

n(cid:88)
(cid:16)

i=1

1
n

(cid:17)

12

As stated in Theorems 6.8.5, 6.8.7, 6.8.8 in [17], any aperiodic, positive
Harris recurrent chain {Zt} processes an unique invariant distribution π, and
the chain {Zt} converges to the invariant distribution π in total variation for
π-a.e. starting point z. We strengthen these results for chain {θt} in Corollary
8.2.

Corollary 8.2. Following Lemma 8.1,
(a) the chain {θt} has an unique invariant probability measure πn
(b) let Θ1 be the set of θ ∈ Θ s.t.

(cid:107)Px

θ (θt ∈ ·) − πn(·)(cid:107)t.v. → 0 as t → ∞,

(8.1)

then πn(Θ1) = 1.

(c) πn and the Lesbegue measure are absolutely continuous w.r.t. each other.

(d) πn has a positive density function over Θ.
(e) (8.1) holds for almost all θ ∈ Θ (in sense of Lesbegue measure).
Proof. Clearly the chain {θt} is aperiodic. Then parts (a)(b) are immediate
consequences of Lemma 8.1 and Theorems 6.8.5, 6.8.7, 6.8.8 in [17]. Proceed to
θ (θt ∈
prove part (c). On the ﬁrst hand, part (b) and absolutely continuity of Px
·) w.r.t.
the Lesbegue measure imply absolutely continuity of πn w.r.t.
the
Lesbegue measure. On the other hand, the invariant measure πn is a maximal
irreducibility measure (See Deﬁnition 8.3) by Theorems 10.0.1 and 10.1.2 in [16].
Hence the Lesbegue measure is absolutely continuous w.r.t. πn. Finally, parts
(b)(c) imply (d)(e).
Deﬁnition 8.3. Let π be a positive measure on the state space of chain {Zt}.
If for any z and set A in the state space π(A) > 0 implies the accessibility of set
A by the chain from any start point z, we say π is an irreducible measure for the
chain (or the chain is π-irreducible). An irreducible measure π∗ specifying the
minimal family of null sets, i.e. π∗(A) = 0 =⇒ π(A) = 0 for any irreducible
measure, is called a maximal irreducibility measure.

9 Concentration of the Invariant Distribution

Lemma 9.1 shows that the invariant distribution πn concentrates on positive
recurrent ball Bβ.

Lemma 9.1. Following Corollaries 8.2, the invariant probability measure π
concentrates on Bβ as

π(Bc

β) = O(n−γ2 ) for any γ2 ∈ (0, 1/2 − γ1)

if β(n) = 1 + O(nγ2 ) increases with n, while the ball Bβ shrinks with radius

βrn = O(n−1/2+γ1+γ2).

13

Proof. By (7.4),

(cid:90)

Θ

u(θ)π(dθ)

(cid:16)
θ u(θ1)π(dθ) −
Ex
−η

0 =

≤

Θ

Θ

a(cid:107)θ − ˆθn(cid:107)2 − bn(cid:107)θ − ˆθn(cid:107) − cn

(cid:17)

π(dθ)

where a, bn, cn deﬁned in Lemma 7.1. Rearranging terms yields

a(cid:107)θ − ˆθn(cid:107)2 − bn(cid:107)θ − ˆθn(cid:107) − cn

π(dθ)

a(cid:107)θ − ˆθn(cid:107)2 − bn(cid:107)θ − ˆθn(cid:107) − cn

π(dθ).

(cid:17)

(cid:17)

(cid:90)
(cid:90)
(cid:90)
(cid:90)

(cid:16)
−(cid:16)

≤

Bc
β

Bβ

At θ ∈ Bc
β,
aβ2(cid:107)θ − ˆθn(cid:107)2 − bnβ(cid:107)θ − ˆθn(cid:107) − cn ≥ aβ2r2

n − bnβrn − cn

n − bn(β − 1)rn
n − bnrn

(cid:1)

≥ a(β2 − 1)r2

≥ (β2 − 1)(cid:0)ar2
(cid:17) ≤ cn +

≥ (β2 − 1)cn

b2
n
4a

−(cid:16)

At θ ∈ Bβ,

Thus

a(cid:107)θ − ˆθn(cid:107)2 − bn(cid:107)θ − ˆθn(cid:107) − cn

π(Bc
β)
π(Bβ)

≤ 1

β2 − 1

n/4a

cn + b2
cn

Noting that bn, c1/2
increases with n yields

n (cid:16) n−1/2+γ1, letting β(n)−1 (cid:16) nγ2 for any γ2 ∈ (0, 1/2−γ1)

π(Bc
β)
π(Bβ)

≤ 1

β2 − 1

n/4a

cn + b2
cn

= O(n−γ2)

while the ball Bβ has shrinking radius

βrn = βO(n−1/2+γ1) = O(n−1/2+γ1+γ2 ).

10 Results for Discrete Exponential Family

If the suﬃcient statistic φ(X) in the exponential family is discrete, we have a
similar conclusion as stated in Theorem 10.1.
In contrast to positive Harris
recurrence of {θt} in Theorem 3.1 for the continuous case, {θt} for the discrete
case is a Markov chain with countable state space, and may admit multiple
invariant distributions.

14

Theorem 10.1. Consider an exponential family of discrete probability distri-
butions. Assume (A1), (A2), (A3), (A4), (A5) and
(A6) φ(X ) is ﬁnite, and for each j = 1, ..., d, elements in φj(X ) have rational

ratios.

The conclusion in Theorem 3.1 is also true.

Proof. If the suﬃcient statistic φ(X) in the exponential family is discrete and
i=1 φ(X (m)
has ﬁnite possible values, the CD gradient ˆg(θ) = 1
)
n
has ﬁnite possible values g1, g2, ..., gs. Starting from any initial parameter guess
θ0 ∈ Θ, the chain

n

i

(cid:80)n
i=1 φ(xi)− 1

(cid:80)n
 gk

s(cid:88)

 t(cid:88)

t(cid:88)

θt = θ0 + η

ˆg(θj) = θ0 + η

I (ˆg(θj) = gk)

j=1

k=1

j=1

lies in a countable state space ˜Θ ⊂ Θ. And ˜Θ ∩ Bβ is a ﬁnite set due to (A6).
By decomposition theorem, ˜Θ can be partitioned uniquely as

˜Θ = ˜Θ0 ∪ ˜Θ1 ∪ ˜Θ2 ∪ ...

where ˜Θ0 is the set of transient states and the ˜Θi, i ≥ 1 are disjoint, irreducible
closed set of recurrent states. The chain either remains forever in ˜Θ0, or it lies
eventually in some ˜Θi, i ≥ 1.
We argue by contradiction that the ﬁrst of these possibility does not occur.
Suppose the chain {θt} forever lies in ˜Θ0. By Corollary 8.1, Bβ is positively
recurrent. Thus the chain visits ˜Θ0∩Bβ inﬁnitely many times, and thus visits at
least one state in ˜Θ0∩Bβ inﬁnitely many times for reason that ˜Θ0∩Bβ ⊆ ˜Θ∩Bβ
is ﬁnite. Such a state is recurrent, contradicting to it belonging to the set of
transient states ˜Θ0.

Therefore the chain will eventually lies in the ﬁrst irreducible set of recurrent
states it enters, and converges to the corresponding invariant distribution. Each
of them πn concentrates in Bβ with

π(Bc
β)
π(Bβ)

≤ 1

β2 − 1

n/4a

cn + b2
cn

.

The same convergence rate with that in Theorem 3.1 can be obtained.

Theorem 10.1 establishes the convergence property of CD algorithm for rel-
ative simple exponential family of discrete distributions. But it suﬃces to guar-
anteed the success of CD algorithm for Restricted Boltzmann Machines (RBM)
which has φ(X ) = {0, 1}d.
Also, it is also noteworthy that CD algorithm converges to the MLE even for
more complicated cases in which φ(X ) is inﬁnite and/or elements in φ(X ) have
irrational ratios, if one takes the ﬁnite precision of computation into account.

15

Due to the ﬁnite precision of any computer, numbers are always rounded or
truncated. In real world, the update rule of CD algorithm is

(cid:17)(cid:35)

(cid:16)

n(cid:88)

i=1

φ(Xi) − 1
n

φ

X (m)

i

+ ε

˜θnew = θnew + ε = θ + η

(cid:34)

n(cid:88)

i=1

1
n

(cid:105)

(cid:104)

where ε is the numerical error incurred such that θnew is substituted by its
nearby grid point ˜θnew. As

u(˜θ1) − u(θ) =

u(˜θ1) − u(θ1)

+ [u(θ1) − u(θ)] ≤ O((cid:107)ε(cid:107)) + [u(θ1) − u(θ)] ,

we have the following drift condition akin to (7.4)

(cid:104)

Ex
θ

u(˜θ1) − u(θ)

(cid:105) ≤ −η(a(cid:107)θ − ˆθn(cid:107)2 − bn(cid:107)θ − ˆθn(cid:107) − (cn + O((cid:107)ε(cid:107)))).

If the precision of computation copes well with the sample size n, i.e. (cid:107)ε(cid:107) (cid:16) cn,
the chain {˜θt} is positively recurrent to the ball Bβ centering at the MLE
ˆθn. Noting the ball Bβ contains ﬁnitely many grid points, a similar argument
to Theorem 10.1 can prove that the chain {˜θt} admits invariant distributions
concentrating around the MLE.

11 Numerical Experiments

11.1 Bivariate Normal Distribution

We conduct numerical experiments on the bivariate normal model

(cid:19)

p(θ) =

(x − µ)T Σ−1(x − µ)

(11.1)

with unknown mean µ (parameter θ) and known covariance matrix

(cid:18)

− 1
2

1(cid:113)
(2π)2 |Σ| exp
(cid:20) 1

Σ =

(cid:21)

0.5
1

0.5

.
Figure 1-3 shows CD path {µt}t≥0 given a sample X of size n = 50, 100, 500
with true parameter µ∗ = (0, 0), respectively. For each data sample, CD-3 (i.e.
CD with m=3) is applied to learn the parameter µ.
In each of Figure 1-3, subplot (a) shows the CD paths {µt}t≥0 in the pa-
rameter space with diﬀerent start points µ = (3, 3), (−3, 3), (3,−3), (−3,−3).
They illustrate that the estimated parameter initially directly moves towards
to true parameter but eventually randomly walks around the true parameter.
Furthermore, comparison of Figures 1(a), 2(a), 3(a) shows that the region of
the random walk decreases in size as the sample size n increases.

16

(a)

(b)

(c)

(d)

Figure 1: Simulation results of the bivariate normal distribution with N = 50.

Subplot (b) shows the true gradient of the likelihood function in each case.
Subplot (c) presents the approximate gradient used by CD. For each grid point
in the subplot (c), we run CD-3 5 times and draw 5 estimated gradients to
reveal the randomness in CD approach. Subplot (d) reveals the directions of
these gradients by normalizing the magnitude of these gradients. According to
the plots (b) and (c), it can be observed that the magnitude and direction of
the gradient become smaller and more stochastic when the point moves closer
to the true parameter. Comparing the three ﬁgures we can see that the range
of randomness decreases as the sample size increases. This is exactly what our
theory predicts.

11.2 Restricted Boltzmann Machines

In our next experiment, we simulate data from the Restricted Boltzmann Ma-
chines (RBM). The CD method is a standard way for inferring RBM during
the training of deep belief neural network [2]. There are m visible units V =
(V1, ..., Vm) to represent observable data and n hidden units H = (H1, ..., Hn)
to capture dependencies between observed variables. In the simulation, we focus
on the binary RBM which the random variables (V , H) take values (v, h) ∈
{0, 1}m+n and the joint distribution of (V , H) is given by p (v, h) = 1
Z e−E(v,h)
with the energy function

17

(a)

(b)

(c)

(d)

cihi

(11.2)

i=1

j=1

j=1

i=1

Figure 2: Simulation results of the bivariate normal distribution with N = 100.

m(cid:88)

wijhivj − m(cid:88)

bjvj − n(cid:88)

In the simulation, the data sets is generated from a RBM with the weight ma-

E (v, h) = − n(cid:88)
(cid:20)0.5 0.5
(cid:21)
triangular part show the approximate gradients. Let x =(cid:2)x1 x2 x3 x4
(cid:2)w11 w21 w12 w22

imate gradients for N = 102, 104 and 106. In each ﬁgure, subplots in the lower
=

. Subplot (i, j) in the lower triangular part gives the
projections of the gradient onto the plane (xi, xj), at those points ˜x satisfying
xk equal to 0.5 approximately, for k not equal to i or j. The corresponding di-
rections of these gradients are given in the upper triangular part of each ﬁgure.
Again,the behaviour of the CD approximate gradients is in agreement with our
theory.

and ci = bj = 0 for i, j = 1, 2.Figure 4-6 shows the approx-

(cid:3)T

trix w =

0.5 0.5

(cid:3)T

A Proof of Lemma 2.1 and Theorem 6.1

A.1 Proof of Lemma 2.1

In Lemma 4.1, The result that (4.1) and (4.2) hold asymptotically with proba-
bility 1 follows from standard theorems in large sample theory [12]. Therefore
it suﬃces to show inequality (4.3) holds asymptotically. Therefore it suﬃces

18

(a)

(b)

(c)

(d)

Figure 3: Simulation results of the bivariate normal distribution with N = 500.

to show inequality (4.3) holds with asymptotically probability 1. To this end,
we applied limit theorem for empirical processes [13], which relates the tail of
empirical process to covering number of function class.
Deﬁnition A.1. (covering number) Let (F, D) be an arbitrary semi-metric
space. Then the covering number N (,F, D) is the minimal number of balls of
radius  > 0 needed to cover F. Formally,

N (,F, D) = min{k : ∃f1, . . . , fk ∈ F s.t. sup
f∈F

min
1≤j≤k

D(f, fk) < }.

Theorem A.1. (Theorem 2.14.9 in [13]) Let X1, ..., Xn i.i.d. and F be a class
of functions f : X → [0, 1]. If

then for every t > 0

(cid:32)

P

sup
f∈F

n(cid:88)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) 1√

n

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) > t
(cid:33)

(cid:19)s

(cid:18) C2t√

s

≤

e−2t2

,

[f (Xi) − Ef (X1)]

i=1

19

N (,F,L2,Q) ≤

sup
Q

, ∀ 0 <  < C1

where s, C1 are constants, Q is a probability measure on X , and

L2,Q(f1, f2) (cid:44)

(f1(x) − f2(x))2Q(dx),

(cid:18) C1

(cid:19)s



(cid:115)(cid:90)

X

Figure 4: Simulation results of RBM with N = 102.

where constant C2 only depends on C1.

We proceed to bound the tail supΘ (cid:107)vn(fθ)(cid:107) by using Theorem A.1.
(cid:32)
(cid:33)

Lemma A.1. Assume (A1), (A2) and (A4) and X1, ..., Xn

i.i.d.∼ pθ∗ . Then

φ(y)km

θ (Xi, y)dy −

φ(y)km

θ pθ∗ (y)dy

2 +γ1

→ 1

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 1

n

(cid:90)

n(cid:88)

X

i=1

P

sup
θ∈Θ

(cid:90)

X

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) < n− 1

as n → ∞, for any γ1 ∈ (0, 1/2).

20

Figure 5: Simulation results of RBM with N = 104.

Proof. Let fθ : x (cid:55)→(cid:82) φ(y)km

θ (x, y)dy, then

φ(y)km

θ (Xi, y)dy = fθ(Xi)

(cid:90)
(cid:90)

X

X

φ(y)km

θ pθ∗ (y)dy =

=

=

(cid:19)

(cid:19)

km
θ (x, y)pθ∗ (x)dx

dy

φ(y)km

θ (x, y)dy

pθ∗ (x)dx

(cid:18)(cid:90)

X

φ(y)

(cid:18)(cid:90)

X

(cid:90)
(cid:90)
(cid:90)

X

X

X

fθ(x)pθ∗ (x)dx.

21

Figure 6: Simulation results of RBM with N = 106.

For j = 1, ..., d, let

vn(f j

θ ) (cid:44) n−1/2

= n−1/2

n(cid:88)
n(cid:88)

i=1

i=1

(cid:20)(cid:90)

[f (Xi) − Ef (X)]

φj(y)km

θ (Xi, y)dy −

22

(cid:90)

(cid:21)

.

φj(y)km

θ pθ∗ (y)dy

and view vn(f j
θ (x) − f j
f j

θ(cid:48)(x) =

X

(cid:90)

φj(y)km

θ (x, y)dy −

(cid:90)
θ ) as a stochastic process indexed by θ ∈ Θ.
m−1(cid:88)
m−1(cid:88)

X
φj − Kθ(cid:48)K m−i−1

θ(cid:48)(x, y)dy −
ki

φj(y)km−i

(cid:2)KθK m−i−1

(cid:20)(cid:90)
(cid:90)

φj(y)km

θ(cid:48) (x, y)dy

(cid:90)

φj

=

=

i=0

X

X

θ

θ

θ

X

i=0

θ(cid:48)(x)| ≤ m−1(cid:88)

implying

|f j
θ (x) − f j

(cid:21)

ki+1
θ(cid:48) (x, y)dy

φj(y)km−i−1

θ(cid:48)(x, y)dy

θ

(cid:3) (y)ki
(cid:90)

ρ(Kθ, Kθ(cid:48)) × sup
y∈X

|K m−i−1

θ

φj(y)| ×

X

ki
θ(cid:48)(x, y)dy

i=0

≤ mCζ(cid:107)θ − θ(cid:48)(cid:107)

where ρ is the metric and ζ is the Lipchitz constant introduced by Assumption
(A4). It concludes that

L2,Q(f j

θ(cid:48)) ≤ mCζ(cid:107)θ − θ(cid:48)(cid:107).

θ , f j

sup
Q

Denoting by F j the function class of {f j

θ , θ ∈ Θ}, it follows that
N (,F j,L2,Q) ≤ N (/mCζ, Θ,(cid:107) · (cid:107)) = O(−d).

sup
Q

Applying Theorem A.1 to function class F j yields

(cid:18)

P

|vn(f j

sup
Θ

(cid:19)

→ 0

nγ1√
d

θ )| >
(cid:19)

≤ d(cid:88)

j=1

as n → ∞. Further,

(cid:18)

P

sup
Θ

(cid:107)n−1/2vn(fθ)(cid:107) > n−1/2+γ1

(cid:18)

sup
Θ

P

|vn(f j

θ )| >

nγ1√
d

(cid:19)

→ 0

as n → ∞, completing the proof.

A.2 Proof of Theorem 6.1
Proof. We ﬁrst show that, if Z0 ∈ Bc, Mt = V (Zt∧T ) + (t ∧ T )δ1 is a super-
martingale adapted to Zt’s canonical ﬁltration Gt. The adaptedness of Mt to
Gt follows T being a Gt-stopping time. It suﬃces to show Ez [Mt+1 − Mt|Gt] ≤
0, then we also have integrability of non-negative Mt by induction EzMt ≤

23

EzMt−1 ≤ ... ≤ EzM0 = V (z) < ∞. Indeed,

(Mt+1 − Mt)I(T ≤ t) = [(V (ZT ) + T δ1) − (V (ZT ) + T δ1)] I(T ≤ t)

= 0

(Mt+1 − Mt)I(T ≥ t + 1) = [(V (Zt+1) + (t + 1)δ1) − (V (Zt) + tδ1)] I(T ≥ t + 1)

= [V (Zt+1) − V (Zt) + δ] I(T ≥ t + 1)

implying for z ∈ Bc
Ez [Mt+1 − Mt|Gt] = Ez [(Mt+1 − Mt)I(T ≤ t)|Gt] + Ez [(Mt+1 − Mt)I(T ≥ t + 1)|Gt]

(i)

(ii)

= Ez [(V (Zt+1) − V (Zt) + δ) I(T ≥ t + 1)|Gt]
= Ez [(V (Zt+1) − V (Zt) + δ)|Gt] I(T ≥ t + 1)
= Ez [(V (Zt+1) − V (Zt) + δ)|Zt] I(T ≥ t + 1)
(iii)≤ [−δ + δ] I(T ≥ t + 1)
= 0

where (i) follows T is a Gt-stopping time, and thus {T ≥ t + 1} ∈ Gt, (ii) is
due to the Markov property of {Zt} and (iii) follows Zt ∈ Bc (given T ≥ t + 1
and z ∈ Bc) and the drift condition in Deﬁnition 7.2. Consequently, EzMt ≤
EzM0 = V (z) for z ∈ Bc. That is

EzV (Zt∧T ) + Ez(t ∧ T )δ ≤ V (z).

implying with non-negativeness of V

Taking t → ∞, the monotone convergence theorem yields

Ez(t ∧ T )δ ≤ V (z).

EzT ≤ V (z)/δ for z ∈ Bc

(cid:90)

Bc

Furthermore, one step analysis gives
EzT = Pz(Z1 ∈ B) +

(1 + Ez1 T ) p(z, z1)dz1

(cid:90)

(cid:90)

Bc
1
δ

= 1 +

≤ 1 +

(Ez1T )p(z, z1)dz1

V (z1)p(z, z1)dz1

Bc

for z ∈ B, completing the proof.

Acknowledgements

This work is supported by NSF of US under grant DMS-1407557. The authors
would like to thank Prof. Lester Mackey, Dr. Rachel Wang and Weijie Su for
valuable advice.

24

References

[1] Hinton, G. E. (2002). Training products of experts by minimizing con-

trastive divergence. Neural Computation 14(8) 1771–1800.

[2] Hinton, G., S. Osindero and Y. Teh (2006). A fast learning algorithm

for deep belief nets. Neural Computation 18(7) 15271554.

[3] Hinton, G. and R. Salakhutdinov (2006). Reducing the dimensionality

of data with neural networks. Science 313(5786) 504-507.

[4] Bengio, Y., P. Lamblin, D. Popovici, H. Larochelle, and U. Mon-
treal (2007). Greedy layer-wise training of deep networks. In Advances in
Neural Information Processing Systems 19 153.

[5] MacKay, D. (2001). Failures of the one-step learning algorithm. In Avail-
able electronically at http://www.inference.phy.cam.ac.uk/mackay/
abstracts/gbm.html.

[6] Teh, Y., M. Welling, S. Osindero and G. Hinton (2003). Energy-
based models for sparse overcomplete representations. The Journal of Ma-
chine Learning Research 4 1235-1260.

[7] Williams, C. K. and F. V. Agakov (2002). An analysis of contrastive
divergence learning in gaussian boltzmann machines. Institute for Adaptive
and Neural Computation.

[8] Yuille (2005). The convergence of contrastive divergence. In Advances in

neural information processing systems 17 1593-1600.

[9] Sutskever, I. and Tieleman, T. (2010). On the Convergence Properties
of Contrastive Divergence. In International Conference on Artiﬁcial Intelli-
gence and Statistics 789-795.

[10] Hyv¨arinen, A. (2006). Consistency of pseudolikelihood estimation of fully

visible Boltzmann machines. Neural Computation 18(10) 2283-2292.

[11] Rudolf, D. (2011). Explicit error bounds for Markov chain Monte Carlo.

arXiv preprint arXiv:1108.3201.

[12] Lehmann, E.L. and Casella, G. (1998). Theory of point estimation 31.

Springer Science and Business Media.

[13] Van Der Vaart, A. W. and Wellner, J. A. (1996). Weak Conver-

gence. Springer, New York.

[14] Foster, F. G. (1953). On the stochastic matrices associated with certain

queuing processes. The Annals of Mathematical Statistics 355–360.

[15] Tweedie, R. L. (1976). Criteria for classifying general Markov chains.

Advances in Applied Probability 737–771.

25

[16] Meyn, S. P. and Tweedie, R. L. (2012). Markov chains and stochastic

stability. Springer Science and Business Media.

[17] Durrett, R. (2010). Probability: theory and examples. Cambridge uni-

versity press.

26

