6
1
0
2

 
r
a

 

M
2
1
 
 
]

.

C
O
h
t
a
m

[
 
 

1
v
6
3
9
3
0

.

3
0
6
1
:
v
i
X
r
a

Uniform value for some nonexpansive optimal control problems

with general evaluations

Xiaoxi Li∗

March 15, 2016

Abstract

We study the long-run properties of optimal control problems in continuous time, where
[16]
the running cost of a control problem is evaluated by a probability measure over R+.
introduced an asymptotic regularity condition for a sequence of probability measures to study
the limit properties of the value functions with respect to the evaluation. In the particular
case of t-horizon Cesàro mean or ρ-discounted Abel mean, this condition implies that the
horizon t tends to inﬁnity or the discount factor ρ tends to zero. For the control system
deﬁned on a compact domain and satisfying some nonexpansive condition, [16] proved the
existence of general limit value, i.e. the value function uniform converges as the evaluation
becomes more and more regular. Within the same context, we prove the existence of general
uniform value, i.e.
for any ε > 0, there is an optimal control that guarantees the general
limit value up to ε for all control problems where the cost is evaluated by suﬃciently regular
probability measures.

Keywords Optimal control, uniform value, long-run average value, general evaluation

AMS Classiﬁcation 49J15, 93C15, 37A99

1 Introduction

Let U be a compact subset of a separable metric space. A control u is a measurable function
from R+ to U . Denote by U the set of all controls. We consider the following control system:

where f : Rd × U → Rd, and y0 ∈ Rd is the initial state. Let g : Rd × U → R be the running cost
function. We make the following assumptions on f and g throughout the article:

y

′(t) = f(cid:0)y(t), u(t)(cid:1), y(0) = y0.

(1.1)

the function g : Rd × U → R is Borel measurable and bounded;
the function f : Rd × U → Rd is Borel measurable, and satisﬁes:
(∗). ∃L ≥ 0, ∀(y, y) ∈ R2d, ∀u ∈ U, ||f (y, u) − f (y, u)|| ≤ L||y − y||,
(∗∗). ∃a > 0, ∀(y, u) ∈ Rd × U, ||f (y, u)|| ≤ a(1 + ||y||).

(1.2)

∗Economics and Management School, Wuhan University, Luojia Hill, Wuchang, 430072 Wuhan, China. Email:

xxleewhu@gmail.com.




1

Then for a given initial point y0 ∈ Rd, any control u ∈ U deﬁnes a unique absolutely continuous
solution to Equation (1.1) on R+, which we denote by y(t, u, y0).

Denote by J = hU, g, f i the optimal control problem described above. ∆(R+) is the set of
probability measures on R+ and any θ ∈ ∆(R+) is called an evaluation. The θ-value of the
control problem J is deﬁned as:

Vθ(y0) = inf
u∈U

γθ(y0, u), with γθ(y0, u) =Z +∞

0

g(cid:0)y(t, u, y0), u(t)(cid:1)dθ(t).

(1.3)

Speciﬁc evaluations and the corresponding value functions include

Cesàro mean: ∀t > 0, θt has a density s 7→ fθt

(s) = 1

t 1[0,t](s), and the t-horizon value is

Vθt

(y0) = inf
u∈U

1

t Z t

0

g(cid:0)y(s, u, y0), u(s)(cid:1)ds

Abel mean: ∀ρ ∈ (0, 1], θρ has a density s 7→ fθρ(s) = ρe−ρs, and the ρ-discounted value is

Vθρ(y0) = inf

u∈UZ +∞

0

ρe−ρsg(cid:0)y(s, u, y0), u(s)(cid:1)ds

In this article, we are interested in the long-run properties of the control problem J .

In
particular cases, we look at the convergence of V¯θt as t tends to inﬁnity, the convergence of Vθρ as
ρ tends to zero, and their equality (the asymptotic approach). In case of convergence, the uniform
approach further asks for each ε > 0 the existence of ε-optimal control for all control problems
with suﬃciently large t and/or with suﬃciently small ρ. The uniform approach emphasizes the
robustness of the optimal control with respect to a long but unknown duration of the control
problem.

In the literature, the asymptotic approach has been extensively exploited. Most of the results
are obtained by assuming certain ergodic condition or controllability condition (cf. [1, 2, 3, 5, 11]).
A diﬀerent approach making no ergodic/controllability assumption is initialized by [18]. Notably,
the obtained limit value function (whenever exists) is in general dependent of the initial state. To
be more precise, they pose a compact nonexpansive assumption on the control problems, i.e. the
control system is deﬁned on a compact domain and satisﬁes certain mild nonexpansive condition.
It is proven that for any compact nonexpansive control problem, V¯θt converges uniformly (in the
initial state) as t tends to inﬁnity, and moreover for any ε > 0, ε-optimal control exists for all
t-horizon problems with suﬃciently large t.

This nonexpansive approach for the asymptotic study of the average value in optimal control
has recently been advanced in the literature toward diﬀerent directions: [7] for optimal control
and diﬀerential games linked to PDE techniques, [6] and [13] for stochastic control system, [19]
for singularly perturbed control system.

On the other hand, several Tauberian-type results are established in various contexts of op-
timal control ([17] for deterministic optimal control, [6] for stochastic optimal control, [15] for
diﬀerential games), which state that the uniform convergence of t-horizon value as t tends to

2

inﬁnity is equivalent to that of ρ-discounted value as ρ tends to zero, and both limits are equal
in case of convergence.

Motivated by those Tauberian-type results, the asymptotic study of the optimal control prob-
lems with general evaluations aims at deﬁning a unique limit value function that is independent
of the particular chosen evaluation sequence (for example, to be Cesàro mean or Abel mean).
For this aim, [16] introduced the notion of s-total variation to deﬁne an asymptotic regularity
condition for evaluation sequences. This condition is used to study the asymptotic behavior of Vθ
as θ becomes more and more regular, and in particular, it submerges the asymptotic conditions
"t → +∞" and "ρ → 0" as particular cases. In the context of compact nonexpansive control
problems, [16] proved the existence of general limit value (cf. Deﬁnition 2.2), i.e. there is uniform
convergence of Vθ as θ becomes more and more regular.

[13] also exploited the asymptotic appoach for control problems with a larger family of eval-
uations. In his paper, only a family of probability measures that are absolutely continuous with
respect to the Lebesgue measure is considered, and a diﬀerent asymptotic regularity condition
from that in [16] is introduced for the study.

Compared to the large body of literature on the asymptotic approach for control problems,
results via uniform approach are relatively rare apart from [18]. Our article follows the research
lines of [18] and [16] to conduct a uniform analysis for compact nonexpansive control problems
with general evaluations. Using the asymptotic regularity condition in [16], we prove the existence
of general uniform value (cf. Deﬁnition 2.8), i.e.
for any ε > 0, there is ε-optimal control to
guarantee the general limit value for all θ-evaluated problems with θ suﬃciently regular. This
generalizes the result of uniform approach in [18] from t-horizon average to general evaluations.

The proof for the existence of the robust optimal control is constructive. In [18], to obtain an
ε-optimal control for all long but unknown duration, the construction is by blocks such that on
each of them, the average payoﬀ is above the limit value, and the value at the starting state is
nondecreasing. New diﬃculty arises in our context since now the optimal control aims at doing
well at the same time against a much greater family of evluations, and one can not expect the
construction in [18] to work as well. To achieve this and especially to obtain a convexity condition

needed for the proof of a minmax theorem (cid:0)cf. Equation (4.11)(cid:1), we introduce randomization

on the control.

Meanwhile, there are some optimality criteria in the inﬁnite-horizon control literature that

are related to the uniform approach, such as the undiscounted criterion (cid:0)cf. Deﬁnition 3.3(cid:1),
t R t
0 g(cid:0)y(s, u, y0), u(s)(cid:1)ds,

where a total cost is deﬁned as the limiting average cost lim supt→∞
or the weighted average criterion (cf. Deﬁnition 3.5), where a total cost is deﬁned as a weighted
average of the limiting average cost and the ρ-discounted cost. We shall demonstrate that
compared with these optimal criteria for long-run control problems, our uniform approach with
general evaluations is rather strong (cf. Proposition 3.4 and Proposition 3.6).

1

The organization of the paper is as follows. In Section 2, we present the basic model and
introduce the main poblems under study. The main result is presented in Section 3. We ﬁrst
illustrate it by an example and then discuss its implications for other long-run optimality criteria.
The proof of the main result is given in Section 4, which consists of two parts: some preliminary
results concerning random controls and the main part of the proof.

3

2 Preliminaries

Let J = hU, g, f i be optimal control problem deﬁned by Equation (1.1) for which Condition
(1.2) is assumed. Let θ ∈ ∆(R+) be an evaluation for the control problem J , and let Vθ(y0) be
the θ-value of J deﬁned by Equation (1.3). The following notion is introduced in [16] for the
aim of deﬁning a regularity for θ.

Deﬁnition 2.1 For any s ≥ 0, the s-total variation of an evaluation θ is:

T Vs(θ) = max

Q∈B(R+)

|θ(Q) − θ(Q + s)|,

where B(R+) is the family of Borel subsets in R+.

Indeed, we use sup0≤s≤S T Vs(θ) for some S > 0 to measure the regularity of any evaluation θ,
and we shall conduct the asymptotic analysis and the uniform analysis for the control problem
J as sup0≤s≤S T Vs(θ) becomes suﬃciently small. The reader is refered to Section 3.3 in [16]
for an extensive discussion on this regularity condition. In particular, the vanishing condition
"sup0≤s≤S T Vs(θ) → 0 for some S > 0" corresponds to either "t → +∞" for the Cesàro mean
or "ρ → 0" for the Abel mean.

The following formal deﬁnition is concerned with the asymptotic analysis of control problems

with general evaluations.

Deﬁnition 2.2 The optimal control problem J = hU, g, f i has a general limit value given by
some function V deﬁned on Rd if: for each ε > 0 there is some η > 0 and S > 0 such that:

∀θ ∈ ∆(R+), (cid:16) sup

0≤s≤S

T Vs(θ) ≤ η =⇒(cid:0)∀y0 ∈ Rd, |Vθ(y0) − V (y0)| ≤ ε(cid:1)(cid:17).

We restrict ourselves to the study of the following class of control problems.

Deﬁnition 2.3 The optimal control problem J = hU, f, gi is compact nonexpansive if:
(A.1) the control dynamic (1.1) has a compact invariant set Y ⊆ Rd, i.e. y(t, u, y0) ∈ Y , ∀t ≥ 0
for all u ∈ U and y0 ∈ Y ;
(A.2) the running cost g does not depend on u and is continuous in y ∈ Y ;
(A.3) the control dynamic (1.1) is nonexpansive, i.e.,

∀y1, y2 ∈ Rd, sup
a∈U

inf

b∈U(cid:10)y1 − y2, f (y1, a) − f (y2, b)(cid:11) ≤ 0.

(2.1)

Remark 2.4 The nonexpansive condition (A.3) implies the following property (cf. Proposition
3.7 in [18]): ∀y1, y2 ∈ Rd, ∀u ∈ U , ∃v ∈ U s.t.

||y(t, u, y1) − y(t, v, y2)|| ≤ ||y1 − y2||, ∀t ≥ 0.

4

Consider a control problem J = hU, g, f i compact nonexpansive with the invariant set Y .
[16] (cf. Corollary 4.5) proved that J has a general limit value characterized by the following
function:

V ∗(y0) = sup

θ∈∆(R+)

inf
s∈R+

inf

u∈UZ ∞

0

g(cid:16)y(t + s, u, y0)(cid:17)dθ(t), ∀y0 ∈ Y.

(2.2)

Proposition 2.5 [16] Let J = hU, f, gi be compact nonexpansive with invariant set Y . Then
the general limit value of J exists and is equal to V ∗.

Remark 2.6 As opposed to most papers in the control literature, say for example [1, 2, 3, 5, 11],
no ergodic condition or controllability condition is assumed for compact nonexpansive control
problems. This can be underlined by the fact that in general the limit function V ∗ deﬁned in
(2.2) depends on the initial state y0.

We study in this paper a value notion that is stronger than the general limit value, namely the
general uniform value (the "uniform approach"), which requests the existence of robust controls
that is approximately optimal for all θ-evaluated conctrol problems with θ suﬃciently regular
(i.e., sup0≤s≤S T Vs(θ) suﬃciently small for some ﬁxed S > 0).

For the aim of a formal analysis of the uniform approach, we ﬁrst introduce the notion of

random control.

Deﬁnition 2.7 A random control is a pair(cid:0)(Ω, B(Ω), λ), u(cid:1), where (Ω, B(Ω), λ) is some stan-

dard Borel probability space and u : Y × Ω × [0, +∞) → U is a Borel measurable mapping (we
have extended the deﬁnition of a random control to dependent on the initial state for later use).

Denote by ˜U the set of all random controls, which is convex in the following sense. For any
u1, u2 ∈ ˜U and α ∈ [0, 1], deﬁne u = αu1 + (1 − α)u2 to take the value of u1 with probability α
and of u2 with probability (1 − α). It is easy to construct a product probability space such that
u is in ˜U .

We might shortly write Ω for the triple (Ω, B(Ω), λ). Let (Ω, u) be a random control, then
for any initial point y0 ∈ Y and any ω ∈ Ω, uω(y0, ·) := u(y0, ω, ·) deﬁned from [0, +∞) to U
is a (pure) control in U , which we denote by uω(y0). The Borel probability space Ω serves as a
random device for the controller to choose a pure control in U .

The expected θ-evaluated payoﬀ induced by any random control (Ω, u) and initial point y0 is

denoted by

γθ(y0, u) =ZΩ

γθ(cid:0)y0, uω(y0)(cid:1)dλ(ω) =ZΩZ[0,+∞)

g(cid:16)y(t, uω(y0), y0)(cid:17)dθ(t)dλ(ω),

and the expected θ-value in random controls is ˜Vθ(y0) = inf u∈ ˜U γθ(y0, u). The payoﬀ function
γθ(y0, ·) is aﬃne 1 in u, thus the value function in random controls is the same as that in pure
controls, that is, ˜Vθ(y0) = Vθ(y0) for all y0 ∈ Y and θ ∈ ∆(R+).

1To see this point, consider for example the Borel probability space to be (cid:0)[0, 1], B([0, 1]), λ(cid:1) where λ is the
2 : Y × [0, 1] × [0, +∞) → U two random controls, any α ∈ [0, 1], and let

Lebesgue measure. We take any u

1, u

5

Deﬁnition 2.8 The optimal control problem J = hU, f, gi has a general uniform value if the
general limit value exists, say equal to V ∗, and moreover for each ε > 0 there is some η > 0,
S > 0 and a random control u ∈ ˜U such that:

∀θ ∈ ∆(R+), (cid:16) sup

0≤s≤S

T Vs(θ) ≤ η =⇒(cid:0)∀y0 ∈ Y, γθ(y0, u) ≤ V ∗(y0) + ε(cid:1)(cid:17).

The control u appearing in the above deﬁnition is called an ε-optimal control for the control

problem J .

3 Main result

Our main result of this article is the following

Theorem 3.1 Assume that the optimal control problem J = hU, f, gi is compact nonexpansive.
Then it has a general uniform value V ∗.

Remark 3.2 [18] proved the existence of pure ε-optimal control for the compact nonexpansive
control problems when the running cost is evaluated by Cesàro means. Our result generalizes it
to general evaluations. Nevertheless, new diﬃculty arises and our proof techniques diﬀer from
[18] and is partially inspired by [20], where some analogous results in a discrete-time framework
are established.

It is unknown whether pure ε-optimal control exists for compact nonexpansive control problems

with general evaluations.

The rest of this section is devoted to comments of our main result. We ﬁrst present in
Subsection 3.1 a toy example of control problem that is compact nonexpansive thus general
uniform value exists. Next in Subsection 3.2, we link the general uniform value to other long-run
optimal criteria in the control literature.

3.1 Illustration of Theorem 3.1 by a toy example

This subsection contains a simple example of economic development. We show that its associated
control problem is compact nonexpansive, thus according to Theorem 3.1 the general uniform
value exists.

A society is concerned with its long-run economic development – which is denoted by a state
variable x1, and also with the accompanied pollution impact – which is denoted by a state variable
x2. Let π(x1) be the instantaneous beneﬁt (assumed to be continuous) of the development and
let c(x2) be the instantaneous cost (assumed to be continuous) of the pollution. u ∈ [0, 1] = U is

u

3 : Y × [0, 1] × [0, +∞) → U be the random control as a convex combination of u
2(y0, ω−α
2).

1(y0, ω
for any y0 ∈ Y , u
change of variables, we obtain γθ(y0, u

3(y0, ω, t) = u
1) + (1 − α) · γθ(y0, u

3(y0, ω, t) = u

α , t) for ω ∈ [0, α] and u

3) = α · γθ(y0, u

6

1 and u

2 with coeﬃcient α:
1−α , t) for ω ∈ (α, 1]. Using the

the control variable, which can be interpreted as the policy of develeopment, such as the number
of vehicles allowed to enter into the metropolis per day, or the volume of carbon released into
the atmosphere per year, etc. We denote X = (x1, x2), and the dynamic of X(t) is governed by
(A, B > 0 constant)

X ′(t) = f(cid:0)X(t), u(t)(cid:1) =(cid:18) u(t)(cid:0)A − x1(t)(cid:1)

u(t)(cid:0)B − x2(t)(cid:1) (cid:19) , ∀t ≥ 0.

Starting from X(0) = (0, 0), we have x′
2(t) ≥ 0 and x1(t) ≤ A, x2(t) ≤ B for all t ≥ 0.
u(t) is the control level at time t, which promotes a positive growth of both the develepment
x1(t) and the pollution x2(t).

1(t) ≥ 0, x′

Let us deﬁne g(X) = c(x2) − π(x1) to be the runnning cost function, then the associated
optimal control problem J = hU, f, gi satisﬁes the assumptions in (1.2). Moreover, we verify
that J is compact nonexpansive thus it has a general uniform value:

• [0, A] × [0, B] is its compact invariant set;

• g(X, u) = g(X) does not depend on u and is continuous in X;

• the control dynamic X ′(t) = f(cid:0)X(t), u(t)(cid:1) is nonexpansive:

(z1, z2), for any u ∈ [0, 1], take v = u. Then we obtain:

let X = (x1, x2) and Z =

DX − Z, f (X, u) − f (Z, u)E =*(cid:18) x1 − z1

x2 − z2 (cid:19) , u(cid:18) (cid:0) − x1 + z1(cid:1)

(cid:0) − x2 + z2(cid:1) (cid:19)+ ≤ 0.

3.2 Implications of the general uniform value to other optimality criteria

In this subsection, we link the general uniform value to other solution criteria such as the undis-
counted criterion (limiting avarage payoﬀ, cf. [9]) and the weighted average criterion (cf. [10, 14]).
As the asymptotic/uniform approach, both the undiscounted value and the limit value of the
weighted average value (when the discount factor tends to zero) are also optimality criteria for
long-run control problems. We show that the general uniform value is actually a stronger value
notion.

Below we assume that the control problem J = hU, f, gi under consideration is compact
nonexpansive, and let Y ⊆ Rd be its invariant set. Following Theorem 3.1, J has a general
uniform value V ∗.

Deﬁnition 3.3 The undiscounted value ˆV of the control problem J with limiting average
payoﬀ is deﬁned as: ∀y0 ∈ Y, ˆV (y0) = inf u∈U lim supt→∞ γt(y0, u).

Proposition 3.4 Let J = hU, f, gi be a compact nonexpansive control problems. Then ˆV is
identical with the general uniform value V ∗.

7

Proof : V ∗ is the general uniform value of J , so limt→∞ Vt(y0) = V ∗(y0), ∀y0 ∈ Y . On one
hand, Vt(y0) = inf u∈U γt(y0, u) implies that V ∗(y0) ≤ ˆV (y0). On the other hand, the existence of
the general uniform value V ∗ implies that: ∀ε > 0, ∃T0 > 0, ∃u, s.t. γt(y0, u) ≤ V ∗(y0)+ε, ∀y0 ∈
Y, ∀t ≥ T0. Thus for all ε > 0, ˆV (y0) ≤ lim supt→∞ γt(y0, u) ≤ V ∗(y0) + ε. This completes the
proof of the proposition. Q.E.D.

Deﬁnition 3.5 Let β ∈ (0, 1) and ρ ∈ (0, 1], the (β, ρ)-weighted average value of the control
problem J = hU, f, gi is deﬁned as: ∀y0 ∈ Rd,

¯Vρ,β(y0) = inf
u∈U

¯γρ,β(y0, u), where ¯γρ,β(y0, u) =(cid:26)βγρ(y0, u) + (1 − β) lim sup

t→∞

γt(y0, u)(cid:27).

Proposition 3.6 Let J = hU, f, gi be a compact nonexpansive control problem. Then for any
β ∈ (0, 1), limρ→0 ¯Vρ,β = V ∗.

Proof : We ﬁx β ∈ (0, 1) and y0 ∈ Y . For each ρ ∈ (0, 1], we use the deﬁnition of ¯Vρ,β(y0)

and Proposition 3.4 to obtain that:

¯Vρ,β(y0) = inf

u∈U(cid:26)βγρ(y0, u) + (1 − β) lim sup

t→∞

γt(y0, u)(cid:27) ≥ βVρ(y0) + (1 − β)V ∗(y0).

Thus by the fact that limρ→0 Vρ(y0) = V ∗(y0), we have:

lim inf

ρ→0

¯Vρ,β(y0) ≥ β lim inf
ρ→0

Vρ(y0) + (1 − β)V ∗(y0) = V ∗(y0).

Now we prove the other direction lim inf ρ→0 ¯Vρ,β(y0) ≤ V ∗(y0). For each pair ρ ∈ (0, 1] and
t > 0, we consider the β-weighted evaluation θ(ρ, t) whose density function is:

fθ(ρ,t)(s) = βfθρ(s) + (1 − β)f¯θt (s) = βρe−ρs + (1 − β)

1
t

1[0,t](s), ∀s ≥ 0.

V ∗ is the general uniform value of J , thus for any ε > 0, there is some η > 0, S > 0 and
∗) ≤ V ∗(y0) + ε, ∀θ ∈ ∆(R+) with T V S(θ) ≤ η. Consider now
control u
the evaluation θ to be separately (θρ) and (¯θt). An easy computation gives us (cf. Section 3.3 in
[16] for details):

∗ such that: γθ(y0, u

∀ρ ∈ (0, 1], t ≥ 0, T V S(θρ) ≤ 1 − e−Sρ and T V S(¯θt) ≤ min{S/t, 1}.

This means that there is ρ0 > 0 and T0 > 0 such that: ∀ρ ≤ ρ0, t ≥ T0,

Taking the convex combination, we obtain that: ∀(ρ, t) ∈ (0, ρ0] × [T0, +∞),

T V S(θρ) ≤ η and T V S(¯θt) ≤ η, thus T V S(cid:0)θ(ρ, t)(cid:1) ≤ η.

∗) ≤ V ∗(y0) + ε.
We take lim sup as t tends to inﬁnity and use the deﬁnition of ¯Vρ,β to obtain:

∗) + (1 − β)γt(y0, u

∗) = γθ(ρ,t)(y0, u

βγρ(y0, u

(3.1)

¯Vρ,β(y0) ≤ βγρ(y0, u

∗) + (1 − β) lim sup

t→∞

γt(y0, u

∗) ≤ V ∗(y0) + ε.

Finally, taking lim inf as ρ tends to zero on both sides of the above inequality yields:

Since ε > 0 can be arbitrarily, the proof is then complete. Q.E.D.

lim inf

ρ→0

¯Vρ,β(y0) ≤ V ∗(y0) + ε.

8

Remark 3.7 Equation (3.1) implies that the ε-optimal control for the general uniform value V ∗
is also ε-optimal for all (ρ, β)-weighted average problems provided that ρ is small enough. In
particular, the choice of the weight parameter β is irrelevant.

Note that our proof of the above two results replies only on the existence of the general
uniform value V ∗ but not on the particular properties of the compact nonexpansive conditions.

4 Proof of Theorem 3.1

4.1 Some preliminary results

We introduce several further notations concerning random controls. Our use of random con-
trol/strategy in continuous-time control/game problem follows [8] (see [4] for the introduction of
randomized strategies for inﬁnite games). We are going to work on a set S of probability spaces,
which is stable by countable product. To ﬁx the ideas, choose

S = {([0, 1]n, B([0, 1]n), λn) , for some n ∈ N∗ ∪ {∞}},

where B([0, 1]n) is the class of Borel sets of [0, 1]n, λn is the Lebesgue measure on Rn for n < ∞
and B([0, 1]∞) is the product Borel-ﬁeld, λ∞ is the product measure for n = ∞.

We might write simply u ∈ ˜U without explicitly mentioning the underlying probability space.

distribution of the state at time t. Any z ∈ ∆(Y ) is a probability distribution over Y . Let z be

For any (y0, u) ∈ Y × ˜U and t ≥ 0, we denote ˜y(t, u, y0) = RΩ δy(t,uω (y0),y0)dλ(ω) for the
the distribution of the initial state, then t 7→ ˜y(t, u, z) = RY RΩ δy(t,uω (y0),y0)dλ(ω)dz(y0) is the

expected trajectory in ∆(Y ) with respect to z. The above notations are consistent when a point
y0 ∈ Y is identiﬁed with the Dirac measure δy0 ∈ ∆(Y ).

For any ˜y ∈ ∆(Y ), let g(˜y) = Rp∈Y g(p)d˜y(p) be the aﬃne extension of g. Using Fubini’s

theorem, we have: for any y0 ∈ Y and u ∈ ˜U ,

γθ(y0, u) =Z[0,+∞)ZΩ

g(cid:16)y(t, uω(y0), y0)(cid:17)dλ(ω)dθ(t) =Z[0,+∞)

g(cid:16)˜y(t, u, y0)(cid:17)dθ(t).

Before proceeding to the proof of Theorem 3.1, we establish in the rest of this subsection

several preliminary results concerning the nonexpansive property of the dynamic in ∆(Y ).

Let dKR be the Kantorovich-Rubinstein distance on ∆(Y ):

∀z, z′ ∈ ∆(Y ), dKR(z, z′) = sup

,

hdz′(cid:12)(cid:12)(cid:12)(cid:12)

where Lip(1) denotes the set of bounded 1-Lipschitz functions deﬁned on Y .

h∈Lip(1)(cid:12)(cid:12)(cid:12)(cid:12)
ZY

hdz −ZY

9

Lemma 4.1 For any z1, z2 in ∆(Y ) and u : Y × Ω × [0, +∞) → U a random control, there
exists some random control v : Y × ˆΩ × Ω × [0, +∞) → U such that

dKR(cid:16)˜y(t, u, z1), ˜y(t, v, z2)(cid:17) ≤ dKR(z1, z2), ∀t ≥ 0.

Proof : We ﬁx u : Y × Ω × [0, +∞) → U a random control deﬁned on the probability space

(cid:0)Ω, B(Ω), λ(cid:1). We ﬁrst show the result for z1 and z2 being Dirac measures. Let p, q in Y . uω(p) :

[0, +∞) → U is a pure control for any ω ∈ Ω. By the nonexpansive assumption, Proposition 3.6
p(q, ω, ·) : [0, +∞) → U , which
in [18] implies that: for given ω ∈ Ω, there is some pure control u
we denote by u

p(q, ω, ·), such that

p
ω(q) := u

Moreover, one can take the mapping (q, t) 7→ u

p(q, ω, t) jointly measurable2. Letting

(cid:13)(cid:13)(cid:13)
y(cid:0)t, uω(p), p(cid:1) − y(cid:0)t, u

p

ω(q), q(cid:1)(cid:13)(cid:13)(cid:13)

p
ω(q, t) := u

≤(cid:13)(cid:13)p − q(cid:13)(cid:13), ∀t ≥ 0.

(4.1)

p

u

p(q, t) =(cid:0)u

ω(q, t)(cid:1)ω∈Ω for all t ≥ 0, this deﬁnes a random control:

u

p : Y × Ω × [0, +∞) 7→ U
p
(q, ω, t) 7→ u
ω(q, t).

From Equation (4.1), we deduce that: for any t ≥ 0,

dKR(cid:16)˜y(t, u, p), ˜y(t, u

p, q)(cid:17) = dKR(cid:16)ZΩ
≤ZΩ(cid:13)(cid:13)(cid:13)

≤ ||p − q||.

δy(t,uω (p),p)dλ(ω),ZΩ
y(cid:0)t, uω(p), p(cid:1) − y(cid:0)t, u

p

ω(q), q(cid:1)(cid:13)(cid:13)(cid:13)

δy(t,up

ω (q),q)dλ(ω)(cid:17)

dλ(ω)

(4.2)

Consider now z1, z2 in ∆(Y ). By the Kantorovich-Rubinstein duality formula (cf. Theorem 5.10
in [22]), there is a coupling ξ(·, ·) ∈ ∆(Y × Y ) with ﬁrst marginal z1 and second marginal z2,
satisfying:

dKR(z1, z2) =ZY ×Y (cid:13)(cid:13)p − q(cid:13)(cid:13)dξ(p, q).

(4.3)

Let q 7→ ξ(·|q) ∈ ∆(Y ) be the conditional distribution of ξ on its ﬁrst marginal. Consider now
the Borel isomorphic mapping (between two standard Borel spaces)

where

Deﬁne now

hq : ˆΩ =(cid:16)[0, 1], B(cid:0)[0, 1](cid:1), λ(cid:17) 7→(cid:16)Y, B(Y ), ξ(·|q)(cid:17)

∀B ∈ B(Y ), ξ(B|q) = λ(cid:0)h−1

q (B)(cid:1) .

v : Y × ˆΩ × Ω × [0, +∞)

7→ U

(q, ˆω, ω, t)

7→ v(ˆω,ω)(q, t) = u

hq(ˆω)
ω

(q, t),

2Indeed, consider for (2.1): we ﬁx any y2 ∈ Y and a· : x 7→ ax ∈ U measurable. Then we can apply the
measurable selection theorem (cf. Theorem 9.1 in [23]) for the optimization problem inf b∈U (cid:10)y1 − y2, f (y1, ay1 ) −
f (y2, b)(cid:11) to obtain the existence of a measurable mapping b· : x 7→ bx ∈ U satisfying (cid:10)y1 − y2, f (y1, ay1 ) −
f (y2, by1 )(cid:11) ≤ 0, ∀y1 ∈ Y . The same augument as in the proof of Proposition 3.6 in [18] implies the result.

10

which is jointly measurable as the composition function of u

·
ω(q, t) and hq(ˆω). (Ωv, v) is then

a random control deﬁned on the product Borel probability space (cid:0) ˆΩ ⊗ Ω, B( ˆΩ ⊗ Ω), λ2(cid:1). The

interpretation of the random control v is that: at each initial point q ∈ Y , v(q, ·) randomly takes
the value u

p(q, ·) according to the probability law dξ(p|q).

Next, we check that v satisﬁes the nonexpansive condition. For any t ≥ 0 and q ∈ supp(z2),
we use Fubini’s theorem (ﬁrst on ˆΩ×Ω and then on Y ×Y ) and the change of variable "p = hq(ˆω)"
to obtain:

˜y(t, v, z2) =ZY

We then deduce that:

δ

δ

hq ( ˆω)
ω

hq ( ˆω)
ω

δ
y(t,up

(q),q(cid:1)dλ2(ˆω, ω)dz2(q)
(q),q(cid:1)dλ(ˆω)#dλ(ω)dz2(q)
ω(q),q)dλ(ω)#)dξ(p|q)dz2(q)

˜y(cid:0)t, v, q(cid:1)dz2(q) =ZY Z ˆΩ×Ω
y(cid:0)t,u
=ZY Z ˆΩ"ZΩ
y(cid:0)t,u
=ZY ZY "ZΩ
=ZY ×Y
p, q(cid:1)dξ(p, q).
˜y(cid:0)t, u
˜y(cid:0)t, u, p(cid:1)dz1(p), ZY ×Y
p, q(cid:1)dξ(p, q)(cid:17)
˜y(cid:0)t, u
˜y(t, u, p)dξ(p, q), ZY ×Y
p, q(cid:1)dξ(p, q)(cid:1)
˜y(cid:0)t, u
p, q(cid:1)(cid:17)dξ(p, q)

dKR(cid:16)˜y(cid:0)t, u, p(cid:1), ˜y(cid:0)t, u

dKR(cid:16)˜y(t, u, z1), ˜y(t, v, z2)(cid:17) = dKR(cid:16)ZY
= dKR(cid:0)ZY ×Y
≤ZY ×Y
(cid:16)by (4.2)(cid:17) ≤ZY ×Y
(cid:16)by (4.3)(cid:17) = dKR(z1, z2).

This completes our proof for the lemma. Q.E.D.

||p − q||dξ(p, q)

The proof of Theorem 3.1 involves some compact property of the set of random controls for
compact non-expansive control problem, as we will show in Lemma 4.7: the "limit trajectory"
in ∆(Y ) (cf. Lemma 4.6) of a sequence of random controls can be arbitrarily approximated by
the trajectory induced by one random control. Here the random control that we are going to
construct is deﬁned as concatenations of certain sequence of random controls, namely behavior
control. Formal deﬁnitions are as follows.

Deﬁnition 4.2 For any two random controls (Ωu, u) and (Ωv, v) and a time T > 0. The
concatenation of u and v at time T is deﬁned as the random control (Ωu × Ωv, u ⊕T v) with:

∀(cid:0)y0, (ω1, ω2), t(cid:1) ∈ Y × (Ωu × Ωv) × [0, +∞),

[u ⊕T v](ω1,ω2)(y0, t) = 1{t<T }uω1(y0, t) + 1{t≥T }vω2(cid:16)y(cid:0)y0, uω1(y0), T(cid:1), t − T(cid:17).

11

Deﬁnition 4.3 Fix 0 = t0 < t1 · ·· < tm < · · · a partition of [0, +∞), and (Ωm, u

m)m≥1 a

sequence of random controls. Let (cid:0) ⊗m

inductively as: u

1 and u

[1] = u

[m+1] = u

m′=1 Ωm′, u

[m](cid:1)m≥1 be a sequence of random controls deﬁned

m+1 for any m ≥ 1. The behavior control

[m] ⊕tm u

is deﬁned as the concatenations of (u

m) sequentially at points (tm):

(cid:0)Ω[∞], u

[∞](cid:1) :=(cid:0) ⊗m≥1 Ωm, u

1 ⊕t1 · · ·u

t ⊕tm · · ·(cid:1)

for any (cid:0)y0, (ωm)m≥1, t(cid:1) in Y × Ω[∞] × [0, +∞),

u

[∞]
(ωm)m≥1

(y0, t) = Xm≥1

1{tm−1≤t<tm}u

[m]
ωm(y0, t), where ωm := (ω1, ..., ωm).

Remark 4.4 The behavior control (Ω[∞], u
space Ω[∞] = ⊗m≥1Ωm, which, as a countable union, is still in S.

[∞]) is also a random control with the product Borel

Remark 4.5 On the other hand, from Kuhn’s theorem (cf.
for any random
control, one can construct a behavior control such that the trajectories in ∆(Y ) induced by them

[4], Section 5):

are the same. More precisely, we ﬁx (tm)m≥0 a partition of [0, +∞),(cid:0)Ωu, u(cid:1) a random control and
y0 ∈ Y an initial state. Then, there exists some behavior control (cid:0)⊗m≥1 Ωm, u(cid:1) as concatenations
m(cid:1)m≥1 at points (tm) such that starting from y0, the
of some sequence of random controls (cid:0)Ωm, u

trajectories in ∆(Y ) generated by both u and u are the same, i.e. ˜y(t, u, y0) = ˜y(t, u, y0),
∀t ≥ 0, a.e.

sequence of random controls (u

k)k≥1 with the same initial point y0.

k, y0)(cid:1)k≥1 is the sequence of trajectories in ∆(Y ) generated by a
Fix any y0 in Y . (cid:0)˜y(·, u
Deﬁnition 4.6 A measurable mapping t 7→ y(t) from R+ to(cid:0)∆(Y ), dKR(cid:1) is a limit trajectory
of the sequence (cid:0)˜y(·, u

k, y0)(cid:1)k≥1 if there is some ψ(k) such that for any m ≥ 1, ˜y(·, u

converges (for dKR) to y(·) uniformly on [0, m] as k tends to inﬁnity.

ψ(k), y0)

We ﬁrst show that the limit trajectory exists for any sequence.

Lemma 4.7 (cid:0)˜y(·, u

k, y0)(cid:1)k≥1 has a limit trajectory in ∆(Y ).

Proof : Fix an m ≥ 0, we look at the restriction of each ˜y(·, u

k, y0) on the compact interval
k, y0) : k ≥ 1} are continuous mappings from [m, m + 1] to

[m, m + 1]. Then the family {˜y(·, u

the compact domain (cid:0)∆(Y ), dKR(cid:1). One can use Ascoli’s theorem to deduce the existence of a
ψ(k), y0)(cid:1)k≥1 on [m, m + 1]. To obtain this, it is suﬃcient
uniform convergent subsequence (cid:0)˜y(·, u
k, y0) : k ≥ 1(cid:9)(cid:0)restricted on [m, m + 1](cid:1) is equicontinuous.
for us to prove that the family (cid:8)˜y(·, u

We ﬁx any k ≥ 1 and s, t ∈ [m, m + 1]. Then by the deﬁnition of dKR, we deduce that

dKR(cid:16)˜y(t, u

k, y0), ˜y(s, u

k, y0)(cid:17) ≤ZΩ(cid:13)(cid:13)(cid:13)

y(cid:0)t, u
≤ a(cid:0)1 + sup

y∈Y

12

k

ω(y0), y0(cid:1) − y(cid:0)s, u
kyk(cid:1)|t − s|,

k

ω(y0), y0(cid:1)(cid:13)(cid:13)(cid:13)

dλ(ω)

(4.4)

k

ω(y0), y0(cid:1) is absolutely contin-
where we have used in the last inequality the fact that t 7→ y(cid:0)t, u
uous (cid:0)cf. assumptions in (1.2) (cid:1) . As Y ⊆ Rd is compact, Equation (4.4) proves that the family
k, y0) : k ≥ 1(cid:9) (cid:0)restricted on [m, m + 1](cid:1) is equicontinuous. By extracting subsequences
(cid:8)˜y(·, u
k, y0)(cid:1)k≥1. Q.E.D.
for each m, we obtain the existence of a limit trajectory of (cid:0)˜y(·, u
k, y0)(cid:1)k. Then for any ε > 0,
Lemma 4.8 Let y(·) : t 7→ y(t) be a limit trajectory of (cid:0)˜y(·, u

∗ whose trajectory in ∆(Y ) is ε-close to y(·) along time, i.e.,

there is some behavior control u

∀ε > 0, ∃u

∗ ∈ ˜U , s.t. dKR(cid:16)˜y(t, u

∗, y0), y(t)(cid:17) ≤ ε, ∀t ≥ 0.

Proof : The idea is to construct a behavior control u

∗ by consecutive intervals, such that on
k} whose trajectory is close to the
each of them, u
limit y. The proof relies on the nonexpansive property established in Lemma 4.1, which ensures
that by iteration, the trajectory generated by u

∗ follows one random control in the family {u

∗ is close to y on the whole R+.

Let ε > 0 be ﬁxed. The behavior control u

∗ will be constructed as concatenations of a

sequence of random controls (ˆu

Km) (to be speciﬁed later on) at points {1, 2, 3, ...}.

By deﬁnition, t 7→ y(t) is a limit trajectory of (cid:0)˜y(·, u

m ≥ 0, there exists some Km+1 > 0 such that:

k, y0)(cid:1)k in ∆(Y ) for dKR, so for each

dKR(cid:16)˜y(t, u

Km+1, y0), y(t)(cid:17) ≤ εm+1, ∀t ∈ [m, m + 1].

(4.5)

Following Remark 4.5, we could have assumed that each u

Km+1 is a behavior control, and let

Km+1 : Y × Ω × [0, +∞) → U be the component of u

Km+1 on interval [m, m + 1].

u

In order to deﬁne the behavior control u

∗ , it is suﬃcient to construct by induction a sequence

of random controls (ˆu

Km)m≥1 such that for all m ≥ 1,

dKR(cid:16)˜y(t, u

[m], y0), y(t)(cid:17) ≤ 2

m

Xℓ=1

εℓ, ∀t ∈ [0, m],

where

[1] = ˆu

K1 and u

[m] = ˆu

u

K1 ⊕1 · · · ⊕m−1 ˆu

Km, m ≥ 2.

For m = 1, let ˆu

K1 = u

[0, 1]. This initializes our induction.

K1, then from Equation (4.5), dKR(cid:16)˜y(t1, u

[1], y0), y(t)(cid:17) ≤ ε, ∀t ∈

Assume that ˆu

K1, ..., ˆu

Km are deﬁned and let u

[m] = ˆu

m = 1) satisfy:

K1 ⊕1 · · · ⊕m−1 ˆu

Km (u

[1] = ˆu

dKR(cid:16)˜y(t, u

[m], y0), y(t)(cid:17) ≤ 2

m

Xl=1

εl, ∀t ∈ [0, m].

K1 for

(4.6)

Next we construct the control ˆu
do this, we consider the two distributions ˜y(m, u

Km+1 thus complete the deﬁnition of u

[m], y0) and ˜y(m, u

[m+1] on [m, m + 1). To
Km+1, y0). Take t = m in

13

Equation (4.5) and in Equation (4.6), we use the triangle inequality obtain a bound on the
distance between them:

dKR(cid:16)˜y(m, u

[m], y0), ˜y(m, u

Km+1, y0)(cid:17) ≤ 2

m

Xl=1

εl + εm+1.

(4.7)

Consider then the random control u
Lemma 4.1 to deduce the existence of some random control ˆu
˜y(m, u

[m], y0) such that:

Km+1 on the initial distribution ˜y(m, u

Km+1, y0), and apply
Km+1 on the starting distribution

dKR(cid:16)˜y(cid:0)∆, ˆu
≤ dKR(cid:16)˜y(m, u

m

Km+1, ˜y(m, u

[m], y0), ˜y(m, u

[m], y0)(cid:1), ˜y(∆, ¯u
Km+1, y0)(cid:17)

≤ 2

εl + εm+1, ∀∆ ∈ [0, 1].

Xl=1

Km+1, ˜y(m, u

Km+1, y0)(cid:17)

By deﬁnition of u

Km+1, we have that for all ∆ ∈ [0, 1],

Deﬁne now ˆu

[m+1] = ˆu

[m] ⊕m ˆu

Km+1. This gives us:

Km+1, ˜y(m, u

Km+1, ˜y(m, u

˜y(cid:0)∆, u

˜y(cid:0)∆, ˆu

Km+1, y0)(cid:1) = ˜y(cid:0)∆ + m, u
[m], y0)(cid:1) = ˜y(cid:0)∆ + m, u

Km+1, y0(cid:1).
[m+1], y0(cid:1).

(4.8)

(4.9)

(4.10)

We substitute Equation (4.9) and Equation (4.10) back into Equation (4.8) and use the change

the variable to obtain that:

∀t ∈ [m, m + 1], dKR(cid:16)˜y(t, u

[m+1], y0), ˜y(t, u

Km+1, y0)(cid:17) ≤ 2

εl + εm+1.

m

Xl=1

Finally, with the help of deﬁnition of Km+1 in Equation (4.5), we obtain that:
[m, m + 1],

for any t ∈

dKR(cid:16)˜y(t, u

[m+1], y0), ¯y(t)(cid:17) ≤ 2

m

Xl=1

εl + εm+1 + εm+1 = 2

εl.

m+1

Xl=1

This ﬁnishes the inductive deﬁnition of the sequence (cid:0)ˆu

To conclude, we set the behavior control

Km(cid:1)m≥1.

∗ = ˆu

u

K1 ⊕1 · · ·ˆu

Km ⊕m · · ·,

as concatenations of (cid:0)ˆu

Km(cid:1)m≥1 at points {m ≥ 1}, and by our inductive construction:

∞

∀t ≥ 0, dKR(cid:16)˜y(t, u

∗, y0), ¯y(t)(cid:17) ≤ 2

εℓ =

Xℓ=1

2ε

1 − ε

≤ 3ε,

as long as ε ∈ (0, 1

3 ]. This completes our proof for the lemma by considering ε′ = ε/3. Q.E.D.

14

4.2 Main part of the proof

This section is devoted for the proof of Theorem 3.1, which is divided into three parts.

Part A aims at establishing certain optimality properties for a sequence of controls (Lemma
4.9); In Part B, we use the compact nonexpansive property (Lemma 4.8) to obtain a "limit"
control of the above sequence ensuring that the average cost on each (consecutive) block of ﬁxed

length is no more than V ∗ (cid:0)Equation (4.18)(cid:1); Part C concludes the proof through a comparison

of the (normalized) θ-evaluated payoﬀ to the average cost by blocks.

Part A. For any t ≥ 0 , S > 0, y0 ∈ Y and u ∈ ˜U , denote

For T ≥ 0, we put

γt,S(y0, u) =

1

S Z[t,t+S]

g(cid:0)˜y(s, u, y0)(cid:1)ds, ∀y0 ∈ Y.

ϕT,S(y0) = inf
u∈ ˜U

sup

µ∈∆([0,T ])Z[0,T ]

γt,S(y0, u)dµ(t).

Fix any T, S, y0. We ﬁrst prove a minmax result for ϕT,S(y0). We denote for each s ≥ 0:

βs(µ, S) =

1

S Z min{T,s}

max{0, s−S}

dµ(t) =

1
S

µ(cid:16)[0, T ] ∩ [s − S, s](cid:17).

Then from the deﬁnition of γt,S(y0, u), we obtain that

γt,S(y0, u)dµ(t) = Z[0,T ]  1
Z[0,T ]
(′′Fubini’s theorem′′) = Z[0,T +S]

βs(µ, S)g (˜y(s, u, y0)) ds.

S Z[t,t+S]

g(cid:0)˜y(s, u, y0)(cid:1)ds!dµ(t)

Note that for each ﬁxed S > 0 and µ ∈ ∆([0, T ]), the mapping t 7→ βt(µ, S) deﬁnes a density

function of some evaluation over R+, which we denote by ς(µ, S). This enables us to write

ϕT,S(y0) = inf
u∈ ˜U

sup

µ∈∆([0,T ])

γς(µ,S)(y0, u).

Next, we use Sion’s minmax theorem (cf. Appendix A.3 in [21]) to show that the operators
"inf" and "sup" of the above equation commute. Indeed, ˜U is convex; ∆([0, T ]) is convex and
weak-* compact and the payoﬀ function (µ, u) 7→ γς(µ,S)(y0, u) is aﬃne in both µ and u; moreover
the function γt,S(y0, u) is continuous in t for given u (g is continuous in y and each trajectory is
absolutely continuous), and so is u 7→ γς(µ,S)(y0, u). Then we obtain:

ϕT,S(y0) = sup

µ∈∆([0,T ])

inf
u∈ ˜U

γς(µ,S)(y0, u) = sup

µ∈∆([0,T ])

Vς(µ,S)(y0).

(4.11)

15

Lemma 4.9 For any ε > 0, there is some S0 > 0 such that

∀T ≥ 0, ∃u

T ∈ ˜U : ∀y0 ∈ Y, γt,S0(y0, u

T ) ≤ V ∗(y0) + ε, ∀t ≤ T.

Proof : Let θ ∈ ∆(R+) be an evaluation that is absolutely continuous with respect to the
Lebesgue measure, thus it admits a density function t 7→ fθ(t). For any s ≥ 0, we denote

Is(θ) =R[0,+∞)(cid:12)(cid:12)fθ(t + s) − fθ(t)(cid:12)(cid:12)dt. Then one obtains that (cf. Lemma 3.3 in [16]):
For any S > 0, T ≥ 0 and µ ∈(cid:0)[0, T ](cid:1), we apply the above expression for ς(µ, S) so as to obtain

the following bound: ∀s ∈ [0, S],

Is(θ)/2 ≤ T Vs(θ) ≤ Is(θ).

1

=

Is(cid:0)ς(µ, S)(cid:1)
S Z[0,T +S]hµ(cid:0)[t − S, t − S + s] ∩ [0, T ](cid:1) + µ(cid:0)[t, t + s] ∩ [0, T ](cid:1)idt
=Z[0,T +S]Z[t−S,t−S+s]∩[0,T ]

dµ(s′)dt +Z[0,T +S]Z[t,t+s]∩[0,T ]
·(cid:16)µ(cid:0)[−S, T + s] ∩ [0, T ](cid:1) + µ(cid:0)[0, s + T + S] ∩ [0, T ](cid:1)(cid:17)

dµ(s′)dt

=

≤

s
S
2s
S

,

(4.12)

where we have used Fubini’s theorem to obtain the third equality.

According to Proposition 2.5, the general limit value exists and is equal to V ∗, i.e. for any

ε > 0, we take η > 0 and S′ > 0 such that:

∀θ ∈ ∆(R+),   sup

0≤s≤S′

T V (θ) ≤ η′ =⇒ (∀y0 ∈ Y, |Vθ(y0) − V ∗(y0)| ≤ ε/2)! .

(4.13)

Take S0 = max{ 2S′

η′ , S′}. From Equation (4.12), we obtain that:

∀s ∈ [0, S′], S ≥ S0, T > 0, µ ∈ ∆(cid:0)[0, T ](cid:1), T Vs (ς(µ, S)) ≤

thus by Equation (4.13): ∀y0 ∈ Y, |Vς(µ,S)(y0) − V ∗(y0)| ≤ ε/2.

2s
S

≤

2S′
S0

≤ η′,

Next, from Equation (4.11), ϕT,S(y0) = supµ∈∆([0,T ]) Vς(µ,S)(y0), we deduce that

∀ε > 0, ∃S0 > 0 : ∀S ≥ S0, ∀T ≥ 0, ∀y0 ∈ Y, |ϕT,S (y0) − V ∗(y0)| ≤ ε/2.

(4.14)

Finally, according to the deﬁnition ϕT,S(y0) = inf u supµ γς(µ,S)(y0, u), we have

∀T > 0, ∃u

T ∈ ˜U : ∀y0 ∈ Y, γt,S0(y0, u

T ) ≤ ϕT,S0(y0) + ε/2, ∀t ∈ [0, T ].

Together with Equation 4.14, one obtains

∀ε > 0, ∃S0 > 0 : ∀T ≥ 0, ∃u

T ∈ ˜U : ∀y0 ∈ Y, γt,S0(y0, u

T ) ≤ V ∗(y0) + ε, ∀t ≤ T.

(4.15)

16

The proof of the lemma is then complete. Q.E.D.

T ∈ ˜U for any T > 0
Part B. Fix now any ε > 0, and consider S0 > 0 and the random control u
given as in Lemma 4.9. We take an increasing sequence (Tk)k≥1 in R+ and ﬁx any y0 ∈ Y . For
Tk in ∆(Y ). Thus from Equation (4.15), we
each k ≥ 1, t 7→ ˜y(t, u
obtain:

Tk , y0) is the trajectory of u

γt,S0(y0, u

Tk ) =

1

S0 Z[t,t+S0]

g(cid:0)˜y(t, u

Tk , y0)(cid:1) ds ≤ V ∗(y0) + ε for all t ≤ Tk.
Tk , y0)(cid:1)k≥1 i.e.

there is a
Tψ(k), y0) converges uniformly to ¯y(·) on each [m, m + 1]. Since
subsequence ψ(k) such that ˜y(·, u
g is continuous on the compact invariant set Y , and ∆(Y ) is weak-* compact for the topology

Let ¯y(·) : t 7→ ¯y(t) be a limit trajectory of the sequence (cid:0)˜y(·, u
induced by the distance dKR (cid:0)cf. Theorem 6.9 in [22](cid:1), we let k tend to inﬁnity (cid:0)along the
subsequence ψ(k)(cid:1) in Equation (4.16) to get

(4.16)

(4.17)

1

S0 Z[t,t+S0]

g(cid:0)¯y(s)(cid:1)ds ≤ V ∗(y0) + ε, ∀t ≥ 0.

Now we apply Lemma 4.8 for the sequence (cid:0)˜y(·, u

obtain the existence of some behavior control u

Tk , y0)(cid:1)k≥1 and its limit trajectory ¯y(·) to

∗ such that:

Together with Equation (4.17), we obtain:

∗, y0), ¯y(t)(cid:1) ≤ ε, ∀t ≥ 0.

dKR(cid:0)˜y(t, u
S0 Z[t,t+S0]

1

γt,S0(y0, u

∗) =

g(cid:0)˜y(s, u

∗, y0)(cid:1)ds ≤ V ∗(y0) + 2ε, ∀t ≥ 0.

(4.18)

Part C. Let θ ∈ ∆(R+) be any evaluation. We integrate Equation (4.18) over t ≥ 0 with
respect to θ, to obtain:

V ∗(y0) + 2ε ≥Z[0,+∞)

γt,S0(y0, u

∗)dθ(t)

=Z[0,+∞)  1

S0 Z[t,t+S0]

g (˜y(s, u

∗, y0)) ds! dθ(t)

(4.19)

(′′Fubini’s theorem′′) =Z[0,+∞)

=γς(θ,S0)(y0, u

∗),

βs(θ, S0)g (˜y(s, u

∗, y0)) ds

where βs(θ, S0) = 1
its density function.

S0 R s

max{0,s−S0} dθ(t), ∀s ≥ 0, and ς(θ, S0) is the evaluation with s 7→ βs(θ, S0)

Next, we show that

|γθ(y0, u

∗) − γς(θ,S0)(y0, u

∗)| ≤ sup

Q∈B(R+)

|θ(Q) − ς(θ, S0)(Q)| ≤ 2T VS0(θ).

(4.20)

17

Indeed, the ﬁrst inequality follows from Hahn’s decomposition theorem applied to the sign

measure "θ − ς(θ, S0)" (cid:0)cf. Lemma 3.7 in [16](cid:1). Let Q be any Borel set on R+. We write

dθ(t) for all s ≥ 0 by considering θ as a probability measure over [−S0, 0)∪R+

βs(θ, S0) = 1
null on [−S0, 0). We have

s−S0

S0 R s
ς(θ, S0)(Q) =Zs∈Q

βs(θ, S0)ds =

(′′Fubini’s theorem′′) = Zt∈Q−S0  1

1

S0 Zs∈Q Zt∈[s−S0,s]
S0 Zs∈[t,t+S0]

dθ(t)! ds
ds! dθ(t)

Thus we have |θ(Q) − ς(θ, S0)(Q)| = |θ(Q) − θ(Q − S0)| ≤ θ([0, S0)) + T VS0(θ) ≤ 2T VS0(θ). This
proves Equation (4.20) by taking the supremum over Q ∈ B(R+).

= θ(Q − S0).

Finally, we substitute Equation (4.19) into Equation (4.20), to obtain:

γθ(y0, u

∗) ≤ V ∗(y0) + 2T VS0(θ) ≤ V ∗(y0) + 3ε,

for all θ ∈ ∆(R+) with sup0≤s≤S0 T Vs(θ) ≤ ε.

To conclude, we have obtained that: ∀ε, ∃S0 > 0, ∃u

∗ ∈ ˜U ,

∀θ ∈ ∆(R+), (cid:16) sup

0≤s≤S0

T Vs(θ) ≤ ε =⇒ γθ(y0, u

∗) ≤ V ∗(y0) + 3ε, ∀y0 ∈ Y(cid:17).

As ε > 0 is arbitrary, this proves Theorem 3.1 by taking "η = ε" and "S = S0".

Acknowledgments

The main results in this article forms Chapter 4 of my PhD thesis submitted to Université
Pierre et Marie Curie - Paris 6, 2015 September. I wish to thank my supervisor Sylvain Sorin
for numerous helpful comments. Discussions with Marc Quincampoix, Fabien Gensbittel and
Marco Mazzola are also acknowledged. Thanks go to Cheng Wan for her careful proofreading
on part of this manuscript. Part of the work was done when the author was an ATER fellow
at Université Paris-1 (UFR 06) during the academic year 2014-2015, and at Université Cergy-
Pontoise (THEMA & UFR économie et gestion) during the academic year 2015-2016.

References

[1] O. Alvarez and M. Bardi, Ergodicity, stabilization, and singular pertubations for Bellman-

Isaacs equations, Mem. Amer. Math. Soc., (204)2010, vi+77 pp.

[2] M. Arisawa, Ergodic problem for the Hamilton-Jacobi-Belmann equations, Ann. Henri

Poincaré, Analyse Nonlinéaire, (14)1997, pp. 415–438.

18

[3] M. Arisawa and P. L. Lions, On ergodic stochastic control, Comm. Partial Diﬀerential

Equations, (23)1998, pp. 2187–2217.

[4] R. Aumann, Mixed and behavior strategies in inﬁnite extensive games, in Advances in Game
Theory, Annals of Mathematics Studies 52, M. Dresher and L.S. Shapley eds., 1964, pp.
627–650.

[5] A. Bensoussan, Perturbation Methods in Optimal Control, Wiley/Gauthiers-Villas, Chich-

ester, 1988.

[6] R. Buckdahn, D. Goreac and M. Quincampoix, Existence of asymptotic values for
nonexpansive stochastic control systems, Applied Mathematics and Optimization, (70)2014,
pp. 1–28.

[7] P. Cannarsa P. and M. Quincampoix, Vanishing discount limit and nonexpansive op-
timal control and diﬀerential games, SIAM Journal on Control and Optimization, 53(2007),
pp. 1789–1814.

[8] P. Cardaliaguet, Diﬀerential games with asymmetric information, SIAM Journal on Con-

trol and Optimization, 46(2007), pp. 818–838.

[9] D. Carlson, A. Haurie and A. Leizarowitz, Inﬁnite Horizon Optimal Control: Deter-

ministic and Stochastic Systems, 2nd ed., Springer, 1991.

[10] J. Filar and O.J. Vrieze, Weighted reward criteria in competitive Markov decision pro-

cesses, Zeitschrift fur Operations Research, 36(1992), pp. 343–358.

[11] V. Gaitsgory, On the use of the averaging method in control problems, (Russian) Diﬀer-

entsialnye Uravneniya, 22(1986), pp. 1876–1886.

[12] D. Goreac, Asymptotic control for a class of piecewise deterministic markov processes

associated to temperate viruses, SIAM J. Control Optimization, 53(2015), pp. 1860–1891.

[13] D. Goreac, A note on general Tauberian-type results for controlled stochastic dynamics,

Electronic Communications in Probability, 20(2015), pp. 1–12.

[14] A. Haurie, Turnpikes in multidiscount rate environments and GCC policy evaluation, in

Optimal Control and Diﬀerential Games, G.Zaccour (ed.), Kluwer, (2002), pp. 39–52.

[15] D. Khlopin, On uniform Tauberian theorems for dynamic games, arXiv:1412.7331, 2015.

[16] X. Li, M. Quincampoix and J. Renault, Limit value for optimal control with general

means, Discrete and Continuous Dynamical Systems–Series A, (36)2016, pp. 2113–2132.

[17] M. Oliu-Barton and G. Vigeral, A uniform Tauberian theorem in optimal control, in
Advances in Dynamic Games, Annals of the International Society of Dynamic Games, P.
Cardaliaguet and R. Cressman eds., Birkhauser, 12(2013), pp. 199–215.

[18] M. Quincampoix and J. Renault, On the existence of a limit value in some nonexpansive
optimal control problems, SIAM Journal on Control and Optimization, 49(2011), pp. 2118–
2132.

[19] M. Quincampoix and S. Hayk, Averaging problem for weakly coupled nonexpansive con-
trol systems, Nonlinear Analysis: Theory, Methods & Applications, 113(2015), pp. 147–158.

19

[20] J. Renault and X. Venel, A distance on some probability spaces, with applications to

Markov decision processes and repeated games, hal:00674998, 2013.

[21] S. Sorin, A First Course on Zero-Sum Repeated Games, Springer, 2002.

[22] C. Villani, Optimal Transportation: Old and New, Springer, 2009.

[23] D.H. Wagner, Survey of measurable selection theorems, SIAM Journal on Control and

Optimization, 15(1977), pp. 859–903.

20

