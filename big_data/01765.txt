6
1
0
2

 
r
a

M
5

 

 
 
]

.

A
N
h
t
a
m

[
 
 

1
v
5
6
7
1
0

.

3
0
6
1
:
v
i
X
r
a

ACCURATE PRINCIPAL COMPONENT ANALYSIS VIA

A FEW ITERATIONS OF ALTERNATING LEAST SQUARES

ARTHUR SZLAM, ANDREW TULLOCH, AND MARK TYGERT

Abstract. A few iterations of alternating least squares with a random starting point provably
suﬃce to produce nearly optimal spectral- and Frobenius-norm accuracies of low-rank approximations
to a matrix; iterating to convergence is unnecessary. Thus, software implementing alternating least
squares can be retroﬁtted via appropriate setting of parameters to calculate nearly optimally accurate
low-rank approximations highly eﬃciently, with no need for convergence.

Key words. low-rank approximation, principal component analysis, alternating least squares,

alternating minimization, randomized algorithm

1. Introduction. Low-rank approximations are popular throughout the sciences
and engineering, often in the form of principal component analysis, and converting
any low-rank approximation to a singular value decomposition or principal component
analysis is trivial and eﬃcient, as detailed, for example, by [2], [1], or [3]. To calculate
an accurate approximation to a matrix A, we consider the low-rank approximations

A ≈ S0T0, A ≈ S1T1, A ≈ S2T2,

. . . ,

(1.1)

with Si being a tall and skinny matrix and Ti being a short and fat matrix, produced
via iterations starting from S0 — iterations called “alternating least squares” by [4]
(among others): for each i = 0, 1, 2, . . . , having Si already, we obtain Ti minimizing
the norm

kSiTi − Ak,

then, having Ti already, we obtain Si+1 minimizing the norm

kSi+1Ti − Ak,

(1.2)

(1.3)

where these norms denote the spectral or Frobenius norms (the Frobenius norm of
a matrix is the square root of the sum of the squares of the absolute values of the
entries of the matrix); speciﬁcally, we use the minimizers

and

for i = 0, 1, 2, . . . , where

and

Ti = S(−1)

i A

Si+1 = AT (−1)

i

S(−1)
i

= (S∗

i Si)−1S∗
i

T (−1)
i

= T ∗

i (TiT ∗

i )−1

(1.4)

(1.5)

(1.6)

(1.7)

(see Section 2 below for precise deﬁnitions, particularly for the inverse and pseudo-
inverse). The appendix reviews the well-known fact that these minimize both the
spectral and Frobenius norms.

1

Following [2], we demonstrate that the approximations attain high accuracy after
just a few of these iterations. Speciﬁcally, the remainder of the present paper has
the following structure: Section 2 sets notational conventions used throughout the
paper. Via mathematical analysis, Section 3 proves the high accuracy. Section 4
illustrates the high accuracy via numerical examples with a Matlab prototype available
at http://tygert.com/software.html

The accompanying prototype comes complete with a comprehensive collection of
tests, but is all in Matlab. The present paper provides a fully rigorous basis for more
general software packages implementing alternating least squares to be retroﬁtted
via appropriate setting of parameters to calculate nearly optimally accurate low-rank
approximations, with no need to wait for convergence.

2. Notation. This section sets our notational conventions. For any full-rank
square matrix A, we use A−1 to denote the inverse of A. For any rank-deﬁcient
square matrix A, we use A−1 to denote the pseudoinverse of A; the pseudoinverse of
A is the matrix representing the inverse of A with its domain restricted to the row
space of A (plus the identically zero map restricted to the null space of A). Needless
to say, for a full-rank square matrix, the pseudoinverse is the same as the inverse.

For any matrix A, we denote by A∗ the adjoint (that is, the conjugate transpose)

of A, so that the spectral norm of A is given by the action of A on vectors via

kAk2 = q max

v : v∗v=1

v∗A∗Av,

(2.1)

and the Frobenius norm kAkF of A is the square root of the sum of the squares of
the absolute values of the entries of A. The spectral and Frobenius norms of a vector
viewed as a matrix with a single row or column are the same, and are also known as
the Euclidean norm of the vector. A deﬁnition equivalent to (2.1) is

kAk2 = max

v : kvk2=1

kAvk2,

(2.2)

where the norms of the vectors are the Euclidean norms.

3. Analysis of accuracy. This section demonstrates that the procedure —
alternating least squares — described in the introduction produces a highly accurate
approximation SiTi to the given matrix A even for a small number i of iterations,
provided that S0 is one of the random matrices used by [2] (for example, the entries
of S0 can be independent and identically distributed standard normal variates). The
demonstration is simply a reduction to the proof of accuracy for similar algorithms
by [2]; we leave the brunt of the proof (together with a discussion of the intuitions
behind the proof) to [2]. We begin by proving several lemmas.

The following lemma provides an explicit expression for Si from the iterations

in (1.4) and (1.5).

Lemma 3.1. Given matrices A and S0 for the iterations in (1.4) and (1.5), the

matrix Si coming from those iterations can be expressed as

Si = (AA∗)iS0B0B1B2 · · · Bi−1

for i = 1, 2, 3, . . . , where we deﬁne

Bi = (S∗

i AA∗Si)−1S∗

i Si.

2

(3.1)

(3.2)

Proof. Combining (1.4)–(1.7) and (3.2) yields

Si+1 = AA∗Si(S∗

i AA∗Si)−1S∗

i Si = AA∗SiBi

(3.3)

for i = 0, 1, 2, . . . . Iterating the recurrence in (3.3) yields (3.1).

The following lemma follows straightforwardly from using singular value decom-

positions.

Lemma 3.2. Suppose that A, S, and T are matrices such that

where

T = S(−1)A,

S(−1) = (S∗S)−1S∗.

Then, the ranks of S∗A, T , and AT ∗ are all equal.
Proof. Combining (3.4) and (3.5) yields that

and

T = (S∗S)−1S∗A

T A∗ = (S∗S)−1S∗AA∗.

We form the full singular value decompositions

and

A = UAΣAV ∗
A

S = USΣSV ∗
S ,

(3.4)

(3.5)

(3.6)

(3.7)

(3.8)

(3.9)

where UA, VA, US, and VS are unitary, and all entries of ΣA and ΣS are nonnegative
and are zero oﬀ the main diagonals. Combining (3.6)–(3.9) yields

S∗A = VSΣ∗

SU ∗

SUAΣAV ∗
A,

and

T = VSΣ(−1)

S U ∗

SUAΣAV ∗
A,

T A∗ = VSΣ(−1)

S U ∗

SUAΣ(2)

A U ∗
A,

(3.10)

(3.11)

(3.12)

S

is the same as Σ∗

where Σ(−1)
reciprocals, and where Σ(2)
of the diagonal entries of ΣA on its diagonal.

A = ΣAΣ∗

S, but replacing its nonzero diagonal entries with their
A is the square diagonal matrix with the squares

Combining (3.10)–(3.12) and the fact that UA, VA, US, and VS are unitary yields

rank(S∗A) = rank(Σ∗

SW ΣA),

rank(T ) = rank(Σ(−1)

S W ΣA),

3

(3.13)

(3.14)

and

rank(T A∗) = rank(Σ(−1)

S W Σ(2)
A ),

where W is the unitary matrix

W = U ∗

SUA.

(3.15)

(3.16)

The claim stated in the lemma (that the ranks of S∗A, T , and AT ∗ are all equal)
then follows from the combination of (3.13)–(3.15) and the facts that Σ(−1)
S W ΣA is
the same as Σ∗
SW ΣA with its rows rescaled by nonzero multiples (so that they have
the same row space), and that (assuming A is square) Σ(−1)
A is the same as
Σ(−1)
S W ΣA with its columns rescaled by nonzero multiples (so that they have the
same column space); of course, the rank of a matrix is equal to the dimension of
its row space (which is the same as the dimension of its column space). If A is not
square, then either Σ(−1)
S W ΣA augmented by columns of
zeros and with its columns rescaled by nonzero multiples or Σ(−1)
S W ΣA is the same as
S W Σ(2)
Σ(−1)
A augmented by columns of zeros and with its columns rescaled by nonzero
multiples (so that again they have the same column space, which is the same as the
column space of the original, unaugmented Σ(−1)

A is the same as Σ(−1)

S W Σ(2)

S W Σ(2)

S W ΣA or Σ(−1)

S W Σ(2)
A ).

The adjoint of the preceding lemma is the following.
Corollary 3.3. Suppose that A, S, and T are matrices such that

where

S = AT (−1),

T (−1) = T ∗(T T ∗)−1.

(3.17)

(3.18)

Then, the ranks of AT ∗, S, and S∗A are all equal.
The following lemma follows from using both Lemma 3.2 and Corollary 3.3.
Lemma 3.4. Given the matrices A, Ti, and Si+1 from (1.4) and (1.5), the ranks
0 A, T0, AT ∗
3 , . . . are

2 A, T2, AT ∗

3 A, T3, AT ∗

1 A, T1, AT ∗

2 , S3, S∗

of S∗
all equal.

0 , S1, S∗

1 , S2, S∗

Proof. This lemma follows from induction on i = 0, 1, 2, . . . , using Lemma 3.2

and Corollary 3.3.

The following theorem follows from (3.1) and Lemma 3.4.
Theorem 3.5. Given matrices A and S0 for the iterations in (1.4) and (1.5),
the column space of the matrix Si coming from those iterations is the same as the
column space of (AA∗)iS0, for i = 1, 2, 3, . . . .

Proof. As seen from (3.1), the column space of Si is a subspace of the column
space of (AA∗)iS0. Moreover, the row space of (AA∗)iS0 is a subspace of the row
space of A∗S0, so

rank((AA∗)iS0) ≤ rank(A∗S0).

(3.19)

As already mentioned, the column space of Si is a subspace of the column space

of (AA∗)iS0; if the subspace were not the whole space, then

rank(Si) < rank((AA∗)iS0),

(3.20)

4

and then combining (3.19) and (3.20) would yield

rank(Si) < rank(A∗S0) = rank(S∗

0 A),

(3.21)

contradicting Lemma 3.4 . . . thus, the claim stated in the theorem must be true.

Finally, calculating Ti minimizing (1.2) constructs the best approximation SiTi to
A such that the column space of the approximation lies in the column space of Si —
which is the same as the column space of (AA∗)iS0, as Theorem 3.5 proves — where
“best” means minimizing the discrepancy in the spectral norm, which is the same
as minimizing the discrepancy in the Frobenius norm, as reviewed in the appendix.
This produces a highly accurate approximation SiTi to A even for a small number i
of iterations, as proven by [2], provided that S0 is one of the random matrices used
by [2] (for example, the entries of S0 can be independent and identically distributed
standard normal variates) — iterating until convergence is unnecessary.

4. Numerical examples. This section presents several numerical experiments
on an implementation in Matlab of the algorithm (alternating least squares) discussed
in the introduction. Although the numerical experiments discussed here are somewhat
limited in order to keep the presentation succinct, the codes together with software
extensively testing them are available at http://tygert.com/software.html

We consider various values for positive integers m and n, as speciﬁed in the
captions for Tables 4.1–4.3, and calculate rank-k approximations to the m × n matrix

A = F ΣG,

(4.1)

where F and G are m × m and n × n unitary discrete Fourier transforms, respectively,
and Σ is an m × n matrix whose entries are all zeros except for the diagonal entries

for i = 1, 2, . . . , k, and

Σi,i = δ⌊i/2⌋/(k/2)

Σi,i = δ ·

min{m, n} − i

min{m, n} − k − 1

(4.2)

(4.3)

for i = k + 1, k + 2, . . . , min{m, n} (⌊i/2⌋ is the greatest integer less than or equal to
i/2); the tables below specify various values for k and δ. Thus, the spectral norm of
A is 1:

kAk2 = 1.

(4.4)

5

The headings of Tables 4.1–4.3 have the following meanings:

• j is the number of iterations conducted.
• k is the rank of the approximation constructed — the number of columns in

Si from (1.2), which is also the number of rows in Ti from (1.2).

• δ is the spectral-norm accuracy of the best possible rank-k approximation.
• ǫ is the spectral-norm accuracy of the calculated rank-k approximation, with
the spectral-norm accuracy computed via 100 iterations of the power method.
• t is the time in seconds required to compute the approximation (without using
any fast Fourier transforms to leverage the special structure of the matrix A).
The tables illustrate the importance of using at least one (preferably two or more)
iterations, as then the accuracy (ǫ) of the computed approximation is nearly the best
possible (δ). The accuracies are indeed excellent, even with just a couple iterations.
The timings scale as expected, roughly in proportion to the number of entries in the
matrices; we used Matlab version R2015B on an Apple MacBook Pro with a 2.6 GHz
Intel Core i7 processor.

Table 4.1

m = 2048, n = 4096

j
0
1
2
10
0
1
2
10
0
1
2
10
0
1
2
10

k
2
2
2
2
10
10
10
10
2
2
2
2
10
10
10
10

δ

1e-03
1e-03
1e-03
1e-03
1e-03
1e-03
1e-03
1e-03
1e-11
1e-11
1e-11
1e-11
1e-11
1e-11
1e-11
1e-11

ǫ

t

5.8e-01
8.1e-01
1.2e+00
4.5e+00
7.5e-01
1.4e+00
2.1e+00
8.0e+00
4.1e-01
7.8e-01
1.2e+00
4.3e+00
7.5e-01
1.4e+00
2.1e+00
8.0e+00

1.4e-02
1.0e-03
1.0e-03
1.0e-03
1.8e-02
1.2e-03
1.0e-03
1.0e-03
1.3e-10
1.0e-11
1.0e-11
1.0e-11
2.4e-10
1.0e-11
1.0e-11
1.0e-11

6

j

0
1
2
10
0
1
2
10
0
1
2
10
0
1
2
10

j

0
1
2
10
0
1
2
10
0
1
2
10
0
1
2
10

k

2
2
2
2
10
10
10
10
2
2
2
2
10
10
10
10

k

2
2
2
2
10
10
10
10
2
2
2
2
10
10
10
10

Table 4.2

m = 4096, n = 4096

δ

1e-03
1e-03
1e-03
1e-03
1e-03
1e-03
1e-03
1e-03
1e-11
1e-11
1e-11
1e-11
1e-11
1e-11
1e-11
1e-11

ǫ

t

1.5e-02
1.0e-03
1.0e-03
1.0e-03
2.2e-02
1.3e-03
1.0e-03
1.0e-03
2.6e-10
1.0e-11
1.0e-11
1.0e-11
4.2e-10
1.0e-11
1.0e-11
1.0e-11

9.6e-01
1.9e+00
2.8e+00
1.0e+01
1.8e+00
3.6e+00
5.4e+00
2.1e+01
9.6e-01
1.9e+00
2.9e+00
1.0e+01
1.8e+00
3.6e+00
5.5e+00
2.0e+01

Table 4.3

m = 4096, n = 8192

δ

1e-03
1e-03
1e-03
1e-03
1e-03
1e-03
1e-03
1e-03
1e-11
1e-11
1e-11
1e-11
1e-11
1e-11
1e-11
1e-11

ǫ

t

2.0e+00
3.7e+00
5.7e+00
2.0e+01
3.8e+00
7.5e+00
1.1e+01
4.1e+01
1.8e+00
3.8e+00
5.6e+00
2.1e+01
3.6e+00
7.1e+00
1.1e+01
4.0e+01

1.2e-02
1.0e-03
1.0e-03
1.0e-03
2.1e-02
1.4e-03
1.0e-03
1.0e-03
1.5e-10
1.0e-11
1.0e-11
1.0e-11
5.3e-10
1.0e-11
1.0e-11
1.0e-11

7

Appendix A. Common minimizers for the spectral & Frobenius norms.
This appendix reviews the fact that, given matrices A and S, one matrix T

minimizing the norm

with the norm being the spectral norm or the Frobenius norm, is

kST − Ak,

T = S(−1)A,

where the so-called “pseudoinverse” of S is

S(−1) = (S∗S)−1S∗,

with the inverse and pseudoinverse deﬁned in Section 2.

Indeed, for the spectral norm, for any T — not just that in (A.2),

(A.1)

(A.2)

(A.3)

.

(A.4)

2

2

(A.5)

(A.6)

(A.7)

(A.8)

kST − Ak2

2 = k(ST − A)∗(ST − A)k2
I − SS(−1) (cid:19) (ST − A)(cid:19)∗(cid:18) SS(−1)
= (cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
I − SS(−1) (cid:19) (ST − A)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2
(cid:18)(cid:18) SS(−1)
(cid:18) ST − SS(−1)A
(cid:18) SS(−1)
= (cid:13)(cid:13)(cid:13)(cid:13)
I − SS(−1) (cid:19) (ST − A)(cid:13)(cid:13)(cid:13)(cid:13)
SS(−1)A − A (cid:19)(cid:13)(cid:13)(cid:13)(cid:13)
= (cid:13)(cid:13)(cid:13)(cid:13)
(cid:18) ST − SS(−1)A
SS(−1)A − A (cid:19)(cid:13)(cid:13)(cid:13)(cid:13)
2 ≤ (cid:13)(cid:13)(cid:13)(cid:13)

≤ kST − SS(−1)Ak2

2

2

2

2

The deﬁnition of the spectral norm in (2.1) yields

kSS(−1)A − Ak2

2 + kSS(−1)A − Ak2
2.

Combining (A.4) and (A.5) yields

kSS(−1)A − Ak2 ≤ kST − Ak2 ≤ qkST − SS(−1)Ak2

2 + kSS(−1)A − Ak2
2,

so that (A.1) is minimal for the spectral norm when

ST = SS(−1)A.

For the Frobenius norm, for any T ,

kST − Ak2

F =

n

Xk=1

kSt[k] − a[k]k2
2,

where t[1], t[2], . . . , t[n] are the columns of T , and a[1], a[2], . . . , a[n] are the columns
of A. Using the above result for the spectral norm with t[k] replacing T and with a[k]
replacing A, the right-hand side of (A.8) is minimal when

St[k] = SS(−1)a[k]

(A.9)

for k = 1, 2, . . . , n, which happens to be equivalent to (A.7) for the full A for all its
columns simultaneously.

8

Thus, for both the spectral and Frobenius norms, (A.1) is minimal when (A.7)

holds, and (A.7) clearly holds for T deﬁned in (A.2).

[A similar argument uses the identity

(ST − A)∗(ST − A) −(cid:16)(I − SS(−1))A(cid:17)∗

(I − SS(−1))A

= (ST − A)∗(ST − A) − A∗(I − SS(−1))A = (ST − A)∗SS(−1)(ST − A),

the fact that the right-hand side of (A.10) is nonnegative deﬁnite, and the relations

= (S∗(ST − A))∗ (S∗S)−1 (S∗(ST − A)) ,

(A.10)

kST − Ak2

2 = max

v : v∗v=1

v∗(ST − A)∗(ST − A)v,

(A.11)

k(I − SS(−1))Ak2

2 = max

v : v∗v=1

v∗(cid:16)(I − SS(−1))A(cid:17)∗

(I − SS(−1))Av,

(A.12)

kST − Ak2

F =

n

Xk=1

(e[k])∗(ST − A)∗(ST − A)e[k],

(A.13)

and

k(I − SS(−1))Ak2

F =

n

Xk=1

(e[k])∗(cid:16)(I − SS(−1))A(cid:17)∗

(I − SS(−1))Ae[k],

(A.14)

where e[1], e[2], . . . , e[n] are the unit basis vectors, with e[k] being the column vector
of all zeros, except for its kth entry, which is 1.]

REFERENCES

[1] M. Gu, Subspace iteration randomization and singular value problems, SIAM J. Sci. Comput.,

37 (2015), pp. A1139–A1173.

[2] N. Halko, P.-G. Martinsson, and J. Tropp, Finding structure with randomness: probabilistic
algorithms for constructing approximate matrix decompositions, SIAM Review, 53 (2011),
pp. 217–288.

[3] D. Woodruff, Sketching as a Tool for Numerical Linear Algebra, vol. 10 of Foundations and

Trends in Theoretical Computer Science, Now publishers, 2014.

[4] F. W. Young, Y. Takane, and J. de Leeuw, The principal components of mixed measurement
level multivariate data: an alternating least squares method with optimal scaling features,
Psychometrika, 43 (1978), pp. 279–281.

9

