6
1
0
2

 
r
p
A
 
4

 
 
]

.

C
O
h
t
a
m

[
 
 

2
v
0
6
1
6
0

.

3
0
6
1
:
v
i
X
r
a

Stochastic Variance Reduction for Nonconvex Optimization

Sashank J. Reddi

Ahmed Hefny

sjakkamr@cs.cmu.edu

ahefny@cs.cmu.edu

Carnegie Mellon University

Carnegie Mellon University

Suvrit Sra

suvrit@mit.edu

Barnab´as P´ocz´os

bapoczos@cs.cmu.edu

Massachusetts Institute of Technology

Carnegie Mellon University

Alex Smola

alex@smola.org

Carnegie Mellon University

Original circulated date: 5th February, 2016.

Abstract

We study nonconvex ﬁnite-sum problems and analyze stochastic variance reduced gradient
(Svrg) methods for them. Svrg and related methods have recently surged into prominence for
convex optimization given their edge over stochastic gradient descent (Sgd); but their theoret-
ical analysis almost exclusively assumes convexity. In contrast, we prove non-asymptotic rates
of convergence (to stationary points) of Svrg for nonconvex optimization, and show that it is
provably faster than Sgd and gradient descent. We also analyze a subclass of nonconvex prob-
lems on which Svrg attains linear convergence to the global optimum. We extend our analysis
to mini-batch variants of Svrg, showing (theoretical) linear speedup due to mini-batching in
parallel settings.

1

Introduction

We study nonconvex ﬁnite-sum problems of the form

n(cid:88)

i=1

min
x∈Rd

f (x) :=

1
n

fi(x),

(1)

where neither f nor the individual fi (i ∈ [n]) are necessarily convex; just Lipschitz smooth (i.e.,
Lipschitz continuous gradients). We use Fn to denote all functions of the form (1). We optimize
such functions in the Incremental First-order Oracle (IFO) framework (Agarwal & Bottou, 2014)
deﬁned below.
Deﬁnition 1. For f ∈ Fn, an IFO takes an index i ∈ [n] and a point x ∈ Rd, and returns the pair
(fi(x),∇fi(x)).

1

Algorithm

Sgd

GradientDescent

Svrg

Msvrg

Nonconvex

O(cid:0)1/2(cid:1)
O(cid:0)1/2(cid:1)
n/)(cid:1)
O(cid:0)n + (n2/3/)(cid:1)
O(cid:0)n + (
O(cid:0)min(cid:8)1/2, n2/3/(cid:9)(cid:1) O(cid:0)min(cid:8)1/2,
n/(cid:9)(cid:1)

O (n/)
√

O (n/)

√

Convex

Gradient Dominated

O(cid:0)1/2(cid:1)

O(cid:0)(n + n2/3τ ) log(1/)(cid:1)

O (nτ log(1/))

−

Fixed Step Size?

×
√
√
×

Table 1: Table comparing the IFO complexity of diﬀerent algorithms discussed in the paper. The complexity
is measured in terms of the number of oracle calls required to achieve an -accurate solution (see Deﬁnition 2).
Here, by ﬁxed step size, we mean that the step size of the algorithm is ﬁxed and does not dependent on 
(or alternatively T , the total number of iterations). The complexity of gradient dominated functions refers
to the number of IFO calls required to obtain -accurate solution for a τ -gradient dominated function (see
Section 2 for the deﬁnition). For Sgd, we are not aware of any speciﬁc results for gradient dominated
functions. Also, [f (x0) − f (x∗)] and (cid:107)x0 − x∗(cid:107) (where x0 is the initial point and x∗ is an optimal solution
to (1)) are assumed to be constant for a clean comparison. The results marked in red are the contributions
of this paper.

IFO based complexity analysis was introduced to study lower bounds for ﬁnite-sum problems.
Algorithms that use IFOs are favored in large-scale applications as they require only a small amount
ﬁrst-order information at each iteration. Two fundamental models in machine learning that proﬁt
from IFO algorithms are (i) empirical risk minimization, which typically uses convex ﬁnite-sum
models; and (ii) deep learning, which uses nonconvex ones.

The prototypical IFO algorithm, stochastic gradient descent (Sgd)1 has witnessed tremendous
progress in the recent years. By now a variety of accelerated, parallel, and faster converging versions
are known. Among these, of particular importance are variance reduced (VR) stochastic meth-
ods (Schmidt et al., 2013; Johnson & Zhang, 2013; Defazio et al., 2014a), which have delivered
exciting progress such as linear convergence rates (for strongly convex functions) as opposed to sub-
linear rates of ordinary Sgd (Robbins & Monro, 1951; Nemirovski et al., 2009). Similar (but not
same) beneﬁts of VR methods can also be seen in smooth convex functions. The Svrg algorithm
of (Johnson & Zhang, 2013) is particularly attractive here because of its low storage requirement in
comparison to the algorithms in (Schmidt et al., 2013; Defazio et al., 2014a).
Despite the meteoric rise of VR methods, their analysis for general nonconvex problems is largely
missing. Johnson & Zhang (2013) remark on convergence of Svrg when f ∈ Fn is locally strongly
convex and provide compelling experimental results (Fig. 4 in (Johnson & Zhang, 2013)). However,
problems encountered in practice are typically not even locally convex, let alone strongly convex. The
current analysis of Svrg does not extend to nonconvex functions as it relies heavily on convexity
for controlling the variance. Given the dominance of stochastic gradient methods in optimizing
deep neural nets and other large nonconvex models, theoretical investigation of faster nonconvex
stochastic methods is much needed.

Convex VR methods are known to enjoy the faster convergence rate of GradientDescent but
with a much weaker dependence on n, without compromising the rate like Sgd. However, it is not
clear if these beneﬁts carry beyond convex problems, prompting the central question of this paper:

For nonconvex functions in Fn, can one achieve convergence rates faster than both Sgd
and GradientDescent using an IFO? If so, then how does the rate depend on n and
on the number of iterations performed by the algorithm?

Perhaps surprisingly, we provide an aﬃrmative answer to this question by showing that a careful
selection of parameters in Svrg leads to faster convergence than both Sgd and GradientDescent.

1We use ‘incremental gradient’ and ‘stochastic gradient’ interchangeably, though we are only interested in ﬁnite-sum

problems.

2

To our knowledge, ours is the ﬁrst work to improve convergence rates of Sgd and GradientDes-
cent for IFO-based nonconvex optimization.

Main Contributions. We summarize our main contributions below and also list the key results

convergence of nonconvex Svrg (see Corollary 2).

in Table 1.
• We analyze nonconvex stochastic variance reduced gradient (Svrg), and prove that it has faster
rates of convergence than GradientDescent and ordinary Sgd. We show that Svrg is faster
than GradientDescent by a factor of n1/3 (see Table 1).
• We provide new theoretical insights into the interplay between step-size, iteration complexity and
• For an interesting nonconvex subclass of Fn called gradient dominated functions (Polyak, 1963;
Nesterov & Polyak, 2006), we propose a variant of Svrg that attains a global
linear rate of
convergence. We improve upon many prior results for this subclass of functions (see Section 3.1).
To the best of our knowledge, ours is the ﬁrst work that shows a stochastic method with linear
convergence for gradient dominated functions.
• We analyze mini-batch nonconvex Svrg and show that it provably beneﬁts from mini-batching.
Speciﬁcally, we show theoretical linear speedups in parallel settings for large mini-batch sizes. By
using a mini-batch of size b (< n2/3), we show that mini-batch nonconvex Svrg is faster by a
factor of b (Theorem 7). We are not aware of any prior work on mini-batch ﬁrst-order stochastic
methods that shows linear speedup in parallel settings for nonconvex optimization.
• Our analysis yields as a byproduct a direct convergence analysis for Svrg for smooth convex
• We examine a variant of Svrg (called Msvrg) that has faster rates than both GradientDescent

functions (Section 4).

and Sgd.

1.1 Related Work

Convex. Bertsekas (2011) surveys several incremental gradient methods for convex problems. A key
reference for stochastic convex optimization (for min Ez[F (x, z)]) is (Nemirovski et al., 2009). Faster
rates of convergence are attained for problems in Fn by VR methods, see e.g., (Defazio et al., 2014a;
Johnson & Zhang, 2013; Schmidt et al., 2013; Koneˇcn´y et al., 2015; Shalev-Shwartz & Zhang, 2013;
Defazio et al., 2014b). Asynchronous VR frameworks are developed in (Reddi et al., 2015). Agarwal
& Bottou (2014); Lan & Zhou (2015) study lower-bounds for convex ﬁnite-sum problems. Shalev-
Shwartz (2015) prove linear convergence of stochastic dual coordinate ascent when the individual fi
(i ∈ [n]) are nonconvex but f is strongly convex. They do not study the general nonconvex case.
Moreover, even in their special setting our results improve upon theirs for the high condition number
regime.

Nonconvex. Sgd dates at least to the seminal work (Robbins & Monro, 1951); and since then
it has been developed in several directions (Poljak & Tsypkin, 1973; Ljung, 1977; Bottou, 1991;
Kushner & Clark, 2012).
In the (nonsmooth) ﬁnite-sum setting, Sra (2012) considers proximal
splitting methods, and analyzes asymptotic convergence with nonvanishing gradient errors. Hong
(2014) studies a distributed nonconvex incremental ADMM algorithm.

These works, however, only prove expected convergence to stationary points and often lack
analysis of rates. The ﬁrst nonasymptotic convergence rate analysis for Sgd is in (Ghadimi & Lan,
2013), who show that Sgd ensures (cid:107)∇f(cid:107)2 ≤  in O(1/2) iterations. A similar rate for parallel and
distributed Sgd was shown recently in (Lian et al., 2015). GradientDescent is known to ensure
(cid:107)∇f(cid:107)2 ≤  in O(1/) iterations (Nesterov, 2003, Chap. 1.2.3).

The ﬁrst analysis of nonconvex Svrg seems to be due to Shamir (2014), who considers the special
problem of computing a few leading eigenvectors (e.g., for PCA); see also the follow up work (Shamir,
2015). Finally, we note another interesting example, stochastic optimization of locally quasi-convex
functions (Hazan et al., 2015), wherein actually a O(1/2) convergence in function value is shown.

3

2 Background & Problem Setup

We say f is L-smooth if there is a constant L such that

(cid:107)∇f (x) − ∇f (y)(cid:107) ≤ L(cid:107)x − y(cid:107),

∀ x, y ∈ Rd.

Throughout, we assume that the functions fi in (1) are L-smooth, so that (cid:107)∇fi(x) − ∇fi(y)(cid:107) ≤
L(cid:107)x − y(cid:107) for all i ∈ [n]. Such an assumption is very common in the analysis of ﬁrst-order methods.
Here the Lipschitz constant L is assumed to be independent of n. A function f is called λ-strongly
convex if there is λ ≥ 0 such that

f (x) ≥ f (y) + (cid:104)∇f (y), x − y(cid:105) + λ

2(cid:107)x − y(cid:107)2 ∀x, y ∈ Rd.

The quantity κ := L/λ is called the condition number of f , whenever f is L-smooth and λ-strongly
convex. We say f is non-strongly convex when f is 0-strongly convex.
where a function f is called τ -gradient dominated if for any x ∈ Rd

We also recall the class of gradient dominated functions (Polyak, 1963; Nesterov & Polyak, 2006),

f (x) − f (x∗) ≤ τ(cid:107)∇f (x)(cid:107)2,

(2)
where x∗ is a global minimizer of f . Note that such a function f need not be convex; it is also easy
to show that a λ-strongly convex function is 1/2λ-gradient dominated.
We analyze convergence rates for the above classes of functions. Following Nesterov (2003);
Ghadimi & Lan (2013) we use (cid:107)∇f (x)(cid:107)2 ≤  to judge when is iterate x approximately stationary.
Contrast this with Sgd for convex f , where one uses [f (x) − f (x∗)] or (cid:107)x − x∗(cid:107)2 as a convergence
criterion. Unfortunately, such criteria cannot be used for nonconvex functions due to the hardness
of the problem. While the quantities (cid:107)∇f (x)(cid:107)2 and f (x) − f (x∗) or (cid:107)x − x∗(cid:107)2 are not comparable
in general (see (Ghadimi & Lan, 2013)), they are typically assumed to be of similar magnitude.
Throughout our analysis, we do not assume n to be constant, and report dependence on it in our
results. For our analysis, we need the following deﬁnition.
Deﬁnition 2. A point x is called -accurate if (cid:107)∇f (x)(cid:107)2 ≤ . A stochastic iterative algorithm is
said to achieve -accuracy in t iterations if E[(cid:107)∇f (xt)(cid:107)2] ≤ , where the expectation is over the
stochasticity of the algorithm.

We introduce one more deﬁnition useful in the analysis of Sgd methods for bounding the variance.
Deﬁnition 3. We say f ∈ Fn has a σ-bounded gradient if (cid:107)∇fi(x)(cid:107) ≤ σ for all i ∈ [n] and x ∈ Rd.

2.1 Nonconvex SGD: Convergence Rate

Stochastic gradient descent (Sgd) is one of the simplest algorithms for solving (1); Algorithm 1 lists
its pseudocode. By using a uniformly randomly chosen (with replacement) index it from [n], Sgd

Algorithm 1 SGD

t=0

Input: x0 ∈ Rd, Step-size sequence: {ηt > 0}T−1
for t = 0 to T − 1 do

Uniformly randomly pick it from {1, . . . , n}
xt+1 = xt − ηt∇fit (x)

end for

uses an unbiased estimate of the gradient at each iteration. Under appropriate conditions, Ghadimi
& Lan (2013) establish convergence rate of Sgd to a stationary point of f . Their results include the
following theorem.

4

√
Theorem 1. Suppose f has σ-bounded gradient; let ηt = η = c/
x∗ is an optimal solution to (1). Then, the iterates of Algorithm 1 satisfy

T where c =

(cid:114)

E[(cid:107)∇f (xt)(cid:107)2] ≤

min

0≤t≤T−1

2(f (x0) − f (x∗))L

T

σ.

(cid:113) 2(f (x0)−f (x∗))

Lσ2

, and

√
For completeness we present a proof in the appendix. Note that our choice of step size η requires
knowing the total number of iterations T in advance. A more practical approach is to use a ηt ∝ 1/
t
or 1/t. A bound on IFO calls made by Algorithm 1 follows as a corollary of Theorem 1.

Corollary 1. Suppose function f has σ-bounded gradient, then the IFO complexity of Algorithm 1
to obtain an -accurate solution is O(1/2).

√
As seen in Theorem 1, Sgd has a convergence rate of O(1/

T ). This rate is not improvable in
general even when the function is (non-strongly) convex (Nemirovski & Yudin, 1983). This barrier
is due to the variance introduced by the stochasticity of the gradients, and it is not clear if better
rates can be obtained Sgd even for convex f ∈ Fn.

3 Nonconvex SVRG

We now turn our focus to variance reduced methods. We use Svrg (Johnson & Zhang, 2013), an
algorithm recently shown to be very eﬀective for reducing variance in convex problems. As a result,
it has gained considerable interest in both machine learning and optimization communities. We seek
to understand its beneﬁts for nonconvex optimization. For reference, Algorithm 2 presents Svrg’s
pseudocode.

Observe that Algorithm 2 operates in epochs. At the end of epoch s, a full gradient is calculated
at the point ˜xs, requiring n calls to the IFO. Within its inner loop Svrg performs m stochastic
updates. The total number of IFO calls for each epoch is thus Θ(m + n). For m = 1, the algorithm
reduces to the classic GradientDescent algorithm. Suppose m is chosen to be O(n) (typically
used in practice), then the total IFO calls per epoch is Θ(n). To enable a fair comparison with Sgd,
we assume that the total number of inner iterations across all epochs in Algorithm 2 is T . Also
note a simple but important implementation detail: as written, Algorithm 2 requires storing all the
(0 ≤ t ≤ m). This storage can be avoided by keeping a running average with respect to
iterates xs+1
the probability distribution {pi}m

t

i=0.

Algorithm 2 attains linear convergence for strongly convex f (Johnson & Zhang, 2013); for non-
strongly convex functions, rates faster than Sgd can be shown by using an indirect perturbation
argument—see e.g., (Koneˇcn´y & Richt´arik, 2013; Xiao & Zhang, 2014).

We ﬁrst state an intermediate result for the iterates of nonconvex Svrg. To ease exposition, we

deﬁne

Γt =(cid:0)ηt − ct+1ηt

βt

(cid:1),

− η2

t L − 2ct+1η2

t

(3)

for some parameters ct+1 and βt (to be deﬁned shortly).

Our ﬁrst main result is the following theorem that provides convergence rate of Algorithm 2.

Theorem 2. Let f ∈ Fn. Let cm = 0, ηt = η > 0, βt = β > 0, and ct = ct+1(1 + ηβ + 2η2L2) + η2L3
such that Γt > 0 for 0 ≤ t ≤ m − 1. Deﬁne the quantity γn := mint Γt. Further, let pi = 0 for
0 ≤ i < m and pm = 1, and let T be a multiple of m. Then for the output xa of Algorithm 2 we have

E[(cid:107)∇f (xa)(cid:107)2] ≤ f (x0) − f (x∗)

T γn

,

where x∗ is an optimal solution to (1).

5

m = x0 ∈ Rd, epoch length m, step sizes {ηi > 0}m−1

i=0 , S = (cid:100)T /m(cid:101), discrete probability

Algorithm 2 SVRG(cid:0)x0, T, m,{pi}m

i=0,{ηi}m−1

i=0

(cid:1)

m

i=0

distribution {pi}m

(cid:80)n
xs+1
0 = xs
i=1 ∇fi(˜xs)
gs+1 = 1
for t = 0 to m − 1 do
n

1: Input: ˜x0 = x0
2: for s = 0 to S − 1 do
3:
4:
5:
6:
7:
8:
9:
10:
11: end for
12: Output: Iterate xa chosen uniformly random from {{xs+1

Uniformly randomly pick it from {1, . . . , n}
t = ∇fit (xs+1
vs+1
xs+1
t+1 = xs+1
end for

˜xs+1 =(cid:80)m

) − ∇fit (˜xs) + gs+1

t − ηtvs+1

i=0 pixs+1

t

t

i

}m−1
t=0 }S−1
s=0 .

t

Furthermore, we can also show that nonconvex Svrg exhibits expected descent (in objective)
after every epoch. The condition that T is a multiple of m is solely for convenience and can be
removed by slight modiﬁcation of the theorem statement. Note that the value γn above can depend
on n. To obtain an explicit dependence, we simplify it using speciﬁc choices for η and β, as formalized
below.
Theorem 3. Suppose f ∈ Fn. Let η = µ0/(Lnα) (0 < µ0 < 1 and 0 < α ≤ 1), β = L/nα/2,
m = (cid:98)n3α/2/(3µ0)(cid:99) and T is some multiple of m. Then there exists universal constants µ0, ν > 0
such that we have the following: γn ≥ ν

Lnα in Theorem 2 and

E[(cid:107)∇f (xa)(cid:107)2] ≤ Lnα[f (x0) − f (x∗)]

,

T ν

where x∗ is an optimal solution to the problem in (1) and xa is the output of Algorithm 2.

By rewriting the above result in terms IFO calls, we get the following general corollary for

nonconvex Svrg.
Corollary 2. Suppose f ∈ Fn. Then the IFO complexity of Algorithm 2 (with parameters from
Theorem 3) for achieving an -accurate solution is:

(cid:40)
O(cid:0)n + (n1− α

2 /)(cid:1) ,

O (n + (nα/)) ,

if α < 2/3,
if α ≥ 2/3.

IFO calls =

Corollary 2 shows the interplay between step size and the IFO complexity. We observe that the
number of IFO calls is minimized in Corollary 2 when α = 2/3. This gives rise to the following key
results of the paper.
Corollary 3. Suppose f ∈ Fn. Let η = µ1/(Ln2/3) (0 < µ1 < 1), β = L/n1/3, m = (cid:98)n/(3µ1)(cid:99) and
T is some multiple of m. Then there exists universal constants µ1, ν1 > 0 such that we have the
following: γn ≥ ν1

Ln2/3 in Theorem 2 and

E[(cid:107)∇f (xa)(cid:107)2] ≤ Ln2/3[f (x0) − f (x∗)]

,

T ν1

where x∗ is an optimal solution to the problem in (1) and xa is the output of Algorithm 2.
Corollary 4. If f ∈ Fn, then the IFO complexity of Algorithm 2 (with parameters in Corollary 3)
to obtain an -accurate solution is O(n + (n2/3/)).

√
Note the rate of O(1/T ) in the above results, as opposed to slower O(1/

T ) rate of Sgd (The-

orem 1). For a more comprehensive comparison of the rates, refer to Section 6.

6

Algorithm 3 GD-SVRG(cid:0)x0, K, T, m,{pi}m

i=0,{ηi}m−1

i=0

(cid:1)

Input: x0 ∈ Rd, K, epoch length m, step sizes {ηi > 0}m−1
for k = 0 to K do

xk = SVRG(xk−1, T, m,{pi}m

i=0,{ηi}m−1
i=0 )

i=0 , discrete probability distribution {pi}m

i=0

end for
Output: xK

3.1 Gradient Dominated Functions

Before ending our discussion on convergence of nonconvex Svrg, we prove a linear convergence
rate for the class of τ -gradient dominated functions (2). For ease of exposition, assume that τ > n1/3,
a property analogous to the “high condition number regime” for strongly convex functions typical
in machine learning. Note that gradient dominated functions can be nonconvex.

Theorem 4. Suppose f is τ -gradient dominated where τ > n1/3. Then, the iterates of Algorithm 3
with T = (cid:100)2Lτ n2/3/ν1(cid:101), m = (cid:98)n/(3µ1)(cid:99), ηt = µ1/(Ln2/3) for all 0 ≤ t ≤ m − 1 and pm = 1 and
pi = 0 for all 0 ≤ i < m satisfy

E[(cid:107)∇f (xk)(cid:107)2] ≤ 2−k[(cid:107)∇f (x0)(cid:107)2].

Here µ1 and ν1 are the constants used in Corollary 3.

In fact, for τ -gradient dominated functions we can prove a stronger result of global linear con-

vergence.
Theorem 5. If f is τ -gradient dominated (τ > n1/3), then with T = (cid:100)2Lτ n2/3/ν1(cid:101), m = (cid:98)n/(3µ1)(cid:99),
ηt = µ1/(Ln2/3) for 0 ≤ t ≤ m − 1 and pm = 1 and pi = 0 for all 0 ≤ i < m, the iterates of
Algorithm 3 satisfy

E[f (xk) − f (x∗)] ≤ 2−k[f (x0) − f (x∗)].

Here µ1, ν1 are as in Corollary 3; x∗ is an optimal solution.

An immediate consequence is the following.

Corollary 5. If f is τ -gradient dominated, the IFO complexity of Algorithm 3 (with parameters
from Theorem 4) to compute an -accurate solution is O((n + τ n2/3) log(1/)).

Note that GradientDescent can also achieve linear convergence rate for gradient dominated
functions (Polyak, 1963). However, GradientDescent requires O(n + nτ log(1/)) IFO calls to
obtain an -accurate solution as opposed to O(n + n2/3τ log(1/)) for Svrg. Similar (but not the
same) gains can be seen for Svrg for strongly convex functions (Johnson & Zhang, 2013). Also
notice that we did not assume anything except smoothness on the individual functions fi in the
above results. In particular, the following corollary is also an immediate consequence.
Corollary 6. If f is λ-strongly convex and the functions {fi}n
i=1 are possibly nonconvex, then
the number of IFO calls made by Algorithm 3 (with parameters from Theorem 4) to compute an
-accurate solution is O((n + n2/3κ) log(1/)).

Recall that here κ denotes the condition number L/λ for a λ-strongly convex function. Corollary 6
follows from Corollary 5 upon noting that λ-strongly convex function is 1/2λ-gradient dominated.
Theorem 5 generalizes the linear convergence result in (Johnson & Zhang, 2013) since it allows
nonconvex fi. Observe that Corollary 6 also applies when fi is strongly convex for all i ∈ [n],
though in this case a more reﬁned result can be proved (Johnson & Zhang, 2013).

Finally, we note that our result also improves on a recent result on Sdca in the setting of
Corollary 6 when the condition number κ is reasonably large – a case that typically arises in machine

7

learning. More precisely, for l2-regularized empirical loss minimization, Shalev-Shwartz (2015) show
that Sdca requires O((n + κ2) log(1/) iterations when the fi’s are possibly nonconvex but their
sum f is strongly convex. In comparison, we show that Algorithm 3 requires O((n + n2/3κ) log(1/))
iterations, which is an improvement over Sdca when κ > n2/3.

4 Convex Case

In the previous section, we showed nonconvex Svrg converges to a stationary point at the rate
O(n2/3/T ). A natural question is whether this rate can be improved if we assume convexity? We
provide an aﬃrmative answer. For non-strongly convex functions, this yields a direct analysis (i.e.,
not based on strongly convex perturbations) for Svrg. While we state our results in terms of
stationarity gap (cid:107)∇f (x)(cid:107)2 for the ease of comparison, our analysis also provides rates with respect
to the optimality gap [f (x) − f (x∗)] (see the proof of Theorem 6 in the appendix).
Theorem 6. If fi is convex for all i ∈ [n], pi = 1/m for 0 ≤ i ≤ m − 1, and pm = 0, then for
Algorithm 2, we have

E[(cid:107)∇f (xa)(cid:107)2] ≤ L(cid:107)x0 − x∗(cid:107)2 + 4mL2η2[f (x0) − f (x∗)]

T η(1 − 4Lη)
where x∗ is optimal for (1) and xa is the output of Algorithm 2.

,

We now state corollaries of this theorem that explicitly show the dependence on n in the conver-

gence rates.

√
Corollary 7. If m = n and η = 1/(8L
n) in Theorem 6, then we have the following bound:
√
n(16L(cid:107)x0 − x∗(cid:107)2 + [f (x0) − f (x∗)])
E[(cid:107)∇f (xa)(cid:107)2] ≤ L

,

where x∗ is optimal for (1) and xa is the output of Algorithm 2.

T

T

The above result uses a step size that depends on n. For the convex case, we can also use step

sizes independent of n. The following corollary states the associated result.

Corollary 8. If m = n and η = 1/(8L) in Theorem 6, then we have the following bound:

E[(cid:107)∇f (xa)(cid:107)2] ≤ L(16L(cid:107)x0 − x∗(cid:107)2 + n[f (x0) − f (x∗)])

,

where x∗ is optimal for (1) and xa is the output of Algorithm 2.

We can rewrite these corollaries in terms of IFO complexity to get the following corollaries.

Corollary 9. If fi is convex for all i ∈ [n], then the IFO complexity of Algorithm 2 (with parameters
from Corollary 7) to compute an -accurate solution is O(n + (
Corollary 10. If fi is convex for all i ∈ [n], then the IFO complexity of Algorithm 2 (with parameters
from Corollary 8) to compute -accurate solution is O(n/).

n/)).

√

These results follow from Corollary 7 and Corollary 8 and noting that for m = O(n) the total
IFO calls made by Algorithm 2 is O(n). It is instructive to quantitatively compare Corollary 9 and
Corollary10. With a step size independent of n, the convergence rate of Svrg has a dependence
that is in the order of n (Corollary 8). But this dependence can be reduced to
n by either carefully
selecting a step size that diminishes with n (Corollary 7) or by using a good initial point x0 obtained
by, say, running O(n) iterations of Sgd.

√

We emphasize that the convergence rate for convex case can be improved signiﬁcantly by slightly
modifying the algorithm (either by adding an appropriate strongly convex perturbation (Xiao &
Zhang, 2014) or by using a choice of m that changes with epoch (Zhu & Yuan, 2015)). However, it
is not clear if these strategies provide any theoretical gains for the general nonconvex case.

8

5 Mini-batch Nonconvex SVRG

In this section, we study the mini-batch version of Algorithm 2. Mini-batching is a popular strat-
egy, especially in multicore and distributed settings as it greatly helps one exploit parallelism and
reduce the communication costs. The pseudocode for mini-batch nonconvex Svrg (Algorithm 4) is
provided in the supplement due to lack of space. The key diﬀerence between the mini-batch Svrg
and Algorithm 2 lies in lines 6 to 8. To use mini-batches we replace line 6 with sampling (with
replacement) a mini-batch It ⊂ [n] of size b; lines 7 to 8 are replaced with the following updates:

(cid:88)
t − ηtus+1

it∈It

t

us+1
t = 1|It|
xs+1
t+1 = xs+1

(cid:0)∇fit(xs+1

t

) − ∇fit(˜xs)(cid:1) + gs+1,

When b = 1, this reduces to Algorithm 2. Mini-batch is typically used to reduce the variance of the
stochastic gradient and increase the parallelism. Lemma 4 (in Section G of the appendix) shows the
reduction in the variance of stochastic gradients with mini-batch size b. Using this lemma, one can
derive the mini-batch equivalents of Lemma 1, Theorem 2 and Theorem 3. However, for the sake of
brevity, we directly state the following main result for mini-batch Svrg.

Theorem 7. Let γn denote the following quantity:

(cid:0)η − ct+1η

β − η2L − 2ct+1η2(cid:1).

γn := min

0≤t≤m−1

t L3/b for 0 ≤ t ≤ m − 1. Suppose η = µ2b/(Ln2/3)
where cm = 0, ct = ct+1(1 + ηβ + 2η2L2/b) + η2
(0 < µ2 < 1), β = L/n1/3, m = (cid:98)n/(3bµ2)(cid:99) and T is some multiple of m. Then for the mini-batch
version of Algorithm 2 with mini-batch size b < n2/3, there exists universal constants µ2, ν2 > 0 such
that we have the following: γn ≥ ν2b

Ln2/3 and

E[(cid:107)∇f (xa)(cid:107)2] ≤ Ln2/3[f (x0) − f (x∗)]

bT ν2

,

where x∗ is optimal for (1).

√
It is important to compare this result with mini-batched Sgd. For a batch size of b, Sgd
√
bT ) (Dekel et al., 2012) (obtainable by a simple modiﬁcation of Theorem 1).
obtains a rate of O(1/
b dependence on the batch size. In contrast, Theorem 7 shows that Svrg
Speciﬁcally, Sgd has a 1/
has a much better dependence of 1/b on the batch size. Hence, compared to Sgd, Svrg allows more
eﬃcient mini-batching. More formally, in terms of IFO queries we have the following result.
Corollary 11. If f ∈ Fn, then the IFO complexity of the mini-batch version of Algorithm 2 (with
parameters from Theorem 7 and mini-batch size b < n2/3) to obtain an -accurate solution is O(n +
(n2/3/)).

Corollary 11 shows an interesting property of mini-batch Svrg. First, note that b IFO calls are
required for calculating the gradient on a mini-batch of size b. Hence, Svrg does not gain on IFO
complexity by using mini-batches. However, if the b gradients are calculated in parallel, then this
leads to a theoretical linear speedup in multicore and distributed settings. In contrast, Sgd does not
yield an eﬃcient mini-batch strategy as it requires O(b1/2/2) IFO calls for achieving an -accurate
solution (Li et al., 2014). Thus, the performance of Sgd degrades with mini-batching.

6 Comparison of the convergence rates

In this section, we give a comprehensive comparison of results obtained in this paper. In particular,
we compare key aspects of the convergence rates for Sgd, GradientDescent, and Svrg. The
comparison is based on IFO complexity to achieve an -accurate solution.

9

Dependence on n: The number of IFO calls of Svrg and GradientDescent depend explicitly
on n. In contrast, the number of oracle calls of Sgd is independent of n (Theorem 1). However, this
comes at the expense of worse dependence on . The number of IFO calls in GradientDescent
is proportional to n. But for Svrg this dependence reduces to n1/2 for convex (Corollary 7) and
n2/3 for nonconvex (Corollary 3) problems. Whether this diﬀerence in dependence on n is due to
nonconvexity or just an artifact of our analysis is an interesting open problem.

Dependence on : The dependence on  (or alternatively T ) follows from the convergence rates
of the algorithms. Sgd is seen to depend as O(1/2) on , regardless of convexity or nonconvexity.
In contrast, for both convex and nonconvex settings, Svrg and GradientDescent converge as
O(1/). Furthermore, for gradient dominated functions, Svrg and GradientDescent have global
linear convergence. This speedup in convergence over Sgd is especially signiﬁcant when medium to
high accuracy solutions are required (i.e.,  is small).

Assumptions used in analysis: It is important to understand the assumptions used in deriving
the convergence rates. All algorithms assume Lipschitz continuous gradients. However, Sgd requires
two additional subtle but important assumptions: σ-bounded gradients and advance knowledge of
T (since its step sizes depend on T ). On the other hand, both Svrg and GradientDescent do
not require these assumptions, and thus, are more ﬂexible.

Step size / learning rates: It is valuable to compare the step sizes used by the algorithms.
The step sizes of Sgd shrink as the number of iterations T increases—an undesirable property. On
the other hand, the step sizes of Svrg and GradientDescent are independent of T . Hence, both
these algorithms can be executed with a ﬁxed step size. However, Svrg uses step sizes that depend
on n (see Corollary 3 and Corollary 7). A step size independent of n can be used for Svrg for
convex f , albeit at cost of worse dependence on n (Corollary 8). GradientDescent does not have
this issue as its step size is independent of both n and T .

Dependence on initial point and mini-batch: Svrg is more sensitive to the initial point in
comparison to Sgd. This can be seen by comparing Corollary 3 (of Svrg) to Theorem 1 (of Sgd).
Hence, it is important to use a good initial point for Svrg. Similarly, a good mini-batch can be
beneﬁcial to Svrg. Moreover, mini-batches not only provides parallelism but also good theoretical
guarantees (see Theorem 7). In contrast, the performance gain in Sgd with mini-batches is not very
pronounced (see Section 5).

7 Best of two worlds

We have seen in the previous section that Svrg combines the beneﬁts of both GradientDescent
and Sgd. We now show that these beneﬁts of Svrg can be made more pronounced by an appropriate
step size under additional assumptions. In this case, the IFO complexity of Svrg is lower than those
of Sgd and GradientDescent. This variant of Svrg (Msvrg) chooses a step size based on the
total number of iterations T (or alternatively ). For our discussion below, we assume that T > n.
√
T , µ1/(Ln2/3)} (µ1 is the
Theorem 8. Let f ∈ Fn have σ-bounded gradients. Let ηt = η = max{c/
universal constant from Corollary 3), m = (cid:98)n/(3µ1)(cid:99), and c =
. Further, let T be a
multiple of m, pm = 1, and pi = 0 for 0 ≤ i < m. Then, the output xa of Algorithm 2 satisﬁes

2Lσ2

(cid:113) f (x0)−f (x∗)
(cid:111)

,

T ν1

(cid:114)

E[(cid:107)∇f (xa)(cid:107)2]
≤ ¯ν min

(cid:110)

2

2(f (x0) − f (x∗))L

Ln2/3[f (x0) − f (x∗)]

T

σ,

where ¯ν is a universal constant, ν1 is the universal constant from Corollary 3 and x∗ is an optimal
solution to (1).
Corollary 12. If f ∈ Fn has σ-bounded gradients, the IFO complexity of Algorithm 2 (with param-
eters from Theorem 8) to achieve an -accurate solution is O(min{1/2, n2/3/}).

10

Figure 1: Neural network results for CIFAR-10, MNIST and STL-10 datasets. The top row represents the
results for CIFAR-10 dataset. The bottom left and middle ﬁgures represent the results for MNIST dataset.
The bottom right ﬁgure represents the result for STL-10.

An almost identical reasoning can be applied when f is convex to get the bounds speciﬁed in

Table 1. Hence, we omit the details and directly state the following result.
Corollary 13. Suppose fi is convex for i ∈ [n] and f has σ-bounded gradients, then the IFO
√
complexity of Algorithm 2 (with step size η = max{1/(L
n)}, m = n and pi = 1/m for
0 ≤ i ≤ m − 1 and pm = 0) to achieve an -accurate solution is O(min{1/2,

√
T ), 1/(8L

√

n/}).

Msvrg has a convergence rate faster than those of both Sgd and Svrg, though this beneﬁt
is not without cost. Msvrg, in contrast to Svrg, uses the additional assumption of σ-bounded
gradients. Furthermore, its step size is not ﬁxed since it depends on the number of iterations T .
While it is often diﬃcult in practice to compute the step size of Msvrg (Theorem 8), it is typical
to try multiple step sizes and choose the one with the best results.

8 Experiments

We present our empirical results in this section. For our experiments, we study the problem of
multiclass classiﬁcation using neural networks. This is a typical nonconvex problem encountered in
machine learning.

Experimental Setup. We train neural networks with one fully-connected hidden layer of 100
nodes and 10 softmax output nodes. We use (cid:96)2-regularization for training. We use CIFAR-102,
MNIST3, and STL-104 datasets for our experiments. These datasets are standard in the neural
networks literature. The (cid:96)2 regularization is 1e-3 for CIFAR-10 and MNIST, and 1e-2 for STL-10.
The features in the datasets are normalized to the interval [0, 1]. All the datasets come with a
predeﬁned split into training and test datasets.

We compare Sgd (the de-facto algorithm for training neural networks) against nonconvex Svrg.
The step size (or learning rate) is critical for Sgd. We set the learning rate of Sgd using the popular
t−inverse schedule ηt = η0(1 + η(cid:48)(cid:98)t/n(cid:99))−1, where η0 and η(cid:48) are chosen so that Sgd gives the best

2www.cs.toronto.edu/~kriz/cifar.html
3http://yann.lecun.com/exdb/mnist/
4https://cs.stanford.edu/~acoates/stl10/

11

0100200300400# grad / n1.451.501.551.601.65Training lossSGDSVRG0100200300400# grad / n10-510-410-310-210-1100101k∇f(xt)k2SGDSVRG0100200300400# grad / n0.380.400.420.440.460.480.500.520.54Test ErrorSGDSVRG050100150200250300# grad / n0.3500.3550.3600.3650.3700.3750.380Training lossSGDSVRG050100150200250300# grad / n10-910-810-710-610-510-410-310-210-1k∇f(xt)k2SGDSVRG050100150200250300# grad / n1.41.51.61.71.81.9Training lossSGDSVRGperformance on the training loss. In our experiments, we also use η(cid:48) = 0; this results in a ﬁxed step
size for Sgd. For Svrg, we use a ﬁxed step size as suggested by our analysis. Again, the step size
is chosen so that Svrg gives the best performance on the training loss.

Initialization & mini-batching. Initialization is critical to training of neural networks. We
use the normalized initialization in (Glorot & Bengio, 2010) where parameters are chosen uniformly

from [−(cid:112)6/(ni + no),(cid:112)6/(ni + no)] where ni and no are the number of input and output layers of

the neural network, respectively.

For Svrg, we use n iterations of Sgd for CIFAR-10 and MINST and 2n iterations of Sgd for
STL-10 before running Algorithm 2. Such initialization is standard for variance reduced schemes
even for convex problems (Johnson & Zhang, 2013; Schmidt et al., 2013). As noted earlier in
Section 6, Svrg is more sensitive than Sgd to the initial point, so such an initialization is typically
helpful. We use mini-batches of size 10 in our experiments. Sgd with mini-batches is common in
training neural networks. Note that mini-batch training is especially beneﬁcial for Svrg, as shown
by our analysis in Section 5. Along the lines of theoretical analysis provided by Theorem 7, we use
an epoch size m = n/10 in our experiments.
Results. We report objective function (training loss), test error (classiﬁcation error on the test
set), and (cid:107)∇f (xt)(cid:107)2 (convergence criterion throughout our analysis) for the datasets. For all the
algorithms, we compare these criteria against the number of eﬀective passes through the data, i.e.,
IFO calls divided by n. This includes the cost of calculating the full gradient at the end of each
epoch of Svrg. Due to the Sgd initialization in Svrg and mini-batching, the Svrg plots start
from x-axis value of 10 for CIFAR-10 and MNIST and 20 for STL-10. Figure 1 shows the results for
our experiment. It can be seen that the (cid:107)∇f (xt)(cid:107)2 for Svrg is lower compared to Sgd, suggesting
faster convergence to a stationary point. Furthermore, the training loss is also lower compared to
Sgd in all the datasets. Notably, the test error for CIFAR-10 is lower for Svrg, indicating better
generalization; we did not notice substantial diﬀerence in test error for MNIST and STL-10 (see
Section H in the appendix). Overall, these results on a network with one hidden layer are promising;
it will be interesting to study Svrg for deep neural networks in the future.

9 Discussion

In this paper, we examined a VR scheme for nonconvex optimization. We showed that by employing
VR in stochastic methods, one can perform better than both Sgd and GradientDescent in the
context of nonconvex optimization. When the function f in (1) is gradient dominated, we proposed
a variant of Svrg that has linear convergence to the global minimum. Our analysis shows that
Svrg has a number of interesting properties that include convergence with ﬁxed step size, descent
property after every epoch; a property that need not hold for Sgd. We also showed that Svrg, in
contrast to Sgd, enjoys eﬃcient mini-batching, attaining speedups linear in the size of the mini-
batches in parallel settings. Our analysis also reveals that the initial point and use of mini-batches
are important to Svrg.

Before concluding the paper, we would like to discuss the implications of our work and few
caveats. One should exercise some caution while interpreting the results in the paper. All our
theoretical results are based on the stationarity gap. In general, this does not necessarily translate
to optimality gap or low training loss and test error. One criticism against VR schemes in nonconvex
optimization is the general wisdom that variance in the stochastic gradients of Sgd can actually help
it escape local minimum and saddle points. In fact, Ge et al. (2015) add additional noise to the
stochastic gradient in order to escape saddle points. However, one can reap the beneﬁt of VR
schemes even in such scenarios. For example, one can envision an algorithm which uses Sgd as an
exploration tool to obtain a good initial point and then uses a VR algorithm as an exploitation tool
to quickly converge to a good local minimum. In either case, we believe variance reduction can be
used as an important tool alongside other tools like momentum, adaptive learning rates for faster
and better nonconvex optimization.

12

References

Agarwal, Alekh and Bottou, Leon.

A lower bound for the optimization of ﬁnite sums.

arXiv:1410.0723, 2014.

Bertsekas, Dimitri P. Incremental gradient, subgradient, and proximal methods for convex optimiza-
tion: A survey. In S. Sra, S. Nowozin, S. Wright (ed.), Optimization for Machine Learning. MIT
Press, 2011.

Bottou, L´eon. Stochastic gradient learning in neural networks. Proceedings of Neuro-Nımes, 91(8),

1991.

Defazio, Aaron, Bach, Francis, and Lacoste-Julien, Simon. SAGA: A fast incremental gradient
method with support for non-strongly convex composite objectives. In NIPS 27, pp. 1646–1654,
2014a.

Defazio, Aaron J, Caetano, Tib´erio S, and Domke, Justin. Finito: A faster, permutable incremental

gradient method for big data problems. arXiv:1407.2710, 2014b.

Dekel, Ofer, Gilad-Bachrach, Ran, Shamir, Ohad, and Xiao, Lin. Optimal distributed online pre-
diction using mini-batches. The Journal of Machine Learning Research, 13(1):165–202, January
2012. ISSN 1532-4435.

Ge, Rong, Huang, Furong, Jin, Chi, and Yuan, Yang. Escaping from saddle points - online stochastic
gradient for tensor decomposition. In Proceedings of The 28th Conference on Learning Theory,
COLT 2015, pp. 797–842, 2015.

Ghadimi, Saeed and Lan, Guanghui. Stochastic ﬁrst- and zeroth-order methods for nonconvex
stochastic programming. SIAM Journal on Optimization, 23(4):2341–2368, 2013. doi: 10.1137/
120880811.

Glorot, Xavier and Bengio, Yoshua. Understanding the diﬃculty of training deep feedforward neural
networks. In In Proceedings of the International Conference on Artiﬁcial Intelligence and Statistics
(AISTATS10), 2010.

Hazan, Elad, Levy, Kﬁr, and Shalev-Shwartz, Shai. Beyond convexity: Stochastic quasi-convex

optimization. In Advances in Neural Information Processing Systems, pp. 1585–1593, 2015.

Hong, Mingyi. A distributed, asynchronous and incremental algorithm for nonconvex optimization:

An admm based approach. arXiv preprint arXiv:1412.6058, 2014.

Johnson, Rie and Zhang, Tong. Accelerating stochastic gradient descent using predictive variance

reduction. In NIPS 26, pp. 315–323, 2013.

Koneˇcn´y, Jakub and Richt´arik, Peter. Semi-Stochastic Gradient Descent Methods. arXiv:1312.1666,

2013.

Koneˇcn´y, Jakub, Liu, Jie, Richt´arik, Peter, and Tak´aˇc, Martin. Mini-Batch Semi-Stochastic Gradient

Descent in the Proximal Setting. arXiv:1504.04407, 2015.

Kushner, Harold Joseph and Clark, Dean S. Stochastic approximation methods for constrained and

unconstrained systems, volume 26. Springer Science & Business Media, 2012.

Lan, Guanghui and Zhou, Yi.

An optimal

randomized incremental gradient method.

arXiv:1507.02000, 2015.

Li, Mu, Zhang, Tong, Chen, Yuqiang, and Smola, Alexander J. Eﬃcient mini-batch training for
stochastic optimization. In Proceedings of the 20th ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, KDD ’14, pp. 661–670. ACM, 2014.

Lian, Xiangru, Huang, Yijun, Li, Yuncheng, and Liu, Ji. Asynchronous Parallel Stochastic Gradient

for Nonconvex Optimization. In NIPS, 2015.

Ljung, Lennart. Analysis of recursive stochastic algorithms. Automatic Control, IEEE Transactions

on, 22(4):551–575, 1977.

13

Nemirovski, A., Juditsky, A., Lan, G., and Shapiro, A. Robust stochastic approximation approach

to stochastic programming. SIAM Journal on Optimization, 19(4):1574–1609, 2009.

Nemirovski, Arkadi and Yudin, D. Problem Complexity and Method Eﬃciency in Optimization.

John Wiley and Sons, 1983.

Nesterov, Yurii. Introductory Lectures On Convex Optimization: A Basic Course. Springer, 2003.

Nesterov, Yurii and Polyak, Boris T. Cubic regularization of newton method and its global perfor-

mance. Mathematical Programming, 108(1):177–205, 2006.

Poljak, BT and Tsypkin, Ya Z. Pseudogradient adaptation and training algorithms. Automation

and Remote Control, 34:45–67, 1973.

Polyak, B.T. Gradient methods for the minimisation of functionals. USSR Computational Mathe-

matics and Mathematical Physics, 3(4):864–878, January 1963.

Reddi, Sashank, Hefny, Ahmed, Sra, Suvrit, Poczos, Barnabas, and Smola, Alex J. On variance
reduction in stochastic gradient descent and its asynchronous variants. In NIPS 28, pp. 2629–
2637, 2015.

Robbins, H. and Monro, S. A stochastic approximation method. Annals of Mathematical Statistics,

22:400–407, 1951.

Schmidt, Mark W., Roux, Nicolas Le, and Bach, Francis R. Minimizing Finite Sums with the

Stochastic Average Gradient. arXiv:1309.2388, 2013.

Shalev-Shwartz, Shai. SDCA without duality. CoRR, abs/1502.06177, 2015.

Shalev-Shwartz, Shai and Zhang, Tong. Stochastic dual coordinate ascent methods for regularized

loss. The Journal of Machine Learning Research, 14(1):567–599, 2013.

Shamir, Ohad. A stochastic PCA and SVD algorithm with an exponential convergence rate.

arXiv:1409.2848, 2014.

Shamir, Ohad. Fast stochastic algorithms for SVD and PCA: Convergence properties and convexity.

arXiv:1507.08788, 2015.

Sra, Suvrit. Scalable nonconvex inexact proximal splitting. In NIPS, pp. 530–538, 2012.

Xiao, Lin and Zhang, Tong. A proximal stochastic gradient method with progressive variance

reduction. SIAM Journal on Optimization, 24(4):2057–2075, 2014.

Zhu, Zeyuan Allen and Yuan, Yang. Univr: A universal variance reduction framework for proximal

stochastic gradient method. CoRR, abs/1506.01972, 2015.

Appendix

A Nonconvex SGD: Convergence Rate

Proof of Theorem 1

√
Theorem. Suppose f has σ-bounded gradient; let ηt = η = c/
x∗ is an optimal solution to (1). Then, the iterates of Algorithm 1 satisfy

T where c =

(cid:113)

E[(cid:107)∇f (xt)(cid:107)2] ≤

min

0≤t≤T−1

2(f (x0)−f (x∗))L

T

σ.

(cid:113) 2(f (x0)−f (x∗))

Lσ2

, and

Proof. We include the proof here for completeness. Please refer to (Ghadimi & Lan, 2013) for a
more general result.

14

The iterates of Algorithm 1 satisfy the following bound:

E[f (xt+1)] ≤ E[f (xt) +(cid:10)∇f (xt), xt+1 − xt(cid:11)

+ L

2 (cid:107)xt+1 − xt(cid:107)2]

≤ E[f (xt)] − ηtE[(cid:107)∇f (xt)(cid:107)2] + Lη2
≤ E[f (xt)] − ηtE[(cid:107)∇f (xt)(cid:107)2] + Lη2

2

t

E[(cid:107)∇fit(xt)(cid:107)2]

(4)

(5)
The ﬁrst inequality follows from Lipschitz continuity of ∇f . The second inequality follows from the
update in Algorithm 1 and since Eit[∇fit(xt)] = ∇f (xt) (unbiasedness of the stochastic gradient).
The last step uses our assumption on gradient boundedness. Rearranging Equation (5) we obtain

t

2 σ2.

E[(cid:107)∇f (xt)(cid:107)2] ≤ 1

ηt

E[f (xt) − f (xt+1)] + Lηt

2 σ2.

(6)

Summing Equation (6) from t = 0 to T − 1 and using that ηt is constant η we obtain

min

t

T

E[(cid:107)∇f (xt)(cid:107)2] ≤ 1
≤ 1
≤ 1
≤ 1√

t=0

E[(cid:107)f (xt)(cid:107)2]

(cid:88)T−1
E[f (x0) − f (xT )] + Lη
2 σ2
(cid:16) 1
T η (f (x0) − f (x∗)) + Lη
2 σ2

2 σ2(cid:17)
(cid:0)f (x0) − f (x∗)(cid:1) + Lc

T η

c

T

.

The ﬁrst step holds because the minimum is less than the average. The second and third steps
are obtained from Equation (6) and the fact that f (x∗) ≤ f (xT ), respectively. The ﬁnal inequality
√
follows upon using η = c/

T . By setting

(cid:114)

c =

2(f (x0) − f (x∗))

Lσ2

in the above inequality, we get the desired result.

B Nonconvex SVRG

In this section, we provide the proofs of the results for nonconvex Svrg. We ﬁrst start with few
useful lemmas and then proceed towards the main results.

Lemma 1. For ct, ct+1, βt > 0, suppose we have

ct = ct+1(1 + ηtβt + 2η2

t L2) + η2

t L3.

Let ηt, βt and ct+1 be chosen such that Γt > 0 (in Equation (3)). The iterate xs+1
satisfy the bound:

t

in Algorithm 2

E[(cid:107)∇f (xs+1

t

)(cid:107)2] ≤ Rs+1

t − Rs+1

t+1

Γt

,

where Rs+1

) + ct(cid:107)xs+1
Proof. Since f is L-smooth we have

:= E[f (xs+1

t

t

t − ˜xs(cid:107)2] for 0 ≤ s ≤ S − 1.

E[f (xs+1

t+1 )] ≤ E[f (xs+1

t

) + (cid:104)∇f (xs+1
+ L

t+1 − xs+1
), xs+1
(cid:107)2].
t+1 − xs+1

2 (cid:107)xs+1

t

t

t

(cid:105)

15

Using the Svrg update in Algorithm 2 and its unbiasedness, the right hand side above is further
upper bounded by

E[f (xs+1

t

) − ηt(cid:107)∇f (xs+1

t

Consider now the Lyapunov function

)(cid:107)2 + Lη2

2 (cid:107)vs+1

t

t

(cid:107)2].

(7)

Rs+1

t

:= E[f (xs+1

t

) + ct(cid:107)xs+1

t − ˜xs(cid:107)2].

For bounding it we will require the following:

t+1 − ˜xs(cid:107)2] = E[(cid:107)xs+1

E[(cid:107)xs+1
= E[(cid:107)xs+1

t+1 − xs+1

t

t+1 − xs+1
t − ˜xs(cid:107)2
, xs+1

t

(cid:107)2 + (cid:107)xs+1
t+1 − xs+1
t − ˜xs(cid:107)2]

+ 2(cid:104)xs+1
(cid:107)2 + (cid:107)xs+1
− 2ηtE[(cid:104)∇f (xs+1
(cid:107)2 + (cid:107)xs+1

t − ˜xs(cid:107)2]
)(cid:107)2 + 1

(cid:107)∇f (xs+1

t

t

2βt

= E[η2

t (cid:107)vs+1

t

≤ E[η2

t (cid:107)vs+1

+ 2ηtE(cid:104) 1

t

t + xs+1

t − ˜xs(cid:107)2]

t − ˜xs(cid:105)]

), xs+1

t − ˜xs(cid:105)]

t − ˜xs(cid:107)2(cid:105)

2 βt(cid:107)xs+1

.

(8)

The second equality follows from the unbiasedness of the update of Svrg. The last inequality follows
from a simple application of Cauchy-Schwarz and Young’s inequality. Plugging Equation (7) and
Equation (8) into Rs+1

t+1 , we obtain the following bound:
t+1 ≤ E[f (xs+1
Rs+1

) − ηt(cid:107)∇f (xs+1

t

t

t

t

t

t (cid:107)vs+1

+ E[ct+1η2

)(cid:107)2 + Lη2
(cid:107)2 + ct+1(cid:107)xs+1
(cid:107)∇f (xs+1
)(cid:107)2 + 1

(cid:107)2]
2 (cid:107)vs+1
t − ˜xs(cid:107)2]
2 βt(cid:107)xs+1

+ 2ct+1ηtE(cid:104) 1
) −(cid:16)
(cid:16) Lη2
(cid:17) E[(cid:107)vs+1
+ (ct+1 + ct+1ηtβt) E(cid:2)(cid:107)xs+1

(cid:17)(cid:107)∇f (xs+1
t − ˜xs(cid:107)2(cid:3) .

ηt − ct+1ηt

2 + ct+1η2

)(cid:107)2

(cid:107)2]

2βt

+

βt

t

t

t

t

t

t

≤ E[f (xs+1

t − ˜xs(cid:107)2(cid:105)

t

t+1 ≤ E[f (xs+1
Rs+1
)]
ηt − ct+1ηt

−(cid:16)
+(cid:2)ct+1
t −(cid:0)ηt − ct+1ηt

(cid:0)1 + ηtβt + 2η2

≤ Rs+1

− η2

− η2

βt

βt

t L − 2ct+1η2

t L2(cid:1) + η2

t

t L − 2ct+1η2

t

(cid:17) E[(cid:107)∇f (xs+1
t L3(cid:3) E(cid:2)(cid:107)xs+1
t − ˜xs(cid:107)2(cid:3)
(cid:1)E[(cid:107)∇f (xs+1

)(cid:107)2].

)(cid:107)2]

t

t

To further bound this quantity, we use Lemma 3 to bound E[(cid:107)vs+1
in Equation (9), we see that

t

(9)
(cid:107)2], so that upon substituting it

The second inequality follows from the deﬁnition of ct and Rs+1

t

, thus concluding the proof.

Proof of Theorem 2
Theorem. Let f ∈ Fn. Let cm = 0, ηt = η > 0, βt = β > 0, and ct = ct+1(1 + ηβ + 2η2L2) + η2L3
such that Γt > 0 for 0 ≤ t ≤ m − 1. Deﬁne the quantity γn := mint Γt. Further, let pi = 0 for

16

0 ≤ i < m and pm = 1, and let T be a multiple of m. Then for the output xa of Algorithm 2 we have

E[(cid:107)∇f (xa)(cid:107)2] ≤ f (x0) − f (x∗)

T γn

,

where x∗ is an optimal solution to (1).
Proof. Since ηt = η for t ∈ {0, . . . , m − 1}, using Lemma 1 and telescoping the sum, we obtain

E[(cid:107)∇f (xs+1

t

)(cid:107)2] ≤ Rs+1

0 − Rs+1

m

.

γn

(cid:88)m−1
This inequality in turn implies that(cid:88)m−1

t=0

where we used that Rs+1
and that Rs+1
epochs to obtain

m = E[f (xs+1
0 = E[f (˜xs)] (since xs+1

E[(cid:107)∇f (xs+1

t

t=0

)(cid:107)2] ≤ E[f (˜xs) − f (˜xs+1)]

γn

,

(10)

m )] = E[f (˜xs+1)] (since cm = 0, pm = 1, and pi = 0 for i < m),
0 = ˜xs, as pm = 1 and pi = 0 for i < m). Now sum over all

S−1(cid:88)

m−1(cid:88)

s=0

t=0

1
T

E[(cid:107)∇f (xs+1

t

)(cid:107)2] ≤ f (x0) − f (x∗)

T γn

.

(11)

The above inequality used the fact that ˜x0 = x0. Using the above inequality and the deﬁnition of
xa in Algorithm 2, we obtain the desired result.

Proof of Theorem 3
Theorem. Suppose f ∈ Fn. Let η = µ0/(Lnα) (0 < µ0 < 1 and 0 < α ≤ 1), β = L/nα/2,
m = (cid:98)n3α/2/(3µ0)(cid:99) and T is some multiple of m. Then there exists universal constants µ0, ν > 0
such that we have the following: γn ≥ ν

Lnα in Theorem 2 and

E[(cid:107)∇f (xa)(cid:107)2] ≤ Lnα[f (x0) − f (x∗)]

T ν

,

where x∗ is an optimal solution to the problem in (1) and xa is the output of Algorithm 2.
Proof. For our analysis, we will require an upper bound on c0. We observe that c0 = µ2
0L
n2α
where θ = 2η2L2 + ηβ. This is obtained using the relation ct = ct+1(1 + ηβ + 2η2L2) + η2L3 and
the fact that cm = 0. Using the speciﬁed values of β and η we have

(1+θ)m−1

θ

θ = 2η2L2 + ηβ =

2µ2
0
n2α +

µ0

n3α/2

≤ 3µ0
n3α/2

.

The above inequality follows since µ0 ≤ 1 and n ≥ 1. Using the above bound on θ, we get

c0 =

µ2
0L
n2α

(1 + θ)m − 1

µ0L((1 + θ)m − 1)

=

θ

2µ0 + nα/2

n3α/2 )(cid:98)n3α/2/3µ0(cid:99) − 1)
≤ µ0L((1 + 3µ0
2µ0 + nα/2
≤ n−α/2(µ0L(e − 1)),

17

(12)

wherein the second inequality follows upon noting that (1+ 1
1
l )l = e (here e is the Euler’s number). Now we can lower bound γn, as

l )l is increasing for l > 0 and liml→∞(1+

(cid:0)η − ct+1η
β − η2L − 2ct+1η2(cid:1)
β − η2L − 2c0η2(cid:1) ≥ ν

≥(cid:0)η − c0η

γn = min

t

Lnα ,

where ν is a constant independent of n. The ﬁrst inequality holds since ct decreases with t. The
second inequality holds since (a) c0/β is upper bounded by a constant independent of n as c0/β ≤
µ0(e − 1) (follows from Equation (12)), (b) η2L ≤ µ0η and (c) 2c0η2 ≤ 2µ2
0(e − 1)η (follows from
Equation (12)). By choosing µ0 (independent of n) appropriately, one can ensure that γn ≥ ν/(Lnα)
for some universal constant ν. For example, choosing µ0 = 1/4, we have γn ≥ ν/(Lnα) with
ν = 1/40. Substituting the above lower bound in Equation (11), we obtain the desired result.

Proof of Corollary 2
Corollary. Suppose f ∈ Fn. Then the IFO complexity of Algorithm 2 (with parameters from
Theorem 3) for achieving an -accurate solution is:

(cid:40)
O(cid:0)n + (n1− α

2 /)(cid:1) ,

O (n + (nα/)) ,

if α < 2/3,
if α ≥ 2/3.

IFO calls =

Proof. This result follows from Theorem 3 and the fact that m = (cid:98)n3α/2/(3µ0)(cid:99). Suppose α < 2/3,
then m = o(n). However, n IFO calls are invested in calculating the average gradient at the end
of each epoch. In other words, computation of average gradient requires n IFO calls for every m

iterations of the algorithm. Using this relationship, we get O(cid:0)n + (n1− α
gradient (per epoch) is of lower order, leading to O(cid:0)n + (nα/)(cid:1) IFO calls.

On the other hand, when α ≥ 2/3, the total number of IFO calls made by Algorithm 2 in each
epoch is Ω(n) since m = (cid:98)n3α/2/(3µ0)(cid:99). Hence, the oracle calls required for calculating the average

2 /)(cid:1) in this case.

C GD-SVRG

Proof of Theorem 4

Theorem. Suppose f is τ -gradient dominated where τ > n1/3. Then, the iterates of Algorithm 3
with T = (cid:100)2Lτ n2/3/ν1(cid:101), m = (cid:98)n/(3µ1)(cid:99), ηt = µ1/(Ln2/3) for all 0 ≤ t ≤ m − 1 and pm = 1 and
pi = 0 for all 0 ≤ i < m satisfy

E[(cid:107)∇f (xk)(cid:107)2] ≤ 2−k[(cid:107)∇f (x0)(cid:107)2].

Here µ1 and ν1 are the constants used in Corollary 3.

Proof. Corollary 3 shows that the iterates of Algorithm 3 satisfy

Substituting the speciﬁed value of T in the above inequality, we have

E[(cid:107)∇f (xk)(cid:107)2] ≤ Ln2/3E[f (xk−1) − f (x∗)]
(cid:0)E[f (xk−1) − f (x∗)](cid:1)

E[(cid:107)∇f (xk)(cid:107)2] ≤ 1
2τ
E[(cid:107)∇f (xk−1)(cid:107)2].
≤ 1

T ν1

.

2

The second inequality follows from τ -gradient dominance of the function f .

18

Proof of Theorem 5
Theorem. If f is τ -gradient dominated (τ > n1/3), then with T = (cid:100)2Lτ n2/3/ν1(cid:101), m = (cid:98)n/(3µ1)(cid:99),
ηt = µ1/(Ln2/3) for 0 ≤ t ≤ m − 1 and pm = 1 and pi = 0 for all 0 ≤ i < m, the iterates of
Algorithm 3 satisfy

E[f (xk) − f (x∗)] ≤ 2−k[f (x0) − f (x∗)].

Here µ1, ν1 are as in Corollary 3; x∗ is an optimal solution.
Proof. The proof mimics that of Theorem 4; now we have the following condition on the iterates of
Algorithm 3:

E[(cid:107)∇f (xk)(cid:107)2] ≤ E[f (xk−1) − f (x∗)]

(13)
However, f is τ -gradient dominated, so E[(cid:107)∇f (xk)(cid:107)2] ≥ E[f (xk) − f (x∗)]/τ , which combined with
Equation (13) concludes the proof.

2τ

.

D Convex SVRG: Convergence Rate

Proof of Theorem 6
Theorem. If fi is convex for all i ∈ [n], pi = 1/m for 0 ≤ i ≤ m − 1, and pm = 0, then for
Algorithm 2, we have

E[(cid:107)∇f (xa)(cid:107)2] ≤ L(cid:107)x0 − x∗(cid:107)2 + 4mL2η2[f (x0) − f (x∗)]

,

E[(cid:107)xs+1
≤ E[(cid:107)xs+1

t+1 − x∗(cid:107)2] = E[(cid:107)xs+1

T η(1 − 4Lη)
where x∗ is optimal for (1) and xa is the output of Algorithm 2.
Proof. Consider the following sequence of inequalities:
t − ηvs+1
t − x∗(cid:107)2]
(cid:107)2]
t − x∗(cid:107)2] + η2E[(cid:107)vs+1
t − x∗(cid:105)]
− 2ηE[(cid:104)vs+1
t − x∗(cid:107)2] + η2E[(cid:107)vs+1
(cid:107)2]
− 2ηE[f (xs+1
) − f (x∗)]
t − x∗(cid:107)2] − 2η(1 − 2Lη)E[f (xs+1
+ 4Lη2E[f (˜xs) − f (x∗)]
t − x∗(cid:107)2] − 2η(1 − 4Lη)E[f (xs+1
+ 4Lη2E[f (˜xs) − f (x∗)] − 4Lη2E[f (xs+1

= E[(cid:107)xs+1

≤ E[(cid:107)xs+1

≤ E[(cid:107)xs+1

, xs+1

t

t

t

t

t

t

) − f (x∗)]

) − f (x∗)]

) − f (x∗)].

t

The second inequality uses unbiasedness of the Svrg update and convexity of f . The third inequality
follows from Lemma 6. Deﬁning the Lyapunov function

and summing the above inequality over t, we get

P s := E[(cid:107)xs

2η(1 − 4Lη)

m − x∗(cid:107)2] + 4mLη2E[f (˜xs) − f (x∗)],
m−1(cid:88)

) − f (x∗)] ≤ P s − P s+1.

E[f (xs+1

t

t=0

19

m = x0 ∈ Rd, epoch length m, step sizes {ηi > 0}m−1
i=0, mini-batch size b

i=0 , S = (cid:100)T /m(cid:101), discrete probability

distribution {pi}m

Algorithm 4 Mini-batch SVRG
1: Input: ˜x0 = x0
2: for s = 0 to S − 1 do
3:
4:
5:
6:
7:

(cid:80)n
xs+1
0 = xs
i=1 ∇fi(˜xs)
gs+1 = 1
for t = 0 to m − 1 do
(cid:80)
n
t − ηtus+1

it∈It

m

t

Choose a mini-batch (uniformly random with replacement) It ⊂ [n] of size b
us+1
t = 1
b
xs+1
t+1 = xs+1
end for

) − ∇fit (˜xs)) + gs+1

(∇fit (xs+1

8:
9:
10:
11: end for
12: Output: Iterate xa chosen uniformly random from {{xs+1

˜xs+1 =(cid:80)m

i=0 pixs+1

}m−1
t=0 }S−1
s=0 .

t

t

i

This due is to the fact that

P s+1 = E[(cid:107)xs+1

m − x∗(cid:107)2] + 4mLη2E[f (˜xs+1) − f (x∗)]
) − f (x∗)].

m−1(cid:88)

m − x∗(cid:107)2] + 4Lη2

E[f (xs+1

t

= E[(cid:107)xs+1

The above equality uses the fact that pm = 0 and pi = 1/m for 0 ≤ i < m. Summing over all epochs
and telescoping we then obtain

E[f (xa) − f (x∗)] ≤ P 0(cid:0)2T η(1 − 4Lη)(cid:1)−1

.

t=0

The inequality also uses the deﬁnition of xa given in Alg 2. On this inequality we use Lemma 5,
which yields

E[(cid:107)∇f (xa)(cid:107)2] ≤ 2LE[f (xa) − f (x∗)]
≤ L(cid:107)x0 − x∗(cid:107)2 + 4mL2η2[f (x0) − f (x∗)]

T η(1 − 4Lη)

.

It is easy to see that we can obtain convergence rates for E[f (xa) − f (x∗)] from the above

reasoning. This leads to a direct analysis of Svrg for convex functions.

E Minibatch Nonconvex SVRG

Proof of Theorem 7

The proofs essentially follow along the lines of Lemma 1, Theorem 2 and Theorem 3 with the
added complexity of mini-batch. We ﬁrst prove few intermediate results before proceeding to the
proof of Theorem 7.

Lemma 2. Suppose we have

R

s+1
t

:= E[f (xs+1

t

) + ct(cid:107)xs+1

t − ˜xs(cid:107)2],
η2
t L3
b

) +

,

2η2
t L2
b

ct = ct+1(1 + ηtβt +

20

for 0 ≤ s ≤ S − 1 and 0 ≤ t ≤ m − 1 and the parameters ηt, βt and ct+1 are chosen such that

(cid:18)

(cid:19)

ηt − ct+1ηt
βt

− η2

t L − 2ct+1η2

t

≥ 0.

Then the iterates xs+1
size b satisfy the bound:

t

in the mini-batch version of Algorithm 2 i.e., Algorithm 4 with mini-batch

(cid:16)
) −(cid:16)

E[(cid:107)∇f (xs+1

t

)(cid:107)2] ≤

R
ηt − ct+1ηt

βt

s+1

t − R
− η2

s+1
t+1

t L − 2ct+1η2

t

(cid:17) ,

s+1

t+1 ≤ E[f (xs+1

R

t

ηt − ct+1ηt

(cid:16) Lη2
(cid:17) E[(cid:107)us+1
+ (ct+1 + ct+1ηtβt) E(cid:2)(cid:107)xs+1

2 + ct+1η2

+

βt

t

t

t

(cid:17)(cid:107)∇f (xs+1
t − ˜xs(cid:107)2(cid:3) .

(cid:107)2]

)(cid:107)2

t

R

t

s+1

t+1 ≤ E[f (xs+1
)]
ηt − ct+1ηt

−(cid:16)
(cid:104)
t −(cid:0)ηt − ct+1ηt

(cid:0)1 + ηtβt + 2η2

− η2

− η2

ct+1

s+1

+

βt

t

(cid:1) + η2

t L − 2ct+1η2
t L2
t L3
b
b
t L − 2ct+1η2

t

≤ R

(cid:17) E[(cid:107)∇f (xs+1
(cid:105) E(cid:2)(cid:107)xs+1
t − ˜xs(cid:107)2(cid:3)
(cid:1)E[(cid:107)∇f (xs+1

)(cid:107)2]

)(cid:107)2].

t

t

βt

Proof. Using essentially the same argument as the proof of Lemma. 1 until Equation (9), we have

We use Lemma 4 in order to bound E[(cid:107)us+1
tion (14), we see that

t

(14)
(cid:107)2] in the above inequality. Substituting it in Equa-

The second inequality follows from the deﬁnition of ct and R

s+1
t

, thus concluding the proof.

Our intermediate key result is the following theorem that provides convergence rate of mini-batch

Svrg.

Theorem 9. Let γn denote the following quantity:

(cid:0)η − ct+1η

β − η2L − 2ct+1η2(cid:1).

γn := min

0≤t≤m−1

Suppose ηt = η and βt = β for all t ∈ {0, . . . , m− 1}, cm = 0, ct = ct+1(1 + ηtβt + 2η2
for
t ∈ {0, . . . , m − 1} and γn > 0. Further, let pm = 1 and pi = 0 for 0 ≤ i < m. Then for the output
xa of mini-batch version of Algorithm 2 with mini-batch size b, we have

) + η2

t L2
b

t L3
b

E[(cid:107)∇f (xa)(cid:107)2] ≤ f (x0) − f (x∗)

T γn

,

where x∗ is an optimal solution to (1).
Proof. Since ηt = η for t ∈ {0, . . . , m − 1}, using Lemma 2 and telescoping the sum, we obtain

(cid:88)m−1

t=0

E[(cid:107)∇f (xs+1

t

)(cid:107)2] ≤ R

s+1

0 − R
γn

s+1
m

.

21

This inequality in turn implies that(cid:88)m−1

E[(cid:107)∇f (xs+1

t

t=0

)(cid:107)2] ≤ E[f (˜xs) − f (˜xs+1)]

γn

,

where we used that R
and that R
epochs and using the fact that ˜x0 = x0, we get the desired result.

m = E[f (xs+1
0 = E[f (˜xs)] (since xs+1

m )] = E[f (˜xs+1)] (since cm = 0, pm = 1, and pi = 0 for i < m),
0 = ˜xs, as pm = 1 and pi = 0 for i < m). Now sum over all

s+1

s+1

We now present the proof of Theorem 7 using the above results.

Theorem. Let γn denote the following quantity:

(cid:0)η − ct+1η

β − η2L − 2ct+1η2(cid:1).

γn := min

0≤t≤m−1

t L3/b for 0 ≤ t ≤ m − 1. Suppose η = µ2b/(Ln2/3)
where cm = 0, ct = ct+1(1 + ηβ + 2η2L2/b) + η2
(0 < µ2 < 1), β = L/n1/3, m = (cid:98)n/(3bµ2)(cid:99) and T is some multiple of m. Then for the mini-batch
version of Algorithm 2 with mini-batch size b < n2/3, there exists universal constants µ2, ν2 > 0 such
that we have the following: γn ≥ ν2b

Ln2/3 and

E[(cid:107)∇f (xa)(cid:107)2] ≤ Ln2/3[f (x0) − f (x∗)]

bT ν2

,

where x∗ is optimal for (1).

Proof of Theorem 7. We ﬁrst observe that using the speciﬁed values of β and η we obtain

θ :=

2η2L2

b

+ ηβ =

2µ2
2b
n4/3

+

µ2b
n

≤ 3µ2b
n

.

The above inequality follows since µ2 ≤ 1 and n ≥ 1. For our analysis, we will require the following
bound on c0:

(1 + θ)m − 1

c0 =

µ2
2b2L
bn4/3

θ

≤ n−1/3(µ2L(e − 1)),

µ2bL((1 + θ)m − 1)

2bµ2 + bn1/3

=

(15)

wherein the ﬁrst equality holds due to the relation ct = ct+1(1 + ηtβt + 2η2
inequality follows upon again noting that (1 + 1/l)l is increasing for l > 0 and liml→∞(1 + 1
Now we can lower bound γn, as

) + η2

t L2
b

t L3
b

, and the
l )l = e.

(cid:0)η − ct+1η
β − η2L − 2ct+1η2(cid:1)
β − η2L − 2c0η2(cid:1) ≥ bν2

≥(cid:0)η − c0η

t

γn = min

,

Ln2/3

where ν2 is a constant independent of n. The ﬁrst inequality holds since ct decreases with t. The
second one holds since (a) c0/β is upper bounded by a constant independent of n as c0/β ≤ µ2(e−1)
2(e − 1)η (again due to
(due to Equation (15)), (b) η2L ≤ µ2η (as b < n2/3) and (c) 2c0η2 ≤ 2µ2
Equation (15) and the fact b < n2/3). By choosing an appropriately small constant µ2 (independent
of n), one can ensure that γn ≥ bν2/(Ln2/3) for some universal constant ν2. For example, choosing
µ2 = 1/4, we have γn ≥ bν2/(Ln2/3) with ν2 = 1/40. Substituting the above lower bound in
Theorem 9, we get the desired result.

22

F MSVRG: Convergence Rate

Proof of Theorem 8
√
Theorem. Let f ∈ Fn have σ-bounded gradients. Let ηt = η = max{c/
universal constant from Corollary 3), m = (cid:98)n/(3µ1)(cid:99), and c =
multiple of m, pm = 1, and pi = 0 for 0 ≤ i < m. Then, the output xa of Algorithm 2 satisﬁes

T , µ1/(Ln2/3)} (µ1 is the
. Further, let T be a

(cid:114)

E[(cid:107)∇f (xa)(cid:107)2]
≤ ¯ν min

(cid:110)

2

2(f (x0) − f (x∗))L

T

σ,

Ln2/3[f (x0) − f (x∗)]

(cid:113) f (x0)−f (x∗)
(cid:111)

2Lσ2

,

T ν1

where ¯ν is a universal constant, ν1 is the universal constant from Corollary 3 and x∗ is an optimal
solution to (1).
√
Proof. First, we observe that the step size η is chosen to be max{c/

T , µ1/(Ln2/3)} where

(cid:114)

c =

f (x0) − f (x∗)

2Lσ2

.

√
Suppose η = µ1/(Ln2/3), we obtain the convergence rate in Corollary 3. Now, lets consider the case
where η = c/

T . In this case, we have the following bound:

t

(cid:107)2]

E[(cid:107)vs+1
= E[(cid:107)∇fit(xs+1

≤ 2(cid:0)E[(cid:107)∇fit(xs+1
≤ 2(cid:0)E[(cid:107)∇fit(xs+1

t

t

) − ∇fit(˜xs) + ∇f (˜xs)(cid:107)2]

)(cid:107)2 + (cid:107)∇fit(˜xs) − ∇f (˜xs)(cid:107)2](cid:1)
)(cid:107)2 + (cid:107)∇fit(˜xs)(cid:107)2](cid:1)

t

≤ 4σ2.

The ﬁrst inequality follows from Lemma 7 with r = 2. The second inequality follows from (a)
σ-bounded gradient property of f and (b) the fact that for a random variable ζ, E[(cid:107)ζ − E[ζ](cid:107)2] ≤
E[(cid:107)ζ(cid:107)2]. The rest of the proof is along exactly the lines as in Theorem 1. This provides a convergence
√
rate similar to Theorem 1. More speciﬁcally, using step size c/

T , we get

E[(cid:107)f (xa)(cid:107)2] ≤ 2

2(f (x0) − f (x∗))L

T
√
The only thing that remains to be proved is that with the step size choice of max{c/
√
the minimum of two bounds hold. Consider the case c/
following:

(16)
T , µ1/(Ln2/3)},
T > µ1/(Ln2/3). In this case, we have the

σ.

(cid:114)

(cid:113) 2(f (x0)−f (x∗))L

T

Ln2/3[f (x0)−f (x∗)]

T ν1

2

σ

=

√

2LT

2ν1σ

Ln2/3(cid:112)f (x0) − f (x∗)
(cid:26) 2ν1

≤ 2ν1/µ1 ≤ ¯ν := max

µ1
√
where ν1 is the constant in Corollary 3. This inequality holds since c/
the above inequality, we have

(cid:114)

2

2(f (x0) − f (x∗))L

T

σ ≤ ¯νLn2/3[f (x0) − f (x∗)]

T

23

(cid:27)

,

,

µ1
2ν1

T > µ1/(Ln2/3). Rearranging

√
in this case. Note that the left hand side of the above inequality is precisely the bound obtained by
T ≤ µ1/(Ln2/3), the inequality holds
using step size c/
in the other direction. Using these two observations, we have the desired result.

√
T (see Equation (16)). Similarly, when c/

G Key Lemmatta

Lemma 3. For the intermediate iterates vs+1

t

computed by Algorithm 2, we have the following:

t − ˜xs(cid:107)2].
Proof. The proof simply follows from the proof of Lemma 4 with It = {it}.

(cid:107)2] ≤ 2E[(cid:107)∇f (xs+1

)(cid:107)2] + 2L2E[(cid:107)xs+1

E[(cid:107)vs+1

t

t

We now present a result to bound the variance of mini-batch Svrg.

Lemma 4. Let us+1
mini-batch size b. Then,

t

be computed by the mini-batch version of Algorithm 2 i.e., Algorithm 4 with

E[(cid:107)us+1

t

(cid:107)2] ≤ 2E[(cid:107)∇f (xs+1

t

)(cid:107)2] + 2L2

b

E[(cid:107)xs+1

t − ˜xs(cid:107)2].

Proof. For the ease of exposition, we use the following notation:

(cid:0)∇fit(xs+1

t

) − ∇fit(˜xs)(cid:1) .

(cid:88)

it∈It

ζ s+1
t =

1
|It|

We use the deﬁnition of us+1

t

to get
(cid:107)2] = E[(cid:107)ζ s+1
t + ∇f (˜xs)(cid:107)2]
t + ∇f (˜xs) − ∇f (xs+1
)(cid:107)2] + 2E[(cid:107)ζ s+1

E[(cid:107)us+1
= E[(cid:107)ζ s+1
≤ 2E[(cid:107)∇f (xs+1

t

t

= 2E[(cid:107)∇f (xs+1

t

)(cid:107)2] +

E

2
b2

)(cid:107)2]

t

) + ∇f (xs+1
](cid:107)2]

t − E[ζ s+1

t

t

(cid:0)∇fit(xs+1

t

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:88)

it∈It

) − ∇fit(˜xs) − E[ζ s+1

t

](cid:1)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2

The ﬁrst inequality follows from Lemma 7 (with r = 2) and the fact that E[ζ s+1
∇f (˜xs). From the above inequality, we get

t

] = ∇f (xs+1

t

) −

t

(cid:107)2]

E[(cid:107)us+1
≤ 2E[(cid:107)∇f (xs+1
≤ 2E[(cid:107)∇f (xs+1

t

t

)(cid:107)2] +
)(cid:107)2] +

) − ∇fit(˜xs)(cid:107)2]

t

E[(cid:107)∇fit(xs+1
2
b
2L2
b

E[(cid:107)xs+1

t − ˜xs(cid:107)2]

The ﬁrst inequality follows from the fact that the indices it are drawn uniformly randomly and
independently from {1, . . . , n} and noting that for a random variable ζ, E[(cid:107)ζ − E[ζ](cid:107)2] ≤ E[(cid:107)ζ(cid:107)2].
The last inequality follows from L-smoothness of fit.

H Experiments

Figure 2 shows the remaining plots for MNIST and STL-10 datasets. As seen in the plots, there is
no signiﬁcant diﬀerence in the test error of Svrg and Sgd for these datasets.

24

Figure 2: Neural network results for MNIST and STL-10. The leftmost result is for MNIST. The
remaining two plots are of STL-10.

I Other Lemmas

We need Lemma 5 for our results in the convex case.
Lemma 5 (Johnson & Zhang (2013)). Let g : Rd → R be convex with L-Lipschitz continuous
gradient. Then,

(cid:107)∇g(x) − ∇g(y)(cid:107)2 ≤ 2L[g(x) − g(y) − (cid:104)∇g(y), x − y(cid:105)],

for all x, y ∈ Rd.
Proof. Consider h(x) := g(x) − g(y) − (cid:104)∇g(y), x − y(cid:105) for arbitrary y ∈ Rd. Observe that ∇h is also
L-Lipschitz continuous. Note that h(x) ≥ 0 (since h(y) = 0 and ∇h(y) = 0, or alternatively since h
deﬁnes a Bregman divergence), from which it follows that

[h(x − ρ∇h(x))]
[h(x) − ρ(cid:107)∇h(x)(cid:107)2 + Lρ2

ρ

0 ≤ min
≤ min
= h(x) − 1

ρ

2L(cid:107)∇h(x)(cid:107)2.

2 (cid:107)∇h(x)(cid:107)2]

Rewriting in terms of g we obtain the required result.

Lemma 6 bounds the variance of Svrg for the convex case. Please refer to (Johnson & Zhang,

2013) for more details.
Lemma 6 ((Johnson & Zhang, 2013)). Suppose fi is convex for all i ∈ [n]. For the updates in
Algorithm 2 we have the following inequality:

E[(cid:107)vs+1

t

(cid:107)2] ≤ 4L[f (xs+1

t

) − f (x∗) + f (˜xs − f (x∗)].

Proof. The proof follows upon observing the following:

(cid:107)2 = E[(cid:107)∇fit(xs+1

E[(cid:107)vs+1
≤ 2E[(cid:107)∇fit(xs+1

) − ∇fit(x∗)(cid:107)2]

t

t

t

) − ∇fit(xs+1

0

) + ∇f (˜xs)(cid:107)2]

+ 2E[(cid:107)∇fit(˜xs) − ∇fit(x∗) − (∇f (˜xs) − ∇f (x∗))(cid:107)2]

≤ 2E[(cid:107)∇fit(xs+1

t

) − ∇fit(x∗)(cid:107)2]

+ 2E[(cid:107)∇fit(˜xs) − ∇fit(x∗)(cid:107)2]
t − f (x∗) + f (˜xs) − f (x∗)].

≤ 4L[f (xs+1

The ﬁrst inequality follows from Cauchy-Schwarz and Young inequality; the second one from E[(cid:107)ξ −
E[ξ](cid:107)2] ≤ E[(cid:107)ξ(cid:107)2], and the third one from Lemma 5.

25

050100150200250300# grad / n0.0440.0460.0480.0500.0520.0540.0560.0580.060Test ErrorSGDSVRG050100150200250300# grad / n10-410-310-210-1100101k∇f(xt)k2SGDSVRG050100150200250300# grad / n0.580.600.620.640.660.680.70Test ErrorSGDSVRGLemma 7. For random variables z1, . . . , zr, we have

E(cid:2)(cid:107)z1 + ... + zr(cid:107)2(cid:3) ≤ rE(cid:2)(cid:107)z1(cid:107)2 + ... + (cid:107)zr(cid:107)2(cid:3) .

26

