Deep Reinforcement Learning from Self-Play in Imperfect-Information Games

6
1
0
2

 
r
a

M
3

 

 
 
]

G
L
.
s
c
[
 
 

1
v
1
2
1
1
0

.

3
0
6
1
:
v
i
X
r
a

Johannes Heinrich
David Silver
University College London, UK

J.HEINRICH@CS.UCL.AC.UK
D.SILVER@CS.UCL.AC.UK

Abstract

Many real-world applications can be described
as large-scale games of imperfect information.
To deal with these challenging domains, prior
work has focused on computing Nash equilib-
ria in a handcrafted abstraction of the domain.
In this paper we introduce the ﬁrst scalable end-
to-end approach to learning approximate Nash
equilibria without any prior knowledge. Our
method combines ﬁctitious self-play with deep
reinforcement learning. When applied to Leduc
poker, Neural Fictitious Self-Play (NFSP) ap-
proached a Nash equilibrium, whereas common
reinforcement learning methods diverged.
In
Limit Texas Hold’em, a poker game of real-
world scale, NFSP learnt a competitive strategy
that approached the performance of human ex-
perts and state-of-the-art methods.

1. Introduction
Games have a tradition of encouraging advances in ar-
tiﬁcial intelligence and machine learning (Samuel, 1959;
Tesauro, 1995; Campbell et al., 2002; Riedmiller et al.,
2009; Gelly et al., 2012; Bowling et al., 2015). Game
theory deﬁnes a game as a domain of conﬂict or cooper-
ation between several entities (Myerson, 1991). One mo-
tivation of studying the simpler recreational games is to
develop algorithms that will scale to more complex, real-
world games such as airport and network security, ﬁnancial
and energy trading, trafﬁc control and routing (Lambert III
et al., 2005; Nevmyvaka et al., 2006; Bazzan, 2009; Tambe,
2011; Urieli & Stone, 2014; Durkota et al., 2015). Most of
these real-world games involve decision making with im-
perfect information and high-dimensional information state
spaces. Unfortunately, many machine learning methods,
that have been applied to classical games, lack convergence
guarantees for learning in imperfect-information games.
On the other hand, many game-theoretic approaches lack
the ability to extract relevant patterns and generalise from
data. This results in limited scalability to large games, un-
less the domain is abstracted to a manageable size using

human expert knowledge, heuristics or modelling. How-
ever, acquiring human expertise often requires expensive
resources and time.
In addition, humans can be eas-
ily fooled into irrational decisions or assumptions (Selten,
1990; Ariely & Jones, 2008). This motivates algorithms
that learn useful strategies end-to-end.
In this paper we introduce NFSP, a deep reinforcement
learning method for learning approximate Nash equilib-
ria of imperfect-information games. NFSP agents learn
by playing against themselves without explicit prior knowl-
edge. Technically, NFSP extends and instantiates Fictitious
Self-Play (FSP) (Heinrich et al., 2015) with neural network
function approximation. An NFSP agent consists of two
neural networks and two kinds of memory. Memorized ex-
perience of play against fellow agents is used by reinforce-
ment learning to train a network that predicts the expected
values of actions. Experience of the agent’s own behaviour
is stored in a separate memory, which is used by super-
vised learning to train a network that predicts the agent’s
own average behaviour. An NFSP agent acts cautiously by
sampling its actions from a mixture of its average, routine
strategy and its greedy strategy that maximizes its predicted
expected value. NFSP approximates ﬁctitious play, which
is a popular game-theoretic model of learning in games that
converges to Nash equilibria in some classes of games, e.g.
two-player zero-sum and many-player potential games.
We empirically evaluate our method in two-player zero-
sum computer poker games. In this domain, current game-
theoretic approaches use heuristics of card strength to ab-
stract the game to a tractable size (Zinkevich et al., 2007;
Gilpin et al., 2007; Johanson et al., 2013). While Limit
Texas Hold’em (LHE), a poker game of real-world scale,
has got within reach of being solved with current compu-
tational resources (Bowling et al., 2015), most other poker
and real-world games remain far out of scope without ab-
straction. Our approach does not rely on engineering such
abstractions or any other prior knowledge. NFSP agents
leverage deep reinforcement learning to learn directly from
their experience of interacting in the game. When ap-
plied to Leduc poker, NFSP approached a Nash equilib-
rium, whereas common reinforcement learning methods di-

Deep Reinforcement Learning from Self-Play in Imperfect-Information Games

verged. We also applied NFSP to LHE, learning directly
from the raw inputs. NFSP learnt a competitive strategy,
approaching the performance of state-of-the-art methods
based on handcrafted abstractions.

2. Background
In this section we provide a brief overview of reinforcement
learning, extensive-form games and ﬁctitious self-play. For
a more detailed exposition we refer the reader to (Sutton
& Barto, 1998), (Myerson, 1991), (Fudenberg, 1998) and
(Heinrich et al., 2015).

2.1. Reinforcement Learning

its gain, Gt =(cid:80)T

Reinforcement learning (Sutton & Barto, 1998) agents typ-
ically learn to maximize their expected future rewards from
interaction with an environment. The environment is usu-
ally modelled as a Markov decision process (MDP). An
agent behaves according to a policy that speciﬁes a distri-
bution over available actions at each state of the MDP. The
agent’s goal is to improve its policy in order to maximize
i=t Ri+1, which is a random variable of
the agent’s cumulative future rewards starting from time t.
Many reinforcement learning algorithms learn from se-
quential experience in the form of transition tuples,
(st, at, rt+1, st+1), where st is the state at time t, at is
the action chosen in that state, rt+1 the reward received
thereafter and st+1 the next state that the agent transitioned
to. A common objective is to learn the action-value func-
tion, Q(s, a) = Eπ [Gt | St = s, At = a], deﬁned as the
expected gain of taking action a in state s and following
policy π thereafter. An agent is learning on-policy if it
learns about the policy that it is currently following. In the
off-policy setting an agent learns from experience of an-
other agent or another policy, e.g. a previous policy.
Q-learning (Watkins & Dayan, 1992) is a popular off-
policy reinforcement learning method. It learns about the
greedy policy, which at each state takes the action of the
highest estimated value. Storing and replaying past expe-
rience by applying off-policy reinforcement learning to the
respective transition tuples is known as experience replay
(Lin, 1992). Fitted Q Iteration (FQI) (Ernst et al., 2005) is
a batch reinforcement learning method that replays expe-
rience with Q-learning. Neural Fitted Q Iteration (NFQ)
(Riedmiller, 2005) and Deep Q Network (DQN) (Mnih
et al., 2015) are extensions of FQI that use neural network
function approximation with batch and online updates re-
spectively.

2.2. Extensive-Form Games
Extensive-form games are a model of sequential interac-
tion involving multiple players. Assuming rationality, each

1, ai

1, si

2, ai

2, ..., si

k, ai

k=1 πi(si

t) =(cid:81)t−1

player’s goal is to maximize his payoff in the game.
In
imperfect-information games, each player only observes
his respective information states, e.g.
in a poker game
a player only knows his own private cards but not those of
other players. Each player chooses a behavioural strat-
egy that maps information states to probability distribu-
tions over available actions. We assume games with per-
fect recall, i.e.
each player’s current information state
si
t implies knowledge of the sequence of his information
states and actions, si
t, that led to this in-
formation state. The realization-probability (Von Stengel,
1996), xπi(si
k), determines the proba-
bility that player i’s behavioural strategy, πi, contributes
t. A strategy proﬁle
to realizing his information state si
π = (π1, ... , πn) is a collection of strategies for all play-
ers. π−i refers to all strategies in π except πi. Given
a ﬁxed strategy proﬁle π−i, any strategy of player i that
achieves optimal payoff performance against π−i is a best
response. An approximate or -best response is suboptimal
by no more than . A Nash equilibrium is a strategy pro-
ﬁle such that each player’s strategy in this proﬁle is a best
response to the other strategies. Similarly, an approximate
or -Nash equilibrium is a proﬁle of -best responses. In a
Nash equilibrium no player can gain by deviating from his
strategy. Therefore, a Nash equilibrium can be regarded as
a ﬁxed point of rational self-play learning. In fact, Nash
equilibria are the only strategy proﬁles that rational agents
can hope to converge on in self-play (Bowling & Veloso,
2001).

2.3. Fictitious Self-Play
Fictitious play (Brown, 1951) is a game-theoretic model
of learning from self-play. Fictitious players choose best
responses to their opponents’ average behaviour. The av-
erage strategies of ﬁctitious players converge to Nash equi-
libria in certain classes of games, e.g. two-player zero-sum
and many-player potential games (Robinson, 1951; Mon-
derer & Shapley, 1996). Leslie & Collins (2006) intro-
duced generalised weakened ﬁctitious play.
It has simi-
lar convergence guarantees as common ﬁctitious play, but
allows for approximate best responses and perturbed av-
erage strategy updates, making it particularly suitable for
machine learning.
Fictitious play is commonly deﬁned in normal form, which
is exponentially less efﬁcient for extensive-form games.
Heinrich et al. (2015) introduce Full-Width Extensive-
Form Fictitious Play (XFP) that enables ﬁctitious players
to update their strategies in behavioural, extensive form, re-
sulting in linear time and space complexity. A key insight
is that for a convex combination of normal-form strate-
gies, ˆσ = λ1 ˆπ1 + λ2 ˆπ2, we can achieve a realization-
equivalent behavioural strategy σ, by setting it to be propor-
tional to the respective convex combination of realization-

Deep Reinforcement Learning from Self-Play in Imperfect-Information Games

probabilities,
σ(s, a) ∝ λ1xπ1(s)π1(s, a)+λ2xπ2 (s)π2(s, a)∀s, a, (1)

where λ1xπ1 (s) + λ2xπ2 (s) is the normalizing constant for
the strategy at information state s.
In addition to deﬁn-
ing a full-width average strategy update of ﬁctitious play-
ers in behavioural strategies, equation (1) prescribes a way
to sample data sets of such convex combinations of strate-
gies. Heinrich et al. (2015) introduce Fictitious Self-Play
(FSP), a sample- and machine learning-based class of al-
gorithms that approximate XFP. FSP replaces the best re-
sponse computation and the average strategy updates with
reinforcement and supervised learning respectively. In par-
ticular, FSP agents generate datasets of their experience in
self-play. Each agent stores its experienced transition tu-
ples, (st, at, rt+1, st+1), in a memory, MRL, designated
for reinforcement learning. Experience of the agent’s own
behaviour, (st, at), is stored in a separate memory, MSL,
designated for supervised learning. Self-play sampling is
set up in a way that an agent’s reinforcement learning mem-
ory approximates data of an MDP deﬁned by the other
players’ average strategy proﬁle. Thus, an approximate so-
lution of the MDP by reinforcement learning yields an ap-
proximate best response. Similarly, an agent’s supervised
learning memory approximates data of the agent’s own av-
erage strategy, which can be learned by supervised classiﬁ-
cation.

3. Neural Fictitious Self-Play
NFSP is an evolution of FSP, introducing multiple ex-
tensions such as neural network function approximation,
reservoir sampling, anticipatory dynamics and a fully
agent-based approach. An NFSP agent interacts with the
other players in a game and memorizes its experience of
game transitions and its own behaviour. NFSP treats these
memories as two datasets suitable for deep reinforcement
learning and supervised classiﬁcation.
In particular, the
agent trains a neural network, FQ, to predict action values,
Q(s, a), from the data in MRL using off-policy reinforce-
ment learning. The resulting network deﬁnes the agent’s
approximate best response strategy, β = -greedy(FQ),
which selects a random action with probability  and other-
wise chooses the action that maximizes the predicted action
values. The NFSP agent trains a separate neural network,
FS, to imitate its own past behaviour using supervised clas-
siﬁcation on the data in MSL. This network maps states to
action probabilities and deﬁnes the agent’s average strat-
egy, π = FS. During play, the agent chooses its actions
from a mixture of its two strategies, β and π.
While ﬁctitious players usually best respond to the average
strategy of their opponents, in continuous-time dynamic
ﬁctitious play (Shamma & Arslan, 2005) players choose

Algorithm 1 Neural Fictitious Self-Play (NFSP) with
DQN
Require:

Γ {Game}
MRL, MSL {RL and SL memories}
FQ, FS {Action value and policy networks}
β = -GREEDY(FQ) {Best response policy}
π = FS {Average policy}
σ {Current policy}

Ensure: π an approximate Nash equilibrium in self-play

function STEP()

st, rt, ct ← OBSERVE(Γ)
at ← THINK(st, rt, ct)
ACT(Γ, at)
end function
function THINK(st, rt, ct)

if ct = 0 {episode terminated} then

σ ← SAMPLEPOLICY(β, π)

end if
if st−1 (cid:54)= nil then

τt ← (st−1, at−1, rt, st, ct)
UPDATERLMEMORY(MRL, τt)

end if
at ← SAMPLEACTION(σ)
if σ = β then

UPDATESLMEMORY(MSL, (st, at))

end if
st−1 ← st
at−1 ← at
β ← REINFORCEMENTLEARNING(MRL)
π ← SUPERVISEDLEARNING(MSL)

end function
function REINFORCEMENTLEARNINIG(MRL)

FQ ← DQN(MRL)
return -GREEDY(FQ)

end function
function SUPERVISEDLEARNING(MSL)

FS ← Apply stochastic gradient descent to loss

E(s,a)∼MSL [− log π(s, a)]

return FS
end function

t

t + η d

dt ˆπ−i

best responses to a short-term prediction of their oppo-
nents’ average normal-form strategies, ˆπ−i
. The
authors show that for appropriate, game-dependent choice
of η stability of ﬁctitious play at equilibrium points can be
improved. NFSP uses ˆβi
t as a discrete-
time approximation of the derivative that is used in these
t+1 − ˆπi
anticipatory dynamics. Note that ∆ˆπi
t is the
normal-form update direction of common discrete-time ﬁc-
titious play. In order for an NFSP agent to compute an ap-
proximate best response, βi, to its opponents’ anticipated
average strategy proﬁle, σ−i ≡ ˆπ−i + η( ˆβ−i − ˆπ−i), the

dt ˆπi
t ∝ ˆβi

t+1 − ˆπi

t ≈ d

Deep Reinforcement Learning from Self-Play in Imperfect-Information Games

(cid:2)Gi

t

(cid:80)T

(cid:12)(cid:12) St = s, At = a(cid:3). This can

agent iteratively evaluates and maximizes its action val-
ues, Qi(s, a) ≈ Eβi,σ−i
be achieved by off-policy reinforcement learning, e.g. Q-
learning or DQN, on experience of play against the oppo-
nents’ anticipated strategy, σ−i. To ensure that the agents’
reinforcement learning memories, MRL, contain this kind
of experience, NFSP requires all agents to choose their ac-
tions from σ ≡ (1 − η)ˆπ + η ˆβ, where η ∈ R is called the
anticipatory parameter.
Fictitious play usually keeps track of the average of
normal-form best response strategies that players have cho-
ˆβi
sen in the game, ˆπi
t. Heinrich et al. (2015)
propose to use sampling and machine learning to gener-
ate data on and learn convex combinations of normal-form
strategies in extensive form. E.g. we can generate a set
of extensive-form data of πi
T by sampling whole episodes
of the game, using βi
t, t = 1, ... , T , in proportion to their
weight, 1
T , in the convex combination. NFSP uses reservoir
sampling (Vitter, 1985; Osborne et al., 2014) to memorize
experience of its average best responses. The agent’s su-
pervised learning memory, MSL, is a reservoir to which it
only adds experience when following its approximate best
response policy β. An NFSP agent regularly trains its av-
erage policy network, π = FS, to match its average be-
haviour stored in its supervised learning memory, e.g. by
optimizing the log-probability of past actions taken. Al-
gorithm 1 presents NFSP, using DQN for reinforcement
learning.

T = 1
T

t=1

4. Experiments
We evaluate NFSP and related algorithms in Leduc
(Southey et al., 2005) and Limit Texas Hold’em poker
games. Most of our experiments measure the exploitabil-
ity of learned strategy proﬁles. In a two-player zero-sum
game, the exploitability of a strategy proﬁle is deﬁned as
the expected average payoff that a best response proﬁle
achieves against it. An exploitability of 2δ yields at least
a δ-Nash equilibrium.

4.1. Robustness of XFP

To understand how function approximation interacts with
FSP, we begin with some simple experiments that emulate
approximation and sampling errors in the full-width algo-
rithm XFP. Firstly, we explore what happens when the per-
fect averaging used in XFP is replaced by an incremental
averaging process closer to gradient descent. Secondly, we
explore what happens when the exact table lookup used in
XFP is replaced by an approximation with epsilon error.
Figure 1 shows the performance of XFP with default, 1/T ,
and constant stepsizes for its strategy updates. We see im-
proved asymptotic but lower initial performance for smaller

Figure 1. The impact of constant stepsizes on the performance of
full-width ﬁctitious play in Leduc Hold’em.

stepsizes. For constant stepsizes the performance seems to
plateau rather than diverge. With reservoir sampling we
can achieve an effective stepsize of 1/T . However, the
results suggest that exponentially-averaged reservoir sam-
pling can be a viable choice too, as exponential averaging
of past memories would approximately correspond to using
a constant stepsize.
XFP with stepsize 1 is equivalent to a full-width iterated
best response algorithm. While this algorithm converges to
a Nash equilibrium in ﬁnite perfect-information two-player
zero-sum games, the results suggest that with imperfect in-
formation this is not generally the case. The Poker-CNN
algorithm introduced by Yakovenko et al. (2016) stores a
small number of past strategies which it iteratively com-
putes new strategies against. Replacing strategies in that
set is similar to updating an average strategy with a large
stepsize. This might lead to similar problems as shown in
Figure 1.

Figure 2. The performance of XFP in Leduc Hold’em with
uniform-random noise added to the best response computation.

 0.1 1 10 1 10 100 1000ExploitabilityIterationsXFP, stepsize 1/TXFP, stepsize 1XFP, stepsize 0.5XFP, stepsize 0.1XFP, stepsize 0.05XFP, stepsize 0.01 0.1 1 10 1 10 100 1000ExploitabilityIterationsXFP, no noiseXFP, noise 0.5XFP, noise 0.4XFP, noise 0.3XFP, noise 0.2XFP, noise 0.1Deep Reinforcement Learning from Self-Play in Imperfect-Information Games

Our NFSP agents add random exploration to their poli-
cies and use noisy stochastic gradient updates to learn ac-
tion values, which determine their approximate best re-
sponses. Therefore, we investigated the impact of random
noise added to the best response computation, which XFP
performs by dynamic programming. At each backward
induction step, we pass back a uniform-random action’s
value with probability  and the best action’s value oth-
erwise. Figure 2 shows monotonically decreasing perfor-
mance with added noise. However, performance remains
stable and keeps improving for all noise levels.

4.2. Convergence of NFSP

We empirically investigate the convergence of NFSP to
Nash equilibria in Leduc Hold’em. We also study whether
removing or altering some of NFSP’s components breaks
convergence.
One of our goals is to minimize reliance on prior knowl-
edge. Therefore, we attempt to deﬁne an objective encod-
ing of information states in poker games. Contrary to other
work on computer poker (Zinkevich et al., 2007; Gilpin
et al., 2007; Johanson et al., 2013), we do not engineer
any higher-level features. Poker games usually consist of
multiple rounds. At each round new cards are revealed to
the players. We represent each rounds’ cards by a k-of-n
encoding. E.g. LHE has a card deck of 52 cards and on
the second round three cards are revealed. Thus, this round
is encoded with a vector of length 52 and three elements
set to 1 and the rest to 0. In Limit Hold’em poker games,
players usually have three actions to choose from, namely
{fold, call, raise}. Note that depending on context, calls
and raises can be referred to as checks and bets respec-
tively. Betting is capped at a ﬁxed number of raises per
round. Thus, we can represent the betting history as a ten-
sor with 4 dimensions, namely {player, round, number of
raises, action taken}. E.g. heads-up LHE contains 2 play-
ers, 4 rounds, 0 to 4 raises per round and 3 actions. Thus
we can represent a LHE betting history as a 2 × 4 × 5 × 3
tensor. In a heads-up game we do not need to encode the
fold action, as a two-player game always ends if one play-
ers gives up. Thus, we can ﬂatten the 4-dimensional tensor
to a vector of length 80. Concatenating with the card in-
puts of 4 rounds, we encode an information state of LHE
as a vector of length 288. Similarly, an information state of
Leduc Hold’em can be encoded as a vector of length 30, as
it contains 6 cards with 3 duplicates, 2 rounds, 0 to 2 raises
per round and 3 actions.
For learning in Leduc Hold’em, we manually calibrated
NFSP for a fully connected neural network with 1 hidden
layer of 64 neurons and rectiﬁed linear activations. We then
repeated the experiment for various network architectures
with the same parameters. In particular, we set the sizes

of memories to 200k and 2m for MRL and MSL respec-
tively. MRL functioned as a circular buffer containing a re-
cent window of experience. MSL was updated with reser-
voir sampling (Vitter, 1985). The reinforcement and super-
vised learning rates were set to 0.1 and 0.005, and both
used vanilla Stochastic Gradient Descent (SGD) without
momentum for stochastic optimization of the neural net-
works. Each agent performed 2 stochastic gradient updates
of mini-batch size 128 per network for every 128 steps in
the game. The target network of the DQN algorithm was
reﬁtted every 300 updates. NFSP’s anticipatory parame-
ter was set to η = 0.1. The -greedy policies’ exploration
started at 0.06 and decayed to 0, proportionally to the in-
verse square root of the number of iterations.

Figure 3. Learning performance of NFSP in Leduc Hold’em.

Figure 3 shows NFSP approaching Nash equilibria for vari-
ous network architectures. We observe a monotonic perfor-
mance increase with size of the networks. NFSP achieved
an exploitability of 0.06, which full-width XFP typically
achieves after around 1000 full-width iterations.

Figure 4. Breaking learning performance in Leduc Hold’em by
removing essential components of NFSP.

 0.01 0.1 1 10 1000 10000 100000 1e+06ExploitabilityIterations128 hidden neurons64 hidden neurons32 hidden neurons16 hidden neurons8 hidden neurons 0.01 0.1 1 10 1000 10000 100000 1e+06ExploitabilityIterationsNFSPNFSP with sliding window SL memoryNFSP with exponentially-averaging SL reservoirNFSP with anticipatory parameter of 0.5Deep Reinforcement Learning from Self-Play in Imperfect-Information Games

In order to investigate the relevance of various compo-
nents of NFSP, e.g.
reservoir sampling and anticipatory
dynamics, we conducted an experiment that isolated their
effects. Figure 4 shows that these modiﬁcations led to
decremental performance. In particular, using a ﬁxed-size
sliding window to store experience of the agents’ own be-
haviour led to divergence. For a high anticipatory param-
eter of 0.5 NFSP’s performance plateaued. Finally, using
exponentially-averaged reservoir sampling for supervised
learning memory updates led to noisy performance.

4.3. Comparison to DQN

Several stable algorithms have previously been proposed
for deep reinforcement learning, notably the DQN algo-
rithm (Mnih et al., 2015). However, the empirical stabil-
ity of these algorithms was only previously established in
single-agent, perfect (or near-perfect) information MDPs.
Here, we investigate the stability of DQN in multi-agent,
imperfect-information games, in comparison to NFSP.

Figure 5. Comparing performance to DQN in Leduc Hold’em.

DQN learns a deterministic, greedy strategy. This is sufﬁ-
cient to behave optimally in MDPs, which the algorithm is
designed for.
Imperfect-information games, on the other
hand, generally require stochastic strategies for optimal
behaviour. Thus, in addition to DQN’s -greedy strat-
egy, we store its actions in a supervised learning mem-
ory, MSL, and learn its average behaviour. This aver-
age policy does not affect DQN’s runtime behaviour at
all, as it is never played. We implement this variant of
DQN by using NFSP with an anticipatory parameter of
η = 1. We set most of DQN’s parameters to be equal
to those found for NFSP in the previous section’s exper-
iments. This is motivated by the supervised learning pa-
rameters not directly affecting DQN’s performance. We
trained DQN with all combinations of the following param-
eters: Learning rate {0.2, 0.1, 0.05}, decaying exploration
starting at {0.06, 0.12} and reinforcement learning mem-

ory {2m reservoir, 2m sliding window}. We then chose the
best-performing result of DQN and compared to the perfor-
mance of NFSP that was achieved in the previous section’s
experiment. DQN achieved its best performing result with
a learning rate of 0.1, exploration starting at 0.12 and a
sliding window memory of size 2m.
Figure 5 shows that DQN’s deterministic strategy is highly
exploitable, which is expected as imperfect-information
games usually require stochastic policies. DQN’s aver-
age behaviour does not approach a Nash equilibrium either.
This is notable because DQN stores its experience in a re-
play memory and thus would effectively learn against the
opponents’ average behaviour as long as its replay mem-
ory is big enough to keep track of it. This is quite sim-
ilar to a ﬁctitious play. However, because DQN agents
use their -greedy strategies in self-play their experience
is highly correlated over time and focussed on only a sub-
set of states. We believe this is the main reason for NFSP’s
superior performance in our experiments. NFSP agents use
an ever more slowly changing average policy in self-play.
Thus, their experience varies more slowly, resulting in a
more stable data distribution contained in their memories.
This might help their training of neural networks and adap-
tation to each other. Other common reinforcement learning
methods have been shown to exhibit similarly stagnating
performance in poker games (Ponsen et al., 2011; Heinrich
& Silver, 2015).

4.4. Limit Texas Hold’em

We applied NFSP to LHE, a game that is popular with
humans. Since in 2008 a computer program beat expert
human LHE players for the ﬁrst time in a public compe-
tition, modern computer agents are widely considered to
have achieved super-human performance (Newall, 2013).
The game was essentially solved by Bowling et al. (2015).
We evaluated our agents against SmooCT, a Smooth UCT
(Heinrich & Silver, 2015) agent which achieved 3 sil-
ver medals in the Annual Computer Poker Competition
(ACPC) in 2014. Learning performance was measured in
milli-big-blinds won per hand, mbb/h, i.e. one thousandth
of a big blind that players post at the beginning of a hand.
We manually calibrated NFSP by trying 9 conﬁgurations.
We achieved the best performance with the following pa-
rameters. The neural networks were fully connected with
four hidden layers of 1024, 512, 1024 and 512 neurons with
rectiﬁed linear activations. The memory sizes were set to
600k and 30m for MRL and MSL respectively. MRL
functioned as a circular buffer containing a recent win-
dow of experience. MSL was updated with exponentially-
averaged reservoir sampling (Osborne et al., 2014), replac-
ing entries in MSL with minimum probability 0.25. We
used vanilla SGD without momentum for both reinforce-

 0.01 0.1 1 10 1000 10000 100000 1e+06ExploitabilityIterationsNFSPDQN, average strategyDQN, greedy strategyDeep Reinforcement Learning from Self-Play in Imperfect-Information Games

Match-up
escabeche
SmooCT
Hyperborean

NFSP
-52.1 ± 8.5
-17.4 ± 9.0
-13.6 ± 9.2

Table 1. Performance of NFSP’s greedy-average strategy against
the top 3 agents of the ACPC 2014.

plied to games have relied on human expert knowledge.
Deep Blue used a human-engineered evaluation function
for chess (Campbell et al., 2002). In computer Go, Maddi-
son et al. (2015) and Clark & Storkey (2015) trained deep
neural networks from data of expert human play. In com-
puter poker, current game-theoretic approaches use heuris-
tics of card strength to abstract the game to a tractable size
(Zinkevich et al., 2007; Gilpin et al., 2007; Johanson et al.,
2013). Waugh et al. (2015) recently combined one of these
methods with function approximation. However, their full-
width algorithm has to implicitly reason about all infor-
mation states at each iteration, which is prohibitively ex-
pensive in large domains.
In contrast, NFSP focuses on
the sample-based reinforcement learning setting where the
game’s states need not be exhaustively enumerated and the
learner may not even have a model of the game’s dynamics.
Many successful applications in games have relied on local
search (Campbell et al., 2002; Browne et al., 2012). Lo-
cal search algorithms efﬁciently plan decisions in a game
at runtime, e.g. via Monte Carlo simulation or limited-
depth backward induction. However, common simulation-
based local search algorithms have been shown to diverge
when applied to imperfect-information poker games (Pon-
sen et al., 2011; Heinrich & Silver, 2015). Furthermore,
even game-theoretic methods do not generally achieve un-
exploitable behaviour when planning locally in imperfect-
information games (Burch et al., 2014; Ganzfried & Sand-
holm, 2015; Lis´y et al., 2015). Another problem of local
search is the potentially prohibitive cost at runtime if no
prior knowledge is injected to guide the search. This poses
the question of how to obtain this prior knowledge. Sil-
ver et al. (2016) trained convolutional neural networks on
human expert data and then used a self-play reinforcement
learning procedure to optimize these networks further. By
using these neural networks to guide a high-performance
local search, they beat a Go grandmaster 5 to 0.
In this
work, we evaluate our agents without any local search at
runtime. If local search methods for imperfect-information
games were developed, strategies trained by NFSP could
be a promising choice for guiding the search.
Nash equilibria are the only strategy proﬁles that rational
agents can hope to converge on in self-play (Bowling &
Veloso, 2001). TD-Gammon (Tesauro, 1995) is a world-
class backgammon agent, whose main component is a neu-

Figure 6. Performance of playing against SmooCT. The estimated
standard error of each evaluation is less than 10 mbb/h.

ment and supervised learning, with learning rates set to 0.1
and 0.01 respectively. Each agent performed 2 stochastic
gradient updates of mini-batch size 256 per network for ev-
ery 256 steps in the game. The target network of the DQN
algorithm was reﬁtted every 1000 updates. NFSP’s antici-
patory parameter was set to η = 0.1. The -greedy policies’
exploration started at 0.08 and decayed to 0, more slowly
than in Leduc Hold’em. In addition to NFSP’s main, aver-
age strategy proﬁle we also evaluated the best response and
greedy-average strategies, which deterministically choose
actions that maximize the predicted action values or prob-
abilities respectively.
To provide some intuition for win rates in heads-up LHE,
a player that always folds will lose 750 mbb/h, and expert
human players typically achieve expected win rates of 40-
60 mbb/h at online high-stakes games. Similarly, the top
half of computer agents in the ACPC 2014 achieved up to
50 mbb/h between themselves. While training, we periodi-
cally evaluated NFSP’s performance against SmooCT from
symmetric play for 25000 hands each. Figure 6 presents
the learning performance of NFSP. NFSP’s average and
greedy-average strategy proﬁles exhibit a stable and rela-
tively monotonic performance improvement, and achieve
win rates of around -50 and -20 mbb/h respectively. The
best response strategy proﬁle exhibited more noisy per-
formance, mostly ranging between -50 and 0 mbb/h. We
also evaluated the ﬁnal greedy-average strategy against the
other top 3 competitors of the ACPC 2014. Table 1 presents
the results.

5. Related work
Reliance on human expert knowledge can be expensive,
prone to human biases and limiting if such knowledge
is suboptimal. Yet many methods that have been ap-

-800-700-600-500-400-300-200-100 0 100 0 5e+06 1e+07 1.5e+07 2e+07 2.5e+07 3e+07 3.5e+07mbb/hIterationsSmooCTNFSP, best response strategyNFSP, greedy-average strategyNFSP, average strategyDeep Reinforcement Learning from Self-Play in Imperfect-Information Games

ral network trained from self-play reinforcement learning.
While its algorithm, based on temporal-difference learn-
ing, is sound in two-player zero-sum perfect-information
games, it does not generally converge in games with im-
perfect information. DQN (Mnih et al., 2015) combines
temporal-difference learning with experience replay and
deep neural network function approximation. It achieved
human-level performance in a majority of Atari games,
learning from raw sensory inputs. However, these Atari
games were set up as single-agent domains with potential
opponents ﬁxed and controlled by the Atari emulator. Our
experiments showed that DQN agents were unable to ap-
proach a Nash equilibrium in Leduc Hold’em, where play-
ers were allowed to adapt dynamically. Yakovenko et al.
(2016) trained deep neural networks in self-play in com-
puter poker, including two poker games that are popular
with humans. Their networks performed strongly against
heuristic-based and simple computer programs. Expert
human players were able to outperform their agent, al-
beit over a statistically insigniﬁcant sample size.
It re-
mains to be seen whether their approach converges in prac-
tice or theory.
In contrast, we have empirically shown
NFSP’s convergence to approximate Nash equilibria in
Leduc Hold’em. Furthermore, the approach is principled
and builds on the theory of ﬁctitious play in extensive-form
games.

6. Conclusion
We have introduced NFSP, the ﬁrst end-to-end deep rein-
forcement learning approach to learning approximate Nash
equilibria of imperfect-information games from self-play.
NFSP addresses three problems. Firstly, NFSP agents learn
without prior knowledge. Secondly, they do not rely on
local search at runtime. Thirdly, they converge to approx-
imate Nash equilibria in self-play. Our empirical results
provide the following insights. The performance of ﬁc-
titious play degrades gracefully with various approxima-
tion errors. NFSP converges reliably to approximate Nash
equilibria in a small poker game, whereas DQN’s greedy
and average strategies do not. NFSP learned a competitive
strategy in a real-world scale imperfect-information game
from scratch without using explicit prior knowledge.
In this work, we focussed on imperfect-information two-
player zero-sum games. Fictitious play, however, is also
guaranteed to converge to Nash equilibria in cooperative,
potential games. It is therefore conceivable that NFSP can
be successfully applied to these games as well. Further-
more, recent developments in continuous-action reinforce-
ment learning (Lillicrap et al., 2015) could enable NFSP
to be applied to continuous-action games, which current
game-theoretic methods cannot deal with directly.

Acknowledgements
We thank Peter Dayan, Marc Lanctot and Marc Bellemare
for helpful discussions and feedback. This research was
supported by the UK Centre for Doctoral Training in Fi-
nancial Computing and by the NVIDIA Corporation.

References
Ariely, Dan and Jones, Simon. Predictably irrational.

HarperCollins New York, 2008.

Bazzan, Ana LC. Opportunities for multiagent systems and
multiagent reinforcement learning in trafﬁc control. Au-
tonomous Agents and Multi-Agent Systems, 18(3):342–
375, 2009.

Bowling, Michael and Veloso, Manuela. Rational and con-
vergent learning in stochastic games. In Proceedings of
the 17th International Joint Conference on Artiﬁcal In-
telligence, volume 17, pp. 1021–1026, 2001.

Bowling, Michael, Burch, Neil, Johanson, Michael, and
Tammelin, Oskari. Heads-up limit holdem poker is
solved. Science, 347(6218):145–149, 2015.

Brown, George W. Iterative solution of games by ﬁctitious
play. Activity analysis of production and allocation, 13
(1):374–376, 1951.

Browne, Cameron B, Powley, Edward, Whitehouse,
Daniel, Lucas, Simon M, Cowling, Peter I, Rohlfshagen,
Philipp, Tavener, Stephen, Perez, Diego, Samothrakis,
Spyridon, and Colton, Simon. A survey of Monte Carlo
IEEE Transactions on Computa-
tree search methods.
tional Intelligence and AI in Games, 4(1):1–43, 2012.

Burch, Neil, Johanson, Michael, and Bowling, Michael.
Solving imperfect information games using decomposi-
tion. In 28th AAAI Conference on Artiﬁcial Intelligence,
2014.

Campbell, Murray, Hoane, A Joseph, and Hsu, Feng-
hsiung. Deep blue. Artiﬁcial intelligence, 134(1):57–83,
2002.

Clark, Christopher and Storkey, Amos. Training deep con-
volutional neural networks to play go. In Proceedings of
the 32nd International Conference on Machine Learn-
ing, pp. 1766–1774, 2015.

Durkota, Karel, Lis`y, Viliam, Boˇsansk`y, Branislav, and
Kiekintveld, Christopher. Optimal network security
hardening using attack graph games. In Proceedings of
the 24th International Joint Conference on Artiﬁcal In-
telligence, 2015.

Deep Reinforcement Learning from Self-Play in Imperfect-Information Games

Ernst, Damien, Geurts, Pierre, and Wehenkel, Louis. Tree-
based batch mode reinforcement learning. In Journal of
Machine Learning Research, pp. 503–556, 2005.

search in imperfect information games. In Proceedings
of the 14th International Conference on Autonomous
Agents and Multi-Agent Systems, 2015.

Fudenberg, Drew. The theory of learning in games, vol-

ume 2. MIT press, 1998.

Ganzfried, Sam and Sandholm, Tuomas. Endgame solving
In Proceedings
in large imperfect-information games.
of the 14th International Conference on Autonomous
Agents and Multi-Agent Systems, 2015.

Gelly, Sylvain, Kocsis, Levente, Schoenauer, Marc, Sebag,
Mich`ele, Silver, David, Szepesv´ari, Csaba, and Teytaud,
Olivier. The grand challenge of computer go: Monte
Carlo tree search and extensions. Communications of
the ACM, 55(3):106–113, 2012.

Gilpin, Andrew, Hoda, Samid, Pena, Javier, and Sandholm,
Tuomas. Gradient-based algorithms for ﬁnding Nash
equilibria in extensive form games. In Internet and Net-
work Economics, pp. 57–69. Springer, 2007.

Heinrich, Johannes and Silver, David. Smooth UCT search
in computer poker. In Proceedings of the 24th Interna-
tional Joint Conference on Artiﬁcal Intelligence, 2015.

Heinrich, Johannes, Lanctot, Marc, and Silver, David. Fic-
In Proceed-
titious self-play in extensive-form games.
ings of the 32nd International Conference on Machine
Learning, 2015.

Johanson, Michael, Burch, Neil, Valenzano, Richard, and
Bowling, Michael. Evaluating state-space abstractions in
extensive-form games. In Proceedings of the 12th Inter-
national Conference on Autonomous Agents and Multi-
Agent Systems, pp. 271–278, 2013.

Lambert III, Theodore J, Epelman, Marina A, and Smith,
Robert L. A ﬁctitious play approach to large-scale opti-
mization. Operations Research, 53(3):477–489, 2005.

Leslie, David S and Collins, Edmund J. Generalised weak-
ened ﬁctitious play. Games and Economic Behavior, 56
(2):285–298, 2006.

Lillicrap, Timothy P, Hunt, Jonathan J, Pritzel, Alexander,
Heess, Nicolas, Erez, Tom, Tassa, Yuval, Silver, David,
and Wierstra, Daan. Continuous control with deep re-
inforcement learning. arXiv preprint arXiv:1509.02971,
2015.

Lin, Long-Ji. Self-improving reactive agents based on re-
inforcement learning, planning and teaching. Machine
learning, 8(3-4):293–321, 1992.

Lis´y, Viliam, Lanctot, Marc, and Bowling, Michael. On-
line monte carlo counterfactual regret minimization for

Maddison, Chris J, Huang, Aja, Sutskever, Ilya, and Sil-
ver, David. Move evaluation in go using deep convolu-
tional neural networks. The International Conference on
Learning Representations, 2015.

Mnih, Volodymyr, Kavukcuoglu, Koray, Silver, David,
Rusu, Andrei A, Veness, Joel, Bellemare, Marc G,
Graves, Alex, Riedmiller, Martin, Fidjeland, Andreas K,
Ostrovski, Georg, et al. Human-level control through
deep reinforcement learning. Nature, 518(7540):529–
533, 2015.

Monderer, Dov and Shapley, Lloyd S. Fictitious play prop-
erty for games with identical interests. Journal of eco-
nomic theory, 68(1):258–265, 1996.

Myerson, Roger B. Game Theory: Analysis of Conﬂict.

Harvard University Press, 1991.

Nevmyvaka, Yuriy, Feng, Yi, and Kearns, Michael. Re-
inforcement learning for optimized trade execution. In
Proceedings of the 23rd International Conference on
Machine Learning, pp. 673–680. ACM, 2006.

Newall, P. Further Limit Hold ’em: Exploring the Model

Poker Game. Two Plus Two Publishing, LLC, 2013.

Osborne, Miles, Lall, Ashwin, and Van Durme, Benjamin.
Exponential reservoir sampling for streaming language
models. In Proceedings of the 52nd Annual Meeting of
the Association for Computational Linguistics, pp. 687–
692, 2014.

Ponsen, Marc, de Jong, Steven, and Lanctot, Marc. Com-
puting approximate Nash equilibria and robust best-
responses using sampling. Journal of Artiﬁcial Intelli-
gence Research, 42(1):575–605, 2011.

Riedmiller, Martin. Neural ﬁtted q iteration–ﬁrst experi-
ences with a data efﬁcient neural reinforcement learning
method. In Machine Learning: ECML 2005, pp. 317–
328. Springer, 2005.

Riedmiller, Martin, Gabel, Thomas, Hafner, Roland, and
Lange, Sascha. Reinforcement learning for robot soccer.
Autonomous Robots, 27(1):55–73, 2009.

Robinson, Julia. An iterative method of solving a game.

Annals of Mathematics, pp. 296–301, 1951.

Samuel, Arthur L. Some studies in machine learning us-
ing the game of checkers. IBM Journal of research and
development, 3(3):210–229, 1959.

Deep Reinforcement Learning from Self-Play in Imperfect-Information Games

Yakovenko, Nikolai, Cao, Liangliang, Raffel, Colin, and
Fan, James. Poker-cnn: A pattern learning strategy for
making draws and bets in poker games using convolu-
tional networks. In 30th AAAI Conference on Artiﬁcial
Intelligence, 2016.

Zinkevich, Martin, Johanson, Michael, Bowling, Michael,
and Piccione, Carmelo. Regret minimization in games
with incomplete information. In Advances in Neural In-
formation Processing Systems, pp. 1729–1736, 2007.

Selten, Reinhard. Bounded rationality. Journal of Institu-

tional and Theoretical Economics, pp. 649–658, 1990.

Shamma, Jeff S and Arslan, G¨urdal. Dynamic ﬁctitious
play, dynamic gradient play, and distributed convergence
IEEE Transactions on Automatic
to Nash equilibria.
Control, 50(3):312–327, 2005.

Silver, David, Huang, Aja, Maddison, Chris J., Guez,
Arthur, Sifre, Laurent, van den Driessche, George,
Schrittwieser, Julian, Antonoglou, Ioannis, Panneer-
shelvam, Veda, Lanctot, Marc, Dieleman, Sander,
Grewe, Dominik, Nham, John, Kalchbrenner, Nal,
Sutskever, Ilya, Lillicrap, Timothy, Leach, Madeleine,
Kavukcuoglu, Koray, Graepel, Thore, and Hassabis,
Demis. Mastering the game of go with deep neural net-
works and tree search. Nature, 529:484–489, 2016.

Southey, Finnegan, Bowling, Michael, Larson, Bryce,
Piccione, Carmelo, Burch, Neil, Billings, Darse, and
Rayner, Chris. Bayes bluff: Opponent modelling in
poker. In In Proceedings of the 21st Annual Conference
on Uncertainty in Artiﬁcial Intelligence, pp. 550–558,
2005.

Sutton, Richard S and Barto, Andrew G. Reinforcement
learning: An introduction, volume 1. Cambridge Univ
Press, 1998.

Tambe, Milind. Security and game theory: algorithms, de-
ployed systems, lessons learned. Cambridge University
Press, 2011.

Tesauro, Gerald. Temporal difference learning and td-
gammon. Communications of the ACM, 38(3):58–68,
1995.

Urieli, Daniel and Stone, Peter. Tactex’13: a champion
In Proceedings of the
adaptive power trading agent.
13th International Conference on Autonomous Agents
and Multi-Agent Systems, pp. 1447–1448, 2014.

Vitter, Jeffrey S. Random sampling with a reservoir. ACM
Transactions on Mathematical Software (TOMS), 11(1):
37–57, 1985.

Von Stengel, Bernhard. Efﬁcient computation of behavior
strategies. Games and Economic Behavior, 14(2):220–
246, 1996.

Watkins, Christopher JCH and Dayan, Peter. Q-learning.

Machine learning, 8(3-4):279–292, 1992.

Waugh, Kevin, Morrill, Dustin, Bagnell, J. Andrew, and
Bowling, Michael. Solving games with functional regret
estimation. In 29th AAAI Conference on Artiﬁcial Intel-
ligence, 2015.

