6
1
0
2

 
r
a

 

M
5
1

 
 
]

V
C
.
s
c
[
 
 

1
v
5
9
5
4
0

.

3
0
6
1
:
v
i
X
r
a

Nested Invariance Pooling and RBM Hashing

for Image Instance Retrieval

Olivier Mor`ere1,2, Jie Lin1, Antoine Veillard2, Vijay Chandrasekhar1

Institute for Infocomm Research, A*STAR, Singapore1

Universit´e Pierre et Marie Curie, Paris, France2

Abstract. The goal of this work is the computation of very compact
binary hashes for image instance retrieval. Our approach has two novel
contributions. The ﬁrst one is Nested Invariance Pooling (NIP), a method
inspired from i-theory, a mathematical theory for computing group in-
variant transformations with feed-forward neural networks. NIP is able
to produce compact and well-performing descriptors with visual repre-
sentations extracted from convolutional neural networks. We speciﬁcally
incorporate scale, translation and rotation invariances but the scheme
can be extended to any arbitrary sets of transformations. We also show
that using moments of increasing order throughout nesting is important.
The NIP descriptors are then hashed to the target code size (32-256 bits)
with a Restricted Boltzmann Machine with a novel batch-level regular-
ization scheme speciﬁcally designed for the purpose of hashing (RBMH).
A thorough empirical evaluation with state-of-the-art shows that the
results obtained both with the NIP descriptors and the NIP+RBMH
hashes are consistently outstanding across a wide range of datasets.

Keywords: Image Instance Retrieval, CNN, Invariant Representation,
Hashing, Unsupervised Learning, Regularization

1 Introduction

Small binary image representations such as 64-bit hashes are a deﬁnite must for
fast image instance retrieval. Compact hashes provide more than enough capac-
ity for any practical purposes, including internet-scale problems. In addition, a
compact hash is directly addressable in RAM and enables fast matching using
ultra-fast Hamming distances.

State-of-the-art global image descriptors such as Fisher Vectors (FV) [1],
Vector of Locally Aggregated Descriptors (VLAD) [2] and Convolutional Neural
Network (CNN) features [3, 4] allow for robust image matching. However, the
dimensionality of such descriptors is typically very high: 4096 to 65536 ﬂoating
point numbers for FVs [1] and 4096 for CNNs [3]. Bringing such high-dimensional
representations down to compact hashes is a considerable challenge.

Authors contributed equally to this work.

2

Olivier Mor`ere1,2, Jie Lin1, Antoine Veillard2, Vijay Chandrasekhar1

Deep learning has achieved remarkable success in many visual tasks such
as image classiﬁcation [3, 5], image retrieval [4], face recognition [6, 7] and pose
estimation [8]. Here,we propose a deep learning framework for binary hashing
that generates extremely compact, yet discriminative descriptors. A series of
nested pooling layers introduced in the pipeline, provide higher invariance and
robustness to common transformations like rotation and scale. A RBM layer for
hashing is introduced at the end of the pipeline to map real-valued data to binary
hashes. The proposed deep learning pipeline generates hashes that consistently
and signiﬁcantly outperform other state-of-the-art methods at code size from
256 down to very small sizes like 32 bits, on several popular benchmarks.

2 Background and Related Work

Our image instance retrieval pipeline starts with the computation of high-dimensional
vectors referred to as global descriptors, followed by a hashing step to obtain com-
pact representations. Here, we review the state-of-the-art in global descriptors
and hashing methods.

Global Descriptors. State-of-the-art global descriptors for image instance
retrieval are based on either FV [1]/VLAD [2] or Convolutional Neural Networks
(CNN) [4]. Several variants of FV/VLAD [9–12] have been proposed, since it
was ﬁrst proposed for instance retrieval [1]. In recent work, CNN descriptors
have begun being applied to the computation of global descriptors for image
retrieval [4, 13–17].

Razavian et al. [13] evaluate the performance of CNN activations from fully
connected layer on a wide range of tasks including instance retrieval, and show
initial promising results. After that, Babenko et al. [4] show that a pre-trained
CNN can be ﬁne tuned with domain speciﬁc data (objects, scenes, etc.) to im-
prove retrieval performance on relevant datasets. In [14], the authors propose
extracting activations of fully connected layer from multiple regions sampled in
an image, followed by aggregating VLAD descriptors on these local CNN ac-
tivations. While this results in highly performant descriptors, the starting rep-
resentations are orders of magnitude larger than descriptors proposed in this
work. [15, 18] show that spatial max pooling of intermediate maps is an eﬀective
representation and higher performance can be achieved compared to using the
fully connected layers. Babenko et al [16] in their very recent work, show that
sum-pooling of intermediate feature maps performs better than max-pooling,
when the image representation is whitened. Note that the approach in [16] pro-
vide limited invariance to translation, but not to scale or rotation. Another very
recent work [17] proposes pooling across regional bounding boxes in the image,
similar to the popular R-CNN approach [19] used for object detection.

Unlike FVs based on interest point detectors like the Diﬀerence-of-Gaussian
(DoG) detector, CNN does not have a built-in mechanism to ensure resilience
to geometric transformations like scale and rotation. In particular, the perfor-
mance of CNN descriptors quickly degrade when the objects in the query and
the database image are rotated or scaled diﬀerently. To illustrate this, in Fig-

Nested Invariance Pooling and RBM Hashing for Image Instance Retrieval

3

Fig. 1: Comparison of CNN and FV descriptors retrieval performance with rotated
queries on Holidays and Graphics datasets (see experimental section for details on
datasets). FVDoG, FVDS, and FVDM are Fisher Vectors based on DoG interest points,
Dense interest points at Single scale, and Dense interest points at Multiple scales
respectively: all use the SIFT descriptor. FVDoG is robust to rotation, while CNN,
FVDS and FVDM suﬀer a sharp drop in performance as query image is rotated.

ure 1, we show retrieval results when query images are rotated with respect to
database images for descriptors: (a) Fisher Vectors based on Diﬀerence of Gaus-
sian interest points and SIFT descriptors (FVDoG), (b) Fisher Vectors based on
dense interest points and SIFT descriptors, at just one scale (FVDS), (c) Fisher
Vectors based on dense interest points and SIFT descriptors, at multiple scales
in the image (FVDM), and (d) CNN descriptors based on the ﬁrst fully con-
nected layer of OxfordNet [5]. Schemes apart from FVDoG suﬀer a sharp drop
in performance as geometric transforms are applied. In this work, we focus on
how to systematically incorporate groups of invariance into the CNN pipeline.

Hashing. In this work, we are focused on image instance retrieval with
compact descriptors produced by unsupervised hashing on global descriptors.
Semantic image retrieval with supervised hashing is outside the scope of this
work. Examples of popular unsupervised hashing methods include Locality Sen-
sitive Hashing (LSH) [20], Iterative Quantization (ITQ) [21], Spectral Hash-
ing (SH) [22] and Restricted Boltzmann Machines (RBM) [23, 24]. Gong et al.
propose the popular ITQ [21]. ITQ ﬁrst performs Principal Component Anal-
ysis (PCA) to reduce dimensionality, then applies rotations to distribute vari-
ance across dimensions, and ﬁnally binarizes each dimension according to its
sign. Besides hashing, quantization based methods such as Product Quantiza-
tion (PQ) [2, 25, 26] divide the raw descriptor into smaller blocks and vector
quantization is performed on each block. While this results in highly compact
descriptors composed of sub-quantizer indices, the resulting representation is not
binary and cannot be compared with Hamming distance.

−200−150−100−5005010015020000.10.20.30.40.50.60.70.80.9Query Rotation AngleMean Average PrecisionGraphicsOxfordNetFVDoGFVDSFVDM4

Olivier Mor`ere1,2, Jie Lin1, Antoine Veillard2, Vijay Chandrasekhar1

Fig. 2: Our proposed pipeline for image instance retrieval applies Nested Invariance
Pooling (NIP) to produce robust and compact descriptors from CNNs followed by an
RBM specially regularized for Hashing (RBMH).

3 Contributions

The goal of this work is the computation of very compact binary hashes for image
instance retrieval. To that end, we propose a multi-stage pipeline as shown on
Figure 2 with the following contributions:

– First, we propose Nested Invariance Pooling (NIP), a method to produce
compact global image descriptors from visual representations extracted from
CNNs. Our method draws its inspiration from the i-theory [27–29], a math-
ematical theory for computing group invariant transformations with feed-
forward neural networks. We speciﬁcally incorporate scale, translation and
rotation invariance but the scheme can be extended to any arbitrary sets
of transformations. We also show that using moments of increasing order
throughout nesting is important. Resulting NIP descriptors are invariant to
various types of image transformations and we show that the process sig-
niﬁcantly improves retrieval results while keeping dimensionality low (512
dimensions).

– Then, the NIP descriptors are hashed to the target code size (32-256 bits)
with a Restricted Boltzmann Machine (RBM). We propose a novel batch-
level regularization scheme speciﬁcally designed for the purpose of hashing,
a scheme we refer to as RBMH from hereon.

– A thorough empirical evaluation with state-of-the-art shows that the results
obtained both with the NIP descriptors and the NIP+RBMH hashes are
consistently outstanding across a wide range of datasets. To the best of our
knowledge, the results reported at 128 bits hashes are the highest reported
results in state-of-the-art literature.

GroupTransform.0101011010100001111110000.50.50.50.50.50.50.50.50.50.5InvariantDescriptorCompactHash32-­‐256	  bits512	  dim.GT;n=2GR;n!1GS;n=1Batch RegularizerInput ImageConvolutional Neural NetRBMH1. CNN feature extraction2. Nested Invariance Pooling3. RBM for HashingNested Invariance Pooling and RBM Hashing for Image Instance Retrieval

5

Fig. 3: (a) A single convolution-pooling operation from a CNN schematized for a single
input layer and single output unit. The parallel with i-theory shows that the universal
building block of CNNs is compatible with the incorporation of invariance to local
translations of the input according to the theory. (b) A speciﬁc succession of convolution
and pooling operations learnt by the CNN (depicted in red) computes the pool5 feature
fi for each feature map i from the RGB image data. A number of transformations g
can be applied to the input x in order to vary the response fi(g.x).

4 Method

4.1 Nested Invariance Pooling
Let an image x ∈ E and a group G of transformations acting over E with
group action G × E → E denoted with a dot (.). The orbit of x by G is the
subset of E deﬁned as Ox = {g.x ∈ E|g ∈ G}. The orbit corresponds to the set
of transformations of x under groups such as rotations, translations and scale
changes. It can be easily shown that Ox is globally invariant to the action of
any element of G and thus any descriptor computed directly from Ox would be
globally invariant to G.

The i-theory builds invariant representations for a given object x ∈ E in
relation with a predeﬁned template t ∈ E from the distribution of the dot
products Dx,t = {< g.x, t >∈ R|g ∈ G} = {< x, g.t >∈ R|g ∈ G} over the orbit.
The following representation (for any n ∈ N∗) is proven to have proper invariance

and selectivity properties provided that the group is compact or locally compact:

(cid:18)(cid:90)
G | < g.x, t > |ndg

(cid:19) 1

n

1(cid:82)

G dg

µG,t,n(x) =

(1)

Note that the sequence (µG,t,n(x))n∈N∗ is analogous to a histogram. In practice,
the theory extends well (with approximate invariance) to non-locally compact
groups and even to continuous non-group transformations (e.g. out-of-plane ro-
tations, elastic deformations) provided that proper class-speciﬁc templates can
be chosen [27]. Recent work on face veriﬁcation [30] and music classiﬁcation [31]
apply the theory to non-compact groups with good results.

6

Olivier Mor`ere1,2, Jie Lin1, Antoine Veillard2, Vijay Chandrasekhar1

Popular CNN architectures for classiﬁcation such as AlexNet [3] and Ox-
fordNet [5] share a common building block: a succession of convolution-pooling
operations designed to model increasingly high-level visual representations of
the data. As shown in Figure 3 (a), the succession of convolution and pooling
operations in a typical CNN is in fact a way to incorporate local translation
invariance strictly compliant with the framework proposed by the i-theory. The
network architecture provides the robustness as predicted by the i-theory, while
parameter tuning via back propagation ensures a proper choice of templates.

We build our NIP descriptors starting from the already locally robust pool5
feature maps of OxfordNet. Global invariance to several transformation groups
are then sequentially incorporated following the i-theory framework. The speciﬁc
transformation groups considered in this study are translations GT , rotations GR
and scale changes GS. For every feature map i of the pool5 layer (0 ≤ i < 512),
we denote fi(x) the corresponding unit’s output. As shown on Figure 3 (b),
transformations g are applied on the input image x varying the output of the
pool5 feature fi(g.x). Note that the transformation fi is non-linear due to multi-
ple convolution-pooling operations thus not strictly a mathematical dot product
but can still be viewed as an inner product. Accordingly, the pooling scheme
used by NIP with G ∈ {GT , GR, GS} is:

(cid:18)(cid:90)

1(cid:82)

(cid:19) 1

n

m−1(cid:88)

j=0

=

1
m

 1

n

XG,i,n(x) =

G dg

G

fi(g.x)ndg

fi(gj.x)n

(2)

when Ox is discretized into m samples. The corresponding global image descrip-
tors are obtained after each pooling step by concatenating the moments for the
individual features:

XG,n(x) = (XG,i,n(x))0≤i<512

(3)

As shown in Equation 2, the pooling operation has an order parameter n
deﬁning the “hardness” of the pooling. n = 1 is average pooling while n → +∞
on the other extreme is max-pooling. n = 2 is analogous to standard deviation.
Subsequently, we refer to the moments for n = 1, 2, +∞ as AG, SG and MG.
Work on i-theory [31] has shown that it is possible to chain multiple types of
group invariances one after the other [31]. We apply this principle on our NIP
descriptors by making them invariant to several transformations. For instance,
following scale invariance with average (n = 1) by translation invariance with
hard max-pooling (n → +∞) is done by:

(cid:33)

(cid:32)

(cid:82)

max
gt∈GT

1
gs∈GS

(cid:90)

(cid:33)

(cid:32)

1
ms

ms−1(cid:88)

fi(gt,jgs,it.x)

i=0

(4)

dgs

gs∈GS

fi(gtgs.x)dgs

= max

j∈[0,mt−1]

Operations are sometimes commutable (e.g. AG and AG(cid:48)) and sometimes not
(e.g. AG and MG(cid:48)) depending on the speciﬁc combination of moments so the

Nested Invariance Pooling and RBM Hashing for Image Instance Retrieval

7

sequence of transformations does matter for NIP. The hardness parameter n
must also be chosen carefully. Empirically, we found pooling progressively with
increasing moments (e.g. AG, then SG, then MG) to work well as presented in
the experiments section.

pool5

AGS

AGS -AGT

AGS -AGT -AGR

Fig. 4: Distances for 3 matching pairs from the UKB dataset. For each pair, 4 pairwise
distances (L2-normalized) are computed corresponding to the following descriptors:
pool5, AGS , AGS -AGT and AGS -AGT -AGR . Adding scale invariance makes the most
diﬀerence on (b), translation invariance on (c), and rotation on (a) which is consistent
with the scenarios suggested by respective image pairs.

Figure 4 provides an insight on how adding diﬀerent types of invariance with
our NIP scheme will aﬀect the matching distance on diﬀerent image pairs of
matching objects. With the incorporation of each new transformation group, we
notice that the relative reduction in matching distance is the most signiﬁcant
with the image pair which is the most aﬀected by the transformation group.

4.2 Restricted Boltzmann Machine for Hashing

The NIP descriptors are subsequently hashed to the target dimentionality with
an RBM layer. The motivation is to obtain mutually independent dimensions
while distributing variance evenly across them in a way similar to ITQ [21].
The main originality of our RBM is its batch-level regularization scheme which
is speciﬁcally designed for hashing. We subsequently refer to this variant as
RBMH.

An RBM is a bipartite Markov random ﬁeld with the input layer x ∈ RI
connected to a latent layer z ∈ RJ via a set of undirected weights W ∈ RIJ .
The input and latent layers are also parameterised by their corresponding biases
c and b, respectively. Since the units within a layer are conditionally independent

(a)(b)(c)8

Olivier Mor`ere1,2, Jie Lin1, Antoine Veillard2, Vijay Chandrasekhar1

pairwise, the activation probabilities of one layer can be sampled by ﬁxing the
states of the other layer, and using distributions given by logistic functions (a
sigmoid activation function is chosen since binary hashes are desired):

P (zj|x) = 1/(1 + exp(−wjx − bj)),
(cid:62)
i z − ci)).
P (xi|z) = 1/(1 + exp(−w

(5)

(6)

As a result, alternating Gibbs sampling can be performed between the two layers.
The sampled states are used to update the parameters {W, b, c} through batch
gradient descent using the contrastive divergence algorithm [32] to approximate
the maximum likelihood of the input distribution. The hashed descriptors are
obtained by binarizing the latent units at 0.5.

Proper regularization is key during the training of RBM. The popular RBM
proposed by Nair and Hinton [24] encourages latent representations to be sparse.
This improves separability which is desirable for classiﬁcation task. For hashing,
it is desirable to encourage the representation to make eﬃcient use of the limited
latent subspace. RBMH achieves this goal by controlling sparsity in a way to
maximize the entropy not only within every hash but also between the same
bit of diﬀerent hashes. This eﬀectively encourages (a) half the bits to be active
for a given hash, and (b) each hash bit to be equiprobable across images. We
introduce a regularization term at the batch as in [33]. For a batch B, we deﬁne
a regularization term:

(cid:88)

(cid:88)

xα∈B

jα

h(B) =

tjα log zjα + (1 − tjα) log(1 − zjα),

(7)

where tα are the target activations for each data sample α. We choose the tl
jα such
that each {tl
jα}α for ﬁxed j is distributed according
to the uniform distribution U (0, 1) eﬀectively maximizing entropy. The overall
objective function becomes:

jα}j for ﬁxed α and each {tl

(cid:19)

{W,b,c} −
arg min

log

P (xα, zα) + λh(B)

,

(8)

(cid:88)

α

(cid:18) (cid:88)

zα∈B

with λ the regularization constant.

Figure 5 shows the activation probabilities of the hash bits between RBMH
and the RBM proposed by Nair and Hinton [24]. The comparison is for 32-
bits hashes. In Figure 5 (a), the mean probability of activation is nearly 0.5 in
both cases. Nevertheless, we can see that probabilities are much more evenly
distributed across bits with BRMH. In Figure 5 (b), retrieval results on UKB
and Holidays show that the RBMH is able to outperform the standard RBM
specially at lower code sizes (32 or 64 bits).

5 Experiments

5.1 Evaluation Framework

We evaluate the performances on 4 popular datasets for image instance retrieval:
(1) Holidays. The INRIA Holidays dataset [34] consists of outdoor holiday pic-

Nested Invariance Pooling and RBM Hashing for Image Instance Retrieval

9

(a)

(b)

Fig. 5: (a) Activation probabilities of hash bits between RBMH and the RBM proposed
by Nair&Hinton [24]. We compute the statistics with 32 bits binary hashes on Holidays
dataset (1491 images in total). (b) Comparison of our RBMH with RBM [24] in terms
of mAP on Holidays and UKB. Both schemes are built upon the best NIP descriptors.
RBMH outperforms RBM at very low code sizes.

tures. There are 500 queries and 991 database images. (2) UKB. The University
of Kentucky Benchmark dataset [35] consists of 2550 groups of common objects,
4 images per object. All 10200 images are used as queries. (3) Oxford5K. The
Oxford buildings dataset [36] consists of 5063 images representing landmark
buildings in Oxford. The query set contains 11 diﬀerent landmarks, each repre-
sented by 5 queries. (4) Graphics. The Graphics dataset is part of the Stanford
Mobile Visual Search dataset [37], which was used in the MPEG standard ti-
tled Compact Descriptors for Visual Search (CDVS) [38]. This dataset contains
objects like CDs, DVDs, books, prints, business cards. There are 500 unique
objects, 1500 queries, and 1000 database images. Note that Graphics is diﬀerent
from the other datasets as it contains images of rigid objects captured under
widely varying scale and rotation changes.

For large-scale experiments, we present results on the 4 datasets combined
with the 1 million MIR-FLICKR distractor images [39]. Most schemes, including
our approach, require an unsupervised training step. We train on a randomly
sampled set of 150K images from the 1.2 million ImageNet dataset [40]. No class
labels are used in this work.

For the starting global descriptor representation, we use the pool5 layer from
the 16-layer OxfordNet [5], which is widely adopted in instance retrieval liter-
ature [15–17]. The input image size for OxfordNet is ﬁxed at 224 × 224. The
dimensionality of the pool5 descriptor is 25088, organized as 512 feature maps
of size 7 × 7.
For rotation invariance, rotated input images are padded with the mean
pixel value computed from the ImageNet dataset. The step size for rotations is
10 degrees yielding 36 rotated images per orbit. For scale changes, 10 diﬀerent
center crops are considered varying from 50% to 100% of the total image. For
translations, the entire feature map is used for every feature, resulting in an
orbit size of 7 × 7 = 49.

0816243200.250.50.751NIP+RBMActivation Prob.0816243200.250.50.751NIP+RBMHNeuronsActivation Prob.326412825630405060708090100BitsmAP  NIP+RBM, HolidaysNIP+RBMH, HolidaysNIP+RBM, UKBNIP+RBMH, UKB10

Olivier Mor`ere1,2, Jie Lin1, Antoine Veillard2, Vijay Chandrasekhar1

For retrieval with ﬂoating point descriptors, L2 normalization is ﬁrst applied
followed by L2 distance computation. For retrieval with binary descriptors, we
use hamming distance computation. We evaluate retrieval results with mean
Average Precision (mAP). To be consistent with the literature, 4× Recall @
R = 4 is provided for UKB.

Table 1: Retrieval results (mAP) for diﬀerent sequences of transformation groups and
moments. For UKB, 4× Recall @ R = 4 is shown between parentheses. GT , GR, GS
denote the groups of translations, rotations and scale changes respectively. Note that
averages commute with other averages so the sequence order of the composition does
not matter when only averages are involved. Best results are achieved by choosing
speciﬁc moments. AGS -SGT -MGR corresponds to the best average performer. fc6 and
pool5 are provided as a baseline.

Sequence

Dims

Dataset

Oxford5K Holidays

UKB

Graphics

pool5
fc6
AGT
AGR
AGS
AGT -AGR
AGT -AGS
AGR -AGS
AGT -AGR -AGS
AGS -SGT -MGR

25088
4096

512
25088
25088
512
512
25088
512

0.427
0.461

0.477
0.462
0.430
0.418
0.537
0.494
0.484

0.707
0.782

0.800
0.779
0.716
0.796
0.811
0.815
0.833

0.823(3.11)
0.910(3.50)

0.924(3.56)
0.954(3.72)
0.828(3.12)
0.955(3.73)
0.931(3.61)
0.959(3.75)
0.971(3.82)

0.315
0.312

0.322
0.500
0.394
0.417
0.430
0.552
0.509

512

0.592

0.838 0.975(3.84) 0.589

5.2 Results

Evaluation of NIP descriptors. As shown in Table 1, we ﬁrst study the eﬀects
of incorporating various transformation groups and using diﬀerent moments on
NIP descriptors. We present results for all possible combinations of transforma-
tion groups for average pooling (order does not matter as averages commute)
and for the single best performer which is AGS -SGT -MGR (order matters). First,
we point out the eﬀectiveness of the pool5 layer. Although it performs notably
worse than fc6 as-is, a simple average pooling over the space of translations AGT
makes it both better and 8 times more compact than fc6. Similar observations
have also been reported by [16, 18]. Second, on average, accuracy signiﬁcantly
increases with the number of transformation groups involved. Third, choosing
statistical moments diﬀerent than averages further improve the retrieval results.

Nested Invariance Pooling and RBM Hashing for Image Instance Retrieval

11

In Table 1, we observe that AGS -SGT -MGR performs signiﬁcantly better than
AGT -AGR -AGS . Notably, the best combination corresponds to an increase in
the orders of the moments: A being a ﬁrst-order moment, S second order and
M of inﬁnite order. A diﬀerent way of stating this is that a more invariant
representation requires higher and higher orders of pooling.

Table 2: Retrieval performance comparing NIP to other state-of-the-art methods. We
include results in recent papers with comparable dimensionality of descriptors reported
in those papers. L2 distance is used for all methods.

Method

T-embedding [9]
T-embedding [9]
FV+Proj [41]

FC+PCAWhitening [13]
FC+VLAD+PCA [14]
FC+Finetune+PCAWhitening [4]
Conv+MaxPooling [15]
FV+FC+PCAWhitening [42]
Conv+SPoC+PCAWhitening [16]
R-MAC+PCAWhitening [17]
R-MAC+PCAWhitening [17]

NIP (Ours)
NIP+PCAWhitening (Ours)

Dims

1024
512
512

500
512
512
256
512
256
512
256

512
256

Dataset

Oxford5K Holidays UKB

0.560
0.528

-

0.322

-

0.557
0.533

-

0.589
0.668
0.561

0.592
0.609

0.720
0.700
0.789

0.642
0.784
0.789
0.716
0.827
0.802

-
-

3.51
3.49
3.36

-
-

3.30

-

3.37
3.65

-
-

0.838 3.84
0.836 3.83

Overall, AGS -SGT -MGR improves results over starting representation pool5
by 39% (Oxford5K) to 87% (Graphics) depending on the dataset. Better im-
provements with Graphics can be explained with the presence of many rotations
in the dataset (smaller objects taken under diﬀerent angles) while Oxford5K con-
sisting mainly of upright buildings is less signiﬁcantly helped by incorporating
rotation invariance.

Comparing NIP with state-of-the-art including variants of VLAD/FV [9,
41], deep descriptors [4,15–17] and descriptors combining deep CNN and VLAD/
FV [14, 42]. As shown in Table 2, we observe that 512-D NIP descriptors largely
outperform most state-of-the-art methods with 512 or higher dimensions, on all
datasets. Following [15–17], we also perform PCA whitening to reduce the di-
mensionality of NIP to 256. One can see that the 256-D NIP descriptors yield
superior performance to [15–17] on all datasets.

12

Olivier Mor`ere1,2, Jie Lin1, Antoine Veillard2, Vijay Chandrasekhar1

First, we compare NIP to the most related papers [15–17] which propose 256-
D deep descriptors by aggregating convolutional features with various pooling
operations 1. [15, 16, 18] can be considered a special case of our work, with just
one layer of pooling, which only provided limited levels of translation invariance.
The very recently proposed Regional Maximum Activation of Convolutions (R-
MAC) [17] reports outstanding results on building dataset Oxford5K with very
small dimensionality (e.g. 0.668 mAP for 512-D R-MAC and 0.561 mAP for
256-D R-MAC). The authors propose a fast R-CNN type pooling [43], which is
eﬀective when the object of interest is in a small portion of the image. Such an
approach will be less eﬀective when the object of interest is aﬀected by groups
of distortions like rotation and perspective, and located at the centre of the
image. Here, we observe that nested pooling over many types of distortions with
progressively increasing moments is essential to achieving geometric invariance
and high retrieval performance with low dimensional descriptors. Besides, we
argue that the technique proposed in [17] can be incorporated with NIP to
further improve performance.

(a) Holidays

(b) UKB

(c) Oxford5K

(d) Graphics

Fig. 6: Comparison of RBMH with other hashing methods on 4 benchmark datasets. All
methods are built upon the best NIP descriptors. To examine the eﬀect of compression,
we also present retrieval results using uncompressed NIP descriptors.

1 Note that [15–17] extract deep descriptors from images with size larger than
576 × 576, while we use 224 × 224 in this work. As shown in [4], there is poten-
tial improvement if larger image size adopted in deep descriptors extraction.

326412825601020304050607080BitsmAP326412825600.511.522.533.54Bits4 x Recall @ R=432641282560102030405060BitsmAP32641282560102030405060BitsmAP  NIP+ITQNIP+BPBCNIP+PCAHashNIP+LSHNIP+SKLSHNIP+SHNIP+RBMNIP+RBMHNIPNested Invariance Pooling and RBM Hashing for Image Instance Retrieval

13

Next, we note that [15] reports better results on Holidays (0.881 mAP) and
Oxford5K (0.844 mAP), with very high-dimensional descriptors (from 10K to
100K). These very high dimensional descriptors are obtained by combining CNN
descriptors with spatial max pooling [18]. In contrast, our results are generated
using only 256 to 512 dimensional descriptors.

Table 3: Retrieval performance comparing NIP+RBMH to other state-of-the-art meth-
ods at small codesizes (from 32 to 512 bits). ADC denotes asymmetric distance com-
putation [2, 44].

Method

Binarized FV [1]
FV+SSH [44]
FV+SSH [44]
FV+SSH [44]
FV+PQ [2]
VLAD+PQ [25]
VLAD+CQ [25]
VLAD+SQ [26]

FC+Finetune+PCAWhitening [4]
Conv+MaxPooling [15]

Binarized NIP (Ours)
NIP+RBMH (Ours)
NIP+RBMH (Ours)
NIP+RBMH (Ours)

Dims

(size in bits)

520(520)
256(256)
128(128)
32(32)
128(128)
128(128)
128(128)
128(128)

16(512)
256(256)

Dist.

Cosine
ADC
ADC
ADC
ADC
L2
L2
L2

L2
Cosine

512(512) Hamming
256(256) Hamming
128(128) Hamming
32(32) Hamming

Dataset

Oxford5K Holidays UKB

-
-
-
-
-
-
-
-

0.418
0.436

0.477
0.445
0.359
0.274

0.460
0.544
0.499
0.334
0.506
0.586
0.644
0.639

0.609
0.578

2.79
3.08
2.91
2.18
3.10
2.88
3.19
3.06

2.41

-

0.781 3.70
0.739 3.59
0.705 3.38
0.458 2.26

Evaluation of NIP+RBMH binary hashes. NIP+RBMH binary hashes
are produced by feeding invariant NIP descriptors into the proposed RBM hash-
ing layer. Small scale retrieval results with NIP+RBMH are shown in Figure 6.
We compare NIP+RBMH to other popular unsupervised hashing methods at
code sizes from 32 to 256 bits, including ITQ [21], Bilinear Projection Bi-
nary Codes (BPBC) [45], PCAHash [21], LSH [20], SKLSH [46], SH [22] and
RBM [23, 24]. We used the software provided by the authors in [21, 23] to gen-
erate results for the baseline hashing methods. In addition, we also include the
results of 512-D NIP descriptors, as the baseline uncompressed scheme.

We observe that NIP+RBMH outperforms other methods at most code sizes
on all data sets. First, there is a signiﬁcant improvement at smaller code sizes
like 32 bits, due to the proposed batch-level regularization: 0.457 vs. 0.369 in
terms of mAP, compared to the second performing method RBM on Holidays at

14

Olivier Mor`ere1,2, Jie Lin1, Antoine Veillard2, Vijay Chandrasekhar1

32 bits. Second, the improvements of NIP+RBMH over other methods becomes
smaller as code size increases (except SKLSH). For code size larger than 256 bits,
the performances of all methods approach the upper bound, i.e., uncompressed
NIP descriptors. Finally, compared to uncompressed NIP descriptors, there is
a marginal drop for all methods on UKB at 256 bits, while performance gap is
larger for other datasets.

(a) Holidays+1M

(b) UKB+1M

(c) Oxford5K+1M

(d) Graphics+1M

Fig. 7: Comparison of RBMH with other hashing methods on large scale retrieval
experiments. All methods are based on the best NIP descriptors.

Comparing NIP+RBMH with state-of-the-art including methods com-
pressing VLAD/FV with direct binarization [1], hashing [44] and PQ [2, 26],
methods based on compact deep descriptors [4, 15].

As shown in Table 3, ﬁrst, a simple binarization strategy applied to our best
performing NIP descriptor is suﬃcient to obtain signiﬁcantly better accuracy
than [1, 4] at comparable code size (512 bits), e.g., 3.7 vs. 2.79 in [1] for 4× Re-
call @ R = 4 on UKB. Second, NIP+RBMH outperforms state-of-the-art by a
signiﬁcant margin at comparable code sizes (from 32 to 256 bits). NIP+RBMH
achieves the best performance on Holidays at small code size (128 bits), 0.705
vs. 0.644 mAP reported in [25] (to our knowledge, the state-of-the-art on this
dataset with 128-bit descriptors). Note that Hamming distance is used for our
binary descriptors, while other methods like PQ variants employ Euclidean dis-
tances (L2 or ADC), which typically result in higher accuracy than Hamming
distance, at the expense of higher computational cost.

326412825601020304050BitsmAP326412825600.511.522.533.5Bits4 x Recall @ R=43264128256051015202530BitsmAP3264128256051015202530BitsmAP  NIP+ITQNIP+BPBCNIP+PCAHashNIP+LSHNIP+SKLSHNIP+SHNIP+RBMNIP+RBMHNested Invariance Pooling and RBM Hashing for Image Instance Retrieval

15

Large scale experiments. In Figure 7, we present large scale retrieval
results, combining the 1 million MIR FLICKR distractor images with each data
set respectively. Trends consistent with small scale retrieval results in Figure 6
are observed.

6 Conclusions

In this work, we proposed a method to produce global image descriptors from
CNNs which are both compact and robust to typical geometric transformations.
The method provides a practical and mathematically proven way for computing
invariant object representations with feed-forward neural networks. To achieve
global geometric invariance, we introduce a series of nested pooling layers at
intermediate levels of the deep CNN network. We further introduce a RBM layer
with a novel batch-level regularization scheme for generating compact binary
descriptors. Through a thorough evaluation with state-of-the-art, we show that
the proposed method outperforms state-of-the-art by a signiﬁcant margin.

16

Olivier Mor`ere1,2, Jie Lin1, Antoine Veillard2, Vijay Chandrasekhar1

References

1. Perronnin, F., Liu, Y., Sanchez, J., Poirier, H.: Large-scale Image Retrieval
with Compressed Fisher Vectors. In: Computer Vision and Pattern Recognition
(CVPR). (2010)

2. J´egou, H., Perronnin, F., Douze, M., S´anchez, J., P´erez, P., Schmid, C.: Aggre-
gating local image descriptors into compact codes. IEEE Transactions on Pattern
Analysis and Machine Intelligence (PAMI) 34(9) (2012) 1704–1716

3. Krizhevsky, A., Sutskever, I., Hinton, G.E.:

Imagenet classiﬁcation with deep
convolutional neural networks. In: Neural Information Processing Systems (NIPS).
(2012)

4. Babenko, A., Slesarev, A., Chigorin, A., Lempitsky, V.: Neural Codes for Image

Retrieval. In: European Conference on Computer Vision (ECCV). (2014)

5. Simonyan, K., Zisserman, A.: Very Deep Convolutional Networks for Large-Scale
In: International Conference on Learning Representations

Image Recognition.
(ICLR). (2015)

6. Taigman, Y., Yang, M., Ranzato, M., Wolf, L.: DeepFace: Closing the Gap to
Human-Level Performance in Face Veriﬁcation. In: Computer Vision and Pattern
Recognition (CVPR). (2014)

7. Sun, Y., Wang, X., Tang, X.: Deep learning face representation from predicting

10,000 classes. In: Computer Vision and Pattern Recognition (CVPR). (2014)

8. Toshev, A., Szegedy, C.: Deeppose: Human pose estimation via deep neural net-

works. In: Computer Vision and Pattern Recognition (CVPR). (2014)

9. J´egou, H., Zisserman, A.: Triangulation embedding and democratic aggregation
for image search. In: Computer Vision and Pattern Recognition (CVPR). (2014)
10. Tolias, G., Avrithis, Y., Jegou, H.: To aggregate or not to aggregate: Selective
match kernels for image search. In: International Conference on Computer Vision
(ICCV). (2013)

11. Chen, D.M., Tsai, S.S., Chandrasekhar, V., Takacs, G., Vedantham, R.,
Grzeszczuk, R., Girod, B.: Residual Enhanced Visual Vector as a Compact Signa-
ture for Mobile Visual Search. In: Signal Processing. (2012)

12. Lin, J., Duan, L.Y., Huang, T., Gao, W.: Robust Fisher Codes for Large Scale
Image Retrieval. In: International Conference on Acoustics and Signal Processing
(ICASSP). (2013)

13. Razavian, A.S., Azizpour, H., Sullivan, J., Carlsson, S.: CNN Features Oﬀ-the-
Shelf: An Astounding Baseline for Recognition. In: Computer Vision and Pattern
Recognition Workshops. (2014)

14. Y. Gong, L. Wang, R.G., Lazebnik, S.: Multi-scale orderless pooling of deep
convolutional activation features. In: European Conference on Computer Vision
(ECCV). (2014)

15. Razavian, A.S., Sullivan, J., Maki, A., Carlsson, S.: A baseline for visual instance
retrieval with deep convolutional networks. In: International Conference on Learn-
ing Representations (ICLR). (2015)

16. Babenko, A., Lempitsky, V.: Aggregating local deep features for image retrieval.

In: International Conference on Computer Vision (ICCV). (2015)

17. Tolias, G., Sicre, R., J´egou, H.: Particular object retrieval with integral max-

pooling of CNN activations. In: arXiv:1511.05879. (2015)

18. Azizpour, H., Razavian, A.S., Sullivan, J., Maki, A., Carlsson, S.: From generic
to speciﬁc deep representations for visual recognition. In: Computer Vision and
Pattern Recognition Workshops. (2015)

Nested Invariance Pooling and RBM Hashing for Image Instance Retrieval

17

19. Girshick, R., Donahue, J., Darrell, T., Malik, J.: Rich feature hierarchies for accu-
rate object detection and semantic segmentation. In: Computer Vision and Pattern
Recognition (CVPR). (2014)

20. Datar, M., Immorlica, N., Indyk, P., Mirrokni, V.S.: Locality-Sensitive Hashing
Scheme based on p-stable Distributions. In: Annual Symposium on Computational
Geometry. (2004)

21. Gong, Y., Lazebnik, S., Gordo, A., Perronnin, F.: Iterative Quantization: A Pro-
crustean Approach to Learning Binary Codes for Large-scale Image Retrieval. In:
IEEE Transaction on Pattern Analysis and Machine Intelligence (PAMI). (2013)
In: Neural Information

22. Weiss, Y., Torralba, A., Fergus, R.: Spectral Hashing.

Processing Systems (NIPS). (2008)

23. Hinton, G.E., Salakhutdinov, R.R.: Reducing the dimensionality of data with

neural networks. Science 313 (2006) 504–507

24. Nair, V., Hinton, G.: 3D Object Recognition with Deep Belief Nets. In: Neural

Information Processing Systems (NIPS). (2009)

25. Zhang, T., Du, C., Wang, J.: Composite quantization for approximate nearest
neighbor search. In: International Conference on Machine Learning (ICML). (2014)
26. Zhang, T., Qi, G.J., Tang, J., Wang, J.: Sparse composite quantization. In: Com-

puter Vision and Pattern Recognition (CVPR). (2015)

27. Anselmi, F., Leibo, J.Z., Rosasco, L., Mutch, J., Tacchetti, A., Poggio, T.:
Unsupervised learning of invariant representations in hierarchical architectures.
arXiv:1311.4158 (2013)

28. Anselmi, F., Poggio, T.: Representation learning in sensory cortex: a theory.

CBMM memo n 26 (2010)

29. Anselmi, F., Leibo, J.Z., Rosasco, L., Mutch, J., Tacchetti, A., Poggio, T.: Magic
materials: a theory of deep hierarchical architectures for learning sensory represen-
tations. CBCL paper (2013)

30. Liao, Q.L., Leibo, J.Z., Poggio, T.: Learning invariant representations and appli-
cations to face veriﬁcation. In: Neural Information Processing Systems (NIPS).
(2013)

31. Zhang, C., Evangelopoulos, G., Voinea, S., Rosasco, L., Poggio, T.: A deep repre-
sentation for invariance and music classiﬁcation. In: International Conference on
Acoustics and Signal Processing (ICASSP). (2014)

32. Hinton, G.E.: Training products of experts by minimizing contrastive divergence.

Neural Computation 14(8) (2002) 1771–1800

33. Goh, H., Thome, N., Cord, M., Lim, J.H.: Unsupervised and supervised visual
codes with restricted Boltzmann machines. In: European Conference on Computer
Vision (ECCV). (2012)

34. J´egou, H., Douze, M., Schmid, C.: Hamming Embedding and Weak Geometric
Consistency for Large Scale Image Search. In: European Conference on Computer
Vision (ECCV). (2008)

35. Nist´er, D., Stew´enius, H.: Scalable Recognition with a Vocabulary Tree. In: Com-

puter Vision and Pattern Recognition (CVPR). (2006)

36. Philbin, J., Chum, O., Isard, M., Sivic, J., Zisserman, A.: Object Retrieval with
Large Vocabularies and Fast Spatial Matching. In: Computer Vision and Pattern
Recognition (CVPR). (2007)

37. Chandrasekhar, V., D.M.Chen, S.S.Tsai, N.M.Cheung, H.Chen, G.Takacs,
Y.Reznik, R.Vedantham, R.Grzeszczuk, J.Back, B.Girod: Stanford Mobile Visual
Search Data Set. In: ACM Multimedia Systems Conference (MMSys). (2011)

38. ISO/IEC-JTC1/SC29/WG11/N12202: Evaluation Framework for Compact De-

scriptors for Visual Search. (2011)

18

Olivier Mor`ere1,2, Jie Lin1, Antoine Veillard2, Vijay Chandrasekhar1

39. Mark J. Huiskes, B.T., Lew, M.S.: New Trends and Ideas in Visual Concept
Detection: The MIR Flickr Retrieval Evaluation Initiative. In: ACM International
Conference on Multimedia Information Retrieval. (2010)

40. Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A large-
scale hierarchical image database. In: Computer Vision and Pattern Recognition
(CVPR). (2009)

41. Gordoa, A., Rodriguez-Serrano, J., Perronnin, F., Valveny, E.:

Leveraging
category-level labels for instance-level image retrieval. In: Computer Vision and
Pattern Recognition (CVPR). (2012)

42. Perronnin, F., Larlus, D.: Fisher vectors meet neural networks: A hybrid clas-
In: Computer Vision and Pattern Recognition (CVPR).

siﬁcation architecture.
(2015)

43. Girshick, R.: Fast R-CNN.

In: International Conference on Computer Vision

(ICCV). (2015)

44. Gordo, A., Perronnin, F., Gong, Y., Lazebnik, S.: Asymmetric distances for binary
IEEE Transactions on Pattern Analysis and Machine Intelligence

embeddings.
(PAMI) 36(1) (2014) 33–47

45. Gong, Y., Kumar, S., Rowley, H., Lazebnik, S.: Learning Binary Codes for High-
Dimensional Data Using Bilinear Projections. In: Computer Vision and Pattern
Recognition (CVPR). (2013)

46. Raginsky, M., Lazebnik, S.: Locality-Sensitive Binary Codes from Shift-Invariant

Kernels. In: Neural Information Processing Systems (NIPS). (2009)

