Incorporating Copying Mechanism in Sequence-to-Sequence Learning

Jiatao Gu† Zhengdong Lu‡ Hang Li‡ Victor O.K. Li†

†Department of Electrical and Electronic Engineering, The University of Hong Kong

{jiataogu, vli}@eee.hku.hk
‡Huawei Noah’s Ark Lab, Hong Kong

{lu.zhengdong, hangli.hl}@huawei.com

6
1
0
2

 
r
a

 

M
2
2

 
 
]
L
C
.
s
c
[
 
 

2
v
3
9
3
6
0

.

3
0
6
1
:
v
i
X
r
a

Abstract

We address an important problem in
sequence-to-sequence (Seq2Seq) learning
referred to as copying,
in which cer-
tain segments in the input sequence are
selectively replicated in the output se-
quence. A similar phenomenon is ob-
servable in human language communica-
tion. For example, humans tend to re-
peat entity names or even long phrases
in conversation. The challenge with re-
gard to copying in Seq2Seq is that new
machinery is needed to decide when to
perform the operation.
In this paper, we
incorporate copying into neural network-
based Seq2Seq learning and propose a new
model called COPYNET with encoder-
decoder structure. COPYNET can nicely
integrate the regular way of word gener-
ation in the decoder with the new copy-
ing mechanism which can choose sub-
sequences in the input sequence and put
them at proper places in the output se-
quence. Our empirical study on both syn-
thetic data sets and real world data sets
demonstrates the efﬁcacy of COPYNET.
For example, COPYNET can outperform
regular RNN-based model with remark-
able margins on text summarization tasks.

Introduction

1
Recently,
neural network-based sequence-to-
sequence learning (Seq2Seq) has achieved re-
markable success in various natural language pro-
cessing (NLP) tasks, including but not limited to
Machine Translation (Cho et al., 2014; Bahdanau
et al., 2014), Syntactic Parsing (Vinyals et al.,
2015b), Text Summarization (Rush et al., 2015)
and Dialogue Systems (Vinyals and Le, 2015).

Seq2Seq is essentially an encoder-decoder model,
in which the encoder ﬁrst transforms the input se-
quence to a certain representation which can then
transforms the representation into the output se-
quence. Adding the attention mechanism (Bah-
danau et al., 2014) to Seq2Seq, ﬁrst proposed
for automatic alignment in machine translation,
has led to signiﬁcant improvement on the perfor-
mance of various tasks (Shang et al., 2015; Rush et
al., 2015). Different from the canonical encoder-
decoder architecture, the attention-based Seq2Seq
model revisits the input sequence in its raw form
(array of word representations) and dynamically
fetches the relevant piece of information based
mostly on the feedback from generation of the out-
put sequence.

In this paper, we explore another mechanism
important to the human language communication,
called the “copying mechanism”. Basically, it
refers to the mechanism that locates a certain seg-
ment of the input sentence and puts the segment
into the output sequence. For example, in the
following two dialogue turns we observe differ-
ent patterns in which some subsequences (colored
blue) in the response (R) are copied from the input
utterance (I):
I:
R: Nice to meet you, Chandralekha.

Hello Jack, my name is Chandralekha.

I:

This new guy doesn’t perform exactly
as we expected.

R: What do you mean by "doesn’t perform

exactly as we expected"?
Both the canonical encoder-decoder and its
variants with attention mechanism rely heavily
on the representation of “meaning”, which might
not be sufﬁciently inaccurate in cases in which
the system needs to refer to sub-sequences of in-
put like entity names or dates.
In contrast, the
copying mechanism is closer to the rote memo-

rization in language processing of human being,
deserving a different modeling strategy in neural
network-based models. We argue that it will ben-
eﬁt many Seq2Seq tasks to have an elegant uniﬁed
model that can accommodate both understanding
and rote memorization. Towards this goal, we pro-
pose COPYNET, which is not only capable of the
regular generation of words but also the operation
of copying appropriate segments of the input se-
quence. Despite the seemingly “hard” operation
of copying, COPYNET can be trained in an end-to-
end fashion. Our empirical study on both synthetic
datasets and real world datasets demonstrates the
efﬁcacy of COPYNET.

2 Background: Neural Models for
Sequence-to-sequence Learning

Seq2Seq Learning can be expressed in a prob-
abilistic view as maximizing the likelihood (or
some other evaluation metrics (Shen et al., 2015))
of observing the output (target) sequence given an
input (source) sequence.

2.1 RNN Encoder-Decoder
RNN-based Encoder-Decoder is successfully ap-
plied to real world Seq2Seq tasks, ﬁrst by Cho et
al. (2014) and Sutskever et al. (2014), and then
by (Vinyals and Le, 2015; Vinyals et al., 2015a).
In the Encoder-Decoder framework, the source se-
quence X = [x1, ..., xTS ] is converted into a ﬁxed
length vector c by the encoder RNN, i.e.

c = φ({h1, ..., hTS})

(1)
ht = f (xt, ht−1);
where {ht} are the RNN states, c is the so-called
context vector, f is the dynamics function, and φ
summarizes the hidden states, e.g. choosing the
last state hTS.
In practice it is found that gated
RNN alternatives such as LSTM (Hochreiter and
Schmidhuber, 1997) or GRU (Cho et al., 2014) of-
ten perform much better than vanilla ones.

The decoder RNN is to unfold the context vec-
tor c into the target sequence, through the follow-
ing dynamics and prediction model:

st = f (yt−1, st−1, c)
p(yt|y<t, X) = g(yt−1, st, c)

(2)

where st is the RNN state at time t, yt is the pre-
dicted target symbol at t (through function g(·))
with y<t denoting the history {y1, ..., yt−1}. The
prediction model is typically a classiﬁer over the
vocabulary with, say, 30,000 words.

2.2 The Attention Mechanism
The attention mechanism was ﬁrst introduced to
Seq2Seq (Bahdanau et al., 2014) to release the
burden of summarizing the entire source into a
ﬁxed-length vector as context. Instead, the atten-
tion uses a dynamically changing context ct in the
decoding process. A natural option (or rather “soft
attention”) is to represent ct as the weighted sum
of the source hidden states, i.e.

TS(cid:88)

τ =1

αtτ hτ ; αtτ =

(cid:80)

eη(st−1,hτ )
τ(cid:48) eη(st−1,hτ(cid:48) )

(3)

ct =

where η is the function that shows the correspon-
dence strength for attention, approximated usually
with a multi-layer neural network (DNN). Note
that in (Bahdanau et al., 2014) the source sen-
tence is encoded with a Bi-directional RNN, mak-
ing each hidden state hτ aware of the contextual
information from both ends.

3 COPYNET

From a cognitive perspective, the copying mech-
anism is related to rote memorization, requiring
less understanding but ensuring high literal ﬁ-
delity. From a modeling perspective, the copying
operations are more rigid and symbolic, making
it more difﬁcult than soft attention mechanism to
integrate into a fully differentiable neural model.
In this section, we present COPYNET, a differen-
tiable Seq2Seq model with “copying mechanism”,
which can be trained in an end-to-end fashion with
just gradient descent.

3.1 Model Overview
As illustrated in Figure 1, COPYNET is still an
encoder-decoder (in a slightly generalized sense).
The source sequence is transformed by Encoder
into representation, which is then read by Decoder
to generate the target sequence.
Encoder: Same as in (Bahdanau et al., 2014), a
bi-directional RNN is used to transform the source
sequence into a series of hidden states with equal
length, with each hidden state ht corresponding to
word xt. This new representation of the source,
{h1, ..., hTS}, is considered to be a short-term
memory (referred to as M in the remainder of the
paper), which will later be accessed in multiple
ways in generating the target sequence (decoding).

Figure 1: The overall diagram of COPYNET. For simplicity, we omit some links for prediction (see
Sections 3.2 for more details).

Decoder: An RNN that reads M and predicts
the target sequence. It is similar with the canoni-
cal RNN-decoder in (Bahdanau et al., 2014), with
however the following important differences

• Prediction: COPYNET predicts words based
on a mixed probabilistic model of two modes,
namely the generate-mode and the copy-
mode, where the latter picks words from the
source sequence (see Section 3.2);

• State Update: the predicted word at time t−1
is used in updating the state at t, but COPY-
NET uses not only its word-embedding but
also its corresponding location-speciﬁc hid-
den state in M (if any) (see Section 3.3 for
more details);

• Reading M: in addition to the attentive read
to M, COPYNET also has“selective read”
to M, which leads to a powerful hybrid of
content-based addressing and location-based
addressing (see both Sections 3.3 and 3.4 for
more discussion).

3.2 Prediction with Copying and Generation
We assume a vocabulary V = {v1, ..., vN}, and
use UNK for any out-of-vocabulary (OOV) word.
In addition, we have another set of words X , for
all the unique words in source sequence X =
{x1, ..., xTS}. Since X may contain words not
in V, copying sub-sequence in X enables COPY-
NET to output some OOV words. In a nutshell,
the instance-speciﬁc vocabulary for source X is
V ∪ UNK ∪ X .

Given the decoder RNN state st at time t to-
gether with M, the probability of generating any
target word yt, is given by the “mixture” of proba-
bilities as follows
p(yt|st, yt−1, ct, M) = p(yt, g|st, yt−1, ct, M)
(4)

+ p(yt, c|st, yt−1, ct, M)

where g stands for the generate-mode, and c the
copy mode. The probability of the two modes are
given respectively by



1
Z

(cid:40) 1

Z

1
Z

(cid:80)

eψg(yt),

0,

eψg(UNK)

j:xj =yt

0

p(yt, g|·)=

p(yt, c|·)=

yt ∈ V

yt ∈ X ∩ ¯V
yt (cid:54)∈ V ∪ X
yt ∈ X
otherwise

eψc(xj ),

(5)

(6)

modes, Z =(cid:80)

v∈V∪{UNK} eψg(v) +(cid:80)

where ψg(·) and ψc(·) are score functions for
generate-mode and copy-mode, respectively, and
Z is the normalization term shared by the two
x∈X eψc(x).
Due to the shared normalization term, the two
modes are basically competing through a softmax
function (see Figure 1 for an illustration with ex-
ample), rendering Eq.(4) deviated from the canon-
ical deﬁnition of mixture model (McLachlan and
Basford, 1988). This is also pictorially illustrated
in Figure 2. The score of each mode is calculated:
Generate-Mode:
The same scoring function as
in the generic RNN encoder-decoder (Bahdanau et

hello    ,     my     name   is    Tony  Jebara   . Attentive	Readhi     ,     Tony  Jebara<eos>   hi     ,     Tonyh1h2h3h4h5s1s2s3s4h6h7h8“Tony”DNNEmbedding for “Tony”SelectiveRead for “Tony”(a) Attention-based Encoder-Decoder (RNNSearch)(c) State Updates4SourceVocabularySoftmaxProb(“Jebara”) = Prob(“Jebara”, g) + Prob(“Jebara”, c)… ...(b) Generate-Mode & Copy-Mode𝜌MM(cid:80)

τ(cid:48):xτ(cid:48) =yt−1

where K is the normalization term which equals
p(xτ(cid:48), c|st−1, M), considering there
may exist multiple positions with yt−1 in the
source sequence.
In practice, ρtτ is often con-
centrated on one location among multiple appear-
ances, indicating the prediction is closely bounded
to the location of words.

In a sense ζ(yt−1) performs a type of read to
M similar to the attentive read (resulting ct) with
however higher precision.
In the remainder of
this paper, ζ(yt−1) will be referred to as selective
read. ζ(yt−1) is speciﬁcally designed for the copy
mode: with its pinpointing precision to the cor-
responding yt−1, it naturally bears the location of
yt−1 in the source sequence encoded in the hidden
state. As will be discussed more in Section 3.4,
this particular design potentially helps copy-mode
in covering a consecutive sub-sequence of words.
If yt−1 is not in the source, we let ζ(yt−1) = 0.
3.4 Hybrid Addressing of M
We hypothesize that COPYNET uses a hybrid
strategy for fetching the content in M, which com-
bines both content-based and location-based ad-
dressing. Both addressing strategies are coordi-
nated by the decoder RNN in managing the atten-
tive read and selective read, as well as determining
when to enter/quit the copy-mode.

Both the semantics of a word and its location
in X will be encoded into the hidden states in M
by a properly trained encoder RNN. Judging from
our experiments, the attentive read of COPYNET is
driven more by the semantics and language model,
therefore capable of traveling more freely on M,
even across a long distance. On the other hand,
once COPYNET enters the copy-mode, the selec-
tive read of M is often guided by the location in-
formation. As the result, the selective read often
takes rigid move and tends to cover consecutive
words, including UNKs. Unlike the explicit de-
sign for hybrid addressing in Neural Turing Ma-
chine (Graves et al., 2014; Kurach et al., 2015),
COPYNET is more subtle: it provides the archi-
tecture that can facilitate some particular location-
based addressing and lets the model ﬁgure out the
details from the training data for speciﬁc tasks.
Location-based Addressing: With the location
information in {hi}, the information ﬂow

ζ(yt−1)

update−−−→ st

predict−−−→ yt

sel. read

−−−−→ ζ(yt)

provides a simple way of “moving one step to the
right” on X. More speciﬁcally, assuming the se-

Figure 2: The illustration of the decoding proba-
bility p(yt|·) as a 4-class classiﬁer.

al., 2014) is used, i.e.
ψg(yt = vi) = v(cid:62)

st,

(cid:17)

(cid:16)

i Wost,

vi ∈ V ∪ UNK (7)
where Wo ∈ R(N +1)×ds and vi is the one-hot in-
dicator vector for vi.
Copy-Mode:
xj is calculated as

The score for “copying” the word

h(cid:62)
j Wc

ψc(yt = xj) = σ

xj ∈ X (8)
where Wc ∈ Rdh×ds, and σ is an activation
that is either an identity or a non-linear function
such as tanh. When calculating the copy-mode
score, we use the hidden states {h1, ..., hTS} to
“represent” each of the word in the source se-
quence {x1, ..., xTS} since the bi-directional RNN
encodes not only the content, but also the location
information into the hidden states in M. The loca-
tion informaton is important for copying (see Sec-
tion 3.4 for related discussion). Note that we sum
the probabilities of all xj equal to yt in Eq. (6) con-
sidering that there may be multiple source symbols
for decoding yt. Naturally we let p(yt, c|·) = 0 if
yt does not appear in the source sequence, and set
p(yt, g|·) = 0 when yt only appears in the source.
3.3 State Update
COPYNET updates each decoding state st with
the previous state st−1, the previous symbol yt−1
and the context vector ct following Eq. (2) for the
generic attention-based Seq2Seq model. However,
there is some minor changes in the yt−1−→st path
for the copying mechanism. More speciﬁcally,
yt−1 will be represented as [e(yt−1); ζ(yt−1)](cid:62),
where e(yt−1) is the word embedding associated
with yt−1, while ζ(yt−1) is the weighted sum of
hidden states in M corresponding to yt

p(xτ , c|st−1, M), xτ = yt−1
otherwise

0

(9)

(cid:88)TS

ρtτ hτ

τ =1

ζ(yt−1) =

(cid:40) 1

ρtτ =

K

unk𝑋𝑉𝑋∩𝑉’(exp𝜓-𝑣/	|	𝑣/=𝑦4’(∑exp𝜓6𝑥8		|	𝑥8=𝑦4 9:’(∑exp𝜓6𝑥8+9:exp𝜓-𝑣/	|	𝑥8=𝑦4,𝑣/=𝑦4’(exp	[𝜓-unk]*Z	is	the	normalization	term.sel. read

lective read ζ(yt−1) concentrates on the (cid:96)th word
update−−−→st
in X, the state-update operation ζ(yt−1)
acts as “location ← location+1”, making st
favor the ((cid:96)+1)th word in X in the prediction
predict−−−→ yt in copy-mode. This again leads to
st
−−−−→ζ(yt) for the state up-
the selective read ˆht
date of the next round.
Handling Out-of-Vocabulary Words Although
it is hard to verify the exact addressing strategy as
above directly, there is strong evidence from our
empirical study. Most saliently, a properly trained
COPYNET can copy a fairly long segment full of
OOV words, despite the lack of semantic infor-
mation in its M representation. This provides a
natural way to extend the effective vocabulary to
include all the words in the source. Although this
change is small, it seems quite signiﬁcant empiri-
cally in alleviating the OOV problem. Indeed, for
many NLP applications (e.g., text summarization
or spoken dialogue system), much of the OOV
words on the target side, for example the proper
nouns, are essentially the replicates of those on the
source side.

4 Learning

Although the copying mechanism uses the “hard”
operation to copy from the source and choose to
paste them or generate symbols from the vocab-
ulary, COPYNET is fully differentiable and can
be optimized in an end-to-end fashion using back-
propagation. Given the batches of the source and
target sequence {X}N and {Y }N , the objectives
are to minimize the negative log-likelihood:

(cid:104)

N(cid:88)

T(cid:88)

k=1

t=1

L = − 1
N

(cid:105)

log

p(y(k)

t

|y(k)
<t , X (k))

,

(10)

where we use superscript to index the instances.
Since the tribalistic model for observing any tar-
get word is a mixture of generate-mode and copy-
mode, there is no need for any additional labels
for modes. The network can learn to coordinate
the two modes from data. More speciﬁcally, if one
particular word y(k)
can be found in the source se-
quence, the copy-mode will contribute to the mix-
ture model, and the gradient will more or less en-
courage the copy-mode; otherwise, the copy-mode
is discouraged due to the competition from the
shared normalization term Z. In practice, in most
cases one mode dominates.

t

5 Experiments
We report our empirical study of COPYNET on the
following three tasks with different characteristics
1. A synthetic dataset on with simple patterns;
2. A real-world task on text summarization;
3. A data set for simple single-turn dialogues.

a b x c d y e f −→ g h x m,

5.1 Synthetic Dataset
Dataset: We ﬁrst randomly generate transforma-
tion rules with 5∼20 symbols and variables x &
y, e.g.
with {a b c d e f g h m} being regular symbols
from a vocabulary of size 1,000. As shown in the
table below, each rule can further produce a num-
ber of instances by replacing the variables with
randomly generated subsequences (1∼15 sym-
bols) from the same vocabulary. We create ﬁve
types of rules, including “x → ∅”. The task is
to learn to do the Seq2Seq transformation from
the training instances. This dataset is designed to
study the behavior of COPYNET on handling sim-
ple and rigid patterns. Since the string to repeat
are random, they can also be viewed as some ex-
treme cases of rote memorization.

Rule-type
x → ∅
x → x
x → x x
x y → x
x y → x y

Examples (e.g. x = i h k, y = j c)
a b c d x e f → c d g
a b c d x e f → c d x g
a b c d x e f → x d x g
a b y d x e f → x d i g
a b y d x e f → x d y g

Experimental Setting: We select 200 artiﬁcial
rules from the dataset, and for each rule 200 in-
stances are generated, which will be split into
training (50%) and testing (50%). We compare
the accuracy of COPYNET and the RNN Encoder-
Decoder with (i.e. RNNsearch) or without atten-
tion (denoted as Enc-Dec). For a fair compari-
son, we use bi-directional GRU for encoder and
another GRU for decoder for all Seq2Seq models,
with hidden layer size = 300 and word embedding
dimension = 150. We use bin size = 10 in beam
search for testing. The prediction is considered
correct only when the generated sequence is ex-
actly the same as the given one.

It is clear from Table 1 that COPYNET signiﬁ-
cantly outperforms the other two on all rule-types
except “x → ∅”, indicating that COPYNET can ef-
fectively learn the patterns with variables and ac-
curately replicate rather long subsequence of sym-
bols at the proper places.This is hard to Enc-Dec

Rule-type

Enc-Dec
RNNSearch
COPYNET

x

x

xy

x
xy
→ ∅ → x → xx → x → xy
100
0.0
2.6
99.0
77.5
97.3

2.9
40.7
68.2

3.3
69.4
93.7

1.5
22.3
98.3

Table 1: The test accuracy (%) on synthetic data.

due to the difﬁculty of representing a long se-
quence with very high ﬁdelity. This difﬁculty can
be alleviated with the attention mechanism. How-
ever attention alone seems inadequate for handling
the case where strict replication is needed.

A closer look (see Figure 3 for example) re-
veals that the decoder is dominated by copy-mode
when moving into the subsequence to replicate,
and switch to generate-mode after leaving this
area, showing COPYNET can achieve a rather pre-
cise coordination of the two modes.

Figure 3: Example output of COPYNET on the
synthetic dataset. The heatmap represents the ac-
tivations of the copy-mode over the input sequence
(left) during the decoding process (bottom).

5.2 Text Summarization
Automatic text summarization aims to ﬁnd a con-
densed representation which can capture the core
meaning of the original document.
It has been
recently formulated as a Seq2Seq learning prob-
lem in (Rush et al., 2015; Hu et al., 2015), which
essentially gives abstractive summarization since
the summary is generated based on a represen-
In contrast, extractive
tation of the document.
summarization extracts sentences or phrases from
the original text to fuse them into the summaries,
therefore making better use of the overall struc-
ture of the original document. In a sense, COPY-
NET for summarization lies somewhere between
two categories, since part of output summary is ac-
tually extracted from the document (via the copy-
ing mechanism), which are fused together possi-
bly with the words from the generate-mode.
Dataset: We evaluate our model on the recently
published LCSTS dataset (Hu et al., 2015), a large
scale dataset for short text summarization. The

dataset is collected from the news medias on Sina
Weibo1 including pairs of (short news, summary)
in Chinese. Shown in Table 2, PART II and III are
manually rated for their quality from 1 to 5. Fol-
lowing the setting of (Hu et al., 2015) we use Part
I as the training set and and the subset of Part III
scored from 3 to 5 as testing set.

-

PART III

1106
725

PART I
2,400,591

PART II
10,666
8685

Dataset
no. of pairs
no. of score ≥ 3
Table 2: Some statistics of the LCSTS dataset.
Experimental Setting: We try COPYNET that is
based on character (+C) and word (+W). For the
word-based variant the word-segmentation is ob-
tained with jieba2. We set the vocabulary size to
3,000 (+C) and 10,000 (+W) respectively, which
are much smaller than those for models in (Hu
et al., 2015). For both variants we set the em-
bedding dimension to 350 and the size of hidden
layers to 500. Following (Hu et al., 2015), we
evaluate the test performance with the commonly
used ROUGE-1, ROUGE-2 and ROUGE-L (Lin,
2004), and compare it against the two models in
(Hu et al., 2015), which are essentially canonical
Encoder-Decoder and its variant with attention.
Models

RNN
(Hu et al., 2015)
RNN context
(Hu et al., 2015)

COPYNET

ROUGE scores on LCSTS (%)
R-1
+C
21.5
+W 17.7
+C
29.9
+W 26.8
34.4
+C
+W 35.0

R-L
18.6
15.8
27.2
24.1
31.3
32.0

R-2
8.9
8.5
17.4
16.1
21.6
22.3

Table 3: Testing performance of LCSTS, where
“RNN” is canonical Enc-Dec, and “RNN context”
its attentive variant.

It is clear from Table 3 that COPYNET beats
the competitor models with big margin. Hu
et al. (2015) reports that the performance of a
word-based model is inferior to a character-based
one. One possible explanation is that a word-
based model, even with a much larger vocabulary
(50,000 words in Hu et al. (2015)), still has a large
proportion of OOVs due to the large number of en-
tity names in the summary data and the mistakes
in word segmentation. COPYNET, with its ability
to handle the OOV words with the copying mech-
anism, performs however slightly better with the
word-based variant.

1www.sina.com
2https://pypi.python.org/pypi/jieba

Pattern: 705 502 X504 339 270 584 556→510 771 581 557 022 230 X115 102 172 862 X950* Symbols are represented by their indices from 000 to 999** Dark color represents large value.510 771 581 557 263 022 230970 991 575 325 688115 102 172 862 970 991 575 325 688950705 502 970 991 575 325 688504 339 270 584 556 The Source Sequence<0>510 771 581 557 263 022 230970 991 575 325 688115 102 172 862 970 991 575 325 688Input:The Target SequencePredict:Figure 4: Examples of COPYNET on LCSTS compared with RNN context. Word segmentation is
applied on the input, where underlined are OOV words. The highlighted words (with different colors)
are those words with copy-mode probability higher than the generate-mode. We also provide literal
English translation for the document, the golden, and COPYNET, while omitting that for RNN context
since the language is broken.

5.2.1 Case Study
We make the following interesting observations
about the summary from textscCopyNet (Figure 4,
and more in the supplementary material): 1) most
words are from copy-mode, but the summary is
usually still ﬂuent; 2) COPYNET tends to cover
consecutive words in the original document, but it
often puts together segments far away from each
other, indicating a sophisticated coordination of
content-based addressing and location-based ad-
dressing; 3) COPYNET handles OOV words really
well: it can generate acceptable summary for doc-
ument with many OOVs, and even the summary
itself often contains many OOV words.
In con-
trast, the canonical RNN-based approaches often
fail in cases like that.

It is quite intriguing that COPYNET can often
ﬁnd important parts of the document, a behav-
ior with the characteristics of extractive summa-
rization, while it often generate words to “con-
nect” those words, showing its aspect of abstrac-
tive summarization.

5.3 Single-turn Dialogue
In this experiment we follow the work on neural
dialogue model proposed in (Shang et al., 2015;
Vinyals and Le, 2015; Sordoni et al., 2015), and
test COPYNET on single-turn dialogue. Basically,
the neural model learns to generate a response to

user’s input, from the given (input, response) pairs
as training instances.
Dataset: We build a simple dialogue dataset
based on the following three instructions:

1. Dialogue instances are collected from Baidu
Tieba3 with some coverage of conversations
of real life e.g., greeting and sports, etc.
hi, my name is x → hi, x

2. Patterns with slots like

are mined from the set, with possibly multi-
ple responding patterns to one input.

3. Similar with the synthetic dataset, we enlarge
the dataset by ﬁlling the slots with suitable
subsequence (e.g. name entities, dates, etc.)
To make the dataset close to the real conversations,
we also maintain a certain proportion of instances
with the response that 1) do not contain entities or
2) contain entities not in the input.
Experimental Setting: We create two datasets:
DS-I and DS-II with slot ﬁlling on 173 collected
patterns. The main difference between the two
datasets is that the ﬁlled substrings for training and
testing in DS-II have no overlaps, while in DS-I
they are sampled from the same pool. For each
dataset we use 6,500 instances for training and
1,500 for testing. We compare COPYNET with
canonical RNNSearch, both character-based, with
the same model conﬁguration in Section 5.1.

3http://tieba.baidu.com

Input(1):  今天上午9 点半，复旦投毒案将在上海二中院公开审理。被害学生黄洋的亲属已从四川抵达上海，其父称待刑事部分结束后，再提民事赔偿，黄洋92 岁的奶奶依然不知情。今年4 月，在复旦上海医学院读研究生的黄洋疑遭室友林森浩投毒，不幸身亡。新民网Today 9:30, the Fudanpoisoning case will be will on public trial at the Shanghai Second Intermediate Court.The relatives of the murdered studentHuang Yanghas arrived at Shanghai from Sichuan. His father said that they will start the lawsuit forcivil compensationafter thecriminal section. HuangYang92-year-old grandmother is still unaware of his death. In April, a graduate student at FudanUniversity Shanghai Medical College, Huang Yang is allegedly poisoned and killed by his roommate Lin Senhao. Reported by Xinmin______________________________________________________________Golden:林森浩投毒案今日开审92 岁奶奶尚不知情(the case ofLin Senhaopoisoning is on trial today, his 92-year-old grandmother is still unaware of this)RNN context:  复旦投毒案：黄洋疑遭室友投毒凶手已从四川飞往上海，父亲命案另有4人被通知家属不治？CopyNet:  复旦投毒案今在沪上公开审理(the Fudanpoisoning case is on public trial today in Shanghai )Input(2):  华谊兄弟（300027）在昨日收盘后发布公告称，公司拟以自有资金3.978 亿元收购浙江永乐影视股份有限公司若干股东持有的永乐影视51 % 的股权。对于此项收购，华谊兄弟董秘胡明昨日表示：“和永乐影视的合并是对华谊兄弟电视剧业务的一个加强。HuayiBrothers (300027) announced that the company intends to buywith its own fund 397.8 million51% of Zhejiang YongleFilm LTD's stake owned by a number of shareholders of YongleFilm LTD. For this acquisition, the secretary of the board,Hu Ming, said yesterday: "the merging with YongleFilm is to strengthen HuayiBrothers on TV business".______________________________________________________________Golden:华谊兄弟拟收购永乐影视５１％股权(HuayiBrothers intends to acquire51% stake of Zhejiang YongleFilm)RNN context:  华谊兄弟收购永乐影视51%股权：与永乐影视合并为“和唐”影视合并的“UNK”和“UNK”的区别？CopyNet:  华谊兄弟拟3.978 亿收购永乐影视董秘称加强电视剧业务( HuayiBrothersis intended to397.8 millionacquisition of YongleFilmsecretariescalledto strengthen theTVbusiness )Input(3):  工厂，大门紧锁，约20 名工人散坐在树荫下。“我们就是普通工人，在这里等工资。”其中一人说道。7 月4 日上午，记者抵达深圳龙华区清湖路上的深圳愿景光电子有限公司。正如传言一般，愿景光电子倒闭了，大股东邢毅不知所踪。The door of factory is locked.About 20 workers are scattered to sit under the shade. “We are ordinary workers, waiting for our salary” one of them said. In the morning of July 4th, reporters arrived at YuanjingPhotoelectron Corporation located at QinghuRoad, LonghuaDistrict, Shenzhen. Just as the rumor, YuanjingPhotoelectron Corporation is closed down and the big shareholder Xing Yi is missing.______________________________________________________________Golden:深圳亿元级LED 企业倒闭烈日下工人苦等老板( Hundred-million CNY worth LED enterprise is closed down and workers wait for the boss under the scorching sun)RNN context:  深圳“<UNK>”：深圳<UNK><UNK>，<UNK>，<UNK>，<UNK> CopyNet: 愿景光电子倒闭20 名工人散坐在树荫下(YuanjingPhotoelectron Corporation is closed down, 20 workers are scattered to sit under the shade)Figure 5: Examples on the testing set of DS-II shown as the input text and golden, with the outputs
of RNNSearch and CopyNet. Words in red rectangles are unseen in the training set. The highlighted
words (with different colors) are those words with copy-mode probability higher than the generate-mode.
Green cirles (meaning correct) and red cross (meaning incorrect) are given based on human judgment on
whether the response is appropriate.

DS-I (%)

Models
RNNSearch
COPYNET

Top1
44.1
61.2

Top10
57.7
71.0

DS-II (%)

Top1
13.5
50.5

Top10
15.9
64.8

Table 4: The decoding accuracy on the two testing
sets. Decoding is admitted success only when the
answer is found exactly in the Top-K outputs.

We compare COPYNET and RNNSearch on
DS-I and DS-II in terms of top-1 and top-10 ac-
curacy (shown in Table 4), estimating respectively
the chance of the top-1 or one of top-10 (from
beam search) matching the golden. Since there are
often many good responses to a input, top-10 accu-
racy appears to be closer to the real world setting.
As shown in Table 4, COPYNET signiﬁcantly
outperforms RNNsearch, especially on DS-II. It
suggests that introducing the copying mechanism
helps the dialogue system master the patterns in
dialogue and correctly identify the correct parts of
input, often proper nouns, to replicate in the re-
sponse. Since the ﬁlled substrings have no over-
laps in DS-II, the performance of RNNSearch
drops signiﬁcantly as it cannot handle words un-
seen in training data. In contrast, the performance
of COPYNET only drops slightly as it has learned
to ﬁll the slots with the copying mechanism and
relies less on the representation of the words.
5.3.1 Case Study
As indicated by the examples in Figure 5, COPY-
NET accurately replicates the critical segments
from the input with the copy-mode, and generates
the rest of answers smoothly through the generate-
mode. Note that in (2) and (3), the decoding se-
quence is not exactly the same with the standard
one, yet still correct regarding to their meanings.
In contrast, although RNNSearch usually gener-
ates answers in the right formats, it fails to catch

the critical entities in all three cases because of the
difﬁculty brought by the unseen words.

6 Related Work

Our work is partially inspired by the recent work
of Pointer Networks (Vinyals et al., 2015a), in
which a pointer mechanism (quite similar with the
proposed copying mechanism) is used to predict
the output sequence directly from the input. In ad-
dition to the difference with ours in application,
(Vinyals et al., 2015a) cannot predict outside of
the set of input sequence, while COPYNET can
naturally combine generating and copying.

COPYNET is also related to the effort to solve
the OOV problem in neural machine translation.
Luong et al. (2015) introduced a heuristics to post-
process the translated sentence using annotations
on the source sentence. In contrast COPYNET ad-
dresses the OOV problem in a more systemic way
with an end-to-end model. However, as COPY-
NET copies the exact source words as the output, it
cannot be directly applied to machine translation.
The copying mechanism can also be viewed as
carrying information over to the next stage without
any nonlinear transformation. Similar ideas are
proposed for training very deep neural networks in
(Srivastava et al., 2015; He et al., 2015) for clas-
siﬁcation tasks, where shortcuts are built between
layers for the direct carrying of information.

7 Conclusion and Future Work

We proposed COPYNET to incorporate the copy-
ing mechanism into Seq2Seq learning framework.
For future work, we will extend this idea to the
task where the source and target are in different
languages, for example, machine translation.

Input(1):  我出生在上海。/I was born in ShanghaiGolden:   真巧，我也来自上海，老乡啊!   /What a coincidence, I also come from Shanghai, we are fellow townsman!RNNSearch:   真巧，我也来自海南，老乡啊!   /What a coincidence, I also come from Hainan, we are fellow townsman!CopyNet:   真巧，我也来自上海，老乡啊!   /What a coincidence, I also come from Shanghai, we are fellow townsman!Input(2):  今天去爬黄山吗？/Shall we climbing Huangshan?Golden:   去爬泰山吧，黄山没意思!  /Climb Taishan. Huangshanis boring!RNNSearch:   去爬长白山吧，衡山没意思!       /Climb Changbaishan, Hengshanis boring!CopyNet:   去爬嵩山吧，黄山没意思!   /Climb Songshan, Huangshanis boring!Input(3):  你喜欢喝雪碧还是喜欢可乐？/Would you like to drink Sprite or Coke?Golden:    都不喜欢，谢谢!                            /Not at all, thanks!RNNSearch:    我喜欢喝蜂蜜! /Ilike to drink honey!CopyNet:    我喜欢喝可乐!   /Ilike to drink coke!Shiqi Shen, Yong Cheng, Zhongjun He, Wei He, Hua
Wu, Maosong Sun, and Yang Liu. 2015. Minimum
risk training for neural machine translation. CoRR,
abs/1512.02433.

Alessandro Sordoni, Michel Galley, Michael Auli,
Chris Brockett, Yangfeng Ji, Margaret Mitchell,
Jian-Yun Nie, Jianfeng Gao, and Bill Dolan. 2015.
A neural network approach to context-sensitive gen-
eration of conversational responses. arXiv preprint
arXiv:1506.06714.

Rupesh Kumar Srivastava, Klaus Greff, and J¨urgen
Schmidhuber. 2015. Highway networks. arXiv
preprint arXiv:1505.00387.

Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in neural information process-
ing systems, pages 3104–3112.

Oriol Vinyals and Quoc Le. 2015. A neural conversa-

tional model. arXiv preprint arXiv:1506.05869.

Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly.
In Advances in Neural
2015a. Pointer networks.
Information Processing Systems, pages 2674–2682.

Oriol Vinyals, Łukasz Kaiser, Terry Koo, Slav Petrov,
Ilya Sutskever, and Geoffrey Hinton. 2015b. Gram-
mar as a foreign language. In Advances in Neural
Information Processing Systems, pages 2755–2763.

References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2014. Neural machine translation by jointly
arXiv preprint
learning to align and translate.
arXiv:1409.0473.

Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio.
2014. Learning
phrase representations using rnn encoder-decoder
for statistical machine translation. arXiv preprint
arXiv:1406.1078.

Alex Graves, Greg Wayne,

2014. Neural turing machines.
arXiv:1410.5401.

and Ivo Danihelka.
arXiv preprint

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. 2015. Deep residual learning for image recog-
nition. arXiv preprint arXiv:1512.03385.

Sepp Hochreiter and J¨urgen Schmidhuber.

1997.
Neural computation,

Long short-term memory.
9(8):1735–1780.

Baotian Hu, Qingcai Chen, and Fangze Zhu. 2015. Lc-
sts: a large scale chinese short text summarization
dataset. arXiv preprint arXiv:1506.05865.

Karol Kurach, Marcin Andrychowicz,

and Ilya
Sutskever. 2015. Neural random-access machines.
arXiv preprint arXiv:1511.06392.

Chin-Yew Lin. 2004. Rouge: A package for auto-
matic evaluation of summaries. In Stan Szpakowicz
Marie-Francine Moens, editor, Text Summarization
Branches Out: Proceedings of the ACL-04 Work-
shop, pages 74–81, Barcelona, Spain, July. Associa-
tion for Computational Linguistics.

Thang Luong, Ilya Sutskever, Quoc Le, Oriol Vinyals,
and Wojciech Zaremba. 2015. Addressing the rare
word problem in neural machine translation. In Pro-
ceedings of the 53rd Annual Meeting of the Associ-
ation for Computational Linguistics and the 7th In-
ternational Joint Conference on Natural Language
Processing (Volume 1: Long Papers), pages 11–19,
Beijing, China, July. Association for Computational
Linguistics.

Geoffrey J McLachlan and Kaye E Basford. 1988.
Mixture models. inference and applications to clus-
tering. Statistics: Textbooks and Monographs, New
York: Dekker, 1988, 1.

Alexander M Rush, Sumit Chopra, and Jason We-
2015. A neural attention model for ab-
arXiv preprint

ston.
stractive sentence summarization.
arXiv:1509.00685.

Lifeng Shang, Zhengdong Lu, and Hang Li. 2015.
Neural responding machine for short-text conversa-
tion. arXiv preprint arXiv:1503.02364.

A supplemental materials
A.1 Text Summarization
We present more examples of summarization results on the testing set of LCSTS, following the same
instructions discussed in Section 5.2 comparing COPYNET and RNN encoder as you can see in Figure 6
and Figure 7.

Figure 6: Examples 1-4 of COPYNET on LCSTS compared with RNN context. Word segmentation is
applied on the input, where underlined are OOV words. The highlighted words (with different colors) are
those words with copy-mode probability higher than the generate-mode. We also provide literal English
translation for the document, the golden, and COPYNET, while omitting that for RNN context since the
language is broken.

Input(1):  截至2012 年10 月底，全国累计报告艾滋病病毒感染者和病人492191例。卫生部称，性传播已成为艾滋病的主要传播途径。至2011 年9 月，艾滋病感染者和病人数累计报告数排在前6 位的省份依次为云南、广西、河南、四川、新疆和广东，占全国的75.8 % 。。At	the	end	of	October	2012,	the	national	total	of	reported	HIV	infected	people	and	AIDS	patients	is	492,191	cases.	The	Health	Ministry	saidsexualtransmission	has	become	the	main	route	of	transmission	of	AIDS.	To	September	2011,the	six	provinces	with	the	mostreported	HIV	infected	people	and	AIDS	patientswere	Yunnan,	Guangxi,	Henan,Sichuan,	Xinjiang	and	Guangdong,	accounting	for	75.8%	of	the	country.______________________________________________________________Golden:卫生部：性传播成艾滋病主要传播途径Ministry	of	Health:	Sexually	transmission	became	the	main	route	of	transmission	of	AIDSRNN context:  全国累计报告艾滋病患者和病人<UNK>例艾滋病患者占全国<UNK>%，性传播成艾滋病高发人群？CopyNet:  卫生部：性传播已成为艾滋病主要传播途径Ministry	of	Health:	Sexually	transmission	has	become	the	main	route	of	transmission	of	AIDSInput(2):  中国反垄断调查风暴继续席卷汽车行业，继德国车企奥迪和美国车企克莱斯勒“沦陷”之后，又有12 家日本汽车企业卷入漩涡。记者从业内人士获悉，丰田旗下的雷克萨斯近期曾被发改委约谈。Chinese	antitrust	investigation	continues	to	sweep	the	automotive	industry.	After	GermanyAudi	carand	the	US	Chrysler	"fell",	there	are	12	Japanese	car	companies	involved	in	the	whirlpool.	Reporters	learned	from	the	insiders	that	Toyota's	Lexus	has	been	asked	to	report	to	the	Development	and	Reform	Commission	recently.______________________________________________________________Golden:发改委公布汽车反垄断进程：丰田雷克萨斯近期被约谈the	investigation	by	Development	and	Reform	Commission:Toyota's	Lexus	has	been	asked	to	reportRNN context:  丰田雷克萨斯遭发改委约谈：曾被约谈丰田旗下的雷克萨斯遭发改委约谈负人被约谈CopyNet:  中国反垄断继续席卷汽车行业12 家日本汽车企业被发改委约谈Chinese	antitrust	investigation	continues	to	sweep	the	automotive	industry.12	Japanese	car	companies	are	asked	to	report	tohe	Development	and	Reform	CommissionInput(3):  镁离子电池相比锂电池能量密度提升了近一倍，这意味着使用了镁电池的电动车，纯电续航也将有质的提升。但目前由于电解质等技术壁垒，要大规模量产并取代锂电池还为时过早。The	energy	density	of	Magnesiumion	batteries	almost	doubles	that	of	lithium	battery,	which	means	that	for	the	electric	vehiclesusing	of	magnesiumbatterieswill	last	longer	even	at	pure	electric	power.	But	currently	due	to	the	technical	barriers	to	theelectrolyte,	it	is	still	too	early	for	the	massproduction	of	it	and	replacing	lithium	batteries..______________________________________________________________Golden:锂电池或将被淘汰能量密度更高的镁电池亦大势所趋Lithium	batteries	will	be	phased	out,	magnesium	battery	withenergy	density	higherwill	be	the	future	trendRNN context:  <UNK>、<UNK>、<UNK>、<UNK>、<UNK>、<UNK>、<UNK>、<UNK>、<UNK>、<UNK>、<UNK>、<UNK>电池了CopyNet:  镁离子电池问世：大规模量产取代锂电池Magnesium	ion	battery	is	developed	:	mass	production	of	it	will	replace	lithium	batteries	Input(4):  青岛德诚涉及青岛港骗贷风波，虽然是一家中小型的民营企业，但据调查，其是福布斯富豪陈基鸿的旗下产业。其仓单质押融资绝大部分投向了房地产，而眼下中小房企出事的这么多，资金链断裂连带着揭出仓单质押融资问题会很多。QingdaoTak-shingis	inovledin	fraudstorm	ofQingdao	Port.	Although	it	is	a	medium-sized	private	enterprises,	but	it	is	owned	by	Chen	Hong,	one	of	Forbes's	richest,according	to	the	survey.	The	vast	majority	of	its	warehouse	receipt	financing	pledge	to	invest	in	real	estate,	but	now	small	real	estate	companies	are	in	trouble,even	with	money-strand	breaks	revealing	more	Warehouse	Receipt	Financing	problems.______________________________________________________________Golden:一福布斯富豪事涉青岛港骗贷风波陷房产资金危局One	of	Forbes's	richest	is	involved	in	Qingdao	Portfraudand	get	trapped	in	the	fund	crisisRNN context:  青岛骗贷风波背后的房企资金链：<UNK>质押融资近8亿，融资一半投向了三四个非核心项目的机会吗？CopyNet:  青岛青岛港骗贷风波调查：资金链断裂质押融资问题The	investigation	of	Qingdao	Port	fraud:	funding	strand	breaks	and	pledge	financing	problemsFigure 7: Examples 5-7 of COPYNET on LCSTS compared with RNN context. Word segmentation is
applied on the input, where underlined are OOV words. The highlighted words (with different colors) are
those words with copy-mode probability higher than the generate-mode. We also provide literal English
translation for the document, the golden, and COPYNET, while omitting that for RNN context since the
language is broken.

A.2 Single-turn Dialogue System
We present more examples of single-turn dialogue results on the testing set of the dialogue dataset,
following the same instructions discussed in Section 5.3, comparing COPYNET and RNN encoder as
you can see in Figure 8.

Figure 8: Examples on the testing set of DS-II shown as the input text and golden, with the outputs of
RNNSearch and CopyNet. Words in red rectangles are unseen in the training set. The highlighted words
(with different colors) are those words with copy-mode probability is higher than the generate-mode.
Green cirles (meaning correct) and red cross (meaning incorrect) are given based on human judgment
whether the response is appropriate.

Input(5):  1 . 掌握技巧融会贯通；2 . 学会融资；3 . 懂法律；4 . 保持自信；5 . 测试+ 尝试；6 . 了解客户的需求；7 . 预测+ 衡量+ 确保；8 . 做好与各种小bug做斗争的心态；9 . 发现机遇保持创业激情。1.master	the	skills;	2	Learn	tofinance;	3.	understand	the	law;	4.	Be	confident;	5.	test+	trial;	6.	understand	the	need	of	customers;	7	forecast+measure+ensure;	8.	mentally	prepared	to	fight	all	kinds	of	small	bugs	;	9	discover	opportunities	and	keep	the	passion	of	start-up.______________________________________________________________Golden:初次创业者必知的10 个技巧The	10	tips	for	the	first	time	start-ups	RNN context:  6个方法让你创业的6个<UNK>与<UNK>，你怎么看懂你的创业故事吗？（6家）CopyNet:  创业成功的9 个技巧The	9	tips	for	success	in	start-upInput(6):  9 月3 日，总部位于日内瓦的世界经济论坛发布了《2014 -2015 年全球竞争力报告》，瑞士连续六年位居榜首，成为全球最具竞争力的国家，新加坡和美国分列第二位和第三位。中国排名第28 位，在金砖国家中排名最高。On	September	3,	the	Geneva	based	World	Economic	Forum	released	“	The	Global	Competitiveness	Report	2014-2015”.Switzerland	topped	the	list	for	six	consecutive	years	,	becoming	the	world‘s	most	competitive	country.	Singapore	and	the	United	States	are	in	the	second	and	third	place	respectively.	China	is	in	the	28th	place,	ranking	highest	among	the	BRIC	countries.______________________________________________________________Golden:全球竞争力排行榜中国居28 位居金砖国家首位The	Global	competitiveness	ranking	list,	China	is	in	the	28th	place,	the	highest	among	BRIC	countries.RNN context:  2014-2015年全球竞争力报告：瑞士连续6年居榜首中国居28位(首/3———访榜首)中国排名第28位CopyNet:  2014 -2015 年全球竞争力报告：瑞士居首中国第282014--2015	Global	Competitiveness	Report:	Switzerland	topped	and	China	the	28thInput(7):  瑞典媒体日前报道，“按照购买力平价的计算方式，中国经济总量29 日超过美国，成为世界第一”。专家分析，其可能是将全年总量除以365天得出的平均值，但一个国家的经济不可能是匀速增长的，此外，购买力平价本身也存在高估中国的问题。Swedish	media	recently	reported	that,	“in	accordance	with	the	calculation	of	purchasing	power	parity,	China‘s	GDP	exceeded	United	Stateson	29th,	becoming	the	first	in	the	world.”	Expert	analyzed,	this	may	be	the	average	over	365	days,	but	a	country’s	economic	growth	is	not	likely	to	be	on	the	same	speed.	Moreover,	there	is	problem	of	over	estimate	Chine	with	PPP	itself.______________________________________________________________Golden:观察：中国经济总量29 日超美？专家称“不靠谱”Watch:	China‘s	GDPsurpassesthe	United	States	on	the	29th?	Experts	say	“do	not	believe	it”RNN context:  外媒:中国经济总量29日成为世界第一中国经济总量29日超美成世界第一，但不是中国的“问题”而是“<UNK>”的是福是祸CopyNet: 中国经济总量超美国成世界第一China's	GDP	surpasses	theUnitedStates,	becoming	the	world's	NO.1Input(1):  我大青岛四季宜人/Our great Qingdao has a good weather!Golden:   壮我大青岛～～～/Our Qingdao is great~~~RNNSearch:   壮我大哈尔滨～～～                     /Our Harbinis great~~~CopyNet:   壮我大青岛～～～/Our Qingdao is great~~~Input(2):看了昨晚利物浦的比赛了吗？/Did you watch Liverpool’s game last night?Golden: 我这眼睛比裁判都强!                   /My eyes are stronger than the refereeRNNSearch:   我这眼睛比裁判都强! /My eyes are stronger than the refereeCopyNet:   昨晚睡觉了没看利物浦怎么样了/I slept last night and I did not see how is LiverpoolInput(3):  女儿会游蝶泳/Daughter can swim butterfly?Golden:   我连游泳都不会T^T/I cannot swim at all T^TRNNSearch:   我连游泳都不会T^T                 /I cannot swim at all T^TCopyNet:   我连游泳都不会T^T                  /I cannot swim at all T^TInput(4):  苏有朋是谁?/Who is Su You Peng?Golden:   苏有朋是一个男明星。/Su You Pengis a male star.CopyNet:   苏有是一个男明星。/Su You is a male starRNNSearch:吴亦凡是一个男明星。/Wu Yifanis a male star.