6
1
0
2

 
r
a

 

M
0
1

 
 
]

.

C
O
h
t
a
m

[
 
 

1
v
9
2
3
3
0

.

3
0
6
1
:
v
i
X
r
a

Improved convergence rates for Lasserre-type hierarchies of upper

bounds for box-constrained polynomial optimization

Etienne de Klerk

Tilburg University

PO Box 90153, 5000 LE Tilburg, The Netherlands

E.deKlerk@uvt.nl

Roxana Hess∗

LAAS-CNRS, Universit´e de Toulouse

LAAS, 7 avenue du colonel Roche, 31400 Toulouse, France

rhess@laas.fr

Monique Laurent

Centrum Wiskunde & Informatica (CWI), Amsterdam and Tilburg University

CWI, Postbus 94079, 1090 GB Amsterdam, The Netherlands

M.Laurent@cwi.nl

March 11, 2016

Abstract

We consider the problem of minimizing a given n-variate polynomial f over the hypercube [−1, 1]n.
An idea introduced by Lasserre, is to ﬁnd a probability distribution on [−1, 1]n with polynomial density
[−1,1]n f (x)h(x)dµ(x), where dµ(x) is a
ﬁxed, ﬁnite Borel measure supported on [−1, 1]n. It is known that, for the Lebesgue measure dµ(x) = dx,
√
one may show an error bound O(1/
r) if h is a sum-of-squares density, and an O(1/r) error bound if
h is the density of a beta distribution. In this paper, we show an error bound of O(1/r2), if dµ(x) =

function h (of given degree r) that minimizes the expectation (cid:82)
(cid:16)(cid:81)n

dx (the well-known measure in the study of orthogonal polynomials), and h has a
Schm¨udgen-type representation with respect to [−1, 1]n, which is a more general condition than a sum
of squares. The convergence rate analysis relies on the theory of polynomial kernels, and in particular
on Jackson kernels. We also show that the resulting upper bounds may be computed as generalized
eigenvalue problems, as is also the case for sum-of-squares densities.

(cid:17)−1

(cid:112)1 − x2

i

i=1

Keywords: box-constrained global optimization, polynomial optimization, Jackson kernel, semideﬁnite
programming, generalized eigenvalue problem, sum-of-squares polynomial

AMS classiﬁcation: 90C60, 90C56, 90C26.

∗Most of this work was done while the second author was staying at CWI in autumn 2015. She would like to thank CWI,
and in particular M. Laurent for the hospitality and support during her stay, and Universit´e Paul Sabatier, ´Ecole Doctorale
Syst`emes and ´Ecole des Docteurs de l’Universit´e F´ed´ederale Toulouse Midi-Pyr´en´ees for the funding.

1

Improved convergence rates for Lasserre-type hierarchies

2

1

Introduction

1.1 Background results

We consider the problem of minimizing a given n-variate polynomial f ∈ R[x] over the compact set K =
[−1, 1]n, i.e., computing the parameter

fmin = min
x∈K

f (x).

(1.1)

This is a hard optimization problem which contains, e.g., the well known NP-hard maximum stable set and
maximum cut problems in graphs (see, e.g., [15, 16]). It falls within box-constrained (aka bound-constrained)
optimization which has been widely studied in the literature. In particular iterative methods for bound-
constrained optimization are described in the books [1, 5, 6], including projected gradient and active set
methods. The latest algorithmic developments for box-constrained global optimization are surveyed in the
recent thesis [14]; see also [7] and the references therein for recent work on active set methods, and a list
of applications. The box-constrained optimization problem is even of practical interest in the (polynomially
solvable) case where f is a convex quadratic problem, and dedicated active set methods have been developed
for this case; see [8].

In this paper we will focus on the question of ﬁnding a sequence of upper bounds converging to the global
minimum and allowing a known estimate on the rate of convergence. It should be emphasized that it is in
general a diﬃcult challenge in non-convex optimization to obtain such results. Following Lasserre [9, 10],
our approach will be based on reformulating problem (1.1) as an optimization problem over measures and
then restricting to subclasses of measures that we are able to analyze. Sequences of upper bounds have been
recently proposed and analyzed in [4, 3], in the present paper we will propose new bounds for which we can
prove a sharper rate of convergence. We now introduce our approach.

As observed by Lasserre [9], problem (1.1) can be reformulated as

(cid:90)

fmin =

min

µ∈M(K)

K

f (x)dµ(x),

where M(K) denotes the set of probability measures supported on K. Hence an upper bound on fmin may
be obtained by considering a ﬁxed probability measure µ on K. In particular, the optimal value fmin is
obtained when selecting for µ the Dirac measure at a global minimizer x∗ of f in K.

Lasserre [10] proposed the following strategy to build a hierarchy of upper bounds converging to fmin.
The idea is to do successive approximations of the Dirac measure at x∗ by using sum-of-squares (SOS)
density functions of growing degrees. More precisely, Lasserre [10] considered a set of Borel measures µr
obtained by selecting a ﬁxed, ﬁnite Borel measure µ on K (like, e.g., the Lebesgue measure) together with
a polynomial density function that is a sum-of-squares (SOS) polynomial of given degree r.

When selecting for µ the Lebesque measure on K this leads to the following hierarchy of upper bounds

on fmin, indexed by r ∈ N:

(cid:90)

(cid:90)

f (r)
K

:= inf

h∈Σ[x]r

K

K

h(x)f (x)dx s.t.

h(x)dx = 1,

(1.2)

where Σ[x]r denotes the set of sum-of-squares polynomials of degree at most r.

The convergence to fmin of the bounds f (r)
K

is an immediate consequence of the following theorem, which

holds for general compact sets K and continuous functions f .

Improved convergence rates for Lasserre-type hierarchies

3

Theorem 1.1 [10, cf. Theorem 3.2] Let K ⊆ Rn be compact, let µ be an arbitrary ﬁnite Borel measure
supported by K, and let f be a continuous function on Rn. Then, f is nonnegative on K if and only if

f g2dµ ≥ 0 ∀g ∈ R[x].

(cid:90)

K

(cid:90)

fmin = inf

h∈Σ[x]

f hdµ s.t.

K

K

(cid:90)

hdµ = 1.

(1.3)

Therefore, the minimum of f over K can be expressed as

In the recent work [3], it is shown that for a compact set K ⊆ [0, 1]n one may obtain a similar result
using density functions arising from (products of univariate) beta distributions. In particular, the following
theorem is implicit in [3].

Theorem 1.2 [3] Let K ⊆ [0, 1]n be a compact set, let µ be an arbitrary ﬁnite Borel measure supported by
K, and let f be a continuous function on Rn. Then, f is nonnegative on K if and only if

(cid:90)
(cid:81)n
(cid:81)n

K

K

(cid:82)
(cid:90)

f hdµ ≥ 0

i=1 xβi
i=1 xβi

i (1 − xi)ηi
i (1 − xi)ηi
(cid:90)

for all h of the form

h(x) =

,

(1.4)

where the β(cid:48)

is and η(cid:48)

is are nonnegative integers. Therefore, the minimum of f over K can be expressed as

fmin = inf
h

f hdµ s.t.

hdµ = 1,

K

K

(1.5)

where the inﬁmum is taken over all beta-densities h of the form (1.4).

For the box K = [0, 1]n and selecting for µ the Lebesgue measure, we obtain a hierarchy of upper bounds
is the optimum value of the program (1.5) when the inﬁmum is taken over

r converging to fmin, where f H
f H
r
all beta-densities h of the form (1.4) with degree r.

The rate of convergence of the upper bounds f (r)
K

respectively. It is shown in [4] that f (r)
K
all convex bodies and thus the box [0, 1]n or [−1, 1]n) and the stronger rate f H
in [3] for the box K = [0, 1]n. While the parameters f (r)
K
(in fact, a generalized eigenvalue computation problem, see [10]), an advantage of the parameters f H
r
their computation involves only elementary operations (see [3]).

r has been investigated recently in [4] and [3],
r) for a large class of compact sets K (including
r − fmin = O(1/r) is shown
can be computed using semideﬁnite optimization
is that

and f H
√
− fmin = O(1/

Another possibility to get a hierarchy of upper bounds is grid search, where one takes the best function
evaluation at all rational points in K = [0, 1]n with given denominator r. It has been shown in [3] that these
bounds have a rate of convergence in O(1/r2). However, the computation of the order r bound needs an
exponential number rn of function evaluations.

1.2 New contribution

In the present work we continue this line of research. For the box K = [−1, 1]n, our objective is to build a new
hierarchy of measure-based upper bounds, for which we will be able to show a sharper rate of convergence in

Improved convergence rates for Lasserre-type hierarchies

4

O(1/r2). We obtain these upper bounds by considering a speciﬁc Borel measure µ (speciﬁed below in (1.7))
and polynomial density functions with a so-called Schm¨udgen-type SOS representation (as in (1.6) below).

We ﬁrst recall the relevant result of Schm¨udgen [19], which gives SOS representations for positive poly-

nomials on a basic closed semi-algebraic set (see also, e.g., [17],[11, Theorem 3.16], [13]).

p =(cid:80)

Theorem 1.3 (Schm¨udgen [19]) Consider the set K = {x ∈ Rn | g1(x) ≥ 0, . . . , gm(x) ≥ 0} where
g1, . . . , gm ∈ R[x], and assume that K is compact. If p ∈ R[x] is positive on K, then p can be written as

(cid:81)
i∈I gi, where σI (I ⊆ [m]) are sum-of-squares polynomials.
For the box K = [−1, 1]n, described by the polynomial inequalities 1−x2

n ≥ 0, we consider

1 ≥ 0, . . . , 1−x2

I⊆[m] σI

polynomial densities that allow a Schm¨udgen-type representation of bounded degree r:

(1.6)

where the polynomials σI are sum-of-squares polynomials with degree at most r − 2|I| (to ensure that the
degree of h is at most r). We will also ﬁx the following Borel measure µ on [−1, 1]n (which, as will be recalled
below, is associated to some orthogonal polynomials):

(cid:88)

I⊆[n]

(cid:89)

i∈I

h(x) =

σI (x)

(1 − x2
i ),

(cid:32) n(cid:89)

(cid:113)

1 − x2

i

(cid:33)−1

dµ(x) =

π

dx.

(1.7)

This leads to the following new hierarchy of upper bounds f (r) for fmin.

i=1

Deﬁnition 1.4 Let µ be the Borel measure from (1.7). For r ∈ N consider the parameters:

(cid:90)

f (r) := inf
h

[−1,1]n

(cid:90)

[−1,1]n

f hdµ s.t.

hdµ = 1,

(1.8)

where the inﬁmum is taken over the polynomial densities h that allow a Schm¨udgen-type representation (1.6),
where each σI is a sum-of-squares polynomial with degree at most r − 2|I|.

The convergence of the parameters f (r) to fmin follows as a direct application of Theorem 1.1. A main
result in this paper is to show that the bounds f (r) have a rate of convergence in O(1/r2). Moreover we will
show that the parameter f (r) can be computed through generalized eigenvalue computations.

Theorem 1.5 Let f ∈ R[x] be a polynomial and fmin be its minimum value over the box [−1, 1]n. For any
r large enough, the parameters f (r) deﬁned in (1.8) satisfy:
f (r) − fmin = O

(cid:18) 1

(cid:19)

.

r2

As already observed above this result compares favorably with the estimate: f (r)
K

− fmin = O

shown

in [4] for the bounds f (r)
K based on using SOS densities. (Note however that the latter convergence rate holds
for a larger class of sets K that includes all convex bodies; see [4] for details.) The new result also improves
the estimate: f H
r obtained by using densities arising from
beta distributions.

(cid:1), shown in [3] for the bounds f H

r − fmin = O(cid:0) 1

r

We now illustrate the optimal densities appearing in the new bounds f (r) on an example.

(cid:17)

(cid:16) 1√

r

Improved convergence rates for Lasserre-type hierarchies

5

Example 1.6 Consider the minimization of the Motzkin polynomial
2) − 48x2

over the hypercube [−1, 1]2, which has four global minimizers at the points(cid:0)± 1

f (x1, x2) = 64(x4

2 + 1

1x2

2 + x2

1x4

1x2

1 shows the optimal density function h∗ computed when solving the problem (1.8) for degrees 12 and 16,
respectively. Note that the optimal density h∗ shows four peaks at the four global minimizers of f in [−1, 1]2.
The corresponding upper bounds from (1.8) are f (12) = 0.8098 and f (16) = 0.6949.

(cid:1), and fmin = 0. Figure

2 ,± 1

2

Figure 1: Graphs of h∗ on [−1, 1]2 (deg(h∗) = 12, 16) for the Motzkin polynomial.

Strategy and outline of the paper

are able to show that(cid:82)

In order to show the convergence rate in O(1/r2) of Theorem 1.5 we need to exhibit a polynomial density
function hr of degree at most r which admits a SOS representation of Schm¨udgen-type and for which we
[−1,1]n f hdµ − fmin = O(1/r2). The idea is to ﬁnd such a polynomial density which
approximates well the Dirac delta function at a global minimizer x∗ of f over [−1, 1]n. For this we will use
the well established Polynomial Kernel Method (KPM) and more speciﬁcally we will use the Jackson kernel,
a well known tool in approximation theory to yield best (uniform) polynomial approximations of continuous
functions.

The paper is organized as follows. Section 2 contains some background information about the polynomial
kernel method needed for our analysis of the new bounds f (r). Speciﬁcally we introduce Chebyshev poly-
nomials in Section 2.1 and Jackson kernels in Section 2.2, and then we use them in Section 2.3 to construct
suitable polynomial densities hr giving good approximations of the Dirac delta function at a global minimizer
of f in the box. We then carry out the analysis of the upper bounds on fmin in Section 3.1 for the univariate
case and in Section 3.2 for the general multivariate case, thus proving the result of Theorem 1.5. In Section
4 we show how the new bounds f (r) can be computed as generalized eigenvalue problems and in Section 5
we conclude with some numerical examples illustrating the behaviour of the bounds f (r).

−1−0.8−0.6−0.4−0.200.20.40.60.81−10100.20.40.60.811.2−1−0.8−0.6−0.4−0.200.20.40.60.81−1−0.500.5100.20.40.60.811.2Improved convergence rates for Lasserre-type hierarchies

6

Notation

h =(cid:80)k
degree at most r (of the form h =(cid:80)k

Throughout, Σ[x] denotes the set of all sum-of-squares (SOS) polynomials (i.e., all polynomials h of the form
i=1 pi(x)2 for some polynomials p1, . . . , pk and k ∈ N) and Σ[x]r denotes the set of SOS polynomials of
i=1 pi(x)2 for some polynomials pi of degree at most r/2). For α ∈ Nn,
Supp(α) = {i ∈ [n] : αi (cid:54)= 0} denotes the support of α and, for α, β ∈ Nn, δα,β ∈ {0, 1} is equal to 1 if and
only if α = β.

2 Background on the polynomial kernel method

Our goal is to approximate the Dirac delta function at a given point x∗ ∈ Rn as well as possible using
polynomial density functions of bounded degrees. This is a classical question in approximation theory. In
this section we will review how this may be done using the polynomial kernel method and, in particular,
using Jackson kernels. This theory is usually developed using the Chebyshev polynomials, and we start
by reviewing their properties. We will follow mainly the work [20] for our exposition and we refer to the
handbook [2] for more background information.

2.1 Chebyshev polynomials

We will use the univariate polynomials Tk(x) and Uk(x), respectively known as the Chebyshev polynomials
of the ﬁrst and second kind. They are deﬁned as follows:

Tk(x) = cos(k arccos(x)), Uk(x) =

sin((k + 1) arccos(x))

sin(arccos(x))

for x ∈ [−1, 1], k ∈ N,

(2.1)

and they satisfy the following recurrence relationships:

T0(x) = 1, T−1(x) = T1(x) = x, Tk+1(x) = 2xTk(x) − Tk−1(x),

U0(x) = 1, U−1(x) = 0, Uk+1(x) = 2xUk(x) − Uk−1(x).

(2.2)

(2.3)

As a direct application one can verify that

Tk(0) =

(−1) k

2

for k odd

for k even

, Tk(1) = 1, Uk(1) = k + 1, Uk(−1) = (−1)k(k + 1)

for k ∈ N.

(2.4)

0

The Chebyshev polynomials have the extrema

max

x∈[−1,1]

|Tk(x)| = 1 and max
x∈[−1,1]

|Uk(x)| = k + 1,

attained at x = ±1 (see, e.g., [2, §22.14.4, 22.14.6]).

The Chebyshev polynomials are orthogonal for the following inner product on the space of integrable

functions over [−1, 1]:

(cid:90) 1

−1

(cid:104)f, g(cid:105) =

√
f (x)g(x)
1 − x2
π

dx,

and their orthogonality relationships read:

(cid:104)Tk, Tm(cid:105) = 0 if k (cid:54)= m, (cid:104)T0, T0(cid:105) = 1, (cid:104)Tk, Tk(cid:105) = 1

2 if k ≥ 1.

(2.5)

(2.6)

Improved convergence rates for Lasserre-type hierarchies

7

For any r ∈ N the Chebyshev polynomials Tk (k ≤ r) form a basis of the space of univariate polynomials
with degree at most r. One may write the Chebyshev polynomials in the standard monomial basis using the
relations:

k(cid:88)
k−1(cid:88)

i=0

(cid:98) k

2 (cid:99)(cid:88)
2 (cid:99)(cid:88)

k
2
(cid:98) k−1

m=0

Tk(x) =

Uk−1(x) =

t(k)
i xi =

u(k)
i xi =

i=0

m=0

(2x)k−2m,

(−1)m (k − m − 1)!
m!(k − 2m)!
(−1)m (k − m − 1)!
m!(k − 1 − 2m)!

(2x)k−1−2m,

k > 0

k > 1.

See, e.g.
[2, Chap. 22]. From this, one may derive a bound on the largest coeﬃcient in absolute value
appearing in the above expansions of Tk(x) and Uk−1(x). A proof for the following result will be given in
the Appendix.

Lemma 2.1 For any ﬁxed integer k > 1, one has:

| ≤ max
0≤i≤k

|t(k)

(cid:0)4k − 5 − √

| = 2k−1−2ψ(k) k(k − ψ(k) − 1)!
8k2 − 7(cid:1)(cid:7) for k ≥ 4. Moreover, the right hand side of
ψ(k)!(k − 2ψ(k))!

(2.7)

i

|u(k)

i

max

0≤i≤k−1

where ψ(k) = 0 for k ≤ 4 and ψ(k) =(cid:6) 1
(cid:17)−1

with the function(cid:81)n

(cid:16)
π(cid:112)1 − x2

8

i=1

(2.7) increases monotonically with increasing k.

In the multivariate case we use the following notation. We let dµ(x) denote the Lebesgue measure on [−1, 1]n

i

as density function:

(cid:18)

(cid:113)

(cid:19)−1

dx

(2.8)

dµ(x) =

π

1 − x2

i

and we consider the following inner product for two integrable functions f, g on the box [−1, 1]n:

n(cid:89)
(cid:90)

i=1

n(cid:89)

i=1

(cid:104)f, g(cid:105) =

f (x)g(x)dµ(x)

[−1,1]n

Tα(x) =

Tαi(xi) for x ∈ Rn.
(cid:19)|Supp(α)|

(cid:18) 1

(cid:104)Tα, Tβ(cid:105) =

2

(which coincides with (2.5) in the univariate case n = 1). For α ∈ Nn, we deﬁne the multivariate Chebyshev
polynomial

The multivariate Chebyshev polynomials satisfy the following orthogonality relationships:

δα,β

(2.9)

and, for any r ∈ N, the set of Chebyshev polynomials {Tα(x) : |α| ≤ r} is a basis of the space of n-variate
polynomials of degree at most r.

2.2 Jackson kernels

A classical problem in approximation theory is to ﬁnd a best (uniform) approximation of a given continuous
function f : [−1, 1] → R by a polynomial of given maximum degree r. Following [20], a possible approach is

Improved convergence rates for Lasserre-type hierarchies

8

to take the convolution f (r)

KPM of f with a kernel function of the form

Kr(x, y) =

√

π

1 − x2π(cid:112)1 − y2

1

gr
0T0(x)T0(y) + 2

(cid:32)

(cid:33)

gr
kTk(x)Tk(y)

,

r(cid:88)

k=1

where r ∈ N and the coeﬃcients gr

k are selected so that the following properties hold:

(1) The kernel is positive: Kr(x, y) > 0 for all x, y ∈ [−1, 1].

(2) The kernel is normalized: gr

0 = 1.

(3) The second coeﬃcients gr

The function f (r)

KPM is then deﬁned by

1 tend to 1 as r → ∞.
(cid:112)

(cid:90) 1

1 − y2Kr(x, y)f (y)dy.

f (r)
KPM(x) =

0 = 1, the kernel is normalized: (cid:82) 1

−1

π

(cid:82) 1

KPM(x)dx =(cid:82) 1

1 − x2, and we have:
As the ﬁrst coeﬃcient is gr
−1 f (x)dx. The positivity of the kernel Kr implies that the integral operator f (cid:55)→ f (r)
−1 f (r)
KPM
is a positive linear operator, i.e., a linear operator that maps the set of nonnegative integrable functions on
[−1, 1] into itself. Thus the general (Korovkin) convergence theory of positive linear operators applies and
one may conclude the uniform convergence result

−1 Kr(x, y)dy = T0(x)/π

√

(2.10)

r→∞(cid:107)f − f (r)

KP M(cid:107)∞ = 0
KP M(cid:107)∞ = max−1+≤x≤1− |f (x)−f (r)

lim

for any  > 0, where (cid:107)f−f (r)
of [−1, 1] because of the denominator in the kernel Kr.)

KPM(x)|. (One needs to restrict to subintervals

In what follows we select the following parameters gr

k for k = 1, . . . , r, which deﬁne the so-called Jackson

kernel, again denoted by Kr(x, y):

k = 1
gr
= 1

r+2 ((r + 2 − k) cos(kθr) + sin(kθr)
r+2 ((r + 2 − k)Tk(cos θr) + Uk−1(cos θr) cos θr),

cos θr)

sin θr

(2.11)

where we set

k is the one minimizing the quantity (cid:82)

θr :=

r + 2

π

.

This choice of the parameters gr
ensures that the corresponding Jackson kernel is maximally peaked at x = y (see [20, §II.C.3]).

[−1,1]2 Kr(x, y)(x − y)2dxdy, which

One may show that the Jackson kernel Kr(x, y) is indeed positive on [−1, 1]2; see [20, §II.C.2]. Moreover
1 = cos(θr) = cos(π/(r + 2)) → 1 if r → ∞ as required. This is in fact true
k| ≤ 1 for all k, since |Tk(cos θr)| ≤ 1
k, showing that

0 = 1 and, for k = 1, we have gr
gr
for all k, as will follow from Lemma 2.2 below. Note that one has |gr
and |Uk−1(cos θr)| ≤ k. For later use, we now give an estimate on the Jackson coeﬃcients gr
1 − gr

k is in the order O(1/r2).

Lemma 2.2 Let d ≥ 1 and r ≥ d be given integers, and set θr = π
only on d) such that the following inequalities hold:

r+2 . There exists a constant Cd (depending

|1 − gr

k| ≤ Cd(1 − cos θr) ≤ Cdπ2
2(r + 2)2

for all 0 ≤ k ≤ d.

Improved convergence rates for Lasserre-type hierarchies

9

For the constant Cd we may take: Cd = d2(1 + 2cd), where

cd = 2d−1−2ψ(d) d(d − ψ(d) − 1)!
ψ(d)!(d − 2ψ(d))!

and ψ(d) =

Proof. Deﬁne the polynomial

0
(cid:6) 1

8

(cid:0)4d − 5 − √

8d2 − 7(cid:1)(cid:7)

for d ≤ 4
for d ≥ 4.

(2.12)

Pk(x) = 1 − r + 2 − k

Tk(x) − 1
r + 2
with degree k. Then, in view of relation (2.11), we have: 1− gr
k = Pk(cos θr). Recall from relation (2.4) that
Tk(1) = 1 and Uk−1(1) = k for any k ∈ N. This implies that Pk(1) = 0 and thus we can factor Pk(x) as
i=0 pixi, then it

Pk(x) = (1 − x)Qk(x) for some polynomial Qk(x) with degree k − 1. If we write Pk(x) =(cid:80)k
follows that Qk(x) =(cid:80)k−1

i=0 qixi, where the scalars qi are given by

xUk−1(x),

r + 2

i(cid:88)

qi =

for i = 0, 1, . . . , k − 1.

pj

(2.13)

It now suﬃces to observe that for any 0 ≤ i ≤ k and k ≤ d, the pi’s are bounded by a constant depending
i xi and

only on d, which will imply that the same holds for the scalars qi. For this, set Tk(x) = (cid:80)k
Uk−1(x) =(cid:80)k−1

i xi. Then the coeﬃcients pi of Pk(x) can be expressed as

i=0 t(k)

i=0 u(k)

j=0

p0 = 1 − r + 2 − k

r + 2

t(k)
0 , pi =

r + 2 − k
r + 2

i − u(k)
i−1
t(k)
r + 2

(1 ≤ i ≤ k).

For all 0 ≤ k ≤ d the coeﬃcients of the Chebyshev polynomials Tk, Uk−1 can be bounded by an absolute
constant depending only on d. Namely, by Lemma 2.1, |t(k)
| ≤ cd for all 0 ≤ i ≤ k and k ≤ d,
where cd is as deﬁned in (2.12). As k ≤ d ≤ r, we have r + 2 − k ≤ r + 2 and thus |pi| ≤ 1 + 2cd
for all 0 ≤ i ≤ k ≤ d. Moreover, using (2.13), |qi| ≤ d(cd + 1) for all 0 ≤ i ≤ k − 1. Putting things
together we can now derive: 1 − gr
i=0 qi(cos θr)i, so that
k| ≤ (1− cos θr)Cd, after setting Cd = d2(1 + 2cd).
2 for all x ∈ [0, π], we obtain the desired inequality from

k = (1 − cos θr)Qk(cos θr), where Qk(cos θr) =(cid:80)k−1

|Qk(cos θr)| ≤(cid:80)k−1

i=0 |qi| ≤ d2(1 + 2cd). This implies |1− gr

Finally, combining with the fact that 1 − cos x ≤ x2
the lemma statement.

|,|u(k)

2

i

i

2.3 Jackson kernel approximation of the Dirac delta function

If one approximates the Dirac delta function δx∗ at a given point x∗ ∈ [−1, 1] by taking its convolution with
the Jackson kernel Kr(x, y), then the result is the function:

KPM(x − x∗) =
δ(r)

√

1
1 − x2

π

1 + 2

(cid:32)

(cid:33)
kTk(x)Tk(x∗)
gr

.

r(cid:88)

k=1

See [20, eq. (72)]. As mentioned in [20, eq. (75)–(76)], the function δ(r)
to the Gaussian density:

KPM is in fact a good approximation

KPM(x − x∗) ≈
δ(r)

1√
2πσ2

exp

− (x − x∗)2

2σ2

with σ2 ≈

1 − x∗2 +

3x∗2 − 2
r + 1

.

(2.14)

(cid:18) π

(cid:19)2(cid:20)

r + 1

(cid:18)

(cid:19)

(cid:21)

Improved convergence rates for Lasserre-type hierarchies

10

Figure 2: The Jackson kernel approximation δ(r)
corresponding scatterplots show the values of the Gaussian density function in (2.14) with x∗ = 0.

KPM to the Dirac delta at x∗ = 0 for r = 8, 16, 32, 64. The

(Recall that the Dirac delta measure may be deﬁned as a limit of the Gaussian measure when σ ↓ 0.) This
approximation is illustrated in Figure 2 for several values of r.

KPM(x − x∗)dx = (cid:82) 1

By construction, the function δ(r)

−1 δ(r)
Lebesgue measure. It is convenient to consider the following univariate polynomial:

KPM(x − x∗) is nonnegative over [−1, 1] and we have the normalization:
−1 δx∗ (x)dx = 1. Hence, it is a probability density function on [−1, 1] for the

(cid:82) 1

r(cid:88)

hr(x) = 1 + 2

kTk(x)Tk(x∗),
gr

(2.15)

KPM(x − x∗) =

so that δ(r)
convergence analysis of the new bounds f (r).

√

π

1
1−x2 hr(x). The following facts follow directly, which we will use below for the

k=1

(cid:82) 1

1−x2 = 1. In other words, hr is a probability density function for the measure(cid:0)π

Lemma 2.3 For any r ∈ N the polynomial hr
−1 hr(x)
on [−1, 1].

is nonnegative over [−1, 1] and
dx

1 − x2(cid:1)−1

from (2.15)

√
dx

√

π

−1−0.500.50123456789xImproved convergence rates for Lasserre-type hierarchies

11

3 Convergence analysis

In this section we analyze the convergence rate of the new bounds f (r) and we show the result from Theorem
1.5. We will ﬁrst consider the univariate case in Section 3.1 (see Theorem 3.3) and then the general multi-
variate case in Section 3.2 (see Theorem 3.6). As we will see, the polynomial hr arising from the Jackson
kernel approximation of the Dirac delta function, introduced above in relation (2.15), will play a key role in
the convergence analysis.

3.1 The univariate case

We consider a univariate polynomial f and let x∗ be a global minimizer of f in [−1, 1]. As observed in
√
dx
Lemma 2.3 the polynomial hr from (2.15) is a density function for the measure
1−x2 . The key observation
now is that the polynomial hr admits a Schm¨udgen-type representation, of the form σ(x)+σ1(x)(1−x2) with
σ0, σ1 sums-of-squares polynomials, since it is non-negative over [−1, 1]. This fact will allow us to use the
polynomial hr to get feasible solutions for the program deﬁning the bound f (r). It follows from the following
classical result (see e.g. [11, Thm 3.23 (i)]), that characterizes univariate polynomials that are nonnegative
on [−1, 1]. (Note that this is a strengthening of Schm¨udgen’s theorem (Theorem 1.3) in the univariate case.)

π

Theorem 3.1 (Fekete, Markov-Luk`acz) Let p(x) be a univariate polynomial of degree m. Then p(x) is
nonnegative on the interval [−1, 1] if and only if it has the following representation:

for some sum-of-squares polynomials σ0 of degree 2(cid:100)m/2(cid:101) and σ1 of degree 2(cid:100)m/2(cid:101) − 2.

p(x) = σ0(x) + (1 − x2)σ1(x)

We start with the following technical lemma.

Lemma 3.2 Let f be a polynomial of degree d written in the Chebyshev basis as f =(cid:80)d

k=0 fkTk, let x∗ be
a global minimizer of f in [−1, 1] and let hr be the polynomial from (2.15). For any integer r ≥ d we have:

Combining with f (x∗) =(cid:80)d
k=0 fkTk(x∗) gives:
(cid:90) 1

f (x)hr(x)

−1

f (x)hr(x)

−1

√

dx
1 − x2

π

d(cid:88)

k=0

√

dx
1 − x2

π

=

fkTi(x∗)gr
k.

− f (x∗) =

d(cid:88)

k=1

fkTk(x∗)(gr

k − 1).

(3.1)

(3.2)

where Cf = ((cid:80)d
Proof. As f =(cid:80)d

to obtain:

(cid:90) 1

−1

(cid:90) 1

f (x)hr(x)

√

dx
1 − x2

π

− f (x∗) ≤ Cf

(r + 2)2 ,

k=1 |fk|) Cdπ2

2

and Cd is the constant from Lemma 2.2.

k=0 fkTk and hr = 1 + 2(cid:80)r

k=1 gr

kTk(x∗)Tk, we use the orthogonality relationships (2.6)

Improved convergence rates for Lasserre-type hierarchies

12

Now we use the upper bound on gr

k − 1 from Lemma 2.2 and the bound |Tk(x∗)| ≤ 1 to conclude the proof.

2

We can now conclude the convergence analysis of the bounds f (r) in the univariate case.

k=0 fkTk be a polynomial of degree d. For any integer r ≥ d we have:

Theorem 3.3 Let f =(cid:80)d
where Cf = ((cid:80)d

k=1 |fk|) Cdπ2

2

f (r) − fmin ≤ Cf

(r + 1)2 ,

and Cd is the constant from Lemma 2.2.

Proof. Using the degree bounds in Theorem 3.1 for the sum-of-squares polynomials entering the decom-

ting Cf = ((cid:80)d

position of the polynomial hr, we can conclude that for r even, hr is feasible for the program deﬁning
the parameter f (r) and for r odd, hr is feasible for the program deﬁning the parameter f (r+1). Set-
(r+2)2 for r even, and
2

k=1 |fk|) Cdπ2
(r+1)2 for odd r. The result of the theorem now follows.

and using Lemma 3.2, this implies: f (r) − fmin ≤ Cf

f (r) − fmin ≤ Cf

2

3.2 The multivariate case

We consider now a multivariate polynomial f and we let x∗ = (x∗
minimizer of f on [−1, 1]n, i.e. f (x∗) = fmin.

1, . . . , x∗

n) ∈ [−1, 1]n denote a global

In order to obtain a feasible solution to the program deﬁning the parameter f (r) we will consider products
of the univariate polynomials hr from (2.15). Namely, given integers r1, . . . , rn ∈ N we deﬁne the n-tuple
r = (r1, . . . , rn) and the n-variate polynomial:

Hr(x1, . . . , xn) =

hri(xi).

(3.3)

We group in the next lemma some properties of the polynomial Hr.

i=1

Lemma 3.4 The polynomial Hr satisﬁes the following properties:

(i) Hr is non-negative on [−1, 1]n.

(ii) (cid:82)
(iii) Hr has a Schm¨udgen-type representation of the form: Hr(x) =(cid:80)
σI is a sum-of-squares polynomial of degree at most 2(cid:80)n

[−1,1]n Hr(x)dµ(x) = 1, where dµ is the measure from (1.7).

i=1(cid:100)ri/2(cid:101) − 2|I|.

I⊆[n] σI (x)(cid:81)

i∈I (1 − x2

i ), where each

Proof.

(i) and (ii) follow directly from the corresponding properties of the univariate polynomials hri, and

n(cid:89)

Improved convergence rates for Lasserre-type hierarchies

(iii) follows using Theorem 3.1 applied to the polynomials hri.

The next lemma is the analog of Lemma 3.2 for the multivariate case.

13

2

polynomials as f = (cid:80)

Lemma 3.5 Let f be a multivariate polynomial of degree d, written in the basis of multivariate Chebyshev
α∈Nn:|α|≤d fαTα, and let x∗ be a global minimizer of f in [−1, 1]n. Consider r =

(r1, . . . , rn), where each ri is an integer satisfying ri ≥ d, and the polynomial Hr from (3.3). We have:

f (x)Hr(x)dµ(x) − f (x∗) ≤ Cf

[−1,1]n

α:|α|≤d |fα|) Cdπ2

2

and Cd is the constant from Lemma 2.2.

n(cid:88)

i=1

1

(ri + 2)2 ,

where Cf = ((cid:80)
Proof. As f =(cid:80)

α:|α|≤d fαTα and Hr =(cid:81)n

i=1 h(ri)(xi) =(cid:81)n
(cid:88)

i=1(1 + 2(cid:80)ri
n(cid:89)

fαTα(x∗)

gri
αi

.

α:|α|≤d

orthogonality relationships (2.9) among the multivariate Chebyshev polynomials to derive:

ki=1 gri
ki

Tki(xi)), we can use the

Combining with f (x∗) =(cid:80)

(cid:90)
Using the identity: (cid:81)n
− 1)| ≤(cid:80)n
|(cid:81)n

i=1(gri
αi

[−1,1]n

f (x)Hr(x)dµ(x) =

[−1,1]n
α:|α|≤d fαTα(x∗) this gives:

f (x)Hr(x)dµ(x) − f (x∗) =

− 1) = (cid:80)n

(cid:88)
αj − 1)(cid:81)n

α:|α|≤d

i=1

n(cid:89)

i=1

− 1).

gri
αi

fαTα(x∗)(

i=1(gri
αi
j=1 |grj

| ≤ 1, we get:
αj|
αj − 1|. Now use |Tα(x∗)| ≤ 1 and the bound from Lemma 2.2 for each |1 − grj

and the fact that |grk

k=j+1 grk
αk

j=1(grj

αk

to conclude the proof.

2

(cid:90)

(cid:90)

We can now show our main result, which implies Theorem 1.5.

α:|α|≤d fαTα be a n-variate polynomial of degree d. For any integer r ≥ n(d + 2),

Theorem 3.6 Let f =(cid:80)
where Cf = ((cid:80)

we have:

α:|α|≤d |fα|) Cdπ2

2

f (r) − fmin ≤ Cf n3

(r + 1)2 ,

and Cd is the constant from Lemma 2.2.

the condition r ≥ n(d + 2) implies s ≥ d and thus ri ≥ d for all i. Moreover, we have: 2(cid:80)n

Proof. Write r − n = sn + n0, where s, n0 ∈ N and 0 ≤ n0 < n, and deﬁne the n-tuple r = (r1, . . . , rn),
setting ri = s + 1 for 1 ≤ i ≤ n0 and ri = s for n0 + 1 ≤ i ≤ n, so that r − n = r1 + . . . + rn. Note that
i=1(cid:100)ri/2(cid:101) =
2n0(cid:100)(s + 1)/2(cid:101) + 2(n − n0)(cid:100)s/2(cid:101), which is equal to r − n + n0 for even s and to r − n0 for odd s and thus
always at most r. Hence the polynomial Hr from (3.3) has degree at most r. By Lemma 3.4 (ii),(iii), it

Improved convergence rates for Lasserre-type hierarchies

14

follows that the polynomial Hr is feasible for the program deﬁning the parameter f (r). By Lemma 3.5 this
implies that

f (r) − fmin ≤

f (x)Hr(x)dµ(x) − f (x∗) ≤ Cf

(cid:90)

[−1,1]n

1

(ri+2)2 = n0

(s+3)2 + n−n0

(s+2)2 ≤ n

(s+2)2 =

(r+n−n0)2 ≤ n3

n3

2

n(cid:88)

1

(ri + 2)2 .
(r+1)2 , since n0 ≤ n − 1.

i=1

Finally,(cid:80)n

i=1

4 Computing the parameter f (r) as a generalized eigenvalue prob-

lem

As the parameter f (r) is deﬁned in terms of sum-of-squares polynomials (cf. Deﬁnition 1.4), it can be
computed by means of a semideﬁnite program. As we now observe, as the program (1.8) has only one aﬃne
constraint, f (r) can in fact be computed in a cheaper way as a generalized eigenvalue problem.

Using the inner product from (2.5), the parameter f (r) can be rewritten as

f (r) = min
h∈R[x]

(cid:104)f, h(cid:105) such that

(cid:104)h, T0(cid:105) = 1, h(x) =(cid:80)

I⊆[n] σI (x)(cid:81)

σI ∈ Σ[x], deg(σI ) ≤ r − 2|I| ∀I ⊆ [n].

i∈I (1 − x2
i ),

(4.1)

denote the set of sequences β ∈ Nn with |β| ≤ (cid:98) r−2|I|

that σI is a sum-of-squares polynomial, i.e., of the form (cid:80)

For convenience we use below the following notation. For a set I ⊆ [n] and an integer r ∈ N we let ΛI
(cid:99). As is well known one can express the condition
k pk(x)2 for some pk ∈ R[x], as a semideﬁnite
program. More precisely, using the Chebyshev basis to express the polynomials pk, we obtain that σI is a
sum-of-squares polynomial if and only if there exists a matrix variable M I indexed by ΛI
r, which is positive
semideﬁnite and satisﬁes:

2

r

σI =

M I

β,γTβTγ.

(4.2)

(cid:88)

β,γ∈ΛI

r

For each I ⊆ [n], we introduce the following matrices AI and BI , which are also indexed by the set ΛI
for β, γ ∈ ΛI

r, with entries:

r and,

(cid:89)
(cid:89)
β,γ = (cid:104)f, TβTγ
AI
i∈I
β,γ = (cid:104)T0, TβTγ
BI

i∈I

(1 − x2
i )(cid:105),
i )(cid:105).
(1 − x2

(4.3)

We will indicate in the Appendix how to compute the matrices AI and BI .

We can now reformulate the parameter f (r) as follows.

Lemma 4.1 Let AI and BI be the matrices deﬁned as in (4.3) for each I ⊆ [n]. Then the parameter f (r)
can be reformulated using the following semideﬁnite program in the matrix variables M I (I ⊆ [n]):

Tr (AI M I ) such that M I (cid:23) 0 ∀I ⊆ [n],

Tr (BI M I ) = 1.

(4.4)

(cid:88)

I⊆[n]

f (r) = min

M I :I⊆[n]

(cid:88)

I⊆[n]

Proof.

Using relation (4.2) we can express the polynomial variable h in (4.1) in terms of the matrix

Improved convergence rates for Lasserre-type hierarchies

15

variables M I and obtain:

(cid:88)

I⊆[n]

h =

(cid:88)

β,γ∈ΛI

r

(cid:89)

i∈I

(1 − xi)2.

M I

β,γTβTγ

First this permits to reformulate the objective function (cid:104)f, h(cid:105) in terms of the matrix variables M I in the
following way:

Second we can reformulate the constraint (cid:104)T0, h(cid:105) = 1 using:

(cid:104)f, h(cid:105) =(cid:80)
=(cid:80)
=(cid:80)
(cid:104)T0, h(cid:105) =(cid:80)
=(cid:80)
=(cid:80)

I

I

(cid:80)
(cid:80)
(cid:80)
(cid:80)

β,γ M I
β,γ M I

β,γ(cid:104)f, TβTγ
β,γAI

β,γ

I
I Tr (AI M I ).

β,γ M I
β,γ M I

β,γ(cid:104)T0, TβTγ
β,γBI

β,γ

I
I Tr (BI M I ).

(cid:81)
i∈I (1 − x2
i )(cid:105)

(cid:81)
i∈I (1 − x2
i )(cid:105)

From this follows that the program (4.1) is indeed equivalent to the program (4.4).

2

The program (4.4) is a semideﬁnite program with only one constraint. Hence, as we show next, it is

equivalent to a generalized eigenvalue problem.

Theorem 4.2 For I ⊆ [n] let AI and BI be the matrices from (4.3) and deﬁne the parameter:

λ(I) = max(cid:8)λ | AI − λBI (cid:23) 0(cid:9) = min(cid:8)λ | AI x = λBI x for some non-zero vector x(cid:9) .

One then has f (r) = minI⊆[n] λ(I).

Proof. The dual semideﬁnite program of the program (4.4) is given by:

sup(cid:8)λ | AI − λBI (cid:23) 0 ∀I ⊆ [n](cid:9) .

(4.5)

We ﬁrst show that the primal problem (4.4) is strictly feasible. To see this it suﬃces to show that Tr (BI ) > 0,
since then one may set MI equal to a suitable multiple of the identity matrix and thus one gets a strictly
feasible solution to (4.4). Indeed, the matrix BI is positive semideﬁnite since, for any scalars gβ,

(cid:88)

(cid:90)

(cid:88)

gβxβ)2(cid:89)

(1 − x2

i )dµ(x) ≥ 0.

i∈I
Thus Tr (BI ) ≥ 0 and moreover Tr (BI ) > 0 since BI is nonzero.

β,γ

β

gβgγBI

βγ =

(

[−1,1]n

Moreover, the dual problem (4.5) is also feasible, since λ = fmin is a feasible solution. This follows from
the fact that the polynomial f − fmin is nonnegative over [−1, 1]n, which implies that the matrix AI − fminBI
is positive semideﬁnite. Indeed, using the same argument as above for showing that BI (cid:23) 0, we have

(cid:88)

β,γ

(cid:90)

[−1,1]n

gβgγ(AI − fminBI )β,γ =

(f (x) − fmin)g(x)2dµ(x) ≥ 0.

Since the primal problem is strictly feasible and the dual problem is feasible, there is no duality gap and the
2
dual problem attains its supremum. The result follows.

Improved convergence rates for Lasserre-type hierarchies

16

5 Numerical examples

We examine the polynomial test functions which were also used in [4] and [3], and are described in the
appendix to this paper.

The numerical examples given here only serve to illustrate the observed convergence behavior of the
sequence f (r) as compared to the theoretical convergence rate. In particular, the computational demands
for computing f (r) for large r are such that it cannot compete in practice with the known iterative methods
referenced in the introduction.

For the polynomial test functions we list (Table 1) the values of f (r) for even r up to r = 48, obtained by
solving the generalized eigenvalue problem in Theorem 4.2 using the eig function of Matlab. Recall that for
step r of the hierarchy the polynomial density function h is of Schm¨udgen type and has degree r.

For the examples listed the computational time is negligible, and therefore not listed; recall that the
computation of f (r) for even n requires the solution of 2n generalised eigenvalue problems indexed by subsets

I ⊂ [n], where the order of the matrices equals(cid:0)n+(cid:98)r/2−|I|(cid:99)

(cid:1); cf. Theorem 4.2.

n

Table 1: The upper bounds f (r) for the test functions.

Booth

Matyas Motzkin Three-Hump

Styblinski-Tang
n = 2
n = 3

Rosenbrock

n = 2

n = 3

318.0367
245.9925
187.2490
142.8774
111.0703
88.3594
71.5983
59.0816
49.5002

-40.1625
-47.6759
-55.4061
-64.0426
-70.2894
-76.0311
-80.5870
-85.4149
-88.5665

145.3633
118.0554
91.6631
71.1906
57.3843
47.6354
40.3097
34.5306
28.9754
24.6380
21.3151
18.7250
16.6595
14.9582
13.5114
12.2479
11.0441
10.0214
9.1504
8.4017
7.7490
7.1710

4.1844
3.9308
3.8589
3.8076
3.0414
2.4828
2.0637
1.7417
1.4891
1.2874
1.1239
0.9896
0.8779
0.7840
0.7044
0.6363
0.5776
0.5266
0.4821
0.4430
0.4084
0.3778

1.1002
0.8764
0.8306
0.8098
0.7309
0.6949
0.5706
0.5221
0.4825
0.4081
0.3830
0.3457
0.3016
0.2866
0.2590
0.2306
0.2215
0.2005
0.1815
0.1754
0.1597
0.1462

24.6561
15.5022
9.9919
6.5364
4.5538
3.3453
2.5814
2.0755
1.7242
1.4716
1.2830
1.1375
1.0216
0.9263
0.8456
0.7752
0.7129
0.6571
0.6070
0.5622
0.5220
0.4860

-27.4061
-34.5465
-40.0362
-47.4208
-51.2011
-56.0904
-58.8010
-61.8751
-63.9161
-65.5717
-67.2790
-68.2078
-69.5141
-70.3399
-71.0821
-71.8284
-72.2581
-72.8953
-73.3011
-73.6811
-74.0761
-74.3070

157.7604
96.8502
68.4239
51.7554
39.0613
30.3855
24.0043
19.5646
16.2071
13.6595
11.6835
10.1194
8.8667
7.8468
7.0070
6.3083
5.7198
5.2215
4.7941
4.4266
4.1070
3.8283

r

6
8
10
12
14
16
18
20
22
24
26
28
30
32
34
36
38
40
42
44
46
48

We note that the observed rate of convergence seems in line with the O(1/r2) error bound.

As a second numerical experiment, we compare (see Table 2) the upper bound f (r) to the upper bound
deﬁned in (1.2). Recall that the bound f (r)
corresponds to using sum-of-squares density functions of
K

f (r)
K

Improved convergence rates for Lasserre-type hierarchies

17

solving a single generalized eigenvalue problem with matrices of order(cid:0)n+(cid:98)r/2−|I|(cid:99)

degree at most r and the Lebesgue measure. As shown in [4], the computation of f (r)
K

(cid:1). Thus the computation

may be done by

of f (r)
K

is signiﬁcantly cheaper than that of f (r).

n

Table 2: Comparison of the upper bounds f (r) and f (r)
K
Hump Camel and Motzkin Functions.

for Booth, Matyas, Three–

Booth Function

Matyas Function

Three–Hump Camel

Function

Motzkin Polynomial

f (r)
K

118.383

97.6473

69.8174

63.5454

47.0467

41.6727

34.2140

28.7248

25.6050

21.1869

19.5588

16.5854

15.2815

13.4626

12.2075

11.0959

9.9938

9.2373

f (r)

145.3633

118.0554

91.6631

71.1906

57.3843

47.6354

40.3097

34.5306

28.9754

24.6380

21.3151

18.7250

16.6595

14.9582

13.5114

12.2479

11.0441

10.0214

f (r)
K
4.2817

3.8942

3.6894

2.9956

2.5469

2.0430

1.8335

1.4784

1.3764

1.1178

1.0686

0.8742

0.8524

0.7020

0.6952

0.5760

0.5760

0.4815

f (r)
4.1844

3.9308

3.8589

3.8076

3.0414

2.4828

2.0637

1.7417

1.4891

1.2874

1.1239

0.9896

0.8779

0.7840

0.7044

0.6363

0.5776

0.5266

f (r)
K

29.0005

9.5806

9.5806

4.4398

4.4398

2.5503

2.5503

1.7127

1.7127

1.2775

1.2775

1.0185

1.0185

0.8434

0.8434

0.7113

0.7113

0.6064

f (r)

24.6561

15.5022

9.9919

6.5364

4.5538

3.3453

2.5814

2.0755

1.7242

1.4716

1.2830

1.1375

1.0216

0.9263

0.8456

0.7752

0.7129

0.6571

f (r)
K
1.0614

0.8294

0.8010

0.8010

0.7088

0.5655

0.5655

0.5078

0.4060

0.4060

0.3759

0.3004

0.3004

0.2819

0.2300

0.2300

0.2185

0.1817

f (r)
1.1002

0.8764

0.8306

0.8098

0.7309

0.6949

0.5706

0.5221

0.4825

0.4081

0.3830

0.3457

0.3016

0.2866

0.2590

0.2306

0.2215

0.2005

r

6

8

10

12

14

16

18

20

22

24

26

28

30

32

34

36

38

40

It is interesting to note that, in almost all cases, f (r) > f (r)
K

. Thus even though the measure dµ(x)
and the Schm¨udgen-type densities are useful in getting improved error bounds, they mostly do not lead to
improved upper bounds for these examples. This also suggests that it might be possible to improve the error
result f (r)
K

r) in [4], at least for the case K = [−1, 1]n.

√
− fmin = O(1/

Finally, it is shown in [4] that one may obtain feasible points corresponding to bounds like f (r) through
sampling from the probability distribution deﬁned by the optimal density function. In particular, one may
use the method of conditional distributions (see e.g., [12, Section 8.5.1]). For K = [0, 1]n, the procedure is
described in detail in [4, Section 3].

References

[1] D.P. Bertsekas, Constrained Optimization and Lagrange Multiplier Methods, Athena Scientiﬁc, Belmont,

MA (1996)

[2] M. Abramowitz, I.A. Stegun (eds.). Handbook of Mathematical Functions with formulas, graphs, and

mathematical tables, 10th ed., Applied Mathematics Series 55, New York (1972)

Improved convergence rates for Lasserre-type hierarchies

18

[3] E. de Klerk, J.B. Lasserre, M. Laurent, Z. Sun. Bound-constrained polynomial optimization using only

elementary calculations, arxiv: 1507.04404 (2015)

[4] E. de Klerk, M. Laurent, Z. Sun. Convergence analysis for Lasserre’s measure-based hierarchy of upper

bounds for polynomial optimization, arXiv: 1411.6867 (2014)

[5] R. Fletcher. Practical Methods of Optimization, 2nd ed., John Wiley & Sons, Inc., New York (1987)

[6] P.E. Gill, W. Murray, M.H. Wright. Practical Optimization, Academic Press, New York (1981)

[7] W.W. Hager and H. Zhang. A new active set algorithm for box constrained optimization. SIAM Journal

on Optimization 17(2), 526–557 (2006)

[8] P. Hungerl¨ander and F. Rendl. A feasible active set method for strictly convex quadratic problems with

simple bounds. SIAM Journal on Optimization 25(3), 1633–1659 (2015).

[9] J.B. Lasserre. Global optimization with polynomials and the problem of moments, SIAM Journal on

Optimization 11(3), 796–817 (2001)

[10] J.B. Lasserre. A new look at nonnegativity on closed sets and polynomial optimization. SIAM Journal

on Optimization 21(3), 864–885 (2011)

[11] M. Laurent. Sums of squares, moment matrices and optimization over polynomials, in Emerging Ap-
plications of Algebraic Geometry, Vol. 149 of IMA Volumes in Mathematics and its Applications, M.
Putinar and S. Sullivant (eds.), Springer, pages 157-270 (2009)

[12] A.M. Law. Simulation Modeling and Analysis, 4th ed., Mc Graw-Hill (2007)

[13] M. Marshall. Positive Polynomials and Sums of Squares, Mathematical Surveys and Monographs 146,

American Mathematical Society (2008)

[14] L. P´al. Global optimization algorithms for bound constrained problems. PhD thesis, Univer-
sity of Szeged (2010) Available at http://www2.sci.u-szeged.hu/fokozatok/PDF/Pal_Laszlo/
Diszertacio_PalLaszlo.pdf

[15] M.-J. Park, S.-P. Hong. Rank of Handelman hierarchy for Max-Cut. Operations Research Letters 39(5),

323–328 (2011)

[16] M.-J. Park, S.-P. Hong. Handelman rank of zero-diagonal quadratic programs over a hypercube and its

applications. Journal of Global Optimization 56(2), 727–736 (2013)

[17] A. Prestel, C.N. Delzell. Positive Polynomials - From Hilbert’s 17th Problem to Real Algebra, Springer

Monographs in Mathematics, Springer (2001)

[18] T.J. Rivlin. Chebyshev polynomials: From Approximation Theory to Algebra and Number Theory, 2nd

ed., Pure and Applied Mathematics, John Wiley & Sons, New York (1990)

[19] K. Schm¨udgen. The K-moment problem for compact semi-algebraic sets. Mathematische Annalen 289,

203–206 (1991)

[20] A. Weisse, G. Wellein, A. Alvermann, H. Fehske. The kernel polynomial method, Rev. Mod. Phys. 78,

275–306 (2006). Preprint version: http://arxiv.org/abs/cond-mat/0504627

Improved convergence rates for Lasserre-type hierarchies

19

Appendix

A. Proof of Lemma 2.1

We give here a proof of lemma 2.1, which we repeat for convenience.

Lemma 2.1 For any ﬁxed integer k > 1, one has:

where ψ(k) = 0 for k ≤ 4 and ψ(k) =(cid:6) 1

0≤i≤k−1

max

i

8

|u(k)

| ≤ max
0≤i≤k

|t(k)

(cid:0)4k − 5 − √

| = 2k−1−2ψ(k) k(k − ψ(k) − 1)!
8k2 − 7(cid:1)(cid:7) for k ≥ 4. Moreover, the right hand side of
ψ(k)!(k − 2ψ(k))!

(2.7)

i

the equation increases monotonically with increasing k.

Proof. We recall the representation of the Chebyshev polynomials in the monomial basis:

k(cid:88)
k−1(cid:88)

i=0

(cid:98) k

2 (cid:99)(cid:88)
2 (cid:99)(cid:88)

k
2
(cid:98) k−1

m=0

Tk(x) =

Uk−1(x) =

t(k)
i xi =

u(k)
i xi =

i=0

m=0

(2x)k−2m,

(−1)m (k − m − 1)!
m!(k − 2m)!
(−1)m (k − m − 1)!
m!(k − 1 − 2m)!

(2x)k−1−2m,

k > 0

k > 1.

(cid:22) k
(cid:22) k − 1

2

(cid:23)
(cid:23)

,

.

2

So, concretely, the coeﬃcients are given by

t(k)

k−2m = (−1)m · 2k−1−2m · k(k − m − 1)!
m!(k − 2m)!
,
(k − m − 1)!
k−1−2m = (−1)m · 2k−1−2m ·
u(k)
m!(k − 1 − 2m)!

,

k > 0, 0 ≤ m ≤

k > 1, 0 ≤ m ≤

It follows directly that t(k)
implies the inequality on the left hand side of (2.7).

k−2m = k

k−2m u(k)

k−1−2m and thus |t(k)

k−2m| > |u(k)

k−1−2m| for m < k

2 and all k > 1 which

Now we show that the value of max0≤m≤(cid:98) k

2(cid:99) |t(k)

k−2m| is attained for m = ψ(k). For this we examine the

quotient

(k − 2m)(k − 2m − 1)
4(m + 1)(k − m − 1)

|t(k)
k−2(m+1)|
k−2m| =
|t(k)
8k2 − 7(cid:1). Hence the function m (cid:55)→ |t(k)
√

k2 − 4mk + 4m2 + 2m − k
4mk − 4m2 − 8m + 4k − 4
Observe that this quotient is at most 1 if and only if m1 ≤ m ≤ m2, where we set m1 = 1
and m2 = 1
8
monotone decreasing for m1 ≤ m ≤ m2. Moreover, as (cid:98)m1(cid:99) ≤ m1, we deduce that |t(k)
Observe furthermore that m1 ≥ 0 if and only if k ≥ 4, and m2 ≥ k

8k2 − 7(cid:1)
k−2m| is monotone increasing for m ≤ m1 and
k−2(cid:98)m1(cid:99)|.

(cid:0)4k − 5 − √
k−2(cid:100)m1(cid:101)| ≥ |t(k)

(cid:0)4k − 5 +

(A.1)

=

8

.

2 for all k > 1.

Therefore, in the case k ≥ 4, max0≤m≤(cid:98) k
2(cid:99) |t(k)
k−2ψ(k)|. In the case 1 < k ≤ 4, max0≤m≤(cid:98) k

to |t(k)
|t(k)
k | = 2k−1.
Finally we show that the rightmost term of (2.7) increases monotonically with k. We show the inequality:

k−2m| is attained at (cid:100)m1(cid:101) = ψ(k), and thus it is equal
k−2m| is attained at m = 0, and thus it is equal to
2(cid:99) |t(k)

|t(k)
k−2ψ(k)| ≤ |t(k+1)
this time we are interested in the behavior for increasing k, i.e., in the map k (cid:55)→ |t(k)

k+1−2ψ(k+1)| for k ≥ 4. For this we consider again the sequence of Chebyshev coeﬃcients, but
k−2m|. So, for ﬁxed m,

Improved convergence rates for Lasserre-type hierarchies

20

we consider the quotient
k+1−2m|
|t(k+1)
k−2m| =
|t(k)

2k−2m(k + 1)(k − m)! m! (k − 2m)!

2k−1−2mk(k − m − 1)! m! (k + 1 − 2m)!

= 2 · k + 1

k

·

k − m

k + 1 − 2m

,

which is equal to 2 if m = 0, and at least 1 if m > 0 since every factor is at least 1. Thus, for m = ψ(k), we
obtain:

(cid:0)4k − 5 − √
k−2ψ(k)| ≤ |t(k+1)
|t(k)
k+1−2ψ(k)|.
(cid:16)

8

8k2 − 7(cid:1), so that ψ(k) = (cid:100)φ(k)(cid:101). The map

(A.2)

Consider the map φ : [4,∞) → R, k (cid:55)→ φ(k) = 1
φ is monotone increasing, since its derivative φ(cid:48)(k) = 1
is positive for all
k ≥ 4. Hence, we have: ψ(k) ≤ ψ(k + 1). Then, in view of (A.1) (and the comment thereafter), we have
|t(k+1)
k+1−2m| ≤ |t(k+1)

if m ≤ ψ(k + 1), and thus

k+1−2(m+1)|

8k2−7−2k
√
8k2−7
2

4 − 16k

8k2−7

√
2

√

=

8

(cid:17)

k+1−2ψ(k+1)|.
Combining (A.2) and (A.3), we obtain the desired inequality: |t(k)

|t(k+1)
k+1−2ψ(k)| ≤ |t(k+1)

k−2ψ(k)| ≤ |t(k+1)

k+1−2ψ(k+1)|.

(A.3)

2

B. Useful identities for the Chebychev polynomials

Recall the notation dµ(x) to denote the Lebesgue measure with the function(cid:81)n
(cid:90) 1
(cid:104)T α, T βT γ(cid:89)

i )dµ(xi) ·(cid:89)

Tαi(xi)Tβi(xi)Tγi(xi)(1 − x2

(1 − x2

(cid:90) 1

(cid:89)

i )(cid:105) =

i=1

function. In order to compute the matrices AI and BI we need to evaluate the following integrals:

(cid:16)

π(cid:112)1 − x2

i

(cid:17)−1

as density

Tαi(xi)Tβi(xi)Tγi(xi)dµ(xi).

i(cid:54)∈I

−1

i∈I

i∈I

−1

Thus we can now assume that we are in the univariate case. Suppose we are given integers a, b, c ≥ 0 and
the goal is to evaluate the integrals

Ta(x)Tb(x)Tc(x)dµ(x) and

Ta(x)Tb(x)Tc(x)(1 − x2)dµ(x).

(cid:90) 1

−1

(cid:90) 1

−1

(cid:90) 1

−1

We use the following identities for the (univariate) Chebyshev polynomials:

TaTb =

1
2

(Ta+b + T|a−b|), TaTbTc =

1
4

(Ta+b+c + T|a+b−c| + T|a−b|+c + T||a−b|−c|),

so that

TaTbTcT2 =

Using the orthogonality relation(cid:82) 1

(Ta+b+c+2 + T|a+b+c−2| + T|a+b−c|+2 + T||a+b−c|−2|

1
8
+T|a−b|+c+2 + T||a−b|+c−2| + T||a−b|−c|+2 + T|||a−b|−c|−2|).

−1 Tadµ(x) = δ0,a, we obtain that

TaTbTcdµ(x) =

1
4

(δ0,a+b+c + δ0,a+b−c + δ0,|a−b|+c + δ0,|a−b|−c).

Moreover, using the fact that 1 − x2 = (1 − T2)/2, we get

TaTbTc(1 − x2)dµ(x) =

1
2

TaTbTc(1 − T2)dµ(x) =

(cid:90) 1

−1

(cid:90) 1

−1

(cid:90) 1

−1

1
2

TaTbTcdµ(x) − 1
2

(cid:90) 1

−1

TaTbTcT2dµ(x),

Improved convergence rates for Lasserre-type hierarchies

21

and thus(cid:90) 1

−1

TaTbTc(1 − x2)dµ(x) =

1
(δ0,a+b+c + δ0,a+b−c + δ0,|a−b|+c + δ0,|a−b|−c)
8
− 1
16

(δ0,a+b+c−2 + δ0,|a+b−c|−2 + δ0,|a−b|+c−2 + δ0,||a−b|−c|−2).

C. Test functions
Booth Function n = 2, fmin = f (0.1, 0.3) = 0, f ([−1, 1]2) ≈ [0, 2 500]

f (x) = (10x1 + 20x2 − 7)2 + (20x1 + 10x2 − 5)2

= 250(T2(x1) + T2(x2)) + 800 T1(x1)T1(x2) − 340 T1(x1) − 380 T1(x2) + 574

Matyas Function n = 2, fmin = f (0, 0) = 0, f ([−1, 1)]2) ≈ [0, 100]

f (x) = 26(x2

1 + x2

2) − 48x1x2 = 13(T2(x1) + T2(x2)) − 48T1(x1)T1(x2) + 26

Motzkin Polynomial n = 2, fmin = f (± 1

2 ,± 1

2 ) = 0, f ([−1, 1)]2) ≈ [0, 80]

f (x) = 64(x4

1x2

2 + x2

1x4

2) − 48x2

1x2

2 + 1 = 4(T4(x1) + T4(x1)T2(x2)

+ T2(x1)T4(x2) + T4(x2)) + 20 T2(x1)T2(x2) + 16 (T2(x1) + T2(x2)) + 13

Three-Hump Camel Function n = 2, fmin = f (0, 0) = 0, f ([−1, 1)]2) ≈ [0, 2 000]

f (x) =

56
6

1 − 54 · 1.05x4
x6

1 + 50x2

1 + 25x1x2 + 25x2
2

=

56
192

T6(x1) +

1625

4

T4(x1) + 58725

64 T2(x1) + 25 T1(x1)T1(x2) + 12.5 T2(x2) + 14525

24

Styblinski-Tang Function n = 2, 3, fmin = −39.17 · n,f ([−1, 1)]2 ≈ [−70, 200]

f (x) =

312.5x4

j − 200x2

j + 12.5xj =

T4(xj) +

225
4

T2(xj) +

25
2

T1(xj) +

275
16

n(cid:88)

j=1

(cid:18) 625

n(cid:88)

16

j=1

(cid:19)

Rosenbrock Function n = 2, 3, fmin = 0, f ([−1, 1)]2) ≈ [0, 4 000]

f (x) =

j=1

100(2.048 · xj+1 − 2.0482 · x2

n−1(cid:88)
n−1(cid:88)
(cid:2)12.5 · 2.0484 T4(xj) − 100 · 2.0483 T2(xj)T1(xj+1) + (0.5 + 50 · 2.0482)2.0482 T2(xj)
+50 · 2.0482 T2(xj+1) − 4.096 T1(xj) − 100 · 2.0483 T1(xj+1) + 1 + 2.0482(37.5 · 2.0482 + 50.5)(cid:3)

j )2 + (2.048 · xj − 1)2

j=1

=

