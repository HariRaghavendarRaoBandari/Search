6
1
0
2

 
r
a

M
4

 

 
 
]
S
D
.
s
c
[
 
 

1
v
2
1
5
1
0

.

3
0
6
1
:
v
i
X
r
a

Rapidly Mixing Markov Chains: A Comparison of Techniques

(A Survey)

Venkatesan Guruswami∗

Disclaimer: This unpublished survey was written in 2000, and is being posted unedited in its

original form, in response to requests for a permanent URL that can be cited.

It is thus outdated and does not reﬂect the state of the art in 2016.

Abstract

For many fundamental sampling problems, the best, and often the only known, approach to
solving them is to take a long enough random walk on a certain Markov chain and then return the
current state of the chain. Techniques to prove how long “long enough” is, i.e., the number of steps
in the chain one needs to take in order to be sufﬁciently close to the stationary distribution of the
chain, are of great importance in obtaining estimates of running times of such sampling algorithms.
In this report, we survey existing techniques to bound the mixing time of Markov chains. The
mixing time of a Markov chain is exactly captured by the “spectral gap” of its underlying transition
matrix. The spectral gap is closely related to a geometric parameter called “conductance” which
is a measure of the “edge-expansion” of the Markov chain. Conductance also captures the mixing
time up to square factors. Lower bounds on conductance, which give upper bounds on the mixing
time, are typically obtained by a technique called “canonical paths” where the idea is to ﬁnd a
set of paths, one between every unequal source-destination pair, such that no edge is very heavily
congested.

Unlike conductance, the canonical paths approach cannot always show rapid mixing of a rapidly
mixing chain. It is known that this “drawback” disappears if we allow the ﬂow between a pair of
states to be spread along multiple paths. We prove that for a large class of Markov chains, including
all the ones that we use in the sampling applications we will be interested in, canonical paths does
capture rapid mixing, i.e., we show that small mixing time implies the existence of some collection
of paths with low edge congestion. Allowing multiple paths to route the ﬂow still does help a
great deal in the design of such ﬂows, and this is best illustrated by a recent result of Morris and
Sinclair [34] on the rapid mixing of a natural Markov chain for sampling 0-1 knapsack solutions;
this result seems to rely critically on fractional ﬂows.

An entirely different approach to prove rapid mixing, which in fact historically preceded the
conductance/canonical paths based approach, is “Coupling”. Coupling is a very elegant technique
and has been used to prove rapid mixing of several chains where designing good canonical paths
seems to be a hideous task. “Path Coupling” is a related technique discovered by Bubley and
Dyer [5] that often tremendously reduces the complexity of designing good Couplings. We present
several applications of Path Coupling in proofs of rapid mixing, and these invariably lead to much
better bounds on mixing time than known using conductance, and moreover Coupling based proofs
usually turn out to be much simpler. These applications motivate the question of whether Coupling
indeed can be made to work whenever the chain is rapidly mixing. This question was answered
in the negative in very recent work by Kumar and Ramesh [27], who showed that no Coupling
strategy can prove the rapid mixing of the famous Jerrum-Sinclair chain for sampling perfect and
near-perfect matchings (the chain is known to be rapidly mixing via a canonical paths argument).

∗Computer Science Department, Carnegie Mellon University, Pittsburgh, PA. guruswami@cmu.edu. Survey written in

2000 when the author was a student at MIT.

1 Introduction
Suppose W
is a large ﬁnite set of combinatorial structures (for example the set of feasible solutions to a
combinatorial optimization problem), and let p be a probability distribution on W
. The general “sam-
at random according to the distribution p . The Markov
pling” problem is then to pick an element of W
chain Monte Carlo method, which is the subject of our study here, provides an elegant technique to
efﬁciently solve this general computational task in a wide variety of contexts.

Sampling problems are inherently interesting, and in addition turn out to have many computational

applications, the most notable ones being:

• Approximate counting: Here we want to estimate the size of W

to a very good accuracy. It is
well known [23] that, provided a certain technical condition known as self-reducibility is met,
almost uniform sampling (that is sampling from a distribution that is statistically close to the
uniform distribution) is possible in polynomial time if and only if approximate counting is. This
has been one of the main motivations, at least from the computer science point of view, behind
the rapid progress that has been made in this area. In particular, for a host of counting problems
including several very hard #P-complete problems, the Markov chain Monte Carlo method is the
only known approach to approximate the number of feasible solutions.

• Statistical physics: Here the space W
represents possible conﬁgurations of statistical mechanical
system, and p
, in which the probability of a conﬁg-
uration is related to its energy. The task is to sample conﬁgurations according to p , in order to
examine properties of a “typical” physical conﬁguration.

is a “natural” probability distribution on W

In this report, we focus only on the sampling problem and omit the connections to counting since
these involve by now standard reductions. The Markov chain Monte Carlo method has been a great
success story in solving sampling problems. It solves the sampling problem by the following approach.
An underlying “Markov chain” M on the state space W
is speciﬁed through a stochastic transition
probability matrix of dimension |W
| whose (x,y)th entry speciﬁes the probability P(x,y) that the
chain moves from state x to state y in a single step (we assume states of M are labeled by elements
of W
). Starting at any state x0, there is a natural random walk X0 = x0,X1,X2, . . . deﬁned on M such
that Pr[Xt+1|X0, . . . ,Xt] = Pr[Xt+1|Xt] where the latter conditional probability is speciﬁed by the matrix
P, i.e., Pr[Xt+1 = y|Xt = x] = P(x,y). In other words we start at state X0 and at each time step t, we
make a move to a next state Xt+1 by moving to a random state from the current state Xt according to the
transition probabilities of the chain. Note the crucial “forgetting property” of Markov chains: the state
at time t + 1 depends probabilistically on the state at time t, but not on the state at any other time.

|×|W

To sample according to a distribution p , the Markov chain M is deﬁned in such a way that it is
such that Pr[Xt = y|X0 = x] → h (y) as
ergodic, i.e., has a (unique) stationary distribution h on W
, and moreover the transition probabilities are set up so that h = p .
, for all pairs of states x,y ∈ W
t → ¥
Now we may sample from W
, take a
random walk on the Markov chain (which we will loosely refer to as “simulating the Markov chain” in
the sequel) for some number, T , of steps, and then output the ﬁnal state. The ergodicity of M implies
that, by taking T large enough, we can ensure that the output state is arbitrarily close to the desired
distribution p .

according to p as follows: starting from an arbitrary state in W

One of the most appealing things about this method is its simplicity – in fact in most applications it
is not hard to construct a Markov chain having the above properties. The crux of the method, which is
also its sticking point, is to obtain good upper bounds on the mixing time of the chain, i.e., the number
of simulation steps T necessary before the Markov chain is close to its stationary distribution. This

2

is critical as this forms the crucial factor in the running time of any sampling algorithm that uses the
chain. Since our aim is to sample from a set W which is very large, we would like T to be much smaller
than the size of W
|. We shall refer to such chains as
rapidly mixing. Over the years several deep and novel analytic tools have been developed and reﬁned
to bound mixing times of Markov chains. It is the goal of this report to survey the known techniques
for proving rapid mixing, to present representative examples of their use, and to compare and contrast
their scope, their relative strengths and limitations, and their applicability to various contexts.

, say at most a polynomial in the logarithm of |W

Organization. We begin in the next section by reviewing the relevant deﬁnitions and properties of
Markov chains, and by giving a precise characterization of when a Markov chain mixes rapidly in
terms of its spectral properties.
In Section 3 we discuss the notion of conductance and its relation
to the spectral gap of the chain. Section 4 discusses the canonical paths approach and some of its
generalizations that yield bounds on the conductance and the spectral gap, and also proves that for a
large class of chains a small mixing time implies the existence of some collection of good canonical
paths. We then present an illustrative application of this technique to the problem of sampling 0-1
knapsack solutions in Section 5. Section 6 discusses Coupling which is an entirely different approach
to bounding the mixing time, gives an illustrative example of Coupling in action, and also discusses
Path Coupling, which is a useful design tool in constructing Couplings. Several elegant applications
of Path Coupling are presented in Section 7. In Section 8 we discuss the recent result of [27] which
proves that Coupling is in fact weaker than conductance, in that there are chains with large conductance
which cannot be shown to be rapidly mixing by any Coupling strategy. Finally, we conclude with a few
remarks and open questions in Section 9.

Acknowledgments. This survey was written as part of the author’s Area Examination at MIT, the
goal of which was to survey the papers by Bubley and Dyer [5], Anil Kumar and Ramesh [27], and
Morris and Sinclair [34]. This survey (speciﬁcally Sections 5, 6.4, 7.1 and 8) uses liberal portions of
the contents of these papers. This work was also inﬂuenced greatly by the reading of the survey by
Jerrum [19], and the paper by Sinclair [37], among several other papers. I would like to thank Kumar
and Ramesh for sending me a copy of the most recent version of their paper [27].

2 Preliminaries on Markov Chains
A Markov chain on state space W
is completely speciﬁed by the transition matrix P whose entry P(x,y)
represents the probability that the chain moves from state x to state y is a single transition; i.e., P(x,y) =
Pr[Xt+1 = y|Xt = x] for all t ≥ 0. Thus in order to study and analyze the properties of the Markov chain,
it sufﬁces to investigate the properties of this matrix P.

2.1 Basic deﬁnitions
Starting from an initial distribution m (0), the distribution of the chain after t steps m (t) is clearly given
W ). Thus, when using a Markov
by m (t) = m (0)Pn (here we view the distributions as row vectors in R
chain to randomly sample from its state space, we must study the evolution of m (t) as t increases, and
we would like m (t) to (quickly) approach a limiting stationary distribution, say p ; it is not surprising
that p must be ﬁxed under steps of the chain.
Deﬁnition 2.1 A row vector p ∈ R
matrix P if (a) p (x) ≥ 0 for all x ∈ W

is a stationary distribution for a Markov chain M with transition
, (b) (cid:229)

p (x) = 1, and (c) p = p P.

x∈W

3

W
Deﬁnition 2.2 A Markov chain M is said to be ergodic if it has a stationary distribution.

Clearly, we would like (and need) all Markov chains we use for sampling to be ergodic, so next we turn
to conditions on the chain which will ensure ergodicity.

Deﬁnition 2.3 A Markov chain M (with transition matrix P) is said to be irreducible if for all x,y ∈ W
there is an m such that Pm(x,y) > 0, i.e y is eventually reachable from x with non-zero probability.

,

Irreducibility guarantees that the underlying chain is connected, so that starting at any state it is
possible to reach all the other states. It is clearly desirable (and necessary) to impose this requirement
when using a Markov chain to sample from a set W
. We next impose another condition on the chains
we will study, namely aperiodicity; this is merely a technical condition imposed to simplify analysis,
and does not cause any loss of generality as we can turn any (periodic) chain into an aperiodic one
by simply adding loop probabilities of 1/2 at each state, and this clearly does not affect the stationary
distribution.

Deﬁnition 2.4 A chain M over state space W

is aperiodic iff for all x ∈ W

,

gcd{m : Pm(x,x) > 0} = 1.

A central theorem in the classical theory of stochastic process is the following:

Theorem 2.1 Any ﬁnite, irreducible, aperiodic Markov chain is ergodic.

Deﬁnition 2.5 Suppose M (deﬁned over state space W
be reversible (with respect to p ) iff

) has a stationary distribution p . M is said to

p (x)P(x,y) = p (y)P(y,x) for all x,y ∈ W

.

(1)

The conditions of (1) are known as detailed balance equations. The condition of reversibility does
cause some loss of generality, but the ease of analysis gained by making this requirement more than
compensates the sacriﬁce made. Moreover, reversible chains will be general enough for our appli-
cations, and for the rest of the section we focus attention solely on ﬁnite, irreducible, aperiodic and
reversible Markov chains.

The detailed balance conditions also permit an easy proof that a certain distribution is indeed the

stationary distribution of an ergodic Markov chain, as is formalized below.

Lemma 2.2 For a Markov chain M deﬁned on state space W
p on W
with respect to p .

that satisﬁes the conditions (1), then p

, if there exists a probability distribution
is a stationary distribution of M and M is reversible

Proof: We easily verify that p P = p . Indeed,

(p P)(x) = (cid:229)

p (y)P(y,x) = (cid:229)

p (x)P(x,y) = p (x)(cid:229)

P(x,y) = p (x) .

✷

y

y

y

Note that in the deﬁnition of ergodicity we did not require the stationary distribution to be unique,
is in fact

but the conditions of Lemma 2.2 together with irreducibility, are sufﬁcient to guarantee that p
the unique stationary distribution.

4

2.2 Spectral theory of reversible Markov chains

Since a stationary distribution of a Markov chain is simply a left eigenvector of its transition matrix
P, it is natural that in order to study the rate of convergence of the chain to its stationary distribution,
we should try to investigate the spectral properties of P. The reversibility constraint implies that one
can view P as a self-adjoint operator on a suitable inner product space and this permits us to use the
well-understood spectral theory of self-adjoint operators. This approach was ﬁrst undertaken in [8]
(also see [39] for a nice exposition).

The relevant inner product space is L2(p −1) which is the space of real-valued functions on W

the following inner product:1

, with

(2)

hf ,y i = (cid:229)
x∈W

f (x)y (x)

p (x)

.

It is easy to check that the detailed-balance conditions (1) imply that hf P,y i = hf ,y Pi, so that P
is a self-adjoint operator on L2(p −1). Now, by standard linear algebra, it is well known that such
a P has N = |W
| real eigenvalues 1 = l 0 > l 1 ≥ l 2 ≥ ··· ≥ l N−1 ≥ −1; the chain deﬁned by P is
ergodic iff l N−1 > −1. Also, the space L2(p −1) has an orthonormal basis comprising of eigenvectors
p = v0,v1,v2, . . . ,vN−1 of P corresponding to the eigenvalues l 0,l 1, . . . ,l N−1.
can be written as m (0) = c0p + c1v1 + ··· + cN−1vN−1 where
Now, our initial distribution on W
m (0)(x)p (x)
ci = hm (0),vii (so in particular c0 = (cid:229)
p (x) = 1). The distribution after t steps is then given by

x

m (t) = m (0)Pt = p + c1l t

1v1 +··· + cN−1l t

N−1vN−1.

(3)

From the above, it is clear that the chain is ergodic whenever l N−1 > −1, as then all eigenvalues
l i, 1 ≤ i ≤ N − 1, have absolute value less than 1, and as t → ¥
become insigniﬁcant, and m (t) → p . For an ergodic chain, Equation (3) also clearly demonstrates that
the rate of convergence to p
is governed by the second-largest eigenvalue in absolute value, l max =
max{l 1,|l N−1|}. We now make this statement precise. For x ∈ W
, denote by Pt(x,·) the distribution of
the state of the Markov chain at time t, when the chain starts at time t = 0 in state x.

, terms corresponding to them will

Deﬁnition 2.6 The variation distance at time t with initial state x is deﬁned as the statistical difference
between distributions Pt(x,·) and p (·), i.e
D x(t) =

1
2

y∈W

|Pt(x,y)− p (y)|.

We will measure the rate of convergence using the function t x, which quantiﬁes the mixing time, and
which is deﬁned for e > 0 by

t x(e ) = min{t : D x(t′) ≤ e for all t′ ≥ t} .

(4)

(It is easy to see that if D x(t) ≤ e
then D x(t′) ≤ e for all t′ ≥ t as well.) With this notation, we will say
a Markov chain is rapidly mixing if t x(e ) is O(poly(log(N/e ))) (in applications the number of states
N will be exponential in the problem size n, so this amounts to saying that we need to simulate the
chain only for poly(n) steps in order to get a “good” sample from W
). The following makes precise
our intuition that a large value of the spectral gap (1− l max) exactly captures the rapid convergence to
1It is easy to see that the stationary distribution satisﬁes p (x) > 0 for all x ∈ W whenever the chain is irreducible, so the

stationarity. A proof can be found in [8, 2].

inner product is well-deﬁned.

5

(cid:229)
2

t x(e ) ≥ 1

Propostion 2.3 The quantity t x(e ) satisﬁes
(i) t x(e ) ≤ (1− l max)−1(cid:16) lnp (x)−1 + lne −1(cid:17).
l max(1− l max)−1 ln(2e )−1.
(ii) maxx∈W
In light of the above Proposition, if we want rapid convergence to the stationary distribution ir-
respective of the starting state (which is desirable for our applications in sampling where we would
like to start at some arbitrary state), a large gap (1− l max) is both a necessary and sufﬁcient condition.
Moreover, in practice the smallest eigenvalue l N−1 is unimportant: a crude approach is to add a holding
probability of 1/2 to every state, i.e., replace P by 1
2 (I + P), where I is the N × N identity matrix. This
ensures that all eigenvalues are positive while decreasing the spectral gap (1− l 1) only by a factor of
2. The upshot is that in order to study mixing times of Markov chains, one needs to focus attention on
the second-largest eigenvalue l 1, and bound it away from 1.

2.3 Characterizations of second-largest eigenvalue
We now present the known characterizations of the second largest eigenvalue l 1 of self-adjoint matri-
ces, which will be useful in obtaining good bounds on the spectral gap (1− l 1).
Lemma 2.4 (Rayleigh-Ritz) Let P be a self-adjoint operator on a ﬁnite-dimensional inner product
space with inner product h·,·i. Suppose the eigenvalues of P are l 0 ≥ l 1 ≥ ··· ≥ l m and v0 is an
eigenvector of eigenvalue l 0. Then

l 1 = sup
x⊥v0

hx,xPi
hx,xi

.

(5)

Proof: Let v0,v1, . . . ,vm be an orthonormal basis of eigenvectors corresponding to the eigenvalues
l 0, . . . ,l m respectively. Since x ⊥ v0, we can write x as x = c1v1 +··· + cmvm, so that

hx,xPi =

m(cid:229)

i=1

l ic2

i ≤ l 1

m(cid:229)

i=1

i = l 1hx,xi.
c2

When x = v1, equality is achieved, and hence the result follows.
We next present another characterization which at ﬁrst glance seems a bit unwieldy, but it turns out
to be quite useful in that very natural geometrical arguments about a Markov chain can yield upper
bounds on l 1 via this characterization [8].
Lemma 2.5 (Variational characterization) Let P be a self-adjoint operator on a ﬁnite-dimensional
, let Q(x,y) = p (x)P(x,y) = Q(y,x). Then, the second-
inner product space L2(p −1), and for x,y ∈ W

✷

largest eigenvalue of P satisﬁes:

x,y∈W (y (x)− y (y))2Q(x,y)
x,y∈W (y (x)− y (y))2p (x)p (y)
3 Two broad approaches to proving Rapid Mixing

1− l 1 = infy

.

(6)

We saw in the last section that establishing rapid mixing for a Markov chain amounts to bounding the
second largest eigenvalue l 1 of the transition matrix P away from 1 by a poly(log N)−1 amount. The
spectrum of the chain is very hard to analyze directly, so we either need tools to analyze the spectral gap
(using the characterizations presented in the previous section), or somehow analyze the chain directly
without resorting to spectrum.

6

(cid:229)
(cid:229)
3.1 Coupling

One simple and elegant approach to bound mixing times without explicitly bounding the spectral gap
is Coupling. A “coupling” argument is in fact the classical approach to bound mixing times of Markov
chains. Coupling was ﬁrst used by Aldous [1] to show rapid mixing, and has since found several
applications in proving rapid mixing of a variety of chains. We will deﬁne Coupling formally and
discuss some of its applications in detail in later Sections, but at a very high level the idea behind
Coupling is the following. One sets up two stochastic processes X = (Xt) and Y = (Yt ) on the state
space W
both of which individually are faithful copies of the Markov chain M (whose mixing time
we wish to bound). However, their joint evolution is set up in a way that encourages (Xt) and (Yt ) to
coalesce rapidly, so that Xt = Yt for all sufﬁciently large t. The relevance to rapid mixing is obvious
from the Coupling Lemma [1, 19] which states that the probability that the coupling time exceeds some
value t for a certain distribution p ′ for X0 is an upper bound on the variation distance between the
stationary distribution p of M and the distribution of the chain at time t starting from distribution p ′.
Note that we did not explicitly deal with the spectrum of the chain, and this is one advantage of this
approach. We will come back to a detailed discussion of Coupling in Sections 6 through 8.

3.2 Conductance

Let us now look at approaches aimed at establishing rapid mixing via directly bounding the spectral
gap. These use geometric properties of the chain and the characterizations of l 1 given by Equations (5)
and (6) to prove a lower bound on the spectral gap (1− l 1). The relevant geometric parameter is the

conductance of the chain which is deﬁned below.

Deﬁnition 3.1 The conductance of M is deﬁned by

F = F (M)

def
= min

S⊂W

0<p (S)≤1/2

Q(S, ¯S)
p (S)

,

(7)

where Q(x,y) = p (x)P(x,y) = p (y)P(y,x), p (S) is the probability density of S under the stationary
distribution p of M, and Q(S, ¯S) is the sum of Q(x,y) over all (x,y) ∈ S× (W − S).

The conductance may be viewed as a weighted version of edge expansion of the graph underlying
the chain M. For a ﬁxed S, the quotient in Equation (7) is just the conditional probability that the chain
in equilibrium escapes from the subset S of the state space in one step, given that it is initially in S. Thus
F measures the ability of M to escape from any small region of the state space, and hence to make
rapid progress to the stationary distribution. It is not therefore very surprising that the conductance F
would govern the rapid mixing properties of the chain, which in turn is related to the second-largest
eigenvalue l 1 (by Proposition 2.3). This is made precise in the following result from [36, 38]; related
results appear in [3, 31, 33]. Note that the result proves that the conductance captures mixing rate up to
square factors, and thus obtaining a good lower bound on F

is equivalent to proving rapid mixing.

Theorem 3.1 The second eigenvalue of a reversible chain satisﬁes

1− 2F ≤ l 1 ≤ 1−

2

2

.

(8)

7

F
Proof: We only prove the inequality (1 − l 1) ≤ 2F which shows, together with Proposition 2.3,
implies that a large conductance (of the order of 1/poly(n) where n is the problem size) is necessary
for rapid mixing. Our proof follows the elegant approach of Alon [3] who proved a similar result
for expansion of unweighted graphs. A proof of the other direction: (1− l 1) ≥
2 , can be found in
In order to prove l 1 ≥ (1− 2F ), we use the characterization of Equation (5). The largest eigenvalue

[38, 31].

2

(speciﬁed as a real-valued function

) as follows:

of P equals 1 and has p as its eigenvector. Deﬁne a vector f ∈ R
on W
if x ∈ S
if x /∈ S

f (x) =(cid:26) p (x)p ( ¯S)
−p (x)p (S)

Note that h f ,p i = (cid:229)

f (x)p (x)
p (x) = (cid:229)

x

x f (x) = 0, hence by Equation (5), we have

h f , f Pi
h f , fi ≤ l 1 .

Deﬁne g(x)

def
= f (x)

p (x) . Now

h f , fi = (cid:229)
= (cid:229)

x

x∈S

g2(x)p (x)

= (cid:229)

f 2(x)
p (x)
p ( ¯S)2p (x) + (cid:229)

x

(−p (S))2p (x) = p (S)p ( ¯S)

x∈ ¯S

h f , f Pi = (cid:229)
= (cid:229)
= (cid:229)

x

x

x

g(x)(g(y)− g(x))Q(x,y)

f (x)(cid:229)

y

x,y

= (cid:229)

g(x)g(y)Q(x,y)

y f (y)P(y,x)
p (x)
Q(x,y) +(cid:229)
g2(x)(cid:229)
x∈S−p ( ¯S)(p (S) + p ( ¯S)) (cid:229)
g2(x)p (x) +(cid:16) (cid:229)
+(cid:16) (cid:229)
x∈ ¯S−p (S)(p ( ¯S) + p (S))(cid:229)

x,y

Q(x,y)(cid:17)
Q(x,y)(cid:17)

y∈ ¯S

y∈S

= h f , fi− Q(S, ¯S) = p (S)p ( ¯S)− Q(S, ¯S).

(9)

(10)

(11)

From (9), (10) and (11), we get that for any set S,

Q(S, ¯S)
p (S)p ( ¯S) ≥ 1− l 1.

Since p ( ¯S) ≥ 1/2, this implies F ≥ 1−l 1
Corollary 3.2 Let M be a ﬁnite, reversible, ergodic Markov chain with loop probabilities P(x,x) ≥ 1/2
be the conductance of M. Then the mixing time of M satisﬁes t x(e ) ≤
for all states x, and let F
2F −2(lnp (x)−1 + lne −1).

, as desired.

✷

2

A direct analysis of the conductance is sometimes possible by exploiting an underlying geometric
interpretation of M, in which states of M are identiﬁed with certain polytopes, and transitions with

8

F
W
their common facets. A lower bound on conductance then follows from an appropriate “isoperimetric
inequality” of the graph under consideration. This has been fruitful in a few applications, for example
the estimation of the volume of a convex body by Dyer, Frieze and Kannan [11], and a Markov chain
over linear extensions of a partial order by Karzanov and Khachiyan [26]. A more recent example
where the conductance is tackled “directly” is the work of Dyer, Frieze and Jerrum [10] who prove
an upper bound on F
to show that certain classes of Markov chains for sampling independent sets in
sparse graphs do not mix rapidly. The conductance is still not very amenable to computation in general,
and we need further tools that can be used to deduce good lower bounds on the conductance. It is this
task to which we turn next.

4 Rapid mixing via canonical paths

We saw in the last section that in order to prove rapid mixing of a Markov chain, all we need is a good
lower bound on the conductance (and hence the spectral gap) of the chain. In this section, we explore a
useful piece of technology developed in [20, 36, 37] to prove such a lower bound. The basic idea behind
the method is to try and associate canonical paths between every pair of states, in such a way that no
transition of the chain is used by too many paths. Intuitively, if such a set of paths exists, this means
that the chain has no severe bottlenecks which could impede mixing. We now turn to formalizing this
intuition.

4.1 Bounding Conductance using Canonical paths
We ﬁrst formalize some terminology and notation. Let M be an ergodic Markov chain on a ﬁnite set W
.
We deﬁne the weighted directed graph G(M) with vertex set W
and with an edge e between an ordered
pair (x,y) of weight Q(e) = Q(x,y) = p (x)P(x,y) whenever P(x,y) > 0. We call this the underlying
graph of M.
A set of canonical paths for M is a set G of simple paths {g xy} in the graph G(M), one between
each ordered pair (x,y) of distinct vertices. In order to bound the conductance, we would like to have
a set of canonical paths that do not overload any transition of the Markov chain. To measure this
“overloading”, we deﬁne the path congestion parameter [20, 36] for a set of canonical paths G as:

r (G ) = max
e∈G(M)

1

Q(e)

g xy∋e

p (x)p (y),

(12)

where the maximum is over all oriented edges e in G(M), and Q(e) = Q(x,y) if e = (x,y). Think of the
Markov chain as a ﬂow network in which p (x)p (y) units of ﬂow travel from x to y along g xy, and Q(e),
which equals the probability that the Markov chain in the stationary distribution will use the transition
e in a single step, serves as the capacity of e. The quantity r (G ) measures the maximum overloading
of any edge relative to its capacity. The path congestion r = r (M) of the chain M is deﬁned as the
minimum congestion achievable by any set of canonical paths, i.e.,

r = infG

r (G ).

(13)

The following simple result conﬁrms our intuition that a set of paths with low congestion implies a

large value of conductance.

9

(cid:229)
Lemma 4.1 For any reversible Markov chain and any set of canonical paths G

, we have

F ≥

1

2r (G )

.

Proof: Pick S ⊂ W with 0 < p (S) ≤ 1/2 such that F = Q(S, ¯S)
, the total
ﬂow from S to ¯S is p (S)p ( ¯S), and all this must ﬂow across the cut [S : ¯S], which has capacity Q(S, ¯S).
Hence there must exist an edge e in the cut [S : ¯S] such that

p (S) . For any choice of paths G

1

Q(e)

g xy∋e

p (x)p (y) ≥

p (S)p ( ¯S)
Q(S, ¯S) ≥

p (S)
2Q(S, ¯S)

=

1
2F

.

✷

Corollary 4.2 For any reversible Markov chain, and any choice of canonical paths G
largest eigenvalue l 1 satisﬁes

l 1 ≤ 1−

1

8r 2(G )

.

, the second-

(14)

4.2 Relating Spectrum to congestion directly
Since the relation between r and (1−l 1) above proceeded by appealing to the conductance, the bound
of Corollary 4.2 is potentially rather weak because of the appearance of the square. So we now pursue
a direct approach to bound l 1 based on the existence of “good” canonical paths. This was ﬁrst achieved
by Diaconis and Strook [8], but we follow a treatment by Sinclair [37] as it gives the best bounds for
the examples considered later.

In order to state the new bound on l 1, we modify the measure r (G ) to take into account the lengths

of the paths as well. For a set G = {g xy} of canonical paths, the key quantity is now

¯r (G ) = max
e

1

Q(e)

p (x)p (y)|g xy|,

g xy∋e

(15)

where |g xy| stands for the length of the path g xy. The parameter ¯r
by minimizing over the choice of G

.

is deﬁned analogously to Equation (13)

Note that it is reasonable to allow general length functions l(e) on the edges e, compute |g xy| in
terms of this length function, and thus obtain a quantity similar to ¯r (G ) above. In fact, Diaconis and
Strook use the length function l(e) = 1/Q(e), and Kahale [24] considers good length functions that
will lead to the best bounds for speciﬁc chains. We will be content with the unit length function for our
purposes.

Intuitively, the existence of short paths which do not overload any edge should imply that the chain
mixes very rapidly. Indeed, it turns out that the variational characterization (6) can now be used to
bound l 1 directly in terms of
¯r (G ). This is stated in the theorem below; we will not prove this theorem,
but will later prove a more general version of this result (namely Theorem 4.6, which also appears in
[37].

Theorem 4.3 ([37]) For any reversible Markov chain, and any choice of canonical paths G
largest eigenvalue l 1 satisﬁes

1
¯r (G )

.

l 1 ≤ 1−

10

, the second-

(16)

(cid:229)
(cid:229)
A useful way to use the above result is the following version which bounds the spectral gap in terms
. This version of the result is the

of the path congestion r and the length of a longest path used in G
most used in bounding mixing times using this approach.

Corollary 4.4 For any reversible Markov chain, and any choice of canonical paths G
largest eigenvalue l 1 satisﬁes

, the second-

(17)

where ℓ = ℓ(G ) is the length of a longest path in G

.

l 1 ≤ 1−

1

r (G )ℓ

.

The above often leads to much sharper bounds on mixing times than (14) because the maximum

path length ℓ will usually be signiﬁcantly lesser than the estimate obtained for r .

4.3 Known applications of canonical paths

The “canonical paths” approach has been applied successfully to analyze a variety of Markov chains
including those for sampling perfect matchings and approximating the permanent [20, 8], estimating the
partition function of the Ising model [21], sampling bases of balanced matroids [17], sampling regular
bipartite graphs [25], sampling 0-1 knapsack solutions [12], etc. All these papers with the exception
of [17] use more or less the same technique to bound the path congestion that is due to [20] – they
use the state space to somehow “encode” the paths that use any given transition, so that the number of
paths through any edge will be comparable to the number of states of the chain. Feder and Mihail [17]
give a random collection of canonical paths and use a variant of “Hall’s condition” (for existence of
perfect matchings in bipartite graph) to show a small expected congestion and maximum path length
for this collection of paths. They also prove a version of Corollary 4.4 which applies with expected
path lengths and congestion instead of worst case values.

4.4 Path congestion is weaker than Conductance

The canonical paths technique is very useful, but it is natural to ask whether, like conductance, it too
captures rapid mixing up to some polynomial factor (recall that conductance captures mixing time up

to square factors). In other words, does a large conductance or a large spectral gap (1− l 1) always
imply a small value of r (G ) for some choice of canonical paths G ? Unfortunately we give a simple
example below to show that the answer is no — the same example also appears in [37].
Example. Consider the complete bipartite graph K2,n−2 on vertex set {1,2, . . . ,n} and edges {(1,i), (2,i) :
3 ≤ i ≤ n} where n is even, and deﬁne transition probabilities corresponding to the random walk on this
graph, namely at each step stay where you are with probability 1/2, else move to a neighbor chosen uni-
formly at random. The stationary distribution p of this Markov chain is given by: p (1) = p (2) = 1/4
and p (i) = 1/2(n− 2) for i = 3,4, . . . ,n, and hence Q(e) = 1/4(n− 2) for all edges e. Since n is even
it is easy to verify that the conductance of this chain is F = 1/2, and hence using Equation (8) we get
l 1 ≤ 7/8. However, since p (1)p (2) = 1/16 and Q(e) = 1/4(n− 2) for all edges e, the path connecting
states 1 and 2 alone implies that the best value for r (G ) or ¯r (G ) obtainable using canonical paths is
W (n). Hence r and ¯r could in fact be much larger than the quantity (1 − l 1)−1 which governs the

mixing time.

11

4.5 Resistance: a generalization of path congestion

In order to alleviate the shortcoming of the canonical paths technique which was just discussed, we
now present a natural generalization of this approach that will end up capturing mixing times exactly
(and will thus be “as good as” conductance). The idea, again due to Sinclair [37], is to spread the ﬂow
on path g xy between a pair (x,y) of states among several paths. As before, we view G(M) as a ﬂow
network where one unit of ﬂow has to be routed from x to y for every ordered pair (x,y) of distinct
vertices, and each (oriented) edge e has “capacity” Q(e). The difference from the canonical paths
approach is that, we now allow the ﬂow between x and y to be split among multiple paths, i.e., we are
looking for a fractional multicommodity ﬂow that minimizes the congestion. Considering the similarity
with the earlier approach, it is natural to suppose that this new measure will yield similar bounds on the
mixing rate. As we shall see, this will be the case, and in fact this seemingly innocuous generalization
to multiple paths allows us to capture rapid mixing exactly!

Formally, a ﬂow in G(M) is a function f : P → R+ which satisﬁes
for all x,y ∈ X, x 6= y,

f (p) = 1

p∈Pxy

where Pxy is the set of all simple directed paths from x to y in G(M) and P = ∪x6=yPxy. The quality
of a ﬂow is measured by the congestion parameter R( f ), deﬁned analogously to Equation (12) by

R( f )

def
= max

e

1

Q(e)

x,y

p∈Pxy:p∋e

p (x)p (y) f (p),

(18)

and one can deﬁne elongated congestion ¯R( f ), similar to Equation 15, by accounting for the lengths
of the paths:

¯R( f )

def
= max

e

1

Q(e)

x,y

p∈Pxy:p∋e

p (x)p (y) f (p)|p|.

(19)

We have the following results parallel to those of Lemma 4.1, Corollary 4.2, Theorem 4.3 and Corol-
lary 4.4.

Lemma 4.5 For any reversible Markov chain and any ﬂow f , we have

F ≥

1

2R( f )

and hence

l 1 ≤ 1−

1

8R( f )2 .

Theorem 4.6 For any reversible Markov chain, and any ﬂow f , the second-largest eigenvalue l 1 sat-
isﬁes

l 1 ≤ 1−

1

¯R( f )

.

(20)

Corollary 4.7 For any reversible Markov chain, and any ﬂow f , the second-largest eigenvalue l 1
satisﬁes

l 1 ≤ 1−

1

R( f )ℓ( f )

.

(21)

where ℓ( f ) is the length of a longest path p with f (p) > 0.

12

(cid:229)
(cid:229)
(cid:229)
(cid:229)
(cid:229)
We now provide a proof of Theorem 4.6 as we had promised before the statement of Theorem 4.3

(note that the statement of Theorem 4.6 clearly generalizes that of Theorem 4.3).

Proof of Theorem 4.6: We need to prove (1− l 1) ≥ 1/ ¯R( f ) for any ﬂow f . We use Equation (6) to
bound (1− l 1), namely

1− l 1 = infy

x,y∈W (y (x)− y (y))2Q(x,y)
x,y∈W (y (x)− y (y))2p (x)p (y)

.

(22)

Now for any y

, and any ﬂow f , the denominator in the above expression can be written as:

x,y∈W

p∈Pxy

(y (x)− y (y))2p (x)p (y) = (cid:229)
= (cid:229)
≤ (cid:229)
= (cid:229)
≤ (cid:229)
= ¯R( f )(cid:229)

p (x)p (y)(y (x)− y (y))2
f (p)(cid:16) (cid:229)
p (x)p (y) (cid:229)
e∈p
p (x)p (y) (cid:229)
f (p)|p| (cid:229)
e∈p
(y (e+)− y (e−))2(cid:229)
p∈Pxy:p∋e
(y (e+)− y (e−))2Q(e) ¯R( f )
Q(x,y)(y (x)− y (y))2.

p∈Pxy

p∈Pxy

x,y

x,y

x,y

x,y

e

e

x,y

f (p)

(y (e+)− y (e−))(cid:17)2
(y (e+)− y (e−))2
p (x)p (y) f (p)|p|

(Here e− and e+ denote the start and end vertices of the oriented edge e, and we have used Cauchy-
Schwartz inequality in the third step above.) The result now follows from (22).

✷

Deﬁnition 4.1 (Resistance) The resistance R = R(M) of chain M is deﬁned as the minimum value of
R( f ) over all ﬂows f , and like the conductance is an invariant of the chain. Formally,

R = inf
f

R( f ) .

(23)

4.6 Resistance captures rapid mixing
By Lemma 4.5, note that l 1 ≤ 1− 1
8R2 , so a small resistance leads to rapid mixing. We will now see
that in fact the converse is true, in other words a small mixing time implies a small resistance, i.e., the
existence of a ﬂow f with small congestion R( f ). Thus resistance overcomes the shortcoming of path
congestion (since low path congestion was not a necessary condition for rapid mixing, as was shown
by the example in Section 4.4).

t x(1/4). Then the resistance R = R(M) of M satisﬁes R ≤ 16t .

Theorem 4.8 ([37]) Consider an irreducible, reversible, ergodic Markov chain M over W
maxx∈W
Proof: We will demonstrate a ﬂow f with R( f ) ≤ 16t . Let t = 2t . The ﬂow between x and y will be
routed as follows: Consider the set P (t)
xy of all (not necessarily simple) paths of length t from x to y in
xy route f (p) (cid:181) prob(p) units of ﬂow on p, where prob(p) is the probability
G(M), and for each p ∈ P (t)

and let t =

13

(cid:229)
(cid:229)
(cid:229)
(cid:229)
(cid:229)
that the Markov chain makes the sequence of transitions deﬁned by p in the ﬁrst t steps when starting
in state x. Since t = 2t , it is easy to see that for any pair x,y, P (t)

xy 6= /0, and in fact

Pt(x,y)
p (y) ≥

1
8

.

(24)

Thus for p ∈ P (t)
the R( f ).

xy , we have f (p) = prob(p)/((cid:229)

(t)
xy

q∈P

prob(q)) = prob(p)/Pt (x,y). Now let us estimate

p (x)p (y)prob(p)

Pt(x,y)

(using (24))

1

Q(e)

x,y

8

Q(e)

x,y

R( f ) = max

e

≤ max

e

≤ max

e

p∈P(t)

xy :p∋e
p (x)prob(p)

(t)
p∈P
xy
p∋e

8

Q(e) · tQ(e) = 8t = 16t

where we used the fact that the ﬁnal double summation is simply the probability that the Markov
chain traverses the edge e within t steps when started in the stationary distribution p over W
, and this
probability, by the union bound, is at most t times the probability that this happens in one step, and is
thus at most tQ(e).
✷

Remark A. It is also possible to prove (see [37]), using techniques of the approximate max-ﬂow min-
cut theorem for uniform multicommodity ﬂow [28], that l 1 ≥ 1 − O( log N
R ). This gives the weaker
bound t = W (R/log N), but is interesting in its own right.
Remark B. Note that since we used paths of length 2t
in the above proof, the ﬂow f also satisﬁes
¯R( f ) = O(t 2). This, together with (20), implies that ¯R = inf f ¯R( f ) captures rapid mixing as well.
The work of Kahale [24] actually shows that the bound on ¯R( f ), call it m , obtained by minimizing
over all length functions on the transitions and all ﬂows, can be computed to arbitrary precision by
reduction to a semideﬁnite program, and satisﬁes l 1 ≥ 1− O( log2 N
4.7 Path congestion almost always captures rapid mixing!

).

m

In the next section, we will see a resistance based proof (due to [34]) of rapid mixing of a natural
Markov chain for sampling 0-1 Knapsack solutions. This problem was open for a long time, and
had deﬁed all attempts to prove rapid mixing based on canonical paths. In light of the example in
Section 4.4, it is natural to ask if this chain (which we now know mixes rapidly) also cannot have low
path congestion, and whether the generalization to resistance was really necessary.

In this section, we will show that, for a broad class of Markov chains, including all the ones we
consider in applications here, the path congestion r (deﬁned in Equations (12) and (13)) characterizes
rapid mixing up to polynomial (in the problem size) factors. We show that if you can achieve low
congestion with multiple paths, i.e., if the chain has low resistance, then you can also achieve low
congestion by routing all the ﬂow on just a single path. The proof is actually very simple, and is based
on randomized rounding to relate the optimum congestion of “fractional” and “unsplittable” ﬂows, but
we were surprised that it does not seem to have been observed or made explicit in the literature.

14

(cid:229)
(cid:229)
(cid:229)
(cid:229)
Theorem 4.9 Consider an ergodic, reversible Markov chain M with stationary distribution p on a
of size N, and let the resistance of M be R. Let L = maxx6=y p (x)p (y), and let Qmin =
state space W
mine:Q(e)>0 Q(e). Then there exists a set of canonical paths G
r (G ) = O(cid:16)R + logN

Qmin(cid:17).

such that

Proof: By the deﬁnition of the resistance R, we know that there exists a ﬂow f which routes p (x)p (y)
units of ﬂow between every ordered pair (x,y) of distinct states x 6= y, such that every (oriented) edge
e has at most Q(e)R units of ﬂow passing through it. Hence there is a feasible fractional ﬂow f
which routes fxy = p (x)p (y)/L ≤ 1 units of ﬂow between x and y, and with “capacity” on edge e at
most C(e) = max{ Q(e)R
,1}. We can now use a result of Raghavan and Thompson [35], who used
randomized rounding to show the following: There are absolute constants b0 and b1 such that if all
edge capacities equal 1, and all demands are at most 1, and there is a fractional ﬂow satisfying all the
demands with congestion on edge e at most m
f (e) ≥ 1, then there is an unsplittable ﬂow which satisﬁes
all the demands by routing the demand for each source-destination pair along a single path, and which
has congestion at most b0m

f (e) + b1 log N on edge e.

Applying this to our situation with m

f (e) = C(e), we conclude that there exists a set G of canonical
paths which can route fxy units of ﬂow from x to y such that at most b0C(e)+b1 log N units ﬂow through
any edge e, or equivalently, it can route p (x)p (y) = L
fxy units of ﬂow between every pair (x,y) such
that at most b0L C(e) + b1L

log N units ﬂow through any edge e. This implies that

r (G ) ≤ b0 max{R,

Qmin} + b1

Qmin

log N .

and the stated result follows.

✷

Theorem 4.9 actually implies that r = O(R) for a wide variety of Markov chains, and thus for these
chains r also characterizes rapid mixing. Indeed, this will be the case whenever log N L
= O(1),
which will normally always be the case unless the stationary distribution varies widely in the mass
it gives to points of the state space, or there are very small non-zero transition probabilities in the
chain. As an example consider Markov chains with uniform stationary distribution. Then log N L
=
O(1) whenever P(x,y) = W ( log N
N ) for all x,y such that P(x,y) > 0. For most chains in applications to
sampling, we will have N = 2O(n) where n is the problem size and each non-zero P(x,y) will be at least
1/poly(n), hence this condition will indeed be met.

Qmin

Qmin

5 Sampling 0-1 Knapsack solutions

We describe an example of random walk on the truncated hypercube which was only very recently
shown to be rapidly mixing using a fractional multicommodity ﬂow with low congestion [34], but had
resisted all efforts of proving such a result using canonical paths (with just one path between every
source-destination pair). Our result from the previous section (Theorem 4.9) applies to this chain; this
shows that even though spreading ﬂow across multiple paths might in principle be not more powerful
than sending all the ﬂow along a single canonical path, it could be still be easier to deal with in actually
designing the ﬂow. (The example from this section is also covered by the framework of what Feder
and Mihail [17] did, where they prove a version of the small path congestion implies small mixing time
result using expected path lengths and congestion instead of worst case values.)

15

L
L
L
L
The Problem. We are interested in sampling from the set W
of feasible solutions to the 0-1 knapsack
problem deﬁned by the vector a of item sizes and the knapsack capacity b; i.e., for a positive real vector
a = (ai)n

i=1 and a real number b,
W = W

a,b = {x ∈ {0,1}n : a· x =

aixi ≤ b} .

n(cid:229)

i=1
There is a one-one correspondence between vectors x ∈ W
and subsets X of items whose aggregated
weight does not exceed b, given by X = {i : xi = 1}. We will write a(X ) for the weight of X, i.e.,
a(X ) = (cid:229)
has been proposed for the purposes of sampling

A particularly simple Markov chain MK on W

i∈X ai.

uniformly at random from W

. If the current state is X ⊆ {1,2, . . . ,n} then

1. With probability 1/2 stay at X (this holding probability is to make the chain aperiodic), else
2. Pick an item i ∈ {1,2, . . . ,n} uniformly at random.

If i ∈ X move to X − {i}; if i /∈ X and

a(X ∪{i}) ≤ b, move to X ∪{i}, else stay at X.

The chain is aperiodic since P(X ,X ) ≥ 1/2 for all states X, and it is irreducible since every pair of
states can be connected via the empty set. Moreover, it is clear that each non-zero transition probability
P(X ,Y ), X 6= Y , equals P(X ,Y ) = P(Y,X ) = 1
2n . By Theorem 2.1 and Lemma 2.2 therefore, MK is
ergodic with uniform stationary distribution. Despite all the recent activity in proving rapid mixing,
this simple example was not known to be rapidly mixing until the work of [34]. The best prior known
bound on the mixing time, obtained via the canonical paths technique, was exp(O(√n(log n)5/2)) [12],
which beats the trivial bound of exp(O(n)) but is still exponential.

We will now sketch the proof of [34] that this chain has a mixing time of O(n8), and is thus indeed
rapidly mixing. The proof will follow the resistance approach, i.e., we will ﬁnd a ﬂow f that routes
one unit of ﬂow between every pair of unequal states, using multiple paths for each pair to “spread”
the ﬂow, and then use Corollary 4.7 to bound the mixing time. Indeed, if L( f ) is the length of the
longest ﬂow carrying path, and C( f ) is the maximum ﬂow across any (oriented) edge of the chain, then
combining Corollary 4.7 and Proposition 2.3 shows that

t X (e ) ≤ 2n

C( f )
|W
|

L( f )(n + lne −1) .

(25)

Hence our goal now is to construct a ﬂow f with L( f ) = poly(n) and C( f ) = |W

|poly(n). Note
that a shortest path between states X and Y can be viewed as a permutation of the symmetric difference
X ⊕Y , the set of items that must be added to or removed from the knapsack in passing from X to Y .
A natural approach to deﬁning a good ﬂow seems to be to spread the unit ﬂow from X to Y evenly
among all permutations of X ⊕ Y . The problem with this approach, however, is that many of these
permutations will tend to violate the knapsack constraint, as too many items will have been added at
some intermediate point; i.e., the permutation is unbalanced. The way to circumvent this problem is to
deﬁne a family of permutations, which are all “balanced” and also “sufﬁciently random”, and spread the
ﬂow evenly among them. Proving the existence of such permutations, called balanced almost uniform
permutations in [34], forms the main technical component of this proof.

We will now deﬁne the notion of balanced almost uniform permutations formally, and state the
Theorems from [34] guaranteeing their existence. (We will not prove these theorems as they are quite
technical and doing so will take us too far away from our main theme of focusing on Markov chain tech-
niques.) We will, however, show how to construct a good ﬂow f for our purposes given the existence
of the necessary balanced almost uniform permutations.

16

Deﬁnition 5.1 Let {wi}m
non-negative integer. A permutation s ∈ Sm is ℓ-balanced, if for all k, 1 ≤ k ≤ m,

i=1 be a set of real weights, and let M = maxi≤m|wi| and W = (cid:229)

i wi. Let ℓ be a

min{W,0}− ℓM ≤

k(cid:229)

i=1

ws (i) ≤ max{W,0} + ℓM .

(26)

Deﬁnition 5.2 Let s be a random variable taking values in Sm, and let a ∈ R. We call s a a -uniform

permutation if

[s {1,2, . . . ,k} = U ] ≤ a ×(cid:18)m
Prs

k(cid:19)−1

for every k, 1 ≤ k ≤ m, and every U ⊆ {1,2, . . . ,m} of size k.

The main theorem from [34] on the existence of balanced almost uniform permutations is the fol-

lowing:

Theorem 5.1 ([34]) There is a universal constant C such that for any m and any set of weights {wi}m
i=1,
there exists a 7-balanced Cm2-uniform permutation on {wi}. Moreover, if |(cid:229)
i wi| > 15maxi|wi|, then
there exists a 0-balanced Cm2-uniform permutation on {wi}.
Constructing a good ﬂow
Lemma 5.2 For arbitrary weights {ai} and b, there exists a multicommodity ﬂow f in G(MK) which
routes one unit of ﬂow between every pair of unequal vertices, with C( f ) = O(|W
|n5) and L( f ) = O(n).

Combining with Equation (25) we therefore conclude
Theorem 5.3 ([34]) The mixing time of the Markov chain MK satisﬁes t X (e ) = O(n8 lne −1) for every
starting state X. The chain is thus rapidly mixing.

Proof of Lemma 5.2: Let X ,Y be arbitrary states of W
, X 6= Y . We wish to send one unit of ﬂow
from X to Y . As discussed earlier, our idea is to spread this ﬂow evenly among a family of balanced
almost uniform permutations of X ⊕Y , except that we isolate a constant number of “heavy” items H
from X ⊕ Y , and route the ﬂow along balanced almost uniform permutations of (X ⊕ Y )\ H and add
or remove some elements of H repeatedly along the path to maintain ﬁne balance (we always want the
knapsack to be ﬁlled to capacity between (roughly) min{a(X ),a(Y )} and max{a(X ),a(Y )}: an upper
bound on the weight packed in the knapsack is clearly necessary to deﬁne a feasible path, while the
lower bound is used in the analysis to bound the total ﬂow through any edge by “encoding” each ﬂow
path which uses that edge using an element of the state space).

We now proceed with the formal analysis. We wish to obtain an upper bound on the maximum ﬂow
that passes through any state Z (this will clearly also provide an upper bound on the ﬂow through any
transition (Z,Z1) of the chain). Let X ,Y be states such that the ﬂow between them passes through Z.
Let H be the 29 elements of X ⊕Y with the largest weight (set H = X ⊕Y if |X ⊕Y| ≤ 29); breaking
ties according to index order. Deﬁne HX = H ∩ X, HY = H ∩ Y , S = (X ⊕ Y ) \ H and m = |S|. Let
{wi}m
i=1 be an arbitrary enumeration of the weights of items in S, where elements in Y receive positive
signs and those in X receive in negative signs (since we want to add elements in S∩Y and remove those
in S∩ X). The paths we use for our ﬂow will correspond to permutations of indices in S that satisfy the
speciﬁc “balance” requirement described below.

17

Claim. There is an absolute constant C such that there exists a Cm2-uniform family of permutations
each one (call it s ) of which satisﬁes the following “balance” condition:

min{a(Y )− a(X ),0}− a(HY ) ≤

k(cid:229)

i=1

ws (i) ≤ max{a(Y )− a(X ),0} + a(HX ),

(27)

for every k, 1 ≤ k ≤ m.
Proof. We will assume |X ⊕ Y| ≥ 29, for otherwise S = /0 and m = 0, and there is nothing to prove.
Let W = (cid:229) m
i=1 wi = a(Y ) − a(X ) + a(HX ) − a(HY ), and M = maxi|wi|. Let us assume, w.l.o.g, that
W ≥ a(HX )− a(HY ) (the other case is symmetric), so it is easy to see that the above condition (27) is
equivalent to

− a(HY ) ≤

k(cid:229)

i=1

ws (i) ≤ W + a(HY )

(28)

Comparing with condition (26), and allowing for both cases W ≥ 0 and W < 0, it is easy to see that
an ℓ-balanced permutation satisﬁes (28) above whenever ℓM ≤ min{a(HY ),W + a(HY )}. Thus, when
|W| > 15M, we can use 0-balanced permutations guaranteed by Theorem 5.1 for our purposes. When
|W| ≤ 15M, we have a(HX )− a(HY ) ≤ W ≤ 15M. Also a(HX ) + a(HY ) = a(H) ≥ 29M. Combining
these two inequalities we get a(HY ) ≥ 7M. Thus when W ≥ 0, we have 7M ≤ min{a(HY ),W + a(HY )},
and thus we can use a Cm2-uniform family of 7-balanced permutations to satisfy (28). When W < 0,
we have W ≥ −15M and together with a(HX )− a(HY ) ≤ W this implies
= 7M −W

14M − 2W

a(HY ) ≥

2

29M −W

2

≥

and thus once again 7M ≤ min{a(HY ),W + a(HY )}, and we can use a 7-balanced Cm2-uniform family
of permutations.
✷ (Claim)

We now specify the ﬂow paths between X and Y (the ﬂow will be evenly split among all these paths).
The paths will follow the permutations s of the family guaranteed by above Claim, except that along
the way we will use elements of H to keep the knapsack as full as possible, and we will remove elements
of H as necessary to make room for elements of S∩Y to be added. Hence each intermediate state will
be of the form H0∪ ((X \HX )⊕{s (1), . . . ,s (k)}) for some k ≤ m and H0 ⊆ H. The path corresponding
to a particular s
• If k < m and ws (k+1) > 0, then add s (k + 1) if possible (i.e., current knapsack has room for the
item); else delete an (arbitrary) element from H0.

is deﬁned by the following transitions:

• If k < m and ws (k+1) < 0, then add an element form H − H0 if possible (so that knapsack is near

full); otherwise remove s (k + 1).

• If k = m (i.e., all elements in S have been handled), add an element of HY if possible; otherwise

delete an element from HX.

By the upper bound of Condition (27), we have a(X )− a(HX ) + (cid:229)
i=1 ws (i) ≤ max{a(X ),a(Y )} ≤ b
so that we can always remove enough elements of H to make room for ws (k+1) during its turn to be
added. Moreover, the lower bound of Condition (27) implies that for any intermediate state Z on any
ﬂow path, a(Z ∪ H) ≥ min{a(X ),a(Y )}, and since we always keep the knapsack as full as possible,
there exist elements h1,h2 ∈ H such that a(Z∪{h1,h2}) ≥ min{a(X ),a(Y )}. In what follows h1,h2 are
ﬁxed elements of H that depend only on Z,X ,Y .

k+1

18

To estimate the ﬂow through Z, we will “encode” each pair X ,Y of states whose ﬂow paths use
(plus some auxiliary information), so that we can argue that C( f ) is not too large

Z by a state Z′ ∈ W
compared to |W

|. The encoding Z′ is deﬁned by

Z′ =(cid:0)(X ⊕Y )\ (Z ∪{h1,h2})(cid:1)∪ (X ∩Y ).

(Note that this is the complement of Z ∪{h1,h2} in the multiset X ∪Y . Thus it is reasonable to expect
that Z′ will supply a lot of the “missing” information about X ,Y that cannot be obtained from Z,h1,h2.)
Now

a(Z′) = a(X ) + a(Y )− a(Z ∪{h1,h2})
≤ a(X ) + a(Y )− min{a(X ),a(Y )}
= max{a(X ),a(Y )} ≤ b

.

so that Z′ ∈ W
We now wish to upper bound the number of pairs (X ,Y ) that could be mapped to a given Z′. Note
that Z ∩ Z′ = X ∩Y and Z′ ⊕ (Z ∪{h1,h2}) = X ⊕Y , and knowing X ⊕Y , we also know H (since these
form the 29 largest elements of X ⊕Y , ties broken according to index order). Thus Z,Z′,h1,h2 together
ﬁx X ∩Y , X ⊕Y , H and S = (X ⊕Y )\ H. In order to completely specify X and Y , we add some more
information to the encoding, namely the subset U ⊆ S that have been “affected” (i.e., added/removed)
by the time the path from X to Y reaches Z, and also H′ = H ∩ X.2 Thus, the pair (X ,Y ) one of whose
ﬂow paths passes through Z is encoded by the 5-tuple:

fZ(X ,Y ) = (Z′,h1,h2,U,H′) .

We now verify that Z and fZ(X ,Y ) do pinpoint X ,Y . Indeed, we already argued Z,Z′,h1,h2 alone ﬁx
X ∩Y , X ⊕Y , H and S. Now it is easy to verify that X = (U ∩ Z′)∪ ((S\U )∩ Z)∪ (X ∩Y )∪ H′ and
similarly Y = (U ∩ Z)∪ ((S\U )∩ Z′)∪ (X ∩Y )∪ (H \ H′).
We are now ready to bound C( f ) by estimating the cumulative ﬂow f (Z) through Z. For each X ,Y
such that there is a ﬂow path from X to Y passing through Z and whose encoding equals fZ(X ,Y ) =
(Z′,h1,h2,U,H′), there will be non-zero ﬂow only for paths corresponding to those permutations s
of {1,2, . . . ,m} (here m = |S|) that satisfy s {1,2, . . . ,|U|} = U. By the Cm2-uniformity of the family
|U|(cid:1)−1. Thus summing over all U ⊆ S, we still have only Cm3 units of ﬂow for each
at most Cm2(cid:0) m
ﬁxed (Z′,h1,h2,H′). Now there are |W
| choices for Z′, and n2 choices for the pair (h1,h2), and once
(Z′,h1,h2) are ﬁxed, so is H, and thus there are at most 229 possible choices of H′ ⊆ H for each choice
of Z′. In all, we have

of permutations we use to spread the ﬂow, we can conclude that the total ﬂow over all such paths is

Thus C( f ) = O(|W
L( f ) = O(n), and the proof of Lemma 5.2 is complete.

|n5) as well, and since all paths we use to route ﬂows clearly have length O(n),

✷

f (Z) ≤ |W

|· n2 · 229 ·Cm3 = O(|W

|n5) .

6 Coupling and Path Coupling

We have so far focused on conductance based techniques for proving rapid mixing, and saw a non-
trivial application to sampling 0-1 knapsack solutions. The classical approach to bounding the mixing

2The encoding U we use is slightly different from the one Morris and Sinclair [34] use in their proof.

19

time is in fact via a different approach, viz. Coupling. The basic idea behind the coupling argument is
very intuitive: suppose we wish to show that a Markov chain M starting from distribution p ′ converges
to its stationary distribution p within a small number of steps. Consider running the chain on a joint
process (X , Y ) where both X , Y are individually faithful copies of M and where X starts of at state
X0 distributed according to p ′ and Y starts of in state Y0 distributed according to p . Thus at any time
step t, the distribution of Yt equals p . Now if the joint evolution of (Xt,Yt ) is designed to encourage
them to coalesce rapidly, i.e., the “distance” between Xt and Yt decreases rapidly, then for large enough
t, say t ≥ t′, we will have Xt = Yt, with high probability, say (1− e ). Since the distribution of Yt is p , it
is easy to see that this implies that the mixing time to get within e of the stationary distribution when
the chain starts off in distribution p ′, is at most t′ (by the “Coupling Lemma” which we will state and
prove formally shortly).

6.1 The Coupling Lemma
Deﬁnition 6.1 (Coupling) Let M be a ﬁnite, ergodic Markov chain deﬁned on state space W with
transition probabilities P(·,·). A (causal) coupling is a joint process (X , Y ) = (Xt,Yt ) on W ×W
, such
that each of the processes X , Y , considered marginally, is a faithful copy of M. In other words, we
require that, for all x,x′,y,y′ ∈ W

,

Pr[Xt+1 = x′|Xt = x∧Yt = y] = P(x,x′) and,
Pr[Yt+1 = y′|Xt = x∧Yt = y] = P(y,y′) .

✷

Note that the above conditions are consistent with (Xt) and (Yt ) being independent evolutions of

M, but does not imply it. In fact the whole point of Coupling is to allow for the possibility that

Pr[Xt+1 = x′ ∧Yt+1 = y′|Xt = x∧Yt = y] 6= P(x,x′)P(y,y′)

in order to encourage Xt and Yt to coalesce rapidly.
Remark. In applications to bounding mixing time, (Xt) will typically be Markovian, while we allow
Y to be Non-Markovian or history dependent, i.e., Yt could depend upon X0, . . . Xt and Y0, . . . ,Yt−1,
as long as it remains faithful to the original chain M. One can also imagine allowing the process Y
to make its moves dependent on future moves of X , i.e., Yt can depend upon Xt+1,Xt+2, etc. Such a
coupling is called a non-causal coupling. We will only be concerned with causal couplings here, and
the term “Coupling” will always refer only to a causal coupling.

If it can be arranged that coalescence occurs rapidly, independently of the initial states X0,Y0, we
may then deduce that M is rapidly mixing. The key result here is the Coupling Lemma, which seems
to have ﬁrst explicitly appeared in [2].

Lemma 6.1 (Coupling Lemma) Let M be a ﬁnite, ergodic Markov chain, and let (Xt,Yt ) be a cou-
pling for M. Suppose that Pr[Xt 6= Yt ] ≤ e , uniformly over the choice of initial state (X0,Y0). Then the
mixing time t (e ) of M (starting from any state) is bounded above by t.
Proof: Let X0 = x be arbitrary and let Y0 be distributed according to the stationary distribution p of
M. Let A ⊆ W

be an arbitrary event. We have

Pr[Xt ∈ A] ≥ Pr[Yt ∈ A∧ Xt = Yt ]

≥ 1− Pr[Yt /∈ A]− Pr[Xt 6= Yt ]
≥ Pr[Yt ∈ A]− e
= p (A)− e ,

20

and this implies the variation distance between Pt(x,·) and p , D x(t), is at most e , as desired.

In light of the above Lemma, Coupling is a natural technique to prove rapid mixing of Markov chains.
And as we will convince the reader in this section and the next, Coupling is a very crisp and elegant
technique and when it works, it invariably establishes better bounds on mixing time than known through
conductance, and avoids the slackness which is typical of conductance/canonical paths based proofs.
We illustrate this by a simple example below.

✷

6.2 An illustrative example of Coupling in action
We consider the “Bernoulli-Laplace diffusion model”, whose state space W
subsets of [n] = {1,2, . . . ,n}, and we wish to sample an element u.a.r from W
without loss of generality. A natural chain on W
X ⊆ [n] with |X| = k)

is the set of all k-element
. We assume k ≤ n/2
is the following (let the current state be the subset

• Pick rX ∈ {0,1} u.a.r; If rX = 0, remain at X.
• If rX = 1, pick i ∈ X u.a.r and j ∈ [n]\ X u.a.r and move to Y = X ∪{ j}\{i}.

It is easy to that this chain is ergodic with uniform stationary distribution p (X ) = N−1 for all X ∈ W
k(cid:1). We will show using Coupling that this chain mixes in O(k log(k/e )) time (we will later
where N =(cid:0)n

mention the sort of weak bounds that more complicated conductance/resistance based proofs give even
for this very simple example).

,

Theorem 6.2 The mixing time of the above Markov chain satisﬁes t X (e ) = O(k log(k/e )) irrespective
of the starting state X.

Proof: The proof is based on a Coupling that is actually quite simple to set up. The transition
(Xt,Yt ) → (Xt+1,Yt+1) is deﬁned as follows:

1. If Xt = Yt, then pick Xt+1 as M would and set Yt+1 = Xt+1; else

2. If rXt = 0, set Xt+1 = Xt, and Yt+1 = Yt.
3. If rXt = 1, then: Let S = Xt \ Yt and T = Yt \ Xt (note that |S| = |T|); ﬁx an arbitrary bijection
g : S → T . Pick i ∈ Xt u.a.r and j ∈ [n]\ Xt u.a.r and set Xt+1 = Xt ∪{ j}\{i}. Deﬁne i′ ∈ Yt and
j′ ∈ [n]\Yt as follows:

• If i ∈ Xt ∩Yt, then i′ = i, else i′ = g(i)
• If j /∈ Yt, j′ = j, else (now j ∈ T ) j′ = g−1( j).

Now set Yt+1 = Yt ∪{ j′}\{i′}.

It is easy to see that (Xt) and (Yt ) are individually just copies of M, so the above is a legal (in fact
Markovian) coupling. We assume k ≥ 2 to avoid trivialities. Denote by Dt the random variable Xt ⊕Yt.
We wish to bound the expectation

E[|Dt+1||Dt ] ≤ (1−

1
k

)|Dt| ,

(29)

21

as this will imply E[|Dt||D0] ≤ (1− 1
|D0| ≤ 2k, we obtain

k )t|D0|. Since |Dt| is a non-negative integer random variable, and

Pr[|Dt| > 0|D0] ≤ E[|Dt||D0]
1
)t
≤ 2k· (1−
k

which is at most e provided t ≥ k ln(2ke −1). Invoking the Coupling Lemma 6.1, we obtain that the mix-
ing time is O(k ln(k/e )), as promised. It remains therefore to establish (29) which basically quantiﬁes
the fact that Xt and Yt tend to “coalesce”.

Let q = |Xt ⊕Yt|, and let q′ = |Xt+1 ⊕Yt+1|. We want the expectation of q′ for a given q. Consider

now the choices in Step (3) of the Coupling. Four cases now arise:

(i) j ∈ [n]\ (Xt ∪Yt) and i ∈ Xt \Yt: Then q′ = q− 2.
(ii) j ∈ Yt \ Xt and i ∈ Xt ∩Yt: Then q′ = q− 2.
(iii)

j ∈ Yt \ Xt and i ∈ Xt \Yt, j 6= g(i): Then q′ = q− 4.

(iv) In all other cases q′ = q.

k− q/2

k

· (−2) +

q/2− 1

k

Thus the expected value of the change q′ − q is
q/2

n− k− q/2

n− k

q/2

n− k ·

E[q′ − q] =

q/2
k · (−2) +
k(n−k) )q ≤ (1− 1

· (−4)
✷ (Theorem 6.2)

n− k ·
k )q (as k ≥ 2).

·
and this gives E[q′|q] ≤ (1− n−2
Comparison with performance of Canonical Paths. The best bound achievable for this problem via
the canonical paths/conductance based approach seems to be (see [37]) to bound ¯R by demonstrating
a fractional ﬂow that routes one unit between every pair of unequal states, and this gives ¯R ≤ k2(n−k)2
n(n−1) .
For k = W (n), say k = n/2, this gives a bound on mixing time equal to O(n2 log((cid:0)n
k(cid:1)e −1)) = O(n3 +
n2 loge −1), which is signiﬁcantly worse than the O(n(log n + loge −1)) bound we proved using Cou-
pling! In fact, in this case (k = n/2), the second-largest eigenvalue is known exactly: l 1 = 1− 2/n,
so that even getting the best bound on the spectral gap, only yields a mixing time of O(n2 + nloge −1)
(using Proposition 2.3)! These crisp and signiﬁcantly improved bounds seem to be typical of Coupling
whenever it works. We will later (in Section 7.2) also see an application of sampling from subsets
of [n] of size at most k (this is just the “uniform” version of the knapsack problem, where all items
have the same size), where Coupling gives a much better bound mixing time than seems possible using
techniques of Section 5.

6.3 Known applications of Coupling

Owing to its intuitive appeal, Coupling has been a very popular and successful technique in rapid
mixing results. Some instructive examples of Coupling that have appeared in the literature are in
sampling proper k-colorings of a graph [18, 5, 40], linear extensions of a partial order [30, 6], points
in a convex body [7], independent sets in low-degree graphs [29, 5, 15], general contingency tables on
2 rows [13], etc. Even Broder’s original paper [4] on sampling from the set of perfect matchings of a
bipartite graph used a complicated Coupling argument, which was later found to have an error [32].

22

6.4 Path Coupling

Despite the conceptual simplicity and appeal of Coupling, it can often get very difﬁcult to design
couplings appropriate to speciﬁc situations that arise in sampling problems. The problem is one of
“engineering”: how do we encourage (Xt) and (Yt ) to coalesce while at the same time meeting the
apparently contradicting requirement of keeping the individual processes faithful to M? This can lead
to severe technical complexities (see [30] to get an impression of this). This led Bubley and Dyer [5] to
invent an elegant solution to the task of designing Couplings: they called it “Path Coupling”. The idea
behind Path Coupling is to deﬁne the coupling only for “adjacent” states, i.e., only for pairs of states
(and hopefully the task is easier for such pairs of states), and
then extend the coupling to arbitrary pairs of states by composition of adjacent couplings along a path.
In fact, the discovery of Path Coupling has led to a spurt of Coupling based rapid mixing proofs, and
indeed most of the applications cited in Section 6.3 use Path Coupling. We now state and prove the
“Path Coupling” lemma (a version taken from [14]):

in a carefully chosen subset S of W × W

r−1
ℓ=0

Lemma 6.3 (Path Coupling Lemma) Let d be an integer valued metric deﬁned on W ×W which takes
such that for all (Xt,Yt ) ∈ W × W
values in {0,1, . . . ,D}. Let S be a subset of W × W
, there exists a path
d (Zℓ,Zℓ+1) =
Xt = Z0,Z1, . . . ,Zr = Yt between Xt and Yt where (Zℓ,Zℓ+1) ∈ S for 0 ≤ ℓ < r, and (cid:229)
d (Xt,Yt ). (Equivalently, d
is deﬁned by specifying a graph H with vertex set W
and edge set S, and
weights on edges in S, and d (X ,Y ) is simply the shortest path between X and Y in this graph.) Suppose
a Coupling (X ,Y ) 7→ (X′,Y ′) of the Markov chain M is deﬁned on all pairs (X ,Y ) ∈ S (note that (X′,Y ′)
need not lie in S) such that there exists a b < 1 such that E[d (X′,Y ′)] ≤ b E[d (X ,Y )] for all (X ,Y ) ∈ S.
Then the mixing time t (e ) of M satisﬁes t (e ) ≤ ln(De −1)
(1−b ) .
Remark. One can also bound the mixing time in the case b = 1 [5, 14]. For the applications we will
use to illustrate this technique, we will actually have b < 1, so to keep things simple we do not discuss
the b = 1 case.
Proof: First, we observe that the Coupling on S can be extended in an obvious way to a Coupling on
the entire space W × W
. Pick a “path” Xt = Z0,Z1, . . . ,Zr = Yt such that
d (Zℓ,Zℓ+1) (use a deterministic choice rule for resolving ties). Deﬁne the coupling
d (X ,Y ) = (cid:229)
(Xt,Yt ) 7→ (Xt+1,Yt+1) as follows: First select Xt+1 = Z′0 ∈ W
according to the probability distribution
P(X ,·). Now select Z′1 according to the distribution induced by the pairwise coupling of the adjacent
states Z0 and Z1, conditioned on the choice of Z′0; then select Z′2 using the pairwise coupling of (Z1,Z2),
and so on, ending with Z′r = Yt+1. It is easy to verify, by induction of the path length r, that Yt+1 has been
selected according to the distribution P(Yt ,·), so (Xt,Yt ) 7→ (Xt+1,Yt+1) does deﬁne a “legal” coupling
that obeys conditions of Deﬁnition 6.1. Now

. Indeed let (Xt,Yt ) ∈ W × W

r−1
ℓ=0

E[d (Xt+1,Yt+1)] ≤ E[
r−1

r−1

ℓ=0

d (Z′ℓ,Z′ℓ+1)]

E[d (Z′ℓ,Z′ℓ+1)]

=

ℓ=0

≤ b
= bd

r−1

d (Zℓ,Zℓ+1)

ℓ=0
(Xt,Yt ),

23

(cid:229)
(cid:229)
(cid:229)
where we have used the fact that d
of Theorem 6.2, this gives E[d (Xt,Yt )] ≤ b tD, and thus Pr[Xt 6= Yt ] ≤ E[d (Xt ,Yt )] ≤ e whenever t ≥
ln(De −1)
(1−b ) . Invoking the Coupling Lemma 6.1, the claimed bound on the mixing time t (e ) follows.

is a metric, and linearity of expectation. Now as in the proof

Remark. The notion of “adjacency” in the graph H deﬁned for Path Coupling need not have anything
to do with the transitions in the Markov chain that is being studied. In fact, two states that are adjacent
in the Path Coupling graph H need not even be reachable from one another in the Markov chain.

✷

7 Some applications of Path coupling

In this section, we present a few applications of path coupling to Markov chains for interesting sampling
problems.

7.1 Sampling k-colorings of a graph
Given a graph G = (V,E) with maximum degree D
from the set W

k(G) of (proper) k-colorings of G. Let C = {1,2, . . . ,k} be the set of colors.

A natural Markov chain for the above problem, known in the literature as “Glauber dynamics”, is

, consider the task of sampling uniformly at random

the following. Suppose the current state is a coloring X:

• Choose v ∈ V u.a.r, and c ∈ C u.a.r. If Xv→c (i.e., X with the color of v changed to c) is a proper

coloring of F, then move to Xv→c, else remain at X.

Jerrum [18] (see also [19]) ﬁrst proved, using Coupling, that the above chain rapidly mixes for
k > 2D
. We will now present a simple proof due to Bubley and Dyer [5] of this fact using Path Coupling.
We remark that Vigoda [40] recently established that this chain mixes rapidly for k > 11
, using Path
6
Coupling on a different chain, and then using that to deduce the mixing time of the Glauber dynamics.
Following [5], we present the result in a more general set-up that captures “coloring-type” prob-

lems, and then deduce the result for coloring from that.

The general set-up is the following. Let V and C be ﬁnite sets, and let n = |V| and k = |C|, and
we consider a ﬁnite Markov chain M with state space W ⊆ CV , the set of functions from V to C, and
unique stationary distribution p . The transition structure of M is similar to the graph coloring case we
considered above: From a current state X ∈ W
, pick v ∈ V according to a ﬁxed distribution J on V , and
and c ∈ C according to a distribution k X,v that depends only on X and v, and make the transition to Xv→c
(where Xv→c(w) equals c if w = v, and equals X (w) otherwise). We assume that k X,v(c) = 0 whenever
Xv→c /∈ W
. Path Coupling yields the following result for this class of problems (for distributions A and
B, kA− Bk denotes their statistical difference or variation distance):
Theorem 7.1 ([5]) Let W = CV , and let

b = max

X,Y∈W

,i∈Vn1− J(i) + (cid:229)

j∈V

J( j)kk X, j − k Y, jk | Y = Xi→c for some c ∈ C, and Y 6= Xo .

Then, if b < 1, the mixing time of M satisﬁes t (e ) ≤ ln(ne −1)/(1− b ).
Proof: We set up a Path Coupling with “adjacency graph” being all non-equal pairs (X ,Y ) such that
Y = Xi→c for some i,c, and the metric d used is the Hamming metric (so d (X ,Y ) = 1 for adjacent
pairs). For such a pair (X ,Y ) deﬁne the coupling to (X′,Y ′) as follows: X′ is distributed according to

24

D
P(X ,·), namely: pick v ∈ V according to J and c0 ∈ C according to k X,v, and set X′ = Xv→c0. Next pick
c1 ∈ C as follows: with probability min{1,k Y,v(c0)/k X,v(c0)} let c1 = c0, otherwise pick c1 according
to the distribution g (c) =
It is easy to see that marginally we choose c1 according to k Y,v, so the above deﬁnes a “legal”
coupling for the chain M. It is also easy to verify that Pr[c1 6= c0] = kk Y,v − k X,vk. Now since d (X ,Y )

max{0,k Y,v(c)−k X,v(c)}

kk Y,v−k X,vk

changes by at most 1 in one step of the chain, we have

.

E[d (X′,Y ′)] = 1− Pr[d (X′,Y ′) = 0] + Pr[d (X′,Y ′) = 2]

= 1− J(i)Pr[c0 = c1|v = i] +(cid:229)
j6=i
= 1− J(i)(1−kk Y,i − k X,ik) +(cid:229)
≤ bd

(X ,Y )

j6=i

J( j)Pr[c0 6= c1|v = j]
J( j)kk Y, j − k X, jk

(since d (X ,Y ) = 1). The result now follows from the Path Coupling Lemma 6.3.
Application to Coloring. Consider the Markov chain with state space all (not necessarily proper)
k-colorings of G and transitions at state X deﬁned as follows.

✷

1. Choose v at random from V according to distribution J and c u.a.r from C.

2. If v is properly colored in Xv→c, then move to X′ = Xv→c else remain at X.

This is an extension of the Glauber dynamics we discussed earlier (except that we allow more general
distributions to select v from), to all of CV (we do so in order to be able to apply Theorem 7.1). This
does not cause any problems since the non-proper colorings are transient states, and the stationary
distribution is uniform over all proper k-colorings of G, and zero elsewhere. Moreover, if we start from
a proper k-coloring, then we visit only states that correspond to proper k-colorings, so the mixing time
of this chain is an upper bound on the mixing time of the Glauber dynamics. Note that this chain is not
reversible, but Theorem 7.1 applies for such chains as well.

Let us now apply Theorem 7.1. Let d(v) denote the degree of vertex v, and let m be the number
of edges in G. We will use J to be proportional to the degree of the vertex, so that J(v) = d(v)/2m. If
colorings X and Y differ only on vertex i, then k Y, j = k X, j unless j = i or j ∼ i (here j ∼ i stands for
adjacency in the graph G). When j = i, k X,i(X (i)) = d(i)+1
k , and similarly for the
color Y (i), while k X,i(c) = k Y,i(c) for all colors c 6= X (i),Y (i). Hence kk Y,i − k X,ik = d(i)/k. When
j ∼ i, every color that would be accepted in X (resp. Y ), except possibly Y (i) (resp. X (i)) would be
k . Thus the parameter b
accepted in Y (resp. X) as well, and hence kk Y, j−k X, jk = 1
(from Theorem 7.1)
satisﬁes
) +(cid:229)

and k Y,i(X (i)) = 1

d(i)

.

k

b ≤ 1−

d(i)
2m

(1−

k

d( j)
2mk

j∼i

Hence b < 1 whenever

k > max

v∈V {d(v) + (cid:229)

w∼v

d(w)
d(v) } .

This condition is certainly satisﬁed when k > 2D

, so using Theorem 7.1 we conclude

Theorem 7.2 ([5]) The Glauber dynamics for sampling proper k-colorings of a graph G with maximum
degree D

is rapidly mixing (with mixing time O(knlog(ne −1))) whenever k > 2D

.

25

7.2 Sampling “Uniform Knapsack” solutions

We consider another elegant application of Path Coupling. We are interested in sampling from the
space W
of subsets of [n] = {1,2, . . . ,n} of size at most k. This resembles the problem of sampling
k-element subsets of [n] that we considered in Section 6.2, but turns out to be trickier. Note also that
this problem is a special case of the 0-1 knapsack problem (which we considered in Section 5) when
all items to be packed have the same size.

The Markov chain MK we will study will be the same as the one in Section 5, namely from a state
X ⊆ [n], |X| ≤ k, pick rX ∈ {0,1} u.a.r. If rX = 0 remain at X. If rX = 1, pick an i ∈ [n] u.a.r and move
to X \{i} if i ∈ X and to X ∪{i} if i /∈ X and |X| < k. We will use Path Coupling to prove
Theorem 7.3 The mixing time of the Markov chain MK satisﬁes t (e ) = O(nlog(ke −1)).
Proof: We will use Path Coupling with the (somewhat unusual) metric d (X ,Y ) =|X ⊕Y|+||X|−|Y||.
Note that d (X ,Y ) ≥ 2 whenever X 6= Y . The set of “adjacent” pairs S ⊆ W ×W
the Coupling is: S = {(X ,Y ) : X ,Y ∈ W ∧ d (X ,Y ) = 2}. It is easy to see that the metric d and the set S
Now consider (X ,Y ) ∈ S with d (X ,Y ) = 2; we wish to deﬁne a Coupling (X ,Y ) 7→ (X′,Y ′). There

satisfy the conditions required by the Path Coupling Lemma 6.3.

for which we will deﬁne

are two possibilities for (X ,Y ):

(i) One of X ,Y is a subset of the other, say Y ⊂ X (the other case is symmetric), |Y| = |X|− 1.
(ii) |X| = |Y| and |X ⊕Y| = 2.
We consider each of these cases in turn.
Case (i): Let Y = X \{p} for some p ∈ [n]. Now the Coupling (X′,Y ′) is deﬁned as follows:
(1) Pick rX ∈ {0,1} and i ∈ [n] u.a.r. If i = p then set rY = 1− rX ; otherwise set rY = rX .
(2) If rX = 0 set X′ = X. Else if i ∈ X set X′ = X \{i}, else set X′ = X ∪{i} if |X| < k and X′ = X

otherwise.

(3) If rY = 0 set Y ′ = Y . Else if i ∈ Y set Y ′ = Y \{i}, else set Y ′ = Y ∪{i} if |Y| < k and Y ′ = Y

otherwise.

n )d (X ,Y ).

It is easy to see that d (X′,Y ′) = 2 except when i = p, in which case, since we have cleverly designed
the Coupling by setting rY = 1 − rX so that only one of X ,Y “ﬁres”, d (X′,Y ′) = 0. Thus we have
E[d (X′,Y ′)] = (1− 1
Case (ii): |X| = |Y| and |X ⊕Y| = 2. Let X = S∪{p} and Y = S∪{q} for some p 6= q. The Coupling
(X′,Y ′) is deﬁned as follows:
(1) Pick rX ∈ {0,1} and i ∈ [n] u.a.r. Set rY = rX . If i /∈ {p,q}, set j = i. If i = p (resp. q) set j = q

(resp. p).

(2) If rX = 0 set X′ = X. Else if i ∈ X set X′ = X \{i}, else set X′ = X ∪{i} if |X| < k and X′ = X

otherwise.

(3) If rY = 0 set Y ′ = Y . Else if j ∈ Y set Y ′ = Y \{ j}, else set Y ′ = Y ∪{ j} if |Y| < k and Y ′ = Y

otherwise.

26

Combining both the above cases we get E[d (X′,Y ′)] ≤ (1− 1

Once again, the Coupling has been constructed so that d (X′,Y ′) = 2 whenever i /∈ {p,q}; d (X′,Y ′) = 0
if i = p and rX = 1, and d (X′,Y ′) ≤ 2 in all cases. Thus we have E[d (X′,Y ′)] ≤ (1− 1
2n )d (X ,Y ) always. Also, the maximum
value D of d (X0,Y0) over all pairs (X0,Y0) ∈ W × W
is clearly 2k. By Theorem 7.1 therefore, we have
shown that MK has mixing time t (e ) = O(nlog(ke −1)), completing the proof.
✷
Comparison with Canonical paths. Even for this special case of 0-1 knapsack, the best bound that we
get using the multicommodity ﬂow based analysis of Section 5 (without any change) is only O(n6), and
it is almost inconceivable that such an approach can hope to yield a bound better than O(n3). Coupling
gave us a much better O(nlog(ke −1)) bound, and the proof was in fact much easier than using canonical
paths!

2n )d (X ,Y ).

Remark. The uniformity of weights seems critical to our argument above. The “asymmetry” created
when items have widely varying sizes seems to make it difﬁcult for any natural Coupling strategy to
work.

7.3 Linear extensions of a partial order
We are given a partially ordered set (P,(cid:22)) where |P| = n, and we want to sample u.a.r from the space
of all linear orders that extend (cid:22). (A linear order extending (cid:22) is a permutation a1,a2, . . . ,an of the
elements of P such that ai (cid:22) a j implies i ≤ j.)
A natural Markov chain with uniform stationary distribution over W was shown to be rapid mixing
by Karzanov and Khachiyan via conductance arguments that exploited the geometry of the space [26].
Dyer and Frieze [9] improved the conductance estimate, and hence the bound on the mixing time, of
this chain, and this gave a mixing time of O(n5 log n + n4 loge −1).

In this section, we will sketch a chain MJ

le, which is a slight variant of the chain discussed above,
and show (using Path Coupling) that it has a mixing time of O(n3 log(ne −1)), which signiﬁcantly
improves the best “conductance based” bound for this problem. The chain and its analysis are due to
Bubley and Dyer [6] (see also [19] for an exposition).

Actually this algorithm can be used to sample u.a.r from any set W of permutations of elements of P

that satisﬁes the following “closure” property: If s = (a1,a2, . . . ,an)∈ W
. . . ,a j−1,ai,a j+1, . . . ,an) ∈ W
tation still lies in W
positions in the interval [i, j], also lie in W
closure property.

(i.e., the positions of ai and a j can be swapped and the resulting permu-
), then all permutations which are obtained from s by placing ai and a j at arbitrary
. Clearly the linear extensions of a partial order have this

and s ◦(i, j) = (a1, . . . ,ai−1,a j,ai+1,

The transitions from one linear extension to another in the chain are obtained by (pre)-composing
with a random transposition (p, p + 1) (if this yields a valid linear order); however, instead of selecting
p ∈ [n− 1] uniformly, p is chosen according to a distribution J on [n− 1] that gives greater weight to
values near the center of the range. Formally, the chain MJ
le is deﬁned as follows. Let the current state
be Xt. Then the next state Xt+1 is deﬁned by the following random experiment:
(1) Pick p ∈ [n− 1] according to the distribution J, and r ∈ {0,1} u.a.r
(2) If r = 1 and Xt ◦ (p, p + 1) ∈ W
To use Path Coupling we need to specify an “adjacency” structure for the state space W

. We
say two states g and g′ are adjacent if g′ = g◦ (i, j) for some transposition (i, j) with 1 ≤ i < j ≤ n,
and the “distance” d (g,g′) in this case is deﬁned to be j − i. Since this distance is symmetric (i.e.,
d (g,g′) = d (g′,g)), this adjacency structure yields a weighted, undirected graph H on vertex set W
.

, then Xt+1 = Xt ◦ (p, p + 1); otherwise Xt+1 = Xt.

27

W
to a metric on W

One can verify that the shortest path between adjacent states g,g′ in H is the direct one that uses the
edge (g,g′). We may thus extend d
to
be the length of a shortest path from g to h in H, and all conditions of the Path Coupling Lemma 6.3
are now met. It remains to deﬁne a coupling (g,h) 7→ (g′,h′) for adjacent states g,h and then bound
E[d (g′,h′)].
The Coupling is deﬁned as follows. Let (g,h) be a pair of adjacent states in H and let h = g◦ (i, j).

by deﬁning d (g,h) for arbitrary states g,h ∈ W

Then the transition to (g′,h′) is deﬁned by the following experiment:

rh = 1− rg; otherwise set rh = rg.

(i) Pick p ∈ [n− 1] according to distribution J, and rg ∈ {0,1} u.a.r. If j − i = 1 and p = i, set
(ii) If rg = 1 and g◦ (p, p + 1) ∈ W
(iii) If rh = 1 and h◦ (p, p + 1) ∈ W
Lemma 7.4 For adjacent states g and h, for a suitable choice of the probability distribution J, we have

then set g′ = g◦ (p, p + 1) else set g′ = g.
then set h′ = h◦ (p, p + 1) else set h′ = h.

6

E [d (g′,h′) | g,h] ≤(cid:16)1−

In light of Lemma 6.3, this implies that the mixing time of MJ

n3 − n(cid:17)d (g,h) .
le is O(n3 log(ne −1)) (since the
“diameter” D of the graph H is easily seen to be at most(cid:0)n
2(cid:1)). It thus only remains to prove Lemma 7.4.
Proof of Lemma 7.4: We only provide the skeleton of the proof; details can be found in [6]. When
h = g◦ (i, j), it is easy to see that when p /∈ {i − 1,i, j − 1, j}, we will have h′ = g′ ◦ (i, j) and thus
d (g′,h′) = d (g,h) = j−i. When p = i−1 or p = j, it is again easily checked that E [d (g′,h′) | g,h, p =
i− 1∨ p = j] ≤ d (g,h) + 1/2.
The “interesting case” is when p = i or p = j− 1. These are symmetric, so let us focus on the case
p = i. There are two sub-cases: j−i = 1 and j−i ≥ 2. First, consider the case j−i = 1. In this case, we
have made sure, by setting rh = 1− rg, that only one of g or h “ﬁres” in the Coupling, and thus g′ = h′
and therefore d (g′,h′) = 0! In the case j− i ≥ 2, by the “closure” property of W
discussed earlier (this
is the only place where we use this closure property), we know both g◦ (i,i + 1), h◦ (i,i + 1) ∈ W
, thus
either rX = rY = 0 and then d (g′,h′) = d (g,h), or rX = rY = 1 and d (g′,h′) = j − i− 1 = d (g,h)− 1.
Hence d (g′,h′) is less than d (g,h) in expectation.

Summing up, it follows from the above discussion that

E [d (g′,h′) |g,h] ≤ d (g,h)− −J(i− 1) + J(i) + J( j− 1)− J( j)

2

.

(30)

=z (p + 1)(n− p− 1) where z = 6/(n3 − n) is
Specializing the probability distribution J(·) to be J(p)
a normalizing constant, and using d (g,h) = j− i, we get from (30) that E [d (g′,h′)] ≤ (1− z )d (g,h).
✷ (Lemma 7.4)

def

8 Coupling is weaker than Conductance

We have seen several Coupling based proofs in the last Section which are not only extremely simple
and elegant, but also end up giving much better bounds on mixing time than known via conductance
based arguments. So, is Coupling the panacea as far as bounding mixing times goes? In particular, is
Coupling as powerful as conductance, and does it capture rapid mixing exactly?

28

This fundamental question was unanswered for a long time until recently when Kumar and Ramesh [27]

proved the following important result: For the famous Jerrum-Sinclair chain for sampling perfect and
near-perfect matchings, no Coupling argument can show rapid mixing (the chain is known to be rapidly
mixing using a canonical paths argument [20]). Hence Coupling is actually “weaker” than conductance!
We discuss the salient features behind their proof in this section.
The Jerrum-Sinclair Chain. We are given a bipartite graph G = (V1,V2,E) with |V1| = |V2| = n and
the goal is to sample u.a.r from the set P of perfect and near-perfect matchings of G (a near-perfect
matching is a matching that saturates all but two vertices of G). Jerrum and Sinclair [20] proposed the
following natural Markov chain MJS for sampling from P: At each state M, the chain moves to a state
M′ deﬁned by the following random experiment:
(i) Pick r ∈ {0,1} u.a.r and an edge e ∈ E u.a.r.
(ii) If r = 0 set M′ = M; Else
(iii) If M is a perfect matching: Then set M′ = M \{e} if e ∈ M, or else M′ = M.
(iv) Suppose M is a near-perfect matching. Let e = (u,v). There are two cases:

(a) If u,v are both unmatched in M, set M′ = M ∪{e}. [Add Move]
(b) If exactly one of u,v is unmatched, then set M′ = M \{e′}∪{e} where e′ is the edge in M

incident on whichever of u,v is matched. [Swap Move]

(v) If none of the above conditions are met, set M′ = M.

A special graph G. Anil Kumar and Ramesh [27] show that for a certain graph G, every Coupling
strategy on the above chain will require time exponential in n. This graph has some special properties
which are used in the proof; these are:

cn ) perfect matchings for some constant c > 1.

1. G has W ( n!
2. Each vertex of G has degree at least a n, for some a < 1/2.
3. For every pair of vertices, the intersection of their neighborhoods has size at most a n/2.

Such a graph G can be shown to exist using the probabilistic method (see for example the ﬁnal version
of [27]).

Modeling the Coupling Process. The coupling process C = (X , Y ) is speciﬁed by transition proba-
bilities pC (v,w) where v = (a,b) ∈ P× P, and w = (c,d) ∈ P× P are pairs of states in P. Note that
pC (v,w) could even be a function of the history, i.e., the transition probabilities could vary with time
(we do not show the time dependence for notational convenience, but it should be treated as implicit).
Since we are aiming for a negative result and wish to rule out the existence of any Coupling based
proof, the only thing we will (and can) assume about these probabilities is that the processes X and
Y must individually be faithful copies of MJS, or in other words: If v = (x,y), then for each x′ ∈ P
and for each time instant t, (cid:229) w∈T (x′) pC (v,w) = P(x,x′) where T (x′) = {(x′,z) | z ∈ P} and P(·,·) is the
transition probabilities of the chain MJS, and a similar equation for P(y,y′) for each y′ ∈ P.
Idea behind the Proof. The basic structure of the proof is the following: Deﬁne a “distance” between
the two states X ,Y in a Coupling, relative to which the states will have a tendency to drift away from

29

each other in any Coupling, i.e., most transitions of any Coupling are distance increasing. Then ana-
lyze this drifting behavior and show that staring with two states (X0,Y0) at a distance Q (n) apart, any
Coupling will require exponential number of steps t before the states Xt,Yt become equal, with say a
probability of 1/2. This gives an exponential lower bound on the Coupling time for any strategy, as
desired.

8.1 Details of the Analysis

We partition the states of the Coupling chain C into layers L(i), i = 0, . . . ,2n according to the “distance”
i between its elements, where L(i) contains of all pairs (M,N) ∈ P × P such that |M ⊕ N| = i. We
further partition each set L(i) into two sets Bot(i) and Top(i), where Bot(i) = {(M,N)| ∃ vertex v
which is unmatched in exactly one of M,N}, and Top(i) = {(M,N)| either both M and N are perfect
matchings or both are near-perfect matchings with the same unmatched vertices}.
A move in C from L(i) to L( j) is leftwards or distance reducing if j < i, and rightwards or distance
increasing if j > i. Since G has W ( n!
cn ) perfect matchings, with overwhelming probability, the start state
of the Coupling lies in L(i) for some i ≥ n/4. For simplicity therefore, we assume that the Coupling C
begins at some state in L(i0), i0 ≥ n/4.
The idea now is to upper bound the probabilities of the leftward transitions and lower bound the
probabilities of the rightward transitions, and then use these bounds to show that the Coupling has a
tendency to drift towards the right. Finally, this will imply that the (expected) number of steps to reach
a state in L(0) will be exponentially large, giving us our desired result.
The Key Lemmas. We now state the main Lemmas which bound transition probabilities between
different layers. We will later use the statements of these Lemmas give us the desired “rightward drift”.
We give a representative proof of one of the Lemmas (the proofs of the other Lemmas can be found in
[27], and we do not reproduce them here).

Lemma 8.1 No transition in C can change the distance by more than 4.

Lemma 8.2 For any coupling strategy, the sum of transition probabilities from (M,N) ∈ Bot(i) to
vertices in L( j), j < i, is at most 2i+1
m .

a n/2−i−2

2m

.

j=i+1 Bot(j) is at least

Lemma 8.3 For any coupling strategy, the sum of the transition probabilities from (M,N) ∈ Bot(i)
into Si+4
Lemma 8.4 For any coupling strategy, the sum of the transition probabilities from (M,N) ∈ Bot(i)
into Top(i)∪ Top(i + 1) is at most i+3
2m .
Lemma 8.5 For any coupling strategy, all transitions from (M,N) ∈ Top(i) are to vertices in either
Top(i) or in Bot(j) for some j ≥ i− 2.
We only prove Lemma 8.3 as it is the key Lemma that establishes a tendency of any Coupling to drift
to the right. This should give a ﬂavor of the sort of arguments necessary to prove the other Lemmas as
well.
Proof of Lemma 8.3: Since (M,N) ∈ Bot(i), three cases arise: (a) M is a near-perfect matching and
N is a perfect matching; (b) M is a perfect matching and N is a near-perfect matching; and (c) Both M
and N are near-perfect matchings with at most one common unmatched vertex. Case (b) is symmetric
to Case (a), so we consider Cases (a) and (c) in turn.

30

a n−|M\N|

Case (a): M is near-perfect and N is perfect. Let a ∈ V1 and b ∈ V2 be the unmatched vertices in M.
We consider only one situation that will increase |M⊕ N| and then lower bound the probability that this
situation occurs. The situation is: M moves to M′ = M +e− (u,u′) where e = (a,u) and (u,u′) ∈ M∩N.
Now |M′ ⊕ N| = |M ⊕ N| + 2. N can move to N′ where either N′ = N or N′ = N − f for some edge
f ∈ N. In either case |M′ ⊕ N′| ≥ |M ⊕ N| + 1. Furthermore, u′ and b are unmatched in M′, and since
(u′,b) /∈ N, at least one of them is matched in N′. We thus conclude (M′,N′) ∈ Bot(j), for some j > i.
2m , for any
Now the probability that this situation occurs is clearly at least
coupling strategy.
Case (b): M and N are both near-perfect. Suppose M have vertices a ∈ V1 and b ∈ V2 unmatched and
N has vertices c ∈ V1 and d ∈ V2 unmatched. Let us assume that b 6= d (while c could equal a).
We once again focus on a particular class of moves which M makes. Suppose M chooses an edge
e = (b,u), where u is not adjacent to d and (u,u′) ∈ M∩ N for some u′ ∈ V2 (by our assumption about G
there exist at least a n/2−|M\N| ≥ a n/2−i such edges e. If e is picked (i.e., M′ = M +e−(u,u′)) then
|M′ ⊕ N| = |M ⊕ N| + 2. It is easy to verify now that the only moves for N that can reduce the distance
back by 2 are when it choose the unique edge (c,c′) ∈ M, if any, or the unique edge (d,d′) ∈ M, to swap
in. The probability of either of these happening is at most 2
2m for any coupling strategy. Furthermore,
in this case u′ ∈ V2 is unmatched in M′ and must be matched in N′ because (u,u′) ∈ N (it lies in M∩ N)
and (u,d) /∈ E by the choice of u. Hence (M′,N′) lies in Bot(). Summing up, (M′,N′) ∈ Bot(j) for j > i
with probability at least
✷ (Lemma 8.3)

which is at least a n−i

a n/2−i−2

2m

.

2m

8.2 Bounding the Coupling Time

With the above Lemmas in place, we are ready to ﬁnish off the analysis bounding the coupling time.
The rightward drifting behavior of any Coupling C can be predicted (qualitatively) given the above
Lemmas. We now see how to quantify this intuition. We deﬁne a sequence of random variables
Z0,Z1, . . . which represent the layer number of some intermediate states of the Coupling. We will show
that Pr[Zt = 0] ∼ te−Q (n), and this will imply an exponential lower bound on the Coupling time.
Deﬁne Z0 to the layer number of the starting state of the Coupling C . As discussed earlier, we
assume Z0 ≥ n/4. Also assume, by virtue of Lemma 8.5, that the starting state is in a Bot() set rather
than a Top() set.
For i > 0, the random variable Zi is deﬁned as follows. If Zi−1 = 0 then Zi = 0. Otherwise, Zi is the

layer number of the ﬁrst state A reached in the Coupling C that has the following properties:

1. A /∈ L(Zi−1).
2. A is in some Bot() set or in L(0).

Lemma 8.6 For every i ≥ 1, |Zi − Zi−1| ≤ 8
Proof: Follows easily from Lemmas 8.1 and 8.5.

✷

The Lemma below quantiﬁes the “rightward drifting” behavior of the sequence Z0,Z1, . . ..
. Then Pr[Zi > Zi−1|Zi−1] ≥ pi

Lemma 8.7 Deﬁne pi =
Proof: By Lemma 8.3, Zi > Zi−1 happens with probability at least pi. By Lemma 8.5, Zi < Zi−1
only if the ﬁrst vertex visited after leaving Bot(Zi−1) for the last time is either in L( j), j < i, or is in

and qi = 5(Zi−1+1)

a n/2−Zi−1−2

pi+qi

2m

2m

.

31

Top(Zi−1)∪ Top(Zi−1 + 1). By Lemmas 8.2 and 8.4, this probability is at most qi. The claimed result
now follows.

✷

that

Let b > 0 be a constant such that

5(b n+1)
a n/2−b n−2 ≤ 1

16. Then it is easy to see using the above Lemma

[Zi − Zi−1 | Zi−1;0 < Zi−1 ≤ b n] ≥

E
Zi

1
4

.

(31)

Combining Lemma 8.6 with the above Equation, we will be able to bound the Coupling time by ap-
pealing to the following submartingale inequality [27] (see also [16]).

Propostion 8.8 Let Z0,Z1,Z2,··· be a sequence of random variables with the following properties (for
some R,D

,M > 0):

1. Zi ≥ 0, for all i ≥ 0. Further Zi = 0 ⇒ Zi+1 = 0, for all i ≥ 0.
2. |Zi − Zi−1| ≤ D
3. E [Zi − Zi−1 | Zi−1;0 < Zi−1 ≤ R] ≥ M, for all i, i ≥ 1.

for all i, i ≥ 1.

Let T be the random variable deﬁned as min{i ≥ 0|Zi = 0}. Then
D 2 + te− M(R−D )

MZ0

Pr[T ≤ t|Z0] ≤ e−

D 2

.

Note that the above is very similar in spirit to Azuma’s inequality applied to submartingales, except
that the assumption (3) above is made only when conditioned on 0 < Zi−1 ≤ R, and not for any value
of Zi−1 (as is done in Azuma’s inequality).
Let us now apply the above Proposition to our setting. Let te be the earliest instant at which the
probability that coupling time exceeds te falls below e . Deﬁne T = min{i ≥ 0|Zi = 0}. Then, applying
Proposition 8.8 with D = 8, M = 1/4, R = b n and Z0 ≥ n/4, we get
1− e ≤ Pr[T ≤ te |Z0] ≤ te e−Q (n) .

It follows that te ≥ (1− e )exp(Q (n)). We have thus proved the following:
Theorem 8.9 ([27]) Consider any Coupling process for the Markov chain MJS for sampling from per-
fect and near-perfect matchings. The probability that this process has “coupled” exceeds (1− e ) only
after time W ((1−e )eQ (n)). Thus, no proof of rapid mixing of MJS exists based on the Coupling Lemma.
9 Concluding Remarks and Open Questions

We have seen that the mixing rate of a Markov chain is captured by the spectral gap and also by a
geometric parameter called Conductance. We discussed ways to bound the conductance, and also ways
to bound the spectral gap directly, based on construction of canonical paths or ﬂows between every pair
of states that do not overload any transition of the Markov chain. The “ﬂow” based approach led to the
notion of resistance which also captures the spectral gap (up to square factors). We showed that for a
large class of chains, the existence of “good” canonical paths with low edge-congestion also captures
mixing time, and thus is no weaker than the resistance based approach. We nevertheless demonstrated
that spreading the ﬂow along multiple paths might still be a very useful design tool by discussing the
recent result of [34] on the rapid mixing of a natural chain for sampling 0-1 knapsack solutions.

32

We then turned to an entirely different approach to proving rapid mixing: Coupling. We discussed
“Path Coupling” which is a useful tool in designing good Couplings. We saw several simple and elegant
applications of Coupling which invariably gave much better bounds on mixing time than known through
conductance. One of these examples was the 0-1 knapsack problem with uniform item sizes for which
we proved a much better mixing time bound than seems possible using the (more difﬁcult) approach of
[34].

Despite the appeal of Coupling in several applications, it turns out that Coupling is weaker than
conductance in the sense that there are Markov chains with an exponential gap between their actual
mixing time and that which can be deduced using any Coupling strategy. We discussed the result of
[27] which showed such a result for the famous Jerrum-Sinclair chain for sampling uniformly from the
set of perfect and near-perfect matchings of a bipartite graph.

There are several natural questions on the relative power of the various techniques that are worthy

of more detailed study. We list some of them below.

• The result of Kumar and Ramesh [27] is quite natural and says that Coupling cannot work when
there is a measure of distance relative to which the states have a tendency to drift away from each
other in any Coupling strategy. It will be nice to ﬁnd other chains for which Coupling cannot
prove rapid mixing. This might shed some light on how to tackle the question we raise next.

• Is there a subclass of Markov chains for which Coupling characterizes rapid mixing (up to poly-
nomial factors)? What kinds of structure in the underlying problem enables easy design of good
couplings, i.e., what makes a problem “Coupling friendly”?

• It almost seems that whenever Path Coupling works there is a “natural” notion of adjacency and
a distance metric ﬁxing which gives a rather easy proof of rapid mixing. For several problems
for which the natural choice for these notions does not work, no known Coupling based proof
seems to be in sight as well. It will be interesting to shed some light on this, and investigate how
one may make Coupling work when most natural choices for doing Path Coupling do not work
out.

• Finally there are several questions still open about designing and analyzing rapidly mixing

Markov chains for speciﬁc sampling problems. Some of our favorite ones are:

– Bipartite graphs with a given degree sequence (for sampling regular bipartite graphs, a
rapidly mixing Markov Chain was given in [25]). More generally, contingency tables with
given row and column sums (the 2× n case was solved in [13] using Path Coupling).
– Independent sets in graphs with maximum degree 5. (The case D ≤ 4 has been considered
in [29, 15], and a “negative” result for D ≥ 6 appears in [10].)
– Proper k-colorings of a graph when k < 11
.
6
– Perfect matchings in a general bipartite graph.

References

[1] D. Aldous. Random walks on ﬁnite groups and rapidly mixing Markov chains. S´eminnaire de

Probabilit´es XVII 1981/82, Springer Lecture Notes in Mathematics 986, 1983, pp. 243-297.

[2] D. Aldous. Some inequalities for reversible Markov chains. Journal of the London Mathematical

Society, 25 (1982), pp. 564-576.

33

D
[3] N. Alon. Eigenvalues and expanders. Combinatorica, 6 (1986), pp. 83-96.

[4] A. Broder. How hard is it to marry at random? (On the approximation of the permanent). Proc. of

18th STOC, pp. 50-58, 1986.

[5] R. Bubley and M. Dyer. Path coupling: a technique for proving rapid mixing in Markov chains.

Proc. of 38th FOCS, pp. 223-231, 1997.

[6] R. Bubley and M. Dyer. Faster random generation of linear extensions. Proc. of the 9th ACM

Symposium on Discrete Algorithms, pp. 35-354, 1998.

[7] R. Bubley, M. Dyer and M. Jerrum. An elementary analysis of a procedure for sampling points in

a convex body. Random Structures and Algorithms, 12 (1998), pp. 213-235.

[8] P. Diaconis and D. Strook. Geometric bounds for eigenvalues of Markov chains. Annals of Applied

Probability, 1 (1991), pp. 36-61.

[9] M. Dyer and A. Frieze. Computing the volume of convex bodies: a case where randomness prov-
ably helps. In Probabilistic Combinatorics and its Applications, Proc. of AMS Symposia in Applied
Mathematics, 44 (1991), pp. 123-170.

[10] M. Dyer, A. Frieze and M. Jerrum. On counting independent sets in sparse graphs. Proc. of 40th

FOCS, pp. 210-217, 1999.

[11] M. Dyer, A. Frieze and R. Kannan. A random polynomial time algorithm for approximating the

volume of convex bodies. Journal of the ACM, 38 (1991), pp. 1-17.

[12] M. Dyer, A. Frieze, R. Kannan, A. Kapoor, L. Perkovic and U. Vazirani. A sub-exponential time
algorithm for approximating the number of solutions to a multidimensional knapsack problem.
Combinatorics, Probability and Computing, 2 (1993), pp. 271-284.

[13] M. Dyer and C. Greenhill. A genuinely polynomial-time algorithm for sampling two-rowed con-
tingency tables. Proc. of the 25th International Colloquium on Automata, Languages and Pro-
gramming (ICALP), pp. 339-350, Aalborg, Denmark (1998),

[14] M. Dyer and C. Greenhill, A more rapidly mixing Markov chain for graph colourings. Random

Structures and Algorithms, 13 (1998), pp. 285-317.

[15] M. Dyer and C. Greenhill. On Markov chains for independent sets. Journal of Algorithms, 35

(2000), pp. 17 - 49.

[16] B. Hajek. Hitting time and occupation time bounds implied by drift analysis with applications.

Advances in Applied Probability, 14 (1982), pp. 502-525.

[17] T. Feder and M. Mihail. Balanced Matroids. Proc. of 24th STOC, pp. 26-38, 1992.

[18] M. Jerrum. A very simple algorithm for estimating the number of k-colourings of a low-degree

graph. Random Structures and Algorithms, 7 (1995), pp. 157-165.

[19] M. Jerrum. Mathematical foundations of the Markov chain Monte Carlo method. In Probabilis-
tic Methods for Algorithmic Discrete Mathematics, Algorithms and Combinatorics 16, Springer-
Verlag, 1998, pp. 116-165.

34

[20] M. Jerrum and A. Sinclair. Approximating the permanent. SIAM Journal on Computing, 18

(1989), pp. 1149-1178.

[21] M. Jerrum and A. Sinclair. Polynomial-time approximation algorithms for the Ising model. SIAM

Journal on Computing, 22 (1993), pp. 1087-1116.

[22] M. Jerrum and A. Sinclair. The Markov chain Monte Carlo method: an approach to approximate
counting and integration. In Approximation Algorithms for NP-hard problems, D.S Hochbaum
ed., PWS Publishing, Boston, 1997, pp. 482-520.

[23] M. Jerrum, L. G. Valiant and V. V. Vazirani. Random generation of combinatorial structures from

a uniform distribution. Theoretical Computer Science, 43 (1986), pp. 169-188.

[24] N. Kahale. A semideﬁnite bound for mixing rates of Markov chains. DIMACS Technical Report

95-41, September 1995.

[25] R. Kannan, P. Tetali and S. Vempala. Simple Markov chain algorithms for generating bipartite

graphs and tournaments. Proc. of 8th SODA, 1997.

[26] A. Karzanov and L. Khachiyan. On the conductance of order Markov chains, Technical Report

DCS 268, Rutgers University, June 1990.

[27] V.S. Anil Kumar and H. Ramesh. Coupling vs. conductance for the Jerrum-Sinclair chain. Proc.

of 40th FOCS, pp. 241-251, 1999.

[28] T. Leighton and S. Rao. An approximate max-ﬂow min-cut theorem for uniform multicommodity
ﬂow problems with applications to approximation algorithms. Proc. of 29th STOC, pp. 422-431,
1988.

[29] M. Luby and E. Vigoda. Approximately counting up to four. Proc. of 29th STOC, pp. 682-687,

1997.

[30] P. Matthews. Generating random linear extensions of a partial order. The Annals of Probability,

19 (1991), pp. 1367-1392.

[31] M. Mihail. Conductance and convergence of Markov chains: a combinatorial treatment of ex-

panders. Proc. of the 30th FOCS, pp. 526-531, 1989.

[32] M. Mihail. On coupling and the approximation of the permanent. Information Processing Letters,

30 (1989), pp. 91-95.

[33] B. Mohar. Isoperimetric numbers of graphs. Journal of Combinatorial Theory, Series B, 47

(1989), pp. 274-291.

[34] B. Morris and A. Sinclair. Random walks on truncated cubes and sampling 0-1 knapsack solu-

tions. Proc. of 40th FOCS, pp. 230-240, 1999.

[35] P. Raghavan and C. D. Thompson. Randomized rounding: a technique for provably good algo-

rithms and algorithmic proofs. Combinatorica, 7 (1987), pp. 365-374.

[36] A. Sinclair. Algorithms for random generation and counting: a Markov chain approach. Ph.D

thesis, University of Edinburgh, June 1988.

35

[37] A. Sinclair. Improved bounds for mixing rates of Markov chains and multicommodity ﬂow. Com-

binatorics, Probability and Computing, 1 (1992), pp. 351-370.

[38] A. Sinclair and M. Jerrum. Approximate counting, uniform generation, and rapidly mixing

Markov chains. Information and Computation, 82 (1989), pp. 93-133.

[39] S. Vadhan. Rapidly mixing Markov chains and their applications. Essay, Churchill College, Cam-

bridge University, May 1996.

[40] E. Vigoda. Improved bounds for sampling colorings. Proc. of 40th FOCS, pp. 51-59, 1999.

36

