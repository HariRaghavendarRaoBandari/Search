6
1
0
2

 
r
a

M
5

 

 
 
]
L
M

.
t
a
t
s
[
 
 

1
v
0
0
7
1
0

.

3
0
6
1
:
v
i
X
r
a

HIGH-DIMENSIONAL METRICS IN R

VICTOR CHERNOZHUKOV, CHRISTIAN HANSEN, MARTIN SPINDLER

Abstract. The package High-dimensional Metrics (hdm) is an evolving collection of statistical meth-

ods for estimation and quantiﬁcation of uncertainty in high-dimensional approximately sparse models.

It focuses on providing semi-parametrically eﬃcient estimators, conﬁdence intervals, and signiﬁcance

testing for low-dimensional subcomponents of the high-dimensional parameter vector. This vignette

oﬀers a brief introduction and a tutorial to the implemented methods. R and the package hdm are open-

source software projects and can be freely downloaded from CRAN: http://cran.r-project.org.

Contents

Introduction

1.
2. How to get started

3. Prediction using Approximate Sparsity

3.1. Prediction in Linear Models using Approximate Sparsity
R implementation

Example
4.

Inference on Target Regression Coeﬃcients

4.1.
4.2.

Intuition for the Orthogonality Principle in Linear Models via Partialling Out
Inference: Conﬁdence Intervals and Signiﬁcance Testing

4.3. Application: Estimation of the treatment eﬀect in a linear model with many confounding

factors

Instrumental Variable Estimation in a High-Dimensional Setting

5.
5.1. Estimation and Inference

R Implementation
5.2. Application: Economic Development and Institutions

5.3. Application: Impact of Eminent Domain Decisions on Economic Outcomes
6.

Inference on Treatment Eﬀects in a High-Dimensional Setting

6.1. Treatment Eﬀects Parameters – a short Introduction

6.2. Estimation and Inference of Treatment eﬀects
R Implementation

6.3. Application: 401(k) plan participation
7. Conclusion

Appendix A. Data Sets

Version: March 8, 2016.

1

2
3

3

3
6

6
7

8
11

12

14
14

15
15

17
18

18

19
20

20
22

23

2

High-Dimensional Metrics in R

A.1. Pension Data

A.2. Growth Data
A.3.

Institutions and Economic Development – Data on Settler Mortality

A.4. Data on Eminent Domain
References

23

23
24

24
25

1. Introduction

Analysis of high-dimensional models, models in which the number of parameters to be estimated is

large relative to the sample size, is becoming increasingly important. Such models arise naturally in
modern data sets which have many measured characteristics available per individual observation as in,

for example, population census data, scanner data, and text data. Such models also arise naturally even
in data with a small number of measured characteristics in situations where the exact functional form

with which the observed variables enter the model is unknown and we create many technical variables,

a dictionary, from the raw characteristics. Examples covered by this scenario include semiparametric
models with nonparametric nuisance functions. More generally, models with many parameters relative

to the sample size often arise when attempting to model complex phenomena.
With increasing availability of such data sets in economics and other data science ﬁelds, new methods

for analyzing those data have been developed. The R package hdm contains implementations of recently
developed methods for high-dimensional approximately sparse models, mainly relying on forms of lasso

and post-lasso as well as related estimation and inference methods. The methods are illustrated with
econometric applications, but are also useful in other disciplines such as medicine, biology, sociology or

psychology.
The methods which are implemented in this package are distinct from already available methods in

other packages in the following three major ways:

1) First, we provide eﬃcient estimators and uniformly valid conﬁdence intervals for various low-
dimensional causal/structural parameters appearing in high-dimensional approximately sparse

models. For example, we provide eﬃcient estimators and uniformly valid conﬁdence intervals
for a regression coeﬃcient on a target variable (e.g., a treatment or policy variable) in a high-

dimensional sparse regression model. We also provide estimates and conﬁdence intervals for
average treatment eﬀect (ATE) and average treatment eﬀect for the treated (ATET), as well

extensions of these parameters to the endogenous setting.

2) Second, we provide theoretically grounded, data-driven choice of the penalty level λ in the lasso

method. Because of this, we call the resulting method the “rigorous” lasso (=rlasso). The

preﬁx r in function names should underscore this. In high-dimensions setting cross-validation
is very popular, but it lacks a theoretical justiﬁcation and some theoretical proposals for the

choice of λ are often not feasible.

3) Third, we provide a version of lasso for linear regressions speciﬁcally that expressly handles

non-Gaussian and heteroskedastic errors.

2. How to get started

R is an open source software project and can be freely downloaded from the CRAN website along with
its associated documentation. The R package hdm can be downloaded from cran.r-project.org. To

3

install the hdm package from R we simply type,

install.packages("hdm")

The most current version of the package (development version) is maintained at R-Forge and can

installed by

install.packages("hdm", repos="http://R-Forge.R-project.org")

Provided that your machine has a proper internet connection and you have write permission in the
appropriate system directories, the installation of the package should proceed automatically. Once the

hdm package is installed, it can be loaded to the current R session by the command,

library(hdm)

Online help is available in two ways. For example, you can type:

help(package="hdm")

help(rlasso)

The former command gives an overview over the available commands in the package, and the latter
gives detailed information about a speciﬁc command.

More generally one can initiate a web-browser help session with the command,

help.start()

and navigate as desired. The browser approach is better adapted to exploratory inquiries, while the

command line approach is better suited to conﬁrmatory ones.
A valuable feature of R help ﬁles is that the examples used to illustrate commands are executable, so

they can be pasted into an R session or run as a group with a command like,

example(rlasso)

3. Prediction using Approximate Sparsity

3.1. Prediction in Linear Models using Approximate Sparsity. Consider high dimensional ap-

proximately sparse linear regression models. These models have a large number of regressors p, possibly
much larger than the sample size n, but only a relatively small number s = o(n) of these regressors are

important for capturing accurately the main features of the regression function. The latter assumption

4

High-Dimensional Metrics in R

makes it possible to estimate these models eﬀectively by searching for approximately the right set of

regressors.
The model reads

yi = x′

iβ0 + εi, E[εixi] = 0,

β0 ∈ Rp,

i = 1, . . . , n

where yi are observations of the response variable, xi = (xi,j , . . . , xi,p)’s are observations of p−dimensional
regressors, and εi’s are centered disturbances, where possibly p ≫ n. Assume that the data sequence is
i.i.d. for the sake of exposition, although the framework covered is considerably more general. An impor-
tant point is that the errors εi may be non-Gaussian or heteroskedastic (Belloni, Chen, Chernozhukov, and Hansen,
2012).

The model can be exactly sparse, namely

kβ0k0 ≤ s = o(n),

or approximately sparse, namely that the values of coeﬃcients, sorted in decreasing order, (|β0|(j))p
obey,

j=1

|β0|(j) ≤ Aj−a(β0),

a(β0) > 1/2,

j = 1, ..., p.

An approximately sparse model can be well-approximated by an exactly sparse model with sparsity
index

In order to get theoretically justiﬁed performance guarantees, we consider the Lasso estimator with

data-driven penalty loadings:

s ∝ n1/(2a(β0)).

ˆβ = arg min
β∈Rp

En[(yi − x′

iβ)2] +

λ
n|| ˆΨβ||1

where ||β||1 = Pp
j=1 |βj|, ˆΨ = diag( ˆψ1, . . . , ˆψp) is a diagonal matrix consisting of penalty loadings, and
En abbreviates the empirical average. The penalty loadings are chosen to insure basic equivariance of
coeﬃcient estimates to rescaling of xi,j and can also be chosen to address heteroskedastiticy in model
errors. We discuss the choice of λ and ˆΨ below.
Regularization by the ℓ1-norm naturally helps the Lasso estimator to avoid overﬁtting, but it also
shrinks the ﬁtted coeﬃcients towards zero, causing a potentially signiﬁcant bias. In order to remove
some of this bias, consider the Post-Lasso estimator that applies ordinary least squares to the model ˆT
selected by Lasso, formally,

The Post-Lasso estimate is then deﬁned as

ˆT = support( ˆβ) = {j ∈ {1, . . . , p} : | ˆβ| > 0}.

˜β ∈ arg min

β∈Rp

En


yi −

p

X

j=1

xi,j βj

2




: βj = 0

if ˆβj = 0,

∀j.

In words, the estimator is ordinary least squares applied to the data after removing the regressors that
were not selected by Lasso. The Post-Lasso estimator was introduced and analysed in Belloni and Chernozhukov

(2013).

5

A crucial matter is the choice of the penalization parameter λ. With the right choice of the penalty

level, Lasso and Post-Lasso estimators possess excellent performance guarantees: They both achieve
the near-oracle rate for estimating the regression function, namely with probability 1 − γ − o(1),

qEn[(x′

i( ˆβ − β0))2] . p(s/n) log p.

In high-dimensions setting, cross-validation is very popular in practice but lacks theoretical justiﬁcation

and so may not provide such a performance guarantee. In sharp contrast, the choice of the penalization
parameter λ in the Lasso and Post-Lasso methods in this package is theoretical grounded and feasible.

Therefore we call the resulting method the “rigorous”Lasso method and hence add a preﬁx r to the
function names.
In the case of homoscedasticity, we set the penalty loadings ˆψj = qEnx2
i,j , which insures basic equivari-
ance properties. There are two choices for penalty level λ: the X-independent choice and X-dependent
choice. In the X-independent choice we set the penalty level to

where Φ denotes the cumulative standard normal distribution, ˆσ is a preliminary estimate of σ = √Eε2,
and c is a theoretical constant, which is set to c = 1.1 by default for the Post-Lasso method and c = .5

λ = 2cˆσΦ−1(1 − γ/(2p)),

for the Lasso method, and γ is the probability level, which is set to γ = .1 by default. The parameter
γ can be interpreted as the probability of mistakenly not removing X’s when all of them have zero

coeﬃcients. In the X-dependent case the penalty level is calculated as

where

λ = 2cˆσΛ(1 − γ|X),

Λ(1 − γ|X) = (1 − γ) − quantile of n||En[xiei]||∞|X,

where X = [x1, . . . , xn]′ and ei are iid N (0, 1), generated independently from X; this quantity is
approximated by simulation. The X-independent penalty is more conservative than the X-dependent

penalty. In particular the X-dependent penalty automatically adapts to highly correlated designs, using
less aggressive penalization in this case Belloni, Chernozhukov, and Hansen (2010).
In the case of heteroskedasticity, the loadings are set to ˆψj = qEn[x2
i ], where ˆεi are preliminary esti-
mates of the errors. The penalty level can be X-independent (Belloni, Chen, Chernozhukov, and Hansen,
2012):

ij ˆε2

λ = 2c√nΦ−1(1 − γ/(2p)),

or it can be X-dependent and estimated by a multiplier bootstrap procedure (Chernozhukov, Chetverikov, and Kato,

2013):

λ = c × cW (1 − γ),

where cW (1 − γ) is the 1 − γ-quantile of the random variable W , conditional on the data, where

W := √n max

1≤j≤p |2En[xij ˆεiei]|,

6

High-Dimensional Metrics in R

where ei are iid standard normal variables distributed independently from the data, and ˆεi denotes an
estimate of the residuals.
estimation proceeds by iteration. The estimates of residuals ˆεi are initialized by running least squares
of yi on ﬁve regressors that are most correlated to yi. This implies conservative starting values for λ and
the penalty loadings, and leads to the initial Lasso and Post-Lasso estimates, which are then further

updated by iteration. The resulting iterative procedure is fully justiﬁed in the theoretical literature.

R implementation. The function rlasso implements Lasso and post-Lasso, where the preﬁx “r”

signiﬁes that these are theoretically rigorous versions of Lasso and post-Lasso. The default option
is post-Lasso, post=TRUE. This function returns an object of S3 class rlasso for which methods like

predict, print, summary are provided.
lassoShooting.fit is the computational algorithm that underlies the estimation procedure, which

implements a version of the Shooting Lasso Algorithm (Fu, 1998). The user has several options for
choosing the non-default options. Speciﬁcally, the user can decide if an unpenalized intercept should

be included (TRUE by default). The option penalty of the function rlasso allows diﬀerent choices
for the penalization parameter and loadings. It allows for homoscedastic or heterosceastic errors with

default homoscedastic = FALSE. Moreover, the dependence structure of the design matrix might be

taken into consideration for calculation of the penalization parameter with X.dependent.lambda =
TRUE. In combination with these options, the option lambda.start allows the user to set a starting

value for λ for the diﬀerent algorithms. Moreover, the user can provide her own ﬁxed value for the
penalty level – instead of the data-driven methods discussed above – by setting homoscedastic =

"none" and supplying the value via lambda.start.
The constants c and γ from above can be set in the option penalty. The quantities ˆε, ˆΨ, ˆσ are calculated
in a iterative manner. The maximum number of iterations and the tolerance when the algorithms should
stop can be set with control.

Example. (Prediction Using Lasso and Post-Lasso) Consider generated data from a sparse linear
model:

set.seed(1)

n = 100 #sample size

p = 100 # number of variables

s = 3 # nubmer of variables with non-zero coefficients

X = matrix(rnorm(n*p), ncol=p)

beta = c(rep(5,s), rep(0,p-s))

Y = X%*%beta + rnorm(n)

Next we estimate the model, print the results, and make in-sample and out-of sample predictions. We
can use methods print and summarize to print the results, where the option all can be set to FALSE

to limit the print only to the non-zero coeﬃcients.

lasso.reg = rlasso(Y~X,post=FALSE) # use lasso, not-Post-lasso

print(lasso.reg, all=FALSE) # can also do summary(lasso.reg, all=FALSE)

7

##

## Call:

## rlasso(formula = Y ~ X, post = FALSE)

##

##

X1

X2

X3

X83

## 4.66808

4.81418

4.77023 -0.02741

yhat.lasso = predict(lasso.reg)

#in-sample prediction

Xnew = matrix(rnorm(n*p), ncol=p) # new X

Ynew =

Xnew%*%beta + rnorm(n) #new Y

yhat.lasso.new = predict(lasso.reg, newdata=Xnew) #out-of-sample prediction

post.lasso.reg = rlasso(Y~X,post=TRUE) #now use post-lasso

print(post.lasso.reg, all=FALSE)

# or use summary(post.lasso.reg, all=FALSE)

##

## Call:

## rlasso(formula = Y ~ X, post = TRUE)

##

##

X1

X2

X3

## 4.945 5.049 4.984

yhat.postlasso = predict(post.lasso.reg) #in-sample prediction

yhat.postlasso.new = predict(post.lasso.reg, newdata=Xnew) #out-of-sample prediction

MAE<- apply(cbind(abs(Ynew-yhat.lasso.new), abs(Ynew - yhat.postlasso.new)),2, mean)

names(MAE)<- c("lasso MAE", "Post-lasso MAE")

print(MAE, digits=2) # MAE for Lasso and Post-Lasso

##

##

lasso MAE Post-lasso MAE

0.85

0.77

4. Inference on Target Regression Coefficients

Here we consider inference on the target coeﬃcient α in the model:

yi = diα0 + x′

iβ0 + ǫi, Eǫi(x′

i, d′

i)′ = 0.

Here di is a target regressor such as treatment, policy or other variable whose regression coeﬃcient α0
we would like to learn (Belloni, Chernozhukov, and Hansen, 2014). If we are interested in coeﬃcients of
several or even many variables, we can simply write the model in the above form treating each variable
of interest as di in turn and then applying the estimation and inference procedures described below.

8

High-Dimensional Metrics in R

We assume approximate sparsity for x′

β0, namely a(β0) > 1. This condition translates into having a sparsity index s ≪ √n. In general di is

correlated to xi, so α0 cannot be consistently estimated by the regression of yi on di. To keep track of
the relationship of di to xi, write

iβ0 with suﬃcient speed of decay of the sorted components of

di = x′

iπd

0 + ρd

i , Eρd

i xi = 0.

0 , namely a(πd

To estimate α0, we also impose approximate sparsity on the regression function x′
speed of decay of sorted components of πd
The Orthogonality Principle. Note that we can not use naive estimates of α0 based simply on
applying Lasso and Post-Lasso to the ﬁrst equation. Such a strategy in general does not produce root-n
consistent and asymptotically normal estimators of α, due to the possibility of large omitted variable bias
resulting from estimating the nuisance function x′
iβ0 in high-dimensional setting. In order to overcome
the omitted variable bias, we need to use orthogonalized estimating equations for α0. Speciﬁcally we
seek to ﬁnd a score ψ(wi, α, η), where wi = (yi, x′

i)′ and η is the nuisance parameter, such that

0 with suﬃcient

0 ) > 1.

iπd

Eψ(wi, α0, η0) = 0,

∂
∂η

Eψ(wi, α0, η0) = 0.

The second equation is the orthogonality condition, which states that the equations are not sensitive
to the ﬁrst-order perturbations of the nuisance parameter η near the true value. The latter property
allows estimation of these nuisance parameters η0 by regularized estimators ˆη, where regularization is
done via penalization or selection. Without this property, regularization may have too much eﬀect on
the estimator of α0 for regular inference to proceed.
The estimators ˆα of α0 solve the empirical analog of the equation above,

where we have plugged in the estimator ˆη for the nuisance parameter. Due to the orthogonality property
the estimator is ﬁrst-order equivalent to the infeasible estimator ˜α solving

Enψ(wi, ˆα, ˆη) = 0,

Enψ(wi, ˜α, η0) = 0,

where we use the true value of the nuisance parameter. The equivalence holds in a variety of models
under plausible conditions. The systematic development of the orthogonality condition for inference on

low-dimensional parameters in modern high-dimensional settings is given in Chernozhukov, Hansen, and Spindler
(2015a).

In turns out that in the linear model the orthogonal equations are closely connected to the classical
ideas of partialling out.

4.1. Intuition for the Orthogonality Principle in Linear Models via Partialling Out. One
way to think about estimation of α0 is to think of the regression model:

where ρy
is the
i
residual that is left after partialling out the linear eﬀect of xi from di, both done in the population.

is the residual that is left after partialling out the linear eﬀect of xi from yi and ρd
i

ρy
i = α0ρd

i + ǫi,

Note that we have Eρy
partialling out, α0 is the population regression coeﬃcient in the univariate regression of ρy
is the Frisch-Waugh-Lovell theorem. Thus, α = α0 solves the population equation:

0 is the linear projection of yi on xi. After
i . This

i xi = 0, i.e. ρy

i = yi − x′

0 where x′

i on ρd

iπy

iπy

9

The score associated to this equation is:

E(ρy

i − αρd

i )ρd

i = 0.

ψ(wi, α, η) = (yi − x′

iπy) − α(di − x′
i − αρd

iπd))(di − x′
i )ρd
i ,

iπd),
η0 = (πy ′

ψ(wi, α0, η0) = (ρy

η = (πy ′

, πd′

)′,

0 , πd′

0 ).

It is straightforward to check that this score obeys the orthogonality principle; moreover, this score is
the semi-parametrically eﬃcient score for estimating the regression coeﬃcient α0.
In low-dimensional settings, the empirical version of the partialling out approach is simply another way
to do the least squares. Let’s verify this in an example. First, we generate some data

set.seed(1)

n =5000; p = 20; X = matrix(rnorm(n*p), ncol=p)

colnames(X) = c("d", paste("x", 1:19, sep=""));xnames = colnames(X)[-1]

beta = rep(1,20)

y = X%*%beta + rnorm(n)

dat = data.frame(y=y, X)

We can estimate α0 by running full least squares:

# full fit

fmla = as.formula(paste("y ~ ", paste(colnames(X), collapse= "+")))

full.fit= lm(fmla, data=dat)

summary(full.fit)$coef["d",1:2]

##

Estimate Std. Error

## 0.97807455 0.01371225

Another way to estimate α0 is to ﬁrst partial out the x-variables from yi and di, and run least squares
on the residuals:

fmla.y = as.formula(paste("y ~ ", paste(xnames, collapse= "+")))

fmla.d = as.formula(paste("d ~ ", paste(xnames, collapse= "+")))

# partial fit via ols

rY = lm(fmla.y, data = dat)$res

rD = lm(fmla.d, data = dat)$res

partial.fit.ls= lm(rY~rD)

summary(partial.fit.ls)$coef["rD",1:2]

##

Estimate Std. Error

## 0.97807455 0.01368616

10

High-Dimensional Metrics in R

One can see that the estimates are identical, while standard errors are nearly identical. In fact, the stan-

dard errors are asymptotically equivalent due to the orthogonality property of the estimating equations
associated with the partialling out approach.

In high-dimensional settings, we can no longer rely on the full least-squares and instead may rely on
Lasso or Post-Lasso for partialling out. This amounts to using orthogonal estimating equations, where

we estimate the nuisance parameters by Lasso or Post-Lasso. Let’s try this in the above example, using
Post-Lasso for partialling out:

# partial fit via post-lasso

rY = rlasso(fmla.y, data =dat)$res

rD = rlasso(fmla.d, data =dat)$res

partial.fit.postlasso= lm(rY~rD)

summary(partial.fit.postlasso)$coef["rD",1:2]

##

Estimate Std. Error

## 0.97273870 0.01368677

We see that this estimate and standard errors are nearly identical to the previous estimates given above.

In fact they are asymptotically equivalent to the previous estimates in the low-dimensional settings, with
the advantage that, unlike the previous estimates, they will continue to be valid in the high-dimensional

settings.
The orthogonal estimating equations method – based on partialling out via Lasso or post-Lasso – is

implemented by the function rlassoEffect, using method= "partialling out":

Eff= rlassoEffect(X[,-1],y,X[,1], method="partialling out")

summary(Eff)$coef[,1:2]

## Estimate. Std. Error

## 0.97273870 0.01368677

Another orthogonal estimating equations method – based on the double selection of covariates – is
implemented by the the function rlassoEffect, using method= "double selection". Algorithmically

the method is as follows:

(1) Select controls xij ’s that predict yi by Lasso.
(2) Select controls xij ’s that predict di by Lasso.
(3) Run OLS of yi on di and the union of controls selected in steps 1 and 2.

Eff= rlassoEffect(X[,-1],y,X[,1], method="double selection")

summary(Eff)$coef[,1:2]

## Estimate. Std. Error

## 0.97807455 0.01415624

11

4.2. Inference: Conﬁdence Intervals and Signiﬁcance Testing. The function rlassoEffects

does inference – namely construction of conﬁdence intervals and signiﬁcance testing – for target vari-
ables. Those can be speciﬁed either by the variable names, an integer valued vector giving their position

in x or by a logical vector indicating the variables for which inference should be conducted. It returns
an object of S3 class rlassoEffect for which the methods summary, print, confint, and plot are

provided. rlassoEffects is a wrap function for the function rlassoEffect which does inference for
a single target regressor. The theoretical underpinning is given in Belloni, Chernozhukov, and Hansen

(2014) and for a more general class of models in Belloni, Chernozhukov, and Kato (2014).
Here is an example of the use.

set.seed(1)

n = 100 #sample size

p = 100 # number of variables

s = 3 # nubmer of non-zero variables

X = matrix(rnorm(n*p), ncol=p)

beta = c(rep(3,s), rep(0,p-s))

y = 1 + X%*%beta + rnorm(n)

We can do inference on a set of variables of interest, e.g. the ﬁrst, second, third, and the ﬁftieth:

lasso.effect = rlassoEffects(x=X, y=y, index=c(1,2,3,50))

print(lasso.effect)

##

## Call:

## rlassoEffects(x = X, y = y, index = c(1, 2, 3, 50))

##

## Coefficients:

##

V1

V2

V3

V50

## 2.94448 3.04127 2.97540 0.07196

summary(lasso.effect)

## [1] "Estimates and significance testing of the effect of target variables"

##

Estimate. Std. Error t value Pr(>|t|)

## V1

## V2

## V3

2.94448

0.08815 33.404

<2e-16 ***

3.04127

0.08389 36.253

<2e-16 ***

2.97540

0.07804 38.127

<2e-16 ***

## V50

0.07196

0.07765

0.927

0.354

## ---

## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

confint(lasso.effect)

##

2.5 %

97.5 %

12

High-Dimensional Metrics in R

## V1

2.77171308 3.1172421

## V2

2.87685121 3.2056979

## V3

2.82244962 3.1283583

## V50 -0.08022708 0.2241377

The two methods are ﬁrst-order equivalent in both low-dimensional and high-dimensional settings under

regularity conditions. Not surprisingly we see that in the numerical example of this section, the estimates

and standard errors produced by the two methods are very close to each other.
For logistic regression we provide the functions rlassologit and rlassologitEffects. Further infor-

mation can be found in the help.

4.3. Application: Estimation of the treatment eﬀect in a linear model with many confound-

ing factors. A part of empirical growth literature has focused on estimating the eﬀect of an initial
(lagged) level of GDP (Gross Domestic Product) per capita on the growth rates of GDP per capita. In

particular, a key prediction from the classical Solow-Swan-Ramsey growth model is the hypothesis of
convergence, which states that poorer countries should typically grow faster and therefore should tend

to catch up with the richer countries, conditional on a set of institutional and societal characteristics.
Covariates that describe such characteristics include variables measuring education and science policies,

strength of market institutions, trade openness, savings rates and others.

Thus, we are interested in a speciﬁcation of the form:

yi = α0di +

p

X

j=1

βjxij + εi,

where yi is the growth rate of GDP over a speciﬁed decade in country i, di is the log of the initial level of
GDP at the beginning of the speciﬁed period, and the xij ’s form a long list of country i’s characteristics
at the beginning of the speciﬁed period. We are interested in testing the hypothesis of convergence,
namely that α1 < 0. Given that in the Barro and Lee (1994) data, the number of covariates p is large
relative to the sample size n, covariate selection becomes a crucial issue in this analysis. We employ

here the partialling out approach (as well as the double selection version) introduced in the previous
section.

First, we load and prepare the data

data(GrowthData)

dim(GrowthData)

## [1] 90 63

y = GrowthData[,1,drop=F]

d = GrowthData[,3, drop=F]

X = as.matrix(GrowthData)[,-c(1,2,3)]

varnames = colnames(GrowthData)

Now we can estimate the eﬀect of the initial GDP level. First, we estimate by OLS:

13

xnames= varnames[-c(1,2,3)] # names of X variables

dandxnames= varnames[-c(1,2)] # names of D and X variables

# create formulas by pasting names (this saves typing times)

fmla= as.formula(paste("Outcome ~ ", paste(dandxnames, collapse= "+")))

ls.effect= lm(fmla, data=GrowthData)

Second, we estimate the eﬀect by the partialling out by Post-Lasso:

dX = as.matrix(cbind(d,X))

lasso.effect = rlassoEffect(x=X, y=y, d=d, method="partialling out")

summary(lasso.effect)

## [1] "Estimates and significance testing of the effect of target variables"

##

Estimate. Std. Error t value Pr(>|t|)

## [1,] -0.04432

0.01532 -2.893 0.00381 **

## ---

## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Third, we estimate the eﬀect by the double selection method:

dX = as.matrix(cbind(d,X))

doublesel.effect = rlassoEffect(x=X, y=y, d=d, method="double selection")

summary(doublesel.effect)

## [1] "Estimates and significance testing of the effect of target variables"

##

Estimate. Std. Error t value Pr(>|t|)

## xgdpsh465 -0.04432

0.01685 -2.631 0.00851 **

## ---

## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

We then collect results in a nice latex table:

library(xtable)

table= rbind(summary(ls.effect)$coef["gdpsh465",1:2],

summary(lasso.effect)$coef[,1:2],

summary(doublesel.effect)$coef[,1:2])

colnames(table)= c("Estimate", "Std. Error") #names(summary(full.fit)£coef)[1:2]

rownames(table)= c("full reg via ols", "partial reg

via post-lasso ", "partial reg via double selection")

tab= xtable(table, digits=c(2, 2,5))

tab

14

High-Dimensional Metrics in R

full reg via ols

partial reg via post-lasso
partial reg via double selection

Estimate Std. Error

-0.01

-0.04
-0.04

0.02989

0.01532
0.01685

We see that the OLS estimates are noisy, which is not surprising given that p is comparable to n. The

partial regression approaches, based on Lasso selection of covariates in the two projection equations,
in contrast yields much more precise estimates, which does support the hypothesis of conditional con-

vergence. We note that the partial regression approaches represent a special case of the orthogonal
estimating equations approach.

5. Instrumental Variable Estimation in a High-Dimensional Setting

In many applied settings the researcher is interested in estimating the (structural) eﬀect of a variable
(treatment variable), but this variable is endogenous, i.e. correlated with the error term. In this case,

instrumental variables (IV) methods are used for identiﬁcation of the causal parameters.
We consider the linear instrumental variables model:

(1)

(2)

yi = α0di + γ0x′
iΠ + β0x′

di = z′

i + εi,

i + vi,

i, z′

i, z′

i)] = 0, E[vi(x′

i)] = 0, but E[εivi] 6= 0 leading to endogeneity.

where E[εi(x′
In this setting di
is a scalar endogenous variable of interest, zi is a pz-dimensional vector of instruments and xi is a
px-dimensional vector of control variables.
In this section we present methods to estimate the eﬀect α0 in a setting where either x is high-
dimensional or z is high-dimensional. Instrumental variables estimation with very many instruments
was analysed in Belloni, Chen, Chernozhukov, and Hansen (2012), the extension with many instruments

and many controls in Chernozhukov, Hansen, and Spindler (2015b).

5.1. Estimation and Inference. To get eﬃcient estimators and uniformly valid conﬁdence intervals
for the structural parameters there are diﬀerent strategies which are asymptotically equivalent where

again orthogonalization (via partialling out) is a key concept.
In the case of the high-dimensional instrument zi and low-dimensional xi. We predict the endogenous
variable di using (Post-)Lasso regression of di on the instruments zi and xi, forcing the inclusion of
xi. Then we compute the IV estimator (2SLS) ˆα of the parameter α0 using the predicted value ˆdi
as instrument and using xi’s as controls. We then perform inference on α0 using ˆα and conventional
heteroskedasticity robust standard errors.
In the case of the low-dimensional instrument zi and high-dimensional xi, we ﬁrst partial out the
eﬀect of xi from di, yi, and zi by (Post-)Lasso. Second, we then use the residuals to compute the IV
estimator (2SLS) ˆα of the parameter α0. We then perform inference on α0 using ˆα and conventional
heteroskedasticity robust standard errors.

15

In the case of the high-dimensional instrument zi and high-dimensional xi the algorithm described in
Chernozhukov, Hansen, and Spindler (2015b) is adopted.

R Implementation. The wrap function rlassoIV handles all of the previous cases. It has the options
select.X and select.Z which implement selection of either covariates or instruments, both with default

values set to TRUE. The class of the return object depends on the chosen options, but the methods
summary, print and confint are available for all. The functions rlassoSelectX and rlassoSelectZ

do selection on x-variables only and z-variables only. Selection on both is done in rlassoIV. All
functions employ the option post=TRUE as default, which uses post-Lasso for partialling out. By setting

post=FALSE we can employ Lasso instead of Post-Lasso. Finally, the package provides the standard

function tsls, which implements the “classical” two-stage least squares estimation.

5.2. Application: Economic Development and Institutions. Estimating the causal eﬀect of in-
stitutions on output is complicated by the simultaneity between institutions and output: speciﬁcally,

better institutions may lead to higher incomes, but higher incomes may also lead to the development of
better institutions. To help overcome this simultaneity, Acemoglu, Johnson, and Robinson (2001) use

mortality rates for early European settlers as an instrument for institution quality. The validity of this
instrument hinges on the argument that settlers set up better institutions in places where they are more

likely to establish long-term settlements, that where they are likely to settle for the long term is related
to settler mortality at the time of initial colonization, and that institutions are highly persistent. The

exclusion restriction for the instrumental variable is then motivated by the argument that GDP, while

persistent, is unlikely to be strongly inﬂuenced by mortality in the previous century, or earlier, except
through institutions.

In this application, we consider the problem of selecting controls. The input raw controls are the
Latitude and the continental dummies. The technical controls can include various polynomial transfor-

mations of the Latitude, possibly interacted with the continental dummies. Such ﬂexible speciﬁcations
of raw controls results in the high-dimensional x, with dimension comparable to the sample size.

First, we process the data

data(AJR); y = AJR$GDP; d = AJR$Exprop; z = AJR$logMort

x = model.matrix(~ -1 + (Latitude + Latitude2 + Africa +

Asia + Namer + Samer)^2, data=AJR)

dim(x)

## [1] 64 21

Then we estimate an IV model with selection on the X

AJR.Xselect = rlassoIV(x=x, d=d, y=y, z=z, select.X=TRUE, select.Z=FALSE)

summary(AJR.Xselect)

## [1] "Estimation and significance testing of the effect of target variables in the IV regression model"

##

coeff.

se. t-value p-value

16

High-Dimensional Metrics in R

## d1 0.771 0.203

3.798 0.000146 ***

## ---

## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

confint(AJR.Xselect)

##

2.5 %

97.5 %

## d1 0.3731934 1.168885

It is interesting to understand what the procedure above is doing. In essence, it partials out xi from
yi, di and zi using Post-Lasso and applies the 2SLS to the residualized quantities.
Let us investigate partialling out in more detail in this example. We can ﬁrst try to use OLS for
partialling out:

# parialling out by linear model

fmla.y = GDP ~ (Latitude + Latitude2 + Africa + Asia + Namer + Samer)^2

fmla.d = Exprop ~ (Latitude + Latitude2 + Africa + Asia + Namer + Samer)^2

fmla.z = logMort ~ (Latitude + Latitude2 + Africa + Asia + Namer + Samer)^2

rY = lm(fmla.y, data = AJR)$res

rD = lm(fmla.d, data = AJR)$res

rZ = lm(fmla.z, data = AJR)$res

ivfit.lm = tsls(y=rY,d=rD, x=NULL, z=rZ, intercept=FALSE)

print(cbind(ivfit.lm$coef, ivfit.lm$se),digits=3)

##

[,1] [,2]

## d1 1.27 1.73

We see that the estimates exhibit large standard errors. The imprecision is expected because dimension
of x is quite large, comparable to the sample size.

Next, we replace the OLS operator by post-Lasso for partialling out

# parialling out by lasso

rY = rlasso(fmla.y, data = AJR)$res

rD = rlasso(fmla.d, data = AJR)$res

rZ = rlasso(fmla.z, data = AJR)$res

ivfit.lasso = tsls(y=rY,d=rD, x=NULL, z=rZ, intercept=FALSE)

print(cbind(ivfit.lasso$coef, ivfit.lasso$se), digits=3)

##

[,1] [,2]

## d1 0.771 0.203

This is exactly what command rlassoIV is doing by calling the command rlassoSelectX, so the
numbers we see are identical to those reported ﬁrst. In comparison to OLS results, we see precision is

improved by doing selection on the exogenous variables.

17

5.3. Application: Impact of Eminent Domain Decisions on Economic Outcomes. Here we

investigate the eﬀect of pro-plaintiﬀ decisions in cases of eminent domain (government’s takings of
private property) on economic outcomes. The analysis of the eﬀects of such decisions is complicated by

the possible endogeneity between judicial decisions and potential economic outcomes. To address the
potential endogeneity, we employ an instrumental variables strategy based on the random assignment

of judges to the federal appellate panels that make the decisions. Because judges are randomly assigned
to three-judge panels, the exact identity of the judges and their demographics are randomly assigned

conditional on the distribution of characteristics of federal circuit court judges in a given circuit-year.
Under this random assignment, the characteristics of judges serving on federal appellate panels can only

be related to property prices through the judges’ decisions; thus the judge’s characteristics will plausibly
satisfy the instrumental variable exclusion restriction. For further information on this application and

the data set we refer to Chen and Yeh (2010) and Belloni, Chen, Chernozhukov, and Hansen (2012).
First, we load the data an construct the matrices with the controls (x), instruments (z), outcome (y),

and treatment variables (d). Here we consider regional GDP as the outcome variable.

data(EminentDomain)

z <- EminentDomain$logGDP$z

x <- EminentDomain$logGDP$x

y <- EminentDomain$logGDP$y

d <- EminentDomain$logGDP$d

As mentioned above, y is the economic outcome, the logarithm of the GDP, d the number of pro plaintiﬀ

appellate takings decisions in federal circuit court c and year t, x is a matrix with control variables, and
z is the matrix with instruments. Here we consider socio-economic and demographic characteristics of

the judges as instruments.

First, we estimate the eﬀect of the treatment variable by simple OLS and 2SLS using two instruments:

ED.ols = lm(y~cbind(d,x))

ED.2sls = tsls(y=y, d=d, x=x, z=z[,1:2], intercept=FALSE)

Next, we estimate the model with selection on the instruments.

lasso.IV.Z = rlassoIV(x=x, d=d, y=y, z=z, select.X=FALSE, select.Z=TRUE)

# or lasso.IV.Z = rlassoIVselectZ(x=X, d=d, y=y, z=Z)

summary(lasso.IV.Z)

## [1] "Estimates and significance testing of the effect of target variables in the IV regression model"

##

coeff.

se. t-value p-value

## d1 0.01330 0.01882

0.707

0.48

confint(lasso.IV.Z)

##

2.5 %

97.5 %

18

High-Dimensional Metrics in R

## d1 -0.02358423 0.05018017

Finally, we do selection on both the x and z variables.

lasso.IV.XZ = rlassoIV(x=x, d=d, y=y, z=z, select.X=TRUE, select.Z=TRUE)

summary(lasso.IV.XZ)

## Estimates and Significance Testing of the effect of target variables in the IV regression model

##

coeff.

se. t-value p-value

## d1 -0.005441 0.188125 -0.029

0.977

confint(lasso.IV.XZ)

##

2.5 %

97.5 %

## d1 -0.3741584 0.3632765

Comparing the results we see, that the OLS estimates indicate that the inﬂuence of pro plaintiﬀ appellate
takings decisions in federal circuit court is signiﬁcantly positive, but the 2SLS estimates which account

for the potential endogeneity render the results insigniﬁcant. Employing selection on the instruments
yields similar results. Doing selection on both the x- and z-variables results in extremely imprecise

estimates.
Finally, we compare all results

tab

Estimate Std. Error

ols regression
IV estimation

selection on Z

0.01
0.02

0.01

0.0052848
0.0183376

0.0188178

selection on X and Z

-0.01

0.1881246

6. Inference on Treatment Effects in a High-Dimensional Setting

In this section, we consider estimation and inference on treatment eﬀects when the treatment variable
d enters non-separably in determination of the outcomes. This case is more complicated than the ad-

ditive case, which is covered as a special case of Section 3. However, the same underlying principle
– the orthogonality principle – applies for the estimation and inference on the treatment eﬀect pa-

rameters. Estimation and inference of treatment eﬀects in a high-dimensional setting is analysed in
Belloni, Chernozhukov, Fern´andez-Val, and Hansen (2013).

6.1. Treatment Eﬀects Parameters – a short Introduction. In many situations researchers are

asked to evaluate the eﬀect of a policy intervention. Examples are the eﬀectiveness of a job-related
training program or the eﬀect of a newly developed drug. We consider n units or individuals, i =
1, . . . , n. For each individual we observe the treatment status. The treatment variable Di takes the

19

value 1, if the unit received (active) treatment, and 0, if it received the control treatment. For each

individual we observe the outcome for only one of the two potential treatment states. Hence, the
observed outcome depends on the treatment status and is denoted by Yi(Di).
One important parameter of interest is the average treatment eﬀect (ATE):

E[Y (1) − Y (0)] = E[Y (1)] − E[Y (0)].

This quantity can be interpreted as the average eﬀect of the policy intervention.

Researchers might also be interested in the average treatment eﬀect on the treated (ATET) given by

E[Y (1) − Y (0)|D = 1] = E[Y (1)|D = 1] − E[Y (0)|D = 1].

This is the average treatment eﬀect restricted to the population the treated individuals.
When treatment D is randomly assigned conditional on confounding factors X, the ATE and ATET can

be identiﬁed by regression or propensity score weighting methods. Our identiﬁcation and estimation

method rely on the combination of two methods to create orthogonal estimating equations for these
parameters.1
In observational studies, the potential treatments are endogenous, i.e.
jointly determined with the
outcome variable. In such cases, causal eﬀects may be identiﬁed with the use of a binary instrument

Z that aﬀects the treatment but is independent of the potential outcomes. An important parameter in
this case is the local average treatment eﬀect (LATE):

E[Y (1) − Y (0)|D(1) > D(0)].

The random variables D(1) and D(0) indicate the potential participation decisions under the instrument
states 1 and 0. LATE is the average treatment eﬀect for the subpopulation of compliers – those units

that respond to the change in the instrument. Another parameter of interest is the local average
treatment eﬀect of the treated (LATET):

E[Y (1) − Y (0)|D(1) > D(0), D = 1],

which is the average eﬀect for the subpopulation of the treated compliers.
When the instrument Z is randomly assigned conditional on confounding factors X, the LATE and

LATET can be identiﬁed by instrumental variables regression or propensity score weighting methods.
Our identiﬁcation and estimation method rely on the combination of two methods to create orthogonal

estimating equations for these parameters.

6.2. Estimation and Inference of Treatment eﬀects. We consider the estimation of the eﬀect of

an endogenous binary treatment, D, on an outcome variable, Y , in a setting with very many potential

control variables.
required for the estimation of the LATE and LATET.

In the case of endogeneity, the presence of a binary instrumental variable, Z, is

1It turns out that the orthogonal estimating equations are the same as doubly robust estimating equations, but

emphasizing the name ”doubly robust” could be confusing in the present context.

20

High-Dimensional Metrics in R

When trying to estimate treatment eﬀects, the researcher has to decide what conditioning variables

to include. In the case of a non-randomly assigned treatment or instrumental variable, the researcher
must select the conditioning variables so that the instrument or treatment is plausibly exogenous. Even

in the case of random assignment, for a precise estimation of the policy variable selection of control
variables is necessary to absorb residual variation, but overﬁtting should be avoided. For uniformly

valid post-selection inference, “orthogonal ”estimating equations as described above are they key to the
eﬃcient estimation and valid inference. We refer to Belloni, Chernozhukov, Fern´andez-Val, and Hansen

(2013) for details.

R Implementation. The package contains the functions rlassoATE, rlassoATET, rlassoLATE, and
rlassoLATE to estimate the corresponding treatment eﬀects. All functions have as arguments the out-

come variable y, the treatment variable d, and the conditioning variables x. The functions rlassoLATE,
and rlassoLATE have additionally the argument z for the binary instrumental variable. For the calcu-

lation of the standard errors bootstrap methods are implemented, with options to use Bayes, normal,
or wild bootstrap. The number of repetitions can be speciﬁed with the argument nRep and the default

is set to 500. By default no bootstrap standard errors are provided (bootstrap="none"). For the
functions the logicals intercept and post can be speciﬁed to include an intercept and to do Post-Lasso

at the selection steps. The family of treatment functions returns an object of class rlassoTE for which
the methods print, summary, and confint are available.

6.3. Application: 401(k) plan participation. Though it is clear that 401(k) plans are widely used

as vehicles for retirement saving, their eﬀect on assets is less clear. The key problem in determining
the eﬀect of participation in 401(k) plans on accumulated assets is saver heterogeneity coupled with

nonrandom selection into participation states. In particular, it is generally recognized that some people
have a higher preference for saving than others. Thus, it seems likely that those individuals with the

highest unobserved preference for saving would be most likely to choose to participate in tax-advantaged
retirement savings plans and would also have higher savings in other assets than individuals with lower

unobserved saving propensity. This implies that conventional estimates that do not allow for saver
heterogeneity and selection of the participation state will be biased upward, tending to overstate the

actual savings eﬀects of 401(k) and IRA participation.
Again, we start ﬁrst with the data preparation:

data(pension)

y = pension$tw; d = pension$p401; z = pension$e401

X = pension[,c("i2", "i3", "i4", "i5", "i6", "i7", "a2", "a3", "a4", "a5",

"fsize", "hs", "smcol", "col", "marr", "twoearn", "db", "pira", "hown")]

Now we can compute the estimates of the target treatment eﬀect parameters:

pension.late = rlassoLATE(X,d,y,z)

summary(pension.late)

21

## Estimation and significance testing of the treatment effect

## Type: LATE

## Bootstrap: not applicable

##

coeff.

se. t-value p-value

## TE 12189 2734

4.458 8.27e-06 ***

## ---

## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

pension.latet = rlassoLATET(X,d,y,z)

summary(pension.latet)

## Estimation and significance testing of the treatment effect

## Type: LATET

## Bootstrap: not applicable

##

coeff.

se. t-value p-value

## TE 12687 3590

3.534 0.00041 ***

## ---

## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

pension.ate =

rlassoATE(X,d,y)

summary(pension.ate)

## Estimation and significance testing of the treatment effect

## Type: ATE

## Bootstrap: not applicable

##

coeff.

se. t-value p-value

## TE 10490 1920

5.464 4.67e-08 ***

## ---

## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

pension.atet = rlassoATET(X,d,y)

summary(pension.atet)

## Estimation and significance testing of the treatment effect

## Type: ATET

## Bootstrap: not applicable

##

coeff.

se. t-value p-value

## TE 11810 2844

4.152 3.29e-05 ***

## ---

## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

The results are summarized into a table, which is then processed using xtable to produce the latex

output:

22

High-Dimensional Metrics in R

Estimate Std. Error

ATE 10490.07

ATET 11810.45
LATE 12188.66

LATET 12686.87

1919.99

2844.33
2734.12

3590.09

7. Conclusion

We have provided an introduction to some of the capabilities of the R package hdm. Inevitably, new
applications will demand new features and, as the project is in its initial phase, unforeseen bugs will

show up. In either case comments and suggestions of users are highly appreciated. We shall update the
documentation and the package periodically. The most current version of the R package and its accom-

panying vignette will be made available at the homepage of the maintainer and cran.r-project.org.
See the R command vignette() for details on how to ﬁnd and view vignettes from within R .

Appendix A. Data Sets

23

In this section we describe brieﬂy the data sets which are contained in the package and used afterwards.

They might also be of general interest either for illustrating methods or for classroom presentation.

A.1. Pension Data. In the United States 401(k) plans were introduced to increase private individual
saving for retirement. They allow the individual to deduct contributions from taxable income and al-

low tax-free accrual of interest on assets held within the plan (within an account). Employers provide
401(k) plans, and employers may also match a certain percentage of an employee’s contribution. Because

401(k) plans are provided by employers, only workers in ﬁrms oﬀering plans are eligible for participa-
tion. This data set contains individual level information about 401(k) participation and socio-economic

characteristics.
The data set can be loaded with

data(pension)

A description of the variables and further references are given in Chernozhukov and Hansen (2004) and
at the help page, for this example called by

help(pension)

The sample is drawn from the 1991 Survey of Income and Program Participation (SIPP) and consists

of 9,915 observations. The observational units are household reference persons aged 25-64 and spouse
if present. Households are included in the sample if at least one person is employed and no one is

self-employed. All dollar amounts are in 1991 dollars. The 1991 SIPP reports household ﬁnancial data
across a range of asset categories. These data include a variable for whether a person works for a

ﬁrm that oﬀers a 401(k) plan. Households in which a member works for such a ﬁrm are classiﬁed as
In addition, the survey also records the amount of 401(k) assets. Households
eligible for a 401(k).

with a positive 401(k) balance are classiﬁed as participants, and eligible households with a zero balance

are considered nonparticipants. Available measures of wealth in the 1991 SIPP are total wealth, net
ﬁnancial assets, and net non-401(k) ﬁnancial assets. Net non-401(k) assets are deﬁned as the sum

of checking accounts, U.S. saving bonds, other interest-earning accounts in banks and other ﬁnancial
institutions, other interest-earning assets (such as bonds held personally), stocks and mutual funds less

non-mortgage debt, and IRA balances. Net ﬁnancial assets are net non-401(k) ﬁnancial assets plus
401(k) balances, and total wealth is net ﬁnancial assets plus housing equity and the value of business,

property, and motor vehicles.

A.2. Growth Data. Understanding what drives economic growth, measured in GDP, is a central

question of macroeconomics. A well-known data set with information about GDP growth for many
countries over a long period was collected by Barro and Lee (1994). This data set can be loaded by

data(GrowthData)

24

High-Dimensional Metrics in R

This data set contains the national growth rates in GDP per capita (Outcome) for many countries with

additional covariates. A very important covariate is gdpsh465, which is the initial level of per-capita
GDP. For further information we refer to the help page and the references herein, in particular the

online descriptions of the data set.

A.3. Institutions and Economic Development – Data on Settler Mortality. This data set was

collected by Acemoglu, Johnson, and Robinson (2001) to analyse the eﬀect of institutions on economic
development. The data can be loaded with

data(AJR)

The data set contains measurements of GDP, settler morality, an index measuring protection against
expropriation risk and geographic information (latitude and continent dummies). There are 64 obser-

vations on 11 variables.

A.4. Data on Eminent Domain. Eminent domain refers to the government’s taking of private prop-

erty. This data set was collected to analyse the eﬀect of the number of pro-plaintiﬀ appellate takings
decisions on economic outcome variables such as house price indices.

The data set is loaded into R by

data(EminentDomain)

The data set consists of four “sub data sets”which have the following structure:

• y: outcome variable, a house price index or a local GDP index,
• d: the treatment variable, represents the number of pro-plaintiﬀ appellate takings decisions in

federal circuit court c and year t

• x: exogenous control variables that include a dummy variable for whether there were rele-
vant cases in that circuit-year, the number of takings appellate decisions, and controls for the

distribution of characteristics of federal circuit court judges in a given circuit-year

• z: instrumental variables, here characteristics of judges serving on federal appellate panels

The four data sets diﬀer mainly in the dependent variable, which can be repeat-sales FHFA/OFHEO

house price index for metro (FHFA) and non-metro (NM) areas , the Case-Shiller home price index
(CS), and state-level GDP from the Bureau of Economic Analysis.

References

25

Acemoglu, D., S. Johnson, and J. A. Robinson (2001): “The Colonial Origins of Comparative Development: An

Empirical Investigation,” American Economic Review, 91(5), 1369–1401.

Barro, R.

J.,

and J.-W. Lee

(1994):

“Data

set

for

a

panel

of

139

countries,” NBER,

http://www.nber.org/pub/barro.lee.html.

Belloni, A., D. Chen, V. Chernozhukov, and C. Hansen (2012): “Sparse Models and Methods for Optimal Instru-

ments with an Application to Eminent Domain,” Econometrica, 80, 2369–2429, Arxiv, 2010.

Belloni, A., and V. Chernozhukov (2013): “Least Squares After Model Selection in High-dimensional Sparse Models,”

Bernoulli, 19(2), 521–547, ArXiv, 2009.

Belloni, A., V. Chernozhukov, I. Fern´andez-Val, and C. Hansen (2013): “Program Evaluation with High-

Dimensional Data,” arXiv:1311.2645, ArXiv, 2013.

Belloni, A., V. Chernozhukov, and C. Hansen (2010): “Inference for High-Dimensional Sparse Econometric Models,”

Advances in Economics and Econometrics. 10th World Congress of Econometric Society. August 2010, III, 245–295,

ArXiv, 2011.

(2014): “Inference on Treatment Eﬀects After Selection Amongst High-Dimensional Controls,” Review of Eco-

nomic Studies, 81, 608–650, ArXiv, 2011.

Belloni, A., V. Chernozhukov, and K. Kato (2014): “Uniform post-selection inference for least absolute deviation

regression and other Z-estimation problems,” Biometrika.

Chen, D. L., and S. Yeh (2010): “The Economic Impacts of Eminent Domain,” unpublished manuscript.
Chernozhukov, V., D. Chetverikov, and K. Kato (2013): “Gaussian approximations and multiplier bootstrap for

maxima of sums of high-dimensional random vectors,” Annals of Statistics, 41, 2786–2819.

Chernozhukov, V., and C. Hansen (2004): “The impact of 401(k) participation on the wealth distribution: An

instrumental quantile regression analysis,” Review of Economics and Statistics, 86(3), 735–751.

Chernozhukov, V., C. Hansen, and M. Spindler (2015a): “Valid Post-Selection and Post-Regularization Inference:

An Elementary, General Approach,” Annual Review of Economics, 7(1), 649–688.

(2015b): “Valid Post-Selection and Post-Regularization Inference In Linear Models with Many Controls and

Instruments,” American Economic Review: Papers and Proceedings.

Fu, W. J. (1998): “Penalized Regressions: The Bridge versus the Lasso,” Journal of Computational and Graphical

Statistics, 7(3), 397–416.

