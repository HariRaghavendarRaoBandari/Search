6
1
0
2

 
r
a

M
7

 

 
 
]

V
C
.
s
c
[
 
 

1
v
2
5
2
2
0

.

3
0
6
1
:
v
i
X
r
a

Journal of Intelligent and Fuzzy Systems 0 (2016) 0
IOS Press

0

Drift Robust Non-rigid Optical Flow
Enhancement for Long Sequences

Wenbin Li a, Darren Cosker b and Matthew Brown c
a Department of Computer Science, University College London, WC1E 6BT, UK
E-mail: w.li@cs.ucl.ac.uk
b Centre for the Analysis of Motion, Entertainment Research and Applications (CAMERA),
University of Bath, BA2 7AZ, UK. E-mail: d.p.cosker@bath.ac.uk
c Google, Mountain View, CA 94043, US
E-mail: m.brown@bath.ac.uk

Abstract. It is hard to densely track a nonrigid object in long term, which is a fundamental research issue in the computer
vision community. This task often relies on estimating pairwise correspondences between images over time where the error is
accumulated and leads to a drift. In this paper, we introduce a novel optimisation framework with an Anchor Patch constraint. It
is supposed to signiﬁcantly reduce overall errors given long sequences containing nonrigidly deformable objects. Our framework
can be applied to any dense tracking algorithm, e.g. optical ﬂow. We demonstrate the success of our approach by showing
signiﬁcant error reduction on 6 popular optical ﬂow algorithms applied to a range of realworld nonrigid benchmarks. We also
provide quantitative analysis of our approach given synthetic occlusions and image noise.

Keywords: Computer Vision, Dense Tracking, Anchor Patch, Optical Flow, Drift, Long Sequence

1. Introduction

Tracking a set of landmark points through multiple
images is a fundamental research issue in computer vi-
sion. We deﬁne tracking in this work as the estima-
tion of corresponding sets of vertices, pixels or land-
mark points between a reference frame and any other
frame in the same image sequence. In the last two
decades, optical ﬂow has become a popular approach
for tracking through image sequences [7,4,16,17,25]
in ﬁelds [21,20,12,28]. Compared with feature match-
ing methods e.g. [19], optical ﬂow provides subpixel
accuracy and dense correspondence between a pair of
images. In this work, we focus in particular on improv-
ing tracking in image sequences using optical ﬂow, and
our contribution applies to this class of algorithm.

One of the main drawbacks of optical ﬂow is drift [6,
15]. Errors accumulated between frames over time re-
sult in movement away from the correct tracking tra-
jectory. Between single image pairs, this problem may
not be noticeable. However, accumulation when track-
ing across long sequences can be particularly problem-
atic. Several authors have previously attempted to re-

Fig. 1. A mesh is tracked through a long nonrigid sequence by using
our method.

duce optical ﬂow drift in tracking. DeCarlo et al. [7]
introduce contour information on a human face to im-
prove tracking stability, while Borshukov et al. [4]
employ manual correction. More recently, Bradley et
al. [5] proposed an optimisation method constrained
by additional tracking information from multiview
video sequences. Beeler et al. [2] then introduced the
concept of anchor frames for human face tracking. In
this approach, the sequence is decomposed into several
clips based on anchor images which are visually sim-
ilar to a reference frame. Their optimisation method
shortens the tracking distance from reference frames
to the target frame to help alleviate errors. However,

1876-1364/16/$17.00 c(cid:13) 2016 – IOS Press and the authors. All rights reserved

Frame 56ReferenceFrame 234Li et al. / Drift Robust Non-rigid Optical Flow Enhancement for Long Sequences

1

their approach is domain speciﬁc (faces), and assumes
that the entire face will return to a neutral expression
(the anchor) several times throughout the sequence. In
general, it is difﬁcult to label anchor frames on general
object sequences with large displacement motion e.g.
waving cloth, as there is usually signiﬁcant deforma-
tion between the reference frame and the other frames.
In addition, repeated patterns are typically not global
as observed in a face (return to a neutral expression).
Rather, they occur in smaller local regions at intermit-
tent intervals.

In this work, we focus on tracking long video se-
quences using optical ﬂow algorithms, and speciﬁcally
concentrate on reducing drift. The general strategy of
our approach is to shorten tracking distances for lo-
cal regions throughout a long sequence. Our proposed
framework combines long term feature matching with
optical ﬂow estimation. It may be applied to the track-
ing of general objects with large displacement mo-
tion, and results in a signiﬁcant reduction in drift. We
ﬁrst detect Anchor Frames for a sequence (Sec. 4).
This provides an initial set of start points for track-
ing the sequence. Our main contribution is extend-
ing this approach by proposing the concept of Anchor
Patches (Sec. 5). These are corresponding points and
patches throughout the sequence which are propagated
directly from the reference frame. Our framework sub-
stantially reduces overall drift on a tracked image se-
quence, and may be applied to any optical ﬂow algo-
rithm in a straightforward manner. In our evaluation,
we apply the proposed optimisation framework on 6
popular optical ﬂow estimation algorithms to illustrate
it’s applicability. We provide analysis of our method
using 6 synthetic benchmark sequences (Sec. 7) gen-
erated using a method similar to [10], three of which
are degraded by adding occlusion, gaussian noise and
salt&pepper noise. In addition, we show its applica-
bility on a popular publicly available real world facial
sequence with manually annotated ground truth. We
show that our proposed optimisation framework signif-
icantly improves tracking accuracy and reduces overall
drift when compared against the baseline optical ﬂow
approaches alone.

This paper is organized as follows: In Sec. 2, an
overview of our proposed optimisation framework is
outlined. Sec. 3, 4, 5 and 6 give details of the four ma-
jor steps in our framework. In Sec. 7, we evaluate our
approach using 6 optical ﬂow algorithms tested on 6
synthetic benchmark sequences and a real world facial
sequence.

2. System Overview

Our proposed optimisation framework reduces over-
all optical ﬂow drift given long image sequences,
and provides additional robustness against other issues
such as large displacements and occlusions. The major
procedure is shown in Table 1.The aim of our Anchor
Patch optimisation Framework (APO) is accurately
tracking a mesh denoted by MR = (VR, ER, FR) from
a reference frame IR to every other frame Ii in the se-
quence. Mi = (Vi, Ei, Fi) denotes the corresponding
mesh on frame Ii. In the following sections, the four
major steps are discussed in detail.

3. Step One: Computing Optical Flow Fields

The ﬁrst step is to compute an optical ﬂow ﬁeld
between every frame and its successor over a long
video sequence in both forward and backward direc-
tions (Fig. 2). In our evaluation, we consider appli-
cation of our APO framework on a number of dense
correspondence optical ﬂow or tracking approaches,
e.g. Brox et al. [6], Classic+NL [24] and ITV-L1 [26].
Let wi→i+1 denote the optical ﬂow ﬁeld from frame
Ii to frame Ii+1. Similarly we have w(cid:48)
i+1→i denoting
the optical ﬂow ﬁeld from frame Ii+1 to frame Ii in
the backward direction. The optical ﬂow ﬁeld between
frame Ii and Ij where i < j (Forward direction), is
i<j wi→i+1. Similarly,
the optical ﬂow ﬁeld between frame Ij and Ii where
i < j (Backward direction), is denoted by w(cid:48)
j→i as
w(cid:48)

denoted by wi→j as wi→j =(cid:80)
j→i =(cid:80)

In order to evaluate the optical ﬂow at a speciﬁc
pixel x = (x, y)T , an Error Score E(w) is proposed
here, where w = (u, v)T is the optical ﬂow vector at
pixel x. The pixel x in frame Ii is matched to pixel
x(cid:48) = (x(cid:48), y(cid:48))T in frame Ii+1 where x(cid:48) = x + w. The
Error Score E(w) is calculated as the weighted Root
Mean Square (RMS) error at a 3× 3 pixel area centred
on pixel x and x(cid:48).

j>i w(cid:48)

j→j−1.

(cid:115)

E(w) =

α1d(x, y) + α2dc(x, y) + α3dd(x, y)

α1 + α2 + α3

dd(x, y) = d(x − 1, y − 1) + d(x + 1, y + 1)
+ d(x − 1, y + 1) + d(x + 1, y − 1)

dc(x, y) = d(x − 1, y) + d(x + 1, y)
+ d(x, y − 1) + d(x, y + 1)

d(x, y) = |Ii(x, y) − Ii+1(x + u, y + v)|2

(1)

2

Li et al. / Drift Robust Non-rigid Optical Flow Enhancement for Long Sequences

Input: A reference frame, a triangle mesh and an image sequence

Step 1. Computing Optical ﬂow ﬁelds (Sec. 3)

1.1 Compute optical ﬂow ﬁelds in both forward (wi→i+1) and backward (w(cid:48)
1.2 Deﬁne the Error Score function

i+1→i)

Step 2. Detect anchor frames and propagate the entire mesh to these frames (Sec. 4)

2.1 Match SIFT features from the reference to every other frame
2.2 Compute the general Error Score on matchings
2.3 Label the anchor frames from any frame with the low general Error Score

Step 3. Label anchor patches on non-anchor frames (Sec. 5)

3.1 Reuse the SIFT feature matching from 2.1
3.2 Propagate patches from the reference using Barycentric Coordinate Mapping
Step 4. Track remaining patches from anchor frames to non-anchor frames (Sec. 6)

4.1 Propagate patches from the reference to anchor frames (Sec. 6.1)

4.1.1 Compute concatenating optical ﬂow ﬁeld wR→A
4.1.2 Propagate patches from anchor frames to non-anchor frames using wR→A
4.1.3 Reﬁne the patches using Error Score

4.2 Propagate patches from anchor frames to non-anchor frames (Sec. 6.2)

4.2.1 Track the patches from the reference to frame i using wA→i
4.2.2 Track the patches from the Nearest Anchor Patches to frame i
4.2.3 Eliminate the vertex position conﬂicts between 4.2.1 and 4.2.2

Output: A mesh tracked throughout the entire image sequence

The major steps of the Anchor Patch optimisation framework.

Table 1

Fig. 2. Step One. The optical ﬂow ﬁelds are computed in both forward (wi→i+1) and backward (w(cid:48)
images pair in the sequence where the ﬁrst frame is labelled as a reference frame.

i+1→i) directions between every adjacent

Where α1, α2 and α3 are weights for controlling
the contribution of each pixel in the 3 × 3 area. Us-
ing this 3 × 3 kernel is supposed to give extra robust-
ness against the subpixel accuracy and illumination
changes [16,17,14,18]. In our experiments, all these
weights are set as α1 = 1, α2 = 0.25 and α3 = 0.125
which refer to the distance from the centre pixel x of
the area. This Error Score is intended to evaluate the
optical ﬂow at a speciﬁc pixel. We also use it to evalu-
ate feature matching scores later in our framework.

4. Step Two: Labeling Anchor Frames

After obtaining our optical ﬂow ﬁelds, anchor
frames are then detected in a similar manner to
Beeler et al. [2], with the difference that we employ

SIFT for feature matching as opposed to Normalised
Cross Correlation (NCC), and additionally use our Er-
ror Score function (Sec. 3) to evaluate matches. The
main procedure is as follows (Fig. 3):

– Feature Capture. A set of SIFT features SR
is detected in the reference frame IR. Note that
other features could be employed, but we select
SIFT due to the general high accuracy and robust-
ness. Here we apply the GPU version matching
approach [9] to perform correspondence match-
ing of SIFT feature sets SR to feature set Si of
any other frames Ii.

– Outlier Rejection. The aim of this selection pro-
cess is removing outliers from our feature match-
ing on all the frames. Correspondence matches

1iiw1iiwReference FrameiI1iILi et al. / Drift Robust Non-rigid Optical Flow Enhancement for Long Sequences

3

Fig. 3. Step Two. The frames are detected as anchor frames (Red) because of the similar appearance to the reference (Blue). These anchor frames
partition the entire sequence into several independent clips which allows tracking performing in parallel.

Fig. 4. The anchor frames are selected based on our general error score which is computed by comparing the reference frame to every other
frame in our Carton benchmark sequence.

of the SIFT feature set SR between the ref-
erence frame IR and the target frame Ii are
performed. We select the matches which meet
|x − x(cid:48)| < τ where x is feature position in IR,

(cid:8)x ∈ SR, x = (x, y)T(cid:9); x(cid:48) is the corresponding

feature position in Ii; τ is a threshold which is set
as 30 pixels in our experiments. We ﬁnd this sim-
ple outlier rejection strategy sufﬁcient for most of
cases in our experiments (Sec. 7). More sophisti-
cated outlier rejection method such as [22] could
also be employed.

– General Error Score. The general error score is
computed for every image as the average of the
overall Error Score E(w) (Eq. (1)). Frames that
contain the lowest general error score (below a
speciﬁc threshold) are selected as anchor frames
denoted IA and the other frames are non-anchor
frames. It is because that the general error score is
supposed to quantise the general appearance de-
formation where low score presents the small ap-
pearance change. Fig. 4 shows this process on our
Carton benchmark sequence.

After labeling anchor frames that are visually simi-
lar to reference frame, these are used as a basis to par-
tition the entire image sequence into several indepen-
dent clips. This also allows computation in the next
steps to be performed in parallel. In addition, the mesh
MR is propagated from the reference frame IR to each
anchor frame IA using SIFT matches and a direct opti-
cal ﬂow ﬁeld between them. More detail can be found
in Sec. 6.1. The propagated mesh in an anchor frame
is denoted MA = (VA, EA, FA). Because of large dis-

placement motion between anchor frames, and the fact
that many images in a deformable sequence may not
return to a reference point, these alone are typically in-
sufﬁcient to provide reliable tracking. In the next sec-
tion, the Anchor Patch concept will be introduced to
overcome this issue.

5. Step Three: Labeling Anchor Patches

Fig. 6. Anchoring patches using Barycentric Coordinate Mapping
and SIFT features.

The motivation of the original Anchor Frame method [2]

is to provide multiple Starting Points for tracking.
Since error accumulates, the technique is intended to
reduce overall error accumulation across long image
sequences. However, as mentioned in the previous sec-
tion, large displacement motion and complex motion
may yield a fact that most images in a video sequence
have signiﬁcant visual differences from the reference
frame.

The main observation in long image tracking is that
local spatial patterns throughout a sequence may be re-
peated - i.e. part of a cloth might return to the same

Reference FrameAnchor Frame20406080100120140160180200220240Frame IndexGeneral Error ScoreAnchorAnchorAnchorRef.1f2f3f'1f'2f'3fv'vRIiINon-anchor FrameReference FrameSearch WindowSIFT FeaturesVertices4

Li et al. / Drift Robust Non-rigid Optical Flow Enhancement for Long Sequences

Fig. 5. Step Three. Anchor patches (blue patches) are label on non-anchor frames within every clip using SIFT feature matching and Barycentric
Coordinate Mapping between reference frame and non-anchor frame.

position several times throughout a video. We take ad-
vantage of these repeating regions in order to track be-
tween shorter segments, and thus alleviate error ac-
cumulation. Apart from taking an entire image as an-
chor information, an Anchor Patch is deﬁned as a set
of individual vertices or a group of pixels in the non-
reference frame (any other frame in the sequence),
which are highly correspondent to a speciﬁc part of
the reference. The beneﬁt of using anchor patches is
to provide additional information for correcting accu-
mulated errors when tracking using optical ﬂow. This
technique can also reduce the impact of a low-quality
anchor frame (i.e. the one is too dissimilar from the
reference frame). Before anchoring patches on non-
anchor frames, we ﬁrst obtain a set of high-quality
SIFT feature matches between the reference frame and
non-anchor frames, i.e. those frames are not already
labelled as the reference frame, or an existing anchor
frame. This process proceeds as follows:

– Feature Capture. In order to save the computa-
tional time, we reuse the SIFT feature sets from
Step Two (Sec. 4). Here the SIFT feature set is de-
noted as SR in the reference frame IR; Si presents
a feature set of non-anchor frame Ii.

– Matching Selection. We also reuse the reﬁned
matchings from Step Two (Sec. 4). This process
generates a matches set mR→i from SR to Si.

The set of matches mR→i is used as our initial basis
for anchoring patches on non-anchor frames. In order
to obtain ﬁnal anchor patches, Barycentric Coordinate
Mapping and Error Reﬁnement are applied as follows:
5.0.1. Barycentric Coordinate Mapping

We suppose to determine the pixel position in a
non-anchor frame which corresponds to the position
of a vertex on the reference mesh MR in IR. These
correspondences provide our baseline for stable track-
ing throughout the image sequence. Fig. 6 illustrates
the process of anchoring patches where v = (x, y)T
denotes a vertex in MR; f∗ = (x∗, y∗)T , and de-
notes SIFT features in the reference frame IR. Sim-

(cid:20) f1 f2 f3

f(cid:48)
1 f(cid:48)

2 f(cid:48)

3

(cid:21)

(cid:20) v

v(cid:48)

 =

(cid:21) β1

β2
β3

∗, y(cid:48)
ilarly, f(cid:48)
∗ = (x(cid:48)
∗)T denotes SIFT features in a
non-anchor frame Ii. For the non-anchor frame Ii, we
have {fk → f(cid:48)
k ∈ mR→i, k = 1, 2, 3 . . .} which de-
notes previously obtained corresponding SIFT feature
matches. We wish to calculate the new vertex position
v(cid:48) = (x(cid:48), y(cid:48))T in the non-anchor frame Ii. We do this
by searching for the three nearest SIFT features f∗ in a
small 5× 5 search window centred on the vertex of in-
terest v. Next, v(cid:48) is calculated by solving the Barycen-
tric Coordinate Mapping equations as:

(2)

Where β∗ are intermediate variables that satisfy
β1 + β2 + β3 = 1. In practice we found this technique
to provide an accurate transformation when applied to
small region (5×5 pixel block). However, more sophis-
ticated (although slower) interpolation methods could
also be used. The process is performed on every vertex
in MR.
5.0.2. Error Reﬁnement
After Barycentric Coordinate Mapping, candidate
anchor patches denoted by v(cid:48)
∗ are obtained in non-
anchor frames Ii. We also have matches v∗ → v(cid:48)
∗,
the strength of which can be evaluated using our
error equation (1). Using this error, we select ﬁ-
nal anchor patches in a non-anchor frame Ii using
{P (v(cid:48)
∗) < η} where η is a predeﬁned
threshold.

∗)|E(v∗ → v(cid:48)

6. Step Four: Mesh Propagation

The objective of our optimisation framework is to
track a mesh MR from the reference frame to every
other frame in an image sequence. Given tracking in-
formation from the previous sections, this process is
separated into two steps: ﬁrst, the mesh MR is prop-
agated from reference frame to anchor frames (Sec. 4

Reference FrameAnchor FrameLi et al. / Drift Robust Non-rigid Optical Flow Enhancement for Long Sequences

5

Fig. 7. Step Four. Tracking other patches from the anchor frame and nearest anchor patches within a clip where the blue patches are anchor
patches, selected from Nearest Anchor Patch.

and 6.1). Second, the propagated mesh MA is prop-
agated from anchor frames to the non-anchor frames
within the clip (Sec. 6.2).

6.1. Propagating from the reference frame to anchor

frames

The mesh propagation process from the reference

frame to the anchor frame is as follows:

– Computing the optical ﬂow ﬁeld. The optical
ﬂow ﬁeld wR→A directly between the reference
frame to the anchor frame is computed by sum up
pairwise optical ﬂow ﬁelds wi→i+1 in between.
– Matching selection. We propagate the whole
mesh MR from the reference to the anchor
frames. For every vertex in MR, high error
matches in anchor frames are eliminated (see Er-
ror Reﬁnement).

– Barycentric Coordinate Mapping. The posi-
tions of those eliminated vertices are recomputed
by applying Barycentric Coordinate Mapping to
low error matches. The operation is shown in
Fig. 6.

After this stage, information for every vertex in MR
is established from the reference frame to the anchor
frame.

6.2. Propagating from anchor frames to non-anchor

frames

The entire image sequence is partitioned into clips
which are bound by different anchor frames. The prop-
agation process can be individually performed within
these clips in parallel. Within these clips, the anchor
patches are supposed to improve overall tracking sta-
bility and accuracy. In order to use anchor patches in
this process, we deﬁne Nearest Anchor Patch as fol-
lows. For vertex v in MA, the Nearest Anchor Patch

of v on frame Ii is the anchor patch(cid:8)v(cid:48)

i+k|v → v(cid:48)

on non-anchor frame Ii+k which is nearest to Ii in the

(cid:9)

i+k

image sequence. Fig. 8 shows an example where frame
Ii+k is the frame which is nearest to frame Ii in im-
age sequence and contains anchor patch v(cid:48)
i+k matching
to v in anchor frame IA. The main tracking procedure
proceeds (Fig. 7) as follows:

– Mesh propagation. In order to establish track-
ing information between anchor frames and non-
anchor frame, the mesh MA is ﬁrst propagated
from anchor frame IA to non-anchor frames Ii
using the previously calculated optical ﬂow ﬁeld
wA→i from Step One (Sec. 3).

– Anchor patches propagation. The Nearest An-
chor Patch of each vertex v in MA is searched
through the whole clip then propagated to non-
anchor frame Ii using the optical ﬂow ﬁeld in the
forward w∗→i or backward w(cid:48)

i+k→i direction.

– Conﬂict eliminating. After propagating the mesh
and nearest anchor patches to non-anchor frame
Ii, there may be position conﬂict on some of the
propagated vertices. As shown in Fig. 8, ˜vi and ˜v(cid:48)
i
are not in the same desired position. In order to
eliminate the conﬂict, the position of {vi|v → vi}
matching to v can be calculated using the sum
of all weighted candidate positions e.g. ˜vi and ˜v(cid:48)
(Eq.3) based on the Error Score.

i

vi =

E(v → ˜v(cid:48)
E(v → ˜v(cid:48)

i)˜vi + E(v → ˜vi)˜v(cid:48)
i) + E(v → ˜vi)

i

(3)

Due to the fact that the anchor frames divide the
overall sequence into smaller clips, this allows the
mesh propagation in between to be calculated in paral-
lel. In the next section we perform an evaluation of our
framework.

7. Evaluation

We evaluate APO with a range of 6 popular opti-
cal ﬂow estimation methods which are publicly avail-

Anchor Frame6

Li et al. / Drift Robust Non-rigid Optical Flow Enhancement for Long Sequences

Fig. 8. Vertex conﬂict can happen when mesh and anchor patches are propagated to target frame Ii. Here v(cid:48)
matched to v.

i+k is an anchor patch that is strongly

able from the Middlebury Evaluation System [1]. Com-
bined local-global Optical Flow (CLG-TV) [8], Large
Displacement Optical Flow (LDOF) [6] and Clas-
sic+NL [24] are state of the art while the Horn and
Schunck (HS) [13], Black and Anandan (BA) [3,24],
Improved TV-L1 (ITV-L1) [26] are classic optical ﬂow
frameworks and also widely used. CLG-TV is a high
speed approach that uses a combination of bilateral ﬁl-
tering and anisotropic regularization and also one of
the top three algorithms in the normalized interpola-
tion error test from Middlebury. LDOF is an integra-
tion of rich feature descriptors and variational opti-
cal ﬂow and one of best current optical ﬂow estima-
tion algorithms for large displacement motion. Clas-
sic+NL provides high performance in the Middleburry
evaluation by formalizing the median ﬁltering heuristic
and Lorentzian penalty as explicit objective functions
in an improved TV-L1 framework. The HS method is
a pioneering technique optical ﬂow. BA provides im-
provements to the HS framework by introducing robust
quadratic error formulation. ITV-L1 is a recent and
increasingly popular optical ﬂow framework which
uses a similar numerical optimisation scheme to Clas-
sic+NL. Our choice of a mixture of newer, state of
the art methods, with older traditional approaches, is
to highlight the fact that irrespective of the approach
used, our APO framework provides signiﬁcantly im-
proved tracking in all cases.

For our evaluation, we compare the optical ﬂow es-
timation methods previously mentioned – with and
without our optimisation framework – on 7 long
benchmark sequences with ground truth. Table 2 gives
an overview of the benchmark sequences used in our
evaluation. In previous work Garg et al. released to the
community a set of ground truth data for evaluating
optical ﬂow algorithms over long sequences. This is as
opposed to the Middlebury dataset, which just consid-
ers optical ﬂow between pairs of images, and is there-

fore not applicable to our framework. The sequences
of Garg et al. contains 60 frames and are generated
using interpolated dense Motion Capture (MOCAP)
data from real deformations of a waving ﬂag [27]. As
shown in Fig. 9, we use the same MOCAP data to gen-
erate a long video sequence and three other degraded
sequences, each of which contains 237 frames of size
500 × 500 pixels. The three degraded sequences are
generated in order to test the robustness of our APO
framework under different image conditions. They are
generated by individually adding synthetic occlusions,
gaussian noise and salt & pepper noise with the same
parameters described in [10]. In order to increase the
diversity of the sequences, we include three other se-
quences. One is a Talking Face Video (Frank) sequence
which contains 300 frames with 68 ground truth an-
notation points per frame. The other two are also syn-
thetic benchmark sequences generated using MOCAP
data of Salzmann et al. [23] from the carton and servi-
ette deformations. One contains 266 frames of size
1024 × 768 while the other contains 307 frames of
the same image size. In addition, we also consider the
effect of the number of SIFT features detected in the
frame, and how this affects overall tracking stability
of the APO framework. All optical ﬂow algorithms
are applied with default parameter settings from their
original papers.

Our baseline optical ﬂow based tracking strategy –
for each of the above algorithms – is performed as fol-
lows: First, the optical ﬂow ﬁeld is computed (in for-
ward direction) for every pair of adjacent frames in
the sequence. We then mark the initial tracking points
in the ﬁrst frame using the same ground truth points
in the same frame of the sequence (Table 2). The
correspondent points in the next frame are computed
based on the optical ﬂow ﬁeld in between. This pro-
cess is repeated until correspondent landmark points
are obtained in every frame of the sequence. The aver-

AIiIikIv'vikAM'wikiwAiviiAnchor FrameNon-anchor FrameNon-anchor FrameVertices inof anchor frameAIAnchor patches in non-anchor frameikIInferred patches in non-anchor frameiIv'Li et al. / Drift Robust Non-rigid Optical Flow Enhancement for Long Sequences

7

Image Size (pix.)
Sequence Length
Annotation Points

Avg. Feature Amount

Original
500 × 500

Occlusion
500 × 500

237
160

364.80

237
160

358.32

Information of the Benchmark Sequences
Guass.N
500 × 500

500 × 500

S&P.N

Carton

1024 × 768

237
160

266
81

237
160

566.13

Table 2

Serviette

1024 × 768

Frank

720 × 576

307
63

300
68

1276.50

2498.01

3315.49

2071.11

An overview of the benchmark sequences in our evaluation. That includes 4 attributes of image size (pixel), sequence length, number of ground
truth annotation points per frame and average SIFT feature amount per frame.

even without APO. It is because that the LDOF frame-
work takes into account both regular optical ﬂow en-
ergy and the feature technique. The latter contributes
additional accuracy.

Table 3(b) shows another experiment, in which we
performs Garg et al. [10] and Pizarro et al. [22] on
our benchmark sequences (results on Carton, Serviette
and Frank are not available.) using a direct tracking
strategy. Here we compute the optical ﬂow ﬁelds di-
rectly from the reference to any other frames of the se-
quence. The annotation points are then directly tracked
to the test frames using those ﬂow ﬁelds. Note that the
numbers in Table 3(b) may be slightly different from
their original work [11]. It is because that, ﬁrst, our se-
quences are extended to 237 frames which is around 3
times longer; second, we evaluate the tracking results
of only 160 annotation points instead of all the pixels.
We observe that the both state-of-the-art approaches
(Garg et al. and Pizarro et al.) give higher accuracy
than any other baseline methods in Table 3(a). The hid-
den conditions are (1) the tracking distance is mini-
mum for Garg et al. and Pizarro et al. which very much
reduces the accumulate errors; (2) both Garg et al. and
Pizarro et al. shows high accuracy for nonrigid surface
tracking in the record [10,22]. And all our sequences
contain single nonrigid object. However, such direct
tracking strategy cannot handle the situation where ob-
jects may be temporally out of the scene. In addition,
the object appearance in the reference may be signif-
icantly different from the one in some other frames
of the sequence. That brings extra difﬁculty to opti-
cal ﬂow estimation. In this measure (Table 3(b)), our
APO degrades to correct the tracking using the long
term features. In the Orignal and Occlusion cases, the
result shows that using APO still yields lower (or the
same) errors over the baseline methods even those have
given really minuscule measures ( < 1.60 pixel ). We
also observe that APO slightly reduces the accuracy on
the noisy cases because the extra noises affect the fea-
ture detection. Please note that the results are shown as
”N/A“ on Carton, Serviette and Frank as the baselines
Garg at al. and Pizarro at al. are not available.

Fig. 9. Our long nonrigid synthetic sequences with ground truth are
rendered by using ﬂag MOCAP data [27] and Garg et al. [11]. Im-
ages are adopted from [10]

age Endpoint Error (EE) [1] is then calculated against
the ground truth annotation points. We then apply our
APO framework using the same optical ﬂow ﬁelds.).
Note that the parameter values relevant to the APO
framework are initially and experimentally selected,
but then remain constant in all our evaluations.

Table 3(a) shows the measurement of average End-
point Error (AEE) in pixels over all the frames of
the sequences. We highlight the top three best AEE
measures for each sequence using superscripts next
to different values. Notice that APO signiﬁcantly re-
duces the AEE compared to the baseline optical ﬂow
methods. Our optimisation framework yields the best
AEE measure in all the cases. For instance, ITV-L1
with APO performs the best in sequence Original
while LDOF with APO yields the best result in se-
quence Frank. We also observe that although in the
Guass.Noise and S&P.Noise sequences the improve-
ment is less than in the unaltered sequences, the over-
all result is still an improvement with the addition of
APO. We also observe that LDOF gives good results

OriginalOcclusionGuass.NoiseS&P.NoiseReferenceFrame 40Ground Truth3D MoCap DataDense Interpolation Surface8

Li et al. / Drift Robust Non-rigid Optical Flow Enhancement for Long Sequences

(a) Average Endpoint Error (AEE) comparison of different methods with our optimisation framework on the
benchmark sequences.

Methods
BA [3]

BA + APO
CLG-TV [8]

CLG-TV + APO

HS [13]
HS + APO
LDOF [6]

LDOF + APO
Classic+NL [24]
Classic+NL + APO

ITV-L1 [26]
ITV-L1 + APO

Average Endpoint Error in pix (AEE)

Original

Occlusion

6.14
1.722
8.59
2.25
29.16
11.68
6.21
1.753
7.07
2.15
5.73
1.501

8.03
1.912
10.93
2.97
30.44
12.88
6.39
1.671
10.61
3.18
8.25
2.333

Guass.N
11.02
7.891
20.28
12.31
29.74
17.79
16.24
11.65
12.65
8.312
17.29
9.533

S&P.N
7.79
5.041
33.93
18.99
29.43
17.21
24.14
13.12
9.50
6.462
14.49
7.703

Carton
10.56
2.77
28.94
6.95
27.69
10.25
6.33
1.181
5.72
1.342
5.34
1.703

Serviette

5.18
1.561
32.17
9.43
37.90
10.03
5.51
1.842
6.62
2.033
7.11
2.36

Frank
17.57
6.60
19.29
7.05
31.27
14.19
14.73
3.121
17.32
3.442
17.91
3.693

(b) Average Endpoint Error (AEE) comparison of Garg et al. and Pizarro et al. on the benchmark sequences (directly
tracking from the reference to any other frames).

Average Endpoint Error in pix (AEE)

Methods

Original

Occlusion

Guass.N

Garg et al., PCA [10]

Garg et al., PCA [10] + APO

Garg et al., DCT [10]

Garg et al., DCT [10] + APO

Pizarro et al. [22]

Pizarro et al. [22] + APO

0.61
0.60
0.591
0.591
0.71
0.79

1.64
1.65
1.86
1.85
0.99
0.991

0.71
0.691
0.74
0.73
0.79
0.81

Table 3

S&P.N
1.21
1.23
1.54
1.53
0.98
0.981

Carton
N/A
N/A
N/A
N/A
N/A
N/A

Serviette

N/A
N/A
N/A
N/A
N/A
N/A

Frank
N/A
N/A
N/A
N/A
N/A
N/A

Average Endpoint Error (AEE) comparison on our long benchmark sequences.

While we consider ourselves primarily with tracking
over long sequences, the shorter sequences are con-
sider as well. In Table 4, the AEE measures of vari-
ous methods are compared on the ﬁrst 30 frames of
our benchmark sequences. We observe similar AEE
measures as in the long sequence case (Table 2(a)).
The APO framework signiﬁcantly increases the track-
ing accuracy – outperforming the baseline tracking
methods in all cases even given degradation (e.g.
Gauss.Noise and S&P.Noise). Moreover, the BA with
APO is also observed to overﬁt in the noisy sequences
while Classic+NL with APO yields the best measures
in both sequences of Gauss.Noise and S&P.Noise.

We also evaluate the effect on tracking accuracy
by varying the number of selected features. Different
numbers (50% and 0%) of features are randomly se-
lected from the initial full detection feature set be-
fore performing Anchor Patch detection. Information
on our total number of features can be found in Table 2,
e.g. there are 364.80 features averagely on each frame
of the sequence Original. Table 5 shows an AEE com-

parison given various numbers of features. We observe
that AEE is improved given more features in all cases.
Another interesting observation is that our optimisa-
tion framework provides lower error against the base-
line tracking strategy even given sparse or no features
(0% feature). Note that in this case, our APO frame-
work defaults to using an optical ﬂow method with just
the Anchor Frame approach [2]. Also note – for exam-
ple by comparing to Table 3 – that this indicates that
the APO framework also provides signiﬁcant tracking
improvement over using anchor frames alone.

We also make the visual comparisons on two of
our sequences, Frank and Serviette. The former is real
world sequence with ground truth annotation points,
while the latter is synthetic sequence overlaid with a
ground truth mesh. In Fig. 10, we observe noticeable
drift problems given the baseline optical ﬂow tracking
strategy. Also note that more details can be found in the
corresponding video footage where we visually show
that our framework signiﬁcantly reduces the drift.

Li et al. / Drift Robust Non-rigid Optical Flow Enhancement for Long Sequences

9

Average Endpoint Error (AEE) on the First 30 Frames
Occlusion

Guass.N

Methods
BA [3]

BA + APO
CLG-TV [8]

CLG-TV + APO

HS [13]
HS + APO
LDOF [6]

LDOF + APO
Classic+NL [24]
Classic+NL + APO

ITV-L1 [26]
ITV-L1 + APO

Original

1.57
1.413
2.40
2.10
33.67
16.11
2.38
1.152
1.63
1.51
1.55
0.991

1.72
1.65
2.60
2.24
35.70
16.32
2.37
0.971
1.76
1.332
1.76
1.312

3.87
3.663
6.71
6.53
35.05
13.78
3.96
3.75
3.612
3.541
6.27
5.77

Table 4

S&P.N
2.71
2.132
8.77
8.39
34.50
19.37
4.03
2.66
2.513
1.991
5.07
4.65

Carton
2.37
2.17
8.10
4.79
26.16
9.78
3.90
0.891
2.18
1.242
2.37
1.693

Serviette
1.563
1.131
5.54
5.11
22.08
6.33
2.52
1.442
1.75
1.68
2.01
1.71

Frank
8.76
5.40
8.60
7.35
12.76
9.19
8.51
2.821
8.77
3.703
9.22
3.482

Average Endpoint Error (AEE) comparison of different methods with our optimisation framework on the ﬁrst 30 frames of the benchmark
sequences.

Average Endpoint Error (AEE) on Different Feature Distributions

Methods

Original

Occlusion

BA [3], No APO

APO, 100% Feature
APO, 50% Feature
APO, 0% Feature

CLG-TV [8], No APO
APO, 100% Feature
APO, 50% Feature
APO, 0% Feature
HS [13], No APO

APO, 100% Feature
APO, 50% Feature
APO, 0% Feature
LDOF [6], No APO
APO, 100% Feature
APO, 50% Feature
APO, 0% Feature

Classic+NL [24], No APO

APO, 100% Feature
APO, 50% Feature
APO, 0% Feature

ITV-L1 [26], No APO
APO, 100% Feature
APO, 50% Feature
APO, 0% Feature

6.14
1.722
3.64
5.12
8.59
2.25
4.86
6.94
29.16
11.68
18.13
24.73
6.21
1.753
3.21
5.08
7.07
2.15
4.00
5.96
5.73
1.501
3.59
4.77

8.03
1.912
4.71
6.44
10.93
2.97
6.51
9.11
30.44
12.88
20.28
27.11
6.39
1.671
3.09
5.24
10.61
3.18
6.39
7.78
8.25
2.333
5.17
6.92

Guass.N
11.02
7.891
8.06
9.23
20.28
12.31
14.39
16.83
29.74
17.79
20.66
23.97
16.24
11.65
12.18
14.11
12.65
8.312
9.48
11.64
17.29
9.533
10.93
12.50

S&P.N
7.79
5.041
6.12
7.21
33.93
18.99
22.72
26.03
29.43
17.21
19.91
23.40
24.14
13.12
15.02
18.46
9.50
6.462
7.33
8.98
14.49
7.703
8.47
10.31

Carton
10.56
2.77
5.89
8.69
28.94
6.95
15.36
23.57
27.69
10.25
17.39
24.09
6.33
1.181
2.90
5.45
5.72
1.342
3.89
4.78
5.34
1.703
3.41
4.43

Serviette

5.18
1.561
2.98
4.35
32.17
9.43
19.91
24.03
37.90
10.03
25.99
33.11
5.51
1.842
3.74
4.89
6.62
2.033
4.00
6.00
7.11
2.36
5.00
5.95

Frank
17.57
6.60
10.63
12.69
19.29
7.05
12.00
15.07
31.27
14.19
23.45
29.17
14.73
3.121
8.66
11.76
17.32
3.442
10.14
13.27
17.91
3.693
10.11
14.29

Average Endpoint Error (AEE) comparison on the benchmark sequences with varying feature distributions.

Table 5

The computational consumption of our framework
heavily relies on the supplementary optical ﬂow method,
because we need to calculate the optical ﬂow ﬁelds

twice (forward and backward) for every pair of adja-
cent images. Apart from this, our framework can be
implemented in a parallel computation fashion. An-

10

Li et al. / Drift Robust Non-rigid Optical Flow Enhancement for Long Sequences

chor frames divide the sequence into clips which give
multiple start points for tracking. In the implementa-
tion, a GPU version of SIFT approach [9] is applied for
feature detection and matching (around 10 frames per
second on our benchmarks). The whole framework is
constructed under CUDA platform. Assuming all opti-
cal ﬂow ﬁelds are obtained, our framework reach real-
time efﬁciency (around 2 frames per second) on our
benchmarks using on a 2.9Ghz Xeon 8-cores, NVIDIA
Quadro FX 580, 16Gb memory computer.

8. Conclusion

In this paper, we have presented a novel optimi-
sation framework using Anchor Patches constraint,
which improves the tracking on mesh or sparse points
through long image sequences. Our optimisation frame-
work temporally anchors the image regions throughout
the sequence in order to mitigate the effect of Error
Accumulation (Drift). In the evaluation, our approach
combined with 6 popular optical ﬂow algorithms and
show signiﬁcant improvement against baselines meth-
ods on 7 benchmark sequences. Such datasets include
6 synthetic benchmark sequences with realworld de-
formation and 1 realworld sequence.

9. Acknowledgements

We thank Ravi Garg and Lourdes Agapito for pro-
viding their GT datasets. We also thank Gabriel Bros-
tow and the UCL Vision Group for their generous com-
ments. The authors are supported by the EPSRC CDE
EP/L016540/1 and CAMERA EP/M023281/1; and
EPSRC projects EP/K023578/1 and EP/K02339X/1.
References

[1] Simon Baker, Daniel Scharstein, J. Lewis, Stefan Roth,
Michael Black, and Richard Szeliski. A database and evalu-
ation methodology for optical ﬂow. International Journal of
Computer Vision (IJCV’11), 92:1–31, 2011. ISSN 0920-5691.
[2] T. Beeler, F. Hahn, D. Bradley, B. Bickel, P. A. Beardsley,
C. Gotsman, R. W. Sumner, and M. H. Gross. High-quality
passive facial performance capture using anchor frames. ACM
Transactions on Graphics (TOG’11), 30(4):75, 2011.

[3] M.J. Black and P. Anandan. The robust estimation of multiple
motions: Parametric and piecewise-smooth ﬂow ﬁelds. Com-
puter vision and image understanding (CVIU’96), 63(1):75–
104, 1996.

[4] G. Borshukov, D. Piponi, O. Larsen,

JP Lewis, and
C. Tempelaar-Lietz. Universal capture: image-based facial
In ACM SIGGRAPH’05
animation for the matrix reloaded.
Courses, page 16. ACM, 2005.

[5] D. Bradley, W. Heidrich, T. Popa, and A. Sheffer. High resolu-
tion passive facial performance capture. ACM Transactions on
Graphics (TOG’10), 29(4):41, 2010.

[6] T. Brox and J. Malik. Large displacement optical ﬂow: De-
IEEE
scriptor matching in variational motion estimation.
Transactions on Pattern Analysis and Machine Intelligence
(PAMI’11), 33:500–513, 2011. ISSN 0162-8828.

[7] D. DeCarlo and D. Metaxas. The integration of optical ﬂow
and deformable models with applications to human face shape
and motion estimation. In Computer Vision and Pattern Recog-
nition (CVPR’96), pages 231–238, 1996.

[8] M. Drulea and S. Nedevschi. Total variation regularization of
local-global optical ﬂow. In Intelligent Transportation Systems
(ITSC’11), pages 318–323. IEEE, 2011.

[9] Brian Fulkerson and Stefano Soatto. Really quick shift: Im-
age segmentation on a gpu. In Trends and Topics in Computer
Vision, pages 350–358. Springer, 2012.

[10] R. Garg, A. Roussos, and L. Agapito. A variational approach
to video registration with subspace constraints. International
journal of computer vision (IJCV’13), 104(3):286–314, 2013.
[11] Ravi Garg, Anastasios Roussos, and Lourdes Agapito. Dense
variational reconstruction of non-rigid surfaces from monocu-
lar video. In IEEE Conference on Computer Vision and Pattern
Recognition (CVPR’13), pages 1272–1279, 2013.

[12] Clement Godard, Peter Hedman, Wenbin Li, and Gabriel J
Brostow. Multi-view reconstruction of highly specular surfaces
in uncontrolled environments. In International Conference on
3D Vision (3DV’15), pages 19–27. IEEE, 2015.

[13] B.K.P. Horn and B.G. Schunck. Determining optical ﬂow. Ar-

tiﬁcial intelligence, 17(1-3):185–203, 1981.

[14] Wenbin Li. Nonrigid Surface Tracking, Analysis and Evalua-

tion. PhD thesis, University of Bath, 2013.

[15] Wenbin Li, Darren Cosker, and Matthew Brown. An anchor
patch based optimisation framework for reducing optical ﬂow
drift in long image sequences. In Asian Conference on Com-
puter Vision (ACCV’12), pages 112–125, November 2012.

[16] Wenbin Li, Darren Cosker, Matthew Brown, and Rui Tang.
Optical ﬂow estimation using laplacian mesh energy.
In
IEEE Conference on Computer Vision and Pattern Recognition
(CVPR’13), pages 2435–2442. IEEE, June 2013.

[17] Wenbin Li, Yang Chen, JeeHang Lee, Gang Ren, and Darren
Cosker. Robust optical ﬂow estimation for continuous blurred
scenes using rgb-motion imaging and directional ﬁltering. In
IEEE Winter Conference on Application of Computer Vision
(WACV’14), pages 792–799. IEEE, March 2014.

[18] Wenbin Li, Yang Chen, JeeHang Lee, Gang Ren, and Darren
Cosker. Blur robust optical ﬂow using motion channel. Neuro-
computing, 0(0):12, 2016.

[19] D.G. Lowe. Distinctive image features from scale-invariant
keypoints. International journal of computer vision (IJCV’04),
60(2):91–110, 2004.

[20] Zhihan Lv, Alex Tek, Franck Da Silva, Charly Empereur-Mot,
Matthieu Chavent, and Marc Baaden. Game on, science-how
video game technology may help biologists tackle visualiza-
tion challenges. PloS one, 8(3), 2013.

[21] Zhihan Lv, Alaa Halawani, Shengzhong Feng, Haibo Li, and
Shaﬁq Ur Réhman. Multimodal hand and foot gesture interac-
tion for handheld devices. ACM Transactions on Multimedia
Computing, Communications, and Applications (TOMM), 11
(1):10, 2014.

[22] Daniel Pizarro and Adrien Bartoli. Feature-based deformable
surface detection with self-occlusion reasoning. International
Journal of Computer Vision (IJCV’12), 97:54–70, 2012. ISSN
0920-5691.

Li et al. / Drift Robust Non-rigid Optical Flow Enhancement for Long Sequences

11

(a) Visual comparison of different methods on the frame 88 of the sequence Frank.

(b) Visual comparison of different methods on the frame 192 of the sequence Serviette.

Fig. 10. Visual comparison and AEE measures on sequences of Frank and Serviette.

[23] M. Salzmann, R. Hartley, and P. Fua. Convex optimization for
deformable surface 3-d tracking. In International Conference
on Computer Vision (ICCV’07), pages 1–8, 2007.

[24] Deqing Sun, S. Roth, and M.J. Black. Secrets of optical ﬂow
estimation and their principles. In Computer Vision and Pattern
Recognition (CVPR’10), pages 2432–2439, 2010.

[25] Rui Tang, Darren Cosker, and Wenbin Li. Global alignment
for dynamic 3d morphable model construction. In Workshop
on Vision and Language (V&LW’12), 2012.

[26] A. Wedel, T. Pock, C. Zach, H. Bischof, and D. Cremers. An
improved algorithm for tv-l 1 optical ﬂow. In Statistical and
Geometrical Approaches to Visual Motion Analysis, pages 23–
45. Springer, 2009.

[27] R. White, K. Crane, and D.A. Forsyth. Capturing and animat-
ing occluded cloth. ACM Transactions on Graphics (TOG’07),
26(3):34, 2007.

[28] Jiachen Yang, Yancong Lin, Zhiqun Gao, Zhihan Lv, Wei Wei,
and Houbing Song. Quality index for stereoscopic images by
separately evaluating adding and subtracting. PloS one, 10(12),

Ground truthBACLG-TVHSLDOFClassic+NLITV-L1BA+APOCLG-TV+APOHS+APOLDOF+APOClassic+NL+APOITV-L1+APOGround truthBACLG-TVHSLDOFClassic+NLITV-L1BA+APOCLG-TV+APOHS+APOLDOF+APOClassic+NL+APOITV-L1+APO12

Li et al. / Drift Robust Non-rigid Optical Flow Enhancement for Long Sequences

2015.

