6
1
0
2

 
r
a

 

M
6
2

 
 
]

.

R
P
h
t
a
m

[
 
 

2
v
2
1
3
6
0

.

3
0
6
1
:
v
i
X
r
a

A RANK-BASED MEAN FIELD GAME IN THE STRONG FORMULATION

ERHAN BAYRAKTAR AND YUCHONG ZHANG

Abstract. We discuss a natural game of competition and solve the corresponding mean ﬁeld game
with common noise when agents’ rewards are rank-dependent. We use this solution to provide an
approximate Nash equilibrium for the ﬁnite player game and obtain the rate of convergence.

1. Introduction

Mean ﬁeld games (MFGs), introduced independently by [8] and [6], provide a useful approxi-
mation for the ﬁnite player Nash equilibrium problems in which the players are coupled through
their empirical distribution. In particular, the mean ﬁeld game limit gives an approximate Nash
equilibrium, in which the agents’ decision making is decoupled. In this paper we will consider a
particular game in which the interaction of the players is through their ranks. Our main goal is to
construct an approximate Nash equilibrium for a ﬁnite player game when the agents’ dynamics are
modulated by common noise.

Rank-based mean ﬁeld games, which have non-local mean ﬁeld interactions, have been suggested
as an extension of their model of oil production in [4] and analyzed more generally by the recent
paper by Carmona and Lacker [3] using the weak formulation, when there is no common noise.
There are currently no results on the rank-dependent mean ﬁeld games with common noise. In
order to solve the problem with common noise, we will make use of the mechanism in [7] by solving
the strong formulation of the rank-dependent mean ﬁeld game without common noise and then by
observing that purely rank-dependent reward functions are translation invariant.

The rest of the paper is organized as follows: In Section 2 we introduce the N-player game in
which the players are coupled through the reward function which is rank-based. In Section 3 we
consider the case without common noise. We ﬁrst ﬁnd the mean ﬁeld limit, discuss the uniqueness
of the Nash equilibrium, and construct an approximate Nash equilibrium using the mean ﬁeld limit.
Using these results, in Section 4 we use the mechanism in [7] and obtain respective results for the
common noise.

2. The N -player game

We consider N players each of whom control her own state variable and are rewarded based on
their ranking. We will denote by Xi the i-th player’s state variable, and assume that it satisﬁes
the following stochastic diﬀerential equation (SDE)

dXi,t = ai,tdt + σdBi,t + σ0dWt, Xi,0 = 0,

where ai represents the control by agent i, and (Bi)i=1,...,N and W are independent standard Brown-
ian motions deﬁned on some ﬁltered probability space (Ω,F,{Ft}, P), representing the idiosyncratic

Key words and phrases. Mean Field Games, competition, common noise, rank-dependent interaction, non-local

interaction, strong formulation.

This research is supported in part by the National Science Foundation.

1

2

ERHAN BAYRAKTAR AND YUCHONG ZHANG

noises and common noise, respectively. The game ends at time T > 0, when each player receives a
rank-based reward minus the running cost of eﬀort, which we will assumed to be quadratic ca2 for
some constant c > 0. In order to precisely deﬁne the rank-based reward

¯µN :=

1
N

δXi,T

N

Xi=1

denote the empirical measure of the terminal state of the N -player system. Then ¯µN (−∞, Xi,T ]
gives the fraction of players that ﬁnishes the same or worse than player i. Let R × [0, 1] ∋ (x, r) 7→
R(x, r) ∈ R be a bounded continuous function that is non-decreasing in both arguments. For any
probability measure µ on R, write Rµ(x) = R(x, µ(−∞, x]) = R(x, Fµ(x)) where Fµ denotes the
cumulative distribution function of µ. The reward player i receives is given by
R¯µN (Xi,T ) = R(Xi,T , ¯µN (−∞, Xi,T ]) = R(Xi,T , F¯µN (Xi,T )).

When R(x, r) is independent of x, the compensation scheme is purely rank-based. In general, we
could have a mixture of absolute performance compensation and relative performance compensa-
tion. The objective of each player is to observe the progress of all players and choose his eﬀort level
to maximize the expected payoﬀ, while anticipating the other players’ strategies.

The players’ equilibrium expected payoﬀs, as functions of time and state variables, satisfy a
system of N coupled nonlinear partial diﬀerential equations subject to discontinuous boundary
conditions, which appears to be analytically intractable. Fortunately, in a large-population game,
the impact of any individual on the whole population is very small. So it is often good enough
for each player to ignore the private state of any other individual and simply optimize against the
aggregate distribution of the population. As a consequence, the equilibrium strategies decentralize
in the limiting game as N → ∞. We shall use the mean ﬁeld limit to construct approximate Nash
equilibrium for the N -player game, both in the case with and without common noise.

3. Mean Field Approximation when there is no common noise

In this section, we assume σ0 = 0. Solving the mean ﬁeld game consists of two sub-problems: a
stochastic control problem and a ﬁxed-point problem (also called the consistency condition). For
any Polish space X , denote by P(X ) the space of probability measures on X , and P1(X ) := {µ ∈
P(X ) :RX |x|dµ(x) < ∞}.
We ﬁrst ﬁx a distribution µ ∈ P(R) of the terminal state of the population, and consider a single

player’s optimization problem:

where

v(t, x) := sup

a

Et,x(cid:20)Rµ(XT ) −Z T

t

ca2

sds(cid:21)

(3.1)

(3.2)
B is a Brownian motion, and a ranges over the set of progressively measurable processes satisfying

dXs = asds + σdBs,

ER T
0 |as|ds < ∞. The associated dynamic programming equation is
σ2vxx − ca2(cid:27) = 0

a (cid:26)avx +

vt + sup

1
2

with terminal condition v(T, x) = Rµ(x). Using the ﬁrst-order condition, we obtain that the
candidate optimizer is a∗ = vx
2c , and the Hamilton-Jacobi-Bellman (HJB) equation can be written
as

vt +

σ2vxx +

1
2

(vx)2
4c

= 0.

u(t, x) = EZ(cid:20)exp(cid:18) 1

2cσ2 Rµ(x + σ√T − tZ)(cid:19)(cid:21)

(3.3)

The above equation can be linearized using the transformation u(t, x) := e(2cσ2)−1v(t,x), giving

3

ut +

1
2

σ2uxx = 0.

Together with the boundary condition u(T, x) = e(2cσ2)−1Rµ(x), we can easily write down the solu-
tion:

where Z is a standard normal random variable, possibly deﬁned on another probability space with
measure PZ . Let us further write u as an integral:

u(t, x) =Z ∞
=Z ∞

−∞

−∞

exp(cid:18) 1
exp(cid:18) 1

2cσ2 Rµ(x + σ√T − tz)(cid:19) 1
exp(cid:18)−
√2π
exp(cid:18)−
2cσ2 Rµ(y)(cid:19)
p2πσ2(T − t)

1

z2

2 (cid:19) dz
(y − x)2
2σ2(T − t)(cid:19) dy.

Using dominated convergence theorem, we can diﬀerentiate under the integral sign and get

1

−∞

ux(t, x) =Z ∞
exp(cid:18) 1
=Z ∞
exp(cid:18) 1
= EZ(cid:20)exp(cid:18) 1

exp(cid:18)−
2cσ2 Rµ(y)(cid:19)
p2πσ2(T − t)
2cσ2 Rµ(x + σ√T − tz)(cid:19) 1
exp(cid:18)−
√2π
2cσ2 Rµ(x + σ√T − tZ)(cid:19)
σ√T − t(cid:21) .
Z

−∞

(y − x)2
2σ2(T − t)(cid:19) (y − x)
σ2(T − t)
2 (cid:19)

σ√T − t

z2

dz

z

Similarly, we obtain

uxx = EZ(cid:20)exp(cid:18) 1

2cσ2 Rµ(x + σ√T − tZ)(cid:19) Z 2 − 1

σ2(T − t)(cid:21) .

dy

(3.4)

(3.5)

Using (3.3)-(3.5), together with the boundedness and monotonicity of R, we easily get the following
estimates. Note that all bounds are independent of µ.

Lemma 3.1. The functions u and v satisfy

0 ≤ ux(t, x) ≤

1

K

0 < K−1 ≤ u(t, x) ≤ K, − kRk∞ ≤ v(t, x) ≤ kRk∞,
0 ≤ vx(t, x) ≤ 2cσK 2r 2
|vxx(t, x)| ≤
T − t

σr 2
|uxx(t, x)| ≤

√T − t
T − t

π
2K
σ2

π

1

,

,

4cK 2(1 + K 2π−1)

1

,

√T − t

,

where K := exp((2cσ2)−1kRk∞).

Since vxx is bounded, the drift coeﬃcient a∗ = vx

2c is Lipschitz continuous in x. It follows that
the optimally controlled state process, denoted by X∗, has a strong solution on [0, T ). Observe
that

0 ≤Z T
So the optimal cumulative eﬀort is bounded by some constant independent of µ. It also implies
t a∗(s, X∗s )ds + σ(Bu − Bt) has a well-deﬁned limit as u → T . Standard veriﬁcation

ds = 2σK 2r2(T − t)

a∗(s, X∗s )ds ≤Z T

σK 2p2/π
√T − s

< ∞.

π

t

t

that X∗u = x +R u

4

ERHAN BAYRAKTAR AND YUCHONG ZHANG

theorem yields that the solution to the HJB equation is the value function of the problem (3.1)-
(3.2), and that a∗ is the optimal Markovian feedback control. Finally, using dominated convergence
theorem again, we can show that for t < T ,

ux(t, x) = 0.

lim
x→±∞

The same limit also holds for a∗ since u is bounded away from zero. In other words, the optimal
eﬀort level is small when the progress is very large in absolute value. This agrees with many real
life observations that when a player has a very big lead, it is easy for him to show slackness; and
when one is too far behind, he often gives up on the game instead of trying to catch up.

3.1. Existence of a Nash equilibrium. For each ﬁxed µ ∈ P(R), solving the stochastic control
problem (3.1)-(3.2) yields a value function v(t, x; µ) and a best response a∗(t, x) = (2c)−1vx(t, x; µ).
Suppose the game is started at time zero, with zero initial progress, the optimally controlled state
process X µ of the generic player satisﬁes the SDE

dXt =

vx(t, Xt; µ)

2c

dt + σdBt, X0 = 0.

(3.6)

Finding a Nash equilibrium for the limiting game is equivalent to ﬁnding a ﬁxed point of the
T ), where L(·) denotes the law of its argument. We shall sometimes refer to

mapping Φ : µ 7→ L(X µ

such a ﬁxed point an equilibrium measure.

Theorem 3.1. The mapping Φ has a ﬁxed point.

Proof. Similar to [1], we will use Schauder’s ﬁxed point theorem. Observe that for any µ ∈ P(R),
we have

T|2 ≤ E(2σK 2r 2T
This implies the set of Φ(µ) = L(X µ
T ) is tight in P(R), hence relatively compact for the topology
of weak convergence by Prokhorov theorem. Recall that P1(R) = {µ ∈ P(R) : RR |x|dµ(x) < ∞}.
Equip P1(R) with the topology induced by the 1-Wasserstein metric:

+ σ|BT|)2 =: C0.

E|X µ

π

W1(µ, µ′) := inf(cid:26)ZR2 |x − y|dπ(x, y) : π ∈ P1(R2) with marginals µ and µ′(cid:27)

= sup(cid:26)ZR

ψdµ −ZR

ψdµ′ : ψ ∈ Lip1(R)(cid:27) .

Here Lip1(R) denotes the space of Lipschitz continuous functions on R whose Lipschitz constant
is bounded by one. It is known that (P1(R), W1) is a complete separable metric space (see e.g. [9,
Theorem 6.18]). We shall work with a subset of P1(R) deﬁned by
E := {µ ∈ P1(R) :ZR |x|2dµ(x) ≤ C0}.

It is easy to check that E is non-empty, convex and closed (for the topology induced by the W1
metric). Moreover, one can show using [9, Deﬁnition 6.8(iii)] that any weakly convergent sequence
{µn} ⊆ E is also W1-convergent. Therefore, E is also relatively compact for the topology induced
by the W1 metric. So we have found a non-empty, convex and compact set E such that Φ maps E
into itself. It remains to show Φ is continuous on E. In the rest of the proof, the constant C may
change from line to line.

Let {µk} ⊆ E such that W1(µk, µ) → 0 as k → ∞. We wish to show W1(Φ(µk), Φ(µ)) → 0. Note

that

W1(Φ(µk), Φ(µ)) ≤ E|X µk

T − X µ

T| ≤Z T

0

E|vx(t, X µk

t

; µk) − vx(t, X µ

t ; µ)|dt.

From Lemma 3.1, we know that |vx(t, X µk
thanks to the dominated convergence theorem, it suﬃces to show for t ∈ [0, T ),

; µk) − vx(t, X µ

t ; µ)| ≤ C√T−t

. Since R T

0

t

E|vx(t, X µk

t

; µk) − vx(t, X µ

t ; µ)| → 0.

By Lemma 3.1 and mean value theorem, we have that

5

C√T−t

dt < ∞,

|vx(t, X µk

t

; µk) − vx(t, X µ

t ; µ)|.
So to show W1(Φ(µk), Φ(µ)) → 0 it suﬃces to demonstrate for each ﬁxed t ∈ [0, T ),

t ; µk) − vx(t, X µ

t

t ; µ)| ≤ |vx(t, X µk
T − t|X µk

≤

C

; µk) − vx(t, X µ
t − X µ

t | + |vx(t, X µ

t ; µk)| + |vx(t, X µ

t ; µk) − vx(t, X µ

t ; µ)|

and

E|vx(t, X µ

t ; µk) − vx(t, X µ
E|X µk

t − X µ

t | → 0.

t ; µ)| → 0,

(3.7)

(3.8)

(cid:12)(cid:12)(cid:12)(cid:12)

We ﬁrst show (3.7). Using the estimates in Lemma 3.1, we get
E|vx(t, X µ

t ; µk) − vx(t, X µ
u(t, X µ

t ; µ)|
t ; µ)[ux(t, X µ

= 2cσ2E(cid:12)(cid:12)(cid:12)(cid:12)

≤ C E|ux(t, X µ

t ; µk) − ux(t, X µ

t ; µk) − ux(t, X µ
u(t, X µ
C
√T − t

t ; µ)| +

t ; µ)] + ux(t, X µ
t ; µk)u(t, X µ
t ; µ)
E|u(t, X µ

t ; µ) − u(t, X µ

t ; µk)|.

t ; µ)[u(t, X µ

t ; µ) − u(t, X µ

t ; µk)]

Since all integrands are bounded, to show the expectations converge to zero, it suﬃce to check that
the integrands converge to zero a.s. Fix ω ∈ Ω, we know from (3.4) that

|ux(t, X µ
≤ C EZ(cid:20)

t (ω); µk) − ux(t, X µ
Rµk (X µ

|Z|

σ√T − t(cid:12)(cid:12)(cid:12)

t (ω); µ)|
t (ω) + σ√T − tZ) − Rµ(X µ

t (ω) + σ√T − tZ)(cid:12)(cid:12)(cid:12)(cid:21) .

Since W1(µk, µ) → 0, µk also converges to µ weakly, and the cumulative distribution function Fµk (x)
converges to Fµ(x) at every point x at which Fµ is continuous. It follows from the continuity of
R that Rµk (x) converges to Rµ(x) at every point x at which Fµ is continuous. Since Fµ has at
most countably many points of discontinuity, the random variable (the randomness comes from Z)
inside the expectation converges to zero PZ -a.s. Dominated convergence theorem then allows us to
interchange the limit and the expectation, giving that

Similarly, from (3.3) we obtain

|ux(t, X µ

t (ω); µk) − ux(t, X µ

t (ω); µ)| → 0.

t (ω), µk) − u(t, X µ

|u(t, X µ
Again, using that Fµ has countably many points of discontinuity, one can show that

t (ω) + σ√T − tZ) − Rµ(X µ

t (ω), µ)| ≤ C EZ(cid:12)(cid:12)(cid:12)

t (ω) + σ√T − tZ)(cid:12)(cid:12)(cid:12)

Rµk (X µ

.

Putting everything together, we have proved (3.7).

|u(t, X µ

t (ω), µk) − u(t, X µ

t (ω), µ)| → 0.

Next, we show (3.8) by Gronwall’s inequality. Let ǫ > 0 be given. We have for any r ∈ [0, t],

E|X µk

r − X µ

0

r | ≤Z r
≤Z r

0

E|vx(s, X µk
E(cid:20) C
T − s|X µk

s ; µk) − vx(s, X µ
s − X µ

s ; µ)| ds
s | + |vx(s, X µ

s ; µk) − vx(s, X µ

s ; µ)|(cid:21) ds

6

ERHAN BAYRAKTAR AND YUCHONG ZHANG

By (3.7) and bounded convergence theorem, we obtain

0
So for k large enough, we have

Z t

E|vx(s, X µ

s ; µk) − vx(s, X µ

s ; µ)|ds → 0.

E|X µk

r − X µ

r | ≤

By Gronwall’s inequality,

C

T − tZ r

0

E|X µk

s − X µ

s |ds + ǫe− Ct

T −t .

E|X µk

t − X µ

t | ≤ ǫe− Ct

T −t +

C

T − tZ t

0

ǫe− Ct

T −t e

C(t−s)
T −t ds = ǫ.

This completes the proof of (3.8), and thus the continuity of Φ. By Schauder’s ﬁxed point theorem,
there exists a ﬁxed point of Φ in the set E.
(cid:3)

3.2. Uniqueness of Nash equilibrium. Nash equilibrium is rarely unique except under the
following monotonicity assumption which is in the spirit of [8]:

Assumption 3.1. For any two ﬁxed points µ and µ′ of Φ, we have
(Rµ − Rµ′)(x)d(µ − µ′)(x) ≤ 0.

ZR

Remark 3.1. Assumption 3.1 is satisﬁed when Rµ(x) = Fµ(x) and all ﬁxed points of Φ have
continuous cumulative distribution function. To see this, note that if Fµ and Fµ′ are continuous
functions, then

(Fµ − Fµ′)d(µ − µ′) =

1
2

[(µ − µ′)(R)]2 = 0.

ZR

If the ﬁxed points of Φ do not necessarily have continuous cumulative distribution function, then
Assumption 3.1 is still satisﬁed if Fµ is replaced by its “regular” version ˜Fµ(x) := 1
2 (Fµ(x+) +
Fµ(x−)) (see [5, Theorem B]).
Proposition 3.1. Under Assumption 3.1, Φ has at most one ﬁxed point.

Proof. Suppose µ and µ′ are two ﬁxed points of Φ. To simplify notation, write v(t, x) := v(t, x; µ)
and v′(t, x) := v(t, x; µ′). Let X µ and X µ′
be the optimally controlled state processes (starting at
zero) in response to µ and µ′, respectively. Let t ∈ (0, T ). Using Itˆo’s lemma and the PDE satisﬁed
by v and v′, it is easy to show that

Ev(t, X µ

t ) = v(0, 0) + EZ t

0

1
4c

(vx)2(s, X µ

s )ds.

and

Ev′(t, X µ

t ) = v′(0, 0) + EZ t

0

1

4c(cid:2)2v′xvx − (v′x)2(cid:3) (s, X µ

s )ds

Write ∆v := v − v′, we obtain by subtracting (3.10) from (3.9) that
[(∆v)x(s, X µ

E∆v(t, X µ

t ) = ∆v(0, 0) + EZ t

0

1
4c

s )]2ds

Letting t → T and using the continuity of v and v′ at the terminal time, we get
[(∆v)x(s, X µ

T )] = E∆v(T, X µ

E[(Rµ − Rµ′)(X µ

T ) = ∆v(0, 0) + EZ T

0

1
4c

s )]2ds.

(3.9)

(3.10)

(3.11)

7

1
4c

[(∆v)x(s, X µ′

s )]2ds.

(3.12)

T ) = −∆v(0, 0) + EZ T
T ), µ′ = L(X µ′

0

T ), we get
s )]2ds = E[(Rµ − Rµ′)(X µ

T )] + E[(Rµ′ − Rµ)(X µ′
T )]

(Rµ − Rµ′)(x)d(µ − µ′)(x) ≤ 0,

Now, exchange the role of µ and µ′. We also have

E[(Rµ′ − Rµ)(X µ′

T )] = −E∆v(T, X µ′

Adding (3.11) and (3.12), and using that µ = L(X µ

[(∆v)x(s, X µ

s )]2 + [(∆v)x(s, X µ′

1
4c

0 ≤

EZ T

0

=ZR
where the last inequality follows from Assumption 3.1. This implies
s ) dP × dt-a.e.

s ) = v′x(s, X µ′

vx(s, X µ′

By the uniqueness of the solution of the SDE (3.6), we must have X µ

T = X µ′

T a.s. and µ = µ′. (cid:3)

3.3. Approximate Nash equilibrium of the N -player game. The MFG solution allows us to
construct, using decentralized strategies, an approximate Nash equilibrium of the N -player game
when N is large. In MFG literature, this is typically done using results from the propagation of
chaos. Here we in some sense have a simpler problem since the mean-ﬁeld interaction does not
enter the dynamics of the state process. And it is this special structure that allows us to handle
rank-based terminal payoﬀ which fails to be Lipschitz continuous in general.

Deﬁnition 3.1. A progressively measurable vector a = (a1, . . . , aN ) is called an ǫ-Nash equilibrium
of the N -player game if

0 |ai,t|dt < ∞ for any i ∈ {1, . . . , N}; and

(i) ER T
(ii) for any i ∈ {1, . . . , N}, and any progressively measurable process β satisfying ER T
∞, we have

0 |βt|dt <

E(cid:20)R¯µN,a (X ai
i,T =R T

i,T ) −Z T

0

ca2

t dt(cid:21) ,
i,tdt(cid:21) + ǫ ≥ E(cid:20)R
β = (a1, . . . , ai−1, β, ai+1, . . . , aN ) and ¯µN,a = 1

i,T ) −Z T

(X β

cβ2

N,ai
β

¯µ

0

.

j=1 δX

where X β

0 βtdt+σBi,T , ai

We now state an additional H¨older condition on R which allows us to get the convergence rate.

N PN
It holds, for example, when R(x, r) = A(x)rp + B(x) where p ∈ (0,∞) and A ∈ L∞(R).
Assumption 3.2. There exist constants L > 0 and α ∈ (0, 1] such that |R(x, r1) − R(x, r2)| ≤
L|r1 − r2|α for any r1, r2 ∈ [0, 1] and x ∈ R.
Theorem 3.2. Let Assumption 3.2 hold. For any ﬁxed point µ of Φ, ¯ai,t := (2c)−1vx(t, X ¯ai
i = 1, . . . , N form an O(N−α/2)-Nash equilibrium of the N -player game as N → ∞.
Proof. Let µ be a ﬁxed point of Φ, and let ¯ai,t be deﬁned as in the theorem statement. To keep
the notation simple, we omit the superscript of any state process if it is controlled by the optimal
Markovian feedback strategy (2c)−1vx(t, x; µ). Let

i,t; µ),

aj
j,T

be the value of the limiting game where X satisﬁes (3.6), and

V := v(0, 0; µ) = E(cid:20)Rµ(XT ) −Z T
:= E(cid:20)R¯µN (Xi,T ) −Z T

J N
i

0

0

c¯a2

i,sds(cid:21)

1
4c

v2

x(s, Xs; µ)ds(cid:21)

8

ERHAN BAYRAKTAR AND YUCHONG ZHANG

be the net gain of player i in an N -player game, if everybody use the candidate approximate Nash
equilibrium (¯a1, . . . , ¯aN ). Here ¯µN = 1
i=1 δXi,T . Since our state processes do not depend on
the empirical measure (the interaction is only through the terminal payoﬀ), each Xi is simply an
independent, identical copy of X. Hence

N PN

V = E(cid:20)Rµ(Xi,T ) −Z T

0

c¯a2

i,sds(cid:21) .

Let us ﬁrst show that J N

i and V are close. We have

J N
i − V = E[R¯µN (Xi,T ) − Rµ(Xi,T )].

It follows from the α-H¨older continuity of R that

i − V | ≤ E|R¯µN (Xi,T ) − Rµ(Xi,T )| ≤ LE[|F¯µN (Xi,T ) − Fµ(Xi,T )|α] ≤ LE[k ˆF N
|J N

µ − Fµkα
∞]

where for n ∈ N, ˆF n
µ denotes the empirical cumulative distribution function of n i.i.d. random
variables with cumulative distribution function Fµ. By Dvoretzky-Kiefer-Wolfowitz inequality, we
have

It follows that

P(cid:16)k ˆF N

µ − Fµk∞ > ǫ(cid:17) ≤ 2e−2N ǫ2

.

∞] = LZ ∞

µ − Fµkα
2e−2N z2/α

i − V | ≤ LE[k ˆF N
|J N
≤ LZ ∞
= O(N−α/2) as N → ∞.

dz =

0

0
2L

P(cid:16)k ˆF N
(4N )α/2 Z ∞

0

µ − Fµkα
2 y2/α

e− 1

∞ > z(cid:17) dz

dy

Next, consider the system where player i makes a unilateral deviation from the candidate approx-
imate Nash equilibrium (¯a1, . . . , ¯aN ); say, he chooses an admissible control β. Denote his controlled
state process by X β
i , and the state processes of all other players by Xj as before for j 6= i. Let
¯νN := 1
+Pj6=i δXj,T ) be the corresponding empirical measure of the terminal states, and

N (δX β

i,T

J N,β
i

:= E(cid:20)R¯νN (X β

i,T ) −Z T

0

cβ2

s ds(cid:21)

be the corresponding net gain for player i. We have

J N,β

cβ2

i − V = E(cid:20)R¯νN (X β
= EhR¯νN (X β
≤ EhR¯νN (X β

s ds(cid:21) − E(cid:20)Rµ(Xi,T ) −Z T
i,T ) −Z T
i,T ) −Z T
i,T )i + E(cid:20)Rµ(X β
i,T ) − Rµ(X β
i,T )i
i,T ) − Rµ(X β

0

0

0

c¯a2

i,sds(cid:21)
s ds(cid:21) − E(cid:20)Rµ(Xi,T ) −Z T

0

cβ2

c¯a2

i,sds(cid:21)

where the inequality follows from the optimality of ¯ai for the i-th player’s problem. Similar to how
we estimate |J N
i − V |, we have
i − V ≤ LE[|F¯νN (X β
J N,β

9

i,T ) − Fµ(X β
i,T )|α]
N (cid:16)1 − Fµ(X β
i,T )(cid:17) +
N − 1
N k ˆF N−1
Ek ˆF N−1

N − 1

N

N

+

+

µ

N − 1
N (cid:16) ˆF N−1
− Fµk∞(cid:19)α(cid:21)
− Fµk∞(cid:19)α

µ

µ

1

= LE(cid:20)(cid:12)(cid:12)(cid:12)(cid:12)
≤ LE(cid:20)(cid:18) 1
≤ L(cid:18) 1

N

(X β

i,T ) − Fµ(X β

α(cid:21)
i,T )(cid:17)(cid:12)(cid:12)(cid:12)(cid:12)

= O(N−α/2) as N → ∞,

where we used Jensen’s inequality in the fourth step. Combining the two estimates, we obtain

J N,β
i − J N

i ≤ J N,β

i − V + |V − J N

i

| = O(N−α/2) as N → ∞.

This shows (¯a1, . . . , ¯aN ) is an O(N−α/2)-approximate Nash equilibrium.

(cid:3)

Remark 3.2. Without Assumption 3.2, one can still use the continuity and boundedness of R to
get convergence; that is, the MFG solution still provides an approximate Nash equilibrium of the
N -player game. However, the convergence rate is no longer valid.

4. Mean Field Approximation when there is common noise

In this section, we assume σ0 > 0 and R(x, r) is independent of x; the latter means the reward
is purely rank-dependent. Unlike the case with only idiosyncratic noises, since the common noise
does not average out as N → ∞, the limiting environment becomes a random measure instead of
a deterministic one. So the MFG problem now reads:

(i) Fix a random measure µ of the terminal distribution of the population where the randomness
comes from the common noise W , and solve the stochastic control problem faced by a
representative player:

where

V (µ) = sup

a

E(cid:20)Rµ(XT ) −Z T

0

1
2

a2

sds(cid:21) ,

dXs = asds + σdBs + σ0dWs, X0 = 0.

Denote the optimally controlled state process by X µ.
T|W ).

(ii) Find a ﬁxed point of the mapping Ψ : µ 7→ L(X µ

(4.1)

(4.2)

For µ ∈ P(R), denote by µ(· + q) the probably measure obtained by shifting µ to the left by q ∈ R.
Observe that when R is independent of x, we have Rµ(x + q) = R(Fµ(x + q)) = R(Fµ(·+q)(x)) =
Rµ(·+q)(x). So we are precisely in the framework of translation invariant MFGs. In fact, purely
rank-based functions should be another important example of translation invariant functions besides
the convolution and local interaction given in [7]. In the general case without translation invariance,
results have only been obtained in the weak formulation, see [3].

In the remaining discussion, let us refer to the problems (3.1)-(3.2) and (4.1)-(4.2) together with
their respective ﬁxed point problems as MFG0 and MFGcn, respectively. A direct application of
[7, Theorem 2.5] yields the following existence result.

10

ERHAN BAYRAKTAR AND YUCHONG ZHANG

Proposition 4.1. Let ¯µ be a (deterministic) equilibrium measure of MFG0. Then

is a (random) equilibrium measure of MFGcn. Moreover, the optimal control associated with ¯µ for
MFG0 is also an optimal open loop control associated with µ for MFGcn.

µ := ¯µ(· − σ0WT )

The intuition is that the whole population is aﬀected in parallel by the common noise. Thus, the
eﬀect of common noise is essentially cancelled out in the optimization problem due to translation
invariance. Such a random equilibrium measure is clearly σ(W )-measurable, hence is a strong MFG
solution in the language of [2].

Remark 4.1. Here we are not taking the optimal feedback form a∗(t, x) = (2c)−1vx(t, x; ¯µ) and
plugging in the state process X with common noise, but directly using a∗(t, X◦t ; ¯µ) where X◦ is the
solution to (3.2) as an open loop control for MFGcn. Such a control is independent of the common
noise. Here it is reasonable to use open loop controls because for an N -player game, the individual
can observe all state processes. When N is large, the individual noises average out. Thus, observing
the entire system should give some information about the common noise. Passing to the MFG limit,
the agent should be allowed more information than the one generated by his own state process.

Theorem 4.1. Let Assumption 3.2 hold. Let ¯µ be an equilibrium measure of MFG0, and let
¯ai,t = (2c)−1vx(t, X◦i,t; ¯µ) where X◦i is the solution to (3.2) with B replaced by Bi. Then (¯a1, . . . , ¯aN )
form an O(N−α/2)-Nash equilibrium of the N -player game with common noise.

Proof. Let µ and X◦ be deﬁned as in Proposition 4.1 and Remark 4.1. Also let X := X◦ + σ0W
and Xi := X◦i + σ0W . By Proposition 4.1, we have

Also let

0

1
4c

V := V (µ) = E(cid:20)Rµ(XT ) −Z T
:= E(cid:20)R¯µN (Xi,T ) −Z T
N PN

J N
i

0

v2

x(s, X◦s ; ¯µ)ds(cid:21) .
i,sds(cid:21)

c¯a2

be the net gain of player i in an N -player game, if everybody use the candidate approximate Nash
equilibrium (¯a1, . . . , ¯aN ). Here ¯µN := 1
i=1 δXi,T denotes the empirical measure of the terminal
state of the system. Translation invariance and the deﬁnition of µ imply

Rµ(XT ) = Rµ(X◦T + σ0WT ) = Rµ(·+σ0WT )(X◦T ) = R¯µ(X◦T ).

Similarly, since

¯µN =

1
N

N

Xi=1

we also have

δX ◦

i,T +σ0WT =  1

N

N

Xi=1

δX ◦

i,T! (· − σ0WT ) =: ¯µN

◦ (· − σ0WT ),

Hence we can rewrite V and J N

i as

R¯µN (Xi,T ) = R¯µN (X◦i,T + σ0WT ) = R¯µN

◦

(X◦i,T ).

V = E(cid:20)R¯µ(X◦i,T ) −Z T

0

c¯a2

i,sds(cid:21)

and J N

i = E(cid:20)R¯µN

◦

(X◦i,T ) −Z T

0

c¯a2

i,sds(cid:21) ,

where we also used that X◦i has the same distribution as X◦. From the proof of Theorem 3.2, we
know

|J N
i − V | ≤

2L

(4N )α/2 Z ∞

0

e− 1

2 y2/α

dy = O(N−α/2) as N → ∞.

11

Next, suppose player i makes a unilateral deviation to some admissible control β. Denote his
, respectively. We have

controlled state process with and without common noise by X β
X β

i and X◦,β

i + σ0W . Let ¯νN := 1

i = X◦,β

i

N (δX β

i,T

+Pj6=i δXj,T ) and
i,T ) −Z T
:= E(cid:20)R¯νN (X β

0

J N,β
i

cβ2

s ds(cid:21) .

We have by the deﬁnition of V and the H¨older continuity of R that

J N,β

i − V = E(cid:20)R¯νN (X β
= EhR¯νN (X β
≤ EhR¯νN (X β

0

cβ2

i,T ) −Z T
i,T ) − Rµ(X β
i,T ) − Rµ(X β
i,T ] = 1

s ds(cid:21) − V
i,T ) −Z T
i,T )i + E(cid:20)Rµ(X β
i,T )i ≤ LEh|¯νN (−∞, X β
N +(cid:16) 1

0

cβ2

s ds(cid:21) − V
i,T ] − µ(−∞, X β

i,T ]|αi .

By translation invariance, ¯νN (−∞, X β
i,T ]. By deﬁnition of µ,
µ(−∞, X β
i,T ]. Also note that for j 6= i, L(X◦j,T ) = L(X◦T ) = ¯µ since ¯µ is an
equilibrium measure for MFG0. Therefore, by expressing everything in terms of ¯µ, X◦ and X◦,β,
we are again back in the framework without common noise. The proof of Theorem 3.2 implies that

i,T ] = ¯µ(−∞, X◦,β

N Pj6=i δX ◦

j,T(cid:17) (−∞, X◦,β

J N,β

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
i − V ≤ LE

= LE(cid:20)(cid:12)(cid:12)(cid:12)(cid:12)
≤ L(cid:18) 1

N

1
N

1
N

+

δX ◦

1




+
N Xj6=i

(1 − F¯µ(X◦,β
N − 1

N

Ek ˆF N−1

¯µ

i,T )) +

i,T ](cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
 (−∞, X◦,β

(X◦,β

α


¯µ

 − ¯µ
j,T
N − 1
N (cid:16) ˆF N−1
− F¯µk∞(cid:19)α
i − J N

i,T ) − F¯µ(X◦,β

α(cid:21)
= O(N−α/2) as N → ∞.

i,T )(cid:17)(cid:12)(cid:12)(cid:12)(cid:12)

where ˆF n
mulative distribution function F¯µ. We conclude that J N,β
is an O(N−α/2)-Nash equilibrium of the N -player game with common noise.

¯µ denotes the empirical cumulative distribution function of n i.i.d. random variables with cu-
i ≤ O(N−α/2) and that (¯a1, . . . , ¯aN )

(cid:3)

Remark 4.2. The arbitrary control β in the above proof may depend on the common noise. How-
ever, the additional information of the common noise gives each player very little advantage when
everyone else use their respective ¯ai’s which are independent of the common noise.

References

[1] Ren´e Carmona and Fran¸cois Delarue. Probabilistic analysis of mean-ﬁeld games. SIAM J. Control Optim.,

51(4):2705–2734, 2013.

[2] Ren´e Carmona, Fran¸cois Delarue, and Daniel Lacker. Mean ﬁeld games with common noise. To appear in Annals

of Probability.

[3] Ren´e Carmona and Daniel Lacker. A probabilistic weak formulation of mean ﬁeld games and applications. Ann.

Appl. Probab., 25(3):1189–1231, 2015.

[4] Olivier Gu´eant, Jean-Michel Lasry, and Pierre-Louis Lions. Mean ﬁeld games and applications. In Paris-Princeton
Lectures on Mathematical Finance 2010, volume 2003 of Lecture Notes in Math., pages 205–266. Springer, Berlin,
2011.

[5] Edwin Hewitt. Integration by parts for Stieltjes integrals. Amer. Math. Monthly, 67:419–423, 1960.
[6] Minyi Huang, Roland P. Malham´e, and Peter E. Caines. Large population stochastic dynamic games: closed-loop

McKean-Vlasov systems and the Nash certainty equivalence principle. Commun. Inf. Syst., 6(3):221–251, 2006.

[7] Daniel Lacker and Kevin Webster. Translation invariant mean ﬁeld games with common noise. Electron. Commun.

Probab., 20:no. 42, 13, 2015.

12

ERHAN BAYRAKTAR AND YUCHONG ZHANG

[8] Jean-Michel Lasry and Pierre-Louis Lions. Mean ﬁeld games. Jpn. J. Math., 2(1):229–260, 2007.
[9] C´edric Villani. Optimal transport: old and new, volume 338 of Grundlehren der Mathematischen Wissenschaften

[Fundamental Principles of Mathematical Sciences]. Springer-Verlag, Berlin, 2009.

(Erhan Bayraktar) Department of Mathematics, University of Michigan.

E-mail address: erhan@umich.edu

(Yuchong Zhang) Department of Statistics, Columbia University.

E-mail address: yz2915@columbia.edu

