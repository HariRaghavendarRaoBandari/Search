General Tensor Spectral Co-clustering

for Higher-Order Data

Tao Wu

Purdue University
West Lafayette, IN

wu577@purdue.edu

Austin R. Benson
Stanford University

Stanford, CA

arbenson@stanford.edu

David F. Gleich
Purdue University
West Lafayette, IN

dgleich@purdue.edu

6
1
0
2

 
r
a

M
1

 

 
 
]
I
S
.
s
c
[
 
 

1
v
5
9
3
0
0

.

3
0
6
1
:
v
i
X
r
a

ABSTRACT
Spectral clustering and co-clustering are well-known tech-
niques in data analysis, and recent work has extended spec-
tral clustering to square, symmetric tensors and hypermatri-
ces derived from a network. We develop a new tensor spec-
tral co-clustering method that applies to any non-negative
tensor of data. The result of applying our method is a si-
multaneous clustering of the rows, columns, and slices of a
three-mode tensor, and the idea generalizes to any number
of modes. The algorithm we design works by recursively
bisecting the tensor into two pieces. We also design a new
measure to understand the role of each cluster in the tensor.
Our new algorithm and pipeline are demonstrated in both
synthetic and real-world problems. On synthetic problems
with a planted higher-order cluster structure, our method
is the only one that can reliably identify the planted struc-
ture in all cases. On tensors based on n-gram text data,
we identify stop-words and semantically independent sets;
on tensors from an airline-airport multimodal network, we
ﬁnd worldwide and regional co-clusters of airlines and air-
ports; and on tensors from an email network, we identify
daily-spam and focused-topic sets.

Keywords
tensor; spectral clustering; co-clustering

1.

INTRODUCTION

Clustering is a fundamental task in data mining that aims
to assign closely related entities to the same group. Tra-
ditional methods optimize some aggregate measure of the
strength of pairwise relationships between items, such as the
sum of similarity between all pairs of entities in the same
group. Spectral clustering is a particularly powerful tech-
nique for computing the clusters when the pairwise similari-
ties are encoded into the adjacency matrix of a graph. How-
ever, many graph-like datasets are more naturally described
by higher-order connections among several entities. For in-
stance, multilayer or multiplex networks describe the inter-
actions between several graphs simultaneously with layer-
node-node relationships [21].

Tensors are a common representation for many of these
higher-order datasets. A tensor, or hypermatrix, is a mul-
tidimensional array with an arbitrary number of indices or
modes. A tensor with two-modes is equivalent to a matrix,
and a tensor with three-modes looks like a three-dimensional
brick. We recently proposed a Tensor Spectral Clustering
(TSC) framework as a generalization of spectral methods

for higher-order graph data [6]. This method was designed
for the case when the higher-order tensor recorded the oc-
currences of small subgraph patterns within the network.
For instance, the i, j, kth element of the tensor denoted the
presence of a triangle or a directed 3-cycle. The method
was particularly successful at identifying clusters that cor-
responded to diﬀerent layers of a directed network.

The TSC framework had a number of limitations, how-
ever. First, it was primarily designed for the case when the
tensor arose based on some underlying graph. The partition-
ing metric used was designed explicitly for this case. Thus,
the applications are limited in scope and cannot model,
for example, multiplex networks. Second, the TSC frame-
work involved viewing the tensor as the transitions on a
higher-order Markov chain—similar to how spectral cluster-
ing views pairwise data as the transitions of a ﬁrst-order
Markov chain—and then using a spacey random walk on
this higher-order Markov chain to identify the clusters [5].
When the data are sparse, the spacey random walk required
a correction whose magnitude is proportional to the spar-
sity in the tensor.
In many instances, this correction was
substantial because the tensor was extremely sparse. This
made it diﬃcult to accurately identify clusters.

Here we develop the General Tensor Spectral Co-clustering
(GTSC) framework for clustering general sparse tensor data.
Our method is based on a new super-spacey random walk
model that more accurately models higher-order Markov
chains and avoids the correction necessary for the previous
spacey random walk model. Furthermore, we show how to
use our method on rectangular tensor data through a tensor
symmetrization procedure [29]. This allows us to simulta-
neously cluster the rows, columns, and slices of three-mode
tensors. The idea generalizes to any number of modes where
we cluster the objects represented in each mode indepen-
dently. We also introduce a variant on the well-known con-
ductance measure for partitioning graphs [30] that we call
biased conductance and describe how this provides a tensor
partition quality metric. In particular, biased conductance
is approximately the exit probability from a set following
our new super-spacey random walk model.

The algorithm underlying our GTSC framework recur-
sively partitions the tensor data. We stop the process when
the partitioning metric is bad, which lets us cluster the data
without specifying the number of clusters ahead of time.
This provides an advantage over existing tensor decomposi-
tion methods such as PARAFAC that require the number of
clusters as an input to the algorithms. Finally, we show that
our method asymptotically scales linearly (up to logarithmic

factors) in the size of data. In Section 4.2, we perform nu-
merical scaling experiments to demonstrate the scalability
of our method.

Our contributions are summarized as follows:

We use extensive experiments on both synthetic and real-
world problems to validate the eﬀectiveness of our method.
For the synthetic experiments, we devise a “planted clus-
ter” model for tensors and show that GTSC has superior
performance compared to other state-of-the-art clustering
methods in recovering the planted clusters. In Section 6, we
analyze tensor data from a variety of real-world domains, in-
cluding text mining, air travel, and communication. We ﬁnd
that our GTSC framework identiﬁes stop-words and seman-
tically independent sets in n-gram tensors, worldwide and
regional airlines and airports in a ﬂight multiplex network,
and topical sets in an email network.
• We create a new super-spacey random walk model for ten-
sor data and use it to create a partitioning quality measure
based on a notion of biased conductance (Section 3).
• We develop GTSC, a new framework for clustering rect-
angular tensor data (Section 4, Algorithm 1). The frame-
work simultaneously clusters the rows, columns, and slices
of the tensor.
• We show that GTSC outperforms other state-of-the-art
clustering methods at identifying the planted cluster struc-
ture in synthetic tensor data, in terms of normalized mu-
tual information, F1 score, and adjusted rand index (Sec-
tion 5).
• We empirically demonstrate that GTSC identiﬁes coher-

ent clusters in real-world datasets (Section 6).1

+

Let A ∈ Rn×n

2. FIRST-ORDER SPECTRAL METHODS
We ﬁrst review graph clustering methods from the view of
graph cuts and random walks, and then review the standard
spectral clustering method using sweep cuts. In Section 3,
we generalize these notions to higher-order data in order to
develop our GTSC framework.
2.1 Preliminaries and Notation
be the adjacency matrix of an undirected
graph G = (V, E) and let n = |V | be the number of nodes
in the graph. Deﬁne D = diag(Ae) to be the diagonal
matrix of degrees of vertices in V . The graph Laplacian is
L = D − A and the transition matrix is P = AT D−1. The
transition matrix represents the transition probabilities of
a random walk on the graph.
If a walker is at node j, it
transitions to node i with probability Pij = Aij/Djj.
2.2 Conductance and Markov Chains
One of the most widely-used quality metrics for parti-
tioning a graph’s vertices into two sets S and ¯S = V \S is
conductance [30]. Intuitively, conductance measures the ra-
tio of the number of edges in the graph that go between S
and ¯S to the number of edges in S or ¯S. Formally, we deﬁne
conductance as:

φ(S) = cut(S)/min(cid:0)vol(S), vol( ¯S)(cid:1),
(cid:88)
(cid:88)

where

cut(S) =

Aij

and

vol(S) =

Aij.

i∈S,j∈ ¯S

i∈S,j∈V

(1)

(2)

1Code and data for this paper are available at: https://github.
com/wutao27/GtensorSC.

A set S with small conductance is a good partition (S, ¯S).
The numerator minimizes the edges going between the sets,
and the denominator encourages either S or ¯S to be large
in volume and that both are balanced.

The following well-known proposition relates conductance

to random walks on the graph.

Observation 1

([23]). Let G be undirected, connected,
and not bipartite. Start a random walk (Xt)t∈N where the
initial state X0 is randomly chosen following the stationary
distribution of the random walk. Then for any set S ∈ V ,

φ(S) = max(cid:8)Pr(X1 ∈ ¯S | X0 ∈ S), Pr(X1 ∈ S | X0 ∈ ¯S)(cid:9).

This provides an alternative view of conductance—it mea-
sures the probability that one step of a random walk will
traverse between S and ¯S. Again, small conductance is in-
dicative of a good partition: a small probability means that
the sets have more internal connections than external con-
nections. This random walk view, in concert with the super-
spacey random walk, will serve as the basis for our biased
conductance idea to partition tensors in Section 3.4.
2.3 Spectral Partitioning with Sweep Cuts

Finding the set of minimum conductance is an NP-hard
combinatorial optimization problem [31]. However, there are
real-valued relaxations of the problem that are tractable to
solve and provide a guaranteed approximation [24, 12]. The
most well known computes an eigenvector called the Fiedler
vector and then uses a sweep cut to identify a partition based
on this eigenvector.

The Fiedler eigenvector z solves Lz = λDz where λ is the
second smallest generalized eigenvalue. This can be equiv-
alently formulated in terms of the random walk transition
matrix P . Speciﬁcally,
−1A)z = λz ⇔ zT P = (1 − λ)z.
Lz = λDz ⇔ (I − D
In other words, the Fiedler vector is simultaneously the gen-
eralized eigenvector with the second smallest generalized
eigenvalue of the Laplacian and degree and the left eigenvec-
tor of P with the second largest eigenvalue. This equivalence
is important for our generalizations to higher-order data in
Section 3.

The sweep cut procedure to identify a low-conductance set

S from z is as follows:
1. Sort the vertices by z as zσ1 ≤ zσ2 ≤ ··· ≤ zσn .
2. Consider the n − 1 candidate sets Sk = {σ1, σ2,··· , σk}

for 1 ≤ k ≤ n − 1

3. Choose S = argminSk
The solution set S from this algorithm satisﬁes the cele-

brated Cheeger inequality [24, 11]: φ(S) ≤ 4(cid:112)φopt, where

φ(Sk) as the solution set.

φopt = minS⊂V φ(S) is the minimum conductance over any
set of nodes. This procedure is extremely eﬃcient since Sk+1
and Sk diﬀer only in the vertex σk+1. The conductance value
of the set can be updated in time proportional to the degree
of vertex σk1 and thus the sweep cut procedure only takes
linear time in the number of edges in the graph.

To summarize, the spectral method requires two compo-
nents: the second left eigenvector of P and the conductance
criterion.

3. HIGHER-ORDER SPECTRAL METHOD
We now generalize the ideas from spectral graph partition-
ing to nonnegative tensor data. We ﬁrst review our notation

for tensors and then review how tensor data can be inter-
preted as a higher-order Markov chain. We brieﬂy review
our prior work on Tensor Spectral Clustering before intro-
ducing the new super-spacey random walk that we use here.
This super-spacey random walk will allow us to compute a
vector akin to the Fiedler vector for a tensor and to gen-
eralize conductance to tensors. Furthermore, we generalize
the ideas from co-clustering in bipartite graph data [13] to
rectangular tensors.
3.1 Preliminaries and Tensor Notation

We use T to denote a tensor. As a generalization of a
matrix, it may have up to m indices—called an m-mode
tensor—and so an individual element is Ti1,i2,··· ,im . We will
work with non-negative tensors where Ti1,i2,··· ,im ≥ 0. We
call a subset of the tensor entries with all but the ﬁrst el-
ement ﬁxed a column of the tensor. For instance, the j, k
column is T:,j,k. A tensor is square if the dimension of all
the modes is equal and rectangular if not; a square tensor is
symmetric if it is equal for any permutation of the indices.
For simplicity in the remainder of our exposition, we will
focus on the three-mode case but everything we talk about
generalizes to an arbitrary number of modes. (See [17, 5]
for representative examples of how the generalizations look.)
We use two operations between a tensor and a vector. First,
a tensor-vector product with a three-mode tensor can output
a vector, which we denote by:

y = T x2 ⇔ yi =(cid:80)
A = T [x] ⇔ Ai,j =(cid:80)

j,k Ti,j,kxjxk.

k Ti,j,kxk.

Second, a tensor-vector product can also produce a matrix,
which we denote by:

3.2 Higher-order Markov Chains

Recall from Section 2 that we could form the transition
matrix for a Markov chain from a square non-negative ma-
trix A by normalizing the columns of the matrix AT . We
can generalize this idea to deﬁne a higher-order Markov
chain by normalizing a square tensor. This leads to a prob-
ability transition tensor P :

Pi,j,k = Ti,j,k/(cid:80)

i Ti,j,k

(3)

where we assume (cid:80)

(In Section 3.3, we will
discuss the sparse case where the column T:,j,k may not have
any non-zero entries.)

i Ti,j,k > 0.

Entries of P can be interpreted as the transition proba-

bilities of a higher-order Markov chain Zt:

Pi,j,k = Pr(Zt+1 = i | Zt = j, Zt−1 = k).

In terms of random walks, if the last two states were j and
k, then the next state is i with probability Pi,j,k.

It is possible to turn any higher-order Markov chain into
a ﬁrst-order Markov chain on the product state space of all
ordered pairs (i, j). The new Markov chain moves to the
state-pair (i, j) from (j, k) with probability Pi,j,k. Comput-
ing the Fiedler vector associated with this chain would be
one possible strategy. However, this approach has two im-
mediate problems. First, the eigenvector is of size n2, which
quickly becomes too large. Second, the eigenvector gives in-
formation about the product space—not the original data.
In other words, it is unclear how to use this eigenvector even
if we could aﬀord to compute and store it.

In our prior work, we used the spacey random walk and
spacey random surfer stochastic processes for a scalable ran-
dom walk process [6, 5]. This stochastic process is non-
Markovian and generates a sequence of states Xt as follows.
After arriving at state Xt, the walker promptly “spaces out”
and forgets the state Xt−1, yet it still wants to transition
according to the higher-order transitions P . So it invents
a state Yt by drawing a random state from its history and
then transition to state Xt+1 with probability PXt+1,Xt,Yt .
If Ht denotes the history of the process up to time t,2 then

(cid:0)1 +(cid:80)t

r=1 Ind{Xr = j}(cid:1) .

(4)

Pr(Yt = j | Ht) = 1

t+n

In this case, we assume that the process has a non-zero prob-
ability of picking any state by inﬂating its history count
by 1 visit. The spacey random surfer is a generalization
where the walk follows the above process with probability
α and teleports at random following a stochastic vector v
with probability 1 − α. This is akin to how the PageRank
random walk includes teleportation.

These spacey random walk processes are instances of ver-
tex reinforced random walks [4, 28]. Limiting stationary dis-
tributions are solutions to the multilinear PageRank prob-
lem [17]:

αP x2 + (1 − α)v = x.

(5)

As shown in [5], the limiting distribution x represents the
stationary distribution of the transition matrix P [x]. The
transition matrix P [x] asymptotically approximates the spacey
walk or spacey random surfer. Thus, it is feasible to compute
an eigenvector of this matrix and use it with the sweep cut
procedure on a generalized notion of conductance. However,
we had to assume that all the columns of T were non-zero,
which does not occur in real-world datasets. In other words,
all of the n2 possible transitions from pairs of states were
well deﬁned. In our prior work, we adjusted the tensor T
and replaced any columns of all zeros with the uniform dis-
tribution vector. (This is easy to do in a way that does not
require n2 storage.) Because the number of zero-columns
could be extremely large, this was not ideal—although it
gave promising results when the tensors arose from graphs
for a notion of conductance that was speciﬁc to these graphs.
We deal with this issue more generally in the following sec-
tion, and note that our new solution outperforms the old
one as described in the experiments.

3.3 A Stochastic Process for Sparse Tensors

Here we consider another model of the random surfer that
avoids the issue of undeﬁned transitions—which correspond
to columns of T that are all zero—entirely. If the surfer at-
tempts to use an undeﬁned transition, then the surfer moves
to a random state drawn from history. Formally, deﬁne the
set of feasible states by

F = {(j, k) |(cid:88)

Ti,j,k > 0}.

(6)

i

Here, the set F denotes all the columns in T that are non-
zero. The transition probabilities of our proposed stochastic

2Formally, this is the σ-algebra generated by the states
X1, . . . , Xt.

αPr(Xt+1 = i | Xt = j, Yt = k, Ht)Pr(Yt = k | Ht)
(8)

Pr(Xt+1 = i | Xt = j, Yt = k, Ht)

(7)

process are given by

Pr(Xt+1 = i | Xt = j, Ht)

= (1 − α)vi +

(cid:40)
Ti,j,k/(cid:80)
n+t (1 +(cid:80)t

1

=

i Ti,j,k
r=1 Ind{Xr = i})

(j, k) ∈ F
(j, k) (cid:54)∈ F ,

where vi is the teleportation probability. Again Yt is chosen
according to Equation (4). We call this process the super-
spacey random surfer because when the transitions are not
deﬁned it picks a random state.

P be the normalized tensor Pi,j,k = Ti,j,k/(cid:80)

This process is also a vertex-reinforced random walk. Let
i Ti,j,k only for
the columns in F and where all other entries are zero. Sta-
tionary distributions of the stochastic process must satisfy
the equation:

αP x2 + α(1 − (cid:107)P x2(cid:107)1)x + (1 − α)v = x,

(9)

where x is a probability distribution vector. (See Appendix
A for the derivation.) At least one solution must exist, which
follows directly from the Brouwer ﬁxed-point theorem. Here
we give a suﬃcient condition for it to be unique.

Theorem 3.1. If α < 1/(2m − 1) then there is a unique
solution x to (9) for the general m-mode tensor. Further-
more, the iterative ﬁxed point algorithm

xk+1 = αP x2

k + α(1 − (cid:107)P x2

k(cid:107)1)xk + (1 − α)vk

(10)

will converge to this solution.

(See the Appendix B for the proof.)

In practice we found high values (e.g., 0.95) of α did not
impede convergence. We use α = 0.8, vi = 1/n for all of our
experiments in this paper.
3.4 Biased Conductance for Tensor Partitions
From Observation 1 in Section 2.2, we know that conduc-
tance may be interpreted as the exit probability between
two sets that form a partition of the nodes in the graph.
In this section, we derive an equivalent ﬁrst-order Markov
chain from the stationary distribution of the super-spacey
random surfer. If this Markov chain was guaranteed to be
reversible, then we could apply the standard deﬁnitions of
conductance and the Fiedler vector. This will not generally
be the case, and so we introduce a biased conductance mea-
sure to partition this non-reversible Markov chain with re-
spect to starting in the stationary distribution of the super-
spacey random walk. We use the second largest, real-valued
eigenvector of the Markov chain as an approximate Fiedler
vector. Thus, we can use the sweep cut procedure described
in Section 2.3 to identify the partition.

In the following derivation, we use the property of the two

tensor-vector products:

P [x]x = P x2.

The stationary distribution x for this super-spacey random
surfer (described in Section 3.3) is equivalently the station-
ary distribution of the Markov chain with transition matrix

α(cid:0)P [x] + x(eT − eT P [x])(cid:1) + (1 − α)veT .

Figure 1: Symmetrization of a rectangular tensor. The ten-
sor is ﬁrst embedded into a larger square tensor (left) and
then this square tensor is symmetrized (right).

(To understand this expression, note that x ≥ 0 and eT x =
1.) We introduce the following ﬁrst-order Markov chain

˜P = P [x] + x(eT − eT P [x]).

This matrix represents a useful (but crude) approximation
of the higher-order structure in the data. First, we deter-
mine how often we visit states using the super-spacey random
surfer to get a vector x. Then the Markov chain ˜P will tend
to have a large probability of spending time in states where
the higher-order information concentrates. This matrix rep-
resents a ﬁrst-order Markov chain on which we can compute
an eigenvector and run a sweep cut.

We now deﬁne a biased conductance to partition it.

Biased Conductance. Consider a random walk (Xt)t∈N.
The biased conductance φp(S) of a set S ⊂ {1, . . . , n} is

φp(S) = max(cid:8)Pr(X1 ∈ ¯S | X0 ∈ S), Pr(X1 ∈ S | X0 ∈ ¯S)(cid:9),

where X0 is chosen according to a ﬁxed distribution p.

This deﬁnition is general and has a few subtle aspects that
we wish to acknowledge. First, the initial state X0 is not
chosen following the stationary distribution (as in the stan-
dard deﬁnition on a reversible chain) but following p in-
stead. This is why we call it biased conductance. We apply
this measure to ˜P using p = x (the stationary distribu-
tion of the super-spacey walk). This choice emphasizes the
higher-order information.

We use the eigenvector of ˜P with the second-largest real
eigenvalue as an analogue of the Fiedler vector. If the chain
were reversible, this would be exactly the Fiedler vector.
When it is not, then it encodes largely the same type of
information and is a practical heuristic. It is important to
note that although ˜P is a dense matrix, we can implement
the two operations we need with ˜P in time and space that
depends only on the number of non-zeros of the sparse ten-
sor P using standard iterative methods for eigenvalues of
matrices.
3.5 Rectangular Tensor and Co-clustering

So far, we have only considered square, symmetric tensor
data. However, tensor data are often rectangular. This is
usually the case when the diﬀerent modes represent diﬀerent
types of data. For example, in Section 6, we examine a ten-
sor T ∈ Rp×n×n of airline ﬂight data, where Ti,j,k represents
that there is a ﬂight from airport j to airport k on airline
i. Our approach is to embed the rectangular tensor into a

Algorithm 1 General Tensor Spectral Co-clustering

, α ∈ (0, 1)

Require:

+

Symmetric square tensor T ∈ Rn×n×n
Stopping criterion max-size, min-size, φ∗
Partitioning C of indices {1, . . . , n}.

Ensure:
1: C = {{1, . . . , n}}
2: IF n ≤ min-size: RETURN
3: Generate transition tensor P by

(cid:40)
Tijk/(cid:80)n

i=1 Tijk

if (cid:80)n

i=1 Tijk > 0

Pijk =

0

otherwise

4: Compute super-spacey stationary vector x (Equation

with bias p = x.

(9)) and form P [x].
˜P = P [x] + x(eT − eT P [x]) (that is, zT ˜P = λzT ).

5: Compute second largest left, real-valued eigenvector z of
6: σ ← Sort eigenvector z
7: (S, φp) ← Biased Conductance Sweep Cut(σ, P [x])
8: if n ≥ max-size or φp ≤ φ∗ then
9:
10:
11:
12: end if
13: RETURN C

CS = Algorithm 1 on sub-tensor T S,S,S.
C ¯S = Algorithm 1 on sub-tensor T ¯S, ¯S, ¯S.
C = CS ∪ C ¯S.

(cid:20) 0

larger square tensor and then symmetrize this tensor, using
approaches developed by Ragnarsson and Van Loan [29].
After the embedding, we can run our algorithm to simul-
taneously cluster rows, columns, and slices of the tensor.
This approach is similar in style to the symmetrization of
bipartite graphs for co-clustering proposed by Dhillon [13].
Let U be an n-by-m-by-(cid:96) rectangular tensor. Then we
embed U into a square three-mode tensor T with n + m + (cid:96)
dimensions and where Ui,j,k = Ti,j+n,k+n+m. This is illus-
trated in Figure 1 (left). Then we symmetrize the tensor by
using all permutations of the indices Figure 1 (right). The
result is, when viewed as a 3-by-3-by-3 block tensor,

T =

0
0

0
0 U (3,2,1)

0

U (2,3,1)

0

0
0

0 U (1,3,2)
0
U (3,1,2) 0

0
0

0

U (2,1,3)

0

U (1,2,3) 0
0
0

0
0

The tensor U (1,3,2) is just a generalized transpose of U with
the dimensions permuted.

Finally, we note that “rectangular” tensors are not deter-
mined by sizes of modes only. We may consider a tensor T
to be rectangular if each mode represents a diﬀerent type of
object. The important idea is that if a tensor is declared to
be rectangular, then the result of clustering is a subset of
each mode.

4. THE ALGORITHM

In this section, we put together the pieces from Section 3
to build the GTSC framework and analyze its computational
complexity. In Section 4.3 we also derive a “popularity” met-
ric for clusters that will be useful for discussing clustering
results on real-world data in Section 6.
4.1 Recursive Two-way Cuts

Our GTSC algorithm works by recursively applying the
sweep cut procedure, similar to the recursive bisection pro-
cedures for clustering matrix-based data [8]. We continue

partitioning as long as the clusters are large enough or we
can get good enough splits. Speciﬁcally, if a cluster has di-
mension less than a speciﬁed size min-size, we do not con-
sider it for splitting. Otherwise, the algorithm recursively
splits the cluster if either (1) its dimension is above some
threshold max-size or (2) the biased conductance of a new
split is less than a target value φ∗. The overall algorithm is
summarized in Algorithm 1.

We also have a couple of pre-processing steps. First, we
have to symmetrize the data if the tensor is rectangular.
Second, we look for “empty” indices that do not participate
in the tensor structure. Formally, index i is empty if T ijk =
0 for all j and k.
4.2 Computational Complexity

We now provide an analysis of the running time of our
algorithm. Let N be the number of non-zeros in the tensor
T . First, note that the pre-processing (tensor symmetriza-
tion and ﬁnding empty nodes) takes O(N ) time. Now, we
examine the computational complexity of a single partition:
1. Generating the transition tensor P costs O(N ).
2. Each step of (10) to ﬁnd the stationary distribution is

O(N ).

3. Constructing P [x] costs O(N ).

(The matrix ˜P is not

formed explicitly).

4. Each iteration of the eigenvector computation takes time
linear in the number of non-zeros in P [x], which is O(N ).
5. Sorting the eigenvector takes O(n log n) computations,

which is negligible considering N is big compared to n.

6. The sweep cut takes time O(n + N ), which is O(N ).

In practice, we ﬁnd that only a few iterations are needed
to compute the stationary distribution, which is consistent
with past results [6, 17]. For these systems, we do not know
how many iterations are needed for the eigenvector compu-
tations. However, for the datasets we analyze in this paper,
the eigenvector computation is not prohibitive. Thus, we
can think of the time for each cut as roughly linear in the
number of non-zeros. Provided that the cuts are roughly
balanced, the depth of the recursion tree is O(log N ), and
the total time is O(N log N ). Again, in our experiments,
this is the case.

To backup our analysis, we tested the scalability of our
algorithm on a data tensor of English 3-grams (see Section 6
for a full description of the dataset). We varied the number
of non-zeros in the data tensor from ﬁve million down to a
few hundred by removing non-zeroes uniformly at random.
We used a laptop with 8GB of memory and 1.3GHz of CPU
to run Algorithm 1 on these tensors with max-size = 100,
min-size = 5, and φ∗ = 0.4. Figure 2 shows the results,
and we see that the scaling is roughly linear.
4.3 Ranking Clusters with Popularity Scores
For our analysis of real-world datasets in Section 6, it
will be useful to order clusters based on the volume of their
interactions with other clusters. To this end, we compute
a “popularity score” for each cluster. Let k be the number
of clusters and deﬁne a k × k interaction matrix M by the
total tensor weight between the clusters. Formally, for a
i∈Si,j∈Sj ,k Tijk. Finally, deﬁne
the popularity score of cluster i as the PageRank score of
node i in the graph induced by M . (To handle corner cases,
isolated nodes in this graph are given a popularity score of
0). For our experiments, we compute the PageRank score

three-mode tensor, Mij =(cid:80)

(cid:21)

.

the variance σ that controls the group weights is 2 or 4. For
each value of σ, we create 5 sample datasets.

g , ny

Rectangular Tensor Data. For rectangular data, we
distinguish between the indices for each mode of our mode-
3 tensor. We label the modes as x, y, and z. The groups are
g, 1 ≤ g ≤
generated by mode subgroups of size nx
20. Each subgroup size is sampled from the same normal
distribution as before with the same truncated minimum
g ×
g. Each group g is assigned a weight in

value. The dimension of the data tensor is then(cid:80)20
(cid:80)20

g ×(cid:80)20

g, and nz

g=1 nx

g=1 ny

g=1 nz

the same way as the synthetic square data.

j , and gz

Index triples (i, j, k) now correspond to three subgroups
i , gy
gx
k, and we generate within-group and across-
group triples similarly to the square case. The tw within-
group triples are chosen by ﬁrst uniformly selecting a group
and then uniformly selecting one index from each subgroup.
The ta across-group triples are chosen by (1) selecting a
mode (x, y, or z) uniformly at random, (2) selecting the
mode index proportional to the weights wg, and (3) selecting
the other two indices uniformly at random from the other
groups. The value of triple tw and ta is assigned in a similar
manner to the square tensor case.
synthetic datasets for each value of σ ∈ {2, 4}.
5.2 Clustering Methods and Evaluation

We again set tw = 10, 000 and ta = 3, 000 and generate 5

We compared the results of our GTSC framework to sev-
eral other state-of-the-art methods for clustering tensor data.
GTSC. This is the method presented in this paper (Algo-

rithm 1). We use the parameters max-size = 100, min-size =
5, and φ∗ = 0.35.3

TSC. This is the original tensor spectral clustering algo-
rithm [6]. We use the algorithm with recursive bisection to
ﬁnd 20 clusters.

PARAFAC. The PARAFAC method is a widely used
tensor decomposition procedure [18] that ﬁnds an approxi-
mation to the tensor by the sum of outer-products of vectors.
We compute a rank-20 decomposition using the Tensor Tool-
box [2, 10], and then assign nodes to clusters by taking the
index of the vector with highest value in the nodes index.
We use the default tolerance of 10−4 and a maximum of 1000
iterations.

Spectral Clustering. Our clustering framework (Algo-
rithm 1) works on mode-2 tensors, i.e., matrices.
In this
case, with α = 1, our algorithm reduces to a standard spec-
tral clustering method. We create a matrix M from the
tensor data T by summing along the third mode: Mij =
k=1 Ti,j,k. We then run Algorithm 1 with the same pa-

(cid:80)n

rameters as GTSC.

Evaluation metrics. We evaluate the clustering results
using the Adjusted Rand Index (ARI) [20], Normalized Mu-
tual Information (NMI) [22], and F1 score. The ground
truth labels correspond to the generated groups.
5.3 Experimental Results

Table 1 depicts the performances of the four algorithms.
In all cases, GTSC has the best performance. In the square
tensor case when σ = 4, the standard spectral method per-
forms as well as GTSC. However, when σ = 2, the score
drops for all four methods with the least impact on GTSC
3 We tested several values φ∗ ∈ [0.3, 0.4] and obtained
roughly the same results.

Figure 2: Time to compute a partition on the English 3-
grams as a function of the number of non-zeros in the ten-
sors. We see that the algorithm scales roughly linearly in
the number of non-zeros (the red line).

with α = 0.99 and uniform teleportation vector.

5. SYNTHETIC EXPERIMENTS

We generate tensors with planted cluster structures and
try to recover the planted clusters. We compare our GTSC
framework with a variety of other methods and ﬁnd that it
recovers the planted structure most often.
5.1 Synthetic Dataset Generation

data tensor will have mode 3 and dimension(cid:80)20

Square Tensor Data. We ﬁrst generate 20 groups of
nodes that will serve as our planted clusters. The number of
nodes in each group ng is drawn from a normal distribution
with mean 20 and variance 5, with truncated minimum value
so that each group has at least 4 nodes. Our square synthetic
g=1 ng. For
each group g we also assign a weight wg where the weight
depends on the group number. For group i, the weight is
2π)−1 exp(−(i−10.5)2/(2σ2)), where σ varies by experi-
(σ
ment. With these weights, groups 10 and 11 have the largest
weight and groups 1 and 20 have the smallest weight. As
described next, these weights will be used to provide a skew
in the distribution of the number of interactions for a given
dimension.

√

Non-zeros correspond to interactions between three in-
dices, and we call these assignments triples. We generate
tw triples whose indices are within a group and ta triples
whose indices span across more than one group. The tw
triples are chosen by ﬁrst uniformly selecting a group g and
then uniformly selecting three indices i, j, and k from group
g and ﬁnally assigning a weight of wg. Formally, the value
of the tensor data tensor T is

Ti,j,k = wg.

If the same three indices are chosen more than once in the
sampling procedure, we simply increment the value in the
tensor. For the ta triples that span multiple groups, the
sampling procedure ﬁrst selects an index i from group gi
proportional to the weights of the group. In other words, in-
dices in group g are chosen proportional to wg. Two indices
j and k are then selected uniformly at random from groups
gj and gk other than gi. Finally, the weight in the tensor is
assigned to be the average of the three group weights:

Ti,j,k = (wgi + wgj + wgk )/3.

For our experiments, tw = 10, 000 and ta = 1, 000, and

log # non-zeros in the tensor05101520log CPU seconds-8-6-4-20246Table 1: Results of various clustering methods on the synthetically generated data. Bold indicates the best mean performance
in terms of ARI, NMI, or F1 score, and ± entries are the standard deviations over 5 trials. Our method (GTSC) has the best
performance in all cases.

ARI

NMI

F1

ARI

NMI

F1

Square tensor with σ = 4

Rectangular tensor with σ = 4

Square tensor with σ = 2

Rectangular tensor with σ = 2

GTSC
TSC

PARAFAC

Spectral Clustering

GTSC
TSC

PARAFAC

Spectral Clustering

0.99 ± 0.01
0.42 ± 0.05
0.82 ± 0.05
0.99 ± 0.01

0.78 ± 0.13
0.41 ± 0.11
0.48 ± 0.08
0.43 ± 0.07

0.99 ± 0.00
0.60 ± 0.04
0.94 ± 0.02
0.99 ± 0.01

0.89 ± 0.06
0.60 ± 0.09
0.67 ± 0.04
0.66 ± 0.04

0.99 ± 0.01
0.45 ± 0.04
0.83 ± 0.04
0.99 ± 0.01

0.79 ± 0.12
0.44 ± 0.10
0.50 ± 0.07
0.47 ± 0.06

0.97 ± 0.06
0.38 ± 0.17
0.81 ± 0.04
0.91 ± 0.06

0.96 ± 0.06
0.28 ± 0.08
0.10 ± 0.04
0.38 ± 0.07

0.98 ± 0.03
0.53 ± 0.15
0.90 ± 0.02
0.94 ± 0.04

0.97 ± 0.04
0.44 ± 0.10
0.24 ± 0.05
0.52 ± 0.05

0.97 ± 0.05
0.41 ± 0.16
0.82 ± 0.04
0.91 ± 0.06

0.96 ± 0.06
0.32 ± 0.08
0.15 ± 0.04
0.41 ± 0.07

and TSC. Both PARAFAC and standard spectral clustering
lose nearly half of their performances. Similar conclusions
hold for the rectangular case.

The value of σ aﬀects the concentration of the weights and
how certain groups of nodes interact with others. When σ
is small, weights in the middle groups (e.g., 10 and 11) are
very high, and many across-group triangles have an index in
these groups. This skew reﬂects properties of the real-world
networks we examine in the next section. For example, with
n-grams in text data, stop words such as ‘a’, ‘the’, and ‘we’
are responsible for the majority of connections in the tensor.
As evidenced by the results in Table 1, our GTSC framework
handles this skew much better than other methods.

6. REAL-WORLD EXPERIMENTS

We now use our GTSC framework to cluster real-world
tensor datasets. Table 2 describes the relevant statistics of
these datasets.

Table 2: Statistics of tensor datasets.

Dataset

Airline-airport
English 3-grams
Chinese 3-grams
English 4-grams
Chinese 4-grams

Enron email

Size

539 × 2, 939 × 2, 939

square 30,966
square 18,387
square 23,734
square 14,728

185 × 184 × 184 × 34

# non-zeros

51, 982

1,020,009
966,138
1,034,307
1,002,660

61, 891

Airline-airport. This dataset consists of global air ﬂight
routes from 539 airlines and 2, 939 airports from OpenFlights4.
The 539 × 2, 939 × 2, 939 data tensor T summarizes ﬂights
connecting airports on several airlines. Formally, Tijk is 1 if
airline i ﬂies between airports j and k and 0 otherwise.

English 3,4-grams. An n-gram is a contiguous sequence
of n words from a sequence of text. We generated a ten-
sor dataset from the one million most frequent 3-grams and
4-grams from the Corpus of Contemporary American En-
glish.56 The 3-gram tensor entry Tijk is given by the number
4http://openﬂights.org/data.html#route
5http://www.ngrams.info/intro.asp
6The datasets contain ties in frequency, so the actual num-

Figure 3: Visualization of the Airline-airport data tensor.
The x and y axes index airports and the z axis indexes
airlines. A dot represents that an airline ﬂies between those
two airports. On the left, indices are sorted randomly. On
the right, indices are sorted by the co-clusters found by our
GTSC framework, which reveals structure in the tensor.

of times that words i, j, and k co-occur in an n-gram in the
corpus. The 4-gram tensor dataset is deﬁned analogously.
We symmetrize these tensors for our method.

Chinese 3,4-grams. We also constructed n-gram tensor
datasets from the Chinese language using the Google Books
n-grams dataset.7 We constructed 3- and 4-gram tensors in
the same way as the English dataset.

Enron email. This dataset is constructed from emails
between Enron employees with labeled topics [7]. The tensor
data represents the volume of communication between two
employees discussing a given topic during a particular week.
In total, there are 185 weeks of data, 184 employees, and 34
topics, leading to a 185 × 184 × 184 × 34 tensor where Tijkl
is the number of emails between employee j and k on topic
l during week i.
In all of our experiments, we use the stopping criterion
φ∗ = 0.4 for Algorithm 1. For the n-gram data and airline-
airport data, we use the parameters max-size = 100 and
min-size = 5; for the Enron email data, we use max-size =
50 and min-size = 10.
6.1 Airline-airport

Our GTSC framework groups the airports and airlines

ber of n-grams is only roughly 1 million—see Table 2.
7https://books.google.com/ngrams

Table 4: Fraction of words in the top two groups (in terms
of popularity, see Section 4.3) that are among the top 100
(200) most frequently used words in the English (Chinese)
written language corpora.

English

Chinese

Groups

3-gram 4-gram

3-gram 4-gram

1st Group
2nd Group

24/29
23/25

11/11
42/47

42/84
33/74

31/31
23/36

into 129 co-clusters. Figure 3 illustrates the connectivity of
the tensor with a random ordering of the indices (left) and
the ordering given by the popularity of co-clusters (right).
We can see that after the co-clustering, there is clear struc-
ture in the data tensor. Table 3 summarizes some of the
larger clusters found by our method, and we discuss these
clusters in more detail below.

The co-cluster with the highest popularity score is the
one marked as Worldwide Metropolises. This group is com-
posed of large international airports in cities such as Beijing
and New York City. The 250 airports in this group are re-
sponsible for 59% of the total routes, even though they only
account for 8.5% of the total number of airports. Figure 3
illustrates this result—the airports with the highest indices
are connected to almost every other airport. This cluster is
analogous to the “stop word” group we will see in the n-gram
experiments.

Groups with medium levels of popularity are organized
geographically. Our GTSC framework found one large clus-
ter for Europe, the United States, China/Taiwan, Ocea-
nia/SouthEast Asia, and Mexico/Americas.
Interestingly,
Canc´un International Airport is included with the United
States cluster, probably due to the large amounts of tourism.
In addition to the large groups, we ﬁnd many mini groups
which consist of 5–30 airports. These airports compose the
long, concentrated diagonal in Figure 3. The airports and
airlines in these co-clusters are also closely connected geo-
graphically but they are more isolated than the large, con-
tinental co-clusters.
6.2 English n-grams

Our GTSC framework clusters the 3-gram and 4-gram
tensors into 486 and 383 groups, respectively. We rank the
groups by decreasing order of the popularity score described
in Section 4.3. We ﬁnd several conclusions that hold for both
tensor datasets. Highly ranked groups are mostly composed
of stop words (i.e., common words) such as the, a, we, is, by
(we deﬁne stop words as those most connected with other
words). In fact, the top two groups consist nearly entirely of
stop words (see Table 4). In addition, 48% (3-gram) and 64%
(4-gram) of words in the ﬁrst group are prepositions (e.g.,
in, of, as, to) and link verbs (e.g., is, get, does). In the sec-
ond group, 64% (3-gram) and 57% (4-gram) of the words are
pronouns (e.g., we, you, them) and link verbs. This result
matches the structure of English language where link verbs
can connect both prepositions and pronouns whereas prepo-
sitions and pronouns are unlikely to appear in close vicinity.
Since prepositions are more common than pronouns, they
rank ﬁrst.

Groups ranked in the middle mostly consist of semanti-
cally related English words. For instance, a few of these

groups are {cheese, cream, sour, low-fat, frosting, nonfat,
fat-free}, {bag, plastic, garbage, grocery, trash, freezer}, {in-
fection, chronic, sexually, transmitted, diseases, hbv}. The
lowest ranked groups are comprised mostly of non-English
words that appear in English text. For example, one such
group is {je, ne, sais, quoi}, which is a French phrase. We
may consider these groups as outliers.

The clustering the 4-gram tensor contains some groups
that the 3-gram tensor fails to ﬁnd. For example, one cluster
is {german, chancellor, angela, merkel, gerhard, schroeder,
helmut, kohl}. Angela Merkel, Gerhard Schroeder, and Hel-
mut Kohl have all been German chancellors, but it requires
a 4-gram to make this connection strong. Likewise, some
clusters only appear from clustering the 3-gram tensor. One
such cluster is {church, bishop, catholic, priest, greek, or-
thodox, methodist, roman, priests, episcopal, churches, bish-
ops}.
In 3-grams, we may see phrases such as “catholic
church bishop”, but 4-grams containing these words likely
also contain stop words, e.g., “bishop of the church”. How-
ever, since stop words are in a highly ranked cluster, the
connection is destroyed once the top clusters are formed.
6.3 Chinese n-grams

We ﬁnd that many of the conclusions from the English
n-gram datasets also hold for the Chinese n-gram datasets.
For example, many of the words in the top two groups are
stop words (see Table 4). Groups scoring in the middle con-
sist of semantically similar words, and low-scoring groups are
foreign language phrases that appear in Chinese literature.
However, there are also several diﬀerences from the En-
glish n-gram data. First, we ﬁnd that groups in Chinese
are usually larger than those in English. The average sizes
of small-to-medium groups (3–100 words) is 6.8 for English
and 10 for Chinese. We suspect this pattern is due to the
fact that Chinese words are more semantically dense than
English words. For example, it is common that Chinese
words have multiple meanings and can co-occur with a large
number of words.

We note that there are several words form the top two
groups that are not typically considered as stop words. For
example, society, economy, develop, -ism, nation, govern-
ment are among the top 100 most common words. (These
are translations of the Chinese words). This is a consequence
of the dataset coming from scanned Chinese-language books
and is a known issue with the Google Books corpus [27]. In
this case, it is a feature as we are illustrating the eﬃcacy
of our tensor clustering framework rather than making any
linguistic claims.
6.4 Enron Email Tensor

In total, the algorithm ﬁnds 23 co-clusters of topics, peo-
ple, and time. The most popular group corresponds to three
topics, 19 people, and 0 time intervals. Similar to the n-
grams and airport-airline data, this cluster corresponds to
high-volume entities, in this case common topics and people
who send a lot of emails. The three topics are “Daily busi-
ness”, “too few words”, and “no matching topic”, which ac-
count for roughly 90% of the total email volume. (The latter
two topics are capturing outliers and emails that do not fall
under an obvious category). The 19 employees include 11
managers: the CEO, (vice) preseidents, and directors. These
employees are involved in 42% of the total emails. There is
no time interval in this co-cluster because these high-volume

Table 3: High-level descriptions of the larger co-clusters found by our GTSC framework on the Airline-airport dataset. The
algorithm ﬁnds one co-cluster of international hubs and large commercial airlines and several geographically coherent groups.

Name

# Airports # Airlines Airports description

Airlines description

Worldwide metropolises

Europe
United States
China/Taiwan

Oceania/S.E. Asia

Mexico/Americas

250

184
137
170

302

399

77

32
9
33

52

68

Large hubs, e.g., Beijing Capital
and JFK in New York
177 in Europe, rest in Morocco
136 in U.S., Canc´un International
136 in China or Taiwan,

231 in Oceania or S.E. Asia,

396 in Mexico or Central and
South America

Large commercial airlines, e.g.,
United, Air Canada, Air China
29 European airlines
29 all U.S. airlines
21 in China/Taiwan 14 in S.
Korea and Thailand
41 in East Asia or Canada 66
in China, Japan, or Canada
43 in Mexico or Central and
South America

these techniques for the higher-order interactions encoun-
tered in tensor data. We note that recent work has looked
at co-clustering several entity types simultaneously [16, 3].
However, these methods are still based on ﬁrst-order infor-
mation (graph structure).

Our GTSC framework is immediately applicable as a clus-
tering method for multiplex networks [25]. Indeed, we found
interesting structure in the airline-airport dataset, which is
an example of a multiplex network. Related work in this area
includes multi-view clustering [32], ensemble methods [15],
and multi-network clustering [26]. However, these methods
are specialized for multiplex networks, whereas our method
provides a general tensor clustering framework.

8. CONCLUSION

Tensors are increasingly common in modern applications
and clustering tensor data is fundamental for discovering
patterns and data analysis. However, tensor clustering is a
diﬃcult task for two reasons: higher-order structure in ten-
sors is diﬃcult to model and obvious extensions of models
to higher-order data are computationally challenging.
In
this paper we proposed the General Tensor Spectral Co-
clustering (GTSC) method. Our method addresses these
issues by modeling higher-order data with a new stochas-
tic process, the super-spacey random walk, which is a vari-
ant of a higher-order Markov chain. Our iterative solver
and sweep cut procedure for biased conductance can achieve
near-linear complexity in the number of non-zeros in tensors.
In synthetic experiments our GTSC out-performs state-of-
the-art spectral methods and tensor decomposition methods
and can eﬃciently handle skew in the distribution of the in-
dices of the non-zeros in the tensor. Furthermore, our GTSC
framework can ﬁnd clear cluster structures in various tensor
datasets, including English and Chinese n-gram text, an air-
line and airport multiplex network, and Enron email data.
In terms of future work, we’d like to create tensors that
bridge information from multiple modes. For instance, the
clusters from the 3-gram data were diﬀerent from the 4-gram
data and it would be useful to have a holistic tensor to jointly
parition both 3- and 4-gram information. This is important
because some of the clusters in the n-gram data correspond
to automatically extracted knowledge, such as the cluster
with the names of various German chancellors. This aspect
of our output also merits further investigation as it would
require overlapping clusters to be useful to knowledge ex-
traction eﬀorts.

Figure 4: Enron email volume on three labeled topics. Our
GTSC framework ﬁnds a co-cluster consisting of these three
topics at the time points labeled in red, which seems to
correlate with various events involving the CEO.

topics and employees are balanced throughout time.

We also found several interesting co-clusters. One con-
sists of the topics “California bankruptcy”, “California legis-
lature”, and “India (general)”, during three weeks in Decem-
ber 2000 and January 2001, and 13 employees. These time
points correspond to various events involving CEO Skilling
(Figure 4). Each of the 13 employees in the co-cluster sent
at least one email from at least one of the topics. Another
co-cluster consists of the topics “General newsfeed”, “Down-
fall newsfeed”, and “Federal Energy Regulatory Commis-
sion/Department of Energy” and several weeks from March
2001 and December 2001. These time intervals coincide with
investor James Chanos ﬁnding problems with Enron in early
2001 and the serious ﬁnancial troubles encountered by the
company in late 2001.

7. RELATED WORK

There are a variety of methods for tensor decomposition
that could be used to cluster a tensor [9, 19]. For instance,
Huang et al. use a higher-order SVD as a basis for clus-
tering [19] and Anandkumar et al. [1] use a latent-variable
model. We studied standard algorithms for the PARAFAC
decomposition in this light and found that our tensor clus-
tering method outperformed this general strategy.

Our work provides a technique for “co-clustering”, i.e, ﬁnd-
ing clusters of diﬀerent entities through their interactions.
Prior work in this area has focused on ﬁnding groups of in-
dices in the rows and columns of a matrix of interaction
data [13, 14]. Our work provides the ﬁrst generalization of

Dec.00June.01Dec.01050100150CEO SkillingappointedCEO SkillingStartedCalif. bankruptcyCalif. legislatureIndia (general)Number of emailsAcknowledgements. This work was supported by NSF IIS-
1422918 and DARPA SIMPLEX. ARB is supported by a Stan-
ford Graduate Fellowship. We are grateful to our colleagues who
helped revise early drafts.

9. REFERENCES
[1] A. Anandkumar, R. Ge, D. Hsu, S. M. Kakade, and

M. Telgarsky. Tensor decompositions for learning latent
variable models. J. Mach. Learn. Res., 15(1):2773–2832,
2014.

[2] B. W. Bader, T. G. Kolda, et al. Matlab tensor toolbox

version 2.6. Available online, February 2015.

[3] B.-K. Bao, W. Min, K. Lu, and C. Xu. Social event

detection with robust high-order co-clustering. In ICMR,
pages 135–142, 2013.

[4] M. Bena¨ım. Vertex-reinforced random walks and a

conjecture of pemantle. Ann. Prob., 25(1):361–392, 1997.

pages 526–531, 1989.

[25] M. E. Newman. The structure and function of complex

networks. SIAM review, 45(2):167–256, 2003.

[26] J. Ni, H. Tong, W. Fan, and X. Zhang. Flexible and robust

multi-network clustering. In KDD, pages 835–844, 2015.

[27] E. A. Pechenick, C. M. Danforth, and P. S. Dodds.

Characterizing the Google Books corpus: strong limits to
inferences of socio-cultural and linguistic evolution. PloS
one, 10(10):e0137041, 2015.

[28] R. Pemantle. A survey of random processes with
reinforcement. Probability Surveys, 4:1–79, 2007.

[29] S. Ragnarsson and C. F. Van Loan. Block tensors and

symmetric embeddings. Linear Algebra Appl.,
438(2):853–874, 2013.

[30] S. E. Schaeﬀer. Graph clustering. Computer Science

Review, 1(1):27–64, 2007.

[31] D. Wagner and F. Wagner. Between min cut and graph

[5] A. Benson, D. F. Gleich, and L.-H. Lim. The spacey

bisection. In MFCS, pages 744–750, 1993.

random walk: a stochastic process for higher-order data.
arXiv, cs.NA:1602.02102, 2016.

[6] A. R. Benson, D. F. Gleich, and J. Leskovec. Tensor

spectral clustering for partitioning higher-order network
structures. In SDM, pages 118–126, 2015.

[7] M. W. Berry, M. Browne, and B. Signer. Topic annotated

enron email data set. LDC, 2001.

[8] D. Boley. Principal direction divisive partitioning. Data

Mining and Knowledge Discovery, 2(4):325–344, 1998.
[9] X. Cao, X. Wei, Y. Han, Y. Yang, and D. Lin. Robust

tensor clustering with non-greedy maximization. In IJCAI,
pages 1254–1259, 2013.

[10] E. C. Chi and T. G. Kolda. On tensors, sparsity, and
nonnegative factorizations. SIAM Journal on Matrix
Analysis and Applications, 33(4):1272–1299, 2012.

[11] F. Chung. Four proofs for the Cheeger inequality and graph

partition algorithms. In ICCM, pages 751–772, 2007.
[12] F. R. L. Chung. Spectral Graph Theory. AMS, 1992.
[13] I. S. Dhillon. Co-clustering documents and words using

bipartite spectral graph partitioning. In KDD, pages
269–274, 2001.

[14] I. S. Dhillon, S. Mallela, and D. S. Modha.

Information-theoretic co-clustering. In KDD, pages 89–98,
2003.

[15] X. Z. Fern and C. E. Brodley. Solving cluster ensemble

problems by bipartite graph partitioning. In ICML,
page 36, 2004.

[16] B. Gao, T.-Y. Liu, X. Zheng, Q.-S. Cheng, and W.-Y. Ma.

Consistent bipartite graph co-partitioning for
star-structured high-order heterogeneous data co-clustering.
In KDD, pages 41–50, 2005.

[17] D. F. Gleich, L.-H. Lim, and Y. Yu. Multilinear PageRank.

SIAM J. Matrix Ann. Appl., 36(4):1507–1541, 2015.

[18] R. A. Harshman. Foundations of the PARAFAC procedure:

Models and conditions for an “explanatory” multimodal
factor analysis. UCLA Working Papers in Phonetics,
16:1–84, 1970.

[19] H. Huang, C. Ding, D. Luo, and T. Li. Simultaneous tensor

subspace selection and clustering: the equivalence of high
order SVD and k-means clustering. In KDD, pages
327–335, 2008.

[20] L. Hubert and P. Arabie. Comparing partitions. Journal of

classiﬁcation, 2(1):193–218, 1985.

[21] M. Kivel¨a, A. Arenas, M. Barthelemy, J. P. Gleeson,

Y. Moreno, and M. A. Porter. Multilayer networks. Journal
of Complex Networks, 2(3):203–271, 2014.

[22] A. Lancichinetti, S. Fortunato, and J. Kert´esz. Detecting
the overlapping and hierarchical community structure in
complex networks. New J. Phys., 11(3):033015, 2009.
[23] M. Meil˘a and J. Shi. A random walks view of spectral

segmentation. In AISTATS, 2001.

[24] M. Mihail. Conductance and convergence of markov

chains—a combinatorial treatment of expanders. In FOCS,

[32] D. Zhou and C. J. Burges. Spectral clustering and

transductive learning with multiple views. In ICML, pages
1159–1166, 2007.

APPENDIX
A. STATIONARY DISTRIBUTION

We provide a heuristic proof here and a reference for how to
make it formal. Suppose the process has run for a very long time
and that xt is the current empirical distribution. From equa-
tion (7), we have

Pr(Xt+1 = i|Xt = j) = (1−α)vi+α(cid:0)(cid:88)

Pi,j,kxt(k)+

(cid:88)

xt(k)xt(i)(cid:1).

(j,k)∈F

(j,k)(cid:54)∈F

Because the process has run for a very long time, the empirical
distribution xt is essentially ﬁxed. Thus, the stochastic proe-
cess can be approximated by a Markov chain with the transition
matrix:

αP [xt] + αxt(eT − eT P [xt]) + (1 − α)veT .

At stationarity of the super-spacey random walk, the stationary
distribution of this Markov chain must be equal to xt. A more
formal proof is to use the results from [4] to show that (9) is
the necessary condition of the stationary distribution in a vertex
reinforced random walk.

B. PROOF OF THEOREM 3.1
Let R denote the mode-1 unfolding of P :

R = [P (:, :, 1) | P (:, :, 2) | · · · | P (:, :, n)].

Note that R(x ⊗ x) = P x2 where ⊗ is the Kronecker product.
Assume x and y are two solutions of (9). Let rx = (cid:107)R(x ⊗ x)(cid:107)1
and ry = (cid:107)R(y ⊗ y)(cid:107)1. Then
(cid:107)x − y(cid:107)1 ≤ α(cid:107)R(x ⊗ x − y ⊗ y)(cid:107)1 + α(cid:107)(1 − rx)x + (1 − ry)y(cid:107)1.

By Lemma 4.5 of [17], the ﬁrst term
α(cid:107)R(x ⊗ x − y ⊗ y)(cid:107)1 ≤ α(cid:107)R(cid:107)1(cid:107)x ⊗ x − y ⊗ y(cid:107)1 ≤ 2α(cid:107)x − y(cid:107)1.
The second term satiﬁes

α(cid:107)(1 − rx)(x − y)(cid:107)1 + α|ry − rx|

≤ α(cid:107)x − y(cid:107)1 + α(cid:107)R(x ⊗ x − y ⊗ y)(cid:107)1 ≤ 3α(cid:107)x − y(cid:107)1.

Combining the above two facts, we know when α < 1/5 the so-
lution is unique. For an m-mode tensor, this idea generalizes to
α < 1/(2m − 1).
analysis shows that (cid:107)xk+1 − x∗(cid:107)1 ≤ 5α(cid:107)xk − x∗(cid:107)1, and so the

For the convergence of the ﬁxed point algorithm (10), the same

iteration converges when the solution is unique.

