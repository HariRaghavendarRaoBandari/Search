6
1
0
2

 
r
a

 

M
1
1

 
 
]
P
A

.
t
a
t
s
[
 
 

1
v
3
9
5
3
0

.

3
0
6
1
:
v
i
X
r
a

Fast Detection of Block Boundaries in Block Wise
Constant Matrices: An Application to HiC data

Vincent Brault∗, Julien Chiquet and C´eline L´evy-Leduc

March 14, 2016

UMR MIA-Paris, AgroParisTech, INRA, Universit´e Paris-Saclay

Abstract

We propose a novel approach for estimating the location of block
boundaries (change-points) in a random matrix consisting of a block
wise constant matrix observed in white noise. Our method consists in
rephrasing this task as a variable selection issue. We use a penalized
least-squares criterion with an (cid:96)1-type penalty for dealing with this is-
sue. We ﬁrst provide some theoretical results ensuring the consistency
of our change-point estimators. Then, we explain how to implement
our method in a very eﬃcient way. Finally, we provide some empir-
ical evidence to support our claims and apply our approach to HiC
data which are used in molecular biology for better understanding the
inﬂuence of the chromosomal conformation on the cells functioning.

1

Introduction

Detecting automatically the block boundaries in large block wise constant
matrices corrupted with noise is a very important issue which may have
several applications. One of the main situations in which this problem oc-
curs is in the study of HiC data. It corresponds to one of the most recent
chromosome conformation capture technologies that have been developed
to better understand the inﬂuence of the chromosomal conformation on the
cells functioning. This technology is based on a deep sequencing approach
∗The authors would like to thank the French National Research Agency ANR, which
partly supported this research through the ABS4NGS project (ANR-11-BINF-0001-06).

1

and provides read pairs corresponding to pairs of genomic loci that physi-
cally interacts in the nucleus, see [12] for more details. The raw measure-
ments provided by HiC data are often summarized as a square matrix where
each entry at row i and column j stands for the total number of read pairs
matching in position i and position j, respectively, see [4] for further details.
Positions refer here to a sequence of non-overlapping windows of equal sizes
covering the genome.

Blocks of diﬀerent intensities arise among this matrix, revealing interact-
ing genomic regions among which some have already been conﬁrmed to host
co-regulated genes. The purpose of the statistical analysis is then to pro-
vide a fully automated and eﬃcient strategy to determine a decomposition
of the matrix in non-overlapping blocks, which gives, as a by-product, a list
of non-overlapping interacting chromosomic regions. In the following, our
goal will thus be to design an eﬃcient and fully automated method to ﬁnd
the block boundaries, also called change-points, of non-overlapping blocks
in very large matrices which can be modeled as block wise constant matrices
corrupted with white noise.

An abundant literature is dedicated to the change-point detection is-
sue for one-dimensional data both from a theoretical and practical point of
view. From a practical point of view, the standard approach for estimat-
ing the change-point locations is based on least- square ﬁtting, performed
via a dynamic programming algorithm (DP). Indeed, for a given number
of change-points K, the dynamic programming algorithm, proposed by [2]
and [6], takes advantage of the intrinsic additive nature of the least-square
objective to recursively compute the optimal change-points locations with a
complexity of O(Kn2) in time, see [10]. This complexity has recently been
improved by [14] in some speciﬁc cases.

However, in general one-dimensional situations, the computational bur-
den of these methods is prohibitive to handle very large data sets. In this
situation, [8] proposed to rephrase the change-point estimation issue as a
variable selection problem. This approach has also been extended by [20] to
ﬁnd shared change-points between several signals. In the two-dimensional
case, namely when matrices have to be processed, no method has been pro-
posed, to the best of our knowledge, for providing the block boundaries of
non overlapping blocks of very large n × n matrices. Typically, we aim at
being able to handle 5000 × 5000 matrices, which corresponds to matrices
having 2.5×107 entries. The only statistical approach proposed for retrieving
such non-overlapping block boundaries in this two-dimensional framework
is the one devised by [11] but it is limited to the case where the block wise
matrix is assumed to be block wise constant on the diagonal and constant

2

outside the diagonal blocks.

The diﬃculties that we have to face with in the two-dimensional frame-
work are the following. Firstly, it has to be noticed that the classical dy-
namic programming algorithm cannot be applied in such a framework since
the Markov property does not hold anymore. Secondly, the group-lars ap-
proach of [20] cannot be used in this framework since it would only provide
change-points in columns and not in rows. Thirdly, although very eﬃcient
for image denoising, neither the generalized Lasso approach devised by [19]
nor the fused Lasso signal approximator of [9], which are implemented in the
R packages genlasso and flsa, respectively, give access to the boundaries
of non-overlapping blocks of a noisy block wise constant matrix. This fact
is illustrated in Figure 2. The ﬁrst column of this ﬁgure contains the block
wise constant matrix of Figure 1 corrupted with additional noise in high sig-
nal to noise ratio contexts. The denoising of these noisy matrices obtained
by the packages genlasso and flsa is displayed in the second and third
columns of Figure 1, respectively. Note that, for obtaining these results, we
used the default parameters of these packages and for the parameter λ we
used the one giving the denoised matrix being the closest to the original one
in terms of recovered blocks.

Figure 1: Block wise constant matrix without noise.

In this paper, our goal is thus to design a statistical method for es-
timating the location of the boundaries of non-overlapping blocks from a
block wise constant matrix corrupted with white noise. To the best of our
knowledge, there is indeed no statistical procedure for answering this spe-
ciﬁc question in the literature that is both computationally and statistically
eﬃcient.

The paper is organized as follows. In Section 2, we ﬁrst describe how to
rephrase the problem of two-dimensional change-point estimation as a high
dimensional sparse linear model and give some theoretical results which

3

1
=
σ

2
=
σ

Original data

genlasso

flsa

Figure 2: Left: Matrix of Figure 1 corrupted with Gaussian white noise of
variance σ. Middle: Denoising obtained with genlasso. Right: Denoising
obtained with flsa.

prove the consistency of our change-point estimators. In Section 3, we de-
scribe how to eﬃciently implement our method. Then, we provide in Section
4 experimental evidence of the relevance of our approach on synthetic data.
We conclude in Section 6 by a thorough analysis of a HiC dataset.

2 Statistical framework

2.1 Statistical modeling

In this section, we explain how the two-dimensional retrospective change-
point estimation issue can be seen as a variable selection problem. Our
goal is to estimate t(cid:63)
) from the
random matrix Y = (Yi,j)1≤i,j≤n deﬁned by

2,1, . . . , t(cid:63)

1,1, . . . , t(cid:63)

1 = (t(cid:63)

2 = (t(cid:63)

2,K(cid:63)
2

) and t(cid:63)

1,K(cid:63)
1

where U = (Ui,j) is a blockwise constant matrix such that

Y = U + E,

(1)

Ui,j = µ(cid:63)
k,(cid:96)

if t(cid:63)

1,k−1 ≤ i ≤ t(cid:63)

1,k − 1 and t(cid:63)

2,(cid:96)−1 ≤ j ≤ t(cid:63)

2,(cid:96) − 1,

4

1,0 = t(cid:63)

2,0 = 1 and t(cid:63)

with the convention t(cid:63)
2 +1 = n + 1. An
example of such a matrix U is displayed in Figure 3. The entries Ei,j of the
matrix E = (Ei,j)1≤i,j≤n are iid zero-mean random variables. With such a
deﬁnition the Yi,j are assumed to be independent random variables with a
blockwise constant mean.

1 +1 = t(cid:63)

1,K(cid:63)

2,K(cid:63)

t(cid:63)
2,0

t(cid:63)
2,1

t(cid:63)
2,2

t(cid:63)
2,3

t(cid:63)
2,K(cid:63)
2

t(cid:63)
2,K(cid:63)

t(cid:63)
2,0

2 +1
t(cid:63)
1,0 B1,1
0
0
0
0
0

t(cid:63)
1,1

B7,1

0
0
0
0
0
0
0
0
0
0
B11,1
0
0
B13,1
0
0
0
0

0
0
0

0
0
0

0

t(cid:63)
1,2

t(cid:63)
1,K(cid:63)
1

t(cid:63)
1,K(cid:63)

1 +1

t(cid:63)
2,1

t(cid:63)
2,2

t(cid:63)
2,3

t(cid:63)
2,K(cid:63)
2

t(cid:63)
2,K(cid:63)

2 +1

0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0

B1,5

B7,5

0
0
0
0
0

0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
B11,5
0
0
0
0
B13,5
0
0
0
0
0
0
0
0

0
0
0

0
0
0

0

0
0
0
0
0

0
0
0
0
0

0
0
0
0
0

B1,13
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
B7,13
0
0
0
0
0
0
0
0
0
0
0
0
B11,8 B11,10 B11,13
0
0
0
0
0
0
B13,8 B13,10 B13,13
0
0
0
0
0
0
0
0
0
0
0
0

B1,8 B1,10
0
0
0
0
0
0
B7,8 B7,10
0
0
0
0
0
0
0
0
0
0

0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0

0
0
0

0
0
0

0
0
0

0
0
0

0
0
0

0
0
0

0

0

0

0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0

0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0

t(cid:63)
1,0

t(cid:63)
1,1

t(cid:63)
1,2

t(cid:63)
1,K(cid:63)
1

t(cid:63)
1,K(cid:63)

1 +1

µ(cid:63)

1,1

µ(cid:63)
1,2 µ(cid:63)

1,3 µ(cid:63)
1,4

µ(cid:63)

1,5

µ(cid:63)

2,1

µ(cid:63)
2,2 µ(cid:63)

2,3 µ(cid:63)
2,4

µ(cid:63)

2,5

µ(cid:63)

3,1

3,2 µ(cid:63)
µ(cid:63)

3,3 µ(cid:63)
3,4

µ(cid:63)

3,5

µ(cid:63)

4,1

µ(cid:63)
4,2 µ(cid:63)

4,3 µ(cid:63)
4,4

µ(cid:63)

4,5

Figure 3:
K(cid:63)

Left: An example of a matrix U with n = 16, K(cid:63)

1 = 3 and

Y = TBT(cid:62) + E,

2 = 4. Right: The matrix B associated to this matrix U.
Let T be a n× n lower triangular matrix with nonzero elements equal to
one and B a sparse matrix containing null entries except for the Bi,j such
}. Then, (1) can be rewritten
that (i, j) ∈ {t(cid:63)
as follows:

2,0, . . . , t(cid:63)

1,0, . . . , t(cid:63)

} × {t(cid:63)

1,K(cid:63)
1

2,K(cid:63)
2

(2)
where T(cid:62) denotes the transpose of the matrix T. For an example of a
matrix B, see Figure 3. Let Vec(X) denotes the vectorization of the matrix
X formed by stacking the columns of X into a single column vector then
Vec(Y) = Vec(TBT(cid:62))+ Vec(E). Hence, by using that Vec(AXC) = (C(cid:62)⊗
A)Vec(X), where ⊗ denotes the Kronecker product, (2) can be rewritten
as:

(3)
where Y = Vec(Y), X = T ⊗ T, B = Vec(B) and E = Vec(E). Thanks to
these transformations, Model (1) has thus been rephrased as a sparse high
dimensional linear model where Y and E are n2 × 1 column vectors, X is

Y = XB + E,

5

(cid:9) ,

2 + λn(cid:107)B(cid:107)1

(cid:98)B(λn) = Argmin

(cid:8)(cid:107)Y − XB(cid:107)2

a n2 × n2 matrix and B is n2 × 1 sparse column vectors. Multiple change-
point estimation Problem (1) can thus be addressed as a variable selection
problem:

(4)

B∈Rn2

and of (t(cid:63)

1,k)1≤k≤K(cid:63)

1

2,k)1≤k≤K(cid:63)

where (cid:107)u(cid:107)2

provides estimators of (t(cid:63)

2 and (cid:107)u(cid:107)1 are deﬁned for a vector u in RN by (cid:107)u(cid:107)2

2 =(cid:80)N
and (cid:107)u(cid:107)1 =(cid:80)N
i=1 u2
i
i=1 |ui|. Criterion (4) is related to the popular Least Abso-
Thanks to the sparsity enforcing property of the (cid:96)1-norm, the estimator (cid:98)B
lute Shrinkage and Selection Operator (LASSO) in least-square regression.
those of B. Hence, retrieving the positions of the non zero elements of (cid:98)B thus
of B is expected to be sparse and to have non-zero elements matching with
us deﬁne by (cid:98)A(λn) the set of active variables:
For each j in (cid:98)A(λn), consider the Euclidean division of (j − 1) by n, namely
(cid:98)t1 = ((cid:98)t1,k)1≤k≤|(cid:98)A1(λn)| ∈ {rj + 1 : j ∈ (cid:98)A(λn)},
where(cid:98)t1,1 <(cid:98)t1,2 < ··· <(cid:98)t1,|(cid:98)A1(λn)|,

(cid:98)t2 = ((cid:98)t2,(cid:96))1≤(cid:96)≤|(cid:98)A2(λn)| ∈ {qj + 1 : j ∈ (cid:98)A(λn)}

(cid:110)
(cid:111)
j ∈ {1, . . . , n2} : (cid:98)Bj(λn) (cid:54)= 0

In (5), |(cid:98)A1(λn)| and |(cid:98)A2(λn)| correspond to the number of distinct elements
in {rj : j ∈ (cid:98)A(λn)} and {qj : j ∈ (cid:98)A(λn)}, respectively.

(cid:98)t2,1 <(cid:98)t2,2 < ··· <(cid:98)t2,|(cid:98)A2(λn)|.

(j − 1) = nqj + rj then

(cid:98)A(λn) =

. More precisely, let

(5)

2

.

As far as we know, neither thorough practical implementation nor the-
oretical grounding have been given so far to support such an approach for
change-point estimation in the two-dimensional case. In the following sec-
tion, we give theoretical results supporting the use of such an approach.

2.2 Theoretical results

In order to establish the consistency of the estimators(cid:98)t1 and(cid:98)t2 deﬁned in

(5), we shall use assumptions (A1–A4). These assumptions involve the two
following quantities
1,k| ∧ min
1,k+1 − t(cid:63)
|t(cid:63)
0≤k≤K(cid:63)
|µ(cid:63)
k+1,(cid:96) − µ(cid:63)

2,k+1 − t(cid:63)
|t(cid:63)
k,(cid:96)| ∧

|µ(cid:63)
k,(cid:96)+1 − µ(cid:63)

I (cid:63)
min = min

J (cid:63)
min =

0≤k≤K(cid:63)

2,k|,

k,(cid:96)|,

2

1
min
1 ,1≤(cid:96)≤K(cid:63)

2 +1

1≤k≤K(cid:63)

1≤k≤K(cid:63)

min
1 +1,1≤(cid:96)≤K(cid:63)

2

6

which corresponds to the smallest length between two consecutive change-
points and to the smallest jump size between two consecutive blocks, respec-
tively.

(A1) The random variables (Ei,j)1≤i,j≤n are iid zero mean random variables
such that there exists a positive constant β such that for all ν in R,
E[exp(νE1,1)] ≤ exp(βν2).

(A2) The sequence (λn) appearing in (4) is such that (nδnJ (cid:63)

min)−1λn → 0,

as n tends to inﬁnity.

(A3) The sequence (δn) is a non increasing and positive sequence tending

2/ log(n) → ∞, as n tends to inﬁnity.

to zero such that nδnJ (cid:63)
min ≥ nδn.

min

(A4) I (cid:63)

Proposition 1. Let (Yi,j)1≤i,j≤n be deﬁned by (1) and (cid:98)t1,k, (cid:98)t2,k be deﬁned
by (5). Assume that |(cid:98)A1(λn)| = K(cid:63)
(cid:27)
(cid:18)(cid:26)
(cid:12)(cid:12) ≤ nδn

1 and that |(cid:98)A2(λn)| = K(cid:63)
(cid:26)
(cid:12)(cid:12) ≤ nδn
(cid:12)(cid:12)(cid:98)t2,k − t(cid:63)

(cid:12)(cid:12)(cid:98)t1,k − t(cid:63)

tending to one, then,

2 , with probabilty

(cid:27)(cid:19)

∩

P

1,k

2,k

max
1≤k≤K(cid:63)

2

max
1≤k≤K(cid:63)

1

→ 1,
as n → ∞.

(6)

The proof of Proposition 1 is based on the two following lemmas. The
ﬁrst one comes from the Karush-Kuhn-Tucker conditions of the optimization
problem stated in (4). The second one allows us to control the supremum
of the empirical mean of the noise.

Lemma 2. Let (Yi,j)1≤i,j≤n be deﬁned by (1). Then, (cid:98)U = X(cid:98)B, where X
and (cid:98)B are deﬁned in (3) and (4) respectively, is such that
(cid:98)Uk,(cid:96) =
sign((cid:98)Bj), if (cid:98)Bj (cid:54)= 0,
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≤ λn
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
(cid:98)Uk,(cid:96)
, if (cid:98)Bj = 0,

Yk,(cid:96) − n(cid:88)
Yk,(cid:96) − n(cid:88)

n(cid:88)
n(cid:88)

n(cid:88)
n(cid:88)

n(cid:88)
n(cid:88)

λn
2

k=rj +1

k=rj +1

k=rj +1

k=rj +1

(cid:96)=qj +1

(cid:96)=qj +1

(cid:96)=qj +1

(cid:96)=qj +1

(7)

(8)

2

where qj and rj are the quotient and the remainder of the Euclidean division
of (j − 1) by n, respectively, that is (j − 1) = nqj + rj. In (7), sign denotes
the function which is deﬁned by sign(x) = 1, if x > 0, −1, if x < 0 and

7

0 if x = 0. Moreover, the matrix (cid:98)U, which is such that (cid:98)U = Vec((cid:98)U), is
blockwise constant and satisﬁes (cid:98)Ui,j = (cid:98)µk,(cid:96), if (cid:98)t1,k−1 ≤ i ≤ (cid:98)t1,k − 1 and
(cid:98)t2,(cid:96)−1 ≤ j ≤(cid:98)t2,(cid:96) − 1, k ∈ {1, . . . ,|(cid:98)A1(λn)|}, (cid:96) ∈ {1, . . . ,|(cid:98)A2(λn)|}, where the
(cid:98)t1,k,(cid:98)t2,k, (cid:98)A1(λn) and (cid:98)A2(λn) are deﬁned in (5).
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≥ xn

Lemma 3. Let (Ei,j)1≤i,j≤n be random variables satisfying (A1). Let also
n/ log(n) → ∞, then
(vn) and (xn) be two positive sequences such that vnx2

 → 0, as n → ∞,

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(sn − rn)−1

 max

1≤rn<sn≤n
|rn−sn|≥vn

P

sn−1(cid:88)

j=rn

En,j

the result remaining valid if En,j is replaced by Ej,n.

The proofs of Proposition 1, Lemmas 2 and 3 are given in Section A.

Remark. If Y is a non square matrix having n1 rows and n2 columns,
with n1 (cid:54)= n2, the result of Proposition 1 remains valid if in Assumption
2/ log(n2) → ∞ and
(A3) δn is replaced by δn1,n2 satisfying n1δn1,n2J (cid:63)
n2δn1,n2J (cid:63)

2/ log(n1) → ∞, as n1 and n2 tend to inﬁnity.

min

min

3

Implementation

In order to identify a series of change-points we look for the whole path of
solutions in (4), i.e., { ˆB(λ), λmin < λ < λmax} such that | ˆA(λmax)| = 0 and
| ˆA(λmin)| = s with s a predeﬁned maximal number of activated variables.
To this end it is natural to adopt the famous homotopy/LARS strategy of
[16, 5]. Such an algorithm identiﬁes in Problem (4) the successive values
of λ that correspond to the activation of a new variable, or the deletion of
one that became irrelevant. However, the existing implementations do not
apply here since the size of the design matrix X – even for reasonable n
– is challenging both in terms of memory requirement and computational
burden. To overcome these limitations, we need to take advantage of the
particular structure of the problem.
In the following lemmas (which are
proved in Section A), we show that the most involving computations in the
LARS can be made extremely eﬃciently thanks to the particular structure
of X .
Lemma 4. For any vector v ∈ Rn2, computing X v and X (cid:62)v requires at
worse 2n2 operations.

8

(cid:19)

Lemma 5. Let A = {a1, . . . , aK} and for each j in A let us consider the
Euclidean division of j − 1 by n given by j − 1 = nqj + rj, then

(cid:18)(cid:16)X (cid:62)X(cid:17)
Moreover, for any non empty subset A of distinct indices in(cid:8)1, . . . , n2(cid:9), the

= ((n − (qak ∨ qa(cid:96))) × (n − (rak ∨ ra(cid:96))))1≤k,(cid:96)≤K .

(9)

A,A

1≤k,(cid:96)≤K

matrix X (cid:62)

A XA is invertible.

Lemma 6. Assume that we have at our disposal the Cholesky factorization
of X (cid:62)
A XA. The updated factorization on the extended set A∪{j} only requires
solving a |A|-size triangular system, with complexity O(|A|2). Moreover, the
downdated factorization on the restricted set A\{j} requires a rotation with
negligible cost to preserve the triangular form of the Cholesky factorization
after a column deletion.

Remark. We were able to obtain a closed-form expression of the inverse
A XA)−1 for some special cases of the subset A, namely, when the quo-
(X (cid:62)
tients/ratios associated with the Euclidean divisions of the elements of A are
endowed with a particular ordering. Moreover, for addressing any general
problem, we rather solve systems involving X (cid:62)
A XA by means of a Cholesky
factorization which is updated along the homotopy algorithm. These up-
dates correspond to adding or removing an element at a time in A and are
performed eﬃciently as stated in Lemma 6.

These lemmas are the building blocks for our LARS implementation
given in Algorithm 1, where we detail the leading complexity associated
with each part. The global complexity is in O(mn2 + ms2) where m is the
ﬁnal number of steps in the while loop. These steps include all the succes-
sive additions and deletions needed to reach s, the ﬁnal targeted number
of active variables. At the end of day, we have m block wise prediction ˆY
associated with the series of m estimations of ˆB(λ). The above complex-
ity should be compared with the usual complexity of the LARS algorithm,
when no particular structure is at play in Problem (4):
in such a case, a
implementation of the LARS as in [1] would be at least in O(mn4 + ms2).
Concerning the memory requirements, we only need to store the n × n
data matrix Y once. Indeed, since we have at our disposal the analytic form
of any sub matrix extracted from X (cid:62)X , we never need to compute neither
store this large n2 × n2 matrix. This paves the way for quickly processing
data with thousands of rows and columns.

9

Algorithm 1: Fast LARS for two-dimensional change-point estima-
tion

Input: data matrix Y, maximal number of active variables s.

// Initialization
Start with no change-point A ← ∅, ˆB = 0
Compute current correlations ˆc = X (cid:62)Y with Lemma 4
while λ > 0 or |A| < s do

// O(n2)

// Update the set of active variables
Determine next change-point(s) by setting λ ← (cid:107)ˆc(cid:107)∞ and
A ← {j : ˆcj = λ}
A XA with Lemma 5
Update the Cholesky factorization of X (cid:62)
// O(|A|2)

// Compute the direction of descent

Get the unormalized direction ˜wA ←(cid:0)X (cid:62)
(cid:113)

// O(|A|2)
Normalize wA ← α ˜wA with α ← 1/
Compute the equiangular vector uA = XAwA and a = X (cid:62)uA with
// O(n2)
Lemma 4

˜w(cid:62)
Asign(ˆcA)

·AX·A(cid:1)−1 sign(ˆcA)

α−aj

, λ+cj
α+aj

(cid:111)
(cid:111)

(cid:110) λ−cj
(cid:110)− ˆBA/wA

// Compute the direction step
Find the maximal step preserving equicorrelation
γin ← min+
j∈Ac
Find the maximal step preserving the signs
γout ← min+
j∈A
The direction step that preserves both is ˆγ ← min(γin, γout)
Update the correlations ˆc ← ˆc − ˆγa and ˆBA ← ˆBA + ˆγwA
accordingly

// O(n)

// Drop variable crossing the zero line
if γout < γin then

Remove existing change-point(s) A ← A\(cid:110)

Downdate the Cholesky factorization of X (cid:62)

(cid:111)

j ∈ A : ˆBj = 0
A XA

// O(|A|)

Output: Sequence of triplet (A, λ, ˆB) recorded at each iteration.

10

4 Simulation study

In this Section, we conduct a set of simulation studies to assess the perfor-
mances of our proposal. First, we report the computational performances of
Algorithm 1 and of its practical implementation in terms of timings. Second,
we report the statistical performances of our estimators (5) for recovering the
true change-points by means of Receiver Operating Characteristic (ROC)
curves.

4.1 Data generation

All synthetic data are generated from Model (1). We control the computa-
tional diﬃculty of the problem by varying the sample size n. The statistical
diﬃculty is controlled by varying σ, the standard deviation of the Gaus-
sian noise E. We chose diﬀerent patterns for the true matrix U(cid:63) designed
to mimic the variety of block matrix structures met in Hi-C data. These
patterns are obtained by changing the parameters µ(cid:63)
k,(cid:96)s, each of whom con-
trolling the intensity in block (k, (cid:96)) of U(cid:63). We consider four diﬀerent scenarii,
all with K(cid:63)
2 = 4 change-points
along the columns.

1 = 4 change-points along the rows and K(cid:63)

(cid:16)

(cid:16)

µ(cid:63),(1)
k,(cid:96)

µ(cid:63),(3)
k,(cid:96)

(cid:17)

(cid:17)

=

=




 ,
 ,

1 0 1 0 1
0 1 0 1 0
1 0 1 0 1
0 1 0 1 0
1 0 1 0 1
1 0 0 0 0
0 1 1 1 1
0 1 1 0 0
0 1 0 1 0
0 1 0 0 1



 ,

(cid:16)

µ(cid:63),(2)
k,(cid:96)

(cid:16)

µ(cid:63),(4)
k,(cid:96)

(cid:17)

=

(cid:17)



=

1 0 0 0 0
0 1 0 0 0
0 0 1 0 0
0 0 0 1 0
0 0 0 0 1
0 −1
0
1
0 −1
0
1

0 −1 −1 −1 −1
−1 −1
0
−1
1
0
−1 −1
0
−1
0
1

 .

The ﬁrst (µ(cid:63),(1)

(10)
k,(cid:96) ) corresponds to a “checkerboard-shaped” matrix, that is,
a natural two dimensional extension of a one dimensional piece-wise constant
problem. The second (µ(cid:63),(2)
k,(cid:96) ) deﬁnes a block diagonal model that mimics the
cis-interactions in the human Hi-C experiments: these are the most usual
interactions found in the cell, which occur between nearby elements along
the genome. The third (µ(cid:63),(3)
k,(cid:96) ) conﬁgurations describe
more complex patterns that can be found in Hi-C experiments, which also
correspond to more diﬃcult change-points problems.

k,(cid:96) ) and fourth (µ(cid:63),(4)

11

Examples of matrices Y are displayed in Figure 4 for these four scenarii,
with n = 100 and σ = 1 which corresponds to a relatively small level of
noise in this problem.

µ(cid:63),(1)

µ(cid:63),(2)

µ(cid:63),(3)

µ(cid:63),(4)

Figure 4: Data matrices Y drawn from Model 1 for σ = 1, n = 100 and
various block wise pattern for U(cid:63).

4.2 Competitors and implementation details

In our experiments, we compare our methodology with popular methods for
segmentation and variable selection that we adapted to the speciﬁc problem
of two-dimensional change-points detection:

1. First, we adapt Breiman et al.’s classiﬁcation and regression trees [3]
(hereafter called CART) by using the successive boundaries provided
by CART as change-points for the two-dimensional data. We use the
implementation provided by the publicly available R package rpart.

2. Second, we adapt Harchaoui and L´evy-Leduc’s method [8] (hereafter
HL), which is the exact one-dimensional counterpart of our approach.
To analyse two-dimensional data, we apply this procedure to each row
of Y in order to recover the change-points of each row. The change-
points appearing in the diﬀerent rows are claimed to be change-points
for the two-dimensional data either if they appear at least in one row
(variant HL1) or if they appear in ([n/2] + 1) rows (variant HL2). This
approach is ﬁtted by solving n Lasso problems (one per row of Y) by
means of the R package glmnet.

3. Third, we consider an adaptation of the fused-Lasso (hereafter FL2D).
Indeed, as illustrated in the introduction, the basic 2-dimensional
fused-Lasso for signal approximator is not tailored for recovering change

12

points. We thus consider the following variant, which applied a fused-
Lasso penalty on the following linear model:



(cid:125)



(cid:124)

β(F L)
1

...

...

β(F L)
n
β(F L)
n+1

(cid:123)(cid:122)

β(F L)
2n
B(F L)



(cid:125)

+ E



(cid:124)

Y =

1n

0n
...
...
0n

0n
1n
. . .

···

···

. . .
1n
0n

···
. . .
. . .
. . .
···
X (F L)

(cid:123)(cid:122)

0n
...
...
0n
1n

In
...
...
...
In

where 1n (resp. 0n) is a size-n column vector of ones (resp. zeros), In a
n× n-diagonal matrix of ones and Y,E are deﬁned as in Equation (3).
in row)
The FL2D method detects a change-point in columns (resp.
i+1 with 1 ≤ i ≤ n − 1 (resp.
if two successive values β(F L)
n + 1 ≤ i ≤ 2n − 1) are diﬀerent. To solve this problem, we must ﬁt a
general fused-Lasso problem. We rely on the R package genlasso for
this task.

and β(F L)

i

4. Finally, our own procedure, that we call blockseg, is implemented
in the R package blockseg which is available from the Comprehen-
sive R Archive Network (CRAN, [17]). Most of the computation are
performed in C++ using the library armadillo for linear algebra [18].

In what follows, all experiments were conducted on a Linux workstation

with Intel Xeon 2.4 GHz processor and 8 GB of memory.

4.3 Numerical performances

We start by presenting in Figure 5 the computational time for 100 runs
of each method for ﬁnding n change-points in a matrix drawn from the
“checkerboard” scenario, with n = 100 and σ = 5.
Independent of its
statistical performance, we can see on this small problem that the adaptation
of the fused-Lasso cannot be used for analyzing real Hi-C problems. On
the other hand, our modiﬁed CART procedure is extremely fast. However,
we will see that its statistical performances are quite poor. Finally, our
implementation blockseg is quite eﬃcient as it clearly outperforms HL. This
should be emphasized since blockseg is a two-dimensional method dealing
with data with size n2, while HL is a 1-dimensional approach that addresses
two univariate problems of size n.

13

s
e
r
u
d
e
c
o
r
P

linear scale

logarithm scale

Figure 5: Violin plots of the computational times for each procedure with
a linear scale (left) and logarithm scale (right): CART methodology (CART),
adaptation of [8] (HL), our method (blockseg) and fused LASSO (FL).

(cid:16)

(cid:17)

µ(cid:63),(1)
k,(cid:96)

We now consider blockseg on its own in order to study the scalability of
our approach regarding the problem dimension. To this end, we generated
“checkerboard” matrix
given in (10) with various sizes n (from 100
to 5000) and various values of the maximal number of activated variables
s (from 50 to 750). The median runtimes obtained from 4 replications (+
2 for warm-up) are reported in Figures 6. The left (resp. the right) panel
gives the runtimes in seconds as a function of s (resp. of n). These results
give experimental evidence for the theoretical complexity O(mn2 + ms2)
that we established in Section 3 and thus for the computational eﬃciency of
our approach: applying blockseg to matrices containing 107 entries takes
less than 2 minutes for s = 750.

4.4 Statistical performances

We evaluate the performance of the diﬀerent competitors for recovering the
true change-points in the 4 scenarii deﬁned in Section 4.1 for an increasing
level of diﬃculty. We draw 1000 datasets for each scenario for a varying level
of noise σ ∈ {1, 2, 5, 10} and for a problem size of n = 100. Note that we
use this relatively small problem size to allow the comparison with methods
HL and FL2D that would not work for greater values of n.

14

FLblocksegHLCART050000100000150000Time [milliseconds]FLblocksegHLCART1e+031e+041e+05Time [milliseconds])
e
l
a
c
s
-
g
o
l

,
s
d
n
o
c
e
s
(

s
g
n
m

i

i
t

sparsity level (s)

sample size (n)

Figure 6: Left: Computational time (in seconds) for various values of n as
a function of the sparsity level s = |A| reached at the end of the algorithm.
Right: Computation time (in seconds) as a function of sample size n.

Figure 7 shows the results in terms of receiver operating characteristic
(ROC) curves for recovering the change-points in rows, averaged over the
1000 runs. Similar results hold for the change-points in columns. This Figure
exhibits the very good performance of our method, which outperforms its
competitors by retrieving the change-points with a very small error rate
even in high noise level frameworks. Moreover, our method seems to be less
sensitive to the block pattern shape in matrix U than the other ones. In
order to further assess our approach we give in Figure 8 the boxplots of the
Area Under Curve (AUC) for the diﬀerent ROC curves. We also give in
Table 1 the mean of the AUC and the associated standard deviation.

In order to further compare the diﬀerent approaches we generated ma-
trices Y satisfying Model (1) with a “checkerboard” matrix
given in
(10) for n ∈ {50, 100, 250}. We observe from Table 2 that the performance
of our method are on a par with those of FL2D for n = 50 and 100. However,
for n = 250 the computational burden of FL2D is so large that the results are
not available, see the blue crosses in Table 2. The AUC are also displayed
with boxplots in Figure 9.

µ(cid:63),(1)
k,(cid:96)

(cid:16)

(cid:17)

5 Model selection

In the previous experiments we did not need to explain how to choose the
number of estimated change-points since we used ROC curves for comparing

15

llllllllllllllllllllllllllllll10−1100101102103200400600sample size (n)llllll100250500100025005000llllllllllllllllllllllllllllll10−1100101102103010002000300040005000sparisty (s)lllll50100250500750e
t
a
r

e
v
i
t
i
s
o
p

e
u
r
T

e
t
a
r

e
v
i
t
i
s
o
p

e
u
r
T

e
t
a
r

e
v
i
t
i
s
o
p

e
u
r
T

e
t
a
r

e
v
i
t
i
s
o
p

e
u
r
T

σ = 1

σ = 2

σ = 5

σ = 10

1

o
i
r
a
n
e
c
s

2

o
i
r
a
n
e
c
s

3

o
i
r
a
n
e
c
s

4

o
i
r
a
n
e
c
s

False positive rate

False positive rate

False positive rate

False positive rate

Figure 7: ROC curves for the estimated change-points in rows for blockseg
(dotted green), HL1 (double-dashed purple), HL2 (in dotted blue), CART (solid
red) and FL2D (long-dashed orange). Each row is associated to a scenario
depicted in Section 4.1.

16

0.000.250.500.751.000.000.250.500.751.000.000.250.500.751.000.000.250.500.751.000.000.250.500.751.000.000.250.500.751.000.000.250.500.751.000.000.250.500.751.000.000.250.500.751.000.000.250.500.751.000.000.250.500.751.000.000.250.500.751.000.000.250.500.751.000.000.250.500.751.000.000.250.500.751.000.000.250.500.751.000.000.250.500.751.000.000.250.500.751.000.000.250.500.751.000.000.250.500.751.000.000.250.500.751.000.000.250.500.751.000.000.250.500.751.000.000.250.500.751.000.000.250.500.751.000.000.250.500.751.000.000.250.500.751.000.000.250.500.751.000.000.250.500.751.000.000.250.500.751.000.000.250.500.751.000.000.250.500.751.00Scenario 1

σ = 1 σ = 2 σ = 5 σ = 10
0.972
0.644

0.913

0.733

(0.0145)
0.918

(0.102)
0.618

(0.0427)
0.576

(0.0744)
0.482

(0.0421)
0.738

(0.139)
0.535

(0.0708)
0.448

(0.0713)
0.497

(0.0988)
0.623

(0.127)
0.407

(0.102)
0.337

(0.0734)
0.498

(0.118)
0.608

(0.13)
0.363

(0.108)
0.323

(0.072)
0.486

Scenario 2

σ = 2 σ = 5 σ = 10
0.896
0.617

0.689

(0.0555)
0.603

(0.125)
0.505

(0.0874)
0.374

(0.0777)
0.487

(0.107)
0.601

(0.127)
0.382

(0.105)
0.326

(0.0727)
0.491

(0.123)
0.603

(0.127)
0.351

(0.107)
0.317

(0.0745)
0.484

σ = 1
0.977

(0.0206)
0.608

(0.116)
0.635

(0.0535)
0.498

(0.0653)
0.496

(0.107)

(0.107)

(0.117)

(0.119)

(0.112)

(0.124)

(0.126)

(0.118)

Scenario 3

σ = 1 σ = 2 σ = 5 σ = 10
0.983

0.945

0.758

0.63

(0.0114)
0.799

(0.0855)
0.575

(0.0458)
0.524

(0.0612)
0.474

(0.0391)
0.772

(0.0956)
0.479

(0.0819)
0.384

(0.0711)
0.485

(0.113)
0.667

(0.121)
0.391

(0.0981)
0.326

(0.0716)
0.495

(0.125)
0.623

(0.121)
0.368

(0.105)
0.319

(0.0738)
0.502

σ = 1
0.983

(0.00927)
0.969

(0.051)
0.556

(0.0252)
0.616

(0.0527)
0.484

Scenario 4

σ = 2 σ = 5 σ = 10
0.707
0.977

0.866

(0.0179)
0.931

(0.0722)
0.504

(0.0514)
0.416

(0.0696)
0.493

(0.102)
0.789

(0.135)
0.418

(0.0974)
0.327

(0.0714)
0.512

(0.124)
0.68

(0.134)
0.368

(0.11)
0.316

(0.067)
0.516

blockseg

FL2D

HL1

HL2

CART

blockseg

FL2D

HL1

HL2

CART

(0.106)

(0.11)

(0.114)

(0.115)

(0.0905)

(0.0889)

(0.0985)

(0.111)

Table 1: Mean and standard deviation of the area under the ROC curve for
the diﬀerent scenarii, diﬀerent algorithms and diﬀerent values of the noise
variance.

the methodologies. However, in real data applications, it is necessary to
propose a methodology for estimating the number of change-points. This is
what we explain in the following.
In practice, we take s = K2

max where Kmax is an upper bound for K(cid:63)
1
and K(cid:63)
2 . For choosing the ﬁnal change-points we shall adapt the well-known
stability selection approach devised by [15]. More precisely, we randomly
choose M times n/2 columns and n/2 rows of the matrix Y and for each
subsample we select s = K2
max active variables. Finally, after the M data
resamplings, we keep the change-points which appear a number of times

17

Scenario 1

Scenario 2

σ

Scenario 3

σ

Scenario 4

σ

σ

Figure 8:
scenarii and the diﬀerent algorithms as a function of the noise variance.

Boxplots of the area under the ROC curve for the diﬀerent

larger than a given threshold. By the deﬁnition of the change-points given

in (5), a change-point(cid:98)t1,k or(cid:98)t2,(cid:96) may appear several times in a given set of
(cid:17)

resampled observations. Hence, the score associated with each change-point
corresponds to the sum of the number of times it appears in each of the M
subsamplings.

(cid:16)

To evaluate the performances of this methodology, we generated obser-
µ(cid:63),(1)
vations according to the “checkerboard” model deﬁned in (1) with
k,(cid:96)
deﬁned in (10), s = 225 and M = 100. The results are given in Figure 10
which displays the score associated to each change-point for a given matrix
Y (top). We can see from the top part of Figure 10 some spurious change-
points appearing near from the true change-point positions.
In order to
identify the most representative change-point in a given neighborhood, we
keep the one with the largest score among a set of contiguous candidates.
The result of such a post-processing is displayed in the bottom part of Figure
10 and in Figure 11. More precisely the boxplots associated to the estima-

18

lllllllllllllllllllllllllllllllllllllllllllllll0.250.500.751.0012510CARTFLHL1HL2blocksegllllllllllllllllllllllllllllllllllllllllllllllllllllllllll0.250.500.751.0012510CARTFLHL1HL2blockseglllllllllllllllllllllllllllllllllllllllllll0.250.500.751.0012510CARTFLHL1HL2blocksegllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll0.250.500.751.0012510CARTFLHL1HL2blocksegσ = 1

σ = 2

n = 50 n = 100 n = 250
0.896

0.972

0.993

n = 50 n = 100 n = 250
0.791

0.982

0.923

(0.0425)
0.814

(0.132)
0.574

(0.0598)
0.56

(0.101)
0.445

(0.0162)
0.906

(0.0997)
0.619

(0.0426)
0.573

(0.0642)
0.479

(0.00463)

X

0.66

(0.0255)
0.59

(0.0432)
0.498

(0.0789)
0.679

(0.133)
0.467

(0.0899)
0.424

(0.0972)
0.487

(0.0398)
0.753

(0.128)
0.527

(0.084)
0.451

(0.0713)
0.487

(0.00865)

X

0.611

(0.0513)
0.472

(0.0467)
0.512

(0.123)

(0.108)

(0.0589)

(0.125)

(0.114)

(0.0708)

σ = 5

σ = 10

n = 50 n = 100 n = 250
0.646

0.739

0.91

n = 50 n = 100 n = 250
0.577

0.766

0.642

(0.127)
0.631

(0.132)
0.382

(0.106)
0.333

(0.0905)
0.488

(0.11)
0.629

(0.125)
0.397

(0.107)
0.342

(0.0775)
0.501

(0.0394)

X

0.481

(0.0909)
0.341

(0.0451)
0.497

(0.112)
0.602

(0.118)
0.364

(0.103)
0.325

(0.083)
0.466

(0.124)
0.616

(0.115)
0.35

(0.108)
0.313

(0.0729)
0.483

(0.0867)

X

0.386

(0.115)
0.317

(0.0539)
0.48

(0.115)

(0.119)

(0.0917)

(0.129)

(0.131)

(0.117)

blockseg

FL2D

HL1

HL2

CART

blockseg

FL2D

HL1

HL2

CART

Table 2: Mean and standard deviation of the area under the ROC curve
as a function of the standard deviation of the noise, the algorithms and the
size of the matrices. The crosses correspond to cases where the results are
not available.

tion of K(cid:63)
1 (resp. the histograms of the estimated change-points in rows) are
displayed in the bottom part of Figure 10 (resp. in Figure 11) for diﬀerent
values of σ and diﬀerent thresholds (thresh) expressed as a percentage of
the largest score. We can see from these ﬁgures that when thresh is in the
interval [20, 40] the number and the location of the change-points are very
well estimated even in the high noise level case.

In order to further assess our methodology including the post-processing
step and to be in a framework closer to our real data application, we gener-

19

σ = 1

σ = 2

n

σ = 5

n

σ = 10

n

n

Figure 9: Boxplots of the area under the ROC curve as a function of the
standard deviation of the noise, the size of the matrices and the methods.

σ = 1

σ = 2

σ = 5

Figure 10: Top: Scores associated to each estimated change-points for
diﬀerent values of σ; the true change-point positions in rows and columns
are located at 101, 201, 301 and 401. Bottom: Boxplots of the estimation
of K(cid:63)

1 for diﬀerent values of σ and thresh after the post-processing step.

20

lllllllllllll0.20.40.60.81.050100250CARTFLHL1HL2blocksegllll0.20.40.60.81.050100250CARTFLHL1HL2blocksegllllllllllllllll0.20.40.60.81.050100250CARTFLHL1HL2blocksegllllllllllllll0.20.40.60.81.050100250CARTFLHL1HL2blockseglllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll51015201020304050lllllllllllllllllllllllllllllllllllllllllllll05101520251020304050lllllllllllllllllllllll0204060801020304050σ = 1

σ = 2

σ = 5

%
0
5

%
0
4

%
0
3

%
0
2

%
0
1

Figure 11: Barplots of the estimated change-points for diﬀerent variances
(columns) and diﬀerent thresholds (rows) for the model µ(cid:63),(1).

(cid:16)

µ(cid:63),(1)
k,(cid:96)

(cid:17)

1 = K(cid:63)

1 = K(cid:63)
ated observations following (1) with n = 1000 and K(cid:63)
2 = 100 where we
except
used for the matrix U the same shape as the one of the matrix
that K(cid:63)
2 = 100. In this framework, the proportion of change-points is
thus ten times larger than the one of the previous case. The corresponding
results are displayed in Figures 12, 13 and 14. We can see from the last
ﬁgure that taking a threshold equal to 20% provides the best estimations of
the number and of the change-point positions. This threshold corresponds
to the lower bound of the thresholds interval obtained in the previous con-
ﬁguration. Our package blockseg provides an estimation of the matrix U
for any threshold given by the user as we shall explain in the next section.

21

010020030040050001002003004000100200300400500010020030040001002003004000100200300400010020030040050001002003004000100200300400500010020030040001002003004000100200300400010020030040050001002003004000100200300400500010020030040001002003004000100200300400010020030040050001002003004000100200300400500010020030040001002003004000100200300400010020030040050001002003004000100200300400500010020030040001002003004000100200300400σ = 1

σ = 2

σ = 5

Figure 12: Boxplots of the estimation of K(cid:63)
thresholds (thresh) after the post-processing step.

1 for diﬀerent values of σ and

σ = 1

σ = 2

σ = 5

%
0
5

%
0
4

%
0
3

%
0
2

%
0
1

Figure 13: Barplots of the estimated change-points for diﬀerent variances
(columns) and diﬀerent thresholds (rows) in the case where n = 1000 and
K(cid:63)

1 = K(cid:63)

2 = 100.

22

lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll0501001501020304050lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll0501001502001020304050lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll050100150200102030405001002003004000200400600800010020030040002004006008000100200300400020040060080001002003004000200400600800010020030040002004006008000100200300400025050075001002003004000250500750010020030040002505007500100200300400025050075001002003004000250500750010020030040002505007501000010020030040002505007501000010020030040002505007501000010020030040002505007501000010020030040002505007501000σ = 1

σ = 2

σ = 5

%
0
5

%
0
4

%
0
3

%
0
2

%
0
1

Figure 14: Zoom of the barplots of Figure 13.

23

0100200300400050100150200250010020030040005010015020025001002003004000501001502002500100200300400050100150200250010020030040005010015020025001002003004000501001502002500100200300400050100150200250010020030040005010015020025001002003004000501001502002500100200300400050100150200250010020030040005010015020025001002003004000501001502002500100200300400050100150200250010020030040005010015020025001002003004000501001502002506 Application to HiC data

In this section, we apply our methodology to publicly available HiC data
(http://chromosome.sdsc.edu/mouse/hi-c/download.html) already stud-
ied by [4]. This technology is based on a deep sequencing approach and
provides read pairs corresponding to pairs of genomic loci that physically
interacts in the nucleus, see [12] for more details. The raw measurements
provided by HiC data is therefore a list of pairs of locations along the chro-
mosome, at the nucleotide resolution. These measurement are often sum-
marized as a square matrix where each entry at row i and column j stands
for the total number of read pairs matching in position i and position j,
respectively. Positions refer here to a sequence of non-overlapping windows
of equal sizes covering the genome. The number of windows may vary from
one study to another:
[12] considered a Mb resolution, whereas [4] went
deeper and used windows of 40kb (called hereafter the resolution).

In our study, we processed the interaction matrices of Chromosomes 1
and 19 of the mouse cortex at a resolution 40 kb and we compared the num-
ber and the location of the estimated change-points found by our approach
with those obtained by [4] on the same data since no ground truth is avail-
able. More precisely, in the case of Chromosome 1, n = 4930 and in the case
of Chromosome 19, n = 1534.

Let us ﬁrst give the results obtained by using our methodology. Figure
15 displays the change-point locations obtained for the diﬀerent values of
the threshold used in our adaptation of the stability selection approach and

deﬁned in Section 5. The corresponding estimated matrices (cid:98)Y = (cid:98)U for

Chromosome 1 and 19 are displayed in Figure 16 when the thresholds are
equal to 10, 15 and 20%, which correspond to the red horizontal levels in
Figure 15.

In order to compare our approach with the technique devised by [4],
we display in Figure 17 the number of change-points in rows found by our
methodology as a function of the threshold and a red line corresponding to
the number of change-points found by [4]. Note that we did not display the
change-points in columns in order to save space since they are similar.

We also compute the two parts of the Hausdorﬀ distance for the change-

points in rows which is deﬁned by

where(cid:98)t and(cid:98)tB are the change-points in rows found by our approach and

= max

d1

(11)

(cid:16)(cid:98)tB,(cid:98)t
(cid:17)

d

(cid:16)

(cid:16)(cid:98)tB,(cid:98)t
(cid:17)

, d2

(cid:16)(cid:98)tB,(cid:98)t
(cid:17)(cid:17)

,

24

Figure 15: Plots of the estimated change-points locations (x-axis) for dif-
ferent thresholds (y-axis) from 0.5% to 50% by 0.5% for Chromosome 1
(left) and Chromosome 19 (right). The estimated change-point locations
associated to threshold which are multiples of 5% are displayed in red.

10%

15%

20%

1

e
m
o
s
o
m
o
r
h
C

9
1

e
m
o
s
o
m
o
r
h
C

Figure 16: Estimated matrices (cid:98)Y = (cid:98)U for Chromosomes 1 and 19 for the

thresholds 10, 15 and 20%.

25

llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll0102030405001000200030004000llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll0102030405050010001500Figure 17: Number of change-points in rows found by our approach as
a function of the threshold (in %) for the interaction matrices of Chromo-
some 1 (left) and Chromosome 19 (right) of the mouse cortex. The red line
corresponds to the number of change-points found by [4].

[4], respectively. In (11),

d1 (a, b) = sup
b∈b

inf
a∈a

|a − b| ,

d2 (a, b) = d1 (b, a) .

(12)

(13)

More precisely, Figure 18 displays the boxplots of the d1 and d2 parts of
the Hausdorﬀ distance without taking the supremum in orange and blue,
respectively.

Figure 18: Boxplots for the inﬁmum parts of the Hausdorﬀ distances d1
(orange) and d2 (blue) between the change-points found by [4] and our
approach for the Chromosome 1 (left) and the Chromosome 19 (right) of
the mouse cortex for the diﬀerent thresholds in %.

We can observe from Figure 18 that some diﬀerences indeed exist be-
tween the segmentations produced by the two approaches but that the

26

llllllllllllllllllllllllllllllllllllllllll2003004005006000.00.51.01.52.0llllllllllllllllllllllllllllllll4080120160051015lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll010020030000.10.20.30.40.50.60.70.80.911.11.21.31.41.51.61.71.81.92llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll0501001502000123456789101112131415boundaries of the blocks are quite close when the number of estimated
change-points are the same, which is the case when thresh = 1.8% (left)
and 10% (right).

In the case where the number of estimated change-points are on a par
with those of [4], we can see from Figure 19 that the change-points found
with our strategy present a lot of similarities with those found by the HMM
based approach of [4]. However, contrary to our method, the approach of
[4] can only deal with binned data at the resolution of several kilobases
of nucleotides. The very low computational burden of our strategy paves
the way for processing data collected at a very high resolution, namely at
the nucleotide resolution, which is one of the main current challenges of
molecular biology.

Figure 19: Topological domains detected by [4] (upper triangular part of
the matrix) and by our method (lower triangular part of the matrix) from
the interaction matrix of Chromosome 1 (left) and Chromosome 19 (right)
of the mouse cortex with a threshold giving 232 (resp 85) estimated change-
points in rows and columns.

7 Conclusion

In this paper, we proposed a novel approach for retrieving the boundaries of a
block wise constant matrix corrupted with noise by rephrasing this problem
as a variable selection issue. Our approach is implemented in the R package
blockseg which is available from the Comprehensive R Archive Network
(CRAN). In the course of this study, we have shown that our method has

27

two main features which make it very attractive. Firstly, it is very eﬃcient
both from the theoretical and practical point of view. Secondly, its very low
computational burden makes its use possible on very large data sets coming
from molecular biology.

A Proofs

λn

=

j

j

2

,

,

Using that X (cid:62)Y = (T ⊗ T)

A.1 Proofs of statistical results

Proofof Lemma 2. A necessary and suﬃcient condition for a vector (cid:98)B in
Rn2 to minimize the function Φ deﬁned by: Φ(B) =(cid:80)n2
(cid:80)n2
i=1(Yi − (XB)i)2 +
Φ at (cid:98)B that is:
i=1 |Bi|, is that the zero vector in Rn2 belongs to the subdiﬀerential of
if (cid:98)Bj (cid:54)= 0,
if (cid:98)Bj = 0.

λn
2

(cid:62)Y = (T(cid:62) ⊗ T(cid:62))Y = Vec(T(cid:62)YT), where

(cid:12)(cid:12)(cid:12)(cid:12) ≤ λn
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≥ xn

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(sn − rn)−1

(cid:16)X (cid:62)(Y − X(cid:98)B)
(cid:17)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:16)X (cid:62)(Y − X(cid:98)B)
(cid:17)
(cid:80)n
(cid:96)=j Yk,(cid:96), and that (cid:98)U = X(cid:98)B, Lemma 2 is proved.
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(sn − rn)−1
sn−1(cid:88)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≥ xn
 .
≤ (cid:88)
 ≤ exp[−η(sn − rn)xn](E(exp(ηE1,1)))sn−rn

En,j ≥ xn

(sn − rn)−1

(T(cid:62)YT)i,j =(cid:80)n
 max

P

By (A1) and the Markov inequality, we get that for all positive η,

Proofof Lemma 3. Note that

sn−1(cid:88)

j=rn

En,j

j=rn

1≤rn<sn≤n
|rn−sn|≥vn

1≤rn<sn≤n
|rn−sn|≥vn

En,j

k=i

P

P

j=rn

sn−1(cid:88)
(sn − rn)−1

P

By taking η = xn/(2β), we get that

≤ exp[−η(sn − rn)xn + βη2(sn − rn)].

 ≤ exp[−x2

sn−1(cid:88)

En,j ≥ xn

j=rn

28

n(sn − rn)/(4β)].

Since the same result is valid for −En,j, we get that

 max

P

1≤rn<sn≤n
|rn−sn|≥vn

(cid:18)(cid:26)
(cid:18)

≤ P

P

max
1≤k≤K(cid:63)

1

max
1≤k≤K(cid:63)

1

j=rn

sn−1(cid:88)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(sn − rn)−1
(cid:27)
(cid:12)(cid:12) > nδn
(cid:12)(cid:12)(cid:98)t1,k − t(cid:63)
(cid:19)
(cid:12)(cid:12) > nδn
(cid:12)(cid:12)(cid:98)t1,k − t(cid:63)

1,k

1,k

En,j

(cid:26)

∪

which concludes the proof of Lemma 3.

Proofof Proposition 1. Since

 ≤ 2n2 exp[−x2

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≥ xn

nvn/(4β)],

(cid:12)(cid:12)(cid:98)t2,k − t(cid:63)
(cid:12)(cid:12)(cid:98)t2,k − t(cid:63)

2,k

max
1≤k≤K(cid:63)

2

(cid:18)

+ P

max
1≤k≤K(cid:63)

2

(cid:27)(cid:19)
(cid:12)(cid:12) > nδn
(cid:19)
(cid:12)(cid:12) > nδn
|(cid:98)t2,k − t(cid:63)

2,k

,

2

(14)

2

2

.

k=1

Cn =

max
1≤k≤K(cid:63)

{1, . . . , K(cid:63)
deﬁned by

nδn) ≤ (cid:80)K(cid:63)

it is enough to prove that both terms in (14) tend to zero for proving (6).
We shall only prove that the second term in the rhs of (14) tends to zero, the
P(|(cid:98)t2,k − t(cid:63)
2,k| >
proof being the same for the ﬁrst term. Since P(max1≤k≤K(cid:63)
2,k| > nδn), it is enough to prove that for all k in
(cid:27)
(cid:26)
2,k| > nδn}. Let Cn be

2}, P(An,k) → 0, where An,k = {|(cid:98)t2,k − t(cid:63)
2,k| < I (cid:63)
It is enough to prove that, for all k in {1, . . . , K(cid:63)
Cn) tend to 0, as n tends to inﬁnity.
Let us ﬁrst prove that for all k in {1, . . . , K(cid:63)

min,2/2
(15)
2}, P(An,k∩Cn) and P(An,k∩
2}, P(An,k∩Cn) → 0. Observe
2}. For a
2,k. Applying (7) and (8) with rj + 1 = n,
2,k on the other

given k, let us assume that(cid:98)t2,k ≤ t(cid:63)
qj + 1 = (cid:98)t2,k on the one hand and rj + 1 = n, qj + 1 = t(cid:63)

2,k+1, for all k in {1, . . . , K(cid:63)

|(cid:98)t2,k − t(cid:63)

hand, we get that

that (15) implies that t(cid:63)

2,k−1 <(cid:98)t2,k < t(cid:63)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)t(cid:63)
2,k−1(cid:88)
j=(cid:98)t2,k
1 +1,k −(cid:98)µK(cid:63)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≤ λn.
Hence using (1), the notation: E([a, b]; [c, d]) = (cid:80)b
deﬁnition of (cid:98)U given by Lemma 2, we obtain that
1 +1,k+1) + E(n; [(cid:98)t2,k, t(cid:63)

(cid:12)(cid:12)(cid:12)(t(cid:63)
2,k −(cid:98)t2,k)(µ(cid:63)

2,k−1(cid:88)
j=(cid:98)t2,k

(cid:12)(cid:12)(cid:12) ≤ λn,

j=c Ei,j and the

2,k − 1])

(cid:98)Un,j

(cid:80)d

Yn,j −

i=a

K(cid:63)

t(cid:63)

which can be rewritten as follows

29

(cid:12)(cid:12)(cid:12)(t(cid:63)
2,k −(cid:98)t2,k)(µ(cid:63)

1 +1,k − µ(cid:63)

K(cid:63)

1 +1,k+1) + (t(cid:63)
K(cid:63)

2,k −(cid:98)t2,k)(µ(cid:63)

1 +1,k+1 −(cid:98)µK(cid:63)
2,k − 1])(cid:12)(cid:12) ≤ λn.

+E(n; [(cid:98)t2,k, t(cid:63)

K(cid:63)

1 +1,k+1)

Thus,
P(An,k ∩ Cn) ≤ P(λn/(nδn) ≥ |µ(cid:63)

+ P({|µ(cid:63)

+ P({|E(n; [(cid:98)t2,k, t(cid:63)

K(cid:63)

1 +1,k −(cid:98)µK(cid:63)
2,k − 1])|/|t(cid:63)

1 +1,k − µ(cid:63)
2,k −(cid:98)t2,k| ≥ |µ(cid:63)
1 +1,k+1| ≥ |µ(cid:63)

K(cid:63)

K(cid:63)

1 +1,k+1|/3)

K(cid:63)

1 +1,k − µ(cid:63)
1 +1,k − µ(cid:63)

1 +1,k+1|/3} ∩ Cn)
1 +1,k+1|/3}∩ An,k).

K(cid:63)

K(cid:63)

K(cid:63)

(16)

The ﬁrst term in the rhs of (16) tends to 0 by (A2). By Lemma 3 with
xn = J (cid:63)
min/3, vn = nδn and (A3) the third term in the rhs of (16) tends to

0. Applying Lemma 2 with rj + 1 = n, qj + 1 =(cid:98)t2,k on the one hand and

2,k+1)/2 on the other hand, we get that

2,k+1)/2−1(cid:88)

(cid:98)Un,j

j=t(cid:63)

2,k

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≤ λn.

(t(cid:63)

2,k+t(cid:63)

Yn,j −

1 +1,k+1 within the interval [t(cid:63)

2,k, (t(cid:63)

2,k +t(cid:63)

2,k+1)/2−

2,k + t(cid:63)

2,k+t(cid:63)

rj + 1 = n, qj + 1 = (t(cid:63)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(t(cid:63)
2,k+1)/2−1(cid:88)
2,k, (cid:98)Un,j =(cid:98)µK(cid:63)
Since(cid:98)t2,k ≤ t(cid:63)
1 +1,k+1−(cid:98)µK(cid:63)
1] and we get that
2,k)|µ(cid:63)
2,k+1−t(cid:63)
(t(cid:63)

j=t(cid:63)

K(cid:63)

2,k

P(cid:16)

λn ≥ (t(cid:63)

+ P(cid:0)(t(cid:63)

2,k+1 − t(cid:63)

2,k)|µ(cid:63)
2,k+1 − t(cid:63)

Therefore the second term in the rhs of (16) can be bounded by

2,k, (t(cid:63)

2,k+t(cid:63)

2,k+1)/2−1])|.

1 +1,k+1|/2 ≤ λn+|E(n,|[t(cid:63)
(cid:17)
1 +1,k+1|/12
2,k, (t(cid:63)

2,k)−1(cid:12)(cid:12)E(n, ,|[t(cid:63)

1 +1,k − µ(cid:63)

K(cid:63)

K(cid:63)

2,k+1)/2 − 1])(cid:12)(cid:12)

2,k + t(cid:63)
≥ |µ(cid:63)

K(cid:63)

1 +1,k − µ(cid:63)

1 +1,k+1|/6

K(cid:63)

(cid:17)

By Lemma 3 and (A2), (A3) and (A4), we get that both terms tend to zero
as n tends to inﬁnity. We thus get that P(An,k ∩ Cn) → 0, as n tends to
inﬁnity.
Let us now prove that P(An,k ∩ Cn) tend to 0, as n tends to inﬁnity.

Observe that

P(An,k ∩ Cn) = P(An,k ∩ D((cid:96))

n ) + P(An,k ∩ D(m)

n ) + P(An,k ∩ D(r)
n ),

30

where

n =(cid:8)∃p ∈ {1, . . . , K(cid:63)}, (cid:98)t2,p ≤ t(cid:63)
n =(cid:8)∀k ∈ {1, . . . , K(cid:63)}, t(cid:63)
n =(cid:8)∃p ∈ {1, . . . , K(cid:63)}, (cid:98)t2,p ≥ t(cid:63)

(cid:9) ∩ Cn,
2,k−1 <(cid:98)t2,k < t(cid:63)
(cid:9) ∩ Cn.

D((cid:96))
D(m)
D(r)

2,p−1

2,p+1

2,k+1

(cid:9) ∩ Cn,

2−1(cid:88)

K(cid:63)

Using the same arguments as those used for proving that P(An,k ∩ Cn) → 0,
we can prove that P(An,k ∩ D(m)
n ) → 0, as n tends to inﬁnity. Let us now
prove that P(An,k ∩ D((cid:96))

n ) → 0. Note that

min/2} ∩ {(cid:98)t2,k+1 − t(cid:63)

2,k −(cid:98)t2,k > I (cid:63)
−(cid:98)t2,K(cid:63)

P({t(cid:63)

P(D((cid:96))

n ) ≤

min/2})
Applying (7) and (8) with rj + 1 = n, qj + 1 =(cid:98)t2,k on the one hand and

2,k > I (cid:63)

+P(t(cid:63)

min/2).

> I (cid:63)

(17)

2,K(cid:63)
2

k=1

2

rj + 1 = n, qj + 1 = t(cid:63)

2,k on the other hand, we get that

Thus,

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≤ λn.

t(cid:63)

Yn,j −

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)t(cid:63)
2,k−1(cid:88)
2,k−1(cid:88)
(cid:98)Un,j
j=(cid:98)t2,k
j=(cid:98)t2,k
min/2} ∩ {(cid:98)t2,k+1 − t(cid:63)
1 +1,k − µ(cid:63)
1 +1,k+1| ≥ |µ(cid:63)

K(cid:63)

K(cid:63)

+P({|µ(cid:63)

P({t(cid:63)
≤ P(λn/(nδn) ≥ |µ(cid:63)

2,k −(cid:98)t2,k > I (cid:63)
1 +1,k −(cid:98)µK(cid:63)
∩{(cid:98)t2,k+1 − t(cid:63)
+P({|E(n; [(cid:98)t2,k, t(cid:63)
min/2})
2,k > I (cid:63)
2,k −(cid:98)t2,k > I (cid:63)
2,k − 1])|/(t(cid:63)

2,k > I (cid:63)
1 +1,k+1|/3)
1 +1,k − µ(cid:63)
2,k −(cid:98)t2,k) ≥ |µ(cid:63)
min/2}).

∩{t(cid:63)

K(cid:63)

K(cid:63)

min/2})

1 +1,k+1|/3}

K(cid:63)

1 +1,k − µ(cid:63)

K(cid:63)

1 +1,k+1|/3}

K(cid:63)

(18)

Using the same arguments as previously we get that the ﬁrst and the third
term in the rhs of (18) tend to zero as n tends to inﬁnity. Let us now focus
on the second term of the rhs of (18). Applying (7) and (8) with rj + 1 = n,
2,k on the other

qj + 1 =(cid:98)t2,k+1 on the one hand and rj + 1 = n, qj + 1 = t(cid:63)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≤ λn.

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
(cid:98)t2,k+1−1(cid:88)

(cid:98)t2,k+1−1(cid:88)

hand, we get that

(cid:98)Un,j

Yn,j −

j=t(cid:63)

j=t(cid:63)

2,k

2,k

31

1 +1,k+1)((cid:98)t2,k+1 − t(cid:63)

2,k;(cid:98)t2,k+1 − 1])| ≤ λn.

2,k) + E(n, [t(cid:63)

The second term of the rhs of (18) is thus bounded by

Hence,
|(µ(cid:63)

K(cid:63)

1 +1,k −(cid:98)µK(cid:63)
P({λn((cid:98)t2,k+1 − t(cid:63)
∩{(cid:98)t2,k+1 − t(cid:63)
+P({((cid:98)t2,k+1 − t(cid:63)

K(cid:63)

2,k)−1 ≥ |µ(cid:63)
min/2})
2,k > I (cid:63)
2,k)−1|E(n, [t(cid:63)
2,k > I (cid:63)

1 +1,k − µ(cid:63)
1 +1,k+1|/6}
2,k;(cid:98)t2,k+1 − 1])| ≥ |µ(cid:63)
min/2}),

∩{(cid:98)t2,k+1 − t(cid:63)

K(cid:63)

1 +1,k − µ(cid:63)

K(cid:63)

1 +1,k+1|/6}

K(cid:63)

which tend to zero by Lemma 3, (A2), (A3) and (A4). It is thus proved that
the ﬁrst term in the rhs of (17) tends to zero as n tends to inﬁnity. The
same arguments can be used for addressing the second term in the rhs of

(17) since(cid:98)t2,K(cid:63)

2 +1 = n and hence(cid:98)t2,K(cid:63)

Using similar arguments, we can prove that P(An,k ∩ D(r)

n ) → 0, which

2 +1 − t(cid:63)

2,K(cid:63)
2

> I (cid:63)

min/2.

concludes the proof of Proposition 1.

A.2 Proofs of computational lemmas
Proofof Lemma 4. Consider X v for instance (the same reasoning applies for
X (cid:62)v): we have X v = (T⊗ T)v = Vec(TVT(cid:62)) where V is the n× n matrix
such that Vec(V) = v. Because of its triangular structure, T operates as
a cumulative sum operator on the columns of V. Hence, the computations
for the jth column is done by induction in n operations. The total cost for
the n columns of TV is thus n2. Similarly, right multiplying a matrix by
T(cid:62) boils down to perform cumulative sums over the rows. The ﬁnal cost
for X v = Vec(TVT(cid:62)) is thus 2n2 in case of a dense matrix V, and possibly
less when V is sparse.
Proofof Lemma 5. Let A = {a1, . . . , aK}, then

(cid:62)
A,A = (T ⊗ T)
•,A(T ⊗ T)•,A,

(19)

(cid:16)X (cid:62)X(cid:17)

(cid:62)
where (T ⊗ T)•,A (resp. (T ⊗ T)
•,A) denotes the columns (resp. the rows)
of T ⊗ T lying in A. For j in A, let us consider the Euclidean division of
j − 1 by n given by: (j − 1) = nqj + rj, then (T⊗ T)•,j = T•,qj +1 ⊗ T•,rj +1.
Hence, (T ⊗ T)•,A is a n2 × K matrix deﬁned by:
(T⊗T)•,A =

(cid:104)
T•,qa1+1 ⊗ T•,ra1+1; T•,qa2+1 ⊗ T•,ra2+1; . . . ; T•,qaK +1 ⊗ T•,raK +1

(cid:105)

.

32

Thus,

(T ⊗ T)•,A = T•,QA ∗ T•,RA, where QA = {qa1 + 1, . . . , qaK + 1},
RA = {ra1 + 1, . . . , raK + 1}

and ∗ denotes the Khatri-Rao product, which is deﬁned as follows for two
n × n matrices A and B

A ∗ B = [a1 ⊗ b1; a2 ⊗ b2; . . . an ⊗ bn] ,

(cid:17) ◦(cid:16)

(cid:17)

T(cid:62)
•,QAT•,QA

T(cid:62)
•,RAT•,RA

,

where the ai (resp. bi) are the columns of A (resp. B). Using (25) of Theorem
2 in [13], we get that

(T ⊗ T)

(cid:16)
n − (rak ∨ ra(cid:96)). By (19), (cid:0)X (cid:62)X(cid:1)

(cid:62)
•,A(T ⊗ T)•,A =

where ◦ denotes the Hadamard or entry-wise product. Observe that by
deﬁnition of T, (T(cid:62)
•,RAT•,RA)k,(cid:96) =
A,A is a Gram matrix which is positive
and deﬁnite since the vectors T•,qa1+1 ⊗ T•,ra1+1, T•,qa2+1 ⊗ T•,ra2+1, . . . ,
T•,qaK +1 ⊗ T•,raK +1 are linearly independent.

•,QAT•,QA)k,(cid:96) = n − (qak ∨ qa(cid:96)) and (T(cid:62)

Proofof Lemma 6. The operations of adding/removing a column to a Cholesky
factorization are classical and well treated in books of numerical analysis,
see e.g. [7]. An advantage of our settings is that there is no additional com-
putational cost for computing X (cid:62)X(cid:5)j when entering a new variable j thanks
to the closed-form expression (9).

References

[1] F. Bach, R. Jenatton, J. Mairal, and G. Obozinski. Optimization
with sparsity-inducing penalties. Foundations and Trends R(cid:13) in Machine
Learning, 4(1):1–106, 2012.

[2] R. Bellman. On the approximation of curves by line segments using
dynamic programming. Commun. ACM, 4(6):284–, 1961. ISSN 0001-
0782. doi: 10.1145/366573.366611.

[3] L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone.
Statistics/Probability Series.

Classiﬁcation and Regression Trees.
Wadsworth Publishing Company, Belmont, California, U.S.A., 1984.

33

[4] J. R. Dixon, S. Selvaraj, F. Yue, A. Kim, Y. Li, Y. Shen, M. Hu, J. S.
Liu, and B. Ren. Topological domains in mammalian genomes identiﬁed
by analysis of chromatin interactions. Nature, 485(7398):376–380, 2012.

[5] B. Efron, T. Hastie, I. Johnstone, R. Tibshirani, et al. Least angle

regression. The Annals of statistics, 32(2):407–499, 2004.

[6] W. D. Fisher. On grouping for maximum homogeneity. Journal of
ISSN

the American Statistical Association, 53(284):789–798, 1958.
01621459.

[7] G. H. Golub and C. F. Van Loan. Matrix computations. JHU Press,

2012. 3rd edition.

[8] Z. Harchaoui and C. L´evy-Leduc. Multiple change-point estimation
with a total variation penalty. Journal of the American Statistical
Association, 105(492):1480–1493, 2010.

[9] H. Hoeﬂing. A path algorithm for the fused lasso signal approximator.

J. Comput. Graph. Statist., 19(4):984–1006, 2010.

[10] S. Kay. Fundamentals of statistical signal processing: detection theory.

Prentice-Hall, Inc., 1993.

[11] C. L´evy-Leduc, M. Delattre, T. Mary-Huard, and S. Robin. Two-
dimensional segmentation for analyzing hi-c data. Bioinformatics, 30
(17):i386–i392, 2014.

[12] E. Lieberman-Aiden, N. L. Van Berkum, L. Williams, M. Imakaev,
T. Ragoczy, A. Telling, I. Amit, B. R. Lajoie, P. J. Sabo, M. O.
Dorschner, et al. Comprehensive mapping of long-range interactions
reveals folding principles of the human genome.
science, 326(5950):
289–293, 2009.

[13] S. Liu and G. Trenkler. Hadamard, khatri-rao, kronecker and other

matrix products. Int. J. Inform. Syst. Sci., 4:160–177, 2008.

[14] R. Maidstone, T. Hocking, G. Rigaill, and P. Fearnhead. On opti-
mal multiple changepoint algorithms for large data. Statistics and
Computing, pages 1–15, 2016.
10.1007/
s11222-016-9636-3.

ISSN 1573-1375.

doi:

[15] N. Meinshausen and P. B¨uhlmann. Stability selection. Journal of the
Royal Statistical Society: Series B (Statistical Methodology), 72(4):
417–473, 2010.

34

[16] M. R. Osborne, B. Presnell, and B. A. Turlach. A new approach to
variable selection in least squares problems. IMA journal of numerical
analysis, 20(3):389–403, 2000.

[17] R Core Team. R: A Language and Environment for Statistical
Computing. R Foundation for Statistical Computing, Vienna, Austria,
2015. URL http://www.R-project.org/.

[18] C. Sanderson. Armadillo: An open source C++ linear algebra library
for fast prototyping and computationally intensive experiments. Tech-
nical report, NICTA, 2010.

[19] R. J. Tibshirani and J. Taylor. The solution path of the generalized

lasso. Ann. Statist., 39(3):1335–1371, 2011.

[20] J.-P. Vert and K. Bleakley. Fast detection of multiple change-points
In Advances in Neural

shared by many signals using group lars.
Information Processing Systems, pages 2343–2351, 2010.

35

