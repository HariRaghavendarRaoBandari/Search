Beyond Sharing Weights for Deep Domain Adaptation

Artem Rozantsev, Mathieu Salzmann, Pascal Fua

{firstname.lastname}@epfl.ch

Computer Vision Laboratory, ´Ecole Polytechnique F´ed´erale de Lausanne, Lausanne,

Switzerland

Abstract. Deep Neural Networks have demonstrated outstanding performance
in many Computer Vision tasks but typically require large amounts of labeled
training data to achieve it. This is a serious limitation when such data is difﬁcult
to obtain. In traditional Machine Learning, Domain Adaptation is an approach to
overcoming this problem by leveraging annotated data from a source domain, in
which it is abundant, to train a classiﬁer to operate in a target domain, in which
labeled data is either sparse or even lacking altogether. In the Deep Learning case,
most existing methods use the same architecture with the same weights for both
source and target data, which essentially amounts to learning domain invariant
features. Here, we show that it is more effective to explicitly model the shift from
one domain to the other. To this end, we introduce a two-stream architecture, one
of which operates in the source domain and the other in the target domain. In
contrast to other approaches, the weights in corresponding layers are related but
not shared to account for differences between the two domains. We demonstrate
that this both yields higher accuracy than state-of-the-art methods on several ob-
ject recognition and detection tasks and consistently outperforms networks with
shared weights in both supervised and unsupervised settings.

Keywords: Domain adaptation, object detection/recognition, deep learning.

6
1
0
2

 
r
a

 

M
1
2

 
 
]

V
C
.
s
c
[
 
 

1
v
2
3
4
6
0

.

3
0
6
1
:
v
i
X
r
a

1 Introduction

Deep Neural Networks [1, 2] have emerged as powerful tools that outperform traditional
Computer Vision algorithms in a wide variety of tasks, but only when sufﬁciently large
amounts of training data are available. This is a severe limitation in ﬁelds in which
obtaining such data is either difﬁcult or expensive. For example, this work was initially
motivated by our need to detect drones against complicated backgrounds with a view
to developing anti-collision systems. Because the set of possible backgrounds is nearly
inﬁnite, creating an extensive enough training database of representative real images
proved to be very challenging.

Domain Adaptation [3] and Transfer Learning [4] have long been used to overcome
this difﬁculty by making it possible to exploit what has been learned in one speciﬁc
domain, for which enough training data is available, to effectively train classiﬁers in a
related but different domain, where only very small amounts of additional annotations,
or even none, can be acquired. Following the terminology of Domain Adaptation, we
will refer to the domain in which enough annotated data is available as the source do-
main and the one with only limited amounts of such data, or none at all, as the target

2

Artem Rozantsev, Mathieu Salzmann, Pascal Fua

Fig. 1. Our two-stream architecture. One stream operates on the source data and the other on
the target one. Their weights are not shared. Instead, we introduce loss functions that prevent
corresponding weights from being too different from each other.

domain. In the drone case discussed above, the source domain can comprise synthetic
images of drones, which can be created in arbitrary quantities, and the target domain
a relatively small number of annotated real images. Again as in the domain adaptation
literature, we will refer to this scenario as the supervised one. An even more difﬁcult
situation arises when there is absolutely no annotated data in the target domain. We will
refer to this as the unsupervised scenario. In this paper, we will investigate both of the
aforementioned scenarios.

Recently, Domain Adaptation has been investigated in the context of Deep Learn-
ing with promising results. The simplest approach involves ﬁne-tuning a Convolutional
Neural Network (CNN) pre-trained on the source data using labeled target samples [5,
6]. This, however, results in overﬁtting when too little target data is available. Further-
more, it is not applicable in the unsupervised case. To overcome these limitations, other
works have focused on using both source and target samples to learn a network where
the features for both domains, or their distributions, are related via an additional loss
term [7–10]. To the best of our knowledge, all such methods use the same deep archi-
tecture with the same weights for both source and target domains. As such, they can be
understood as attempts to make the features invariant to the domain shift, that is, the
changes from one domain to the other.

In this paper, we postulate that imposing feature invariance might be detrimental
to discriminative power. To verify this hypothesis, we introduce a different approach
that explicitly models the domain shift by allowing the CNN weights to compensate for
it. To this end, we rely on the two-stream architecture depicted by Fig. 1. One stream
operates on the source domain and the other on the target one. Their weights are not
shared. Instead, we introduce a loss function that is lowest when they are linear trans-
formations of each other. In short, our approach models the domain shift by learning
features adapted to each domain, but not fully independent, to account for the fact that
both domains depict the same object categories.

Motivated by our drone detection application, we demonstrate the beneﬁts of our
approach on a Unmanned Aerial Vehicles (UAV) image dataset, consisting of a large
amount of synthetic data and a small amount of real images. We show that our ap-
proach lets us successfully leverage the synthetic data. In particular, we can reach the
same level of accuracy as a model trained on a large set of real data by using only 5-10%
of real data in conjunction with synthetic examples. This could have a huge impact in
many research areas, such as human pose estimation, where labeling new data is time-
consuming and costly, but generating synthetic examples is relatively easy. Further-
more, to demonstrate the generality of our approach, we also apply it to two standard
benchmark Domain Adaptation datasets, covering both the supervised and unsupervised
scenarios and different network architectures. Our non-shared weight approach consis-

Beyond Sharing Weights for Deep Domain Adaptation

3

tently outperforms those that share all the weights and improves on the state-of-the-art
in all cases.

2 Related work

In many practical applications, classiﬁers and regressors may have to operate on many
kinds of related but visually different image data. The differences are often large enough
for an algorithm that has been trained on one kind of images to perform badly on an-
other. Therefore, new training data has to be acquired and annotated to retrain it. Since
this is typically expensive and time-consuming, there has long been a push to develop
Domain Adaptation techniques that allow re-training with minimal amounts of new data
or even none. We review here brieﬂy some recent trends, with a focus on Deep Learning
based methods, which are most related to our work.

A natural approach to Domain Adaptation is to modify a classiﬁer trained on the
source data using the available labeled target data. This was done, for example, using
SVM [11, 12], Boosted Decision Trees [13] and other classiﬁers [14]. In the context of
Deep Learning, ﬁne-tuning [5, 6] essentially follows this pattern. In practice, however,
when little labeled target data is available, this often results in overﬁtting.

Another approach is to learn a metric between the source and target data, which
can also be interpreted as a linear cross-domain transformation [15] or a non-linear
one [16]. Instead of working on the samples directly, several methods involve repre-
senting each domain as one separate subspace [17–19]. A transformation can then be
learned to align them [19]. Alternatively, one can interpolate between the source and
target subspaces [17, 18]. In [20], this interpolation idea was extended to Deep Learn-
ing by training multiple unsupervised networks with increasing amounts of target data.
The ﬁnal representation of a sample was obtained by concatenating all intermediate
ones. It is unclear, however, why this concatenation should be meaningful to classify a
target sample.

Another way to handle the domain shift is to explicitly try making the source and
target data distribution similar. While many metrics have been proposed to quantify the
similarity between two distributions, the most widely used in the Domain Adaptation
context is the Maximum Mean Discrepancy (MMD) [21]. The MMD has been used to
re-weight [22, 23] or select [24] source samples such that the resulting distribution be-
comes as similar as possible to the target one. An alternative is to learn a transformation
of the data, typically both source and target, such that the resulting distributions are
as similar as possible in MMD terms [25–27]. In [28], MMD was used within a shal-
low neural network architecture. However, this method relied on SURF features [29] as
initial image representation and thus only achieved limited accuracy.

Recently, using Deep Networks to learn features has proven effective at increas-
ing the accuracy of Domain Adaptation methods. In [30], it was shown that using
DeCAF features instead of hand-crafted ones mitigates the Domain Shift effects even
without performing any kind of adaptation. However, performing adaptation within a
Deep Learning framework was shown to boost accuracy further [31, 7–10]. For exam-
ple, in [31], a Siamese architecture was introduced to minimize the distance between
pairs of source and target samples, which requires training labels available in the tar-
get domain thus making the method unsuitable for unsupervised Domain Adaptation.

4

Artem Rozantsev, Mathieu Salzmann, Pascal Fua

The MMD has also been used to relate the source and target data representations [7,
8] thus making it possible to avoid working on individual samples. In [9, 10], a loss
term that encodes an additional classiﬁer predicting from which domain each sample
comes was introduced. This was motivated by the fact that, if the learned features are
domain-invariant, such a classiﬁer should exhibit very poor performance.

All these Deep Learning approaches rely on the same architecture with the same
weights for both the source and target domains. In essence, they attempt to reduce the
impact of the domain shift by learning domain-invariant features. In practice, however,
domain invariance might very well be detrimental to discriminative power. As discussed
in the introduction, this is the hypothesis we set out to test in this work by introducing
an approach that explicitly models the domain shift instead of attempting to enforce
invariance to it. We will see in the results section that this yields a signiﬁcant accuracy
boost over networks with shared weights.

3 Our Approach
In this section, we introduce our Deep Learning approach to Domain Adaptation. At
its heart lies the idea that, for the model to adapt to different domains, the network
weights should be related, yet different for each of the two domains. This constitutes a
major difference between our approach and the competing ones discussed in Section 2.
To implement this idea, we therefore introduce a two-stream architecture, such as the
one depicted by Fig. 1. The ﬁrst stream operates on the source data, the second on the
target one, and they are trained jointly. While we allow the weights of the corresponding
layers to differ between the two streams, we prevent them from being too far from of
each other by introducing appropriately designed loss functions. Additionally we use
the MMD between the learned source and target representations. This combination lets
us encode the fact that, while different, the two domains are related.
i}N t
i=1 be the sets of training images
from the source and target domains, respectively, with Y s = {ys
i } and Y t = {yt
i}
being the corresponding labels. To handle unsupervised target data as well, we assume,
without loss of generality, that the target samples are ordered, such that only the ﬁrst
l = 0 in the unsupervised scenario. Furthermore,
N t
let θs
j denote the parameters, that is, the weights and biases, of the jth layer of
the source and target streams, respectively. We train the network by minimizing a loss
function of the form

More formally, let Xs = {xs

l ones have valid labels, where N t

i}N s
i=1 and Xt = {xt

j and θt

L(θs, θt|Xs, Y s, Xt, Y t) = Ls + Lt + Lw + LMM D,

(1)

(2)

(3)

(4)

(5)

Ls =

Lt =

1
N s

1
N t
l

c(θs|xs

i , ys
i )

c(θt|xt

i, yt
i )

i=1

N s(cid:88)
l(cid:88)
(cid:88)

i=1

N t

j∈Ω

Lw = λw

rw(θs

j , θt
j)

LMM D = λuru(θs, θt|Xs, Xt) ,

Beyond Sharing Weights for Deep Domain Adaptation

5

i, y·

where c(θ·|x·
i) is a standard classiﬁcation loss, such as the logistic loss or the hinge
loss. rw(·) and ru(·) are the weight and unsupervised regularizers discussed below. The
ﬁrst one represents the loss between corresponding layers of the two streams. The sec-
ond encodes the MMD measure and favors similar distributions of representations of
source and target data. These regularizers are weighted by coefﬁcients λw and λu, re-
spectively. In practice, we found our approach to be robust to the speciﬁc values of these
coefﬁcients and we set them to 1 in all our experiments. Ω denotes the set of indices
of the layers whose parameters are not shared. This set is problem-dependent and, in
practice, can be obtained from validation data, as demonstrated in our experiments.

Weight regularizer. While our goal is to go beyond sharing the layer weights, we still
believe that corresponding weights in the two streams should be related. This models the
fact that the source and target domains are related, and prevents overﬁtting in the target
stream, when only very few labeled samples are available. Our weight regularizer rw(·)
therefore represents the distance between the source and target weights in a particular
. This, however, would
penalize even small differences between the weights and makes the whole system too
rigid. To add more ﬂexibility, we therefore use the exponential loss function and write

layer. The simplest choice would be the L2 norm(cid:13)(cid:13)θs

2

j) = exp(cid:0)(cid:107)θs

j − θt

rw(θs

j , θt

j − θt

(cid:13)(cid:13)2
j(cid:107)2(cid:1) − 1 .

j

(6)

j) = exp(cid:0)(cid:107)ajθs

While the exponential loss of Eq. 6 gives more ﬂexibility than the L2 loss, it still
tends to keep the weights of both streams very close to each other, which might be too
restrictive when the domains differ signiﬁcantly. We therefore propose to further relax
this prior by allowing the weights in one stream to undergo a linear transformation. We
express this as

j , θt

rw(θs

(7)
where aj and bj are scalar parameters that encode the linear transformation. These pa-
rameters are different for each layer j ∈ Ω and are learned at training time together
with all other network parameters. While simple, this approach lets us model parame-
ter transformations such as those depicted by Fig. 2 and has proven to be effective in
practice. For example, this parametrization can account for global illumination changes
in the ﬁrst layer of the network. We have tried replacing the simple linear transforma-
tion of Eq. 7 by more sophisticated ones, such as quadratic or more complicated linear
transformations. However, this has not resulted in any performance improvement.

j(cid:107)2(cid:1) − 1 ,

j + bj − θt

Unsupervised regularizer. In addition to regularizing the weights of corresponding
layers in the two streams, we also aim at learning a ﬁnal representation, that is, the
features before the classiﬁer layer, that is domain invariant. To this end, we introduce a
regularizer ru(·) designed to minimize the distance between the distributions of repre-
sentations of the source and target data. Following the popular trend in Domain Adap-
tation [32, 7], we rely on the Maximum Mean Discrepancy (MMD) [21] to encode this
distance.

As the name suggests, given two sets of data, the MMD measures the distance be-
tween the mean of the two sets after mapping each sample to a Reproducing Kernel

6

Artem Rozantsev, Mathieu Salzmann, Pascal Fua

ﬁlters

biases

ﬁlters

biases

(a)

(b)

Fig. 2. Correlation between weights of the corresponding ﬁrst convolutional layer of CNNt and
CNNs, regularized by L2 (a) and exponential norm, deﬁned by Eq. 7 (b). The red dots denote
the correlation between the corresponding layer parameters. The green line shows the initial esti-
mate for the linear transformation and the blue line illustrates its ﬁnal estimate after the training
process. (best seen in color)

Hilbert Space (RKHS). In our context, let f s
i ) be the feature representation
at the last layer of the source stream, and similarly for the target stream. The squared
MMD between the source and target domains can be expressed as

i (θs, xs

i = f s

MMD2({f s

i },{f t

j}) =

φ(f s

i ) − 1
N t

φ(f t
j )

,

(8)

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 1

N s

N s(cid:88)

i=1

N t(cid:88)

j=1

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2

where φ(·) denotes the mapping to RKHS. In practice, this mapping is typically un-
known. However, expanding Eq. 8 yields

MMD2({f s

i },{f t

i }) =

φ(f s

i )T φ(f s
i(cid:48) )
(N s)2

− 2

φ(f s

i )T φ(f t
j )
N sN t

+

φ(f t

j(cid:48) )

j )T φ(f t
(N t)2

.

(9)

By using the kernel trick, the inner products can be replaced by kernel values. This lets
us rewrite our regularizer as

ru(θs, θt|Xs, Xt) =

k(f s

i , f s
(N s)2 − 2
i(cid:48) )

i , f t
j )

k(f s
N sN t +

j(cid:48) )

k(f t

j , f t
(N t)2

,

(10)

i,i(cid:48)

j,j(cid:48)
is, and where k(·,·)
where the dependency on the network parameters comes via the f·
is a kernel function. In practice, we make use of the standard RBF kernel k(u, v) =
exp (−(cid:107)u − v(cid:107)2/σ2), with σ2 being the bandwidth of the kernel. In all our experiments,
we found our approach to be insensitive to the choice of σ2 and we therefore set it to 1.

i,j

(cid:88)

i,i(cid:48)

(cid:88)

(cid:88)

i,j

(cid:88)

(cid:88)

j,j(cid:48)

(cid:88)

Training. To learn the model parameters, we ﬁrst pre-train the source stream using
the source data only. We then simultaneously optimize the weights of both streams ac-
cording to the loss of Eqs. 2-5 using both source and target data, with the target stream
weights initialized from the pre-trained source weights. Note that this also requires ini-
tializing the linear transformation parameters of each layer, aj and bj for all j ∈ Ω. We
initialize these values to aj = 1 and bj = 0, thus encoding the identity transformation.
All parameters are then learned jointly using backpropagation with the AdaDelta algo-
rithm [33]. Note that we rely on mini-batches, and thus in practice use all the terms of
our loss over these mini-batches rather than over the entire source and target datasets.

Beyond Sharing Weights for Deep Domain Adaptation

7

Depending on the task, we use different network architectures, to provide a fair
comparison with the baselines. For example, for digit classiﬁcation we rely on the stan-
dard network structure of [34] for each stream, and, for the Ofﬁce benchmark, we adopt
the AlexNet [35] architecture, as was done in [7].

4 Experimental Results

In this section, we show the beneﬁts of our approach in both the supervised and un-
supervised scenarios using different network architectures. Since our motivating appli-
cation was drone detection, we ﬁrst thoroughly evaluate our method for this task. We
then demonstrate that it generalizes well to other problems by testing it on standard
benchmark datasets.

4.1 Leveraging Synthetic Data for Drone Detection

As drones and UAVs become ever more numerous in our skies, it will become increas-
ingly important for them to see and avoid colliding with each other. Unfortunately,
training videos are scarce and do not cover a wide enough range of possible shapes,
poses, lighting conditions, and backgrounds against which they can be seen. However,
it is relatively easy to generate large amounts of synthetic examples, which can be used
to supplement a small number of real images and increase the detection accuracy [36].
We show here that our approach allows us to exploit these synthetic images more effec-
tively than other state-of-the-art Domain Adaptation techniques.

Dataset and Evaluation Setup. We used the approach of [36] to create a large set of
synthetic examples. We also collected three sets of real images, such as those depicted
by Fig. 3, which we used for training, validation, and testing, respectively. In our exper-
iments, we treat the synthetic images as samples from the source domain and the real
ones as samples from the target domain.

We report results using two versions of this dataset, which we refer to as UAV-
200 (small) and UAV-200 (full). Their sizes are given in Table 1. They only differ in the
number of synthetic and negative samples used at training and testing time. The ratio
of positive to negative samples in the ﬁrst dataset is more balanced than in the second
one. For UAV-200 (small), we therefore express our results in terms of accuracy, which is
commonly used in Domain Adaptation and can be computed as

Accuracy =

# correctly classiﬁed examples

# all examples

.

(11)

Using this standard metric facilitates the comparison against the baseline methods whose
publicly available implementations only output classiﬁcation accuracy.

In real detection tasks, however, the training datasets are typically quite unbal-
anced, since one usually encounters many negative windows for each positive one. UAV-
200 (full) reﬂects this more realistic scenario, in which the accuracy metric is poorly-
suited. For this dataset, we therefore compare various approaches in terms of precision-
recall. Precision corresponds to the number of true positives detected by the algorithm

8

Artem Rozantsev, Mathieu Salzmann, Pascal Fua

Synthetic

Real

s
e
v
i
t
i
s
o
p

s
e
v
i
t
a
g
e
n

s
e
v
i
t
i
s
o
p

s
e
v
i
t
a
g
e
n

Test real data

Fig. 3. Our UAV dataset. Top: Synthetic and real training examples. Bottom: Real samples from
the test dataset.

Dataset

Training

Pos

(Real)

(Synthetic)

Validation

Testing

Neg
(Real)

Pos
(Real)

Neg
(Real)

Pos
(Real)

Neg
(Real)

UAV-200 (full)
UAV-200 (small)
Table 1. Statistics of our two UAV datasets. Note that UAV-200 (small) is more balanced than
UAV-200 (full).

135000
6750

1500
1500

3100
3100

200
200

32800
1640

190000
9500

500
500

Precision (AveP), which is computed as(cid:82) 1

divided by the total number of detections. Recall is the number of true positives divided
by the number of test examples labeled as positive. Additionally, we report the Average
0 p(r)dr, where p and r denote precision and

recall, respectively.

For both datasets, we follow the supervised Domain Adaptation scenario. In other
words, training data is available with labels for both source and target domains. Testing
is then done on the images from the test data, as described in Table 1.

Network Design. Since drone detection is a relatively new problem, there is no gen-
erally accepted network architecture, and we had to design our own. As illustrated by
Fig. 1, our network consists of two streams, one for the synthetic data and one for the
real data. Each stream is a CNN that comprises three convolutional and max-pooling
layers, followed by two fully-connected ones. The classiﬁcation layer encodes a hinge
loss, which was shown to outperform the logistic loss in practice [37, 38].

Some pairs of layers in the two streams share their weights while others do not. To
identify what the optimal arrangement should be, as well as which loss function should

Beyond Sharing Weights for Deep Domain Adaptation

9

Fig. 4. Evaluation of the best network architecture. The x-axis denotes the network conﬁgu-
ration, where a ‘+’ sign indicates that the corresponding network layers are regularized with a
loss function and a ‘−’ sign that the weights are shared for the corresponding layers. We show
the results of applying different loss functions: L2 norm (red circles) and exponential loss Eq. 7
(blue triangles). These experiments were performed on the validation data of UAV-200 (full). (best
seen in color)

be employed between the non-shared layers, we trained different models corresponding
to all possible combinations and evaluated them on the validation data. Fig. 4 depicts the
results. In this ﬁgure, the + and − signs indicate whether the weights are stream-speciﬁc
or shared. We can see that using the exponential loss to connect the layers that do not
share their weights typically yields better accuracy then using the L2 norm. The best
performance overall is obtained when the ﬁrst two convolutional layers are connected
by an exponential loss function and the others share their weights. Our understanding
is that, even though the synthetic and real images feature the same objects, they differ
in appearance, which is mostly encoded by the ﬁrst network layers. Thus, allowing the
weights of the streams to differ in these layers lets each network better adapt to its
domain. In any event, we used this architecture in the remainder of our drone detection
experiments.

Evaluation. We ﬁrst compare our approach to other Domain Adaptation methods on
UAV-200 (small). As can be seen in Table 2, it signiﬁcantly outperforms many state-
of-the-art baselines in terms of accuracy. In particular, we believe that outperforming
DDC [7] goes a long way towards validating our hypothesis that modeling the domain
shift in more effective than trying to be invariant to it. This is because, as discussed
in Section 2, DDC relies on minimizing the MMD loss between the learned source
and target representations much as we do, but uses a single stream for both source and
target data. In other words, except for the non-shared weights, it is the method closest
to ours. Note, however, that the original DDC paper used a slightly different network
architecture than ours. To avoid any bias, we therefore modiﬁed this architecture so it
matches ours in our own DDC implementation.

We then turn to the complete dataset UAV-200 (full). In this case, the baselines whose
implementations only output accuracy values become less relevant because it is not a
good metric for unbalanced data. We therefore compare our approach against DDC [7],
which we found to be our strongest competitor in the previous experiment, and against

10

Artem Rozantsev, Mathieu Salzmann, Pascal Fua

ITML [15]
ARC-t assymetric [16]
ARC-t symmetric [16]
HFA [39]
DDC [7]
Ours

Accuracy

0.60
0.55
0.60
0.75
0.89
0.92

Table 2. Comparison to other domain adaptation techniques on the UAV-200 (small) dataset.

CNN (trained on Synthetic only (S))
CNN (trained on Real only (R))
CNN (pre-trained on S and ﬁne-tuned on R):

Loss: Lt
Loss: Lt + Lw (with ﬁxed source CNN)
CNN (pre-trained on S and ﬁne-tuned on R and S:)

Loss: Ls + Lt [36]

DDC [7] (pre-trained on S and ﬁne-tuned on R and S)
Our approach (pre-trained on S and ﬁne-tuned on R and S)

Loss: Ls + Lt + Lw
Loss: Ls + Lt + LMM D
Loss: Ls + Lt + Lw + LMM D

AveP

(Average Precision)

0.314
0.575

0.612
0.655

0.569
0.664

0.673
0.711
0.757

Table 3. Comparing our method against baselines approaches on the UAV-200 (full) dataset. As
discussed in Section 3, the terms Ls, Lt, Lw, and LMM D correspond to the elements of the loss
function, deﬁned in Eqs. 2, 3, 4, 5, respectively.

the Deep Learning approach of [36], which also tackled the drone detection problem.
We also turn on and off some of our loss terms to quantify their inﬂuence on ﬁnal
performance. We give the results in Table 3. In short, all the loss terms contribute
to improving the AveP of our approach, which itself outperforms all the baselines by
large margins. More speciﬁcally, we get a 10% boost over DDC and a 20% boost over
using real data only. By contrast, simply using real and synthetic examples together,
as was done in [36], does not really yield much improvement. Interestingly, allowing
the weights of both streams to be independent while the outputs of their last fully-
connected layers are regularized with an MMD loss (Our approach: Loss: Ls + Lt +
LMM D) achieves lower accuracy than our approach, which regularizes the weights of
both streams. We attribute this to overﬁtting of the target stream to the target data.

Inﬂuence of the Number of Real and Synthetic Samples. Using synthetic data in
the UAV detection scenario is motivated by the fact that it is hard and time consuming
to collect large amounts of real data. We therefore evaluate the inﬂuence of the ratio
of synthetic to real data. To this end, we ﬁrst ﬁx the number of synthetic samples to
32800, as in UAV-200 (full) and vary the amount of real positive samples from 200 to
5000. The results of this experiments are reported in Fig. 5(a), where we again compare

Beyond Sharing Weights for Deep Domain Adaptation

11

(a)

(b)

Fig. 5. Inﬂuence of the ratio of synthetic to real data. (a) AveP of our approach (violet stars),
DDC (blue triangles), and training using real data only (red circles) as a function of the number
of real samples used given a constant number of synthetic ones. (b) AveP of our approach (violet
stars) and DDC (blue triangles) as a function of the number of synthetic examples used given a
small and constant number of real one. (best seen in color)

our approach to DDC [7] and to the same CNN model trained on the real samples
only. Our model always outperforms the one trained on real data only. This suggests
that it remains capable of leveraging the synthetic data, even though more real data is
available, which is not the case of DDC. More importantly, looking at the leftmost point
on our curve shows that, with only 200 real samples, our approach performs similarly to,
and even slightly better than, a model trained using 2500 real samples. In other words,
one only needs to collect 5-10% of labeled training data to obtain good results with our
approach, which, we believe, can have a signiﬁcant impact in practical applications.

Fig. 5(b) depicts the results of a second experiment in which we ﬁxed the number of
real samples to 200 and increased the number of synthetic ones from 0 to 32800. Note
that the AveP of our approach steadily increases as more synthetic data is used. DDC
also improves but we systematically outperform it except when we use no synthetic
samples, in which case both approaches reduce to a CNN trained on real data only.

4.2 Evaluation on Standard Classiﬁcation Benchmarks
To demonstrate the generality of our approach, we further evaluate it on two standard
domain adaptation benchmarks for image classiﬁcation, one to test the supervised sce-
nario and the other the unsupervised one. Following standard practice, we express our
results in terms of accuracy, as deﬁned in Eq. 11.
Supervised domain adaptation using the Ofﬁce dataset. The Ofﬁce dataset [15]
comprises three different sets of images (Amazon, DSLR, Webcam) featuring 31 classes
of objects. Fig. 6 depicts some images from the three different domains of the Ofﬁce
dataset. For our experiments, we used the evaluation protocol proposed in [15], which
corresponds to the supervised scenario. Speciﬁcally, following [15], we used the labels
of 20 randomly sampled images for each class for the Amazon domain and 8 labeled
images per class for the DLSR and Webcam domains, when used as source datasets.
For the target domain, we only used 3 randomly selected labeled images per class. The
rest of the dataset, however, was used as unlabeled data for the calculation of the MMD
loss of Eq. 5.

Fig. 7(a) illustrates the network architecture we used for this experiment. Each
stream corresponds to the standard AlexNet CNN [35] with the additional adaptation

12

Artem Rozantsev, Mathieu Salzmann, Pascal Fua

n
o
z
a
m
A

m
a
c
b
e
W

R
L
S
D

Fig. 6. Some examples from three domains in the Ofﬁce dataset.

(a)

(b)

Fig. 7. Ofﬁce dataset. (a) Our network architecture that proved to be best based on experiments on
the validation set. (b) Several experiments on the validation set. On the x-axis we brieﬂy describe
the conﬁguration that we use. For each layer ‘+’ denotes that the weights of the network are not
shared, while ‘-’ means that corresponding layers share weights.
layers proposed in the DDC model of [7]. As in [7], we start with the pre-trained model
and ﬁne tune it. However, instead of forcing the weights of both streams to be shared, we
allow them to deviate from each other and regularize this deviation using the loss func-
tion introduced in Section 3. To identify which layers should not share their weights and
which ones should, we used a validation set consisting of 5 examples per object category
in the Amazon → Webcam domain adaptation task. Fig. 7(b) depicts some of the results
of this validation procedure. As we can see, not sharing the last two fully-connected
layers achieves the highest accuracy on this subset. Note that we only performed this
validation on the Amazon → Webcam task and used the same architecture for the other
tasks reported below.

In Table 4, we compare our approach against other Domain Adaptation techniques
on the three commonly-reported source/target pairs. It outperforms them on almost
all pairs. More importantly, the comparison against DDC conﬁrms that allowing the
weights not to be shared increases accuracy.

Unsupervised domain adaptation using MNIST-USPS. The MNIST [34] and USPS [41]
datasets for digit classiﬁcation both feature 10 different classes of images correspond-
ing to the 10 digits. They have recently been employed for the task of Domain Adapta-
tion [42].

For this experiment, we used the evaluation protocol of [42], which involves ran-
domly selecting of 2000 images from MNIST and 1800 images from USPS and using
them interchangeably as source and target domains. As in [42], we work in the unsuper-

Beyond Sharing Weights for Deep Domain Adaptation

13

Accuracy

GFK [18]
SA [19]
DA-NBNN [40]
DLID [20]
DeCAF6+T [30]
DaNN [28]
DDC [7]
Ours
Table 4. Comparison to other domain adaptation techniques on the Ofﬁce standard benchmark.
We evaluate on all 31 categories, according to the standard evaluation protocol, described in [15].

A → W D → W W → D Average
0.530
0.464
0.599
0.450
0.685
0.528
0.519
0.733
0.807
0.536
0.841
0.876

0.613
0.648
0.766
0.782
0.948
0.712
0.954
0.949

0.694
0.919
0.938

0.663
0.699
0.762
0.899

-

0.835
0.963
0.988

-

(a)

(b)

Fig. 8. Architecture for the MNIST-USPS dataset. (a) Best CNN architecture based on our
validation procedure. (b) Results of some of our validation experiments to determine, which
layers should not share weights and instead be connected by a regularization loss. The x-axis
denotes the network conﬁguration, where ‘+’ denotes that the weights of a particular layer are
not shared, while ‘-’ means that they are.

vised setting, and thus ignore the target domain labels at training time. Following [32],
as the image patches in the USPS dataset are only 16 × 16 pixels, we rescaled the im-
ages from MNIST to the same size and applied L2 normalization of the gray-scale pixel
intensities. For our tests, we rely on the standard CNN architecture of [34] and followed
a similar validation procedure to that used for the previous two datasets to determine
which layers should not share their weights. Speciﬁcally, as for the Ofﬁce dataset, since
there is no explicit validation set, we randomly sampled 5 images for each class from
the target domain and used them to select the best network architecture. We found that
allowing all layers of the network not to share their weights yielded the best perfor-
mance. Fig. 8(a) depicts the ﬁnal structure of the CNN. In Fig. 8(b), we provide the
results of some of the validation experiments.

In Table 5, we compare our approach against other Domain Adaptation techniques.
Our approach outperforms the baselines by a large margin. Overall we have compared
our approach with various methods [30, 28, 7], which involve using deep architectures
to extract domain invariant representations for both source and target domains. We fur-
ther evaluated our approach with respect to methods [43, 18, 19, 42] that do not use deep
networks. In all cases we believe that our method shows superior performance due to
its ability to adapt the feature representation to each domain, while still keeping these
representations close to each other.

14

Artem Rozantsev, Mathieu Salzmann, Pascal Fua

Accuracy

NA

PCA

SA [19] GFK [18] TCA [43] SSTCA [43] TSL [44] JCSL [42] DDC [7] Ours
method
M→U
0.478 0.607
U→M
0.631 0.673
0.554 0.640
AVG.
Table 5. Comparison against other domain adaptation techniques on the MNIST+USPS standard
benchmark.

0.454 0.451 0.486
0.333 0.334 0.222
0.394 0.392 0.354

0.408
0.274
0.341

0.406
0.222
0.314

0.346
0.226
0.286

0.435
0.341
0.388

0.467
0.355
0.411

4.3 Discussion

In all the experiments reported above, allowing the weights not to be shared in at least
some of the layers of our two-stream architecture boosts performance. This validates
our initial hypothesis that explicitly modeling the domain shift is generally beneﬁcial.
However the optimal choice of which layers should or should not share their weights
is application dependent. In the UAV case, allowing the weights in the ﬁrst two layers to
be different yields top performance, which we understand to mean that the domain shift
is caused by low-level changes that are best handled in the early layers. By contrast,
for the Ofﬁce dataset, it is best to only allow the weights in the last two layers to differ.
This network conﬁguration was determined using Amazon and Webcam images, such
as those shown in Fig. 6. Close examination of these images reveals that the differences
between them are not simply due to low-level phenomena, such as illumination changes,
but to more complex variations. It therefore seems reasonable that the higher layers of
the network should be domain-speciﬁc, since they typically encode this type of high-
level information. Finally, for MNIST+USPS, it is optimal not to share any weights,
which we take to be a consequence of the networks being relatively small and therefore
requiring adaptation of all the parameters to provide sufﬁcient ﬂexibility.

Fortunately, we have shown that it is possible to use validation data to decide which

conﬁguration is best, which makes our two-steam approach a practical one.

5 Conclusion

In this paper, we have postulated that Deep Learning approaches to Domain Adapta-
tion should not focus on learning features that are invariant to the domain shift, which
makes them less discriminative. Instead we should explicitly model the domain shift.
To prove this, we have introduced a two-stream CNN architecture, where the weights
of the streams are not shared. To nonetheless encode the fact that both streams should
be related, we encourage these weights to remain close to being linear transformations
of each other by introducing an additional term in the loss function.

Our experiments have clearly validated our hypothesis. Our approach consistently
yields higher accuracy than networks that share all weights for the source and target
data. Furthermore, we have outperformed the state-of-the-art on three challenging ob-
ject detection and recognition datasets involving both supervised and unsupervised sce-
narios. In the future, we intend to study if more complex weight transformations could
help us improve our results, with a particular focus on designing effective constraints
for the parameters of these transformations.

Beyond Sharing Weights for Deep Domain Adaptation

15

References

1. Hinton, G., Osindero, S., Teh, Y.: A Fast Learning Algorithm for Deep Belief Nets. Neural

Computation 18 (2006) 1391–1415

2. LeCun, Y., Bottou, L., Orr, G., M¨uller, K.: Efﬁcient Backprop. In: Neural Networks: Tricks

of the Trade. Springer (1998)

3. Jiang, J.: A Literature Survey on Domain Adaptation of Statistical Classiﬁers. Technical

report, University of Illinois at Urbana-Champaign (2008)

4. Pan, S., Yang, Q.: A Survey on Transfer Learning.

engineering 22 (2010)

IEEE trans. on knowledge and data

5. Girshick, R., Donahue, J., Darrell, T., Malik, J.: Rich feature hierarchies for accurate object

detection and semantic segmentation. arXiv Preprint (2013)

6. Oquab, M., Bottou, L., Laptev, I., Sivic, J.: Learning and transferring mid-level image rep-

resentations using convolutional neural networks. In: CVPR. (2014)

7. Tzeng, E., Hoffman, J., Zhang, N., Saenko, K., Darrell, T.: Deep Domain Confusion: Maxi-

mizing for Domain Invariance. arXiv Preprint (2014)

8. Long, M., Cao, Y., Wang, J., Jordan, M.I.: Learning Transferable Features with Deep Adap-

tation Networks. In: International Conference on Machine Learning. (2015)

9. Ganin, Y., Lempitsky, V.: Unsupervised Domain Adaptation by Backpropagation. In: Inter-

national Conference on Machine Learning. (2015)

10. Tzeng, E., Hoffman, J., Darrell, T., Saenko, K.: Simultaneous Deep Transfer Across Domains

and Tasks. In: International Conference on Computer Vision. (2015)

11. Duan, L., Tsang, I., D.Xu, Maybank, S.: Domain Transfer SVM for Video Concept Detec-

tion. In: Conference on Computer Vision and Pattern Recognition. (2009) 1375–1381

12. Bergamo, A., Torresani, L.: Exploiting weakly-labeled web images to improve object clas-

siﬁcation: a domain adaptation approach. In: NIPS. (2010)

13. Becker, C., Christoudias, M., Fua, P.: Non-Linear Domain Adaptation with Boosting. In:

Advances in Neural Information Processing Systems. (2013)

14. H. Daum´e, I., Marcu, D.: Domain Adaptation for Statistical Classiﬁers. J. Artif. Int. Res.

26(1) (2006) 101–126

15. Saenko, K., Kulis, B., Fritz, M., Darrell, T.: Adapting Visual Category Models to New

Domains. In: European Conference on Computer Vision. (2010) 213–226

16. Kulis, B., Saenko, K., Darrell, T.: What You Saw is Not What You Get: Domain Adaptation
In: Conference on Computer Vision and Pattern

Using Asymmetric Kernel Transforms.
Recognition. (2011)

17. Gopalan, R., Li, R., Chellappa, R.: Domain Adaptation for Object Recognition: An Unsu-

pervised Approach. In: International Conference on Computer Vision. (2011)

18. Gong, B., Shi, Y., Sha, F., Grauman, K.: Geodesic Flow Kernel for Unsupervised Domain

Adaptation. In: Conference on Computer Vision and Pattern Recognition. (2012)

19. Fernando, B., Habrard, A., Sebban, M., Tuytelaars, T.: Unsupervised Visual Domain Adap-
tation Using Subspace Alignment. In: International Conference on Computer Vision. (2013)
20. Chopra, S., Balakrishnan, S., Gopalan, R.: DLID: Deep Learning for Domain Adaptation by
Interpolating Between Domains. In: International Conference on Machine Learning. (2013)
21. Gretton, A., Borgwardt, K., Rasch, M., Sch¨olkopf, B., Smola, A.: A Kernel Method for the

Two-Sample Problem. arXiv Preprint (2008)

22. Huang., J., Smola, A., Gretton., A., Borgwardt, K., Scholkopf, B.: Correcting Sample Se-
lection Bias by Unlabeled Data. In: Advances in Neural Information Processing Systems.
(2006)

23. Gretton, A., Smola, A., Huang, J., Schmittfull, M., Borgwardt, K., Sch¨olkopf, B.: Covariate

Shift by Kernel Mean Matching. Journal of the Royal Statistical Society 3(4) (2009) 5–13

16

Artem Rozantsev, Mathieu Salzmann, Pascal Fua

24. Gong, B., Grauman, K., Sha, F.: Connecting the Dots with Landmarks: Discriminatively
Learning Domain-Invariant Features for Unsupervised Domain Adaptation. In: International
Conference on Machine Learning. (2013)

25. Pan, S., Tsang, I., Kwok, J., Yang, Q.: Domain Adaptation via Transfer Component Analysis.

IEEE Transactions on Neural Networks 22(2) (2011) 199–210

26. Muandet, K., Balduzzi, D., Sch¨olkopf, B.: Domain Generalization via Invariant Feature

Representation. In: International Conference on Machine Learning. (2013)

27. Baktashmotlagh, M., Harandi, M., Lovell, B., Salzmann, M.: Unsupervised Domain Adap-
tation by Domain Invariant Projection. In: International Conference on Computer Vision.
(2013)

28. Ghifary, M., Kleijn, W.B., Zhang, M.: Domain Adaptive Neural Networks for Object Recog-

nition. arXiv Preprint (2014)

29. Bay, H., Ess, A., Tuytelaars, T., Van Gool, L.: SURF: Speeded Up Robust Features. Com-

puter Vision and Image Understanding 10(3) (2008) 346–359

30. Donahue, J., Jia, Y., Vinyals, O., Hoffman, J., Zhang, N., Tzeng, E., Darrell, T.: DeCAF:
A Deep Convolutional Activation Feature for Generic Visual Recognition. In: International
Conference on Machine Learning. (2014)

31. Chopra, S., Hadsell, R., LeCun, Y.: Learning a Similarity Metric Discriminatively, with Ap-
plication to Face Veriﬁcation. In: Conference on Computer Vision and Pattern Recognition.
(2005)

32. Long, M., Wang, J., Ding, G., Sun, J., Yu, P.: Transfer Feature Learning with Joint Distribu-

tion Adaptation. In: International Conference on Computer Vision. (2013) 2200–2207

33. Zeiler, M.D.: ADADELTA: an Adaptive Learning Rate Method. Computing Research

Repository (2012)

34. LeCun, Y., Bottou, L., Bengio, Y., Haffner, P.: Gradient-Based Learning Applied to Docu-

ment Recognition. IEEE (1998)

35. Krizhevsky, A., Sutskever, I., Hinton, G.: ImageNet Classiﬁcation with Deep Convolutional

Neural Networks. In: Advances in Neural Information Processing Systems. (2012)

36. Rozantsev, A., Lepetit, V., Fua, P.: On Rendering Synthetic Images for Training an Object

Detector. Computer Vision and Image Understanding 137 (2015) 24–37

37. Jin, J., Fu, K., Zhang, C.: Trafﬁc Sign Recognition with Hinge Loss Trained Convolutional
Neural Networks. IEEE Transactions on Intelligent Transportation Systems 15 (2014) 1991–
2000

38. Jaderberg, M., Simonyan, K., Zisserman, A., Kavukcuoglu, K.: Spatial Transformer Net-

works. arXiv Preprint (2015)

39. Li, W., Duan, L., Xu, D., Tsang, I.W.: Learning with Augmented Features for Supervised
IEEE Transactions on Pattern

and Semi-Supervised Heterogeneous Domain Adaptation.
Analysis and Machine Intelligence (2014) 1134–1148

40. Tommasi, T., Caputo, B.: Frustratingly Easy NBNN Domain Adaptation. In: International

Conference on Computer Vision. (2013)

41. Hull, J.: A Database for Handwritten Text Recognition Research.

Pattern Analysis and Machine Intelligence 16 (1994) 550–554

IEEE Transactions on

42. Fernando, B., Tommasi, T., Tuytelaars, T.: Joint Cross-Domain Classiﬁcation and Subspace

Learning for Unsupervised Adaptation. Pattern Recognition Letters 65 (2015) 60–66

43. Pan, S., Tsang, I., Kwok, J., Yang, Q.: Domain Adaptation via Transfer Component Analysis.

In: International Joint Conference on Artiﬁcial Intelligence. (2009) 1187–1192

44. Si, S., Tao, D., Geng, B.: Bregman Divergence-Based Regularization for Transfer Subspace

Learning. IEEE Trans. Knowl. Data Eng. 22(7) (2010) 929–942

