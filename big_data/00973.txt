6
1
0
2

 
r
a

M
3

 

 
 
]
S
D
.
s
c
[
 
 

1
v
3
7
9
0
0

.

3
0
6
1
:
v
i
X
r
a

Tight Analysis of a Multiple-Swap Heuristic for Budgeted Red-Blue

Median

Zachary Friggstad∗

Yifeng Zhang

Department of Computing Science
{zacharyf,yifeng2}@ualberta.ca

University of Alberta

Abstract

Budgeted Red-Blue Median is a generalization of classic k-Median in that there are two
sets of facilities, say R and B, that can be used to serve clients located in some metric space. The
goal is to open kr facilities in R and kb facilities in B for some given bounds kr, kb and connect
each client to their nearest open facility in a way that minimizes the total connection cost.

We extend work by Hajiaghayi, Khandekar, and Kortsarz [2012] and show that a multiple-swap
local search heuristic can be used to obtain a (5 + )-approximation for Budgeted Red-Blue
Median for any constant  > 0. This is an improvement over their single swap analysis and beats
the previous best approximation guarantee of 8 by Swamy [2014].
We also present a matching lower bound showing that for every p ≥ 1, there are instances of
Budgeted Red-Blue Median with local optimum solutions for the p-swap heuristic whose cost
is 5 + Ω
times the optimum solution cost. Thus, our analysis is tight up to the lower order
terms. In particular, for any  > 0 we show the single-swap heuristic admits local optima whose
cost can be as bad as 7 −  times the optimum solution cost.

(cid:16) 1

(cid:17)

p

1

Introduction

Facility location problems crop up in many areas of computing science and operations research. A
typical problem involves a set of clients and possible facility locations located in a metric space. The
goal is to open some facilities and connect each client to some open facility as cheaply as possible.
These problems become diﬃcult when there are costs associated with opening facilities or additional
constraints that ensure we cannot open too many facilities.

We study Budgeted Red-Blue Median, one particular instance of this type of problem. Here we
are given a set of clients C, a set of red facilities R, and a set of blue facilities B. These are located in
some metric space with metric distances d(i, j) ≥ 0 for any two i, j ∈ C ∪ R ∪ B. Additionally, we are
∗This research was undertaken, in part, thanks to funding from the Canada Research Chairs program and an NSERC

Discovery Grant.

1

cost(R ∪ B) :=

min
i∈R∪B

d(i, j).

(cid:88)

j∈C

given two integer bounds kr ≤ |R| and kb ≤ |B|. The goal is to select/open kr red facilities R and kb
blue facilities B to minimize

The classic NP-hard k-Median problem appears as a special case when, say, R = ∅. Thus, Budgeted
Red-Blue Median is NP-hard. In this paper, we focus on approximation algorithms for Budgeted
Red-Blue Median, in particular on local search techniques.

1.1 Previous Work

The study of Budgeted Red-Blue Median from the perspective of approximation algorithms was
initiated by Hajiaghayi, Khandekar, and Kortsarz [9], where they obtain a constant-factor approxi-
mation by a local search algorithm that iteratively tries to swap one red and/or one blue facility in
the given solution. They do not specify the constant in their analysis, but it looks to be greater than
8. Citing [9] as inspiration, Krishnaswamy et al. studied a generalization of Budgeted Red-Blue
Median known as Matroid Median [10]. Here, a matroid structure is given over the set of facilities
and we can only open a set of facilities if they form an independent set in the matroid. They obtain a
constant-factor approximation for Matroid Median through rounding an LP relaxation. This was
later reﬁned to an 8-approximation by Swamy [15].

The special case of k-Median is a classic optimization problem and has received a lot of attention
from both theoretical and practical communities. The best approximation guarantee known so far is
2.675 by Byrka et al.
[5], who build heavily on the breakthrough work of Li and Svensson for the
problem [11].

While local search techniques have been used somewhat infrequently in the design of approximation
algorithms in general, it may be fair to say that they have seen the most success in facility location
problems. For almost 10 years, the best approximation for k-Median was based on a local search
algorithm. Arya et al.
[3] show that a multiple-swap heuristic leads to a (3 + )-approximation for
k-Median for any constant  > 0. This analysis was simpliﬁed in [8], which inspires much of our
analysis.

√
2)-approximation for Uncapacitated Fa-
Other successful local search applications include a (1 +
cility Location is also obtained through local search [3, 6]. Local search has been very helpful in
approximating Capacitated Facility Location, the ﬁrst constant-factor approximation was by
P´al, Tardos, and Wexler [13] and the current best approximation is a (5 + )-approximation by Bansal,
Garg, and Gupta [4]. In the special case when all capacities are uniform, Aggarwal et al. [1] obtain
a 3-approximation. Even more examples of local search applied to other facility location variants can
be found in [2, 7, 8, 12, 14].

1.2 Our Results and Techniques

We show that a multiswap generalization of the local search algorithm considered in [9] is a (5 + )-
approximation for Budgeted Red-Blue Median. That is, for a value p say the p-swap heuristic
is the algorithm that, upon given an initial feasible solution, tries to swap up to p facilities of each

2

colour. If no such swap produces a cheaper solution, it terminates. Otherwise, it iterates with the
now cheaper solution. The formal description is given in Algorithm 1 in Section 2.

Say that a solution is locally optimum for the p-swap heuristic if no cheaper solution can be found
by swapping up to p facilities of each colour. Let OP T denote the cost of an optimum solution. Our
main result is the following.

√
Theorem 1 Any locally optimum solution for the p-swap heuristic has cost at most (5 + O(1/
OP T .

p)) ·

Using standard techniques (brieﬂy mentioned in Section 2), this readily leads to a polynomial-time
approximation algorithm. By choosing p = θ(1/2) we have the following.

Theorem 2 For any constant  > 0, Budgeted Red-Blue Median admits a polynomial-time
(5 + )-approximation.

This improves over the 8-approximation for Budgeted Red-Blue Median in [15]. We emphasize
the approximation guarantee from Theorem 1 is for Budgeted Red-Blue Median only, the 8-
approximation in [15] is still the best approximation for the general Matroid Median problem.
Indeed, [10] show that Matroid Median cannot be approximated within any constant factor using
any constant number of swaps even in the generalization of Budgeted Red-Blue Median where
there can be a super-constant number facility colours.

We also present a lower bound that matches our analysis up the lower order terms.
Theorem 3 For any integers p, (cid:96) with 1 ≤ p ≤ (cid:96)/2, there is an instance of Budgeted Red-Blue
Median that has a locally-optimum solution for the p-swap heuristic with cost at least
OP T .
By letting (cid:96) → ∞ but keeping p ﬁxed, we see that the p-swap heuristic cannot guarantee a ratio better
than 5 + 2
p . So, Theorem 1 is tight up to lower order terms. Also, for p = 1 we see that the single-swap
heuristic analyzed in [9] is not better than a 7-approximation.

p − 10p

(cid:17)·

(cid:16)

5 + 2

(cid:96)+1

Local search techniques are typically analyzed by constructing a set of candidate test swaps where
some facilities in the optimum solution are swapped in and some from the local optimum are swapped
out in order to generate a useful inequality. One of the main features of the k-Median analysis in
[3] and [8] is that such swaps can be considered that ensure each facility in the global optimum is
swapped in once and, by averaging some swaps, each facility in the local optimum is swapped out to
the extent of at most 1 + O() times. Each time a facility in the local optimum is swapped out, they
pay an additional 2 times the global optimum cost for some clients to reassign them.

We obtain only a 5 +  approximation because we end up swapping out some facilities in the local
optimum solution to the extent of 2 + O(), thereby paying an additional 2 + O() more than in the
k-Median analysis. Ultimately, this is because some of our initial swaps generate inequalities that
depend positively on client assignment costs in the local optimum. So we consider additional swaps
that do not introduce any more positive dependence on the local optimum to cancel them out.

3

This issue was also encountered in the analysis in [9]. In some sense, we are showing that this is the
only added diﬃculty over the standard k-Median analysis. However, the averaging arguments we use
are a fair bit more sophisticated than the analysis for k-Median.

1.3 Organization

Section 2 presents the algorithm and describes some useful notation. In particular, it presents a way
to decompose the global and local optimum solution into structured groups that are examined in the
analysis. Section 3 analyzes the quality of locally optimum solutions to prove Theorem 1. Section 4
proves Theorem 3 with an explicit construction of a bad example. We conclude with some remarks in
Section 5.

2 Notation and Preliminaries

Say that a feasible solution is a pair (R, B) of subsets R ⊆ R and B ⊆ B with |R| = kr and |B| = kb.
Algorithm 1 describes the local search algorithm.

Algorithm 1 The p-Swap Heuristic for Budgeted Red-Blue Median

Let (R, B) be arbitrary feasible solution.
while there is some feasible solution (R(cid:48), B(cid:48)) with |R − R(cid:48)| ≤ p

and |B − B(cid:48)| ≤ p and cost(R(cid:48) ∪ B(cid:48)) < cost(R ∪ B) do

(R, B) ← (R(cid:48), B(cid:48))

end while
return (R, B)

While a single iteration of Algorithm 1 can be executed in nO(p) (where n is the total number of
locations in the problem), it may be that the number of iterations is not polynomially bounded. We can
employ a well-known trick to ensure it does terminate in a polynomial number of steps while losing only
another  in our analysis. The idea is to perform the update only if cost(R(cid:48)∪B(cid:48)) ≤ (1−/∆)·cost(R∪B)
where ∆ is some quantity that is polynomial in the input size. Our analysis is compatible with this
approach; one can check that the total weight of all inequalities we consider is polynomially bounded.
For example, see [3] for details. We do not focus any further on this detail, and instead work toward
analyzing the cost of the solutions produced by Algorithm 1 as it is stated.
From now on, let S = R ∪ B with R ⊆ R, B ⊆ B denote an arbitrary local optimum solution. That
is, there is no cheaper solution (R(cid:48), B(cid:48)) with |R− R(cid:48)| ≤ p and |B − B(cid:48)| ≤ p. Also ﬁx a global optimum
solution O = R∗ ∪ B∗ where R∗ ⊆ B and B∗ ⊆ B. We assume that S ∩ O = ∅. This is without loss of
generality, as we can duplicate each facility location in the input and say that S use the ﬁrst copies
and O use the second copies. It is easy to verify that S is still a local optimum solution.
To help analyze the cost, we will introduce some notation. For any client j ∈ C, let sj ∈ S denote
the local optimum facility is closest to j and oj ∈ O be the optimum facility that is closest to j. For
brevity, let cj = d(j, sj) be the cost of assigning j in the local optimum and c∗
j = d(j, oj) the cost of

4

assigning j in the global optimum. Thus, cost(S) =(cid:80)

j∈C cj and cost(O) =(cid:80)

i∗ ∈ O we let N∗(i∗) = {j ∈ C : oj = i∗} and for any i ∈ S we let N (i) = {j ∈ C : sj = i}.
Let φ : O → S map each facility in O to its nearest facility in S, breaking ties arbitrarily. For i ∈ S, let
deg(i) = |φ−1(i)|. If deg(i) (cid:54)= 0, let cent(i) be the facility in φ−1(i) that is closest to i, again breaking
ties arbitrarily.

j∈C c∗

j . For any facility

We also borrow some additional notation from [9].
Deﬁnition 1 (very good, good, bad facility) A facility i ∈ S is very good if deg(i) = 0, good if
no i∗ ∈ φ−1(i) has the same colour as i, and bad otherwise.
The analysis in [9] divides S∪O into blocks that satisfy certain properties. We require slightly stronger
properties than their blocks guarantee. We also use a slightly diﬀerent notion of what it means for
some i ∈ S to be a leader. The required properties are summarized in the following lemma.

Lemma 1 We can partition S ∪ O into blocks T satisfying the following properties.

• |T ∩ R| = |T ∩ R∗| and |T ∩ B| = |T ∩ B∗|.
• For every i ∈ S ∩ T , we also have φ−1(i) ⊆ T . For every i∗ ∈ O ∩ T , we have φ(i∗) ∈ T .
• There is some facility ˆi ∈ T ∩ S with deg(ˆi) > 0 designated as the leader that has the following
properties. Every other i ∈ T ∩ S − {ˆi} is either good or very good and all good i ∈ T ∩ S − {ˆi}
have the same colour.

We focus on analyzing one block at a time to prove the approximation guarantee. This provides
us with a cleaner way to describe the test swaps and the additional structure will help handle the
inevitable cases where we have to swap out some i ∈ S but cannot swap in all of φ−1(i). For example,
this can happen if all blue facilities i ∈ B have deg(i) being very large (so all deg(i(cid:48)) = 0 facilities are
red). We will still need to close some of them in order to open facilities in B∗ when generating bounds
via test swaps.

2.1 Generating the Blocks
We prove Lemma 1 in this section. First, we describe how to partition S ∪ O into groups. These
will then be combined to form the ﬁnal blocks. We say that a group is a subset G of S ∪ O where
|G ∩ S| = |G ∩ O|, there is exactly one ˆi ∈ G ∩ S with deg(ˆi) > 0, and G ∩ O = φ−1(ˆi). Call this
facility ˆi the representative of the group.

We classify groups G in one of three ways.

• Balanced: |G ∩ R| = |G ∩ R∗| and |G ∩ B| = |G ∩ B∗|.
• Good: ˆi is a good facility and all other i ∈ G ∩ S − ˆi have a diﬀerent colour than ˆi. Note this

means |G ∩ R| = |G ∩ R∗| ± 1.

5

• Bad: G is neither balanced nor good.

Note that here the good and bad are referring to groups, we emphasize that these are diﬀerent than
good and bad facilities. Algorithm 2 describes a procedure for forming groups in a particular way that
will be helpful in creating the ﬁnal blocks.

Algorithm 2 Procedure for partitioning into groups

S(cid:48) ← S, O(cid:48) ← O
while ∃ some facility i in S(cid:48) with deg(i) > 0 do

else

G ← φ−1(i) + i
if G ∪ X is a balanced group for some X ⊆ S(cid:48) then
G(cid:48) ← G ∪ X
else if G ∪ X is a good group for some X ⊆ S(cid:48) then
G(cid:48) ← G ∪ X
Let X ⊆ {i(cid:48) ∈ S(cid:48) : deg(i(cid:48)) = 0} such that G ∪ X is a bad group and
all facilities in S(cid:48) − X have the same colour.
G(cid:48) ← G ∪ X.
end if
Output group G(cid:48)
S(cid:48) ← S(cid:48) − G(cid:48), O(cid:48) ← O(cid:48) − G(cid:48)

(cid:46) c.f. Lemma 2

(cid:46) i is the representative of G(cid:48)

end while

Lemma 2 Each iteration correctly executes (i.e. succeeds in creating a group).

Proof. By a simple counting argument, there are always exactly
: deg(i) (cid:54)= 0}|

(cid:48) − {i ∈ S

|O

(cid:48)

very good facilities in S(cid:48). So we can always ﬁnd a subset of very good facilities X such that G ∪ X
is a group. We prove that if the ﬁrst two if conditions are false then we can ﬁnd X to ensure S(cid:48) − X
only contains facilities of one colour.

There are 2 cases. Suppose i is bad and, without loss of generality, that it is also red. Because
we cannot extend G to be a balanced group, either there are less than |φ−1(i) ∩ B| very good blue
facilities in S(cid:48) or less than |φ−1(i) ∩ R| − 1 very good red facilities in S(cid:48). In either case, ﬁrst add all
very good facilities from the “deﬁcient” colour to X to use up that colour and then add enough very
good facilities to X of the other colour to ensure |X| = |φ−1(i)| − 1.

In the other case when i is good, we again assume without loss of generality that it is red. Because
we cannot form a good group, there are fewer than |φ−1(i)| − 1 very good blue facilities in S(cid:48). Use
them up when forming X and then add enough very good red facilities so that |X| = |φ−1(i)| − 1.
Let G be the collection of groups output by Algorithm 2. We now show how to piece these groups
together to form blocks.

6

Algorithm 3 Procedure for generating blocks

G(cid:48) ← G
while there is some balanced group G in G(cid:48) do

Output G as a block with its own representative being the leader.
G(cid:48) ← G(cid:48) − G.

end while
while there are good groups G, G(cid:48) ∈ G(cid:48) with diﬀerent coloured representatives do

Output the block G ∪ G(cid:48) and choose either representative as the leader.
G(cid:48) ← G(cid:48) − {G, G(cid:48)}

end while
while there is some bad group G ∈ G(cid:48) do

Let G0 ⊆ G(cid:48) − G consist only of good groups such that G + G0 is a block.
Output G + G0 with the representative of G as the leader.
G(cid:48) ← G(cid:48) − (G + G0).

end while

(cid:46) c.f. Lemma 3.

It is easy to verify that any “block” that is output by this algorithm indeed satisﬁes the properties
listed in Lemma 1.

The following lemma explains why this procedure correctly executes and why all groups are used up.
That is, it ﬁnishes the partitioning of S ∪ O into blocks. For a union of groups G∗ = G1 ∪ . . . ∪ Gk,
deﬁne the blue deﬁciency of G∗ as |G∗ ∩ B∗| − |G∗ − B|.

Lemma 3 If G is a bad group considered in some iteration of the last loop, we can ﬁnd the corre-
sponding G0 so that G + G0 is a block. Furthermore, after the last while loop terminates then G(cid:48) = ∅.

Proof. Suppose, without loss of generality, that the blue very good facilities were used up the ﬁrst time
a bad group was formed in Algorithm 2. Thus, for every bad group G(cid:48) ∈ G we have |G∩ B| < |G∩ B∗|.

Let G be a group considered in some iteration of the last loop in Algorithm 3. As observed above, the
blue deﬁciency of G is strictly positive.
The blue deﬁciency of the union of all groups in G(cid:48) is 0 because we have only removed blocks from
G(cid:48) up to this point and, by deﬁnition, a block has blue deﬁciency 0. Thus, there must be some other
group G(cid:48) ∈ G(cid:48) with strictly negative blue deﬁciency. It cannot be that G(cid:48) is bad, otherwise it has
nonnegative blue deﬁciency. It also cannot be that G(cid:48) is balanced or that it is a good group with a
red representative, because such blocks also have nonnegative blue deﬁciency.
Therefore, G(cid:48) must be good with a blue representative. Good blocks with blue representatives have
blue deﬁciency exactly -1. Add this G(cid:48) to G0. Iterating this argument with G(cid:48) + G0, we add good
groups with blue representatives to G0 until G(cid:48) + G0 is a block. The layout in Figure 1 depicts a block
constructed in this manner, the leftmost group in the ﬁgure is the bad group G and the remaining
groups form G0.
After the last loop there are no groups with bad representatives or balanced groups. Furthermore, all
good groups must have the same colour of representative by the second loop. If there were any good
group, the blue deﬁciency of the union of groups in G(cid:48) would then be nonzero, so there cannot be any

7

good groups left. That is, at the end of the last loop there are no more groups in G(cid:48).

2.2 Standard Bounds

Before delving into the analysis we note the following two bounds. The ﬁrst has been used extensively
in local search analysis and was ﬁrst proven in [3] and the second was proven in [9]. For convenience
we will include the proofs here.

Lemma 4 For any j ∈ C, d(j, φ(oj)) − cj ≤ 2c∗
j .

Proof. By the triangle inequality and the deﬁnition of φ, we have

d(j, φ(oj)) ≤ d(j, oj) + d(oj, φ(oj)) ≤ c
j + d(oj, sj) ≤ 2c
∗
∗
j + cj.

Lemma 5 For any j ∈ C, d(j, cent(φ(oj))) − cj ≤ 3c∗

j + cj.

Proof. By the triangle inequality and Lemma 4, it suﬃces to prove d(φ(oj), cent(φ(oj))) ≤ c∗
By deﬁnition of cent() and φ,

j + cj.

d(φ(oj), cent(φ(oj))) ≤ d(φ(oj), oj) ≤ d(sj, oj) ≤ c
∗
j + cj.

Finally, we often consider operations that add or remove a single item from a set. To keep the notation
cleaner, we let S + i and S − i refer to S ∪ {i} and S − {i}, respectively, for sets S and items i.

3 Multiswap Analysis

Recall that we are assuming S = R ∪ B is a locally optimum solution with respect to the heuristic
that swaps at most p facilities of each colour and that O = R∗∪ B∗ is some globally optimum solution.
We assume p = t2 + 1 for some suﬃciently large integer t.
B = T ∩ B∗ denote the red and blue
Focus on a single block T . For brevity, let T ∗
facilities from the optimum solution in T . Similarly let TR = T ∩R and TB = T ∩B denote the red and
blue facilities from the local optimum solution in T . The main goal of this section is to demonstrate
the following inequality for group T

R = T ∩ R∗ and T ∗

Theorem 4 For some absolute constant γ that is independent of t, we have

0 ≤ (cid:88)

j∈N∗(T ∗

R∪T ∗
B)

(cid:104)(cid:16)

(cid:17)

1 +

γ
t

j − cj
∗
c

(cid:88)

(cid:104)(cid:16)

(cid:17) · c

∗
j +

(cid:105)

.

· cj

γ
t

4 +

γ
t

j∈N (TR∪TB)

(cid:105)

+

8

Figure 1: Illustration of a block T . The facilities on the top are in T ∩ O and the facilities on the
bottom are in T ∩ S. The directed edges depict φ, and the thick edges connect cent(i) to i. The
facilities coloured black lie in B, the facilities coloured white lie in R, and the facilities coloured grey
could either lie in B or R. Note that B = {i1, i2, i3, i4}. The layout of the ﬁgure is suggestive of how
the block was constructed by adding “good” groups to the initial bad group in Algorithm 3

We ﬁrst show the simple details of how this yields our main result.

Proof of Theorem 1. Summing the inequalities stated in Theorem 4 over all blocks T , we see

(cid:18)

0 ≤(cid:88)

j∈C

5 +

2γ
t

(cid:19)

j −(cid:16)

· c
∗

(cid:17) · cj.

1 − γ
t

Multiplying through by t

t−γ , when t > γ this shows

0 ≤(cid:88)
(cid:18)

j∈C

5 +

5t + 2γ
t − γ

(cid:19)

7γ
t − γ

j − cj.
· c
∗
(cid:18)

· c
∗
j =

5 +

(cid:19)

7γ
t − γ

· cost(S

∗

).

Rearranging, we see

cost(S) =

(cid:88)

j∈C

cj ≤(cid:88)

j∈C

Recall that p = t2 + 1 where p is the number of swaps considered in the local search algorithm. Thus,
√
Algorithm 1 is a (5 + O(1/
The analysis breaks into a number of cases based on whether T ∗
B are large. In each of the
cases, we use the following notation and assumptions. Let ˆi denote the leader in T . Without loss of
generality, assume all other i ∈ TB ∪ TR with deg(i) > 0 are blue facilities. Let B = {i ∈ TB − ˆi :
deg(i) > 0}. Figure 1 illustrates this notation.

R and/or T ∗

p))-approximation.

The swaps we consider in these cases are quite varied, but we always ensure we swap in cent(i) whenever
some i ∈ S ∩ T with deg(i) > 0 is swapped out. This way, we can always bound the reassignment cost
of each client j by using either Lemma 4 or Lemma 5.

9

ˆiˆicent(ˆi)cent(ˆi)i1i1i2i2i3i3i4i4cent(i1)cent(i1)cent(i2)cent(i2)cent(i3)cent(i3)cent(i4)cent(i4)R| ≤ t2,|T ∗

3.1 Case |T ∗
In this case, we simply swap out all of TR ∪ TB and swap in all of T ∗
optimum solution and because this swaps at most t2 facilities of each colour, we have.

B| ≤ t

R ∪ T ∗

B. Because R ∪ B is a locally

0 ≤ cost(S ∪ (T

R ∪ T
∗

B) − (TR ∪ TB)) − cost(S)
∗

Of course, after the swap each client will move to its nearest open facility. As is typical in local search
analysis, we explicitly describe a (possibly suboptimal) reassignment of clients to facilities to upper
bound this cost change.
Each j ∈ N∗(T ∗
R ∪ T ∗
j − cj. Each j ∈ N (TR ∪ TB) − N∗(T ∗
c∗
open after the swap. By Lemma 4, the assignment cost change is bounded by 2c∗
j that has not already been reassigned remains at sj and incurs no assignment cost change. Thus,

B) is moved from sj to oj which incurs an assignment cost change of exactly
B) is moved to φ(oj). Note that φ(oj) (cid:54)∈ T so it remains
j . Every other client

R ∪ T ∗

0 ≤ (cid:88)

(cid:88)

j − cj) +
∗
(c

∗
2c
j

j∈N∗(T ∗

R∪T ∗
B)

j∈N (TR∪TB)

which is even better than what we are required to show for Theorem 4.

We note that the analysis Section 3.4 could subsume this analysis (with a worse constant), but we
have included it here anyway to provide a gentle introduction to some of the simpler aspects of our
approach.

b ∈ T ∗

B| ≥ t + 1

R| ≥ t2 + 1,|T ∗

j − cj for j with oj = i∗

3.2 Case |T ∗
We start by brieﬂy discussing some challenges in this case. In the worst case, all of the ib ∈ TB have
deg(i) being very large. The issue here is that we need to swap in each i∗
B in order to generate
terms of the form c∗
b . But this requires us to swap out some ib. Since we do
not have enough swaps to simply swap in all of φ−1(ib), we simply swap in cent(ib).
Any client j with sj being closed and oj ∈ φ−1(ib)− cent(ib) cannot be reassigned to φ(oj), so we send
it to cent(φ(oj)) and use Lemma 5 to bound the reassignment cost. This leaves a term of the form
+cj, so we have to consider additional swaps involving −cj to cancel this out. These additional swaps
cause us to lose a factor of roughly 5 instead of 3.
Another smaller challenge is that we do not want to swap out the leader ˆi ∈ T ∩ S for a variety of
technical reasons. However, since |T ∗
B| are both big, this is not a problem. When we swap
out some i∗ ∈ T ∩ O, we will just swap in a randomly chosen facility in T ∩ S − ˆi of the same colour.
The probability any particular facility is swapped in this way is very small. Ultimately, each facility
in T ∩ S will be swapped out 2 + O(1/t) times in expectation.
To be precise, we partition the set of clients in N (TR ∪ TB) into two groups:

R| and |T ∗

Cbad := N (B) ∩ N

∗

R − cent(B))
∗

(T

and Cok := N (TR ∪ TB − ˆi) − Cbad.

We omit N (ˆi) from Cok because we will never close ˆi in this case.

10

The ﬁrst group is dubbed bad because there may be a swap where both sj and φ(oj) are closed yet
oj is not opened so we can only use Lemma 5 to bound their reassignment cost. In fact, some clients
j ∈ Cok may also be involved in such a swap, but we are able to use an averaging argument for these
clients to show that the resulting +sj term from using Lemma 5 appears with negligible weight and
does not need to be cancelled.

We consider the following two types of swaps to generate our initial inequality.

• For each i∗

b ∈ T ∗

out ib and swap in i∗
cent(ib).

B, choose a random ib ∈ TB − ˆi. If ib (cid:54)∈ B (i.e. deg(ib) = 0) then simply swap
b and

b . If ib ∈ B then swap out ib and a random ir ∈ TR − ˆi and swap in i∗

• For each i∗

r ∈ T ∗

R − cent(B), swap in i∗

r and swap out a randomly chosen ir ∈ TR − ˆi.

By choosing facilities at “random”, we mean uniformly at random from the given set and this should
be done independently for each invokation of the swap.

Lemma 6

0 ≤ (cid:88)

j∈N∗(T ∗

B∪T ∗
R)

(cid:18) t + 1

t

· c
j − cj
∗

(cid:19)

(cid:88)

j∈Cok

+

(cid:20)(cid:18)

2 +

(cid:19)

5
t

(cid:21)

cj

+

(cid:88)

t + 1

t

j∈Cbad

∗
c
j +

1
t

∗
(3c
j + cj).

t and that either

|TR|
|TR−ˆi| and βB =

|TB|
|TB−ˆi| . Note that βR, βB ≤ t+1
Proof. For brevity, we will let βR =
βR = 1 or βB = 1.
First consider a swap of the ﬁrst type that swaps in {i∗
b , cent(ib)} and swaps out {ib, ir} for some ib
with deg(ib) > 0. Because R ∪ B is a local optimum the cost of the solution does not decrease after
performing this swap. We provide an upper bound on the reassignment cost.
Each j ∈ N∗({i∗
j − cj.
Every client j ∈ N ({ib, ir}) that has not yet been reassigned is ﬁrst moved to φ(oj). If this φ(oj)
remains open, assign j to it. By Lemma 4, the assignment cost for j increases by at most 2c∗
j . If φ(oj)
is not open then φ(oj) = ib (because deg(ir) = 0) so we instead move j to cent(φ(oj)) = cent(ib).
j + cj. This can only happen if sj ∈ {ir, ib}
Lemma 5 shows the assignment cost increases by at most 3c∗
and φ(oj) = ib.

b , cent(ib)}) is reassigned from sj to oj and incurs an assignment cost change of c∗

Combining these observations and using slight overestimates, we see

(cid:88)

0 ≤

j∈N∗({i∗

b ,cent(ib)})

(cid:88)

(cid:88)

j∈N ({ib,ir})

φ(oj )(cid:54)=ib

j∈N ({ib,ir})

φ(oj )=ib

j − cj) +
∗
(c

∗
2c
j +

∗
(3c
j + cj).

(1)

Now, if the random choice for ib in the swap has deg(ib) = 0, then swapping {ib} out and {i∗
generates an even simpler inequality:

b} in

j − cj) +
∗
(c

∗
j .
2c

(2)

0 ≤ (cid:88)

j∈N∗(i∗
b )

(cid:88)

j∈N (ib)

11

To see this, just reassign each j ∈ N∗(i∗
to φ(oj) (which remains open because deg(ib) = 0) and use Lemma 4.
Consider the expected inequality that is generated for this ﬁxed i∗
that follow straight from the deﬁnitions and the swap we just performed.

b ) from sj to oj and reassign the remaining j ∈ N (ib) from sj

b . We start with some useful facts

• Any j ∈ N∗(cent(B)) has oj open with probability
• Any j ∈ Cbad has sj being closed with probability
• Any j ∈ Cok − N (TR) has sj being closed with probability

1|TB−ˆi| .
1|TB−ˆi| .

1|TB−ˆi| . When this happens, if oj is

not opened then φ(oj) must be open.
That is, j ∈ Cok means oj ∈ T ∗
B then φ(oj) = ˆi (by the structure of block
T ) which remains open. If oj ∈ cent(B) then either φ(oj) was not closed, or else cent(φ(oj)) = oj
was opened.

B ∪ cent(B). If oj ∈ T ∗

• Any j ∈ Cok∩N (TR) has sj being closed with probability

|B|
|TB−ˆi|·

then we move j to cent(φ(oj)). However, this can only happen with probability
since it must be that φ(oj) is the blue facility that was randomly chosen to be closed.

1|TR−ˆi| . If oj and φ(oj) are closed,
1|TR−ˆi|

1|TB−ˆi| ·

Averaging (1) over all random choices and using some slight overestimates we see

1

|TB − ˆi| · (cid:88)
(cid:88)
(cid:88)

∗
j + cj) +
(3c

j∈N∗(cent(B))

j∈Cok∩N (TR)

j − cj)
∗
(c



∗
2c
j

j∈Cok−N (TR)
(|B|2c
∗
∗
j + 3c
j + cj).

0 ≤ (cid:88)

j∈N∗(i∗
r )

j − cj) +
∗
(c

 (cid:88)

1

j∈Cbad
1

|TR − ˆi|

1

+

+

|TB − ˆi|
|TB − ˆi| ·
j − cj) + βB · (cid:88)
 (cid:88)
(cid:88)

b ∈ N∗(T ∗
∗
(c

∗
(3c
j + cj) +

B) shows

j∈N∗(cent(B))

Summing over all i∗

0 ≤ (cid:88)

j∈N∗(T ∗
B)

+βB ·

j − cj)
∗
(c

 +

∗
2c
j

|TR − ˆi| · (cid:88)

βB

(3)

((2|B| + 3)c
∗
j + cj).

j∈Cbad

j∈Cok−N (TR)

j∈Cok∩N (TR)
R − cent(B) and swaps out some
Next, consider the second type of swap that swaps in some i∗
randomly chosen ir ∈ TR − ˆi. Over all such swaps, the expected number of times each ir ∈ TR − ˆi is
r) from sj to oj and
swapped out is
every other j ∈ N (ir) from sj to φ(oj) which is still open because deg(ir) = 0. Thus,

|TR−ˆi| . In each such swap, we reassign j ∈ N∗(i∗

R|−|B|
|T ∗
|TR−ˆi| = βR − |B|

r ∈ T ∗

(cid:88)

0 ≤

j − cj) +
∗
(c

j∈N∗(T ∗

R−cent(B))

(cid:18)
βR − |B|
|TR − ˆi|

(cid:19)

· (cid:88)

j∈Cok∩N (TR)

∗
2c
j

12

Scaling this bound by βB, adding it to (3), and recalling |TR| ≥ t2 shows

0 ≤ (cid:88)

∗
(c

j − cj) + βB · (cid:88)
 (cid:88)

∗
(3c
j + cj) +

j∈N∗(TR)

(cid:88)

j∈N∗(T ∗
B)

+βB ·

j − cj)
∗
(c

 + βB · (cid:88)

∗
2c
j +

j∈Cok∩N (TR)

(cid:20)(cid:18)

2βR +

(cid:19)

3
t2

(cid:21)

.

· c
∗
j +

1

t2 · cj

j∈Cbad

j∈Cok−N (TR)
t and also βB · βR ≤ t+1

t

Recall that βB, βR ≤ t+1
Our next step is to cancel terms of the form +cj in the bound from Lemma 6 for j ∈ Cbad. To do
this, we again perform the second type of swap for each i ∈ T ∗
R − cent(B) but reassign clients a bit
diﬀerently in the analysis.

to complete the proof of Lemma 6.

Lemma 7

0 ≤ (cid:88)

j∈Cbad

j − cj) +
∗
(c

t + 1

t

· (cid:88)

j∈Cok∩N (TR)

∗
2c
j

R − cent(B), swap i∗
r) to i∗

r, we only reassign those in Cbad ∩ N∗(i∗

r ∈ T ∗
Proof. For each i∗
reassigning all j ∈ N∗(i∗
other j ∈ N (ir) can be reassigned to φ(oj) and which increases the cost by at most 2c∗
j .
Summing over all i∗
sj closed at most βR ≤ t+1
Adding the bounds stated in Lemmas 6 and 7 shows that Theorem 4 holds in this case.

r, observing that Cbad ⊆ T ∗

r in and a randomly chosen ir ∈ Tr − ˆi. Rather than
r). Since deg(ir) = 0 then any

R − cent(B), and also observing that each j ∈ Cok has

times in expectation, we derive the inequality stated in Lemma 7.

t

B| ≤ t

R| ≥ t2 + 1,|T ∗

3.3 Case |T ∗
In this case, we start by swapping in all of T ∗
B and swapping out all of TB (including, perhaps, ˆi if it is
blue). In the same swap, we also swap in cent(TB) and swap out a random subset of the appropriate
number of facilities in TR − ˆi. This is possible as |TR − ˆi| ≥ t ≥ |cent(TB)|. By random subset, we
mean among all subsets of Tr − ˆi of the necessary size, choose one uniformly at random.
As with Section 3.2, we begin with a deﬁnition of bad clients that is speciﬁc to this case:

Cbad := N (TB) ∩ N

∗

R − cent(TB)).
∗

(T

Clients j ∈ Cbad may be involved in swaps where both sj and φ(oj) is closed yet oj is not opened and
we cannot make this negligible with an averaging argument.

Lemma 8

0 ≤

(cid:88)

j − cj) +
∗
(c

j∈N∗(T ∗

B∪cent(TB))

∗
j + cj) +
(3c

(cid:88)

j∈Cbad

∗
j + cj)
(3c

(cid:88)

j∈N (TR)

1
t

13

Proof. After the swap, reassign every j ∈ N∗(T ∗
B ∪ cent(TB)) from sj to oj, for a cost change of
j − cj. Every other j that has sj being closed is ﬁrst reassigned to φ(oj). If this is not open, then
c∗
further move j to cent(oj) which must be open because the only facilities i ∈ TR ∪ TB with deg(i) > 0
that were closed lie in TB and we opened cent(TB).
If j ∈ N (TB) − Cbad then oj ∈ T ∗
we have moved j to cent(φ(oj)) and the cost change is 3c∗
Finally, if j ∈ N (TR) then we either move j to φ(oj) or to cent(φ(oj)) if φ(oj) is not open. The
j + cj by Lemmas 4 and 5. However, note that sj ∈ TR
worst-case bound on the reassignment cost is 3c∗
is closed with probability only 1/t, since we close a random subset of Tr − ˆi of size at most t and
|Tr − ˆi| ≥ t2.

B ∪ cent(TB) and we have already assigned j to oj. If j ∈ Cbad then

j + cj by Lemma 5.

We still need to swap in T ∗
r and swap out a randomly
chosen ir ∈ TR − ˆi. The analysis of these swaps is essentially the nearly identical swaps in Section
3.2, so we omit it and merely summarize what we get by combining the resulting inequalities with the
inequality from Lemma 8.

R − cent(TB). For each such facility i∗

r, swap in i∗

Lemma 9

0 ≤ (cid:88)

(cid:88)

(cid:18) t2 + 1

t2

j − cj) +
∗
(c

· 2c
∗
j +

· (3c
∗
j + cj)

+

1
t

∗
(3c
j + cj)

R∪T ∗
B)

j∈N∗(T ∗

j∈Cbad
We cancel the +cj terms for j ∈ Cbad with one further collection of swaps. For each i∗
we swap in i∗
obtain from these swaps. It is proven in essentially the same way as Lemma 7.

R− cent(TB)
r and a randomly chosen ir ∈ TR − ˆi. The following lemma summarizes a bound we can

r ∈ T ∗

j∈N (TR)

(cid:19)

(cid:88)

Lemma 10

0 ≤ (cid:88)

j∈Cbad

t2 + 1

t2

∗
j .
2c

· (cid:88)
(cid:18) t2 + 3t + 1

j∈N (TR)

j − cj) +
∗
(c
(cid:88)

j − cj) +
∗
(c

j∈N (TR∪TB)

t2

0 ≤ (cid:88)

j∈N∗(T ∗

R∪T ∗
B)

(cid:19)

.

· 4c
∗
j +

· cj

1
t

Adding this to the bound from Lemma 9 shows

B| ≥ t + 1

R| ≤ t2,|T ∗

3.4 Case |T ∗
R and deg(i) > 0 for each i ∈ B, then |B| ≤ t2 as well. We will swap all of T ∗
Because φ−1(i) ⊆ T ∗
all of TR, but we will also swap some blue facilities at the same time. Let B(cid:48) = B and let B
B of size |B|.
arbitrary subset of T ∗
If ˆi (cid:54)∈ TR ∪ B(cid:48) then add ˆi to B(cid:48).

then add cent(ˆi) to B

R ∪ B
If cent(ˆi) (cid:54)∈ T ∗
(cid:48)
b ∈ T ∗
or ib ∈ TB − B(cid:48) to B(cid:48) to ensure |B
to B

B − B

(cid:48)| − |B(cid:48)|(cid:12)(cid:12)(cid:12) ≤ 1 Add an arbitrary i∗

(cid:48)| = |B(cid:48)|.

(cid:12)(cid:12)(cid:12)|B

(cid:48)

(cid:48)

(cid:48)

(cid:48)

. At this point,

R for
be an

14

We begin by swapping out TR ∪ B(cid:48) and swapping in T ∗
. The following list summarizes the
important properties of this selection, the ﬁrst point emphasizes that this swap will not improve the
objective function since S is a locally optimum solution for the p-swap heuristic where p = t2 + 1.

R ∪ B

(cid:48)

(cid:48)| ≤ t2 + 1 and |T ∗

• |B(cid:48)| = |B
• T ∗
• For each i ∈ TR ∪ TB with deg(i) > 0, i was swapped out and cent(i) was swapped in.

R was swapped in and TR was swapped out.

R| ≤ t2.

The following is precisely the clients j that will be moved to cent(φ(oj)) in our analysis.

(cid:48)

Cbad := [N (TR ∪ B

) − N
As before, deﬁne Cok = N (TR ∪ TB) − Cbad.
The following bound is generated from swapping out TR ∪ B(cid:48) and swapping in T ∗
from the same arguments we have been using throughout the paper.

)] ∩ {j : φ(oj) ∈ TR ∪ B

R ∪ B
∗

(cid:48)}.

(T

∗

(cid:48)

0 ≤ (cid:88)

j∈N∗(T ∗

R∪B

(cid:88)

(cid:88)

j − cj) +
∗
(c

(cid:48)

)

∗
2c
j +

∗
(3c
j + cj)

j∈Cok∩N (TR∪B(cid:48))

j∈Cbad

R ∪ B

(cid:48)

and follows

Lemma 11

(cid:48)

B − B

) → (TB − B(cid:48)) be an arbitrary bijection of the remaining blue facilities that
b and swapping out
b ). Note that every facility ib swapped out in this way has deg(ib) = 0. So we can derive two

Next, let κB : (T ∗
were not swapped. For every i∗
κB(i∗
possible inequalities from such swaps.

, consider the eﬀect of swapping in i∗

(cid:48)

0 ≤ (cid:88)

j − cj) +
∗
(c

b ∈ T ∗
B − B
(cid:88)

∗
2c
j

and 0 ≤ (cid:88)

j − cj) +
∗

(c

∗
j .

2c

(4)

j∈N∗(i∗
b )

j∈N (κB(i∗
b ))

b)∩Cbad
The second inequality follows from only reassigning clients j ∈ N∗(i∗
Adding the bound in Lemma 11 to the sum of both inequalities in over all i∗
that κB(T ∗

B − B) ∩ (TR ∪ B(cid:48)) = ∅, we see

j∈N∗(i∗

b ) ∩ Cbad from sj to oj.
(cid:48)

b ∈ T ∗

B − B

j∈N(κB(i∗
b))

and noting

(cid:88)

0 ≤ (cid:88)

j − cj) +
∗
(c

∗
4c
j .

j∈N∗(T ∗

R∪T ∗
B)

j∈N (TR∪TB)

(cid:88)

4 Locality Gaps

Here we prove Theorem 3. Let p, (cid:96) be integers satisfying p ≥ 1 and (cid:96) ≥ 2p. Consider the instance with
kr = p + 1 and kb = p((cid:96) + 1) depicted in Figure 2. Here, β = 2p and α = β · ((cid:96) − p).
The cost of the local optimum solution is α · (p + 1) + β · p(cid:96) + p2((cid:96) + 1) and the cost of the global
optimum solution is simply p2((cid:96) + 1). Through some careful simpliﬁcation, we see the local optimum
solution has cost at least 5 + 2

(cid:96)+1 times the global optimum solution.

p − 10p

15

Figure 2: Illustration of the bad locality gap. Blue facilities are depicted with black and red facilities
are depicted with white. The top facilities are the global optimum and the bottom are the local
optimum (all of R and B is depicted in the picture). Each client is represented by a small black dot.
The metric is the shortest path metric of the presented graph, if two locations are not connected in
the picture then their distance is a very large value. Every edge in the right-most group with p2((cid:96) + 1)
clients has length 1. Recall β = 2p and α = ((cid:96) − p)2p.

To complete the proof of Theorem 3, we must verify that the presented local optimum solution indeed
cannot be improved by swapping up to p facilities of each colour.

We verify that the solution depicted in Figure 2 is indeed a locally optimum solution. Suppose
0 ≤ R ≤ p red facilities and 0 ≤ B ≤ p blue facilities are swapped. We break the analysis into four
simple cases.

In what follows, we refer to the leftmost collection of only red facilities in Figure 2 as the left group,
the rightmost collection of only blue facilities as the right group, and the remaining facilities as the
middle group. We also let the term subgroup refer to one of the p smaller collections of facilities in
the middle group. In each case, let B(cid:48) ≤ B denote the number of global optimum facilities from the
middle group that are swapped in. Recall that ˆi denotes the local optimum facility in the left group.

Case R = 0

The only clients that can move to a closer facility are the B(cid:48) clients in the middle group that have
their associated optimum facilities swapped in. Also, precisely B · (p − B + B(cid:48)) facilities in the right
group are not adjacent to any open facility so their assignment cost increases by 2.
Overall, the assignment cost change is exactly 2B · (p − B + B(cid:48)) − βB(cid:48). As β = 2p and B ≤ p, this
quantity is minimized at B(cid:48) = B leaving us with a cost change of 2Bp− βB = 0. So, if R = 0 then no
choice of blue facilities leads to an improving swap.

Case R ≥ 1 and ˆi is not swapped out.
In the left group, precisely R clients move to their close facility and the total savings is −αR. In the
middle group, precisely B(cid:48) clients move to their close facility and the total savings is −βB(cid:48).

16

ˆi000000000p+1``ptimesp(`+1)p↵↵↵      11..................In fact, it is easy to see that the cheapest such swap occurs when the B(cid:48) ≤ p ≤ (cid:96) facilities in the
middle group that are swapped in are part of subgroups where the local optimum facility is swapped
out (which is why we assume R ≥ 1). So, there are exactly R(cid:96) − B(cid:48) other clients j where both oj
and sj are closed and each pays an additional ≥ β to be connected. Finally, the right group pays an
additional 2B(p − B + B(cid:48)) to be connected.
Overall, the cost increases by 2B(p− B + B(cid:48)) + β(R(cid:96)− 2B(cid:48))− αR. As 2B− 2β = 2B− 4p ≤ −2p, this is
minimized at B(cid:48) = B. The cost change is then 2Bp + β(R(cid:96)− 2B)− αR. Recall that α = ((cid:96)− p)β < (cid:96)β,
so this is, in turn, minimized when R = 1.
Reducing further, the cost change is 2Bp + β(cid:96) − 2Bβ − α. Setting B = p to maximize, the change is
2p2 + 2p(cid:96) − 4p2 − ((cid:96) − p)2p = 0. So, no swap that swaps at least one red facility but not ˆi can ﬁnd a
cheaper solution.

Case R = 1 and ˆi is swapped out.
The cost change in the left group is (p − 1)α ≥ 0 since p clients must move an additional α and only
one client saves α. The cost change from the remaining groups is the same as in the ﬁrst case R = 0,
so the overall assignment cost does not decrease.

Case R ≥ 2 and ˆi is swapped out.
The cost change in the ﬁrst group is exactly (p + 1− 2R)α. Similar to the second case, the cost change
in this case is minimized when each subgroup that has its local optimum facility closed also has one
of its global optimum facility opened, and all B(cid:48) facilities opened in the middle group belong to a
subgroup having its local optimum closed.
The cost change is then 2B(p− B + B(cid:48)) + β((R− 1)(cid:96)− 2B(cid:48)) + α(p + 1− 2R). Again, this is minimized at
B(cid:48) = B which yields a cost change of 2Bp + β((R − 1)(cid:96)− 2B) + α(p + 1− 2R). Now, β(cid:96) ≤ 2α because
(cid:96) ≥ 2p, so this is further minimized at R = p and the cost increase is 2Bp + β((p− 1)(cid:96)− 2B)− α(p− 1).
Again, setting B = p to minimize the cost change we see it is 2p2 + β((p − 1)(cid:96) − 2p) − α(p − 1).
Expanding with β = 2p and α = 2p((cid:96) − p), the cost change ﬁnally seen to be
−2p2 + 2p(p − 1)(cid:96) − 2p((cid:96) − p)(p − 1) = 2p3 − 4p2.

The last expression is nonnegative for p ≥ 2.

4.1 Summarizing
No matter which ≤ p red and ≤ p blue facilities are swapped, the above analysis shows the assignment
cost does not decrease. The only potentially concerning aspect is that the very last case derived an
inequality that only holds when p ≥ 2. Still, this analysis does apply to the single-swap case (i.e.
p = 1) since the last case with R ≥ 2 does not need to be considered when p = 1.

17

5 Conclusion

We have demonstrated that a natural p-swap local search procedure for Budgeted Red-Blue Me-
√
dian is a (5 + O(1/
p))-approximation. This guarantees a better approximation ratio than the
single-swap heuristic from [9], which we showed may ﬁnd solutions whose cost is (7 − ) · OP T for
arbitrarily small . Our analysis is essentially tight in that the p-swap heuristic may ﬁnd solutions
whose cost is (5 + 2

p − ) · OP T .

More generally, one can ask about the p-swap heuristic for the generalization where there are many
diﬀerent facility colours. If the number of colours is part of the input then any local search procedure
that swaps only a constant number of facilities in total cannot provide good approximation guarantees
[10]. However, if the number of diﬀerent colours is bounded by a constant, then perhaps one can get
better approximations through multiple-swap heuristics.

However, generalizing the approaches taken with Budgeted Red-Blue Median to this setting seems
more diﬃcult; one challenge is that it is not possible to get such nicely structured blocks. It would also
be interesting to see what other special cases of Matroid Median admit good local-search based
approximations. For example, the Mobile Facility Location problem studied in [2] is another
special case of Matroid Median that admits a (3 + )-approximation through local search.

Finally, the locality gap of the p-swap heuristic for k-Median is known to be 3 + 2
p [3] and we
p for Budgeted Red-Blue Median. Even if the multiple-swap
have just shown it is at least 5 + 2
heuristic for the generalization to a constant number of colours can provide a good approximation,
this constant may be worse than the alternative 8-approximation obtained through Swamy’s general
Matroid Median approximation [15].

References

[1] A. Aggarwal, L. Anand, M. Bansal, N. Garg, N. Gupta, S. Gupta, and S. Jain. A 3-approximation

for facility location with uniform capacities. In Proc. of IPCO, 2010.

[2] S. Ahmadian, Z. Friggstad, and C. Swamy. Local search heuristics for the mobile facility location.

In Proc. of SODA, 2013.

[3] V. Arya, N. Garg, R. Khandekar, A. Meyerson, K. Munagala, and V. Pandit. Local search

heuristics for k-median and facility location problems. SIAM J. Comput, 33(3):544–562, 2004.

[4] M. Bansal, N. Garg, N. Gupta. A 5-approximation for capacitated facility location. In Proc. of

ESA, 2012.

[5] J. Byrka, T. Pensyl, B. Rybicki, A. Srinivasan, and K. Trinh. An improved approximation for

k-median, and positive correlation in budgeted optimization. In Proc. of SODA, 2015.

[6] M. Charikar and S. Guha.

Improved combinatorial algorithms for facility location problems.

SIAM J. Comput, 34(4):803–824, 2005.

18

[7] I. Gørtz and V. Nagarajan. Locating depots for capacitated vehicle routing. In Proc. of APPROX,

2011.

[8] A. Gupta and K. Tangwongsan. Simpler analysis of local search algorithms for facility location.

CoRR, abs/0809.2554, 2008.

[9] M. Hajiaghayi, R. Khandekar, and G. Kortsarz. Local search algorithms for the red-blue median

problem. Algorithmica, 63(4):795–814, 2012.

[10] R. Krishnaswamy, A. Kumar, V. Nagarajan, Y. Sabharwal, and B. Saha. The matroid median

problem. In Proc. of SODA, 2011

[11] S. Li and O. Svensson. Approximating k-median via pseudo-approximation. In Proc. of STOC,

2013.

[12] M. Mahdian and M. P´al. Universal facility location In Proc. of ESA, 2011.

[13] M. P´al, ´E. Tardos, and T. Wexler. Facility location with nonuniform hard capacities. In Proc. of

FOCS, 2001.

[14] Z. Svitkina and ´E. Tardos. Facility location with hierarchical facility costs. ACM Transactions

on Algorithms, 6(2), 2010.

[15] C. Swamy. Improved approximation algorithms for matroid and knapsack problems and applica-

tions. In Proc. of APPROX, 2014.

19

