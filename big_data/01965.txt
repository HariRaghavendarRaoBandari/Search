Towards a DSL for Perception-Based Safety Systems

Johann Thor Mogensen Ingibergsson∗ and Stefan-Daniel Suvei∗ and
Mikkel Kragh Hansen† and Peter Christiansen† and Ulrik Pagh Schultz∗

∗ University of Southern Denmark, Campusvej 55, 5230 Odense M, Denmark

† Department of Engineering, Aarhus University, Finlandsgade 22, 8200 Aarhus N, Denmark

Email: {jomo|stdasu|ups}@mmmi.sdu.dk or {pech|mkha}@eng.au.dk

6
1
0
2

 
r
a

M
7

 

 
 
]

O
R
.
s
c
[
 
 

1
v
5
6
9
1
0

.

3
0
6
1
:
v
i
X
r
a

I. INTRODUCTION

Agriculture sees a high number of injuries and fatalities,
even in developed countries [1]. Field robots are a solution
to this problem, but the issue with ﬁeld robots is that they
are prone to failure [2], which has led to the use of Model-
Driven Engineering (MDE). In robotics, MDE is used to
improve development time and reliability, for example in
regards to behavior [5]. In order to ensure safe autonomous
operation, robust and reliable risk detection and obstacle
avoidance must be performed. In this regard, ﬁeld robots
are highly dependent on perception sensors and algorithms
to understand and react on the environment. The robot has to
observe a large area; it must be fast, reliable and robust; it is
constrained to function with low computational resources due
to embedded hardware; and it might have lower priority than
control and must not jam [3]. This imposes severe constraints
on the software that interacts with sensors. Therefore, we
propose a DSL [3] for safety concerns in perception systems.
Our current research question is to investigate if the safety
rules should be split between distinct domains (behavior [5]
and perception [3]), or combined into one (this paper).

II. METHOD AND EXPERIMENTAL SETUP

using a Hough transform to enable the robot to position itself
in the middle of the row. The camera detects humans using a
C++ implementation of a pedestrian detector by Doll´ar [7].
A human position is estimated using the tilt of the camera,
the bounding box position and a ﬂat surface assumption. If
it detects a human, the robot will sound warnings and limit
the maximum speed according to the distance to the human,
and ultimately bring the robot to a full stop.

Fig. 2 visualizes an example from an actual ﬁeld trial.
The red cylinder denotes a detected pedestrian, the blue lines
denote the detected rows, and the yellow rectangle denotes
the closest obstacle detected by the lidar. The traversable
width of the row is decreased by the algorithm (indicated
by the yellow line), such that
the robot will effectively
navigate around the closest obstacle. Due to a small distance
to the detected pedestrian, the robot sounds a warning and
decreases its speed.

Fig. 1: Field robot prototype with lidar and camera sensors.

For investigating the research question, we built a ﬁeld
robot prototype (Fig. 1) to experiment with language design.
First step is a test case implemented without MDE. To this
end, we have extended a simple robot called Frobit, shown
above, with non-trivial safety based on different sensors
to prototype an agricultural ﬁeld robot. The sensors are a
Velodyne HDL-32E lidar with 32 channels for 360 degree
3D view and a Logitech C920 HD webcam. The prototype is
designed for operation within an orchard. The lidar is used
for detecting the orchard row to enable general navigation,
and for object detection in front of the robot
to enable
obstacle avoidance [6]. The distances from the obstacles to
the robot are used to adjust to the speed. The rows are found

Fig. 2: Example of row navigation and obstacle avoidance.
The robot has detected the rows (blue lines), a close ob-
stacle (yellow rectangle), and a pedestrian (red cylinder). It
adjusts its navigation (yellow line) and decreases its speed
accordingly.

III. LANGUAGE DESIGN

We propose ideas of how to achieve sufﬁcient safety
levels for the perception system and the ﬁeld robot as a
whole. We want to create a DSL that combines behavioral
safety [5] with our proposed perception safety system [3].
An outline of the DSL utilizing safety from both systems,
can be seen below:

e x i s t s p i n camera . a l l ( p e d e s t r i a n ) :

s t o p ; }
d i s t a n c e ( p ) < 1m { sound emergency ;
c a p s p e e d ; }
d i s t a n c e ( p ) < 3m { sound move away ;
d i s t a n c e ( p ) < 5m { sound pl ea se m ov e aw a y ; }

h i s t h = h i s t o g r a m ( camera . image , b i n s = 10 , n o r m a l i z e d =

s i z e ( x ) >0) / s i z e ( h . b i n s ) <0.3 {
s i z e ( x ) >0) / s i z e ( h . b i n s ) <0.1 { s t o p ;
s i z e ( x ) > 0 ) − min ( x i n h . b i n s :

s i z e (

max ( x i n h . b i n s :

t r u e ) :
s i z e ( x i n h . b i n s :
c a p s p e e d ; }
s i z e ( x i n h . b i n s :
}
x ) > 0 ) < 1000 px { s t o p ; }
d i s t a n c e ( o ) < 5m { c a p s p e e d ; }
d i s t a n c e ( o ) < 0 . 5m { sound emergency ;
l a s e r s a i n l a s e r s ( a l i v e ) :
c o u n t ( a ) < 32 { c a p s p e e d ; }
c o u n t ( a ) < 26 { s t o p ; }

e x i s t s o i n l a s e r . a l l ( O b j e c t s ) :

s t o p ; }

Here, the output of the different sensors can be subjected
to rules, that will enable the system to analyze the trust-
worthiness of the sensor and act accordingly. The ﬁrst rule
uses a pedestrian detector to calculate the distance to the
nearest human and will then use cognitive safety to react,
i.e., try to handle the situation before the emergency system
kicks in. In addition to this, a functional layer performs a
histogram analysis of the incoming images, verifying that
the camera sensor is working. Similarly for a lidar, cognitive
safety and functional safety are divided between different
functionalities. In the ﬁrst rule, the lidar detects objects using
information from all available laser beams. Depending on
the distance of an object, the speed of the robot is decreased
accordingly. For the functional layer, it is possible to look
at graceful degradation for the lidar, as it consists of many
laser beams that provide a certain degree of redundancy. This
is indicated by “lasers(alive)”. Functionality corresponding
to the above code has been tested on the robot. The code
developed without the proposed language, is 8454 lines of
C++ and Python ROS code. The language that we propose
could, if implemented, cut down the code to 14 lines. The
next steps will be to follow MISRA [4] rules to extend the
trustworthiness of the code into the domain of standards.

REFERENCES

[1] Agricultural Statistics Board, “Agricultural Safety: 2009 Injuries to

Adults on Farms,” 2013.

[2] M. Reichardt, T. F¨ohst, and K. Berns, “On software quality-motivated
design of a real-time framework for complex robot control systems,”
in Int. Workshop on Software Quality and Maintainability, 2013.

[3] J. T. M. Ingibergsson, U. P. Schultz, and D. Kraft, “Towards Declara-
tive Safety Rules for Perception Speciﬁcation Architectures,” submit-
ted to DSLRob Workshop 2015, 2015.

[4] MISRA, MISRA-C Guidelines for the Use of the C Language in
Critical Systems. Motor Industry Software Reliability Assoc., 2012.
[5] S. Adam, M. Larsen, K. Jensen, and U. P. Schultz, “Towards Rule-
Based Dynamic Safety Monitoring for Mobile Robots,” in SIMPAR,
Vol. 8810 in LNCS, pp. 207–218, Springer, 2014.

[6] M. Kragh, R. N. Jørgensen, and H. Pedersen, “Object Detection and
Terrain Classiﬁcation in Agricultural Fields Using 3D Lidar Data,”
in 10th International Conference on Computer Vision Systems, ICVS
2015.

[7] Piotr Doll´ar, Piotr’s Computer Vision Matlab Toolbox (PMT), at

http://vision.ucsd.edu/˜pdollar/toolbox/doc/
index.html .

