6
1
0
2

 
r
a

 

M
1
1

 
 
]

G
L
.
s
c
[
 
 

1
v
3
1
7
3
0

.

3
0
6
1
:
v
i
X
r
a

Cost-sensitive Learning for Bidding in

Online Advertising Auctions

Flavian Vasile1 and Damien Lefortier1,2

1 Criteo, Inc.

2 University of Amsterdam

{f.vasile, d.lefortier}@criteo.com

Abstract

One of the most challenging problems in computational advertising is the prediction
of ad click and conversion rates for bidding in online advertising auctions. State-of-
the-art prediction methods include using the maximum entropy framework (also known
as logistic regression) and log linear models. However, one unaddressed problem in
the previous approaches is the existence of highly non-uniform misprediction costs. In
this paper, we present our approach for making cost-sensitive predictions for bidding
in online advertising auctions. We show that one can get signiﬁcant lifts in ofﬂine and
online performance by using a simple modiﬁcation of the logistic loss function.

1

Introduction

Online advertising is becoming a big part of the global marketing reaching $170 billion revenue in 2015
[1]. Depending on the goal of the advertising campaign, different pricing schemes exist, but, out of them,
brand and performance advertising are the most prevalent. Brand advertising is used by advertisers that
want to maximize the exposure of their advertising message to online users and is priced in terms of num-
ber of ad impressions, with the cost usually referred as CPM (cost-per-mille). By contrast, performance
advertising is appealing to advertisers that are interested in reaching certain measurable goals such as
increased number of visits to their websites, increased number of leads, sales or downloads. In this case,
the cost is referred as CPC (cost-per-(ad)click) or CPA (cost-per-action/conversion).
The marketplace that makes online advertising possible is roughly formed out of three types of players,
namely the advertiser (the demand of ad display opportunities), the publisher (the offer of ad display op-
portunities) and the auction house, represented by an RTB (real-time bidding) platform. Most of the RTB
platforms function using a 2nd price model [2], where advertisers or agents representing the advertisers
bid for display opportunities, and the winner pays the maximum between the bid of the second participant
in the auction and the reserve price. In order to determine the winner for CPC and CPA clients, where
the pay-off to the publisher is conditioned on a user action, the bids get converted in expected gains (also
known as eCPMs) using click and conversion rate (CR) prediction models. One differentiating factor
between the click and conversion models, is that for conversion (sale) prediction models, the ratio of pos-
itives to negatives is much smaller, thus making the learning problem more challenging. For this reason,
we will concentrate our attention on improving the performance of conversion models.

1

One important aspect of the marketplace is that the numeric range of the possible CPAs is very large
and it depends on the economic value of the action that the advertiser is trying to incentivize for. The
resulting eCPMs vary from ones based on expectations over actions that are frequent and low-value (e.g.
conversions on classiﬁed ads1) and ones that are extremely rare and high-value (e.g. successful mortgage
applications). Importantly, an improvement in prediction performance on high CPA sales has a bigger
impact on the revenue than a similar improvement that affects low CPA trafﬁc. To take this into account
during evaluation, recently proposed metrics on bidders performance are making use of the advertisers’
CPAs [3, 4].

Recent business-aware ofﬂine metrics
Indeed, taking the example of a conversion-rate predictor, the
classical mean squared error (MSE) can be interpreted as the ofﬂine metric that penalizes the raw volume
of poorly explained observed sales. This metric can be extended to weight the display-level squared error
with the CPA of the corresponding advertiser and to therefore penalize the model proportionally with
the unexplained revenue (mentioned as a special case in [4]) — thus yielding a Weighted MSE (MSEW).
Another example is the Utility metric, recently proposed in [4], which takes into account both the potential
upside of the display represented by the CPA, but also the associated display cost (modeled as a Gamma
distribution over potential display costs given the observed cost). This metric can be interpreted as the
ofﬂine metric counterpart of the change in proﬁt. We detail all these metrics in Section 3.

Shortcomings of current approaches Current state-of-the-art response rate prediction methods range
from logistic regression [5, 6], to log-linear models [7], to a combination of log-linear models with deci-
sion tress [8], and to combining pure response rate prediction with ad ranking [9]. We see a discrepancy
between the CPA-aware ofﬂine metrics and the standard loss functions of the current prediction models,
such as the log loss function optimized in the logistic regression. As a result, we focus on the following
questions: Can our predictive models beneﬁt from taking into account the sales’ CPAs? How to better add
this information during learning in order to improve the performance of our models? To the best of our
knowledge, the only solution to this question was proposed recently in [3], where the authors design spe-
ciﬁc loss functions that take into account the bidder economic performance and which inspired the work
on the Utility metric in [4]. By comparison, we investigate the use of the well-established cost-sensitive
learning framework to tackle this task.

Proposal The learning framework where the misclassiﬁcation costs vary across examples is called
example-dependent cost-sensitive learning. In [10], the authors classify the cost-sensitive learning ap-
proaches in three main classes as follows. The ﬁrst approach makes learning cost-sensitive by adding
costs to the learner — this works for any model that falls under the statistical query model [11]. The
second approach changes the ﬁnal decision function and assigns each example to its lowest cost-sensitive
risk [12]. Finally, the third approach changes directly the training dataset by duplicating each example by
a factor proportional to its relative cost, which has the advantage that it works with any error minimizing
classiﬁer [10]. Our proposal falls under the ﬁrst approach, where we alter the loss function to take into
account the relative misprediction costs. One of the main reasons for our choice was the relative ease of
implementation in our current system.
In Section 2, we present our method for taking into account the advertisers’ CPAs using cost-sensitive
learning and discuss its impact on learning and regularization. In Section 3, we present the experimental
results when applying our method to improve a state-of-the-art conversion-rate prediction model used as
a sub-model for predicting either the expected number of sales or the expected sales amount generated by
a user following a display for bidding in online advertising auctions. We present both ofﬂine experiments
using a large real-world dataset collected at Criteo and online experiments on live trafﬁc through an A/B

1 Personal ads that usually contain personal services advertising, consumer-to-consumer home renting and home

selling offers.

2

W N LL =

1
N

(−yi log(pi) − (1 − yi) log(1 − pi)) ∗ ci

(1)

where N is the size of the dataset, yi is the true outcome, pi is the prediction and ci is the cost associated
with instance i.
Advertiser-constant revenue weighted logistic loss We propose a simple cost weighting scheme that
weighs each display by the CPA of the corresponding advertiser: ci = CP Ai for all displays i = 1..N,
where CP Ai is the CPA of the advertiser associated with the ith display. This is equivalent with gener-
ating a dataset where the examples from each advertiser are re-sampled proportionally with its CPA and
where all positive examples (sales) have equal economic value:

N(cid:88)

i

N(cid:88)

i

test. We show that our method brings statistically signiﬁcant improvements ofﬂine with a large effect size
on our metrics. We also show that our method brings signiﬁcant gains in online performance.

2 Cost-sensitive Logistic Regression

2.1 Cost-sensitive learning and Risk Minimization

As discussed in [3], current state-of-the-art models for online bidding suffer from ”misspeciﬁcation”2,
both due to omitted variables bias and due to functional form misspeciﬁcation. To improve the perfor-
mance of models under misspeciﬁcation, we show how to align better the loss function with the actual
ofﬂine evaluation metrics using a cost-sensitive approach. In order to extend the logistic regression to cost
sensitive tasks, we change the original loss function (NLL) to weighted negative log likelihood (denoted
as WNLL).

W N LLCP A =

1
N

(−yi log(pi) − (1 − yi) log(1 − pi)) ∗ CP Ai

(2)

The W N LLCP A loss is calibrated, i.e. is minimized by predicting the true conversion rate of the ad.

Relationship with U tility and M SEW Minimizing M SEW is equivalent with maximizing expected
proﬁt (a.ka. U tility) when display costs are uniform (see [4, 3]). By analogy, W N LLCP A is the logis-
tic counterpart of M SEW and in Section 3 we show that, empirically, it has good performance when
evaluated using both M SEW and U tility.

The alignment of N LL and W N LLCP A with ad revenue We expect that the weighted loss should
align more with the revenue than the original log loss. In order to check that, we simulate the impact of
a constant over-prediction of the true conversion rate: q = p × 1.2 on the N LL and W N LLCP A as a
function of an increasing p and decreasing associated CP A: pi ∈ [0.1%, 1%] and CP Ai = 1/pi. We
assume equal display trafﬁc for all i, so each i has equal revenue: Revenuei ∝ CP Ai × pi.
We plot in Figure 1 the per display expectations of: N LL, W N LLCP A and revenue, without (a) and with
(b) rescaling by substracting H(p), the entropy of the true conversion rate (thus obtaining the weighted
and un-weighted KL divergences of p and q: DKL(p|q) = H(p, q)− H(p)). We rescale by H(p) in order
to make the expectations of the loss due to over-prediction comparable across different CR levels. As
expected, we observe that the un-weighted loss puts more mass on higher CR areas, having no relationship
with the expected revenue, but that the weighted loss is extremely well-aligned with the expected revenue.

2A regression model is considered ”misspeciﬁed” when one of the variables is correlated with the error term.

3

(a)

(b)

Figure 1: The plots of the expectations of N LL, W N LLCP A and revenue over a CR range, without (a)

and with (b) rescaling by substracting H(p).

2.2

Impact of weighting on learning with SGD and L-BFGS

Let us now discuss how to do learning in the case of cost-sensitive logistic regression, i.e. using WNLLCPA
as loss function, using two well-known learning algorithms: limited memory BFGS [13] (L-BFGS) and
stochastic gradient descent [14] (SGD). These two algorithms are widely used for learning click and
conversion prediction models in the context of display advertising (see, e.g., [9, 8] for SGD, and [5, 15]
for L-BFGS initialized with SGD). Using an advertiser-constant revenue weighting scheme is equivalent
to deﬁning an importance weight for each display (see above). In our case, the weight is the average CPA
of the advertiser associated with each display.

2.2.1 Learning with importance weights
Batch case: L-BFGS

L-BFGS is a batch algorithm (as, e.g., gradient descent), which means that one pass over the data is
required before each update of the weights. In this case, using an importance weight for each example
during the learning is straightforward [16]. Indeed, in batch algorithms, we typically ﬁrst compute the
sum of the gradients of all examples before updating the weights once, and adding x time the gradient of
an example or adding x times the gradient once is the same when computing this sum. So we can simply
do that, i.e. multiplying the gradient by x – which is both easy and inexpensive too add in the code of
L-BFGS. Note that this is not an approximation.

Online case: SGD

On the other hand, SGD is an online, or streaming algorithm, which means that an update of the weights
is performed after seeing each example. In this case, adding an importance weight of x is not equivalent
to multiplying the gradient by x when doing the update as in the batch case covered above [17]. Indeed,
if we would have an example duplicated x times in the training set, we would do x independent updates,
which is not equivalent with doing a single update that is x times larger [17]. We could therefore do
x successive updates (and multiply the gradient by the fractional part if greater than 0 as CP Ai is not
necessarily an integer), which is less approximate but not efﬁcient (although, ideally, these updates would
be done at different times during the learning and not successively as the training set is often randomly

4

shufﬂed at the beginning). [17] also proposes an approximated way to perform importance weight aware
updates without doing x updates, which is time-consuming, and better than multiplying by x.
Here, we simply multiply the gradient with the importance weight CP Ai, which is an approximate solu-
tion, but straightforward to implement. As discussed above we use SGD only for warm-starting L-BFGS
and it seems that, in this case, even an approximate method of including importance weights is sufﬁcient
(see Section 3). Our experiments, excluded for brevity, show that this method works as well as the method
of doing CP Ai successive updates in our setting. Since this method showed no additional value, there
was no beneﬁt in exploring the method from [17].

2.2.2 Impact on the regularization parameter

In most applications, the logistic regression objective function is accompanied by a regularization term Ω
on the weights w (typically, L1 or L2 regularization terms) to prevent overﬁtting the training data. We
obtain the following loss function for regularized WNLLCPA:

(−yi log(pi) − (1 − yi) log(1 − pi)) ∗ CP Ai + λ ∗ Ω(w)

N(cid:88)

i

L =

1
N

where λ is a hyper-parameter to tune called the regularization parameter. In this paper, we use L2 reg-
ularization where Ω(w) = (cid:107)w(cid:107)2
2. Importantly, when doing cost-sensitive logistic regression, this hyper-
parameter λ needs to be adapted (as, e.g., when adding new features). To do that, we use the following
simple heuristic to adapt λ depending on the value of the importance weights used, i.e. of the average
CPA of each advertiser:

(cid:80)

λW N LLCP A = λN LL ∗

i CP Ai
N

We investigated how well this heuristic performs in comparison with the optimal value of λ (for results,
see Section 3).

3 Experiments

In this section, we present our experimental results when applying our method to improve a state-of-the-art
conversion-rate prediction model used as a sub-model for predicting either the expected number of sales
or the expected sales amount generated by a user following a display for bidding in online advertising
auctions. First, we present ofﬂine experiments using a data set of more than 3 billion examples with
binary labels (sale vs. no-sale) and hundreds of millions of unique attribute values from the production
click logs of Criteo from March 2015. Then, we present online experiments on live trafﬁc.

3.1 Ofﬂine metrics

For the ofﬂine evaluation of our WNLLCPA model, we use ofﬂine metrics that approximate the business
impact of the model change, namely Normalized Weighted MSE (NMSEW) and Utility (discussed in
Section 1). The Normalized Weighted MSE shows the relative improvement in MSEW of the model to be
evaluated versus a baseline predictor, in our case the average empirical CR rate of the dataset, similar to
the normalization in [8, 15]. We denote si the binary outcome variable indicating if there was a sale or
not, xi the input display features vector, and ci the display cost. In order to model ofﬂine the potential
change in proﬁt due to a prediction model change, the Utility measure has been proposed recently in [4].
Since the observed proﬁt in historical data is ﬁxed, the metric makes the assumption that the display costs

5

are coming from a second price auction and that they are generated according to a Gamma distribution
conditioned on the observed display cost: P(˜c|c) ∝ ˜cβc exp(−β˜c) (free parameter β).

Weighted Mean Squared Error (MSEW)

((si − p(xi) · CP Ai)2

Normalized Weighted Mean Squared Error (NMSEW)

(cid:88)

i

1
N
1 −

(Expected) Utility (cid:88)

M SEWM odel

(cid:90) p(xi)∗CP Ai

M SEWAverageEmpiricalCR

(si · CP Ai − ˜c)P(˜c|ci)d˜c

0

i

3.2 Ofﬂine results

We compare the difference in performance between the standard logistic regression NLL and the weighted
version WNLLCPA using L-BFGS initialized with SGD [5, 15]. Given the fact that our cost-weigthing
scheme re-weights each advertiser’s negative and positive examples by a constant, the resulting conver-
sion probabilities remain calibrated and we can make the loss function change only in the conversion-rate
model, while keeping the rest of the systems unchanged. For the regularization parameter λ, our exper-
iments show that using our heuristic (Section 2.2.2) allows us to reach results that are comparable with
the best λ (tuned manually), as shown by Figure 2. We see that the optimal lambda value increases the
performance on high CPA trafﬁc, but decreases the performance on other areas of trafﬁc (overall, the
heuristic lambda is within noise of the optimal one).
In our experiments, we therefore use the updated regularization parameter λ heuristic, which has the great
beneﬁt of avoiding to re-tuning λ every time the costs, i.e. CP Ai, change:

Figure 2: The relative lift in performance of W N LLCP A in terms of U tility(β2) as a function of the

regularization parameter λ with respect to λheuristic

The results in Table 1 are divided over two optimization tasks used at Criteo that aim to respectively
optimize the number of sales and the total value of sales for each advertiser. We observe that WNLLCPA
performs signiﬁcantly better than NLL on all metrics and with a higher magnitude than typically observed
model improvements.

6

Number of Sales
Sales Amount

N M SEW
+(11.34, 13.28)% +(5.66, 6.43)% +(4.27, 5.43)%
+(7.03, 10.39)%
+(7.79, 7.97)% +(1.53, 1.77)%

U tility(β1) U tility(β2)

Table 1: Weighting the Logistic Regression: NLL vs. WNLLCPA on the test set; (β2 > β1)

3.3 Online experiments

We ran an A/B test of the change of the loss function to WNLLCPA in the conversion-rate model for
both number of sales prediction and total value of sales prediction and we observed signiﬁcant savings
in display cost, coupled with an increase in sales performance (number of sales, total sales amount) for
the advertisers. Our change therefore resulted in a signiﬁcant positive impact on the projected long-term
revenue. In terms of development and operational costs, the change in the loss function took only a couple
of weeks to put in production, since the code change is minimal, as shown in Section 2. Furthermore, the
observed training time of the model did not change.

4 Conclusion

In this paper, we investigated the impact of applying the cost-sensitive learning framework in the context
of bidding in online advertising auctions. We introduced a problem-speciﬁc weighting scheme for the
logistic loss that we denoted WNLLCPA, which weights differently displays (and their associated sales)
from different advertisers, based on their expected value, namely their CPA. We applied this change to a
state-of-the-art conversion-rate prediction model used as a sub-model for predicting either the expected
number of sales or the expected sales amount generated by a user following a display. Compared with the
vanilla version of the logistic loss, we showed large improvements in ofﬂine metrics (Normalized Weighted
MSE, Utility), followed by meaningful impact on the business metrics in the live bidding system.
In the next steps, we will continue our research in two different directions: ﬁrst, we plan to compare our
current approach with other cost-sensitive learning methods, such as the “costing” approach [10]. As a
second direction, we will look into different problem-speciﬁc weighting schemes, such as weighting-by-
proﬁt and weighting with product-level cost estimators.

Acknowledgments We would like to thank our colleagues Olivier Chapelle, Nicolas Le Roux, Olivier
Koch, Etienne Sanson, Cyrille Dubarry, Alexandre Gilotte and Dmitry Pavlov for their useful comments
on early versions of this paper.

References

[1] “Digital advertising spend in 2015.” http://www.statista.com/statistics/237974/

online-advertising-spending-worldwide/. Accessed: 2015-10-15.

[2] B. Edelman, M. Ostrovsky, and M. Schwarz, “Internet advertising and the generalized second price
auction: Selling billions of dollars worth of keywords,” tech. rep., National Bureau of Economic
Research, 2005.

[3] P. Hummel and R. P. McAfee, “Loss functions for predicted click through rates in auctions for online

advertising,” Preprint, Google Inc, 2013.

7

[4] O. Chapelle, “Ofﬂine evaluation of response prediction in online advertising auctions,” in Proceed-
ings of the 24th International Conference on World Wide Web Companion, pp. 919–922, Interna-
tional World Wide Web Conferences Steering Committee, 2015.

[5] O. Chapelle, E. Manavoglu, and R. Rosales, “Simple and scalable response prediction for display
advertising,” ACM Transactions on Intelligent Systems and Technology (TIST), vol. 5, no. 4, p. 61,
2014.

[6] H. B. McMahan, G. Holt, D. Sculley, M. Young, D. Ebner, J. Grady, L. Nie, T. Phillips, E. Davydov,
D. Golovin, et al., “Ad click prediction: a view from the trenches,” in Proceedings of the 19th ACM
SIGKDD international conference on Knowledge discovery and data mining, pp. 1222–1230, ACM,
2013.

[7] D. Agarwal, R. Agrawal, R. Khanna, and N. Kota, “Estimating rates of rare events with multiple
hierarchies through scalable log-linear models,” in Proceedings of the 16th ACM SIGKDD interna-
tional conference on Knowledge discovery and data mining, pp. 213–222, ACM, 2010.

[8] X. He, J. Pan, O. Jin, T. Xu, B. Liu, T. Xu, Y. Shi, A. Atallah, R. Herbrich, S. Bowers, et al.,
“Practical lessons from predicting clicks on ads at facebook,” in Proceedings of 20th ACM SIGKDD
Conference on Knowledge Discovery and Data Mining, pp. 1–9, ACM, 2014.

[9] C. Li, Y. Lu, Q. Mei, D. Wang, and S. Pandey, “Click-through prediction for advertising in twit-
ter timeline,” in Proceedings of the 21th ACM SIGKDD International Conference on Knowledge
Discovery and Data Mining, pp. 1959–1968, ACM, 2015.

[10] B. Zadrozny, J. Langford, and N. Abe, “Cost-sensitive learning by cost-proportionate example
weighting,” in Data Mining, 2003. ICDM 2003. Third IEEE International Conference on, pp. 435–
442, IEEE, 2003.

[11] M. Kearns, “Efﬁcient noise-tolerant learning from statistical queries,” Journal of the ACM (JACM),

vol. 45, no. 6, pp. 983–1006, 1998.

[12] C. Elkan, “The foundations of cost-sensitive learning,” in International joint conference on artiﬁcial

intelligence, vol. 17, pp. 973–978, Citeseer, 2001.

[13] D. C. Liu and J. Nocedal, “On the limited memory bfgs method for large scale optimization,” Math-

ematical programming, vol. 45, no. 1-3, pp. 503–528, 1989.

[14] L. Bottou, “Large-scale machine learning with stochastic gradient descent,” in COMPSTAT ’10,

pp. 177–186, Springer, 2010.

[15] D. Lefortier, A. Truchet, and M. de Rijke, “Sources of variability in large-scale machine learning

systems,” in Machine Learning Systems (NIPS 2015 Workshop), 2015.

[16] J. Nocedal, “Updating quasi-newton matrices with limited storage,” Mathematics of computation,

vol. 35, no. 151, pp. 773–782, 1980.

[17] N. Karampatziakis and J. Langford, “Online importance weight aware updates,” arXiv preprint

arXiv:1011.1576, 2010.

8

