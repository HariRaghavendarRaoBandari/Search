Tree-to-Sequence Attentional Neural Machine Translation

Akiko Eriguchi, Kazuma Hashimoto, and Yoshimasa Tsuruoka
The University of Tokyo, 3-7-1 Hongo, Bunkyo-ku, Tokyo, Japan

{eriguchi, hassy, tsuruoka}@logos.t.u-tokyo.ac.jp

6
1
0
2

 
r
a

 

M
2
2

 
 
]
L
C
.
s
c
[
 
 

2
v
5
7
0
6
0

.

3
0
6
1
:
v
i
X
r
a

Abstract

Most of the existing neural machine trans-
lation (NMT) models focus on the con-
version of sequential data and do not di-
rectly take syntax into consideration. We
propose a novel end-to-end syntactic NMT
model, extending a sequence-to-sequence
model with the source-side phrase struc-
ture. Our model has an attention mech-
anism that enables the decoder to gener-
ate a translated word while softly align-
ing it with phrases as well as words of
the source sentence. Experimental re-
sults on the WAT’15 English-to-Japanese
dataset demonstrate that our proposed
model outperforms sequence-to-sequence
attentional NMT models and compares fa-
vorably with the state-of-the-art tree-to-
string SMT system.

Introduction

1
Machine Translation (MT) has traditionally been
one of the most complex language processing
problems, but recent advances of Neural Machine
Translation (NMT) make it possible to perform
translation using a simple end-to-end architecture.
In the Encoder-Decoder model (Cho et al., 2014b;
Sutskever et al., 2014), a Recurrent Neural Net-
work (RNN) called the encoder reads the whole
sequence of source words to produce a ﬁxed-
length vector, and then another RNN called the
decoder generates the target words from the vec-
tor. The Encoder-Decoder model has been ex-
tended with an attention mechanism (Bahdanau
et al., 2015; Luong et al., 2015a), which allows
the model to jointly learn the soft alignment be-
tween the source language and the target lan-
guage. NMT models have achieved state-of-the-
art results in English-to-French and English-to-

Figure 1: Alignment between an English phrase
and a Japanese word.

German translation tasks (Luong et al., 2015b; Lu-
ong et al., 2015a). However, it is yet to be seen
whether NMT is competitive with traditional SMT
approaches in translation tasks for structurally dis-
tant language pairs such as English-to-Japanese.

Figure 1 shows a pair of parallel sentences in
English and Japanese. English and Japanese are
linguistically distant in many respects; they have
different syntactic constructions, and words and
phrases are deﬁned in different lexical units.
In
this example, the Japanese word “ₐ” is aligned
to the English words “green” and “tea”, and the
English word sequence “a cup of” is aligned to
a special symbol “null”, which is not explicitly
translated into any Japanese word. One way to
solve this mismatch problem is to consider the
phrase structure of the English sentence and align
the phrase “a cup of green tea” to “ₐ”. How-
ever, the existing NMT models do not allow us to
perform this kind of alignment.

In this paper, we propose a novel attentional
NMT model to take advantage of syntactic in-
formation. Following the phrase structure of a
source sentence, we encode the sentence recur-
sively in a bottom-up fashion to produce a vector
representation of the sentence and decode it while
aligning the input phrases and words to the out-
put. Our experimental results on the English-to-

Japanese translation task show that our proposed
model achieves the best translation performance
among all the previous systems.

2 Neural Machine Translation
2.1 Encoder-Decoder Model
NMT is an end-to-end approach to data-driven
machine translation (Kalchbrenner and Blunsom,
2013; Sutskever et al., 2014; Bahdanau et al.,
2015). In other words, the NMT models directly
estimate the conditional probability p(y|x) given
a large collection of source and target sentence
pairs (x, y). An NMT model consists of an en-
coder process and a decoder process, and hence
they are often called Encoder-Decoder models. In
the Encoder-Decoder models, a sentence is treated
as a sequence of words.
In the encoder pro-
cess, the encoder embeds each of the source words
x = (x1, x2,··· , xn) into a d-dimensional vector
space. The decoder then outputs a word sequence
y = (y1, y2,··· , ym) in the target language given
the information on the source sentence provided
by the encoder. Here, n and m are the lengths
of the source and target sentences, respectively.
RNNs allow one to effectively embed sequential
data into the vector space.
In the RNN encoder, the i-th hidden unit hi ∈
Rd×1 is calculated given the i-th input xi and the
previous hidden unit hi−1 ∈ Rd×1,
hi = fenc(xi, hi−1),

(1)

where fenc is a non-linear function, and the initial
hidden unit h0 is usually set to zeros. The encod-
ing function fenc is recursively applied until the n-
th hidden unit hn is obtained. The RNN Encoder-
Decoder models assume that hn represents a vec-
tor of the meaning of the input sequence up to the
n-th word.

After encoding the whole input sentence into
the vector space, we decode it in a similar way.
The initial decoder unit s1 is initialized with the
input sentence vector (s1 = hn). Given the pre-
vious target word and the j-th hidden unit of the
decoder, the conditional probability that the j-th
target word is generated is calculated as follows:

p(yj|y<j, x) = g(yj−1, sj),

(2)

where g is a non-linear function. The j-th hidden
unit of the decoder is calculated as follows:

sj = fdec(yj−1, sj−1).

(3)

It is known that RNNs suffer from the vanishing
gradient problem through backpropagation (Ben-
gio et al., 1994), which makes it hard for RNNs
to handle long-range dependencies in the sequen-
tial data. One way to address this problem is using
Long Short-Term Memory (LSTM) units (Hochre-
iter and Schmidhuber, 1997; Gers et al., 2000) in
place of vanilla RNN units. The t-th LSTM unit
consists of several gates and two different types
of states: a hidden unit ht and a memory cell
ct ∈ Rd×1,

it = σ(W (i)xt + U (i)ht−1 + b(i)),
ft = σ(W (f )xt + U (f )ht−1 + b(f )),
ot = σ(W (o)xt + U (o)ht−1 + b(o)),
˜ct = tanh(W (˜c)xt + U (˜c)ht−1 + b(˜c)),
ct = it (cid:12) ˜ct + ft (cid:12) ct−1,
ht = ot (cid:12) tanh(ct),

(4)
where each of it, ft, ot and ˜ct ∈ Rd×1 denotes
an input gate, a forget gate, an output gate, and a
state for updating the memory cell, respectively.
W (·) ∈ Rd×d and U (·) ∈ Rd×d are weight ma-
trices, b(·) ∈ Rd×1 is a bias, and xt ∈ Rd×1 is
the word embedding of the t-th input word. σ(·)
is the logistic function, and the operator (cid:12) denotes
element-wise multiplication between vectors.

2.2 Encoder-Decoder Model with Attention

Mechanism

The NMT models with an attention mecha-
nism (Bahdanau et al., 2015; Luong et al., 2015a)
have been proposed to softly align each decoder
state with the encoder states. The attention mech-
anism allows the NMT models to explicitly quan-
tify how much each encoder state contributes to
the word prediction at each time step.

In the attentional NMT model in Luong et al.
(2015a), at the j-th step of the decoder process,
the alignment score αj(i) between the i-th source
hidden unit hi and the j-th target hidden unit sj is
calculated as follows:

(cid:80)n
exp(hi · sj)
k=1 exp(hk · sj)

,

αj(i) =

(5)
where hk · sj is the inner product of hi and sj,
which is used to directly calculate the similarity
score between hk and sj. The j-th context vector
dj is calculated as the summation vector weighted
by αj(i):

n(cid:88)

i=1

dj =

αj(i)hi.

(6)

Figure 2: Encoder-Decoder model with attention
mechanism.

To incorporate the attention mechanism into the
decoding process, the context vector is used for the
the j-th word prediction by putting an additional
hidden layer ˜sj:

˜sj = tanh(Wd[sj; dj] + bd),

(7)
where [sj; dj] ∈ R2d×1 is the concatenation of sj
and dj, and Wd ∈ Rd×2d and bd ∈ Rd×1 are a
weight matrix and a bias, respectively. The model
predicts the j-th word by using the softmax func-
tion:

p(yj|y<j, x) = softmax(Ws ˜sj + bs),

(8)
where Ws ∈ R|V |×d and bs ∈ R|V |×1 are a weight
matrix and a bias, respectively. |V | stands for the
size of the vocabulary of the target language. Fig-
ure 2 shows an example of the NMT model with
an attention mechanism.

2.3 Objective Function of NMT Model
The objective function to train the NMT models
is the sum of the log-likelihoods of the translation
pairs in the training data:

J(θ) =

1
|D|

(cid:88)

(x,y)∈D

Figure 3: Proposed model: Tree-to-sequence at-
tentional NMT model.

syntactic structure into consideration in the NMT
model. We focus on the phrase structure of a sen-
tence and construct a sentence vector from phrase
vectors in a bottom-up fashion. A sentence vector
in the tree-based encoder is therefore composed
of the structured information rather than the se-
quential data. Figure 3 shows our proposed model,
which we call a tree-to-sequence attentional NMT
model.

In Head-driven Phrase Structure Grammar
(HPSG) (Sag et al., 2003), a sentence is composed
of multiple phrase units and represented as a bi-
nary tree as shown in Figure 1. Following the
structure of the sentence, we construct a tree-based
encoder on top of the standard sequential encoder.
The k-th parent hidden unit h(phr)
for the k-th
phrase is calculated using the left and right child
hidden units hl

k as follows:

k and hr

k

h(phr)

k

= ftree(hl

k, hr

k),

(10)

log p(y|x),

(9)

where ftree is a non-linear function.

where D denotes a set of parallel sentence pairs.
The model parameters θ are learned through
Stochastic Gradient Descent (SGD).

3 Tree-to-Sequence Model with

Attentional Mechanism

3.1 Tree-based Encoder + Sequence Encoder
The exsiting NMT models treat a sentence as a se-
quence of words and neglects the structure of a
sentence inherent in language. We propose a novel
tree-based encoder in order to explicitly take the

We construct a tree-based encoder with LSTM
units.
In other words, each node in the binary
tree is represented with an LSTM unit. When ini-
tializing the leaf units of the tree-based encoder,
we employ the sequential LSTM units described
in Section 2.1. Each non-leaf node is also repre-
sented with an LSTM unit, and we employ Tree-
LSTM (Tai et al., 2015) to calculate the LSTM
unit of the parent node which has two child LSTM
∈ Rd×1 and the
units. The hidden unit h(phr)
∈ Rd×1 for the k-th parent
memory cell c(phr)

k

k

node are calculated as follows:

k + b(fl)),
k + b(fr)),

k + b(i)),

ik = σ(U (i)
l hl
k = σ(U (fl)
f l
l hl
k = σ(U (fr)
l hl
f r
ok = σ(U (o)
l hl
˜ck = tanh(U (˜c)

r hr
k + U (i)
(fl)
r hr
k + U
k + U (fr)
r hr
k + b(o)),
k + U (o)
r hr
r hr
k + U (˜c)
l hl
= ik (cid:12) ˜ck + f l
k (cid:12) cl
k + f r
= ok (cid:12) tanh(c(phr)
),

k + b(˜c)),
k (cid:12) cr
k,

c(phr)
k
h(phr)

k

k

k, f r

(11)
k , oj, ˜cj ∈ Rd×1 are an input
where ik, f l
gate, the forget gates for left and right child units,
an output gate, and a state for updating the mem-
ory cell, respectively. cl
k are the mem-
ory cells for the left and right child units, respec-
tively. U (·) ∈ Rd×d denotes a weight matrix, and
b(·) ∈ Rd×1 represents a bias.

k and cr

Since Tree-LSTM is a generalization of chain-
structured LSTM (Tai et al., 2015), our proposed
tree-based encoder with LSTM units is a natural
extension of the conventional sequential encoder.
Our method, Tree-LSTM on sequential LSTMs,
is different from the original Tree-LSTM in cal-
culating the Tree-LSTM units for the leaf nodes.
They use only word embeddings for the leaf nodes
without any contextual information. By contrast,
we let the sequential LSTM units substitute for
the leaves of the tree-based encoder, and the leaf
nodes capture contextual information. As a result,
phrase units of the tree-based encoder can be com-
posed of the child units with richer contextual in-
formation.

Initial Decoder Setting

3.2
Now, we have two different sentence vectors: one
is from sequence encoder and the other from the
tree-based encoder. As shown in Figure 3, we pro-
vide another Tree-LSTM unit which has the ﬁnal
sequential encoder unit (hn) and tree-based en-
coder unit (h(phr)
root ) as two child units and set it as
the initial decoder s1 as follows:

s1 = gtree(hn, h(phr)

root ),

(12)

where gtree is the same function as ftree with an-
other set of Tree-LSTM parameters. This initial-
ization allows the decoder to capture information
from both the sequential data and phrase struc-
tures. Zoph and Knight (2016) proposed a simi-
lar method using a Tree-LSTM for initializing the

n(cid:88)

2n−1(cid:88)

decoder, with which they translate multiple source
languages to one target language.

When the syntactic parser fails to output a parse
tree for a sentence, we encode the sentence with
the sequential encoder by setting h(phr)
root = 0. Our
proposed tree-based encoder therefore works with
any sentences.

3.3 Attentional Mechanism in Our Model
We adopt the attention mechanism into our tree-
to-sequence model in a novel way. It gives atten-
tion not only to sequential hidden units but also to
phrase hidden units. This attentional mechanism
tells us which words or phrases in the source sen-
tence are important when the model decodes a tar-
get word. The j-th context vector dj is composed
of the sequential and the phrase vectors:

dj =

αj(i)hi +

αj(i)h(phr)

i

.

(13)

i=1

i=n+1

We should note that a binary tree has n − 1 phrase
nodes if it has n leaves. We set a ﬁnal decoder ˜sj
in the same way as Equation (7).

the

adopt

In addition, we

input-feeding
method (Luong et al., 2015a) in our model, which
is a method for feeding ˜sj−1, the previous unit
to predict the word yj−1, into the current target
hidden unit sj,

sj = fdec(yj−1, sj−1, ˜sj−1),

(14)

where ˜sj−1 is concatenated with sj−1.
It con-
tributes to the enrichment in the calculation of the
decoder, because ˜sj−1 is a very informative unit
which can be used not only to predict the output
word but also to be compacted with attentional
context vectors.

3.4 Sampling-Based Approximation to the

NMT Models

The computational bottleneck of training the NMT
models is in the calculation of the softmax layer
described in Equation (8), since the computational
cost increases linearly with the size of the vocab-
ulary (Jean et al., 2015).
In order to speed up
the training, we employ BlackOut (Ji et al., 2016),
a sampling-based approximation method. Black-
Out works very fast in RNN Language Models
(RNNLMs) even with a million word vocabulary.
At each word prediction step in the training,
BlackOut estimates the conditional probability in

Train
Development
Test

Sentences
1,346,946
1,790
1,812

Parsed successfully
1,346,946
1,789
1,811

sentence pairs
|V | in English
|V | in Japanese

Train (small)
100,000
25,478
23,532

Train (large)
1,346,946
87,796
65,680

Table 1: Dataset in ASPEC corpus.

Table 2: Training dataset and the vocabulary sizes.

Equation (2) for the target word and K negative
samples using a weighted softmax function. The
negative samples are drawn from the unigram dis-
tribution raised to the power α ∈ [0, 1] (Mikolov
et al., 2013). BlackOut is shown to be closely re-
lated to Noise Contrastive Estimation (NCE) (Gut-
mann and Hyv¨arinen, 2012) and achieves bet-
ter perplexity than the original softmax and NCE
in RNNLMs. We therefore investigate whether
BlackOut is also effective in training the NMT
models. Note that BlackOut can be used as the
original softmax once the training is ﬁnished.

4 Experiments

4.1 Training Data
We applied the proposed model to the English-
to-Japanese translation dataset given in WAT’151.
Following Zhu (2015), we extracted the ﬁrst 1.5
million translation pairs from the training data.
To obtain the phrase structures of the source
sentences, i.e., English, we used the probabilis-
tic HPSG parser Enju (Miyao and Tsujii, 2008).
For the target language, i.e., Japanese, we used
KyTea (Neubig et al., 2011), a Japanese segmenta-
tion tool, and performed the pre-processing steps
recommended in WAT’15 2. We then ﬁltered out
the translation pairs whose sentence lengths are
longer than 50 and whose source sentences are not
parsed successfully. Table 1 shows the details of
the datasets used in our experiments. We carried
out two experiments on a small training dataset
to investigate the effectiveness of our proposed
model and on a large training dataset to compare
our proposed methods with the submitted systems.
The vocabulary is constructed by words ob-
served in the training data more than or equal to
N times. We set N = 2 for the small training
dataset and N = 5 for the large training dataset.
The Out-Of-Vocabulary (OOV) words are mapped
to the special token “unk”. We add another special
symbol “eos” for both languages and insert it at

the end of all the sentences. Table 2 shows the de-
tails of each training dataset and its corresponding
vocabulary size.

4.2 Training Details
The biases, softmax parameters, and BlackOut pa-
rameters are initialized with zeros. Parameter α
of BlackOut is set to 0.4 as recommended by Ji et
al. (2016). Following J´ozefowicz et al. (2015), we
initialize the biases of the forget gates of LSTM
and Tree-LSTM with 1.0. The remaining model
parameters in the NMT models in our experiments
are uniformly initialized in [−0.1, 0.1]. The model
parameters are optimized by the mini-batch size
of 128 with plain SGD. The initial learning rate of
SGD is 1.0. We halve the learning rate in SGD
every epoch after 5 epochs (Luong et al., 2015a).
Gradient norms are clipped to 3 to avoid exploding
gradient problems (Pascanu et al., 2012).

Our code is implemented in C++ using the
Eigen library3, a template library for linear alge-
bra, and we run all of the experiments on multi-
core CPUs4. It takes about a week to train a model
on the large training dataset.
Small Training Dataset We conduct experi-
ments with our proposed model and the attentional
NMT model with input-feeding. Each model has
256-dimensional hidden units and word embed-
dings. The number of negative samples K of
BlackOut is set to 500 or 2000.
Large Training Dataset Our proposed model
has 512-dimensional hidden units and word em-
beddings. K is set to 2500.

4.3 Decoding process
We use beam search to decode a target sentence
for an input sentence x and calculate the sum
of the log-likelihoods of the target sentence y =
(y1,··· , ym) as the beam score:

score(x, y) =

log p(yj|y<j, x).

(15)

m(cid:88)

j=1

1http://orchid.kuee.kyoto-u.ac.jp/WAT/
2http://orchid.kuee.kyoto-u.ac.jp/WAT/

baseline/dataPreparationJE.html

3http://eigen.tuxfamily.org/index.php
416 threads on Intel(R) Xeon(R) CPU E5-2667 v3 @

3.20GHz

Decoding in the NMT models is a generative pro-
cess and depends on the target language model
given a source sentence. The score becomes
smaller as the sentence becomes longer, and thus
simple beam search does not work well when de-
coding a long sentence (Cho et al., 2014a; Pouget-
Abadie et al., 2014).
In our preliminary experi-
ments, the beam search with the length normal-
ization in Cho et al. (2014a) was not effective
in English-to-Japanese translation. The method
in Pouget-Abadie et al. (2014) needs to estimate
the conditional probability p(x|y) using another
NMT model and thus is not suitable for our work.
In this paper, we use statistics on sentence
lengths in beam search. Assuming that the length
of a target sentence correlates with the length of
a source sentence, we redeﬁne the score of each
candidate as follows:

m(cid:88)

score(x, y) = Lx,y +

log p(yi|y<i, x), (16)

Lx,y = log p(len(y)|len(x)),

i=1

(17)

where Lx,y is the penalty for the conditional prob-
ability of the target sentence length len(y) given
the source sentence length len(x).
It allows
the model to decode a sentence by considering
the length of the target sentence.
In our exper-
iments, we computed the conditional probability
p(len(y)|len(x)) in advance following the statis-
tics collected in the ﬁrst one million pairs of the
training dataset. We allow the decoder to generate
up to 100 words.

4.4 Evaluation

We evaluate the models by two automatic eval-
uation metrics RIBES (Isozaki et al., 2010) and
BLEU (Papineni et al., 2002) following WAT’15.
We use the KyTea-based evaluation script for the
translation results5. The RIBES score is a metric
based on rank correlation coefﬁcients with word
precision, and the BLEU score is based on n-gram
word precision and a brevity penalty for outputs
shorter than the references. RIBES is known to
have stronger correlation with human judgements
than BLEU in translation between English and
Japanese as discussed in Isozaki et al. (2010).

5http://lotus.kuee.kyoto-u.ac.jp/WAT/
evaluation/automatic_evaluation_systems/
automaticEvaluationJA.html

5 Results and Discussion
5.1 Small Training Dataset
Table 3 shows the perplexity, BLEU and RIBES
on the development data with the Attentional
NMT (ANMT) models trained on the small
dataset. We conducted the experiments with our
proposed method using BlackOut and softmax.
We decode a translation by our proposed beam
search with a beam size of 20.

As shown in Table 3, the results of our proposed
model with BlackOut improve as the number of
negative samples K increases. Although the result
of softmax is better than those of BlackOut (K =
500, 2000), the training time of softmax is about
three times longer than that of BlackOut even with
the small dataset.

As to the results of the ANMT model, reversing
the word order in the input sentence decreases the
scores in English-to-Japanese translation, which
contrasts with the results of other language pairs
reported in previous work (Sutskever et al., 2014;
Luong et al., 2015a). By taking syntactic infor-
mation into consideration, our proposed model
improves the scores, compared to the sequential
attention-based approach.

We have found that better perplexity does not
always lead to better translation scores with Black-
Out as shown in Figure 3. One of the possible rea-
sons is that BlackOut distorts the word distribution
by the modiﬁed unigram-based negative sampling
where frequent words can be treated as the nega-
tive samples multiple times at each training step.
Table 4 shows the development results of our
proposed method with BlackOut (K = 2000)
by simple beam search and our proposed beam
search. The beam size is set to 6 or 20 in the sim-
ple beam search, and to 20 in our proposed search.
We can see that our proposed search outperforms
simple beam search in both scores. Unlike RIBES,
the BLEU score is sensitive to the beam size and
becomes lower as the size increases. We found
that the brevity penalty had a relatively large im-
pact on the BLEU score in simple beam search as
the beam size increased. Our search method works
better than simple beam search by keeping long
sentences in the candidates with a large beam size.

5.2 Large Training Dataset
Table 5 shows the experimental results of BLEU
and RIBES scores achieved by the trained mod-
els on the large dataset. We decode the target sen-

Perplexity RIBES BLEU Time/epoch (min.)

Proposed model
Proposed model
Proposed model (Softmax)
ANMT (Luong et al., 2015a)
+ reverse input
ANMT (Luong et al., 2015a)
+ reverse input

K
500
2000
—
500
500
2000
2000

19.6
21.0
17.9
21.6
22.6
23.1
26.1

71.8
72.6
73.2
70.7
69.8
71.5
69.5

20.0
20.5
21.8
18.5
17.7
19.4
17.5

55
70
180
45
45
60
60

Table 3: Evaluation results on the development data using the small training data. The training time per
epoch is also shown, and K is the number of negative samples in BlackOut.

Simple beam search
Proposed Beam Search

Beam size RIBES BLEU
20.0
19.5
20.5

72.3
72.3
72.6

6
20
20

Table 4: Effects of the beam search on the devel-
opment data.

tences by our proposed beam search with the beam
size of 206.
Comparison with the NMT models The model
of Zhu (2015) is the ANMT model (Bahdanau et
al., 2015) with a bi-directional LSTM encoder,
and uses 1024-dimensional hidden units and 1000-
dimensional word embeddings. The model of
Lee et al. (2015) is also the ANMT model (Bah-
danau et al., 2015) with a bi-directional Gated
Recurrent Unit (GRU) encoder, and uses 1000-
dimensional hidden units and 200-dimensional
word embeddings. Our single proposed atten-
tional NMT model outperforms the best result of
Zhu (2015)’s end-to-end NMT model with ensem-
ble and unknown replacement by +1.16 RIBES
and by +0.46 BLEU. Our RIBES score is higher
than those of Zhu (2015)’s best model which is
a hybrid of the attentional NMT and Statistical
Machine Translation (SMT) models and Lee et al.
(2015)’s attentional NMT model with rich prepro-
cessing and special character-based decoding.
Comparison with the SMT models PB, HPB
and T2S are the baseline SMT systems in
WAT’15: a phrase-based model, a hierarchical
phrase-based model, and a tree-to-string model,
respectively (Nakazawa et al., 2015). The state-of-
the-art model in WAT’15 is Neubig et al. (2015)’s
tree-to-string SMT model enhanced with rerank-
ing by the ANMT model (Bahdanau et al., 2015)
using a bi-directional LSTM encoder. Our pro-

6We found one sentence which did not end with eos, and
then we decoded it again with the beam size of 1000 follow-
ing Zhu (2015).

Model
Proposed model
ANMT with LSTMs (Zhu, 2015)
+ Ensemble, unk replacement
+ System combination,

3 pre-reordered ensembles

ANMT with GRUs (Lee et al., 2015)
+ English spell correction,

character-based decoding

RIBES BLEU
81.42
34.65
79.70
32.19
34.19
80.27
80.91
36.21

81.15

35.75

PB baseline
HPB baseline
T2S baseline
T2S model (Neubig and Duh, 2014)
+ ANMT Rerank (Neubig et al., 2015)

69.19
74.70
75.80
79.65
81.38

29.80
32.56
33.44
36.58
38.17

Table 5: Evaluation results on the test data.

posed end-to-end NMT model compares favorably
with Neubig et al. (2015).

5.3 Qualitative Analysis
We illustrate the translation of test data by our
model and several attentional relations when de-
coding a sentence.
In Figure 4, an English sen-
tence represented as a binary tree is translated into
Japanese and several attentional relations between
English words or phrases and Japanese word are
shown with the attentional score α. We can see
the target words softly alined to source words and
phrases.

As to Figure 4 (a), “ᨔ” in Japanese means
“liquid crystal”, and it has the stronger attentional
score (α = 0.77) with the English phrase “liq-
uid crystal for active matrix”. This is because the
j-th target hidden unit sj has the contextual in-
formation about the previous words y<j and our
proposed model aligned it to the proper phrase
unit of “liquid crystal for active matrix”. As
shown in Figure 4 (b), our proposed model softly
aligned the Japanese word “” to the phrase
“the cells” with the highest alignment score.
In
Japanese, there is no deﬁnite article like “the” in
English, and it is usually aligned to null described
as Section 1. Our proposed model can ﬂexibly

porate structural information into the NMT mod-
els, Cho et al. (2014a) proposed to jointly learn
structures inherent in source-side languages but
did not report improvement of translation perfor-
mance. These studies motivated us to investigate
the role of syntactic structures explicitly given by
existing syntactic parsers in the NMT models.

The attention mechanism (Bahdanau et al.,
2015) has promoted NMT onto the next stage. It
enables the NMT models to translate while align-
ing the source and target. Luong et al. (2015a)
reﬁned the attention model so that it can dynam-
ically focus on local windows rather than the en-
tire sentence, and they also proposed a more ef-
fective path in the calculation of the attentional
NMT model. Subsequently, several attentional
NMT models have been proposed (Cheng et al.,
2015; Cohn et al., 2016); however each model is
based on the existing sequential attentional mod-
els and does not focus on a syntactic structure of
languages.

7 Conclusion

In this paper, we propose a novel syntactic ap-
proach that extends attentional NMT models. We
focus on the phrase structure of the input sen-
tence and build a tree-based encoder following to
the parsed tree. Our proposed tree-based encoder
is a natural extension of the sequential encoder
model, where the leaf units of the tree-LSTM
in the encoder can work together with the origi-
nal sequential LSTM encoder. Moreover, the at-
tention mechanism allows the tree-based encoder
to align not only the input words but also input
phrases with the output words. Experimental re-
sults on the WAT’15 English-to-Japanese transla-
tion dataset demonstrate that our proposed model
achieves the best RIBES score and outperforms
the sequential attentional NMT model.

Acknowledgments

This work was supported by CREST, JST, and
JSPS KAKENHI Grant Number 15J12597.

References
[Bahdanau et al.2015] Dzmitry Bahdanau, Kyunghyun
Cho, and Yoshua Bengio. 2015. Neural Machine
Translation by Jointly Learning to Align and Trans-
late. In Proceedings of the 3rd International Con-
ference on Learning Representations.

Figure 4: Example of a translation and the atten-
tional relations by our proposed model.

learn the attentional relations between English and
Japanese.

We found that our model translated the word
“active” into “ឋ”, a synonym of the reference
word “”. We also found similar exam-
ples in other sentences, where our model output
synonyms of the reference words, e.g. “᝕” and
“᝕ ឋ” (“female” in English) and “NASA” and
“̶” (“National Aeronautics and Space
Administration, NASA” in English). These trans-
lations are penalized in terms of BLEU scores, but
they do not necessarily mean that the translations
were wrong. This point may be supported by the
fact that the NMT models were highly evaluated
in WAT’15 by crowd sourcing (Nakazawa et al.,
2015).

6 Related Work

Kalchbrenner and Blunsom (2013) were the ﬁrst
to propose an end-to-end NMT model using Con-
volutional Neural Networks (CNNs) as the source
encoder and using RNNs as the target decoder.
The Encoder-Decoder model can be seen as an
extension of their model, and it replaces CNNs
with RNNs using GRUs (Cho et al., 2014b) or
LSTMs (Sutskever et al., 2014) as the source en-
coder.

Sutskever et al. (2014) have shown that mak-
ing the input sequences reversed is effective in a
French-to-English translation task, and the tech-
nique has also proven effective in translation tasks
between other European language pairs (Luong et
al., 2015a). All of the NMT models mentioned
above are based on sequential encoders. To incor-

[Bengio et al.1994] Yoshua Bengio, Patrice Simard,
and Paolo Frasconi. 1994. Learning Long-Term De-
pendencies with Gradient Descent is Difﬁcult. IEEE
Transactions on Neural Networks, 5(2):157–166.

[Cheng et al.2015] Yong Cheng, Shiqi Shen, Zhongjun
He, Wei He, Hua Wu, Maosong Sun, and Yang Liu.
2015. Agreement-based Joint Training for Bidirec-
tional Attention-based Neural Machine Translation.
arXiv: 1512.04650.

Linguistics and the 7th International Joint Confer-
ence on Natural Language Processing (Volume 1:
Long Papers), pages 1–10.

[Ji et al.2016] Shihao Ji, S. V. N. Vishwanathan, Na-
dathur Satish, Michael J. Anderson, and Pradeep
Dubey. 2016. BlackOut: Speeding up Recurrent
Neural Network Language Models With Very Large
In Proceedings of the 4th Interna-
Vocabularies.
tional Conference on Learning Representations.

[Cho et al.2014a] KyungHyun Cho, Bart van Merrien-
boer, Dzmitry Bahdanau, and Yoshua Bengio.
2014a. On the Properties of Neural Machine Trans-
lation: Encoder-Decoder Approaches. In Proceed-
ings of Eighth Workshop on Syntax, Semantics and
Structure in Statistical Translation (SSST-8).

[J´ozefowicz et al.2015] Rafal

J´ozefowicz, Wojciech
Zaremba, and Ilya Sutskever. 2015. An Empiri-
cal Exploration of Recurrent Network Architectures.
In Proceedings of the 32nd International Confer-
ence on Machine Learning, volume 37, pages 2342–
2350.

[Cho et al.2014b] Kyunghyun Cho, Bart van Merrien-
boer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi
Bougares, Holger Schwenk, and Yoshua Bengio.
2014b. Learning Phrase Representations using RNN
Encoder–Decoder for Statistical Machine Transla-
In Proceedings of the 2014 Conference on
tion.
Empirical Methods in Natural Language Process-
ing, pages 1724–1734.

[Cohn et al.2016] Trevor Cohn, Cong Duy Vu Hoang,
Ekaterina Vymolova, Kaisheng Yao, Chris Dyer,
and Gholamreza Haffari.
Incorporating
Structural Alignment Biases into an Attentional
In Proceedings of the
Neural Translation Model.
15th Annual Conference of
the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies. to appear.

2016.

[Gers et al.2000] Felix A. Gers, J¨urgen Schmidhuber,
and Fred A. Cummins. 2000. Learning to Forget:
Continual Prediction with LSTM. Neural Computa-
tion, 12(10):2451–2471.

[Gutmann and Hyv¨arinen2012] Michael U. Gutmann
and Aapo Hyv¨arinen. 2012. Noise-contrastive esti-
mation of unnormalized statistical models, with ap-
plications to natural image statistics. Journal of Ma-
chine Learning Research, 13(1):307–361, February.

[Hochreiter and Schmidhuber1997] Sepp Hochreiter
and J¨urgen Schmidhuber. 1997. Long Short-Term
Memory. Neural Computation, 9(8):1735–1780,
November.

[Isozaki et al.2010] Hideki

Isozaki, Tsutomu Hirao,
Kevin Duh, Katsuhito Sudoh, and Hajime Tsukada.
2010. Automatic Evaluation of Translation Quality
In Proceedings of the
for Distant Language Pairs.
2010 Conference on Empirical Methods in Natural
Language Processing, pages 944–952.

[Jean et al.2015] S´ebastien Jean, Kyunghyun Cho,
Roland Memisevic, and Yoshua Bengio. 2015. On
Using Very Large Target Vocabulary for Neural Ma-
In Proceedings of the 53rd An-
chine Translation.
nual Meeting of the Association for Computational

[Kalchbrenner and Blunsom2013] Nal Kalchbrenner
and Phil Blunsom. 2013. Recurrent continuous
In Proceedings of the 2013
translation models.
Conference on Empirical Methods
in Natural
Language Processing, pages 1700–1709.

[Lee et al.2015] Hyoung-Gyu Lee, JaeSong Lee, Jun-
Seok Kim, and Chang-Ki Lee. 2015. NAVER Ma-
In Pro-
chine Translation System for WAT 2015.
ceedings of the 2nd Workshop on Asian Translation
(WAT2015), pages 69–73.

[Luong et al.2015a] Thang Luong, Hieu Pham, and
Christopher D. Manning.
2015a. Effective Ap-
proaches to Attention-based Neural Machine Trans-
In Proceedings of the 2015 Conference on
lation.
Empirical Methods in Natural Language Process-
ing, pages 1412–1421, September.

[Luong et al.2015b] Thang Luong,

Ilya Sutskever,
Quoc Le, Oriol Vinyals, and Wojciech Zaremba.
2015b. Addressing the Rare Word Problem in Neu-
ral Machine Translation. In Proceedings of the 53rd
Annual Meeting of the Association for Computa-
tional Linguistics and the 7th International Joint
Conference on Natural Language Processing (Vol-
ume 1: Long Papers), pages 11–19.

[Mikolov et al.2013] Tomas Mikolov, Ilya Sutskever,
Kai Chen, Greg S Corrado, and Jeff Dean. 2013.
Distributed representations of words and phrases
In Advances in Neu-
and their compositionality.
ral Information Processing Systems 26, pages 3111–
3119.

[Miyao and Tsujii2008] Yusuke Miyao and Jun’ichi
Tsujii. 2008. Feature Forest Models for Proba-
bilistic HPSG Parsing. Computational Linguistics,
34(1):35–80, March.

[Nakazawa et al.2015] Toshiaki Nakazawa, Hideya
Mino, Isao Goto, Graham Neubig, Sadao Kuro-
hashi, and Eiichiro Sumita. 2015. Overview of
In Pro-
the 2nd Workshop on Asian Translation.
ceedings of the 2nd Workshop on Asian Translation
(WAT2015), pages 1–28.

[Neubig and Duh2014] Graham Neubig and Kevin
Duh. 2014. On the elements of an accurate tree-
In Proceed-
to-string machine translation system.
ings of the 52nd Annual Meeting of the Association
for Computational Linguistics (Volume 2: Short Pa-
pers), pages 143–149.

[Zoph and Knight2016] Barret Zoph and Kevin Knight.
In Pro-
2016. Multi-Source Neural Translation.
ceedings of the 15th Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies.
to appear.

[Neubig et al.2011] Graham Neubig, Yosuke Nakata,
and Shinsuke Mori. 2011. Pointwise Prediction for
Robust, Adaptable Japanese Morphological Analy-
sis. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 529–533.

[Neubig et al.2015] Graham Neubig, Makoto Mor-
ishita, and Satoshi Nakamura.
2015. Neural
Reranking Improves Subjective Quality of Machine
Translation: NAIST at WAT2015. In Proceedings of
the 2nd Workshop on Asian Translation (WAT2015),
pages 35–41.

[Papineni et al.2002] Kishore Papineni, Salim Roukos,
Todd Ward, and Wei-Jing Zhu.
2002. BLEU:
A Method for Automatic Evaluation of Machine
In Proceedings of the 40th Annual
Translation.
Meeting on Association for Computational Linguis-
tics, pages 311–318.

[Pascanu et al.2012] Razvan Pascanu, Tomas Mikolov,
and Yoshua Bengio. 2012. Understanding the ex-
ploding gradient problem. arXiv: 1211.5063.

[Pouget-Abadie et al.2014] Jean
Bart

Pouget-Abadie,
Dzmitry Bahdanau,
van Merrienboer,
Kyunghyun Cho, and Yoshua Bengio.
2014.
Overcoming the curse of sentence length for neural
machine translation using automatic segmentation.
In Proceedings of SSST-8, Eighth Workshop on
Syntax, Semantics and Structure in Statistical
Translation, pages 78–85.

[Sag et al.2003] Ivan A. Sag, Thomas Wasow, and
Emily Bender. 2003. Syntactic Theory: A Formal
Introduction. Center for the Study of Language and
Information, Stanford, 2nd edition.

[Sutskever et al.2014] Ilya Sutskever, Oriol Vinyals,
2014. Sequence to Sequence
and Quoc V. Le.
In Advances in
Learning with Neural Networks.
Neural Information Processing Systems 27, pages
3104–3112.

[Tai et al.2015] Kai Sheng Tai, Richard Socher, and
Christopher D. Manning. 2015. Improved Semantic
Representations From Tree-Structured Long Short-
In Proceedings of the
Term Memory Networks.
53rd Annual Meeting of the Association for Compu-
tational Linguistics and the 7th International Joint
Conference on Natural Language Processing (Vol-
ume 1: Long Papers), pages 1556–1566.

[Zhu2015] Zhongyuan Zhu. 2015. Evaluating Neural
Machine Translation in English-Japanese Task.
In
Proceedings of the 2nd Workshop on Asian Transla-
tion (WAT2015), pages 61–68.

