6
1
0
2

 
r
a

M
4

 

 
 
]

.

R
P
h
t
a
m

[
 
 

1
v
9
9
4
1
0

.

3
0
6
1
:
v
i
X
r
a

Mesoscopic eigenvalue statistics of Wigner matrices

Yukun He∗

Antti Knowles†

March 7, 2016

We prove that the linear statistics of the eigenvalues of a Wigner matrix converge to a universal
Gaussian process on all mesoscopic spectral scales, i.e. scales larger than the typical eigenvalue
spacing and smaller than the global extent of the spectrum.

1. Introduction

Let H be an N×N Wigner matrix – a Hermitian random matrix with independent upper-triangular
entries with zero expectation and constant variance. We normalize H so that as N → ∞ its
spectrum converges to the interval [−2, 2], and therefore its typical eigenvalue spacing is of order
N−1. In this paper we study linear eigenvalue statistics of H of the form

(cid:18) H − E

(cid:19)

η

Tr f

,

(1.1)

where f is a test function, E ∈ (−2, 2) a ﬁxed reference energy inside the bulk spectrum, and η
an N -dependent spectral scale. We distinguish the macroscopic regime η (cid:16) 1, the microscopic
regime η (cid:16) N−1, and the mesoscopic regime N−1 (cid:28) η (cid:28) 1. The limiting distribution of (1.1) in
macroscopic regime is by now well understood; see [2]. Conversely, in the microscopic regime the
limiting distribution of (1.1) is governed by the distribution of individual eigenvalues of H. This
question has recently been the focus of much attention, and the universality of the emerging Wigner-
Dyson-Mehta (WDM) microscopic eigenvalue statistics for Wigner matrices has been established
in great generality; we refer to the surveys [9, 14] for further details.

In this paper we focus on the mesoscopic regime. The study of linear eigenvalue statistics of
Wigner matrices on mesoscopic scales was initiated in [18,19]. In [18], the authors consider the case
of Gaussian H (the Gaussian Orthogonal Ensemble) and take f (x) = (x− i)−1, in which case (1.1)
is η times the trace of the resolvent of H at E + iη. Under these assumptions, it is proved in [18]
that, after a centring, the linear statistic (1.1) converges in distribution to a Gaussian random
variable on all mesoscopic scales N−1 (cid:28) η (cid:28) 1.
In [19] this result was extended to a class of
Wigner matrices for the range of mesoscopic scales N−1/8 (cid:28) η (cid:28) 1. Recently, the results of [19]
were extended in [17] to arbitrary Wigner matrices, mesoscopic scales N−1/3 (cid:28) η (cid:28) 1, and general
test functions f subject to mild regularity and decay conditions. Apart from the works [17, 19] on
Wigner matrices, mesoscopic eigenvalue statistics have also been analysed for invariant ensembles;
see [5, 8] and the references therein.

∗ETH Z¨urich, Departement Mathematik. Email: yukun.he@math.ethz.ch.
†ETH Z¨urich, Departement Mathematik. Email: knowles@math.ethz.ch.

1

(cid:90) (cid:18) f (x) − f (y)

(cid:19)2

EZ(f )2 =

1
2π2

(cid:90)

1
π

|ξ|| ˆf (ξ)|2 dξ ,

Let Z = (Z(f ))f denote the Gaussian process obtained as the mesoscopic limit of a centring
of (1.1). From the works cited above, it is known that the variance of Z(f ) is the square of the
Sobolev H 1/2-norm of f :

x − y

dx dy =

where ˆf (ξ) ..= (2π)−1/2(cid:82) f (x)e−iξx dx. Hence, a remarkable property of Z is scale invariance: Z d=

Zλ, where Zλ(f ) ..= Z(fλ) and fλ(x) ..= f (λx). It may be shown that Z is obtained by extrapolating
the microscopic WDM statistics to mesoscopic scales, and we therefore refer to its behaviour as the
WDM mesoscopic statistics. In light of the microscopic universality results mentioned above, the
emergence of WDM statistics on mesoscopic scales is therefore not surprising.

(1.2)

All of the models described above, including Wigner matrices, correspond to mean-ﬁeld models
without spatial structure. In [10, 11], linear eigenvalue statistics were analysed for band matrices,
where matrix entries are set to be zero beyond a certain distance W (cid:54) N from the diagonal. Band
matrices are a commonly used model of quantum transport in disordered media. Wigner matrices
can be regarded as a special case W = N of band matrices. Unlike the mean-ﬁeld Wigner matrices,
band matrices possess a nontrivial spatial structure. An important motivation for the study of
mesoscopic eigenvalue statistics of band matrices arises from the theory of conductance ﬂuctuations;
we refer to [10] for more details. The results of [10,11] hold in the regime W −1/3 (cid:28) η (cid:28) 1, and hence
for the special case of Wigner matrices they hold for N−1/3 (cid:28) η (cid:28) 1. An important conclusion
of [10, 11] is that for band matrices there is a sharp transition in the mesoscopic spectral statistics,
predicted in the physics literature [1]: above a certain critical spectral scale ηc the mesoscopic
spectral statistics are no longer governed by the Wigner-Dyson-Mehta mesoscopic statistics (1.2),
but by new limiting statistics, referred to as Altshuler-Shklovskii (AS) statistics in [10, 11], which
are not scale invariant like (1.2). For instance for the d-dimensional (d = 1, 2, 3) AS statistics, the
right-hand side of (1.2) is replaced by 1
π
range of mesoscopic scales such that, although the microscopic eigenvalue statistics are expected to
satisfy the WDM statistics, the mesoscopic statistics do not, and instead satisfy the AS statistics.
Hence, the WDM statistics on microscopic and mesoscopic scales do in general not come hand in
hand.
In this paper we establish the WDM mesoscopic statistics for Wigner matrices in full generality.
Our results hold on all mesoscopic scales N−1 (cid:28) η (cid:28) 1 and all Wigner matrices whose entries
have ﬁnite moments of order 4 + o(1). We require our test functions to have 1 + o(1) continuous
derivatives and be subject to mild decay assumptions, as in [17]. The precise statements are given
in Section 2 below.

(cid:82) |ξ|1−d/2 | ˆf (ξ)|2 dξ; see [10, 11]. In particular, there is a

Our proof is based on two main ingredients: families of self-consistent equations for moments
of linear statistics inspired by [19], and the local semicircle law for Wigner matrices from [12, 16].
Our analysis of the self-consistent equations departs signiﬁcantly from that of [19], since repeating
the steps there, even using the optimal bounds provided by the local semicircle law, requires the
lower bound η (cid:29) N−1/2 on the spectral scale. In addition, dealing with general test functions f
instead of f (x) = (x− i)−1 requires a new family of self-consistent equations that is combined with
the Helﬀer-Sj¨ostrand representation for general functions of H. We perform the proof in two major
steps.
In the ﬁrst step, performed in Section 4, we consider traces of resolvents G ≡ G(z) = (H − z)−1,
corresponding to taking f (x) = (x − i)−1 in (1.1). Denoting by G the normalized trace of G and
(cid:104)X(cid:105) ..= X − EX, we derive a family of self-consistent equations (see (4.22) below) for the moments
E(cid:104)G∗(cid:105)n(cid:104)G(cid:105)m following [19], obtained by expanding one factor inside the expectation using the
resolvent identity and then applying a standard cumulant expansion (see Lemma 3.1 below) to

2

the resulting expression of the form Ef (h)h. The main work of the ﬁrst step is to estimate the
error terms of the self-consistent equation. An important ingredient is a careful estimate of the
remainder term in the cumulant expansion (see Lemma 4.6 (i) below), which allows us to remove
the condition η (cid:29) N−1/2 on the spectral scale that would be required if one merely combined the
local semicircle law with the approach of [19]. Other important tools behind these estimates are
new precise high-probability bounds on the entries of the powers Gk of the resolvent (see Lemma
4.4 below) and a further family of self-consistent equations for EGk (see Lemma 4.8 below).

In the second step, performed in Section 5, we consider general test functions f . The starting
point is the well-known Helﬀer-Sj¨ostrand respresentation of (1.1) as an integral of traces of resol-
vents. An important ingredient of the proof is a self-consistent equation (see (5.21) below) that is
used on the integrand of the Helﬀer-Sj¨ostrand representation. Compared to the ﬁrst step, we face
the additional diﬃculty that the arguments z of the resolvents are now integrated over, and may in
particular have very small imaginary parts. Handling such integrals for arbitrary mesoscopic scales
η and comparatively rough test functions in C1+o(1) requires some care, and we use two diﬀerent
truncation scales N−1 (cid:28) ω (cid:28) σ (cid:28) η in the imaginary part of z, which allow us to extract the
leading term. See Section 5 for a more detailed explanation of the truncation scales. The error
terms are estimated by a generalization of the estimates established in the ﬁrst step.

Finally, in Section 6 we give a simple truncation and comparison argument that allows us to
consider without loss of generality Wigner matrices whose entries have ﬁnite moments of all order,
instead of ﬁnite moments of order 4 + o(1).

Conventions. We regard N as our fundamental large parameter. Any quantities that are not
explicitly constant or ﬁxed may depend on N ; we almost always omit the argument N from our
notation. We use C to denote a generic large positive constant, which may depend on some ﬁxed
parameters and whose value may change from one expression to the next. Similarly, we use c to
denote a generic small positive constant.

2. Results

We begin this section by deﬁning the class of random matrices that we consider.
Definition 2.1 (Wigner matrix). A Wigner matrix is a Hermitian N ×N matrix H = H∗ ∈ CN×N
whose entries Hij satisfy the following conditions.

(i) The upper-triangular entries (Hij

.. 1 (cid:54) i (cid:54) j (cid:54) N ) are independent.

(ii) We have EHij = 0 for all i, j, and E|√
(iii) There exists constants c, C > 0 such that E|√
We distinguish the real symmetric case, where Hij ∈ R for all i, j, and the complex Hermitian case,
where EH 2

N Hij|4+c−2δij (cid:54) C for all i, j.

N Hij|2 = 1 for i (cid:54)= j.

ij = 0 for i (cid:54)= j.

For conciseness, we state our results for the real symmetric case. Analogous results hold for the
Our ﬁrst result is on the convergence of the trace of the resolvent G(z) ..= (H − z)−1, where

complex Hermitian case; see Remark 2.4 below.
Im z (cid:54)= 0. The Stieltjes transform of the empirical spectral measure of H is

Tr G(z) .

(2.1)

G(z) ..=

1
N

3

For x ∈ R, z ∈ C, and Im z (cid:54)= 0, the Wigner semicircle law  and its Stieltjes transform m are
deﬁned by

(2.2)
Denote by H ..= {z ∈ C : Im z > 0} the complex upper half-plane. Let (Y (b))b∈H denote the
complex-valued Gaussian process with mean zero and covariance

x − z

dx .

(x) ..=

1
2π

(4 − x2)+ ,

m(z) ..=

(cid:90) (x)

(cid:112)

E(Y (b1)Y (b2)) = −
for all b1, b2 ∈ H. For instance, we can set

2

(b1 − b2)2

(cid:18) 2

(cid:19)2 ∞(cid:88)

b + i

k=0

,

√

E(Y (b1)Y (b2)) = 0

(cid:18) b − i

(cid:19)k

b + i

ϑk ,

k + 1

(2.3)

(2.4)

Y (b) =

1√
2

where (ϑk)∞
k=0 is a family of independent standard complex Gaussians, where, by deﬁnition, a
standard complex Gaussian is a mean-zero Gaussian random variable X satisfying EX 2 = 0 and
E|X|2 = 1. Finally, for E ∈ R and η > 0, we deﬁne the process ( ˆY (b))b∈H through

ˆY (b) ..= N η (G(E + bη) − m(E + bη))

for all b ∈ H. We may now state our ﬁrst result.
Theorem 2.2 (Convergence of the resolvent). Let H be a real symmetric Wigner matrix.
Fix α ∈ (0, 1) and set η ..= N−α. Fix E ∈ (−2, 2). Then the process ( ˆY (b))b∈H converges in the
sense of ﬁnite-dimensional distributions to (Y (b))b∈H as N → ∞. That is, for any ﬁxed p and
b1, b2, ...bp ∈ H, we have

d−→ (Y (b1), ..., Y (bp))

(2.5)

as N → ∞.

( ˆY (b1), ..., ˆY (bp))

Our second result is on the convergence of the trace of general functions of H. For ﬁxed r, s > 0,
denote by C1,r,s(R) the space of all real-valued C1-functions f such that f(cid:48) is r-H¨older continuous
uniformly in x, and |f (x)| +|f(cid:48)(x)| = O((1 +|x|)−1−s). Let (Z(f ))f∈C1,r,s(R) denote the real-valued
Gaussian process with mean zero and covariance

E(Z(f1)Z(f2)) =

dx dy

(2.6)

for all f1, f2 ∈ C1,r,s(R) (see also (1.2)). Our next result is the weak convergence of the process

1
2π2

(cid:90) (f1(x) − f1(y))(f2(x) − f2(y))
(cid:19)
(cid:18) H − E

(cid:18) x − E

(x − y)2

(cid:90) 2

(cid:19)

(x)f

−2

η

dx ,

−

η

ˆZ(f ) ..= Tr f

(2.7)

where f ∈ C1,r,s(R). We may now state our second result.
Theorem 2.3 (Convergence of general test functions). Let H be a real symmetric Wigner
matrix. Fix α ∈ (0, 1) and set η ..= N−α. Fix E ∈ (−2, 2). Then the process ( ˆZ(f ))f∈C1,r,s(R)
converges in the sense of ﬁnite-dimensional distributions to (Z(f ))f∈C1,r,s(R) as N → ∞. That is,
for any ﬁxed p and f1, f2, ..., fp ∈ C1,r,s(R), we have

( ˆZ(f1), ..., ˆZ(fp))

d−→ (Z(f1), ..., Z(fp))

(2.8)

as N → ∞.

4

Remark 2.4. In the complex Hermitian case, Theorems 2.2 and 2.3 remain true up to an additional
factor 1/2 in the covariances. More precisely, if H is a complex Wigner matrix then (2.5) is replaced
by

( ˆY (b1), ..., ˆY (bp))

(Y (b1), ..., Y (bp))

d−→ 1√
2

and (2.8) by

( ˆZ(f1), ..., ˆZ(fp))

d−→ 1√
2

(Z(f1), ..., Z(fp)) .

(2.9)

(2.10)

The minor modiﬁcations to the proof in the complex Hermitian case are given in Section 7 below.

3. Tools

Let M be an N × N matrix. We use the notations Mij

The rest of this paper is devoted to the proofs of Theorems 2.2 and 2.3. In this section we collect
notations and tools that are used throughout the paper.
ij ≡
(M∗)ij = M ji. We denote by (cid:107)M(cid:107) the operator norm of M , and abbreviate M ..= 1
N Tr M . It
is easy to see that (cid:107)G(E + iη)(cid:107) (cid:54) |η|−1. For σ > 0, we use N (0, σ2) to denote the real Gaussian
random variable with mean zero and variance σ2, and NC(0, σ2) d= σNC(0, 1) the complex Gaussian
random variable with mean zero and variance σ2. We abbreviate (cid:104)X(cid:105) ..= X − EX for any random
variable X with ﬁnite expectation. Finally, if h is a real-valued random variable with ﬁnite moments
of all order, we denote by Ck(h) the kth cumulant of h, i.e.

n ≡ (Mij)n, M∗n ≡ (M∗)n, M∗

Ck(h) ..= (−i)k ·(cid:0)∂k

λ log Eeiλh(cid:1)(cid:12)(cid:12)λ=0 .

(3.1)

We now state the cumulant expansion formula that is a central ingredient of the proof. The
formula is analogous to the corresponding formula in [19], and its proof is obtained as a minor
modiﬁcation whose details we omit.

Lemma 3.1 (Cumulant expansion). Let h be a real-valued random variable with ﬁnite moments
of all order, and f a complex-valued smooth function on R. Then for any ﬁxed l ∈ N we have

Ef (h)h =

Rl+1 = O(1) · E(cid:12)(cid:12)hl+2 · 1{|h|>N τ−1/2}

1
k!

Ck+1(h)Ef (k)(h) + Rl+1 ,

l(cid:88)
(cid:12)(cid:12) ·(cid:13)(cid:13)f (l+1)(cid:13)(cid:13)∞ + O(1) · E|h|l+2 ·

k=0

(cid:12)(cid:12)f (l+1)(x)(cid:12)(cid:12) .

(3.2)

(3.3)

sup

|x|(cid:54)N τ−1/2

provided all expectations in (3.2) exist. For any ﬁxed τ > 0, the remainder term Rl+1 satisﬁes

The bulk of the proof is performed on Wigner matrices satisfying a stronger condition than

Deﬁnition 2.1 (iii) by having entries with ﬁnite moments of all order.

Definition 3.2. We consider the subset of Wigner matrices obtained from Deﬁnition 2.1 by re-
placing (iii) with

(iii)’ For each p ∈ N there exists a constant Cp such that E|√

N Hij|p (cid:54) Cp for all N, i, j.

We focus on Wigner matrices satisfying Deﬁnition 3.2 until Section 6, where we explain how
to relax the condition (iii)’ to (iii) using a Green function comparison argument; see Section 6 for
more details.

We shall deduce Theorem 2.3 from Theorem 2.2 using the Helﬀer-Sj¨ostrand formula [7], which

is summarized in the following result whose standard proof we omit.

5

Lemma 3.3 (Helffer-Sj¨ostrand formula). Let f ∈ C1,r,s(R) with some r, s > 0. Let ˜f be the
almost analytic extension of f deﬁned by

˜f (x + iy) ..= f (x) + i(f (x + y) − f (x)) .

(3.4)

If f is further in C2(R), we can also set

(3.5)
c (R) be a cutoﬀ function satisfying χ(0) = 1, and by a slight abuse of notation write

Let χ ∈ C∞
χ(z) ≡ χ(Im z). Then for any λ ∈ R we have

˜f (x + iy) ..= f (x) + iyf(cid:48)(x) .

(cid:90)

f (λ) =

1
π

∂¯z( ˜f (z)χ(z))

λ − z

d2z ,

(3.6)

C

where ∂¯z

..= 1

2 (∂x + i∂y) is the antiholomorphic derivative and d2z the Lebesgue measure on C.

The following deﬁnition introduces a notion of a high-probability bound that is suited for our

purposes. It was introduced (in a more general form) in [12].
Definition 3.4 (Stochastic domination). Let
X = (X (N )(u) : N ∈ N, u ∈ U (N )) ,

Y = (Y (N )(u) : N ∈ N, u ∈ U (N ))

be two families of nonnegative random variables, where U (N ) is a possibly N -dependent parameter
set. We say that X is stochastically dominated by Y , uniformly in u, if for all (small) ε > 0 and
(large) D > 0 we have

(3.7)
for large enough N (cid:62) N0(ε, D). If X is stochastically dominated by Y , we use the notation X ≺ Y .
The stochastic domination will always be uniform in all parameters, such as z and matrix indices,
that are not explicitly constant.

sup
u∈U (N )

X (N )(u) > N εY (N )(u)

(cid:105) (cid:54) N−D

P(cid:104)

We end the section with the local semicircle law for Wigner matrices from [12, 16]. For a recent
survey of the local semicircle law, see [3], where the following version of the local semicircle law is
stated.
Theorem 3.5 (Local semicircle law). Let H be a Wigner matrix satisfying Deﬁnition 3.2, and
deﬁne the spectral domain

S ..= {E + iη : |E| (cid:54) 10, 0 < η (cid:54) 10} .

(cid:115)

|Gij(z) − δijm(z)| ≺

max

i,j

Im m(z)

N η

+

1
N η

(3.8)

Then we have the bounds

and

(3.9)
uniformly in z = E + iη ∈ S. Moreover, outside the spectral domain we have the stronger estimates

,

|G(z) − m(z)| ≺ 1
N η

and

uniformly in z ∈ H \ S.

max

i,j

|Gij(z) − δijm(z)| ≺ 1√
N

|G(z) − m(z)| ≺ 1
N

,

6

(3.10)

(3.11)

4. Convergence of the resolvent

In this section we prove the following weaker form of Theorem 2.2.
Theorem 4.1. Theorem 2.2 holds for Wigner matrices H satisfying Deﬁnition 3.2, and the con-
vergence also holds in the sense of moments.

For the statements of the following results we abbreviate G ≡ G(E+iη) and [G] ..= G−m(E+iη).

The following result is a special case of Theorem 4.1.
Proposition 4.2. Under the assumptions of Theorem 4.1 we have

N η [G]

d−→ NC

0,

(cid:16)

(cid:17)

1
2

as N → ∞. The convergence also holds in the sense of moments.

The main work in this section is to show the one-dimensional case from Proposition 4.2, whose
proof can easily be extended to the general case of Theorem 4.1 (see Section 4.4 below). Recall the
notation (cid:104)X(cid:105) ..= X − EX. Proposition 4.2 is a direct consequence of the following lemma.
Lemma 4.3. Under the assumptions of Theorem 4.1 the following holds.

(i) For ﬁxed m, n ∈ N satisfying m + n (cid:62) 2 we have

 n!

E(cid:104)G∗(cid:105)n(cid:104)G(cid:105)m =

2n N 2n(α−1) + O(N 2n(α−1)−c0)
O(N (m+n)(α−1)−c0)

if m = n
if m (cid:54)= n ,

(4.1)

(4.2)

(4.3)

(4.4)

where

(ii) We have

c0 ≡ c0(α) ..=

1
3

min{α, 1 − α} .

[G] − (cid:104)G(cid:105) = EG − m(E + iη) = O(N α−1−c0) ,

with c0 deﬁned in (4.3).

As advertised, Proposition 4.2 follows immediately from Lemma 4.3. Indeed, suppose Lemma
4.3 holds. From (4.2) we ﬁnd that N 1−α(cid:104)G(cid:105) converges to NC(0, 1/2) in the sense of moments, and
hence also in distribution. Proposition 4.2 therefore follows from (4.4).

The bulk of this section, Sections 4.1–4.3, is devoted to the proof of Lemma 4.3.

4.1. Preliminary estimates on G. We begin with estimates on the entries of Gk. For k (cid:62) 2,
the bounds provided by the following lemma are signiﬁcantly better than those obtained for Gk by
applying the estimate (3.8) to each entry of the matrix product. For instance, a straightforward
application of (3.8) yields |(Gk)ij| ≺ N k(1+α)/2−1, which is not enough to conclude the proof of
Lemma 4.3. The following result yields bounds that grow slower with k and in addition provide
extra smallness for the oﬀdiagonal entries of Gk. Both of these features are necessary for the proof
of Lemma 4.3.
Lemma 4.4. Under the conditions of Theorem 4.1, for any ﬁxed k ∈ N+ we have

as well as

uniformly in i, j.

(cid:12)(cid:12)(cid:0)Gk(cid:1)

ij

(cid:12)(cid:12) ≺

if i = j
if i (cid:54)= j ,

(4.5)

(4.6)

(cid:12)(cid:12)(cid:10)Gk(cid:11)(cid:12)(cid:12) ≺ N kα−1
(cid:40)

N (k−1)α
N (k−1/2)α−1/2

7

Proof. We ﬁrst prove (4.5). The case k = 1 is easy. Indeed, from (3.9) we get

|(cid:104)G(cid:105)| (cid:54) |[G]| + |E[G]| ≺ N α−1

Next, for k (cid:62) 2 we write

(4.7)
as desired, where we used the deﬁnition of ≺ combined with the trivial bound (cid:107)G(cid:107) (cid:54) N α to estimate
E[G].

(cid:18) (H − E)/η + i
(cid:18) H − E
x2+1 )k. Note that f : R → C is smooth, and for any n ∈ N, |f (n)(x)| =
(cid:1), we obtain from Lemma 3.3 that
where we deﬁned f (x) ..= ( x+i
O((1 + |x|)−2). We deﬁne ˜f as in (3.5) and let χ be as in Lemma 3.3 and satisfy χ(y) = 1 for

|y| (cid:54) 1. Writing fη(x) ..= f(cid:0) x−E

(cid:19)k · η−k = f

(H − E)2/η2 + 1

· η−k ,

Gk =

(cid:19)

(4.8)

η

η

(cid:90)

fη(H) =

1
π

∂¯z( ˜fη(z)χ(z/η))

H − z

d2z ,

C

so that

(cid:10)Gk(cid:11) =

(cid:90)

1

2πηk

R2

(cid:18)

iyf(cid:48)(cid:48)

η (x)χ(y/η) +

f(cid:48)
η(x)χ(cid:48)(y/η)

In order to estimate the right-hand side, we use (3.9) and (3.11) to obtain

(cid:19)(cid:10)G(x + iy)(cid:11) dx dy .

(4.9)

(4.10)

i
η

fη(x)χ(cid:48)(y/η) − y
η

(cid:12)(cid:12)(cid:10)G(x + iy)(cid:11)(cid:12)(cid:12) ≺ 1
(cid:90)

N|y|

(cid:90)

uniformly for |y| ∈ (0, 1) and x. Hence,

(cid:12)(cid:12)(cid:12)iyf(cid:48)(cid:48)
η (x)χ(y/η)(cid:10)G(x + iy)(cid:11)(cid:12)(cid:12)(cid:12)dx dy ≺ 1

1

2πηk

R2

2πηk

R2

(cid:12)(cid:12)(cid:12) 1

N

(cid:12)(cid:12)(cid:12)dx dy =

f(cid:48)(cid:48)
η (x)χ(y/η)

O(1)
N ηk .

(4.11)

(Note that the use of stochastic domination inside the integral requires some justiﬁcation. In fact,
we use that a high-probability bound of the form (4.10) holds simultaneously for all x ∈ R and
|y| ∈ (0, 1). We refer to [3, Remark 2.7 and Lemma 10.2] for further details.) Similarly, by our
choice of χ, we ﬁnd

(cid:90)

(cid:12)(cid:12)(cid:12)(cid:12) i

η

fη(x)χ(cid:48)(y/η)(cid:10)G(x + iy)(cid:11)(cid:12)(cid:12)(cid:12)(cid:12)dx dy ≺ 1

(cid:90)

1

2πηk

R2

(cid:12)(cid:12)(cid:12)(cid:12)dx dy =

N η2 fη(x)χ(cid:48)(y/η)

(cid:12)(cid:12)(cid:12)(cid:12) 1

O(1)
N ηk .

An analogous estimate yields

(cid:90)

2πηk

|y|(cid:62)η

(cid:12)(cid:12)(cid:12)(cid:12) y

η

η(x)χ(cid:48)(y/η)(cid:10)G(x + iy)(cid:11)(cid:12)(cid:12)(cid:12)(cid:12)dx dy ≺ 1

f(cid:48)

N ηk .

1

2πηk

R2

Altogether we have |(cid:104)Gk(cid:105)| ≺ N kα−1, which is (4.5).

Next, we prove (4.6). For k = 1, we use the well-known bound |m(z)| < 1, which follows using

an elementary estimate from the fact that m is the unique solution of

satisfying Im m(z) Im z > 0. Thus by (3.8) we have

m(z) +

1

m(z)

+ z = 0

(cid:40)

|(G)ij| ≺

1
N (α−1)/2

if i = j
if i (cid:54)= j ,

(4.12)

(4.13)

which is (4.6) for k = 1. The extension to k (cid:62) 2 follows again using Lemma 3.1, and we omit the
details.

8

Lemma 4.4 is very useful in estimating the expectations involving entries of G, in combination

with the following elementary result about stochastic domination.

(i) If X1 ≺ Y1 and X2 ≺ Y2 then X1X2 ≺ Y1Y2.

Lemma 4.5.
(ii) Suppose that X is a nonnegative random variable satisfying X (cid:54) N C and X ≺ Φ for some

deterministic Φ (cid:62) N−C. Then EX ≺ Φ.

N Hii|2, we ﬁnd from Deﬁnition 3.2 (iii)’
4.2. Proof of Lemma 4.3 (i). Abbreviating ζi
that ζi = O(1). We write z ..= E + iη and often omit the argument z from our notation. Note that
G = (H − z)−1 and G∗ = (H − ¯z)−1. In particular, Theorem 3.5 also holds for G∗ with obvious
modiﬁcations accounting for the diﬀerent sign of η. For m, n (cid:62) 1, we need to compute

..= E|√

E(cid:104)G∗(cid:105)n(cid:104)G(cid:105)m = E(cid:104)(cid:104)G(cid:105)m−1(cid:104)G∗(cid:105)n(cid:105)G .

By the resolvent identity we have

G =

1
z

GH − 1
z

so that

E(cid:104)G∗(cid:105)n(cid:104)G(cid:105)m =

1
z

E(cid:104)(cid:104)G∗(cid:105)n(cid:104)G(cid:105)m−1(cid:105)GH =

Since H is symmetric, for any diﬀerentiable f = f (H) we set

∂

∂Hij

f (H) =

∂

∂Hji

f (H) ..=

d
dt

E(cid:104)(cid:104)G∗(cid:105)n(cid:104)G(cid:105)m−1(cid:105)GijHji .

1
zN

I ,

(cid:88)
(cid:12)(cid:12)(cid:12)t=0
f(cid:0)H + t ∆(ij)(cid:1) ,

i,j

where ∆(ij) denotes the matrix whose entries are zero everywhere except at the sites (i, j) and (j, i)
kl = (δikδjl + δjkδil)(1 + δij)−1. We then compute the last averaging in
where they are one: ∆(ij)
(4.15) using the formula (3.2) with f = fij(H) ..= (cid:104)(cid:104)G∗(cid:105)n(cid:104)G(cid:105)m−1(cid:105)Gij, h = Hji, and obtain

i,j

(cid:88)
(cid:88)
(cid:88)

i,j

i,j

E ∂((cid:104)(cid:104)G∗(cid:105)n(cid:104)G(cid:105)m−1(cid:105)Gij)

∂Hji

E(cid:104)(cid:104)G∗(cid:105)n(cid:104)G(cid:105)m−1(cid:105) ∂Gij
∂Hji
E ∂((cid:104)(cid:104)G∗(cid:105)n(cid:104)G(cid:105)m−1(cid:105))

∂Hji

(1 + δji(ζi − 1)) + L

(1 + δji)

Gij(1 + δji) + K + L

=.. (a) + (b) + K + L ,

E ∂((cid:104)(cid:104)G∗(cid:105)n(cid:104)G(cid:105)m−1(cid:105)Gii)

∂Hii

(ζi − 2)

zE(cid:104)G∗(cid:105)n(cid:104)G(cid:105)m =

=

1
N 2

1
N 2

+

1
N 2

K = N−2(cid:88)
(cid:34) l(cid:88)

i

1
k!

i,j

k=2

L = N−1 ·(cid:88)

(cid:35)

.

where

and

(4.14)

(4.15)

(4.16)

(4.17)

(4.18)

(4.19)

(4.20)

Here l is a ﬁxed positive integer to be chosen later, and R(ji)
to Rl+1 in (3.2). More precisely, we have the bound

l+1 is a remainder term deﬁned analogously

+ R(ji)
l+1

k

∂Hji

Ck+1(Hji)E ∂k((cid:104)(cid:104)G∗(cid:105)n(cid:104)G(cid:105)m−1(cid:105)Gij)
(cid:12)(cid:12) ·(cid:13)(cid:13)∂l+1
(cid:0)H(cid:1)(cid:13)(cid:13)∞
(cid:0)H (ij) + x∆(ij)(cid:1)(cid:12)(cid:12) ,

l+2(cid:12)(cid:12) · E sup

l+21{|Hji|>N τ−1/2}

(cid:12)(cid:12)∂l+1

ji fij

ji fij

l+1 = O(1) · E(cid:12)(cid:12)Hji
+ O(1) · E(cid:12)(cid:12)Hji

R(ji)

x

9

where we deﬁne H (ij) ..= H − Hij∆(ij), so that the matrix H (ij) has zero entries at the positions
(i, j) and (j, i), and abbreviate ∂ij

. Note that for G = (H − z)−1 we have

..= ∂
∂Hij

= −(GikGlj + GilGkj)(1 + δkl)−1 ,

(4.21)

which gives

∂Gij
∂Hkl

(a) = N−2(cid:88)

i,j

E(cid:104)(cid:104)G∗(cid:105)n(cid:104)G(cid:105)m−1(cid:105)(−GijGij − GiiGjj)

= −N−1E(cid:104)G∗(cid:105)n(cid:104)G(cid:105)m−1(cid:104)G2(cid:105) − E(cid:104)G∗(cid:105)n(cid:104)G(cid:105)m−1(cid:104)G2(cid:105)
= −N−1E(cid:104)G∗(cid:105)m(cid:104)G(cid:105)m−1(cid:104)G2(cid:105) − E(cid:104)G∗(cid:105)n(cid:104)G(cid:105)m(cid:104)G(cid:105)
− 2E(cid:104)G∗(cid:105)n(cid:104)G(cid:105)mEG + E(cid:104)G∗(cid:105)m(cid:104)G(cid:105)m−1E(cid:104)G(cid:105)2 .

(cid:2)nE(cid:104)G∗(cid:105)n−1(cid:104)G(cid:105)m−1GG∗2 + (m − 1)E(cid:104)G∗(cid:105)n(cid:104)G(cid:105)m−2G3(cid:3) .

Similarly, a straightforward calculation gives

(b) = − 2
N 2

Altogether we obtain

E(cid:104)G∗(cid:105)n(cid:104)G(cid:105)m =

E(cid:104)G∗(cid:105)n(cid:104)G(cid:105)m−1E(cid:104)G(cid:105)2

1
T

E(cid:104)G∗(cid:105)n(cid:104)G(cid:105)m+1 − 1
T
1
+
T N
− K
T

E(cid:104)G∗(cid:105)n(cid:104)G(cid:105)m−1(cid:104)G2(cid:105) +
− L
T

2n
N 2T

+

2m − 2
N 2T

E(cid:104)G∗(cid:105)n−1(cid:104)G(cid:105)m−1GG∗2 ,

E(cid:104)G∗(cid:105)n(cid:104)G(cid:105)m−2G3

(4.22)

where T ..= −z − 2EG. From (3.9), (4.12), and Lemma 4.5 it is easy to see that

(cid:12)(cid:12)(cid:12) 1

T

(cid:12)(cid:12)(cid:12) = O(1) ,

and the implicit constant depends only on the distance to the spectral edge

κ ..= 2 − |E| .

In (4.22), the last term is the leading term. The calculation of (4.22) consists of computing the
leading term and estimating the subleading terms. We aim to show that the subleading terms are
of order N (m+n)(α−1)−c0.

We begin with L. For k (cid:62) 2 deﬁne

..= N−(k+3)/2(cid:88)

Jk

(4.23)

(4.24)

(4.25)

(4.26)

(4.27)

Lemma 4.6. Let R(ji)
(i) For any ﬁxed D0 > 0 there exists some l0 = l0(D0) (cid:62) 2 such that

l+1 be as in (4.19).

(ii) For all ﬁxed k (cid:62) 2 we have

where c0 is deﬁned in (4.3).

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) .

k

i,j

∂Hji

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)E ∂k((cid:104)(cid:104)G∗(cid:105)n(cid:104)G(cid:105)m−1(cid:105)Gij)
(cid:88)
l0+1 = O(cid:0)N−D0(cid:1) .
Jk = O(cid:0)N (m+n)(α−1)−c0(cid:1) ,

R(ji)

i,j

10

Before proving Lemma 4.6, we show how to use it to estimate L. By setting D0 = (m+n)(1−α)

in (4.26), we obtain

for some l0 (cid:62) 2. From Deﬁnition 3.2 (iii)’ we get

for all k (cid:62) 2. Thus (4.27) and (4.28) together imply

(cid:88)

i,j

R(ji)

l0+1 = O(cid:0)N (m+n)(α−1)(cid:1)
(cid:12)(cid:12)Ck(Hji)(cid:12)(cid:12) = O(N−k/2)
L = O(cid:0)N (m+n)(α−1)−c0(cid:1) ,

i,j

max

(4.28)

(4.29)

as desired.
Proof of Lemma 4.6 (i). Let D0 > 0 be given. Fix i, j, and choose τ = min{α/2, (1 − α)/4} in
(4.20). Deﬁne W ..= Hij∆(ij) and ˆH ..= H (ij) = H − W . Let ˆG ..= ( ˆH − E − iη)−1. We have the
resolvent expansions

ˆG = G + (GW )G + (GW )2 ˆG

(4.30)

and

G = ˆG − ( ˆGW ) ˆG + ( ˆGW )2G .

(4.31)
Note that only two entries of W are nonzero, and they are stochastically dominated by N−1/2. Then
| ˆGab| ≺ N−(α−1)/2,
the trivial bound max
a,b
|Gab| (cid:54) N α, and the fact ˆG is

| ˆGab| (cid:54) N α together with (4.13) and (4.30) show that max
a(cid:54)=b

| ˆGaa| ≺ 1. Combining with (4.31), the trivial bound max

and max

a

a,b

independent of W , we have

and

max
a(cid:54)=b

sup

|Hji|(cid:54)N τ−1/2

|Gab| ≺ N−(α−1)/2 ,

max

a

sup

|Hji|(cid:54)N τ−1/2

|Gaa| ≺ 1 .

Now let us estimate the last term in (4.20). We have the derivatives

(4.32)

(4.33)

∂jiGab = −(GajGib + GaiGjb)(1 + δji)−1 ,

∂ji(cid:104)G(cid:105) =

(G2)ji(1 + δji)−1 ,

∂ji(G2)ab =(cid:0)(G2)ajGib + (G2)aiGjb + (G2)bjGia + (G2)biGja

(cid:1)(1 + δji)−1 ,

2
N

and

. Hence for any ﬁxed l ∈ N, ∂l+1

..= ∂
where ∂ij
∂Hij
N (G∗2)ab, Gab, and G∗
1
N (G2)ab, 1
the sum of the degrees of (cid:104)G(cid:105), (cid:104)G∗(cid:105), 1
the factors other than Gab and G∗
(4.32) and (4.33) we know, for any ﬁxed l ∈ N,

ji fij is a polynomial in the variables (cid:104)G(cid:105), (cid:104)G∗(cid:105),
ab, with a, b ∈ {i, j}. Note that in each term of the polynomial,
N (G∗2)ab is m + n − 1, so that the product of
ab is trivially bounded by O(N (m+n−1)α) for all H. Together with

N (G2)ab, and 1

sup

|x|(cid:54)N τ−1/2

ji fij

(cid:12)(cid:12)(cid:12)∂l+1
(cid:0) ˆH + x∆(ij)(cid:1)(cid:12)(cid:12)(cid:12) ≺ N (m+n−1)α .
(cid:12)(cid:12)∂l0+1

(cid:0) ˆH + x∆(ij)(cid:1)(cid:12)(cid:12) = O(N−(D0+2)) .

fij

ji

E|Hji|l0+2 · E sup

x(cid:54)N τ−1/2

Note that E|Hji|l+2 = O(N−(l+2)/2), and we can ﬁnd l0 = l0(D0, m, n) (cid:62) 2 such that

(4.34)

11

Finally, we estimate the ﬁrst term of (4.20). Note that by the trivial bound |Gij| (cid:54) N α, we
, then by

f (H)(cid:107)∞ = O(N C(m,n,l0)). From Deﬁnition 3.2 (iii)’ we ﬁnd max

|Hij| ≺ 1√

N

have (cid:107)∂l0+1
H¨older’s inequality we have

ji

E(cid:12)(cid:12)Hji

(cid:12)(cid:12) ·(cid:13)(cid:13)∂l+1
ji fij(H)(cid:13)(cid:13)∞ = O(N−(D0+2)) .

i,j

l+21{|Hji|>N τ−1/2}

(4.35)

Combining (4.34) and (4.35), we obtain from (4.20) that R(ji)
follows.

l0+1 = O(N−(D0+2)), from which (4.26)

Proof of Lemma 4.6 (ii). We begin with the case k = 2, which gives rise to terms of three types
depending on how many derivatives act on Gij. We deal with each type separately.

Step 1. The ﬁrst type is

J2,1

Note that

..= N−5/2(cid:88)

i,j

(cid:12)(cid:12)(cid:12)(cid:12)E(cid:104)(cid:104)G∗(cid:105)n(cid:104)G(cid:105)m−1(cid:105) ∂2Gij

∂Hji

2

(cid:12)(cid:12)(cid:12)(cid:12) .

∂2Gij
∂Hji

2 = a1 · GiiGjjGij + a2 · Gij

3 ,

where a1, a2 are some constants depending on the value of δji. Together with (4.6) and Lemma
4.5, we ﬁnd

J2,1 (cid:54) N−5/2 · N 2 · E(cid:12)(cid:12)(cid:104)(cid:104)G∗(cid:105)n(cid:104)G(cid:105)m−1(cid:105)(cid:12)(cid:12) · O(N (α−1)/2+ε) = O(N α/2+ε−1) · E(cid:12)(cid:12)(cid:104)G∗(cid:105)n(cid:104)G(cid:105)m−1(cid:12)(cid:12)

for any ﬁxed ε > 0. Together with (4.5) and Lemma 4.5, we ﬁnd

J2,1 = O(N α/2+ε−1+(m+n−1)(α−1)) = O(N (m+n)(α−1)−c0) ,

where in the last inequality we chose ε small enough depending on α.

Step 2. The second type is

..= N− 5

2

J2,2

Since

i,j

(cid:88)

(cid:12)(cid:12)(cid:12)(cid:12)E ∂2((cid:104)(cid:104)G∗(cid:105)n(cid:104)G(cid:105)m−1(cid:105))

∂Hji

2

(cid:12)(cid:12)(cid:12)(cid:12) .

Gij

(4.36)

(4.37)

(4.38)

(4.39)

∂(cid:104)G∗(cid:105)
∂Hji

=

a3
N

· (G∗2)ij and

∂2(cid:104)G∗(cid:105)
∂Hji

2 =

a4
N

· (G∗2)iiG∗

jj +

a5
N

· (G∗2)jjG∗

ii +

· (G∗2)ijG∗

ij

a6
N

for some constants a3, a4 and a5, we see that the most dangerous term of J2,2 is of the form

..= N−7/2 ·(cid:88)

(cid:12)(cid:12)E(cid:104)G∗(cid:105)n−1(cid:104)G(cid:105)m−1(G∗2)iiG∗

jjGij

(cid:12)(cid:12) .

P2,2

By (4.5), (4.6), and Lemma 4.5 we have

i,j

P2,2 = O(N−7/2+2+(m+n−2)(α−1)+α+(α−1)/2+ε) = O(N (m+n)(α−1)−α/2+ε)

for any ﬁxed ε > 0. The other terms of J2,2 are estimated similarly. By choosing ε small enough,
we obtain

J2,2 = O(cid:0)N (m+n)(α−1)−c0(cid:1) .

12

Step 3. The third type is

J2,3

..= N−5/2(cid:88)
..= N−7/2 ·(cid:88)

(cid:12)(cid:12)(cid:12)(cid:12) .
(cid:12)(cid:12)(cid:12)(cid:12)E ∂((cid:104)(cid:104)G∗(cid:105)n(cid:104)G(cid:105)m−1(cid:105))
(cid:12)(cid:12)E(cid:104)G∗(cid:105)n−1(cid:104)G(cid:105)m−1(G∗2)ijGiiGjj

∂Gij
∂Hji

i,j

∂Hji

(4.40)

(4.41)

(cid:12)(cid:12) .

The most dangerous term in J2,3 is of the form

P2,3

Again by (4.5), (4.6), and Lemma 4.5 we have

i,j

P2,3 = O(N−7/2+2+(m+n−2)(α−1)+3α/2−1/2+ε) = O(N (m+n)(α−1)−c0)

for any ﬁxed ε > 0. The other terms of J2,3 are estimated similarly. Thus we get

Step 4. Putting the estimates of the three types in Steps 1-3 together, we ﬁnd

J2,3 = O(cid:0)N (m+n)(α−1)−c0(cid:1) .
J2 = O(cid:0)N (m+n)(α−1)−c0(cid:1) ,

For k (cid:62) 3, the estimates are easier than those in k = 2 because of the small prefactor N−(k+3)/2

which concludes the proof of (4.27) for k = 2.
in the deﬁnition of Jk. Analogously to the case k = 2, we obtain for any ﬁxed k (cid:62) 3 and ε > 0,

Jk = O(N (m+n)(α−1)+1−k/2+ε) ,

for any ε > 0, from which (4.27) follows. We omit further details.

Now we look at the term K deﬁned in (4.18), whose estimate is contained in the next lemma.

Lemma 4.7. We have

Proof. Let us ﬁrst consider

K = O(N (m+n)(α−1)−α/2) .

B ..= N−2(cid:88)

i

(cid:12)(cid:12)(cid:12)(cid:12) ∂((cid:104)(cid:104)G∗(cid:105)n(cid:104)G(cid:105)m−1(cid:105)Gii)

∂Hii

E

(cid:12)(cid:12)(cid:12)(cid:12) .

(4.42)

(4.43)

The estimate of B is similar to that of J2, namely we will have terms of two types depending on
whether the derivative acts on Gii or not. We then estimate the terms by Lemmas 4.4 and 4.5,
which easily yields

B = O(N (m+n)(α−1)−α+ε) = O(N (m+n)(α−1)−α/2) .

From Deﬁnition 3.2 (iii)’ we get max

i

|ζi − 2| = O(1), hence K = O(B). This ﬁnishes the proof.

In order to conclude the proof, we need to use that the expectation of Gk is typically much
smaller than Gk itself. Lemma 4.4 implies that E|Gk| ≺ N (k−1)α, which is not enough to conclude
the proof. We need some extra decay from the expectation, which is provided by the following
result.

13

Lemma 4.8. Let c0 be deﬁned as in (4.3). We have

EGk = O(cid:0)N (k−1)α−c0(cid:1)

for k = 2, 3.

(4.44)

Proof. The proof is analogous to that of Lemma 4.6, and we omit unnecessary details.

Let us ﬁrst consider EG2. Again by the resolvent identity and cumulant expansion, we arrive

EG +

E(cid:104)G(cid:105)(cid:104)G2(cid:105) +

2
T

1
T N

EG3 − K(2)
T

− L(2)
T

,

at

where

and

EG2 =

1
T

L(2) =

K(2) = N−2(cid:88)
(cid:88)

(cid:34) l(cid:88)

i

i,j

k=2

E ∂(G2)ii
∂Hii

(ζi − 2) ,

(cid:35)

,

1
N

Ck+1(Hji)E ∂k(G2)ij

k + R(2,ji)

l+1

∂Hji

(4.45)

(4.46)

(4.47)

(4.48)

(4.50)

(4.51)

and we recall the deﬁnition (4.23) of T . Here R(2,ji)
l+1
in (4.20). We can argue similarly as in the proof of Lemma 4.6 (i) and show that R(2,ji)

is a remainder term deﬁned analogously to R(ji)
l+1
l0+1 = O(N−1)

for some l0 ∈ N. Thus we have |L(2)| (cid:54) l0(cid:80)

O(J (2)

k ) + O(1), where

k=2

..= N−(k+3)/2(cid:88)

J (2)
k

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)E ∂k(G2)ij

∂Hji

k

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) .

Analogously to the proof of Lemma 4.6 (ii), we ﬁnd

i,j

2 = O(N−5/2 · N 2 · N α+ε) = O(N α+ε−1/2) ,
J (2)
k = O(N α−1/2) for k (cid:62) 3. This shows |L(2)| = O(N α+ε−1/2) for any
for any ﬁxed ε > 0, and J (2)
ﬁxed ε > 0. Similar as in Lemma 4.7, one can show K(2) = O(N α/2). By Lemma 4.4 and 4.5, we

have (cid:12)(cid:12)E(cid:104)G(cid:105)(cid:104)G2(cid:105)(cid:12)(cid:12) = O(N (α−1)+(2α−1)+ε) = O(N 3α−2+ε) ,

(cid:12)(cid:12)EG3(cid:12)(cid:12) = O(N−1+2α+ε) ,

1
N

(4.49)

for any ﬁxed ε > 0. Hence by using (4.23) and choosing ε small enough, we obtain

(cid:12)(cid:12)EG2(cid:12)(cid:12) (cid:54) O(N 2α−1+ε) + O(N α/2) + O(N α+ε−1/2) = O(N α−c0) .

The proof of the case k = 3 is similar, and we omit the details.

Armed with Lemmas 4.6 and 4.8, we may now conclude the proof of Lemma 4.3 (i). We still
have to estimate the subleading terms on the right-hand side of (4.22). From (4.5), Lemma 4.5,
and Lemma 4.8 we have

E(cid:104)G∗(cid:105)n(cid:104)G(cid:105)m−2G3 = E(cid:104)G∗(cid:105)n(cid:104)G(cid:105)m−2(cid:0)EG3 + (cid:104)G3(cid:105)(cid:1) = O(cid:0)N (m+n)(α−1)+2−c0(cid:1) .

Moreover, (4.5) and Lemma 4.5 imply

E(cid:104)G∗(cid:105)n(cid:104)G(cid:105)m−1(cid:104)G2(cid:105) = O(cid:0)N (m+n)(α−1)+1−c0(cid:1) ,

14

as well as

E(cid:104)G∗(cid:105)n(cid:104)G(cid:105)m+1 = O(cid:0)N (m+n)(α−1)−c0(cid:1) ,
E(cid:104)G∗(cid:105)n(cid:104)G(cid:105)m−1E(cid:104)G(cid:105)2 = O(cid:0)N (m+n)(α−1)−c0(cid:1) .

(4.52)

(4.53)

Applying (4.29), Lemma 4.8, and (4.50)–(4.53) to (4.22), together with (4.23), we obtain

E(cid:104)G∗(cid:105)n−1(cid:104)G(cid:105)m−1GG∗2 + O(cid:0)N (m+n)(α−1)−c0(cid:1) .

E(cid:104)G∗(cid:105)n(cid:104)G(cid:105)m =

2n
N 2T

By the resolvent identity,

Moreover, (4.5) and Lemmas 4.5 and 4.8 give (cid:12)(cid:12)E(cid:104)G∗(cid:105)n−1(cid:104)G(cid:105)m−1G∗2(cid:12)(cid:12) = O(N (m+n−2)(α−1)+α−c0).

(GG∗ − G∗2) = − N 2α
4

(G − G∗) − N α
2i

GG∗2 =

z − ¯z

G∗2 .

(4.54)

1

We therefore conclude that

E(cid:104)G∗(cid:105)n(cid:104)G(cid:105)m = − n
2T

which yields

E(cid:104)G∗(cid:105)n(cid:104)G(cid:105)m = − n
2T

N 2α−2E(cid:104)G∗(cid:105)n−1(cid:104)G(cid:105)m−1(G − G∗) + O(cid:0)N (m+n)(α−1)−c0(cid:1) ,
N 2α−2E(cid:104)G∗(cid:105)n−1(cid:104)G(cid:105)m−1(EG − EG∗) + O(cid:0)N (m+n)(α−1)−c0(cid:1)
N 2α−2E(cid:104)G∗(cid:105)n−1(cid:104)G(cid:105)m−1 + O(cid:0)N (m+n)(α−1)−c0(cid:1)
E(cid:104)G(cid:105)m = O(cid:0)N m(α−1)−c0(cid:1)

n
2

by (4.5) and Lemma 4.5. Writing ξ ..= Im G, we have EG−EG∗ = 2iEξ. Moreover, (3.9) and (4.12)
imply that T = −z − 2EG = −2iEξ + O(N−c0). Together with (4.5) and Lemma 4.5 we have

E(cid:104)G∗(cid:105)n(cid:104)G(cid:105)m =

for m, n (cid:62) 1.

The preceding argument can also be used to show

for all m (cid:62) 2. In fact, one can start with
E(cid:104)G(cid:105)m =

(cid:88)

i,j

1
zN

E(cid:104)(cid:104)G(cid:105)m−1(cid:105)GijHji

and apply Lemma 3.1 to get an analogue of (4.22), which is

E(cid:104)G(cid:105)m =

+

E(cid:104)G(cid:105)m+1 − 1
1
T
T
2m − 2
N 2T

E(cid:104)G(cid:105)m−1E(cid:104)G(cid:105)2 +
− L(3)
T

E(cid:104)G(cid:105)m−2G3 − K(3)
T

1
T N

.

E(cid:104)G(cid:105)m−1(cid:104)G2(cid:105)

Here T = −z − 2EG, and K(3), L(3) are deﬁned analogously as K and L in (4.22). Due to the
absence of G∗, there is no leading term in (4.57) as the last term in (4.22). One can easily apply
our previous techniques and show every term in RHS of (4.57) is bounded by O(N m(α−1)−c0).

By taking complex conjugation in (4.56) we also have

E(cid:104)G∗(cid:105)n = O(cid:0)N n(α−1)−c0(cid:1)

(4.58)
for all n (cid:62) 2. Now (4.2) follows from (4.55), (4.56), and (4.58) combined with induction. This
concludes the proof of Lemma 4.3 (i).

15

(4.55)

(4.56)

(4.57)

EG2 − K(4) − L(4)(cid:17)

E ∂Gii
∂Hii

(ζi − 2) ,

1
N

1 + E(cid:104)G(cid:105)2 +

(cid:16)
K(4) = N−2(cid:88)
(cid:34) l(cid:88)

i

i,j

k=2

L(4) = N−1(cid:88)

Ck+1(Hji)E ∂kGij
∂Hji

k + R(4,ji)

l+1

,

(cid:35)

.

(4.59)

(4.60)

(4.61)

4.3. Proof of Lemma 4.3 (ii). Again by the resolvent identity and cumulant expansion, we
have

1
U
where U ..= −z − EG, z = E + iη,

EG =

and

|L(4)| (cid:54) l0(cid:80)

k=2

Here R(4,ji)
l+1
in the proof of Lemma 4.6 (i) and show that R(4,ji)

is a remainder term deﬁned analogously to R(ji)

l+1 in (4.20). We can argue similarly as
l0+1 = O(N−2) for some l0 ∈ N. Thus we have

O(J (4)

k ) + O(N−1), where

J (4)
k

..= N−(k+3)/2(cid:88)

i,j

(cid:12)(cid:12)(cid:12)(cid:12)E ∂kGij

∂Hji

k

(cid:12)(cid:12)(cid:12)(cid:12) .

(4.62)

Analogously to the proof of Lemma 4.6 (ii), we ﬁnd

2 = O(N−5/2 · N 2 · N (α−1)/2+ε) = O(N α/2−1+ε) ,
J (4)
k = O(N α/2−1) for k (cid:62) 3. This shows |L(2)| = O(N α/2−1+ε) for any
for any ﬁxed ε > 0, and J (4)
ﬁxed ε > 0. Similar as in Lemma 4.7, one can show K(4) = O(N−1). By (4.56) and Lemma 4.8,
we have

E(cid:104)G(cid:105)2 = O(N 2α−2−c0) and

EG2 = O(N α−1−c0) .

1
N

Altogether we have

EG(z + EG) + 1 = O(N α−1−c0) .

(4.63)

Recall that m(z) is the unique solution of x2 + zx + 1 = 0 satisfying sgn(Im m(z)) = sgn(Im z) =
sgn(η) > 0. Let ˜m(z) be the other solution of x2 + zx + 1 = 0. An application of Lemma 5.5 in [3]
gives

min{|EG − m(z)|, |EG − ˜m(z)|} =

(4.64)
where we recall the deﬁnition (4.24) of κ. Since G = (H−z)−1, we know sgn(Im G) = sgn(Im z) > 0.
Also, we have Im ˜m(z) (cid:54) −c for some c = c(κ) > 0. This shows |EG− ˜m(z)| (cid:62) c. Thus from (4.64)
we have

κ

O(N α−1−c0)

√

= O(N α−1−c0) ,

|EG − m(z)| = O(N α−1−c0) ,

which completes the proof.
4.4. Proof of Theorem 4.1. Let ˜Y (b) ..= N 1−α(cid:104)G(E + bη)(cid:105). As in the one-dimensional case,
Proposition 4.2, Theorem 4.1 follows from the following lemma, which generalize Lemma 4.3.

Lemma 4.9. Let c0 be deﬁned as in (4.3). Under the assumptions of Theorem 4.1 the following
holds.

16

(i) For ﬁxed m, n (cid:62) 1 and i1, ..., im, j1, ..., jn ∈ {1, 2, ..., p}, we have

E(cid:2) ˜Y (bi1)··· ˜Y (bim) ˜Y (cid:0)bj1
where the notation(cid:80)(cid:81) means summing over all distinct ways of partitioning bi1, ..., bim, bj1, ..., bjn

(bil−bjk )2 + O(N−c0)

(cid:1)··· ˜Y (cid:0)bjn

if m = n
if m (cid:54)= n ,

(cid:1)(cid:3) =

O(N−c0)

(4.65)

(cid:40)(cid:80)(cid:81) −2

into pairs bil, bjk , and each summand is the product of the n pairs.

(ii) For any ﬁxed b ∈ H, we have

ˆY (b) − ˜Y (b) = E ˆY (b) = O(N−c0) .

Suppose Lemma 4.9 holds. Result (4.65) implies

( ˜Y (b1), ..., ˜Y (bp))

d−→ (Y (b1), ..., Y (bp)) .

Theorem 4.1 then follows from (4.66).

(4.66)

(4.67)

Proof of Lemma 4.9. The argument is very close to that of Lemma 4.3. Indeed, we see that

E(cid:2) ˜Y (bi1)··· ˜Y (bim) ˜Y (cid:0)¯bj1
= N (m+n)(1−α)E(cid:2)(cid:104)G(E + bi1η)(cid:105)···(cid:104)G(E + bimη)(cid:105)(cid:104)G(E + bj1η)(cid:105)···(cid:104)G(E + bjnη)(cid:105)(cid:3) ,

(cid:1)··· ˜Y (cid:0)¯bjn

(cid:1)(cid:3)

(4.68)

and this can be calculated in a similar way as we compute E(cid:104)G(cid:105)m(cid:104)G∗(cid:105)n = E(cid:104)G(E + iη)(cid:105)m(cid:104)G(E − iη)(cid:105)n
in Section 4.2. Most of our previous techniques and estimates can be applied to the new computa-
tion, and the only diﬀerence is when using the resolvent identity (for example in (4.54)), we now
have

G(E + bik η)G(E + bjlη) =

1

bik − bjl

(G(E + bik η) − G(E + bjlη))

instead of

GG∗ =

(G − G∗) .

1
2i

This will give us diﬀerent constants in the leading terms, and lead to the induction step

=

−2

E(cid:2) ˜Y (bi1)··· ˜Y (bim) ˜Y (cid:0)bj1
n(cid:88)

(cid:1)··· ˜Y (cid:0)bjn
(cid:1)(cid:3)
(cid:1) ˜Y (cid:0)bj1
E(cid:2) ˜Y (bi1)··· ˜Y (cid:0)bim−1
E(cid:2) ˜Y (bi1)··· ˜Y (bim)(cid:3) = O(N−c0)
and E(cid:2) ˜Y (cid:0)bj1

(bim − bjk )2

k=1

(cid:1)··· ˜Y (cid:0)bjn

(cid:1) / ˜Y (cid:0)bjk
(cid:1)··· ˜Y (cid:0)bjn

(cid:1)(cid:3) + O(N−c0)
(cid:1)(cid:3) = O(N−c0)

for m, n (cid:62) 1. One can also show that

for m, n (cid:62) 2. These results together imply (4.65).

Moreover, (4.66) says nothing but N 1−αE(cid:0)G(E + bη)− m(E + bη)(cid:1) = O(N−c0), and this can be
shown using the steps in Section 4.3, in which we proved N 1−αE(cid:0)G(E + iη)−m(E+iη)(cid:1) = O(N−c0).

17

5. Convergence of general functions

Similar as in the resolvent case, in Section 5 we prove the following analogue of Theorem 2.3.
Theorem 5.1. Theorem 2.3 holds for Wigner matrices H satisfying Deﬁnition 3.2, and the con-
vergence also holds in the sense of moments.

Let us abbreviate fη(x) ..= f(cid:0) x−E

η

(cid:1), and denote [Tr fη(H)] ..= Tr fη(H)−(cid:82) 2−2 (x)fη(x) dx. Our
(cid:18)

(cid:90) (cid:18) f (x) − f (y)

(cid:19)2

(cid:19)

dx dy

(5.1)

next result is a particular case of Theorem 5.1.
Proposition 5.2. Let η, E, H be as in Theorem 5.1. Then

[Tr fη(H)]

d−→ N

0 ,

1
2π2

x − y

as N → ∞. The convergence also holds in the sense of moments.

Our main work in is section will be to show the above 1-dimensional case, since the proof can
easily be extended to the general case (see Section 5.5). Recall that for a random variable X,
(cid:104)X(cid:105) ..= X − EX. Proposition 5.2 is a direct consequence of the following lemma.
Lemma 5.3. Under the conditions of Theorem 5.1, we have the following results.

(i) For any n (cid:62) 2

E(cid:104)Tr fη(H)(cid:105)n =

n − 1
2π2

where c = c (r, s, α) > 0.

(cid:90) (cid:18) f (x) − f (y)

(cid:19)2

x − y

dx dy · E(cid:104)Tr fη(H)(cid:105)n−2 + O(N−c/n2

) ,

(5.2)

(ii) The random variables [Tr fη(H)] and (cid:104)Tr fη(H)(cid:105) are close in the sense that
[Tr fη(H)] − (cid:104)Tr fη(H)(cid:105) = E[Tr fη(H)] = O(N−rs2c0/16) ,

(5.3)

with c0 deﬁned in (4.3).

Assume Lemma 5.3 holds. Then (5.2) and Wick’s theorem imply

(cid:18)

(cid:90) (cid:18) f (x) − f (y)

(cid:19)2

(cid:19)

(cid:104)Tr fη(H)(cid:105)

d−→ N

1
2π2

(5.4)
as N → ∞. Note that the above result is proved in a stronger sense that we have convergence in
moments. Proposition 5.2 then follows from (5.3).

dx dy

x − y

0 ,

Sections 5.1 to 5.3 are devoted to proving Lemma 5.3 (i). Before starting the proof, we give
some explanations of the ideas, especially the choice of truncations in the proof. We use Lemma
3.3 to write fη(H) in the form (5.6) below, where we scale the cutoﬀ function χ to be supported in
an interval of size O(σ), with N−1 (cid:28) σ (cid:28) η. This scaling ensures that when we integrate ϕf , the
integral of the last term in (5.7) below dominates over the others.

We then write E(cid:104)Tr fη(H)(cid:105)n as an integral over Cn, written (cid:82) F in (5.8) below. The leading
contribution to(cid:82) F arises from the region {|y1|, ...,|yn| (cid:62) ω}, where ω (cid:29) N−1 is a second truncation
In order to ensure that (cid:82) F is small in the complementary region, we require that ω (cid:28)
σ. Then, when estimating (cid:82)

|y1|<ω F , the integral over z1 yields a factor that is small enough
to compensate the integrals from the other variables. We use the notations σ = N−(α+β) and
ω = N−(α+γ), so that 0 < β < γ. Except for these qualitative requirements, we also have to
be quantitative careful on the scalings. For all steps of the analysis to work, we have further
requirements on the exponents γ and β; for instance, the last step in (5.18) below requires nβ (cid:54)
rsγ/4. Combining all requirements, we are led to set β as in (5.5) below.

scale.

18

5.1. Transformation by Helﬀer-Sj¨ostrand formula. Let f ∈ C1,r,s(R) with r, s > 0, and
without loss of generality we assume s (cid:54) 1. Fix n (cid:62) 2, and deﬁne σ ..= N−(α+β), where

β ..=

rs2 c0
24n2 ,

(5.5)

and c0 is deﬁned in (4.3). We deﬁne ˜f as in (3.4). Let χ be as in Lemma 3.3 satisfying χ(y) = 1
for |y| (cid:54) 1, and χ(y) = 0 for |y| (cid:62) 2. An application of Lemma 3.3 gives

where

ϕf (x + iy) =

∂¯z( ˜fη(z)χ(z/σ))

H − z

d2z =

(cid:90)
η(x)(cid:1)χ(y/σ) − 1

C

σ

fη(H) =

(cid:16)

C

1
π

(cid:90)
(i − 1)(cid:0)f(cid:48)
(cid:17)
η(x + y) − f(cid:48)
(cid:90)

fη(x)χ(cid:48)(y/σ)

.

1
2π
i
σ

+

ϕf (z)G(z) d2z ,

(cid:0)fη(x + y) − fη(x)(cid:1)χ(cid:48)(y/σ)

(5.6)

(5.7)

(cid:90)

Thus

F ,

ϕf (z1)··· ϕf (zn)E(cid:104) G1

(cid:105)···(cid:104) Gn (cid:105) d2z1 ··· d2zn =..

E(cid:104)Tr fη(H)(cid:105)n = N n
(5.8)
..= (H − zk)−1 for i ∈ {1, 2, ..., n}. Note that χ(y/σ) ≡ 0 for |y| (cid:62) 2σ, and we only need

where Gk
to consider the integral for |y1|, ...,|yn| (cid:54) 2σ.
5.2. The subleading terms. Let ω ..= N−(α+γ) with γ ..= 4nβ/rs, and by (5.5) we have α + β <
2} and Y ..= {|y1|, ...,|yn| ∈ [ω, 2σ]}, where we recall
α + γ < 1. We deﬁne X ..= {|x1|, ...,|xn| (cid:54) 2− κ

the deﬁnition (4.24) of κ. We have a lemma about(cid:82) F outside the region X × Y .
Lemma 5.4. For F as in (5.8) we have(cid:90)
Proof. We ﬁrst estimate(cid:82)

Rn×Y c F . By the estimates (3.9) and (3.11) we know

F = O(N−β/2) .

(X×Y )c

uniformly in {|y1|, ...,|yn| (cid:54) 2σ}. Since χ(cid:48)(y/σ) = 0 for |y| < σ, we have
η(x)) · 1
y

(cid:12)(cid:12)(cid:12)ϕf (z) · 1

(cid:12)(cid:12)(cid:104) G1
(cid:12)(cid:12)(cid:12) d2z = O(1) ·

|y1 ··· yn|N n

(cid:105)···(cid:104) Gn (cid:105)(cid:12)(cid:12) ≺
(cid:90)
(cid:12)(cid:12)(cid:12) dx dy
(cid:12)(cid:12)(cid:12)(f(cid:48)
(cid:90)
(cid:12)(cid:12)(cid:12)(f(cid:48)(a + bN−β) − f(cid:48)(a)) · 1

η(x + y) − f(cid:48)

|y|<ω

= O(1) ·

|y|<ω

(cid:90)

1

y

|b|<N β−γ

(cid:12)(cid:12)(cid:12) da db ,

b

where in the second step we used the change of variables

a ..= (x − E)/η and b ..= y/σ .
By the H¨older continuity and decay of the function f(cid:48), we know
(|b|N−β)r,

(cid:12)(cid:12)f(cid:48)(a + bN−β) − f(cid:48)(a)(cid:12)(cid:12) (cid:54) C min

(cid:26)

(cid:27)

(cid:18)

1

(cid:19)1−q

1 + |a|1+s
1

1 + |a|1+s

(cid:54) C(|b|N−β)rq

19

(5.9)

(5.10)

(5.11)

(5.12)

(5.13)

2 > 1. Thus we have

b

2(1+s)

(cid:90)

(cid:16)

|y|<ω

1(cid:54)|b|(cid:54)2

|y|(cid:62)σ

y

|y|∈[ω,σ)

y

|b|rq0−1

1

da db = O(N−rq0γ) .

|b|<N β−γ

We also have

1 + |a|1+s/2

(cid:12)(cid:12)(cid:12)(cid:12)ϕf (z) · 1

y

(cid:62) s

4 , so that (1 + s)(1− q0) = 1 + s

where we used the change of variables (5.12), and abbreviate

for all q ∈ [0, 1]. Choose q = q0(s) ..= s

(cid:12)(cid:12)(cid:12)(cid:12) da db = O(N β) ,

(cid:12)(cid:12)(cid:12)(cid:12) d2z = O(N−rq0β) ·

(cid:12)(cid:12)(cid:12)(cid:12) d2z = O(N−rq0β) .
(cid:12)(cid:12)(cid:12)(cid:12)ψf (a, b) · 1

(cid:90)
(cid:12)(cid:12)(cid:12)(cid:12)ϕf (z) · 1
(cid:12)(cid:12)(cid:12)(cid:12) d2z = N β
(cid:90)

(cid:90)
Similarly, one can show that (cid:90)
(cid:12)(cid:12)(cid:12)(cid:12)ϕf (z) · 1
N−β(i − 1)(cid:0)f(cid:48)(a + bN−β) − f(cid:48)(a)(cid:1)χ(b) −(cid:0)f (a + bN−β) − f (a)(cid:1)χ(cid:48)(b)
(cid:17)
(cid:12)(cid:12)(cid:12)(cid:12) d2z1 ··· d2zn
(cid:12)(cid:12)(cid:12)(cid:12)(cid:90)
(cid:12)(cid:12)(cid:12)(cid:12) (cid:54) n
(cid:12)(cid:12)(cid:12)(cid:12) d2z2 ··· d2zn
(cid:90)
= O(cid:0)N−rq0γ · N (n−1)β(cid:1) (cid:54) O(cid:0)N−rsγ/4 · N (n−1)β(cid:1) (cid:54) O(cid:0)N−β(cid:1) .
Next, we estimate(cid:82)
(cid:12)(cid:12)(cid:12)(cid:12)ϕf (z) · 1
(cid:32)(cid:90)

(cid:12)(cid:12)(cid:12)(cid:12)ϕf (z1) · 1
(cid:12)(cid:12)(cid:12)(cid:12) dz1
(cid:90) (cid:12)(cid:12)(cid:12)(cid:12)ϕf (z2) · 1
(cid:12)(cid:12)(cid:12)(cid:12)ψf (a, b) · 1
(cid:12)(cid:12)(cid:12)(cid:12) da db
(cid:90)

(cid:90)
(cid:12)(cid:12)F(cid:12)(cid:12) ≺
(cid:12)(cid:12)(cid:12)(cid:12)ϕf (z1) · 1
(cid:12)(cid:12)(cid:12)(cid:12) d2z (cid:54) N β

X c×Y F . By the decay of the functions f and f(cid:48), we have

1
2π
+ if (a)χ(cid:48)(b)

··· ϕf (zn) · 1
yn

Using lemma 4.5 we have

··· ϕf (zn) · 1
yn

y1

ψf (a, b) ..=

F

Rn×Y c

|a|(cid:62) κ

2η

(cid:90)

=

|y1|<ω

|y1|<ω

|y1|<ω

(cid:33)

(cid:90)

(cid:90)

(cid:90)

y1

.

y1

b

1

1

1 + |a|1+s/2

da +

|a|(cid:62) κ

2η

1 + |a|1+s da +

|a|(cid:62) κ

2η

|a|(cid:62) κ

2η

|f (a)| da

(5.14)

(5.15)

(5.16)

(5.17)

(5.18)

(5.19)

(5.20)

y

|x|>2− κ
= O(N β) ·

2

= O(N β−sα/2) .

where a, b are deﬁned as in (5.12). Hence

(cid:12)(cid:12)(cid:12)(cid:12)(cid:90)

F

X c×Y

(cid:12)(cid:12)(cid:12)(cid:12)ϕf (z1) · 1
(cid:12)(cid:12)(cid:12)(cid:12) ≺
(cid:90)
(cid:90)
= O(cid:0)N (n−1)β(cid:1) ·
= O(cid:0)N nβ−sα/2) (cid:54) O(N−β(cid:1) .

{|x1|>2− κ

{|x1|>2− κ

2 }×Y

y1

(cid:12)(cid:12)(cid:12)(cid:12) d2z1 ··· d2zn
(cid:12)(cid:12)(cid:12)(cid:12) d2z1

··· ϕf (zn) · 1
yn

(cid:12)(cid:12)(cid:12)(cid:12)ϕf (z1) · 1

y1

2 , |y1|(cid:62)ω}

Combining (5.18) and (5.20), we get(cid:90)

(cid:90)

X×Y

F =

F + O(N−β/2) .

20

5.3. The main computation. Now let us focus on the integral(cid:82)

X×Y F . Note that now we are
in the “good” region where it is eﬀective to apply cumulant expansion to the resolvent. In X ∩ Y ,
(cid:105)···(cid:104) Gn (cid:105). Note that this is very close to the expression we
we want to compute the quantity E(cid:104) G1
had in (4.14). Let us abbreviate

Qm
for all 1 (cid:54) k (cid:54) m (cid:54) n, and ζi
get an analogue of (4.22):

..= (cid:104) G1
..= E|√

(cid:105)···(cid:104) Gm (cid:105) and Q(k)
N Hii|2. We proceed the computation as in Section 4.2, and

..= Qm/(cid:104) Gk

(cid:105)

m

EQn−1(cid:104) Gn 2(cid:105)

(5.21)

EQn =

EQn(cid:104) Gn (cid:105) − 1
Tn

EQn−1E(cid:104) Gn (cid:105)2 +

1

1
Tn
− ˜K
Tn

− ˜L
Tn

+

2

N 2Tn

n−1(cid:88)

k=1

N Tn
2 · Gn

,

EQ(k)

n−1 Gk

(cid:1)

(cid:105)(cid:105) · Gn

ii

where Tn

and

..= −zn − 2E Gn ,

(cid:105)···(cid:104) Gn−1

˜K = N−2(cid:88)
(cid:34) l(cid:88)

E ∂(cid:0)(cid:104)(cid:104) G1
Cl+1(Hji)E ∂k(cid:0)(cid:104)(cid:104) G1

i

∂Hii

1
k!

i,j

k=2

˜L = N−1(cid:88)

(ζi − 2) ,

(cid:1)

(cid:35)

.

(cid:105)···(cid:104) Gn−1

(cid:105)(cid:105) · Gn

ij

k

∂Hji

+ ˜R(ji)
l+1

l+1 is a remainder term deﬁned analogously to R(ji)

Here ˜R(ji)
ω, and we have estimates analogue to those in Section 4.2. We state these estimates in the next
lemma and omit the proof.
Lemma 5.5. Let us extend the deﬁnition of c0 in (4.3) to a function c0(·) : (0, 1) → R such that

l+1 in (4.20). Note in Y , we have |y1|, ...,|yn| (cid:62)

1
3
The following results holds uniformly in X × Y .
(i) Analogously to Lemma 4.4, for any m ∈ N+ and k = 1, 2, ..., n, we have

min{x, 1 − x} .

c0(x) ..=

(5.22)

(5.23)

(5.24)

(5.25)

(5.26)

(5.27)

as well as

(cid:12)(cid:12)(cid:0) Gk m(cid:1)

ij

(ii) Analogously to (4.23), we have

(iii) Analogously to Lemma 4.6, we have

(iv) Analogously to Lemma 4.7, we have

if i = j
if i (cid:54)= j .

(cid:12)(cid:12)(cid:104) Gk m(cid:105)(cid:12)(cid:12) ≺ N m(α+γ)−1
(cid:40)
(cid:12)(cid:12) ≺

N (m−1)(α+γ)
N (m−1/2)(α+γ)−1/2

(cid:12)(cid:12)(cid:12) 1

Tn

(cid:12)(cid:12)(cid:12) = O(1) .

(cid:12)(cid:12) ˜L(cid:12)(cid:12) = O(N n(α+γ−1)−c0(α+γ)) .
(cid:12)(cid:12) ˜K(cid:12)(cid:12) = O(N n(α+γ−1)−(α+γ)/2) .

21

(v) Analogously to Lemma 4.8, we have

E Gk

2 = O(N α+γ−c0(α+γ))

(5.28)

for k = 1, 2, ..., n.

Applying Lemma 5.5 to (5.21) yields

EQn =

2

N 2Tn

EQ(k)

n−1 Gk

2 · Gn + O(N n(α+γ−1)−c0(α+γ))

(5.29)

uniformly in X × Y . Note that by the deﬁnition of β and γ we have γ = sc0(α)/6n < c0(α)/2,
which gives c0(α + γ) > 5c0(α)/6 > 0. Since we have the simple estimate

n−1(cid:88)

k=1

(cid:90)

|ϕf (z)| d2z = O(η) ,

we know(cid:90)

(cid:90)

F = N n

(cid:90)

n−1(cid:88)

X×Y
2N n−2

Tn

X×Y

k=1

=

ϕf (z1)··· ϕf (zn) EQn d2z1 ··· d2zn

X×Y
ϕf (z1)··· ϕf (zn)EQ(k)

n−1 Gk

2 · Gn d2z1 ··· d2zn + O(N−2c0(α)/3) ,

where in the estimate of the error term we implicitly used nγ = sc0(α)/6 (cid:54) c0(α)/6. By symmetry,
it suﬃces to ﬁx k ∈ {1, 2, ..., n − 1}, and consider the integral over X × Y of

Fkn

..=

2N n−2

Tn

ϕf (z1)··· ϕf (zn)EQ(k)

n−1 Gk

2 · Gn

.

As before, we summarize the necessary estimates into a lemma.

Lemma 5.6. Let Fkn be as in (5.32). Then we have the following estimates.
..= {(x, y) ∈ X × Y : ykyn > 0,|xk − xn| (cid:54) ηN−(n+1)γ}. Then

(i) Let A1

(cid:12)(cid:12)(cid:12)(cid:12)(cid:90)

Fkn

A1

(cid:12)(cid:12)(cid:12)(cid:12) ≺ N−γ .

(ii) Let A2

..= {(x, y) ∈ X × Y : ykyn > 0,|xk − xn| ∈ (ηN−(n+1)γ, ηN (n+1)γ/s]}. Then

(cid:90)

Fkn = O(N−c0(α)/4) ,

where the function c0(·) is deﬁned in (5.22).

A2

(iii) For A3

..= {(x, y) ∈ X × Y : |xk − xn| > ηN (n+1)γ/s }, we have

(cid:12)(cid:12)(cid:12)(cid:12)(cid:90)

A3

(cid:12)(cid:12)(cid:12)(cid:12) ≺ N−γ .

Fkn

22

(5.30)

(5.31)

(5.32)

(5.33)

(5.34)

(5.35)

Proof. (i) By Lemma 5.5 (i)-(ii) we have

uniformly in A1. Thus by (5.30) and the decay of f and f(cid:48) we know

(cid:12)(cid:12)(cid:12)(cid:12)(cid:90)

˜Fkn

A1

(cid:12)(cid:12)(cid:12)(cid:12) ≺ ω−n · ηn−2 ·

Tn

EQ(k)

n−1 Gk

2 · Gn

(cid:12)(cid:12)(cid:12)(cid:12) ≺ ω−n

(cid:12)(cid:12)(cid:12)(cid:12) N n−2
(cid:90) (cid:12)(cid:12)ϕf (zk) · ϕf (zn)(cid:12)(cid:12)1{|xk−xn|(cid:54)η·N−(n+1)γ} d2zk d2zn
(cid:90) (cid:12)(cid:12)ψf (ak, bk) · ψf (an, bn)(cid:12)(cid:12)1{|ak−an|(cid:54)N−(n+1)γ} dak dbk dan dbn

= O(ω−n · ηn)
= O(N nγ · N−(n+1)γ) = O(N−γ) ,

where we use the change of variables

ai = (xi − E)/η and bi = yi/σ ,

i = k, n,

and ψf is deﬁned as in (5.17).

(ii) Note that our assumption (5.5) on β shows ηN (n+1)γ/s = O(N−α/2). By the resolvent

identity, the semicircle law (3.9), and Lemma 5.5 we know

EQ(k)

n−1 Gk

(cid:12)(cid:12)(cid:12)(cid:12) N n−2
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)N n−2EQ(k)

2 · Gn

(cid:12)(cid:12)(cid:12)(cid:12)
(cid:18)(cid:104) Gk
(cid:18) ω−1N−c0(α+γ)

2(cid:105) + E Gk
Tn(zk − zn)

n−1

Tn

=

2

+

(cid:104) Gn (cid:105) − (cid:104) Gk

(cid:105) + E( Gn − Gk

Tn(zk − zn)2

)

(N ω)−1 + (N ω)−1 + N−c0(α+γ)

(cid:19)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
(cid:19)

≺ ω−(n−2) ·
= O(η−n · N 3nγ−c0(α+γ)) (cid:54) O(η−n · N−c0(α)/3)

ηN−(n+1)γ

+

η2N−2(n+1)γ

(5.36)

(5.37)

(5.38)

(5.39)

(5.40)

uniformly in A2. Hence (5.30) yields(cid:90)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:90)

(cid:12)(cid:12)(cid:12)(cid:12) ≺ ω−n · ηn−2 ·

(iii) Similar as in (5.36), we know

Fkn

A3

A2

= O(ω−n · ηn)

Note that in {|ak − an| > N (n+1)γ/s}, either |ak| > 1
decay conditions of f and f(cid:48), we have

Fkn = O(N−c0(α)/4) .

(cid:90) (cid:12)(cid:12)ϕf (zk) · ϕf (zn)(cid:12)(cid:12)1{|xk−xn|>ηN (n+1)γ/s} d2zk d2zn
(cid:90) (cid:12)(cid:12)ψf (ak, bk) · ψf (an, bn)(cid:12)(cid:12)1{|ak−an|>N (n+1)γ/s} dak dbk dan dbn .
(cid:12)(cid:12)(cid:12)(cid:12)(cid:90)

(cid:12)(cid:12)(cid:12)(cid:12) ≺ ω−n · ηn · N−(n+1)γ = N−γ .

2 N (n+1)γ/s or |an| > 1

Fkn

A3

2 N (n+1)γ/s. Hence by the

Let A4

..= {(x, y) ∈ (X × Y ) : ykyn < 0,|xk − xn| (cid:54) ηN (n+1)γ/s }. Note that Cn is the disjoint
union of A1, ..., A4. The next result is about the integral of Fkn in A4, which gives the leading
contribution.

23

Lemma 5.7. We have

(cid:90)

Fkn =

1
2π2

A4

(cid:90) (cid:18) f (x) − f (y)

x − y

(cid:19)2

dx dy · E(cid:104)Tr fη(H)(cid:105)n−2 + O(N−rsβ/9) ,

(5.41)

where β is deﬁned in (5.5).

Proof. Step 1. By symmetry, let us consider A5
ηN (n+1)γ/s }. Similar as in (5.38), we have

..= {(x, y) ∈ A4 : yk (cid:62) ω, yn (cid:54) −ω,|xk − xn| (cid:54)

N n−2
Tn

EQ(k)

n−1 Gk

2 · Gn = N n−2EQ(k)
n−1

E Gn − E Gk
Tn(zk − zn)2 + O(η−n · N−c0(α)/3)

uniformly in A5. Note the semicircle law (3.9) now gives

E Gn − E Gk

Tn

E Gn − E Gk
−zn − 2E Gn = −1 + O(N−c0(α+γ))

=

uniformly in A5. By (5.30) we know

(cid:90)

A5

Fkn = N n−2
= − 2

(cid:90)

(cid:90)

A5

(cid:48)
5

A

where we decompose A5 = A

(zk − zn)2 d2z1 ··· d2zn + O(N−c0(α)/3)

ˆFkn + O(N−c0(α)/3) ,

−2

(cid:90)

1

ϕf (z1)··· ϕf (zn)EQ(k)
n−1
(zk − zn)2 ϕf (zk)ϕf (zn) d2zk d2zn ·
5 × A
ˆFkn

(cid:48)(cid:48)
5 , with A

(cid:48)

..= ϕf (z1)··· ϕf (zn−1)/ϕf (zk) EQ(k)

(cid:48)(cid:48)
5

A

n−1 .

(cid:48)
5 depends on (xk, yk, xn, yn). Here ˆFkn is deﬁned as

(5.42)

(5.43)

Let X (k,n) ..= {|x1|, ...,|xk−1|,|xk+1|, ...,|xn−1| (cid:54) 2− κ
ω}, and note that A

(cid:48)(cid:48)

5 = X (k,n) × Y (k,n). Applying Lemma 5.4 with n replaced by n − 2, we get

2}, and Y (k,n) ..= {|y1|, ...,|yk−1|,|yk+1|, ...,|yn−1| (cid:62)

ˆFkn = E(cid:104)Tr fη(H)(cid:105)n−2 + O(N−β/2) .

(5.44)

(cid:90)

(cid:48)(cid:48)
5

A

By the decay conditions of f and f(cid:48),

1

A

1

(cid:48)
5

(zk − zn)2 ϕf (zk)ϕf (zn) d2zk d2zn
(zk − zn)2 ϕf (zk)ϕf (zn)1{yk(cid:62)ω, yn(cid:54)−ω} d2zk d2zn + O(N−γ)
(ak − an + i (bk − bn)N−β)2 1{bk(cid:62)N β−γ , bn(cid:54)−N β−γ} d2zk d2zn + O(N−γ)
Ψ + O(N−γ) ,

ψf (ak, bk)ψf (an, bn)

(cid:90)
(cid:90)
(cid:90)
(cid:90)

=

=

=..

(cid:90)

A5

where in the second last step we use the change of variables in (5.37), and ψf is as in (5.17).
Note that one can repeat the steps in the proof of Lemma 4.4 for any f ∈ C1,r,s(R) instead of
f (x) = ( x+i

x2+1 )k, and get

Together with Lemma 4.5 we know

Fkn = −2 E(cid:104)Tr fη(H)(cid:105)n−2

(cid:12)(cid:12)(cid:12)(cid:12) · O(cid:0)N−β/2(cid:1) + O(N−γ/2) .

(5.45)

(5.46)

(cid:12)(cid:12)(cid:104)fη(H)(cid:105)(cid:12)(cid:12) ≺ 1.
(cid:12)(cid:12)(cid:12)(cid:12)(cid:90)
(cid:90)

Ψ +

Ψ

24

Step 2. We now compute(cid:82) Ψ. Let us set

ψf,1(a, b) ..=

N−β(cid:0)f(cid:48)(a + bN−β) − f(cid:48)(a)(cid:1)χ(b) ,
(cid:0)f (a + bN−β) − f (a)(cid:1)χ(cid:48)(b) , and ψf,3(a, b) ..=

i − 1
2π

ψf,2(a, b) ..= − 1
2π

(cid:90)

(cid:90)

f (a)χ(cid:48)(b) ,

i
2π

(5.47)

which gives ψf (a, b) = ψf,1(a, b) + ψf,2(a, b) + ψf,3(a, b). Let

..=

Ψi,j

ψf,i(ak, bk)ψf,j(an, bn)

(ak − an + i (bk − bn)N−β)2 1{bk(cid:62)N β−γ , bn(cid:54)−N β−γ} dak dbk dan dbn ,

with 1 (cid:54) i, j (cid:54) 3. We will calculate (cid:82) Ψ by calculating 6 diﬀerent integrals (cid:82) Ψi,j, subject to
Let us ﬁrst consider(cid:82) Ψ1,1. Note that by (5.13),

symmetry.

(cid:12)(cid:12)f(cid:48)(a + bN−β) − f(cid:48)(a)(cid:12)(cid:12) (cid:54) C(|b|N−β)rq0

(cid:18)

1

1 + |a|1+s/2

,

(5.48)

where q0 = q0(s) = s

2(s+1)

(cid:90)

(cid:62) s

4 > 0. Thus

Ψ1,1 (cid:54) C

(cid:90)
Now we consider (cid:82) Ψ2,1. Note that χ(cid:48)(b) = 0 for |b| < 1. Using integration by parts on the

|bkbn|N−2β N−2β−2rq0β|bkbn|rq0χ(bk)χ(bn)1{bk(cid:62)N β−γ , bn(cid:54)−N β−γ} dbk dbn

= O(N−2rq0β) .

1

(cid:19)

variable ak, we know

(cid:90)

Ψ2,1 =

=
where ˜ψf,2(a, b) ..= − 1

2π

ψf,2(ak, bk)ψf,1(an, bn)

(cid:90)
(cid:90)
(cid:0)f(cid:48)(a + bN−β) − f(cid:48)(a)(cid:1)χ(cid:48)(b). Then by (5.48) we know
(cid:90)

(ak − an + i (bk − bn)N−β)2 1{bk(cid:62)1, bn(cid:54)−N β−γ} dak dbk dan dbn
˜ψf,2(ak, bk)ψf,1(an, bn)
ak − an + i (bk − bn)N−β 1{bk(cid:62)1, bn(cid:54)−N β−γ} dak dbk dan dbn ,

(5.49)

Ψ2,2 =

Ψ2,1 = O(N β · N−rq0β · N−β−rq0β) = O(N−2rq0β) .

Similarly,(cid:82) Ψ3,1 = O(N−rq0β).
Now we move to(cid:82) Ψ2,2. Using integration by parts on the variables ak and an, we know
(cid:90)
(cid:90)
log(cid:0)ak − an + i (bk − bn)N−β(cid:1) ˜ψf,2(ak, bk) ˜ψf,2(an, bn)1{bk(cid:62)1, bn(cid:54)−1} dak dbk dan dbn
= O(cid:0) log N · N−2rq0β(cid:1) = O(N−rq0β) .
Similarly,(cid:82) Ψ3,2 = O(N−rq0β/2) (cid:54) O(N−rsβ/8).
(cid:90)

(cid:90)
(cid:90) (cid:18)
(ak − an + i (bk − bn)N−β)2 χ(cid:48)(bk)χ(cid:48)(bn)1{bk(cid:62)1, bn(cid:54)−1} dak dbk dan dbn
(cid:90) (cid:18) f (ak) − f (an)
(cid:19)2
χ(cid:48)(bk)χ(cid:48)(bn)1{bk(cid:62)1, bn(cid:54)−1} dak dbk dan dbn
(ak − an + i (bk − bn)N−β)
(cid:90) (cid:18) f (ak) − f (an)
(cid:19)2
χ(cid:48)(bk)χ(cid:48)(bn)1{bk(cid:62)1, bn(cid:54)−1} dak dbk dan dbn + O(N−β/3)
dak dan + O(N−β/3) ,

The leading contribution comes from Ψ3,3. Note that

Ψ3,3 = − 1
4π2

f (ak) − f (an)

=

1
8π2

ak − an

f (ak)f (an)

(cid:19)2

=

1
8π2
= − 1
8π2

ak − an

25

where the second last step is an elementary estimate whose details we omit. Hence by (5.45) and
Lemma 4.5 we have

dx dy · E(cid:104)Tr fη(H)(cid:105)n−2 + O(N−rsβ/9) .

(5.50)

Similarly, let A6

..= {(x, y) ∈ A4 : yk (cid:54) −ω, yn (cid:62) ω,|xk − xn| (cid:54) ηN (n+1)γ/s }, and we have

dx dy · E(cid:104)Tr fη(H)(cid:105)n−2 + O(N−rsβ/9) .

(5.51)

(cid:90)
(cid:90)

(cid:90)

Fkn =

1
4π2

A5

Fkn =

1
4π2

A6

Fkn =

n − 1
2π2

(cid:90) (cid:18) f (x) − f (y)
(cid:90) (cid:18) f (x) − f (y)

x − y

(cid:19)2
(cid:19)2

x − y

(cid:19)2
(cid:90) (cid:18) f (x) − f (y)
(cid:19)2
(cid:90) (cid:18) f (x) − f (y)

x − y

Thus by (5.50) and (5.51) we conclude proof.

Note that Lemma 5.6 and 5.7 imply

dx dy · E(cid:104)Tr fη(H)(cid:105)n−2 + O(N−rsβ/9) .

Together with Lemma 5.4 and (5.31), we have

n − 1
2π2

E(cid:104)Tr fη(H)(cid:105)n =

dx dy · E(cid:104)Tr fη(H)(cid:105)n−2 + O(N−rsβ/9) ,

x − y
which ﬁnishes the proof of Lemma 5.3 (i).
5.4. Proof of Lemma 5.3 (ii). Let f ∈ C1,r,s(R) with r, s > 0, and without loss of generality
we assume s (cid:54) 1. We deﬁne ˜f as in (3.4). Let σ = N−(α+β), where we deﬁne β = sc0/4 instead
in (5.5). Let χ be as in Lemma 3.3 satisfying χ(y) = 1 for |y| (cid:54) 1, and χ(y) = 0 for |y| (cid:62) 2. An
application of Lemma 3.3 gives

(5.52)

E[Tr fη(H)] = N

ϕf (x + iy) (EG(x + iy) − m(x + iy)) dx dy =..

˜F ,

(5.53)

(cid:90)

(cid:90)

where ϕf is deﬁned as in (5.7). Note that (3.9) and (3.11) imply

uniformly in x ∈ R, |y| (cid:54) 1. Then we have

(cid:12)(cid:12)G(x + iy) − m(x + iy))(cid:12)(cid:12) ≺ 1
(cid:12)(cid:12)(cid:12)(cid:12) ≺

(cid:90)

|y|(cid:54)σ

˜F

N|y|

(cid:12)(cid:12)(cid:12)(cid:12)ϕf (z) · 1
(cid:12)(cid:12)(cid:12)(cid:12)ϕf (z) · 1

(cid:12)(cid:12)(cid:12)(cid:12) d2z = O(N−rq0β) ,
(cid:12)(cid:12)(cid:12)(cid:12) d2z = O(N β−sα/2) = O(N−sc0/2) .

y

y

(cid:12)(cid:12)(cid:12)(cid:12)(cid:90)
(cid:12)(cid:12)(cid:12)(cid:12) ≺

|y|(cid:54)σ
(cid:62) s

(cid:90)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:90)

where q0 = q0(s) = s

2(1+s)

4 . Also, we have

˜F

|y|(cid:62)σ, |x|>2− κ

2

|y|(cid:62)σ, |x|>2− κ

2

An analogue of (4.4) yields

uniformly in |x| (cid:54) 2 − κ

(cid:90)

|y|(cid:62)σ, |x|(cid:54)2− κ

2

EG(x + iy) − m(x + iy) = O(N (α+β)−1−c0(α+β))
2 , |y| (cid:62) σ, where the function c0(·) is deﬁned as in (5.22). Thus
˜F = O(N (α+β)−c0(α+β) · η) = O(N β−c0(α+β)) = O(N−c0/2) .

Altogether we have (5.3).

26

(5.54)

(5.55)

(5.56)

(5.57)

5.5. Remark on the general case. Let us turn to Theorem 5.1. As in the 1-dimensional case,
we ﬁrst show that

as N → ∞, where ˜Z(fi) ..= (cid:10) Tr fi( H−E

( ˜Z(f1), ..., ˜Z(fp))

d−→ (Z(f1), ..., Z(fp))

)(cid:11) for 1 (cid:54) i (cid:54) p. In order to show (5.58), it suﬃces to

(5.58)

compute E[ ˜Z(fi1)··· ˜Z(fin)], i1, ..., in ∈ {1, 2, ..., p}, and this follows exactly the same way as we
compute E(cid:104)Tr fη(H)(cid:105)n. Theorem 5.1 then follows from the estimate E ˆZ(fi) = O(N−rs2c0/16) for
all i ∈ {1, 2, ..., p}, which is Lemma 5.3 (ii).

η

6. Relaxing the moment condition

In this section we use a Green function comparison argument to pass from Theorems 4.1 and 5.1
to Theorems 2.2 and 2.3.
Recall G ..= G(E +iη) = (H −E−iη)−1, and [G] ..= G−m(E +iη) with E, η deﬁned in Theorem

2.2. Similar as in Section 4, we have a particular case of Theorem 2.2.

Proposition 6.1. Let η, E, H be as in Theorem 2.2. Then

as N → ∞.

N 1−α[G]

d−→ NC

(cid:16)

0,

1
2

(cid:17)

(6.1)

In this section we only sketch a proof of Proposition 6.1, and the other results can be proved

analogously. We begin with the following lemma.

Lemma 6.2. Fix m > 2 and let X be a real random variable, with absolutely continuous law,
satisfying

EX = 0 ,

EX 2 = σ2 ,

E|X|m (cid:54) Cm

(6.2)

for some constant Cm > 0. Let λ > 2σ. Then there exists a real random variable Y that satisﬁes

EY = 0,

EY 2 = σ2 ,

(6.3)
In particular, E|Y |m (cid:54) 3Cm. Moreover, if m > 4 and σ = 1, then there exists a real random
variable Z matching the ﬁrst four moments of Y , and satisﬁes |Z| (cid:54) 6Cm.

|Y | (cid:54) λ ,

P(X (cid:54)= Y ) (cid:54) 2Cmλ−m .

The existence of Y is a slight modiﬁcation of Lemma 7.6 in [13], and the construction of Z is

contained in the proof of Theorem 2.5 in [13]; we omit further details.

The next lemma is an easy application of Lemma 6.2.

Lemma 6.3. Let H be a real symmetric Wigner matrix, whose entries have absolutely continuous
law. Let c be as in Deﬁnition 2.1. Then there exists a real symmetric Wigner matrix H (1) satisfying
Deﬁnition 2.1 and

P(H (1)

ij

max

i,j

(cid:54)= Hij) = O(N−2−c/4+δij ) ,

|H (1)

ij | (cid:54) N−ε ,

max

i,j

(6.4)

where ε = ε(c) ..= c
Deﬁnition 3.2, such that for all i, j,

4(4+c) > 0. Moreover, there exists a real symmetric Wigner matrix H (2) satisfying

E(cid:0)H (2)

(cid:1)k = E(cid:0)H (1)

(cid:1)k ,

ij

ij

(6.5)

where 1 (cid:54) k (cid:54) 4 − 2δij.

27

Proof. Fix c, C > 0 such that E|√
m ..= 4 + c − 2δij, X ..=
(cid:1)2 = EHij
variable H (1)
ij

E(cid:0)H (1)

N Hij, λ ..= N 1/2−ε, Cm
..= N−1/2Y such that the family {H (1)

EH (1)

|H (1)

√

ij = 0 ,

ij

N Hij|4+c−2δij (cid:54) C for all i, j. By using Lemma 6.2 with
..= C, we construct, for each Hij, a random

ij }i(cid:54)j is independent and
P(Hij (cid:54)= H (1)

ij | (cid:54) N−ε ,

ij ) (cid:54) 2CN−2−c/4+δij ,

and we also have

2 ,
√
E|

N H (1)

ij |4+c−2δij (cid:54) 3C .

Hence we have proved the existence of H (1).

√

..= N−1/2Z matching the ﬁrst four moments of H (1)

For i < j, by using the second part of Lemma 6.2 on Y =
H (1)
ij , a random variable H (2)
ij }i<j is independent. Moreover, we have the bound |√
{H (2)
has uniformly bounded moments of all order. Let us denote ζi
struct random variables H (2)
ii
This completes the proof.

N H (2)
ii

such that

√

ij

N H (2)

N H (1)
ii
d∼ N (0, ζi) and the family {H (2)

N H (1)

ij | (cid:54) 6C, which ensures
..= E|√

ij , we construct, for each
ij , and the family
N H (2)
ij
|2. Then we can con-
ij }i(cid:54)j is independent.

√

Now we look at Proposition 6.1.

Proof of Proposition 6.1. Let H be as in Theorem 2.2. Note that it suﬃces to consider the
case that the entries of H have absolutely continuous law. Otherwise consider the matrix

H(cid:48) ..= (1 − e−2N )1/2 · H + e−N V ,

where V is a GOE matrix independent of H. Then H(cid:48) also satisﬁes Deﬁnition 2.1. Let G(cid:48) ..=
(H(cid:48) − E − iη)−1, and [G(cid:48)] ..= G(cid:48) − m(E + iη) with E, η deﬁned in Theorem 2.2. The resolvent

identity G(cid:48) − G = G(H − H(cid:48))G(cid:48) implies(cid:12)(cid:12)[G(cid:48)] − [G](cid:12)(cid:12) ≺ e−N/2 .
(cid:19)

We can then construct H (1) and H (2) from H, as in Lemma 6.3.

Let z ..= E + iη. We have already obtained from Proposition 4.2 that

(cid:18) 1

(cid:16)

(cid:17)

N 1−α

1

Tr

H (2) − z

N

− m(z)

d−→ NC

0,

1
2

,

(6.6)

and we need to show (6.6) holds with H (2) replaced by H. We ﬁrst compare the local spectral
statistics of H (2) and H (1) using the Green function comparison method from [15]; see also Section
4 of [9] for an overview. Fix a bijective ordering map on the index set of the independent matrix
entries,

φ : {(i, j) : 1 (cid:54) i (cid:54) j (cid:54) N} −→ {1, ..., γ(N )} , γ(N ) ..=

N (N + 1)

2

,

and we assume φ(i, i) = i for i = 1, 2, ..., N . Denote by Hγ the Wigner matrix whose matrix entries
hij = H (2)
ij otherwise; in particular H (2) = H0 and H (1) = Hγ(N ). Let
ij
F = F (x + iy) be a complex-valued, smooth, bounded function, with bounded derivatives. Then

if φ(i, j) (cid:54) γ and hij = H (1)

(cid:18) 1

N

N 1−α

(cid:18)

(cid:18)
(cid:20)
γ(N )(cid:88)

EF

γ=1

Tr

(cid:18) 1

1

H (2) − z

− m(z)

Tr

1

Hγ−1 − z

N

(cid:19)(cid:19)

(cid:18)

− EF

(cid:19)(cid:19)

(cid:18) 1
(cid:18)

N

N 1−α

1

Tr

H (1) − z

− m(z)

(cid:18) 1

N

Tr

1

Hγ − z

− m(z)

(cid:19)(cid:19)

(cid:19)(cid:19)(cid:21)

.

=

EF

N 1−α

− m(z)

− EF

N 1−α

28

Now we focus on the term

(cid:18)

(cid:18) 1

EF

N 1−α

1

Tr

Hγ−1 − z

− m(z)

− EF

N 1−α

N
For γ > N , let (i, j) = φ−1(γ). Note that i < j, and we deﬁne

N

(cid:19)(cid:19)

(cid:18)

(cid:18) 1

(cid:19)(cid:19)

− m(z)

=.. εγ .

Tr

1

Hγ − z

˜V = H (2)

ij ∆(ij) ,
kl = (δikδjl + δjkδil)(1 + δij)−1. Denote
and recall from section 4.2 that the matrix ∆(ij) satisﬁes ∆(ij)
Q ..= Hγ−1 − ˜V . Then Hγ−1 = Q + ˜V , and ˜V is independent of Q. Also, Hγ = Q + ˆV . Deﬁne the
Green functions

ij ∆(ij) , ˆV = H (1)

R ..=

1

Q − z

, S ..=

1

Hγ−1 − z

, T ..=

1

Hγ − z

,

and the resolvent expansion gives

S = R − R ˜V R + (R ˜V )2 − (R ˜V )3R + (R ˜V )4R − (R ˜V )5S .

(6.7)

Since ˜V has only at most two nonzero entries, when computing the (k, l) matrix entry of this
matrix identity, each term is a ﬁnite sum involving matrix entries of S or R and H (2)
ij , e.g. (S ˜V S)kl =
ji Sil. Let ˚S ..= N 1−α(S−m(z)), and ˚R, ˚T are deﬁned analogously. Set ξ ..= ˚S− ˚R,
SkiH (2)
and note that one can easily obtain ξ from (6.7). Similarly, µ ..= ˚T − ˚R, and we have an explicit
expansion for

ij Sjl+SkjH (2)

εγ = EF (˚S) − EF (˚T ) = EF (˚R + ξ) − EF (˚R + µ) .

(6.8)
Now we expand F (˚R + ξ) and F (˚R + µ) around ˚R using Taylor expansion. The detailed formulas
of the expansion can be found in Section 4.1 of [9], and we omit them here. Since the ﬁrst four
moments of the entries of H (1) and H (2) coincide, the error is bounded by the terms with factors
(H (1)

ji )n2 in the expansion, where m1 + n1, m2 + n2 (cid:62) 5. Since

ji )n1 or (H (2)

ij )m1(H (1)

ij )m2(H (2)

E(cid:12)(cid:12)H (1)

ij

(cid:12)(cid:12)m = N−(m−4−c)ε · E(cid:12)(cid:12)H (1)
(cid:12)(cid:12)m = O(N− m

E(cid:12)(cid:12)H (2)

ij

ij

2 )

(cid:12)(cid:12)4+c = O(N−2−c/2) ,

and

(cid:18)

for all m (cid:62) 5, a routine estimate shows the rest terms are bounded by O(N−2−c/2). Thus we have
εγ = O(N−2−c/2) uniformly in γ > N . Similarly, one can show εγ = O(N−1−c/2) uniformly for
γ (cid:54) N . Thus

EF

N 1−α

Tr

1

H (2) − z

− m(z)

− EF

N 1−α

Tr

1

H (1) − z

− m(z)

= O(N−c/2) .

(cid:19)(cid:19)

The transition from H (1) to H is immediate, since we have

(cid:18) 1
(cid:18)

N

(cid:18) 1

(cid:12)(cid:12)(cid:12)(cid:12)EF
(cid:54) O(P(H (1) (cid:54)= H)) (cid:54)(cid:88)

N 1−α

Tr

N

(cid:19)(cid:19)

(cid:18)
(cid:19)(cid:19)

(cid:18) 1
(cid:18)

N

1

H (1) − z
P(H (1)

ij

− EF

− m(z)
(cid:54)= Hij) = O(N−c/4) .

N 1−α

(cid:18) 1

N

(cid:19)(cid:19)(cid:12)(cid:12)(cid:12)(cid:12)

Tr

1

H − z

− m(z)

(6.9)

i,j

Note that by an approximation argument, for F in the above class,

suﬃcient in showing XN

d→ X. Thus we have ﬁnished the proof.

EF (XN ) = EF (X) is

lim
N→∞

29

7. The complex Hermitian case

We conclude the paper with a remark on the complex Hermitian case. As mentioned in Remark
2.4, in the complex case we now have (2.9) and (2.10) instead of Theorem 2.2 and 2.3. We omit
the complete statements of the results here. The proof in complex Hermitian case replies on the
complex cumulant expansion, which we state in the lemma below, whose proof is omitted.

Lemma 7.1. (Complex cumulant expansion) Let h be a complex random variable with all its mo-
ments exist. The (p, q)-cumulant of h is deﬁned as

C(p,q)(h) ..= (−i)p+q ·

(cid:18) ∂p+q
∂sp∂tq log Eeish+it¯h

(cid:19)(cid:12)(cid:12)(cid:12)(cid:12)s=t=0

.

Let f : C2 → C be a smooth function, and we denote its holomorphic derivatives by

f (p,q)(z1, z2) ..=

∂p+q
p∂z2

∂z1

q f (z1, z2) .

Then for any ﬁxed l ∈ N, we have

Ef (h, ¯h)¯h =

l(cid:88)

p+q=0

1

p! q!

C(p,q+1)(h)Ef (p,q)(h, ¯h) + Rl+1 ,

(7.1)

given all integrals in (7.1) exists. Here Rl+1 is the remainder term depending on f and h, and for
any τ > 0, we have the estimate

Rl+1 = O(1) · E(cid:12)(cid:12)hl+2 · 1{|h|>N τ−1/2}

+ O(1) · E|h|l+2 · max

p+q=l+1

(cid:13)(cid:13)f (p,q)(z, ¯z)(cid:13)(cid:13)∞
(cid:12)(cid:12) · max
(cid:13)(cid:13)∞ .
(cid:13)(cid:13)f (p,q)(z, ¯z) · 1{|z|(cid:54)N τ−1/2}

p+q=l+1

Using Lemma 7.1 it is not hard to extend the argument of Sections 4–5 to the complex case.

We sketch the required modiﬁcations.
Let H be a complex Wigner matrix. An argument analogous to Section 6 shows that it suﬃces
to consider H satisfying Deﬁnition 3.2. Let G ..= G(E + iη) = (H − E − iη)−1 with E, η deﬁned in
Theorem 2.2. Let m, n (cid:62) 1. Since H is complex hermitian, for any diﬀerentiable f = f (H) we set

(cid:12)(cid:12)(cid:12)t=0

f(cid:0)H + t ˜∆(ij)(cid:1) ,

∂

∂Hij

f (H) ..=

d
dt

(7.2)

(7.3)

(7.4)

where ˜∆(ij) denotes the matrix whose entries are zero everywhere except at the site (i, j) where it
is one: ˜∆(ij)

kl = δikδjl. Then by using Lemma 7.1 with h = Hij we have

zE(cid:104)G∗(cid:105)n(cid:104)G(cid:105)m =

1
N

(cid:88)
(cid:88)

i,j

i,j

=

1
N 2

E(cid:104)(cid:104)G∗(cid:105)n(cid:104)G(cid:105)m−1(cid:105)GijHji
E ∂((cid:104)(cid:104)G∗(cid:105)n(cid:104)G(cid:105)m−1(cid:105)Gij)

∂Hij

+ ˆK + ˆL ,

where ˆK and ˆL are deﬁned analogously to K and L in (4.17). Note that

∂Gij
∂Hkl

= −GikGlj ,

30

and by (7.3) we have

E(cid:104)G∗(cid:105)n(cid:104)G(cid:105)m =

1
T
− ˆK
T

− ˆL
T

+

n

N 2T

E(cid:104)G∗(cid:105)n−1(cid:104)G(cid:105)m−1GG∗2 ,

E(cid:104)G∗(cid:105)n(cid:104)G(cid:105)m+1 − 1
T

E(cid:104)G∗(cid:105)n(cid:104)G(cid:105)m−1E(cid:104)G(cid:105)2 +

m − 1
N 2T

E(cid:104)G∗(cid:105)n(cid:104)G(cid:105)m−2G3

(7.5)

where T = −z−2EG. By a comparison of (7.5) and its real analogue (4.22), we see that the leading
term is now halved. By estimating the subleading terms in a similar fashion, one can show that
instead of (4.55), we have

N 2α−2E(cid:104)G∗(cid:105)n−1(cid:104)G(cid:105)m−1 + O(cid:0)N (m+n)(α−1)−c0(cid:1) ,

E(cid:104)G∗(cid:105)n(cid:104)G(cid:105)m =

n
4

which agrees with our statement that we have an additional factor of 1/2 in the covariances.

Acknowledgements

The authors were partially supported by the Swiss National Science Foundation grant 144662 and
the SwissMAP NCCR grant.

References

[1] B.L. Altshuler and B.I. Shklovskii. Repulsion of energy levels and the conductance of small

metallic samples. Zh. Eksp. Teor. Fiz. (Sov. Phys. JETP) 91 (64), 220 (1986).

[2] Z.D. Bai and J. Yao. On the convergence of the spectral empirical process of Wigner matrices.

Bernoulli 11 (6), 1059–1092 (2005).

[3] F. Benaych-Georges and A. Knowles. Lectures on the local semicircle law for Wigner matrices.

Preprint arXiv:1601.04055.

[4] A. Bloemendal, A. Knowles, H.-T. Yau, and J. Yin. On the principal components of sample

covariance matrices. Preprint arXiv:1404.0788.

[5] J. Breuer and M. Duits. Universality of mesoscopic ﬂuctuations for orthogonal polynomial

ensembles. Comm. Math. Phys. 342 (2), 491–531 (2016).

[6] C. Cacciapuoti, A. Msltsev, and B. Schlein. Bounds for the Stieltjes transform and the density
of states for Wigner matrices. Probab. Theory Related Fields. DOI: 10.1007/s00440-014-0586-4.

[7] E.B. Davies. The functional calculus. J. London Math Soc. (2) 52 (1), 166–176 (1995).

[8] M. Duits and K. Johansson. On mesoscopic equilibrium for linear statistics in Dyson’s Brow-

nian Motion. Preperint arXiv:1312.4295.

[9] L. Erd˝os. Universality of Wigner random matrices: a survey of recent results. Russian Math.

Surveys 66 (3), 67–198 (2011).

[10] L. Erd˝os and A. Knowles. The Altshuler-Shklovskii formulas for random band matrices I: the

unimodular case. Comm. Math. Phys. 333, 1365–1416 (2015).

31

[11] L. Erd˝os and A. Knowles. The Altshuler-Shklovskii formulas for random band matrices II: the

general case. Ann. H. Poincar´e 16, 709–799 (2015).

[12] L. Erd˝os, A. Knowles, H.-T. Yau, and J. Yin. The local semicircle law for a general class of

random matrices. Elect. J. Prob. 18, Article 59, 1–58 (2013).

[13] L. Erd˝os, A. Knowles, H.-T. Yau, and J. Yin. Spectral statistics of Erd˝os-R´enyi graphs II:

eigenvalue spacing and the extreme eigenvalues. Comm. Math. Phys. 314, 587–640 (2012).

[14] L. Erd˝os and H.-T. Yau. Universality of local spectral statistics of random matrices. Bull.

Amer. Math. Soc. 49 (3), 377–414 (2012).

[15] L. Erd˝os, H.-T. Yau, and J. Yin. Universality for generalized Wigner matrices with Bernoulli

distribution. Journal of Combinatorics 2 (1), 15–82 (2011).

[16] L. Erd˝os, H.-T. Yau, and J. Yin. Rigidity of eigenvalues of generalized Wigner matrices.

Advances in Mathematics 229 (3), 1435–1515 (2012).

[17] A. Lodhia and N. J. Simm. Mesoscopic linear statistics of Wigner matrices. Preprint

arXiv:1503.03533.

[18] A. Boutet de Monvel and A. Khorunzhy. Asymtotic distribution of smoothed eigenvalue den-
sity. I. Gaussian random matrices. Random Oper. and Stoch Equ., Vol. 7, No. 1, 1–22 (1999).

[19] A. Boutet de Monvel and A. Khorunzhy. Asymtotic distribution of smoothed eigenvalue den-
sity. II. Wigner random matrices. Random Oper. and Stoch Equ., Vol. 7, No. 2, 149–168 (1999).

32

