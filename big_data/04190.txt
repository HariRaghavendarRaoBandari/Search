6
1
0
2

 
r
a

 

M
4
1

 
 
]

G
L
.
s
c
[
 
 

1
v
0
9
1
4
0

.

3
0
6
1
:
v
i
X
r
a

Online Isotonic Regression

Wojciech Kotłowski
Pozna´n University of Technology
Wouter M. Koolen
Centrum Wiskunde & Informatica

Alan Malek
University of California at Berkeley

WKOTLOWSKI@CS.PUT.POZNAN.PL

WMKOOLEN@CWI.NL

MALEK@BERKELEY.EDU

Abstract

We consider the online version of the isotonic regression problem. Given a set of linearly or-
dered points (e.g., on the real line), the learner must predict labels sequentially at adversarially
chosen positions and is evaluated by her total squared loss compared against the best isotonic (non-
decreasing) function in hindsight. We survey several standard online learning algorithms and show
that none of them achieve the optimal regret exponent; in fact, most of them (including Online
Gradient Descent, Follow the Leader and Exponential Weights) incur linear regret. We then prove
that the Exponential Weights algorithm played over a covering net of isotonic functions has regret
is bounded by O(T 1/3 log2/3(T )) and present a matching Ω(T 1/3) lower bound on regret. We also
provide a computationally efﬁcient version of this algorithm. We also analyze the noise-free case,
in which the revealed labels are isotonic, and show that the bound can be improved to O(log T ) or
even to O(1) (when the labels are revealed in the isotonic order). Finally, we extend the analysis
beyond squared loss and give bounds for log-loss and absolute loss.
Keywords: online learning, isotonic regression, isotonic function, monotonic, nonparametric re-
gression, exp-concave loss.

1. Introduction

We propose a problem of sequential prediction in the class of isotonic (non-decreasing) functions.
At the start of the game, the learner is given a set of T linearly ordered points (e.g., on the real
line). Then, over the course of T trials, the adversary picks a new (as of yet unlabeled) point and the
learner predicts a label from [0, 1] for that point. Then, the true label (also from [0, 1]) is revealed,
and the learner suffers the squared error loss. After T rounds the learner is evaluated by means of
the regret, which is its total squared loss minus the loss of the best isotonic function in hindsight.

This is the online version of isotonic regression, a fundamental problem in statistics, which
concerns ﬁtting a sequence of data where the prediction is an isotonic function of the covariate
(Ayer et al., 1955; Brunk, 1955; Robertson et al., 1998). Isotonicity constraints arise naturally in
many structured problems; e.g. predicting the height of children as a function of age, autocorrelation
functions, or biomedical applications such as estimating drug dose responses (Stylianou and Flournoy,
2002). The problem is interesting as, while being simple and commonly used in practice, it is an
example of nonparametric regression, where then number of parameters describing the available
functions grows linearly with the number of data points. A natural question to ask is whether there
are efﬁcient, provably low regret algorithms for online isotonic regression.

c(cid:13) W. Kotłowski, W.M. Koolen & A. Malek.

KOTŁOWSKI KOOLEN MALEK

Since online isotonic regression concerns minimizing a convex loss function over the convex
set of feasible prediction strategies (isotonic functions), it can be analyzed within the framework of
online convex optimization (Shalev-Shwartz, 2001). We thus start with a survey of popular online
learning algorithms in our setting and argue that most of them (including Online Gradient Descent,
Follow the Leader and Exponential Weights) suffer regret that is linear in the number of data points
in the worst case. We believe that the fact that most standard approaches fail makes the considered
problem particularly interesting. We also show that the Exponentiated Gradient algorithm delivers
O(√T log T ) regret guarantee, which although nontrivial, is, however, still suboptimal.

We then propose an algorithm, which achieves the regret bound O(T 1/3 log2/3(T )). The algo-
rithm is a simple instance of Exponential Weights that plays on a covering net (discretization) of the
class of isotonic functions. This algorithm turns out to be computationally efﬁcient, with O(T 4/3)
time per trial. We show a lower bound Ω(T 1/3) on the regret of any algorithm, which proves that
the proposed algorithm is optimal (up to a logarithmic factor).

We also analyze the noise-free case, in which the labels revealed by the adversary are isotonic,
so that the loss of the best isotonic function is 0. We show that the achievable worst-case regret in
this case scales only logarithmically in T . Furthermore, if we additionally assume that the labels are
queried in the isotonic order (from left to right), the achievable worst-case regret drops to 1. In both
cases, we are able to determine the minimax algorithm, and the actual value of the minimax regret.
Finally, we also go beyond the squared loss. We show how to adapt our discretized Exponential
Weights algorithm to logarithmic loss and get the same regret guarantee as for the squared loss case.
We also consider isotonic regression with absolute loss. We show that the minimax regret is of order
O(√T ) and is achieved, up to a logarithmic factor, by the Exponentiated Gradient algorithm.

1.1. Related work

Isotonic regression has been extensively studied in statistics starting from work by Ayer et al. (1955);
Brunk (1955). The excellent book by Robertson et al. (1998) provides a history of the subject and
numerous references to the statistical literature.
Isotonic regression has applications throughout
statistics (e.g. nonparametric regression, estimating monotone densities, parameter estimation and
statistical tests under order constraints, multidimensional scaling (Robertson et al., 1998)) and to
more practical problems in biology, medicine, psychology, etc. (Kruskal, 1964; Stylianou and Flournoy,
2002; Obozinski et al., 2008; Luss et al., 2012).

The classical problem of minimizing an isotonic function under squared loss (the ofﬂine coun-
terpart of this paper) has usually been studied in statistics under a generative model yi = f (xi) + ǫi
with f (xi) being some isotonic function and ǫi being random i.i.d. noise variables (Van de Geer,
1990; Birg´e and Massart, 1993; Zhang, 2002). It is known (see, e.g., Zhang, 2002) that the statisti-
cal risk of the isotonic regression function E[ 1
T is the sample size. Interestingly, this matches (up to a logarithmic factor) our results on online
isotonic regression, showing that the online version of the problem is not fundamentally harder.

T kbf − fk2] converges at the rate of O(T −2/3), where

In machine learning, isotonic regression is used to calibrate class probability estimates (Zadrozny and Elkan,

2002; Niculescu-Mizil and Caruana, 2005; Menon et al., 2012; Narasimhan and Agarwal, 2013; Vovk et al.,
2015), for ROC analysis (Fawcett and Niculescu-Mizil, 2007), for learning Generalized Linear
Models and Single Index Models (Kalai and Sastry, 2009; Kakade et al., 2011), for data cleaning
(Kotłowski and Słowi´nski, 2009) and for ranking (Moon et al., 2010). Recent work by Kyng et al.
(2015) proposes fast algorithms under general partial order constraints. None of these works are

2

ONLINE ISOTONIC REGRESSION

directly related to the subject of this paper. The one related problem we found is online learning
with logarithmic loss for the class of monotone predictors as studied by Cesa-Bianchi and Lugosi
(2001), who give an upper bound on the minimax regret (the bound is not tight for our case).

We also note that the problem considered here falls into a general framework of online nonpara-
metric regression. Rakhlin and Sridharan (2014) give nonconstructive upper and lower bound on
the minimax regret, but using their bounds for a particular function class requires upper and lower
bounds on its sequential entropy. In turn, our upper bound is achieved by an efﬁcient algorithm,
while the lower bound follows from a simple construction. Gaillard and Gerchinovitz (2015) pro-
pose an algorithm, called Chaining Exponentially Weighted Average Forecaster, that is based on
aggregation on two levels: On the ﬁrst level a multi-variable version of Exponentiated Gradient is
used, while on the second level the Exponential Weights algorithm is used. The combined algo-
rithm works for any totally bounded (in terms of metric entropy) set of functions, which includes
our case. It is, however, computationally inefﬁcient in general (an efﬁcient adaptation of the algo-
rithm is given for the H¨older class of functions, to which our class of isotonic functions does not
belong). In contrast, we achieve the optimal bound by using a simple and efﬁcient Exponential
Weights algorithm on a properly discretized version of our function class (interestingly, the Expo-
nential Weights algorithm is shown by Gaillard and Gerchinovitz (2015) to have suboptimal bound
for general nonparametric classes).

2. Problem statement
Let x1 ≤ x2 ≤ . . . ≤ xT , be a set of T linearly ordered points (e.g., on the real line), denoted
by X. We call a function f : X → R isotonic (order-preserving) on X if f (xi) ≤ f (xj) for any
xi ≤ xj. Given data (y1, x1), . . . , (yT , xT ), the isotonic regression problem is to ﬁnd an isotonic f
that minimizesPT
t=1(yt− f (xt))2, and such optimal function is called isotonic regression function.
We consider the online version of the isotonic regression problem. The adversary chooses X =
{x1, . . . , xT} which is given in advance to the learner. In each trial t = 1, . . . , T , the adversary
picks a yet unlabeled point xit, it ∈ {1, . . . , T} and the learner predicts withbyit ∈ [0, 1]. Then, the
actual label yit ∈ [0, 1] is revealed, and the learner is charged by the squared loss (yit −byit)2. Thus,

The goal of the learner is to have small regret, which is the difference of the cumulative loss and

the learner predicts all points x1, . . . xT but in an adversarial order.

the cumulative loss of the best isotonic function in hindsight:

RegT :=

TXt=1
(yt −byt)2 − min

isotonic f

TXt=1
(yt − f (xt))2.

Note that neither the labels nor the learner’s predictions are required to be isotonic on X. In what
follows, we assume without loss of generality that x1 < x2 < . . . < xT , because equal consecutive
points xi = xi+1 constrain the adversary (f (xi) = f (xi+1) for any function f ) but not the learner.

Fixed-design We now argue that without showing X to the learner in advance the problem is
hopeless; if the adversary can choose xit online, any learning algorithm will suffer regret at least
1
4 T (a linear regret implies very little learning is happening since playing randomly obtains linear

regret). To see this, assume the adversary chooses xi1 = 0; given learner’s prediction byi1, the

adversary can choose yi1 ∈ {0, 1} to cause loss at least 1
4. Now, after playing round t, the adversary
chooses xit+1 = xit − 2−t if yit = 1 or xit+1 = xit + 2−t if yit = 0. This allows the adversary to

3

KOTŁOWSKI KOOLEN MALEK

At trial t = 1 . . . T :
Adversary chooses index it, such that it /∈ {i1, . . . , it−1}.
Adversary reveals label yit ∈ [0, 1].

Learner predictsbyit ∈ [0, 1].
Learner suffers squared loss (yit −byit)2.

Figure 1: Online protocol for isotonic regression.

set yit+1 to any value and still respect isotonicity. So regardless ofbyit+1, the adversary inﬂicts loss

at least 1
4 . This guarantees that if yit = 1, xiq < xit for all future points q = t + 1, . . . , T ; similarly,
if yit = 0, xiq > xit for all q > t. Hence, the labels’ assignment is always isotonic on X, and the
loss of the best isotonic function in hindsight is 0 (by choosing f (xi) = yi, i = 1, . . . , T ) while the
total loss of the learner is at least 1

4 T .

Thus, the learner needs to know X in advance. On the other hand, the particular values xi ∈ X do
not play any role in this problem, as it is only the order on X that matters. Thus, we may without
loss of generality assume that xi = i and represent isotonic functions by vectors f = (f1, . . . , fT ),
where fi := f (i). We denote by F the set of all [0, 1]-valued isotonic functions:
F = {f = (f1, . . . , fT ) : 0 ≤ f1 ≤ f2 ≤ . . . ≤ fT ≤ 1}

Using this notation, the protocol for online isotonic regression is presented in Figure 1.

We will use bLT = PT
PT
t=1(yt − ft)2 to denote the total loss of the isotonic function f ∈ F. The regret of the algorithm
can then be concisely expressed as RegT = bLT − minf∈F LT (f ).

t=1(yt −byt)2 to denote the total loss of the algorithm and LT (f ) =

The ofﬂine solution. The classic solution to the isotonic regression problem is computed by the
Pool Adjacent Violators Algorithm (PAVA) (Ayer et al., 1955). The algorithm is based on the obser-
vation that if the labels of any two consecutive points i, i + 1 violate isotonicity, then f∗i = f∗i+1
in the optimal solution and we may merge both points to their average. This process repeats and
terminates in at most T steps with the optimal solution. Efﬁcient O(T ) time implementations exist
(de Leeuw et al., 2009). There are two important properties of the isotonic regression function f∗
that we will need later (Robertson et al., 1998):

1. The function f∗ is piecewise constant and thus its level sets partition {1, . . . , T}.
2. The value of f∗ on any level set is equal to the weighted average of labels within that set.

3. Blooper reel

The online isotonic regression problem concerns minimizing a convex loss function over the convex
class of isotonic functions. Hence, the problem can be analyzed with online convex optimization
tools (Shalev-Shwartz, 2001). Unfortunately, we ﬁnd that most of the common online learning
algorithms completely fail on the isotonic regression problem in the sense of giving linear regret
guarantees or, at best, suboptimal rates of O(√T ); see Table 1. We believe that the fact that most
standard approaches fail makes the considered problem particularly interesting and challenging.

4

ONLINE ISOTONIC REGRESSION

General bound Bound for online IR

Algorithm

Online GD
EG
FTL
Exponential Weights

G2D2√T
G∞D1√T log d

G2D2d log T
d log T

T
√T log T
T 2 log T
T log T

Table 1: Comparison of general bounds as well as bounds specialized to online isotonic regression
for various standard online learning algorithms. For general bounds, d denotes the dimen-
sion of the parameter vector (equal to T for this problem), Gp is the bound on the Lp-norm
of the loss gradient, and Dq is the bound on the Lq-norm of the parameter vector. Bounds
for FTL and Exponential Weights exploit the fact that the square loss is 1
2-exp-concave
(Cesa-Bianchi and Lugosi, 2006).

In the usual formulation of online convex optimization, for trials t = 1, . . . , T , the learner
predicts with a parameter vector wt ∈ Rd, the adversary reveals a convex loss function ℓt, and the
learner suffers loss ℓt(wt). To cast our problem in this framework, we set the prediction of the
t xit)2. There are two ways to

t xit and the loss to ℓt(wt) = (yit − w⊺

learner at trial t tobyit = w⊺

parameterize wt, xit ∈ Rd:

1. The learner predicts some f ∈ F and sets w = f. Then, xi is the i-th unit vector (with i-th
coordinate equal 1 and the remaining coordinates equal 0). Note that supw kwk2 = √T and
k∇ℓ(w)k2 ≤ 2 in this parameterization.

2. The learner predicts some f ∈ F and sets w = (f1 − f0, f2 − f1, . . . , fT +1 − fT ) ∈ RT +1,
i.e. the vector of differences of f (we used two dummy variables f0 = 0 and fT +1 = 1);
then, xi has the ﬁrst i coordinates equal to 1 and the last T − i coordinates equal to 0. Note
that kwk1 = 1, k∇ℓk∞ ≤ 2, but supy,w k∇ℓ(w)k2 = 2√T .

Table 1 lists the general bounds and their specialization to online isotonic regression for several
standard online learning algorithms: Online Gradient Descent (GD) (Zinkevich, 2003), Exponen-
tiated Gradient (EG) Kivinen and Warmuth (1997) when applied to exp-concave losses (which in-
clude squared loss (Cesa-Bianchi and Lugosi, 2006)), Follow the Leader1, and Exponential Weights
(Hazan et al., 2007). EG is assumed to be used in the second parameterization, while the bounds for
the remaining algorithms apply to both parameterizations (since G2D2 = Ω(√T ) in both cases).
EG is the only algorithm that provides a meaningful bound, of order O(√T log T ), as shown
in Appendix A. All the other bounds are vacuous (linear in T or worse). While this fact does not
completely rule out these algorithms since we do not know whether their bounds are tight in the
worst case for isotonic regression, we will exhibit sequences of outcomes that cause GD, FTL and
Exponential Weights to incur linear regret.

1. The Online Newton algorithm introduced by Hazan et al. (2007) is equivalent to FTL for squared loss.

5

KOTŁOWSKI KOOLEN MALEK

Theorem 1 For any learning rate η ≥ 0 and any initial parameter vector f 1 ∈ F, the Online
Gradient Descent algorithm, deﬁned as

f t = argmin

f∈F nkf − f t−1k2 + 2η(ft−1,it−1 − yit−1)o ,

suffers at least T

4 regret in the worst case.

Proof The adversary reveals the labels in the isotonic order (it = t for all t), and all the labels are

zero. Then, ℓt(f t) = ℓt(f 1), and the total loss of the algorithmbLT is equal to the loss of the initial
parameter vector: bLT = LT (f 1) =Pt f 2
1,t. This follows from the fact that f t and f t−1 can only
differ on the ﬁrst t − 1 coordinates (ft,q = ft−1,q for q ≥ t) so only the coordinates of the already
labeled points are updated. To see this, note that the parameter update can be decomposed into the
“descent” stepef t = f t−1 − 2ηft−1,t−1et−1 (where ei is the i-th unit vector), and the “projection”
step f t = argminf∈F kf −ef tk2 (which is actually the isotonic regression problem). The descent
step decreases (t − 1)-th coordinate by some amount and leaves the remaining coordinates intact.
Since f t−1 is isotonic, eft−1,t ≤ . . . ≤ eft−1,T and efq ≤ eft for all q < t. Hence, the projection step
will only affect the ﬁrst t − 1 coordinates.
By symmetry, one can show that when the adversary reveals the labels in the antitonic order
(it = T−t+1 for all t), and all the labels are 1, thenbLT =Pt(1−f1,t)2. Since f 2
1,t+(1−f1,t)2 ≥ 1

for any f1,t, the loss suffered by the algorithm on one of these sequences is at least T
4 .

2

Theorem 2 For any regularization parameter λ > 0 and any regularization center f 0 ∈ F, the
Follow the (Regularized) Leader algorithm deﬁned as:

f t = argmin

f∈F nλkf − f 0k2 +

(fiq − yiq )2o,
t−1Xq=1

suffers at least T

2 regret in the worst case.

Proof The proof uses exactly the same arguments as the proof of Theorem 1: If the adversary re-
veals labels equal to 0 in the isotonic order, or labels equal to 1 in the antitonic order, then ft,t = f0,t
for all t. This is because the constraints in the minimization problem are never active (argmin over
f ∈ RT returns an isotonic function).

We used a regularized version of FTL in Theorem 2, because otherwise FTL does not give

unique predictions for unlabeled points.

Theorem 3 The Exponential Weights algorithm deﬁned as:

f t =ZF

f pt(f ) dµ(f ),

where

pt(f ) =

RF

q=1(fiq−yiq )2

2 Pt−1
e− 1
2 Pt−1
q=1(fiq−yiq )2

e− 1

dµ(f )

,

with µ being a uniform (Lebesgue) measure over F, suffers regret Ω(T ) in the worst case.
The proof of Theorem 3 is long and is deferred to Appendix B.

6

ONLINE ISOTONIC REGRESSION

4. Optimal algorithm

We have hopefully provided a convincing case that many of the standard online approaches do not
work for online isotonic regression. Is this section, we present an algorithm that does: Exponential
Weights over a discretized version of F. We show that it achieves O(T 1/3(log T )2/3) regret which
matches, up to log factors, the O(T 1/3) lower bound we prove in the next section.
The basic idea is to form a covering net of all isotonic functions by discretizing F with resolution
1
K , to then play Exponential Weights on this covering net with a uniform prior, and to tune K to get
the best bound. We take as our covering net FK ⊂ F the set of isotonic functions which take values
of the form k

K , k = 0, . . . , K, i.e.

kt
K

K items into T + 1 bins, (see, e.g., DeTemple and Webb, 2014, section 2.4).

for some kt ∈ {0, . . . , K}(cid:27) .

FK :=(cid:26)f ∈ F : ft =
K (cid:1), since the enumeration of all isotonic function in FK
Note that FK is ﬁnite. In fact |FK| =(cid:0)T +K
is equal to the number of ways to distribute the K possible increments among bins [0, 1), . . . , [T −
1, T ), [T, T + 1). The ﬁrst and last bin are to allow for isotonic functions that start and end at
K (cid:1) ways to allocate
arbitrary values. It is a well known fact from combinatorics that there are(cid:0)T +K
t, each f in FK is given weight e− 1
efﬁcient implementation is given in Algorithm 1.
Using K =(cid:24)(cid:16)

4 log(T +1)(cid:17)1/3(cid:25), the regret of Exponential Weights with the uniform prior on the

The algorithm we propose is the Exponential Weights algorithm over this covering net; at round
q=1(fiq−yiq )2 and we play the weighted average of fit. An

Theorem 4

2 Pt−1

T

covering net FK has regret bounded by:

RegT ≤

3

22/3

T 1/3(cid:0)log(T + 1)(cid:1)2/3 + 2 log(T + 1).

LT (f ) ≤

log |FK|

η

= 2 log |FK|,

bLT − min

f∈FK

Proof Due to exp-concavity of the squared loss, running Exponential Weights with η = 1/2 guar-
antees that:

(see, e.g., Cesa-Bianchi and Lugosi, 2006, Proposition 3.1).

Let f∗ = argminf∈F LT (f ) be the isotonic regression function. The regret is

Reg =bLT − LT (f∗)
=bLT − min

f∈FK

LT (f ) + min
f∈FK

{z
Let us start with bounding ∆K. Let f + be a function obtained from f∗ by rounding each value
K for some kt ∈ {0, . . . , K}. It follows that f + ∈ FK and
f∗t to the nearest number of the form kt
∆K ≤ LT (f +) − LT (f∗). Using ℓt(x) := (yt − x)2, we have
t )2 − (yt − f∗t )2 = (f +

t ) − ℓt(f∗t ) = (yt − f +

t + f∗t − 2yt).

t − f∗t )(f +

ℓt(f +

|

(1)

.

LT (f ) − LT (f∗)
}

:=∆K

7

KOTŁOWSKI KOOLEN MALEK

Let Tc = {t : f∗t = c} be the level set of the isotonic regression function. It is known (Robertson et al.,
1998, see also Section 2) that (as long as |Tc| > 0):
|Tc|Xt∈Tc

yt = f∗t = c,

(2)

1

i.e., the isotonic regression function is equal to the average over all labels within each level set.
Now, choose any level set Tc with |Tc| > 0. Note that f + is also constant on Tc and denote its value
by c+. Summing (1) over Tc gives:

Xt∈Tc

ℓt(f +

t ) − ℓt(f∗t ) =Xt∈Tc

(c+ − c)(c+ + c − 2yt)

(from (2))

yt

= |Tc|(c+ − c)(c+ + c) − 2(c+ − c)Xt∈Tc
= |Tc|(c+ − c)(c+ + c) − 2|Tc|(c+ − c)c
= |Tc|(c+ − c)2
=Xt∈Tc

(f +
t − f∗t )2.

Since for any t, |f +

t − f∗t | ≤ 1

2K , we can sum over the level sets of f∗ to bound ∆K:

∆K ≤ LT (f +) − LT (f∗) ≤

TXt=1

ℓt(f +

t ) − ℓt(f∗f ) =

t − f∗t )2 ≤
(f +

T
4K 2 .

TXt=1

Combining these two bounds, we get:

RegT ≤ 2 log |FK| +

T
4K 2 ≤ 2K log(T + 1) +

T
4K 2 ,

where we used |FK| =(cid:0)T +K
to 0 gives K∗ =(cid:16)
4 log(T +1)(cid:17)1/3

T

K (cid:1) ≤ (T +1)K.2 Optimizing the bound over K by setting the derivative

. Taking K = ⌈K∗⌉ and plugging it in into the bound gives:
T 1/3(cid:0)log(T + 1)(cid:1)2/3 + 2 log(T + 1),

T
4K∗ ≤

22/3

3

RegT ≤ 2(K∗ + 1) log(T + 1) +

where we used K∗ ≤ K ≤ K∗ + 1.

The importance of being discrete Surprisingly, playing weighted averages over F does not work
(Theorem 3), but playing over a covering net does work. Indeed, the uniform prior exhibits wild
behavior by concentrating all mass around the “diagonal” monotonic function with constant slope
1/T , whereas the discretized version with the suggested tuning for K still has non-negligible mass
everywhere.

2. (cid:0)T +K

K (cid:1) = (T +1)·...(T +K)

1·...·K

; we get the bound by noticing that T +k

k

≤ T + 1 for k > 1.

8

ONLINE ISOTONIC REGRESSION

Algorithm 1: Efﬁcient Exponential Weights on the covering net

Input: Game length T , discretization K
Initialize βj
for t = 1, . . . , T do

s = 1 for all s = 1, . . . , T , j = 0, . . . , K;

1 and vk

T = βk

T for all k = 0, . . . , K;

s + βk

s−1wk

s−1 for all k = 0, . . . , K;

s + βk

s+1vk

s+1 for all k = K, . . . , 0;

Receive it;
1 = βk
Initialize wk
for s = 2, . . . , it do

s ← Jk > 0Kwk−1
wk
end
for s = T − 1, . . . , it do
s ← Jk < KKwk+1
vk
end
PK
PK

ˆyit ←
Receive yit and update βj
it

K wk
vk
k=0
it
it
vk
k=0 wk
it
it

;

k

end

= e− 1

2 ( j

K −yit )2 for all j = 0, . . . , K;

4.1. Efﬁcient Implementation
A na¨ıve implementation of exponential averaging has an intractable complexity of O(|Fk|) per
round. Fortunately, one can use dynamic programming to derive an efﬁcient weights update that is
able to predict in O(T K) time per round for arbitrary prediction orders and O(K) per round when
predicting in isotonic order. See Algorithm 1 for pseudocode.

Say we currently need to predict at it. We can compute the Exponential Weights prediction by

dynamic programming: for each k = 0, . . . , K let:

wk

s = X0≤f1≤...≤fs= k

K

2 Pq<t:iq <s(fiq −yiq )2

e− 1

and

vk

s = Xk

K =fs≤...≤fT ≤1

2 Pq<t:iq >s(fiq−yiq )2

e− 1

,

so that the exponentially weighted average prediction is:
2 Pq<t(fiq−yiq )2

ˆyit = Pf∈FK
Pf∈FK

fite− 1
e− 1

2 Pq<t(fiq−yiq )2 = PK
PK

k
itvk
K wk
it
k=0
k=0 wk
vk
it
it

.

Now we can compute the wk
s = e− 1
βj
1 = βk
wk

1 and then sweeping right:

2 ( j
K −ys)2 if s ∈ {i1, . . . , it−1} and 1 otherwise, we can calculate wk

s in one sweep from s = 1 to s = it for all k = 0, . . . , K. If we deﬁne
s by starting with

wk

s+1 =

2 Pq<t:iq ≤s(fiq−yiq )2

e− 1

2 Pq<t:iq <s(fiq−yiq )2

e− 1

K

X0≤f1≤...≤fs+1= k
= X0≤j≤k
= X0≤j≤k

s wj
βj
s.

βj

s X0≤f1≤...≤fs= j

K

9

KOTŁOWSKI KOOLEN MALEK

The equations for vk
algorithm. We can speed it up to O(T K) by using

s are updated symmetrically right-to-left, which gives an O(T K 2) per round

wk+1

s+1 = X0≤j≤k

and similarly for vk+1
s+1 .

βj
swj

s + βk+1

s wk+1

s

= wk

s+1 + βk+1

s wk+1

s

,

Acceleration for predicting in isotonic order When working in isotonic order (meaning it = t),
we can speed up the computation to O(K) per round (independent of T ) by the following tricks.
First, we do not need to spend work maintaining vk
between rounds t− 1 and t the wk
k, hence speeding up the computation to O(K) per round.

s do not change for s < t, and we only need to compute wk

t = (cid:0)T−t+K−k

(cid:1). Moreover,

t as they satisfy vk

t for all

K−k

5. Lower bound

Theorem 5 All algorithms must suffer

RegT = Ω(T 1/3).

The full proof is given in Appendix C.
Proof (sketch) We proceed by constructing the difﬁcult sequence explicitly. Split the T points
(1, . . . , T ) into K consecutive segments (1, . . . , m), (m + 1, . . . , 2m), . . . , (m(K − 1) + 1, . . . , T ),
where in each segment there are m = T
K consecutive points (for simplicity assume T is divisible
by K). Let t ∈ k mean that t is in the k-th segment, k = 1, . . . , K. Now, suppose the adversary
generates the labels i.i.d. with yt ∼ Bernoulli(pk) when t ∈ k, and p1 ≤ . . . ≤ pK. The total
loss of the best isotonic function is then bounded above by the total loss of the constant function
equal to pk in each segment, hence the expected regret of any algorithm can be lower-bounded by
Ep[RegT ] ≥PK

EpkhPt∈k(byt − pk)2i. We thus decomposed the problem into K independent

problems, each of which concerns bounding the expected regret against a constant comparator in
T
In each segment, the adversary picks pk ∈ {pk,0, pk,1}, where pk,0 = 1
2K and
K trials.
pk,1 = 1
2K , which guarantees that for any choice of the the adversary, it holds p1 ≤ . . . ≤ pK.
We then show that the expected regret within each segment can be lower-bounded by:

4 + k−1

4 + k

k=1

Xt∈k

m
4

E[(byt − pk)2] ≥

m

E[(bpk − pk)2] ≥

16K 2 Pr(bpk 6= pk),

wherebpk ∈ {pk,0, pk,1} depends on the predictions {byt}t∈k, but not on the probability pk. Following
the worst-case probability of error by maxpk∈{pk,0,pk,1} Pr(bpk 6= pk) ≥ 1

what is known as Le Cam’s lower-bounding method (Le Cam, 1986; Tsybakov, 2009), we bound
√m
K ). Summing over
k = 1, . . . , K and tuning the number of segments to K = Θ(T 1/3) to optimize the bound, gives
Ω(T 1/3) lower bound on the worst-case regret.

2 (1−

10

ONLINE ISOTONIC REGRESSION

6. Noise-free case

In this section, we are concerned with a particular case of “easy data”, when the labels revealed by
the adversary are actually isotonic: y1 ≤ y2 ≤ . . . ≤ yT , so that the loss of the best isotonic function
is zero. We show that the achievable worst-case regret in this case scales only logarithmically in
T . Furthermore, if we additionally assume that the labels are revealed in the isotonic order, the
achievable worst-case regret is bounded by 1. Interestingly, we were able to determine the minimax
algorithm, and the exact value of the minimax regret in both cases. Our ﬁndings are summarized in
the two theorems below. The proofs and the minimax predictions are given in Appendix D.

Theorem 6 Assume the labels revealed by the adversary are isotonic. Then, the regret of the
minimax algorithm is bounded above by:

RegT ≤

1
4

log2(T + 1).

Furthermore, when T = 2k − 1 for some positive integer k, any algorithm suffers regret at least
1
4 log2(T + 1).
Theorem 7 Assume the labels are isotonic, and they are revealed in the isotonic order (it = t for
all t). Then, the regret of the minimax algorithm is bounded above by:

where αT is deﬁned recursively as: α1 = 1
suffers regret at least αT .

RegT ≤ αT ≤ 1,

4 and αt = (cid:0) αt−1+1

2

(cid:1)2. Furthermore, any algorithm

Finally, we note that the logarithmic regret can also be obtained by using the Exponentiated Gra-

dient algorithm with its learning rate tuned for the noise-free case (see Appendix A and Kivinen and Warmuth,
1997, for details).

7. Other loss functions

We discuss extensions of the isotonic regression problem where the squared loss is replaced by the
logarithmic loss (log-loss) and the absolute loss respectively.

7.1. Log-loss

role in isotonic regression, as its minimization is equivalent to maximum likelihood estimation
for Bernoulli distributions under isotonic constraints (Robertson et al., 1998). It is convenient to

The log-loss, deﬁned for y,by ∈ [0, 1] by ℓ(y,by) = −y logby− (1− y) log(1−by), plays an important
replace the log-loss by its relative entropy version Dφ(ykby) = φ(y) − φ(by) − (y −by)⊺φ′(by),
which is the Bregman divergence generated by φ(y) = −y log y − (1 − y) log(1 − y), the binary
entropy. A surprising fact in isotonic regression is that minimizing the sum of Bregman divergences
Pt Dφ(ytkft) in the class of isotonic functions f ∈ F leads to the same optimal solution, no matter

what φ is: the isotonic regression function f∗ (Robertson et al., 1998).

Since the log-loss is 1-exp-concave (Cesa-Bianchi and Lugosi, 2006, page 46), we may use the

Exponential Weights algorithm on the discretized class of functions:

FK =(cid:8)f ∈ F : ∀t, ft ∈ {z0, z1, . . . , zK}(cid:9)

11

KOTŁOWSKI KOOLEN MALEK

(we now use a non-uniform discretization {z0, z1, . . . , zK}, to be speciﬁed later). Following the
steps of the proof of Theorem 4, we obtain a regret bound:

RegT ≤ log(cid:18)T + K

K (cid:19) + LT (f +) − LT (f∗),

t=1 Dφ(ytkft), f + is deﬁned by: f +

where LT (f ) = PT
t = argminz∈{z0,z1,...,zK} Dφ(f∗t kz), and
we used the fact that the isotonic regression function f∗ minimizes LT (f ) over F (see above). Let
Tc = {t : f∗t = c} be a non-empty level set of f∗. Using the averaging property of f∗ (2), and the
fact that f + is constant on Tc (denote its value by c+), we have:

Xt∈Tc

Dφ(ytkf +

t ) − Dφ(ytkf∗t ) =Xt∈Tc

(from (2))

φ(c) − φ(c+) − (yt − c+)ψ′(c+) + (yt − c)ψ(c)

= |Tc|Dφ(ckc+) + (ψ′(c) − ψ′(c+))Xt∈Tc
= |Tc|Dφ(ckc+)
=Xt∈Tc
Dφ(f∗t kf +
t ).

(yt − c)

Summing over the level sets gives LT (f +) − LT (f∗) = Pt Dφ(f∗t kf +
t ). To introduce the ap-
propriate discretization points, we follow (de Rooij and van Erven, 2009). For any y ∈ [0, 1] and
ψ ∈ [0, π/2], we let ψ(y) = arcsin√y, so that y = sin2(ψ). The parameterization ψ has a
nice property, that the values of Dφ on uniformly located neighboring points are also close to uni-
2K (cid:9) ∪
form. We discretize the interval [0, π/2] into k + 1 points {ψ0, . . . , ψK} =(cid:8) π
4K(cid:9), which is almost uniform, with two additional points on the boundaries. Then, we
(cid:8) π

4K , π
deﬁne zk = y(ψk) = sin2(ψk), k = 0, . . . , K. Using Lemma 4 from de Rooij and van Erven
(2009):

2K , . . . , π(K−1)

2 − π

(2 − √2)π2
K 2 . From now on we proceed as in the proof of

K 2

,

Theorem 4, to get O(T 1/3 log2/3(T )) bound. Thus, we showed:

Dφ(f∗t kf +

t ) ≤
which bounds LT (f +) − LT (f∗) by (2 − √2) T
Theorem 8 Using K =&(cid:18) 2(2−√2)T

on the covering net:

log(T +1)(cid:19)1/3', the log-loss regret of discretized Exponential Wights
FK =(cid:8)f ∈ F : ∀t, ft ∈ {z0, z1, . . . , zK}(cid:9) ,
3(2 − √2)1/3

2K ) for k = 1, . . . , K − 1, has the following

4K ), and zk = sin2( πk

T 1/3(cid:0)log(T + 1)(cid:1)2/3 + log(T + 1).

22/3

where z0 = sin2( π
upper bound:

4K ), zK = cos2( π

RegT ≤

7.2. Absolute loss

in the context of isotonic discrimination/classiﬁcation (Dykstra et al., 1999; Kotłowski and Słowinski,

Absolute loss |byit−yit| is a popular loss function in modeling data with isotonic functions, especially

12

ONLINE ISOTONIC REGRESSION

2013). However, the online version of this problem turns out to be rather uninteresting for us, since it
can be solved in an essentially optimal way (up to O(√log T ) factor) by using the vanilla Exponen-
tiated Gradient algorithm. Applying the standard EG regret bound (Kivinen and Warmuth, 1997;
Cesa-Bianchi and Lugosi, 2006, also c.f. Section 3) results in a O(√T log T ) bound, whereas a
lower bound of Ω(√T ) comes from the setting of prediction with expert advice (Cesa-Bianchi and Lugosi,
2006): we constrain the adversary to only play with one of the two constant (isotonic) functions
ft ≡ 0 or ft ≡ 1, and apply the standard lower bound for the 2-experts case.
8. Conclusions and open problem

We introduced the online version of the isotonic regression problem, in which the learner must
sequentially predict the labels as well as the best isotonic function. We gave a computationally
efﬁcient version of the Exponential Weights algorithm which plays on a covering net for the set of
isotonic functions and proved that its regret is bounded by O(T 1/3 log2/3(T )). We also showed a
lower bound Ω(T 1/3) on the regret of any algorithm, essentially closing the gap.

There are some interesting directions for future research. First, we believe that the discretization
(covering net) is not needed in the algorithm, and a carefully devised continuous prior would work
as well. We were, however, unable to ﬁnd such a prior that would lead to the optimal regret bound,
while keeping the algorithm computationally efﬁcient. Secondly, we are interested to see whether
some regularized version of FTL (e.g., by means of relative entropy), or the forward algorithm
(Vovk-Azoury-Warmuth) (Azoury and Warmuth, 2001) could work for this problem. However, the
most interesting research direction is the extension to the partial order case.
In this setting, the
learner is given a set of points X = {x1, . . . , xT}, together with a partial order relation (cid:23) on X.
The goal of the learner is to sequentially predict the labels not much worse than the best function
which respects the isotonic constraints: xi (cid:23) xj → f (xi) ≥ f (xj). A typical application would
be nonparametric data modeling with multiple features, where domain knowledge may tell us that
increasing the value of any of the features is likely to increase the value of the label. The off-line
counterpart has been extensively studied in the statistics literature (Robertson et al., 1998), and the
optimal isotonic function shares many properties (e.g., averaging within level sets) with the linear
order case. The discretized Exponential Weights algorithm, which was presented in the paper, can
be extended to deal with partial orders, and the analysis closely follows the proof of Theorem 4,
except that the size of the covering net FK is no longer O(T K), but now depends on the structure
of (cid:23). We believe that |FK| is the right quantity to measure the complexity of the problem and
the algorithm will remain competitive in this more general setting. Unfortunately, the algorithm is
no longer efﬁciently implementable, as it suffers from the same problems that plague inference in
graphical models on general graphs. It thus remains an open problem to ﬁnd an efﬁcient algorithm
for the partial order case.

References

M. Ayer, H. D. Brunk, G. M. Ewing, W. T. Reid, and E. Silverman. An empirical distribution
function for sampling with incomplete information. Annals of Mathematical Statistics, 26(4):
641–647, 1955.

K. Azoury and M. Warmuth. Relative loss bounds for on-line density estimation with the exponen-

tial family of distributions. Journal of Machine Learning, 43(3):211–246, 2001.

13

KOTŁOWSKI KOOLEN MALEK

L. Birg´e and P. Massart. Rates of convergence for minimum contrast estimators. Probability Theory

and Related Fields, 97:113–150, 1993.

H. D. Brunk. Maximum likelihood estimates of monotone parameters. Annals of Mathematical

Statistics, 26(4):607–616, 1955.

N. Cesa-Bianchi and G. Lugosi. Worst-case bounds for the logarithmic loss of predictors. Machine

Learning, 43(3):247–264, 2001.

N. Cesa-Bianchi and G. Lugosi. Prediction, learning, and games. Cambridge University Press,

2006.

J. de Leeuw, K. Hornik, and P. Mair. Isotone optimization in R: Pool-adjacent-violators algorithm

(PAVA) and active set methods. Journal of Statistical Software, 32:1–24, 2009.

S. de Rooij and T. van Erven. Learning the switching rate by discretising Bernoulli sources online.

In AISTATS, pages 432–439, 2009.

D. DeTemple and W. Webb. Combinatorial reasoning: An introduction to the art of counting. John

Wiley & Sons, 2014.

R. Dykstra, J. Hewett, and T. Robertson. Nonparametric, isotonic discriminant procedures. Biomet-

rica, 86(2):429–438, 1999.

T. Fawcett and A. Niculescu-Mizil. PAV and the ROC convex hull. Machine Learning, 68(1):

97–106, 2007.

P. Gaillard and S. Gerchinovitz. A chaining algorithm for online nonparametric regression.

In

Conference on Learning Theory (COLT), pages 764–796, 2015.

E. Hazan, A. Agarwal, and S. Kale. Logarithmic regret algorithms for online convex optimization.

Machine Learning, 69(2–3):169–192, 2007.

S. M. Kakade, V. Kanade, O. Shamir, and A. Kalai. Efﬁcient learning of generalized linear and

single index models with isotonic regression. In NIPS, pages 927–935. 2011.

A. T. Kalai and R. Sastry. The isotron algorithm: High-dimensional isotonic regression. In COLT,

2009.

J. Kivinen and M. K. Warmuth. Exponentiated gradient versus gradient descent for linear predictors.

Information and Computation, 132(1):1–63, 1997.

W. Kotłowski and R. Słowi´nski. Rule learning with monotonicity constraints.

In ICML, pages

537–544, 2009.

W. Kotłowski and R. Słowinski. On nonparametric ordinal classiﬁcation with monotonicity con-

straints. IEEE Transactions on Knowledge and Data Engineering, 25(11):2576–2589, 2013.

J. B. Kruskal. Multidimensional scaling by optimizing goodness of ﬁt to a nonmetric hypothesis.

Psychometrika, 29(1):1–27, 1964.

14

ONLINE ISOTONIC REGRESSION

R. Kyng, A. Rao, and S. Sachdeva. Fast, provable algorithms for isotonic regression in all ℓp-norms.

In NIPS, 2015.

L. Le Cam. Asymptotic Methods in Statistical Decision Theory. Springer-Verlag, 1986.

R. Luss, S. Rosset, and M. Shahar. Efﬁcient regularized isotonic regression with application to

gene–gene interaction search. Annals of Applied Statistics, 6(1):253–283, 2012.

A. K. Menon, X. Jiang, S. Vembu, C. Elkan, and L. Ohno-Machado. Predicting accurate probabili-

ties with a ranking loss. In ICML, 2012.

T. Moon, A. Smola, Y. Chang, and Z. Zheng. Intervalrank: Isotonic regression with listwise and

pairwise constraint. In WSDM, pages 151–160. ACM, 2010.

H. Narasimhan and S. Agarwal. On the relationship between binary classiﬁcation, bipartite ranking,

and binary class probability estimation. In NIPS, pages 2913–2921. 2013.

A. Niculescu-Mizil and R. Caruana. Predicting good probabilities with supervised learning.

In

ICML, pages 625–632, 2005.

G. Obozinski, C. E. Grant, G. R. G. Lanckriet, M. I. Jordan, and W. W. Noble. Consistent proba-

bilistic outputs for protein function prediction. Genome Biology, 2008 2008.

A. Rakhlin and K. Sridharan. Online nonparametric regression. In COLT, pages 1232–1264, 2014.

T. Robertson, F. T. Wright, and R. L. Dykstra. Order Restricted Statistical Inference. John Wiley &

Sons, 1998.

A. Saumard and J. A. Wellner. Log-concavity and strong log-concavity: A review. Statistics Surveys,

8:45–114, 2014.

S. Shalev-Shwartz. Online learning and online convex optimization. In Foundations and Trends in

Machine Learning, volume 4, pages 107–194. 2001.

M. Stylianou and N. Flournoy. Dose ﬁnding using the biased coin up-and-down design and isotonic

regression. Biometrics, 58(1):171–177, 2002.

A. B. Tsybakov. Introduction to Nonparametric Estimation. Springer-Verlag, 2009.

S. Van de Geer. Estimating a regression function. Annals of Statistics, 18:907–924, 1990.

V. Vovk, I. Petej, and V. Fedorova. Large-scale probabilistic predictors with and without guarantees

of validity. In NIPS, pages 892–900. 2015.

B. Zadrozny and C. Elkan. Transforming classiﬁer scores into accurate multiclass probability esti-

mates. In KDD, pages 694–699, 2002.

C.-H. Zhang. Risk bounds in isotonic regression. The Annals of Statistics, 30(2):528–555, 2002.

M. Zinkevich. Online convex programming and generalized inﬁnitesimal gradient ascent. In ICML,

pages 928–936, 2003.

15

KOTŁOWSKI KOOLEN MALEK

Appendix A. The Exponentiated Gradient (EG) bound

We will ﬁrst cast the online isotonic regression problem to the equivalent problem of minimizing
square loss over (T + 1)-dimensional probability simplex ∆T +1.

Given f ∈ F, deﬁne the (T + 1)-dimensional vector of increments of f by p(f ) = (f1 −
f0, f2 − f1, . . . , fT +1 − fT ), where we used two dummy variables f0 = 0 and fT +1 = 1. Note that
p(f ) ∈ ∆T +1, and there is one-to-one mapping between elements from F and the corresponding
elements from ∆T +1, with the inverse mapping f (p) given by ft(p) =Pt
q=1 pt. The loss in the
simplex parameterization is given by:

ℓt(pt) =(cid:16)yit −Xj≤it

pt,j(cid:17)2

=(cid:16)yit − p⊺

t xit(cid:17)2

,

where xit is the vector with the ﬁrst it coordinates equal to 1. The Exponentiated Gradient (EG)
algorithm (Kivinen and Warmuth, 1997) is deﬁned through the update:

pt,j =

pt−1,je−η(∇ℓt−1(pt−1))j
PT +1
k=1 pt−1,ke−η(∇ℓt−1(pt−1))k

,

with p0 being some initial distribution. The prediction of the algorithm is thenbyit = Pj≤it pt,j.

We now use the standard upper bound for the regret of EG:
Theorem 9 (Theorem 5.10 by Kivinen and Warmuth (1997)) Let {(xt, yt)}T
t=1 be a sequence of
outcomes such that for all t, maxi xt,i − mini xt,i ≤ R. For any p ∈ ∆T +1 with LT (p) ≤ K and
D(pkp0) ≤ D for some p0, the EG algorithm with initial distribution p0 and learning rate η tuned
as:

2√D

η =

R(√2K + R√D)

,

have the following bound on its cumulative loss:

bLT ≤ LT (p) + √2KD +

R2D(pkp0)

2

.

We apply this theorem to our problem with the sequence permuted by (i1, . . . , it) and R = 1.
We choose p0 to be a uniform distribution on ∆T +1, which means D(pkp0) ≤ log(T +1) = D. We
also use a crude bound on the loss of best comparator p∗ = argminp LT (p), LT (p∗) ≤ 1
4 T = K
(this is because the loss of the best comparator is lower than the loss of the constant function f
equal to the arithmetic mean of the data). This suggests tuning the learning rate to:

η =

2plog(T + 1)
q T
2 +plog(T + 1)

,

to get the following regret bound:

RegT ≤r T log(T + 1)

2

+

log(T + 1)

2

.

16

ONLINE ISOTONIC REGRESSION

Appendix B. Proof of Theorem 3 (the bound for Exponential Weights algorithm)

Let the adversary reveal the labels in the isotonic order (it = t for all t), and they are all equal to 0.

At trial t, the prediction of the algorithmbyt = f ⊺

ftpt(f ) df1 . . . dfT ,

where

t xt = ft,t is given by:

byt =ZF

pt(f ) =

ZF
|

,

2 Pq<t f 2

q

e− 1
2 Pq<t f 2

e− 1

q df1 . . . dfT

=Z

{z

}

pt(f ) df1 . . . dft−1 dft+1 . . . dfT

2 Pq<t f 2

e− 1

q df1 . . . dft−1(cid:19)(cid:18)Zz≤ft+1≤...≤fT ≤1

dft+1 . . . dfT(cid:19)

2 Pn

t=1 f 2

e− 1

t df1 . . . dfn.

2 x2

dx denote the antiderivative of the Gaussian.

We calculate the marginal distribution pt(ft = z):

pt(ft = z) =Z0≤f1≤ft−1≤z≤ft+1...≤fT ≤1

=

1

Z(cid:18)Z0≤f1≤...≤ft−1≤z

,

=

1
Z

where:

G(z, t − 1)

(1 − z)T−t
(T − t)!
G(z, n) =Z0≤f1≤...≤fn≤z
We now calculate G(z, n). Let F (x) = R e− 1

Recursively applying the relation:

Z z

ft−1

e− 1

2 f 2

t

1
k!

(F (z) − F (ft))k =

(F (z) − F (ft−1))k+1

(k + 1)!

.

G(z, n) =

(F (z) − F (0))n

n!

,

we get:

so that:

pt(ft = z) =

(1 − z)T−t(F (z) − F (0))t−1,
Denote pt(ft = z) concisely as φ(z). Then, we have:

1
Z′

where Z′ = Z(t − 1)!(T − t)! .

Let α = t−1

T−1 and deﬁne:

zφ(z) dz.

byt =Z 1

0

g(z) = (1 − α) log(1 − z) + α(F (z) − F (0)).

Note that φ(z) = 1

Z ′ e(T−1)g(z). We have:

g′(z) = −

1 − α
1 − z

+

α

e− 1

2 z2

,

F (z) − F (0)
17

KOTŁOWSKI KOOLEN MALEK

and:

α

α

1 − α
(1 − z)2 −

−

g′′(z) = −

(F (z) − F (0))2 e−z2
Thus, g is (strictly) concave, which implies that φ is log-concave. Furthermore, due to strict con-
cavity of F (z) for z > 0, we have: F (0) < F (z) − ze− 1
1 − α
1 − z

2 z2 for z > 0, which implies:

so that g′(α) < 0. On the other hand, also from the concavity of F (z), F (z) ≤ F (0) + z, which
together with e− 1

F (z) − F (0)

g′(z) < −

< 0.

α
z

2 z2

2 z2

ze− 1

+

,

≥ 1 − 1

2 z2 implies:

g′(z) ≥ −

1 − α
1 − z

+ 2(cid:18)1 −

1
8

.

+

z

α(cid:16)1 − 1
2 z2(cid:17)
α2(cid:19) =

1
1 − α

2

+

1
4

α2 ≥ 1 > 0.

2 and α, which means that the (unique) maximizer z∗ =

This means that:

g′(cid:18) α

2(cid:19) ≥ −
argmax g(z) is in the range(cid:0) α

1 − α
1 − α
2 , α(cid:1).

Thus g′(z) switches the sign between α

2

We now use (Saumard and Wellner, 2014, Proposition 5.2) which states that for log-concave

densities, the density at the mean is not much smaller than the density at the mode:

which means that after taking logarithms, dividing by T − 1 and using the deﬁnition of z∗,

1
√3e

sup

1

z

z

φ(z),

φ(z) ≤ φ(byt) ≤ sup
(1 + log √3) ≤ g(byt) ≤ g(z∗).
2(cid:19) ≤ g(z∗) + g′(cid:18) α
2(cid:19)(cid:18)byt −
T−1 (1 + log √3)

α

1

1

α
2 −

≥

g′(cid:0) α
2(cid:1)

2(cid:19)(cid:18)byt −

α

2(cid:19) ,

(1 + log √3).

T − 1

T − 1

From concavity of g,

g(z∗) −
g(byt) ≤ g(cid:18) α
2(cid:19) + g′(cid:18) α
2(cid:1) ≥ 1, implies:
which, together with g′(cid:0) α
g(byt) − g(z∗)
g′(cid:0) α
2(cid:1)
(1 + log √3) ≥

byt ≥
Hence,byt > α

α
2
4 when:
α
2 −

α
2 −

≥

+

1

T − 1

α
4

=⇒

T ≥ 1 +

4(1 + log √3)

α

.

2 and when T ≥ 14. Therefore, when T ≥ 14, for all α ≥ 1

2, we
2 is implied by t ≥ ⌊T /2⌋+1,

64. Since α ≥ 1

Note, that this holds when α ≥ 1
we conclude that when T ≥ 14,

havebyt ≥ α

4 > 1

8, which means ℓt(f t) = (byt−0)2 > 1
LT (f ) = bLT ≥

RegT =bLT − min

f∈F

TXt=⌊ T

2 ⌋+1

1
64 ≥

T
128

.

18

ONLINE ISOTONIC REGRESSION

Appendix C. Full proof of Theorem 5

We proceed by constructing the difﬁcult sequence explicitly. First, split the T points (1, . . . , T )
into K consecutive segments (1, . . . , m), (m + 1, . . . , 2m), . . . , (m(K − 1) + 1, . . . , T ), where
in each segment there are m = T
K consecutive points (assume for now that T is divisible by K).
Let t ∈ k mean that t is in the k-th segment, k = 1, . . . , K, i.e. t ∈ k whenever k = (cid:6) t
m(cid:7).
Now, suppose the adversary picks a K-vector p = (p1, . . . , pK) ∈ [0, 1]K such that p1 ≤ p2 ≤
. . . ≤ pK and generates the labels in the isotonic order (any order would work as well) such that
yt ∼ Bernoulli(pk) when t ∈ k. Let f p ∈ F denote an isotonic function such that ft = pk when
t ∈ k. We lower bound the expected regret:

=

f∈F

LT (f )(cid:21)
Ep[RegT ] = Ep(cid:20)bLT − inf
≥ EphbLT − LT (f p)i
EpkXt∈k
(byt − yt)2 − (pk − yt)2
KXk=1
EpkXt∈k
(byt − pk)(byt + pk − 2yt)2
KXk=1
EpkXt∈k
(byt − pk)2 ,
KXk=1
P =(p = (p1, . . . , pK ) : pk ∈(cid:26) 1

k − 1
2K

1
4

+

,

+

4

=

=

k

2K(cid:27)) .

where the last equality is from Epk[yt] = pk. Now we assume the adversary picks p from the
following set:

There are 2K vectors in P, all satisfying p1 ≤ p2 ≤ . . . ≤ pK, and 1
instance, when K = 2, P =n( 1

2 ), ( 1
2 , 1
2K and pk,1 = 1

Fix k and denote pk,0 = 1

4 , 1
4 + k−1

4 , 3

4 ), ( 1

4 )o.
2 ), ( 1
2 , 3
4 + k
2K , i.e. pk ∈ {pk,0, pk,1}. Deﬁne:
pk,i,i=0,1(cid:8)|yk − pk,i|(cid:9) ,

4 ≤ pk ≤ 3

4 for all k. For

where yk = 1

bpk = argmin
mPt∈kbyt. We now show that:
Xt∈k
(byt − pk)2 ≥

m
4

(bpk − pk)2.

(3)

Without loss of generality, assume pk = pk,0. Then, if bpk = pk,0, the inequality clearly holds
because the right-hand side is 0. On the other hand, ifbpk = pk,1, then from the deﬁnition ofbpk we

19

KOTŁOWSKI KOOLEN MALEK

have |yk − pk,1| ≤ |yk − pk,0|, which means yk ≥ 1

2 (pk,0 + pk,1). This implies:

Xt∈k
(byt − pk,0)2 =Xt∈k
=(cid:18)Xt∈k
=(cid:18)Xt∈k

(byt − yk + yk − pk,0)2
(byt − yk)2 + 2(byt − yk)(yk − pk,0)(cid:19) + m(yk − pk,0)2
(byt − yk)2(cid:19) + m(yk − pk,0)2

≥ m(yk − pk,0)2 ≥

(pk,1 − pk,0)2.

m
4

does not depend on pk. Hence, the worst-case regret can be lower bounded by:

Thus, (3) holds. Note that bpk depends on {byt, t ∈ k} (which in turn depend on the labels), but it

max
y1,...,yT

RegT ≥ max
p∈P

m
4

KXk=1

Epk[(bpk − pk)2] =

KXk=1

max

pk∈{pk,0,pk,1}

m

16K 2 Pr(bpk 6= pk).

Let us ﬁx k and bound the k-the term in the sum on the right-hand side. At this moment, we reduced
the problem to ﬁnding the minimax lower bound on the probability of error in binary hypothesis
testing, essentially following what is known as Le Cam’s method (Le Cam, 1986) Using (Tsybakov,
2009, Theorem 2.2.i):

max

pk∈{pk,0,pk,1}

Pr(bpk 6= pk) ≥

1 − TV(pk,0, pk,1)

2

,

where TV(pk,0, pk,1) is the total variation distance between pk,0 and pk,1 treated as distributions
over {yt, t ∈ k}. From Pinsker’s inequality:
1
2

TV2(pk,0, pk,1) ≤

D(pk,0kpk,1) =

m
4

,

where D(·k·) is the Kullback-Leibler divergence, which we bound by Taylor approximation with
pk,1. Since pk,0, pk,1 ∈ [ 1

respect to pk,1 around pk,0 up to the second order, and ep is some convex combination of pk,0 and

4 , 3
4 ],

4 , 3

1

ep(1−ep) is maximized forep ∈ { 1

16m

TV2(pk,0, pk,1) ≤

(pk,1 − pk,0)2 =

4

m
K 2 .

(pk,1 − pk,0)2
ep(1 −ep)
4}, so that:

Plugging this into our bound gives:

max

pk∈{pk,0,pk,1}

Summing over k = 1, . . . , K and using the fact that m = T

Pr(bpk 6= pk) ≥
32K 2 1 −

T

√m
K

1 −
2

,

K gives:

√T

K 3/2! .

20

max
y1,...,yT

RegT ≥

ONLINE ISOTONIC REGRESSION

Choosing K = cT 1/3 for some c > 1 gives:

max
y1,...,yT

RegT ≥

c3/2 − 1
32c5/2

T 1/3.

Choosing any c > 1, c = O(1), such that K divides T ﬁnishes the proof.

Appendix D. Proofs for Section 6 (noise-free case)

D.1. Proof of Theorem 6 (arbitrary order of outcomes)
We ﬁrst give a sequence of outcomes such that when T = 2k − 1 for some positive integer k, any
algorithm will suffer exactly 1
4 log2(T + 1) loss. The adversary picks a point in the middle of the
range, i1 = 2k−1. After the algorithm predicts, the adversary chooses yi1 to be 0 or 1, depending
which of these two incurs more loss to the algorithm. Hence, no matter what the algorithm predicts,
the loss is at least 1
4. If yi1 = 0, then 2k−1 − 1 points on the left-hand side of yi1 are labeled to 0 in
the next trials (which is required due to noise-free regime), and the algorithm will possibly suffer no
loss on these points. Then, the adversary repeats the above procedure of choosing the middle point
on the remaining 2k−1 − 1 points on the right-hand side of yi1. Analogously, when yi1 = 1, the
points on the right-hand side are all labeled to 1, and the adversary recursively play on remaining
the left-hand side points. This procedure can be carried out k times, until no more points remains.
Hence, the total loss incurred by the algorithm is at least 1

4 log2(n + 1).
Next, we determine the upper bound on the value of the minimax regret:

4 k = 1

V = min
byi1

max
yi1

. . . min
yiT

max
yiT

TXt=1

1
4

log2(T + 1),

(yit −byit)2 ≤

where the labels are constrained to be isotonic, y1 ≤ . . . ≤ yT . We will get the predictions of the
minimax algorithm as a by-product of the calculations. This implies that the minimax algorithm
suffers regret at most 1

4 log2(T + 1), and the bound on V is tight whenever T = 2k − 1.

In the ﬁrst trial, the adversary reveals outcome i1, which splits the set of unknown labels
into two disjoint sets (y1, . . . , yit−1) and (yit+1, . . . , yT ). The minimax algorithm knows that
0 ≤ y1, . . . , yit−1 ≤ yit and yit ≤ yit+1, . . . , yT ≤ 1 (due to noise-free case). Then, in the fu-
ture trials, each of these sets of unknown consecutive labels will be recursively split into smaller
sets. At any moment of time, for any set of unknown labels (yi, . . . , yj) with j ≥ i, we know
that yi−1 ≤ yi, . . . , yj ≤ yj+1, and yi−1 and yj+1 has already been revealed (we use y0 = 0 and
yT +1 = 1). Hence, the minimax algorithm will play a separate minimax game for each set of un-
known labels (yi, . . . , yj), bounded in the range [yi−1, yj+1]. We use this observation as a basis for
the recursion. Let V (u, v, n) denote the minimax value of the game for a set of n not yet revealed
consecutive labels lower-bounded by u, and upper-bounded by v. We get the recursion:

V (u, v, n + 1) = max

k∈{0,...,n}

V (u, v, n + 1, k),

(4)

where:

V (u, v, n + 1, k) = min
by∈[u,v]

max

y∈[u,v]n(y −by)2 + V (u, y, k) + V (y, v, n − k)o ,

21

KOTŁOWSKI KOOLEN MALEK

which follows from the fact that ﬁrst the adversary reveals (k + 1)-th point, then the algorithm

predicts withby for that point, and ﬁnally the outcome y is revealed, while the set is split into two

sets of smaller size. The minimax regret can be read out from V (0, 1, T ). To start the recursion, we
deﬁne V (u, v, 0) = 0.

We now prove by induction on n that:

V (u, v, n) = βn(v − u)2,

(5)

where βn is some coefﬁcient independent of u and v. Assume n+1 unknown labels, lower-bounded
by u, and upper-bounded by v. We ﬁx k ∈ {0, . . . , n} and calculate the optimal prediction of the
algorithm for V (u, v, n + 1, k):

by = argmin

by∈[u,v]

max

y∈[u,v]n(y −by)2 + βk(y − u)2 + βn−k(v − y)2o ,

where we used the inductive assumption. The function inside max is convex in y, hence the solution
w.r.t. y is y ∈ {u, v}. First note that if βk − βn−k > 1, the function inside max is increasing in y for

any choice ofby ∈ [u, v], hence the optimal choice for the adversary is y = v, and the optimal choice
for the algorithm isby = v. Similarly, if βk − βn−k < −1, the function inside max is decreasing in
y, which results in the optimal choice y = u andby = u. When −1 ≤ βk − βn−k ≤ 1, it is easy

to check that the optimal prediction is obtained by setting the function inside max equal for both
choices of y ∈ {u, v}. This gives:
by =

Thus, depending on the value of βk − βn−k, V (u, v, n + 1, k) is given by:

(βk − βn−k) ∈ [u, v].

u − v

u + v

2

+

2

where

V (u, v, n + 1, k) = (u − v)2βn,k,

βn,k =

From (4), we have:

βk
1
4 (βk − βn−k)2 + 1
βn−k

2 (βk + βn−k) + 1

4

if βk − βn−k > 1,
if − 1 ≤ βk − βn−k ≤ 1,
if βk − βn−k < −1.

(6)

βn+1 = max

k∈{0,...,n}

βn,k,

which proves our inductive hypothesis (5).
What is left to show is that βn ≤ 1

4 log2(n + 1). We will prove it by induction on n. For
n = 0, β0 = 0 and thus the bound trivially holds. We now show that βn,k, as deﬁned in (6), is
nondecreasing in βk and βn−k. We ﬁx βn−k, and calculate the derivative with respect to βk:

dβn,k
dβk

=

1
1
2 (βk − βn−k + 1)
0

if βk − βn−k > 1,
if − 1 ≤ βk − βn−k ≤ 1,
if βk − βn−k < −1,

22

ONLINE ISOTONIC REGRESSION

which is nonnegative. Hence, βn,k is nondecreasing with βk for any ﬁxed βn−k. An analogous
arguments shows that βn,k is nondecreasing with βn−k for any ﬁxed βk. Hence, we can replace βk
and βn−k by their upper bounds from the inductive argument, and then βn,k (as well as βn+1) will
not decrease. Thus, to show that 1
4 log2((n + 1) + 1) is the upper bound on βn+1, it sufﬁces to
show that for any n, k, βn,k ≤ 1
4 log2(k + 1) and βn−k =
4 log2(n − k + 1) in (6).
4 log2(n + 2),
because k ≤ n. Case βk − βn−k < −1 is covered in an analogous way. We are left with the case
−1 ≤ βk − βn−k ≤ 1. It sufﬁces to show that:

We proceed by cases in (6). When βk − βn−k > 1, βn,k = βk = 1

4 log2(n + 2) after substituting βk = 1

4 log2(k + 1) ≤ 1

1

(βk − βn−k)2 +
|

1
2

{z

(βk + βn−k) +

≤

log2(24βk + 24βn−k )

,

(7)

=g(βk,βn−k)

=f (βk,βn−k)

1
4

}

1
4

|

{z

}

4 log2(n + 2) when βk = 1

because the right-hand side is equal to 1
4 log2(1 + k) and βn−k =
1
4 log2(1 + n− k). Assume w.l.o.g. that βk ≤ βn−k (because both f (·,·) and g(·,·) are symmetric in
their arguments). For any δ, g(x+δ, y+δ) = g(x, y)+δ, and similarly f (x+δ, y+δ) = f (x, y)+δ.
Thus, proving f (x, y) ≥ g(x, y) is equivalent to proving f (0, y − x) ≥ g(0, y − x). Given the
condition −1 ≤ βn−k − βk ≤ 1, we thus need to show that: f (0, y) ≥ g(0, y) for any 0 ≤ y ≤ 1,
which translates to:

log2(cid:16)1 + 24y(cid:17) ≥ (1 + y)2,

for 0 ≤ y ≤ 1.

This inequality can be shown by splitting the range [0, 1] into [0, 1
4 , 1], lower-bounding
the left-hand side by its Taylor expansion up to the second order around points 0, 1
4, respec-
tively (with the second derivative replaced by its lower bound in a given range), and showing that
the corresponding quadratic inequality always holds within its range. We omit the details here.
Unfortunately, we were unable to ﬁnd a more elegant proof of this inequality.

4 ] and [ 3

2, and 3

4 ], [ 1

4 , 3

D.2. Proof of Theorem 7 (isotonic order of outcomes)

We determine the value of the minimax regret:

V = min
by1

max
y1∈[0,1]

min
by1

max
y2∈[y1,1]

. . . min
byT

max

yT ∈[yT −1,1]

TXt=1
(yt −byt)2,

getting the predictions of the minimax algorithm as a by-product of the calculations. Note that any
algorithm will suffer regret at least V for some sequences of labels, while the minimax algorithm
will suffer regret at most V for any sequence of labels. Let:

VT−t(yt) = min
byt+1

max

yt+1∈[yt,1]

. . . min
byT

max

yT ∈[yT −1,1]

TXq=t+1

(yq −byq)2

be the value-to-go function, which is the worst-case loss suffered by the minimax algorithm in T − t
trials t + 1, . . . , T , given the last revealed label was yt. The minimax regret V can be read out from
VT (0). We use the following recursion:

Vn(c) = min
by

max

y∈[c,1]n(y −by)2 + Vn−1(y)o ,

23

KOTŁOWSKI KOOLEN MALEK

where we used V0(y) = 0. We start with calculating V1(c) (which corresponds to the last trial
t = T ). The minimax prediction is given by:

argmin

max
y∈[c,1]

(by − y)2 = argmin

max{(by − c)2, (by − 1)2} =

by

by
and the value-to-go function is V1(c) = 1
4 (1 − c)2. We now prove by induction that Vn(c) =
αn(1 − c)2 for some αn > 0 (which clearly holds for n = 1 with α1 = 1
4, as shown above). By the
induction argument,

c + 1

,

2

max

Vn(c) = min
by

The last equality is due to the fact that the function inside the min max is convex in y, therefore the

y∈[c,1]n(by − y)2 + αn−1(1 − y)2o = min

y∈{c,1}n(by − y)2 + αn−1(1 − y)2o .
optimal y is on the boundary of the feasible range [c, 1]. It is easy to check that the optimalby makes

the expression inside min max equal for both choices of y, so that:

max

by

The expression inside max is a convex function of yt, therefore the optimal yt is on the boundary

of feasible range {yt−1, b}. The algorithm predicts withbyt, such that the expression inside max has

the same value for yt = yt−1 and yt = b. This gives:
− αT

b − yt−1

b + yt−1

t+1

2

2

,

and:

This ﬁnishes the inductive proof for Vn(c). The value of the minimax regret is given by VT (0) = αT .
Now, given that α1 = 1

4 < 1, we have inductively for all n:

+ αn−1

c − 1
2

,

c + 1

2

byt =

byt =

2

(cid:19)2
Vn(c) = (by − 1)2 =(cid:18) αn−1 + 1
(1 − c)2.
{z
}
≤(cid:18) 1 + 1
2 (cid:19)2

αn =(cid:18) αn−1 + 1

|
(cid:19)2

= 1,

=αn

2

24

