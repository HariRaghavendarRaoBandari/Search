Homemade assembly and parameterization of a High Performance Cluster
using PelicanHPC with Flops testing and controlled temperature thanks to

MCUs Arduino project

T. Maquart1, G. Maquart2

1Laboratory of Mechanical of Contacts and Structures (LaMCoS), CNRS, UMR 5259, INSA-Lyon,

2Nuclear Physics Institute of Lyon (IPNL), Claude Bernard University Lyon 1 (UCBL),

University of Lyon, (tristan.maquart@insa-lyon.fr)

(guillaume.maquart@in2p3.fr)

March 22, 2016

Abstract

This article shows a lower cost realization of a compute cluster using Debian distribution such as PelicanHPC.
We will explain parameterization and network conﬁguration for master and compute slave nodes. Performance
testing will take place using f lops.f ﬁle given by MPI. The results will be compared between diﬀerents clusters.
We will explain quickly how the temperature is controlled by a microcontroller unit.

Keywords PelicanHPC, High Performance Cluster, Parallel programming, Message Passing Interface, Home-
made, Arduino project.

Introduction

Parallel programming is a very new tool using multiple computing cores for a given program, increasing the
speedup in the parallelized instructions of the program. In this article we are particularly interested to test the
performance of a cluster using the f lops.f ﬁle given by MPI. The cluster is based on the Debian distribution named
PelicanHPC [1] (For High Performance Clusters) with Xfce desktop environment which is lightweight (version used
is PelicanHPC 3.1 [3]). Pelican is a live CD distribution that provides the ability to boot a full Linux operating
system and access many software from the media (CD, DVD or USB stick) without installation on the hard disk,
and without altering its content. Note that this system has great potential thanks to its support for parallel
computing based on MPI using Fortran (77, 90, 95), C, C ++, but also interpreted languages such as GNU Octave
or semi-interpreted as Phyton.

6
1
0
2

 
r
a

 

M
0
2

 
 
]

C
D
.
s
c
1
[
 
 
1
v
1
4
2
6
0

.

3
0
6
1
:
v
i
X
r
a

2 Setup

The PXE boot (acronym for Pre-boot eXecution Environment) allows a workstation to boot from the network by
recovering an operating system image that is on a server [2]. This system image could be a gross operating system
or an operating system with custom personal software components. This cluster in picture 1 has a master node
that manages the startup by PXE boot other the network of two slave nodes. As was said earlier, the PelicanHPC
system is only live version, that is to say all data and conﬁguration made on the live partition is deleted when the
cluster is shutdown or restarted. Note that all nodes must be in the same subnet (see picture 2) to allow starting
and they must have PXE option enabled in their BIOS. It may be noted that it is strongly recommended to leave
PelicanHPC manage his own DHCP server to avoid conﬂicts. This calculation system have a hybrid memory since
each of the cores of the APU AMD A8 6600K share the same memory (RAM), but the memories are diﬀerent from
one processor to another. Now look at the expected maximum theoretical performance:

CP U : (4, 2 ∗ 4 ∗ 4) ∗ 3 = 201, 6 Gf lops

(1)

1

This theoretical performance is upper than the reality. In this calculation we have taken 4 f lops per cycle. It is
really important to control the cooling of each processors to avoid BIOS maximum temperature during important
calculations. Each processors are well-equipped by a LM35DZ temperature sensor in their heatsinks, thereafter a
resolution of an unstationary diﬀerential equation of thermal conduction give the real temperature of cores. These
sensors are connected to a microcontroller chip ATMEGA328P-PU with Arduino UNO bootloader for an easily
acces to the open source project. An Arduino sketch is uploaded on this chip to control on/oﬀ of additional fans.
A SIM800L module is connected to the MCU for sending processors temperatures by Short Message Service to
directors numbers. A serial communication beetween this additional module and ATMEGA328P-PU using GSM
AT commands allows the microcontroller chip to control SMS data. This chip run 350 lines of code in loop all
twenty seconds. An LCD light screen shows informations about sensors. We can have more information of Arduino
project by surﬁng on their website [4].

Figure 1: Picture of the cluster

Figure 2: Network conﬁguration

3 Results and performance

We will analyse quickly the results we can show on picture 3. There is three diﬀerent cluster including our
homemade PelicanHPC high performance cluster. Two others are cluster from Nuclear Physics Institute of Lyon
(IPNL) respectively with 16 and 24 cores. In our small network architecture with three motherboards, we can notice
that performance is almost a linear function of the number of cores. Moreover, thanks to this results, homemade

2

Homemade Cluster

Lyosrv019

Lyosrv024

Clusters quick description

3*AMD A8 6600K
4 cores/CPU
4.2 Ghz/core
8Go DDR3-1866/CPU

4*Intel(R) Xeon(R) CPU E5645
6 cores/CPU
2.40 Ghz/core
4Go (Low Frequency)/CPU

4*Intel(R) Xeon(R) CPU E5-2640

6 cores/CPU
2.50 Ghz/core
16,5Go (Low Frequency)/CPU

Table 1: Clusters quick description

12 cores assembly appears to be more eﬃcient than other clusters of IPNL. We will highlight that this test is
running on f lops.f ﬁle given by MPI and it’s not the real performance of your personnal parallelized code because
each MPI codes uses diﬀerents physically part of clusters. However, this gives an overall idea of performance and
quality of communication between computing cores. Table 1 lists physically description of clusters.

Figure 3: Mﬂops of diﬀerents cluster versus Homemade PelicanHPC

4 Conclusion

It is important to know the performance of a cluster in order to adapt a parallel code. Based on certain formulations
using diﬀerents parts of the cluster, the resolution will be slowest in some cases, that’s why it’s important to adjust
code to the considered cluster conﬁgurations. To talk about price of this installation, we can have 4*3 cores clocked
at 4,2 Ghz for less than 650 euros (including motherboards and network installations). The price and simplicity
of implementation of this Debian distribution in a compute cluster is a real beneﬁt not to ignore. A single cluster
like this may prove more eﬃcient than industrial cluster nodes with 16 cores or more.

References

[1] http://pelicanhpc.awict.net/, Website, PelicanHPC oﬃcial Website.

[2] http://pareto.uab.es/mcreel/PelicanHPC/, Website, PelicanHPC tutorial

Barcelona.

from Autonomous University of

[3] http://distrowatch.com/table.php?distribution=pelicanhpc, Website, PelicanHPC Distribution download.

[4] https://www.arduino.cc/, Website, Arduino oﬃcial Website.

3

