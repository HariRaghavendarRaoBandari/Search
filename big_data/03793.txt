6
1
0
2

 
r
a

 

M
1
1

 
 
]
L
C
.
s
c
[
 
 

1
v
3
9
7
3
0

.

3
0
6
1
:
v
i
X
r
a

Training with Exploration Improves a Greedy Stack-LSTM Parser

Miguel Ballesteros♦♠ Yoav Goldberg♣ Chris Dyer♠ Noah A. Smith♥

♦NLP Group, Pompeu Fabra University, Barcelona, Spain

♠School of Computer Science, Carnegie Mellon University, Pittsburgh, PA, USA
♣Computer Science Department, Bar-Ilan University, Ramat Gan, 5290002 Israel
♥Computer Science & Engineering, University of Washington, Seattle, WA, USA

miguel.ballesteros@upf.edu, yoav.goldberg@gmail.com

Abstract

We adapt the greedy Stack-LSTM depen-
dency parser of Dyer et al. (2015) to
support a training-with-exploration proce-
dure using dynamic oracles (Goldberg and
Nivre, 2013) instead of cross-entropy min-
imization. This form of training, which
accounts for model predictions at training
time rather than assuming an error-free ac-
tion history, improves parsing accuracies
for both English and Chinese, obtaining
very strong results for both languages. We
discuss some modiﬁcations needed in or-
der to get training with exploration to work
well for a probabilistic neural-network de-
pendency parser.

1 Introduction

Natural language parsing can be formulated as a
series of decisions that read words in sequence
and incrementally combine them to form syn-
tactic structures; this formalization is known as
transition-based parsing, and is often coupled with
a greedy inference procedure (Yamada and Mat-
sumoto, 2003; Nivre, 2003; Nivre, 2004; Nivre,
2008). The literature on transition-based parsing
is vast, but all works share in common a classiﬁ-
cation component that takes into account features
of the current parser state1 and predicts the next
action to take conditioned on the state. The state
is of unbounded size.

Dyer et al (2015) presented a parser in which the
parser’s unbounded state is embedded in a ﬁxed-
dimensional continuous space using recurrent neu-
ral networks. Coupled with a recursive tree com-
position function, the feature representation is able
to capture information from the entirety of the

1The term “state” refers to the collection of previous de-
cisions (sometimes called the history), resulting partial struc-
tures, which are stored in a stack data structure, and the words
remaining to be processed.

state, without resorting to locality assumptions
that were common in most other transition-based
parsers. The use of a novel stack-LSTM data
structure allows the parser to maintain a constant
time per-state update, and retain an overall linear
parsing time.

The Dyer et al parser was trained to mini-
mize cross-entropy relative to an empirical dis-
tribution of gold-standard sequences (obtained by
transforming labeled dependency treebanks using
a manually deﬁned procedure), and at test time,
the parser makes greedy decisions according to the
learned model. Although this setup obtains very
good performance, the training and testing con-
ditions are mismatched in the following way: at
training time the historical context of an action is
always from the gold standard, but at test time, it
will be a model prediction.

In this work, we adapt the training criterion to
consider predictions made by the model at test
time. In recent work, Goldberg and Nivre (2012;
2013) introduced the concept of a dynamic ora-
cle, which dynamically choses an optimal (rela-
tive to the total attachment accuracy) action given
an imperfect history. By interpolating between al-
gorithm states sampled from the model and those
sampled from the training data, more robust pre-
dictions at test time can be made. We show that
the technique can be used to improve also the al-
ready very strong parser of Dyer et al.

Notation We follow the convention that vectors
are written with lowercase, boldface letters (e.g., v
or vw); matrices are written with uppercase, bold-
face letters (e.g., M, Ma, or Mab), and scalars are
written as lowercase letters (e.g., s or qz). Struc-
tured objects such as sequences of discrete sym-
bols are written with lowercase, bold, italic let-
ters (e.g., w refers to a sequence of input words).
We use a semicolon to denote vector concatena-
tion (e.g., [a; b]).

2 The Parsing Model

Our departure point is the parsing model described
in (Dyer et al., 2015). We do not describe the
model in detail, and defer the reader to the orig-
inal work. At each stage t of the parsing process,
the parser state is encoded into a vector pt, which
is used to compute the probability of the parer ac-
tion at time t as:

p(zt | pt) =

exp (cid:0)g⊤

zt pt + qzt(cid:1)

Pz ′∈A(S,B) exp (cid:0)g⊤

z ′pt + qz ′(cid:1)

,

(1)

where gz is a column vector representing the (out-
put) embedding of the parser action z, and qz is
a bias term for action z. The set A(S, B) repre-
sents the valid transition actions that may be taken
in the current state. Since pt encodes information
about all previous decisions made by the parser,
the chain rule may be invoked to write the proba-
bility of any valid sequence of parse transitions z
conditional on the input as:

p(z | w) =

|z|
Y
t=1

p(zt | pt).

(2)

3 Training Procedure

The parser is trained to maximize the conditional
probability of taking a “correct” action at each
parsing state. The deﬁnition of what constitutes
a “correct” action is the major difference between
static oracle which was used in (Dyer et al., 2015)
and the dynamic oracle explored here.

Regardless of the oracle, our training imple-
mentation constructs a computation graph (nodes
that represent values,
linked by directed edges
from each function’s inputs to its outputs) for the
negative log probability for the oracle transition
sequence as a function of the current model pa-
rameters and uses forward- and backpropagation
to obtain the gradients respect to the model param-
eters (Lecun et al., 1998, section 4).

3.1 Static Oracle Training
With a static oracle, the training procedure com-
putes a canonical reference series of transitions
for each gold parse tree.
It then runs the parser
through this canonical sequence of transitions,
while keeping track of the state representation pt
at each step t, as well as the distribution over tran-
sitions p(zt | pt) which is predicted by the current

classiﬁer for the state representation. Once the end
of the sentence is reached, the parameters are up-
dated towards minimizing the cross-entropy of the
reference transition sequence (Equation 2) by min-
imizing the cross-entropy (i.e., maximizing the
log-likelihood) of the correct transition p(zgt | pt)
at each state along the path.

3.2 Training with Exploration using

Dynamic Oracles

In the static oracle case, the parser is trained to
predict the best transition to take at each parsing
step, assuming all previous transitions were cor-
rect. Since the parser is likely to make mistakes
at test time and encounter states it has not seen
during training, this training criterion is problem-
atic (Daum´e III et al., 2009; Ross et al., 2011;
Goldberg and Nivre, 2012; Goldberg and Nivre,
2013, inter alia). Instead, we would prefer to train
the parser to behave optimally also after making a
mistake (under the constraints that it cannot back-
track or ﬁx any previous decision). We thus need
to include in the training examples states that re-
sult from wrong parsing decisions, together with
the optimal transitions to take in these states. To
this end we reconsider which training examples
to show, and what it means to behave optimally
on these training examples. The training with ex-
ploration using dynamic oracles framework sug-
gested by Goldberg and Nivre (2012; 2013) pro-
vides answers to these questions.

While the application of dynamic oracle train-
ing is relatively straightforward, some adaptations
were needed to accommodate the probabilistic
training objective. These adaptations mostly fol-
low Goldberg (2013).

3.2.1 Dynamic Oracles
A dynamic oracle is the component that, given a
gold parse tree, provides the optimal set of possi-
ble actions to take under a given parser state. In
contrast to static oracles that derive a canonical
sequence for each gold parse tree and say noth-
ing about parsing states that do not stem from this
canonical path, the dynamic oracle is well-deﬁned
for states that result from parsing mistakes, and
may produce more than a single gold action for a
given state. Under the dynamic oracle framework,
a parsing action is said to be optimal in a given
state if the best tree that can be reached after tak-
ing the action is no worse (in terms of accuracy
with respect to the gold tree) than the best tree that

could be reached prior to taking that action.

cies.

Goldberg and Nivre (2013) deﬁne the arc-
decomposition property of transition systems, and
show how to derive efﬁcient dynamic oracles
for transition systems that are arc-decomposable.2
Unfortunately, the arc-standard transition system
does not have this property. While it is possible to
compute dynamic oracles for the arc-standard sys-
tem (Goldberg et al., 2014), the computation re-
lies on a dynamic programming algorithm which
is polynomial in the length of the stack. As the dy-
namic oracle has to be queried for each parser state
seen during training, the use of this dynamic oracle
will make the training times several times longer.
We chose instead to switch to the arc-hybrid tran-
sition system (Kuhlmann et al., 2011), which is
very similar to the arc-standard system but is arc-
decomposable and hence admits an efﬁcient O(1)
dynamic oracle, resulting in only negligible bur-
den on the training time. We implemented the dy-
namic oracle to the arc-hybrid system as described
by Goldberg (2013).

3.2.2 Training with Exploration
In order to expose the parser to conﬁgurations that
are likely to result from incorrect parsing deci-
sions, we make use of the probabilistic nature of
the classiﬁer. During training, instead of follow-
ing the gold action, we sample the next transition
according to the output distribution the classiﬁer
assigns to the current conﬁguration. Another op-
tion, taken by Goldberg and Nivre, is to follow the
one-best action predicted by the classiﬁer. How-
ever, initial experiments showed that the one-best
approach did not work well. Because the neural-
network classiﬁer becomes accurate early on in the
training process, the one-best action is likely to be
correct, and the parser is then exposed to very few
error states in its training process. By sampling
from the predicted distribution, we are effectively
increasing the chance of straying from the gold
path during training, while still focusing on mis-
takes that receive relatively high parser scores. We
believe further formal analysis of this method will
reveal connections to reinforcement learning and,
perhaps, other methods for learning complex poli-

2Speciﬁcally: for every possible parser conﬁguration p
and group of arcs A, if each arc in A can be derived from
p, then a valid tree structure containing all of the arcs in A
can also be derived from p. This is a sufﬁcient condition, but
whether it is necessary is unknown; hence the question of an
efﬁcient, O(1) dynamic oracle for the augmented system is
an open research problem.

Taking this idea further, we could increase the
number of error-states observed in the training
process by changing the sampling distribution so
as to bias it toward more low-probability states.
We do this by raising each probability to the power
of α (0 < α ≤ 1) and re-normalizing. This trans-
formation keeps the relative ordering of the events,
while shifting probability mass towards less fre-
quent events. As we show below, this turns out to
be very beneﬁcial for the conﬁgurations that make
use of external embeddings. Indeed, these conﬁg-
urations achieve high accuracies and shard class
distributions early on in the training process.

The parser is trained to optimize the log likeli-
hood of a correct action zg at each parsing state pt
according to Equation 1. When using the dynamic
oracle, a state pt may admit multiple correct ac-
g = {zgi, . . . , zgk }. Our objective in such
tions z
cases is that the set of correct actions receives high
probability. We optimize for the log of p(z
g | pt)
deﬁned as:

p(zg | pt) = X
zgi ∈zg

p(zgi | pt)

(3)

A similar approach was taken by Charniak and
Johnson (2005) and Goldberg (2013) in the con-
text of log-linear probabilistic models.

4 Experiments

We follow the setup of Dyer et al (2015).

Optimization Procedure Details Parameter op-
timization was performed using stochastic gradi-
ent descent with an initial learning rate of η0 =
0.1, and the learning rate was updated on each
pass through the training data as ηt = η0/(1 + ρt),
with ρ = 0.1 and where t is the number of epochs
completed. No momentum was used. To mitigate
the effects of “exploding” gradients, we clipped
the ℓ2 norm of the gradient to 5 before apply-
ing the weight update rule (Sutskever et al., 2014;
Graves, 2013). An ℓ2 penalty of 1 × 10−6 was
applied to all weights. Matrix and vector pa-
rameters were initialized with uniform samples in
±p6/(r + c), where r and c were the number
of rows and columns in the structure (Glorot and
Bengio, 2010).

4.1 Data

We used the same data setup as Chen and Manning
(2014) and Dyer et al (2015), namely an English
task and a Chinese task.

• For English, we used the Stanford Dependen-
cency (SD) treebank (Marneffe et al., 2006)
used in (Chen and Manning, 2014) which is
the closest model published, with the same
splits.3 The part-of-speech tags are predicted
by using the Stanford POS tagger (Toutanova
et al., 2003) with an accuracy of 97.3%.
This treebank contains a negligible amount
of non-projective arcs (Chen and Manning,
2014).

• For Chinese, we use the Penn Chinese Tree-
bank 5.1 (CTB5) following Zhang and Clark
(2008),4 with gold part-of-speech tags which
is also the same as in Chen and Manning
(2014).

Language model word embeddings were gener-
ated from the AFP portion of the English Gi-
gaword corpus (version 5), and from the com-
plete Chinese Gigaword corpus (version 2), as
segmented by the Stanford Chinese Segmenter
(Tseng et al., 2005).

4.2 Results

Table 1 shows the results of the parser with arc-
hybrid and dynamic oracles for the English data
set (with Stanford Dependencies) and the Chinese
CTB data sets with and without pretrained word
embeddings, respectively. The table also shows
the best result obtained with the static oracle, for
the sake of comparison between static and dy-
namic training strategies.

The results achieved by the dynamic oracle for
English are actually one of the best results ever re-
ported in this data set, achieving 93.56 unlabelled
attachment score. This is remarkable given that
the parser uses a completely greedy inference pro-
cedure. Moreover, the Chinese numbers establish
the state-of-the-art by using the same settings as in
(Chen and Manning, 2014).

The error-exploring dynamic-oracle training al-
ways improves above static oracle training using

3Training: 02-21. Development: 22. Test: 23.
4Training: 001–815, 1001–1136. Development: 886–

931, 1148–1151. Test: 816–885, 1137–1147.

the same transition system, but the arc-hybrid sys-
tem slightly under-performs the arc-standard sys-
tem when trained with static oracle. Transforming
the sampling distribution towards preferring less
probable events (α = 0.75) is especially beneﬁcial
when training with pretrained word embeddings.

5 Related Work

Training greedy parsers on non-gold outcomes, fa-
cilitated by dynamic oracles, has been explored
by several researchers in different ways (Goldberg
and Nivre, 2012; Goldberg and Nivre, 2013; Gold-
berg et al., 2014; Honnibal et al., 2013; Honni-
bal and Johnson, 2014; G´omez-Rodr´ıguez et al.,
2014; Bj¨orkelund and Nivre, 2015; Tokg¨oz and
Eryi˘git, 2015; G´omez-Rodr´ıguez and Fern´andez-
Gonz´alez, 2015). More generally, training greedy
inference systems by paying attention to the ex-
pected classiﬁer behavior during test time has been
explored under the imitation learning and learn-
ing to search (SEARN) frameworks (Abbeel and
Ng, 2004; Daum´e III and Marcu, 2005; Vlachos,
2012; He et al., 2012; Daum´e III et al., 2009; Ross
et al., 2011). Directly modeling the probability of
making a mistake has also been explored for pars-
ing (Yazdani and Henderson, 2015).

The use of recurrent neural networks to condi-
tionally predict actions in sequence given a history
are spurring increased interest in training regimes
that make the learned model more robust to test-
time prediction errors. Solutions based on cur-
riculum learning (Bengio et al., 2015), expected
loss training (Shen et al., 2015), and reinforce-
ment learning have been proposed (Ranzato et al.,
2016).

Time and test time prediction was recently ex-
plored by Bengio et al. (2015) for the task of
language modeling. However, to the best of our
knowledge this is the ﬁrst time that such a train-
ing procedure is explored together with neural net-
works for parsing. The results show that the con-
sistent gains in accuracy due to dynamic oracle
training that were observed in previous work per-
sist also for the very strong LSTM-based parser,
although some care had to be taken to adapt the
training procedures to the probabilistic nature of
the objective, and to the high accuracy levels of
the underlying model.

6 Conclusions

Dyer et al. (2015) presented stack LSTMs, re-

Development:

Language
Chinese (–pretraining)
Chinese (+pretraining)
English (–pretraining)
English (+pretraining)
Test:.

Arc-std.
(static)

Arc-hybrid

(static)

Arc-hybrid
(dynamic)

UAS
87.23
87.23
92.58
93.11

LAS UAS
85.87
82.65
87.11
85.87
92.64
90.24
90.80
93.16

LAS UAS
79.84
85.96
87.41
85.64
93.01
90.26
90.88
93.38

Arc-hybrid

(dynamic, α = 0.75)
LAS
84.59
85.87
90.64
91.29

LAS UAS
86.32
84.37
87.41
85.99
93.04
90.68
93.51
91.03

Arc-std.
(static)

Arc-hybrid

(static)

Arc-hybrid
(dynamic)

Language
Chinese (–pretraining)
Chinese (+pretraining)
English (–pretraining)
English (+pretraining)

UAS
85.48
86.85
92.40
93.04

LAS UAS
83.94
85.66
86.94
85.36
92.08
90.04
90.87
92.78

LAS UAS
84.03
86.07
87.05
85.46
92.66
89.80
90.67
93.15

Arc-hybrid

(dynamic, α = 0.75)
LAS
84.53
86.21
90.60
91.42

LAS UAS
86.13
84.46
87.65
85.63
92.73
90.43
93.56
91.05

Table 1: Unlabeled attachment scores and labeled attachment scores on the development sets (above)
and the ﬁnal test sets (below) for Chinese and English (SD). The columns marked with “Arc-std.” show
the results of the parser arc-standard oracle, which are equivalent numbers to the ones presented by
Dyer et al. (2015). The columns marked with “Arc-hybrid” show the results of the parser with the arc-
hybrid oracle. The columns marked with “static” (“dynamic”) show the results of the parser with static
(dynamic) oracles.

current neural networks for sequences, with push
and pop operations, and used them to imple-
ment a state-of-the-art
transition-based depen-
dency parser. The parser uses a greedy learning
strategy which potentially provides very high pars-
ing speed while still achieving state-of-the-art re-
sults.

In this paper, we demonstrate that there is still
further improvement by training the greedy parser
on non-gold outcomes; training with dynamic or-
acles provides a push over the baseline, achiev-
ing 93.56 UAS for English, while still maintaining
very fast (and greedy) parsing speed.

Acknowledgments

This work was sponsored in part by the U. S. Army
Research Laboratory and the U. S. Army Research
Ofﬁce under contract/grant number W911NF-10-
1-0533, and in part by NSF CAREER grant IIS-
1054319. Miguel Ballesteros is supported by the
European Commission under the contract numbers
FP7-ICT-610411 (project MULTISENSOR) and
H2020-RIA-645012 (project KRISTINA). Yoav
Goldberg is supported by the Intel Collabora-
tive Research Institute for Computational Intelli-
gence (ICRI-CI) and the Israeli Science Founda-

tion (grant number 1555/15).

References

Pieter Abbeel and Andrew Y. Ng. 2004. Apprentice-
ship learning via inverse reinforcement learning. In
Proceedings of the International Conference on Ma-
chine Learning (ICML).

Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and
Noam Shazeer. 2015. Scheduled sampling for se-
quence prediction with recurrent neural networks.
arXiv preprint arXiv:1506.03099.

Anders Bj¨orkelund and Joakim Nivre. 2015. Non-
deterministic oracles for unrestricted non-projective
transition-based dependency parsing.
In Proceed-
ings of the 14th International Conference on Pars-
ing Technologies, pages 76–86, Bilbao, Spain, July.
Association for Computational Linguistics.

Eugene Charniak and Mark Johnson. 2005. Coarse-
to-ﬁne n-best parsing and MaxEnt discriminative
reranking. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 173–180.

Danqi Chen and Christopher D. Manning. 2014. A
fast and accurate dependency parser using neural
networks.
In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP).

Hal Daum´e III and Daniel Marcu. 2005. Learning
as search optimization: Approximate large margin
methods for structured prediction.
In Proceedings
of the International Conference on Machine Learn-
ing (ICML).

Hal Daum´e III, John Langford, and Daniel Marcu.
2009. Search-based structured prediction. Machine
Learning, 75:297–325.

Chris Dyer, Miguel Ballesteros, Wang Ling, Austin
Matthews, and Noah A. Smith. 2015. Transition-
based dependency parsing with stack long short-
term memory. In Proceedings of the Annual Meet-
ing of the Association for Computational Linguistics
(ACL).

Xavier Glorot and Yoshua Bengio. 2010. Understand-
ing the difﬁculty of training deep feedforward neural
networks. In Proceedings of the International Con-
ference on Machine Learning (ICML).

Yoav Goldberg and Joakim Nivre. 2012. A dynamic
oracle for arc-eager dependency parsing.
In Pro-
ceedings of the Conference on Computational Lin-
guistics (COLING).

Yoav Goldberg and Joakim Nivre. 2013. Training
deterministic parsers with non-deterministic oracles.
Transactions of the association for Computational
Linguistics (TACL), 1:403–414.

Yoav Goldberg, Francesco Sartorio, and Giorgio Satta.
2014. A tabular method for dynamic oracles in
transition-based parsing. Transactions of the asso-
ciation for Computational Linguistics, 2.

Yoav Goldberg.

2013. Dynamic-oracle transition-
based parsing with calibrated probabilistic output.
In Proc. of IWPT.

Carlos G´omez-Rodr´ıguez and Daniel Fern´andez-
Gonz´alez. 2015. An efﬁcient dynamic oracle for
unrestricted non-projective parsing.
In Proceed-
ings of the Association for Computational Linguis-
tics (ACL), pages 256–261, Beijing, China, July. As-
sociation for Computational Linguistics.

Carlos G´omez-Rodr´ıguez, Francesco Sartorio, and
Giorgio Satta. 2014. A polynomial-time dynamic
oracle for non-projective dependency parsing.
In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP),
pages 917–927, Doha, Qatar, October. Association
for Computational Linguistics.

Alex Graves. 2013. Generating sequences with recur-

rent neural networks. CoRR, abs/1308.0850.

He He, Hal Daum´e III, and Jason Eisner. 2012. Imita-

tion learning by coaching. In Proc. NIPS.

Matthew Honnibal and Mark Johnson. 2014.

Joint
incremental disﬂuency detection and dependency
parsing. Transactions of the association for Com-
putational Linguistics (TACL), 2:131–142.

Matthew Honnibal, Yoav Goldberg, and Mark John-
son. 2013. A non-monotonic arc-eager transition
system for dependency parsing. In Proceedings of
the Seventeenth Conference on Computational Nat-
ural Language Learning (CoNLL), pages 163–172.
Citeseer.

Marco Kuhlmann, Carlos G´omez-Rodr´ıguez, and Gior-
gio Satta. 2011. Dynamic programming algorithms
for transition-based dependency parsers.
In Pro-
ceedings of the Association for Computational Lin-
guistics.

Yann Lecun, L´eon Bottou, Yoshua Bengio, and Patrick
Haffner. 1998. Gradient-based learning applied to
document recognition. In Proc. IEEE.

Marie-Catherine De Marneffe, Bill Maccartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of Language Resources and Evaluation
Conference (LREC).

Joakim Nivre. 2003. An Efﬁcient Algorithm for Pro-
jective Dependency Parsing. In Proceedings of the
8th International Workshop on Parsing Technologies
(IWPT), pages 149–160.

Joakim Nivre. 2004. Incrementality in deterministic
dependency parsing.
In Proceedings of the Work-
shop on Incremental Parsing: Bringing Engineering
and Cognition Together.

Joakim Nivre. 2008. Algorithms for deterministic in-
cremental dependency parsing. Computational Lin-
guistics, 34:4:513–553. MIT Press.

Marc’Aurelio Ranzato, Sumit Chopra, Michael Auli,
and Wojciech Zaremba. 2016. Sequence level train-
ing with recurrent neural networks. In Proceedings
of ICLR 2016.

St´ephane Ross, Geoffrey J. Gordon, and J. Andrew
Bagnell. 2011. A reduction of imitation learning
and structured prediction to no-regret online learn-
ing. In Proceedings of the Conference on Artiﬁcial
Intelligence and Statistics (AISTATS).

Shiqi Shen, Yong Cheng, Zhongjun He, Wei He, Hua
Wu, Maosong Sun, and Yang Liu. 2015. Minimum
risk training for neural machine translation. CoRR,
abs/1512.02433.

Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014.
Sequence to sequence learning with neural net-
works. In Proceedings of Neural Information Pro-
cessing Systems (NIPS).

Alper Tokg¨oz and G¨uls¸en Eryi˘git. 2015. Transition-
based dependency dag parsing using dynamic ora-
cles. In Proceedings of the Student Research Work-
shop of the Association for Computational Linguis-
tics (ACL), pages 22–27, Beijing, China, July. Asso-
ciation for Computational Linguistics.

Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings NAACL.

Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel
Jurafsky, and Christopher Manning. 2005. A con-
ditional random ﬁeld word segmenter for SIGHAN
bakeoff 2005. In Proc. Fourth SIGHAN Workshop
on Chinese Language Processing.

Andreas Vlachos. 2012. An investigation of imita-
tion learning algorithms for structured prediction.
In Proceedings of the European Workshop on Rein-
forcement Learning (EWRL), pages 143–154.

Hiroyasu Yamada and Yuji Matsumoto. 2003. Statis-
tical dependency analysis with support vector ma-
chines.
In Proceedings of the 8th International
Workshop on Parsing Technologies (IWPT), pages
195–206.

Majid Yazdani and James Henderson. 2015.

Incre-
mental recurrent neural network dependency parser
with search-based discriminative training. In Proc.
CoNLL.

Yue Zhang and Stephen Clark. 2008. A tale of two
parsers: Investigating and combining graph-based
and transition-based dependency parsing.
In Pro-
ceedings of the Conference on Empirical Methods
in Natural Language Processing (EMNLP).

