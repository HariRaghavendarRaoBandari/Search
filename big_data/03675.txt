6
1
0
2

 
r
a

 

M
9
2

 
 
]
E
M

.
t
a
t
s
[
 
 

2
v
5
7
6
3
0

.

3
0
6
1
:
v
i
X
r
a

Optimal Data Collection for Randomized

Control Trials

By Pedro Carneiro, Sokbae Lee, and Daniel Wilhelm∗

29 March 2016

Abstract

In a randomized control trial, the precision of an average treatment eﬀect
estimator can be improved either by collecting data on additional individuals, or
by collecting additional covariates that predict the outcome variable. We propose
the use of pre-experimental data such as a census, or a household survey, to
inform the choice of both the sample size and the covariates to be collected. Our
procedure seeks to minimize the resulting average treatment eﬀect estimator’s
mean squared error, subject to the researcher’s budget constraint. We rely on an
orthogonal greedy algorithm that is conceptually simple, easy to implement (even
when the number of potential covariates is very large), and does not require any
tuning parameters. In two empirical applications, we show that our procedure
can lead to substantial gains of up to 58%, either in terms of reductions in data
collection costs or in terms of improvements in the precision of the treatment
eﬀect estimator, respectively.

JEL codes: C55, C81.

Key words: randomized control trials, big data, data collection, optimal survey
design, orthogonal greedy algorithm, survey costs.

∗Carneiro: University College London, Institute for Fiscal Studies (IFS), and Centre for Microdata
Methods and Practice (CeMMAP); Lee: IFS and CeMMAP; Wilhelm: University College London
and CeMMAP. We thank Frank Diebold, Kirill Evdokimov, Michal Kolesar, David McKenzie, Ulrich
M¨uller, and participants at various seminars for helpful discussions. An early version of this paper
was presented at Columbia University and Princeton University in September 2014, and at New York
University and University of Pennsylvania in December 2014. This work was supported in part by the
European Research Council (ERC-2014-CoG-646917-ROMIA) and by the UK Economic and Social
Research Council (ESRC) through a grant (RES-589-28-0001) to the ESRC CeMMAP.

1

I

Introduction

This paper is motivated by the observation that empirical research in economics increas-
ingly involves the collection of original data through laboratory or ﬁeld experiments
(see, e.g. Duﬂo, Glennerster, and Kremer, 2007; Banerjee and Duﬂo, 2009; Bandiera,
Barankay, and Rasul, 2011; List, 2011; List and Rasul, 2011; Hamermesh, 2013, among
others). This observation carries with it a call and an opportunity for research to
provide econometrically sound guidelines for data collection.

We consider the decision problem faced by a researcher designing the survey for a
randomized control trial (RCT). We assume that the goal of the researcher is to obtain
precise estimates of the average treatment eﬀect using the experimental data. Data
collection is costsly and the researcher is restricted by a budget, which limits how much
data can be collected. We focus on optimally trading oﬀ the number of individuals
included in the RCT and the choice of covariates elicited as part of the data collection
process.

Even though this is a simple setting, what we describe is typical of situations where
there is collection of original data, in laboratory or ﬁeld experiments. Often the goal
is to estimate an average treatment eﬀect parameter, or a set of similar parameters,
with the set of such parameters to be estimated being sometimes ﬁxed a priori in pre-
analysis plans. Covariates are usually collected and included in regression models, and
again, advanced consideration of which covariates to include in the model is also part
of pre-analysis plans (in which case it is especially important to select the optimal set
of covariates to include in the survey). The trade-oﬀ between sample size and survey
size is a natural one when budgets are limited.

We assume the researcher has access to pre-experimental data from the population
from which the experimental data will be drawn or at least from a population that
shares similar second moments of the variables to be collected. The data set includes
all the potentially relevant variables that one would consider collecting for the analysis
of the experiment. Then we consider the following survey design problem. A researcher
faces a ﬁxed budget, which she can use to survey a large number of subjects, or to collect
a large set of covariates. The researcher needs to choose the optimal combination of
the sample size and covariate selection, which maximizes precision, given the available
budget. The decision about collecting covariates as well as the sample size of the
experiment is made before the experiment takes place.

The experimenter’s decision is informed by the pre-experimental dataset available
In general, for each possible sample size, one needs to consider all possible

to her.

2

combinations of covariates within the budget. Therefore, we need to solve a diﬃcult
combinatorial optimization problem.1 This problem is especially challenging when the
set of potential variables to choose from is not very small, a case that is increasingly
encountered in today’s big data environment. Fortunately, with the increased availabil-
ity of high-dimensional data, methods for the analysis of such data sets have received
growing attention in several ﬁelds, including economics (Belloni, Chernozhukov, and
Hansen, 2014). This literature makes available a rich set of new tools, which can be
adapted to our study of optimal survey design.

In this paper, we propose the use of a computationally attractive algorithm based
on the orthogonal greedy algorithm (OGA) – also known as the orthogonal matching
pursuit; see, for example, Tropp (2004) and Tropp and Gilbert (2007) among many
others. To implement the OGA, it is necessary to specify the stopping rule, which in
turn generally requires a tuning parameter. One attractive feature of our algorithm is
that, once the budget constraint is given, there is no longer the need to choose a tuning
parameter to implement the proposed method, as the budget constraint plays the role
of a stopping rule. In other words, we develop an automated OGA that is tailored to
our own decision problem. Furthermore, it performs well even when there are a large
number of potential covariates in the pre-experimental data set.

There is a large and important body of literature on the design of experiments,
starting with Fisher (1935). There also exists an extensive body of literature on sample
size (power) calculations; see, for example, McConnell and Vera-Hern´andez (2015) for a
practical guide. Both bodies of literature are concerned with the precision of treatment
eﬀect estimates, but neither addresses the problem that concerns us. For instance,
McConnell and Vera-Hern´andez (2015) have developed methods to choose the sample
size when cost constraints are binding, but they neither consider the issue of collecting
covariates nor its trade-oﬀ with selecting the sample size.

Both our paper and the standard literature on power calculations rely on the avail-
ability of information in pre-experimental data. The calculations we propose can be
seen as a substantive reformulation and extension of the more standard power calcula-
tions, which are an important part of the design of any RCT. When conducting power
calculations, one searches for the sample size that allows the researcher to detect a
particular eﬀect size in the experiment. The role of covariates can be accounted for if
one has pre-deﬁned the covariates that will be used in the experiment, and one knows

1It may be possible to derive simple solutions to this problem only in settings where the function
relating survey costs with the sample size and the covariates to be collected takes very simple forms;
however, this will be unrealistic in most cases.

3

(based on some pre-experimental data) how they aﬀect the outcome. Then, once the
signiﬁcance level and power parameters are determined (specifying the type I and type
II errors one is willing to accept), all that matters is the impact of the sample size on
the variance of the treatment eﬀect.

Suppose that, instead of asking what is the minimum sample size that allows us to
detect a given eﬀect size, we asked instead how small an eﬀect size we could detect with
a particular sample size (this amounts to a reversal of the usual power calculation). In
this simple setting with pre-deﬁned covariates, the sample size would deﬁne a particular
survey cost, and we would essentially be asking about the minimum size of the variance
of the treatment eﬀect that one could obtain at this particular cost, which would
lead to a question similar to the one asked in this paper. Therefore, one simple way
to describe our contribution is that we adapt and extend the information in power
calculations to account for the simultaneous selection of covariates and sample size,
explicitly considering the costs of data collection.

To illustrate the application of our method we examine two recent experiments for
which we have detailed knowledge of the process and costs of data collection. We ask
two questions. First, if there is a single hypothesis one wants to test in the experiment,
concerning the impact of the experimental treatment on one outcome of interest, what
is the optimal combination of covariate selection and sample size given by our method,
and how much of an improvement in the precision of the impact estimate can we obtain
as a result? Second, what are the minimum costs of obtaining the same precision of
the treatment eﬀect as in the actual experiment, if one was to select covariates and
sample size optimally (what we call the “equivalent budget”)?

We ﬁnd from these two examples that by adopting optimal data collection rules, not
only can we achieve substantial increases in the precision of the estimates (statistical
importance) for a given budget, but we can also accomplish sizeable reductions in the
equivalent budget (economic importance). To illustrate the quantitative importance of
the latter, we show that the optimal selection of the set of covariates and the sample
size leads to a reduction of about 45 percent (up to 58 percent) of the original budget
in the ﬁrst (second) example we consider, while maintaining the same level of the
statistical signiﬁcance as in the original experiment.

To the best of our knowledge, no paper in the literature directly considers our data
collection problem. Some papers address related but very diﬀerent problems (see Hahn,
Hirano, and Karlan, 2011; List, Sadoﬀ, and Wagner, 2011; Bhattacharya and Dupas,
2012; McKenzie, 2012; Dominitz and Manski, 2016). They study some issues of data
measurement, budget allocation or eﬃcient estimation; however, they do not consider

4

the simultaneous selection of the sample size and covariates for the RCTs as in this
paper. Because our problem is distinct from the problems studied in these papers,
we give a detailed comparison between our paper and the aforementioned papers in
Section VI.

More broadly, this paper is related to a recent emerging literature in economics that
emphasizes the importance of micro-level predictions and the usefulness of machine
learning for that purpose. For example, Kleinberg, Ludwig, Mullainathan, and Ober-
meyer (2015) argue that prediction problems are abundant in economic policy analysis,
and recent advances in machine learning can be used to tackle those problems. Fur-
thermore, our paper is related to the contemporaneous debates on pre-analysis plans
which demand, for example, the selection of sample sizes and covariates before the
implementation of an RCT; see, for example, Coﬀman and Niederle (2015) and Olken
(2015) for the advantages and limitations of the pre-analysis plans.

The remainder of the paper is organized as follows.

In Section II, we describe
our data collection problem in detail. In Section III, we propose the use of a simple
algorithm based on the OGA. In Section IV, we discuss the costs of data collection in
experiments. In Section V, we present two empirical applications, in Section VI, we
discuss the existing literature, and in Section VII, we give concluding remarks. Online
appendices provide details that are omitted from the main text.

II Data Collection Problem

Suppose we are planning a RCT in which we randomly assign individuals to either a
treatment (D = 1) or a control group (D = 0) with corresponding potential outcomes
Y1 and Y0, respectively. After administering the treatment to the treatment group,
we collect data on outcomes Y for both groups so that Y = DY1 + (1 − D)Y0. We
also conduct a survey to collect data on a potentially very high-dimensional vector of
covariates Z (e.g. from a household survey covering demographics, social background,
income etc.) that predicts potential outcomes. These covariates are a subset of the
universe of predictors of potential outcomes, denoted by X. Random assignment of D
means that D is independent of potential outcomes and of X.

Our goal is to estimate the average treatment eﬀect β0 := E[Y1 − Y0] as precisely as
possible, where we measure precision by the ﬁnite sample mean-squared error (MSE)
of a treatment eﬀect estimator. Instead of simply regressing Y on D, we want to make
use of the available covariates Z to improve the precision of the resulting treatment

5

eﬀect estimator.2 Therefore, we consider estimating β0 in the regression

Y = α0 + β0D + γ(cid:48)

0Z + U,

(1)

0)(cid:48) is a vector of parameters to be estimated and U is an error term.
where (α0, β0, γ(cid:48)
The implementation of the RCT requires us to make two decisions that may have a
signiﬁcant impact on the resulting treatment eﬀect estimator’s precision:

1. Which covariates Z should we select from the universe of potential predictors X?

2. From how many individuals (n) should we collect data on (Y, D, Z)?

Obviously, a large experimental sample size n improves the precision of the treatment
eﬀect estimator. Similarly, collecting more covariates, in particular strong predictors of
potential outcomes, reduces the variance of the residual ε which, in turn, also improves
the precision of the estimator. At the same time collecting data from more individuals
and on more covariates is costly so that, given a ﬁnite budget, we want to ﬁnd a
combination of sample size n and covariate selection Z that leads to the most precise
treatment eﬀect estimator possible.

In this section, we propose a procedure to make this choice based on a pre-experimental

data set on Y and X, such as a pilot study or a census from the same population from
which we plan to draw the RCT sample.3 The combined data collection and estimation
procedure can be summarized as follows:

1. Obtain pre-experimental data Spre on (Y, X).
2. Use data in Spre to select the covariates Z and sample size n.
3. Implement the RCT and collect the experimental data Sexp on (Y, D, Z).
4. Estimate the average treatment eﬀect using Sexp.

5. Compute standard errors.

We now describe the ﬁve steps listed above in more detail. The main component of
our procedure consists of a proposal for the optimal choice of n and Z in Step 2, which
is described more formally in Section III.

2One may want to collect covariates also for other reasons, but we focus here on improving precision
of the treatment eﬀect estimator and in the next subsection consider the use of covariates for checking
balance.

3In fact, we do not need the populations to be identical, but only require second moments to be

the same.

6

Step 1. Obtain pre-experimental data. We assume the availability of data
on outcomes Y ∈ R and covariates X ∈ RM for the population from which we
plan to draw the experimental data. We denote the pre-experimental sample of
size N by Spre := {Yi, Xi}N
i=1. Our framework allows the number of potential
covariates, M , to be very large (possibly much larger than the sample size N ).
Typical examples would be census data, household surveys, or data from other,
similar experiments. Another possible candidate is a pilot experiment that was
carried out before the larger-scale role out of the main experiment, provided that
the sample size N of the pilot study is large enough for our econometric analysis
in Step 2.

Step 2. Optimal selection of covariates and sample size. We want to use
the pre-experimental data to choose the sample size, and which covariates should
be in our survey. Let S ∈ {0, 1}M be a vector of ones and zeros of the same
dimension as X. We say that the jth covariate (X (j)) is selected if Sj = 1,
and denote by XS the subvector of X containing elements that are selected by
S. For example, consider X = (X (1), X (2), X (3)) and S = (1, 0, 1). Then XS =
(X (1), X (3)). For any vector of coeﬃcients γ ∈ RM , let I(γ) ∈ {0, 1}M denote
the nonzero elements of γ. Suppose X contains a constant term. We can then
rewrite (1) as

Y = β0D + γ(cid:48)

I(γ)XI(γ) + U (γ),

(2)

where γ ∈ RM and U (γ) := Y − β0D − γ(cid:48)
I(γ)XI(γ). For a given γ and sample size
n, we denote by ˆβ(γ, n) the OLS estimator of β0 in a regression of Y on D and
XI(γ) using a random sample {Yi, Di, Xi}n
Data collection is costly and therefore constrained by a budget of the form
c(S, n) ≤ B, where c(S, n) are the costs of collecting the variables given by
selection S from n individuals, and B is the researcher’s budget.

i=1.

Our goal is to choose the experimental sample size n and the covariate selection
S so as to minimize the ﬁnite sample MSE of ˆβ(γ, n), i.e., we want to choose n
and γ to minimize

(cid:16) ˆβ(γ, n)

(cid:12)(cid:12)(cid:12) D1, . . . , Dn

(cid:17)

:= E

(cid:20)(cid:16) ˆβ(γ, n) − β0

(cid:17)2(cid:12)(cid:12)(cid:12)(cid:12) D1, . . . , Dn

(cid:21)

.

M SE

subject to the budget constraint. The following lemma characterizes the MSE of
the estimator under the homoskedasticity assumption that Var(U (γ)|D = 1) =

7

Var(U (γ)|D = 0) for any γ ∈ RM . This assumption is satisﬁed, for example, if
the treatment eﬀect is constant across individuals in the experiment.
Lemma 1. Assume that Var(U (γ)|D = 1) = Var(U (γ)|D = 0) for any γ ∈ RM .

i=1 Di,

(cid:12)(cid:12)(cid:12) D1, . . . , Dn

(cid:17)

Var (Y − γ(cid:48)X | D = 0)

n ¯Dn(1 − ¯Dn)

.

=

(3)

Then, letting ¯Dn := n−1(cid:80)n
(cid:16) ˆβ(γ, n)

M SE

The proof of this Lemma can be found in the appendix. Note that for each (γ, n),
the MSE is minimized by the equal splitting between the treatment and control
groups. Hence, suppose that the treatment and control groups are of exactly the
same size (i.e., ¯Dn = 0.5). By Lemma 1, minimizing the MSE of the treatment
eﬀect estimator subject to the budget constraint,

(cid:16) ˆβ(γ, n)

(cid:12)(cid:12)(cid:12) D1, . . . , Dn

(cid:17)

min

n∈N+, γ∈RM

M SE

s.t.

c(I(γ), n) ≤ B,

(4)

is equivalent to minimizing the residual variance in a regression of Y on X (con-
ditional on D = 0), divided by the sample size,

min

n∈N+, γ∈RM

1
n

Var (Y − γ(cid:48)X | D = 0)

s.t.

c(I(γ), n) ≤ B,

Importantly, the MSE expression depends on the data only through Var(Y −
γ(cid:48)X|D = 0), which can be estimated before the randomization takes place, i.e.
using the pre-experimental sample Spre. Therefore, the sample counterpart of our
population optimization problem (4) is

N(cid:88)

i=1

min

n∈N+, γ∈RM

1
nN

(Yi − γ(cid:48)Xi)2

s.t.

c(I(γ), n) ≤ B.

(5)

In Section III, we describe a computationally attractive OGA that approximates
the solution to (5). The OGA has been studied extensively in the signal extraction
literature and is implemented in most statistical software packages. Appendices
A and D show that this algorithm possesses desirable theoretical and practical
properties.

The basic idea of the algorithm (in its simplest form) is straightforward. Fix a
sample size n. Start by ﬁnding the covariate that has the highest correlation with

8

the outcome. Regress the outcome on that variable, and keep the residual. Then,
among the remaining covariates, ﬁnd the one that has the highest correlation
with the residual. Regress the outcome onto both selected covariates, and keep
the residual. Again, among the remaining covariates, ﬁnd the one that has the
highest correlation with the new residual, and proceed as before. We iteratively
select additional covariates up to the point when the budget constraint is no
longer satisﬁed. Finally, we repeat this search process for alternative sample
sizes, and search for the combination of sample size and covariate selection that
minimizes the MSE. Denote the OGA solution by (ˆn, ˆγ) and let ˆI := I(ˆγ) denote
the selected covariates. See Section III for more details.

Note that, generally speaking, the OGA requires us to specify how to terminate
the iterative procedure. One attractive feature of our algorithm is that the budget
constraint plays the role of the stopping rule, without introducing any tuning
parameters.

Step 3. Experiment and data collection. Given the optimal selection of co-
variates ˆI and sample size ˆn, we randomly assign ˆn individuals to either the
treatment or the control group (with equal probability), and collect the co-
variates Z := XˆI from each of them. This yields the experimental sample
Sexp := {Yi, Di, Zi}ˆn

i=1 from (Y, D, XˆI).

Step 4. Estimation of the average treatment eﬀect. We regress Yi on
(1, Di, Zi) using the experimental sample Sexp. The OLS estimator of the co-
eﬃcient on Di is the average treatment eﬀect estimator ˆβ.
Step 5. Computation of standard errors. Assuming the two samples Spre
and Sexp are independent, and that treatment is randomly assigned, the presence
of the covariate selection Step 2 does not aﬀect the asymptotic validity of the
standard errors that one would use in the absence of Step 2. Therefore, asymp-
totically valid standard errors of ˆβ can be computed in the usual fashion (see,
e.g., Imbens and Rubin, 2015).

II.A Discussion

In this subsection, we discuss some of conceptual and practical properties of our pro-
posed data collection procedure.

9

Availability of Pre-Experimental Data. As in standard power calculations, pre-
experimental data provide essential information for our procedure. The availability of
such data is very common, ranging from census data sets and other household surveys
to studies that were conducted in a similar context as the RCT we are planning to
implement. In addition, if no such data set is available, one may consider running a
pilot project that collects pre-experimental data. We recognize that in some cases it
might be diﬃcult to have the required information readily available. However, this is a
problem that aﬀects any attempt to a data-driven design of surveys, including standard
power calculations. Even when pre-experimental data are imperfect, such calculations
provide a valuable guide to survey design, as long as the available pre-experimental
data are not very diﬀerent from the ideal data.
In particular, our procedure only
requires second moments of the pre-experimental variables to be similar to those in the
population of interest.

The Optimization Problem in a Simpliﬁed Setup.
In general, the problem
in equation (5) does not have a simple solution. To gain some intuition about the
trade-oﬀs in this problem, in Appendix C we consider a simpliﬁed setup in which all
covariates are orthogonal to each other, and the budget constraint has a very simple
form. We show that if all covariates have the same price, then one wants to choose
covariates up to the point where the percentage increase in survey costs equals the
percentage reduction in the MSE from the last covariate. Furthermore, the elasticity
of the MSE with respect to changes in sample size should equal the elasticity of the
MSE with respect to an additional covariate. If the costs of data collection vary with
covariates, then this conclusion is slightly modiﬁed. If we organize variables by type
according to their contribution to the MSE, then we want to choose variables of each
type up to the point where the percent marginal contribution of each variable to the
MSE equals its percent marginal contribution to survey costs.

In RCTs, covariates typically do not only
Imbalance and Re-randomization.
serve as a means to improving the precision of treatment eﬀect estimators, but also
for checking whether the control and treatment groups are balanced. See, for example,
Bruhn and McKenzie (2009) for practical issues concerning randomization and balance.
To rule out large biases due to imbalance, it is important to carry out balance checks for
strong predictors of potential outcomes. Our procedure selects the strongest predictors
as long as they are not too expensive (e.g. household survey questions such as gender,
race, number of children etc.) and we can check balance for these covariates. However,

10

in principle, it is possible that our procedure does not select a strong predictor that
is very expensive (e.g. baseline test scores). Such a situation occurs in our second
empirical application (Section V.B). In this case, in Step 2, we recommend running
the OGA a second time, forcing the inclusion of such expensive predictors. If the MSE
of the resulting estimate is not much larger than that from the selection without the
expensive predictor, then we may prefer the former selection to the latter so as to
reduce the potential for bias due to imbalance at the expense of slightly larger variance
of the treatment eﬀect estimator.

An alternative approach to avoiding imbalance considers re-randomization until
some criterion capturing the degree of balance is met (e.g., Bruhn and McKenzie
(2009), Morgan and Rubin (2012, 2015)). Our criterion for the covariate selection
procedure in Step 2 can readily be adapted to this case: we only need to replace our
variance expression in the criterion function by the modiﬁed variance in Morgan and
Rubin (2012), which accounts for the eﬀect of re-randomization on the treatment eﬀect
estimator.

Expensive, Strong Predictors. When some covariates have similar predictive power,
but respective prices that are substantially diﬀerent, our covariate selection procedure
may produce a suboptimal choice. For example, if the covariate with the highest price
is also the most predictive, OGA selects it ﬁrst even when there are other covariates
that are much cheaper but only slightly less predictive. In Section V.B, we encounter
an example of such a situation and propose a simple robustness check for whether
removing an expensive, strong predictor may be beneﬁcial.

Properties of the Treatment Eﬀect Estimator. Since the treatment indicator
is assumed independent of X, standard asymptotic theory of the treatment eﬀect esti-
mator continues to hold for our estimator (despite the addition of a covariate selection
step). For example, it is unbiased, consistent, asymptotically normal, and adding the
covariates X in the regression in (1) cannot increase the asymptotic variance of the
estimator. In fact, inclusion of a covariate strictly reduces the estimator’s asymptotic
variance as long as the corresponding true regression coeﬃcient is not zero. All these
results hold regardless of whether the true conditional expectation of Y given D and
X is in fact linear and additive separable as in (1) or not. In particular, in some appli-
cations one may want to include interaction terms of D and X (see, e.g., Imbens and
Rubin, 2015). Finally, the treatment eﬀect can be allowed to be heterogeneous (i.e.
vary across individuals i) in which case our procedure estimates the average of those

11

treatment eﬀects.

An Alternative to Regression. Step 4 consists of running the regression in (1).
There are instances when it is desirable to modify this step. For example, if the selected
sample size ˆn is smaller than the number of selected covariates, then the regression in
(1) is not feasible. However, if the pre-experimental sample Spre is large enough, we
can instead compute the OLS estimator ˆγ from the regression of Y on XˆI in Spre. Then
use Y and Z from the experimental sample Sexp to construct the new outcome variable
:= Yi− ˆγ(cid:48)Zi and compute the treatment eﬀect estimator ˆβ from the regression of ˆY ∗
ˆY ∗
i
on (1, Di). This approach avoids ﬁtting too many parameters when the experimental
sample is small and has the additional desirable property that the resulting estimator
is free from bias due to imbalance in the selected covariates.

i

III A Simple Greedy Algorithm

In practice, the vector X of potential covariates is typically high-dimensional, which
makes it challenging to solve the optimization problem (5). In this section, we propose
a computationally feasible algorithm that is both conceptually simple and easy to
implement.

We split the joint optimization problem in (5) over n and γ into two nested prob-
lems. The outer problem searches over the optimal sample size n, which is restricted
to be on a grid n ∈ N := {n0, n1, . . . , nK}, while the inner problem determines the
optimal selection of covariates for each sample size n:

N(cid:88)

i=1

min
n∈N

1
n

min
γ∈RM

1
N

(Yi − γ(cid:48)Xi)2

s.t.

c(I(γ), n) ≤ B.

(6)

To convey our ideas in a simple form, suppose for the moment that the budget con-
straint has the following linear form,

c(I(γ), n) = n · |I(γ)| ≤ B,

where |I(γ)| denotes the number of non-zero elements of γ. Note that the budget
constraint puts the restriction on the number of selected covariates, that is, |I(γ)| ≤
B/n.

It is known to be NP-hard (non-deterministic polynomial time hard) to ﬁnd a

12

solution to the inner optimization problem in (6) subject to the constraint that γ
has m non-zero components, also called an m-term approximation, where m is the
integer part of B/n in our problem. In other words, solving (6) directly is not feasible
unless the dimension of covariates, M , is small (Natarajan, 1995; Davis, Mallat, and
Avellaneda, 1997).

There exists a class of computationally attractive procedures called greedy algo-
rithms that are able to approximate the infeasible solution. See Temlyakov (2011)
for a detailed discussion of greedy algorithms in the context of approximation theory.
Tropp (2004), Tropp and Gilbert (2007), Barron, Cohen, Dahmen, and DeVore (2008),
Zhang (2009), Huang, Zhang, and Metaxas (2011), Ing and Lai (2011), and Sancetta
(2016), among many others, demonstrate the usefulness of greedy algorithms for signal
recovery in information theory, and for the regression problem in statistical learning.
We modify a popular variant called the OGA and, in particular, use a group OGA
(see, for example, Huang, Zhang, and Metaxas (2011)).

tor v of N observations v1, . . . , vN , let (cid:107)v(cid:107)N := (1/N(cid:80)N

To formally deﬁne our proposed algorithm, we introduce some notation. For a vec-
i )1/2 denote the empirical

i=1 v2

L2-norm and let Y := (Y1, . . . , YN )(cid:48).

Suppose that the covariates X (j), j = 1, . . . , M , are organized into p pre-determined
groups XG1, . . . , XGp, where Gk ⊆ {1, . . . , p} indicates the covariates of group k. We
denote the corresponding matrices of observations by bold letters (i.e., XGk is the
N ×|Gk| matrix of observations on XGk, where |Gk| denotes the number of elements of
the index set Gk). By a slight abuse of notation, we let Xk := X{k} be the column vector
of observations on Xk when k is a scalar. One important special case is that in which
each group consists of a single regressor. Furthermore, we allow for overlapping groups;
in other words, some elements can be included in multiple or even all groups.The group
structure occurs naturally in experiments where data collection is carried out through
surveys whose questions can be grouped in those concerning income, those concerning
education, and so on.

Suppose that the largest group size Jmax := maxk=1,...,p |Gk| is small, so that we can
XGj )/N =
In what follows, assume that
XGj )/N = I|Gj| without loss of generality. Let | · |2 denote the (cid:96)2 norm. The

implement orthogonal transformations within each group such that (X(cid:48)
I|Gj|, where Id is the d-dimensional identity matrix.
(X(cid:48)
following procedure describes our algorithm.

Gj

Gj

Step 1. Set the initial sample size n = n0.

Step 2. Group OGA for a given sample size n:

13

(a) initialize the inner loop at k = 0 and set the initial residual ˆrn,0 = Y, the

initial covariate indices ˆIn,0 = ∅ and the initial group indices ˆGn,0 = ∅;

(b) separately regress ˆrn,k on each group of regressors in {1, . . . , p}\ ˆGn,k; call

ˆjn,k the group of regressors with the largest (cid:96)2 regression coeﬃcients,

(cid:12)(cid:12)(cid:12)X(cid:48)

(cid:12)(cid:12)(cid:12)2

ˆjn,k := arg

max

j∈{1,...,p}\ ˆGn,k

ˆrn,k

Gj

;

add ˆjn,k to the set of selected groups, ˆGn,k+1 = ˆGn,k ∪ {ˆjn,k};

(c) regress Y on the covariates XˆIn,k+1
regression coeﬃcient ˆγn,k+1 := (X(cid:48)
ˆrn,k+1 := Y − XˆIn,k+1

ˆγn,k+1;

ˆIn,k+1

where ˆIn,k+1 := ˆIn,k ∪ Gˆjn,k
XˆIn,k+1

; call the
Y and the residual

)−1X(cid:48)

ˆIn,k+1

(d) increase k by one and continue with (b) as long as c(ˆIn,k, n) ≤ B is satisﬁed;
(e) let kn be the number of selected groups; call the resulting submatrix of

selected regressors Z := XˆIn,kn

and ˆγn := ˆγn,kn, respectively.

Step 3. Set n to the next sample size in N , and go to Step 2 until (and including)
n = nK.

Step 4. Set ˆn as the sample size that minimizes the MSE:

ˆn := arg min
n∈N

1
nN

(Yi − Ziˆγn)2 .

N(cid:88)

i=1

The algorithm above produces the selected sample size ˆn, the selection of covariates
ˆI := ˆIˆn,kˆn with kˆn selected groups and ˆm := m(ˆn) := |ˆIˆn,kˆn| selected regressors. Here,
ˆγ := ˆγˆn is the corresponding coeﬃcient vector on the selected regressors Z.

Remark 1. Theorem A.1 in Appendix A gives the ﬁnite-sample bound on the MSE of
the average treatment eﬀect estimator resulting from our OGA method. The natural
target for this MSE is an infeasible MSE when γ0 is known a priori. Theorem A.1
establishes conditions under which the diﬀerence between the MSE resulting from our
method and the infeasible MSE decreases at a rate of 1/k as k increases, where k is the
number of the steps in the OGA. It is known in a simpler setting than ours that this
rate 1/k cannot generally be improved (see, e.g., Barron, Cohen, Dahmen, and DeVore,
2008). In this sense, we show that our proposed method has a desirable property. See
Appendix A for further details.

14

Remark 2. There are many important reasons for collecting covariates, such as checking
whether randomization was carried out properly and identifying heterogeneous treat-
ment eﬀects, among others. If a few covariates are essential for the analysis, we can
guarantee their selection by including them in every group Gk, k = 1, . . . , p.

IV The Cost of Data Collection

In this section, we discuss the speciﬁcation of the cost function c(S, n) that deﬁnes the
budget constraint of the researcher. In principle, it is possible to construct a matrix
containing the value of the costs of data collection for every possible combination of
S and n without assuming any particular form of relationship between the individual
entries. However, determination of the costs for every possible combination of S and n is
a cumbersome and, in practice, probably infeasible exercise. Therefore, we consider the
speciﬁcation of cost functions that capture the costs of all stages of the data collection
process in a more parsimonious fashion.

We propose to decompose the overall costs of data collection into three compo-
nents: administration costs cadmin(S), training costs ctrain(S, n), and interview costs
cinterv(S, n), so that

c(S, n) = cadmin(S) + ctrain(S, n) + cinterv(S, n).

(7)

In the remainder of this section, we discuss possible speciﬁcations of the three types of
costs by considering ﬁxed and variable cost components corresponding to the diﬀerent
stages of the data collection process. The exact functional form assumptions are based
on the researcher’s knowledge about the operational details of the survey process. Even
though this section’s general discussion is driven by our experience in the empirical
applications of Section V, the operational details are likely to be similar for many
surveys, so we expect the following discussion to provide a useful starting point for
other data collection projects.

We start by specifying survey time costs. Let τj, j = 1, . . . , M , be the costs of
collecting variable j for one individual, measured in units of survey time. Similarly, let
τ0 denote the costs of collecting the outcome variable, measured in units of survey time.
Then, the total time costs of surveying one individual to elicit the variables indicated
by S is

T (S) := τ0 +

τjSj.

M(cid:88)

j=1

15

IV.A Administration and Training Costs

A data collection process typically incurs costs due to administrative work and training
prior to the start of the actual survey. Examples of such tasks are developing the
questionnaire and the program for data entry, piloting the questionnaire, developing
the manual for administration of the survey, and organizing the training required for
the enumerators.

Fixed costs, which depend neither on the size of the survey nor on the sample size
of survey participants, can simply be subtracted from the budget. We assume that B
is already net of such ﬁxed costs.

Most administrative and training costs tend to vary with the size of the question-
naire and the number of survey participants. Administrative work such as development
of the questionnaire, data entry, and training protocols are independent of the number
of survey participants, but depend on the size of the questionnaire (measured by the
number of positive entries in S) as smaller questionnaires are less expensive to prepare
than larger ones. We model those costs by

cadmin(S) := φT (S)α,

(8)

where φ and α are scalars to be chosen by the researcher. We assume 0 < α < 1, which
means that marginal costs are positive but decline with survey size.

Training of the enumerators depends on the survey size, because a longer survey
requires more training, and on the number of survey participants, because surveying
more individuals usually requires more enumerators (which, in turn, may raise the
costs of training), especially when there are limits on the duration of the ﬁeldwork. We
therefore specify training costs as

ctrain(S, n) := κ(n) T (S),

(9)

where κ(n) is some function of the number of survey participants.4 Training costs are
typically lumpy because, for example, there exists only a limited set of room sizes one

4It is of course possible that κ depends not only on n but also on T (S). We model it this way for

simplicity, and because it is a sensible choice in the applications we discuss below.

16

can rent for the training, so we model κ(n) as a step function:

 κ1

κ2

κ (n) =

0 < n ≤ n1
if
if n1 < n ≤ n2
...

.

Here, κ1, κ2, . . . is a sequence of scalars describing the costs of sample sizes in the ranges
deﬁned by the cut-oﬀ sequence n1, n2, . . ..

IV.B Interview Costs

Enumerators are often paid by the number of interviews conducted, and the payment
increases with the size of the questionnaire. Let η denote ﬁxed costs per interview that
are independent of the size of the questionnaire and of the number of participants.
These are often due to travel costs and can account for a substantive fraction of the
total interview costs. Suppose the variable component of the interview costs is linear
so that total interview costs can be written as

cinterv(S, n) := nη + np T (S),

(10)

where T (S) should now be interpreted as the average time spent per interview, and p is
the average price of one unit of survey time. We employ the speciﬁcation (7) with (8)–
(10) when studying the impact of free day-care on child development in Section V.A.

Remark 3. Because we always collect the outcome variable, we incur the ﬁxed costs
nη and the variable costs npτ0 even when no covariates are collected.

Remark 4. Non-ﬁnancial costs are diﬃcult to model, but could in principle be added.
They are primarily related to the impact of sample and survey size on data quality.
For example, if we design a survey that takes more than four hours to complete, the
quality of the resulting data is likely to be aﬀected by interviewer and interviewee
fatigue. Similarly, conducting the training of enumerators becomes more diﬃcult as
the survey size grows. Hiring high-quality enumerators may be particularly important
in that case, which could result in even higher costs (although this latter observation
could be explicitly considered in our framework).

17

IV.C Clusters

In many experiments, randomization is carried out at a cluster level (e.g., school level),
rather than at an individual level (e.g., student level). In this case, training costs may
depend not only on the ultimate sample size n = c nc, where c and nc denote the
number of clusters and the number of participants per cluster, respectively, but on a
particular combination (c, nc), because the number of required enumerators may be
diﬀerent for diﬀerent (c, nc) combinations. Therefore, training costs (which now also
depend on c and nc) may be modeled as

ctrain(S, nc, c) := κ(c, nc) T (S).

(11)

The interaction of cluster and sample size in determining the number of required enu-
merators and, thus, the quantity κ(c, nc), complicates the modeling of this quantity
relative to the case without clustering. Let µ(c, nc) denote the number of required sur-
vey enumerators for c clusters of size nc. As in the case without clustering, we assume
that the training costs is lumpy in the number of enumerators used:

 κ1

κ2

κ(c, nc) :=

0 < µ(c, nc) ≤ µ1
if
if µ1 < µ(c, nc) ≤ µ2
...

.

The number of enumerators required, µ(c, nc), may also be lumpy in the number of
interviewees per cluster, nc, because there is a lower and an upper bound to how many
interviews each enumerator can carry out. Also, the number of enumerators needed for
the survey typically increases in the number of clusters in the experiment. Therefore,
we model µ(c, nc) as

µ(c, nc) := (cid:98)µc(c) · µn(nc)(cid:99),

where (cid:98)·(cid:99) denotes the integer part, µc(c) := λc for some constant λ (i.e., µc(c) is
assumed to be linear in c), and

 µn,1

µn,2

µn(nc) :=

0 < nc ≤ n1
if
if n1 < nc ≤ n2
...

.

In addition, while the variable interview costs component continues to depend on
the overall sample size n as in (10), the ﬁxed part of the interview costs is determined

18

by the number of clusters c rather than by n. Therefore, the total costs per interview
become

cinterv(S, nc, c) := ψ(c)η + cncp T (S),

(12)

where ψ(c) is some function of the number of clusters c.

IV.D Covariates with Heterogeneous Prices

In randomized experiments, the data collection process often diﬀers across blocks of
covariates. For example, the researcher may want to collect outcomes of psychological
tests for the members of the household that is visited. These tests may need to be
administered by trained psychologists, whereas administering a questionnaire about
background variables such as household income, number of children, or parental ed-
ucation, may not require any particular set of skills or qualiﬁcations other than the
training provided as part of the data collection project.

Partition the covariates into two blocks, a high-cost block (e.g., outcomes of psycho-
logical tests) and a low-cost block (e.g., standard questionnaire). Order the covariates
such that the ﬁrst Mlow covariates belong to the low-cost block, and the remaining
Mhigh := M − Mlow together with the outcome variable belong to the high-cost block.
Let

Mlow(cid:88)

M(cid:88)

τjSj

Tlow(S) :=

τjSj

and

Thigh(S) := τ0 +

j=1

j=Mlow+1

be the total time costs per individual of surveying all low-cost and high-cost covariates,
respectively. Then, the total time costs for all variables can be written as T (S) =
Tlow(S) + Thigh(S).

Because we require two types of enumerators, one for the high-cost covariates and
one for the low-cost covariates, the ﬁnancial costs of each interview (ﬁxed and variable)
may be diﬀerent for the two blocks of covariates. Denote these by ψlow(c, nc)ηlow +
cncplowTlow(S) and ψhigh(c, nc)ηhigh + cncphighThigh(S), respectively.

The ﬁxed costs for the high-cost block are incurred regardless of whether high-
cost covariates are selected or not, because we always collect the outcome variable,
which here is assumed to belong to this block. The ﬁxed costs for the low-cost block,
however, are incurred only when at least one low-cost covariate is selected (i.e., when
j=1 Sj > 0). Therefore, the total interview costs for all covariates can be written as

(cid:80)Mlow

19

(cid:110) Mlow(cid:88)

cinterv(S, n) := 1

j=1

(cid:111)

Sj > 0

(ψlow(c, nc)ηlow + cncplowTlow(S))

+ ψhigh(c, nc)ηhigh + cncphighThigh(S).

(13)

The administration and training costs can also be assumed to diﬀer for the two types
of enumerators. In that case,

cadmin(S) := φlowTlow(S)αlow + φhighThigh(S)αhigh,
ctrain(S, n) := κlow(c, nc) Tlow(S) + κhigh(c, nc) Thigh(S).

(14)

(15)

We employ speciﬁcation (7) with (12)–(15) when, in Section V.B, we study the impact
on student learning of cash grants which are provided to schools.

V Empirical Applications

V.A Access to Free Day-Care in Rio

In this section, we re-examine the experimental design of Attanasio et al. (2014), who
evaluate the impact of access to free day-care on child development and household
resources in Rio de Janeiro. In their dataset, access to care in public day-care centers,
most of which are located in slums, is allocated through a lottery, administered to
children in the waiting lists for each day-care center.

Just before the 2008 school year, children applying for a slot at a public day-care
center were put on a waiting list. At this time, children were between the ages of
0 and 3. For each center, when the demand for day-care slots in a given age range
exceeded the supply, the slots were allocated using a lottery (for that particular age
range). The use of such an allocation mechanism means that we can analyze this
intervention as if it was a RCT, where the oﬀer of free day-care slots is randomly
allocated across potentially eligible recipients. Attanasio et al. (2014) compare the
outcomes of children and their families who were awarded a day-care slot through the
lottery, with the outcomes of those not awarded a slot.

The data for the study were collected mainly during the second half of 2012, four
and a half years after the randomization took place. Most children were between the
ages of 5 and 8. A survey was conducted, which had two components: a household
questionnaire, administered to the mother or guardian of the child; and a battery of
health and child development assessments, administered to children. Each household

20

was visited by a team of two ﬁeld workers, one for each component of the survey.

The child assessments took a little less than one hour to administer, and included
ﬁve tests per child, plus the measurement of height and weight. The household survey
took between one and a half and two hours, and included about 190 items, in addition
to a long module asking about day-care history, and the administration of a vocabulary
test to the main carer of each child.

As we explain below, we use the original sample, with the full set of items collected
in the survey, to calibrate the cost function for this example. However, when solving
the survey design problem described in this paper we consider only a subset of items
of these data, with the original budget being scaled down properly. This is done for
simplicity, so that we can essentially ignore the fact that some variables are missing
for part of the sample, either because some items are not applicable to everyone in
the sample, or because of item non-response. We organize the child assessments into
three indices: cognitive tests, executive function tests, and anthropometrics (height and
weight). These three indices are the main outcome variables in the analysis. However,
we use only the cognitive tests and anthropometrics indices in our analysis, as we have
fewer observations for executive function tests.

We consider only 40 covariates out of the total set of items on the questionnaire.
The variables not included can be arranged into four groups: (i) variables that can be
seen as ﬁnal outcomes, such as questions about the development and the behavior of
the children in the household; (ii) variables that can be seen as intermediate outcomes,
such as labor supply, income, expenditure, and investments in children; (iii) variables
for which there is an unusually large number of missing values; and (iv) variables that
are either part of the day-care history module, or the vocabulary test for the child’s
carer (because these could have been aﬀected by the lottery assigning children to day-
care vacancies). We then drop four of the 40 covariates chosen, because their variance is
zero in the sample. The remaining M = 36 covariates are related to the respondent’s
age, literacy, educational attainment, household size, safety, burglary at home, day
care, neighborhood, characteristics of the respondent’s home and its surroundings (the
number of rooms, garbage collection service, water ﬁlter, stove, refrigerator, freezer,
washer, TV, computer, Internet, phone, car, type of roof, public light in the street,
pavement, etc.). We drop individuals for whom at least one value in each of these
covariates is missing, which leads us to use a subsample with 1,330 individuals from
the original experimental sample, which included 1,466 individuals.

21

Calibration of the cost function. We specify the cost function (7) with compo-
nents (8)–(10) to model the data collection procedure as implemented in Attanasio et
al. (2014). We calibrate the parameters using the actual budgets for training, admin-
istrative, and interview costs in the authors’ implementation. The contracted total
budget of the data collection process was R$665,000.5

For the calibration of the cost function, we use the originally planned budget of
R$665,000, and the original sample size of 1,466. As mentioned above, there were
190 variables collected in the household survey, together with a day-care module and
In total, this translates into a total of roughly 240 variables.6
a vocabulary test.
Appendix B provides a detailed description of all components of the calibrated cost
function.

Implementation.
In implementing the OGA, we take each single variable as a pos-
sible group (i.e., each group consists of a singleton set). We studentized all covariates
to have variance one. To compare the OGA with alternative approaches, we also con-
sider LASSO and POST-LASSO for the inner optimization problem in Step 2 of our
procedure. The LASSO solves

N(cid:88)

min

γ

1
N

(cid:88)

(Yi − γ(cid:48)Xi) + λ

|γj|

(16)

i=1

j

with a tuning parameter λ > 0. The POST-LASSO procedure runs an OLS regression
of Yi on the selected covariates (non-zero entries of γ) in (16). Belloni and Cher-
nozhukov (2013), for example, provide a detailed description of the two algorithms. It
is known that LASSO yields biased regression coeﬃcient estimates and that POST-
LASSO can mitigate this bias problem. Together with the outer optimization over the
sample size using the LASSO or POST-LASSO solutions in the inner loop may lead
to diﬀerent selections of covariate-sample size combinations. This is because POST-
LASSO re-estimates the regression equation which may lead to more precise estimates
of γ and thus result in a diﬀerent estimate for the MSE of the treatment eﬀect estima-
tor.

5There were some adjustments to the budget during the period of ﬁeldwork.
6The budget is for the 240 variables (or so) actually collected. In spite of that, we only use 36 of
these as covariates in this paper, as the remaining variables in the survey were not so much covariates
as they were measuring other intermediate and ﬁnal outcomes of the experiment, as we have explained
before. The actual budget used in solving the survey design problem is scaled down to match the use
of only 36 covariates.

22

In both LASSO implementations, the penalization parameter λ is chosen so as to
satisfy the budget constraint as close to equality as possible. We start with a large
value for λ, which leads to a large penalty for non-zero entries in γ, so that few or
no covariates are selected and the budget constraint holds. Similarly, we consider a
very small value for λ which leads to the selection of many covariates and violation of
the budget. Then, we use a bisection algorithm to ﬁnd the λ-value in this interval for
which the budget is satisﬁed within some pre-speciﬁed tolerance.

Table 1: Day-care (outcome: cognitive test)

ˆn

Method
1,330
Experiment
2,677
OGA
LASSO
2,762
POST-LASSO 2,677

1

| ˆI| Cost/B
36
1
0
1

0.9939
0.99475
0.9939

EQB

RMSE
0.025285 R$562,323
0.018776 R$312,363
0.018789 R$313,853
0.018719 R$312,363

Relative EQB

1

0.555
0.558
0.555

Table 2: Day-care (outcome: health assessment)

ˆn

Method
1,330
Experiment
2,762
OGA
LASSO
2,762
POST-LASSO 2,677

1

| ˆI| Cost/B
36
0
0
1

0.99475
0.99475
0.9939

EQB

RMSE
0.025442 R$562,323
0.018799 R$308,201
0.018799 R$308,201
0.018735 R$306,557

Relative EQB

1

0.548
0.548
0.545

Results. Tables 1 and 2 summarize the results of the covariate selection procedures.
For the cognitive test outcome, OGA and POST-LASSO select one covariate (“| ˆI|”),7
whereas LASSO does not select any covariate. The selected sample sizes (“ˆn”) are 2,677
for OGA and POST-LASSO, and 2,762 for LASSO, which are almost twice as large
as the actual sample size in the experiment. The performance of the three covariate
selection methods in terms of the precision of the resulting treatment eﬀect estimator is
measured by the square-root value of the minimized MSE criterion function (“RMSE”)

7For OGA, it is an indicator variable whether the respondent has ﬁnished secondary education,
which is an important predictor of outcomes; for POST-LASSO, it is the number of rooms in the
house, which can be considered as a proxy for wealth of the household, and again, an important
predictor of outcomes.

23

from Step 2 of our procedure. The three methods perform similarly well and improve
precision by about 25% relative to the experiment. Also, all three methods manage to
exhaust the budget, as indicated by the cost-to-budget ratios (“Cost/B”) close to one.
We do not put any strong emphasis on the selected covariates as the improvement of
the criterion function is minimal relative to the case that no covariate is selected (i.e.,
the selection with LASSO). The results for the health assessment outcome are very
similar to those of the cognitive test with POST-LASSO selecting one variable (the
number of rooms in the house), whereas OGA and LASSO do not select any covariate.
To assess the economic gain of having performed the covariate selection procedure
after the ﬁrst wave, we include the column “EQB” (abbreviation of “equivalent bud-
get”) in Tables 1 and 2. The ﬁrst entry of this column in Table 1 reports the budget
necessary for the selection of ˆn = 1,330 and all covariates, as was carried out in the
experiment. For the three covariate selection procedures, the column shows the budget
that would have suﬃced to achieve the same precision as the actual experiment in
terms of the minimum value of the MSE criterion function in Step 2. For example, for
the cognitive test outcome, using the OGA to select the sample size and the covari-
ates, a budget of R$312,363 would have suﬃced to achieve the experimental RMSE of
0.025285. This is a huge reduction of costs by about 45 percent, as shown in the last
column called “relative EQB”. Similar reductions in costs are possible when using the
LASSO procedures and also when considering the health assessment outcome.

Appendix D presents the results of Monte Carlo simulations that mimic this dataset,
and shows that all three methods select more covariates and smaller sample sizes as we
increase the predictive power of some covariates. This ﬁnding suggests that the covari-
ates collected in the survey were not predicting the outcome very well and, therefore,
in the next wave the researcher should spend more of the available budget to collect
data on more individuals, with no (or only a minimal) household survey. Alternatively,
the researcher may want to redesign the household survey to include questions whose
answers are likely better predictors of the outcome.

V.B Provision of School Grants in Senegal

In this subsection, we consider the study by Carneiro et al. (2015) who evaluate, using
a RCT, the impact of school grants on student learning in Senegal. The authors
collect original data not only on the treatment status of schools (treatment and control)
and on student learning, but also on a variety of household, principal, and teacher
characteristics that could potentially aﬀect learning.

24

The dataset contains two waves, a baseline and a follow-up, which we use for the
study of two diﬀerent hypothetical scenarios. In the ﬁrst scenario, the researcher has
access to a pre-experimental dataset consisting of all outcomes and covariates collected
in the baseline survey of this experiment, but not the follow-up data. The researcher
applies the covariate selection procedure to this pre-experimental dataset to ﬁnd the
optimal sample size and set of covariates for the randomized control trial to be carried
out after the ﬁrst wave. In the second scenario, in addition to the pre-experimental
sample from the ﬁrst wave the researcher now also has access to the post-experimental
outcomes collected in the follow-up (second wave). In this second scenario, we treat
the follow-up outcomes as the outcomes of interest and include baseline outcomes in
the pool of covariates that predict follow-up outcomes.

As in the previous subsection, we calibrate the cost function based on the full
dataset from the experiment, but for solving the survey design problem we focus on
a subset of individuals and variables from the original questionnaire. For simplicity,
we exclude all household variables from the analysis, because they were only collected
for 4 out of the 12 students tested in each school, and we remove covariates whose
sample variance is equal to zero. Again, for simplicity, of the four outcomes (math
test, French test, oral test, and receptive vocabulary) in the original experiment, we
only consider the ﬁrst one (math test) as our outcome variable. We drop individuals
for whom at least one answer in the survey or the outcome variable is missing. This
sample selection procedure leads to sample sizes of N = 2, 280 for the baseline math
test outcome. For the second scenario discussed above where we use also the follow-up
outcome, the sample size is smaller (N = 762) because of non-response in the follow-up
outcome and because we restrict the sample to the control group of the follow-up. In the
ﬁrst scenario in which we predict the baseline outcome, dropping household variables
reduces the original number of covariates in the survey from 255 to M = 142. The
remaining covariates are school- and teacher-level variables.
In the second scenario
in which we predict follow-up outcomes, we add the three baseline outcomes to the
covariate pool, but at the same time remove two covariates because they have variance
zero when restricted to the control group. Therefore, there are M = 143 covariates in
the second scenario.

Calibration of the cost function. We specify the cost function (7) with compo-
nents (13)–(15) to model the data collection procedure as implemented in Carneiro et
al. (2015). Each school forms a cluster. We calibrate the parameters using the costs
faced by the researchers and their actual budgets for training, administrative, and in-

25

terview costs. The total budget for one wave of data collection in this experiment,
excluding the costs of the household survey, was approximately $192,200.

For the calibration of the cost function, we use the original sample size, the original
number of covariates in the survey (except those in the household survey), and the orig-
inal number of outcomes collected at baseline. The three baseline outcomes were much
more expensive to collect than the remaining covariates. In the second scenario, we
therefore group the former together as high-cost variables, and all remaining covariates
as low-cost variables. Appendix B provides a detailed description of all components of
the calibrated cost function.

Implementation. The implementation of the covariate selection procedures is iden-
tical to the one in the previous subsection except that we consider here two diﬀerent
speciﬁcations of the pre-experimental sample Spre, depending on whether the outcome
of interest is the baseline or follow-up outcome.

Results. Table 3 summarizes the results of the covariate selection procedures. Panel
(a) shows the results of the ﬁrst scenario in which the baseline math test is used as the
outcome variable to be predicted. Panel (b) shows the corresponding results for the
second scenario in which the baseline outcomes are treated as high-cost covariates and
the follow-up math test is used as the outcome to be predicted.

For the baseline outcome in panel (a), the OGA selects only | ˆI| = 14 out of the 145
covariates with a selected sample size of ˆn = 3, 018, which is about 32% larger than the
actual sample size in the experiment. The results for the LASSO and POST-LASSO
methods are similar. As in the previous subsection, we measure the performance of the
three covariate selection methods by the estimated precision of the resulting treatment
eﬀect estimator (“RMSE”). The three methods improve the precision by about 7%
relative to the experiment. Also, all three methods manage to essentially exhaust
the budget, as indicated by cost-to-budget ratios (“Cost/B”) close to one. As in the
previous subsection, we measure the economic gains from using the covariate selection
procedures by the equivalent budget (“EQB”) that each of the method requires to
achieve the precision of the experiment. All three methods require equivalent budgets
that are 7-9% lower than that of the experiment.

All variables that the OGA selects as strong predictors of baseline outcome are
plausibly related to student performance on a math test:8 They are related to important

8Online Appendix E shows the full list and deﬁnitions of selected covariates for the baseline out-

come.

26

Table 3: School grants (outcome: math test)

Method

ˆn

| ˆI| Cost/B

RMSE

EQB

Relative EQB

(a) Baseline outcome

2,280
experiment
3,018
OGA
LASSO
2,985
POST-LASSO 2,985

142
14
18
18

1

0.99966
0.99968
0.99968

0.0042272
0.003916
0.0039727
0.0038931

$30,767
$28,141
$28,669
$27,990

762
experiment
6,755
OGA
LASSO
6,755
POST-LASSO 6,755

143
0
0
0

1

(b) Follow-up outcome
0.0051298
0.0027047
0.0027047
0.0027047

0.99961
0.99961
0.99961

$52,604
$22,761
$22,761
$22,761

(c) Follow-up outcome, no high-cost covariates

762
experiment
5,411
OGA
LASSO
5,444
POST-LASSO 6,197

143
140
136
43

1

0.99879
0.99908
0.99933

0.0051298
0.0024969
0.00249
0.0024624

$52,604
$21,740
$22,082
$21,636

(d) Follow-up outcome, force baseline outcome

762
experiment
1,314
OGA
LASSO
2,789
POST-LASSO 2,789

143
133
1
1

1

0.99963
0.9929
0.9929

0.0051298
0.0040293
0.0043604
0.0032823

$52,604
$41,256
$42,815
$32,190

1

0.91
0.93
0.91

1

0.43
0.43
0.43

1

0.41
0.42
0.41

1

0.78
0.81
0.61

aspects of the community surrounding the school (e.g., distance to the nearest city),
school equipment (e.g., number of computers), school infrastructure (e.g., number of
temporary structures), human resources (e.g., teacher–student ratio, teacher training),
and teacher and principal perceptions about which factors are central for success in
the school and about which factors are the most important obstacles to school success.
For the follow-up outcome in panel (b), the budget used in the experiment increases
due to the addition of the three expensive baseline outcomes to the pool of covariates.
All three methods select no covariates and exhaust the budget by using the maximum
feasible sample size of 6,755, which is almost nine times larger than the sample size
in the experiment. The implied precision of the treatment eﬀect estimator improves
by about 47% relative to the experiment, which translates into the covariate selection

27

methods requiring less than half of the experimental budget to achieve the same pre-
cision as in the experiment. These are substantial statistical and economic gains from
using our proposed procedure.

Sensitivity Checks.
In RCT’s, baseline outcomes tend to be strong predictors of
the follow-up outcome. One may therefore be concerned that, because the OGA ﬁrst
selects the most predictive covariates which in this application are also much more
expensive than the remaining low-cost covariates, the algorithm never examines what
would happen to the estimator’s MSE if it ﬁrst selects the most predictive low-cost
covariates instead. In principle, such selection could lead to a lower MSE than any
selection that includes the very expensive baseline outcomes. As a sensitivity check
we therefore perform the covariate selection procedures on the pool of covariates that
excludes the three baseline outcomes. Panel (c) shows the corresponding results. In
this case, all methods indeed select more covariates and smaller sample sizes than
in panel (b), and achieve a slightly smaller MSE. The budget reductions relative to
the experiment as measured by EQB are also almost identical to those in panel (b).
Therefore, both selections of either no covariates and large sample size (panel (b))
and many low-cost covariates with somewhat smaller sample size (panel (c)) yield
very similar and signiﬁcant improvements in precision or signiﬁcant reductions in the
experimental budget, respectively.9

As discussed in Section II.A, one may want to ensure balance of the control and
treatment group, especially in terms of strong predictors such as baseline outcomes.
Checking balance requires collection of the relevant covariates. Therefore, we also
perform the three covariate selection procedures when we force each of them to include
the baseline math outcome as a covariate. In the OGA, we can force the selection of
a covariate by performing group OGA as described in Section III, where each group
contains a low-cost covariate together with the baseline math outcome. For the LASSO
procedures, we simply perform the LASSO algorithms after partialing out the baseline
math outcome from the follow-up outcome. The corresponding results are reported in
panel (d). Since baseline outcomes are very expensive covariates, the selected sample
sizes relative to those in panels (b) and (c) are much smaller. OGA selects a sample
size of 1,314 which is almost twice as large as the experimental sample size, but about

9Note that there is no sense in which need to be concerned about identiﬁcation of the minimizing
set of covariates. There may indeed exist several combinations of covariates that yield similar precision
of the resulting treatment eﬀect estimator. Our objective is highest possible precision without any
direct interest in the identities of the covariates that achieve that minimum.

28

4-5 times smaller than the OGA selections in panels (b) and (c). In contrast to OGA,
the two LASSO procedures do not select any other covariates beyond the baseline math
outcome. As a result of forcing the selection of the baseline outcome, all three methods
achieve an improvement in precision, or reduction of budgets respectively, of around
20% relative to the experiment. These are still substantial gains, but the requirement
of checking balance on the expensive baseline outcome comes at the cost of smaller
improvements in precision due to our procedure.

VI Relation to the Existing Literature

In this section, we discuss related papers in the literature. We emphasize that the
research question in our paper is diﬀerent from those studied in the literature and that
our paper is a complement to the existing work.

In the context of experimental economics, List, Sadoﬀ, and Wagner (2011) suggest
several simple rules of thumb that researchers can apply to improve the eﬃciency of
their experimental designs. They discuss the issue of experimental costs and estimation
eﬃciency but did not consider the problem of selecting covariates.

Hahn, Hirano, and Karlan (2011) consider the design of a two-stage experiment
for estimating an average treatment eﬀect, and proposed to select the propensity score
that minimizes the asymptotic variance bound for estimating the average treatment
eﬀect. Their recommendation is to assign individuals randomly between the treatment
and control groups in the second stage, according to the optimized propensity score.
They use the covariate information collected in the ﬁrst stage to compute the optimized
propensity score.

Bhattacharya and Dupas (2012) consider the problem of allocating a binary treat-
ment under a budget constraint. Their budget constraint limits what fraction of the
population can be treated, and hence is diﬀerent from our budget constraint. They
discuss the costs of using a large number of covariates in the context of treatment
assignment.

McKenzie (2012) demonstrates that taking multiple measurements of the outcomes
after an experiment can improve power under the budget constraint. His choice problem
is how to allocate a ﬁxed budget over multiple surveys between a baseline and follow-
ups. The main source of the improvement in his case comes from taking repeated
measures of outcomes; see Frison and Pocock (1992) for this point in the context
of clinical trials. In the set-up of McKenzie (2012), a baseline survey measuring the

29

outcome is especially useful when there is high autocorrelation in outcomes. This would
be analogous in our paper to devoting part of the budget to the collection of a baseline
covariate, which is highly correlated with the outcome (in this case, the baseline value
of the outcome), instead of just selecting a post-treatment sample size that is as large
as the budget allows for. In this way, McKenzie (2012) is perhaps closest to our paper
in spirit.

In a very recent paper, Dominitz and Manski (2016) proposed the use of statistical
decision theory to study allocation of a predetermined budget between two sampling
processes of outcomes: a high-cost process of good data quality and a low-cost process
with non-response or low-resolution interval measurement of outcomes. Their main
concern is data quality between two sampling processes and is distinct from our main
focus, namely the simultaneous selection of the set of covariates and the sample size.

VII Concluding Remarks

We develop data-driven methods for designing a survey in a randomized experiment
using information from a pre-existing dataset. Our procedure is optimal in a sense that
it minimizes the mean squared error of the average treatment eﬀect, and can handle a
large number of potential covariates as well as complex budget constraints faced by the
researcher. We have illustrated the usefulness of our approach by showing substantial
improvements in precision of the resulting estimator or substantial reductions in the
researcher’s budget in two empirical applications.

Some important issues remain as interesting future research topics. First, in prin-
ciple, one could also consider the optimization of other criteria, e.g., the joint mini-
mization of type-I and type-II errors of a test of the null hypothesis of no-treatment
eﬀect. Second, we have assumed that the pre-experimental sample Spre is large, and
therefore the diﬀerence between the minimization of the sample average and that of the
population expectation is negligible. However, if the sample size of Spre is small (e.g.,
in a pilot study), one may be concerned about over-ﬁtting, in the sense of selecting too
many covariates. A straightforward solution would be to add a term to the objective
function that penalizes a large number of covariates via some information criteria (e.g.,
the Akaike information criterion (AIC) or the Bayesian information criterion (BIC)).

30

References

Attanasio, Orazio, Ricardo Paes de Barros, Pedro Carneiro, David Evans,
Lycia Lima, Rosane Mendonca, Pedro Olinto, and Norbert Schady. 2014.
“Free Access to Child Care, Labor Supply, and Child Development.” Discussion
paper.

Bandiera, Oriana, Iwan Barankay, and Imran Rasul. 2011. “Field Experiments

with Firms.” Journal of Economic Perspectives, 25(3), 63–82.

Banerjee, Abhijit V., and Esther Duﬂo. 2009. “The Experimental Approach to

Development Economics.” Annual Review of Economics, 1(1), 151–78.

Barron, Andrew R., Albert Cohen, Wolfgang Dahmen, and Ronald A. De-
Vore. 2008. “Approximation and Learning by Greedy Algorithms.” Annals of Statis-
tics, 36(1), 64–94.

Belloni, Alexandre, and Victor Chernozhukov. 2013. “Least Squares after Model

Selection in High-Dimensional Sparse Models.” Bernoulli, 19(2), 521–47.

Belloni, Alexandre, Victor Chernozhukov, and Christian Hansen. 2014.
“High-Dimensional Methods and Inference on Structural and Treatment Eﬀects.”
Journal of Economic Perspectives, 28(2), 29–50.

Bhattacharya, Debopam, and Pascaline Dupas. 2012. “Inferring Welfare Maxi-
mizing Treatment Assignment under Budget Constraints.” Journal of Econometrics,
167(1), 168–96.

Bruhn, Miriam, and David McKenzie. 2009. “In Pursuit of Balance: Randomiza-
tion in Practice in Development Field Experiments.” American Economic Journal:
Applied Economics, 1(4), 200–32.

Carneiro, Pedro, Oswald Koussihou`ed´e, Nathalie Lahire, Costas Meghir,
and Corina Mommaerts. 2015. “Decentralizing Education Resources: School
Grants in Senegal.” NBER Working Paper 21063.

Coﬀman, Lucas C., and Muriel Niederle. 2015. “Pre-analysis Plans Have Limited
Upside, Especially Where Replications Are Feasible.” Journal of Economic Perspec-
tives, 29(3), 81–98.

Davis, Geoﬀrey, St´ephane Mallat, and Marco Avellaneda. 1997. “Adaptive

Greedy Approximations.” Constructive Approximation, 13(1), 57–98.

Dominitz, Jeﬀ, and Charles F. Manski. 2016. “MORE DATA OR BETTER

DATA? A Statistical Decision Problem.” Working Paper.

31

Duﬂo, Esther, Rachel Glennerster, and Michael Kremer. 2007. “Using Ran-
domization in Development Economics Research: A Toolkit.” In Handbook of De-
velopment Economics, Volume 4, ed. T. Paul Schults, and John Strauss, 3895–962.
Amsterdam: Elsevier.

Fisher, Ronald A. 1935. The Design of Experiments. Edinburgh: Oliver and Boyd.

Freedman, David A. 2008a. “On Regression Adjustments in Experiments with Sev-

eral Treatments.” Annals of Applied Statistics, 2(1), 176–96.

2008b. “On Regression Adjustments to Experimental Data.” Advances in

Applied Math, 40(2), 180–93.

Frison, Lars, and Stuart J. Pocock. 1992. “Repeated Measures in Clinical Trials:
Analysis Using Mean Summary Statistics and its Implications for Design.” Statistics
in Medicine, 11, 1685–704.

Hahn, Jinyong, Keisuke Hirano, and Dean Karlan. 2011. “Adaptive Experimen-
tal Design Using the Propensity Score.” Journal of Business and Economic Statistics,
29(1), 96–108.

Hamermesh, Daniel S. 2013. “Six Decades of Top Economics Publishing: Who and

How?” Journal of Economic Literature, 51(1), 162–72.

Huang, Junzhou, Tong Zhang, and Dimitris Metaxas. 2011. “Learning with

Structured Sparsity.” Journal of Machine Learning Research, 12, 3371–412.

Imbens, Guido W. and Donald B. Rubin 2015. Causal Inference for Statistics,
Social, and Biomedical Sciences: An Introduction. New York: Cambridge University
Press.

Ing, Ching-Kang, and Tze Leung Lai. 2011. “A Stepwise Regression Method and
Consistent Model Selection for High-Dimensional Sparse Linear Models.” Statistica
Sinica, 21(4), 1473–513.

Kleinberg, Jon, Jens Ludwig, Sendhil Mullainathan, and Ziad Obermeyer.

2015. “Prediction Policy Problems.” American Economic Review, 105(5), 491–95.

List, John, Sally Sadoﬀ, and Mathis Wagner. 2011. “So You Want to Run an
Experiment, Now What? Some Simple Rules of Thumb for Optimal Experimental
Design.” Experimental Economics, 14(4), 439–57.

List, John A. 2011. “Why Economists Should Conduct Field Experiments and 14

Tips for Pulling One Oﬀ.” Journal of Economic Perspectives, 25(3), 3–16.

List, John A., and Imran Rasul. 2011. “Field Experiments in Labor Economics.” In
Handbook of Labor Economics, Volume 4A, ed. Orley Ashenfelter, and David Card,
103–228. Amsterdam: Elsevier.

32

McConnell, Brendon, and Marcos Vera-Hern´andez. 2015. “Going Beyond Sim-
ple Sample Size Calculations: A Practitioner’s Guide.” Institute for Fiscal Studies
(IFS) Working Paper W15/17.

McKenzie, David. 2012. “Beyond Baseline and Follow-up: The Case for More T in

Experiments.” Journal of Development Economics, 99(2), 210–21.

Morgan, K. L., and D. B. Rubin 2012. “Rerandomization to improve covariate

balance in experiments,” The Annals of Statistics, 40(2), 1263–1282.

Morgan, K. L., and D. B. Rubin 2015: “Rerandomization to Balance Tiers of
Covariates,” Journal of the American Statistical Association, 110(512), 1412–1421.

Natarajan, Balas K. 1995. “Sparse Approximate Solutions to Linear Systems.”

SIAM Journal on Computing, 24(2), 227–34.

Olken, Benjamin A. 2015. “Promises and Perils of Pre-analysis Plans.” Journal of

Economic Perspectives, 29(3), 61–80.

Sancetta, Alessio. 2016. “Greedy Algorithms for Prediction.” Bernoulli, 22(2), 1227–

77.

Temlyakov, Vladimir N. 2011. Greedy Approximation. Cambridge: Cambridge Uni-

versity Press.

Tropp, Joel A. 2004. “Greed is Good: Algorithmic Results for Sparse Approxima-

tion.” IEEE Transactions on Information Theory, 50(10), 2231–42.

Tropp, Joel A., and Anna C. Gilbert. 2007. “Signal Recovery from Random Mea-
surements via Orthogonal Matching Pursuit.” IEEE Transactions on Information
Theory, 53(12), 4655–66.

Zhang, Tong. 2009. “On the Consistency of Feature Selection Using Greedy Least

Squares Regression.” Journal of Machine Learning Research, 10, 555–68.

33

Appendix A: Large-Budget Properties of the Algo-

rithm

In this appendix, we provide non-asymptotic bounds on the empirical risk of the OGA
approximation ˆf := Zˆγ. Following Barron, Cohen, Dahmen, and DeVore (2008), we
deﬁne

|βk|2 : βk ∈ R|Gk| and f =

X(cid:48)

Gk

βk

p(cid:88)

k=1

(cid:111)

.

(cid:110) p(cid:88)
When the expression f =(cid:80)p

(cid:107)f(cid:107)LN

:= inf

k=1

1

k=1 X(cid:48)
. This gives f := γ(cid:48)

βk is not unique, we take the true f to be one with
the minimum value of (cid:107)f(cid:107)LN
0X and f := Xγ0 for some γ0. Note
that f is deﬁned by X with the true parameter value γ0, while ˆf is an OGA estimator
of f using only Z. The following theorem bounds the ﬁnite sample approximation to
the MSE of the treatment eﬀect estimator

Gk

1

(cid:92)M SE ˆn,N (ˆf ) := (cid:107)Y − ˆf(cid:107)2

N /ˆn,

which is equal to the objective function in (5). Note that (cid:92)M SE ˆn,N (ˆf ) can also be called
the “empirical risk”.

The following theorem is a modiﬁcation of Theorem 2.3 of Barron, Cohen, Dahmen,
and DeVore (2008). Our result is diﬀerent from Barron, Cohen, Dahmen, and DeVore
(2008) in two respects: (i) we pay explicit attention to the group structure, and (ii)
our budget constraint is diﬀerent from their termination rule.
XGj )/N = I|Gj| for each j = 1, . . . , p. Suppose N is
Theorem A.1. Assume that (X(cid:48)
a ﬁnite subset of N+, c : {0, 1}M × N+ → R some function, and B > 0 some constant.
Then the following bound holds:

Gj

(cid:92)M SE ˆn,N (ˆf ) − (cid:92)M SE ˆn,N (f ) ≤ 4(cid:107)f(cid:107)2LN

1

ˆn

1

min{p, kˆn}

.

(A.1)

(cid:18)

(cid:19)

The theorem provides a non-asymptotic bound on the empirical risk of the OGA
approximation, but the bound also immediately yields asymptotic consistency in the
following sense. Suppose (cid:107)f(cid:107)LN
< ∞ (Remark A.1 discusses this condition). Then,
the empirical risk of ˆf is asymptotically equivalent to that of the true predictor f either
if the selected sample size ˆn → ∞ or if both the total number of groups p and the
number of selected groups kˆn diverge to inﬁnity. Consider, for example, the simple
case in which, for a given sample size n, data collection on every covariate incurs the

1

A-1

same costs (i.e., (cid:101)c(n)) and each group consists of a single covariate. Then the total
data collection costs are equal to the number of covariates selected multiplied by(cid:101)c(n)
(i.e., c(S, n) =(cid:101)c(n)(cid:80)M
j=1 Sj). Assuming that(cid:101)c(n) is non-decreasing in n, we then have

1

ˆn min{p, kˆn} =

1

ˆn min{M, ˆm} =

ˆn min{M,(cid:98)B/(cid:101)c(ˆn)(cid:99)},

1

where (cid:98)x(cid:99) denotes the largest integer smaller than x. Therefore, we obtain consistency

if ˆnM →p ∞ and ˆnB/(cid:101)c(ˆn) →p ∞. Continue to assume that (cid:101)c(n) is increasing in n

and N contains sample sizes bounded away from zero. Then, both rate conditions are
satisﬁed, for example, as the budget increases, B → ∞, and the costs per covariate
does not increase faster than linearly in the sample size.10 Note that consistency can
hold irrespectively of whether the number of covariates M is ﬁnite or inﬁnite.
Remark A.1. The condition (cid:107)f(cid:107)LN
< ∞ is trivially satisﬁed when p is ﬁnite. In the
case p → ∞, the condition (cid:107)f(cid:107)LN
< ∞ requires that not all groups of covariates
(cid:80)∞
are equally important in the sense that the coeﬃcients βk, when their (cid:96)2 norms are
sorted in decreasing order, need to converge to zero fast enough to guarantee that
k=1 |βk|2 < ∞.

1

1

1

< ∞ by its population counterpart.

Remark A.2. If suitable laws of large numbers apply, we can also replace the condition
(cid:107)f(cid:107)LN
Remark A.3. The minimal sample size n0 in N could, for example, be determined
by power calculations (see, e.g. Duﬂo, Glennerster, and Kremer, 2007; McConnell and
Vera-Hern´andez, 2015) that guarantee a certain power level for an hypothesis test of
β = 0.

Proofs
Proof of Lemma 1. Let Ui(γ) := Yi−γ(cid:48)Xi−β0Di. The homoskedastic error assumption
implies that conditional on D1, . . . , Dn, the ﬁnite-sample MSE of ˆβ(γ) is

(cid:16) ˆβ(γ)

Var

(cid:12)(cid:12)(cid:12) D1, . . . , Dn

(cid:17)

Var (Ui(γ) | D1, . . . , Dn)

=

1
n

(cid:110)(cid:16)

n−1

(cid:17)(cid:16)

Di

n(cid:88)

i=1

1 − n−1

(cid:17)(cid:111)−1

n(cid:88)

i=1

Di

10In fact, the costs could be allowed to increase with n at any rate as long as B → ∞ at a faster

rate, so that we have ˆnB/(cid:101)c(ˆn) →p ∞.

A-2

(cid:110)(cid:16)

n−1

Var (Ui(γ) | Di)

=

=

1
n

1
n

n(cid:88)
(cid:110)(cid:16)

i=1

(cid:17)(cid:16)
n(cid:88)

n(cid:88)
(cid:17)(cid:16)

i=1

(cid:17)(cid:111)−1
n(cid:88)

1 − n−1

Di

Di

i=1

i=1

Var (Yi − γ(cid:48)Xi | Di = 0)

n−1

1 − n−1

Di

(cid:17)(cid:111)−1

.

Di

Q.E.D.

Proof of Theorem A.1. This theorem can be proved by arguments similar to those used
in the proof of Theorem 2.3 in Barron, Cohen, Dahmen, and DeVore (2008). In the
subsequent arguments, we ﬁx n and leave indexing by n implicit.
First, letting ˆrk−1,i denote the ith component of ˆrk−1, we have

(cid:107)ˆrk−1(cid:107)2

N = N−1

ˆrk−1,iYi

N(cid:88)
N(cid:88)

i=1

X(cid:48)
Gj ,iβj

(cid:105)
(cid:104) ∞(cid:88)

+

|βj|2

N−1|ˆr(cid:48)

k−1XGk|2
(cid:105)

N−1|ˆr(cid:48)

X(cid:48)

Gk

βk

+

|βj|2

k−1XGk|2,

N(cid:88)
(cid:13)(cid:13)(cid:13)N

i=1

= N−1

ˆrk−1,iUi + N−1

ˆrk−1,i

≤ (cid:107)ˆrk−1(cid:107)N

≤ 1
2

X(cid:48)

i=1

k=1

(cid:13)(cid:13)(cid:13)Y −
(cid:16)(cid:107)ˆrk−1(cid:107)2
N −(cid:13)(cid:13)(cid:13)Y −

∞(cid:88)
(cid:13)(cid:13)(cid:13)Y −
∞(cid:88)

N +

X(cid:48)

Gk

βk

Gk

∞(cid:88)

k=1

(cid:13)(cid:13)(cid:13)2

N

βk

k=1

j=1

∞(cid:88)
(cid:104) ∞(cid:88)
(cid:13)(cid:13)(cid:13)2
(cid:17)
(cid:104) ∞(cid:88)

j=1

N

j=1

(cid:107)ˆrk−1(cid:107)2

≤ 2

|βj|2

j=1

(cid:105)

N−1|ˆr(cid:48)

k−1XGk|2.

(A.2)

which implies that

Note that if the left-hand side of (A.2) is negative for some k = k0, then the conclusion
of the theorem follows immediately for all m ≥ k0 − 1. Hence, we assume that the
left-hand side of (A.2) is positive, implying that

(cid:16)(cid:107)ˆrk−1(cid:107)2

N −(cid:13)(cid:13)(cid:13)Y −

∞(cid:88)

k=1

(cid:104) ∞(cid:88)
(cid:17)2 ≤ 4

(cid:13)(cid:13)(cid:13)2

N

j=1

X(cid:48)

Gk

βk

(cid:105)2

|βj|2

N−2|ˆr(cid:48)

k−1XGk|2
2.

(A.3)

Let Pk denote the projection matrix Pk := XGk(X(cid:48)

XGk)−1X(cid:48)
where the second equality comes from the assumption that (X(cid:48)

Gk

= N−1XGkX(cid:48)
,
Gk
XGk)/N = I|Gk|.

Gk

Gk

A-3

Hence, it follows from the fact that Pk is the projection matrix that

N − (cid:107)Pkˆrk−1(cid:107)2
N .
Because ˆrk is the best approximation to Y from ˆIn,k, we have

(cid:107)ˆrk−1 − Pkˆrk−1(cid:107)2

N = (cid:107)ˆrk−1(cid:107)2

(cid:107)ˆrk(cid:107)2

N ≤ (cid:107)ˆrk−1 − Pkˆrk−1(cid:107)2
N .

k = Pk, we have

(cid:107)ˆrk(cid:107)2

Combining (A.5) with (A.4) and using the fact that P 2
N − (cid:107)Pkˆrk−1(cid:107)2
N − (cid:107)N−1XGkX(cid:48)
N − N−2|ˆr(cid:48)
∞(cid:88)

N ≤ (cid:107)ˆrk−1(cid:107)2
= (cid:107)ˆrk−1(cid:107)2
= (cid:107)ˆrk−1(cid:107)2

Now, combining (A.6) and (A.3) together yields

(cid:16)(cid:107)ˆrk−1(cid:107)2

N −(cid:13)(cid:13)(cid:13)Y −

N ≤ (cid:107)ˆrk−1(cid:107)2

(cid:107)ˆrk(cid:107)2

X(cid:48)

N − 1
4

βk

Gk

N

ˆrk−1(cid:107)2
k−1XGk|2
2,

Gk

N

(cid:13)(cid:13)(cid:13)2

N

(cid:17)2(cid:104) ∞(cid:88)

j=1

(cid:105)−2

.

|βj|2

As in the proof of Theorem 2.3 in Barron, Cohen, Dahmen, and DeVore (2008), let
ak := (cid:107)ˆrk(cid:107)2

N . Then (A.7) can be rewritten as

βk(cid:107)2

k=1 X(cid:48)

Gk

N − (cid:107)Y −(cid:80)∞

k=1

(cid:104) ∞(cid:88)

j=1

(cid:105)−2(cid:17)

|βj|2

(cid:16)

ak ≤ ak−1

1 − ak−1
4

(A.4)

(A.5)

(A.6)

(A.7)

.

(A.8)

men, and DeVore (2008) gives the desired result, provided that a1 ≤ 4[(cid:80)∞
DeVore (2008), the initial condition is satisﬁed if a0 ≤ 4[(cid:80)∞
that a0 > 4[(cid:80)∞

Then the induction method used in the proof of Theorem 2.1 in Barron, Cohen, Dah-
j=1 |βj|2]2.
As discussed at the end of the proof of Theorem 2.3 in Barron, Cohen, Dahmen, and
j=1 |βj|2]2. If not, we have
j=1 |βj|2]2, which implies that a1 < 0 by (A.8). Hence, in this case, we

have that (cid:107)ˆr1(cid:107)2

k=1 X(cid:48)

Gk

βk(cid:107)2

N for which there is nothing else to prove.

Then, we have proved that the error of the group OGA satisﬁes

(cid:13)(cid:13)(cid:13)2

N

X(cid:48)

Gk

βk

+

4
m

(cid:105)2

|βj|2

(cid:104) p(cid:88)

j=1

, m = 1, 2, . . . .

N ≤ (cid:107)Y −(cid:80)∞
N ≤(cid:13)(cid:13)(cid:13)Y − p(cid:88)

(cid:107)ˆrm(cid:107)2

k=1

A-4

Equivalently, we have, for any n ∈ N and any k ≥ 1,

(cid:107)Y − ˆfn,k(cid:107)2

N − (cid:107)Y − f(cid:107)2

N ≤ 4(cid:107)f(cid:107)2LN

1

.

k

Because N is a ﬁnite set, the desired result immediately follows by substituting in the
deﬁnition of ˆf and kˆn.
Q.E.D.

Appendix B: Cost Functions Used in Section V

In this appendix, we provide detailed descriptions of the cost functions used in Section
V.

Calibration of the Cost Function in Section V.A

Here, we give a detailed description of components of the cost function used in Section
V.A.

• Administration costs. The administration costs in the survey were R$10,000
and the average survey took two hours per household to conduct (i.e., T (S) = 120
measured in minutes). Therefore,

cadmin(S, n) = φ(120)α = 10,000.

If we assume that, say, α = 0.4 (which means that the costs of 60 minutes are
about 75.8 percent of the costs of 120 minutes), we obtain φ ≈ 1,473.

• Training costs. The training costs in the survey were R$25,000, that is,

ctrain(S, n) = κ(1, 466) · 120 = 25,000,

so that κ(1, 466) ≈ 208. It is reasonable to assume that there exists some lumpi-
ness in the training costs. For example, there could be some indivisibility in hotel
rooms that are rented, and in the number of trainers required for each training

A-5

session. To reﬂect this lumpiness, we assume that

0 < n ≤ 1,400

150 if
208 if 1,400 < n ≤ 3,000
250 if 3,000 < n ≤ 4,500
300 if 4,500 < n ≤ 6,000
350 if

6,000 < n

.



κ(n) =

Note that, in this speciﬁcation, κ(1,466) ≈ 208, as calculated above. We take
this as a point of departure to calibrate κ(.). Increases in sample size n are likely
to translated into increases in the required number of ﬁeld workers for the survey,
which in turn lead to higher training costs. Our experience in the ﬁeld (based
on running surveys in diﬀerent settings, and on looking at diﬀerent budgets for
diﬀerent versions of this same survey) suggests that, in our example, there is some
concavity in this cost function, because an increase in the sample size, in principle,
will not require a proportional increase in the number of interviewers, and an
increase in the number of interviewers will probably require a less than proportion
increase in training costs. For example, we assume that a large increase in the
size of the sample, from 1,500 to 6,000, leads to an increase in κ(n) from 208 to
300 (i.e., an increase in overall training costs of about 50 percent).

• Interview costs. Interview costs were R$630,000, accounting for the majority

of the total survey costs, that is,

cinterv(S, n) = 1,466 · η + 1,466 · p · 120 = 630,000,

so that η + 120p ≈ 429.74. The costs of traveling to each household in this survey
were approximately half of the total costs of each interview. If we choose η = 200,
then the ﬁxed costs η amount to about 47 percent of the total interview costs,
which is consistent with the actual costs of the survey. Then we obtain the price
per unit of survey time as p ≈ 1.91. It is also reasonable to assume that half of the
variable costs per individual are due to the collection of the three outcomes in the
survey, because their administration was quite lengthy. The costs of collecting
the outcomes could also be seen as ﬁxed costs (equal to 0.955 × 120 = 114.6),
which means that the price per unit of survey time for each of the remaining

A-6

covariates is about 0.955. In sum, we can rewrite interview costs as

cinterv(S, n) = 1,466 × (200 + 114.6) + 1, 466 × 0.955 × 120 = 630,000.

• Price per covariate. We treat the sample obtained from the original experiment
as Spre, a pilot study or the ﬁrst wave of a data collection process, based on which
we want to decide which covariates and what sample size to collect in the next
wave. We perform the selection procedure for each outcome variable separately,
j=1 Sj). For simplicity, we assume that to ask
each question on the questionnaire takes the same time, so that τ0 = τj = τ
j=1 Sj) = 120. Note that we set
τ0 = τ here, but the high costs of collecting the outcome variables are reﬂected in
j=1 Sj). The actual
j=1 Sj = 40, and

and thus adjust T (S) = τ (1 +(cid:80)M
for every question; therefore, T (S) = τ (1 +(cid:80)M
the speciﬁcation of η above. This results in τ = 120/(1 +(cid:80)M
number of covariates collected in the experiment was 40; so(cid:80)M

thus τ ≈ 3.

• Rescaled budget. Because we use only a subsample of the original experimental
sample, we also scale down the original budget of R$665,000 down to R$569,074,
which corresponds to the costs of selecting all 36 covariates in the subsample;
that is, c(1, 1,330) where 1 is a 36-dimensional vector of ones and c(S, n) is the
calibrated cost function.

Calibration of the Cost Function in Section V.B

Here, we present a detailed description of components of the cost function used in
Section V.B.

• Administration costs. The administration costs for the low- and high-cost
covariates were estimated to be about $5,000 and $24,000, respectively. The high-
cost covariates were four tests that took about 15 minutes each (i.e., Thigh(S) =
60). For the low-cost covariates (teacher and principal survey), the total survey
time was around 60 minutes, so Tlow(S) = 60. High- and low-cost variables were
collected by two diﬀerent sets of enumerators, with diﬀerent levels of training
and skills. Therefore,

φlow(60)αlow = 5,000

and

φhigh(60)αhigh = 24,000.

A-7

If we assume that, say, αlow = αhigh = 0.7, we obtain φlow ≈ 285 and φhigh ≈
1,366.

• Training costs. µhigh and µlow are the numbers of enumerators collecting high-
and low-cost variables, respectively. The training costs for enumerators in the
high and low groups increase by 20 for each set of additional 20 low-cost enumer-
ators, and by 12 for each set of 4 high-cost enumerators:

κlow(c, nc) := 20

and

k · 1{20(k − 1) < µlow(c, nc) ≤ 20k}

19(cid:88)
17(cid:88)

k=1

k=1

7(cid:88)

κhigh(c, nc) := 12

k · 1{4(k − 1) < µhigh(c, nc) ≤ 4k}.

This is reasonable because enumerators for low-cost variables can be trained in
large groups (i.e., groups of 20), while enumerators for high-cost variables need
to be trained in small groups (i.e., groups of 4). However, training a larger group
demands a larger room, and, in our experience, more time in the room. The
lumpiness comes from the costs of hotel rooms and the time of the trainers.
The numbers 20 and 12 as the average costs of each cluster of enumerators were
chosen based on our experience with this survey (even if the design of the training
and the organization of the survey was not exactly the same as the stylized
version presented here), and reﬂect both the time of the trainer and the costs
of hotel rooms for each type of enumerators. Because the low-cost variables are
questionnaires administered to principals and teachers, in principle the number
of required enumerators only depends on c (i.e., µlow(c, nc) = (cid:98)λlowc(cid:99)). High-
cost variables are collected from students, and therefore the number of required
enumerators should depend on c and nc, so µhigh(c, nc) = (cid:98)λhighcµn,high(nc)(cid:99). We
assume that the latter increases again in steps, in this case of 10 individuals per
cluster, that is,

µn,high(nc) :=

k · 1{10(k − 1) < nc ≤ 10k}.

k=1

We let λlow = 0.14 (capturing the idea that one interviewer could do about seven
schools) and λhigh = 0.019 (capturing the idea that one enumerator could perhaps
work with about 50 children). The training costs in the survey were $1,600 for

A-8

the low-cost group of covariates and $1,600 for the high-cost group of covariates.
• Interview costs. We estimate that interview costs in the survey were $150,000

and $10,000 for the high- and low-cost variables, respectively, i.e.

ψlow(350)ηlow + 350 · plow · 60 = 10,000

and

ψhigh(350, 24)ηhigh + 350 · 24phigh · 60 = 150,000.

We set ψlow(c) = µlow(c) and ψhigh(c, nc) = µhigh(c, nc), the number of required
enumerators for the two groups, so that ηlow and ηhigh can be interpreted as ﬁxed
costs per enumerator. From the speciﬁcation of µlow(c) and µhigh(c, nc) above,
we obtain µlow(350) = 50 and µhigh(350, 24) = 20. The ﬁxed costs in the survey
were about ψlow(350)ηlow = 500 and ψhigh(350, 24)ηhigh = 1,000 for low- and high-
cost covariates. Therefore, ηlow = 500/50 = 10 and ηhigh = 1,000/20 = 50.
Finally, we can solve for the prices plow = (10,000 − 500)/(350 × 60) ≈ 0.45 and
phigh = (150,000 − 1,000)/(350 × 24 × 60) ≈ 0.3.

• Price per covariate. For simplicity, we assume that to ask each low-cost ques-
tion takes the same time, so that τj = τlow for every low-cost question (i.e.,
j = 1, . . . , Mlow), and that each high-cost question takes the same time (i.e.,
τj = τhigh) for all j = Mlow + 1, . . . , M . The experimental budget contains fund-
ing for the collection of one outcome variable, the high-cost test results at follow-
up, and three high-cost covariates at baseline. We modify Thigh(S) accordingly:
j=Mlow+1 Sj) = 4τhigh so that τhigh = 60/4 = 15. Similarly,
originally there were 255 low-cost covariates, which leads to τlow = 120/255 ≈
0.47.

Thigh(S) = τhigh(1 +(cid:80)M

• Rescaled budget. As in the previous subsection, we use only a subsample of
the original experimental sample. Therefore, we scale down the original budget
to the amount that corresponds to the costs of collecting all covariates used in
the subsample. As a consequence, the rescaled budget is $25,338 in the case of
baseline outcomes and $33,281 in the case of the follow-up outcomes.

A-9

Appendix C: A Simple Formulation of the Problem

Uniform Cost per Covariate

Take the following simple example where: (1) all covariates are orthogonal to each
other; (2) all covariates have the same price, and the budget constraint is just B = nk,
where n is sample size and k is the number of covariates. Order the covariates by the
contribution to the MSE, so that the problem is to choose the ﬁrst k covariates (and
the corresponding n).

Deﬁne σ2(k) = (1/N )(cid:80)N

i=1(Yi − γ(cid:48)

0,kXi)2, where γ0,k is the same as the vector of
true coeﬃcients γ0 except that all coeﬃcients after the (k +1)th coeﬃcient are set to be
zeros, and let M SE(k, n) = (1/n)σ2(k). For the convenience of using simple calculus,
suppose that k is continuous, ignoring that k is a positive integer, and that σ2(k) is
twice continuously diﬀerentiable. This would be a reasonable ﬁrst-order approximation
when there are a large number of covariates, which is our set-up in the paper. Because
we ordered the covariates by the magnitude of their contribution to a reduction in the
MSE, we have ∂σ2(k)/∂k < 0, and ∂2σ2(k)/∂k2 > 0.

The problem we solve in this case is just

min
n,k

1
n

σ2(k)

s.t. nk ≤ B.

Assume we have an interior solution and that n is also continuous. Replace the budget
constraint in the objective function and we obtain

min
n,k

k
B

σ2(k).

This means that k is determined by

or

σ2(k) + k

∂σ2(k)

∂k

= 0,

σ2(k)

k

+

∂σ2(k)

∂k

= 0,

(C.1)

which in this particular case does not depend on B. Then, n is given by the budget
constraint (i.e., n = B/k).

Another way to see where this condition comes from is just to start from the budget

A-10

constraint. If we want to always satisfy it then, starting from a particular choice of n
and k yields

n · dk + k · dn = 0,

or

dn
dk

= −n
k

.

Now, suppose we want to see what happens when k increases by a small amount.

In that case, keeping n ﬁxed, the objective function falls by

1
n

∂σ2(k)

∂k

dk.

This is the marginal beneﬁt of increasing k. However, n cannot stay ﬁxed, and needs
to decrease by (n/k)dk to keep the budget constraint satisﬁed. This means that the

objective function will increase by(cid:18)

(cid:19)

(cid:16)−n

(cid:17)

dk.

k

− 1
n2

σ2 (k)

This is the marginal cost of increasing k.

At the optimum, in an interior solution, marginal costs and marginal beneﬁts need

to balance out, so

or

1
nk

σ2 (k) dk = − 1
n

∂σ2 (k)

∂k

dk

σ2 (k)

k

+

∂σ2 (k)

∂k

= 0,

which reproduces (C.1).

There are a few things to notice in this simple example.

(1) The marginal costs of an increase in k are increasing in σ2 (k). This is because
increases in n are more important role for the MSE when σ2 (k) is large than
when it is small.

(2) The marginal costs of an increase in k are decreasing in k. This is because when k
is large, adding an additional covariate does not cost much in terms of reductions
in n.

(3) A large n aﬀects the costs and beneﬁts of increasing k in similar way. Having a
large n reduces beneﬁts of additional covariates because it dilutes the decrease in

A-11

σ2 (k). Then, on one hand, it increases costs through the budget constraint, as a
larger reduction in n is needed to compensate for the same change in k. However,
on the other hand, it reduces costs, because when n is large, a particular reduction
in n makes much less diﬀerence for the MSE than in the case where n is small.

(4) We can rewrite this condition as

1
k

+

∂σ2(k)/∂k

σ2 (k)

= 0,

where the term (∂σ2(k)/∂k)/σ2 (k) is the percentage change in the unexplained
variance from an increase in k.

If we combine

dk
k
which comes from the budget constraint, and

dn
n

=

,

1

∂M SE (n, k)

M SE (n, k)

∂n

= − 1
n

,

we notice that the percentage decrease in M SE from an increase in n is just (dn)/n,
the percentage change in n, which in turn is just equal to (dk)/k. So what the condition
above says is that we want to equate the percentage change in the unexplained variance
from a change in k to the percentage change in the MSE from the corresponding change
in n.

Perhaps even more interesting is to notice that k is the survey cost per individual in
this very simple example. Then this condition says that we want to choose k to equate
the percentage change in the survey costs per individual ((dk)/k) to the percentage
change in the residual variance

∂σ2(k)/∂k

σ2 (k)

dk.

This condition explicitly links the impacts of k on the survey costs and on the reduction
in the MSE.

Adding ﬁxed costs F of visiting each individual is both useful and easy in this very
simple framework. Suppose there are a ﬁxed costs F of going to each individual, so the
budget constraint is n (F + k) = B. Proceeding as above, we can rewrite our problem
as

F + k

min
n,k

B

σ2 (k) .

A-12

This means that k is determined by

or

σ2 (k) + (F + k)

∂σ2 (k)

∂k

= 0,

1

F + k

+

∂σ2 (k)/∂k

σ2 (k)

= 0.

Note that, when there are large ﬁxed costs of visiting each individual, increasing k is
not going to be that costly at the margin. It makes it much easier to pick a positive
k. However, other than that, the main lessons (1)–(4) of this simple model remain
unchanged.

Variable Cost per Covariate

If covariates do not have uniform costs, then the problem is much more complicated.
Consider again a simple set-up where all the regressors are orthogonal, and we order
them by their contribution to the MSE. However, suppose that the magnitude of each
covariate’s contribution the MSE takes a discrete ﬁnite number of values. Let R denote
the set of these discrete values. Let r denote an element of R and R = |R| (the total
number of all elements in R). There are many potential covariates within each r group,
each with a diﬀerent price p. The support of p could be diﬀerent for each r. So, within
each r, we will then order variables by p. The problem will be to determine the optimal
k for each r group. Let k ≡ {kr : r ∈ R}.

(cid:34)(cid:88)

r∈R

min
n,k

1
B

cr(kr)

σ2 (k) .

A-13

(cid:88)

r∈R

The problem is

cr(kr) ≤ B,

min
n,k

where cr (kr) =(cid:80)kr
also write it as cr (kr) = pr (kr) kr, where pr (kr) = ((cid:80)kr

σ2 (k)

s.t.

1
n

l=1 pl are the costs of variables of type r used in the survey. We can
l=1 pl)/kr. Because we order the
r = ∂σ2 (k)/∂kr, which

variables by price (from low to high), ∂pr (kr) /∂kr > 0. Let σ2
is a constant (this is what deﬁnes a group of variables).

Then, assume we can approximate pl (kr) by a continuous function and that we
have an interior solution. Then, substituting the budget constraint in the objective
function:

(cid:35)

From the ﬁrst-order condition for kr,

∂cr (kr)

∂kr

σ2 (k) +

(cid:35)

cr(kr)

(cid:34)(cid:88)

r∈R

∂σ2 (k)

∂kr

= 0,

or

(cid:80)

∂cr (kr)/∂kr
r∈R cr(kr)

= −∂σ2 (k)/∂kr

σ2 (k)

.

What this says is that, for each r, we choose variables up to the point where the
percent marginal contribution of the additional variable to the residual variance equals
the percent marginal contribution of the additional variable to the costs per interview,
just as in the previous subsection.

Appendix D: Simulations

In this appendix, we study the ﬁnite sample behavior of our proposed data collection
procedure, and compare its performance to other variable selection methods. We con-
sider the linear model from above, Y = γ(cid:48)X +ε, and mimic the data-generating process
in the day-care application of Section V.A with the cognitive test outcome variable.

First, we use the dataset to regress Y on X. Call the regression coeﬃcients ˆγemp and
the residual variance ˆσ2
emp. Then, we regress Y on the treatment indicator to estimate
the treatment eﬀect ˆβemp = 0.18656. We use these three estimates to generate Monte
Carlo samples as follows. For the pre-experimental data Spre, we resample X from the
empirical distribution of the M = 36 covariates in the dataset and generate outcome
variables by Y = γ(cid:48)X + ε, where ε ∼ N (0, ˆσ2

emp) and

γ = ˆγemp +

1
2

sign(ˆγemp)κ¯γ.

We vary the scaling parameter κ ∈ {0, 0.3, 0.7, 1} and ¯γ := (¯γ1, . . . , ¯γ36)(cid:48) is speciﬁed in
three diﬀerent fashions, as follows:

• “lin-sparse”, where the ﬁrst ﬁve coeﬃcients linearly decrease from 3 to 1, and all

others are zero, that is,

¯γk :=

(cid:40)

3 − 2(k − 1)/5, 1 ≤ k ≤ 5
otherwise

0,

;

A-14

• “lin-exp”, where the ﬁrst ﬁve coeﬃcients linearly decrease from 3 to 1, and the

remaining decay exponentially, that is,

(cid:40)

¯γk :=

e−k,

3 − 2(k − 1)/5, 1 ≤ k ≤ 5

;

k > 5

• “exp”, where exponential decay ¯γk := 10e−k.

When κ = 0, the regression coeﬃcients γ are equal to those in the empirical ap-
plication. When κ > 0, we add one of the three speciﬁcations of ¯γ to the coeﬃcients
found in the dataset, thereby increasing (in absolute value) the ﬁrst few coeﬃcients11

11Because all estimated coeﬃcients in the dataset (ˆγemp) are close to zero and roughly of the same
magnitude, we simply pick the ﬁrst ﬁve covariates that have the highest correlation with the outcome
variable.

Figure 1: Regression coeﬃcients in the simulation when κ = 0.3

A-15

 0510152025303540-1-0.500.511.52regression coefficientsdataexplin-sparselin-expmore than the others, and thus increasing the importance of the corresponding regres-
sors for prediction of the outcome. Figure 1 displays the regression coeﬃcients in the
dataset (i.e., when κ = 0, denoted by the blue line labeled “data”), and γ for the three
diﬀerent speciﬁcations when κ = 0.3.

For each Monte Carlo sample from Spre, we apply the OGA, LASSO, and POST-
LASSO methods, as explained in Section V.A. The cost function and budget are spec-
iﬁed exactly as in the empirical application. We store the sample size and covariate
selection produced by each of the three procedures, and then mimic the randomized
experiment Sexp by ﬁrst drawing a new sample of X from the same data-generating
process as in Spre. Then we generate random treatment indicators D, so that outcomes
are determined by

Y = ˆβempD + γ(cid:48)X + ε,

where ε is randomly drawn from N (0, ˆσ2
estimator ˆβ of β as described in Step 4 of Section II.

emp). We then compute the treatment eﬀect

The results are based on 500 Monte Carlo samples, N = 1,330, which is the sample
size in the dataset, and N a ﬁne grid from 500 to 4,000. All covariates, those in the
dataset as well as the simulated ones, are studentized so that their variance is equal to
one.

For the diﬀerent speciﬁcations of ¯γ, Tables D.1–D.3 report the selected sample size
(ˆn), the selected number of covariates (| ˆI|), the ratio of costs for that selection divided
by the budget B, the square root of the estimated MSE,
standard deviation of the estimated average treatment eﬀect (bias( ˆβ) and sd( ˆβ)), and
the RMSE of ˆβ across the Monte Carlo samples of the experiment.

(cid:113)(cid:92)M SE ˆn,N (ˆf ), the bias and

Overall, all three methods perform similarly well across diﬀerent designs and the
number of selected covariates tends to increase as κ becomes large. No single method
dominates other methods, although POST-LASSO seems to perform slightly better
than LASSO. In view of the Monte Carlo results, we argue that the empirical ﬁndings
reported in Section V.A are likely to result from the lack of highly predictive covariates
in the empirical example.

Appendix E: Variables Selected in the School Grants

Example

A-16

B
Q
E

)
ˆβ
(
E
S
M
R

)
ˆβ
(
d
s

4
7
0
9
,
6
5
$

6
8
5
8
,
4
3
$

1
8
7
6
,
5
3
$

9
5
6
1
,
5
3
$

4
7
0
9
,
6
5
$

6
7
0
7
,
7
3
$

7
1
0
3
,
9
3
$

0
3
7
1
,
7
3
$

4
7
0
9
,
6
5
$

2
2
7
7
,
9
3
$

1
7
9
3
,
2
4
$

5
9
0
7
,
8
3
$

4
7
0
9
,
6
5
$

6
3
5
8
,
3
4
$

9
8
5
1
,
3
4
$

3
5
0
0
,
8
3
$

8
4
0
0
5
0
.
0

1
0
8
8
3
0
.
0

8
8
3
9
3
0
.
0

7
2
7
7
3
0
.
0

3
5
4
9
4
0
.
0

2
6
2
8
3
0
.
0

2
8
6
1
4
0
.
0

3
6
7
8
3
0
.
0

8
4
9
0
5
0
.
0

3
8
4
1
4
0
.
0

7
1
9
3
4
0
.
0

6
8
6
2
4
0
.
0

4
1
1
5
0
.
0

8
4
3
3
4
0
.
0

6
3
3
5
4
0
.
0

5
1
2
1
4
0
.
0

1
8
9
9
4
0
.
0

8
3
8
8
3
0
.
0

2
7
3
9
3
0
.
0

8
5
7
7
3
0
.
0

1
0
5
9
4
0
.
0

5
7
2
8
3
0
.
0

3
1
7
1
4
0
.
0

6
4
7
8
3
0
.
0

2
9
9
0
5
0
.
0

5
7
4
1
4
0
.
0

7
5
9
3
4
0
.
0

3
8
6
2
4
0
.
0

6
4
1
1
5
0
.
0

9
8
3
3
4
0
.
0

8
7
3
5
4
0
.
0

7
6
9
0
4
0
.
0

)
ˆβ
(
s
a
i
b

4
8
2
4
3
0
0
.
0
−

8
9
5
4
3
0
0
0
.
0

4
7
8
0
2
0
0
.
0

4
9
3
9
6
0
0
0
.
0

2
9
9
6
3
0
0
0
.
0

5
0
9
3
1
0
0
.
0

9
8
0
3
9
0
0
0
.
0
−

1
5
7
0
2
0
0
.
0
−

3
6
5
6
8
0
0
0
.
0
−

1
5
1
0
2
0
0
.
0
−

6
1
5
7
5
0
0
0
.
0

3
9
6
9
1
0
0
.
0

5
3
5
1
2
0
0
.
0
−

2
6
1
4
4
0
0
0
.
0

6
0
1
8
5
0
0
0
.
0
−

6
2
7
8
4
0
0
.
0
−

8
9
4
2
0
.
0

9
4
2
9
1
0
.
0

8
1
4
9
1
0
.
0

2
2
2
9
1
0
.
0

4
9
4
2
0
.
0

1
5
7
9
1
0
.
0

6
4
3
0
2
0
.
0

6
9
6
9
1
0
.
0

3
5
9
4
2
0
.
0

2
5
5
0
2
0
.
0

5
4
1
1
2
0
.
0

7
7
1
0
2
0
.
0

8
3
9
4
2
0
.
0

6
6
5
1
2
0
.
0

3
8
3
1
2
0
.
0

6
5
9
9
1
0
.
0

e
s
r
a
p
s
-
n

i
l

:
s
t
l
u
s
e
r

n
o
i
t
a
l

u
m
S

,

)
ˆf
(
N
ˆn
E
S

(cid:92)M

(cid:113)

i

:
1
.
D
e
l

b
a
T

B
/
t
s
o
C

1

3
4
5
9
9
.
0

8
7
2
9
9
.
0

7
5
4
9
9
.
0

1

3
4
4
9
9
.
0

8
8
9
.
0

1
2
3
9
9
.
0

1

3
3
4
9
9
.
0

7
4
7
8
9
.
0

9
2
9
8
9
.
0

1

3
3
4
9
9
.
0

3
1
5
8
9
.
0

3
1
5
8
9
.
0

|

ˆI

|

6
3

4
.
1

1
.
0

0
.
1

6
3

9
.
3

9
.
5

4
.
4

6
3

0
.
4

1
.
6

7
.
5

6
3

0
.
4

9
.
6

9
.
6

ˆn

0
3
3
,
1

8
0
5
,
2

7
8
5
,
2

9
2
5
,
2

0
3
3
,
1

0
5
3
,
2

8
2
2
,
2

0
2
3
,
2

0
3
3
,
1

6
4
3
,
2

8
1
2
,
2

6
4
2
,
2

0
3
3
,
1

6
4
3
,
2

2
7
1
,
2

2
7
1
,
2

O
S
S
A
L

A
G
O

t
n
e
m

i
r
e
p
x
E

0

d
o
h
t
e

M

e
l
a
c
S

O
S
S
A
L
-
T
S
O
P

t
n
e
m

i
r
e
p
x
E

3
.
0

O
S
S
A
L

A
G
O

O
S
S
A
L
-
T
S
O
P

t
n
e
m

i
r
e
p
x
E

7
.
0

O
S
S
A
L

A
G
O

O
S
S
A
L
-
T
S
O
P

O
S
S
A
L

A
G
O

O
S
S
A
L
-
T
S
O
P

t
n
e
m

i
r
e
p
x
E

1

A-17

p
x
e
-
n

i
l

:
s
t
l
u
s
e
r

n
o
i
t
a
l

u
m
S

i

,

)
ˆf
(
N
ˆn
E
S

(cid:92)M

(cid:113)

B
Q
E

)
ˆβ
(
E
S
M
R

)
ˆβ
(
d
s

)
ˆβ
(
s
a
i
b

4
7
0
,
9
6
5
$

2
8
6
,
8
4
3
$

1
6
2
,
7
5
3
$

5
5
7
,
1
5
3
$

4
7
0
,
9
6
5
$

0
4
5
,
9
7
3
$

4
0
0
,
3
0
4
$

6
7
8
,
7
7
3
$

4
7
0
,
9
6
5
$

0
9
9
,
8
0
4
$

2
3
2
,
0
4
4
$

9
1
2
,
0
0
4
$

4
7
0
,
9
6
5
$

6
5
7
,
1
5
4
$

5
6
7
,
4
5
4
$

8
1
2
,
6
9
3
$

3
8
5
1
5
0
.
0

5
9
1
7
3
0
.
0

6
5
8
3
0
.
0

4
9
8
7
3
0
.
0

2
3
4
9
4
0
.
0

2
8
1
0
4
0
.
0

5
4
8
0
4
0
.
0

1
3
6
9
3
0
.
0

6
5
5
0
5
0
.
0

7
9
2
1
4
0
.
0

9
7
6
2
4
0
.
0

4
3
3
3
4
0
.
0

1
5
5
9
4
0
.
0

6
3
3
2
4
0
.
0

1
0
2
4
4
0
.
0

2
4
9
1
4
0
.
0

4
6
5
1
5
0
.
0

3
2
7
3
0
.
0

5
4
8
3
0
.
0

4
2
9
7
3
0
.
0

7
5
4
9
4
0
.
0

7
9
1
0
4
0
.
0

4
8
0
4
0
.
0

7
8
5
9
3
0
.
0

6
3
4
0
5
0
.
0

3
0
3
1
4
0
.
0

6
1
7
2
4
0
.
0

8
5
3
3
4
0
.
0

4
8
4
9
4
0
.
0

4
0
3
2
4
0
.
0

8
1
2
4
4
0
.
0

7
7
9
1
4
0
.
0

3
3
0
7
2
0
0
.
0

1
6
9
2
4
0
0
0
.
0
−

4
7
3
3
0
0
.
0
−

6
5
9
6
7
0
0
0
.
0

5
4
6
5
1
0
0
.
0
−

9
4
3
4
1
0
0
.
0
−

7
7
3
9
1
0
0
.
0

2
1
8
5
2
0
0
.
0
−

7
2
5
1
4
0
0
.
0

7
6
2
7
1
0
0
.
0
−

2
8
1
1
7
0
0
0
.
0
−

5
5
0
3
1
0
0
.
0

4
6
0
4
3
0
0
.
0
−

6
0
1
5
2
0
0
.
0
−

6
4
7
5
1
0
0
.
0
−

8
8
4
7
7
0
0
0
.
0

5
6
9
4
2
0
.
0

9
4
2
9
1
0
.
0

1
4
9
1
0
.
0

5
1
2
9
1
0
.
0

2
9
4
2
0
.
0

8
6
8
9
1
0
.
0

2
5
6
0
2
0
.
0

6
1
8
9
1
0
.
0

6
3
9
4
2
0
.
0

5
0
8
0
2
0
.
0

2
6
1
2
0
.
0

2
2
5
0
2
0
.
0

4
6
9
4
2
0
.
0

4
7
8
1
2
0
.
0

7
8
9
1
2
0
.
0

4
7
3
0
2
0
.
0

:
2
.
D
e
l

b
a
T

B
/
t
s
o
C

1

1
4
5
9
9
.
0

5
7
2
9
9
.
0

6
4
9
9
.
0

1

1
2
4
9
9
.
0

9
6
5
8
9
.
0

8
8
2
9
9
.
0

1

7
4
2
9
9
.
0

1
5
5
8
9
.
0

5
5
9
8
9
.
0

1

7
8
1
9
9
.
0

3
9
7
8
9
.
0

7
8
7
8
9
.
0

|

ˆI

|

6
3

3
.
1

1
.
0

0
.
1

6
3

0
.
4

7
.
6

5
.
4

6
3

7
.
4

7
.
7

5
.
6

6
3

0
.
5

0
.
9

0
.
9

ˆn

0
3
3
,
1

9
0
5
,
2

8
8
5
,
2

0
3
5
,
2

0
3
3
,
1

3
4
3
,
2

6
8
1
,
2

3
1
3
,
2

0
3
3
,
1

1
0
3
,
2

4
3
1
,
2

6
0
2
,
2

0
3
3
,
1

6
8
2
,
2

0
8
0
,
2

8
7
0
,
2

O
S
S
A
L

A
G
O

t
n
e
m

i
r
e
p
x
E

0

d
o
h
t
e

M

e
l
a
c
S

O
S
S
A
L
-
T
S
O
P

t
n
e
m

i
r
e
p
x
E

3
.
0

O
S
S
A
L

A
G
O

O
S
S
A
L
-
T
S
O
P

t
n
e
m

i
r
e
p
x
E

7
.
0

O
S
S
A
L

A
G
O

O
S
S
A
L
-
T
S
O
P

O
S
S
A
L

A
G
O

O
S
S
A
L
-
T
S
O
P

t
n
e
m

i
r
e
p
x
E

1

A-18

p
x
e

:
s
t
l
u
s
e
r

n
o
i
t
a
l
u
m
S

i

,

)
ˆf
(
N
ˆn
E
S

(cid:92)M

(cid:113)

:
3
.
D
e
l

b
a
T

B
Q
E

)
ˆβ
(
E
S
M
R

)
ˆβ
(
d
s

)
ˆβ
(
s
a
i
b

4
7
0
,
9
6
5
$

6
2
4
,
8
4
3
$

1
4
9
,
6
5
3
$

3
0
4
,
1
5
3
$

4
7
0
,
9
6
5
$

0
5
9
,
9
5
3
$

0
6
5
,
9
8
3
$

2
6
6
,
9
5
3
$

4
7
0
,
9
6
5
$

7
8
2
,
2
6
3
$

8
2
1
,
1
9
3
$

3
0
9
,
1
6
3
$

4
7
0
,
9
6
5
$

5
1
0
,
4
6
3
$

7
5
8
,
2
9
3
$

3
2
0
,
3
6
3
$

6
9
9
3
5
0
.
0

6
3
2
7
3
0
.
0

3
1
8
8
3
0
.
0

2
1
5
9
3
0
.
0

2
0
2
1
5
0
.
0

7
2
7
8
3
0
.
0

3
6
2
0
4
0
.
0

2
8
9
9
3
0
.
0

2
1
9
0
5
0
.
0

8
7
7
0
4
0
.
0

3
6
4
2
4
0
.
0

7
8
2
7
3
0
.
0

7
5
9
1
5
0
.
0

9
6
8
8
3
0
.
0

9
1
0
9
3
0
.
0

6
0
1
9
3
0
.
0

3
4
0
4
5
0
.
0

7
3
2
7
3
0
.
0

9
4
8
8
3
0
.
0

9
4
5
9
3
0
.
0

6
4
2
1
5
0
.
0

9
2
7
8
3
0
.
0

3
4
2
0
4
0
.
0

6
9
9
9
3
0
.
0

7
4
9
0
5
0
.
0

9
8
7
0
4
0
.
0

1
9
4
2
4
0
.
0

2
5
2
7
3
0
.
0

8
9
8
1
5
0
.
0

6
4
8
8
3
0
.
0

4
2
0
9
3
0
.
0

5
3
1
9
3
0
.
0

7
7
0
3
8
0
0
0
.
0

6
1
6
6
1
0
0
.
0

8
2
3
9
4
0
0
0
.
0
−

4
0
4
4
4
0
0
0
.
0
−

2
2
5
9
8
0
0
0
.
0
−

1
5
9
6
1
0
0
.
0

4
9
0
2
2
0
0
.
0
−

2
5
5
4
1
0
0
.
0

4
9
6
2
1
0
0
.
0
−

9
9
3
5
1
0
0
.
0

6
6
1
1
1
0
0
.
0

8
0
2
3
2
0
0
.
0
−

4
1
0
4
3
0
0
.
0
−

1
3
0
2
2
0
0
.
0

3
9
3
6
1
0
0
.
0

5
4
6
5
8
0
0
0
.
0
−

3
5
9
4
2
0
.
0

4
3
2
9
1
0
.
0

4
9
3
9
1
0
.
0

3
0
2
9
1
0
.
0

7
4
9
4
2
0
.
0

6
2
4
9
1
0
.
0

4
8
1
0
2
0
.
0

7
7
3
9
1
0
.
0

6
4
9
4
2
0
.
0

7
5
4
9
1
0
.
0

3
3
2
0
2
0
.
0

5
0
4
9
1
0
.
0

8
4
9
4
2
0
.
0

4
9
4
9
1
0
.
0

8
9
2
0
2
0
.
0

8
4
4
9
1
0
.
0

B
/
t
s
o
C

1

8
3
5
9
9
.
0

8
7
2
9
9
.
0

2
5
4
9
9
.
0

1

5
0
6
9
9
.
0

1
1
9
9
.
0

4
1
5
9
9
.
0

1

5
0
6
9
9
.
0

9
3
0
9
9
.
0

5
1
5
9
9
.
0

1

3
0
6
9
9
.
0

8
0
0
9
9
.
0

6
1
5
9
9
.
0

|

ˆI

|

6
3

3
.
1

1
.
0

0
.
1

6
3

9
.
2

9
.
4

5
.
3

6
3

0
.
3

1
.
5

5
.
3

6
3

0
.
3

2
.
5

5
.
3

ˆn

0
3
3
,
1

1
1
5
,
2

8
8
5
,
2

9
2
5
,
2

0
3
3
,
1

1
1
4
,
2

1
9
2
,
2

0
8
3
,
2

0
3
3
,
1

8
0
4
,
2

9
7
2
,
2

6
7
3
,
2

0
3
3
,
1

7
0
4
,
2

1
7
2
,
2

7
7
3
,
2

O
S
S
A
L

A
G
O

t
n
e
m

i
r
e
p
x
E

0

d
o
h
t
e

M

e
l
a
c
S

O
S
S
A
L
-
T
S
O
P

t
n
e
m

i
r
e
p
x
E

3
.
0

O
S
S
A
L

A
G
O

O
S
S
A
L
-
T
S
O
P

t
n
e
m

i
r
e
p
x
E

7
.
0

O
S
S
A
L

A
G
O

O
S
S
A
L
-
T
S
O
P

O
S
S
A
L

A
G
O

O
S
S
A
L
-
T
S
O
P

t
n
e
m

i
r
e
p
x
E

1

A-19

Table E.1: School grants (outcome: math test): selected covariates in panel (a) of
Table 3

OGA

LASSO

POST-LASSO

Child is male
Dist. to Dakar
Dist. to city
Village pop.
Piped water

Child is male
Village pop.
Piped water
Teach-stud
No. computers
Req. (h) teach. qual. No. computers
Req. (h) teach. att.
Obs. (h) manuals
Books acq. last yr.
Any parent transfer
Teacher bacc. plus
Teach. train. math
Obst. (t) class size
Measure. equip.

Child is male
Dist. to Dakar
Dist. to city
Village pop.
Piped water
No. computers
Req. (h) teach-stud
Hrs. tutoring
Books acq. last yr.
Provis. struct.
NGO cash cont.
Any parent transfer
NGO promised cash
Avg. teach. exp.
Teacher bacc. plus

Req. (h) teach-stud
Hrs. tutoring
Books acq. last yr.
Provis. struct.
NGO cash cont.
Any parent transfer
NGO promised cash
Avg. teach. exp.
Teacher bacc. plus
Obs. (t) student will. Obs. (t) student will.
Obst. (t) class size
Silence kids

Obst. (t) class size
Silence kids

A-20

Table E.2: Deﬁnition of variables in Table E.1

Variable
Child is male
Village pop.
Piped water
Teach–stud
No. computers
Req. (h) teach. qual.

Req. (h) teach. att.

Obs. (h) manuals

Books acq. last yr.
Any parent transfer
Teacher bacc. plus
Teach. train. math
Obst. (t) class size
Measure. equip.
Dist. to Dakar
Dist. to city
Req. (h) teach–stud

Deﬁnition
Male student
Size of the population in the village
School has access to piped water
Teacher–student ratio in the school
Number of computers in the school
Principal believes teacher quality is a major requirement
for school success
Principal believes teacher attendance is a major requirement
for school success
Principal believes the lack of manuals is a major obstacle
to school success
Number of manuals acquired last year
Cash contributions from parents
Teacher has at least a baccalaureate degree
Teacher received special training in math
Teacher believes class size is a major obstacle to school success
There is measurement equipment in the classroom
Distance to Dakar
Distance to the nearest city
Principal believe teacher–student ratio is a major requirement
for school success
Hours of tutoring by teachers
Number of provisional structures in school
Cash contributions by NGO
Promised cash contributions by NGO
Average experience of teachers in the school

Hrs. tutoring
Provis. struct.
NGO cash cont.
NGO promised cash
Avg. teach. exp.
Obst. (t) student will. Teacher believes the lack of student willpower is one of the

Obst. (t) class size

Silence kids

main obstacles to learning in the school
Teacher believes the lack of classroom size is one of the main
obstacles to learning in the school
Teacher has to silence kids frequently

A-21

