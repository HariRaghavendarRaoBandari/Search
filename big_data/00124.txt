Learning Multilayer Channel Features for

Pedestrian Detection

Jiale Cao, Yanwei Pang, and Xuelong Li

1

6
1
0
2

 
r
a

M
1

 

 
 
]

V
C
.
s
c
[
 
 

1
v
4
2
1
0
0

.

3
0
6
1
:
v
i
X
r
a

Abstract—Pedestrian detection based on the combination of
Convolutional Neural Network (i.e., CNN) and traditional hand-
crafted features (i.e., HOG+LUV) has achieved great success.
Generally, HOG+LUV are used to generate the candidate pro-
posals and then CNN classiﬁes these proposals. Despite its
success, there is still room for improvement. For example, CNN
classiﬁes these proposals by the full-connected layer features
while proposal scores and the features in the inner-layers of CNN
are ignored. In this paper, we propose a unifying framework
called Multi-layer Channel Features (MCF) to overcome the
drawback. It ﬁrstly integrates HOG+LUV with each layer of
CNN into a multi-layer image channels. Based on the multi-layer
image channels, a multi-stage cascade AdaBoost is then learned.
The weak classiﬁers in each stage of the multi-stage cascade is
learned from the image channels of corresponding layer. With
more abundant features, MCF achieves the state-of-the-art on
Caltech pedestrian dataset (i.e., 10.40% miss rate). Using new
and accurate annotations, MCF achieves 7.98% miss rate. As
many non-pedestrian detection windows can be quickly rejected
by the ﬁrst few stages, it accelerates detection speed by 1.43
times. By eliminating the highly overlapped detection windows
with lower scores after the ﬁrst stage, it’s 4.07 times faster with
negligible performance loss.

Index Terms—Pedestrian Detection, Multilayer Channel Fea-

tures (MCF), HOG+LUV, CNN, NMS.

I. INTRODUCTION

P EDESTRIAN detection based on Convolutional Neural

Network (i.e., CNN) has achieved great success recently
[5], [30], [6], [4], [8]. The main process of CNN based
methods can be divided into two steps: proposal extraction
and CNN classiﬁcation. Firstly, the candidate proposals are
extracted by the traditional pedestrian detection algorithm
(e.g., ACF [11] and LDCF [22]). Then, these proposals are
classiﬁed into pedestrian or non-pedestrian by the CNN [6],
[1].

Despite its great success,

it still exists some room to
improve it. 1) Most methods only use the last layer features
in CNN with softmax or SVM to classify the proposals. In
fact, different layers in CNN represents the different image
characteristic. The ﬁrst few layers can better describe the
image local variance, whereas the last few layers abstract
the image global structure. It means that each layer in CNN
contains different discriminative features, which can be used
for learning the classiﬁer. 2) Some methods only use the

Y. Pang and J. Cao are with the School of Electronic Informa-
tion Engineering, Tianjin University, Tianjin 300072, China. e-mails:
{pyw,connor}@tju.edu.cn

X. Li is with the Center for OPTical IMagery Analysis and Learning
(OPTIMAL), State Key Laboratory of Transient Optics and Photonics, Xi’an
Institute of Optics and Precision Mechanics, Chinese Academy of Sciences,
Xi’an 710119, Shaanxi, P. R. China. e-mail: xuelong li@opt.ac.cn.

traditional methods based on the handcrafted features (i.e.,
HOG+LUV [10]) to generate the candidate proposals while
ignoring the proposal scores. 3) Due to the large amount of
convolutional operations, the methods based very deep CNN
(e.g., VGG16 [25]) run very slowly on the common CPU (e.g.,
about 8s).

Recently, researchers have done some work to solve the
above problems. Li et al. [35] proposed to train the cascaded
multiple CNN models of different resolutions. As the low
resolution CNN can early reject many background regions,
it avoids scanning the full image with high resolution CNN
and then reduces the computation cost. However, the training
process of multiple CNN models is relatively complex. Cai et
al. [1] proposed the complexity-aware cascade to seamlessly
integrate handcrafted features and the last layer features in
CNN into a unifying detector. However,
it still does not
make full use of the multi-layer features in CNN. Bell et al.
[19] concatenated the multiple layers of CNN into the ﬁxed-
size ROI pooling. With more abundant feature abstraction,
it outperforms fast-RCNN [18]. Though its success, it needs
complex operations of L2-normalized, concatenated, scaled,
and dimension-reduced. Moreover, it ignores the scores of the
proposals.

In this paper, we propose a unifying framework, which is
called Multi-layer Channel Features (MCF). Firstly, it inte-
grates handcrafted image channels (i.e., HOG+LUV) and each
layer of CNN into the multi-layer image channels. HOG+LUV
image channels are set as the ﬁrst layer, which contains 10
image channels. The layers in CNN correspond to the remain-
ing layers, respectively. Secondly, zero-order, one-order, and
high-order features are extracted to generate a large number
of candidate feature pools in each layer. Finally, a multi-stage
cascade AdaBoost is used to select the discriminative features
and efﬁciently classify object and background. The weak
classiﬁers in each stage of multi-stage cascade are learned
based on the candidate features from corresponding layer.
To further accelerate detection speed, the highly overlapped
detection windows with lower scores are eliminated after the
ﬁrst stage. Overall, the contributions of this paper and the
merits of the proposed methods (MCF) can be summarized as
follows:

1) The unifying framework MCF is proposed. MCF seam-
lessly integrates HOG+LUV image channels and each
layer of CNN into a unifying multi-layer image chan-
nels. Due to the diverse characteristic of different layers,
these layers can provide more rich feature abstractions.
2) Multi-stage cascade AdaBoost is learned from multi-
layer image channels. It can achieve better performance

with more abundant feature abstractions and quickly
reject many detection windows by the ﬁrst few stages.
3) The highly overlapped detection windows with lower
scores are eliminated after the ﬁrst stage. Thus, it can
reduce the large computation cost of CNN operations.
With very little performance loss, it’s 4.07 times faster.
Finally, it’s possible that MCF with very deep CNN
(e.g., VGG16 [25]) can run at 0.54 fps on the common
CPU, while it achieves 11.05% miss rate on original
Caltech pedestrian set.

4) MCF achieves the state-of-the-art performance on Cal-
tech pedestrian dataset (the log-average miss rate is
10.40%), which outperforms CompACT-Deep [1] by
1.35%. Using new and more accurate annotations [37]
of the test set, MCF achieves 7.98% miss rate, which is
superior to other methods.

The rest of the paper is organized as follows. Firstly, we
give a review about pedestrian detection. Then, our methods
are introduced in Sec. III. Sec. IV shows the experimental
results. Finally, we conclude this paper in Sec. V.

II. RELATED WORK

According to whether or not CNN is used, pedestrian de-
tection can be divided into two main manners: the handcrafted
channels based methods and CNN based methods. Handcrafted
channels based methods are relatively simple and efﬁcient,
whereas CNN based methods are much more effective but
inefﬁcient. We ﬁrstly give a review about the handcrafted
channels based methods and then introduce some methods
based on CNN.

Haar features based cascade AdaBoost detector is one of
the most famous object detection methods [14], [33]. It can
quickly reject a large number of non-object detection windows
by the early stages of the cascade. Dalal and Triggs [12]
proposed to use the Histogram of Oriented Gradients (HOG)
to describe the image local variance. It can work very well
with a linear SVM. To handle pose variations of objects,
Felzenszwalb et al. [13] proposed the Deformable Part Model
(DPM) based on HOG features, which is a mixture of six
deformable part models and one root model.

By integrating cascade AdaBoost [14], [20] and HOG
features [12], Doll´ar et al. [10] proposed Integral Channel
Features (ICF). Firstly, it extracts the local sum features from
HOG channels and LUV color channels (i.e., HOG+LUV).
Then, cascade AdaBoost [20], [36] is used to learn the
classiﬁer. To further speedup the detection, Doll´ar et al. [11]
then proposed Aggregated Channel Features (ACF), which
downsamples the image channels by a factor of 4.

Following ICF [10], SquaresChnFtrs [9], InformedHaar
[21], LDCF [22], Filtered Channel Features (FCF) [7], and
NNNF [31] have also been proposed. They all employ
the same image channels (i.e., HOG+LUV) as ICF.
In
SquaresChnFtrs [9], the pixel sums of local square regions in
each channel are used for learning the classiﬁer. InformedHaar
[21] incorporates the statistical pedestrian model
into the
design of simple haar-like features. Inspired by [23], Nam et
al. [22] proposed to calculate the decorrelated channels by

2

convolving the PCA-like [24] ﬁlters with HOG+LUV image
channels. Recently, Zhang et al. [7] proposed to put the above
different types of channel features into a unifying framework
(i.e., FCF). FCF generates the candidate feature pool by con-
volving a ﬁlter bank (RadomFilters, Checkerboards, etc) with
HOG+LUV image channels. It’s found that using the simple
Checkboards ﬁlters could achieve very good performance.
Based on the appearance constancy and shape symmetry, Cao
et al. [31] proposed NNNF features.

Recently, deep Convolutional Neural Network (CNN) based
methods have also achieved great success in object detection
[26], [15], [19], [29], [18], [27], [28]. Generally speaking,
it ﬁrstly generates the candidate object proposals [17], [16]
and then uses the trained CNN model [26], [15] to classify
these proposals. Hosang et al. [6] generalized CNN model
for pedestrian detection after using the handcrafted features
based methods to extract the candidate pedestrian proposals.
To eliminate the number of hard negative proposals in the
background, Tian et al. [3] proposed to jointly optimize
pedestrian detection with semantic tasks. Recently, Tian et
al. [2] proposed to learn deep strong part models to handle
the problem of pedestrian occlusion. Li et al. [4] proposed
the scale-aware fast-RCNN by incorporating a large scale
sub-network and a small scale sub-network into a unifying
architecture.

Despite the success of CNN based pedestrian detection,
it still exists some room to improve it. Firstly,
the score
information of the candidate proposals can be used to boost
the detection performance. Secondly, each layer in CNN
contains some discriminative features, which can be used
for learning classiﬁer and rejecting non-pedestrian detection
windows early. Cai et al. [1] proposed to seamlessly integrate
CNN and handcrafted features. Though it uses the proposal
score information, it still ignores features of the inner layer
of CNN. Sermanet et al. [5] proposed to concatenate the
ﬁrst layer feature and the second layer together. Bell et al.
[19] proposed to use skip pooling to integrate multiple layers.
It’s called skip-layer connections. Despite its success, there
is still some problems: 1) It ignores the proposals scores;
2) The proposal should pass through the whole CNN before
classiﬁcation; 3) The skip-layer operations in [19] is relatively
complex.

III. OUR METHODS
A. Multi-layer Channel Features (MCF)

The layers in CNN represent

the different and diverse
image characteristic. The image channels in the ﬁrst few
layers can better describe the image local variance, while the
image channels in the last few layers can abstract the image
global structure. Meanwhile, the handcrafted image channels
(e.g., HOG+LUV) can be also able to describe the image
variations very well. HOG channels can describe the image
local edge directions and variances. LUV channels capture the
image color information. Compared to the layers in CNN, the
handcrafted image channels are very simple and efﬁcient. In
this paper, we integrate HOG+LUV and the layers of CNN to
construct Multi-layer Channel Features (MCF).

3

Fig. 2. Test process of basic MCF.

Fig. 3. MCF are generated by HOG+LUV and C2-C5 of VGG16. C1 of
VGG16 are not used for constructing MCF.

MULTI-LAYER IMAGE CHANNELS. THE FIRST LAYER IS HOG+LUV, THE
REMAINING LAYERS ARE THE CONVOLUTIONAL LAYERS (I.E., C1 TO C5)

TABLE I

IN VGG16.

Layer

Name

Size
Num

L1
HOG
LUV

128 × 64

10

L2

L3

C1

64 × 32

64

C2

32 × 16

128

L4
VGG16
C3
16 × 8
256

L5

L6

C4
8 × 4
512

C5
4 × 2
512

For example, a ﬁve-layer image channels can be generated by
HOG+LUV and C2-C5 of VGG16. C1 of VGG16 is not used.
The corresponding MCF are shown in Fig. 3.

Feature Extraction Features can be divided into three
classes: zero-order feature, one-order feature, and high-order
feature. In zero-order feature extraction, a single pixel itself is
used as a feature and no neighboring pixels are used. One-
order feature is deﬁned as the pixel sums or averages in
the local or non-local regions in each channel. High-order
feature deﬁned by the difference of the sums or averages
of two or more different regions. For L1 (i.e., HOG+LUV),
there are many successful methods for feature extraction,
including ICF [10], ACF [11], SquaresChnFtrs [9], Informed-

Fig. 1. The basic architecture of MCF. It can be divided into three steps:
multi-layer channel generation, feature extraction, and multi-stage cascade
AdaBoost classiﬁer.

First of all, we give an overview about our proposed Multi-
layer Channel Features (i.e., MCF). Fig. 1 show the basic ar-
chitecture of MCF. It can be divided into three parts: 1) Firstly,
multi-layer image channels from L1 to LN are generated. The
traditional handcrafted image channels (i.e., HOG+LUV) are
used for the ﬁrst layer (i.e., L1). The convolutional layers from
C1 to C(N-1) in CNN construct the remaining layers from
L2 to LN. In each layer, there are multiple image channels.
2) The second step is feature extraction. Zero-order, one-
order, and high-order features can be calculated in the image
channels of each layer. 3) Finally, the multi-stage cascade
AdaBoost
is learned from the candidate features of each
layer one after another. The weak classiﬁers in each stage
of multi-stage cascade are learned from the candidate features
of corresponding layer. For example, the weak classiﬁers in
Stage 2 (i.e., S2) are learned from candidate features F2 of
Layer 2 (i.e., L2).

Fig. 2 shows the test process of MCF. Give the test
image, the image channels in L1 (i.e., HOG+LUV) are ﬁrstly
computed. Detection windows are generated by scanning the
test
image. These detection windows are classiﬁed by S1
using the weak classiﬁers learned from L1. Some detection
windows will be rejected by S1. For the detection windows
accepted by S1, the image channels in L2 are computed.
Then the accepted detection windows are classiﬁed by S2
using the weak classiﬁers learned from L2. The above process
is repeated from L1 to LN. Finally, the detection windows
accepted by all the stages (i.e., S1 to SN) will be merged by
NMS. The merged detection windows are the ﬁnal pedestrian
windows.

Multi-layer Image Channels Row 1 in Fig. 1 shows the
multi-layer image Channels. It consists of N layers. In each
layer, there are multiple image channels. Table I shows the
speciﬁc parameters of multi-layer image channels based on
HOG+LUV and VGG16. It contains six layers from L1 to
L6. L1 is the handcrafted image channels (i.e., HOG+LUV).
L2-L6 are ﬁve convolutional layers (i.e., C1-C5) in VGG16.
Row 3 shows the image size in each layer. The image size
in L1 is 128 × 64. The sizes of L2-L6 are 64 × 32, 32 ×
16, 16 × 8, 8 × 4, and 4 × 2, respectively. Row 4 shows the
number of the channels in each layer. L1 contains 10 image
channels. L2-L6 each have 64, 128, 256, 512, and 512 image
channels, respectively. In Table I, all the convolutional layers
in CNN (i.e., C1 to C5) are used for constructing the multi-
layer image channels. In fact, only part convolutional layers
in CNN can also construct the multi-layer image channels.

…Multi-layer Channels…Feature ExtractionMulti-stage Cascade...+HOG+LUVCNN (e.g., AlexNet, VGG, and GooleNet)L1L2L3L4LN...............F1F2F3F4FNS1++...+++...+++...++...++…S2S3S4SNHandcrafted ChannelsC(N-1)C1C2C3++Test ImageCompute image channels in L1Classify detection windows by S1Compute image channels in LNClassify detection windows by SNNMSPedestrian windowsGenerate detection windowsCompute image channels in L2Classify detection windows by S2...RejectAcceptRejectAcceptRejectNon-Pedestrian windowsAccept...…Multi-layer Channels…Feature ExtractionMulti-stage Cascade...+HOG+LUVCNN (i.e., VGG16)L1L2L3L5............F1F2F3F5S1++...+++...+++...++…S2S3S5Handcrafted ChannelsC5C1C2C3+4

Fig. 5. Multi-stage cascade AdaBoost. Each stage learns the classiﬁers from
the candidate features of corresponding layer.

where NAll represents the number of the total weak classiﬁers.
Based on soft-cascade [20], the reject thresholds are set after
each weak classiﬁer.

The advantages about the multi-stage cascade AdaBoost
structure can be concluded as the following: 1) Firstly, it
avoids learning classiﬁer from a very large feature pooling
(e.g., more than one million); 2) Secondly, it makes full use of
the information from multi-layer image channels. Thus, it can
enrich the feature abstraction. 3) Finally, many non-pedestrian
detection windows can be quickly rejected by the features in
the ﬁrst few layers. Thus, it avoids the computation cost of the
remaining layers in CNN and accelerates the detection speed.

B. Elimination of Highly Overlapped Windows

Pedestrian detection is a multiple instance problem. Gener-
ally, the adjacent area around the pedestrian exists many posi-
tive detection windows. Many of these positive detection win-
dows around pedestrians highly overlap. Though multi-stage
cascade AdaBoost structure can reject many non-pedestrian
detection windows, it cannot reject the positive detection win-
dows around pedestrians. When the cascade classiﬁer based
on very deep CNN (e.g., VGG16), the computation cost of
these positive detection windows are large.

In fact, there is no need to put all the highly overlapped
windows accepted by ﬁrst stage into the remaining stages.
Detection windows accepted by the ﬁrst stage each have a clas-
siﬁcation score. The highly overlapped windows with lower
scores can be eliminated after the ﬁrst stage. To eliminate
these highly overlapped windows with lower scores, Non-
Maximum Suppression (i.e., NMS) is used after the ﬁrst stage.
The overlap ratio O(w1, w2) of detection windows can be
deﬁned in the following:

O(w1, w2) =

,

(3)

area(w1 ∩ w2)
area(w1 ∪ w2)

where w1 and w2 are two detection windows. If O(w1, w2) >
θ, it means that w1 and w2 highly overlap. Then the detection
window with lower score will be eliminated. Instead of the
standard threshold θ = 0.5, a larger threshold is used here.
Experimental results show that θ = 0.8 can accelerate the
detection speed with little performance loss. Fig. 6 shows the
speciﬁc test process of MCF by eliminating highly overlapped
detection windows.

IV. EXPERIMENTS

The challenging Caltech pedestrian detection dataset [32],
[34] is employed for the evaluation. It consists of 11 videos.
The ﬁrst 6 videos are used for training and the rest videos
are used for testing. The raw training images are formed

Fig. 4. Feature Extraction in Multi-layer image channels. (a) feature extraction
in L1 (HOG+LUV), where one-order (ACF) and high-order features (NNNF)
are used. (b) feature extraction in L2-LN (the layers of CNN), where zero-
order features are extracted. Zero-order feature means that a single pixel value
in each channel is used as a feature.

Haar [21], LDCF [22], FCF [7], and NNNF [31]. ICF, ACF,
and SquaresChnFtrs can be seen as one-order features. In-
formedHaar, LDCF, FCF, and NNNF are high-order features.
Compared to the other features, ACF has the fastest detec-
tion speed. NNNF has the best trade-off between detection
speed and detection performance. Due to the simplicity and
effectiveness, ACF and NNNF are used for feature exaction
in L1. The number of image channels from CNN is relatively
large. For example, the fourth convolutional layer (i.e., C4) in
VGG16 has 512 image channels (see Table I). To reduce the
computation cost and avoid a very large number of candidate
features, only zero-order feature is used. It means that each
pixel value in image channels of each layer is used for the
candidate feature. The speciﬁc feature extraction in multi-layer
image channels can be seen in Fig. 4.

Multi-stage Cascade AdaBoost Cascade AdaBoost is a
popular method for object detection. Based on multi-layer
image channels, we propose the multi-stage cascade AdaBoost
for pedestrian detection. Fig. 5 gives the speciﬁc explanations
about multi-stage cascade. The features in Si are learned from
the candidate features Fi of Li, where i=1, 2, ..., N. Firstly, k1
weak classiﬁers in S1 are learned from the candidate features
F1 extracted from L1. Based on the hard negative samples
and positive samples, k2 weak classiﬁers in S2 are then learned
from F2. The remaining stages are trained in the same manner.
Finally, multi-stage (i.e., N-stage) cascade AdaBoost classiﬁer
can be obtained. This strong classiﬁer H(x) can be expressed
as the following equation:

ki(cid:88)

kN(cid:88)

k1(cid:88)
N(cid:88)

j=1

ki(cid:88)

H(x) =

=

hj
1(x) + ... +

hj
i (x) + ... +

j=1

j=1

hj
i (x),

hj
N (x)

(1)

i=1

j=1

where x represents the samples (windows), hj
i (x) represents
the j-th weak classiﬁer in Stage i. k1, k2, ..., kN are the
number of weak classiﬁers in each stage, respectively. How
to set the value of k1, k2, ..., kN is an open problem. In this
paper, one of simple structure is used as the follows:

k1 = NAll/2,
k2 = k3 = ... = kN = NAll/(2 × (N − 1)),

(2)

.........One-orderHigh-orderHigh-orderZero-order(a) (b)…Feature ExtractionMulti-stage Cascade...+............F1F2F3FNS1++...+++...+++...++…S2S3SN11kh11h12h22kh13h33kh1Nh1Nkh...............+()Hx++++++++++++5

TABLE II

√

MISS RATES (MR) OF MCF BASED ON HOG+LUV AND THE DIFFERENT
LAYERS IN CNN.
MEANS THAT THE CORRESPONDING LAYER IS USED.

HOG+LUV IS ALWAYS USED FOR THE FIRST LAYER. THE LAYERS IN

ALEXNET OR VGG16 ARE USED FOR THE REMAINING LAYERS.

Name

MCF-2
MCF-3
MCF-4
MCF-5
MCF-6

Name

MCF-2
MCF-3
MCF-4
MCF-5
MCF-6

HOG
LUV
√
√
√
√
√

HOG
LUV
√
√
√
√
√

AlexNet

C1

C2

C3

√
√

√

√
√
√

VGG16

C1

C2

C3

√
√

√

√
√
√

C4
√
√
√
√

C4
√
√
√
√

C5
√
√
√
√
√

C5
√
√
√
√
√

MR (%) ∆ MR (%)

20.08
18.43
17.40
18.01
17.29

N/A
1.65
2.68
2.07
2.79

MR (%) ∆ MR (%)

18.52
17.14
15.40
14.78
14.31

N/A
1.38
3.12
3.74
4.21

A. Self-Comparison of MCF

In this section, some intermediate experimental results on
original Caltech training set are reported to show how to setup
the effective and efﬁcient MCF. Some speciﬁc experimental
setup is as follows. HOG+LUV are used for the ﬁrst layer.
The convolutional layers in CNN (i.e., AlexNet or VGG16)
correspond to the remaining layers. The feature extraction in
HOG+LUV is ACF. Feature extraction in the layers of CNN is
just zero-order feature (single pixel). To speed up the training,
negative samples are generated by ﬁve round training of
original ACF [11], where the number of trees in each round are
32, 128, 512, 1024, and 2048, respectively. Finally, multi-stage
cascade which consists of 4096 level-2 decision trees is learned
based on these negative samples and positive samples. The ﬁrst
stage contains the ﬁrst 2048 decision trees. The remaining
stages equally split the remaining 2048 decision trees. For
example, HOG+LUV and C2 to C5 of CNN construct a
ﬁve-layer image channels. Then, the corresponding ﬁve-stage
cascade can be learned. The ﬁrst stage S1 has 2048 weak
classiﬁers. The remaining stages (i.e., S2-S5) each have 512
weak classiﬁers. Miss Rates are log-averaged over the range
of FPPI = [10−2,100], where FPPI represents False Positive
Per Image.

√

Table II shows Miss Rates (MR) of MCF based on
HOG+LUV and the different layers in CNN. The results based
on AlexNet and VGG16 are both shown here.
means that
the corresponding layer is used for MCF. HOG+LUV image
channels are always used for the ﬁrst layer. The layers (i.e.,
C1, C2, ... or C5) are used for the remaining layers. MCF-N
means that there are N layers in MCF. For example, MCF-3 in
Row 3 are generated by HOG+LUV, C4 and C5 of AlexNet.
The ﬁrst layer is HOG+LUV image channels. The second
layer is the fourth convolutional layer (i.e., C4) of AlexNet.
The last layer is the ﬁfth layer (i.e., C5) of AlexNet. Based
on multi-layer image channels, the corresponding multi-stage
cascade is learned. There are the following observations from

Fig. 6. Test process of the fast version of MCF where the technique of NMS
is used to eliminating highly overlapped detection windows with lower scores.

by sampling one image per 30 frames. It results in 4250
images for training, where there are 1631 positive samples.
The corresponding training data is called Caltech.

To enlarge the training samples, Caltech10x is used. It
samples one image per 3 frames in the training videos. As
a result, there are 42,782 images in which there are 16,376
positive samples. Please note that the testing data is same
as [32], [34] whenever the Caltech or Caltech10x is used. It
contains 4024 images in which there are 1014 pedestrians.

The ﬁrst layer in MCF is HOG+LUV image channels [10],
which contains one normalized gradient magnitude channel,
six histograms of oriented gradient channels, and three color
channels. Two popular CNN models (i.e., AlexNet [26] and
VGG16 [25]) are used for constructing the remaining layers
in MCF. Instead of using original input size 227 × 227 or
224 × 224, we use the size 128 × 64 for pedestrian detection.
For AlexNet, stride 4 in the ﬁrst convolutional layer is replaced
by stride 2. The input size 6×6 in the ﬁrst full connected layer
is replaced by the size 8× 4. For VGG16, the input size 7× 7
of the ﬁrst full connected layer is replaced by the size 4 × 2.
The other initial parameters follow the pre-trained models on
ImageNet. The ﬁnal parameters in AlexNet and VGG16 are
ﬁne-tuned on the Caltech10x.

Feature extraction in L1 (i.e., HOG+LUV) is ACF [11]
(Section IV.A) or NNNF [31] (Section IV.B). Feature ex-
traction in the remaining layers (i.e., the layers of CNN) is
zero-order feature (single pixel). The ﬁnal classiﬁer consists
of 4096 level-2 or level-4 decision trees. The decision tree
number of each stage are k1 = 2048, k2 = k3 = ... = kN =
2048/(N − 1), respectively. N is the number of the layers in
MCF.

Eliminate highly overlapped windows with lower scoresTest ImageCompute image channels in L1Classify detection windows by S1Compute image channels in LNClassify detection windows by SNNMSPedestrian windowsGenerate detection windowsCompute image channels in L2Classify detection windows by S2...RejectAcceptRejectAcceptRejectNon-Pedestrian windowsAccept...EliminateTABLE III

REJECTED NUMBER AND REJECTED RATIO BY THE STAGES IN MCF-6 ARE

SHOWN. ‘*’ MEANS THAT THE AVERAGE NUMBER OF DETECTION

WINDOWS ACCEPTED BY STAGE 1 ARE SHOWN.

HOG+LUV and AlexNet
Number

HOG+LUV and VGG16
Number

Stage

S1
S2
S3
S4
S5
S6
Total

159*
35
35
21
14
8
113

Ratio
N/A
22.0%
22.0%
13.2%
8.8%
5.0%
71.0%

159*
23
21
33
29
15
121

Ratio
N/A
14.5%
13.2%
20.8%
18.2%
9.4%
76.1%

TABLE IV

MISS RATES (MR) AND DETECTION TIME OF MCF-2 AND MCF-6.

MCF-2 IS BASED ON HOG+LUV AND C5 OF CNN. MCF-6 IS BASED ON

HOG+LUV AND C1-C5 OF CNN.

HOG+LUV and AlexNet
MCF-2
20.08
2.99

MCF-6
17.29
2.30

HOG+LUV and VGG16
MCF-2
18.52
7.69

MCF-6
14.31
5.37

MR (%)
Time (s)

Table II: 1) Compared to MCF-2, MCF-N (N>2) usually
achieves the better performance. For example, the miss rate
of MCF-6 based on VGG16 in the last row is lower than
that of MCF-2 by 4.21%; 2) Generally, with increase of
the layer number,
the miss rate of MCF becomes lower
and the detection performance becomes better. The above
observations demonstrate that the middle layers in CNN can
enrich the feature abstraction. It means that each layer in CNN
contains some discriminative features, which can be used for
classiﬁcation.

Table III shows the average number and the ratio of de-
tection windows rejected by each stage in MCF-6. MCF-6 is
based on HOG+LUV and all the ﬁve convolutional layers in
CNN. Thus, the multi-stage cascade AdaBoost in MCF has six
stages from S1 to S6. ‘*’ means that the average number of
detection windows accepted by stage 1, instead of that rejected
by stage 1, is shown. As the weak classiﬁers in S1 are both
learned from HOG+LUV, the number of detection windows
accepted by S1 are same (i.e., 159). Among the 159 accepted
detection windows, about 71% and 76.1% detection windows
are rejected by the cascade based on AlexNet and that based on
VGG16, respectively. Overall, the multi-stage cascade based
on VGG16 can reject more detection windows. Speciﬁcally,
the ﬁrst two stage stages (i.e., S2 and S3) based on AlexNet
reject more detection windows than that based on VGG16.
The middle two stages (i.e., S4 and S5) based on VGG16 can
reject more detection windows than that based on AlexNet.

As multi-stage cascade can reject many detection windows
by the ﬁrst few stages, MCF can accelerate the detection speed.
Table IV compares the detection time and detection perfor-
mance between MCF-2 and MCF-6. MCF-2 uses HOG+LUV
and C5 in CNN to construct two-layer image channels. Then
two-stage cascade is learned. MCF-6 uses HOG+LUV and
all the ﬁve convolutional layers from C1 to C5 in CNN to

6

MISS RATES AND DETECTION TIME VARY WITH θ. MCF USED HERE IS

BASED ON HOG+LUV AND ALEXNET.

TABLE V

θ

INF
0.50
0.80
0.85
0.90

MR (%)
20.08
23.70
20.97
20.30
19.82

MCF-2

Time (s)

2.99
0.44
1.15
1.76
2.03

MCF-6

Time (s)

2.30
0.34
0.86
1.35
1.57

MR (%)
17.29
21.65
18.06
17.34
17.32

construct six-layer image channels. Then six-stage cascade is
learned. The detection time shown in Table IV is based on the
common CPU (i.e., Intel Core i7-3700). No matter CNN model
is AlexNet or VGG16, MCF-6 have the better performance and
the faster detection speed. For example, based on VGG16, the
miss rates of MCF-2 and MCF-6 are 18.52% and 14.31%,
respectively. The detection times of MCF-2 and MCF-6 are
7.69s and 5.37s, respectively. Thus, the miss rate of MCF-6 is
lower than that of MCF-2 by 4.21%, while the speed of MCF-
6 is 1.43 times faster than that of MCF-2. The reasons can be
explained as the following: 1) As MCF-6 uses all the layers
in CNN to learn the classiﬁer, it can learn more abundant
features. Thus, it has a better performance. 2) MCF-2 needs
to calculate all the layers of CNN (i.e., C1 to C5) before
classifying the detection windows accepted by S1. MCF-6 just
needs to calculate the i-th layer of CNN before classifying the
detection windows by Si (i=2,3,...,6). In the Table III, MCF-6
reject 66.7% detection windows before S6. Thus, MCF-6 has
faster detection speed than MCF-2.

Though the speed of MCF-6 is faster than that of MCF-2,
it’s still very slow. To further accelerate the detection speed,
the highly overlapped detection windows with lower scores
accepted by the ﬁrst stage (i.e., S1) are eliminated by NMS.
As stated in section III.B, the threshold θ is an important factor
to balance detection speed and detection performance. Table
V shows that miss rates and detection time vary with θ. MCF-
2 and MCF-6 based on HOG+LUV and AlexNet in Table IV
are used for the baseline (i.e., θ =INF). When θ = 0.5, the
detection speed is very fast, but the detection performance
drops rapidly. For MCF-6, the detection speed of MCF-6 with
θ = 0.5 is 6.76 times faster than original MCF, while the
miss rate of MCF with θ = 0.5 is higher than original MCF
by 3.36%. Thus, it’s not a good choice. When θ = 0.9, the
detection performance is almost no loss, while the detection
speed is not signiﬁcantly improved. Thus, the trade-off choice
is θ = 0.8. With little performance loss (e.g., 0.77%) in MCF-
6, it’s 2.67 times faster than original MCF. In the following
section, MCF with θ = 0.8 are called MCF-f.

Table VI summarizes MCF-2, MCF-6 and MCF-6-f. MCF-
6-f is the fast version of MCF-6, where the highly overlapped
detection windows are eliminated after the ﬁrst stage. There
are the following observations: 1) MCF-6 and MCF-6-f both
have the lower miss rates. Speciﬁcally, MCF-6 and MCF-6-
f based on AlexNet have lower miss rates than MCF-2 by
2.79% and 2.02%, respectively. MCF-6 and MCF-6-f based
on VGG16 have lower miss rates than MCF-2 by 4.21% and

MISS RATE (MR) AND DETECTION TIME OF MCF-2, MCF-6, AND

TABLE VI

MCF-6-F. MCF-2 IS BASED ON HOG+LUV AND C5 IN CNN. MCF-6 IS

BASED ON HOG+LUV AND C1-C5 IN CNN. MCF-6-F IS THE FAST

VERSION OF MCF-6.

MR (%)
Time (s)

MR (%)
Time (s)

MCF-2
20.08
2.99

MCF-2
18.52
7.69

HOG+LUV and AlexNet

MCF-6
17.29
2.30

HOG+LUV and VGG16

MCF-6
14.31
5.37

MCF-6-f

18.06
0.86

MCF-6-f

14.89
1.89

3.63%, respectively. 2) MCF-6 and MCF-6-f is faster than
MCF-2. For example, detection time of MCF-2 based VGG16
is 7.69s and that of MCF-6-f based on VGG16 is 1.89s. It
means that detection speed of MCF-6-f is 4.07 times faster
than that of MCF-2. 3) With little performance loss, MCF-6-f
have faster detection speed than MCF-6. The loss of MCF-6-f
based on AlexNet is 0.77%, and the loss of MCF-6-f based
on VGG16 is 0.58%.

B. Comparison with the state-of-the-Art

In this section, MCF is based on HOG+LUV and all the
ﬁve convolutional layers (i.e., C1-C5) in VGG16. Thus, MCF
contains six-layer image channels. The features extracted in
the ﬁrst layer (i.e., HOG+LUV) are NNNF [31], which is
one of the state-of-the-art features. The features extracted
in the remaining ﬁve layers (i.e., C1-C5) are zero-order
feature (single pixel). Caltech10x is used for training the ﬁnal
classiﬁer. To speedup the training process, negative samples
are accumulated by ﬁve rounds of original NNNF, where the
number of the trees in each round is 32, 128, 512, 2048, and
4096, respectively. The resulting classiﬁer contains 4096 level-
4 decision trees. S1 contains 2048 decision trees. S2-S6 each
have 409 decision trees. In [37], Zhang et al. provided a new,
high quality ground truth for the training and test sets. The
new annotations of Caltech10x is also used for training MCF.
Original Caltech test set and new Caltech test set are both used
for the evaluations.

Fig. 7 compares MCF with some state-of-the-art methods on
the original annotations of the test set. ACF [11], LDCF [22],
Checkboards [7], NNNF [31], DeepParts [2], and CompACT-
Deep [1] are used. ACF [10] are trained on INRIA dataset
[12]. The other methods are trained based on Caltech10x
dataset. MCF achieves the state-of-the-art performance, which
outperforms CompACT-Deep [1], DeepParts [2], NNNF [31],
and Checkboards [7] by 1.35%, 1.49%, 6.38%, and 8.07%,
respectively.

Miss rates and Frames Per Second (FPS) of some methods
based on CNN are visualized in Fig. 8. Detection time of the
methods are all tested on the common CPU (i.e., Intel Core
i7-3700). The best choice is that the miss rate is as small
as possible while FPS is as large as possible. Though ACF
[11] has very fast detection speed (9.49 fps), miss rate of
ACF are very large. Fast RCNN reported in [4] has the better

7

Fig. 7. ROC of Caltech test set (reasonable).

Fig. 8. Miss rates and FPS on Caltech pedestrian dataset are shown. Detection
time of the methods are all tested on the common CPU (i.e., Intel Core i7-
3700).

Fig. 9. ROC of Caltech test set using the new and accurate annotations
[37]. Miss rates log-averaged over the FPPI range of [10−2,100] and the
FPPI range of [10−4,100] are shown. They are represented by M R−2 and
M R−4, respectively. M R−2 (M R−4) are shown in the legend.

performance (11.82%), but the speed is very slow. MCF-f is
the fast version of MCF with little performance loss (0.65%).
Compared to Fast RCNN [4], MCF is 4.5 times faster than
it, while MCF has also lower miss rate than it by 0.77%.
Therefore, MCF has a better trade-off between detection time
and detection performance.

Based on the new and accurate annotations of the Caltech
test set, Fig. 9 compares MCF with some state-of-the-art meth-

10−310−210−1100101.05.10.20.30.40.50.64.801false positives per imagemiss rate  94.73% VJ68.46% HOG63.26% LatSvm−V244.22% ACF−Caltech24.80% LDCF18.47% Checkerboards16.78% NNNF11.89% DeepParts11.75% CompACT−Deep10.40% MCF1/161/81/41/21/12481632101.1101.3101.5101.7frames per secondlog−average miss rate  [9.49fps/44.20%] ACF[14.10fps/53.90%] Crosstalk[0.16fps/21.90%] LatSvm−L2[0.63fps/34.60%] InformedHaar[0.12fps/29.20%] SpatialPooling[3.62fps/24.80%] LDCF[0.12fps/21.90%] SpatialPooling+[0.50fps/18.47%] Checkboards[1.14fps/16.78%] NNNF[0.12fps/11.82%] Fast RCNN[0.54fps/11.05%] MCF−f10−310−210−1100101.05.10.20.30.40.50.64.801false positives per imagemiss rate  92.67% (95.81%) VJ64.68% (78.06%) HOG61.07% (70.50%) LatSvm−V241.81% (57.88%) ACF−Caltech23.72% (38.27%) LDCF15.81% (28.57%) Checkerboards14.39% (25.50%) NNNF12.90% (25.15%) DeepParts9.15% (18.84%) CompACT−Deep7.98% (15.45%) MCF8

[12] N. Dalal and B. Triggs, “Histograms of Oriented Gradients for Human

Detection,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2005.

[13] P. Felzenszwalb, R. Girshick, D. McAllester, and D. Ramanan, “Object
Detection with Discriminatively Trained Part Based Models,” IEEE Trans.
Pattern Anal. Mach. Intell., vol. 32, no. 9., pp. 1627-1645, 2010.

[14] P. Viola and M. Jones, “Robust Real-Time Face Detection,” Int. J.

Comput. Vis., vol. 57, no. 2, pp. 137-154, 2004.

[15] R. Girshick, J. Donahue, T. Darrell, and J. Malik, “Rich Feature
Hierarchies for Accurate Object Detection and Semantic Segmentation,”
in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2014.

[16] J. R. R. Uijlings, K. E. A. van de Sande, T. Gevers, and A. W. M.
Smeulders, “Selective Search for Object Recognition,” Int. J. Comput.
Vis., vol. 104, no. 2, pp. 154-171, 2013.

[17] M. Cheng, Z. Zhang, W. Lin, and P. Torr, “BING: Binarized Normed
Gradients for Objectness Estimation at 300fps,” in Proc. IEEE Conf.
Comput. Vis. Pattern Recognit., 2014.

[18] R. Girshick, “Fast R-CNN,” in Proc. Int. Conf. Comput. Vis., 2015.
[19] S. Bell, C. Lawrence Zitnick, K. Bala, and R. Girshick, “Inside-Outside
Net: Detecting Objects in Context with Skip Pooling and Recurrent
Neural Networks,” CoRR abs/1512.04143, 2015.

[20] L. Bourdev and J. Brandt, “Robust Object Detection Via Soft Cascade,”

in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2005.

[21] S. Zhang, C. Bauckhage, and A. B. Cremers, “Informed Haar-like
Features Improve Pedestrian Detection,” in Proc. IEEE Conf. Comput.
Vis. Pattern Recognit., 2014.

[22] W. Nam, P. Doll´ar, and J. H. Han, “Local Decorrelation for Improved

Detection,” in Proc. Adv. Neural Inf. Process, 2014.

[23] B. Hariharan, J. Malik, and D. Ramanan. “Discriminative Decorrelation
for Clustering and Classiﬁcation,” in Proc. Eur. Conf. Comput. Vis., 2012.
[24] Y. Pang, S. Wang, and Y. Yuan, “Learning Regularized LDA by
Clustering,” IEEE Trans. Neural Netw. Learning Syst., vol. 25, no. 12,
pp. 2191-2201, 2014.

[25] K. Simonyan and A. Zisserman, “Very Deep Convolutional Networks

for Large-Scale Image Recognition,” CoRR abs/1409.1556, 2015.

[26] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet Classiﬁcation
with Deep Convolutional Neural Networks,” in Proc. Adv. Neural Inf.
Process, 2012.

[27] D. Zhang, J. Han, J. Han, and L. Shao, “Cosaliency Detection Based on
Intrasaliency Prior Transfer and Deep Intersaliency Mining,” IEEE Trans.
Neural Netw. Learning Syst. vol. 25, no. 12, 2015.

[28] J. Li, X. Mei and D. Prokhorov, “Deep Neural Network for Structural
Prediction and Lane Detection in Trafﬁc Scene,” IEEE Trans. Neural
Netw. Learning Syst., 2016.

[29] S. Ren, K. He, R. Girshick, and J. Sun, “Faster R-CNN: Towards Real-
Time Object Detection with Region Proposal Networks,” in Proc. Adv.
Neural Inf. Process, 2015.

[30] B. Yang, J. Yan, Z. Lei, and S. Z. Li, “Convolutional Channel Features,”

in Proc. Int. Conf. Comput. Vis., 2015.

[31] J. Cao, Y. Pang, and X. Li, “Pedestrian Detection Inspired by Appear-

ance Constancy and Shape Symmetry,” CoRR abs/1511.08058, 2015.

[32] P. Doll´ar, C. Wojek, B. Schiele, and B. Perona, “Pedestrian detection: A
benchmark,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2009.
[33] Y. Pang, J. Cao, and X. Li, “Learning Sampling Functions for Efﬁcient

Object Detection,” IEEE Transcations on Cybernetics, 2015.

[34] P. Doll´ar, C. Wojek, B. Schiele, and P. Perona, “Pedestrian Detection:
An Evaluation of the State of the Art,” IEEE Trans. Pattern Anal. Mach.
Intell., vol. 34, no. 4, pp. 743-761, 2012.

[35] H. Li, Z. Lin, X. Shen, J. Brandt, and G. Hua, “A Convolutional Neural
Network Cascade for Face Detection,” in Proc. IEEE Conf. Comput. Vis.
Pattern Recognit., 2015.

[36] C. Zhang and P. Viola, “Multiple-Instance Pruning For Learning Efﬁ-

cient Cascade Detectors,” in Proc. Adv. Neural Inf. Process, 2007.

[37] S. Zhang, R. Benenson, M. Omran, J. Hosang, and B. Schiele, “How
Far are We from Solving Pedestrian Detection?” CoRR abs/1602.01237,
2016.

ods: CompACT-Deep [1], DeepParts [2], Checkboards [7], and
NNNF [31]. Miss rates log-averaged over the FPPI range of
[10−2,100] and the FPPI range of [10−4,100] are shown. They
are represented by M R−2 and M R−4. M R−2 (M R−4) are
shown in the legend. MCF is trained based on the Caltech10x
with the new annotations. M R−2 and M R−4 of MCF achieve
7.98% and 15.45%, respectively. They are superior to all the
other methods. Speciﬁcally, M R−2 of MCF is 1.17%, 4.92%,
and 6.41% lower than that of CompACT-Deep [1], DeepParts
[2], and NNNF [31]. Compared to M R−2 of MCF, M R−4 of
MCF has the better performance. Speciﬁcally, M R−4 of MCF
is 3.39%, 9.70%, and 10.05% lower than that of CompACT-
Deep [1], DeepParts [2], and NNNF [31]. It means that MCF
stably outperforms the other state-of-the-art methods.

V. CONCLUSION

In this paper, we have proposed a unifying framework,
which is called Multi-layer Channels Features (MCF). Firstly,
the handcrafted image channels and the layers in CNN con-
struct
the multi-layer image channels. Then a multi-stage
cascade are learned from the features extracted in the layers,
respectively. The weak classiﬁers in each stage are learned
from the corresponding layer. On the one hand, due to the
much more abundant candidate features, MCF achieves the
state-of-the-art performance on Caltech pedestrian dataset (i.e.,
10.40% miss rate). Using the new and accurate annotations of
the Caltech pedestrian dataset, miss rate of MCF is 7.98%,
which is superior to other methods. On the other hand, due to
the cascade structure, MCF rejects many detection windows by
the ﬁrst few stages and then accelerates the detection speed. To
further speedup the detection, the highly overlapped detection
windows are eliminated after the ﬁrst stage. Finally, MCF with
VGG16 can run on the CPU by 0.54 fps.

REFERENCES

[1] Z. Cai, M. Saberian, and N. Vasconcelos, “Learning Complexity-Aware
Cascades for Deep Pedestrian Detection,” in Proc. Int. Conf. Comput.
Vis., 2015.

[2] Y. Tian, P. Luo, X. Wang, and X. Tang, “Deep Learning Strong Parts for

Pedestrian Detection,” in Proc. Int. Conf. Comput. Vis., 2015.

[3] Y. Tian, P. Luo, X. Wang, and X. Tang, “Pedestrian Detection Aided
by Deep Learning Semantic Tasks,” in Proc. IEEE Conf. Comput. Vis.
Pattern Recognit., 2015.

[4] J. Li, X. Liang, S. Shen, T. Xu, and S. Yan, “Scale-Aware Fast R-CNN

for Pedestrian Detection,” CoRR abs/1601.04798, 2015.

[5] P. Sermanet, K. Kavukcuoglu, S. Chintala, and Y. LeCun. “Pedestrian
Detection with Unsupervised Multi-Stage Feature Learning,” in Proc.
IEEE Conf. Comput. Vis. Pattern Recognit., 2013.

[6] J. Hosang, M. Omran, R. Benenson, and B. Schiele, “Taking a Deeper
Look at Pedestrians,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit.,
2015.

[7] S. Zhang, R. Benenson, and B. Schiele, “Filtered Channel Features
for Pedestrian Detection,” in Proc. IEEE Conf. Comput. Vis. Pattern
Recognit., 2015.

[8] R. Benenson, M. Omran, J. Hosang, and B. Schiele, “Ten Years of
Pedestrian Detection, What Have We Learned?” in Proc. Eur. Conf.
Comput. Vis., 2014.

[9] R. Benenson, M. Mathias, T. Tuytelaars, and L. Van Gool, “Seeking
the Strongest Rigid Detector,” in Proc. IEEE Conf. Comput. Vis. Pattern
Recognit., 2013.

[10] P. Doll´ar, Z. Tu, P. Perona, and S. Belongie, “Integral Channel Features,”

in Proc. Brit. Mach. Vis. Conf., 2009.

[11] Doll´ar, R. Appel, S. Belongie, and P. Perona, “Fast Feature Pyramids
for Object Detection,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 36,
no. 8, pp. 1532-1545, Aug. 2014.

