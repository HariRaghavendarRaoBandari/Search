Regularization to orthogonal-polynomials ﬁtting with application to magnetization

data

1 Department of Physics Science and Technology,

Bao Xu1,2

Baotou Teachers’ College, Baotou 014030, China and

2 Key Laboratory of Magnetism and Magnetic Materials at Universities

of Inner Mongolia Autonomous Region, Baotou 014030, China

An obstacle encountered in applying orthogonal-polynomials ﬁtting is how to select out the proper
ﬁtting expression. By adding a Laplace term to the error expression and introducing the concept
of overﬁtting degree, a regularization and corresponding cross validation scheme is proposed for
two-variable polynomials ﬁtting. While the Fortran implementation of above scheme is applied
to magnetization data, a satisfactory ﬁtting precision is reached, and overﬁtting problem can be
quantitatively assessed, which therefore oﬀers the quite reliable base for future comprehensive in-
vestigations of magnetocaloric and phase-transition properties of magnetic functional materials.

PACS numbers: 07.05.Kf, 02.60.Ed, 75.10.-b

I.

INTRODUCTION

Surface ﬁt using two-variable orthogonal polynomials has reasonable advantages.[1] One among many is not necessary
to solve a normal equation which is probably ill-conditioned. A second one usually mentioned in literatures is that,
the coeﬃcients of orthogonal polynomials in the ﬁtting expression do not depend on those of preceding polynomials.
Hence, ﬁt with two-variable orthogonal polynomials has been widely used in the ﬁeld such as image processing. While
applying it to magnetization data, another advantage that might be worth mentioning but rarely mentioned is rooted
in physics. When pressure in the sample chamber does not change, magnetization of the sample, M , as one of the
thermodynamic functions, can be completely determined as a function of two independent parameters, magnetic ﬁeld
H and temperature T .

Application of this method to magnetization data is not new. For example, a two-step category has been employed to
ﬁt the magnetization data of gadolinium, and the ﬁtting expression is subsequently used to estimate the corresponding
magnetic entropy change.[2] The main features of our method are as follow.

Firstly, in recursively generating orthogonal polynomials with classical or modiﬁed Gram-Schmidt schemes, orthog-
onality of the polynomials will progressively deteriorate. So an iterating scheme is selected in order to preserve the
orthogonality. (Generally speaking, as has been pointed out before, one more orthogonalization process is suﬃcient
to signiﬁcantly improve the orthogonality.)

Secondly, a regularization method is introduced. In previous ﬁttings to magnetization data, the overﬁtting problem
has rarely been taken into account. The reason for this probably originates in that not very-high-order polynomials
are included in the ﬁnal ﬁtting expression. However, our numerical results suggest that ﬂuctuations indeed appear
between experimentally recorded data, and such ﬂuctuations become even severer near to the boundaries. So, we are
convinced that overﬁtting occurs and has to be addressed. That’s why the regularization scheme is introduced to
relieve the probable overﬁtting. Although the speciﬁc formulations are diﬀerent, the present idea of regularization is
similar to that used in reference,[3] which uses gridding and interpolation to obtain the normal equation to be solved.
Thirdly, a cross-validation scheme is employed to select out the proper regularization parameter. The main idea is
to divide the experimentally recorded data into three groups, namely, the training, cross-validation, and test groups.
Among them, the training group includes the most elements, and the other two have less data. For each of diﬀerent
regularization parameters, use the training group to generate orthogonal polynomials and determine the unknowns
in the ﬁtting expression by minimizing training error; apply the cross-validation group to selecting out the optimal
regularization parameter that corresponds to the minimum of cross-validation error; and assess the applicability of
the determined expression by computing test error. For orthogonal polynomials ﬁtting, it’s found that the common
cross-validation method can not eﬃciently select out the proper regularization parameter. To overcome this problem,
the concept of overﬁtting degree is introduced and used to monitor ﬁtting performance.

6
1
0
2

 
r
a

 

M
1
1

 
 
]

A
N
.
s
c
[
 
 

1
v
2
3
5
3
0

.

3
0
6
1
:
v
i
X
r
a

4Project supported by National Natural Science Foundation of China (Grant No. 61565013) and The Scientiﬁc Research Project of the Inner

Mongolia Autonomous Region Colleges and Universities (Grant No. NJZZ199).

4Corresponding author. E-mail: xubao79@163.com

2

Fourthly, two methods to implement sampling are provided. In principle, a good ﬁtting expression should not be
aﬀected by the sample method to obtain the training group. However, our results show an apparent diﬀerence in the
cross-validation performance, which is associated with the sampling with respect to magnetic ﬁeld or temperature.
The reason for the diﬀerent performance is attributed to the (dimensionless and size-normalized) averaged increment
of recorded magnetic ﬁelds unequal to that of recorded temperatures within the experimental measurement range.

Fifthly, an extended-precision version of the algorithm is used. On completely determining the ﬁtting expression,
the magnetization at particular magnetic ﬁeld and temperature calculated from the orthogonal polynomials, should
not be diﬀerent from the value estimated from the linearly independent functions, which are used to generate those
orthogonal polynomials. However, in performing a double-precision version of the algorithm, we noticed in practice
that the values obtained by the two methods are actually diﬀerent. So, during recursively computing the values
of linearly independent functions and subsequently generating orthogonal polynomials, the error accumulation is
signiﬁcant. That’s why we implement the algorithm at an extended-precision level in spite of more computational
time.

Sixthly, an equally-spaced distribution of both Hi and Tj is not a prerequisite to ﬁt the magnetization data
M (Hi, Tj). In principle, our method is also suitable for magnetization data randomly distributed over the H-T plane.
Finally, after obtaining the ﬁtting expression, it’s ready to estimate the magnetocaloric quantities at arbitrary mag-
netic ﬁeld and temperature within the measuring range, and comprehensively investigate magnetic-phase-transition
properties including the order of transition, critical exponents, anomalous speciﬁc heat, and so forth.

The rest of this paper is organized as follows: Next section provides the general formulations and key algorithms

used in this work; Section 3 applies the algorithm to magnetization data; and conclusions are put in Section 4.

II. GENERAL FORMULATION AND KEY ALGORITHMS

Following formulations are quite general and not restricted to magnetization data. So the experimental data are

denoted by as xi, yi and zi instead of Hi, Ti and Mi.

For experimental data (xi, yi, zi) with i = 1, 2,··· , N , two-variable orthogonal polynomials Ps(x, y) are deﬁned as

A. Two-variable orthogonal polynomials

satisfy

N(cid:88)

i=1

Pt(xi, yi)Ps(xi, yi) = δt,s

N(cid:88)

i=1

(Pt(xi, yi))2 .

(1)

where, δt,s is the Kronecker δ-symbol. By using the Gram-Schmidt orthogonalization process (classical or modiﬁed
scheme) to the linearly independent functions

ht(x, y) = 1, x, y, x2, xy, y2,···

with integer numbers 0 ≤ t ≤ L. Ps(x, y) can be recursively generated as follows

Ps(x, y) = asshs(x, y) +

astPt(x, y), 0 ≤ s ≤ S.

In above expression, S is the largest index of the generated orthogonal polynomials. It is smaller than or equal to the
number of created linear independent function L. If we assign as,0 = 1, coeﬃcients ass and ast (1 ≤ t < s ≤ S) are
determined as

s−1(cid:88)

t=0

(cid:80)N
(cid:80)N
(cid:80)N
(cid:80)N

ass = −

i=1 (P0(xi, yi))2

i=1 P0(xi, yi)hs(xi, yi)

ast = −ass

i=1 Pt(xi, yi)hs(xi, yi)

i=1 (Pt(xi, yi))2

,

.

(2)

(3)

Note that the orthogonal polynomials deﬁned above are not normalized. The normalized version can be conveniently

obtained through dividing coeﬃcients ast (t = 0,··· , s) by 2-norm

(cid:104)(cid:80)N
i=1 (Pt(xi, yi))2(cid:105)1/2
(cid:35)−1

as

ass = −

P0(xi, yi)hs(xi, yi)

,

(cid:34) N(cid:88)
N(cid:88)

i=1

ast = −ass

Pt(xi, yi)hs(xi, yi).

i=1

B. Coeﬃcients of orthogonal polynomials in the ﬁtting expression

After the orthogonal polynomials are obtained, we can expand the ﬁtting expression f (x, y) with Ps(x, y) as

3

(4)

(5)

(6)

(7)

(8)

S(cid:88)

t=0

f (x, y) =

btPt(x, y),

[f (xi, yi) − zi]2

N(cid:88)

i=1

σ1 =

1
N

N(cid:88)

bt =

ziPt(xi, yi),

(0 ≤ t ≤ S).

where S ≤ L denotes the maximum index of orthogonal polynomials used in the ﬁtting expression. By minimizing
ﬁtting error

the coeﬃcient of normalized orthogonal polynomials in the ﬁtting expression is determined as

i=1

C.

Iterating orthogonalization

Using above orthogonalization processes, it’s found that orthogonality becomes poorer and poorer. Although it’s
better than the the classical (CGS) scheme, performance of the modiﬁed Gram-Schmidt (MGS) scheme unavoidably
becomes poor with increasing the largest index of orthogonal polynomials, S. Since the orthogonality is closely related
to ﬁtting precision, we use the following iterating scheme (IGS) to improve the orthogonality.

Step 1 Recursively compute the values of linearly independent functions h(i, s) (i = 1, ..., N ; s = 0, ..., L).
(Assume that the ﬁrst s polynomials Pt(:) with 0 ≤ t ≤ s − 1 have been orthonormalized and assigned to h(:, t)
with 0 ≤ t ≤ s − 1. Estimate the value of the s + 1-th orthonormalized polynomial Ps(:) and assign it to h(:, s); and
save coeﬃcients a(s, t) with 0 ≤ t ≤ s and b(s). )

t=0 δ(t) · h(:, t);

(2-b) Re-orthogonalize h(:, s) = h(:, s) −(cid:80)s−1

Step 2 Re-orthogonalize h(:, s) and update a(:, :).
(2-a) Compute the modiﬁcation coeﬃcient δ(t) = (cid:104)h(:, s); h(:, t)(cid:105), 0 ≤ t ≤ s − 1;
(2-c) Update coeﬃcients a(s, t) = a(s, t) + δ(t), 0 ≤ t ≤ s − 1;
(2-d) Judge whether the orthogonality criterion is satisﬁed. If true then continue; else go back to (2-a).
Step 3 Normalize h(:, s) and update a(:, :).
(3-a) Compute the 2-norm p = ||h(:, s)||2;
(3-b) Normalize h(:, s) as h(:, s) = h(:, s)/p;
(3-c) Update coeﬃcients a(s, s) = p;
(3-d) Update coeﬃcients a(s, t) = −a(s, t) · a(s, s), 0 ≤ t ≤ s − 1.
Step 4 Compute coeﬃcients of h(:, s) in the ﬁtting expression b(s) = (cid:104)h(:, s); f (:)(cid:105).
Step 5 Judge whether ﬁtting precision matches the criterion. If true then S = s and break out the loop; else

continue.

Step 6 Subtract the projection of h(:, t) from h(:, s) and update a(:, :).
(6-a) Compute the coeﬃcients of subsequent orthogonal polynomials a(t, s) = (cid:104)h(:, t); h(:, s)(cid:105), s + 1 ≤ t ≤ L;
(6-b) Subtract the projection of h(:, t) from h(:, s) h(:, t) = h(:, t) − a(t, s) · h(:, s), s + 1 ≤ t ≤ L.
Step 7 Update s and go back to Step 2.

D. Overﬁtting and regularization

4

If the largest index S is not a big number, then the changing tendency of experimental data can not be properly
reﬂected. Now ﬁtting error is large and underﬁtting happens. For decreasing ﬁtting error, more orthogonal poly-
nomials are successively generated and added to the ﬁtting expression until the wished precision is achieved. (The
more orthogonal polynomials, the higher ﬁtting precision.) However, too many polynomials will lead to strong local
ﬂuctuations in the ﬁtted surface, and overﬁtting happens. The reason for this is that higher-order polynomials gen-
erally imply more inﬂection points. The degree of overﬁtting can be controlled by regularization like adding so-called
penalty functions into the error expression, in order to strengthen the stiﬀness of ﬁtted surfaces.

In contrast to the method of using penalty functions, we implement the regularization by adding a Laplace term
to the error expression. In essence, the Laplace method aims at suppressing the changing rate of curve slope. After
adding a Laplace term

∇2f (x, y) = 0,

with regularization parameter λ, the error expression (7) is rewritten as

It’s easy to see that the Laplace term in (9) aﬀects only coeﬃcients bt. By minimizing (9), it is obtained that

σ2 =

1
N

N(cid:88)

i=1

[f (xi, yi) − zi]2 + λ(cid:2)∇2f (xi, yi)(cid:3)2
−λRtQt +(cid:80)N

i=1 ziPt(xi, yi)

.

,

where,

bt =

(9)

(10)

(11)

(12)

λ (Qt)2 + 1

t−1(cid:88)
N(cid:88)

r=0

Rt =

Qt =

brQr,

∇2Pt(xi, yi)

i=1

λ|RtQt| (cid:29) | N(cid:88)
λ (Qt)2 (cid:29)(cid:88)

i=1

i

ziPt(xi, yi)|,

(Pt(xi, yi))2 ;

with 0 ≤ t ≤ S. We next examine whether the Laplace method above really leads to regularization. Firstly, when
λ = 0, equation(10) reduces to the non-regularized case (8). Secondly, if t ≤ 2, the Laplace term has no contribution
to bt since Q0 = Q1 = Q2 = 0 (corresponding to linear ﬁtting). Thirdly, the Laplace term starts to play its role when
t ≥ 3. If λ is large enough so that equations (11) and (12) are satisﬁed,

then bt reduces to

namely,

(cid:80)t−1

r=0 brQr

Qt

,

= −

bt ≈ − λRtQt

Qt

λ(Qt)2 = − Rt
t(cid:88)

Rt+1 =

brQr ≈ 0.

If we assume that Rt ≈ 0 when t ≥ t0, then Rt+1 = btQt + Rt ≈ btQt ≈ 0, which implies that bt ≈ 0 since Qt (cid:54)= 0.
Hence, the Laplace term introduced above makes bt rapidly decay with increasing t, so that overﬁtting is avoided and
regularization is realized.

r=0

E. Cross validation and overﬁtting degree

5

Now, we invoke a cross-validation process to select out a proper regularization parameter λ. Dividing the whole
data into three groups, one of which includes much more data, labelled “training group”, and the other two has fewer
data, labelled “cross-validation group” and “test group”, respectively. For each ﬁxed value of λ, the training group is
used to determine coeﬃcient bs by minimizing σ2 in (9), and the corresponding training error is computed as

Subsequently, the cross-validation group selects out the value of λ that minimizes the cross-validation error

(13)

(14)

(15)

Ntrain(cid:88)

i=1

Ncv(cid:88)

j=1

Ntest(cid:88)

j=1

σtr =

1

Ntrain

[f (xi, yi) − zi]2 .

σcv =

1
Ncv

[f (xi, yi) − zi]2 .

σtest =

1

Ntest

[f (xi, yi) − zi]2 .

Finally, the test group assesses the applicability of the determined ﬁtting expression by calculating the test error

In contrast to the ordinary scheme used in the ﬁeld such as machine learning, the model used here has two parameters

that require determining, namely, the number of orthogonal polynomials S and the regularization parameter λ.

By setting certain routine-terminating criterion, the optimal choice of S can be determined by minimizing the
training error σtr. A useful criterion can be deﬁned by assessing the changing tendency of ﬁtting error. For example,
on increasing S from S0, if the ﬁtting error is not apparently decreased, it is reasonable to consider S0 to be the
optimal value of S.

Another task is to determine parameter λ. Practically, we ﬁnd that both σtr and σcv decrease with enlarging S at
ﬁxed λ or with increasing λ at ﬁxed S. Thus, the optimal value of λ can not be identiﬁed to the one that minimizes
σcv.

This motivates us to construct a new quantity to characterize the degree of overﬁtting (and also underﬁtting).
Typically, when σcv is approximately equal to σtr, underﬁtting happens; if σcv is much larger than σtr, overﬁtting
occurs. We can deﬁne overﬁtting degree γ as

It is identiﬁed as underﬁtting if γ (cid:28) −1, and overﬁtting when γ (cid:29) 1.

(cid:12)(cid:12)(cid:12) σcv − σtr

σtr

(cid:12)(cid:12)(cid:12).

γ = ln

F. Size normalization

It’s noticed that the calculated value of ﬁtting error depends on the measurement unit used for experimentally
recorded data (Xi, Yi, Zi) (i = 1,··· , N ) where Xi ∈ [Xmin, Xmax], Yi ∈ [Ymin, Ymax], and Zi ∈ [Zmin, Zmax]. For
comparison purposes, we do size normalization as follow

X = fX (x) = Xmin + x(Xmax − Xmin),
Y = fY (y) = Ymin + y(Ymax − Ymin),
Z = fZ(z) = Zmin + z(Zmax − Zmin).

where, x, y, z ∈ [0, 1]. Hence, the ﬁtting error is calculated from the after-transformed data. With an inverse
transformation, the physical quantities are obtained in the measurement unit.

In dividing the experimental data into three groups, a uniform sampling algorithm is executed in order to optimize
ﬁtting performance. Original experimental data are ﬁrstly sorted in terms of a sample parameter, X or Y . Then the

G. Uniform sampling

uniform sample is executed on a pro-rata basis, which is regulated by the sampling factor, and the sorted data are
put into the training, cross-validation, and test groups according to the sampling factor. Numerical results show that
diﬀerence appears between diﬀerent sampling methods, which is attributed to the diﬀerent data density along X axis
with that along Y axis.

H. Coeﬃcients of linear independent functions hi(x, y) in the ﬁtting expression

6

After coeﬃcients ast and bs being determined, the functional value at arbitrary location (x, y) within the measuring
range can be estimated in a similar way to that used in the ﬁtting routine. Another method is to estimate from
linearly independent functions as

S(cid:88)

f (x, y) =

ctht(x, y),

(16)

t=0

where ct is the coeﬃcient of the t-th linearly independent functions in the ﬁtting expression, and can be readily
calculated from ats and bt. The latter scheme is recommended for lower computation cost.

Practically, in implementing a double-precision version of the algorithm, it’s found that the ﬁtting value calculated
from linearly independent functions signiﬁcantly diﬀers from that computed from orthogonal polynomials, when
the power exponent of the ﬁtting expression is very high. After an extended-precision algorithm is applied, the
diﬀerence decreases.
If the extended-precision operation is also used to recursively generate linearly independent
functions, the diﬀerence is not longer obvious. These facts suggest that round-oﬀ error is rapidly accumulated while
recursively computing the linearly independent functions. Hence, it is required to carefully consider the inﬂuence of
error accumulations in ﬁtting using two-variable orthogonal polynomials.

I. Some useful recursive formula

Since the ﬁtting expression is essentially the linear combination linearly independent functions h(x, y), utilizing
recursive properties of the corresponding partial derivative and integral, it’s quite convenient to compute the physical
quantities of interest. Followings are typical recursive algorithms at given x and y used in this work.

1. h(i) ← hi(x, y) used to compute linear independent functions as well as magnetization

h(0) = 1; h(1) = x; h(2) = y; s = 1;
For m ≥ 2
{s = s + m;

(cid:26)

x · h(s − m),

y · h(s + j − m − 1), 1 ≤ j ≤ m;

j = 0;

h(s + j) =
m = m + 1.}

2. h(i) ← ∂2hi(x,y)

∂x2

used to compute the x-component of Laplace term

h(j) = 0 (0 ≤ j ≤ 2, and4 ≤ j ≤ 5);
h(3) = 2; s = 3;
For m ≥ 3
{s = s + m;

 m

h(s + j) =

m = m + 1.}

m−2 · x · h(s − m),
j = 0;
y · h(s + j − m − 1), 1 ≤ j ≤ m − 2;
m − 1 ≤ j ≤ m;

0,

(17)

(18)

7

FIG. 1: Comparison of experimental data that are processed by the Origin software with the ﬁtting eﬀect using two-variable
orthogonal polynomials at regularization parameter λ = 0. Shown in (a) are recorded experimental data (3789) processed by
the Origin software. By using one half of the whole data (1895) as the training group, and implementing ﬁtting routines, ﬁtting
errors that are achieved and the number of orthogonal polynomials that are used list as follow (b) 9.77E-04, 15; (c) 9.67E-05,
49; (d) 8.75E-06, 92; (e) 9.62E-07, 156; (f) 1.68E-07, 350.

3. h(i) ← ∂2hi(x,y)

∂y2

used to compute the y-component of Laplace term

h(j) = 0 (0 ≤ j ≤ 4);
h(5) = 2; s = 3;
For m ≥ 3
{s = s + m;



h(s + j) =

m = m + 1.}

0 ≤ j ≤ 1;

0,

x · h(s + j − m), 2 ≤ j ≤ m − 1;
m−2 · y · h(s − 1), j = m;

m

(19)

III. ALGORITHM TEST: APPLICATION TO MAGNETIZATION DATA

The formula given above are quite general. For application to magnetization data, it’s only needed to replace X,
Y , Z and their corresponding size normalizations with H, T , M and H = fH (x), T = fT (y), M = fM (z). Here, we
ﬁt the magnetization data of polycrystalline samples La1.2Sr1.8Mn2O7 obtained with Physical Property Measurement
System (PPMS) of Quantum Design Company. More details can be found in reference. [4]

A. Fitting without regularization

The total number of data used is 3789. With one half of the data (1895) uses as the training group, the ﬁtting
results are shown in Fig. 1. Note that satisfactory ﬁtting precision can be reached using two-variable orthogonal

(a)(d)(b)(f)(e)(c)polynomials. However, one cannot assess the degree of overﬁtting from the ﬁtting precision, which suggests the
necessity to introduce cross-validation.

8

The whole data are uniformly sampled according to temperature and the sampling factor is set to 3, namely, two
thirds of the data being put into the training group, half of the rest one third into the cross-validation group and
half into the test group. By setting regularization parameter λ = 0, we discuss the eﬀect of the number of orthogonal
polynomials (S) on ﬁtting performance. The calculated results are shown in Table 1. For reference, the overﬁtting
degree corresponding to the test error is deﬁned in a similar way with that in (15)

(cid:12)(cid:12)(cid:12) σtest − σtr

σtr

(cid:12)(cid:12)(cid:12).

γ(cid:48) = ln

(20)

It’ noted that both γ and γ(cid:48) reﬂect ﬁtting performance, since the data in the cross-validation and test groups can
be interchanged. With increasing S, ﬁtting error decreases and overﬁtting degree increases, which suggests that the
overﬁtting degree can be used to monitor ﬁtting performance, although it can not always select out the optimal
regularization parameter.

Table 1. Fitting errors and overﬁtting degrees without regularization (λ = 0). For comparison, the training
error with S = 0 (i.e., only h0 = 1 is used) is 0.81885E − 01.

S

2
16
50
92
230

σtr

0.879011E-02
0.868127E-03
0.966997E-04
0.934213E-05
0.407959E-06

σcv

0.898435E-02
0.752606E-03
0.873995E-04
0.158334E-04
0.162667E-04

σtest

0.887978E-02
0.918941E-03
0.104586E-03
0.901059E-05
0.624748E-06

γ

-3.81
-2.02
-2.34
-0.36
3.66

γ(cid:48)
-4.58
-2.84
-2.51
-3.34
-0.63

B. Fitting with regularization

Table 2. Fitting errors and overﬁtting degrees at diﬀerent regularization parameters λ with the ﬁxed number
of orthogonal polynomials S = 78. Parameter λ is expressed as λ = e−x for clarity.

x

13
15
17
19
21
23

σtr

0.357571E-02
0.193027E-02
0.903203E-03
0.489599E-03
0.200504E-03
0.493752E-04

σcv

0.346652E-02
0.184704E-02
0.883599E-03
0.535881E-03
0.322430E-03
0.845318E-04

σtest

0.363449E-02
0.195522E-02
0.907491E-03
0.486862E-03
0.201699E-03
0.526989E-04

γ

-3.49
-3.14
-3.83
-2.36
-0.50
-0.34

γ(cid:48)
-4.11
-4.35
-5.35
-5.17
-5.34
-2.92

Given regularization parameter λ, coeﬃcients of orthogonal polynomials in the ﬁtting expression, bt, are determined
according to (10). On increasing the number of orthogonal polynomials, training error σtr as well as cross-validation
error σcv decreases until terminating the ﬁtting routine according to some criterion. Hence, the number of orthogonal
polynomials, S, at ﬁxed λ is automatically identiﬁed by the criterion that judges where to terminate the ﬁtting
routine.
The rising question is how to identify the best λ? Shown in Table 2 are ﬁtting errors σtr, σcv, σtest), and correspond-
ing overﬁtting degrees γ, γ(cid:48), at the ﬁxed number of orthogonal polynomials, S = 78. The regularization parameter
λ is expressed as λ = e−x with 13 ≤ x ≤ 23 for clarity. It’s noticed that σtr, σcv and σtest decrease with lowering
the stiﬀness of the surface to ﬁt (x increases and therefore λ decreases). Hence the ordinary cross-validation scheme
seems not quite useful here. Generally speaking, the overﬁtting degree (γ) reﬂects the ﬁtting performance although
it does not monotonously increase while the stiﬀness decreases.

It’s found that the number of polynomials S aﬀects the ﬁtting error not as obviously as λ does. For example, in
the case with S = 78, lowering λ leads to two orders of change in the ﬁtting error. With ﬁxed λ, the ﬁtting error with
S = 112 is comparable to that with S = 78.

Another factor that needs considering is the sample factor. In analysing the dependence of overﬁtting degree on
the sample factor, it is noticed that σtr, σcv, σtr increase with increasing sample factor, namely, reducing the size of
the training group. The overﬁtting degree decreases (with magnitude increases) with the sample factor, suggesting
that the value of σcv/σtr decreases. By comparing cases with diﬀerent sampling factors, it’s noticed that, in spite of

9

ﬁtting errors as well as overﬁtting degree changing with the sample factor the evolving tendencies of the overﬁtting
degree with λ are similar. Thus we can ﬁnd valuable clues to identify the best λ.

Table 3. Fitting errors and overﬁtting degrees at diﬀerent regularization parameters λ. The number of
orthogonal polynomials S is automatically determined by the routine-terminating criterion. Parameter λ is
expressed as λ = e−x for clarity.

x

10
20
30
40

S

50
78
156
201

σtr

0.528493E-02
0.329224E-03
0.568479E-05
0.696329E-06

σcv

0.510816E-02
0.420514E-03
0.233510E-04
0.273960E-05

σtest

0.531742E-02
0.326656E-03
0.524090E-05
0.724544E-06

γ

-3.40
-1.28
1.13
1.07

γ(cid:48)
-5.07
-4.82
-2.55
-3.19

Table 3 summarizes the optimal ﬁtting precision and corresponding overﬁtting degrees with diﬀerent regularization
It’s noted that overﬁtting begins to occur at
parameter λ. The sampling factor is the same to preceding tables.
λ = 30. Since the cross-validation and test group can interchange data, one needs to compare γ with γ(cid:48) in order to
select out the proper λ.

C. Further Discussions

Here we discuss aspects that have not been mentioned above.
First of all, the algorithm in this paper can be further optimized so that the computing eﬃciency is increased and
memory decreased. For example, in the re-orthogonalization step of iterating orthogonalization, the projection of
the just orthogonalized polynomial is subtracted from all those subsequent polynomials which are not orthogonalized
yet; this deﬁnitely increases the the computing cost since redundant polynomials are generated to ensure the ﬁtting
precision.

Secondly, if only physical quantities that are at experimental-recorded magnetic ﬁelds and temperatures are con-
cerned, the computing results for the linearly independent functions and orthogonal polynomials used to ﬁt can be
saved to compute these physical quantities.

Thirdly, after the ﬁtting expression obtained, uniform gridding and interpolation of the data can be readily achieved.
Thus, besides the iterative method used in this work, operations like numerical derivatives and integrals can be easily
executed.

Fourthly, when the coeﬃcients of linear independent functions in orthogonal polynomials and those of the orthogonal
polynomials in the ﬁtting expression are computed, we have not considered the eﬀect of measuring errors. However,
the measuring error is always there. What’s more, the measuring error of experimental data at extremely weak ﬁeld
is usually bigger. Because we have taken into accounted all experimental data with equal weight, the measuring
error under weak ﬁeld will do harm to global ﬁtting performance. To solve this problem, one method is to abandon
the data under weak ﬁeld; another method is to introduce a local weight factor that depends on magnetic ﬁeld and
temperature so that the inﬂuence of experimental data is conﬁned within a nearby area.

Fifthly, it’s noted that in above ﬁtting, rotational symmetry of magnetic systems has not been considered. Because of
this symmetry, the terms involving even-number powers of magnetic ﬁeld should not appear in the analytic expression
of magnetization. While talking about the properties closely associated with this symmetry, it needs eliminating such
terms in the set of linearly independent functions.

Finally, since the algorithm considers only the dependence on magnetic ﬁeld and temperature, the eﬀect of other
processes such as rotation of crystal grains and the change of sample volume are not adequate disclosed, that may be
the reason for larger ﬁtting error at weak ﬁelds. Another issue not considered here is the inﬂuence of demagnetization
factor whose value in polycrystalline materials is hard computed.

IV. CONCLUSIONS

To conclude, by adding the Laplace term into the ﬁtting error expression, the regularization method and corre-
sponding cross-validation scheme are introduced to two-variable orthogonal polynomials ﬁtting. After applying to
magnetization data, it’s found that the regularization scheme does play its role through rapidly suppressing the co-
eﬃcients of higher-order terms in the ﬁtting expression and therefore eﬀectively relieving the overﬁtting problem.
With the aid of the concept of overﬁtting degree, It’s also shown that the cross validation scheme can be used to
select out the proper regularization parameter. The inﬂuences of sampling parameter and sampling factor are also

analysed. Thus it oﬀers the quite reliable base for the following investigations of the magnetic-entropy-change and
phase-transition properties of magnetic functional materials.

V. ACKNOWLEDGEMENTS

I would thank Wu Hong-ye for many helps in software usage and constructive discussions in developing the method
in this work. And my thanks also give to Wu Ke-han and Zhou Min for providing experimental data before their
paper published.

10

1 Leon S J, Bj¨orck ˚A and Gander W 2012 Numer. Linear Algebra Appl. 20 492-532
2 Li Z T, Wu P F, Tao Y Q and Mao D K 1999 Acta Phys. Sin. 48(S) S126 (in Chinese)
3 D’Errico J R 2006 Understanding Gridﬁt. Available:

http://www.mathworks.com/matlabcentral/ﬁleexchange/8998-surface-ﬁtting-using-gridﬁt.

4 Wu K H, Wan S L, Xu B, Liu S B, Zhao J J and Lu Y, To be published.

