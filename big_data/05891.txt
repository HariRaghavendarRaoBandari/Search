6
1
0
2

 
r
a

 

M
8
1

 
 
]

.

R
P
h
t
a
m

[
 
 

1
v
1
9
8
5
0

.

3
0
6
1
:
v
i
X
r
a

Asymptotic Expansions for Moment

Functionals of Perturbed Discrete Time

Semi-Markov Processes

Mikael Petersson∗

Abstract

In this paper, we study mixed power-exponential moment function-
als of nonlinearly perturbed semi-Markov processes in discrete time.
Conditions under which the moment functionals of interest can be ex-
panded in asymptotic power series with respect to the perturbation
parameter are given. We show how the coeﬃcients in these expan-
sions can be computed from explicit recursive formulas. In particular,
the results of the present paper have applications for studies of quasi-
stationary distributions.

Keywords: Semi-Markov process, Perturbation, Asymptotic expansion, Re-
newal equation, Solidarity property, First hitting time.

MSC2010: Primary 60K15; Secondary 41A60, 60K05.

1

Introduction

The aim of this paper is to present asymptotic power series expansions for
some important moment functionals of non-linearly perturbed semi-Markov
processes in discrete time and to show how the coeﬃcients in these expan-
sions can be calculated from explicit recursive formulas. These asymptotic
expansions play a fundamental role for the main result in Petersson (2016),
which is a sequel of the present paper.

For each ε ≥ 0, we let ξ(ε)(n), n = 0, 1, . . . , be a discrete time semi-
Markov process on the state space X = {0, 1, . . . , N}. It is assumed that

∗Department of Mathematics, Stockholm University, SE-106 91 Stockholm, Sweden,

mikpe@math.su.se.

1

the process ξ(ε)(n) depends on ε in the sense that its transition probabilities
Q(ε)
ij (n) are continuous at ε = 0 when considered as a function of ε. Thus,
we can, for ε > 0, interpret the process ξ(ε)(n) as a perturbation of ξ(0)(n).
Throughout the paper, we consider the case where the states {1, . . . , N}
is a communicating class of states for ε small enough. Transitions to state 0
may, or may not, be possible both for the perturbed process and the limiting
process. It will also be natural to consider state 0 as an absorbing state but
the results hold even if this is not the case.

Our main objects of study are the following mixed power-exponential

moment functionals,

φ(ε)
ij (ρ, r) =

∞Xn=0

nreρng(ε)

ij (n), ω(ε)

ijs(ρ, r) =

nreρnh(ε)

ijs(n),

(1)

∞Xn=0

where ρ ∈ R, r = 0, 1, . . . , i, j, s ∈ X,

g(ε)
ij (n) = Pi{µ(ε)

j = n, µ(ε)

0 > µ(ε)

j },

h(ε)
ijs(n) = Pi{ξ(ε)(n) = s, µ(ε)

0 ∧ µ(ε)

j > n},

and µ(ε)

j

is the ﬁrst hitting time of state j.

As is well known, power moments, exponential moments, and, as in (1),
a mixture of power and exponential moments, often play important roles in
various applications. One reason that the moments deﬁned by equation (1)
is of interest is that the probabilities P (ε)
0 > 0}
satisfy the following discrete time renewal equation,

ij (n) = Pi{ξ(ε)(n) = j, µ(ε)

P (ε)
ij (n) = h(ε)

iij (n) +

nXk=0

P (ε)
ij (n − k)g(ε)

ii (n), n = 0, 1, . . .

This can, for example, be used in studies of quasi-stationary distributions as
is illustrated in Petersson (2016).

Under the assumption that mixed power-exponential moments for tran-
sition probabilities can be expanded in asymptotic power series with respect
to the perturbation parameter, we obtain corresponding asymptotic expan-
sions for the moment functionals in equation (1). These expansions together
with explicit formulas for calculating the coeﬃcients in the expansions are
the main results of this paper.

In order to achieve this, we use methods from Gyllenberg and Silvestrov
(2008) where corresponding moment functionals for continuous time semi-
Markov processes are studied. These methods are based on ﬁrst deriving

2

recursive systems of linear equations connecting the moments of interest with
moments of transition probabilities and then successively build expansions
for solutions of such systems.

Analysis of perturbed Markov chains and semi-Markov processes con-
stitutes a large branch of research in applied probability, see, for exam-
ple, the books by Kartashov (1996), Yin and Zhang (1998), Koroliuk and
Limnios (2005), Gyllenberg and Silvestrov (2008), and Avrachenkov, Filar,
and Howlett (2013). More detailed comments on this and additional refer-
ences are given in Petersson (2016).

Let us now brieﬂy outline the structure of the present paper. In Section 2
we deﬁne perturbed discrete time semi-Markov processes and formulate our
basic conditions. Then, systems of linear equations for exponential moment
functionals are derived in Section 3 and in Section 4 we show convergence
for the solutions of these systems. Finally, in Section 5, we present the
main results which give asymptotic expansions for mixed power-exponential
moment functionals.

2 Perturbed Semi-Markov Processes

For every ε ≥ 0, let (η(ε)

In this section we deﬁne perturbed discrete time semi-Markov processes and
formulate some basic conditions.
n , κ(ε)

n ), n = 0, 1, . . . , be a discrete time Markov
renewal process, i.e., a homogeneous Markov chain with state space X × N,
where X = {0, 1, . . . , N} and N = {1, 2, . . .}, an initial distribution Q(ε)
i =
P{η(ε)
0 = i}, i ∈ X, and transition probabilities which do not depend on the
current value of the second component, given by

Q(ε)

ij (k) = P{η(ε)

n+1 = j, κ(ε)

n+1 = k | η(ε)

n = i, κ(ε)

n = l}, k, l ∈ N, i, j ∈ X.

In this case, it is known that η(ε)
state space X and transition probabilities,

n , n = 0, 1, . . . , is also a Markov chain with

p(ε)
ij = P{η(ε)

n+1 = j | η(ε)

n = i} =

Q(ε)

ij (k), i, j ∈ X.

∞Xk=1

Let us deﬁne τ (ε)(0) = 0 and τ (ε)(n) = κ(ε)

n , for n ∈ N.
Furthermore, for n = 0, 1, . . . , we deﬁne ν(ε)(n) = max{k : τ (ε)(k) ≤ n}.
The discrete time semi-Markov process associated with the Markov renewal
process (η(ε)

n ) is deﬁned by the following relation,

1 + · · · + κ(ε)

n , κ(ε)

ξ(ε)(n) = η(ε)

ν(ε)(n), n = 0, 1, . . . ,

3

and we will refer to Q(ε)

ij (k) as the transition probabilities of this process.

In the semi-Markov process deﬁned above, we have that (i) κ(ε)

n are the
times between successive moments of jumps, (ii) τ (ε)(n) are the moments of
the jumps, (iii) ν(ε)(n) are the number of jumps in the interval [0, n], and
(iv) η(ε)

n is the embedded Markov chain.

It is sometimes convenient to write the transition probabilities of the

semi-Markov process as Q(ε)

ij (k) = p(ε)

ij f (ε)

ij (k), where

f (ε)
ij (k) = P{κ(ε)

n+1 = k | η(ε)

n = i, η(ε)

n+1 = j}, k ∈ N, i, j ∈ X,

are the conditional distributions of transition times.

n = j} and µ(ε)

We now deﬁne random variables for ﬁrst hitting times. For each j ∈ X,
let ν(ε)
and µ(ε)
j = min{n ≥ 1 : η(ε)
are the ﬁrst hitting times of state j for the embedded Markov chain and the
semi-Markov process, respectively. Note that the random variables ν(ε)
and
µ(ε)

, which may be improper, take values in the set {1, 2, . . . , ∞}.
Let us deﬁne

j = τ (ν(ε)

j ). Then, ν(ε)

j

j

j

j

g(ε)
ij (n) = Pi{µ(ε)

j = n, ν(ε)

0 > ν(ε)

j }, n = 0, 1, . . . , i, j ∈ X,

and

g(ε)
ij = Pi{ν(ε)

0 > ν(ε)

j }, i, j ∈ X.

Here, and in what follows, we write Pi(A(ε)) = P{A(ε) | η(ε)
0 = i} for any event
A(ε). Corresponding notation for conditional expectation will also be used.
Moment generating functions for distributions of ﬁrst hitting times are

deﬁned by

φ(ε)
ij (ρ) =

∞Xn=0

eρng(ε)

ij (n) = Eieρµ

j χ(ν(ε)

0 > ν(ε)

j ), ρ ∈ R, i, j ∈ X.

(ε)

(2)

Furthermore, let us deﬁne the following exponential moment functionals

for transition probabilities,

p(ε)
ij (ρ) =

∞Xn=0

eρnQ(ε)

ij (n), ρ ∈ R, i, j ∈ X,

where we deﬁne Q(ε)

ij (0) = 0.

Let us now introduce the following conditions, which we will refer to

frequently throughout the paper:

A:

(a) p(ε)

ij → p(0)

ij , as ε → 0, i 6= 0, j ∈ X.

4

(b) f (ε)

ij (n) → f (0)

ij (n), as ε → 0, n ∈ N, i 6= 0, j ∈ X.

B: g(0)

ij > 0, i, j 6= 0.

C: There exists β > 0 such that:

(a) lim sup0≤ε→0 p(ε)
(b) φ(0)

ii (βi) ∈ (1, ∞), for some i 6= 0 and βi ≤ β.

ij (β) < ∞, for all i 6= 0, j ∈ X.

It follows from conditions A and B that {1, . . . , N} is a communicating
class of states for suﬃciently small ε. Let us also remark that if p(0)
i0 = 0 for
all i 6= 0, it can be shown that part (b) of condition C always holds under
conditions A, B, and C(a).

3 Systems of Linear Equations

In this section we derive systems of linear equations for exponential moment
functionals.

We ﬁrst consider the moment generating functions φ(ε)

ij (ρ), deﬁned by

equation (2). By conditioning on (η(ε)

1 , κ(ε)

1 ), we get for each i, j 6= 0,

∞Xk=1

φ(ε)

ij (ρ) =Xl∈X
∞Xk=1

=

(ε)

Ei(eρµ

j χ(ν(ε)

0 > ν(ε)

j )|η(ε)

1 = l, κ(ε)

1 = k)Q(ε)

il (k)

eρkQ(ε)

ij (k) +Xl6=0,j

Eleρ(k+µ

(ε)
j

∞Xk=1

)χ(ν(ε)

0 > ν(ε)

j )Q(ε)

il (k).

Relation (3) gives us the following system of linear equations,

φ(ε)
ij (ρ) = p(ε)

ij (ρ) +Xl6=0,j

p(ε)
il (ρ)φ(ε)

lj (ρ), i, j 6= 0.

(3)

(4)

In what follows it will often be convenient to use matrix notation. Let us

introduce the following column vectors,

Φ(ε)

j (ρ) =hφ(ε)
j (ρ) =hp(ε)

p(ε)

1j (ρ)

· · · φ(ε)

1j (ρ)

· · · p(ε)

N j(ρ)iT
N j(ρ)iT

, j 6= 0,

, j ∈ X.

(5)

(6)

5

For each j 6= 0, we also deﬁne N ×N-matrices jP(ε)(ρ) = kjp(ε)

ik (ρ)k where

the elements are given by

jp(ε)

ik (ρ) =(cid:26) p(ε)

0

ik (ρ)

i = 1, . . . , N, k 6= j,
i = 1, . . . , N, k = j.

(7)

Using (5), (6), and (7), we can write the system (4) in the following matrix

form,

j (ρ) = p(ε)
Φ(ε)

j (ρ) + jP(ε)(ρ)Φ(ε)

j (ρ), j 6= 0.

(8)

Note that the relations given above hold for all ρ ∈ R even in the case
where some of the quantities involved take the value inﬁnity. In this case we
use the convention 0 · ∞ = 0 and the equalities may take the form ∞ = ∞.
Let us now derive a similar type of system for the following exponential

moment functionals,

ω(ε)
ijs(ρ) =

eρnPi{ξ(ε)(n) = s, µ(ε)

0 ∧ µ(ε)

j > n}, ρ ∈ R, i, j, s ∈ X.

∞Xn=0

First, note that

ω(ε)
ijs(ρ) = Ei

= Ei

∞Xn=0
j −1Xn=0

(ε)
0 ∧µ

(ε)

µ

eρnχ(ξ(ε)(n) = s, µ(ε)

0 ∧ µ(ε)

j > n)

eρnχ(ξ(ε)(n) = s).

We now decompose ω(ε)

ijs(ρ) into two parts,

ω(ε)
ijs(ρ) = Ei

κ

(ε)

1 −1Xn=0

eρnχ(ξ(ε)(n) = s) + Ei

µ

(ε)
0 ∧µ

(ε)

j −1Xn=κ

(ε)
1

eρnχ(ξ(ε)(n) = s).

(9)

Let us ﬁrst rewrite the ﬁrst term on the right hand side of equation (9).

By conditioning on κ(ε)

1 we get, for i, s 6= 0,

Ei

κ

(ε)

1 −1Xn=0

eρnχ(ξ(ε)(n) = s)

κ

(ε)

Ei
1 −1Xn=0
δ(i, s)  k−1Xn=0

=

=

∞Xk=1
∞Xk=1

eρnχ(ξ(ε)(n) = s)(cid:12)(cid:12)(cid:12) κ(ε)
eρn! Pi{κ(ε)

1 = k}.

1 = k Pi{κ(ε)

1 = k}

6

eρnχ(ξ(ε)(n) = s) = δ(i, s)ϕ(ε)

i (ρ), i, s 6= 0.

ρ = 0,
1 − 1)/(eρ − 1) ρ 6= 0.

(ε)

(10)

(11)

Let us now consider the second term on the right hand side of equation

(9). By conditioning on (η(ε)

1 , κ(ε)

1 ) we get, for i, j, s 6= 0,

It follows that

Ei

where

(ε)

κ

1 −1Xn=0
i (ρ) =( Eiκ(ε)

ϕ(ε)

(Eieρκ

1

µ

(ε)
0 ∧µ

Ei

eρnχ(ξ(ε)(n) = s)

(ε)

(ε)
1

j −1Xn=κ
= Xl6=0,j
= Xl6=0,j

It follows that

µ

(ε)
0 ∧µ

Ei

tions,

1 = l, κ(ε)

1 = k Q(ε)

il (k)

eρnχ(ξ(ε)(n) = s)(cid:12)(cid:12)(cid:12) η(ε)
eρ(k+n)χ(ξ(ε)(n) = s) Q(ε)

µ

µ

(ε)

(ε)

(ε)
1

(ε)
0 ∧µ

(ε)
0 ∧µ

∞Xk=1
∞Xk=1

j −1Xn=κ
j −1Xn=0

Ei
El
j −1Xn=κ
eρnχ(ξ(ε)(n) = s) = Xl6=0,j

(ε)
1

(ε)

il (k).

p(ε)
il (ρ)ω(ε)

ljs(ρ), i, j, s 6= 0.

(12)

From (9), (10), and (12) we now get the following system of linear equa-

ω(ε)
ijs(ρ) = δ(i, s)ϕ(ε)

p(ε)
il (ρ)ω(ε)

ljs(ρ), i, j, s 6= 0.

(13)

i (ρ) +Xl6=0,j

In order to write this system in matrix form, let us deﬁne the following

column vectors,

s (ρ) =hδ(1, s)ϕ(ε)
bϕ(ε)
js (ρ) =hω(ε)

ω(ε)

1 (ρ)

· · ·

δ(N, s)ϕ(ε)

, s 6= 0,

(14)

1js(ρ)

· · · ω(ε)

, j, s 6= 0.

(15)

N (ρ)iT

N js(ρ)iT

7

Using (7), (14), and (15), the system (13) can be written in the following

matrix form,

s (ρ) + jP(ε)(ρ)ω(ε)

js (ρ), j, s 6= 0.

(16)

ω(ε)

js (ρ) = bϕ(ε)

We close this section with a lemma which will be important in what

follows.
Lemma 1. Assume that we for some ε ≥ 0 and ρ ∈ R have that g(ε)
i, k 6= 0 and p(ε)
statements are equivalent:

ik > 0,
ik (ρ) < ∞, i 6= 0, k ∈ X. Then, for any j 6= 0, the following

(a) Φ(ε)

j (ρ) < ∞.

(b) ω(ε)

js (ρ) < ∞, s 6= 0.

(c) The inverse matrix (I − jP(ε)(ρ))−1 exists.

Proof. For each j 6= 0, let us deﬁne a matrix valued function jA(ε)(ρ) =
kja(ε)

ik (ρ)k by the relation

jA(ε)(ρ) = I + jP(ε)(ρ) + (jP(ε)(ρ))2 + · · · , ρ ∈ R.

(17)

Since each term on the right hand side of (17) is non-negative, it follows
that the elements ja(ε)
ik (ρ) are well deﬁned and take values in the set [0, ∞].
Furthermore, the elements can be written in the following form which gives
a probabilistic interpretation,

ja(ε)

ik (ρ) = Ei

eρτ (ε)(n)χ(ν(ε)

0 ∧ ν(ε)

j > n, η(ε)

n = k), i, k 6= 0.

∞Xn=0

Let us now show that

Φ(ε)
j (ρ) = jA(ε)(ρ)p(ε)

j (ρ), ρ ∈ R, j 6= 0.

In order to do this, ﬁrst note that, for j 6= 0,

χ(ν(ε)

0 > ν(ε)

j ) =

∞Xn=0Xk6=0

χ(ν(ε)

0 ∧ ν(ε)

j > n, η(ε)

n = k, η(ε)

n+1 = j).

(18)

(19)

(20)

Using (20) and the regenerative property of the semi-Markov process, the

following is obtained, for i, j 6= 0,

φ(ε)
ij (ρ) =

=

∞Xn=0Xk6=0
∞Xn=0Xk6=0

(ε)

Eieρµ

j χ(ν(ε)

0 ∧ ν(ε)

j > n, η(ε)

n = k, η(ε)

n+1 = j)

Eieρτ (ε)(n)χ(ν(ε)

0 ∧ ν(ε)

j > n, η(ε)

n = k)p(ε)

kj (ρ).

(21)

8

From (18) and (21) we get

φ(ε)

ij (ρ) =Xk6=0

and this proves (19).
Let us now deﬁne

ja(ε)

ik (ρ)p(ε)

kj (ρ), i, j 6= 0,

ω(ε)

ij (ρ) =Xs6=0

Then, we have

ω(ε)
ijs(ρ) =

∞Xn=0
ij (ρ) =( Ei(µ(ε)

(Eieρ(µ

ω(ε)

eρnPi{µ(ε)

0 ∧ µ(ε)

j > n}, ρ ∈ R,

0 ∧ µ(ε)
j )

(ε)
(ε)
0 ∧µ
j

ρ = 0,
) − 1)/(eρ − 1) ρ 6= 0.

(22)

i, j 6= 0.

(23)

(24)

(25)

(26)

(27)

(28)

(29)

Also notice that

Eieρ(µ

(ε)
(ε)
0 ∧µ
j

) = Eieρµ

j χ(ν(ε)

0 > ν(ε)

j ) + Eieρµ

0 χ(ν(ε)

0 < ν(ε)

j ), i, j 6= 0.

(ε)

(ε)

Using similar calculations as above, it can be shown that

Eieρµ

(ε)

0 χ(ν(ε)

0 < ν(ε)

ja(ε)

ik (ρ)p(ε)

k0 (ρ), i, j 6= 0.

j ) =Xk6=0
ik (ρ)(cid:16)p(ε)

ja(ε)

It follows from (22), (25), and (26) that

Eieρ(µ

(ε)
(ε)
0 ∧µ
j

) =Xk6=0

kj (ρ) + p(ε)

k0 (ρ)(cid:17) , i, j 6= 0.

Let us now show that (a) implies (b).
By iterating relation (8) we obtain,

Φ(ε)
j (ρ) = (I + jP(ε)(ρ) + · · · + (jP(ε)(ρ))n)p(ε)

j (ρ)

+ (jP(ε)(ρ))n+1Φ(ε)

j (ρ), n = 1, 2, . . .

Since Φ(ε)

j (ρ) < ∞, it follows from (28) that

(jP(ε)(ρ))n+1Φ(ε)

j (ρ) → 0, as n → ∞.

The assumptions of the lemma guarantee that Φ(ε)

j (ρ) > 0. From this and
relation (29) we can conclude that (jP(ε)(ρ))n+1 → 0, as n → ∞. It is known
that this holds if and only if the matrix series (17) converges in norms, that

9

is, jA(ε)(ρ) is ﬁnite. From this and relations (23), (24), and (27) it follows
that (b) holds.

Next we show that (b) implies (c).
By summing over all s 6= 0 in relation (16) it follows that

ω(ε)

j (ρ) = ϕ(ε)(ρ) + jP(ε)(ρ)ω(ε)

j (ρ), ρ ∈ R,

(30)

where

and

ω(ε)

1j (ρ)

j (ρ) =hω(ε)
ϕ(ε)(ρ) =hϕ(ε)

By iterating relation (30) we get

· · · ω(ε)

, j 6= 0,

N j(ρ)iT
N (ρ)iT

1 (ρ)

· · · ϕ(ε)

.

ω(ε)

j (ρ) = (I + jP(ε)(ρ) + · · · + (jP(ε)(ρ))n)ϕ(ε)(ρ)

+ (jP(ε)(ρ))n+1ω(ε)

j (ρ), n = 1, 2, . . .

(31)

It follows from (b) and the deﬁnition of ω(ε)

j (ρ) < ∞.
So, letting n → ∞ in (31) and using similar arguments as above, it follows
that the matrix series (17) converges in norms. It is then known that the
inverse matrix (I − jP(ε)(ρ))−1 exists, that is, (c) holds.

ij (ρ) that 0 < ω(ε)

Let us ﬁnally argue that (c) implies (a).
If (I − jP(ε)(ρ))−1 exists, then the following relation holds,

(I − jP(ε)(ρ))−1 = I + jP(ε)(ρ)(I − jP(ε)(ρ))−1.

(32)

Iteration of (32) gives

(I − jP(ε)(ρ))−1 = I + jP(ε)(ρ) + (jP(ε)(ρ))2 + · · · + (jP(ε)(ρ))n

+ (jP(ε)(ρ))n+1(I − jP(ε)(ρ))−1, n = 1, 2, . . .

(33)

Letting n → ∞ in (33) it follows that jA(ε)(ρ) = (I − jP(ε)(ρ))−1 < ∞. From
(19) we now see that (a) holds.

4 Convergence of Moment Functionals

In this section it is shown that the solutions of the systems derived in Section
3 converge as the perturbation parameter tends to zero. In addition, we prove
some properties for the solution of a characteristic equation.

10

Let us deﬁne

kφ(ε)

ij (ρ) = Eieρµ

j ), ρ ∈ R, i, j, k ∈ X.

(ε)

j χ(ν(ε)

0 ∧ ν(ε)

k > ν(ε)

If the states {1, . . . , N} is a communicating class and φ(ε)

ii (ρ) ≤ 1 for some
i 6= 0, then it can be shown (see, for example, Petersson (2015)) that the
following relation holds for all j 6= 0,

(1 − φ(ε)

ii (ρ))(1 − iφ(ε)

jj (ρ)) = (1 − φ(ε)

jj (ρ))(1 − jφ(ε)

ii (ρ)).

(34)

Relation (34) is useful in order to prove various solidarity properties for
ii (ρ) = 1, relation (34) reduces to

semi-Markov processes. In particular, if φ(ε)

(1 − φ(ε)

jj (ρ))(1 − jφ(ε)

ii (ρ)) = 0.

(35)

From the regenerative property of the semi-Markov process it follows that

φ(ε)
ii (ρ) = jφ(ε)

ii (ρ) + iφ(ε)

ij (ρ)φ(ε)

ji (ρ), j 6= 0, i.

(36)

Since {1, . . . , N} is a communicating class, we have iφ(ε)
ii (ρ) = 1 it follows from (36) that jφ(ε)

ij (ρ) > 0 and
ii (ρ) < 1. From
jj (ρ) = 1 for all j 6= 0. Thus, we have

φ(ε)
ji (ρ) > 0. So, if φ(ε)
this and (35) we can conclude that φ(ε)
the following lemma:

Lemma 2. Assume that we for some ε ≥ 0 have that g(ε)
Then, if we for some i 6= 0 and ρ ∈ R, have that φ(ε)
φ(ε)
jj (ρ) = 1 for all j 6= 0.

kj > 0 for all k, j 6= 0.
ii (ρ) = 1, it follows that

Let us now deﬁne the following characteristic equation,

φ(ε)
ii (ρ) = 1.

(37)

where i 6= 0 is arbitrary. The root of equation (37) plays an important role
for the asymptotic behaviour of the corresponding semi-Markov process, see,
for example, Petersson (2016).

The following lemma gives limits of moment functionals and properties

for the root of the characteristic equation.

Lemma 3. If conditions A–C hold, then there exists δ ∈ (0, β] such that the
following holds:

(i) φ(ε)

kj (ρ) → φ(0)

kj (ρ) < ∞, as ε → 0, ρ ≤ δ, k, j 6= 0.

11

(ii) ω(ε)

kjs(ρ) → ω(0)

kjs(ρ) < ∞, as ε → 0, ρ ≤ δ, k, j, s 6= 0.

(iii) φ(0)

jj (δ) ∈ (1, ∞), j 6= 0.

(iv) For suﬃciently small ε, there exists a unique non-negative root ρ(ε) of

the characteristic equation (37) which does not depend on i.

(v) ρ(ε) → ρ(0) < δ as ε → 0.

Proof. Let i 6= 0 and βi ≤ β be the values given in condition C. It follows
from conditions B and C that φ(0)
ii (ρ) is a continuous and strictly increasing
function for ρ ≤ βi. Since φ(0)
ii (βi) > 1, there exists a
unique ρ′ ∈ [0, βi) such that φ(0)

ii (ρ′) = 1. Moreover, by Lemma 2,

ii (0) = g(0)

ii ≤ 1 and φ(0)

φ(0)
jj (ρ′) = 1, j 6= 0.

For all j 6= 0, we have

jj (ρ′) = kφ(0)
φ(0)

jj (ρ′) + jφ(0)

jk (ρ′)φ(0)

kj (ρ′), k 6= 0, j.

It follows from (38), (39), and condition B, that

φ(0)
kj (ρ′) < ∞, k, j 6= 0.

(38)

(39)

(40)

From (40) and Lemma 1 we get that det(I − jP(0)(ρ′)) 6= 0, for j 6= 0.
Under condition C, the elements of I − jP(0)(ρ) are continuous functions for
ρ ≤ β. This implies that we for each j 6= 0 can ﬁnd βj ∈ (ρ′, βi] such that
det(I − jP(0)(βj)) 6= 0. By condition C we also have that p(0)
kj (βj) < ∞ for
k 6= 0, j ∈ X. It now follows from Lemma 1 that φ(0)
kj (βj) < ∞, k, j 6= 0. If
we deﬁne δ = min{β1, . . . , βN }, it follows that

φ(0)
kj (ρ) < ∞, ρ ≤ δ, k, j 6= 0.

Now, let ρ ≤ δ be ﬁxed. Relation (41) and Lemma 1 imply that

det(I − jP(0)(ρ)) 6= 0, j 6= 0.

Note that we have

p(ε)
kj (ρ) = p(ε)

kj

∞Xn=0

eρnf (ε)

kj (n), k, j ∈ X.

12

(41)

(42)

(43)

Since f (ε)
conditions A and C that

kj (n) are proper probability distributions, it follows from (43) and

p(ε)
kj (ρ) → p(0)

kj (ρ) < ∞, as ε → 0, k 6= 0, j ∈ X.

(44)

It follows from (42) and (44) that there exists ε1 > 0 such that we for
all ε ≤ ε1 have that det(I − jP(ε)(ρ)) 6= 0 and p(ε)
kj (ρ) < ∞, for all k, j 6= 0.
Using Lemma 1 once again, it now follows that φ(ε)
kj (ρ) < ∞, k, j 6= 0, for
all ε ≤ ε1. Moreover, in this case, the system of linear equations (8) has a
unique solution for ε ≤ ε1 given by

Φ(ε)
j (ρ) = (I − jP(ε)(ρ))−1p(ε)

j (ρ), j 6= 0.

(45)

From (44) and (45) it follows that

φ(ε)
kj (ρ) → φ(0)

kj (ρ) < ∞, as ε → 0, k, j 6= 0.

This completes the proof of part (i).
For the proof of part (ii) we ﬁrst note that, since φ(ε)

kj (ρ) < ∞ for ε ≤ ε1,
k, j 6= 0, it follows from Lemma 1 that ω(ε)
kjs(ρ) < ∞ for ε ≤ ε1, k, j, s 6= 0.
From this, and arguments given above, we see that the system of linear
equations given by relation (16) has a unique solution for ε ≤ ε1 given by

ω(ε)

(ε)

js (ρ) = (I − jP(ε)(ρ))−1bϕ(ε)
1 = Pj∈X p(ε)

Now, since Eieρκ

ij (ρ), it follows from (11) and (44) that
i (ρ) < ∞ as ε → 0, i 6= 0. Using this and relations (44) and

s (ρ), j, s 6= 0.

(46)

ϕ(ε)
i (ρ) → ϕ(0)
(46) we can conclude that part (ii) holds.
By part (i) we have, in particular, φ(ε)
j 6= 0. Furthermore, since ρ′ < δ and φ(0)
it follows from (38) that φ(0)

jj (δ) → φ(0)
jj (δ) < ∞ as ε → 0, for all
jj (ρ) is strictly increasing for ρ ≤ δ,

jj (δ) > 1, j 6= 0. This proves part (iii).

Let us now prove part (iv).
It follows from (i) and (iii) that we can ﬁnd ε2 > 0 such that φ(ε)

jj (δ) ∈
(1, ∞), j 6= 0, for all ε ≤ ε2. By conditions A and B there exists ε3 > 0 such
that, for each i 6= 0 and ε ≤ ε3, the function g(ε)
ii (n) is not concentrated at
zero. Thus, for every i 6= 0 and ε ≤ min{ε2, ε3}, we have that φ(ε)
ii (ρ) is a con-
tinuous and strictly increasing function for ρ ∈ [0, δ]. Since φ(ε)
ii ≤ 1
and φ(ε)
i ) = 1.
By Lemma 2, the root of the characteristic equation does not depend on i so
we can write ρ(ε) instead of ρ(ε)

ii (δ) > 1, there exists a unique ρ(ε)

i ∈ [0, δ) such that φ(ε)

ii (ρ(ε)

ii (0) = g(ε)

. This proves part (iv).

i

13

Finally, we show that ρ(ε) → ρ(0) as ε → 0.
Let γ > 0 such that ρ(0) + γ ≤ δ be arbitrary. Then φ(0)

ii (ρ(0) − γ) < 1 and
φ(0)
ii (ρ(0) + γ) > 1. From this and part (i) we get that there exists ε4 > 0 such
that φ(ε)
ii (ρ(0) + γ) > 1, for all ε ≤ ε4. So, it follows that
|ρ(ε) − ρ(0)| < γ for ε ≤ min{ε2, ε3, ε4}. This completes the proof of Lemma
3.

ii (ρ(0) − γ) < 1 and φ(ε)

5 Expansions of Moment Functionals

In this section, asymptotic expansions for mixed power-exponential moment
functionals are constructed. The main results are given by Theorems 1 and
2.

Let us deﬁne the following mixed power-exponential moment functionals

for distributions of ﬁrst hitting times,

φ(ε)
ij (ρ, r) =

∞Xn=0

nreρng(ε)

ij (n), ρ ∈ R, r = 0, 1, . . . , i, j ∈ X.

By deﬁnition, φ(ε)

ij (ρ, 0) = φ(ε)

ij (ρ).

We also deﬁne the following mixed power-exponential moment functionals

for transition probabilities,

p(ε)
ij (ρ, r) =

∞Xn=0

nreρnQ(ε)

ij (n), ρ ∈ R, r = 0, 1, . . . , i, j ∈ X.

By deﬁnition, p(ε)

ij (ρ, 0) = p(ε)

ij (ρ).

It follows from conditions A–C and Lemma 3 that, for ρ < δ and suﬃ-
ciently small ε, the functions φ(ε)
ij (ρ) are arbitrarily many times
diﬀerentiable with respect to ρ, and the derivatives of order r are given by
φ(ε)
ij (ρ, r) and p(ε)

ij (ρ, r), respectively.

ij (ρ) and p(ε)

Recall from Section 3 that the following system of linear equations holds,

φ(ε)
ij (ρ) = p(ε)

ij (ρ) +Xl6=0,j

Diﬀerentiating relation (47) gives

il (ρ)φ(ε)
p(ε)

lj (ρ), i, j 6= 0.

(47)

φ(ε)
ij (ρ, r) = λ(ε)

ij (ρ, r) +Xl6=0,j

p(ε)
il (ρ)φ(ε)

lj (ρ, r), r = 1, 2, . . . , i, j 6= 0,

(48)

14

where

λ(ε)
ij (ρ, r) = p(ε)

ij (ρ, r) +

rXm=1(cid:18) r

m(cid:19)Xl6=0,j

p(ε)
il (ρ, m)φ(ε)

lj (ρ, r − m).

(49)

In order to write relations (47), (48), and (49) in matrix form, let us

deﬁne the following column vectors,

Φ(ε)

j (ρ, r) =hφ(ε)
j (ρ, r) =hp(ε)
j (ρ, r) =hλ(ε)

p(ε)

λ(ε)

1j (ρ, r)

· · · φ(ε)

1j (ρ, r)

· · ·

1j (ρ, r)

· · · λ(ε)

p(ε)

N j(ρ, r)iT
N j(ρ, r)iT
N j(ρ, r)iT

, j 6= 0,

, j 6= 0,

, j 6= 0.

(50)

(51)

(52)

Let us also, for j 6= 0, deﬁne N × N-matrices jP(ε)(ρ, r) = kjp(ε)

ik (ρ, r)k

where the elements are given by

jp(ε)

ik (ρ, r) =(cid:26) p(ε)

0

ik (ρ, r)

i = 1, . . . , N, k 6= j,
i = 1, . . . , N, k = j.

(53)

Using (47)–(53) we can for any j 6= 0 write the following recursive systems

of linear equations,

Φ(ε)
j (ρ) = p(ε)

j (ρ) + jP(ε)(ρ)Φ(ε)

j (ρ),

and, for r = 1, 2, . . . ,

Φ(ε)
j (ρ, r) = λ(ε)

j (ρ, r) + jP(ε)(ρ)Φ(ε)

j (ρ, r),

where

j (ρ, r) = p(ε)
λ(ε)

j (ρ, r) +

rXm=1(cid:18) r

m(cid:19)jP(ε)(ρ, m)Φ(ε)

j (ρ, r − m).

(54)

(55)

(56)

Let us now introduce the following perturbation condition, which is as-

sumed to hold for some ρ < δ, where δ is the parameter in Lemma 3:

P∗

k: p(ε)

ij (ρ, r) = p(0)
ij (ρ, r) + pij[ρ, r, 1]ε + · · · + pij[ρ, r, k − r]εk−r + o(εk−r),
for r = 0, . . . , k, i 6= 0, j ∈ X, where |pij[ρ, r, n]| < ∞, for r = 0, . . . , k,
n = 1, . . . , k − r, i 6= 0, j ∈ X.

15

For convenience, we denote p(0)
Note that if condition P∗

ij (ρ, r) = pij[ρ, r, 0], for r = 0, . . . , k.
k holds, then, for r = 0, . . . , k, we have the

following asymptotic matrix expansions,
jP(ε)(ρ, r) = jP[ρ, r, 0] + jP[ρ, r, 1]ε + · · ·+ jP[ρ, r, k − r]εk−r + o(εk−r), (57)
p(ε)
j (ρ, r) = pj[ρ, r, 0] + pj[ρ, r, 1]ε + · · · + pj[ρ, r, k − r]εk−r + o(εk−r). (58)
Here, and in what follows, o(εp) denotes a matrix-valued function of ε where
all elements are of order o(εp). The coeﬃcients in (57) are N × N-matrices
jP[ρ, r, n] = kjpik[ρ, r, n]k with elements given by

and the coeﬃcients in (58) are column vectors deﬁned by

0

jpik[ρ, r, n] =(cid:26) pik[ρ, r, n]
pj[ρ, r, n] =(cid:2)p1j[ρ, r, n]

i = 1, . . . , N, k 6= j,
i = 1, . . . , N, k = j,

· · ·

pN j[ρ, r, n](cid:3)T

.

Let us now deﬁne the following matrix, which will play an important role

in what follows,

jU(ε)(ρ) = (I − jP(ε)(ρ))−1.

Under conditions A–C, it follows from Lemmas 1 and 3 that jU(ε)(ρ) is well
deﬁned for ρ ≤ δ and suﬃciently small ε.

The following lemma gives an asymptotic expansion for jU(ε)(ρ).

Lemma 4. Assume that conditions A–C and P∗
following asymptotic expansion,

k hold. Then we have the

jU(ε)(ρ) = jU[ρ, 0] + jU[ρ, 1]ε + · · · + jU[ρ, k]εk + o(εk),

(59)

where

jU[ρ, n] =(cid:26) (I − jP(0)(ρ))−1
jU[ρ, 0]Pn

q=1 jP[ρ, 0, q]jU[ρ, n − q] n = 1, . . . , k.

n = 0,

(60)

Proof. As already mentioned above, conditions A–C ensure us that the in-
verse jU(ε)(ρ) exists for suﬃciently small ε. In this case, it is known that the
expansion (59) exists under condition P∗
k. To see that the coeﬃcients are
given by (60), ﬁrst note that

I = (I − jP(ε)(ρ))jU(ε)(ρ)

= (I − jP(0)(ρ) − jP[ρ, 0, 1]ε − · · · − jP[ρ, 0, k]εk + o(εk))
× (jU[ρ, 0] + jU[ρ, 1]ε + · · · + jU[ρ, k]εk + o(εk)).

(61)

By ﬁrst expanding both sides of equation (61) and then, for n = 0, 1, . . . , k,
equating coeﬃcients of εn in the left and right hand sides, we get formula
(60).

16

We are now ready to construct asymptotic expansions for Φ(ε)

j (ρ, r).

Theorem 1. Assume that conditions A–C and P∗

k hold. Then:

(i) We have the following asymptotic expansion,

Φ(ε)

j (ρ) = Φj[ρ, 0, 0] + Φj[ρ, 0, 1]ε + · · · + Φj[ρ, 0, k]εk + o(εk),

where

Φj[ρ, 0, n] =( Φ(0)
Pn

j (ρ)
q=0 jU[ρ, q]pj[ρ, 0, n − q] n = 1, . . . , k.

n = 0,

(ii) For r = 1, . . . , k, we have the following asymptotic expansions,

Φ(ε)

j (ρ, r) = Φj[ρ, r, 0] + Φj[ρ, r, 1]ε + · · · + Φj[ρ, r, k − r]εk−r + o(εk−r),

where

Φj[ρ, r, n] =( Φ(0)
Pn

and, for t = 0, . . . , k − r,

j (ρ, r)
q=0 jU[ρ, q]λj[ρ, r, n − q] n = 1, . . . , k − r,

n = 0,

λj[ρ, r, t] = pj[ρ, r, t] +

rXm=1(cid:18) r

m(cid:19) tXq=0

jP[ρ, m, q]Φj[ρ, r − m, t − q].

Before proceeding with the proof of Theorem 1 we would like to comment
on the reason that the theorem is stated in such a way that Φ(ε)
j (ρ, r), for
r = 1, . . . , k, has an expansion of order k−r. The reason is that this is exactly
what we need for the main result in Petersson (2016), which, we remind, is a
sequel of the present paper. However, it is possible to construct asymptotic
expansions of diﬀerent orders than the ones stated in the theorem. In that
case, appropriate changes in the perturbation condition should be made. The
same remark applies to Lemma 5 and Theorem 2 below.

Proof. Under conditions A–C, we have, for suﬃciently small ε, that the
recursive systems of linear equations given by relations (54), (55), and (56),
all have ﬁnite components. Moreover, the inverse matrix jU(ε)(ρ) = (I −
jP(ε)(ρ))−1 exists, so these systems have unique solutions.

17

It follows from (54), Lemma 4, and condition P∗

k that

Φ(ε)
j (ρ) = jU(ε)(ρ)p(ε)

j (ρ)

= (jU[ρ, 0] + jU[ρ, 1]ε + · · · + jU[ρ, k]εk + o(εk))
× (pj[ρ, 0, 0] + pj[ρ, 0, 1]ε + · · · + pj[ρ, 0, k]εk + o(εk)).

(62)

By expanding the right hand side of equation (62), we see that part (i)

of Theorem 1 holds.

With r = 1, relation (56) takes the form

λ(ε)
j (ρ, 1) = p(ε)

j (ρ, 1) + jP(ε)(ρ, 1)Φ(ε)

j (ρ).

(63)

From (63), condition P∗

k, and part (i), we get

λ(ε)

j (ρ, 1) = pj[ρ, 1, 0] + · · · + pj[ρ, 1, k − 1]εk−1 + o(εk−1)

+ (jP[ρ, 1, 0] + · · · + jP[ρ, 1, k − 1]εk−1 + o(εk−1))
× (Φj[ρ, 0, 0] + · · · + Φj[ρ, 0, k − 1]εk−1 + o(εk−1)).

(64)

Expanding the right hand side of (64) gives

λ(ε)

j (ρ, 1) = λj[ρ, 1, 0] + λj[ρ, 1, 1]ε + · · · + λj[ρ, 1, k − 1]εk−1 + o(εk−1), (65)

where

λj[ρ, 1, t] = pj[ρ, 1, t] +

tXq=0

jP[ρ, 1, q]Φj[ρ, 0, t − q], t = 0, . . . , k − 1.

It now follows from (55), (65), and Lemma 4 that

Φ(ε)
j (ρ, 1) = jU(ε)(ρ)λ(ε)

j (ρ, 1)

= (jU[ρ, 0] + · · · + jU[ρ, k − 1]εk−1 + o(εk−1))
× (λj[ρ, 1, 0] + · · · + λj[ρ, 1, k − 1]εk−1 + o(εk−1)).

(66)

By expanding the right hand side of equation (66) we get the expansion in
part (ii) for r = 1. If k = 1, this concludes the proof. If k ≥ 2, we can repeat
the steps above, successively, for r = 2, . . . , k. This gives the expansions and
formulas given in part (ii).

Let us now deﬁne the following mixed power exponential moment func-

tionals, for i, j, s ∈ X,

ω(ε)
ijs(ρ, r) =

∞Xn=0

nreρnPi{ξ(ε)(n) = s, µ(ε)

0 ∧ µ(ε)

j > n}, ρ ∈ R, r = 0, 1, . . .

18

Notice that ω(ε)

ijs(ρ, 0) = ω(ε)

ijs(ρ).

ijs(ρ) and p(ε)

It follows from conditions A–C and Lemma 3 that for ρ < δ and suﬃ-
ciently small ε, the functions ω(ε)
ij (ρ) are arbitrarily many times
diﬀerentiable with respect to ρ, and the derivatives of order r are given by
ω(ε)
ijs(ρ, r) and p(ε)
ij (ρ, r), respectively. Under these conditions we also have
that the functions ϕ(ε)
i (ρ), deﬁned by equation (11), are diﬀerentiable. Let
us denote the corresponding derivatives by ϕ(ε)
Recall from Section 3 that the functions ω(ε)

i (ρ, r).
ijs(ρ) satisfy the following sys-

tem of linear equations,

ω(ε)
ijs(ρ) = δ(i, s)ϕ(ε)

i (ρ) +Xl6=0,j

Diﬀerentiating relation (67) gives

p(ε)
il (ρ)ω(ε)

ljs(ρ), i, j, s 6= 0.

(67)

ω(ε)
ijs(ρ, r) = θ(ε)

ijs(ρ, r) +Xl6=0,j

where

p(ε)
il (ρ)ω(ε)

ljs(ρ, r), r = 1, 2, . . . , i, j, s 6= 0,

(68)

θ(ε)
ijs(ρ, r) = δ(i, s)ϕ(ε)

i (ρ, r) +

rXm=1(cid:18) r

m(cid:19)Xl6=0,j

p(ε)
il (ρ, m)ω(ε)

ljs(ρ, r − m).

(69)

In order to rewrite these systems in matrix form, we deﬁne the following

column vectors,

1js(ρ, r)

· · · ω(ε)

θ(ε)

ω(ε)

js (ρ, r) =hω(ε)
js (ρ, r) =hθ(ε)
s (ρ, r) =hδ(1, s)ϕ(ε)
bϕ(ε)

1js(ρ, r)

1 (ρ, r)

N js(ρ, r)iT
N js(ρ, r)iT

θ(ε)

δ(N, s)ϕ(ε)

· · ·

· · ·

N (ρ, r)iT

, j, s 6= 0,

(70)

, j, s 6= 0,

(71)

, s 6= 0.

(72)

Using (53) and (67)–(72), we can for each j, s 6= 0 write the following

recursive systems of linear equations,

and, for r = 1, 2, . . . ,

s (ρ) + jP(ε)(ρ)ω(ε)

js (ρ),

ω(ε)

js (ρ) = bϕ(ε)

ω(ε)

js (ρ, r) = θ(ε)

js (ρ, r) + jP(ε)(ρ)ω(ε)

js (ρ, r),

19

(73)

(74)

where

s (ρ, r) +

θ(ε)

js (ρ, r) = bϕ(ε)

rXm=1(cid:18) r

m(cid:19)jP(ε)(ρ, m)ω(ε)

js (ρ, r − m).

(75)

In order to construct asymptotic expansions for the vectors ω(ε)

js (ρ, r), we
can use the same technique as in Theorem 1. However, a preliminary step
needed in this case is to construct asymptotic expansions for the functions
ϕ(ε)
i (ρ, r). In order to do this, we ﬁrst derive an expression for these functions.

ψ(ε)

i (ρ, r) =

nreρnPi{κ(ε)

1 = n}, ρ ∈ R, r = 0, 1, . . . , i ∈ X.

(76)

Let us deﬁne

Note that

∞Xn=0
i (ρ, r) =Xj∈X

ψ(ε)

(78)

(79)

(80)

(81)

p(ε)
ij (ρ, r), ρ ∈ R, r = 0, 1, . . . , i ∈ X.

(77)

Thus, the functions ψ(ε)
respect to ρ and the corresponding derivatives are given by ψ(ε)

i (ρ, 0) are arbitrarily many times diﬀerentiable with

i (ρ, r).
i (ρ), deﬁned by equation (11), can be written as

The function ϕ(ε)

i (ρ) =( ψ(ε)

(ψ(ε)

ϕ(ε)

i (0, 1)
ρ = 0,
i (ρ, 0) − 1)/(eρ − 1) ρ 6= 0.

From (76) and (78) it follows that

ψ(ε)
i (ρ, 0) = (eρ − 1)ϕ(ε)

i (ρ) + 1, ρ ∈ R.

Diﬀerentiating both sides of (79) gives

ψ(ε)
i (ρ, r) = (eρ − 1)ϕ(ε)

i (ρ, r) + eρ

i (ρ, m), r = 1, 2, . . .

If ρ = 0, equation (80) implies

ψ(ε)
i (0, r) = rϕ(ε)

i (0, r − 1) +

i (0, m), r = 2, 3, . . .

From this it follows that, for r = 1, 2, . . . ,

ϕ(ε)

i (0, r) =

i (0, r + 1) −

1

r + 1 ψ(ε)

m (cid:19)ϕ(ε)

i (0, m)! .

m(cid:19)ϕ(ε)

r−1Xm=0(cid:18) r
m(cid:19)ϕ(ε)
r−2Xm=0(cid:18) r
r−1Xm=0(cid:18)r + 1

20

If ρ 6= 0, equation (80) gives, for r = 1, 2, . . . ,

ϕ(ε)

i (ρ, r) =

1

eρ − 1 ψ(ε)

i (ρ, r) − eρ

r−1Xm=0(cid:18) r

m(cid:19)ϕ(ε)

i (ρ, m)! .

(82)

Using relations (77), (81), and (82), we can recursively calculate the
derivatives of ϕ(ε)
i (ρ). Furthermore, it follows directly from these formu-
las that we can construct asymptotic expansions for these derivatives. The
formulas are given in the following lemma.

Lemma 5. Assume that conditions A–C hold.

(i) If, in addition, condition P∗

k holds, then for each i 6= 0 and r = 0, . . . , k

we have the following asymptotic expansion,

ψ(ε)
i (ρ, r) = ψi[ρ, r, 0] + ψi[ρ, r, 1]ε + · · · + ψi[ρ, r, k − r]εk−r + o(εk−r),

where

ψi[ρ, r, n] =Xj∈X

pij[ρ, r, n], n = 0, . . . , k − r.

(ii) If, in addition, ρ = 0 and condition P∗

k+1 holds, then for each i 6= 0

and r = 0, . . . , k we have the following asymptotic expansion,

ϕ(ε)
i (0, r) = ϕi[0, r, 0] + ϕi[0, r, 1]ε + · · · + ϕi[0, r, k − r]εk−r + o(εk−r),

where, for n = 0, . . . , k − r,

ϕi[0, r, n] =

1

r + 1 ψi[0, r + 1, n] −

r−1Xm=0(cid:18)r + 1

m (cid:19)ϕi[0, m, n]! .

(iii) If, in addition, ρ 6= 0 and condition P∗

k holds, then for each i 6= 0 and

r = 0, . . . , k we have the following asymptotic expansion,

ϕ(ε)

i (ρ, r) = ϕi[ρ, r, 0] + ϕi[ρ, r, 1]ε + · · · + ϕi[ρ, r, k − r]εk−r + o(εk−r),

where, for n = 0, . . . , k − r,

ϕi[ρ, r, n] =

1

eρ − 1 ψi[ρ, r, n] − eρ

r−1Xm=0(cid:18) r

m(cid:19)ϕi[ρ, m, n]! .

21

Using (72) and Lemma 5 we can now construct the following asymptotic

expansions, for r = 0, . . . , k, and s 6= 0,

bϕ(ε)
s (ρ, r) = bϕs[ρ, r, 0] +bϕs[ρ, r, 1]ε + · · · +bϕs[ρ, r, k − r]εk−r + o(εk−r). (83)

The next lemma gives asymptotic expansions for ω(ε)

js (ρ, r).

Theorem 2. Assume that conditions A–C hold. If ρ = 0, we also assume
that condition P∗
k holds.
Then:

k+1 holds. If ρ 6= 0, we also assume that condition P∗

(i) We have the following asymptotic expansion,

ω(ε)

js (ρ) = ωjs[ρ, 0, 0] + ωjs[ρ, 0, 1]ε + · · · + ωjs[ρ, 0, k]εk + o(εk),

where

js (ρ)

ωjs[ρ, 0, n] =( ω(0)
Pn
q=0 jU[ρ, q]bϕs[ρ, 0, n − q] n = 1, . . . , k.

n = 0,

(ii) For r = 1, . . . , k, we have the following asymptotic expansions,

ω(ε)

js (ρ, r) = ωjs[ρ, r, 0]+ ωjs[ρ, r, 1]ε+· · ·+ ωjs[ρ, r, k −r]εk−r + o(εk−r),

where

ωjs[ρ, r, n] =( ω(0)
Pn

and, for t = 0, . . . , k − r,

js (ρ, r)
q=0 jU[ρ, q]θjs[ρ, r, n − q] n = 1, . . . , k − r,

n = 0,

θjs[ρ, r, t] = bϕs[ρ, r, t] +

rXm=1(cid:18) r

m(cid:19) tXq=0

jP[ρ, m, q]ωjs[ρ, r − m, t − q].

Proof. Under conditions A–C, we have, for suﬃciently small ε, that the
recursive systems of linear equations given by relations (73), (74), and (75),
all have ﬁnite components. Moreover, the inverse matrix jU(ε)(ρ) = (I −
jP(ε)(ρ))−1 exists, so these systems have unique solutions. Since we, by
Lemma 5, have the expansions given in equation (83), the proof is from this
point analogous to the proof of Theorem 1.

22

References

[1] Avrachenkov, K.E., Filar, J.A., Howlett, P.G.: Analytic Perturbation

Theory and Its Applications. SIAM, Philadelphia (2013)

[2] Gyllenberg, M., Silvestrov, D.S.: Quasi-Stationary Phenomena in Non-
linearly Perturbed Stochastic Systems. De Gruyter Expositions in Math-
ematics, vol. 44. Walter de Gruyter, Berlin (2008)

[3] Kartashov, M.V.: Strong Stable Markov Chains. VSP, Utrecht and

TBiMC, Kiev (1996)

[4] Koroliuk, V.S., Limnios, N.: Stochastic Systems in Merging Phase

Space. World Scientiﬁc, Singapore (2005)

[5] Petersson, M.: Quasi-stationary asymptotics for perturbed semi-Markov
processes in discrete time. Research Report 2015:2, Department of Math-
ematics, Stockholm University, 36 pp. (2015)

[6] Petersson, M.: Asymptotics for quasi-stationary distributions of per-

turbed discrete time semi-Markov processes. arXiv (2016)

[7] Yin, G., Zhang, Q.: Continuous-Time Markov Chains and Applications.
A Singular Perturbation Approach. Applications of Mathematics, vol.
37. Springer, New York (1998)

23

