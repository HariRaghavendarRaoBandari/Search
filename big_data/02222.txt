6
1
0
2

 

b
e
F
2
2

 

 
 
]
P
A
h
t
a
m

.

[
 
 

1
v
2
2
2
2
0

.

3
0
6
1
:
v
i
X
r
a

ROBUST IMAGING WITH ELECTROMAGNETIC WAVES IN NOISY

ENVIRONMENTS

LILIANA BORCEA∗ AND JOSSELIN GARNIER†

Abstract. We study imaging with an array of sensors that probes a medium with single fre-
quency electromagnetic waves and records the scattered electric ﬁeld. The medium is known and
homogenous except for some small and penetrable inclusions. The goal of inversion is to locate and
characterize these inclusions from the data collected by the array, which are corrupted by additive
noise. We use results from random matrix theory to obtain a robust inversion method. We assess
its performance with numerical simulations and quantify the beneﬁt of measuring more than one
component of the scattered electric ﬁeld.

Key words. imaging, electromagnetic waves, random matrix theory.

1. Introduction. We study an inverse problem for Maxwell’s equations, where
unknown scatterers in a medium are to be determined from data collected with an
array of sources and receivers. The sources emit signals which propagate in the
medium and the receivers record the backscattered waves. The recordings, indexed
by the source and receiver pair, are the entries in the response matrix, the data, which
are contaminated with additive noise.

Sensor array imaging is an important technology in nondestructive testing, med-
ical imaging, radar, seismic exploration, and elsewhere. It operates over application
speciﬁc ranges of frequencies and ﬁxed or synthetic apertures. We consider imaging
with single frequency waves and a ﬁxed array of N sensors that play the dual role of
sources and receivers. The unknown scatterers lie in a homogeneous medium. They
are P penetrable inclusions of small size with respect to the wavelength. As shown in
[5] they can be described by their center location (cid:126)yp and reﬂectivity tensor ρp which
depends on the volume and shape of their support, and their electric permittivity p,
for p = 1, . . . , P .

There are many algorithms for locating scatterers from the response matrix. Re-
verse time or Kirchhoﬀ migration [11] and the similar ﬁltered backprojection [14] are
common in seismic and radar imaging with broadband data. Matched ﬁeld processing
[7] is popular in underwater acoustics. Qualitative methods such as linear sampling
[12], the factorization method [20], and the MUSIC (Multiple Signal Classiﬁcation)
algorithm [22] are useful for imaging with single frequency waves, and give high res-
olution results if the noise is weak.

A study of noise eﬀects on the MUSIC method can be found in [16]. It determines
upper bounds on the noise level that guarantee the exact recovery of point scatterers in
a sparse scene. Here we consider stronger noise and use results from random matrix
theory [10, 13, 8, 21] to obtain a robust MUSIC type localization method. Such
results have led to scatterer detection tests in [3, 4], and to ﬁlters of backscattered
waves in random media in [6, 1]. They are also the foundation of a robust sonar array
imaging method of well separated point scatterers in [17]. We extend the results
in [17] to electromagnetic waves, for locating one or more inclusions and estimating
their reﬂectivity tensor. We also quantify the beneﬁts of measuring more than one
component of the scattered electric ﬁeld.

∗Department of Mathematics, University of Michigan, Ann Arbor, MI 48109. borcea@umich.edu
†Laboratoire de Probabilit´es et Mod`eles Al´eatoires & Laboratoire Jacques-Louis Lions, Universit´e

Paris Diderot, 75205 Paris Cedex 13, France. garnier@math.univ-paris-diderot.fr

1

a

A

(cid:126)e2

(cid:126)e3

(cid:126)e1

L

(cid:126)y2

(cid:126)y1

(cid:126)y3

Fig. 2.1. Illustration of the inversion setup. The array A is square planar with side a, in the
cross-range plane spanned by (cid:126)e1 and (cid:126)e2. The imaging region is at range scale L from the array. In
the illustration there are three inclusions at locations (cid:126)yp, for p = 1, 2, 3.

The paper is organized as follows: We begin in section 2 with the mathematical
formulation of the inverse problem, and then give in section 3 the data model. The
singular value decomposition analysis of the response matrix is described in section
4. The inversion method uses this analysis and is presented in section 5. Numerical
simulation results are shown in section 6. We end with a summary in section 7.

2. Formulation of the problem. Consider a homogeneous isotropic medium
with electric permittivity o and magnetic permeability µo, which contains P pene-
trable inclusions supported in the disjoint simply connected domains Ωp with smooth
boundaries, centered at locations (cid:126)yp, for p = 1, . . . , P . The inclusions are modelled
by the piecewise constant electric permittivity

P(cid:88)

P(cid:91)

((cid:126)x) =

p1Ωp ((cid:126)x) + o1R3\Ω((cid:126)x),

Ω =

Ωp,

(2.1)

p=1

p=1

where 1Ω((cid:126)x) is the indicator function of the domain Ω, and p are scalar real valued
for p = 1, . . . , P .
The medium is probed with N sensors that are both sources and receivers. They
are closely spaced at locations {(cid:126)xr}1≤r≤N in a set A, so they behave like a collective
entity, the array. For convenience we let A be planar and square, of side a, the array
aperture. This allows us to introduce a system of coordinates with origin at the center
of the array and orthonormal basis {(cid:126)e1, (cid:126)e2, (cid:126)e3}. The vectors (cid:126)e1, (cid:126)e2 span the plane of
the array, named the cross-range plane, and the line along (cid:126)e3 is the range direction.
The distance between the array and the inclusions is of order L, the range scale. See
Figure 2.1 for an illustration.
We index consistently a source sensor by s and a receiver sensor by r, altough
both s and r take values in the same set {1, . . . , N}. The sources are point dipoles
with current density (cid:126)jsδ((cid:126)x − (cid:126)xs), and the resulting electric ﬁeld (cid:126)E satisﬁes

(cid:126)∇ × (cid:126)∇ × (cid:126)E((cid:126)x; (cid:126)xs) − k2 ((cid:126)x)
o

(cid:126)E((cid:126)x; (cid:126)xs) = ik

(cid:126)jsδ((cid:126)x − (cid:126)xs),

(2.2)

and the radiation condition

(cid:20)

|(cid:126)x|

lim|(cid:126)x|→∞

(cid:126)∇ × (cid:126)E((cid:126)x; (cid:126)xs) − ik

2

(cid:21)
|(cid:126)x| × (cid:126)E((cid:126)x; (cid:126)xs)

(cid:126)x

= 0.

(2.3)

(cid:114) µo

o

Equation (2.3) is derived from Maxwell’s equations at frequency ω, and

k =
√
is the wavenumber, where co = 1/

ω
co

=

2π
λ

We decompose (cid:126)E in the direct ﬁeld (cid:126)Eo and the scattered one denoted by (cid:126)E,

µoo is the wave speed and λ is the wavelength.

(cid:126)E((cid:126)x; (cid:126)xs) = (cid:126)E((cid:126)x; (cid:126)xs) − (cid:126)Eo((cid:126)x; (cid:126)xs).

(2.4)

The direct ﬁeld solves the analogue of (2.2)–(2.3) in the homogeneous medium, and
can be written explicitly as

(cid:126)Eo((cid:126)x; (cid:126)xs) = ik

G((cid:126)x, (cid:126)xs)(cid:126)js,

using the dyadic Green’s tensor

G((cid:126)x, (cid:126)z) =

(cid:18)

(cid:19) eik|(cid:126)x−(cid:126)z|

4π|(cid:126)x − (cid:126)z| .

∇∇T
k2

I3 +

(2.5)

(2.6)

(cid:114) µo

o

The scattered ﬁeld (cid:126)E is recorded at the N receivers, and the experiment may be
repeated by emitting waves sequentially from all the sources. Each source may use up
to three illuminations, with linearly independent (cid:126)js ∈ R3, and the receivers may record
one or all three components of (cid:126)E. We call the data for all possible illuminations and
recordings complete. All other cases, with one or two illuminations from each source
and one or two components of (cid:126)E measured at the receivers are called incomplete.
The goal is to locate and characterize the inclusions using the complete or incomplete
measurements.

We simplify the problem by assuming that the inclusions are small with respect
to the wavelength, so we can represent them as in [5] by their center locations (cid:126)yp and
3 × 3 reﬂectivity tensors ρp, for p = 1, . . . , P . These tensors depend on the shape of
the domains Ωp and the relative electric permittivity p/o. The inverse problem is to
determine the locations (cid:126)yp and reﬂectivity tensors ρp from the array measurements.

3. Data model. Let α be a small and positive dimensionless parameter which

scales the size of the inclusions

[Vol(Ωp)]1/3

∼ α (cid:28) 1,

(3.1)
and denote by (cid:126)E q((cid:126)x; (cid:126)xs) the scattered ﬁeld due to the source at (cid:126)xs, with (cid:126)js = (cid:126)eq. It
has the following asymptotic expansion in the limit α → 0, as shown in [5],

p = 1, . . . , P,

λ

(cid:126)E q((cid:126)x; (cid:126)xs) = ik3

G((cid:126)x, (cid:126)yp)ρpG((cid:126)yp, (cid:126)xs)(cid:126)eq + O(α4) .

(3.2)

The reﬂectivity tensor of the m−th inclusion is deﬁned by

(cid:114) µo

P(cid:88)

o

p=1

(cid:19)

− 1

(cid:18) p

o

ρp = α3

Mp,

(3.3)

using the α-independent polarization tensor Mp. We refer to [5] for details on Mp,
and to section 6 for its calculation in our numerical simulations. It suﬃces to say that
Mp is a symmetric, real valued 3 × 3 matrix which depends on the shape of Ωp and
the relative electric permittivity p/o.

3

3.1. The noiseless data model. Since k, µo and o are known, let us model

the noiseless data gathered with the receiver and source pair (r, s) by

(cid:112)o/µo

(cid:16)(cid:126)E 1, (cid:126)E 2, (cid:126)E 3

(cid:17)

,

d((cid:126)xr, (cid:126)xs) =

(3.4)
This is a 3 × 3 complex matrix, the (r, s) block of the 3N × 3N complete data matrix
D collected by the array,

r, s = 1, . . . , N.

ik3

D = (d((cid:126)xr, (cid:126)xs))r,s=1,...,N .

(3.5)

To acquire D directly, the array would use the sources sequentially, with current
densities (cid:126)eqδ((cid:126)x−(cid:126)xs), for q = 1, 2, 3 and s = 1, . . . , N , and would measure the scattered
ﬁelds (cid:126)E q((cid:126)x; (cid:126)xs) at the receivers. This acquisition method is not optimal in noisy
environments, and may be improved as explained in the next section.
When the sensors emit and receive only along one or two directions, we have
incomplete measurements. Let us denote by S ⊂ {1, 2, 3} the set of indices of the
measured components of the electric ﬁeld, and deﬁne the 3 × |S | sensing matrix

S = ((cid:126)eq)q∈S ,

(3.6)
with columns given by the unit vectors (cid:126)eq with q ∈ S . We assume for convenience
that the source excitation currents are also limited to these directions, and denote
the incomplete data matrix by DS . It is a complex |S |N × |S |N matrix that can be
written in terms of D as

DS = diag(cid:0)ST , . . . , ST(cid:1) Ddiag (S, . . . , S) .

(3.7)

The mathematical model of D follows from (3.2) and (3.4). We write it using the
forward map F , which takes the unknown locations and reﬂectivity tensors of the
inclusions to the space C3N×3N of matrices where D lies,

(cid:32) P(cid:88)

(cid:33)

D ≈ F ((cid:126)y1, . . . , (cid:126)yP , ρ1, . . . , ρP ) =

G((cid:126)xr, (cid:126)yp)ρpG((cid:126)yp, (cid:126)xs)

(3.8)

p=1

r,s=1,...,N.

The forward map for the incomplete measurements is deduced from (3.7)–(3.8). We
assume henceforth that F gives a good approximation of D, as is the case for suﬃ-
ciently small α, and treat the approximation in (3.8) as an equality.

3.2. Noisy data and the Hadamard acquisition scheme. In any practical
setting the measurements are contaminated with noise, which may be mitigated by
the data acquisition scheme, as we now explain.

Suppose that the N sources do not emit waves sequentially, but at the same
time, for 3N experiments indexed by (n, q), with n = 1, . . . , N and q = 1, 2, 3. In the
(n, q) experiment the excitation from the source at (cid:126)xs is (cid:126)js,(n,q), and gathering the
measurements for q = 1, 2, 3, we obtain the following model of the recordings at the
receiver location (cid:126)xr,

N(cid:88)

s=1

4

dH

r,n =

d((cid:126)xr, (cid:126)xs)js,n + wr,n.

(3.9)

This is a 3×3 complex matrix, determined by d((cid:126)xr, (cid:126)xs) deﬁned in (3.4), the excitation
matrices js,n with columns (cid:126)js,(n,q) for q = 1, 2, 3, and the noise matrix wr,n. The
complete 3N × 3N measurement matrix DH has the block structure

DH =(cid:0)dH

(cid:1)

r,n

r,n=1,...,N

,

and it is modeled by

DH = DJ + W,

(3.10)
where J is the excitation matrix with blocks js,n for s, n = 1, . . . , N , and W is the
noise matrix with blocks wr,n for r, n = 1, . . . , N .
We assume that the excitation currents are normalized so that the entries in J
have unit amplitudes, and take as is usual a noise matrix W with mean zero and
independent complex Gaussian entries with standard deviation σ. This gives

E(cid:104)W†W(cid:105)

= σ2I3N ,

where † denotes the complex conjugate and transpose, E is the expectation, and I3N
is the 3N ×3N identity matrix. The acquisition scheme uses an invertible J, to obtain
(3.11)

(cid:101)D = DHJ−1 = D + W, W = WJ−1.

This is a corrupted version of D, but the noise level in W, which has mean zero
complex Gaussian entries, is reduced for a good choice of J. Such a choice would give

E(cid:2)W†W(cid:3) =(cid:0)J−1(cid:1)†E(cid:104)W†W(cid:105)

J−1 = σ2(cid:0)J−1(cid:1)†

J−1 = σ2

W

I3N ,

(3.12)

and a minimum σW . We conclude that J must be unitary, up to a multiplicative
constant factor, and its determinant should be maximal to get the smallest

σ

σW =

| det(J)|1/(3N )

.

It is shown in [18] that the determinant of 3N × 3N matrices with entries in the
complex unit disk is bounded above by (3N )3N/2, with equality attained by complex
Hadamard matrices. These have entries with modulus one, and are unitary up to the
scaling by their determinant. Thus, by using a Hadamard matrix J, we can mitigate

the noise in (cid:101)D and reduce its standard deviation to

√
σW = σ/

3N .

(3.13)

The results extend verbatim to the incomplete measurement setup. The diﬀerence
is that in (3.9) we have ST d((cid:126)xr, (cid:126)xs)S instead of d((cid:126)xr, (cid:126)xs), and the 3 × 3 current
excitation matrix js,n is replaced by ST js,nS, the blocks in the complex |S |N ×|S |N

Haddamard matrix. The noise level in(cid:101)DS = DS + WS
is reduced to σ/(cid:112)|S |N .
correlation structure, let us denote by C = E(cid:104)W†W(cid:105)
with σW = O(cid:0)σ/|det(J)|1/(3N )(cid:1).

Remark: In case that the noise at nearby receivers is correlated, with known
the Hermitian covariance ma-
trix, and suppose that it is positive deﬁnite, with O((σ2)3N ) determinant. Then we
can use its square root to deﬁne an invertible matrix J so that C1/2J−1 is unitary up
to scaling by its determinant, and obtain that W in (3.11) has uncorrelated entries

(3.14)

5

4. Singular value decomposition analysis. Our imaging algorithm is based
on the Singular Value Decomposition (SVD) of the response matrix. We begin in
section 4.1 with the SVD analysis in the noiseless case. This uses the rank calculation
of the matrix of Green’s functions given in section 4.2. The eﬀect of the additive noise
on the SVD is discussed in section 4.3.

4.1. SVD analysis of the noiseless data matrix. Let us introduce the 3N×3

matrix G((cid:126)y) of Green’s functions, with block structure

 G((cid:126)x1, (cid:126)y)

...

 ,

G((cid:126)xN , (cid:126)y)

G((cid:126)y) =

P(cid:88)

and use the reciprocity relation G((cid:126)x, (cid:126)y) = G((cid:126)y, (cid:126)x)T to write the model in (3.8) as

D =

G((cid:126)yp)ρpG((cid:126)yp)T .

(4.2)

We show in section 4.2 that in our setting G((cid:126)y) is full rank, so let

p=1

G((cid:126)y) = H((cid:126)y)Σ((cid:126)y)V((cid:126)y)†

(4.3)
be its SVD, with H ∈ C3N×3 satisfying H†H = I3, and unitary V ∈ C3×3. The singu-
lar values of G((cid:126)y) are in the diagonal positive deﬁnite 3×3 matrix Σ((cid:126)y). Substituting
(4.3) in (4.2) gives

(4.1)

(4.4)

(4.5)

(4.6)

(4.7)

P(cid:88)

D =

H((cid:126)yp)RpH((cid:126)yp)T ,

with complex symmetric 3 × 3 matrices

p=1

Rp = Σ((cid:126)yp)V((cid:126)yp)†ρpV((cid:126)yp)Σ((cid:126)yp),

where the bar denotes complex conjugate. Moreover, if

Rp = UpSpV†

p

is an SVD of Rp, we obtain from (4.4) that

P(cid:88)

D =

H((cid:126)yp)UpSpV†

pH((cid:126)yp)T .

p=1

Our imaging algorithm uses the relation between the column space (range) of
G((cid:126)y), or equivalently H((cid:126)y), and the subspace spanned by the left singular vectors
of D. This follows from (4.7) and depends on the number of inclusions and their
separation distance, as we now explain.

Single inclusion. When P = 1 we see from (4.7) that D has an SVD with nonzero
singular values σ1, σ2, σ3, the entries in S1, and a 3N × 3 matrix of corresponing left
singular vectors H((cid:126)y1)U1. However, even when the singular values are distinct, the
singular vectors are deﬁned up to an arbitrary phase, so we cannot assume that the

6

computed 3N × 3 matrix U of left singular vectors of D equals H((cid:126)y1)U1. We work
instead with projection matrices which are uniquely deﬁned, and satisfy

†
1H((cid:126)y1)† = H((cid:126)y1)H((cid:126)y1)†,
UU† = H((cid:126)y1)U1U

(4.8)

because U1 is unitary. We conclude that the column space of D is the same as that
of G((cid:126)y1), and moreover, that the 3N × 3N projection on this space can be calculated
as UU† using the left singular vectors of D, or equivalently as H((cid:126)y1)H((cid:126)y1)†, using
the left singular vectors of G((cid:126)y1).
Multiple inclusions. When 1 < P < N , the rank of D depends on the locations
of the inclusions [15]. Generically∗ it equals 3P , as we assume here to simplify the
presentation. Let σ1, . . . , σ3P be the nonzero singular values of D and

U = (u1, . . . , u3P )

the 3N × 3P matrix of its corresponding left singular vectors. The projections
H((cid:126)yp)H((cid:126)yp)† are no longer the same as those obtained from 3N × 3 blocks in U,
unless the inclusions are so far apart that the column spaces of G((cid:126)yp) and G((cid:126)yp(cid:48)) are
orthogonal for p (cid:54)= p(cid:48). Nearby inclusions interact and the singular values and vectors
of D are not associated with a single inclusion. However, we conclude from (4.2) and
(4.7) that

range (G((cid:126)yp)) = range (H((cid:126)yp)) ⊂ range(D) = span{u1, . . . , u3P}.

(4.9)

Moreover, it is shown in [5, Proposition 4.3] that

G((cid:126)y)(cid:126)e ∈ range(D)

iﬀ (cid:126)y ∈ {(cid:126)y1, . . . , (cid:126)yP},

(4.10)
for any (cid:126)e such that G((cid:126)y)(cid:126)e (cid:54)= 0. In our setting G((cid:126)y) has full rank, so (4.10) holds
for any unit vector (cid:126)e ∈ R3. This implies in particular that the left singular vectors of
G((cid:126)y), the columns of H((cid:126)y), are in the range of D if and only if (cid:126)y coincides with the
location of an inclusion.

The results are very similar for incomplete measurements, with data modeled by

P(cid:88)

p=1

DS =

GS ((cid:126)yp)ρpGS ((cid:126)yp)T ,

(4.11)

in terms of the |S |N × 3 matrices of Green’s functions

GS ((cid:126)y) = diag(cid:0)ST , . . . , ST(cid:1)G((cid:126)y).

(4.12)
We show in the next section that the rank of GS is still three, as in the complete
measurement case, but its condition number may be much worse. This plays a role
in the inversion with noisy measurements. We denote with the same symbol U the
matrix of left singular vectors of DS and H((cid:126)y) the matrix of left singular vectors of
GS ((cid:126)y). We conclude as in the complete measurement case that when P = 1,

UU† = H((cid:126)y1)H((cid:126)y1)†,

and when 1 < P < N , using [5, Proposition 4.3],
range (GS ((cid:126)y)) = range (H((cid:126)y)) ⊂ range (DS )

iﬀ (cid:126)y ∈ {(cid:126)y1, . . . , (cid:126)yP}.
∗The rank may be smaller than 3P for very special locations of the inclusions [15].

(4.13)

7

4.2. Rank and conditioning of matrix of Green’s functions. As mentioned
above and explained in section 5, the rank and condition number of G((cid:126)y) play a role in
imaging with noisy measurements. We analyze them here using the Hermitian matrix

N(cid:88)

Q((cid:126)y) = G((cid:126)y)†G((cid:126)y) =

≈ N(cid:88)

r=1

r=1

1

(4π|(cid:126)xr − (cid:126)y|)2

G((cid:126)xr, (cid:126)y)†G((cid:126)xr, (cid:126)y)
(cid:20)
I3 − ((cid:126)xr − (cid:126)y)
|(cid:126)xr − (cid:126)y|

((cid:126)xr − (cid:126)y)T
|(cid:126)xr − (cid:126)y|

(cid:21)

,

(4.14)

whose eigenvalues equal the square of the singular values of G((cid:126)y), the entries in Σ((cid:126)y).
The approximation in (4.14) uses deﬁnition (2.6) of the Green’s tensor, written as

G((cid:126)xr, (cid:126)y) =

eik|(cid:126)xr−(cid:126)y|
4π|(cid:126)xr − (cid:126)y|

(cid:20)
I3 − ((cid:126)xr − (cid:126)y)
|(cid:126)xr − (cid:126)y|

(cid:18) 1

(cid:19)(cid:21)

kL

((cid:126)xr − (cid:126)y)T
|(cid:126)xr − (cid:126)y| + O

,

(4.15)

and neglects the O(1/(kL)) residual which is small when the inclusions are many
wavelengths away from the array.
Equation (4.14) shows that Q((cid:126)y) is, up to some scalar factors, the sum of projec-
tion matrices on the plane orthogonal to (cid:126)xr − (cid:126)y. As (cid:126)xr varies in the array aperture,
(cid:126)xr − (cid:126)y changes direction in a cone of opening angle of order a/L. The smaller this
angle, the worse the conditioning of Q((cid:126)y). Then let us consider the small aperture
regime a (cid:28) L, where we can approximate the right hand side in (4.14) by

I3 − (cid:126)e3(cid:126)eT

3 − a
LN

 az2
N(cid:88)

L

r=1

r,1
L

−zr1

azr,1zr,2



 ,

azr,1zr,2

L
az2
r,2
L

−zr2

−zr1
−zr2
− a|zr|2

L

(4.16)

Q((cid:126)y) ≈ N

(4πL)2

using the notation

zr = (xr − y)/a = (zr,1, zr,2),

N(cid:88)

for (cid:126)xr = (xr, 0) and (cid:126)y = (y, L) satisfying |y| (cid:46) a (cid:28) L. Here we neglected an
O(a3/L3) matrix residual in the sum over the N sensors. The leading part of (4.16),
N/(4πL)2[I3 − (cid:126)e3(cid:126)eT
3 ], is proportional to the orthogonal projection on the cross-range
plane, and determines the two larger eigenvalues of Q((cid:126)y), which are approximately
N/(4πL)2. The third eigenvalue is much smaller, by a factor of order a2/L2. Thus,
Q((cid:126)y) is full rank, but it is poorly conditioned in the small aperture regime.
For incomplete measurements we can determine the rank of GS ((cid:126)y) from that of

QS ((cid:126)y) = GS ((cid:126)y)†GS ((cid:126)y) =

G((cid:126)xr, (cid:126)y)†SST G((cid:126)xr, (cid:126)y).

(4.17)

r=1

The estimation of the eigenvalues of QS ((cid:126)y) is similar to the above. We obtain that
when a (cid:28) L, and for measurements along a single direction (cid:126)e1 or (cid:126)e2, the matrix Q((cid:126)y)
has one large eigenvalue approximated by N/(4πL)2, a second eigenvalue smaller by
a factor of order a2/L2, and an even smaller third one. When the measurements
are made in the longitudinal direction (cid:126)e3, all the eigenvalues are much smaller than
N/(4πL)2. This is the worse measurement setup. Finally, when all the transversal
components of the ﬁeld are measured, we note that since

SST = I3 − (cid:126)e3(cid:126)eT
3 ,
8

S = ((cid:126)e1, (cid:126)e2),

(cid:101)B = B + W,

QS is a small perturbation of Q. It has almost the same condition number as Q((cid:126)y),
and this improves as the ratio a/L grows.

4.3. Additive noise eﬀects on the SVD. To unify the discussion for complete

and incomplete measurements, let us consider the generic problem

(4.18)
for a low rank R matrix B ∈ CM×M corrupted with additive noise. In our context
M = 3N for complete measurements and |S |N for incomplete ones, and R = 3P .

Moreover, (cid:101)B equals (cid:101)D or (cid:101)DS , and it is modeled by (3.11) and (3.14) in the Hadamard
The matrix (cid:101)B has singular values(cid:101)σj and left singular vectors(cid:101)uj, for j = 1, . . . , M .

data acquisition scheme described in section 3.2. The noise matrix W has mean
√
zero and independent, identically distributed complex Gaussian entries of standard
deviation σ/

Its rank is typically M because of the almost surely full rank noise matrix, but we
are interested in its few singular values that can be distinguished from noise, and the
associated singular vectors. These are perturbations of the singular values σj and left
singular vectors uj of B, for j = 1, . . . , R, and are described below in the asymptotic
limit M → ∞.

M .

We begin with the asymptotic approximation of the square of the Frobenius norm

which satisﬁes [21, 19, 8, 13].

(cid:107)(cid:101)B(cid:107)2

F

M(cid:88)

j=1

(cid:101)σ2

j ,

=

j,q=1

M(cid:88)
|(cid:101)Bjq|2 =
 → R(cid:88)

 1

M

M

M(cid:88)

(cid:101)σ2
j − σ2

j=1

j=1

j + σ2Z0,
σ2

as M → ∞,

(4.19)

where the convergence is in distribution, and Z0 follows a Gaussian distribution with
mean zero and variance one. We use this result in section 5 to estimate the noise level
σ from the measurements. The behavior of the leading singular values and singular

vectors of (cid:101)B described in the next theorem is used in section 5 to obtain a robust
Theorem 4.1. Let B ∈ CM×M be the matrix of ﬁxed rank R in (4.18), and (cid:101)B

localization of the inclusions.

√
its corrupted version by the noise matrix W with mean zero, independent, identically
distributed complex Gaussian entries of standard deviation σ/
(i) For j = 1, . . . , R, and in the limit M → ∞, the perturbed singular values satisfy

M .

√

M

and

(cid:33)1/2

(cid:32)

(cid:34)(cid:101)σj − σj

(cid:33)(cid:35)

1 +

σ2
σ2
j

→ σ√
2

1 − σ2
σ2
j

M 2/3((cid:101)σj − 2σ) → σ

Z2,

(4.21)
where the convergence is in distribution, Z0 follows a Gaussian distribution with mean
zero and variance one, and Z2 follows a type-2 Tracy Widom distribution.
(ii) Under the additional assumption that the R nonzero singular values of B are

if σj < σ,

22/3

Z0,

if σj > σ,

(4.20)

(cid:32)

9

juj|2 → 1 − σ2
†
σ2
j
|(cid:101)u†
quj| → 0,
(cid:16) −

(cid:90) ∞

(4.23)

(4.24)

distinct, and for indices j such that σj > σ, the left singular vectors uj and (cid:101)uj of B
and (cid:101)B satisfy in the limit M → ∞
|(cid:101)u

(4.22)

,

and for q (cid:54)= j ≤ R,

where the convergence is in probability.

The type-2 Tracy-Widom distribution has the cumulative distribution function

ΦTW2 given by

(cid:17)

ΦTW2(z) = exp

(x − z)ϕ2(x)dx

,

with ϕ(x) the solution of the Painlev´e equation

z

ϕ(cid:48)(cid:48)(x) = xϕ(x) + 2ϕ(x)3, ϕ(x) (cid:39) Ai(x), x → ∞,

(4.25)
and Ai the Airy function. The expectation of Z2 is E[Z2] (cid:39) −1.771 and its variance
is Var(Z2) (cid:39) 0.813. Details about the Tracy-Widom distributions can be found in [9].
Theorem 4.1 was already stated in [17] for the special case R = 1. The proof of
the extension to an arbitrary rank R can be obtained from the method described in
[10]. Note that formula (4.20) seems to predict that the standard deviation of the
order M−1/2.
In fact the standard deviation becomes of order M−2/3. Following
[8], we can anticipate that there are interpolating distributions which appear when
σj = σ + wM−1/3 for some ﬁxed w.

perturbed singular value (cid:101)σj cancels when σj (cid:38) σ, but this is true only to leading

5. Inversion with noisy data. We use the singular value decomposition anal-
ysis in the previous section to obtain a robust inversion method for noisy array data.
We begin in section 5.1 with the estimation of the noise level. Then we formulate in
section 5.2 the method for localizing the inclusions. The estimation of their reﬂectivity
tensor is described in section 5.3.

5.1. Estimation of the noise level. A detailed analysis of the estimation of
σ, for a variety of cases, is given in [17]. Here N (cid:29) P and we assume that at least
one singular value of the noisy data matrix can be distinguished from the others, so
we can image. This simpliﬁes the estimation of σ, as we now explain.

Let us begin by rewriting (4.19) as

M(cid:88)

j − (M − 4R)σ2 ∼ R(cid:88)
(cid:101)σ2

(cid:2)σ2
j −(cid:0)(cid:101)σ2

j − 4σ2(cid:1)(cid:3) + σ2Z0,

j=R+1

j=1

(5.1)

where we recall that R = 3P , and M = 3N for complete measurements and |S |N
otherwise. The symbol ∼ stands for approximate, in the asymptotic regime M (cid:29) 1,
and there is no bias in the right hand side in the absence of the inclusions i.e., when

σj = 0 and(cid:101)σj ∼ 2σ for the ﬁrst indices j by (4.21).
M(cid:88)
(cid:101)σ2

σe =

1

M − 4R

j ,

The unbiased estimate of the noise level σ follows from (5.1),

(5.2)

j=R+1

10

but it requires prior knowledge of the number of inclusions. If this is unknown, we
can use the empirical estimate

M(cid:88)

(cid:101)σ2

j ,

σe =

1

M − 4Re

where Re is the number of singular values of (cid:101)B that are signiﬁcantly larger than the

others. The estimate (5.3) is very close to (5.2) because M (cid:29) R ≥ Re.

j=Re+1

5.2. Localization of the inclusions. We assume henceforth that the noiseless
matrix B, equal to D for complete measurements and DS for incomplete ones, has
R = 3P distinct singular values indexed in decreasing order as σ1, . . . , σR. The
corresponding left singular vectors are the columns uj of the M × R matrix

(5.3)

U = (u1, . . . , uR) .

The singular values of the noisy matrix (cid:101)B are denoted by (cid:101)σj, for j = 1, . . . , M . We
are interested in the ﬁrst (cid:101)R of them, which can be distinguished from the noise.
The eﬀective rank (cid:101)R is determined using Theorem 4.1,

(5.4)

(cid:101)R = max{j = 1, . . . , M such that (cid:101)σj > σerθ}.

(5.5)

(5.6)

where the threshold rθ is deﬁned

rθ = 2 +

1

(2M ) 2

3

TW2(1 − θ),
Φ−1

(cid:101)U =

(cid:16)(cid:101)u1, . . . ,(cid:101)u(cid:102)R

(cid:17)

with ΦTW2 the cumulative distribution function (4.24) of the Tracy-Widom distribu-
tion of type 2. As shown in [17, 2], by the Neyman-Pearson lemma, the decision rule
(5.5) maximizes the probability of detection for a given false alarm rate θ. This is
a user deﬁned number satisfying 0 < θ (cid:28) 1. In our case M is large, so for all θ we
collect the singular values that are signiﬁcantly larger than 2σe.

We let

be the matrix of the leading left singular vectors of (cid:101)B. The MUSIC method [5]
of (cid:101)U, for some vector (cid:126)e and search points (cid:126)y. With (cid:126)e = (cid:126)e1 we obtain the MUSIC
determines the locations of the inclusions by projecting G((cid:126)y)(cid:126)e or GS ((cid:126)y)(cid:126)e on the range

(5.7)

imaging function

IMUSIC ((cid:126)y) =

(cid:13)(cid:13)(cid:13)(cid:16)

IM − (cid:101)U(cid:101)U†(cid:17)G((cid:126)y)(cid:126)e1

(cid:13)(cid:13)(cid:13)−1

F

,

(5.8)

in the case of complete measurements, where M = 3N . It is the same for incomplete
measurements, except that G((cid:126)y) is replaced by GS ((cid:126)y) and M = |S |N .
(4.10). As the noise level grows the eﬀective rank (cid:101)R decreases, the subspace spanned
by the columns of (cid:101)U changes, and the MUSIC images deteriorate. We show next how

If noise were negligible, (5.8) would peak at the locations of the inclusions due to

to improve the localization method using the results in Theorem 4.1.

11

5.2.1. Localization of one inclusion or well separated inclusions. The
robust localization is based on (4.8) and its equivalent for incomplete measurements,
and Theorem 4.1. It consists of the following steps:

Step 1. Estimate the (cid:101)R singular values of the unknown unperturbed matrix B from

equation (4.20), rewritten as

Replacing σ by its estimate σe, and choosing the root that is larger than σe, we obtain

Step 2. Estimate the changes of direction of the leading singular vectors, using (4.22),

(cid:32)

(cid:33)

1 +

.

σ2
σ2
j

(cid:101)σj ≈ σj
(cid:113)(cid:101)σ2
(cid:104)(cid:101)σj +

(cid:105)
j − (2σe)2
(cid:32)
(cid:33)2

,

j = 1 −

σe
σe
j

,

j = 1, . . . ,(cid:101)R.

j = 1, . . . ,(cid:101)R.

j, q = 1, . . . ,(cid:101)R,

j (cid:54)= q.

σe
j =

1
2

|(cid:101)u

†
juj|2 ≈ cos2 θe
|(cid:101)u†
quj|2 ≈ 0,

(5.9)

(5.10)

(5.12)

We also have from (4.23) that

(5.11)
Step 3. For the search point (cid:126)y calculate the matrix G((cid:126)y) given by (4.1) for complete
measurements, or GS ((cid:126)y) given by (4.12) for incomplete measurements. Determine the
matrix H((cid:126)y) of their left singular vectors, and calculate

When (cid:126)y = (cid:126)y1, we conclude from (4.8) that

T((cid:126)y) = (cid:101)U†H((cid:126)y)H((cid:126)y)†(cid:101)U.
T((cid:126)y1) = (cid:101)U†UU†(cid:101)U.

Here U is unknown, but (5.10)–(5.11) give that the components of T((cid:126)y1) satisfy

Tjq((cid:126)y1) ≈ δjq cos2 θe
j ,

 (cid:101)R(cid:88)

j,q=1

I((cid:126)y) =

(cid:2)Tjq((cid:126)y) − δjq cos2 θe

γ2
j

j, q = 1, . . . ,(cid:101)R.
−1/2

(cid:3)2

j

(5.13)

,

(5.14)

Step 4. Calculate the imaging function

where γj are some positive weighting coeﬃcients. We use them to emphasize the
contributions of the terms for large singular values, and give less weight to those for
singular values close to the estimated noise level σe. In the numerical simulations

(cid:26)

(cid:27)

3(σe

j − σe)
σe

γj = min

1,

.

(5.15)

The estimator of (cid:126)y1 is the argument of the maximum of I((cid:126)y).

12

This algorithm may be used for localizing multiple inclusions that are suﬃciently
far apart, so that the column spaces of H((cid:126)yj) and H((cid:126)yq), for j (cid:54)= q, are approximately
orthogonal. To understand what suﬃciently far means, we can analyze the decay of
the inner products of the columns of G((cid:126)yj) and G((cid:126)yq) with the distance |(cid:126)yq − (cid:126)yj|.
This is similar to studying the spatial support of

F((cid:126)yj, (cid:126)yq) =

eik(|(cid:126)xr−(cid:126)yq|−|(cid:126)xr−(cid:126)yj|)

(4π)2|(cid:126)xr − (cid:126)yj||(cid:126)xr − (cid:126)yq| ,

(5.16)

N(cid:88)

r=1

the point spread function of the reverse time (Kirchhoﬀ) migration method for sonar
imaging. The known resolution limits of this method give that the inclusions are well
separated when

|yq − yj| (cid:29) λL
a

,

|yq,3 − yj,3| (cid:29) λL2
a2 .

(5.17)

Here we used the notation (cid:126)yq = (yq, yq,3) and similar for (cid:126)yj.

5.2.2. Localization of multiple inclusions. Imaging of nearby inclusions is
more diﬃcult because they interact, and multiple inclusions may be associated with
one singular value and vector. To address this case we modify Steps 3-4 of the imaging
method using the results (4.9)–(4.10), which imply that

hq((cid:126)y) =

†
l hq((cid:126)y)]ul

[u

iﬀ (cid:126)y ∈ {(cid:126)y1, . . . , (cid:126)yP}.

(5.18)

Here hq((cid:126)y) are the left singular vectors of G((cid:126)y), the columns of H((cid:126)y) for q = 1, 2, 3,
and the right hand side is their projection on the span of {u1, . . . , uR}, the range
of the noiseless data matrix. It suﬃces to work with q = 1, which gives the largest
contribution to the data model in equations (4.4)–(4.5). The analysis in section 4.2
shows that the third singular value of G((cid:126)y) is very small in the small aperture regime,
and for incomplete measurements along (cid:126)e1 or (cid:126)e2 the matrix GS ((cid:126)y) has only one large
singular value. Thus, the terms with h2 and h3 may give small contributions to D or
DS and consequently, h2 and h3 may have small projections on the subspace spanned

by {uj}1≤j≤(cid:101)R when j ≤ (cid:101)R < R. This is why we use h1 in the localization.
singular vectors. However, we can calculate their projection on the span{(cid:101)ul} for
l = 1, . . . ,(cid:101)R, which satisﬁes by (5.11) and (5.18)

We cannot work with (5.18) directly, because we do not know the unperturbed

(cid:101)ul[(cid:101)u

l h1((cid:126)y)] =(cid:101)ul

†

qh1((cid:126)y)] ≈(cid:101)ul((cid:101)u

†
l uq)[u†

†
†
l h1((cid:126)y)],
l ul)[u

(5.19)

for (cid:126)y ∈ {(cid:126)y1, . . . , (cid:126)yP}. Taking the Euclidian norm in (5.19) and using (5.10) we obtain

l = 1, . . . ,(cid:101)R,

†
l h1((cid:126)y)|
| cos θl|

,

(5.20)

and the norm of (5.18) gives

R(cid:88)

l=1

R(cid:88)

q=1

((cid:101)u
l h1((cid:126)y)| ≈ |(cid:101)u
R(cid:88)

†
|u

1 = (cid:107)h1((cid:126)y)(cid:107)2 =

|u

†
l h1((cid:126)y)|2,

(cid:126)y ∈ {(cid:126)y1, . . . , (cid:126)yP}.

(5.21)

l=1

13

We have only the ﬁrst (cid:101)R terms in the right hand side of (5.21), and we use them to

obtain the imaging function

1 −

(cid:101)R(cid:88)

|(cid:101)u

I((cid:126)y) =

−1/2

†
jh1((cid:126)y)|2
cos2 θe
j

We expect from (5.18) that when (cid:101)R = R, the imaging function (5.22) peaks at
In general (cid:101)R < R,
and (5.22) peaks at the location of the stronger inclusions, which contribute to the (cid:101)R

(cid:126)y = (cid:126)yj, for j = 1, . . . , P . This happens only for weak noise.

j=1

.

distinguishable singular values.

5.3. Estimation of the reﬂectivity tensor. If we knew the locations {(cid:126)yp}1≤p≤P

of the inclusions, we could obtain from (3.11) and (4.4) that

(5.22)

H((cid:126)yp)†(cid:101)D H((cid:126)yp) = Rp + ∆Rp ,

p = 1, . . . , P.

(5.23)

This is for complete measurements, and the error

∆Rp = H((cid:126)yp)†WH((cid:126)yp) +

[H((cid:126)yp)†H((cid:126)yl)]Rl [H((cid:126)yp)†H((cid:126)yl)]T

(5.24)

P(cid:88)

l(cid:54)=p,l=1

p = 1, . . . , P.

(5.25)

is due to the noise and interaction of the inclusions. The interaction is small when
the inclusions are well separated.

We only have estimates (cid:126)ye

p of (cid:126)yp, the peaks of the imaging function deﬁned in the

previous section, so we calculate instead

(cid:101)Rp = H((cid:126)ye

p),

p)†(cid:101)D H((cid:126)ye
(cid:101)Rp = Rp + (cid:101)∆Rp ,
p)(cid:101)RpΓ((cid:126)ye

These are modeled by

where (cid:101)∆Rp includes (5.24) and additional terms due to location errors (cid:126)ye

(5.26)
p − (cid:126)yp. The

estimate of ρp is motivated by deﬁnition (4.5) of Rp,
p)T ,

ρe
p = Γ((cid:126)ye

(5.27)

where

(5.28)
Recall from (4.3) that Σ((cid:126)y) is the 3 × 3 matrix of singular values of G((cid:126)y) and V((cid:126)y)
the 3 × 3 matrix of its right singular vectors.

Γ((cid:126)y) = V((cid:126)y)Σ((cid:126)y)−1.

We can model (5.27) by

ρe

p = ρp + ∆ρp ,

(5.29)

with error ∆ρp that consists of two parts. The ﬁrst part depends on the noise, the
interaction of the inclusions and the error in their estimated locations

p)(cid:101)∆Rp Γ((cid:126)ye
The second part vanishes when the inclusion is correctly localized
p)T − Γ((cid:126)yp)RpΓ((cid:126)yp)T .

p)RpΓ((cid:126)ye

= Γ((cid:126)ye

= Γ((cid:126)ye

∆(1)
ρp

p)T .

∆(2)
ρp

(5.30)

(5.31)

14

5.3.1. Discussion. The numerical results in section 6 show that the inclusion lo-
calization in cross-range is excellent for the case of complete measurements. However,
the range localization deteriorates quickly with noise in the small aperture regime.
This can be understood from the analysis in section 4.2, equation (4.16) in particular,
which shows that Q((cid:126)y) = G((cid:126)y)†G((cid:126)y) has two leading orthonormal eigenvectors vq
which are approximately in span{(cid:126)e1, (cid:126)e2}, for q = 1, 2. They correspond to eigenval-
ues that are approximately N/(4πL2). The third eigenvector is v3 ≈ (cid:126)e3, for a much
smaller eigenvalue by a factor of the order of a2/L2. These eigenvalues are the squares
of the entries in Σ((cid:126)y), and the eigenvectors are the columns of V((cid:126)y).

We also see from (4.1) and (4.15) that the left singular vectors of G((cid:126)y) are

with phases

hq((cid:126)y) ≈ N−1/2

k|(cid:126)xr − (cid:126)y| = kL

 ,

 eik|(cid:126)x1−(cid:126)y|vq
(cid:18)

eik|(cid:126)xN−(cid:126)y|vq

y3 − L

...

1 +

+

L

q = 1, 2,

(cid:19)

+ O

|xr − y|2

2L2

a (cid:28) L,

(cid:18) a3

(cid:19)

λL

.

(5.32)

(5.33)

These vectors deﬁne the projection matrices hq((cid:126)y)hq((cid:126)y)† used in the localization
method, for q = 1, 2. The singular vector h3((cid:126)y) is unlikely to play a role when noise
is present. Moreover, the projection matrices are approximately independent of the
range component y3 of (cid:126)y = (y, y3), due to cancellation in the diﬀerence of the phase
(5.33). Thus, range estimation becomes impossible in the small aperture regime.

Both terms in the error ∆ρp in the estimation of the reﬂectivity tensor (5.27)
have large components when the diagonal matrix Σ = diag(Σ1, Σ2, Σ3) has a small
entry. This is because we take its inverse in (5.28). Since in the small aperture regime

and the third column of V((cid:126)y) is approximately (cid:126)e3, we expect(cid:0)∆ρp

Σ3((cid:126)y) (cid:28) Σ1((cid:126)y) ≈ Σ2((cid:126)y),

ij to be large for i
or j equal to 3. The other components of the error should be of the order of the noise
and inclusion interaction. This is indeed demonstrated by the numerical simulations
in section 6. The condition number of Σ((cid:126)y) improves for larger ratios a/L, and so do
the estimates of ρp.

(cid:1)

5.3.2. Incomplete measurements. The estimation of the reﬂectivity tensors
from incomplete measurements can be done formally as above, with H((cid:126)y) the matrix
of left singular vectors of GS ((cid:126)y) and Σ((cid:126)y) the matrix of its singular values. The
matrix Σ((cid:126)y) has at most one large entry in this case, so the eﬀective rank (cid:101)R in the
results are expected to be much worse when only one component of the electric ﬁeld
is measured, that is for S = ((cid:126)eq) and some q ∈ {1, 2, 3}. As shown in section 4.2,

presence of noise is smaller than in the complete measurement case. This translates
into worse localization of the inclusions. Moreover, more components of the errors
∆ρp are large, due to the inversion of Σ((cid:126)y) in (5.28).

6. Numerical results. In this section we present numerical results that assess
the performance of the imaging method described in section 5. We begin with the
setup of the simulations in section 6.1. Then we compare in section 6.2 the asymptotic
limits stated in Theorem 4.1 to the empirical statistics of the singular values and
vectors obtained with Monte Carlo simulations. The inversion results are in section
6.3 for one inclusion and in section 6.4 for multiple inclusions.

15

6.1. Description of the numerical simulations. We present results for two
scattering regimes. The ﬁrst is called a large aperture regime, because the array
aperture is as large as the range of the inclusions L = a = 10λ. The second is
a small aperture regime with L = 100λ (cid:29) a = 10λ. The sensors are placed on
a regular square grid in the array aperture, with spacing λ/2, so N = 441. The
three-dimensional search domain is sampled in steps of λ/2.

We model the scattered ﬁeld using equation (3.2), for inclusions with reﬂectivity
tensor (3.3). Since the equation is linear, we factor out the small scaling factor α3.
The inclusions are ellipsoids with scaled semiaxes ap,j, for p = 1, . . . , P and j = 1, 2, 3.
Their scaled volume is

3(cid:89)

q=1

|Ωp| =

4π
3

ap,q.

In a system of coordinates with axes of the ellipsoid, the polarization tensor in (3.3)
is diagonal. We use the system of coordinates centered at the array, with basis
{(cid:126)e1, (cid:126)e2, (cid:126)e3}, which is a rotation of that of the ellipsoids by some matrix Rp. The
polarization tensors are [5]

Mp = |Ωp| Rp diag

1

1 + (p/o − 1)Dp,q

, q = 1, 2, 3

RT
p ,

(6.1)

where Dp,q are the depolarization factors of the ellipsoids [23, Section 3.3], given by
the elliptic integrals

(cid:19)

(cid:18)

(cid:90) ∞

For example, in the case of one inclusion at (cid:126)y1 = (1,−1, L), we take a1,q = q, for
q = 1, 2, 3, the contrast 1/o = 10, and some rotation R1 to obtain the reﬂectivity
tensor

Dp,q =

|Ωp|
2

p,q)(cid:2)(cid:81)3

0

(s + a2

ds

l=1(s + ap,l)2(cid:3)1/2
 .

 55.4

α−3ρ1 =

−7.28
−13.43 −22.64

−7.28 −13.43
70.82 −22.64
70.75

.

(6.2)

(6.3)

The noise matrix W is generated with the MATLAB command randn, as in

W =

σ√
2M

(randn(M ) + i randn(M )) ,

(6.4)

where M = 3N for complete measurements and |S |N for incomplete ones. The noise
level σ is chosen as a percentage of the largest singular vale σ1 of the noiseless data
matrix D. Thus, when we say 50% noise, we mean that σ = 0.5σ1.

M . Therefore, the measured (cid:101)D = D + W has the signal-to-

Remark: The typical amplitude of the entries of the unperturbed matrix D is
√
σ1/M , while the noise entries in W (with the Hadamard acquisition scheme) have
standard deviation σ/
M ). In the simulations σ/σ1 varies between 10% and 75%.
noise ratio SNR = σ1/(σ
This is very strong noise. For example, when σ/σ1 = 50%, the SNR is 2/
M = 0.05
in the complete measurement case, where M = 3N = 1323. This small SNR means
that the signal is very weak and buried in noise.

√

√

16

Fig. 6.1. Plots of mean[(cid:101)σj ]/σ vs. σj /σ, for j = 1, 2, 3. The empirical estimates of the means

are shown with solid blue lines and the asymptotic estimates (6.5) are shown with dotted red lines.
The top row is for the large aperture regime and the bottom row for the small aperture regime.

6.2. Statistics of the singular values and singular vectors. We compare
here the asymptotic behavior of the singular values and singular vectors stated in
Theorem 4.1 to the empirical estimates of their statistics obtained with Monte Carlo
simulations. We take a single inclusion with reﬂectivity tensor (6.3) and one thousand
samples of the noise matrix (6.4).

6.2.1. Complete measurements. Because there is a single inclusion, the data

matrix B = D has rank three. In the large aperture regime its singular values are

σ1 = 1.703, σ2 = 1.126, σ3 = 0.183,

and in the small aperture regime they are

σ1 = 0.021, σ2 = 0.015, σ3 = 3.01 · 10−5.

The diﬀerence in the magnitudes of σ1 and σ2 in the two regimes is due to the
geometrical spreading factor of order 1/(4πL)2. As expected from the discussion in
section 4.2, we have σ3 (cid:28) σ2 (cid:46) σ1 in the small aperture regime.

values(cid:101)σj of (cid:101)D, for j = 1, 2, 3 and σ ∈ (0, 2σ1). The abscissa is σj/σ and the ordinate
is mean[(cid:101)σj]/σ. The results are in excellent agreement with the asymptotic formulas

We plot with solid blue lines in Figure 6.1 the empirical means of the singular

if σj > σ,
if σj < σ,

(6.5)

(cid:0)1 + σ2/σ2

(cid:1)

j

(cid:101)σj ≈

(cid:26) σj

2σ

plotted in the ﬁgure with dotted red lines. The top row in Figure 6.1 is for the large
aperture regime and the bottom row for the small aperture regime. The plots are
similar, except that in the small aperture case σ3 cannot be distinguished from noise.

In Figure 6.2 we plot the standard deviations of(cid:101)σj and note that they are much
smaller than the means, so(cid:101)σj are approximately deterministic. We show the plots of

17

Fig. 6.2. Plots of std[(cid:101)σj ]/σ vs. σj /σ, for j = 1, 2, 3 in the large aperture regime. The empirical

estimates are shown with solid blue lines and the asymptotic estimates (6.6) with dotted red lines.

(cid:40)

std((cid:101)σj) ≈

(6N )1/2 (1 − σ2/σ2

σ

j )1/2

√

σ

(6N )2/3

0.813

if σj > σ,
if σj < σ.

(6.6)

Fig. 6.3. Plots of mean[cos2 θ1] (left) and std[cos2 θ1]/mean[cos2 θ1] (right) vs. σ1/σ, in the
large aperture regime. The empirical estimates are shown with solid blue lines and the asymptotic
estimates (6.6) are plotted with dotted red lines.

std((cid:101)σj) in the large aperture regime, and compare them with the asymptotic ones in

Theorem 4.1,

Here we used that std(Z2) ≈ √
In Figure 6.3 we display the empirical mean of cos2 θ1 = |(cid:101)u1u1|2 and compare it

0.813. The results at small aperture are similar.

with its asymptotic prediction (4.22). We also plot the standard deviation, and note
that it is much smaller than the mean when σ1 > σ, as expected from Theorem 4.1.
The plots look the same for the large and small aperture regimes, and for the other
angles θ2 and θ3, when σ2 and σ3 are greater than σ.

6.2.2. Incomplete measurements. We do not include plots for the incomplete
measurements, where B = DS , because they do not add much information. Instead,
we display below the eﬀect of the sensing matrix S on the singular values of DS . As
explained in section 5, we expect that the inversion algorithm will be most eﬀective

when the eﬀective rank (cid:101)R equals the rank R = 3 of DS . Since (cid:101)R is the number of
When S = ((cid:126)e1), the singular values of (cid:101)DS are

singular values of DS that are larger than 2σ, the more small singular values DS has,
the worse the inversion results.

σ1 = 1.099, σ2 = 0.087, σ3 = 0.007

in the large aperture regime and

σ1 = 0.015, σ2 = 1.7 · 10−5, σ3 = 1.6 · 10−8

18

Fig. 6.4. Empirical mean of the noise level estimate (5.2) (left plot) and standard deviation

(right plot). The results are normalized by σ. The abscissa is σ1/σ.

in the small aperture regime. The results are similar for S = ((cid:126)e2), and show that as
predicted by the analysis in section 4.2, only one singular value is large. The case
with S = ((cid:126)e3) is much worse, because all the singular values are small, and are likely
to be dominated by noise. Explicitly, we get

σ1 = 0.145, σ2 = 0.065, σ3 = 0.023

in large aperture regime and

σ1 = 2.3 · 10−5, σ2 = 1.3 · 10−5, σ3 = 1.9 · 10−7

in the small aperture regime. Finally, when both cross-range components of the
electric ﬁeld are measured using S = ((cid:126)e1, (cid:126)e2), the singular values of DS are similar
to those of D. Thus, we expect inversion results which are comparable to those for
complete measurements.

6.2.3. Estimation of the noise level. We display in Figure 6.4 the empirical
mean and standard deviation of the unbiased estimate (5.2) of the noise level, using

the complete measurement matrix (cid:101)D. We note that σe ≈ mean[σe] ≈ σ. The estimate

(5.3) is very similar, because we have many sensors. For the same reason we obtain
similar estimates of σ from incomplete measurements. If the number of sensors were
smaller, then the reﬁned estimation methods proposed in [17] could be used.

6.3. Inversion results for one inclusion. We present results for the inclusion
at (cid:126)y1 = (λ,−λ, L), with reﬂectivity tensor (6.3), in both the large and small aperture
regimes. We compare the MUSIC method (5.8) with the imaging function (5.14), and
estimate the reﬂectivity tensor as in section 5.3.

6.3.1. Complete measurements. Let us begin with Figure 6.5, which shows
the surface plots of IM U SIC ((cid:126)y) in the planes y3 = L and y2 = −λ, respectively. These
are obtained in the large aperture regime, for two realizations of the noise matrix:
one for 25% noise (bottom row) and the other for 75% noise (top row). At the higher
noise level the MUSIC function fails to localize the inclusion, as it has no peak. For
comparison, we display in Figure 6.6 the results obtained with our imaging function
(5.14), for the same realization at 75% noise. We do not show the images for 25%
noise because they are similar. The images are a signiﬁcant improvement over those
in Figure 6.5, and they clearly localize the inclusion in range and cross-range.

To illustrate the statistical stability of our imaging function (5.14), we display in
Figure 6.7 the histograms of the peak locations along the three coordinate axes, for
one hundred realizations of the noise at 25%, 50% and 75% noise level. The eﬀective

19

Fig. 6.5. MUSIC imaging of one inclusion in the large aperture regime and complete measure-
ments. We display IM U SIC ((cid:126)y) in the plane y3 = L (left) and y2 = −λ (right). The axes are in
units of the wavelength. The top row is for 75% noise and the bottom row for 25%.

Fig. 6.6. Imaging function (5.14) in the large aperture regime, for complete measurements and
75% noise. We display I((cid:126)y) in the plane y3 = L (left) and y2 = −λ (right). The axes are in units
of the wavelength.

rank of the noisy data matrix (cid:101)D is (cid:101)R = 1 at 75% noise level and (cid:101)R = 2 for the weaker

noise. We note that the focusing in cross-range is perfect for all realizations. The
range focusing suﬀers slightly at the higher noise levels, but the error is at most 1.5λ.
Imaging is more diﬃcult in the small aperture, as seen in Figure 6.8. Although the
cross-range localization remains robust up to 50% noise, and the error is bounded by λ,
which is an improvement over the resolution limit λL/a = 10λ of migration imaging,
the range localization fails even at 10% noise. This is because the eﬀective rank of

(cid:101)D is (cid:101)R = 2 for all the simulations in Figure 6.8, and as explained in section 5.3.1,

the projection matrices on the subspace spanned by the ﬁrst two singular vectors are
approximately independent of the range of the inclusion. To determine the range we
need an array of larger aperture, or two arrays that view the inclusion from diﬀerent
directions. Alternatively, we may probe the medium with broad-band pulses and
estimate the range from travel times of the scattered returns.

The histograms of the relative errors in the estimation of the reﬂectivity tensor

20

Fig. 6.7. The histograms of the peak location in y1 (top), y2 (middle) and y3 (bottom) of the
imaging function (5.14) in the large aperture regime. The left column is for σ/σ1 = 75%, the middle
column for 50% and the right column for 25%. The abscissa is in units of the wavelength.

are displayed in Figures 6.9, for the large aperture regime and at 25% and 50% noise
levels. The estimates are obtained using equations (5.25) and (5.27), and the peak
location of the imaging function (5.14). The plots in the left column are for ρ11, in
the middle column for ρ33, and in the right column for ρ13. The results for ρ22 and
ρ12 are similar to those for ρ11, and the results for ρ23 are similar to those for ρ13.
We note that the errors are below 10% at 25% noise and naturally, they increase with
the noise level. As expected, the errors are larger for ρ33 and ρ13, but not by a big
factor. This is because the condition number of the search matrix G((cid:126)y), deﬁned as
the ratio of its largest and smallest singular values, equals 2.6 in the large aperture
regime. The estimates are worse in the small aperture regime, where the condition
number of G((cid:126)y) is 23.3, as shown in Figure 6.10. Here we also have errors due to the
poor estimates of the range component of the inclusion location.

6.3.2. Incomplete measurements. If the sensors measure a single component
of the scattered electric ﬁeld, say along (cid:126)eq, the best choice is for q = 1 or 2. The
inversion results are very poor when q = 3, even at low levels of noise, as expected from
the discussion in section 4.2. We show here results for the sensing matrix S = ((cid:126)e1),
in the large aperture regime, and compare them with those in Figures 6.7 and 6.9.
We do not show results in the small aperture regime because they do not add more
information, and they are, as expected, worse than in the large aperture regime.

The histogram plots in Figure 6.11 show that the inclusion localization is worse
than in the complete measurement case (Figure 6.7), but not dramatically so. How-
ever, the errors in the estimation of the reﬂectivity tensor are much larger, even at
the 25% noise level. Compare the results in Figure 6.12 to those on the top row of
Figure 6.9. This demonstrates the beneﬁt of measuring more than one component of

21

Fig. 6.8. The histograms of the peak location in y1 (top), y2 (middle) and y3 (bottom) of the
imaging function (5.14) in the small aperture regime. The left column is for σ/σ1 = 50%, the middle
column for 25% and the right column for 10%. The abscissa is in units of the wavelength.

Fig. 6.9. Histograms of the relative errors of the components ρ11 (left), ρ33 (middle) and ρ13
(right) of the reﬂectivity tensor ρ, for the large aperture regime and complete measurements at 25%
noise (top row) and 50% noise (bottom row). The abscissa is in percent.

the scattered electric ﬁeld.

We do not display inversion results from incomplete data with the sensing matrix
S = ((cid:126)e1, (cid:126)e2), because they are comparable to those in the complete measurements
case. This is expected from the discussion in section 4.2.

6.4. Inversion results for multiple inclusions. We present results for three
inclusions at (cid:126)y1 = (λ,−λ, L), (cid:126)y2 = (−10λ, 6λ, L) and (cid:126)y3 = (5λ, 4λ, L). They are

22

Fig. 6.10. Histograms of the relative errors of the components ρ11 (left), ρ33 (middle) and ρ13
(right) of the reﬂectivity tensor ρ, for the small aperture regime and complete measurements at 25%
noise. The abscissa is in percent.

Fig. 6.11. The histograms of the peak location in y1 (top), y2 (middle) and y3 (bottom) of the
imaging function (5.14), in the large aperture regime, for incomplete measuremenst with S = ((cid:126)e1).
The left column is for σ/σ1 = 75%, the middle column for 50% and the right column for 25%. The
abscissa is in units of the wavelength.

localized using the imaging function (5.22), and their reﬂectivities are estimated as in
(5.27). The ﬁrst inclusion has the reﬂectivity (6.3) and for the other two we have

(6.7)

(6.8)

and

α−3ρ2 =

α−3ρ3 =

−29.43
69.99
−12.04 −8.53

−8.53
111.60

 91.12 −29.43 −12.04
 99.37

−26.89
5.35
101.12 −10.37
5.35
−26.89 −10.37
137.06

 .
 .

In the large aperture regime the matrix D has rank R = 9, and singular values

1.76 = σ1 > σ2 > . . . > σ5 = 1.14, 0.57 = σ6 > σ7 > σ8 = 0.18, σ9 = 0.07.

23

Fig. 6.12. Histograms of the relative errors of the components ρ11 (left), ρ33 (middle) and ρ13
(right) of the reﬂectivity tensor ρ, in the large aperture regime, for incomplete measuremenst with
S = ((cid:126)e1) and 25% noise. The abscissa is in percent.

Fig. 6.13. Image of three inclusions in the large aperture regime, for complete measurements
and 25% noise (left) and 75% noise (right). The display is in the plane y3 = L. The axes are in
units of the wavelength. The locations of the inclusions are indicated with red circles.

The rank of D is still nine in the small aperture case, but the matrix has three very
small singular values that are indistinguishable from noise at levels as small as 1%.
0.034 = σ1 > σ2 > . . . > σ6 = 0.012, 6.39 · 10−5 = σ7 > σ8 > σ9 = 2.12 · 10−5.

For brevity we present only results in the complete measurement case, and a few
realizations of the noise. The histograms of the estimated peak locations and of the
errors in the estimation of the reﬂectivities are similar to those in the previous section.

6.4.1. Large aperture regime. We display in Figure 6.13 the imaging function
(5.22) in the plane y3 = L. The image on the left is for data with 25% noise, where

the eﬀective rank of (cid:101)D is (cid:101)R = 6. The right image is for 75% noise, where the eﬀective
rank decreases to (cid:101)R = 3. The exact locations of the inclusions are indicated with red
peaks are more prominent when (cid:101)R is closer to R i.e., for the weaker noise. Note in

circles, and they coincide with the peaks of the imaging function. As expected, the

particular that the inclusion at (cid:126)y2 is barely visible at the 75% noise level, because
the singular vector h1((cid:126)y2) of G((cid:126)y2) has a small projection on span{u1, u2, u3}. The
images in Figure 6.14 tell a similar story. They are displayed in the planes y2 = yp,2
for p = 1, 2, 3, and show that the range resolution is worse than that in cross-range,
specially at the higher noise level. Moreover, consistent with Figure 6.13, the inclusion
at (cid:126)y2 is diﬃcult to localize at 75% noise.

We present in Table 6.4.1 the relative errors (in percent) of the estimation (5.27)
of the components of the reﬂectivities ρp, for p = 1, 2, 3. Because the inclusions are

24

Fig. 6.14. Image of three inclusions in the large aperture regime, for complete measurements
and 25% noise (top) and 75% noise (bottom). The displays are in the planes y2 = yp,2, for p = 1, 2, 3.
The axes are in units of the wavelength.

0% noise

25% noise

75% noise

Inclusion 1
Inclusion 2
Inclusion 3
Inclusion 1
Inclusion 2
Inclusion 3
Inclusion 1
Inclusion 2
Inclusion 3

ρ22
ρ11
0.02
0.01
0.3
0.2
0.02
0.03
0.5
0.6
5.1
3.7
1.2
2.9
1.8
3.3
5.2
7.4
0.7
2.1
Table 6.1

ρ33
0.1
0.3
0.04

1
2.4
7.7
5.7
8.9
7.4

ρ12
0.03
0.7
0.3
4.9
10.7
48.0
7.1
4.7
45.2

ρ13
0.06
1.9
0.1
3.8
19.3
21.7
31.3
34.7
18.1

ρ33
0.08
2.9
0.2
5.0
44.7
21.7
9.9
98.3
49.4

Table of relative errors (in percent) of the estimated components of the reﬂectivity tensors in

the large aperture regime.

well localized, the errors of ρp are of the form (5.24), and are mostly due to the noise.
The interaction between the inclusions is negligible, as seen from the very accurate
results at zero noise level. This is expected because the inclusions are further apart
than λL/a = λ. The errors increase with the noise, but they remain small for some
components, mainly on the diagonal of the reﬂectivity tensors.

6.4.2. Small aperture regime. As we saw in the previous section, imaging
is more diﬃcult in the small aperture regime. Range localization is impossible even
at small levels of noise. Moreover, the resolution of the left image in Figure 6.15,
obtained at 25% noise, is worse than that in Figure 6.13. The inclusions interact
in this case, because they are separated by smaller distances than λL/a = 10λ, and
this adds to the diﬃculty. The right image in Figure 6.15 shows that one inclusion is
obscured at 50% noise.

We do not show the errors in the estimation of the reﬂectivity tensor because they
are bad even at small noise levels, due to the poor conditioning of the matrix Σ((cid:126)y)
in (5.28), and the interaction of the inclusions.

25

Fig. 6.15. Image of three inclusions in the large aperture regime, for complete measurements
and 25% noise (left) and 50% noise (right). The display is in the plane y3 = L. The axes are in
units of the wavelength. The locations of the inclusions are indicated with red circles.

7. Summary. We introduced and analyzed a robust methodology for detection,
localization, and characterization of small electromagnetic inclusions from measure-
ments of the time-harmonic electric ﬁeld corrupted by additive noise. The methodol-
ogy is motivated by the statistical analysis of the noisy data matrix gathered by an
array of sensors. In particular, it uses random matrix theory results about low rank
perturbations of large random matrices. The detection of the inclusions is carried out
by the inspection of the top singular values of the data matrix. The localization is
done with an imaging function that uses the predictable angles between the singular
vectors of the noisy data matrix and of the noiseless matrix. The characterization of
the inclusions amounts to estimating their reﬂectivity tensor which depends on the
shape of their support and their electric permittivity.

The inversion methodology is robust with respect to a signiﬁcant level of additive
noise, much more than standard imaging methods like MUSIC can handle. We clarify
that MUSIC localization fails because the singular vectors of the noisy data matrix
are not collinear to those of the unperturbed matrix. A main result of the paper is
that the angles between these singular vectors can be predicted and used to obtain
a robust inclusion localization. We also explain that the resolution and stability of
inversion are dependent on each other, and quantify the stability gain due to larger
array aperture and more than one component of the electric ﬁeld being recorded and
processed.

Acknowledgements. Liliana Borcea’s work was partially supported by AFOSR

Grant FA9550-15-1-0118.

[1] R. Alonso, L. Borcea, G. Papanicolaou, and C. Tsogka, Detection and imaging in strongly

REFERENCES

backscattering randomly layered media, Inverse Problems, 27 (2011), p. 025004. 1

[2] H. Ammari, J. Garnier, and V. Jugnon, Detection, reconstruction, and characterization
algorithms in wave sensor imaging, Discrete and Continuous Dynamical Systems – Series
S, 8 (2015), pp. 389–417. 11

[3] H. Ammari, J. Garnier, H. Kang, W.-K. Park, and K. Sølna, Imaging schemes for perfectly

conducting cracks, SIAM Journal on Applied Mathematics, 71 (2011), pp. 68–91. 1

[4] H. Ammari, J. Garnier, and K. Sølna, A statistical approach to target detection and localiza-
tion in the presence of noise, Waves in Random and Complex Media, 22 (2012), pp. 40–65.

26

[5] H. Ammari, E. Iakovleva, D. Lesselier, and G. Perrusson, MUSIC-type electromagnetic
imaging of a collection of small three-dimensional inclusions, SIAM Journal on Scientiﬁc
Computing, 29 (2007), pp. 674–709. 1, 3, 7, 11, 16

[6] A. Aubry and A. Derode, Detection and imaging in a random medium: A matrix method
to overcome multiple scattering and aberration, Journal of Applied Physics, 106 (2009),
p. 044903. 1

[7] A. B. Baggeroer, W. Kuperman, P. N. Mikhalevsky, et al., An overview of matched ﬁeld
methods in ocean acoustics, IEEE Journal of Oceanic Engineering, 18 (1993), pp. 401–424.
1

[8] J. Baik, G. Ben Arous, and S. P´ech´e, Phase transition of the largest eigenvalue for nonnull
complex sample covariance matrices, Annals of Probability, 33 (2005), pp. 1643–1697. 1,
9, 10

[9] J. Baik, R. Buckingham, and J. DiFranco, Asymptotics of Tracy-Widom distributions and
the total integral of a Painlev´e II function, Communications in Mathematical Physics, 280
(2008), pp. 463–497. 10

1

[10] F. Benaych-Georges and R. R. Nadakuditi, The eigenvalues and eigenvectors of ﬁnite,
low rank perturbations of large random matrices, Advances in Mathematics, 227 (2011),
pp. 494–521. 1, 10

[11] B. Biondi, 3D seismic imaging, vol. 14, Society of Exploration Geophysicists, Tulsa, 2006. 1
[12] F. Cakoni, D. Colton, and P. Monk, The linear sampling method in inverse electromagnetic
scattering, vol. 80 of CBMS-NSF Regional Conference Series in Applied Mathematics,
SIAM, Philadelphia, 2011. 1

[13] M. Capitaine, C. Donati-Martin, D. F´eral, et al., Central limit theorems for eigenvalues
of deformations of Wigner matrices, Annales de l’Institut Henri Poincar´e, Probabilit´es et
Statistiques, 48 (2012), pp. 107–133. 1, 9

[14] M. Cheney and B. Borden, Fundamentals of radar imaging, vol. 79, SIAM, Philadelphia,

2009. 1

[15] A. J. Devaney, Super-resolution processing of multi-static data using time reversal and music,

tech. report, Northeastern University Report, 2000. 7

[16] A. C. Fannjiang, The MUSIC algorithm for sparse objects: a compressed sensing analysis,

Inverse Problems, 27 (2011), p. 035013. 1

[17] J. Garnier and K. Sølna, Applications of random matrix theory for sensor array imaging
with measurement noise, in Random Matrix Theory, Interacting Particle Systems, and
Integrable Systems, Cambridge University Press, Cambridge, 2014, pp. 223–246. 1, 10, 11,
19

[18] J. Hadamard, R´esolution d’une question relative aux d´eterminants, Bull. Sci. Math, 17 (1893),

pp. 240–246. 5

[19] I. M. Johnstone, On the distribution of the largest eigenvalue in principal components anal-

ysis, Annals of Statistics, 29 (2001), pp. 295–327. 9

[20] A. Kirsch and N. Grinberg, The factorization method for inverse problems, Oxford Univer-

sity Press, Oxford, 2008. 1

[21] V. A. Marchenko and L. A. Pastur, Distribution of eigenvalues for some sets of random

matrices, Matematicheskii Sbornik, 114 (1967), pp. 507–536. 1, 9

[22] R. O. Schmidt, Multiple emitter location and signal parameter estimation, IEEE Transactions

on Antennas and Propagation, 34 (1986), pp. 276–280. 1

[23] A. Sihvola and I. Lindell, Polarizability modeling of heterogeneous media, Progress in Elec-
tromagnetics Research (PIER 6), Dielectric Properties of Heterogeneous Materials, Priou,
A.(ed.), Elsevier, Amsterdam, (1992). http://ww.jpier.org/PIER/pier06/03.900105.pdf. 16

27

