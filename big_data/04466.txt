Sequential Voting Promotes Collective Discovery

in Social Recommendation Systems

L. Elisa Celis†, Peter M. Kraﬀt‡, Nathan Kobe†
† ´Ecole Polytechnique F´ed´ereal de Lausanne, Switzerland

‡Massachusetts Institute of Technology, USA

elisa.celis@epﬂ.ch, pkraﬀt@mit.edu, nathan.kobe@epﬂ.ch

6
1
0
2

 
r
a

 

M
4
1

 
 
]
I
S
.
s
c
[
 
 

1
v
6
6
4
4
0

.

3
0
6
1
:
v
i
X
r
a

Abstract

One goal of online social recommendation systems is to har-
ness the wisdom of crowds in order to identify high qual-
ity content. Yet the sequential voting mechanisms that are
commonly used by these systems are at odds with existing
theoretical and empirical literature on optimal aggregation.
This literature suggests that sequential voting will promote
herding—the tendency for individuals to copy the decisions
of others around them—and hence lead to suboptimal con-
tent recommendation. Is there a problem with our practice,
or a problem with our theory? Previous attempts at answer-
ing this question have been limited by a lack of objective
measurements of content quality. Quality is typically deﬁned
endogenously as the popularity of content in absence of so-
cial inﬂuence. The ﬂaw of this metric is its presupposition
that the preferences of the crowd are aligned with underlying
quality. Domains in which content quality can be deﬁned ex-
ogenously and measured objectively are thus needed in order
to better assess the design choices of social recommendation
systems. In this work, we look to the domain of education,
where content quality can be measured via how well students
are able to learn from the material presented to them. Through
a behavioral experiment involving a simulated massive open
online course (MOOC) run on Amazon Mechanical Turk, we
show that sequential voting systems can surface better con-
tent than systems that elicit independent votes.

Introduction

Many social media sites, news aggregators, Q&A websites,
massive open online courses (MOOCs), and other online
platforms such as Amazon, Yelp and YouTube rely on so-
cial recommendation in order to aggregate the opinions of
their many users. Content is then sorted or ranked using this
information. Recommendations are solicited by, e.g., allow-
ing users to like, rate, or, when presented with items in a
ranked order, upvote content, and aggregate opinions are of-
ten displayed publicly. Such sequential voting systems, in
which individual or aggregated previous votes are publicly
displayed and continuously updated, are now widespread on
the internet.

Yet literature on collective intelligence and the wisdom
of crowds suggests that these sequential voting systems

Copyright © 2016, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.

may be suboptimal at surfacing high quality content. Se-
quential voting produces an avenue through which previ-
ous decisions can inﬂuence future decisions. Social inﬂu-
ence through this avenue has been shown to exist across
a diversity of online systems, including social recommen-
dation systems (Salganik, Dodds, and Watts 2006; Much-
nik, Aral, and Taylor 2013; van de Rijt et al. 2014). How-
ever, the classic Condorcet jury theorem and related results
suggest that independent votes will yield better aggregation
than correlated ones (Ladha 1995). Further theoretical work
on “herding” and “information cascades” also suggests that
sequential votes can lead to group convergence on subopti-
mal outcomes (Bikhchandani, Hirshleifer, and Welch 1992;
Chamley 2004). Empirical work on aggregation for point es-
timation tasks has also lent support to these theoretical re-
sults (Lorenz et al. 2011). In summary, online ratings can ul-
timately be very diﬀerent when votes are sequential instead
of independent, and the prevailing hypothesis is that these
diﬀerences may be detrimental in the sequential setting.

However, recent work has begun to question the universal-
ity of this prediction. Researchers have investigated whether
social inﬂuence actually leads to miscalibrated rankings
(Stoddard 2015) as well as whether social inﬂuence can
sometimes help improve rankings (Lerman and Hogg 2014;
Abeliuk et al. 2015). The results thus far support the idea
that sequential votes may not do much harm to collective
outcomes in social recommendation systems and leave open
the possibility that mechanisms that allow social inﬂuence
may in fact accelerate the discovery of good content and al-
low for eﬀective rankings.

In order to assess the outcomes of sequential voting sys-
tems, this line of work makes one simple but potentially
problematic assumption. They assume that content quality is
equivalent to popularity as measured by independent private
votes. However, there is no reason a priori to privilege pop-
ularity in the independent context over the sequential one,
and doing so precludes the possibility that social interaction
might ultimately surface qualitatively better content. Do-
mains in which content quality can be deﬁned exogenously
and measured objectively are thus needed in order to better
assess the design choices of social recommendation systems.

Our Contribution
To address this problem, we select a domain in which we
have an objective measure of underlying quality: the edu-
cation domain. Here, quality is measured by how well stu-
dents are able to learn from the material presented to them.
We measure learning in two ways: by testing students on
material related to the content they consume, and by ask-
ing students to self-report their skill level before and after
consuming content.1 This gave us two exogenous metrics of
quality, which allow us to objectively compare diﬀerent con-
tent ranking methods.

We then designed an experiment to test the eﬀect of
sequential voting on the quality of surfaced content. We
compare against independent voting, in which participants
cannot view previous votes. We further benchmark against
content curated by experts and randomly selected content.
Based on the convergence of the web industry on the use
of sequential voting systems, we posit that sequential voting
can actually lead to better content ranking than independent
voting.

We ﬁnd that self-reported learning of content consumers
was signiﬁcantly higher when the content was chosen by
sequential votes compared to all other conditions, includ-
ing the expert condition. Consistent with this trend, average
test scores are also always higher in the sequential condition,
though not statistically so. Hence, sequential voting can al-
low for the discovery of high-quality content without expert
intervention, suggesting a beneﬁcial eﬀect of social inﬂu-
ence.

Approach

Our goal is to determine if sequential voting can select
better answers in a student forum than other types of cu-
ration methods, including independent votes and expert-
grading. The type of student forum we consider is common
in MOOCs and other online Q&A sites. It begins with a
single question (often posted by a user) followed by a se-
quence of answers posted by peers. These answers are then
voted on by other students. Rather than using an existing live
MOOC, we take an approach that has been used recently in
the education literature to better isolate aspects of learning,
e.g., (Coetzee et al. 2015) and use a simulated MOOC using
workers from Amazon Mechanical Turk (AMT) as students.
In this manner we simulated a MOOC forum in which con-
tent was generated, ranked and consumed by AMT workers
as participants.

Our goal was to quantitatively measure each participant’s
skill after being exposed to ﬁltered peer-generated content.
This was measured by 1) an exam on the material and 2)
by two survey questions answered at the end of the entire
course that asked participants to self-report their knowledge
before and after course completion. Each course progressed
through three phases:
• Content generation – which simulates students answering

questions in a forum,

1Self-reported learning can be a proxy for motivation (Schunk

2003), and is correlated with actual learning (Anaya 1999).

• Vote collection – which simulates the discovery of con-
tent from the generation phase via either sequential or in-
dependent upvotes, and

• Testing – which allows us to measure students’ knowl-
edge after consuming content surfaced in the voting
phase.

See the Experiment section for more details.

Importantly, as is the case in many social recommenda-
tion systems, having a content generation phase allowed us
to present material generated by peer participants. We were
then able to study the eﬀect that sequential vs independent
voting in the collection phase had on the testing phase. Par-
ticipants were allowed to engage in at most one of the three
phases in each course in order to isolate the learning that
occurs within each phase.

We hypothesized that students shown content discovered
by sequential votes would outperform those shown con-
tent discovered by independent votes due to an underlying
herding eﬀect that allows for greater eﬃciency in identify-
ing good explanations.2 In addition, we compared against
benchmarks consisting of a random condition, in which ran-
dom answers were shown, and an expert condition, in which
the authors graded each explanation and explanations with
top grades were shown.

Experiment

We ran two simulated MOOC courses; one on introductory
programming, and another on introductory art styles. Each
course had ﬁve lessons: the Computer Science course taught
Assignment, If-Else Statements, For Loops, Functions and
Recursion, while the Art course taught the deﬁning fea-
tures of Photorealism, Realism, Expressionism, Impression-
ism, Abstract, Cubism, Color-Field, Hard-Edge, Nanga and
Mughal. The authors are experts in computer science, and
one of the authors is knowledgeable in art. The lessons and
questions for the Computer Science course were developed
jointly, and the lessons and questions for the Art course were
developed by the knowledgeable author. In order to validate
the authors’ decisions, two professors in Computer Science
and one professor in Art, each with more than 15 years ex-
perience teaching introductory courses in their respective ar-
eas, were consulted, as described in the External Validators
section.

Each of the two course began with a content phase in
which 10 participants answered a set of questions for each
lesson in the course and gave explanations for their answers.
This phase simulates populating the content of a forum. This
phase was followed by a voting phase that had 4 conditions
with 25 participants in each condition. In the voting phase,
participants voted on which explanations they thought were
good. One condition in this phase had independent votes and
three conditions had votes that were public to others present
in that condition. Finally, there was a testing phase that had 6

2Formal models for describing and predicting herd behavior
have emerged in many ﬁelds (see (Raafat, Chater, and Frith 2009)
for a summary); the exact mechanism through which herding or
social inﬂuence occurs is immaterial to our study, we simply allow
for it (or not) by showing (or hiding) votes.

Figure 1: The experimental design for both courses followed
the depiction above. Each panel (representing a phase)
was run as a separate batch of experiments. Within each
batch, participants were randomly assigned across condi-
tions within the phase. Participants were only allowed to par-
ticipate in one phase (and hence one condition) per course.
Arrows indicate dependencies between conditions across
phases. Blue conditions did not require participants.

conditions with 100 participants in each condition, with one
condition for each of the voting phase conditions in addition
to an expert graded condition and a random condition. Each
phase is described in more detail below; see Figure 1 for a
ﬂowchart depicting the diﬀerent phases and conditions.
Participants
Our experiment participants were recruited on Amazon Me-
chanical Turk (AMT), an online crowdsourcing platform
in which requesters can post discrete, typically short, tasks
with replicates called HITs. If an AMT worker ﬁnds and ac-
cepts a HIT, the worker gets paid upon successful comple-
tion. In our setting, each phase of each lesson corresponded
to a task, and each task had many HITs falling into each of
our diﬀerent experimental conditions. We conducted our ex-
periments over a two-week period in May of 2015. All HITs
within a single phase of each lesson were posted simultane-
ously and completed in under 12 hours.

Workers within a course participated in at most one phase,
but were allowed to participate in both courses if they so de-
sired. We placed weak selection criteria on the AMT work-
ers, allowing workers with a minimum of 50 HITs com-
pleted and at least 90% of completed HITs approved (i.e.,
marked as correct by the job requester). The workers were
payed $3.5, $3.5 and $2 per HIT for the content phase, vot-
ing phase and testing phase respectively, leading to an eﬀec-
tive hourly wage of $6.49 as measured by AMT.3 In total,

3We expect the hourly wage to actually be higher due to known
factors such as skimming, in which workers accept and work on
multiple HITs at once (Gong 2012).

Figure 2: Depicts a sample lesson from the Art course. Each
lesson contained a brief description of two styles of art along
with a sample painting corresponding to each, as depicted.
The sample lesson for the Computer Science course con-
tained a brief description of the topic along with a sample
piece of pseudocode and its correct output.

1211 unique workers participated in the experiment.

Experiment Phases
Content Phase During the content phase, participants
were presented with brief 3-5 sentence lessons (as illustrated
in Figure 2). In each lesson, the participants were prompted
to read the lesson, answer a question on the material, and
provide an explanation as to why they chose their answers.
In the Art course participants were shown a painting and
asked to identify the art style of the painting given two
choices (as illustrated in Figure 3). In the Computer Science
course, the participants were asked to give the output of a
short piece of pseudocode (as illustrated in Figure 4).
Voting Phase
In the voting phase, participants were pre-
sented with the same brief lessons as in the content phase.
The ﬁrst part of the question was identical to the content
phase, but instead of being asked to provide an explanation,
participants were asked to upvote the explanations generated
in the content phase that they thought were good (as illus-
trated in Figure 5). The format was modeled after common
Q&A platforms and MOOC forums.

This phase had two types of conditions, one in which the
numbers of upvotes from previous participants were made
public and the other in which these numbers were kept hid-
den. Public upvotes allow for social inﬂuence as participants
can incorporate the information into their decisions to up-
vote. This type of condition forms our sequential condition.
We compared against a condition where the number of up-
votes were private (same as Figure 5, except the green boxes
were not displayed), which we call the independent condi-
tion.
Testing Phase During the testing phase, the goal was to
measure a participant’s ability to learn solely from explana-
tions generated in the content phase and chosen by the voting
phase. No lesson was presented and vote information was

Figure 4: Depicts a sample question from the content phase
of the Computer Science course (for the lesson on For
Loops). The participant is shown a short piece of pseu-
docode, asked to type the output of the code, and asked to
give an explanation for their answer.

sisted of a short piece of pseudocode and asked for its out-
put, again without any explanation. As all questions had an
objective right answer, each question was worth 0 or 1 point,
depending on whether the answer was incorrect or correct.
Self-Reported Learning
Participants in all conditions were presented with the same
optional survey at the end of the course, which included a
self-assessment of learning. Participants reported their skill
levels on the topic (Computer Science or Art) before and af-
ter the course on a scale from 0 (none) to 4 (expert). We mea-
sure self-reported learning by taking the diﬀerence between
the before and after self-reported skill levels. While individ-
ual estimates of learning may be inﬂated overall, diﬀerences
across conditions must be real since participants were ran-
domly assigned to conditions.
External Validators
We further had external validators in art and computer sci-
ence grade the student explanations in order to indepen-
dently measure the quality of the content curated by the dif-
ferent conditions. This additional step was conducted due
to the unexpected ﬁnding (discussed below) that the expert-
curated condition did not outperform the other conditions,
and was sometimes signiﬁcantly worse. The external valida-
tion conﬁrmed that the expert condition had selected good
content as evaluated by external graders and lends weight to
the ﬁnding. Moreover, the topics and question format were
conﬁrmed by the external validators to be similar to what is
used in their courses.

The external validators were professors who had no previ-
ous knowledge of the experiment and no prior collaboration
with the authors. Each has more than 15 years experience
teaching introductory courses in their ﬁeld, and either has
developed or is in the process of developing a MOOC on
such material. The external validators were asked to grade

Figure 3: Depicts a sample question from the content phase
of the Art course (for the lesson on Photorealism and Real-
ism). The participant was shown a painting, asked whether
it represented a painting of type A or B, and asked to give an
explanation for their answer.

not shown. Instead, the participants were presented with the
questions asked in content and voting phases, along with two
explanations from the content phase (as illustrated in Figure
6). The two explanations the participants were shown varied
across conditions.

The participants were randomly assigned to one of four
diﬀerent types of conditions: sequential, independent, ran-
dom and expert. The sequential conditions displayed the
top two most upvoted explanations from the sequential con-
ditions in the voting phase. We had three parallel repli-
cates of the sequential condition. The independent condi-
tion displayed the top two most upvoted explanations from
the independent condition of the voting phase. The random
condition displayed two explanations selected uniformly at
random from all the explanations generated in the content
phase. In the expert condition, the authors independently
manually graded the content. The authors’ grades were av-
eraged into a ﬁnal grade for each explanation, and the two
explanations with the best average grades were presented.
Ties were broken at random so that exactly two explanations
were presented in every condition.

The tests given to participants consisted of four multiple-
choice questions per lesson in the Art course similar to the
one depicted in Figure 3 but without the prompt for an expla-
nation, and one numerical question per lesson in the Com-
puter Science course. The computer science question con-

Figure 6: Depicts a sample testing phase: The student ex-
planations, which were generated in the content phase and
selected in the voting phase, are displayed in lieu of a les-
son.

man 2009; Szabo and Huberman 2010; Cheng et al. 2014),
which also strives to quantify the eﬀect of social inﬂuence
on popularity (Salganik, Dodds, and Watts 2006; Krumme
et al. 2012; Stoddard 2015). Other work has examined the
impact of social inﬂuence on various online user behav-
iors more generally (Salganik and Watts 2008; Chen 2008;
Muchnik, Aral, and Taylor 2013; van de Rijt et al. 2014;
Wang, Wang, and Wang 2014). Our work contributes to
these areas by identifying the eﬀect that social inﬂuence has
on the discovery of high quality content in a domain where
quality can be objectively deﬁned.

There are many mechanisms used for social recommen-
dation systems. Variations, both on the allowable user in-
put and on the form of aggregation, exist. These variations
have been studied widely in theory and practice, and an
overview is outside the scope of this work (see, e.g., (Aska-
lidis and Stoddard 2013) and references cited within). We
select a speciﬁc form of of user input (upvotes on ranked
content) and the simplest and most common form of aggre-
gation (sorting by number of upvotes) since these choices
are prevalent in the education context. Most popular MOOC
forums, including Coursera, MIT-X, Harvard-X, and Stan-
ford Online, as well as Q&A websites such as Yahoo! An-
swers, Stack Overﬂow, Baidu Knows and Quora use varia-
tions of this type of sequential voting mechanism.

Analysis

We now present an overview of the participant responses fol-
lowed by the results of our experiments using both quality

Figure 5: Depicts a sample voting phase in the social condi-
tion. The independent condition does not display the green
boxes containing the previous upvotes. Only a portion of the
display is shown here. 10 explanations were presented at a
given time, sorted according to the number of upvotes re-
ceived thus far in the sequential condition and sorted ran-
domly in the independent condition.

on the “usefulness of the explanation to other students who
will have to answer similar questions”, and assigned a grade
between 0-10 to each explanation generated in the content
phase (100 explanations in Art, and 50 explanations in CS).

Related Work

There are a number of avenues of research in computa-
tional social science related to our study. Our work is re-
lated to the large literature on identifying content quality,
which primarily strives to develop automated techniques for
quality prediction in online settings. One way to identify
the quality of content is based on the contributor’s repu-
tation and past work (Chen et al. 2006; Dom and Paran-
jpe 2008). Speciﬁc content features can also be used, such
as the inclusion of references to external resources, the
length, the utility or the veriﬁability (Adamic et al. 2008;
Kelly et al. 2007; Kim and Oh 2009). Another growing
related area involves popularity prediction (Hogg and Ler-

Figure 7: Explanation grades given by external validators
normalized per-lesson to a 0-1 scale. Error bars are standard
errors of the mean. The plot shows that from the perspec-
tive of the external validators, explanations selected by the
sequential and independent conditions are indistinguishable,
while the explanations selected in the expert condition tend
to be better and the random condition tends to be worse; de-
spite this, we later see that the sequential condition tends
to outperform the rest with respect to self-reported learning
and demonstrated skill.

metrics, self-reported learning and test scores.4

Descriptive Statistics
The conditions for the testing phase were the sequential con-
ditions (3 replicates), the independent condition (1 repli-
cate), the random condition (1 replicate) and the expert-
curated condition (1 replicate). We used three runs of the
sequential condition because prior work suggests that so-
cial information may lead to diﬀerent outcomes across repli-
cates (Salganik, Dodds, and Watts 2006). These diﬀerent
outcomes occur because of herding amplifying the ﬁrst few
votes in each replicate.

100 HITs for each testing condition were posted on AMT;
i.e., 600 HITs for the Art course and 600 HITs for the Com-
puter Science course. The dataset was ﬁltered so that only
the results from workers who had completed all ﬁve lessons
were retained. In some cases we had more participants than
HITs, presumably from workers who clicked on our ad but
did not accept the HIT. The number of completed lessons per
condition ranged from 97 to 106 with an average of 100.92.

Explanation Quality
Our external validators independently graded all student ex-
planations on a 0-10 scale. The combined score is an expla-
nation’s raw grade. If we think of the external validators as
instructors in the course, this would be the grade they assign
to an explanation posted on the course forum.

The overall average explanation grade in the Art course
was 6.13, and the average best explanation grade (across
lessons) in this course was 8.45. The average explanation

Figure 8: Self-reported skill levels before and after task com-
pletion, versus test scores. The test scores are more corre-
lated with self-reported skill after a lesson, suggesting that
self-reported learning can be thought of as a proxy for ac-
tual learning. Error bars are standard errors of the means.
Regression lines are ﬁtted to the raw data.

grade in the Computer Science course was approximately
2.36, and the average best explanation grade in the Com-
puter Science course was 5.7.

We use these grades to evaluate the quality of the con-
tent selected by the diﬀerent conditions. We focus on the
average grades of the two explanations selected in each con-
dition of the voting phase. We normalize the grades so that,
for each lesson, they lie on a 0-1 scale by dividing by the
maximum grade in a given lesson.5 We then compared the
average grade of the top explanations selected by each con-
dition (see Figure 7).

In both the CS and the Art courses the explanation grades
in the expert condition are signiﬁcantly higher than the ex-
planation grades in the sequential condition (two-sided t-
test with p = 0.009976 for Computer Science and p =
0.0001374 for Art). We include this result to illustrate the
diﬀerences in explanation quality as judged by an exter-
nal validator. In particular, this shows that there is a range

4The code and data used for our analysis are available online:
https://github.com/pkraﬀt/Sequential-Voting-Promotes-Collective-
Discovery-in-Social-Recommendation-Systems.

5This is necessary as some lessons are naturally harder than
others. Without normalization the data across courses would not be
comparable.

0.000.250.500.751.00ArtCSCourseAverage Explanation GradesExperiment ConditionExpertSequentialIndependentRandomlllll0.000.250.500.751.0001234Self−Reported Level BeforeTest Scorelllll0.000.250.500.751.0001234Self−Reported Level AfterTest ScoreFigure 9: Comparison of self-reported learning scores. Error
bars are standard errors of the mean. The sequential condi-
tion produces a signiﬁcant increase in self-reported learning
compared to the other conditions.

Figure 10: Comparison of test scores. Error bars are stan-
dard errors of the mean. The sequential condition remains
on average the best in terms of performance in both courses,
though not always signiﬁcantly.

in explanation quality (especially in the Computer Science
course). This result also validates the authors’ grades as used
in the expert-curated condition. Though not statistically sig-
niﬁcantly, independent votes achieve identify higher graded
explanations than sequential votes in both courses, as would
be suggested by the literature on social inﬂuence. However,
our desired metric was the eﬀect on the student, i.e., the test
scores and self-reported learning of a content consumer, not
the grades given by experts.

Learning
In the testing phase we took explanations selected in the
voting phase and assessed how well participants learned
when consuming that material as instruction. We measure
the change in self-reported skill level from before and after
the lessons. In addition, we measure the participants’ knowl-
edge with a test on the subject material after the lesson was
completed.

The self-reported skill level before and after the course
was submitted on a 0-4 scale with 0 being “none” and 4
being “expert”. The test scores are normalized to be from
0 to 1. The self-reported skill level after task completion
has a much higher correlation with test scores than the
self-reported skill level from before the task (0.31186 and
0.11505 respectively; see Figure 8). Hence, self-reported
skill after the course is a better proxy for actual skill level
than self-reported skill level before the course.

We ﬁnd that self-reported learning in the sequential condi-
tion is signiﬁcantly higher than in the independent condition
(two-sided t-test p < 0.0005312), in the random condition
(two-sided t-test p = 0.002109), and in the expert condition
(two-sided t-test p = 0.04359). These diﬀerences remain
signiﬁcant controlling for the course type and the experi-
ment instance using mixed eﬀects models (with ﬁxed eﬀects
for the course type and random eﬀects for the particular ex-
periment instances). Breaking the analysis up by course, the
trends are the same in all comparisons (see Figure 9), how-
ever the comparison to the expert condition is not signiﬁcant
in the Computer Science course, and the comparison to the
random condition is not signiﬁcant in the Art course.

Consistent with these trends, average test scores were
higher in the sequential condition than in the independent,
random, and expert conditions, though these comparisons
were not statistically signiﬁcant (see Figure 10). The sequen-
tial condition is signiﬁcantly better than the expert condition
(two-sided t-test p = 0.02682) and random (two-sided t-test
p < 0.0001) in the Art course, though not the Computer Sci-
ence course. Those single comparisons are signiﬁcant. Us-
ing the mixed eﬀects model to look at both courses jointly,
the comparison to the expert condition, but not to random,
remains signiﬁcant. The eﬀect is weaker in one context (CS)
than the other (Art) because the sequential and independent
conditions were more similar in the CS test phase. Unlike
in the Art course, the CS voting phase resulted in similar
top explanations across sequential and independent condi-
tions, likely because the content-generation phase resulted
in fewer good explanations.

Herding
We hypothesized that the diﬀerences we observed in learn-
ing across conditions was due to a beneﬁcial herding eﬀect
which allows for greater eﬃciency in identifying good ex-
planations. To test this hypothesis, we examined the skew-
ness of the distribution of the ultimate number of votes each
explanation got per question. Following previous work (Gini
1912; Salganik, Dodds, and Watts 2006), we ﬁrst take the
number of votes each explanation got and divide those num-
bers by the total number of votes per question. These nor-
malized popularity values are the “market shares” of popu-
larity of each explanation. We then use the average diﬀer-
ence in market share between items as a measure of the
skewness of the distribution, or the “inequality” in popu-
larity. Higher inequality values in the sequential condition
compared to the independent condition indicates that herd-
ing may be occurring.

We ﬁnd in the Art course that average inequality across
questions is signiﬁcantly higher in the sequential condition
than the independent condition (two-sided t-test p < 1e-5),
suggesting a herding eﬀect may be at play. We also ﬁnd a
trend in the same direction in the Computer Science course,

0.000.250.500.751.001.25ArtCSCourseSelf−Reported LearningConditionExpertSequentialIndependentRandom0.000.250.500.75ArtCSCourseAverage Test ScoresConditionExpertSequentialIndependentRandomopposed to a true pretest-posttest) was used so that partic-
ipants would have the same context with which to address
their skill. Having a (quasi)-pretest-posttest design was fea-
sible for self-reported learning but not for actual test scores,
since taking a test requires more time than completing a sur-
vey. This is one of the main limitations of our work, and fur-
ther investigation is required in order to determine whether
the results on test scores are indeed signiﬁcant.

In our statistical tests, we used a p-value cutoﬀ of 0.05 for
declaring signiﬁcance. If we instead use a Bonferroni cor-
rection, then the p-value cutoﬀ would become 0.001. This
correction accounts for 50 tests, which is larger than the 30
tests in our ﬁnal script for analyzing the data and larger than
the 17 that we report. Using this substantially more conser-
vative p-value, we still ﬁnd that the sequential condition has
signiﬁcantly higher self-reported learning than the indepen-
dent condition, but notably, the comparisons to the expert
condition are no longer signiﬁcant.

Domain Diﬀerences. As is readily apparent in Figures 9
and 10, both the test scores and self-reported learning are
lower in the Computer Science course than in the Art course.
The disparity could be accounted for by the fact that the ex-
planations in the Computer Science course were much worse
than those in the Art course according to the grades assigned
by the external validators (see the section on Explanation
Quality); in other words, the content consumers had overall
lower-quality explanations to help them learn, which could
explain the lower learning scores overall.

Inadequacy of Experts.
In some cases, our evidence sug-
gests that the sequential condition is better than the expert
condition; in fact, in some cases even the random condition
was on average (although not signiﬁcantly) better than the
expert condition. A priori, this result is very surprising, and
initially we were concerned that one reason for it could be
that the expert grading was performed poorly. The external
validators were brought on at this point to grade the con-
tent in order to independently validate the experts’ decisions.
The fact that the external validation agreed with the author’s
decisions gives weight to the conclusion that sequential vot-
ing can in fact outperform experts; hence we do not believe
this is a limitation of our work, but rather a point for further
reﬂection.

One possible explanation for why expert-selected content
could be worse is the mechanism used for curation: The ex-
perts did not vote on which pairs of explanations would be
best. They graded each explanation individually, and the ex-
planations with the top two grades were selected for use in
the expert condition. It is possible that sequential voting al-
lows for better joint selection of explanations compared to a
traditional grading method.

Herding. We hypothesized that the diﬀerences in learning
between the sequential and independent conditions was due
to a beneﬁcial herding eﬀect that led to better ranked con-
tent. Higher inequality values, measured with the Gini index

Figure 11: Average inequality (Gini index) over the mar-
ket share of upvotes. Error bars are standard errors of the
mean. Higher inequality values in the sequential condition
compared to the independent condition suggest that herding
may be occurring.

but the diﬀerence is not statistically signiﬁcant (see Figure
11).

Discussion

Main Results. The results using our objective measure-
ments of quality paint a diﬀerent picture than the grades
of the external validators. Participants’ self-reported assess-
ments suggest that the sequential condition curates better
content than all other conditions. This ﬁnding is notable
since the sequential condition outperforms the independent
condition (which had equivalent grades from the external
validators), and since the sequential condition is not out-
performed by the expert condition (which had signiﬁcantly
higher grades from the external validators). The diﬀerences
in self-reported learning across conditions cannot be due to
participants having a tendency to tell us what we want to
hear (i.e., report learning when none occurred) because of
the randomization of participants to diﬀerent conditions.

Self-reported learning is an important metric since per-
ceived progress is strongly correlated to motivation (Schunk
2003). Moreover, self-reported learning can often be used
as a valid proxy for more direct measures of learn-
ing (Anaya 1999). Therefore, observing an improvement in
self-reported learning alone is enough reason to adopt the
sequential voting procedure.

Limitations. Participants’ actual test scores follow the
trend observed in the self-reported assessments, though not
always with statistical signiﬁcance. While we did not expect
to be under-powered, we believe that, due to the diﬀerences
in experimental design between the two measurements, we
were underpowered on test scores and not on self-reported
learning. Test scores used a standard between-subjects
design while self-reported learning used a quasi-pretest-
posttest between-subjects design (where the “pretest” and
“posttest” were both end-survey questions). The latter de-
sign should yield lower variance in the dependent variable,
and thus will have higher power. A quasi-pretest-posttest (as

0.00.10.20.30.4ArtCSCourseAverage Inequality(Gini 1912), in the sequential condition compared to inde-
pendent conditions indicate that herding may be occurring.
However our ﬁndings were not conclusive in the CS course.
The participants in the CS course only generated a few good
explanations, while the participants in the Art course gener-
ated many. This factor likely led to high “inequality” in the
distribution of votes for CS even in the independent condi-
tion, thereby diminishing the eﬀect of herding.

Conclusion & Future Work

Despite many models of collective intelligence clearly pre-
dicting deﬁcits in aggregate outcomes due to social inﬂu-
ence, our work suggests that sequential voting may indeed
improve ranking and collective discovery in online social
recommendation systems. However, the mechanism through
which sequential voting has this positive eﬀect remains un-
clear. While there is some evidence that herding may be
playing a role, for example, by emphasizing content that
might have been overlooked in the independent voting set-
ting, another possibility is that people are responding to oth-
ers with anticorrelated votes in an intelligent manner; be-
havior that has recently been observed empirically (Sipos,
Ghosh, and Joachims 2014). Our understanding of collec-
tive intelligence, even in this simple interaction context, is
clearly in a nascent stage; better models and further analy-
ses are required.

Future work that uses the education domain to achieve
objective measurements of underlying quality should also
address some of the limitations of our present work. For
example, we see teasing out the diﬀerences between self-
reported learning and test scores as an important topic for fu-
ture work. More comprehensive tests, delays between learn-
ing and testing, a true pre-test / post-test design, and larger
sample sizes of students could be employed in order to iden-
tify whether our null results on test scores are actually a
false negative. Interesting high-level questions in this vein
include: are participants implicitly distinguishing between
what they were taught as opposed to what they learned in
the self-assessment? How do test scores and self-reported
learning correlate, if at all, with reported conﬁdence? What
type of model (if any) of social inﬂuence is warranted by the
results we observed, and can we increase the eﬀects to pro-
duce better outcomes? And what other forms of information
can we make public in a way that improves learning?

Our work also leaves open the mechanism through which
information assimilation beneﬁts learning. Understanding
this mechanism would allow us to predict how student
knowledge varies as a function of the type and amount of
information revealed, and could lead to designing systems
which maximize beneﬁcial information, e.g., ranking con-
tent (as in (Lerman and Hogg 2014)) using learning as a
quality metric.

The diﬀerence we observed between the expert and se-
quential conditions also warrants further investigation. This
ﬁnding, if general, suggests that sequential voting might lead
not just to the crowd being able to discover better content
than it could through standard wisdom-of-crowds aggrega-
tion, but also better content than selected via expert grading.

Identifying the conditions under which crowds can outper-
form experts is an important open area of research in collec-
tive intelligence, and our work suggests that even a simple
interaction mechanism may at times achieve this goal.

Acknowledgments

We would like to thank the anonymous reviewers for their
insightful comments which have aided in the exposition of
this work. We would also like to thank our three external val-
idators: Prof. Nicolas Bock (University of Lausanne), Prof.
Jean-C´edric Chappelier (EPFL) and Prof. Djamila Sam-
Haroud (EPFL), and the anonymous Amazon Mechanical
Turk workers who participated in our study. The authors ob-
tained approval for the experiment from the ´Ecole Polytech-
nique F´ed´ereal de Lausanne (EPFL) Human Research Ethics
Committee (HREC) in Switzerland. This material is based
upon work supported by the National Science Foundation
Graduate Research Fellowship under Grant No. 1122374.
Any opinion, ﬁndings, and conclusions or recommendations
expressed in this material are those of the authors(s) and
do not necessarily reﬂect the views of the National Science
Foundation

References

[Abeliuk et al. 2015] Abeliuk, A.; Berbeglia, G.; Cebrian,
M.; and Van Hentenryck, P. 2015. The beneﬁts of social
inﬂuence in optimized cultural markets. PLoS ONE 10(4).
[Adamic et al. 2008] Adamic, L. A.; Zhang, J.; Bakshy, E.;
and Ackerman, M. S. 2008. Knowledge sharing and Yahoo
Answers: Everyone knows something. In Proceedings of the
International Conference on the World Wide Web (WWW),
1013–1022.
[Anaya 1999] Anaya, G. 1999. College impact on student
learning: Comparing the use of self-reported gains, stan-
dardized test scores, and college grades. Research in Higher
Education 40(5):499–526.
[Askalidis and Stoddard 2013] Askalidis, G., and Stoddard,
G. 2013. A theoretical analysis of crowdsourced content cu-
ration. In Proceedings of the 3rd Workshop on Social Com-
puting and User Generated Content.
[Bikhchandani, Hirshleifer, and Welch 1992] Bikhchandani,
S.; Hirshleifer, D.; and Welch, I. 1992. A theory of fads,
fashion, custom, and cultural change as informational
cascades. Journal of Political Economy 992–1026.
[Chamley 2004] Chamley, C. 2004. Rational Herds: Eco-
nomic Models of Social Learning. Cambridge University
Press.
[Chen et al. 2006] Chen, W.; Zeng, Q.; Wenyin, L.; and Hao,
T. 2006. A user reputation model for a user-interactive ques-
tion answering system. In Proceedings of the Second Inter-
national Conference on Semantics, Knowledge, and Grid.
[Chen 2008] Chen, Y.-F. 2008. Herd behavior in purchasing
books online. Computers in Human Behavior 24(5):1977–
1992.
[Cheng et al. 2014] Cheng, J.; Adamic, L.; Dow, P. A.;
Kleinberg, J. M.; and Leskovec, J. 2014. Can cascades be

self-fulﬁlling prophecies in an artiﬁcial cultural market. So-
cial Psychology Quarterly 71(4):338–355.
[Salganik, Dodds, and Watts 2006] Salganik, M. J.; Dodds,
P. S.; and Watts, D. J. 2006. Experimental study of in-
equality and unpredictability in an artiﬁcial cultural market.
Science 311(5762):854–856.
[Schunk 2003] Schunk, D. H. 2003. Self-eﬃcacy for reading
and writing: Inﬂuence of modeling, goal setting, and self-
evaluation. Reading & Writing Quarterly 19(2):159–172.
[Sipos, Ghosh, and Joachims 2014] Sipos, R.; Ghosh, A.;
and Joachims, T. 2014. Was this review helpful to you?:
it depends! context and voting patterns in online content. In
Proceedings of the 23rd International Conference on World
Wide Web (WWW).
[Stoddard 2015] Stoddard, G. 2015. Popularity dynamics
and intrinsic quality in Reddit and Hacker News. In AAAI
Conference on Web and Social Media (ICWSM).
[Szabo and Huberman 2010] Szabo, G., and Huberman,
B. A. 2010. Predicting the popularity of online content.
Communications of the ACM 53:184–192.
[van de Rijt et al. 2014] van de Rijt, A.; Kang, S. M.;
Restivo, M.; and Patil, A.
2014. Field experiments of
success-breeds-success dynamics. Proceedings of the Na-
tional Academy of Sciences 111(19):6934–6939.
[Wang, Wang, and Wang 2014] Wang, T.; Wang, D.; and
Wang, F. 2014. Quantifying herding eﬀects in crowd wis-
dom. In Proceedings of the ACM International Conference
on Knowledge Discovery and Data Mining (KDD).

Turk:

Market

and

1995.

1912.

2012.
Skimming

failure in
cherry-picking.

predicted? In Proceedings of the 23rd International Confer-
ence on the World Wide Web (WWW), 925–936. ACM.
[Coetzee et al. 2015] Coetzee, D.; Lim, S.; Fox, A.; Hart-
mann, B.; and Hearst, M. A. 2015. Structuring interactions
for large-scale synchronous peer learning. In Proceedings
of the 18th ACM Conference on Computer-Supported Co-
operative Work and Social Computing (CSCW), 1139–1152.
ACM.
[Dom and Paranjpe 2008] Dom, B., and Paranjpe, D. 2008.
A Bayesian technique for estimating the credibility of ques-
tion answerers. In Proceedings of the Society for Industrial
and Applied Mathematics.
Variabilit`a e mutabilit`a.
[Gini 1912] Gini, C.
Reprinted in Memorie di metodologica statistica (Ed. Pizetti
E, Salvemini, T). Rome: Libreria Eredi Virgilio Veschi 1.
[Gong 2012] Gong, A.
Mechanical
http://compsocsci.blogspot.ch/2012/03/market-failure-
in-mechanical-turk.html.
[Hogg and Lerman 2009] Hogg, T., and Lerman, K. 2009.
In Pro-
Stochastic models of user-contributory web sites.
ceedings of the Third AAAI Conference on Weblogs and So-
cial Media (ICWSM).
[Kelly et al. 2007] Kelly, D.; Wacholder, N.; Rittman, R.;
Sun, Y.; Kantor, P.; Small, S.; and Strzalkowski, T. 2007.
Using interview data to identify evaluation criteria for in-
teractive, analytical question-answering systems. Journal of
the American Society for Information Science and Technol-
ogy 58(8):80–88.
[Kim and Oh 2009] Kim, S., and Oh, S. 2009. User’s rele-
vance criteria for evaluating answers in a social Q&A site.
Journal of the American Society for Information Science and
Technology 60:716–727.
[Krumme et al. 2012] Krumme, C.; Cebrian, M.; Pickard,
G.; and Penetland, S. 2012. Quantifying social inﬂuence
in an online cultural market. PLoS ONE 7(5).
[Ladha 1995] Ladha, K. K.
Information pooling
through majority-rule voting: Condorcet’s jury theorem with
correlated votes. Journal of Economic Behavior & Organi-
zation 26(3):353–372.
[Lerman and Hogg 2014] Lerman, K., and Hogg, T. 2014.
Leveraging position bias to improve peer recommendation.
PLoS One 9(6).
[Lorenz et al. 2011] Lorenz, J.; Rauhut, H.; Schweitzer, F.;
and Helbing, D. 2011. How social inﬂuence can undermine
the wisdom of crowd eﬀect. Proceedings of the National
Academy of Sciences 108(22):9020–9025.
[Muchnik, Aral, and Taylor 2013] Muchnik, L.; Aral, S.; and
Taylor, S. J. 2013. Social inﬂuence bias: A randomized
experiment. Science 341(6146):647–651.
[Raafat, Chater, and Frith 2009] Raafat, R. M.; Chater, N.;
and Frith, C. 2009. Herding in humans. Trends in Cog-
nitive Sciences 13(10):420–428.
[Salganik and Watts 2008] Salganik, M. J., and Watts, D. J.
2008. Leading the herd astray: An experimental study of

