6
1
0
2

 
r
a

 

M
6
1

 
 
]

R

I
.
s
c
[
 
 

1
v
3
5
9
4
0

.

3
0
6
1
:
v
i
X
r
a

Mitigation Procedures to Rank Experts
through Information Retrieval Measures

Matthieu Vergne1,2

1 Center for Information and Communication Technology, FBK-ICT

Via Sommarive, 18 I-38123 Povo, Trento, Italy

vergne@fbk.eu

2 Doctoral School in Information and Communication Technology

Via Sommarive, 5 I-38123 Povo, Trento, Italy
matthieu.vergne@unitn.it

2016-03-15

Abstract

In order to ﬁnd experts, different approaches build rankings of people, assuming
that they are ranked by level of expertise, and use typical Information Retrieval
(IR) measures to evaluate their effectiveness. However, we ﬁgured out that expert
rankings (i) tend to be partially ordered, (ii) incomplete, and (iii) consequently
provide more an order rather than absolute ranks, which is not what usual IR
measures exploit. To improve this state of the art, we propose to revise the formalism
used in IR to design proper measures for comparing expert rankings. In this report,
we investigate a ﬁrst step by providing mitigation procedures for the three issues,
and we analyse IR measures with the help of these procedures to identify interesting
revisions and remaining limitations. From this analysis, we see that most of the
measures can be exploited for this more generic context because of our mitigation
procedures. Moreover, measures based on precision and recall, usually unable to
consider the order of the ranked items, are of ﬁrst interest if we represent a ranking
as a set of ordered pairs. Cumulative measures, on the other hand, are speciﬁcally
designed for considering the order but suffer from a higher complexity, motivating
the use of precision/recall measures with the right representation.

Keywords— Expert Recommendation, Information Retrieval, Design Evaluation

1

Introduction

When dealing with complex situations, it is sometimes recommended to rely on experts
in related domains to obtain the best feedback for achieving our goals. We work in
Requirements Engineering (RE), in which people are asked to identify and maintain

1

To the extent possible under law, Matthieu Vergne has waived all copyright and related or neighboring rights
to this technical report. For a more detailed description of this waiving, visit:
https://creativecommons.org/publicdomain/zero/1.0/

the speciﬁcations of a system, which often requires to have a broad knowledge about
the system and its environment, thus motivating the involvement of domain experts. To
ﬁnd experts, many systems have been developed with various techniques based on past
activities [Mockus and Herbsleb, 2002], question-answer patterns [Zhang et al., 2007],
language models [Balog, 2008, Serdyukov and Hiemstra, 2008], as well as other and
more comprehensive approaches [McDonald and Ackerman, 2000, Tang et al., 2008].
Basically, they aim at building a ranking of people ordered by decreasing level of
expertise, allowing to recommend the top people as the most expert. To evaluate these
approaches, mainly inspired from document retrieval techniques, it is common to use
Information Retrieval (IR) measures, which compare the rankings produced to some
reference rankings.

While we designed our own approach [Vergne and Susi, 2014], we noticed that
building a reference ranking does not result naturally to a complete and totally ordered
ranking [Vergne, 2016]. Yet, these assumptions are common in IR, leading these
measures to be hardly applicable to expert rankings unless we enforce them to be
complete and totally ordered. This concern becomes particularly relevant when aiming
for Open Source communities, with people freely joining and leaving the community,
making infeasible for a human to know exactly who is expert in what in a community
of hundreds of members. This observation leads us to consider that these reference
rankings tend to be incomplete (all people are not ranked) and partially ordered (several
people can be at the same rank). Both these assumptions make the ranks of the people a
poor indicator to consider, because this rank can change by introducing new people or
by introducing additional orders (making it closer to a total order). At the opposite, the
order before such modiﬁcations is preserved once they are applied, making it a more
reliable property to consider.

In this report, we ﬁrst propose to investigate procedures in Section 3 to mitigate
these new properties, not considered by usual IR measures. Then, in Section 4, we
analyse usual IR measures described in the literature by (i) identifying how to make
them applicable to our general context by applying the previous procedures and (ii)
what are their advantages and limitations. Finally, in Section 5, we draw a summary of
this analysis highlighting the most interesting measures and the remaining issues for
potential future works.

2 Problem: Comparing Expert Rankings
Context We work on a recommender system aiming for recommending the most
expert people on some topics, which is achieved by building rankings of experts. To
evaluate our system, we need to compare the rankings produced between each other
to assess its stability, or compare them to some references to assess its correctness.
Consequently, we are looking for similarity (or distance) measures to apply for com-
paring two rankings of experts A and B. Usually, IR measures are used by relying on
the similarity between expert retrieval and document retrieval, but expert rankings have
speciﬁc properties which need to be considered to design proper comparison measures:
Incompleteness Each ranking can be limited in size, especially human-made rankings, leading
to have only few people ranked compared to the whole set of available people.

2

Moreover, we can face situations where we should compare a ranking on a subset
of people to another ranking on a different (but usually overlapping) subset,
which means that we should be able to deal with rankings providing different
information.

Partial order Some people might be on the same rank, which means that they have the same
expertise or, more generally, that we are not able to tell which one is more
expert than the other. This can happen even if the human who made the ranking
is more knowledgeable on the topic than other people, while the latter would
provide a more complete order [Vergne, 2016]. So we cannot simply assume that
a reference ranking must be totally ordered.

Order consistency Both these observations lead to a third one, which is that the rank is not a reliable
In case of incompleteness, this rank can vary among
property to count on.
rankings on different people, yet the order would be the same (e.g. a > b > c
is compatible with b > c > d, although b and c have different ranks). Also
with partial order, the rank assigned to equal elements implies to consider it like
a conﬂict (e.g. although a > b > c > d is compatible with a > (b, c) > d,
the equal rank of b and c leads to at least one being in conﬂict with the former
ranking). Moreover, expertise is highly domain-speciﬁc [Ericsson, 2006], making
the identiﬁcation of the exact level of expertise achieved by a person particularly
hard. Most of the time, the people ranked are just ordered based on who appears
to be more or less expert, independently of their actual level, which again supports
that the key property to consider is not the rank of a person, but the relative order
compared to other people.

Problem/RQ We could design our own measure, but we want ﬁrst to know if existing
measures could be used, which is the purpose of this report. Thus, we are asking: is
there existing similarity/distance measures that can be used to compare expert rankings?
Today, the answer is assumed to be afﬁrmative because expert ﬁnding systems are
inspired from document retrieval systems [Balog, 2012], so we use well known IR
measures to validate them. The problem is that, by analysing these measures and how
they are used, we can see that they do not ﬁt the three requirements of incompleteness,
partial order, and order consistency.

3 Mitigation Procedures

In order to investigate properly existing measures, it is important to consider the speciﬁc
requirements of our expert rankings: the fact that they represent an order only, which can
be partial and incomplete. One way to do is by ﬁnding measures which already consider
such properties, which would allow us to build entirely on existing measures, but as
we will see in later sections no measure properly deal with all 3 properties. Another
way to increase the chances of ﬁnding interesting measures is to consider additional
procedures which deal with these properties, so existing measures can be enriched in
order to properly satisfy each requirement. In this section, we present a procedure for
each of these 3 properties.

3

3.1 Order: Ranking vs. Set of Ordered Pairs
When comparing rankings, it is important to consider the order in which the elements
are ranked. Effort has been made to design measures which look at the order of the
ranked items, like CGk and derived measures (Section 4.9 and after). If the measure is
already designed for considering orders, then we can use it as-is and deal with expert
rankings as an ordered list of people, e.g. (a, b, c). So the items considered by the
measure are people and the measure directly manage the orders in its own way.

Yet, other measures not considering orders might be of interest, for instance because
they are simpler or because the measures considering the orders are based on speciﬁc
strategies. If we face such a situation, it is interesting to investigate a way to use it such
that we can consider the order. In our case, we replace a ranking by its equivalent set of
ordered pairs, e.g. a > b > c = {a > b, a > c, b > c}, so an item is an ordered pair of
people and the measure does not need to consider any order, because they are directly
part of the items. In particular, two rankings based on the same people r1 = a > b > c
and r2 = c > b > a lead to compare two sets r1 = {a > b, a > c, b > c} and
r2 = {c > b, c > a, b > a}. Looking at the union of these two sets, like precision
(Section 4.1) which does not consider the order, results in an empty set interpreted as a
complete difference although they rank the same people, which is what we expect when
we look at how the two rankings are reversed.

However, this representation as a set of ordered pairs still has a potential limitation:
whether a pair is reversed or absent, the interpretation can be the same. For instance, if
we have :

• r1 = a > b > c = {a > b, a > c, b > c},
• r2 = c > b > a = {c > b, c > a, b > a},
• r3 = c > b = {c > b},

then we see that comparing r1 to r2 by looking at the shared pairs (∅) gives the same
result than comparing r1 to r3, although r2 is closer to r1 because it has the same set
of elements. This problem can be solved by differentiating an absent item from an
“opposite” item, but none of the measures analysed in this report provide such a feature.

3.2 Partial Order: Maximal Similarity
A ranking of experts can be partially ordered simply because one does not have enough
information to differentiate the expertise of two people. In such a case, it is important
that the measure used to compare two rankings does not enforce arbitrary assumptions,
which is what happen when it requires to have a totally ordered ranking. For instance,
a measure based on the rank of the item might be tricked by a misalignment due to a
partial order: a > b > c > d is not conﬂictual with a > (b, c) > d while it is conﬂictual
with a > c > b > d. Yet, both cases are treated similarly: because b and c have the same
rank (2, 2.5 or 3 depending on the method used) then at least one of them is “wrong”,
which is clearly not expected if the reference is the partial one. Of course, having more
than two elements at the same rank makes it even worse, potentially making such a lack
of information appearing as more conﬂictual than a single, but explicit, conﬂict.

4

If we face a measure which considers the order of the ranking, we assume that two
rankings should be considered as different based on their explicit misalignments, so
when two items are ordered in a reversed way. To do so, we consider Algorithm 1 which,
from two partially ordered rankings, compares all the possible totally ordered rankings
and take the maximal similarity. In the case of a distance, the initial value (−∞) should
be reversed (∞) and we should take the minimal value instead of the maximal one.

Algorithm 1 Similarity computation for partial rankings.
Input r1, r2: partially ordered rankings
Input s: similarity function for totally ordered rankings
Output sim: similarity value between a and b
1: R1 = buildP ossibleT otalRankingsF or(r1)
2: R2 = buildP ossibleT otalRankingsF or(r2)
3: sim = −∞
4: for each (r(cid:48)
5:
6: end for

2) ∈ R1 × R2 do
2))

1, r(cid:48)

sim = max(sim, s(r(cid:48)

1, r(cid:48)

The method buildP ossibleT otalRankingsF or(r) provides the set of totally or-
dered rankings (1 person per rank) which are compatible with the partially ordered
ranking r. For instance, for a partial ranking r = a > (b, c) > d, the function returns
the two possible total rankings {a > b > c > d, a > c > b > d}. If the ranking
is already totally ordered, then only one ranking (the same) is returned, and if both
rankings are totally ordered, only the similarity measure s(r1, r2) is computed, so we go
back to the original measure. This is how we see that this algorithm properly generalizes
measures assuming total orders.

Incompleteness: Homogenization

3.3
Expert rankings are often limited in size: we can restrict them because we are only
interested in the top people, or we might face practical limitations, like human-made
rankings which cannot rank hundreds of people in a reliable way (e.g. because of fatigue
or simply lacks of information). Of course, human-made rankings can be combined to
complete each other, but conﬂicts can occur [Vergne, 2016] and need to be mitigated.
This is usually achieved by assuming that a broad agreement is correct: if a majority of
rankings say the same, then it should be true. Assuming such equivalence is arguable,
because to be able to evaluate the expertise of someone, one needs himself to know
what to evaluate and how, what is correct, etc. which means to have enough expertise
too [Ericsson, 2006]. So if, among all the human-made rankings, only few of them are
made by actual experts, an agreement-based ranking might loose correct information in
favour of social agreements.

Consequently, if the available rankings do not allow to obtain a complete ranking,
we need to use a measure able to deal with it. With this in mind, we might still be
interested in measures which assume that the rankings are complete, or at least that rank
the same set of people. In such a case, we need to adapt them to deal with incomplete
rankings, which does not mean only having one ranking subset of the other, because

5

each ranking could be incomplete, meaning that each ranks people that the other does
not. For us, if a person is missing for a ranking A, then no constraint is given, so any rank
is ﬁne, in particular the rank given by the other ranking B. In other word, independently
of the rank given by B, this person does not intervene in the comparison between the
two rankings A and B. Consequently, we propose to homogenize the two rankings by
removing the people which are not in both rankings.

This is consistent with the fact that we consider order rankings (Section 3.1): by
reducing a ranking, we change the ranks of the lowest people, so measures relying on
the rank are impacted, but the order is preserved, so measures considering the order
only are not impacted. In particular, Algorithm 1, which deals with the partial order
issue, provides the same result whether or not this procedure is applied. An important
point is that this procedure does not generalizes to more than 2 rankings: it works only
because we are making a 1-to-1 comparison. In the case where we compare 3 rankings
A, B, and C, removing a person from A and B because C does not have it means that
we ignore any potential misalignment between A and B regarding this person. Finally,
some measures could assume that a missing/extra element should hurt the similarity, in
which case no homogenization should be considered to not corrupt the results of the
measure.

4 Analysis of IR Similarity Measures

[Balog, 2012] presents usual IR measures, established by the TREC community, for
evaluating expert ﬁnding methods, which are evaluated in exactly the same way as
document retrieval systems. From their point of view, this is a reasonable choice, since
“the quality of rankings can be estimated independently of what we rank if quality
measures for individual items are alike”. We conﬁrmed from one of the authors that this
sentence essentially means it does not matter whether we rank documents or experts
(or other objects), we can use the same measures. Although we might agree on the
feasibility of applying the same measures, we don’t see clear evidences that the measures
cited are such well-ﬁtted measures for a generic purpose.

In this section, we investigate a broad set of IR measures designed to evaluate how
well an IR system performs upon a query q (or a set of queries Q). The measures
investigated are based on [Balog, 2012] and [Manning et al., 2008], but also other re-
sources taken from the Web about IR1,2,3. For each measure, we provide the deﬁnition
of the measure and analyse its use for comparing expert rankings, considering also the
procedures described in Section 3 to complete them when necessary.

1Introduction to IR: http://nlp.stanford.edu/IR-book/html/htmledition/evalua

tion-in-information-retrieval-1.html

2IR on Wikipedia: https://en.wikipedia.org/w/index.php?title=Information_r

etrieval&oldid=679995615

3Cumulative Gain and derived measures on Wikipedia: https://en.wikipedia.org/w/index

.php?title=Discounted_cumulative_gain&oldid=674619295

6

4.1 Precision (P)
Theory

P (q) =

|{relevant items for q} ∩ {retrieved items for q}|

|{retrieved items for q}|

Application to expert rankings A and B With this measure, we have two types of
items: the retrieved ones and the relevant ones. We can use the ranking A for the
retrieved items and B for the relevant ones, such that the intersection of the numerator
retrieves the common items between the two, making it a proper similarity measure.
The issue here is that this measure does not consider any order, what can be ﬁxed
by using ordered pairs, as described in Section 3.1. However, because there is no
difference between absent and reversed pairs, we might be interested in reducing as
much as possible the differences caused by absent pairs, especially the ones which
cannot be shared, which happens for people who are not ranked by both rankings This
can be reduced by restricting the rankings to their shared people by homogenization, as
described in Section 3.3.

For the remaining pairs, the partial ordering can still lead to absent pairs, which
are considered like reversed pairs. If this behaviour is not wanted, we can reverse the
logics by computing the maximal similarity with the procedure described in Section 3.2.
This is a matter of interpretation: we might be interested in using an optimistic measure
or a pessimistic one, leading to use the procedure or not. A better measure might be
one which differentiates between explicit agreement (same pair), explicit disagreement
(reversed pair), and uncertain agreement (absent pair).

Finally, we can use this measure to compare a ranking to a reference or to compare
two rankings in a symmetric way. In the ﬁrst case, the reference can be naturally used
as relevant items while the other ranking, the one evaluated, can be naturally used as
retrieved items. The difﬁculty comes with the symmetric comparison: because the
numerator considers only one ranking, it implies to have a different normalization factor
depending on which ranking is used for what, making it potentially asymmetric. To
make the measure properly symmetric (P (A, B) = P (B, A)), we can ensure that both
rankings provide the same number of items, which can be achieved by using the three
mitigation procedures presented. If some of them are not used, we can still ensure
the symmetry by combining both uses in a symmetric computation, for instance with
P (A,B)+P (B,A)

2

.

4.2 Recall (R)
Theory

R(q) =

|{relevant items for q} ∩ {retrieved items for q}|

|{relevant items for q}|

Application to expert rankings A and B Recall is in many ways similar to precision,
and we can apply the same reasoning to obtain the same conclusions. The only difference

7

is on the normalization factor, which is the number of relevant items instead of the
number of retrieved items, but we can see that by reversing the two we obtain the
same formula than for precision (P (A, B) = R(B, A)). Rather than an additional
measure, we see that it offers a complementary measure that we already used to make
the precision measure symmetric. Indeed, by computing P (A,B)+P (B,A)
we actually
compute P (A,B)+R(A,B)
). If precision or recall appear as
good similarity measures, one might consider this formula as a good way to combine
the strengths of both, especially if we don’t use all the mitigation procedures.

(and also R(A,B)+R(B,A)

2

2

2

4.3 Fall-out
Theory

fall-out(q) =

|{non-relevant items for q} ∩ {retrieved items for q}|

|{non-relevant items for q}|

Application to expert rankings A and B Like recall offers a complementary per-
spective to precision by changing the normalization factor, fall-out offers yet another
complementary perspective by considering the complement of the relevant items. In
fact, we can use it in the very same way than recall, but to compute a distance measure
(as opposed to similarity) because we look at the complement items. This is possible as
long as the items are ordered pairs: if the complement corresponds to the reversed pairs
(not reversed and absent pairs), then we deal with the very same information. If the
items were people only, then the complement would be the non-ranked people (reversing
the ranks would have no effect), which is arguably justiﬁable because the ranked people
are usually the most relevant people, not the relevant ones, so the complement would
not be the non-relevant people only. Anyway, this strategy is not interesting because it
does not allow to consider the order of the people, justifying that we use the ordered
pairs instead.

Actually, it might be interesting to investigate to which extent recall and fall-out
complement each other: although they are strictly opposed for explicit pairs, the absent
pairs (present only in the other ranking) are still part of the computation. In particular,
it could be interesting to see if we can ﬁnd an equivalence between the fall-out and a
complemented recall, with or without the maximal similarity procedure described in
Section 3.2. We consider such a deep analysis out of the scope of this report and limit
our conclusion to the following: due to their similarity, the fall-out measure shows as
much interest as precision and recall as a candidate measure to compare rankings of
experts.

4.4 F-Score
Theory

Fβ(q) =

(1 + β2)(P (q).R(q))

β2P (q) + R(q)

8

F0.5, F1 and F2 seems particularly used to, respectively, give priority to precision, have
a balanced measure, or give priority to recall.

Application to expert rankings A and B It combines precision and recall and allow
to have a symmetric measure with F1, which is interesting when no one is assumed to
be a reference. If another value is used for β, we might, like mentioned before, compute
both Fβ(A, B) and Fβ(B, A) and combine them in a balanced way to have a symmetric
measure. Consequently, this measure acts as another solution to combine both precision
and recall values, but with an additional complexity which makes it harder to interpret.

4.5 Precision at k (P@k)
Theory

P @k(q) =

|{relevant items for q} ∩ {k ﬁrst retrieved items for q}|

k

Application to expert rankings A and B At the opposite of the usual precision
computation, P@k aims at evaluating a reduced set of retrieved items, not all of them.
If, like precision, we use a set of ordered pairs, we need to choose the right pairs to
consider, which is not straightforward. Indeed, the rank of a person is described, with
this representation, by several pairs, so we need to make a trade-off between the number
of pairs to consider for each person and the number of persons represented by these
pairs. Additionally, assuming that a good trade-off would be identiﬁed for a given k,
choosing k might also suffer some arbitrariness.

It seems to us more interesting to use this measure with people as items (rather than
pairs) in order to evaluate the correctness of the top of the ranking. Still, no order is
considered by this measure, but we can reduce this issue if the ranking used to represent
the relevant items is also reduced to the k ﬁrst items. In such a case, we actually use
the same formula than for precision, but with an additional reduction to k items of
both the compared rankings, and evaluating it with an increasing k allows to analyse
the full set, as shown for instance with AveP in Section 4.7. Nevertheless, the order is
only superﬁcially considered and requires to analyse several values. It also adds to the
fact that this measure analyses the rank differences, not only the orders, so we need
to homogenize the rankings to avoid the misalignments due to having different people
ranked.

4.6 R-Precision
Theory We compute P@k with k = |relevant items for q|, which can be different for
each query q.

Application to expert rankings A and B This is equivalent to the P@k measure,
but applied to a single k value, and with the necessity of knowing the exact number of
relevant items, which is different depending on the representation used (ranking or set
of pairs). By using a ranking, the number of relevant items is the size of the ranking

9

providing the relevant items, and with the homogenization, both rankings have the same
size and items leading to have always 1, which is useless. By using a set of ordered pairs,
the number of relevant items is the number of pairs, and because of the homogenization,
it is ﬁnally equivalent to compute the precision as described in Section 4.1.

4.7 Average Precision (AveP)
Theory

AveP (q) =

(cid:80)n

k=1 δk(q)P @k(q)
|relevant items for q|

with n the number of retrieved items for the query q and δk(q) the relevance of the k-th
item retrieved (1 if it is relevant, 0 otherwise). In the case where we rank the whole set
of items, so all the relevant items are for sure retrieved, the formula can be rewritten
more like a typical weighted average, thus justifying its naming:

(cid:80)n

(cid:80)n

AveP (q) =

k=1 δk(q)P @k(q)

k=1 δk(q)

Application to expert rankings A and B As we see from the second formula, we
compute an average of P@k over the consecutive k, which means that the order directly
inﬂuences the ﬁnal result. This is an example of use of P@k as described in Section 4.5,
where several values are computed for an analysis able to consider the order. Indeed,
although we consider the same number of P@k, by having relevant items sooner in the
ranking we consider higher values of P@k, leading to a higher value of AveP.

4.8 Mean Average Precision (MAP)
Theory

(cid:80)

M AP (Q) =

q∈Q AveP (q)

|Q|

Application to expert rankings A and B With this measure, we change the level of
evaluation: while the previous measures allowed us to compare two rankings designed
for a speciﬁc query q, this measure aims at aggregating the values over the whole set
of queries Q to evaluate the overall performance of the recommender system. This
strategy can be applied to any query-speciﬁc measure described before, and MAP simply
applies it to AveP. In other words, this is not an interesting measure for identifying a
comparison function between two rankings, but the general strategy is interesting as an
overall measure building on them, so it is not completely out of scope and deserves to
be mentioned.

10

4.9 Cumulative Gain (CGk)
Theory

CGk(q) =

k(cid:88)

i=1

reli(q)

with k the number of top ranks to evaluate and reli(q) the relevance of the i-th retrieved
item for q. Like P@k, the aim is to evaluate a reduced amount of items, but the value
sums with lower ranks instead of being computed rather independently.

Application to expert rankings A and B From this measure, we consider a different
kind of evaluation, where the relevance of an item is provided by an external function
rel providing higher values for more relevant items. With this ﬂexibility, it is possible
to consider the orders even with a ranking representation by using a function giving a
higher relevance to items which should be closer to the top. For instance, assuming that
one ranking is used as a reference, we can assign a relevance of 1 to the last item of this
ranking and k to the top item, or a more complex distribution to give more weight to
some ranks. Then, CGk is computed for the other ranking (the retrieved items) based on
this reference. If we need a symmetric value, we can imagine to compute an average of
two values: one computing CGk for the ranking A based on the ranking B, and another
for the ranking B based on A.

Although it might seem an interesting measure because the order can be considered
through the design of rel, the order still has a little impact, because values are summed
up on the sole criteria of the presence of the item. It means that, for the same ranking
but re-ordered, the same value is computed, showing that it gives few information
individually. The power of this measure occurs when we compute it for different k
values, so we can analyse the evolution of the value in a graph, showing for instance that
we gain more relevance when looking at long rankings, so the most relevant items are
actually ranked quite low. Analysing a graph is more complex than analysing a single
value, so using this measure requires a greater effort to draw conclusions, which is one
of the critics we could made. Additionally, this measure lacks the required normalization
to reliably compare values for one ranking to values for another.

Because this measure relies on the k ﬁrst items, it means that we should be able to
identify what comes “ﬁrst”, so using the representation of a set of ordered pairs, which
is not ordered, does not seem adapted. However, if we are computing only the value for
k (i.e. the whole set) then all pairs should be considered and no problem of identifying
the “ﬁrst” pairs occur. In this case, we can for example use this representation with a
binary function: 1 if the pair is in the right order, 0 otherwise. Actually, it can be better
by assigning a high relevance to pairs in the right order, a low relevance to reversed
pairs, and a middle relevance to absent pairs, making the measure able to make the
difference between absent and reversed pairs. This is an advantage of this measure
compared to the previous ones, based on precision and recall, which cannot make this
difference. However, we still face the limitation of the normalization, which hurts our
ability to compare the values computed for different rankings.

11

4.10 Discounted CG (DCGk)
Theory

DCGk(q) = rel1(q) +

k(cid:88)

i=2

reli(q)
log2(i)

Like CGk, we sum up the relevance values, but this time by applying a logarithmic
weight to decrease the global relevance if highly relevant items appear too late in the
ranking. We might take the freedom of slightly changing the logarithmic value in order
to rewrite the formula in a simpler way:

k(cid:88)

k(cid:88)

i=1

DCG(cid:48)

k(q) =

reli(q)

log2(i + 1)

i=1

With such a shape, we see better how it simply enriches the terms of the sum with a
weighting factor compared to CGk. Additionally, it also allows to make the link with
another formula used for DCGk:

DCGk(q) =

2reli(q) − 1
log2(i + 1)

which prioritizes the relevance value upon the weighting factor.

Application to expert rankings A and B At the opposite of CGk, DCGk directly
involves the index of the item in the computation, so we need to map each item to an
index. This addition makes it inapplicable to the representation using sets of ordered
pairs because these sets are not ordered (unless we assign the same index to all the
pairs, in which case it is equivalent to CGk). Consequently, we can only use the ranking
representation, and the rel function should be designed carefully based on a reference
ranking (A or B) before to compute the DCGk value of the other ranking (resp. B or
A). In this context, DCGk only revises the weighting strategy, so it does not provide
anything more than what CGk already provides (rel can be directly designed to compute
the same values). In other words, we can summarize DCGk (and its variants) as a
specialization of CGk applied to rankings with a speciﬁc class of rel functions. In
particular, DCGk does not introduce any normalization allowing to compare values
computed for different rankings.

4.11 Normalized DCG (NDCGk)
Theory

N DCGk(q) =

DCGk(q)
IDCGk(q)

where IDCGk is the ideal DCGk, meaning the value of DCGk when the items are
re-ordered to maximize it (i.e. sorted by decreasing relevance).

12

Application to expert rankings A and B This measure solves the main issue of
the previous ones: the normalization applied allows to compare values computed for
different rankings to see which one is better. As such, what is interesting with this
measure is, like for MAP, the generic strategy it involves: divide the value of the “sub-
measure” (here DCGk) by the best value achievable with this “sub-measure”. This
integral dependency to the “sub-measure” allows to use it even if we use a different one.
In particular, we might be interested in using CGk on the sets of ordered pairs, leading
to have a normalized version following the same strategy:

N CGk(q) =

CGk(q)
ICGk(q)

where ICGk is the ideal CGk, meaning the value of CGk when the pairs retrieved are
the same than the reference, or said another way the value of CGk when we evaluate the
ranking A while taking A also as the reference (or evaluating B based on B).

5 Discussion

As we saw, many usual IR measures described in Section 4 are interesting for comparing
two rankings of experts, whether we are looking for symmetric or reference-based
comparisons. However, due to the speciﬁc properties of our rankings, the usefulness
of these measures cannot be highlighted without using the procedures proposed in
Section 3. In particular, two main types of measures show up: the measures based on
precision and recall, and the cumulative measures.

Precision and recall (sections 4.1 and 4.2), and more extensively with fall-out
(Section 4.3), show an interesting complementarity worth to exploit. The fact that they
do not consider any order makes them particularly suited for a representation using sets
of ordered pairs, described in Section 3.1. Measures derived from them show additional
interests: the F-score provides an interesting combination strategy (Section 4.4), and
if P@k has many issues (Section 4.5), particular uses appear to be interesting, like
r-precision for ordered pairs (Section 4.6) and AveP for rankings (Section 4.7).

On the other hand, Cumulative measures like CGk (Section 4.9) use an external rele-
vance function rel which makes them ﬂexible enough to adapt to both representations,
rankings as well as sets of ordered pairs. While one of the main issues of CGk is its lack
of normalization, Section 4.11 shows how to ﬁx it, and Section 4.10 shows interesting
functions to use, although they are more adapted to rankings than sets of ordered pairs.
However, another main issue of these measures is the need to compute a set of them (for
different k values) rather than a single one, at the opposite of precision/recall measures.
Additionally, while all these measures are worth investigating for our purpose, i.e.
to compare two rankings of experts, we might argue that it is not enough to evaluate a
complete approach, which is the ultimate goal of an evaluation. Indeed, each ranking
corresponds to a speciﬁc query q, while evaluating an approach implies to analyse it
over a full set of queries Q. Nevertheless, usual IR measures already consider such
facilities, like for MAP (Section 4.8) which uses a generic method applicable to any of
the described measures (simple average over the set of queries).

13

Of course, this report remains short in its analysis, its aim being to identify measures
worth to investigate rather than making a deep investigation of each of them. With the
procedures proposed in Section 3, we were able to highlight potential uses of usual
measures for a more general context than what they were designed for. In particular, by
showing how they appear to be equivalent or complementary, and which issues might be
faced when using them, this report provides a comprehensive basis for future works.

6 Conclusion

In this report, we investigated usual IR measures to evaluate their applicability to a more
generic context than what they were designed for. In particular, we highlighted the need
to consider rankings which are incomplete and partially ordered, leading to focus on
the relative order of the ranked items rather than their absolute ranks. We proposed
procedures to adapt IR measures, in order to consider these properties, and shown that
most IR measures remain applicable with their help. In particular, by representing a
ranking as a set of ordered pairs, measures based on precision and recall appear as
the most interesting measures. Although cumulative measures (i.e. CGk and derived)
have been designed precisely for advanced analysis, the simplicity of computation and
interpretation of the precision/recall measures enforce this regain of interest.

This report, although it considers a comprehensive list of IR measures, remains
superﬁcial in its analysis by focusing on speciﬁc adaptations and potential uses. An
interesting future work would be a deeper investigation on the consequences of the
procedures proposed to adapt these measures, identifying the differences and equiva-
lences, with some highlights on the computation performances. In particular, it would be
interesting to know how usual measures can be rewritten to consider sets of ordered pairs
naturally, rather than using additional procedures like homogenization and maximal
similarity. For instance, by replacing the rank (or index) of an item in current formaliza-
tions by the number of ordered pairs which rank this item lower than others, thus relying
only on the relative order rather than an absolute rank, it might be possible to generalize
these measures and make them able to deal with partial orders and incompleteness more
naturally.

References
[Balog, 2008] Balog, K. (2008). People search in the enterprise. PhD thesis, University
of Amsterdam, http://krisztianbalog.com/publications/phd-th
esis/.

[Balog, 2012] Balog, K. (2012). Expertise Retrieval. Foundations and Trends R(cid:13) in
Information Retrieval, 6(2-3):127–256, ISSN: 1554-0669, 1554-0677, DOI:
10.1561/1500000024, http://www.nowpublishers.com/article/
Details/INR-024.

[Ericsson, 2006] Ericsson, K. A., editor (2006).

of Expertise and Expert Performance.

The Cambridge Handbook
Cambridge University Press, Cam-

14

bridge ; New York, ISBN: 978-0-521-84097-2 978-0-521-60081-1,
http://www.cambridge.org/us/academic/subjects/psychol
ogy/cognition/cambridge-handbook-expertise-and-expert-
performance.

[Manning et al., 2008] Manning, C. D., Raghavan, P., and Sch¨utze, H. (2008). Intro-
duction to information retrieval. Cambridge University Press, New York, ISBN:
978-0-521-86571-5.

[McDonald and Ackerman, 2000] McDonald, D. W. and Ackerman, M. S. (2000).
Expertise recommender: a ﬂexible recommendation system and architecture.
In Proceedings of the 2000 ACM conference on Computer supported coopera-
tive work, pages 231–240. ACM Press, ISBN: 978-1-58113-222-9, DOI:
10.1145/358916.358994, http://portal.acm.org/citation.cf
m?doid=358916.358994.

[Mockus and Herbsleb, 2002] Mockus, A. and Herbsleb, J. D. (2002).

Exper-
In Proceed-
tise browser: a quantitative approach to identifying expertise.
ings of the 24th International Conference on Software Engineering, ICSE ’02,
pages 503–512, New York, NY, USA. ACM, ISBN: 1-58113-472-X, DOI:
10.1145/581339.581401, http://doi.acm.org/10.1145/581339.
581401.

[Serdyukov and Hiemstra, 2008] Serdyukov, P. and Hiemstra, D. (2008). Mod-
In Proceed-
eling Documents As Mixtures of Persons for Expert Finding.
ings of the IR Research, 30th European Conference on Advances in Informa-
tion Retrieval, ECIR’08, pages 309–320, Berlin, Heidelberg. Springer-Verlag,
ISBN: 3-540-78645-7 978-3-540-78645-0, http://dl.acm.org/c
itation.cfm?id=1793274.1793313.

[Tang et al., 2008] Tang, J., Zhang, J., Yao, L., Li, J., Zhang, L., and Su, Z. (2008).
ArnetMiner: Extraction and Mining of Academic Social Networks. In Proceed-
ings of the 14th ACM SIGKDD International Conference on Knowledge Discov-
ery and Data Mining, KDD ’08, pages 990–998, New York, NY, USA. ACM,
ISBN: 978-1-60558-193-4, DOI: 10.1145/1401890.1402008, http:
//doi.acm.org/10.1145/1401890.1402008.

[Vergne, 2016] Vergne, M. (2016). Gold Standard for Expert Ranking: A Survey on
the XWiki Dataset. Technical Report arXiv:1603.03809 [cs.SE], http://arxiv.
org/abs/1603.03809.

[Vergne and Susi, 2014] Vergne, M. and Susi, A. (2014). Expert Finding Using
Markov Networks in Open Source Communities.
In Jarke, M., Mylopoulos, J.,
Quix, C., Rolland, C., Manolopoulos, Y., Mouratidis, H., and Horkoff, J., edi-
tors, Advanced Information Systems Engineering, number 8484 in Lecture Notes
in Computer Science, pages 196–210. Springer International Publishing, ISBN:
978-3-319-07880-9 978-3-319-07881-6, http://link.springer.
com/chapter/10.1007/978-3-319-07881-6_14.

15

[Zhang et al., 2007] Zhang, J., Ackerman, M. S., and Adamic, L. (2007). Ex-
In Pro-
pertise networks in online communities:
ceedings of the 16th international conference on World Wide Web, WWW ’07,
pages 221–230, New York, NY, USA. ACM, ISBN: 978-1-59593-654-7,
DOI: 10.1145/1242572.1242603, http://doi.acm.org/10.1145/
1242572.1242603.

structure and algorithms.

16

