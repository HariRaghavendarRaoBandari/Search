6
1
0
2

 
r
p
A
4

 

 
 
]
L
M

.
t
a
t
s
[
 
 

3
v
3
3
7
4
0

.

3
0
6
1
:
v
i
X
r
a

Structured and Efﬁcient Variational Deep Learning with Matrix Gaussian

Posteriors

Christos Louizos
AMLAB, Informatics Institute, University of Amsterdam
Max Welling
AMLAB, Informatics Institute, University of Amsterdam
Canadian Institute for Advanced Research (CIFAR)

C.LOUIZOS@UVA.NL

M.WELLING@UVA.NL

Abstract

We introduce a variational Bayesian neural net-
work where the parameters are governed via
a probability distribution on random matrices.
Speciﬁcally, we employ a matrix variate Gaus-
sian (Gupta & Nagar, 1999) parameter posterior
distribution where we explicitly model the co-
variance among the input and output dimensions
of each layer. Furthermore, with approximate
covariance matrices we can achieve a more ef-
ﬁcient way to represent those correlations that
is also cheaper than fully factorized parameter
posteriors. We further show that with the “local
reprarametrization trick” (Kingma et al., 2015)
on this posterior distribution we arrive at a Gaus-
sian Process (Rasmussen, 2006) interpretation of
the hidden units in each layer and we, similarly
with (Gal & Ghahramani, 2015), provide connec-
tions with deep Gaussian processes. We continue
in taking advantage of this duality and incor-
porate “pseudo-data” (Snelson & Ghahramani,
2005) in our model, which in turn allows for
more efﬁcient sampling while maintaining the
properties of the original model. The validity of
the proposed approach is veriﬁed through exten-
sive experiments.

1. Introduction
While deep learning methods are beating every record in
terms of predictive accuracy, they do not yet provide the
user with reliable conﬁdence intervals. Yet, for most ap-
plications where decisions are made based on these predic-
tions, conﬁdence intervals are key. Take the example of

Preliminary work. Copyright 2016 by the author(s).

an autonomous driving vehicle that enters a new unknown
trafﬁc situation: recognizing that predictions become un-
reliable and handing the steering wheel back to the driver
is essential. Similarly, when a physician diagnoses a pa-
tient with some ailment and prescribes a drug with poten-
tially severe side effects, it is essential that she/he knows
when predictions are unreliable and additional investiga-
tion is necessary. These considerations have motivated us
to develop a fully Bayesian deep learning framework that
is accurate, efﬁcient and delivers reliable conﬁdence inter-
vals.
Furthermore, by being Bayesian we can also harvest an-
other property as a byproduct; natural protection against
overﬁtting. Instead of making point estimates for the pa-
rameters of the network, which can overﬁt and provide
erroneously certain predictions, we estimate a full poste-
rior distribution over these parameters. Armed with these
posterior distributions we can now perform predictions us-
ing the posterior predictive distribution, i.e. we can now
marginalize over the network parameters and make pre-
dictions on the basis of the datapoints alone. As a result
we can both obtain the aforementioned conﬁdence intervals
and better regularize our networks, which is very important
in problems where we do not have enough data relative to
the amount of features.
Obtaining the parameter posterior distributions for large
neural networks is however intractable. To this end, many
methods for approximate posterior inference have been de-
vised. Markov Chain Monte Carlo (MCMC) methods are
one class of methods that have been explored in this context
via Hamiltonian Monte Carlo (Neal, 2012) and stochastic
gradient methods (Welling & Teh, 2011; Ahn et al., 2012).
Another family of methods that provide deterministic ap-
proximations to the posterior are based on variational in-
ference. These cast inference as an optimization problem
and minimize the KL-divergence between the approximate
and true posterior. There have been many recent attempts

Structured and Efﬁcient Variational Deep Learning with Matrix Gaussian Posteriors

that have adopted this paradigm (Graves, 2011; Hern´andez-
Lobato & Adams, 2015; Blundell et al., 2015; Kingma
et al., 2015). However, most of these approaches assume
a fully factorized posterior distribution over the neural net-
work weights. We conjecture that this assumption is very
restricting as the “true” posterior distribution does have
some correlations among the network weights. Therefore
by using a fully factorized posterior distribution the learn-
ing task becomes “harder” as there is not enough informa-
tion sharing among the weights.
We therefore introduce a variational Bayesian neural net-
work that instead of treating each element of the weight
matrix independently, it treats the weight matrix as a whole
via a matrix variate Gaussian distribution (Gupta & Na-
gar, 1999), i.e. a distribution over random matrices. This
parametrization will signiﬁcantly reduce the amount of
variance-related parameters that we have to estimate: in-
stead of estimating a separate variance for each weight
we can now estimate separate variances for each row and
column of the weight matrix, i.e input and output feature
speciﬁc variances. This will immediately introduce corre-
lations, and consequently information sharing, among the
weights. As a result, it will allow for an easier estimation
of the weight posterior uncertainty.
In addition, we will also provide a distinct relation be-
tween our model and deep (multi-output) Gaussian Pro-
cesses (Damianou & Lawrence, 2013); this relation arises
through the application of the “local reparametrization
trick” (Kingma et al., 2015) on the matrix variate Gaus-
sian distribution. This fact reveals an interesting property
for this Bayesian neural network: we can now sample more
efﬁciently while maintaining the properties of the original
model through the introduction of pseudo-data (Snelson &
Ghahramani, 2005).

2. Beyond fully factorized parameter

posteriors

2.1. Matrix variate Gaussian distribution

The matrix variate Gaussian (Gupta & Nagar, 1999) is a
three parameter distribution that governs a random matrix,
e.g. W:
p(W) = MN (M, U, V)

2 tr(cid:2)V−1(W − M)T U−1(W − M)(cid:3)(cid:1)

exp(cid:0) − 1

=

(2π)np/2|V|n/2|U|n/2

(1)
where M is a r × c matrix that is the mean of the distri-
bution, U is a r × r matrix that provides the covariance of
the rows and V is a c × c matrix that governs the covari-
ance of the columns of the matrix. According to (Gupta &
Nagar, 1999) this distribution is essentially a multivariate

Gaussian distribution where:

p(vec(W)) = N (vec(M), V ⊗ U)

where vec(·) is the vectorization operator (i.e. stacking the
columns into a single vector) and ⊗ is the Kronecker prod-
uct. Despite the fact that the matrix variate Gaussian is a
simple generalization of the multivariate case it provides us
a straightforward way to separate the correlations among
the rows and columns of the matrix, which implicitly af-
fects the correlations among the input and output hidden
units.

2.2. Variational inference with matrix variate Gaussian

posteriors

For the following we will assume that each input to a layer
is augmented with an extra dimension containing 1’s so
as to account for the biases and thus we are only deal-
ing with weights W on this expanded input.
In order
to obtain a matrix variate Gaussian posterior distribution
for these weights we can work in a pretty straightforward
way: the derivation is similar to (Graves, 2011; Kingma &
Welling, 2014; Blundell et al., 2015; Kingma et al., 2015).
Let pθ(W), qφ(W) be a matrix variate Gaussian prior and
posterior distribution with parameters θ, φ respectively and
i=1 be the training data sampled from the empirical
(xi, yi)N
distribution ˜p(x, y). Then the following lower bound on
the marginal log-likelihood can be derived:

(cid:20)(cid:90)
(cid:20)

L(φ; θ) = E ˜p(x,y)[log p(Y|X)] ≤

pθ(W)p(Y|X, W)

E ˜p(x,y)

= E ˜p(x,y)

qφ(W) log

(cid:2) log p(Y|X, W)(cid:3)−

qφ(W)

Eqφ(W)

(cid:21)

dW

(2)

(cid:21)

− KL(qφ(W)||pθ(W))

al.,

(Graves,

2011; Blundell
refer

(cid:2) log p(Y|X, W)(cid:3) as the expected log-likelihood

Following
et
2015;
Kingma et al., 2015) we will
to L(X,Y) =
Eqφ(W)
and to Lc = −KL(qφ(W)||pθ(W)) as the complexity
loss. To estimate L(X,Y) we will use simple Monte
Carlo integration along with the “reparametrization
trick” (Kingma & Welling, 2014; Rezende et al., 2014):

(cid:2) log p(Y|X, W)(cid:3) =

Eqφ(W)

L(cid:88)

i=1

1
L

log p(Y|X, W(l))

1

1
2

2 E(l)V

(cid:0)i.e. Eij ∼ N (0, 1)(cid:1)

W(l) = M + U
E(l) ∼ MN (0, I, I)
As for the complexity loss Lc; due to the relation with
the multivariate Gaussian we can still calculate the KL-

(3)

Structured and Efﬁcient Variational Deep Learning with Matrix Gaussian Posteriors

divergence between the matrix variate Gaussian prior and
posterior efﬁciently in closed form.
However, maintaining a full covariance over the rows and
columns of the weight matrix is both memory and compu-
tationally intensive. In order to still have a tractable model
we approximate each of the covariances with a diagonal
matrix (i.e.
independent rows and columns) for simplic-
ity1. This approximation provides a per-layer parametriza-
tion that requires signiﬁcantly less parameters than a sim-
ple fully factorized Gaussian posterior: we have a total of
(nin × nout) + nin + nout parameters, whereas a fully fac-
torized Gaussian posterior has 2(nin×nout) per layer. This
in turn makes the posterior uncertainty estimation easier as
there are both fewer parameters to learn and also “informa-
tion sharing” among the weights due to the induced corre-
lations.
With this diagonal approximation to the covariance matri-
ces the KL-divergence between the matrix variate Gaussian
posterior q(W|M, σ2
cI) and a standard isotropic ma-
trix variate Gaussian prior p(W|0, I, I) for a matrix of size
r × c corresponds to the following simple expression:

rI, σ2

KL(q(W|M, σ2

rI, σ2

(cid:32)(cid:18) r(cid:88)
(cid:18) r(cid:88)

i=1

− c

1
2

σ2
ri

(cid:19)(cid:18) c(cid:88)
cI)||p(W|0, I, I)) =
+ (cid:107)M(cid:107)2
(cid:19)

(cid:19)
(cid:18) c(cid:88)

σ2
cj

j=1

F − rc

(cid:19)(cid:33)

− r

log σ2
cj

log σ2
ri

(4)

i=1

j=1

The derivation for arbitrary covariance matrices is given in
the appendix.

2.3. Deep matrix variate Bayesian nets as deep

multi-output Gaussian Processes

Directly using the expected log-likelihood estimator 3
yields increased variance and higher memory requirements,
as it was pointed in (Kingma et al., 2015). Fortunately, sim-
ilarly to a standard multivariate Gaussian, the inner product
between a matrix and a matrix variate Gaussian is again a
matrix variate Gaussian (Gupta & Nagar, 1999) and as a re-
sult we can use the “local reparametrization trick” (Kingma
et al., 2015). Let AM×r, with M ≤ r, be a minibatch of
M inputs with dimension r that is the input to a network
layer; the inner product BM×c = AW, where W is a
matrix variate variable with size r × c, has the following
1Note that we could also easily use rank-1 matrices with diag-
onal corrections (Rezende et al., 2014) and increase the ﬂexibility
of our posterior. For example we could apply the rank-1 approxi-
mation to the square root of the covariance matrix (as we directly
use it for sampling), i.e. C
2 = Dc +uuT where Dc is a diagonal
matrix with positive elements.

1

distribution:

p(B|A) = MN (AM, AUAT , V)

(5)

As we can see, after the inner product the inputs A become
dependent due to the non-diagonal row covariance AUAT .
Furthermore, the resulting matrix variate Gaussian main-
tains the same marginalization properties as a multivariate
Gaussian. More speciﬁcally, if we marginalize out a row
from the B matrix, then the resulting distribution depends
only on the remaining inputs, i.e. it corresponds to simply
removing that particular input from the minibatch. This
fact exposes a Gaussian Process (Rasmussen, 2006) nature
for the output B of each layer.
To make the connection even clearer we can consider an
example similar to the one presented in (Gal & Ghahra-
mani, 2015). Let’s assume that we have a neural network
with one hidden layer and one output layer. Furthermore,
let X, with dimensions N × Dx, be the input to the net-
work and Y, with dimensions N × Dy, be the target vari-
able. Finally, let’s also assume that for the ﬁrst weight ma-
trix pθ1 (W1) = MN (0, U0
1) and that for the second
weight matrix pθ2 (W2) = MN (0, U0
2). Now we can
deﬁne the following generative model:

1, V0

2, V0

W1 ∼ MN (0, U0

1, V0

1); W2 ∼ MN (0, U0

2, V0
2)

B = XW1; F = ψ(B)W2
Y ∼ MN (F, τ−1IN , IDy )

each column of Y, i.e. p(Y|X) =(cid:81)Dy

where ψ(·) is a nonlinearity and MN (F, τ−1IN , IDy )
corresponds to an independent multivariate Gaussian over
i=1 N (yi|fi, τ−1IN ),
where fi is a column of F2. Now if we make use of the ma-
trix variate Gaussian property 5 we have that the generative
model becomes:

B|X ∼ MN (0, XU0
F|B ∼ MN (0, σ(B)U0
Y|F ∼ MN (F, τ−1IN , IDy )

1XT , V0
1)

2σ(B)T , V0
2)

or else equivalently:

vec (B)|X ∼ N (0, ˆKθ1(X, X))
vec (F)|B ∼ N (0, ˆKθ2(B, B))
vec (Y)|F ∼ N (vec(F), τ−1(IN ⊗ IDy ))

(cid:0)ψ(z1)Uψ(z2)T(cid:1)3. In other words, we have a composition

where ˆKθ(z1, z2) = Kout ⊗ Kin(z1, z2; U) = V ⊗

2Note that this is just a simplifying assumption and not a lim-
itation for our method. We could instead also model the correla-
tions among the output variables Y if we used a full covariance
CDy instead of IDy .

3ψ(·) is the identity function for the input layer.

Structured and Efﬁcient Variational Deep Learning with Matrix Gaussian Posteriors

of GPs where the covariance of each GP is governed by a
kernel function of a speciﬁc form; it is the kroneker product
of a global output and an input dependent kernel function,
where the latter is composed of ﬁxed dimension nonlin-
ear basis functions (the inputs to each layer) weighted by
their covariance. Essentially this kernel provides a distri-
bution for each layer that is similar to a (correlated) multi-
output GP, which was previously explored in the context
of shallow GPs (Yu et al., 2006; Bonilla et al., 2007a;b).
Therefore, in order to obtain the marginal likelihood of the
targets Y we have to marginalize over the function val-
ues B and F, which results into a deep GP (Damianou &
Lawrence, 2013) with the aforementioned kernel function
for each GP:
log p(Y|X) =

(cid:2)N (vec(F), τ−1(IN ⊗ IDy ))(cid:3)

log Epθ1 (B|X)pθ2 (F|B)

A similar scenario was also considered theoretically
in (Duvenaud et al., 2014).
Now in order to ob-
tain the posterior distribution of the parameters W we
will perform variational inference. We place a matrix
variate Gaussian posterior distribution over the weights
of
qφ1 (W1)qφ2(W2) =
MN (M1, U1, V1)MN (M2, U2, V2), and the marginal
likelihood lower bound in eq. 2 becomes:
L(φ1,2, θ1,2) ≤

the neural network,

i.e.

(cid:2) log p(Y|X, W1, W2)(cid:3)−

(cid:21)
Eqφ1 (W1)qφ2 (W2)
KL(qφi(Wi)||pθi(Wi))
(cid:20)
2(cid:88)

L(X,Y)(φ1, φ2) +

(cid:21)

(6)

Lc(φi, θi)

i=1

E ˜p(x,y)

(cid:20)
− 2(cid:88)

i=1

= E ˜p(x,y)

Noting that Y only depends on X, W1, W2 through
F = ψ(B)W2 = ψ(XW1)W2, and applying the
reparametrization trick, i.e.

qφ1 (W1)qφ2(W2) log p(Y|F(X, W1, W2))dW1,2 =
˜qφ1 (B|X)˜qφ2 (F|B) log p(Y|F)dBdF

(cid:90)
(cid:90)

where (using 5),

˜qφ1(B|X) = N (vec(µφ1(X)), ˆKφ1(X, X))
˜qφ2 (F|B) = N (vec(µφ2(B)), ˆKφ2(B, B))

where φ1 = (M1, U1, V1), φ2 = (M2, U2, V2) are the
variational parameters and µφ(z) = ψ(z)M is the mean
function. As we can see, ˜qφ1 (B|X), ˜qφ2 (F|B) can be con-
sidered as approximate posterior GP functions while the

local reparametrization trick provides the connection be-
tween the primal and dual GP view of the model. The vari-
ational objective thus becomes:
L(φ1,2, θ1,2) ≤ E ˜p(x,y)

(cid:2) log p(Y|F)(cid:3)+

E˜qφ1 (B|X)˜qφ2 (F|B)

(cid:20)
2(cid:88)

(cid:21)

+

Lc(φi, θi)

(7)

i=1

2.4. Efﬁcient sampling and pseudo-data

Sampling distribution 5 for every layer is however compu-
tationally intensive as we have to calculate the square root
of the row covariance Kin(A, A; U) = AUAT (which
has a cubic cost w.r.t.
the amount of datapoints in A)
every time. A simple solution is to only use its diago-
nal for sampling. This corresponds to samples from the
marginal distribution of each pre-activation latent variable
bi in the minibatch A. More speciﬁcally, we have that bi
follows a multivariate Gaussian distribution where the co-
variance is controlled by two sources: the local scalar row
variance (i.e. per datapoint feature correlations) and the
global column, i.e. pre-activation latent variable (or tar-
get variable in the case of the output layer), covariance:

p(bi|ai) = N (aiM,(cid:0)aiUaT

(cid:1) (cid:12) V).

i

Despite its simplicity however this approach does not use
the Gaussian Process nature of our model. In order to fully
utilize this property we adopt an idea from the GP litera-
ture: the concept of pseudo-data (Snelson & Ghahramani,
2005). More speciﬁcally, we introduce pseudo inputs ˜A
and pseudo outputs ˜B for each layer in the network and
sample the distribution of each pre-activation latent vari-
able bi conditioned on the pseudo-data:
p(bi|ai, ˜A, ˜B) =N
12Σ−1
12Σ−1

(cid:0) ˜B − ˜AM(cid:1),
(cid:19)
(cid:1) (cid:12) V

(cid:0)σ22 − σT

aiM + σT

11 σ12

(cid:18)

(8)

11

where each of the covariance terms can be estimated as:

Σ11 = ˜AU ˜AT ; σ12 = ˜AUaT

i ; σ22 = aiUaT
i

(9)

As can be seen, the pseudo-data directly affect the distribu-
tion of each pre-activation latent variable: if the inputs are
similar to the pseudo-inputs then the variance of the latent
variable bi decreases and the mean is shifted towards the
pseudo-data. This allows each layer in the network to be
more certain in particular regions of the input space. How-
ever, if the inputs are not similar to the pseudo-inputs then
the distribution of bi depends mostly on the parameters of
the underlying matrix variate Gaussian posterior.
It should be noted that the amount of pseudo-data for each
layer Np should be Np < D, where D is the dimensional-
ity of the input, as we are using a linear kernel for the row

Structured and Efﬁcient Variational Deep Learning with Matrix Gaussian Posteriors

covariance (that becomes non-linear via the neural network
nonlinearities) that has ﬁnite rank D. This enforces that
the pseudo-data combined with a real input ai provide a
positive deﬁnite kernel ˆK for the joint Gaussian output dis-
tribution p( ˜B, bi| ˜A, ai). Furthermore, we also ”dampen”
Σ11 by adding to it a small diagonal matrix σ2I where
σ2 = 1e−8. This corresponds to assuming ”noisy” pseudo-
observations ˜B (Rasmussen, 2006) (where the noise is i.i.d.
from N (0, σ2)) which helps avoiding numerical instabili-
ties during optimization (this is particularly helpful with
limited precision ﬂoating-point).
At ﬁrst glance it might seem that we now overparametrize
each neural network layer, however in practice this does
not seem to be the case. From our experience relatively
few pseudo-data per layer (compared to the input dimen-
sionality) are necessary for increased performance. This
still yields less parameters than fully factorized Gaussian
posteriors. In addition, note that with the pseudo data for-
mulation we could also assume that the weight posterior
has zero mean M = 0 (in GP parlance this corresponds to
removing the mean function); this would reduce the num-
ber of parameters even further and still provide a useful
model. This assumption leads to sampling the following
distribution:

(cid:18)

p(bi|ai, ˜A, ˜B) =
12Σ−1
σT

N

11

˜B,(cid:0)σ22 − σT

12Σ−1

11 σ12

(cid:1) (cid:12) V

(cid:19)

(10)

(cid:20)
2(cid:88)

(cid:21)

Finally, since we want a fully Bayesian model, we also
place fully factorized “dropout posteriors” on both ˜A and
˜B along with log-uniform priors, as it was described
in (Kingma et al., 2015). The ﬁnal form of the bound 7
with the inclusion of the pseudo-data is:
L(φ1,2, θ1,2) ≤
E ˜p(x,y)

(cid:2) log p(Y|F)(cid:3)+

E
qφ1 (B, ˜A1, ˜B1|X)qφ2 (F, ˜A2, ˜B2|B)

+

Lc(φi, θi)

(11)

i=1

where:

qφi(B, ˜A, ˜B|X) = ˜qφi(B|X, ˜A, ˜B)qφi( ˜A)qφi( ˜B)
Lc(φi, θi) = −KL(q(Wi)||p(Wi))−

− KL(q( ˜Ai)||p( ˜Ai)) − KL(q( ˜Bi)||p( ˜Bi))

where now φi, θi also include the parameters of the distri-
butions of the pseudo-data. The KL-divergence for these
can be found at (Kingma et al., 2015). We can thus read-
ily optimize the marginal likelihood lower bound of eq. 11
w.r.t. the parameters of the posterior and the pseudo data
with stochastic gradient ascent.

2.5. Computational complexity

A typical variational Bayesian neural network with a fully
factorized Gaussian posterior sampled “locally” (Kingma
et al., 2015) has asymptotic per-datapoint time complexity
O(D2) for the mean and variance in each layer, where D
is the input/output dimensionality. Our model adds the ex-
tra cost of inverting Σ−1
11 , that has cubic complexity with
respect to the amount of pseudo-data M for each layer.
Therefore the asymptotic time complexity is O(D2 + M 3)
and since usually M << D, this does not incur a signiﬁ-
cantly extra computational cost.

3. Related work
(Graves, 2011) ﬁrstly introduced a practical way of varia-
tional inference for neural networks. Despite the fact that
the proposed (biased) estimator had good performance on
a recurrent neural network task, it was not as effective
on the regression task of (Hern´andez-Lobato & Adams,
2015). (Blundell et al., 2015) proposed to use an alterna-
tive unbiased estimator that samples on the relatively high
variance weight space but nonetheless provided good per-
formance on a reinforcement learning task. (Kingma et al.,
2015) subsequently presented the “local reparametrization
trick”, which makes use of Gaussian properties so as to
sample in the function space, i.e.
the hidden units. This
provides both reduced memory requirements as well as re-
duced variance for the expected log-likelihood estimator.
However, for their model they still use a fully factorized
posterior distribution that doubles the amount of parame-
ters in each layer and does not allow the incorporation of
pseudo-data.
(Gal & Ghahramani, 2015) also provides connections be-
tween Bayesian neural networks and deep Gaussian pro-
cesses, but they only consider independent Gaussians for
each column of the weight matrix (which in our case cor-
respond to p(W) = MN (M, σ2I, I)) and do not model
the variances of the hidden units. Furthermore the approx-
imating variational distribution is quite limited as it corre-
sponds to simple Bernoulli noise and delta approximating
distributions for the weight matrix: it is a mixture of two
delta functions for each column of the weight matrix, one
at zero and the other at the mean of the Gaussian. This is
in contrast to our model where we can explicitly learn the
(possibly non-diagonal) covariance for both the input and
output dimensions of each layer through the matrix vari-
ate Gaussian posterior. In addition, sampling is done in the
weight space and not the function space as in our model,
thus preventing the use of pseudo-data.
Finally, (Hern´andez-Lobato & Adams, 2015) also assume
fully factorized posterior distributions and uses Expecta-
tion Propagation (Minka, 2001) instead of variational in-

Structured and Efﬁcient Variational Deep Learning with Matrix Gaussian Posteriors

ference. Closed form approximations bypass the need for
sampling in the model, which in turn makes it easier to
converge. However their derivation is limited to rectiﬁed
linear nonlinearities and regression problems, thus limiting
the applicability of their model. Furthermore, since each
datapoint is treated as new during the update of the param-
eters, special care has to be given so as to not perform a
lot of passes through the dataset since this will in general
shrink the variances of the weights of the network.

4. Experiments
All of the models were coded in Theano (Bergstra et al.,
2010) and optimization was done with Adam (Kingma &
Ba, 2015), using the default hyper-parameters and temporal
averaging. We parametrized the prior for each weight ma-
trix as p(W) = MN (0, I, I) unless stated otherwise. Fol-
lowing (Hern´andez-Lobato & Adams, 2015) we also divide
the input to each layer (both real and pseudo) by the square
root of its dimensionality so as to keep the scale of the out-
put (before the nonlinearity) independent of the incoming
connections. We used rectiﬁed linear units (Nair & Hin-
ton, 2010) (ReLU) and we initialized the mean of each ma-
trix variate Gaussian via the scheme proposed in (He et al.,
2015). For the initialization of the pseudo-data we sampled
the entries of ˜A, ˜B from U[−0.01, 0.01]. We used one pos-
terior sample to estimate the expected log-likelihood before
we update the parameters.
We test under two different scenarios: regression and clas-
siﬁcation. For the regression task we experimented with the
UCI (Asuncion & Newman, 2007) datasets that were used
in “Probabilistic Backpropagation” (PBP) (Hern´andez-
Lobato & Adams, 2015) and in “Dropout as a Bayesian
Approximation” (Gal & Ghahramani, 2015). For the clas-
siﬁcation task we evaluated our model on the permuta-
tion invariant MNIST benchmark dataset, so as to compare
against other popular neural network models.
Finally we also performed a toy regression experiment on
the same artiﬁcially generated data as (Hern´andez-Lobato
& Adams, 2015), so that we can similarly visualize the pre-
dictive distribution that our model provides.

4.1. Regression experiments

For the regression experiments we followed a similar ex-
perimental protocol with (Hern´andez-Lobato & Adams,
2015): we randomly keep 90% of the dataset for train-
ing and use the remaining to test the performance. This
process is repeated 20 times (except from the “Protein”
dataset where it is performed 5 times and the “Year” dataset
where it is performed once) and the average values along
with their standard errors are reported at Table 1. Follow-
ing (Hern´andez-Lobato & Adams, 2015) we also introduce

r I, τ−1

a Gamma prior, p(τ ) = Gam(a0 = 6, b0 = 6) and poste-
rior q(τ ) = Gam(a1, b1) for the precision of the Gaussian
likelihood and we parametrized the matrix variate Gaus-
sian prior for each layer as p(W) = MN (0, τ−1
c I),
where p(τr), p(τc) = Gam(a0 = 1, b0 = 0.5) and
q(τr)q(τc) = Gam(ar, br)Gam(ac, bc)4. We optimized
a1, b1, ar, br, ac, bc along with the remaining variational
parameters. We do not use a validation set5 and instead
train the networks up until convergence in the training set.
We use one hidden layer of 50 units for all of the datasets,
except for the larger “Protein” and “Year” datasets where
we use 100 units. We normalized the inputs x of the net-
work to zero mean and unit variance but we did not normal-
ize the targets y. Instead we parametrized the network out-
put as y = f (x)(cid:12) σy + µy where f (·) represents the neu-
ral network and µy, σy are the, per-dimension, mean and
standard deviation of the target variable, estimated from the
training set. Similarly to (Gal & Ghahramani, 2015) we set
the upper bound of the variational dropout rate to 0.005,
0.05 and we used 10 pseudo-data pairs for each layer for
all of the datasets, except for the smaller “Yacht” dataset
where we used 5 and the bigger “Protein” and “Year” where
we used 20.
As we can see from the results at Table 1 our model pro-
vides signiﬁcantly lower root mean square errors, com-
pared to VI (Graves, 2011), PBP (Hern´andez-Lobato &
Adams, 2015) and Dropout (Gal & Ghahramani, 2015), on
all of the datasets. In addition, we also observe better per-
formance according to the predictive log-likelihoods; our
model outperforms VI and PBP on all of the datasets and
is better than Dropout on 8 out of 10. These results empiri-
cally verify the effectiveness of our mdoel: with the matrix
variate Gaussian posteriors along with the Gaussian Pro-
cess interpretation we have a model that is very ﬂexible and
consequently can both better ﬁt the data, and, in the case of
the predictive log-likelihoods, make an accurate estimation
of the predictive uncertainty.

4.2. Classiﬁcation experiments

For the classiﬁcation experiments we trained networks with
a varying number of layers and hidden units per layer. We
used the last 10000 samples of the training set as a valida-
tion set for model selection, minibatches of 200 datapoints
and set the upper bound for the variational dropout rate to
0.25. We used the same amount of pseudo-data pairs for
each layer, but tuned those according to the validation set

4For

this

choice

of

distribution
r I, τ−1

Eq(τr )q(τc)[KL(q(W|M, U, V)||p(W|0, τ−1
and the KL-divergence between q(τr)q(τc) and p(τr)p(τc) can
be computed in closed form.

c I))]

5We hypothesize that the results could possibly be improved
even further if we tune the amount of pseudo-data and the upper
bound of the drop rate per layer according to a validation set.

both

Structured and Efﬁcient Variational Deep Learning with Matrix Gaussian Posteriors

Dataset
Boston
Concrete
Energy
Kin8nm
Naval
Pow. Plant
Protein
Wine
Yacht
Year

Avg. Test RMSE and Std. Errors

VI

VI

PBP

VMG

-2.54±0.08
4.32±0.29
-2.98±0.03
7.19±0.12
-1.45±0.03
2.65±0.08
1.14±0.01
0.10±0.00
5.84±0.00
0.01±0.00
-2.78±0.01
4.33±0.04
-2.84±0.00
4.84±0.03
-0.93±0.02
0.65±0.01
-1.29±0.02
6.89±0.67
9.034±NA 8.879±NA 8.849±NA 8.780±NA -3.622±NA -3.603±NA -3.588±NA -3.589±NA

Avg. Test LL and Std. Errors
Dropout
-2.46±0.25
-2.57± 0.09
-3.04±0.09
-3.16±0.02
-1.99±0.09
-2.04±0.02
0.90±0.01
0.95±0.03
3.80±0.05
3.73±0.01
-2.80±0.05
-2.84±0.01
-2.89±0.01
-2.97±0.00
-0.97±0.01
-0.93±0.06
-1.63±0.02
-1.55±0.12

Dropout
2.97±0.85
5.23± 0.53
1.66±0.19
0.10±0.00
0.01±0.00
4.02±0.18
4.36±0.04
0.62±0.04
1.11±0.38

-2.90±0.07
-3.39±0.02
-2.39±0.03
0.90±0.01
3.73±0.12
-2.89±0.01
-2.99±0.01
-0.98±0.01
-3.43±0.16

PBP

3.01±0.18
5.67±0.09
1.80±0.05
0.10±0.00
0.01±0.00
4.12±0.03
4.73±0.01
0.64±0.01
1.02±0.05

VMG

2.81±0.11
4.70±0.14
1.16±0.03
0.08±0.00
0.00±0.00
3.88±0.03
4.14±0.01
0.61±0.01
0.77±0.06

Table 1. Average test set RMSE, predictive log-likelihood and standard errors for the regression datasets. VI, PBP and Dropout corre-
spond to the variational inference method of (Graves, 2011), probabilistic backpropagation (Hern´andez-Lobato & Adams, 2015) and
dropout uncertainty (Gal & Ghahramani, 2015). VMG (Variational Matrix Gaussian) corresponds to the proposed model.

performance (we set an upper bound of 150 pseudo-data
pairs per layer). We did not use any kind of data augmen-
tation or preprocessing. The results can be seen at Table 2.

Method
Max. Likel. (Simard et al., 2003)
Dropout (Srivastava, 2013)
DropConnect (Wan et al., 2013)
Bayes B. SM (Blundell et al., 2015)

Var. Dropout (Kingma et al., 2015)

VMG

# layers Test err.
2×800
-
2×800
2×400
2×800
2×1200
3×150
3×250
3×500
3×750
2×400
3×150
3×250
3×500
3×750

1.60
1.25
1.20
1.36
1.34
1.32
≈ 1.42
≈ 1.28
≈ 1.18
≈ 1.09
1.15
1.18
1.11
1.08
1.05

Table 2. Test errors for the permutation invariant MNIST dataset.
Bayes B. SM correspond to Bayes by Backprop with the scale
mixture prior and the variational dropout results are from the Vari-
ational (A) model that doesn’t downscale the KL-divergence (so
as to keep the comparison fair).

As we can observe our Bayesian neural network performs
better than other popular neural networks models for small
network sizes. For example, with only three hidden layers
of 150 units it achieves 1.18% test error on MNIST, a re-
sult that is better than maximum likelihood (Simard et al.,
2003), Dropout (Srivastava, 2013), DropConnect (Wan
et al., 2013) and Bayes by Backprop (Blundell et al., 2015),
where all of the aforementioned methods have signiﬁcantly
bigger architectures than our model. Furthermore, it is also
signiﬁcantly better than a neural network of the same size
trained with variational dropout (Kingma et al., 2015). We

can probably attribute this effect to the Gaussian Process
property; for regular neural networks a small network size
means that there are not enough parameters to learn an ef-
fective classiﬁer. This is in contrast to our model where
through the learned pseudo-data we can maintain this prop-
erty and consequently increase the ﬂexibility of the model
and compensate for the lack of network size.

4.3. Toy experiment

In order to visually access the quality of the uncertainty that
our model provides, we also performed an experiment on
the simple toy dataset that was used in (Hern´andez-Lobato
& Adams, 2015). We sampled 20 inputs x from U[−4, 4]
n +n where
and parametrized the target variable as yn = x3
n ∼ N (0, 9). We then ﬁtted a neural network with Matrix
Gaussian posteriors (with diagonal covariance matrices), a
neural network that had a fully factorized Gaussian distri-
bution for the weights and a dropout network. All of the
networks had a single hidden layer of 100 units. For our
model We used two pseudo-data pairs for the input layer,
four for the output layer and set the upper bound of the vari-
ational dropout rate to 0.2. The dropout rate for the dropout
network was zero for the input layer and 0.2 for the hidden
layer. The resulting predictive distributions (after 200 sam-
ples) can be seen in ﬁgure 1 (with three standard deviations
around the mean).
As we can see the network with Matrix Gaussian pos-
teriors provides a realistic predictive distribution that
seems slightly better compared to the one obtained from
PBP (Hern´andez-Lobato & Adams, 2015).
Interestingly,
the simple fully factorized Gaussian (sampled with the “lo-
cal reparametrization trick”) neural network failed to ob-
tain a good ﬁt for the data as it was severely underﬁtting
due to the limited amount of datapoints. This resulted
into a very uncertain and noisy predictive distribution that
vaguely captured the mean function. This effect is not ob-

Structured and Efﬁcient Variational Deep Learning with Matrix Gaussian Posteriors

(a) Factorized Gaussian

(b) PBP

(c) Dropout

(d) Matrix Gaussian

Figure 1. Predictive distributions for the toy dataset. Grey areas
correspond to ±3 standard deviations around the mean function.

served with our model; with the Gaussian Process property
we effectively increase the ﬂexibility of our model thus al-
lowing the weight posterior to be closer to the prior with-
out severe loss in performance. Furthermore, we only have
a handful of variance parameters to learn, which conse-
quently provides easier and more robust posterior uncer-
tainty estimation. Finally, it seems that the dropout network
provides a predictive distribution that is slightly “overﬁt-
ted” as the conﬁdence intervals do not diverge as heavily in
areas where there are no data.

5. Conclusions
We introduce a scalable variational Bayesian neural net-
work where the parameters are governed by a probabil-
ity distribution over random matrices:
the matrix variate
Gaussian. By utilizing properties of this distribution we
can see that our model can be considered as a composition
of Gaussian Processes with nonlinear kernels of a speciﬁc
form. This kernel is formed from the kroneker product of
two separate kernels; a global output kernel and an input
speciﬁc kernel, where the latter is composed from ﬁxed di-
mension nonlinear basis functions (the inputs to each layer)
weighted by their covariance. We continue in exploiting
this duality and introduce pseudo input-output pairs for
each layer in the network, which in turn better maintain the
Gaussian Process properties of our model thus increasing
the ﬂexibility of the posterior distribution.
We tested our model in two scenarios:
the same regres-
sion task as PBP (Hern´andez-Lobato & Adams, 2015)
and Dropout uncertainty (Gal & Ghahramani, 2015) and
the benchmark permutation invariant MNIST classiﬁcation
task. For the regression task we found that our model over-

all achieves better RMSE and predictive log-likelihoods
than VI (Graves, 2011), PBP and Dropout uncertainty. For
the classiﬁcation task we found that our model provides
better errors than state of the art methods for small archi-
tectures. This demonstrates the effectiveness of the Gaus-
sian Process property; with the pseudo-data we increase
the ﬂexibility of our model thus countering the fact that we
have a limited capacity neural network.
Finally, we also empirically veriﬁed the quality of the pre-
dictive distribution that our model provides on the same toy
experiment as PBP (Hern´andez-Lobato & Adams, 2015).

Acknowledgements
We would like to thank anonymous reviewers for their feed-
back. This research is supported by TNO, Scyfer B.V.,
NWO, Google and Facebook.

References
Ahn, Sungjin, Balan, Anoop Korattikara, and Welling,
Max. Bayesian posterior sampling via stochastic gra-
In Proceedings of the 29th Inter-
dient ﬁsher scoring.
national Conference on Machine Learning, ICML 2012,
Edinburgh, Scotland, UK, June 26 - July 1, 2012, 2012.

Asuncion, Arthur and Newman, David. Uci machine learn-

ing repository, 2007.

Bergstra, James, Breuleux, Olivier, Bastien, Fr´ed´eric,
Lamblin, Pascal, Pascanu, Razvan, Desjardins, Guil-
laume, Turian, Joseph, Warde-Farley, David, and Ben-
gio, Yoshua. Theano: a cpu and gpu math expression
In Proceedings of the Python for scientiﬁc
compiler.
computing conference (SciPy), volume 4, pp. 3. Austin,
TX, 2010.

Blundell, Charles, Cornebise, Julien, Kavukcuoglu, Koray,
and Wierstra, Daan. Weight uncertainty in neural net-
works. Proceedings of the 32nd International Confer-
ence on Machine Learning, ICML 2015, Lille, France,
6-11 July 2015, 2015.

Bonilla, Edwin V, Agakov, Felix V, and Williams, Christo-
pher. Kernel multi-task learning using task-speciﬁc fea-
tures. In International Conference on Artiﬁcial Intelli-
gence and Statistics, pp. 43–50, 2007a.

Bonilla, Edwin V, Chai, Kian M, and Williams, Christo-
In Ad-
pher. Multi-task gaussian process prediction.
vances in neural information processing systems, pp.
153–160, 2007b.

Damianou, Andreas C. and Lawrence, Neil D. Deep gaus-
sian processes. In Proceedings of the Sixteenth Interna-
tional Conference on Artiﬁcial Intelligence and Statis-

Structured and Efﬁcient Variational Deep Learning with Matrix Gaussian Posteriors

tics, AISTATS 2013, Scottsdale, AZ, USA, April 29 - May
1, 2013, pp. 207–215, 2013.

Neal, Radford M. Bayesian learning for neural networks,
volume 118. Springer Science & Business Media, 2012.

Osband, Ian, Blundell, Charles, Pritzel, Alexander, and
Van Roy, Benjamin. Deep exploration via bootstrapped
dqn. arXiv preprint arXiv:1602.04621, 2016.

Rasmussen, Carl Edward. Gaussian processes for machine

learning. 2006.

Rezende, Danilo Jimenez, Mohamed, Shakir, and Wierstra,
Daan. Stochastic backpropagation and approximate in-
In Proceedings of
ference in deep generative models.
the 31th International Conference on Machine Learn-
ing, ICML 2014, Beijing, China, 21-26 June 2014, pp.
1278–1286, 2014.

Simard, Patrice Y, Steinkraus, Dave, and Platt, John C. Best
practices for convolutional neural networks applied to vi-
sual document analysis. pp. 958. IEEE, 2003.

Snelson, Edward and Ghahramani, Zoubin. Sparse gaus-
In Advances in
sian processes using pseudo-inputs.
neural information processing systems, pp. 1257–1264,
2005.

Srivastava, Nitish.

dropout, 2013.

Improving neural networks with

Wan, Li, Zeiler, Matthew, Zhang, Sixin, Cun, Yann L, and
Fergus, Rob. Regularization of neural networks using
In Proceedings of the 30th International
dropconnect.
Conference on Machine Learning (ICML-13), pp. 1058–
1066, 2013.

Welling, Max and Teh, Yee W. Bayesian learning via
stochastic gradient langevin dynamics. In Proceedings
of the 28th International Conference on Machine Learn-
ing (ICML-11), pp. 681–688, 2011.

Yu, Kai, Chu, Wei, Yu, Shipeng, Tresp, Volker, and Xu,
Zhao. Stochastic relational models for discriminative
link prediction. In Advances in neural information pro-
cessing systems, pp. 1553–1560, 2006.

Duvenaud, David K., Rippel, Oren, Adams, Ryan P., and
Ghahramani, Zoubin. Avoiding pathologies in very deep
In Proceedings of the Seventeenth Interna-
networks.
tional Conference on Artiﬁcial Intelligence and Statis-
tics, AISTATS 2014, Reykjavik, Iceland, April 22-25,
2014, pp. 202–210, 2014.

Gal, Yarin and Ghahramani, Zoubin. Dropout as a bayesian
approximation: Representing model uncertainty in deep
learning. arXiv preprint arXiv:1506.02142, 2015.

Graves, Alex. Practical variational inference for neural net-
In Advances in Neural Information Processing

works.
Systems, pp. 2348–2356, 2011.

Gupta, Arjun K and Nagar, Daya K. Matrix variate distri-

butions, volume 104. CRC Press, 1999.

He, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, and Sun,
Jian. Delving deep into rectiﬁers: Surpassing human-
In Pro-
level performance on imagenet classiﬁcation.
ceedings of the IEEE International Conference on Com-
puter Vision, pp. 1026–1034, 2015.

Hern´andez-Lobato,

Jos´e Miguel and Adams, Ryan.
Probabilistic backpropagation for scalable learning of
In Proceedings of the 32nd
bayesian neural networks.
International Conference on Machine Learning, ICML
2015, Lille, France, 6-11 July 2015, pp. 1861–1869,
2015.

Kingma, Diederik and Ba, Jimmy. Adam: A method for
International Conference on

stochastic optimization.
Learning Representations (ICLR), San Diego, 2015.

Kingma, Diederik P and Welling, Max. Auto-encoding
International Conference on Learn-

variational bayes.
ing Representations (ICLR), 2014.

Kingma, Diederik P, Salimans, Tim, and Welling, Max.
Variational dropout and the local reparametrization trick.
Advances in Neural Information Processing Systems,
2015.

Minka, Thomas P. Expectation propagation for approx-
In Proceedings of the Sev-
imate bayesian inference.
enteenth conference on Uncertainty in artiﬁcial intelli-
gence, pp. 362–369. Morgan Kaufmann Publishers Inc.,
2001.

Nair, Vinod and Hinton, Geoffrey E. Rectiﬁed linear units
improve restricted boltzmann machines. In Proceedings
of the 27th International Conference on Machine Learn-
ing (ICML-10), pp. 807–814, 2010.

Structured and Efﬁcient Variational Deep Learning with Matrix Gaussian Posteriors

A. KL divergence between matrix variate

Gaussian prior and posterior

Let MN 0(M0, U0, V0) and MN 1(M1, U1, V1) be two
matrix variate Gaussian distributions for random matrices
of size n × p. We can use the fact that the matrix variate
Gaussian is a multivariate Gaussian if we ﬂatten the matrix,
i.e. MN 0(M0, U0, V0) = N0(vec(M0), V0 ⊗ U0), and
as a result use the KL-divergence between two multivariate
Gaussians:

KL(N0||N1) =

1
2

tr(Σ−1

(cid:18)
1 Σ0)+
(cid:19)
1 (µ1 − µ0)−
+ (µ1 − µ0)T Σ−1
(cid:18)
− K + log
+(cid:0) vec(M1) − vec(M0)(cid:1)T
(cid:0)V1 ⊗ U1
(cid:19)

tr(cid:0)(V1 ⊗ U1)−1(V0 ⊗ U0)(cid:1)+
(cid:1)−1(cid:0) vec(M1) − vec(M0)(cid:1)−

|Σ1|
|Σ0|

1
2

=

− np + log

|V1 ⊗ U1|
|V0 ⊗ U0|

Now to compute each term in the KL efﬁciently we need
to use some properties of the vectorization and Kronecker
product:

= tr(U−1

1 ⊗ U−1
1 V0) ⊗ (U−1
1 U0) tr(V−1
1 V0)

ta = tr(cid:0)(V1 ⊗ U1)−1(V0 ⊗ U0)(cid:1)
1 )(V0 ⊗ U0)(cid:1)
= tr(cid:0)(V−1
= tr(cid:0)(V−1
1 U0)(cid:1)
(cid:1)−1
tb =(cid:0) vec(M1) − vec(M0)(cid:1)T(cid:0)V1 ⊗ U1
(cid:0) vec(M1) − vec(M0)(cid:1)
= tr(cid:0)(M1 − M0)T U−1

= vec(M1 − M0)T (V−1
= vec(M1 − M0)T vec(U−1

1 (M1 − M0)V−1

1 ⊗ U−1

1

1 ) vec(M1 − M0)

(cid:1)
1 (M1 − M0)V−1
1 )

So putting everything together we have that:

KL(MN 0,MN 1) =

1
2

(cid:18)
+ tr(cid:0)(M1 − M0)T U−1

tr(U−1

1 U0) tr(V−1
1 V0)+
1 (M1 − M0)V−1
(cid:19)

− np + p log |U1| + n log |V1|−
− p log |U0| − n log |V0|

(15)

1

(cid:1)−

B. Different toy dataset
We also performed an experiment with a different toy
dataset that was employed in (Osband et al., 2016). We
generated 12 inputs from U [0, 0.6] and 8 inputs from
U [0.8, 1]. We then transform those inputs via:

yi = xi + i + sin(4(xi + i)) + sin(13(xi + i))

where i ∼ N (0, 0.0009). We continued in ﬁtting
four neural networks that had two hidden-layers with 50
units each. The ﬁrst was trained with probabilistic back-
propagation (Hern´andez-Lobato & Adams, 2015), and the
remaining three with our model while varying the nonlin-
earities among the layers: we used ReLU, cosine and hy-
perbolic tangent activations. For our model we set the up-
per bound of the variational dropout rate to 0.2 and we used
2 pseudo data pairs for the input layer and 4 for the rest.
The resulting predictive distributions can be seen at Fig-
ure 2.

(12)

(a) PBP

(b) MG ReLU

tc = log

|V1 ⊗ U1|
|V0 ⊗ U0|
|U1|p|V1|n
|U0|p|V0|n

= log
= p log |U1| + n log |V1|−
− p log |U0| − n log |V0|

(c) MG cosine

(d) MG tanh

Figure 2. Predictive distributions for the toy dataset. Grey areas
correspond to ±{1, 2} standard deviations around the mean func-
tion.

(13)

(14)

