On the Whittle Index for Restless Multi-armed

Hidden Markov Bandits

Rahul Meshram and D. Manjunath

Aditya Gopalan

Deptt. of Elecl. Engg.

Deptt. of Elecl. Commun. Engg.

IIT Bombay, Mumbai INDIA

Indian Inst. of Science, Bangalore INDIA.

1

6
1
0
2

 
r
a

 

M
5
1

 
 
]

Y
S
.
s
c
[
 
 

1
v
9
3
7
4
0

.

3
0
6
1
:
v
i
X
r
a

Abstract—We consider a restless multi-armed bandit in which
each arm can be in one of two states. When an arm is sampled,
the state of the arm is not available to the sampler. Instead, a
binary signal with a known randomness that depends on the state
of the arm is made available. No signal is displayed if the arm
is not sampled. An arm-dependent reward is accrued from each
sampling. In each time step, each arm changes state according to
known transition probabilities which in turn depend on whether
the arm is sampled or not sampled. Since the state of the arm
is never visible and has to be inferred from the current belief
and a possible binary signal, we call this the hidden Markov
bandit. Our interest is in a policy to select the arm(s) in each
time step that maximizes the inﬁnite horizon discounted reward.
Speciﬁcally, we seek the use of Whittle’s index in selecting the
arms.

We ﬁrst analyze the single-armed bandit and show that
it admits an approximate threshold-type optimal policy when
the ‘no-sample’ action is subsidized. Next, we show that this
also satisﬁes an approximate-indexability property. Numerical
examples support the analytical results.

I. INTRODUCTION

A. Motivation

Opportunistic access in time-slotted multi-channel com-
munication systems for Gilbert-Elliot channels [1] is being
extensively studied. In the typical model there are N channels
and each channel can be in one of two states—a good state and
a bad state. Each channel independently evolves between these
two states according to a two-state Markov chain. The sender
can transmit on one of these N channels in each time slot. If
the selected channel is in the good state, then the transmission
is successful, and if it is in the bad state, it is unsuccessful.
The sender receives instantaneous error-free feedback about
the result of the transmission in both these cases. If the sender
knows the transition probabilities of the channels, then using
the feedback, it can calculate a ‘belief’ for the state of each
channel in a slot. This belief may be used to select the channel
in each slot to optimize a suitable reward function. This system
and its myriad variations have been studied as restless multi-
armed bandit (RMAB) problems.

Consider a system as above except that now the probability
of success in the good state and of failure in the bad state are
both less than one and the sender knows these probabilities.
This generalization of the Gilbert-Elliot channel means that
the sender does not get perfect information about the state of
the channel from the feedback. However, it can update its a
posteriori belief about the state of the channel based on the
feedback, and use this updated belief in the subsequent slot.

As a second motivating example, consider an advertisement
(ad) placement system (APS) for a user in a web browsing
session. Assume that the APS has to place one ad from M
candidate ads each of which has a known click-through proba-
bility and an expected reward determined from the user proﬁle.
It is conceivable that the click-through probabilities for ads in
a session depend on the history of the ads shown; users often
react differently depending upon the frequency with which
an ad is shown. Some users may, due to annoyance, respond
negatively to repeated display of an ad, which has the effect of
lowering the click-through probability if they were shown this
ad in the past. Others may convert disinterest to curiosity if an
ad is repeated thereby increasing the click-through probability.
Yet other users may be more random or oblivious to what has
been shown, and may behave independently of the history.

The effect of history on a user’s interest can be modeled
as follows. A state is associated with each ad and the state
changes at
the end of each session (the state intuitively
signiﬁes the interest level of the user in the ad). The transition
probabilities for this change of state depend on the whether the
ad is shown or not shown to the user in the session. Assume
that the state change behavior is independent of the past and of
the state change of the other ads. Each state is associated with
a value of click-through probability and expected revenue. The
state transition and the click-through probabilities determine
the ‘type’ or proﬁle of the user. In each session the APS only
observes a ‘signal’ or outcome (click or no-click) for the ad
that it displayed and no signal for those that are not displayed.
The action and the outcome is used to update its belief about
the current state of the user for each ad. The objective of the
APS would be to choose the ad in each session that optimizes
a long term objective. Clearly, this is also a RMAB with the
added generalization that the transition probabilities for the
arms depend on the action in that stage.

In this paper we analyze this generalization of the restless
multi-armed bandit—the states are never observed and the
transition probabilities depend in general on the action chosen.
To the best of our knowledge, such systems have not been
considered adequately in the literature.

B. Literature Overview

Restless multi-armed bandit are a special class of partially
observed Markov decisions processes (POMDPs) and are in
general PSPACE-hard [2] but many special cases have been
studied. Early work on restless bandit models for scheduling

in a dynamic spectrum access are in [3], [4]. In [3] one of
N Gilbert-Elliot channels have to be chosen for transmission
in each slot by an opportunistic transmitter. The objective is
to maximize the inﬁnite horizon discounted throughput. It is
shown that a policy based on the Whittle’s index is optimal. In
[4] multiple service classes are considered and the objective is
to maximize a utility function based on the queue occupancy of
the classes. The conditions for a myopic policy to be optimal
are derived. Myopic policies are also the subject of interest in
several other recent works, including [5]–[7]. Utility functions
are used in [8] that consider a system similar to that of [3].
Opportunistic spectrum access as POMDPs are studied in [9]–
[11].

Whittle’s index, ﬁrst proposed in [12], though not always
optimal has been widely used as a ‘nearly’ optimal policy
in many restless bandit problems [3], [13]–[17]. The key
step in determining if such a policy can be used in a given
RMAB is to analyze the one armed bandit as a POMDP and
prove that the arm is Whittle-indexable. These analyses borrow
signiﬁcantly from early work on POMDPs that model machine
repair problems like in [18]–[20]. We describe these next.

In [18], a machine is modeled as a two-state Markov chain
with three actions and it is shown that the optimal policy is
of the threshold type with three thresholds. In [20], a similar
model is considered and the formulas for the optimal costs and
the policy are obtained. This and some additional models are
considered in [19] and, once again, several structural results
are obtained. Also see [21] for more such models.

The key features in all of the above single-arm problems are
as follows. One or more of the actions provides the sampler
with exact information about the state of the Markov chain.
Furthermore, the transition probability of the state of the arms
does not depend on the action. These are also the features of
each of the arms of the RMAB models discussed earlier. In this
paper we consider a model that drops both these restrictions.
Since the state is never observed but only estimated from the
signals when the arm is sampled, our model can be called a
‘hidden Markov model restless multi-armed bandit.’ We note
that a rested hidden Markov bandit has been studied in [22],
where the state of an arm does not change if it is not sampled.
The (arguably simpler) information structure in a hidden rested
bandit admits an analytical solution via Gittins indices.

We describe the model details in the next section.

II. MODEL DESCRIPTION AND PRELIMINARIES

There are N independent arms of a restless multi-armed
bandit. Time is slotted and indexed by t. Each arm has two
states, 0 and 1. Let Xn(t) ∈ {0, 1} be the state of arm n at
the beginning of time t. Let An(t) ∈ {0, 1} denote the action
in slot t for arm n, i.e.,

An(t) =(1 Arm n is sampled in slot t,

0 Arm n is not sampled in slot t.

n=1 An(t) = 1 for all t, exactly one
arm is sampled in each slot. Arm n changes state at the end

We will assume that PN

2

(1 − λn,0)

λn,0

0

1

(1 − λn,1)

No Signal
Reward: ηn,2

λn,1

No Signal
Reward: ηn,2

Arm n is not sampled (An(t) = 0)

(1 − µn,0)

µn,0

0

1

(1 − µn,1)

Observe 1 w.p. ρn,0
Reward: ηn,0

µn,1

Observe 1 w.p. ρn,1
Reward: ηn,1

Arm n is sampled (An(t) = 1)

Fig. 1. Top: State transition probabilities,
the expected reward, and the
probability of binary signal 1 being observed when the arm is not sampled.
Bottom: The corresponding quantities when the arm is sampled

of each slot according to transition probabilities that depend
on An(t). Deﬁne the following transition probabilities.

Pr (Xn(t + 1) = 0|Xn(t) = 0, An(t) = 0) = λn,0
Pr (Xn(t + 1) = 0|Xn(t) = 1, An(t) = 0) = λn,1
Pr (Xn(t + 1) = 0|Xn(t) = 0, An(t) = 1) = µn,0
Pr (Xn(t + 1) = 0|Xn(t) = 1, An(t) = 1) = µn,1

In slot t, if arm n is in state i and it is sampled, then a
binary signal Zn(t) is observed and a reward Rn,i(t, 1) is
accrued (Note that the reward is not observed; only the signal
can be observed). If the arm is not sampled, then a reward
Rn,i(t, 0) is accrued and no signal is observed. Let

Pr (Zn(t) = 1 | Xn(t) = i) = ρn,i

and denote

Rn,i(t, 1) = ηn,i Rn,i(t, 0) = ηn,2.

Fig. 1 illustrates the model and the parameters.

In most applications, Zn(t) = 1 would correspond to a
‘good’ or favorable output e.g., a successful transmission or
click-through in the motivating examples. Hence, we will
make the reasonable assumption that ρn,0 < ρn,1 and ηn,0 <
ηn,1 for all n.
Remark 1:
• In the communication system example that maximizes
throughput, no reward is accrued if there is no transmis-
sion. Also, in the APS example, no revenue is accrued
if there is no ad displayed. Thus in both these cases,
ηn,2 = 0 is reasonable.

• Further, for communication over Gilbert-Elliot channels,

λn,i = µn,i for i = 0, 1.

2) If A(t) = 1 and Z(t) = 1 then

π(t + 1) = γ1(π(t)) :=

π(t)ρ0µ0 + (1 − π(t))ρ1µ1

π(t)ρ0 + (1 − π(t))ρ1

3

.

3) Finally, if A(t) = 0, i.e., the arm is not sampled at t,

then

π(t + 1) = γ2(π(t)) := π(t)λ0 + (1 − π(t))λ1.

The policy φ(t) : Ht → {0, 1} with 1 indicating sampling
the arm and 0 indicating not sampling the arm. The following
is well known [18], [24], [25]: (1) π(t) captures the infor-
mation in Ht, in the sense that it is a sufﬁcient statistic for
constructing policies depending on the history, (2) optimal
strategies can be restricted to stationary Markov policies,
and (3) the optimum objective or value function, V (π), is
determined by solving the following dynamic program

V (π) = max {ρ(π) + β (ρ(π)V (γ1(π)) + (1 − ρ(π))×

V (γ0(π))) , η2 + βV (γ2(π))} ,

(2)

where ρ(π) = πρ0 + (1 − π)ρ1.

Let π be the belief at the beginning of time slot t = 1.
Let VS(π) be the optimal value of the objective function if
A(1) = 1, i.e., if the arm is sampled, and VN S(π) be the
optimal value if A(1) = 0, i.e., if the arm is not sampled. We
can now write the following.

(1)

VS(π) = ρ(π) + β (ρ(π)V (γ1(π))

+(1 − ρ(π))V (γ0(π)))

We assume that λn,i, µn,i, and ρn,i are known. The sampler
cannot directly observe the state of the arm, and hence does
not know the state of the arms at the beginning of each time
slot. Instead, it can maintain the posterior or belief distribution
πn(t) that arm n is in state 0 given all past actions and ob-

servations, i.e., πn(t) = Pr(cid:0)Xn(t) = 0 | (An(s), Zn(s))t−1
s=1(cid:1),

and is assumed known at the beginning of slot t. Thus the
expected reward from sampling arm n is

πn(t)ηn,0 + (1 − πn(t))ηn,1
and that from not sampling the arm is ηn,2.

Deﬁne the vector π(t) = [π1, . . . , πN ] ∈ [0, 1]N . Let Ht
denote the history of actions and observed signals up to the be-
ginning of time slot t, i.e., Ht ≡ (An(s), Zn(s))1≤n≤N,1≤s<t.
In each slot, exactly one arm is to be sampled and let
φ = {φ(t)}t>0 be the sampling strategy with φ(t) deﬁned
as follows. φ(t) : Ht → {1, . . . , N } maps the history upto
time slot t to the action of sampling one of the N arms at
time slot t. Let

Aφ

n(t) =(1 if φ(t) = n,

0 if φ(t) 6= n.

The inﬁnite horizon expected discounted reward under sam-

pling policy φ is given by

Vφ(π) := E( ∞
Xt=1

βt−1  N
Xn=1

Aφ

n(t) (πn(t) ηn,0

+(1 − πn(t)) ηn,1) +(cid:0)1 − Aφ

n(t)(cid:1) ηn,2(cid:19)(cid:27).

Here β, 0 < β < 1, is the discount factor and the initial belief
is π, i.e.,Pr (Xn(1) = 0) = πn. Our interest is in a strategy
that maximizes Vφ(π) for all π ∈ [0, 1]N

We begin by analyzing the single arm bandit in the next
section. Before proceeding we state the following background
lemma derived from [23] that will be useful. The proof is
given in the Appendix for the sake of completeness.

Lemma 1 ( [23]): If f : ℜn

+ → ℜ+ is a convex function

then for x ∈ ℜn
function.

+, g(x) := ||x||1f(cid:16) x

||x||1(cid:17) is also a convex

Notation. For sets A and B, A \ B is used to denote all the

elements in A which are not in B.

III. APPROXIMATE THRESHOLD POLICY OF THE RESTLESS

SINGLE ARMED BANDIT WITH HIDDEN STATES

For notational convenience we will drop the subscript n in
the notation of the previous section. Further, we will assume
that η0 = ρ0 and η1 = ρ1. Thus η0 and η1 will be in
(0, 1) while there will be no restrictions on the range of η2.
Extending the results to the case of arbitrary η0, and η1 is
straightforward and is in Section V-B.

Recall that π(t) = Pr (X(t) = 0 | Ht) and we can use
Bayes theorem to obtain π(t + 1) from π(t), A(t) and Z(t)
as follows.

1) If A(t) = 1, i.e., the arm is sampled, and Z(t) = 0 then

VN S(π) = η2 + βV (γ2(π)),

V (π) = max{VS(π), VN S(π)}

(3)

Our ﬁrst objective is to describe the structure of the value
function of the single arm system as a function of two
variables—π the belief and η2 the reward for not sampling. We
begin by analyzing the structure of V (π, η2), VS(π, η2), and
VN S(π, η2) when one of π or η2 is ﬁxed. To keep the notation
simple, when the dependence on η2 is not made explicit it is
ﬁxed. The following is proved in the Appendix.

Lemma 2:
1) (Convexity of value functions over the belief state) For
ﬁxed η2, V (π), VN S(π) and VS(π) are all convex
functions of π.

2) (Convexity and monotonicity of value functions over
passive reward) For a ﬁxed π, V (π, η2), VS(π, η2), and
VN S(π, η2) are non-decreasing and convex in η2.

(cid:3)
Next we determine the range of η2 for which one of VS(π)
and VN S(π) dominates the other for all π ∈ [0, 1]. Clearly,
if η2 = ∞ then not sampling is the optimal action for all π.
This is also true for sufﬁciently large values of η2. For this
case, we can simplify (3) to obtain

V∞(π, η2) = VN S,∞(π, η2) =

η2

,

1 − β
βη2
1 − β

π(t + 1) = γ0(π(t))

:=

π(t)(1 − ρ0)µ0 + (1 − π(t))(1 − ρ1)µ1

π(t)(1 − ρ0) + (1 − π(t))(1 − ρ1)

VS,∞(π, η2) = πρ0 + (1 − π)ρ1 +

.

(4)

.

The subscript ∞ is used to indicate that η2 is large. Further,
VS,∞ is the value function when η2 is large and the arm is

sampled for one step and the optimal action of not sampling
is performed in all subsequent steps.

Similarly, if η2 = −∞ then sampling is the optimal action
for all π. Further, as we show in the Appendix, V−∞(π, η2)
has the following linear form.

V−∞(π, η2) = VS,−∞(π, η2) = mπ + c

m =

c =

ρ0 − ρ1

1 − β(µ0 − µ1)
ρ1 + βµ1(ρ0−ρ1)
1−β(µ0−µ1)
1 − β

VN S,−∞(π, η2) = η2 + βV (γ2(π), −∞)

= [βm(λ0 − λ1)] π + [η2 + β(c + λ1m)] .
(5)

As before, the subscript −∞ indicates that η2 is very small.
Further, VN S,−∞ is the value function when the arm is not
sampled in the ﬁrst step and the optimal action of sampling is
performed in all subsequent steps. We see that VN S,−∞(π, η2)
is also linear in π. Fig. 2 illustrates VS,∞, VN S,∞, VS,−∞, and
VS,−∞ for a sample set of parameters.

From the preceding we see that as η2 is decreased from ∞,
since ρ0 and η2 are bounded, for large values of η2, the optimal
action will be to not sample, i.e., VS(π, η2) < VN S(π, η2)
for all π and the value functions VS and VN S are as in (4).
Similarly, as η2 is increased from −∞, for large negative
values of η2, the optimal action is to sample, implying that
VS(π, η2) > VN S(π, η2) for all π, and VS and VN S would be
as in (5). This leads us to conclude the following.

Lemma 3: For a ﬁxed π, VS(π, η2) and VN S(π, η2) intersect
at least once. Thus we can deﬁne (1) ηL such that for all
η2 < ηL, sampling is the optimal policy for all π, and (2) ηH
such that for all η2 > ηH , not sampling is the optimal policy
for all π. Further,

ηH = ρ1

ηL = ρ1 −

(ρ1 − ρ0) [1 − β(λ0 − µ1)]

1 − β(µ0 − µ1)

.

(6)

(cid:3)

Eq. (6) is derived in the Appendix.
We are now ready to state the ﬁrst main result of this paper.

Theorem 1 (Approximately threshold-type optimal policies):
For a restless single-armed hidden Markov bandit with two
states, 0 < ρ0 < ρ1 < 1 and ηL ≤ η2 ≤ ηH , there exists
0 < β1 < 1 such that for all β ≤ β1, either (a) a threshold-type
optimal policy exists, i.e., there exists πT ∈ [0, 1] for which it
is optimal to sample at π ∈ [0, πT ] and to not sample at π ∈
(πT , 0], or (b) an approximately threshold-type optimal policy
exists, i.e., there exist ǫ > 0 and πT , π◦ ∈ [0, 1] with ρ(π◦) =
η2 such that an optimal policy samples at π ∈ [0, πT ] \ (π◦ −
ǫ, π◦ + ǫ) and does not sample at π ∈ (πT , 1]\ (π◦ − ǫ, π◦ + ǫ).

Remark. The result essentially states that, upto a suitable
discount factor 0 < β < β1, an optimal policy has a threshold-
structure at all belief states [0, 1], except possibly within a
small “hole” of width ǫ punched out around the point π◦.

4

Proof: Deﬁne the intervals S1 and S2 as follows.

S1 = {π : π ∈ [0, 1] : η2 < ρ(π)}
S2 = {π : π ∈ [0, 1] : η2 ≥ ρ(π)}

In the following we will use the subscript β to make the
dependence of VS, VN S and V on β explicit. For notational
convenience, deﬁne

Va,β(π, η2)

:= [ρ(π)Vβ (γ1(π), η2) +

(1 − ρ(π))Vβ (γ0(π), η2)] .

From (3), we see that βVa,β(π, η2) is the second term for the
expression for VS,β(π, η2). From Lemma 3, we know that for
all β, VS(π) and VN S(π) intersect at least once in [0, 1] when
ηH ≤ η2 ≤ ηL. This leads us to deﬁne

πT (η2) := inf {π ∈ [0, 1] : VS,β(π, η2) = VN S,β(π, η2)}

Clearly, Vβ (π, η2) and Va,β(π, η2) are bounded for all π ∈
[0, 1]. This follows from ρ0, ρ1, and η2 being bounded and
0 < β < 1, Further, in the Appendix we show that for ﬁxed
π and η2, Vβ(π, η2) is an increasing function of β.

For every belief state π ∈ [0, 1] satisfying ρ(π) 6= η2, let us

deﬁne β1(π) as

β1(π)

:= sup(cid:26)β ∈ (0, 1) :

|η2 − ρ(π)|

β

> |Vβ(γ2(π)) − Va,β(π)|(cid:27).(7)

Such a β1(π) exists in1 (0, 1] because, as we have argued
previously, the difference between V and Va is bounded, and
moreover, |η2 − ρ(π)| > 0; thus, there must exist β ∈ (0, 1)
satisfying the inequality above. Now put, for ǫ ≥ 0,

Cǫ := {π ∈ [0, 1] : |ρ(π) − η2| ≥ ǫ} ,

and deﬁne

β1,ǫ := inf {β1(π) : π ∈ Cǫ} .

It follows that β1,ǫ is ﬁnite (i.e., the set Cǫ is nonempty)

whenever either

1) η2 /∈ {ρ(π) : π ∈ [0, 1]}. In this case we will have a
(perfect) threshold-type optimal policy by taking ǫ = 0
⇒ Cǫ = [0, 1] as will follow below.

2) η2 ∈ {ρ(π) : π ∈ [0, 1]} and ǫ < max{π◦, 1 − π◦}
with ρ(π◦) = η2 (note that in this case S1 = [0, π◦)
and S2 = [π◦, 1]). In this case, by taking any 0 <
ǫ < max{π◦, 1 − π◦}, we will have an approximate
threshold-type optimal policy as will follow below.

We now claim that for any ǫ for which β1,ǫ is ﬁnite, and
any β < β1,ǫ, the optimal policy chooses to sample in the
region S1 ∩ Cǫ and not to sample in the region S2 ∩ Cǫ.

First, for π ∈ S1 ∩ Cǫ, VS,β(π, η2) > VN S,β(π, η2). To see

this, write

VS,β(π, η2) − VN S,β(π, η2) = (ρ(π) − η2)

−β (Vβ (γ2(π), η2) − Va,β(π, η2)) .

1We follow the standard convention that sup{x : x ∈ ∅} = −∞ (resp.
inf{x : x ∈ ∅} = +∞), where ∅ denotes the empty set, and in this case we
say that the supremum (resp. inﬁmum) does not exist or is not ﬁnite.

3

2.5

2

)
π
(
V

1.5

1

0.5

0

 
0

 

 (π, 2)

 (π, 1.5)

V

V

V

NS,∞

NS,∞

S,−∞

 (π, −1.5)

0.1

0.2

0.3

0.4

0.5

π

0.6

0.7

0.8

0.9

1

5

 

V

V

V

V

S, −∞

NS, −∞

(π, η
)
L
(π,η
)
L
)
H
(π,η

)
H

(π, η

S, ∞

NS, ∞

0.1

0.2

0.3

0.4

0.5
π

0.6

0.7

0.8

0.9

1

1.2

1

0.8

)
π
(
V

0.6

0.4

0.2

0

 
0

VN S,∞(π, η2) and VS,−∞(π, η2).

ηL and ηH .

Fig. 2. Illustrating (4) and (5) and the calculation of ηL and ηH . VS,−∞(π, η2) is a constant. As η2 is decreased from a large value, VN S,−∞(π, η2) shifts
downwards. Figure on the right shows that VN S,−∞(π, η2) and VS,−∞(π, η2) meet at π = 1 when η2 = ηL. Similarly, VN S,∞(π, η2) and VS,∞(π, η2)
meet at π = 0 when η2 = ηH . For both these ﬁgures, we have used η0 = ρ0 = 0.2, η1 = ρ1 = 0.8, λ0 = 0.2, λ1 = 0.7, µ0 = 0.3, µ1 = 0.8 and
β = 0.3.

For π ∈ S1, the term in the ﬁrst parentheses in the RHS above
is positive. We now consider two cases. If the term in the
second parentheses is negative, then the RHS is positive and
the claim holds. On the other hand, if the term is positive, then
from the deﬁnition of β1,ǫ, for all β < β1, the second term is
less than the ﬁrst and for this case too the claim follows.

On the other hand, for π ∈ S2 ∩ Cǫ, the claim follows by

observing that

Va,β(π, η2) − Vβ(γ2(π), η2) <

η2 − ρ(π)

β

.

whenever β < β1(π). Hence VS(π) < VN S(π) for β < β1,ǫ.
This completes the proof.

A. Numerical Examples

Theorem 1 introduces two approximations—an upper bound
on the discount factor, and a ‘hole’ in [0, π] where we do
not know the optimal policy. We believe that this is just an
artifact of the proof technique and that the restriction on β
and hole need not actually exist. To see this we conduct
extensive numerical experiments in which the value functions
were evaluated numerically using value iteration. Fig. 3 shows
the plots for VS(π) and VN S(π) for a sample set of µi, λi,
and ρi for different values of the discount factor β and η2
We see that in all the cases, there is just one threshold even
when β is very large. This leads us to believe that both the
approximations may not be needed, and to state the following
Conjecture 1 (Approximately threshold-type optimal poli-
cies): For a restless single-armed hidden Markov bandit with
two states with 0 < ρ0 < ρ1 < 1, a threshold-type optimal
policy exists, i.e., there exists πT ∈ [0, 1] for which it is
optimal
to sample at π ∈ [0, πT ] and to not sample at
π ∈ (πT , 0].

IV. APPROXIMATE INDEXABILITY OF THE RESTLESS

MULTI-ARMED BANDIT WITH HIDDEN STATES

We are now ready to analyze the multi-armed bandit. As we
have discussed in the introduction, ﬁnding the optimal policy

is, in general, a hard problem. A heuristic that is widely used in
optimally selecting the arm at each time step is due to Whittle
[12]. This heuristic is in general suboptimal but has a good
empirical performance and a large class of practical problems
use this policy because of its simplicity. In some cases, it can
also be shown to be optimal, e.g., [3]. The arm selection in
each time slot proceeds as follows. The belief vector π(t) is
used to calculate the Whittle’s index (deﬁned below) for each
arm and the arm with the highest index is sampled. To be
able to compute such an index for each arm, we ﬁrst need
to determine if the arm is indexable. Toward determining the
indexability, we ﬁrst deﬁne,

Pβ(η2)

:= {π ∈ [0, 1] : A∗
= {π ∈ [0, 1] : VS,β(π, η2) ≤ VN S,β(π, η2)}.

η2 (π) = 0}

For a given β, Pβ(η2) is the set of all π for which not sampling
is the optimal action. From [12], indexability of an arm is
deﬁned as follows.

Deﬁnition 1 (Indexability): An arm is indexable if Pβ(η2)
for the single-armed bandit process with reward η2 mono-
tonically increases from ∅ to the entire state space [0, 1] as
η2 increases from −∞ to ∞, i.e., Pβ(η(a)
2 ) = ∅
whenever η(a)
2 . A restless multi-armed bandit problem
is indexable if every arm is indexable.

2 ) \ Pβ(η(b)

2 ≤ η(b)

Deﬁnition 2 (Approximate indexability): For ǫ ≥ 0, an arm
is said to be ǫ-indexable for the single-armed bandit process
if, for η(a)
2 ) ⊆ [˜π−ǫ, ˜π+ǫ]
for some ˜π ∈ [0, 1].

2 , we have Pβ(η(a)

2 )\Pβ(η(b)

2 < η(b)

Next we deﬁne the Whittle index for an arm in state π.
Deﬁnition 3: If an indexable arm is in state π, its Whittle

index W (π) is

W (π) = inf
η2
= inf
η2

Pβ(η2)

{η2 : VS,β(π, η2) = VN S,β(π, η2)}.

6

 

V

(π)

S

V

NS

(π)

0.2

0.4

0.6

0.8

1

 

V

(π)

S

V

NS

(π)

0.2

0.4

0.6

0.8

1

1.8

1.7

1.6

1.5

)
π
(
V

1.4

1.3

1.2

1.1

1

 
0

π

π

β = 0.99; πT = 0.673.

η2 = 0.5; π◦ = 0.5.

β = 0.6; πT = 0.604.

 

V

(π)

S

V

NS

(π)

0.2

0.4

0.6

0.8

1

2.6

2.5

2.4

2.3

)
π
(
V

2.2

2.1

2

1.9

1.8

 
0

 

V

(π)

S

V

NS

(π)

0.2

0.4

0.6

0.8

1

π

π

β = 0.99; πT = 0.248.

η2 = 0.7; π◦ = 0.25.

β = 0.6; πT = 0.248.

 

V

(π)

S

V

NS

(π)

0.2

0.4

0.6

0.8

1

85.2

85.1

85

84.9

84.8

)
π
(
V

84.7

84.6

84.5

84.4

84.3

84.2

 
0

 

V

(π)

S

V

NS

(π)

0.2

0.4

0.6

0.8

1

51.85

51.8

51.75

51.7

51.65

)
π
(
V

51.6

51.55

51.5

51.45

51.4

51.35

 
0

70.3

70.2

70.1

70

)
π
(
V

69.9

69.8

69.7

69.6

69.5

 
0

)
π
(
V

2.3

2.2

2.1

2

1.9

1.8

1.7

1.6

1.5

1.4

1.3

 
0

π

π

β = 0.99 πT = 0.06

η2 = 0.85; π◦ = 0.0625.

β = 0.6; πT = 0.06.

Fig. 3. VN S (π), and VS(π) are plotted for different η2 and β. Observe the single threshold in all the cases. The threshold πT and the π◦ are also indicated
for each case. Here we have used ρ0 = η0 = 0.1, ρ1 = η1 = 0.9, µ0 = 0.1, µ1 = 0.9, λ0 = 0.9, and λ1 = 0.1.

W (π) is the minimum reward η2 such that optimal action
is to not sample an arm at that π. Our next objective is to
show that the arms in our problem are all indexable. From
Theorem 1, showing indexability requires us to show that
πT decreases monotonically as η2 increases. We need the
following lemma (proof is in the Appendix) to help make that
claim.

Lemma 4: Let πT (η2) = inf{0 ≤ π ≤ 1 : VS(π, η2) =

VN S(π, η2)} ∈ [0, 1]. If

exist) is also bounded. This means that we can ﬁnd β2 such
that for all 0 < β < β2, the relation (8) holds. We will also
require β to be in (0, β1) from Theorem 1.

Thus,

letting β3 = min{β1, β2}, we get

the ﬁrst
crossing point πT (η2) is monotone non-decreasing with η2. To
complete the proof, note that the only other states π > πT (η2)
at which the optimal action may play the no-sampling action
will
lie within an ǫ-radius hole around π◦, as shown in
Theorem 1. This establishes the conclusion of the theorem.

that

7

∂VS(π, η2)

∂η2

(cid:12)(cid:12)(cid:12)(cid:12)π=πT (η2)

<

∂VN S(π, η2)

∂η2

,

(8)

(cid:12)(cid:12)(cid:12)(cid:12)π=πT (η2)

then πT (η2) is a monotonically decreasing function of η2.
Thus showing that (8) holds for an arm implies indexability.
Remark 2: It is possible that Vβ(π, η2) is not differentiable
with respect to η2. In that case the partial derivative above
should be taken to be the right partial derivative. Note that
such a partial derivative exists because Vβ(π, η2) is bounded
and convex.

We are now ready to state and prove the second key result

of the paper, on the (approximate-)indexability of the arm.

Theorem 2: (Approximate Indexability of the bandit) For a
restless single-armed hidden Markov bandit with two states,
0 < ρ0 < ρ1 < 1 and ηL ≤ η2 ≤ ηH , there exists a β2,
0 < β2 < 1, and ǫ ≥ 0 such that for all β < β2 the arm is
ǫ-indexable.

Proof: Let πT (η2) = inf{0 ≤ π ≤ 1 : VS(π, η2) =

VN S(π, η2)} ∈ [0, 1].

Taking

the

partial

derivative

of VS,β(π, η2),

VN S,β(π, η2) with respect to η2 we obtain

∂VS,β(π, η2)

∂η2

= β(cid:20)ρ(π)

∂Vβ (γ1(π), η2)

+(1 − ρ(π))

∂η2
∂Vβ (γ0(π), η2)

∂η2

∂VN S,β(π, η2)

∂η2

= 1 + β

∂Vβ (γ2(π), η2)

∂η2

.

Taking (10) - (9), we obtain

1 + β

∂Vβ (γ2(π), η2)

∂η2

+(1 − r(π))

− β(cid:20)r(π)
(cid:21) .

∂η2

∂Vβ (γ0(π), η2)

∂Vβ (γ1(π), η2)

∂η2

We now show that the above is greater than 0 at π = πT (η2).
After rearranging the terms this requirement reduces to requir-
ing that

and

(cid:21) , (9)

(10)

A. Numerical Examples

Theorem 2 tells us that the restless multi-armed bandit with
hidden states is approximately indexable. Like in Theorem 1,
we believe that the approximation is just an artifact of the
proof technique and result is possibly more generally true and
also without the restriction on β. This is also borne out by
an extensive numerical study that we conducted. In Fig. 4
we show a sample plot of πT (η2), the threshold belief as a
function of the passive subsidy η2 for different β. We see that
πT increases with η2 leading us to believe that indexability is
more generally true.

To be useful in practice, we need to be able to compute
the index. A closed-form expression for either V (π), πT (η2),
or W (π) is hard. This will be discussed in more detail in
Section V-A. A simple solution would be to numerically
evaluate and precompute the W (π) at a suitable. We now
present some numerical evidence of the goodness of V (π)
as compared to a simple myopic policy. The myopic policy
would simply index the arms using [πn(t)η0 + (1 − πn(t))η1]
for arm n. This is the expected instantaneous payoff when the
arm is sampled in slot t.

We considered a 10-armed bandit with the following set of

parameters:

η0 = [0.1, 0.1, 0.2, 0.4, 0.2, 0.1, 0.3, 0.3, 0.35, 0.05]
η1 = [0.9, 0.95, 0.8, 0.9, 0.6, 0.5, 0.95, 0.7, 0.85, 0.5]
µ0 = [0.1, 0.9, 0.3, 0.9, 0.3, 0.9, 0.3, 0.8, 0.9, 0.5]
µ1 = [0.9, 0.1, 0.9, 0.3, 0.9, 0.3, 0.9, 0.3, 0.1, 0.02]
λ0 = [0.9, 0.9, 0.1, 0.1, 0.9, 0.9, 0.9, 0.8, 0.9, 0.5]
λ1 = [0.1, 0.1, 0.8, 0.8, 0.4, 0.3, 0.4, 0.3, 0.1, 0.02].

Further, ρ0 = η0, and ρ1 = η1. Initial belief

π(1) = [0.2, 0.4, 0.3, 0.7, 0.5, 0.8, 0.6, 0.1, 0.65, 0.35]T.

The arm with the highest index is chosen to be sampled in
each slot. Fig. 5 plots the value cumulative value function as
a function of time for the myopic policy and for the Whittle-
index based policy. Observe that the Whittle index based
policy is signiﬁcantly better for large β.

1
β

>((cid:20)ρ(π)

∂Vβ (γ1(π), η2)

∂η2

+(1 − ρ(π))

∂Vβ (γ0(π), η2)

∂η2

(cid:21)π=πT (η2)

−(cid:20) ∂Vβ (γ2(π), η2)

∂η2

(cid:21)π=πT (η2)).

Since Vβ(π, η2) is a bounded function for 0 < β < 1,
ηL < η2 < ηH , π ∈ [0, 1], partial derivative of Vβ(π, η2) with
respect to η2 (or the right derivative if the derivative does not

In this paper we are able to provide a structural property
through Theorems 1 and 2, but a obtain a closed-form expres-
sions for the value function V (π), the threshold πT (η2), or the

(11)

V. DISCUSSION AND CONCLUSION

A. Complications from Hidden States

2

)

η
(

T

π

1

0.8

0.6

0.4

0.2

0

 

 

β = 0.99

β =0.6

0.4

0.5

0.6

0.7

0.8

0.9

1

η 
2

8

 

β =0.99

β = 0.6

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

η
2

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

 

0
0.1

2

)

η
(

T

π

Fig. 4. πT (η2) is plotted for β = 0.6 and β = 0.99. The left plot is for the same set of same parameters as in Fig. 3 whereas the right plot uses
ρ0 = η0 = 0.1, ρ1 = η1 = 0.95, µ0 = λ0 = 0.9, and µ1 = λ1 = 0.1.

90

80

70

60

50

40

30

20

10

n
o

i
t
c
n
u

f
 

l

e
u
a
v
 
l

a
m

i
t

p
O

 

Myopic policy

Whittle index policy

0

 
0

100

200

300

Iteration

400

500

600

β = 0.99

n
o

i
t
c
n
u

f
 

l

e
u
a
v
 
l

a
m

i
t

p
O

2

1.8

1.6

1.4

1.2

1

0.8

 

 

Myopic policy

Whittle index policy

10

20

30

40

50

60

70

80

90

100

Iteration 

β = 0.6.

Fig. 5. The optimal value functions vs number of iterations with different discount factor β using Whittle index based policy and myopic policy

Whittle’s index W (π) have been elusive. We brieﬂy discuss
the complications that the hidden states of the arms that makes
it difﬁcult to obtain these quantities as compared to the other
extant models.

Most models in the literature assume that when an arm is
sampled, its state is correctly observed. In our model, this
means that when the arm is sampled, the binary signal could
just correspond to the state of the arm and have ρ0 = 0 and
ρ1 = 1. In this case, γ0(π) = µ0 and γ1(π) = µ1 both of
which are independent of π. Compare this with the γis for
our model that are non linear functions of π! Further, in the
models where the state is observed, we will have

policy like above will not work. A similar argument applies
if the arm is sampled and a 0 is observed.

While obtaining closed-form expressions appears to be hard
the following properties of the γs, obtained from ﬁrst and
second derivatives, may be useful in obtaining approximations.
We will not explore that in this paper.

Property 1:
1) If λ0 < λ1 then γ2(π) is linear decreasing in π. Further,

λ0 ≤ γ2(π) ≤ λ1.

2) If λ0 > λ1 then γ2(π) is linear increasing in π. Further,

λ1 ≤ γ2(π) ≤ λ0.

3) If µ0 > µ1 then γ1(π) is convex increasing in π. Further,

VS(π) = (1 − π) + β(1 − π)V (µ1) + βπV (µ0),

VN S(π) = η2 + βV (γ2(π)).

This means that VS(π) can be evaluated by evaluating V (π)
at two points. Further, the structure of the optimal policy will
be to continue to sample while the sampled arm is observed
to be in the good state. If the arm is sampled to be in the bad
state, then wait till π crosses πT before sampling again. The
number of slots to wait for this is easy to determine if πT is
known. In our case, if the arm is sampled and a binary 1 is
observed, the new π depends on the current value of π and a

µ1 ≤ γ1(π) ≤ µ0.

4) If µ0 > µ1 then γ0(π) is concave increasing in π.

Further, µ1 ≤ γ0(π) ≤ µ0.

5) γ0(0) = γ1(0) = µ1 and γ0(1) = γ1(1) = µ0. Further,

if µ0 > µ1 then γ1(π) < γ0(π) for 0 < π < 1.

2

(cid:3)

B. Generalizing η0 and η1

First, we discuss relaxing the assumptions on η0 and η1.
First consider η0 < η1. The basic conclusions remain the same

except that ηL and ηH will now be as follows.

B. Proof of Lemma 2

9

(η1 − η0)[1 − β(λ0 − µ1)]

1 − β(µ0 − µ1)

ηL =

ηH = η1

Also, in this case the slope and intercept of V−∞ will be

m =

c =

,

η0 − η1

1 − β(µ0 − µ1)
η1 + βµ1m

.

1 − β

For the case of η0 > η1, the ηL and ηH will be as follows.

ηL = η1 −

ηH = η0.

β(η1 − η0)(µ1 − λ1)]

1 − β(µ0 − µ1)

,

Further, in this case, the optimal action will be to sample the
arm when η2 is very small and to not sample when η2 is very
large. The equations for m, c, VS,∞, VN S,∞, VS,−∞, VN S,−∞,
are all identical to that in (4) and (5).

C. Future Directions

Several interesting prospects for future work are open. We
would of course like to know for sure if the single armed
bandit indeed has a single threshold sampling policy. As we
mention earlier, the complexity of the γis makes such a proof
hard and the ‘usual’ techniques that have been used in the
literature do not appear to be useful. The restriction on β in
the main results are in the same spirit as that of [26]. The
approximation is introduced here.

Since we do not have a closed-form expression for V (π) and
W (π) provably good approximations may be sought. Also,
since the Whittle index based policy is itself suboptimal, we
could seek other optimal policies that can provide guarantees
on the approximation to optimality.

A. Proof of Lemma 1

APPENDIX

Let x, y ∈ ℜn

+ and 0 ≤ α ≤ 1. Then we have the following.

g (αx + (1 − α)x)

= ||αx + (1 − α)y||1f(cid:18) αx + (1 − α)y
= ||αx + (1 − α)y||1f(cid:18)

||αx + (1 − α)y||1(cid:19)

||αx + (1 − α)y||1
y

(1 − α)||y||1

α||x||1

x

||x||1

||y||1(cid:19)

+

+

||αx + (1 − α)y||1

≤ ||αx + (1 − α)y||1(cid:20)

(1 − α)||y||1

||αx + (1 − α)y||1

α||x||1

||αx + (1 − α)y||1

f(cid:18) x

||x||1(cid:19)

f(cid:18) y

||y||1(cid:19)(cid:21)

= α||x||1f(cid:18) x

||x||1(cid:19) + (1 − α)||y||1f(cid:18) y

||y||1(cid:19)

= αg(x) + (1 − α)g(y)

The inequality in the ﬁfth line follows from convexity of f.

For part (1), we ﬁrst prove that V (π) is convex by induction
and use this to show that VS(π) and VN S(π) are also convex.
Let

V1(π) = max {πρ0 + (1 − π)ρ1, η2} ,

Vn+1(π) = max {η2 + βVn(γ2(π)), ρ(π) +

β [ρ(π)Vn(γ1(π)) + (1 − ρ(π))Vn(γ0(π))]} .(12)

Now deﬁne

b0

:= [πµ0(1 − ρ0) + (1 − π)µ1(1 − ρ1),

(1 − µ0)(1 − ρ0)π + (1 − π((1 − µ1)(1 − ρ1)]T ,

b1

:= [πµ0ρ0 + (1 − π)µ1ρ1,

(1 − µ0)ρ0π + (1 − π((1 − µ1)ρ1]T ,

(13)

and write (12) as
Vn+1(π) = ||b1||1 + β||b1||1Vn(cid:18) b1

||b1||1(cid:19) + β||b0||1Vn(cid:18) b0

||b0||1(cid:19) .

Here superscript T denotes the transpose. Clearly, V1(π) is
linear and hence convex. Making the induction hypothesis that
Vn(π) is convex in π, Vn+1(π) is convex from Lemma 1 and
by induction Vn(π) is convex for all n. From Chapter 7 of
[24] and Proposition 2.1 of Chapter 2 of [25], Vn(π) → V (π)
and hence V (π) is convex, Further,

V ′′
N S(π) = βV ′′(γ2(π)) (γ′

2(π))2

and hence VN S is also convex. Using the notation from (13),
we can write
VS(π) = ||b1||1 + β||b1||1V (cid:18) b1

||a1||b1(cid:19) + β||b0||1V (cid:18) b0

||b0||1(cid:19) .

The ﬁrst term in the RHS above is clearly convex in π. Since
V (π) is convex, from Lemma 1, the second and third terms
are also convex. Thus VS(π) is convex.

To prove the second part of the lemma we rewrite the

recursion of (12) as follows.

V1(π, η2) = max{ρ(π), η2}

Vn+1(π, η2) = max {η2 + βVn(γ2(π), η2), ρ(π)+

ρ(π)βVn(γ1(π), η2) + (1 − ρ(π)) βVn(γ0(π), η2)} (14)

Here we have made explicit the dependence of V (π) on η2. We
see that V1(π, η2) is monotone non decreasing and convex in
η2. Make the induction hypothesis that for a ﬁxed π, Vn(π, η2)
is monotone non decreasing and convex in η2. Then, in (14),
the ﬁrst term of the max function is the sum of two non
decreasing convex functions of η2. The second term is a
constant plus a convex sum of two non decreasing convex
functions of η2. Thus it is also non decreasing and convex in
η2. The max operation preserves convexity. Thus Vn+1(π, η2)
is also non decreasing and convex in η2 and by induction, all
Vn(π, η2) are non decreasing and convex in η2. As in the ﬁrst
part of the lemma, Vn(π, η2) → V (π, η2) and this completes
the proof for V (π). From (3), the assertion on VS(π) and
VN S(π) follows.

10

C. Derivation of Eqn. (5)

To show linearity of V (π, −∞) in π, consider the recur-

sion,

the coefﬁcient of π is always negative. Thus the minimum
value of the RHS of the preceding equation is achieved at
π = 1 corresponding to πL = 1 and ηL as in (6).

V0(π) = ρ(π),

Vn+1(π) = ρ(π) + ρ(π)βVn(γ1(π)) + (1 − ρ(π)) βVn(γ0(π)).

E. Proof that Vβ(π, η2) is increasing in β

Arguing like in the proof of Lemma 2 and making the
induction hypothesis that Vn(π) is linear in π, Vn+1(π) is also
linear in π. This is shown below by writing Vn(π) = ˜mπ + ˜c
and observing that the denominator of γ0(π) is (1 − ρ(π)) and
that of γ1(π) is ρ(π) and the numerators are both linear in π.

If βa > βb, we need to show that Vβa(π, η2) > Vβb (π, η2).

Like in earlier proofs, we use an induction argument. Let

VS,β,0(π, η2)
VN S,β,(π, η2)
Vβ,0(π, η2)

:= ρ(π),
:= η2,
:= max{VS,β,0(π, η2), VN S,β,0(π, η2)},

Vn+1(π) = ρ(π) (1 + β ( ˜m(γ1(π)) + ˜c))

and deﬁne

+β (1 − ρ(π)) ( ˜m(γ0(π)) + ˜c)

= ρ(π) + β ˜m (πρ0µ0 + (1 − π)ρ1µ1)

+β ˜m (π(1 − ρ0)µ0 + (1 − π)(1 − ρ1)µ1) + β˜c

The linearity of Vn(π) for all n follows from induc-
tion and since Vn(π) → V (π),
linearity of V (π) also
follows. The slope m and intercept c of V (π) is ob-
tained by writing V (π) = mπ + c in V (π) = ρ(π) +
β [ρ(π)V (γ1(π)) + (1 − ρ(π))γ0(π)] , i.e.,

mπ + c = ρ(π) + β (ρ(π)) (m(γ1(π)) + c)
+β (1 − ρ(π)) (m(γ0(π)) + c) .

and equating the coefﬁcients.

D. Derivation of Eqn. (6)

First, for every π ∈ [0, 1] deﬁne the set of all η2 for which

VS and VN S are equal.

VS,β,n+1(π, η2)

:= ρ(π) + β [ρ(π)Vβ,n(γ1(π), η2)+

VN S,β,n+1(π, η2)
Vβ,n+1(π, η2)

(1 − ρ(π))Vβ,n(γ0(π), η2)] ,

:= η2 + βVβ,n(γ2(π), η2),
:= max{VS,β,n(π, η2), VN S,β,n(π, η2)}.
(15)

Clearly, VS,β,1(π, η2), VN S,β,1(π, η2) and Vβ,1(π, η2) are

all increasing in β.

Now make the induction hypothesis that VS,β,n(π, η2),
VN S,β,n(π, η2) and by inspection of
(15) we see that
VS,β,n+1(π, η2), VN S,β,n+1(π, η2), and Vβ,n+1(π, η2) are all
increasing in β. Further, like in the proof Lemma 2, we know
that

Vn,S,β(π, r2) → VS,β(π, r2),

Vn,N S,β(π, r2) → VN S,β(π, r2),

Vn,β(π, r2) → Vβ (π, r2),

˜η2(π)

:= {η2 : VN S(π, η2) = VS(π, η2)},

and the claim follows.

Clearly, ηH and ηL can be deﬁned from ˜η2(π) as follows.

ηL := min
π∈[0,1]

˜η2(π),

ηH := max
π∈[0,1]

˜η2(π),

πL := arg min
π∈[0,1]

˜η2(π),

πH := arg max
π∈[0,1]

˜η2(π).

If η2 = ηH and π = πH , or if η2 = ηL and π = πL, then
both actions of sampling and not sampling are optimal. From
this we see that for a ﬁxed π, VS(π, η2) and VN S(π, η2) do
not intersect for η2 ∈ [−∞, ηL) or for η2 ∈ (ηH , ∞] for any
π ∈ [0, 1]. This in turn means that VS(π, η2) and VN S(π, η2)
intersect for η2 ∈ [ηL, ηH ] for every π ∈ [0, 1].

We ﬁrst obtain ηH and πH by equating VN S(π, ∞) and
VS(π, ∞) from (4) and (5). This gives us η2 = πρ0+(1−π)ρ1
which achieves its maximum at π = 0 corresponding to ηH =
ρ1 and πH = 0.

To obtain ηL and πL, we equate the VS(π, −∞) and

VN S(π, −∞) from Eqs. (4) and (5), to obtain
(ρ0 − ρ1)(1 − β(λ0 − λ1))

η2 =

1 − β(µ0 − µ1)
β(ρ1 − ρ0)(µ1 − λ1)

1 − β(µ0 − µ1)

.

ρ1 −

π +

F. Proof of Lemma 4

Proof: Frame the contrapositive, i.e., assume that πT (η2)
is not a monotonically decreasing function of η2 at ˆη2. It is
enough to show that

∂VS(π, η2)

∂η2

≥

∂VN S(π, η2)

∂η2

(cid:12)(cid:12)(cid:12)(cid:12)π=πT (ˆη2)

.

(cid:12)(cid:12)(cid:12)(cid:12)π=πT (ˆη2)

Suppose there exists a ˆη2 ∈ [ηL, ηH ] such that πT (η2) is
increasing at ˆη2 i.e., there exists a c > 0, such that for all
ǫ ∈ [0, c]

πT (ˆη2) < πT (ˆη2 + ǫ).

This implies that for all ǫ ∈ (0, c)

VS (πT (ˆη2), ˆη2 + ǫ) ≥ VN S (πT (ˆη2), ˆη2 + ǫ) .

(16)

Further, from the deﬁnition of πT (η2), Since πT (ˆη2), we also
have

VS (πT (ˆη2), ˆη2) = VN S (πT (ˆη2), ˆη2) .

(17)

Using (16) and (17) we can write the following.

Since 0 < λ0, λ1 < 1, |λ0 − λ1| < 1. Similarly, |µ0 − µ1| <
1. Hence, from our assumption that ρ0 < ρ1, and 0 < β < 1,

VS (πT (ˆη2), ˆη2 + ǫ) − VS (πT (ˆη2), ˆη2)
≥ VN S (πT (ˆη2), ˆη2 + ǫ) − VN S (πT (ˆη2), ˆη2) .

11

[21] J. S. Hughes, “A note on quality control under Markovian deterioration,”
Operations Research, vol. 28, no. 2, pp. 421–424, March-April. 1980.
[22] V. Krishnamurthy and R. J. Evans, “Hidden Markov model for multiarm
bandits: a methodology for beam scheduling in multitarget tracking,”
IEEE Transactions on Signal Processing, vol. 49, no. 12, pp. 2893–
2908, December 2001.

[23] K. J. Astrom, “Optimal control of Markov processes with incomplete
state information II. The convexity of loss function,” Mathematical
Analysis and Applications, vol. 26, no. 2, pp. 403–406, May 1969.

[24] D. P. Bertsekas, Dynamic Programming and Optimal Control, vol. 1,

Athena Scientiﬁc, Belmont, Massachusetts, 1st edition, 1995.

[25] D. P. Bertsekas, Dynamic Programming and Optimal Control, vol. 2,

Athena Scientiﬁc, Belmont, Massachusetts, 1st edition, 1995.

[26] C. C. White III, “Optimal control-limit strategies for a partially observed
replacement problem,” International Journal of System Science, vol. 10,
no. 3, pp. 321–331, 1979.

Dividing both sides of the above inequality by ǫ, taking limits
as ǫ → 0, and evaluating at π = πT (ˆη2) gives us

∂VS(π)

∂η2

This completes the proof.

(cid:12)(cid:12)(cid:12)(cid:12)π=πT (ˆη2)

≥

∂VN S(π)

∂η2

.

(cid:12)(cid:12)(cid:12)(cid:12)π=πT (ˆη2)

REFERENCES

[1] Edgar N Gilbert, “Capacity of a Burst-Noise Channel,” Bell system

technical journal, vol. 39, no. 5, pp. 1253–1265, 1960.

[2] C. H. Papadimitriou and J. H. Tsitsiklis, “The complexity of optimal
queueing network control,” Mathematics of Operations Research, vol.
24, no. 2, pp. 293–305, May 1999.

[3] K. Liu and Q. Zhao,

“Indexability of restless bandit problems and
optimality of Whittle index for dynamic multichannel access,” IEEE
Transactions Information Theory, vol. 56, no. 11, pp. 5557–5567,
November 2010.

[4] C. Lott and D. Teneketzis, “On the optimality of the index rule in multi-
channel allocation for single-hop mobile networks with multiple service
classes,” Probability in the Engineering and Information Sciences, vol.
14, pp. 259–297, 2010.

[5] Q. Zhao, B. Krishnamachari, and K. Liu, “On myopic sensing for multi-
channel opportunistic access: structure, optimality, and performance,”
IEEE Transactions on Wireless Communication, vol. 7, no. 12, pp. 5431–
5440, December 2008.

[6] K. Wang, L. Chen, and Q. Liu,

“On optimality of myopic sensing
policy with imperfect sensing in multi-channel opportunistic access,”
IEEE Transactions on Communications, vol. 61, no. 9, pp. 3854–3862,
September 2013.

[7] K. Wang, L. Chen, and Q. Liu, “On optimality of myopic policy for
opportunistic access with nonidentical channels and imperfect sensing,”
IEEE Transactions on Vehicular Technology, vol. 63, no. 5, pp. 2478–
2483, June 2014.

[8] W. Ouyang, A. Eyrilmaz, and N. Shroff,

“Asymptotically optimal
downlink scheduling over Markovian fading channels,” in Proceedings
of IEEE INFOCOM, 2012, pp. 1224–1232.

[9] Q. Zhao, L. Tong, A. Swami, and Y. Chen, “Decentralized cognitive
MAC for opportunistic spectrum access in ad hoc networks: A POMDP
framework,” IEEE Journal on Selected Areas in Communications, vol.
25, no. 3, pp. 589–600, April 2007.

[10] Y. Chen, Q. Zhao, and A. Swami, “Joint design and separation principle
for opportunistic spectrum access in the presence of sensing errors,”
IEEE Transactions on Information Theory, vol. 54, no. 5, pp. 2053–
2071, May 2008.

[11] C. Li and M. J. Neely, “Network utility maximization over partially
observable Markovian channels,” Performance Evaluation, vol. 70, no.
7–8, pp. 528–548, July 2013.

[12] P. Whittle, “Restless bandits: Activity allocation in a changing world,”

Journal of Applied Probability, vol. 25, no. A, pp. 287–298, 1988.

[13] M. H. Veatch and L. M. Wein, “Scheduling a make-to-stock queue:
Index policies and hedging points,” Operations Research, vol. 44, no.
4, pp. 634–647, July-August 1996.

[14] J. L. Ny, M. Dahleh, and E. Feron,

“Multi-UAV dynamic routing
with partial observations using restless bandit allocation indices,” in
Proceedings of American Control Conference (ACC 2008),, 2008, pp.
4220–4225.

[15] W. Ouyang, S. Murugesan, A. Eyrilmaz, and N. Shroff, “Exploiting
channel memory for joint estimation and scheduling in downlink net-
works,” in Proceedings of IEEE INFOCOM, 2011.

[16] K. Avrachenkov, U. Ayesta, J. Doncel, and P. Jacko, “Congestion control
of TCP ﬂows in Internet routers by means of index policy,” Computer
Networks, vol. 57, no. 17, pp. 3463–3478, 2013.

[17] K. Avrachenkov and V. S. Borkar, “Whittle index policy for crawling
ephemeral content,” Tech. Rep. Research Report No. 8702, INRIA,
2015.

[18] S. M. Ross, “Quality control under Markovian deterioration,” Manage-

ment Science, vol. 17, no. 9, pp. 587–596, May 1971.

[19] E. L. Sernik and S. I. Marcus, “On the computation of optimal cost
function for discrete time Markov models with partial observations,”
Annals of Operations Research, vol. 29, pp. 471–512, 1991.

[20] E. L. Sernik and S. I. Marcus,

“Optimal cost and policy for a
Markovian replacement problem,” Journal of Optimization Theory and
Applications, vol. 71, no. 1, pp. 403–406, Oct. 1991.

