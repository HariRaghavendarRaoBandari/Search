6
1
0
2

 
r
a

 

M
0
1

 
 
]
T
G
.
s
c
[
 
 

1
v
1
9
4
3
0

.

3
0
6
1
:
v
i
X
r
a

Bayesian Opponent Exploitation in Imperfect-Information Games

Sam Ganzfried

sam.ganzfried@gmail.com

Abstract

The two most fundamental problems in computational game theory are computing a Nash equilib-
rium and learning to exploit opponents given observations of their play (aka opponent exploitation). The
latter is perhaps even more important than the former: Nash equilibrium does not have a compelling the-
oretical justiﬁcation in game classes other than two-player zero-sum, and furthermore for all games one
can potentially do better by exploiting perceived weaknesses of the opponent than by following a static
equilibrium strategy throughout the match. The natural setting for opponent exploitation is the Bayesian
setting where we have a prior model that is integrated with observations to create a posterior opponent
model that we respond to. The most natural, and a well-studied prior distribution is the Dirichlet distribu-
tion. An exact polynomial-time algorithm is known for best-responding to the posterior distribution for
an opponent assuming a Dirichlet prior with multinomial sampling in the case of normal-form games;
however, for the case of imperfect-information games the best known algorithm is a sampling algorithm
based on approximating an inﬁnite integral without theoretical guarantees. The main result is the ﬁrst
exact algorithm for accomplishing this in imperfect-information games. We also present an algorithm
for another natural setting where the prior is the uniform distribution over a polyhedron.

1 Introduction

Imagine you are playing a game repeatedly against one or more opponents. What algorithm should you use
to maximize your performance? The classic “solution concept” in game theory is the Nash equilibrium. In a
Nash equilibrium σ, each player is simultaneously maximizing his payoff assuming the opponents all follow
their components of σ. So we should just ﬁnd a Nash equilibrium strategy for ourselves and play it in all the
game iterations, right?

Unfortunately, there are some complications. First, there can exist many Nash equilibria, and if the op-
ponents are not following the same one that we have found (or are not following one at all), then our strategy
would have no performance guarantees. Second, ﬁnding a Nash equilibrium is challenging computationally:
it is PPAD-hard and is widely conjectured that no polynomial-time algorithms exist [2]. These challenges
apply to both extensive-form games (of both perfect and imperfect information) and to normal-form games,
for games with more than two players and two-player non-zero-sum games. While a particular Nash equi-
librium may happen to perform well in practice,1 there is no theoretically compelling justiﬁcation for why
computing one and playing it repeatedly is a good approach. Two-player zero-sum games do not face these
challenges: there exist polynomial-time algorithms for computing an equilibrium [11], and there exists a
game value that is guaranteed in expectation in the worst case by all equilibrium strategies regardless of
the strategy played by the opponent (and this value is the best worst-case guaranteed payoff for any of our
strategies). However, even for this game class it would be desirable to deviate from equilibrium in order to
learn and exploit perceived weaknesses of the opponent; for instance, if the opponent has played Rock in
each of the ﬁrst thousand iterations of rock-paper-scissors, it seems desirable to put additional probability
mass on paper beyond the equilibrium value of 1
3 .

1An agent for three player limit Texas hold ’em poker computed using the counterfactual regret minimization algorithm (which
converges to Nash equilibrium in certain game classes) has been shown to perform well in practice despite a lack of theoretical
justiﬁcation [6].

1

Thus, learning to exploit opponents’ weaknesses is desirable in all game classes. One approach would
be to construct an opponent model consisting of a single mixed strategy that we believe the opponent is
playing given our observations of his play and a prior distribution (perhaps computed from a database of
historical play). This approach has been successfully applied to exploit weak agents in limit Texas hold
’em poker, a large imperfect-information game [4].2 A drawback of this approach is that it is potentially
not robust.
It is very unlikely that the opponent’s strategy matches this point estimate exactly, and we
could perform poorly if our model is incorrect. A more robust approach, which is the natural one to use
in this setting, is to use a Bayesian model, where the prior and posterior are full distributions over mixed
strategies of the opponent, not single mixed strategies. A natural prior distribution, which has been studied
and applied in this context previously, is the Dirichlet distribution. The pdf of the Dirichlet distribution
returns the belief that the probabilities of K rival events are xi given that each event has been observed
αi − 1 times: f (x, α) = 1
, and
that, assuming multinomial sampling, the posterior distribution after including new observations is also a
Dirichlet distribution with parameters updated based on the new observations.

.3 Some notable properties are that the mean is E[Xi] = αi(cid:80)

(cid:81) xαi−1

k αk

B(α)

i

Prior work has presented an efﬁcient algorithm for optimally exploiting an opponent in normal-form
games in the Bayesian setting with a Dirichlet prior [3]. The algorithm is essentially the ﬁctitious play
rule [1]. Given prior counts αi for each opponent action, the algorithm increments the counter for an action
by one each time it is observed, and then best responds to a model for the opponent where he plays each
strategy in proportion to the counters. This algorithm would also extend directly to sequential extensive-
form games of perfect information, where we maintain independent counters at each of the opponent’s
decision nodes; this would also work for games of imperfect information where the opponent’s private
information is observed after each round of play (so that we would know exactly what information set
he took the observed action from). For all of these game classes the algorithm would apply to both zero
and general-sum games, for any number of players. However, it would not apply to imperfect-information
games where the opponent’s private information is not observed after gameplay. An algorithm exists for
approximating a Bayesian best response in imperfect-information games, which uses importance sampling
to approximate the value of an inﬁnite integral. This algorithm has been successfully applied to limit Texas
hold ’em poker [15].4 However, it is only a heuristic approach with no theoretical performance guarantees.
In this paper, we present the ﬁrst algorithm that is provably optimal for this problem. The algorithm runs
in time polynomial in the number of the opponent’s information states. We also present an algorithm for
another natural prior distribution: the uniform distribution over a polyhedron.

2 Meta-Algorithm

At ﬁrst glance the problem of developing efﬁcient algorithms for optimizing against a posterior distribution,
which is a full probability distribution over mixed strategies for the opponent (which are themselves proba-
bility distributions over his pure strategies) seems daunting. We need to be able to both compactly represent
the posterior distribution and efﬁciently compute a best response to it. Fortunately, we show that our payoff
of playing any strategy σi against a probability distribution over mixed strategies for the opponent equals our
payoff of playing σi against the expectation of the distribution. Thus, we need only represent and respond
to the single strategy that is the expectation of the distribution, and not to the full distribution. While this
result was likely known previously, we have not seen it spelled out explicitly, and it is important enough to

2This approach used an approximate Nash equilibrium strategy as the prior and is applicable even when historical data is not

available, though if additional data were available a more informed prior that capitalizes on the data would be preferable.

3The normalizing constant B(α) is the beta function B(α) =
4In addition to Bayesian Best Response, the paper also considers approaches for approximating Max A Posteriori Response and

, where Γ(n) = (n − 1)! is the gamma function.

i αi)

(cid:81) Γ(αi)
Γ((cid:80)

Thompson’s Response.

2

be highlighted so that it is on the radar of the AI community.
pure strategy s−j ∈ S−j. Then, by deﬁnition of expected utility,

Suppose the opponent is playing mixed strategy σ−i, where σ−i(s−j) is the probability that he plays

ui(σi, σ−i) =

[σ−i(s−j) · ui(σi, s−j)]

We can generalize this naturally to the case where the opponent is playing according to a probability

distribution with pdf f−i over mixed strategies as follows:

(cid:88)

s−j∈S−j

(cid:90)

Let f−i denote the expectation of f−i. That is, f−i is the mixed strategy that selects action s−j with proba-
bility

ui(σi, f−i) =

σ−i∈Σ−i

[f−i(σ−i) · ui(σi, σ−i)]

(cid:90)

[σ−i(s−j) · f−i(σ−i)] .

Then we have the following result.

Theorem 1.

σ−i∈Σ−i

That is, the payoff against the expectation of a strategy distribution equals the payoff against the full distri-
bution.

ui(σi, f−i) = ui(σi, f−i).

Proof.

=

=

=

=

ui(σi, f−i)

(cid:88)
(cid:88)

s−j∈S−j

(cid:34)
(cid:34)(cid:90)
 (cid:88)

s−j∈S−j

σ−i∈Σ−i

(cid:90)
(cid:90)

σ−i∈Σ−i

σ−i∈Σ−i

j∈S−j
[ui(σi, σ−i) · f−i(σ−i)]

(cid:90)

σ−i∈Σ−i

ui(σi, s−j)

[σ−i(s−j) · f−i(σ−i)]

[ui(σi, s−j) · σ−i(s−j) · f−i(σ−i)]

[ui(σi, s−j) · σ−i(s−j) · f−i(σ−i)]

(cid:35)
(cid:35)



= ui(σi, f−i)

Theorem 1 applies to both normal-form and extensive-form games (with both perfect and imperfect
information), for any number of players (we could let σ−i be a joint strategy proﬁle for all agents besides
ourselves).
Now suppose the opponent is playing according a prior distribution p(σ−i), and let p(σ−i|x) denote
the posterior probability given observations x. Let p(σ−i|x) denote the expectation of p(σ−i|x). As an
immediate consequence of Theorem 1, we have the following corollary:
Corollary 1. ui(σi, p(σ−i|x)) = ui(σi, p(σ−i|x)).

3

Algorithm 1 Meta-algorithm for Bayesian opponent exploitation
Inputs: Prior distribution p0, response functions rt for 0 ≤ t ≤ T

M0 ← p0(σ−i)
R0 ← r0(M0)
Play according to R0
for t = 1 to T do

xt ← observations of opponent’s play at time step t
pt ← posterior distribution of opponent’s strategy given prior pt−1 and observations xt
Mt ← expectation of pt
Rt ← rt(Mt)
Play according to Rt

Corollary 1 implies the meta-procedure for optimizing against an opponent who plays according to p

given by Algorithm 1.

There are several challenges for applying Algorithm 1. First, the algorithm assumes that we can com-
pactly represent the prior and posterior distributions pt, which have an inﬁnite domain (the set of mixed
strategy proﬁles for the opponents). Second, it requires a procedure to efﬁciently compute the posterior
distributions given the prior and the observations, which will involve having to update potentially inﬁnitely
many strategies. Third, it requires an efﬁcient procedure to compute the expectation of pt. And fourth,
it requires that the full posterior distribution from one round be compactly represented to be used as the
prior distribution in the next round. We can address the fourth challenge by using the following modiﬁed pt
update step:

pt ← posterior distribution of opponent’s strategy

given prior p0 and observations x1, . . . , xt.

We will be using this new rule in our main algorithm.

The response functions rt could be a standard best response, for which linear time algorithms exist in
imperfect-information games, and a recent approach has enabled efﬁcient computation in extremely large
games [8].
It could also be a more robust form of a best response, e.g., one that places a limit on the
exploitability of our own strategy, perhaps one that varies over time depending on estimators for how much
we have won or lost [5, 7, 9]. In particular, the restricted Nash response has been demonstrated to outperform
a full best response against agents in limit Texas hold ’em poker, whose actual strategy may differ from the
exact model [9].

3 Exploitation Algorithm for Dirichlet Prior

As described in Section 1 the Dirichlet distribution is the conjugate prior for the multinomial distribution,
and therefore the posterior is also a Dirichlet distribution, with the parameters αi updated to reﬂect the new
observations. Thus, the expectation of the posterior can be computed efﬁciently by computing the strategy
for the opponent in which he plays each strategy in proportion to the updated weight, and Algorithm 1 yields
an exact efﬁcient algorithm for computing the Bayesian best response in normal-form games with a Dirichlet
prior. However, the algorithm does not apply to games of imperfect information since we do not observe the
private information held by the opponent, and therefore do not know which of his action counters we should
increment. In this section we will present a new algorithm for this setting. We ﬁrst present the algorithm
in the context of a representative motivating game in Section 3.1, and present the algorithm for the general
setting in Section 3.3.

4

3.1 Motivating game
Consider the following two-player game where we are player 2 and the opponent is player 1. Both players
ante $1, and then player 1 is dealt a King (K) and Jack (J) with probability 1
2, while player 2 is always dealt
a Queen (Q). Player 1 is then allowed to bet $1 (b) or fold (f), and player 2 is allowed to call or fold vs. a
bet. If player 1 folds, then player 2 wins the $2 pot; if player 1 bets and player 2 folds then player 1 wins
the $2 pot; if player 1 bets and player 2 calls then the player with the higher card wins the $4 pot. (The full
rules are presented for completeness and not needed for the analysis; all that is needed is the set of private
states and actions available to player 1).

Figure 1: Motivating game. Chance ﬁrst deals player 1 a king or jack with probability 1
2 each at the green
node. Then player 1 selects bet or fold at a red node. Then player 2 chooses to call or fold facing a bet at a
blue node.

If we observe player 1’s card after each hand, then we can apply the approach described above, where
we maintain a counter for player 1 choosing each action with each card that is incremented for the selected
action. However, if we do not observe player 1’s card after the hand (e.g., if he folds), then we would not
know whether to increment the counter for the king or the jack.

To simplify analysis, we will assume that we never observe the opponent’s private card after the hand
(which is not quite realistic since we would observe his card if he bets and we call); we can assume that
we do not observe our payoff either until all game iterations are complete, since that could allow us to draw
inferences about the opponent’s card. There are no known efﬁcient algorithms even for the simpliﬁed case
of fully unobservable opponent’s private information. We suspect that an algorithm for the case of partial
observability (when the opponent’s private information is sometimes observed and sometimes not) can be
constructed based on the algorithm we present here, and we plan to study this problem in future work.
Let C denote player 1’s card, and A denote his action. Then P (C = K) = P (C = J) = 1
2. Let
qb|K denote the probability that player 1 bets given a king: qb|K ≡ P r(A = b|C = K). If we were using
a Dirichlet prior with parameters α1 and α2 (where α1 − 1 is the number of times that action b has been
observed with a king, and α2 − 1 is the number of times f has been observed with a king), then

P r(qb|K) = Dir(qb|K; α1, α2) =

(qα1−1
b|K )(1 − qb|K)α2−1

B(α1, α2)

In general given observations O, Bayes’ rule gives the following, where q is a mixed strategy that is given
mass p(q) under the prior, and p(q|O) is the posterior:

p(q|O) =

P (O|q)p(q)

P (O)

5

(cid:80)
c∈C P (O, C = c|q)p(q)
(cid:80)
c P (O|C = c, q)p(c)p(q)

p(O)

=

=

p(O)

(P (O|K, q)p(K) + P (O|J, q)p(J))p(q)

=

=

p(O)

(P (O|K, q) + P (O|J, q))p(q)

2p(O)

qO|Kp(q) + qO|J p(q)

=

2p(O)

Assume that action b was observed in a new time step but player 1’s card was not observed. Let αKb − 1
be the number of times we have observed him play b with K according to the prior, and similarly for the
other parameters.

1

×
2B(αKb, αKf )B(αJb, αJf )p(O)
(1 − qb|J )αJf−1
b|J (1 − qb|J )αJf−1)]
(cid:90)
The general expression for the expectation of a continuous random variable is

P (q|O) =
b|K (1 − qb|K)αKf−1qαJb−1
[qαKb
+(qαKb−1
(1 − qb|K)αKf−1qαJb
b|K

(cid:90)

(cid:90)

(cid:90)

(cid:90)

b|J

E[A] =

aP (A = a)da =

a

p(A, B)dbda =

ap(A, B)dbda

a

a

b

a

b

Now let us compute the expectation of the posterior distribution of the opponent’s probability of playing b
with a jack.

P (A = b|O, C = J) =

P (A = b|O, C = J, qb|J )P (qb|J|O)dqb|J

(cid:90)

qb|J

(cid:90)
(cid:82)

=

=

(cid:82)
qb|J P (qb|J|O)dqb|J = E[qb|J|O]
qb|J P (q|O)dqb|Kdqb|J

qb|J

qb|K

p(O)

1

×

(cid:90)

=

(cid:90)

2B(αKb, αKf )B(αJb, αJf )p(O)
b|K (1 − qb|K)αKf−1qαJb
(qαKb

b|J (1 − qb|J )αJf−1

qb|J
+(qαKb−1

b|K

qb|K
(1 − qb|K)αKf−1qαJb+1

b|J

(1 − qb|J )αJf−1)dqb|Kdqb|J

B(αKb + 1, αKf )B(αJb + 1, αJf ) + B(αKb, αKf )B(αJb + 2, αJf )

=

2B(αKb, αKf )B(αJb, αJf )p(O)

(cid:90)

(cid:90)

The ﬁnal equation can be obtained by observing that
b|K (1 − qb|K)αKf−1qαJb
qαKb
B(αKb + 1, αKf )B(αJb + 1, αJf )

b|J (1 − qb|J )αJf−1

qb|J

qb|K

dqb|Kdqb|J

6

(cid:90)

=

qb|J

(cid:90)

b|J (1 − qb|J )αJf−1
qαJb
B(αJb + 1, bJ )

b|K (1 − qb|K)αKf−1
qαKb
B(αKb + 1, αKf )

dqb|Kdqb|J

(cid:90)

=

qb|J

qb|K

b|J (1 − qb|J )αJf−1
qαJb
B(αJb + 1, αJf )

1dqb|J = 1,

since the integrands are themselves Dirichlet distributions and all probability distributions integrate to 1.
Similarly

(cid:90) (cid:90) (qαKb−1

b|K

(1 − qb|K)αKf−1qαJb+1
B(αKb, αKf )B(αJb + 2, αJf )

b|J

(1 − qb|J )αJf−1)

= 1.

Letting Z denote the denominator, we have

P (b|O, J) =

B(αKb + 1, αKf )B(αJb + 1, αJf ) + B(αKb, αKf )B(αJb + 2, αJf )

Z

,

(1)

where the normalization term is

Z = B(αKb + 1, αKf )B(αJb + 1, αJf ) + B(αKb, αKf )B(αJb + 2, αJf )

+B(αKb, αKf + 1)B(αJb, αJf + 1) + B(αKb, αKf )B(αJb, αJf + 2)

P (f|O, J), P (b|O, K), and P (f|O, K) can be computed analogously. As stated earlier, B(α) =
where Γ(n) = (n − 1)!, which can be computed efﬁciently.

(cid:81) Γ(αi)
Γ((cid:80)

i αi)

Note that the algorithm we have presented applies for the case where we play one more game iteration
and collect one additional observation. However, it is problematic for the general case we are interested
in where we play many game iterations, since the posterior expression in Equation 1 is not Dirichlet, and
therefore we cannot just apply the same procedure in the next iteration using the computed posterior as
the new prior. We will need to derive a new expression for P (b|O, J) for this setting. Suppose that we
have observed the opponent play action b θb times and f θf times (in addition to the number of ﬁctitious
observations reﬂected in the prior α), though we do not observe his card. Note that there are θb + 1 possible
ways that he could have played b θb times: 0 times with K and θb times with J, 1 time with K and θb − 1
times with J, etc. Thus the expression for p(q|O) will have θb + 1 terms in it instead of two (we can view
θb + 1 as being a constant if we assume that the number of game iterations is a constant, but in any case this
is linear in the number of iterations).

We have the new equation

P (q|O) =

1

2B(αKb, αKf )B(αJb, αJf )p(O)

·

qαKb−1+i
b|K

(1 − qb|K)αKf−1+jqαJb−1+(θb−i)

b|J

(1 − qb|J )αJf−1+(θf−j)

θb(cid:88)

θf(cid:88)

i=0

j=0

Using similar reasoning as above, this gives

i

(cid:80)
(cid:80)
j [B(αKb + i, αKf + j)B(αJb + θb − i + 1, αJf + θf − j)]
(cid:88)

[B(αKb + i, αKf + j)B(αJb + θb − i + 1, αJf + θf − j)

Z

The normalization term is

P (b|O, J) =

(cid:88)

Z =

(2)

i

j

7

+B(αKb + i, αKf + j)B(αJb + θb − i, αJf + θf − j + 1)]

Thus the algorithm for responding to the opponent is the following. We start with the prior counters on
each private information and action combination, αKb, αKf , αJb, αJf . We keep separate counters θb, θf for
the number of times we have observed the actions b and f during the course of play. Then we combine these
counters according to Equation 2 in order to compute the strategy for the opponent that is the expectation of
the posterior given the prior and observations, and we best respond to this strategy, which gives us the same
payoff as best responding to the full posterior distribution according to Theorem 1.

3.2 Example
Suppose the prior is that the opponent played b with K 10 times, played f with K 3 times, played b with J 4
times, and played f with J 9 times. Thus αKb = 10, αKf = 3, αJb = 4, αJf = 9. Now suppose we observe
him play b at the next iteration. Applying our algorithm using Equation 1 gives us

p(b|O, J) =

B(11, 3)B(5, 9) + B(10, 3)(6, 9)

= 0.0011655 · 0.00015540 + 0.00151515 · 0.00005550 =

p(f|O, J) =

B(10, 4)B(4, 10) + B(10, 3)(4, 11)

= 0.00034965 · 0.00034965 + 0.00024975 · 0.00151515 =

2.65209525e−7

Z

5.00663835e−7

Z

Therefore

p(b|O, J) =

2.65209525e−7

2.65209525e−7 + 5.00663835e−7 = 0.3462837838.

So we think that with a jack he is playing a strategy that bets with probability 0.346 and folds with
probability 0.654. Notice that previously we thought his probability of betting with a jack was 4
13 = 0.308,
and had we been in the setting where we always observe his card after gameplay and observed that he had a
jack, the posterior probability would be 5

An alternative “na¨ıve” (and incorrect) approach would have been to increment the counter for αJb by
αJb
, the ratio of the prior probability that he bets given J to the total prior probability that he bets. This
14 = 0.306, which differs signiﬁcantly from

αJb+αKb
would give a posterior probability of him betting with J of 4+ 4
the correct value.

14

14 = 0.357.

Similarly we compute his strategy with a king:

Z

Z

Z

Z

p(b|O, K) =

B(5, 9)B(11, 3) + B(4, 9)(12, 3)

= 0.00015540 · 0.00116550 + 0.00050505 · 0.00091575 =

p(f|O, K) =

B(4, 10)B(10, 4) + B(4, 9)(10, 5)

= 0.00034965 · 0.00034965 + 0.00050505 · 0.00009990 =

6.43618238e−7

Z

1.72709618e−7

Z

p(b|O, K) =

6.43618238e−7

6.43618238e−7 + 1.72709618e−7 = 0.7884284676.

8

14 = 0.786, and 10+ 10

So we think he will bet with king with probability 0.788 and fold with probability 0.212. By comparison,
14 = 0.765 (so the “na¨ıve” incorrect approach would actually have re-
13 = 0.769, 11
10
duced the probability despite observing a bet). Interestingly, in this case the posterior probability is actually
higher than if we were in the fully observable setting and saw that he had a king and bet which would
correspond to the Dirichlet posterior.

14

3.3 General setting
We now consider the general setting where the opponent can have n different states of private information
according to an arbitrary distribution π and can take m different actions. Assume he is given private infor-
mation xi with probability πi, for i = 1, . . . , n, and can take action ki, for i = 1, . . . , m. Assume the prior is
Dirichlet with parameters αij for the number of times action j was played with private information i (so the
expectation of the prior has the player selecting action kj at information state xi with probability

αij(cid:80)

).

j αij

P r(C = xi) = πi

P r(A = kj|C = xi) = qkj|xi

(cid:81)

j qαij−1
kj|xi

B(αi1, . . . , αim)

P r(qkj|xi) = Dir(qkj|xi; αi1, . . . , aim) =

As before, using Bayes’ rule we have

=

=

P (O|q)p(q)

p(O)

P (O)

p(q|O) =

i P (O, C = xi|q)p(q)

(cid:80)
(cid:80)
i P (O|C = xi, q)πip(q)
p(q)(cid:80)
(cid:80)

p(O)
i P (O|xi, q)πi
p(O)

i [P (O|xi)p(q)πi]

=

p(O)

=

(cid:80)n

P (q|O) =

(cid:81)n
j=1 qαjh−1
kh|xj

(cid:105)

i=1 B(αi1, . . . , αim)

h=1

i=1

πiqkj∗|xi

(cid:81)m

(cid:104)
p(O)(cid:81)n
(cid:82)(cid:104)
(cid:104)
(cid:80)n
p(O)(cid:81)n
(cid:104)

j |x∗

qk∗

i=1

(cid:81)

i

πi

i

j B(γ1j, . . . , γnj)

(cid:105)

,

(cid:81)m

(cid:81)n
j=1 qαjh−1
kh|xj

(cid:105)(cid:105)

πiqkj∗|xi
i=1 B(αi1, . . . , αim)

h=1

Z

9

P (A = kj∗|O, C = xi∗) =

(cid:80)

=

Now assume that action kj∗ was observed in a new time step, while the opponent’s private information

was not observed.

We now compute the expectation for the posterior probability that the opponent plays kj∗ with private

information xi∗ as done in Section 3.1.

where γij = αij + 2 if i = i∗ and j = j∗, γij = αij + 1 if j = j∗ and i (cid:54)= i∗, and γij = αij otherwise. If

we denote the numerator by τi∗j∗ then Z =(cid:80)

i∗ τi∗j∗.

As in Section 3.1, we will need to generalize this to the case of multiple observed actions in order to
obtain an efﬁcient algorithm, because the posterior is not Dirichlet and can not be used directly as the prior
for the next iteration. Suppose we have observed action kj θj times (in addition to the number of ﬁctitious
times indicated by the prior counts αij). We compute P (q|O) analogously as
(cid:81)n
j=1 qαjh−1+ρjh

(cid:80)n

(cid:105)

πi

(cid:104)
(cid:80){ρab}(cid:81)m
p(O)(cid:81)n
where the(cid:80){ρab} is over all values 0 ≤ ρab ≤ θb with(cid:80)
θb−ρ1b(cid:88)
θb(cid:88)

We can write this as Σ1 . . . Σm where

P (q|O) =

i=1

a ρab = θb for each b, for 1 ≤ a ≤ n, 1 ≤ b ≤ m.
θb−(cid:80)n−1
r=0 ρrb(cid:88)

h=1

kh|xj
i=1 B(αi1, . . . , αim)

,

Σb =

. . .

.

ρ1b=0

ρ2b=0

ρnb=0

For each b, the number of terms equals the number of ways of distributing the θb observations amongst the
n possible private information states, which equals
C(θb + n − 1, n − 1) =

(θb + n − 1) . . . n

= O(nθb).

θb!

Since we must do this for each of the m actions, the total number of terms is O(nT m), where T is the
total number of game iterations which upper bounds the θb’s. So the number of terms is exponential in the
number of game iterations and the number of actions, but polynomial in the number of private information
states. The ﬁnal expression for P (A = kj∗|O, C = xi∗) is

P (A = kj∗|O, C = xi∗) =

h B(α1h + ρ1h, . . . , αnh + ρnh)

Z

(3)

4 Algorithm for Uniform Prior Distribution Over Polyhedron

Another natural prior that has been studied previously is the uniform distribution over a polyhedron. This
can model the situation when we think the opponent is playing uniformly at random within some region of a
ﬁxed strategy, such as a speciﬁc Nash equilibrium or a “population average” strategy based on historical data.
Prior work has used this model to generate a class of opponents who are signiﬁcantly more sophisticated than
opponents who play uniformly at random over the entire space [5]). For example, in rock-paper-scissors,
we may think the opponent is playing a strategy uniformly at random out of strategies that play each action
with probability within [0.31,0.35], as opposed to completely random over [0,1].

Let vi,j denote the jth vertex for player i, where vertices correspond to mixed strategies. Let p0 denote
i,j denotes the probability that player i plays the strategy corre-
the prior distribution over vertices, where p0
sponding to vertex vi,j. Let Vi denote the number of vertices for player i. Algorithm 2 gives an algorithm
for computing the Bayesian best response in this setting. Correctness follows straightforwardly by applying
Corollary 1 with the formula for the expectation of the uniform distribution.

5 Conclusion

One of the most fundamental problems in game theory is that of learning to play optimally against oppo-
nents who may make mistakes. We have presented the ﬁrst exact algorithm for performing exploitation in

10

(cid:104)

(cid:80)

πi

i

(cid:80){ρab}(cid:81)

(cid:105)

Algorithm 2 Algorithm for opponent exploitation with uniform prior distribution over polyhedron
Inputs: Prior distribution over vertices p0, response functions rt for 0 ≤ t ≤ T

M0 ← strategy proﬁle assuming opponent i plays each vertex vi,j with probability p0
R0 ← r0(M0)
Play according to R0
for t = 1 to T do

i,j = 1
Vi

for i = 1 to N do

ai ← action taken by player i at time step t
for j = 1 to Vi do

i,j ← pt−1
pt
Normalize the pt

i,j

· vi,j(ai)
i,j’s so they sum to 1

Mt ← strategy proﬁle assuming opponent i plays each vertex vi,j with probability pt
Rt ← rt(Mt)
Play according to Rt

i,j

imperfect-information games in the Bayesian setting using a very natural and the most well-studied prior dis-
tribution for this problem, the Dirichlet distribution. The algorithm runs in time polynomial in the number of
information states for the opponent. Previously an exact algorithm had only been presented for normal-form
games, and the best previous algorithm was a heuristic with no guarantees. We focused on the setting where
the opponent’s private information was not observed after each iteration. We observed interesting counter-
intuitive phenomena in a natural game. Future work can extend our analysis to many important settings.
For example, we would like to study the setting when the opponent’s private information is observed after
some iterations and not after others and general sequential games where the agents can take multiple actions
(we focused on the case where each player took one action). We would also like to consider algorithms that
could have an improved dependence on the number of actions and game iterations. Perhaps some of these
extensions can be developed straightforwardly from our results. Relatedly, we would like to extend analysis
for any number of agents. Our algorithm is not specialized for two-player zero-sum games (it applies to
general-sum games); if we are able to compute the expectation of the posterior strategy against multiple op-
ponent agents, then best responding to this strategy proﬁle is just a single agent optimization and can be done
in time linear in the size of the game regardless of the number of opponents. While the Dirichlet is the most
natural and well-studied prior for this problem, we would also like to study other important distributions. We
have also presented an algorithm for a natural prior that is the uniform distribution over a polyhedron, which
could model the situation where we think the opponent is playing a strategy from a uniform distribution in
a region around a particular strategy, such as a speciﬁc Nash equilibrium or a “population average” strategy
based on historical data.

While our main algorithm is likely impractical for a large number of game iterations or actions due
to its exponential dependence, it would be interesting to compare it experimentally for a small number of
iterations and actions to the approximation heuristic that has been previously successful in limit Texas hold
’em poker [15]. Opponent exploitation is a fundamental problem, and our algorithm and extensions could
be applicable to any domain modeled as an imperfect-information game. For example, many security game
models have imperfect information, e.g., [10, 12], and opponent exploitation in security games has been a
very active area of recent study, e.g., [13, 14].

11

6 Acknowledgments

We thank Michael Bowling for contributing to the analysis of the motivating game in Section 3.1.

References

[1] George W. Brown. Iterative solutions of games by ﬁctitious play. In Tjalling C. Koopmans, editor,

Activity Analysis of Production and Allocation, pages 374–376. John Wiley & Sons, 1951.

[2] Xi Chen and Xiaotie Deng. Settling the complexity of 2-player Nash equilibrium. In Proceedings of

the Annual Symposium on Foundations of Computer Science (FOCS), 2006.

[3] Drew Fudenberg and David Levine. The Theory of Learning in Games. MIT Press, 1998.

[4] Sam Ganzfried and Tuomas Sandholm. Game theory-based opponent modeling in large imperfect-
information games. In Proceedings of the International Conference on Autonomous Agents and Multi-
Agent Systems (AAMAS), 2011.

[5] Sam Ganzfried and Tuomas Sandholm. Safe opponent exploitation. ACM Transactions on Economics
and Computation (TEAC), 2015. Special issue on selected papers from EC-12. Early version appeared
in EC-12.

[6] Richard Gibson. Regret Minimization in Games and the Development of Champion Multiplayer Com-

puter Poker-Playing Agents. PhD thesis, University of Alberta, 2014.

[7] Michael Johanson and Michael Bowling. Data biased robust counter strategies. In International Con-

ference on Artiﬁcial Intelligence and Statistics (AISTATS), 2009.

[8] Michael Johanson, Kevin Waugh, Michael Bowling, and Martin Zinkevich. Accelerating best response
calculation in large extensive games. In Proceedings of the International Joint Conference on Artiﬁcial
Intelligence (IJCAI), 2011.

[9] Michael Johanson, Martin Zinkevich, and Michael Bowling. Computing robust counter-strategies.
In Proceedings of the Annual Conference on Neural Information Processing Systems (NIPS), pages
1128–1135, 2007.

[10] Christopher Kiekintveld, Milind Tambe, and Janusz Marecki. Robust bayesian methods for Stackel-
berg security games (extended abstract). In Autonomous Agents and Multi-Agent Systems, 2010. Short
paper.

[11] Daphne Koller, Nimrod Megiddo, and Bernhard von Stengel. Fast algorithms for ﬁnding randomized
strategies in game trees. In Proceedings of the 26th ACM Symposium on Theory of Computing (STOC),
pages 750–760, 1994.

[12] Joshua Letchford and Vincent Conitzer. Computing optimal strategies to commit to in extensive-form

games. In Proceedings of the ACM Conference on Electronic Commerce (EC), 2010.

[13] Thanh H. Nguyen, Rong Yang, Amos Azaria, Sarit Kraus, and Milind Tambe. Analyzing the effec-
tiveness of adversary modeling in security games. In Proceedings of the AAAI Conference on Artiﬁcial
Intelligence (AAAI), 2013.

12

[14] James Pita, Manish Jain, Milind Tambe, Fernando Ord´o˜nez, and Sarit Kraus. Robust solutions to
Stackelberg games: Addressing bounded rationality and limited observations in human cognition. Ar-
tiﬁcial Intelligence Journal, 174(15):1142–1171, 2010.

[15] Finnegan Southey, Michael Bowling, Bryce Larson, Carmelo Piccione, Neil Burch, Darse Billings,
and Chris Rayner. Bayes’ bluff: Opponent modelling in poker. In Proceedings of the 21st Annual
Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pages 550–558, July 2005.

13

