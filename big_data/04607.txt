6
1
0
2

 
r
a

 

M
5
1

 
 
]

.

O
A
n
i
l
n
[
 
 

1
v
7
0
6
4
0

.

3
0
6
1
:
v
i
X
r
a

Auditory power-law activation-avalanches exhibit a fundamental computational

ground-state

Institute of Neuroinformatics and Institute of Computational Science,

University of Zurich and ETH Zurich, Winterthurerstr. 190, 8057 Zurich, Switzerland

Ruedi Stoop and Florian Gomez

(Dated: March 16, 2016)

The cochlea provides a biological information-processing paradigm that we only begin to under-
stand in its full complexity. Our work reveals an interacting network of strongly nonlinear dynami-
cal nodes, on which even simple sound input triggers subnetworks of activated elements that follow
power-law size statistics (’avalanches’). From dynamical systems theory, power-law size distribu-
tions relate to a fundamental ground-state of biological information processing. Learning destroys
these power laws. These results strongly modify the models of mammalian sound processing and
provide a novel methodological perspective for understanding how the brain processes information.

The mammalian neocortex has recently been suggested
to host neuronal activation avalanches [1–3] of power-
law distributed size. Evidence from simulated synchro-
nizing phase oscillator networks [4, 5] and other models
[6–9] supports this suggestion; however, the methods un-
derlying the experimental results are delicate, and most
of the models use strongly simpliﬁed neuronal dynam-
ics or implement particular balance conditions. Because
avalanches were related to a critical state in cortex, which
has opened a discussion on the signiﬁcance of such a po-
tential state regarding information propagation and pro-
cessing capacity [3, 6, 9–12], additional supporting evi-
dence would be desirable.

The mammal’s hearing sensor, the cochlea, can be seen
as an information-processing antecedent of the neocortex
that avoids most of the mentioned experimental and the-
ory diﬃculties. In essence, the cochlea consists of a con-
tinuous, ﬂuid-embedded stiﬀness-graded sensorial basilar
membrane coiled to a snail-shaped form. Its physical ba-
sis has recently become well-understood, from a combina-
tion of mostly linear hydrodynamics and nonlinear ampli-
ﬁer physics [13, 14]. To understand how this relates to a
discrete network, we focus on the signal pathway. A pure
tone (sound containing only one frequency) arriving at
the mammalian cochlea, elicits on the basilar membrane
one shallow surface wave that travels down the mem-
brane to its ’resonant place’. Here, the wave becomes
strongly ampliﬁed, by frequency-speciﬁc nonlinear active
units (the so-called outer hair cells). Beyond this point,
dissipation by ﬂuidal viscous losses annihilates the wave
[13, 14]. The relationship between frequency and place of
strongest ampliﬁcation, measured from the basilar mem-
brane’s base, is near-logarithmic. On the logarithmic fre-
quency scale, the interval across which a pure-tone sig-
nal is noticeably ampliﬁed, is essentially of constant size
(Suppl. Mat. 1 a), which suggests a natural partition
of the cochlea into sections. These sections provide the
nodes of our network.

Based on the detailed biophysics and nonlinear dy-
namics at work in the cochlea [13, 14], we developed a
model of the sensor consisting of such sections [15–17].

μ = -0.005

A = -20 dB

A = -40 dB

μ = -0.05

-25
a)

]
B
d
[
 
e
d
u
t
i
l

p
m
a

0
b)

]
B
d
[
 
e
d
u
t
i
l

p
m
a

μ = -0.8

 A = -80 dB

-50

400

frequency [Hz]

1500

-80

400

frequency [Hz]

1500

FIG. 1: Small-signal ampliﬁer characteristics of an isolated
node of the cochlear network [13]. Response vs. deviation
from the resonant frequency, a) for diﬀerent distances from
bifurcation point (µ ∈ {−0.05, −0.1, −0.2, −0.4, −0.8}), b) for
diﬀerent signal strengths (steps of 10 dB rms w.r.t. reference
level 1).

This model reproduces virtually all mesoscopic biophys-
ical cochlear observations ([15–19]; in particular Suppl.
Mat. of Refs. [15, 17, 18]). Fundamental for this is that
the sections share the dynamical properties of the micro-
scopic ampliﬁcation-providing outer hair cells [14, 20],
which is well-modeled by a stimulated Hopf process

˙z = (µ + i)ωchz − ωch|z|2z − ωchF (t); z, F (t) ∈ C,

where z(t) denotes the response amplitude, F (t) a stim-
ulation signal, wch is the characteristic frequency of the
Hopf system, and µ is the Hopf parameter [13, 14, 20–22].
At values µ < 0, the system is below bifurcation to self-
oscillation, but responds towards stimulation signals F (t)
as a small-signal ampliﬁer [23, 24]. Dissipation by ﬂu-
idal viscous losses can be described by tailored 6th-order
Butterworth low-pass ﬁlters [16, 17]. The main charac-
teristics of the isolated node dynamics are collected in
Fig. 1. When embedded into a compound cochlea, the
response proﬁles broaden due to the sections’ interaction
with neighboring ones, reproducing the biological data

[25] extremely well [16]. The distance of µ from bifur-
cation at µ = 0 deﬁnes how strongly a node ampliﬁes
an incoming signal; we choose this parameter to match
the human hearing sensor. The biophysical properties of
the cochlea suggest selecting the characteristic frequen-
cies of the nodes according to a geometric sequence. We
will use a software implementation of an earlier hard-
ware realization of 29 sections or nodes, taking care of 7
octaves (14.08 − 0.11 kHz). Our partition is optimal in
the sense that ﬁner partitions yield for the human am-
pliﬁcation range, identical results, but coarser partitions
lead to distortions in the frequency dependence, if suﬃ-
ciently strong ampliﬁcation is required (Suppl. Mat. 2).
All nodes will have identical Hopf parameters (’ﬂat tun-
ing’ µ ≡ −0.25), until we deal with learning, where we
condition the network towards chosen sounds, by tuning
unsuited nodes towards weaker ampliﬁcation [15].

A node is deﬁned to be ’activated’ by a signal of arbi-
trary form, if the ampliﬁed signal reaches above the psy-
choacoustic audibility threshold of -50 dB [18, 26], but
diﬀering settings of the threshold, e.g. -40 or -60 dB, do
not compromise any of the following results. While pure-
tone stimulations lead to essentially one activated node,
all other (even simple) stimuli lead to complex activation
response patterns. In addition to the directly activated
nodes (network ’roots’), nodes become activated as the
result of the nonlinear interactions among already ac-
tivated nodes: Interacting ’parent nodes’ of frequencies
f1, f2 activate ’child nodes’ further down the cochlear
duct at combination-tone frequencies | nf1 − mf2 |,
m, n ∈ N. In our network model of hearing, this mech-
anism contributes the directed links between activated
nodes (Suppl. Mat. 3 and 4). This paradigm contrasts
the superposition model of the classical linear frequency
analyzer cochlea.

Such stimulation-speciﬁc activation-networks are the
focus of this work. Fig.
2 exhibits the activation-
network from the input of two complex tones (our com-
plex tones always consist of ﬁve consecutive harmon-
ics, starting with the fundamental frequency). Activated
nodes relate either to a harmonic of a stimulation, or to
a combination-tone of already activated nodes; combina-
tion tones have a prominent role in human pitch percep-
tion [17, 18, 27, 28]. A 0.65-kHz tone, e.g., is generated as
the 2f1 −f2 combination tone, with f1,2 = {2, 3.35} kHz.
Complex activation patterns correspond at the node level
to receptive ﬁelds that become increasingly intricate as
we move down the cochlea (Suppl. Mat. 1b), reﬂected in
ever more complicated wave forms (Suppl. Mat. 3). For
obtaining activation networks, this necessitates inferring
from the Fourier spectrum of a node’s signal whether a
node is a child or a root f (Suppl. Mat. 3). While
our cochlea is optimized to reproduce all salient features
of mammalian hearing, this level of quality is not re-
quired. The statistical network properties that we fo-
cus on are robust regarding sensor variation, as long as

combination-tones are suﬃciently generated.

2

13

10

14

12

15

18

19

11

9

3

5

6

27

7

8

4

17

20

22

23

21

26

24

16

1

node

activation level (dB):

29

>-10

-20 -30

-40 -50

<-50 

FIG. 2: Response to stimulation by two complex tones (base
frequencies: 2 and 3.35 kHz (black arrows), with 5 harmonics
each, both -60 dB strong). Top panel: Avalanche activation-
network, numbers indicating the location of activated nodes.
Origins of incident red arrows indicate ’parents’ of activated
nodes (cf. video, Suppl. Mat. 4). Lower panel: activity on
the unrolled cochlea. For identical input complexity, distinct
network sizes are triggered. Cochlea of 29 nodes, covering
(0.11, 14.08) kHz logarithmically, µ = −0.25 for all nodes.

We will show that randomly chosen simple inputs gen-

erate power-law activation-network size distributions

P (s) ∼ s−α

(where size s is the number of links). This happens au-
tonomously, without the need for additional conditions,
such as dynamical synapses or excitatory-inhibitory bal-
ance. We will then see how learning drives the network
away from power-law behavior. By the close evolutionary
and functional relationship between sensory and cortical
signal processing [29], our results open up a new per-
spective for understanding generic biological signal pro-
cessing. On this more general level, our conclusion will
be that power-law avalanche-size distributions character-
ize a fundamental ground-state of biological information
processing.

The distribution of the activity A along the cochlea ex-
hibits the complexity of the interaction among the nodes
(computationally, the activation A at node j ∈ {1, ..., 29}
is deﬁned as A(j) := 1
i=1 Θ(fi, j), where fi denotes
the ith stimulation, N is the total number of stimula-
tions, and Θ(fi, j) is 1 if node j is activated and 0 other-

N PN

wise). Single complex sounds of fundamental frequency
uniformly sampled from a non-logarithmic (0, 15)-kHz
frequency interval generate an activities A that still
closely follow the sampling power-law (α = −1). Mix-
tures of two and three complex sounds, however, gener-
ate µ-dependent power-laws A ∝ f − ˜α, where 0 < ˜α < 1
(see Fig. 3a). Because of combination tone dependence
on input level, stimulations were randomly chosen from
a (−80, −40)-dB interval. Regarding the human ear, this
corresponds to low to moderate sound levels of 30-70 dB
SPL [18]. Fixing sound level at -60 dB yields identi-
cal results. The origin of this behavior is clariﬁed by

two-tone mixtures

μ=-0.15

μ=-0.2

μ=-0.25

three-tone mixtures

μ=-0.1

μ=-0.15

μ=-0.2

μ=-0.25

μ=-0.3

10

frequency [kHz] of node

0.1

10

frequency [kHz] of node

0.1

1

A

0.5

0.3

1

a)

0.5

A

0.1

0.04

b)

0.01

~ s - 1.5

P(s)

0.1

0.0

10 -4

P

-4

10

20

10 -5

30

-50 dB

-70 dB

s

L

-60 dB

400

100

s

400

FIG. 3: Activation power laws. a) Cumulated activity A
from stimulations by two complex tones of uniformly random
input frequencies and random sound levels ∈ (−80, −40) dB
(rms) per tone. Inset: Results from mixtures of three complex
tones. Dotted: maximum likelihood estimates. b) Activation-
network size s distribution P (s) for mixtures of two complex
tones, of randomly chosen intermediate sound levels. Line L
and green arrow: Result of learning (see corresponding sec-
tion; µ = −0.25 (circles), µ = −1 (dashed), µ = −2 (circles
and dashed)). Inset: Fixed sound levels (see text).

the corresponding activation-network size statistics that
share the power-law property (where size s is deﬁned
as the number of activated links). The exponent and
the extension of that power-law regime are governed by
two counteracting mechanisms: The level of nonlinearity
present (µ) and the input sound level. Going to higher

3

(lower) sound levels or having µ closer (further away)
from bifurcation both drive the distribution away from
the maximal power-law regime. The latter is character-
ized by exponents α ≃ 1.5, for the standard µ = −0.25
value and random intermediate sound levels (cf. Fig.
3b). In the ﬁgure, dotted lines of slope -1.5 indicate the
regions across which a power-law test value p > 0.9 holds
[30]. The power-law regime excludes input-speciﬁc eﬀects
to the left, and ﬁnite size eﬀects of the cochlea combined
with stimulation speciﬁcs to the right. These results raise
the question whether we deal here with a manifestation
of self-organized branching universality with power-law
exponent α = 3
2 [31, 32]. This particularly since our
activation-networks parallel the neuronal avalanches in
neocortical networks [1–3], and because theoretical mod-
els of neocortical networks [6–8, 12] obtained similar ex-
ponents and have emphasized such a connection. On this
background, relative to the -50 dB activation threshold,
our results at ﬁxed stimulation strengths of -70, -60, -50
dB could be interpreted as ’underdeveloped’, ’fully de-
veloped’ and ’overdeveloped’. In this order, they would
correspond to a transition from subcritical to critical and
to supercritical behavior, respectively. The minor bump
obtained for -60 dB would then indicate a slightly super-
critical behavior or a ﬁnite size eﬀect, or a combination
of both.

By scale invariance, power law distributed avalanches
express a network’s unbiasedness regarding the nature of
the information to be processed. To demonstrate that
the power-law is lost when we bias the cochlea towards a
desired sound, we move the Hopf parameters of sections
unrelated to the sound further away from the bifurcation
point, a process that extensive usage seems to be made of
in mammalian auditory scene analysis [15]. Such a ’tun-
ing’ is closely related to learning in neural networks; how
learning aﬀects neocortical activity power laws is a topic
of strong current interest. Already the detuning of two
adjacent nodes generates substantial modiﬁcations of the
elicited activation-networks. If for two complex inputs of
fundamental frequencies f (1,2)
= {1.331, 2.120} kHz and
ﬁve harmonics each (each sound at -70 dB rms), nodes 11
and 12 are jointly gradually detuned from µ = −0.25 to
µ = −1, the associated activation-network shrinks sub-
stantially (Fig. 4a, and video Suppl. Mat. 5). Dur-
ing this process, the associated activation-network size
distributions gradually lose their power-law characteris-
tics. This is demonstrated in Fig. 3b for the detuning of
two frequency bands (nodes 15, 16 and nodes 19, 20, 21)
from µ = −0.25 over µ = −1.0 to µ = −2.0. Under this
change, the initial power-law distribution s−1.5 converts
gradually into a strictly convex shape (line L), indicating
that learning generically drives the network away from
power-law criticality. This speciﬁc tuning example is
generic in the following sense: Substantially simpler tun-
ing patterns do not produce a sound targeting eﬀect suﬃ-
cient to be qualiﬁed as listening, which is co-expressed by

0

the persistence of the power-law nature of P (s). For more
information on modalities and real world applications of
this learning implementation, readers should consult Ref.
[15], from which our example was selected.

S(ε)

S(ε)=ε

4

25

21

detuned sections

6

5

7

8

19

24

10

12

9

11

20

15

18

16

14

a)

29

90

50

n1,2

20

10

26

22

23

17

b)

27

edges

nodes 

detuning

13

c)

detuning

~ n1.2

2

˜
S

0.3

0.5

- μ11,12

1

0.5

7

2

10

n2

20

30

FIG. 4: Eﬀect of tuning on activation networks. a) Unbiased
(blue) vs. tuned (red) cochlea. The red graph is a subgraph
of the blue graph. b) Network characteristics edge number
n1, node number n2, as a function of the de-tuning of µ11,12.
c) Small-worldness ˜S as a function of number of nodes.

Learning also aﬀects the ’small-worldness’ property of
our network. To measure ’small-worldness’ [33], we use
the indicators γ = (Clustering coeﬃcient of the consid-
ered network) / (Clustering coeﬃcient of the Erd¨os-R´enyi
random network) and λ = (av. shortest path-length of
the considered network) / (av. shortest path-length of
the Erd¨os-R´enyi random network). A network is ’small-
world’ if its ’small-worldness’ ˜S := γ/λ exceeds unity.
For ﬂat tuning, we typically have ˜S ≃ 1.7 (cf. Fig. 4
c). Upon the tuning that changes the activation-network
as in Fig. 4a, this value consistently drops, i.e., tuning
works against small-worldness.

There is an information-dynamics argument as to why
nature might choose power-law distributions. A sys-
tem’s ’ground-state’ must lack bias towards a particu-
lar signal and provide the basis for the system’s abil-
ity to quickly adapt to new requirements (inputs, tasks)
and yet maintain past information for all signals simi-
larly. A natural description framework of these proper-
ties is within the ’Thermodynamic Formalism’ [34–37].
Focusing on exponential scaling in time n, of proba-
bility (fractal dimensions) or of support (Lyapunov ex-
ponents), e.g., systems are described by a free energy
1
n log Zn(β) (where Zn is usually a sum
F (β) = limn→∞
of Boltzmann contributions) or by the associated entropy

ε

FIG. 5: Eﬀect of learning on power-law distributions, thermo-
dynamic formalism of invariant measures (signals) framework
[34–37]. Blue: Entropy S(ε) of intermittent systems, power-
law characteristics. Red: Entropy of non-power-law behavior,
as the result of focusing on a particular measure. The observ-
ability O of an invariant measure ε decays with time t as
O(ε, t) ∼ e−t (ε−S(ε)) (red and blue arrow lengths). Points
on the diagonal ε = S(ε) refer to measures that decay slower
than exponentially. Green arrows and the circle illustrate the
learning process.

function S(ε) = βε+F (β), obtained from F (β) by a Leg-
endre transform. The probability of observing a value of
an observable ε then scales as [35]

P (ε, n) dε ∼ e−n(ε−S(ε)) dε;

taking logarithms, we obtain in the thermodynamic limit
ε = S(ε). While ’normally behaved’ systems have one
’observable invariant measure’, our power-law distributed
invariant density measures admit a continuum of observ-
able invariant measures [38] (interpretable as a phase
transition). This provides a link to our experimentally
observed power-law distributions: Starting from a com-
putational ground-state (power-law characteristics and
correlations that decay slower than exponentially), learn-
ing forces the system to focus on a particular measure,
which destroys the power-law (Fig. 5). Whereas in the
ground-state the prediction of the evolution of the sys-
tem is extremely diﬃcult, for the tuned system this is
much simpler [38], which is co-expressed by an increased
computation (measured as the reduction of the complex-
ity of prediction of the system [39]) after learning. Since
the speciﬁc heat diverges ( d2S(ε)
dε2 = 0) and there is no
latent heat trace, the ground-state system would indeed
be at the critical point. Away from the thermodynamic
limit, i.e. for real-world systems, generalized space con-
straints generically generate power-law deviations that
in the thermodynamic limit vanish in a well-controlled
manner [40].

Our approach opens many opportunities to connect
with psychophysics and electrophysiology. An example
fully in line with our interpretation is anesthesia (known

to produce functional disconnection in the posterior com-
plex, causing loss of information capacity by interrupting
cortical communication [41]), which pulls the dynamics
away from the pre-anesthesia power law [42].

[1] J.M. Beggs and D. Plenz. J. Neurosci. 23, 11167 (2003).
[2] J.M. Beggs and D. Plenz. J. Neurosci. 24, 5216 (2004).
[3] T. Petermann, T.C. Thiagarajan, M.A. Lebedev, M.A.L.
Nicolelis, D.R. Chialvo, and D. Plenz. Proc. Natl. Acad.
Sci. U.S.A 106, 15921 (2009).

[4] S. Boccaletti, V. Latora, Y. Moreno, M. Chavez, and D.-

U Hwang. Phys. Rep. 424, 175 (2006).

[5] M. Chavez, D.-U. Hwang, A. Amann, H.G.E. Hentschel,

and S. Boccaletti. Phys. Rev. Lett. 94, 218701 (2005).

[6] C.W. Eurich, J.M. Herrmann, and U. Ernst. Phys. Rev.

E 66, 066137 (2002).

[7] A. Levina, J.M. Herrmann, and T. Geisel. Nat. Phys. 3,

857 (2007).

[8] D. Millman, S. Mihalas, A. Kirkwood, and E. Niebur.

Nat. Phys. 6, 801 (2010).

[9] F. Lombardi, H.J. Herrmann, C. Perrone-Capano, D.
Plenz, and L. de Arcangelis. Phys. Rev. Lett. 108, 228703
(2012).

[10] V.M. Eguiluz, D.R. Chialvo, G. Cecchi, M. Baliki, and

A.V. Apkarian. Phys. Rev. Lett. 94, 018102 (2005).

[11] C. Haldeman and J.M. Beggs. Phys. Rev. Lett. 94,

058101 (2005).

[12] L. de Arcangelis and H.J. Herrmann. Proc. Natl. Acad.

Sci. U.S.A. 107, 3977 (2010).

[13] A. Kern and R. Stoop. Phys. Rev. Lett. 91, 128101

(2003).

[14] A. Kern. A nonlinear biomorphic Hopf-ampliﬁer model

of the cochlea. PhD Thesis, ETH Z¨urich (2003).

[15] F. Gomez, V. Saase, N. Buchheim, and R. Stoop. Phys.

Rev. Appl. 1, 014003, (2014).

[16] S. Martignoli, J.-J. van der Vyver, A. Kern, Y. Uwate,

and R. Stoop. Appl. Phys. Lett. 91, 064108 (2007).

[17] S. Martignoli and R. Stoop. Phys. Rev. Lett. 105, 048101

(2010).

[18] F. Gomez and R. Stoop. Nat. Phys. 10, 530 (2014).
[19] S. Martignoli, F. Gomez, and R. Stoop. Sci. Rep. 3, 2676

5

(2013).

[20] F. Gomez, T. Lorimer, and R. Stoop. Phys. Rev. Lett.,

in press /arXiv (2015).

[21] V.M. Egu´ıluz, M. Ospeck, Y. Choe, A.J. Hudspeth, and
M.O. Magnasco. Phys. Rev. Lett. 84, 5232-5235 (2000).
[22] Camalet, S., Duke, T., J¨ulicher, F. & Prost, J. Proc. Natl.

Acad. Sci. U.S.A. 97, 3183-3188 (2000).

[23] K. Wiesenfeld and B. McNamara. Phys. Rev. Lett. 55,

13-16 (1985).

[24] B. Derighetti, M. Ravani, R. Stoop, P.F. Meier, E. Brun,

and R. Badii. Phys. Rev. Lett. 55, 1746-1749 (1985).

[25] M. Ruggero. Curr. Opin. Neurobiol. 2, 449 (1992).
[26] G.F. Smoorenburg. J. Acoust. Soc. Am. 48, 924 (1970).
[27] R. Stoop and A. Kern. Phys. Rev. Lett. 93, 268103

(2004).

[28] R. Stoop and A. Kern. Proc. Natl. Acad. Sci. U.S.A. 101,

9179 (2004).

[29] T. Lorimer, F. Gomez, and R. Stoop. Sci. Rep. 5, 12492

(2015).

[30] A. Deluca and A. Corral. Acta Geophys. 61, 1351 (2013).
[31] S. Zapperi, K.B. Lauritsen, and H.E. Stanley. Phys. Rev.

Lett. 75, 4071 (1995).

[32] S. Hergarten. Phys. Rev. Lett. 109, 148001 (2012).
[33] M.D. Humphries and K. Gurney. PLoS One 3, e0002051

(2008).

[34] D. Ruelle. Thermodynamic Formalism (Cambridge Uni-

versity Press, 2004).

[35] J. Peinke, J. Parisi, O.E. R¨ossler, and R. Stoop. En-

counter with Chaos (Springer, 1992).

[36] C. Beck, and F. Schl¨ogl. Thermodynamics of Chaotic
Systems: An Introduction (Cambridge University Press,
1995).

[37] P. Gaspard. Chaos, Scattering and Statistical Mechanics

(Cambridge University Press, 2005).

[38] R. Stoop, N. Stoop, and L. Bunimovich. J. Stat. Phys.

114, 1127 (2004).

[39] R. Stoop and N. Stoop. Chaos 14, 675 (2004).
[40] T. Lorimer, F. Gomez, and R. Stoop. Sci. Rep. 5, 12492

(2015).

[41] M.T. Alkire, A.G. Hudetz, and G. Tononi. Science 322,

876 (2008).

[42] T. Bellay, A. Klaus, S. Seshadri, and D. Plenz. eLife 4,

e07224. DOI: 10.7554 (2015).

