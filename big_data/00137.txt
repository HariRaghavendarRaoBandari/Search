6
1
0
2

 
r
a

M
1

 

 
 
]

.

R
P
h
t
a
m

[
 
 

1
v
7
3
1
0
0

.

3
0
6
1
:
v
i
X
r
a

A Short Proof of Strassen’s Theorem Using Convex Analysis

Benjamin Armbruster∗

March 2, 2016

Abstract

We give a simple proof of Strassen’s theorem on stochastic dominance using linear program-

ming duality, without requiring measure-theoretic arguments. The result extends to generalized

inequalities using conic optimization duality and provides an additional, intuitive optimization

formulation for stochastic dominance.

1

Introduction

Strassen’s theorem (1965) is a fundamental theorem in the theory of stochastic dominance.

It

characterizes a stochastic dominance relationship between two random variables as an almost sure

comparison of the two on the same probability space. Formally, the theorem states:

Strassen’s Theorem. Z (cid:23)icv Y (Z (cid:23)cv Y ) if and only if there exists random variables Y ′ D= Y
and Z ′ D= Z such that Z ′ ≥ E[Y ′|Z ′] (Z ′ = E[Y ′|Z ′]) a.s.

Here (cid:23)icv denotes dominance in increasing concave stochastic order, also known as second order
stochastic dominance; (cid:23)cv denotes dominance in concave stochastic order; and D= denotes equality

in distribution. We say that random variable Z dominates random variable Y in increasing concave

stochastic order, Z (cid:23)icv Y , if E[u(Z)] ≥ E[u(Y )] for all (weakly) increasing concave functions utility

functions u. Similarly, we say Z (cid:23)cv Y if E[u(Z)] ≥ E[u(Y )] for all concave functions u.

Stochastic dominance constraints have been used in optimization problems under uncertainty

since Dentcheva and Ruszczy´nski [3] and have grown in popularity since then (see the +100 papers

∗

armbrusterb@gmail.com

1

citing [3]). They are often used to ensure that the chosen decision is preferred to some benchmark

action. According to Lizyayev [11], there are three categories of constraint formulations: distri-

bution based, majorization, and revealed-preference type. Strassen’s theorem is the basis of the

majorization approach (e.g., (2)). The dual formulation in our proof ((3),(4)) is new. It is similar

to the dual program in [9] but easier to interpret as a comparison of utilities:

let u be a utility

function; ai = u(yi); bj = u(zj); cj = u′(zj); and the constraints ensure that the utility function is

concave and increasing.

One beneﬁt of the majorization approach based on Strassen’s theorem is that it easily extends

to vector-valued random variables. We will consider random variables on Rk, with the customary

Borel σ-algebra. For technical reasons we will assume that the random variables have bounded

support. While this extension is not novel [14, 2] and has been used in optimization problems

[1, 8], we can further extend it to use generalized inequalities, which is novel. For this, consider a

proper (i.e., convex, not containing a line, and closed) cone K. Then for arbitrary vectors y and z

we write the generalized inequality z ≥K y if z − y ∈ K. This cone also redeﬁnes the notion of an

increasing function: u : Rk → R is an increasing function if u(z) ≥ u(y) for all z ≥K y. Increasing

functions are of course used to deﬁne (cid:23)icv. Using the nonnegative orthant cone, gives us the usual

componentwise inequality and associated deﬁnition of an increasing function. Thus, the precise

claim we are proving is as follows.

Generalized Strassen’s Theorem. For random vectors Y and Z with bounded support on Rk,
Z (cid:23)icv Y (Z (cid:23)cv Y ) if and only if there exists random variables Y ′ D= Y and Z ′ D= Z such that

Z ′ ≥K E[Y ′|Z ′] (Z ′ = E[Y ′|Z ′]) a.s.

One of the frequent criticisms of stochastic dominance constraints and a motivation for the above

generalization is that the constraints are too conservative since they require that E[u(Z)] ≥ E[u(Y )]

for a large class of utility functions. Here the choice of cone determines the directions in which the

utility functions must be increasing, and this allows us to vary the level of “conservatism” in the

stochastic dominance constraint. On one extreme is the cone K = {x : w · x ≥ 0}, a half-space,

requiring that the utility function be increasing in all directions making an acute angle to w. In

that case Z (cid:23) Y iﬀ w · Z (cid:23) w · Y in the sense of scalar stochastic dominance. Thus, w are the

weights at which we trade-oﬀ the diﬀerent components of the outcomes Y and Z. At the other

2

extreme is a very pointed cone K = {wα : α ≥ 0} only requiring that the utility functions increase

in the direction w, and in between is the usual nonnegative orthant cone, K = {x : x ≥ 0}, requiring

the utility to increase along all coordinate directions. We can also control the size of the cone (i.e.,

the level of conservatism) manually, by constructing the convex cone of our choice.

This extension leads to a new connection with conic optimization: Z (cid:23)icv Y is equivalent to

a feasibility problem in convex optimization problem involving the cone deﬁning the generalized

inequality among vectors (see (2)). For example, this optimization problem is a linear program if

the generalized inequality is deﬁned by a polyhedral cone, and it is a semideﬁnite program, if we

consider the vector space of symmetric matrices and the generalized inequality is deﬁned by the

cone of positive semideﬁnite matrices.

There is a long history of proofs of Strassen’s theorem and extensions [7, 14, 2, 12, 15, 6, 5, 4,

10, 13]. The theorem was ﬁrst proved by Hardy, Littlewood, and P´olya (1929) for scalar random

variables with ﬁnite support. This result was extended to ﬁnite-dimensional random vectors by

Sherman [14]. Most of the proofs appear in the probability literature; use detailed measure-theoretic

arguments; and only consider the case of (cid:23)cv (which lacks economic intuition because it does not

require the utility functions to be increasing). In contrast, this proof is aimed at researchers in

optimization and avoids measure-theoretic arguments. It is purely geometric, relying essentially on

a single use of Farkas’ lemma and is signiﬁcantly shorter than any of the others.

2 Proof

We initially focus on the case of (cid:23)icv and where Y and Z are scalar and have ﬁnite support on {yi}

and {zj}, respectively. In that case,

∃ Y ′, Z ′ s.t. Y ′ D

= Y, Z ′ D

= Y, E[Y ′|Z ′] − Z ′ ≤ 0

is equivalent term by term to

∃ pij s.t. pij ≥ 0 ∀i, j,

−Xj

pij = − Pr[Y = yi] ∀i, Xi

pij = Pr[Z = zj] ∀j, Xi

pij(yi − zj) ≤ 0 ∀j.

3

(1)

(2a)

(2b)

The equivalence follows from choosing pij = Pr[Y ′ = yi, Z ′ = zj] and applying Bayes’ theorem to

the last term. Using slack variables for the inequality in (2b) and then applying Farkas’ lemma1,

this is equivalent to the infeasible dual system

¬∃ ai, bj , cj s.t.

ai ≤ bj + cj(yi − zj) ∀i, j,

cj ≥ 0 ∀j,

Xj

bj Pr[Z = zj] −Xi

ai Pr[Y = yi] < 0.

Thus, Z (cid:23)icv Y can also be written as a linear optimization problem

0 ≤ min

ai,bj ,cj Xj

bj Pr[Z = zj] −Xi

ai Pr[Y = yi]

s.t. ai ≤ bj + cj(yi − zj) ∀i, j

cj ≥ 0 ∀j.

(3a)

(3b)

(3c)

(4a)

(4b)

(4c)

Lemma. The inequalities (3b)–(3c) have a solution iﬀ there exists functions u and s such that

u(x′) ≤ u(x) + s(x)(x′ − x) ∀x, x′,

s(x) ≥ 0 ∀x,

Xx∈{zj}

u(x) Pr[Z = x] − Xx∈{yi}

u(x) Pr[Y = x] < 0.

(5a)

(5b)

Proof. The “if” direction holds by selecting ai = u(yi), bj = u(zj ), and cj = s(zj). Now for the

“only if” direction, consider a solution of (3b)–(3c). Deﬁne u(x) = minj {bj + cj(x − zj)}. This

function is concave since it is a minimum of aﬃne functions and increasing since cj ≥ 0. Let s(x)

be a supergradient of u(x). Thus (5a) holds. Since (3b) implies ai ≤ u(yi) and the deﬁnition of u

implies u(zj) ≤ bj, it follows that (3c) implies (5b), proving the claim.

Note that (5a) states that u is an increasing concave utility function with supergradient s,

and (5b) states that E[u(Z)] < E[u(Y )]. Thus, (3) is equivalent to there not existing an increasing

concave utility function u, where E[u(Z)] < E[u(Y )]. This is equivalent to for all increasing concave

utility functions u, E[u(Z)] ≥ E[u(Y )], which is the deﬁnition of Z (cid:23)icv Y and proves the theorem.

1The variant used here is, Ax = b, x ≥ 0 is feasible iﬀ A⊤y ≥ 0, b⊤y < 0 is infeasible.

4

The key to the theorem is duality: 1) the variables, pij, of the joint probability distribution

are dual to the concavity constraints on the utility function (3b); and 2) the slopes of the utility

function, cj, are dual to the inequality constraints Z ′ ≥ E[Y ′|Z ′] in (2b).

The same proof holds for (cid:23)cv except that now the cj variables are free.

Essentially the same proof holds for the vector-valued case, except that we treat the cj as

vectors; the product in (3b) as an inner product; and the constraint cj ≥ 0 is meant as cj ∈ K∗,

where K∗ = {z : z · y ≥ 0 ∀y ∈ K} is the dual cone of K.

We now show the extension to general distributions by more carefully stepping through the

application of Farkas’ lemma (i.e., going from (2) to (3)) using the Hahn-Banach separation theorem.

Let qY and qZ in L1(Rk) be the distributions of Y and Z and let SY and SZ be their supports.

Thus, p(y, z), the joint distribution of (Y ′, Z ′) is in L1(Rk × Rk). We further deﬁne slack variables

r(z) ∈ (L1(Rk))k. Now we can deﬁne the linear operator A

Ap = (cid:18)−ZSZ

p(y, z)dz,ZSY

p(y, z)dy,ZSY

p(y, z)(y − z)1(z ∈ SZ)dy(cid:19) ,

(6)

where 1(·) denotes the indicator function. The operator A is bounded because y − z only ranges

on some bounded set. Hence A is a continuous operator. Hence the following convex cone,

C = {Ap + (0, 0, r) : p(y, z) ≥ 0 ∀(y, z), r(z) ∈ K ∀z, p ∈ L1(Rk × Rk), r ∈ (L1(Rk))k},

(7)

is closed. We deﬁne the righthand side of (2) as v = (−qY , qZ , 0). Then the separating Hahn-

Banach theorem states that exactly one of the following is true: v ∈ C (i.e., (2)) or there exists a

supporting hyperplane (i.e., a continuous linear function) h that strictly separates the point v from

C, that is h(v) < α ≤ h(x) for all x ∈ C. Since C is a cone, we may assume α = 0. Since L∞ is the

dual space of L1, we can deﬁne this linear function using a vector (a, b, c) so that h(x) = (a, b, c) · x

where a ∈ L∞(Rk), b ∈ L∞(Rk) and c ∈ (L∞(Rk))k. Then h(v) = (a, b, c) · (−qY , qZ, 0) < 0 is

equivalent to

−Z a(y)qY (y)dy +Z b(z)qZ (z)dz < 0,

(8)

essentially (3c). Now h(x) ≥ 0 for all x ∈ C iﬀ (a, b, c) · (Ap) ≥ 0 for all {p : p(y, z) ≥ 0 ∀y, z}

and c(z) · r(z) ≥ 0 for all z and r ∈ {r : r(z) ∈ K ∀z}. Thus h(x) ≥ 0 for all x ∈ C iﬀ

5

(A∗(a, b, c))(y, z) ≥ 0 for all (y, z) and c(z) ∈ K∗ for all z. Here A∗ is the adjoint (i.e., transpose)

of operator A. Putting the pieces together, v ∈ C is equivalent to there not existing (a, b, c) such

that (8) and

− a(y) + b(z) + c(z) · (y − z) ≥ 0 ∀y ∈ SY , z ∈ SZ .

(9)

The restrictions to the support come from the indicator functions and the domains of integration

in (6). This is essentially (3). The rest of the proof follows as before.

Acknowledgments

I thank Thibaut Barthelemy, Jim Luedtke, and Fatemeh Hamidi Sepehr for

suggestions improving the manuscript.

References

[1] B. Armbruster and J. Luedtke. Multivariate dominance constrained stochastic programs. IIE

Transactions, 47(1):1–14, 2015.

[2] D. Blackwell. Equivalent comparisons of experiments. The Annals of Mathematical Statistics,

24(2):265–272, 1953. URL http://dx.doi.org/10.1214/aoms/1177729032.

[3] D. Dentcheva

and A. Ruszczy´nski.

Optimization with stochastic

dominance

constraints.

SIAM Journal

on Optimization,

14(2):548–566,

2003.

URL

http://link.aip.org/link/?SJE/14/548/1.

[4] J. Elton and T. Hill. On the basic representation theorem for convex domination of measures.

Journal of mathematical analysis and applications, 228(2):449–466, 1998.

[5] J. Elton and T. P. Hill. Fusions of a probability distribution. The Annals of Probability,

20(1):421–454, 1992. URL http://dx.doi.org/10.1214/aop/1176989936.

[6] P. Fischer and J. A. R. Holbrook. Balayage deﬁned by the nonnegative convex func-

tions. Proceedings of the American Mathematical Society, 79(3):445–445, 1980. URL

http://dx.doi.org/10.1090/S0002-9939-1980-0567989-9.

[7] G. H. Hardy, J. E. Littlewood, and G. P´olya. Some simple inequalities satisﬁed by convex

functions. Messenger of Mathematics, 58:145–152, 1929.

6

[8] W. B. Haskell, J. G. Shanthikumar, and Z. M. Shen. Optimization with a class of multivariate

integral stochastic order constraints. Annals of Operations Research, 206(1):147–162, 2013.

URL http://dx.doi.org/10.1007/s10479-013-1337-0.

[9] T. Kuosmanen. Performance measurement and best-practice benchmarking of mutual funds:

combining stochastic dominance criteria with data envelopment analysis. Journal of Productiv-

ity Analysis, 28(1-2):71–86, 2007. URL http://dx.doi.org/10.1007/s11123-007-0045-7.

[10] T. Lindvall. On Strassen’s theorem on stochastic domination. Electron. Commun. Probab.,

4:51–59, 1999.

[11] A. Lizyayev. Stochastic dominance eﬃciency analysis of diversiﬁed portfolios: classiﬁcation,

comparison and reﬁnements. Annals of Operations Research, 196(1):391–410, 2012. URL

http://dx.doi.org/10.1007/s10479-012-1123-4.

[12] L. Mirsky. Majorization of vectors and inequalities for convex functions. Monatshefte f¨ur

Mathematik, 65(2):159–169, 1961. URL http://dx.doi.org/10.1007/BF01307025.

[13] A. M¨uller and L. Rschendorf. On the optimal stopping values induced by general

dependence structures.

Journal of Applied Probability, 38(3):672–684, 2001.

URL

http://www.jstor.org/stable/3216120.

[14] S. Sherman. On a theorem of Hardy, Littlewood, Polya, and Blackwell. Proc Natl Acad Sci U

S A, 37(12):826–831, 1951.

[15] V. Strassen. The existence of probability measures with given marginals. The Annals of

Mathematical Statistics, 36:423–439, 1965.

7

