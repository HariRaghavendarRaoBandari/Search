6
1
0
2

 
r
a

M
5

 

 
 
]

.

R
P
h
t
a
m

[
 
 

1
v
7
2
7
1
0

.

3
0
6
1
:
v
i
X
r
a

Branching diﬀusion representation of semilinear PDEs

and Monte Carlo approximation ∗

Pierre Henry-Labord`ere†

Nizar Touzi¶

Nadia Oudjane ‡

Xavier Warin (cid:107)

Xiaolu Tan§

March 8, 2016

Abstract

We provide a representation result of parabolic semi-linear PD-Es, with polynomial
nonlinearity, by branching diﬀusion processes. We extend the classical representation
for KPP equations, introduced by Skorokhod [23], Watanabe [27] and McKean [18], by
allowing for polynomial nonlinearity in the pair (u, Du), where u is the solution of the
PDE with space gradient Du. Similar to the previous literature, our result requires
a non-explosion condition which restrict to “small maturity” or “small nonlinearity”
of the PDE. Our main ingredient is the automatic diﬀerentiation technique as in [15],
based on the Malliavin integration by parts, which allows to account for the nonlin-
earities in the gradient. As a consequence, the particles of our branching diﬀusion
are marked by the nature of the nonlinearity. This new representation has very im-
portant numerical implications as it is suitable for Monte Carlo simulation. Indeed,
this provides the ﬁrst numerical method for high dimensional nonlinear PDEs with
error estimate induced by the dimension-free Central limit theorem. The complexity
is also easily seen to be of the order of the squared dimension. The ﬁnal section of this
paper illustrates the eﬃciency of the algorithm by some high dimensional numerical
experiments.

Key words. Semilinear PDEs, branching processes, Monte-Carlo methods.

∗We are grateful to Vincent Bansaye, Julien Claisse, Emmanuel Gobet and Gaoyue Guo for valuable
comments and suggestions. X. Tan and N. Touzi gratefully acknowledge the ﬁnancial support of the ERC
321111 Roﬁrm, the ANR Isotace, and the Chairs Financial Risks (Risk Foundation, sponsored by Soci´et´e
G´en´erale) and Finance and Sustainable Development (IEF sponsored by EDF and CA).

†Soci´et´e G´en´erale, Global Market Quantitative Research, pierre.henry-labordere@sgcib.com
‡EDF R&D & FiME, Laboratoire de Finance des March´es de l’Energie, nadia.oudjane@edf.fr
§CEREMADE, University of Paris-Dauphine, PSL Research University, tan@ceremade.dauphine.fr
¶Ecole Polytechnique Paris, Centre de Math´ematiques Appliqu´ees, nizar.touzi@polytechnique.edu
(cid:107)EDF R&D & FiME, Laboratoire de Finance des March´es de l’Energie, xavier.warin@edf.fr

1

1

Introduction

The objective of the present paper is to provide a probabilistic representation for
the solution of a nonlinear parabolic second order partial diﬀerential equation (PDE)
which is suitable for a high dimensional Monte Carlo approximating scheme. Our main
results achieve this goal in the context of semilinear PDEs:

−∂tu − Lu = f (u, Du), uT = g,

t < T, x ∈ Rd,

with polynomial non-linearity ft,x(y, z) in the solution and its gradient, diﬀusion gen-
erator L, and bounded terminal condition g.

Previous representation results were obtained in the literature by means of back-
ward stochastic diﬀerential equations, as introduced by Pardoux and Peng [20]. The
Monte Carlo numerical implications of this representation were introduced by Bally
& Pag`es [2], Bouchard & Touzi [5] and Zhang [28], and generated a large stream of
the literature. However, these methods can be viewed as a Monte Carlo version of
the ﬁnite elements methods, and as such, are subject to the problem of curse of di-
mensionality. Our primary goal is to avoid this numerical problem so as to be capable
to handle genuinely high-dimensional problems. This however will be achieved at the
cost of some limitations...

Our main representation result is obtained by using the branching diﬀusion trick to
absorb the nonlinearity, as illustrated by Skorokhod [23], Watanabe [27] and McKean
[18] in the context of the KPP equation, see also the extensions in Rasulov, Raimova
& Mascagni [21] and our previous paper [14] where the representation is also shown
to allow for path-dependency.

Since the gradient is also involved in the nonlinearity, our representation result is a
signiﬁcant improvement of the classically well-know representation of KPP equations.
We observe that the polynomial nonlinearity naturally induces some restrictions needed
to ensure the non-explosion of the corresponding solution. As a consequence, our
representation holds under technical conditions of small maturity or small nonlinearity
of the PDE.

The main idea for our representation is to use the Monte Carlo automatic diﬀeren-
tiation technique in addition to the branching diﬀusion representation. The automatic
diﬀerentiation in Monte Carlo approximation of diﬀusions was successfully used in
the previous literature by Fourni´e et al. [11], Bouchard, Ekeland & Touzi [4], Henry-
Labord`ere, Tan & Touzi [15], and Doumbia, Oudjane & Warin [7]. The resulting
branching diﬀusion in the representation diﬀers from that of the original founding pa-
pers [23, 27, 18] by introducing marks for the particles born at each branching. The
mark of the particle determines the nature of the diﬀerentiation, and thus induces the
corresponding automatic diﬀerentiation weight.

We next illustrate the main idea behind our representation in the context of the

following extension of the one-dimensional Burgers equation:

Lu :=

1
2

∆u, and f (y, z) =

(y2 + yz).

1
2

2

(cid:104) ¯F (T )

(cid:90) T

(cid:105)

(cid:2)φ(cid:0)T(1), W 1

T(1)

(cid:1)(cid:3),

ρ > 0 on R+, and denote ¯F (t) :=(cid:82) ∞

Let W 1 be a Brownian motion, and τ 1 an independent random variable with density
t ρ(s)ds. We also introduce another independent
random variable I 1 which takes the values 0 and 1 with equal probability. Then,
denoting by Et,x the expectation operator conditional on the starting data Wt = x at
time t, we obtain from the Feynman-Kac formula the representation of the solution u
as:

u(0, x) = E0,x

g(WT )
¯F (T )

+

f (u, Du)(t, Wt)

ρ(t)dt

= E0,x

0

ρ(t)

where T(1) := τ 1 ∧ T , and

φ(t, y)

:=

1{t≥T}
¯F (T )

g(y)+

1{t<T}
ρ(t)

(uDI1u)(t, y).

(1.1)

We next consider the two alternative cases for the value of I 1.

• On the event set {I 1 = 0}, it follows from the Markov property that:

(uDI 1

u)(t, y) = u(t, y)2 = Et,y

(cid:2)φ(t + τ 1, W 1

t+τ 1)(cid:3)2.

The tricky branching diﬀusion representation now pops up naturally by rewriting
the last expression in terms of independent copies (W 1,1, τ 1,1) and (W 1,2, τ 1,2)
as:

(uDI1u)(t, y) = Et,y
= Et,y

(cid:2)φ(cid:0)t + τ 1,1, W 1,1
(cid:2)φ(cid:0)t + τ 1,1, W 1,1

t+τ 1,1

t+τ 1,1

(cid:1)(cid:3)Et,y
(cid:1)φ(cid:0)t + τ 1,2, W 1,2

(cid:2)φ(cid:0)t + τ 1,2, W 1,1
(cid:1)(cid:3).

t+τ 1,2

t+τ 1,2

(cid:1)(cid:3)

where Et,y denotes the expectation operator conditional on W 1,1
t = y.
Substituting this expression in (1.1) and using the tower property, we see that
the branching mechanism allows to absorb the nonlinearity.

t = W 1,2

• On the event set {I 1 = 1}, we arrive similarly to the expression

(uDI1u)(t, y) = Et,y

(cid:2)φ(cid:0)t + τ 1,2, W 1,2

t+τ 1,2

∂yEt,y

(cid:1)(cid:3)∂yEt,y

(cid:2)φ(cid:0)t + τ 1,1, W 1,1
(cid:104) W 1,2
(cid:1)(cid:3) = Et,y
t+τ 1,2 − W 1,2

(cid:2)φ(cid:0)t + τ 1,2, W 1,2
φ(cid:0)t + τ 1,2, W 1,1

t+τ 1,1

t

t+τ 1,2

(cid:1)(cid:3).
(cid:1)(cid:105)

t+τ 1,2

,

τ 1,2

Our main representation is based on the following automatic diﬀerentiation:

which is an immediate consequence of the diﬀerentiation with respect to the heat
kernel, i.e. the marginal density of the Brownian motion. By the independence
of W 1,1 and W 1,2, this provides:

(cid:104) W 1,2
t+τ 1,2 − W 1,2

t

φ(cid:0)t + τ 1,1, W 1,1

(cid:1)φ(cid:0)t + τ 1,2, W 1,2

t+τ 1,1

t+τ 1,2

(cid:1)(cid:105)

,

(uDI1u)(t, y) = Et,y

τ 1,2

so that the branching mechanism allows again to absorb the nonlinearity by
substituting in (1.1) and using the tower property.

3

The two previous cases are covered by denoting T(1,i) := T ∧ (τ 1 + τ 1,i) for i = 0, 1,
and introducing the random variable:

W 1 := 1{I 1=0}+1{I 1=1}

∆W 1,2

T(1,2)
∆T(1,2)

, with ∆W 1,2

T(1,2)

:= W 1,2

T(1,2)

−W 1,2

T(1)

, ∆T(1,2) := T(1,2)−T(1),

so that

u(0, x) = E0,x

(cid:104)

1{T(1)=T} g(WT )

(cid:81)2

i=1

(cid:16)
¯F (T ) + 1{T(1)<T} W 1
1{T(1,i)=T} g(W 1,i
T )

ρ(T(1))

¯F (∆T(1,i)) + 1{T(1,i)<T}

u)(cid:0)T(1,i),W 1,i

ρ(∆T(1,i))

T(1,i)

(uDI1,i

(cid:1)

(cid:17)(cid:105)

.

Our main representation result is obtained by iterating the last procedure, and solv-
ing the integrability problems which arise because of the singularity introduced by
the random variable W 1. The automatic diﬀerentiation, which is the main additional
ingredient to the branching diﬀusion representation, is illustrated in the previous ex-
ample when the operator L corresponds to the Brownian motion. This extends to the
case of a more general diﬀusion operator by the so-called Bismuth-Elworthy-Li formula
based on the Malliavin integration by parts formula, see Fourni´e et al. [11] for its use
in the context of Monte Carlo approximation and the extension to other sensitivities.
Our main result provides a probabilistic representation of the solution of the semi-
linear PDE, with polynomial nonlinearity, in terms of a branching diﬀusion. This
requires naturally a technical condition ensuring the existence of a non-exploding so-
lution for the PDE which can be either interpreted as a small maturity or a small
nonlinearity condition. This new representation provides a new ingredient for the
analysis of the corresponding PDE as it can be used to argue about existence, unique-
ness, and regularity. We shall indeed prove a C1−regularity result in order to prove
the main Theorem 3.5.

Moreover, our new representation has an important numerical implication as it is
suitable for high dimensional Monte Carlo approximation. This is in fact the ﬁrst high
dimensional general method for nonlinear PDEs ! The practical performance of the
method is illustrated on a numerical example in dimension d = 20. The convergence of
the numerical method is a direct consequence of the law of large numbers. The rate of
convergence is also a direct consequence of the central limit theorem, and is therefore
dimension-free. The complexity of the method is easily shown to be of the order of d2,
which cannot be avoided by the very nature of the equation whose second order term
involves d × d matrices calculations.

The paper is organized as follows. Section 2 introduces the marked branching
diﬀusion. The main representation result is stated in Section 3. We next provide
further discussions in Section 4 on the validity of our representation for systems of
semilinear PDEs, and the possible combination with the unbiased simulation technique
of [15, 7]. The Monte Carlo numerical implications of our representation in high
dimension are reported in Section 5 with an illustration by a numerical experiment in
dimension 20. Finally, we provide more numerical examples in Section 6.

4

2 The marked branching diﬀusion

2.1 Semilinear PDE with polynomial nonlinearity
Let d ≥ 1, Md denotes the set of all d× d matrices, and (µ, σ) : [0, T ]× Rd → Rd × Md
the coeﬃcient functions. For a function u : [0, T ]×Rd → R, we denote by Du and D2u
the gradient and the Hessian of the function u(t, x) w.r.t. variable x. Let m ≥ 0 be a
positive integer, we consider a subset L ⊂ Nm+1, and a sequence of functions (c(cid:96))(cid:96)∈L
((cid:96)0, (cid:96)1,··· , (cid:96)m) ∈ L, denote |(cid:96)| :=(cid:80)m
and (bi)i=1,··· ,m, where c(cid:96) : [0, T ] × Rd → R and bi : [0, T ] × Rd → Rd. For every (cid:96) =
i=0 (cid:96)i. A generator function f : [0, T ]×Rd×R×Rd
(cid:88)

is then deﬁned by

m(cid:89)

(cid:0)bi(t, x) · z(cid:1)(cid:96)i.

f (t, x, y, z)

:=

c(cid:96)(t, x) y(cid:96)0

(2.1)

(cid:96)=((cid:96)0,(cid:96)1,··· ,(cid:96)m)∈L

i=1

Given two matrix A, B ∈ Md, denote A : B := Trace(AB(cid:62)). We will consider the
following semilinear PDE:

∂tu + µ · Du +

1
2

σσ(cid:62) : D2u + f (·, u, Du) = 0, on [0, T ) × Rd, and u(T, .) = g, (2.2)

for some bounded Lipschitz function g : Rd −→ R.

Indeed, for (cid:96) = (0, 0,··· , 0), we have c(cid:96)(t, x)y(cid:96)0(cid:81)m

i=1

(cid:0)bi(t, x) · z(cid:1)(cid:96)i = c(cid:96)(t, x).

Remark 2.1. The nonlinearity (2.1) includes the simplest case of a source term.

2.2 Age-dependent marked branching process

In preparation of the representation result, let us ﬁrst introduce a branching process,
characterized by a distribution density function ρ : R+ → R+, a probability mass

function (p(cid:96))(cid:96)∈L (i.e. p(cid:96) ≥ 0 and(cid:80)

(cid:96)∈L p(cid:96) = 1).

Instead of the usual exponential arrival time, we shall consider a branching particle
process with arrival time of distribution density function ρ. At the arrival time, the
particle branches into |(cid:96)| oﬀsprings with probability p(cid:96), among which, (cid:96)i particles
carry the mark i, i = 0, . . . , m. Then regardless of its mark, each descendant particle
performs the same but independent branching process as the initial particle.

To construct the above process, we will consider a probability space (Ω,F, P)

equipped with

• a sequence of i.i.d. positive random variables (τ m,n)m,n≥1 of density function ρ,
• a sequence of i.i.d. random elements (I m,n)m,n≥1 with P(I m,n = (cid:96)) = p(cid:96), (cid:96) ∈ L.

In addition, the sequences (τ m,n)m,n≥1 and (I m,n)m,n≥1 are independent.

We now construct an age-dependent branching process, with (τ m,n)m,n≥1 and

(I m,n)m,n≥1, using the following procedure.

1. We start from a particle marked by 0, indexed by (1), of generation 1, whose

arrival time is given by T(1) := τ 1,1 ∧ T .

5

2. Let k = (k1,··· , kn−1, kn) ∈ Nn be a particle of generation n, with arrival time

Tk. When Tk < T , we let Ik = I n,πn(k), where

πn is an injection from Nn to N,

and at time Tk, it branches into |Ik| oﬀspring particles, which constitute n+1-the
generation, and are indexed by (k1,··· , kn, i) for i = 1,··· ,|Ik|.

3. When Ik = (ˆ(cid:96)0, ˆ(cid:96)1,··· , ˆ(cid:96)m), we have |ˆ(cid:96)| oﬀspring particles, among which we mark
the ﬁrst ˆ(cid:96)0 particles by 0, the next ˆ(cid:96)1 particles by 1, and so on, so that each
particle has a mark i for i = 0,··· , m.

4. For a particle k = (k1,··· , kn, kn+1) of generation n + 1, we denote by k− :=
(k1,··· , kn) the “parent” particle of k, and the arrival time of k is given by

Tk :=(cid:0)Tk− + τ n+1,πn+1(k)(cid:1) ∧ T .

5. In particular, for a particle k = (k1,··· , kn) of generation n, and Tk− is its birth
time and also the arrival time of k−. Moreover, for the initial particle k = (1),
one has k− = ∅, and T∅ = 0.
The above procedure deﬁnes a marked age-dependent branching process. We de-

(cid:9), when t ∈ [0, T ),

when t = T,

note further

θk := mark of k, Kn

t :=

(cid:40)(cid:8)k of generation n s.t. Tk− ≤ t < Tk

{k of generation n s.t. Tk = T},

and also

Kn
t := ∪s≤tKn

s , Kt := ∪n≥1Kn

t

and Kt := ∪n≥1Kn
t .

Clearly, Kt (resp. Kn
in the system at time t, and Kt (resp. Kn
generation n) which have been alive before time t.

t ) denotes the set of all living particles (resp. of generation n)
t ) denotes the set of all particles (resp. of

Example 2.2. Let us consider the case d = 1, with

f (t, x, y, z) := c0,0(t, x) + c1,0(t, x)y + c1,1(t, x)yz.

In this case, m = 1, L = {¯(cid:96)1 = (1, 0), ¯(cid:96)2 = (1, 1)}. For the sake of clarity, we present an
typical path of the associated age-dependent process, with graphical illustration below.
The process starts from time 0 with one particle indexed by (1). At terminal time T ,
the number of particles alive is 3, with

KT = (cid:8)(1, 2, 1), (1, 1, 1, 1), (1, 1, 1, 2)(cid:9),

• At time T(1), particle (1) branches into two particles (1, 1) and (1, 2).
• At time T(1,1), particle (1, 1) branches into (1, 1, 1) and (1, 1, 2).
• At time T(1,2), particle (1, 2) branches into (1, 2, 1).
• At time T(1,1,2), particle (1, 1, 2) dies out without any oﬀspring particle.
• At time T(1,1,1), particle (1, 1, 1) branches into (1, 1, 1, 1) and (1, 1, 1, 2).

6

• The particles in blue are marked by 0, and the particles in red are marked by 1.

(1, 1, 1, 1)


aaaaa

(1, 1, 1, 2)

(1, 1, 1)

!!!!!!!!!!!!
PPPPPPPPPP

(1, 2, 1)

`````````````

-

(1)

(1, 1, 2)

(1, 1)


HHHHHHHHHHH

(1, 2)

0

T(1)

Proposition 2.3. Assume that (cid:80)

T(1,1) T(1,2)
(cid:96)∈L |(cid:96)|p(cid:96) < ∞. Then the age-dependent branching
process is well deﬁned on [0, T ], i.e. the number of particles in Kt is ﬁnite a.s. for all
t ∈ [0, T ].
Proof. See e.g. Theorem 1 of Athreya and Ney [1, Chapter IV.1], or Harris [12, pp.
138-139].

T(1,1,2)T(1,1,1)

T

2.3 The marked branching diﬀusion

We next equip each particle with a Brownian motion in order to deﬁne a branching
Brownian motion.

We consider a sequence of independent d-dimensional Brownian motion (W m,n)m,n≥1,

which are also independent of (τ m,n, I m,n)m,n≥1. Deﬁne W (1)

(cid:3) and then for each k = (k1,··· , kn) ∈ KT \ {(1)}, deﬁne

all t ∈(cid:2)0, T(1)

t = ∆W (1)

:= W 1,1

t

for

t

:= W k−

W k
t

Tk− + ∆W k

t−Tk−, with ∆W k

for all t ∈ [Tk−, Tk]. (2.3)
is a branching Brownian motion. For each k ∈ KT , we deﬁne an

t−Tk− := W n,πn(k)
t−Tk− ,

t )t∈[Tk−,Tk] by means of the following SDE

Then (W k· )k∈KT
associated diﬀusion process (X k

(cid:90) t

µ(cid:0)s, X k

(cid:1)ds +

(cid:90) t

σ(cid:0)s, X k

(cid:1)dW k

t = X k−
X k

t ∈ [Tk−, Tk], P-a.s.,

s

Tk−

Tk− +

s ,
0 = x0 for some constant x0 ∈
where for particle (1), we ﬁx the initial condition X (1)
Rd. The well-posedness of the last SDE is guaranteed by standard conditions on the
coeﬃcients µ, σ contained in Assumption 3.1.

(2.4)

Tk−

s

The process (X k· )k∈KT
introduce the sub-σ-ﬁelds

F0 := σ(cid:8)τ m,n, I m,n : m, n ≥ 1(cid:9), Fm := σ(cid:8)W i,n, τ i,n, I i,n : n ≥ 1, i ≤ m(cid:9), m ≥ 1. (2.5)

is our main marked branching diﬀusion process. We ﬁnally

7

3 The main representation

We shall provide a representation result for a class the semilinear PDEs (2.2) under
general abstract conditions. More explicit suﬃcient conditions are provided later.

3.1 Branching diﬀusion representation of semilinear PDEs

We ﬁrst collect the conditions on the marked branching diﬀusion which are needed for
our main results.

(cid:96) ∈ L, and (cid:80)
strictly positive on [0, T ], and such that F (T ) :=(cid:82) ∞

Assumption 3.1. (i) The probability mass function (p(cid:96))(cid:96)∈L satisﬁes p(cid:96) > 0 for all
(cid:96)∈L |(cid:96)| p(cid:96) < ∞. The density function ρ : R+ → R+ is continuous and

(ii) (µ, σ) : [0, T ] × Rd → Rd × Md are bounded continuous, and Lipschitz in x.
(iii) c(cid:96) : [0, T ] × Rd → R and bi : [0, T ] × Rd → Rd are bounded continuous.

T ρ(t)dt > 0.

Our next assumption is the key automatic diﬀerentiation condition on the under-

lying diﬀusion X

t,x
s deﬁned by

(cid:90) s

t

µ(cid:0)r, X

(cid:1)dr +

t,x
r

σ(cid:0)r, X

(cid:1)dWr,

t,x
r

(cid:90) s

t

X

t,x
s = x +

s ∈ [t, T ],

(3.1)

where W is a d-dimensional Brownian motion independent of the branching diﬀusion.
Assumption 3.2. There is a measurable functional W(t, s, x, (Wr − Wt)r∈[t,s]) sat-
isfying (t, x) (cid:55)→ W(t, s, x, (Wr − Wt)r∈[t,s]) is continuous, and for any s ∈ [t, T ] and
bounded measurable function φ : Rd → R, one has

∂xE(cid:2)φ(cid:0)X

t,x
s

(cid:1)(cid:3) = E(cid:104)
φ(cid:0)X

(cid:1)W(t, s, x, (Wr − Wt)r∈[t,s])

t,x
s

(cid:105)

.

Remark 3.3. In case (µ, σ) ≡ (µ0, σ0) for some constant (µ0, σ0) ∈ Rd × Md, where
σ0 is not generate, then an example of such automatic diﬀerentiation function can be
given by

W(cid:0)t, s, x, (Wr − Wt)r∈[t,s]

(cid:1) := (σ(cid:62)

.

0 )−1 Ws − Wt
s − t

For general coeﬃcient functions (µ, σ) satisfying some regularity and non-degeneracy
conditions, one can ﬁnd such functional W using Malliavin calculus (see more discus-
sions in Section 3.2).

Now, for each particle k ∈ KT , we recall that it is born at time Tk− and dies out at
time Tk, its mark is given by θk and its branching type is given by Ik. Let us denote

We next introduce for a smooth function u ∈ C1,2([0, T ] × Rd)

Wk := 1{θk=0} + 1{θk(cid:54)=0} bθk (Tk−, X k
(cid:104) (cid:89)
(cid:104) (cid:89)

T ) − g(X k
m(cid:88)
F (∆Tk)

Tk−)1{θk(cid:54)=0}

Tk−) · W(cid:0)Tk−, Tk, X k
(cid:105)(cid:104) (cid:89)
(cid:17)
1{θk=i}bi · Du

1{θk=0}u +

j=1(Kj

T \Kj
T )

k∈∪n

j=1Kj

Wk

g(X k

(Tk−, X k

Tk−)

,

k∈∪n

T

(cid:16)

(cid:105)

ψn

:=

Tk−, ∆W k· (cid:1).

cIk (Tk, X k
Tk

pIk

(3.2)

(cid:105)

)

Wk
ρ(∆Tk)

(3.3)

k∈Kn+1

T

i=1

8

for all n ≥ 1, and the corresponding limit

(cid:104) (cid:89)

k∈KT

ψ :=

g(X k

T ) − g(X k
F (∆Tk)

Tk−)1{θk(cid:54)=0}

(cid:105)(cid:104) (cid:89)

k∈KT \KT

Wk

cIk (Tk, X k
Tk

)

pIk

Wk
ρ(∆Tk)

(cid:105)

.

(3.4)

Further, notice that the above branching diﬀusion process (Tk, X k· )k∈KT
and random
variables ψ, ψn are deﬁned with initial condition (0, x0) on interval [0, T ]. By exactly
the same way, we can deﬁne the system with initial condition (t, x) on interval [t, T ],
let us denote them respectively by (T t

, ψt,x, and ψt,x
n .

, X t,x,k

·

k, W t,k·

We now provide a ﬁrst result, under strong regularity conditions, which provides a
better understanding of our representation. We emphasize that our main representa-
tion result in Theorem 3.5 below will be established under more transparent conditions.

has a solution u ∈ C1,2([0, T ]× Rd) with E(cid:104)(cid:82) T

Proposition 3.4. Let Assumptions 3.1 and 3.2 hold true. Suppose that the PDE (2.2)
< ∞, for some

(cid:105)
s )(cid:12)(cid:12)ds

t,x

)
k∈Kt

T

(t, x) ∈ [0, T ] × Rd. Assume further that (ψt,x

t
n )n≥1 is uniformly integrable. Then

(cid:12)(cid:12)f (·, u, Du)(s, X
and u(t, x) = E(cid:2)ψt,x(cid:3).

ψt,x ∈ L1

Proof. (i) It suﬃces to consider (t, x) = (0, x0). Since g is bounded, it follows from the
integrability condition on the process f (., u, Du) and the Feynma-Kac formula that

b0 · Dxu(0, x0) = E(cid:104)
= E(cid:104)(cid:16)

·

ψ1 b0 · W(cid:0)0, T(1), x0, ∆W (1)

·

ψ1 −

1

F (T(1))

from the fact that E(cid:2)W(cid:0)0, T, x, (Ws)s∈[0,T ]

(cid:1)(cid:3) = 0.

where the ﬁrst equality follows by Lemma A.3 of [15] and the second equality follows

(iii) For k ∈ K2
in formula (3.5) and (3.6). Then, with F1 deﬁned in (2.5),

T , change the initial condition from (0, x0) to (Tk−, X k

Tk−) = (T(1), X (1)
T(1)

)

Tk−) = E(cid:104)

u(Tk−, X k

1{k∈K2
T }

(cid:105)

(cid:12)(cid:12)(cid:12)F1

,

+ 1{k∈K2

T \K2

T }Ψk

g(X k
T )
F (∆Tk)

9

u(0, x0) = E(cid:104)
= E(cid:104)

0,x0
T

g(cid:0)X
g(cid:0)X (1)
(cid:16) cI(1)

T

1

F (T )
1

F (T(1))

1

ρ(T(1))

pI(1)

0

1

(cid:105)

ρ(s)

0,x0
s

uI(1),0

(cid:90) T
(cid:1)F (T ) +
(cid:1)1{T(1)=T} +
m(cid:89)

f(cid:0)·, u, Du(cid:1)(cid:0)s, X
(cid:0)bi · Du(cid:1)I(1),i(cid:17)(cid:0)T(1), X (1)

(cid:1)ρ(s)ds
(cid:1)1{T(1)<T}
)(cid:1) is integrable. Then under Assumptions 3.2,
(cid:1)(cid:105)

b0 · W(cid:0)0, T(1), x0, ∆W (1)

(cid:1)(cid:105)
(cid:17)

g(x0)1{T(1)=T}

(cid:105)

T(1)

i=1

·

, (3.6)

(3.5)

= E[ψ1].

(ii) Next, let b0 ∈ Rd be a constant vector, and assume in addition that the random
variable ψ1

(cid:0)b0 · W(0, T(1), x0, ∆W (1)

(cid:17)

(cid:105)

(cid:12)(cid:12)(cid:12)F1

by the Markov property, and by Assumption 3.2,

Tk−) = E(cid:104)(cid:16)
(cid:104)(cid:81)

cIk

)

Du(Tk−, X k

1{k∈K2
T }

g(X k

T ) − g(X k
F (∆Tk)

Tk−)

+ 1{k∈K2
W(Tk−, Tk, X k

T \K2

T }Ψk
Tk−, ∆W k· )

ρ(∆Tk)

(cid:16)

1{θk(cid:48) =0}u+(cid:80)m

where Ψk := 1
.
Plugging these expressions in the deﬁnition of ψ1 in (3.3), it follows from the integra-

(cid:105)
(cid:3).
bility of ψ2 and the tower property of conditional expectations that u(0, x0) = E(cid:2)ψ2
(cid:3) = E(cid:2)ψ(cid:3),
u(0, x0) = E(cid:2)ψn

(cid:17)
i=1 1{θk(cid:48) =i}bi·Du

(iv) Iterating this procedure, we see that

for all n ≥ 1,

E(cid:2)ψn

(Tk(cid:48)−, X k(cid:48)

(Tk,X k
Tk
pIk

and therefore u(0, x0) = lim
n→∞

Tk(cid:48)−)

k(cid:48)−=k

(cid:3),

,

where the last equality follows by the uniform integrability condition of (ψn)n≥1.

We now state our main representation result under abstract conditions on the
automatic diﬀerentiation weight function W(·) involving the slight modiﬁcation of ψ:

(cid:104)(cid:89)

k∈KT

˜ψ :=

g(X k

T ) − g(X k

Tk−)1{θk(cid:54)=0 or k=(1)}
F (∆Tk)

Wk

(cid:105)(cid:104) (cid:89)

cIk (Tk, X k
Tk

)

k∈KT \KT

pIk

(cid:105)

Wk
ρ(∆Tk)

, (3.7)

with ˜ψt,x deﬁned by an obvious change of origin. Explicit suﬃcient conditions for the
validity of the next result will be reported in Section 3.2 below.

Theorem 3.5. Let Assumptions 3.1 and 3.2 hold true, and suppose in addition that
for all (t, x) ∈ [0, T ] × Rd, there is some ε > 0 such that

and (cid:0) ˜ψs,yW(s, T s

)(cid:1)

(ψs,y)(s,y)∈Bε(t,x)

(1), y, ∆W s,(1)

·

(s,y)∈Bε(t,x)

are uniformly integrable, where Bε(t, x) := {(s, y) ∈ [0, T ] × Rd : |s − t| + |x − y| ≤ ε}.
Then, the function u(t, x) := E[ψt,x] is a continuous viscosity solution of the semilinear
PDE (2.2). Moreover Du exists and is continuous.

Proof. (i)Notice that the solution of SDE (3.1) is continuous w.r.t. its initial condition
(t, x), and recall that (t, x) (cid:55)→ W(t, s, x, (Wr − Wt)r∈[t,s] is also continuous, then under
the uniform integrability condition on (ψt,x), one obtains that u : [0, T ] × Rd → R is
continuous. Similarly, let us deﬁne

)(cid:3) = E(cid:2)ψt,xbi(t, x)·W(t, T t

(1), x, ∆W t,(1)

·

)(cid:3),

(1), x, ∆W t,(1)

·

which is also continuous by the uniformly integrability condition.
(ii) Let us deﬁne φ : [0, T ] × Rd → R by

vi(t, x) := E(cid:2) ˜ψt,xbi(t, x)·W(t, T t
φ(cid:0)I(1), T(1), X (1)

1

T(1)

F (T(1))

(cid:1)1{T(1)=T}
g(cid:0)X (1)
(cid:1) :=
(cid:16) cI(1)
(cid:0)ψ
= E(cid:2)ψ(cid:12)(cid:12) F1
(cid:3),

T(1),X (1)

ρ(T(1))

pI(1)

+

1

T

10

T(1)(cid:1)I(1),0

(cid:0)T(1), X (1)

T(1)

vi

(cid:1)I(1),i(cid:17)

m(cid:89)

i=1

(3.8)

1{T(1)<T}

where F1 is deﬁned in (2.5). Notice that φ(i, t, x) is continuous in x, then it follows
by Assumption 3.2 and Lemma A.3 of [15] that

(cid:1)W(0, T(1), x0, ∆W (1)

·

)(cid:3) = E(cid:2)ψW(0, T(1), x0, ∆W (1)

·

Du(0, x0) = E(cid:2)φ(cid:0)I(1), T(1), X (1)

)(cid:3).

T(1)

By changing the initial condition from (0, x0) to (t, x) and notice that

and one obtains that Du : [0, T ]×Rd → Rd is continuous from the uniform integrability

(iii) Using the expression in (3.8) and the law of I(1) and T(1), and with similar argu-
ments as in (3.5), it follows that

E[W(t, T t

(1), x, ∆W t,(1)

Du(t, x) = E(cid:2) ˜ψt,xW(t, T t

·

)1{T t

(1)=T}] = 0,

)(cid:3),

·

(1), x, ∆W t,(1)

)(cid:1). Moreover, one has vi(t, x) = bi(t, x) · Du(t, x).
(cid:105)

f(cid:0)·, u, Du(cid:1)(s, X

(cid:90) T
s − x| ≥ 1}, then by the ﬂow property

t,x
s )ds

t,x
T

t,x

.

t

f(cid:0)·, u, Du(cid:1)(s, X

(cid:105)

t,x
s )ds

,

it follows that

of(cid:0) ˜ψt,xW(t, T t

(1), x, ∆W t,(1)

·

u(t, x) = E[ψt,x] = E(cid:104)
g(cid:0)X
u(t, x) = E(cid:104)

u(hh, X

t,x
hh

) +

(cid:1) +
(cid:90) hh

t

Let h > 0, denote hh := (t + h)∧ inf{s > t : |X
of X

, one has

t,x

and we may verify by standard arguments that u is a viscosity solution of PDE (2.2).

3.2 More explicit suﬃcient conditions

We now provide some explicit suﬃcient conditions which guarantee the validity of the
conditions of Theorem 3.5. Deﬁne |ϕ|∞ := supx∈Rd |ϕ(x)| for any bounded function
i=1 |φi|∞ for any bounded vector function φ : Rd → Rd.
ϕ : Rd → R, and |φ|∞ := supd
We ﬁrst recall the Bismut-Elworthy-Li formula from Malliavin calculus, which was
used by Fourni´e, Lasry, Lebuchoux, Lions and Touzi [11] as an automatic diﬀerentiation
tool, see also [4], [5] and [10] for subsequent usefulness of the automatic diﬀerentiation
in the context of the Monte Carlo approximation of nonlinear PDEs. We emphasize
that such automatic diﬀerentiation function is not unique.

Assumption 3.6. The coeﬃcients µ, σ are bounded continuous, with bounded contin-
uous partial gradients Dµ, Dσ, and σ is uniformly elliptic.

Notice that (X

t,x

s )s∈[t,T ], as deﬁned by (3.1), is completely determined by (t, x, (Ws−

Wt)s∈[t,T ]). We then introduce the corresponding ﬁrst variation process Y :

Yt := Id, dYs = Dµ(s, X

t,x
s )Ysds +

Dσi(s, X

t,x
s )YsdW i

s, for s ∈ [t, T ], P-a.s.,

(3.9)

where Id denotes the d × d identity matrix, and σi(t, x) ∈ Rd denotes the i-th column
of matrix σ(t, x). Then one has the following result (see e.g. Exercise 2.3.5 of Nualart
[19, p.p. 125], or Proposition 3.2. of [11]).

i=1

11

d(cid:88)

Proposition 3.7. Let Assumption 3.6 hold true, then Assumption 3.2 holds true with
the choice of automatic diﬀerentiation function W deﬁned by

W(cid:0)t, s, x, (Wr − Wt)r∈[t,s]

(cid:1) :=

(cid:90) s

t

(cid:2)σ−1(r, X

1
s − t

(cid:3)(cid:124)

t,x
r )Yr

dWr.

(3.10)

Remark 3.8. When µ ≡ 0 and σ(t, x) ≡ σ0 for some non-degenerate constant matrix
σ0 ∈ Md, one then has Yt ≡ Id and so that

W(cid:0)t, s, x, (Wr − Wt)r∈[t,s]

(cid:1) = (cid:0)σ(cid:62)

0

(cid:1)−1 Ws − Wt

s − t

.

With the above choice of automatic diﬀerentiation weight function (3.10), we can
now derive some upper bounds for random variables (ψn, n ≥ 1). Recall that Lg is
: |xi| ≤ Lg}
the Lipschitz constant of g, denote by B∞

and W t,x,s := W(cid:0)t, s, x, (Wr − Wt)r∈[t,s]

0 (Lg) := {(x1,··· , xd) ∈ Rd

(cid:1). Then for n ≥ 1, q > 1, we introduce two
(cid:1)(cid:12)(cid:12)(cid:12)q(cid:105)
s − x)(cid:1)(cid:0)bi(t, x) · W t,x,s

constants C1,q and C2,q by
C1,q := |g|q∞ ∨

sup

t,x

0≤t<s≤T, x∈Rd, i=1,··· ,m, b0∈B∞

and

C2,q :=

sup

0≤t<s≤T, x∈Rd, i=1,··· ,m

s − t bi(t, x) · W t,x,s

0 (Lg)

E(cid:104)(cid:12)(cid:12)(cid:12)(cid:0)b0 · (X
E(cid:104)(cid:12)(cid:12)√
(cid:98)C2,q := C2,q

sup

(cid:96)∈L, t∈(0,T ]

(cid:12)(cid:12)q(cid:105)

,

(cid:17)q−1

.

(cid:16)|c(cid:96)|∞

− q

2(q−1)

t

p(cid:96)

ρ(t)

and then(cid:98)C1,q :=

C1,q

F (T )q−1

,

Remark 3.9. (i) Under Assumption 3.6, the tangent process Y is deﬁned by a linear
SDE, which has ﬁnite moment of any order q ≥ 1. Then the two constant C1,q and
C2,q are both ﬁnite. And for all k ∈ KT , one has

(cid:12)(cid:12)q (cid:12)(cid:12)(cid:12)F0

(cid:105)(cid:111) ≤ C1,q, E(cid:104)(cid:16)(cid:112)

∆Tk|Wk|(cid:17)q(cid:12)(cid:12)(cid:12)F0

(cid:105) ≤ C2,q, (3.11)

max
where the sub-σ-ﬁeld F0 is deﬁned in (2.5).
(ii) Notice that for a random variable N ∼ N (0, 1) and non-negative integer q ≥ 0, one
π. Then if (µ, σ) ≡ (0, σ0), for some constant (µ0, σ0) ∈
has E[|N|q] = 2
Rd × Md, and W as in Remark 3.8, it follows by direct computation that

2 Γ(cid:0) q+1

2

(cid:110)|g|q∞, E(cid:104)(cid:12)(cid:12)(Dg · ∆Xk)Wk
(cid:1)/
(cid:0)b(cid:62)

√

q

sup
b0∈B∞

0 (Lg)

C1,q ≤ |g|q∞ ∨(cid:16)

(cid:16) 2q + 1

(cid:17)

2

(cid:107)b(cid:62)
i (σ0σ(cid:62)

0 σ0σ(cid:62)
0 b0

√

/

π,

2q−1Γ

(cid:1) + max
(cid:0)σ0σ(cid:62)

0

i=1,··· ,m

(cid:1)−1bi(cid:107) q

(cid:17)q
0 )−1bi(cid:107)∞
(cid:17)
(cid:16) q + 1

√
/

π.

2∞ 2

q
2 Γ

2

and

C2,q = max

i=1,··· ,m

(cid:107)b(cid:62)

i

We are now ready for the main explicit suﬃcient conditions for the validity of the
representation Theorem 3.5. Notice that the following conditions can be interpreted
either as a small maturity or small nonlinearity restriction.

12

(cid:16) 1
(cid:0)(cid:98)C2,q
(ii) T <(cid:82) ∞(cid:98)C1,q

F (T )

(cid:17)q
(cid:96)∈L |c(cid:96)|∞ x|(cid:96)|(cid:1)−1dx.
(cid:80)

Assumption 3.10. For some q > 1, one of the following two items holds true.
(i) Both C1,q

and sup(cid:96)∈L,t∈(0,T ] C2,q

are bounded by 1.

1√

p(cid:96)

tρ(t)

(cid:16)|c(cid:96)|∞

(cid:17)q
(cid:16)|c(cid:96)|∞

(cid:17)q

1√

tρ(t)

Remark 3.11. (i) To ensure that sup(cid:96)∈L,t∈(0,T ] C2,q
is necessary to choose (p(cid:96))(cid:96)∈L such that
density function such that ρ(t) ≥ Ct−1/2.

(ii) To ensure that (cid:98)C2,q is ﬁnite, one needs to choose the density function ρ such that

is uniformly bounded, and to choose a

is bounded by 1, it

|c(cid:96)|∞
p(cid:96)

p(cid:96)

ρ(t) ≥ Ct

− q
2(q−1) , and hence it is necessary that q ∈ (2,∞) so that

2(q−1) ∈ ( 1

q

2 , 1).

Theorem 3.12. Consider the automatic diﬀerentiation function (3.10), and suppose
that Assumptions 3.1, 3.6 and 3.10 hold true.

(i) Then Assumptions 3.2 holds, and(cid:0)ψt,x, ˜ψt,xW(t, T t
(1))(cid:1)
(ii) If Assumption 3.10 holds with some q ≥ 2, then E(cid:2)|ψt,x|2(cid:3) < ∞.

(t,x)∈[0,T ]×Rd is uni-
formly integrable. Consequently, u(t, x) := E[ψt,x] is a viscosity solution of PDE (2.2).

(1), x, ∆W t

Proof. (i) First, using Proposition 3.7, it is clear that Assumption 3.2 holds true with
the choice of automatic diﬀerentiation function in (3.10).
(ii) Next, for q ≥ 1, let us introduce

By conditioning on F0, it follows from (3.11), together with direct computation, that

(cid:17)q(cid:105)

.

1

pIk

√

(cid:16)|cIk|∞
(1))(cid:12)(cid:12)q(cid:3) ≤ CE[χq∞],

∆Tkρ(∆Tk)

(1), x, ∆W t

(3.12)

(cid:16)

(cid:104)(cid:89)

k∈KT

χq∞ :=

C1,q

1

F (∆Tk)

(cid:17)q(cid:105)(cid:104) (cid:89)

C2,q

k∈KT \KT

E[|ψ|q] ≤ E[χq∞] and E(cid:2)(cid:12)(cid:12) ˜ψW(t, T t
(1))(cid:1)
(cid:88)

follows that(cid:0)ψt,x, ˜ψt,xW(t, T t
η(T ) = (cid:98)C1,q,

(1), x, ∆W t

η(cid:48)(t) +

for some constant depending only on the Lipschitz constant Lg.
(iii) When Assumption 3.10 (i) holds true for some q > 1, then it is clear that E[|ψ|q] ≤
1. Notice that the above argument is independent of the initial condition (0, x0), it

(iv) When Assumption 3.10 (ii) holds true for some q > 1. Consider the ODE on [0, T ]:

(t,x)∈[0,T ]×Rd is uniformly integrable.

(cid:98)C2,q (cid:107)c(cid:96)(cid:107)∞ η(t)|(cid:96)| = 0.

Under Assumption 3.10 (ii), it is clear that the above ODE admits a unique ﬁnite
solution on [0, T ]. We next introduce a sequence of random variables

(cid:104) (cid:89)

(cid:98)χq

n :=

(cid:98)C1,q

(cid:105)(cid:104) (cid:89)

k∈∪n

j=1Kj

T

F (∆Tk)

k∈∪n

j=1(Kj

T \Kj
T )

|cIk|∞
pIk

1

ρ(∆Tk)

(cid:105)

η(Tk−)

,

(cid:105)(cid:104) (cid:89)

k∈Kn+1

T

and

(cid:98)χq∞ := lim
n→∞(cid:98)χq

n =

(cid:104) (cid:89)

(cid:98)C1,q

(cid:98)C2,q

|cIk|∞
pIk

1

ρ(∆Tk)

(cid:105)

.

(cid:96)∈L

(cid:98)C2,q
(cid:105)(cid:104) (cid:89)

k∈KT

F (∆Tk)

k∈KT \KT

13

Then by the same arguments as in the proof of Proposition 3.4, it is easy to check that

(cid:90) T

0

η(0) = η(T ) +

1

∀n ≥ 1;

(cid:3),

n

(cid:88)

(cid:96)∈L

(cid:98)C2,q |c(cid:96)|∞ η(t)|(cid:96)|dt = E(cid:2)(cid:98)χq
(cid:3) = E(cid:2)(cid:98)χq
E(cid:2)(cid:12)(cid:12)ψ(cid:12)(cid:12)q(cid:3) ≤ E(cid:2)χq∞(cid:3) ≤ E(cid:2)(cid:98)χq∞(cid:3) ≤ lim inf
(cid:3) = η(0) < ∞.
E(cid:2)(cid:98)χq
E(cid:2)|ψt,x|q(cid:3) ≤ sup
(1))(cid:1)

(t,x)∈[0,T ]×Rd is uniformly integrable.

η(t) < ∞,

(1), x, ∆W t

sup

(t,x)∈[0,T ]×Rd

t∈[0,T ]

n→∞

n

(t,x)∈[0,T ]×Rd is uniformly integrable. The same arguments using (3.12)

and hence by direct computation, it follows that

Changing the origin from (0, x0) to (t, x), we see that

and hence(cid:0)ψt,x(cid:1)
show that(cid:0) ˜ψt,xW(t, T t

4 Further discussions

Representation of the PDE system Let us consider a PDE system (vj)j=1,··· ,n,
where for each j, vj : [0, T ] × Rd → R satisﬁes

∂tvj + µj · Dvj +

1
2

aj : D2vj + f (·, v1,··· , vn, Dv1,··· , Dvn) = 0,

for some diﬀusion coeﬃcient function (µj, aj) : [0, T ] × Rd −→ Rd × Sd, and some
polynomial function f : [0, T ]×Rd×Rn× (Rd)n −→ R. Our methodology immediately
applies to this context, and provides a stochastic representation for the solution of
the above PDE system, by means of a regime-changed branching diﬀusions: at every
branching time, the independent oﬀspring particles perform subsequently diﬀerent
branching diﬀusion regime.

Representation in view of unbiased simulation With the same idea of proof,
we can also obtain an alternative representation result, with a frozen coeﬃcient SDE
in place of SDE (2.4). When the coeﬃcient function a ≡ a0 for some constant a0 ∈ Sd,
this has signiﬁcant application in terms of Monte Carlo approximation, as it leads to
a representation random variable which can be simulated exactly, while the branching
diﬀusion process X k·
in (2.4) needs a time discretization technique and hence creates
some discretization error in the simulation. Let us present this alternative represen-
(cid:124)
tation formula in the case of constant diﬀusion coeﬃcient case, i.e. a ≡ a0 = σ0σ
0 for
some non-degenerate constant matrix σ0 ∈ Md.

Let(cid:98)L := L∪{∂}, where ∂ represents an artiﬁcial index; ˆp = (ˆp(cid:96))(cid:96)∈L be a probability
mass function and ((cid:98)I m,n)m,n≥1 be a sequence of i.i.d. random variables of distribution
ˆp, and independent of the sequences of i.i.d Brownian motion ((cid:99)W m,n)m,n≥1 and i.i.d
positive random variable ((cid:98)T m,n)m,n≥1 of density function ρ. Then following exactly
process, denoted by ((cid:98)Tk)
with branching type (cid:98)Ik := (cid:98)I n,πn(k). Here, when (cid:98)Ik =

the same procedure in Section 2.2, we can construct another age-dependent branching

k∈(cid:98)KT

14

(cid:16)

+

(4.2)

(ˆ(cid:96)0,··· , ˆ(cid:96)m) ∈ L, it produces |ˆ(cid:96)| oﬀspring particles, marked by i = 0,··· , m exactly as
in Step 3 in the construction of age-dependent process KT in Section 2.2; when Ik = ∂,

it produces only one oﬀspring particle, marked by m + 1. Then for every k ∈ (cid:98)KT , we
equipped it with an independent Brownian motion(cid:99)W k· as in (3.2). Next, let us deﬁne
0 = x0, and subsequently for every k ∈ (cid:98)KT ,
(cid:98)X (1)
)∆(cid:98)Tk + σ0∆(cid:99)W k
(cid:98)X k(cid:98)Tk
∆(cid:98)Tk

+ µ((cid:98)Tk−, (cid:98)X k(cid:98)Tk−

, with (cid:98)X k(cid:98)Tk−

:= (cid:98)X k(cid:98)Tk−

:= (cid:98)X k−(cid:98)Tk−

(4.1)

.

For this case, the automatic diﬀerentiation functions take a particularly simple formula,
which is compatible with the purpose of the unbiased simulation algorithm. Let us

introduce(cid:99)Wk

:= 1{θk=0} + bθk ((cid:98)Tk−, (cid:98)X k(cid:98)Tk−

∆(cid:99)W k
∆(cid:98)Tk
∆(cid:98)Tk
(cid:17) · (σ(cid:62)
) − µ((cid:98)T(k−)−, (cid:98)X k−(cid:98)T(k−)−

) · (σ(cid:62)

0 )−1

)

1{θk∈{1,··· ,m}}

∆(cid:99)W k
∆(cid:98)Tk
∆(cid:98)Tk

)

c(cid:98)Ik

(cid:105)

,

F (∆Tk)

0 )−1

1{θk=∂}.

(cid:99)Wk

), we obtain

and similarly

Tk−)1θk(cid:54)=0

(cid:99)Wk
ρ(∆(cid:98)Tk)

((cid:98)Tk, (cid:98)X k(cid:98)Tk
ˆp(cid:98)Ik

(cid:105)(cid:104) (cid:89)
k∈(cid:98)KT \(cid:98)KT

µ((cid:98)Tk−, (cid:98)X k(cid:98)Tk−
and below (3.4)) by ((cid:98)X k· ,(cid:99)Wk, ˆp(cid:98)Ik
Finally, setting c∂ ≡ 1, and replacing (X k· ,Wk, pIk ) in the deﬁnition of ψ and ˜ψ (in
T ) − g((cid:98)X k
g((cid:98)X k
(cid:98)ψ :=

(cid:104) (cid:89)
k∈(cid:98)KT
˜(cid:98)ψ.
motion ((cid:99)W k· )
k∈(cid:98)KT
:= x0 + µ0∆(cid:98)T(1) + σ0∆(cid:99)W (1)
(cid:98)X µ0,k(cid:98)T(1)
for k ∈ (cid:98)KT \{(1)} by the same induction relation as
and the subsequent process (cid:98)X µ0,k(cid:98)Tk
k as in (4.2) by replacing (cid:98)X k by (cid:98)X µ0,k, and replacing
in (4.1). We then introduce (cid:99)W µ0
µ((cid:98)T(k−)−, (cid:98)X k−(cid:98)T(k−)−
it deﬁnes a new random variable (cid:98)ψµ0. Finally, by changing the initial condition (0, x0)
, (cid:98)ψµ0,t,x etc.
and time interval [0, T ] to (t, x) and [t, T ], one obtains(cid:99)W t,k, (cid:98)T t

) by µ0 when k = (1). Replacing ((cid:98)X k,(cid:99)Wk) by ((cid:98)X µ0,k,(cid:99)W µ0
˜(cid:98)ψ

, and then introduce another diﬀusion process (cid:98)X µ0,k

Next, given a constant vector µ0 ∈ Rd, we keep the same branching Brownian

k, (cid:98)ψt,x,

k ) in (4.3),

·

by

,

∆T(1)

(4.3)

t,x

Proposition 4.1. Suppose that Assumptions 3.1 holds true, and the semilinear PDE
(2.2) has uniqueness for bounded viscosity solution. Suppose in addition that for every
(t, x) ∈ [0, T ] × Rd, and µ0 lies in a neighborhood of µ(t, x), one has

(cid:98)ψµ0,t,x and (cid:98)ψµ0,t,x∆(cid:99)W t,(1)
/∆(cid:98)Tt,(1)
∆(cid:98)T t
and (cid:0)˜(cid:98)ψ
∆(cid:99)W t,(1)
∆(cid:98)T t

((cid:98)ψt,x)(t,x)∈[0,T ]×Rd

t,x

(1)

(1)

is integrable,

/∆(cid:98)Tt,(1)

(cid:1)

(t,x)∈[0,T ]×Rd

and the family of random variables

15

are uniformly integrable with uniformly bounded expectation, deﬁne ˆu(t, x) := E[ψt,x].
Then the derivative Dˆu exists, ˆu and Dˆu are both continuous; and moreover, u is the
unique bounded viscosity solution of semilinear PDE (2.2).

(i) First, by the uniform integrability condition, ˆu is bounded

Sketch of proof.
continuous. Let us introduce

˜u(µ0, t, x) := E[(cid:98)ψµ0,t,x]

and ˆv(t, x) := E(cid:104)˜(cid:98)ψ

t,x

∆(cid:99)W t,(1)
∆(cid:98)T t

(1)

/∆(cid:98)Tt,(1)

(cid:105)

.

Notice that ˆv is uniformly bounded and continuous. Recall that W is a standard d-
dimensional Brownian motion independent of the branching diﬀusion process, we also
introduce

:= x + µ0(s − t) + σ0(Ws − Wt), s ∈ [t, T ],

where µ0 ∈ Rd is a constant vector in a neighborhood of µ(t, x). Then one obtains as
in (3.5) that

s

(cid:98)X t,x
˜u(µ0, t, x) = E(cid:104)

(1)

(cid:1)1{(cid:98)T t
g(cid:0)(cid:98)X t,x(cid:98)T t
1{(cid:98)I(1)=∂}
ρ((cid:98)T t
(1)=T} +
(cid:16) c(cid:98)I(1)
m(cid:89)
1{(cid:98)I(1)(cid:54)=∂}
ˆu(cid:98)I(1),0
ρ((cid:98)T t
ˆp(cid:98)I(1)
(cid:90) T
(cid:16)
(cid:1) +

(1))
(µ − µ0) · ˆv + f (·, ˆu, ˆv)

(1))ˆp∂

i=1

1
(1))

F ((cid:98)T t
g(cid:0)(cid:98)X t,x

+

T

t

= E(cid:104)

(cid:1)

(1), (cid:98)X t,x(cid:98)T t
(cid:1)1{(cid:98)T t

(1)

(1)

(1)<T}

(cid:105)

(cid:0)(µ − µ0) · ˆv(cid:1)(cid:0)(cid:98)T t
(cid:17)(cid:0)(cid:98)T t
(1), (cid:98)X t,x(cid:98)T t
(b · ˆv)(cid:98)I(1),i
(cid:17)(cid:0)s, (cid:98)X t,x
(cid:105)
(cid:1)ds
m(cid:89)
(cid:88)

.

s

c(cid:96) ˆu(cid:96)0

(cid:96)∈L

i=1

By standard argument, (t, x) (cid:55)→ ˜u(µ0, t, x) is a viscosity solution of

− ∂tu + µ0 · Du +

1
2

a0 : D2u + (µ − µ0) · ˆv +

(bi · ˆv)(cid:96)i = 0,

with terminal condition g. Since ˆu and ˆv are bounded continuous, the above PDE has
uniqueness for bounded viscosity solution, which induces that ˜u(µ0, t, x) is independent
of µ0 and ˆu(t, x) = ˜u(µ0, t, x) for µ0 in a neighborhood of µ(t, x).
(ii) We can then compute the derivative Dx ˜u(µ0, t, x) and then set µ0 := µ(t, x), it
follows that

Dˆu(t, x) = Dx ˜u(µ(t, x), t, x) = ˆv(t, x),

which is also bounded continuous. This implies that ˆu(t, x) is a viscosity solution
of (2.2), and we hence conclude the proof by uniqueness of the viscosity solution of
(2.2).

The integrability and square integrability of (cid:98)ψ can be analyzed in exactly the same
way as in Theorem 3.12. We just notice that the above deﬁned random variable (cid:98)ψ can
random variables (cid:98)I m,n and r.v. (cid:98)T m,n of distribution density function ρ. It is then in

be simulated exactly from a sequence of Gaussian random variable, discrete distributed

particular interesting to serve as a Monte-Carlo estimator for u(0, x0).

16

On the representation of fully nonlinear PDEs Formally, one can also
obtain a representation result for fully nonlinear PDE, using the same automatic dif-
ferentiation functions of order 2. However, this raises a serious integrability problem
which can not be solved by conditions as in Assumption 3.10. To illustrate the main
diﬃculty, let us consider the following PDE in the one-dimensional case d = 1:

u(T, x) = g(x),

∂tu +

D2u + f0(D2u) = 0, on [0, T ] × Rd,

(4.4)

1
2

where f0(γ) = c0γ for some constant c0 > 1
2 . Notice that there is only one term
in function f0, then a natural guess for the representation is to consider a branching
Brownian motion with exactly one oﬀspring particle at every arrival time. This can
be seen as a Brownian motion W equipped with a sequence of random time mark
(Ti)i=1,··· ,NT , where

Ti

τ j,1, NT := inf

i : Ti ≥ T

Notice that for any t > 0 and bounded measurable function φ : R → R, one has

Then arguing as in Theorem 3.5, we may expect that u(0, x0) = E[(cid:98)ψ(cid:3), with

φ(x + Wt)

∂2
xx

.

(cid:110)

(cid:111)

.

(cid:105)

t − t
W 2
t2

j=1

:= T ∧ i(cid:88)
E(cid:2)φ(x + Wt)(cid:3) = E(cid:104)
NT −1(cid:89)

1

F (T − TNT −1)

i=1

(cid:98)ψ := g(x + WT )

(WTi+1 − WTi)2 − (Ti+1 − Ti)

(Ti+1 − Ti)2ρ(Ti − Ti−1)

,

c0

simplicity, let g ≡ 1, and notice that F ≤ 1. Then by taking conditional expectation,

provided that (cid:98)ψ is integrable. However, the integrability of (cid:98)ψ could fail in general. For
one has, for some constant C > 0 and c1 := E(cid:2)(cid:12)(cid:12)c0(W 2
E(cid:2)|(cid:98)ψ|(cid:3) ≥ E(cid:104) NT −1(cid:89)
(cid:105)
≥ E(cid:104) c1
≥ CE(cid:104)

(Ti+1 − Ti)ρ(Ti − Ti−1)
c1

1 − 1)(cid:12)(cid:12)(cid:3), that

1{T1≤T /2,T2−T1<T /2,T3−T2≥T}

(T2 − T1)ρ(T2 − T1)

(cid:90) T /2

dt = ∞.

T3 − T2

ρ(T1)

(cid:105)

1{T2−T1<T /2}

= C

(cid:105)

i=1

c1

c1

c1

(T2 − T1)ρ(T2 − T1)

1
t

0

Of course, for linear PDEs as in (4.4), one can simulate a Brownian motion with
volatility coeﬃcient 1 + 2c0 whenever 1 + 2c0 > 0 to obtain the solution. But it is not
the case for general fully nonlinear PDEs.

On the representation results by BSDE Another probabilistic representation
of semilinear parabolic PDE is the Backward Stochastic Diﬀerential Equation (BSDE)
proposed by Pardoux and Peng [20]. Namely, given a classical solution u of semilinear
PDE (2.2), we deﬁne

(Yt, Zt) := (cid:0)u(t, X

0,x0)(cid:1).

0,x0
t

), σDu(t, X

17

Then (Y, Z) provides a solution to BSDE

Yt = g(X

0,x0
T

) +

0,x0
s

, Ys, σ−1(s, X

0,x0
s

)Zs

(cid:90) T

t

f(cid:0)s, X

(cid:1)ds − ZsdWs,

t ∈ [0, T ], P-a.s.

Based on the discretization technique on the BSDE, one can then obtain a prob-
abilistic numerical solution for semilinear parabolic PDEs, see e.g. Bouchard and
Touzi [5], and Zhang [28], etc. Generally speaking, these numerical schemes for BSDE
need a (time-consuming) simulation-regression technique to compute the conditional
expectation appearing in the schemes.

Our representation result induces a pure Monte Carlo simulation algorithm, which
avoids the regression procedure in the numerical schemes of BSDEs. Nevertheless, our
numerical method provides only the solution of PDE at time 0, and it needs some
restrictive conditions on the coeﬃcient functions f such as Assumption 3.10 to obtain
a ﬁnite variance estimator. We will provide more numerical examples as well as some
variance reduction techniques in Section 5 below.

5 A Monte Carlo algorithm

5.1 The implementation of the numerical algorithm

The above representation result in Theorem 3.5 induces a Monte Carlo algorithm to

compute the solution of PDE (2.2), by simulating the random variable ψ or (cid:98)ψ. We

provide here some discussion on the implementation of the numerical algorithm.

The choice of density function ρ As discussed in Remark 3.11, to ensure
Assumption 3.10, a necessary condition is to choose ρ(t) ≥ Ct−1/2. A natural candidate
as distribution, which is also easy to be simulated, is the gamma distribution Γ(κ, θ),
with κ ≤ 1

2 , whose density function is given by

1

ρ0(t) =

where Γ(κ) :=(cid:82) ∞
(cid:90) ∞
0 sκ−1e−sds. In particular, one has

Γ(κ)θκ tκ−1 exp(−t/θ)1{t>0},

F k :=

ρ0(t)dt = 1 − γ(κ, ∆Tk/θ)

Γ(κ)

∆Tk

, where γ(κ, t) :=

(5.1)

(cid:90) t

0

sκ−1e−sds.

Complexity The dimension d of the problem, the choice of (p(cid:96))(cid:96)∈L and ρ will of
course inﬂuence the complexity of algorithm. First, the complexity is proportional
to the number of particles in the branching process, i.e. #KT , and for each particle,
the complexity of simulation and calculation is of order Cd2. Let us denote n0 :=

(cid:96)∈L p(cid:96)|(cid:96)| and m(t) := E(cid:2)#Kt
(cid:80)

(cid:3).

Proposition 5.1. (i) The function m(t) is given by

∞(cid:88)

0F ∗,k(t), where F ∗,k(t) := P(cid:2)τ 1,1 + ··· + τ 1,k < t(cid:3).

nk

m(t) =

(5.2)

k=0

18

(ii) Let ρ be given by (5.1), then F ∗,k(t) = 1

Γ(kκ) γ(kκ, x/θ) and hence

∞(cid:88)

m(t) =

γ(kκ, t/θ)nk
0

.

Γ(kκ)

(i) Using Lemma 4.4.3 of Athreya and Ney [1], one has that satisﬁes the

(cid:82) t
Proof.
0 m(t− s)ρ(s)ds, whose solution is given explicitly by (5.2).
equation m(t) = 1 + n0
Further, when ρ is the density function of Gamma distribution, the function F ∗,k(t)
can be computed explicitly.

k=0

α +

σ2
2

+ c sin(x1 + ··· + xd)

3d + 1

2d

eα(T−t)(cid:17)

eα(T−t).

5.2 A high dimensional numerical example
We ﬁrst focus on a simple numerical example in high dimension. Let (µ, σ) ≡ (0, σ0)
Id, and f (t, x, y, z) = k(t, x) + cy(b · z), where
for some constant matrix σ0 = 1√
b := 1

d

d (1 + 1

d , 1 + 2

(cid:16)
d ,··· , 2) and
k(t, x) := cos(x1 + ··· + xd)

With terminal condition g(x) = cos(x1 + ··· + xd), the explicit solution of semilinear
PDE (2.2) is given by

u(t, x) = cos(x1 + ··· + xd)eα(T−t).

In our numerical experiment, we set α = 0.2, c = 0.15, T = 1, and x0 = 0.51Id, where
1Id stands for the unit vector in Rd for d = 5, 10 and 20. We would like to emphasize
that, to the best of our knowledge, no alternative methods are available for solving
such a high-dimensional semilinear PDE. In Table 1, we report the analytic solution of
the semilinear PDE and that of the corresponding linear PDE by setting c = 0. The
diﬀerent results indicate that the nonlinearity term has an impact.

5
Dimension
Linear Solution
-1.0436
Non linear solution -0.97851

10
0.3106
0.34646

20
-0.9661
-1.0248

Table 1: Analytical solution for the linear PDE (i.e., c = 0) versus analytical solution for
the semilinear PDE in d = 5, 10 and 20.

For numerical implementations, we use gamma distribution (5.1), with κ = 0.5
and θ = 2.5. On each test performed, a computation is achieved with n particles.
An estimation E with n particles is then calculated. The standard deviation of E
is estimated with 1000 runs of n particles and its log-plot is reported below on the
diﬀerent ﬁgures for diﬀerent values of n. We also show on some ﬁgures the convergence
of the solution obtained on the average of the 1000 runs.

On Figures 1, 2, 3, we illustrate that the Monte Carlo method converges easily to
our analytic solution. Computational costs are estimated on one core of a Laptop core
I7 processor 2.2 GHz and are reported in Table 2 for a number of simulations equal to
96000 permitting to get a solution with an error less than 0.1%.

19

Figure 1: Estimation and standard deviation observed in d = 5 depending on the log of the
number of particles used.

Figure 2: Estimation and standard deviation observed in d = 10 depending on the log of
the number of particles used.

Figure 3: Estimation and standard deviation in d = 20 depending on the log of the number
of particles used.

6 Some extensive tests

This section is devoted to additional tests. Having illustrated previously that our
algorithm is eﬃcient for solving high-dimensional semilinear PDEs, we focus on some
examples from dimension 1 to 3. Note that our results have been benchmarked against

20

Dimension 5
Time

550

10
717

20
956

Table 2: Computational time in seconds for 96000 trajectories computed 1000 times on one
core for κ = 0.5, θ = 2.5.

0,x

t = (1 − e−t)1I + e−tx + σ

Z ∼ N (0, Id).

(6.1)

(cid:114)

1 − e−2t

Z,

2

a Finite Diﬀerence method in d = 1 and d = 2. Unfortunately, the ﬁnite diﬀerence
(cid:80)d
method is no more available in d = 3. All our numerical examples share the following
characteristics: µ(t, x) = 1 − x, σ ≡ 0.5Id and x0 = 1I. T is chosen equal to 1,
i x(i)− 1)+ for x ∈ Rd. Notice that with the above coeﬃcients, SDE (3.1)
g(x) = ( 1
d
is a linear SDE, whose solution can exactly simulated:

X

(cid:113) e2t−1

2

The Malliavin weight used in the algorithm can be computed explicitly and is given

(cid:1). We will compare numerical results from four diﬀerent schemes.

by Z/(cid:0)σ

• (scheme a) using the representation (3.4) with the explicit solution (6.1) of the

SDE (3.1).

• (scheme b) using the representation (4.3) with freezing coeﬃcient techniques.
• (scheme c) using the representation (4.3), enhanced by the resampling scheme

(see Appendix for more details).

(cid:80)

• (scheme d) using the representation (3.4), enhanced by the resampling scheme.
The density function ρ is that of the gamma law with parameters κ and θ. If not
indicated, the parameters of the law are set to κ = 0.5 and θ = 2.5 and the probability
pl are chosen equal. On each test, a calculation is achieved with n particles (starting
with n = 1562 for scheme a and with n = 100000 for schemes b and c). The n particles
are shared on 96 processors and each processor i calculates an estimation Ei of the
solution with n
96 particles. Then an estimation E with n particles is achieved with
E = 1
i Ei. When importance sampling is used, in order to avoid communications
96
that breaks parallelism, it is used on each processor so with n
96 particles on each
processor. The standard deviation of E is estimated with 1000 runs of n particles and
its log is reported on the diﬀerent ﬁgures below for diﬀerent values of n. We expect that
by quadrupling the values of n, the standard deviation std divides by a factor 2 and
the plot (log(n), log(std)) should be linear with a slope equal to − 1
2 . The theoretical
rate of convergence is also plotted on each ﬁgure (as in our previous example, the
solutions are obtained on the average of the 1000 runs).

6.1 Some examples in one space dimension

• For d = 1, we take f (t, x, y, z) := 0.2y2 + 0.3y3. Results on Figure 4 show
that the method converges. Scheme a is far more eﬀective than scheme b and
that the importance sampling of scheme c is eﬀective. The log of the standard

21

deviation decreases for all schemes linearly with the log of the particle number
as predicted by the theory. Note that the computational cost for 1000 runs with
25000 particles on one core is equal to 490 seconds for scheme a, 200 seconds
with scheme b and 260 seconds with scheme c.

Figure 4: Estimation and standard deviation obtained in d = 1 for f (t, x, y, z) := 0.2y2 +
0.3y3

• As a second example in d = 1, we take a Burgers type nonlinearity f (t, x, y, z) =
0.15yz. Results on Figure 5 show that all the schemes converge to our numerical
ﬁnite diﬀerence solution. Note that the computational cost for 1000 runs with
25000 particles on one core is roughly equal to 200 seconds for scheme a, 100
seconds for scheme b, 300 seconds for scheme c.

Figure 5: Estimation and standard deviation obtained in d = 1 for f (t, x, y, z) = 0.15yz.

• As a third example in d = 1, we keep the same nonlinearity with f (t, x, y, z) =
0.3yz. We expect that the variance of the results will be higher than in the
previous case. This is observed in Figure 6. Scheme a still converges. Scheme
b converges slowly and Importance Sampling of scheme c permits to get faster
convergence and to recover the good rate in the log of the standard deviation
decay. The computational times are the same as in our previous test.

• As a fourth example in d = 1, we take a nonlinearity with f (t, x, y, z) = 0.08z2.
Results are shown in Figure 7. The importance sampling of scheme c is required

22

Figure 6: Estimation and standard deviation obtained in d = 1 for f (t, x, y, z) = 0.3yz.

to achieve proper convergence. Scheme a converges quickly. Computational times
are the same as before (same type of branching).

Figure 7: Estimation and standard deviation obtained in d = 1 for f (t, x, y, z) = 0.08z2.

• As a last example in d = 1, we keep the same type of nonlinearity f (t, x, y, z) =
0.2z2. Schemes b and c don’t converge anymore. We only test scheme a using
diﬀerent values for the parameters κ and θ (see Figure 8). The change in θ does
not seem to change convergence properties. The change in κ (from 0.5 to 0.4)
does not seem to modify our results. However, some tests, not reported here,
show that the variance can increase a lot using κ around 0.25. Then, as the
average jump size is proportional to θ, it is more eﬃcient to take some quite high
values for θ in order to reduce the computational time. For the same reason, it
is optimal to choose a κ equal to 0.5. In Table 3, we report the computational
time, associated to diﬀerent choices of (κ, θ), as a multiplicative factor of the
computational eﬀort with benchmark parameters κ = 0.5, θ = 2.5.

We notice that for all the parameters, the decay in the variance is far from the expected
theoretical one (see Figure 8). We then use our benchmark parameters and compare
the results obtained using scheme a and scheme d (importance sampling is used here).
Results are reported on Figure 9. They illustrate that the importance sampling method
allows to improve the convergence rate.

23

Figure 8: Estimation and standard deviation obtained in d = 1 for f (t, x, y, z) = 0.2z2.
Diﬀerent values for κ and θ are used.

Table 3: Computational time, associated to diﬀerent choices of (κ, θ), as a multiplicative
factor of the computational eﬀort with benchmark parameters κ = 0.5, θ = 2.5

κ
θ
Time

0.5
1
6.63

0.5
2.5
1

0.5
5
0.49

0.4
2.5
2.85

0.4
5.
1.02

Figure 9: Estimation and standard deviation obtained in d = 1 for f (t, x, y, z) = 0.2z2 with
and with out importance sampling.

6.2 Some examples in two space dimensions

Although the eﬃciency of our algorithm was illustrated on our previous experiments,
this Monte-Carlo method cannot compete a PDE deterministic methods in d = 1. In
this section, we focus on d = 2, where advantages of PDE implementation remain but
are not so obvious.

• For the ﬁrst example in d = 2, we take f (t, x, y, z) := 0.15y1I.z. On Figure 10,
we give the results obtained using our three schemes showing that Importance
Sampling is needed. Note that the computation cost for 1000 runs with 25000
particles on one core is roughly equal to 230 seconds for scheme a, 90 seconds for

24

scheme b, 580 seconds for scheme c.

Figure 10: Estimation and standard deviation obtained in d = 2 for f (t, x, y, z) := 0.15y1I.z.

• For the second example in d = 2, we take f (t, x, y, z) := 0.04(z.1I)2. The conver-
gence of Scheme a is easily achieved while Scheme b converge poorly as shown
in Figure 11. Importance sampling method improve the convergence. Computa-
tional costs are the same as in our ﬁrst d = 2 tests.

Figure 11: Estimation and standard deviation obtained in d = 2 for f (t, x, y, z) :=
0.04(z.1I)2.

• For the third example, we test the inﬂuence of the coeﬃcients on Scheme a
for a non linearity f (t, x, y, z) := K(z.1I)2 with K = 0.05, K = 0.1, K = 0.2.
Using Scheme b and c, we cannot get proper convergence due to high variances
observed. On Figure 12, we give the convergence obtained with the diﬀerent K
values and on Figure 13 the standard deviation associated. As the coeﬃcients
grow, the variance of the results gets higher preventing the method from converge
when K = 0.2.

• At last we test the inﬂuence of the function g. The representation of the solu-
tion involves the product of g functions so we expect that the variance of the
(cid:80)d
result is highly sensitive to the scaling of this function. Here we choose to keep
f (t, x, r, p) := 0.05(Du.1I)2 and take diﬀerent values for the g function. On ﬁgure
i x(i) − 1)+ and give the convergence of schemes a and
14 we take g(x) = 2( 1
d

25

Figure 12: Convergence of scheme a for diﬀerent K values.

Figure 13: Standard deviation of the scheme a for diﬀerent K values.

b and the standard deviation associated. Comparing to ﬁgure 13 (K = 0.05), we
(cid:80)d
see a net increase in the variance of the result for scheme a. When importance
sampling is used (scheme d) the decay in term of variance is more regular. In-
i x(i) − 1)+, we give the results
creasing the function g such that g(x) = 3( 1
d
obtained on ﬁgure 15. Here importance sampling is really necessary to recover a
good rate of convergence.

(cid:80)d
i x(i) − 1)+.

Figure 14: Estimation and standard deviation observed in dimension 2 for case 4, g(x) =
2( 1
d

6.3 An example in three space dimensions

We take f (t, x, y, z) := 0.15(z.1I)2. Results are given on Figure 16, still showing that
importance sampling is necessary while using discretization of the scheme and that the
exact scheme has a lower variance.

26

(cid:80)d
i x(i) − 1)+.

Figure 15: Estimation and standard deviation observed in dimension 2 for case 4, g(x) =
3( 1
d

Figure 16: Estimation and standard deviation observed in dimension 3 for case 1.

A Resampling scheme for branching processes

Notice that our estimator (3.4) and (4.3) are provided as a product of some random
variables. Then similar to Doumbia, Oudjane and Warin [7], one can use the re-
sampling scheme (or interacting particle systems), see Del Moral [6]. Intuitively, this
scheme replaces the expectation of a product by a product of expectations, which
potentially stabilizes the Monte-Carlo estimator.

Let us ﬁrst introduce the Markov chain (Xn)n≥1, taking values in ∪p≥1([0, T ]2 ×
R2d ×{0,··· , m}× L)p such that X1 = (0, 0, x0, x0, 0, I0) with I0 = (1, 0,··· 0) ∈ Nm+1
and for any n ≥ 1, one deﬁnes

(cid:16)Xn, (Tk−, Tk, X k

Xn+1 :=

(cid:17)

.

Tk− , X k
Tk

, θk, Ik)k∈∪n

p=1Kp

T

Notice that this Markov chain has an absorbing state since for any ω ∈ Ω there is
a generation n(ω) for which all branches have died (either having no oﬀspring before
reaching T or having reached T ) implying Kn+1
(ω) = ∅ and consequently Xn+1(ω) =
Xn(ω). Then (Xn)n≥0 is a Markov chain. We next introduce

T

Gn(Xn) :=

(cid:104)(cid:89)

k∈Kn

T

g(X k

T ) − g(X k
F (∆Tk)

Tk−)1{θk(cid:54)=0}

Wk

27

(cid:105)(cid:104) (cid:89)

cIk (Tk, X k
Tk

)

k∈(Kn

T \Kn
T )

pIk

(cid:105)

Wk
ρ(∆Tk)

, (A.1)

so that

∞(cid:89)

n=1

ψ =

Gn(Xn).

Notice that the above representation consists of a product from contributions from
each generation n ≥ 1. Since the number of generation prior to the maturity T is
ﬁnite a.s., the last product only involves ﬁnite number of terms, a.s. We also observe
that except for the trivial case of constant function g, E[|Gn(Xn)|] (cid:54)= 0. By iteration,
it is easy to see that

(cid:16) ∞(cid:89)

EPn−1(cid:2)(cid:12)(cid:12)Gn(Xn)(cid:12)(cid:12)(cid:3)(cid:17) EP∞(cid:104) ∞(cid:89)

n=1

n=1

(cid:105)

sgn(Gn(Xn))

,

EP0[ψ] =

where given P0, one deﬁnes Pn by dPn
dPn−1

:=

|Gn(Xn)|

EPn−1 [|Gn(Xn)|]

, for n ≥ 1.

The particle algorithm consists in simulating the dynamics of an interacting particle
), on ∪p≥1([0, T ]2 × R2d × {0,··· , m} × L)p, from step

n = 1 to n = ∞ and then to approximate each expectation EPn−1(cid:2)(cid:12)(cid:12)Gn(Xn)(cid:12)(cid:12)(cid:3) by the

system of size N , (ξ1,N

,··· ξN,N

p

p

empirical mean value of the simulation. The algorithm can be given as an iteration of
the following two steps, initiated by n = 1,
Selection step Given N copies of simulation (ξi,N

n )i=1,··· ,N of Xn, one draws ran-
domly and independently N particles among the current particle system with a

probability

(cid:80)N
|Gp(ξi,N
)|
j=1 |Gp(ξi,N

p

p

;

|)

Evolution step Each new selected particle evolves randomly and independently ac-

cording to the transition of the Markov chain (Xn) between n and n + 1.

Finally u(0, x0) is approximated as a product of empirical averages:

.

(A.2)

∞(cid:89)

(cid:16) 1

N(cid:88)

n )|(cid:17)(cid:16) 1
N(cid:88)
ﬁnite generation, then the above product(cid:81)∞
(cid:81)nN
n=1, where nN := inf{n ≥ 1| ξi,N

|Gn(ξi,N

n=1

i=1

N

N

n

∞(cid:89)

n )(cid:1)(cid:17)
sgn(cid:0)Gn(ξi,N

i=1

n=1

Notice again that, for every simulation (ξi,N ), the maturity T is attained for some
n=1 can be restricted to the a ﬁnite product

has reached T for all i = 1,··· N}.

References

[1] K. B. Athreya and P. E. Ney, Branching processes, Springer-Verlag, New York,

1972. Die Grundlehren der mathematischen Wissenschaften, Band 196.

[2] V. Bally and P. Pages, Error analysis of the quantization algorithm for obstacle

problems, Stochastic Processes & Their Applications, 106(1), 1-40, 2003.

[3] H. Bauke and S. Mertens,Random numbers for large-scale distributed Monte

Carlo simulations, Physical Review E, 75(6):066701, 2007.

[4] B. Bouchard, I. Ekeland and N. Touzi, On the Malliavin approach to Monte Carlo
approximation of conditional expectations, Finance and Stochastics, 8, 45-71, 2004.

28

[5] B. Bouchard and N. Touzi, Discrete-time approximation and Monte-Carlo sim-
ulation of backward stochastic diﬀerential equations, Stochastic Process. Appl.,
111(2):175-206, 2004.

[6] P. Del Moral, Feynman-Kac formulae, Genealogical and interacting particle sys-
tems with applications, Probability and its Applications , Springer-Verlag, New
York, 2004.

[7] M. Doumbia, N. Oudjane and X. Warin, Computing expectations for general SDE

with pure Monte Carlo methods, preprint, 2016.

[8] E. B. Dynkin, Superdiﬀusions and positive solutions of nonlinear partial diﬀeren-
tial equations, Appendix A by J.-F. Le Gall and Appendix B by I. E. Verbitsky.
University Lecture Series, 34. American Mathematical Society, 2004.

[9] A. M. Etheridge, An Introduction to Superprocesses, American Mathematical So-

ciety, 2000.

[10] A. Fahim, N. Touzi, and X. Warin. A probabilistic numerical method for fully

nonlinear parabolic PDEs. The Annals of Applied Probability, 1322-1364, 2011.

[11] Fourni´e, E., Lasry, J.M. , Lebuchoux, J., Lions, P.L, Touzi, N. : Applications of
Malliavin Calculus to Monte Carlo Methods in Finance., Finance and Stochastics,
3, 391-412, 1999.

[12] T. E. Harris, The Theory of Branching Processes, Die Grundlehren der Mathe-

matischen Wissenschaften 119, Springer, Berlin, 1963.

[13] P. Henry-Labord`ere, Cutting CVA’s Complexity, Risk magazine (Jul 2012). Ex-
tended version: Counterparty risk valuation: A marked branching diﬀusion ap-
proach, ssrn, http://ssrn.com/abstract=1995503.

[14] P. Henry-Labord`ere, X. Tan, N. Touzi. A numerical algorithm for a class of
BSDEs via the branching process, Stochastic Processes and their Applications,
124(2):1112-1140, 2014.

[15] P. Henry-Labord`ere, X. Tan, N. Touzi. Unbiased simulation of stochastic diﬀer-

ential equations, preprint, 2015.

[16] G. Kersting and F.C. Klebaner, Sharp conditions for nonexplosions and explosions

in Markov jump processes, Anna. Proba. 23(1), 268-272, 1995.

[17] P.E. Kloeden and E. Platen, Numerical Solution of Stochastic Diﬀerential Equa-

tions, Stochastic Modelling and Applied Probability, Vol. 23, Springer, 1992.

[18] H. P. McKean, Application of Brownian motion to the equation of Kolmogorov-

Petrovskii-Piskunov, Comm. Pure Appl. Math., Vol 28, 323-331, 1975.

[19] D. Nualart. The Malliavin calculus and related topics. Vol. 1995. Berlin: Springer,

2006.

[20] E. Pardoux and S. Peng, Adapted solutions of backward stochastic diﬀerential

equations, System and Control Letters, 14, 55-61, 1990.

29

[21] A. Rasulov, G. Raimova, and M. Mascagni, Monte Carlo solution of Cauchy prob-
lem for a nonlinear parabolic equation. Mathematics and Computers in Simulation,
80(6):1118-1123, 2010.

[22] G. Teschl, Ordinary Diﬀerential Equations and Dynamical Systems, American

Mathematical Society, Graduate Studies in Mathematics, Volume 140, 2012.

[23] A.V. Skorokhod Branching diﬀusion processes. Theory of Probability & Its Ap-

plications, 9(3):445-449, 1964.

[24] D. W. Stroock, S. R. S. Varadhan, Multidimensional Diﬀusion Processes,

Springer, 1979.

[25] J. Zhang, A numerical scheme for backward stochastic diﬀerential equations, An-

nals of Applied Probability, 14(1), 459-488, 2004.

[26] A. Rasulov, G. Raimova, M. Mascagni, Monte Carlo solution of Cauchy problem
for a nonlinear parabolic equation, Mathematics and Computers in Simulation,
80(6), 1118-1123, 2010.

[27] S. Watanabe, On the branching process for Brownian particles with an absorbing

boundary. Journal of Mathematics of Kyoto University, 4(2):385-398, 1965.

[28] J. Zhang, A numerical scheme for backward stochastic diﬀerential equations, An-

nals of Applied Probability, 14(1), 459-488, 2004.

30

