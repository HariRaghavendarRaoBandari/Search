Recurrent Dropout without Memory Loss

Stanislau Semeniuta1 Aliaksei Severyn2 Erhardt Barth1
1Universit¨at zu L¨ubeck, Institut f¨ur Neuro- und Bioinformatik

{stas,barth}@inb.uni-luebeck.de

2Google Research

severyn@google.com

6
1
0
2

 
r
a

 

M
6
1

 
 
]
L
C
.
s
c
[
 
 

1
v
8
1
1
5
0

.

3
0
6
1
:
v
i
X
r
a

Abstract

This paper presents a novel approach to re-
current neural network (RNN) regulariza-
tion. Differently from the widely adopted
dropout method, which is applied to for-
ward connections of feed-forward archi-
tectures or RNNs, we propose to drop neu-
rons directly in recurrent connections in a
way that does not cause loss of long-term
memory. Our approach is as easy to imple-
ment and apply as the regular feed-forward
dropout and we demonstrate its effective-
ness for the most popular recurrent net-
works: vanilla RNNs, Long Short-Term
Memory (LSTM) and Gated Recurrent
Unit (GRU) networks. Our experiments
on three NLP benchmarks show consistent
improvements even when combined with
conventional feed-forward dropout.

1 Introduction

have

and for

recently
Recurrent Neural Networks
gained increased attention in the NLP com-
munity for their superior ability to model and
learn from sequential data,
show-
ing state-of-the-art
results on various public
benchmarks ranging from sentence classiﬁca-
Irsoy and Cardie, 2014;
tion (Wang et al., 2015;
Liu et al., 2015)
prob-
language mod-
lems
elling
Zhang et al., 2015),
(Zhang and Lapata, 2014)
text
and
prediction
tasks (Sutskever et al., 2014).

(Dyer et al., 2015)
(Kim et al., 2015;
generation

sequence-to-sequence

and

various

tagging

to

Having shown excellent ability to capture and
learn complex linguistic phenomena, RNN archi-
tectures, like any other type of deep learning mod-
els, are prone to overﬁtting, especially when the
number of training examples at hand is limited.

Among the ﬁrst and most widely used tech-
niques to avoid overﬁtting and increase general-
izability of neural networks is the dropout regu-
larization (Hinton et al., 2012). Since its introduc-
tion, it has become, together with the L2 weight
decay, the de-facto standard neural network regu-
larization method.

limited with unclear beneﬁts.

While showing signiﬁcant improvements when
used in feed-forward architectures, e.g., Convolu-
tional Neural Networks (Krizhevsky et al., 2012),
in RNNs has been
the application of dropout
somewhat
In-
deed, so far dropout in RNNs has been applied
in the same fashion as in feed-forward architec-
tures:
it is typically injected between input-to-
hidden, hidden-to-hidden, and hidden-to-output
layers, i.e., along the input axis, but not between
the recurrent connections (time axis). Given that
RNNs are mainly used to model sequential data
with the goal of capturing short- and long-term in-
teractions, it seems natural to also regularize the
recurrent weights. This observation has led us to
the idea of applying dropout to the recurrent con-
nections in RNNs.

Hence, in this paper we propose an idea of a re-
current dropout and give answers to the following
important questions: (i) how to apply the dropout
in recurrent connections of different RNN archi-
tectures, e.g., vanilla RNNs, LSTMs and GRUs
in a way that prevents possible corruption of the
long-term memory; (ii) what is the relationship
between our recurrent dropout and the widely
adopted dropout in input-to-hidden and hidden-to-
output connections; (iii) how the dropout mask in
RNNs should be sampled: once per step or once
per sequence. The latter question of sampling the
mask appears to be crucial in some cases to make
the recurrent dropout work and, to the best of our
knowledge, has received very little attention in the
literature.

Regarding empirical evaluation, we ﬁrst high-
light the problem of information loss in memory
cells of LSTMs when applying recurrent dropout.
We demonstrate that previous approaches of drop-
ping hidden state vectors cause loss of mem-
ory while our proposed method to use dropout
mask in hidden state updates does not suffer
from this problem. We also experiment on three
widely adopted NLP tasks: Language Modelling,
Named Entity Recognition and Twitter Sentiment
Analysis. The results demonstrate that recur-
rent dropout helps to achieve better regulariza-
tion and yields improvements across all the tasks,
even when combined with the conventional feed-
forward dropout.

The paper is structured as follows: Section 2
brieﬂy discusses previous efforts to improve reg-
ularization of recurrent architectures, Section 3
introduces RNN architectures used in this paper
and gives the detailed explanation of our recurrent
dropout technique, Section 4 provides experimen-
tal results, while Section 5 concludes the paper.

2 Related Work

Deep Neural Networks are machine-learning mod-
els with high capacity, i.e., they typically opti-
mize a very large number of parameters. This
often leads to overﬁtting, especially, when the
amount of training data is small and has led to
a lot of research directed towards improvement
of neural networks generalization. Since dropout
has proven to be a very effective and easy-to-use
method, a number of works improving this method
have been published. We will focus on the ones re-
lated to recurrent neural network regularization.

Pham et al. (2013) and Zaremba et al. (2014)
have shown that LSTMs can be effectively reg-
ularized when using dropout in forward connec-
tions. While this already allows for effective regu-
larization of recurrent networks, it is intuitive that
introducing dropout also in the hidden state may
force it to create more robust representations. In-
deed, Moon et al. (2015) have extended the idea
of dropping neurons in forward direction and pro-
posed to drop cell states as well. The authors are
able to achieve good results on a Speech Recogni-
tion task but their approach has a number of lim-
itations. We will discuss these limitations later in
Section 3.4. Bluche et al. (2015) carry out a study
to ﬁnd where dropout is most effective, e.g. input-
to-hidden or hidden-to-output connections. The

authors conclude that it is more beneﬁcial to use
it once in the correct spot, rather than put it ev-
erywhere. Bengio et al. (2015) have proposed an
algorithm called scheduled sampling to improve
performance of recurrent networks on sequence-
to-sequence labeling tasks. A disadvantage of this
work is that the scheduled sampling is speciﬁcally
tailored to this kind of tasks, what makes it im-
possible to use in, for example, sequence-to-label
tasks.

The main contribution of this paper is the intro-
duction of a new regularization technique speciﬁ-
cally tailored for RNNs – recurrent dropout. It is
based on a widely used dropout method which has
been proven to be highly effective and easy to ap-
ply. The main difference between our approach
and the existing approaches to RNN regulariza-
tion is that we apply the dropout directly to the
recurrent connections. We demonstrate that ap-
plying dropout to arbitrary vectors in LSTM and
GRU cells may lead to loss of memory thus hin-
dering the ability of the network to encode long-
term information. In other words, our technique
allows for adding a strong regularizer on the model
weights responsible for learning short and long-
term dependencies without affecting the ability to
model long-term relationships, which are espe-
cially important to model when dealing with natu-
ral language. We demonstrate that our approach is
effective when applied to LSTMs, GRUs and plain
RNNs.

3 Recurrent Dropout

This section describes how dropout could be
applied to recurrent connections of three most
widely used types of recurrent nets: vanilla RNNs,
LSTMs and GRUs.

3.1 Dropout
Dropout (Hinton et al., 2012) is one of the few
effective tools today for regularizing neural net-
works. It is widely adopted in feed-forward archi-
tectures applied to various layers:

y = f (Wd(x)),

(1)

where x is the layer’s input, W is the matrix of
weights, f is an activation function, y is the vector
of activations, and d is dropout operation deﬁned
as:

d(x) =(mask ∗ x,

(1 − p)x

if train phase
otherwise,

(2)

where p is the dropout rate and mask is a vector,
sampled from the Bernoulli distribution with suc-
cess probability 1 − p.

3.2 Dropout in vanilla RNNs
Vanilla RNNs were one of the ﬁrst ideas for mod-
eling sequential data with neural networks. For-
mally, RNNs process the input sequences as fol-
lows:

ht = f (Wh[xt, ht−1] + bh),

(3)

where xt is the input at time step t; ht and ht−1
are hidden vectors that encode the current and
previous states of the network; Wh is parameter
matrix that models input-to-hidden and hidden-to-
hidden (recurrent) connections; b is a vector of
bias terms, and f is the activation function.

As RNNs model sequential data by a fully-
connected layer, we apply dropout in the same
way as it is applied in the feed-forward networks.
Speciﬁcally, we modify Equation 3 in the follow-
ing way:

ht = f (Wh[xt, d(ht−1)] + bh),

(4)

where d is the dropout function from Equation 2.
While recurrent and feed-forward fully connected
layers are similar, there is a signiﬁcant difference:
in feed-forward networks every fully-connected
layer processes its input only once, while it is not
the case for the recurrent layer: each training ex-
ample is a sequence composed of a number of in-
puts. When applying dropout this results in hid-
den state being dropped on every step. This obser-
vation leads to the question of how to sample the
dropout mask. There are two options: sample it
once per whole training sequence (per-sequence)
or to sample a new mask on every step (per-
step). We discuss these two strategies for sampling
dropout mask in more detail in Section 3.4.

3.3 Dropout in LSTM and GRU networks
Despite simplicity and effectiveness of vanilla
RNNs on a number of sequence modeling tasks,
their training have been shown to pose serious
problems, primarily caused by so called vanish-
ing and exploding gradients (Bengio et al., 1994).
While the problem of exploding gradients can be
effectively addressed by performing gradient clip-
ping (Pascanu et al., 2013), the vanishing gradi-
ents appear to be a more serious issue. There are
two primary directions to address the latter issue:

Short-Term

(i) use more sophisticated Hessian-free optimiza-
tion algorithms instead of Stochastic Gradient De-
scend (SGD), e.g., Martens and Sutskever (2011),
and (ii) make adjustments in the network architec-
ture, e.g., by introducing gated inputs.
Long
net-
works
(Hochreiter and Schmidhuber, 1997)
have introduced the concept of gated inputs in
RNNs, which effectively allow the network to
preserve its memory over a larger number of
time steps during both forward and backward
passes, thus alleviating the problem of vanishing
gradients.
is expressed with the
following equations:

Memory

Formally,

it

it
ft
ot
gt







=


σ(Wi(cid:2)xt, ht−1(cid:3) + bi)
σ(Wf(cid:2)xt, ht−1(cid:3) + bf )
σ(Wo(cid:2)xt, ht−1(cid:3) + bo)
f (Wg(cid:2)xt, ht−1(cid:3) + bg)

ct = ft ∗ ct−1 + it ∗ gt

ht = ot ∗ f (ct),




(5)

(6)

(7)

where it, ft, ot are input, output and forget gates at
step t; gt is the vector of cell updates and ct is the
updated cell vector used to update the hidden state
ht; σ is the sigmoid function and ∗ is the element-
wise multiplication.

Our approach is to apply dropout to the cell up-

date vector ct as follows:

ct = ft ∗ ct−1 + it ∗ d(gt)

(8)

In contrast, Moon et al. (2015) propose to ap-
ply dropout directly to the cell values and use per-
sequence sampling:

ct = d(ft ∗ ct−1 + it ∗ gt)

(9)

We will discuss the limitations of the approach
of Moon et al. (2015) in Section 3.4 and support
our arguments with empirical evaluation in Sec-
tions 4.1 and 4.2.
Gated Recurrent Unit
(GRU) networks are
a
recur-
rent network with hidden state protected by
gates (Cho et al., 2014). Different from LSTMs,
GRU networks use only two gates rt and zt to
update the cell’s hidden state ht:

recently introduced variant of

a

rt(cid:19) =(cid:18)σ(Wz(cid:2)xt, ht−1(cid:3) + bz)
σ(Wr(cid:2)xt, ht−1(cid:3) + br)(cid:19)
(cid:18)zt

(10)

h

g

i

c

f

o

g

r

z

h

(a) RNN

(b) LSTM

(c) GRU

Figure 1: Illustration of the three types of recurrent networks used in the paper. Arrows, squares and
circles represent connections, hidden states and gates respectively. Dashed squares illustrate places where
we apply dropout.

gt = f (Wg(cid:2)xt, rt ∗ ht−1(cid:3) + bg)

ht = (1 − zt) ∗ ht−1 + zt ∗ gt

(11)

(12)

Similarly to the LSTMs, we propoose to apply

dropout to the hidden state updates vector gt:

ht = (1 − zt) ∗ ht−1 + zt ∗ d(gt)

(13)

To the best of our knowledge, this work is the ﬁrst
to study the effect of recurrent dropout in GRU
networks.

3.4 Dropout and memory
Before going further with the explanation of our
approach we need to clarify one notational differ-
ence between LSTM and GRU networks. Tradi-
tionally, LSTM networks use “cells” to store his-
tory and “hidden states” to output the information
to the rest of the network. GRU networks do not
have this distinction. In this subsection we will re-
fer to LSTM cells and GRU hidden states when we
say network hidden states.

Firstly, we found that an intuitive idea to drop
previous hidden states directly, as proposed in
Moon et al. (2015), produces mixed results. We
have observed that it helps the network to gen-
eralize better when not coupled with the forward
dropout, but is no longer beneﬁcial when used
together with regular forward dropout. While
in some cases applying both forward and hid-
den state dropout yields an improvement in the
network performance, we found this outcome to
be rare and hard to achieve. Below we discuss
two primary problems with the dropout method of
Moon et al. (2015).

The ﬁrst problem arises from the fact that gated
networks do not overwrite their hidden state on
every processing step.
they gradually
change it by using the element-wise multiplica-
tion with gate values and summation with a new
hidden state candidate. Therefore, after a neu-
ron has stored a value it will remain in neuron’s

Instead,

memory until the forget gate is activated. Drop-
ping a neuron in LSTM or GRU networks is ef-
fectively equivalent to activating the forget gate
because when it is zeroed out by the dropout it
loses all of its history. As a consequence, training
with per-step and per-sequence mask sampling re-
sults in very different dynamics in a network dur-
ing training. In case of a per-step mask sampling,
network’s ability to learn long-term relationships
is reduced because every unit’s history gets deleted
very often. In general a network can still be able to
overcome this by constantly refreshing its hidden
state, but this is not trivial to learn and forces a net-
work to behave similarly to vanilla RNNs. As an
example, when training a network with 0.2 recur-
rent dropout the probability that a neuron’s state
will be zeroed in 20 steps is almost 0.99. This can
be addressed by sampling a dropout mask once per
sequence. Moon et al. (2015) make the same ob-
servation and propose to use per-sequence mask
sampling as well. However, in this case some neu-
rons behave as if dropout was not used at all, while
other neurons remain idle when processing a given
input sequence.

The second problem is caused by the scaling of
neuron activations. Consider the hidden state up-
date rule in the test phase of an extremely simpli-
ﬁed version of a gated network with all the gates
always equal to 1:

ht = (ht−1 + gt)p,

(14)

where gt are update vectors, computed by Eq. 5
for LSTM and Eq. 11 for GRU networks and p
is the probability to not drop a neuron. As ht−1
was, in turn, computed using the same rule, we
can rewrite this equation as:

ht = ((ht−2 + gt−1)p + gt)p

(15)

Recursively expanding h for every timestep re-
sults in the following equation:

ht = ((((h0 + g0)p + g1)p + ...)p + gt)p (16)

Pushing p inside parenthesis, Eq. 16 can be written
as:

t

Sampling

Short

Medium

Train

Test

Train

Test

ht = pt+1h0 +

pt−i+1gi

(17)

recurrent dropout in hidden states

per-step

per-sequence

100% 100% 25%
25%
100% 25% 100% <25%

recurrent dropout in hidden state updates
per-step

100% 100% 100% 100%
100% 100% 100% 100%

per-sequence

Table 1: Accuracies on the Temporal Order task.

4 Experiments

First, we empirically demonstrate the issues linked
to memory loss when using various dropout
techniques
in recurrent nets (see Sec. 3.4).
For this purpose we experiment with training
LSTM networks on one of the synthetic tasks
from (Hochreiter and Schmidhuber, 1997), specif-
ically the Temporal Order task. We then validate
the effectiveness of our recurrent dropout when
applied to vanilla RNNs, LSTMs and GRUs on
three diverse public benchmarks: Language Mod-
elling, Named Entity Recognition, and Twitter
Sentiment classiﬁcation.

4.1 Synthetic Task
Data. In this task a network processes sequences
generated as follows: at ﬁxed positions in a se-
quence, e.g., in the beginning and middle, non-
noisy symbols are chosen from {A, B}, while the
remaining symbols are random. The task is to
classify a sequence into one of four classes ({AA,
AB, BA, BB}) based on the order of non-noisy
symbols. We encode the symbols using two bits.
The networks are trained on 200 minibatches with
32 sequences and tested on 10k sequences.
Setup. We use LSTM with one layer that con-
tains 256 hidden units and recurrent dropout with
0.5 strength. We use SGD with a learning rate of
0.1 and train for 5k epochs. We generate data so
that every sequence is split into three parts with the
same size and emit one meaningful symbol in ﬁrst
and second parts of a sequence. The prediction is
taken after the full sequence has been processed.
We use two modes in our experiments: Short
with sequence length 15 and Medium with se-
quence length 30.
Results. Table 1 reports the results on the Tempo-
ral Order task when recurrent dropout is applied

Xi=0

Since p is a value between zero and one, sum com-
ponents that are far away in the past are multi-
plied by a very low value and are effectively re-
moved from the summation. Thus, even though
the network would be able to learn long-term de-
pendencies, it will not be able to use them dur-
ing test phase. The fact that Moon et al. (2015)
have achieved an improvement can be explained
by the experimentation domain. Le et al. (2015)
have proposed a simple yet effective way to ini-
tialize vanilla RNNs and reported that they have
achieved a good result in the Speech Recognition
domain while having an effect similar to the one
caused by Eq. 17. One can reduce the inﬂuence
of this effect by selecting a low dropout rate. This
solution however is partial, since it only increases
the number of steps required to completely for-
get past history and does not remove the problem
completely.

One important note is that the dropout function

from Eq. 2 can be implemented as:

d(x) =(mask ∗ x/p,

x

if train phase
otherwise

(18)

In this case the above argument holds as well,
but instead of observing vanishing hidden states
during testing, we will observe exploding hidden
states during training.

Our approach addresses both problems dis-
cussed previously. Since we drop only candidates,
we do not delete neuron’s history during training,
which allows us to use per-step mask sampling
while still being able to learn long-term dependen-
cies. Moreover, our approach allows for solving
the scaling issue, as Eq. 17 becomes:

ht = ph0 +

t

Xi=0

p gi = ph0 + p

gi

(19)

t

Xi=0

Thus, our approach allows to freely apply dropout
in the recurrent connections of a gated network
without hindering its ability to process long-term
relationships.

Finally, we note that none of the discussed prob-
lems affect vanilla RNNs because they overwrite
their hidden state at every timestep.

Dropout rate

Sampling

RNN

LSTM

GRU

Valid

Test Valid

Test Valid

Test

recurrent dropout in hidden states; no forward dropout

–

per-step
per-step

per-sequence
per-sequence

146.0
125.1
127.7
127.8
131.0

139.0
118.1
119.3
120.4
122.3

130.0
113.0
124.0
121.0
137.7

125.2
108.7
116.5
113.0
126.2

130.3
102.5
114.7
116.2
139.5

–

recurrent dropout in hidden state updates; no forward dropout
130.3
98.7
97.8
98.5
104.2

125.2
100.0
98.0
100.7
96.8

130.0
106.1
102.8
106.3
103.2

per-sequence
per-sequence

per-step
per-step

–
–
–
–
–

–
–
–
–
–

–

recurrent dropout in hidden state updates; with forward dropout
100.0
93.3
109.4
94.7
111.0

89.5
87.0
95.5
87.6
101.8

94.1
91.6
100.6
92.4
107.8

122.3
117.3
119.2
120.5
128.0

128.4
124.1
125.9
128.3
136.9

per-sequence
per-sequence

per-step
per-step

126.1
98.1
108.9
109.8
130.4

126.1
94.4
93.4
93.4
99.3

96.1
89.2
104.1
90.5
105.4

0.0
0.25
0.5
0.25
0.5

0.0
0.25
0.5
0.25
0.5

0.0
0.25
0.5
0.25
0.5

Table 2: Perplexity scores on Language Modelling task (lower is better). Note that for RNNs we report
results when applying recurrent dropout to hidden states only: with and without forward dropout (top
and bottom sections of the table). Networks with forward dropout use 0.2 and 0.5 dropout rates in input
and output connections respectively.

to hidden states or hidden state updates.

When dropout is applied to hidden states with
per-sequence sampling networks are able to dis-
cover the long-term dependency, but fail to use
it on the test set due to the scaling issue.
Inter-
estingly, in Medium case results on the test set
are worse than random. Networks trained with
per-step sampling exhibit different behavior:
in
Short case they are capable of capturing the tem-
poral dependency and generalizing to the test set,
but require 10-20 times more iterations to do so. In
Medium case these networks do not ﬁt into the al-
located number of iterations. If dropout is applied
in hidden state updates, networks are able to solve
the problem in all cases. We have also ran the
same experiments for longer sequences, but found
that the results are equivalent to the Medium case.

4.2 Language Modeling

deﬁned training, validation and test splits, and a
vocabulary of 10k words.
Setup. In our LM experiments we use recurrent
networks with a single layer with 256 hidden units.
Network parameters were initialized uniformly in
[-0.05, 0.05]. For training, we use plain SGD
with batch size 32 with the maximum norm gradi-
ent clipping (Pascanu et al., 2013). Learning rate,
clipping threshold and number of Backpropaga-
tion Through Time (BPTT) steps were set to 0.1,
30 and 15 respectively for vanilla RNNs, 1, 10 and
35 for LSTMs and 0.1, 20 and 35 for GRUs. For
the learning rate decay we use the following strat-
egy: if the validation error does not decrease af-
ter each epoch, we divide the learning rate by 1.5.
The aforementioned choices were largely guided
by the work of Mikolov et al. (2014)1. To ease re-
producibility of our results on the LM and syn-
thetic tasks, we have released the source code of

Data. Following Mikolov et al. (2011) we use
the Penn Treebank Corpus to train our Language
Modeling (LM) models. The dataset contains ap-
proximately 1 million words and comes with pre-

1We found that to achieve reasonable results with vanilla
RNNs on the LM task, it was important to construct training
batches similarly to Mikolov et al. (2014), where the authors
backpropagate 10 steps back every 5 steps forward.

our experiments2.
Results. Table 2 reports the results for vanilla
RNNs, LSTM and GRU networks. For LSTM and
GRU networks we also present results when the
dropout is applied directly to hidden states as in
(Moon et al., 2015).

We make the following observations: (i) drop-
ping hidden state updates yields better results than
dropping hidden states; (ii) per-step mask sam-
pling is better when dropping hidden state directly;
(iii) contrary to our expectations, when we apply
dropout to hidden state updates per-step sampling
seems to yield results similar to per-sequence sam-
pling; (iv) applying dropout to hidden state up-
dates rather than hidden states in some cases leads
to a perplexity decrease by more than 30 points;
and ﬁnally (v) our approach is effective even when
combined with the forward dropout – for LSTMs
we are able to bring down perplexity on the vali-
dation set from 130 to 91.6.

4.3 Named Entity Recognition
Data. To assess our recurrent Named Entity
Recognition (NER) taggers when using recurrent
dropout we use a public benchmark from CONLL
2003
(Tjong Kim Sang and De Meulder, 2003).
The dataset contains approximately 300k words
split into train, validation and test partitions. Each
word is labeled with either a named entity class
it belongs to, such as Location or Person, or
as being not named. The majority of words are
labeled as not named entities. The vocabulary size
is about 22k words.
Setup.
Previous state-of-the-art NER systems
have shown the importance of using word con-
text features around entities. Hence, we slightly
modify the architecture of our recurrent networks
to consume the context around the target word
by simply concatenating their embeddings. The
size of the context window is ﬁxed to 5 words
(the word to be labeled, two words before and
two words after). The recurrent
layer size is
1024 units. The network inputs include only word
embeddings (initialized with pretrained word2vec
embeddings (Mikolov et al., 2013) and kept static)
and capitalization features. For training we use the
RMSProp algorithm (Dauphin et al., 2015) with ρ
ﬁxed at 0.9 and a learning rate of 0.01 and mul-
tiply the learning rate by 0.99 after every epoch.
We also combine our recurrent dropout (with per-

2https://github.com/stas-semeniuta/drop-rnn

Dropout rate RNN LSTM GRU

5 word long sequences
85.32
86.42

85.60
86.48

0.0
0.25

86.00
86.70

15 word long sequences

0.0
0.25

86.12
86.95

86.12
86.88

86.44
87.10

Table 3: F1 scores (higher is better) on NER task.

sequence mask sampling) with the conventional
forward dropout with the rate 0.2 in input and 0.5
in output connections. Lastly, we found that us-
ing relu(x) = max(x, 0) nonlinearity resulted in
higher performance than tanh(x).

To speed up the training we use a length ex-
pansion approach described in (Ng et al., 2015),
where training is performed in two stages: (i) we
ﬁrst sample short 5-words input sequences with
their contexts and train for 25 epochs; (ii) we
ﬁne tune the network on input 15-words sequences
for 10 epochs. We found that further ﬁne tuning
on longer sequences yielded negligible improve-
ments. Such strategy allows us to signiﬁcantly
speed up the training when compared to training
from scratch on full-length input sentences. We
use full sentences for testing.
Results. F1 scores of our taggers are reported in
Table 3 when trained on short 5-word and longer
15-word input sequences. We note that the gap
between networks trained with and without our
dropout scheme is larger for networks trained on
shorter sequences. It suggests that dropout in re-
current connections might have an impact on how
well a network generalizes to sequences that are
longer than the ones used during training. The
gain from using recurrent dropout is larger for
the LSTM network. We have experimented with
higher recurrent dropout rates, but found that it
led to excessive regularization.

We

sentiment

use Twitter

(Rosenthal et al., 2015).

4.4 Twitter Sentiment Analysis
Data.
cor-
from SemEval-2015 Task 10 (subtask
pus
B)
It contains 15k
labeled tweets split into training and validation
partitions. The total number of words is approx-
imately 330k and the vocabulary size is 22k.
The task consists of classifying a tweet into three
classes: positive, neutral, and negative.

Dropout rate Twitter13 LiveJournal14 Twitter15 Twitter14 SMS13 Sarcasm14

0

0.25

0

0.25

0

0.25

67.54
66.35

67.97
69.11

67.09
69.04

71.20
67.70

69.82
71.39

70.80
72.10

RNN

59.35
59.91

LSTM

57.84
61.35

GRU

59.07
60.34

68.90
67.76

67.95
68.08

67.02
69.65

64.51
64.46

61.47
65.45

67.11
65.73

53.58
48.74

53.49
53.80

51.01
54.77

Table 4: F1 scores (higher is better) on Sentiment Evaluation task

Performance of a classiﬁer is measured by the av-
erage of F1 scores of positive and negative
classes. We evaluate our models on a number of
datasets that were used for benchmarking during
the last years.
Setup. We use recurrent networks in the standard
sequence labeling manner - we input words to a
network one by one and take the label at the last
step. Similarly to (Severyn and Moschitti, 2015),
we use 1 million of weakly labeled tweets to pre-
train our networks. We use networks composed of
500 neurons in all cases. Our models are trained
with the RMSProp algorithm with a learning rate
of 0.001. We use our recurrent dropout regular-
ization with per-step mask sampling. All the other
settings are equivalent to the ones used in the NER
task.
Results. The results of these experiments are pre-
sented in Table 4. Note that in this case our al-
gorithm decreases the performance of the vanilla
RNNs while this is not the case for LSTM and
GRU networks. This is due to the nature of the
problem: differently from LM and NER tasks,
a network needs to aggregate information over a
long sequence. Vanilla RNNs notoriously have
difﬁculties with this and our dropout scheme im-
pairs their ability to remember even further. The
best result over most of the datasets is achieved
by the GRU network with recurrent dropout. The
only exception is the Twitter2015 dataset, where
the LSTM network shows better results.

5 Conclusions

bined with conventional forward dropout. We
have shown that for LSTMs and GRUs applying
dropout to arbitrary cell vectors results in subopti-
mal performance. We discuss in detail the cause of
this effect and propose a simple solution to over-
come it. The effectiveness of our approach is ver-
iﬁed on three different public NLP benchmarks.

Our ﬁndings along with our empirical results
allow us to answer the questions posed in Sec-
tion 1: (i) it is straight-forward to use dropout in
vanilla RNNs due to their strong similarity with
the feed-forward architectures, while its applica-
tion to LSTM and GRU networks requires some
care. We demonstrate that recurrent dropout is
most effective when applied to hidden state update
vectors rather than hidden states; (ii) we observe
an improvement
in the network’s performance
when our recurrent dropout is coupled with the
standard forward dropout, though the extent of this
improvement depends on the values of dropout
rates; (iii) contrary to our expectations, networks
trained with per-step and per-sequence mask sam-
pling produce similar results when using our re-
current dropout method, both being better than the
dropout scheme proposed by Moon et al. (2015).
While our experimental results show that ap-
plying recurrent dropout method leads to signif-
icant improvements across various NLP bench-
marks (especially when combined with conven-
tional forward dropout), its beneﬁts for other tasks,
e.g., sequence-to-sequence prediction, or other do-
mains, e.g., Speech Recognition, remain unex-
plored. We leave it as our future work.

This paper presents a novel recurrent dropout
method speciﬁcally tailored to the gated recurrent
neural networks. Our approach is easy to im-
plement and is even more effective when com-

Acknowledgments

This project has received funding from the Eu-
ropean Union’s Framework Programme for Re-

search and Innovation HORIZON 2020 (2014-
2020) under the Marie Skodowska-Curie Agree-
ment No. 641805. Stanislau Semeniuta thanks
the support from Pattern Recognition Company
GmbH. We gratefully acknowledge the support of
NVIDIA Corporation with the donation of the Ti-
tan X GPU used for this research.

References

Bengio,

[Bengio et al.1994] Yoshua

Patrice
Simard, and Paolo Frasconi. 1994. Learning
long-term dependencies with gradient descent
is difﬁcult.
IEEE Transactions on Neural
Networks, 5(2):157–166.

[Bengio et al.2015] Samy Bengio, Oriol Vinyals,
Navdeep Jaitly, and Noam Shazeer.
2015.
Scheduled sampling for sequence prediction
with recurrent neural networks.
CoRR,
abs/1506.03099.

[Bluche et al.2015] Theodore Bluche, Christopher
2015.
Kermorvant, and J´erˆome Louradour.
Where to apply dropout in recurrent neural net-
works for handwriting recognition? In 13th In-
ternational Conference on Document Analysis
and Recognition, ICDAR 2015, Tunis, Tunisia,
August 23-26, 2015, pages 681–685.

[Cho et al.2014] KyungHyun Cho, Bart van Mer-
rienboer, Dzmitry Bahdanau, and Yoshua Ben-
gio. 2014. On the properties of neural ma-
chine translation: Encoder-decoder approaches.
CoRR, abs/1409.1259.

[Dauphin et al.2015] Yann N. Dauphin, Harm
de Vries, Junyoung Chung, and Yoshua Bengio.
2015. Rmsprop and equilibrated adaptive learn-
ing rates for non-convex optimization. CoRR,
abs/1502.04390.

[Dyer et al.2015] Chris Dyer, Miguel Ballesteros,
Wang Ling, Austin Matthews, and A. Noah
Smith.
2015. Transition-based dependency
parsing with stack long short-term memory. In
ACL, pages 334–343. Association for Compu-
tational Linguistics.

[Hinton et al.2012] Geoffrey E. Hinton, Nitish Sri-
vastava, Alex Krizhevsky, Ilya Sutskever, and
Ruslan Salakhutdinov. 2012. Improving neural
networks by preventing co-adaptation of feature
detectors. CoRR, abs/1207.0580.

[Hochreiter and Schmidhuber1997] Sepp Hochre-
1997. Long
Neural Comput.,

iter and J¨urgen Schmidhuber.
short-term memory.
9(8):1735–1780, November.

[Irsoy and Cardie2014] Ozan Irsoy and Claire
Cardie. 2014. Opinion mining with deep re-
current neural networks.
In Proceedings of
the 2014 Conference on Empirical Methods in
Natural Language Processing (EMNLP), pages
720–728. Association for Computational Lin-
guistics.

[Kim et al.2015] Yoon Kim, Yacine

Jernite,
David Sontag, and Alexander M. Rush. 2015.
Character-aware
language models.
CoRR, abs/1508.06615.

neural

[Krizhevsky et al.2012] Alex Krizhevsky,

Ilya
Sutskever, and Geoffrey E. Hinton.
2012.
Imagenet classiﬁcation with deep convolutional
In F. Pereira, C.J.C. Burges,
neural networks.
L. Bottou, and K.Q. Weinberger,
editors,
Advances
Information Process-
ing Systems 25, pages 1097–1105. Curran
Associates, Inc.

in Neural

[Le et al.2015] Quoc V. Le, Navdeep Jaitly, and
Geoffrey E. Hinton. 2015. A simple way to
initialize recurrent networks of rectiﬁed linear
units. CoRR, abs/1504.00941.

[Liu et al.2015] Pengfei Liu, Xipeng Qiu, Xinchi
Chen, Shiyu Wu, and Xuanjing Huang. 2015.
Multi-timescale long short-term memory neu-
ral network for modelling sentences and docu-
ments. In ACL. Association for Computational
Linguistics.

[Martens and Sutskever2011] James Martens and
Ilya Sutskever. 2011. Learning recurrent neu-
ral networks with hessian-free optimization. In
Lise Getoor and Tobias Scheffer, editors, Pro-
ceedings of the 28th International Conference
on Machine Learning (ICML-11), ICML ’11,
pages 1033–1040, New York, NY, USA, June.
ACM.

[Mikolov et al.2011] T. Mikolov, S. Kombrink,
L. Burget, J.H. Cernocky, and Sanjeev Khudan-
pur. 2011. Extensions of recurrent neural net-
work language model.
In Acoustics, Speech
and Signal Processing (ICASSP), 2011 IEEE
International Conference on, pages 5528–5531,
May.

[Mikolov et al.2013] Tomas Mikolov,

Ilya
Sutskever, Kai Chen, Greg S Corrado, and
Jeff Dean. 2013. Distributed representations of
words and phrases and their compositionality.
In C.J.C. Burges, L. Bottou, M. Welling,
Z. Ghahramani, and K.Q. Weinberger, editors,
Advances
Information Process-
ing Systems 26, pages 3111–3119. Curran
Associates, Inc.

in Neural

[Mikolov et al.2014] Tomas Mikolov, Armand
Joulin, Sumit Chopra, Micha¨el Mathieu, and
Marc’Aurelio Ranzato. 2014. Learning longer
memory in recurrent neural networks. CoRR,
abs/1412.7753.

[Moon et al.2015] Taesup Moon, Heeyoul Choi,
Hoshik Lee, and Inchul Song.
2015. Rn-
ndrop: A novel dropout for rnns in asr. Au-
tomatic Speech Recognition and Understanding
(ASRU).

[Ng et al.2015] Joe Yue-Hei Ng, Matthew J.
Hausknecht, Sudheendra Vijayanarasimhan,
Oriol Vinyals, Rajat Monga,
and George
Toderici.
Beyond short snippets:
Deep networks for video classiﬁcation. CoRR,
abs/1503.08909.

2015.

[Pascanu et al.2013] Razvan

Pascanu,

Tomas
Mikolov, and Yoshua Bengio. 2013. On the
difﬁculty of training recurrent neural networks.
In Proceedings of the 30th International Con-
ference on Machine Learning,
ICML 2013,
Atlanta, GA, USA, 16-21 June 2013, pages
1310–1318.

[Pham et al.2013] Vu Pham, Christopher Kermor-
vant, and J´erˆome Louradour. 2013. Dropout
improves recurrent neural networks for hand-
writing recognition. CoRR, abs/1312.4569.

[Rosenthal et al.2015] Sara Rosenthal,

Preslav
Nakov, Svetlana Kiritchenko, Saif Mohammad,
Alan Ritter, and Veselin Stoyanov.
2015.
Semeval-2015 task 10: Sentiment analysis in
twitter. In Proceedings of the 9th International
Workshop on Semantic Evaluation (SemEval
2015), pages 451–463, Denver, Colorado, June.
Association for Computational Linguistics.

2015.

[Severyn and Moschitti2015] Aliaksei

Severyn
and Alessandro Moschitti.
Twitter
sentiment analysis with deep convolutional
neural networks.
In Proceedings of the 38th
International ACM SIGIR Conference on
Research and Development
in Information
Retrieval, Santiago, Chile, August 9-13, 2015,
pages 959–962.

[Sutskever et al.2014] Ilya

Oriol
Vinyals, and Quoc V. Le. 2014. Sequence to
sequence learning with neural networks.
In
NIPS, pages 3104–3112.

Sutskever,

[Tjong Kim Sang and De Meulder2003] Erik

F.
Tjong Kim Sang and Fien De Meulder. 2003.
Introduction to the conll-2003 shared task:
Language-independent named entity recogni-
tion. In Proceedings of the Seventh Conference
on Natural Language Learning at HLT-NAACL
2003 - Volume 4, CONLL ’03, pages 142–
147, Stroudsburg, PA, USA. Association for
Computational Linguistics.

[Wang et al.2015] Xin Wang, Yuanchao Liu,
Chengjie SUN, Baoxun Wang, and Xiaolong
Wang. 2015. Predicting polarities of tweets
by composing word embeddings with long
short-term memory. In ACL, pages 1343–1353.
Association for Computational Linguistics.

[Zaremba et al.2014] Wojciech Zaremba,

Ilya
Sutskever, and Oriol Vinyals. 2014. Recur-
rent neural network regularization.
CoRR,
abs/1409.2329.

[Zhang and Lapata2014] Xingxing Zhang

and
2014. Chinese poetry gen-
Mirella Lapata.
eration with recurrent neural networks.
In
Proceedings of the 2014 Conference on Empir-
ical Methods in Natural Language Processing
(EMNLP), pages 670–680. Association for
Computational Linguistics.

[Zhang et al.2015] Xingxing Zhang, Liang Lu, and
Mirella Lapata. 2015. Tree recurrent neural
networks with application to language model-
ing. CoRR, abs/1511.00060.

