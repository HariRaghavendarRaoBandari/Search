6
1
0
2

 
r
a

M
9

 

 
 
]

.

R
P
h
t
a
m

[
 
 

1
v
9
5
9
2
0

.

3
0
6
1
:
v
i
X
r
a

Improved adaptive Multilevel Monte Carlo

and applications to finance

Mohamed Ben Alaya∗1, Kaouther Hajji1, and Ahmed Kebaier†‡1

1 Universit´e Paris 13, Sorbonne Paris Cit´e, LAGA, CNRS (UMR 7539)

mba@math.univ-paris13.fr hajji@math.univ-paris13.fr kebaier@math.univ-paris13.fr

March 10, 2016

Abstract

This paper focuses on the study of an original combination of the Euler Multilevel
Monte Carlo introduced by Giles [12] and the popular importance sampling technique.
To compute the optimal choice of the parameter involved in the importance sampling
method, we rely on Robbins-Monro type stochastic algorithms. On the one hand, we ex-
tend our previous work [2] to the Multilevel Monte Carlo setting. On the other hand, we
improve [2] by providing a new adaptive algorithm avoiding the discretization of any addi-
tional process. Furthermore, from a technical point of view, the use of the same stochastic
algorithms as in [2] appears to be problematic and we are reduced to employ a speciﬁc
version of stochastic algorithms with projection (see e.g. Laruelle, Lehalle and Pag`es [23]).
We ﬁrstly prove the almost sure convergence of this stochastic algorithm towards the op-
timal parameter. Then, we prove a central limit theorem of type Lindeberg-Feller for
the new adaptive Euler Multilevel Monte Carlo algorithm together with non-asymptotic
Berry-Essen bounds. Finally, we illustrate the eﬃciency of our method through applica-
tions in option pricing for the Heston model.

MSC 2010: 60E07, 60G51, 60F05, 62L20, 65C05, 60H35.

Keywords: Multilevel Monte Carlo, Stochastic algorithm, Robbins-Monro, variance re-
duction, Lindeberg-Feller Central Limit Theorem, Euler scheme, Finance, Heston model..

Introduction
We are interested in estimating the expected payoﬀ value Eψ(XT ) in option pricing problems,
with T > 0 and (Xt)0≤t≤T is a given diﬀusion model deﬁned on B = (Ω,F, (Ft)t≥0, P).

∗Supported by Laboratory of Excellence MME-DII http://labex-mme-dii.u-cergy.fr/
†This research beneﬁted from the support of the chair ”Risques Financiers”, Fondation du Risque.
‡Supported by Laboratory of Excellence MME-DII http://labex-mme-dii.u-cergy.fr/

1

In view of reducing the variance in the estimation, we envisage using the importance sampling
technique. For the Gaussian setting, the practical use of this idea with Monte Carlo (MC)
methods was ﬁrstly studied by Arouna [1], Galasserman, Heidelberger and Shahabuddin [14].
Later on, Ben Alaya, Hajji and Kebaier [2] studied the use of this procedure with the Statistical
Romberg (SR) algorithm known for reducing the computation time in the MC method. The
approach used in [2], consists on applying the Girsanov theorem, to obtain

Eψ(XT ) = Eg(θ, X θ

T , WT ),

where g : Rq × Rd × Rq → R, θ ∈ Rq

and (X θ

t )0≤t≤T is solution to

(cid:32)

dX θ

t =

b(X θ

t ) +

(cid:33)

dt +

θjσj(X θ
t )

q(cid:88)
q(cid:88)
T , WT )) + Var(cid:0)∇xg(θ, X θ

j=1

j=1

σj(X θ

t )dW j
t .

(cid:1) ,

T , WT ) · U θ

T

(1)

(2)

In this case, the optimal θ∗

SR reducing the asymptotic variance of the SR method is given by

θ∗
SR = arg min

θ∈Rq

Var(g(θ, X θ

where U θ is is a given diﬀusion associated to the process (X θ
t )t∈[0,T ] deﬁned on an extension
˜B = ( ˜Ω, ˜F, ( ˜Ft)t≥0, ˜P) of the initial space B (see further on). Here, for θ ∈ Rq and x ∈ Rd,
∇xg(θ, X θ
T , WT ) denotes the gradient of the function g with respect to the second variable at
the point (θ, x, y). From a practical point of view, it is worth introducing the discretized version
by considering the optimal θ

∗,n
SR given by

˜E(cid:0)g2(θ, X n

∗,n
SR = arg min
θ
θ∈Rq

T ) + (∇xg(θ, X n

T ) · U n

T )2(cid:1) ,

where X n

T (resp. U n

T ) is the Euler scheme, with time step T /n, associated to XT (resp. UT ).

The aim of this paper is to combine the Multilevel Monte Carlo (MLMC) with the impor-
tance sampling technique with a properly chosen optimal θ based on stochastic algorithms.
The MLMC method was introduced and popularized for ﬁnancial applications by Giles [12].
This method can be seen as a generalized version of SR method introduced by Kebaier in [21].
It has been extensively applied to various ﬁelds of numerical probability (Brownian stochastic
diﬀerential equations, L´evy-driven stochastic diﬀerential equations and more general numerical
analysis problems, see e.g.
[4, 9, 10, 13, 17]). For more references, we refer to the web page
https://people.maths.ox.ac.uk/gilesm/mlmc_community.html and the references therein.
Note that Jourdain and Lelong [19] introduced another approach based on a deterministic min-
imization of the empirical estimation of the variance Var(g(θ, X θ
T , WT )). This approach was
recently used by Kebaier and Lelong [22] to the MLMC methods combined with importance
sampling technique. In these two last references, the authors used deterministic optimization
algorithm instead of stochastic algorithms.

In the context of Euler discretization scheme, the idea of the MLMC method is to apply
the classical MC method for several nested levels of time step sizes and to compute diﬀerent
numbers of paths on each level, from a few paths when the time step size is small to many paths
when the step size is large. More precisely, the Euler MLMC method uses information from

2

a sequence of computations with decreasing step sizes and approximates the quantity Eψ(XT )
by

N0(cid:88)

Qn =

1
N0

ψ(X m0

T,i ) +

i=1

(cid:96)=1

i=1

N(cid:96)(cid:88)

(cid:16)

L(cid:88)

1
N(cid:96)

ψ(X (cid:96),m(cid:96)

T,i ) − ψ(X (cid:96),m(cid:96)−1

T,i

)

, m ∈ N \ {0, 1}.

The time step sizes is deﬁned by T /m(cid:96), (cid:96) = 0, 1, ..., L where L = log n
factor. Here, X (cid:96),m(cid:96)
T /m(cid:96)−1. It turns out that the optimal θ∗ in this case is solution to the problem:

log m and m is the reﬁnement
denote the Euler schemes of XT with time steps T /m(cid:96) and

and X (cid:96),m(cid:96)−1

T

T

Var(cid:0)∇xg(θ, X θ

T , WT ) · U θ

T

θ∗ = arg min

θ∈Rq

see Section 1 for more details.

A new idea in the current paper, is to approximate the optimal parameter θ∗ through the

minimization of

(cid:17)

(cid:1) ,

(cid:32)(cid:115)

(cid:33)2 ,
(cid:1), as (cid:96) tends to inﬁnity,

(3)

v(cid:96)(θ) := E

m(cid:96)

(m − 1)T

(ψ(X m(cid:96),θ

) − ψ(X m(cid:96)−1,θ

))

T

which is nothing but a proper approximation of Var(cid:0)∇xg(θ, X θ

T

T , WT ) · U θ

T

T , n ∈ N∗, denotes the Euler scheme of X θ

where X n,θ
T with time step T /n. The advantage
of this new approach is that the variance is computed without any need to discretize the
sophisticated process U appearing naturally in the limit variance of the problem. This is useful
for practitioners since we need less conditions on the regularity of the payoﬀ ψ and the diﬀusion
coeﬃcients (in ﬁnance, the computation of ∇ψ could constitute a constraint).
In our previous work [2] on the adaptive SR method, we used a constrained version of
∗,n
Robbins Monro algorithm [7, 8] and an unconstrained one [24] to compute θ
SR. From a theo-
retical point of view, the use of these two stochastic algorithms seems to be problematic in the
proof of the central limit theorem of the adaptive Euler MLMC. To overcome these technical
diﬃculties, we use an other version of Robbins-Monro type algorithms, namely the stochastic
algorithm with projection (see [23]). Moreover, in [2] the proof of the central limit theorem for
the adaptive SR algorithm uses the law stable convergence theorem for the error Euler scheme
obtained in Jacod and Protter [18] and which is given by

√

n (X n − X)

stably

=⇒ U,

as n → ∞.

This theorem cannot be used in the MLMC context, since we should consider the Euler error
on two consecutive levels m(cid:96)−1 and m(cid:96). To cope with situation, we rather use Theorem 3 in [3]
given by

(cid:115)

m(cid:96)

(m − 1)T

(X m(cid:96) − X m(cid:96)−1)

stably

=⇒ U,

as (cid:96) → ∞.

In the next section, we introduce the MLMC method and the discretized version of our
problem. In section 2, we give the results on the convergence of the discretized optimal θ∗.

3

Moreover, we establish the convergence results of the problem of estimating θ∗ using the con-
strained version of Robbins-Monro algorithm. In section 3, we ﬁrst introduce the new adaptive
algorithm obtained by combining together the importance sampling procedure and the Euler
MLMC method. Then, we prove a Lindeberg-Feller central limit theorem for this new algo-
rithm (see Theorem 3.2). Further, we obtain a Berry-Essen type bound (this non-asymptotic
result was not proved in our previous work [2]). In section 4, we give a complexity analysis of
the adaptive Euler MLMC algorithm and we proceed to numerical simulations to illustrate the
eﬃciency of this new method for pricing an European call option under the Heston model.

1 Preliminaries
Let (Xt)0≤t≤T be the process with values in Rd, solution to the diﬀusion

q(cid:88)

j=1

q(cid:88)

j=1

q(cid:88)

j=1

dXt = b(Xt)dt +

σj(Xt)dW j

t , X0 = x ∈ Rd,

(4)

where W = (W 1, . . . , W q) is a q-dimensional Brownian motion on some given ﬁltered probability
space B = (Ω,F, (Ft)t≥0, P) and (Ft)t≥0 is the standard Brownian ﬁltration. The functions
b : Rd −→ Rd and σj : Rd −→ Rd, 1 ≤ j ≤ q, satisfy condition

(Hb,σ) ∀x, y ∈ Rd

|b(x) − b(y)| +

|σj(x) − σj(y)| ≤ Cb,σ|x − y|, with Cb,σ > 0.

This ensures strong existence and uniqueness of solution of (4). In practice, we consider the
Euler continuous approximation X n of the process X, with time step δ = T /n given by

dX n

t = b(Xηn(t))dt +

σj(Xηn(t))dWt,

ηn(t) = [t/δ]δ.

(5)

It is well known that under condition (Hb,σ) we have the almost sure convergence of X n towards
X together with the following property (see e.g. Bouleau and L´epingle [6])
|Xt − X n
t |p

(P) ∀p ≥ 1,

t | ∈ Lp

and E

|X n

(cid:20)

(cid:21)

sup
0≤t≤T

|Xt|, sup
0≤t≤T

≤ Kp(T )
np/2 ,

sup
0≤t≤T

where Kp(T ) is a positive constant depending only on b, σ, T , p and q.
step sizes and approximates the quantity Eψ(XT ) by

The Euler MLMC method uses information from a sequence of computations with decreasing

N0(cid:88)

N(cid:96)(cid:88)

(cid:16)

L(cid:88)

1
N(cid:96)

Qn =

1
N0

ψ(X m0

T,i ) +

i=1

(cid:96)=1

i=1

ψ(X (cid:96),m(cid:96)

T,i ) − ψ(X (cid:96),m(cid:96)−1

T,i

)

, m ∈ N \ {0, 1}.

(cid:17)

The time step sizes is deﬁned by T /m(cid:96), (cid:96) = 0, 1, ..., L where L = log n
factor. For the ﬁrst empirical mean, the random variables (X m0

log m and m is the reﬁnement
T,i )1≤i≤N0 are independent copies

4

T,i

)1≤i≤N(cid:96) are independent copies of (X (cid:96),m(cid:96)

T which denotes the Euler scheme with time step T. For (cid:96) ∈ {1, ..., L}, the couples
of X m0
, X (cid:96),m(cid:96)−1
(X (cid:96),m(cid:96)
) whose components denote the
T,i
Euler schemes with time steps T /m(cid:96) and T /m(cid:96)−1. However, for ﬁxed (cid:96), the simulation of X (cid:96),m(cid:96)
and X (cid:96),m(cid:96)−1
has to be based on the same Brownian path. Giles [12] proved that the MLMC
method reduces eﬃciently the computational complexity of the combination of Monte Carlo
method and the Euler discretization scheme. In fact, the complexity in the Monte Carlo method
is equal to n2α+1 and is reduced to n2α(log n)2 in the MLMC method where α ∈ [1/2, 1] is the
T ) − Eψ(XT ). So, it is
order of the rate of convergence of the weak error given by εn = Eψ(X n
worth introducing the following assumption

, X (cid:96),m(cid:96)−1

T

T

T

T

(Hεn)

for α ∈ [1/2, 1] nαεn → Cψ, Cψ ∈ R.

In their recent works, Ben Alaya and Kebaier [4] proved a central limit theorem for the Euler
MLMC method with a rate of convergence equal to nα, α ∈ [1/2, 1] (see Theorem 4 of [4]).
They also obtained a Central Limit Theorem for higher order schemes (see [3]). In [4], they
ﬁrst consider the sample sizes N(cid:96) of the form

L(cid:88)

N(cid:96) =

n2α(m − 1)T

m(cid:96)a(cid:96)

(cid:96)=1

(cid:96) ∈ {0, ..., L} and L =

a(cid:96),

log n
log m

,

(6)

L(cid:88)

where (a(cid:96))(cid:96)∈N is a real sequence of positive terms satisfying

L(cid:88)
Then, under assumptions (Hεn) and (Hψ) which is given by

a(cid:96) = ∞ and lim
L→∞

((cid:80)L

(cid:96)=1 a(cid:96))p/2

(W)

lim
L→∞

(cid:96)=1

(cid:96)=1

1

ap/2
(cid:96) = 0, for p > 2.

(Hψ) P(XT /∈ D ˙ψ) = 0, where D ˙ψ := {x ∈ Rd | ψ is diﬀerentiable at x},

they proved that this method is tamed by a central limit theorem. More precisely, the global
error normalized by nα converges in law to a Gaussian random variable with bias equal to Cψ
and a limit variance equal to

(cid:113) m(cid:96)
(m−1)T (X m(cid:96)−X m(cid:96)−1) deﬁned on ˜B an extension
where U is the weak limit process of the error
of the initial space B (see Theorem 3 in Ben Alaya and Kebaier [4] ). The process U is solution
to

Var (∇ψ(XT ) · UT ) ,

(7)

dUt = ˙b(Xt)Utdt +

˙σj(Xt)UtdW j

˙σj(Xt)σ(cid:96)(Xt)d ˜W (cid:96)j
t ,

(8)

where ˜W is a q2-dimensional standard Brownian motion, deﬁned on the extension ˜B, indepen-
dent of W , and ˙b (respectively ( ˙σj)1≤j≤q) is the Jacobian matrix of b (respectively (σj)1≤j≤q).
Our target now is to use the Euler MLMC introduced above to approximate Eψ(XT ) =
Eg(θ, X θ

q(cid:88)

j=1

q(cid:88)

j,(cid:96)=1

t − 1√
2

, ˆWT,i) +

g(θ, X (cid:96),m(cid:96),θ

T,i

, W (cid:96)

T,i) − g(θ, X (cid:96),m(cid:96)−1,θ

T,i

, W (cid:96)

T,i)

,

(9)

T , WT ), θ ∈ Rq by
N0(cid:88)
1
N0

g(θ, ˆX m0,θ

T,i

i=1

L(cid:88)

(cid:96)=1

1
N(cid:96)

N(cid:96)(cid:88)

(cid:16)

i=1

5

(cid:17)

T

where X n,θ
ψ(x)e−θ·y− 1
importance sampling. According to relation (7), our limit variance is given by

is the Euler scheme associated to X θ
T (2) with time step T /n and g(θ, x, y) =
2|θ|2T ,∀x ∈ Rd and y ∈ Rq, obtained by using Girsanov theorem in view to use

Var(cid:0)∇xg(θ, X θ

T , WT ) · U θ

T

(cid:1) ,

where U θ is the weak limit process of the error r(cid:96)(X m(cid:96),θ − X m(cid:96)−1,θ) where r(cid:96) =
on the extension ˜B and solution to

(cid:32)

q(cid:88)

(cid:33)

q(cid:88)

dU θ

t =

˙b(X θ

t ) +

θj ˙σj(X θ
t )

U θ

t dt +

j=1

j=1

˙σj(X θ

t )U θ

t dW j

t − 1√
2

˙σj(X θ

t )σ(cid:96)(X θ

t )d ˜W (cid:96)j

t . (10)

q(cid:88)

j,(cid:96)=1

Note that U and U θ are the same processes obtained for the asymptotic behavior of (X m(cid:96) − X)
and (X m(cid:96),θ − X θ). In fact, θ∗ is given by

v(θ), with v(θ) := E(cid:104)

θ∗ = arg min

θ∈Rq

(∇ψ(XT ) · UT )2 e−θ·WT + 1

.

(11)

2|θ|2T(cid:105)

(cid:113) m(cid:96)

(m−1)T deﬁned

From a practical point of view, it is natural to choose the optimal θ∗ minimizing the associated
variance. Note that from a practical point of view the quantity v(θ) is not explicit, we use the
Euler scheme to discretize X and we introduce the following quantity θ∗
v(cid:96)(θ), with

(cid:96) := arg min

θ∈Rq

(cid:32)(cid:115)

(cid:33)2

v(cid:96)(θ) := E

m(cid:96)

(m − 1)T

(ψ(X m(cid:96),θ

) − ψ(X m(cid:96)−1,θ

))

T

T

(cid:32)(cid:115)

= E

m(cid:96)

(m − 1)T

(ψ(X m(cid:96)

T ) − ψ(X m(cid:96)−1

T

(cid:33)2

))

e−θ·WT + 1

2|θ|2T

 .

(12)

The last relation is obtained by using a change of probability. In fact, according to Girsanov
theorem, the process (Bθ, X) under Pθ has the same law as (W, X θ) under P.

Remark 1. The new variance associated to the MLMC is only based on the discretization of
the process X. In fact, this is diﬀerent from the problem addressed in our previous work [2],
where we discretized the process U in order to compute
θ∗
n := arg min

SR(θ) := E(cid:16)(cid:104)

T )2 + (∇ψ(X n,θ

2|θ|2T(cid:17)

T )2(cid:105)

SR(θ), with vn
vn

e−θ·WT + 1

T ).U n,θ

ψ(X n,θ

. (13)

θ∈Rq

2 Stochastic algorithm with projection
The existence and uniqueness of θ∗

(cid:96) is ensured by the following result.

6

Proposition 2.1. Suppose σ and b are in C 1 and satisfy condition (Hb,σ). Let ψ satisfying
P((ψ(X m(cid:96)

)) (cid:54)= 0) > 0 and

T ) − ψ(X m(cid:96)−1

T

|ψ(x) − ψ(y)| ≤ C(1 + |x|p + |y|p)|x − y|,

for some C, p > 0.

(14)

Then, the function θ (cid:55)→ v(cid:96)(θ) is C 2 and strictly convex with ∇v(cid:96)(θ) = EH(cid:96)(θ, X m(cid:96)
for all (cid:96) ∈ N \ {0, 1} where

T , X m(cid:96)−1

T WT )

H(cid:96)(θ, X m(cid:96)

T , X m(cid:96)−1

T

, WT ) := (θT − WT )

Moreover, there exists a unique θ∗

Proof. The function θ (cid:55)→ (cid:16)

r(cid:96)(ψ(X m(cid:96)

(cid:16)

(cid:17)2
))
(cid:96) ∈ Rq such that minθ∈Rq v(cid:96)(θ) = v(cid:96)(θ∗
(cid:96) ).

T ) − ψ(X m(cid:96)−1
(cid:17)2

r(cid:96) (ψ(X m(cid:96)

T

e−θ·WT + 1

2|θ|2T .

(15)

))

e−θ·WT + 1
T , X m(cid:96)−1

T

2|θ|2T is inﬁnitely continuously
, WT ). Note that, for c > 0 we

diﬀerentiable with a ﬁrst derivative equal to H(cid:96)(θ, X m(cid:96)
have

T

T ) − ψ(X m(cid:96)−1
(cid:16)

|H(cid:96)(θ, X m(cid:96)

T , X m(cid:96)−1

T

sup
|θ|≤c

, WT )| ≤ (cT + |WT|)

r(cid:96) (ψ(X m(cid:96)

T ) − ψ(X m(cid:96)−1

T

))

ec|WT |+ 1

2 c2T .

T ) − ψ(X m(cid:96)−1

T

(cid:12)(cid:12)(cid:12)2(cid:13)(cid:13)(cid:13)(cid:13)a

))

.

Using H¨older’s inequality, we obtain that

E sup
|θ|≤c

|H(cid:96)(θ, X m(cid:96)

T , X m(cid:96)−1

It is clear that e 1
property (P),

1

T

, WT )| ≤ e

2 c2T(cid:13)(cid:13)ec|WT |(cT + |WT|)(cid:13)(cid:13) a
2 c2T(cid:13)(cid:13)ec|WT |(cT + |WT|)(cid:13)(cid:13) a
E(cid:12)(cid:12)(cid:12)r(cid:96) (ψ(X m(cid:96)

a−1

a−1

(cid:13)(cid:13)(cid:13)(cid:13)(cid:12)(cid:12)(cid:12)r(cid:96) (ψ(X m(cid:96)
(cid:12)(cid:12)(cid:12)2a

< ∞.

∀a > 1, sup

T ) − ψ(X m(cid:96)−1
E sup|θ|≤c |H(cid:96)(θ, X m(cid:96)
Hence, we conclude the boundedness of sup(cid:96)
Lebesgue’s theorem, we deduce that v(cid:96) is C 1 in Rq and ∇v(cid:96)(θ) = EH(cid:96)(θ, X m(cid:96)
the same way, we prove that v(cid:96) is of class C 2 in Rq. So, we have

T , X m(cid:96)−1

))

T

T

(cid:96)

(16)
, WT )|. According to
, WT ). In

T , X m(cid:96)−1

T

Hess(v(cid:96)(θ)) =
E

(cid:20)(cid:0)(θT − WT )(θT − WT )tr + T Iq

(cid:1)(cid:16)

r(cid:96) (ψ(X m(cid:96)

T ) − ψ(X m(cid:96)−1

T

(cid:17)2

))

e−θ.WT + 1

2|θ|2T

(cid:21)

.

is ﬁnite. Moreover, using (14), it follows from

(cid:17)2

Since P(ψ(X m(cid:96)

T ) − ψ(X m(cid:96)−1

T

) (cid:54)= 0) > 0, we get for all u ∈ Rq\{0}

utr Hess(v(cid:96)(θ)) u =

(cid:20)

E

(T|u|2 + (u.(θT − WT ))2)

(cid:16)

r(cid:96) (ψ(X m(cid:96)

T ) − ψ(X m(cid:96)−1

T

7

(cid:17)2

))

(cid:21)

> 0.

e−θ.WT + 1

2|θ|2T

(cid:20)(cid:16)
(cid:20)

Hence, v(cid:96) is strictly convex. Consequently, to prove that the unique minimum is attained
for a ﬁnite value of θ, it will be suﬃcient to prove that lim|θ|→∞ v(cid:96)(θ) = +∞. Recall that
v(cid:96)(θ) = E

. Using Fatou’s lemma, we get

e−θ.WT + 1

r(cid:96) (ψ(X m(cid:96)

2|θ|2T

))

(cid:17)2

(cid:21)

T

T ) − ψ(X m(cid:96)−1
(cid:16)

r(cid:96) (ψ(X m(cid:96)

))

T

T ) − ψ(X m(cid:96)−1
≤ lim inf
|θ|→+∞

(cid:21)

e−θ.WT + 1

2|θ|2T

(cid:17)2
(cid:20)(cid:16)

E

r(cid:96) (ψ(X m(cid:96)

T ) − ψ(X m(cid:96)−1

T

(cid:17)2

))

e−θ.WT + 1

2|θ|2T

(cid:21)

.

+ ∞ = E

lim inf
|θ|→∞

T

))(cid:1)2e
E(cid:16)

T

This completes the proof.
Theorem 2.1. Suppose σ and b are in C 1 and satisfy condition (Hb,σ). Let ψ satisfying
P((ψ(X m(cid:96)

)) (cid:54)= 0) > 0 and condition (14). Then,

T ) − ψ(X m(cid:96)−1

as (cid:96) → ∞.

(cid:96)−→θ∗,
θ∗
Proof. First of all, we will prove that (θ∗
(cid:96) )(cid:96)∈N is a Rq-bounded sequence. By way of contradiction,
| = +∞.
let us suppose that there is a subsequence (θ∗
) (cid:54)= 0}, we have the convergence in probability
This implies that on the event {ψ(X m(cid:96)
WT + 1

)k∈N that diverges to inﬁnity, limk→∞ |θ∗

of the quantity(cid:0)r(cid:96) (ψ(X m(cid:96)k

Then, we can extract a subsequence that converges almost surely towards +∞. Therefore, we
apply Fatou’s lemma and we get limk→∞ v(cid:96)k(θ∗

T )−ψ(X m(cid:96)−1
−θ∗
(cid:96)k

|2T towards +∞.

) = +∞ while

) − ψ(X m(cid:96)k−1

2|θ∗

(cid:96)k

(cid:96)k

(cid:96)k

T

T

(cid:96)k

v(cid:96)k(θ∗

(cid:96)k

) ≤ v(cid:96)k(0) ≤ sup

r(cid:96)(ψ(X m(cid:96)k

T

) − ψ(X m(cid:96)k−1

T

))

< ∞.

(cid:96)

The boundedness of the last expression is obtained using (14) and property (P). This leads to
a contradiction and we deduce that there is some M > 0 such that |θ∗
} is
∞ ∈ S as k tends to
reduced to the singleton set {θ∗}. Let us consider a subsequence θ∗
inﬁnity. According to Proposition 2.1 above, we have

(cid:96)| ≤ M for all (cid:96) ∈ N.
→ x for some subsequence θ∗
→ θ∗

Now, it remains to prove that the set S = {x ∈ Rq : θ∗

(cid:96)k

(cid:96)k

(cid:96)k

) = E(cid:104)

T − WT )(cid:0)r(cid:96) (ψ(X m(cid:96)k

T

(θ∗

(cid:96)k

∇v(cid:96)k(θ∗

(cid:96)k

) − ψ(X m(cid:96)k−1

T

−θ∗
(cid:96)k

.WT + 1

2|θ∗

(cid:96)k

= 0.

|2T(cid:105)

Now, let 1 < ˜a < a, using H¨older’s inequality with the boundedness of θ∗
established in the
ﬁrst part of the proof, we check easily that there exists c > 0 depending on ˜a, T and M such
that

(cid:96)k

E(cid:104)(cid:12)(cid:12)(θ∗

(cid:96)k

T − WT )(cid:0)r(cid:96) (ψ(X m(cid:96)k

T

))(cid:1)2e

) − ψ(X m(cid:96)k−1

T

−θ∗
(cid:96)k

.WT + 1

2|θ∗

(cid:96)k

(cid:17)2

))(cid:1)2e
|2T(cid:12)(cid:12)˜a(cid:105) ≤
(cid:13)(cid:13)(cid:13)(cid:13)(cid:16)

c

r(cid:96) (ψ(X m(cid:96)k

T

) − ψ(X m(cid:96)k−1

T

)

(cid:17)2(cid:13)(cid:13)(cid:13)(cid:13)˜a

a

We get the uniform integrability thanks to (16). Therefore, according to the stable convergence
theorem (see Theorem 3 in Ben Alaya and Kebaier [4]), we have
=⇒ ∇ψ(XT ).UT ,

) − ψ(X m(cid:96)k−1

(cid:17) stably

as (cid:96) → ∞.

ψ(X m(cid:96)k

(cid:16)

(17)

r(cid:96)

)

T

T

8

So, we obtain

∞) = E(cid:104)

∇v(θ∗

∞T − WT )(cid:0)∇ψ(XT ).UT

(θ∗

(cid:1)2e−θ∗∞.WT + 1

2|θ∗∞|2T(cid:105)

= 0.

We complete the proof using the uniqueness of the minimum.

Now, we focus our interest on the eﬀective approximation of the above quantities θ∗

(cid:96) and θ∗.
i )(cid:96)∈N) such that limi→∞ θi = θ∗
(cid:96) ) almost surely. Let K ⊂ Rq be a compact convex subset containing 0.

Our aim is to construct a sequence (θi)i∈N (resp. for ﬁxed (cid:96), (θm(cid:96)
(resp. limi→∞ θm(cid:96)
For θ0 ∈ K, we introduce the sequences (θi)i∈N and (θm(cid:96)

(cid:40) θi+1 = ΠK [θi − γi+1H(θi, XT,i+1, UT,i+1, WT,i+1)]

i )i∈N deﬁned recursively by

i = θ∗

θm(cid:96)
i+1 = ΠK

i − γi+1H(cid:96)(θm(cid:96)
θm(cid:96)

i

, X m(cid:96)

T,i+1, X m(cid:96)−1

T,i+1, WT,i+1)

,

(18)

(cid:104)

(cid:105)

(cid:17)2

where ΠK is the Euclidean projection onto the constraint set K, H and H(cid:96) are given respectively
by the following expressions

H(θ, XT , UT , WT ) := (θT − WT ) (∇ψ(XT ) · UT )2 e−θ·WT + 1
H(cid:96)(θ, X m(cid:96)

, WT ) := (θT − WT )

2|θ|2T
T ) − ψ(X m(cid:96)−1

T , X m(cid:96)−1

r(cid:96) (ψ(X m(cid:96)

T

T

)

(cid:16)

e−θ·WT + 1

2|θ|2T

(19)

and the gain sequence (γi)i∈N is a decreasing sequence of positive real numbers satisfying

∞(cid:88)

∞(cid:88)

γi = ∞ and

i < ∞.
γ2

(20)

i=1

i=1

Theorem 2.2. Suppose σ and b are in C 1 and satisfy condition (Hb,σ). Assume that P((∇ψ(XT )·
UT ) (cid:54)= 0) > 0 and for all (cid:96) ∈ N, P((ψ(X m(cid:96)
)) (cid:54)= 0) > 0. Moreover, let ψ and its
derivatives satisfying (14). Then, the following assertions hold.

T ) − ψ(X m(cid:96)−1

T

• If θ∗ = arg min v(θ) is unique s.t ∇v(θ∗) = 0 and θ∗ ∈ K then θi
• If θ∗

(cid:96) = arg min v(cid:96)(θ) is unique s.t ∇v(cid:96)(θ∗

a.s−→
i→∞ θ∗.
a.s−→
(cid:96) ∈ K then θm(cid:96)
i→∞ θ∗
(cid:96) .

(cid:96) ) = 0 and θ∗

i

Proof. Concerning the ﬁrst assertion, according to Theorem A.1. in Laruelle, Lehalle and Pag`es
[23] on Robbins Monro algorithm with projection: to prove that θi −→
ﬁrstly that

i→∞ θ∗, we need to check

∀θ (cid:54)= θ∗,

(cid:104)∇v(θ), θ − θ∗(cid:105) > 0.

This is satisﬁed using ∇v(θ∗) = 0 and thanks to the convexity of v. Secondly, we have to check
the non explosion condition given by

∃C > 0 such that ∀θ ∈ K, E(cid:2)|H(θ, XT , UT , WT )|2(cid:3) < C(1 + |θ2|).

9

Using H¨older’s inequality, we can check that

E(cid:2)|H(θ, XT , UT , WT )|2(cid:3) ≤ e|θ|2T E1/a(cid:2)|∇ψ(XT )|4a(cid:3)
(cid:104)|UT| 8a
Since E [|∇ψ(XT )|4a] is ﬁnite and E(cid:104)|UT| 4a
E(cid:2)|H(θ, XT , UT , WT )|2(cid:3) ≤ Ce|θ|2T E a−1

E a−1

that

(cid:105)

a−1

2a

a−1

2a

(cid:104)(cid:12)(cid:12)e−θ·WT (θT − WT )(cid:12)(cid:12) 4a

(cid:105) E a−1
(cid:104)(cid:12)(cid:12)e−θ·WT (θT − WT )(cid:12)(cid:12) 4a

(cid:105)

a−1

a−1

Using θ ∈ K, we conclude that supθ∈K
assertion, we aim to prove that for ﬁxed (cid:96) ∈ N \ {0, 1}, θm(cid:96)
thanks to the convexity of v(cid:96) ensured by Proposition 2.1, we prove that

a.s−→
i→∞ θ∗

(22)
E [|H(θ, XT , UT , WT )|2] < ∞. Concerning the second
(cid:96) ) = 0 and

(cid:96) . Using ∇v(cid:96)(θ∗

2a

.

i

is ﬁnite thanks to ( ˜P), there exists C > 0 such

(cid:105)

.

(21)

∀θ (cid:54)= θ∗
(cid:96) ,

(cid:104)∇v(cid:96)(θ), θ − θ∗

(cid:96)(cid:105) > 0.

For the non explosion condition, we use H¨older’s inequality

E(cid:104)|H(cid:96)(θ, X m(cid:96)

T , X m(cid:96)−1

T

, WT )|2(cid:105) ≤ e|θ|2T E1/a

(cid:20)(cid:12)(cid:12)(cid:12)r(cid:96) (ψ(X m(cid:96)

(cid:12)(cid:12)(cid:12)4a(cid:21)

Using (16), there exists C > 0 such that

E(cid:104)|H(cid:96)(θ, X m(cid:96)

T , X m(cid:96)−1

T

Using θ ∈ K, we conclude that supθ∈K

, WT )|2(cid:105) ≤ Ce|θ|2T E a−1
E(cid:104)|H(cid:96)(θ, X m(cid:96)

a

T , X m(cid:96)−1

T

T ) − ψ(X m(cid:96)−1

T

a−1

)

E a−1

a

(cid:104)(cid:12)(cid:12)e−θ·WT (θT − WT )(cid:12)(cid:12) 2a
(cid:105)

(cid:104)(cid:12)(cid:12)e−θ·WT (θT − WT )(cid:12)(cid:12) 2a
, WT )|2(cid:105)

< ∞.

a−1

.

(cid:105)

.

(23)

(24)

Remark 2. If the optimal θ is not in a compact set, we always do better (see [20]). In fact, even
if it doesn’t converge towards the minimal variance, it converges towards a smaller variance.

3 Adaptive Euler Multilevel Monte Carlo algorithm
Let ˜B = ( ˜Ω, ˜F, ( ˜Ft)t≥0, ˜P) be the extension probability space introduced above endowed with
the ﬁltration ˜FT,i = σ(Wt,j, ˜Wt,j, j ≤ i, t ≤ T ) given in the beginning of this chapter. In what
follows, let (θn

i )i≥0, n ∈ N and (θi)i≥0 be two families of sequences satisfying



(H(cid:48)
θ)

For each (cid:96) ∈ N \ {0, 1}, (θm(cid:96)
(θi)i∈N ∈ K ⊂ Rq and (θm(cid:96)

i )i≥0 and (θi)i≥0 are ( ˜FT,i)i≥0-adapted,
i )i∈N ∈ K ⊂ Rq,

lim
i→∞( lim

(cid:96)→∞ θm(cid:96)

i ) = lim

(cid:96)→∞ θ∗

(cid:96) = θ∗, P-a.s.,

10

with deterministic limits θ∗ and θ∗
(cid:96) .
In this section we prove a central limit theorem for the adaptive Euler MLMC method which

approximates our initial quantity of interest Eψ(XT ) = E(cid:104)
N0(cid:88)

L(cid:88)

N(cid:96)(cid:88)

(cid:18)

2|θ|2T(cid:105)

T )e−θ·WT − 1

ψ(X θ

by

(cid:96),m(cid:96),θ(cid:96),m(cid:96)
i−1
T,i

, W (cid:96)

T,i) − g(θ(cid:96),m(cid:96)

i−1 , X

(cid:96),m(cid:96)−1,θ(cid:96),m(cid:96)
i−1
T,i

g(θ(cid:96),m(cid:96)

i−1 , X

g(θm0

i−1, X

m0,θm0
i−1
T,i

, ˆWT,i)+

(cid:19)

, W (cid:96)

T,i)

,

1
N0

1
N(cid:96)

i=1

(cid:96)=1

i=1

where for all x ∈ Rd and y ∈ Rq, g(θ, x, y) = ψ(x)e−θ·y− 1
, i ∈ N) satisfying (H(cid:48)
(θ(cid:96),m(cid:96)
empirical means are independent, the Brownian path as well as (θm(cid:96)
each empirical mean in the Euler MLMC.

, i ∈ N)(cid:96)∈N are independent copies of (θm(cid:96)

i

i

i

2|θ|2T , L = log n

(25)
log m. The sequences
θ). Note that the (L + 1)
, i ∈ N) is independent in

3.1 Asymptotic behavior of the variance of the algorithm
Theorem 3.1. For the above setting, assume that b and σ are C 1 functions satisfying (Hb,σ)
and ψ satisfying assumptions (Hεn), (Hψ) and

|ψ(x) − ψ(y)| ≤ C(1 + |x|p + |y|p)|x − y|,

for some C, p > 0.

(26)

Then, for the choice of N(cid:96), (cid:96) ∈ {0, 1, ..., L} given by (6), the following convergence holds

n→∞ n2αE(cid:2)|Qn − Eψ(XT )|2(cid:3) = ˜σ2 + C 2

lim

ψ,

for α ∈ [1/2, 1],

where Cψ and α are given by relation (Hεn) and ˜σ2 := ˜E(cid:104)

[∇ψ(XT ) · UT ]2 e−θ∗·WT + 1

Proof. At ﬁrst, we rewrite the total error as follows

nα (Qn − Eψ(XT )) = Q1

n + Q2

n + nα(Eψ(X n

where Q1

n and Q2

n are given by the following expressions

T ) − Eψ(XT )),
(cid:33)

(cid:32)

N0(cid:88)

i=1

1
N0

Q1

n := nα

g(ˆθm0

i−1, ˆX

m0,ˆθm0
i−1
T,i

, WT,i) − Eψ(X 1
T )

.

(28)

2|θ∗|2T(cid:105)

.

(27)

(cid:34) L(cid:88)

(cid:96)=1

N(cid:96)(cid:88)

i=1

1
N(cid:96)

Q2

n := nα

(cid:18)

g(θ(cid:96),m(cid:96)

i−1 , X

(cid:96),m(cid:96),θ(cid:96),m(cid:96)
i−1
T,i

, WT,i) − g(θ(cid:96),m(cid:96)

Since Q1

n are independent and centered, we write also

n and Q2

n2αE(cid:2)|Qn − Eψ(XT )|2(cid:3) = E(cid:2)|Q1

n|2(cid:3) + E(cid:2)|Q2

T ) − Eψ(XT ))2.

, WT,i)
T ) − ψ(X m(cid:96)−1

T

ψ(X m(cid:96)

(cid:105)(cid:17)(cid:105)

)

.

i−1 , X

(cid:96),m(cid:96)−1,θ(cid:96),m(cid:96)
i−1
T,i

− E(cid:104)
n|2(cid:3) + n2α(Eψ(X n

11

Using assumption (Hεn), the last term of the previous expression converges towards the dis-
cretization constant C 2
• Step 1. For the ﬁrst term E|Q1
i−1, X
is a martingale with respect to the ﬁltration ˜FT,k, then we write

, WT,i) − Eψ(X m0

(cid:19)
T ), k ≥ 1

ψ as n goes to inﬁnity.

(cid:18)(cid:80)k

i=1 g(θm0

m0,θm0
i−1
T,i

E|Q1

n|2 = E

n|2, noticing that
N0(cid:88)
(cid:18)
E(cid:16)

(cid:34)(cid:12)(cid:12)(cid:12)(cid:12)g(θm0

i−1, X

ψ(X m0

T )2e−θm0

E

i=1

(cid:32)

n2α
N 2
0

N0(cid:88)

i=1

=

n2α
N 2
0

(cid:35)(cid:33)

, WT,i) − Eψ(X m0
T )

(cid:12)(cid:12)(cid:12)(cid:12)2 | ˜FT,i−1
(cid:105)2(cid:19)
i−1|2T(cid:17) −(cid:104)Eψ(X m0

2|θm0

T )

.

m0,θm0
i−1
T,i

i−1·WT + 1

Since X m0
the last equality by introducing a new couple of random variables (X m0
˜FT = ∪i≥1

i−1 is ˜FT,i−1-measurable and thanks to Girsanov theorem, we obtain
T,i ⊥⊥ ˜FT,i−1 and θm0
T , WT ) independent of
(cid:80)L

˜FT,i. As N0 = n2(m−1)T

, we write

i−1|2T(cid:17) −(cid:104)Eψ(X m0

T )

(cid:105)2(cid:19)

.

(29)

ψ(X m0

T )2e−θm0

i−1·WT + 1

2|θm0

E|Q1

n|2 =

a0

(m − 1)T(cid:80)L

n2α−2a0

(cid:96)=1 a(cid:96)
i−1)i∈N ∈ K, we have

Since (θm0

∃c > 0,

sup
i∈N

T )2e−θm0

i−1·WT + 1

2|θm0

T )2ec|WT |+ c2

2 T .

i−1|2T(cid:12)(cid:12)(cid:12) ≤ ψ(X m0

(cid:18)

E(cid:16)

(cid:96)=1 a(cid:96)

1
N0

N0(cid:88)
(cid:12)(cid:12)(cid:12)ψ(X m0

i=1

This upper bound is clearly integrable using property (P) and assumption (26) together with the
H¨older’s inequality. Therefore, by the dominated convergence theorem and under assumption
(H(cid:48)

θ), we obtain that

ψ(X m0

T )2e−θm0

i−1·WT + 1

2|θm0

ψ(X m0

T )2e

−θ∗

m0·WT + 1

2|θ∗

m0|2T(cid:17)

.

Thus, by applying Cesaro’s lemma, we obtain that

= E(cid:16)
i−1|2T(cid:17)
i−1|2T(cid:17) −(cid:104)Eψ(X m0
= E(cid:16)

ψ(X m0

T )2e

−θ∗

T )

(cid:105)2(cid:19)

lim
i→∞

E(cid:16)
(cid:18)
E(cid:16)

N0(cid:88)

i=1

lim
i→∞

1
N0

As(cid:80)L

ψ(X m0

T )2e−θm0

i−1·WT + 1

2|θm0

m0|2T(cid:17) − E2(ψ(X m0

T )).

m0·WT + 1

2|θ∗

(cid:96)=1 a(cid:96) −→

(cid:96)→∞ ∞ and α ∈ [1/2, 1], we conclude thanks to relation (29) that

E|Q1

n|2 −→

n→∞ 0.

So it remains now to study the asymptotic behavior of E|Q2

n|2.

12

(30)

(31)

• Step 2. We consider the second term E|Q2

(cid:32) L(cid:88)

n|2 and we write
N(cid:96)(cid:88)

m(cid:96),m(cid:96)−1,θ(cid:96),m(cid:96)
i−1
T,i

Z

nα
N(cid:96)

(cid:33)2 ,

(cid:96)=1

i=1

E|Q2

n|2 = E

where

m(cid:96),m(cid:96)−1,θ(cid:96),m(cid:96)
i−1
T,i

Z

=

(cid:18)

g(θ(cid:96),m(cid:96)

i−1 , X

(cid:96),m(cid:96),θ(cid:96),m(cid:96)
i−1
T,i

, W (cid:96)

T,i) − g(θ(cid:96),m(cid:96)

i−1 , X

(cid:96),m(cid:96)−1,θ(cid:96),m(cid:96)
i−1
T,i

, W (cid:96)
T,i)
T ) − ψ(X m(cid:96)−1
− E[ψ(X m(cid:96)

T

(cid:17)

)]

.

(32)

As (Z

m(cid:96),m(cid:96)−1,θ(cid:96),m(cid:96)
i−1
T,i

)1≤(cid:96)≤L are independent, we have

L(cid:88)

(cid:96)=1

E|Q2

n|2 =

E

n2α
N 2
(cid:96)

i=1

(cid:32) N(cid:96)(cid:88)

(cid:33)2

.

m(cid:96),m(cid:96)−1,θ(cid:96),m(cid:96)
i−1
T,i

Z

By the same argument as in the ﬁrst step, noticing that for each (cid:96) ∈ {1, ..., L},
is ˜FT,k martingale, we write

E|Q2

(cid:32) L(cid:88)
n|2 = E
L(cid:88)

(cid:96)=1

=

n2α
N 2
(cid:96)

(cid:34)

N(cid:96)(cid:88)
N(cid:96)(cid:88)

i=1

E

(cid:34)(cid:18)
(cid:16)E(cid:16)

n2α
N(cid:96)

1
N(cid:96)

(cid:96)=1

i=1

m(cid:96),m(cid:96)−1,θm(cid:96)
i−1
T,i

Z

(cid:35)(cid:33)

(cid:19)2 | ˜FT,i−1
−(cid:16)Eψ(X m(cid:96)

[ψ(X m(cid:96)

T ) − ψ(X m(cid:96)−1

T

)]2e−θm(cid:96)

i−1·WT + 1

2|θm(cid:96)

i−1|2T(cid:17)
(cid:17)2(cid:19)(cid:21)

(cid:18)(cid:80)k

m(cid:96),m(cid:96)−1,θ(cid:96),m(cid:96)
i−1
T,i

i=1 Z

(cid:19)

, k ≥ 1

T ) − Eψ(X m(cid:96)−1
(33)
i−1 is ˜FT,i−1-measurable and thanks to
Since for each (cid:96) ∈ {1, ..., L}, X (cid:96),m(cid:96)
(cid:80)L
Girsanov theorem, we obtain the last equality by introducing a new couple of random variables
(cid:96)=1 a(cid:96), (cid:96) ∈ {0, ..., L} (see
(X m(cid:96)
relation (6)), we write

T , WT ) independent of ˜FT = ∪i≥1

˜FT,i. As N(cid:96) = n2α(m−1)T

T,i ⊥⊥ ˜FT,i−1 and θm(cid:96)

m(cid:96)a(cid:96)

)

T

.

L(cid:88)

E|Q2

n|2 =

1(cid:80)L

a(cid:96)

m(cid:96)

(m − 1)T

1
N(cid:96)

(cid:96)=1 a(cid:96)

(cid:96)=1

E(cid:16)
N(cid:96)(cid:88)
1(cid:80)L

i=1

−

[ψ(X m(cid:96)

T

T ) − ψ(X m(cid:96)−1
(cid:16)
L(cid:88)

(cid:104)Eψ(X m(cid:96)

a(cid:96)

r(cid:96)

(cid:96)=1 a(cid:96)

(cid:96)=1

i−1|2T(cid:17)
(cid:105)(cid:17)2

)]2e−θm(cid:96)

i−1·WT + 1

2|θm(cid:96)

T ) − Eψ(X m(cid:96)−1

T

)

.

(34)

Now, for the last term of relation (33), under assumption (Hψ), we apply the Taylor’s expansion
theorem twice and we get

ψ(X m(cid:96)

T ) − ψ(X m(cid:96)−1

T

) = ∇ψ(XT ).U m(cid:96),m(cid:96)−1

T

+ (X m(cid:96)

T − XT )ε(XT , X m(cid:96)

T − XT ) − (X m(cid:96)−1

T

− XT )ε(XT , X m(cid:96)−1

T

− XT ).

13

− XT )

The function ε is given by the Taylor-young expansion, so it satisﬁes ε(XT , X m(cid:96)
(cid:96)→∞ 0. By property (P), we get the tightness of r(cid:96) (X m(cid:96)
P−→
and ε(XT , X m(cid:96)−1
(cid:17) P−→
r(cid:96) (X m(cid:96)−1

− XT ) and we deduce
(X m(cid:96)

− XT )ε(XT , X m(cid:96)−1

T − XT )ε(XT , X m(cid:96)

− XT )

(cid:16)

r(cid:96)

T

T

T

(cid:96)→∞ 0.

T − XT )
P−→
(cid:96)→∞ 0
T − XT ) and

So, according to the stable convergence theorem, we conclude that

T

T − XT ) − (X m(cid:96)−1
(cid:17) stably

(cid:16)

(35)

)
Using (26), it follows from property (P) that

r(cid:96)

T

ψ(X m(cid:96)

∀a > 1, sup

as (cid:96) → ∞.

E(cid:12)(cid:12)(cid:12)r(cid:96)

=⇒ ∇ψ(XT ).UT ,

T ) − ψ(X m(cid:96)−1
(cid:16)
(cid:17)(cid:17)k → ˜E (∇ψ(XT ).UT )k < ∞ for k ∈ {1, 2}.

T ) − ψ(X m(cid:96)−1

(cid:17)(cid:12)(cid:12)(cid:12)2a

< ∞.

ψ(X m(cid:96)

)

)

T

(cid:96)

Hence, we deduce using relation (35) that

Then, r(cid:96)
Consequently, by applying Toeplitz lemma, we obtain that

))

(36)
converges to ˜E (∇ψ(XT ).UT ) = 0 as (cid:96) goes to inﬁnity.

(cid:104)Eψ(X m(cid:96)

(cid:105)(cid:17)2

T ) − Eψ(X m(cid:96)−1

T

)

= 0.

r(cid:96)

E(cid:16)
(cid:16)
(cid:16)E(ψ(X m(cid:96)
ψ(X m(cid:96)
T )) − E(ψ(X m(cid:96)−1
L(cid:88)

(cid:17)
T ) − ψ(X m(cid:96)−1

T

T

(cid:16)

lim
(cid:96)→∞

1(cid:80)L
i−1 ∈ K, we have

a(cid:96)

r(cid:96)

(cid:96)=1 a(cid:96)

(cid:96)=1

(cid:17)2

(cid:12)(cid:12)(cid:12)(cid:12)(cid:16)

Hence, by using Cesaro’s lemma, we obtain that

We focus now on the asymptotic behavior of the ﬁrst term on the right hand side on relation
(33). Since θm(cid:96)

ψ(X m(cid:96)

T ) − ψ(X m(cid:96)−1

∃c > 0, sup
ec|WT |+ c2
i∈N
This upper bound is clearly integrable using property (P) and assumption (26) together with
the H¨older’s inequality. Therefore, by the dominated convergence and under assumption (H(cid:48)
θ),
we obtain that

T ) − ψ(X m(cid:96)−1

i−1 ·WT + 1

e−θ(cid:96),m(cid:96)

ψ(X m(cid:96)

i−1 |2T

2|θ(cid:96),m(cid:96)

)

)

T

T

2 T .

E

lim
i→∞

[ψ(X m(cid:96)

T ) − ψ(X m(cid:96)−1

T

)]2e−θ(cid:96),m(cid:96)

i−1 ·WT + 1

2|θ(cid:96),m(cid:96)

i−1 |2T

=

(cid:12)(cid:12)(cid:12)(cid:12) ≤(cid:16)

(cid:17)2

(cid:19)
E(cid:16)

E(cid:16)

(cid:96) |2T(cid:17)

)]2e−θ∗

(cid:96) ·WT + 1

2|θ∗

(cid:96) |2T(cid:17)

)]2e−θ∗

(cid:96) ·WT + 1

2|θ∗

.

.

[ψ(X m(cid:96)

T

T ) − ψ(X m(cid:96)−1
(cid:19)

[ψ(X m(cid:96)

T ) − ψ(X m(cid:96)−1

T

(cid:18)

(cid:18)

N(cid:96)(cid:88)

lim
i→∞

1
N(cid:96)

E

[ψ(X m(cid:96)

T ) − ψ(X m(cid:96)−1

T

)]2e−θ(cid:96),m(cid:96)

i−1 ·WT + 1

2|θ(cid:96),m(cid:96)

i−1 |2T

=

i=1

14

Otherwise, thanks to (35), we have

r(cid:96) [ψ(X m(cid:96)

T ) − ψ(X m(cid:96)−1

T

)]e− 1
2 θ∗

(cid:96) ·WT + 1

4|θ∗

=⇒
(cid:96)→∞ ∇ψ(XT ) · UT e− 1
(cid:96) |2T stably

2 θ∗·WT + 1

4|θ∗|2T .

(37)

Moreover, for a > 1 we have by Cauchy-Schwartz inequality

E(cid:12)(cid:12)(cid:12)r(cid:96) [ψ(X m(cid:96)

T ) − ψ(X m(cid:96)−1

T

)]e− 1
2 θ∗

(cid:96) ·WT + 1

4|θ∗

(cid:96) |2T(cid:12)(cid:12)(cid:12)2a
(cid:20)

≤ r2a

E(cid:12)(cid:12)(cid:12)ψ(X m(cid:96)

(38)
(cid:96) )1≤(cid:96)≤L ∈ K and thanks to (26) together with property (P), we obtain thanks to (38)

e

)

(cid:96)

2

Since (θ∗

T ) − ψ(X m(cid:96)−1

T

a(2a+1)

|θ∗
(cid:96) |2T .

T ) − ψ(X m(cid:96)−1

T

)]e− 1
2 θ∗

(cid:96) ·WT + 1

4|θ∗

< ∞.

(39)

(cid:12)(cid:12)(cid:12)4a(cid:21) 1

2

(cid:96) |2T(cid:12)(cid:12)(cid:12)2a

E(cid:12)(cid:12)(cid:12)r(cid:96) [ψ(X m(cid:96)

sup

(cid:96)

Then, by the stable convergence obtained in (37) and the uniform integrability property given
by (39) and under the assumption (H(cid:48)

θ), we deduce

E(cid:16)

(cid:96)→∞ r2
lim

(cid:96)

[ψ(X m(cid:96)

T ) − ψ(X m(cid:96)−1

T

)]2e−θ∗

(cid:96) ·WT + 1

2|θ∗

(cid:96) |2T(cid:17)

=

E(cid:16)

[∇ψ(XT ) · UT ]2 e−θ∗·WT + 1

2|θ∗|2T(cid:17)

.

(40)

Now, considering once again the ﬁrst term on the right hand side of relation (33) and using
Toeplitz lemma, we obtain that

L(cid:88)

1(cid:80)L

lim
(cid:96)→∞

a(cid:96)

m(cid:96)

(m − 1)T

(cid:96)=1 a(cid:96)

(cid:96)=1

E(cid:16)

[ψ(X m(cid:96)

T ) − ψ(X m(cid:96)−1

T

)]2e−θ∗

(cid:96) ·WT + 1

2|θ∗

E(cid:16)

2|θ∗|2T(cid:17)

[∇ψ(XT ) · UT ]2 e−θ∗·WT + 1

.

(41)

Hence, combining this last result together with relation (31), we conclude thanks to (33) that

[∇ψ(XT ) · UT ]2 e−θ∗·WT + 1

˜P-a.s.

,

(42)

n|2 = ˜E(cid:16)

E|Q2

lim
(cid:96)→∞

This completes the proof.

Our aim now is to prove a central limit theorem for the adaptive Euler MLMC method.

(cid:96) |2T(cid:17)

=

2|θ∗|2T(cid:17)

3.2 Central Limit Theorem and Berry-Essen type bound
Theorem 3.2. Under assumptions of Theorem 3.1 and for the choice of N(cid:96), (cid:96) ∈ {0, 1, ..., L}
given by (6), the following convergence holds

where ˜σ2 := ˜E(cid:104)

nα (Qn − Eψ(XT ))

2|θ∗|2T(cid:105)

.

[∇ψ(XT ) · UT ]2 e−θ∗·WT + 1

n→∞ N(cid:0)Cψ, ˜σ2(cid:1) ,

L−→

15

Proof. We consider the same decomposition given by relation (27) in the beginning of the proof
of Theorem 3.1. Under assumption (Hεn), the last term on the right hand side of this relation
converges to Cψ as n goes to ∞. For the convergence of the term Q1
n, we use the result (31)
P−→
obtained in Theorem 3.1 and we deduce that Q1
n→∞ 0. Concerning the convergence of the
n
term Q2
n, we plan to use the Lindeberg-Feller central limit theorem (see Theorem 5.1) with the
Lyapunov condition. We introduce the independent random variables

(S(cid:96),N(cid:96), (cid:96) ∈ {1, ..., L}) where (S(cid:96),k =

nα
N(cid:96)

m(cid:96),m(cid:96)−1,θ(cid:96),m(cid:96)
i−1
T,i

Z

, k ≥ 1) for (cid:96) ∈ {1, ..., L},

(43)

k(cid:88)

i=1

and we need to check the assertions A1. and A3. in Theorem 5.1. More precisely, we will prove

(cid:80)L

) = ˜E(cid:16)
(cid:80)L

A1.

lim
L→∞

E(S2

(cid:96),N(cid:96)

(cid:96)=1

[∇ψ(XT ) · UT ]2 e−θ∗·WT + 1

A3. For p > 2,

Concerning the ﬁrst assertion A1., we have that (cid:80)L

lim
L→∞

) = 0.

E(Sp

(cid:96),N(cid:96)

(cid:96)=1

relation (42) in the proof of Theorem 3.1, we get

2|θ∗|2T(cid:17)

.

E(S2

(cid:96),N(cid:96)

) = E(Q2

(cid:96)=1

n)2. Therefore, using

L(cid:88)

(cid:96)=1

lim
(cid:96)→∞

E(S2

(cid:96),N(cid:96)

) = ˜E(cid:16)

[∇ψ(XT ) · UT ]2 e−θ∗·WT + 1

˜P-a.s.

,

2|θ∗|2T(cid:17)

Now, it remains to verify the assertion A3. We get by Burkholder’s inequality (see Theorem
2.10 in [15]): for p > 2, there exists Cp > 0 such that

Moreover, by using Jensen’s inequality, we obtain for p > 2

Using that N(cid:96) = n2α(m−1)T

(cid:96)=1 a(cid:96) (see relation (6)) and by conditioning, we get

(cid:96)=1

(cid:96)=1

E|S(cid:96),N(cid:96)|p ≤ Cp
L(cid:88)

E|S(cid:96),N(cid:96)|p ≤ L(cid:88)
(cid:80)L
E|S(cid:96),N(cid:96)|p ≤ L(cid:88)
L(cid:88)

((cid:80)L
L(cid:88)

E|S(cid:96),N(cid:96)|p ≤ 2p−1Cp

m(cid:96)a(cid:96)

Cp

(cid:96)=1

(cid:96)=1

(cid:96)=1

L(cid:88)

(cid:96)=1

We write also

E

nαp
N p/2

(cid:96)

1
N(cid:96)

(Z m(cid:96),m(cid:96)−1,θ(cid:96),m(cid:96)

i

T,i

)2

.

(44)

(cid:34)

N(cid:96)(cid:88)
(cid:34)

i=1

N(cid:96)(cid:88)

i=1

Cp

E

nαp
N p/2

(cid:96)

1
N(cid:96)

(Z m(cid:96),m(cid:96)−1,θ(cid:96),m(cid:96)

i

T,i

(cid:35)p/2

(cid:35)

)p

.

(cid:21)(cid:33)

E

(cid:96) (Z m(cid:96),m(cid:96)−1,θ(cid:96),m(cid:96)
rp

T,i

i

)p| ˜FT,i−1

i=1

A(cid:96) + 2p−1Cp

L(cid:88)

(cid:96)=1

((cid:80)L

ap/2
(cid:96)
(cid:96)=1 a(cid:96))p/2

B(cid:96).

(45)

(cid:32)

(cid:20)

N(cid:96)(cid:88)

ap/2
(cid:96)
(cid:96)=1 a(cid:96))p/2

E

1
N(cid:96)

((cid:80)L

ap/2
(cid:96)
(cid:96)=1 a(cid:96))p/2

16

where

A(cid:96) =

B(cid:96) =

N(cid:96)(cid:88)
(cid:16)E(cid:104)

1
N(cid:96)

E(cid:104)(cid:12)(cid:12)(cid:12)r(cid:96) (ψ(X m(cid:96)

T ) − ψ(X m(cid:96)−1
(cid:105)(cid:17)p

T

.

i=1
r(cid:96) (ψ(X m(cid:96)

T ) − ψ(X m(cid:96)−1

T

))

(cid:12)(cid:12)(cid:12)p

))

e−(p−1)θm(cid:96)

i−1·WT −(p/2−3/2)|θm(cid:96)

i−1|2T(cid:105)

,

i−1 is ˜FT,i−1-measurable and thanks to
Since for each (cid:96) ∈ {1, ..., L}, X (cid:96),m(cid:96)
Girsanov theorem, we obtain the last equality (45) by introducing a new couple of random
˜FT,i. By using relation (36) together with
variables (X m(cid:96)
assumption (W), we deduce that the last term on the right hand side in (45) converges to 0 as
(cid:96) goes to inﬁnity

T , WT ) independent of ˜FT = ∪i≥1

T,i ⊥⊥ ˜FT,i−1 and θm(cid:96)

B(cid:96) = 0.

(46)

L(cid:88)

(cid:96)=1

((cid:80)L

ap/2
(cid:96)
(cid:96)=1 a(cid:96))p/2

lim
(cid:96)→∞

Now, we focus on the convergence of the ﬁrst term on the right hand side in (45). Since
i−1 ∈ K, there exists c > 0 such that
θm(cid:96)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)r(cid:96) (ψ(X m(cid:96)

sup
i∈N

T ) − ψ(X m(cid:96)−1

T

))

e−(p−1)θm(cid:96)

i−1·WT −(p/2−3/2)|θm(cid:96)

(cid:12)(cid:12)(cid:12)p

i−1|2T(cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12)p

≤(cid:12)(cid:12)(cid:12)r(cid:96) (ψ(X m(cid:96)

e(p−1)c|WT |+(p/2−3/2)c2T .
This upper bound is clearly integrable using property (P) and assumption (26) together with
the H¨older’s inequality. Therefore, by the dominated convergence and under assumption (H(cid:48)
θ),
we obtain that

T ) − ψ(X m(cid:96)−1

))

T

E(cid:104)(cid:12)(cid:12)(cid:12)r(cid:96) (ψ(X m(cid:96)

lim
i→∞

T ) − ψ(X m(cid:96)−1

T

i−1|2T(cid:105)
(cid:12)(cid:12)(cid:12)p

))

i−1·WT −(p/2−3/2)|θm(cid:96)
T ) − ψ(X m(cid:96)−1

T

e−(p−1)θ∗

(cid:96) ·WT −(p/2−3/2)|θ∗

Then, by applying Cesaro’s lemma, we have that

E(cid:104)(cid:12)(cid:12)(cid:12)r(cid:96) (ψ(X m(cid:96)

T ) − ψ(X m(cid:96)−1

e−(p−1)θm(cid:96)

i−1·WT −(p/2−3/2)|θm(cid:96)

Moreover, we obtain under assumption (H(cid:48)

θ) and thanks to relations (37) and (38) that

T ) − ψ(X m(cid:96)−1

T

))

e−(p−1)θ∗

(cid:96) ·WT −(p/2−3/2)|θ∗

(cid:96) |2T(cid:105)

(cid:96) |2T(cid:105)

.

.

E(cid:104)(cid:12)(cid:12)(cid:12)r(cid:96) (ψ(X m(cid:96)

T ) − ψ(X m(cid:96)−1

E(cid:104)(cid:12)(cid:12)(cid:12)r(cid:96) (ψ(X m(cid:96)

T

= lim
(cid:96)→∞

e−(p−1)θm(cid:96)

i−1·WT −(p/2−3/2)|θm(cid:96)

T ) − ψ(X m(cid:96)−1

T

))

e−(p−1)θ∗

(cid:96) ·WT −(p/2−3/2)|θ∗

(cid:96) |2T(cid:105)

2|θ∗|2T(cid:17)

[∇ψ(XT ) · UT ]p e−θ∗·WT + 1

.

(47)

(cid:12)(cid:12)(cid:12)p
= ˜E(cid:16)

17

))

e−(p−1)θm(cid:96)

(cid:12)(cid:12)(cid:12)p
= E(cid:104)(cid:12)(cid:12)(cid:12)r(cid:96) (ψ(X m(cid:96)
(cid:12)(cid:12)(cid:12)p
= E(cid:104)(cid:12)(cid:12)(cid:12)r(cid:96) (ψ(X m(cid:96)
(cid:12)(cid:12)(cid:12)p

))

))

T

(cid:12)(cid:12)(cid:12)p

i−1|2T(cid:105)

i−1|2T(cid:105)

N(cid:96)(cid:88)

i=1

N(cid:96)(cid:88)

i=1

lim
i→∞

1
N(cid:96)

lim
(cid:96)→∞

1
N(cid:96)

Once again, by using assumption (W) (lim(cid:96)→∞

(cid:80)L

(cid:96)=1 ap/2

(cid:96) = 0), we conclude that

((cid:80)L

1

(cid:96)=1 a(cid:96))p/2

lim
(cid:96)→∞

This completes the proof.

L(cid:88)

(cid:96)=1

E|S(cid:96),N(cid:96)|p = 0.

n|2 +(cid:80)L

(cid:80)L

Now, we prove a Berry-Essen type bound on our central limit theorem. This improves
n|3 +

E|S(cid:96),N(cid:96)|2 and ρn = E|Q1

the relevance of the above result. We set s2

n = E|Q1

(cid:96)=1

E|S(cid:96),N(cid:96)|3, where Q1

(cid:96)=1

n and S(cid:96),N(cid:96) are respectively given by relations (28) and (43).

Remark 3. The Berry-Essen type bound is a new type of result that we obtained in this paper
compared to our previous work [2].

Proposition 3.1. Under assumptions of Theorem 3.1, we deﬁne by Fn the distribution function
of nα(Qn − Eψ(X n

T ))/sn. Then, we have

|Fn(x) − G(x)| ≤ 6ρn
s3
n

,

where G is the distribution function of a standard Gaussian random variable. Moreover, if ψ
is continuous Lipschitz function, there exists a constant C > 0 such that

|Fn(x) − G(x)| ≤ 1
s3
n

L(cid:88)

a3/2
(cid:96)

.

((cid:80)L

C

(cid:96)=1 a(cid:96))3/2

(cid:96)=1

√
For the optimal choice a(cid:96) = 1, the obtained Berry-Essen type bound is of order 1/
log n.
Proof. Using Theorem 2 page 544 in [11] and since nα(Qn−E(ψ(X n
(cid:96)=0 Xn,(cid:96),
by taking p = 3 in both inequalities (44) and (45) in the proof of Theorem 3.2, we deduce a
Berry-Essen bound on our central limit theorem.

n =(cid:80)L

T ))) = Q1

n +Q2

|Fn(x) − G(x)| ≤ 6ρn
s3
n

.

(48)

When ψ is Lipschitz, using property (P), there exists a positive constant C depending on b,σ,
T and ψ such that

ρn ≤

((cid:80)L

C

(cid:96)=1 a(cid:96))3/2

(cid:96)=1

L(cid:88)

a3/2
(cid:96)

.

Hence, the Berry-Essen type bound on our central limit theorem is given by

|Fn(x) − G(x)| ≤ 1
s3
n

and s3

n are asymptotically constant.

L(cid:88)

a3/2
(cid:96)

,

((cid:80)L

C

(cid:96)=1 a(cid:96))3/2

(cid:96)=1

18

4 Complexity analysis and numerical results
According to Theorem 3.2, we deduce that for a total error of order 1/nα, α ∈ (1/2, 1], the
minimal computational eﬀort necessary to run the adaptive MLMC algorithm is obtained for
a sequence of sample sizes speciﬁed by relation (6) This leads to a time complexity given by

(cid:33)

L(cid:88)

(cid:32)
(cid:32)

(cid:18)(m − 1)T

a0 log m

CM LM C = C ×

= C ×

N0 +

N(cid:96)(m(cid:96) + m(cid:96)−1)

with C > 0

(cid:96)=1

n2α(m − 1)T

L(cid:88)

a0

(cid:96)=1

a(cid:96) + n2α (m2 − 1)T

m

(cid:33)

a(cid:96)

with C > 0.

L(cid:88)

(cid:96)=1

1
a(cid:96)

L(cid:88)

(cid:96)=1

So the time complexity reaches its minimum for the choice of weights a∗
The optimal complexity of the Euler MLMC method is given by

(cid:96) = 1, (cid:96) ∈ {1, ..., L}.

CM LM C = C ×

n2α log n +

(m2 − 1)T
m(log m)2 n2α(log n)2

= O(n2α(log n)2).

(cid:19)

We conclude that the adaptive MLMC method is more eﬃcient in terms of time complexity in
comparison with the Monte Carlo method one CM C = O(n2α+1).
We consider the problem of the an option pricing under the Heston model. We remind that
the the Heston model is a popular stochastic volatility model in ﬁnance introduced by Heston

in [16] solution to (cid:40)

(cid:112)

(cid:112)

dSt = rStdt +
dVt = κ(¯v − Vt)dt + σ

VtStdW 1
t
VtρdW 1

t + σ

(cid:112)

Vt

(cid:112)

1 − ρ2dW 2
t ,

where W 1 and W 2 are two independent Brownian motions. Parameters κ, σ, ¯v and r are
strictly positive constants and |ρ| ≤ 1. In this model, κ is the rate at which Vt reverts to ¯v, ¯v is
the long run average price variance, σ is the volatility of the variance, r is the interest rate and
ρ is a correlation term. Firstly, our aim is to use the importance sampling method in order to
reduce the variance when computing the price of an European option, with strike K, under the
Heston model. The price is e−rT Eψ(ST ) where ψ(ST ) = (ST − K)+ is the payoﬀ of the option.
After a density transformation, given by Girsanov theorem, the price will be deﬁned by:

The price of the European call option is approximated by

e−rT E(cid:104)
(cid:105)

g(θ, Sn,θ
T )

e−rT E(cid:104)

2|θ|2T(cid:105)

ψ(Sθ
T )

e−θ.WT − 1

,

θ ∈ R2.

ψ(Sn,θ

T ) e−θ.WT − 1

,

θ ∈ R2.

= e−rT E(cid:104)
(cid:20)(cid:16)∇ψ(Sn,θ

2|θ|2T(cid:105)

(cid:17)2

e−2θ.WT −|θ|2T

(cid:21)

,

ˆθ∗
E
n = arg min

θ∈R2

T ).U n,θ

T

19

The optimal ˆθ∗ of the MLMC method is given by

where U n,θ denotes the Euler discretization scheme obtained when we replace coeﬃcients b and σ
of relation (10) by the corresponding parameters in the Heston model. Here, in order to compare
the adaptive Euler MLMC algorithm with the adaptive SR and adaptive Monte Carlo ones, we
use the constrained algorithm to approximate ˆθ∗ which is given by this routine let (Ki)i∈N denote
i=0 Ki = Rd and Ki (cid:40) ◦Ki+1,∀i ∈ N. For
an increasing sequence of compact sets satisfying ∪∞
0 ∈ K0, αn
θn
i )i∈N

0 = 0 and a gain sequence (γi)i∈N satisfying (20), we deﬁne the sequence (θn

i , αn

recursively by if

i−1 − γiH M L(θn
θn
θn
i = θn
i = θn
else θn

i−1, Sn
T,i, U n
i−1 − γiH M L(θn
i−1, Sn
i = αn
0 and αn
i−1 + 1,
i T − WT,i)

T,i, U n

T,i, WT,i) ∈ Kαn
T,i, U n

, then
T,i, WT,i), and αn

i−1

i = αn

i−1

(49)

(∇ψ(Sn

i−1, Sn

T,i, WT,i) = (θn

i−1|2T . We
where H M L(θn
choose the parameters in the Heston model: S0 = 100, V0 = 0.01, K = 100, the free interest
rate r = log(1.1), σ = 0.2, k = 2, ¯v = 0.01, ρ = 0.5 and maturity time T = 1. We run a number
of iterations M = 500 000 and we obtain the two-dimensional vector of the optimal theta:
ˆθ∗
n = (0.4398, 0.5902). Our aim now, is to compare the importance sampling Multilevel Monte
Carlo method (denoted MLMCIS) with the two methods already studied in our previous work
[2], the importance sampling Statistical Romberg method (SRIS) and importance sampling
Monte Carlo method (MCIS) to compute the price of the European call option:

T,i)2 e−θn

i−1.WT,i+ 1

T,i).U n

2|θn

- MLMC+IS method: European option price approximation with N(cid:96) given by expression

(6)

e−rT
N0

N0(cid:88)

i=1

+

g(ˆθn

L(cid:88)

M , Sm0,ˆθn

T,i

, U n

T,i, WT,i)

M

N(cid:96)(cid:88)

(cid:16)

e−rT
N(cid:96)

(cid:96)=1

i=1

g(ˆθn

M , S(cid:96),m(cid:96),ˆθn

T,i

M

T,i, WT,i) − g(ˆθn

M , S(cid:96),m(cid:96)−1,ˆθn

T,i

M

, U n

(cid:17)

, U n

T,i, WT,i)

.

(50)

- SR+IS method: European option price approximation method with N1 = n2 and N2 = n 3

2

N1(cid:88)

e−rT
N1

√

g(˜θn

M , ˆS

T,i

n,˜θn
M

, U n

T,i, WT,i)

i=1

e−rT
N2

+

N2(cid:88)

(cid:16)

i=1

g(˜θn

M , Sn,˜θn

T,i

M

T,i, WT,i) − g(˜θn

, U n

M , S

√

T,i

n,˜θn
M

(cid:17)

, U n

T,i, WT,i)

- MC+IS method: European option price approximation with N = n2

N(cid:88)

i=1

e−rT
N

N(cid:88)

i=1

g(θn

M , Sn,θn

T,i

M

) =

e−rT
N

20

ψ(Sn,θn

M

T,i

)e−θn

M .WT,i− 1

2|θn

M|2T .

.

(51)

(52)

We consider a set of values N = 30, we compute the diﬀerent estimators of the European call
option (50), (51) and (52), for diﬀerent values of the discretization steps n. For each value n
and for each method, we compute the CPU time and the mean squared-error which is given by

(53)

M(cid:88)

i=1

M SE =

1
M

(Real value − Simulated value).

Hence, for each given n and for each method, we provide a couple of points (MSE, CPU time)
which are plotted on Figure 1.

Figure 1: CPU time versus MSE for European call option

Now let us interpret Figure 1. The curves of the importance sampling Statistical Romberg
(SRIS) and the importance sampling Multilevel Monte Carlo (MLMCIS) methods are displaced
below the curve of the importance sampling Monte Carlo (MCIS) method. Therefore, for a
given error, the number of values computed in one second by the two methods is larger than the
values computed by the MCIS procedure. For a Mean Square Error (MSE) lower than 10−5,
we observe that the MLMCIS procedure becomes more eﬀective than both methods.

5 Appendix

5.1 Lindeberg Feller Central Limit Theorem for independent ran-

dom variables

We recall ﬁrst the Lindeberg Feller central limit theorem for independent random variables.

21

Theorem 5.1 (Lindeberg Feller Central Limit Theorem [5]). Let (kn)n∈N be a sequence such
that kn −→ ∞, as n −→ ∞ and for each n ∈ N we consider a sequence Xn1, Xn2, ..., Xnkn of
independent centered and real square integrable random variables. We make the following two
assumptions.

A1 . There exists a positive constant v such that(cid:80)kn
A2 . Lindeberg’s condition holds: that is for all ε > 0,(cid:80)kn

i=1

E(Xni)2 −→

n→∞ v.

E(|Xni|21|Xni|≥ε) −→

n→∞ 0. Then

i=1

kn(cid:88)

L−→ N (0, v)

Xni

as n → ∞.

i=1

Remark. The following assumption known as the Lyapunov condition implies the Lindeberg’s
condition A2..

A3 . There exists a real number a > 1 such that

kn(cid:88)

E(cid:2)|Xni|2a(cid:3) −→

n→∞ 0.

k=1

References

[1] B. Arouna. Adaptative Monte Carlo method, a variance reduction technique. Monte Carlo

Methods Appl., 10(1):1–24, 2004.

[2] M. Ben Alaya, K. Hajji, and A. Kebaier. Importance sampling and statistical Romberg

method. Bernoulli, 21(4):1947–1983, 2015.

[3] M. Ben Alaya and A. Kebaier. Multilevel Monte Carlo for Asian options and limit theo-

rems. Monte Carlo Methods Appl., 20(3):181–194, 2014.

[4] M. Ben Alaya and A. Kebaier. Central limit theorem for the multilevel Monte Carlo Euler

method. Ann. Appl. Probab., 25(1):211–234, 2015.

[5] P. Billingsley. Convergence of probability measures. John Wiley & Sons Inc., New York,

1968.

[6] N. Bouleau and D. L´epingle. Numerical methods for stochastic processes. Wiley Series in
Probability and Mathematical Statistics: Applied Probability and Statistics. John Wiley
& Sons Inc., New York, 1994. A Wiley-Interscience Publication.

[7] H. F. Chen, G. Lei, and A. J. Gao. Convergence and robustness of the Robbins-Monro
algorithm truncated at randomly varying bounds. Stochastic Process. Appl., 27(2):217–231,
1988.

22

[8] H. F. Chen and Y. M. Zhu. Stochastic approximation procedures with randomly varying

truncations. Sci. Sinica Ser. A, 29(9):914–926, 1986.

[9] N. Collier, A. Haji-Ali, F. Nobile, E. von Schwerin, and R. Tempone. A continuation

multilevel Monte Carlo algorithm. BIT, 55(2):399–432, 2015.

[10] S. Dereich. Multilevel Monte Carlo algorithms for L´evy-driven SDEs with Gaussian cor-

rection. Ann. Appl. Probab., 21(1):283–311, 2011.

[11] W. Feller. An introduction to probability theory and its applications. Vol. II. Second

edition. John Wiley & Sons, Inc., New York-London-Sydney, 1971.

[12] M. B. Giles. Multilevel Monte Carlo path simulation. Oper. Res., 56(3):607–617, 2008.

[13] M. B. Giles and L. Szpruch. Antithetic multilevel Monte Carlo estimation for multi-
dimensional SDEs without L´evy area simulation. Ann. Appl. Probab., 24(4):1585–1620,
2014.

[14] P. Glasserman, P. Heidelberger, and P. Shahabuddin. Asymptotically optimal importance
sampling and stratiﬁcation for pricing path-dependent options. Math. Finance, 9(2):117–
152, 1999.

[15] P. Hall and C. C. Heyde. Martingale limit theory and its application. Academic Press Inc.
[Harcourt Brace Jovanovich Publishers], New York, 1980. Probability and Mathematical
Statistics.

[16] S. L. Heston. A closed-form solution for options with stochastic volatility with applications

to bond and currency options. Review of Financial Studies, 6:327–343, 1993.

[17] H. Hoel, E. von Schwerin, A. Szepessy, and R. Tempone. Implementation and analysis of
an adaptive multilevel Monte Carlo algorithm. Monte Carlo Methods Appl., 20(1):1–41,
2014.

[18] J. Jacod and P. Protter. Asymptotic error distributions for the Euler method for stochastic

diﬀerential equations. Ann. Probab., 26(1):267–307, 1998.

[19] B. Jourdain and J Lelong. Robust adaptive importance sampling for normal random

vectors. Annals of Applied Probability, 19 (5):1687–1718, 10 2009.

[20] R. Kawai. Optimal importance sampling parameter search for L´evy processes via stochastic

approximation. SIAM J. Numer. Anal., 47(1):293–307, 2008/09.

[21] A. Kebaier. Statistical Romberg extrapolation: a new variance reduction method and

applications to option pricing. Ann. Appl. Probab., 15(4):2681–2705, 2005.

[22] J. Kebaier, A. Lelong. Coupling importance sampling and multilevel monte carlo using

sample average approximation. 2015.

23

[23] S. Laruelle, C. Lehalle, and G. Pag`es. Optimal posting price of limit orders: learning by

trading. Math. Financ. Econ., 7(3):359–403, 2013.

[24] V. Lemaire and G. Pag`es. Unconstrained recursive importance sampling. Ann. Appl.

Probab., 20(3):1029–1067, 2010.

24

