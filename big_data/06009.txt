Readability-based Sentence Ranking for

Evaluating Text Simpliﬁcation

Sowmya Vajjala

Iowa State University, USA
sowmya@iastate.edu

Detmar Meurers

University of Tuebingen, Germany
dm@sfs.uni-tuebingen.de

6
1
0
2

 
r
a

 

M
8
1

 
 
]
L
C
.
s
c
[
 
 

1
v
9
0
0
6
0

.

3
0
6
1
:
v
i
X
r
a

Abstract

We propose a new method for evaluat-
ing the readability of simpliﬁed sentences
through pair-wise ranking. The validity
of the method is established through in-
corpus and cross-corpus evaluation exper-
iments. The approach correctly identi-
ﬁes the ranking of simpliﬁed and unsim-
pliﬁed sentences in terms of their reading
level with an accuracy of over 80%, signif-
icantly outperforming previous results.
To gain qualitative insights into the na-
ture of simpliﬁcation at the sentence level,
we studied the impact of speciﬁc linguistic
features. We empirically conﬁrm that both
word-level and syntactic features play a
role in comparing the degree of simpliﬁ-
cation of authentic data.
this research, we created
To carry out
a new sentence-aligned corpus from pro-
fessionally simpliﬁed news articles. The
new corpus resource enriches the empir-
ical basis of sentence-level simpliﬁcation
research, which so far relied on a single
resource. Most importantly, it facilitates
cross-corpus evaluation for simpliﬁcation,
a key step towards generalizable results.
Introduction

1
Text simpliﬁcation is the process of simplifying
the linguistic form of a text without losing its
meaning.
It has applications in several domains
ranging from language learning (Petersen and Os-
tendorf, 2007) and biomedical information extrac-
tion (Jonnalagadda and Gonzalez, 2010) for hu-
man readers all the way to automatic simpliﬁ-
cation designed to improve parsing by machines
(Chandrasekar et al., 1996). While manual sim-
pliﬁcation relies entirely on expert writers, semi-
automatic approaches serve as an assistive tool for

writers, alerting them of text passages that may be
difﬁcult to read for the target audience and indi-
cating how to rewrite them (Candido et al., 2009).
Automatic text simpliﬁcation approaches, gener-
ating simpliﬁed text from an unsimpliﬁed version
by means of hand-crafted rules, data-driven meth-
ods, or hybrid techniques have also been proposed
(e.g., Woodsend and Lapata, 2011; Siddharthan
and Mandya, 2014).

The nature of the simpliﬁcation performed de-
pends on the purpose of the approach. Thus, the
evaluation of a system that aims to improve the
parser speed (Chandrasekar et al., 1996) also dif-
fers from one that was developed to support spo-
ken language understanding (Tur et al., 2011)In
an educational context, typically the purpose is to
adapt the text to a level of complexity facilitat-
ing comprehension by the target audience, such as
language learners or students at a particular grade
level. It thus is important to be able to evaluate
the complexity of simpliﬁed and unsimpliﬁed ver-
sions of a text – which is the issue we address in
this paper. The approach is equally applicable to
identifying those parts of a text that are particu-
larly complex and thus constitute good targets for
simpliﬁcation.

Text simpliﬁcation is generally evaluated in one
of three ways:
through small-scale user evalua-
tions, with a machine translation metric, or us-
ing readability measures (Siddharthan, 2014). We
explore the third option. Evaluating text simpli-
ﬁcation using readability assessment is typically
carried out with traditional readability formulae.
For example, Woodsend and Lapata (2011) make
use of the Flesch-Kincaid Reading Ease formula.
Such readability formulae are based on surface-
level features: the average sentence length, word
length, or lists of difﬁcult words (cf. DuBay,
2006). While such features often correlate with the
actual causes of difﬁculty in a piece of text (e.g.,
complex syntax, infrequent words), manipulating

these surface features does not necessarily result in
more readable texts (cf. Klare, 1974); they merely
are surface indicators of a broad range of underly-
ing linguistic and psychological characteristics of
authentic texts targeting different audiences. Re-
cent research also showed that more sophisticated,
linguistically-grounded models support a more re-
liable assessment of readability (e.g., Nelson et al.,
2012). Although readability assessment is primar-
ily studied at a text-level, recent research explored
it for sentences as well (e.g., Napoles and Dredze,
2010; Medero and Ostendorf, 2011; Pil´an et al.,
2014; Dell’Orletta et al., 2014; Vajjala and Meur-
ers, 2014a). Being able to assess the readability
at the sentence level is important to identify tar-
gets for simpliﬁcation and to quantify the degree
of simpliﬁcation performed for a given sentence.
Vajjala and Meurers (2014a) studied senten-
tial readability for text simpliﬁcation. They show
that a relative comparison instead of an abso-
lute classiﬁcation is better suited to identifying
the difﬁcult sentences compared to simpliﬁed ver-
sions. They used a regression model trained on
whole documents to compare the readability of
parallel sentences from the sentence-aligned Wiki-
SimpleWiki corpus (Zhu et al., 2010). While
agreeing with the general perspective, we propose
a ranking approach which more directly captures
the idea of relative levels of readability, and we
show that it signiﬁcantly outperforms the state of
the art for evaluating sentential simpliﬁcation.

To support an evaluation of sentential simpli-
ﬁcation across different corpus resources,
test-
ing whether something general has been learned,
we created a new corpus of aligned simple-
complex sentence pairs. We collected the data
from the web site OneStopEnglish.com, which of-
fers manually simpliﬁed versions of news articles
for language learners. We tested our approach
with this corpus, with the standard Wikipedia-
SimpleWikipedia sentence-aligned corpus, and
using cross-corpus evaluation. We show that
our approach outperforms previous approaches for
identifying sentential simpliﬁcations and that the
performance generalizes across corpora.

In terms of a qualitative analysis, we compare
groups of features in terms of their contribution to
the ranking model and ﬁnd that both word-level
and sentence-level properties play a role in rank-
ing the sentences by their reading level. While the
psycholinguistic measures of word properties ﬁg-

ure prominently among the top features, the best-
performing model consists of all the features.
In sum, the contributions of this paper are:

1. We propose an approach to evaluate text sim-
pliﬁcation methods in terms of reading lev-
els. Through multiple cross-corpus tests, we
show that the approach performs at an accu-
racy of over 80%.

2. We compiled a new corpus of sentence-level
simpliﬁcations to obtain a broader empirical
basis on which to evaluate and train text sim-
pliﬁcation systems.

3. We explored the role of individual features
and feature groups for this task, including a
comparison them across corpora.

4. In terms of the practical application context
we are targeting, the quantitative and qualita-
tive results in this paper establish that the ap-
proach can meaningfully be used in practice
to evaluate simpliﬁcation systems developed
with the aim of reducing the difﬁculty of in-
formative text for language learners.

The paper is organized as follows: Sections 2
and 3 describe the corpora and feature set we used.
Section 4 discusses our experiments and their re-
sults. Sections 5 and 6 put our research in context
and conclude the paper.

2 Corpora
The practical purpose of our approach is to eval-
uate text simpliﬁcation approaches aimed at help-
ing language learners. Hence we train and test our
models on two corpora created with this target au-
dience in mind.

2.1 OneStopEnglish corpus
OneStopEnglish (OSE) is a resource website for
English teachers published by the Macmillan Ed-
ucation Group. They offer Weekly News Lessons1
consisting of news articles sourced from the news-
paper The Guardian. The articles are rewritten by
teaching experts in a way targeting English lan-
guage learners at three reading levels (elementary,
intermediate, advanced). We obtained permission
to use the articles for research purposes and down-
loaded the weekly lessons from September 2012–
March 2014, which resulted in a collection of 76
1http://www.onestopenglish.com/skills/
news-lessons/weekly-topical-news-lessons

article triplets. Each article is included in its ele-
mentary, its intermediate, and its advanced version
so that overall the corpus contains 228 articles.
Corpus pre-processing The weekly lessons are
pdf ﬁles consisting of a pre-test about the topic of
the article, the re-written news article, and exer-
cises related to the article. We ﬁrst parsed the pdfs
using iTextPDF2 to extract the article text, exclud-
ing everything else. Since our aim is to compare
different versions of a sentence, we took each ar-
ticle triplet and sentence-aligned two at a time us-
ing TF-IDF and cosine similarity, following previ-
ous research on monolingual sentence alignment
(Nelken and Shieber, 2006; Zhu et al., 2010).

OSE3 For the sentences which exist in all
three versions of an article (elementary, interme-
diate, advanced), we obtain a triplet of sentences.
We selected all triplets for which each pair of sen-
tences differed and was above a minimum sim-
ilarity threshold of 0.7 (based on manual quali-
tative analysis using different thresholds). Over-
all, we identiﬁed 837 sentence triplets and refer to
this corpus as OSE3.3 An example of a sentence
rewritten across the three levels is shown below:

Adv: In Beijing, mourners and admirers made
their way to lay ﬂowers and light candles at
the Apple Store.

Int: In Beijing, mourners and admirers came to
lay ﬂowers and light candles at the Apple
Store.

Ele: In Beijing, people went to the Apple Store

with ﬂowers and candles.

OSE2 We compiled a second corpus consist-
ing of pairs of sentences, which we will refer to as
OSE2. We extracted the 3113 pairs of sentences
(elementary-intermediate, intermediate-advanced,
or elementary-advanced) that differed and were
above the minimum similarity threshold.

2.2 Wikipedia-SimpleWiki corpus
Simple English Wikipedia (SimpleWiki) targets
children and adults who are learning English,4 so a
corpus of sentence pairs from Wikipedia and Sim-
pleWiki suits our goal to compare sentence-level

2http://itextpdf.com
3We will share our sentence-aligned corpora for research

purposes under a standard CC BY NC SA license.

4http://simple.wikipedia.org/wiki/

Simple_English_Wikipedia

text simpliﬁcation. We use the sentence-aligned
corpus created by Zhu et al. (2010). They com-
piled a collection of ∼65k parallel articles from
Wikipedia and SimpleWiki to create a sentence-
aligned corpus consisting of ∼100k pairs. We
used a subset of this corpus consisting of 80,912
sentence pairs, after removing the sentence pairs
that are identical in both versions.

3 Features and Methods
3.1 Features
Vajjala and Meurers (2014a) introduce a range of
lexical, syntactic, morphological, and psycholin-
guistic features to build a document-level read-
ability model that performed on par with existing
commercial and academic systems on the Com-
mon Core State Standard test set for English (CC-
SSO, 2010). They show that the model can also be
applied at the sentence-level.

Given our goal of studying the effectiveness of
ranking over regression and the relevance of spe-
ciﬁc features for sentence-level ranking, we built
our models using the same feature set they used,
which makes a direct comparison possible. The
feature set of consists of four groups of features:
• LEX consists of lexical diversity and density
features primarily based on type-token and
POS ratios,
inspired by Second Language
Acquisition (SLA) research, and lexical se-
mantic properties from WordNet (Miller,
1995) such as the average number of senses
of a word.

• SYN includes syntactic features based on
speciﬁc patterns extracted from parse trees,
including measures of syntactic complexity
from SLA research.

• CEL is a group of features encoding morpho-
syntactic properties of lemmas, estimated us-
ing the Celex (Baayen et al., 1995) database.
• PSY contains word-level psycholinguistic
features such as concreteness, meaningful-
ness and imageability extracted from the
MRC psycholinguistic database (Wilson,
1988) and various Age of Acquisition (AoA)
measures released by Kuperman et al. (2012).

3.2 Methods
We model sentential complexity as a pair-wise
ranking problem. Pair-wise ranking is one of the

learning-to-rank approaches, typically used in in-
formation retrieval for ranking search results (Li,
2014). In that scenario, it is used to compare a pair
of documents in terms of their relevance to a given
query. In our case, the aim of the ranker given a
pair of sentences (where one is the simpliﬁed ver-
sion of the other) is to predict which one of them
is simpler than the other. Thus, the learning prob-
lem for us is to compare sentence pairs within a
group of sentences and rank them based on their
complexity, trying to minimize inversion of ranks.
To apply ranking, we need to have a numeric
score for (the feature vector of) each sentence. In
Wiki and OSE2, we assigned a reading level of 2
to the more difﬁcult version and 1 to its simpli-
ﬁed version in the sentence pair. For the sentence
triplets in OSE3, we used the sentences scores
3 (advanced), 2 (intermediate), and 1 (elemen-
tary). Since pair-wise ranking only considers rela-
tive ranks, the ranking procedure is not dependent
on the speciﬁc absolute reading level of a sentence.
In the case of sentences that were split into two in
the simpliﬁed version, we scored both the simple
sentences as 1 so that no pair-wise constraints are
generated between them.

Ranking: We explored three ranking algo-
rithms.

RankSVM (Herbrich et al., 2000)

trans-
forms ranking into a pair-wise classiﬁcation prob-
lem and uses a Support Vector Machine for learn-
ing to rank the sentence pairs. It is one of the most
commonly used ranking algorithms in NLP tasks.
and was also employed in a related task, for rank-
ing children’s literature texts based on their read-
ing level (Ma et al., 2012).

RankNet (Burges et al., 2005)

is a pair-wise
ranking algorithm that is a modiﬁed version of
a traditional back-propagation-based neural net-
work, applied to ranking problems. It is known to
perform well in practice and was successfully used
in a real-life search engine to rank search results.

RankBoost (Freund et al., 2003)

is an algo-
rithm that uses boosting for pair-wise ranking. It
uses a linear combination of several weak rankers
to produce the ﬁnal ranking. The algorithm was
typically used in collaborative ﬁltering problems.
We used publicly available implementations
these algorithms for
training our models:

of

SVMrank (Joachims, 2006)5 for RankSVM and
RankLib6 software for RankNet and RankBoost.
Evaluation: Since our learning goal is to mini-
mize the number of wrongly ranked pairs, we eval-
uate the approach in terms of the percentage of
correctly ordered pairs. In other words, we report
the percentage of pairs in which the difﬁcult ver-
sion gets a higher rank than its simpliﬁed counter-
part. We refer to this as accuracy.

4 Experiments
4.1 Reference performance on Wiki dataset
We start with an experiment directly comparing
the ranking approach with the results reported
in (Vajjala and Meurers, 2014a) on the Wiki-
SimpleWiki data set. They used a regression
model trained on documents to get the reading lev-
els for sentences. Their model identiﬁed the rank
order correctly in 59% of the cases and assigned
equal score to both versions of the sentence in
11% of the cases. So, randomly considering half
of the 11% cases as correctly ordered and half as
wrongly ordered, one obtains a ranking accuracy
of 64.5% for their model.

Replacing regression with ranking, we trained a
model using SVMRank on the entire Wiki dataset
in a 10-fold cross validation (CV) setup. The rank-
ing model achieves an accuracy of 82.7%, which
is a signiﬁcant improvement over the baseline (p
< 0.01). The Standard Deviation between the ten
folds is 8.4%. This high level of variability sug-
gests that the nature of what constitutes simpliﬁ-
cations in SimpleWiki varies signiﬁcantly, as may
be expected for a collaborative editing setup – a
potentially interesting issue to explore in the fu-
ture.

As several text simpliﬁcation approaches (Zhu
et al., 2010; Woodsend and Lapata, 2011) used
the Flesch-Kincaid Grade Level formula, which is
based on the average word and sentence length as
surface features, as a readability measure for text
simpliﬁcation, we also determined the accuracy
of ranking the sentences in the Wiki-SimpleWiki
data using this formula and obtained an accuracy
of 72.3%.

As summed up in Table 1, on the Wiki-
SimpleWiki dataset the ranking approach clearly

5http://www.cs.cornell.edu/people/tj/

svm_light/svm_rank.html

6http://sourceforge.net/p/lemur/wiki/

RankLib

outperforms the regression approach of Vajjala
and Meurers (2014a) and the Flesch-Kincaid read-
ability formula. The rich linguistic feature set we
have adapted from Vajjala and Meurers (2014a)
thus can clearly outperform the surface-based
readability formula, but the richer information
only becomes effective when the relative readabil-
ity of pairs of sentences is learned using a dedi-
cated ranking algorithm.

Approach
Vajjala and Meurers (2014a)
Flesch-Kincaid formula
Our RankSVM approach

Accuracy

64.5%
72.3%
82.7%

Table 1: Ranking accuracy on Wiki-SimpleWiki

To explore things further, we next compared dif-
ferent ranking approaches and tested the general-
izability of the ranking approach in a cross-corpus
setup and in multi-level simpliﬁcation scenarios.

4.2 Ranking algorithms and generalizability
To compare the three ranking algorithms intro-
duced in Section 3.2, we ﬁrst trained ranking mod-
els for the WIKI and OSE2 corpora. To make the
results comparable for these two corpora, we se-
lected 2,000 sentence pairs for each of the training
sets (WIKI-TRAIN, OSE2-TRAIN), and used the
remaining part as the test set (WIKI-TEST: 78,912
pairs, OSE2-TEST: 1,113 pairs).

Table 2 presents the performance of the three
ranking algorithms on the two training sets for
within and cross-corpus evaluation. As a baseline,
the Flesch-Kincaid formula results in 69.0% for
WIKI-TEST and 69.6% for OSE2-TEST.

TRAIN
TEST
WIKI WIKI
WIKI OSE2
OSE2 WIKI
OSE2 OSE2

RankSVM RankNet RankBoost

81.8%
74.6%
77.5%
81.5%

72.5%
59.1%
73.8%
69%

76.4%
70.2%
74.8%
75.5%

Table 2: Accuracies for the three rank algorithms

RankSVM performs best among the ranking al-
gorithms we tried. This also generalizes to the
cross-corpus tests. In the following experiments,
we therefore only report the results for RankSVM.
Cross-corpus evaluation always shows a drop in
performance. The drop is smaller for the model
trained on the OSE2 corpus, which suggests that
the OSE2 corpus covers a broader, more represen-
tative range of simpliﬁcations. Taking that idea

further, we explored improving cross-corpus per-
formance using two methods enriching the train-
ing data.

Improving cross-corpus performance

4.3
First, we combined the two training sets to create
a hybrid training set WIKI-OSE2-TRAIN, which
should increase the representativity and range of
the simpliﬁcations included in the training data.

Second, we used the three level corpus OSE3
to train the ranker to simultaneously consider a
broader range of simpliﬁcations: the ranker will
learn a single set of weights for ranking the
three pairs in a set for OSE3, instead of three
sets of weights for ranking each pair indepen-
dently. We randomly selected 750 sentence triplets
from the OSE3 corpus as training set (OSE3-
TRAIN), leaving the remaining 87 as held-out test
set (OSE3-TEST). Table 3 shows the results on
the three test sets for the models trained on WIKI-
OSE2-TRAIN and OSE3-TRAIN. The baseline
accuracy obtained using the Flesch-Kincaid for-
mula for OSE3-TEST is 71.6%.

WIKI-OSE2-TRAIN OSE3-TRAIN

WIKI-TEST
OSE2-TEST
OSE3-TEST

81.3%
80.7%
79.7%

78.6%
82.4%
79.7%7

Table 3: Accuracies for the extended training sets

As expected, the accuracy for the combined, more
varied training set WIKI-OSE2-TRAIN results in
a comparable performance across the three tests
sets. It seems to account for a broader range of
simpliﬁcation options. The results for the OSE3-
TRAIN set, providing the ranker with triples over
which to learn the weights, are less clear.

Overall, the fact that cross-corpus and same-
corpus results are relatively close together sup-
ports
reliable sentence-
level readability ranking models which generalize
across very different data sets can be built.

the assumption that

Inﬂuence of the amount of training data

4.4
While the WIKI-OSE2-TRAIN contains more di-
verse data, it also contains twice as much data as
the two smaller training sets it consists of. To iso-
late the effect of the training data size, we explored
how the low cross-corpus performance of 74.6%
of for the WIKI-TRAIN model on the OSE2-TEST
7The identical overall performance of both models on the

OSE3-TEST set differs in terms of individual instances.

we saw in Table 2 is improved simply by in-
creasing the amount of training data. We there-
fore trained on increasingly larger portions of the
Wiki-SimpleWiki data set up to the full set of 80k
pairs. We tested on OSE2-TEST and additionally
on OSE3-TEST to lower the risk of idiosyncrasies
speciﬁc to a single test set. Figure 1 shows the ac-

model trained with all features. We train on the
WIKI-OSE2-TRAIN dataset, which as we saw in
Table 3 generalized well across different test sets.

Figure 1: Accuracies for increasing training size

curacies for models trained on increasing amount
of Wiki-SimpleWiki training data.

The curve is essentially ﬂat, with the model on
the largest training set (80k) reaching an accu-
racy of 76.3%, less than two percent above the re-
sult we obtained using only 2k pairs for training,
and signiﬁcantly below the 80.7% obtained for the
model trained on the 4k WIKI-OSE2-TRAIN set
(cf. Table 3). The Wiki-SimpleWiki data thus
does not seem to offer the variety of simpliﬁcation
needed to generalize better to the OSE test sets.

4.5 Feature Selection
Turning from experiments establishing the overall
validity of the approach to the impact of the differ-
ent features, the next experiments explore feature
selection. In addition to characterizing how much
can be achieved with how little, feature selection
gives us an opportunity to better understand the
linguistic characteristics of simpliﬁcation. We ex-
plored which features contribute the most as single
features or as feature groups.

Impact of feature groups: First, we investi-
gated the contribution of different feature groups
to ranking accuracy. Figure 2 presents the perfor-
mance of ranking models trained using the four
feature groups (LEX, SYN, CEL, PSY) and a

Figure 2: Performance of different feature groups

Figure 2 shows that the performance of the indi-
vidual feature groups varies with the test-sets used.
For example, CEL features result in lower accu-
racy for OSE3-TEST compared to other feature
groups, whereas PSY features performed poorly
for WIKI. For all test sets, the model trained with
all the features outperforms the individual feature
group models. For a generalizing approach to the
evaluation of text simpliﬁcation, modeling multi-
ple dimensions of readability thus is more useful
than choosing a single aspect.
Impact of individual features: To understand
the linguistic characteristics of simpliﬁcation, it
is useful to identify which individual features are
more informative for these authentic data sets.
Hence, we trained single feature ranking models
with each of the training sets and ranked the fea-
tures based on their performance on the test sets.
The list of single features achieving over 60% for
in-corpus test-set evaluation are shown in Table 4
for the WIKI data and Table 5 for the OSE2 data.8
For the WIKI data only six features individu-
ally performed with an accuracy above 60%: four
SYN, one LEX, one PSY. For the OSE2 data,
this was the case for eight features: two SYN, one
LEX, ﬁve PSY. Both lists thus include a combina-
tion of word-level and syntactic features, with syn-

8We experimented with a range of AoA norms and lexi-
cal diversity measures, but for space reasons include only the
best AoA and lexical diversity feature here. Interestingly, the
accuracies obtained for the various AoA norms differed sub-
stantially, between 37% and 72.8%, also due to coverage.

 0 10 20 30 40 50 60 70 80 0 10 20 30 40 50 60 70 80Accuracy of ranker (percentage)Training Data size in 1000s of pairs (Wiki-Simple Wiki corpus)Variation of Ranking Accuracy with Training Data sizeOSE2-TestOSE3-Test 0 10 20 30 40 50 60 70 80 90OSE2OSE3Wikiaccuracytest dataLexSynCelPsyAllFeature
num. subtrees
(SUBTREES)
corrected type-token ratio
(CTTR)
sentence length
(SENLEN)
avg. Age of Acquisition
Kup.-Lem. (AOA)
num. constituents per tree
(CONST)
avg. length of t-unit
(MLT)

Group Accur.
SYN
72.1%

LEX

70.4%

SYN

69.7%

PSY

64.8%

SYN

63.3%

SYN

63.2%

Table 4: Performance of single feature ranking
models for WIKI-TRAIN/WIKI-TEST

Feature
AOA
CTTR
SUBTREES
avg. length of clause
(MLC)
avg. word imagery rating
(IMAGERY)
avg. word familiarity
rating (FAMILIARITY)
avg. Colorado
meaningfulness rating of
words
(MEANINGFULNESS)
avg. concreteness rating
(CONCRETENESS)

Group Accur.
72.8%
PSY
66.7%
LEX
SYN
64.4%
63.2%
SYN

PSY

63.2%

PSY

63.2%

PSY

63.2%

PSY

61.7%

Table 5: Performance of single feature ranking
models for OSE2-TRAIN/OSE2-TEST

tactic simpliﬁcation playing more of a role for the
WIKI dataset and lexical choices relating to psy-
cholinguistic characteristics being more relevant
for the OSE2 data.

Sentence length is more predictive for WIKI
than for OSE2, probably because the WIKI
dataset contains a lot of deletions (∼45% of the
sentence pairs show major deletions) compared to
the OSE dataset, where sentences were mostly
rewritten or paraphrased instead of deleting con-
tent. In line with this observation, sentence length
as a single feature for OSE2-TEST data achieves
an accuracy of only 57.5%, compared to the 69.7%
for WIKI-TEST.

The role and interdependence of the different

psycholinguistically motivated features (age of ac-
quisition, concreteness, meaningfulness, imagery)
for the OSE2 data is interesting and would merit
further study. A good understanding of the role of
these features would be directly relevant for im-
proving lexical simpliﬁcation approaches such as
that of Jauhar and Specia (2012), which already
integrates some features from the MRC psycholin-
guistic database to rank word substitutes for lexi-
cal simpliﬁcation.

4.6 Simpliﬁcation at different levels
We next explored, whether the nature of the
simpliﬁcation differs between advanced sentences
being simpliﬁed compared to intermediate sen-
tences being (further) simpliﬁed.
To investi-
gate this, we split the OSE3-TRAIN and OSE3-
TEST datasets into two pairs of datasets ADV-INT-
TRAIN, ADV-INT-TEST and INT-ELE-TRAIN,
INTR-ELE-TEST. Table 6 shows the differences in
the performance of the ranking approach between
the two levels of simpliﬁcation.

Training Data

ADV-INT
73.6%
81.6%

INT-ELE
74.7%
80.5%

ADV-INT-TEST
INT-ELE-TEST

Table 6: Simpliﬁcation at different levels

The performance on the INT-ELE-TEST set is
better than that on the ADV-ELE-TEST set, inde-
pendent of whether the model was trained on the
ADV-INT or INT-ELE training data. To understand
the reason, we explored the nature of the simpli-
ﬁcation involved at these two different levels by
testing the predictive power of individual features.
in-
dividually achieved an accuracy of over 60%
for intermediate to beginner level simpliﬁcation.
For advanced to intermediate, only AoA features
achieved an accuracy of above 60%.

Table 7 shows the list of features that

The better overall performance at the inter-
mediate to elementary simpliﬁcation level and
the higher number of informative features at that
level indicate that the nature of the simpliﬁcation
between advanced and intermediate sentences is
more subtle – and possibly the already broad fea-
ture set warrants further extension to capture addi-
tional characteristics of more advanced levels.

For example, many of the syntactic features
mentioned in the feature selection discussion

Group Accur.
Feature
PSY
AOA
PSY
IMAGERY
LEX
CTTR
PSY
MEANINGFULNESS
PSY
CONCRETENESS
PSY
FAMILIARITY
SYN
MLC
SYN
SUBTREES
avg. senses per word LEX

77%
67.8%
67.8%
66.7%
65.5%
64.4%
64.4%
64.4%
64.4%

Example 2

Int: DNA taken from the wisdom tooth of a Eu-
ropean hunter-gatherer has given scientists a
glimpse of modern humans before the rise of
farming.

Ele: Scientists have taken DNA from the tooth of
a European hunter-gatherer and have found
out what modern humans looked like before
they started farming.

Table 7: Accuracy of single feature ranking mod-
els for INT-ELE simpliﬁcation

Example 3

(SUBTREES, MLC, MLT) are correlated with text
length. However, simpliﬁcation can also involve
sentence rewrites that do not affect the sentence
length as such (e.g., paraphrasing, active/passive,
reordering), which may warrant inclusion of fea-
tures targeting more speciﬁc syntactic construc-
tions or idiomatic word usage characteristic of ad-
vanced levels of English.

4.7 Error Analysis
Finally, to understand if there is a systematic pat-
tern in the errors made by the ranker, we manually
performed a qualitative analysis of errors. For this,
we took the results of training with OSE3-TRAIN
data and testing with the OSE3-TEST. This is
the smallest test set (87 triplets), which given the
79.7% accuracy allows us to analyze 53 misclas-
siﬁed pairs. The following are four example sen-
tence pairs/triplets from the test set. While the ﬁrst
two were ranked correctly by the ranker, the last
two illustrate cases where the ranker failed.

Example 1

Adv: He warned that it was too early to use oxy-
tocin as a treatment for the social difﬁculties
caused by autism and cautioned against buy-
ing oxytocin from suppliers online.

Int: He warned that it was too early to use oxy-
tocin as a treatment for the social difﬁculties
caused by autism and said people should not
buy oxytocin online.

Ele: He said that it was too early to use oxy-
tocin as a treatment for the social difﬁculties
caused by autism and said people should not
buy oxytocin online.

Adv: Its inventor, Bob Propst, said in 1997, “the
cubiclizing of people in modern corporations
is monolithic insanity.”

Int: Its inventor, Bob Propst, said, in 1997, “the
use of cubicles in modern corporations is
crazy.”

Ele: The inventor, Bob Propst, said,

in 1997,
“the use of cubicles in modern companies is
crazy.”

Example 4

Adv: A special “auditor” declares him 96.9%
“made in France” and Montebourg visits to
present him with a medal.

Int: A special “auditor” declares him 96.9%
“made in France” and Montebourg visited to
present him with a medal.

In Example 1,

the transformation from Adv
to Int is primarily paraphrasing (“and cautioned
against buying oxytocin” vs. “and said people
should not buy oxytocin”) where was the transfor-
mation from Int to Ele is that of a simple lexical
substitution (“He warned” vs. “He said”). How-
ever, in Example 2, there was more re-ordering
and paraphrasing (“before the rise of farming”
vs. “before they started farming”). In both these
cases, our model correctly identiﬁed the changes
as a simpliﬁcation. The model thus effectively
identiﬁes paraphrases and lexical substitutions at
multiple levels.

However, the model is not as effective for the
sentence triplet in Example 3. It provides the cor-
rect pairwise rankings Int > Ele and Adv > Ele,
but then incorrectly determines Int > Adv. So the
model correctly identiﬁed a simple lexical substi-
tution between Int and Ele, but failed to identify

the transformation from “the cubiclizing of peo-
ple” to “the use of cubicles” and from “mono-
lithic insanity” to “crazy” as a simpliﬁcation. This
could possibly be because the parse structure as
such did not alter much despite the rephrasing
and neither “cubiclizing” nor the noun or lemma-
tized verb “cubiclize” exist in the psycholinguis-
tic databases we used. Including broad coverage
frequency measures of word usage could help ad-
dress examples of this type. If the example at hand
is typical, for the purpose of simpliﬁcation evalua-
tion at issue here word form frequencies would be
preferable over lemma frequencies.

Finally, in Example 4 the only change between
the two versions is a tense difference (“visits”
vs. “visited”), which the model fails to rank cor-
rectly. It is debatable whether this change in tense
indeed represents a simpliﬁcation so that the case
does not provide useful information on how to im-
prove the approach.

Apart from the relevance of integrating broad-
coverage frequency measures characterizing word
form usage, our qualitative error analysis did not
identify systematic failures of our models. The
broad coverage of linguistic features integrated in
a ranking approach successfully capture the rela-
tive differences in readability which characterize
authentic simpliﬁcation data at the sentence level.

5 Related Work

Evaluation of a text simpliﬁcation approach is typ-
ically done in two ways. Most approaches are
evaluated by comparing sentences using a com-
bination of traditional readability measures, hu-
man ﬂuency and grammaticality judgments of the
generated output, and machine translation met-
rics (e.g., Barlacchi and Tonelli, 2013; ˇStajner et
al., 2014; Siddharthan and Mandya, 2014). Some
approaches evaluate the effect of text simpliﬁca-
tion on their target audience in terms of human
recall and comprehension (e.g., Canning et al.,
2000; Williams and Reiter, 2008; Bradley, 2012).
Other recent work reported the usage of linguis-
tic complexity measures that go beyond traditional
readability formulae for evaluation of text simpli-
ﬁcation at a document level (ˇStajner and Saggion,
2013).

Comparing simpliﬁed versions of individual
sentences with unsimpliﬁed versions in terms of
text complexity is a rather recent endeavor. For
example, sentence-level text complexity was ex-

plored in Intelligent Computer Assisted Language
Learning to identify suitable sentences for creating
learning exercises for German and Swedish learn-
ers (Segler, 2007; Pil´an et al., 2014). Dell’Orletta
et al. (2014) explored the linguistic nature of
features contributing to sentential readability in
the context of developing Italian text simpliﬁca-
tion system for adults with intellectual disabilities.
However, the corpus used does not provide paral-
lel texts with easy and difﬁcult versions.
In the
absence of a sentence-aligned simpliﬁcation cor-
pus, the authors treat each sentence in the easy-to-
read texts as easy. As Vajjala and Meurers (2014b,
Fig. 1) showcases, this is a very problematic as-
sumption. Even for a sentence-aligned simpliﬁca-
tion corpus such as the Wiki-SimpleWiki data set
the only thing guaranteed is that there is one sen-
tence which is harder than the simple one.

Napoles and Dredze (2010) considered a binary
classiﬁcation of Wiki-SimpleWiki at both text and
sentence level, using a range of lexical and syn-
tactic features. They also work with the simpli-
fying assumption that all sentences in Wikipedia
are difﬁcult and those in SimpleWiki are simple.
An interesting aspect of the results of Napoles
and Dredze (2010) and also of Dell’Orletta et al.
(2014) is that they achieve the highest classiﬁca-
tion accuracy at text and sentence level when com-
bining all features.

Vajjala and Meurers (2014a) compared sen-
tences in terms of their reading levels using their
readability model and showed that sophisticated
linguistically motivated readability models can ef-
fectively identify the differences between sen-
tences. In the current paper, we extend their re-
search by exploring sentential simpliﬁcation eval-
uation as a ranking problem and showed that rank-
ing achieves superior performance to regression
for this task.

Ranking has been used for readability assess-
ment at the document level (Ma et al., 2012) and
for related tasks such as ordering MT system out-
put sentences in terms of their language quality
(Avramidis, 2013), and for ranking sentences in
opinion summarization (Kim et al., 2013). To the
best of our knowledge, simpliﬁcation evaluation
was not explored as a ranking problem before.

6 Conclusions

We presented an approach to evaluate automatic
text simpliﬁcation systems in terms of the read-

ing level of individual sentences. The approach
involves the use of a pairwise ranking approach to
compare unsimpliﬁed and simpliﬁed versions of
sentences in terms of the reading level. It identi-
ﬁes the order in terms of their reading level cor-
rectly with an accuracy of over 80%, the best
accuracy for this task we are aware of. We
performed in-corpus and cross-corpus evaluations
with two very different sentence-aligned corpora
and showed that the approach generalizes well
across corpora. The approach performs at a level
that should make it useful in practice to auto-
matically evaluate text simpliﬁcation for language
learners in real-life educational settings.

In terms of the linguistic nature of simpliﬁca-
tion, we studied the role of individual features and
groups of features in predicting the ranking or-
der between simpliﬁed and unsimpliﬁed versions
of the sentences. We found that for the OSE
data set, psycholinguistic features such as Age-of-
Acquisition are the most predictive individual fea-
tures. For the Wiki-SimpleWiki data set, syntactic
features also ﬁgure prominently. However, an ap-
proach using the full range of features systemati-
cally results in the best performance and general-
izes best in the cross-corpus settings.

Pursuing the question whether there is a singu-
lar notion of simpliﬁcation, we studied the differ-
ences in text simpliﬁcation occurring at different
levels of readability. Our features identify simpli-
ﬁcation between intermediate and elementary lev-
els better compared than between advanced and
intermediate level. It is possible that this is due to
a higher degree of simpliﬁcation between the for-
mer, but we also plan to study whether other types
of features could be added to identify character-
istics at higher levels of readability, such as fea-
tures targeting speciﬁc constructions or idiomatic
usage. The small qualitative error analysis we
performed revealed that a broad coverage method
capturing the frequency of word usage may further
improve results.

To carry out the research, we created a new cor-
pus of sentence-aligned simpliﬁed texts based on
OneStopEnglish texts rewritten by experts for lan-
guage learners into three reading levels. The new
corpus resource can empirically enrich future re-
search on sentence-level simpliﬁcation, helping to
ensure that the results obtained are more gener-
ally valid than for the single Wiki-SimpleWiki re-
source that was available so far.

Outlook Adding frequency features capturing
word usage and exploring feature selection in
more detail by selecting the best features for the
ranker while removing the correlated ones (Geng
et al., 2007) are the immediate directions we
would like to pursue.

It would also be interesting to apply the ap-
proach to evaluate the output of automatic text
simpliﬁcation systems and compare their perfor-
mance in terms of readability. Going beyond com-
plexity, in the long term it could be interesting to
extend the approach to a full framework for evalu-
ating automatic text simpliﬁcation systems by in-
tegrating aspects of ﬂuency and grammaticality.

References
Eleftherios Avramidis. 2013. Sentence-level ranking
with quality estimation. Machine Translation, 27(3-
4):239–256.

R. H. Baayen, R. Piepenbrock, and L. Gulikers.
1995. The CELEX lexical databases. CDROM,
http://www.ldc.upenn.edu/Catalog/
readme_files/celex.readme.html.

Gianni Barlacchi and Sara Tonelli. 2013. Ernesta: A
sentence simpliﬁcation tool for children’s stories in
italian. In 14th International Conference on Compu-
tational Linguistics and Intelligent Text Processing,
(CICLing), pages 476–487. Springer.

Jeremy Bradley.

2012. Computergesteuerte Hilfe
f¨ur deutschsprachige Aphasiker und Aphasikerin-
nen. Ph.D. thesis, Technische Universit¨at Wien,
Wien.

C.J.C. Burges, T. Shaked, E. Renshaw, A. Lazier,
M. Deeds, N. Hamilton, and G. Hullender. 2005.
In Pro-
Learning to rank using gradient descent.
ceedings of the International Conference on Ma-
chine Learning, pages 89–96.

Arnaldo Candido,

Jr., Erick Maziero, Caroline
Gasperin, Thiago A. S. Pardo, Lucia Specia, and
Sandra M. Aluisio. 2009. Supporting the adaptation
of texts for poor literacy readers: a text simpliﬁca-
tion editor for brazilian portuguese. In Proceedings
of the Fourth Workshop on Innovative Use of NLP
for Building Educational Applications, EdAppsNLP
’09, pages 34–42, Stroudsburg, PA, USA.

Yvonne Canning, John Tait, Jackie Archibald, and Ros
Crawley.
2000. Cohesive generation of syntac-
In Third Inter-
tically simpliﬁed newspaper text.
national Workshop on Text, Speech and Dialogue,
TSD 2000, Brno, Czech Republic, September 13-16,
2000, pages 145–150. Springer.

CCSSO. 2010. Common core state standards for en-
glish language arts & literacy in history/social stud-
ies, science, and technical subjects. appendix B: Text

exemplars and sample performance tasks. Technical
report, National Governors Association Center for
Best Practices, Council of Chief State School Of-
ﬁcers. http://www.corestandards.org/
assets/Appendix_B.pdf.

R. Chandrasekar, Christine Doran, and B. Srinivas.
1996. Motivations and methods for text simpliﬁca-
tion. In Proceedings of the 16th International Con-
ference on Computational Linguistics (COLING),
pages 1041–1044.

Felice Dell’Orletta, Martijn Wieling, Andrea Cimino,
Giulia Venturi, and Simonetta Montemagni. 2014.
Assessing the readability of sentences: Which cor-
In Proceedings of the Ninth
pora and features?
Workshop on Innovative Use of NLP for Building
Educational Applications (BEA9), pages 163–173,
Baltimore, Maryland, USA. ACL.

William H. DuBay. 2006. The Classic Readability
Studies. Impact Information, Costa Mesa, Califor-
nia.

Y. Freund, R. Iyer, R. Schapire, , and Y. Singer. 2003.
An efﬁcient boosting algorithm for combining pref-
The Journal of Machine Learning Re-
erences.
search, 4:933–969.

Xiubo Geng, Tie-Yan Liu, Tao Qin, and Hang Li. 2007.
Feature selection for ranking. In Proceedings of SI-
GIR Conference, pages 548–552.

Ralf Herbrich, Thore Graepel, and Klaus Obermayer,
2000. Large margin rank boundaries for ordinal re-
gression, pages 115–132. MIT Press, Cambridge,
MA.

Sujay Kumar Jauhar and Lucia Specia. 2012. Uow-
shef: Simplex – lexical simplicity ranking based on
contextual and psycholinguistic features. In In pro-
ceedings of the First Joint Conference on Lexical
and Computational Semantics (SEM).

T. Joachims. 2006. Training linear SVMs in linear
time. In Proceedings of the 12th ACM SIGKDD in-
ternational conference on Knowledge discovery and
data mining, pages 217–226. ACM Press.

Siddhartha Jonnalagadda and Graciela Gonzalez.
2010. Biosimplify: an open source sentence sim-
pliﬁcation engine to improve recall in automatic
biomedical information extraction. In AMIA Annual
Symposium Proceedings, pages 351–356.

Hyun Duk Kim, Malu G Castellanos, Meichun Hsu,
ChengXiang Zhai, Umeshwar Dayal, and Riddhi-
man Ghosh. 2013. Ranking explanatory sentences
In Proceedings of the
for opinion summarization.
36th international ACM SIGIR conference on Re-
search and development in information retrieval,
pages 1069–1072.

George R. Klare. 1974. Assessing readability. Read-

ing Research Quarterly, 10(1):62–102.

Victor Kuperman, Hans Stadthagen-Gonzalez, and
Marc Brysbaert. 2012. Age-of-acquisition ratings
for 30,000 english words. Behavior Research Meth-
ods, 44(4):978–990.

Hang Li. 2014. Learning to Rank for Information Re-
trieval and Natural Language Processing. Morgan
and Claypool Publishers.

Yi Ma, Eric Fosler-Lussier, and Robert Lofthus. 2012.
Ranking-based readability assessment for early pri-
In Proceedings of the
mary children’s literature.
2012 Conference of the North American Chapter
of the Association for Computational Linguistics:
Human Language Technologies, NAACL HLT ’12,
pages 548–552, Stroudsburg, PA, USA. Association
for Computational Linguistics.

Julie Medero and Marie Ostendorf. 2011. Identifying
targets for syntactic simpliﬁcation. In ISCA Interna-
tional Workshop on Speech and Language Technol-
ogy in Education (SLaTE 2011).

George Miller. 1995. Wordnet: a lexical database for
english. Communications of the ACM, 38(11):39–
41, November.

Courtney Napoles and Mark Dredze. 2010. Learn-
ing simple wikipedia: a cogitation in ascertaining
abecedarian language. In Proceedings of the NAACL
HLT 2010 Workshop on Computational Linguistics
and Writing: Writing Processes and Authoring Aids,
CL&W ’10, pages 42–50, Stroudsburg, PA, USA.
Association for Computational Linguistics.

Rani Nelken and Stuart M. Shieber. 2006. Towards ro-
bust context-sensitive sentence alignment for mono-
lingual corpora. In In 11th Conference of the Euro-
pean Chapter of the Association of Computational
Linguistics, pages 161–168. Assoc. for Computa-
tional Linguistics.

J. Nelson, C. Perfetti, D. Liben, and M. Liben. 2012.
Measures of text difﬁculty: Testing their predic-
tive value for grade levels and student performance.
Technical report, The Council of Chief State School
Ofﬁcers.

Sarah E. Petersen and Mari Ostendorf. 2007. Text sim-
pliﬁcation for language learners: A corpus analysis.
In Speech and Language Technology for Education
(SLaTE), pages 69–72.

Ildik´o Pil´an, Elena Volodina, and Richard Johans-
son. 2014. Rule-based and machine learning ap-
proaches for second language sentence-level read-
In Proceedings of the Ninth Workshop on
ability.
Innovative Use of NLP for Building Educational
Applications (BEA9), pages 174–184, Baltimore,
Maryland, USA. ACL.

Thomas M. Segler. 2007. Investigating the Selection
of Example Sentences for Unknown Target Words in
ICALL Reading Texts for L2 German. Ph.D. the-
sis, Institute for Communicating and Collaborative
Systems, School of Informatics, University of Edin-
burgh.

Sanja ˇStajner, Ruslan Mitkov, and Horacio Saggion.
2014. One step closer to automatic evaluation of text
In Proceedings of the 3rd
simpliﬁcation systems.
Workshop on Predicting and Improving Text Read-
ability for Target Reader Populations (PITR), pages
1–10, Gothenburg, Sweden. ACL.

Advaith Siddharthan and Angrosh Mandya. 2014. Hy-
brid text simpliﬁcation using synchronous depen-
dency grammars with hand-written and automati-
In Proceedings of the 14th
cally harvested rules.
Conference of the European Chapter of the Associa-
tion for Computational Linguistics, pages 722–731,
Gothenburg, Sweden, April. ACL.

Advaith Siddharthan. 2014. A survey of research on
text simpliﬁcation. International Journal of Applied
Linguistics: Special issue on Recent Advances in
Automatic Readability Assessment and Text Simpli-
ﬁcation, 165:2:259–298.

Gokhan Tur, Dilek Hakkani-T¨ur, Larry Heck, and
2011. Sentence simpliﬁcation
S. Parthasarathy.
for spoken language understanding. In Proceedings
of the IEEE International Conference on Acoustics,
Speech, and Signal Processing, (ICASSP), pages
5628–5631.

Sowmya Vajjala and Detmar Meurers. 2014a. Assess-
ing the relative reading level of sentence pairs for
text simpliﬁcation. In Proceedings of the 14th Con-
ference of the European Chapter of the Association
for Computational Linguistics (EACL), pages 288–
297, Gothenburg, Sweden, April. ACL, Association
for Computational Linguistics.

Sowmya Vajjala and Detmar Meurers. 2014b. Read-
ability assessment for text simpliﬁcation: From an-
alyzing documents to identifying sentential simpli-
ﬁcations. International Journal of Applied Linguis-
tics, Special Issue on Current Research in Readabil-
ity and Text Simpliﬁcation.

Sandra Williams and Ehud Reiter. 2008. Generating
basic skills reports for low-skilled readers*. Nat.
Lang. Eng., 14:495–525, October.

M.D. Wilson.

1988.

The MRC psycholinguistic
database: Machine readable dictionary, version 2.
Behavioural Research Methods, Instruments and
Computers, 20(1):6–11.

Kristian Woodsend and Mirella Lapata. 2011. Learn-
ing to simplify sentences with quasi-synchronous
grammar and integer programming. In Proceedings
of the Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP), pages 409–420.
Assoc. for Computational Linguistics.

Zhemin Zhu, Delphine Bernhard, and Iryna Gurevych.
2010. A monolingual tree-based translation model
for sentence simpliﬁcation. In Proceedings of The
23rd International Conference on Computational
Linguistics (COLING), August 2010. Beijing, China,
pages 1353–1361.

Sanja ˇStajner and Horacio Saggion. 2013. Readabil-
ity indices for automatic evaluation of text simpliﬁ-
cation systems: A feasibility study for spanish. In
Proceedings of the Sixth International Joint Confer-
ence on Natural Language Processing, pages 374–
382, Nagoya, Japan, October. Asian Federation of
Natural Language Processing.

