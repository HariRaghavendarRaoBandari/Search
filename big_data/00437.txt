6
1
0
2

 
r
a

M
1

 

 
 
]

V
C
.
s
c
[
 
 

1
v
7
3
4
0
0

.

3
0
6
1
:
v
i
X
r
a

Technical Report:

1

Band selection for nonlinear unmixing of

hyperspectral images as a maximal click problem

Tales Imbiriba(1), Student Member, IEEE, Jos´e Carlos Moreira Bermudez(1), Senior Member, IEEE,

C´edric Richard(2), Senior Member, IEEE

Abstract

Kernel-based nonlinear mixing models have been applied to unmix spectral information of hyperspectral images when the type
of mixing occurring in the scene is too complex or unknown. Such methods, however, usually require the inversion of matrices
of sizes equal to the number of spectral bands. Reducing the computational load of these methods remains a challenge in large
scale applications. This paper proposes a centralized method for band selection (BS) in the reproducing kernel Hilbert space
(RKHS). It is based upon the coherence criterion, which sets the largest value allowed for correlations between the basis kernel
functions characterizing the unmixing model. We show that the proposed BS approach is equivalent to solving a maximum clique
problem (MCP), that is, searching for the biggest complete subgraph in a graph. Furthermore, we devise a strategy for selecting
the coherence threshold and the Gaussian kernel bandwidth using coherence bounds for linearly independent bases. Simulation
results illustrate the efﬁciency of the proposed method.

Hyperspectral data, nonlinear unmixing, band selection, kernel methods, maximum clique problem.

Index Terms

I. INTRODUCTION

The unmixing of spectral information acquired by hyperspectral sensors is at the core of many remote sensing applications
such as land use analysis, mineral detection, environment monitoring and ﬁeld surveillance [1], [2]. Such information is typically
mixed at the pixel level due to the low resolution of hyperspectral devices or because distinct materials are combined into a
homogeneous mixture [3]. The observed reﬂectances then result from mixtures of several pure material signatures present in
the scene, called endmembers. Considering that the endmembers have been identiﬁed, hyperspectral unmixing (HU) refers to
estimating the proportional contribution of each endmember to each pixel in a scene.

The linear mixture model is widely used to identify and quantify pure components in remotely sensed images due to its
simple physical interpretation. Though the linear model leads to simple unmixing algorithms and facilitates implementation,
there are many situations to which it is not applicable. These include scenes where there is complex radiation scattering

T. Imbiriba and J.-C. M. Bermudez are with the Department of Electrical Engineering, Federal University of Santa Catarina at Florian´opolis, SC, 88040-
900, Brazil. C. Richard is with the University of Nice Sophia-Antipolis, Nice 06108, France (e-mail: cedric.richard@unice.fr), Lagrange Laboratory (CNRS,
OCA), in collaboration with Morpheme team (INRIA Sophia-Antipolis). The work of J.-C. M. Bermudez was partly supported by Conselho Nacional de
Desenvolvimento Cientﬁco e Tecnolgico (CNPq) grants 305377/2009-4, 400566/2013-3 and 141094/2012-5. The work of C. Richard was partly supported
by ANR grants ANR-12- BS03-003 (Hypanema), by the CNRS Imagin project under grant 2015OPTIMISME, by the BNPSI ANR Project no ANR-13-
BS-03-0006-01.

2

among several endmembers, as may happen in some vegetation areas [4]. In such situations, nonlinear mixing models must be
considered [5], [6]. Several nonlinear mixing models have been proposed in the literature. A review of the existing models can
be found in [5]. The complexity of the mixture mechanisms that may be present in a real scene has led to the consideration
of ﬂexible nonlinear mixing models that can model generic nonlinear functions. Kernel methods provide a non-parametric
representation of functional spaces, and can model nonlinear mixings of arbitrary characteristics [5]–[10].

Kernel-based methods are efﬁcient machine learning techniques [11]–[13] that consist mainly of linear algorithms operating in
high dimensional reproducing kernel Hilbert spaces (RKHS), into which the data have been mapped using kernel functions [11].
Working in such high dimensional feature spaces is possible due to the so-called kernel trick, which allows the computation
of inner products in the feature space through a kernel function in the input space [14]. A limitation of kernel methods for HU
is that they usually require the inversion of matrices whose dimensions equal the number of spectral bands. Thus, reducing
their computational cost remains a challenge for their use in large-scale applications.

A possible way of reducing this cost is to perform band selection (BS) prior to unmixing [15]. Though BS has been actively
employed in classiﬁcation of spectral patterns [16]–[20], subspace projection techniques [21]–[23] tend to be preferred over
BS [24], [25] for reducing the complexity of linear unmixing processes. This is mainly because high-dimensional data are
conﬁned to a low-dimensional simplex in linearly-mixed images with only a few endmembers [3]. However, the simplex property
is not preserved in the presence of nonlinearly-mixed pixels [6], rendering projection techniques less attractive. Nevertheless,
BS is also a challenging problem for nonlinear unmixing since the selection procedure should ideally match the characteristics
of the unmixing model. Thus, BS methods developed for linear mixed pixels cannot be directly applied to the nonlinear case.
In a previous work [26], we proposed a BS method based on the kernel k-means algorithm to identify clusters of spectral
bands in the corresponding RKHS. The cluster prototypes are then the selected bands. This method reduces signiﬁcantly
the computation time required for nonlinear unmixing without compromising the accuracy of abundance estimation. In this
approach, however, each band is selected based on its distance to the others in the RKHS, and not as a function of the resulting
accuracy of the unmixing procedure. In addition, it requires to set the ﬁnal number of bands a priori. Hence, some cluster
prototypes can be close to others and degrade problem conditioning if this parameter is overestimated.

In [27], the authors proposed a low-complexity coherence-based greedy approach for controlling the size of kernel models for
online system identiﬁcation. As the coherence criterion makes the needed bridge between the number of basis kernel functions
in the unmixing model and an upper bound on the reconstruction error, such approach could may also be applied to BS in
RKHS. However, its greedy nature which is appropriate for online settings would lose efﬁciency otherwise.

In this paper we introduce a new coherence-based method for BS in the RKHS. The coherence criterion is used to set
the largest correlation between the basis kernel functions included in the unmixing model. We show that this BS approach is
equivalent to search for a maximum clique in a graph, that is, the largest complete subgraph in this graph. Starting from a
tentative dictionary cardinality, the proposed method determines both the dictionary size and its elements in order to satisfy the
required coherence criterion. Using the maxCQL algorithm [28] to solve the maximum clique problem, the new method results
in dictionaries of kernel functions, and thus spectral bands, that are less coherent than those obtained using kernel k-means
initialized with dictionaries of the same size.

This paper is organized as follows. First, we review nonlinear unmixing models usually considered for HU. Then, we
introduce kernel-based models and the associated estimation framework. Next, we consider the problem of BS in RKHS. We
introduce kernel k-means strategy and our new algorithm based on maximum clique search. We provide promising simulation
results to illustrate the performance of the proposed method using both synthetic and real images. Finally, we present some

3

concluding remarks.

II. HYPERSPECTRAL IMAGES AND UNMIXING

Observed pixels in HIs are usually modeled as a function, possibly nonlinear, of the endmembers and an additive noise that

accounts for the measurement noise plus a modeling error, namely,

r = ψ(M ) + n

(1)
where r = [r1, . . . , rL](cid:62) is a vector of observed reﬂectances in L spectral bands, M = [m1, . . . , mR] is the L × R matrix of
R endmembers, whose i-th column mi corresponds to an endmember, n is a white Gaussian noise (WGN) vector, and function
ψ represents an unknown mixing mechanism. Several models of the form (1) were proposed in the literature, depending on
the linearity or nonlinearity of ψ, the nature of mixture, and other properties [6].

A. The linear mixing model

The linear mixing model (LMM) considers only interactions of light rays with a single material, neglecting interactions

between light and several materials [3]. The LMM assumes that r is a convex combination of the endmembers, namely,

r = M α + n
subject to 1(cid:62)α = 1 and α (cid:23) 0

(2)

where α = [α1, . . . , αR](cid:62) denotes the vector of abundances of each endmember in M, and (cid:23) is the entrywise ≥ operator.
Being proportions, the entries of α cannot be negative and should sum to one. The observation r(cid:96) in the (cid:96)-th wavelength of (2)
can be written as

r(cid:96) = m(cid:62)

λ(cid:96)

α + n(cid:96)

(3)

where mλ(cid:96) denotes the (cid:96)-th row of M written as a column vector. In the noiseless case (n(cid:96) = 0), the sum-to-one and positivity
constraints over α in (2) restrict the data to a simplex whose vertices are the endmembers.

B. Nonlinear mixing models

Several nonlinear models have been proposed to describe complex mixing mechanisms. See [6] and references therein. We

now review two popular models that will be used later.

The generalized bilinear model (GBM) [29] is deﬁned as:

R−1(cid:88)

R(cid:88)

i=1

j=i+1

subject to 1(cid:62)α = 1 and α (cid:23) 0

r = M α +

δij αiαj mi (cid:12) mj + n

(4)

where each parameter δij ∈ [0, 1] characterizes the interaction of endmembers mi and mj, and (cid:12) denotes the Hadamard
product. For simplicity, we shall consider a simpliﬁed version of this model where all the bilinear terms in (4) are weighted
by a single parameter δ = δij for all (i, j).

The post nonlinear mixing model (PNMM) [30] is deﬁned as follows:

r = g(M α) + n

(5)

4

where g is a nonlinear function applied to the noiseless LMM. Thanks to function g, the PNMM speciﬁes a large family of
nonlinear mixing models via a single expression. For instance, the PNMM considered in [10] is given by

r = (M α)ξ + n

(6)

where (v)ξ denotes the exponentiation applied to each entry of v. For ξ = 2, (6) is a bilinear model closely related to the
GBM but without a linear term. The PNMM has been explored with different forms for g [31], [32].

The GBM and the PNMM models essentially describe situations where the light interacts ﬁrst with an endmember, and then
with a second one, before being captured by the hyperspectral sensor. Other nonlinear models can be considered depending
on the characteristics of the scene [29], [30], [33]–[39]. More importantly, information about these characteristics is usually
missing, and it makes sense to consider nonparametric models that do not rely on strong assumptions.

III. LS-SVR FOR HYPERSPECTRAL UNMIXING

Kernel-based methods consist of mapping observations from the original input space into a feature space by means of a
nonlinear function. Nonlinear regression problems can be addressed in an efﬁcient way in this new space as they are converted
to a linear problem. We shall now review the main deﬁnitions related to RKHS [12], [40]–[42].

A. Mercer kernels and RKHS

The theory of positive deﬁnite kernels emerged from the study of positive deﬁnite integral operators [43], and was further

generalized for the study of positive deﬁnite matrices [44]. It was established that, to every positive deﬁnite function

(7)
deﬁned over a non-empty compact M ⊂ Rd, there corresponds one and only one family of real-valued functions on M that
deﬁnes a Hilbert space H endowed with an unique inner product (cid:104)·,·(cid:105)H and the associated norm (cid:107)·(cid:107)H, and admitting κ as a
reproducing kernel [14]. This means that κ(·, m) ∈ H for all m ∈ M, and has the reproducing property deﬁned as:

κ : M × M → R

ψ(m) = (cid:104)ψ, κ(·, m)(cid:105)H

(8)

for all ψ ∈ H and m ∈ M. Replacing ψ by κ(·, m(cid:48)) in (8) leads to:

κ(m, m(cid:48)) = (cid:104)κ(·, m), κ(·, m(cid:48))(cid:105)H

(9)
for all m, m(cid:48) ∈ M. Equation (9) is the origin of the now generic denomination reproducing kernel to refer to κ. Note that H
can be restricted to the span of {κ(·, m) : m ∈ M} because, according to the reproducing property (8), nothing outside this
set affects ψ evaluated at any point of M. Let us denote by ϕ the map from M to H that assigns κ(·, m) to m. Relation (9)
implies that κ(m, m(cid:48)) = (cid:104)ϕ(m), ϕ(m(cid:48))(cid:105)H. This means that the kernel κ evaluates the inner product of any pair of elements
of M mapped into H without any explicit knowledge of ϕ or H. This principle is called the kernel trick.

Several kernel functions have been considered in a variety of applications during the past two decades [45]. Among the

(10)

most frequently used kernels, we highlight the Gaussian kernel:

where σ is the kernel bandwidth.

κ(m, m(cid:48)) = exp

(cid:18)

−(cid:107)m − m(cid:48)(cid:107)2

2σ2

(cid:19)

B. LS-SVR: least squares support vector regression

This section describes the use of a state-of-the-art kernel method for nonlinear unmixing of hyperspectral data. Consider an
observation r(cid:96) at the (cid:96)-th wavelength, that is, the (cid:96)-th entry of r, and the column vector mλ(cid:96) of the R endmember signatures
at the (cid:96)-th wavelength, that is, the (transposed) (cid:96)-th row of M. By analogy with the LMM (3), we write:

5

(11)
with ψ a real-valued function in a RKHS H that characterizes the nonlinear interactions between the endmembers, and n(cid:96)
an additive noise at the (cid:96)-th band. In order to estimate ψ in the least squares sense, we can formulate the following convex
optimization problem, also called LS-SVR [13]:

r(cid:96) = ψ(mλ(cid:96)) + n(cid:96)

Consider the Lagrangian function

such that

L(ψ, e, β) =

(cid:107)ψ(cid:107)2H +

1
2

1
2µ

(cid:96) = 1, . . . , L.

β(cid:96) (e(cid:96) − r(cid:96) + ψ(mλ(cid:96))).

(12)

(13)

where β = [β1, . . . , βL](cid:62) is the vector of Lagrange multipliers. Using the directional derivative with respect to ψ [46], the
conditions for optimality with respect to the primal variables ψ and e(cid:96) are given by

min
ψ∈H

(cid:107)ψ(cid:107)2H +

1
2

1
2µ

L(cid:88)

(cid:96)=1

e2
(cid:96)

e(cid:96) = r(cid:96) − ψ(mλ(cid:96) ),

L(cid:88)

(cid:96)=1

(cid:96) − L(cid:88)

e2

(cid:96)=1

L(cid:88)

ψ∗ =

β(cid:96)κ(., mλ(cid:96))

(14)

(15)

(16)

(17)

(18)

(cid:96)=1

e∗
(cid:96) = µβ(cid:96)

Substituting (14) and (15) in (13), we obtain the following function to be maximized with respect to β:

L(ψ∗, e∗, β) = − 1
2

(cid:62)

β

(K + µI) β + β

(cid:62)

r,

where K is the Gram matrix whose (i, j)-th entry is deﬁned by κ(mλi, mλj ). Now we can state the following dual problem:

Its solution is obtained by solving the linear system:

∗

β

= arg max

(cid:62)

β

(K + µI) β + β

(cid:62)

r.

− 1
2

β

 −I K + µI



 r

β

 = 0.

Although the formulation (12)–(17) allows one to address an estimation problem in H by solving the linear system (18), this
approach is computationally demanding since it involves the inversion of L × L matrices. This issue is critical, as modern
hyperspectral image sensors employ hundreds of contiguous bands with an ever increasing spatial resolution. Hence, it is of
major interest to consider band selection techniques that lead to signiﬁcant computational cost reduction without noticeable
quality loss. Considering (14), a possible strategy is to focus on a reduced-order model of the form:

where ID ⊂ {1, . . . , L} is an M-element (M < L) subset of indexes. We shall call D = {κ(., mλj )}j∈ID the dictionary.

ψ =

βjκ(., mλj )

(19)

(cid:88)

j∈ID

6

IV. BAND SELECTION

BS has been an active topic of research for classiﬁcation of spectral patterns, see [16]–[20] and references therein. Subspace
projection techniques [21]–[23] tend, however, to be preferred over BS [24], [25] for reducing the complexity of linear
unmixing processes. They use the property that high-dimensional hyperspectral data are conﬁned to a low-dimensional simplex
in linearly-mixed images with only a few endmembers [3]. This assumption becomes invalid when nonlinear mixing phenomena
are involved. Recently, in a preliminary work [26], we introduced a BS strategy method that employs the kernel k-means
algorithm to identify clusters of spectral bands in the RKHS where nonlinear unmixing is performed. The HU results obtained
were encouraging. One drawback of the approach in [26] is the need for an arbitrary choice of the order of the nonlinear model
(the dimension of the dictionary). Given the order, band selection is performed based on the distances among different bands in
the RKHS. Hence, the optimality of the solution is not driven by any direct measure of modeling accuracy. In this section, we
brieﬂy review the kernel k-means approach. Then we introduce a new strategy based on the so-called coherence criterion [27]
and maximum clique search in a graph. Although these two approaches are connected, they differ in their formulation and in
the characteristics of the sets of bands they select.

A. Kernel k-means for band selection

Kernel k-means (KKM) is a direct extension of the k-means clustering algorithm [47]. It maps the input data mλ(cid:96) into a
RKHS H, and groups their images κ(·, mλ(cid:96)) into disjoint clusters C1, . . . ,CM based on their relative distance in H. Since
determining centroids in H is intractable, KKM calculates distances using the reproducing property (9).

Given a cluster Ck enclosing points {κ(·, mλ(cid:96))}(cid:96)∈Ck, its centroid is deﬁned as

(cid:88)

i∈Ck

νk =

1
Nk

κ(·, mλi)

where Nk is the number of points in Ck. The squared distance of any point κ(·, mλ(cid:96) ) to νk is computed as

(cid:88)
(cid:107)κ(·, mλ(cid:96) ) − νk(cid:107)2H = κ(mλ(cid:96), mλ(cid:96))
(cid:88)
(cid:88)

− 1
Nk

i∈Ck

+

1
N 2
k

i∈Ck

j∈Ck

κ(mλ(cid:96) , mλi)

κ(mλi , mλj )

and the clustering error to minimize is deﬁned as

E(ν1, . . . , νK) =

M(cid:88)

(cid:88)

k=1

(cid:96)∈Ck

(cid:107)κ(·, mλ(cid:96)) − νk(cid:107)2H.

Each cluster Ck is then represented by the band (cid:96)k corresponding to the closest point to its centroid νk:

(cid:96)k = arg min

(cid:96)∈Ck

(cid:107)κ(·, mλ(cid:96) ) − νk(cid:107)2H.

(20)

(21)

(22)

(23)

The global kernel k-means (GKKM) algorithm uses the principles described above for incremental clustering [47]. GKKM does
not suffer from poor convergence to local minima and produces near-optimal solutions that are robust to cluster initialization.
A fast GKKM (FGKKM) version that performs a unique KKM run and greatly reduces the complexity of the algorithm can
also be used. For more details on KKM for BS, the reader is invited to refer to [26].

B. Coherence criterion for dictionary selection

Coherence is a parameter of fundamental interest for characterizing dictionaries of atoms in linear sparse approximation
problems [48]. It was ﬁrst introduced as an heuristic quantity for Matching Pursuit in [49]. Formal studies followed in [50],
and were enriched for Basis Pursuit in [51], [52].

Consider a set of kernel functions {κ(·, mλ(cid:96))}(cid:96)=1,...,M in H. The deﬁnition of coherence was extended to RKHS as [27]:

7

µ = max
i(cid:54)=j

|(cid:104)κ(·, mλi), κ(·, mλj )(cid:105)H|
|κ(mλi, mλj )|

(24)

where κ is a unit-norm kernel. Otherwise, replace κ(·, mλi) with κ(·, mλi)/(cid:112)κ(mλi, mλi) in (24). Parameter µ is the

= max
i(cid:54)=j

largest absolute value of the off-diagonal entries in the Gram matrix. It reﬂects the largest cross correlation in the dictionary
{κ(·, mλ(cid:96) )}(cid:96), and is equal to zero for every orthonormal basis. A dictionary is said to be incoherent when its coherence µ is
small. Although its deﬁnition is rather simple, coherence possesses important properties [27]. In particular, it can be shown
that the kernel functions in the dictionary D = {κ(·, mλ(cid:96))}(cid:96)=1,...,M are linearly independent if (M − 1)µ < 1. This sufﬁcient
condition illustrates that the coherence (24) provides valuable information on a dictionary at low computionnal cost. Other
properties are discussed in [27].

Kernel-based dictionary learning methods usually consider approximate linear dependence conditions to evaluate whether
a candidate kernel function κ(·, mλi) can be reasonably well represented by a combination of the kernel functions that are
already in the dictionary D. To avoid excessive computational complexity, a greedy dictionary learning method has been
introduced in [27]. It consists of inserting the candidate κ(·, mλi) into the dictionary D provided its coherence is still below
a given threshold µ0, namely,

|κ(mλi, mλj )| ≤ µ0

max
j∈ID

(25)
where µ0 is a parameter [0, 1[ determining both the maximum coherence in D and its cardinality |D|. Using coherence criterion
for BS allows to explicitly limit the correlation of kernel functions in the dictionary. This contrasts with the kernel k-means
strategy, which starts from a number of dictionary elements prescribed by the user without taking the coherence of kernel
functions into consideration.

The coherence criterion (25) was proposed within the context of parameter estimation from streaming data. The design
of the dictionary follows a greedy strategy. The ﬁrst kernel function is selected arbitrarily, and each new candidate kernel
function is tested using (25) to determine if it deserves being included in the dictionary. This procedure is appropriate for
online applications because of its minimal computational cost. However, alternatives should be sought which may lead to more
effective solutions in batch mode applications.

C. Band selection as a maximum clique problem

Consider a set of kernel functions {κ(·, mλ(cid:96))}(cid:96)=1,...,L. Determining a subset D with a prescribed coherence level can be
viewed as a two-step procedure. The ﬁrst step aims at listing all the pairs of functions that satisfy the coherence rule (25).
This can be performed by constructing a L × L binary matrix B with entries deﬁned as:

 1

0

Bij =

if

|κ(mλi , mλj )| ≤ µ0

otherwise.

(26)

8

Fig. 1: The maximum clique problem (MCP)

The second step consists of ﬁnding in B, up to a simultaneous reordering of its rows and columns, the largest submatrix of only
ones. This problem can be recast as determining a maximum clique in an undirected graph G = {V, E}, where each vertex (cid:96) of
V = {1, . . . , L} corresponds to a candidate function κ(·, mλ(cid:96) ), and edges in E ⊆ V × V connecting the vertices are deﬁned
by the adjacency matrix B. Two vertices are said to be adjacent if they are connected by an edge. A complete subgraph of
G is one whose vertices are pairwise adjacent. The maximal clique problem (MCP) consists of ﬁnding the maximal complete
subgraph of G [53]. This problem is NP-Complete [54]. Figure 1 illustrates this problem within the context of BS. This ﬁgure
shows for instance that the coherence of κ(·, mλ1) and κ(·, mλ4) is lower than the preset threshold µ0, and the coherence of
κ(·, mλ1) and κ(·, mλ2 ) is larger than µ0. This graph has one maximum clique deﬁned by the set of vertices ID = {1, 3, 4, 5},
which means that the coherence of the dictionary D = {κ(., mλj )}j∈ID is lower than µ0 and it has maximum cardinality. A
vast literature exists on maximum clique problems (MCP), see [55] and references therein. The next section reviews the main
algorithms for MCP.

D. The maximum clique problem

MCP has a wide range of practical applications arising in a number of domains such as bioinformatics, coding theory,
economics, social network analysis, etc. Given its theoretical importance and practical interests, considerable efforts have
been devoted for deriving exact and heuristic algorithms. Efﬁcient exact methods have been designed mainly based on the
branch-and-bound (B&B) framework. Dynamic bounds on the clique size are used to prune (or discard) branches during
search, and then dramatically reducing the search space [56]. Although algorithms are now much faster and efﬁcient than their
past counterparts [57], the inherent complexity of exact methods can still lead to a prohibitive computation time when large
problems are addressed [55]. To handle problems whose optimal solutions cannot be reached within a reasonable time, various
heuristic and metaheuristic algorithms have been derived with the purpose of providing sub-optimal solutions in an acceptable
time. In this paper, however, we shall focus on exact algorithms since our application concerns small graphs with a number
of vertices equal to the number of bands.

Since the introduction of the Carraghan and Pardalos (CP) exact algorithm [56], many reﬁnements have been proposed to
improve its performance with a focus on two main issues. The ﬁrst one is to tighten the upper bound on the maximum clique
during search for the purpose of more efﬁcient subtree pruning. The second one is to improve the branching rule, and then
select the most promising vertices to expand candidate cliques. In [55], the authors classify the exact MCP algorithms into
four groups, depending on their strategies for pruning and branching. The ﬁrst group solves sub-clique problems for each
vertex with iterative deepening and pruning strategies. Examples are the CP algorithm [56] and its improved version [58]. Both

154329

algorithms are sensitive to the order of vertices, which can result in drastically different execution times for a given graph [58].
A second group is based on vertex coloring techniques [59]. The most prominent algorithms in this group use B&B strategies
based on subgraph coloring. Examples of algorithms are BT and the recent MCQ, MCR, MaxCliqueDyn, BB-MaxClique,
among others [55]. The third group improves the basic CP by tightening candidate sets via the removal of vertices that cannot
be used to extend the current clique to a maximum clique. Along this line, three B&B algorithms, denoted DF, χ and χ+DF
were proposed in [60]. The fourth group consists of the exact methods based on MaxSAT [28], which improve the techniques
based on vertex coloring. The MaxCLQ algorithm proposed in [28] is considered to be very effective and solved the DIMACS
problem (p hat1000–3) for the ﬁrst time [55]. A complex approach (ILS&MaxCLQ) that combines different algorithms such
as the MaxCLQ, MCS and the ILS, was recently proposed [61]. A comparative discussion on exact methods is presented
in [55]. The MaxCLQ and ILS&MaxCLQ were the only methods to solve all the presented problems, with the smallest CPU
times for the former.

V. ALGORITHMS

We shall now introduce kernel BS algorithms based on the coherence criterion. As a baseline for performance comparisons,
we consider ﬁrst a greedy strategy that consists of testing candidate kernel functions sequentially and inserting them into the
dictionary if coherence stays below a threshold value µ0. Next, we propose an exact strategy based on MCP solving.

A. Automatic parameter settings

Before describing the kernel BS methods, we brieﬂy present a procedure for automatic parameter setting. It allows to set

the coherence threshold µ0 and Gaussian kernel bandwidth σ2 given a desired number of elements in the dictionary.

Let Kσ be the L × L Gram matrix whose (i, j)-th entry is deﬁned by κσ(mλi, mλj ), where κσ denotes the Gaussian
kernel (10) parameterized by the bandwidth σ2. Let D be an M-element dictionary with coherence µ and index set ID. Then,
as shown in [27], a sufﬁcient condition for linear independence of the M elements of D is given by (M − 1)µ < 1. We write:

µ <

1

(M − 1)

.

(27)

The objective is to build a dictionary with (approximately) M linearly independent elements. We thus propose to set the
coherence threshold µ0 as:

µ0 =

1

(M − 1)

(28)

and adjust σ2 to obtain a Gram matrix Kσ whose entries are close to µ0 in some sense. Indeed, on the one hand, if all
the off-diagonal entries of Kσ are smaller than µ0, then D contains the L available elements. On the other hand, if all the
off-diagonal entries of Kσ are greater then µ0, then D should be composed of only one element. Therefore, we propose to
adjust σ2 such that E{(Kσij )(i(cid:54)=j)} = µ0, where E{·} is the expected value and can be approximated as

L−1(cid:88)

L(cid:88)

i=1

j=i+1

Kσij .

[K1ij ]1/σ2 − µ0

2

(29)

(30)

Then, we set σ2 as the solution of the following optimization problem:

E{(Kσij )|(i(cid:54)=j)} ≈

 2

2

L2 − L

L−1(cid:88)

L(cid:88)

i=1

j=i+1

σ2 = arg min

σ2

L2 − L

s. t. σ2 ∈ R+.

10

where K1 = Kσ is the Gram matrix for σ = 1. Finally, we determine KD as the largest sub-matrix of Kσ whose all off-
diagonal entries satisfy (25). We emphasize that since Kσij ≤ 1, (29) is a decreasing function of σ−2, and thus (30) has a
unique solution.

B. Algorithms

In this section we present the two band selection algorithms using the greedy and clique approaches that will be used in

Section VI.

The greedy coherence-based approach is presented in Algorithm 1. The inputs to Algorithm 1 are the desired number M
of bands in the ﬁnal dictionary, and the L × L Gaussian kernel Gram matrix with σ = 1 and entries K1ij = κ(mλi , mλj ) =

exp(cid:0)−0.5(cid:107)mλi − mλj(cid:107)2(cid:1). It returns the index of selected bands and the the Gaussian kernel bandwidth σ2. Initialization

occurs in line 1, where the index set ID is initialized with the ﬁrst spectral band index, the number Nb of bands in the dictionary
is set to one, and the coherence threshold µ0 is adjusted according to (28). Next, σ2 is determined by solving problem (30) in
line 2, and the Gram matrix Kσ is computed with the optimum σ2 in line 3. From line 4 to line 13 the algorithm sequentially
tests all the L − 1 remaining bands using condition (25). Breaking the parts down, in line 5 a zero vector c of length Nb is
created, and the off diagonal terms ((cid:96),IDj ) of the Gram matrix Kσ are stored in c. If the maximum absolute value of the
entries of c is less than the coherence threshold (line 9), then the (cid:96)-th band index is added to ID, and Nb is incremented by
one (lines 10 and 11). Finally, the algorithm returns the complete set of selected bands and the kernel bandwidth in line 14.

: The L × L Gram matrix K1 = (Kσ)σ=1, and the desired number M of atoms.

Algorithm 1: Greedy Coherence-based Band Selection (GCBS)
Input
Output: The indices ID of selected atoms, and the Gaussian kernel bandwidth σ2.
1 Initialization: ID = {1}, Nb = 1, µ0 = 1/(M − 1);
2 Find σ2 solving (30);
3 Compute Kσ using σ2 obtained in line 2;
4 for (cid:96) := 2 to L do

5

6

7

8

9

10

11

c := 0Nb×1;
for j := 1 to Nb do
;

cj := Kσ(cid:96),ID j

end
if max(|cj|) ≤ µ0 then

Insert (cid:96) into ID;
Nb := Nb + 1;

end

12
13 end
14 return ID, σ2;

The clique coherence-based band selection method is described in Algorithm 2. Similarly to Algorithm 1, the inputs are
K1 and M. The adjacency matrix B in initialized with zeros (line 1), the vertices vector V with the indices of all available
wavelengths, µ0 following (28), and ID as an empty set. The kernel bandwidth is computed in line 2, and the Gram matrix

is computed for the optimum σ2 in line 3. Through line 4 to 10 every entry of the upper diagonal part of B is set according
to (26). In line 11 the MaxCLQ algorithm is used to ﬁnd the indices of the maximum clique in the graph. These indices are
assigned to the dictionary index set ID, which is returned in line 10 together with the kernel bandwidth.

11

: The L × L Gram matrix K1 = (Kσ)σ=1, and the desired number M of atoms.

Algorithm 2: Clique Coherence-based BS (CCBS)
Input
Output: The indices ID of selected atoms, and the Gaussian kernel bandwidth σ2.
1 Initialization: B := 0L×L, V = {1, . . . , L}, µ0 = 1/(M − 1), ID c = {∅};
2 Find σ2 solving (30);
3 Kσ using σ2 obtained in line 2;
4 for i := 1 to L − 1 do
5

for j := i + 1 to L do
if [Kσij ] ≤ µ0 then

Bij := 1;

end

6

7

8

end

9
10 end
11 ID := MaxCLQ(V, B);
12 return ID, σ2;

Note that M is used in Algorithm 2 and Algorithm 1 as a design parameter, which is required to obtain the coherence

threshold and the Gaussian kernel bandwidth. The number Nb of bands in the ﬁnal dictionary can differ from M.

A. The SK-Hype

VI. APPLICATION

This section reviews the SK-Hype algorithm1 for nonlinear unmixing of HIs [9]. It considers the mixing model consisting

of a linear trend parametrized by the abundance vector α and a nonlinear residual component ψ. This model is given by

r(cid:96) = u α(cid:62)mλ(cid:96) + (1 − u) ψ(mλ(cid:96) ) + n(cid:96)

(31)

where u ∈ [0, 1] controls the amount of linear contribution to the model and ψ(·) is an unknown function in an RKHS H.
SK-Hype solves the optimization problem

(cid:18) 1

min
α,ψ,u

1
2

(cid:107)α(cid:107)2 +

1

1 − u

(cid:107)ψ(cid:107)2H

u

+

1
2µ

subject to α (cid:23) 0, 1(cid:62)α = 1, and

(cid:19)

L(cid:88)

e2
(cid:96)

e(cid:96) = r(cid:96) − u α(cid:62)mλ(cid:96) − (1 − u) ψ(mλ(cid:96)).

1Matlab code available at www.cedric-richard.fr

(cid:96)=1

(32)

12

which is convex under mild continuity conditions [9]. Problem (32) is solved using a two stage alternating iterative procedure
with respect to (α, ψ) and u. For ﬁxed u and Lagrange multipliers β and γ, the dual problem of (32) is given by [9]



 β

γ



max
β,γ

G(u, β, γ) =

 β
(cid:62) Ku + µI uM
(cid:62) β

 r

uM(cid:62)

uI

γ

− 1
2

+

0

γ
subject to γ (cid:23) 0

with Ku = uMM(cid:62) + (1 − u)K. Solving (33) is equivalent to solving the linear system

Denoting β

∗ and γ∗ the solutions of (33), the solution of the primal problem (32) for u ﬁxed is [9]

0

uI

uM(cid:62)

 −I Ku + µI uM


ψ∗ = (1 − u)(cid:80)L

α∗ = M(cid:62)β∗+γ∗

e∗
(cid:96) = µ β∗

(cid:96)

1(cid:62)(M(cid:62)β∗+γ∗)





r

β

γ

 = 0.

(cid:96)=1 β∗

(cid:96) κ(·, mλ(cid:96))

(33)

(34)

(35)

(36)

The alternating optimization is completed by using (35) in [9], deﬁning the resulting cost function J(u), solving

and continue by iteratively solving (34) and (36) to ﬁnd the global solution [9].

min

u

J(u)

subject to 0 < u < 1

B. Simulation with synthetic data

This section presents simulation results using synthetic data to illustrate the performance of the proposed unmixing method
under controlled conditions for which the abundance values are known. We constructed synthetic images using two sets of
endmembers. The ﬁrst set had 8 endmembers extracted from the spectral library of the ENVI software and correspond to the
spectral signatures of minerals present in the Cuprite mining ﬁeld in Nevada. The minerals are alunite, calcite, epidote, kaolinite,
buddingtonite, almandine, jarosite and lepidolite, and their spectra consisted of 420 contiguous bands, covering wavelengths
from 0.3951µm to 2.56µm. The second set was extracted from the Pavia University data acquired by the ROSIS spectrometer.
It has 610 × 340 pixels with 103 bands over the spectral range of 430–680 nm (Figure 2a). The data also has a ground
truth labelling 42776 pixels (out of the 207400) into 9 classes labeled asphalt, meadows, gravel, trees, painted metal sheets,
bare soil, bitumen, self-blocking bricks and shadows (Figure 2b). We extracted the endmembers from this data set using the
vertex component analysis algorithm (VCA [22]), and considering only the labeled pixels. We constructed four 2000-pixel
hyperspectral images (N = 2000), each using 8 endmembers (R = 8) from the Cuprite or Pavia data, and the simpliﬁed GBM
or PNMM mixing models (see Section II) with δ = 1 and ξ = 0.7, respectively. The abundances were obtained by uniformly

13

sampling from the simplex, i.e., obeying the positivity and sum-to-one constraints. WGN was added to all images with power
adjusted to produce a 21dB SNR. We consider the root mean square error (RMSE) in abundance estimation

RMSE =

(cid:107)αn − α∗

n(cid:107)2

(37)

(cid:118)(cid:117)(cid:117)(cid:116) 1

N R

N(cid:88)

n=1

and the CPU time required for both BS (when applicable) and unmixing (averaged over 100 unmixings of the same HIs) to
compare the different BS strategies. All unmixings were performed using a Gaussian kernel and considering either the full
set of bands or smaller sets selected using the BS strategies presented in Section IV. SK-Hype was implemented for the full
set of bands. The kernel bandwidth for SK-Hype was selected among the values σskp ∈ {0.5σ, σ, 2σ, 10σ, 20σ} to obtain
the minimum RMSE, where σ is the solution of (30), for M = 30. The global kernel k-means (GKKM) algorithm [26]
implementation requires the number of bands to be ﬁxed a priori. We considered a selection approach based on the Akaike
Information Criterion and given by [62]

M = arg min

M

[E(ν1, . . . , νM ) + λM ]

(38)

where the parameter λ controls the complexity of the model, and needs to be found empirically. The kernel bandwidth σkkm
also needs to be selected for GKKM. A grid search was performed using a small part (200 pixels) of the synthetic image to
ﬁnd λ and σkkm that would lead to a good RMSE performance. The parameters were chosen among the values λ ∈ {2, 4, 6}
and σkkm ∈ {0.5σ, σ, 2σ, 10σ, 20σ}, again with σ being the solution of (30), for M = 30. The parameter set leading to
the best performance in terms of RMSE for the abundances was then selected. It is important to notice that, in general, the
abundance ground truth is not available from real data. Thus, the RMSE in abundance estimation could not be used in design
as a measure to select model parameters. Hence, the SK-Hype and GKKM designs used in this comparison are based on a
quasi-optimal choice of parameters for these methods, which could not be determined in practice. The proposed design for the
BS methods, however, can be employed in practical applications.

BS with the CCBS and GCBS algorithms was performed using M ∈ {5, 10, 20, 30}, with parameters µ0 and σ adjusted
using the methodology presented in Section V-A. We emphasize that this parameter setting strategy assumes no prior knowledge
about the abundance ground truth.

The simulation results are summarized in Tables I to IV. In these tables, the ﬁrst column shows the BS strategy considered
prior to unmixing. SK-Hype in this column indicates the solution without BS. The symbol ”(r)” besides CCBS or GCBS means
that we have randomized the order of the bands prior to applying the BS strategy. The second column shows the obtained
RMSE and the standard deviation (STD) in abundance estimation. The third column lists the average CPU time elapsed in the
(BS + unmixing) process. Column four shows the number of selected bands Nb, and last column shows the coherence of the
ﬁnal dictionary.

Tables I and II show the results for HIs built with Cuprite endmembers and using, respectively, the GBM and the PNMM
mixing models. Note that the RMSE obtained using the BS algorithms are very close to those obtained using all bands.
Nevertheless the reduction in number of bands obtained through BS is at least tenfold. The computational complexity advantage
of the BS methods is evidenced by the required average CPU time, which show reductions by factors ranging from 50 to 110,
depending on the algorithm and parameter settings. Note also that the number of bands in the ﬁnal dictionary tends to be larger
than the value M used to initialize the algorithms. This increase in the anticipated number of bands is obtained to optimize
the dictionary coherence, what is not possible in the GKKM algorithm. As expected, the number of bands remained the same
for the clique algorithm (CCBS) for each value of M, and the slight changes in the RMSE results indicate that the maximum

14

TABLE I: RMSE. 100 runs, 2000 pxl., 8 endmembers (Cuprite), SNR=21dB, GBM, SK-Hype. µ0 computed using Equation (28)
for a given M, and σ is found solving problem (30).

Strategy

SK-Hype

GKKM

CCBS

CCBS (r)

GCBS

GCBS (r)

CCBS

CCBS (r)

GCBS

GCBS (r)

CCBS

CCBS (r)

GCBS

GCBS (r)

CCBS

CCBS (r)

GCBS

GCBS (r)

RMSE ± STD
0.0680 ± 0.0028
0.0664 ± 0.0026

Av. Time

301.08 ± 17.93
25.40 ± 0.22

M = 5, µ0 = 0.25, σ = 0.2548

0.0687 ± 0.0028
0.0687 ± 0.0028
0.0724 ± 0.0031
0.0721 ± 0.0030

3.10 ± 0.14
3.13 ± 0.12
2.91 ± 0.02
3.15 ± 0.15

Nb
420

36

10

10

8

7.13 ± 0.97

M = 10, µ0 = 0.1111, σ = 0.1320

0.0678 ± 0.0027
0.0679 ± 0.0027
0.0685 ± 0.0028
0.0688 ± 0.0028

2.85 ± 0.13
2.89 ± 0.17
2.57 ± 0.02
2.65 ± 0.06

16

16

16

13.09 ± 1.10

M = 20, µ0 = 0.0526, σ = 0.0965

0.0659 ± 0.0026
0.0660 ± 0.0026
0.0670 ± 0.0027
0.0678 ± 0.0027

2.96 ± 0.15
3.01 ± 0.17
2.59 ± 0.02
2.67 ± 0.08

21

21

20

15.95 ± 1.13

M = 30, µ0 = 0.0345, σ = 0.0503

0.0637 ± 0.0024
0.0637 ± 0.0024
0.0637 ± 0.0024
0.0644 ± 0.0025

5.54 ± 0.22
5.74 ± 0.18
3.32 ± 0.04
2.83 ± 0.07

42

42

41

33.39 ± 1.43

µ

-

0.5893

0.2482

0.2482

0.2482

0.2331

0.1108

0.1108

0.1104

0.0996

0.0520

0.0520

0.0525

0.0467

0.0339

0.0339

0.0344

0.0326

clique is not unique. For the greedy approach (GCBS), however, different numbers of bands are obtained at each execution due
to initial randomization, and the results in terms of RMSE and CPU time vary slightly. In general, randomization did not have
any signiﬁcant impact on the results. Finally, one should note from these tables that the coherence-based algorithms produced
dictionaries with coherence close to µ0, and 2 to 23 times smaller than the coherence obtained using GKKM.

Tables III and IV show the results for the HIs created with the Pavia endmembers using the GBM and PNMM respectively.
Although the results in Tables III and IV follows the same pattern that the results in Tables I and II, we highlight that for the
Pavia HIs the number of available bands is 103 in contrast to the 420 used in the previous example. This explains the smaller
improvement in the Av. Time when using the BS algorithms which is about 3 to 4 times smaller than using all the bands.
Another difference in the results is that using the BS algorithms, and its reasoning for setting µ0 and σ2, the best results in
terms of RMSE were obtained by the proposed method CCBS with M = 30 in both Tables. When concerning the number of
bands, the ﬁnal Nb were closer to m than in the previous example. For the coherence of the ﬁnal dictionary the same pattern
obtained in Tables I and II repeats for the Pavia HIs.

C. Simulation with real data

When working with real data ground truth for the fractional abundances are rarely available. Thus, we compare the abundance
estimation results obtained using a full band approach and using the proposed band selection strategy. First, the data is unmixed

TABLE II: RMSE. 100 runs, 2000 pxl., 8 endmembers (Cuprite), SNR=21dB, PNMM, SK-Hype. µ0 computed using
Equation (28) for a given M, and σ is found solving problem (30).

15

Strategy

SK-Hype

GKKM

CCBS

CCBS (r)

GCBS

GCBS (r)

CCBS

CCBS (r)

GCBS

GCBS (r)

CCBS

CCBS (r)

GCBS

GCBS (r)

CCBS

CCBS (r)

GCBS

GCBS (r)

RMSE ± STD
0.0728 ± 0.0030
0.0729 ± 0.0030

Av. Time

277.03 ± 4.30
25.52 ± 0.18

M = 5, µ0 = 0.25, σ = 0.2548

0.0748 ± 0.0031
0.0749 ± 0.0031
0.0764 ± 0.0032
0.0776 ± 0.0033

2.99 ± 0.10
3.12 ± 0.18
2.85 ± 0.06
2.99 ± 0.15

Nb
420

36

10

10

8

7.13 ± 0.97

M = 10, µ0 = 0.1111, σ = 0.1320

0.0746 ± 0.0031
0.0745 ± 0.0031
0.0757 ± 0.0032
0.0757 ± 0.0031

2.85 ± 0.19
2.84 ± 0.14
2.57 ± 0.04
2.64 ± 0.10

16

16

16

13.09 ± 1.10

M = 20, µ0 = 0.0526, σ = 0.0965

0.0735 ± 0.0029
0.0737 ± 0.0029
0.0753 ± 0.0031
0.0753 ± 0.0031

2.87 ± 0.12
2.96 ± 0.17
2.55 ± 0.03
2.56 ± 0.04

21

21

20

15.95 ± 1.13

M = 30, µ0 = 0.0345, σ = 0.0503

0.0740 ± 0.0029
0.0740 ± 0.0029
0.0737 ± 0.0029
0.0742 ± 0.0030

5.41 ± 0.18
5.62 ± 0.19
3.24 ± 0.04
2.74 ± 0.07

42

42

41

33.39 ± 1.43

µ

-

0.7760

0.2482

0.2482

0.2482

0.2331

0.1108

0.1108

0.1104

0.0996

0.0520

0.0520

0.0525

0.0467

0.0339

0.0339

0.0344

0.0326

(a) Pavia University represen-
tation.

(b) Ground truth for the Pavia Uni-
versity scene.

Fig. 2: Pavia University. In (a) the Pavia University HI is represented using the bands 5, 30, and 50. In (b) the classiﬁed areas
are labelled from 1 to 9, while 0 corresponds to unclassiﬁed areas.

0100200300100200300400500600  0100200300100200300400500600012345678916

TABLE III: RMSE. 100 runs, 2000 pxl., 8 endmembers (Pavia), SNR=21dB, GBM, SK-Hype. µ0 computed using Equation (28)
for a given M, and σ is found solving problem (30).

Strategy

SK-Hype

GKKM

RMSE ± STD
0.0810 ± 0.0035
0.0852 ± 0.0038

Av. Time

15.2468 ± 0.3231

5.69 ± 0.01

M = 5, µ0 = 0.25, σ = 0.2385

CCBS

CCBS (rand)

GCBS

GCBS (rand)

0.0845 ± 0.0037
0.0845 ± 0.0037
0.0848 ± 0.0037
0.0862 ± 0.0038

4.62 ± 0.05
4.64 ± 0.05
4.54 ± 0.02
5.02 ± 0.21

M = 10, µ0 = 0.1111, σ = 0.1

CCBS

CCBS (rand)

GCBS

GCBS (rand)

0.0813 ± 0.0035
0.0813 ± 0.0035
0.0824 ± 0.0035
0.0832 ± 0.0036

3.51 ± 0.04
3.53 ± 0.05
3.65 ± 0.03
3.76 ± 0.12

Nb
103

5

6

6

6

4.89 ± 0.37

12

12

12

9.58 ± 0.75

M = 20, µ0 = 0.0526, σ = 0.0498

CCBS

CCBS (rand)

GCBS

GCBS (rand)

0.0795 ± 0.0034
0.0794 ± 0.0034
0.0795 ± 0.0034
0.0804 ± 0.0035

3.43 ± 0.04
3.45 ± 0.04
3.49 ± 0.02
3.45 ± 0.07

20

20

20

16.55 ± 0.88

M = 30, µ0 = 0.0345, σ = 0.0353

CCBS

CCBS (rand)

GCBS

GCBS (rand)

0.0784 ± 0.0034
0.0784 ± 0.0033
0.0787 ± 0.0034
0.0790 ± 0.0034

3.68 ± 0.03
3.68 ± 0.04
3.67 ± 0.03
3.54 ± 0.06

25

25

25

21.09 ± 1.02

µ

-

0.5347

0.2402

0.2395

0.2338

0.1812

0.1098

0.1098

0.1080

0.0907

0.0383

0.0437

0.0499

0.0408

0.0314

0.0311

0.0300

0.0282

using the SK-Hype algorithm using all the available spectral bands, what yields the estimated abundances αskp
n , n = 1, . . . , N.
The unmixing is then done for all each of the BS methods presented in Section IV. Generically denominating the BS-based
estimated abundances as αbs
n , n = 1, . . . , N, the RMSE between the SK-Hype abundances and those obtained using a given
BS algorithm is computed as

(cid:118)(cid:117)(cid:117)(cid:116) N(cid:88)

RMSE =

(cid:107)αskp

n − αbs

n (cid:107)2/(N × R).

(39)

n=1

The images used are shown in Figure 3 and Figure 2. The ﬁrst image is a scene from the Cuprite mining ﬁeld site in
Nevada, acquired by the AVIRIS instrument. It has originally 224 spectral bands, from which we have removed the water
absorption bands, resulting in 188 bands. This scene has 7371 pixels and previous analysis identiﬁed ﬁve minerals (Sphene,
Montmorillonite, Kaolinite, Dumortierite, and Pyrope) to have strong components in this particular region [63]. The endmember
matrix was extracted using the VCA algorithm [22]. The second image is the scene from the Pavia University described in
Section VI-B. It has 207400 pixels and the endmembers were also extracted using VCA, see Section VI-B.

Tables V and VI show the abundance RMSE results obtained using (39). For both tables, the RMSE performance is
compatible to that obtained using synthetic images, and the savings in computational complexity can be inferred from the CPU
time reduction by a factor of at least 13 (for M = 30) for the Cuprite scene and at least 3 (for M = 30) for the Pavia scene.

TABLE IV: RMSE. 100 runs, 2000 pxl., 8 endmembers (Pavia), SNR=21dB, PNMM, SK-Hype. µ0 computed using
Equation (28) for a given M, and σ is found solving problem (30).

17

Strategy

SK-Hype

GKKM

CCBS

CCBS (r)

GCBS

GCBS (r)

CCBS

CCBS (r)

GCBS

GCBS (r)

CCBS

CCBS (r)

GCBS

GCBS (r)

CCBS

CCBS (r)

GCBS

GCBS (r)

RMSE ± STD
0.0839 ± 0.0035
0.0878 ± 0.0038

Av. Time

14.6747 ± 0.3073

5.31 ± 0.02

M = 5, µ0 = 0.25, σ = 0.2385

0.0861 ± 0.0037
0.0861 ± 0.0037
0.0877 ± 0.0038
0.0882 ± 0.0039

4.34 ± 0.04
4.34 ± 0.05
4.17 ± 0.02
4.56 ± 0.23

Nb
103

5

6

6

6

4.89 ± 0.37

M = 10, µ0 = 0.1111, σ = 0.1

0.0835 ± 0.0035
0.0835 ± 0.0035
0.0852 ± 0.0035
0.0857 ± 0.0036

3.27 ± 0.03
3.25 ± 0.04
3.32 ± 0.01
3.38 ± 0.08

12

12

12

9.58 ± 0.75

M = 20, µ0 = 0.0526, σ = 0.0498

0.0817 ± 0.0034
0.0817 ± 0.0034
0.0817 ± 0.0034
0.0828 ± 0.0035

3.22 ± 0.04
3.23 ± 0.05
3.27 ± 0.02
3.24 ± 0.05

20

20

20

16.55 ± 0.88

M = 30, µ0 = 0.0345, σ = 0.0353

0.0804 ± 0.0033
0.0803 ± 0.0033
0.0806 ± 0.0033
0.0810 ± 0.0034

3.43 ± 0.05
3.45 ± 0.03
3.48 ± 0.05
3.33 ± 0.06

25

25

25

21.09 ± 1.02

µ

-

0.5347

0.2402

0.2395

0.2338

0.1812

0.1098

0.1098

0.1080

0.0907

0.0383

0.0437

0.0499

0.0408

0.0314

0.0311

0.0300

0.0282

Fig. 3: Cuprite scene used in [63].

In comparing CCBS and GCBS with GKKM one should note the signiﬁcant reduction obtained in dictionary coherence for
the same model complexity (Nb).

VII. CONCLUSIONS

In this paper we have proposed a centralized method for nonlinear unmixing of hyperspectral images, which employs band
selection in in the reproducing kernel Hilbert space (RKHS). The proposed method is based on the coherence criterion, which
incorporates a measure of the quality of the dictionary in the RKHS for the nonlinear unmixing. We have shown that the
proposed BS approach is equivalent to solving a maximum clique problem (MCP). Contrary to competing methods that do

102030405060708010203040506070809018

TABLE V: Cuprite image. RMSE between the abundances estimated with SK-Hype (all bands) and BS + SK-Hype.

Strategy

SK-Hype

GKKM

CCBS

GCBS

CCBS

GCBS

CCBS

GCBS

CCBS

GCBS

RMSE ± STD

-

CPU Time Nb
188

282.42

µ

-

9

9

13

19.289

15.2023

17.7114

18.4835

0.0777 ± 0.0036
M = 5, µ0 = 0.25, σ = 0.0963
0.0805 ± 0.0038
0.0833 ± 0.0040
M = 10, µ0 = 0.1111, σ = 0.0489
0.0659 ± 0.0027
16
0.0695 ± 0.0029
M = 20, µ0 = 0.0526, σ = 0.0260
0.0477 ± 0.0015
25
0.0484 ± 0.0015
M = 30, µ0 = 0.0345, σ = 0.0178
0.0378 ± 0.0010
35
0.0395 ± 0.0011

14.5721

17.0942

16.9595

20.6932

20.4790

15

25

34

0.8162

0.2495

0.2483

0.1090

0.1090

0.0471

0.0493

0.0333

0.0300

TABLE VI: Pavia University image. RMSE between the abundances estimated with SK-Hype (all bands) and BS + SK-Hype.

Strategy

SK-Hype

GKKM

CCBS

GCBS

CCBS

GCBS

CCBS

GCBS

CCBS

GCBS

RMSE ± STD

-

CPU Time Nb
1740.47
103

µ

-

6

6

13

533.48

513.21

568.10

0.0446 ± 0.0015
M = 5, µ0 = 0.25, σ = 0.2492
0.0659 ± 0.0037
0.0650 ± 0.0036
M = 10, µ0 = 0.1111, σ = 0.1017
0.0435 ± 0.0016
12
0.0500 ± 0.0023
M = 20, µ0 = 0.0526, σ = 0.0503
0.0301 ± 0.0008
21
0.0309 ± 0.0009
M = 30, µ0 = 0.0345, σ = 0.0336
0.0260 ± 0.0007
26
0.0263 ± 0.0007

495.13

497.92

488.67

535.64

538.63

488.66

12

26

21

0.5066

0.2499

0.2499

0.1024

0.1019

0.0433

0.0472

0.0336

0.0336

not include an efﬁcient choice of the model parameters, the proposed method requires only an initial guess on the number of
selected bands. Simulation results employing both synthetic and real data illustrate the quality of the unmixing results obtained
with the proposed method, which leads to abundance estimations as accurate as those obtained using the full-band SK-Hype
method, at a small fraction of the computational cost.

[1] J. M. Bioucas-Dias, A. Plaza, G. Camps-Valls, P. Scheunders, N. Nasrabadi, and J. Chanussot, “Hyperspectral remote sensing data analysis and future

challenges,” IEEE Geoscience and Remote Sensing Magazine, vol. 1, no. 2, pp. 6–36, 2013.

REFERENCES

19

[2] Manolakis, “Detection algorithms for hyperspectral imaging applications,” Signal Processing Magazine, IEEE, vol. 19, no. 1, pp. 29–43, 2002.
[3] N. Keshava and J.F. Mustard, “Spectral unmixing,” IEEE Signal Processing Magazine, vol. 19, no. 1, pp. 44–57, 2002.
[4] T. W. Ray and B. C. Murray, “Nonlinear spectral mixing in desert vegetation,” Remote Sensing of Environment, vol. 55, no. 1, pp. 59–64, 1996.
[5] R. Heylen, M. Parente, and P. Gader, “A review of nonlinear hyperspectral unmixing methods,” IEEE J. on Selected Topics in Applied Earth Observations

and Remote Sensing, vol. 7, no. 6, pp. 1844–1868, June 2014.

[6] N. Dobigeon, J.-Y. Tourneret, C. Richard, J. C. M. Bermudez, S. McLaughlin, and A. O. Hero, “Nonlinear unmixing of hyperspectral images: Models

and algorithms,” IEEE Signal Processing Magazine, vol. 31, no. 1, pp. 82–94, Jan 2014.

[7] X. Wu, X. Li, and L. Zhao, “A kernel spatial complexity-based nonlinear unmixing method of hyperspectral imagery,” in Proc. LSMS/ICSEE, 2010,

pp. 451–458.

[8] X. Li, J. Cui, and L. Zhao, “Blind nonlinear hyperspectral unmixing based on constrained kernel nonnegative matrix factorization,” Signal, Image and

Video Processing, vol. 8, no. 8, pp. 1555–1567, 2012.

[9] J. Chen, C. Richard, and P. Honeine, “Nonlinear unmixing of hyperspectral data based on a linear-mixture/nonlinear-ﬂuctuation model,” Signal Processing,

IEEE Transactions on, vol. 61, pp. 480–492, Jan 2013.

[10] J. Chen, C. Richard, and P. Honeine, “Nonlinear estimation of material abundances in hyperspectral images with L1-norm spatial regularization,” IEEE

Transactions on Geoscience and Remote Sensing, 2013 (to appear).

[11] V. N. Vapnik, The nature of statistical learning theory, Springer, New York, NY, 1995.
[12] A. J. Smola and B. Sch¨olkopf, “A tutorial on support vector regression,” Statistics and computing, vol. 14, no. 3, pp. 199–222, 2004.
[13] J. A. K. Suykens, T. Van Gestel, J. De Brabanter, B. De Moor, and J. Vandewalle, Least Squares Support Vector Machines, World Scientiﬁc, Singapore,

2002.

[14] N. Aronszajn, “Theory of reproducing kernels,” Transactions of the American Mathematical Society, vol. 68, 1950.
[15] C.-I. Chang, Hyperspectral data processing - Algorithm design and analysis, Wiley, 2014.
[16] Q. Du and H. Yang, “Similarity-based unsupervised band selection for hyperspectral image analysis,” Geoscience and Remote Sensing Letters, IEEE,

vol. 5, no. 4, pp. 564–568, 2008.

[17] P. Est´evez, M. Tesmer, C. Perez, and J. M. Zurada, “Normalized mutual information feature selection,” Neural Networks, IEEE Transactions on, vol.

20, no. 2, pp. 189–201, 2009.

[18] A. Mart´ınez-Us´o, F. Pla, J. M. Sotoca, and P. Garc´ıa-Sevilla, “Clustering-based hyperspectral band selection using information measures,” Geoscience

and Remote Sensing, IEEE Transactions on, vol. 45, no. 12, pp. 4158–4171, 2007.

[19] J. Feng, L. C. Jiao, X. Zhang, and T. Sun, “Hyperspectral band selection based on trivariate mutual information and clonal selection,” vol. 52, no. 7,

pp. 4092–4105, 2014.

[20] J. Feng, L. Jiao, F. Liu, T. Sun, and X. Zhang, “Mutual-information-based semi-supervised hyperspectral band selection with high discrimination, high

information, and low redundancy,” vol. 53, pp. 2956–2969, 2015.

[21] J. M. Bioucas-Dias and J. M. P. Nascimento, “Hyperspectral Subspace Identiﬁcation,” IEEE Transactions on Geoscience and Remote Sensing, vol. 46,

no. 8, pp. 2435–2445, 2008.

[22] J. M. P. Nascimento and J. M. Bioucas-Dias, “Vertex Component Analysis: A fast algorithm to unmix hyperspectral data,” IEEE Transactions on

Geoscience and Remote Sensing, vol. 43, no. 4, pp. 898–910, April 2005.

[23] J. M. Bioucas-Dias and A. Plaza, “An overview on hyperspectral unmixing: Geometrical, statistical, and sparse regression based approaches,” Geoscience

and Remote Sensing Symposium (IGARSS), 2011 IEEE International, pp. 1135–1138, 2011.

[24] C.-I. Chang and S. Wang, “Constrained band selection for hyperspectral imagery,” vol. 44, no. 6, pp. 1575–1585, 2006.
[25] C.-I. Chang and K.-H. Liu, “Progressive band selection of spectral unmixing for hyperspectral imagery,” IEEE Transactions on Geoscience and Remote

Sensing, vol. 52, no. 4, pp. 2002–2017, 2014.

[26] T. Imbiriba, J.C.M. Bermudez, C. Richard, and J.-Y. Tourneret, “Band selection in rkhs for fast nonlinear unmixing of hyperspectral images,” in Signal

Processing Conference (EUSIPCO), 2015 23rd European, Aug 2015, pp. 1651–1655.

[27] C. Richard, J. C. M. Bermudez, and P. Honeine, “Online prediction of time series data with kernels,” IEEE Transactions on Signal Processing, vol. 57,

no. 3, pp. 1058–1067, March 2009.

[28] C.-M. Li and Z. Quan, “An efﬁcient branch-and-bound algorithm based on maxsat for the maximum clique problem.,” in AAAI, 2010, vol. 10, pp.

128–133.

[29] A. Halimi, Y. Altmann, N. Dobigeon, and J.-Y. Tourneret, “Nonlinear Unmixing of Hyperspectral Images Using a Generalized Bilinear Model,” IEEE

Transactions on Geoscience and Remote Sensing, vol. 49, no. 11, pp. 4153–4162, Nov. 2011.

[30] C. Jutten and J. Karhunen, “Advances in nonlinear blind source separation,” in Proc. International Symposium on Independent Component Analysis and

Blind Signal Separation (ICA), 2003, pp. 245–256.

20

[31] Y. Altmann, N. Dobigeon, and J.-Y. Tourneret, “Nonlinearity detection in hyperspectral images using a polynomial post-nonlinear mixing model.,” IEEE

Transactions on Image Processing, vol. 22, no. 4, pp. 1267–1276, 2013.

[32] Y. Altmann, A. Halimi, N. Dobigeon, and J.-Y. Tourneret, “Supervised nonlinear spectral unmixing using a polynomial post nonlinear model for

hyperspectral imagery,” in Proc. IEEE ICASSP, 2011, pp. 1009–1012.

[33] W. Fan, B. Hu, J. Miller, and M. Li, “Comparative study between a new nonlinear model and common linear model for analysing laboratory simulated-

forest hyperspectral data,” International Journal of Remote Sensing, vol. 30, no. 11, pp. 2951–2962, 2009.

[34] J. M. P. Nascimento and J. M. Bioucas-Dias, “Nonlinear mixture model for hyperspectral unmixing,” in Proc. SPIE, 2009, vol. 7477.
[35] B. Hapke, Theory of Reﬂectance and Emittance Spectroscopy, Cambridge University Press, 1993.
[36] C.C. Borel and Siegfried A. W. Gerstl, “Nonlinear spectral mixing models for vegetative and soil surfaces,” Remote Sensing of Environment, vol. 47,

no. 3, pp. 403–416, Jan 1994.

[37] B. Somers, K. Cools, S. Delalieux, J. Stuckens, D. Van der Zande, W. W. Verstraeten, and P. Coppin, “Nonlinear hyperspectral mixture analysis for tree

cover estimates in orchards,” Remote Sensing of Environment, vol. 113, no. 6, pp. 1183–1193, February 2009.

[38] T. W. Ray and B. C. Murray, “Nonlinear spectral mixing in desert vegetation,” Remote Sensing of Environment, vol. 55, no. 1, pp. 59–64, 1996.
[39] J. Broadwater and A. Banerjee, “A comparison of kernel functions for intimate mixture models,” in Proc. IEEE IGARSS, 2009, pp. 1–4.
[40] G. Kimeldorf and G. Wahba, “Some results on Tchebychefﬁan spline functions,” Journal of Mathematical Analysis and Applications, vol. 33, pp. 82–95,

1971.

[41] E. Kreyszig, Introductory functional analysis with applications, vol. 81, wiley New York, 1989.
[42] B. Sch¨olkopf, R. Herbrich, and A. J. Smola, “A generalized representer theorem,” in Computational learning theory. Springer, 2001, pp. 416–426.
[43] J. Mercer, “Functions of positive and negative type and their connection with the theory of integral equations,” Philos. Trans. Roy. Soc. London Ser. A,

vol. 209, pp. 415–446, 1909.

[44] E. H. Moore, “On properly positive hermitian matrices,” Bull. American Mathematical Society, vol. 23, pp. 59, 1916.
[45] B. Sch¨olkopf and A. J. Smola, Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond, The MIT Press, 2001.
[46] H. Kadri, E. Duﬂos, M. Davy, P. Preux, and S. Canu, “General framework for nonlinear functional regression with reproducing kernel hilbert spaces,”

Research Report RR-6908, INRIA, 2009.

[47] G. F. Tzortzis and A. C. Likas, “The global kernel k-means algorithm for clustering in feature space.,” IEEE Transactions on Neural Networks, vol.

20, pp. 1181–1194, 2009.

[48] A. Tropp, J, “Greed is good: Algorithmic results for sparse approximation,” Information Theory, IEEE Transactions on, vol. 50, no. 10, pp. 2231–2242,

2004.

[49] S. G. Mallat and Z. Zhang, “Matching pursuits with time-frequency dictionaries,” Signal Processing, IEEE Transactions on, vol. 41, no. 12, pp.

3397–3415, 1993.

[50] D. L. Donoho and X. Huo, “Uncertainty principles and ideal atomic decomposition,” Information Theory, IEEE Transactions on, vol. 47, no. 7, pp.

2845–2862, 2001.

[51] M. Elad and A. M. Bruckstein, “A generalized uncertainty principle and sparse representation in pairs of bases,” Information Theory, IEEE Transactions

on, vol. 48, no. 9, pp. 2558–2567, 2002.

[52] D. L. Donoho and M. Elad, “Optimally sparse representation in general (nonorthogonal) dictionaries via 1 minimization,” Proceedings of the National

Academy of Sciences, vol. 100, no. 5, pp. 2197–2202, 2003.

[53] P. M. Pardalos and J. Xue, “The maximum clique problem,” Journal of global Optimization, vol. 4, no. 3, pp. 301–328, 1994.
[54] R. M. Karp, Reducibility among combinatorial problems, Springer, 1972.
[55] Q. Wu and J.-K. Hao, “A review on algorithms for maximum clique problems,” European Journal of Operational Research, vol. 242, no. 3, pp. 693–709,

2015.

[56] R. Carraghan and P. M. Pardalos, “An exact algorithm for the maximum clique problem,” Operations Research Letters, vol. 9, no. 6, pp. 375–382, 1990.
[57] C.-M. Li, Z. Fang, and K. Xu, “Combining maxsat reasoning and incremental upper bound for the maximum clique problem,” in Tools with Artiﬁcial

Intelligence (ICTAI), 2013 IEEE 25th International Conference on. IEEE, 2013, pp. 939–946.

[58] P. R. J. ¨Osterg˚ard, “A fast algorithm for the maximum clique problem,” Discrete Applied Mathematics, vol. 120, no. 1, pp. 197–207, 2002.
[59] N. Biggs, “Some heuristics for graph coloring,” Graph Colourings, Longman, New York, pp. 87–96, 1990.
[60] T. Fahle, “Simple and fast: Improving a branch-and-bound algorithm for maximum clique,” in AlgorithmsESA 2002, pp. 485–498. Springer, 2002.
[61] E. Maslov, M. Batsyn, and P. M. Pardalos, “Speeding up branch and bound algorithms for solving the maximum clique problem,” Journal of Global

Optimization, vol. 59, no. 1, pp. 1–21, 2014.

[62] C. D. Manning, P. Raghavan, and H. Sch¨utze, Introduction to information retrieval, vol. 1, Cambridge university press, Cambridge, 2008.
[63] T. Imbiriba, J.C.M. Bermudez, C. Richard, and J.-Y. Tourneret, “Nonparametric detection of nonlinearly mixed pixels and endmember estimation in

hyperspectral images,” Image Processing, IEEE Transactions on, vol. 25, no. 3, pp. 1136–1151, March 2016.

