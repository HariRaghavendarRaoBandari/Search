6
1
0
2

 
r
a

 

M
9
1

 
 
]
T
S
h
t
a
m

.

[
 
 

1
v
0
0
1
6
0

.

3
0
6
1
:
v
i
X
r
a

THE KATO-TEMPLE INEQUALITY AND EIGENVALUE CONCENTRATION

JOSHUA CAPE, MINH TANG, AND CAREY E. PRIEBE

March 22, 2016

Department of Applied Mathematics and Statistics, Johns Hopkins University

Abstract. In this paper we use the setting of inhomogeneous random graphs to present an extension of the
Kato-Temple inequality for bounding eigenvalues. We prove under mild assumptions that the top eigenvalues
(i.e. largest in magnitude and ordered) of the adjacency matrix concentrate around the corresponding top
eigenvalues of the edge probability matrix with high probability, even when the latter have multiplicity
(Theorem 3.5). The proof techniques we employ do not strictly depend upon the underlying random graph
setting and are readily extended to yield high-probability bounds for perturbations of singular values of
rectangular matrices in a quite general random matrix setting (Theorem 4.4).

1. Introduction

We study simple undirected random graphs on n vertices generated via the inhomogeneous Erd¨os-R´enyi
model, G(n, P ), where P := (Pi,j )1≤i,j≤n ∈ [0, 1]n×n denotes the symmetric edge probability matrix. This
independent edge model generalizes numerous widely-studied random graph models, including the classical
Erd¨os-R´enyi model, the random dot product graph model [10], and the stochastic block model.

For a graph G where G ∼ G(n, P ), the eigenvalues and eigenvectors of its corresponding (symmetric)
adjacency matrix A ∈ {0, 1}n×n are interpreted as encoding structural information about the graph (e.g.
community structure). Given that Ai,j ∼ Bernoulli(Pi,j ) for all i < j and by ignoring the main diago-
nal entries, the correspondence P = E[A] allows us to think of the matrix P as representing the model’s
“expected graph.” When viewed from this perspective, it is natural to study the relationship between the
eigenvalues (eigenvectors, respectively) of A and P . Of particular interest is determining conditions under
which the eigenvalues (eigenvectors, respectively) of A and P are quantitatively close to one another with
high probability (for example, when the spectral norm of the matrix diﬀerence is small, denoted kA − Pk2).
Under such conditions, random graphs from the underlying model are often said to “concentrate.”
largest in magnitude and
ordered) of A concentrate with high probability around the corresponding top eigenvalues of P (in the sense
of Euclidean distance), even when the latter have multiplicity. Both our main result for the inhomogeneous
random graph model (Theorem 3.5) and our proof techniques diﬀer noticeably from those in the random
graph literature, as we shall make explicit below. Moreover, Theorem 3.5 can be easily extended beyond the
random graph setting to yield high-probability bounds for perturbations of singular values of rectangular
matrices in a quite general random matrix setting (Theorem 4.4).

In this paper, we prove under mild assumptions that the top eigenvalues (i.e.

In the inhomogeneous random graph literature, concentration bounds are known for each eigenvalue of
A, denoted σi(A), both around its median and around its expectation, E[σi(A)] [1]. Unfortunately, since
the latter quantities are inaccessible in practice, the bounds obtained therein are not of practical use. For
graphs with maximum expected degree at least C ln n for some constant C, [7] proves an upper bound of
order O(√DM ln n) with high probability for kA− Pk2, where DM denotes the maximum expected degree of
the underlying graph. In [6] this bound is improved to (2 + o(1))√DM under the stronger assumption that
DM ≫ ln4 n. Both [7] and [6] obtain bounds on the spectral norm kA − Pk2; Weyl’s inequality then yields
bounds which hold uniformly for all ordered eigenvalue pairs σi(A) and σi(P ). Alternative approaches for
studying the spectrum of the adjacency matrix include the Stieltjes transform method from random matrix
theory (see for example [11]).

The relationship between the top eigenvalues of A and P was brieﬂy considered in [2] although only for
the purpose of proving the main limit theorem therein. Our Theorem 3.5 improves upon Lemma 2 in [2] by
removing a distinct eigenvalue assumption and by yielding tighter high-probability bounds for the distances
between pairs of the top eigenvalues of A and P which are of the same order. Our proof technique adapts the

1

2

JOSHUA CAPE, MINH TANG, AND CAREY E. PRIEBE

original, deterministic setting in [5] to a new setting involving randomness. In this way, our work also diﬀers
from a (deterministic) generalization of the Kato-Temple inequality found in [4]. Our result is diﬀerent from
[1] since the quantity E[σi(A)] is in general not equal to σi(E[A]). For the top eigenvalues of A which are of
the same order as one another and as the corresponding top eigenvalues of P , we obtain tighter bounds than
the (uniform) bounds in [7] and [6] by instead directly bounding the distance between each eigenvalue pair.

This work is partially supported by the XDATA, GRAPHS, and SIMPLEX programs of the Defense
Advanced Research Projects Agency (DARPA), and by the Acheson J. Duncan Fund for the Advancement
of Research in Statistics.

Acknowledgments

2. Setup and notation

We let h·,·i denote the dot product (i.e. standard Euclidean inner product) between two vectors, k · k
denote the vector norm induced by the dot product, and k · k2 denote the spectral norm of a matrix. The
identity matrix is implicitly understood when we write the diﬀerence of a matrix with a scalar.
As prefaced in Section 1, we study simple undirected random graphs on n vertices generated by the
inhomogeneous Erd¨os-R´enyi model, G ∼ G(n, P ), via the corresponding adjacency matrix A := AG. We
emphasize that here P is unknown but deterministic. Both matrices A and P have rank at most n, and we
make no further rank assumptions (e.g. bounded rank), even allowing their rank to grow with n.

We denote the (not necessarily distinct) real eigenvalues of (the real symmetric matrix) P in decreasing

(global) order by

(2.1)

σ↓1(P ) ≥ σ↓2 (P ) ≥ ··· ≥ σ↓n(P )

and similarly for A. Given an open interval in the positive half of the real line, (α, β) ⊂ R>0, we denote the
d eigenvalues of P that lie in this interval (locally) by

(2.2)

α < σ↑1(P )(α,β) ≤ σ↑2 (P )(α,β) ≤ ··· ≤ σ↑d(P )(α,β) < β,

and again similarly for A. When the underlying interval of interest is understood, we omit the corresponding
interval subscript and rely upon the superscript arrows to distinguish whether the frame of reference for
a particular eigenvalue is either global or local. Our choice of ordering in Eqn. (2.2) is motivated by the
notation in [5] and is intended to improve the ease of translation between our paper and [5].

Remark 2.1. Given the n × n real matrices A and P , it will be useful to consider the corresponding real
symmetric 2n × 2n matrices ˜A and ˜P deﬁned as
˜A :=(cid:18) 0 A

A⊤ 0 (cid:19) and ˜P :=(cid:18) 0

P

P ⊤ 0 (cid:19) .

In particular, it is well-known that the non-zero eigenvalues of ˜A and ˜P correspond to the signed singular
values of A and P , respectively ([3], Theorem 7.3.3). This correspondence between the singular values of
arbitrary matrices and the eigenvalues of Hermitian matrices shall play an important role in the generalization
of Theorem 3.5 to Theorem 4.4.

Remark 2.2. For later convenience we now reproduce two lemmas from [5] and the Kato-Temple inequality
as presented in [4]. These classical results all depend upon the following setting.

Let H be a self-adjoint operator on a Hilbert space. Assume a (normalized) vector w is in the domain of

H and deﬁne η := hHw, wi along with ǫ := k(H − η)wk, noting that η2 + ǫ2 = kHwk2.
Lemma 2.3 (Lemma 1 in [5]). For every α such that α < η (where α = −∞ is permitted), the interval
(α, η + ǫ2

η−α ] contains a point in the spectrum of H.

Lemma 2.4 (Lemma 2 in [5]). For every β such that β > η (where β = ∞ is permitted), the interval
[η − ǫ2

β−η , β) contains a point in the spectrum of H.

THE KATO-TEMPLE INEQUALITY AND EIGENVALUE CONCENTRATION

3

Theorem 2.5 (Kato-Temple inequality; Theorem 2 in [4]). Suppose that ǫ2 < (β − η)(η − α) where α < β.
Then spectrum(H) ∩ (α, β) 6= ∅. Moreover, if the only point of the spectrum of H in the interval (α, β) is
the eigenvalue σ(H), then

η −

ǫ2

β − η ≤ σ(H) ≤ η +

ǫ2
η − α

.

3. Results

Let w1, . . . , wn denote unit (orthonormal) eigenvectors corresponding to the eigenvalues of P . Since
A = (A − P ) + P , we may view the matrix A − P as an additive perturbation with respect to P . The
following proposition makes an important probabilistic observation about the tail behavior of A − P .
Proposition 3.1. Let i, j ∈ [n] and consider wi, wj as deﬁned above. Then for any t > 1,
(3.1)

Pr[|w⊤j (A − P )wi| ≥ t] ≤ 2 exp[−(t − 1)2].

Proof. Firstly, by the triangle inequality and using the entry-wise notation wi
A := (Aa,b)1≤a,b≤n, we have

:= [wi,1, . . . , wi,n]⊤ and

(3.2)

(cid:12)(cid:12)w⊤j (A − P )wi(cid:12)(cid:12) ≤(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
Xa<b
a=1 Pa,awi,awj,a| ≤ |Pn
Note that |Pn
the orthonormality of the set {wi}n
mean zero independent random variables, hence by Hoeﬀding’s inequality
2(Aa,b − Pa,b)wi,awj,b| ≥ t] ≤ 2 exp(cid:18)
Pr[|Xa<b

2(Aa,b − Pa,b)wi,awj,b(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

+(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

Xa=1

(3.3)

a=1 wi,awj,a| = |hwi, wji| ≤ 1 since the entries of P are in [0, 1] and by
i=1. The ﬁrst term on the right side of Eqn. (3.2) is the sum of n(n− 1)/2

n

Pa,awi,awj,a(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

−2t2

Pa<b(2wi,awj,b)2(cid:19) ≤ 2 exp(−t2).

The ﬁnal inequality follows from the observation that

n

Xa<b

(2wi,awj,b)2 ≤ 2(

Xa,b=1

w2

i,aw2

j,b) = 2(hwi, wii × hwj , wji) = 2.

Combining Eqn. (3.2) and Eqn. (3.3) yields Eqn. (3.1).

(cid:3)

Remark 3.2. In the current setting, P is allowed to have positive values along its main diagonal, even though
by assumption we consider simple graphs and hence A is necessarily hollow. If P is further assumed to be
hollow or if we allow graphs with self-loops, then precisely P = E[A] and the above probability bound
becomes 2 exp(−t2) for t > 0.
Remark 3.3. We will make repeated use of Proposition 3.1 in the context of applying union bounds. For
example, when considering the tuples (i, j) ∈ [n] × [n], we have

Pr[|w⊤j (A − P )wi| ≤ t for all (i, j)] ≥ 1 − n2 × 2 exp(−(t − 1)2).

Remark 3.4. For each i ∈ [n] deﬁne the vector ˜wi := [ 1√2
wi]⊤ ∈ R2n. Observe that by construction
h ˜A ˜wi, ˜wji = hAwi, wji for all i, j ∈ [n]. Moreover, the same relationship holds for matrices ˜P and ˜A − ˜P .
Hence, Proposition 3.1 immediately extends to the collection ˜A, ˜P , and { ˜wi}n
i=1. This correspondence will
however diﬀer in the proof of Theorem 4.4.

wi, 1√2

We now state our theorem for the inhomogeneous random graph setting and oﬀer several additional

remarks before proceeding with its proof.
Theorem 3.5. Let matrices A ∈ {0, 1}n×n and P ∈ [0, 1]n×n correspond to the inhomogeneous random
graph model as in Section 2. Suppose α ∈ R>0 is such that the interval (α,∞) contains precisely the d
largest (in magnitude) eigenvalues of both A and P . Fix k ∈ [d]. Then for any t > 1,
(3.4)

σ↑k(A) ≥ σ↑k(P ) − t

4

JOSHUA CAPE, MINH TANG, AND CAREY E. PRIEBE

with probability at least 1 − (d − k + 1)2 × 2 exp(−((
Also,

t

(d−k+1)2+1 − 1)2).

(3.5)

σ↑k(A) ≤ σ↑k(P ) + t +

kkA − Pk2

2 + (3σ↑k(P ) + t − (1/2)α)t
σ↑1 (P ) − t − α

t

k2+1 − 1)2).

with probability at least 1 − k2 × 2 exp(−(
Remark 3.6. The numerator’s dependence upon α in Eqn. (3.5) can be removed by weakening the upper
bound to exclude the term −(1/2)αt. The denominator’s dependence upon α may also be removed by
imposing additional assumptions on the size of the gap |σ↑1 (P ) − α|.
Remark 3.7. Our proof depends upon several new observations with respect to Kato’s original argument and
our setting. In particular, for wi (re)deﬁned as corresponding to σ↑i (P ), the matrix (hAwi, wji)1≤i,j≤d need
not be diagonal, so {wi}d
i=1 need not constitute an orthonormal collection of “almost” eigenvectors of A in
the sense of [5]. Rather, we interpret the notion of “almost” via the probabilistic observation in Proposition
3.1. Moreover, this proposition is the source of randomness which extends our proof technique beyond Kato’s
deterministic setting. Of additional note is that the vectors wi as deﬁned in this paper agree in function
and notation with Kato’s original paper, with the distinction that our setting provides a canonical choice
for these vectors.
Proof. We note at the onset that working with ˜A, ˜P , and ˜wi is not strictly necessary in this proof; however,
the usefulness of these quantities will be evident in Section 4 when we adapt our proof to a more general
setting. We emphasize that here the vectors wi and ˜wi are associated with σ↑i (P ).

We now introduce the following notation (see Section 3 in [5]). For each i ∈ [d], deﬁne ηi as

ηi := h ˜A ˜wi, ˜wii = h(A − P )wi, wii + σ↑i (P ),

(3.6)
where the right hand side follows from the more general observation that for all i, j ∈ [d] we have h ˜A ˜wi, ˜wji =
w⊤j (A − P )wi + σ↑i (P )δi,j with the indicator function δi,j := I{i=j} arising via the orthonormality of the set
{wi}d
i=1.
By Proposition 3.1, loosely speaking we have that ηi ≈ σ↑i (P ). The ηi’s further diﬀer from the case
considered by [5] in that they are only “almost” in nondecreasing order (i.e. ordered up to possibly constant-
order shift factors which are small with high probability). We shall render this diﬀerence immaterial by only
needing to consider lower and upper bounds (with high probability) for the entire set {ηi}d
i=1.
As was considered in [5], we desire an interval (α, β) ⊂ R>0 which contains {σ↑i (A)}d
i=1 as eigenvalues
of ˜A and no others, along with the collection {ηi}d
i=1 (with high probability). Since all of these quantities
are positive, we may allow β = ∞ and hence are only interested in the existence of α > 0. Here α may be
interpreted as a threshold which separates a collection of the largest “signal” eigenvalues from the remainder
of the spectrum. We emphasize that no additional assumptions are made concerning the multiplicity of these
“signal” eigenvalues.

3.1. Lower bound: For each i ∈ [d], deﬁne

ǫi := k( ˜A − ηi) ˜wik.

For k ∈ [d], deﬁne the (d − k + 1)-dimensional manifold Md−k+1 by
Md−k+1 := span{uk, . . . , ud},

where {uk, . . . , ud} are eigenvectors of ˜A associated with {σ↑k(A), . . . , σ↑d(A)}. Also deﬁne

d

with coeﬃcients ri such that

˜w :=

ri ˜wi

Xi=k

d

k ˜wk2 =

|ri|2 = 1, and

Xi=k

h ˜w, uii = 0 for i = k + 1, . . . , d.

THE KATO-TEMPLE INEQUALITY AND EIGENVALUE CONCENTRATION

5

At this point, set

η := h ˜A ˜w, ˜wi and ǫ := k( ˜A − η) ˜wk.

It is straightforward to check that we also have η = hAw, wi and ǫ = k(A − η)wk for w :=Pd
By Lemma 2 in [5], the interval hη − ǫ2
β−η , β(cid:17) contains a point in the spectrum of ˜A. Note that by
construction, ˜w ∈ M⊥d−k =: Nd−k; moreover, ˜A ˜w ∈ Nd−k. In the Hilbert space Nd−k, however, the spectrum
of ˜A does not contain σ↑k+1(A), . . . , σ↑d(A) since uk+1, . . . , ud /∈ Nd−k. Thus, by another application of Lemma
β−η , β(cid:17) must be σ↑k(A) associated with uk. As mentioned
2 in [5], the eigenvalue of ˜A in the interval hη − ǫ2
above, we may set β = ∞ for the interval (α, β). So, together with the observation that

i=k riwi.

(3.7)

i ηi ≥ mink≤i≤dηi ≥ σ↑k(P ) − maxk≤i≤d|w⊤i (A − P )wi|,
r2

we therefore have that

d

d

d

r2
i ηi +

Xi=k
σ↑k(A) ≥ η
Xi=k
=
≥ σ↑k(P ) − maxk≤i≤d|w⊤i (A − P )wi| −(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

rirj(w⊤j (A − P )wi)

≥ σ↑k(P ) − t − (d − k + 1)2t
= σ↑k(P ) − ((d − k + 1)2 + 1)t

Xi6=j;k

d

Xi6=j;k

rirj(w⊤j (A − P )wi)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

with probability at least 1 − (d − k + 1)2 × 2 exp(−(t − 1)2) by applying a union bound via Proposition 3.1.
Replacing t with t/((d − k + 1)2 + 1) yields the lower bound.
3.2. Upper bound: We now proceed to demonstrate the more-involved upper bound. In what follows, we
shall (quite naturally) redeﬁne several quantities from the previous section.
For each i ∈ [d], again deﬁne

For k ∈ [d], (re)deﬁne the k-dimensional manifold Mk by

where {u1, . . . , uk} are eigenvectors of ˜A associated with {σ↑1(A), . . . , σ↑k(A)}. Also (re)deﬁne

ǫi := k( ˜A − ηi) ˜wik.

Mk := span{u1, . . . , uk},

k

˜w :=

ri ˜wi

Xi=1

k

k ˜wk2 =

|ri|2 = 1, and

Xi=1

h ˜w, uii = 0 for i = 1, . . . , k − 1.
η := h ˜A ˜w, ˜wi and ǫ := k( ˜A − η) ˜wk.

with coeﬃcients ri such that

Now set

It is straightforward to check by further algebraic expansion based on the deﬁnitions of η and ηi that

(3.8)

η = hAw, wi =

k

k

r2
i ηi +

Xi=1

Xi6=j

rirj w⊤j (A − P )wi

and ǫ = k(A − η)wk for w :=Pk
Moreover, we note for all i, j ∈ [k] that h ˜A ˜wi, ˜A ˜wji = hAwi, Awji and

i=1 riwi.

6

(3.9)

JOSHUA CAPE, MINH TANG, AND CAREY E. PRIEBE

hAwi, Awji = h(A − ηi)wi, (A − ηj)wji + (ηi + ηj)hAwi, wji − η2

i δi,j.

By Lemma 1 in [5], the interval (cid:16)α, η + ǫ2
η−αi contains a point in the spectrum of ˜A. Note that by construc-
tion, ˜w ∈ M⊥k−1 =: Nk−1; moreover, ˜A ˜w ∈ Nk−1. In the Hilbert space Nk−1, however, the spectrum of ˜A
does not contain σ↑1(A), . . . , σ↑k−1(A) since u1, . . . , uk−1 /∈ Nk−1. Thus, by another application of Lemma 1
η−αi must be σ↑k(A) associated with uk. Hence
in [5], the eigenvalue of ˜A in the interval given by (cid:16)α, η + ǫ2
η2 + ǫ2 − αη
η − α

σ↑k(A) ≤ η +

ǫ2
η − α

(3.10)

=

.

Next, we bound the quantity η2 + ǫ2. By applying deﬁnitions and expanding, we see that

η2 + ǫ2 = h ˜A ˜w, ˜wi2 + k( ˜A − h ˜A ˜w, ˜wi) ˜wk2 = k ˜A ˜wk2 =

k

Xi,j=1

rirjh ˜A ˜wi, ˜A ˜wji.

As noted above, h ˜A ˜wi, ˜A ˜wji = hAwi, Awji for all terms in the sum, so

η2 + ǫ2

k

k

k

k

=

r2

r2

Xi6=j

rirjhAwi, Awji


rirjhAwi, Awji
i hAwi, Awii! +

i(cid:1)!
i (cid:0)h(A − ηi)wi, (A − ηi)wii + η2
rirj (ηi + ηj)hAwi, wji
Xi6=j
rirjh(A − ηi)wi, (A − ηj)wji +

Xi=1
rirj (k(A − ηi)wikk(A − ηj)wjk) +

Xi,j=1
=  k
Xi=1
=  k
Xi=1
+
Xi6=j

Xi,j
rirjh(A − ηi)wi, (A − ηj)wji +
Xi,j
Xi=1
≤ (

rirj(ηi + ηj)h(A − P )wi, wji.

ǫi|ri|)2 +

Xi=1

Xi6=j

Xi6=j

Xi6=j

Xi=1

r2
i η2

i +

r2
i η2

i +

r2
i η2

i +

≤

k

k

k

k

k

k

=

k

k

k

rirj (ηi + ηj )hAwi, wji

rirj (ηi + ηj )hAwi, wji

The third equality follows from the observation in Eqn.
(3.9). The ﬁrst inequality follows from apply-
ing Cauchy-Schwarz. The ﬁnal line follows from the formulation of the ǫi’s and the observation that
hAwi, wji = h(A − P )wi, wji for i 6= j.
By incorporating the term −αη, we can further upper bound σ↑k(A) in Eqn. (3.10) by the quantity

i=1 ǫi|ri|(cid:17)2
(cid:16)Pk
+Pk
(cid:16)Pk

i=1 r2

i ηi(ηi − α) +Pk
i (ηi − α)(cid:17) +(cid:16)Pk

i=1 r2

i6=j rirj (ηi + ηj − α)(w⊤j (A − P )wi)
i6=j rirj(w⊤j (A − P )wi)(cid:17)

.

Next, we consider two cases for the term Pk
Case 1: Suppose Pk
i6=j rirj (w⊤j (A − P )wi) ≥ 0. Then the bound can be weakened slightly to exclude the
second (positive) term in the denominator. With probability at least 1 − k2 × 2 exp(−(t − 1)2), the ﬁrst

i6=j rirj(w⊤j (A − P )wi).

THE KATO-TEMPLE INEQUALITY AND EIGENVALUE CONCENTRATION

7

term in the denominator is bounded below by σ↑1(P ) − t − α (by the same method as in Eqn. (3.7)) and
simultaneously the third sum in the numerator is bounded above by 2k2t(σ↑k(P ) + t − α/2). Hence,

σ↑k(A) ≤ (cid:16)Pk

i=1 ǫi|ri|(cid:17)2
(cid:16)Pk

i=1 r2

+(cid:16)Pk
i (ηi − α)(cid:17)

i=1 r2

i ηi(ηi − α)(cid:17)

+

2k2t(σ↑k(P ) + t − α/2)

σ↑1 (P ) − t − α

.

At this point, the material diﬀerence between our bound and the corresponding expression in Kato’s original
proof is the presence of the right-most term in the above inequality. Kato next introduces new coeﬃcients
which eliminate the need for each ri; together with the quantities α, ǫi, and ηi (each for all i ∈ [k]), he
constructs an equation whose roots relate to upper-bounds for σ↑k(A) (see Section 3 in [5], Eqns. 22–30). By
employing the same constructive approach, we obtain the following inequality, followed by a further bound
to complete Case 1.

Case 2: Suppose Pk

σ↑k(A) ≤ ηk +

k

Xi=1

ǫ2
i

ηi − α

+

2k2t(σ↑k(P ) + t − α/2)

σ↑1 (P ) − t − α

≤ σ↑k(P ) + t +

kkA − Pk2
σ↑1(P ) − t − α

2

2k2t(σ↑k(P ) + t − α/2)

.

+

σ↑1(P ) − t − α

i6=j rirj (w⊤j (A − P )wi) < 0. Then, by the same methods as in Case 1, we have
σ↑k(A) ≤ (cid:16)Pk
= (cid:16)Pk

i ηi(ηi − α)(cid:17)
i ηi(ηi − α − k2t)(cid:17)

2k2t(σ↑k(P ) + t − α/2)
σ↑1(P ) − (k2 + 1)t − α

i=1 r2

i=1 r2

+

i=1 r2

+(cid:16)Pk
i (ηi − α) − k2t(cid:17)
+(cid:16)Pk
i (ηi − α − k2t)(cid:17)
i ηi(cid:17)

i=1 ǫi|ri|(cid:17)2
(cid:16)Pk
i=1 ǫi|ri|(cid:17)2
(cid:16)Pk
k2t(cid:16)Pk
i (ηi − α − k2t)(cid:17)

i=1 r2

i=1 r2

i=1 r2

+

2k2t(σ↑k(P ) + t − α/2)
σ↑1 (P ) − (k2 + 1)t − α

+

(cid:16)Pk

with probability at least 1 − k2 × 2 exp(−(t − 1)2). Again, we now appeal to the root-based argument from
[5] and adapt it accordingly to handle the modiﬁed (from the original in [5]) ﬁrst term along with the two
additional terms. These considerations yield the ﬁrst inequality below, and the second inequality corresponds
to further upper-bounding the resulting quantity.

σ↑k(A) ≤ ηk +  k
Xi=1
k2t(cid:16)Pk
(cid:16)Pk
≤ σ↑k(P ) + t +

i=1 r2

+

ǫ2
i

ηi − α − k2t!

i=1 r2

i (ηi − α − k2t)(cid:17)

i ηi(cid:17)
kkA − Pk2

+

2

2k2t(σ↑k(P ) + t − α/2)
σ↑1(P ) − (k2 + 1)t − α

σ↑1(P ) − (k2 + 1)t − α

+

k2t(σ↑k(P ) + t)

σ↑1 (P ) − (k2 + 1)t − α

+

2k2t(σ↑k(P ) + t − α/2)
σ↑1(P ) − (k2 + 1)t − α

Now bound the maximum of the bounds in Case 1 and Case 2 to yield

(3.11)

σ↑k(A) ≤ σ↑k(P ) + t +

kkA − Pk2

2 + 3k2tσ↑k(P ) + 3k2t2 − k2tα
σ↑1(P ) − (k2 + 1)t − α

.

Lastly, replace t with t/(k2 + 1) which in particular implies that (1/2)t ≤ k2t ≤ t and 3k2t2 ≤ t2 for any
positive integer k, thereby completing the proof for the upper bound.

(cid:3)

8

JOSHUA CAPE, MINH TANG, AND CAREY E. PRIEBE

4. Discussion and Extensions

For collections of top eigenvalues of A and P which have the same order and are suﬃciently separated
from the remainder of their respective spectra, our bounds are tighter than existing results in the random
graph literature. For example, in the RDPG model, under suitable conditions, the top eigenvalues of A and
P are known to have the same order along with kA − Pk2
2 (see for example [2]), so for n suﬃciently large
and isolating these eigenvalues yields by Theorem 3.5 that

d

|σ↑i (A) − σ↑i (P )| = OPr(1).

Xi=1

In this (RDPG) setting, our work diﬀers from [2] in that we do not use the second moment matrix
considered therein as ∆ := E[XiX⊤i ] = diag{λ1, . . . , λd′} with λ1 > ··· > λd′ > 0, where Xi ∼iid F
according to some distribution F on X ⊂ Rd′
with hx, x′i ∈ [0, 1] for all x, x′ ∈ X where X := [X1, . . . , Xn]⊤
and P = XX⊤ ∈ [0, 1]n×n. In this model, conditional on P and assuming δ > 0 where δ denotes the smaller
of the minimum eigengap and d′-th eigenvalue of ∆, then for d′ ≥ d and the interval (0,∞), Lemma 2 in [2]
yields

(4.1)

(4.2)

d′

Xi=1

|σ↑i (A) − σ↑i (P )|2 = OPr(cid:0)δ−2 × log n(cid:1)2

.

This bound holds for the largest eigenvalues of A and P but makes no assumption on their relative order;
we emphasize that d′ ≥ d. When the top eigenvalues have the same order, however, this bound is weaker
than Eqn. (4.1) since it introduces the quantities ∆ and δ while making additional assumptions.

Remark 4.1. Using the same ideas and techniques as above, Theorem 3.5 may be altered to hold more
generally for (ﬁnite) intervals of the form (α, β) where β < ∞.
In diﬀerent settings where the top d
eigenvalues are not necessarily of the same order, our upper bound is not guaranteed to be tight given its
dependence upon the ratio of the largest local eigenvalue to the smallest local eigenvalue; on the other hand,
if we restrict our focus to a subset of the top d eigenvalues in a particular interval, say, the (d − 1)th and
dth largest eigenvalues, then by further restricting the range to a smaller interval containing only these
eigenvalues, our results may yield tighter bounds.

Remark 4.2. Our results further extend to the more general (than RDPG) setting of latent position random
graphs [9] in which the signal matrix, depending upon a general positive deﬁnite kernel, κ (viewed as an
integral operator), is not necessarily of ﬁnite ﬁxed rank as n increases. Here P := (κ(Xi, Xj))1≤i,j≤n where
Xi and Xj are latent positions with Xi, Xj ∼iid F . In the dense case, suppose we ﬁx integers i and j such
that (globally) the following ordering holds for the eigenvalues of the integral operator induced by κ i.e. the
eigenvalues of the operator (T f )(x) :=R κ(x, y)f (y)F (dy)
Loosely speaking, then for n large the corresponding eigenvalues of P are roughly of order n× σ↓i (κ) through
n × σ↓j (κ). Here kA − Pk2 grows at most as n1/2 with high probability, and an interval (with endpoints
depending upon n) may be taken to be (α(n), β(n)) with endpoints of order roughly n × σ↓j (κ) and n ×
σ↓i (κ), respectively. In this context, our results yield tight concentration of the eigenvalues of A around the
corresponding eigenvalues of P in the prescribed interval. Analogous results hold for “semi-sparse graphs”
i.e. those for which average vertex degree is at least on the order of ln4 n.

σ↓i (κ) > σ↓i+1(κ) ≥ ··· ≥ σ↓j−1(κ) > σ↓j (κ).

Our proof hinges upon several important observations, one being the probabilistic tail behavior of the
matrix A − P (see Proposition 3.1). Generalizing this behavior to a broader random matrix setting is
straightforward; in fact, a broad class of models for random matrices is encapsulated via the following
property as noted in [8].
Deﬁnition 4.3. An m × n random (error) matrix E is called (C, c, γ)-concentrated if for all unit vectors
u ∈ Rm, v ∈ Rn, and for every t > 0,
(4.3)

Pr[|u⊤Ev| ≥ t] ≤ C exp(−ctγ).

THE KATO-TEMPLE INEQUALITY AND EIGENVALUE CONCENTRATION

9

In particular, the inhomogeneous random graph setting corresponds to the case of (C, c, γ)-concentrated
where m = n, t > 1, C = γ = 2, c = 1, and where the only vectors necessary for consideration are the
normalized eigenvectors of P . Additionally, we observe that 2 exp(−t2) ≤ 2 exp(−(t − 1)2) for t > 1.
Theorem 3.5 can be generalized in the following way. We frame this generalization in the context of a
signal-noise model with tail probability bounds. Replace A with B + E (thought of as the observed data)
and P with B (the signal), so A− P therefore becomes E (the additive error). A similar setting is considered
in [8] but which further assumes that the rank of B is known and ﬁnite. We emphasize that the following
generalization is in terms of the singular values of B and B + E and equivalently (by Remark 2.1) in terms
of the eigenvalues of ˜B and ˜B + ˜E.
Theorem 4.4. For matrices B, E ∈ Rm×n, suppose E is (C, c, γ)-concentrated with positive constants C, c,
and γ. Let α ∈ R>0 be such that the interval (α,∞) contains precisely the d largest positive singular values
of B and B + E (equivalently, the d largest eigenvalues in magnitude of ˜B and ˜B + ˜E). Fix k ∈ [d]. Then
for any t > 0,

(4.4)
with probability at least 1 − (d − k + 1)2 × 2C exp(−c(
Also,

σ↑k( ˜B + ˜E) ≥ σ↑k( ˜B) − t

(4.5)

σ↑k( ˜B + ˜E) ≤ σ↑k( ˜B) + t +

t

2((d−k+1)2+1) )γ).
2 + (3σ↑k( ˜B) + t − (1/2)α)t

kk ˜Ek2

σ↑1( ˜B) − t − α

t

2(k2+1) )γ).

with probability at least 1 − k2 × 2C exp(−c(
Proof. The proof follows essentially mutatis mutandis as in Theorem 3.5. In particular, one only needs to
consider (in terms of the previous notation) the matrices ˜A and ˜P with ˜wi denoting the eigenvector of ˜P
corresponding to its eigenvalue σ↑i ( ˜P ). The observable diﬀerences in the probability statements correspond
Instead, since A − P is (C, c, γ)-concentrated, it is
to the fact that Remark 3.4 is no longer in eﬀect.
straightforward to check that then ˜A − ˜P is (2C, c/2γ, γ)-concentrated (see Lemma 21 in [8]) and that
kA − Pk2 = k ˜A − ˜Pk2.

(cid:3)

References

1. Noga Alon, Michael Krivelevich, and Van Vu, On the concentration of eigenvalues of random symmetric matrices, Israel

Journal of Mathematics 131 (2002), no. 1, 259–267.

2. Avanti Athreya, Carey E. Priebe, Minh Tang, Vince Lyzinski, David J. Marchette, and Daniel L. Sussman, A limit theorem

for scaled eigenvectors of random dot product graphs, Sankhya A (2015), 1–18.

3. Roger A. Horn and Charles R. Johnson, Matrix analysis, Cambridge University Press, 2012.
4. Evans M. Harrell II, Generalizations of temple’s inequality, Proceedings of the American Mathematical Society 69 (1978),

no. 2, 271–276.

5. Tosio Kato, On the upper and lower bounds of eigenvalues, Physical Review Letters 77 (1950), 334–339.
6. Linyuan Lu and Xing Peng, Spectra of edge-independent random graphs, The Electronic Journal of Combinatorics 20

(2013), no. 4, 1–18.

7. Roberto I. Oliveira, Concentration of the adjacency matrix and of the laplacian in random graphs with independent edges,

arXiv:0911.0600 (2010).

8. Sean O’Rourke, Van Vu, and Ke Wang, Random perturbation of

low rank matrices:

Improving classical bounds,

arXiv:1311.2657 (2013).

9. Minh Tang, Daniel L. Sussman, and Carey E. Priebe, Universally consistent vertex classiﬁcation for latent positions graphs,

The Annals of Statistics 41 (2013), no. 3, 1406–1430.

10. Stephen Young and Edward Scheinerman, Random dot product graph models for social networks, Algorithms and Models

for the Web-Graph (2007), 138–149.

11. Xiao Zhang, Raj Rao Nadakuditi, and M. E. J. Newman, Spectra of random graphs with community structure and arbitrary

degrees, Phys. Rev. E 89 (2014), 042816.
E-mail address: joshua.cape@jhu.edu

E-mail address: mtang10@jhu.edu

E-mail address: cep@jhu.edu

