6
1
0
2

 
r
a

 

M
8
1

 
 
]

.

A
N
h
t
a
m

[
 
 

1
v
3
7
7
5
0

.

3
0
6
1
:
v
i
X
r
a

A FRAMEWORK FOR STRUCTURED LINEARIZATIONS OF

MATRIX POLYNOMIALS IN VARIOUS BASES

LEONARDO ROBOL, RAF VANDEBRIL, AND PAUL VAN DOOREN

Abstract. We present a framework for the construction of linearizations for
scalar and matrix polynomials based on dual bases which, in the case of or-
thogonal polynomials, can be described by the associated recurrence relations.
The framework provides an extension of the classical linearization theory for
polynomials expressed in non-monomial bases and allows to represent polyno-
mials expressed in product families, that is as a linear combination of elements
of the form φi(λ)ψj (λ), where {φi(λ)} and {ψj (λ)} can either be polynomial
bases or polynomial families which satisfy some mild assumptions.

We show that this general construction can be used for many diﬀerent
purposes. Among them, we show how to linearize sums of polynomials and
rational functions expressed in diﬀerent bases. As an example, this allows
to look for intersections of functions interpolated on diﬀerent nodes without
converting them to the same basis.

We then provide some constructions for structured linearizations for ⋆-even
and ⋆-palindromic matrix polynomials. The extensions of these constructions
to ⋆-odd and ⋆-antipalindromic of odd degree is discussed and follows imme-
diately from the previous results.

1. Introduction

In recent years much interest has been devoted to ﬁnding linearizations for poly-
nomials and matrix polynomials. The Frobenius linearization, i.e., the classical
companion, has been the de-facto standard in polynomial eigenvalue problems and
polynomial rootﬁnding for a long time [19, 21]. Nevertheless, recently much work
has been put into developing other families of linearizations. Among these some lin-
earizations preserve spectral symmetries available in the original problem [23,25,27],
others linearize matrix polynomials formulated in non-monomial bases [1, 9] and
also some variations are based on an idea of Fiedler about decomposing companion
matrices into products of simple factors [2, 10, 11, 17].

In this work we take as inspiration the results of Dopico, P´erez, Lawrence and
Van Dooren [24] that characterize the structure of some permuted Fiedler lineariza-
tions by using dual minimal bases [18]. We extend the results in a way that allows

(Leonardo Robol) Department of Computer Science, KU Leuven, Celestijnenlaan

200A, 3001 Heverlee, Belgium.

(Leonardo Robol) Department of Computer Science, KU Leuven, Celestijnenlaan

200A, 3001 Heverlee, Belgium.

(Paul Van Dooren) ICTEAM, Universite Catholique de Louvain, Batiment Euler, Av-

enue G. Lemaitre 4, B-1348 Louvain-la Neuve, Belgium.

E-mail addresses: leonardo.robol@cs.kuleuven.be , raf.vandebril@cs.kuleuven.be ,

paul.vandooren@uclouvain.be.

Key words and phrases. Matrix polynomials, Rational functions, Non-monomial bases, Palin-
dromic matrix polynomials, Even matrix polynomials, Strong linearizations, Dual minimal bases.

1

2

LINEARIZATIONS OF MATRIX POLYNOMIALS IN VARIOUS BASES

us to deal with many more formulations, and we use it to derive numerous diﬀerent
linearizations. We also use these examples to prove the eﬀectiveness of this result
as a tool for constructing structured linearizations (thus preserving spectral sym-
metries in the spirit of the works cited above) and also linearizations for sums of
polynomials and rational functions.

In particular, we consider the rootﬁnding case of polynomials that are expressed
as linear combination of elements in a so-called product family; the most com-
mon case where this can be applied is when considering two diﬀerent polynomial
bases {φi} and {ψj} and representing polynomials as sums of objects of the form
φi(λ)ψj (λ). This apparently artiﬁcial construction has, however, many interesting
applications, such as ﬁnding intersections of polynomials and rational functions
deﬁned in diﬀerent bases.

In Section 3 we give a formal deﬁnition of what we call a product family of
polynomials, denoted by φ ⊗ ψ. We deﬁne the vector πk,φ(λ) to be the one with the
elements of the family as entries and we show that πk,φ⊗ψ(λ) is given by πk,φ(λ) ⊗
πk,ψ(λ). We present a theorem that allows to linearize every polynomial written
as a linear combination of elements in a product family, and we also generalize the
construction to the product of more than two families in Section 3.1. We consider
a certain class of dual polynomial bases (with the notation of the classical work by
Forney [18]) of a polynomial vector πk,φ(λ), which we identify with linear matrix
polynomials Lk,φ(λ) such that Lk,φ(λ)πk,φ(λ) = 0, which will be used as a tool to
build linearizations. At the end of the section we introduce an explicit construction
for linearizing polynomial families arising from orthogonal and interpolation bases.
We cover the case of every polynomial basis endowed with a recurrence relation, and
we provide explicit constructions for the Lagrange, Newton, Hermite and Bernstein
cases. We describe the dual bases for all these cases and, as shown by Theorem 15,
they are the only ingredient required to build the linearizations.

The rest of the paper deals with the problem of exploiting this freedom of choice

to obtain many interesting results.

In Section 4 we show how to linearize the sum of two scalar polynomials or
rational functions expressed in diﬀerent bases, without the need of an explicit basis
conversion. This can have important applications in the cases where interpolation
polynomials are obtained from experimental data (that cannot be resampled - so
there is no choice for the interpolation basis) or in cases where an explicit change
of basis is badly conditioned.

Inﬁnite eigenvalues may appear when linearizing the sum of polynomials. We
report numerical experiments that show that they do not aﬀect the numerical ro-
bustness of the approach in many cases. Moreover, we show that for the rational
case, under mild hypotheses, it is possible to construct strong linearizations which
do not have spurious inﬁnite eigenvalues.

In Section 5 we turn our attention to preserving spectral symmetries and we
provide explicit constructions for linearizations of ⋆-even/odd and ⋆-palindromic
matrix polynomials. We show that a careful choice of the dual bases for use in
Theorem 15 yields linearizations with the same spectral symmetries of the original
matrix polynomial.

Finally, in Section 6, we describe a numerical approach to deﬂate the inﬁnite
eigenvalues that are present in some of the constructions, based on the staircase
algorithm of Van Dooren [28].

LINEARIZATIONS OF MATRIX POLYNOMIALS IN VARIOUS BASES

3

In Section 7 we draw some conclusions and we propose some possible development

for future research.

2. A general framework to build linearizations

2.1. Notation. In the following we will often work with the vector space of poly-
nomials of degree at most k on a ﬁeld F, denoted as Fk[λ].

In the study of strong linearizations is also important to consider the rev op-
erator, which reverses the coeﬃcients of the polynomial when represented in the
monomial basis.

Deﬁnition 1. Given a non zero matrix polynomial P (λ) = Pn

i=0 Piλi we deﬁne
its degree as the largest integer i > 0 such that Pi 6= 0, that is the maximum of all
the degrees of the entries of P (λ). We denote it by deg P (λ).

Deﬁnition 2. Given a matrix polynomial P (λ), its reversed polynomial, denoted
by rev P (λ), is deﬁned by rev P (λ) := xdeg P (λ)P (λ−1).

Intuitively, a linearization for a matrix polynomial P (λ) is a linear matrix poly-
nomial L(λ) such that L(λ) is singular only when P (λ) is. However, this is not
suﬃcient in most cases since there is also the need to match eigenvectors and par-
tial multiplicities, so the deﬁnition has to be a little more involved. We refer to the
work of De Ter´an, Dopico and Mackey [12] for a complete overview of the subject.

Deﬁnition 3. A matrix polynomial E(λ) is said to be unimodular if it is invertible
in the ring of matrix polynomials or, equivalently, if det E(λ) is a non zero constant
of the ﬁeld.

Deﬁnition 4 (Extended unimodular equivalence). Let P (λ) and Q(λ) be matrix
polynomials. We say that they are extended unimodularly equivalent if there exist
positive integers r, s and two unimodular matrix polynomials E(λ) and F (λ) of
appropriate dimensions such that

E(λ)(cid:20)Ir

P (λ)(cid:21) F (λ) =(cid:20)Is

Q(λ)(cid:21) .

Deﬁnition 5 (Linearization). A linear matrix polynomial L(λ) is a linearization
for a matrix polynomial P (λ) if P (λ) is extended unimodularly equivalent to L(λ).

In order to preserve the complete eigenstructure of a matrix polynomial, it is
of interest to maintain also the inﬁnite eigenvalues, which are deﬁned as the zero
eigenvalues of the reversed polynomial. To achieve this we have to extend Deﬁni-
tion 5.

Deﬁnition 6 (Spectral equivalence). Two matrix polynomials P (λ) and Q(λ)
are spectrally equivalent if P (λ) is extended unimodularly equivalent to Q(λ) and
rev P (λ) is extended unimodularly equivalent to rev Q(λ).

Deﬁnition 7 (Strong linearization). A linear matrix polynomial L(λ) is said to be
a strong linearization for a matrix polynomial P (λ) if it is spectrally equivalent to
L(λ).

4

LINEARIZATIONS OF MATRIX POLYNOMIALS IN VARIOUS BASES

2.2. Working with product families of polynomials. The linearizations that
we build in this work concern polynomials expressed as linear combinations of
elements of a product family. Let us add more details about this concept.

With the term family of polynomials (or polynomial family) we mean any set
of elements in F[λ] indexed on a ﬁnite totally ordered set (I, 6). To denote these
objects we use the notation {φi(λ) | i ∈ I} or its more compressed form {φi(λ)} or
even {φi} whenever the index set I and the variable λ are clear from the context.
Often the set I will be a segment of the natural numbers or a subset of Nd endowed
with the lexicographical order, as in Deﬁnition 8.

An important example of such families are the polynomials φi(λ) forming a basis
for the polynomials of degree up to k. Another extension that deserves our attention
is the following.

Deﬁnition 8. Given two families of polynomials {φi} for i = 0, . . . , ǫ and {ψj} for
j = 0, . . . , η, we deﬁne the product family as the indexed set deﬁned by:

φ ⊗ ψ := {φi(λ)ψj (λ), i = 0, . . . , ǫ, j = 0, . . . , η}.

with the lexicographical order (so that (i, j) 6 (i′, j′) if either i < i′ or i = i′ and
j 6 j′).

We introduce some notation that will make it easier in the following to deal with
these product families and their use in linearizations. We use the symbol πk,φ(λ)
to denote the column vector

πk,φ(λ) :=


φk(λ)

...

φ0(λ)

.




We will often identify πk,φ(λ) with the family {φi | i = 0, . . . , k} since they are just
diﬀerent representations of the same mathematical object.

Notice that Deﬁnition 8 is easily extendable to the product of an arbitrary num-
ber of families. In this case we always consider the lexicographical order on the
new family, which is particular convenient because then we have

πk,φ(1) ⊗...⊗φ(j) (λ) = πǫ1,φ(1) (λ) ⊗ . . . ⊗ πǫj ,φ(j) (λ).

Remark 9. Whenever the family {φi} is a basis for the polynomials of degree at
most k, every polynomial p(λ) ∈ Fk[λ] can be expressed as

k

p(λ) =

ajφj(λ).

Xj=0

In particular, the scalar product with πk,φ(λ) is a linear isomorphism between Fk+1
and the vector space of polynomials of degree at most k. We have

Γφ : Fk+1 −→

Fk[λ]

a

7−→ Γφ(a) := aT πk,φ(λ)

.

With the above notation Γ−1

φ (p(λ)) is the vector of coordinates of p(λ) expressed

in the basis {φi}.

We recall the following deﬁnitions that can be found in [18].

Deﬁnition 10. A matrix polynomial G(λ) ∈ F[λ]k×n is a polynomial basis if its
rows are a basis for a subspace of the vector space of polynomial n-tuples.

LINEARIZATIONS OF MATRIX POLYNOMIALS IN VARIOUS BASES

5

Deﬁnition 11 (Dual basis). Two polynomial bases G(λ) ∈ Fk×n and H(λ) ∈ Fj×n
are dual if G(λ)H(λ)T = 0 and j + k = n.

We are interested in a particular subclass of dual bases which are relevant for

our construction. We will call them dual linear bases.

Deﬁnition 12 (Full row-rank linear dual basis). We say that a k × (k + 1) matrix
pencil Lk,φ(λ) is a full row-rank linear dual basis to πk,φ(λ) (or, analogously, for a
polynomial family {φi}) if Lk,φ(λ)πk,φ(λ) = 0, and Lk,φ(λ) has full row rank for
any λ ∈ F.

Often we just say that Lk,φ(λ) is a full row-rank linear dual basis, meaning
that it is dual to πk,φ(λ). Since the family {φi} is reported in the notation that
we use for Lk,φ(λ), there is no risk of ambiguity.
In the context of developing
strong linearizations, we also give the following deﬁnition (which can again be
found in [18]):
Deﬁnition 13 (Minimal basis). A basis G(λ) ∈ Fk×n is said to be minimal if the
sum of degrees of its rows is minimal among all the possible bases of the vector
space that they span.

We are particularly interested in dual minimal bases, that is bases that are both
minimal and dual bases. In [18] it is shown that this is equivalent to asking that
G(λ) and H(λ) are of full row rank for any λ ∈ F and the same holds for their
leading coeﬃcient (which is analogous to asking that they are of full row rank for
every λ ∈ F).

Remark 14. In the rest of the paper we will often consider full row-rank linear dual
bases (which will sometimes be minimal) related to polynomial families {φi}. In
order to make the exposition simpler we will call these bases dual, without adding
the term linear and full row-rank. However, it must be noted that these are a
very particular kind of dual bases and most of the results could not hold in a more
general context.

2.3. Building linearizations using product families. Let P (λ) be a polyno-
mial (or a matrix polynomial) expressed as a linear combination of elements of a
product family φ ⊗ ψ. In this section we provide a way of linearizing it starting
from the coeﬃcients of this representation. In order to obtain this construction we
rely on the following extension of the main result of [24].
Theorem 15. Let Lk,φ(λ), Lk,ψ(λ) ∈ Ck×(k+1)[λ] be dual
linear bases for two
polynomial families {φi} and {ψi}. Assume that the elements of each polyno-
mial family have no common divisor, that is there exists a vector wk,⋆ such that
πk,⋆(λ)T wk,⋆ = 1 for ⋆ ∈ {φ, ψ}. Then the matrix polynomial

L(λ) :=(cid:20) λM1 + M0 Lη,φ(λ)T ⊗ I

Lǫ,ψ(λ) ⊗ I

0

(cid:21)

is a linearization for P (λ) = (πη,φ(λ) ⊗ I)T (λM1 + M0)(πǫ,ψ(λ) ⊗ I), which is a
polynomial expressed in the product family φ ⊗ ψ. Moreover, this linearization is
strong1 if the dual bases are minimal.

1Notice that the linearization is guaranteed to be strong for the matrix polynomial formally
deﬁned by (πη,φ(λ) ⊗ I)T (λM1 + M0)(πǫ,ψ(λ) ⊗ I). In particular, this expression might provide a
matrix polynomial with leading coeﬃcient zero, but we still need to consider that polynomial and
not the one with the leading zero coeﬃcients removed, otherwise the strongness might be lost.

6

LINEARIZATIONS OF MATRIX POLYNOMIALS IN VARIOUS BASES

Proof. We mainly follow the proof given in [24]. Recall that we can ﬁnd bk,⋆ such
that the matrix polynomial

k,⋆ (cid:21)
Sk,⋆(λ) :=(cid:20)Lk,⋆(λ)

bT

is unimodular [4], and we know that Sk,⋆(λ)πk,⋆(λ) = αk,⋆(λ)ek+1. This can be
rewritten as πk,⋆(λ) = αk,⋆(λ)S−1
k,⋆(λ)ek+1. Since the entries of πk,⋆(λ) do not have
any common factor we conclude that αk,⋆(λ) is a nonzero constant (and so we can
drop the dependency on λ). We remark that rescaling the vector bk,⋆ by a non-
zero constant preserves the unimodularity of Sk,⋆(λ) (since it is equivalent to left
multiplying by an invertible diagonal matrix). For this reason we can assume that
bk,⋆ is chosen so that Sk,⋆(λ)πk,⋆(λ) = ek+1. We deﬁne Vk,⋆(λ) := Sk,⋆(λ)−1 so
that Vk,⋆(λ)ek+1 = πk,⋆(λ). With these hypotheses we have that

Lk,⋆(λ)Vk,⋆(λ) =(cid:2)I

0(cid:3) ,

V T
k,⋆(λ)LT

k,⋆(λ) =(cid:20)I
0(cid:21)

Now observe that the matrix polynomial L(λ) can be transformed by means of a
unimodular transformation in the following way:

η,φ(λ) ⊗ I X(λ)

(cid:20)V T

0

I (cid:21)(cid:20) λM1 + M0 LT

Lǫ,ψ(λ) ⊗ I

η,φ(λ) ⊗ I

0

(cid:21)(cid:20)Vǫ,ψ(λ) ⊗ I

Y (λ)

0

I(cid:21) =: ˜P (λ)

where ˜P (λ) can be chosen as follows:

˜P (λ) :=


0

0
0 P (λ)
I

0

I
0

0
 , P (λ) = (πη,φ(λ) ⊗ I)T (λM1 + M0)(πǫ,ψ(λ) ⊗ I).

The matrices X(λ) and Y (λ) can be chosen in order to put zeros in the top-left
corner almost everywhere, and the only non zero block P (λ) can be retrieved by
knowing Vk,⋆(λ)ek, as usual for ⋆ ∈ {φ, ψ}.

We now check that the linearization is strong. Similarly to the previous step, we

can ﬁnd a constant vector uk,⋆ such that

˜Sk,⋆(λ) =(cid:20)

uk,⋆

rev Lk,⋆(λ)(cid:21) ,

⋆ ∈ {φ, ψ}

and ˜Sk,⋆(λ) rev π(λ) = ˜αk,⋆(λ)e1. Since the entries of πk,⋆(λ) do not share any
common factor, we get that ˜αk,⋆ is a nonzero constant. As in the previous case,
applying a diagonal scaling does not change the unimodularity so we can assume
that ˜αk,⋆ = 1. Deﬁne Wk,⋆(λ) = ˜Sk,⋆(λ)−1 so that Wk,⋆(λ)e1 = rev πk,⋆(λ).

We can perform another unimodular transformation on the reversed polynomial.

Let A(λ) be deﬁned as follows:

η,φ(λ) ⊗ I

(cid:20)W T

0

ˆX(λ)

I (cid:21)(cid:20) M1 + λM0

rev Lǫ,ψ(λ) ⊗ I

rev LT

η,φ(λ) ⊗ I

0

(cid:21)(cid:20)Wǫ,ψ(λ) ⊗ I

ˆY (λ)

0

I(cid:21) .

Notice that rev Lk,⋆(λ)Wk,⋆(λ) = [0 I] so we can write

A(λ) =


A1,1(λ)

0
0

0
0
I

0
I

0


LINEARIZATIONS OF MATRIX POLYNOMIALS IN VARIOUS BASES

7

by appropriately choosing ˆX(λ) and ˆY (λ). In particular we have

A1,1(λ) = (rev πη,φ(λ) ⊗ I)T (M1 + λM0)(rev πǫ,ψ(λ) ⊗ I) = rev P (λ)

if the degree of P (λ) is maximum (i.e., if the coeﬃcient that goes in front of the
maximum degree term in the previous relation is not zero).
(cid:3)

3. Further extensions of linearizations

In this section we will show some concrete examples for the choice of the product
families φ ⊗ ψ and some related applications. In particular, we show that this ap-
parently abstract way of rewriting polynomials is useful in many diﬀerent situations
in order to solve some problems directly, without unneeded change of bases.

The most natural problem to consider is the case where both {φi} and {ψi} are

polynomial bases for the polynomials of degree up to ǫ and η, respectively.

We recall that in this case the product family φ ⊗ ψ is, in general, not a basis
since it contains too many vectors for the dimension of the vector space that it
needs to span. However, this extra ﬂexibility in the representation will be useful in
many situations.

3.1. An extension to more than two bases. Given the above formulation for
a linearization of a polynomial expressed in a product family, it is natural to ask if
the framework can be extended to cover more than two bases, that is to product
families of the form

φ(1) ⊗ . . . ⊗ φ(j) := {φ(1)
i1

. . . φ(j)
ij

| is = 0, . . . , ks, s = 1, . . . , j}

where {φ(s)

i

| i = 0, . . . , ks} are families of polynomials for s = 1, . . . , j.

We show that there is no need to extend Theorem 15, but it is suﬃcient to
construct two appropriate dual bases Lǫ,φ(λ) and Lη,ψ(λ) to deal with this case.
We only need to prove that the hypotheses of Theorem 15 are satisﬁed.

Deﬁnition 16. Let Lǫ,φ(λ) and Lη,ψ(λ) be two dual bases for two families {φi}
and {ψi}. Let w be a constant vector such that wT πη,ψ(λ) is a nonzero constant,
and A an invertible matrix. We say that the matrix

Lk,φ⊗ψ(λ) =(cid:20) A ⊗ Lη,ψ(λ)
Lǫ,φ(λ) ⊗ wT(cid:21) ,

k := (ǫ + 1)(η + 1) − 1,

is a product dual basis of Lǫ,φ(λ) and Lη,ψ(λ). We denote it as Lǫ,φ(λ)×Lη,ψ(λ).

Notice that, since the product dual basis is not unique, the previous notation
actually denotes a family of such matrices so we should be writing Lk,φ⊗ψ(λ) ∈
Lǫ,φ(λ)×Lη,ψ(λ). However, in the following we will often write, by slight abuse of
notation, Lk,φ⊗ψ(λ) = Lǫ,φ(λ)×Lη,ψ(λ).

The above deﬁnition can be extended easily to a product of arbitrary families,

by means of the following.

Deﬁnition 17. We say that, for any families of polynomials {φ(1)
i }, the
matrix Lk,φ(1) ⊗...⊗φ(j) (λ) is a product dual basis for these families, and we denote
it as Lǫ1,φ(1) × . . . ×Lǫj ,φ(j) (λ), where

i }, . . . , {φ(j)

Lk,φ(1)⊗...⊗φ(j) (λ) = (Lǫ1,φ(1) (λ)× . . . ×Lǫj−1,φ(j−1) (λ))×Lφ(j) (λ).

8

LINEARIZATIONS OF MATRIX POLYNOMIALS IN VARIOUS BASES

Notice that the above formula provides a recursive manner for computing such
product dual bases. In the next lemma we show that they can be used to construct
linearizations in the spirit of Theorem 15.

Lemma 18. Let Lk,φ⊗ψ(λ) = Lǫ,φ(λ)×Lη,ψ(λ) be a product dual basis. Then

(i) If πk,φ⊗ψ(λ) is the vector containing the elements of the product family φ ⊗ ψ,

then Lk,φ⊗ψ(λ)πk,φ⊗ψ(λ) = 0.

(ii) Lk,φ⊗ψ(λ) is a rectangular matrix with full row rank and size k × (k + 1) for

all values of λ.

Proof. We ﬁrst check condition (i). Notice that we have πk,φ⊗ψ(λ) = πǫ,φ(λ) ⊗
πη,ψ(λ), according to the ordering speciﬁed in Deﬁnition 8. For this reason we can
write

Lk,φ⊗ψ(λ)πk,φ⊗ψ(λ) =(cid:20) Aπǫ,φ(λ) ⊗ Lη,ψ(λ)πη,ψ(λ)

Lǫ,φ(λ)πǫ,φ(λ) ⊗ wT πη,ψ(λ)(cid:21) = 0.

The number of rows and columns can be veriﬁed by direct inspection in Deﬁni-
tion 16. Concerning condition (ii) we shall check that, for any λ, the only vectors
in the right kernel of Lk,φ⊗ψ(λ) are multiples of πk,φ⊗ψ(λ). Let v(λ) be such a
vector, so that Lk,φ⊗ψ(λ)v(λ) = 0. We can partition v(λ) = [v0(λ) . . . vǫ(λ)]T
according to the Kronecker structure of Lk,φ⊗ψ(λ) so, recalling that A is invertible,
we have

Lk,φ⊗ψ(λ)v(λ) = 0 ⇐⇒ (Lη,ψ(λ)vj (λ) = 0

(Lǫ,φ(λ) ⊗ wT )v(λ) = 0

j = 0, . . . , ǫ.

The ﬁrst relation tells us that vj(λ) = αj(λ)πη,ψ(λ), due to Lη,ψ(λ) being of full
row rank. If we set α(λ) = [α0(λ) . . . αǫ(λ)]T we have v(λ) = α(λ) ⊗ πη,ψ(λ), so
that the last equation becomes Lǫ,φ(λ)α(λ)⊗ wT πη,ψ(λ) = 0. Since wT πη,ψ(λ) 6= 0,
the only solution is given by α(λ) = πǫ,φ(λ).
(cid:3)

Remark 19. The proof of Lemma 18 shows that this construction is not the only
possible one. As an immediate example, we could have deﬁned Lǫ,φ(λ)×Lη,ψ(λ) to
be the matrix

˜Lk,φ⊗ψ(λ) =(cid:20)wT ⊗ Lη,ψ(λ)
Lǫ,φ(λ) ⊗ A (cid:21) ,

k := (ǫ + 1)(η + 1) − 1,

with the same hypotheses of Deﬁnition 16, and the proof would have been essentially
the same.

Remark 20. Lemma 18 justiﬁes the notation Lk,φ⊗ψ(λ) that we have used until
now, since the product dual basis is a dual basis for the product family φ ⊗ ψ.

Remark 21. Given the structure of the matrix Lǫ,φ×Lη,ψ(λ) that we have deﬁned
above it might be natural to ask if the more general matrix

M (λ) =(cid:20)A ⊗ Lη,ψ(λ)
Lǫ,φ(λ) ⊗ B(cid:21) ,

A ∈ Ck1×(η+1), B ∈ Ck2×(ǫ+1)

and such that k1η + k2ǫ = (η + 1)(ǫ + 1) − 1 can be a product dual basis when A
and B are of full row-rank. The answer is no unless k1 = ǫ + 1 or k2 = η + 1 and
so we are again back in the above two cases, as the next lemma shows.

LINEARIZATIONS OF MATRIX POLYNOMIALS IN VARIOUS BASES

9

Lemma 22. Let M (λ) be a matrix of the form

M (λ) =(cid:20)A ⊗ Lη,ψ(λ)
Lǫ,φ(λ) ⊗ B(cid:21) ,

A ∈ Ck1×(ǫ+1), B ∈ Ck2×(η+1)

with Lǫ,φ(λ) and Lη,ψ(λ) dual bases for πǫ,φ(λ) and πη,ψ(λ), A and B of full row
rank with k1 and k2 rows and ǫ + 1 and η + 1 columns, respectively. Then the right
kernel of M (λ) has dimension at least 1 + (ǫ + 1 − k1)(η + 1 − k2) for at least one
value of λ if either Aπǫ,φ(λ) 6≡ 0 or Bπη,ψ(λ) 6≡ 0.

Proof. Let SA = {v | Av = 0} and SB = {w | Bw = 0} be the right kernels of A
and B which have dimensions (ǫ + 1 − k1) and (η + 1 − k2), respectively. We have
that the span of πǫ,φ(λ) ⊗ πη,ψ(λ) and SA ⊗ SB are included in the kernel of M (λ).
Since Aπǫ,φ(λ) 6= 0 (or, analogously, Bπη,ψ(λ) 6= 0) for at least one λ we have that
the dimension of the union of these two spaces is at least 1 + (ǫ + 1 − k1)(η + 1 − k2),
which concludes the proof.
(cid:3)

Lemma 18 can be generalized to the product of more families of polynomials,

yielding the following.

Corollary 23. Let Lǫ1,φ(1) (λ)× . . . ×Lǫj ,φ(j) (λ) be a product dual basis of j dual
bases. Then it has full row rank and the only elements in its right kernel are
multiples of πk,φ(1) ⊗...⊗φ(j)(λ), independently of the construction chosen (either the
one of Lemma 18 or Remark 19)

Proof. Exploit the recursive deﬁnition of Lǫ1,φ(1) (λ)× . . . ×Lǫj ,φ(j) (λ) and apply
Lemma 18.
(cid:3)

The construction of these product dual bases allows us to formulate the following
result, which can be seen as an extension of Theorem 15 that makes it possible to
handle more than two bases at once.

i }, . . . , {φ(j)
Theorem 24. Let {φ(1)
mials. Then the matrix polynomial

i } and {ψ(1)

i }, . . . , {ψ(l)

i } be families of polyno-

L(λ) =(cid:20)

λM1 + M0

(Lǫ1,φ(1) × . . . ×Lǫj ,φ(j) (λ))T

Lη1,ψ(1) × . . . ×Lηl,ψ(l) (λ)

0

(cid:21)

is a linearization for the polynomial

P (λ) = (πǫ1,φ(1) (λ) ⊗ . . . ⊗ πǫj ,φ(j) (λ))T (λM1 + M0)(πη1 ,ψ(1) (λ) ⊗ . . . ⊗ πηl,ψ(l) (λ)).

Proof. Apply Theorem 15, whose hypothesis are satisﬁed because of Lemma 18 and
Corollary 23.
(cid:3)

Here is an example of the structure that the matrix Lǫ,φ×Lη,ψ(λ) can have in
a simple case. Let {φi} be the Chebyshev basis, while the family {ψi} is any
degree graded polynomial family. The matrix Lǫ,φ×Lη,ψ(λ) can be realized by the

10

LINEARIZATIONS OF MATRIX POLYNOMIALS IN VARIOUS BASES

following by choosing A = I and w = eη+1.

Lǫ,φ×Lη,ψ(λ) =

Lη,ψ(λ)

Lη,ψ(λ)

1

−2λ
. . .

. . .

. . .

1

1

Lη,ψ(λ)

Lη,ψ(λ)

. . .
−2λ

1

1
−λ





.





In order to give an example of how these variations behave in practice, we con-

sider what happens when taking the product basis of several monomial bases.

The monomial basis, in this setting, is rather special. In fact, the elements of
the product family of two monomial bases are of the form λiλj = λi+j and so they
correspond to elements of a (larger) monomial basis. However, notice that this
is not true in general, as for example when considering φi(λ) belonging to other
polynomial bases.

We can exploit this fact by rephrasing any polynomial expressed in the monomial
basis as a polynomial in the product family of two monomial bases (like in [24]) or
also in the product family of more bases, by using the framework above.

We show here some examples to illustrate some possibilities. Let p(λ) =P3

a degree 3 polynomial. Then we can obtain diﬀerent linearizations for it.

As a ﬁrst example, choosing {ψi} = {1, λ, λ2} and {φi} = {1} yields the classical

i=0 piλi

Frobenius form:

We can instead choose {ψi} = {φi} = {1, λ} and obtain a symmetric linearization
(this is only one of the possibilities for distributing the coeﬃcients):

But we can also choose to set {ψi} = {1, λ} ⊗ {1, λ} and {φi} = {1}, and we obtain:

L(λ) =


λp3 + p2

1

1
2 p1
−λ

1

1
2 p1

p0

1 −λ
−λ

.




One thing can be noticed immediately: we have increased the dimension of the
problem.
In fact the matrix L(λ) that we have used in the lower part has its
dimension increased by 1 since it represents λ two times. This has the consequence
that while L1,1,φ(λ) has full row rank its reversal does not, and so the linearization
is not strong. In fact, here we have a spurious inﬁnite eigenvalue.

L(λ) =


L(λ) =


λp3 + p2

1

p0

p1
−λ
1 −λ

λp3 + p2

1
2 p1
1

1
1
2 p1
p0 −λ
0
−λ


 .


 .

LINEARIZATIONS OF MATRIX POLYNOMIALS IN VARIOUS BASES

11

3.2. Handling orthogonal bases. This section is devoted to study the diﬀerent
structure of the dual basis Lk,φ(λ) when {φi} is a non-monomial basis.

We ﬁrst deal with the case where the basis {φi(λ)} is degree graded and satisﬁes

a three-terms recurrence relation of the form

(1)

αφj+1(λ) = (λ − β)φj (λ) − γφj−1(λ),

α 6= 0,

j > 0,

which includes all the orthogonal polynomials with a constant three term recurrence
(with the possible exception of the ﬁrst two elements of the basis). Notice, however,
that the result can be easily generalized to more general recurrences.

Lemma 25. Let {φi} be a degree graded basis satisfying the three-terms recurrence
relation (1). Then the linear matrix polynomial Lk,φ(λ) of size k × (k + 1) deﬁned
as follows

α (β − λ)

. . .

γ
. . .
α

Lk,φ(λ) :=


Lk,φ(λ)πk,φ(λ) = 0,

. . .

(β − λ)
φ0(λ) −φ1(λ)

γ

with πk,φ(λ) :=


φk(λ)

...

φ0(λ)






.

has full row rank for any λ ∈ F and is such that

Moreover, the leading coeﬃcient of Lk,φ(λ) has full row rank.

Proof. It is immediate to verify that Lk,φ(λ)πk,φ(λ) = 0, since each row of Lk,φ(λ)
but the last one is just the recurrence relation of (1) and the last one yields
φ0(λ)φ1(λ) − φ1(λ)φ0(λ) = 0.

We can then check that the matrix has full row rank. Notice that the ﬁrst k
columns of Lk,φ(λ) form an upper triangular matrix with determinant αk−1φ0(λ).
The basis is degree graded so φ0(λ) is an invertible constant and Lk,φ(λ) contains
an invertible matrix of order k × k, thereby proving our claim.

It is immediate to verify the last claim, since the leading coeﬃcient of Lk,φ(λ)
with the ﬁrst column removed is a diagonal matrix with nonzero elements on the
diagonal, and so it is invertible.
(cid:3)

We can immediately construct some examples for the application of the theorem.
Consider the Chebyshev basis of the ﬁrst kind {Ti(λ)}, which satisﬁes a recurrence
relation of the form:

Then we have that the matrix polynomial

Tj+1(λ) = 2λTj(λ) − Tj−1(λ).

1 −2λ
. . .

Lǫ,T (λ)

L(λ) =(cid:20)λM1 + M0 Lη,T (λ)T
is a linearization for the polynomial p(λ) = Pǫ

(cid:21) ,

0

Lk,T (λ) :=

i=1Pη

j=1(λM1 + M0)i,j Ti(λ)Tj(λ).
As shown in [24], the product Ti(λ)Tj (λ) can be rephrased in terms of sums of
Chebyshev polynomials, and this can be used to build a linearization for polyno-
mials expressed in the Chebyshev basis (so without product families involved).

1
. . .
. . .
1 −2λ

1




1
−λ

12

LINEARIZATIONS OF MATRIX POLYNOMIALS IN VARIOUS BASES

3.3. Handling interpolation bases. The framework covers orthogonal bases, but
there are some other interesting cases, as for example the interpolation bases such
as Lagrange, Newton and Hermite.

In this section we study their structures. Recall that, by Theorem 15, once we
have constructed the dual basis Lk,φ(λ) for one of these bases, we need to ensure
that Lk,φ(λ)πk,φ(λ) = 0 and that Lk,φ(λ) has full row rank. In order to have a
strong linearization we also require the dual basis to be minimal.

3.4. The Lagrange basis. Let σ(1)
two (not necessarily
disjoint) sets of pairwise diﬀerent nodes in the complex plane. Then we can deﬁne
the weights and the Lagrange polynomials by

1 , . . ., σ(1)

1 , . . ., σ(2)

and σ(2)

η

ǫ

t(s)
i

(σ(s)

i − σ(s)
j ),

:=Yj6=i

l(s)
i

(λ) :=

(λ − σ(s)

j ),

1
t(s)

i Yj6=i

s ∈ {1, 2}.

In the following let φj (λ) = l(1)
j (λ), coherently with the notation
used before. The linearization for a polynomial expressed in a product family, built
according to Theorem 15, has the following structure:

j (λ) and ψj(λ) = l(2)

L(λ) =(cid:20)λM1 + M0 Lη,ψ(λ)T

Lǫ,φ(λ)

0

(cid:21)

where

t(1)
1 (λ − σ1) −t(1)

2 (λ − σ2)

. . .

. . .

Lk,φ(λ) =


t(1)
k−1(λ − σk−1) −t(1)

k (λ − σk)




and Lk,ψ(λ) can be deﬁned in an analogous way.

Lemma 26. The matrix Lk,φ(λ) deﬁned above is a dual minimal basis for the La-
grange basis {φi} constructed on the nodes σ1, . . . , σk (that is, it is dual to πk,φ(λ)).

Proof. It is easy to verify that Lk,φ ∈ C[λ]k×(k+1) and

Lk,φ(λ)πk,φ(λ) = 0,

πk,φ(λ) :=


l(1)
k (λ)
...
l(1)
0 (λ)

.




It remains to show that the matrix Lk,φ(λ) has full row rank for any λ ∈ F. For all
values of λ that are not equal to the nodes the ﬁrst k columns are upper triangular
with non-zero elements on the diagonal, and so the hypotheses is satisﬁed.
It
remains to deal with the cases where λ = σi for some i = 1, . . . , k − 1.

We note that in this case one of the columns of the matrix is zero, but removing
it yields a square matrix which is block diagonal with only two diagonal blocks. The
top-left one is upper triangular and invertible, while the bottom-right one is lower
triangular and invertible, since they both have nonzero elements on the diagonal.
Notice that the ﬁrst k columns of the leading coeﬃcient of Lk,φ are upper triangu-
lar with nonzero elements on the diagonal. This implies that the leading coeﬃcient
has full row rank, thus proving the minimality of Lk,φ(λ).
(cid:3)

LINEARIZATIONS OF MATRIX POLYNOMIALS IN VARIOUS BASES

13

3.5. Constructing a classical Lagrange linearization. Besides building lin-
earizations for polynomial expressed in product families of Lagrange bases, the
above formulation can be used to linearize a polynomial expressed in a Lagrange
basis built on the union of the nodes.

In fact, we observe that if we have two Lagrange polynomials l(1)

j (λ)
deﬁned according to the previous notation then their product is almost a Lagrange
polynomial for the union of the nodes. More precisely, assume that we have a set
of nodes σ1, . . . , σn and let l(1)
j (λ) be Lagrange polynomials relative to
the nodes σ1, . . . , σk and σk+1, . . . , σn, respectively. Then if li(λ) are the Lagrange
polynomials related to all the nodes we have that

(λ) and l(2)

(λ) and l(2)

i

i

li(λ) =(l(1)

(λ) · l(2)
i
j (λ) · l(2)
l(1)

j (λ) · λ−σj+k
(λ) · λ−σj

i−k1

σj+k −σs
σi−σs+k
σj −σs
σi−σs

i 6 k

i > k

.

σi−σj+k Qs6=j
σi−σj Qs6=j

It is worth noting that these formulas become much more straightforward if one
considers unscaled Lagrange polynomials by getting rid of the normalization factor,
since in that case we obtain:

li(λ) =(l(1)

(λ) · l(2)
i
j (λ) · l(2)
l(1)

j (λ) · (λ − σj+k)
(λ) · (λ − σj)

i−k1

i 6 k

i > k

.

The part missing from the product of two Lagrange polynomials in order to obtain
the one with the union of the nodes is always linear and so can be placed as a
coeﬃcient in the top-left matrix polynomial λM1 + M0.

Remark 27. Notice that it is possible to choose two equal nodes in σ1, . . . , σk and
σk+1, . . . , σn. This allows to obtain a Lagrange linearization with repeated nodes,
which is a special case of Hermite linearization, where it is possible to interpolate
a polynomial imposing the value of its ﬁrst derivative at the nodes. By using
the product dual bases it is possible to extended this construction to higher order
derivatives. However, such a construction would have redundancy in the polynomial
family, thus leading to linearizations which has inﬁnite eigenvalues. In Section 3.7
we present a direct construction of the dual basis for the Hermite basis that does
not.

3.6. Explicit construction for the Newton basis. Another concrete example
is the construction of the Newton basis linearization. We can consider, similarly to
the Lagrange case, a set of nodes σ1, . . . , σn and assume to have two Newton bases,
one built using σ1, . . . , σk, and the other built using σk+1, . . . , σn.

To construct the linearization we need to ﬁnd Lk,φ(λ) which satisﬁes the require-

ments of Theorem 15. A possible choice is given by the following

1 σk − λ

. . .

Lk,φ(λ) :=


. . .
1

σ1 − λ

,




The matrix Lk,φ(λ) has the right dimensions k×(k+1), full row-rank for any λ, and
is such that the product Lk,φ(λ)πk,φ(λ) = 0. Moreover, the leading coeﬃcient has
full row rank so we also have the minimality and all the hypotheses of Theorem 15
are satisﬁed.

Qk

πk,φ(λ) =


j=1(λ − σj )

...

λ − σ1

1

.




14

LINEARIZATIONS OF MATRIX POLYNOMIALS IN VARIOUS BASES

3.7. Linearizations in the Hermite basis. Recently a linearization for polyno-
mials expressed in the Hermite basis has been presented by Fassbender, P´erez and
Shayanfar in [16].

The Hermite basis can be seen as a generalization of the Lagrange basis where
not only the values of the functions at the nodes are considered, but also the values
of their derivatives.

Assume that we have a set of nodes σ1, . . . , σn, and that we have interpolated a
function assigning the derivative up to the s-order, for some s > 1 (the case s = 1
gives the Lagrange basis). The order s can also vary depending on the node. We
can then consider the basis given by the following vector polynomial:

ω(λ)

(λ−σ1)s1

...

ω(λ)

(λ−σ1)

...

ω(λ)

(λ−σn)sn

...

ω(λ)

(λ−σn)









πk,φ(λ) =

,

ω(λ) :=

n

Yj=1

(λ − σj)sj ,

k =

sj.

n

Xj=1

A generic polynomial expressed in this basis can be written as p(λ) = pT πk,φ(λ)
where p is the column vector with the coeﬃcients in the Hermite basis.

We want to show that it is possible to formulate a linearization for the Hermite
basis in our framework. We already have the vector πk,φ(λ) so we simply need to
ﬁnd a linear matrix polynomial Lk,φ(λ) of the correct dimension that has full row
rank and such that Lk,φ(λ)πk,φ(λ) = 0.

Lemma 28. The matrix polynomial Lk,φ(λ) deﬁned as follows

Lk,φ(λ) =


with

Jσ1 (λ) −(λ − σ2)es1 eT
s2

. . .

. . .

Jσn−1(λ) −(λ − σn)esn−1 eT
sn

˜Jσn (λ)

,




Jσj (λ) :=


λ − σj −1
. . .

. . .
. . .

−1

λ − σj




,

λ − σj −1
. . .

˜Jσj (λ) :=


. . .

λ − σj −1

,




is a dual basis for the Hermite basis {φi} of orders si, i = 0, . . . , n.

Proof. We can check directly that Lk,φ(λ)πk,φ(λ) = 0, and so it only remains to
verify that the row rank is maximum. We notice that for any λ 6= σj the matrix
is upper triangular with non zero elements on the diagonal and so the condition is
obviously satisﬁed. For λ = σj then the diagonal block Jσj (λ) is singular. Assume,
for simplicity, that j = 1, and consider the matrix S obtained by removing the ﬁrst

LINEARIZATIONS OF MATRIX POLYNOMIALS IN VARIOUS BASES

15

column of Lk,φ(λ). We notice that S has the following structure:

−I
s1−1 −(σ1 − σ2)eT
0T
s2

Jσ2 (σ1)

S :=





−(σ1 − σ3)es2 eT
s3

. . .

. . .

Jσn−1 (σ1) −(σ1 − σn)esn−1 eT
sn

˜Jσn (σ1)





To prove that S is invertible we consider the trailing submatrix ˜S obtained by
removing the ﬁrst block row and column. We can transform ˜S by means of block
column operations so that

˜SX =(cid:20) ˜uT

B(σ1) −ek−σ1−1(cid:21) ,

σ1 − σn

u(λ) =


−(σ1 − σ2)es2

...

−(σ1 − σn−1)esn

0sn−1




and B(σ1) is block diagonal with the Jσj (σ1) of size sj on the block diagonal, except
the last one which is of size sn − 1. Since uT B(σ1)−1ek−σ1−1 = 0 we can write

det S ˜X = det B(σ1) ·(cid:2)(σ1 − σn) + ˜uT B(σ1)−1ek−σ1−1(cid:3) =
Yj=1

(σ1 − σj)sj · (σ1 − σn)sn−1(σ1 − σn) =

Yj=1

n−1

=

n

(σ1 − σj)sj 6= 0.

This proves that S ˜X is invertible, concluding the proof.

(cid:3)

The above lemma guarantees the applicability of Theorem 15 for the case of Her-
mite polynomials (and matrix polynomials), so we have an explicit way of building
linearizations in this basis.

3.8. Bernstein basis. A last example that is relevant in the context of computer
aided design is the Bernstein basis, which is the building block of B´ezier curves
[7,13–15]. Given an interval [α, β], we can deﬁne the family of Bernstein polynomials
of degree k as follows:

φi(λ) :=(cid:18)k

i(cid:19)(λ − α)i(β − λ)k−i,

i = 0, . . . , n

We show that also these polynomials ﬁt in our construction.

Lemma 29. The linear matrix polynomial Lk,φ(λ) deﬁned as follows

(cid:0) k
k−1(cid:1)(λ − β)

(cid:0)k
k(cid:1)(λ − α)
. . .

Lk,φ(λ) :=


. . .
(cid:0)k
0(cid:1)(λ − β)



(cid:0)k
1(cid:1)(λ − α)

is a dual minimal basis for the Bernstein polynomials of degree k deﬁned above.

Proof. A direct computation shows that Lk,φ(λ)πk,φ(λ) = 0. Moreover, notice that
for any λ 6= β the ﬁrst k columns of Lk,φ(λ) form a square upper triangular matrix
with non-zero diagonal elements, and for any λ 6= α the last k columns are an
invertible lower triangular matrix. This guarantees that the row rank is maximum

16

LINEARIZATIONS OF MATRIX POLYNOMIALS IN VARIOUS BASES

for any λ ∈ F. Since the leading coeﬃcient has the ﬁrst k columns which are upper
triangular and invertible we also have the minimality.
(cid:3)

4. Linearizing sums of polynomials and rational functions

4.1. Linearizing the sum of two polynomials. In this section we present an-
other example of linearization which deals with the following problem: assume that
we are given two polynomials p(λ) and q(λ) of which we want to ﬁnd the intersec-
tions, that is the values of λ such that q(λ) = p(λ), and assume that p(λ) and q(λ)
are expressed in diﬀerent bases.

Normally one would solve the problem by considering the polynomial r(λ) =
p(λ) − q(λ) and ﬁnding its roots, for example, by using a linearization. However,
this requires to perform a change of basis on at least one of the two polynomials,
and this operation is possibly ill-conditioned (see [20] for a related analysis).

In the case of interpolation bases, such as Newton or Lagrange, this could be also
useful when one wants to ﬁnd intersections of functions that have been sampled in
diﬀerent data points. In this case it might even not be possible to resample the
function (think of measured data).

Here we show how to linearize the problem directly.

Theorem 30. Let p(λ) and q(λ) be two polynomials of the following form:

ǫ

η

p(λ) :=

pjφj(λ),

q(λ) :=

qjψj(λ).

Xj=0

Xj=0

and let Lǫ,φ(λ) and Lη,ψ(λ) be dual bases for {φi} and {ψi}. Then the matrix
polynomial

L(λ) :=(cid:20)pwT

ψ − wφqT LT
Lη,ψ(λ)

0

ǫ,φ(λ)

(cid:21) ,

w⋆ := Γ−1

⋆ (1),

⋆ ∈ {φ, ψ}

where Γ−1
Remark 9), is a linearization for r(λ) := p(λ) − q(λ).

⋆ (1) is the vector of the coeﬃcients of the constant 1 in the basis ⋆ (see

Proof. w⋆ := Γ−1
⋆ (1) means that w⋆πk,⋆(λ) = 1 for ⋆ ∈ {φ, ψ} where k is either ǫ or
η depending on the choice. By Lemma 25 we know that L(λ) is a linearization for

πT
ǫ,φ(λ)(pwT
This concludes the proof.

ψ − wφqT )πη,ψ(λ) = p(λ) · 1 − 1 · q(λ) = r(λ).

(cid:3)

Remark 31. We notice that the linearization above, according to Theorem 15, is a
linearization for a polynomial of degree d := ǫ + η, but r(λ) is of degree max{ǫ, η} 6
d. The reason for this is that this is actually a linearization for a polynomial of
grade d that could have some leading zero coeﬃcients, thus having degree smaller
than d. The grade is deﬁned as the maximum degree of the monomials, while the
degree is the maximum of the non-zero ones.

The diﬀerence between grade and degree cause inﬁnite eigenvalues to appear
when we solve the eigenvalue problem obtained through Theorem 30 numerically.
However, the ﬁnite eigenvalues that we get are still the actual roots of r(λ).

The framework of Section 3.1 could be used to extend the above result to the sum
of an arbitrary number of polynomials (possibly all expressed in diﬀerent bases).
This can be obtained by combining the proof of Theorem 30 with the result of
Theorem 24.

LINEARIZATIONS OF MATRIX POLYNOMIALS IN VARIOUS BASES

17

example. Let p1(λ) = Pn

Numerical experiment 1. In this example we test the framework on the following
i=0 pi,2Ti(λ) be two polynomials
expressed in the monomial and Chebyshev basis of the ﬁrst kind, respectively. We
want to ﬁnd the roots of their sum q(λ) = p1(λ) + p2(λ). The columns of Table 1
represent, in the following order, the result of these diﬀerent approaches to solve
the problem that we tested:

i=0 pi,1λi and p2(λ) =Pn

(1) Converting p2(λ) to the monomial basis and using the Frobenius lineariza-
tion to ﬁnd the roots of the sum (by means of the command roots in
MATLAB).

(2) Converting p1(λ) to the Chebyshev basis and using the colleague lineariza-
tion [3, 22] to ﬁnd the roots of the sum of p1(λ) and p2(λ). The colleague
pencil has been solved using the QZ method in MATLAB.

(3) Constructing the linearization of Theorem 30 and solving it with the QZ

method (using eig in MATLAB).

(4) Constructing the linearization of Theorem 30 and deﬂating the spurious
inﬁnite eigenvalues by means of the strategy that will be proposed in Sec-
tion 62.

The polynomials have also been, by means of symbolical computations, converted
to the monomial basis and the roots have been computed using MPSolve [8] to
guarantee 16 accurate digits. These results have been used as a reference to measure
the errors, which have been summarized in Table 1 and Figure 2. In all the cases
the inﬁnite eigenvalues have either been deﬂated a priori, or have been detected by
the QZ algorithm and so we could deﬂate them a posteriori, so the numbers that we
report refer to the errors on the ﬁnite eigenvalues. In particular, we reported the
2-norm of the vectors containing the absolute errors in the computed roots. The
coeﬃcients of the polynomials have been generated by using the randn function.
Each experiment has been repeated 50 times and only the average error is reported.
The bad results obtained in the cases where a basis conversion has been per-
formed can be explained by looking at the conditioning of the matrix representing
the change of basis between monomial and the Chebyshev bases.

The conditioning is exponentially growing (see [20] for a related discussion), and
as n grows above 50 it cannot be guaranteed to compute even a single correct digit
in double precision (see Figure 1, where the exponential growth is clearly visible),
and so the results start to deteriorate very quickly.

4.2. Finding intersections of the sum of two rational functions. The results
of Section 4.1 admit an interesting extension to ﬁnding the zeros of a sum of ratios
of polynomials. This has the pleasant side eﬀect of mitigating the numerical issues
that might be encountered when dealing with a large number of inﬁnite eigenvalues.
Let f (λ) be a rational function of the form

f (λ) :=

p(λ)
q(λ)

+

r(λ)
s(λ)

,

with p(λ), q(λ), r(λ), and s(λ) polynomials, of which we want to ﬁnd the zeros. We
assume, in the following, that the numerators do not share any common factor with
the denominators, otherwise the roots of the common factors will also be obtained

2The approach of Section 6 has been moved to the end of this work because it is fairly general
and does not add much information about the structure of this linearization. Moreover, it can be
applied to other examples that will follow.

18

LINEARIZATIONS OF MATRIX POLYNOMIALS IN VARIOUS BASES

Change of basis between monomial and Chebyshev

10265

10193

10121

1049

2
k
·
k

n

i

i

g
n
n
o
i
t
i
d
n
o
C

10−23

0

200

400

600

Degree

Figure 1. Conditioning of the change of basis matrix between
the monomial and Chebyshev basis. The dashed line represents
the level 1
u , where u is the machine precision. Beyond that point
no correct digits can be guaranteed on the computed coeﬃcients.

Degree Monomial Chebyshev Theorem 30 Theorem 30 + deﬂation

5
10
20
40
80
160
320
640

7.03e-16
1.23e-14
1.95e-11
1.25e-04
1.29e+00
4.37e+00
9.85e+00
1.91e+01

5.19e-16
2.07e-15
4.48e-15
1.15e-14
7.62e-09
1.05e-01
2.97e+00
1.52e+01

5.44e-16
2.00e-15
2.49e-15
5.59e-15
9.76e-15
6.90e-14
1.07e-13
4.40e-13

8.40e-16
2.33e-15
4.08e-15
6.45e-15
1.69e-14
3.63e-14
7.57e-14
1.27e-13

Table 1. Numerical errors in the computation of the (ﬁnite) roots
of the polynomial p1(λ) + p2(λ) of the numerical experiment 1.

as eigenvalues of the linearization. With this assumption, we have that the roots
of f (λ) are the ones of f (λ)q(λ)s(λ) that is of the polynomial

t(λ) := p(λ)s(λ) + r(λ)q(λ).

In this section we will linearize the polynomial t(λ). However, for simplicity we will
sometimes inappropriately say that a linearization for t(λ) is also a linearization
for f (λ), since they share the same zeros.

For simplicity we ﬁrst consider the case in which all the polynomials are given
in the monomial basis, and we will handle the case where two diﬀerent bases are
used to deﬁne the polynomials p(λ), q(λ), r(λ) and s(λ) later.

LINEARIZATIONS OF MATRIX POLYNOMIALS IN VARIOUS BASES

19

Monomial conversion
Chebyshev conversion

Theorem 30

Theorem 30 + deﬂation

s
r
o
r
r
e

e
t
u

l
o
s
b
a

f
o
m
r
o
N

104

10−3

10−10

10−17

101

102

Degree

103

Figure 2. Norm of the absolute errors in the computation of the
roots of p1(λ) + p2(λ), where p1(λ) is a polynomial expressed in
the monomial basis while p2(λ) is one expresesed in the Chebyshev
one.

Theorem 32. Let f (λ) = p(λ)
s(λ) a rational function obtained as the sum of
two rational functions expressed in the monomial basis (so that p(λ), q(λ), r(λ) and
s(λ) are all polynomials). Assume that the numerators and the denominators do
not share any common factor. Then the matrix polynomial

q(λ) + r(λ)

L(λ) =(cid:20)psT + qrT LT

Lη(λ)

ǫ (λ)
0

(cid:21)

is a linearization for f (λ), where p, q, r and s are the column vectors containing the
coeﬃcients of the polynomials (padded with some leading zeros if the dimensions do
not match) and Lk(λ) is the dual basis for the monomial basis of degree k.

Proof. It suﬃces to follow the same reasoning of the proof of Theorem 30, so that
we obtain that L(λ) is a linearization for

πT
ǫ (λ)(psT + qrT )πη(λ) = p(λ)s(λ) + r(λ)q(λ) = f (λ)s(λ)q(λ),

which concludes the proof.

(cid:3)

The result can also be extended to the case where diﬀerent polynomial bases are

involved. More precisely, we have the following corollary.

Corollary 33. Let p(λ), q(λ), r(λ) and s(λ) polynomials deﬁned as follows:

ǫ

ǫ

η

siψi(λ)

η

Xi=0

p(λ) =

piφi(λ),

q(λ) =

qiψi(λ),

s(λ) =

for some polynomial bases {φi} and {ψi}. Then the matrix polynomial

Xi=0

r(λ) =

qiφi(λ),

Xi=0
Xi=0
L(λ) =(cid:20)psT + qrT LT

Lη,ψ(λ)

0

ǫ,φ(λ)

(cid:21)

20

LINEARIZATIONS OF MATRIX POLYNOMIALS IN VARIOUS BASES

is a linearization for both f1(λ) = p(λ)
s(λ) , where p, q, r
and s are the column vectors containing the coeﬃcients of the polynomials (padded
with some leading zeros if the dimensions do not match).

s(λ) and f2(λ) = p(λ)

q(λ) + r(λ)

r(λ) + q(λ)

Proof. By following the same proof of Theorem 32 we obtain that L(λ) is a lin-
earization for the polynomial

t(λ) = πT

ǫ,φ(λ)(psT + qrT )πη,ψ(λ) = p(λ)s(λ) + q(λ)r(λ)

which has the same roots as the rational functions

f1(λ) =

p(λ)
q(λ)

+

r(λ)
s(λ)

,

f2(λ) =

p(λ)
r(λ)

+

q(λ)
s(λ)

.

This concludes the proof.

(cid:3)

Remark 34. The above result shows that we can handle two speciﬁc cases. First,
the case where each rational function is deﬁned using polynomial in a certain basis,
and second, the one where both the denominators and the numerators share a
common basis.

An application of the above results is to ﬁnd the intersection of two rational
functions. As in the previous case, L(λ) is linearization for a polynomial of grade
max{deg p(λ), deg q(λ)} + max{deg r(λ), deg s(λ)} + 1 while the degree of the poly-
nomial f (λ)s(λ)q(λ) is max{deg p(λ) + deg s(λ), deg r(λ) + deg q(λ)}.

Since the ﬁrst quantity is always larger than the second one, the linearization
introduces at least one inﬁnite eigenvalue. However, in many interesting cases, such
as when the degree of the numerator and the denominator are the same in each
rational function, we only have one spurious inﬁnite eigenvalue, that can be deﬂated
easily.

The result can however be improved and, for these cases, we can build a strong

linearization relying on the following.

Theorem 35. Let f (λ) be a rational function with the same hypotheses and nota-
tion of Corollary 33, and assume that there exist two ǫ × (ǫ − 1) matrices A and B
such that

Then the matrix polynomial

πǫ,φ(λ) = (λA + B)πǫ−1,φ(λ)

L(λ) =(cid:20)(λA + B)T psT − (λA + B)T qrT LT

Lη,ψ(λ)

0

ǫ−1,φ(λ)

(cid:21)

is a strong linearization for f1(λ) and f2(λ).

Proof. By applying again Theorem 15 we obtain that L(λ) is a linearization for

t(λ) = πT
= πT
= p(λ)s(λ) + q(λ)r(λ).

ǫ−1,φ(λ)(cid:2)(λA + B)T psT − (λA + B)T qrT(cid:3) πη,ψ(λ)
ǫ,φ(λ)(cid:0)psT − qrT(cid:1) πη,ψ(λ)

Since t(λ) has degree ǫ + η, which is the size of L(λ), there are no extra inﬁnite
eigenvalues, and so this is a strong linearization.
(cid:3)

Remark 36. The hypotheses of Theorem 35 are satisﬁed in many cases. Some
concrete examples are the following:

LINEARIZATIONS OF MATRIX POLYNOMIALS IN VARIOUS BASES

21

(i) When {φi} is a degree-graded basis for Fk[λ] then φk+1(λ) has degree k + 1
and we can ﬁnd a so that λk = aT πk,φ(λ). If we choose α to be the leading
coeﬃcient of φk+1(λ) we have

φk+1(λ) − λαaT πk,φ(λ) = bT πk,φ(λ)

for some b ∈ Fk+1, since the left-hand side has degree k. This implies that

πk+1,φ(λ) =


λ


αaT
. . .

. . .

0
...
0

1

bT

. . .

0
...
0




+








1

πk,φ(λ).

(ii) When {φi} is an orthogonal basis then it is also degree-graded and so the above
result applies. In this case, however, it is very easy to get an explicit expression
for α, a and b, since they just contain the coeﬃcients of the recurrence relation
that allows to obtain φk+1(λ) starting from the previous terms.

(iii) If {φi} is the Lagrange basis we can still ﬁnd suitable matrices A and B so
that the hypothesis are satisﬁed. Assume that πk,φ(λ) is the Lagrange basis
on the interpolation nodes σ1, . . . , σk and that πk+1,φ(λ) has the additional
node σk+1. Then we have

λ−σk+1
σk −σk+1

πk+1,φ(λ) =

σk+1−σk Qk−1

j=1

1

where α =

σk −σj

σk+1−σj

.

α(λ − σk)eT
1

. . .




λ−σk+1
σ1−σk+1

πk,φ(λ),

Remark 37. Notice that the requirement needs to hold only for one of the two
families of polynomials. If the relation holds on {ψi} instead of {φi} the procedure
is analogous.

As a concrete example, we report here how the (non strong) linearization looks

when considering the following rational function:

f (λ) =

2λ2 − 1

λ2 + λ + 3

+

T1(λ) + T0(λ)
T1(λ) − T0(λ)

.

We have that p, q, r, s are given by

2
0

p =
−1
 ,


1
1

q =
3
 ,


r =(cid:20)1
1(cid:21) ,

s =(cid:20) 1
−1(cid:21) .

We get

L(λ) =(cid:20)psT + qrT LT

Lη,ψ(λ)

0

ǫ,φ(λ)

(cid:21) =


1
3 −1
1 −λ
1
2
4
1 −λ

0
1
0 −λ
0
0

.




22

LINEARIZATIONS OF MATRIX POLYNOMIALS IN VARIOUS BASES

Degree Theorem 32 Theorem 35

5
10
20
40
80
160
320
640

3.29e-16
4.37e-16
5.47e-16
6.87e-16
1.14e-15
1.72e-15
2.57e-15
4.21e-15

2.98e-16
4.01e-16
5.07e-16
5.75e-16
7.93e-16
1.40e-15
2.06e-15
3.53e-15

Table 2. Norm of the absolute error on the computed (ﬁnite)
roots of the rational function f (λ).

By Theorem 35 we can also obtain a strong linearization for f (λ). In the monomial
case the A and B matrices of the hypothesis are given by

A = e(k+1)

1

(e(k)

1 )T ,

B =


0

. . . 0

Ik

,




where e(k)
yields the linearization

i

is the i-th column of Ik. A straightforward application of the theorem

L(λ) =


3λ + 1

1 − λ

2
1

4
−λ

1
−λ
0




which is a strong linearization for the rational function f (λ).

In the following we report some numerical experiments that show the eﬀective-

ness of the approach.

Numerical experiment 2. Here we test the linearization for the solution of the sum
of rational functions. We generate four polynomials p(λ), q(λ), r(λ) and s(λ) of the
same degree n, and with p(λ), q(λ) being in the monomial basis and r(λ) and s(λ)
in the Chebyshev one.

We then ﬁnd the zeros of the rational function

f (λ) :=

p(λ)
q(λ)

+

r(λ)
s(λ)

by applying Theorem 32 and Theorem 35 and using the QZ algorithm on the ob-
tained linearizations. We compare the results with those obtained by symbolically
computing the coeﬃcients of the polynomial t(λ) := p(λ)s(λ) + r(λ)q(λ) and com-
puting its roots with 16 guaranteed digits using MPSolve [8]. The experiments have
been repeated 50 times and an average has been taken. The results are reported in
Table 2 and Figure 3.

5. Preserving even, odd and palindromic structures

In this section we deal with the following problem: we consider the case where
a matrix polynomial has a ⋆-even, ⋆-odd or ⋆-palindromic structure. These are

LINEARIZATIONS OF MATRIX POLYNOMIALS IN VARIOUS BASES

23

Theorem 32
Theorem 35

10−14

r
o
r
r
e

e
t
u

l
o
s
b
A

10−15

101

102

Degree

103

Figure 3. Norm of the absolute error on the computed roots of
the rational function f (λ). Both the strong and non-strong version
of the linearization have been tested.

often found in applications and are of particular interest since they induce some
symmetries on the spectrum.

For this reason it is important to develop linearizations that enjoy the same
structure, so the symmetries in the spectrum will be preserved. Many authors have
investigated this in recent years, providing diﬀerent solutions [25,27]. Linearizations
for these structures have been found by exploiting the generality of the L1 and L2
spaces of linearizations introduced in [26]. Our approach here leads to very similar
results, but is instead based on the freedom that we have in choosing the polynomial
families {φi} and {ψi}.

Here we often use ⋆ in place of the transpose or conjugate transpose operator,
since the constructions are valid for both choices. We give the deﬁnitions of these
structures.

Deﬁnition 38. A matrix polynomial P (λ) is ⋆-even if P (λ) = P (−λ)⋆. Similarly,
we say that P (λ) is ⋆-odd if P (λ) = −P (−λ)⋆.

Deﬁnition 39. A matrix polynomial P (λ) is said to be ⋆-palindromic if P (λ) =
rev P (λ)⋆. Similarly. we say that P (λ) is anti ⋆-palindromic if P (λ) = − rev P (λ)⋆.

Notice that all these relations induce a certain symmetry on the coeﬃcients in

the monomial basis. In particular, we have the following:

Lemma 40. Let P (λ) a matrix polynomial. Then,

(i) If the matrix polynomial is ⋆-palindromic or anti ⋆-palindromic the eigenvalues

come in pairs (λ, 1

λ ) when ⋆ = T and (λ, 1

λ ) when ⋆ = H

(ii) If the matrix polynomial is ⋆-even or ⋆-odd the eigenvalues come in pairs

(λ, −λ) when ⋆ = T and (λ, −λ) when ⋆ = H .

Proof. Follows immediately by the deﬁnitions above noting that singularity is pre-
served by all the symmetries considered there.
(cid:3)

24

LINEARIZATIONS OF MATRIX POLYNOMIALS IN VARIOUS BASES

5.1. Even and odd polynomials. In this section we deal with linearizing even
and odd polynomials. In practice we will only consider the case of even polynomials
since the other is analogous.

Theorem 41. Let P (λ) = P2k+1

2k + 1. Then the even matrix polynomial

i=0 Piλi be a ⋆-even matrix polynomial of grade

(−1)k(λP2k+1 + P2k)

I

L(λ) =





is a linearization for P (λ).

. . .

λI
. . .

. . .

. . .
I

λP1 + P0

λI

I

−λI

. . .
. . .

I

−λI





Proof. It is immediate to verify that the matrix polynomial is ⋆-even. In order to
check that it is a linearization for the correct polynomial we can see that the top-
right block and bottom-left block are of the form Lk,φ(λ) ⊗ Im and Lk,ψ(λ) ⊗ Im,
respectively, with Lk,⋆(λ) being dual bases for

πk,φ(λ) =


λk−1
...
λ
1

,




πk,ψ(λ) =


(−1)k−1λk−1

...
−λ
1

.




Applying Theorem 15 yields that L(λ) is a linearization for the matrix polynomial

(πk,φ(λ) ⊗ Im)T diag((−1)j(λP2j+1 + P2j))j=0,...,k(πk,ψ(λ) ⊗ Im) = P (λ),

which concludes the proof.

(cid:3)

5.2. Palindromic polynomials. A similar procedure can be applied to obtain
⋆-palindromic linearizations for ⋆-palindromic polynomials. However, the construc-
tion in this case is slightly more complicated. We ﬁrst prove the following lemma,
which provides linearizations with ⋆-palindromic oﬀ-diagonal blocks, and then we
show how to choose the top-left block to make the whole matrix polynomial ⋆-
palindromic.

Theorem 42. Let {φi} and {ψi} be the polynomial bases deﬁned by

φi = λk−i,

ψi = λi,

i = 0, . . . , k.

Then two dual bases Lk,φ(λ) and Lk,ψ(λ) for {φi} and {ψi}, respectively, are given
by

1 −λ
. . .

Lk,φ(λ) =


. . .
1 −λ




,

λ −1
. . .

Lk,ψ(λ) =


. . .
λ −1




LINEARIZATIONS OF MATRIX POLYNOMIALS IN VARIOUS BASES

25

and the ⋆-palindromic matrix polynomial

L(λ) =(cid:20) λM + M ⋆

Lk,ψ(λ) ⊗ Im

Lk,φ(λ)⋆ ⊗ Im

0

(cid:21) , M =


M1,1
...
Mk,1

. . . M1,k
...
. . . Mk,k

,




where Mij ∈ Cm×m, is a linearization for the degree 2k − 1 matrix polynomial
deﬁned by

P (λ) =

k

Xi,j=1

M ⋆

j,iλk+j−i−1 +

Mi,jλk+j−i.

k

Xi,j=1

Proof. It is immediate to verify that the given matrices Lk,φ(λ) and Lk,ψ(λ) are
indeed dual bases. By applying Theorem 15 we get P (λ) as

P (λ) =(cid:2)λk−1Im · · ·

Im(cid:3) (λM + M ⋆)


Im
...

λk−1Im

.




(cid:3)

The result above can be used to construct ⋆-palindromic linearizations for ⋆-
j=0 Pjλj be such a polynomial,
and assume we want to describe a procedure to choose the block coeﬃcients of M
in Theorem 42 in order to make L(λ) a linearization for P (λ).

palindromic matrix polynomials. Let P (λ) = Pn

Deﬁnition 43 (Block Diagonal Sum). Let X = bdsk(M, d) be the matrix deﬁned
as the sum of the matrices along the d-th block diagonal of the matrix M partitioned
in blocks of size k. We refer to X as the d-th block diagonal sum of M . Whenever
the block matrix M does not have a d-th diagonal (since it is too small), we deﬁne
bdsk(M, d) to be the zero matrix.

Remark 44. The linear matrix polynomial L(λ) deﬁned in Theorem 42 is a lin-
earization for a matrix polynomial P (λ) of degree 2k − 1 if and only if the relation

Pj = bdsm(M, j − k) + bdsm(M, k − j − 1)⋆

holds for any j = 0, . . . , 2k − 1.

Notice that Remark 44 can also be used to build the linearization starting from

its coeﬃcients. In fact, the relation for j ∈ {0, 2k − 1} simpliﬁes to:

P0 = M ⋆

1,k,

P2k−1 = M1,k.

Having determined the term in position (1, k), one can then proceed to ﬁll in the
others by imposing the condition of Remark 44.

Here we provide a concrete example of such a construction. However, we stress

that is not the only possible choice.

polynomial. Then the matrix polynomial of Theorem 42 with

i=0 Piλi a degree 2k − 1 and ⋆-palindromic matrix

Theorem 45. Let P (λ) = Pn
M =


is a ⋆-palindromic linearization for P (λ).

0m · · ·
...
0m · · ·

P ⋆
0m
0
...
...
0m P ⋆

k−1




26

LINEARIZATIONS OF MATRIX POLYNOMIALS IN VARIOUS BASES

Proof. Notice that, in the formula above, we have that the only non zero diagonal
elements of M are on the last column and Mi,k = P ⋆
i−1 = P2k−i. We can check
that the equality of Remark 44 holds. If 0 6 j 6 k − 1 we have bdsm(M, j − k) = 0
and bdsm(M, k − j − 1)⋆ = M ⋆
i,k where i is such that k − i = k − j − 1 (being on the
(k − j − 1)-th diagonal). This implies that i = j + 1 and so M ⋆
j , as desired.
On the other hand, if k 6 j 6 2k − 1 we similarly have bdsm(M, k − j − 1) = 0 and
bdsm(M, j − k) = Mi,k with k − i = j − k so that i = 2k − j. This again implies
that Mi,k = P2k−i = P2k−(2k−j) = Pj . This concludes the proof.
(cid:3)

i,k = P ⋆

6. Deflation of infinite eigenvalues

We have observed that in the polynomial sum case of Section 4.1 the linearization
built according to Theorem 30 is generally not strong and might have many inﬁnite
eigenvalues.

In this section we show what the structure of the inﬁnite eigenvalues is and a
possible strategy to deﬂate them based on a simpliﬁed approach inspired by [5, 28].
In our case we have the advantage of knowing exactly which eigenvalue we want
to deﬂate and we can completely characterize the structure of the block in the
Kronecker canonical form corresponding to the inﬁnite eigenvalue.

Lemma 46. Let L(λ) be the lineraization obtained from Theorem 30 for the sum
of two arbitrary polynomials. Then there exist two unitary bases Q and Z such that

1
. . .

. . .
. . .

0




.




1
0

λB1 − B0(cid:21) ,
QH L(λ)Z =(cid:20)I − λJ λA1 − A0

0

J =

Proof. Such a decomposition can be obtained by following the deﬂation procedure
for the inﬁnite eigenvalue described in [5] and [28]. We only need to prove that the
pencil obtained in the top-left entry of the transformed matrix is exactly I − λJ.
Let A, B be matrices such that L(λ) = A − λB. We note that B has nullity equal
to 1 in our construction. Recall that the columns of Q and Z are orthogonal bases
of the sequence of spaces deﬁned by

Zi =({0}

B−1Qi−1

if i = 0
otherwise

,

Qi = AZi,

where B−1 is the pre-image of B. The fact that B has nullity 1 implies that the
dimension of Zi can increase at most of 1 at each step. This means that there
exist a unique diagonal block in the Kronecker canonical form corresponding to the
inﬁnite eigenvalue, whose size is exactly equal to the algebraic multiplicity of it. (cid:3)

We can use the algorithm described in [5] to compute the matrices Q and Z and
then solve the pencil λB1 − B0 instead of L(λ). Experiments using this strategy
were reported in Section 4.1.

For a more in-depth discussion of the above approach to deﬂation see the work
of Berger and Reis [6] which is based on the analysis originally carried out by
Wong [29].

LINEARIZATIONS OF MATRIX POLYNOMIALS IN VARIOUS BASES

27

7. Conclusions

We have provided an extension of the main theorem of [24] to construct lin-
earizations. This new result makes it easier to prove that many matrix polynomials
are linearizations for, among others, sum of polynomials, rational functions, and
allows to realize structure preserving pencils.

We think that the ﬂexibility oﬀered by the adjustment of the dual basis in the
pencil L(λ) allows even for further improvement and for the coverage of more struc-
tures. We think that in many cases this construction can be used as an alternative
to other approaches to ﬁnd structured linearizations, such as looking in the spaces
L1 and L2 from [26].

Acknowledgements

This paper presents research results of the Belgian Network DYSCO (Dynami-
cal Systems, Control, and Optimization), funded by the Interuniversity Attraction
Poles Programme initiated by the Belgian Science Policy Oﬃce.

We wish to thank Piers Lawrence, who helped to understand the construction

of dual bases for the Lagrange case, and Thomas Mach for inspiring discussions.

References

[1] Amir Amiraslani, Robert M. Corless, and Peter Lancaster. Linearization of matrix polyno-
mials expressed in polynomial bases. IMA Journal of Numerical Analysis, 29(1):141–157,
2009.

[2] Efstathios N. Antoniou and Stavros Vologiannidis. A new family of companion forms of

polynomial matrices. Electronic Journal Linear Algebra, 11:78–87, 2004.

[3] Stephen Barnett. A companion matrix analogue for orthogonal polynomials. Linear Algebra

and its Applications, 12(3):197–202, 1975.

[4] Theodorus G. J. Beelen and Paul Van Dooren. A pencil approach for embedding a polyno-
mial matrix into a unimodular matrix. SIAM Journal on Matrix Analysis and Applications,
9(1):77–89, 1988.

[5] Theodorus G. J. Beelen and Paul Van Dooren. An improved algorithm for the computation of
Kronecker’s canonical form of a singular pencil. Linear Algebra and its Applications, 105:9–65,
1988.

[6] Thomas Berger and Timo Reis. Controllability of linear diﬀerential-algebraic systemsa survey.

In Surveys in Diﬀerential-Algebraic Equations I, pages 1–61. Springer, 2013.

[7] Dario A. Bini, Luca Gemignani, and Joab R. Winkler. Structured matrix methods for CAGD:
an application to computing the resultant of polynomials in the Bernstein basis. Numerical
linear algebra with applications, 12(8):685–698, 2005.

[8] Dario A. Bini and Leonardo Robol. Solving secular and polynomial equations: A multipreci-

sion algorithm. Journal of Computational and Applied Mathematics, 272:276292, 2014.

[9] Robert M. Corless. On a generalized companion matrix pencil for matrix polynomials ex-
pressed in the Lagrange basis. In Symbolic-numeric computation, Trends Mathematics, pages
1–15. Birkh¨auser, Basel, 2007.

[10] Fernando De Ter´an, Froil´an M. Dopico, and D. Steven Mackey. Fiedler companion lineariza-
tions and the recovery of minimal indices. SIAM Journal on Matrix Analysis and Applica-
tions, 31(4):2181–2204, 2010.

[11] Fernando De Ter´an, Froil´an M. Dopico, and D. Steven Mackey. Fiedler companion lineariza-
tions for rectangular matrix polynomials. Linear Algebra and its Applications, 437(3):957–
991, 2012.

[12] Fernando De Ter´an, Froil´an M. Dopico, and D. Steven Mackey. Spectral equivalence of matrix
polynomials and the index sum theorem. Linear Algebra and its Applications, 459:264–333,
2014.

[13] Gerald Farin. A History of Curves and Surfaces in CAGD. Handbook of Computer Aided

Geometric Design, page 1, 2002.

28

LINEARIZATIONS OF MATRIX POLYNOMIALS IN VARIOUS BASES

[14] Gerald Farin. Curves and surfaces for computer-aided geometric design: a practical guide.

Elsevier, 2014.

[15] Rida T. Farouki, Tim N. T. Goodman, and Thomas Sauer. Construction of orthogonal bases
for polynomials in Bernstein form on triangular and simplex domains. Computer Aided Geo-
metric Design, 20(4):209–230, 2003.

[16] Heike Fassbender, Javier A. P´erez, and Nikta Shayanfar. A sparse linearization for Hermite

interpolation matrix polynomials. MIMS Eprint, 2015.

[17] Miroslav Fiedler. A note on companion matrices. Linear Algebra and its Applications,

372:325–331, 2003.

[18] G. David Forney Jr. Minimal bases of rational vector spaces, with applications to multivari-

able linear systems. SIAM Journal on Control, 13(3):493–520, 1975.

[19] Felix R. Gantmacher. The Theory of Matrices. Vol. 1. AMS Chelsea Publishing, Providence,

RI, 1998. Translated from the Russian by K. A. Hirsch, Reprint of the 1959 translation.

[20] Walter Gautschi. The condition of Vandermonde-like matrices involving orthogonal polyno-

mials. Linear algebra and its applications, 52:293–300, 1983.

[21] Israel Gohberg, Peter Lancaster, and Leiba Rodman. Matrix polynomials, volume 58. SIAM,

1982.

[22] Isadore J. Good. The colleague matrix, a Chebyshev analogue of the companion matrix. The

Quarterly Journal of Mathematics, 12(1):61–68, 1961.

[23] Nicholas J. Higham, D. Steven Mackey, Niloufer Mackey, and Fran¸coise Tisseur. Symmetric
linearizations for matrix polynomials. SIAM Journal on Matrix Analysis and Applications,
29(1):143–159, 2006.

[24] Piers W. Lawrence, Froil´an Dopico, Paul Van Dooren, and Javier P´erez. Fiedler-like lineariza-

tions of polynomial matrices. to be submitted, 2015.

[25] D. Steven Mackey, Niloufer Mackey, Christian Mehl, and Volker Mehrmann. Structured poly-
nomial eigenvalue problems: Good vibrations from good linearizations. SIAM Journal on
Matrix Analysis and Applications, 28(4):1029–1051, 2006.

[26] D. Steven Mackey, Niloufer Mackey, Christian Mehl, and Volker Mehrmann. Vector spaces of
linearizations for matrix polynomials. SIAM Journal on Matrix Analysis and Applications,
28(4):971–1004, 2006.

[27] D. Steven Mackey, Niloufer Mackey, Christian Mehl, and Volker Mehrmann. Numerical meth-
ods for palindromic eigenvalue problems: Computing the anti-triangular Schur form. Numer-
ical Linear Algebra with Applications, 16(1):63–86, 2009.

[28] Paul Van Dooren. The computation of Kronecker’s canonical form of a singular pencil. Linear

Algebra and its Applications, 27:103–140, 1979.

[29] Kai-Tak Wong. The eigenvalue problem λTx+ Sx. Journal of Diﬀerential Equations,

16(2):270–280, 1974.

