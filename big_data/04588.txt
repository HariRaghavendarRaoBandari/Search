6
1
0
2

 
r
a

 

M
5
1

 
 
]

V
C
.
s
c
[
 
 

1
v
8
8
5
4
0

.

3
0
6
1
:
v
i
X
r
a

Classiﬁcation with Repulsion Tensors: A Case Study on Face

Recognition

Hawren Fang∗

March 16, 2016

Abstract

We consider dimensionality reduction methods for face recognition in a supervised setting, using
an image-as-matrix representation. A common procedure is to project image matrices into a smaller
space in which the recognition is performed. These methods are often called “two-dimensional” in
the literature and there exist counterparts that use an image-as-vector representation. When two
face images are close to each other in the input space they may remain close after projection -
but this is not desirable in the situation when these two images are from diﬀerent classes, and this
often aﬀects the recognition performance. We extend a previously developed ‘repulsion Laplacean’
technique based on adding terms to the objective function with the goal or creation a repulsion
energy between such images in the projected space. This scheme, which relies on a repulsion graph,
is generic and can be incorporated into various two-dimensional methods. It can be regarded as a
multilinear generalization of the repulsion strategy by Kokiopoulou and Saad [Pattern Recog., 42
(2009), pp. 2392–2402]. Experimental results demonstrate that the proposed methodology oﬀers
signiﬁcant recognition improvement relative to the underlying two-dimensional methods.

1

Introduction

Numerous linear dimensionality reduction methods have been developed for classiﬁcation tasks. Among
these are a class methods which convert the data items to vectors in numerical form. For example, in
face recognition, a face image is transformed into a column vector by concatenating rows or columns.
A linear projector is obtained using the training data. The projector projects both training and test
data into a lower dimensional space, in which the recognition task is performed. Typical examples of
these include principal component analysis (PCA) [32, 31], linear discriminant analysis (LDA) [3, 5, 23],
locality preserving projections (LPP) [9, 10], and neighborhood preserving projections (NPP) [8]. There
are counterparts of LPP and NPP which enforce orthogonality constraint on the projector. They are
denoted by OLPP and ONPP [14, 15], respectively.

Two-dimensional projection methods, treating each image as a matrix, with a goal of exploiting
spatial redundancy.
In the literature, such methods often utilize the key word ‘two-dimensional’ to
charaterize them. They can be categorized into two diﬀerent types. The ﬁrst type of methods aim to
preserve the distinct features of the data matrices in the projection [17, 29, 35, 36, 37]. These methods
can be regarded as two-dimensional generalizations of the classical principal component analysis.

The other type of methods aim to preserve the closeness of neighboring data items in the projected
space. In a supervised setting, two items are regarded as neighbors if they share the same class label.
Examples of such methods include two-dimensional linear discriminant analysis (2D-LDA) [11, 20],
two-dimensional neighborhood preserving projections (2D-NPP) [22, 26], and two-dimensional locality
preserving projections (2D-LPP) [4, 7, 25].

The aforementioned methods are summarized in Table 1, where the synonyms in the application of

face recognition, if any, are also listed.

An enhanced graph-based dimensionality reduction technique, repulsion Laplaceans [16], was pro-
posed to improve the classiﬁcation methods which use image-as-vector representation. The objective is

∗Email: hrfang@yahoo.com

1

Method

Abbrev.

Table 1: Linear dimensionality reduction methods.
In Face Recog.
Eigenfaces
Fisherfaces
Laplacianfaces
-

PCA
LDA
LPP
NPP

Data

1D

2D

Principal Component Analysis
Linear Discriminant Analysis
Locality Preserving Projections
Neighborhood Preserving Projections
Principal Component Analysis
Linear Discriminant Analysis
Locality Preserving Projections
Neighborhood Preserving Projections

References
[12, 32, 31]
[3, 5, 23]
[9, 10]
[8, 14, 15]
[17, 29, 35, 36, 37]
[11, 21, 38]
[4, 7, 25]
[26, 22]

2D-PCA 2D-Eigenfaces
2D-LDA 2D-Fisherfaces
2D-LPP 2D-Laplacianfaces
2D-NPP -

to repel from other data points that are not from the same class but that close to each other in the input
high-dimensional space.

In this paper, we develop a generalized methodology using repulsion tensors to improve the two-
dimensional projection methods. This method is generic and can be applied to various methods, such
as 2D-OLPP, 2D-ONPP, and 2D-LDA. The experiments on face recognition demonstrate signiﬁcant
improvements in the recognition performance over the existing two-dimensional methods. Compared to
the peers using the image-as-vector representation, the proposed technique achieves substantial compu-
tational savings.

The rest of this paper is organized as follows. Section 2 formulates the two-dimensional projection
methods in tensor form, establishes the connections between them, and gives a uniﬁed view. The
enhancement with repulsion tensors is presented in Section 3, which also gives a uniﬁed framework for
computing the projectors. Section 4 reports on experimental results with face recognition. The paper
ends with conclusing remarks and a discussion of future work in Section 5. Appendix A introduces the
concept of tensors and some related operations useful in this paper. Appendix B reviews the linear
dimensionality reduction methods using image-as-vector representation.

The notational conventions in this paper are as follows. We denote scalars and vectors by lowercase
letters (e.g., m, n), matrices by uppercase letters (e.g., U, V ), and tensors by calligraphic letters (e.g.,
A, B). An exception is that I, J, K are used for upper bounds of indices. For a third order tensor
A ∈ RI×J×K, its (i, j, k) entry is denoted by aijk. We also use MATLAB-like notation. For example,
A(i, j, k) = aijk, and A(:, :, k) is the matrix formed by elements aijk for i = 1, . . . , I and j = 1, . . . , J
and the speciﬁed k. We use In to denote the identity matrix of size n-by-n, and en to denote the vector
of ones of length n. The norm k · k indicates the Frobenius norm.

2 Tensor formulation of 2D methods

Table 1 summarizes various known two-dimensional methods for image subspace analysis. These methods
treat each image as a matrix Xk ∈ Rm1×m2. The general form of the bilateral projection is

Yk = U T XkV ∈ Rd1×d2,

k = 1, . . . , n,

(1)

where d1 ≤ m1 and d2 ≤ m2. The transformation matrices U and V from the training data are applied to
a test image X to obtain a projected image Y = U T XV , and the recognition is performed by comparing
Y to Y1, . . . , Yn. When U is an identity matrix (d1 = m1) or V is an identity matrix (d2 = m2), the
projection is said to be unilateral.

In this section, we formulate these methods in tensor form. Readers are referred to [1] for tensor

concepts and operations. A brief introduction of the main ideas is also given in Appendix A.

Aggregating the matrices in (1), we obtain the third order tensors X ∈ Rm1×m2×n and Y ∈ Rd1×d2×n,
such that X (:, :, k) = Xk and Y(:, :, k) = Yk for k = 1, . . . , n. The relation between X and Y can be
written succinctly as

Y = X ×1 U T ×2 V T .

(2)

2

Constraints should be imposed to the transformation matrices U and V . A popular choice is the

orthogonality

U T U = Id1,

V T V = Id2 .

(3)

To facilitate the discussion, we introduce the concept of tensor trace. Given a fourth order tensor

B = [bijkh] ∈ RI×J×I×J , we deﬁne the tensor trace by

tr[1,2;3,4](B) =

IXi=1

JXj=1

bijij ,

(4)

where the subscript [1, 2; 3, 4] speciﬁes the order of the dimensions respect to which the trace is taken.
This is a generalization of the trace of a square matrix. In this paper we always use the order [1, 2; 3, 4]
in the tensor trace. Hence we abbreviate tr[1,2;3,4](B) as tr(B). A useful property which parallels the
relation kAk2 = tr(AAT ) is

kAk2 = tr(hA, Ai[3;3]),

where A = RI×J×K is a third order tensor, and hA, Ai[3;3] is the mode-[3; 3] contracted tensor product.
In the following discussion, we will use tensor and matrix notation interchangeably.

2.1 Two-dimensional PCA

We consider a class of tensor methods which can be regarded as high order generalizations of the classical
PCA [17, 29, 35, 36, 37]. The discussion starts with a bilateral projection method, also referred to as
the generalized low rank approximation of matrices (GLRAM) [37] or the concurrent subspaces analysis
(CSA) [35]. This is a special case of the high order orthogonal iteration (HOOI) of tensors [19, 29].

The goal here is to ﬁnd a reduced representation Yk ∈ Rd1×d2 of each Xk ∈ Rm1×m2 for k = 1, . . . , n.
The reconstructed data, associated with two orthogonal matrices U and V (U T U = Id1 and V T V = Id2 ),

is bXk = U YkV T . Each bX is a rank-d approximation of X, where d = min{d1, d2}. The reconstruction

errors are measured by

kXk − U YkV T k2

(5)

Now let fk(Y ) = kXk − U Y V T k2, which equals

nXk=1

kXk − bXkk2 =

nXk=1

tr(XkX T

k ) + tr(Y Y T ) − 2 tr(Y U T XkV ).

Since fk(Y ) is convex, the minimum of fk(Y ) is achieved by setting ∇fk = 0, from which we obtain
Y = U T XkV . Therefore, when (5) is minimized, (1) is satisﬁed.

Substituting (1) into (5), we obtain

kXk − U U T XkV V T k2 =

nXk=1

nXk=1

kXkk2 − kU T XkV k2 = kX k2 − kX ×1 U T ×2 V T k2.

As a result, minimizing kX − Y ×1 U ×2 V k2 is equivalent to maximizing kX ×1 U T ×2 V T k2. We end-up
with the following problem:

( maximize

U, V

kX ×1 U T ×2 V T k2

subject to U T U = Id1 , V T V = Id2 .

Note that we can write kX ×1 U T ×2 V T k2 in tensor trace form as

tr(hX ×1 U T ×2 V T , X ×1 U T ×2 V T i[3;3]).

(6)

(7)

A variant of the above method is called the generalized two-dimensional PCA [17]. The formulation
is essentially the same as (6), except that before dimensionality reduction, the data matrices are rigidly

translated so that its centroid is at the origin. To be speciﬁc, the translated matrices are eXk = Xk − ¯X

3

for i = 1, . . . , n, where ¯X = 1

nPn
i=1 Xi is the centroid matrix. Let eX be the translated tensor such that
eX (:, :, k) = eXk for k = 1, . . . , n. Then
eX = X − X ×3

where en ∈ Rn is the column vector of ones. In a similar discussion leading to (6), we obtain the program

×3 en = X − X ×3 (

) = X ×3 (In −

eneT
n
n

eneT

eT
n
n

n ),

1
n

( maximize

kX ×1 U T ×2 V T ×3 Jnk2
subject to U T U = Id1, V T V = Id2 ,

U,V

(8)

where Jn = In − 1
tensor trace as

n eneT

n is the centering matrix. The objective function can be written in the form of

tr(X ×1 U T ×2 V T ×3 Jn, X ×1 U T ×2 V T ).

(9)

Note that Jn is a projection matrix. Hence J 2
We call the dimensionality reduction method which solves (8) the 2D-PCA hereafter.

n = Jn = J T

n . This property has been used in deriving (9).

There is no closed form of solution to (6) and (8). One can use the high order orthogonal iteration

(HOOI) [19] to compute U and V .

One may perform 2D-PCA with the projection applied to only one side of the input image matrices
[36]. For example, we set V as the identity Im2 and therefore the projection is in the form Yi = U T Xi ∈
Rd1×m2 for i = 1, . . . , n. This unilateral projection is equivalent to performing PCA on the columns of
all images.

2.1.1 Connection to PCA

Recall that PCA maximizes the trace of covariance matrix of the embedded data subject to orthogonal
projection. See, e.g., Appendix B.1. Now we draw the corresponding property of 2D-PCA. Substituting
(2) into the objective function of (8), we obtain

kY ×3 Jnk2 =

where ¯Y =Pn

k=1 Yk. Here

nXk=1

(Yk − ¯Y )(Yk − ¯Y )T# ,

kYk − ¯Y k2 = tr" nXk=1
nXk=1

1
n

(Yk − ¯Y )(Yk − ¯Y )T

is called the image covariance matrix, whose trace is maximized by 2D-PCA subject to the orthogonality
constraints (3). If m2 = 1, then Y1, . . . , Yn are indeed column vectors, in which case 2D-PCA is equivalent
to PCA.

2.2 Two-dimensional LPP

LPP and OLPP, reviewed in Appendix B.3, are linear dimensionality reduction methods using image-
as-vector representation. Their two-dimensional counterparts, using image-as-matrix representation [4,
7, 25], are presented next.

The step to construct the symmetric weight matrix W = [wij ] ∈ Rn×n is the same as that in LPP
and OLPP. We also compute the graph Laplacian L = D − W , where D ∈ Rn×n is the diagonal matrix

LPP and OLPP minimize the objective function (56) in Appendix B.4. Correspondingly, we minimize

j=1 wij for i = 1, . . . , n. See Appendix B.2 for details.

formed by elements dii =Pn

1
2

nXi,j=1

wij kYi − Yjk2 =

1
2

nXi,j=1
tr(cid:2)wij (Yi − Yj)(Yi − Yj)T(cid:3)
j 
= tr
nXi=1

nXi,j=1

wij YiY T

diiYiY T

i −

4

(10)

= tr" d2Xk=1
= tr" d2Xk=1
d2Xk=1

=

diiY(:, k, i)Y(:, k, i)T −

d2Xk=1
nXi=1
Y(:, k, :)(D − W )Y(:, k, :)T#
tr(cid:2)Y(:, k, :)LY(:, k, :)T(cid:3)

= tr(hY ×3 L, Yi[3;3]),

nXi,j=1

wij Y(:, k, i)Y(:, k, j)T

(11)

where the tensor trace in (11) is deﬁned in (4). Note that the last term (11) is a tensor generalization
of tr(Y LY T ) in (56).

We can substitute (2) into (11) and obtain

tr(hX ×1 U T ×2 V T ×3 L, X ×1 U T ×2 V T i[3;3]).

(12)

Constraints are required in the minimization of (12). For example, we can impose the orthogonality

U T U = Id1 and V T V = Id2 in (3). This option leads to the 2D-OLPP method,

minimize

U,V

tr(hY ×3 L, Yi[3;3])

subject to Y = X ×1 U ×2 V,

U T U = Id1 , V T V = Id2.

(13)



How to solve this optimization problem (13) will be discussed in a uniﬁed framework in Section 3.3.

Alternatively, we may consider concurrently maximizing

tr(hX ×1 U T ×2 V T ×3 D, X ×1 U T ×2 V T i[3;3]),

(14)

which corresponds to tr(Y DY T ) in the LPP program (54). Minimizing the ratio of (12) to (14) subject
to U T U = Id1 and V T V = Id2 is a tensor generalization of the trace ratio optimization problem [24, 34].
Rather than solving this challenging problem, we will give a workaround in a uniﬁed framework in
Section 3.3. This corresponds to the alternating algorithm in [7]. We call the resulting method 2D-LPP.
When m2 = 1, X1, . . . , Xn are indeed column vectors, and the data tensor X can be represented by
a matrix [X1, . . . , Xn], in which case 2D-LPP is reduced to the formulation of LPP (54), and 2D-OLPP
is equivalent to OLPP.

2.3 Two-dimensional NPP

Recall that NPP and ONPP, reviewed in Appendix B.4, are linear dimensionality reduction methods
that use the image-as-vector representation. Here we present their two-dimensional counterparts, using
the image-as-matrix representation [22, 26].

Firstly, we compute a weight matrix W = [wij ] ∈ Rn×n in the same way as that of NPP and ONPP.

See Appendix B.2 for a discussion. Then we consider the objective function to minimize

kY(:, :, i) −

wij Y(:, :, j)k2

(15)

kYi −

nXi=1

nXj=1

wij Yjk2 =

=

=

=

nXj=1

kY(:, k, i) −

nXi=1

wij Y(:, k, j)k2

nXj=1

kY(:, k, :) − Y(:, k, :) ×3 W k2

kY(:, k, :) ×3 (In − W )k2

nXi=1
d2Xk=1
d2Xk=1
d2Xk=1

5

= kY ×3 (In − W )k2,

(16)

which parallels the objective function of NPP and ONPP (56).

To match the format of (11), we rewrite (16) in the tensor trace form as

tr(hY ×3 H, Yi[3;3]),

where H = (In − W )T (In − W ). We can further substitute (2) into (17) and obtain

tr(hX ×1 U T ×2 V T ×3 H, X ×1 U T ×2 V T i[3;3]).

(17)

(18)

Again, one can impose the orthogonality constraints U T U = Id1 and V T V = Id2 , yielding the

2D-ONPP method. Another option is to concurrently maximizing

We would like to write (19) in tensor trace form as

kX ×1 U T ×2 V T k2.

tr(hX ×1 U T ×2 V T , X ×1 U T ×2 V T i[3;3]),

(19)

(20)

which is same as the objective function of GLRAM and CSA (7).

Minimizing the ratio (18) to (20) subject to U T U = Id1 and V T V = Id2 is a challenging tensor trace
ratio optimization problem. A workaround will be given in the uniﬁed framework in Section 3.3. This
corresponds to the alternating algorithm in [22]. The resulting method is called 2D-NPP.

Note that if m2 = 1, then X1, . . . , Xn are indeed column vectors, and the data tensor X can be repre-
sented by a matrix [X1, . . . , Xn], in which case 2D-NPP and 2D-ONPP are reduced to the formulations
of NPP (57) and ONPP (58), respectively.

2.3.1 Connection to 2D-PCA

Comparing the 2D-ONPP method to the 2D-PCA program (8), there are only two diﬀerences. Firstly,
2D-ONPP ‘minimizes’ (18), whereas 2D-PCA ‘maximizes’ (9). Both impose the orthogonality constraints
U T U = Id1 and V T V = Id2 . Secondly, the multiplication ×3(In − 1
n ) in (9) corresponds to the
multiplication ×3(In −W ) in (18). If a complete graph is used and the weights are uniformly distributed,
then wij = 1
n . In this case, the objective functions of 2D-PCA
and 2D-ONPP are identical. This is a property similar to the connection between PCA and ONPP
[14, 15].

n for i, j = 1, . . . , n, yielding W = 1

n eneT

n eneT

2.4 Two-dimensional LDA

We now present the two-dimensional counterpart of LDA [38], using image-as-matrix representation
instead of image-as-vector representation.

Suppose we are given training data matrices X1, . . . , Xn ∈ Rm1×m2. Each data sample Xi is associ-

ated with a class label c(i). For each class j = 1, . . . , c, there is an index set

Cj = {i : c(i) = j},

whose size is denoted by nj = |Cj|.

The reduced data is obtained from the bilateral transformation Yk = U T XkV for k = 1, . . . , n. The
i=1 Yi. The

Yi, and the global mean is ¯Y = 1

mean of each class j is denoted by ¯Yj = 1
within-scatter measure is deﬁned by

nPn

njPi∈Cj
cXj=1Xi∈Cj

Dw = tr(

(Yi − ¯Yj )(Yi − ¯Yj)T ),

and the between-scatter measure is

Db = tr(

cXj=1

nj( ¯Y − ¯Yj)( ¯Y − ¯Yj)T ).

6

(21)

(22)

Note that if m2 = 1, then X1, . . . , Xn are indeed column vectors and the data tensor X can be
represented by a matrix [X1, . . . , Xn], in which case Dw in (21) and Db in (22) are the same as tr(U T SwU )
and tr(U T SbU ) in LDA, where Sw and Sb are deﬁned in (59) and (60), respectively. Conceptually, the
goal is to minimize Dw and maximize Db, corresponding to minimizing tr(U T SwU ) and maximizing
tr(U T SbU ) in LDA.

To write (21) and (22) in the tensor trace form, we aggregate Y1, . . . , Yn to form a data tensor Y
such that Y(:, :, k) = Yk. For each class j, we form a data tensor Yj to store all Yi with c(i) = j. In
MATLAB-like notation, Yj = Y(:, :, c(i) == j). We also deﬁne a corresponding translated data tensor

eYj by eYj (:, :, k) = Yj(:, :, k) − ¯Yj for k = 1, . . . , nj. The relation between eYj and Yj can be written as

×3 enj = Y − Y ×3 (

) = Y ×3 (Inj −

1
nj

enj eT

nj ).

enj eT
nj
nj

Hence we have

eT
nj
nj

eYj = Yj − Yj ×3
(Yi − ¯Yj)(Yi − ¯Yj)T ) = Xi∈Cj

tr(Xi∈Cj

where Jnj = Inj − 1
nj
has been used for the last equality.

enj eT

kYi − ¯Yjk2 = keYjk2 = tr(heYj ,eYji[3;3]) = tr(hYj ×3 Jnj , Yji[3;3]),

nj . Note that since Jnj is a projection matrix, J T

nj = Jnj = J 2

nj . This property

Now we can rewrite (21) as

Dw =

cXj=1

tr(Xi∈Cj

(Yi − ¯Yj )(Yi − ¯Yj )T ) =

cXj=1

tr(hYj ×3 Jnj , Yji[3;3]) = tr(hY ×3 S, Yi[3;3]),

(23)

where in MATLAB-like notation, S(Ck, Ck) = Jnk for k = 1, . . . , c, and S(i, j) = 0 if c(i) 6= c(j).
Substituting (2) into (23), we obtain the tensor trace form of Dw as

Dw = tr(hX ×1 U T ×2 V T ×3 S, X ×1 U T ×2 V T i[3;3]).

(24)

The matrix S actually is the Laplacian matrix of a label graph with even weights [25]. To be precise,
let W = In − S with In ∈ Rn×n the identity. Then W = [wij ] ∈ Rn×n satisﬁes

wij =(cid:26) 1/nk

0

if c(i) = c(j);
otherwise.

(25)

This matrix S has rank n − c. See, for example, [10, 14, 15]. The discussion gives a connection to
2D-LPP.

Now we consider Db in (22), where

cXj=1

nj( ¯Y − ¯Yj)( ¯Y − ¯Yj)T = (

cXj=1

nj) ¯Y ¯Y T − (

cXj=1

nj ¯Yj ) ¯Y T − ¯Y (

cXj=1

nj ¯Yj)T +

nj ¯Yj ¯Y T
j

cXj=1

= −n ¯Y ¯Y T +

Hence we can rewrite (22) as

nj ¯Yj ¯Y T
j .

cXj=1

Db = −n tr( ¯Y ¯Y T ) +

cXj=1

njtr( ¯Yj ¯Y T

j ) = −nk ¯Y k2 +

njk ¯Yjk2.

cXj=1

(26)

The two terms in (26) can be written in tensor trace form as follows. First,

nk ¯Y k2 = nkY ×3

eT
n

k2 = tr(hY ×3

eneT
n
n

, Yi[3;3]).

7

Then we have

kYk2 − nk ¯Y k2 = tr(hY ×3 Jn, Yi[3;3]),

(27)

where Jn = In − eneT

n

n . Note that (27) is the same as the objective function (9) of 2D-PCA. Secondly,

njk ¯Yjk2 =

cXj=1

tr(hYj ×3

enj eT
nj
nj

cXj=1

), Yi[3;3]) = tr(hY ×3 W, Yi[3;3]),

where W = [wij ] ∈ Rn×n is deﬁned in (25). Then we have

kYk2 −

cXj=1

njk ¯Yjk2 = tr(hY ×3 S, Yi[3;3]),

(28)

where S = In − W . Note that (28) is identical to (23). Substituting (27) and (28) into (26), we obtain

Substituting (2) into (29), we obtain the tensor trace form of Db as

Db = tr(hY ×3 (Jn − S), Yi[3;3]).

Db = tr(hX ×1 U T ×2 V T ×3 (Jn − S), X ×1 U T ×2 V T i[3;3]).

(29)

(30)

To obtain U and V , one could maximize the ratio of (24) to (30) subject to U T U = Id1 and V T V =
Id2 , where Yk = U T XkV for k = 1, . . . , n. Instead of solving this challenging optimization problem, we
will solve a related problem in a uniﬁed framework in Section 3.3. This corresponds to the alternating
algorithm in [38].

2.5 An uniﬁed view

All the methods discussed in this section can be categorized into three cases.

1. The ﬁrst case is to minimize tr(hY ×3 A, Yi[3;3]). The methods are 2D-OLPP and 2D-ONPP.

2. The second case is to maximize tr(hY ×3 B, Yi[3;3]). The methods are GLRAM/CSA and 2D-PCA.

3. The last case is to minimize tr(hY ×3 A, Yi[3;3]) and to maximize tr(hY ×3 B, Yi[3;3]) concurrently.

The methods are 2D-LPP, 2D-NPP, and 2D-LDA.

Table 2: Minimization of tr(hY ×3 A, Yi[3;3]) and maximization of tr(hY ×3 B, Yi[3;3]).
Method

Matrix B

Matrix A

Ref.

GLRAM/CSA
2D-PCA
2D-OLPP
2D-LPP
2D-ONPP
2D-NPP
2D-LDA
2D-OLPP-R
2D-LPP-R
2D-ONPP-R
2D-NPP-R
2D-LDA-R

-
-

L = D − W
L = D − W

H = (In − W )T (In − W )
H = (In − W )T (In − W )

S = In − W
L − βL(r)
L − βL(r)
H − βL(r)
H − βL(r)
S − βL(r)

(12)
(12)
(18)
(18)
(24)

Jn = In − 1

n eneT

n

In

-
D
-
In

Jn − S

-
D
-
In

Jn − S

Ref.
(7)
(9)

(14)

(20)
(30)

Table 2 lists the corresponding matrices A and B and their references of all these methods. Each ‘-’
indicates that there is no matrix A or no matrix B, implying the single objective of maximization or
minimization. The methods using repulsion tensors, to be described in the next section, are also listed.
The next section will also give a uniﬁed framework for computing the projectors U and V of all these
methods.

8

3 Enhancement by repulsion tensors

Classical linear dimensionality reduction methods, such as LDA and LPP, project the data in vector form
into a lower dimensional space. There are cases that two between-class data points which are close each
other in the high dimensional space remain close after being projected to the lower dimensional space.
An enhanced dimensionality reduction technique, called the repulsion Laplaceans [16], was proposed to
address such issues. Here we develop a corresponding two-dimensional method for classiﬁcation of data
matrices.

3.1 Repulsion graph

The methodology presented here is based on the use of repulsion graphs [16]. The technique requires
two graphs that deﬁne the repulsion graph.

1. The ﬁrst is the supervised label graph G = (V, E) such that (i, j) ∈ E if data items i and j have

the same label.

2. The second is an unsupervised aﬃnity graph G(a) = (V, E(a)) such that (i, j) ∈ E(a) if data items
i and j are close to each other in the input space. In practice, we construct a kNN graph for G(a).

3. The repulsion graph is denoted by G(r) = (V, E(r)), and its edges are deﬁned by (i, j) ∈ E(r) if

and only if (i, j) ∈ E(a) and (i, j) /∈ E.

Note that all G, G(a) and G(r) are undirected and share the same vertices V = {1, . . . , n} as data
indices. The high level idea is that if items i and j are not in the same class ((i, j) /∈ E) but are close to
each other ((i, j) ∈ E(a)) we should add a penalty force to ‘repel’ them from each other in the projection
process.

We will eventually construct a graph Laplacian L(r) of the repulsion graph G(r) = (V, E(r)). Hence
ij > 0. In practice, we use the Gaussian weights
ij = 0 if (i, j) /∈ E(r) . The weights form a weight matrix

each edge (i, j) ∈ E(r) is assigned with a weight w(r)
(51); see Appendix B.2. We also let w(r)
W (r) = [w(r)

ij ] ∈ Rn×n. The repulsion Laplacian matrix is L(r) = D(r) − W (r), where D(r) = W (r)en.

This matrix L(r) has been utilized to modify linear dimensionality reduction methods to project
vector data [16]. For example, given the training data X = [x1, . . . , xn] ∈ Rm×n, the OLPP method
minimizes tr(U T XLX T U ) subject to U T U = Id (d < m), where L ∈ Rn×n is the graph Laplacian. The
improvement is achieved by incorporating L(r) into the objective function. To be precise, we minimize
tr(U T X(L − βL(r))X T U ) subject to U T U = Id, where β > 0 is a preset penalty parameter. The
enhanced method is denoted by OLPP-R. This technique can also be used to improve LDA and ONPP
in the same way; see [16] for details.

3.2 Repulsion tensor

Recall that in the common framework of the two-dimensional methods, we project data matrix Xk ∈
Rm1×m2 to Yk ∈ Rd1×d2 via Yk = U T XkV for k = 1, . . . , n. Except for GLRAM/CSA and 2D-PCA, the
methods presented in Section 2 all have an objective function to minimize in the form

tr(hX ×1 U ×2 V ×3 A, X ×1 U ×2 V i[3;3]).

(31)

Following the discussion in Section 3.1, we have a repulsion graph G(r) = (V (r), E(r)) and its Lapla-
cian L(r) = D(r) − W (r) ∈ Rn×n. Recall that the goal in 2D-LPP and 2D-OLPP, is to make neighboring
points deﬁned by G = (V, E) close to each other in the projected space. Therefore we minimize (10).
Here the objective is to repel the neighboring points deﬁned by G(r) = (V (r), E(r)). Hence we maximize

w(r)

ij kYi − Yjk2 = tr(hY ×3 L(r), Yi[3;3]).

(32)

1
2

nXi=1

9

The equation (32) can be seen from the derivation of (11). Substituting (2) into (32), we obtain

tr(hX ×1 U T ×2 V T ×3 L(r), X ×1 U T ×2 V T i[3;3]),

(33)

where hX ×1 U T ×2 V T ×3 L(r), X ×1 U T ×2 V T i[3;3] is called the repulsion tensor.

As an illustration, we incorporate the maximization of (33) into the minimization of (12). Subtracting

β·(33) from (12), we obtain

tr(hX ×1 U T ×2 V T ×3 (L − βL(r)), X ×1 U T ×2 V T i[3;3]),

(34)

which is the objective function to minimize. Here the penalty parameter β > 0 is preset. If we impose the
orthogonality constraints U T U = Id1 and V T V = Id2 , then the resulting method is called 2D-OLPP-R.
If we follow 2D-LPP to maximize (14) concurrently, then the method is called 2D-LPP-R.

In 2D-ONPP, 2D-NPP, and 2D-LDA, there is a matrix which plays the role of graph Laplacian
in 2D-LPP and 2D-OLPP. Hence the repulsion technique described above can be applied. We denote
the modiﬁed methods by 2D-ONPP-R, 2D-NPP-R, and 2D-LDA-R, which incorporate the repulsion
technique. The column ‘Matrix A’ of Table 2 lists all the matrices in the role of graph Laplacian.

3.3 The uniﬁed framework

All the methods in Table 2 can be categorized into three categories. The ﬁrst has the objective is to
minimize

(35)
subject to U T U = Id1 and V T V = Id2 . Methods in this category are 2D-OLPP, 2D-ONPP, 2D-OLPP-R
and 2D-ONPP-R. In the second case, the objective is to maximize

tr(hX ×1 U T ×2 V T ×3 A, X ×1 U T ×2 V T i[3;3])

tr(hX ×1 U T ×2 V T ×3 B, X ×1 U T ×2 V T i[3;3])

(36)

subject to U T U = Id1 and V T V = Id2 . Methods in this category are GLRAM/CSA and 2D-PCA. In the
last case, the objective is to minimize (35) and maximize (36) concurrently. One could also impose the
orthogonality constraints U T U = Id1 and V T V = Id2. However this leads to a challenging optimization
problem. A practical workaround will be discussed later in this section. Methods in the last category
are 2D-LPP, 2D-NPP, 2D-LDA, 2D-LPP-R, 2D-NPP-R, and 2D-LDA-R.

We start with the ﬁrst case. There is no closed form solution to the problem of minimizing (35)

subject to U T U = Id1 and V T V = Id2 . An alternating process to solve this problem is as follows.

For simplicity, we ﬁx U ∈ Rm1×d1 and minimize (35) in terms of V ∈ Rm2×d2. Letting Z1 = X ×1 U T

and substituting it into (35), we obtain

tr(hZ1 ×2 V T ×3 A, Z1 ×2 V T i[3;3]) = tr(

d1Xi=1

(V T Z1(i, ; , :)A)(V T Z1(i, :, :))T )

(37)

= tr(V T (

Z1(i, :, :)AZ1(i, :, :)T )V ).

d1Xi=1

Hence the minimizer of (37) subject to V T V = Id2 consists of the bottom d2 eigenvectors corresponding
to the smallest eigenvalues of

A1 =

Z1(i, :, :)AZ1(i, :, :)T .

(38)

d1Xi=1

If we take the top eigenvectors, we obtain the maximizer of (37) subject to V T V = Id2 .

Likewise, we can ﬁx V ∈ Rm2×d2 and minimize (35) in terms of U ∈ Rm1×d1. Similarly, we let

Z2 = X ×2 V T , and the minimizer subject to U T U = Id1 consists of the bottom d1 eigenvectors of

A2 =

d2Xj=1

Z2(:, j, :)AZ2(:, j, :)T .

(39)

10

We can solve the two sub-problems alternatively. The discussion leads to an alternating process to
minimize (35) subject to U T U = Id1 and V T V = Id2. The pseudo-code is given in Algorithm 1. This is
the high order orthogonal iteration (HOOI) of tensors [19, 29].

input: X ∈ Rm1×m2×n, A ∈ Rn×n.
output: projectors U ∈ Rm1×d1, V ∈ Rm2×d2.
Set initial U ← Im1 ;
repeat

Z1 ← X ×1 U T ;

A1 ←

Z1(i, :, :)AZ1(i, :, :)T ;

Compute the bottom d2 eigenvectors of A1vi = λivi;
V ← [v1, . . . , vd2];
Z2 ← X ×2 V T ;

A2 ←

Z2(:, j, :)AZ2(:, j, :)T ;

d1Xi=1

d2Xj=1

Compute the bottom d1 eigenvectors of A2ui = λiui;
U ← [u1, . . . , ud1];

until convergence or max number of iterations is met.

Algorithm 1: Alternating process #1 for U, V .

Consider Algorithm 1. Each update of U reduces the value of the objective function (35), so does the
update of V . The constraints U T U = Id1 and V T V = Id2 make the objective function (35) bounded.
Hence Algorithm 1 must converge. However, it does not guarantee the convergence to the global mini-
mum, although each sub-problem can be solved optimally. Indeed, the result depends on the initial U .
In practice, we simply set the initial U as Im1 . Note that although Im1 is not of size m1-by-d1, in the
consequent computation the U is of size m1-by-d1 and satisﬁes U T U = Id1.

The methods in the second category, i.e., 2D-PCA and GLRAM/CSA, maximize (36) subject to
U T U = Id1 and V T V = Id2 . we can just substitute −B for A in Algorithm 1. Equivalently, we can
replace A by B and use the ‘top’ eigenvectors instead of the ‘bottom’ eigenvectors.

Now we consider the last case, minimizing (35) and maximizing (36) concurrently. Following the
discussion leading to (38), we ﬁx U ∈ Rm1×d1. Then maximizing (36) in terms of V ∈ Rm2×d2 is the
same as maximizing tr(V T B1V ), where

B1 =

d1Xi=1

Z1(i, :, :)BZ1(i, :, :)T ,

(40)

with Z1 = X ×1 U T . Now the sub-problem is to minimize tr(V T A1V ) and maximize tr(V T B1V )
concurrently, where A1 and B1 are deﬁned in (38) and (40), respectively. If we enforce the orthogonality
constraint V T V = Id2 , then it is a trace ratio optimization problem [24, 34]. For a better computational
eﬃciency, we consider the related but simpler program

( minimize

V

tr(V T A1V )

subject to V T B1V = Id2 .

(41)

The minimizer V ∈ Rm2×d2 of (41) consists of the bottom d2 eigenvectors of the generalized eigenvalue
problem A1vi = λiB1vi.

Likewise, if we ﬁx V , then we will lead to the program

( minimize

U

tr(U T A2U )

subject to U T B2U = Id1 ,

11

(42)

where

B2 =

d2Xj=1

Z2(:, j, :)BZ2(:, j, :)T

(43)

with Z2 = X ×2 V T . We conclude an iterative algorithm which solves (41) and (42) alternatively. The
pseudo-code is given in Algorithm 2, where we use the same initial U in Algorithm 1.

input: X ∈ Rm1×m2×n; A, B ∈ Rn×n.
output: projectors U ∈ Rm1×d1, V ∈ Rm2×d2.
Set initial U ← Im1 ;
repeat

Z1 ← X ×1 U T ;

A1 ←

B1 ←

Z1(i, :, :)AZ1(i, :, :)T ;

Z1(i, :, :)BZ1(i, :, :)T ;

Compute the bottom d2 eigenvectors of A1vi = λiB1vi;
V ← [v1, . . . , vd2];
Z2 ← X ×2 V T ;

A2 ←

B2 ←

Z2(:, j, :)AZ2(:, j, :)T ;

Z2(:, j, :)BZ2(:, j, :)T ;

d1Xi=1
d1Xi=1

d2Xj=1
d2Xj=1

Compute the bottom d1 eigenvectors of A2ui = λiB2ui;
U ← [u1, . . . , ud1];

until convergence or max # iterations is met.

Algorithm 2: Alternating process #2 for U, V .

3.4 Unilateral versus bilateral

All the two-dimensional projections listed in Table 2 can be unilateral or bilateral. The alternating
processes given in Algorithms 1 and 2 are for bilateral projections. The unilateral case is indeed simpler.
We just need to solve one eigenvalue or generalized eigenvalue problem, and the optimal solution is
reached with just one iteration. In the literature, [4, 11, 22, 25, 36] use unilateral projections, whereas
bilateral projections are adopted by [17, 35, 37, 38].

The clear advantage of unilateral projections is their computational eﬃciency. On the other hand,
bilateral projections perform better in exploiting spatial redundancy, and therefore need fewer dimensions
for the projected space. This is important in the application of data compression [37].

Consider bilateral projections. GLRAM/CSA [37] and 2D-PCA [17] which maximize (36) usually
converge in 2 or 3 iterations in practice. This is conﬁrmed in our experiments. Other methods which
minimize (35) require more iterations.
In the face recognition experiments, we limit the number of
iterations to 5. We found that more iterations usually help little in improving the recognition rate.

3.5 Considerations for 2D-LDA & 2D-LDA-R

There is a variant of Algorithm 2, described as follows. Instead of (41), another possibility is to maximize
tr(V T B1V ) subject to V T A1V = Id2 . This option leads to computing the top d2 eigenvectors of the
generalized eigenvalue problem

Likewise, the other eigenvalue problem to solve in the alternating process is

B1vi = λiA1vi.

B2ui = λiA2ui.

12

(44)

(45)

We use this variant to compute the 2D-LDA projections, since we found that it usually achieves better
performance in face recognition. However, it results in a problem in 2D-LDA-R where the repulsion
tensor is incorporated. Note that solving the generalized eigenvalue problems (44) and (45), we need the
matrices A1 and A2 being positive deﬁnite, which is no longer guaranteed with repulsion. The situation
is even worse in the case of bilateral projections due to the iteration.

In practice, we set smaller penalty parameter β for a modest repulsion power. In addition, for bilateral
projections, we use only one iteration and in this iteration U and V are computed independently, i.e., like
computing the unilateral projections twice for U and V , respectively. These changes make 2D-LDA-R
a useful method in face recognition. However, instability occurs occasionally. See Section 4.3 for an
example in the experiment on the UMIST face images.

3.6 Pre-processing and post-processing

In LDA, LPP, NPP and their variants, it is common to pre-process the training data matrix by PCA, in
order to avoid a singular matrix in the eigenvalue computation. Likewise, in 2D-LDA, 2D-LPP, 2D-NPP
and their variants, one can pre-process the data by 2D-PCA. This can sometimes improve the stability,
explained as follows.

Consider Algorithms 1 and 2. We would like to ensure that the matrices A1, B1 ∈ Rm2×m2 and
A2, B2 ∈ Rm1×m1 are of full rank. We concentrate on A1. The discussion for B1, A2, B2 is similar. The
rank of A1 is at most d1 min{m2, rank(A)}. Hence if d1rank(A) < m2, we are guaranteed a singular A1.
It is known that in supervised mode, rank(A) ≤ n − c, where n is the number of training images and
c is the number of classes [10, 14, 15]. In practice, without the singularity problem, good performance
can be achieved with very small d1 or d2. Hence when the image size is large or the number of training
images is small, pre-processing by 2D-PCA helps improve the stability. We found that it is an issue in
the experiments on the AR database.

Another approach is to post-process the projected data by a one-dimensional method, in order to
improve data compression or recognition performance. Examples include GLRAM + SVD [37], 2D-LPP
+ PCA [4], and 2D-LDA + LDA [38]. For the sake of simplicity, we did not adopt this approach in the
experiments. The two directions open a door to numerous composite projections which deserve further
investigations.

4 Experimental results

The experiments were performed in MATLAB on a PC equipped with a four-core Intel Xeon E5504
@ 2.0GHz processor and 4GB memory. We use 4 face image databases, ORL faces [28], UMIST faces
[6], AR faces [23], and ESSEX faces [30]. We compare empirically the performance of the repulsion
methodology (see Section 3), the existing two-dimensional methods (see Section 2), and the classical
linear dimensionality methods using image-as-vector representation (see Appendix B).

We report the results of 2D-PCA, 2D-LDA, 2D-PCA-R and 2D-LDA-R, as well as the results of
2D-LPP, 2D-NPP, 2D-OLPP-R, and 2D-ONPP-R. In practice, we found that 2D-LPP and 2D-NPP are
generally better options than 2D-OLPP and 2D-ONPP. On the other hand, 2D-OLPP-R and 2D-ONPP-
R often outperform 2D-LPP-R and 2D-NPP-R. Hence we omit the results of 2D-OLPP, 2D-ONPP,
2D-LPP-R, and 2D-NPP-R in this section.

Each recognition rate reported is the average from 20 random realizations of the training images, and
the rest images are used for testing. In Tables 3–6, we list the best recognition rate and the corresponding
dimension for each method and each database. The statistics of the one-dimensional methods are from
[16], except for the NPP and LDA-R methods, whose numbers are obtained from the recent experiments.

4.1 Face image databases

The ORL (Olivetti Research Laboratory) face database [28] contains images of 40 subjects. Each subject
has 10 grayscale images of size 112-by-92 with various facial expressions (smiling/non-smiling, etc.). In
total there are 400 images. Sample face images of the ﬁrst two individuals are displayed in Figure 1.

13

Figure 1: Sample ORL face images.

The UMIST database contains grayscale images of 20 subjects. We use a set of cropped images. Each
subject has from 19 to 48 images of size 112-by-92. In total there are 575 images. Figure 2 gives sample
images of the ﬁrst individual.

Figure 2: Sample UMIST face images.

We use the AR database [23] which contains 1,008 grayscale 768-by-576 images of 126 subjects, 8
images per subject. These images were cropped and downsized to be 224-by-184 before the recognition
is performed in the experiments. Figure 3 presents the sample images of the ﬁrst two subjects.

Figure 3: Sample AR face images.

We also use the ESSEX database [30]. The 95’ version contains 1,440 color 200-by-180 images of 72
subjects. Each subject has 20 images. We converted these color images to grayscale so they can be
represented as matrices.

4.2 Performance sensitivity to repulsion

There are two parameters to select for the repulsion technique. One is the number k of neighbors per
vertex, used for constructing a repulsion graph. The other is the penalty parameter β, which determines
the strength of the repulsion term.

To examine the sensitivity of the recognition performance to these parameters, we conducted two
experiments as follows. We use the 2D-ONPP-R and 2D-OLPP-R methods, in both unilateral and
bilateral projections. For unilateral projection, we set d2 = 10 and the result is marked by ‘(U)’. For
bilateral projection, we set d1 = d2 = 10 and the result is marked by ‘(B)’. We use the images in the
ORL database, and set the number of training images per class as 5. In one experiment, we set k = 6
and β = 0.3, 0.35, . . . , 1. In the other experiment, we set k = 1, 2, . . . , 20 and β = 0.5. The results are
shown in Figure 4.

14

0.98

0.96

0.94

0.92

0.9

0.88

0.86

0.84

0.82

e

t

a
r
 

n
o

i
t
i

n
g
o
c
e
r

0.8

 
0.3

ORL −− accuracy vs. β

 

2D−OLPP−R (U)
2D−ONPP−R (U)
2D−OLPP−R (B)
2D−ONPP−R (B)

0.4

0.5

0.6

β

0.7

0.8

0.9

1

0.965

0.96

0.955

0.95

0.945

e

t

a
r
 

n
o

i
t
i

n
g
o
c
e
r

ORL −− accuracy vs. k

 

2D−OLPP−R (U)
2D−ONPP−R (U)
2D−OLPP−R (B)
2D−ONPP−R (B)
14

16

18

20

0.94

 

2

4

6

8

12

10
k

Figure 4: Sensitivity of performance to varying β (left) and k (right) for the ORL database.

It can be observed from Figure 4 that the recognition rate changes modestly as long as β are k are

large enough.

4.3 Face recognition results

We report the results for 4 databases: ORL, UMIST, AR, and ESSEX. Regarding the repulsion parameters,
we use k = 6 in all cases. In both 2D-OLPP-R and 2D-ONPP-R, we set β = 0.5, but for the ESSEX
database, we set β = 1.0. As discussed in Section 3.5, we need a smaller β for 2D-LDA-R, in which we
set β = 0.2.

The results for the ORL database are displayed in Figure 5, where the number of training images
per class is set as 5. The left plot shows the recognition rates with the unilateral projections with the
column dimension d2 = 2, 4, . . . , 20, whereas the right plot shows the recognition rates with the bilateral
projections with the row/column dimension d1 = d2 = 2, 4, . . . , 20.

ORL −− # train−per−class=5

 

ORL −− # train−per−class=5

 

2D−PCA
2D−LDA
2D−LPP
2D−NPP
2D−LDA−R
2D−OLPP−R
2D−ONPP−R

10

12
unilateral d
2

14

16

18

20

2D−PCA
2D−LDA
2D−LPP
2D−NPP
2D−LDA−R
2D−OLPP−R
2D−ONPP−R

8

10

12
bilateral d
=d
2
1

14

16

18

20

0.65
 
2

4

6

8

0.2
 
2

4

6

Figure 5: Accuracy versus dimension for the ORL database; unilateral projection (left) and bilateral
projection (right).

Two observations from Figure 5 can be made.

1. In both settings of unilateral and bilateral projections, 2D-LDA-R improves 2D-LDA, and 2D-

OLPP-R and 2D-ONPP-R outperform 2D-LPP and 2D-NPP handsomely.

2. 2D-OLPP-R is the best option for the ORL database, whereas 2D-ONPP-R is the close runner-up.

15

1

0.95

0.9

0.85

0.8

0.75

0.7

e

t

a
r
 

n
o

i
t
i

n
g
o
c
e
r

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

e

t

a
r
 

n
o

i
t
i

n
g
o
c
e
r

Table 3 lists the best recognition rates and the corresponding dimensions, using the 7 two-dimensional
(2D) methods and their one-dimen-sional (1D) peers. On average, the 1D methods and 2D methods
are comparable in performance. However, the 1D methods with repulsion Laplaceans perform slightly
better than the 2D methods with repulsion tensors.

Table 3: Best achieved error rates and the corresponding dimensions for the ORL database.

1D Method

method

# dim.

error method

PCA
LDA
LPP
NPP
LDA-R
OLPP-R
ONPP-R

90
65
60
15
70
55
90

5.12% 2D-PCA
6.95% 2D-LDA
10.8% 2D-LPP
9.50% 2D-NPP
2.90% 2D-LDA-R
2.82% 2D-OLPP-R
3.40% 2D-ONPP-R

2D Method
unilateral

bilateral

# dim.

10
2
2
2
2
18
18

error # dim.
5.10%
4.15%
7.60%
7.53%
4.23%
3.20%
4.03%

16
8
10
10
6
10
10

error
4.60%
10.6%
22.3%
17.3%
3.78%
3.55%
3.50%

Figure 6 displays the results for the UMIST database, where the number of training images is set as
10. Table 4 lists the best recognition rates and the corresponding dimensions. The observations made
previously for the ORL image set are also valid here, except for that the performance of the 2D-LDA-R
bilateral projection deteriorates quickly while the dimension increases. An explanation of this instability
is given in Section 3.5. Among the two-dimensional methods, both 2D-OLPP-R and 2D-ONPP-R are
the apparent winners for the UMIST and ORL databases.

UMIST −− # train−per−class=10

UMIST −− # train−per−class=10

1

0.95

0.9

0.85

0.8

0.75

e

t

a
r
 

n
o

i
t
i

n
g
o
c
e
r

0.7
 
2

4

6

8

 

2D−PCA
2D−LDA
2D−LPP
2D−NPP
2D−LDA−R
2D−OLPP−R
2D−ONPP−R

1

0.9

0.8

0.7

0.6

0.5

0.4

e

t

a
r
 

n
o

i
t
i

n
g
o
c
e
r

 

2D−PCA
2D−LDA
2D−LPP
2D−NPP
2D−LDA−R
2D−OLPP−R
2D−ONPP−R

10

12
unilateral d
2

14

16

18

20

 
2

4

6

8

10

12
=d
bilateral d
1
2

14

16

18

20

Figure 6: Accuracy versus dimension for the UMIST database; unilateral projection (left) and bilateral
projection (right).

The results for the AR database are illustrated in Figure 7, where we set the number of training images
as 4. Note that except for 2D-PCA, we pre-process the data by 2D-PCA, explained in Section 3.6. Table 5
lists the best recognition rates and the corresponding dimensions. 2D-ONPP-R and 2D-OLPP-R are still
the best two-dimensional methods in performance. In addition, 2D-LDA-R with unilateral projection
performs as good in this case. It is interesting to note that 2D-LPP and 2D-NPP yield very similar
results. Also note that PCA and 2D-PCA does not perform that well for this case.

Figure 8 displays the results for the ESSEX database, where we set the number of training images as
10. Table 6 lists the best recognition rates and the corresponding dimensions. In this case, 2D-OLPP-R
outperforms the other methods, including all 1D methods. This database is the hardest one tested in
[16]. It hints the potential of the repulsion tensors for challenging classiﬁcation tasks.

16

Table 4: Best achieved error rates and the corresponding dimensions for the UMIST database.

1D Method

method

# dim.

error method

PCA
LDA
LPP
NPP
LDA-R
OLPP-R
ONPP-R

65
30
10
15
95
30
15

4.12% 2D-PCA
3.44% 2D-LDA
4.03% 2D-LPP
4.56% 2D-NPP
1.62% 2D-LDA-R
0.93% 2D-OLPP-R
1.45% 2D-ONPP-R

2D Method
unilateral

bilateral

# dim.

4
2
4
2
2
16
2

error # dim.
2.77%
1.67%
5.29%
5.29%
1.57%
1.79%
2.23%

6
8
8
8
6
16
6

error
3.67%
1.07%
1.48%
8.66%
3.44%
1.48%
2.42%

AR −− # train−per−class=4

 

1

0.9

0.8

0.7

0.6

0.5

0.4

e

t

a
r
 

n
o

i
t
i

n
g
o
c
e
r

AR −− # train−per−class=4

 

2D−PCA
2D−LDA
2D−LPP
2D−NPP
2D−LDA−R
2D−OLPP−R
2D−ONPP−R

2D−PCA
2D−LDA
2D−LPP
2D−NPP
2D−LDA−R
2D−OLPP−R
2D−ONPP−R

0.95

0.9

0.85

0.8

0.75

0.7

0.65

0.6

0.55

e

t

a
r
 

n
o

i
t
i

n
g
o
c
e
r

0.5
 
2

4

6

8

10

12
unilateral d
1

14

16

18

20

 
2

4

6

8

10

12
bilateral d
=d
2
1

14

16

18

20

Figure 7: Accuracy versus dimension for the AR database; unilateral projection (left) and bilateral
projection (right).

Table 5: Best achieved error rates and the corresponding dimensions for the AR database.

1D Method

method

# dim.

error method

PCA
LDA
LPP
NPP
LDA-R
OLPP-R
ONPP-R

100
100
15
20
85
75
45

28.6% 2D-PCA
10.3% 2D-LDA
7.12% 2D-LPP
7.60% 2D-NPP
5.98% 2D-LDA-R
3.96% 2D-OLPP-R
4.04% 2D-ONPP-R

2D Method
unilateral

bilateral

# dim.

20
6
6
6
6
12
12

error # dim.
27.8%
6.31%
7.04%
7.24%
5.92%
6.61%
6.61%

20
8
8
8
14
18
12

error
28.3%
7.03%
9.10%
8.66%
7.25%
6.66%
6.76%

17

ESSEX −− # train−per−class=10

ESSEX −− # train−per−class=10

0.95

0.9

0.85

0.8

0.75

e
t
a
r
 
n
o
i
t
i
n
g
o
c
e
r

0.7
 
2

4

6

8

 

2D−PCA
2D−LDA
2D−LPP
2D−NPP
2D−LDA−R
2D−OLPP−R
2D−ONPP−R

1

0.9

0.8

0.7

0.6

0.5

0.4

e
t
a
r
 
n
o
i
t
i
n
g
o
c
e
r

 

2D−PCA
2D−LDA
2D−LPP
2D−NPP
2D−LDA−R
2D−OLPP−R
2D−ONPP−R

10

12
unilateral d
2

14

16

18

20

 
2

4

6

8

10

12
=d
bilateral d
1
2

14

16

18

20

Figure 8: Accuracy versus dimension for the ESSEX database; unilateral projection (left) and bilateral
projection (right).

Table 6: Best achieved error rates and the corresponding dimensions for the ESSEX database.

1D Method

method

# dim.

error method

PCA
LDA
LPP
NPP
LDA-R
OLPP-R
ONPP-R

65
55
10
15
20
25
65

15.1% 2D-PCA
61.6% 2D-LDA
15.3% 2D-LPP
17.3% 2D-NPP
10.4% 2D-LDA-R
12.5% 2D-OLPP-R
9.88% 2D-ONPP-R

2D Method
unilateral

bilateral

# dim.

2
2
2
2
2
2
2

error # dim.
10.2%
6.38%
10.2%
11.1%
6.72%
7.01%
10.2%

8
8
8
6
6
4
4

error
11.7%
11.3%
16.0%
15.2%
11.4%
6.35%
11.2%

In addition to the attractive recognition rates by 2D-OLPP-R and 2D-ONPP-R. It is worth noting
that the performance of 2D-OLPP-R and 2D-ONPP-R is very insensitive to the variation of the reduced
dimension in all our tests. This is an appealing feature, especially considering that in real applications
we may not have the labels of test images.

4.4

1D methods versus 2D methods

One-dimensional (1D) methods use the image-as-vector representation, whereas two-dimensional (2D)
methods use the image-as-matrix representation. This fundamental diﬀerence governs the computation.
Whether 2D methods or 1D methods are faster depends on several factors. For example, size of the
images, number of training faces, how the algorithms are implemented, and the numerical libraries used,
etc.

The high level idea is that 1D methods, which vectorize images, need to solve a much larger eigenvalue
problem. On the other hand, the computation of the 2D methods is often dominated by forming the
eigenvalue problem, i.e., computing (38), (39), (40), and (43).
In practice, this part is usually less
expensive than the eigenvalue computation of the 1D method. Indeed, it has been reported that 2D
methods are more eﬃcient than their 1D counterparts, e.g., [4, 25, 36, 37, 38]. We have also observed
this property in our experiments. Note however that one can downsize the image matrices to reduce the
cost of the eigenvalue computation.

18

5 Conclusions and future work

We have developed a methodology with repulsion tensors to enhance the two-dimensional projection
methods for classiﬁcation.
It can be regarded as a multilinear generalization of a technique called
repulsion Laplaceans [16]. The key idea is improve the projection by repelling data items which are from
diﬀerent classes but close to each other in the input space. The experiments on face recognition exhibit
signiﬁcant improvement of recognition performance by the proposed technique.

Some two-dimensional projection methods can be formulated as a tensor trace ratio optimization
problem. Instead of solving this problem directly, we use Algorithm 2 as an eﬃcient workaround. It
may be worth developing eﬃcient algorithms for the tensor trace ratio optimization problem.

All the methods presented in this paper are linear. Hence it may fail to discover the underlying
latent structure if the image manifold is highly nonlinear. A research topic is how to learn the nonlinear
structure of images or higher order tensor objects in the supervised setting.

Acknowledgement

Supported by grant NSF/CCF-1318597, this work was mainly done in 2012, while the author was a
research associate in the University of Minnesota. Thanks to Yousef Saad for his support and his help
in editing the manuscript.

References

[1] B. W. Bader and T. G. Kolda. Algorithm 862: MATLAB tensor classes for fast algorithm proto-

typing. ACM Transactions on Mathematical Software, 32(4):635–653, December 2006.

[2] M. Belkin and P. Niyogi. Laplacian eigenmaps for dimensionality reduction and data representation.

Neural Comput., 15(6):1373–1396, 2003.

[3] P. N. Bellhumeur, J. P. Hespanha, and D. J. Kriegman. Eigenfaces vs. Fisherfaces: Recognition
using class speciﬁc linear projection. IEEE Trans. Pattern Anal. and Mach. Intell., 19(7):711–720,
1997.

[4] S. Chen, H. Zhao, M. Kong, and B. Luo. 2d-lpp: A two-dimensional extension of locality preserving

projections. Neurocomput., 70, 2007.

[5] R. A. Fisher. The use of multiple measurements in taxonomic problems. Annals of Eugenics,

7(2):179–188, 1936.

[6] D. B. Graham and N. M. Allinson. Face recognition: From theory to applications. In H. Wechsler,
P. J. Phillips, V. Bruce, F. Fogelman-Soulie, and T. S. Huang, editors, NATO ASI Series F,
Computer and Systems Sciences, Vol. 163, pages 446–456, 1998.

[7] X. He, D. Cai, and P. Niyogi. Tensor subspace analysis.

In Advances in Neural Information

Processing Systems 18 (NIPS), 2005.

[8] X. He, D. Cai, S. Yan, and H.-J. Zhang. Neighborhood preserving embedding. In IEEE International

Conference on Computer Vision (ICCV), pages 1208–1213, 2005.

[9] X. He and P. Niyogi. Locality preserving projections. In Advances in Neural Information Processing

Systems 16 (NIPS), 2003.

[10] X. He, S. Yan, Y. Hu, P. Niyogi, and H.-J. Zhang. Face recognition using Laplacianfaces. IEEE

Trans. Pattern Anal. and Mach. Intell., 27(3):328–340, 2005.

[11] X.-Y. Jing, H.-S. Wong, and D. Zhang. Face recognition based on 2D Fisherface approach. Pattern

Recognition, 39(4):707–710, 2006.

19

[12] I.T. Jolliﬀe. Principal Component Analysis. Springer, 2nd edition, 2002.

[13] E. Kokiopoulou, J. Chen, and Y. Saad. Trace optimization and eigenproblems in dimension reduc-

tion methods. Numer. Linear Algebra Appl., 18(3):565–602, 2011.

[14] E. Kokiopoulou and Y. Saad. Orthogonal neighborhood preserving projections. In The 5th IEEE

International Conference on Data Mining (ICDM), pages 234–241, 2005.

[15] E. Kokiopoulou and Y. Saad. Orthogonal neighborhood preserving projections: A projection-based
dimensionality reduction technique. IEEE Trans. Pattern Anal. and Mach. Intell., 29(12):2143–
2156, 2007.

[16] E. Kokiopoulou and Y. Saad. Enhanced graph-based dimensionality reduction with repulsion

Laplaceans. Pattern Recognition, 42(11):2392–2402, 2009.

[17] H. Kong, L. Wang, E. K. Teoh, X. Li, J.-G. Wang, and R. Venkateswarlu. Generalized 2D principal
component analysis for face image representation and recognition. Neural Networks, 18(5-6):585–
594, 2005.

[18] L. D. Lathauwer, B. D. Moor, and J. Vandewalle. A multilinear singular value decomposition.

SIAM J. Matrix Anal. Appl., 21(4):1253–1278, 2000.

[19] L. D. Lathauwer, B. D. Moor, and J. Vandewalle. On the best rank-1 and rank-(r1, r2, ..., rn)

approximation of higher-order tensors. SIAM J. Matrix Anal. Appl., 21(4):1324–1342, 2000.

[20] M. Li and B. Yuan. A novel statistical linear discriminant analysis for image matrix: Two-
dimensional Fisherfaces. In The 8th International Conference on Signal Processing (ICSP), pages
1419–1422, 2004.

[21] M. Li and B. Yuan. 2D-LDA: A statistical linear discriminant analysis for image matrix. Pattern

Recogn. Lett., 26(5):527–532, 2005.

[22] Z. Li and M. Du. 2d-npp: An extension of neighborhood preserving projection. In IEEE Interna-

tional Conference on Computational Intelligence and Security, pages 410–414, 2007.

[23] A. M. Mart´ınez and A. C. Kak. PCA versus LDA. IEEE Trans. Pattern Anal. and Mach. Intell.,

23(2):228–233, 2001.

[24] T. T. Ngo, M. Bellalij, and Y. Saad. The trace ratio optimization problem for dimensionality

reduction. SIAM J. Matrix Anal. Appl., 31(5):2950–2971, 2010.

[25] B. Niu, Q. Yang, S. C. K. Shiu, and S. K. Pal. Two-dimensional Laplacianfaces method for face

recognition. Pattern Recognition, 41:3237–3242, 2008.

[26] C.-X. Ren and D.-Q. Dai. 2d-onpp: Two dimensional extension of orthogonal neighborhood pre-
serving projections for face recognition. In Chinese Conference on Pattern Recognition (CCPR),
pages 1–6, 2008.

[27] S. T. Rowies and L. K. Saul. Nonlinear dimensionality reduction by locally linear embedding.

Science, 290:2323–2326, 2000.

[28] F. S. Samaria and A. C. Harter. Parameterisation of a stochastic model for human face identiﬁcation.
In Proceedings of 2nd IEEE Workshop on Applications of Computer Vision, pages 138–142, 1994.

[29] B. N. Sheeban and Y. Saad. Higher order orthogonal iteration of tensors (HOOI) and its relation
to PCA and GLRAM. In SIAM International Conference on Data Mining (SDM), pages 355–365,
2007.

[30] L. Spacek. University of ESSEX face database, 2002. http://cswww.essex.ac.uk/mv/allfaces/index.html.

20

[31] M. Turk and A. Pentland. Eigenfaces for recognition. Journal of Cognitive Neuroscience, 3(1):71–86,

1991.

[32] M. Turk and A. Pentland. Face recognition using eigenfaces. In IEEE Computer Society Conference

on Computer Vision and Pattern Recognition (CVPR), pages 586–591, 1991.

[33] M. A. O. Vasilescu and D. Terzopoulos. Multilinear subspace analysis of image ensembles. In Proc.

of the European Conf. on Computer Vision (ECCV), pages 447–460, 2002.

[34] H. Wang, S. Yan, D. Xu, X. Tang, and T. S. Huang. Trace ratio vs. ratio trace for dimensionality
reduction. In IEEE Computer Society Conference on Computer Vision and Pattern Recognition
CVPR, 2007.

[35] D. Xu, S. Yan, L. Zhang, S. Lin, H.-J. Zhang, and T. S. Huang. Reconstruction and recognition
of tensor-based objects with concurrent subspaces analysis. IEEE Trans. Circuits and Systems for
Video Technology, 18(1):36–47, Jan. 2008.

[36] J. Yang, D. Zhang, A. F. Frangi, and J.-y. Yang. Two-dimensional PCA: A new approach to
appearance-based face representation and recognition. IEEE Trans. Pattern. Anal. Mach. Intell.,
26(1):131–137, 2004.

[37] J. Ye. Generalized low rank approximations of matrices. J. of Machine Learning, 61:167–191, 2005.

[38] J. Ye, R. Janardan, and Q. Li. Two-dimensional linear discriminant analysis. In Advances in Neural

Information Processing Systems 17 (NIPS), 2004.

A Basics of tensors

An rth order tensor is an array of r indices. A vector is a ﬁrst order tensor, and a matrix is a second
order tensor. Diﬀerent dimensions are also called modes. To facilitate the presentation, we will illustrate
with third and fourth order tensors with speciﬁc modes. However, the deﬁnitions for higher order tensors
in general modes can be straightforwardly extended. The readers are referred to [1] for details.

Let A, B ∈ RI×J×K be third order tensors, where I, J, K are positive integers. The inner product

hA, Bi of tensors A, B is deﬁned by

hA, Bi =

IXi=1

JXj=1

KXk=1

aijkbijk.

(46)

Two tensors A, B are orthogonal to each other if hA, Bi = 0. The Frobenius norm of a tenor A is deﬁned
by

(47)

kAk =phA, Ai.

We can transform a higher order tensor to a matrix by merging dimensions and rearranging the

elements. This is called ‘unfolding’ [18], ‘ﬂattening’ [33], or ‘matricizing’ [1] a tensor.

For example, given a third order tensor A = [aijk] ∈ RI×J×K, we can have

A(1) = [a(1)
A(2) = [a(2)
A(3) = [a(3)

ip ] ∈ RI×JK, aijk = a(1)
jp ] ∈ RJ×IK, aijk = a(2)
kp ] ∈ RK×IJ , aijk = a(3)

ip , p = j + (k − 1)J,

jp , p = k + (i − 1)K,

kp , p = i + (j − 1)I,

where by default we have used the forward cyclic ordering. The other option of ordering is backward
cyclic [1]. We use A(1;2,3) and A(1;3,2) to specify the forward and backward cyclic orderings, respectively.
Here is another example to matricize a fourth order tensor A = [aijkh] ∈ RI×J×K×H as

A(1,2;3,4) = [a(1,2;3,4)

pq

], aijkh = a(1,2;3,4)

pq

,

(48)

21

where p = i + (j − 1)I and q = k + (h − 1)K.

The mode-3 product of a tensor A ∈ RI×J×K times a matrix U ∈ RH×K is deﬁned by

(A ×3 U )(i, j, h) =

aijkuhk.

KXk=1

The tensor-matrix products in other modes are deﬁned similarly.

The mode-[3; 3] contracted product of two third order tensors A ∈ RI1×J1×K and B ∈ RI2×J2×K is

deﬁned by

hA, Bi[3;3](i1, j1, i2, j2) =

ai1i2kbi2j2k,

KXk=1

where the ﬁrst 3 in [3; 3] indicates the third mode of A, and the second 3 refers to the third mode of B.
The deﬁnition can be generalized to diﬀerent modes and multiple modes [1].

A useful property is that given A ∈ RI×J×K, we let B = hA, Ai[3;3], and then have

kAk2 = tr(B(1,2;3,4)),

(49)

where B(1,2;3,4) is the result of matricization of B. See (48) for the deﬁnition. With the tensor trace
notation introduced in Section 2, we can write (49) as kAk2 = tr(hA, Ai[3;3]), which is the high order
generalization of kAk = tr(AAT ).

B Image-as-vector methods

We review the linear dimensionality reduction methods which transform X = [x1, x2, . . . , xn] ∈ Rm×n
into Y = [y1, y2, . . . , yn] ∈ Rd×n (d < m) by a linear mapping Y = U T X in order to preserve some
properties. This matrix U can be applied to project test data for recognition tasks.
If the original
numerical data items, such as face images, are not one-dimensional, they are converted to column
vectors x1, x2, . . . , xn ∈ Rm. The mapping is called orthogonal, if U ∈ Rm×d consists of orthonormal
columns, i.e., U T U = Id.

B.1 Principal component analysis

Principal Component Analysis (PCA) performs an orthogonal mapping Y = U T X to maximize the
variance of the projected vectors. More precisely, the objective function to maximize is

kyi − ¯yk2 = kY − ¯yeT

n k2 = kY − (

1
n

Y en)eT

n k2 = kU T X(In −

1
n

eneT

n )k2

(50)

nXi=1

nPn

subject to U T U = Id, where ¯y = 1
Hence the maximizer of (50) is the left d singular vectors of X(In − 1
d singular values, or equivalently the top d eigenvectors of X(In− 1
n eneT
is a projection matrix and therefore J 2

j=1 yj is the mean and en ∈ Rn is the column vector of ones.
n ) corresponding to the largest
n eneT
n

n eneT
n )X T . Note that Jn := In− 1

n = Jn = J T
n .

In the methods described subsequently in this appendix, it is common to pre-process the training
data matrix X by PCA, in order to avoid a singular matrix in the eigenvalue computation. By abuse of
notation, we also denote by X the resulting matrix preprocessed by PCA.

B.2 Aﬃnity graph

Several dimensionality reduction methods utilize an aﬃnity graph G = (V, E), where V = {1, . . . , n}
consists of indices of data items. Data item j is deemed related to data item i if (i, j) ∈ E. If the input
data are unsupervised, one may construct a kNN graph or an ǫ-graph for an aﬃnity graph. The graph
is made undirected if symmetry is desired. In supervised learning, each data item is associated with a
class label. For example, in face recognition, each training image is of subject. A label graph G = (V, E)

22

is deﬁned by that (i, j) ∈ E if data items i and j have the same label, i.e., in the same class. It has been
observed that supervised graphs often outperform their unsupervised peers in the recognition tasks.

Each edge (i, j) ∈ E is associated with a weight wij as the measure of inﬂuence between the two

neighboring points i and j. A popular choice is the Gaussian weights:

wij =(cid:26) e−kxi−xj k2/t

0

if (i, j) ∈ E,
otherwise,

(51)

where t > 0 is some constant. Alternatively, we can use the binary weights from driving t → 0.
It
is common to employ an undirected aﬃnity graph when the Gaussian weights or the induced binary
weights are used.

Another weighting scheme, proposed in the Locally Linear Embedding (LLE) [27], is from minimizing

the function

(52)



minimize

wij

kxi −

wij xjk2

subject to wij = 0 for (i, j) /∈ E,
j=1 wij = 1 for all i.

nXj=1

nXi=1
Pn

subject to wij = 0 if (i, j) /∈ E, and Pn

j=1 wij = 1 for i = 1, . . . , n. The minimizer of (52) is obtained
from solving n symmetric linear systems. See [27] for more information. Note that the resulting weights
are usually asymmetric and can be negative. The discussion on the aﬃnity graph and the weighting
scheme applies not only to this appendix but also to Sections 2 and 3.

B.3 Locality preserving projection

Consider the objective function to minimize: 1
i,j=1 wij kyi−yjk2, where wij ’s are non-negative weights,
e.g., the Gaussian weights (51). In what follows we need the weight matrix W = [wij ] ∈ Rn×n being
j=1 wij, and denote the graph

symmetric. Let D ∈ Rn×n be the diagonal matrix formed by dii = Pn

Laplacian by L = D − W . After some algebra, we have

2Pn

1
2

nXi,j=1

wij kyi − yjk2 = tr(Y LY T ).

(53)

Laplacian eigenmaps [2] is a nonlinear dimensionality reduction method that minimizes (53) subject
to Y DY T = Id and Y Den = 0. Locality Preserving Projection (LPP) [9, 10] also minimizes (53), but
imposes the linear projection constraint Y = U T X and solves

( minimize

U

tr(U T XLX T U )

subject to U T XDX T U = Id.

This is equivalent to solving the generalized eigenvalue problem

The minimizer of the program (54) is

XLX T ui = λiXDX T ui.

U = [u1, . . . , ud] ∈ Rn×d,

(54)

(55)

where u1, . . . , ud are the eigenvectors corresponding to the smallest generalized eigenvalues of (55). Note
that the program (54) can be regarded as a workaround to minimize tr(U T XLX T U ) and maximize
tr(U T XDX T U ) concurrently.

Another option is to replace the constraint U T XDX T U = Id in (54) by simply the orthogonal
mapping, i.e., U T U = Id, and then the minimizer U of (53) consists of the d eigenvectors of XLX T ,
corresponding to the d smallest eigenvalues. We call this method the Orthogonal Locality Preserving
Projection (OLPP) [15, 16]. In practice, we use the supervised label graph and the Gaussian weights
(51) for both LPP and OLPP.

23

B.4 Neighborhood preserving projection

The nonlinear dimensionality reduction method LLE [27] minimizes

kyi −

nXi=1

nXj=1

wij yjk2 = kY − Y W T k2

subject to Y Y T = Id and Y en = 0.

One can impose the linear projection Y = U T X and solve

( minimize

U

kU T X(In − W )T k2

subject to U T XX T U = Id.

(56)

(57)

We call the resulting method Neighborhood Preserving Projection (NPP) [8]. The minimizer of (57)
consists of the d eigenvectors corresponding to the d smallest generalized eigenvalues of

X(In − W )T (In − W )X T ui = λiXX T ui.

A related method, Orthogonal Neighborhood Preserving Projections (ONPP) [14, 15], solves

( minimize

U

subject to U T U = Id,

kU T X(In − W )T k2

(58)

which replaces U T XX T U = Id in (57) by the U T U = Id. The solution is formed by the d left singular
vectors of X(In − W )T corresponding to the d smallest singular values, or equivalently the bottom d
eigenvectors of X(In − W )T (In − W )X T . Note that both NPP and ONPP do not need weights being
symmetric. Therefore, it is common to adopt the weighting scheme (52) of LLE.

It is interesting to note that (In − W )T (In − W ) in NPP and ONPP plays the role of the Laplacian
matrix L in LPP and OLPP [13]. This observation is important to the repulsion techniques in [16] and
in this paper.

B.5 Linear discriminant analysis

The method of Linear Discriminant Analysis (LDA) [3, 5] can be formulated as follows. Suppose we are
given high dimensional data X = [x1, x2, . . . , xn] ∈ Rm×n with each data sample xi associated with a
class label c(i). Let

Cj = {i : c(i) = j}

be the index set of all data items in class j, and nj = |Cj| be the size of class j. The mean of each class
j is denoted by ¯xj = 1
i=1 xi. The within-scatter matrix of
X is deﬁned by

xi, and the global mean is ¯x = 1

njPi∈Cj

and the between-scatter matrix of X is

Sw =Xj Xi∈Cj
Sb =Xj

nPn

(xi − ¯xj)(xi − ¯xj )T ,

nj(¯x − ¯xj )(¯x − ¯xj )T .

(59)

(60)

Since the lower dimensional data are obtained by a linear mapping Y = U T X ∈ Rd×n, the between-
scatter matrix and within-scatter matrix of Y are U T SwU and U T SbU , respectively.

We would like to maximize tr(U T SbU ) and minimize tr(U T SwU ) in some way. It is common to solve

( maximize

U

tr(U T SbU )

subject to U T SwU = Id,

24

(61)

The maximizer of (61) is U = [u1, . . . , ud], the d largest generalized eigenvalues of

Sbui = λiSwui.

Note that LPP can be regarded a generalization of LDA. See [10] for a discussion.

In comparison, the projections of LPP, OLPP, NPP, and ONPP can be performed in either supervised
or unsupervised mode, whereas the PCA projection is unsupervised and the LDA projection is supervised.

25

