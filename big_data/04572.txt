6
1
0
2

 
r
a

 

M
5
1

 
 
]
L
M

.
t
a
t
s
[
 
 

1
v
2
7
5
4
0

.

3
0
6
1
:
v
i
X
r
a

On the exact recovery of sparse signals via conic relaxations

Hongbo Dong∗

March 16, 2016

Abstract

In this note we compare two recently proposed semideﬁnite relaxations for the sparse

linear regression problem by Pilanci, Wainwright and El Ghaoui ("Sparse learning via boolean
relaxations", 2015) and Dong, Chen and Linderoth (”Relaxation vs. Regularization: A conic
optimization perspective of statistical variable selection", 2015). We focus on the cardinality
constrained formulation, and prove that the relaxation proposed by Dong, etc. is theoreti-
cally no weaker than the one proposed by Pilanci, etc. Therefore any sufﬁcient conditions
of exact recovery derived by Pilanci can be readily applied to the Dong’s relaxation, includ-
ing their results on high probability recovery for Gaussian ensemble. Finally we provide
empirical evidence that Dong’s relaxation requires much fewer observations to guarantee
the recovery of true support.

1 Two convex relaxations for sparse linear regression

Given a collection of observed sample points (xi , yi ) ∈ Rp × Y , the goal of a sparse learning
task is to learn a linear function x 7→ βT x that is then used to predict an outcome of y ∈ Y for
future/unseen data, where β is restricted to have a small number of nonzero entries). Such a
task can be modeled as the following cardinality constrained optimization problem

1

n

n

Xi=1

f ¡βT xi ; yi¢ .

min
β∈Rp ,
kβk0≤k

(1)

With the cardinality constraint, (1) is usually highly nonconvex and difﬁculty to solve to global
optimality. The authors in [2] considered the following regularized version,

1

n

n

Xi=1

min
β∈Rp ,
kβk0≤k

f ¡βT xi ; yi¢+ ρkβk2

2.

(2)

∗Department of Mathematics and Statistics, Washington State University, Pullman, WA 99163

1

One of the key results in [2] shows that (2) can be equivalently formulated as minimizing a con-
vex function over a subset of binary vectors,

(3)

1

2ρ

v T X D(z)X T v −

min
z∈{0,1}p,
Pj z j ≤k

max

v∈Rn(−
|

n

Xi=1

,

f ∗(vi ; yi ))
}

G(z)

{z

where G(z) is convex because as it is the max function of inﬁnite many linear functions, and

f ∗(v, y) := supt∈R©st − f (t , y)ª is the conjugate function of f (·; y).

In this note we focus on the important special case of sparse linear regression, i.e., we consider
the following cardinality-constrained quadratic program,

νℓ0 := min

β

1

2kX β− yk2
2 +

1

2

ρkβk2

2, s.t . kβk0 ≤ k,

(ℓ0 : c ar d)

The authors of [2] further proposed to relax the binary condition z ∈ {0, 1}p in (3) to z ∈ [0, 1]p ,
and studied the conditions under which such a relaxation is exact. When specialized to the
sparse linear regression problem, the continuous relaxation takes the following form of a semidef-
inite program,

νPW G := min

t∈ℜ,z∈[0,1]p

0.5t

s.t . "t

y

y

ρ X D(z)X T# º 0, e T z ≤ k,

In + 1

(SDPPW G )

where e is a vector with all entries 1 in proper dimension, and D(z) is a diagonal matrix whose
entries are zi , i = 1, ..., p. It can also be equivalently written as the following compact form,

νPW G =

1

2

min

z∈[0,1]p ,eT z≤k

y Tµ 1

ρ

X D(z)X T + In¶−1

y.

Following a different approach, authors of [1] recently proposed another semideﬁnite relaxation
for sparse linear regression where the ℓ-0 norm appears as a regularized term. When modiﬁed
as a convex relaxation for the cardinality constrained form (3), their proposed semideﬁnite re-
laxation is,

νDC L := min

b∈Rp ,B∈S p

1

−y T X

2¿· y T y
−X T y ρIp + X T X¸ ,·1 bT
B ¸À
s.t . ·1 bT
B ¸ º 0

b

b

bi

·zi
bi Bi i¸ º 0,∀i ,

p

Xi=1

zi ≤ k.

2

(SDPDC L)

In this note we compare these two semideﬁnite relaxations. We show that the relaxation (SDPDC L)
is no weaker than (SDPPW G) in this section. In section 2 we establish a result that characterizes
a certiﬁcate of exactness for the convex relaxation (SDPDC L), hence extends a key result in [2] to
(SDPDC L). Section 3 concerns the probability of exact recovery for the case of Gaussian ensem-
ble, where we show empirically (SDPDC L) can recover the true support of with much less data
points.

We ﬁrst state a technical lemma that will be used soon.
Lemma 1. For any X ∈ ℜn×p and ρ > 0, we have

min

β∈Rp½ 1

2kX β− yk2
2 +

1
2

ρkβk2

2¾ =

1
2

y Tµ 1

ρ

X X T + In¶−1

y

Proof. Straightforward computation.

By Lemma 1, (SDPPW G ) can be reformulated as

2

1

1

min

2°°°XpD(z)β− y°°°

min
β∈Rp

νPW G =

z∈[0,1]p ,eT z≤k

2 +
where pD(z) is a diagonal matrix with the i-th diagonal entry pzi .
Proposition 1. νℓ0 ≥ νDC L ≥ νPW G.
Proof. Suppose that ( ˜b, ˜B, ˜z) is optimal in (SDPPW G). Without loss of generality we may assume
that ˜zi =

for all ˜Bi i 6= 0, and ˜zi = 0 otherwise. Therefore ˜zi ∈ [0, 1],∀i . Deﬁne ˜β as

ρ°°β°°

˜b2
i
˜Bi i

2
2 ,

(4)

2

2

˜βi =( ˜z− 1

i
0

˜bi ,

i f ˜zi > 0
i f ˜zi = 0.

1

Then obviously ˜b = pD( ˜z) ˜β and ( ˜β, ˜z) is feasible in (4). We have
X X T ¸ ,·1
−y T X
2 +

−X T y ρIn + X X T¸ ,·1

˜B ¸À ≥
≥

2¿· y T y

−y T X

˜bT

˜b

˜b

1

1

1

2

2¿· y T y
−X T y
2°°°XpD( ˜z) ˜β− y°°°

The ﬁrst inequality is because of ˜B º ˜b ˜bT . The second inequality is because of ˜Bi i ˜zi ≥ ˜bi , which
implies ˜Bi i ≥ ˜β2
As (SDPDC L) satisﬁes the Slater condition, strong duality holds and the dual of (SDPDC L ) is

i , and the ﬁnal inequality is by the characterization (4).

ρtrace( ˜B )

2

1

˜bT

˜b ˜bT¸À+
ρ°° ˜β°°
2 ≥ νPW G.

2

2

3

νDC L =

1

2

y T y + max
τ,λ,t ,d −
s.t . ·

1

1

2

2

kλ

τ−
−X T y − t X T X + ρI − D(d )¸ º 0
τ
·λ ti
ti di¸ º 0,∀i .

−y T X − t T

(SDPDC L : d ual )

2 Certiﬁcate of exactness

Proposition 1 implies that if νPW G = νℓ0, then νDC L = νℓ0. Therefore all sufﬁcient conditions for
the exactness of (SDPPW G) readily carry over to (SDPDC L). Authors of [2] provided a character-
ization of a certiﬁcate of exactness for the continuous relaxation of (3), as well as a specialized
result on (SDPPW G). We restate their characterization result in Theorem 1, and provide a paral-
lel result in Theorem 2 for (SDPDC L).
Theorem 1 (Corollary 2 in [2]). The convex relaxation (SDPPW G) is exact if and only if there is a
subset S ⊆ {1, ..., p}, where |S| ≤ k, such that there exists λ ∈ R+,
j M y| > λ, ∀ j ∈ S, and
j M y| ≤ λ, ∀ j ∉ S,

|X T
|X T

(5)

(6)

where X j ∈ Rn is the j-th column of X , and M :=¡In + ρ−1XS X T
S¢−1

.

Using this result, the authors were able to prove a high-probability exact recovery condition
for the special case of Gaussian ensembles. We leave the discussion of Gaussian ensemble in
the next section. Here we provide a parallel characterization of certiﬁcates of exactness for
(SDPDC L).
Theorem 2. Let S ⊆ {1, ..., n}, |S| = k and z∗ be a binary vector such that z∗i = 1,∀i ∈ S and z∗i =
0,∀i ∉ S. Further let b∗ be the optimal solution of the ridge regression in the restricted subspace,
i.e.,

b∗ ∈ arg min

β∈Rp©kX β− yk2

2 + ρkβk2

2¯¯ β j = 0,∀ j ∉ Sª

Then (b∗, b∗b∗T , z∗) is optimal to (SDPDC L) if and only if there exists a vector ˜d ∈ R
˜λ ∈ R+ such that

p

+ and scalar

ρ−1X T X + Ip − D( ˜d ) º 0,
i M y¢2
i M y¢2

˜λ = ˜di¡X T
˜λ ˜di ≥¡X T

,

,

.

4

S¢−1
where M :=¡In + ρ−1XS X T

∀i ∈ S,
∀i ∉ S

(7)

(8)

(9)

The proof of Theorem 2 exploits the optimality conditions of (SDPDC L) and its dual, and is given
in detail in the appendix section. We remark that one can directly show that the conditions in
Theorem 2 are no stronger than those in Theorem 1.
Remark 1. Suppose that λ is the scalar such that (5) and (6) hold, then (7) – (9) hold for ˜λ and ˜d ,
where

˜λ := max½³X T

j M y´2

: i ∈ S¾, ˜di = ˜λ³X T

j M y´−2

,∀i ∈ S, and ˜di = 1,∀i ∉ S.

Note that ˜di ∈ [0, 1] for all i by construction. Therefore (7) holds. (8) and (9) are also valid by
construction.

3 Empirical comparison on exact recovery rate for Gaussian ensem-

ble

2

mi n

β∗2

γ2+kβ∗Sk2

log p, and that we solve (SDPPW G) with ρ = pn. Then with

In this section we consider the special case of Gaussian ensemble, where the design matrix
X ∈ Rn×p is generated with i.i.d. N(0,1) entries. A “true" signal β∗ is generated to be k-sparse,
i.e., it has only k number of nonzero entries, and each nonzero entry is of the order 1/pk. The
response vector y is generated by y = X β∗ + ǫ, where ǫ has i.i.d N (0, γ2) entries. The follow-
ing result is established in [2], which characterizes the size of n needed to guarantee the exact
recovery of the support of β∗ with high probability.
Theorem 3. There are constants c0 and c1, such that the following holds. Suppose that we are
given a sample size n > c0
probability at least 1− 2e−c1n, the relaxation (SDPPW G) is exact, i.e., νPW G = νℓ0.
Here β∗mi n is the minimal nonzero entry (in absolute value) of β∗. Note that Proposition 1 en-
sures that under the same conditions, νDC L = νℓ0 with (at least the same) high probability.
In the remaining part of this section we empirically evaluate the exact recovery for the Gaussian
ensemble case. We compare the probabilities of exact recovery by (SDPPW G) and (SDPDC L) for
various n and p. To avoid potential numerical issues in solution precision, we exploit Theo-
rem 1 and Theorem 2 to directly search for the certiﬁcates. Such a strategy enables us to test
whether (SDPPW G ) and (SDPDC L) provide the global solution to (ℓ0 : c ar d) on large number of
simulated data sets without explicitly solving the semideﬁnite relaxations many times.
Given simulated data (X , y, β∗), let S denote the support of β∗. Let ρ > 0 be ﬁxed, it is straight-
forward to test whether conditions in Theorem 1 are satisﬁed. If so, then (SDPPW G ) recovers
the true support of β∗. The situation is slightly more complicated for (SDPDC L) and Theorem 2.
Here we describe a bisection algorithm to search for the dual certiﬁcates ˜λ and ˜d, provided that
the support of β∗ is used as the index set S.

5

3.1 A bisection algorithm to search for the dual certiﬁcates for (SDPDC L )

Without loss of generality we assume that S = {1, ...,|S|}. Firstly if X T
i M y = 0 for some i ∈ S,
then the convex relaxation (SDPDC L) is not exact unless the trivial case where X T M y = 0 for
all i . Then without loss of generality we can assume that ˜di = ˜λ¡X T
i M y¢−2
, for all i ∈ S, and
˜di = ˜λ−1¡X T
, for all i ∉ S. Therefore the problem of testing (7) – (9) is then equivalent to
testing whether there exists ˜λ > 0 such that the following function is nonpositive,

i M y¢2

f ( ˜λ) := λmax½·DS ( ˜λ)

0

0

D ¯S( ˜λ)¸− ρ−1X T X − Ip¾ ,

(10)

i M y¢−2
, i = |S|+ 1, ..., p.

where λ{·} is the largest eigenvalue function, DS( ˜λ) is a |S|×|S| diagonal matrix with diagonal
entries ˜λ¡X T
, i = 1, ...,|S|, and similarly D ¯S ( ˜λ) is a diagonal matrix with diagonal entries
i M y¢2
˜λ−1¡X T
Note that f ( ˜λ) is a convex function when ˜λ > 0. This is because λmax{·} is a convex function
and non-decreasing in terms of diagonal entries, and ˜λ−1 is convex when ˜λ > 0. It is known
that a subgradient of f ( ˜λ) can be computed from an eigenvector associated with the largest
eigenvalue in (10). Indeed, let u be such an eigenvector, then

h( ˜λ) :=Xi∈S¡X T

i M y¢−2

u2

i − λ−2Xi∈ ¯S¡X T

i M y¢2

i ∈ ∂ f ( ˜λ).
u2

In other words, given any ˆλ > 0, f (λ) ≥ f ( ˆλ)+ h( ˆλ)(λ− ˆλ) for all λ > 0. A ﬁnal ingredient needed
for a bisection algorithm is the initial interval. Consider the diagonal entries of the matrix in
(10), obviously if ˜λ¡Xi M y¢−2
i Xi +
1) ≥ 0 for some i ∉ S, then f ( ˜λ) ≥ 0. Therefore we can restrict ourself in a region such that
˜λ¡Xi M y¢−2
i Xi + 1) ≤ 0 for all i ∉ S.
This provides initial upper and lower bounds such that if there exists ˆλ such that f ( ˆλ) < 0, ˆλ
must be in

i Xi + 1) ≥ 0 for some i ∈ S, or λ−1¡Xi M y¢2

− (ρ−1X T

− (ρ−1X T

− (ρ−1X T

− (ρ−1X T

i Xi + 1) ≤ 0 for all i ∈ S, and λ−1¡Xi M y¢2
i∈S n¡Xi M y¢2

i Xi + 1)−1o , min

·max
i∈ ¯S n¡Xi M y¢2

(ρ−1X T

i Xi + 1)o¸ .

Then the problem of testing whether there is ˜λ such that f ( ˜λ) can be solved by the following
bisection algorithm,

(ρ−1X T

(ρ−1X T

2 and evaluate f ( ˆλ);

1. Start with ℓ = maxi∈ ¯Sn¡Xi M y¢2
2. Let ˆλ = ℓ+u
3. If f ( ˆλ) ≤ 0, return YES; otherwise compute h( ˆλ);
4. If h( ˆλ) = 0, return NO. If h( ˆλ) > 0, u ← ˆλ −

ˆλ

i Xi + 1)−1o and u = mini∈Sn¡Xi M y¢2

(ρ−1X T

i Xi + 1)o;

h( ˆλ)
u − ℓ > ǫ, where ǫ is a ﬁxed precision tolerance, then go to step 2. Otherwise return NO.

h( ˆλ)

; otherwise if h( ˆλ) < 0, ℓ ← ˆλ −

ˆλ

. If

6

3.2 Numerical simulations

Using this bisection algorithm, we conduct similar experiments as shown in Figure 1 of [2]. For

each value of p (denoted as d in all plots), the true sparsity is set as§pp¨, and the number of
data points n = αk log(p − k). The true signal β∗i , (i = 1, ...k) is generated to be 1 or -1 with same
probability. The Figures 1 through 6 show the exact support recovery rate for α ∈ [1, 10], when ρ
are chosen to be 2pn, 3pn, 4pn, 6pn, 8pn, 12pn.

The numerical simulation illustrates that (SDPDC L) can recover the support of true signals with
signiﬁcant less data points than that of (SDPPW G ). Also the exact recovery rate of (SDPDC L)
appears to be much less sensitive to the choice of ρ. This result further motivates us to study
scalable approximate methods, such as those based on low rank factorization of the matrix B ,
to solve (SDPDC L).

y
r
e
v
o
c
e
r

t
r
o
p
p
u
s

t
c
a
x
e

f
o

y
t
i
l
i

b
a
b
o
r
P

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

 
1

2

3

N u m b e r of d at a p oi n t s v s . r e c ov e r y r at e , ρ = 2

 

PWG d=64
PWG d=128
PWG d=256
PWG d=512
DCL d=64
DCL d=128
DCL d=256
DCL d=512

9

10

4

8
C on t r ol p ar am e t e r α : n = α k l og( d )

7

5

6

Figure 1: Exact support recovery rate when ρ = 2pn.

7

y
r
e
v
o
c
e
r

t
r
o
p
p
u
s

t
c
a
x
e

f
o

y
t
i
l
i

b
a
b
o
r
P

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

 
1

2

3

N u m b e r of d at a p oi n t s v s . r e c ov e r y r at e , ρ = 3

 

PWG d=64
PWG d=128
PWG d=256
PWG d=512
DCL d=64
DCL d=128
DCL d=256
DCL d=512

9

10

4

8
C on t r ol p ar am e t e r α : n = α k l og( d )

6

7

5

Figure 2: Exact support recovery rate when ρ = 3pn

y
r
e
v
o
c
e
r

t
r
o
p
p
u
s

t
c
a
x
e

f
o

y
t
i
l
i

b
a
b
o
r
P

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

 
1

2

3

N u m b e r of d at a p oi n t s v s . r e c ov e r y r at e , ρ = 4

 

PWG d=64
PWG d=128
PWG d=256
PWG d=512
DCL d=64
DCL d=128
DCL d=256
DCL d=512

9

10

4

8
C on t r ol p ar am e t e r α : n = α k l og( d )

7

5

6

Figure 3: Exact support recovery rate when ρ = 4pn

8

y
r
e
v
o
c
e
r

t
r
o
p
p
u
s

t
c
a
x
e

f
o

y
t
i
l
i

b
a
b
o
r
P

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

 
1

2

3

N u m b e r of d at a p oi n t s v s . r e c ov e r y r at e , ρ = 6

 

PWG d=64
PWG d=128
PWG d=256
PWG d=512
DCL d=64
DCL d=128
DCL d=256
DCL d=512

9

10

4

8
C on t r ol p ar am e t e r α : n = α k l og( d )

7

5

6

Figure 4: Exact support recovery rate when ρ = 6pn

y
r
e
v
o
c
e
r

t
r
o
p
p
u
s

t
c
a
x
e

f
o

y
t
i
l
i

b
a
b
o
r
P

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

 
1

2

3

N u m b e r of d at a p oi n t s v s . r e c ov e r y r at e , ρ = 8

 

PWG d=64
PWG d=128
PWG d=256
PWG d=512
DCL d=64
DCL d=128
DCL d=256
DCL d=512

9

10

4

8
C on t r ol p ar am e t e r α : n = α k l og( d )

5

6

7

Figure 5: Exact support recovery rate when ρ = 8pn

9

y
r
e
v
o
c
e
r

t
r
o
p
p
u
s

t
c
a
x
e

f
o

y
t
i
l
i

b
a
b
o
r
P

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

 
1

2

3

N u m b e r of d at a p oi n t s v s . r e c ov e r y r at e , ρ = 12

 

PWG d=64
PWG d=128
PWG d=256
PWG d=512
DCL d=64
DCL d=128
DCL d=256
DCL d=512

9

10

4

8
C on t r ol p ar am e t e r α : n = α k l og( d )

5

6

7

Figure 6: Exact support recovery rate when ρ = 12pn

References

[1] Hongbo Dong, Kun Chen, and Jeff Linderoth. Regularization vs. Relaxation: A conic opti-

mization perspective of statistical variable selection. Submitted to Math. Prog. A, 2015.

[2] Mert Pilanci, Martin J. Wainwright, and Laurent El Ghaoui. Sparse learning via Boolean

relaxations. Mathematical Programming (Series B), 151:63–87, 2015.

Appendix

Proof of Theorem 2.

Proof. Without loss of generality we assume S = {1, ..., k}. When z = z∗, the constraints in (SDPDC L)
enforce that Bi i = bi = 0,∀i ∉ S. Further since z∗ is binary, we may assume B∗ = b∗b∗T without
loss of generality, where b∗ solves the restricted regression problem:

2

2 + ρkβk2 ⇒ b∗ ="¡ρI + X T

S XS¢−1

0|Sc|

X T

S y

#

and 0|S| is a |S|×1 zero vector. By strong duality and the KKT conditions, there exists b∗, B∗ such
that (b∗, B∗, z∗) is optimal in (SDPDC L) if and only if there exists dual variables (τ, λ, t , d ) such

b∗S ∈ arg min

β∈R|S|°°XSβ− y°°

10

that the ﬁrst order optimality condition holds

τ

·
−X T y − t X T X + ρI − D(d )¸ º 0
−y T X − t T
·λ ti
ti di¸ º 0,∀i
b∗ b∗b∗T¸ = 0
(b∗i )2¸ = 0, ∀i ∈ S

−y T X − t T
·λ ti
ti di¸•· 1

b∗T

b∗i

b∗i

τ

−X T y − t X T X + ρI − D(d )¸•· 1
·

(11)

(12)

(13)

(14)

In (14), only complementarity conditions for i ∈ S are needed because for all j ∉ S, z∗j equals
0, which implies that b∗j = 0 by feasibility and thus the complementarity condition holds. Now
we aim to derive simpler conditions on the existence of such dual variables (τ, λ, t , d ). With
condition (11), (13) can be equivalently written as

τ

·
−X T y − t X T X + ρI − D(d )¸· 1

−y T X − t T

b∗¸ = 0,

which is further equivalent to the following two equations (15) and (16),

t = (X T X + ρI − D(d ))b∗ − X T y
τ = b∗T t + y T X b∗ = b∗T (X T X + ρI − D(d ))b∗.

Now we exploit (15) and (16) to eliminate t and τ in (11),

τ

·
−X T y − t X T X + ρIp − D(d )¸ =·b∗T

−y T X − t T

Ip ¸£X T X + ρI − D(d )¤£b∗

Therefore conditions (11) and (13) are equivalent to (15), (16) and

X T X + ρIp − D(d ) º 0.

(15)

(16)

(17)

Ip¤ .

Now we consider conditions (12) and (14). Again with (12), (14) is equivalent to

·λ ti
ti di¸· 1

b∗i¸ = 0,∀i ∈ S ⇐⇒ ti = −di b∗i , λ = −ti b∗i = di (b∗i )2,∀i ∈ S

We claim that for all i ∈ S, ti = −di b∗i
quadratic form in the restricted subspace corresponding to S,

is implied by (15). Indeed, as b∗ minimizes the convex

0 =

d

d βi¯¯¯¯β=b∗ kXSβ− yk2

2 + ρkβk2 = 2X T
= 2¡X T

i (XSb∗− y)+ 2ρb∗i , ∀i ∈ S,
i X b∗− X T

i y + ρb∗i¢ .

11

So the i-th row of (15) can be equivalently written as,

ti = X T

i (X b∗ − y)+ ρb∗i − di b∗i = −di b∗i , ∀i ∈ S.

Therefore conditions (12) and (14) can be simpliﬁed as,

di ≥ 0,
λ = di (b∗i )2,
λdi ≥ t 2
i ,

∀i = 1, ..., p,
∀i ∈ S,
∀i ∉ S.

Note that for all i ∉ S, b∗i = 0. So by (15), ti = X T
conditions (11) – (14) are equivalent to

i (X b∗ − y) for all i ∉ S. Therefore the optimality

(15), (16), (17), λ = di (b∗i )2,∀i ∈ S, λdi ≥¡X T

i (X b∗− y)¢2

,∀i ∉ S.

Note that (15) and (16) simply state that and t and τ are uniquely determined once d is ﬁxed,
where t and τ do not appear in other conditions. To complete the proof it sufﬁces to prove two
sets of equalities:

and

b∗i = ρ−1X T

i (X b∗− y),

∀i ∈ S,

(18)

(19)
Our conclusion then follows after a rescaling ˜d = di ρ−1 and ˜λ = λρ3. Indeed, the equalities (18)
and (19) can be proved by using the Sherman-Morrison-Woodbury formula,

i (ρI + XS XS)−1 y,

i (X b∗− y) = X T

− X T

∀i .

S y − y´
S´ y
∀i

X T

X T

i ³XS¡ρI + X T
S XS¢−1
i (X b∗− y) = X T
X T
i ³I − XS¡ρI + X T
S XS¢−1
= −X T
S¢−1
i ¡I + ρ−1XS X T
= −X T
S XS¢−1
b∗S =¡ρI + X T
=hρ−1I − ρ−2X T
S¢−1
S ¡I + ρ−1XS X T
= ρ−1X T

X T

S y

y;

S¢−1
S ¡I + ρ−1XS X T

y.

XSi X T

S y

12

