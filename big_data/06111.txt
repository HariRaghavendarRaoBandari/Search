How Transferable are Neural Networks in NLP Applications?

Lili Mou,1 Zhao Meng,1 Rui Yan,2 Ge Li,1 Yan Xu,1 Lu Zhang,1 Zhi Jin1

1Software Institute, Peking University, 100871

Key Laboratory of High Conﬁdence Software Technologies (Peking University),

Ministry of Education, China

doublepower.mou@gmail.com, zhaomeng.pku@outlook.com

6
1
0
2

 
r
a

 

M
9
1

 
 
]
L
C
.
s
c
[
 
 

1
v
1
1
1
6
0

.

3
0
6
1
:
v
i
X
r
a

{lige,xuyan14,zhanglu,zhijin}@sei.pku.edu.cn

2Baidu Inc. rui.yan.peking@gmail.com

Abstract

Transfer learning is aimed to make use of
valuable knowledge in a source domain
to help the model performance in a tar-
get domain. It is particularly important to
neural networks because neural models are
very likely to be overﬁtting. In some ﬁelds
like image processing, many studies have
shown the effectiveness of neural network-
based transfer learning. For neural NLP,
however, existing studies have only ca-
sually applied transfer learning, and con-
clusions are inconsistent.
In this paper,
we conduct a series of empirical studies
and provide an illuminating picture on the
transferability of neural networks in NLP.1

1

Introduction

Transfer learning, or sometimes known as domain
adaptation,2 plays an important role in various
natural language processing (NLP) applications,
especially when we do not have a large enough
dataset for the task of interest (called the target
task T ). In such scenarios, we would like to trans-
fer or adapt knowledge in datasets from other do-
mains (called the source domains/tasks S) so as
to mitigate the problem of overﬁtting and to im-
prove performance. For traditional feature-rich or
kernel-based models, researchers have developed
a variety of elegant methods for domain adap-
tation; examples include EasyAdapt (Daum´e III,
2007; Daum´e III et al., 2010), instance weighting
(Jiang and Zhai, 2007; Foster et al., 2010), struc-

1We release our code on https://sites.google.com/site/
transfernlp/, which is implemented based on Mou et al.
(2015a).

2In this paper, we do not distinguish the conceptual differ-
ence between transfer learning and domain adaptation. Do-
main—in the sense we use throughout this paper—is deﬁned
by datasets.

tural correspondence learning (Blitzer et al., 2006;
Prettenhofer and Stein, 2010), etc.

Recently, deep neural networks are emerging as
the prevailing technical solution to almost every
ﬁeld in NLP. Although capable of learning highly
nonlinear features, deep neural networks are very
prone to overﬁtting (Peng et al., 2015) compared
with traditional methods. Transfer learning there-
fore becomes even more important to neural mod-
els. Fortunately, neural networks can be trained
in a transferable way by their incremental learn-
ing nature: we can directly use trained (tuned) pa-
rameters from a source task to initialize the net-
work in the target task; alternatively, we may also
train two tasks simultaneously with some param-
eters shared. But analysis is needed regarding the
performance of neural transfer learning.

Existing studies have already shown some evi-
dence of the transferability of neural features. For
example, in image processing, low-level neural
layers resemble much to Gabor ﬁlters or color
blobs (Zeiler and Fergus, 2014; Krizhevsky et
al., 2012); they can be transferred well to differ-
ent tasks, e.g., different image classiﬁcation tasks.
Donahue et al. (2014) suggest that high-level lay-
ers are also transferable in general visual recogni-
tion; Yosinski et al. (2014) further investigate the
transferability of neural layers in different levels
of abstraction.

Although transfer learning is promising in im-
age processing, conclusions appear to be less clear
in NLP applications. Image pixels are low-level
signals, which are generally continuous and less
related to semantics. By contrast, natural lan-
guage tokens are discrete: each word well reﬂects
the thought of humans, but neighboring words do
not share as much information as pixels in images
do. Previous neural NLP studies have casually ap-
plied transferring techniques, but their results are
not consistent. Collobert and Weston (2008) ap-

ply multi-task learning to SRL, NER, POS, and
CHK,3 but obtain only 0.04–0.21% error reduc-
tion4 (out of a base error rate of 16–18%). Bow-
man et al. (2015), however, improve a natural lan-
guage inference task from an accuracy of 71.3%
to 80.8% by initializing parameters with an ad-
ditional dataset of 550,000 samples. Therefore,
more systematic studies are needed to shed light
on transferring neural networks in NLP applica-
tions.

Our Contributions
In this paper, we investigate the question “How
transferable are neural networks in NLP applica-
tions?”

We distinguish two scenarios of transfer: (1)
transferring knowledge to a semantically equiva-
lent task but with a different dataset, which is se-
mantically equivalent in general; (2) transferring
knowledge to a task that is semantically different
but shares the same neural topology/architecture
so that neural parameters can be transferred. We
further distinguish two transfer methods: (1) using
the parameters trained on S to initialize T (INIT),
and (2) multi-task learning (MULT), i.e., training
S and T simultaneously. (Please see Sections 2
and 4).
Speciﬁcally, our study mainly focuses on the

following research questions:
RQ1: How transferable are neural networks be-
tween two tasks with similar or different se-
mantics in NLP applications?

RQ2: How transferable are different layers of

NLP neural models?

RQ3: How transferable are INIT and MULT, re-
spectively? What is the effect of combining
these two methods?

We conducted extensive experiments over three
datasets on classifying sentence pairs. We lever-
aged the widely-studied convolutional neural net-
work (CNN) as our neural model.

Based on our experimental results, we have the
following main observations, some of which are
unexpected.

3The acronyms refer to semantic role labeling, named en-
tity recognition, part-of-speech tagging, and chunking, re-
spectively.

4Here, we quote the accuracies obtained by using unsu-
pervised pretraining of word embeddings. This is the highest
performance in that paper; using pretrained word embeddings
is also a consensus in the literature. Besides, we would like
to emphasize that our paper mainly focuses on transferring
knowledge from supervised tasks.

• Whether a neural network is transferable in
NLP depends largely on how semantically
similar the tasks are.

• The output layer is mainly speciﬁc to the
dataset and not transferable. The perfor-
mance gain of neural domain adaptation
comes mainly from transferring hidden lay-
ers. Word embeddings are likely to be trans-
ferable to semantically different tasks, but
the boost is not large if they have been pre-
trained in an unsupervised way on a large
corpus.

• MULT appears to be slightly better than (but
generally comparable to) INIT in our exper-
iment; combining MULT and INIT does not
result in further gain.

The rest of this paper is organized as follows.
Section 2 introduces the datasets that our model
is transferred across; Section 3 details the neu-
ral architecture and experimental settings. We de-
scribe two approaches (INIT and MULT) for trans-
fer learning in Section 4. We present experimental
results in Sections 5–6, and have concluding re-
marks in Section 7.
2 Datasets
In our study, we used three open datasets as fol-
lows.

Language

• Stanford Natural

Inference
(SNLI): a newly-released large dataset con-
taining more than 550,000 sentence pairs.5
The task is to recognize whether a sentence
can be entailed (E) from another sentence,
or the two sentences are contradictory (C)
to each other. Besides, there is also a third
target label, indicating the two sentences are
irrelevant, denoted as neutral (N).

• Sentences Involving Compositional Knowl-
edge (SICK): a small dataset with exactly the
same classiﬁcation objective as SNLI.6

• Microsoft Research Paraphrase Corpus
(MSRP): a (small) dataset for paraphrase
detection.7 The objective is binary classiﬁca-
tion: judging whether two sentences have the
same meaning.

Table 1 illustrates several examples of the two
tasks; relevant statistics are also provided.

Although SNLI/SICK and MSRP are related to
each other more or less, they are semantically dif-

5http://nlp.stanford.edu/projects/snli/
6http://http://alt.qcri.org/semeval2014/task1/
7http://research.microsoft.com/en-us/downloads/

Premise

Natural Language Inference (SNLI and SICK)

Two men on bicycles competing in a race.
People are riding bikes.

Hypothesis Men are riding bicycles on the streets.

E
C
N

Paraphrase

A few people are catching ﬁsh.
Paraphrase Detection (MSRP)
The DVD-CCA then appealed to the state
Supreme Court.
The DVD CCA appealed that decision
to the U.S. Supreme Court.
Earnings per share from recurring operations
will be 13 cents to 14 cents.

Non-
Paraphrase That beat the company’s April earnings

forecast of 8 to 9 cents a share.

SNLI
550,152
10,000
10,000

SICK
4,439
495
4,906

MSRP
3,575
501
1,725

#Train
#CV
#Test

Table 1: Examples and statistics of the SNLI,
SICK, and MSRP datasets.

ferent. Therefore, we can distinguish two kinds
of transfer regarding semantic similarity: seman-
tically equivalent transfer (SNLI→SICK) and se-
mantically different transfer (SNLI→MSRP).
It should be noticed that in image or speech pro-
cessing (Yosinski et al., 2014; Wang and Zheng,
2015), the input of neural networks is pretty much
consists of raw signals; hence, low-level feature
detectors are almost always transferable, even if
Yosinski et al. (2014) manually distinguish artiﬁ-
cial objects and natural ones in an image classiﬁ-
cation task.

Distinguishing semantic relatedness—which
emerges from very low layers of either word em-
beddings or its successive layer—is speciﬁc to
NLP and also a new insight of our paper. As we
shall see in Sections 5 and 6 , the two scenarios
lead to different results.

3 The Neural Model and Settings

As all the above datasets can be viewed as a classi-
ﬁcation task over sentence pairs, we may use a sin-
gle neural model to solve the three problems in a
uniﬁed manner. That is to say, the neural architec-
ture is the same among different datasets, which
makes it possible to investigate transfer learning
regardless of whether the tasks are semantically
equivalent.

Figure 1 depicts the basic neural network in our
study. We leverage the widely-studied convolu-
tional neural network (CNN) to capture the seman-
tics of a single sentence; the convolution window
size is 5. Then the vector representations of two

Figure 1: The neural model in our study.

sentences are combined by concatenation and fed
to a hidden layer before the softmax output. Such
a two-step strategy is called the “Siamese” struc-
ture (Bromley et al., 1993), and also used for vari-
ous sentence pair modeling applications (Hu et al.,
2014; Mou et al., 2015b).

In our experiments, the embeddings were pre-
trained by word2vec (Mikolov et al., 2013); all
embeddings and hidden layers were 100 dimen-
sional as in Bowman et al. (2015); we designated
the relatively small dimension because of compu-
tational concerns.

We applied stochastic gradient descent with a
mini-batch size of 50 for optimization.
In each
setting, we tuned the parameters as follows: learn-
ing rate from {1, 0.3, 0.1, 0.03}, power decay of
learning rate from {fast, moderate,
low} (de-
ﬁned by how much, after one epoch, the learn-
ing rate residual is: 0.1x, 0.3x, 0.9x, resp). We
regularized our network by dropout with a rate
from {0, 0.1, 0.2, 0.3}. Note that we might not
run nonsensical settings, e.g., a larger dropout
rate if the network has already been underﬁtting
(i.e., accuracy has decreased when the dropout
rate increases). However, we might try additional
smaller learning rates,
if needed, especially in
transfer learning by INIT. We report the test per-
formance associated with the highest validation
accuracy.

To setup a baseline, we trained our model with-
out transfer 5 times by different random parameter
initializations. Results are reported in Table 2. Our
model has achieved reasonable performance that is
comparable to similar networks reported in the lit-
erature with all three datasets. Therefore, our im-
plementation is fair and suitable for further study
of transfer learning.

twomenon. . . 0peopleare. . . 0softmaxConvolutionMax poolingEmbedding        Hidden layers         OutputRelated model

76.3

77.6 (RNN, Bowman+,2015)
71.3 (RNN, Bowman+,2015)
69.6 (Arc-I CNN, Hu+,2014)

Dataset Avg acc.±std.
SNLI
SICK 70.9 ± 1.3
MSRP 69.0 ± 0.5
Table 2: Accuracy (%) without transfer. We also
include related models for comparison (Hu et al.,
2014; Bowman et al., 2015), showing that our
model has achieved comparable results; thus we
are ready to investigate into transfer learning.

It should be mentioned that the goal of this pa-
per is not to outperform state-of-the-art results; in-
stead, we would like to conduct a fair compari-
son of different methods and settings for transfer
learning in NLP.

4 Transfer Methods

Transfer learning aims to use knowledge in a
source domain to aid the target domain. As neural
networks are usually trained incrementally with
gradient descent (or variant), it is straightforward
to use gradient information in both source and tar-
get domains for optimization so as to accomplish
knowledge transfer. Depending on how samples
in source and target domains are scheduled, there
are two main approaches to neural network-based
transfer learning:

• Parameter initialization (INIT). The INIT ap-
proach ﬁrst trains the network on S, and then
directly uses the tuned parameters to initial-
ize the network for T . In the INIT approach,
we may ﬁx ( ) the parameters in the target
domain (Glorot et al., 2011), i.e., no training
is performed on T . But when labeled data
are available in T , it would be better to ﬁne-
) the parameters like Bowman et al.
tune (
(2015).
INIT is also related to unsupervised pre-
training such as word embedding learning
(Mikolov et al., 2013) and autoencoders
(Bengio et al., 2007).
In these approaches,
parameters that are (pre)trained in an unsu-
pervised way are transferred to initialize the
model for a supervised task (Plank and Mos-
chitti, 2013). However, our paper focuses
on “supervised pretraining,” which means we
transfer knowledge from a labeled source do-
main.

• Multi-task learning (MULT). MULT, on the
other hand, simultaneously trains samples in
both domains (Collobert and Weston, 2008;

Liu et al., 2016). The overall cost function is
given by

J = λJT + (1 − λ)JS

(1)
where JT and JS are the individual cost func-
(Both JT and JS are
tion of each domain.
normalized by the number of training sam-
ples.) λ ∈ (0, 1) is a hyperparameter balanc-
ing the two domains.
It is nontrivial to optimize Equation 1 in prac-
tice by gradient-based methods. One may
take the partial derivative of J and thus λ
goes to the learning rate (Liu et al., 2016),
but the model is then vulnerable because it is
likely to blowup for large learning rates (mul-
tiplied by λ or 1 − λ) and be stuck in local
optima for small ones.
Collobert and Weston (2008) alternatively
choose a data sample from either domain
with a certain probability (controlled by λ)
and take the derivative for the data sample.
In this way, domain transfer is independent
of learning rates, but we may not be able to
fully use the entire dataset of S if λ is large.
Due to the limitation of time and space, we
adopted the latter approach in our experiment
for simplicity. (More in-depth analysis may
be needed in future work.) Formally, our
multi-task learning strategy is as follows.

1 Switch to T by prob. λ, or to S by
2 Compute the gradient of the next data

prob. 1 − λ.
sample in the particular domain.

Further, INIT and MULT can be combined
straightforwardly, and we obtain the third setting:
• Combination (MULT+INIT). We ﬁrst pre-
train on the source domain S for parameter
initialization, and then train S and T simul-
taneously.
From a theoretical perspective, INIT and MULT
work in different ways. In the MULT approach,
the source domain regulates the model by “alias-
ing” the error surface of the target domain; hence
the neural network is less prone to overﬁtting. In
INIT, T ’s error surface remains intact. Before
training on the target dataset, the parameters are
initialized in such a meaningful way that they con-
tain additional knowledge in the source domain.
However, in an extreme case where T ’s error sur-
face is convex, INIT is ineffective because the pa-
rameters can reach the global optimum regardless
of their initialization. Fortunately, deep neural
networks usually have highly complicated, non-

(cid:154) (cid:155)(cid:160) ¡ ♂1convex error surfaces. By properly initializing pa-
rameters with the knowledge of S, we can rea-
sonably expect that the parameters are in a better
“catchment basin,” and that the INIT approach can
transfer knowledge from S to T .
5 Results of Transferring by INIT

We ﬁrst analyze how INIT behaves in NLP-based
transfer learning.
In addition to two different
transfer scenarios regarding semantic relatedness
as described in Section 2, we further evaluated
two settings: (1) ﬁne-tuning parameters
, and
. Existing
(2) freezing parameters after transfer
evidence shows that frozen parameters would gen-
erally hurt the performance (Peng et al., 2015), but
it provides a more direct understanding how trans-
ferable the features are (because the factor of tar-
get domain optimization is ruled out). Therefore,
we included this setting in our experiments. More-
over, we transferred parameters layer by layer to
answer our second research question. Notice that
the E H O and E H O settings8 are inap-
plicable to SNLI→MSRP, because the output tar-
gets do not share same meanings and numbers of
target classes.

Through Subsections 5.1–5.3, we initialized the
parameters of SICK and MSRP with the ones cor-
responding to the highest validation accuracy of
SNLI. In Subsection 5.4, we further investigated
when the parameters are ready to be transferred
during the training on S.
5.1 Overall Performance
Table 3 shows the main results of INIT. A quick
observation is that using SNLI information signif-
icantly improves the SICK performance, which is
not surprising and also reported in Bowman et al.
(2015).

For MSRP, however, there is nearly no improve-
ment in performance regardless of how we trans-
fer the parameters. Although in previous stud-
ies, researchers have mainly drawn positive con-
clusions about transfer learning, we ﬁnd a nega-
tive result similar to ours upon careful examina-
tion of Collobert and Weston (2008). In that paper,
the authors report transferring NER, POS, CHK,
and pretrained word embeddings improves the
SRL task by 1.91–3.90% accuracy (out of 16.54–
18.40% error rate), with gain mainly due to word
embeddings.
In the settings that use pretrained

8Please refer to the caption of Table 3 for the (cid:52), (cid:50),

symbols.

and

,

Setting
Majority

E H O

E(cid:52) H(cid:50) O(cid:50)
E H(cid:50) O(cid:50)
E H O(cid:50)
E H(cid:50) O(cid:50)
E H O(cid:50)

E H O

SNLI→SICK SNLI→MSRP
66.5
69.0
68.1
66.4
–
69.9
+c68.8
–

70.9
69.3
70.0
+b43.1
71.0
76.3
+b77.6

Table 3: Main results of neural transfer learning
by INIT. We report test accuracies (%) in this ta-
ble. E: embedding layer; H: hidden layers; O:

tialized);

output layer. (cid:52): Word embeddings are pretrained
by word2vec; (cid:50): Parameters are randomly ini-

: Parameters are transferred but frozen;
: Parameters are transferred and ﬁne-tuned. +b:
consistent with Bowman et al. (2015); +c: consis-
tent with Collobert and Weston (2008).

word embeddings (which is common in NLP),
NER, POS, and CHK improve the SRL accuracy
by 0.04–0.21%. Likewise, in the SNLI→MSRP
experiment, the E H O(cid:50) setting yields a degra-
dation of 0.2% (∼.5x std and not statistically sig-
niﬁcant). The incapability of transferring is also
proved by locking embeddings and hidden layers

(E H O(cid:50)). We see in this setting, the test per-

formance is even worse than majority-class guess.
Further examining its training accuracy, which
is 65.5%, we conclude that extracted features of
SNLI by our CNN model are irrelevant to MSRP.
The above results are rather frustrating, indi-
cating for RQ1 that neural networks may not be
transferable to NLP tasks of different semantics.
These results are different from those in the image
processing domain, where feature detectors are al-
most always transferable (Donahue et al., 2014;
Yosinski et al., 2014).

5.2 Layer-by-Layer Analysis
To answer RQ2, we next analyze the transferabil-
ity of each layer. Since SNLI→MSRP is non-
transferable, we mainly focus on SNLI→SICK in
this section.
First, we freeze both embeddings and hidden
layers (E H ).
If we further freeze the out-
put layer (O ), the performance in SNLI→SICK
drops by more than 30%, but by tuning the out-
put layer’s parameters (O(cid:50)), we can obtain a sim-
ilar result to the baseline (E(cid:52)H(cid:50)O(cid:50)). In the ﬁne-

tuning settings (E H ), transferring the output

1(cid:154) (cid:155)(cid:160) ¡ ♂(cid:154) (cid:155)(cid:160) ¡ ♂(cid:154) (cid:155)(cid:160) ¡ ♂(cid:154) (cid:155)(cid:160) ¡ ♂1111(cid:154) (cid:155)(cid:160) ¡ ♂(cid:154) (cid:155)(cid:160) ¡ ♂(cid:154) (cid:155)(cid:160) ¡ ♂(cid:154) (cid:155)(cid:160) ¡ ♂(cid:154) (cid:155)(cid:160) ¡ ♂(cid:154) (cid:155)(cid:160) ¡ ♂(cid:154) (cid:155)(cid:160) ¡ ♂111111(cid:154) (cid:155)(cid:160) ¡ ♂111(cid:154) (cid:155)(cid:160) ¡ ♂(cid:154) (cid:155)(cid:160) ¡ ♂(cid:154) (cid:155)(cid:160) ¡ ♂(cid:154) (cid:155)(cid:160) ¡ ♂(cid:154) (cid:155)(cid:160) ¡ ♂11SNLI

SNLI→MSRP
E(cid:50) H(cid:50) O(cid:50) 60.3
E(cid:52) H(cid:50) O(cid:50) 69.0
E(cid:50) 76.4 E H(cid:50) O(cid:50) 67.2
E(cid:52) 76.3 E H(cid:50) O(cid:50) 69.9

Table 4: Analyzing the “supervised” and “unsu-
pervised” pretraining of word embeddings. The
E setting refers to transferring word embeddings
from SNLI. However, SNLI’s embeddings them-
selves are either randomly initialized or pretrained

by word2vec, indicating by E(cid:50) and E(cid:52) in the

ﬁrst column.

layer (O ) improves the accuracy by 1.3% (∼1x
std) compared with O(cid:50). The ﬁnding suggests that
the output layer is mainly speciﬁc to a dataset.
Transferring the output layer’s parameters yields
little (if any) gain.

The hidden layers bring about the main perfor-
mance improvement of transferring. Even if we
freeze embeddings and hidden layers (E H ), we
obtain an accuracy similar to the baseline as long
as the output is not frozen. Likewise, in the ﬁne-
tuning setting, hidden-layer transfer improves the
accuracy by another 5.3% on the basis of transfer-
ring word embeddings (which improves the accu-
racy by 0.1%).

Regarding MSRP, the embeddings are the only
parameters that have been observed to be trans-
ferable, although the improvement (0.9% in accu-
racy, 1.8x std) is not large either. As the above
studies use word2vec to pretrain word embed-

dings (E(cid:52)) on the Wikipedia corpus (which is a

common practice in the literature), a curious ques-
tion is whether word embeddings are transferable,
provided that they are randomly initialized.

To answer

this question, we ﬁrst

trained
SNLI again with word embeddings being ran-
domly initialized, and then transferred the pre-
trained word embeddings (in a supervised man-
ner on SNLI) to MSRP. Table 4 compares the re-
sult of transferring word embeddings (pretrained
by supervised/unsupervied approaches) with non-
transferring. As is seen, random initialization of
word embeddings works ﬁne for large datasets
like SNLI, but performs very poorly in MSRP.
Transferring word embeddings that are pretrained
purely by supervised objective on SNLI signiﬁ-
cantly improves the performance, but is still worse
than word2vec pretraining by 1.8x std. There-
fore, we conclude

Figure 2: Learning curves of different learning
rates (denoted as α).

Word embeddings (purely pretrained by super-
vised approaches on S) are indeed transfer-
able across semantically different tasks. When
large unlabeled corpora are available, however,
unsupervised pretraining of word embeddings
like word2vec is preferable.

5.3 How does learning rate affect transfer?

Bowman et al. (2015) suggest that after transfer-
ring, a large learning rate may damage the knowl-
edge stored in the parameters; in their paper, they
transfer the learning rate information (AdaDelta)
from S and T in addition to the parameters.
Although the rule of the thumb is to choose all
hyperparameters—including the learning rate—by
validation, we are curious whether the above con-
jecture holds. Estimating a rough range of sensi-
ble hyperparameters can ease the burden of model
selection; it also provides evidence to better un-
derstand how transfer learning actually works.

We plot the learning curves of different learning

rates α in Figure 2 (SNLI→SICK, E H O(cid:50)).
(In the ﬁgure, no learning rate decay is applied.)
As we see, with a large learning rate like α = 0.3,
the accuracy increases fast and peaks at the 10th
epoch. Training with a small learning rate (e.g.,
α = 0.01) is slow, but its peak performance is
comparable to large learning rates when iterated
by, say, 100 epochs. Aside from our main conclu-
sions, we have the following additional ﬁnding:

In INIT, transferring learning rate information
is not necessarily useful. A large learning
rate does not damage the knowledge stored
in the pretrained hyperparameters, but acceler-
ates the training process to a large extent. In
all, we may need perform validation to choose
the learning rate if computational resources are
available.

1111(cid:154) (cid:155)(cid:160) ¡ ♂(cid:154) (cid:155)(cid:160) ¡ ♂2060100140Epoch50607080Accuracy(%)α=0.01α=0.03α=0.1α=0.311rule out some undesirable factors which may cause
the failure to transfer from SNLI to MSRP in Sub-
section 5.1.

The SNLI→SICK experiment produces inter-
esting yet undesirable results. Using the second
epoch of SNLI’s training yields the highest trans-
fer performance on SICK, i.e., 78.98%. However,
the SNLI performance itself is comparatively low
at that time (72.65% v.s. 76.26% at epoch 23).
Later, the performance decreases gradually by 1–
2%. Although the degradation is not signiﬁcant,
the tendency is reasonably perceptible, showing
that by ﬁtting S too well the parameters may be
ineffective for T more or less. More evidence is
needed in order to draw conclusions. Neverthe-
less, from Figure 3, we are reasonably safe to con-
clude
When we apply INIT for transferring, only a
few epochs over the source dataset are suf-
ﬁcient to capture transferable knowledge, al-
though the source task performance has not
been optimal.

6 MULT, and its Combination with INIT

To answer RQ3, we investigate how multi-task
learning performs in transfer learning; we also
analyze the effect of the combination of MULT
and INIT. In this section, we applied the setting:
sharing embeddings and hidden layers (denoted

as E H O(cid:50)), analogous to E H O(cid:50) in INIT.

Note that sharing all parameters E H O is not
applicable to MSRP because of different output ob-
jectives; thus we did not apply this setting.

We also tried the combination of MULT and
INIT, i.e., we used the pretrained parameters on
SNLI to initialize the multi-task training of SNLI
and SICK/MSRP. This setting could be visually
represented by E

H

O(cid:50).

In both MULT and MULT+INIT, we had a hy-
perparameter λ ∈ (0, 1) balancing the source and
target tasks (deﬁned in Section 4). λ was tuned
with a granularity of 0.1. As a friendly reminder,
λ = 1 refers to using T only; λ = 0 refers to using
S only. After ﬁnding that λ = 0.1 yields the high-
est performance of MULT in the SNLI+SICK ex-
periment (the thick blue line in Figure 4a), we fur-
ther tuned the λ from 0.01 to 0.09 with a ﬁne-
grained granularity of 0.02.

The results are shown in Figure 4. From the
green curves in the second subplot, we see MULT
(with or without INIT of SNLI) does not improve
the accuracy of MSRP; its inability to transfer is

Figure 3: (a) Learning curve of SNLI. (b) Accu-
racies of SICK and MSRP when parameters are
transferred at a certain epoch during the training
of SNLI. Dotted lines refer to non-transfer, which
can be equivalently viewed as transferring before
training on S, i.e., epoch = 0. Note that the x-axis
shares across the two subplots.

5.4 When is it ready to transfer?
In the above experiments, we transfer the parame-
ters when they achieve the highest validation per-
formance on S. This is a straightforward and intu-
itive practice.
However, we may imagine that the parameters
well-tuned to the source dataset may be too spe-
ciﬁc to it, i.e., the model overﬁts S and thus may
underﬁt T . Another advantage of early transfer
lies in computational concerns.
If we manage
to transfer model parameters after one or a few
epochs on S, we can save much time especially
when S is large.
We therefore made efforts in studying when the
neural model is ready to be transferred. Figure 3a
plots the learning curve of the source task SNLI.
The accuracy increases sharply from epochs 1–5;
later, it reaches a plateau but is still growing slowly
until the 23rd epoch gives the highest validation
accuracy. (Results related to the validation set are
not plotted in the ﬁgure).

We then transferred the parameters at different
stages (epochs) of SNLI’s training to SICK and

MSRP (also with the setting E H O(cid:50)). Their

accuracies are plotted in Figure 3b.

Transferring SNLI to MSRP at any epoch ap-
pears to be frustratingly ineffective. No matter
how the model performs on SNLI, it can hardly
ﬁt MSRP better than without transfer. The results

50607080Accuracy(%)(b)79.076.3SNLI→SICKSNLI→MSRP70Epoch:51015202580SNLIAcc.(%)(a)LearningcurveofSNLI11♥♥11♥♥♥1♥1♥cross-checked with INIT in Section 5. For SICK,
on the other hand, transferability of the neural
model is also consistently positive (blue curves
in Figure 4a), supporting our conclusion to RQ1
that NLP neural transfer learning depends largely
on how similar in semantics the source and target
datasets are.

Moreover, we see that the peak performance of
MULT is 79.6% when λ is 0.03, slightly higher
than 79.0% by INIT in Figure 3b. If λ is large,
(e.g., ≥ 0.5), the performance is generally simi-
lar to without transfer. Furthermore, we ﬁnd that
when λ is very small (say 0.01 in MULT or 0.01
and 0.03 in MULT+INIT), the performance drops
sharply, as it ﬁts S too much and thus underﬁts T.
The transfer performance of MULT+INIT

H

O(cid:50)) remains high for different val-

(E
ues of λ. As the parameters given by INIT have
conveyed sufﬁcient information about the source
task, MULT+INIT consistently outperforms non-
transferring (70.3%) by a large margin. Its peak
performance 77.6% is achieved when λ = 0.6,
which is sightly higher than for merely apply-

ing INIT (76.3% in the E H O(cid:50) setting), but

sightly lower than for MULT only. As these quali-
tative results are not very signiﬁcant and may vary
for different tasks and models, we answer our RQ3
as follows.

In our experiment, MULT and INIT are gener-
ally comparable, and MULT is slightly better
than INIT. We do not obtain further gain by
combining MULT and INIT.9

7 Concluding Remarks

In this paper, we addressed the problem of trans-
fer learning in neural network-based NLP ap-
plications. We conducted experiments on three
datasets, showing that the transferability of neural
NLP models depends on the semantic relatedness
of the source and target tasks. We analyzed the
behavior of different neural layers: word embed-
dings are transferable but the results are similar to
(or worse than) word2vec; hidden layers are the
main transferable features that improve the perfor-
mance; and the output layer is generally unable
to transfer. We also experimented with two trans-
fer methods: parameter initialization (INIT) and
multi-task learning (MULT); they generally per-
form similarly in our experiment. Additional ﬁnd-
ings are boxed in Section 5 and not repeated here.

9For aesthetic purposes, the main results that have been

boxed in Section 1 are not boxed again.

Figure 4: Results of MULT and MULT+INIT,
where we share word embeddings and hid-
den layers.
The tasks are (a) SNLI+SICK,
and (b) SNLI+MSRP. Dotted lines are the non-
transfer setting; dashed lines are the INIT setting

E H O(cid:50), transferred at the peak performance

of SNLI.

Our study provides insight on the transferability of
neural NLP models; the results also help to better
understand neural features in general.

How transferable are the conclusions in this
paper? Although we had in total more than 3000
separate runs of experiments in this paper, we have
to concede that empirical studies are subject to a
variety of factors (e.g., models, tasks, datasets),
and that conclusions may vary in different scenar-
ios. Therefore, along with analyzing our own ex-
perimental data, we have also carefully collected
related results in the literature, serving as addi-
tional evidence in answer to our research ques-
tions. As our results are generally consistent with
previously reported ones, we think the generality
of this work is fair and that the conclusions can be
generalized to similar scenarios.

Future work. Our work also points out some
future directions of research. We would like to an-
alyze different MULT strategies; more efforts are
also needed in developing an effective yet robust
method for multi-task learning.

1♥1♥110.00.20.40.60.81.0λ607080Accuracy(%)(a)79.677.6SNLI+SICK,MULTSNLI+SICK,MULT+INIT0.00.20.40.60.81.0λ607080Accuracy(%)(b)SNLI+MSRP,MULTSNLI+MSRP,MULT+INIT11References
Yoshua Bengio, Pascal Lamblin, Dan Popovici, Hugo
Larochelle, et al. 2007. Greedy layer-wise training
of deep networks. In Advances in Neural Informa-
tion Processing Systems.

John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
In Proceedings of the 2006 Con-
dence learning.
ference on Empirical Methods in Natural Language
Processing, pages 120–128.

Samuel R. Bowman, Gabor Angeli, Christopher Potts,
and Christopher D. Manning. 2015. A large anno-
tated corpus for learning natural language inference.
In Proceedings of the 2015 Conference on Empiri-
cal Methods in Natural Language Processing, pages
632–642.

Jane Bromley, James W Bentz, L´eon Bottou,

Is-
abelle Guyon, Yann LeCun, Cliff Moore, Eduard
S¨ackinger, and Roopak Shah. 1993. Signature ver-
iﬁcation using a “Siamese” time delay neural net-
work. International Journal of Pattern Recognition
and Artiﬁcial Intelligence, 7(04):669–688.

Ronan Collobert and Jason Weston. 2008. A uniﬁed
architecture for natural language processing: Deep
In Pro-
neural networks with multitask learning.
ceedings of the 25th International Conference on
Machine learning, pages 160–167.

Hal Daum´e III, Abhishek Kumar, and Avishek Saha.
2010. Frustratingly easy semi-supervised domain
In Proceedings of the 2010 Workshop
adaptation.
on Domain Adaptation for Natural Language Pro-
cessing, pages 53–59.

Hal Daum´e III. 2007. Frustratingly easy domain adap-
tation. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
256–263.

Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoff-
man, Ning Zhang, Eric Tzeng, and Trevor Darrell.
2014. DeCAF: A deep convolutional activation fea-
ture for generic visual recognition. In Proceedings
of The 31st International Conference on Machine
Learning, pages 647–655.

George Foster, Cyril Goutte, and Roland Kuhn. 2010.
Discriminative instance weighting for domain adap-
tation in statistical machine translation. In Proceed-
ings of the 2010 Conference on Empirical Methods
in Natural Language Processing, pages 451–459.

Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
2011. Domain adaptation for large-scale sentiment
In Pro-
classiﬁcation: A deep learning approach.
ceedings of the 28th International Conference on
Machine Learning, pages 513–520.

Baotian Hu, Zhengdong Lu, Hang Li, and Qingcai
Chen. 2014. Convolutional neural network archi-
tectures for matching natural language sentences.

In Advances in Neural Information Processing Sys-
tems, pages 2042–2050.

Jing Jiang and ChengXiang Zhai.

Instance
In Pro-
weighting for domain adaptation in NLP.
ceedings of the 45th Annual Meeting of the Associa-
tion of Computational Linguistics, pages 264–271.

2007.

Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hin-
ton. 2012. ImageNet classiﬁcation with deep con-
volutional neural networks. In Advances in neural
information processing systems, pages 1097–1105.

Yang Liu, Sujian Li, Xiaodong Zhang, and Zhifang Sui.
Implicit discourse relation classiﬁcation via
2016.
In Proceedings of the
multi-task neural networks.
30th AAAI Conference on Artiﬁcial Intelligence (to
appear).

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems, pages 3111–3119.

Lili Mou, Hao Peng, Ge Li, Yan Xu, Lu Zhang, and Zhi
Jin. 2015a. Discriminative neural sentence model-
ing by tree-based convolution. In Proceedings of the
2015 Conference on Empirical Methods in Natural
Language Processing, pages 2315–2325.

Lili Mou, Men Rui, Ge Li, Yan Xu, Lu Zhang, Rui
Yan, and Zhi Jin. 2015b. Recognizing entailment
and contradiction by tree-based convolution. arXiv
preprint arXiv:1512.08422.

Hao Peng, Lili Mou, Ge Li, Yunchuan Chen, Yangyang
Lu, and Zhi Jin. 2015. A comparative study on
regularization strategies for embedding-based neural
networks. In Proceedings of the 2015 Conference on
Empirical Methods in Natural Language Process-
ing, pages 2106–2111.

Barbara Plank and Alessandro Moschitti. 2013. Em-
bedding semantic similarity in tree kernels for do-
main adaptation of relation extraction. In Proceed-
ings of the 51st Annual Meeting of the Association
for Computational Linguistics, pages 1498–1507.

Peter Prettenhofer and Benno Stein. 2010. Cross-
language text classiﬁcation using structural corre-
spondence learning. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics, pages 1118–1127.

Dong Wang and Thomas Fang Zheng. 2015. Transfer
learning for speech and language processing. arXiv
preprint arXiv:1511.06066.

Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod
Lipson. 2014. How transferable are features in deep
neural networks? In Advances in Neural Informa-
tion Processing Systems, pages 3320–3328.

Matthew D Zeiler and Rob Fergus. 2014. Visualizing
and understanding convolutional networks. In Pro-
ceedings of 13th European Conference on Computer
Vision, pages 818–833.

