FUNDAMENTALS OF P-VALUES: INTRODUCTION

YURI GUREVICH AND VLADIMIR VOVK

Abstract. We explain the concept of p-values presupposing only rudimentary
probability theory. We also use the occasion to introduce the notion of p-function,
so that p-values are values of a p-function.

The explanation is restricted to the discrete case with no outcomes of zero prob-

ability. We are going to address the general case elsewhere.

6
1
0
2

 
r
a

 

M
2
1

 
 
]
T
S
h
t
a
m

.

[
 
 

1
v
2
5
9
3
0

.

3
0
6
1
:
v
i
X
r
a

1. Prelude

Q1: The Cournot Principle that one of you wrote about [3] caught my attention: “It
is practically certain that a predicted event of small probability does not happen.”
How small should the probability be?

A: Traditionally there have been two camps of naive Cournotians, the lax ones using
the 5% threshold and strict ones using the 1% threshold.

Q: Why do you call them naive?

A: A better way is to report an appropriate p-value, without committing yourself to
a threshold.

Q: Oh yes, I heard about p-values and even tried to read about them but got confused.
Do you have a good reference?

A: Cox and Hinkley deﬁne p-values, even though they do not use the term, in §3 of
the their 1974 book “Theoretical Statistics” [2]. That is the best reference that we
have.

Q: How about explaining the concept to me right now? But please take into account
that, while I have been exposed to probability theory and logic, I have not studied
statistics.

A: The general case is somewhat involved [4], but the discrete case is important and
simple enough to explain the concept, especially if one presumes that every outcome
has positive probability.

1Q and A are Quisani (a former student of the ﬁrst author) and the authors respectively.

1

2

YURI GUREVICH AND VLADIMIR VOVK

2. Probability trials

Recall that a discrete probability space is given by a nonempty set Ω and a function

P from Ω to the real interval [0, 1] satisfying the following three requirements.

(1) Ω is countable, i.e. ﬁnite or inﬁnite countable.
(2) Every P(x) ≥ 0.
(3) Px∈Ω P(x) = 1.

To simplify our exposition, we replace (2) with a stronger and relatively innocuous
requirement

(2′) Every P(x) > 0.

The function P naturally extends to subsets E of Ω: P(E) = P{P (x) : x ∈ E}.

Think of the probability space (Ω, P) as a probability trial. Elements of Ω are
outcomes, subsets of Ω are events, and P assigns probabilities to outcomes and events.
The probability distribution P is the null hypothesis of the trial. Think of it as an
alleged probability distribution. The purpose of the trial is to test P.

Q: I guess, a predicted event of small probability, if and when it occurs, provides
falsifying evidence against the null hypothesis.

A: Yes, but we do not need to deﬁne what probabilities count as small. For now, it
suﬃces to say this: The smaller the probability of the event in question, the stronger
the impugning power of the event. Suppose for example that Ω = {a, b, c} and that
P(a) = 990/1000, P(b) = 9/1000 and P(c) = 1/1000. We can order the nonempty
events in the order of increasing impugning power: {a, b, c}, {a, b}, {a, c}, {a}, {b, c},
{b}, {c}.

Q: But the event {a, b, c} has zero impugning power.

A: True, and we don’t say that it has any. We just say that its impugning power is
less than that of the other 6 events. Also, we ignore the empty event because it does
not occur.

3. The logic angle

Q: For logicians, a discrete probabilistic trial (Ω, P) is a relatively simple logic struc-
ture, an extension of real arithmetic with a separate sort Outcome and a function P
of type Outcome → Real, subject to requirements (1)–(3) of §2.

A: In applications, a trial comes with additional information. Here are two examples,
in a simpliﬁed form, from the paper you cited.

P-VALUES

3

Coin Example. A coin is tossed 42 times. The null hypothesis is that all 242 outcomes
are equally probable. The actual outcome has 41 heads and just one tail. Clearly the
impugning power of that outcome exceeds that of a random outcome.

Lottery Example. 4,000,000 people bought 10,000,000 tickets. Any of them could be
the winner; thus these are 4,000,000 potential outcomes. The null hypothesis is that
the probability of a person to win is proportional to the number of his or her tickets.
The wife Donna of the lottery organizer John bought 3 tickets and won the lottery.
Clearly the impugning power of that outcome exceeds that of a random outcome.

Q: Additional information complicates the matter. Clearly there are countless diﬀerent
kinds of additional information. How do statisticians deal with them?

A: They simplify the situation. They replace any additional structure with a linear
preorder of the outcomes and restrict attention to the downward closed events.

Q: Wow, this is quite a simpliﬁcation.

A: Often the linear preorder is given implicitly, by means of a test statistic or a nested
family of events.

Q: Please remind me the necessary deﬁnitions.

4. Test tools

Let (Ω, P) be a probability trial. A preorder of a nonempty set Ω is a binary

relation ≤ on Ω that is reﬂexive and transitive so that

• x ≤ x for every x, and
• if x ≤ y and y ≤ z then x ≤ z.

A preorder ≤ of Ω is linear if

• x ≤ y or y ≤ x for all x, y.

Deﬁnition 1.

• A test order of Ω is a linear preorder on Ω.
• A test pyramid over Ω is a family of events linearly ordered by inclusion.
• A test statistic on Ω is a function from Ω to the reals.

Coin Example, cont. Let Heads (x) and Tails (x) be the numbers of the heads and
tails in an outcome x respectively. The function M(x) = Min {Heads (x), Tails (x)}
is a test statistic. The relation M(x) ≤ M(y) is a linear preorder of the outcomes.
Events {x : M(x) ≤ n}, where n ranges from 0 to 42 form a test pyramid.

4

YURI GUREVICH AND VLADIMIR VOVK

Given a linear preorder ≤ on Ω, an event E ⊆ Ω is downward closed if x ≤ y ∈ E
implies x ∈ E for all x, y. The downward closed events are nested:
if E1, E2 are
downward closed then either E1 ⊆ E2 or E2 ⊆ E1. Indeed if E2 6⊆ E1 and x ∈ E2 −E1
then E1 ⊆ {y : y ≤ x} ⊆ E2.

Note that there may be distinct outcomes x, y with x ≤ y and y ≤ x; such outcomes

x, y called quasi-equivalent. Linear preorders are also known as linear quasi-orders.

Test orders, test pyramids and test statistics are three tools to measure the relative
strength of the impugning evidence provided by a given outcome or event. We adopt
the following convention.

- For a given test order, smaller outcomes provide stronger impugning evidence.
- For a given test statistic, smaller values provide stronger impugning evidence.

And of course, for a given test pyramid of events, smaller events provide stronger
impugning evidence.

In the following two sections we show that the three tools are equivalent in an

appropriate sense.

5. Test orders and test pyramids

If ≤ is a test order, let [≤ x] be the downward closure {y : y ≤ x} of x.

Deﬁnition 2.

• Every test order ≤ induces a test pyramid, namely the family of events [≤ x].
• Every test pyramid F induces a test order

{(x, y) : (∀E ∈ F )[y ∈ E =⇒ x ∈ E]}

In words, the formula (∀E ∈ F )[y ∈ E =⇒ x ∈ E] says that every member of F

that contains y contains x as well.

Lemma 3. If a test order ≤ induces a test pyramid F then F induces ≤.

Proof. Let (cid:22) be the test order induced by F .

x (cid:22) y ⇐⇒ (∀E ∈ F )[y ∈ E =⇒ x ∈ E]

⇐⇒ (∀z ∈ Ω)[y ∈ [≤ z] =⇒ x ∈ [≤ z]]
⇐⇒ x ∈ [≤ y]
⇐⇒ x ≤ y

by the deﬁnition of x (cid:22) y
by the deﬁnition of F

(cid:3)

P-VALUES

5

Deﬁnition 4.

• Two test pyramids are equivalent if they induce the same test order.
• The canonic version ˆF of a test pyramid F is the test pyramid induced by

the test order induced by F .

• A test pyramid F is canonic if it is the canonic version of itself.

Q: Show me a non-canonic test pyramid.

A: Let Ω be the set of numbers ± 1
n where n = 1, 2, . . . . The precise deﬁnition of
the distribution P on Ω is not important provided that every element of Ω has a
positive probability. Let ≤ be the standard order on the numbers ± 1
n . The desired
test pyramid F is the collection of all subsets of Ω downward closed with respect to
≤. Clearly F induces the test order ≤. The pyramid ˆF, induced by ≤, consists of
sets [≤ x] where x ∈ Ω. It is a proper subcollection of F . The diﬀerence F − ˆF
consists of three sets: ∅, [≤ 0] and Ω.

Q: You could have require that a test order ≤ induces the pyramid of all the events
downward closed with respect to ≤. Then your counter-example would not work.

A: That is true. But then a non-canonic pyramid would be obtained from the pyramid
of downward-closed events by omitting one or more of the sets ∅, [≤ 0], Ω.

Q: What about canonic test orders?

A: In our current setup every test order can be viewed as canonic. We saw already
that every test order is induced by a test pyramid. We’ll see below that every test
order is induced by a test statistic. This is not so in the general case. In fact, it is
not so even in the discrete case with outcomes of zero probability.

Lemma 5. Every pyramid F is equivalent to a unique canonic test pyramid, namely
to ˆF .

Proof. Let ≤ be the test order induced by F . By the deﬁnition, ˆF is the collection
of events [≤ x]. Clearly ˆF induces ≤, and so it is equivalent to F .

It remains to check that ˆF is the only canonic test pyramid equivalent to F . Let
F ′ be a canonic test pyramid that is equivalent to F and therefore induces ≤. Since
F ′ is canonic, it is induced by ≤. But then F ′ = ˆF.
(cid:3)

Corollary 6.

(1) If a test pyramid F induces a test order ≤ then ≤ induces the canonic version

ˆF of F .

(2) If a test order ≤ induces a test pyramid F then F is canonic.

Proof. (1) By the deﬁnition of ˆF.

6

YURI GUREVICH AND VLADIMIR VOVK

(2) By Lemma 3, F induces ≤. By (1), F = ˆF.

(cid:3)

6. Test orders and test statistics

Deﬁnition 7.

• Every test order ≤ induces a test statistic f (x) = P[≤ x].
• Every test statistic f induces a test order {(x, y) : f (x) ≤ f (y)}.

For a test statistics f and a real number f , let [f ≤ r] be the event {x : f (x) ≤ r}.

In the following lemma, we take advantage of having no zero-probability outcomes.

Lemma 8. For any test order ≤, we have x ≤ y if and only if P[≤ x] ≤ P[≤ y].

Proof. If x ≤ y then [≤ x] ⊆ [≤ y] and therefore P[≤ x] ≤ P[≤ y]. This establishes
the only-if implication. We prove the if implication by contrapositive. Suppose that
x > y. Then x belongs to [≤ x] but not to [≤ y] so that [≤ x] ) [≤ y]. Since
P(x) > 0, we have P[≤ x] > P[≤ y].
(cid:3)

Lemma 9. For any test statistic f , we have:

(1) If f induces a test order ≤ then every [≤ x] = [f ≤ f (x)].
(2) f (x) ≤ f (y) ⇐⇒ P[f ≤ f (x)] ≤ P[f ≤ f (y)].

Proof.

(1) [≤ x] = {y : y ≤ x} = {y : f (y) ≤ f (x)} = [f ≤ f (x)].

(2) Follows from (1) and Lemma 8.

Lemma 10. If a test order ≤ induces a test statistic f then f induces ≤.

Proof. Let (cid:22) be the test order induced by f .

x ≤ y ⇐⇒ P[≤ x] ≤ P[≤ y]

by Lemma 8,

⇐⇒ f (x) ≤ f (y)
⇐⇒ x (cid:22) y

by the deﬁnition of f ,
by the deﬁnition of (cid:22).

Deﬁnition 11.

• Two test statistics are equivalent if they induce the same test order.
• The canonic version of a test statistic f is the test statistic

ˆf (x) = P[f ≤ f (x)].

• A test statistic is canonic if it is the canonic version of itself.

(cid:3)

(cid:3)

Lemma 12. Every test statistic f is equivalent to a unique canonic test statistics,
namely to ˆf .

P-VALUES

7

Proof. First we check that f is equivalent to ˆf .

f (x) ≤ f (y) ⇐⇒ P[f ≤ f (x)] ≤ P[f ≤ f (y)]

⇐⇒ ˆf (x) ≤ ˆf (y)

by Lemma 9,
by the deﬁnition of ˆf .

Second we check that ˆf is the only canonic test statistic equivalent to f . If f ′ is a
canonic test statistic equivalent to f then

f ′(x) ≤ f ′(y) ⇐⇒ f (x) ≤ f (y)
⇐⇒ ˆf (x) ≤ ˆf (y)

because f and f ′ are equivalent,
because f and ˆf are equivalent.

Thus f ′, ˆf and f are all equivalent and therefore induce the same order ≤.

f ′(x) = P[f ′ ≤ f ′(x)]

= P[≤ x]
= P[ ˆf ≤ ˆf (x)]
= ˆf (x)

as f ′ is canonic
by Lemma 8, part 1

by Lemma 8, part 1
as ˆf is canonic.

(cid:3)

Corollary 13.

• If a test statistic f induces a test order ≤ then ≤ induces the canonic version

ˆf of f .

• The test statistic induced by any test order is canonic.

Proof.

(1) Let f ′ be the test statistic induced by ≤. We have

f ′(x) = P[≤ x]

= P[f ≤ f (x)]
= ˆf (x)

by the deﬁnition of f ′,
by Lemma 9,
by the deﬁnition of ˆf .

(2) Let f be the test statistic induced by a given test order ≤. By Lemma 10, f
induces ≤. By (1), f = ˆf .
(cid:3)

7. Exact p-values

In this section we deﬁne exact p-values. A more general notion of p-value will be

given in the next section.

Deﬁnition 14. Given a probabilistic trial (Ω, P) furnished with a test order ≤, the
exact p-value of an outcome x is the probability P[≤ x].

8

YURI GUREVICH AND VLADIMIR VOVK

Notice the dependence on the test order. Of course, the test order can be given
by means a test pyramid or test statistic. Test statistics are especially popular in
applications. In this connection, let us give an alternative deﬁnition of exact p-values.

Deﬁnition 15. Given a probabilistic trial (Ω, P) furnished with a test statistic f ,
the exact p-value of an outcome x is the number ˆf (x) = P[f ≤ f (x)].

Of course the two deﬁnitions are consistent.

Lemma 16.

(1) If a test statistic f induces a test order ≤ then P[≤ x] = P[f ≤ f (x)].
(2) If a test order ≤ induces a test statistic f then P[f ≤ f (x)] = P[≤ x].

Proof.

(1) Use Lemma 9, part 1.

(2) By Lemma 10, f induces ≤. Now use (1).

(cid:3)

Q: Let’s go back to the two examples of §3, compute the exact p-values of the actual
outcomes and compare them.

A: The examples do not have suﬃcient information. We need to furnish them with
reasonable test tools. For the Coin Example, we can use the test statistic M(x) =
Min {Heads (x), Tails (x)} deﬁned in the continuation of the Coin Example in §4.
With respect to that test statistic, the exact p-value p1 of the actual outcome is as
follows.

p1 = P[f ≤ 1] =

2(1 + 42)

242

<

2 × 26
242 = 2−35

Concerning the Lottery Example, one possible approach is given by the following

graph G.

• The vertices of G are the 4,000,000 lottery participants plus John, the lottery

organizer, whether he bought any lottery tickets or not.

• Two distinct vertices X and Y are connected by an edge in G if and only if

at least one of the following conditions holds:

– X is a parent of Y or the other way round.
– X, Y are spouses.
– X, Y are close friends.

Given the graph G, we have a natural test statistic δ(X) on the lottery participants,
namely the length of the minimal chain of edges from John to X. If John bought at
least one ticket then the minimal value of δ is 0; otherwise it is one. Donna, John’s

wife, is at distance 1 from John, and she bought 4 tickets. We can estimate the exact
p-value p2 of the actual outcome from below.

P-VALUES

9

p2 = P(δ ≤ 1) ≥

4

10, 000, 000

>

22

24 × 210 × 210 = 2−22.

It follows that

p1 < 2−35 < 2−22 < p2.

Q: This is interesting. The win of the lottery organizer’s wife seemed more striking to
me than getting 41 heads in 42 coin tosses. I wonder whether p2 is small enough to
impugn the hypothesis that the lottery was fair.

A: Well, it seems safe to assume that the lottery participants at distance ≤ 1 from
John bought less than 10,000 tickets. Under this assumption,

p2 <

10, 000

10, 000, 000

=

1

1000

= 0.1%

which is small enough for the lax as well as strict Cournotians.

8. p-functions

In this section ε ranges over the non-negative reals.

Lemma 17. Let f be a canonic test statistic.

(1) P[f ≤ ε] = ε if ε ∈ Range(f ).
(2) P[f ≤ ε] = ε if ε = sup(S) for some S ⊆ Range(f ).
(3) P[f ≤ ε] ≤ ε for every ε.

Proof. (1) If ε = f (x) then

P[f ≤ ε] = P[f ≤ f (x)]

= f (x)
= ε

as f is canonic

(2) If ε = sup(S) for some S ⊆ Range(f ) then there is a sequence s1 < s2 < . . . of
elements of S converging to ε. Then

P[f ≤ ε] = P[f ≤ s1] + P[s1 < f ≤ s2] + P[s2 < f ≤ s3] + · · ·

so that the probability

P[f ≤ sn] = P[f ≤ s1] + P[s1 < f ≤ s2] + · · · + P[sn−1 < f ≤ sn]

10

YURI GUREVICH AND VLADIMIR VOVK

converges to P[f ≤ ε] as n → ∞. Therefore

P[f ≤ ε] = lim

n→∞

P[f ≤ sn]

= lim

n→∞

sn

= ε

by (2)

by the choice of s1 < s2 < . . . .

(3) Let ε0 = sup{s ∈ Range(f ) : s ≤ ε}. Then [f ≤ ε] = [f ≤ ε0] and therefore
P[f ≤ ε] = P[f ≤ ε0] = ε0 ≤ ε.
(cid:3)

Deﬁnition 18.

• A p-function is a test statistic f such that P[f ≤ ε] ≤ ε for every ε.
• A p-function f is exact if P[f ≤ ε] = ε for every ε ∈ Range(f ); otherwise f is

conservative.

• Values of a p-function are p-values.

If f is a p-function then cf is a p-function for every c ≥ 1. Indeed

P[cf ≤ ε] = P[f ≤ ε/c] ≤ ε/c ≤ ε.

If c < 1 then cf may not be a p-function. In particular if P[f ≤ ε] = ε for at least
one ε then cf is not a p-function because, for that ε, we have

P[cf ≤ cε] = P[f ≤ ε] = ε > cε.

Theorem 19. For any test statistic f , the following two claims are equivalent:

(1) f is canonic.
(2) f is an exact p-function.

Proof. The implication (1)→(2) is proven in Lemma 17. To prove the implication
(2)→(1), assume that f is an exact p-function, x is an arbitrary outcome and ε = f (x).
We have

P[f ≤ f (x)] = P[f ≤ ε]

= ε

= f (x).

as f is exact

(cid:3)

Corollary 20.

• Exact p-values are values of a exact p-function.
• Every test order induces a unique exact p-function.
• Every test statistic f is equivalent to a unique exact p-function.

P-VALUES

11

9. Data snooping

Q: What about conservative p-functions? Do you really have any use for them?

A: Yes. A good example is the so-called Bonferroni correction. There is a good
explanation in [1].

Q: But what is the idea?

A: Consider a version of our coin example where, for simplicity, you toss a coin only
15 times. Again the null hypothesis is that all outcomes are equally probable, and
again we will use the test statistic M(x) = Min{Heads(x), Tails(x)}. Suppose that
you really want to impugn the null hypothesis. So you repeat the trial over and over
until you encounter an outcome where M is 1. At this point, you report that you
performed a trial and got an outcome with p-value

P[M ≤ 1] =

2(1 + 15)

215

=

25
215 = 2−10 =

1

1024

.

Q: But this is cheating.

A: Exactly. Yet, such data snooping occurs in science though rarely in such a blatant
form; a scientist may genuinely forget about some steps in the preliminary analysis of
data. In this connection, you may enjoy a relevant issue [6] of the XKCD webcomic.

Q: I remember hearing about publication bias. What is it?

A: It is a rather insidious kind of data snooping.
Imagine that a large number of
groups of scientists are testing an important null hypothesis. Eventually some group
gets a signiﬁcant p-value and submits their ﬁndings for publication while the other
groups don’t publish anything on the issue. Some inﬂuential statisticians argue that
most published research ﬁndings are wrong and that publication bias may be the
main reason for that [5].

Q: You cannot avoid multiple testing of a null hypothesis. What do you do?

A: Consider a case where the same probabilistic trial is performed n times, using
test statistics f1, . . . , fn and getting p-values p1, . . . , pn. Of course one would love
to report the p-value min{p1, . . . , pn} but it needs to be adjusted for the multiple
testing of the null hypothesis. The Bonferroni correction is one way to adjust that
p-value, by multiplying it by n. The p-value n·min{p1, . . . , pn} is valid though usually
conservative. More exactly the test statistic f = n min(f1, . . . , fn) is a p-function,
typically conservative. Indeed,

P[f ≤ ε] ≤ P(cid:0) ∪n

i=1 [fi ≤ ε/n](cid:1) ≤

n

X

i=1

P[fi ≤ ε/n] ≤

n

X

i=1

ε/n = ε.

12

YURI GUREVICH AND VLADIMIR VOVK

Acknowledgments. We thank Andreas Blass for useful comments on a draft of this
paper.

References

[1] Herv´e Abdi, “Bonferroni and ˇSid´ak corrections for multiple comparisons,”
http://www.utdallas.edu/~herve/Abdi-Bonferroni2007-pretty.pdf,
accessed January 8, 2016. Appeared in N. J. Salkind (ed.), Encyclopedia of Measurement and
Statistics, Sage Publications, 2007.

[2] D. R. Cox and D. V. Hinkley, “Theoretical Statistics,” Chapman & Hall, 1974.
[3] Yuri Gurevich and Grant O. Passmore, “Impugning Randomness, Convincingly,” Bulletin of

EATCS 104, June 2011; also — with an added Prologue — in Studia Logica 82 (2012), 1–31.

[4] Yuri Gurevich and Vladimir Vovk, “Fundamentals of p-values,” unpublished manuscript.
[5] John P. A. Ioannidis, “Why Most Published Research Findings Are False”, PLoS Medicine,

Volume 2, Issue 8, e124, August 2005.

[6] XKCD, http://xkcd.com/882/, accessed on Jan. 8, 2016.

