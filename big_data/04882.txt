6
1
0
2

 
r
a

 

M
5
1

 
 
]
L
M

.
t
a
t
s
[
 
 

1
v
2
8
8
4
0

.

3
0
6
1
:
v
i
X
r
a

Bias Correction for Regularized Regression and its Application in

Learning with Streaming Data

Qiang Wu

Department of Mathematical Sciences

Middle Tennessee State University

Murfreesboro, TN 37132

qwu@mtsu.edu

Abstract

We propose an approach to reduce the bias of ridge regression and regularization kernel network.

When applied to a single data set the new algorithms have comparable learning performance with

the original ones. When applied to incremental learning with block wise streaming data the new

algorithms are more eﬃcient due to bias reduction. Both theoretical characterizations and simulation

studies are used to verify the eﬀectiveness of these new algorithms.

1 Introduction

As modern technologies allow to collect data much easily, the size of data sets is growing fast in both

dimensionality and number of instances. This makes big data become ubiquitous in many ﬁelds and

draw great attentions of researchers in recent years. In the statistics and machine learning context, many

traditional data processing tools and techniques become inviable due to the big size of the data. New

models and computational techniques are required. This had driven the resurgence of the research in

online learning and the use of parallel computing.

Online learning deals with streaming data. Online algorithms update the knowledge incrementally as

new data come in. The streaming data could be instance wise or block wise. The instance wise streaming

data could be processed as block wise data. This may be preferred in some particular application domains.

For instance, in the dynamic pricing problems (see e.g.

[33, 2]) the price is usually not updated each

time when an instance of sales information becomes available, because customers may not like the price

changing too constantly. When processing block wise streaming data, a base algorithm is applied to

each incoming data block and a coupling method is then used to update the knowledge by combining

the knowledge from the past blocks and the incoming block; see e.g.

[9, 16].

In statistical learning

theory where the knowledge is usually represented by a target function, the simplest way to couple the

information is to use the average of the functions learnt from diﬀerent blocks. In learning with big data,
the divide and conquer algorithm [38] divides the whole data set into smaller subsets, applies a base

1

learning algorithm on each subset, and takes the average of the learnt functions from all subsets as the

target function for prediction purpose. It is computationally eﬃcient because the second stage could be

implemented via parallel computing. Although the divide and conquer algorithm is diﬀerent from the

aforementioned online learning with block wise streaming data, they clearly share the same spirit – a

base algorithm for a single data block is required and the average of the outputs from this algorithm over

multiple data blocks is used. A natural problem arising from these two frameworks is the choice of the

base learning algorithm for a single data set. As an algorithm is eﬃcient and optimal for a single data

set, it is not necessarily eﬃcient and optimal for learning with block wise data.

In this paper we focus on the regression learning problem where a set of observations are collected for

p predictors and a scalar response variable. Assume they are linked by

yi = f∗(xi) + i,

i = 1, 2, . . . , n,

where xi ∈ Rp, yi ∈ R, and i is the zero-mean noise. The target is to recover the unknown true model
f∗ as accurate as possible to understand the impact of predictors and predict the response on unobserved
data. The ordinary least square (OLS) is the most traditional and well developed method. It assumes a

linear model and estimates the coeﬃcients by minimizing the squared error between the responses and the

predictions. The OLS estimator requires the inverse of the covariance matrix of the explanatory variables

and could be numerically unstable if the covariance matrix is singular or has very large condition number.

A variety of regularization approaches have been introduced to overcome the numerical instability and/or

for some other purposes (e.g. sparsity). Typical regularized methods include ridge regression [18, 17],

LASSO [31], elastic net [39] and many others. The nonlinear extension of ridge regression could be

implemented by regularization kernel network [13]. The data are ﬁrst mapped to a feature space. Then

a linear model is built in the feature space which, when projected back to the original space, becomes a

nonlinear model.

Although diﬀerent regularization techniques have diﬀerent properties, they share some common fea-

tures. The estimators obtained from regularized regression are usually biased. The regularization is

helpful to improve the computational stability and reduce the variance. By trading oﬀ the bias and vari-

ance, regularization schemes may lead to smaller prediction errors than unbiased estimators. Therefore,

regularization theory has become an important topic in the statistical learning context.

Regularization algorithms, such as the ridge regression, regularization kernel network, and support

vector machines, have been successful in a variety of regression and classiﬁcation applications. However,

they may be suboptimal when they serve as base algorithms in learning with block wise streaming data or

in the divide and conquer algorithm. When there are many data blocks, the regularization algorithm may

provide good estimator for each data block. By coupling the estimators together, the variance usually

shrinks when more and more data blocks are taken into account. But the bias may not shrink and prevent

the algorithm to achieve optimal performance. To overcome this diﬃculty, adjustments are required to

remove or reduce the bias of the algorithm.

In the paper, we will propose an approach to correct the bias of ridge regression and regularization

kernel network. The resulted two new algorithms and their properties will be described in Section 2 and

Section 4. Their theoretical properties are proved in Section 3 and Section 5, respectively. In Section 6

2

we discuss why the new algorithms are eﬀective in learning with block wise data. In Section 7 simulations

are used to illustrate their eﬀectiveness from an empirical aspect. We close by Section 8 with conclusions

and discussions.

1.1 Related Work

The idea of bias correction has long history in statistics. For instance, bias correction to maximum

likelihood estimation dates at least back to 1950s [25] and a variety method were proposed later on;

see e.g.

[22, 26, 14, 11]. Bias reduction to kernel density estimators was studied in [6, 1, 20, 10]. Bias

correction to nonparametric estimation was studied in [15, 24, 35].

The existence of bias in ridge regression and its impact on statistical inference has been noticed since

its invention [18, 23]. In high dimensional linear models where the dimension greatly exceeds the sample

size, bias correction method was introduced in [7] to correct the projection bias, the diﬀerence of the true

regression coeﬃcient and its projection in the subspace spanned by the sample, which appears because the
sample cannot span the whole Euclidian space Rp as n (cid:28) p. In [36, 7, 19], projection bias correction was
introduced to LASSO in high dimensional linear models. The purpose of projection bias is to construct

accurate p values to facilitate accurate statistical inference such as hypothesis testings and conﬁdence

intervals. It seems the bias caused by regularization has minimal impact for this purpose.

As for regularization kernel network, its predictive consistency has been extensively studied in the

literature; see e.g. [5, 37, 12, 34, 4, 8, 27, 30, 28] and many references therein. Its application was also

extensively explored and shown successful in many problem domains. But to my best knowledge, the idea

of bias correction to improve this algorithm is novel. Note that bias reduction for regularized regression

does not improve the learning performance on a single data set, as illustrated in Section 7. It is worthy

of exploration because of its eﬀectiveness in learning with streaming data or distributed regression.

2 Bias correction for ridge regression

In linear regression, the response variable is assumed to linearly depend on the explanatory variables, i.e.

yi = w(cid:62)xi + b + i

with some w ∈ Rp and b ∈ R. Ridge regression minimizes the penalized mean squared prediction error
on the observed data

n(cid:88)

i=1

1
n

(yi − w(cid:62)xi − b)2 + λ(cid:107)w(cid:107)2,

where λ > 0 is the regularization parameter used to trade oﬀ the ﬁtting error and model complexity.
Let ¯x denote the sample mean of xi’s and ¯y be the sample mean of yi’s. Denote by ˜X = [x1 − ¯x, x2 −
¯x, . . . , xn− ¯x] the centered data matrix for the explanatory variables and ˜Y = (y1− ¯y, y2− ¯y, . . . , yn− ¯y)(cid:62)
the vector of centered response values. Then the sample covariance matrix of x is ˆΣ = 1
˜X ˜X T and the
n
solution to the ridge regression is given by

(cid:16)

ˆw =

1
n

(cid:17)−1

˜X(cid:62) ˜Y

λI + ˆΣ

3

and ˆb = ¯y − ˆw¯x. Here and in the sequel, I denotes the identity matrix (or the identity operator).

The solution ˆw is a biased estimator for w. Deﬁne

and

b(n, λ) = (cid:107)E[ ˆw] − w(cid:107)

v(n, λ) = E(cid:2)(cid:107) ˆw − E[ ˆw](cid:107)2(cid:3) .

Then b(n, λ) is the Euclidian norm of the bias and v(n, λ) is the variance. The mean squared error is

given by

mse(n, λ) = E(cid:2)(cid:107) ˆw − w(cid:107)2(cid:3) = b2(n, λ) + v(n, λ).

Denote by Σ the covariance matrix of the explanatory variables x. Let σi be the eigenvalues of Σ and

vi the corresponding eigenvectors. Then

p(cid:88)

i=1

Σ =

σiviv(cid:62)
i .

The vectors vi are the principal components. The following theorem characterizes the bias and variance
of ridge regression.
Theorem 1. If x is bounded and E[2] < ∞, then

(i) E[ ˆw] converges (λI + Σ)−1Σw as n → ∞.

(ii) If w =(cid:80)p

i=1 civi, then

lim
n→∞ b(n, λ) =

(iii) v(n, λ) = O( 1

nλ2 ).

(cid:118)(cid:117)(cid:117)(cid:116) p(cid:88)

i=1

λ2c2
i

(λ + σi)2 .

Without loss of generality, we assume the eigenvalues are in decreasing order, i.e., σi ≥ σi+1. Then
is increasing. Theorem 1 tells that, for a ﬁxed λ, the bias of ridge regression will be small if the true

λ+σi
model w heavily depends on the ﬁrst several principle components. Conversely, if w heavily depends on

λ

the last several principle components, the bias will be large.

According to Theorem 1 (i), the asymptotic bias of ridge estimator is −λ(λI + Σ)−1w. If we can
subtract it from the ridge regression estimator, we are able to obtain an asymptotically unbiased estimator
ˆw + λ(λI + Σ)−1w. However, this is not computationally feasible because both Σ and w are unknown.
Instead, we propose to replace Σ by its sample version ˆΣ and w by the ridge estimator ˆw. The resulted
new estimator, which we call bias corrected ridge regression estimator, becomes

ˆw(cid:93) = ˆw + λ(λI + ˆΣ)−1 ˆw.

(1)

Since the bias correction uses an estimated quantity, this new estimator is still biased. But the bias

is reduced. Let

b(cid:93)(n, λ) = (cid:107)E[ ˆw(cid:93)] − w(cid:107)

4

and

v(cid:93)(n, λ) = E(cid:2)(cid:107) ˆw(cid:93) − E[ ˆw(cid:93)](cid:107)2(cid:3) .

We have the following conclusion.
Theorem 2. If x is bounded and E[2] < ∞, then

(i) E[ ˆw(cid:93)] converges to (λI + Σ)−2(2λI + Σ)Σw as n → ∞.

(ii) If w =(cid:80)p

i=1 civi, then

n→∞ b(cid:93)(n, λ) =
lim

(iii) vb(n, λ) = O( 1

nλ2 ).

(cid:118)(cid:117)(cid:117)(cid:116) p(cid:88)

i=1

λ4c2
i

(λ + σi)4 .

λ

λ+σi

Since

< 1, the asymptotic bias ˆw(cid:93) is smaller. The bias reduction could be signiﬁcant if the true
model depends only on the ﬁrst several principle components. We also remark that, although v(n, λ) and
v(cid:93)(n, λ) are of the same order, v(cid:93)(n, λ) is found slightly larger in simulations. The overall performance, as
measured by the mean squared error, of these two estimators is comparable when used in learning with

a single data set.

3 Proofs of Theorem 1 and Theorem 2

In this section we proveTheorem 1 and Theorem 2. To this end, we ﬁrst introduce several useful lemmas.

In our analysis, we will deal with vector or operator valued random variables. We need the following

inequalities for Hilbert space valued random variables.
Lemma 3. Let ξ be a random variable with values in a Hilbert space H. Then for any h ∈ H we have

Proof. The proof is quite direct:

E(cid:2)(cid:107)ξ − E[ξ](cid:107)2(cid:3) = E

E

(cid:104)(cid:107)ξ − E[ξ](cid:107)2(cid:105) ≤ E(cid:2)(cid:107)ξ − h(cid:107)2(cid:3) .
(cid:104)(cid:107)(ξ − h) − (E[ξ] − h)(cid:107)2(cid:105)
= E(cid:2)(cid:107)ξ − h(cid:107)2(cid:3) − 2E [(cid:104)ξ − h, E[ξ] − h(cid:105)] + (cid:107)E[ξ] − h(cid:107)2
= E(cid:2)(cid:107)ξ − h(cid:107)2(cid:3) − (cid:107)E[ξ] − h(cid:107)2
≤ E(cid:2)(cid:107)ξ − h(cid:107)2(cid:3) .

(cid:4)
Lemma 4. Let H be a Hilbert space and ξ be a random variable with values in H. Assume that (cid:107)ξ(cid:107) < M
almost surely. Let {ξ1, ξ2, . . . , ξn} be a sample of n independent observations for ξ. Then

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 1

n

n(cid:88)

i=1

E

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2 ≤ M 2

n

ξi − E[ξ]

(cid:34)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 1

n

n(cid:88)

i=1

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
(cid:35)

ξi − E[ξ]

≤ M√
n

.

and

E

5

Proof. Since E[ξi] = E[ξ] for all i and ξi are mutually independent, we have

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 1

n

n(cid:88)

i=1

E

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2 = E

ξi − E[ξ]

(cid:43)(cid:35)

+ (cid:107)E[ξ](cid:107)2

ξi, E[ξ]

(cid:34)(cid:42)

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2 − 2E

n(cid:88)

i=1

1
n

E(cid:104)ξi, ξj(cid:105) − (cid:107)E[ξ](cid:107)2

(cid:107)E[ξ](cid:107)2

n

ξi

i=1

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 1
n(cid:88)
n(cid:88)
n(cid:88)
E(cid:2)(cid:107)ξ(cid:107)2(cid:3) − 1
E(cid:2)(cid:107)ξ(cid:107)2(cid:3)

j=1

i=1

n

=

1
n2

1
=
n
≤ 1
n
≤ M 2
n

.

This proves the ﬁrst inequality. The second one follows from the ﬁrst one and Schwartz inequality.

(cid:4)

In the sequel, we assume x is uniformly bounded by M ≥ 0.

Lemma 5. We have

(cid:104)(cid:107) ˆΣ − Σ(cid:107)2(cid:105) ≤ 10M 4

Proof. Let µ = E[x] be the mean of x. Note that Σ = E(cid:2)(x − µ)(x − µ)(cid:62)(cid:3) = E[xx(cid:62)] − µµ(cid:62) and

and

E

n

similarly

Thus,

ˆΣ =

(xi − ¯x)(xi − ¯x)(cid:62) =

(cid:114) 10

n

E

(cid:104)(cid:107) ˆΣ − Σ(cid:107)(cid:105) ≤ M 2
n(cid:88)
i − E(cid:2)xx(cid:62)(cid:3)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2
+ 2(cid:13)(cid:13)¯x¯x(cid:62) − µµ(cid:62)(cid:13)(cid:13)2
i − E(cid:2)xx(cid:62)(cid:3)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2

+ 8M 2(cid:107)¯x − µ(cid:107)2,

i − ¯x¯x(cid:62).

xix(cid:62)

1
n

i=1

F

n(cid:88)
n(cid:88)

i=1

i=1

xix(cid:62)

xix(cid:62)

(cid:13)(cid:13)(cid:13) ˆΣ − Σ

n

i=1

1
n

n(cid:88)
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 1
(cid:13)(cid:13)(cid:13)2 ≤ 2
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 1
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 1

≤ 2

E

n

n

where (cid:107) · (cid:107)F represent the Frobenius norm and we have used the fact that (cid:107) · (cid:107) ≤ (cid:107) · (cid:107)F for all matrices.
Recall that matrices of d × d form a Hilbert space with the Frobenius norm (cid:107) · (cid:107)F . Applying Lemma

4 to ξ = xx(cid:62) which satisﬁes (cid:107)ξ(cid:107)F = (cid:107)x(cid:107)2 ≤ M 2, we obtain

(2)

 ≤ M 4

n

n(cid:88)

i=1

xix(cid:62)

i − E(cid:2)xx(cid:62)(cid:3)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2
E(cid:2)(cid:107)¯x − µ(cid:107)2(cid:3) ≤ M 2

F

.

n

.

(3)

(4)

Next we apply Lemma 4 to ξ = x and obtain

Then the ﬁrst inequality follows by taking expectation on both sides of (2) and applying (3) and (4).
(cid:4)

The second one follows from the ﬁrst one and Schwartz inequality.

6

Now we can prove the two theorems.

Proof of Theorem 1. Note that E[ ˜Y | ˜X] = ˜Xw and thus E[ ˆw] = (λI + ˆΣ)−1 ˆΣw. We have

(cid:13)(cid:13)(cid:13)(λI + ˆΣ)−1 ˆΣw − (λI + Σ)−1Σw
(cid:13)(cid:13)(cid:13)(cid:16)
≤(cid:13)(cid:13)(cid:13)(λI + ˆΣ)−1( ˆΣ − Σ)w

(cid:13)(cid:13)(cid:13)
(λI + ˆΣ)−1 − (λI + Σ)−1(cid:17)

(cid:13)(cid:13)(cid:13)(λI + ˆΣ)−1(Σ − ˆΣ)(λI + Σ)−1Σw

(cid:13)(cid:13)(cid:13) +

(cid:13)(cid:13)(cid:13)

(cid:13)(cid:13)(cid:13)

Σw

(cid:107) ˆΣ − Σ(cid:107) +
(cid:107) ˆΣ − Σ(cid:107)(cid:107)w(cid:107).

≤ 1
λ
≤ 2
λ

The conclusion (i) follows from

(cid:107)E[ ˆw] − (λI + Σ)−1Σw(cid:107) ≤ E

(cid:104)(cid:13)(cid:13)(cid:13)(λI + ˆΣ)−1 ˆΣw − (λI + Σ)−1Σw

(cid:13)(cid:13)(cid:13)(cid:105)

(cid:104)(cid:107) ˆΣ − Σ(cid:107)(cid:105)
(cid:114) 10

−→ 0.

n

≤ 2(cid:107)w(cid:107)
E
2M 2(cid:107)w(cid:107)

λ

=

λ

The conclusion (ii) is an easy consequence of (i) by noting that

.

lim

n→∞(cid:107)E[ ˆw] − w(cid:107)2 =(cid:13)(cid:13)(λI + Σ)−1Σw − w(cid:13)(cid:13)2
(cid:17)
ˆw = (λI + ˆΣ)−1 ˆΣw + (λI + ˆΣ)−1(cid:16) 1
(cid:13)(cid:13)(cid:13)2(cid:21)
(cid:20)(cid:13)(cid:13)(cid:13) 1

˜XR

˜XR

=

n

n

.

1
n2

n(cid:88)
E(cid:2)ij(xi − ¯x)(cid:62)(xi − ¯x)(cid:3)
E(cid:2)2
i(cid:107)xi − ¯x(cid:107)2(cid:3)

j=1

=

1
n2

E

n(cid:88)
n(cid:88)

i=1

i=1

n

Denote R = [1, . . . , n](cid:62) to be the column vector of residuals. By the fact yi − ¯y = (xi − ¯x)(cid:62)w + i

we have ˜X ˜Y = ˜X ˜X(cid:62)w + ˜XR and

By the fact E[ij| ˜X] = 0 for i (cid:54)= j and (cid:107)xi − ¯x(cid:107) ≤ 2M , we obtain

By Lemma 3 and Lemma 5, we have

(cid:104)(cid:107) ˆw − E[ ˆw](cid:107)2(cid:105) ≤ E

E

≤ 4M 2E[2]

.

(cid:104)(cid:13)(cid:13) ˆw − (λI + Σ)−1Σw(cid:13)(cid:13)2(cid:105)
(cid:20)(cid:13)(cid:13)(cid:13)(λI + ˆΣ)−1 ˆΣw − (λI + Σ)−1Σw
(cid:13)(cid:13)(cid:13)2(cid:21)
(cid:20)(cid:13)(cid:13)(cid:13) 1
(cid:20)(cid:13)(cid:13)(cid:13) ˆΣ − Σ
(cid:13)(cid:13)(cid:13)2(cid:21)

≤ 2E
≤ 4(cid:107)w(cid:107)2
λ2 E
≤ 40M 4(cid:107)w(cid:107)2 + 8M 2E[2]

(cid:13)(cid:13)(cid:13)2(cid:21)

2
λ2 E

˜XR

+

n

nλ2

(cid:20)(cid:13)(cid:13)(cid:13)(λI + ˆΣ)−1(cid:16) 1

n

(cid:17)(cid:13)(cid:13)(cid:13)2(cid:21)

˜XR

+ 2E

(5)

(cid:4)

This veriﬁes (iii).

.

7

Proof of Theorem 2. It is easy to verify that ˆw(cid:93) = (λI + ˆΣ)−2(2λI + ˆΣ)

(cid:104)

(λI + ˆΣ)−2(2λI + ˆΣ) ˆΣw

E

(cid:16) 1

n

˜X ˜Y

(cid:17)

. Therefore, E[ ˆw(cid:93)] =

w − (λI + Σ)−2(cid:2)(λI + Σ)2 − λ2I(cid:3) w

. To prove (i) we write

(λI + ˆΣ)2 − λ2I

(λI + ˆΣ)−2(2λI + ˆΣ) ˆΣw − (λI + Σ)−2(2λI + Σ)Σw

(cid:105)
(λI + ˆΣ)−2 − (λI + Σ)−2(cid:105)

(cid:105)
= (λI + ˆΣ)−2(cid:104)
= λ2(cid:104)
= λ2(λI + ˆΣ)−2(cid:104)
(cid:13)(cid:13)(cid:13)(λI + ˆΣ)−2(2λI + ˆΣ) ˆΣw − (λI + Σ)−2(2λI + Σ)Σw
(cid:13)(cid:13)(cid:13)(cid:105) ≤ 4M 2(cid:107)w(cid:107)

2λ(Σ − ˆΣ) + (Σ − ˆΣ)Σ + ˆΣ(Σ − ˆΣ)
λ2 , (λI + ˆΣ)−2 ˆΣ ≤ 1

(cid:104)(cid:13)(cid:13)(cid:13) ˆΣ − Σ

λ2 , (λI + Σ)−2 ≤ 1

≤ 4(cid:107)w(cid:107)

w

E

(cid:105)
(cid:13)(cid:13)(cid:13) ≤ 4(cid:107)w(cid:107)
(cid:114) 10

λ

λ , and Σ(λI + Σ)−2 ≤ 1

λ

n

λ

(cid:104)(cid:13)(cid:13)(cid:13)(λI + ˆΣ)−2(2λI + ˆΣ) ˆΣw − (λI + Σ)−2(2λI + Σ)Σw

λ , we obtain

(cid:13)(cid:13)(cid:13) ˆΣ − Σ

(cid:13)(cid:13)(cid:13) .

(λI + Σ)−2w.

−→ 0.

(6)

(cid:13)(cid:13)(cid:13)(cid:105)

By (λI + ˆΣ)−2 ≤ 1

The conclusion (i) follows from the following estimate:

(cid:107)E[ ˆw(cid:93)] − ˆΣ)−2(2λI + ˆΣ) ˆΣw(cid:107) ≤ E

The conclusion (ii) is an easy consequence of (i).
To prove (iii), we write ˆw(cid:93) = (λI + ˆΣ)−2(2λI + ˆΣ) ˆΣw + (λI + ˆΣ)−2(2λI + ˆΣ)( 1

˜XR). By Lemma 3,

Lemma 5, the estimates in (6) and (5), we have

(cid:104)(cid:13)(cid:13) ˆw(cid:93) − E[ ˆw(cid:93)](cid:13)(cid:13)2(cid:105) ≤ E

E

≤ 2E

n

(cid:104)(cid:13)(cid:13) ˆw(cid:93) − (λI + Σ)−2(2λI + Σ)Σw(cid:13)(cid:13)2(cid:105)
(cid:20)(cid:13)(cid:13)(cid:13)(λI + ˆΣ)−2(2λI + ˆΣ) ˆΣw − (λI + Σ)−2(2λI + Σ)Σw
(cid:20)(cid:13)(cid:13)(cid:13)(λI + ˆΣ)−2(2λI + ˆΣ)
(cid:16) 1
(cid:20)(cid:13)(cid:13)(cid:13) 1
(cid:20)(cid:13)(cid:13)(cid:13) ˆΣ − Σ

(cid:17)(cid:13)(cid:13)(cid:13)2(cid:21)
(cid:13)(cid:13)(cid:13)2(cid:21)

(cid:13)(cid:13)(cid:13)2(cid:21)

˜XR

˜XR

+ 2E

n

n

8
λ2 E

(cid:13)(cid:13)(cid:13)2(cid:21)

≤ 16(cid:107)w(cid:107)2
λ2 E
≤ 160M 4(cid:107)w(cid:107)2 + 32M 2E[2]

+

,

nλ2

where we have used the (cid:107)(λI + ˆΣ)−2(2λI + ˆΣ)(cid:107) = (cid:107)λ(λI + ˆΣ)−2 + (λI + ˆΣ)−1(cid:107) ≤ 2
λ .

(cid:4)

4 Bias correction for regularization kernel network
When the true regression model is nonlinear, kernel method can be used. Denote by X the space of
explanatory variables. A Mercer kernel is a continuous, symmetric, and positive-semideﬁnite function
K : X × X → R. The inner product deﬁned by (cid:104)K(x,·), K(t,·)(cid:105)K = K(x, t) induces a reproducing
kernel Hilbert space (RKHS) HK associated to the kernel K. The space is the closure of the function
class spanned by {K(x,·), x ∈ X}. The reproducing property f (x) = (cid:104)f, K(x,·)(cid:105)K leads to |f (x)| ≤

(cid:112)K(x, x)(cid:107)f(cid:107)K. Thus HK can be embedded into L∞. We refer to [3] for more other properties of RKHS.

8

n(cid:88)

1
n

The regularization kernel network [13] estimates the true model f∗ by a function ˆf ∈ HK that

minimizes the regularized sample mean squared error

(yi − f (xi))2 + λ(cid:107)f(cid:107)2
K.

The representer theorem [32] tells that ˆf (x) = (cid:80)n

i=1

i=1 ciK(xi, x). So although the RKHS HK may be
inﬁnitely dimensional, the optimization of the regularization kernel network could be implemented in an
n dimensional space. Actually, let K denote the kernel matrix on x1, . . . , xn and Y = (y1,··· , yn)(cid:62). The
coeﬃcients c = (c1, . . . , cn)(cid:62) could be solved by a linear system (λnI + K)c = Y.

by Sf = (f (x1), . . . , f (xn))(cid:62) for f ∈ HK. Its dual operator, S∗ is given by S∗c =(cid:80)n

In [27] an operator representation for ˆf is proved. Let S : HK → Rn be the sampling operator deﬁned
i=1 ciK(xi,·) ∈ HK

for c ∈ Rn. Then we have

The operator 1

n S∗S is a sample version of the integral operator LK deﬁne by

LKf (x) = Et [K(x, t)f (t)] =

K(x, t)f (t)dPX (t)

(cid:18)

ˆf =

1
n

λI +

S∗S

1
n

S∗Y.

(cid:19)−1

(cid:90)

X

where PX is the marginal distribution on X . Note that LK deﬁnes a bounded operator both on L2
(associate to the probability measure PX ) and HK. Let τi and φi be the eigenvalues and eigenfunctions
of LK. Then {φi}∞

i=1 form an orthogonal basis of L2 and

LKf =

τi(cid:104)f, φi(cid:105)

L2 φi,

∀ f ∈ L2.

Also, {ψi =

√

τiφi : τi (cid:54)= 0} form an orthonormal basis of HK and, as an operator on HK,

∞(cid:88)
(cid:88)

i=1

LKf =

i:τi(cid:54)=0

τi(cid:104)f, ψi(cid:105)K ψi,

∀ f ∈ HK.

Moreover, L1/2

K maps all functions in L2 onto HK and

(cid:107)f(cid:107)L2 = (cid:107)L1/2

K f(cid:107)K

∀ f ∈ span{φi : τi (cid:54)= 0} ⊂ L2.

In particular, this is true for all f ∈ HK. Note that span{φi : τi (cid:54)= 0} is the closure of HK in L2. Only
functions in span{φi : τi (cid:54)= 0} can be well approximated by functions in HK.

Regularization kernel network can be regarded as a nonlinear extension of ridge regression. If f∗ ∈ HK
we can measure the diﬀerence between ˆf and f∗ in HK and prove some conclusions that are analogous to
those in Theorem 1. But unfortunately, this is generally not true. To make our result more general, we
measure the diﬀerence between ˆf and f∗ in L2 sense, which is equivalent to measure the mean squared
forecasting error. For this purpose, we deﬁne

(cid:13)(cid:13)(cid:13)ED[ ˆf ] − f∗(cid:13)(cid:13)(cid:13)L2

bK(n, λ) =

(cid:17)2(cid:21)

ED[ ˆf (x)] − f∗(x)

(cid:115)

(cid:20)(cid:16)

=

Ex

9

and

(cid:20)(cid:13)(cid:13)(cid:13) ˆf − ED[ ˆf ]

vK(n, λ) = ED

(cid:13)(cid:13)(cid:13)2

L2

(cid:21)

(cid:20)(cid:16) ˆf (x) − ED[ ˆf (x)]
(cid:17)2(cid:21)

,

= ED,x

where ED is the expectation with respect to the data and Ex is the expectation with respect to x.

(cid:112)K(x, x) < ∞ and |y| ≤ M almost surely, then

(i) ED[ ˆf ] converges to (λ + LK)−1LKf∗ in HK.

Theorem 6. If supx∈X

(ii) If f∗ = (cid:80)

αiφi, then

i:τi(cid:54)=0

(cid:118)(cid:117)(cid:117)(cid:116) (cid:88)
(cid:1) if λ = λ(n) satisﬁes nλ → ∞ and

lim
n→∞ bK(n, λ) =

i:τi(cid:54)=0

λ2α2
i
(λ + τi)2 .
√

nλ → 0.

(iii) vK(n, λ) = O(cid:0)

1

n3/2λ2

Theorem 6 (ii) characterizes the asymptotic bias for target functions that belong to span{φi : τi (cid:54)= 0}
and thus can be well learned by the regularization kernel network. If the target function has a component
orthogonal to span{φi : τi (cid:54)= 0}, the orthogonal component is not learnable and its norm should be added
to the right hand side. The variance bound in Theorem 6 (iii) is presented with the assumptions nλ → ∞
[27, 30]), usually guarantee the regularization
and

nλ → 0, which, according to the literature (e.g.

√

kernel network to achieve the optimal learning rate. When this is not true, an explicit bound can be

found in the proof in Section 5.

Following the same idea as in Section 2, we propose to reduce the bias by using an adjusted function

ˆf .

(7)

The implementation of this new approach is easy. We can verify that

(cid:18)

(cid:19)−1

ˆf (cid:93)(x) = ˆf + λ

λI +

S∗S

1
n

i=1

c(cid:93) = c + λ

λI +

K

c.

ˆf (cid:93)(x) =

c(cid:93)
iK(xi, x)

(cid:19)−1

n(cid:88)
(cid:18)
(ED[ ˆf (cid:93)(x)] − f∗(x))2(cid:105)
(cid:104)
(cid:104)
( ˆf (cid:93)(x) − ED[ ˆf (cid:93)(x)])2(cid:105)
(cid:112)K(x, x) < ∞ and |y| ≤ M almost surely, then

v(cid:93)
K(n, λ) = Ex

b(cid:93)
K(n, λ) = Ex

1
n

.

with

Let

and

Theorem 7. If supx∈X

(i) ED[ ˆf ] converges to (λ + LK)−2(2λI + LK)LKf∗ in HK.

10

(ii) If f∗ =(cid:80)

i:τi(cid:54)=0 αiφi, then

(iii) v(cid:93)

K(n, λ) = O(

1

n3/2λ2 ).

(cid:118)(cid:117)(cid:117)(cid:116) (cid:88)

i:τi(cid:54)=0

λ4α2
i
(λ + τi)4 .

n→∞ b(cid:93)
lim

K(n, λ) =

5 Proofs of Theorem 6 and Theorem 7

The proofs Theorem 6 and Theorem 7 are more complicated than those of Theorem 1 and Theorem 2

because they require techniques to handle the estimation of integral operators. Without loss of generality
we assume κ ≥ 1, M ≥ 1, and λ ≤ 1 throughout this section in order to simplify our notations. We will
always use E for ED in case there is no confusion from the context.

Lemma 8. Let ξ be positive random variable. For any q ≥ 1, E[ξq] =(cid:82) ∞

0 qtq−1 Pr[ξ > t]dt.

The following concentration inequality is proved in [27].

Lemma 9. Let H be a Hilbert space and ξ be a random variable with values in H. Assume that (cid:107)ξ(cid:107) < M
almost surely. Let {ξ1, ξ2, . . . , ξm} be a sample of m independent observations for ξ. Then for any
0 < δ < 1,

2E[(cid:107)ξ(cid:107)2] log(2/δ)

n

.

(8)

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 1

m

m(cid:88)

i=1

ξi − E(ξ)

n

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) ≤ 2M log(2/δ)
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) ≤ 2M
m(cid:88)

ξi − E(ξ)

i=1

(cid:114)

+

(cid:114)

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 1

m

.
When no information regarding E[(cid:107)ξ(cid:107)2] is available, we can use E[(cid:107)ξ(cid:107)2] ≤ M 2 to derive a simpler

estimation as follows. For any 0 < δ < 1, with conﬁdence at least 1 − δ

log(2/δ)

m

.

(9)

Before we state the next lemma, let us recall the Hilbert-Schmidt operators on HK. Let {ei} be a set
of orthonormal basis of HK. An operator T is a Hilbert-Schmidt operator if (cid:107)T(cid:107)2
K is ﬁnite.
A Hilbert-Schmidt operator is also a bounded operator with (cid:107)T(cid:107) ≤ (cid:107)T(cid:107)HS. All Hilbert-Schmidt operators
form a Hilbert space. For g, h ∈ HK, the rank one tensor operator g⊗ h is deﬁned by (g⊗ h)f = (cid:104)h, f(cid:105)Kg
for all f ∈ HK. A tensor operator is a Hilbert-Schmidt operator with (cid:107)g ⊗ h(cid:107)HS = (cid:107)g(cid:107)K(cid:107)h(cid:107)K.

i (cid:107)T ei(cid:107)2

HS =(cid:80)

Lemma 10. For any 0 < δ < 1, we have

with conﬁdence at least 1 − δ. Also, we have

(cid:34)(cid:13)(cid:13)(cid:13)(cid:13) 1

n

E

S∗S − LK

≤ κ4
n

and E

m

(cid:13)(cid:13)(cid:13)(cid:13)(cid:21)

≤ κ2√
n

.

S∗S − LK

S∗S − LK

log(2/δ)

n

(cid:13)(cid:13)(cid:13)(cid:13) 1
(cid:13)(cid:13)(cid:13)(cid:13)2(cid:35)

(cid:13)(cid:13)(cid:13)(cid:13) ≤ 2κ2

(cid:114)
(cid:20)(cid:13)(cid:13)(cid:13)(cid:13) 1

n

11

Proof. Consider the random variable ξ = K(x,·)⊗ K(x,·). It satisﬁes (cid:107)ξ(cid:107)HS = (cid:107)K(x,·)(cid:107)2
κ2, E[ξ] = LK, and 1

(cid:80)n
i=1 K(xi,·) ⊗ K(xi,·). Then the ﬁrst inequality follows from (9).

n S∗S = 1

K = K(x, x) ≤

n

The second and third inequalities have been obtained in [12, 29]. They also follow from Lemma 4
(cid:4)

easily.

Lemma 11. For any 0 < δ < 1, we have

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 1

n

n(cid:88)

i=1

(cid:114)

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) ≤ 2κM
(cid:34)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 1
n(cid:88)

n

f (xi)K(xi,·) − LKf∗

log(2/δ)

n

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2 ≤ κ2M 2
(cid:107)ξ(cid:107)K = |f (x)|(cid:112)K(x, x) ≤ κ(cid:107)f∗(cid:107)∞ ≤ κM.

and E

i=1

n

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
(cid:35)

≤ κM√
n

.

(cid:4)
n S∗Sf∗

with conﬁdence at least 1 − δ. Also, we have

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 1

n

n(cid:88)

i=1

E

f (xi)K(xi,·) − LKf∗

f (xi)K(xi,·) − LKf∗

Proof. Consider the random variable ξ = f∗(x)K(x,·). Clearly ξ ∈ HK and E[ξ] = LKf∗. Since |y| ≤ M
almost surely, |f∗(x)| ≤ M for all x ∈ X . Thus,

Then the conclusions follow from applying (9) and Lemma 4, respectively.

Remark 12. If we extend the sampling operator S to be deﬁned on L2, then the representation 1
makes sense and

n(cid:88)

i=1

S∗Sf∗ =

1
n

1
n

f (xi)K(xi,·).

In the sequel we will always adopt this simpliﬁed notation. However, we need to keep in mind that, in
a general situation where f∗ /∈ HK, 1
cannot get Lemma 11 from Lemma 10 directly.

n S∗S should not be regarded as an operator on HK. Therefore, we

Denote fλ = (λI + LK)−1LKf∗. It is known from [27] that

(cid:8)E[(y − f (x))2] + λ(cid:107)f(cid:107)2

(cid:9) .

K

fλ = arg min
f∈HK

So we have

and as a result

E[(y − fλ(x))2] + λ(cid:107)fλ(cid:107)2

K ≤ E[(y − 0)2] + λ(cid:107)0(cid:107)2

K ≤ M 2

(cid:107)fλ(cid:107)K ≤ M√
λ

.

(10)

(11)

We will need the following lemma to in our proofs.

Lemma 13. For any 0 < δ < 1, we have

S∗(Y − Sfλ) − LK(f∗ − fλ)

(cid:13)(cid:13)(cid:13)(cid:13) 1

n

with conﬁdence at least 1 − δ.

(cid:18) 4√

n

≤ κM

(cid:19)

+

√
2κ
n
λ

log(2/δ)

(cid:13)(cid:13)(cid:13)(cid:13)K

12

Proof. Consider the random variable ξ = (y − fλ(x))K(x,·). Then by (11)

(cid:107)ξ(cid:107)K = |y − fλ(x)|(cid:112)K(x, x) ≤ κ(M + κ(cid:107)fλ(cid:107)K)
K] = E(cid:2)(y − fλ(x))2K(x, x)(cid:3) ≤ κ2E(cid:2)(y − fλ(x))2(cid:3) ≤ κ2M 2.

E[(cid:107)ξ(cid:107)2

and by (10)

(cid:13)(cid:13)(cid:13)(cid:13) 1

(cid:13)(cid:13)(cid:13)(cid:13)K

Applying Lemma 9 we obtain

n

S∗(Y − Sfλ) − LK(f∗ − fλ)

≤ 2κ(M + κ(cid:107)fλ(cid:107)K) log(2/δ)
The desired bound follows by using (11) and noticing(cid:112)2 log(2/δ) ≤(cid:113) 2
Proof of Theorem 6. Note that E[ ˆf ] = E(cid:2)(λI + 1

Now we can prove Theorem 6.

n

+

(cid:114)

2κ2M 2 log(2/δ)

n

.

log(2) log(2/δ) ≤ 2 log(2/δ).

(cid:4)

n S∗S)−1( 1

n S∗Sf∗) − fλ

n S∗S)−1( 1
n S∗S)−1( 1

n S∗Sf∗)(cid:3) . By Lemma 10 and Lemma 11,
≤ E(cid:2)(cid:13)(cid:13)(λI + 1
(cid:13)(cid:13)K
(cid:3)
n S∗Sf∗ − LKf∗)(cid:13)(cid:13)K
≤ E(cid:2)(cid:13)(cid:13)(λI + 1
+ E(cid:2)(cid:13)(cid:13)(cid:8)(λI + 1
n S∗S)−1 − (λI + LK)−1(cid:9) LKf∗(cid:13)(cid:13)K
E(cid:2)(cid:13)(cid:13) 1
n S∗Sf∗ − LKf∗(cid:13)(cid:13)K
(cid:3)
+ E(cid:2)(cid:13)(cid:13)(λI + 1
n S∗S(cid:13)(cid:13)(cid:3)
E(cid:2)(cid:13)(cid:13)LK − 1

n S∗S)(λI + LK)−1LKf∗(cid:13)(cid:13)K

n S∗S)−1(LK − 1

≤ 1
λ

(cid:3)

(cid:3)

(cid:3)

−→ 0.

(12)

+

(cid:107)fλ(cid:107)K
√
≤ κM
n
≤ κM + κ2 (cid:107)fλ(cid:107)K

√

λ

λ

λ

n

we have can prove (i) as follows:

(cid:13)(cid:13)(cid:13)E[ ˆf ] − fλ

(cid:13)(cid:13)(cid:13)K

Since the convergence in HK implies convergence in L2, E[ ˆf ] also converges to (λI + LK)−1LKf∗ in

L2. Therefore the conclusion (ii) holds.

To prove (iii), note that it is veriﬁed in [27] that λfλ = LK(f∗ − fλ) and

By the fact that (cid:107)T(cid:107)2 = (cid:107)T ∗T(cid:107) for an operator T , we have

ˆf − fλ =(cid:0)λI + 1
n S ∗ S(cid:1)−1(cid:13)(cid:13)(cid:13)2
(cid:0)λI + 1

(cid:13)(cid:13)(cid:13)L1/2

K

.

(cid:111)
n S∗(Y − Sfλ) − LK(f∗ − fλ)
n S ∗ S(cid:1)−1(cid:13)(cid:13)(cid:13)
(cid:0)λI + 1
n S∗S)(cid:0)λI + 1
n S∗S(cid:1)(cid:0)λI + 1

n S ∗ S(cid:1)−1(cid:110) 1
(cid:13)(cid:13)(cid:13)(cid:0)λI + 1
n S ∗ S(cid:1)−1
≤ (cid:13)(cid:13)(cid:13)(cid:0)λI + 1
n S ∗ S(cid:1)−1
(cid:13)(cid:13)(cid:13)(cid:0)λI + 1
n S ∗ S(cid:1)−1(cid:0) 1
n S∗S(cid:13)(cid:13) +
(cid:13)(cid:13)LK − 1

LK
(LK − 1

+
≤ 1
λ

1
λ

=

.

n S ∗ S(cid:1)−1(cid:13)(cid:13)(cid:13)
n S ∗ S(cid:1)−1(cid:13)(cid:13)(cid:13)

13

(13)

By Lemma 10 and Lemma 13, we obtain

(cid:13)(cid:13)(cid:13) ˆf − fλ

(cid:13)(cid:13)(cid:13)2

L2

(cid:13)(cid:13)(cid:13)(cid:13)2
(cid:17)2

K

S∗(Y − Sfλ) − LK(f∗ − fλ)

(cid:19)2(cid:16)
(cid:17)5/2

+

√
κ

λ

n

log(2/δ)

log(2/δ)

.

=

(cid:13)(cid:13)(cid:13)L1/2
≤(cid:13)(cid:13)(cid:13)L1/2
(cid:32)

K

≤

K ( ˆf − f∗)
(cid:0)λI + 1
(cid:114)
(cid:18) 2κ2

n

2κ2
λ2

√

n

≤ κ2M 2

log(2/δ)

n

K

(cid:13)(cid:13)(cid:13)2
n S ∗ S(cid:1)−1(cid:13)(cid:13)(cid:13)2(cid:13)(cid:13)(cid:13)(cid:13) 1
(cid:33)
(cid:19)(cid:18) 32
(cid:18) 2κ2

1
λ

2
λ

+

+

+

n

√

2
λ

κ2M 2

n

(cid:18) 4√
(cid:19)(cid:16)
(cid:19)(cid:18) 32

2κ2
n2λ

(cid:18)

Pr [ξ > t] ≤ 4 exp

−

t

ς 2/5(n, λ)

(cid:19)

2κ2
n2λ

(cid:19)

.

λ2
with conﬁdence at least 1 − 2δ. Denote

The random variable ξ = (cid:107) ˆf−fλ(cid:107)4/5

ς(n, λ) = κ2M 2

+

L2 is positive and satisﬁes Pr(cid:2)ξ ≤ ς 2/5 log(2/δ)(cid:3) ≥ 1−2δ or equivalently

λ2

+

n

n

.

Applying Lemma 8 to ξ with q = 5/2 we obtain

(cid:104)(cid:107) ˆf − ED[ ˆf ](cid:107)2

L2

(cid:105) ≤ E

(cid:104)(cid:107) ˆf − fλ(cid:107)2

L2

(cid:105) ≤ 10

ED

(cid:18)

−

t3/2 exp

(cid:90) ∞

0

t

ς 2/5(n, λ)

(cid:19)

√

dt = 15
2

πς(n, λ).

√
If λn → ∞ and λ

n → 0, by (11), we can verify that ς(n, λ) = O(

1

λ2n3/2 ). This proves (iii).

Denote f (cid:93)

λ = (λI + LK)−2(2λI + LK)LKf∗. We can verify that
λ = fλ + λ(λ + LK)−1fλ.
f (cid:93)

This together with (11) implies

λ(cid:107)K ≤ 2(cid:107)fλ(cid:107)K ≤ 2M√
(cid:107)f (cid:93)

λ

.

We need the following two lemmas in the proof of Theorem 7.

(cid:4)

(14)

(15)

Lemma 14. For any 0 < δ < 1, we have

(cid:13)(cid:13)(cid:13)(cid:13) 1

n

S∗(Y − Sf (cid:93)

λ) − LK(f∗ − f (cid:93)
λ)

(cid:13)(cid:13)(cid:13)(cid:13)K

≤ κM

(cid:18) 4κ

√

n

λ

(cid:19)

+

5√
n

log(2/δ)

with conﬁdence at least 1 − δ.
Proof. Consider the random variable ξ = (y − f (cid:93)

λ(x))K(x,·). By (15), we have

λ(x)|(cid:112)K(x, x) ≤ κ(M + 2κ(cid:107)fλ(cid:107)K).

(cid:107)ξ(cid:107)K = |y − f (cid:93)

14

By (14) and (10), we have

E(cid:2)(cid:107)ξ(cid:107)2

K

(cid:19)

(cid:13)(cid:13)(cid:13)2

K

≤ κ2

(y − f (cid:93)

λ(x))2K(x, x)

λ(x))2(cid:105)

(cid:104)
(cid:105) ≤ κ2E
(cid:104)
(cid:3) = E
(cid:18)
2E(cid:2)(y − fλ(x))2(cid:3) + 2λ2(cid:13)(cid:13)(cid:13)L1/2
(y − f (cid:93)
(cid:17)
≤ κ2(cid:16)
K (λI + LK)−1fλ
2E(cid:2)(y − fλ(x))2(cid:3) + 2λ(cid:107)fλ(cid:107)2
(cid:13)(cid:13)(cid:13)(cid:13)K

≤ 2κ(M + 2κ(cid:107)fλ(cid:107)K) log(2/δ)

≤ 2κ2M 2.

(cid:114)

+

n

K

Applying Lemma 9, we obtain

S∗(Y − Sf (cid:93)

λ) − LK(f∗ − f (cid:93)
λ)

(cid:13)(cid:13)(cid:13)(cid:13) 1

n

4κ2M 2 log(2/δ)

n

,

(cid:4)

which in combination with (11) implies the desired conclusion.
Lemma 15. Let g = λ(λI + LK)−2LKf∗. For any 0 < δ < 1, we have

(cid:13)(cid:13)(cid:13)(cid:13) 1

(cid:13)(cid:13)(cid:13)(cid:13)K

(cid:18) 2κ

√

(cid:19)

S∗Sg − LKg

≤ κM

+

2√
n

log(2/δ)

n
with conﬁdence at least 1 − δ.
Proof. Consider the random variable ξ = g(x)K(x,·). Note that g = λ(λI +LK)−1fλ. So (cid:107)g(cid:107)K ≤ (cid:107)fλ(cid:107)K.
Therefore,

n

λ

(cid:107)ξ(cid:107)K = |g(x)|(cid:112)K(x, x) ≤ κ2(cid:107)fλ(cid:107)K.

We also have

Applying Lemma 9, we obtain

(cid:3) = E(cid:2)(g(x))2K(x, x)(cid:3) ≤ κ2(cid:107)g(cid:107)2

L2 ≤ κ2(cid:107)f∗(cid:107)2

L2 ≤ κ2M 2.

(cid:114)

≤ 2κ2(cid:107)fλ(cid:107)K log(2/δ)

n

+

2κ2M 2 log(2/δ)

n

,

(cid:4)

which in combination with (11) implies the desired conclusion.

Proof of Theorem 7. Note that

n S∗Sf∗)(cid:3)

n S∗S)−2(2λI + 1

n S∗S(cid:1)−1(cid:0) 1

n S∗Sf )( 1

n S∗Sf∗(cid:1) + λ(λI + 1

n S∗S)−2( 1

n S∗Sf∗)

K

S∗Sg − LKg

E(cid:2)(cid:107)ξ(cid:107)2
(cid:13)(cid:13)(cid:13)(cid:13)K
(cid:13)(cid:13)(cid:13)(cid:13) 1
E[ ˆf (cid:93)] = E(cid:2)(λI + 1
=(cid:0)λI + 1

n

and

λ = fλ + λ(λI + LK)−2LKf∗.
f (cid:93)

15

So we can write

E[ ˆf (cid:93)] − f (cid:93)

λ =

n S∗S)−1( 1

(λI + 1

(cid:110)
+ λ(cid:0)λI + 1
(cid:110)

n S∗Sf∗) − fλ

(cid:111)
n S∗S(cid:1)−2(cid:0) 1
n S∗Sf∗ − LKf∗(cid:1)
n S∗S)−2 − (λI + LK)−2(cid:111)

+ λ

(λI + 1

LKf∗

= Q1 + Q2 + Q3.

By (12) we have E[(cid:107)Q1(cid:107)K] −→ 0. For Q2, by Lemma 11, we have

For Q3, we can verify that

n S∗S)−2(cid:104)
(cid:13)(cid:13)LK − 1

λ

Q3 = λ(λI + 1
So (cid:107)Q3(cid:107)K ≤ 4(cid:107)fλ(cid:107)

E[(cid:107)Q2(cid:107)K] ≤ 1
λ

n S∗Sf∗ − LKf∗(cid:13)(cid:13)K
E(cid:2)(cid:13)(cid:13) 1
n S∗S(cid:1) LK + 1
n S∗S(cid:1) +(cid:0)LK − 1

2λ(cid:0)LK − 1
n S∗S(cid:13)(cid:13) . By Lemma 10, we obtain
E [(cid:107)Q3(cid:107)K] ≤ 4κ2(cid:107)fλ(cid:107)K

√

−→ 0.

λ

n

√

−→ 0.

(cid:3) ≤ κM
n S∗S(cid:0)LK − 1

n

λ

n S∗S(cid:1)(cid:105)

(λI + LK)−1fλ.

It is easy to check that
(cid:107)L1/2

K

˜Q1(cid:107)K ≤ 2

Combining the estimation for Q1, Q2 and Q3, we obtain

(cid:13)(cid:13)(cid:13)E[ ˆf (cid:93)] − (λI + LK)−2(2λI + LK)LKf∗(cid:13)(cid:13)(cid:13)K

≤ E[(cid:107)Q(cid:107)K] −→ 0.

This proves (i).

The convergence in HK implies convergence in L2 and the latter implies the conclusion (ii).
To prove (iii), we ﬁrst observe that

λ2f (cid:93)

λ = 2λLK(f∗ − fλ) + L2

K(f∗ − f (cid:93)
λ).

So we can write

ˆf (cid:93) − f (cid:93)

λ = (λI + 1

(cid:105)

f (cid:93)
λ

n S∗Y −(cid:0)λI + 1
n S∗S(cid:1)2
(cid:105)
(cid:17) − L2
λ) − LK(f∗ − f (cid:93)
λ)
(cid:105)
K(f∗ − f (cid:93)
λ)
(cid:105)

λ) − LK(f∗ − f (cid:93)
λ)

λ) − LK(f∗ − f (cid:93)
λ)

(cid:105)

n S∗(Y − Sf (cid:93)
λ)

n S∗(Y − Sf (cid:93)

(cid:1) LK(f∗ − f (cid:93)

λ)

= 2λ(λI + 1

+ (λI + 1

= 2λ(λI + 1

+ (λI + 1

+ (λI + 1

n S∗(Y − Sf (cid:93)

n S∗S)−2(cid:104)(cid:0)2λI + 1
n S∗S(cid:1) 1
n S∗S)−2(cid:104) 1
n S∗S(cid:1)(cid:16) 1
n S∗S)−2(cid:104)(cid:0) 1
n S∗S)−2(cid:104) 1
n S∗S(cid:1)(cid:104) 1
n S∗S)−2(cid:0) 1
n S∗S)−2(cid:0) 1
(cid:13)(cid:13)(cid:13)L1/2

n S∗S)−1(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 1

n S∗(Y − Sf (cid:93)

n S∗S − LK

K (λI + 1

= ˜Q1 + ˜Q2 + ˜Q3.

n S∗(Y − Sf (cid:93)

λ) − LK(f∗ − f (cid:93)
λ)

(cid:13)(cid:13)(cid:13)K

16

(cid:13)(cid:13)(cid:13)K

(cid:13)(cid:13)(cid:13)K
(cid:13)(cid:13)(cid:13) 1

(cid:19)2

(cid:13)(cid:13)(cid:13)K

+

n S∗Sg − LKg

K (λI + 1
λ) = λ2(λI + LK)−2LKf∗. Denote g = λ(λI + LK)−2LKf∗. We have

n S∗(Y − Sf (cid:93)

λ) − LK(f∗ − f (cid:93)
λ)

.

n S∗S)−1(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 1

n S∗Sg − LKg

.

By (13), Lemma 10, Lemma 14 and Lemma 15, we obtain

and

(cid:107)L1/2

K

˜Q2(cid:107)K ≤(cid:13)(cid:13)(cid:13)L1/2

n S∗S)−1(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 1

For ˜Q3, we observe that LK(f∗ − f (cid:93)

K

=

(cid:107)L1/2

(cid:13)(cid:13)(cid:13) 1

K (λI + 1

˜Q3(cid:107)K ≤(cid:13)(cid:13)(cid:13)L1/2
(cid:13)(cid:13)(cid:13)K
(cid:13)(cid:13)(cid:13)L1/2
K ( ˆf (cid:93) − f (cid:93)
n S∗S)−1(cid:13)(cid:13)(cid:13)2(cid:18)
λ)
n S∗(Y − Sf (cid:93)
(cid:33)
(cid:18) 10κ
(cid:19)2(cid:16)
(cid:19)(cid:18) 10κ
(cid:104)(cid:107) ˆf (cid:93) − f (cid:93)
(cid:105) ≤ E
(cid:105)

κ2M 2

√

1
λ

+

+

n

n

λ

2

λ

L2

(cid:13)(cid:13)(cid:13)2

K (λI + 1

(cid:13)(cid:13)(cid:13) ˆf (cid:93) − f (cid:93)
≤(cid:13)(cid:13)(cid:13)L1/2
(cid:32)
(cid:114)
(cid:18) 2κ2
(cid:104)(cid:107) ˆf (cid:93) − E[ ˆf (cid:93)](cid:107)2

2κ2
λ2

√

λ2

E

n

n

+

+

2
λ

√

≤ κ2M 2

12√
n
with probability at least 1 − 3δ. This implies
λ(cid:107)2
√
λ2n3/2 ) when λn → ∞ and λ

It is of order O(

L2

L2

λ

1

log(2/δ)

≤

(cid:13)(cid:13)(cid:13)K
λ) − LK(f∗ − f (cid:93)
(cid:17)2
λ)

log(2/δ)

(cid:19)2(cid:16)
(cid:17)5/2

12√
n

log(2/δ)

(cid:18) 2κ2

√

πκ2M 2

√
= 45
4
n
n → 0. This proves (iii).

λ2

(cid:19)(cid:18) 10κ

√

n

λ

+

2
λ

(cid:19)2

.

+

12√
n

(cid:4)

6 Learning with block wise data
In learning with block wise streaming data, let Dt = {(xit, yit)}n
i=1 be the data block collected at time
t. Consider the simple incremental algorithm which learns a function ˆft from Dt by a base algorithm.
Then the target function for prediction uses the average of all the learnt functions available upon time t,

that is,

¯ft(x) =

1
t

t(cid:88)

i=1

ˆfi(x).

We see the target function is updated incrementally:
t − 1
t

¯ft =

¯ft−1 +

1
t

ˆft.

Let b and v denotes the bias and variance of ˆfi. The mean squared error of ¯ft is

mse( ¯ft) = b2 +

v
t

.

When t becomes large, we see the variance term shrinks but the bias term does not. So the performance

of the simple incremental learning method is dominated by the bias. Hence, a base algorithm with small
bias is preferred. This intuition supports the use of bias corrected algorithms.

In divide and conquer algorithm, data blocks do not arrive in time.

Instead, they are artiﬁcially

generated. But from a computational perspective, its idea is the same as the simple incremental learning.

So bias correction is also preferred.

17

7 Simulation study

In this section, we illustrate the use of bias corrected algorithms by a variety of simulations. We will ﬁrst

study their performance on a single data set. Then we verify their eﬀectiveness in learning with block

wise data using both simulated data and real world data.

7.1 Learning with a single data set
Let x ∈ R20 and all the 20 explanatory variables are independent and normally distributed. Let the ith
variable xi has variance 2−i. So the i-th variable is also the ith principal component. We consider two
linear models where

Model 1: w1 = [1, 1,−1,−1, 0, . . . , 0],
Model 2: w2 = [0, . . . , 0, 1, 1,−1,−1],

b1 = 0;
b2 = 0.

Note that the ﬁrst model depends on the ﬁrst four principal components and the second one depends on

the last four principal components. For both models, the noise level is set such that the signal to noise

ratio is 10. We use the sample size n = 100.

We ﬁrst compare the bias, variance, and mean squared error of ridge regression (RR) and bias corrected

ridge regression (BCRR). They are calculated by averaging the bias, variance, and mean squared error

after repeating the experiment 1000 times. The results are shown in Figure 1 and Figure 2. They show

that, within a reasonable range of λ, the BCRR has smaller bias and larger variance. Their minimum

mean squared errors are comparable. BCRR can achieve the minimum mean squared error with a larger

regularization parameter. Since the linear system could be more stable with larger λ, BCRR may be

superior in very high dimensional situation where the covariance matrix is near singular.

Figure 1: Model 1: Bias, variance, and mean squared error of RR and BCRR.

7.2 Incremental learning with block wise streaming data

Now we compare the performance of RR and BARR when they serve as base algorithms in incremental

learning with block wise streaming data. Again we use the two linear models above. The experiment

setting is also the same as before except that 20 data sets are generated in time to simulate the block

streaming data. When each data block comes, 10-fold cross validation is used to select the regularization

18

−10−9−8−7−6−5−4−3−2−1000.20.40.60.811.21.41.61.8log2(λ)bias  RRBCRR−10−9−8−7−6−5−4−3−2−1000.511.522.533.5log2(λ)variance  RRBCRR−10−9−8−7−6−5−4−3−2−1000.511.522.533.5log2(λ)mean squared error  RRBCRRFigure 2: Model 2: Bias, variance, and mean squared error of RR and BCRR.

parameter λ for ridge regression. Then both RR and BCRR use the same λ to estimate the base model

for the current data block. The average of all available base models is then used for prediction. As

the prediction accuracy is the main concern, we use the mean squared prediction error to measure the

learning performance. After repeating the experiment 1000 times, the mean squared prediction error is

plotted in Figure 3. For Model 1, when t is small, the performance of RR and BCRR are similar. As

time goes on and more data blocks come in, the variance drops and the bias becomes the dominating

term impacting the learning performance. BCRR signiﬁcantly outperforms RR. For Model 2, the model

depends on the last four principal components. With the optimal choice of λ,

λ

λ+σi

are close to 1 for

i = 17, 18, 19, 20. We see bias reduction still helps but the improvement is not signiﬁcant.

(a) Model 1

(b) Model 2

Figure 3: Performance of incremental learning with RR and BARR as the base algorithms.

7.3 Real data

To further validate the eﬀectiveness of BCRR, two real-world data sets, the spam data and magic data,

from UCI machine learning repository are employed for empirical study in this research.

19

−30−29−28−27−26−25−24−23−22−21−2000.10.20.30.40.50.60.70.8log2(λ)bias  RRBCRR−30−29−28−27−26−25−24−23−22−21−200.0240.0260.0280.030.0320.0340.0360.0380.04log2(λ)variance  RRBCRR−30−29−28−27−26−25−24−23−22−21−2000.10.20.30.40.50.60.7log2(λ)mean squared error  RRBCRR024681012141618200.020.030.040.050.060.070.080.090.1tmean squared prediction error  RRBCRR0246810121416182012345678x 10−4tmean squared prediction error  RRBCRRWe following the procedure in [16]. Each data set is initially randomly sliced into 20 chucks with

identical size. At each run, one chuck is randomly selected to be the testing data, and the remaining 19

chucks are assumed to come in sequentially. Simulation results for each data sets are averaged across

20 runs. As both problems are classiﬁcation problems, both the mean squared prediction error and

classiﬁcation accuracy are used to measure the learning performance.

For spam data, the results are shown in Figure 4. For magic data, the results are shown in Figure 5.

For both data sets, BCRR is more eﬀective than ridge regression. Note small mean squared prediction

error usually implies small classiﬁcation error. However, such a relationship is not exact. That is why

the classiﬁcation errors ﬂuctuate for magic data, although the mean squared error decreases stably.

Figure 4: Spam data: mean squared prediction error and mean classiﬁcation error on test set.

Figure 5: Magic data: the mean squared error and mean classiﬁcation error on test set.

20

024681012141618200.50.60.70.80.911.11.21.31.41.5tmean squared prediction error  RRBCRR024681012141618200.1050.110.1150.120.1250.130.1350.14tmean classificaiton error  RRBCRR024681012141618200.6060.6070.6080.6090.610.6110.6120.6130.6140.615tmean squared prediction error  RRBCRR024681012141618200.21340.21360.21380.2140.21420.21440.21460.21480.2150.2152tmean classificaiton error  RRBCRR7.4 Kernel models

Kernel methods are eﬀective to handle the data that contain strong nonlinear structure. We applied the

regularization kernel network (RKN) and the bias corrected regularization kernel network (BCRKN) to
the handwritten digits recognition and compare their performance.

The MNIST handwritten digits data [21] is believed to have strong nonlinear structures and has been

a benchmark to test the performance of nonlinear algorithms. It contains 60,000 images of handwritten
digits 0, 1, 2, . . . , 9 as training data and 10,000 images as test data. Each image consists of p = 28× 28 =
784 gray-scale pixel intensities. The digits 2, 3, 5, 8 are considered to be most diﬃcult to recognize and

nonlinear models are able to help.

In our analysis, we consider the classiﬁcation of digit 3 versus digit 8. Note our purpose is to compare

the performance of RKN and BCRKN and verify the eﬀectiveness of bias correction, not to ﬁnd the

best classiﬁer. So we select the Gaussian kernel, set bandwidth parameter as the median of the pairwise

distance between the images and do not optimize it in the learning process. The training data is sliced

into 50 chucks with identical size to mimic the data stream. The mean squared error and classiﬁcation

error on the test data is reported in Figure 6. It is clear that bias correction helps to improve learning

performance.

Figure 6: Classiﬁcation of digit 3 versus digit 8 in the MNIST data.

8 Conclusions and discussions

We proposed two new regularized regression algorithms that are derived by correcting the bias of ridge

regression and regularization kernel network. The bias corrected algorithms are shown to have smaller

bias, while, in general, they have slightly larger variance. When applied to a single data set, the bias

corrected algorithms have comparable performance to the uncorrected algorithms. But the smaller bias

favors their use in learning with block wise data, such as incremental learning with block streaming data

or the divide and conquer algorithm.

21

051015202530354045500.150.160.170.180.190.2tmean squared prediction error  RKNBCRKN051015202530354045500.0260.0280.030.0320.0340.0360.0380.040.042tmean classificaiton error  RKNBCRKNBias correction is found less eﬀective when the true model depends only on the principal components

with very small eigenvalues. Fortunately, this is not common in real applications. When the true model

does depend heavily on the principal components with small eigenvalues, the bias correction performs

similar to the uncorrected algorithm, not worse. Furthermore, the bias corrected algorithms may be

more computationally stable because it allows using larger regularization parameter to achieve the same

learning performance. Therefore, the use of bias corrected algorithms in practice is safe and preferable.

It is natural to consider the possibility and necessity of higher order bias correction. We remark that

deﬁning higher order bias corrected estimators is possible. But it is unnecessary because higher order

bias correction is ineﬀective. To illustrate this, consider the ridge regression. We know from Section 2
that the asymptotic bias of ˆw(cid:93) is −λ2(λI + Σ)−2w. We can subtract an estimate of this asymptotic bias
to obtain a second order bias corrected ridge regression estimator
2 = ˆw(cid:93) + λ2(λI + ˆΣ)−2 ˆw.
ˆw(cid:93)

This estimator will have an asymptotic bias −λ3(λI + Σ)−3w, which can be used to deﬁne the third
order bias corrected estimator. Actually, for any k > 2, we can deﬁne bias corrected estimators of order

k iteratively,

ˆw(cid:93)

k = ˆw(cid:93)

k−1 + λk(λI + ˆΣ)−k ˆw,

which has an asymptotic bias −λk+1(λI+Σ)−(k+1)w. We can also prove that the asymptotic bias decreases
as k increases. However, simulations show higher order bias correction is ineﬀective. As an illustration,

we applied the bias corrected estimators of order 1, 2, and 3 to the streaming data generated for Model 1
in Section 7.2. We see from Figure 7 that the performance of higher order bias correction is even worse.

Figure 7: Model 1: Performance of BCRR of order 1, 2, and 3.

22

024681012141618200.020.030.040.050.060.070.080.090.10.11tmean squared prediction error  BCRRk=1BCRRk=2BCRRk=3References

[1] I. S. Abramson. On bandwidth variation in kernel estimates - a square root law. The Annals of

Statistics, 10(4):1217–1223, 1982.

[2] S. Agrawal, Z. Wang, and Y. Ye. A dynamic near-optimal algorithm for online linear programming.

Operations Research, 62(4):876–890, 2014.

[3] N. Aronszajn. Theory of reproducing kernels. Trans. Amer. Math. Soc., 68:337–404, 1950.

[4] F. Bauer, S. Pereverzev, and L. Rosasco. On regularization algorithms in learning theory. Journal

of complexity, 23(1):52–72, 2007.

[5] O. Bousquet and A. Elisseeﬀ. Stability and generalization. Journal of Machine Learning Research,

2:499–526, 2002.

[6] L. Breiman, W. Meisel, and E. Purcell. Variable kernel estimates of multivariate densities. Techno-

metrics, 19(2):135–144, 1977.

[7] P. B¨uhlmann. Statistical signiﬁcance in high-dimensional linear models. Bernoulli, 19(4):1212–1242,

2013.

[8] A. Caponnetto and E. De Vito. Optimal rates for the regularized least-squares algorithm. Founda-

tions of Computational Mathematics, 7(3):331–368, 2007.

[9] M. Chavent, S. Girard, V. Kuentz-Simonet, B. Liquet, T. M. N. Nguyen, and J. Saracco. A sliced

inverse regression approach for data stream. Computational Statistics, 29(5):1129–1152, 2014.

[10] Y. Chung and B. G. Lindsay. A likelihood-tuned density estimator via a nonparametric mixture

model.

In Nonparametric Statistics and Mixture Models: A Festschrift in Honor of Thomas P.

Hettmansperger, pages 69–89. World Scientiﬁc Publishing, 2011.

[11] F. Cribari-Neto and K. L. Vasconcellos. Nearly unbiased maximum likelihood estimation for the

beta distribution. Journal of Statistical Computation and Simulation, 72(2):107–118, 2002.

[12] E. De Vito, A. Caponnetto, and L. Rosasco. Model selection for regularized least-squares algorithm

in learning theory. Foundations of Computational Mathematics, 5(1):59–85, 2005.

[13] T. Evgeniou, M. Pontil, and T. Poggio. Regularization networks and support vector machines. Adv.

Comput. Math., 13:1–50, 2000.

[14] D. Firth. Bias reduction of maximum likelihood estimates. Biometrika, 80(1):27–38, 1993.

[15] P. Hall. On the bias of variable bandwidth curve estimators. Biometrika, 77(3):529–535, 1990.

[16] H. He, S. Chen, K. Li, and X. Xu. Incremental learning from stream data. Neural Networks, IEEE

Transactions on, 22(12):1901–1914, 2011.

23

[17] A. E. Hoerl and R. W. Kennard. Ridge regression: applications to nonorthogonal problems. Tech-

nometrics, 12(1):69–82, 1970.

[18] A. E. Hoerl and R. W. Kennard. Ridge regression: Biased estimation for nonorthogonal problems.

Technometrics, 12(1):55–67, 1970.

[19] A. Javanmard and A. Montanari. Conﬁdence intervals and hypothesis testing for high-dimensional

regression. Journal of Machine Learning Research, 15(1):2869–2909, 2014.

[20] M. Jones, O. Linton, and J. Nielsen. A simple bias reduction method for density estimation.

Biometrika, 82(2):327–338, 1995.

[21] Y. LeCun. The MINIST database of handwritten digits. http://yann.lecun.com/exdb/mnist/,

accessed in October 2015.

[22] G. McLachlan. A note on bias correction in maximum likelihood estimation with logistic discrimi-

nation. Technometrics, 22(4):621–627, 1980.

[23] R. L. Obenchain. Classical F-tests and conﬁdence regions for ridge regression. Technometrics,

19(4):429–439, 1977.

[24] B. Park, W. Kim, D. Ruppert, M. Jones, D. Signorini, and R. Kohn. Simple transformation tech-

niques for improved non-parametric regression. Scandinavian journal of statistics, pages 145–163,
1997.

[25] M. H. Quenouille. Notes on bias in estimation. Biometrika, 43(3/4):353–360, 1956.

[26] R. L. Schaefer. Bias correction in maximum likelihood logistic regression. Statistics in Medicine,

2(1):71–78, 1983.

[27] S. Smale and D. X. Zhou. Learning theory estimates via integral operators and their approximations.

Constructive Approximation, 26:153–172, 2007.

[28] I. Steinwart, D. R. Hush, and C. Scovel. Optimal rates for regularized least squares regression. In

COLT, 2009.

[29] H. Sun and Q. Wu. Application of integral operator for regularized least-square regression. Mathe-

matical and Computer Modelling, 49(1):276–285, 2009.

[30] H. Sun and Q. Wu. A note on application of integral operator in learning theory. Applied and

Computational Harmonic Analysis, 26(3):416–421, 2009.

[31] R. Tibshirani. Regression shrinkage and selection via the lasso. J. Roy. Statist. Soc. Ser. B,

58(1):267–288, 1996.

[32] G. Wahba. Spline models for observational data. SIAM, 1990.

24

[33] Z. Wang, S. Deng, and Y. Ye. Close the gaps: A learning-while-doing algorithm for single-product

revenue management problems. Operations Research, 62(2):318–331, 2014.

[34] Q. Wu, Y. Ying, and D.-X. Zhou. Learning rates of least-square regularized regression. Foundations

of Computational Mathematics, 6(2):171–192, 2006.

[35] W. Yao. A bias corrected nonparametric regression estimator. Statistics & Probability Letters,

82(2):274–282, 2012.

[36] C.-H. Zhang and S. S. Zhang. Conﬁdence intervals for low dimensional parameters in high dimen-

sional linear models. Journal of the Royal Statistical Society: Series B (Statistical Methodology),

76(1):217–242, 2014.

[37] T. Zhang. Leave-one-out bounds for kernel methods. Neural Computation, 15(6):1397–1437, 2003.

[38] Y. Zhang, J. Duchi, and M. Wainwright. Divide and conquer kernel ridge regression. In Conference

on Learning Theory, pages 592–617, 2013.

[39] H. Zou and T. Hastie. Regularization and variable selection via the elastic net. J. R. Stat. Soc. Ser.

B Stat. Methodol., 67(2):301–320, 2005.

25

