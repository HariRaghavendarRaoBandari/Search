6
1
0
2

 
r
a

 

M
5
1

 
 
]

.

A
N
h
t
a
m

[
 
 

1
v
0
7
5
4
0

.

3
0
6
1
:
v
i
X
r
a

A PRECONDITIONER FOR THE OHTA–KAWASAKI EQUATION

PATRICK E. FARRELL∗ AND JOHN W. PEARSON†

Abstract. We propose a new preconditioner for the Ohta–Kawasaki equation, a nonlocal Cahn–
Hilliard equation that describes the evolution of diblock copolymer melts. We devise a computable
approximation to the inverse of the Schur complement of the coupled second-order formulation via
a matching strategy. The preconditioner achieves mesh independence: as the mesh is reﬁned, the
number of Krylov iterations required for its solution remains approximately constant. In addition,
the preconditioner is robust with respect to the interfacial thickness parameter if a timestep criterion
is satisﬁed. This enables the highly resolved ﬁnite element simulation of three-dimensional diblock
copolymer melts with over one billion degrees of freedom.

Key words. Ohta–Kawasaki equation, preconditioner, Schur complement, nonlocal Cahn–

Hilliard equation

AMS subject classiﬁcations. 65F08, 65M60, 35Q99, 82D60

1. Introduction. The Ohta–Kawasaki equations [11] model the evolution of di-
block copolymer melts. A diblock copolymer is a polymer consisting of two subchains
of diﬀerent monomers that repel each other but are joined by a covalent bond. A large
collection of these molecules is termed a melt. These melts are of scientiﬁc and engi-
neering interest because they undergo phase separation of their diﬀerent constituent
monomers, allowing for the design of nanostructures with particular desirable prop-
erties. The numerical simulation of these equations is an essential tool in exploring
the associated phase diagram [3].

The Ohta–Kawasaki functional describes the free energy of a diblock copolymer

+ σ(cid:12)(cid:12)(cid:12)(−∆N )−1/2(u − m)(cid:12)(cid:12)(cid:12)

2(cid:19) dx,

(1.1)

melt:

E[u] =

1

2ZΩ(cid:18)ε2 |∇u|2 +

1

2(cid:0)1 − u2(cid:1)2

where u = ±1 denotes the two pure phases, Ω is the domain under study ((0, 1)2 or
(0, 1)3), ε ≪ 1 is the interfacial thickness between regions of the pure phases, σ is
the nonlocal energy coeﬃcient, m is the (conserved) average value of u in Ω, and ∆N
denotes the Laplacian with homogeneous Neumann boundary conditions. Assuming
that the dynamics are governed by a H −1(Ω) gradient ﬂow,

(1.2)
the resulting Ohta–Kawasaki dynamic equation on Ω × (0, T ] in second-order form is
given by [12]

(ut, φ)H−1(Ω) + E ′[u; φ] = 0,

ut − ∆w + σ(u − m) = 0,
w + ε2∆u − u(u2 − 1) = 0,

(1.3a)

(1.3b)

∗Mathematical Institute, University of Oxford, Oxford, UK. Center for Biomedical Computing,

Simula Research Laboratory, Oslo, Norway (patrick.farrell@maths.ox.ac.uk).

†School of Mathematics, Statistics and Actuarial Science, University of Kent, Canterbury CT2
7NF, UK (j.w.pearson@kent.ac.uk). This research is funded by EPSRC grants EP/K030930/1,
EP/M018857/1, and a Center of Excellence grant from the Research Council of Norway to the
Center for Biomedical Computing at Simula Research Laboratory. This work used the ARCHER
UK National Supercomputing Service (http://www.archer.ac.uk), via a RAP award to E. S¨uli and
Q. Parsons. We thank NOTUR for the allocation of computing resources on Hexagon. The authors
would like to acknowledge the assistance of C. N. Richardson, J. Ring, M. F. Adams and G. N. Wells
in conducting the numerical experiments.

1

2

P. E. FARRELL AND J. W. PEARSON

with homogeneous Neumann boundary conditions

∇u · n = 0 and ∇w · n = 0,

(1.4)

and with initial condition u(x, 0) = u0(x).

2. Approximating the Schur complement. After applying a ﬁnite element
discretization in space and the θ-method in time, a discrete nonlinear problem must
be solved at each timestep. Each Newton iteration involves solving a linear system of
the form

J(cid:20)δu
δw(cid:21) =(cid:20)(1 + ∆tθσ)M ∆tθK

M (cid:21)(cid:20)δu

−ε2K − ME

δw(cid:21) =(cid:20)f1
f2(cid:21) .

(2.1)

where J is the Jacobian, M is the standard mass matrix with entries of the form

RΩ φiφj dx, K is the standard discretization of the Neumann Laplacian with entries
RΩ ∇φi · ∇φj dx, ME is a mass matrix involving a spatially varying coeﬃcient with
entries RΩ(3u2 − 1)φiφj dx, δu is the update for u, δw is the update for w, f1 and

f2 gather the source term and contributions from previous time levels, and ∆t is
the timestep. As the discretization is reﬁned and the dimension of (2.1) increases,
it becomes impractical to employ direct solvers and preconditioned Krylov methods
must be used instead. As the matrix in (2.1) is nonsymmetric, a suitable iterative
solver such as GMRES [14] is required to compute its solution.

We note however that there are structures within the system that can be ex-
ploited within a solver. For example, M is symmetric positive deﬁnite, K is symmet-
ric positive semideﬁnite (with one zero eigenvalue corresponding to the nullspace of
constants), and ME is symmetric.

Preconditioners for block-structured matrices typically involve approximating the

Schur complement of the system. Let the Jacobian J be partitioned as

J =(cid:20)A B
C D(cid:21) .

Consider the preconditioner

P −1 =(cid:20)A 0

C S(cid:21)−1

=(cid:20)A−1

0

0

S−1(cid:21)(cid:20)

I

−CA−1

0

I(cid:21) ,

(2.2)

(2.3)

where S = D − CA−1B is the Schur complement with respect to A. If exact inner
solves are used for A−1 and S−1, then the preconditioned operator P −1J has minimal
polynomial degree two and GMRES converges exactly in two iterations [8, 10]. Here,
A is a scaled mass matrix and is thus straightforward to solve with standard techniques
such as Chebyshev semi-iteration. Thus, the eﬃcient preconditioning of S is the key
to the fast solution of the Newton step (2.1), and thus of the dynamic Ohta–Kawasaki
equations (1.3a)–(1.3b).

The Schur complement of (2.1) is

S = M + ε2cKM −1K + cMEM −1K,

(2.4)

with constant c = (∆tθ)/(1 + ∆tθσ) > 0. In general it is very diﬃcult to precondition
the sum of diﬀerent matrices. The approach adopted here is the matching strategy
of Pearson and Wathen [13, 2]: the sum is approximated by the product of matrices,

A PRECONDITIONER FOR THE OHTA-KAWASAKI EQUATIONS

3

carefully chosen to match as many terms of the sum as possible. We propose the
approximation

with

S ≈ ˜S = ˆSM −1 ˆS,

ˆS = M + ε√cK.

(2.5)

(2.6)

This approximation ˜S is the product of three invertible matrices, and so its inverse
action can be eﬃciently computed by

˜S−1 = ˆS−1M ˆS−1.

(2.7)

The action of ˆS−1 can be eﬃciently approximated with algebraic multigrid techniques
[7] to yield a computationally cheap preconditioner.

Expanding the approximation (2.5), we ﬁnd that it matches the ﬁrst two terms

of the Schur complement exactly:

˜S = M + ε2cKM −1K + 2ε√cK.

(2.8)

Recall that ME depends on the current estimate of the solution u. With this matching
strategy, the term involving ME in the Schur complement (2.4) has been neglected, so
that ˜S does not vary between Newton iterations and no reassembly or algebraic multi-
grid reconstruction is required. The Schur complement approximation ˜S is straight-
forward and feasible to apply, and its eﬀectiveness within a preconditioner will depend
to a large extent on the eﬀect of the neglected third term from S. In the next section
we present some analysis to explain why we expect our approximation to work well
as a preconditioner.

We note in passing that it is possible to rearrange (2.1) to the symmetric form

1+∆tθσ K

(cid:20) ∆tθ

M

M

−ε2K − ME(cid:21)(cid:20)δw

δu(cid:21) =(cid:20)

1

1+∆tθσ f1

f2

(cid:21) .

It is likely that a similar approach could be used to precondition this rearranged
system, providing one takes into account the singular (1, 1)-block. Furthermore, as
discussed in the next section, it is more straightforward to prove the rates of conver-
gence of iterative methods for symmetric systems. However, whereas the (1, 1)-block
A of (2.1) may be well approximated using Chebyshev semi-iteration, the stiﬀness
matrix K arising in the (1, 1)-block of the rearranged system would require a more
expensive method such as a multigrid process. We therefore prefer to solve the non-
symmetric system (2.1) due to the ease with which we may compute the approximate
action of A−1.

3. Analysis. We now wish to justify why our preconditioner is likely to be ef-
fective for the problem being solved. It is well-known that for nonsymmetric matrix
systems it is extremely diﬃcult to provide a concrete proof for the rate of conver-
gence of an iterative method, as opposed to symmetric systems for which convergence
is controlled only by the eigenvalues of the preconditioned system P −1J. For nonsym-
metric operators the spectrum alone does not determine the convergence of GMRES
[6]; furthermore, the usual techniques for establishing eigenvalue bounds do not apply
to the nonsymmetric case [2]. However, in practice, the tight clustering of eigenvalues

4

P. E. FARRELL AND J. W. PEARSON

for the preconditioned system frequently leads to strong convergence properties, even
though this is not theoretically guaranteed. Given the challenges faced when solving
nonsymmetric matrix systems, we present an analysis that establishes spectral equiv-
alence of a slightly perturbed operator, and corroborate this analysis with numerical
experiments in section 4 which demonstrate mesh independence for the full problem.

Observe that when applying the preconditioner

(cid:20) ˜A 0
C ˜S(cid:21)−1

=(cid:20) ˜A−1

0

0

˜S−1(cid:21)(cid:20)

I

−C ˜A−1

0

I(cid:21)

(3.1)

for the Jacobian J, the crucial steps are applying ˜A−1 and ˜S−1. For the matrix
system (2.1), the inverse of the sub-block A = (1 + ∆tθσ)M may be approximated
accurately and cheaply using a Jacobi iteration or Chebyshev semi-iteration [4, 5, 15].
It is therefore instructive to consider the preconditioned system

C ˜S(cid:21)−1(cid:20)A B
(cid:20)A 0

C D(cid:21) =(cid:20)A−1

0

0

˜S−1(cid:21)(cid:20)

I

−CA−1

0

I(cid:21)(cid:20)A B

˜S−1S(cid:21) ,
C D(cid:21) =(cid:20)I A−1B

0

(3.2)

where the inexactness of our preconditioner arises from the stated approximation ˜S
of the Schur complement S. The eigenvalues of this system, which serve as a guide as
to the eﬀectiveness of the preconditioner, are either equal to 1, or correspond to the
eigenvalues of ˜S−1S.

We therefore examine the spectrum of the preconditioned Schur complement
˜S−1S, the matrix which governs the eﬀectiveness of our algorithm, in the ideal set-
ting where the matrix M + ε√cK is inverted exactly. We ﬁrst present a short result
concerning the reality of the eigenvalues.

Lemma 3.1. The eigenvalues of ˜S−1S are real.
Proof. If v ∈ null(K) and v 6= 0, then

implies

Sv = λ ˜Sv

M v = λM v,

and hence λ = 1 ∈ R.

On the other hand, if v /∈ null(K), then (3.3) implies

KM −1Sv = λKM −1 ˜Sv ⇒ F v = λGv ⇒ v∗F v = λv∗Gv,

where the matrices F and G are given by

F = K + ε2cKM −1KM −1K + cKM −1MF M −1K,
G = K + ε2cKM −1KM −1K + 2ε√cKM −1K.

(3.3)

(3.4)

(3.5)

(3.6)

(3.7)

Using the symmetry of F and G, it follows that v∗F v and v∗Gv are real and positive,
and hence λ ∈ R.
To motivate why ˜S should serve as an eﬀective approximation of S, we now present
a result on the eigenvalues of ˜S−1S in the perturbed setting that K is symmetric
positive deﬁnite. (The matrix K is in practice positive semideﬁnite, with a nullspace
of dimension one.)

A PRECONDITIONER FOR THE OHTA-KAWASAKI EQUATIONS

5

Lemma 3.2. Perturb K to be symmetric positive deﬁnite. Then the eigenvalues

of ˜S−1S satisfy:

λ( ˜S−1S) ∈(cid:20) 1

2

+

1
2ε

√cλ−, 1 +

√c λ+(cid:19) ,

1
2ε

(3.8)

where λ− = min{λmin(M −1ME), 0}, λ+ = max{λmax(M −1ME), 0} respectively, and
λmin, λmax are the minimum and maximum eigenvalues of a matrix.
Proof. First note that the eigenvalues of ˜S−1S and S ˜S−1 are the same by the
similarity of the two matrices. We therefore examine the matrix S ˜S−1, using the
properties that M is symmetric positive deﬁnite, and that ME is symmetric. Using
our assumption on K, K −1 exists, and it can be shown that

S ˜S−1 = (cid:0)M + ε2cKM −1K + cMEM −1K(cid:1)(cid:0)M + ε2cKM −1K + 2ε√cK(cid:1)−1

= I +(cid:0)−2ε√cK + cMEM −1K(cid:1)(cid:0)M + ε2cKM −1K + 2ε√cK(cid:1)−1
= I +(cid:0)−2ε√cM + cME(cid:1) M −1K(cid:0)M + ε2cKM −1K + 2ε√cK(cid:1)−1
= I +(cid:0)−2ε√cM + cME(cid:1)(cid:0)M K −1M + ε2cK + 2ε√cM(cid:1)−1

.

(3.9)

(3.10)

(3.11)

(3.12)

Therefore, each eigenvalue of S ˜S−1 is given by 1 + λR, where λR may be bounded
using a Rayleigh quotient argument, by symmetry of the matrices in (3.12). The
Rayleigh quotient is given by

R :=

vT (−2ε√cM + cME) v

vT (M K −1M + ε2cK + 2ε√cM ) v
= −2ε√c
|

vT (M K −1M + ε2cK + 2ε√cM ) v
}

vT M v

R1

vT MEv

+ c

vT (M K −1M + ε2cK + 2ε√cM ) v
}

R2

.

|

{z
{z

(3.13)

(3.14)

(3.15)

2 by simple algebraic manipulation. Therefore R1 ∈ [− 1

First observe that R1 < 0. To ﬁnd a lower bound on R1, we note that R1 =
−2aT b/(aT a + bT b + 2aT b), where a = K −1/2M v, b = ε√cK 1/2v, and therefore
R1 ≥ − 1
Now note that R2 could be positive or negative, depending on the sign of vT MEv.
If this quantity is positive for some v,
√c
2ε

cvT MEv
2ε√c vT M v

vT MEv
vT M v ≤

λmax(M −1ME),

R2 <

√c
2ε

2 , 0).

(3.16)

=

and otherwise R2 ≤ 0. Similarly, if R2 is negative for some v, then

R2 >

cvT MEv
2ε√cvT M v

=

vT MEv
vT M v ≥

√c
2ε
2ε √cλ−, 1

√c
2ε
2ε √cλ+].

λmin(M −1ME),

(3.17)

and R2 ≥ 0 otherwise. Hence R2 ∈ [ 1
as above.

Combining the bounds for R1 and R2 gives bounds for λR, and hence for λ( ˜S−1S)

6

P. E. FARRELL AND J. W. PEARSON

We highlight that the case where K is positive semideﬁnite rather than positive

deﬁnite is a more diﬃcult one theoretically, as the expression (3.12) for SeS−1 cannot

be derived (it requires the existence of K −1). However it is clear that the spectral
properties of K are almost identical for Dirichlet (positive deﬁnite) and Neumann
(positive semideﬁnite) problems, apart from the single zero eigenvalue in the semidef-
inite setting, and we generally ﬁnd the convergence rates of iterative methods are very
similar as a result. Indeed in practice we observe that the eigenvalues of ˜S−1S in our
numerical experiments reﬂect the predicted bounds very well.

We also note that we observe the eigenvalues of M −1ME to be mesh independent,
and that the bounds for λ( ˜S−1S) can therefore be driven tighter by decreasing ∆t.
Therefore, as the dimension of the matrix system is increased by taking a ﬁner dis-
cretization in space or time, we predict that our preconditioner should not worsen in
performance. Furthermore, if ∆t is chosen to scale like ε2, then the eigenvalue bounds
asymptote to a constant as the interfacial thickness parameter ε → 0. Hence, we also
expect the preconditioner to be robust to changes in this parameter. Note that the
scaling ∆t ∼ ε2 is typically necessary for stability in time discretization schemes [1],
and thus this criterion does not represent an additional restriction on the timestep
size.

In the next section we demonstrate how our proposed solver performs for a prac-

tical test problem, and examine whether the predicted robustness is achieved.

4. Numerical results. The solver was implemented in 338 lines of Python using
version 1.6 of the FEniCS ﬁnite element library [9]1. The experiment conducted used
Ω = (0, 1)3, m = 0.4, ε = 0.02, σ = 100, and an initial condition of

u0(x, y, z) = m + p(x, y, z),

(4.1)

where the perturbation p must be chosen to have integral zero and satisfy ∇p · n = 0
on the boundary. In this experiment we chose

p(x, y, z) =

cos (2πx) cos (2πy) cos (2πz)

50

.

(4.2)

The initial condition for w was computed from u0 via (1.3b). In this parameter regime
the solution is expected to consist of spherical regions of negative material (u ≈ −1)
embedded within a background of positive material (u ≈ 1), Figure 4.1.
The discretization used standard piecewise linear ﬁnite elements for u and w, a
timestep ∆t = ε2, a ﬁnal time T = 300∆t, and an implicitness parameter θ = 0.5.
The mesh was generated to achieve approximately 2.5 × 105 degrees of freedom per
core, to investigate how the number of Krylov iterations required to solve (2.1) varies
as the mesh is reﬁned. The preconditioner (2.3) approximated the action of A−1
with ten Chebyshev semi-iterations with SOR preconditioning, and approximated the
action of S−1 with two Richardson iterations of ˜S−1. Each action of ˆS−1 was in turn
approximated with ﬁve V-cycles of the BoomerAMG algebraic multigrid solver [7].
GMRES was used as the outer Krylov solver.

The essential properties of a good preconditioner are that the Krylov iteration
counts are low, they grow slowly (if at all) with mesh reﬁnement, and they are robust
to variation in parameters. We therefore examined the average number of Krylov
iterations required per Newton step in two numerical experiments: in the the ﬁrst, all
parameters were ﬁxed, and only the mesh was reﬁned; in the second, the interfacial

1The code is available at http://bitbucket.org/pefarrell/ok-solver .

A PRECONDITIONER FOR THE OHTA-KAWASAKI EQUATIONS

7

Fig. 4.1. The ﬁnal concentration u of the simulation described in section 4. The solution
consists of regions of negative material (u ≈ −1, in blue) embedded within a background of positive
material (u ≈ 1, in red). The ﬁgure shows the isosurfaces corresponding to constant values of u.

Degrees of freedom (×106) Number of cores

0.265
2.060
16.24
128.9
1027

1
8
64
512
4096

Iterations

8.2
8.0
8.0
8.0
8.0

Average number of GMRES iterations per Newton step with preconditioner (2.3) and Schur
complement approximation (2.8) as the problem is weakly scaled. The number of iterations required
grows slowly as the mesh is reﬁned.

Table 4.1

ε

∆x

∆t

Iterations

0.02
0.01
0.005

0.01
0.005
0.0025

0.0004
0.0001
0.000025

8.0
8.0
8.0

Table 4.2

Average number of GMRES iterations per Newton step with preconditioner (2.3) and Schur
complement approximation (2.8) as the interfacial thickness ε is varied (and with it the spatial
discretization ∆x and timestep ∆t). The performance of the preconditioner does not change as
ε → 0.

8

P. E. FARRELL AND J. W. PEARSON

thickness ε was reduced, and with it the spatial discretisztion (to resolve the interface)
and the temporal discretization (to retain stability). The experiments were conducted
on Hexagon, a Cray XE6m-200 hosted at the University of Bergen, and ARCHER, a
Cray XC30 hosted at the University of Edinburgh.

The results of the ﬁrst experiment are shown in Table 4.1. The average number
of Krylov iterations per Newton step remains very close to 8, even as the number of
degrees of freedom is increased by four orders of magnitude. The results of the second
experiment are shown in Table 4.2. The average number of Krylov iterations required
per Newton step does not vary as ε → 0.

5. Conclusions. We have presented a new preconditioner for the Ohta–Kawasaki
equations that model diblock copolymer melts. An approximation to the Schur
complement was derived using a matching strategy. The preconditioner proposed
yields mesh independent convergence, is robust to changes in interfacial thickness if a
timestep criterion is satisﬁed, and requires no reassembly or reconstruction between
Newton steps. This enables the solution of very ﬁne discretizations with billions of
degrees of freedom.

References.

[1] B. Beneˇsov´a, C. Melcher, and E. S¨uli, An implicit midpoint spectral ap-
proximation of nonlocal Cahn–Hilliard equations, SIAM J. Num. Anal., 52 (2014),
pp. 1466–1496.

[2] J. Bosch, D. Kay, M. Stoll, and A. Wathen, Fast solvers for Cahn–Hilliard

inpainting, SIAM J. Imaging Sci., 7 (2014), pp. 67–97.

[3] R. Choksi, M. A. Peletier, and J. F. Williams, On the phase diagram for
microphase separation of diblock copolymers: an approach via a nonlocal Cahn–
Hilliard functional, SIAM J. Appl. Math., 69 (2009), pp. 1712–1738.

[4] G. H. Golub and R. S. Varga, Chebyshev semi-iterative methods, successive
over-relaxation iterative methods, and second order Richardson iterative methods,
Part I, Numer. Math., 3 (1961), pp. 147–156.

[5]

, Chebyshev semi-iterative methods, successive over-relaxation iterative
methods, and second order Richardson iterative methods, Part II, Numer. Math.,
3 (1961), pp. 157–168.

[6] A. Greenbaum, V. Pt´ak, and Z. Strakoˇs, Any nonincreasing convergence
curve is possible for GMRES, SIAM J. Matrix Anal. Appl., 17 (1996), pp. 465–
469.

[7] V. E. Henson and U. M. Yang, BoomerAMG: A parallel algebraic multigrid

solver and preconditioner, Appl. Numer. Math., 41 (2002), pp. 155–177.

[8] I. C. F. Ipsen, A note on preconditioning nonsymmetric matrices, SIAM J. Sci.

Comput., 23 (2001), pp. 1050–1051.

[9] A. Logg, K. A. Mardal, G. N. Wells, et al., Automated Solution of

Diﬀerential Equations by the Finite Element Method, Springer, 2011.

[10] M. F. Murphy, G. H. Golub, and A. J. Wathen, A note on preconditioning

for indeﬁnite linear systems, SIAM J. Sci. Comput., 21 (2000), pp. 1969–1972.

[11] T. Ohta and K. Kawasaki, Equilibrium morphology of block copolymer melts,

Macromolecules, 19 (1986), pp. 2621–2632.

[12] Q. Parsons, Numerical Approximation of the Ohta–Kawasaki Functional, mas-

ter’s thesis, University of Oxford, Oxford, UK, 2012.

[13] J. W. Pearson and A. J. Wathen, A new approximation of the Schur com-
plement in preconditioners for PDE-constrained optimization, Numer. Lin. Alg.
Appl., 19 (2012), pp. 816–829.

A PRECONDITIONER FOR THE OHTA-KAWASAKI EQUATIONS

9

[14] Y. Saad and M. Schultz, GMRES: a generalized minimal residual algorithm
for solving nonsymmetric linear systems, SIAM J. Sci. Stat. Comput., 7 (1986),
pp. 856–869.

[15] A. J. Wathen and T. Rees, Chebyshev semi-iteration in preconditioning for
problems including the mass matrix, Electron. Trans. Numer. Anal., 34 (2009),
pp. 125–135.

This figure "ok-clip.png" is available in "png"(cid:10) format from:

http://arxiv.org/ps/1603.04570v1

This figure "ok.png" is available in "png"(cid:10) format from:

http://arxiv.org/ps/1603.04570v1

