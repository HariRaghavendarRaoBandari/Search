Counter-ﬁtting Word Vectors to Linguistic Constraints

Nikola Mrkˇsi´c1, Diarmuid ´O S´eaghdha2, Blaise Thomson2, Milica Gaˇsi´c1

Lina Rojas-Barahona1, Pei-Hao Su1, David Vandyke1, Tsung-Hsien Wen1, Steve Young1

1 Department of Engineering, University of Cambridge, UK

2 Apple Inc.

{nm480,mg436,phs26,djv27,thw28,sjy}@cam.ac.uk

{doseaghdha, blaisethom}@apple.com

6
1
0
2

 
r
a

M
2

 

 
 
]
L
C
.
s
c
[
 
 

1
v
2
9
8
0
0

.

3
0
6
1
:
v
i
X
r
a

Abstract

In this work, we present a novel counter-ﬁtting
method which injects antonymy and synonymy
constraints into vector space representations in
order to improve the vectors’ capability for
judging semantic similarity. Applying this
method to publicly available pre-trained word
vectors leads to a new state of the art perfor-
mance on the SimLex-999 dataset. We also
show how the method can be used to tailor the
word vector space for the downstream task of
dialogue state tracking, resulting in robust im-
provements across different dialogue domains.

east
west
north
south

southeast
northeast
eastward
eastern
easterly

-
-

expensive

pricey
cheaper
costly

overpriced
inexpensive

costly
pricy

overpriced

pricey
afford

British
American
Australian

Britain
European
England

Brits
London
BBC
UK

Britain

Before

After

Table 1: Nearest neighbours for target words using GloVe
vectors before and after counter-ﬁtting

1

Introduction

Many popular methods that induce representations
for words rely on the distributional hypothesis – the
assumption that semantically similar or related words
appear in similar contexts. This hypothesis sup-
ports unsupervised learning of meaningful word rep-
resentations from large corpora (Curran, 2003; ´O
S´eaghdha and Korhonen, 2014; Mikolov et al., 2013;
Pennington et al., 2014). Word vectors trained using
these methods have proven useful for many down-
stream tasks including machine translation (Zou et al.,
2013) and dependency parsing (Bansal et al., 2014).
One drawback of learning word embeddings from
co-occurrence information in corpora is that it tends
to coalesce the notions of semantic similarity and con-
ceptual association (Hill et al., 2014b). Furthermore,
even methods that can distinguish similarity from
association (e.g., based on syntactic co-occurrences)
will generally fail to tell synonyms from antonyms
(Mohammad et al., 2008). For example, words such

as east and west or expensive and inexpensive appear
in near-identical contexts, which means that distribu-
tional models produce very similar word vectors for
such words. Examples of such anomalies in GloVe
vectors can be seen in Table 1, where words such as
cheaper and inexpensive are deemed similar to (their
antonym) expensive.

A second drawback is that similarity and antonymy
can be application- or domain-speciﬁc. In our case,
we are interested in exploiting distributional knowl-
edge for the dialogue state tracking task (DST). The
DST component of a dialogue system is responsi-
ble for interpreting users’ utterances and updating
the system’s belief state – a probability distribution
over all possible states of the dialogue. For exam-
ple, a DST for the restaurant domain needs to detect
whether the user wants a cheap or expensive restau-
rant. Being able to generalise using distributional
information while still distinguishing between se-
mantically different yet conceptually related words

(e.g. cheaper and pricey) is critical for the perfor-
mance of dialogue systems. In particular, a dialogue
system can be led seriously astray by false synonyms.
We propose a method that addresses these two
drawbacks by using synonymy and antonymy rela-
tions drawn from either a general lexical resource
or an application-speciﬁc ontology to ﬁne-tune dis-
tributional word vectors. Our method, which we
term counter-ﬁtting, is a lightweight post-processing
procedure in the spirit of retroﬁtting (Faruqui et al.,
2015). The second row of Table 1 illustrates the
results of counter-ﬁtting: the nearest neighbours cap-
ture true similarity much more intuitively than the
original GloVe vectors. The procedure improves
word vector quality regardless of the initial word vec-
tors provided as input.1 By applying counter-ﬁtting
to the Paragram-SL999 word vectors provided by
Wieting et al. (2015), we achieve new state-of-the-art
performance on SimLex-999, a dataset designed to
measure how well different models judge semantic
similarity between words (Hill et al., 2014b). We
also show that the counter-ﬁtting method can in-
ject knowledge of dialogue domain ontologies into
word vector space representations to facilitate the
construction of semantic dictionaries which improve
DST performance across two different dialogue do-
mains. Our tool and word vectors are available at
github.com/nmrksic/counter-fitting.

2 Related Work

Most work on improving word vector representa-
tions using lexical resources has focused on bringing
words which are known to be semantically related
closer together in the vector space. Some methods
modify the prior or the regularization of the original
training procedure (Yu and Dredze, 2014; Bian et al.,
2014; Kiela et al., 2015). Wieting et al. (2015) use
the Paraphrase Database (Ganitkevitch et al., 2013)
to train word vectors which emphasise word simi-
larity over word relatedness. These word vectors
achieve the current state-of-the-art performance on
the SimLex-999 dataset and are used as input for
counter-ﬁtting in our experiments.

1When we write “improve”, we refer to improving the vector
space for a speciﬁc purpose. We do not expect that a vector
space ﬁne-tuned for semantic similarity will give better results
on semantic relatedness. As Mohammad et al. (2008) observe,
antonymous concepts are related but not similar.

Recently, there has been interest in lightweight
post-processing procedures that use lexical knowl-
edge to reﬁne off-the-shelf word vectors without re-
quiring large corpora for (re-)training as the afore-
mentioned “heavyweight” procedures do. Faruqui
et al.’s (2015) retroﬁtting approach uses similarity
constraints from WordNet and other resources to pull
similar words closer together.

The complications caused by antonymy for distri-
butional methods are well-known in the semantics
community. Most prior work focuses on extracting
antonym pairs from text rather than exploiting them
(Lin et al., 2003; Mohammad et al., 2008; Turney,
2008; Hashimoto et al., 2012; Mohammad et al.,
2013). The most common use of antonymy infor-
mation is to provide features for systems that de-
tect contradictions or logical entailment (Marcu and
Echihabi, 2002; de Marneffe et al., 2008; Zanzotto
et al., 2009). As far as we are aware, there is no
previous work on exploiting antonymy in dialogue
systems. The modelling work closest to ours are
Liu et al. (2015), who use antonymy and WordNet
hierarchy information to modify the heavyweight
Word2Vec training objective; Yih et al. (2012), who
use a Siamese neural network to improve the qual-
ity of Latent Semantic Analysis vectors; Schwartz et
al. (2015), who build a standard distributional model
from co-occurrences based on symmetric patterns,
with speciﬁed antonymy patterns counted as nega-
tive co-occurrences; and Ono et al. (2015), who use
thesauri and distributional data to train word embed-
dings specialised for capturing antonymy.

3 Counter-ﬁtting Word Vectors to

Linguistic Constraints

1, v(cid:48)

2, . . . , v(cid:48)

Our starting point is an indexed set of word vec-
tors V = {v1, v2, . . . , vN} with one vector for each
word in the vocabulary. We will inject semantic re-
lations into this vector space to produce new word
vectors V (cid:48) = {v(cid:48)
N}. For antonymy
and synonymy we have a set of constraints A and
S, respectively. The elements of each set are pairs
of word indices; for example, each pair (i, j) in S is
such that the i-th and j-th words in the vocabulary are
synonyms. The objective function used to counter-ﬁt
the pre-trained word vectors V to the sets of linguistic
constraints A and S contains three different terms:

1. Antonym Repel (AR): This term serves to push
antonymous words’ vectors away from each other in
the transformed vector space V (cid:48):

AR(V (cid:48)) =

(cid:88)

τ(cid:0)δ − d(v(cid:48)

w)(cid:1)

u, v(cid:48)

(u,w)∈A

where d(vi, vj) = 1−cos(vi, vj) is a distance derived
from cosine similarity and τ (x) (cid:44) max(0, x) im-
poses a margin on the cost. Intuitively, δ is the “ideal”
minimum distance between antonymous words; in
our experiments we set δ = 1.0 as it corresponds to
vector orthogonality.

2. Synonym Attract (SA): The counter-ﬁtting
procedure should seek to bring the word vectors of
known synonymous word pairs closer together:

SA(V (cid:48)) =

(cid:88)

τ(cid:0)d(v(cid:48)

w) − γ(cid:1)

u, v(cid:48)

(u,w)∈S

where γ is the “ideal” maximum distance between
synonymous words; we use γ = 0.

3. Vector Space Preservation (VSP): the topol-
ogy of the original vector space describes relation-
ships between words in the vocabulary captured using
distributional information from very large textual cor-
pora. The VSP term bends the transformed vector
space towards the original one as much as possible in
order to preserve the semantic information contained
in the original vectors:

N(cid:88)

(cid:88)

i=1

j∈N (i)

τ(cid:0)d(v(cid:48)

j) − d(vi, vj)(cid:1)

i, v(cid:48)

VSP(V, V (cid:48)) =

For computational efﬁciency, we do not calculate
distances for every pair of words in the vocabulary.
Instead, we focus on the (pre-computed) neighbour-
hood N (i), which denotes the set of words within
a certain radius ρ around the i-th word’s vector in
the original vector space V . Our experiments indi-
cate that counter-ﬁtting is relatively insensitive to the
choice of ρ, with values between 0.2 and 0.4 showing
little difference in quality; here we use ρ = 0.2.

The objective function for the training procedure

is given by a weighted sum of the three terms:
C(V, V (cid:48)) = k1AR(V (cid:48))+k2SA(V (cid:48))+k3VSP(V, V (cid:48))

where k1, k2, k3 ≥ 0 are hyperparameters that con-
trol the relative importance of each term.
In our
experiments we set them to be equal: k1 = k2 = k3.
To minimise the cost function for a set of starting
vectors V and produce counter-ﬁtted vectors V (cid:48), we
run stochastic gradient descent (SGD) for 20 epochs.
An end-to-end run of counter-ﬁtting takes less than
two minutes on a laptop with four CPUs.

3.1

Injecting Dialogue Domain Ontologies into
Vector Space Representations

Dialogue state tracking (DST) models capture users’
goals given their utterances. Goals are represented as
sets of constraints expressed by slot-value pairs such
as [food: Indian] or [parking: allowed]. The set of
slots S and the set of values Vs for each slot make up
the ontology of a dialogue domain.

In this paper we adopt the recurrent neural network
(RNN) framework for tracking suggested in (Hender-
son et al., 2014d; Henderson et al., 2014c; Mrkˇsi´c et
al., 2015). Rather than using a spoken language un-
derstanding (SLU) decoder to convert user utterances
into meaning representations, this model operates
directly on the n-gram features extracted from the
automated speech recognition (ASR) hypotheses. A
drawback of this approach is that the RNN model
can only perform exact string matching to detect the
slot names and values mentioned by the user. It can-
not recognise synonymous words such as pricey and
expensive, or even subtle morphological variations
such as moderate and moderately. A simple way to
mitigate this problem is to use semantic dictionaries:
lists of rephrasings for the values in the ontology.
Manual construction of dictionaries is highly labour-
intensive; however, if one could automatically detect
high-quality rephrasings, then this capability would
come at no extra cost to the system designer.

To obtain a set of word vectors which can be used
for creating a semantic dictionary, we need to inject
the domain ontology into the vector space. This can
be achieved by introducing antonymy constraints be-
tween all the possible values of each slot (i.e. Chinese
and Indian, expensive and cheap, etc.). The remain-
ing linguistic constraints can come from semantic
lexicons: the richer the sets of injected synonyms
and antonyms are, the better the resulting word rep-
resentations will become.

ρ

Model / Word Vectors
Neural MT Model (Hill et al., 2014a)
0.52
Symmetric Patterns (Schwartz et al., 2015)
0.56
Non-distributional Vectors (Faruqui and Dyer, 2015)
0.58
GloVe vectors (Pennington et al., 2014)
0.41
GloVe vectors + Retroﬁtting
0.53
GloVe + Counter-ﬁtting
0.58
Paragram-SL999 (Wieting et al., 2015)
0.69
Paragram-SL999 + Retroﬁtting
0.68
0.74
Paragram-SL999 + Counter-ﬁtting
Inter-annotator agreement
0.67
Annotator/gold standard agreement
0.78
Table 2: Performance on SimLex-999. Retroﬁtting uses
the code and (PPDB) data provided by the authors

4 Experiments
4.1 Word Vectors and Semantic Lexicons
Two different collections of pre-trained word vectors
were used as input to the counter-ﬁtting procedure:

1. Glove Common Crawl 300-dimensional vec-
tors made available by Pennington et al. (2014).

2. Paragram-SL999 300-dimensional vectors

made available by Wieting et al. (2015).

The synonymy and antonymy constraints were ob-

tained from two semantic lexicons:

1. PPDB 2.0 (Pavlick et al., 2015): the latest re-
lease of the Paraphrase Database. A new fea-
ture of this version is that it assigns relation
types to its word pairs. We identify the Equiv-
alence relation with synonymy and Exclusion
with antonymy. We used the largest available
(XXXL) version of the database and only con-
sidered single-token terms.

2. WordNet (Miller, 1995): a well known seman-
tic lexicon which contains vast amounts of high
quality human-annotated synonym and antonym
pairs. Any two words in our vocabulary which
had antonymous word senses were considered
antonyms; WordNet synonyms were not used.

In total, the lexicons yielded 12,802 antonymy and
31,828 synonymy pairs for our vocabulary, which
consisted of 76,427 most frequent words in Open-
Subtitles, obtained from invokeit.wordpress.
com/frequency-word-lists/.

Semantic Resource
Baseline (no linguistic constraints)
PPDB− (PPDB antonyms)
PPDB+ (PPDB synonyms)
WordNet− (WordNet antonyms)
PPDB− and PPDB+
WordNet− and PPDB−
WordNet− and PPDB+
WordNet− and PPDB− and PPDB+
Table 3: SimLex-999 performance when different sets of
linguistic constraints are used for counter-ﬁtting

Glove
0.41
0.43
0.46
0.52
0.50
0.53
0.58
0.58

Paragram

0.69
0.69
0.68
0.74
0.69
0.74
0.74
0.74

Improving Lexical Similarity Predictions

4.2
In this section, we show that counter-ﬁtting pre-
trained word vectors with linguistic constraints im-
proves their usefulness for judging semantic simi-
larity. We use Spearman’s rank correlation coefﬁ-
cient with the SimLex-999 dataset, which contains
word pairs ranked by a large number of annotators
instructed to consider only semantic similarity.

Table 2 contains a summary of recently reported
competitive scores for SimLex-999, as well as the
performance of the unaltered, retroﬁtted and counter-
ﬁtted GloVe and Paragram-SL999 word vectors. To
the best of our knowledge, the 0.685 ﬁgure reported
for the latter represents the current high score. This
ﬁgure is above the average inter-annotator agreement
of 0.67, which has been referred to as the ceiling
performance in most work up to now.

In our opinion, the average inter-annotator agree-
ment is not the only meaningful measure of ceiling
performance. We believe it also makes sense to com-
pare: a) the model ranking’s correlation with the gold
standard ranking to: b) the average rank correlation
that individual human annotators’ rankings achieved
with the gold standard ranking. The SimLex-999
authors have informed us that the average annotator
agreement with the gold standard is 0.78.2 As shown
in Table 2, the reported performance of all the models
and word vectors falls well below this ﬁgure.

Retroﬁtting pre-trained word vectors improves
GloVe vectors, but not the already semantically spe-
cialised Paragram-SL999 vectors. Counter-ﬁtting
substantially improves both sets of vectors, showing
that injecting antonymy relations goes a long way

2This ﬁgure is now reported as a potentially fairer ceiling
performance on the SimLex-999 website: http://www.cl.
cam.ac.uk/˜fh295/simlex.html.

False Synonyms Fixed False Antonyms Fixed

dumb, dense
adult, guardian
polite, proper
strength, might

water, ice

violent, angry

cat, lion

laden, heavy
engage, marry

(cid:88)(cid:88)

(cid:88)(cid:88)
(cid:88)(cid:88)
(cid:88)(cid:88)

(cid:88)

(cid:88)(cid:88)
(cid:88)
(cid:88)

sunset, sunrise
forget, ignore

girl, maid

happiness, luck

south, north
go, come

groom, bride

dinner, breakfast

-

-

Table 4: Highest-error SimLex-999 word pairs using Para-
gram vectors (before counter-ﬁtting)

towards improving word vectors for the purpose of
making semantic similarity judgements.

Table 3 shows the effect of injecting different cate-
gories of linguistic constraints. GloVe vectors beneﬁt
from all three sets of constraints, whereas the quality
of Paragram vectors, already exposed to PPDB, only
improves with the injection of WordNet antonyms.
Table 4 illustrates how incorrect similarity predic-
tions based on the original (Paragram) vectors can
be ﬁxed through counter-ﬁtting. The table presents
eight false synonyms and nine false antonyms: word
pairs with predicted rank in the top (bottom) 200
word pairs and gold standard rank 500 or more posi-
tions lower (higher). Eight of these errors are ﬁxed
by counter-ﬁtting: the difference between predicted
and gold-standard ranks is now 100 or less. Interest-
ingly, ﬁve of the eight corrected word pairs do not
appear in the sets of linguistic constraints; these are
indicated by double ticks in the table. This shows
that secondary (i.e. indirect) interactions through the
three terms of the cost function do contribute to the
semantic content of the transformed vector space.

Improving Dialogue State Tracking

4.3
Table 5 shows the dialogue state tracking datasets
used for evaluation. These datasets come from the
Dialogue State Tracking Challenges 2 and 3 (Hender-
son et al., 2014a; Henderson et al., 2014b).

We used four different sets of word vectors to con-
struct semantic dictionaries: the original GloVe and
Paragram-SL999 vectors, as well as versions counter-
ﬁtted to each domain ontology. The constraints used
for counter-ﬁtting were all those from the previous
section as well as antonymy constraints among the
set of values for each slot. We treated all vocabu-
lary words within some radius t of a slot value as

Dataset

Restaurants

Train Dev Test
1117
1612
1600
225

506
439

Tourist Information
Table 5: Number of dialogues in the dataset splits used
for the Dialogue State Tracking experiments

#Slots

4
9

Restaurants Tourist Info

Word Vector Space
Baseline (no dictionary)
GloVe
GloVe + Counter-ﬁtting
Paragram-SL999
Paragram-SL999 + Counter-ﬁtting
Table 6: Performance of RNN belief trackers (ensembles
of four models) with different semantic dictionaries

60.5
60.9
62.8
61.5
61.9

68.6
72.5
73.4
73.2
73.5

rephrasings of that value. The optimal value of t
was determined using a grid search: we generated a
dictionary and trained a model for each potential t,
then evaluated on the development set. Table 6 shows
the performance of RNN models which used the con-
structed dictionaries. The dictionaries induced from
the pre-trained vectors substantially improved track-
ing performance over the baselines (which used no
semantic dictionaries). The dictionaries created us-
ing the counter-ﬁtted vectors improved performance
even further. Contrary to the SimLex-999 experi-
ments, starting from the Paragram vectors did not
lead to superior performance, which shows that in-
jecting the application-speciﬁc ontology is at least as
important as the quality of the initial word vectors.

5 Conclusion
We have presented a novel counter-ﬁtting method
for injecting linguistic constraints into word vector
space representations. The method efﬁciently post-
processes word vectors to improve their usefulness
for tasks which involve making semantic similarity
judgements. Its focus on separating vector represen-
tations of antonymous word pairs lead to substantial
improvements on genuine similarity estimation tasks.
We have also shown that counter-ﬁtting can tailor
word vectors for downstream tasks by using it to
inject domain ontologies into word vectors used to
construct semantic dictionaries for dialogue systems.

Acknowledgements
We would like to thank Felix Hill for help with the
SimLex-999 evaluation. We also thank the anony-
mous reviewers for their helpful suggestions.

References
[Bansal et al.2014] Mohit Bansal, Kevin Gimpel, and
Karen Livescu. 2014. Tailoring continuous word repre-
sentations for dependency parsing. In Proceedings of
ACL.

[Bian et al.2014] Jiang Bian, Bin Gao, and Tie-Yan Liu.
2014. Knowledge-powered deep learning for word
embedding. In Machine Learning and Knowledge Dis-
covery in Databases.

[Curran2003] James Curran. 2003. From Distributional
to Semantic Similarity. Ph.D. thesis, School of Infor-
matics, University of Edinburgh.

[de Marneffe et al.2008] Marie-Catherine de Marneffe,
Anna N. Rafferty, and Christopher D. Manning. 2008.
Finding contradictions in text. In Proceedings of ACL.
[Faruqui and Dyer2015] Manaal Faruqui and Chris Dyer.
2015. Non-distributional word vector representations.
In Proceedings of ACL.

[Faruqui et al.2015] Manaal Faruqui, Jesse Dodge, Su-
jay K. Jauhar, Chris Dyer, Eduard Hovy, and Noah A.
Smith. 2015. Retroﬁtting Word Vectors to Semantic
Lexicons. In Proceedings of NAACL HLT.

[Ganitkevitch et al.2013] Juri Ganitkevitch, Benjamin Van
Durme, and Chris Callison-burch. 2013. PPDB: The
Paraphrase Database. In Proceedings of NAACL HLT.
[Hashimoto et al.2012] Chikara Hashimoto, Kentaro Tori-
sawa, Stijn De Saeger, Jong-Hoon Oh, and Junichi
Kazama. 2012. Excitatory or inhibitory: A new seman-
tic orientation extracts contradiction and causality from
the Web. In Proceedings of EMNLP-CoNLL.

[Henderson et al.2014a] Matthew Henderson, Blaise
Thomson, and Jason D. Wiliams. 2014a. The Second
Dialog State Tracking Challenge. In Proceedings of
SIGDIAL.

[Henderson et al.2014b] Matthew Henderson, Blaise
Thomson, and Jason D. Wiliams. 2014b. The Third
Dialog State Tracking Challenge. In Proceedings of
IEEE SLT.

[Henderson et al.2014c] Matthew Henderson, Blaise
Thomson, and Steve Young. 2014c. Robust Dia-
log State Tracking using Delexicalised Recurrent
Neural Networks and Unsupervised Adaptation.
In
Proceedings of IEEE SLT.

[Henderson et al.2014d] Matthew Henderson, Blaise
Thomson, and Steve Young. 2014d. Word-Based
Dialog State Tracking with Recurrent Neural Networks.
In Proceedings of SIGDIAL.

[Hill et al.2014a] Felix Hill, Kyunghyun Cho, Sbastien
Jean, Coline Devin, and Yoshua Bengio. 2014a. Em-
bedding word similarity with neural machine transla-
tion. Computing Research Repository.

[Hill et al.2014b] Felix Hill, Roi Reichart, and Anna Ko-
rhonen. 2014b. SimLex-999: Evaluating Semantic

Models with (Genuine) Similarity Estimation. Comput-
ing Research Repository.

[Kiela et al.2015] Douwe Kiela, Felix Hill, and Stephen
Clark. 2015. Specializing word embeddings for simi-
larity or relatedness. In Proceedings of EMNLP.

[Lin et al.2003] Dekang Lin, Shaojun Zhao, Lijuan Qin,
and Ming Zhou. 2003. Identifying synonyms among
distributionally similar words. In Proceedings of IJCAI.
[Liu et al.2015] Quan Liu, Hui Jiang, Si Wei, Zhen-Hua
Ling, and Yu Hu. 2015. Learning semantic word
embeddings based on ordinal knowledge constraints.
In Proceedings of ACL.

[Marcu and Echihabi2002] Daniel Marcu and Abdsem-
mad Echihabi. 2002. An unsupervised approach to rec-
ognizing discourse relations. In Proceedings of ACL.
[Mikolov et al.2013] Tomas Mikolov, Ilya Sutskever, Kai
Chen, Greg S Corrado, and Jeff Dean. 2013. Dis-
tributed Representations of Words and Phrases and their
Compositionality. In Proceedings of NIPS.

[Miller1995] George A. Miller. 1995. WordNet: A Lexi-
cal Database for English. Communications of the ACM.
[Mohammad et al.2008] Saif Mohammad, Bonnie Dorr,
2008. Computing word-pair

and Graeme Hirst.
antonymy. In Proceedings of EMNLP.

[Mohammad et al.2013] Saif M. Mohammad, Bonnie J.
Dorr, Graeme Hirst, and Peter D. Turney. 2013. Com-
puting lexical contrast. Computational Linguistics,
39(3):555–590.

[Mrkˇsi´c et al.2015] Nikola Mrkˇsi´c, Diarmuid ´O S´eaghdha,
Blaise Thomson, Milica Gaˇsi´c, Pei-Hao Su, David
Vandyke, Tsung-Hsien Wen, and Steve Young. 2015.
Multi-domain Dialog State Tracking using Recurrent
Neural Networks. In Proceedings of ACL.

[Ono et al.2015] Masataka Ono, Makoto Miwa, and Yu-
taka Sasaki. 2015. Word Embedding-based Antonym
Detection using Thesauri and Distributional Informa-
tion. In Proceedings of NAACL HLT.

[ ´O S´eaghdha and Korhonen2014] Diarmuid ´O S´eaghdha
and Anna Korhonen. 2014. Probabilistic distributional
semantics. Computational Linguistics, 40(3):587–631.
[Pavlick et al.2015] Ellie Pavlick, Pushpendre Rastogi,
Juri Ganitkevich, Benjamin Van Durme, and Chris
Callison-Burch. 2015. PPDB 2.0: Better paraphrase
ranking, ﬁne-grained entailment relations, word em-
beddings, and style classiﬁcation. In Proceedings of
ACL.

[Pennington et al.2014] Jeffrey Pennington, Richard
Socher, and Christopher Manning.
2014. Glove:
Global Vectors for Word Representation. In Proceed-
ings of EMNLP.

[Schwartz et al.2015] Roy Schwartz, Roi Reichart, and Ari
Rappoport. 2015. Symmetric pattern based word em-
beddings for improved word similarity prediction. In
Proceedings of CoNLL.

[Turney2008] Peter D. Turney. 2008. A uniform approach
to analogies, synonyms, antonyms, and associations. In
Proceedings of COLING.

[Wieting et al.2015] John Wieting, Mohit Bansal, Kevin
Gimpel, and Karen Livescu. 2015. From paraphrase
database to compositional paraphrase model and back.
Transactions of the Association for Computational Lin-
guistics.

[Yih et al.2012] Wen-Tau Yih, Geoffrey Zweig, and
John C. Platt. 2012. Polarity inducing Latent Semantic
Analysis. In Proceedings of ACL.

[Yu and Dredze2014] Mo Yu and Mark Dredze. 2014. Im-
proving lexical embeddings with semantic knowledge.
In Proceedings of ACL.

[Zanzotto et al.2009] Fabio Massimo Zanzotto, Marco
Pennachiotti, and Alessandro Moschitti. 2009. A ma-
chine learning approach to textual entailment recog-
Journal of Natural Language Engineering,
nition.
15(4):551–582.

[Zou et al.2013] Will Y. Zou, Richard Socher, Daniel M.
Cer, and Christopher D. Manning. 2013. Bilingual
word embeddings for phrase-based machine translation.
In Proceedings of EMNLP.

