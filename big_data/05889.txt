6
1
0
2

 
r
a

 

M
8
1

 
 
]

.

R
P
h
t
a
m

[
 
 

1
v
9
8
8
5
0

.

3
0
6
1
:
v
i
X
r
a

Quasi-Stationary Asymptotics for Perturbed

Semi-Markov Processes in Discrete Time

Mikael Petersson∗

Abstract

We consider a discrete time semi-Markov process where the char-
acteristics deﬁning the process depend on a small perturbation pa-
rameter. It is assumed that the state space consists of one ﬁnite com-
municating class of states and, in addition, one absorbing state. Our
main object of interest is the asymptotic behaviour of the joint prob-
abilities of the position of the semi-Markov process and the event of
non-absorption as time tends to inﬁnity and the perturbation param-
eter tends to zero. The main result gives exponential expansions of
these probabilities together with an recursive algorithm for computing
the coeﬃcients in the expansions.

Keywords: Semi-Markov process, Perturbation, Asymptotic Expansion,
Regenerative process, Renewal equation, Solidarity property, First hitting
time.

MSC2010: Primary 60K15; Secondary 41A60, 60K05.

1

Introduction

The aim of this paper is to present a detailed asymptotic analysis of the
long time behaviour of non-linearly perturbed discrete time semi-Markov
processes with absorption.

We consider a discrete time semi-Markov process ξ(ε)(n), on a ﬁnite state
space, depending on a small perturbation parameter ε ≥ 0 in the sense
that its transition probabilities are functions of ε. It is assumed that these
functions are continuous at ε = 0 so that the process ξ(ε)(n) for ε > 0 can

∗Department of Mathematics, Stockholm University, SE-106 91 Stockholm, Sweden,

mikpe@math.su.se.

1

be interpreted as a perturbation of the process ξ(0)(n). Furthermore, we
assume that for ε small enough, the state space can be partitioned into one
communicating class of states {1, . . . , N} and one absorbing state 0. The
absorption time, that is, the ﬁrst hitting time of state 0 for the semi-Markov
process ξ(ε)(n), is denoted by µ(ε)
0 .

Our main object of interest is the asymptotic behaviour of the probabil-

ities

P (ε)
ij (n) = Pi{ξ(ε)(n) = j, µ(ε)

0 > n}, i, j 6= 0,

as n → ∞ and ε → 0.

It turns out that the forms of the asymptotic results depend on if one-
step absorption probabilities vanish asymptotically or if some of them are
non-zero in the limit. In the former case the absorption time µ(ε)
0 → ∞ in
probability as ε → 0 and we get so-called pseudo-stationary asymptotics for
the probabilities P (ε)
0 are stochastically bounded as
ε → 0 and we get so-called quasi-stationary asymptotics for the probabilities
P (ε)

ij (n). In the present paper we give a uniﬁed treatment of both cases.

ij (n). In the latter case, µ(ε)

Our perturbation conditions are formulated in terms of the following

mixed power-exponential moments for transition probabilities:

p(ε)
ij (ρ(0), r) =

∞Xn=0

nreρ(0)nQ(ε)

ij (n), r = 0, 1, . . . , i, j 6= 0,

(1.1)

where Q(ε)
ij (n) are the transition probabilities for the semi-Markov process
and ρ(0) is a non-negative constant determined by the distribution of ﬁrst
return time to the initial state for the limiting semi-Markov process. In the
pseudo-stationary case ρ(0) = 0 and then the moments in (1.1) reduce to
usual power moments.

We allow for smooth non-linear perturbations which means that the mo-
ments in (1.1) may be non-linear function of ε which for r = 0, . . . , k can be
expanded in an asymptotic power series with respect to ε.

As it turns out, the asymptotics of the probabilities P (ε)

ij (n) depends on
the balance between the rate at which the time n → ∞ and the perturbation
ε → 0. If we write n = n(ε) as a function of ε, this balance is characterized
by the following relation:

εrn(ε) → λr ∈ [0, ∞), for some 1 ≤ r ≤ k.

(1.2)

Under assumptions mentioned above and some additional Cram´er type
conditions on moments of transition times and a non-periodicity condition
for the limiting semi-Markov process we obtain the following which is our

2

main result: For any n(ε) → ∞ as ε → 0 in such a way that relation (1.2)
holds, we have

Pi{ξ(ε)(n(ε)) = j, µ(ε)

0 > n(ε)}

exp(−(ρ(0) + c1ε + · · · + cr−1εr−1)n(ε))

ij

→ eπ(0)

eλrcr

as ε → 0, i, j 6= 0.

(1.3)

Relation (1.3) is supplemented with (i) an explicit expression for the constant
ij , (ii) an equation from which ρ(0) can be found at least numerically, and
(iii) a recursive algorithm for computing the coeﬃcients c1, . . . , cr as rational
functions of coeﬃcients in expansions of the moments in Equation (1.1).

eπ(0)
pler form. In this case, ρ(0) = 0 and the constants eπ(0)

In the pseudo-stationary case, the asymptotic relation (1.3) takes a sim-
ij do not depend on
the initial state i and are given by the stationary probabilities of the limiting
semi-Markov process.

In order to prove (1.3) we use the theory of perturbed discrete time re-
newal equations developed in Gyllenberg and Silvestrov (1994), Englund and
Silvestrov (1997), and Silvestrov and Petersson (2013). However, the results
can not be applied directly. This is because conditions for semi-Markov pro-
cesses are naturally formulated in terms of its transition probabilities while
the application of the renewal theory requires conditions for some non-local
characteristics of the semi-Markov process to hold. To prove that the con-
ditions we formulate for semi-Markov processes are suﬃcient for the condi-
tions required for the results from renewal theory we use techniques from
Gyllenberg and Silvestrov (2008). In particular, we need to calculate the co-
eﬃcients in expansions of mixed power-exponential moments for ﬁrst return
times based on the coeﬃcients in the expansions of the moments in Equation
(1.1). This analysis makes up a substantial part of the proof of the main
result and may also have applications beyond the scope of this paper.

The asymptotic relation (1.3) is proved for continuous time semi-Markov
processes in Gyllenberg and Silvestrov (1999, 2008). In Gyllenberg and Silve-
strov (2008) the result is also extended to the case of initial transient states.
Expansions of the type given in Equation (1.3) and similar types of ex-
ponential expansions have also been given for ruin probabilities in perturbed
risk models, see for example Gyllenberg and Silvestrov (2000, 2008), Englund
(2001), Blanchet and Zwart (2010), Ni (2011, 2014), and Petersson (2014).
In the pseudo-stationary case, many of the existing results in the literature
are concerned with an asymptotic analysis of absorption times or other types
of ﬁrst hitting times in various types of Markov and semi-Markov processes,
see for example Keilson (1966), Latouche and Louchard (1978), Latouche
(1991), Avrachenkov and Haviv (2004), Drozdenko (2007), and Jung (2013).
In the quasi-stationary case, almost all papers in the literature deals with

3

models without perturbations. In particular, a great deal of attention has
been given the study of so-called quasi-stationary distributions, see for ex-
ample Darroch and Seneta (1965), Seneta and Vere-Jones (1966), Cheong
(1970), Flaspohler and Holmes (1972), Collet, Mart´ınez, and San Mart´ın
(2013), and van Doorn and Pollett (2013). For models with perturbations,
asymptotic expansions of quasi-stationary distributions are given in Gyllen-
berg and Silvestrov (2008) for continuous time regenerative processes and
semi-Markov processes, and in Petersson (2013) for discrete time regenera-
tive processes.

One of the most extensively studied models of perturbed stochastic pro-
cesses is the model of linearly perturbed Markov chains. In particular, asymp-
totic expansions of stationary distributions have been given for so-called
nearly uncoupled Markov chains. For some results and more references re-
lated to this line of research we refer to Simon and Ando (1961), Schweitzer
(1968), Stewart (1991), Hassin and Haviv (1992), Yin and Zhang (1998,
2003), Altman, Avrachenkov and N´u˜nez-Queija (2004), and Avrachenkov,
Filar, and Howlett (2013).

For more references related to pseudo-stationary and quasi-stationary
asymptotics we refer to the extensive bibliography given in Gyllenberg and
Silvestrov (2008).

Let us ﬁnally brieﬂy outline the structure of the paper. Section 2 presents
exponential expansions for perturbed discrete time regenerative processes.
In Section 3 we present in detail the model of perturbed discrete time semi-
Markov processes and introduce some notation that will be used throughout
the paper. Section 4 derives systems of linear equations for moments of
ﬁrst hitting times and gives a necessary and suﬃcient condition for these
moments to be ﬁnite. In Section 5 we prove some solidarity properties for
moments of ﬁrst hitting times which are essential for our main result. Section
6 constructs asymptotic power series expansions for moments of ﬁrst hitting
times.
In Section 7 we prove a solidarity property of periodicity which is
needed in order to apply the renewal theory. Finally, Section 8 presents the
main asymptotic result.

2 Exponential Expansions for Perturbed

Regenerative Processes

This section presents asymptotic exponential expansions for perturbed dis-
crete time regenerative processes. The results in this section are obtained by
applying a corresponding result for discrete time renewal equations given in

4

Silvestrov and Petersson (2013).

For every ε ≥ 0, let Z (ε)

n , n = 0, 1, . . . , be a regenerative process on a
measurable state space (X , Γ) with proper regeneration times 0 = τ (ε)
0 <
τ (ε)
1 < · · · . Furthermore, let µ(ε) be a random variable, deﬁned on the same
probability space, that takes values in the set {0, 1, . . . , ∞}. Assume that for
each A ∈ Γ, the probabilities P (ε)(n, A) = P{Z (ε)
n ∈ A, µ(ε) > n} satisfy the
renewal equation

P (ε)(n, A) = q(ε)(n, A) +

nXk=0

P (ε)(n − k, A)f (ε)(k), n = 0, 1, . . . ,

where

and

q(ε)(n, A) = P{Z (ε)

n ∈ A, µ(ε) ∧ τ (ε)

1 > n}

f (ε)(k) = P{τ (ε)

1 = k, µ(ε) > τ (ε)

1 }.

Then, we call µ(ε) a regenerative stopping time.

Notice that f (ε)(n) are possibly improper distributions with defect

f (ε) = 1 −

∞Xn=0

f (ε)(n) = P{µ(ε) ≤ τ (ε)

1 },

that is, the defect is given by the stopping probability in one regeneration
period.

Moment generating functions for ﬁrst regeneration times are deﬁned by

φ(ε)(ρ) =

∞Xn=0

eρnf (ε)(n), ρ ∈ R.

We will assume that the distributions of ﬁrst regeneration times satisfy

the following conditions:

A∗: (a) f (ε)(n) → f (0)(n) as ε → 0, for all n = 0, 1, . . . , where the limiting
distribution f (0)(n) is non-periodic and not concentrated at zero.

(b) f (ε) → f (0) ∈ [0, 1) as ε → 0.

B∗: There exists δ > 0 such that:

(a) lim sup0≤ε→0 φ(ε)(δ) < ∞.
(b) φ(0)(δ) > 1.

5

The solution of the following characteristic equation plays a crucial role

in what follows:

φ(ε)(ρ) = 1.

(2.1)

Our ﬁrst lemma gives some basic properties for the solution of Equation

(2.1). The proof can be found in Silvestrov and Petersson (2013).

Lemma 2.1. If conditions A∗ and B∗ hold, then there exists a unique non-
negative solution ρ(ε) of the characteristic equation (2.1) for suﬃciently small
ε. Moreover, we have ρ(ε) → ρ(0) < δ as ε → 0.

The root ρ(ε) of the characteristic equation is only given as the solution
of a non-linear equation. In order to give a more detailed description of the
asymptotic behaviour of ρ(ε) as ε → 0 we can construct an asymptotic ex-
pansion. This requires some perturbation conditions on the following mixed
power-exponential moment generating functions:

φ(ε)(ρ, r) =

∞Xn=0

Note that φ(ε)(ρ, 0) = φ(ε)(ρ).

nreρnf (ε)(n), ρ ∈ R, r = 0, 1, . . .

It follows from condition B∗ that there exist δ > 0 and ε0 > 0 such that
φ(ε)(δ) < ∞ for all ε ≤ ε0. Using this, we get for all ρ < δ, r = 0, 1, . . . , and
ε ≤ ε0 that

φ(ε)(ρ, r) ≤(cid:18)sup

n≥0

nre−(δ−ρ)n(cid:19) φ(ε)(δ) < ∞.

Let us now introduce our perturbation condition:

C∗: φ(ε)(ρ(0), r) = φ(0)(ρ(0), r) + a1,rε + · · · + ak−r,rεk−r + o(εk−r), for r =

0, . . . , k, where |an,r| < ∞, n = 1, . . . , k − r, r = 0, . . . , k.

For convenience we denote a0,r = φ(0)(ρ(0), r), for r = 0, . . . , k.

In order to apply the theory of perturbed renewal equations, we also need

the following condition:

D∗: There exists γ > 0 such that

lim sup
0≤ε→0

∞Xn=0

e(ρ(0)+γ)nq(ε)(n, X ) < ∞.

Furthermore, we deﬁne

Γ0 = {A ∈ Γ : q(ε)(n, A) → q(0)(n, A) as ε → 0, n = 0, 1, . . .}

6

eπ(0)(A) = P∞
P∞

n=0 eρ(0)nq(0)(n, A)
n=0 neρ(0)nf (0)(n)

.

and

Our ﬁrst theorem shows how we can construct an asymptotic expansion
for the root of the characteristic equation based on the coeﬃcients given in
condition C∗ and how this yields asymptotic exponential expansions for the
probabilities P (ε)(n, A), A ∈ Γ0. This result is proved in Silvestrov, Petersson
(2013) for a general renewal equation under slightly diﬀerent conditions. In
the following proof we show that the conditions in the present paper are
suﬃcient in order to apply this result to prove Theorem 2.1.

Theorem 2.1. Assume that conditions A∗, B∗, and C∗ hold.

(i) Then, the root ρ(ε) of the characteristic equation (2.1) has the asymp-

totic expansion

ρ(ε) = ρ(0) + c1ε + · · · + ckεk + o(εk),

where c1 = −a1,0/a0,1 and for n = 2, . . . , k,

cn = −

1

an−q,1cq

a0,1 an,0 +
n−1Xq=1
nXm=2
nXq=m
an−q,m · Xn1,...,nq−1∈Dm,q

+

p

cnp

np!!,

q−1Yp=1

with Dm,q being the set of all non-negative integer solutions to the sys-
tem

n1 + · · · + nq−1 = m,

n1 + · · · + (q − 1)nq−1 = q.

(ii) If, in addition, condition D∗ holds, then for any non-negative integer
valued function n(ε) → ∞ as ε → 0 in such a way that εrn(ε) → λr ∈
[0, ∞) for some 1 ≤ r ≤ k, we have

P (ε)(n(ε), A)

exp(−(ρ(0) + c1ε + · · · + cr−1εr−1)n(ε))

→ eπ(0)(A)

eλrcr

as ε → 0, A ∈ Γ0.

Proof. It follows directly from a result given in Silvestrov and Petersson
(2013) that part (i) holds. Furthermore, it also follows from this result that
part (ii) holds for any A ∈ Γ satisfying the following statements:

(α) lim sup0≤ε→0 |q(ε)(n, A)| < ∞, for all n = 0, 1, . . .

7

n=0 eρ(ε)nq(ε)(n, A) →P∞

(β) P∞
(γ) lim sup0≤ε→0P∞

n=0 eρ(0)nq(0)(n, A), as ε → 0.

n=0 e(ρ(0)+γ)n|q(ε)(n, A)| < ∞, for some γ > 0.

Since we always have 0 ≤ q(ε)(n, A) ≤ 1, it follows that statement (α)
holds for any A ∈ Γ. Also (γ) holds for any A ∈ Γ. This follows from
condition D∗ since 0 ≤ q(ε)(n, A) ≤ q(ε)(n, X ).

Let us ﬁnally show that (β) holds for any A ∈ Γ0.
It follows from Lemma 2.1 that for every β > 0, we have ρ(ε) ≤ ρ(0) + β
for suﬃciently small ε. Let us choose β such that 0 < β < γ, where γ is the
value from condition D∗. Then,

eρ(ε)nq(ε)(n, A)

lim
N→∞

lim sup
0≤ε→0

∞Xn=N +1

≤ lim
N→∞

lim sup
0≤ε→0

∞Xn=N +1

e(ρ(0)+β)nq(ε)(n, X )

(2.2)

e−(γ−β)(N +1) lim sup

0≤ε→0

≤ lim
N→∞

e(ρ(0)+γ)nq(ε)(n, X )! = 0.

∞Xn=0

It now follows from (2.2), Lemma 2.1, and the deﬁnition of Γ0 that for

any A ∈ Γ0,

lim
ε→0

∞Xn=0

eρ(ε)nq(ε)(n, A) = lim

N→∞

lim
ε→0

NXn=0

eρ(ε)nq(ε)(n, A) =

eρ(0)nq(0)(n, A).

∞Xn=0

3 Perturbed Semi-Markov Processes

In this section we deﬁne perturbed discrete time semi-Markov processes.

n , κ(ε)

For every ε ≥ 0, let (η(ε)

n ), n = 0, 1, . . . , be a discrete time Markov
chain on the state space (X, N), where X = {0, 1, . . . , N} and N = {1, 2, . . .}.
We assume that the Markov chain is homogeneous in time and that the tran-
sition probabilities do not depend on the current value of the second compo-
nent. Thus, the process (η(ε)
n ) is characterized by an initial distribution
p(ε)
i = P{η(ε)

0 = i}, i ∈ X, and transition probabilities

n , κ(ε)

Q(ε)

ij (k) = P{η(ε)

n+1 = j, κ(ε)

n+1 = k | η(ε)

n = i}, i, j ∈ X, k ∈ N.

8

Let τ (ε)(0) = 0 and τ (ε)(n) = κ(ε)

for n ≥ 1. Furthermore,
let ν(ε)(n) = max{k ≥ 0 : τ (ε)(k) ≤ n} for n ≥ 0. The semi-Markov process
associated with the Markov chain (η(ε)

1 + · · · + κ(ε)

n ) is deﬁned by

n , κ(ε)

n

ξ(ε)(n) = η(ε)

ν(ε)(n), n = 0, 1, . . .

For the semi-Markov process ξ(ε)(n), we have that κ(ε)

are the times
between successive moments of jumps, τ (ε)(n) are the moments of the jumps,
and ν(ε)(n) are the number of jumps in the interval [0, n].

n

Since the transition probabilities of the Markov chain (η(ε)

n ) do not
depend on the current value of the second component, it follows that η(ε)
is
itself a (homogeneous) Markov chain. Its transition probabilities are given
by

n , κ(ε)

n

p(ε)
ij =

Q(ε)

ij (k) = P{η(ε)

n+1 = j | η(ε)

n = i}, i, j ∈ X,

∞Xk=1

and it is called an embedded Markov chain for the corresponding semi-
Markov process.

It is sometimes convenient to write the transition probabilities of the

Markov chain (η(ε)

n , κ(ε)

n ) as
ij (k) = p(ε)

Q(ε)

ij f (ε)

ij (k), i, j ∈ X, k ∈ N,

where

f (ε)
ij (k) = P{κ(ε)

n+1 = k | η(ε)

n = i, η(ε)

n+1 = j}

are the distributions of transition times.

n = j} and let µ(ε)

Let us also deﬁne random variables for ﬁrst hitting times. Let ν(ε)

j =
min{n ≥ 1 : η(ε)
j ). Then, ν(ε)
is the ﬁrst hitting
time of the embedded Markov chain into state j and µ(ε)
is the ﬁrst hitting
time of the semi-Markov process into state j. Note that ν(ε)
are
both possibly improper random variables taking values in the set N ∪ {∞}.
Throughout the paper, we will use the notation

j = τ (ε)(ν(ε)

and µ(ε)

j

j

j

j

ij (n) = Pi{µ(ε)
g(ε)

j = n, ν(ε)

0 > ν(ε)

j }, i, j ∈ X, n = 0, 1, . . . ,

and

j }, i, j ∈ X.
Here, and in what follows, we write Pi(A) = P(A | η(ε)
Corresponding notation for conditional expectations will also be used.

0 = i) for any event A.

ij = Pi{ν(ε)
g(ε)

0 > ν(ε)

In order to consider the semi-Markov process ξ(ε)(n), for ε > 0, as a
perturbation of the semi-Markov process ξ(0)(n), the following continuity
condition will be used:

9

A: (a) p(ε)
(b) f (ε)

ij → p(0)
ij (n) → f (0)

ij as ε → 0, for all i 6= 0, j ∈ X.

ij (n) as ε → 0, for all i 6= 0, j ∈ X, n ∈ N.

Furthermore, we will assume that {1, . . . , N} is a communicating class of
states for suﬃciently small ε. This is implied by condition A together with
the following condition:

B: g(0)

ij > 0, for all i, j 6= 0.

Transitions to state 0 may, or may not be possible, both for the limiting
process and the perturbed process.

4 Moments of First Hitting Times

In this section we consider moment generating functions of ﬁrst hitting times.
First, a system of linear equations for these moment generating functions are
derived and then, a necessary and suﬃcient condition for them to be ﬁnite
is given.

Moment generating functions of ﬁrst hitting times are deﬁned by

φ(ε)
ij (ρ) = Eieρµ

j χ(ν(ε)

0 > ν(ε)

j ), ρ ∈ R, i, j ∈ X.

(ε)

Alternatively, this can be written as

φ(ε)
ij (ρ) =

∞Xn=0

eρng(ε)

ij (n), ρ ∈ R, i, j ∈ X.

We also deﬁne moment generating functions for transition probabilities:

p(ε)
ij (ρ) =

∞Xn=0

eρnQ(ε)

ij (n), ρ ∈ R, i, j ∈ X.

By conditioning on (η(ε)

1 , κ(ε)

1 ) we get for any i, j 6= 0,

j χ(ν(ε)

0 > ν(ε)

j ) | η(ε)

1 = l, κ(ε)

1 = k)Q(ε)

il (k)

Eleρ(k+µ

(ε)
j

)χ(ν(ε)

0 > ν(ε)

j )Q(ε)

il (k)

(4.1)

φ(ε)
ij (ρ) =

=

(ε)

Ei(eρµ

∞Xk=1
NXl=0
∞Xk=1
eρkQij(k) +Xl6=0,j
ij (ρ) +Xl6=0,j

∞Xk=1

= p(ε)

p(ε)
il (ρ)φ(ε)

lj (ρ).

10

Throughout the paper we will use the convention 0 · ∞ = 0. With this
convention, relation (4.1) holds for all ρ ∈ R and i, j 6= 0, even in the case
where some of the moment generating functions involved take inﬁnite values.
In this case relation (4.1) may take the form ∞ = ∞.

In what follows, it will sometimes be more convenient to work with ma-

trices. For each j 6= 0, we deﬁne column vectors

Φ(ε)

p(ε)

j (ρ) =hφ(ε)
j (ρ) =hp(ε)
ik (ρ) =(cid:26) p(ε)

0

jp(ε)

1j (ρ) φ(ε)

2j (ρ)

· · · φ(ε)

1j (ρ) p(ε)

2j (ρ)

· · ·

p(ε)

N j(ρ)iT
N j(ρ)iT

,

,

(4.2)

(4.3)

ik (ρ)

i = 1, . . . , N, k 6= j,
i = 1, . . . , N, k = j.

and N × N matrices jP(ε)(ρ) = kjp(ε)

ik (ρ)k where the elements are given by

(4.4)

(4.5)

Using (4.2), (4.3), and (4.4), we can write (4.1) in matrix notation:

Φ(ε)
j (ρ) = p(ε)

j (ρ) + jP(ε)(ρ)Φ(ε)

j (ρ), j 6= 0.

The vectors and matrices above are allowed to have entries with the value ∞.
By remarks given above, this means that relation (4.5) holds for all ρ ∈ R.
j (ρ) of

We will now derive an alternative representation for the vector Φ(ε)

moment generating functions.

Let us for each j 6= 0 deﬁne an N × N matrix valued function jA(ε)(ρ) =
ik (ρ)k by

kja(ε)

jA(ε)(ρ) = I + jP(ε)(ρ) + (jP(ε)(ρ))2 + · · · , ρ ∈ R.

(4.6)

Since all elements of the matrices on the right hand side are non-negative,
it follows that jA(ε)(ρ) is well deﬁned and has elements that take values in
the set [0, ∞]. As will be shown next, the elements of jA(ε)(ρ) can be given
a probabilistic interpretation.

Let j 6= 0 be ﬁxed. We deﬁne random variables by

δ(ε)
jk (ρ) =

∞Xn=0

eρτ (ε)(n)χ(ν(ε)

0 ∧ ν(ε)

j > n, η(ε)

n = k), k 6= 0.

(4.7)

Notice that δ(ε)

jj (ρ) = χ(η(ε)

0 = j).

11

For n = 1, 2, . . . , we have

Eieρτ (ε)(n)χ(ν(ε)

0 ∧ ν(ε)

j > n, η(ε)

n = k)

i1,...,in−16=0,j

= Xi0=i; in=k;
= Xi0=i; in=k;

i1,...,in−16=0,j

Ei(eρτ (ε)(n) | η(ε)

1 = i1, . . . , η(ε)

n = in)

p(ε)
im−1im(ρ), i 6= 0, k 6= 0, j.

nYm=1

p(ε)
im−1im

nYm=1

(4.8)

From (4.6), (4.7), and (4.8) it follows that

ja(ε)

ik (ρ) = Eiδ(ε)

jk (ρ), i, k 6= 0.

Let us now derive an alternative formula for Φ(ε)
By deﬁnition we have

j (ρ).

φ(ε)
ij (ρ) = Eieρµ

j χ(ν(ε)

0 > ν(ε)

j ), i, j 6= 0.

(ε)

The indicator function can be written as

χ(ν(ε)

0 > ν(ε)

j ) = χ(η(ε)

1 = j)

+

∞Xn=1 Xk6=0,j

χ(ν(ε)

0 ∧ ν(ε)

j > n, η(ε)

n = k, η(ε)

n+1 = j).

Note that for all i, j 6= 0,

(ε)

Eieρµ

j χ(η(ε)

1 = j) = Ei(eρκ

(ε)
1

| η(ε)

1 = j)p(ε)

ij = p(ε)

ij (ρ)

and

(ε)

Eieρµ

j χ(ν(ε)

0 ∧ ν(ε)

j > n, η(ε)

n = k, η(ε)

n+1 = j)

Eieρτ (ε)(n)χ(ν(ε)

0 ∧ ν(ε)

j > n, η(ε)

n = k)p(ε)

kj (ρ).

From (4.7) and (4.10)–(4.13) it follows that for all i, j 6= 0,

φ(ε)
ij (ρ) =

Eieρτ (ε)(n)χ(ν(ε)

0 ∧ ν(ε)

j > n, η(ε)

n = k)p(ε)

kj (ρ)

∞Xn=1 Xk6=0,j

=

∞Xn=1 Xk6=0,j

∞Xn=0Xk6=0
=Xk6=0

p(ε)
kj (ρ)Eiδ(ε)

jk (ρ).

12

(4.9)

(4.10)

(4.11)

(4.12)

(4.13)

(4.14)

Now using (4.9) we can write (4.14) in matrix notation:

Φ(ε)
j (ρ) = jA(ε)(ρ)p(ε)

j (ρ), ρ ∈ R, j 6= 0.

(4.15)

This representation will now be used to prove the following lemma which
gives a necessary and suﬃcient condition for Φ(ε)

j (ρ) to be ﬁnite.

Lemma 4.1. Assume that for some ε ≥ 0 we have g(ε)
Then Φ(ε)
matrix (I − jP(ε)(ρ))−1 exists.

j (ρ) < ∞ if and only if p(ε)

ij > 0, for all i, j 6= 0.
j (ρ) < ∞, jP(ε)(ρ) < ∞, and the inverse

Proof. Let us ﬁrst assume that Φ(ε)

j (ρ) < ∞.

Since g(ε)

ij > 0 for all i, j 6= 0, it follows from (4.15) that jA(ε)(ρ) and
p(ε)
j (ρ) are ﬁnite. Moreover, it follows from the deﬁnition of jA(ε)(ρ) that
jP(ε)(ρ) < ∞ if jA(ε)(ρ) < ∞, so we have

p(ε)
j (ρ), jP(ε)(ρ), jA(ε)(ρ) < ∞.

(4.16)

The deﬁnition of jA(ε)(ρ) also yields

jA(ε)(ρ) = I + jP(ε)(ρ)(cid:16)I + jP(ε)(ρ) + (jP(ε)(ρ))2 + · · ·(cid:17)

= I + jP(ε)(ρ)jA(ε)(ρ).

(4.17)

It follows from (4.16) that we can rewrite (4.17) as

I = (I − jP(ε)(ρ))jA(ε)(ρ).

This means that (I − jP(ε)(ρ)) has an inverse matrix given by jA(ε)(ρ).

Now assume that p(ε)

j (ρ) < ∞, jP(ε)(ρ) < ∞, and that the inverse matrix

(I − jP(ε)(ρ))−1 exists.

First note that then the following relation holds:

(I − jP(ε)(ρ))−1 = I + jP(ε)(ρ)(I − jP(ε)(ρ))−1.

(4.18)

Iterating Equation (4.18) gives for n = 1, 2, . . . ,

(I − jP(ε)(ρ))−1 = I + jP(ε)(ρ) + · · · + (jP(ε)(ρ))n

+ (jP(ε)(ρ))n+1(I − jP(ε)(ρ))−1.

(4.19)

Since (I − jP(ε)(ρ))−1 < ∞ it follows from (4.19) that we necessarily have

(jP(ε)(ρ))n+1(I − jP(ε)(ρ))−1 → 0, as n → ∞.

(4.20)

13

Letting n → ∞ in (4.19) and using (4.20), it follows that

jA(ε)(ρ) = (I − jP(ε)(ρ))−1 < ∞.

(4.21)

From (4.15) and (4.21) we conclude that Φ(ε)

j (ρ) < ∞.

We supplement Lemma 4.1 with a corresponding result for the moment

generating functions

ij (ρ) = Eieρµ

(ε)

0 χ(ν(ε)

0 < ν(ε)

j ), ρ ∈ R, i, j 6= 0.

Similar calculations as above show that we have the representation

eφ(ε)

j (ρ) = jA(ε)(ρ)p(ε)

0 (ρ), ρ ∈ R, j 6= 0,

eΦ(ε)
j (ρ) =heφ(ε)
eΦ(ε)
0 (ρ) =hp(ε)

p(ε)

2j (ρ)

1j (ρ) eφ(ε)

10 (ρ) p(ε)

20 (ρ)

. . . eφ(ε)

. . . p(ε)

N j(ρ)iT
N 0(ρ)iT

.

where

and

j (ρ)
to be ﬁnite. The proof is analogous to the proof of Lemma 4.1 and is therefore
omitted.

The following lemma gives a necessary and suﬃcient condition foreΦ(ε)

Lemma 4.2. Assume that for some ε ≥ 0 we have g(ε)

ij > 0, for all i, j 6= 0.
0 (ρ) < ∞, jP(ε)(ρ) < ∞, and the inverse

j (ρ) < ∞ if and only if p(ε)

matrix (I − jP(ε)(ρ))−1 exists.

Then eΦ(ε)

5 Solidarity Properties for Moments of First

Hitting Times

In this section we ﬁrst present a condition of Cram´er type for the distribu-
tions of transition times. Then, a solidarity lemma for moment generating
functions of ﬁrst hitting times is proved which motivates the speciﬁc form of
this condition.

We deﬁne moment generating functions for transition times by

ψ(ε)

ij (ρ) =

∞Xn=0

eρnf (ε)

ij (n), ρ ∈ R, i, j ∈ X.

Notice that p(ε)

ij (ρ) = p(ε)

ij ψ(ε)

ij (ρ).

14

For semi-Markov processes it is natural to formulate the Cram´er type

condition corresponding to B∗ in terms of moments of transition times:

lim sup
0≤ε→0

ψ(ε)

ij (β) < ∞, i 6= 0, j ∈ X, for some β > 0.

(5.1)

It can be shown that relation (5.1) together with conditions A and B
imply that part (a) of condition B∗ holds for the moment generating functions
φ(ε)
ii (ρ). However, it need not be that part (b) of condition B∗ holds. In order
to guarantee this, we will use the following condition:

C: There exists β > 0 such that:

(a) lim sup0≤ε→0 ψ(ε)
(b) φ(0)

ii (βi) > 1, for some i 6= 0 and βi ≤ β.

ij (β) < ∞, for all i 6= 0, j ∈ X.

Let us introduce the following moment generating functions:

kφ(ε)

ij (ρ) = Eieρµ

(ε)

j χ(ν(ε)

0 ∧ ν(ε)

k > ν(ε)

j ), ρ ∈ R, i, j, k ∈ X.

Before giving the solidarity lemma, we ﬁrst prove an auxiliary lemma

which gives a connection between φ(ε)

ii (ρ) and φ(ε)

jj (ρ).

Lemma 5.1. Let i 6= 0 be ﬁxed. Assume that we for some ε ≥ 0 and ρ ∈ R
have:

(α) g(ε)

kj > 0, for all k, j 6= 0.

(β) φ(ε)

ii (ρ) ≤ 1.

Then, the following relation holds for all j 6= i:

(1 − φ(ε)

ii (ρ))(1 − iφ(ε)

jj (ρ)) = (1 − φ(ε)

jj (ρ))(1 − jφ(ε)

ii (ρ)).

(5.2)

Proof. By using the regenerative property of the semi-Markov process we can
for any j 6= 0, i write the following relations for moment generating functions:

φ(ε)
ii (ρ) = jφ(ε)

ii (ρ) + iφ(ε)

ij (ρ)φ(ε)

ji (ρ),

ji (ρ) = jφ(ε)
φ(ε)
jj (ρ) = iφ(ε)
φ(ε)
φ(ε)
ij (ρ) = iφ(ε)

ji (ρ) + iφ(ε)
jj (ρ) + jφ(ε)
ij (ρ) + jφ(ε)

jj (ρ)φ(ε)
ji (ρ)φ(ε)
ii (ρ)φ(ε)

ij (ρ).

ji (ρ),

ij (ρ),

15

(5.3)

(5.4)

(5.5)

(5.6)

Recall the we use the convention 0 · ∞ = 0, so relations (5.3)–(5.6) hold for
all ρ ∈ R.

It follows from (α) that

φ(ε)
ii (ρ), φ(ε)

ij (ρ), φ(ε)

ji (ρ), φ(ε)

jj (ρ), iφ(ε)

ij (ρ), jφ(ε)

ji (ρ) ∈ (0, ∞].

From (β), (5.3), and (5.7) we can conclude that

φ(ε)
ii (ρ), φ(ε)

ji (ρ), iφ(ε)

ij (ρ), jφ(ε)

ii (ρ) < ∞.

Furthermore, it follows from, (5.4), (5.7), and (5.8) that

iφ(ε)

jj (ρ), jφ(ε)

ji (ρ) < ∞.

(5.7)

(5.8)

(5.9)

Thus, all generating functions in Equations (5.3) and (5.4) are ﬁnite under
conditions (α) and (β). However, it is not immediate that also φ(ε)
ij (ρ) and
φ(ε)
jj (ρ) are ﬁnite. In order to prove this, let us consider random variables for
successive return times. We deﬁne the the n-th return to a state j for the
embedded Markov chain by ν(ε)

j (0) = 0 and

ν(ε)
j (n) = min{k > ν(ε)

j (n − 1) : η(ε)

k = j}, n = 1, 2, . . .

Corresponding return times for the semi-Markov process are deﬁned by

µ(ε)
j (n) = τ (ε)(ν(ε)

j (n)), n = 0, 1, . . .

Using the variables for return times, we can write

χ(ν(ε)

0 > ν(ε)

j ) = χ(ν(ε)

0 ∧ ν(ε)

i > ν(ε)
j )

χ(ν(ε)

0 ∧ ν(ε)

j > ν(ε)

i (n), ν(ε)

0 ∧ ν(ε)

i (n + 1) > ν(ε)
j ).

(5.10)

+

∞Xn=1

For n = 1, 2, . . . , it follows from the regenerative property of the semi-

Markov process that

(ε)

Eieρµ

j χ(ν(ε)

0 ∧ ν(ε)

j > ν(ε)

= Eieρµ

(ε)
i

(n)χ(ν(ε)

i (n), ν(ε)
j > ν(ε)

0 ∧ ν(ε)
i (n))Eieρµ

i (n + 1) > ν(ε)
j )
0 ∧ ν(ε)
j χ(ν(ε)

0 ∧ ν(ε)

(ε)

i > ν(ε)
j ).

(5.11)

Using (5.10) and (5.11) we obtain

φ(ε)
ij (ρ) = iφ(ε)

ij (ρ) +

∞Xn=1

16

(jφ(ε)

ii (ρ))n

iφ(ε)

ij (ρ).

(5.12)

It follows from (5.3), (5.7), (5.8), and (β) that jφ(ε)

ii (ρ) < 1. Using (5.8),
ij (ρ) < ∞. Then, we can use (5.5),

(5.12), and jφ(ε)
(5.9), and φ(ε)

ii (ρ) < 1 it follows that φ(ε)
ij (ρ) < ∞ to conclude that φ(ε)

jj (ρ) < ∞.

It has now been shown that all generating functions in (5.3)–(5.6) are

ﬁnite and these relations can now be used to prove that (5.2) holds.

We can rewrite (5.4) as

φ(ε)
ji (ρ)(1 − iφ(ε)

jj (ρ)) = jφ(ε)

ji (ρ),

Multiplying (5.3) by (1 − iφ(ε)

jj (ρ)) and using (5.13) we get

φ(ε)
ii (ρ)(1 − iφ(ε)

jj (ρ)) = jφ(ε)

ii (ρ)(1 − iφ(ε)

jj (ρ)) + iφ(ε)

ij (ρ)jφ(ε)

ji (ρ).

(5.13)

(5.14)

Subtracting (1 − iφ(ε)
yield

jj (ρ)) from both sides in (5.14) and then changing signs

(1 − φ(ε)

ii (ρ))(1 − iφ(ε)
= (1 − jφ(ε)

jj (ρ))

ii (ρ))(1 − iφ(ε)

jj (ρ)) − iφ(ε)

ij (ρ)jφ(ε)

ji (ρ).

Similarly, using (5.5) and (5.6) we obtain

(1 − φ(ε)

jj (ρ))(1 − jφ(ε)
= (1 − iφ(ε)

ii (ρ))

jj (ρ))(1 − jφ(ε)

ii (ρ)) − jφ(ε)

ji (ρ)iφ(ε)

ij (ρ).

(5.15)

(5.16)

Relation (5.2) now follows from (5.15) and (5.16).

The next lemma is essential for the proof of our main result. The form
of part (b) of condition C implies that the results of this lemma can be
considered as solidarity properties for moments of ﬁrst hitting times.

Lemma 5.2. Assume that conditions A, B, and C hold. Let i 6= 0 be
the state and 0 < βi ≤ β the number in condition C for which we have
φ(0)
ii (βi) > 1. Then:

(i) There exists ρ′ ∈ [0, βi) such that φ(0)

jj (ρ′) = 1 for any j 6= 0.

(ii) For any j 6= 0, there exists βj ∈ (ρ′, βi] such that φ(0)

jj (βj) > 1 and

φ(0)
kj (βj) < ∞ for all k 6= 0.

(iii) There exists δ ∈ (0, β] such that φ(0)

jj (δ) > 1, j 6= 0 and φ(0)

kj (δ) < ∞,

k, j 6= 0.

17

(iv) There exists ε0 > 0 such that for all ε ≤ ε0 we have φ(ε)

jj (δ) > 1, j 6= 0

and φ(ε)

kj (δ) < ∞, k, j 6= 0.

Proof. It follows from conditions B and C that φ(0)
strictly increasing for ρ ∈ [0, βi]. Moreover, φ(0)
and φ(0)
such that

ii (ρ) is continuous and
i } ≤ 1
ii (βi) > 1. From this it follows that there exists (a unique) ρ′ ∈ [0, βi)

ii (0) = Pi{ν(0)

0 > ν(0)

(5.17)

(5.18)

(5.19)

(5.20)

φ(0)
ii (ρ′) = 1.

Now, for any j 6= 0, i we can write

φ(0)
ii (ρ′) = jφ(0)

ii (ρ′) + iφ(0)

ij (ρ′)φ(0)

ji (ρ′).

We also notice that under condition B,

iφ(0)

ij (ρ′), φ(0)

ji (ρ′) > 0.

It follows from (5.17), (5.18), and (5.19) that

jφ(0)

ii (ρ′) < 1.

Applying Lemma 5.1 with ε = 0 and ρ = ρ′, and using (5.17) we get

(1 − φ(0)

jj (ρ′))(1 − jφ(0)

ii (ρ′)) = 0.

(5.21)

From (5.20) and (5.21) we conclude that φ(0)

jj (ρ′) = 1 and this proves part

(i) of the lemma.

We now prove part (ii).
Let j 6= 0 be arbitrary. It follows from part (i) that there exists ρ′ ∈ [0, βi)

such that φ(0)

jj (ρ′) = 1. For any k 6= 0, j we have

φ(0)
jj (ρ′) = kφ(0)

jj (ρ′) + jφ(0)

jk (ρ′)φ(0)

kj (ρ′).

It follows from (5.22), condition B, and φ(0)

jj (ρ′) = 1 that

φ(0)
kj (ρ′) < ∞, k 6= 0.

(5.22)

(5.23)

Using (5.23) we can apply Lemma 4.1 to conclude that det(I−jP(0)(ρ′)) 6=
0. Under condition C, the elements of the matrix jP(0)(ρ) are continuous
functions of ρ ∈ [0, β]. Since ρ′ < βi ≤ β, we can ﬁnd βj ∈ (ρ′, βi] such
that det(I − jP(0)(βj)) 6= 0. Furthermore, it follows from condition C that
p(0)
kj (βj) < ∞ for all k, j 6= 0, so by Lemma 4.1 we get

φ(0)
kj (βj) < ∞, k 6= 0.

18

Also, since ρ′ < βj and φ(0)
the proof of part (ii).

jj (ρ′) = 1, we have φ(0)

jj (βj) > 1 and this completes

If we deﬁne δ = min{βj : j 6= 0}, part (iii) follows from parts (i) and (ii).
Finally, let us prove part (iv).
By Equation (4.5) we have that the vector Φ(ε)

j (δ) satisﬁes the following

system of linear equations:

j (δ) = p(ε)
Φ(ε)

j (δ) + jP(ε)(δ)Φ(ε)

j (δ).

From part (iii) and Lemma 4.1 it follows that

det(I − jP(0)(δ)) 6= 0, j 6= 0.

From conditions A and C we get

(5.24)

(5.25)

p(ε)
kj (δ) → p(0)

kj (δ) < ∞, as ε → 0, k, j 6= 0.

(5.26)

It follows from (5.25) and (5.26) that we can ﬁnd ε1 > 0 such that for all

j 6= 0 and ε ≤ ε1,

det(I − jP(ε)(δ)) 6= 0, p(ε)

j (δ) < ∞,

jP(ε)(δ) < ∞.

(5.27)

From (5.27) and Lemma 4.1 we conclude that for any j 6= 0 and ε ≤ ε1 it
j (δ) is the unique solution to the

j (δ) < ∞ and, moreover, Φ(ε)

holds that Φ(ε)
system of linear equations (5.24), so we can write

Φ(ε)
j (δ) = (I − jP(ε)(δ))−1p(ε)

j (δ), j 6= 0.

(5.28)

j (δ) as ε → 0. In particular, for any j 6= 0 we have φ(ε)

Furthermore, it follows from (5.26) and (5.28) that for any j 6= 0, we have
j (δ) → Φ(0)
Φ(ε)
jj (δ) →
φ(0)
jj (δ) as ε → 0 and since φ(0)
jj (δ) > 1, j 6= 0, this means that we can ﬁnd
ε2 > 0 such that φ(ε)
jj (δ) > 1 for all j 6= 0 and ε ≤ ε2. It follows that with
ε0 = min{ε1, ε2}, the claims of part (iv) hold and this concludes the proof of
Lemma 5.2.

6 Power Series Expansions for Moments of

First Hitting Times

In this section it is shown how mixed power-exponential moments for ﬁrst
hitting times can be expanded in power series with respect to the pertur-
bation parameter. We ﬁrst derive recursive systems of linear equations for

19

these moments. Then, some properties of asymptotic matrix expansions are
presented. Finally, we construct the desired asymptotic expansions.

Mixed power-exponential moment generating functions of ﬁrst hitting

times are deﬁned by

φ(ε)
ij (ρ, r) = Ei(µ(ε)

j )reρµ

j χ(ν(ε)

0 > ν(ε)

(ε)

j ), ρ ∈ R, r = 0, 1, . . . , i, j ∈ X.

Alternatively, this can be written as

φ(ε)
ij (ρ, r) =

∞Xn=0

nreρng(ε)

ij (n), ρ ∈ R, r = 0, 1, . . . , i, j ∈ X.

Notice that φ(ε)

ij (ρ, 0) = φ(ε)

ij (ρ).

We also deﬁne mixed power-exponential moment generating functions for

transition probabilities:

p(ε)
ij (ρ, r) =

∞Xn=0

nreρnQ(ε)

ij (n), ρ ∈ R, r = 0, 1, . . . , i, j ∈ X.

ij (ρ, 0) = p(ε)

Note that p(ε)
p(ε)
ij ψ(ε)
Markov chain and

ij (ρ, r) where p(ε)

ij (ρ). Also note that we can write p(ε)

ij (ρ, r) =
ij are the transition probabilities for the embedded

ψ(ε)

ij (ρ, r) =

∞Xn=0

nreρnf (ε)

ij (n), ρ ∈ R, r = 0, 1, . . . , i, j ∈ X.

It follows from condition C that there exist β > 0 and ε1 > 0 such that

ψ(ε)

ij (β) < ∞.

sup
ε≤ε1

max
i6=0
j∈X

From this it follows that for all i 6= 0, j ∈ X, ε ≤ ε1, ρ < β, and r = 0, 1, . . . ,
we have

p(ε)

ij (ρ, r) ≤(cid:18)sup

n≥0

nre−(β−ρ)n(cid:19) p(ε)

ij ψ(ε)

ij (β) < ∞.

Under conditions A, B, and C, it is seen from Lemma 5.2 that there exist

δ ∈ (0, β] and ε2 > 0 such that

sup
ε≤ε2

max
i,j6=0

φ(ε)
ij (δ) < ∞.

Using this, we get for all i, j 6= 0, ε ≤ ε2, ρ < δ, and r = 0, 1, . . . ,

φ(ε)

ij (ρ, r) ≤(cid:18)sup

n≥0

nre−(δ−ρ)n(cid:19) φ(ε)

ij (δ) < ∞.

20

Recall from Section 4 that the moment generating functions of ﬁrst hitting

times satisfy the following relations:

φ(ε)
ij (ρ) = p(ε)

ij (ρ) +Xl6=0,j

p(ε)
il (ρ)φ(ε)

lj (ρ), i, j 6= 0.

(6.1)

From the discussion above it follows that for any i, j 6= 0, ε ≤ min{ε1, ε2},
ij (ρ) are arbitrarily many times dif-
ij (ρ)

and ρ < δ, the functions p(ε)
ferentiable with respect to ρ. Moreover, the derivative of order r for p(ε)
and φ(ε)

ij (ρ) and φ(ε)

ij (ρ) are given by p(ε)

ij (ρ, r), respectively.

ij (ρ, r) and φ(ε)

Diﬀerentiating both sides of relation (6.1) gives the following for all ε ≤

min{ε1, ε2} and ρ < δ:

φ(ε)
ij (ρ, r) = λ(ε)

ij (ρ, r) +Xl6=0,j

where

p(ε)
il (ρ)φ(ε)

lj (ρ, r), i, j 6= 0, r = 1, 2, . . . ,

(6.2)

λ(ε)
ij (ρ, r) = p(ε)

ij (ρ, r) +

rXm=1(cid:18) r

m(cid:19)Xl6=0,j

p(ε)
il (ρ, m)φ(ε)

lj (ρ, r − m).

(6.3)

Let us rewrite relations (6.1), (6.2), and (6.3) in matrix notation. For

each j 6= 0, we deﬁne column vectors

Φ(ε)

1j (ρ, r) φ(ε)

2j (ρ, r)

· · · φ(ε)

1j (ρ, r) λ(ε)

2j (ρ, r)

· · · λ(ε)

1j (ρ, r) p(ε)

2j (ρ, r)

· · ·

p(ε)

λ(ε)

p(ε)

j (ρ, r) =hφ(ε)
j (ρ, r) =hλ(ε)
j (ρ, r) =hp(ε)
ik (ρ, r) =(cid:26) p(ε)

jp(ε)

0

N j(ρ, r)iT
N j(ρ, r)iT
N j(ρ, r)iT

,

,

,

(6.4)

(6.5)

(6.6)

and N × N matrices jP(ε)(ρ, r) = kjp(ε)
by

ik (ρ, r)k where the elements are given

ik (ρ, r)

i = 1, . . . , N, k 6= j,
i = 1, . . . , N, k = j.

With these deﬁnitions we have

j (ρ, 0) = Φ(ε)
Φ(ε)

j (ρ), p(ε)

j (ρ, 0) = p(ε)

j (ρ), jP(ε)(ρ, 0) = jP(ε)(ρ).

Using (6.1)–(6.8), we get for r = 0,

Φ(ε)
j (ρ) = p(ε)

j (ρ) + jP(ε)(ρ)Φ(ε)

j (ρ), j 6= 0,

21

(6.7)

(6.8)

(6.9)

and for r = 1, 2, . . . ,

Φ(ε)
j (ρ, r) = λ(ε)

j (ρ, r) + jP(ε)(ρ)Φ(ε)

j (ρ, r), j 6= 0,

where

λ(ε)
j (ρ, r) = p(ε)

j (ρ, r) +

rXm=1(cid:18) r

m(cid:19)jP(ε)(ρ, m)Φ(ε)

j (ρ, r − m).

(6.10)

(6.11)

Relations (6.9), (6.10), and (6.11) allows us to calculate mixed power-
exponential moments of ﬁrst hitting times for a ﬁxed (suﬃciently small) value
of ε. In order to construct asymptotic expansions for these moments, we will
use properties of asymptotic matrix expansions, which will be presented now.
Let A(ε) be an m × n matrix valued function. Suppose that A(ε) on the

interval 0 < ε ≤ ε0 can be represented as

A(ε) = A0 + A1ε + · · · + Akεk + o(εk),

where A0, . . . , Ak are m × n matrices with real-valued elements and o(εk) is
an m × n matrix where all elements are of order o(εk). Then we say that
A(ε) has an expansion of order k.

The following lemma collects some properties for asymptotic matrix ex-
pansions that will be used. These properties are known, but we give a short
proof in order to make the paper more self-contained.

Lemma 6.1. Let A(ε) be an m × n matrix valued function which has an
expansion of order k, and let B(ε) be a p × q matrix valued function which
has an expansion of order l.

(i) If c is a real-valued constant, then C(ε) = cA(ε) has an expansion of

order k and the coeﬃcients are given by

Ci = cAi, i = 0, 1, . . . , k.

(ii) If m = p and n = q, then C(ε) = A(ε) + B(ε) has an expansion of

order k ∧ l and the coeﬃcients are given by

Ci = Ai + Bi, i = 0, 1, . . . , k ∧ l.

(iii) If n = p, then C(ε) = A(ε)B(ε) has an expansion of order k ∧ l and

the coeﬃcients are given by

Ci =

iXj=0

AjBi−j, i = 0, 1, . . . , k ∧ l.

22

(iv) If m = n and det(I − A0) 6= 0, then the inverse matrix C(ε) = (I −
A(ε))−1 exists for suﬃciently small ε and has an expansion of order k
where the coeﬃcients are given by

C0 = (I − A0)−1

and Ci = C0

AjCi−j, i = 1, . . . , k.

iXj=1

Proof. Parts (i), (ii), and (iii) are consequences of elementary algebraic re-
lations.

For the proof of part (iv) we ﬁrst note that since (I −A(ε)) → (I −A0) as
ε → 0, and det(I − A0) 6= 0, it follows that det(I − A(ε)) 6= 0 for suﬃciently
small ε. Thus, the matrix I − A(ε) has an inverse for suﬃciently small ε.
Furthermore, the elements of this inverse matrix are rational functions of the
elements of A(ε). From this it follows that (I − A(ε))−1 → (I − A0)−1, so
we have the representation

C(ε) = C0 + M0(ε),

(6.12)

where C0 = (I − A0)−1 and M0(ε) → 0 as ε → 0.

Now assume that k = 1. Then, using (6.12),

I = (I − A(ε))(I − A(ε))−1

= (I − A0 − A1ε + o(ε))(C0 + M0(ε))
= I + (I − A0)M0(ε) − (A1ε + o(ε))C0 + o(ε).

Rewriting this relation and dividing by ε > 0, we get

M0(ε)

ε

= (I − A0)−1(cid:18)A1 +

o(ε)

ε (cid:19) C0 +

o(ε)

ε

.

Letting ε tend to zero it follows that M0(ε)/ε → C0A1C0 as ε → 0. From
this and relation (6.12) we get the representation

C(ε) = C0 + C1ε + M1(ε),

where C0 = (I − A0)−1, C1 = C0A1C0 and M1(ε)/ε → 0 as ε → 0.

This proves part (iv) for k = 1.
For a general k we can prove the result by induction using the same

technique as above.

We will now use the results above to show how mixed power-exponential
moments of ﬁrst hitting times can be expanded in a power series with respect
to the perturbation parameter and how the coeﬃcients can be calculated
explicitly.

Let us introduce the following perturbation condition which is assumed

to hold for some ρ < δ, where δ is the number from Lemma 5.2:

23

D′: p(ε)

ij (ρ, r) = p(0)
ij (ρ, r)+pij[ρ, r, 1]ε+· · ·+pij[ρ, r, k −r]εk−r +o(εk−r), r =
0, . . . , k, i, j 6= 0, where |pij[ρ, r, n]| < ∞, r = 0, . . . , k, n = 1, . . . , k − r,
i, j 6= 0.

For convenience we denote pij[ρ, r, 0] = p(0)

ij (ρ, r), for r = 0, . . . , k.

To prepare for the next result, note that it follows from condition D′ that
j (ρ, r) and matrices jP(ε)(ρ, r), deﬁned by relations (6.6) and

the vectors p(ε)
(6.7), respectively, have asymptotic expansions

p(ε)
j (ρ, r) = p(0)

j (ρ, r) + pj[ρ, r, 1]ε + · · · + pj[ρ, r, k − r]εk−r + o(εk−r),

and

jP(ε)(ρ, r) = jP(0)(ρ, r) + jP[ρ, r, 1]ε + · · · + jP[ρ, r, k − r]εk−r + o(εk−r),

where the vector coeﬃcients pj[ρ, r, n] are given by

and the coeﬃcients jP[ρ, r, n] = kjpik[ρ, r, n]k are N × N matrices where the
elements are given by

pj[ρ, r, n] =(cid:2)p1j[ρ, r, n] p2j[ρ, r, n]
jpik[ρ, r, n] =(cid:26) pik[ρ, r, n]

0

· · ·

pN j[ρ, r, n](cid:3)T

,

i = 1, . . . , N, k 6= j,
i = 1, . . . , N, k = j.

The following theorem is an essential tool for the proof of the main result

of the present paper.

Theorem 6.1. Assume that conditions A, B, C, and D′ hold and ﬁx some
j 6= 0. Then:

(i) The inverse matrix jU(ε)(ρ) = (I − jP(ε)(ρ))−1 exists for suﬃciently

small ε and has the expansion

jU(ε)(ρ) = jU[ρ, 0] + jU[ρ, 1]ε + · · · + jU[ρ, k] + o(εk),

where

jU[ρ, n] =(cid:26) (I − jP(0)(ρ))−1
jU[ρ, 0]Pn

(ii) We have the expansion

q=1 jP[ρ, 0, q]jU[ρ, n − q] n = 1, . . . , k.

n = 0,

Φ(ε)

j (ρ) = Φj[ρ, 0, 0] + Φj[ρ, 0, 1]ε + · · · + Φj[ρ, 0, k]εk + o(εk),

where

Φj[ρ, 0, n] =( Φ(0)
Pn

j (ρ)
q=0 jU[ρ, q]pj[ρ, 0, n − q] n = 1, . . . , k.

n = 0,

(6.13)

24

(iii) For r = 1, . . . , k, we have the expansion

Φ(ε)

j (ρ, r) = Φj[ρ, r, 0] + Φj[ρ, r, 1]ε + · · · + Φj[ρ, r, k − r]εk−r + o(εk−r),

where the coeﬃcients can be calculated recursively by the formulas

Φj[ρ, r, n] =( Φ(0)
Pn

where, for s = 0, . . . , k − r,

j (ρ, r)
q=0 jU[ρ, q]λj[ρ, r, n − q], n = 1, . . . , k − r,

n = 0,

λj[ρ, r, s] = pj[ρ, r, s] +

rXm=1(cid:18) r

m(cid:19) sXq=0

jP[ρ, m, q]Φj[ρ, r − m, s − q].

Proof. First note that under conditions A, B, and C, it follows from part
(iii) of Lemma 5.2 that Φ(0)
j (ρ) < ∞, for all ρ ≤ δ. Thus, by applying Lemma
4.1 we see that the inverse matrix (I − jP(0)(ρ))−1 exists for all ρ ≤ δ. Using
this and condition D′, part (i) now follows from part (iv) of Lemma 6.1.

For the proof of part (ii) notice that it follows from Equation (6.9) and

part (i) that for suﬃciently small ε we have

Φ(ε)
j (ρ) = (I − jP(ε)(ρ))−1p(ε)

j (ρ).

(6.14)

It follows from (6.14), part (i), condition D′, and part (iii) of Lemma 6.1
that Φ(ε)
j (ρ) has an expansion of order k with coeﬃcients given by Equation
(6.13). This proves part (ii).

Now we consider Equations (6.10) and (6.11) for r = 1:

Φ(ε)
j (ρ, 1) = λ(ε)

j (ρ, 1) + jP(ε)(ρ)Φ(ε)

j (ρ, 1),

where

j (ρ, 1) = p(ε)
λ(ε)

j (ρ, 1) + jP(ε)(ρ, 1)Φ(ε)

j (ρ).

It follows from (6.15) and part (i) that for suﬃciently small ε,

Φ(ε)
j (ρ, 1) = (I − jP(ε)(ρ))−1λ(ε)

j (ρ, 1).

(6.15)

(6.16)

(6.17)

It follows from (6.16), part (ii), condition D′, and parts (ii)-(iii) of Lemma
6.1 that

λ(ε)

j (ρ, 1) = λj[ρ, 1, 0]+ λj[ρ, 1, 1]ε+· · ·+ λj[ρ, 1, k −1]εk−1 +o(εk−1), (6.18)

25

where

λj[ρ, 1, s] = pj[ρ, 1, s] +

sXq=0

jP[ρ, 1, q]Φj[ρ, 0, s − q], s = 0, . . . , k − 1. (6.19)

It now follows from (6.17), (6.18), (6.19), part (i), and part (iii) of Lemma
6.1 that Φ(ε)
j (ρ, 1) has an expansion of order k − 1 with coeﬃcients given by

Φj[ρ, 1, n] =

nXq=0

jU[ρ, q]λj[ρ, 1, n − q], n = 1, . . . , k − 1.

This proves part (iii) for r = 1.

We prove the general result by induction. Let us assume that part (iii)
holds for r = 1, . . . , u − 1, for some u ≤ k. Equations (6.10) and (6.11) give

Φ(ε)
j (ρ, u) = λ(ε)

j (ρ, u) + jP(ε)(ρ)Φ(ε)

j (ρ, u),

where

λ(ε)
j (ρ, u) = p(ε)

j (ρ, u) +

uXm=1(cid:18) u

m(cid:19)jP(ε)(ρ, m)Φ(ε)

j (ρ, u − m).

(6.20)

(6.21)

It follows from (6.20) and part (i) that for suﬃciently small ε,

Φ(ε)
j (ρ, u) = (I − jP(ε)(ρ))−1λ(ε)

j (ρ, u).

(6.22)

It follows from (6.21), part (ii), condition D′, parts (i)-(iii) of Lemma 6.1,
and the induction hypothesis that

λ(ε)

j (ρ, u) = λj[ρ, u, 0]+λj[ρ, u, 1]ε+· · ·+λj[ρ, u, k−u]εk−u+o(εk−u), (6.23)

where for s = 0, . . . , k − u,

λj[ρ, u, s] = pj[ρ, u, s] +

uXm=0(cid:18) u

m(cid:19) sXq=0

jP[ρ, m, q]Φj[ρ, u − m, s − q]. (6.24)

It now follows from (6.22), (6.23), (6.24), part (i), and part (iii) of Lemma
6.1 that Φ(ε)
j (ρ, u) has an expansion of order k − u with coeﬃcients given by

Φj[ρ, u, n] =

nXq=0

jU[ρ, q]λj[ρ, u, n − q], n = 1, . . . , k − u.

This concludes the proof of Theorem 6.1.

26

7 Solidarity Property of Periodicity

In this section we show that the periodicity of the distribution of ﬁrst return
time satisﬁes a solidarity property.

The period of g(ε)

ii (n) is deﬁned by

di = gcd{n ∈ N : g(ε)

ii (n) > 0}, i 6= 0.

In particular, di = 1 means that g(ε)

ii (n) is non-periodic.

In order to guarantee non-periodicity of g(0)

ii (n), we will assume that the

following condition holds:

E: g(0)

jj (n) is non-periodic for some j 6= 0.

It will be shown that the function g(ε)

ii (n) have the same period for all
states i 6= 0. In the proof of this result we will use the convolution operator.
For two real-valued functions f (n), n = 0, 1, . . . , and g(n), n = 0, 1, . . . , the
convolution is deﬁned by

f ∗ g(n) =

nXk=0

f (n − k)g(k), n = 0, 1, . . .

Furthermore, for a function f (n), n = 0, 1, . . . , the k-fold convolution f (∗k)(n)
is deﬁned recursively by f (∗0)(n) = χ(n = 0) and

f (∗k)(n) = f ∗ f (∗(k−1))(n), k = 1, 2, . . .

Notice that f (∗1)(n) = f (n).

Let us introduce the following notation:

kg(ε)

ij (n) = Pi{µj = n, ν(ε)

0 ∧ ν(ε)

k > ν(ε)

j }, n = 0, 1, . . . , i, j, k ∈ X.

In the proof of the following lemma we adopt a technique that is used in
the proof of a similar result for continuous time semi-Markov processes given
in C¸ inlar (1974).

Lemma 7.1. If we for some ε ≥ 0 have g(ε)
for all i, j 6= 0.

ij > 0 for all i, j 6= 0, then di = dj

Proof. Choose i, j 6= 0 arbitrarily. The conclusion is trivial if i = j so let us
assume that i 6= j.

By using the regenerative property of the semi-Markov process we can

for all n = 0, 1, . . . , write down the following relations:

g(ε)
ii (n) = jg(ε)

ii (n) + ig(ε)

ij ∗ g(ε)

ji (n),

(7.1)

27

g(ε)
ji (n) = jg(ε)
g(ε)
jj (n) = ig(ε)
ij (n) = ig(ε)
g(ε)

ji (n) + ig(ε)
jj (n) + jg(ε)
ij (n) + jg(ε)

jj ∗ g(ε)
ji ∗ g(ε)
ii ∗ g(ε)

ij (n),

ij (n).

ji (n),

Iterating Equation (7.2) and using (7.1) we get

g(ε)
ii (n) = jg(ε)

ii (n) +

ig(ε)

ij ∗ (ig(ε)

jj )(∗k) ∗ jg(ε)

ji (n)

+ ig(ε)

ij ∗ (ig(ε)

jj )(∗(m+1)) ∗ g(ε)

ji (n), m = 0, 1, . . .

Similarly, by using Equations (7.3) and (7.4) we get

(7.2)

(7.3)

(7.4)

(7.5)

(7.6)

mXk=0

mXk=0

g(ε)
jj (n) = ig(ε)

jj (n) +

jg(ε)

ji ∗ (jg(ε)

ii )(∗k) ∗ ig(ε)

ij (n)

+ jg(ε)

ji ∗ (jg(ε)

ii )(∗(m+1)) ∗ g(ε)

ij (n), m = 0, 1, . . .

Since g(ε)

ij ∗ ig(ε)

ii (n), ig(ε)

ii (n) has period di, it has all its mass concentrated on the set
It follows from (7.5) with m = 0 that the functions
diN = {di, 2di, . . .}.
ji (n) and ig(ε)
jg(ε)
ij ∗ jg(ε)
ji (n) are all concentrated on the
ij ∗ jg(ε)
set diN. Since ig(ε)
ji (n) is not identically equal to zero, it also follows
from (7.5) that ig(ε)
jj (n) concentrates on diN. It can now be concluded that
all functions on the right hand side of (7.6), except for possibly the last one,
is concentrated on diN. Using this, and that g(ε)
jj (n) is the limit of the right
hand side of (7.6) as m → ∞, we have for any n′ /∈ diN,

jj ∗ jg(ε)

g(ε)
jj (n′) = lim

m→∞

jg(ε)

ji ∗ (jg(ε)

ii )(∗(m+1)) ∗ g(ε)

ij (n′) = 0.

This means that g(ε)
jj (n) is concentrated on the set diN and we can conclude
that dj ≥ di. By using analogous arguments as above, (7.5) and (7.6) can
also be used to show that di ≥ dj. In conclusion, di = dj.

8 Exponential Expansions for Perturbed

Semi-Markov Processes

In this section we give asymptotic exponential expansions for perturbed dis-
crete time semi-Markov processes with absorption. The results are obtained
by applying corresponding results for perturbed regenerative processes given
in Section 2.

28

Our main objective is to give a detailed asymptotic analysis of the prob-

abilities

P (ε)
ij (n) = Pi{ξ(ε)(n) = j, µ(ε)

0 > n}, n = 0, 1, . . . , i, j 6= 0,

as n → ∞ and ε → 0.

Let us assume that the initial distribution of the semi-Markov process
ξ(ε)(n) is concentrated at some state i 6= 0. Then ξ(ε)(n) is a regenerative
process with regeneration times being successive return times to state i. If
state 0 is an absorbing state, these regeneration times are possibly improper
random variables. In Section 2 it was assumed that the regeneration times
were proper random variables. However, the probabilities P (ε)
ij (n), i, j 6= 0, do
not depend on the transition probabilities from state 0. This means that we
can modify these transition probabilities without aﬀecting the probabilities
ij (n), i, j 6= 0. For example, if we take Q(ε)
P (ε)
ij (n) = χ(n = 1)/(N + 1), then
return times to any ﬁxed initial state i 6= 0 can serve as proper regeneration
times. We can apply the results of Section 2 to this modiﬁed process and then
it follows that the results also hold for the process where 0 is an absorbing
state.

By using the regenerative property of the semi-Markov process at return
times to the initial state, we can for any i, j 6= 0 write the following renewal
equation:

P (ε)
ij (n) = h(ε)

ij (n) +

nXk=0

P (ε)
ij (n − k)g(ε)

ii (k), n = 0, 1, . . . ,

where

h(ε)
ij (n) = Pi{ξ(ε)(n) = j, µ(ε)

0 ∧ µ(ε)

i > n}.

It follows that µ(ε)
time for ξ(ε)(n).

0 , the ﬁrst hitting time of state 0, is a regenerative stopping

For the model of perturbed semi-Markov processes, the characteristic

equation takes the form

φ(ε)
ii (ρ) = 1.

(8.1)

It will be shown that Equation (8.1) has a unique solution ρ(ε) for suﬃciently
small ε that does not depend on i.

Furthermore, let us deﬁne

ij = P∞
eπ(0)
P∞

n=0 eρ(0)nh(0)
n=0 neρ(0)ng(0)

ij (n)
ii (n)

, i, j 6= 0.

29

ij does not
0 = ∞ almost surely, so we

depend on i. Indeed, in this case ρ(0) = 0 and µ(0)
get

It is interesting to note that in the pseudo-stationary case,eπ(0)
ij = P∞
eπ(0)
P∞
That is,eπ(0)

is the quotient of the expected number of visits to state j during
an excursion starting from state i and the expected length of this excursion
for the limiting semi-Markov process. It is known that this quantity does not
depend on state i. Moreover, in this case π(0)
ij , j = 1, . . . , N, are the
stationary probabilities for the limiting semi-Markov process.

n=0 h(0)
n=0 ng(0)

ij (n)
ii (n)

EiP∞

n=0 χ(ξ(0)(n) = j, µ(0)

i > n)

Eiµ(0)

i

=

, i, j 6= 0.

ij

j =eπ(0)

Let us formulate condition D′ for ρ = ρ(0):

D: p(ε)

ij (ρ(0), r) = p(0)
ij (ρ(0), r) + pij[ρ(0), r, 1]ε + · · · + pij[ρ(0), r, k − r]εk−r +
o(εk−r), r = 0, . . . , k, i, j 6= 0, where |pij[ρ(0), r, n]| < ∞, r = 0, . . . , k,
n = 1, . . . , k − r, i, j 6= 0.

Under conditions A–D it follows from Theorem 6.1 that we for each i 6= 0

and r = 0, . . . , k have the asymptotic expansion

φ(ε)
ii (ρ(0), r) = bi[r, 0] + bi[r, 1]ε + · · · + bi[r, k − r]εk−r + o(εk−r),

where bi[r, 0] = φ(0)
ii (ρ(0), r), r = 0, . . . , k, i 6= 0, and the coeﬃcients bi[r, n],
r = 0, . . . , k, n = 1, . . . , k − r, i 6= 0, can be calculated from the recursive
formulas given in this theorem.

We now present the main result of this paper.

Theorem 8.1. Assume that conditions A–E hold. Then:

(i) For ε suﬃciently small, there exists a unique root ρ(ε) of the character-
istic equation (8.1) which does not depend on the choice of initial state
i. Moreover, we have the asymptotic expansion

ρ(ε) = ρ(0) + c1ε + · · · + ckεk + o(εk),

where c1 = −bi[0, 1]/bi[1, 0] and for n = 2, . . . , k,

cn = −

1

bi[1, 0] bi[0, n] +
nXq=m
nXm=2

+

bi[1, n − q]cq

n−1Xq=1
bi[m, n − q] · Xn1,...,nq−1∈Dm,q

q−1Yp=1

30

p

cnp

np!!,

with Dm,q being the set of all non-negative integer solutions to the sys-
tem

n1 + · · · + nq−1 = m,

n1 + · · · + (q − 1)nq−1 = q.

(ii) For any non-negative integer valued function n(ε) → ∞ as ε → 0 in

such a way that εrn(ε) → λr ∈ [0, ∞) for some 1 ≤ r ≤ k, we have

Pi{ξ(ε)(n(ε)) = j, µ(ε)

0 > n(ε)}

exp(−(ρ(0) + c1ε + · · · + cr−1εr−1)n(ε))

ij

→ eπ(0)

eλrcr

as ε → 0, i, j 6= 0.

Proof. Throughout the proof, we let the initial state i 6= 0 be ﬁxed. It will
be shown that conditions A–E imply that conditions A∗–D∗ hold for the
functions

f (ε)(n) = g(ε)

ii (n), n = 0, 1, . . . ,

and

q(ε)(n, A) =Xj∈A

h(ε)
ij (n), n = 0, 1, . . . , A ⊆ X.

Then, Theorem 2.1 can be applied in order to prove Theorem 8.1.

Let us ﬁrst show that the function

f (n) = g(ε)

ii (n) = Pi{µ(ε)

i = n, ν(ε)

0 > ν(ε)

i }, n = 0, 1, . . .

satisﬁes condition A∗.

As was shown in Section 4, the vector of moment generating functions

Φ(ε)

i (ρ) satisﬁes the following system of linear equations:

Φ(ε)
i (ρ) = p(ε)

i (ρ) + iP(ε)(ρ)Φ(ε)

i (ρ).

(8.2)

It follows from part (iv) of Lemma 5.2 that there exist ε1 > 0 and δ > 0 such
that Φ(ε)
i (ρ) < ∞ for all ε ≤ ε1 and ρ ≤ δ. Thus, we can use Lemma 4.1 to
conclude that the system (8.2) has a unique solution for ε ≤ ε1 and ρ ≤ δ
given by

Φ(ε)
i (ρ) = (I − iP(ε)(ρ))−1p(ε)

i (ρ).

(8.3)

Using (8.3) and condition A it follows that Φ(ε)
ρ ≤ δ and in particular

i (ρ) → Φ(0)

i (ρ) as ε → 0 for

φ(ε)
ii (ρ) → φ(0)

ii (ρ) as ε → 0, ρ ≤ δ.

(8.4)

Relation (8.4) implies that for all n = 0, 1, . . . , we have g(ε)

as ε → 0. Since φ(ε)
ε → 0. Furthermore, by condition B, the function g(0)

ii , relation (8.4) also implies that g(ε)

ii (0) = g(ε)

ii (n) → g(0)
ii → g(0)

ii (n)
ii as
ii (n) is not concentrated

31

at zero and by applying Lemma 7.1 under condition E, we see that g(0)
non-periodic. Thus, the function g(ε)

ii (n) satisﬁes condition A∗.

ii (n) is

It follows from Lemma 5.2 that the moment generating function

φ(ε)(ρ) = φ(ε)

ii (ρ) =

eρng(ε)

ii (n), ρ ∈ R,

∞Xn=0

satisﬁes condition B∗.

i of the characteristic equation φ(ε)

Applying Lemma 2.1 now shows that there exists a unique non-negative
solution ρ(ε)
ii (ρ) = 1 for suﬃciently small ε,
say ε ≤ ε2. Now for any j 6= i and ε ≤ ε2 we can apply the same arguments
as in the proof of part (i) of Lemma 5.2 to see that we also have φ(ε)
i ) = 1.
Thus, it can be concluded that the root of the characteristic equation (8.1)
does not depend on the initial state i and we can drop the index and just
write ρ(ε).

jj (ρ(ε)

It follows from Theorem 6.1 that condition C∗ holds for the moments

φ(ε)(ρ(0), r) = φ(ε)

ii (ρ(0), r), r = 0, . . . , k.

Part (i) now follows by applying part (i) of Theorem 2.1.
To prove part (ii) we also need to show that the function

q(ε)(n, X) =Xj∈X

h(ε)
ij (n) = Pi{µ(ε)

0 ∧ µ(ε)

i > n}, n = 0, 1, . . . ,

satisﬁes condition D∗. Thus, we need to show that there exists γ > 0 such
that

lim sup
0≤ε→0

∞Xn=0

e(ρ(0)+γ)nPi{µ(ε)

0 ∧ µ(ε)

i > n} < ∞.

(8.5)

(8.6)

In order to do this, ﬁrst note that for any ρ 6= 0 we have

eρnPi{µ(ε)

0 ∧ µ(ε)

i > n} =

∞Xn=0

=

=

∞Xk=n+1

eρk − 1
eρ − 1

∞Xn=0
∞Xk=1

Eieρ(µ

(ε)
(ε)
0 ∧µ
i

) − 1

.

eρ − 1

eρnPi{µ(ε)

0 ∧ µ(ε)

i = k}

Pi{µ(ε)

0 ∧ µ(ε)

i = k}

By Lemma 5.2 there exist δ ∈ (0, β] and ε3 > 0 such that Φ(ε)

i (δ) < ∞,
for all ε ≤ ε3. From this, Lemma 4.1 implies that for any ε ≤ ε3, we have

32

iP(ε)(δ) < ∞ and the inverse matrix (I − iP(ε)(δ))−1 exists. Moreover, since
δ ≤ β, condition C gives that there exists ε4 > 0 such that p(ε)
0 (δ) < ∞
i (δ) < ∞ for

for ε ≤ ε4. By Lemma 4.2, it can now be concluded that eΦ(ε)

ε ≤ min{ε3, ε4}. Using this we get

ii (δ) < ∞, ε ≤ min{ε3, ε4}.

(8.7)

Eieδ(µ

(ε)
(ε)
0 ∧µ
i

) = φ(ε)

ii (δ) +eφ(ε)

It follows from Lemma 2.1 that ρ(0) < δ, so there exists γ > 0 such that

ρ(0) + γ < δ.

(8.8)

Relation (8.5) now follows from (8.6), (8.7), and (8.8).
Applying part (ii) of Theorem 2.1 now shows that part (ii) of Theorem

8.1 holds for all j 6= 0 for which we have

h(ε)
ij (n) → h(0)

ij (n) as ε → 0, n = 0, 1, . . .

(8.9)

However, under condition A, relation (8.9) holds for all j 6= 0 since it is
possible to write h(ε)
ij (n) as a ﬁnite sum where each term in the sum is a
continuous function of quantities given in condition A. This concludes the
proof of Theorem 8.1.

References

[1] Altman, E., Avrachenkov, K. E., N´u˜nez-Queija, R. (2004) Perturbation
analysis for denumerable Markov chains with application to queueing
models. Adv. Appl. Prob., 36, 839–853.

[2] Avrachenkov, K. E., Filar, J. A., Howlett, P. G. (2013) Analytic pertur-

bation theory and its applications. SIAM, Philadelphia.

[3] Avrachenkov, K. E., Haviv, M. (2004) The ﬁrst Laurent series coeﬃ-
cients for singularly perturbed stochastic matrices. Linear Algebra Appl.,
386, 243–259.

[4] Blanchet, J., Zwart, B. (2010) Asymptotic expansions of defective re-
newal equations with applications to perturbed risk models and proces-
sor sharing queues. Math. Meth. Oper. Res., 72, 311–326.

[5] Cheong, C. K. (1970) Quasi-stationary distributions in semi-Markov
processes. J. Appl. Prob., 7, 388–399. (Correction in J. Appl. Prob.,
7, 788.)

33

[6] C¸ inlar, E. (1974) Periodicity in Markov renewal theory. Adv. Appl. Prob.,

6, 61–78.

[7] Collet, P., Mart´ınez, S., San Mart´ın, J. (2013) Quasi-stationary dis-
tributions. Markov chains, diﬀusions and dynamical systems. Springer,
Heidelberg.

[8] Darroch, J. N., Seneta, E. (1965) On quasi-stationary distributions in
absorbing discrete-time ﬁnite Markov chains. J. Appl. Prob., 2, 88–100.

[9] van Doorn, E. A., Pollett, P. K. (2013) Quasi-stationary distributions

for discrete-state models. Eur. J. Oper. Res., 230, 1–14.

[10] Drozdenko, M. (2007) Weak convergence of ﬁrst-rare-event times for
semi-Markov processes. PhD Thesis, M¨alardalen University, School of
Education, Culture and Communication, V¨aster˚as.

[11] Englund, E. (2001) Nonlinearly perturbed renewal equations with ap-

plications. PhD Thesis, Ume˚a University.

[12] Englund, E., Silvestrov, D. S. (1997) Mixed large deviation and ergodic
theorems for regenerative processes with discrete time. In: Jagers, P.,
Kulldorﬀ, G., Portenko, N., Silvestrov, D. (eds) Proceedings of the Sec-
ond Scandinavian-Ukrainian Conference in Mathematical Statistics, vol.
I, Ume˚a, 1997. Also in Theory Stoch. Process., 3(19), no. 1.2, 164–176,
1997.

[13] Flaspohler, D. C., Holmes, P. T. (1972) Additional quasi-stationary dis-

tributions for semi-Markov processes. J. Appl. Prob., 9, 671–676.

[14] Gyllenberg, M., Silvestrov, D. S. (1994) Quasi-stationary distributions

of stochastic metapopulation model. J. Math. Biol., 33, 35–70.

[15] Gyllenberg, M., Silvestrov, D. S. (1999) Quasi-stationary phenomena for
semi-Markov processes. In: Janssen, J., Limnios, N. (eds) Semi-Markov
Models and Applications. Kluwer, Dordrecht, 33–60.

[16] Gyllenberg, M., Silvestrov, D. S. (2000) Cram´er-Lundberg approxima-
tion for nonlinearly perturbed risk processes. Insur. Math. Econom., 26,
75–90.

[17] Gyllenberg, M., Silvestrov, D. S. (2008) Quasi-stationary phenomena
in nonlinearly perturbed stochastic systems. De Gruyter Expositions in
Mathematics, vol. 44. Walter de Gruyter, Berlin.

34

[18] Hassin, R., Haviv, M. (1992) Mean passage times and nearly uncoupled

Markov chains. SIAM J. Discrete Math., 5(3), 386–397.

[19] Jung, B. (2013) Exit times for multivariate autoregressive processes.

Stochastic Process. Appl., 123, 3052–3063.

[20] Keilson, J. (1966) A limit theorem for passage times in ergodic regener-

ative processes. Ann. Math. Statist., 37, 866–870.

[21] Latouche, G. (1991) First passage times in nearly decomposable Markov
chains. In: Stewart, W. J. (ed) Numerical Solution of Markov Chains.
Probability: Pure and Applied, 8. Marcel Dekker, New York, 401–411.

[22] Latouche, G., Louchard, G. (1978) Return times in nearly-completely

decomposable stochastic processes. J. Appl. Prob., 15, 251–267.

[23] Ni, Y. (2011) Nonlinearly perturbed renewal equations: asymptotic re-
sults and applications. PhD Thesis, M¨alardalen University, School of
Education, Culture and Communication, V¨aster˚as.

[24] Ni, Y. (2014) Exponential asymptotical expansions for ruin probability
in a classical risk process with non-polynomial perturbations. In: Sil-
vestrov D., Martin-L¨of, A. (eds) Modern Problems in Insurance Mathe-
matics, EAA Series, Springer international, 69–93.

[25] Petersson, M. (2013) Quasi-stationary distributions for perturbed dis-
crete time regenerative processes. Teor. ˘Imovirn. Mat. Stat., 89, 140–
155. (To appear in Theory Probab. Math. Statist., 89).

[26] Petersson, M. (2014) Asymptotics of ruin probabilities for perturbed dis-
crete time risk processes. In: Silvestrov D., Martin-L¨of, A. (eds) Modern
Problems in Insurance Mathematics, EAA Series, Springer international,
95–112.

[27] Schweitzer, P. J. (1968) Perturbation theory and ﬁnite Markov chains.

J. Appl. Prob., 5, 401–413.

[28] Seneta E., Vere-Jones, D. (1966) On quasi-stationary distributions in
discrete-time Markov chains with a denumerable inﬁnity of states. J.
Appl. Prob., 3, 403–434.

[29] Silvestrov, D. S., Petersson, M. (2013) Exponential expansions for per-
turbed discrete time renewal equations. In: Frenkel, I., Karagrigoriou,
A., Lisnianski, A., Kleyner A. (eds) Applied Reliability Engineering

35

and Risk Analysis: Probabilistic Models and Statistical Inference, Wi-
ley, Chichester, 349–362.

[30] Simon, H. A., Ando, A. (1961) Aggregation of variables in dynamic

systems. Econometrica, 29, 111–138.

[31] Stewart, G. W. (1991) On the sensitivity of nearly uncoupled Markov
chains. In: Stewart, W. J. (ed) Numerical Solution of Markov Chains.
Probability: Pure and Applied, 8. Marcel Dekker, New York, 105–119.

[32] Yin, G., Zhang, Q. (1998) Continuous-time Markov Chains and applica-
tions. A singular perturbation approach. Applications of Mathematics,
37, Springer, New York.

[33] Yin, G., Zhang, Q. (2003) Discrete-time singularly perturbed Markov
chains. In: Stochastic Modelling and Optimization. Springer, New York,
1–42.

36

