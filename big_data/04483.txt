6
1
0
2

 
r
a

 

M
4
1

 
 
]
S
M

.
s
c
[
 
 

1
v
3
8
4
4
0

.

3
0
6
1
:
v
i
X
r
a

Fast calculation of inverse square root with the use of

magic constant – analytical approach

Leonid V. Moroz∗1, Cezary J. Walczyk†2, Andriy Hrynchyshyn‡1,

Vijay Holimath§3, and Jan L. Cie´sli´nski¶2

1Lviv Polytechnic National University, Department of Security Information and Technology,

2Uniwersytet w Bia lymstoku, Wydzia l Fizyki, ul. Cio lkowskiego 1L, 15-245 Bia lystok, Poland

3VividSparks IT Solutions, Hubli 580031, No. 38, BSK Layout, India

st. Kn. Romana 1/3, 79000 Lviv, Ukraine

Abstract

We present a mathematical analysis of transformations used in fast calculation
of inverse square root for single-precision ﬂoating-point numbers. Optimal values
of the so called magic constants are derived in a systematic way, minimizing either
absolute or relative errors at subsequent stages of the discussed algorithm.

Keywords: ﬂoating-point arithmetics; inverse square root; magic constant; Newton-
Raphson method

1

Introduction

Floating-point arithmetics has became wide spread in many applications such as
3D graphics, scientiﬁc computing and signal processing [1, 2, 3]. Basic operators
such as addition, subtraction, multiplication are easier to design and yield higher
performance, high throughput but advanced operators such as division, square root,
inverse square root and trigonometric functions consume more hardware, slower in
performance and slower throughput [4, 5, 6, 7, 8].

Inverse square root function is widely used in 3D graphics especially in lightning
reﬂections [9, 10, 11]. Many algorithms can be used to approximate inverse square
root functions [12, 13, 14, 15, 16]. All of these algorithms require initial seed to
approximate function. If the initial seed is accurate then iteration required for this

∗moroz lv@polynet.lviv.ua
†walcez@gmail.com
‡hrynchyshyn.a@gmail.com
§vijay.holimath@vivid-sparks.com
¶j.cieslinski@uwb.edu.pl

1

function is less time-consuming. In other words, the function requires less cycles. In
most of the case, initial seed is obtained from Look-Up Table (LUT) and the LUT
consume signiﬁcant silicon area of a chip. In this paper we present initial seed using
so called magic constant [17, 18] which does not require LUT and we then used this
magic constant to approximate inverse square root function using Newton-Raphson
method and discussed its analytical approach.

We present ﬁrst mathematically rigorous description of the fast algorithm for
computing inverse square root for single-precision IEEE Standard 754 ﬂoating-point
numbers (type ﬂoat).

1. ﬂoat InvSqrt(ﬂoat x){
ﬂoat halfnumber = 0.5f * x;
2.
int i = *(int*) &x;
3.
i = R-(i>>1);
4.
x = *(ﬂoat*)&i;
5.
x = x*(1.5f-halfnumber*x*x);
6.
x = x*(1.5f-halfnumber*x*x);
7.
return x ;
8.
9. }

This code, written in C, will be referred to as function InvSqrt. It realizes a fast
algorithm for calculation of the inverse square root. In line 3 we transfer bits of
varaible x (type ﬂoat) to variable i (type int). In line 4 we determine an initial
value (then subject to the iteration process) of the inverse square root, where R =
0x5f 3759df is a “magic constant”. In line 5 we transfer bits of a variable i (type
int) to the variable x (type ﬂoat). Lines 6 and 7 contain subsequent iterations of
the Newton-Raphson algoritm.

The algorithm InvSqrt has numerous applications, see [19, 20, 21, 22, 23]. The
most important among them is 3D computer graphics, where normalization of vec-
tors is ubiquitous. InvSqrt is characterized by a high speed, more that 3 times higher
than in computing the inverse square root using library functions. This property
is discussed in detail in [24]. The errors of the fast inverse square root algorithm
depend on the choice of R. In several theoretical papers [18, 24, 25, 26, 27] (see
also the Eberly’s monograph [9]) attempts were made to determine analytically the
optimal value (i.e. minimizing errors) of the magic constant. These attempts were
not fully successfull. In our paper we present missing mathematical description of
all steps of the fast inverse square root algorithm.

2 Preliminaries

The value of a ﬂoating-point number can be represented as:

x = (−1)sx(1 + mx)2ex,

(2.1)

where sx is the sign bit (sx = 1 for negative numbers and sx = 0 for positive
numbers), 1 + mx is normalized mantissa (or signiﬁcand), where mx ∈ h0, 1) and,
ﬁnally, ex is an integer.

2

Figure 1: The layout of a 32-bit ﬂoating-point number.

In the case of the standard IEEE-754 a ﬂoating-point number is encoded by
32 bits (Fig. 2). The ﬁrst bit corresponds to a sign, next 8 bits correspond to an
exponent ex and the last 23 bits encodes a mantissa. The fractional part of the
mantissa is represented by an integer (without a sign) Mx:

Mx = Nm mx,

where: Nm = 223,

(2.2)

and the exponent is represented by a positive value Ex resulting from the shift of
ex by a constant B (biased exponent):

Ex = ex + B,

where: B = 127.

Bits of a ﬂoating-point number can be interpreted as an integer given by:

Ix = (−1)sx(Nm Ex + Mx),

(2.3)

(2.4)

where:

(2.5)
In what follows we conﬁne ourselves to positive numbers (sx ≡ bS = 0). Then,
to a given integer Ix ∈ h0, 232 − 1i there corresponds a ﬂoating number x of the form
(2.1), where

Mx = Nm(2−exx − 1)

ex := ⌊N −1

m Ix⌋ − B ,

mx := N −1

m Ix − ⌊N −1

m Ix⌋ .

This map, denoted by f , is inverse to the map x → Ix. In other words,

f (Ix) = x .

(2.6)

(2.7)

The range of available 32-bit ﬂoating-point numbers for which we can determine

inverse square roots can be divided into 127 disjoint intervals:

63

[n=−63

x ∈

∪h22n+1, 22(n+1))
.
|
}

AII
n

An, where: An = {22n} ∪ (22n, 22n+1)
}

|

{z
Therefore, ex = 2n for x ∈ AI
exponents and signiﬁcands of y = 1/√x are given by
ey = 


{z
n ∪{22n} and ex = 2n + 1 for x ∈ AII
n . For any x ∈ An
, my = 


for x = 22n
2/√1 + mx − 1
for x ∈ AI
√2/√1 + mx − 1 for x ∈ AII

for x = 22n
−n
−n − 1 for x ∈ AI
−n − 1 for x ∈ AII

. (2.9)

(2.8)

AI
n

0

n

n

3

n

n

It is convenient to introduce new variables ˜x = 2−2nx and ˜y = 2ny (in order to have
˜y = 1/√˜x). Then:
e˜y = 


for x = 22n
2/√1 + mx − 1
for x ∈ AI
√2/√1 + mx − 1 for x ∈ AII

for x = 22n
0
−1 for x ∈ AI
−1 for x ∈ AII

, m˜y = 


which means that without loss of the generality we can conﬁne ourselves to x ∈ h1, 4):

(2.10)

n

n

n

n

0

,




˜x = 1
˜x ∈ h1, 2)
˜x ∈ h2, 4)

for x = 22n
for x ∈ AI
for x ∈ AII

n

n

.

(2.11)

3 Theoretical explanation of InvSqrt code

In this section we present a mathematical interpretation of the code InvSqrt. The
most important part of the code is contained in the line 4. Lines 4 and 5 produce
a zeroth approximation of the inverse square root of given positive ﬂoating-point
number x (sx = 0). The zeroth approximation will be used as an initial value for
the Newton-Raphson iterations (lines 6 and 7 of the code).

Theorem 3.1. The porcedure of determining of an initial value using the magic
constant, described by lines 4 and 5 of the code, can be represented by the following
function

˜x +

+

˜x +

+

3
4
1
2
1
2

1
4
1
8
1
16

−
−
−




1
8
1
8

t

t

˜x +

+

1
16

t

for ˜x ∈ h1, 2)
for ˜x ∈ h2, t)
for ˜x ∈ ht, 4)

(3.1)

˜y0(˜x, t) =

where

mR := N −1
the ﬂoating-point number f (R), corresponding to the magic constant R, satisﬁes

(3.2)
m R⌋ and µx = 0 for Mx even and µx = 1 for Mx odd. Finally,

m R − ⌊N −1

t = tx = 2 + 4mR + 2µxN −1
m ,

eR = 63 , mR <

1
2

,

(3.3)

where f (R) = (1 + mR)2eR .

Proof: The line 4 in the deﬁnition of the InvSqrt function consists of two operations.
The ﬁrst one is a right bit shift of the number Ix, deﬁned by (2.4), which yields the
integer part of its half:

Ix/2 = ⌊Ix/2⌋ = 2−1Nm(B + ex) + ⌊2−1Nm(2−exx − 1)⌋,

(3.4)

4

The second operation yields

(3.5)
and y0 ≡ f (R − Ix/2) is computed in the line 5. This ﬂoating-point number will be
used as a zeroth approximation of the inverse square root, i.e., y0 ≃ 1/√x (for a
justiﬁcation see the next section). Denoting, as usual,

Iy0 := R − Ix/2,

and remembering that

y0 = (1 + my0)2ey0 ,

R = Nm(eR + B + mR),

we see from (2.6), (3.4) and (3.5) that

my0 = eR + mR − ey0 − N −1

m Ix/2,

ey0 = eR + ⌊mR − N −1

m Ix/2⌋.

(3.6)

(3.7)

(3.8)

Here eR is an integer part and mR is a mantissa of the ﬂoating-point number given
by f (R). It means that eR = 63 and mR < 1/2.

According to formulas (3.4), (3.8) and (2.5), conﬁning ourselves to ˜x ∈ h1, 4),

we obtain:

I˜x/2 = ⌊2−1Nmm˜x⌋ +(cid:26)

2−1NmB

2−1Nm(B + 1)

for ˜x ∈ h1, 2)
for ˜x ∈ h2, 4)

.

(3.9)

Hence

e˜y0 = eR −

B + 1

2

+(cid:26) ⌊mR + 2−1 − N −1

m ⌊2−1Nmm˜x⌋⌋

m ⌊2−1Nmm˜x⌋⌋

⌊mR − N −1
2 (B − 1) = 63 and mR < 1

for ˜x ∈ h1, 2)
for ˜x ∈ h2, 4)

.

(3.10)

2 we get e˜y0 = −1, which

Therefore, requiring eR = 1

means that e˜y0 = ey for ˜x = h1, 2). The condition mR < 1/2 implies

and

m ⌊2−1Nmm˜x⌋⌋ = 0,

⌊mR + 2−1 − N −1
m ⌊2−1Nmm˜x⌋⌋ = (cid:26) 0

⌊mR − N −1

for m˜x ∈ h0, 2mRi
−1 for m˜x ∈ (2mR, 1)

(3.11)

,

(3.12)

which means that

e˜y0 = (cid:26) eR − B+1
eR − B+3

2 = −1 for ˜x ∈ h1, 2 + 4mRi
2 = −2 for ˜x ∈ (2 + 4mR, 4)

,

(3.13)

which ends the proof.

In order to get a simple expression for the mantissa m˜y0 we can make a next

approximation:

⌊2−1Nmm˜x⌋ ≃ 2−1Nmm˜x − 2−1,
which yields a new estimation of the inverse square root:

˜y00 = (1 + m˜y00)2e ˜y00 ,

5

(3.14)

(3.15)

where for t = 2 + 4mR + 2N −1
m :

e˜y00 = −1, m˜y00 = 2−2t − 2−1m˜x
e˜y00 = −1, m˜y00 = 2−2t − 2−1m˜x − 2−1
e˜y00 = −2, m˜y00 = 2−2t − 2−1m˜x + 2−1



Because m˜x = 2−e˜x ˜x − 1, the above equations yield

for ˜x ∈ h1, 2) = ˜AI
for ˜x ∈ h2, ti = ˜AII
for ˜x ∈ (t, 4) = ˜AIII

.

(3.16)

˜y00 = 2e ˜y00 (˜α · ˜x + ˜β),

where:

˜α = −2−(1+e˜x),

˜β =

1
4

t −

1
2

e˜x − e˜y00 +

1
2

,

which means that ˜y00 is a piecewise linear function of ˜x:

˜y00(˜x, t) =


0(˜x, t) = −2−2(˜x − 2−1t − 3)
˜yI
˜yII
0 (˜x, t) = −2−3(˜x − t − 4)
(˜x, t) = −2−4(˜x − t − 8)
˜yIII
0

for ˜x ∈ h1, 2) = ˜AI
for ˜x ∈ h2, ti = ˜AII
for ˜x ∈ (t, 4) = ˜AIII

.

(3.17)

Corollary 3.2. ˜y0(˜x) can be approximated by piece-wise linear function

˜y00(˜x) := ˜y0(˜x, t1),

(3.18)

where t1 = 2 + 4mR + 2N −1

m with a good accuracy (2Nm)−1 ≈ 5.96 · 10−8.

This function is presented on Fig. 2 for a particular value of mR:

mR = (R − 190Nm)/Nm = 3630127/Nm ≃ 0.4327449,

known from the literature. The right part of Fig. 2 shows a very small relative error
(˜y00 − ˜y0)/˜y0, which conﬁrms the validity and accuracy of the approximation (3.14).
In order to improve the accuracy, the zeroth approximation (˜y00) is corrected
twice using the Newton-Raphson method (lines 6 and 7 in the InvSqrt code):

˜y01 = ˜y00 − f (˜y00)/f ′(˜y00),
˜y02 = ˜y01 − f (˜y01)/f ′(˜y01),

where f (y) = y−2 − x. Therefore:

˜y01(˜x, t) = 2−1 ˜y00(˜x, t)(3 − ˜y2
˜y02(˜x, t) = 2−1 ˜y01(˜x, t)(3 − ˜y2

00(˜x, t) ˜x),
01(˜x, t) ˜x).

(3.19)

(3.20)

In this section we gave a theoretical explanation of the InvSqrt code.

In the
original form of the code the magic constant R was guessed. Our interpretation
gives us a natural possibility to treat R as a free parameter. In next section we will
ﬁnd its optimal values, minimizing errors.

6

function 1/√˜x and its zeroth approximation ˜y00(˜x, t) given by (3.17).
Figure 2: Left:
Right: relative error of the zeroth approximation ˜y00(˜x, t) for 2000 random values of ˜x.

4 Minimization of the relative error

Approximations of the inverse square root presented in the previous section depend
on the parameter t directly related to the magic constant. The value of this pa-
rameter can be estimated by analysing the relative error of ˜y0k(˜x, t) with respect to
√˜x:

˜δk(˜x, t) = √˜x˜y0k(˜x, t) − 1, where: k ∈ {0, 1, 2}.

As the best estimation we consider t = t(r)

k minimizing the relative error ˜δk(˜x, t):

(4.1)

∀t6=t(r)

k

max

˜x∈ ˜A |˜δk(˜x, t(r)

k )| < max

˜x∈ ˜A |˜δk(˜x, t)| where: ˜A = ˜AI ∪ ˜AII ∪ ˜AIII .

(4.2)

4.1 Zeroth approximation

In order to determine t(r)
0 we have to ﬁnd extrema of ˜δ0(˜x, t) with respect to ˜x,
to identify maxima, and to compare them with boundary values ˜δ0(1, t) = ˜δI
0(1, t),
˜δ0(2, t) = ˜δI
(4, t) at
the ends of the considered intervals ˜AI , ˜AII , ˜AIII . Equating to zero derivatives of
0(˜x, t), ˜δII
˜δI

0 (2, t), ˜δ0(t, t) = ˜δII

(t, t), ˜δ0(4, t) = ˜δIII

0 (t, t) = ˜δIII

0(2, t) = ˜δII

0 (˜x, t), ˜δIII

(˜x, t):

0

0

0

0 = ∂˜x ˜δI
0 = ∂˜x ˜δII
0 = ∂˜x ˜δIII

0(˜x, t) = 2−3x−1/2(3 − 3x + 2−1t),
0 (˜x, t) = 2−2x−1/2(1 − 3 · 2−2x + 2−2t),
(˜x, t) = 2−2x−1/2(1 − 3 · 2−3x + 2−3t),

0

(4.3)

(4.4)

we ﬁnd local extrema:

˜xI
0 = (6 + t)/6,

We easily verify that ˜δI

˜xII
0 = (4 + t)/3,
0 (˜x, t), ˜δIII

0

0(˜x, t), ˜δII
∂2
˜x

˜δK
0 (˜x, t) < 0,

˜xIII
0 = (8 + t)/3.,

(˜x, t) are concave functions:

for: K ∈ {I, II, III}

7

which means that we have local maxima at ˜xK
them is negative:

0 (where K ∈ {I, II, III}). One of

˜δIII
0m (t) = ˜δ0(˜xIII

0

, t) = −2−1 + 2−3t < 0 for t ∈ (2, 4).

The other maxima, given by

˜δI
0m(t) = ˜δ0(xI
˜δII
0m(t) = ˜δ0(xII

0, t) = −1 + 2−1(1 + t/6)3/2,
0 , t) = −1 + 2 · 3−3/2(1 + t/4)3/2,

(4.5)

are increasing functions of t, satisfying

0m(t) ≤ 0,
0m(t) > 0,
0m(t) > 0,

˜δII
0m(t) < ˜δI
˜δII
0m(t) < ˜δI
˜δII
0m(t) ≥ ˜δI

0m(t) ∧ ˜δI
0m(t) ∧ ˜δI
0m(t) ∧ ˜δI

for t ∈ (2, 3 · 25/3 − 6i,
for t ∈ (3 · 25/3 − 6, 25/3 + 24/3 − 2),
for t ∈ h25/3 + 24/3 − 2, 4).
Because functions ˜δI
(˜x, t) are concave, their minimal values with
respect to ˜x are assumed at boundaries of the intervals ˜AI , ˜AII , ˜AIII . It turns out
that the global minimum is described by the following function:

0 (˜x, t), ˜δIII

0(˜x, t), ˜δII

(4.8)

(4.7)

(4.6)

0

˜δII
0 (t, t) = ˜δIII

0

(t, t) = −1 + 2−1√t,

(4.9)

which is increasing and negative for t ∈ (2, 4). Therefore, taking into account (4.6),
(4.7) and (4.8), the condition (4.2) reduces to

0m(t(r)
˜δI
0m(t(r)
˜δII

0 ) = |˜δII
0 ) = |˜δII

0 (t(r)
0 (t(r)

0 , t(r)
0 )|
0 , t(r)
0 )|

for

for

t(r)
0 ∈ (2,−2 + 24/3 + 25/3),
t(r)
0 ∈ h−2 + 24/3 + 25/3, 4).

The right answer results from equation (4.11):

t(r)
0 ≃ 3.7309796.

(4.10)

(4.11)

(4.12)

Thus we obtain an estimation minimizing maximal relative error of zeroth approx-
imation:

δ0 max = max

˜x∈ ˜A |δ0(˜x, t(r)

0 )| = |δ0(t(r)

0 , t(r)

0 )| ≃ 0.03421281,

(4.13)

(4.14)

and the magic constant R(r)
0 :

0 = Nm(eR + B) + ⌊2−2Nm(t(r)
R(r)

0 − 2) − 2−1⌉ =

= 1597465647 = 0x5F 37642F .

The resulting relative error is presented at Fig.3.

8

Figure 3: Relative error for zeroth approximation of the inverse square root. Grey points
were generated by the function InvSqrt without two lines (6 and 7) of code and with
R = R(r)

0 , for 4000 random values x ∈ h2−126, 2128).

4.2 Newton-Raphson corrections

The relative error can be reduced by Newton-Raphson corrections (3.19) and (3.20).
Substituting ˜y0k(˜x, t) = (1 + ˜δk(˜x, t))/√˜x we rewrite them as

˜δk(˜x, t) = −

1
2

˜δ2
k−1(3 + ˜δk−1(˜x, t)), where: k ∈ {1, 2}.

(4.15)

The quadratic dependence on ˜δk−1 implies a fast convergence of the Newton-Raphson
iterations. Note that obtained functions ˜δk are non-positive. Their derivatives with
respect to ˜x can be easily calculated

˜δk−1(˜x, t)(2 + ˜δk−1(˜x, t))∂˜x ˜δk−1(˜x, t)

∂˜x˜δk(˜x, t) = −
˜δk(˜x, t) = −
∂2
˜x

3
2
3
2

˜δk−1(˜x, t)(2 + ˜δk−1(˜x, t))∂2
˜x
− 3(˜δk−1(˜x, t) + 1)[∂˜x ˜δk−1(˜x, t)]2

˜δk−1(˜x, t)+

One can easily see that that extremes of ˜δk can be determined by studying extremes
and zeros of ˜δk−1.

• Local maxima of ˜δk(˜x, t) correspond to negative local minima of ˜δk−1(˜x, t) or
for zeros of ˜δk−1(˜x, t). However, a local maximum of a non-positive function
can not be a candidate for a global maximum of its modulus (compare (4.2)).
• Local minima of ˜δk(˜x, t) correspond to positive maxima and negative minima

of ˜δk−1(˜x, t), which means that they are given by ˜δI

0m(t), ˜δII

0m(t), see (4.5).

Then, we compute

∂˜δk(˜x, t)
∂˜δk−1(˜x, t)

3
2

= −

˜δk−1(˜x, t)(2 + ˜δk−1(˜x, t)),

(4.16)

which implies that ˜δk(˜x, t) is an increasing function of ˜δk−1(˜x, t) for ˜δk−1(˜x, t) < 0
and a decreasing function of ˜δk−1(˜x, t) for ˜δk−1(˜x, t) > 0.
It means that there

9

only two candidates for the minimum of ˜δ1(˜x, t): one corresponding to the minimal
negative value of ˜δk−1(˜x, t) and one corresponding to the maximal positive value of
˜δk−1(˜x, t). The smallest value of ˜δk(˜x, t) evaluated at boundaries of regions ˜AI , ˜AII
and ˜AIII still is assumed at ˜x = t.

In the case k = 1 (the ﬁrst Newton-Raphston correction) we have negative min-
1m(t)) corresponding to positive maxima of ˜δ0(˜x, t). The minima

ima (˜δI
are decreasing functions of t and satisfy conditions

1m(t) and ˜δII

|˜δII
1m(t)| < |˜δI
|˜δII
1m(t)| ≥ |˜δI

1m(t)|
1m(t)|

for t ∈ (2,−2 + 24/3 + 25/3),
for t ∈ h−2 + 24/3 + 25/3, 4).

(4.17)

(4.18)

In the case k = 2 we get minima at the same locations as for k = 1 and with
the same monotonicity with respect to t. All this leads to the conclusion that the
condition (4.2) for the ﬁrst and second correction will be satisﬁed for the same value
t = t(r)

(smaller than t(r)

1 = t(r)

2

0 ):
t(r)
1 = t(r)

2 ≃ 3.7298003 ,

where t(r)
1

is a solution to the equation

˜δ1(t(r)

1 , t(r)

1 /3

, t(r)

1 ).

1 ) = ˜δ1(4/3 + t(r)
{z

|

˜xII
0

}

Thus we obtained a new estimation of the magic constant:

R(r)
1 = R(r)

2 = Nm(eR + B) + ⌊2−2Nm(t(r)

= 1597463174 = 0x5F 375A86

1 − 2) − 2−1⌉ =

with corresponding maximal relative errors:

˜δ1 max = |˜δ1(t(r)
˜δ2 max = |˜δ2(t(r)

1 , t(r)
1 , t(r)

1 )| ≃ 1.75118 · 10−3 ,
1 )| ≃ 4.60 · 10−6 .

(4.19)

(4.20)

(4.21)

(4.22)

(5.1)

(5.2)

5 Minimization of the absolute error

Similarly as in the previous section we will derive optimal values of the magic
constant minimizing the absolute error of approximations ˜y00(˜x, t), ˜y01(˜x, t) and
˜y02(˜x, t), i.e., by minimizing

In other words, we will ﬁnd t(a)

˜∆k(˜x, t) = ˜y0k(˜x, t) − 1/√˜x,
0 , t(a)
1 and t(a)
˜x∈ ˜A | ˜∆k(˜x, t(a)
for k = 0, 1, 2, where ˜A = ˜AI ∪ ˜AII ∪ ˜AIII .
10

∀t6=t(a)

max

2

k

k ∈ {0, 1, 2}.

such that

k )| < max

˜x∈ ˜A | ˜∆k(˜x, t)|

Figure 4: Relative error of the ﬁrst Newton-Raphson correction of the inverse square root
approximation. Grey points were generated by the fuction InvSqrt without line 7 of the
code and with R = R(r)

1 , for 4000 random values x ∈ h2−126, 2128).

Figure 5: Relative error of the second Newton-Raphson correction of the inverse square
root approximation. Grey points were generated by the function InvSqrt z R = R(r)
1 =
R(r)
2 , for 4000 random values x ∈ h2−126, 2128). The visible blur is a consequence of round-
oﬀ errors.

11

5.1 Zeroth approximation

In order to ﬁnd t(a)
0 minimizing the absolute error of ˜y00(˜x, t), we will compute and
study extremes of ˜∆0(˜x, t) inside intervals ˜AI , ˜AII and ˜AIII , and compare them
with values of ˜∆0(˜x, t) at the ends of the intervals. First, we compute the boundary
values:

2√2 − 1

4

˜∆0(1, t) =

˜∆0(2, t) =

˜∆0(t, t) =

˜∆0(4, t) =

,

1
2

t
8 −
t
8 −
1
1
√t
2 −
t
1
16 −
4

,

.

,

(5.3)

All of them are negative (for t < 4). Derivatives of error functions ( ˜∆I
˜∆II

(˜x, t)) are given by:

0 (˜x, t) and ˜∆III

0

0(˜x, t),

∂˜x ˜∆I
∂˜x ˜∆II
∂˜x ˜∆III

0(˜x, t) = 2−1 ˜x−3/2 − 2−2,
0 (˜x, t) = 2−1 ˜x−3/2 − 2−3,
(˜x, t) = 2−1 ˜x−3/2 − 2−4.

0

Therefore local extrema are located at

˜xI
0a = 22/3,

˜xII
0a = 24/3,

˜xIII
0a = 4

(the locations do not depend on t). The second derivative is negative:

(5.4)

(5.5)

∂2
˜x

˜∆K
0 (˜x, t) = −3 · 2−2x−5/2 < 0

for K ∈ {I, II, III}. Therefore all these extremes are local maxima, given by

˜∆0(˜xI

0a, t) = ˜yI

0(˜xI

˜∆0(˜xII

0a, t) = ˜yII

0 (˜xII

˜∆0(˜xIII

0a , t) = ˜yIII

0

0a)−1/2 =

0a, t) − (˜xI
0a, t) − (˜xII
0a , t) − (˜xIII
(˜xIII

3
4 −
1
2 −
0a )−1/2 =

0a)−1/2 =

3
2 3√2
+
3 3√2
4

+

t,

1
8
1
8

1
16

1
4

.

t −

t,

(5.6)

We see that ˜∆0(˜xIII
0a , t) < 0 for t < 4 (and negative maxima obviously are not
important). Direct computation shows that for t 6 4 the ﬁrst value of (5.6) is the
greatest, i.e.,

˜∆0(˜x, t) = ˜∆0(˜xI

0a, t)

max
˜x∈ ˜A

(5.7)

Evaluating ˜∆0(˜x, t) at the ends of the intervals ˜AI , ˜AII and ˜AIII, we ﬁnd the global
minimum:

+

< 0,

(5.8)

min
˜x∈ ˜A

˜∆0(˜x, t) = ˜∆I

0(1, t) = −

12

1
2

t
8

which enables us to formulate the condition (5.2) in the following form:

max
˜x∈ ˜A

˜∆0(˜x, t) = | min

˜x∈ ˜A

˜∆0(˜x, t)| ⇔

3
4 −

3
2 3√2

+

1
8

t =

1
2 −

t
8

.

(5.9)

Solving this equation we get:

t(a)
0 = −1 + 3 · 22/3 ≃ 3.7622,

(5.10)

which corresponds to a magic constant R(a)
0

given by

R(a)
0 = Nm(eR+B)+⌊2−2Nm(t(a)

0 −2)−2−1⌉ = 1597531127 = 0x5F 3863F 7. (5.11)

The resulting maximal error of the zeroth approxmation reads

∆0 max = max

˜x∈ ˜A |∆k(˜x, t(a)

k )| =

5
8 −

3
4 3√2 ≃ 0.0297246.

(5.12)

Figure 6: Absolute error of zeroth approximation of the inverse square root. Grey points
were generated by the function InvSqrt without two lines (6 and 7) of the code and with
R = R(a)

0 , for 4000 random values x ∈ h1, 4).

5.2 Newton-Raphson corrections

The absolute errror after Newton-Raphson corrections is a non-positive function,
similarly as the relative error. This function reaches its maximal value equal to
zero in intervals ˜AI and ˜AII (which corresponds to zeros of ˜∆I
0 ) and has a
negative maximum in the interval ˜AIII . The other extremes (minima) are decreas-
ing functions of the parameter t. They are located at x deﬁned by the following
equations:

0 i ˜∆II

0 = −

−

75
128 −
9tx2
256

+

27t
256 −
x3
32

,

9t2
512 −

t3

1024

+

2x3/2

+

1

+

27x
64
for x ∈ ˜AI ,

9tx
64

+

3t2x
256 −

27x2
128

+

(5.13)

13

Figure 7: Absolute error of the ﬁrst Newton-Raphson correction of the inverse square root
approximation. Grey points were generated by the function InvSqrt without the line 7 of
the InvSqrt code and with R = R(a)

1 , for 4000 random values ˜x ∈ h1, 4).

0 = −

−

1
4 −
9tx2
1024

3t2
3t
256 −
64 −
x3
256

+

,

t3

1024

+

1

2x3/2

+

+

3x
32

3tx
64
for x ∈ ˜AII .

+

3t2x
512 −

9x2
256

+

(5.14)

The condition (5.2) reduces to the equality of the local minimum (located in ˜AI ) and
˜∆1(1, t) (this is an increasing function of t). The equality is obtained for t = t(a)
1 ,
where

t(a)
1 ≃ 3.74699138,

The corresponding maximal error and a magic constant are given by

∆1 max ≃ 0.001484497,

R(a)

1 = 1597499226 = 0x5F 37E75A .

(5.15)

(5.16)

(5.17)

In the case of the second Newton-Raphson correction the minimization of errors is
obtained similarly, by equating the local minimum ˜∆I
2(1, t). Hence we
get another value a magic constant:

2(˜x, t) with ˜∆I

corresponding to

R(a)

2 = 1597484501 = 0x5F 37ADD5,

t(a)
2 ≃ 3.73996986, ∆2 max ≃ 3.684 · 10−6.

(5.18)

(5.19)

6 Conclusions

In this paper we have presented a theoretical interpretation of the InvSqrt code, giv-
ing a precise meaning to two values of the magic constant exisiting in the literature

14

Figure 8: Absolute error of the second Newton-Raphson correction of the inverse square
root approximation. Grey points were generated by the function InvSqrt of the InvSqrt
code and with R = R(a)

2 , for 4000 random values ˜x ∈ h1, 4).

and adding two more values of the magic constant. Using this magic constant we
have conducted error analysis for Newton-Raphson Method and proved that error
bounds for single precision computation are acceptable. The magic constant can be
easily incorporated in existing ﬂoating point multiplier or ﬂoating point multiply-
add fused and one need to replace LUT with the magic constant.

References

[1] M. Sadeghian and J. Stine: Optimized Low-Power Elementary Function Ap-
proximation for Chybyshev series Approximation, 46th Asilomar Conf. on Sig-
nal Systems and Computers, 2012.

[2] K. Diefendorﬀ, P. K. Dubey, R. Hochprung and H. Scales: Altivec Extension
to PowerPC Accelerates Media Processing, IEEE Micro, pp. 85-95, Mar./Apr.
2000.

[3] D. Harris: A Powering Unit for an OpenGL Lighting Engine, Proc. 35th Asilo-

mar Conf. Singals, Systems, and Computers, pp. 1641-1645, 2001.

[4] D. M. Russinoﬀ: A Mechanically Checked Proof of Correctness of the AMD
K5 Floating Point Square Root Microcode, Formal Methods in System Design,
Vol.14, Issue 1, pp. 75-125,Jan 1999.

[5] J-M Muller, N. Brisebarre, F. Dinechin, C-P. Jeannerod, V. Lefvre, G.
Melquiond, N. Revol, D.Stehl and S. Torres: Software Implementation of
Floating-Point Arithmetic, Handbook of Floating-Point Arithmetic, pp. 321-
372, Oct 2009.

[6] D. E. Metafas and C. E. Goutis: A ﬂoating-point advanced cordic proces-
sor,Journal of VLSI signal processing systems for signal, image and video tech-
nology, Vol.10, Issue 1, pp 53-65, Jan 1995.

[7] M.Cornea, C. Anderson and C. Tsen: Software Implementation of the IEEE
754R Decimal Floating-Point Arithmetic,Software and Data Technologies, Vol.

15

10 of the series Communications in Computer and Information Science pp. 97-
109.

[8] J-M Muller, N. Brisebarre, F. Dinechin, C-P. Jeannerod, V. Lefvre, G.
Melquiond, N. Revol, D.Stehl and S. Torres: Hardware Implementation of
Floating-Point Arithmetic, Handbook of Floating-Point Arithmetic, pp. 269-
320, Oct 2009.

[9] D.H.Eberly: GPGPU Programming for Games and Science, CRC Press 2015.

[10] N.Ide, M.Hirano, Y.Endo, S.Yoshioka, H.Murakami, A.Kunimatsu, T.Sato,
T.Kamei, T.Okada, and M.Suzuki: 2. 44-GFLOPS 300-MHz Floating-Point
Vector-Processing Unit for High-Performance 3D Graphics Computing, IEEE
J. Solid-State Circuits, vol. 35, no. 7, pp. 1025-1033, July 2000.

[11] S. Oberman,G. Favor and F. Weber :(AMD 3DNow! technology: architecture

and implementations. IEEE Micro, Vol.9, No.2, pp. 37-48, Mar/Apr. 1999.

[12] W. Liu and A. Nammarelli: Power Eﬃcient Division and Square root Unit,

IEEE Trans. Comp, vol. 61, No.8, pp. 1059-1070, Aug 2012.

[13] T .J. Kwon and J. Draper: Floating-point Division and Square root Imple-
mentation using a Taylor-Series Expan- sion Algorithm with Reduced Look-Up
Table, 51st Midwest Symposium on Circuits and Systems, 2008.

[14] L.. Xuan and D. J. An: A low latency High-throughput Elementary Func-
tion Generator based on Enhanced double rotation CORDIC, Symposium on
Computer Applications and Communications, 2014.

[15] M. X. Nguyen and A. Dinh-Duc: Hardware-Based Algorithm for Sine and Co-
sine Computations using Fixed Point Processor, 11th International Conf. on
Electrical Engineering/Electronics Computer, Telecommunca- tions and Infor-
mation Technology, 2014.

[16] B. Paharami: Computer Arithmetic Algorithms and Hardware Designs, Oct

2010.

[17] id software, quake3-1.32b/code/game/q math.c , Quake III Arena, 1999.

[18] C. Lomont,

”Fast

root,” Purdue University, Tech.
Rep., 2003. Available online: http://www.matrix67.com/data/InvSqrt.pdf,
http://www.lomont.org/Math/Papers/2003/InvSqrt.pdf.

inverse

square

[19] S.Zafar, R.Adapa: Hardware architecture design and mapping of ”Fast Inverse
Square Root’s algorithm”, Advances in Electrical Engineering (ICAEE), 2014
International Conference on. - IEEE, 2014. - pp. 1-4.

[20] J.Blinn, Floating-point tricks, IEEE Computer Graphics and Applications 17

(4) (1997) 80-84.

[21] Q.Avril, V. Gouranton and B. Arnaldi: Fast Collision Culling in Large-Scale
Environments Using GPU Mapping Function, ACM Eurographics Parallel
Graphics and Visualization, Cagliari, Italy (2012).

[22] E.Ardizzone, R.Gallea, O.Gambino, R.Pirrone: Eﬀective and Eﬃcient Interpo-
lation for Mutual Information based Multimodality Elastic Image Registration,
2003.

16

[23] J.L.V.M. Stanislaus, T.Mohsenin: High Performance Compressive Sensing Re-
construction Hardware with QRD Process, IEEE International Symposium on
Circuits and Systems (ISCAS ’ 12), May 2012.

[24] M.Robertson: A Brief History of InvSqrt, Bachelor Thesis, Univ. of New

Brunswick 2012.

[25] D.Eberly:

Fast

inverse

square

root, Geometric Tools, LLC(2010),

http://geometrictools.com/Documentation/FastInverseSqrt.pdf.

[26] C.McEniry: The Mathematics Behind the Fast Inverse Square Root Function

Code, Tech. rep. 2007.

[27] B.Self: Eﬃciently Computing the Inverse Square Root Using Integer Opera-

tions. May 31, 2012.

17

This figure "Bezwzgledny0.jpeg" is available in "jpeg"(cid:10) format from:

http://arxiv.org/ps/1603.04483v1

This figure "Bezwzgledny1.jpeg" is available in "jpeg"(cid:10) format from:

http://arxiv.org/ps/1603.04483v1

This figure "Wzgledny1.jpeg" is available in "jpeg"(cid:10) format from:

http://arxiv.org/ps/1603.04483v1

This figure "Bezwzgledny2.jpeg" is available in "jpeg"(cid:10) format from:

http://arxiv.org/ps/1603.04483v1

This figure "BityFloat.jpeg" is available in "jpeg"(cid:10) format from:

http://arxiv.org/ps/1603.04483v1

This figure "Wzgledny2.jpeg" is available in "jpeg"(cid:10) format from:

http://arxiv.org/ps/1603.04483v1

