6
1
0
2

 
r
a

 

M
1
1

 
 
]

V
C
.
s
c
[
 
 

1
v
3
8
7
3
0

.

3
0
6
1
:
v
i
X
r
a

Region Graph Based Method for Multi-Object Detection and

Tracking using Depth Cameras∗

Sachin Mehta†

University of Washington

Balakrishnan Prabhakaran

University of Texas at Dallas

sacmehta@uw.edu

bprabhakaran@utdallas.edu

Abstract

In this paper, we propose a multi-object detection
and tracking method using depth cameras. Depth maps
are very noisy and obscure in object detection. We ﬁrst
propose a region-based method to suppress high magni-
tude noise which cannot be ﬁltered using spatial ﬁlters.
Second, the proposed method detect Region of Inter-
ests by temporal learning which are then tracked using
weighted graph-based approach. We demonstrate the
performance of the proposed method on standard depth
camera datasets with and without object occlusions. Ex-
perimental results show that the proposed method is able
to suppress high magnitude noise in depth maps and
detect/track the objects (with and without occlusion).

1. Introduction

The recent emergence of cost-eﬀective depth sen-
sors have triggered signiﬁcant attention of researchers
within computer vision and robotics community.
Depth sensors have their own advantages over visible
light cameras. Depth images are insensitive to vari-
ation in texture and illumination. Moreover, depth
images represent 3D structural information reﬂecting
shape cues and geometry.

There is a growing body of research on topics such as
human detection and human tracking using 3D infor-
mation. Ikemura et al. [6] proposed a method for hu-
man detection. The method extracts relational depth
similarity features and constructs a classiﬁer which is
then used for detecting humans. Xia et al.
[17] pro-
posed human detection method in indoor environment
using depth information. The method ﬁrst identiﬁes
the possible regions that may contain humans which
are then veriﬁed using 3D head model. Zhang et al.

∗Appearing in Proc.
IEEE Winter Conference on Applica-
†This work was done by author before joining University of

tions of Computer Vision, USA, 2016

Washington.

[20] explored the Kinect for detecting falls in elderly
people. The method eventually adopt a known back-
ground subtraction method for detecting people and
analyze the trajectory to detect the fall.

Instead of using only depth maps, some researchers
exploited both RGB and depth channels for human de-
tection and tracking. Han et al. [4] proposed a method
for detecting and tracking humans using both RGB
and depth channels. The method locates the object in
depth map and then extracts the corresponding visual
features from the RGB data. These visual features are
then used for tracking objects in successive frames of
RGB data. Spinello et al.
[13] extends the idea of
Histogram of Oriented Gradients (HOG) and proposed
a conceptually similar Histogram of Oriented Depths
(HOD) method for human detection. Enzweiler et al.
[1] developed a stereo system which combines intensity
images, stereo disparity maps, and optical ﬂow for de-
tecting and tracking people. Song et al. [12] developed
a RGB-D dataset for tracking objects. They imple-
mented and tested diﬀerent algorithms, such as HOG
with Optical ﬂow, on RGB-D data for tracking objects.
Graph-based object tracking methods have been
proposed in past for RGB videos. Gomilla and Meyer
[3] proposed a graph-based object tracking method
whereby each frame is represented as a region ad-
jacency graph and thus, tracking becomes a graph-
matching problem. Wang and Nevatia [15] proposed
a tracking method based on superpixels. Constella-
tion appearance model was constructed based on visual
features extracted from superpixels and then tracking
is performed using Dynamic Bayesian Network. Re-
cently, Yang et al.
[18] proposed a tracking method
using superpixels. They used mid level cues for distin-
guishing between target and the background. Tracking
is then achieved by formulating a map between target
and background.

In this paper, we have successfully applied region
graph based approach for object detection and track-
ing in depth videos. The proposed approach shows a

1

signiﬁcant improvement of about 9% over the existing
methods. Further, depth maps are very noisy and con-
tain high magnitude noise which is diﬃcult to remove
using spatial ﬁlter. We proposed a method to remove
such high magnitude noise from depth maps and have
seen a signiﬁcant improvement of about 23% in track-
ing results over the tracking results obtained without
noise suppression.

Rest of the paper is organized as: The proposed
method for noise suppression, ROI detection, and
tracking is discussed in Section 2. Experimental re-
sults are presented in Section 3 while conclusions are
drawn in Section 4.

2. Proposed Method

In this section, region graph based approach for ROI

detection and tracking is discussed in detail.

2.1. Noise Suppression

Noise in depth videos can be classiﬁed into three
categories: (i) noise from sensors NS, (ii) noise from
boundary of objects NB, and (iii) holes in depth map
NH (caused by fast movements, random surfaces, etc.)
[16]. Noise occurred due to variation in sensors is
evenly distributed throughout the depth map and is
of low magnitude. Such type of noise can be easily re-
moved using spatial ﬁlters such as Gaussian ﬁlter. On
the other hand, noise occurring due to boundary of ob-
jects and holes in depth map is not evenly distributed
and their magnitude is quite high in contrast to noise
arising from sensors. Such type of noise is diﬃcult to
remove using spatial ﬁlters. Based on temporal learn-
ing, Xia et al.
[16] proposed a ﬁltering algorithm for
removing such high magnitude noise. Figure 1(c) shows
a snapshot of the ﬁltered depth map using Xia et al.’s
[16] method. The method is able to remove some part
of high magnitude noise but not all.
In this section,
a method is proposed to remove high magnitude noise
arising from boundary of objects and holes in depth
map.

The proposed method ﬁrst applies a 2D Gaussian
smoothing ﬁlter to remove noise arriving from sensors.
Then, the proposed method segments the depth map
into regions using morphological watershed segmenta-
[10]1. Let us
tion (MWS) proposed by Meyer et al.
assume that R closed regions and RB background re-
gions are detected in spatially ﬁltered depth map DS
using MWS2. Background regions RB are the ones
whose at least P boundary points lie on the border of
depth map. Some of the closed regions might enclose

1Any other image segmentation algorithm can be used.
2We assume that depth map has at least one region.

(a) Original RGB Image

(b) Original Depth Map

(c) Xia’s [16] Method

(d) The Proposed Method

Figure 1. Noise Filtering Example. For the sake of under-
standing, we have shown the depth maps as binary images.

(a)

(b)

Figure 2. Region Categorization: (a) Without merging RP
and RC region (b) With merging RP and RC region

other closed regions. We represent enclosed regions as
RC ⊂ R, enclosing regions as RP ⊂ R, and the remain-
ing closed regions as independent regions RI ⊂ R such
that R = RC ∪RP ∪RI . Figure 2 shows diﬀerent possi-
ble regions. Region RC and RP can be merged and the
merging decision is solely dependent upon application.
For instance, region RC and RP can not be merged for
human part-based tracking while for complete human
tracking, RC and RP can be merged. In the proposed
method, we are interested in complete object tracking
in contrast to part-based tracking and hence, we merge
regions RC and RP .

of each closed region R as: AR[i] = (cid:80)(cid:80) R[i](x, y).

Now, the proposed method computes the area AR

Noise arising from boundary of objects NB and holes
NH in depth map D will also create regions. Though
magnitude of NB and NH will be higher in comparison
to NS, it has lower magnitude in contrast to actual
object size. Figure 3 shows the plot of area of closed
regions contained in Figure 1(b). In Figure 3, we can
see that the area of regions corresponding to noise is
quite less (No. of pixels lies between 0.01 ×104 and

mapped regions of ﬁrst K frames. If ||D(jo → i0)||1
is greater than a particular threshold value4, then we
mark the region R[jo] as ROI.

As the proposed method is region-based, direction
of object movement is an important clue and can help
in optimizing the performance of the proposed method
while searching the ROI in next depth frame. We use
a cardinal system to determine the direction of ROI
movement. Cardinal system can be 4-point or 8-point
or 16-point. In our experiments, we have used 4-point
cardinal system, shown in Figure 4. For determining
the direction of ROI, the proposed method computes
diﬀerence between the mapped regions of two consec-
utive depth maps D(jo → i0) and then decides the
direction of region movement using the four point car-
dinal system.
It is to be noted that the direction is
computed with respect to the center of the region of
depth frame at time (t + 1) i.e. R[j0]. Figure 5(a) and
Figure 5(b) shows snapshots of two consecutive depth
maps where person is waving a hand while the diﬀer-
ence between these depth maps is shown in Figure 5(c).
From Figure 5(c), we can see that the direction of hand
movement is North-East. Given this information, we
can predict that the hand movement in the next depth
frame will be either in East or North direction and we
do not need to search in South and West direction.

Figure 3. Area of closed regions contained in Figure 1(b)
0.3 ×104) in comparison to regions corresponding to
object size (No. of pixels lies between 1 ×104 and 7
×104). Threshold based approach is then applied to
remove noise from spatially ﬁltered depth map DS as:

(1)

(cid:26) R[i], AR[i] > τ

0,

otherwise

R[i] =

where τ is the threshold for eliminating the noise aris-
ing from boundary of objects and holes in depth map.
τ is a function of region area and can be computed as
τ =

, 1 ≤ i ≤ n.

||AR[i]||1

n

Figure 1(d) shows an example of noise suppression
in depth map using the proposed method. From Figure
1(d), it is clear that proposed method is able to remove
the noise (NB, NS, and NH ) eﬀectively.
2.2. Good features to track

Object traverses a very limited space from time t to
time (t+1). We use this important clue to map regions
between two consecutive frames and identify ROI. Let
Gt and Gt+1 be the undirected graphs for frame at time
t and t+1. Displacement between two arbitrary nodes,
i0 ∈ Gt and j0 ∈ Gt+1, with region R[i0] and R[j0]
respectively is deﬁned as: D(jo → i0) = R[j0] \ R[i0].
This measure tells us how much a region has traversed
from time t to (t + 1). Furthermore, region mapping
process is speeded-up by including the threshold δ. If
D(jo → i0) > δ, then we map the regions R[j0] and
R[i0]3.

Besides region mapping, the proposed method si-
multaneously do temporal tracking of “mapped re-
gions” in ﬁrst K depth maps to determine ROI.
ROI can be either growing region (region whose area
changes, as in case of hand waving) or shrinking re-
gion (region whose area changes, as in case of zooming
out) or moving region (region whose position changes,
as in case of rolling ball) or combination of growing,
shrinking, and movement (as in case of box lifting).
The proposed method computes ||D(jo → i0)||1 from

Figure 4. Four Point Cardinal System

(a)

(b)

(c)

Figure 5. Determining direction (a) Depth frame at time t
(direction of hand is east), (b) depth frame at time t + 1
(direction of hand is North-East) and (c) Diﬀerence between
(a) and (b)

2.3. Tracking

The proposed method starts tracking ROI from
(K + 1)th depth map (K depth maps are required for
ROI determination). In order to track ROI, the pro-
posed method creates a weighted graph for each ROI.
Let V and E denotes the number of nodes and edges in

3Empirical value of δ in our experiments is 80.

4Empirical value is 70

graph G of (K + 1)th frame, where each node vi ∈ V
corresponds to each region in the depth map (set of
elements to be tracked) and edges (vi, vj) ∈ E cor-
responding to neighbouring regions5. Let us assume
that node vk represents ROI. Now, a node table is con-
structed for node vk. Node table contains the short-
est distance d between ROI and the remaining regions.
Now, the weights are assigned to edge (vk, vj) ∈ E
as:

(cid:26) 1,

0,

w(vk, vj) =

d(vk, vj) = 1
otherwise

(2)

(a)

(b)

Figure 6 shows an example of constructing a node ta-
ble and assigning weights w to the edges of a graph G.
To illustrate, let us assume that v3 is the ROI (rep-
resented in green color). Now, the proposed method
computes the distance between node v3 and remaining
nodes of graph G and constructs a node table, shown
in Figure 6(b). Based on the values of node table and
Eq. 2, weights are assigned to the edges (see Figure
6(c)). When an object moves, set of attributes (such
as area and perimeter) of adjoining regions are im-
pacted.
In other words, when any movement occurs
in region corresponding to node v3, attributes of the
edges (v3, vj) ∈ E with weight w(v3, vj) = 1 will be
aﬀected most. Movement in node v3 will have an im-
pact on the attributes of nodes v1, v2, v6, v7, and v8
most. Since the attributes of nodes v4 and v5 will have
the least impact due to any movement in v3, we do not
have to consider these nodes for tracking. This ulti-
mately helps in reducing the searching area for object
tracking.
In order to further minimize the searching
area, the proposed method utilizes predicted direction
of the ROI. If the ROI is moving towards West, for
instance, then regions with edges (v3, vj) ∈ E and
weight w(v3, vj) = 1 in the West direction are most
probable regions for tracking. In our example, nodes v1
and v6 are in West of v3 and hence, we need to search
in regions corresponding to these nodes.

Objects can appear or disappear in a scene. The
proposed method runs a background thread at a con-
stant frame interval to check if any ROI is appearing
or disappearing in the scene. This background thread
executes ROI detection method discussed in Section
2.2 and if there is any change in number of ROIs, it
updates the object tracking thread. In case number of
ROIs are more than one, the proposed method initiates
diﬀerent foreground thread to track each ROI. Value of
constant frame interval in our experiments is equal to
number of frames required to detect ROI i.e. K.

5We draw the boundary between objects using distance trans-

form.

(c)

(d)

Figure 6. (a) Graph G, (b) Node table for Graph G shown
in (a), (c) Weighted graph, and (d) Optimized weighted
graph for object tracking
2.4. Multi-object Tracking and Occlusion Detection

Multiple objects or ROIs in a scene can occlude if
they are moving towards each other. Occlusion can be
detected based on ROI’s size and the distance between
ROI’s. Let A and B be two ROI’s detected and de be
the euclidean distance between A and B. If de = 0,
then the two ROI’s are about to occlude. If the area
of A is changing drastically while the change in area
of B is either zero or slight, then we mark ROI A as
occludee.

Let us say that there are N ROI’s with level sets θi.
The Euclidean distance de between ROI Oi and ROI
Oj can be obtained as: de = arg min θi(θj), i (cid:54)= j.
Now, we compute change in the area of ROI O as:
∆AR = |At
R denotes the area of
ROI O at time t.

R |, where At

R \ At−1

The proposed method then computes the occlusion
detection parameter OD between ROI Oi and ROI Oj
as: ODi,j = ∆AR
exp−|de|+1 . If ODi,j < ι, then we say that
occlusion occurs where ι is the threshold parameter
for occlusion detection. We conducted experiments on
samples with two or more humans to adjust the value
of ι. We varied the value of ι from 0 to 1 in our experi-
ments. When we set ι < 0.5, we were able to detect the
occlusion as soon as it occurs. However, when we set
ι ≥ 0.5, we were able to detect the occlusion but only
when at least 25% of the region of two humans were

(a)

(b)

(c)

Figure 7. A sequence in which two humans occlude each
other (ι = 0.4)

overlapping. Hence, we set ι = 0.4 in our experiments.
Figure 7 shows an occlusion example where two hu-
mans occlude each other (in this sequence, both the hu-
mans are in motion). Before the occlusion, the bound-
aries of the humans are complete. During occlusion,
two humans start overlapping each other (Figure 7(b))
and hence, the boundaries are broken. At this point,
the distance between two humans is very less (de ≈ 0).

3. Experimental Results

We evaluated the performance of the proposed
method on 1000+ videos taken from standard datasets
(MSR [9] [14], UT Austin [17], Princeton University
[12], IROS [13], and RWTH [11]). Videos in these
datasets diﬀer in terms of pose, background, actions,
etc. We conducted experiments on a computer hav-
ing Intel i5-2410M 2.30 GHz processor and 6GB DDR3
RAM.

Since the proposed method uses K depth maps for
detecting ROI, it should be considered as an indepen-
dent variable to conduct a fair study. We conducted
experiments to adjust the value of K and results are
shown in Figure 8. From Figure 8, we can see that
the performance plateaus after K = 5. Hence, we used
K = 5 in our experiments.
3.1. Metrics for evaluation

We quantify the performance of the proposed

method using two metrics:

• F1 Score for object detection: We used this
metric for determining the accuracy of the pro-
posed object detection method [7]. F1 score com-
bines both precision and recall and is deﬁned as:
F1 score = 2×P×R
T P +F P and

P+R , where P =

T P

Figure 8. Impact of K on object detection accuracy

T P +F N . Here, P represents precision, R rep-

R = T P
resents recall, T N represents true negative sam-
ples, T P represents true positive samples, and F N
represents false negative samples.

• Success rate (SR) for tracking: For quantita-
tive evaluation of our tracking algorithm, we em-
ployed the criterion used in PASCAL VOC chal-
lenge [2]. If r > 0.5, then frame is tracked success-
fully. Here, r = T ∩G
T ∪G denotes the overlap ratio,
T denotes the tracked region, and G denotes the
ground truth.

To compare the results with previous methods, we ei-
ther take the reported best results or carefully select
the parameters with the provided source code.

3.2. Object Detection Results

We tested the proposed method on all datasets.
Figure 9 shows the snapshots of the object detection
while Table 1 summarizes the accuracy of the proposed
method on diﬀerent datasets. From Table 1, we can
see that the average accuracy of the proposed method
is ≈ 93%. This clearly indicates that the proposed
method is able to detect objects (humans, boxes, etc.)
in most of the cases. The performance of the proposed
method is low in case of IROS [13], Princeton [12] and
RWTH[11] dataset. Since the proposed method detect
and track an object by analysing its motion, it is not
able to detect the objects which are stationary result-
ing in low accuracy.

Further, we compare the performance of the pro-
posed object detection method with Xia et al.’s [16]
noise suppression method in Table 2. From Table 2,
we can see that the proposed noise suppression method
results in higher accuracy than Xia et al.’s [16] method.

(a)

(b)

(c)

(d)

Figure 9. Object Detection Results during diﬀerent activ-
ities: (a) Tennis serve, (b)Person Lifting Box, (c) Person
Wearing Hat, and (d) Person Walking in a room

Table 1. Object detection results on diﬀerent datasets

Dataset

MSRAction3D

MSRDailyActivity3D

UT Austin

IROS

Princeton

RWTH

Precision Recall
100%
100%
100%
100%
100%
100%
84%
91%
81%
96%
93%
84%
Avg. Accuracy

F1 score

100%
100%
100%
87.36%
87.86%
88.27%
93.91%

Table 2. Object detection results with diﬀerent noise Sup-
pression methods (average values across all datasets)

Noise Suppression Method None Xia [16] Ours
93%

F1 score

77%

84%

Table 3. Comparison with diﬀerent object detection meth-
ods (average values across all datasets)

Method
F1 score

Ikemura [6] Xia [17]

61%

73%

Spinello [13] Ours
93%

97%

Comparison with related work: Table 3 con-
tains the quantitative comparison between the pro-
posed method and the related work. From Table 3,
we can see that the proposed method outperform the
methods proposed by Ikemura et al.
[6] and Xia et
[17] while is comparable with the performance
al.
of Spinello et al.’s [13] method.
Ikemura et al.’s [6]
method is window-based and is able to detect people
which are well centered in frame resulting in high false
negatives and low accuracy. Xia et al.’s [17] method
detect the person by identifying the head contour. In
case of occlusions, the method is able to detect the
head contour of the foreground object and hence, re-
sulting in false negatives and low accuracy. Further, we
compared the accuracy of the proposed method with
Combo-HOD detector proposed by Spinello et al. [13].
The accuracy of the proposed method is around 93%
while the accuracy of Combo-HOD detector proposed
by Spinello et al.
[13] is around 97%. Since the pro-
posed method detect objects based on the movement,
it is not able to detect the humans which are station-
ary, resulting in slightly less accuracy than Spinello et
al.’s [13] method.

3.3. Object Tracking Results

For object tracking, we tested the proposed method
on all datasets. Figure 10 shows a snapshot of ob-
ject tracking on one of the RGB-D videos in Princeton
dataset. Video is captured inside a room where 4 peo-
ple are present ( 3 adults and 1 kid). As we can see in
Figure 10, the proposed method is able to track only
3 people out of the 4 people inside the room. One
of the adults present in the scene is stationary. The
proposed method tracks the object based on motion of
the object and hence, failed to detect one of the adults
present inside the scene.

Figure 11 shows a snapshot of tracking a bear. As
we can see in Figure 11, the proposed method is able to
detect and track the bear as well as the box (with and
without occlusion). However, the proposed method is
not able to track the person. This is because the pro-
posed method detect and track the objects which cor-
respond to closed region R and not the background
region RB (see Section 2 for details). The person in

(a)

(b)

(c)

(d)

(e)

(f)

Figure 10. Tracking result on Princeton Dataset: (a-c) RGB
frames, (d-f) depth frames corresponding to RGB frames
(a-c), and (g-i) tracking results

this video belongs to the background region RB and
hence, the proposed method fails to detect and track
the person.

(a)

(b)

(c)

(d)

(e)

(f)

Figure 11. Results of tracking a bear: (a-b) RGB frames,
(c-d) depth frames corresponding to RGB frames (a-b), and
(e-f) tracking results

For quantitative experiments, we computed overlap-
ping ratio r between tracked object and ground truth
data (discussed in Section 3.1). Since RGB-D datasets
are collected using diﬀerent camera settings and under
diﬀerent conditions (such as indoor, outdoor, etc.), r
should be considered as a variable to conduct a fair
study. We conducted experiments to adjust the value
of r and results are shown in Figure 12. From Fig-
ure 12, we can see that the average tracking SR of the
proposed method is around 83% across all datasets at
r = 0.5.

Further, we compare the performance of the pro-
posed object tracking method with Xia et al.’s [16]
noise suppression method in Table 4. From Table 4,
we can see that the proposed noise suppression method
results in higher accuracy than Xia et al.’s [16] method.

Though we are able to detect both the persons in this
scene, we are able to track these persons till they are
walking. Such scenarios resulted in low SR.

Figure 12. Success rate (SR) vs. overlap area (r)

(a)

(b)

(c)

Table 4. Object tracking results with diﬀerent noise Sup-
pression methods

Noise Suppression Method None Xia [16] Ours
83%

60%

SR

74%

Table 5. Comparison with diﬀerent object tracking methods
(average values across all datasets)

Algorithm
RGBD HOG + Optical Flow (on depth data)[12]
RGB HOG + Optical Flow (on RGB data) [12]
Structured output tracking [5]
Visual tracking [8]
Compressive tracking [19]
Proposed method (on depth data)

SR
75%
52%
46%
42%
40%
83%

Comparison with related work: We compare
the performance of the proposed tracking method
against diﬀerent methods in Table 5. From Table 5,
we can see that the proposed method is more robust
than the other methods. The higher accuracy of the
proposed method against other methods is mainly be-
cause of the noise suppression. For instance, RGBOcc
+ OF [12] method detects false features due to presence
of noise, resulting in low accuracy. When we applied
RGBOcc + OF [12] method after suppressing the noise
using the proposed method, we saw that the success
rate increased from 75% to 83%.

It is worth noting that the accuracy reported for
object detection in Table 1 and object tracking in Table
5 is diﬀerent. This is because the proposed method is
able to track only the motion part of the object. For
instance, person might walk for sometime then does
not move for sometime and then again start moving.
In such a case, the proposed method tracks the person
when he/she is moving. An example is shown in Figure
13 where two persons enter the scene (University Hall).
One of them keeps walking while other goes to ATM
machine and waits there for some time to complete his
work. As we can see in Figure 13, the proposed method
is able to track both the person till they are moving.
The proposed method subsequently lose the track of
one of them as there is no movement (see Figure 13(f)).

(d)

(e)

(f)

Figure 13. Example of lost track: (a-b) RGB frames, (c-d)
depth frames, and (e-f) tracking results

3.4. Impact on execution time

The proposed method uses direction of object move-
ment and weighted graph for optimizing the search area
required for ROI detection and tracking. In case of op-
timization, we use direction of object movement and
weighted graph for ROI detection and tracking while
in case of without optimization, we search the entire
frame to detect and track the ROI. Average execution
time required per frame for noise reduction, ROI de-
tection, and ROI tracking on diﬀerent datasets by the
proposed method without optimization and with op-
timization of search area are 888.6 and 540 millisec-
onds respectively i.e. the proposed optimized method
is 1.65× faster than the unoptimized method. Though
the proposed optimization method signiﬁcantly reduces
the execution time, we have not seen any diﬀerence in
the accuracy of the proposed method with and without
optimization.

4. Conclusion

In this paper, we proposed a region graph based
method for noise suppression, object detection, and ob-
ject tracking using depth cameras. The experimental
results show that the proposed method is able to de-
tect and track the objects with and without occlusions.
The proposed approach can be applied in diﬀerent ap-
plications such as human activity recognition.

The advantage of the proposed method can be sum-
marized as: ﬁrst, the proposed method does not require

any training for ROI detection and tracking. Second,
the proposed method uses direction of object move-
ment and weighted graph for ROI detection and track-
ing. This helps in optimizing the searching area for de-
tection and tracking. The limitations of the proposed
method are: (i) it is not able to do re-identiﬁcation
of objects and (ii) object detection and tracking is not
in real-time. RGB-based tracking systems seem to be
more robust towards object re-identiﬁcation, though
their overall accuracy for tracking is lower.
In fu-
ture, we will try to address these limitations by: (i)
complementing the proposed method with RGB data
for person re-identiﬁcation and (ii) utilizing GPU’s for
speeding-up performance of the proposed method.

5. Acknowledgements

This material is based upon work supported par-
tially by the National Science Foundation under Grant
No.
1012975. Any opinions, ﬁndings, and conclu-
sions or recommendations expressed in this material
are those of the author(s) and do not necessarily re-
ﬂect the views of the National Science Foundation.

References

[1] M. Enzweiler, A. Eigenstetter, B. Schiele, and
D. Gavrila. Multi-cue pedestrian classiﬁcation with
partial occlusion handling.
In Computer Vision and
Pattern Recognition (CVPR), 2010 IEEE Conference
on, pages 990–997, June 2010.

[2] M. Everingham, L. Van Gool, C. Williams, J. Winn,
and A. Zisserman. The Pascal Visual Object Classes
(VOC) Challenge. International Journal of Computer
Vision, 88(2):303–338, 2010.

[3] C. Gomila and F. Meyer. Graph-based object tracking.
In Image Processing, 2003. ICIP 2003. Proceedings.
2003 International Conference on, volume 2, pages II–
41–4 vol.3, Sept 2003.

[4] J. Han, E. Pauwels, P. de Zeeuw, and P. de With. Em-
ploying a rgb-d sensor for real-time tracking of humans
across multiple re-entries in a smart environment. Con-
sumer Electronics, IEEE Transactions on, 58(2):255–
263, May 2012.

[5] S. Hare, A. Saﬀari, and P. Torr.

Struck: Struc-
tured output tracking with kernels. In Computer Vi-
sion (ICCV), 2011 IEEE International Conference on,
pages 263–270, Nov 2011.

[6] S. Ikemura and H. Fujiyoshi. Real-time human de-
tection using relational depth similarity features.
In
R. Kimmel, R. Klette, and A. Sugimoto, editors, Com-
puter Vision ACCV 2010, volume 6495 of Lecture
Notes in Computer Science, pages 25–38. Springer
Berlin Heidelberg, 2011.

[7] G. Kootstra and D. Kragic. Fast and bottom-up object
detection, segmentation, and evaluation using gestalt
principles. In Robotics and Automation (ICRA), 2011
IEEE International Conference on, pages 3423–3428,
May 2011.

[8] J. Kwon and K. M. Lee. Visual tracking decompo-
In Computer Vision and Pattern Recognition

sition.
(CVPR), 2010 IEEE Conference on, 2010.

[9] W. Li, Z. Zhang, and Z. Liu. Action recognition based
on a bag of 3d points. In Computer Vision and Pattern
Recognition Workshops (CVPRW), 2010 IEEE Com-
puter Society Conference on, pages 9–14, June 2010.

[10] F. Meyer and S. Beucher. Morphological segmentation.
Journal of Visual Communication and Image Repre-
sentation, 1(1):21 – 46, 1990.

[11] U. Raﬁ, J. Gall, and B. Leibe. A semantic occlu-
sion model for human pose estimation from a single
depth image. In Computer Vision and Pattern Recog-
nition Workshops (CVPRW), 2015 IEEE Conference
on, pages 67–74, June 2015.

[12] S. Song and J. Xiao. Tracking revisited using rgbd
camera: Uniﬁed benchmark and baselines.
In Com-
puter Vision (ICCV), 2013 IEEE International Con-
ference on, pages 233–240, Dec 2013.

[13] L. Spinello and K. Arras. People detection in rgb-d
data. In Intelligent Robots and Systems (IROS), 2011
IEEE/RSJ International Conference on, pages 3838–
3843, Sept 2011.

[14] J. Wang, Z. Liu, Y. Wu, and J. Yuan. Mining actionlet
ensemble for action recognition with depth cameras.
In Computer Vision and Pattern Recognition (CVPR),
2012 IEEE Conference on, June 2012.

[15] W. Wang and R. Nevatia. Robust object tracking us-
ing constellation model with superpixel.
In K. Lee,
Y. Matsushita, J. Rehg, and Z. Hu, editors, Computer
Vision ACCV 2012, volume 7726 of Lecture Notes
in Computer Science, pages 191–204. Springer Berlin
Heidelberg, 2013.

[16] L. Xia and J. Aggarwal. Spatio-temporal depth cuboid
similarity feature for activity recognition using depth
camera. In Computer Vision and Pattern Recognition
(CVPR), 2013 IEEE Conference on, June 2013.

[17] L. Xia, C.-C. Chen, and J. Aggarwal. Human detection
using depth information by kinect. In Computer Vision
and Pattern Recognition Workshops (CVPRW), 2011
IEEE Computer Society Conference on, 2011.

[18] F. Yang, H. Lu, and M.-H. Yang. Robust superpixel
Image Processing, IEEE Transactions on,

tracking.
23(4):1639–1651, April 2014.

[19] K. Zhang, L. Zhang, and M.-H. Yang. Real-time
compressive tracking. In A. Fitzgibbon, S. Lazebnik,
P. Perona, Y. Sato, and C. Schmid, editors, Computer
Vision ECCV 2012, volume 7574 of Lecture Notes
in Computer Science, pages 864–877. Springer Berlin
Heidelberg, 2012.

[20] Z. Zhang, W. Liu, V. Metsis, and V. Athitsos. A
viewpoint-independent statistical method for fall de-
tection. In Pattern Recognition (ICPR), 2012 21st In-
ternational Conference on, 2012.

