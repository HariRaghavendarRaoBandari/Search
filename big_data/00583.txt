Some essential skills and their combination in an
architecture for a cognitive and interactive robot.

Sandra Devin∗, Gr´egoire Milliez∗, Michelangelo Fiore†, Aur´erile Clodic‡ and Rachid Alami‡

∗CNRS, LAAS, Univ de Toulouse, INP, 7 avenue du colonel Roche, F 31400 Toulouse, France
†CNRS, LAAS, Univ de Toulouse, INSA, 7 avenue du colonel Roche, F 31400 Toulouse, France
‡CNRS, LAAS, Univ de Toulouse, LAAS, 7 avenue du colonel Roche, F 31400 Toulouse, France

6
1
0
2

 
r
a

M
2

 

 
 
]

O
R
.
s
c
[
 
 

1
v
3
8
5
0
0

.

3
0
6
1
:
v
i
X
r
a

Abstract—The topic of joint actions has been deeply studied
in the context of Human-Human interaction in order to un-
derstand how humans cooperate. Creating autonomous robots
that collaborate with humans is a complex problem, where it is
relevant to apply what has been learned in the context of Human-
Human interaction. The question is what skills to implement and
how to integrate them in order to build a cognitive architecture,
allowing a robot to collaborate efﬁciently and naturally with
humans. In this paper, we ﬁrst list a set of skills that we consider
essential for Joint Action, then we analyze the problem from the
robot’s point of view and discuss how they can be instantiated in
human-robot scenarios. Finally, we open the discussion on how
to integrate such skills into a cognitive architecture for human-
robot collaborative problem solving and task achievement.

I. INTRODUCTION

To interact with its environment, an autonomous robot needs
to be able to reason about its surroundings and extract a set
of facts that model a semantic representation of the world
state. To act on its environment, the robot also needs to decide
what actions to take and when to act. Adding humans in the
environment requires to enhance the robot with speciﬁc skills.
Indeed, as humans are social agents, considering them just
as moving obstacles or in the same way as a another robot,
would lead to dangerous or at least unpleasant interactions for
the involved humans. Thus, to estimate the quality of a human-
robot collaborative task execution, the robot should not only
take into account the task completion but also the comfort and
safety of its human partners as well as their acceptability of its
actions. A way to increase human comfort when interacting
with the robot is to give human-like social skills to it. In
this paper, we will present a set of skills needed for Joint
Action and that we consider essential and then discuss how
they can be implemented and combined into a coherent control
architecture.

II. REQUESTED SKILLS FOR JOINT ACTIONS

Research in psychology and philosophy have led to a
good understanding of human behaviors during joint actions
and collaboration. As discussed in [1], the analysis of these
studies can help to identify key elements for human-robot joint
actions.

The ﬁrst step when people have to act together is to share
a goal and the intention to achieve it. In [2], Tomassello et
al. deﬁne a goal as the representation of a desired state and
an intention as an action plan chosen in order to reach a goal.

Bratman adds that if you have a shared intention to perform an
action you should agree on the meshing subparts of a shared
plan [3]. To do so, agents engaged in joint actions need to be
able to negotiate the shared plan [4] which requires to have a
common ground [5].

During the execution of the shared plan, agents need to be
able to perceive and represent the other agents involved in
the task. In [6], Sebanz et al. present three necessary skills
concerning the perception and representation of the others
during joint actions:

• Joint attention: the ability to direct the attention of a
partner on a common reference in order to put an element
of the interaction (e.g. an entity, a task) in focus.

• Action observation: agents need to be able to recognize
others’ actions and to understand their effects on the envi-
ronment and the task. Concerning this, Pacherie discusses
two types of predictions in [7]: goal-to-action and action-
to-goal prediction.

• Co-representation: it is also necessary for an agent to
have a representation of others abilities. Knowing what
others know, their goals or their capacities, allows to bet-
ter predict and understand their actions. In other words,
agents need to have a Theory of mind [8].

Finally, during a joint action, agents need to coordinate
with each other. In [9], Knoblich et al. deﬁne two types of
coordination:

• Emergent coordination: it is non voluntary coordination
as entrainment (for example, two people in rocking chairs
will synchronize their movements) or coordination com-
ing from affordances [10] and from access to resources.
• Planned coordination: when people voluntary change
their behavior in order to coordinate. They can do it by
adding what Vesper et al. called coordination smoother
in [11] or by using verbal or non-verbal communication.

III. FROM THE ROBOT’S POINT OF VIEW

To have an intuitive, natural and ﬂuid interaction with its
human partners a robotic system needs to integrate the skills
described in the previous section. However, these skills need
to be adapted, since a robot may have different capacities than
humans, and since humans’ behaviors may be different when
interacting with a robot, compared to when interacting with
another human.

A. Building and maintaining common ground

The ﬁrst skill needed by the robot to interact properly with
a human is to provide means to understand each other in
the situated context of the joint activity. The robot and the
human need to share a common ground, meaning that they
can identify, in their own world representation, the actions
and objects referred to by their counterpart. Robotics systems
rely on sensors to recognize and to localize entities (humans,
robots or objects) in order to build a world state. These sensors
produce coordinates to position the objects according to a
given frame. For example, a stereo camera with an object
recognition software can tell that a mug is in a given position
x, y, z with an orientation of θ, φ, ψ. Humans, instead, use
relations between objects to describe their position. Therefore,
to indicate the location of a mug,
the human would say
that it is on the kitchen table, without giving its coordinates
or orientation. To understand the human references and to
generate understandable utterances, the robot needs therefore
to build a semantic representation of the world, based on the
geometric data it collects from sensors, as done in [12].

We have developed a module based on spatial and temporal
reasoning to generate ”facts” about the current state of the
world [13]. A fact is a property which describes the current
state of an entity (e.g. M U G isN extT o BOT T LE, M U G
isF ull T RU E). This framework generates facts related to
the entities’ position and facts about affordances to know, for
instance, what is visible or reachable to each agent (human
and robot). It also generates facts about agent postures to
know if an agent is pointing toward an object or where an
agent
tries to understand the
human, it should also use these data to improve the information
grounding process.

is looking. When the robot

In addition to the world state (which can be considered
as the robot’s belief state), our situation assessment module
maintains a separate and consistent belief state for each hu-
man, updated by estimating what the human is able to perceive
of the world state. This belief state is an estimation of the list
of facts that the agent believes to be true. In psychology this
ability is called conceptual perspective taking. In [14] we have
used this framework along with a dialog system to implement
a situated dialog and consequently to improve dialog in term
of efﬁciency and success rate.
B. Joint goal establishment

Once the common ground is established and maintained,
the robot needs to share a goal with its human partners.
This goal can be imposed by a human (direct order) but the
robot should be able to proactively propose its help when
needed. To do so, the robot needs to be able to infer high-
level goals by observing and reasoning on its human partners’
activities. This process is called plan recognition or, when
a bigger focus is put on Human-Robot Interaction (HRI)
aspects, intention recognition. There is a rich literature on plan
recognition, using approaches such as classical planning [15],
probabilistic planning [16] or logic-based techniques [17]. In
the context of intention recognition work such as [18], and [19]

introduced theory of mind aspects in plan recognition. In our
system we provide an intention recognition system, using as
inference model a Bayesian Network, which links contextual
information, intentions, actions, and low-level observations. A
key aspect of the system is using Markov Decision Processes
(MDPs) to link intentions to human actions. Each MDP repre-
sents an intention, and is able to compute what is the action,
in a given situation, that a rational agent would execute to
achieve the associated goal. To properly infer a human agent’s
intention, it is important to consider his current estimated
mental belief in the recognition process, as the actions carried
out by the human may be consistent with his intention in his
mental representation of the world but not in the actual world
state. Our system is able to track and maintain, as discussed
earlier, a belief state for each human, which is used as current
state for the MDPs when computing the action rewards.

C. Elaborating and negotiating shared plans

After establishing a joint goal, in order to achieve it, the
robot should be able to create, and negotiate a shared plan
[20] with its human partners. To do so, there are several
possibilities. The human can choose a speciﬁc plan, which
the robot needs to be able to understand. Otherwise the robot
can compute a plan on its own, which may be negotiated with
the human.

1) Shared plans elaboration: In our system, the robot can
generate plans using a high level task planner, called Human-
Aware Task Planner (HATP) [21], which is able to compute
a shared plan for a given goal in a given context. HATP can
plan actions not only for the robot, but also for its human
partners. The computed plan takes into account social rules
(such as effort sharing) and also the knowledge of the other
agents. For example, in some applications and contexts, the
system could try to teach new ways of solving a problem to
the human. In that case, the robot will favor plans where the
human has to perform new tasks in order to make him learn by
experience. The system could also choose that the efﬁciency
is more important than learning in a given situation, in which
case HATP will generate a plan to minimize the number of
unknown tasks to be performed by the human [22].

2) Sharing and negotiation: Once the plan found, it needs
to be shared/negotiated with the collaborator in order to be
accepted by him. When dealing with simple plans, infants can
cooperate without language (using shared attention, intention
recognition and social cues). In situations requiring more
complex ones, language is the preferred modality [23], [24].
There are two possible situations studied in HRI. First,
the human has a plan and needs to share it with the robot.
Some research in robotics studied how a system could acquire
knowledge on plan decomposition from a user [25] and how
dialog can be used to teach new plans to the robot and to
modify these plans [26]. When the robot is the one who has
to share its generated plan with the human, to ensure his
acceptability and comfort, it should allow the user to contest
the plan and to formulate requests on the plan. To do so we
have devised a module able to present the plan higher-level

tasks and to ask for the user’s approval [22]. The user can
also refuse the plan and ask to perform (or not) speciﬁc tasks,
which would result in a new plan generation from the robot,
trying to take into account the user preference concerning task
allocation.

D. Executing shared plans and reacting to humans

1) Maintaining and executing shared plans: When both
agents agree on a plan, the execution process may start. During
this execution, the robot needs to coordinate with its human
partner. This must be achieved both at task planning level,
by synchronizing the robot’s plan with the human’s, and at a
lower level, by adapting the robot’s action to the human. In the
literature, executives like Pike [27] and Chaski [28], explicitly
model humans in their plans and allow the robot to adapt
its behavior to their actions. In our system, as explained in
[29], we are able, using monitoring and task preconditions, to
perform plan-level coordination. The robot executive decides
when to execute the robot actions and assure, at each time,
that the plan is still feasible. In case of an unexpected behavior
of a human, leading to a plan that is non-valid anymore, the
robot is able to quickly ﬁnd a new plan and adapt to the new
situation.

Moreover, the robot should take the human knowledge into
account when managing the shared plan. For example, during
the execution, it may happen that a human might not know how
to perform a task. In this case, the robot should be able to guide
the user to perform the requested task. To make this possible
and to adapt the explanation to the human’s knowledge on the
task to perform, we have created a component which models
and maintains the human’s knowledge on each task. When a
human has to perform a task, the system will check if he knows
how to perform it, and adapt its behavior to this information
by explaining or not the task [22].

The robot executive also models and maintains humans
mental states concerning goals, plans and actions. For exam-
ple, at each time, the robot estimates if its partners are aware
of the current plan and if they know which actions have been
performed and which ones remain to be done. Then, the robot
uses these mental states to detect when a divergent belief can
affect the smooth execution of the shared plan (for example if
a human does not know that he has to perform an action) and
communicates the needed information to inform the human
and hence, correct his belief [30].

2) Actions recognition, execution and coordination: At a
lower level,
the robot needs to correctly interpret human
signals and actions, perform its action in a legible, safe and
acceptable way, and, when an action is a joint action (e.g.
handover), coordinate its execution with its human partner.

In order to perform actions, in our system, the robot is
equipped with a human-aware geometric task and motion
planner [31] (both for navigation or manipulation tasks). It
also has real time control algorithms to react quickly and stop
its movements. As an example, when both the human and
itself are trying to execute manipulation operations in the same

workspace, the robot would wait for the human to achieve his
action to prevent any dangerous situation for the human.

To recognize human actions, the robot needs to observe
human movements and infer what action is being performed.
An interesting idea is using the robot’s own internal models
in order to recognize actions and predict user intents, as
shown by the HAMMER system in [32]. In our system we
perform this process with geometrical reasoning, linking the
position and movement of human joints to points in areas in
the environments, like objects and rooms.

When the action is a joint action, like an handover, our
system uses Partially Observed Markov Decision Processes
(POMDP) to achieve action level coordination and estimate,
from observations, the human’s engagement level in the action,
and to react accordingly [29]. This mechanism was applied in
[33] to guide a human to a destination in a socially acceptable
way, for example by adapting the robot’s speed to the human’s
needs. Our robot is also able to produce proactive behavior
when executing joint actions such as suggesting a solution by
extending its arm when it needs to perform an handover [34].
An other way to coordinate is non-verbal communication. In
[35], Breazeal et al. demonstrate that explicit (expressive social
cues) as well as implicit (communication of implicit mental-
state through non-verbal behaviours) communication is useful
and necessary if we want a ﬂuent human-robot interaction. The
robot needs to be able both to detect and interpret the non-
verbal cues given by its partners and to produce non-verbal
signals in an understandable and pertinent way. However, these
signals need to be adapted to the physiognomy of the robot: for
example, for a robot with a head and without eyes, it has been
established that gaze signals can be replaced by the orientation
of the head [36].

IV. AN ARCHITECTURE FOR A COGNITIVE AND

INTERACTIVE ROBOT

We discuss here below how the skills described above
can be combined in key elements of a cognitive architecture
for a collaborative robot. Pacherie describes a three-level
architecture used to monitor human-human joint actions [7].
It consists of a Shared Distal level which deals with joint
intention issues (commitment, creation and monitoring of a
shared plan), a Shared Proximal level which deals with the
execution of the plan actions coming from the Distal level at
a high level, and a Coupled Motor intention which ensures the
coordination in space and time during the action execution.

In robotics, Alami et al. [37] drew an architecture with
three similar levels. Then, this architecture has been developed
in the context of Human-robot interaction [38], [39], [29]
integrating more and more human-aware skills. Starting from
these architectures we tried to design a cognitive system
for human-robot interaction. This architecture (Figure 1) is
intended to equip the robot with all the skills described in the
previous section.

First, the architecture would require a Situation Assessment
component to understand the environment in which the interac-
tion takes place. To do so, this component needs to collect data

a plan with humans.

Finally, the architecture should contains a component, that
we call Shared Plan Achievement, which ensures the smooth
execution of the shared plan. This component should consider
other agents’ mental states to coordinate with them and com-
municate information when needed. It will send a request to
the Action Achievement component to perform robot actions
and gather information from Human Actions Monitoring
which will interpret humans movements and recognize humans
actions. The Action Achievement component should execute
the robot’s actions in a legible, safe and acceptable way and,
when those actions are joint actions (e.g. handover), it should
ensure the proper coordination with the human partners.

V. CONCLUSION

In this paper, we brieﬂy reviewed a set of skills that we
consider as essential for joint action. Then we discussed
how to apply these skills to HRI and how to integrate them
into a cognitive architecture. However, in the current robotics
context, we are still far from a real cognitive architecture
able to perform ﬂuid and friendly Human-Robot joint actions.
Each of this skills can be more or less sophisticated and each
them is a topic of research by itself. The next step should
be to design and implement, with the robotic community, a
complete cognitive architecture inspired from social sciences
and neuroscience but adapted to the capacities of a robot and
to the behavior of humans, when they interact with robots.

REFERENCES

[1] A. Clodic, R. Alami, R. Chatila, and L. aCNRS, “Key elements
for human-robot joint action,” Frontiers in Artiﬁcial Intelligence and
Applications, vol. 273, pp. 23–33, 2014.

[2] M. Tomasello, M. Carpenter, J. Call, T. Behne, and H. Moll, “Un-
derstanding and sharing intentions: The origins of cultural cognition,”
Behavioral and brain sciences, vol. 28, no. 05, pp. 675–691, 2005.

[3] M. E. Bratman, “Shared intention,” Ethics, pp. 97–113, 1993.
[4] D. G. Pruitt, Negotiation behavior. Academic Press, 2013.
[5] H. H. Clark, R. Schreuder, and S. Buttrick, “Common ground at the
understanding of demonstrative reference,” Journal of verbal learning
and verbal behavior, vol. 22, no. 2, pp. 245–258, 1983.

[6] N. Sebanz, H. Bekkering, and G. Knoblich, “Joint action: bodies and
minds moving together,” Trends in cognitive sciences, vol. 10, no. 2,
pp. 70–76, 2006.

[7] E. Pacherie, “The phenomenology of joint action: Self-agency vs. joint-

agency,” Joint attention: New developments, pp. 343–389, 2012.

[8] S. Baron-Cohen, A. M. Leslie, and U. Frith, “Does the autistic child

have a theory of mind?,” Cognition, vol. 21, no. 1, pp. 37–46, 1985.

[9] G. Knoblich, S. Butterﬁll, and N. Sebanz, “Psychological research on
joint action: theory and data,” Psychology of Learning and Motivation-
Advances in Research and Theory, vol. 54, p. 59, 2011.

[10] J. J. Gibson, “The theory of affordances,” Hilldale, USA, 1977.
[11] C. Vesper, S. Butterﬁll, G. Knoblich, and N. Sebanz, “A minimal
architecture for joint action,” Neural Networks, vol. 23, no. 8, pp. 998–
1003, 2010.

[12] S. Lemaignan, R. Ros, E. A. Sisbot, R. Alami, and M. Beetz, “Grounding
the interaction: Anchoring situated discourse in everyday human-robot
interaction,” International Journal of Social Robotics, vol. 4, no. 2,
pp. 181–199, 2012.

[13] G. Milliez, M. Warnier, A. Clodic, and R. Alami, “A framework
for endowing an interactive robot with reasoning capabilities about
perspective-taking and belief management,” in Int. Symp. on Robot and
Human Interactive Communication, pp. 1103–1109, IEEE, 2014.

[14] E. Ferreira, G. Milliez, F. Lefevre, and R. Alami, “Users’ belief aware-
learning-based situated human-robot dialogue

ness in reinforcement
management,” in IWSDS, 2015.

Fig. 1: A cognitive architecture for Human-Robot Interaction.

from sensors and to perform spatial and temporal reasoning to
describe the environment in term of predicates and properties.
These properties should describe the situation of each entities
in term of relational position (BOB isIn KITCHEN), and state
property (MUG isEmpty FALSE). It should also describe the
situation of agents in term of posture (BOB isPointing MUG),
affordance (BOB canReach MUG), and motion or actions
(BOB isMovingToward MUG).

We previously described how conceptual perspective taking
is important to understand humans’ utterance or assess hu-
mans’ intentions. To build and maintain a separate and consis-
tent mental state for each agent, a Mental State Management
component is needed. Each Mental state consists of a set of
facts related to the environment, like the one generated by the
Situation Assessment component, but also of facts concerning
what the agent knows about other agents’ knowledge on the
goal, the plan and tasks included in the plan and about the
agents’ capacities.

The architecture also requires a component dealing with
Communication for Joint Actions. Based on the information
concerning the agents’ mental states, this component should
provide situated dialogue and perspective taking skills when
discussing with humans or sharing/negotiating a plan. This
component should also allow the robot to produce human-like
non-verbal signals and to correctly recognize and interpret the
signals of its human partners.

The architecture should also involve a component to deter-
mine the robot’s current goal. This is the Intention Prediction
component. First, this component should be able to recognize
humans’ intentions, based on the estimation of their mental
states and communicative feedback. Then it should estimate
if the robot can and has to help humans and choose the most
appropriate goal to execute. During the execution of the chosen
goal, this component should estimate if the humans involved
into the goal are still (or not) committed to it.

Once a goal is chosen, a Shared Plan Elaboration compo-
nent would allow the robot to agree on a shared plan with its
partners. Therefore, this component should contain a high level
task planner, like HATP, capable of computing human-aware
plans considering social costs and other agents mental states.
This component should also allow the robot to share/negotiate

[15] M. Ramırez and H. Geffner, “Plan recognition as planning,” in Proceed-
ings of the 21st international joint conference on Artiﬁcal intelligence.
Morgan Kaufmann Publishers Inc, pp. 1778–1783, Citeseer, 2009.

[16] H. H. Bui, “A general model for online probabilistic plan recognition,”

in IJCAI, vol. 3, pp. 1309–1315, Citeseer, 2003.

[17] P. Singla and R. J. Mooney, “Abductive markov logic for plan recogni-

tion.,” in AAAI, 2011.

[18] C. Breazeal, J. Gray, and M. Berlin, “An embodied cognition approach
to mindreading skills for socially intelligent robots,” The International
Journal of Robotics Research, vol. 28, no. 5, pp. 656–680, 2009.

[19] C. L. Baker and J. B. Tenenbaum, “Modeling human plan recognition
using bayesian theory of mind,” Plan, activity, and intent recognition:
Theory and practice, pp. 177–204, 2014.

[20] B. J. Grosz and C. L. Sidner, “Plans for discourse,” tech. rep., DTIC

Document, 1988.

[21] R. Lallement, L. De Silva, and R. Alami, “Hatp: An htn planner for

robotics,” arXiv preprint arXiv:1405.5345, 2014.

[22] G. Milliez, R. Lallement, M. Fiore, and R. Alami, “Using human
knowledge awareness to adapt collaborative plan generation, explanation
and monitoring,” in ACM/IEEE International Conference on Human-
Robot Interaction, HRI’16, New Zealand, March 7-10, 2016.

[23] F. Warneken, F. Chen, and M. Tomasello, “Cooperative activities in
young children and chimpanzees,” Child Development, pp. 640–663,
2006.

[24] F. Warneken and M. Tomasello, “Helping and cooperation at 14 months

of age,” Infancy, vol. 11, no. 3, pp. 271–294, 2007.

[25] A. Mohseni-Kabir, C. Rich, S. Chernova, C. L. Sidner, and D. Miller,
“Interactive hierarchical task learning from a single demonstration,” in
Tenth Annual ACM/IEEE International Conference on Human-Robot
Interaction, HRI ’15, ACM, 2015.

[26] M. Petit et al., “The coordinating role of language in real-time multi-
modal learning of cooperative tasks,” Autonomous Mental Development,
IEEE Transactions on, vol. 5, pp. 3–17, March 2013.

[27] E. Karpas, S. J. Levine, P. Yu, and B. C. Williams, “Robust execution of
plans for human-robot teams,” in Twenty-Fifth International Conference
on Automated Planning and Scheduling, 2015.

[28] J. Shah, J. Wiken, B. Williams, and C. Breazeal, “Improved human-
robot team performance using chaski, a human-inspired plan execution
system,” in international conference on Human-robot interaction, 2011.

[29] M. Fiore, A. Clodic, and R. Alami, “On planning and task achievement
modalities for human-robot collaboration.,” in The 2014 International
Symposium on Experimental Robotics, 2014.

[30] S. Devin and R. Alami, “An implemented theory of mind to improve
human-robot shared plans execution.,” in ACM/IEEE International Con-
ference on Human-Robot Interaction, HRI’16, New Zealand, March 7-
10, 2016.

[31] E. A. Sisbot and R. Alami, “A human-aware manipulation planner,”
Robotics, IEEE Transactions on, vol. 28, no. 5, pp. 1045–1057, 2012.
[32] Y. Demiris, “Prediction of intent in robotics and multi-agent systems,”

Cognitive processing, vol. 8, no. 3, pp. 151–158, 2007.

[33] M. Fiore, H. Khambhaita, G. Milliez, and R. Alami, “An adaptive and
proactive human-aware robot guide,” in Social Robotics, pp. 194–203,
Springer International Publishing, 2015.

[34] A. K. Pandey, M. Ali, and R. Alami, “Towards a task-aware proactive
sociable robot based on multi-state perspective-taking,” International
Journal of Social Robotics, vol. 5, no. 2, pp. 215–236, 2013.

[35] C. Breazeal, C. D. Kidd, A. L. Thomaz, G. Hoffman, and M. Berlin,
“Effects of nonverbal communication on efﬁciency and robustness in
human-robot teamwork,” in Intelligent Robots and Systems, 2005.(IROS
2005). 2005 IEEE/RSJ International Conference on, pp. 708–713, IEEE,
2005.

[36] J.-D. Boucher, U. Pattacini, A. Lelong, G. Bailly, F. Elisei, S. Fagel,
P. F. Dominey, and J. Ventre-Dominey, “I reach faster when i see
you look: gaze effects in human–human and human–robot face-to-face
cooperation,” Frontiers in neurorobotics, vol. 6, 2012.

[37] R. Alami, R. Chatila, S. Fleury, M. Ghallab, and F. Ingrand, “An archi-
tecture for autonomy,” The International Journal of Robotics Research,
vol. 17, no. 4, pp. 315–337, 1998.

[38] R. Alami, M. Warnier, J. Guitton, S. Lemaignan, and E. A. Sisbot,
“When the robot considers the human...,” in Proceedings of the 15th
international symposium on robotics research, 2011.

[39] R. Alami, “On human models for collaborative robots,” in Collaboration
Technologies and Systems (CTS), 2013 International Conference on,
pp. 191–194, IEEE, 2013.

