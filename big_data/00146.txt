Storm Detection by Visual Learning Using

1

Satellite Images

Yu Zhang, Stephen Wistar, Jia Li, Michael Steinberg and James Z. Wang

6
1
0
2

 
r
a

M
1

 

 
 
]

V
C
.
s
c
[
 
 

1
v
6
4
1
0
0

.

3
0
6
1
:
v
i
X
r
a

Abstract

Computers are widely utilized in today’s weather forecasting as a powerful tool to leverage an
enormous amount of data. Yet, despite the availability of such data, current techniques often fall short
of producing reliable detailed storm forecasts. Each year severe thunderstorms cause signiﬁcant damage
and loss of life, some of which could be avoided if better forecasts were available. We propose a
computer algorithm that analyzes satellite images from historical archives to locate visual signatures of
severe thunderstorms for short-term predictions. While computers are involved in weather forecasts to
solve numerical models based on sensory data, they are less competent in forecasting based on visual
patterns from satellite images. In our system, we extract and summarize important visual storm evidence
from satellite image sequences in the way that meteorologists interpret the images. In particular, the
algorithm extracts and ﬁts local cloud motion from image sequences to model the storm-related cloud
patches. Image data from the year 2008 have been adopted to train the model, and historical thunderstorm
reports in continental US from 2000 through 2013 have been used as the ground-truth and priors in the
modeling process. Experiments demonstrate the usefulness and potential of the algorithm for producing
more accurate thunderstorm forecasts.

A. Background

I. INTRODUCTION

The numerical weather prediction (NWP) approach has been applied to weather forecasting
since the 1950s [1], when the ﬁrst computer ENIAC was invented [2]. John von Neumann, a

Yu Zhang and James Z. Wang are with the College of Information Sciences and Technology, The Pennsylvania State University,

University Park, PA 16802, USA.

Stephen Wistar and Michael Steinberg are with AccuWeather Inc., State College, Pennsylvania, USA
Jia Li is with the Department of Statistics, Eberly College of Science, The Pennsylvania State University, University Park,

PA 16802, USA.

Manuscript received –; revised –.

2

pioneering computer scientist and meteorologist, ﬁrst applied the primitive equation models [3]
on ENIAC. Since then meteorologists have produced reliable weather forecasts, and they have
continued to reﬁne the numerical models, thereby improving the NWP in recent decades. Today,
the model of the European Centre for Medium-range Weather Forecasts (ECMWF) and the
United States’ Global Forecast System (GFS) are two of the most accurate NWP models. At the
500 hPa geopotential height (HGT) level of the atmosphere, both models are reported to have
annual mean correlation coefﬁcients of nearly 0.9 for 5-day forecasts [4]. Other models, such
as UK Met Ofﬁce Uniﬁed model [5], M´et´eo-France ARPEGE-Climat model [6], and Canada
Meteorological Centre model [7], are developed and play important roles in global and regional
weather predictions.

Even though NWP can effectively produce accurate weather forecasts of the general weather
pattern, it is not always reliable in the prediction of extreme weather events, such as severe
thunderstorms, hail storms, hurricanes, and tornadoes, which cause a signiﬁcant amount of
damage and loss of life every year worldwide. Unreliable predictions, either miss or false alarm
ones, could cause massive amounts of loss to the society. The most well-known failed prediction
is the Great Storm of 1987 in UK [8]. The Met Ofﬁce was criticized for not predicting the
storm timely. More recently, the landfall of 2012 Hurricane Sandy in the northeastern US was
not correctly predicted by the National Hurricane Center until two days before it came ashore [9].
In January 2015, the impact of winter storm Juno was overestimated by the US National Weather
Service [10]. Several cities in US East Coast suffered from unnecessary travel bans and public
transportation closures. Much of the loss of life and property due to improper precautions to
severe weather events could be avoided with more accurate (for both severity and location)
weather forecasts.

The numerical methods used in NWP can efﬁciently process large amounts of meteorological
measurements. However, a major drawback of such methods is that they are sensitive to noise
and initial inputs due to the complexity of the models [11], [12]. As a result, although nowadays
we have powerful computers to run complex numerical models, it is difﬁcult to get accurate
predictions computationally, especially in the forecasts of severe weather. To some extent, they
do not interpret the data from a global point of view at a high cognitive level. For instance,
meteorologists can make good judgments of the future weather conditions by looking at the
general cloud patterns and developing trends from a sequence of satellite images by using geo-
graphical knowledge and their experience of past weather events [13], [14]; numerical methods

3

do not capture such high-level clues. Additionally, historical weather records provide valuable
references for making weather forecasts, but numerical methods do not make best use of them.
To address this weakness of numerical models, we develop a computational weather forecast
method that takes advantage of both the global visual clues of satellite data and the historical
records. In particular, we try to ﬁnd synoptic scale features of mid-latitude thunderstorms (to be
speciﬁed later in Section I-C) by computer vision and machine learning approaches. It is worth
mentioning that the purpose of this work is not to replace numerical models in forecasting.
Instead, by tackling the problem using different data sources and modeling methodologies, we
aspire to develop a method that can potentially be used side-by-side with and complement
numerical models.

We analyze the satellite imagery because it provides important clues as meteorologists view
evolving global weather systems. Unlike conventional meteorological measures such as tempera-
ture, humidity, air pressure, and wind speed, which directly reﬂect the physical conditions at the
sensors’ locations, the visual information in satellite images is captured remotely from the orbit,
which means a larger geographic coverage with good resolution. The brightness in an infrared
satellite image indicates the temperature of the cloud tops [15] and therefore reveals information
about the top-level structure of the cloud systems. Manually interpreting such visual information,
we can trace the evidence that corresponds to certain weather patterns, particularly those related
to severe thunderstorms. Moreover, certain cloud patterns can be evident in the early stage of
storm development. As a result, the satellite imagery are very helpful to meteorologists in their
work.

Human eyes can effectively capture the visual patterns related to different types of weather
events. The interpretation requires a high level understanding of meteorological knowledge, which
has been difﬁcult for computers. However, because the broad spatial and temporal coverage of
the satellite images produces too much information to be fully processed, there is an emerging
need for a computerized way to analyze the satellite images automatically. Therefore researchers
are seeking computer vision algorithms to automatically process and interpret the visual features
from the satellite images, aiming to help meteorologists better analyze and predict weather
events. Traditional satellite image analysis techniques include cloud detection with complex
backgrounds [16], cloud type classiﬁcation [17]–[19], detection of cloud overshooting top [20],
[21], and tracking of cyclone movement [22]. These approaches, however, only capture local
visual features without utilizing many high-level global visual patterns, and overlook their

4

development over time, which provides more clues for weather forecasts. With the development
of advanced computer vision algorithms, some recent techniques put more focus on the analysis
of cloud motion and deformation [23]–[26]. In these approaches, cloud motions are estimated
to match and compare cloud patches between adjacent images in a sequence. In this paper, the
proposed algorithm also uses cloud motion estimation in image sequences. It is different from
existing work in that it extracts and models certain patterns of cloud motion, in addition to
capturing the cloud displacement.

Historical satellite data archives as well as meteorological observations are readily available
for recent years. By analyzing large amounts of past satellite images and inspecting the historical
weather records, our system takes advantage of big data to produce storm alerts given a certain
query image sequence. The result provides an alternative prediction that can validate and correct
the conventional forecasts. It can be embedded into an integrative or information fusion system,
as suggested in [27], to work with data and decisions from other sources and produce more
improved storm forecasts.

B. The Dataset

For our research, we acquired all of the GOES-M [28] satellite imagery for the year of 20081,
which was an active year containing abundant severe thunderstorm cases for us to analyze.
The GOES-M satellite moves on a geostationary orbit and was continually facing the North
America area during that year. In addition to the imagery data, each record contains navigation
information, which helps us map each image pixel to its real geo-location. The data is multi-
spectral and contains ﬁve channels covering different ranges of wavelengths, among which we
adopted channel 3 (6.5-7.0µm) and channel 4 (10.2-11.2µm) in our analysis because these two
infrared channels are available both day and night. Both channels are of 4 km resolution and
the observation frequency is four records per hour, i.e., about 15 minutes between the adjacent
images. To make sure two adjacent images in a sequence have a noticeable difference, we
sampled the original sequence and used a sub-sequence with a 2-frames-per-hour frame rate.
We select the 30-minute sampling interval because the algorithm is based on the optical ﬂow
analysis between to adjacent image frames. We need both evident enough visual difference and

1The satellite imagery is publicly viewable and the raw data archive can be requested on the website of US National Oceanic

and Atmospheric Administration (NOAA).

5

relatively good temporal resolution. The 30-minute interval is a empirical selection that considers
both factors. In the rest of the paper, every two adjacent images in a sequence we refer to are
30 minutes away without specifying.

Using the imagery and navigation data blocks in each raw data entry, we reconstruct satellite
images by an equirectangular projection [29]. The pixel mapping from the raw imagery data to
the map coordinate is computed by the navigation data with transformation deﬁned by [28]. The
map resolution is set to be 4 km (per pixel), which best utilizes the information of the raw data.
We reconstruct the aligned images within the range of 60°W to 124°W in longitude and 20°N
to 52°N in latitude, which covers the Continental US (CONUS) area. Hereafter all the image
related computations in our approach are performed on the reconstructed images, on which the
geo-location of each pixel is directly accessible.

We studied the relationship between satellite images and severe thunderstorms, which may
include hailstorms and tornados2. To relate the satellite data with severe thunderstorms, we
retrieved the storm report data from the NOAA National Weather Service3, where all the time
and locations of storms inside the United States since the year of 2000 are archived. The records
act as ground-truth data in the training and provide geographic and temporal priors for the system
to make decisions.

C. Synoptic-scale Visual Storm Signatures

Being evidently visible on the satellite imagery, synoptic-scale (spanning several hundred
kilometers) storm features are studied in this paper. Particularly, we are interested in the synoptic-
scale features of the mid-latitude (30◦-60◦ in latitude) thunderstorms located outside of the Hadley
cell [30] of the atmospheric circulation because storms cooccurring with these features are one
of the major components of storms in the CONUS and other densely populated areas around
the world.

Synoptically, one important atmospheric feature helpful for locating thunderstorms is the
subtropical jet stream4, which is the high-altitude (10-16 km) westerly wind near the boundary

2Severe thunderstorms mostly contain hailstorms and tornados. Storms with hail and/or tornadoes are a subset of thunderstorms.

Hereafter we generally refer to severe thunderstorms as storms.

3http://www.spc.noaa.gov/climo/online/
4Hereafter we refer to the subtropical jet stream as the jet stream for simplicity, regardless of the existence of the polar jet

stream.

6

between the Hadley cell and the Ferrel cell; in other words, the cold and warm air masses. Jet
streams move in a meandering path from west to east, a direction of movement caused by the
Coriolis effect [31]. In the northern hemisphere, elongated cold air masses with low pressure,
namely “troughs”, will have a dip in the jet stream extending southward at various longitudes
around the earth while in other locations a ridge of higher pressure with warm air will send the jet
stream bulging northward. The east side of a trough, which has a southwest to northeast oriented
jet stream, is a region of active atmospheric storminess [32], where cold and warm air masses
collide. Though jet streams themselves are invisible, they can be revealed by thick cumulus clouds
gathering in the unsettled trough regions and areas of clouds spreading northeastward along the
east side of a trough. As a result, an elongated cloud-covered area along a southwest to northeast
direction is a useful large-scale clue for us to locate storms. Fig. 1 shows a GOES-M channel-4
image taken on February 5, 2008. Clearly in the mid-latitude region the southern boundary of the
cloud band goes along a smooth curve, which indicates the jet stream (sketched in dashed line).
Usually the cloud-covered areas to the northwest side of such an easily-perceivable jet stream are
susceptible to storms. In this case, severe thunderstorms developed two hours after the time of
the image in the boxed area of the ﬁgure. Based on our observation, a large proportion of storms
in the CONUS area have similar cloud appearances as in this example. As a result, ﬁnding jet
streams in the satellite image is important for storm prediction [33].

A jet stream travels along a large-scale meandering path, whose wavelength is about 60◦ to
120◦ of longitude long, as wide as a whole continent. The full wavelength is therefore usually
referred to as a long wave. The trough within a long wave only tells us a general large region
that is vulnerable to storms. To locate individual storm cells more precisely, meteorologists look
for more detailed smaller scale cloud patterns caused by short waves traveling through the long
wave. Short waves are synoptic scale features with the wavelength of the order of 1,000 km,
and indicate the disturbance of the mid or upper part of the atmosphere. Horizontally seen from
a satellite image, they appear as smaller waves in the long wave trough. In many cases, we can
observe clouds in “comma shapes” [34], [35] in short waves, i.e., such a cloud has a round
shape due to the circular wind in its vicinity and a short tail towards the opposite direction of
its translation. A comma-shaped cloud patch indicates a turbulent area of rising air often leading
to convection (thunderstorms), especially if it lies on the boundary between cold and warm air
masses. Fig. 1 shows an example of a comma-shaped cloud. A storm area is marked by an
square in the tail of the comma shape in the ﬁgure. Because of its distinctive pattern, the comma

7

shape has been one of the most utilized signature when meteorologists review satellite images
for thunderstorms.

It is necessary to mention that the comma shape is still a signature in the synoptic scale and
therefore used for generally locating a storm system rather than individual storms5. In other
words, groupings of thunderstorms tend to have a comma shape and the shape is used as a ﬁrst
approximation of where severe weather is most likely to occur.

The visual cloud patterns introduced above can often be detected from a single satellite image.
However, a single image is not sufﬁcient to provide all the crucial information about the dynamic
of a storm system. The cloud shapes are not always clear due to the irregular nature of clouds.
In addition, the evolution of storm systems, including cloud areas emerging, disappearing, and
deforming, cannot be revealed unless different satellite images in a sequence are compared.

In practice, an important step in producing weather forecasts is to review satellite images,
during which process meteorologists typically review a series of images, rather than a single
one, to capture critical patterns. They use the differences between the key visual clues (e.g., key
points, edges, etc.) to track the development of storm systems and better locate the jet streams
and comma shapes, among other features. As mentioned in [36], human heuristics are helpful
in improving weather forecasts. Following such guideline, our algorithm attempts to simulate
meteorologists’ cognitive processes in analyzing temporally adjacent satellite image frames. In
particular, two types of motions are regarded crucial to the storm visual patterns: the global
cloud translation with the jet stream and the local cloud rotations producing the comma shapes.
We consider both types of motions for extracting synoptic storm signatures.

D. Objectives

The algorithm proposed in this paper aims at assisting the prediction of severe thunderstorms
from a different perspective from the NWP. As an initial attempt in this direction, we focus
on short-term severe thunderstorm prediction. Synoptical scale features abstracted from known
visual storm patterns as aforementioned are extracted from satellite images and used as clues
to locate regions where thunderstorms tend to happen in the near future. Machine learning is
involved in the classiﬁcation of candidate storm regions by inspecting historical thunderstorm

5Individual thunderstorms typically develop comma shapes to their clouds, but not until they have become e more mature.
Such small-scale scale signatures are also more difﬁcult to detect in the satellite image and therefore not the focus of this paper.

8

Fig. 1. A sample GOES-M satellite channel 4 image taken at 16:02 (GMT) on February 5, 2008. The boxed area underwent a
severe thunderstorm later on that day. The jet stream (marked in dashed curve) can be implied from the distribution of clouds.
The storm area is covered by a comma-shaped cloud patch (surrounded by dashed boundary) in the trough region.

data. The regions, as extracted from synoptical scale features, are not necessarily associated one-
to-one with storms, i.e., a region may contain several storms and each storm needs to be more
precisely located by other techniques. The algorithm in fact simulates meteorologists’ cognition
process when reviewing satellite images as a part of the weather prediction process. It could be
a helpful tool in automating and reﬁning the current weather prediction. It can also be used to
provide assistance to meteorologists so that they can be more efﬁcient and less exhausting.

E. Outline

The rest of the paper is organized as follows. Section II introduces the approach to extract
storm signatures and construct storm features. Section III reports the approach and results of
the machine learning module that accepts or rejects the extracted storm features. In particular,
quantitative benchmarks of the classiﬁer and qualitative case studies that applies the whole
workﬂow are given to demonstrate the effectiveness of the proposed algorithm. Finally, we
present the future work and conclude the paper in Section IV.

II. STORM FEATURE EXTRACTION

Our algorithm extracts synoptic-scale visual thunderstorm features from satellite images. The
cloud motion observed from image sequences is analyzed in depth. In order to simulate human

9

cognition and let computers perceive comma shapes introduced earlier, the visual signatures
are further abstracted into basic cloud motion components: translation and rotation. Several
properties and measurements related to these two components are extracted from cloud motions
and analyzed in the following steps.

Fig. 2 illustrates the workﬂow of the whole system. We employ the optical ﬂow between every
two adjacent satellite images to capture the cloud motion and discover vortex areas that could
potentially trigger storms. Using the historical storm records, vortex features are constructed and
a storm classiﬁer is trained. Given an arbitrary query image sequence, vortexes are extracted in
the same way and then categorized by the classiﬁer.

In particular, for the feature extraction operations, which will be applied both to the training
data and the query data, two steps are carried out. The system ﬁrst estimates a dense optical
ﬂow ﬁeld describing the cloud motion between adjacent image frames. Second, local vortexes
are identiﬁed with the optical ﬂow and vortex descriptors are constructed. The vortex descriptors
combine information from both the visual features and the historical storm records. The following
subsections describe details of these steps.

A. Robust Optical Flow Estimation

Optical ﬂow is a basic technique for motion estimation in image sequences. Given two images
It(x, y) and It+1(x, y), the optical ﬂow (cid:126)Ft(x, y) = {Ut(x, y), Vt(x, y)} deﬁnes a mapping for
each pixel g : (x, y) (cid:55)→ (x + Ut(x, y), y + Vt(x, y)), so that It(g(x, y)) ≈ It+1(x, y). The
vector ﬁeld (cid:126)Ft(x, y) can be therefore regarded as the pixel-wise motions from image It(x, y) to
It+1(x, y). Several approaches for optical ﬂow estimation have been proposed based on different
optimization criteria [37]–[39]. In this work we adopt the pyramid Lucas-Kanade algorithm [40]
to estimate a dense optical ﬂow ﬁeld between every two neighboring image frames.

The GOES-M satellite is in a geostationary orbit and thus remains over the same point on
the earth at all times. As a result, the only objects moving along a satellite image sequence are
the clouds and the optical ﬂow between two images indicates the cloud motion. However, the
non-rigid and dynamic nature of clouds makes the optical ﬂow estimation noisy and inaccurate.
Fig. 3(a) shows the optical ﬂow estimation result of a satellite image in respect to its previous
frame in the sequence. Though the result correctly reﬂects the general cloud motion, ﬂows in
local areas are usually noisy and do not precisely describe the local motions. As to be introduced
later, the optical ﬂow properties adopted in this work involve the gradient operation of the ﬂow

10

Fig. 2. Workﬂow of the storm detection system. Optical ﬂows between adjacent image frames are estimated and vortexes
are extracted base on the ﬂows. Vortex descriptors are then constructed using both the visual information and historical storm
records. Machine learning is performed on the vortex descriptors for the storm detection.

ﬁeld. Thus it is important to get reliable and smooth optical ﬂow estimation. To achieve this goal,
we both pre-process the images and post-process the estimation results. Before calculating the
optical ﬂow between two images, we ﬁrst enhance both of them using the histogram equalization
technique6 [41]. As a result, more ﬁne details on the images are enhanced for better optical ﬂow
estimation.

After the initial optical ﬂow estimation, we smooth the optical ﬂow by applying an iterative
update operation based on the Navier-Stokes equation [42] for modeling ﬂuid motions. Given a

6 Each channel is enhanced separately. On a given channel, the same equalization function (estimated from the ﬁrst frame of

the sequence) is applied to both images so that they are enhanced by the same mapping.

2008$image$archive$Image$sequence$Op5cal$ﬂow$Vortexes$2000=2013$storm$records$ti$ti+1$ti+2$ti+3$Robust'op)cal'ﬂow'es)ma)on'Vortex'extrac)on'and'descrip)on'2008$records$(ground=truth)$All$records$(priors)$2008$vortex$descriptors$Machine'learning'Feature$extrac5on$Storm$classiﬁer$Query$image$sequence$Vortexes$Storm$detec5on$result$Op5cal$ﬂow$Training$Query$ﬂow ﬁeld (cid:126)F (x, y), the equation describes the evolving of the ﬂow over time t:

∂ (cid:126)F
∂t

= (− (cid:126)F · ∇) (cid:126)F + ν∇2 (cid:126)F + (cid:126)f ,

11

(1)

where ∇ = ( ∂
external force applied at the location (x, y).

∂x , ∂

∂y ) is the gradient operator, ν is the viscosity of the ﬂuid, and (cid:126)f (x, y) is the

The three terms in Eq. (1) correspond to the advection, diffusion, and outer force of the ﬂow
ﬁeld respectively. In the method introduced in [43], an initial ﬂow ﬁeld is updated to a stable
status by iteratively applying these three transformations one by one. Compared with a regular
low-pass ﬁltering to the ﬂow ﬁeld, this approach takes into account the physical model of ﬂuid
dynamics, and therefore better approximates the real movement of a ﬂow ﬁeld. We adopt a
similar strategy to smooth the optical ﬂow in our case. The initial optical ﬂow ﬁeld (cid:126)Ft(x, y) is
iteratively updated. Within each iteration the time is incremented by a small interval ∆t, and
there are three steps to get (cid:126)Ft+∆t(x, y) from (cid:126)Ft(x, y):

t+∆t = adv( (cid:126)F (1)

t+∆t = (cid:126)Ft + (cid:126)f ∆t ;

t+∆t)e−νk2∆t) .

1) add force: (cid:126)F (1)
t+∆t,−∆t) ;
2) advect: (cid:126)F (2)
3) diffuse: (cid:126)Ft+∆t = F F T −1(F F T ( (cid:126)F (2)
The ﬁrst step is simply a linear increment of the ﬂow vectors based on the external force. In
the second step, the advection operator adv(·) uses the ﬂow (cid:126)F 1
t+∆t(x, y) at each pixel (x, y) to
predict its location (xt−∆t, yt−∆t) ∆t time ago, and updates (cid:126)F (2)
t+∆t(x, y) by (cid:126)F (1)
t+∆t(xt−∆t, yt−∆t).
In the last step, the diffusion operation is a low-pass ﬁlter applied in the frequency domain,
where k is the distance from a point to the origin, and the bandwidth of the ﬁlter is determined
by the pre-deﬁned ﬂuid viscosity ν and ∆t (to be speciﬁed later). We do not enforce the mass
conservation like the approach in [43] because the two-dimensional ﬂow ﬁeld corresponding
to the cloud movement is not necessarily divergence free. In fact, we use the divergence as a
feature for storm detection in the following procedures.

After several iterations of the update, the ﬂow ﬁeld converges to a stable status and becomes
smoother. The iteration number is typically not large, and we ﬁnd that the ﬁnal result is not
very sensitive to the parameters ν and ∆t. In our system we set the iteration number to 5, the
ﬂuid viscosity ν to 0.001, and time interval ∆t to 1. The noisy optical ﬂow estimation from
the previous stage is treated as the external force ﬁeld (cid:126)f, and initially (cid:126)Ft = 0. Fig. 3(b) shows
the smoothed ﬂow ﬁeld (i.e., (cid:126)Ft+5∆t) from the noisy estimation (Fig. 3(a)). Clearly the major
motion information is kept and the ﬂow ﬁeld is smooth for further analysis.

12

(a) Noisy optical ﬂow estimation

(b) Smoothed optical ﬂow

(c) Divergence-free component of the smoothed ﬂow

(d) Vorticity-free component of the smoothed ﬂow

(e) Magnitude of vorticity

(f) Vortex cores and their expansions

Fig. 3. Sample results of the optical ﬂow analysis. GOES-M satellite images covering the continental US area are analyzed (to
avoid showing too many missing pixels on the southwest corner, a small stripe of US west coast is not shown on the ﬁgures,
though these pixels are included in our analysis). Between two adjacent frames (with 30 minutes’ interval) in a satellite image
sequence, the optical ﬂow from the former frame to the latter one is estimated and processed. The results are plotted on the second
frame. Only large enough ﬂow vectors are drawn on ﬁgure (a-d). (a) The optical ﬂow estimated by the Lucas-Kanade algorithm.
(b) The smoothed optical ﬂow by applying an iterative update based on the Navier-Stokes equation. (c) The divergence-free
component of the smoothed ﬂow ﬁeld. (d) The vorticity-free component of the smoothed ﬂow ﬁeld. (e) Visualization of the
vorticity. Vorticity vectors of the 2D ﬂow ﬁeld are perpendicular to the image plane. Pixels with vorticity vectors toward the
viewer (counter-clockwise rotations) are tinted in green color; and the pixels with vorticity away from the viewer (clockwise
rotations) are tinted in red color. The saturation of color means the magnitude of the corresponding vorticity vector. (f) Vortex
regions detected by the Q-criterion. For visualization purpose (to avoid displaying too a large missing corner at the lower-left,
not all areas in CONUS are shown on the above images.

B. Flow Field Vortex Extraction

13

As introduced in Section I, the rotating and diverging of local cloud patches are two key
signatures for storm detection. In the ﬂow ﬁeld, these two kinds of evidence are embodied by
the vorticity and divergence. The vorticity of a vector ﬁeld (cid:126)F (x, y) = {U (x, y), V (x, y)} is
deﬁned as:

and the divergence is deﬁned as:

(cid:19)

(cid:126)ω(x, y) = ∇ × (cid:126)F (x, y)
∂
∂z
− ∂U (x, y)

(cid:18) ∂
(cid:18) ∂V (x, y)

∂
∂y

∂x

=

=

,

,

(cid:19)

(cid:126)z ;

∂y

× (U (x, y), V (x, y), 0)

div (cid:126)F (x, y) = ∇ · (cid:126)F (x, y)

(cid:19)

∂x

(cid:18) ∂

=

=

,

∂
∂y
∂U (x, y)

∂x

∂x

· (U (x, y), V (x, y))

+

∂V (x, y)

∂y

.

(2)

(3)

It has been proven that for a rigid body, the magnitude of vorticity at any point is twice
the angular velocity of its self rotation [44] (the direction of the vorticity vector indicates the
rotation’s direction). In our case, even though the cloud is non-rigid, we can regard the vorticity
at a certain point as a description of the local rotation.

To better reveal the rotation and divergence, we apply the Helmholtz-Hodge Decomposi-
tion [45] to decompose the ﬂow ﬁeld to a solenoidal (divergence-free) component and a ir-
rotational (vorticity-free) component. Fig. 3(c) and Fig. 3(d) visualize these two components
respectively. In both ﬁgures, areas with densely overlapped vectors plotted are the places with
high vorticity or divergence.

The divergence-free component of the ﬂow ﬁeld is useful for detecting vortexes. On this

component, we inspect the local deformation tensor

∂U /∂x ∂V /∂x
 ,

∂U /∂y ∂V /∂y

∇ (cid:126)F =

which can be decomposed to the symmetric (strain) part S = 1
(rotation) part Ω = 1
their norms:

2(∇ (cid:126)F + ∇ (cid:126)F T ) and the asymmetric
2(∇ (cid:126)F − ∇ (cid:126)F T ). The Q-criterion introduced by [46] takes the difference of

Q =

1
2

((cid:107)Ω(cid:107)2 − (cid:107)S(cid:107)2) =

1
4

(cid:107)(cid:126)ω(cid:107)2 − 1
2

(cid:107)S(cid:107)2 ,

(4)

14

where (cid:107)·(cid:107) is the Frobenius matrix norm. The Q-criterion measures the dominance of the vorticity.
When Q > 0, i.e., the vorticity component dominates the local ﬂow, the corresponding location
is regarded to be in a vortex region. Fig. 3(e) and Fig. 3(f) visualize the vorticity and Q-criterion
of the ﬂow ﬁeld in Fig. 3(c). In Fig. 3(e), pixels are tinted by the corresponding magnitude of
vorticity, and different colors mean different rotating directions. In Fig. 3(f), the vortex regions
are highlighted in red. Clearly only pixels with a dominant vorticity component are selected as
vortexes by the Q-criterion. It is apparent that these vortexes are more prone to be located inside
the storm area (highlighted by yellow boundaries) than the removed high-vorticity pixels. It is
also observed that the vortexes are typically in narrow bending comma shapes as described in
Section I-C. Therefore, they are properly to be regarded as the potential storm elements.

C. Vortex Descriptor

Not all the vortex regions in Fig. 3(f) are related to storms7. As a result we built a descriptor
for each extracted vortex and apply it to a machine learning module (to be introduced later). We
introduce the vortex descriptor in this subsection.

Denoting a certain vortex region in a satellite image as Φ and the area of Φ as Π(Φ), the
following visual clues, both static ones from a single image and dynamic ones from the optical
ﬂow with respect to the previous image frame, are considered in our approach.

1) Mean channel 3 brightness:

w1(Φ) =

2) Mean channel 4 brightness:

w2(Φ) =

where I (3)(x, y) is the brightness of pixel (x, y) in the channel 3 image.

(x,y)∈Φ I (3)(x, y)

Π(Φ)

(x,y)∈Φ I (4)(x, y)

Π(Φ)

,

,

(cid:80)

(cid:80)

(cid:80)

where I (4)(x, y) is the brightness of pixel (x, y) in the channel 3 image.

3) Mean optical ﬂow intensity:

w3(Φ) =

(x,y)∈Φ (cid:107) (cid:126)F (x, y)(cid:107)

Π(Φ)

,

where (cid:126)F (x, y) = (U (x, y), V (x, y)) is the optical ﬂow at pixel (x, y) computed between
the previous and current image frames.

7The types of vortexes outside of the CONUS are unknown because of lack of records.

15

(cid:80)
(x,y)∈Φ (cid:107)θ(x, y)(cid:107)

Π(Φ)

,

4) Mean optical ﬂow direction:

where θ(x, y) = tan−1(cid:16) V (x,y)

U (x,y)

(cid:17)

w4(Φ) =

5) Mean vorticity on the solenoidal optical ﬂow component:

is optical ﬂow (cid:126)F (x, y)’s direction.

where ω(x, y) is the vorticity value of the solenoidal component of optical ﬂow (cid:126)F (x, y)
(sign of the value indicates the vorticity’s direction).

6) Mean divergence on the irrotational optical ﬂow component:

(cid:80)

(cid:80)

w5(Φ) =

(x,y)∈Φ ω(x, y)

,

Π(Φ)

w6(Φ) =

(x,y)∈Φ div (cid:126)F (x, y)

,

Π(Φ)

where (cid:126)F (x, y) is the divergence of the irrotational component of optical ﬂow (cid:126)F (x, y).

7) Maximal Q-value of the vortex:

where Q(x, y) is deﬁned in Eq. (4).

w7(Φ) = max
(x,y)∈Φ

Q(x, y) ,

We also considered the spatial and temporal distributions of the storms that occurred in the
CONUS from 2000 to 2013 using all the historical storm records through these years8. This is
shown in Fig. 4. We divided the CONUS area into 4◦ × 4◦ grids. Inside each grid, we count the
occurrence of storms around each given date (e.g., July 4th) in the history. Storms that occurred
from 5 days before to 5 days after the queried date in each year are counted (e.g., for July 4,
storms from June 29 to July 9 in every year, a total of 154 days, are included9). Denoting the
total storm count in grid (i, j) (the grid counted i-th from the left and j-th from the top) for
date d is Nd(i, j) , the average storm density in the grid is ρd(i, j) = Nd(i,j)
154 .

To describe the storm prior for vortex Φ (extracted from date d), we took the average storm

densities of the pixels it covers:

w8(Φ) =

(cid:80)

(x,y)∈Φ φd(T (x, y))

,

Π(Φ)

8We noticed that NWS revised the criteria for hail reporting from 3/4 to 1 inch in diameter in 2010 so less hails were reported
after that. However, the change has little effect on the storms’ relative spatial distribution used in the machine learning system.

9We count storms around February 28th in non-leap years for storm statistics on February 29th.

16

Fig. 4. Average storm densities (number of storms per 30,000 km2) for the ﬁfth day of each month in US continent. Darker
color means higher storm density. The statistics are based on the historical records from 2000 to 2013. Around each given date
(+/-5 days), we count the total number of storms reported in each state across the past 14 years. The numbers are then divided
by the total number of days and the areas of corresponding states to calculate the average densities.

where T : (x, y) (cid:55)→ (i, j) is a mapping from the pixel (x, y) to the grid index (i, j) it belongs
to.

With all the features introduced above, for vortex Φ we constructed a vortex descriptor X(Φ) =
(w1, w2, w3, w4, w5, w6, w7, w8). Descriptors of training data are fed into a machine learning
algorithm, and eventually we obtain a classiﬁer C(·). For an arbitrary vortex Φ, Y (Φ) = C(X(Φ))
is the predicted status calculated by the classiﬁer. We introduce the machine learning approach
in the next section.

III. STORM SYSTEM CLASSIFICATION AND DETECTION

The vortex regions with large Q values correspond to regions with strong cloud rotations and
our approach extracts them as vortexes. However, they are not necessarily related to the storms
because the cloud rotation is only one aspect of a weather system. In order to create reliable storm
detection, we embedded these vortexes and their descriptors into a machine learning framework.

17

Using the descriptors and ground-truth labels of the extracted candidate vortex regions, a classiﬁer
was trained to distinguish storm-related vortexes from other vortexes.

A. Training Vortexes and Ground-truth Labels

We extracted vortexes from 2008 GOES-M satellite images and used the storm reports in the
same year to assign a ground-truth label for each vortex in the CONUS. A classiﬁer is trained
to associate the vortex features with the storm labels. Image sequences in the ﬁrst ten days of
each month are used for training and those from the 18th to the 22nd days of each month are
used for testing (so that the weather systems in the training and testing data are relatively far
apart and independent). In order to get enough difference yet not to lose too much detail, we
choose the sampling interval between every two images in a sequence to be 30 minutes.

We used the historical storms to assign ground-truth labels to detected vortexes. We denote
the historical storm database as D and each storm entry as (ui, vi, ti) where (ui, vi) is the
location (latitude and longitude values in degrees) and ti is the starting time of the storm.
Assuming a vortex is detected at location (u, v)10 and time t, if at least one record (ui, vi, ti) ∈ D
within 3◦ of latitude and longitude exists between the moments t − 0.5hr and t + 2hr (i.e.
(cid:107)ui − u(cid:107) < 3◦ and (cid:107)vi − v(cid:107) < 3◦ and t − 0.5hr ≤ ti ≤ t + 2hr), the vortex is assigned with a
positive (storm-related) label; otherwise it is assigned with a negative (no-storm) label. We only
assigned labels for vortexes inside the CONUS. For vortexes outside of the CONUS, we had
no historical data indicating whether or not they are storm-related, so we did not use them for
training and testing.

On the 120 selected days (10 for each month), a total of 8,527 storm-related vortexes were
discovered by the above method. We randomly selected the same number of vortexes in the
CONUS that were not storm-related from these days. A binary classiﬁer is then trained on the
17,054 training vortex samples.

B. Random Forest Classiﬁer

We trained a random forest classiﬁer [47] using the training data to distinguish storm-related
vortexes from vortexes not afﬁliated with a storm. The features for each vortex are described in
Section II-C and the ground-truth is deﬁned in Section III-A.

10(u, v) is the geometric center of the vortex.

18

Using the training data, the random forest learns rules to determine whether a vortex of
certain descriptor causes storms. The random forest is essentially an ensemble learning algorithm
that makes predictions by combining the results of multiple individual decision trees trained
from random subsets of the training data. We chose this approach because it has been found
to outperform a single classiﬁer (e.g., SVM). In fact, this strategy resembles the ensemble
forecasting [48] approach in numerical weather prediction, where multiple numerical results
with different initial conditions are combined to make a more reliable weather forecast. For
both numerical and statistical weather forecasting approaches, the ensemble approaches help to
improve the prediction qualities.

C. Testing Vortex Samples and Testing Methods

To develop a benchmark for the performance of the vortex classiﬁer, we used image sequences
from the 18th to the 22nd days of each month in 2008 as the testing data. On each selected
day, the image sequence (with a 30-minute interval) from 10:00 GMT to 18:00 GMT is used
to extract dynamic cloud vortexes. The test dataset contains a total of 48,698 vortexes, among
which 781 are labeled as positive, and the rest 47,917 samples are negative. The descriptors of
these vortexes are classiﬁed by the classiﬁer. We then evaluate the classiﬁcation results in two
ways.

Firstly, we assigned the testing vortexes with ground-truth labels in the same way as how
the training ground-truth labels are assigned. The predicted storm status for each test vortex
is compared with the ground-truth. In this way the classiﬁcation accuracy for single vortexes
are evaluated. Table I presents the classiﬁcation performance of both the training data (by
cross validation) and the testing data. The classiﬁer shows consistent performances on both
the training set and the test set. To demonstrate the effect of including geographic storm priors
in the classiﬁcation, we also trained and tested the random forest classiﬁers only on the visual
features and the storm density separately. Clearly none of the visual features and the historical
storm density standalone performs well alone. Combining visual features with the storm density
signiﬁcantly enhances the classiﬁcation performance.

Secondly, we evaluated the classiﬁer’s ability to predict the future locations of storms. Instead
of comparing each vortex’s classiﬁcation result with its ground-truth label, which reﬂects its status
within the period of (t − 0.5hr, t + 2hr) (t is the timestamp of the vortex), we observed how
much time in advance a storm can be detected by our algorithm. Given a testing vortex extracted

CLASSIFICATION PERFORMANCE AND CONTRIBUTIONS OF VISUAL FEATURES AND HISTORICAL PRIORS

TABLE I

19

Training set
(cross validation)

Testing set

All features

Visual only

Prior only

Overall

83.3%

Sensitivity

89.6%

Speciﬁcity

76.9%

Overall

80.3%

Sensitivity

82.1%

Speciﬁcity

80.2%

68.8%

67.4%

70.3%

67.4%

47.2%

67.7%

76.1%

74.8%

77.3%

77.3%

77.4%

71.2%

Note: Training set contains 5,876 storm-related and 5,876 no-storm vortex regions from 120 days in 2008. 10-fold cross validation
is performed in the evaluation. Testing set contains 2,706 storm-related cells and 7,773 no-storm cells from 60 days far away
from the training data. The feature vector for each vortex is composed by both visual features and storm priors (see Section II-C).
Beside the merged features (results shown in the ﬁrst column), we test the two types of features separately (results shown in
the second and third columns).

at the location (u, v) and time t, we ﬁnd the earliest time t(cid:48) = T (u, v, t) that a neighboring storm
(u(cid:48), v(cid:48), t(cid:48)) ∈ D developed since two hours before t, i.e.,

t(cid:48) = min

ti

{(ui, vi, ti) ∈ D and (cid:107)ui − u(cid:107) < 3◦ and (cid:107)vi − v(cid:107) < 3◦ and ti > t − 2hr} .

The value ∆t = T (u, v, t) − t indicates the time difference between an observed vortex and the
ﬁrst storm to form nearby storm. For a vortex with ∆t > 0, a future storm is predicted if the
classiﬁer can label a vortex as storm-related. The percentages of storms predicted for vortexes
with different ∆t > 0 are shown in Fig. 5. The results show that the proposed algorithm
can effectively predict future storms within several hours of the detection of vortexes and the
prediction reliability increases as ∆t decreases. Particularly, it shows a reasonable prediction
rate for the development of nearby storms within a period of 4 hours after the observation of
visual vortexes in satellite images.

It is necessary to emphasize that the classiﬁcation is performed on already extracted candidate
vortex regions where local clouds are rotating. Most of the cloud-covered regions on the satellite
images without obvious rotational motions have been already eliminated before the algorithm
proceeds to the classiﬁcation stage. Therefore in practice the system achieves a good overall

20

Fig. 5. Storm detection rate with different amounts of time in advance. The distribution of the time differences (∆t) between
the testing vortex samples and storms are plotted in red bars (the last bar is not fully shown because it is much higher than
others). And the percentages of vortexes categorized as storms with different ∆t are plotted in a blue curve. The ﬁgure shows
that the predictivity of the classiﬁer is generally reliable within few hours of time in advance. The storm detection rate decreases
as the time difference becomes longer.

precision for detecting the storms. In addition, the classiﬁcation results for individual vortex
regions can be further corrected and validated based on their spatial and temporal neighboring
relationships.

D. Case Study

To demonstrate the usefulness of the proposed vortex detection and classiﬁcation algorithm
in real scenarios, we present several case studies where our algorithm is applied for automatic
storm detection from satellite images.

Three different cases are presented and the results are visualized in Fig. 6 (one row for each
case). An active storm reporting day , a clear storm-free day, and a cloudy storm-free day are

0"100"200"300"400"500"600"700"800"900"1000"0"0.1"0.2"0.3"0.4"0.5"0.6"0.7"0.8"0.9"1"<1hr"1hr02hr"2hr03hr"3hr04hr"4hr05hr"5hr06hr"6hr07hr"7hr08hr"8hr09hr"9hr010hr">10hr"Number"of"vortexes"Detec?on"rate"Time"diﬀerence"between"vortexes"and"storms"Number"of"vortexes"Detec?on"rate"43100 21

selected to demonstrate the adaptiveness of our algorithm. These three image sequences are not
included in the training process of the storm classiﬁer. In Fig. 6, automatically detected vortexes
are ﬁlled with colors on each image. Vortexes classiﬁed as storms are colored in red, and the
green vortexes are categorized as storm-free cases by the machine learning module. The US state
boundaries are drawn on each image to show the alignment between the satellite images and the
geo-coordinate system. In particular, boundaries of states affected by storms are highlighted by
warm colors from red to yellow. The highlighting color indicates how long after the observation
storms were reported in the corresponding states. As the color turns to yellow, the highlighted
states are affected by storms further in the future. Speciﬁcally, the red color means an ongoing
storm and the yellow color means a storm 6 hours after the observation time. Fig. 6 only shows
the results on the ﬁrst three image frames of each case due to space limitation. Each test image
sequence in fact contains more than three frames, and the storm detection results are consistent
in the subsequent frames for each case.

The ﬁrst case is from May 25, 2008, which was an active storm day, when several severe
storms hit the Midwest US. An obvious cloud vortex system is visible on the satellite images
and it is related to storms reported in nearby states. Our algorithm captures all the major vortexes
from the satellite image sequence, and correctly categories most of them as storms. The result
also shows the capability of the proposed algorithm to assist in short-term storm forecasting.
In most cases there will be storms detected around regions where storms were reported shortly
thereafter.

Different from the ﬁrst case, the second case is strongly negative in that little clouds and
vortexes can be perceived from the images. Results on the second row of Fig. 6 shows that no
storm is predicted because few clouds and vortexes can be captured by the feature extraction
stage of our algorithm. In this case, it is easy for the algorithm to make correct decisions.

In the third case, several cloud systems can be observed above the CONUS, and several vor-
texes are detected by the optical ﬂow analysis. In reality, even though some cloud vortex systems
look dominant on the image sequence, there is no severe storm reported on that day. Results
shown on the third row of Fig. 6 demonstrate that the machine learning module successfully
avoids labeling the detected cloud vortexes as storms. This shows that the system does not over-
predict the occurrence of storms. This beneﬁt is also validated by the standalone test for the
classiﬁer showing that speciﬁcity of the classiﬁer is as high as its sensitivity.

T
M
G
5
1
:
3
1

5
2
/
5
0
/
8
0
0
2

)
c
(

T
M
G
5
4
:
2
1

5
2
/
5
0
/
8
0
0
2

)
b
(

T
M
G
5
1
:
2
1

5
2
/
5
0
/
8
0
0
2

)
a
(

T
M
G
5
1
:
3
1

8
1
/
9
0
/
8
0
0
2

)
f
(

T
M
G
5
4
:
2
1

8
1
/
9
0
/
8
0
0
2

)
e
(

T
M
G
5
1
:
2
1

8
1
/
9
0
/
8
0
0
2

)
d
(

22

d
e
r
o
l
o
c

e
r
a

s
e
x
e
t
r
o
v

d
e
t
c
e
j
e
r

d
n
a

d
e
r

n
i

d
e
r
o
l
o
c

e
r
a

s
e
x
e
t
r
o
v
m
r
o
t
s

d
e
t
c
e
t
e
D

.
s
e
c
n
e
u
q
e
s

e
g
a
m

i

e
t
i
l
l
e
t
a
s

e
z
y
l
a
n
a

o
t

m
h
t
i
r
o
g
l
a

d
e
s
o
p
o
r
p

e
h
t

g
n
i
y
l
p
p
a

f
o

s
e
s
a
c

e
l
p
m
a
x
e

e
e
r
h
T

.
6

.
g
i
F

g
n
i
t
h
g
i
l
h
g
i
h

e
h
t

s
A

.
t
r
o
p
e
r

l
a
c
i
r
o
t
s
i
h

e
h
t

n
o

d
e
s
a
b
m
r
o
t
s

g
n
i
o
g
n
o

n
a

y
b

t
i
h

s
i

e
t
a
t
s

e
h
t

s
e
t
a
c
i
d
n
i

y
r
a
d
n
u
o
b

e
t
a
t
s

d
e
r

A

.
s
r
o
l
o
c
m
r
a
w
n
i

d
e
t
h
g
i
l
h
g
i
h

e
r
a

s

m
r
o
t
s

y
b

t
i
h

s
e
t
a
t
S

.
n
e
e
r
g

n
i

s
m
r
o
t
s

r
o
j
a

M

.

8
0
0
2

,

5
2

y
a
M
n
o

s
e
m
a
r
f

e
e
r
h
T

)
c
-
a
(

.

n
o
i
t
a
v
r
e
s
b
o

e
h
t

m
o
r
f

y
a
w
a

s
r
u
o
h

6

s
a

r
a
f

s
a

s

m
r
o
t
s

e
r
u
t
u
f

e
r
o
m
y
b

d
e
t
c
e
f
f
a

s
i

e
t
a
t
s

g
n
i
d
n
o
p
s
e
r
r
o
c

e
h
t

,

w
o
l
l
e
y

o
t

s
n
r
u
t

r
o
l
o
c

l
l
e
w
s
e
x
e
t
r
o
v

e
h
t

f
o

s
n
o
i
t
a
c
o
l

e
h
t

d
n
a

e
m
a
r
f

h
c
a
e

n
i

s
m
e
t
s
y
s

m
r
o
t
s

s
a

d
e
ﬁ

i
s
s
a
l
c

d
n
a

d
e
t
c
e
t
e
d

e
r
a

s
e
x
e
t
r
o
v

e
l
p
i
t
l
u
M

.

S
U

f
o

s
e
t
a
t
s

n
r
e
t
s
a
e
h
t
u
o
s

d
n
a

,

n
r
e
t
s
a
e

,
l
a
r
t
n
e
c

e
h
t

n
i

d
e
r
r
u
c
c
o

,

5
2

h
c
r
a

M
n
o

s
e
m
a
r
f

e
g
a
m

I

)
i
-
g
(

.

d
e
t
c
e
t
e
d

s
i

x
e
t
r
o
v

o
n

d
n
a

y
a
d

e
h
t

n
o

r
a
e
l
c

y
l
t
s
o
m

s
i

a
e
r
a

t
n
e
n
i
t
n
o
c

S
U
e
h
T

.

8
0
0
2

,

8
1

r
e
b
m
e
t
p
e
S

n
o

s
e
m
a
r
f

e
g
a
m

I

)
f
-
d
(

.

h
t
u
r
t
-
d
n
u
o
r
g

e
h
t

h
c
t
a
m

s
m
e
t
s
y
s

d
u
o
l
c

s
s
e
l
d
r
a
z
a
h

s
a

d
e
z
i
r
o
g
e
t
a
c

e
r
a
m
e
h
t

f
o

t
s
o
m

,
e
c
n
e
u
q
e
s

e
g
a
m

i

e
h
t

n
o

d
e
v
i
e
c
r
e
p

e
r
a

s
n
o
i
t
o
m

l
a
n
o
i
t
a
t
o
r

r
i
e
h
t

d
n
a

s
d
u
o
l
c

h
g
u
o
h
T

.

y
a
d

e
h
t

n
o

d
e
t
r
o
p
e
r

s
i

m
r
o
t
s

o
N

.
8
0
0
2

.
r
e
ﬁ
i
s
s
a
l
c

e
h
t

y
b

T
M
G
5
1
:
3
1

5
2
/
3
0
/
8
0
0
2

)
i
(

T
M
G
5
4
:
2
1

5
2
/
3
0
/
8
0
0
2

)
h
(

T
M
G
5
1
:
2
1

5
2
/
3
0
/
8
0
0
2

)
g
(

23

IV. CONCLUSIONS AND FUTURE WORK

In this paper, we present a thunderstorm detection algorithm that locates storm visual signatures
from satellite images. The algorithm automatically analyzes the visual features from satellite
images and incorporates the historical meteorological records to make storm predictions. As
opposed to traditional ways of weather forecasting, our approach primarily relies on the visual
information from images and tries to extract high-level storm signatures as meteorologists usually
do manually. Without using any sensory measurement (e.g., temperature, air pressure, etc.) as
numerical weather forecasting does, the algorithm captures global cloud patterns solely from
the satellite images, which is helpful in the synoptic-scale storm prediction. Additionally, the
algorithm takes advantage of big data, using historical storm reports in the past years together
with the satellite image archives to learn the correspondence between visual image signatures
and the occurrences of current and future storms. Experiments and case studies show that the
algorithm is effective and robust.

Properties of optical ﬂows between every two images in a sequence are the basic visual
clues adopted in the work. The algorithm can estimate robust and smooth optical ﬂows between
two images and determine the rotations in the ﬂow ﬁeld. Unlike numerical weather forecasting
methods that are sensitive to noise and initial conditions, our approach can consistently detect
reliable visual storm clues hours before the occurrence of a storm. Therefore, the results are
useful for weather forecasts, especially storm forecasts.

The application of historical storm records and machine learning boosts the performance of
the storm extraction algorithm. Standalone vortex detection from the optical ﬂow is not sufﬁcient
to make reliable predictions. The statistical model trained from historical meteorological data
together with the satellite image visual features further selects the extracted vortexes and removes
vortexes not related to storms. In our current algorithm, storm detection is based on individual
vortex regions. In reality, multiple vortexes tend to appear near each other in storm systems both
temporally and geographically. Therefore, taking into account nearby vortexes within a frame
and tracking storm systems across a sequence can improve the overall reliability of the system.
In particular, tracking the development of a storm system will be helpful for analyzing the future
trend of the storm. This problem is nontrivial because the storm systems (clouds) are non-rigid
and highly variant, and the same vortexes in a storm system are not always detectable over time.
Future work needs to tackle the problem of non-rigid object tracking to better make use of the

24

temporal information in the satellite image sequences.

Lastly it should be emphasized that weather systems are highly complex and chaotic, so it is
always a challenging task to make accurate weather forecasts. The proposed algorithm based on
the satellite image visual features is not completely accurate and needs to be further validated.
We expect the algorithm or similar approaches to play a part in producing weather forecasts
rather than making the forecasts alone. It could be incorporated with other existing weather
prediction techniques (e.g. NWP) and used as an automatic tool to reﬁne the current forecasts.
It is useful and valuable because it provides forecasts in a different aspect from the numerical
approaches. The purpose for developing the new algorithm is not to replace the current weather
forecasting models, but to produce additional independent predictions to be combined with other
approaches. As a result, we will focus the future study on how to integrate information from
multiple data sources and predictions from different models to produce more reliable and timely
storm forecasts.

ACKNOWLEDGMENT

This material is based upon work supported by the National Science Foundation under Grant
No. 1027854. Shared computational infrastructure was provided by the Foundation under Grant
No. 0821527. Part of the work was done when J. Z. Wang and J. Li were with the Foundation.
Any opinions, ﬁndings, and conclusions or recommendations expressed in this material are those
of the authors and do not necessarily reﬂect the views of the Foundation. We also thank the US
National Oceanic and Atmospheric Administration (NOAA) for providing the data used in this
research. Siqiong He assisted with data collection. The discussions with Jose A. Piedra-Fernandez
of the University of Almeria, Spain, has been very helpful.

REFERENCES

[1] P. Lynch, “The origins of computer weather prediction and climate modeling,” Journal of Computational Physics, vol.

227, no. 7, pp. 3431–3444, 2008.

[2] G. W. Platzman, “The eniac computations of 1950-gateway to numerical weather prediction,” Bulletin of the American

Meteorological Society, vol. 60, no. 4, pp. 302–312, 1979.

[3] J. Charney, “The use of the primitive equations of motion in numerical prediction,” Tellus, vol. 7, no. 1, pp. 22–26, 1955.
[4] F. Yang, “Review of gfs forecast skills in 2013,” National Centers for Environmental Prediction, 2013.
[5] H. W. Lean, P. A. Clark, M. Dixon, N. M. Roberts, A. Fitch, R. Forbes, and C. Halliwell, “Characteristics of high-resolution
versions of the met ofﬁce uniﬁed model for forecasting convection over the united kingdom,” Monthly Weather Review,
vol. 136, no. 9, pp. 3408–3424, 2008.

25

[6] M. D´equ´e et al., “Documentation arpege-climat,” CNRM (Available from Centre National de Recherches Meteorologiques,

M´et´eo-France, Toulouse, France), 1999.

[7] P. Gauthier, C. Charette, L. Fillion, P. Koclas, and S. Laroche, “Implementation of a 3d variational data assimilation system
at the canadian meteorological centre. part i: The global analysis,” Atmosphere-Ocean, vol. 37, no. 2, pp. 103–156, 1999.

[8] S. D. Burt and D. A. Mansﬁeld, “The great storm of 15–16 october 1987,” Weather, vol. 43, no. 3, pp. 90–110, 1988.
[9] E. S. Blake, T. B. Kimberlain, R. J. Berg, J. Cangialosi, and J. L. Beven II, “Tropical cyclone report: Hurricane sandy,”

National Hurricane Center, vol. 12, pp. 1–10, 2013.

[10] R. L. Winkler, “The importance of communicating uncertainties in forecasts: Overestimating the risks from winter storm

juno,” Risk Analysis, vol. 35, no. 3, pp. 349–353, 2015.

[11] A. C. Lorenc and T. Payne, “4d-var and the butterﬂy effect: Statistical four-dimensional data assimilation for a wide range

of scales,” Quarterly Journal of the Royal Meteorological Society, vol. 133, no. 624, pp. 607–614, 2007.

[12] J. D. Doyle, C. Amerault, C. A. Reynolds, and P. A. Reinecke, “Initial condition sensitivity and predictability of a severe

extratropical cyclone using a moist adjoint,” Monthly Weather Review, vol. 142, no. 1, pp. 320–342, 2014.

[13] V. F. Dvorak, “Tropical cyclone intensity analysis and forecasting from satellite imagery,” Monthly Weather Review, vol.

103, no. 5, pp. 420–430, 1975.

[14] R. A. Scoﬁeld, “The nesdis operational convective precipitation-estimation technique,” Monthly Weather Review, vol. 115,

no. 8, pp. 1773–1793, 1987.

[15] G. Liu, “Satellite microwave remote sensing of clouds and precipitation,” Observation, Theory and Modeling of Atmospheric
Variability: Selected Papers of Nanjing Institute of Meteorology Alumni in Commemoration of Professor Jijia Zhang, vol. 3,
p. 397, 2004.

[16] A. N. Srivastava and J. Stroeve, “Onboard detection of snow, ice, clouds and other geophysical processes using kernel
the ICML 2003 Workshop on Machine Learning Technologies for Autonomous Space

methods,” in Proceedings of
Applications, 2003.

[17] Y. Hong, “Precipitation estimation from remotely sensed imagery using an artiﬁcial neural network cloud classiﬁcation

system,” Journal of Applied Meteorology, vol. 43, pp. 1834–1852, 2004.

[18] A. Behrangi, K. Hsu, B. Imam, and S. Sorooshian, “Daytime precipitation estimation using bispectral cloud classiﬁcation

system,” Journal of Applied Meteorology and Climatology, vol. 49, pp. 1015–1031, 2010.

[19] J. C. Price, “Using spatial context in satellite data to infer regional scale evapotranspiration,” IEEE Transactions on

Geoscience and Remote Sensing, vol. 28, no. 5, pp. 940–948, 1990.

[20] K. Bedka, J. Brunner, R. Dworak, W. Feltz, J. Otkin, and T. Greenwald, “Objective satellite-based detection of overshooting
tops using infrared window channel brightness temperature gradients,” Journal of Applied Meteorology and Climatology,
vol. 49, no. 2, pp. 181–202, 2010.

[21] K. M. Bedka, “Overshooting cloud top detections using msg seviri infrared brightness temperatures and their relationship

to severe weather over europe,” Atmospheric Research, vol. 99, no. 2, pp. 175–189, 2011.

[22] S.-S. Ho and A. Talukder, “Automated cyclone discovery and tracking using knowledge sharing in multiple heterogeneous
satellite data,” in Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data
Mining, 2008, pp. 928–936.

[23] T. Zinner, H. Mannstein, and A. Tafferner, “Cb-tram: Tracking and monitoring severe convection from onset over rapid
development to mature phase using multi-channel meteosat-8 seviri data,” Meteorology and Atmospheric Physics, vol. 101,
no. 3-4, pp. 191–210, 2008.

[24] C. Keil and G. C. Craig, “A displacement and amplitude score employing an optical ﬂow technique,” Weather and

Forecasting, vol. 24, no. 5, pp. 1297–1308, 2009.

26

[25] D. Merk and T. Zinner, “Detection of convective initiation using meteosat seviri: implementation in and veriﬁcation with
the tracking and nowcasting algorithm cb-tram,” Atmospheric Measurement Techniques Discussions, vol. 6, pp. 1771–1813,
2013.

[26] A. N. Evans, “Cloud motion analysis using multichannel correlation-relaxation labeling,” IEEE Geoscience and Remote

Sensing Letters, vol. 3, no. 3, pp. 392–396, 2006.

[27] A. McGovern, A. Kruger, D. Rosendahl, and K. Droegemeier, “Open problem: dynamic relational models for improved
hazardous weather prediction,” in Proceedings of ICML Workshop on Open Problems in Statistical Relational Learning,
vol. 29, 2006.

[28] NOAA, “Earth location users guide (elug). rev. 1, noss/osd3-1998-015r1ud0,” no. NOAA/NESDIS DRL 504-11, 1998.
[29] J. P. Snyder, Map projections–A working manual. USGPO, 1987, no. 1395.
[30] J. Marshall and R. A. Plumb, Atmosphere, ocean and climate dynamics: an introductory text. Academic Press, 1965,

vol. 8.

[31] R. G. Barry and R. J. Chorley, Atmosphere, weather and climate. Routledge, 2009.
[32] A. J. Clark, C. J. Schaffer, W. A. Gallus Jr, and K. Johnson-O’Mara, “Climatology of storm reports relative to upper-level

jet streaks,” Weather and Forecasting, vol. 24, no. 4, pp. 1032–1051, 2009.

[33] J. G. Charney, “The dynamics of long waves in a baroclinic westerly current,” Journal of Meteorology, vol. 4, no. 5, pp.

136–162, 1947.

[34] T. N. Carlson, “Airﬂow through midlatitude cyclones and the comma cloud pattern,” Monthly Weather Review, vol. 108,

no. 10, pp. 1498–1509, 1980.

[35] J. M. Wallace and P. V. Hobbs, Atmospheric science: an introductory survey, vol. 92.
[36] C. A. Doswell III, “Weather forecasting by humans-heuristics and decision making,” Weather and Forecasting, vol. 19,

no. 6, pp. 1115–1126, 2004.

[37] B. K. Horn and B. G. Schunck, “Determining optical ﬂow,” in 1981 Technical Symposium East, 1981, pp. 319–331.
[38] B. D. Lucas and T. Kanade, “An iterative image registration technique with an application to stereo vision.” in IJCAI,

vol. 81, 1981, pp. 674–679.

[39] J. S. P´erez, E. Meinhardt-Llopis, and G. Facciolo, “Tv-l1 optical ﬂow estimation,” Image Processing On Line, vol. 2013,

pp. 137–150, 2013.

[40] J. Bouguet, “Pyramidal implementation of the afﬁne lucas kanade feature tracker description of the algorithm,” Intel

Corporation, vol. 5, 2001.

[41] T. Acharya and A. K. Ray, Image processing: principles and applications, 2005.
[42] R. Temam, Navier–Stokes Equations. American Mathematical Soc., 1984.
[43] J. Stam, “Stable ﬂuids,” in Proceedings of the 26th annual conference on Computer graphics and interactive techniques,

1999, pp. 121–128.

[44] S. I. Green, Fluid Vortices: Fluid Mechanics and Its Applications. Springer, 1995, vol. 30.
[45] G. B. Arfken and H. J. Weber, Mathematical Methods for Physicists, 4th ed. Academic Press: San Diego, 1995.
[46] J. C. Hunt, A. Wray, and P. Moin, “Eddies, streams, and convergence zones in turbulent ﬂows,” in Studying Turbulence

Using Numerical Simulation Databases, 2, vol. 1, 1988, pp. 193–208.

[47] L. Breiman, “Random forests,” Machine learning, vol. 45, no. 1, pp. 5–32, 2001.
[48] F. Molteni, R. Buizza, T. N. Palmer, and T. Petroliagis, “The ecmwf ensemble prediction system: Methodology and

validation,” Quarterly Journal of the Royal Meteorological Society, vol. 122, no. 529, pp. 73–119, 1996.

