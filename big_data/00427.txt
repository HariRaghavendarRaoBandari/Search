A Nonlinear Adaptive Filter Based on the Model of

Simple Multilinear Functionals

Felipe C. Pinheiro and Cássio G. Lopes, Member, IEEE

1

6
1
0
2

 
r
a

M
1

 

 
 
]

Y
S
.
s
c
[
 
 

1
v
7
2
4
0
0

.

3
0
6
1
:
v
i
X
r
a

Abstract—Nonlinear adaptive ﬁltering allows for modeling of
some additional aspects of a general system and usually relies on
highly complex algorithms, such as those based on the Volterra
series. Through the use of the Kronecker product and some basic
facts of tensor algebra, we propose a simple model of nonlinearity,
one that can be interpreted as a product of the outputs of K
FIR linear ﬁlters, and compute its cost function together with
its gradient, which allows for some analysis of the optimiza-
tion problem. We use these results it in a stochastic gradient
framework, from which we derive an LMS-like algorithm and
investigate the problems of multi-modality in the mean-square
error surface and the choice of adequate initial conditions. Its
computational complexity is calculated. The new algorithm is
tested in a system identiﬁcation setup and is compared with other
polynomial algorithms from the literature, presenting favorable
convergence and/or computational complexity.

Index Terms—Adaptive ﬁlters, nonlinear ﬁlters,
squares methods, multilinear algebra, cost function.

least mean

I. INTRODUCTION

T HE collective of nonlinear signal processing techniques

come into play whenever nonlinear effects, such as har-
monic distortion, saturation and general polynomial behavior,
start to become noticeable enough to degrade the performance
of conventional linear techniques. A common problem with
nonlinear ﬁlters is the computational complexity they demand.
Some of the most complex ﬁlters are based on the Volterra and
Wiener models [1]–[3]. It is common to restrict these models
in order to reduce their complexity [4]–[6]. We follow this
approach.

We start by proposing a polynomial technique that could
be thought of as a subclass of the Volterra model. It is based
on representing the input regressor as an iterated Kronecker
product, an idea that has already been used for the estima-
tion of higher order statistics of the regressor [7]—a related
problem—and other Volterra approaches [4]–[6], but we also
embed this Kronecker structure within the ﬁlter itself. This
formalism allows us to promptly derive a cost function in
explicit form. We also compute its gradient and use it in the
derivation of a nonlinear, low-complexity, LMS-like algorithm
with good mean-square performance.

II. SIMPLE MULTILINEAR MODEL

The general K-th order homogeneous Volterra kernel is
a function h(i1, . . . , iK) used to produce the input/output

The authors are with the Department of Electronic Systems Engineering,
Escola Politécnica, University of São Paulo, São Paulo, Brazil (e-mails:
felipe.chaud.pinheiro@usp.br; cassio@lps.usp.br).

The ﬁrst author was supported by a scholarship from CNPq No.

132625/2015-6

The second author was supported by a grant from CNPq No. 311031/2013-7

y(i) =

When we use (2) in (1), we get

(cid:88)
h1(i1)u(i − i1)··· M−1(cid:88)

0≤i1,...,iK <M

M−1(cid:88)
Each factor yos(i) (cid:44)(cid:80)M

i1=0

iK =0

=

h1(i1)··· hK(iK)u(i − i1)··· u(i − iK)

hK(iK)u(i − iK).

(3)

relationship in (1). There, u(i) is the input signal and M is
the length of the ﬁlter.

h(i1, . . . , iK)u(i − i1)··· u(i − iK) (1)

(cid:88)

y(i) =

0≤i1,...,iK <M

Evaluating (1) is a highly complex task. One would take
O(M K) operations per iteration. This can be slightly reduced
should the symmetry in the kernel be explored, but
this
approach could at most decrease the number of operations

to something proportional to(cid:0)M−K+1

(cid:1), which still increases

rapidly. It is necessary to suppose some extra structure in the
kernel itself if one wants a signiﬁcant reduction in complexity.
The structure we propose is kernel separability. Explicitly,
we suppose that there exists functions hs(is), 1 ≤ s ≤ K,
such that

K

h(i1, . . . , iK) = h1(i1)h2(i2)··· hK(iK).

(2)

is=0 hs(ij)u(i − is) could be seen
as the output of a linear FIR ﬁlter. As such, we can represent
them in vector form. We collect the input signals in a 1 × M
row vector1 ui = [u(i) u(i − 1)··· u(i − M + 1)] and a set of
K vectors of size M × 1 represented by {w1, . . . , wK}. We
also say that, for each s, we have

ws = [hs(0) hs(1)··· hs(M − 1)]T .
Now, we have yos(i) = uiws for every s, so that
y(i) = (uiw1)(uiw2)··· (uiwK).

(4)

(5)

This can be represented by Fig. 1. Granted, this implies
a loss of generality from the original Volterra model. Not
every kernel is separable. But this simpliﬁcation allows us to
compute the output of the system in O(KM ) operations per
iteration—a exponential reduction in complexity. Moreover,
this separable kernel allows a convenient formalism for the
derivation of the cost function that also gives us algebraic
insight on the model.

1This notation is due to [8]. Scalars are represented as x(i), vectors as xi,
matrices and constants as capital letters (either is clear from context) and a
boldface font for random quantities.

u(i)

w1
w2
...
wK

×

y(i)

Fig. 1. Block diagram of the Simple Multilinear Model.

+ w

A. Kronecker Representation

Using the identity (A ⊗ B)(C ⊗ D) = (AC) ⊗ (BD) for
the Kronocker product [9], it follows that (5) is equivalent to
(6), because the Kronecker product of scalars is their ordinary
multiplication.

y(i) = (uiw1)··· (uiwK) = (uiw1) ⊗ ··· ⊗ (uiwK)

= (ui ⊗ ··· ⊗ ui)

(w1 ⊗ ··· ⊗ wK)

(cid:124)

(cid:123)(cid:122)

K times

(cid:125)

The Kronecker product is analogous to the tensor product
[10], therefore we can think of the two factors (w1 ⊗ ··· ⊗
wK) and (ui ⊗ ··· ⊗ ui) in (6) as tensors. As such, to better
manipulate and have access to their elements, one can even
index them as
(ui ⊗ ··· ⊗ ui)j1,...,jK = u(i − j1 + 1)··· u(i − jK + 1) (7)
and

(w1 ⊗ ··· ⊗ wK)i1,...,iK = h1(i1)··· hK(iK).

(8)

This should be so that, if we sum the product of (7) and (8)
over the indexes is = js for every s, we get the output of the

system: 2(cid:88)

i1,...,iK

(ui ⊗ ··· ⊗ ui)i1,...,iK (w1 ⊗ ··· ⊗ wK)i1,...,iK = y(i).
(9)
Actually, these objects are a very speciﬁc kind of tensor:
they have rank one and are also called decomposable or
simple. One could think of substituting w1 ⊗ ··· ⊗ wK for a
general tensor and produce much more diverse nonlinearities.
However, such procedure would lead back to the full Volterra
model. This fact sheds algebraical reasoning on what can be
represented by the a product of FIR systems.

Additionally, one can think of the “w” tensor as a K-linear
form [11]—or functional—acting on K copies of the vector
ui. In this case, they are rank one multilinear functionals, also
called simple multilinear functionals, from which we take the
name of our model: the simple multilinear model (SML).

III. THE MEAN SQUARE ERROR OVER THE SML

As it is usual in most adaptive schemes, we pose a problem
related to optimization—in particular, the minimization of a
certain metric, the most common being the mean-square error.
Given a desired random signal d, a random 1× M vector u
and a set of M × 1 column vectors {w1, . . . , wK}, the output
estimation error is deﬁned as e (cid:44) d − u⊗Kw. Under our
notation, w (cid:44) w1⊗···⊗wK (KM×1), and u⊗K (cid:44) u⊗···⊗u
2This fact is used in Appendix A.

2

(1 × KM), the Kronecker product of u with itself K times.
The mean-square error (MSE) is deﬁned as

MSE (cid:44) E|e|2 = E(cid:2)[d − u⊗Kw]

[d − u⊗Kw](cid:3)

∗

= E|d|2 − w

∗E[du⊗K∗
∗E[u⊗K∗u⊗K]w.

] − E[d∗u⊗K]w

(10)

Under the hypothesis of stationarity on both u and d, we

can deﬁne the following constants:

RuK = E[u⊗K∗u⊗K], RuK d = E[u⊗Kd∗

Rd = E|d|2.

] = R

∗
duK

(11)
(12)

Then, the mean square error takes the familiar form in (13).

(6)

MSE(w1, . . . , wK) =
uK d − RuK dw + w
∗
R

Rd − w

∗

∗

RuK w.

(13)

Although the form is familiar, our notation embeds some
aspects of this function. For example, it does not describe a
quadratic surface. Instead, it has degree 2K, when we look at
the individual vectors w1, . . . , wK.

∗

Note that (13) could be seen as a function of a vector
wC built from the stacking of the individual w1, . . . , wK
vectors. We now introduce ∇ws, the gradient with respect to
the coordinates of the vector ws. The complete gradient vector,
∇MSE, over wC, would be the stacking of these gradients. It
RuK ](w1 ⊗ ··· ⊗(cid:99)ws ⊗ ··· ⊗ wK),
is possible to show that (see Appendix A)
∇wsMSE = [−RuK d + w
where (cid:99)ws implies that ws has been substituted for the identity
p (cid:54)= q—which makes w1 ⊗ ··· ⊗ (cid:99)ws ⊗ ··· ⊗ wK = 0 for

In addition, the critical MSE is reached when the gradient
is 0. This implies the trivial solution wp = wq = 0, for some

matrix IM of order M in the product.

each s—or the equation RuK wo = RduK . Usually we are not
interested in the ﬁrst, therefore we will focus on the second.
This equation is reminiscent of the normal equations in linear
estimation. As such, we can reorganize it in the form

(14)

E[u⊗Ku⊗K∗
E[u⊗K∗

]wo = E[u⊗K∗d]
(d − u⊗Kwo)] = 0

E[u⊗K∗eo] = 0,

(15)

that is, a form of the orthogonality principle, with the optimal
output error eo being orthogonal to u⊗K. One can interpret
this as eo being uncorrelated to all of the “degree K” products
of the input.

Due to the structure of RuK (repeated rows), it will always
be singular for K > 1, but under certain conditions we can
guarantee the existence of the non trivial solution. Assume for
d the model in (16),

d = u⊗Kho + n,

(16)
where n is some zero-mean noise uncorrelated with u⊗K
(serving as a model for eo), and ho = ho1 ⊗ ··· ⊗ hoK, for

Algorithm 1 LMS-like algorithm

3

wj [0] = [21−j 0 · · · 0 0]T

Initialization
for j = 1 to K − 1 do
end for
wK [0] = [0 0 · · · 0 0]T
Iteration
for i = 1 to END do
for s = 1 to K do

Compute yos(i) = uiws[i − 1]

end for
for s = 1 to K do

Compute ys(i) = yo1(i)

end for
Compute y(i) = yK (i)yoK (i)
Compute e(i) = d(i) − y(i)
Compute fi = µe(i)u∗
for s = 1 to K do

i

(cid:92)

· · · yos(i) · · ·yoK (i)

⊗K
i

∗
d(i)

(19)

end for

end for

Compute ws[i] = ws[i − 1] + fiys(i)∗

some set of vectors {ho1, . . . , hoK}. Then, one can verify that
ho is a solution to the equation RuK wo = RduK in wo:
RduK = E[u⊗K∗d] = E[u⊗K∗u⊗K]ho + E[u⊗K∗n]

= RuK ho.

(17)

IV. AN LMS INSPIRED ALGORITHM

We take the total gradient ∇MSE over wC, as previously
deﬁned. Then, we use it to form the equation of the steepest
descent.

wC[i] = wC[i − 1] − µ[∇MSE]
∗

.

(18)

As it is usually done for stochastic algorithms, we use the

realizations ui and d(i) to estimate (11).

(cid:101)RuK = u

⊗K∗
i

⊗K
u
i

,

(cid:101)RuK d = u

and compute the gradient accondingly from (14). We deﬁne
(uiws) denotes

(uiws)··· (uiwK), where (cid:92)
ys(i) (cid:44) (uiw1)··· (cid:92)
(cid:101)∇wsMSE = [−d(i)
the absence of the factor (uiws) in the product. Then
· (w1 ⊗ ··· ⊗(cid:99)ws ⊗ ··· ⊗ wK)

⊗K
u
i + w

⊗K∗
u
i

⊗K
u
i

∗

∗

]

= −[d(i) − u

⊗K
∗
i w]

(ui ⊗ ··· ⊗ ui)

· (w1 ⊗ ··· ⊗ IM ⊗ ··· ⊗ wK)

= −e(i)
(uiw1)··· (uiIM )··· (uiwK)
∗
(cid:92)··· (uiws)···(uiwK)uie(i)
= −(uiw1)
∗
= −ys(i)e(i)
∗

ui.

(a) Case I: Algorithms identify-
ing an order 2 SML plant.

(b) Case II: Same as Case I;
higher SNR.

(20)

It follows, then, by using (20) in (18), the update law in

(21), that should be performed for every s, 1 ≤ s ≤ K.

ws[i] = ws[i − 1] + µe(i)ys(i)

∗

∗
u
i

(21)

The algorithm still needs to initialize its variables—and this
is an important point in the implementation. As it can be seen
from the MSE surface, w1 = ··· = wK = 0 is a critical
point. Correspondingly, if every wj is initialized at 0, (21)
shows that they will always be 0. In spite of these problems,
we have empirically found one satisfactory initial condition
for use in high numeric precision environments. And that is:
using w1[0] = [1 0 ··· 0 0]T , w2[0] = [2−1 0 ··· 0 0]T , or
generally wj[0] = [21−j 0 ··· 0 1]T until j = K − 1 and then
wK[0] = 0. This is all summarized in the Algorithm Table 1.
To calculate the computational complexity for algorithm
(21), focus on the multiplications present in Algorithm Table
1. The total number of them, on a given iteration, are, for real
data and K ≥ 2, given by M K + K 2 − K + 2M + 2. In other
words, this an O(M K + K 2) algorithm.

V. SIMULATIONS

A set of two simulations, Cases I and II, on the problem
of system identiﬁcation, with K = 2 and M = 10, were
run. The signal d(i) was an SML plant plus additive zero-
mean Gaussian noise with variance σ2
n (see (16)). The input
signal was drawn from a zero-mean Gaussian distribuition with
unitary variance, collected in the vector ui with a delay-line
n = 10−3 and in Case II,
structure. In Case I, we chose σ2
n = 10−6. In each case, the algorithm was run through
σ2

(c) Case III: Algorithms identify-
ing a different order 2 SML plant.

(d) Case IV: Algorithms identify-
ing an order 3 SML plant.

(e) Case V: Various ﬁlters identify-
ing a smooth plant.

(f) Case VI: Parallel cascade and
SML.

Fig. 2. Curves of the MSE and ESME showing the performance of the SML
algorithm against other polynomial algorithms.

7000 iterations and the curves were averaged through 1000
realizations. The resulting Excess Mean Square Error (EMSE)
curves (calculated as E|u⊗2(h1⊗ h2− w[i− 1])|2) are present
in Figs. 2a and 2b. Plotted together on the graph are the curves
for the Volterra-LMS and the Wiener-LMS, designed for the
Gaussian vector u, as described in [1]. In those algorithms, we
chose3 µ so to get an approximately equal steady-state EMSE.
For Case III we have used a different SML plant. We had

3Stability bounds require a more sophisticated analysis.

10002000300040005000−40−200IterationsdBExcess Mean Square Error (estimated)  VolterraSMLWiener02000400060008000−80−60−40−20020IterationsdBExcess Mean Square Error (estimated)  VolterraWienerSML02000400060008000−80−60−40−20020IterationsdBExcess Mean Square Error (estimated)  SMLVolterraWiener00.511.52x 105−60−40−20020IterationsdBExcess Mean Square Error (estimated)  VolterraWienerSML0500010000−30−20−100Mean Square Error (estimated)IterationsdB  SVVSMLIVPF0510x 104−40−200Excess Mean Square Error (estimated)IterationsdB  CFSMLn = 10−6. µ was again chosen to equalize the steady-state
σ2
EMSE. The rest of the parameters remained the same.

The algorithm was also made to run with K = 3, which
makes Case IV. We had M = 10 and 200,000 iterations,
averaged through 10,000 realizations. We compare the results
with the Volterra-LMS and the Wiener-LMS with parameters
that equalize the steady-state EMSE. The resulting curves are
presented in Fig. 2d. The variance of the additive noise was
chosen as σ2

n = 10−3.

Case V compares the SML-LMS with some relevant LMS-
like polynomial algorithms from the literature. Fig. 2e shows
the Power Filter (PF) [12], Simpliﬁed Volterra (SV) [13], [14],
Sparse Interpolated Volterra (IV) [5], [6], the regular Volterra
ﬁlter and the SML. The task was to identify a smooth, non-
SML, order 2 Volterra plant with M = 21. This was averaged
through 1000 realizations.

Case VI shows a simulation against another ﬁlter,

the
Parallel Cascade Filter (CF) [15]—single branch and LMS
version. This algorithm is similar to the SML for K ≤ 2, so
we simulated them in K = 3, where they are clearly dissimilar.
We identify the same plant as Case IV, with M = 10, through
1000 realizations. The results are on Fig. 2f.
For both Cases V and VI the additive noise variance was
10−3. The parameters were adjusted to provide approximately
the same convergence rate.

A description of all the plants is available with the on-line

materials in the ﬁle plants.pdf.

VI. DISCUSSION

Cases I and II clearly show the system converging to an
EMSE of approximately −40 dB and −70 dB, respectively.
On both Figs. 2a and 2b, we can see the SML-LMS running
slower through the ﬁrst 500 iterations,
then it speeds up
and catches up with—and eventually overtake—the others.
Both the Volterra and Wiener models assume O
operations per iteration, while the SML only O(KM + K 2).
Although it is natural that the SML algorithm is better at
identifying SML plants, it does it faster and with far fewer
arithmetic operations.

(cid:16)(cid:0)M +K−1
(cid:1)(cid:17)

K

Note that

the SML algorithm presents richer dynamics
compared to the other algorithms—something that will be
studied in following publications. Case III, in Fig. 2c, for
example, shows that its behavior does not depend only on
statistics of the regressor ui—in fact, it is also inﬂuenced by
the plant to be identiﬁed. A source of these effects can be
hypothesised as due to the ys(i) factors in (21).

Case IV, in Fig. 2d, shows that the algorithm also converges
for K = 3 and that it has the aforementioned initial irregular
convergence rate, here appearing in a more evident way.

Case V shows that the algorithm is competitive, even in a
non-SML plant—as long as this plant is correlated enough.
The PF used 21 coefﬁcients, the SV used 66, the IV used
60, Volterra used 231 and SML 42. This is another result that
shows the computational simplicity of the SML.

Case VI shows two similar ﬁlters, in the sense that they
are formed through the product of others. SML is a product
of three linear ﬁlters, with a total of 30 coefﬁcients, and CF

4

is a product of a linear ﬁlter with a second order Volterra
ﬁlter, with 66 coefﬁcients. In the task of identifying an SML
plant, they show similar behavior, with the SML being slightly
better—at half the complexity.

VII. CONCLUSION

Through the use of a Kronecker representation, we have
developed the cost function for the SML model. From this
function its gradient was derived motivating a new LMS-like
nonlinear adaptive algorithm. It has very low computational
complexity—an exponential reduction when compared to te
Volterra series—and a competitive mean-square performance
in a variety of cases, but also some other unusual behavior.

Finally, the SML algorithm, just like the Parallel Cascade
Filter [15], allows the extension for models of greater rank.
This path will be pursued in future publications.

APPENDIX A

PROOF OF THE GRADIENT FORMULA

Through (7) and (8), one can interpret (14) as a tensor

equation, in the sense that we can index the matrices as

so to write

(RuK )i1,...,iK
and

j1,...,jK

j1,...,jK = E(u⊗K∗u⊗K)i1,...,iK
(RuK d)j1,...,jK = E(u⊗Kd)j1,...,jK
(cid:89)
(cid:88)
(cid:89)

(RuK d)j1,...,jK

(cid:88)

(cid:89)

j1,...,jK

(cid:96)

(w(cid:96))j(cid:96),

RuK dw =

where (w(cid:96))j(cid:96) is the j(cid:96)-th coordinate of w(cid:96), and
(RuK )i1,...,iK
j1,...,jK

(wp)ip∗

RuK w =

w

∗

(w(cid:96))j(cid:96).

i1,...,iK
j1,...,jK

p

(cid:96)

The other terms from (14) involve only the conjugates of

the entries of w, so they became to zero [8].

The gradient over the vector ws is given by the derivatives

over each of its components:

∂(RuK d)w

∂(ws)jq

=

(RuK d)j1,...,jK

(w(cid:96))j(cid:96)δjs
jq .

j is the Kronecker delta and also the representation of
δi
the identity matrix. In the above expression, the delta—the
identity—takes the place of the factor (ws)js. This observation
leads directly to

∇ws (RuK dw) = RuK d(w1 ⊗ ··· ⊗(cid:99)ws ⊗ ··· ⊗ wK).

For w∗RuK w, we have, remembering we do not derivate
the conjugates [8],
∂(w∗RuK w)

(cid:88)

(cid:89)

(cid:89)

∗
p)ip (RuK )i1,...,iK
j1,...,jK

(w

(w(cid:96))j(cid:96)δjs
jq .

∂(ws)jq

=

(cid:96)(cid:54)=s

i1,...,iK
j1,...,jK

p

∗

∗

) = w

RuK (w1 ⊗ ··· ⊗(cid:99)ws ⊗ ··· ⊗ wK).
Under the same argument,
∇ws(wRuK w
Therefore, we combine those two terms to get
uK d − RuK dw + w
∗
∗
RuK ](w1 ⊗ ··· ⊗(cid:99)ws ⊗ ··· ⊗ wK).
RuK w)

= −∇ws (RuK dw) + ∇ws(w
= [−RuK d + w

∇wsMSE = ∇ws(Rd − w

RuK w)

R

∗

∗

∗

(cid:88)

j1,...,jK

(cid:89)

(cid:96)(cid:54)=s

REFERENCES

[1] T. Ogunfunmi, Adaptive Nonlinear System Indentiﬁcation: The Volterra
and Wiener Model Approaches. Secaucus, NJ, USA: Springer-Verlag
New York, Inc., 2006.

[2] V. Mathews, “Adaptive polynomial ﬁlters,” Signal Processing Magazine,

IEEE, vol. 8, no. 3, pp. 10–26, July 1991.

[3] S. Boyd, L. O. Chua, and C. A. Desoer, “Adaptive polynomial ﬁlters,”
IMA Journal of Mathematical Control and Information, Oxford Univer-
sity Press, vol. 1, no. 3, pp. 243–282, 1984.

[4] R. Nowak and B. Van Veen, “Tensor product basis approximations for
volterra ﬁlters,” Signal Processing, IEEE Transactions on, vol. 44, no. 1,
pp. 36–50, Jan 1996.

[5] E. Batista, O. J. Tobias, and R. Seara, “A fully lms adaptive interpolated
volterra structure,” in Acoustics, Speech and Signal Processing, 2008.
ICASSP 2008. IEEE International Conference on, March 2008, pp.
3613–3616.

[6] E. Batista, O. Tobias, and R. Seara, “A sparse-interpolated scheme
for implementing adaptive volterra ﬁlters,” Signal Processing, IEEE
Transactions on, vol. 58, no. 4, pp. 2022–2035, April 2010.

[7] T. Andre, R. Nowak, and B. Van Veen, “Low rank estimation of
higher order statistics,” in Acoustics, Speech, and Signal Processing,
1996. ICASSP-96. Conference Proceedings., 1996 IEEE International
Conference on, vol. 5, May 1996, pp. 3026–308a vol. 5.
[8] A. H. Sayed, Adaptive Filters. Wiley-IEEE Press, 2008.
[9] J. Brewer, “Kronecker products and matrix calculus in system theory,”
Circuits and Systems, IEEE Transactions on, vol. 25, no. 9, pp. 772–781,
Sep 1978.

[10] S. Roman, Advanced Linear Algebra. Springer, 2007.
[11] W. Greub, Multilinear Algebra, ser. Universitext. Springer New York,

2012.

[12] F. Kuech, A. Mitnacht, and W. Kellermann, “Nonlinear acoustic echo
cancellation using adaptive orthogonalized power ﬁlters,” in Acoustics,
Speech, and Signal Processing, 2005. Proceedings. (ICASSP ’05). IEEE
International Conference on, vol. 3, March 2005, pp. iii/105–iii/108 Vol.
3.

[13] A. Fermo, A. Carini, and G. L. Sicuranza, “Low-complexity nonlinear
adaptive ﬁlters for acoustic echo cancellation in gsm handset receivers,”
European Transactions on Telecommunications, vol. 14, no. 2, pp.
161–169, 2003. [Online]. Available: http://dx.doi.org/10.1002/ett.908

[14] ——, “Simpliﬁed volterra ﬁlters for acoustic echo cancellation in gsm
receivers,” in Signal Processing Conference, 2000 10th European, Sept
2000, pp. 1–4.

[15] T. M. Panicker, V. J. Mathews, and G. L. Sicuranza, “Adaptive parallel-
cascade truncated volterra ﬁlters,” IEEE Transactions on Signal Pro-
cessing, vol. 46, no. 10, pp. 2664–2673, Oct 1998.

plants.pdfTable1:VectorweightsfortheCasesIandII,withK=2.‘12345678910(h1)‘0.544−0.2520.5930.236−0.0770.156−0.5000.025−0.0230.099(h2)‘−0.2040.2740.0230.0240.022−0.274−0.321−0.0700.7120.433Table2:VectorweightsforCaseIII,withK=2.‘12345678910(h1)‘0.08860.20810.1728−0.5515−0.02100.15250.40910.2540−0.40350.1340(h2)‘0.1033−0.22620.03090.38450.20070.06740.3802−0.30550.5574−0.1317Table3:VectorweightsforCasesIVandVI,withK=3.‘12345678910(h1)‘0.35070.2508−0.49190.10220.00790.50220.2354−0.16440.10010.0928(h2)‘0.50390.1495−0.3670−0.1144−0.07570.09960.31850.0174−0.4451−0.2285(h3)‘0.33760.28410.54210.08500.1085−0.30460.02070.00700.21590.37980510152005101520−0.0500.050.10.150.2i1i2h(i1,i2)Figure1:Order2VolterrakernelforCaseV.1