6
1
0
2

 
r
p
A
1

 

 
 
]
h
p
-
t
n
a
u
q
[
 
 

2
v
5
0
7
5
0

.

3
0
6
1
:
v
i
X
r
a

Loophole-free Bell test using electron spins in
diamond: second experiment and additional
analysis
B. Hensen1,2, N. Kalb1,2, M.S. Blok1,2, A.E. Dr´eau1,2, A. Reiserer1,2, R.F.L. Vermeulen1,2,
R.N. Schouten1,2, M. Markham3, D.J. Twitchen3, K. Goodenough1, D. Elkouss1,
S. Wehner1, T.H. Taminiau1,2, and R. Hanson1,2,*

1QuTech, Delft University of Technology, P.O. Box 5046, 2600 GA Delft, The Netherlands
2Kavli Institute of Nanoscience Delft, Delft University of Technology, P.O. Box 5046, 2600 GA Delft, The Netherlands
3Element Six Innovation, Fermi Avenue, Harwell Oxford, Didcot, Oxfordshire OX110QR, United Kingdom
*r.hanson@tudelft.nl

ABSTRACT

The recently reported violation of a Bell inequality using entangled electronic spins in diamonds (Hensen et. al., Nature 526,
682-686) provided the ﬁrst loophole-free evidence against local-realist theories of nature. Here we report on data from a
second Bell experiment using the same experimental setup with minor modiﬁcations. We ﬁnd a violation of the CHSH-Bell
inequality of 2.35± 0.18, in agreement with the ﬁrst run, yielding an overall value of S = 2.38± 0.14. We calculate the resulting
P-values of the second experiment and of the combined Bell tests. We provide an additional analysis of the distribution of
settings choices recorded during the two tests, ﬁnding that the observed distributions are consistent with uniform settings for
both tests. Finally, we analytically study the effect of particular models of random number generator (RNG) imperfection on our
hypothesis test. We ﬁnd that the winning probability per trial in the CHSH game can be bounded knowing only the mean of the
RNG bias, implying that our experimental result is robust for any model underlying the estimated average RNG bias.

Introduction
Ever since its inception, the counterintuitive predictions of quantum theory have stimulated debate about the fundamental
nature of reality. In 1964, John Bell found that the correlations between outcomes of distant measurements allowed under
local realism1 are strictly bounded, while certain quantum mechanical states are predicted to violate this bound.2 Numerous
violations of a Bell inequality in agreement with quantum theory have been reported.3–16 However, due to experimental
limitations additional assumptions were required in all experiments up to 2015 in order to reject the local-realist hypothesis,
resulting in loopholes. Last year we reported the ﬁrst experimental loophole-free violation of the CHSH-Bell inequality using
entangled electron spins associated with nitrogen-vacancy (NV) centers in diamond, separated by 1.3 km.17 Less than three
months after our experiment, two groups observed violations of the CH-Eberhard inequality on spatially-separated photons18, 19
and before the end of the year ﬁrst signatures of a CHSH-Bell violation on single Rubidium atoms were found.20

Below, we report on data from a second loophole-free Bell test performed with the same setup as in Hensen et.al.17
Additionally, we analyse in detail the recorded distribution of settings choices in both the ﬁrst and second datasets. Finally, we
investigate the effect of arbitrary models underlying the bias in the random number generation.

Second run
After ﬁnishing the ﬁrst loophole-free Bell experiment in July 2015, both the A(lice) and B(ob) setups were modiﬁed and used
in various local experiments. In December 2015, we rebuilt the Bell setup for performing a second run of the Bell test, with
three small modiﬁcations compared to the ﬁrst run:

First, we add a source of classical random numbers for the input choices. A random basis choice is now made by applying
an XOR operation between a quantum random bit generated as previously21–23 and classical random bits based on Twitter
messages, as proposed by Pironio.24 In particular, we generate two sets of classical random numbers, one for the basis choice at
A, and one for the basis choice at B (see details in the following sections). At each location, 8 of these bits are fed into an
FPGA. Just before the random basis rotation, the 8 Twitter bits and 1 quantum random bit are combined by subsequent XOR
operations. The resulting bit is used as the input of the same microwave switch as used in the ﬁrst run.17 The XOR operation

takes 70 ns of additional time, shifting the start of the readout pulse to a later time by the same amount. We leave the end of the
readout window unchanged, resulting in the same locality conditions as in the ﬁrst test.

We note that the Twitter-based classical random bits by themselves cannot close the locality loophole: the raw data is
available on the Internet well before the trials and the protocol to derive the bits is deterministic and programmed locally. The
only operations that are performed in a space-like separated manner are the XOR operations between 8 stored bits. Therefore,
strictly speaking only the quantum-RNG is providing fresh random bits. Since a loophole-free Bell test is described solely by
the random input bit generation and the outcome recording at A and B (and in our case the event-ready signal recording at C),
the second run can test the same null hypothesis as the ﬁrst run as these events are unchanged. That being said, the use of the
Twitter-based classical randomness puts an additional constraint on local-hidden-variable models attempting to explain our data.
Second, we set larger (i.e. less conservative) heralding windows at the event-ready detector in order to increase the data rate
compared to the ﬁrst experiment. We start the heralding window about 700 picoseconds earlier, motivated by the data from the
ﬁrst test (grey line in Fig. 3). We predeﬁne a window start of 5426.0 ns after the sync pulse for channel 0, and 5425.1 ns for
channel 1. We set a window length of 50 ns.
Finally, we also use the ψ +- Bell state, which is heralded by two photo-detection events in the same beamsplitter output arm
at the event-ready station. In general the ﬁdelity of this Bell state is lower than that of ψ− due to detector after-pulsing25 (note
that for ψ− the after-pulsing is not relevant because ψ− is heralded by photo detection events in different beamsplitter output
arms). However, we found the after-pulsing effect to be small enough for the detectors used in this run. We set an adapted
window length of the second window of 4 ns and 2.5 ns for channels 0, 1 respectively, where the exponentially decaying
NV emission is still large relative to the after-pulsing probability. As described below, we can combine the ψ−-related and
ψ +-related Bell trials into a single hypothesis test.26

Apart from these modiﬁcations, all settings, analysis software, calibrations and stabilisation routines were identical to those

in the ﬁrst run.17

Random numbers from Twitter
After each potential heralding event (corresponding to the E-events described in the Supplementary Information of Hensen
et.al.17), both at location A and B we take 8 new bits from a predeﬁned random dataset (one for A and one for B) based on
Twitter messages, to send to the FPGA-based random-number combiner.

The random dataset for A was obtained by collecting 139952 messages from Twitter trending topic with hash-tag #2Day-
sUntilMITAM, starting from 14:47:58 November 11th, 2015. The messages were collected using the Python Tweepy-package
(www.tweepy.org). Only the actual message text was used (no headers), consisting of at most 140 Unicode characters. From
each message a single bit was obtained by ﬁrst converting each character into an integer representing its Unicode code point,
converting the integer to the smallest binary bit-string representing that number and ﬁnally taking the parity of all the resulting
bit-strings together (even or uneven number of ones). The dataset for B was similarly obtained from 134501 messages with the
hash-tag #3DaysTillPURPOSE, streamed prior to the dataset A, starting from 16:52:44 November 10th, 2015.

Figure 1. Schematic of random input bit generation by combining bits from a quantum random number generator (QRNG)
and classical random bits from a dataset based on Twitter messages.

We note that although one may expect the Justin Bieber and One Direction fan-bases to be sufﬁciently disjoint to produce
an uncorrelated binary dataset, the hashtag from dataset B featured in 2 out of 139952 tweets of dataset A, and vice-versa in 4
out of 134501 tweets. Still, a Fisher-exact independence test of A (ﬁrst 134501 bits) and B’s dataset results in a P-value of
√
0.63. The bias of the 8-bit parity sets were 0.44% and 0.95%, with statistical uncertainty ( 1
N ) of 0.38% and 0.39% for A,B
2
respectively. As these bits are XOR’ed with bits from the quantum random number generator with much smaller bias, this has
no expected effect on the bias in the used input settings. Finally, we characterized the performance of the FPGA combiners,
which showed no errors on 108 XOR operations.

APD replacement
After 5 days of measurement, the APD at location C corresponding to channel 0 broke down during the daily calibration routine
and was subsequently replaced. To take into account the changed detection-to-output delay for the event-ready ﬁlter settings,

2/12

QRNGTwitterdatasetfast XOR8 bits1 bitMicrowaveswitchtriggerthe laser pulse arrival time was recorded for the new APD before proceeding. We adapted the start of the event-ready window
for channel 0 accordingly, and used this for all the data taken afterwards.
Joint P-value for ψ− and ψ + heralded events
Here we expand the statistical analysis used for the ﬁrst run17 to incorporate the ψ− and ψ + events into one hypothesis test.
For each of these states we perform a different variant of the CHSH game, and then use the methods of Elkouss and Wehner26
to combine the two: The output signal of the “event-ready”-box tm = (ti)m
i=1 now has three possible outcomes, where the tag
ti = 0 still corresponds to a failure (no, not ready) event. We now distinguish two different successful preparations of the boxes
A and B: ti = −1 corresponds to a successful preparations of the ψ− Bell state, and ti = +1 to a ψ + Bell state. In terms of
non-local games, Alice and Bob are playing two different games, where in case ti = −1 they must have (−1)aibixiyi = 1 in
order to win, and in case of ti = +1 they must have (−1)ai(bi⊕1)xiyi = 1 to win. Note that both games have the same maximum
winning probabilities. This means that we can take k := k− + k+, with k− the number of times (−1)aibixiyi = 1, and k+ the
number of times (−1)ai(bi⊕1)xiyi = 1; the remainder of the analysis remains the same and in particular the obtained bound to the
P-value is unchanged (see Elkouss and Wehner,26 page 20). We then have for the adapted CHSH function (see Supplementary
Information of Hensen et.al.17):
|ti|· (−1)ai(bi+

k(cid:48) :=

(1)

m

ti+1
2 )xiyi + 1
2

,

∑

i=1

and adapted total number of events then becomes:

n(cid:48) := |tm| =

m

∑

i=1

|ti|.

(2)

Results
In this test we set the total number of Bell trials n2 = 300. After 210 hours of measurement over 22 days during 1 month, we
ﬁnd S2 = 2.35± 0.18, with S2 the weighted average of Sψ− = (cid:104)x· y(cid:105)(0,0) +(cid:104)x· y(cid:105)(0,1) +(cid:104)x· y(cid:105)(1,0) −(cid:104)x· y(cid:105)(1,1) for ψ− heralded
events (different detectors clicked), and Sψ + = (cid:104)x· y(cid:105)(0,0) +(cid:104)x· y(cid:105)(0,1) −(cid:104)x· y(cid:105)(1,0) +(cid:104)x· y(cid:105)(1,1) for ψ + (same detector clicked).
See Figure 2.
This yields a P-value of 0.029 in the conventional analysis17 (which assumes independent trials, perfect random number
generators and Gaussian statistics), and with k2 = 237 a P-value of 0.061 in the complete analysis17 (which allows for arbitrary
memory between the trial, partially predictable random inputs and makes no assumptions about the probability distributions).

Figure 2. Second loophole-free Bell test results. (a) Summary of the data and the CHSH correlations. We record a total of
n2 = 300 trials of the Bell test. Dotted lines indicate the expected correlation based on the spin readout ﬁdelities and the
characterization measurements presented in Hensen et.al.17 Shown are data for both ψ− heralded events (red, two clicks in
different APD’s at location C), and for ψ + heralded events (blue, two clicks in the same APD). Numbers in bars represent the
amount of correlated and anti-correlated outcomes respectively, for ψ− (red) and ψ + (orange). Error bars shown are

(cid:114)(cid:16)

(cid:17)

1−(cid:104)x· y(cid:105)2

(a,b)

/n(a,b), with n(a,b) the number of events with inputs (a,b).

3/12

Figure 3. CHSH parameter S, number of Bell trials n, and post-selected complete-analysis local P-value versus window start
offset for the event-ready photon detections at location C, for the ﬁrst (grey) and second (orange) dataset. The time-offset
shown is with respect to the predeﬁned windows (corresponding to the dotted line). Conﬁdence region shown is one sigma,
calculated according to the conventional analysis. Shifting the window back in time, the relative fraction of heralding events
caused by photo-detection from laser reﬂections increases, thereby reducing the observed Bell violation.

Combined P-value for the two tests
We now turn to analysing the statistical signiﬁcance of the two runs combined. Extending the conventional analysis, we take the
weighted sum of the CHSH parameters obtained for both tests to ﬁnd Scombined = 2.38± 0.136, yielding a P-value of 2.6·10−3.
For the complete analysis, we give here two cases. The ﬁrst case is where the tests are considered to be fully independent; we
can then combine the P-values using Fisher’s method, resulting in a joint P-value of 1.7· 10−2 for the complete analysis. In the
second case we consider the two runs as a single test; we can then combine the data, and ﬁnd k1 + k2 = 433 for n1 + n2 = 545,
resulting in a joint P-value of 8.0·10−3 for the complete analysis. We emphasize that these are extreme interpretations of a
subtle situation and these P-values should be considered accordingly.

Although the predeﬁned event-ready ﬁlter settings were used for the hypothesis tests presented, the datasets recorded during
the Bell experiments contain all the photon detection times at location C. This allows us to investigate the effect of choosing
different heralding windows in post-processing. Such an analysis does not yield reliable global P-values (look-elswehere
effect), but can give insight in the physics and optimal parameters of the experiment. In Fig. 3 we present the dependence of the
recorded Bell violation S, and number of Bell trials n, if we offset the start of the windows. For negative offsets, photo-detection
events caused by reﬂected laser light starts to play an important role, and as expected the Bell violation decreases since the
event-ready signal is in that regime no longer a reliable indicator of the generation of an entangled state. The observed difference
between the runs in offset times at which the laser reﬂections start to play a role are caused by the less aggressive ﬁlter settings
in the second run. However, we see that in both runs the S-value remains constant up a negative offset of about 0.8 ns, indicating
that the ﬁlter settings were still chosen on the conservative side.

Statistical analysis of settings choices
Both for the Bell run in Hensen et.al.17 and for the Bell run presented above, we are testing a single well-deﬁned null hypothesis
formulated before the experiment, namely that a local-realist model for space-like separated sites could produce data with a
violation at least as large as we observe. The settings independence is guaranteed by the space-like separation of relevant events
(at stations A, B and C). Since no-signalling is part of this local-realist model, there is no extra assumption that needs to be
checked in the data. We have carefully calibrated and checked all timings to ensure that the locality loophole is indeed closed.
Nonetheless, one can still check (post-experiment) for many other types of potential correlations in the recorded dataset if
one wishes to. However, since now many hypotheses are tested in parallel, P-values should take into account the fact that one is

4/12

2101234Event ready signal start offset (ns)10-310-210-1100P-value200300400500600n1.401.601.802.002.202.402.60STable 1. From left to right each column corresponds with: dataset on which statistics are computed, local P-value for the null
hypothesis RNG A is uniform, local P-value for the null hypothesis RNG B is uniform, local P-value for the null hypothesis
RNG A&B is uniform, Fisher’s test, Pearson’s test, and pthreshold and joint P-valuespjoint. The joint P-value for a set of
hypotheses is the probability that for at least one of the hypotheses we observe a P-value less than α where here α = 0.05. This
captures the fact that the more hypotheses we test, the more likely it becomes that one of them will fall below the signiﬁcance
threshold. The value pthreshold the largest threshold for individual tests for which the joint P-value for that row is less than 0.05.
The local P-values in the row should this be compared to this number. This captures the fact that when testing multiple
hypothesis, the local P-values of the individual ones actually need to be much smaller for the overall test to be signiﬁcant. The
local P-values in columns RNG A, RNG B, Fisher and Pearson are exact calculations. The columns RNG A&B, pthreshold and
pjoint are approximations obtained via 105, 104 and 104 trials of a Monte-Carlo simulation, respectively.

doing multiple comparisons (the look-elsewhere effect, LEE). Failure to do so can lead to too many false positives, an effect
well known in particle physics. In contrast, there is no LEE for a single pre-deﬁned null hypothesis as in our Bell test.

Formulation and testing of multiple hypotheses can result in obtaining almost arbitrarily low local P-values, which may
have almost no global signiﬁcance. As an example, recalculating the P-value for the local realist hypothesis, given the ﬁrst
dataset for a window start offset of -900 picoseconds compared to the predeﬁned window starts, results in a local P-value of
0.0081 using the complete analysis (see Fig. 3). Taking this to the extreme by doing a search of the window start offsets for
both channels independently and the joint window length offset, results in a local P-value of 0.0018. These examples clearly
illustrate that without taking into account that multiple hypotheses are being tested, such local P-values can not be used to
assign signiﬁcance.

With these considerations in mind we analyse the settings choices in the two sub-sections below.

Settings choices in the ﬁrst and second dataset
The distribution of the 245 input settings in the ﬁrst dataset (see Figure 4a in Hensen et.al.17) is (n(0,0),n(0,1),n(1,0),n(1,1)) =
(53,79,62,51), with n(a,b) the number times the inputs (a,b) were used. This realisation looks somewhat unbalanced for
a uniform distribution, and one could be motivated to test the null hypothesis that the RNGs are uniform. Performing a
Monte-Carlo simulation of 105 realisations of a uniform multinomial distribution with size n = 245 we ﬁnd a local P-value of
0.053 to get such a distribution or more extreme. We can get further insight by looking at all the setting choices recorded during
the test. Around every potential heralding event about 5000 settings are recorded, for which we ﬁnd a local P-value of 0.57
(Table 1), consistent with a uniform setting distribution.

Many additional tests can be performed on equally many slices or subsets of the data, where one or more of the ﬁlters
(see Supplementary Information of Hensen et.al.17) is relaxed. In Table 1 we list the individual (local) P-values for a set of 4
hypotheses regarding the settings choices, for both the ﬁrst and second dataset.

1. RNG A is uniform

2. RNG B is uniform

3. RNG A and RNG B are jointly uniform
4. Fisher’s exact test27 for n < 5000, Pearson’s χ2 test28 for n > 5000)

For tests 1 and 2 we evaluate a two tailed binomial test with equal success probability. For test 3 we perform a Monte-Carlo
simulation of 105 realisations of a uniform multinomial distribution with size ﬁxed to the number of observations in that
particular row, i.e. n = 245 for the second row in Table 1.

We observe that only one local P-value is below 0.05: Fisher’s exact test on the distribution of the settings in the ﬁrst data set
yields a local P-value of 0.029. However, as described in the next subsection below, when properly taking the look-elsewhere
effect into account this does not result in a signiﬁcant rejection of the uniform settings hypothesis at the 0.05 level. Finally, the
valid Bell trials of the ﬁrst and second dataset combined, shown in the last row of Table 1, are also consistent with uniformly
chosen input settings.

5/12

rehsiFnosraePB&A GNRB GNRA GNRtesatadRun 1 All recorded data4938847494210149393284942337197626130.8720.1590.5680.9560.0160.144Bell trials537962512450.2500.3710.0540.0290.0210.121Run 2All recorded data4529615453094345282954526440181152930.1710.9010.4860.4550.0160.144Bell trials696978843000.1840.7730.5450.8170.0180.1311221481401355450.8640.3920.4520.2110.0200.138Bell trials of both runs combinedn(0,0)n(0,1)n(1,0)n(1,1)njointpthresholdpSigniﬁcance and look-elsewhere effect
We now analyse the signiﬁcance of the local P-values in Table 1 by taking into account the look-elsewhere effect. Say we are
looking for correlations between parameters that are in fact completely independent. Looking at one correlation, it is as if we
take one random sample from a distribution; the probability that it is at 2 sigma or more extreme is thus about 0.05. If we look
for 4 different correlations (assuming all parameters are independent), it is similar to taking 4 random samples, and thus the
probability that at least one is at 2 sigma or more extreme is 1− (1− 0.05)4 = 0.18. In reverse, assuming fully independent
hypotheses, the local P-value p(cid:48) should have obeyed roughly 1− (1− p(cid:48))4 < 0.05, so p(cid:48) < 0.013, to be statistically signiﬁcant
at the 0.05 level.

In our case it is actually more complicated because there can be dependencies between hypotheses. We can numerically get
some of these numbers. For instance, we have simulated the random number generation (RNG) using Monte-Carlo under the
assumption of independent uniform outputs and calculated local P-values for the four hypotheses listed above. The probability
that at least one of these yield local P-value p(cid:48) < 0.05 turns out to be about 0.13 for the 245 events in the Bell test. This is
different from 1− (1− 0.05)4 = 0.18 because of correlations between the tests, but it is clearly much higher than 0.05. In
reverse, to arrive at an overall probability of 0.05 of ﬁnding at least one test yielding local P-value p(cid:48) < pthreshold for the data in
the ﬁrst Bell dataset, we ﬁnd pthreshold = 0.021. In other words, if we would only be looking at the settings corresponding to the
valid Bell trials, then a local P-value below 0.021 would signal a statistically signiﬁcant violation of our hypothesis at the 0.05
level. We do not ﬁnd such evidence for the valid Bell trial data (see ﬁrst row in Table 1).
The last column gives the probability that at least one of the hypothesis tests on the data in that row yields a local P-
value p(cid:48) < 0.05, given uniform settings. In the one-but-last column we give pthreshold, again only for the data set in that row, for
a signiﬁcance at the 0.05 level. These values assume that we would only be testing our hypotheses on that particular row. Since
we are now looking at different rows, pthreshold for each row is a strict upper bound to the pthreshold for the full table, as we are
looking at different cross-sections of the raw data set at the same time; the pthreshold for the full table will thus be lower but it is
not trivial to compute this, given the large dependence between the subsets of data used for each row. However, since we do not
ﬁnd any local P-value to be below pthreshold for the corresponding row, we can conclude that the data does not allow rejection of
the settings independence hypothesis, even without calculating the global pthreshold for the full Table.

Reﬁned analysis of imperfect random number generators
Ideally, the RNGs yield a fully unpredictable random bit in every trial of the Bell test. A deviation from the ideal behaviour
can be denoted by an excess predictability or bias b, that can take on values between 0 and 1
2. In principle the value of b
can be different in every trial of a Bell test, which can be modelled by some probability distribution over the value of b. By
characterising the physical RNGs, we can hope to learn something about the mean τ of this probability distribution. As a
particular example of an underlying probability distribution for the bias, consider the case where the random bit is perfectly
2) with probability f and perfectly unpredictable (b = 0) with probability 1− f . This example could model a
predictable (b = 1
scenario where the random numbers are generated with some spread in time such that some of them are produced so early that
they could be known by the other party before the end of the trial.

A recent analysis of the effect of partial predictability of RNGs on the bound of the CH-Eberhard inequality revealed a
strong dependence on the interpretation of the mean excess predictability,29 estimated from characterisation of the RNGs.
In particular, for a model in which the mean excess predictability ε is distributed (evenly) over all trials, the CH-Eberhard
inequality can be violated even if the relevant Bell parameter J (which can be viewed as an average violation per trial in terms
of probabilities) is much lower than ε. On the other hand, Koﬂer et. al.29 found that in case of an all-or-nothing scenario, such
that in a fraction ε of the trials the RNG is fully predictable and in the rest of the trials fully unpredictable, the threshold value
for a violation is roughly given by J > ε.

Motivated by these ﬁndings, we generalize here the analysis of the effect of imperfect random number generators on the
winning probability per trial in the CHSH game. We extend the analysis in the Supplementary Information of Hensen et.al.17
(see also Elkouss and Wehner26) to the case where any bias b is produced by an arbitrary underlying probability distribution per
trial. That is, there is no maximum bias, but rather the bias can probabilistically take on any value. We ﬁnd that in our case, as
long as the event-ready signal is independent of the random bits, the only relevant parameter is the mean τ of the bias; the
concrete form of the random variable has no impact on the bound on the probability of winning CHSH.

In the analysis below we explicitly take into account the possibility of early production of random bits, which we expect
to be a particular interpretation of the probability distribution over b as above. Indeed we ﬁnd that when the random bits are
perfectly predictable with probability f , and perfectly unpredictable with probability 1− f , then a distribution over the bias b
with a mean of τ = f /2 links the two viewpoints of the analysis.

In order to make the discussion precise, in the following we describe the random variables that characterize the experiment,

then make a rigorous derivation of the winning probability.

6/12

Figure 4. The P-value of the two runs as a function of τ, the mean bias of the RNG.

i=1,Bm = (Bi)m

Properties of the tested LHVM
We introduce the following sequences of random variables. The notation and arguments borrow from earlier work.26, 30–32 Let
Am = (Ai)m
i=1 the histories of
attempts previous to the i-th attempt, Cm = (Ci)m
i=1 is the sequence of
event-ready signals in the case of an event-ready experiment. In an event-ready experiment, we make no assumptions regarding
the statistics of the event-ready station, which may be under full control of the LHVM, and can depend arbitrarily on the history
of the experiment.

i=1 the outputs of the boxes where i is used to label the i-th element, Hm = (Hi)m

i=1 denotes the scores at each attempt and Tm = (Ti)m

i=1,QBm
= (FA

We introduce three sequences of random variables to model each RNG. Let Xm = (Xi)m

i=1 denote the inputs
to the boxes. Let QAm
i )m
i=1 denote two sequences of binary variables that take value 1 if the random
number was generated so early that signaling is possible and 0 otherwise. We call the former an early number and the latter a
i=1 take values in the range [−1/2,1/2] and denote the bias of the
on-time number. Finally, let FAm
random number generators at each attempt. We here assume that these distributions can differ for all i, they do not depend on
the history Hi. Using more involved notation, the same bound can be made if their mean is known conditioned on the history.
The random variable Hi models the state of the experiment prior to the measurement. As such, Hi includes any hidden
variables, sometimes denoted using the letter λ .30 It also includes the history of all possible conﬁgurations of inputs and outputs
of the prior attempts (Xj,Yj,A j,B j,Tj)i−1
j=1.

i )m
i=1,FBm

The null hypothesis (to be refuted) is that our experimental setup can be modelled using a LHVM. LHVMs verify the

i=1,Ym = (Yi)m

= (QA

= (QB

= (FB

i )m

i )m

following conditions:

1. Independent random number generators. Conditioned on the history of the experiment the random numbers are

(3)

independent of each other
i|=Yi,FB

∀i,Xi,FA

i ,QA

i ,QB

i | Hi ,

and of the output of the event-ready signal
i|=Ti | Hi .

i|=Ti,Yi,FB

∀i,Xi,FA

i ,QB

i ,QA

(4)
We allow Xi and Yi to be partially predictable given the history of the experiment. The predictability is governed by some
random variables FA

i ,FB

i we have

i ,by ∈ FB

i . For bx ∈ FA

∀(i,xi,hi),
∀(i,yi,hi),

− bx ≤ Pr(cid:0)Xi = xi|Hi = hi,FA
− by ≤ Pr(cid:0)Yi = yi|Hi = hi,FA

(cid:1) ≤ 1
(cid:1) ≤ 1
(cid:3) ≤ τB. We deﬁne f = max{ fA, fB} and τ = max{τA,τB}.

Furthermore, from the characterization of the devices we have that for all i: E(cid:2)QA
fB,E(cid:2)FB

i = bx,FB

i = bx,FB

i = by

i = by

1
2
1
2

+ by ,

2

2

i

i

+ bx ,

(5)

(cid:3) ≤ fA,E(cid:2)FA

i

(cid:3) ≤ τA,E(cid:2)QB

i

(6)

(cid:3) ≤

2. Locality. The outputs ai and bi only depend on the local input settings and history: they are independent of each other
and of the input setting at the other side, conditioned on the previous history, the current event-ready signal and the inputs
being generated on-time

∀i, (Xi,Ai)|=(Yi,Bi)|QA

i = 0,QB

i = 0,Hi,Ti .

(7)

7/12

10-610-510-410-310-210-1τ10-210-1100P-value1st run2nd run3. Sequentiality of the experiments. Every one of the m attempts takes place sequentially such that any possible signalling

between different attempts beyond the previous conditions is prevented.33

Except for these conditions the variables might be correlated in any possible way.

Winning probability for imperfect random number generators
Here, we derive a tight upper bound on the winning probability of CHSH with imperfect random number generators in an
event-ready setup. For CHSH, the inputs Xi,Yi, outputs Ai,Bi and output of the heralding station Ti take values 0 and 1. If Ti = 0
the scoring variable Ci takes always the value zero, if Ti = 1 then Ci = 1 when x· y = a⊕ b and Ci = 0 in the remaining cases.
We will take the RNGs to have maximum advantage f of producing early random numbers.
Lemma 1. Let m ∈ N, and let a sequence of random variables as described in the previous section correspond with m attempts
of a CHSH heralding experiment. Suppose that the null hypothesis holds, i.e., nature is governed by an LHVM. Given that for

(cid:3) ≤ τ, we have for i ≤ m, any possible history Hi = hi of the experiment,

all i ≤ m: E(cid:2)QA

(cid:3) ≤ τ,E(cid:2)QB

(cid:3) = f ,E(cid:2)FA

(cid:3) = f ,E(cid:2)FB

i

i

i

i

and Ti = 1 that the probability of Ci = 1 is upper bounded by

Pr (Ci = 1|Hi = hi,Ti = 1) ≤ β 1

win = 2 f − f 2 + (1− f )2(cid:0) 3

4 + τ(cid:48) − τ(cid:48)2(cid:1) and τ(cid:48) := min{ 2τ+ f

win ,

2}.
2(1− f ) , 1

where β 1

(12)

(13)

(14)

(16)

(17)
(18)

8/12

(cid:1)

(8)

(9)

(10)
(11)

Proof. Let us ﬁrst bound the effect of the early random numbers in the winning probability. We have

(cid:1)Pr(cid:0)Ci = 1|Hi = hi,Ti = 1,QA
Pr (Ci = 1|Hi = hi,Ti = 1) = ∑
i = 1(cid:1)
≤ Pr(cid:0)QA
+ Pr(cid:0)QA
(cid:1)
≤ 2 f − f 2 + (1− f )2Pr(cid:0)Ci = 1|Hi = hi,Ti = 1,QA = 0,QB = 0(cid:1)

i = 1,QB
i = sx,QB

i = sx,QB

i = 0,QB

i = 0,QB

sx,sy∈{0,1}

i = sy

i = sy

i = 1,QB

Pr(cid:0)QA
i = 1(cid:1) + Pr(cid:0)QA
i = 0(cid:1) + Pr(cid:0)QA
i = 0(cid:1)Pr(cid:0)Ci = 1|Hi = hi,Ti = 1,QA
(cid:3) = E(cid:2)QB
(cid:1)

(cid:3) = f .

second inequality follows from assuming that E(cid:2)QA
(cid:1)Pr(cid:0)Xi = 1|FA

dbxPr(cid:0)FA

Pr (Xi = 1) =

for Yi. For simplicity, we omit the explict conditioning on Hi = hi. First of all, note that since

(cid:90)

i

i

i = bx

i = bx

i = sx,QB

i = sy

Let us now bound the bias for the on-time numbers. We focus on the random numbers Xi; the same argument can be made

The ﬁrst inequality follows by assuming that the CHSH is won with probability one when a random number is early. The

we have together with (5) that

1
2

− E[FA

i ] ≤ Pr (Xi = 1) ≤ 1
2

+ E[FA
i ]

Pr (Xi = 1) = Pr(cid:0)QA
i = 0(cid:1) ≤ 1
Pr(cid:0)Xi = 1|QA

+ τ(cid:48)

which implies 1/2− τ ≤ Pr (Xi = 1) ≤ 1/2 + τ. Furthermore, note that we can expand the probability as

i = 1(cid:1)Pr(cid:0)Xi = 1|QA

i = 1(cid:1) + Pr(cid:0)QA

i = 0(cid:1)Pr(cid:0)Xi = 1|QA

i = 0(cid:1) .

Combining (14), (13) and the assumption E(QA

i ) = f we obtain

where τ(cid:48) := min{ 2τ+ f
on-time. For simplicity, we drop the explicit conditioning on Hi = hi,Ti = 1,QA = 0,QB = 0.

(15)
2}. Let us now expand the probability that Ci = 1 conditioned on the event that both numbers were

2(1− f ) , 1

2

Pr (Ci = 1) = ∑

x,y,z∈{0,1}
(x,y)(cid:54)=(1,1)

Pr (Ai = z,Bi = z,Xi = x,Yi = y) + ∑
z∈{0,1}

Pr (Ai = z,Bi = z⊕ 1,Xi = 1,Yi = 1) .

We can break these probabilities into simpler terms

Pr(cid:0)Ai = a,Bi = b,Xi = x,Yi = y) = Pr (Ai = a,Xi = x)Pr (Bi = b,Yi = y)

= Pr (Xi = x)Pr (Ai = a|Xi = x)Pr (Yi = y)Pr (Bi = b|Yi = y) .

The ﬁrst equality followed by the locality condition, the second one simply by the deﬁnition of conditional probability. With
this decomposition, we can express (16) as

Pr (Ci = 1) = ∑

x,y∈{0,1}
(x,y)(cid:54)=(1,1)

αxβy (χxγy + (1− χx)(1− γy)) + α1β1 (χ1(1− γ1) + (1− χ1)γ1)

= ∑

x,y∈{0,1}

αxβy fx,y .

where we have used the shorthands

χx := Pr (Ai = 1) ,
γy := Pr (Bi = 1) ,
αx := Pr (Xi = x) ,
βy(cid:48) := Pr (Yi = y) ,

(cid:40) χxγy + (1− χx)(1− γy)

χx(1− γy) + (1− χx)γy

fx,y :=

if (x,y) (cid:54)= (1,1) ,
otherwise.

(19)

(20)

(21)
(22)
(23)
(24)

(25)

Now we will expand (20). We know that 1/2− τ ≤ αx,βy ≤ 1/2 + τ. In principle, τ does not need to take the values in the
extreme on the range. Without loss of generality let α0 = 1/2 + τA and β0 = 1/2 + τB, with τA,τB ∈ [−1/2,1/2].

∑

x,y∈{0,1}

αxβy fx,y =

=

2

(cid:18)1
(cid:18)1

+

+ τA

(cid:18)1
(cid:18)1

2
1
2

4

+

4

+

(cid:19)

(cid:19)(cid:18)1
(cid:19)(cid:18)1

2
− τA

+ τB

2

1
2

τA +
− 1
2

f0,0 +

(cid:19)
(cid:19)

+ τB

f1,0 +

τB + τAτB

f0,0 +

(cid:18)1

2

(cid:19)

τA +

τB − τAτB

f1,0 +

1
2

= (τA + τB) f0,0 + (τA − 2τAτB) f0,1
− 1
2

+ (τB − 2τAτB) f1,0 +

4

τA − 1
2

(cid:18)1

(cid:19)

(cid:19)(cid:18)1
(cid:19)(cid:18)1

− τB

f0,1

(cid:19)

+ τA

(cid:18)1
(cid:18)1

2

4

2
− τA
1
2

(cid:18)1

+

4

(cid:19)

− τB

f1,1

2
τA − 1
τB − τAτB
2
τA − 1
− 1
2
2
(cid:19)

τB + τAτB

f0,1

(cid:19)

τB + τAτB

fa,b .

∑

a,b

It thus remains to bound the sum of fx,y. Note that we can write

∑

x,y∈{0,1}

fx,y = (χ0γ0 + (1− χ0)(1− γ0)) + (χ0γ1 + (1− χ0)(1− γ1))

+ (χ1γ0 + (1− χ1)(1− γ0)) + (χ1(1− γ1) + (1− χ1)γ1)

= χ0 (γ0 + γ1) + (1− χ0) (2− γ0 − γ1)

+ χ1 (γ0 + 1− γ1) + (1− χ1) (1− γ0 + γ1) .

f1,1

(26)

(27)

(28)

(29)

(30)

Since (30) is a sum of two convex combinations, it must take its maximum value at one of the extreme points, that is with
χ0 ∈ {0,1} and χ1 ∈ {0,1}. We can thus consider all four combinations of values for χ0 and χ1 given by



3− 2γ0 if (χ0, χ1) = (0,0) ,
3− 2γ1 if (χ0, χ1) = (0,1) ,
1 + 2γ1 if (χ0, χ1) = (1,0) ,
1 + 2γ0 if (χ0, χ1) = (1,1) .

∑

x,y∈{0,1}

fx,y =

Since 0 ≤ γ0,γ1 ≤ 1, we have in all cases that the sum is upper bounded by 3.

(31)

9/12

Now, using (28) we have

Pr (Ci = 1) ≤ 2 (τA + τB − 2τAτB) + 3
(τA + τB)− τAτB

1
2

+
+ τ(cid:48) − τ(cid:48)2

3
=
4
≤ 3
4

(cid:18)1

4

− 1
2

τA − 1
2

τB + τAτB

(cid:19)

(32)

(33)

(34)
where in the ﬁrst inequality we bound f0,0, f0,1, f1,0 by 1. The second inequality follows since τ(cid:48) ≤ 1/2 and for τA,τB below
1/2 (33) is strictly increasing both in τA and τB; this implies that the maximum is found in the extreme: τ = τA = τB.

Finally, we can plug the bound in the winning probability given that both numbers were on time into the winning probability

and we obtain:

Pr (Ci = 1) ≤ 2 f − f 2 + (1− f )2

+ τ(cid:48) − τ(cid:48)2

=

3
4

+

1
2

f − 1
4

f 2 + τ − τ2 − 2 f τ + f 2τ + 2 f τ2 − f 2τ2.

(35)

(cid:18)3

4

(cid:19)

Equation (35) shows the equal footing of 1

2 f and τ. This result highlights the fact that early production of random numbers
is just a particular distribution underlying the bias of the random number generators, where the probability f of producing an
2) and the probability 1− f to unpredictable
early number corresponds to a mixture of completely predictable numbers (τ = 1
numbers.
The ﬁnding that the only relevant RNG parameter for the winning probability of the CHSH game is the mean bias makes a
Bell test based on this winning probability particularly robust. In our two Bell test runs, we ﬁnd a violation in terms of the
CHSH winning probability of about 0.05 and 0.04 respectively, both orders of magnitude larger than the mean bias (< 10−4),
and, given our theory result, independent of the underlying distribution of bias over the trials. As depicted in Fig. 4, this means
for instance that for a fraction of early produced random numbers of 10−3 our P-values are hardly affected. For comparison, in
this scenario an experiment using the CH-Eberhard inequality would require J > 10−3 to obtain a violation,29 which is two
orders of magnitude beyond the state of the art of photonic experiments.18, 19 This difference may be traced back to the use of
event-ready detectors in our experiments, which dramatically increases the ﬁdelity of the entangled state and thus the winning
probability per Bell trial. We stress that the above only holds if the event-ready signal is still independent of the early produced
random bits: in case the random bits are produced so early that they are not anymore space-like separated from the event-ready
signal generation and the event-ready detector could select the Bell trials based on random bits being produced too early. In our
Bell tests, this means that the above robustness spans to random bits being produced a maximum of 690 ns too early.

Conclusion
The loophole-free violation of Bell’s inequality in the second data run reported here further strengthens the rejection of a broad
class of local realistic theories. We ﬁnd that the data is consistent with independent setting choices, both in the ﬁrst and second
dataset, as well as in the combined dataset. Reﬁned analysis of the effect of a bias in the random number generators shows
that only the mean bias plays a role in the winning probability and thus in the P-value bound for our experiments. The large
spatial separation and the strong violation in winning probability per trial of about 0.05 makes our implementation promising
for future applications of non-locality for device-independent quantum key distribution34 and randomness generation.35, 36

References
1. Einstein, A., Podolsky, B. & Rosen, N. Can quantum-mechanical description of physical reality be considered complete?

Physical Review 47, 777–780 (1935).

2. Bell, J. On the einstein-podolsky-rosen paradox. Physics 1, 195–200 (1964).
3. Freedman, S. J. & Clauser, J. F. Experimental test of local hidden-variable theories. Physical Review Letters 28, 938–941

(1972).

4. Aspect, A., Dalibard, J. & Roger, G. Experimental test of Bell’s inequalities using time- varying analyzers. Physical

Review Letters 49, 1804–1807 (1982).

5. Tittel, W., Brendel, J., Zbinden, H. & Gisin, N. Violation of Bell inequalities by photons more than 10 km apart. Physical

Review Letters 81, 3563–3566 (1998).

10/12

6. Weihs, G., Jennewein, T., Simon, C., Weinfurter, H. & Zeilinger, A. Violation of Bell’s inequality under strict einstein

locality conditions. Physical Review Letters 81, 5039–5043 (1998).

7. Rowe, M. A. et al. Experimental violation of a Bell’s inequality with efﬁcient detection. Nature 409, 791–794 (2001).
8. Matsukevich, D. N., Maunz, P., Moehring, D. L., Olmschenk, S. & Monroe, C. Bell inequality violation with two remote

atomic qubits. Physical Review Letters 100, 150404 (2008).

9. Ansmann, M. et al. Violation of Bell’s inequality in josephson phase qubits. Nature 461, 504–506 (2009).
10. Scheidl, T. et al. Violation of local realism with freedom of choice. Proceedings of the National Academy of Sciences 107,

19708–19713 (2010).

11. Hofmann, J. et al. Heralded entanglement between widely separated atoms. Science 337, 72–75 (2012).
12. Pfaff, W. et al. Demonstration of entanglement-by-measurement of solid-state qubits. Nature Physics 9, 29–33 (2013).
13. Giustina, M. et al. Bell violation using entangled photons without the fair-sampling assumption. Nature 497, 227–230

(2013).

14. Christensen, B. G. et al. Detection-loophole-free test of quantum nonlocality, and applications. Physical Review Letters

111, 130406 (2013).

15. Ballance, C. J. et al. Hybrid quantum logic and a test of Bell’s inequality using two different atomic isotopes. Nature 528,

384–386 (2015).

16. Dehollain, J. P. et al. Bell’s inequality violation with spins in silicon. Nature Nanotechnology 11, 242–246 (2016).
17. Hensen, B. et al. Loophole-free Bell inequality violation using electron spins separated by 1.3 kilometres. Nature 526,

682–686 (2015).

18. Giustina, M. et al. Signiﬁcant-loophole-free test of Bell’s theorem with entangled photons. Physical Review Letters 115,

250401 (2015).

19. Shalm, L. K. et al. Strong loophole-free test of local realism*. Physical Review Letters 115, 250402 (2015).
20. Weinfurter, H. A Bell experiment - PQE - the winter colloquium on the Physics of Quantum Electronics (2016).
21. Abell´an, C. et al. Ultra-fast quantum randomness generation by accelerated phase diffusion in a pulsed laser diode. Optics

Express 22, 1645 (2014).

22. Mitchell, M. W., Abellan, C. & Amaya, W. Strong experimental guarantees in ultrafast quantum random number generation.

Physical Review A 91, 012314 (2015).

23. Abell´an, C., Amaya, W., Mitrani, D., Pruneri, V. & Mitchell, M. W. Generation of fresh and pure random numbers for

loophole-free Bell tests. Physical Review Letters 115, 250403 (2015).

24. Pironio, S. Random ’choices’ and the locality loophole. arXiv:1510.00248 [quant-ph] (2015).
25. Bernien, H. et al. Heralded entanglement between solid-state qubits separated by three metres. Nature 497, 86–90 (2013).
26. Elkouss, D. & Wehner, S. (nearly) optimal P-values for all Bell inequalities. arXiv:1510.07233 [quant-ph] (2015).
27. Fisher, R. A. On the interpretation of chi-squared from contingency tables, and the calculation of p. Journal of the Royal

Statistical Society 85, 87–94 (1922).

28. Pearson, K. F. X. on the criterion that a given system of deviations from the probable in the case of a correlated system of
variables is such that it can be reasonably supposed to have arisen from random sampling. Philosophical Magazine Series
5 50, 157–175 (1900).

29. Koﬂer, J., Giustina, M., Larsson, J.-k. & Mitchell, M. W. Requirements for a loophole-free photonic Bell test using

imperfect setting generators. arXiv:1411.4787 [quant-ph] (2014).

30. Brunner, N., Cavalcanti, D., Pironio, S., Scarani, V. & Wehner, S. Bell nonlocality. Reviews of Modern Physics 86, 419–478

(2014).

31. Bierhorst, P. A rigorous analysis of the Clauser-Horne-Shimony-Holt inequality experiment when trials need not be

independent. Foundations of Physics 44, 736–761 (2014).

32. Bierhorst, P. A robust mathematical model for a loophole-free clauser-horne experiment. Journal of Physics A: Mathemati-

cal and Theoretical 48, 195302 (2015).

33. Barrett, J., Collins, D., Hardy, L., Kent, A. & Popescu, S. Quantum nonlocality, Bell inequalities, and the memory loophole.

Physical Review A 66, 042111 (2002).

11/12

34. Ac´ın, A. et al. Device-independent security of quantum cryptography against collective attacks. Physical Review Letters

98, 230501 (2007).

35. Colbeck, R. Quantum and Relativistic Protocols for Secure Multi-Party Computation. Ph.D. thesis, University of

Cambridge (2007).

36. Pironio, S. et al. Random numbers certiﬁed by Bell’s theorem. Nature 464, 1021–1024 (2010).

Acknowledgements
We thank Jan- ˚Ake Larsson, Morgan Mitchell and Krister Shalm for useful discussions.

We acknowledge support from the Dutch Organization for Fundamental Research on Matter (FOM), Dutch Technology
Foundation (STW), the Netherlands Organization for Scientiﬁc Research (NWO) through a VENI grant (THT) and a VIDI
grant (SW) and the European Research Council through project HYSCORE.

Author contributions
Author Contributions B.H. and R.H. devised the experiment. B.H., N.K., A.E.D., A.R., M.S.B., R.F.L.V. and R.N.S. built and
characterized the experimental set-up. K.G. compiled the random Twitter datasets. M.M. and D.J.T. grew and prepared the
diamond device substrates. M.S.B. fabricated the devices. B.H., N.K., A.E.D., A.R. and M.S.B. collected and analysed the
data, with support from T.H.T. and R.H. D.E. and S.W. performed the theoretical analysis. B.H., D.E., S.W. and R.H. wrote the
manuscript. R.H. supervised the project.

12/12

