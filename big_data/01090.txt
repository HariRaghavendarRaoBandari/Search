6
1
0
2

 
r
a

M
3

 

 
 
]

H
O
.
s
c
[
 
 

1
v
0
9
0
1
0

.

3
0
6
1
:
v
i
X
r
a

Using Newton’s method to model a spatial light

distribution of a LED with attached secondary optics

David Kaljuna, Joˇze Petriˇsiˇca, Janez ˇZerovnika,b

aFS, University of Ljubljana, Aˇskerˇceva 6, 1000 Ljubljana, Slovenia

david.kaljun@fs.uni-lj.si

bInstitute of Mathematics, Physics and Mechanics, Jadranska 19, Ljubljana, Slovenia

janez.zerovnik@fs.uni-lj.si

Abstract

In design of optical systems based on LED (Light emitting diode) technol-
ogy, a crucial task is to handle the unstructured data describing properties of
optical elements in standard formats. This leads to the problem of data ﬁt-
ting within an appropriate model. Newtons method is used as an upgrade of
previously developed most promising discrete optimization heuristics show-
ing improvement of both performance and quality of solutions. Experiment
also indicates that a combination of an algorithm that ﬁnds promising initial
solutions as a preprocessor to Newtons method may be a winning idea, at
least on some datasets of instances.

Keywords:
optimization, local search, light distribution, LED

least squares function ﬁtting, Newton method, discrete

Highlights:
• Model for data ﬁtting of LED photometry with the evaluation function

is presented.

• The eﬀects of a numerical method in conjunction with heuristics are

studied.

• Algorithms are developed with the use of C++ programing language.
• The success of developed algorithms is tested on real and artiﬁcial

datasets.

• The results are statistically evaluated.
• The numerical Newtons method prevails on both datasets, and pro-

vides substantial runtime shortening.

Preprint submitted to Applied Mathematics and Computation

March 4, 2016

1. Introduction

The LED (Light emitting diode) industry has been evolving rapidly in
the past several years. The fast pace of research and development in the ﬁeld
had some expected impact. One of the results is a massive use and imple-
mentation od LED elements in all kind of luminaires. While some of these
luminaires are designed for ambient illumination the majority are technical
luminaires that have to conform not only to electrical and mechanical safety
regulations but also to regulations that deﬁne and restrict the photometry
of a certain luminaire. This means that the photometry of a luminaire has
to be deﬁned prior to the production. In order to do that eﬃciently and
with minimal errors the design engineer must virtually test the luminaires
performance. Tools that can be used (OpticsWorks, LigthTools, TracePRO)
[1, 2, 3] do exist and they oﬀer a vast repository of sub-modules to develop
and design custom lenses, reﬂectors, light guides, etc. These universal tools
however do not completely exploit the luminaire design possibilities that
were introduced by the transition from conventional light source technolo-
gies to LED. One of the possibilities which is also the main goal of a bigger
study that incorporates the research presented here is to have an expert or
intelligent system which would be capable of suggesting a secondary lens
combination that would result in a user deﬁned end photometry. In other
words, the system would take some stock secondary LED lenses from dif-
ferent manufactures, place them on a deﬁned LED array and search for the
optimal combination of the lenses so that the resulting photometry would
be as close as possible to the user deﬁned one.

The method could enable the luminaire designer to custom design the
light engine to a speciﬁc area of illumination, while keeping the mechanical
and electrical parts of a luminaire untouched. This would in turn provide
a customer with a tailored solution that would guarantee a maximum eﬃ-
ciency, lower prices, fewer light pollution and the possibility to individualize
the illumination eﬀect while maintaining a consistent visual appearance of
the luminaries. There are several optimization tasks related to development
of the above idea. Here we focus on the approximation of spatial light dis-
tribution with a moderate number of suitable basis functions [4, 5]. The
problem that is deﬁned formally in the next section is motivated by the fol-
lowing. The data describing the properties of the lenses and/or of the desired
light distribution is nowadays usually given in some standard format ﬁles
that correspond to the measured (or desired) values at a number of points
in space. This results in relatively large data ﬁles of unstructured data.
Clearly, if the data can be well enough approximated c.f. as a linear combi-

2

nation of certain basis functions, this may enable faster computations using
less computer storage. Indeed, for some special cases including LED lenses
with symmetric light distribution, it is possible to ﬁnd reasonably good ap-
proximations fast (8 minutes runtime on a Intel Core I7-4790K CPU @ 4
Ghz, the code is written in C++ and is not fully optimized). Suﬃciently
good approximation here means 2-5% RMS error (to be deﬁned later) for
target light distribution, taking into account expected noise in measurement
using current technology. Recent experiments showed that suﬃciently good
approximations can be obtained by some basic optimization algorithms, in-
cluding local search algorithms and genetic algorithms [6, 7, 8]. However,
when using predeﬁned lenses to design a luminaire that closely approximates
a desired light distribution, it may be essential that the approximation error
is much lower.

The same task can also be seen as solving a problem of data compression,
replacing a long unstructured data ﬁle with a much shorter one, in this case
a sequence of parameters. It makes sense to aim at 0% approximation when
considering the data compression task.

As the functions to be approximated are smooth, it is natural to try
to improve the basic discrete optimization methods with continuous opti-
mization techniques, c.f. Newton’s method [9]. Here we consider Newton’s
method both as a standalone (restarted) algorithm and as a post-processor
of other algorithms. The datasets used for testing and analysis are a selec-
tion of real lenses as used in previous studies and an artiﬁcial dataset that is
large enough for statistical analysis. The artiﬁcial dataset is also generated
in a way which assures that 0% approximation is possible. Note that we
have no guarantee that the realistic lenses may be approximated within our
model with arbitrary low RMS error. The rest of the paper is organized as
follows. In section two we discus the problem and present the mathemat-
ical model, section three is all about the algorithms and Newton method
implementation, section four presents the datasets used in the experiment,
section ﬁve provides the experiment set-up, section six unveils the results
section seven wraps everything up in the conclusion. Appendix provides all
equations needed for the Newton method.

2. The model

The method mentioned above seems natural and straightforward, but at
a closer look, we observe some fundamental problems related to realization of
the main idea. Namely, both the spatial light distribution of LED lenses and
the desired illumination are given in the standard data formats, that are just

3

a long unstructured lists of data. In particular when the aim is to construct
a lighting system that provides the desired illumination of the environment,
it is necessary or at least very convenient to have the data in some more
structured format. It is known that the spatial light distribution of some
LED lenses can be approximated by a sum of a small number of certain
basis functions [4]. Provided the approximation is suﬃciently good, it may
be possible to provide designs combining several lenses with controlled error
rate.

This naturally opens several research avenues. For example, it is impor-
tant to have error free or at least very good approximations of the basic
lenses, and to have methods that are stable in the sense that they are not
too sensitive to the noise in the presentation of basic elements.

Here we focus on the ﬁrst above mentioned task, approximation of the
unstructured spatial light distribution data. We search for an approximation
of the Luminous intensity I (Φ; a, b, c) at the polar angle of Φ in the form

K(cid:88)

I (Φ; a, b, c) = Imax

ak cosck (Φ − bk)

(1)

k=1

where K is the number of functions to sum and ak, bk, ck are the function
coeﬃcients that we search for. For brevity, coeﬃcients are written as vectors
a = (a1, a2, . . . , aK), b = (b1, b2, . . . , bK), and c = (c1, c2, . . . , cK). The
interval range of the coeﬃcients is: a = [0, 1], b = [0, 90] and c = [0, 100],
more accurately the discrete values are : a(cid:63) ∈ [0, 0.001, 0.002, . . . , 1], b(cid:63) ∈
[−90,−89.9,−89.8, . . . , 90], and c(cid:63) ∈ [0, 1, 2, . . . , 100]. Here we need to note
two restrictions on the model. First restriction emerges from the LEDs
physical design. The LED can not emit any light to the back side which is
the upper hemisphere in our case. That is why all intermediate values that
are calculated at the combined angle (Φ− bk) greater than 90° equal 0. The
second restriction deals with the slightly unusual description of the light
distribution in standard ﬁles such as Elumdat (ﬁle extension .ldt) [10] and
Iesna (.ies) [11]. These ﬁles present measured candela values per angle Φ on
so called C planes which can be observed on Figure 1. One C plane is actually
only one half of the corresponding cross-section and does not describe the
other half. But from a physical point of view we need to consider the impact
from the other half of the cross-section. Because all the lenses used here are
symmetric, we can simplify the calculation of the intermediate values and
incorporate the impact of the other half by mirroring (multiplying by -1) all
values that are calculated with the combined angle (Φ−bk) less than 0. Note
however that this only works with symmetrical distributions, and should be
reconsidered carefully when the method is to be applied to asymmetrical

4

distributions.

Figure 1: C-planes according to standard. C-planes angles : 0 - 360 — Φ angles : 0 to 90

The goodness of ﬁt is deﬁned as the root mean square error (RM S),

formally deﬁned by the expression:

(cid:118)(cid:117)(cid:117)(cid:116) 1

N

N(cid:88)

i=1

[Im(Φi) − I(Φi, a, b, c)]2

(2)

RM S (a, b, c) =

where RM S represents the error of the approximation.Later in tables we
provide the relative RMS error (RMSp) deﬁned with equation 3. N is
the number of measured points in the input data, Im(Φi) the measured
Luminous intensity value at the polar angle Φ from the input data, and
I(Φi, a, b, c) the calculated Luminous intensity value at the given polar an-
gle Φ.

RM Sp (a, b, c) =

[%]

(3)

(cid:80)N

100 ∗ N ∗ RM S (a, b, c)

i=1 [Im(Φi)]

Remark. The model was successfully applied to LED’s with attached
secondary optics and symmetric light distribution [4] showing that suﬃ-
ciently good approximations (RMS error below 5%) can be obtained using

5

φ = 0°φ = 180°C = 180°C = 0°C = 90°C = 270°C = 135°a sum of only three functions (K = 3). Approximation of spatial light dis-
tribution of a LED with uniform distribution and without a secondary lens
using this type of functions was ﬁrst proposed in [5]. The model was slightly
modiﬁed in [4] where a new normalizing parameter was introduced, and con-
sequently, all other parameters will have values in ﬁxed intervals known in
advance. It should be noted that the modiﬁed model is equivalent to the
original, only the number of parameters and their meaning diﬀer. It may
be interesting to note that due to symmetries of the examples, K = 3 is
suﬃcient for both applications [4, 5]. In general case, we expect that K > 3
functions will be needed for suﬃciently good approximations, and in view
of optimization of the design of a luminaire it is interesting to have an idea
how large the parameter K can grow to assure that the light distribution
ﬁts the desired (and/or standard) suﬃciently well. We do not address this
question here.

When applying the model to the data compression problem, the target
RMS error is 0%. Therefore, we aim to improve the approximation results
that were obtained previously [6, 7] and restrict attention to symmetric light
distributions. Also, we ﬁx K = 3 functions in the model. Besides the dataset
of 14 realistic lenses that was used in some previous studies, here we also
generate an artiﬁcial dataset in which a sample is simply a sum of three
basis functions with randomly chosen parameters. This assures that zero
error approximation is possible for the instances of the artiﬁcial dataset.

We are interested ﬁrst in minimizing the approximation error, and sec-
ond, in computational time of the methods. In the next section we brieﬂy
outline the algorithms we use in the experiments.

3. The algorithms

In previous work [6, 7], the model described above was applied in con-
junction with several custom build algorithms that are based on local search
heuristics and some meta-heuristics. The algorithms implemented include
a steepest descend algorithm, two iterative improvement algorithms with
diﬀerent neighbourhoods and two genetic algorithms, a standard one and a
hybrid one in which the best individuals of every generation are optimized
with the iterative improvement algorithm. For more detailed description of
the algorithms we refer to [6, 7]. The results of the experiments showed that
all of the algorithms applied are capable of providing satisfactory results
on all tested instances, and diﬀered mainly in computational time needed.
The average RMS values obtained on real lenses were around RM S = 2%.

6

Hence, the results mentioned proved that the model is accurate and that
suﬃciently good approximations can be found with a variety of algorithms
for suﬃciently good description of lenses.

However, recall that the model can also be used for data compression
task. Zero or very low RMS error is also essential in the foreseen applica-
tion, in which the pre manufactured lenses are to be combined into a more
complex luminaire with prescribed light distribution.

In the model we use a sum of functions that are smooth and hence the
ﬁrst and second derivatives can be calculated allowing application of contin-
uous optimization methods in addition to the general discrete optimization
meta-heuristics that were used before. We have chosen to use the New-
ton (also known as the Newton–Raphson) iterative method [9] to ﬁnd the
solution that we seek.
It is well known that convergence of the Newton
method largely depends on the initial solution. Therefore we have applied
the method in two ways. First, we use the Newton method as an optimizer
which will pinpoint the local minimum of the solutions found by heuristic
algorithms. In a sense this implementation of the Newton method will be an
extension of the discrete optimization algorithm, used to ﬁnalize the search
to end in a local minimum. (Note that the local minima may be missed by
the discrete optimization algorithms due to predeﬁned length of the discrete
moves.) Second, we use the Newton method as a standalone algorithm that
will on initialization generate a number of random (initial) solutions that
are uniformly scattered over the whole search space and then it will use the
Newton method on a number of the best initial solutions to ﬁnd the local
minimums. Of course, for both implementations to be comparable the it-
eration count has to be controlled so that the overall maximum amount of
computation time will be roughly the same.

Preprocessor multi-start IF. The multi-start iterative improve-
ment with ﬁxed neighbourhood (IF) algorithm [7, 8] ﬁrst initializes
several initial solutions. The initial solutions are randomly chosen from the
whole search space. Each of the initial solutions is then optimized using
the following steps. In the beginning the search step values (a step for a
numerical diﬀerentiation) da = 0.01, db = 1, and dc = Imax
10 are initialized,
giving the 512 neighbours of the initial solution: (a1 ± da, b1 ± db, c1 ± dc,
a2±da, b2±db, c2±dc, a3±da, b3±db, c3±dc). Then the algorithm randomly
chooses a neighbour, and immediately moves to the neighbour if its RMS
value is better than the current RMS value. If no better neighbour is found
after 1000 trials, it is assumed that no better neighbour exists. In this case
the algorithm morphs the neighbourhood by changing the step according to
the formula di+1 = di + d0. More precisely, dai+1 = dai + da0 where da0 is

7

the initial step value. Analogously for db and dc.

This is repeated until i = 10.

If there still is no better solution, the
initial step value is multiplied by 0.9 and the search resumes from the current
solution with a ﬁner initial step. The algorithm stops when the number of
generated solutions reaches Tmax.

Newton method. Newtons method [9, 12, 13] is a well-known numer-
ical optimization method that can provide very good results under certain
assumptions on the evaluation function and on the initial solution. Newtons
method indirectly minimizes the evaluation function by looking for a solu-
tion of a system of nonlinear equations (ﬁrst derivatives of the evaluation
function). Newtons method solves the system of nonlinear equations iter-
atively by approximating it with a system of linear equations in each step
which produce the delta vector. The delta vector is a part of the iterative
sheme xi+1
k. Newton method converges when the delta vector
vanishes, d = 0. At this point the evaluation coeﬃcients found are the local
minimum. Details are given in the Appendix. An obvious assumption is
that the evaluation function has to be a continuous non-linear function for
which ﬁrst and second order derivatives are deﬁned. The initial solution
has to be close enough to a local or global optimum, for Newtons method
to converge. Hence the method is very sensitive to the choice of the initial
solution.

k − di

k = xi

4. The datasets

The experimental study uses two batches of instances, a dataset of 14
instances that correspond to real LED lenses, and a dataset of artiﬁcial
instances generated for purpose of this experiment. The artiﬁcial lenses are
used to obtain more conclusive result on the statistical test, because a sample
of 14 is rather small and may provide statistically insigniﬁcant results. The
real lenses on the other hand show that the algorithms are useful in real life
scenarios.

Real lenses. We have chosen 14 diﬀerent symmetrical lenses which are
meant to be used with a CREE XT-E series LED, from one of the world’s
leading lens manufacturer LEDIL from Finland. We acquired the photomet-
ric data from LEDIL’s on-line catalogue [14] The data was provided in .ies
format, which we then converted to a vector list which is more suitable to
use in our algorithms. LEDIL measured the individual lenses with a 1 polar
precision on four C panels. This means that from every .ies ﬁle we extracted
720 vectors. As the lenses are symmetric we only needed one C panel, and

8

because we are only working on the lower half of the sphere (DLOR) we end
up with 91 vectors (counting the 0 vector) on which we approximate the
model.

Artiﬁcial lenses. In the dataset of 100 artiﬁcial examples, each element in
the dataset was generated as follows.

A value from an interval was generated using uniform random distri-
bution. (Intervals are [0, 1], [0, 90], or [0, 10], depending on the parameter.
More precisely, the random generator chose one of the values from the ﬁnite
sets: a1, a2, a3 ∈ {0, 0.001, 0.002, . . . , 0.999, 1}, b1, b2, b3 ∈ {0, 0.01, 0.02, . . . ,
90} and c1, c2, c3 ∈ {0, 0.1, 0.2, . . . , 10}.) Then the function values or can-
dela values were computed for each polar angle Φ ∈ {0, 1, 2, . . . , 89}. The
candela values for polar angles Φ ∈ {90, 91, 92, . . . , 180} were set to 0. The
data was then encoded into a .ies ﬁle structure, giving a dataﬁle of the same
format as the real lenses have. Note that the data generated assure that in
each case zero RMS error approximation is possible within our model. Sec-
ond, the dataset of 100 samples is suﬃciently large for meaningful statistical
analysis of experimental results.

5. The experiment setup

Before we go ahead and explain the experiment set-up, let us ﬁrst re-
member the evaluation function that is the basis of the Newton method
[9]. We already showed that the goodness of ﬁt is measured with the RM S
value which is calculated from (2). From that we can deﬁne the evaluation
function as:

N(cid:88)

i=1

1
N

E (a, b, c) =

[Imax(a1 cosc1 (Φ − b1) + a2 cosc2 (Φ − b2) + a3 cosc3 (Φ − b3)) − Im(Φi)]2

(4)

where E represents the error to be minimized, N the number of measured
points in the input data, Imax the maximum candela value, Im(Φi) the
measured Luminous intensity value at the polar angle Φ from the input
data, and I(Φi) the calculated Luminous intensity value at the given polar
angle Φ.

The experiment was set-up to provide data from diﬀerent algorithms.
This in turn enables an objective comparison and a statistical test to deter-
mine the best algorithm. Recall that we implemented the Newton method
in two distinct ways. The ﬁrst implementation uses the multi-start version

9

of iterative improvement (IF) to ﬁnd a good approximation which is then
optimized via the Newton method. The second implementation uses the
random generator to generate initial solutions of which 100 best are opti-
mized with the Newton method. Table belowshows diﬀerent algorithms that
were prepared for the experiment.

Table 1: Experiment algorithms.

Conﬁg. Algorithm Multi-start

IF steps

Short runs 1 million
1000000

S-Newton

10
20
50
100

IF10
IF20
IF50
IF100
Long runs 4 million
4000000

L-Newton

IF40
IF80
IF200
IF400

40
80
200
400

NA

100000
50000
20000
10000

NA

100000
50000
20000
10000

1
3
4
5
6

2
7
8
9
10

Time. We ran the algorithms in two diﬀerent lengths. The short run eval-
uates approximately one million possible solutions per instance (lens) in
just under 45s, and the long run approximately four million possible solu-
tions per instance in about 3 minutes on a Core I7 - 4790K CPU. Newtons
method took in average 3 to 4 iterations to converge which means that the
time it took to run the Newtons method is negligible in contrast to the time
the whole algorithm run. Expressed in seconds, the Newtons method took
approximately 2 ∗ 10−3s, opposed to minutes CPU for the heuristics.
In
addition to the diﬀerent time/iteration spans we ran the algorithms on two
instance sets.

Datasets. Recall the two datasets of instances explained above, the dataset
of 14 real lenses and a the dataset of 100 randomly generated artiﬁcial
instances.

Algorithms. We apply the Newton method both as a standalone algorithm
(restarted on randomly generated initial solutions) and as a ﬁnal step af-
ter discrete local search algorithm (IF) outlined above. There are several
algorithms that vary in the number of multi-starts (or, equivalently in the

10

length of each local search). Depending on the length (short run, long run)
and the number of restarts we denote the algorithms by IF10, IF20, IF50,
IF100 and by IF40, IF80, IF200, IF400. The versions without local
search are denoted by S-Newton and L-Newton for short and long runs,
respectively. See Table 1.

6. Experimental results

We will begin the section with a comparison of the raw experimental data
followed by the performance (quality of results) ranking and ﬁnish with the
results of the Wilcoxon Signed rank test.

Table 2: Artiﬁcial lenses statistical data in RMSp for short and long runs. Best two results
are emphasized.
Algorithm

Std. dev.

Mean

Min.

Max.

Short runs 1 million

3.9148E-04
S-Newton 1.3797E-04
2.4287E+02
2.3393E+01
1.3287E+02
1.1211E+01
6.3340E+00
6.8720E+01
7.2277E-01 2.7120E+00 3.4934E-05 1.9051E+01

3.5054E-05
5.8079E-05
4.9583E-05
4.1291E-05

8.5102E-05
3.0447E+01
1.6382E+01
9.2152E+00

IF 10
IF 20
IF 50
IF 100

Long runs 4 million

3.9148E-04
8.5102E-05
L-Newton 1.3797E-04
2.5447E+01
2.4287E+02
1.2085E+01
1.7490E+00
4.2982E+00 3.5054E-05 1.9504E+01
4.0423E-01 1.9606E+00 3.5054E-05 1.4237E+01
2.4068E+00 3.4934E-05 1.8938E+01
6.1717E-01

IF 40
IF 80
IF 200
FI 400

3.5054E-05
4.8905E-05

Experimetal results are given in Figures 2, 3, 4, 5 and are summarized

in Table 2 and Table 3.

Comparison of the algorithms based on raw experimental results.
Obviously, the pure multi-start Newton method is by far the best on artiﬁcial
instances.

On the real lenses, the situation is a bit diﬀerent. The Iterative improve-
ment on several occasions outperforms the Newton method with random
initial solutions. On both datasets, the long run yields only slightly better
results as the short run does (and the short run is four times faster in exe-
cuting). While we have no idea how far from optimal solutions the achieved
values are for real lenses, we know that, by construction, a solution with

11

Figure 2: Best found solution on a short run. Artiﬁcial lenses per algorithm.

0% RMS error exists for each of the artiﬁcial lenses. Because of that it is
worth to note that on the artiﬁcial set, the random algorithms found nearly
optimal solutions in all cases. The RMS errors are in the range of E-04,
which still is not pure 0% RMS error, but the very small diﬀerence could be
due to rounding of the values in the .ies ﬁles.

On the other hand we did not ﬁnd very low RMS values on the real
set. The values that were found corresponded with the values of previous
test that were performed without any numerical assistance. We did however
perform an experiment on the real set with longer running time, in which
we generated 16 million and 64 million initial solutions that showed similar
behaviour. The mean error and the minimum error over 14 lenses decreased
under 3.0% and 1.50% with 16 million generated solutions, and under 2.0%
and 1.0% RMS error after 64 million.

While the success of the Newton method on artiﬁcial lenses is not sur-

12

050100150200250300147101316192225283134374043464952555861646770737679828588919497100S - NewtonIF 10IF 20IF 50IF 100%LENSFigure 3: Best found solution on a long run. Artiﬁcial lenses per algorithm.

prising, it is not clear why the method is struggling on the realistic dataset.
The winners in this comparison are on both sets the same, but on the real
set the diﬀerences between the random Newton and IF assisted Newton al-
gorithms are a lot smaller than on the artiﬁcial set, we even see that on
some instances the IF algorithms are better. This could be due to the fact
that IF algorithm was previously developed for real lenses that are from a
limited range in search space and thus has a slight advantage on the set. The
advantage of the pure Newton method on the artiﬁcial dataset can nicely be
observed from the data scatter in Figure 6. We see that the IF algorithms
provide very high degree of data scatter where the random ones provide
very narrow result window. This may be due to the nature of the search
because the IF algorithms focus in one deﬁned direction which may not be
the best one. Because of that the Newton optimization can not escape the
potential pitfall of the direction. In contrast, the randomly generated so-

13

050100150200250300147101316192225283134374043464952555861646770737679828588919497100L - NewtonIF 40IF 80IF 200IF 400%LENS%LENSTable 3: Real lenses statistical data in RMSp for short and long runs.

Algorithm

Mean

Std. dev.

Min.

Max.

Short runs 1 million

S-Newton

IF 10
IF 20
IF 50
IF 100

3.7429E+00
1.1726E+01
2.9982E+00

1.5780E+01
5.3910E+00
3.9179E+01
1.8657E+01
5.4893E+00
1.0765E+01
5.2665E+00 3.1108E+00 1.1120E+00 1.0768E+01
4.8658E+00 2.6844E+00 1.1081E+00 1.0781E+01

1.7935E+00
2.0361E+00
1.4134E+00

Long runs 4 million

L-Newton 4.6116E+00 2.6540E+00 1.1254E+00 1.0327E+01
3.9179E+01
1.8218E+01
1.0764E+01
5.5141E+00
5.2529E+00
1.0766E+01
4.9059E+00 2.6582E+00 1.3586E+00 1.0786E+01

6.3836E+00
1.7265E+00
1.9582E+00

1.1024E+01
2.9581E+00
2.9357E+00

IF 40
IF 80
IF 200
FI 400

Figure 4: Best found solution on a short run. Real lenses per algorithm.

14

0510152025303540451234567891011121314S-NewtonIF 10IF 20IF 50IF 100%LENSFigure 5: Best found solution on a long run. Real lenses per algorithm.

15

0510152025303540451234567891011121314L-NewtonIF 40IF 80IF 200IF 400%LENSlutions generally ﬁnd lower quality results that at the same time provide
more maneuvering space for the Newton method to ﬁnd the best direction
on more takes.

Figure 6: Min-Max scatter diagram for artiﬁcial lenses. Logarithmic y axis!

A comparison of the algorithms based on weighted ranking.

We assign a weight from 1 to 10 to each instance solution per algorithm.
If the algorithm found the best solution on an instance it would get the
weight 10 and if it found the second best solution it would get the weight 9,
and so on until 1 for the worst solution. The total score of the algorithm is
the sum of the scores on each instance.

In the same way, we compute the score based on the average values per

algorithm and lens. The results are presented in Table 4.

Note that the ranking here compares both short and long runs. As
expected, Table 4 conﬁrms the superiority of the pure Newton method on
the artiﬁcial dataset. However, the situation is much more complicated on

16

3.5054E‐055.8079E‐054.9583E‐054.1291E‐053.5054E‐054.8905E‐053.5054E‐053.5054E‐053.4934E‐053.4934E‐051.3797E‐042.3393E+011.1211E+016.3340E+001.3797E‐041.2085E+011.7490E+004.0423E‐017.2277E‐016.1717E‐013.9148E‐046.8720E+013.9148E‐041.9504E+011.4237E+011.9051E+011.8938E+010.000.000.000.010.101.0010.00100.00Ran1MIF10IF20IF50Ran4MIF40IF80IF200IF100FI400Table 4: Weight based ranking.

R
A
N
K
1
2
3
4
5
6
7
8
9
10

Artiﬁcial

Real

Best

Mean

Best

Mean

Alg.

L-Newton
S-Newton
IF 200
IF 80
IF 100
IF 400
IF 50
IF 40
IF 20
IF 10

Score
944
940
836
810
678
660
529
514
379
212

Alg.

L-Newton
S-Newton
IF 20
IF 100
IF 10
IF 50
IF 400
IF 40
IF 80
IF 200

Score
1000
900
588
585
559
538
385
358
304
283

Alg.

L-Newton
IF 100
IF 400
IF 50
S-Newton
IF 20
IF 200
IF 80
IF 10
IF 40

Score
102
97
96
94
90
88
86
83
23
21

Alg.

L-Newton
S-Newton
IF 10
IF 50
IF 20
IF 100
IF 40
IF 80
IF 400
IF 200

Score
140
124
114
89
77
72
61
39
34
20

Table 5: Asymptotic signiﬁcances of Wilcoxon Signed rank test for results for short runs
on artiﬁcial lenses.

ALG

IF 10

IF 20

IF 50

IF 100

IF 20

IF 50

IF 100

S-Newton

4.078E-11

1.020E-13

1.000E-13

1.000E-13

7.162E-04

3.120E-13

1.000E-13

3.628E-06

1.650E-13

3.234E-08

the dataset of real lenses. Despite the Newton method (long and short run)
being the two best when considering the average results, they are not both
when looking at the best solutions! The long run Newton is still the overall
best, but the short run Newton is on the ﬁfth place outran by two short IF
algorithms and one long IF algorithm. We can also observe that the score
diﬀerences are much smaller on the real set, which indicates that the pure
Newton method is not as superior as it was on the artiﬁcial set. The lesser
superiority could be explained in part by fact that the IF algorithms were
developed using the real lens set. Hence the IF could have some unexpected
advantages. However the Newton method improves the results in all cases.

Wilcoxon test. The third comparison is based on the statistical paired
singed Wilcoxon [15] rank test. The statistical test compares algorithms
pair by pair to estimate the diﬀerence between them. This is done via the
asymptotic diﬀerence. If the value of the asymptotic diﬀerence is lower than
0.05 then the algorithms in pair signiﬁcantly diﬀer one from another. The
asymptotic diﬀerences of algorithm pairs will be presented in tables 5,6,7,
and 8.

A look over the Wilcoxon test results reveals that there mostly are no
similarities between algorithms when they ran on the artiﬁcial set. We can

17

Table 6: Asymptotic signiﬁcances of Wilcoxon Signed rank test for results for long runs
on artiﬁcial lenses.

ALG

IF 40

IF 80

IF 200

IF 400

IF 80

IF 200

IF 400

L-Newton

1.736E-11

3.917E-11

7.950E-09

1.899E-12

3.269E-11

5.563E-01

3.411E-06

4.818E-06

7.044E-05

1.176E-08

Table 7: Asymptotic signiﬁcances of Wilcoxon Signed rank test for results for short runs
on real lenses.

ALG

IF 10

IF 20

IF 50

IF 100

IF 20

IF 50

IF 100

S-Newton

9.815E-04

9.815E-04

9.815E-04

9.815E-04

4.326E-01

4.133E-02

5.936E-01

5.098E-01

9.750E-01

4.703E-01

Table 8: Asymptotic signiﬁcances of Wilcoxon Signed rank test for results for long runs
on real lenses.

ALG

IF 40

IF 80

IF 200

IF 400

IF 80

IF 200

IF 400

L-Newton

9.815E-04

9.815E-04

9.815E-04

9.815E-04

5.509E-01

5.553E-02

3.546E-02

4.703E-01

1.240E-01

4.703E-01

18

Table 9:
increase ∆ in %.

. Real lens RMSp for RAN 4M with and without Newtons method. Quality

Instance RAN 4M Newton ∆
CP12632
CP12634
CP12633
CA11934
CA11268
CP12817
CA11265
CP12636
CA13013
FP13030
CA11525
CA12392
CA11483

27,996
45,8986
10,7706
10,2492
15,1818
29,6252
9,64366
7,62895
2,70647
10,1866
12,0224
6,87747
24,5813

7,6908
9,05513
2,57185
2,7982
5,38851
10,3279
4,1553
4,03813
1,12548
3,34882
3,00557
2,51916
4,0032

72,53
80,27
76,12
72,70
64,51
65,14
56,91
47,07
58,42
67,13
75,00
63,37
83,71

see that the Asymptotic signiﬁcance values are very low, which means that
there are signiﬁcant diﬀerences between algorithms in pairs. We do however
have one exception that is the pair IF 80 - IF 400, where the asymptotic
diﬀerence is just over the margin, so we could say that these two have some
similarities. The story is completely diﬀerent on the real dataset, where we
can ﬁnd that most IF algorithms are similar to random algorithms. This
also corresponds with the ﬁndings of the ranking and RMS error comparison.
The Wilcoxon test provided similar conclusions as the previous tests did, but
we need to be careful because the data sets diﬀer in size and the real lenses
set can be a bit inconclusive as it is a rather small sample with only 14
instances. That is why the artiﬁcial lenses with 100 instances could give a
more accurate result.

7. Conclusion

Here we presented an upgrade of a previously developed most promising
discrete optimization heuristics with a numerical approach to optimization.
It was shown that application of Newtons method gave improvement of both
performance and quality of solutions. In terms of raw performance, we got
from the initial 8 minutes runtime for one algorithm on one lens to about
45s runtime with the upgraded IF 10 or S-Newton algorithm. The stated
runtime is accurate for symmetric lenses and an input of 91 vectors. When

19

working on asymmetric lenses the input will be around 33.000 vectors, this
is when the problem becomes a big data problem. But because of the al-
gorithms construction the runtime should increase to about 2 hours and 15
minutes. The increase will be by a factor of 180, while the amount of vec-
tors is increased by a factor 360. On the asymmetric lenses the runtime will
be lowered from around 24 hours to 2 hours and 15 minutes. Despite the
drastic time shortening the quality of the solutions was not worse thanks
to Newtons method, which enabled us to ﬁnd local minimums on the ma-
jority of solutions found by the heuristic algorithms. Newtons method in
fact successfully minimized the RMS error on all of the experiment cases
with the average of 60% increased quality (minimized RMS) over previous
experiments done in [4, 7]. This can be well observed in Table 9, where we
can see the RMS error found by the RAN 4M algorithm before the appli-
cation of Newtons method and after. We can conclude that the integration
of numerical approach with previously developed heuristics signiﬁcantly im-
proved the application performance to the level at which it is useful in the
main research. On the other hand, we have learned that due to sensitiveness
of Newtons method to the choice of initial solutions, it may be rewarding
to use a preprocessor that may provide promising initial solutions. In par-
ticular, on the dataset consisting of real lenses, the experiment showed that
the initial solutions provided by a discrete local search algorithm improved
the overall performance of the algorithm. This leads to the conclusion that
a combination of an algorithm that ﬁnds promising initial solutions as a
preprocessor to Newtons method may be a winning combination, at least
on some datasets of instances. Hence, in a practical application, it may be
worth developing good heuristics that may handle speciﬁc properties of the
instances and thus provide promising initial solutions for ﬁnal optimization.

8. Acknowledgement

This work was supported in part by ARRS, the Research agency of
Slovenia, grants P1-0285 and ARRS-1000-15-0510. We sincerely thank both
reviewers for carefully reading the manuscript and for constructive remarks.

References

References

[1] Optis, optisworks, http://bit.ly/1CjKp4t, accessed: July 07.

20

[2] Lighttools, http://optics.synopsys.com/lighttools/,

accessed:

2015, July 07.

[3] Lambardes, tracepro, http://www.lambdares.com/, accessed: 2015,

July 07.

[4] D. Kaljun, J. ˇZerovnik, Function ﬁtting the symmetric radiation pat-
tern of a led with attached secondary optic, Opt. Express 22 (24) (2014)
29587–29593.

[5] I. Moreno, C.-C. Sun, Modeling the radiation pattern of leds, Opt.

Express 16 (3) (2008) 1808–1819.

[6] D. Kaljun, J. ˇZerovnik, On local search based heuristics for optimization

problems, Croat. Oper. Res. Rev. 5 (2) (2014) 317–327.

[7] D. Kaljun, D. R. Poklukar, J. ˇZerovnik, Heuristics for optimization of
led spatial light distribution model, Informatica 39 (2) (2015) 317–327.

[8] D. Kaljun, J. ˇZerovnik, Developing led ilumination optics design, in:
G. Papa (Ed.), Advances in Evolutionary Algorithms Research, Nova
Science Publishers, Inc., 400 Oser Ave Suite 1600, Hauppauge NY,
USA, 11788-3619, 2015, Ch. 6.

[9] A. Quarteroni, R. Sacco, F. Saleri, Nonlinear systems and numerical
optimization, in: J. Marsden, L. Sirovich, S. Antman (Eds.), Numerical
Mathematics, Springer-Verlag GmbH,, Tiergartenstrasse 17, D-69121
Heidelberg, Germany, 2015, Ch. 6.

[10] CEN, Light and lighting - measurement and presentation of photomet-
ric data of lamps and luminaries - part 1: Measurement and ﬁle format
en 13032-1:2004+a1:2012, Tech. rep., CEN (2012).

[11] IESNA, Standard ﬁle format for the electronic transfare of photometric

data and related information lm-63-02, Tech. rep., ANSI (2002).

[12] R. Thukral, Introduction to a newton-type method for solving nonlinear

equations, Appl. Math. Comput. 195 (2008) 663–668.

[13] D. Herceg, Means based modiﬁcations of newtons method for solving

nonlinear equations, Appl. Math. Comput. (216) (2013) 6126–6133.

[14] Ledil oy., technical resources,, http://www.ledil.com/products/?y,

accessed: 2015, July 07.

21

[15] F. Wilcoxon, Individual comparisons by ranking methods, Biometrics

1 (1945) 80–83.

Appendix A.

Evaluation function :

N(cid:88)

E (a, b, c) =

1
N

[Imax(a1 cosc1 (Φi − b1) + a2 cosc2 (Φi − b2) + a3 cosc3 (Φi − b3)) − Im(Φi)]2

i=1

(A.1)

Partial equations:

Gi = Imax(a1 cosc1 (Φi − b1) + a2 cosc2 (Φi − b2) + a3 cosc3 (Φi − b3)) − Im(Φi)

(A.2)

F1i = Imax cosc1 (Φi − b1)

F2i = Imax cosc2 (Φi − b2)

F3i = Imax cosc3 (Φi − b3)

F4i = Imaxa1c1 cos(c1−1)(Φi − b1) sin(Φi − b1)

F5i = Imaxa2c2 cos(c2−1)(Φi − b2) sin(Φi − b2)

F6i = Imaxa3c3 cos(c3−1)(Φi − b3) sin(Φi − b3)

F7i = Imaxa1 cosc1 (Φi − b1) ln(cos(Φi − b1))

F8i = Imaxa2 cosc2 (Φi − b2) ln(cos(Φi − b2))

F9i = Imaxa3 cosc3 (Φi − b3) ln(cos(Φi − b3))

S14i = Imaxc1 cos(c1−1)(Φi − b1) sin(Φi − b1)

S17i = Imax cosc1 (Φi − b1) ln(cos(Φi − b1))

S25i = Imaxc2 cos(c2−1)(Φi − b2) sin(Φi − b2)

S28i = Imax cosc2 (Φi − b2) ln(cos(Φi − b2))

S36i = Imaxc3 cos(c3−1)(Φi − b3) sin(Φi − b3)

22

(A.3)

(A.4)

(A.5)

(A.6)

(A.7)

(A.8)

(A.9)

(A.10)

(A.11)

(A.12)

(A.13)

(A.14)

(A.15)

(A.16)

S39i = Imax cosc3 (Φi − b3) ln(cos(Φi − b3))

S41i = Imaxc1 cos(c1−1)(Φi − b1) sin(Φi − b1))

S44i = Imaxa1c1 cos(c1−2)(Φi − b1)(c1 sin2(Φi − b1) − 1)

S47i = −Imaxa1 cos(c1−1)(Φi − b1) sin(Φi − b1)(c1 ln(cos(Φi − b1)) + 1)

S52i = Imaxc2 cos(c2−1)(Φi − b2) sin(Φi − b2))

S55i = Imaxa2c2 cos(c2−2)(Φi − b2)(c2 sin2(Φi − b2) − 1)

S58i = −Imaxa2 cos(c2−1)(Φi − b2) sin(Φi − b2)(c2 ln(cos(Φi − b2)) + 1)

S63i = Imaxc3 cos(c3−1)(Φi − b3) sin(Φi − b3))

S66i = Imaxa3c3 cos(c3−2)(Φi − b3)(c3 sin2(Φi − b3) − 1)

S69i = −Imaxa3 cos(c3−1)(Φi − b3) sin(Φi − b3)(c3 ln(cos(Φi − b3)) + 1)

S71i = Imax cosc1 (Φi − b1) ln(cos(Φi − b1))

(A.17)

(A.18)

(A.19)

(A.20)

(A.21)

(A.22)

(A.23)

(A.24)

(A.25)

(A.26)

(A.27)

S74i = −Imax(a1 cosc1−1(Φi−b1) sin(Φi−b1)−a1c1 ln(cos(Φi−b1)) cosc1−1(Φi−b1) sin(Φi−b1))

S77i = Imaxa1 cosc1 (Φi − b1) ln(cos(Φi − b1)) ln(cos(Φi − b1))

S82i = Imax cosc2 (Φi − b2) ln(cos(Φi − b2))

(A.28)

(A.29)

(A.30)

S85i = −Imax(a2 cosc2−1(Φi−b2) sin(Φi−b2)−a2c2 ln(cos(Φi−b2)) cosc2−1(Φi−b2) sin(Φi−b2))

S88i = Imaxa2 cosc2 (Φi − b2) ln(cos(Φi − b2)) ln(cos(Φi − b2))

(A.31)

(A.32)

23

S93i = Imax cosc3 (Φi − b3) ln(cos(Φi − b3))

(A.33)

S96i = −Imax(a3 cosc3−1(Φi−b3) sin(Φi−b3)−a3c3 ln(cos(Φi−b3)) cosc3−1(Φi−b3) sin(Φi−b3))

(A.34)

(A.35)

(A.36)

S99i = Imaxa3 cosc3 (Φi − b3) ln(cos(Φi − b3)) ln(cos(Φi − b3))

First order derivatives:

∂E(a, b, c)

∂a1

∂E(a, b, c)

∂a3

∂E(a, b, c)

∂b2

∂E(a, b, c)

∂c1

=

=

=

=

i=1

N(cid:88)
N(cid:88)
N(cid:88)
N(cid:88)

i=1

i=1

GiF1i

GiF3i

GiF5i

GiF7i

i=1

∂E(a, b, c)

∂c3

Second order derivatives:

=

=

=

=

∂E(a, b, c)

∂a2

∂E(a, b, c)

∂b1

∂E(a, b, c)

∂b3

∂E(a, b, c)

N(cid:88)

i=1

=

∂c2

GiF9i

i=1

N(cid:88)
N(cid:88)
N(cid:88)
N(cid:88)

i=1

i=1

i=1

GiF2i

GiF4i

GiF6i

GiF8i

F1i F3i

F1i F4i + GiS14i

=

F1i F1i

∂2E(a, b, c)

∂a2∂a1

∂2E(a, b, c)

∂b1∂a1

=

=

N(cid:88)

i=1

F1i F5i

∂2E(a, b, c)

∂b3∂a1

=

∂2E(a, b, c)

∂a2
1

∂2E(a, b, c)

∂a3∂a1

=

N(cid:88)

i=1

∂2E(a, b, c)

∂b2∂a1

∂2E(a, b, c)

∂c1∂a1

=

=

N(cid:88)

∂2E(a, b, c)

∂c2∂a1

=

F1i F8i

F1i F7i + GiS17i

i=1

∂2E(a, b, c)

∂c3∂a1

N(cid:88)

i=1

=

F1i F9i

N(cid:88)

i=1

F1i F2i

N(cid:88)

i=1

F1i F6i

N(cid:88)

i=1

N(cid:88)
N(cid:88)

i=1

i=1

=

=

F2i F2i

F2i F4i

N(cid:88)

i=1

N(cid:88)

i=1

N(cid:88)
N(cid:88)

i=1

i=1

∂2E(a, b, c)

∂a1∂a2

∂2E(a, b, c)

∂a3∂a2

=

=

F2i F1i

F2i F3i

∂2E(a, b, c)

∂a2
2

∂2E(a, b, c)

∂b1∂a2

24

F2i F5i + GiS25i

F2i F7i

=

F2i F8i + GiS28i

N(cid:88)

i=1

=

F2i F6i

N(cid:88)
N(cid:88)

i=1

i=1

F3i F2i

F3i F4i

N(cid:88)

i=1

F3i F8i

N(cid:88)

i=1

=

F4i F2i

∂2E(a, b, c)

∂c3∂a2

F3i F1i

F3i F3i

N(cid:88)
N(cid:88)

i=1

i=1

N(cid:88)

i=1

∂2E(a, b, c)

∂b3∂a2

N(cid:88)

i=1

∂2E(a, b, c)

∂c2∂a2

N(cid:88)

=

F2i F9i

i=1

∂2E(a, b, c)

∂a2∂a3

=

∂2E(a, b, c)

∂b1∂a3

∂2E(a, b, c)

∂b3∂a3

=

=

N(cid:88)

i=1

∂2E(a, b, c)

∂c2∂a3

=

F3i F9i + GiS39i

N(cid:88)

i=1

∂2E(a, b, c)

∂a2∂b1

N(cid:88)

i=1

∂2E(a, b, c)

∂b12

=

∂2E(a, b, c)

∂b2∂a2

∂2E(a, b, c)

∂c1∂a2

=

=

N(cid:88)
N(cid:88)

i=1

i=1

∂2E(a, b, c)

∂a1∂a3

=

∂2E(a, b, c)

∂a2
3

∂2E(a, b, c)

∂b2∂a3

=

=

N(cid:88)

i=1

∂2E(a, b, c)

∂a1∂b1

∂2E(a, b, c)

∂a3∂b1

=

=

∂2E(a, b, c)

∂b2∂b1

∂2E(a, b, c)

∂c1∂b1

=

∂2E(a, b, c)

∂a1∂b2

=

∂2E(a, b, c)

∂a3∂b2

∂2E(a, b, c)

∂b2
2

∂2E(a, b, c)

∂c1∂b2

=

=

N(cid:88)
N(cid:88)

i=1

i=1

=

N(cid:88)

i=1

N(cid:88)

i=1

N(cid:88)
N(cid:88)

i=1

i=1

F3i F5i

F3i F6i + GiS35i

∂2E(a, b, c)

∂c1∂a3

=

F3i F7i

∂2E(a, b, c)

∂c3∂a3

=

F4i F1i + GS41i

F4i F3i

N(cid:88)

i=1

F4i F5i

∂2E(a, b, c)

∂b3∂b1

=

F4i F4i + GiS44i

N(cid:88)

i=1

F4i F6i

N(cid:88)

i=1

∂2E(a, b, c)

∂c2∂b1

=

F4i F8i

F4i F7i + GiS47i

∂2E(a, b, c)

∂c3∂b1

=

N(cid:88)

i=1

F4i F9i

∂2E(a, b, c)

∂a2∂b2

=

F5i F1i

N(cid:88)

i=1

=

F5i F3i

∂2E(a, b, c)

∂b1∂b2

=

F5i F5i + GiS55i

∂2E(a, b, c)

∂b3∂b2

F5i F7i

∂2E(a, b, c)

∂c2∂b2

=

25

N(cid:88)

i=1

N(cid:88)

i=1

F5i F2i + GiS52i

N(cid:88)

i=1

F5i F4i

N(cid:88)

=

F5i F6i

i=1

F5i F8i + GiS58i

N(cid:88)

i=1

=

F5i F9i

∂2E(a, b, c)

∂c3∂b2

N(cid:88)

i=1

F6i F1i

∂2E(a, b, c)

∂a2∂b3

=

F6i F3i + GS63i

∂2E(a, b, c)

∂b1∂b3

=

F6i F4i

F6i F2i

N(cid:88)

i=1

F6i F8i

F6i F6i + GiS66i

N(cid:88)

i=1

=

F7i F2i

F7i F4i + GiS74i

N(cid:88)

i=1

N(cid:88)

i=1

N(cid:88)

i=1

N(cid:88)

i=1

∂2E(a, b, c)

∂b2
3

=

∂2E(a, b, c)

∂c2∂b3

=

F6i F9i + GiS69i

N(cid:88)

i=1

∂2E(a, b, c)

∂a2∂c1

N(cid:88)

i=1

∂2E(a, b, c)

∂b1∂c1

=

F7F1i + GS71i

∂2E(a, b, c)

∂a1∂b3

∂2E(a, b, c)

∂a2
3

∂2E(a, b, c)

∂b2∂b3

=

=

=

N(cid:88)
N(cid:88)

i=1

i=1

F6i F5i

N(cid:88)

i=1

∂2E(a, b, c)

∂c1∂b3

=

F6i F7i

∂2E(a, b, c)

∂c3∂b3

=

N(cid:88)
N(cid:88)

i=1

i=1

=

N(cid:88)

i=1

N(cid:88)

i=1

=

N(cid:88)
N(cid:88)

i=1

∂2E(a, b, c)

∂a1∂c1

=

∂2E(a, b, c)

∂a3∂c1

=

∂2E(a, b, c)

∂b2∂c1

∂2E(a, b, c)

∂c2
1

=

∂2E(a, b, c)

∂a1∂c2

=

∂2E(a, b, c)

∂a3∂c2

∂2E(a, b, c)

∂b2∂c2

∂2E(a, b, c)

∂c1∂c2

=

=

F7i F3i

N(cid:88)

i=1

F8i F1i

N(cid:88)

i=1

F7i F5i

∂2E(a, b, c)

∂b3∂c1

=

F7i F6i

N(cid:88)

i=1

∂2E(a, b, c)

∂c2∂c1

=

F7i F8i

F7i F7i + GiS77i

∂2E(a, b, c)

∂c3∂c1

=

N(cid:88)

i=1

F7i F9i

∂2E(a, b, c)

∂a2∂c2

=

N(cid:88)

i=1

F8i F3i

∂2E(a, b, c)

∂b1∂c2

=

F8i F5i + GiS85i

F8i F2i + GiS82i

N(cid:88)

i=1

F8i F4i

N(cid:88)

∂2E(a, b, c)

∂b3∂c2

N(cid:88)

=

=

F8i F6i

i=1

F8i F8i + GiS88i

F8i F7i

i=1

∂2E(a, b, c)

∂c3∂c2

∂2E(a, b, c)

∂c2
2

N(cid:88)

=

i=1

F8i F9i

∂2E(a, b, c)

∂a1∂c3

=

N(cid:88)

i=1

i=1

∂2E(a, b, c)

∂a2∂c3

=

N(cid:88)

i=1

F9i F2i

F9i F1i

26

Jacobian matrix:

J(a1, ..., c3) =



Delta vector:

d =(cid:2)da1

Right side:

∂2E(a, b, c)

∂a1∂a1

...

∂2E(a, b, c)

∂a1∂c3

···
. . .
···

∂2E(a, b, c)

∂c3∂a1

...

∂2E(a, b, c)

∂c3∂c3



(cid:3)

F9i F3i + GiS93i

N(cid:88)
N(cid:88)

i=1

i=1

=

=

∂2E(a, b, c)

∂a2
3

∂2E(a, b, c)

∂b2∂c3

F9i F5i

N(cid:88)

i=1

∂2E(a, b, c)

∂c1∂c3

=

F9i F7i

∂2E(a, b, c)

=

∂c2
3

N(cid:88)

i=1

=

F9i F4i

F9i F6i + GiS96i

N(cid:88)

i=1

F9i F8i

∂2E(a, b, c)

∂b1∂c3

N(cid:88)

i=1

∂2E(a, b, c)

∂b3∂c3

=

∂2E(a, b, c)

∂c2∂c3

=

F9i F9i + GiS99i

N(cid:88)

i=1

da2

da3

db1

db2

db3

dc1

dc2

dc3

(cid:20) ∂E(a, b, c)

∂a1

(cid:21)T

···

∂E(a, b, c)

∂c3

R(a1, ..., c3) =

System of equations to solve for d:

Coeﬁcient vector:

x =(cid:2)a1

Iterative scheme :

J(xi) × di = R(xi)

a2

a3

b1

b2

b3

c1

c2

c3

(cid:3)

xi+1 = xi − di

(A.37)

(A.38)

27

