6
1
0
2

 
r
a

M
1

 

 
 
]
T
S
h
t
a
m

.

[
 
 

1
v
5
8
2
0
0

.

3
0
6
1
:
v
i
X
r
a

Kernel-based Tests for Joint Independence

Niklas Pﬁster1, Peter Bühlmann1, Bernhard Schölkopf2, and Jonas Peters2

1Seminar für Statistik, ETH Zürich, Switzerland
2MPI for Intelligent Systems, Tübingen, Germany

March 2, 2016

Abstract

We investigate the problem of testing whether d random variables, which may or may not
be continuous, are jointly (or mutually) independent. Our method builds on ideas of the two
variable Hilbert-Schmidt independence criterion (HSIC) but allows for an arbitrary number of
variables. We embed the d-dimensional joint distribution and the product of the marginals into
a reproducing kernel Hilbert space and deﬁne the d-variable Hilbert-Schmidt independence
criterion (dHSIC) as the squared distance between the embeddings. In the population case,
the value of dHSIC is zero if and only if the d variables are jointly independent, as long as the
kernel is characteristic. Based on an empirical estimate of dHSIC, we deﬁne three diﬀerent
non-parametric hypothesis tests: a permutation test, a bootstrap test and a test based on a
Gamma approximation. We prove that the permutation test achieves the signiﬁcance level and
that the bootstrap test achieves pointwise asymptotic signiﬁcance level as well as pointwise
asymptotic consistency (i.e., it is able to detect any type of ﬁxed dependence in the large
sample limit). The Gamma approximation does not come with these guarantees; however, it
is computationally very fast and for small d, it performs well in practice. Finally, we apply
the test to a problem in causal discovery.

Introduction

1
We consider the problem of a non-parametric test for joint or mutual independence of d random
variables. This is a very diﬀerent and more ambitious task than testing pairwise independence
of a collection of random variables. The latter has received considerable attention using kernel-
based methods (Gretton et al., 2005, 2007), and other related approaches for estimating or testing
pairwise (in-)dependence include distance correlations (Székely and Rizzo, 2009, 2014), rank-based
correlations (Bergsma and Dassios, 2014; Leung and Drton, 2016; Nandy et al., 2016) or also
semiparametric copula-based correlations (Liu et al., 2012; Xue and Zou, 2012; Wegkamp and
Zhao, 2016). One of our motivations to consider the problem of nonparametric testing of joint
independence is from the area of causal inference, and we discuss this in Section 5.2: there, inferring
pairwise independence is not suﬃcient.

For testing joint independence, consider the distribution P(X 1,...,X d) of the random vector1
X = (X 1, . . . , X d). By deﬁnition, (X 1, . . . , X d) are jointly or mutually independent if and only if
P(X 1,...,X d) = PX 1 ⊗···⊗ PX d. For a given positive deﬁnite kernel, we map both distributions into
the reproducing kernel Hilbert space (see below for details) and consider their squared distance.
For characteristic kernels (e.g., the popular Gaussian kernel), the embedding of Borel probability
measures is injective and the squared distance is zero if and only if the variables are jointly
independent. For the ﬁnite sample case, we compute a suitable estimator that can be used as
a test statistic. We then construct three statistical tests: two tests are based on permutation

1Throughout the paper, a superscript of X always denotes an index rather than an exponent.

1

and bootstrap procedures, respectively, and a third test approximates the distribution of the test
statistic under independence with a gamma distribution. Our test extends the Hilbert-Schmidt
Independence Criterion (HSIC) (Gretton et al., 2005) and contains it as a special case. We
therefore call the corresponding test procedure d-variable Hilbert-Schmidt Independence Criterion
(dHSIC). We prove that the permutation based approach has correct level and that the bootstrap
approach has asymptotic power equal to one against any ﬁxed alternative. We are not aware of
any other non-parametric independence test for d ≥ 2 variables with the latter property.
We compare our test against the following alternative procedure that constructs a joint inde-
pendence test from a bivariate test: joint independence holds if and only if for all k ∈ {2, . . . , d}
we have that X k is independent of (X 1, . . . , X k−1). In order to construct an independence test,
we might therefore perform d − 1 statistical tests and combine the results using a Bonferroni cor-
rection. Due to performing d−1 tests, this test is of order d times more computationally expensive
than the direct dHSIC approach, see Section 5.3.3. It is further known that Bonferroni procedures
are often conservative in the sense that they accept the null hypothesis too often. At last, such a
test is asymmetric in the d random variables and depends on the order of the random variables.

We regard dHSIC as an all-purpose joint independence test. In this paper, we additionally
apply the testing procedure to causal inference.
In causality, one often works with structural
equation models. These models assume the existence of jointly independent noise variables. Our
test can therefore be used as a goodness-of-ﬁt test and can therefore be used for model selection,
see Section 5.2.

1.1 Contribution
This work extends the two variable HSIC (Gretton et al., 2005, 2007; Smola et al., 2007) to testing
joint independence for an arbitrary number of variables. This idea has been brieﬂy mentioned by
Sejdinovic et al. (2013) for the case of three variables. The methodological development as well
as the mathematical rigorous treatment of the permutation test and the bootstrap test are novel:
both the results about their level in Proposition 3.4 and Theorem 3.6, and the consistency of the
bootstrap in Theorem 3.7. The latter seems to be the ﬁrst result about a nonparametric test for
joint independence which establishes that the asymptotic power is equal to one against any ﬁxed
alternative. In our study, we derive new results for V-statistics which might be of independent in-
terest, see Lemma A.3 (asymptotic diﬀerence between U- and V-statistics), Theorem A.5 (asymp-
totic variance of a V-statistic), Theorem A.6 (asymptotic bias of a V-statistic), Theorem A.9
(asymptotic distribution of a degenerate V-statistic) and Theorem A.13 (asymptotic distribution
of a degenerate resampled V-statistic). For the extension of the Gamma approximation based
test, we compute general formulas both for the mean and for the variance in Proposition 3.8 and
Proposition 3.9.
In order to make our tests accessible we have created an R-package (R Core
Team, 2014), which is available on CRAN.

1.2 Organization of this paper
In Section 2, we deﬁne the embedding of distributions (Section 2.1), the quantity of interest dHSIC
(Section 2.2) and the corresponding estimators (Section 2.3). Section 3 derives the asymptotic
distribution of the test statistic under the null hypothesis of joint independence. Section 4 provides
details on the implementation of the proposed methods. We perform experiments on simulated
and real data sets in Section 5.

2

2 Hilbert-Schmidt independence criterion for d variables
2.1 Reproducing kernel Hilbert spaces
We present a brief introduction to reproducing kernel Hilbert spaces and the theory of mean
embeddings. Given a set X we call a function k : X × X → R a positive semi-deﬁnite kernel
if for any set of points (x1, . . . , xn) ∈ X n the corresponding Gram matrix (k(xi, xj))1≤i,j≤n is
symmetric and positive semi-deﬁnite. Moreover, denote by F(X ) the space of functions from X
to R. Reproducing kernel Hilbert spaces on X are well-behaved sub-classes of F(X ) deﬁned as
follows.
Deﬁnition 2.1 (Reproducing kernel Hilbert space)
Let X be a set, let H ⊆ F(X ) be a Hilbert space. Then H is called a reproducing kernel Hilbert
space (RKHS) if there exists a kernel k on X satisfying

• ∀x ∈ X : k(x,·) ∈ H and
• ∀f ∈ H, ∀x ∈ X : (cid:104)f, k(x,·)(cid:105)H = f (x).

Moreover, we call k a reproducing kernel of H.
It can be shown that for any positive semi-deﬁnite kernel k there exists an RKHS with reproducing
kernel k. Given any positive semi-deﬁnite kernel, we can therefore construct and use the corre-
sponding RKHS. A commonly used positive semi-deﬁnite kernel on Rm is the Gaussian kernel,
deﬁned for all x, y ∈ Rm by

(cid:32)
−(cid:107)x − y(cid:107)2

Rm

(cid:33)

k(x, y) = exp

2σ2

.

(2.1)

One of the strengths of RKHS is that they can be used for embedding complicated objects in
order to use the Hilbert space structure to analyze them. Being able to express inner products
as function evaluations via the reproducing property additionally simpliﬁes computation within
an RKHS. In this paper, we use such an embedding technique to analyze probability distribu-
tions. To this end, we use the Bochner integral to deﬁne an embedding of Mf (X ) := {µ |
µ is a ﬁnite Borel measure on X} into an RKHS.
Deﬁnition 2.2 (mean embedding function)
Let X be a separable metric space, let k be a continuous bounded positive semi-deﬁnite kernel and
let H be the RKHS with reproducing kernel k. Then, the function Π : Mf (X ) → H satisfying for
all µ ∈ Mf (X ) that

(cid:90)

Π(µ) =

k(x,·) µ(dx).

X
is called the mean embedding (associated to k).
In order to infer that two distributions are equal given that their embeddings coincide, it is
necessary that the mean embedding is injective. A kernel is called characteristic if the mean
embedding Π is injective (see Fukumizu et al., 2007). The Gaussian kernel (2.1) on Rm, for
example, is characteristic (e.g. Sriperumbudur et al., 2008, Theorem 7).

2.2 Deﬁnition of dHSIC and independence property
Similar to what is done for the HSIC in the two variable setting (Gretton et al., 2007), our goal
is to develop a non-parametric hypothesis test to determine whether the components of a random
vector X = (X 1, . . . , X d) are mutually independent based on n iid observations X1, . . . , Xn of the
vector X. The variables X 1, . . . , X d are mutually independent if and only if

PX 1 ⊗ ··· ⊗ PX d

= P(X 1,...,X d).

The central idea is to embed both PX 1 ⊗···⊗ PX d and P(X 1,...,X d) into an appropriate RKHS and
then check whether the embedded elements are equal. To keep an overview of all our assumptions,
we summarize the setting used throughout the rest of this work.

3

Setting 1 (dHSIC)
For all j ∈ {1, . . . , d}, let X j be a separable metric space and denote by X = X 1 × ··· × X d the
product space. Let (Ω,F, P) be a probability space and for every j ∈ {1, . . . , d}, let X j : Ω → X j
be a random variable with law PX j . Let (Xi)i∈N be a sequence of iid copies of X = (X 1, . . . , X d).
For j ∈ {1, . . . , d}, let kj : X j × X j → R be a continuous, bounded, positive semi-deﬁnite and
characteristic kernel on X j and denote by Hj the corresponding RKHS. Let k = k1 ⊗ ··· ⊗ kd be
the tensor product of the kernels kj and let H = H1⊗···⊗Hd be the tensor product of the RKHSs
Hj. Let Π : Mf (X ) → H be the mean embedding function associated to k.
It is straightforward to show that this setting ensures that H is an RKHS with reproducing
kernel k, that k is continuous and bounded, that H is separable and only contains continuous
functions, and that Π is injective. Using this setting we can extend the Hilbert-Schmidt indepen-
dence criterion (HSIC) from two variables as described by Gretton et al. (2007) to the case of d
variables. The extension is based on the HSIC characterization via the mean embedding described
by Smola et al. (2007).

Deﬁnition 2.3 (dHSIC)
Assume Setting 1. Then, deﬁne the statistical functional

(cid:16)P(X 1,...,X d)(cid:17)

(cid:13)(cid:13)(cid:13)Π

:=

(cid:16)PX 1 ⊗ ··· ⊗ PX d(cid:17) − Π

(cid:16)P(X 1,...,X d)(cid:17)(cid:13)(cid:13)(cid:13)2

H

dHSIC

and call it the d-variable Hilbert-Schmidt independence criterion (dHSIC).

Therefore, dHSIC is the distance between the joint measure and the product measure after embed-
ding them into an RKHS. Since the mean embedding Π is injective we get the following relation
between dHSIC and joint independence.
Proposition 2.4 (independence property of dHSIC)
Assume Setting 1. Then it holds that

dHSIC(P(X 1,...,X d)) = 0 ⇐⇒ PX 1 ⊗ ··· ⊗ PX d

= P(X 1,...,X d).

Proof This statement follows from the deﬁniteness of the norm and the fact that Π is injective.(cid:3)

This proposition implies that we can use dHSIC as a measure of joint dependence between
variables. In order to make dHSIC accessible for calculations, we express it in terms of the indi-
vidual kernels k1, . . . , kd. This expansion will be the basis of the estimator deﬁned in Section 2.3.

Proposition 2.5 (expansion of dHSIC)
Assume Setting 1. Then it holds that

 d(cid:89)

kj(cid:16)

(cid:17) + E

 d(cid:89)

dHSIC = E

X j

1 , X j

2

j=1

j=1

Proof A proof is given in Appendix C.1.

kj(cid:16)

X j

2j−1, X j

2j

(cid:17) − 2E

 d(cid:89)

kj(cid:16)

j=1

X j

1 , X j

j+1

(cid:17)

(cid:3)

2.3 Estimating dHSIC
Our estimator will be constructed using several V-statistics. We therefore start by summarizing a
few well-known deﬁnitions and the most important results from the theory of V-statistics. Readers
familiar with these topics may skip directly to Deﬁnition 2.6.

4

Let n ∈ N, q ∈ {1, . . . , n}, let X be a metric space, (Ω,F, P) a probability space, X : Ω → X a
random variable with law PX and let (Xi)i∈N be a sequence of iid copies of X, i.e., (Xi)i∈N iid∼ PX.
Furthermore, deﬁne the set of all mappings from {1, . . . , q} to {1, . . . , n} by

Mq(n) := {1, . . . , n}q .

Moreover, consider a measurable and symmetric (i.e., invariant under any permutation of its input
arguments) function g : X q → R, which we denote as core function. The V-statistic

(cid:88)

Mq(n)

Vn(g) :=

1
nq

g(Xi1, . . . , Xiq )

(2.2)

estimates the statistical functional

θg := θg

(cid:0)PX(cid:1) := E (g(X1, . . . , Xq)) .

As opposed to U-statistics, deﬁned in (A.1) in Appendix A, V-statistics are usually biased. In
this work, we nevertheless consider a V-statistic because it can be computed much faster than
the corresponding U-statistic; this is in particular the case if q > 2. While U-statistics have been
extensively studied (e.g. Serﬂing, 1980), results for V-statistics are often restricted to q = 2. Since
for dHSIC we use V-statistics with q = 2d (see Lemma 2.7), we need more general results that are
derived in Appendix A.

The following notation appears throughout the paper in the context of V-statistics and is also
common in the theory of U-statistics, see Serﬂing (1980, Section 5.1.5). Given the core function
g : X q → R we deﬁne for every c ∈ {1, . . . , q − 1} the function gc : X c → R by

gc(x1, . . . , xc) := E (g(x1, . . . , xc, Xc+1, . . . , Xq))

and gq ≡ g. Then, gc is again a symmetric core function such that for every c ∈ {1, . . . , q − 1}, we
have

E (gc(X1, . . . , Xc)) = E (g(X1, . . . , Xq)) = θg.

Further deﬁne ˜g ≡ g − θg and for all c ∈ {1, . . . , q} deﬁne ˜gc ≡ gc − θg to be the centered versions
of the core functions. Moreover, deﬁne for every c ∈ {1, . . . , q},

ξc := Var (gc(X1, . . . , Xc)) = E(cid:0)˜gc(X1, . . . , Xc)2(cid:1) .

(2.3)

We sometimes write ξc(g) to make clear which core function we are talking about.

We deﬁne an estimator for dHSIC by estimating each of the expectation terms in Proposi-

tion 2.5 by a V-statistic.
Deﬁnition 2.6 ( (cid:92)dHSIC)
Assume Setting 1. For all (x1, . . . , xn) ∈ X n deﬁne the estimator (cid:92)dHSIC = ( (cid:92)dHSICn)n∈N as

(cid:92)dHSICn(x1, . . . , xn) :=

1
n2

(cid:88)

M2(n)

− 2
nd+1

kj(cid:16)
d(cid:89)
(cid:88)

j=1

(cid:17)
kj(cid:16)

xj
i1

, xj
i2

d(cid:89)

Md+1(n)

j=1

(cid:88)
(cid:17)

.

+

1
n2d

xj
i1

, xj

ij+1

M2d(n)

j=1

d(cid:89)

kj(cid:16)

xj
i2j−1

, xj
i2j

(cid:17)

if n ∈ {2d, 2d + 1, . . .} and as (cid:92)dHSICn(x1, . . . , xn) := 0 if n ∈ {1, . . . , 2d − 1}.
Whenever it is clear from the context, we drop the functional arguments and just write (cid:92)dHSICn
instead of (cid:92)dHSICn(X1, . . . , Xn). In order to make this estimator more accessible for analysis we

5

can express it as a V-estimator with a single core function. To this end, deﬁne h : X 2d → R to be
the function satisfying for all z1, . . . , z2d ∈ X that

(cid:17)

(2.4)

h(z1, . . . , z2d) =

1

(2d)!

π(1), zj
zj

π(2)

+

π(2j−1), zj
zj

π(2j)

(cid:34) d(cid:89)

(cid:88)

π∈S2d

j=1

kj(cid:16)
d(cid:89)

− 2

kj(cid:16)

(cid:17)

d(cid:89)

j=1

kj(cid:16)
(cid:17)(cid:35)

zj
π(1), zj

π(j+1)

,

j=1

where S2d is the set of permutations on {1, . . . , 2d}. The following proposition shows that (cid:92)dHSIC
is a V-statistic with core function h.
Lemma 2.7 (properties of h)
Assume Setting 1. It holds that the function h deﬁned in (2.4) is symmetric, continuous, and there
exists C > 0 such that for all z1, . . . , z2d ∈ X we have

|h(z1, . . . , z2d)| < C.

Moreover, Vn(h) = (cid:92)dHSICn, see (2.2), and θh = E (h(X1, . . . , X2d)) = dHSIC.
Proof A proof is given in Appendix C.2

(cid:3)

3 Statistical tests for joint independence
Assume Setting 1 and denote by P(X ) the space of Borel probability measures. In this section
we derive three statistical hypothesis tests to test the null hypothesis

H0 :=

against the alternative

HA :=

(cid:26)
(cid:26)

µ ∈ P(X )

µ ∈ P(X )

(cid:12)(cid:12)(cid:12) X ∼ µ = PX, PX = PX 1 ⊗ ··· ⊗ PX d(cid:27)
(cid:12)(cid:12)(cid:12) X ∼ µ = PX, PX (cid:54)= PX 1 ⊗ ··· ⊗ PX d(cid:27)

.

(3.1)

(3.2)

(cid:40)

Since dHSIC equals zero under H0 and is positive otherwise we choose to use n· (cid:92)dHSICn as test
statistic and deﬁne a decision rule ϕ = (ϕn)n∈N encoding rejection of H0 if ϕn = 1 and no rejection
of H0 if ϕn = 0. For all n ∈ {1, . . . , 2d − 1} we deﬁne ϕn := 0 and for all n ∈ {2d, 2d + 1, . . .} and
for all (x1, . . . , xn) ∈ X n we set

ϕn(x1, . . . , xn) :=

0
1

if n · (cid:92)dHSICn(x1, . . . , xn) ≤ cn(x1, . . . , xn)
if n · (cid:92)dHSICn(x1, . . . , xn) > cn(x1, . . . , xn),

(3.3)

where the threshold c = (cn)n∈N remains to be chosen. Ideally, for ﬁxed α ∈ (0, 1) the hypothesis
test should have (valid) level α, i.e.

sup

µ∈H0;X∼µ=PX

P (ϕn(X1, . . . , Xn) = 1) ≤ α,

where X1, X2, . . . iid∼ PX. This is rather hard to satisfy in our setting; we therefore introduce a
weaker condition ensuring that the test respects the level in the large sample limit, i.e.
for all
ﬁxed X with PX ∈ H0 it holds that

P (ϕn(X1, . . . , Xn) = 1) ≤ α,

lim
n→∞

6

where X1, X2, . . . iid∼ PX. In this case, the test is said to have pointwise asymptotic level. Addi-
tionally, it is desirable for the test to be able to detect any form of dependence at least in the large
sample limit, i.e. for all ﬁxed X with PX ∈ HA it holds that

P (ϕn(X1, . . . , Xn) = 1) = 1.

lim
n→∞

A test satisfying this property is called pointwise consistent. The following table summarizes the
properties that our three tests satisfy.

Hypothesis test
Permutation1

consistency
unknown

Bootstrap1

pointwise
(Thm. 3.7)

Gamma approximation

no guarantee

level
valid
(Prop. 3.4)
pointwise
asymptotic
(Thm. 3.6)
no guarantee

speed
slow

slow

fast

In Section 3.1 we consider some of the asymptotic properties of the test statistic n · (cid:92)dHSICn. In
particular, we show the existence of an asymptotic distribution under H0. We then construct
three hypothesis tests of the form (3.3). The ﬁrst two are a permutation test and a bootstrap test
which are discussed in Section 3.2. Both tests are based on resampling and hence do not rely on
an explicit knowledge of the asymptotic distribution under H0. In Section 3.3 we consider a third
test which is based on an approximation of the asymptotic distribution under H0 using a Gamma
distribution.

3.1 Asymptotic behavior of the test statistic
The following theorem is the key result required to show consistency of the bootstrap test.
states that n · (cid:92)dHSICn diverges under HA in the following sense.
Theorem 3.1 (asymptotic distribution of n · (cid:92)dHSICn under HA)
Assume Setting 1. Then under HA it holds for all t ∈ R that

It

P(cid:16)

lim
n→∞

(cid:17)

n · (cid:92)dHSICn ≤ t

= 0.

Proof A proof is given in Appendix C.3.

(cid:3)
We now determine the asymptotic distribution of n · (cid:92)dHSICn under H0, and thereby establish
that under H0, n · (cid:92)dHSICn = OP (1) is of smaller order than under HA. The following theorem
extends Gretton et al. (2007, Theorem 2) from HSIC to dHSIC.
Theorem 3.2 (asymptotic distribution of n · (cid:92)dHSICn under H0)
Assume Setting 1 and recall (2.3). If ξ2(h) > 0, let (Zi)i∈N be a sequence of independent stan-
dard normal random variables on R, let 2 Th2 ∈ L(L2(P(X 1,...,X d),|·|R)) satisfying that for all
1For implementation purposes one can use the Monte-Carlo approximation. This leads to a reasonably fast
implementation with similar level and consistency results. Further details are given at the end of Section 3.2.1 and
Section 3.2.2.
2Given a measure space (Ω, F , µ) the space Lr(µ, |·|R) consists of all measurable functions f : Ω → R satis-
Ω|f (ω)|r µ(dω) < ∞. The corresponding space of equivalence classes of such functions is denoted by
Lr(µ, |·|R). Moreover, we denote the space of all linear bounded operators from a Banach space B onto itself by
L(B).

fying that (cid:82)

7

f ∈ L2(P(X 1,...,X d),|·|R) and for all x ∈ X it holds that

(Th2(f )) (x) =

h2(x, y)f (y) P(X 1,...,X d)(dy)

and let (λi)i∈N be the eigenvalues of Th2, then under H0 it holds that

(cid:90)

X

n · (cid:92)dHSICn

d−→

λiZ 2
i

2
as n → ∞. If ξ2(h) = 0 then under H0 it holds that
d−→

n · (cid:92)dHSICn

as n → ∞.
Proof A proof is given in Appendix C.4.

(cid:18)2d
(cid:19) ∞(cid:88)
(cid:19) ∞(cid:88)
(cid:18)2d

i=1

2

i=1

λi

(cid:3)

Note that the results on permutation do not make use of the asymptotic distributions; the
results on the bootstrap approach cover both cases ξ2(h) > 0 and ξ2(h) = 0, the distinction
appears only in the corresponding proofs.

3.2 Permutation and bootstrap test
Assume Setting 1 and ﬁx n ∈ N. For every function ψ = (ψ1, . . . , ψd) such that for all i ∈ {1, . . . , d}
it holds that ψi : {1, . . . , n} → {1, . . . , n} deﬁne the function gn,ψ : X n → X n satisfying for all
(x1, . . . , xn) ∈ X n that

(cid:16)

gn,ψ(x1, . . . , xn) :=

xψ
n,1, . . . , xψ
n,n

x1
ψ1(i), . . . , xd

ψd(i)

. The diagram in (3.5) illustrates how gn,ψ acts on the sample

(3.4)

(3.5)

(cid:16)

where xψ
n,i :=
(x1, . . . , xn).

Deﬁne

(cid:17)

···

···

x1
x1
1
...
...
xn x1
n

xd
1
...
xd
n

ψ1(1)

x1

xψ
n,1
gn,ψ−→ ...
xψ
n,n x1

...

Bn :=(cid:8)ψ : {1, . . . , n} → {1, . . . , n} | ψ is a function(cid:9)

ψd(n)

ψ1(n)

xd

xd

ψd(1)

...

(cid:17)

···

···

then for a subset An ⊆ Bd

a resampling method.
resampling method.

n we call the family of functions
g := ((gn,ψ)ψ∈An )n∈N

(3.6)
In the next two sections we consider two explicit tests based on this

3.2.1 Permutation test
The permutation test is the resampling test corresponding to the resampling method in (3.6) with
An = (Sn)d, where Sn is the set of permutations on {1, . . . , n}. More precisely, we have the
following deﬁnition.
Deﬁnition 3.3 (permutation test for dHSIC)
Assume Setting 1, assume α ∈ (0, 1) and for all n ∈ N and for all ψ ∈ (Sn)d let gn,ψ be deﬁned
distribution functions deﬁned for all (x1, . . . , xn) ∈ X n and for all t ∈ R by

as in (3.4). Moreover, for all n ∈ {2d, 2d + 1, . . .} let (cid:98)Rn : X n × R → [0, 1] be the resampling

1{n· (cid:92)dHSICn(gn,ψ(x1,...,xn))≤t}.

(3.7)

(cid:98)Rn(x1, . . . , xn)(t) :=

1

(n!)d

(cid:88)

ψ∈(Sn)d

8

Then the α-resampling hypothesis test ϕ = (ϕn)n∈N deﬁned for all n ∈ {1, . . . , 2d − 1} by ϕn := 0
and for all n ∈ {2d, 2d + 1, . . .} and for all (x1, . . . , xn) ∈ X n by

ϕn(x1, . . . , xn) := 1(cid:110)

(cid:111)
n· (cid:92)dHSICn(x1,...,xn)>((cid:98)Rn(x1,...,xn))−1(1−α)

is called α-permutation test for dHSIC.

Given that the resampling method has a group structure and additionally satisﬁes for all X with
PX ∈ H0 that

gn,ψ(X1, . . . , Xn) is equal in distribution to (X1, . . . , Xn),

where X1, X2,··· ∈ PX, it can be shown that tests of this form have valid level. For the permu-
tation test for dHSIC both these properties are satisﬁed, hence it has valid level.
Proposition 3.4 (permutation test for dHSIC has valid level)
Assume Setting 1 and let H0 and HA be deﬁned as in (3.1) and (3.2). Then for all α ∈ (0, 1) the
α-permutation test for dHSIC has valid level α when testing H0 against HA.
Proof A proof is given in Appendix C.5.

(cid:3)

The size of the set (Sn)d is given by (n!)d, therefore computing (3.7) quickly becomes infeasi-
ble. For implementation purposes we generally use a Monte-Carlo approximated version, details
are given in Section 4.2. Surprisingly, it can be shown that whenever the probability distribu-
tion PX is continuous, the Monte-Carlo approximated permutation test also has valid level, see
Proposition B.9 and the comments thereafter.

3.2.2 Bootstrap test
The bootstrap test is the resampling test corresponding to the resampling method in (3.6) with
n.
An = Bd
Deﬁnition 3.5 (bootstrap test for dHSIC)
Assume Setting 1, assume α ∈ (0, 1) and for all n ∈ N and for all ψ ∈ Bd
resampling distribution functions deﬁned for all (x1, . . . , xn) ∈ X n and for all t ∈ R by

be deﬁned as in (3.4). Moreover, for all n ∈ {2d, 2d + 1, . . .} let (cid:98)Rn : X n × R → [0, 1] be the

n let the function gn,ψ

(cid:98)Rn(x1, . . . , xn)(t) :=

1
nnd

(cid:88)

ψ∈Bd

n

1{n· (cid:92)dHSICn(gn,ψ(x1,...,xn))≤t}.

Then the resampling hypothesis test ϕ = (ϕn)n∈N deﬁned for all n ∈ {1, . . . , 2d − 1} by ϕn := 0
and for all n ∈ {2d, 2d + 1, . . .} and for all (x1, . . . , xn) ∈ X n by

ϕn(x1, . . . , xn) := 1(cid:110)

(cid:111)
n· (cid:92)dHSICn(x1,...,xn)>((cid:98)Rn(x1,...,xn))−1(1−α)

is called α-bootstrap test for dHSIC.

Unlike for the permutation test the resampling method in the bootstrap setting is no longer a
group. We can therefore not expect the bootstrap test to have valid level. However, it is possible
to show that it has pointwise asymptotic level and even pointwise consistency. The reason this
can be done is that the resampling method in the bootstrap test is connected to the empirical
n . The following theorem proves that the bootstrap test for

product distribution (cid:98)PX 1

n ⊗ ··· ⊗(cid:98)PX d

dHSIC has pointwise asymptotic level.
Theorem 3.6 (bootstrap test for dHSIC has pointwise asymptotic level)
Assume Setting 1 and let H0 and HA be deﬁned as in (3.1) and (3.2). Then for all α ∈ (0, 1) the
α-bootstrap test for mHSIC has pointwise asymptotic level α when testing H0 against HA.

Proof A proof is given in Appendix C.6.

(cid:3)

9

Almost the same proof can be exploited to prove that the bootstrap test for dHSIC is consistent.

Theorem 3.7 (consistency of the bootstrap test for dHSIC)
Assume Setting 1 and let H0 and HA be deﬁned as in (3.1) and (3.2). Then for all α ∈ (0, 1) the
α-bootstrap test is pointwise consistent when testing H0 against HA.

Proof A proof is given in Appendix C.7.

(cid:3)

Similarly as for the permutation test, the size of the set (Bn)d is nnd which grows quickly.

Again, we may use a Monte-Carlo approximated version, see Section 4.2.

(cid:18)2d
(cid:19) ∞(cid:88)

2

i=1

3.3 Gamma approximation
We showed in Theorem 3.2 that, if ξ2(h) > 0, the asymptotic distribution of n · (cid:92)dHSICn is given
by

λiZ 2
i .

(3.8)

distribution of the form (cid:80)∞

i=1 λiZ 2

The essential idea behind the Gamma approximation (see also Gretton et al., 2005) is that a
i can be approximated fairly well by a Gamma distribution
with matched ﬁrst and second moments (see Satterthwaite, 1946, for basic empirical evidence).
This has, however, only been shown empirically and there are no guarantees that it leads to good
results in the large sample limit. Nevertheless, the approximation seems to work well for small d,
see Section 5, and can be computed much faster than the other approaches.
Here, we approximate the distribution under the null hypothesis assuming ξ2(h) > 0. By
Theorem 3.2 n · (cid:92)dHSICn converges to the expectation of the distribution in (3.8) if ξ2(h) = 0. If
this expectation lies within the acceptance region, neglecting this case preserves the validity of the
level.

The Gamma distribution with parameters α and β is denoted by Gamma(α, β) and corresponds

to the distribution with density

where Γ(t) =(cid:82) ∞

f (x) =

x
β

xα−1e
βαΓ(α)

,

0 xt−1e−xdx is the Gamma function. The ﬁrst two moments of the Gamma(α, β)-
distributed random variable Y are given by E(Y ) = αβ and Var(Y ) = αβ2. In order to match the
ﬁrst two moments we deﬁne for X1, X2, . . . iid∼ PX ∈ H0 the two parameters

αn(PX) :=

(cid:17)(cid:17)2
(cid:16)E(cid:16) (cid:92)dHSICn
(cid:16) (cid:92)dHSICn
(cid:17)

(cid:16) (cid:92)dHSICn
(cid:17)
(cid:17) .
E(cid:16) (cid:92)dHSICn
n · (cid:92)dHSICn(X1, . . . , Xn) ∼ Gamma(cid:0)αn(PX), βn(PX)(cid:1) .

and βn(PX) :=

n Var

Var

Then we make the approximation

(3.9)

In order to use this approximation in a hypothesis test we ﬁrst need to ﬁnd a method to calcu-
late αn(PX) and βn(PX) based only on the ﬁrst n observations X1, . . . , Xn. The following two
propositions give expansions of the moments in terms of the kernel.
Proposition 3.8 (mean of (cid:92)dHSIC)
Assume Setting 1. Then under H0 it holds that,

E(cid:16) (cid:92)dHSICn

(cid:17)

=

1
n

− 1
n

(cid:89)

j(cid:54)=r

d(cid:88)

r=1

E(cid:16)

(cid:17) +

d(cid:89)

E(cid:16)

j=1

d − 1
n

kj(X j

1 , X j
2 )

kj(X j

1 , X j
2 )

(cid:17)

+ O(cid:0)n−2(cid:1)

as n → ∞.

10

Proof A proof is given in Appendix C.8.
Proposition 3.9 (variance of (cid:92)dHSIC)
Assume Setting 1. Then under H0 it holds that,

(cid:16) (cid:92)dHSICn

(cid:17)

Var

(n − 2d)!

n!

= 2

(n − 2d)!
(n − 4d + 2)!

e1(j) + (d − 1)2

(cid:3)

d(cid:89)

j=1

e0(j)2

(cid:34) d(cid:89)
d(cid:88)

j=1

(cid:35)

(cid:89)
d(cid:88)
(cid:18)

j=1

e2(j) +

e1(j)

e0(r)2

j=1

r(cid:54)=j
e2(r) − 2(d − 1)

(cid:89)

r(cid:54)=j

e0(r)2

e2(j)

(cid:19)

e2(j)e2(l)

e0(r)2

+ O

− 5
2

n

d(cid:89)
(cid:89)

j=1

r(cid:54)=j

e1(j)

+ 2(d − 1)

j=1

− 2

d(cid:88)
(cid:88)
e1(j) = E(cid:16)

j(cid:54)=l

+

r(cid:54)=j,l

(cid:89)
2 )2(cid:17)

as n → ∞ and where for all j ∈ {1, . . . , d} we set

e0(j) = E(cid:16)

(cid:17)

kj(X j

1 , X j
2 )

,

kj(X j

1 , X j

Proof A proof is given in Appendix C.9.

,

e2(j) = E

X j
1

(cid:18)

E
X j
2

(cid:16)

kj(X j

1 , X j
2 )

(cid:17)2(cid:19)

.

(cid:3)

Based on these two propositions we only need a method to estimate the terms e0(j), e1(j)
and e2(j) for all j ∈ {1, . . . , d}. One could use a U-statistic (A.1) for each expectation term as
this would not add any bias.
It turns out, however, that a V-statistic also does not add any
asymptotic bias in this particular case. This is due to Theorem A.6, which shows that the bias of
Proposition 3.9. The V-statistics for these terms are given for all (x1, . . . , xn) ∈ X n by

a V-statistic is of order O(cid:0)n−1(cid:1) and hence is consumed by the error terms in Proposition 3.8 and
(cid:80)n
(i) (cid:98)e0(j)(x1, . . . , xn) := 1
(cid:80)n
(ii) (cid:98)e1(j)(x1, . . . , xn) := 1
(cid:16)(cid:80)n
(cid:80)n
(iii) (cid:98)e2(j)(x1, . . . , xn) := 1
Based on these terms we ﬁrst deﬁne the estimator (cid:100)Expn by replacing all appearances of e0(j) by
(cid:17)
(cid:98)e0(j) in the expansion of E(cid:16) (cid:92)dHSICn
given in Proposition 3.8. Then deﬁne the estimator (cid:100)Varn
by replacing all appearances of e0(j), e1(j) and e2(j) by (cid:98)e0(j), (cid:98)e1(j) and (cid:98)e2(j) in the expansion
given in Proposition 3.9. Finally, we deﬁne the estimator (cid:98)αn : X n → R of αn

i1,i2=1 kj(xj
i1,i2=1 kj(xj

(cid:16) (cid:92)dHSICn

i1=1 kj(xj

(cid:17)2

, xj
i2

, xj
i2

, xj
i2

(cid:17)

)2,

i2=1

),

n2

n2

n3

i1

i1

i1

)

.

of Var
for all (x1, . . . , xn) ∈ X n by

(cid:98)αn(x1, . . . , xn) :=

(cid:100)Expn(x1, . . . , xn)2
(cid:100)Varn(x1, . . . , xn)
and the estimator (cid:98)βn : X n → R of βn for all (x1, . . . , xn) ∈ X n by
n(cid:100)Varn(x1, . . . , xn)
(cid:100)Expn(x1, . . . , xn)

(cid:98)βn(x1, . . . , xn) :=

.

Using these estimators we can deﬁne the following hypothesis test.

11

(3.10)

(3.11)

the Gamma((cid:98)αn(x1, . . . , xn),(cid:98)βn(x1, . . . , xn))-distribution, where(cid:98)αn and (cid:98)βn are deﬁned as in (3.10)

Deﬁnition 3.10 (Gamma approximation based test for dHSIC)
Assume Setting 1, let α ∈ (0, 1) and for all n ∈ N let Fn : X n × R → [0, 1] be the functions
satisfying for all (x1, . . . , xn) ∈ X n that Fn(x1, . . . , xn) is the distribution function associated to
and (3.11) respectively. Then the hypothesis test ϕ = (ϕn)n∈N satisfying for all n ∈ {1, . . . , 2d− 1}
that ϕn := 0 and for all n ∈ {2d, 2d + 1, . . .} and for all (x1, . . . , xn) ∈ X n that
(cid:111)
n· (cid:92)dHSICn(x1,...,xn)>Fn(x1,...,xn)−1(1−α)

ϕn(x1, . . . , xn) := 1(cid:110)

is called α-Gamma approximation based test for dHSIC.

Implementation

4
We now provide details about an eﬃcient implementation of the proposed tests and brieﬂy com-
ment on the choice of kernel. All methods are available in the package dHSIC for the R-language
(R Core Team, 2014).

4.1 dHSIC estimator
In this section we describe how one can eﬃciently implement the (cid:92)dHSIC estimator. One eﬃcient
implementation is given in Algorithm 1, where the function Sum takes the sum of all elements in
a matrix, the function ColumnSum takes the sums of the columns of a matrix and the operator ∗
is the element-wise multiplication operator.

Algorithm 1 computing the dHSIC V-estimator
1: procedure dHSIC(x1, . . . , xn)
2:
3:

for j = 1 : d do

Kj ← Gram matrix of kernel kj given x1, . . . , xn
term1 ← (n × n)-matrix with all entries equal to 1
term2 ← 1
term3 ← (1 × n)-matrix with all entries equal to 2
for j = 1 : d do

n

term1 ← term1 ∗ Kj
term2 ← 1
term3 ← 1

n2 · term2 · Sum(Kj)
n · term3 ∗ ColumnSum(Kj)

term1 ← Sum(term1)
term3 ← Sum(term3)
dHSIC ← 1
return dHSIC

m2 · term1 · term2 · term3

4:
5:
6:
7:
8:
9:
10:

11:
12:
13:
14:

4.2 Permutation and bootstrap test
Fix α ∈ (0, 1) and assume we observe (X1, . . . , Xn) = (x1, . . . , xn). In order to apply the permu-
tation test for dHSIC or the bootstrap test for dHSIC, one has to calculate the quantile

(cid:16)(cid:98)Rn(x1, . . . , xn)
(cid:17)−1

(1 − α).

From the deﬁnition of (cid:98)Rn it is clear that this involves (n!)d evaluations of (cid:92)dHSIC for the permu-

tation test and nnp evaluations of (cid:92)dHSIC for the bootstrap test. In both settings this becomes

12

use the Monte-Carlo approximation deﬁned in Deﬁnition B.6. Essentially this involves calculating
the p-value given by

computationally impossible rather fast as n grows. Instead of computing (cid:98)Rn explicitly one can
1 +(cid:12)(cid:12){i ∈ {1, . . . , B} : (cid:92)dHSIC(gn,ψi(x1, . . . , xn)) ≥ (cid:92)dHSIC(x1, . . . , xn)}(cid:12)(cid:12)
(cid:98)pn(x1, . . . , xn) :=
(cid:98)pn(x1, . . . , xn) ≤ α. The corresponding critical value is calculated according to Proposition B.10.

where (ψi)i∈N is a sequence drawn from the uniform distribution on An (i.e. on (Sn)d for the per-
mutation test and on Bd
n for the bootstrap test). The test then rejects the null hypothesis whenever

For further details see Section B.1

1 + B

,

For practical applications one generally ﬁxes the value of B. Davison and Hinkley (1997)

suggest to use a value of B between 99 and 999.

In the following two section we give some additional details speciﬁc to the permutation test

and the bootstrap test.

4.2.1 Permutation test
As shown in the proof of Proposition 3.4 the resampling method g for the permutation test is a
resampling group which satisﬁes the invariance conditions (B.4) and (B.5). This allows us to apply
Proposition B.9 to see that the Monte-Carlo approximated permutation test has valid level for
any ﬁnite B, given that we have continuous random variables as input. Algorithm 2 shows how to
implement the p-value and the critical value for the Monte-Carlo approximated permutation test.

4.2.2 Bootstrap test
We showed that the bootstrap test for dHSIC has pointwise asymptotic level and is pointwise
consistent. Both of these properties are preserved if we use the Monte-Carlo approximation and
let B tend to inﬁnity. Algorithm 2 shows how to implement the p-value and the critical value for
the Monte-Carlo approximated bootstrap test.

4.3 Gamma approximation test
Fix α ∈ (0, 1) and assume we observe (X1, . . . , Xn) = (x1, . . . , xn). Implementing the Gamma
approximation based α-test for dHSIC consists of four steps (see Section 3.3 for notation):

1. for all j ∈ {1, . . . , d} implement the estimators(cid:98)e0(j), . . . ,(cid:98)e2(j),
2. compute the estimates (cid:100)Expn(x1, . . . , xn) and (cid:100)Varn(x1, . . . , xn),
3. using (3.10) and (3.11) compute the estimates (cid:98)αn(x1, . . . , xn) and (cid:98)βn(x1, . . . , xn) and
4. compute the 1 − α quantile of the Gamma((cid:98)αn(x1, . . . , xn),(cid:98)βn(x1, . . . , xn))-distribution.
Gamma((cid:98)αn(x1, . . . , xn),(cid:98)βn(x1, . . . , xn))-distribution calculated in the last step.

The hypothesis test rejects H0 if n · (cid:92)dHSICn(x1, . . . , xn) is larger than the 1 − α quantile of the

4.4 Choice of kernel
The choice of the kernel determines how well certain types of dependence can be detected and
therefore inﬂuences the practical performance of dHSIC. For continuous data a common choice is
a Gaussian kernel as deﬁned in (2.1). It is characteristic, which ensures that all the above results
hold. In particular, any type of dependence can be detected in the large sample limit. We use the
median heuristic for choosing the bandwidth σ by requiring that median{(cid:107)xi − xj(cid:107)Rm : i < j} =
2σ2. This heuristic performs quite well in many practical applications. It may be possible, however,
to extend alternative approaches from two-sample testing to independence testing (e.g. Gretton

13

initialize empty B-dimensional vector T
for k = 1 : B do

Algorithm 2 computing p-value and critical value for the permutation/bootstrap test
1: procedure MonteCarlo-pvalue(x1, . . . xn, B)
2:
3:
4:
5:
6:
7:
8:

ψ ← random element from Sn (permutation) or {1, . . . , n}n (bootstrap)
for i = 1 : n do

initialize d-dimensional vectors ˜x1, . . . , ˜xn
for j = 1 : d do

˜xi[j] ← xψ(i)[j]

T[k] ← dHSIC(˜x1, . . . , ˜xn)
tmp ← #{k ∈ {1, . . . , B}| T[k] ≥ dHSIC(x1, . . . xn))}
pval ← (tmp + 1)/(B + 1)
return pval

9:
10:
11:
12:
13: procedure MonteCarlo-critval(x1, . . . xn, B, α)
14:
15:
16:
17:
18:
19:
20:

initialize d-dimensional vectors ˜x1, . . . , ˜xm
for j = 1 : d do

initialize empty B-dimensional vector T
for k = 1 : B do

˜xi[j] ← xψ(i)[j]

T[k] ← n · dHSIC(˜x1, . . . , ˜xn)
tmp ← #{k ∈ {1, . . . , B}| T[k] = dHSIC(x1, . . . xn))}
ind ← (cid:100)(B + 1) · (1 − α)(cid:101) + tmp
if ind ≤ B then

ψ ← random element from Sn (permutation) or {1, . . . , n}n (bootstrap)
for i = 1 : m do

21:
22:
23:
24:
25:
26:
27:
28:
29:

S ← sort(T) (in ascending order)
critval ← S[ind]
critval ← ∞
return critval

else

14

et al., 2012). For discrete data, we choose a trivial kernel deﬁned by k(x, y) := 1{x=y}. In our
experiments in Section 5 we use a Gaussian kernel with median heuristic for continuous data and
a trivial kernel for the discrete data.

Note that the level and consistency results from Section 3 have been developed for a ﬁxed
kernel, they do not necessarily hold if the kernel (e.g. its bandwidth) is estimated from data. To
circumvent this problem, one could split the sample, estimate the kernel parameters on the ﬁrst
part of the data and perform the testing procedure with the ﬁxed kernel on the second part.

5 Experiments
5.1 Competing method
For comparison purposes we use a multiple testing version of the two variable HSIC test. The
idea is that if we can group several variables together, we can test the newly constructed multi-
variate variable against a diﬀerent variable using the two variable HSIC. In order to test for joint
independence we use the following testing sequence,

use HSIC to test whether X d is independent of [X 1, . . . , X d−1],
use HSIC to test whether X d−1 is independent of [X 1, . . . , X d−2],

1.
2.

...

d − 1. use HSIC to test whether X 2 is independent of X 1.

Finally, we account for the increased family-wise error rate using the Bonferroni correction, i.e.
we perform all tests at level α/(d − 1) and reject the null hypothesis if any of the individual tests
rejects the null hypothesis. In the following sections we refer to this test simply as HSIC.

This method is of course not restricted to HSIC but can be performed for any two variable
independence test. There are, however, several immediate drawbacks to this method. Firstly,
it is known that Bonferroni procedures are often conservative in the sense that they accept the
null hypothesis too often. This becomes particularly evident if this procedure is combined with
a permutation test based HSIC. In that case it can be shown that the smallest possible p-value
after the Bonferroni correction is given by (d− 1)/(B + 1) and hence for B = 100 the test will not
be able to reject the null hypothesis at a level of 5% if d > 6. At last, such a test is asymmetric
in the d random variables and depends on the order of the random variables.

5.2 Causal inference
In causal discovery, we aim at estimating the causal structure from an observed joint distribution.
More formally, we consider additive noise models (Peters et al., 2014) with additive nonlinear
functions and Gaussian noise variables (Bühlmann et al., 2014); these are special cases of structural
equation models (Pearl, 2009). Assume that the distribution PX of a d-dimensional random vector
X = (X 1, . . . , X d) is induced by a collection of d structural equations
j ∈ {1, . . . , d},

f j,k(X k) + N j,

(cid:88)

X j :=

(5.1)

k∈PAj

with PAj being the parents of j in the associated DAG G0. The noise variables N 1, . . . , N d
are normally distributed and are assumed to be jointly independent. An important question in
causality is whether the causal structure, in this case G, can be inferred from the observational
distribution PX. While this is impossible for general structural equation models (e.g. Peters et al.,
2014, Proposition 9), the additive noise structure renders the graph identiﬁable. That is, if f j,k
are assumed to be nonlinear, any other additive noise model (5.1) with a structure diﬀerent from
G0 cannot induce the distribution PX (see Peters et al., 2014, Corollary 31, for the full result). In
other words, using conditional means as functions in the SEM, the corresponding residual variables
will not be jointly independent.

15

We therefore propose the following method for structure learning using generalized additive

model regression (GAM) (Wood and Augustin, 2002).

DAG veriﬁcation method
Given: observations X1, . . . , Xn and a candidate DAG G
1) Use generalized additive model regression (GAM) to regress each node X j on all its parents

PAj and denote the resulting vector of residuals by resj.

2) Perform a d-variable joint independence test (e.g. dHSIC) to test whether

(res1, . . . , resd) is jointly independent.

3) If (res1, . . . , resd) is jointly independent, then the DAG is not rejected.
We can also use this DAG veriﬁcation method to ﬁnd the correct DAG by performing the
check for all possible DAGs with the correct number of nodes. In practice, we expect this method
to accept also supergraphs of the correct graph G0, which can be overcome by a variable selection
method. Since this work concentrates on the dependence structure among the residuals, we instead
consider only fully connected DAGs in the experiments (Section 5.3.4). In practice, we do not
want to iterate over all possible graphs. A more eﬃcient method, which is based on a similar idea,
is the RESIT (regression with subsequent independence test) algorithm described in Peters et al.
(2014, Section 4.1). Also the eﬃcient method CAM (Bühlmann et al., 2014) could be equipped
with a joint independence test as a model check.

5.3 Results
In this section we perform simulations to verify the properties of the three hypothesis tests derived
in the previous section. We separate the experiments into ﬁve parts: level analysis, power analysis,
runtime analysis and causal inference on simulated and a real world data set.

5.3.1 Level analysis
In Section 3 we proved the following results.

(i) The permutation test has valid level (even the Monte-Carlo approximated test with any B).

(ii) The bootstrap test has pointwise asymptotic level.

(iii) The Gamma approximation based test has no guarantee for level.
We verify these results numerically by considering two examples of ﬁxed elements PX ∈ H0. In
iid∼ PX for diﬀerent sample sizes n
both examples we simulate m = 1000 realizations of X1, . . . , Xn
and check how often each of the four hypothesis tests reject the null hypothesis.

Simulation 1 (testing level - three continuous variables)
Consider X 1, X 2, X 3 iid∼ N (0, 1), then for X = (X 1, X 2, X 3) it holds that

PX = PX 1 ⊗ PX 2 ⊗ PX 3 ∈ H0,

where H0 is the null hypothesis deﬁned in (3.1).
Set α = 0.05, B = 25, and
n ∈ {100, 200, . . . , 1000}. The rejection rates for the corresponding four hypothesis tests (per-
mutation, bootstrap, Gamma approximation and eigenvalue) based on m = 1000 repeated
draws of X are plotted in Figure 1.

16

permutation

bootstrap

gamma

e
t
a
r

n
o
i
t
c
e
j
e
r

5
6
0
.
0

5
5
0
.
0

5
4
0
.
0

5
3
0
.
0

e
t
a
r

n
o
i
t
c
e
j
e
r

5
6
0
.
0

5
5
0
.
0

5
4
0
.
0

5
3
0
.
0

e
t
a
r

n
o
i
t
c
e
j
e
r

5
6
0
.
0

5
5
0
.
0

5
4
0
.
0

5
3
0
.
0

200

400

600

800

1000

200

400

600

800

1000

200

400

600

800

1000

sample size n

sample size n

sample size n

Figure 1: Simulation 1 (testing level - three continuous variables): Rejection rates, based on
m = 1000 repetitions, for each of the three diﬀerent hypothesis tests based on dHSIC. The test
has valid level if the rejection rate does not lie far above the dotted red line at 0.05.

Simulation 2 (testing level - continuous and discrete variables)
Consider X 1 ∼ N (0, 1) and X 2 ∼ Bin(20, 0.2) with X 1 and X 2 independent. Then for
X = (X 1, X 2) it holds that

PX = PX 1 ⊗ PX 2 ∈ H0,

where H0 is the null hypothesis deﬁned in (3.1).
Set α = 0.05, B = 100, and
n ∈ {100, 200, . . . , 1000}. The rejection rates for the corresponding four hypothesis tests (per-
mutation, bootstrap, Gamma approximation and eigenvalue) based on m = 1000 repeated
draws of X are plotted in Figure 2.

permutation

bootstrap

gamma

e
t
a
r

n
o
i
t
c
e
j
e
r

5
5
0
.
0

5
4
0
.
0

5
3
0
.
0

e
t
a
r

n
o
i
t
c
e
j
e
r

5
5
0
.
0

5
4
0
.
0

5
3
0
.
0

e
t
a
r

n
o
i
t
c
e
j
e
r

5
5
0
.
0

5
4
0
.
0

5
3
0
.
0

200

400

600

800

1000

200

400

600

800

1000

200

400

600

800

1000

sample size n

sample size n

sample size n

Figure 2: Simulation 2 (testing level - continuous and discrete variables): Rejection rates, based
on m = 1000 repetitions, for each of the three diﬀerent hypothesis tests based on dHSIC. The test
has valid level if the rejection rate does not lie far above the dotted red line at 0.05.

In both simulations we get similar results. We collect the most important observations.

(i) The permutation test achieves level α. This corresponds to what has been proved in the
previous section. As mentioned above, this result is rather surprising as it does not depend
on the choice of B, which in Simulation 1 is very small (B = 25).

(ii) The bootstrap test appears to achieve level α in most cases, even though we only proved

17

that it has pointwise asymptotic level. This is due to the conservative choice of the p-value
in the Monte-Carlo approximation of the bootstrap test.

(iii) The Gamma approximation based test, at least in these two examples, has level close to α
but often slightly exceeds the required level. For larger values of d the Gamma approximation
seems to break done. For instance, if we perform Simulation 1 with 10 variables instead of
three the rejection rate for a sample size of n = 100 is 0.40 and even for n = 200 it is still
0.21. The bootstrap test on the other hand is not aﬀected in this way (in the same setting
we get 0.03 for n = 100 and 0.04 for n = 200).

5.3.2 Power analysis
Assessing the power of a test can be done for many diﬀerent alternatives. Here, we show two
examples, one favoring dHSIC, another one favoring the multiple testing approach with running
HSIC d − 1 times.

Simulation 3 (comparing power - single edge)
For an additive noise model over random variables X 1, . . . , X d,

(cid:88)

X j :=

f j,k(X k) + N j,

j ∈ {1, . . . , d},

k∈PAj

with corresponding DAG G, we sample data in the following way. The noise variables are
Gaussian with a standard deviation sampled uniformly between
2 and 2. Nodes without
√
parents follow a Gaussian distribution with standard deviation sampled uniformly between
2 and 5 · 2. The functions f j,k are sampled from a Gaussian process with Gaussian kernel
5
and bandwidth one.
In this Simulation 3 we choose d = 4 and G to be the graph that contains 1 → 2 as a single
edge. We expect that this setting favors the multiple testing approach: due to its dependence
on the order of the variables, it tests X1 against X2.

√

permutation

bootstrap

gamma

HSIC
dHSIC

e
t
a
r

n
o
i
t
c
e
j
e
r

0
.
1

8
.
0

6
.
0

4
.
0

2
.
0

0
.
0

HSIC
dHSIC

e
t
a
r

n
o
i
t
c
e
j
e
r

0
.
1

8
.
0

6
.
0

4
.
0

2
.
0

0
.
0

HSIC
dHSIC

e
t
a
r

n
o
i
t
c
e
j
e
r

0
.
1

8
.
0

6
.
0

4
.
0

2
.
0

0
.
0

50

100

150

200

50

100

150

200

50

100

150

200

sample size

sample size

sample size

Figure 3: Simulation 3 (comparing power - single edge): Rejection rates, based on m = 1000
repetitions, for each of the three diﬀerent hypothesis tests. The example (in particular the chosen
order of variables) is constructed to favor the pairwise testing approach (HSIC). It performs only
slightly better than dHSIC.

Simulation 4 (comparing power - full DAG)
We simulate the data as described in Simulation 3 but this time using a (randomly chosen)
full DAG G over d = 4 variables, i.e. every pair of two nodes is connected. We expect that
this setting favors dHSIC.

18

permutation

bootstrap

gamma

e
t
a
r

n
o
i
t
c
e
j
e
r

0
.
1

8
.
0

6
.
0

4
.
0

2
.
0

0
.
0

e
t
a
r

n
o
i
t
c
e
j
e
r

0
.
1

8
.
0

6
.
0

4
.
0

2
.
0

0
.
0

HSIC
dHSIC

e
t
a
r

n
o
i
t
c
e
j
e
r

0
.
1

8
.
0

6
.
0

4
.
0

2
.
0

0
.
0

HSIC
dHSIC

HSIC
dHSIC

50

100

150

200

50

100

150

200

50

100

150

200

sample size

sample size

sample size

Figure 4: Simulation 4 (comparing power - full DAG): Rejection rates, based on m = 1000
repetitions, for each of the three diﬀerent hypothesis tests. As expected, dHSIC outperforms the
competing method HSIC that is based on pairwise independence tests.

5.3.3 Runtime analysis
We also compare the runtime of the dHSIC test with the runtime of the multiple testing approach

for HSIC. The computational complexity for the dHSIC test statistic is O(cid:0)dn2(cid:1) as can be seen
such as for the Gaussian kernel), we end up with a computational complexity of O(cid:0)d2n2(cid:1). We

from the considerations in Section 4.1. The multiple testing approach for HSIC computes HSIC
d−1 times, which appears to result in the same computational complexity. But since the dimension
of the input variables for the HSIC tests often depends on d, too (at least in common settings

numerically test these computational complexities by two simulations. In the ﬁrst simulation we
ﬁx n vary d vary; in the second simulation we ﬁx d and vary n. The results are presented in
Figure 5.

HSIC
dHSIC

s
d
n
o
c
e
s

6
6
.
.
0
0

4
4
.
.
0
0

2
2
.
.
0
0

0
0
.
.
0
0

HSIC
dHSIC

s
d
n
o
c
e
s

0
0
1
1

8
8

6
6

4
4

2
2

0
0

0

20

40

60

80

100

500

1000

1500

2000

number of variables d

sample size n

Figure 5: runtime analysis: (left) varying number of variables and ﬁxed sample size (n = 100) and
(right) varying sample size and ﬁxed number of variables (d = 10).

5.3.4 Causal inference (simulated data)
We now apply both tests to the DAG veriﬁcation method described in Section 5.2. As in Simula-
tion 3, we simulate data from an additive noise model. Here, we randomly choose a fully connected
DAG G over d = 4 nodes and choose Gaussian distributed noise variables with standard deviation
sampled uniformly between 1/5 and
2 and 2. We then report how often (out of
m = 1000), the largest p-value leads to the correct DAG. Because of its computational advantage,

2/5 instead of

√

√

19

we show the tests based on the Gamma approximation, which works reasonably well for four nodes
(strictly speaking, we only use the relative size of the p-values). The proposed dHSIC recovers the
correct DAG in more cases than the pairwise approach, see Figure 6.

HSIC
dHSIC

s
G
A
D
t
c
e
r
r
o
c

f
o

r
e
b
m
u
n

0
0
0
1

0
0
8

0
0
6

0
0
4

0
0
2

0

D
I
S

e
g
a
r
e
v
a

2
2
1
1

0
0
1
1

8
8

6
6

4
4

2
2

0
0

HSIC
dHSIC

100

200

300

400

500

0

100

300

500

sample size n

sample size n

Figure 6: Causal inference example (m = 1000 repetitions): (left) shows how often each method
estimated the correct DAG, (right) shows the average structural intervention distance (SID, small
is good) between the estimated and the correct DAG for each method (Peters and Bühlmann,
2015).

5.3.5 Causal inference (real data example)
In this section we apply the DAG veriﬁcation method introduced at the end of Section 5.2 to real
world data. Given 349 measurements of the variables Altitude, Temperature and Sunshine3, we
try to determine the correct causal structure. For three variables there exist a total of 25 possible
DAGs. We use the permutation test for dHSIC (with B = 1000) and the multiple testing approach
for HSIC (also with a permutation test and B = 1000) to every possible DAG and compare the
resulting p-values. The result is shown in Figure 7 (left).

The model with the largest p-value is the one represented by DAG 25, which is given in
Figure 7 (right). This causal structure makes sense from our physical understanding, as we would
expect altitude to eﬀect both sunshine and temperature. The eﬀect of temperature on sunshine
could be explained by intermediate latent variables such as clouds or fog.

The plot illustrates that the dHSIC based test is able to reject all incorrect models, even for
very low thresholds. The competing HSIC method on the other hand is not able to reject all
incorrect DAGs, for example DAG 12 has a p-value of about 0.01 but contains an edge from
Sunshine to Altitude, which is clearly the wrong causal direction.

3The dataset is taken from Mooij et al. (2016, pair0001.txt and pair0004.txt).

20

HSIC
dHSIC

Altitude

Sunshine

Temperature

)
e
u

l
a
v
-
p
(
g
o
l

3
-

4
-

5
-

6
-

7
-

5

10

15

20

25

DAG

Figure 7: Real world data example: The left plot shows log(p-value) for each DAG over three nodes
after applying the DAG veriﬁcation method. Even for small thresholds dHISC is able to reject
all incorrect models, while the competing HSIC method cannot. The graphical representation of
DAG 25 is shown on the right.

6 Summary
We introduced a measure of joint dependence between d variables, which we call the d-variable
Hilbert-Schmidt independence criterion (dHSIC). We thereby extended the two-variable Hilbert-
Schmidt independence criterion (HSIC). As in the HSIC case, we proposed an estimator of dHSIC
using a V-statistic and derived important properties of its asymptotic distribution. This allowed
us to construct three diﬀerent hypothesis tests; a permutation test based on the estimated dHSIC
(Deﬁnition 3.3), a bootstrap test based on the estimated dHSIC (Deﬁnition 3.5) and a test based
on a Gamma approximation (Deﬁnition 3.10).

We were able to prove several properties for these tests. First and foremost we proved that the
bootstrap test achieves pointwise asymptotic level (Theorem 3.6) and is consistent (Theorem 3.7)
for detecting any ﬁxed alternative with asymptotic power equal to one. We are not aware of any
other non-parametric independence test for d ≥ 2 variables with these properties. Moreover, we
proved that the permutation test achieves valid level in Proposition 3.4. In particular, we also
showed that this property carries over to the Monte-Carlo approximated version of the permutation
test. Using the theory on V-statistics, we were able to compute the mean and variance of the
estimator of dHSIC in Proposition 3.8 and Proposition 3.9, hence extending the heuristic used for
the Gamma approximation based test to d ≥ 2 variables. Although this test has no guarantees
on level and consistency, it is computationally very fast and therefore of particular interest for
practical applications.

We considered various simulations illustrating the advantages of dHSIC over a multiple testing
based version of a pairwise approach with HSIC. Notably, dHSIC is computationally less expensive
and the pairwise approach with Monte-Carlo approximation (for ﬁxed B) is not able to reject the
null hypothesis for suﬃciently large d. We also outlined applications for model selection in causal
inference which are based on joint independence of error terms in structural equation models,
using simulated and real data. Our experiments for causal inference showed that both on real and
simulated data dHSIC outperforms the multiple testing approach.

21

A Further results on V-statistics
In this section we extend some of the commonly known results about U-statistics to V-statistics.
An overview of the theory of U-statistics can be found in Serﬂing (1980). Most similar generaliza-
tions in literature only apply to V-statistics of degree 2; we now extend the results to V-statistics
of arbitrary order. Throughout this section we will heavily use the following setting.
Setting 2 (U- and V-statistics)
Let n ∈ N, q ∈ {1, . . . , n}, X a metric space, (Ω,F, P) a probability space, X : Ω → X a random
variable with law PX and (Xi)i∈N a sequence of iid copies of X, i.e., (Xi)i∈N iid∼ PX.
The sequence (Xi)i∈N should be seen as the generating process of observations.
elements on {1, . . . , n} by

For completeness, we now introduce the U-statistic. Deﬁne the set of all combinations of q

Cq(n) := {(i1, . . . , iq) ∈ {1, . . . , n} : i1 < ··· < iq} .

The U-statistic

Un(g) :=

estimates the statistical functional

θg := θg

(cid:18)n
(cid:19)−1 (cid:88)
(cid:0)PX(cid:1) := E (g(X1, . . . , Xq)) ,

g(Xi1, . . . , Xiq ),

Cq(n)

q

(A.1)

see (2.2) for the corresponding V-statistic. An alternative representation which also appears in
literature is given by setting

(cid:88)

Pq(n)

U∗
n(g) :=

1

(n)q

g(Xi1 , . . . , Xiq ),

with Pq(n) := {(i1, . . . , iq) ∈ {1, . . . , n} : i1, . . . , iq distinct} and (n)q := n!
straightforward to see that U∗

n(g) = Un(g).

(n−q)! = |Pq(n)|. It is

A.1 Connection between U-statistics and V-statistics
To derive the asymptotic distribution of V-statistics we show that V-statistics are in an appro-
priate sense good approximations of U-statistics. In order to show results of this type we require
some kind of boundedness condition on the core function. The next deﬁnition introduces such a
condition.
Deﬁnition A.1 (total boundedness condition)
Let r ∈ N, assume Setting 2 and let g ∈ Lr((PX )⊗q,|·|R) be a core function. Then we say that g
satisﬁes the total boundedness condition of order r if for all (i1, . . . , iq) ∈ Mq(q) it holds that

E(cid:2)|g(Xi1, . . . , Xiq )|r(cid:3) < ∞.

In particular, this condition is fulﬁlled if the core function g is a bounded function.

The following result is due to Serﬂing (1980, Lemma, Section 5.7.3).

Lemma A.2 (connection between U- and V-statistics)
Let r ∈ N, assume Setting 2 and let g ∈ Lr((PX )⊗q,|·|R) be a core function satisfying the total
boundedness condition of order r. Then it holds that

E [|Un(g) − Vn(g)|r] = O(cid:0)n−r(cid:1)

as n → ∞.

22

In order to prove some of the asymptotic statements of V-statistics we require a stronger way
of comparing V-statistics with U-statistics than that given in Lemma A.2. For example, when
computing the asymptotic variance of a V-statistic up to an order of n−2 by comparison with the
variance of a U-statistic, we need to estimate the second moment of the diﬀerence to an order of
n−(2+ε). Hence, the result in Lemma A.2 is not suﬃcient. The following technical lemma gives a
decomposition of a V-statistic into the corresponding U-statistic and some remainder terms. We
are not aware of a similar result in literature.
Lemma A.3 (decomposition of a V-statistic)
Assume Setting 2 and let g ∈ L1((PX )⊗q,|·|R) be a core function. For all k ∈ {1, . . . , q − 1},
l ∈ {k + 1, . . . , q} let πkl : {1, . . . , q} → {1, . . . , q − 1} be the unique surjective functions with the
property that πkl(k) = πkl(l) = 1 and for all i, j ∈ {1, . . . , q} \ {k, l} with i < j it holds that
πkl(i) < πkl(j). Deﬁne for all x1, . . . , xq−1 ∈ X the function

and set B := {(i1, . . . , iq) ∈ Mq(n) | at most q − 2 distinct values}. Then it holds that

g(xπkl(1), . . . , xπkl(q)).

(cid:88)

g(Xi1, . . . , Xiq )

(i1,...,iq)∈B

k=1

l=k+1

q(cid:88)

w(x1, . . . , xq−1) :=

q−1(cid:88)
nVn(g) =(cid:0)1 + O(cid:0)n−1(cid:1)(cid:1) Un(w)
+ O(cid:0)n−1(cid:1)(cid:19)

+(cid:0)1 + O(cid:0)n−1(cid:1)(cid:1) (n − q + 1)!
(cid:18)(cid:18)q
and |B| = O(cid:0)nq−2(cid:1) as n → ∞.
 (cid:88)

Proof We begin by introducing

+ nUn(g)

Un(g)

(cid:19)

Sn =

−

n!

2

1

(n)q−1

g(Xi1 , . . . , Xiq ) − (cid:88)



g(Xi1 , . . . , Xiq )

Mq(n)

Pq(n)

and

A = {(i1, . . . , iq) ∈ Mq(n) | at most q − 1 distinct values}.

Then, observe that A = Mq(n) \ Pq(n) and

A \ B = {(i1, . . . , iq) ∈ Mq(n) | exactly q − 1 distinct values}

(cid:26)(cid:0)iπkl(1), . . . , iπkl(q)

=

(cid:1)(cid:12)(cid:12)(cid:12) (i1, . . . , iq−1) ∈ Pq−1(n),

k ∈ {1, . . . , q − 1}, l ∈ {k + 1, . . . , q}

.

(cid:27)

Therefore, it holds that |A| = nq − (n)q and |A \ B| = q(q−1)

(n)q−1. Using this we get

2

|B| = |A| − |A \ B|

2

(n)q−1

= nq − (n)q − q(q − 1)
= nq − n(n − 1)··· (n − (q − 1)) − q(q − 1)
= O(cid:0)nq−2(cid:1)
= nq − nq +

nq−1 + O(cid:0)nq−2(cid:1) − q(q − 1)

q(q − 1)

2

2

2

n(n − 1)··· (n − (q − 2))

nq−1 + O(cid:0)nq−2(cid:1)

23

as n → ∞. We can now make the following calculation

g(Xi1 , . . . , Xiq ) − (cid:88)

Pq(n)



g(Xi1, . . . , Xiq )

g(Xi1, . . . , Xiq )

 (cid:88)
(cid:88)
(cid:88)

A

Mq(n)

Sn =

=

=

1

(n)q−1

1

(n)q−1

1

(n)q−1

(cid:88)

B

1

(n)q−1

g(Xi1, . . . , Xiq )

(cid:88)

Pq(n)

g(Xi1 , . . . , Xiq )

w(Xi1, . . . , Xiq−1 ) +

(cid:88)

Pq−1(n)
1

= U∗

n(w) +

(n)q−1

B

nVn(g) =

1
nq−1

(cid:88)
g(Xi1, . . . , Xiq ) − 1
(cid:88)
nq−1

g(Xi1 , . . . , Xiq )

Mq(n)

+

1
nq−1

Pq(n)

Finally, we can decompose nVn(g) as follows

g(Xi1 , . . . , Xiq ).

(A.2)

=

(n)q

n(g)

(n)q−1
nq−1 Sn +

(cid:18)
nq−1 U∗
=(cid:0)1 + O(cid:0)n−1(cid:1)(cid:1) Sn +
n − q(q − 1)
(cid:18)(cid:18)q
(cid:19)
+ O(cid:0)n−1(cid:1)(cid:19)
=(cid:0)1 + O(cid:0)n−1(cid:1)(cid:1) Sn −

2

+ O(cid:0)n−1(cid:1)(cid:19)

2

Un(g)

Un(g) + nUn(g).

(A.3)

(cid:3)

Combining (A.2) and (A.3) completes the proof of Lemma A.3.

A.2 Consistency of V-statistics
The following theorem is the counterpart of Serﬂing (1980, Theorem A, Section 5.4) for V-statistics.
The proof is a direct application of Lemma A.2 and Serﬂing (1980, Theorem A, Section 5.4).
Theorem A.4 (consistency of a V-statistic)
Assume Setting 2 and let g ∈ L1((PX )⊗q,|·|R) be a symmetric core function satisfying the total
boundedness condition of order 1. Then

Vn(g)

P−→ θg

as n → ∞.
Proof By Serﬂing (1980, Theorem A, Section 5.4) it holds that

P−→ θg
as n → ∞. Furthermore, by Lemma A.2 we have that

Un(g)

E|Un(g) − Vn(g)| = O(cid:0)n−1(cid:1)

as n → ∞. Since convergence in L1 implies convergence in probability we obtain

P−→ θg
as n → ∞, which completes the proof of Theorem A.4.

Vn(g)

(cid:3)

24

A.3 Variance of V-statistics
In the degenerate setting ξ1 = 0, Lemma A.3 allows us to show that the variance of a V-statistic
is equal to that of a U-statistic up to a certain order of n. Its proof relies on Lemma A.3.
Theorem A.5 (asymptotic variance of a V-statistic)
Assume Setting 2 and let g ∈ L2((PX )⊗q,|·|R) be a bounded core function satisfying ξ1 = 0. Then
it holds that

(cid:18)n

(cid:19)−1(cid:18)q

(cid:19)(cid:18)n − q

(cid:19)

ξ2 + O(cid:16)

n− 5

2

(cid:17)

q

2

q − 2

Var (Vn(g)) =

as n → ∞.

Proof It holds that

Var (Vn(g)) = Var (Vn(˜g)) ,

which implies that without loss of generality we can assume that θg = 0. By Lemma A.3 we get
that

as n → ∞, where Sn = Un(w) + 1
Section 5.2.1) results in

(n)q−1

B g(Xi1, . . . , Xiq ). Applying Serﬂing (1980, Lemma A,

Un(g)

(A.4)

nVn(g) =(cid:0)1 + O(cid:0)n−1(cid:1)(cid:1) Sn

(cid:19)

2

−

(cid:19)

+ O(cid:0)n−1(cid:1) − n

(cid:18)(cid:18)q
(cid:80)
(cid:18)n
(cid:19)
(cid:19)(cid:18)n − q
(cid:19)−1(cid:18)q
Var (Un(w)) = O(cid:0)n−1(cid:1) .

q − 2

2

q

Var (Un(g)) =

ξ2 + O(cid:0)n−3(cid:1)

and

Moreover, using that g is bounded it holds that

(cid:33)

Var

1

(n)q−1

g(Xi1 , . . . , Xiq )

(cid:32)

≤

1
(n)2
q−1
≤
1
(n)2
q−1
≤ C|B|2
q−1

(n)2

E

B

(cid:88)
(cid:32)(cid:12)(cid:12)(cid:12)(cid:88)
(cid:88)
= O(cid:0)n−2(cid:1) .

(i1,...,iq)∈B

B

(cid:12)(cid:12)(cid:12)2(cid:33)
E(cid:16)(cid:12)(cid:12)(cid:12)g(Xi1, . . . , Xjq )g(Xi1 , . . . , Xjq )
(cid:12)(cid:12)(cid:12)(cid:17)

g(Xi1, . . . , Xiq )

(cid:88)

(j1,...,jq)∈B

So combining (A.6) and (A.7) shows that

Var (Sn) = O(cid:0)n−1(cid:1)

and

Cov (Un(g), Sn) ≤ (Var (Un(g)) Var (Sn))

2 = O(cid:16)

1

n− 3

2

(cid:17)

.

25

(A.5)

(A.6)

(A.7)

(A.8)

(A.9)

Finally, use (A.4), (A.5), (A.8) and (A.9) to get

Var (nVn(g)) =(cid:0)1 + O(cid:0)n−1(cid:1)(cid:1)2

2

Var (Sn)

(cid:1) + O(cid:0)n−1(cid:1) − n(cid:1)2
+(cid:0)(cid:0)q
− 2(cid:0)1 + O(cid:0)n−1(cid:1)(cid:1)(cid:0)n +(cid:0)q
= O (1) Var (Sn) +(cid:0)n2 + O (n)(cid:1) Var (Un(g)) + O (n) Cov (Un(g), Sn)
(cid:19)
(cid:19)(cid:18)n − q
(cid:19)−1(cid:18)q
(cid:18)n

(cid:1) + O(cid:0)n−1(cid:1)(cid:1) Cov (Un(g), Sn)
ξ2 + O(cid:16)

Var (Un(g))

n− 1

= n2

(cid:17)

2

.

2

q

2

q − 2

Dividing by n2 completes the proof of Theorem A.5.

(cid:3)

It is possible to get a similar result in the non-degenerate case ξ1 > 0. In that case similar

reasoning as in the proof above can be used to get a formula for the variance of the V-statistic.

A.4 Bias of V-statistics
As a further consequence of Lemma A.3 the bias of a V-statistic can be explicitly expressed up to
order n−2.
Theorem A.6 (bias of a V-statistic)
Assume Setting 2 and let g ∈ L2((PX )⊗q,|·|R) be a core function satisfying the total boundedness
condition of order 2. Then it holds that

(cid:18)q

(cid:19)
E (˜g2(X1, X1)) + O(cid:0)n−2(cid:1)

E (Vn(g) − θg) =

1
n

as n → ∞.

2

Proof We use Lemma A.3 to get that

Moreover, using the total boundedness condition of g we can get a constant C > 0 such that

as n → ∞. Hence, using (A.10),(A.11) and the unbiasedness of U-statistics results in

E (n (Vn(g) − θg)) = E (nVn(˜g)) = θw + O(cid:0)n−1(cid:1) .

We can compute θw by using the symmetry of ˜g to get

θw = E (w(X1, . . . , Xq−1)) =

(cid:18)q

(cid:19)

E (˜g2(X1, X1)) .

2

Finally, combining (A.12) and (A.13) and dividing by m concludes the proof of Theorem A.6. (cid:3)

26

(A.11)

(A.12)

(A.13)

B

−

(n)q−1

(cid:19)

nVn(˜g) =(cid:0)1 + O(cid:0)n−1(cid:1)(cid:1) Un(w)
(cid:88)
+(cid:0)1 + O(cid:0)n−1(cid:1)(cid:1)
− n + O(cid:0)n−1(cid:1)(cid:19)
(cid:18)(cid:18)q
(cid:12)(cid:12)(cid:12) ≤
(cid:88)
= O(cid:0)n−1(cid:1)

(n)q−1
|B|
(n)q−1

˜g(Xi1, . . . , Xiq )

(cid:88)

≤ C

1

1

2

B

B

E(cid:12)(cid:12)(cid:12)

(n)q−1

1

˜g(Xi1 , . . . , Xiq )

(A.10)

Un(˜g).

E(cid:12)(cid:12)˜g(Xi1 , . . . , Xiq )(cid:12)(cid:12)

A.5 Asymptotic distribution of V-statistics
A V-statistic is called degenerate if ξ1 = Var(g1(X1)) = 0 and non-degenerate if ξ1 > 0. In this
section we analyze the asymptotic distribution of

nVn(g) for the non-degenerate case (ξ1 > 0) and

• √
• nVn(g) for a special degenerate case (ξ1 = 0, ξ2 > 0).

In the degenerate case the asymptotic distribution depends on the eigenvalues of a particular
integral operator. In order to avoid repeating its deﬁnition we introduce the following setting.
Setting 3 (degenerate asymptotic)
Let g ∈ L2((PX )⊗q,|·|R) be a core function, let (Zj)j∈N be a sequence of independent standard
L2(PX ,|·|R) and for every x ∈ X it holds that

normal random variables on R, let T˜g2 ∈ L(cid:0)L2(PX ,|·|R)(cid:1) with the property that for every f ∈

(cid:90)

X

(T˜g2(f )) (x) =

˜g2(x, y)f (y) PX (dy)

and let (λj)j∈N be the eigenvalues of T˜g2.
In this section we derive the asymptotic distributions for V-statistics based on the corresponding
results for U-statistics.

A.5.1 Non-degenerate case
The following theorem is the counterpart of Serﬂing (1980, Theorem A, Section 5.5.1) for V-
statistics. The proof is a straightforward application of both Lemma A.2 and Serﬂing (1980,
Theorem A, Section 5.5.1).

Theorem A.7 (asymptotic distribution of a V-statistic (non-degenerate))
Assume Setting 2, let g ∈ L2((PX )⊗q,|·|R) be a core function satisfying the total boundedness
condition of order 2 and assume ξ1 > 0. Then it holds that

√

n (Vn(g) − θg)

d−→ N(cid:0)0, q2ξ1

(cid:1)

as n → ∞.

Proof Since convergence in L2 implies convergence in probability Lemma A.2 in particular shows
that

√

n (Vn(g) − Un(g))

P−→ 0

as n → ∞. Combining this with Serﬂing (1980, Theorem A, Section 5.5.1) and Slutsky’s theorem
we get

√

√

√

n (Vn(g) − θg) =

n (Un(g) − θg) +

n (Vn(g) − Un(g))

d−→ N(cid:0)0, q2ξ1

(cid:1)

as n → ∞ which completes the proof of Theorem A.7.

(cid:3)

A.5.2 Degenerate case
Theorem A.9 is the counterpart of Serﬂing (1980, Theorem, Section 5.5.2) for V-statistics. Similar
statements appear in literature (e.g. Gretton et al., 2007, Theorem 2). However, we are not aware
of a complete proof of the statement. The proof requires the following intermediate result.

27

Lemma A.8 (eigenvalue representation of the bias)
Assume Setting 2 and Setting 3, let g ∈ L2((PX )⊗q,|·|R) be a core function satisfying the total
boundedness condition of order 2, assume 0 = ξ1 < ξ2 and assume ˜g2 is positive deﬁnite and
continuous. Then it holds that

E (g2(X1, X1)) =

λi + θg

∞(cid:88)

i=1

Proof Observe that ˜g2 is a continuous positive deﬁnite kernel. We can therefore apply Mercer’s
theorem (see Ferreira and Menegatto, 2009, Theorem 1.1) to get that for all x, y ∈ supp(PX ) it
holds that

converges uniformly.
expectation due the uniform convergence we get

If we now take expectation and use that we can exchange the sum and

˜g2(x, y) =

i=1

λiϕi(x)ϕi(y)

∞(cid:88)
(cid:32) ∞(cid:88)
λiE(cid:16)|ϕi(X1)|2(cid:17)
∞(cid:88)
∞(cid:88)

i=1

i=1

λi,

=

=

(cid:33)

E (˜g2(X1, X1)) = E

λiϕi(X1)ϕi(X1)

where in the last step we used that (ϕi)i∈N forms an orthonormal basis of L2(PX ,|·|R). The result
follows by noting that g2 ≡ ˜g2 + θg, which completes the proof of Lemma A.8.
(cid:3)

i=1

We are now ready to state and prove the ﬁnal result of this section.

Theorem A.9 (Asymptotic distribution of a V-statistic (degenerate))
Assume Setting 2 and Setting 3, let g ∈ L2((PX )⊗q,|·|R) be a core function satisfying the total
boundedness condition of order 2, assume 0 = ξ1 < ξ2 and assume ˜g2 is positive deﬁnite and
continuous. Then it holds that

n (Vn(g) − θg)

d−→

λiZ 2
i

(cid:18)q

(cid:19) ∞(cid:88)

2

i=1

nVn(˜g) =(cid:0)1 + O(cid:0)n−1(cid:1)(cid:1) Sn
+ O(cid:0)n−1(cid:1)(cid:19)

(cid:19)

−

(cid:18)(cid:18)q
(cid:80)

2

+ nUn(˜g)

as n → ∞.
Proof The idea of the proof is to use Lemma A.3 to get the decomposition

Un(˜g)

(A.14)

as n → ∞, where Sn = Un(w) + 1
B ˜g(Xi1 , . . . , Xiq ) and w is deﬁned as in Lemma A.3.
We then calculate the asymptotic behavior of Sn and use Serﬂing (1980, Theorem, Section 5.5.2)
to conclude.

(n)q−1

Begin by analyzing the asymptotic behavior of Sn. To this end, note that by symmetry of the

core function g it holds that

(cid:18)q

(cid:19)

2

E(˜g2(X1, X1)).

θw = E(w(X1, . . . , Xq−1)) =

28

and together with Lemma A.8 it holds that

θw = E(w(X1, . . . , Xq−1)) =

Combining this with Serﬂing (1980, Theorem A, Section 5.4) it follows that

Un(w)

P−→ θw =

λi

(A.15)

as n → ∞. Next, we use the total boundedness condition of g to get a constant C > 0 such that

E(cid:12)(cid:12)(cid:12)

(cid:88)

1

(n)q−1

B

˜g(Xi1, . . . , Xiq )

E(cid:12)(cid:12)˜g(Xi1 , . . . , Xiq )(cid:12)(cid:12)

(cid:19) ∞(cid:88)

i=1

λi.

2

(cid:18)q

(cid:18)q
(cid:19) ∞(cid:88)
(cid:12)(cid:12)(cid:12) ≤
(cid:88)
= O(cid:0)n−1(cid:1)

(n)q−1
|B|
(n)q−1

≤ C

i=1

2

1

B

as n → ∞. Using that L1 convergence implies convergence in probability we get that

as n → ∞. Finally, combining (A.15) and (A.16) this results in

(cid:88)

A2

1

(n)q−1

˜g(Xi1 , . . . , Xiq )

P−→ 0

(cid:18)q

(cid:19) ∞(cid:88)

2

i=1

λi

P−→

Sn

(A.16)

(A.17)

λi

(A.18)

as n → ∞. Now, by the properties of convergence in probability, (A.17) and Serﬂing (1980,

(cid:18)q

(cid:19) ∞(cid:88)

2

i=1

P−→ 0

(cid:18)q

(cid:19) ∞(cid:88)

2

i=1

Theorem A, Section 5.4) we have(cid:0)1 + O(cid:0)n−1(cid:1)(cid:1) Sn
+ O(cid:0)n−1(cid:1)(cid:19)

(cid:18)(cid:18)q

(cid:19)

and

P−→

(A.19)
as n → ∞. Hence, (A.14), (A.18) and (A.19) together with Slutsky’s theorem and Serﬂing (1980,
Theorem, Section 5.5.2) shows that

Un(˜g)

2

d−→
as n → ∞, which completes the proof of Theorem A.9.

n (Vn(g) − θg) = nVn(˜g)

λiZ 2
i

(cid:3)

A.6 Resampling results for U-statistics and V-statistics
In this section we want to consider what happens to the asymptotic behavior of nUn(g) and nVn(g)
if instead of the original data sequence (Xi)i∈N we consider a sequence of resampled data. The
diﬀerences are quite subtle, therefore one needs to be very precise about what resampling means.
Throughout this section we use the following setting.
Setting 4 (resampling)
Let X be a separable metric space, let (Ω,F, P) be a probability space, let X : Ω → X be a random
variable and let (Xi)i∈N be a sequence of iid copies of X. For all n ∈ N, let (Ωn,Fn, Pn) be
d→ X as n → ∞
probability spaces, let X∗
n)) = E(f (X)) for all bounded and continuous functions f : X → R) and let
(i.e. limn→∞ En(f (X∗
(X∗

n : Ωn → X be random variables satisfying that X∗

n,i)i∈{1,...,n} be iid copies of X∗
n.

n

29

n,1, . . . , X∗

The data X∗
n,n should be interpreted as a new sample drawn from a distribution which
converges to PX as n goes to inﬁnity. Resampled data of this type often show up in diﬀerent
types of bootstrapping or permutation techniques. We are interested in ﬁnding properties of the
resampled U-statistc

˜Un(g) :=

and the resampled V-statistic

˜Vn(g) :=

(cid:18)n

q

(cid:16)

(cid:19)−1 (cid:88)
(cid:88)

Cq(n)

g

(cid:16)

1
mq

Mq(n)

X∗

n,i1

g

(cid:17)

X∗

n,i1

, . . . , X∗

n,iq

(cid:17)

.

, . . . , X∗

n,iq

The diﬀerence compared to the normal U-and V-statistic is that the distribution of the sample
X∗
n,1, . . . , X∗
n,n depends on m. Therefore, the results of the previous sections only carry over to
the resampled U-and V-statistics if they are results for which m is kept ﬁxed. Results about the
asymptotic behavior of the resampled U-and V-statistics need to be proved separately. A further
more technical diﬃculty is that for diﬀerent m the random variables ˜Un(g) and ˜Vn(g) are no longer
deﬁned on the same probability space. The following theorem gives us a way of dealing with this
issue and is a slightly modiﬁed version of Skorohod’s theorem (see Billingsley, 2008, Theorem 6.7).

Theorem A.10 (Skorohod’s theorem)
Assume Setting 4. Then there exists a common probability space ( ˜Ω, ˜F, ˜P) and random variables
n,i)i∈{1,...,n}, n ∈ N and ( ˜Xi)i∈N on this probability space satisfying
( ˜X∗
(i) for all n ∈ N, for all i ∈ {1, . . . , n}: ˜X∗
(ii) for all i ∈ N: ˜Xi ∼ PX and,
(iii) ˜X∗

˜P-a.s.−→ ˜Xi as n → ∞.

n,i ∼ PX∗
n,

n,i

In order to avoid ambiguity between the resampled and the original sample we introduce the

following notation
(i) for all n ∈ N and all c ∈ {1, . . . , n} deﬁne

c (x1, . . . , xc) := E(g(x1, . . . , xc, X∗
gn

n,c+1, . . . , X∗

n,q)),

(ii) for all n ∈ N deﬁne

g := E(g(X∗
θn

n,1, . . . , X∗

n,q)),

(iii) for all n ∈ N and all c ∈ {1, . . . , n} deﬁne
c (g) := E((gn
ξn

c (X∗

n,1, . . . , X∗

n,c) − θn

g )2).

The following theorem shows that ˜Un(g) is also consistent with θg in the appropriate sense.

Lemma A.11 (consistency of a resampling U-statistic)
Assume Setting 4 and let g ∈ L1((PX )⊗q,|·|R) be a continuous, bounded core function. Then it
holds that

˜Un(g)

d−→ θg

as n → ∞.

30

Proof Applying Theorem A.10 results in a probability space ( ˜Ω, ˜F, ˜P) and random variables
n,i)i∈{1,...,n}, n ∈ N and ( ˜Xi)i∈N with properties speciﬁed in Theorem A.10. Next, introduce
( ˜X∗
the resampled U-statistic

which has the same distribution under ˜P as ˜Un(g) under Pn and the U-statistic

˜Un(g) :=

g( ˜X∗

n,i1

, . . . , ˜X∗

n,iq

),

Un(g) :=

g( ˜Xi1, . . . , ˜Xiq ),

q

Cq(n)

(cid:18)n
(cid:19)−1 (cid:88)
(cid:19)−1 (cid:88)
(cid:18)n
(cid:19)−1 (cid:88)
(cid:16)
(cid:19)−1 (cid:88)

g( ˜X∗

Cq(n)

q

Cq(n)

(cid:18)n
(cid:18)n

q

which has the same distribution under ˜P as Un(g) under P. It holds that

, . . . , ˜X∗

n,iq

) − g( ˜Xi1 , . . . , ˜Xiq )

n,i1

(cid:17)

˜Un(g) − Un(g) =

=

where w(x1, . . . , xq) := g(x2
all c ∈ {1, . . . , q} the functions

c (x1, . . . , xc) := E(cid:16)

wn

q

Cq(n)

1, . . . , x2

q) − g(x1

w(( ˜Xi1, ˜X∗

n,i1

), . . . , ( ˜Xiq , ˜X∗

n,iq

)),

(A.20)

1, . . . , x1

q) is a symmetric core function. If we deﬁne for

g(x2

1, . . . , x2

c, ˜X∗

n,c+1, . . . , ˜X∗

n,q) − g(x1

1, . . . , x1

c, ˜Xc+1, . . . , ˜Xq)

(cid:17)

it holds by the boundedness of g that there exists a constant C ∈ R such that

ξn
c (w) < C.

sup
n∈N

(A.21)

By (A.20), it holds that for ﬁxed n we can apply Serﬂing (1980, Lemma A, Section 5.2.1) and
together with (A.21) to get

(cid:16) ˜Un(g) − Un(g)

(cid:17)

=

Var

(cid:18)n

(cid:19)−1 n(cid:88)

(cid:18)q

(cid:19)(cid:18)n − q

(cid:19)

q − c

c (w) = O(cid:0)n−1(cid:1) .

ξn

(A.22)

c
For (i1, . . . , iq) ∈ Cq(n) it holds by continuity of g that

c=1

q

as n → ∞ and since g is also bounded the dominated convergence theorem in particular implies
that

= 0.

(A.23)

g( ˜X∗

n,i1

, . . . , ˜X∗

n,iq

, . . . , ˜X∗
n,iq
Combining (A.22) and (A.23) hence proves that

lim
n→∞

n,i1

E(cid:16)
(cid:18)(cid:16) ˜Un(g) − Un(g)
(cid:17)2(cid:19)

g( ˜X∗

E

lim
n→∞

)

˜P-a.s.−→ g( ˜Xi1, . . . , ˜Xiq )
(cid:17)
) − g( ˜Xi1, . . . , ˜Xiq )
E(cid:16) ˜Un(g) − Un(g)
(cid:17)2
E(cid:16)

, . . . , ˜X∗

g( ˜X∗

n,i1

n,iq

(cid:17)2

) − g( ˜Xi1 , . . . , ˜Xiq )

= lim
n→∞

= lim
n→∞

= 0.

Using that convergence in second moment implies convergence in probability we have therefore
shown that

˜Un(g) − Un(g)

P−→ 0

31

as n → ∞. Together with consistency of U-statistics (see Serﬂing, 1980, Theorem A, Section 5.4)
it follows that

˜Un(g) − θg = ( ˜Un(g) − Un(g)) − (θg − Un(g))

P−→ 0

as n → ∞. This concludes the proof of Lemma A.11.

(cid:3)

The following two theorems are extensions of results due to Leucht and Neumann (2009) that
show that U-and V-statistics based on resampled data keep their respective asymptotic distribu-
tions. In Leucht and Neumann (2009) only U-and V-statistics of order 2 (i.e. q = 2) are considered.
We adopted the proofs to work for arbitrary order.
Theorem A.12 (asymptotic distribution of degenerate a resampling U-statistic)
Assume Setting 4 and Setting 3, let g ∈ L2((PX )⊗q,|·|R) be a continuous, bounded core function.
Moreover, assume
(i) for all n ∈ N that gn
(ii) g1 ≡ 0 (which implies ξ1(g) = 0) and
(iii) θg = 0.
Then if ξ2(g) > 0 it holds that

1 ≡ 0,

n ˜Un(g)
as n → ∞ and if ξ2(g) = 0 it holds that

d−→

(cid:18)q

(cid:19) ∞(cid:88)

2

i=1

λi(Z 2

i − 1)

n ˜Un(g)

d−→ 0

as n → ∞.
Proof Applying Theorem A.10 results in a probability space ( ˜Ω, ˜F, ˜P) and random variables
n,i)i∈{1,...,n}, n ∈ N and ( ˜Xi)i∈N with properties speciﬁed in Theorem A.10. For (i1, . . . , iq) ∈
( ˜X∗
Cq(n) it holds by continuity of g that

g( ˜X∗

n,i1

, . . . , ˜X∗

n,iq

)

˜P-a.s.−→ g( ˜Xi1, . . . , ˜Xiq )

as n → ∞ and since g is also bounded the dominated convergence theorem in particular implies

E

lim
n→∞

g( ˜X∗

n,i1

, . . . , ˜X∗

n,iq

) − g( ˜Xi1, . . . , ˜Xiq )

= 0.

(A.24)

(cid:17)2(cid:19)

(cid:18)(cid:16)

w(( ˜Xi1, ˜X∗

n,i1

), . . . , ( ˜Xiq , ˜X∗

n,iq

)),

(A.25)

Next, introduce the resampling U-statistic

˜Un(g) :=

g( ˜X∗

n,i1

, . . . , ˜X∗

n,iq

),

which has the same distribution under ˜P as ˜Un(g) under Pn and the U-statistic

Un(g) :=

g( ˜Xi1, . . . , ˜Xiq ),

which has the same distribution under ˜P as Un(g) under P. It holds that

(cid:17)
) − g( ˜Xi1 , . . . , ˜Xiq )

, . . . , ˜X∗

n,iq

n,i1

˜Un(g) − Un(g) =

=

q

Cq(n)

(cid:18)n
(cid:19)−1 (cid:88)
(cid:19)−1 (cid:88)
(cid:18)n
(cid:19)−1 (cid:88)
(cid:16)
(cid:19)−1 (cid:88)

g( ˜X∗

Cq(n)

q

Cq(n)

(cid:18)n
(cid:18)n

q

q

Cq(n)

32

1, . . . , x2

q) − g(x1

1, . . . , x1

q) is a symmetric core function. Deﬁne for all

where w(x1, . . . , xq) := g(x2
c ∈ {1, . . . , q} the functions

c (x1, . . . , xc) := E(cid:16)

wn

and the functions

1, . . . , x2

g(x2

c (w) := E(cid:16)

ξn

c, ˜X∗

n,c+1, . . . , ˜X∗

n,q) − g(x1

1, . . . , x1

c, ˜Xc+1, . . . , ˜Xq)

c (( ˜X1, ˜X∗
wn

n,1), . . . , ( ˜Xc, ˜X∗

n,c))2(cid:17)

.

Then, it holds by the boundedness of g that there exists a constant C ∈ R such that

Moreover, it holds by assumption (i) and (ii) that

ξn
c (w) < C.

sup
n∈N

n,2, . . . , ˜X∗

n,q) − g(x1

1, ˜X2, . . . , ˜Xq)

(cid:17)

1 (x1) = E(cid:16)

wn

1, ˜X∗
g(x2
1) − g1(x1
1)

1 (x2

= gn
= 0,

ξn

1 (w) = E(cid:16)
2 (w) = E(cid:16)
≤ E(cid:16)
(cid:18)(cid:16)

2 (( ˜X1, ˜X∗
wn
w(( ˜X1, ˜X∗
g( ˜X∗

= E

ξn

1 (( ˜X1, ˜X∗
wn

= 0.

n,1))2(cid:17)
n,2))2(cid:17)
n,q))2(cid:17)

n,1), ( ˜X2, ˜X∗
n,1), . . . , ( ˜Xq, ˜X∗

which immediately implies that

Furthermore, by Jensen’s inequality it holds that

(cid:17)

(A.26)

(A.27)

(cid:17)2(cid:19)

.

(A.28)

2 (w) + O(cid:0)n−3(cid:1) .

(A.29)

By (A.25), it holds for ﬁxed n that we can apply the variance formula for a U-statistic (see Serﬂing,
1980, Lemma A, Section 5.2.1) and together with (A.26) and (A.27) we get

(cid:16) ˜Un(g) − Un(g)
(cid:17)

Var

(cid:18)n

(cid:19)−1 n(cid:88)

c=1

=

q

Hence, together with (A.28) and (A.24) it holds that

and consequently also that

lim
n→∞ Var

c

ξn

n,q) − g( ˜X1, . . . , ˜Xq)
n,1, . . . , ˜X∗
(cid:19)

c (w) = O(cid:0)n−2(cid:1) ξn

q − c

(cid:19)(cid:18)n − q
(cid:18)q
(cid:17)(cid:17)
(cid:16)Un(g) − ˜Un(g)
(cid:16)
(cid:17) ˜P−→ 0
(cid:16)Un(g) − ˜Un(g)

= 0

n

(A.30)
as n → ∞. Therefore, if ξ2(g) > 0, we can apply Slutsky’s theorem together with the result about
the asymptotic distribution of degenerate U-statistics given in Serﬂing (1980, Theorem, Section
5.5.2) to get that

n

n ˜Un(g) = nUn(g) + n

(cid:16) ˜Un(g) − Un(g)

(cid:17) d−→

(cid:18)q

(cid:19) ∞(cid:88)

2

i=1

λi(Z 2

i − 1)

as n → ∞. If ξ2(g) = 0, we apply the variance formula of U-statistics (see Serﬂing, 1980, Lemma
A, Section 5.2.1) to get that

lim
n→∞ Var(nUn(g)) = 0.

33

Hence, applying Slutsky’s theorem together with (A.30) proves that

n ˜Un(g) = nUn(g) + n

(cid:16) ˜Un(g) − Un(g)

(cid:17) d−→ 0

as n → ∞, which completes the proof of Theorem A.12.

(cid:3)

The same result also holds for V-statistics. The proof uses the same technique as the proof of

Theorem A.9 and reduces the V-statistic back to the U-statistic.
Theorem A.13 (asymptotic distribution of degenerate resampling V-statistic)
Assume Setting 4 and Setting 3, let g ∈ L2((PX )⊗q,|·|R) be a continuous, bounded core function.
Moreover, assume
(i) for all n ∈ N that gn
(ii) g1 ≡ 0 (which implies ξ1(g) = 0) and
(iii) θg = 0.
Then if ξ2(g) > 0 it holds that

1 ≡ 0,

as n → ∞ and if ξ2(g) = 0 it holds that

n ˜Vn(g)

d−→

λiZ 2
i

(cid:18)q
(cid:19) ∞(cid:88)
(cid:19) ∞(cid:88)
(cid:18)q

i=1

2

2

i=1

n ˜Vn(g)

d−→

λi

as n → ∞.
Proof Applying Theorem A.10 results in a probability space ( ˜Ω, ˜F, ˜P) and random variables
n,i)i∈{1,...,n}, n ∈ N and ( ˜Xi)i∈N with properties speciﬁed in Theorem A.10. Next, introduce
( ˜X∗
the resampling U-statistic

˜Un(g) :=

g( ˜X∗

n,i1

, . . . , ˜X∗

n,iq

),

which has the same distribution under ˜P as ˜Un(g) under Pn and the resampling V-statistic

(cid:18)n
(cid:19)−1 (cid:88)
(cid:88)

q

Cq(n)

Mq(n)

which has the same distribution under ˜P as ˜Vn(g) under Pn. For ﬁxed n ∈ N we can view ˜Vn(g)
as a V-statistic and apply an adjusted version of Lemma A.3 to get

˜Vn(g) :=

1
nq

g( ˜X∗

n,i1

, . . . , ˜X∗

n,iq

),

n ˜Vn(˜g) =(cid:0)1 + O(cid:0)n−1(cid:1)(cid:1) Sn +
(cid:80)
B ˜g( ˜X∗

(cid:18)

n −

(cid:18)q

(cid:19)

2

+ O(cid:0)n−1(cid:1)(cid:19)

θw = E (w(X1, . . . , Xq−1)) =

E (˜g2(X1, X1)) .

(cid:18)q
(cid:19)
(cid:19) ∞(cid:88)
(cid:18)q

2

2

i=1

˜Un(w)

d−→ θw =

34

as n → ∞, where Sn = ˜Un(w) + 1
function g and the deﬁnition of w given in Lemma A.3 it holds that

, . . . , ˜X∗

(n)q−1

n,i1

n,iq

). By the symmetry of the core

˜Un(˜g)

(A.31)

The consistency of resampled U-statistics given in Lemma A.11 together with Lemma A.8 imply
that

λi

(A.32)

as n → ∞. The boundedness of g combined with the size of the set B given in Lemma A.3 shows
that

(cid:88)

1

(n)q−1

B

˜g( ˜X∗

n,i1

, . . . , ˜X∗

n,iq

) ≤ C|B|
(n)q−1

= O(cid:0)n−1(cid:1) .

Moreover, also by Lemma A.11 it holds that

d−→ 0
as n → ∞. By Theorem A.12 it holds if ξ2(g) > 0 that

˜Un(˜g)

n ˜Un(˜g)

d−→

(cid:0)Z 2
i − 1(cid:1)

λi

∞(cid:88)

i=1

as n → ∞ and if ξ2(g) = 0 that

n ˜Un(˜g)

d−→ 0

(A.36)
as n → ∞. Finally, we can combine (A.31), (A.32), (A.33), (A.34), (A.35), (A.36) and use that
convergence in distribution to a constant implies convergence in probability together with Slutsky’s
theorem to get that if ξ2(g) > 0 it holds that
n ˜Vn(˜g)

d−→

λiZ 2
i

(A.33)

(A.34)

(A.35)

∞(cid:88)
(cid:19) ∞(cid:88)
(cid:18)q

i=1

2

i=1

as n → ∞ and if ξ2(g) = 0 it holds that

n ˜Vn(˜g)

d−→

as n → ∞. This concludes the proof of Theorem A.13.

λi

(cid:3)

B Resampling tests
In this section we want to rigorously introduce resampling tests. Most of this section is based on
Lehmann and Romano (2005), we however adjust a lot of the notation to ﬁt our situation.
Let α ∈ (0, 1), let X be a measurable space, let T = (Tn)n∈N be a test statistic on X , let X
be a random variable with values in X and let (Xi)i∈N be a sequence of iid copies. The main idea
behind resampling tests is to construct data sets based on the original observations (X1, . . . , Xn).
These types of constructions are formalized by resampling methods.
Deﬁnition B.1 (resampling method)
Let X be a measurable space and let (Mn)n∈N ⊆ N be a sequence. If

g =(cid:0)(gn,k)k∈{1,...,Mn}(cid:1)

n∈N

is a family of functions satisfying for all n ∈ N and for all k ∈ {1, . . . , Mn} that

gn,k : X n → X n,

then we call g a resampling method.

Based on a resampling method g we can construct new observations for all n ∈ N and for all

k ∈ {1, . . . , Mn} by deﬁning

Zn,k := gn,k(X1, . . . , Xn).

The new ’resampled’ data (Zn,k)k∈{1,...,Mn} ⊆ X n, n ∈ N is called resampling scheme and for
each n ∈ N the sequence Zn,1, . . . , Zn,Mn should be seen as Mn resampled data sets constructed
from the original observations (X1, . . . , Xn). A resampling method is therefore a formalization
of the concept of resampling Mn times from the original observations (X1, . . . , Xn). Based on a
resampling method we can introduce the resampling distribution function.

35

method. For all n ∈ N the functions (cid:98)RTn : X n × R → [0, 1] deﬁned for all (x1, . . . , xn) ∈ X n and

Deﬁnition B.2 (resampling distribution function)
Let X be a measurable space, let T = (Tn)n∈N be a test statistic on X , let g be a resampling
for all t ∈ R by

(cid:98)RTn (x1, . . . , xn)(t) :=

1
Mn

Mn(cid:88)

k=1

1{Tn(gn,k(x1,...,xn))≤t}

are called the resampling distribution functions (corresponding to test statistic T and resampling
method g).

Fixing n ∈ N and (x1, . . . , xn) ∈ X n it holds that

(cid:98)RTn (x1, . . . , xn) : R → [0, 1]

is non-decreasing, right-continuous and satisﬁes

This implies that (cid:98)RTn (x1, . . . , xn) is a distribution function and thus we can deﬁne the generalized

lim

inverse

t→∞(cid:98)RTn (x1, . . . , xn)(t) = 1.

: (0, 1) → R

lim

t→-∞(cid:98)RTn (x1, . . . , xn)(t) = 0 and
(cid:16)(cid:98)RTn (x1, . . . , xn)
(cid:17)−1
(cid:16)(cid:98)RTn (x1, . . . , xn)
(cid:17)−1

satisfying for all α ∈ (0, 1) that

(α) := inf{t ∈ R | (cid:98)RTn (x1, . . . , xn)(t) ≥ α}.

Based on the resampling distribution functions we can deﬁne a resampling test as follows.
Deﬁnition B.3 (resampling test)
Let α ∈ (0, 1), let X be a separable metric space, let T = (Tn)n∈N be a test statistic on X , let
hypothesis test ϕ = (ϕn)n∈N deﬁned for all n ∈ N and for all (x1, . . . , xn) ∈ X n by

g be a resampling method and let (cid:98)RTn be the corresponding resampling distribution functions. A

ϕn(x1, . . . , xn) := 1{Tn(x1,...,xn)>((cid:98)RTn (x1,...,xn))−1(1−α)}

is called α-resampling test (corresponding to g).

The advantage of resampling tests is that they can be constructed for any test statistic. We now
deﬁne an important subclass of resampling methods.
Deﬁnition B.4 (resampling group)
Let X be a measurable space, let (Mn)n∈N ⊆ N be a sequence and let g be a resampling method. If
g satisﬁes that

G := {gn,1, . . . , gn,Mn}

together with concatenation is a group of transformations on X n, then we call g a resampling
group.
Resampling groups have the important property that for all test statistics T = (Tn)n∈N the
corresponding resampling distribution functions satisfy for all n ∈ N, for all k ∈ {1, . . . , Mn} and

for all (x1, . . . , xn) ∈ X n that(cid:98)RTn (x1, . . . , xn) = (cid:98)RTn(gn,k(x1, . . . , xn)).

(B.1)

This follows immediately from the group property of g. It allows us to prove, given an appropriate
invariance of the resampling group under the null hypothesis, that the corresponding resampling
test achieves level α. The following theorem is a reformulation of Lehmann and Romano (2005,
Theorem 15.2.1).

36

Theorem B.5 (level of resampling tests)
Let α ∈ (0, 1), let X be a separable metric space, let H0, HA ⊆ P(X ) be a null and alternative
hypothesis respectively, let g be a resampling group satisfying under H0 that for all n ∈ N and for
all k ∈ {1, . . . , Mn} it holds that

gn,k(X1, . . . , Xn) is equal in distribution to (X1, . . . , Xn).

Then, the α-resampling test ϕ corresponding to g is a test at level α, when testing H0 against HA.
Proof Fix n ∈ N and let K be a uniformly distributed random variable on {1, . . . , Mn} inde-
pendent of (X1, . . . , Xn). Let (x1, . . . , xn) ∈ Im((X1, . . . , Xn)) and for all k ∈ {1, . . . , Mn} deﬁne

zn,k := gn,k(x1, . . . , xn) then it holds that Tn(zn,K) has the distribution function (cid:98)RTn(x1, . . . , xn).

Hence, using (B.1) and the the properties of the generalized inverse it holds that

Mn(cid:88)

k=1

1
Mn

=

1
Mn

= E(cid:16)

≤ α,

1{Tn(zn,k)>((cid:98)RTn (zn,k))−1(1−α)}
Mn(cid:88)
1{Tn(zn,K )>((cid:98)RTn (x1,...,xn))−1(1−α)}

1{Tn(zn,k)>((cid:98)RTn (x1,...,xn))−1(1−α)}
(cid:17)

k=1

which together with the monotonicity of the integral and the convention

implies that

(cid:32)

E

Mn(cid:88)

k=1

1
Mn

Zn,k = gn,k(X1, . . . , Xn)

1{Tn(Zn,k)>((cid:98)RTn (Zn,k))−1(1−α)}

(cid:33)

≤ α.

(B.2)

Moreover, under H0, i.e. X1, X2, . . . iid∼ PX ∈ H0, it holds by assumption for all k ∈ {1, . . . , Mn}
that (X1, . . . , Xn) is equal in distribution to Zn,k. This in particular implies that under H0 it
holds for all k ∈ {1, . . . , Mn} that

E (ϕn(Zn,k)) = E (ϕn(X1, . . . , Xn)) .

(B.3)

Combining (B.2) and (B.3) results in

P (ϕn(X1, . . . , Xn) = 1) = E (ϕn(X1, . . . , Xn))

Mn(cid:88)

k=1

1
Mn

Mn(cid:88)

k=1

=

1
Mn

(cid:32)

= E
≤ α,

E (ϕn(Zn,k))

1{Tn(Zn,k)>((cid:98)RTn (Zn,k))−1(1−α)}

(cid:33)

which completes the proof of Theorem B.5.

(cid:3)

The invariance assumption of the resampling group in the previous theorem is the same as
the randomization hypothesis given by Lehmann and Romano (2005, Deﬁnition 15.2.1). Unfor-
tunately, there are no similar guarantees that an arbitrary resampling test controls the type II
error in any way. Results of this type need to be checked on a case by case basis by analyzing the
resampling distribution function for the speciﬁc test statistic.

37

B.1 Monte-Carlo approximated resampling tests
Finally, we want to discuss a computational diﬃculty that often arises in the context of resampling
tests. The problem is that in practical applications the parameter Mn from the deﬁnition of a
resampling method grows very fast in n and makes computations impossible for large n. One

method of dealing with this is to approximate the resampling distribution (cid:98)Rn using a Monte-

Carlo approximated version.
Deﬁnition B.6 (Monte-Carlo approximated resampling distribution)
Let X be a measurable space, let T = (Tn)n∈N be a test statistic on X , let g be a resampling
method and let (Ki)i∈N be a sequence of independent uniformly distributed random variables on
: X n×R → [0, 1] be the functions deﬁned for all (x1, . . . , xn) ∈

{1, . . . , Mn}. For all B ∈ N let (cid:98)RB
X n and for all t ∈ R by(cid:98)RB
The following proposition shows that (cid:98)RB
let (cid:98)RTn be the resampling distribution functions and for all B ∈ N let (cid:98)RB
(x1, . . . , xn)(t) = (cid:98)RTn (x1, . . . , xn)(t).

Proposition B.7 (Monte-Carlo approximation of resampling distribution)
Let X be a measurable space, let T = (Tn)n∈N be a test statistic on X , let g be a resampling method,
be the Monte-Carlo
approximated resampling distribution functions. Then for all (x1, . . . , xn) ∈ X n and for all t ∈ R
it holds P-a.s. that

are called the Monte-Carlo approximated resampling distribution functions (corresponding to test
statistic T and resampling method g).

approximates (cid:98)RTn in an appropriate way.

1{Tn(gn,Ki (x1,...,xn))≤t}

(x1, . . . , xn)(t) :=

B(cid:88)

1
B

i=1

Tn

Tn

Tn

Tn

B→∞ (cid:98)RB

lim

Tn

from the deﬁnition of (cid:98)RB

Proof Let (Ki)i∈N be the sequence of uniformly distributed random variables on {1, . . . , Mn}
, then introduce for all k ∈ {1, . . . , Mn} and for all i ∈ N the random

Tn

variables

Y k
i

:= 1{Ki=k}.

i has a Bernoulli distribution with parameter
Y k

1
Mn

. Furthermore, we can write

(cid:98)RB

Tn

(x1, . . . , xn)(t) =

1{Tn(gn,Ki (x1,...,xn))≤t}

i=1

1
B

B(cid:88)
(cid:80)B
Mn(cid:88)
Mn(cid:88)

k=1

k=1

=

1{Tn(gn,k(x1,...,xn))≤t}.

i=1 Y k
i
B

By the strong law of large numbers this implies that P-a.s. it holds that

B→∞ (cid:98)RB

lim

Tn

(x1, . . . , xn)(t) =

1
Mn

1{Tn(gn,k(x1,...,xn))≤t} = (cid:98)RTn (x1, . . . , xn)(t),

which completes the proof of Proposition B.7.

(cid:3)

We are now ready to deﬁne Monte-Carlo approximated resampling test.

Instead of using
the (1 − α)-quantile of the Monte-Carlo approximated resampling distribution we use a slightly
larger critical value. Surprisingly, for resampling groups satisfying the invariance condition in
Theorem B.5, this allows us to achieve level α for any value of B. The trick is that slightly larger
critical value accounts for the uncertainty due to the Monte-Carlo approximation.

We deﬁne the test using the p-value as this leads to easier calculations. The corresponding crit-
ical value can then be calculated via the standard correspondence between p-value and hypothesis
test.

38

Deﬁnition B.8 (Monte-Carlo approximated resampling test)
Let α ∈ (0, 1), let X be a separable metric space, let T = (Tn)n∈N be a test statistic on X , let g
be a resampling method, let B ∈ N, let (Ki)i∈N be a sequence of independent uniformly distributed
random variables on {1, . . . , Mn} and let (k1, . . . , kB) be a realization of (K1, . . . , KB). For all

n ∈ N deﬁne the function (cid:98)pn : X n → [ 1

1 +(cid:12)(cid:12){i ∈ {1, . . . , B} : Tn(gn,ki(x1, . . . , xn)) ≥ Tn(x1, . . . , xn)}(cid:12)(cid:12)

B+1 , 1] satisfying

(cid:98)pn(x1, . . . , xn) :=

Then the hypothesis test ϕ = (ϕn)n∈N deﬁned for all n ∈ N and for all (x1, . . . , xn) ∈ X n by

1 + B

.

ϕn(x1, . . . , xn) := 1{(cid:98)pn(x1,...,xn)≤α},

is called α-Monte-Carlo approximated resampling test.

The function (cid:98)pn is called p-value of the test ϕn. The following proposition shows that the

Monte-Carlo approximated resampling test achieves level α given the appropriate invariance as-
sumptions on g.
Proposition B.9 (Monte-Carlo approximated resampling test has valid level)
Let α ∈ (0, 1), let X be a separable metric space, let H0, HA ⊆ P(X ) be a null and alternative
hypothesis respectively, let T = (Tn)n∈N be a test statistic on X , let B ∈ N and let g be a resampling
group satisfying under H0 that for all n ∈ N and for all k ∈ {1, . . . , Mn} it holds that

gn,k(X1, . . . , Xn) is equal in distribution to (X1, . . . , Xn),

and for all k, l ∈ {1, . . . , Mn} it holds that

P (Tn(gn,k(X1, . . . , Xn)) = Tn(gn,l(X1, . . . , Xn))) = 0.

(B.4)

(B.5)

Then, the corresponding α-Monte-Carlo approximated resampling test ϕ = (ϕn)n∈N has valid level
α HA.
Proof Begin by deﬁning the function f : {1, . . . , Mn}B × X n → {0, . . . , B} satisfying for all
(k1, . . . , kB) ∈ {1, . . . , Mn}B and for all (x1, . . . , xn) ∈ X n that

f (k1, . . . , kB)(x1, . . . , xn) :=(cid:12)(cid:12){i ∈ {1, . . . , B} : Tn(gn,ki(x1, . . . , xn)) ≥ Tn(x1, . . . , xn)}(cid:12)(cid:12),
ftot(x1, . . . , xn) :=(cid:12)(cid:12){i ∈ {1, . . . , Mn} : Tn(gn,i(x1, . . . , xn)) ≥ Tn(x1, . . . , xn)}(cid:12)(cid:12).

and the function ftot : X n → {1, . . . , Mn} satisfying for all (x1, . . . , xn) ∈ X n that

Then, by the invariance assumption (B.4) it holds under H0 for all k, l ∈ {1, . . . , Mn} that

P (ftot(X1, . . . , Xn) = l) = P (ftot(gn,k(X1, . . . , Xn)) = l) .

(B.6)

Moreover, since g is a group it holds P-a.s. that

ftot(gn,k(X1, . . . , Xn)) =

=

1{Tn(gn,i(gn,k(X1,...,Xn)))≥Tn(gn,k(X1,...,Xn))}

1{Tn(gn,i(X1,...,Xn))≥Tn(gn,k(X1,...,Xn))},

which implies together with (B.5) it holds P-a.s. that

Mn(cid:88)
Mn(cid:88)

i=1

i=1

Mn(cid:88)

k=1

39

1{ftot(gn,k(X1,...,Xn))=l} = 1.

(B.7)

Combining (B.6) and (B.7) it holds under H0 that

k=1

P (ftot(gn,k(X1, . . . , Xn)) = l)

Mn(cid:88)
Mn(cid:88)
E(cid:0)1{ftot(gn,k(X1,...,Xn))=l}(cid:1)
(cid:33)
(cid:32) Mn(cid:88)

1{ftot(gn,k(X1,...,Xn))=l}

k=1

E

k=1

P (ftot(X1, . . . , Xn) = l) =

=

=

=

1
Mn

1
Mn

1
Mn

1
Mn

,

which proves that under H0 it holds that ftot(X1, . . . , Xn) is uniformly distributed on {1, . . . , Mn}.
Furthermore, conditioned on ftot(X1, . . . , Xn) = l it holds for all i ∈ {1, . . . , B} that

1{Tn(gn,Ki (X1,...,Xn))≥Tn(X1,...,Xn)}

is Bernoulli

l

Mn

distributed which again conditioned on ftot(X1, . . . , Xn) = l implies that

B(cid:88)

i=1

f (K1, . . . , KB)(X1, . . . , Xn) =

1{Tn(gn,Ki (X1,...,Xn))≥Tn(X1,...,Xn)}

has binomial distribution with parameters B and l
Mn

. It therefore holds under H0 that

P ((cid:98)pn(X1, . . . , Xn) ≤ α)

= P (f (K1, . . . , KB)(X1, . . . , Xn) ≤ (B + 1)α − 1)

P (f (K1, . . . , KB)(X1, . . . , Xn) ≤ (B + 1)α − 1| ftot(X1, . . . , Xn) = l)

Mn(cid:88)

=

=

≤

=
≤ α,

· P (ftot(X1, . . . , Xn) = l)

(cid:19)(cid:18) l

(cid:19)i(cid:18)

(cid:19)B−i

1 − l
Mn

Mn

(x)i (1 − x)B−i λ(dx)

l=1

1
Mn

(cid:90) 1

Mn(cid:88)
(cid:98)(B+1)α−1(cid:99)(cid:88)

(cid:18)B
(cid:98)(B+1)α−1(cid:99)(cid:88)
(cid:18)B
(cid:19)

i=0

l=1

i

0

i
(cid:98)(B + 1)α − 1(cid:99) + 1

i=0

B + 1

where we approximated the sum by an integral and solved the integral using integration by parts.
(cid:3)
This completes the proof of Proposition B.9.

The p-value is underestimated by the choice we made. In fact, as described in Phipson and
Smyth (2010), the level of the test would be preserved even if we chose the p-value slightly
larger. This allows to construct a permutation test which is not only valid in level but actually
achieves exact level. The next proposition speciﬁes the critical value that leads to the Monte-Carlo
approximated resampling test.
Proposition B.10 (critical value of Monte-Carlo approximated resampling test)
Let α ∈ (0, 1), let X be a measurable space, let T = (Tn)n∈N be a test statistic on X , let g be a
resampling method, let B ∈ N, let (Ki)i∈N be a sequence of uniformly distributed random variables

40

on {1, . . . , Mn} and let (k1, . . . , kB) be a realization of (K1, . . . , KB). For all n ∈ N deﬁne the
function cn : X n → R satisfying that cn(x1, . . . , xn) is the

(cid:100)(B + 1)(1 − α)(cid:101) +

1{Tn(gn,ki (x1,...,xn))=Tn(x1,...,xn)}-th largest value

in the vector (Tn(gn,k1(x1, . . . , xn)), . . . , Tn(gn,kB (x1, . . . , xn))) if

B(cid:88)

i=1

B(cid:88)

i=1

(cid:100)(B + 1)(1 − α)(cid:101) +

1{Tn(gn,ki (x1,...,xn))=Tn(x1,...,xn)} ≤ B

and ∞ otherwise. Then the hypothesis test ϕ = (ϕn)n∈N deﬁned for all n ∈ N and for all
(x1, . . . , xn) ∈ X n by

ϕ(x1, . . . , xn) := 1{Tn(x1,...,xn)≥cn(x1,...,xn)},

is equal to the α-Monte-Carlo approximated resampling test.

Proof The following calculation is straight forward:

1{(cid:98)pn(x1,...,xn)≤α}
= 1(cid:110) 1
(cid:80)B
= 1(cid:110) B+1
B (1−α)≤ 1

i=1

B

B

1{Tn (gn,ki

(cid:80)B

(x1,...,xn ))≥Tn (x1 ,...,xn)}≤ B+1

B α− 1

B

i=1

1{Tn (gn,ki

(x1,...,xn ))<Tn(x1,...,xn)}

(cid:111)
(cid:111)

= 1{Tn(x1,...,xn)≥cn(x1,...,xn)}
= ϕn(x1, . . . , xn).

This completes the prove of Proposition B.10.

(cid:3)

The Monte-Carlo approximated resampling test is closely related to the Monte-Carlo resam-
pling distribution function. To see this observe that for large B it holds for all (x1, . . . , xn) ∈ X n
that

cn(x1, . . . , xn) ≈ ((cid:98)RB

Tn

(x1, . . . , xn))−1(1 − α).

As mentioned above cn approximates the (1 − α)-quantile of the Monte-Carlo resampling distri-
bution from above and gets closer as B increases.

C Additional proofs
C.1 Proof of Proposition 2.5
Proof Using the deﬁnition of the mean embedding we get

dHSIC =

=

=

(cid:13)(cid:13)(cid:13)Π
(cid:16)P(X 1,...,X d)(cid:17)(cid:13)(cid:13)(cid:13)2
(cid:16)PX 1 ⊗ ··· ⊗ PX d(cid:17) − Π
(cid:13)(cid:13)(cid:13)(cid:13)2
(cid:13)(cid:13)(cid:13)(cid:13) d(cid:89)
1 ,·(cid:17)(cid:17) − E (k (X1,·))
kj(cid:16)
E(cid:16)
1 ,·(cid:17)(cid:17)(cid:13)(cid:13)(cid:13)(cid:13)2
(cid:13)(cid:13)(cid:13)(cid:13) d(cid:89)
(cid:13)(cid:13)(cid:13)(cid:13)E (k (X1,·))
(cid:13)(cid:13)(cid:13)(cid:13)2
kj(cid:16)
E(cid:16)
(cid:43)
(cid:42) d(cid:89)
E(cid:16)
kj(cid:16)
1 ,·(cid:17)(cid:17)

, E (k (X1,·))

− 2

X j

X j

X j

j=1

j=1

+

H

H

H

H

j=1

H

41

(C.1)

Next we simplify each term individually using the properties of the Bochner integral and the
properties of tensor Hilbert spaces.

(cid:13)(cid:13)(cid:13)(cid:13) d(cid:89)

j=1

E(cid:16)

kj(cid:16)

1 ,·(cid:17)(cid:17)(cid:13)(cid:13)(cid:13)(cid:13)2

H

X j

=

=

=

=

= E

j=1

Hj

Hj

2j

=

X j

j=1

j=1

j=1

j=1

X j

X j

Hj

X j

X j

X j

(cid:29)

X j

1 , X j

2

2j−1, X j

E (k (X1,·)) , E (k (X1,·))

1 ,·(cid:17)(cid:17)(cid:13)(cid:13)(cid:13)(cid:13)2
1 ,·(cid:17)(cid:17)(cid:29)
kj(cid:16)
1 ,·(cid:17)(cid:17)
, E(cid:16)
2 ,·(cid:17)(cid:11)
(cid:17)
, kj(cid:16)
1 ,·(cid:17)
(cid:17)(cid:17)
(cid:17)

(cid:13)(cid:13)(cid:13)(cid:13)E(cid:16)
kj(cid:16)
d(cid:89)
(cid:28)
kj(cid:16)
E(cid:16)
d(cid:89)
E(cid:16)(cid:10)kj(cid:16)
d(cid:89)
kj(cid:16)
E(cid:16)
d(cid:89)
 d(cid:89)
kj(cid:16)
(cid:28)
= E(cid:16)(cid:10)k (X1,·) , k (X2,·)(cid:11)
 d(cid:89)
(cid:17)
kj(cid:16)
(cid:28)
 d(cid:89)
j+1,·(cid:17) ,
kj(cid:16)
kj(cid:16)
d(cid:89)
(cid:28) d(cid:89)
1 ,·(cid:17)(cid:29)
kj(cid:16)
kj(cid:16)
j+1,·(cid:17)
d(cid:89)
 d(cid:89)

(cid:10)kj(cid:16)
, kj(cid:16)
j+1,·(cid:17)
1 ,·(cid:17)(cid:11)
(cid:17)
 d(cid:89)
kj(cid:16)

= E (k (X1, X2))

X j

1 , X j

X j

1 , X j

= E

= E

= E

= E

= E

,

j=1

(cid:17)

H

X j

Hj

X j

i=1

X j

X j

X j

j=1

E

j=1

j=1

2

H

j=1

(cid:13)(cid:13)(cid:13)(cid:13)E (k (X1,·))

(cid:13)(cid:13)(cid:13)(cid:13)2

H

(cid:42) d(cid:89)

j=1

E(cid:16)

kj(cid:16)

1 ,·(cid:17)(cid:17)

X j

(cid:43)

, E (k (X1,·))

H

(C.2)

(C.3)



H

(C.4)

(cid:3)

X j

1 ,·(cid:17)(cid:29)


H

j+1

Combining (C.1), (C.2), (C.3) and (C.4) completes the proof of Proposition 2.5.

j=1

C.2 Proof of Lemma 2.7
Proof We prove the 5 properties separately.

• h is symmetric:

This is immediate by construction.

• h is continuous:

This follows from the continuity of the kernels kj, which is assumed in Setting 1.

42

• h is bounded:

Under Setting 1 we assume that all kj’s are bounded. Hence for all j ∈ {1, . . . , d} let C j > 0
such that for all z1, z2 ∈ X it holds that

|kj(z1, z2)| < C j.
Thus it is clear that for all z1, . . . , z2d ∈ X it holds that

|h(z1, . . . , z2d)| < 4

C j =: C.

d(cid:89)

j=1

• (cid:92)dHSIC is a V-statistic with core function h:

Compute directly,

(cid:88)
(cid:88)

M2d(n)

1
n2d

1

(2d)!

π∈S2d

h (X1, . . . , X2p)

(cid:34) d(cid:89)

(cid:88)

1
n2d

M2d(n)

j=1

(cid:34)

(cid:88)

1

(2d)!

π∈S2d

(cid:88)

d(cid:89)

M2(n)

j=1

1
n2

Vn(h) =

=

=

kj(cid:16)
d(cid:89)
kj(cid:16)

− 2

j=1

X j

+

1
n2d

− 2
nd+1

X j

π(i1), X j

π(i2)

kj(cid:16)

π(i1), X j

π(i2)

X j

π(i1), X j

(cid:17)
kj(cid:16)
d(cid:89)
(cid:88)
kj(cid:16)
d(cid:89)
(cid:88)

j=1

M2d(n)

Md+1(n)

j=1

(cid:17)

+

(cid:17)

X j

π(2j−1), X j

π(i2j )

kj(cid:16)
d(cid:89)
(cid:17)(cid:35)

j=1

π(ij+1)

(cid:17)

(cid:17)(cid:35)

X j

π(2j−1), X j

π(i2j )

X j

π(i1), X j

π(ij+1)

= (cid:92)dHSICn .

• θh = dHSIC:

Again computing directly,

E (h(X1, . . . , X2d)) =

(cid:34)

E

(cid:88)

1

(2d)!

π∈S2d

j=1

j=1

 d(cid:89)
kj(cid:16)
 d(cid:89)
kj(cid:16)
 d(cid:89)
kj(cid:16)
 d(cid:89)
kj(cid:16)
 d(cid:89)
kj(cid:16)

j=1

j=1

+ E

− 2E

(cid:34)

E

− 2E

π(2)

π(2j)

(cid:17)
(cid:17)
(cid:17)(cid:35)
 d(cid:89)
(cid:17) + E
(cid:17)(cid:35)

π(j+1)

j=1

X j

π(1), X j

zj
π(2j−1), zj

zj
π(1), zj

X j

1 , X j

2

zj
1, zj

j+1

j=1

43

(cid:88)

1

(2d)!

π∈S2d

=

= dHSIC .

kj(cid:16)

zj
2j−1, zj

2j

(cid:17)

This completes the proof of Lemma 2.7

C.3 Proof of Theorem 3.1
Proof Let t ∈ R and X1, X2, . . . iid∼ PX ∈ HA ﬁxed, then

(cid:17)

P(cid:16)

(cid:18)√

n · (cid:92)dHSICn(X1, . . . , Xn) ≤ t
= P

n( (cid:92)dHSICn(X1, . . . , Xn) − dHSIC) ≤ t√
n

(cid:3)

(C.5)

(cid:19)

n dHSIC

.

− √

Moreover, by Lemma 2.7 it holds that (cid:92)dHSIC is simply the V-statistic Vn(h) with θh = dHSIC.
Additionally, again by Lemma 2.7 it holds that h is bounded and continuous. If ξ1(h) > 0 then
we can apply Theorem A.7 to see that,

(cid:17) d−→ N(cid:0)0, (2d)2ξ1(h)(cid:1)

(C.6)

as n → ∞. Next assume ξ1(h) = 0, then by Theorem A.5 it holds that

= O(cid:0)n−1(cid:1)

(cid:16) (cid:92)dHSIC
(cid:17)
(cid:17) d−→ 0

= n Var

(cid:16) (cid:92)dHSICn − dHSIC

√

as n → ∞ and since convergence in second moment implies convergence in distribution this implies

(C.7)
as n → ∞. Using the corollary of Slutsky’s theorem given in Lehmann and Romano (2005,
Corollary 11.2.3) and combining (C.5) with (C.6) if ξ1(h) > 0 and (C.5) with (C.7) if ξ1(h) = 0
(cid:3)
completes the proof of Corollary 3.1.

n

n

√

(cid:16) (cid:92)dHSICn − dHSIC
(cid:17)2(cid:19)
(cid:16) (cid:92)dHSICn − dHSIC

(cid:18)

E

n

C.4 Proof of Theorem 3.2
Proof Use Lemma 2.7 to observe that (cid:92)dHSICn is simply the V-statistic Vn(h) with θh = dHSIC.
By Lemma D.3 it holds that ξ1(h) = 0 under H0 and moreover, again by Lemma 2.7 it holds that
h is bounded and continuous. If ξ2(h) > 0, we can apply Theorem A.9 to see that,

(cid:19) ∞(cid:88)
(cid:18)2d

2

i=1

λiZ 2
i

(cid:18)2d

(cid:19) ∞(cid:88)

λi.

n · (cid:92)dHSICn

d−→

n · (cid:92)dHSICn

d−→

as n → ∞. If ξ2(h) = 0, we can apply Theorem A.5 to see that limn→∞ Var(n · (cid:92)dHSICn) = 0.
Combining this with Theorem A.6 and Lemma A.8 hence leads to

2
as n → ∞, which completes the proof of Theorem 3.2.

i=1

(cid:3)

C.5 Proof of Proposition 3.4
Proof Fix n ∈ N, under H0, i.e. X1, X2, . . . iid∼ PX ∈ H0, it holds that the individual coordinates
of Xi are independent. Hence, for all ψ ∈ (Sn)d it holds that (X1, . . . , Xn) is equal in distribution
to (Xψ

n ), so in particular, we have that

1 , . . . , Xψ

gn,ψ(X1, . . . , Xn) is equal in distribution to (X1, . . . , Xn).

(C.8)

Moreover since (Sn)d has a group structure we can apply Theorem B.5 to get that ϕ has level α,
(cid:3)
which completes the proof of Proposition 3.4.

44

C.6 Proof of Theorem 3.6
In order to prove Theorem 3.6 we make the following deﬁnition.
Deﬁnition C.1 (empirical product distribution function)
and for all t ∈ Rd that

Assume Setting 1, then the function (cid:98)Fn : X n × Rd → [0, 1] satisfying for all (x1, . . . , xn) ∈ X n

(cid:98)Fn(x1, . . . , xn)(t) :=

(cid:32)

d(cid:89)

n(cid:88)

i=1

1
n

(cid:33)

1{xj

i≤tj}

j=1
is called the empirical product distribution function.

The following lemma will be essential in the proof of Theorem 3.6 and shows that random draws
from the resampling distribution correspond to independent draws from the empirical product

distribution(cid:98)PX 1

n ⊗ ··· ⊗(cid:98)PX d

n .

Lemma C.2 (bootstrapping property)
Assume Setting 1, let n ∈ N, and for all ψ ∈ Bd
variable with uniform distribution on Bd
Then it holds for all (x1, . . . , xn) ∈ X n that

(cid:1)
are n iid random variables with distribution function (cid:98)Fn(x1, . . . , xn).

gn,Ψ(x1, . . . , xn) =(cid:0)xΨ

n,1, . . . , xΨ
n,n

n and let (cid:98)Fn be the empirical product distribution function.

n let gn,ψ be deﬁned as in (3.4), let Ψ be a random

Proof Let ( ˜Ω, ˜F, ˜P) be the probability space such that Ψ = (Ψ1, . . . , Ψd) : ˜Ω → Bd
n. Then, by the
properties of the uniform distribution it holds that Ψ1, . . . , Ψd are iid with uniform distribution
on Bn. This implies that for all (x1, . . . , xn) ∈ X n, for all i ∈ {1, . . . , n} and for all t ∈ Rd it
holds that

Hence, it holds for all i ∈ {1, . . . , n} that xΨ
by an explicit calculation it holds for all i, j ∈ {1, . . . , n} with i (cid:54)= j and for all k, l ∈ {1, . . . , n}d
that

˜P(cid:0)Ψ(i) = k, Ψ(j) = l(cid:1) = ˜P(cid:0)Ψ(i) = k(cid:1)˜P(cid:0)Ψ(j) = l(cid:1),

which implies that Ψ(i) is independent of Ψ(j). Consequently, it also holds that xΨ
of xΨ

j . This ﬁnally proves that for all (x1, . . . , xn) ∈ X n it holds that
(cid:1)
are iid random variables with distribution function (cid:98)Fn(x1, . . . , xn), which completes the proof of

gn,Ψ(x1, . . . , xn) =(cid:0)xΨ

n,i is independent

n,1, . . . , xΨ
n,n

(cid:3)

Lemma C.2.

Finally, we are ready for the actual proof of Theorem 3.6.

Proof Let X1, X2, . . . iid∼ PX ∈ H0 be ﬁxed and begin by introducing the following notation:

45

˜P(cid:0)xΨ
n,i ≤ t(cid:1) =

xj

j=1

j=1



˜P(cid:16)
n,Ψj (i) ≤ tj(cid:17)
d(cid:89)
 1
(cid:88)
d(cid:89)
(cid:32)
d(cid:89)
n(cid:88)
n,i has distribution function (cid:98)Fn(x1, . . . , xn). Moreover,

n,ψ(i)≤tj}

n,i≤tj}

|Bn|

(cid:33)

j=1

l=1

1{xj

1
n

ψ∈B

1{xj

.

=

=

• Let (Zi)i∈N be a sequence of independent standard normal random variables on R, let Th2 ∈
L(L2(PX,|·|R)) with the property that for every f ∈ L2(PX,|·|R) and for every x ∈ X it
holds that

(Th2(f )) (x) =

h2(x, y)f (y) PX(dy)

(cid:90)

X

and let (λi)i∈N be the eigenvalues of Th2.

• Let (cid:98)Fn be the empirical product distribution function and deﬁne for all t ∈ Rd the population

product distribution function by

F (t) :=(cid:0)PX(cid:1)(cid:0)(−∞, t1] × ··· × (−∞, td](cid:1) =
(cid:18)2d
(cid:19) ∞(cid:88)

n · (cid:92)dHSICn(X1, . . . , Xn)

d−→

2

i=1

d(cid:89)

P(cid:0)X j ≤ tj(cid:1)

j=1

λiZ 2
i

(C.9)

By Theorem 3.2 it follows that

as n → ∞. Moreover, applying the Glivenko-Cantelli theorem (e.g. van der Vaart, 1998, Theo-
rem 19.1), which extends the strong law of large numbers for empirical distributions to uniform
convergence, shows that there exists a subset A0 ⊆ Ω such that P(A0) = 1 and such that for all
ω ∈ A0 it holds for all t ∈ Rd that

(C.10)

lim

n→∞(cid:98)Fn(X1(ω), . . . , Xn(ω))(t) = F (t).
(cid:17)

(cid:16)

Next, for all n ∈ N let Ψn be a uniformly distributed random variable on Bd
and (Xi)i∈N. Then, by Lemma C.2 it holds for all n ∈ N and for all (x1, . . . , xn) ∈ X n that

n independent of X

are iid random variables with distribution function (cid:98)Fn(x1, . . . , xn). Hence, for ﬁxed ω ∈ A0 and

gn,Ψn (x1, . . . , xn) =

xΨn
n,1 , . . . , xΨn
n,n

for all i ∈ N deﬁne xi := Xi(ω), then by (C.10) it holds that

as n → ∞. Hence, we are in the same setting as described in Setting 4.

Since both PX ∈ H0 and(cid:98)PX 1

n ⊗ ··· ⊗(cid:98)PX d

n ∈ H0 it holds by Lemma D.3 for all z ∈ X that

d−→ X

xΨn
n,i

h1(z) = E (h(z, X2, . . . , X2d)) = 0

and for all n ∈ {2d, 2d + 1, . . .} and for all z ∈ X that

We therefore satisfy all requirements of Theorem A.13 and get that if ξ2(h) > 0 it holds that

λiZ 2
i

(C.11)

where h is deﬁned as in (2.4). Moreover, it holds by Theorem 2.4 that

hn

= 0,

(cid:17)

n,2d)

h(z, xΨn

n,2 , . . . , xΨn

1 (z) = E(cid:16)
θh = E (h(X1, . . . , X2d)) = dHSIC(cid:0)PX(cid:1) = 0.
(cid:19) ∞(cid:88)
(cid:18)2d

n,1 , . . . , xΨn

n,n) = n ˜Vn(h)

d−→

2

i=1

n · (cid:92)dHSICn(xΨn

46

n,1 , . . . , xΨn

n,n) = n ˜Vn(h)

(cid:18)2d

(cid:19) ∞(cid:88)

2

i=1

λi

d−→

(C.12)

as n → ∞. We consider both cases individually. If ξ2(h) > 0, let G : R → (0, 1) be the distribution

i , then by (C.11) it holds for all t ∈ R that

as n → ∞ and if ξ2(h) = 0 it holds that
n · (cid:92)dHSICn(xΨn

function of(cid:0)2d

2

(cid:1)(cid:80)∞
n→∞(cid:98)Rn(x1, . . . , xn)(t) = lim

i=1 λiZ 2

lim

(cid:88)

ψ∈Bd

n

1{n· (cid:92)dHSICn(xψ

n,1,...,xψ

n,n)≤t}

(cid:17)
n,n) ≤ t

(cid:17)

1{n· (cid:92)dHSICn(xΨn
n · (cid:92)dHSICn(xΨn

n,n)≤t}
n,1 ,...,xΨn
n,1 , . . . , xΨn

(cid:17)−1

(t) = G−1(t)

1
nnd

n→∞

= lim
n→∞

= lim
n→∞
= G(t).

E(cid:16)
P(cid:16)
(cid:16)(cid:98)Rn(x1, . . . , xn)
(cid:17)−1
(cid:16)(cid:98)Rn(X1, . . . , Xn)

lim
n→∞

Since G is continuous it holds for all t ∈ (0, 1) that

(e.g. Lehmann and Romano, 2005, Lemma 11.2.1). Recall that P(A0) = 1 which implies that it
holds P-a.s. that

(1 − α) = G−1(1 − α).

(C.13)

lim
n→∞

(cid:18)

Finally, we can perform the following calculation

P (ϕn(X1, . . . , Xn) = 1)

lim sup
n→∞

n · (cid:92)dHSICn(X1, . . . , Xn) >

(cid:16)(cid:98)Rn(X1, . . . , Xn)
(cid:17)−1
(cid:17)−1
n · (cid:92)dHSICn(X1, . . . , Xn) ≤(cid:16)(cid:98)Rn(X1, . . . , Xn)

(cid:18)

(1 − α)

(cid:19)

(cid:19)

(1 − α)

= lim sup
n→∞

P

P

= 1 − lim inf
n→∞
= 1 − G(G−1(1 − α)) = α,

where in the last step we use the corollary of Slutsky’s theorem given in Lehmann and Romano
(2005, Corollary 11.2.3) together with (C.9) and (C.13). If ξ2(h) = 0 then it holds for all t ∈ R
that

n→∞(cid:98)Rn(x1, . . . , xn)(t) = lim

lim

(cid:88)

ψ∈Bd

n

1{n· (cid:92)dHSICn(xψ

n,1,...,xψ

n,n)≤t}

(cid:17)
n,n) ≤ t

(cid:17)

n,n)≤t}
n,1 ,...,xΨn
n,1 , . . . , xΨn

1{n· (cid:92)dHSICn(xΨn
n · (cid:92)dHSICn(xΨn
i=1 λi≤t}.

Consequently, we get for all t ∈ (0, 1) that

lim
n→∞

(t) =

Since P(A0) = 1 this in particular implies that it holds P-a.s. that

(cid:18)2d
(cid:19) ∞(cid:88)
(cid:19) ∞(cid:88)
(cid:18)2d

i=1

2

λi.

lim
n→∞

(1 − α) =

λi.

2

i=1

1
nnd

n→∞

= lim
n→∞

= lim
n→∞
= 1{(2d

E(cid:16)
P(cid:16)
2 )(cid:80)∞
(cid:16)(cid:98)Rn(x1, . . . , xn)
(cid:17)−1
(cid:16)(cid:98)Rn(X1, . . . , Xn)
(cid:17)−1

47

Hence, applying Fatou’s lemma we get
P (ϕn(X1, . . . , Xn) = 1)

lim sup
n→∞

(cid:18)

(cid:18)

n · (cid:92)dHSICn(X1, . . . , Xn) >

(cid:17)−1
(cid:16)(cid:98)Rn(X1, . . . , Xn)
n · (cid:92)dHSICn(X1, . . . , Xn) ≤(cid:16)(cid:98)Rn(X1, . . . , Xn)
(cid:17)−1
(cid:17)

P
n→∞ 1{n· (cid:92)dHSICn(X1,...,Xn)≤((cid:98)Rn(X1,...,Xn))

(1−α)}

= 0,

−1

lim inf

= lim sup
n→∞

P

= 1 − lim inf
n→∞

≤ 1 − E(cid:16)

(1 − α)

(cid:19)

(cid:19)

(1 − α)

which completes the proof of Theorem 3.6.

(cid:3)

C.7 Proof of Theorem 3.7
Similar to the proof of Theorem 3.6 the proof of Theorem 3.7 also requires Lemma C.2
Proof Let X1, X2, . . . iid∼ PX ∈ HA be ﬁxed and begin by introducing the following notation:

• Let (Zi)i∈N be a sequence of independent standard normal random variables on R, let Th2 ∈
,|·|R)

,|·|R)) with the property that for every f ∈ L2(PX 1 ⊗ ··· ⊗ PX d

L(L2(PX 1 ⊗ ··· ⊗ PX d
and for every x ∈ X it holds that

(cid:90)

X

(Th2(f )) (x) =

h2(x, y)f (y) PX 1 ⊗ ··· ⊗ PX d

(dy)

(C.14)

and let (λi)i∈N be the eigenvalues of Th2.

• Let (cid:98)Fn be the empirical product distribution function and deﬁne for all t ∈ Rd the population

product distribution function by

(cid:16)PX 1 ⊗ ··· ⊗ PX d(cid:17)(cid:0)(−∞, t1] × ··· × (−∞, td](cid:1) =

F (t) :=

P(cid:0)X j ≤ tj(cid:1)

d(cid:89)

j=1

Applying the Glivenko-Cantelli theorem (e.g. van der Vaart, 1998, Theorem 19.1), which extends
the strong law of large numbers for empirical distributions to uniform convergence, shows that
there exists a subset A0 ⊆ Ω such that P(A0) = 1 and such that for all ω ∈ A0 it holds for all
t ∈ Rd that

Next, for all n ∈ N let Ψn be a uniformly distributed random variable on Bd
and (Xi)i∈N. Then, by Lemma C.2 it holds for all n ∈ N and for all (x1, . . . , xn) ∈ X n that

n independent of X

lim

n→∞(cid:98)Fn(X1(ω), . . . , Xn(ω))(t) = F (t).
(cid:17)
are iid random variables with distribution function (cid:98)Fn(x1, . . . , xn).

gn,Ψn (x1, . . . , xn) =

xΨn
n,1 , . . . , xΨn
n,n

(cid:16)

Fix ω ∈ A0, let (X∗

i )i∈N be iid sequence of random variables with distribution PX 1 ⊗···⊗ PX d

and for all i ∈ N deﬁne xi := Xi(ω). Then, by (C.15) it holds that

(C.15)

xΨn
n,i

d−→ X∗

i

Since both PX 1 ⊗ ··· ⊗ PX d ∈ H0 and (cid:98)PX 1

as n → ∞. Hence, we are in the same setting as described in Setting 4.
z ∈ X that

n ⊗ ··· ⊗(cid:98)PX d

h1(z) = E (h(z, X∗

2, . . . , X∗

2d)) = 0

n ∈ H0 it holds by Lemma D.3 for all

48

and for all n ∈ {2d, 2d + 1, . . .} and for all z ∈ X that

1 (z) = E(cid:16)

hn

(cid:17)

h(z, xΨn

n,2 , . . . , xΨn

n,2d)

= 0,

where h is deﬁned as in (2.4). Moreover, it holds by Proposition 2.4 that

θh = E (h(X∗

1, . . . , X∗

2d)) = dHSIC

= 0.

We therefore satisfy all requirements of Theorem A.13 and get that if ξ2(h) > 0 it holds that

n · (cid:92)dHSICn(xΨn

n,1 , . . . , xΨn

n,n) = n ˜Vn(h)

λiZ 2
i

(C.16)

(cid:16)PX 1 ⊗ ··· ⊗ PX d(cid:17)
(cid:19) ∞(cid:88)
(cid:18)2d
(cid:19) ∞(cid:88)
(cid:18)2d

d−→

i=1

2

d−→

2

i=1

as n → ∞ and if ξ2(h) = 0 it holds that

n · (cid:92)dHSICn(xΨn

n,1 , . . . , xΨn

n,n) = n ˜Vn(h)

λi

(C.17)

as n → ∞, where the λi’s are deﬁned as the eigenvalues of the operator in (C.14). We now
consider both cases individually. If ξ2(h) > 0, let G : R → (0, 1) be the distribution function of

(cid:0)2d

(cid:1)(cid:80)∞

2

i=1 λiZ 2

i , then by (C.16) it holds for all t ∈ R that
(cid:88)

n→∞(cid:98)Rn(x1, . . . , xn)(t) = lim

n→∞

lim

1
nnd

E(cid:16)
P(cid:16)

= lim
n→∞

= lim
n→∞
= G(t).

1{n· (cid:92)dHSICn(xψ

n,1,...,xψ

n,n)≤t}

ψ∈Bd

n

1{n· (cid:92)dHSICn(xΨn
n · (cid:92)dHSICn(xΨn

n,n)≤t}
n,1 ,...,xΨn
n,1 , . . . , xΨn

(cid:17)
n,n) ≤ t

(cid:17)

Since G is continuous it holds for all t ∈ (0, 1) that

(e.g. Lehmann and Romano, 2005, Lemma 11.2.1). Therefore we have shown that for all ω ∈ A0
it holds that

(1 − α) = G−1(1 − α).

(C.18)

If ξ2(h) = 0 then it holds for all t ∈ R that

(t) = G−1(t)

lim
n→∞

(cid:17)−1
(cid:16)(cid:98)Rn(x1, . . . , xn)
(cid:17)−1
(cid:16)(cid:98)Rn(X1(ω), . . . , Xn(ω))
(cid:88)
n→∞(cid:98)Rn(x1, . . . , xn)(t) = lim

lim
n→∞

lim

1
nnd

n→∞

= lim
n→∞

E(cid:16)
P(cid:16)
2 )(cid:80)∞
(cid:16)(cid:98)Rn(x1, . . . , xn)
(cid:17)−1

= lim
n→∞
= 1{(2d

1{n· (cid:92)dHSICn(xψ

n,1,...,xψ

n,n)≤t}

ψ∈Bd

n

(cid:17)
n,n) ≤ t

(cid:17)

n,n)≤t}
n,1 ,...,xΨn
n,1 , . . . , xΨn

1{n· (cid:92)dHSICn(xΨn
n · (cid:92)dHSICn(xΨn
i=1 λi≤t}.

(cid:18)2d

(cid:19) ∞(cid:88)

2

i=1

λi.

(1 − α) =

49

Consequently, we get that

lim
n→∞

A1 :=

(C.19)
By Theorem 3.1 it holds that P(A1) = 1, which implies that P(A0 ∩ A1) = 1. Let ω ∈ A0 ∩ A1,
then by (C.18) and (C.19) there exists a constant t∗ ∈ R such that for all n ∈ N it holds that

n→∞ 1{n· (cid:92)dHSICn(X1(ω),...,Xn(ω))≤t} = 0
lim

.

Introduce the set

and hence

n→∞1(cid:110)

lim

(cid:17)−1

(cid:110)
ω ∈ Ω(cid:12)(cid:12)∀t ∈ R :
(cid:16)(cid:98)Rn(X1(ω), . . . , Xn(ω))
n· (cid:92)dHSICn(X1(ω),...,Xn(ω))≤((cid:98)Rn(X1(ω),...,Xn(ω)))
≤ lim
n→∞ 1{n· (cid:92)dHSICn(X1(ω),...,Xn(ω))≤t∗} = 0.
n→∞ 1(cid:110)

(1 − α) ≤ t∗

(1−α)

n· (cid:92)dHSICn(X1,...,Xn)≤((cid:98)Rn(X1,...,Xn))
(cid:18)

P (ϕn(X1, . . . , Xn) = 0)

n· (cid:92)dHSICn(X1,...,Xn)≤((cid:98)Rn(X1,...,Xn))

1(cid:110)

lim

E

−1

lim
n→∞

= lim
n→∞

= 0,

(cid:111)

(cid:111)

(1−α)

−1

(cid:111) = 0

(cid:111)(cid:19)

(1−α)

−1

This proves that P-a.s. it holds that

and applying the dominated convergence theorem we also get

which completes the proof of Theorem 3.7.

(cid:3)

C.8 Proof of Proposition 3.8
Proof Due to Lemma 2.7 we know that (cid:92)dHSICn is a V-statistic with core function h. Under H0
it holds that θh = 0 and thus applying Lemma A.6 results in

(cid:18)2d
(cid:19)
We can use Lemma D.2 to explicitly calculate(cid:0)2p

E(cid:16) (cid:92)dHSICn

(cid:17)

1
n

=

2

E (h2(X1, X1)) + O(cid:0)n−2(cid:1) .
(cid:1)E(h2(X1, X1)), which together with the inde-

2

pendence assumption under H0 simpliﬁes to the desired expression. This concludes the proof of
(cid:3)
Proposition 3.8.

C.9 Proof of Proposition 3.9
Proof Due to Lemma 2.7 we know that (cid:92)dHSIC is a V-statistic with core function h. Applying
Lemma A.5 thus results in

(cid:16) (cid:92)dHSICn

Var

Using Lemma D.2 we get that

(cid:17)

n− 5

2

.

2

=

2d

2d − 2

(cid:19)(cid:18)n − 2d
(cid:18) n
(cid:19)−1(cid:18)2d
(cid:19)
ξ2 + O(cid:16)
(cid:17)
ξ2 = E(cid:0)h2(X1, X2)2(cid:1)
(cid:32) 10(cid:88)
(cid:33)2
(cid:18)2d
(cid:19)−2
(cid:19)−2 10(cid:88)
(cid:18)2d

i=1

E

ai

=

2

E (aiaj) .

=

2

i,j=1

50

Each term E(aiaj) can be explicitly calculated and simpliﬁed using the independence properties
(cid:3)
under H0 (very tedious). This concludes the proof of Proposition 3.9.

D Technical results related to h
In order to make the calculations in this section more readable we use the following conventions.

• For all j ∈ {1, . . . , d} and for all i1, i2 ∈ {1, . . . , n} we set

kj
i1,i2

:= kj(X j
i1

, X j
i2

).

• For all q, n ∈ N, for all functions g : X n → R and for all i1, . . . , iq, j1, . . . , jn ∈ {1, . . . , n} we

set

Ei1,...,iq (g(Xj1, . . . , Xjn )) =

g(Xj1 , . . . , Xjn ) PX(dXi1 )··· PX(dXiq ).

(cid:90)

(cid:90)

···

X

X

 − E

kj(X j

1 , X j
2 )

Lemma D.1 (expansion of h1)
Assume Setting 1. Then it holds for all z ∈ X that,

j=1

 d(cid:89)
E
E
 d(cid:88)

d − 1
d

E

1
d

r=1

h1(z) =

1
d

+

+

j=1

kj(zj, X j
j )

 d(cid:89)

 d(cid:89)
 − E

 kr(zr, X r
 kr(zr, X r

kj(X j

1 , X j

1 , X j

j+1)

2j)

2r)

j=1

kj(X j

j+1)

r+1)




kj(zj, X j
1 )

j=1

 d(cid:89)
 d(cid:89)
− d(cid:88)

j(cid:54)=r

r=1

E

2j−1, X j

kj(X j

 d(cid:89)

j(cid:54)=r

Proof Recall that

h1(z) = E (h(z, X1, . . . , X2d−1)) .

Next we separate h into 3 terms as follows.

h(z1, . . . , z2d) =

1

(2d)!

zj
π(1), zj

π(2)

(cid:17) (=: b1)
(cid:17) (=: b2)
(cid:17) (=: b3).

π(2j)

zj
π(2j−1), zj

zj
π(1), zj

π(j+1)

if π(1) (cid:54)= 1 ∧ π(2) (cid:54)= 1
if π(1) = 1 ∨ π(2) = 1.

π∈S2d

j=1

j=1

π∈S2d

 d(cid:89)
kj(cid:16)
(cid:88)
 d(cid:89)
kj(cid:16)
(cid:88)
 d(cid:89)
kj(cid:16)
(cid:88)
E2,3
(cid:16)(cid:81)d
(cid:16)(cid:81)d

j=1 kj
j=1 kj

π∈S2d

E2

j=1

1,2

+

1

(2d)!

− 2
(2d)!

 =

(cid:17)

(cid:17)

2,3

51

Now we calculate E2,...,2d (h(X1, . . . , X2d)) by considering these three terms separately.

b1: Begin by letting π ∈ S2d, then

 d(cid:89)

j=1

E2,...,2d

kj
π(1),π(2)

Counting how often each of these cases can occur for π ∈ S2d leads to

b2: Begin by letting π ∈ S2d, r ∈ {1, . . . , p} such that π(2r − 1) = 1 or π(2r) = 1 then

Counting how many combinations are possible for each r and adding all diﬀerent combinations up
gives us

(cid:88)

E2,...,2d

π∈S2d
(2d − 2)(2d − 1)!

(2d)!

j=1

 d(cid:89)
 d(cid:89)

kj
2,3

j=1

1

(2d)!

=

=

d − 1

p

E2,3

 d(cid:89)

j=1

E2,...,2d

kj
π(2j−1),π(2j)

(cid:88)

1

(2d)!

=

E2,...,2d

π∈S2d
2(2d − 1)!

(2d)!

d(cid:88)

r=1

=

1
d

E2,...,2d+1

 =



E2,...,d+2
E2,...,d+1
E2,...,d+2

b3: Begin by letting π ∈ S2d, then

 d(cid:89)

j=1

E2,...,2d

kj
π(1),π(j+1)

j(cid:54)=r

j=1

j=1

kj
2,3

E2

1
p

E2,3

kj
π(1),π(2)


 d(cid:89)
 +
 +
 d(cid:89)
 = E2,...,2d+1
 d(cid:89)
d(cid:88)
 d(cid:89)
(cid:16)(cid:81)d
(cid:16)(cid:81)d
(cid:16)(cid:81)d

j=1 kj
j=1 kj
j(cid:54)=r kj

E2,...,2d+1

kj
1,2


 d(cid:89)

 d(cid:89)
 kr

j(cid:54)=r

1,j+1
2,j+2kr
1,2

kj
π(2j−1),π(2j)

kj
2j,2j+1

(cid:17)
(cid:17)

j(cid:54)=r

2,j+2

r=1

j=1

1,2r

2(2d − 1)!

(2d)!

 d(cid:89)

j=1

E2



kj
1,2

(D.1)

 kr



kj
2j,2j+1

1,2r

kj
2j,2j+1

1,2r

 kr




(D.2)

(cid:17)

if π(1) (cid:54)= 1 ∧ ··· ∧ π(d + 1) (cid:54)= 1
if π(1) = 1
if π(r + 1) = 1 for r ∈ {1, . . . , d}

Counting how often each of these cases can occur for diﬀerent π ∈ S2d and adding all cases up
results in

(cid:88)

π∈S2d
d − 1
2d

1

(2d)!

=

E2,...,2d

kj
π(1),π(j+1)

 d(cid:89)
 d(cid:89)

j=1


 +

E2,...,p+2

d(cid:88)

r=1

j=1

E2,...,p+2

kj
2,j+2

 d(cid:89)

j(cid:54)=r

+

1
2d

E2,...,p+1

1
2d



kj
2,j+2kr
1,2



kj
1,j+1

 d(cid:89)

j=1

(D.3)

(cid:3)

Finally combining (D.1), (D.1) and (D.1) completes the proof of Lemma D.1.

52

Lemma D.2 (expansion of h2 under H0)
Assume Setting 1. Then under H0 it holds for all z1, z2 ∈ X that,

(cid:18)2d
(cid:19)

2

h2(z1, z2) =

kr(zr

1, zr
2)

d(cid:89)

r=1

+ (d − 1)2

r=1

d(cid:89)
d(cid:89)
d(cid:89)

r=1

(=: a1)

E (kr(X r

1 , X r

2 ))

(=: a2)

(=: a3)

+ (d − 1)

E (kr(zr

1, X r

1 ))

+ (d − 1)

E (kr(zr

(=: a4)

r=1

kr(zr

1, zr
2)

kr(zr

1, zr
2)

l(cid:54)=r

(cid:89)
(cid:89)
(cid:89)

l(cid:54)=r

1 ))

2, X r

E(cid:0)kl(X l
E(cid:0)kl(zl
E(cid:0)kl(zr

+

r=1

d(cid:88)
− d(cid:88)
− d(cid:88)
(cid:88)

r=1

r=1

+

r(cid:54)=s

d(cid:88)
d(cid:88)

r=1

r=1

(cid:88)
(cid:88)
(cid:88)

π∈S2d

π∈S2d

π∈S2d

E (kr(zr

1, X r

1 ))

E (kr(zr

2, X r

1 ))

j=1

 d(cid:89)
 d(cid:89)
 d(cid:89)

j=1

j=1

E3,...,2d

E3,...,2d

E3,...,2d

A :=

B :=

C :=

1, X l

1, X l

2)(cid:1)
1)(cid:1)
1 )(cid:1)
(cid:89)
(cid:89)

l(cid:54)=r

2, X s

l(cid:54)=r

(=: a5)

(=: a6)

E(cid:0)kl(X l
2)(cid:1)
2)(cid:1)

1, X l

1, X l

1 ))

l(cid:54)=r,s

(cid:89)
E(cid:0)kl(X l
E(cid:0)kl(X l


kj
π(1),π(2)


 .

kj
π(2j−1),π(2j)

kj
π(1),π(j+1)

(=: a9)

(=: a10).

kr(zr

1, zr
2)

l(cid:54)=r
1 )) E (ks(zs
1, X r

E (kr(zr

2, X r

(=: a7)

2)(cid:1)

1, X l

(=: a8)

− (d − 1)

− (d − 1)

Proof Begin by setting,

Then it holds that,

h2(X1, X2) = E3,...,2d (h(X1, . . . , X2d)) =

1

(2d)!

(A + B − 2C) .

(D.4)

Under the null hypothesis H0 the terms A,B and C can be simpliﬁed using combinatorial

arguments (similar to the ones used in the proof of Lemma D.1).

53

A = 2(2d − 2)!

kr
1,2

d(cid:89)

r=1

+ (2d − 2)(2d − 3)(2d − 2)!

+ 2(2d − 2)(2d − 2)!

+ 2(2d − 2)(2d − 2)!

(cid:1)

3,4

E3,4

3,4

r=1

r=1

r=1

l(cid:54)=r

E3

1,3

2,3

1,3

2,3

E3

E3

E3,4

E3,4

(cid:1)
(cid:1)

(cid:0)kr

(cid:1) (cid:89)

d(cid:89)
d(cid:89)
(cid:0)kr
d(cid:89)
(cid:0)kr
(cid:89)
(cid:0)kl
(cid:1)
(cid:0)kr
(cid:0)ks
(cid:1) E3
(cid:0)kl
(cid:1)
(cid:0)kr
(cid:1) + (d − 1)(2d − 2)!
d(cid:89)
(cid:0)kr
(cid:1) (cid:89)
(cid:1) E3
(cid:0)ks
(cid:1)(cid:89)
(cid:0)kr
(cid:1)(cid:89)
(cid:0)kr

(cid:0)kl
(cid:1)
(cid:1)

(cid:0)kl
(cid:0)kl

E3,4

E3,4

E3,4

l(cid:54)=r,s

l(cid:54)=r,s

1,3

l(cid:54)=r

E3,4

(cid:1)

3,4

3,4

1,3

1,3

(cid:1)

r=1

3,4

2,3

l(cid:54)=r

r=1

r(cid:54)=s

d(cid:88)
(cid:88)
(cid:89)
d(cid:89)

l(cid:54)=r

E3

E3

r=1

(cid:0)kr
d(cid:88)
d(cid:88)

r=1

r=1

1,3

2,3

3,4

(cid:0)kl

3,4

(cid:1)

(cid:0)kr

2,3

(cid:1)

E3

d(cid:89)

r=1

B = 2(2d − 2)!

kr
1,2

+ 4(2d − 2)!

C = 2(2d − 2)!

d(cid:88)

r=1

kr
1,2

+ (d − 1)(2d − 2)!

+ (d − 1)(d − 2)(2d − 2)!

+ (2d − 2)!

E3

(cid:88)

r(cid:54)=s

+ (d − 1)(2d − 2)!

+ (d − 1)(2d − 2)!

E3

E3

Plugging these expressions for A, B and C into (D.4) completes the proof of Lemma D.2.

(cid:3)

Lemma D.3 (degeneracy under H0)
Assume Setting 1. Then under H0 it holds for all z ∈ X that

and therefore in particular that ξ1(h) = 0.

h1(z) = 0,

54

Proof Observe that under H0 it holds for all z ∈ X that

j=1

 d(cid:89)
 d(cid:89)
 d(cid:89)

j=1

j(cid:54)=r

• E

• E

• E

kj(X j

2j−1, X j

kj(zj, X j
1 )

kj(zj, X j
j )

kj(X j

1 , X j
2 )

kj(X j

1 , X j

j+1)



j=1

 d(cid:89)
 = E
 d(cid:89)
 = E
 kr(zr, X r
 d(cid:89)

= E

2j)

j=1

j(cid:54)=r



2r)


 kr(zr, X r

 .

kj(X j

1 , X j

j+1)

r+1)

(D.5)

(D.6)

(D.7)

(cid:3)

Plugging (D.5), (D.6) and (D.7) into the explicit form of h1 given in Lemma D.1 results in

This completes the proof of Lemma D.3.

h1(z) = 0.

Acknowledgements
The authors thank Christoph Lampert and Arthur Gretton for helpful discussion. JP received
support from the ETH-MPI learning center.

References
Bergsma, W. and A. Dassios (2014). A consistent test of independence based on a sign covariance

related to Kendall’s tau. Bernoulli 20 (2), 1006–1028.

Billingsley, P. (2008). Convergence of Probability Measures. John Wiley and Sons.

Bühlmann, P., J. Peters, and J. Ernest (2014). CAM: Causal additive models, high-dimensional

order search and penalized regression. The Annals of Statistics 42 (6), 2526–2556.

Davison, A. C. and D. V. Hinkley (1997). Bootstrap Methods and their Application. Cambridge

University Press.

Ferreira, J. and V. Menegatto (2009). Eigenvalues of integral operators deﬁned by smooth positive

deﬁnite kernels. Integral Equations and Operator Theory 64 (1), 61–81.

Fukumizu, K., A. Gretton, X. Sun, and B. Schölkopf (2007). Kernel measures of conditional

dependence. In Advances in Neural Information Processing Systems (NIPS 20), pp. 489–496.

Gretton, A., O. Bousquet, A. Smola, and B. Schölkopf (2005). Measuring statistical dependence

with Hilbert-Schmidt norms. In Algorithmic learning theory, pp. 63–77. Springer-Verlag.

Gretton, A., K. Fukumizu, C. H. Teo, L. Song, B. Schölkopf, and A. J. Smola (2007). A kernel
statistical test of independence. In Advances in Neural Information Processing Systems (NIPS
20), pp. 585–592.

Gretton, A., D. Sejdinovic, H. Strathmann, S. Balakrishnan, M. Pontil, K. Fukumizu, and B. K.
Sriperumbudur (2012). Optimal kernel choice for large-scale two-sample tests. In Advances in
Neural Information Processing Systems (NIPS 25), pp. 1205–1213.

55

Lehmann, E. L. and J. P. Romano (2005). Testing Statistical Hypotheses. Springer-Verlag.

Leucht, A. and M. H. Neumann (2009). Consistency of general bootstrap methods for degenerate

U-type and V-type statistics. Journal of Multivariate Analysis 100 (8), 1622–1633.

Leung, D. and M. Drton (2016). Testing independence in high dimensions with sums of squares

of rank correlations. ArXiv e-prints (1501.01732).

Liu, H., F. Han, M. Yuan, J. Laﬀerty, and L. Wasserman (2012). High-dimensional semiparametric

gaussian copula graphical models. The Annals of Statistics 40 (4), 2293–2326.

Mooij, J. M., J. Peters, D. Janzing, J. Zscheischler, and B. Schölkopf (2016). Distinguishing cause
from eﬀect using observational data: methods and benchmarks. Journal of Machine Learning
Research, to appear. ArXiv e-prints (1412.3773).

Nandy, P., L. Weihs, and M. Drton (2016). Large-sample theory for the bergsma-dassios sign

covariance. ArXiv e-prints (1602.04387).

Pearl, J. (2009). Causality: Models, Reasoning, and Inference (2nd ed.). New York, USA: Cam-

bridge University Press.

Peters, J. and P. Bühlmann (2015). Structural intervention distance (SID) for evaluating causal

graphs. Neural Computation 27, 771–799.

Peters, J., J. M. Mooij, D. Janzing, and B. Schölkopf (2014). Causal discovery with continuous

additive noise models. The Journal of Machine Learning Research 15 (1), 2009–2053.

Phipson, B. and G. K. Smyth (2010). Permutation p-values should never be zero: calculating
exact p-values when permutations are randomly drawn. Statistical Applications in Genetics and
Molecular Biology 9 (1), 1–16.

R Core Team (2014). R: A Language and Environment for Statistical Computing. Vienna, Austria:

R Foundation for Statistical Computing.

Satterthwaite, F. E. (1946). An approximate distribution of estimates of variance components.

Biometrics Bulletin 2 (6), 110–114.

Sejdinovic, D., A. Gretton, and W. Bergsma (2013). A kernel test for three-variable interactions.

In Advances in Neural Information Processing Systems (NIPS 26), pp. 1124–1132.

Serﬂing, R. J. (1980). Approximation Theorems of Mathematical Statistics. John Wiley and Sons.

Smola, A., A. Gretton, L. Song, and B. Schölkopf (2007). A Hilbert space embedding for distri-
butions. In Algorithmic Learning Theory, Volume 4754 of Lecture Notes in Computer Science,
pp. 13–31. Springer-Verlag.

Sriperumbudur, B. K., A. Gretton, K. Fukumizu, G. Lanckriet, and B. Schölkopf (2008). Injective
Hilbert space embeddings of probability measures. In Conference on Learning Theory (COLT).

Székely, G. J. and M. L. Rizzo (2009). Brownian distance covariance. The Annals of Applied

Statistics 3 (4), 1236–1265.

Székely, G. J. and M. L. Rizzo (2014). Partial distance correlation with methods for dissimilarities.

The Annals of Statistics 42 (6), 2382–2412.

van der Vaart, A. W. (1998). Asymptotic Statistics. Cambridge University Press. Cambridge

Books Online.

Wegkamp, M. and Y. Zhao (2016). Adaptive estimation of the copula correlation matrix for

semiparametric elliptical copulas. Bernoulli 22 (2), 1184–1226.

56

Wood, S. N. and N. H. Augustin (2002). GAMs with integrated model selection using penalized
regression splines and applications to environmental modelling. Ecological Modelling 157 (2–3),
157–177.

Xue, L. and H. Zou (2012). Regularized rank-based estimation of high-dimensional nonparanormal

graphical models. The Annals of Statistics 40 (5), 2541–2571.

57

