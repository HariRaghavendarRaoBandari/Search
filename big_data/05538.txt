Inference for dynamics of continuous variables: the
Extended Plefka Expansion with hidden nodes

B Bravi1 and P Sollich1
1 Department of Mathematics, King’s College London, Strand, London, WC2R 2LS UK

E-mail: barbara.bravi@kcl.ac.uk and peter.sollich@kcl.ac.uk

Abstract. We consider the problem of a subnetwork of observed nodes embedded into a
larger bulk of unknown (i.e. hidden) nodes, where the aim is to infer these hidden states given
information about the subnetwork dynamics. The biochemical networks underlying many
cellular and metabolic processes are important realizations of such a scenario as typically
one is interested in reconstructing the time evolution of unobserved chemical concentrations
starting from the experimentally more accessible ones. As a paradigmatic model we study
the stochastic linear dynamics of continuous degrees of freedom interacting via random
Gaussian couplings. The resulting joint distribution is known to be Gaussian and this
allows us to fully characterize the posterior statistics of the hidden nodes. In particular the
equal-time hidden-to-hidden covariance – conditioned on observations – gives the expected
error when the hidden time courses are predicted based on the observations. We study
this error in the inﬁnite network size limit by resorting to a novel dynamical mean ﬁeld
approximation, the Extended Plefka Expansion, that is based on a path integral description
of the stochastic dynamics. In the stationary regime, we analyze the phase diagram in the
space of the relevant parameters, namely the ratio between the numbers of observed and
hidden nodes, the degree of symmetry of the interactions and the amplitudes of the hidden-
to-hidden and hidden-to-observed couplings relative to the decay constant of the internal
hidden dynamics. In particular, we identify critical regions in parameter space where the
inference error diverges, and determine the corresponding scaling behaviour.

Keywords: Plefka Expansion, Inference, Mean Field, Biochemical Networks, Dynamical
Functional

6
1
0
2

 
r
a

 

M
2
2

 
 
]
h
c
e
m

-
t
a
t
s
.
t
a
m
-
d
n
o
c
[
 
 

2
v
8
3
5
5
0

.

3
0
6
1
:
v
i
X
r
a

1. Introduction

The problem of reconstructing the time evolution of a system given some measurements of
its dynamics has seen much recent interest in the statistical physics community [1–3]. Given
a temporal sequence of observed variables, the task is to infer the states of other variables
that are not observed, based on knowledge of the interaction parameters. Techniques for
tackling this problem could have a signiﬁcant impact in systems biology, where dealing with
missing variables and experimental limitations requires the development of novel inference
frameworks, to enable quantitative modelling on the basis of experimental data [4–6].

In machine learning and pattern recognition, the problem of inference from data has been
addressed using e.g. state space models [7] that introduce “hidden” variables playing the role
of unobserved states. The best-known example of this type is probably the Kalman ﬁlter [8],
for the case of stochastic linear dynamics of continuous degrees of freedom (d.o.f.). We
study such a dynamics here, with interaction parameters drawn at random from a Gaussian
distribution. A wide range of microscopic biological processes can be captured qualitatively
even by this simpliﬁed setting, such as inter- and intra-cellular biochemical networks where
interactions parameters are given by reaction rates and one may be interested in analyzing
ﬂuctuations around a steady state [9]. The joint distribution over observed and hidden states
is Gaussian in our model and this allows us to analyze the posterior statistics of the hidden
dynamics. The second order statistics in particular tells us how the observed dynamics
constrains the unknown hidden dynamics, with the posterior variances quantifying the degree
of uncertainty in hidden state prediction. As our scenario is analytically tractable, we can
study the dependence of this prediction error on key parameters and thus shed light on the
accuracy of the inference process, similarly to what has been done for learning in a linear
perceptron [10, 11].

We tackle the inference problem for our setting by means of the Extended Plefka
Expansion that we have recently developed [12], a dynamical mean-ﬁeld theory for continuous
degrees of freedom. This should become exact in the thermodynamic limit of a large network.
It is also general enough to allow us to treat a wide range of dynamical interactions, i.e.
couplings of any symmetry, so that we can probe both equilibrium and non-equilibrium
networks with only partially observed dynamics. This ﬂexibility makes our approach in
principle widely applicable to real data from systems driven out of equilibrium by ﬂuxes,
with biological networks being a case in point as illustrated e.g. by studies on non-equilibrium
steady states for reaction ﬂuxes [13] and “near” symmetry features in metabolic networks
[14].

Inference problems for non-equilibrium systems have been already investigated for
neural data, using either two-state units (Ising spins) [15] or deterministic continuous-valued
hidden units [16]. These studies were motivated by modelling populations of neurons, and
concentrated on ﬁnding learning rules, while here we study continuous d.o.f. with random

2

linear interactions in the large network limit and our focus is on understanding the prediction
accuracy for hidden states theoretically and from a macroscopic point of view, including its
dependence on key system parameters.
The paper is organized as follows.

In section 2 we set our model in terms of a set
of dynamical equations for observed and hidden continuous variables interacting via linear
Gaussian couplings. A path integral representation of the likelihood (see e.g. [15]) provides
the starting point for the Extended Plefka Expansion. The latter produces an eﬀectively non-
interacting approximation of the original dynamics, more speciﬁcally a Gaussian posterior
probability that is factorized over hidden nodes but incorporates the hidden-to-observed
couplings.
In section 2.1 we derive equations for the posterior means, which give the
best estimate of the hidden dynamics.
In section 2.2 we focus then on posterior second
moments, i.e. hidden-to-hidden correlations, hidden responses and auxiliary correlations, in
the stationary regime. In section 2.3 we consider the thermodynamic limit of inﬁnite network
size, shifting from local correlations to their macroscopic average across the network. The
exactness, at least in some limit cases, of the extended Plefka predictions is shown in section
2.4 by comparison with expressions obtained elsewhere [17] by Kalman ﬁlter and Random
Matrix Theory (RMT) methods.
In section 3.1 we determine the relevant dimensionless
parameters governing prediction accuracy, namely the ratio between the numbers of observed
and hidden nodes, the degree of symmetry of the hidden interactions, and the amplitudes
of the hidden-to-hidden and hidden-to-observed interactions relative to the decay constant
of the internal hidden dynamics. We identify in this parameter space critical points for
the inference error and study its behaviour in the critical regions by appealing to a scaling
analysis (sections 3.2, 3.3, 3.4). We also consider the temporal correlations in the posterior
dynamics of the hidden nodes, revealing interesting long-time behaviour in the critical regions
and we study the posterior relaxation times for general parameter settings in section 4.

2. Extended Plefka Expansion with hidden nodes

We consider a generic network where only the dynamics of a subnetwork of nodes is observed
while the others are hidden and form what we call the “bulk”. Such a situation could arise
because the subnetwork nodes are more precisely characterized from the theoretical point
of view, or experimentally more accessible. We assume that we have noise-free data for the
trajectories of the observed nodes. In biological contexts this is clearly a simpliﬁcation as data
is often available only at discrete time points or corrupted by noise. The constraint that only
a part of the network can be observed is generic, on the other hand. In protein interaction
networks, for example, only a few molecular species can typically be tagged biochemically
in such a way that their concentrations can be tracked with reasonable accuracy [18, 19].

We use the indices i, j = 1, ..., N b for the hidden or bulk variables and a, b = 1, ..., N s for
the observed or subnetwork nodes of the network, and generally use the superscripts s and b

3

to distinguish variables relating to the observed and hidden sectors, respectively. Assuming
that the hidden and observed variables xi, xa interact via linear couplings {Jij}, {Kia}, their
dynamical evolution is described by

(cid:88)
(cid:88)

j

∂txi(t) = −λxi(t) +

∂txa(t) = −λxa(t) +

(cid:88)
(cid:88)

a

Jijxj(t) +

Kiaxa(t) + ξi(t)

Jabxb(t) +

Kajxj(t) + ξa(t)

(2.1a)

(2.1b)

b

j

Jij (respectively Jab) denotes hidden-to-hidden (respectively observed-to-observed) interac-
tions, while the coupling between observed and hidden variables is contained in Kia and Kaj.
We have included a self-interaction term with coeﬃcient λ, which acts as a decay constant
and provides the basic timescale of the dynamics. In more compact notation we can write

where φi(xb(t), xs(t)) = (cid:80)
(cid:80)

∂txi(t) = −λxi(t) + φi(xb(t), xs(t)) + ξi(t)
∂txa(t) = −λxa(t) + φa(xb(t), xs(t)) + ξa(t)

j Jijxj(t) +(cid:80)

a Kiaxa(t) and φa(xb(t), xs(t)) = (cid:80)

(2.2a)

(2.2b)

b Jabxb(t) +

j Kajxj(t) and xb(t) (xs(t)) denotes the whole set of hidden (observed) values.

The dynamical noises ξi, ξa are Gaussian white noises with zero mean and diagonal

covariances Σi, Σa

(cid:104)ξi(t)ξj(t(cid:48))(cid:105) = Σiδijδ(t − t(cid:48))

(cid:104)ξa(t)ξb(t(cid:48))(cid:105) = δabΣaδ(t − t(cid:48))

(2.3)

After discretizing time in elementary time steps ∆, we can write the likelihood – in our case,
the probability of a trajectory of the observed variables – using the Martin–Siggia–Rose–
Janssen–De Dominicis (MSRJD) path integrals formalism [20–22] as

at

δ(cid:0)xa(t + ∆) − xa(t) − ∆[−λxa(t) + φa(xb(t), xs(t)) + ξa(t)](cid:1)

Dxb(cid:89)
δ(cid:0)xi(t + ∆) − xi(t) − ∆[−λxi(t) + φi(xb(t), xs(t)) + ξi(t)](cid:1)(cid:29)
(cid:88)

iˆxi(t)(xi(t + ∆) − xi(t) − ∆[−λxi(t) + φi(xb(t), xs(t)) + ξi(t)])

(cid:26)(cid:88)

DxbD ˆxbD ˆxs exp

ξs, ξb

=

at

iˆxa(t)(xa(t + ∆) − xa(t) − ∆[−λxa(t) + φa(xb(t), xs(t))

(cid:27)(cid:29)

P (xs) =

=

+ ξa(t)]) +

it

(cid:28)(cid:90)
(cid:89)
(cid:28)(cid:90)

it

(cid:90)

=

DxbD ˆxbD ˆxs eH

where DxbD ˆxb and D ˆxs are shorthands for the integrations (cid:81)

it dxi(t)dˆxi(t)/2π and

=

ξs, ξb

(2.4)

4

(cid:81)

it

(cid:88)
at dˆxa(t)/2π respectively and the action H
H =
(cid:88)

at

iˆxa(t)(xa(t + ∆) − xa(t) − ∆[−λxa(t) + φa(xb(t), xs(t))]) − ∆
2

Σa ˆxa(t)ˆxa(t)

iˆxi(t)(xi(t + ∆) − xi(t) − ∆[−λxi(t) + φi(xb(t), xs(t))]) − ∆
2

Σi ˆxi(t)ˆxi(t)

(2.5)

The ˆxs(t), ˆxb(t) are auxiliary variables one introduces to represent the δ function enforcing
the dynamics (2.1) in terms of an exponential. In the ﬁnal step of (2.4) we have applied a
standard Gaussian identity to average over the Gaussian noises, i.e.

(cid:104)ei∆ ˆx·Tξ·(cid:105)ξ· = e−∆ ˆx·T Σ· ˆx·/2

· = s, b

(2.6)

We have used the Itˆo convention [23] to discretize the noise and ξi(t) above is to be read as
the average of the continuous-time noise over the time interval [t, t + ∆], with covariance

(cid:104)ξi(t)ξi(t(cid:48))(cid:105) =

1
∆

(cid:104)ξa(t)ξa(t(cid:48))(cid:105) =

1
∆

Σiδtt(cid:48)

(2.7)
Here δtt(cid:48)/∆ is the discrete-time analogue of δ(t − t(cid:48)). Note that (2.4) can be viewed as a
partition function that normalizes the posterior distribution of hidden trajectories given the
observed trajectory. Because of the conditioning on the observations it is not equal to unity,
so that one has to be careful not to rely on consequences – such as the vanishing of all
moments of ˆxb and ˆxb – that would otherwise follow from this.

Σaδtt(cid:48)

The essence of our approach is to treat the interacting terms φi(xb(t), xs(t)) and
φa(xb(t), xs(t)) within the Extended Plefka Expansion, thus generalizing the approach in [12]
to the presence of observations. The ﬁrst step is to decide what averages to ﬁx as order
parameters for the expansion; we choose the ﬁrst and second moments of the ﬂuctuating
quantities (i.e. the ones we integrate over) xb(t), ˆxb(t) and ˆxs(t). Because we assume that
the trajectory of the observed variables has been observed, the xs(t) are known ∀t and do
not need to be estimated. By analogy with the notation in [12] we introduce shorthands for
the quantities to be averaged and for the order parameters they deﬁne

ˆmb = {xb,−i ˆxb, xbxb,−i ˆxbxb, i ˆxbi ˆxb}
ˆms = {−i ˆxs, i ˆxsi ˆxs}
mb = {µb,−i ˆµb, C bb, Rbb, Bbb}
ms = {−i ˆµs, Bss}

(2.8a)

(2.8b)

(2.8c)

(2.8d)

5

where the components of mb and ms are explicitly deﬁned as

µi(t) = (cid:104)xi(t)(cid:105)
ˆµi(t) = (cid:104)ˆxi(t)(cid:105)
ˆµa(t) = (cid:104)ˆxa(t)(cid:105)

Ci(t, t(cid:48)) = (cid:104)xi(t)xi(t(cid:48))(cid:105)
Ri(t(cid:48), t) = −i(cid:104)ˆxi(t)xi(t(cid:48))(cid:105)
Bi(t, t(cid:48)) = −(cid:104)ˆxi(t)ˆxi(t(cid:48))(cid:105)
Ba(t, t(cid:48)) = −(cid:104)ˆxa(t)ˆxa(t(cid:48))(cid:105)

(2.9a)

(2.9b)

(2.9c)

(2.9d)

(2.9e)

(2.9f)

(2.9g)

These averages are the posterior moments over physical and auxiliary dynamical trajectories.
Note that we keep only the diagonal second moments: this is crucial in order to obtain an
eﬀective non-interacting description from the Plefka expansion.

The starting point of the expansion (see e.g. [12]) is to augment the original partition
α – conjugate to the chosen

function P (xs) with ﬁeld terms – denoted in our context hb
observables, and then consider the Legendre transform of the corresponding free energy

α and hs

(cid:90)

Gα(ms, mb, xs) = ln

DxbD ˆxbD ˆxs eΞα

(2.10)

(2.11)

where

Ξα = Hα + hb

α · ( ˆmb − mb) + hs

α · ( ˆms − ms)

Ξα =

+

The key of the Plefka expansion is the introduction of the parameter α here, which scales
the interacting parts of the Hamiltonian. For α = 0 one then has a non-interacting system
and the Plefka expansion is then a perturbation expansion of Gα around this point, where
one sets α = 1 at the end to recover the full Hamiltonian H ≡ H1.

it

at

In our model, Ξα is given explicitly by

(cid:88)
iˆxi(t)(cid:0)xi(t + ∆) − xi(t) + ∆λixi(t) − α∆φi(xb(t), xs(t))(cid:1) +
(cid:88)
iˆxa(t)(cid:0)xa(t + ∆) − xa(t) + ∆λaxa(t) − α∆φa(xb(t), xs(t))(cid:1) +
(cid:88)
(cid:88)
ψiα(t)(cid:0)xi(t) − µi(t)(cid:1) − ∆
liα(t)(cid:0)iˆxi(t) − iˆµi(t)(cid:1) − ∆
ˆCiα(t, t(cid:48))(cid:0)xi(t)xi(t(cid:48)) − Ci(t, t(cid:48))(cid:1) + ∆2(cid:88)
+ ∆2(cid:88)
(cid:88)
ˆBiα(t, t(cid:48))(cid:0) − ˆxi(t)ˆxi(t(cid:48)) − Bi(t, t(cid:48))(cid:1) +
(cid:88)

(cid:88)

(cid:88)

itt(cid:48)
∆2
2

+ ∆

att(cid:48)

itt(cid:48)

it

itt(cid:48)

it

laα(t)(cid:0)iˆxa(t) − iˆµa(t)(cid:1) +
ˆRiα(t, t(cid:48))(cid:0) − iˆxi(t)xi(t(cid:48)) − Ri(t(cid:48), t)(cid:1) +
(cid:88)
ˆBaα(t, t(cid:48))(cid:0) − ˆxa(t)ˆxa(t(cid:48)) − Ba(t, t(cid:48))(cid:1) +

at

(2.12)

+

∆2
2
− ∆
2

Σi ˆxi(t)ˆxi(t) − ∆
2

it

Σa ˆxa(t)ˆxa(t)

at

6

The ﬁrst two lines and the last give Hα, the Hamiltonian for the interacting dynamics (2.1)
but with the interaction terms φi(xb(t), xs(t)) and φa(xb(t), xs(t)) scaled by α. By deﬁnition
of the Legendre transform, one can derive the conjugate ﬁelds as

α = − 1
h·
∆n

∂Gα
∂m·

· = s, b

(2.13)

where as in (2.12) we choose n = 1 for linear ﬁelds and n = 2 for quadratic ones to obtain
well-deﬁned values in the continuous time limit ∆ → 0. The ﬁelds have components

α = {Ψb
hb
α = {ls
hs

α, ˆC bb
α, lb
α }
α, ˆBss

α , ˆRbb

α }
α , ˆBbb

that are given explicitly by

ψiα(t) = − 1
∆
−iliα(t) = − 1
∆
−ilaα(t) = − 1
∆
ˆRiα(t, t(cid:48)) = − 1
∆2
ˆCiα(t, t(cid:48)) = − 1
∆2
ˆBiα(t, t(cid:48)) = − 1
∆2
ˆBaα(t, t(cid:48)) = − 1
∆2

∂Gα
∂µi(t)
∂Gα

∂(ˆµi(t))

∂Gα

∂(ˆµa(t))
∂Gα

∂Ri(t(cid:48), t)

∂Gα

∂Ci(t, t(cid:48))

∂Gα

∂Bi(t, t(cid:48))

∂Gα

∂Ba(t, t(cid:48))

(2.14a)

(2.14b)

(2.15a)

(2.15b)

(2.15c)

(2.15d)

(2.15e)

(2.15f)

(2.15g)

The original dynamics has no biasing ﬁelds so the condition that deﬁnes the physical values
of the order parameters is simply

h·
α = 0

· = s, b

(2.16)

If one now Taylor expands to second order in α as Gα = G0 + αG1 + (α2/2)G2, and similarly
for hα, then one sees that the physical order parameter values are obtained in the non-
interacting (α = 0) theory by applying eﬀective ﬁelds given by
· = s, b

(2.17)

h2 ·

heﬀ · = −αh1 · − α2
2

One has from (2.12)

(cid:20)(cid:88)

= −∆

∂Ξα
∂α

(cid:21)

iˆxi(t)φi(xb(t), xs(t))

(cid:88)

it

(2.18)

iˆxa(t)φa(xb(t), xs(t)) +

at

7

and inserting this into the general expressions for G1 and G2 [12] gives

(cid:19)

Kajµj(t)

+

(cid:88)

it

(cid:18)(cid:88)

iˆµi(t)

Jijµj(t) +

(2.19)

(cid:19)(cid:21)

Kiaxa(t)

(cid:88)

a

Jabxb(t) +

G1 =

and

G2 =

∂α
= − ∆

0

(cid:29)
(cid:28) ∂Ξα
(cid:20)(cid:88)
(cid:28)(cid:18)
(cid:18) ∂Ξα
(cid:20)(cid:88)

∂α

δ

at

= ∆2

ajtt(cid:48)

=

iˆµa(t)

(cid:18)(cid:88)
(cid:19)(cid:19)2(cid:29)

b

=

0

(cid:88)

j

(cid:88)

ijtt(cid:48)

j

(cid:88)

ijtt(cid:48)

ajδBa(t, t(cid:48))δCj(t, t(cid:48)) +
K 2

ijδBi(t, t(cid:48))δCj(t, t(cid:48)) +
J 2

JijJjiδRi(t, t(cid:48))δRj(t(cid:48), t)

where δCi(t, t(cid:48)), δBa/i(t, t(cid:48)) and δRi(t, t(cid:48)) denote the connected correlators and responses, e.g.
δCi(t, t(cid:48)) = Ci(t, t(cid:48)) − µi(t)µi(t(cid:48)). In the expression for G1 we have assumed that there are
no self-interactions (Jaa = Jii = 0) so that all averages of products decouple into averages
of variables at diﬀerent nodes. In G2 as written in (2.20) we have simpliﬁed by using that,
when one takes the variation of (2.18), all terms containing non-ﬂuctuating quantities (i.e.
observations) vanish. In addition, when the square is calculated, some of the contractions
among terms are zero because of the factorization among diﬀerent nodes in the eﬀective
dynamics at α = 0. The last term, which contains anti-causal responses, must be retained
at this level as it contributes to the derivatives (2.15) deﬁning the ﬁelds.

By taking derivatives of G1 and G2 we can then ﬁnd the eﬀective ﬁelds (at α = 1) from

(2.17)

(cid:90) ∞

j

0

dt(cid:48)(cid:88)
(cid:90) ∞
dt(cid:48)(cid:88)
(cid:90) t
dt(cid:48)(cid:88)
(cid:88)

a

0

0

j

Kiaxa(t) +

iˆµi(t(cid:48))J 2

ijδCj(t, t(cid:48))

JijJjiµi(t(cid:48))δRj(t(cid:48), t)

Kabxb(t) +

iˆµa(t(cid:48))K 2

ajδCj(t, t(cid:48))

iˆµj(t)Jji +

JijJjiδRj(t(cid:48), t)iˆµi(t(cid:48))

(cid:19)

aiδBa(t, t(cid:48))µi(t(cid:48)) +
K 2

ijδBj(t, t(cid:48))µi(t(cid:48))
J 2

leﬀ
a (t) =

leﬀ
i (t) =

j

j

0

a

−

Jijµj(t) +

Kajµj(t) +

(cid:88)

(cid:88)
(cid:90) t
dt(cid:48)(cid:88)
(cid:88)
(cid:88)
i (t) = −(cid:88)
iˆµa(t)Kai −(cid:88)
dt(cid:48)(cid:18)(cid:88)
(cid:90) ∞
(cid:88)
(cid:88)

JijJjiδRj(t(cid:48), t)

aiδBa(t, t(cid:48)) +
K 2

−

a

a

0

b

j

j

i (t(cid:48), t) =
ˆReﬀ

i (t, t(cid:48)) =
ˆC eﬀ

ψeﬀ

j

a

j

(cid:88)

j

jiδBj(t, t(cid:48))
J 2

8

(2.20)

(cid:21)

(2.21a)

(2.21b)

(2.21c)

(2.21d)

(2.21e)

(cid:88)
(cid:88)

j

j

i (t, t(cid:48)) =
ˆBeﬀ

a (t, t(cid:48)) =
ˆBeﬀ

ijδCj(t, t(cid:48))
J 2

ajδCj(t, t(cid:48))
K 2

(2.21f)

(2.21g)

These can be substituted into Ξ0 to give the Plefka approximation for our partition function

P eﬀ(xs) =

DxbD ˆxbD ˆxs eHeﬀ

(2.22)

(cid:90)

iˆxa(t)(xa(t + ∆) − xa(t) + ∆λxa(t) − ∆leﬀ
iˆxi(t)(xi(t + ∆) − xi(t) + ∆λxi(t) − ∆leﬀ

( ˆBeﬀ

a (t, t(cid:48)) +
att(cid:48)
i (t, t(cid:48))xi(t(cid:48)))
ˆReﬀ

δtt(cid:48))ˆxa(t)ˆxa(t(cid:48))

Σa
∆

(2.23)

(cid:88)
a (t)) − ∆2
i (t) − ∆2(cid:88)
2
(cid:88)

t(cid:48)

ψeﬀ

i (t)xi(t) +

(cid:88)

itt(cid:48)

∆2
2

with
Heﬀ =

+

(cid:88)
(cid:88)

at

it

(cid:88)

it

− ∆2
2

Σi
∆

itt(cid:48)

( ˆBeﬀ

i (t, t(cid:48)) +

δtt(cid:48))ˆxi(t)ˆxi(t(cid:48)) + ∆

i (t, t(cid:48))xi(t)xi(t(cid:48))
ˆC eﬀ
The approximating action Heﬀ is factorized over sites as anticipated above. As it is quadratic,
it gives a Gaussian weight for the trajectories at each site. Expression (2.22) then shows
how the eﬀective ﬁelds feature in the approximate posterior (conditional on the observations)
dynamics. For the hidden nodes xi(t), one sees that leﬀ
i (t) is an eﬀective drift; an additional
i (t, t(cid:48)) is a
coloured Gaussian noise also acts on xi(t), with covariance ˆBeﬀ
memory kernel with a simple intuitive interpretation: a ﬂuctuation δxi at time t(cid:48) acts via
Kji as an eﬀective ﬁeld on xj; at time t this produces a response in xj modulated by Rj(t, t(cid:48)),
which then acts back on xi via Kij. Similar roles are played by the eﬀective ﬁelds in the
i (t, t(cid:48)) the covariance of an
auxiliary dynamics, with ψeﬀ
i (t, t(cid:48)) can also be interpreted as a memory kernel in
additional coloured noise on ˆxi(t). ˆReﬀ
this context, but it is a memory “from the future” in the sense that it provides the weight
i (t, t(cid:48)),
with which the future values of ˆxi(t) aﬀect the present ones. The ﬁelds ψeﬀ
which would be zero without observations, eﬀectively constrain the hidden dynamics to be
consistent with the observed trajectories. Finally we note that also the observed dynamics
a (t, t(cid:48)) the correlator of a Gaussian
acquires eﬀective ﬁelds, with leﬀ
coloured noise on xa(t).

i (t) being an eﬀective drift and ˆC eﬀ

a (t) a linear drift and ˆBeﬀ

i (t, t(cid:48)). Finally ˆReﬀ

i (t) and ˆC eﬀ

As result of including ﬁrst and second moments (i.e. means, responses and correlations),
the statistics of hidden (physical and auxiliary) and observed (auxiliary) trajectories is
Gaussian within our approximation:
this is known to be exact for a linear dynamics,
except for the site-diagonal structure of second moments. The posterior means of the
hidden variables can be regarded as the best estimate of the hidden dynamics, while the
equal time posterior variance quantiﬁes the degree of uncertainty for those inferred values
and quantiﬁes the inference error. Both the means and variances can be read oﬀ from

9

the integrand in (2.22), which is proportional to the approximate Gaussian posterior over
hidden trajectories conditioned on observations. The eﬀective ﬁelds that couple to linear
a (t), leﬀ
observables (ψeﬀ
i (t)) determine the posterior means; the ones that correspond
a (t, t(cid:48))) characterize the posterior
i (t, t(cid:48)), ˆBeﬀ
i (t, t(cid:48)), ˆReﬀ
to quadratic observables ( ˆC eﬀ
second moments, i.e. posterior responses and correlations.

i (t, t(cid:48)), ˆBeﬀ

i (t), leﬀ

2.1. Posterior Means

For a Gaussian distribution in general the mean is the point where the probability density
is stationary. To ﬁnd the posterior mean we therefore just have to set the derivatives of the
eﬀective action to zero

∂Heﬀ
∂ ˆxi(t)

= 0,

(cid:90) t

0

∂Heﬀ
∂xi(t)

(cid:90) ∞

= 0

dt(cid:48)(cid:18)

(2.24)

(cid:19)

i (t, t(cid:48)) + Σiδ(t − t(cid:48))
ˆBeﬀ

iˆµi(t(cid:48))

(2.25)

One ﬁnds from these conditions
∂tµi(t) = − λµi(t) + leﬀ

i (t) +

(cid:88)

dt(cid:48) ˆReﬀ

(cid:88)
i (t, t(cid:48))µi(t(cid:48)) −
Kiaxa(t) − Σiiˆµi(t)

0

= − λµi(t) +

Jijµj(t) +

j

a

which is the exact dynamics for the bulk plus an additional term with ˆµi(t) from conditioning
on observations, which can also be viewed as a nonzero mean of the noise caused by
conditioning. Another interpretation of ˆµi(t) is as a back-propagating error, as can be seen
from its dynamics. The latter follows from the second part of (2.24) as

(cid:90) ∞

(cid:90) ∞

0

∂t(iˆµi(t)) = λ(iˆµi(t)) + ψeﬀ

= λ(iˆµi(t)) −(cid:88)

i (t) −

iˆµa(t)Kai −(cid:88)

t

a

j

i (t(cid:48), t)(iˆµi(t(cid:48))) +
ˆReﬀ

i (t, t(cid:48))µi(t(cid:48)) =
ˆC eﬀ

iˆµj(t)Jji

(2.26)

By considering carefully the stationarity conditions for trajectories over a ﬁnite time T one
ﬁnds that ˆµi(T ) = 0, and the dynamics then propagates the values of µi(t) backwards from
there. This dynamical evolution depends on the ˆµa(t), which in turn depend on the dynamics
of observations. In fact one has from the analog for observations of conditions (2.24), i.e.
∂Heﬀ/∂ ˆxa(t) = 0
∂txa(t) = − λxa(t) + leﬀ

(cid:90) ∞

iˆµa(t(cid:48)) =

(cid:19)

(cid:88)
a (t) −

0

Kajµj(t) +

a (t, t(cid:48)) + Σaδ(t − t(cid:48))
ˆBeﬀ
Kabxb(t) − Σaiˆµa(t)

= − λxa(t) +

dt(cid:48)(cid:18)
(cid:88)

(2.27)

j

b

Together, the above equations for the posterior means constitute a forward-backward
propagation, with the ˆµi responsible for the backward portion, i.e. for the ﬂow of information
from the future. The forward-backward structure is consistent with the general theory of

10

conditional stochastic processes, for which the estimation of posterior distributions requires
information to propagate both in the forward and backward direction [24].

To summarize, the averages of auxiliary variables do not vanish identically; instead they
eﬀectively implement the constraints given by the observations. In addition, one can show
that (2.25), (2.26), (2.27) map exactly onto the time evolution for means and ﬁelds obtained
in [15] by saddle point approximation, provided that one considers small couplings so that
the hyperbolic tangents in the mean spin dynamics of [15] can be linearized.

2.2. Posterior Variance

The inverse covariance of the approximating Gaussian distribution can be read oﬀ from the
path integral representation of the likelihood (2.22). In particular it consists of two distinct
blocks, as the distribution is factorized w.r.t. the subnetwork s and bulk b indices. These
blocks can be written symbolically as
− ˆC eﬀ

(cid:33)

(cid:32)

i (t, t(cid:48))

(∂t(cid:48) + λ)δ(t − t(cid:48)) − ˆReﬀ
− ˆBeﬀ

i (t(cid:48), t)
i (t, t(cid:48)) − Σiδ(t − t(cid:48))

C−1
i gen(t, t(cid:48)) =

(∂t + λ)δ(t − t(cid:48)) − ˆReﬀ
a gen(t, t(cid:48)) = − ˆBeﬀ
C−1

i (t, t(cid:48))
a (t, t(cid:48)) − Σaδ(t − t(cid:48))

where the “gen” subscript indicates a “generalized” inverse covariance including the auxiliary
variables ˆx·(t), i.e. (cid:104)(x·(t) − i ˆx·(t))(x·(t(cid:48)) − i ˆx·(t(cid:48)))T(cid:105) (· = s, b). In fact C−1
a gen(t, t(cid:48)) refers
only to the auxiliary variables ˆxs(t) as the xs(t) are ﬁxed by the observations. In principle
all of the second order functions are connected but in what follows we drop the δs for the
sake of brevity. The covariance itself then has the same block structure

(cid:32)

(cid:33)

Ci gen(t, t(cid:48)) =

Ci(t, t(cid:48)) Ri(t, t(cid:48))
Ri(t(cid:48), t) Bi(t, t(cid:48))

Ca gen(t, t(cid:48)) = Ba(t, t(cid:48))

(cid:18)
i (τ, t(cid:48)) + Ri(t, τ )

Note that here and below we write directly the continuous time equations that are obtained
from the discrete time formalism as ∆ → 0 (see for details [25]). To ﬁnd the equations for
the (generalized) covariances, one just takes the identity Ci genC−1
i gen = 1 block by block to
(cid:19)(cid:21)
get(cid:90) +∞
(cid:90) +∞
(cid:90) +∞

(cid:19)(cid:21)
i (τ, t(cid:48))
i (τ, t(cid:48)) + Σiδ(τ − t(cid:48))
ˆBeﬀ
(cid:19)(cid:21)

(∂t(cid:48) + λ)δ(t(cid:48) − τ ) − ˆReﬀ

(∂τ + λ)δ(τ − t(cid:48)) − ˆReﬀ

− Ci(t, τ ) ˆCeﬀ

= δ(t − t(cid:48))

− Ri(t, τ )

(cid:18)
(cid:18)

(cid:18)
(cid:18)

(cid:20)
(cid:20)
(cid:20)

dτ

Ci(t, τ )

(2.28a)

dτ

dτ

Ri(τ, t)

(∂t(cid:48) + λ)δ(t(cid:48) − τ ) − ˆReﬀ

− Bi(t, τ )

i (τ, t(cid:48)) + Σiδ(τ − t(cid:48))
ˆBeﬀ

(cid:19)
i (t(cid:48), τ )
(cid:19)
i (t(cid:48), τ )

= 0

(2.28b)
= δ(t − t(cid:48))

−∞

−∞

−∞

11

(2.28c)

˜Ba(z) +

J 2
ji

˜Bj(z)

+ ˜Ri(z)

JijJji

˜Rj(z)

(cid:19)

− ˜Ri(z)

(cid:19)
(cid:19)(cid:18)(cid:88)
(cid:18)(cid:88)

a

−

(cid:19)

(cid:19)

(cid:18)
z + λ −(cid:88)
(cid:18)(cid:88)

j

J 2
ij

˜Cj(z) + Σi

(cid:19)

j

K 2
ai

˜Ba(z) +

J 2
ji

˜Bj(z)

= 0

(cid:19)−1

(cid:88)
(cid:19)(cid:21)

j

JijJji

˜Rj(−z)

JijJji

˜Rj(−z)

(cid:88)

j

− ˜Ci(z)

˜Ci(z)

˜Bi(z)

a

K 2
ai

(cid:18)(cid:88)
(cid:18)
− z + λ −(cid:88)
(cid:20)(cid:18)
− z + λ −(cid:88)
(cid:18)
z + λ −(cid:88)
(cid:88)

(cid:20)

j

j

j

˜Ba(z)

Σa +

K 2
aj

˜Cj(z)

and similarly for Ca gen

(cid:90) +∞

−∞

−

(cid:20)

dτ Ba(t, τ )

(cid:21)
a (τ, t(cid:48)) + Σaδ(τ − t(cid:48))
ˆBeﬀ

= δ(t − t(cid:48))

(2.29)

i (t(cid:48), t), ˆC eﬀ

i (t, t(cid:48)), ˆBeﬀ

i (t, t(cid:48)) and ˆBeﬀ

a (t, t(cid:48)) from
If we now substitute the expressions for ˆReﬀ
(2.21) we get a closed system of integral equations. To simplify these, we consider long times,
where a stationary regime is reached so that all two-time functions become time translation
invariant (TTI). In an inference problem, stationarity implies also that the quality of the
prediction is the same at all times. At stationarity, the integrals in the above integral
equations (2.28) and (2.29) become convolutions and hence, if we go to double-sided Laplace
transform, simple products. After some simpliﬁcation the Laplace-transformed equations
yield a system of four coupled equations for ˜Ci(z), ˜Ri(z), ˜Bi(z), ˜Ba(z)

= 1

(2.30a)

(2.30b)

JijJji

˜Rj(z)

J 2
ij

˜Cj(z) + Σi

(cid:21)

j

= −1

= 1

(2.30c)

(2.30d)

j

2.3. Thermodynamic Limit

We expect the extended Plefka approach to give exact values for posterior means and
variances in the case of mean-ﬁeld type couplings (i.e. weak and long ranged),
in
the thermodynamic limit of an inﬁnitely large system. More precisely we deﬁne the
thermodynamic limit as the one of an inﬁnitely large bulk and subnetwork, N b, N s → ∞ at
constant ratio α = N s/N b. For our mean-ﬁeld couplings we assume, as in [12], that {Jij} is
a real matrix belonging to the Girko ensemble [26], i.e. its elements are independently and
randomly distributed Gaussian variables with zero mean and variance satisfying

(cid:104)JijJij(cid:105) =

(cid:104)JjiJij(cid:105) =

j2
N b

η j2
N b

12

(2.31)

(2.32)

The parameter η ∈ [−1, 1] controls the degree to which the matrix {Jij} is symmetric. In
particular, the dynamics is non-equilibrium – it does not satisfy detailed balance – whenever
η < 1. Simiarly, {Kai} is taken as a random matrix with uncorrelated zero mean Gaussian
entries of variance

(cid:104)Kai(cid:105) = 0

(cid:104)K 2

ai(cid:105) =

k2
N b

(2.33)
We have introduced amplitude parameters j for {Jij} (hidden-to-hidden) and k for {Kai}
√
N b
(hidden-to-observed) here. The scaling of both types of interaction parameters with 1/
ensures that, when the size of the hidden part of the system increases, the typical contribution
it makes to the time evolution of each hidden and observed variable stays of the same order.
The high connectivity, where all nodes interact with all other ones, implies that in the
thermodynamic limit all nodes become equivalent. Local response and correlation functions
therefore become identical to their averages over nodes, deﬁned as

(cid:88)
(cid:88)
(cid:88)

j

j

j

˜Rbb|s(z) =

˜C bb|s(z) =

˜Bbb|s(z) =

1
N b

1
N b

1
N b

˜Rj(z)

˜Cj(z)

˜Bj(z)

(2.34)

(2.35)

(2.36)

As a consequence, all site indices can be dropped and the correlation and response functions
can be replaced by their mean values, ˜Ci(z) ≡ ˜C bb|s(z), ˜Ri(z) ≡ ˜Rbb|s(z), ˜Bi(z) ≡ ˜Bbb|s(z),
and similarly ˜Ba(z) ≡ ˜Bss|s(z). The superscripts here emphasize that we are considering
moments conditioned on subnetwork values though for brevity we drop them in the rest
of the calculation. Of primary interest is then ˜C(z), the Laplace transformed posterior
(co-)variance function of prediction errors, which as a site-average can be thought of as a
macroscopic measure of prediction performance. This should become self-averaging in the
˜Rj(z). Replacing ˜Rj by
˜R, the prefactor is a sum of N b terms so converges to its average N bηj2/N b = ηj2 in the

thermodynamic limit. To see this, consider e.g. the sum (cid:80)
thermodynamic limit. So we can replace(cid:88)

j JijJji

JijJji

˜Rj(z) ∼ η j2 ˜R(z)

Making this and similar substitutions in the system (2.30), and choosing scalar noise
covariances Σi = σ2

s as in [17], one ﬁnds

b and Σa = σ2

(cid:18)

− C(z)

(cid:18)

αk2

s + k2 ˜C(z)
σ2

(cid:19)

+ j2 ˜B(z)

+ ˜R(z)

z + λ − ηj2 ˜R(z)

˜C(z)

− z + λ − ηj2 ˜R(−z)

− ˜R(z)

j2 ˜C(z) + σ2
b

= 0

(cid:19)

= 1

(2.37a)

(2.37b)

(cid:19)

j

(cid:19)

(cid:18)

(cid:18)

13

(cid:20)(cid:18)
(cid:18)

−

˜B(z)

− z + λ − ηj2 ˜R(−z)

(cid:19)(cid:21)

j2 ˜C(z) + σ2
b

= 1

(cid:19)(cid:18)

(cid:19)−1(cid:18)

+ j2 ˜B(z)

z + λ − ηj2 ˜R(z)

(cid:19)

−

αk2

s + k2 ˜C(z)
σ2

(2.37c)

Here ˜Bss|s(z) = −1/(σ2
(2.37c).

s + k2 ˜C(z)) has already been substituted into equations (2.37a) and

We comment brieﬂy on the relation of the above results to the Fluctuation Dissipation

Theorem (FDT), which in terms of Laplace transforms reads

(cid:2) ˜R(z) − ˜R(−z)(cid:3)

This can be compared with the expression for ˜R(z) that follows from (2.37b) (taken for z
and −z)

z ˜C(z) = −σ2
b
2

λ

(cid:20)
(cid:2) ˜R(z) − ˜R(−z)(cid:3) = z ˜C(z)

b + j2(1 + η) ˜C(z)
σ2

˜R(z) = ˜C(z)

− σ2
b
2

(cid:21)

−

z

j2(1 − η) ˜C(z) + σ2

b

σ2
b

j2(1 − η) ˜C(z) + σ2

b

(2.38)

(2.39)

(2.40)

The r.h.s. of the FDT is then

Comparing with (2.38), the FDT is satisﬁed for symmetric couplings (η = 1) as expected,
while there are progressively stronger deviations from FDT as η decreases towards −1.

2.4. Comparison with known results for particular cases

As a consistency check and to support our claim of exactness in the thermodynamic limit,
we brieﬂy look at particular cases for which ˜C(z) and ˜R(z) can be worked out by alternative
means.

2.4.1. α = 0. This case corresponds to the absence of observations. One has then ˜Bi(z),
˜Ba(z) ≡ 0 as these quantities simply play the role of Lagrange multipliers enforcing the
conditioning on observations. To see this formally from the α → 0 limit of (2.37) one sets
˜Bi(z) = α ˜Di(z) and ˜Ba(z) = α ˜Da(z) where the ˜D stay nonzero for α → 0. One veriﬁes
that under this assumption the system has as solution the responses and correlations known
from [12]

˜R(z) =

1
2η

(z + λ) − 1
2η

(cid:2)(λ + z) +(cid:112)(λ + z)2 − 4j2η(cid:3)(cid:2)(λ − z) +(cid:112)(λ − z)2 − 4j2η(cid:3) − 4

4 Σ

˜C(z) =

(2.41)

(2.42)

(cid:112)(z + λ)2 − 4j2η

14

2.4.2. j = 0.
λ. By solving (2.37), one obtains

˜C(z) =

σ2
s
2k2

σ2

(λ2 − z2)

1 − α −

(cid:26)

In this situation, hidden variables are coupled only via a self-interaction term

(cid:115)(cid:20)

(cid:18) λ2 − z2

(cid:19)

σ2

+

1 − α −

(cid:18) λ2 − z2

(cid:19)(cid:21)2

(cid:18) λ2 − z2

(cid:19)(cid:27)

σ2

+ 4

σ2

(2.43)

where σ = σbk/σs. This coincides with the result in [17], which can be derived separately
using methods from random matrix theory.

2.4.3. η = 1. Here we have symmetric hidden-to-hidden couplings. In general, from (2.37b)
we can get an expression for ˜R(z) in terms of ˜C(z): this is (2.39). Substituting into (2.37a),
˜B(z) can also be worked out as a function of ˜C(z). Using these expressions for ˜R(z) and
˜B(z) in equation (2.37c), one gets a closed algebraic equation for the posterior variance ˜C(z).
While this equation is in general very complicated, for η = 1 it can be simpliﬁed to

z2 = − σ2
b
˜C(z)

+

b

α k2σ2
σ2
s
˜C(z)
1 + k2
σ2
s

+

j2
1 + j2
σ2
b

˜C(z)

+

(cid:20)

λ2

1 + 2 j2
σ2
b

˜C(z)

(cid:21)2

(2.44)

This ﬁfth order equation for the single site posterior covariance predicted by the extended
Plefka expansion with hidden nodes is again conﬁrmed by random matrix theory results [17].

2.4.4. η = 0. Finally we consider the asymmetric hidden interactions case. ˜C(z) is the
solution of a second order equation if σs = σb and k = j (otherwise we would have a higher
order equation)

−(cid:2)(α − 1)j2 + (λ2 − z2)(cid:3) +

(cid:113)(cid:2)(α − 1)j2 + (λ2 − z2)(cid:3)2 + 4αj4

(cid:27)

(cid:26)

(2.45)

˜C(z) =

σ2
b
2αj4

This is the same expression as obtained by calculations in [17] using an explicit average over
the quenched disorder variables Jij and Kia.

3. Power spectrum and critical regions

In this section we study the predictions of the extended Plefka approach for our observed
dynamical system, focussing on the power spectrum of the posterior covariance. This is
given by ˜C(iω), the Laplace transform ˜C(z) evaluated at z = iω.

3.1. Dimensionless system

We would like to understand how ˜C(ω) depends on the parameters λ, j, k, σs, σb, η and α.
The last two of these are already dimensionless. By extracting the appropriate dimensional
scales, we can reduce the other ﬁve parameters to only two dimensionless combinations.

15

From (2.1) one sees that j, k, λ have dimensions t−1, while the dimension of σ2

s/b is
x2t−1. We can build from these the dimensionless parameters γ = j/λ and p = λ/σ. Here
σ = σbk/σs, which has dimension t−1 and contains the observation “intensity” k as well as
the ratio between the dynamical noises σb/σs. The latter is a third dimensionless parameter
but as it only enters one prefactor we will not need to keep it separately.

Extracting appropriate dimensional amplitudes for all four two-point functions, we write

them as

˜C(iω) =

˜R(iω) =

˜B(iω) =

˜Bss(iω) =

σ2
s

k2Cα,γ,η,p(Ω)
Rα,γ,η,p(Ω)
1
j
Bα,γ,η,p(Ω)
1
σ2
b
Bss
1
α,γ,η,p(Ω)
σ2
s

(3.1a)

(3.1b)

(3.1c)

(3.1d)

Here Ω = ω/σ is a dimensionless frequency; similarly Cα,γ,η,p(Ω), Rα,γ,η,p(Ω), Bα,γ,η,p(Ω) and
Bss
α,γ,η,p(Ω) are dimensionless and depend on the dimensionless parameters α, γ, η and p:
for the sake of brevity, we do not write the subscripts indicating this dependence in the
following.

Let us brieﬂy comment on (3.1a). One sees that ˜C(z) is directly proportional to σ2

s and
inversely proportional to k2: the weaker the hidden-to-observed coupling and the stronger
the dynamical noise acting on the observed variables, the less information one can extract
from the subnetwork trajectories and the more uncertain the predictions for the behaviour
of the bulk.

To summarize, we switch from eight original parameters {α, η, λ, j, k, σs, σb, ω} to a set
of ﬁve dimensionless parameters {α, η, γ, p, Ω}. For the dimensionless second moments (3.1),
the system (2.37) becomes

(cid:18)

− C

(γp)C

(cid:20)(cid:18)

B

(cid:19)

(cid:19)
(cid:19)(cid:18)

(cid:18)

− α
1 + C + (γp)2B
− iΩ + p − η γ pR−

− iΩ + p − η γ pR−

(cid:18)

(cid:18)

(cid:19)

(cid:19)
(cid:19)−1(cid:18)

= 0

+ (γp)−1R

iΩ + p − η γ pR

= 1

(3.2a)

− R
(γp)2 C + 1
− α
1 + C + (γp)2B

(cid:19)

(3.2b)

(cid:21)

iΩ + p − η γ pR

− (γp)2C − 1

= 1

(3.2c)
where we have dropped the frequency argument and introduced the shorthand R− = R(−Ω).

16

3.2. Critical scaling
We next analyze in the parameter space α, γ, p (at ﬁxed η) the singularities of C(0), the
(dimensionless) zero frequency posterior covariance. The behaviour of the inference error
itself, the equal time posterior variance C(t − t) = C(0), will be addressed in section 4.3.
Independently of η, we ﬁnd two critical regions that are shown graphically in ﬁgure 1:

(i) ∀p, α = 0 and γ > γc
(ii) ∀γ, p = 0 and 0 < α < 1
The ﬁrst case retrieves the dynamics without observations, for which γ < γc = 1/(1 + η) is
the condition of stability beyond which trajectories typically diverge in time. Interestingly,
as soon as α > 0, the constraints from observations make the solution stable irrespective of
whether γ is smaller or bigger than the critical value. For γ > γc the observed trajectories
would then be divergent, and so would the predicted hidden trajectories, while the error
(posterior variance) of the predictions would remain bounded. It is diﬃcult to conceive of
situations where divergent mean trajectories would make sense, however, so we only consider
the range γ ≤ γc in our analysis.

The second limit, p → 0, corresponds, for ﬁxed ratio between noises σs/σb, to k (cid:29) λ: we
call this scenario an “underconstrained” hidden system. In general for large k the posterior
variance decreases as 1/k2, as used in the scaling (3.1a). But for α < 1 there are directions in
the space of hidden trajectories that are not constrained at all by subnetwork observations,
and their variance will scale as 1/λ2 instead. These directions give a large contributions to
the dimensionless C(Ω) that diverges in the limit k (cid:29) λ. In general one has a similar eﬀect
when k/σs (cid:29) λ/σb, where the noise in the dynamics acts to eﬀectively reduce the relevant
interaction or decay constant.

This behaviour is broadly analogous to what happens in learning of linear functions
from static data [10, 11]: there the prediction error will also diverge when α, which is then
deﬁned as the ratio of number of examples to number of spatial dimensions, is less than
unity and no regularization is applied.

Close to the two critical regions in the space of α, γ and p, C(0) is expected to exhibit
a power-law dependence on the parameters specifying the distance away from the critical
point. We can then study this scaling in the critical region, perform a systematic rescaling
analysis, as is standard in the study of phase transitions [27]. The aim of the analysis, which
we describe in the next two sections, is to ﬁnd the master curves and associated exponents
that describe the approach to the critical point(s).

3.3. Master curves for γ → γc and α → 0
We begin with the behaviour of the posterior covariance power spectrum for γ → γc and
α → 0, i.e. in the vicinity of the “no observations” critical point.

17

Figure 1. Parameter space spanned by α, γ, p. The blue stripe and the red area mark
the values for which the posterior covariance C(0) becomes singular, i.e. respectively α = 0,
γ > γc (∀p) and p = 0, 0 < α < 1 (∀γ).

We already know the limit curve for the power spectrum: it is given by the spectrum at
the critical point. At this point we have an α = 0 system, i.e. without observations (see [12]
and our summary in section 2.4.1). As there the interactions and noise level relating to
observed nodes are then irrelevant, we can set k = j, σb = σs (i.e p = 1/γ, σ = j and
Ω = ω/j). From (2.42) the dimensionless correlation is then

Cγ,η(Ω) =

(cid:20)

(cid:113)(cid:0) 1
γ + iΩ(cid:1)2 − 4η

(cid:21)(cid:20)

4

1

γ − iΩ +

(cid:113)(cid:0) 1
γ − iΩ(cid:1)2 − 4η

(cid:21)

1
γ + iΩ +

(3.3)

− 4

σ2

C(0) becomes singular when γ = γc = 1/(1 + η), as (3.3) then has a pole at Ω = 0. The
approach to (3.3) at decreasing α is plotted in ﬁgure 2 (left).

3.3.1. η = 1. To explain the rescaling procedure for understanding the approach to the
singularity at γ = γc, we ﬁrst focus on the case η = 1. From (2.44) one then has an algebraic
equation for the power spectrum C(Ω)
α
1 + C +

(cid:2)1 + 2(γp)2C(cid:3)2

− Ω2 = − 1

(γp)2

1 + (γp)2C +

C +

(3.4)

p2

The distance from the singularity is controlled by α itself and δγ = γc − γ. Approaching
the singularity along one of these two directions we get two distinct power law divergences
of C(0); a third direction of approach is from nonzero Ω at α = δγ = 0.

18

Approaching along the δγ-direction (δγ → 0 at α = Ω = 0) we ﬁnd from (3.4)

For α → 0 at Ω = 0 and γ = γc, with γc = 1/2 as η = 1, the result is

√
C(0) ∼ 1
p2
δγ

C(0) ∼ α− 1

3

p2

Finally for Ω → 0 at α = δγ = 0 the low frequency tail of C(Ω) is known from [12] as

These three power laws can be combined into the scaling

C(Ω) ∼ 1√
Ω

¯C( ¯Ω, ¯α)

C(Ω) =

¯Ω

¯α

=

=

√
1

p2

Ω

δγ

2 p δγ

α

16 δγ 3

2

(3.5)

(3.6)

(3.7)

(3.8)

(3.9)

(3.10)

where the exponents of δγ in ¯Ω and ¯α are ﬁxed by standard arguments (see [25] for details).
Inserting this ansatz into (3.4) and taking the limit δγ → 0 gives

− 1 + ¯C2 + ¯α ¯C3 +

¯C4 = 0

(3.11)
This equation implicitly determines the master curve, i.e. the scaling function ¯C( ¯Ω, ¯α). This
master curve describes the power spectrum in the region where δγ, α and Ω are all small
but ¯Ω and ¯α are ﬁnite, which requires in particular that the dimensionless frequency Ω must
be of the order of ¯Ω.

¯Ω2
4

3.3.2. −1 < η < 1. Let us now consider η < 1, for which we need to work with the
entire system (3.2). For δγ → 0 at α = 0 one ﬁnds amplitudes C(0) ∼ (1 − η)/2 δγ p2 and
B(0) ∼ −4 δγ2(1 + η)4/p4. At δγ = Ω = 0, we get C(0) ∼ α−1/2. For the third direction
we can read oﬀ from [12] that at α = δγ = 0, the low-frequency tail of the power spectrum
is given by 1/Ω2 for η < 1. Comparing the ﬁrst and third expression suggests to deﬁne a
crossover frequency from 1/Ω2 = (1 − η)/2 δγ p2, giving Ω ∼ p
2 δγ. Using this we deﬁne

√

19

scaling functions for C and B as

C(Ω) =

1 − η
2 δγ p2

¯C( ¯Ω, ¯α)

B(Ω) = − 4 δγ2(1 + η)4

¯B( ¯Ω, ¯α)

(cid:114)1 + η

2 δγ

1 − η2
Ω

p (1 − η)

Ω
Ω∗ =
α (1 − η)
4 δγ2(1 + η)3

¯Ω

¯α

=

=

(3.12)

(3.13)

(3.14)

(3.15)

The somewhat complicated looking prefactors are chosen here to give scaling functions that
will be independent of p and η. The response R, which also features in the original equations
(3.2), does not need to be rescaled as it turns out to be equal to unity to leading order.

We insert the above rescalings into the system (3.2) and again look at the limit δγ → 0.
Some care is needed as there are competing orders of δγ in the equations so that one has to
expand the response R not just to O(1) but to O(δγ). One then ﬁnds simply ¯B( ¯Ω, ¯α) = ¯α
and this makes sense: at α = 0 we must retrieve the results of [12], where the normalization
of the MSRJD path integral leads all moments of auxiliary variables to vanish. The master
curve for the posterior covariance spectrum can also be obtained explicitly, as

¯C( ¯Ω, ¯α) =

1
2¯α

− (1 + ¯Ω2) +

4¯α + (1 + ¯Ω2)2

(3.16)

√
It has the limits ¯C| ¯α→0 ∼ 1/(1 + ¯Ω2), ¯C| ¯Ω→∞ ∼ 1/ ¯Ω2 and ¯C| ¯α→∞ ∼ 1/
¯α. The latter tells us
how the prediction error decreases in the regime where α is still small but larger than δγ2.
The ﬁt provided by the master curve (3.16) for diﬀerent small values of α is shown in ﬁgure
2 (left).

(cid:20)

(cid:113)

(cid:21)

3.3.3. Crossover at η ≈ 1. Above we found diﬀerent power law behaviours and scaling
functions for η = 1 and η < 1, thus a crossover must occur at η ≈ 1. To see this, the two
cases η = 1 and η < 1 can be analyzed as limit cases of a more general scaling ansatz that
accounts explicitly for the eﬀect of  = 1 − η, i.e. the distance from the symmetric value
η = 1. This becomes an additional critical parameter that enters the scaling functions in
the combination

¯ = /(cid:112)δγ

(3.17)

20

We deﬁne these scaling functions via

¯C( ¯Ω, ¯α, ¯)

¯B( ¯Ω, ¯α, ¯)

C(Ω) =

√
(2 + ¯)
δγ p2
2
B(Ω) = − 32 δγ 3
2 + ¯
Ω

2

¯Ω

=

p (2 + ¯)δγ
α (2 + ¯)
32 δγ 3

2

¯α

(3.21)
We then ﬁnd again ¯B = ¯α while the master curve ¯C( ¯Ω, ¯α, ¯) solves a fourth-order equation

=

(8 + ¯C ¯(2 + ¯))2(−4 + ¯C(2 + ¯)(−¯ + ¯C(1 + ¯α ¯C)(2 + ¯))) + ¯C4(2 + ¯)6 ¯Ω2 = 0

(3.22)

The solution of this for a range of diﬀerent ¯ is plotted in ﬁgure 3 (left).
√
δγ ∼ 2

The two previous cases η = 1 and η < 1 (with δγ → 0) are recovered as the limits
√
respectively for ¯ → 0 and ¯ → ∞. In the ﬁrst limit, (2 + ¯)
δγ and the rescaling
relations (3.8), (3.9), (3.10) for η = 1 are retrieved as they should be; accordingly, the
√
equation (3.22) for the master curve becomes exactly (3.11). On the other hand, when
¯ → ∞, (2 + ¯)
δγ =  and we recover the rescalings (3.12)-(3.15) adopted for
η < 1; in this case (3.22) reduces to

√
δγ ∼ ¯

− 1 + ¯C + ¯α ¯C2 + ¯C ¯Ω2 = 0

(3.23)

whose positive solution is given by (3.16).

3.4. Master curves for α → 1 and p → 0
In this section we look at the scaling around the second critical region, α → 1 (i.e. N s = N b)
and p → 0 (i.e. k (cid:29) λ at ﬁxed σs/σb). As will be shown, the results here are independent of
the degree of symmetry η of the interactions among the hidden variables. This is consistent
with the intuition that the critical behaviour is dominated by whether a direction in the
hidden trajectory space is constrained by observations or not, i.e. by hidden-to-observed
rather than hidden-to-hidden interactions.

At α = 1, we ﬁnd the following power law scaling with p of the amplitudes

(3.18)

(3.19)

(3.20)

(3.24)

(3.25)

1

p(cid:112)γ2 + 1
γp(cid:112)γ2 + 1

C(0) ∼
R(0) ∼
B(0) ∼ −1

(3.26)
Approaching from the other direction, p = 0, gives C(0) ∼ 1/δα, with δα = α − 1. At α = 1
and p = 0, ﬁnally, one ﬁnds for small nonzero frequency Ω that C(Ω) ∼ 1/Ω. Equating

21

the three divergences above identiﬁes a crossover frequency Ω∗ = p(cid:112)γ2 + 1 and similarly a

characteristic value for δα. We thus deﬁne rescaled quantities again

¯C( ¯Ω, δ ¯α)
¯R( ¯Ω, δ ¯α)

1

p(cid:112)γ2 + 1
γ p(cid:112)γ2 + 1

C(Ω) =
R(Ω) =
B(Ω) = − ¯B( ¯Ω, δ ¯α)
Ω
¯Ω

=

p(cid:112)γ2 + 1

Ω
Ω∗ =
δα

p(cid:112)γ2 + 1

δ ¯α =

(3.31)
Inserting into the system (3.2), taking p → 0 and keeping only the leading terms one ﬁnds
as the master curve for C(Ω)

¯C( ¯Ω, δ ¯α) =

−δ ¯α +

√
4 + δ ¯α2 + 4 ¯Ω2

2 (1 + ¯Ω2)

(3.27)

(3.28)

(3.29)

(3.30)

(3.32)

(3.34)

(3.35)

(3.36)

(3.37)

(3.38)

√
with limits ¯C|δ ¯α→0 ∼ 1/
1 + ¯Ω2, ¯C| ¯Ω→∞ ∼ 1/ ¯Ω and ¯C|δ ¯α→∞ ∼ 1/δ ¯α. From the latter one
sees again the decrease of the inference error for increasing number of observations (while
remaining in the regime studied here where δα is small, p < δα (cid:28) 1 ). The master curve
predictions for generic δ ¯α are compared to direct numerical evaluation of C in ﬁgure 2 (right).
So far we had focussed on the α → 1 end of the second critical region. As this region
covers the entire range 0 < α < 1, however, one can also study the critical behaviour as
p → 0 for ﬁxed α < 1. The crossover into this region can be seen by taking δ ¯α → −∞,
where ¯C → |δ ¯α|/(1 + ¯Ω2) from (3.32). Including the prefactor from (3.27) gives

C(0) =

1 − α

p2(γ2 + 1)(1 + ¯Ω2)

(3.33)

This suggests that in general, for ﬁnite 1 − α, C will be ∼ 1/p2. Generalizing this one ﬁnds
that the appropriate scaling for small p of all order parameters is

1

¯C( ¯Ω, δ ¯α)
¯R( ¯Ω, δ ¯α)

C(Ω) =
R(Ω) =
B(Ω) = − ¯B( ¯Ω, δ ¯α)
¯Ω

p2(cid:112)γ2 + 1
γ(cid:112)γ2 + 1
p(cid:112)γ2 + 1
|δα|(cid:112)γ2 + 1

δ ¯α =

=

=

Ω

1 − α(cid:112)γ2 + 1

22

By substitution into the system (3.2) and taking the limit p → 0 one can then obtain explicit
solutions for ¯C( ¯Ω, δ ¯α), ¯R( ¯Ω, δ ¯α), ¯B( ¯Ω, δ ¯α); as before these are independent of η. We do not
state the full expressions here but note that in the limit where 1 − α (cid:28) 1 one retrieves the
result (3.33) as required for consistency between the two scaling limits.

Finally it is useful also to look at the α-dependence of C(0) and C(Ω) in the non-critical
range α > 1. For large α we ﬁnd C(0)|α→∞ ∼ 1/α: this means that with many observations
the predictions for the hidden dynamics will become arbitrarily precise, in line with intuition.
The decrease in C(0) with increasing α can be seen in ﬁgure 4, where one observes also that
at ω ∼ O(1) all spectra collapse into a Lorentzian tail. This indicates an exponential decay
of the correlations between prediction errors in the temporal domain. As the amplitude of
the tail is largely α-independent, the typical time of this exponential decay decreases with α:
with many observations, errors in the prediction of the hidden states becomes progressively
less correlated with each other.
In the next section we look more systematically at the
information one can extract on relaxation times from the power spectra.

4. Posterior variance in the time domain
4.1. Relaxation time for γ → γc and α → 0
We look at the relaxation time, which is a measure of time correlations in the errors of inferred
hidden values. We study in particular how it depends on the number of observations and
the interaction parameters.

(cid:82) +∞

The relaxation time can be deﬁned in a mean-squared sense as

τ 2 =

−∞ t2C(t)
2 ˜C(0)

= − 1

2C(0)

d2C(ω)
d2ω

= −

1

2 ¯C(0, ¯α)

d2 ¯C( ¯Ω, ¯α)

d2ω

(cid:12)(cid:12)(cid:12)(cid:12)ω=0

(cid:12)(cid:12)(cid:12)(cid:12)ω=0

(4.1)

As for the other quantities, let us introduce the dimensionless version of this typical timescale

T = στ

(4.2)

and look separately at the two critical regions, starting in this subsection with the ﬁrst one.
As τ is determined directly from the power spectrum, its scaling form follows from that of
C(Ω).

4.1.1. η = 1. The relaxation time is rescaled as
T = T ∗ ¯τ (¯α) =

1

(4.3)
where, by the deﬁnition itself in terms of frequency (4.1), one has T ∗ ∼ 1/Ω∗ and ¯τ (¯α) is
the solution of a system of two equations

¯τ (¯α)

p δγ

(4.4)

(cid:40) (cid:0)2 + 3 ¯α ¯C(cid:1) ¯C ¯τ 2 − 1

−1 + ¯C2 + ¯α ¯C3 = 0

¯C3 = 0

4

23

Figure 2. (Left) Numerical solutions for small α: the approach to the limit curve for α → 0
with γ close to γc is plotted. This master curve and the ones for α = 0.00001, α = 0.0001 and
α = 0.001 are given by blue dashed lines with crosses. In this way one can see the amplitude
variation for relatively large ¯α, whose values are of order ∼ 102 (for α = 0.00001), 103 (for
α = 0.0001) and 104 (for α = 0.001). Even for smaller ¯α, the variation in shape with
this parameter is small: ¯α mainly aﬀects the height of the plateau close to Ω = 0 and the
position of the crossover to the large frequency Lorentzian tail. (Right) Numerical solutions
for small δα = α− 1: the approach to the limit curve for δα → 0 with p close to 0 is plotted.
This master curve and the ones for δα = 0.001, δα = 0.01 and δα = 0.1 are shown as red
dashed lines with circles. From this plot one can examine the variation with δ ¯α, whose
corresponding values are δ ¯α ∼ 1 (for δα = 0.001), δ ¯α ∼ 10 (for δα = 0.01) and δ ¯α ∼ 100
(for δα = 0.1).

which can be obtained by deriving from (3.11) one equation for ¯C(0, ¯α) and one for

d2 ¯C( ¯Ω, ¯α)/d2ω(cid:12)(cid:12)ω=0 and using relations (4.1) and (4.3). For simplicity we have denoted

¯C(0, ¯α) and ¯τ (¯α) as ¯C and ¯τ . From these two equations we note ¯C(0, ¯α)| ¯α→∞ ∼ ¯α− 1
¯τ| ¯α→∞ ∼ ¯α− 2

3 , which implies for δγ2 < α (cid:28) 1

3 and

This power law dependence is visible in ﬁgure 5 (right, see η = 1 curve).

T ∼ α− 2

3

(4.5)

4.1.2. −1 < η < 1. By applying (4.1) with (3.16), one can rescale in this regime according
to

¯τ (¯α)

(4.6)

(cid:114)1 + η

2 δγ

T =

1

(1 − η) p

24

√

√
Figure 3. (Left) Master curves for diﬀerent values of ¯, the parameter indicating eﬀectively
¯Ω tail for ¯ → 0
the distance from η = 1 when δγ is close to zero. One can see the 1/
and the 1/ ¯Ω2 one for ¯ → ∞ (blue and red curves), while intermediate values of ¯ show a
crossover between these two tails. (Right) Numerics for diﬀerent values of η, the symmetry
parameter, at α → 0 and δγ → 0. For η = 0.1, Ω∗ ∼ 3 · 10−3 and for Ω∗ (cid:28) Ω (cid:28) 1 one sees
the Lorentzian tail. For η = 1, Ω∗ ∼ 5 · 10−6 and in the range Ω∗ (cid:28) Ω (cid:28) 1 one has the
Ω tail. For η = 0.9 (i.e.  = 1− η = 0.1, ¯ ≈ 45), the results interpolate between these
∼ 1/
two regimes as expected from the left ﬁgure; the crossover occurs at Ω ∼ 0.01. In fact, the
limit ¯C| ¯Ω→∞ ∼ 1/ ¯Ω2, once one reinserts the dependence on 1 − η = , gives C(Ω) ∼ 3/Ω2
√
Ω: these two tails meet around Ω ∼ 2
for Ω∗ (cid:28) Ω (cid:28) 1, while (3.11) has a tail C(Ω) ∼ 2/
(2 = 0.01 in this case), in agreement with the results for the limit  (cid:28) 1 in the case without
observations [12]. The dashed lines with circles are master curves for Ω in the vicinity of Ω∗
for the diﬀerent η. Dotted lines with crosses trace the master curves at Ω ∼ O(1), which are
independent of α and equal to the curves at α = 0. In the relevant range of large frequencies
they behave essentially as Lorentzians.

and ¯τ (¯α) is given by

¯τ (¯α) =

1

(4¯α + 1) 1

4

with limit ¯τ| ¯α→∞ ∼ ¯α− 1

4 corresponding to
T ∼ α− 1

4

δγ2 < α (cid:28) 1

(4.7)

(4.8)

This dependence can be veriﬁed in ﬁgure 5 (right, see curve for η = 0.1).

4.1.3. Crossover at η ≈ 1. The relaxation time scalings above can be seen as limit cases of
a more general scaling linked to the parameter ¯. From the general master curve (3.22) we

25

Figure 4. C(Ω) for diﬀerent α, at small ﬁxed η. We have chosen γ and p close to their
critical values 1 + η and 0, respectively, in order to see both critical regions as α is increased.
The master curves for α → 0 and α → 1, resulting from the critical rescalings, are plotted
(blue dashed line with crosses and red dashed line with circles respectively): Ω∗ ∼ 10−3 for
the α → 0 master curve, while Ω∗ ∼ 10−1 for the one for α → 1. For Ω∗ (cid:28) Ω (cid:28) 1 one has a
Lorentzian tail C ∼ 1/Ω2 for α small and α big, while a diﬀerent power-law feature, namely
C ∼ 1/Ω, emerges for α → 1. At Ω ∼ O(1) a crossover to Lorentzian behaviour is seen for
any α.

(cid:40)

can derive the following system of two equations

[8 + ¯C¯(2 + ¯)]2[−4 + ¯C(2 + ¯)(−¯ + ¯C(1 + ¯α ¯C)(2 + ¯))] = 0
[−16¯ + ¯C(2 + ¯)(16 − 3¯2 + 4 ¯C¯(2 + ¯) + ¯α ¯C(24 + 5 ¯C¯(2 + ¯))]¯τ 2 =

¯C3(2+¯)¯4
(8+ ¯C¯(2+¯))

(4.9)

where ¯C is shorthand for ¯C(0, ¯α, ¯) and ¯τ for ¯τ (¯α, ¯). Consistent with the role of ¯ discussed
above, one can check that the limit for ¯ → 0 is precisely the system (4.4), while for ¯ → ∞
(4.9) becomes

(cid:40) −1 + ¯C + ¯α ¯C2 = 0

− ¯C + ¯τ 2(−3 + ¯C(4 + 5 ¯α ¯C)) = 0

Here the ﬁrst equation agrees with (3.16) as it should. Solving for ¯τ , one ﬁnds

¯τ (¯α) =

1

(1 + 4¯α) 1

4

26

(4.10)

(4.11)

For generic ¯, the rescaled relaxation time ¯τ (¯α) must then exhibit a crossover in its large ¯α
power law behaviour, i.e. from ¯α− 1
3 . This crossover takes place around ¯α∗ = ¯2/4;
see ﬁgure 5 (left).

4 to ¯α− 2

4.2. Relaxation time for α → 1 and p → 0

We discuss brieﬂy the behaviour of the relaxation time in the second critical region. From
the rescaling of C one has

T =

¯τ (δ ¯α)

1

p(cid:112)(1 + γ2)
(cid:115)
(cid:18)

(cid:19)

(4.12)

(4.13)

and

¯τ (δ ¯α) =

with limit ¯τ|δ ¯α→∞ ∼ 1/δ ¯α giving

1
2

1 −

δ ¯α√
4 + δ ¯α2

T ∼ 1
δα

p < δα (cid:28) 1

(4.14)
The δα dependence of T is shown in ﬁgure 6 (left). As a general unifying feature of the
relaxation times, one can see that they decrease signiﬁcantly with increasing α (ﬁgure 6,
right): as the values of the hidden variables become constrained increasingly strongly by
those of the observed ones, the remaining uncertainty in the prediction becomes local in
time.

(cid:90) ∞

−∞

(cid:90) ∞

−∞

4.3. Equal time posterior variance for γ → γc and α → 0
Finally we turn to the behaviour of the inference error for the prediction of hidden unit
trajectories. This is given by the equal time posterior correlator

where C0 =(cid:82) ∞

C(t − t) = C(0) =

˜C(ω)dω =

σ2
s
k2 σ

C(Ω)d Ω =

C0

σsσb

k

(4.15)

−∞ C(Ω)d Ω is a dimensionless equal time posterior variance. We see that the

size of the error is generically proportional to the noise acting on the dynamics of hidden
and observed variables, and inversely proportional to the hidden-to-observed interaction
strength. Its critical scaling properties depend on whether the integral over Ω that deﬁnes
C0 is dominated by small frequencies Ω ∼ Ω∗, where Ω∗ is the relevant frequency scale in
the appropriate critical region, or by Ω ∼ 1. One has the ﬁrst case when the critical master
curve for C(Ω) has an integrable tail towards large (scaled) frequencies, otherwise the second.

27

Figure 5. (Left) The rescaled relaxation time ¯τ (¯α), normalized at the origin, for α → 0
and γ → γc as a function of ¯α for diﬀerent values of ¯. The two limits, ¯ → ∞ and ¯ → 0,
are highlighted in red and blue respectively. For intermediate values of ¯ one can see an
interpolation between the corresponding power laws, i.e. ¯α− 1
3 , and this crossover
occurs at ¯α∗ = ¯2/4 (for instance, at ¯ = 25, ¯α∗ ∼ 150). (Right) Relaxation time in the
vicinity of the critical region α → 0 and γ → γc, as a function of α and for diﬀerent η. Solid
lines are the numerics, dashed ones with circles the analytic master curves. A plateau for
small α emerges for η close to 1. For δγ2 < α (cid:28) 1 one can see τ ∼ α− 2
3 for η = 1 and
τ ∼ α− 1
4 for η = 0.1, as suggested by eqs. (4.5) and (4.8). The case η = 0.85 interpolates
between these power tails with a crossover at α ∼ 0.0005; this corresponds to the crossover
on the left for ¯ ∼ 20.

4 and ¯α− 2

4.3.1. η = 1. Here the dominant contribution to the integral (4.15) comes from Ω ∼ O(1),
and in this region the master curve for the spectrum is given by (3.3) (the case without
observations). As a consequence we expect the equal time correlator to become essentially
independent of α for small α, as one can see in ﬁgure 7 (left, curves for η = 1). More
generally this implies that the dependence on α is smooth, and to leading order unaﬀected
by the vicinity of the critical region.

4.3.2. −1 < η < 1. Here the dominant contribution to the prediction error comes from
Ω ∼ Ω∗, thus one has to evaluate the integral of the master curve (3.16). To do so we note
from (3.12) that C0 can be written in scaled form as
(1 − η)Ω∗
2 δγ p2

¯C0(¯α)

C0 =

(4.16)

with ¯C0(¯α) =(cid:82) ∞

region.

−∞ ¯C( ¯Ω, ¯α)d ¯Ω. This function encodes the entire α-dependence in the critical
It has a ﬁnite limit for ¯α → 0 while for large ¯α it decays as ¯C0 ∼ ¯α−1/4 as one

28

Figure 6. (Left) Relaxation time in the vicinity of the critical region α → 1 and p → 0, as a
function of α. As the variation with γ is weak in this regime, we ﬁx γ = 0.1. Solid lines are
the numerics, dashed ones with circles the analytic master curves. One can see that τ stays
roughly constant for α < 1 while it drops to smaller and smaller values for increasing α > 1.
(Right) Relaxation time for small p and δγ as a function of α: an interpolation between the
behaviours in the left plot and in ﬁgure 5 (right) can be seen here. We stress that the red
master curve with circles is expected to give a good ﬁt for α ≈ 1 only, consistent with our
results.

C(0) ∼

(1 − η)2

p(cid:112)2δγ(1 + η)

¯α− 1

(1 − η)

7

1
p

1

4 α− 1

4

can show by noting that the relevant frequencies ¯Ω in (3.16) are then ∼ ¯α−1/4. Using this
asymptotic behaviour in (4.16) and substituting also the expressions for Ω∗ and ¯α from (3.14)
and (3.15), respectively, one obtains

4 =

4 (1 + η)

(4.17)
We thus predict C(0) ∼ α−1/4, and this is consistent with the numerics, see e.g. ﬁgure 7 (left,
curves for η = 0.1), where ¯α (cid:29) 1 corresponds to δγ2 (cid:28) α (cid:28) 1 from (3.15): in this range
the zero frequency amplitude is then independent of δγ, and the value of the latter appears
only as the lower limit of the range where this result applies. The power law behaviour
C(0) ∼ α−1/4 is also consistent with the scaling ˜C(0) ∼ τ C(0) that one would generally
expect, barring any exceptions due to strongly non-exponential correlations: recall here that
we found previously τ ∼ α−1/4 and ˜C(0) ∼ α−1/2.

4.4. Equal time posterior variance for α → 1 and p → 0
In the second critical region and focussing on α → 1, we have an interesting marginal case
where the equal-time variance (4.15) has contributions from all frequencies ranging from the

29

critical frequency scale Ω ∼ Ω∗ to Ω ∼ 1. This is because the power spectrum (3.32) for
critical frequencies has a 1/ ¯Ω tail for large ¯Ω = Ω/Ω∗, which gives a logarithmically divergent
integral (4.15). This divergence is cut oﬀ only by the crossover to a Lorentzian tail when
Ω = O(1). Including the prefactor from (3.27), one thus estimates

p(cid:112)γ2 + 1

Ω∗

C0 ≈

(cid:90) 1/Ω∗

¯C( ¯Ω, δ ¯α)d ¯Ω

2

0

The fraction in front of the integral equals unity as Ω∗ = p(cid:112)γ2 + 1 from (3.30) so from the
logarithmic divergence. Writing ln(1/Ω∗) as(cid:82) 1/Ω∗
(cid:90) ∞

1/ ¯Ω tail one ﬁnds C0 ≈ 2 ln(1/Ω∗) to leading order. All of the interesting dependence on
α is in the next subleading term, which is relevant in practice as it only competes with a
d ¯Ω/(1 + ¯Ω), this subleading term can be

split oﬀ in the form

(4.18)

0

C0 ≈ 2 ln(1/Ω∗) + 2

[ ¯C( ¯Ω, δ ¯α) − (1 + ¯Ω)−1]d ¯Ω

(4.19)

0

The remaining integral is convergent at the upper limit so we have taken the upper limit
1/Ω∗ to inﬁnity as is appropriate to get the leading contribution for p → 0. The integral is
then a function of δ ¯α only, which one ﬁnds varies as |δ ¯α| for δ ¯α → −∞ and as const.−ln(δ ¯α)
for δ ¯α → ∞. The two dominant terms (4.19) are plotted (red line) in ﬁgure 8 where the
linear scale inset clearly shows the linear dependence on |δ ¯α| ∝ 1 − α.

As a common trend across the two critical regions we have the intuitively reasonable
result that the inference error decreases when the number of observed variables gets bigger,
see ﬁgure 7 (right).

5. Conclusion

In this paper we considered a network of continuous degrees of freedom where some nodes
are hidden and the others observed. In such a setting we discussed an application of the
Extended Plefka Expansion to the problem of inferring hidden states over time when one
has seen the dynamics of the observed ones. In particular we focussed on the case of a linear
dynamics with Langevin noise. This choice was motivated by the fact that the posterior
statistics, i.e. the statistics of hidden trajectories conditioned on observations, is Gaussian
and thus fully understandable in terms of ﬁrst and second moments. The posterior means
give us the optimal hidden state predictions, and the posterior variances the error bars on
these, hence also the expected inference error.

The approximation we make by the Extended Plefka Expansion is to consider the second
moments of the posterior as local in the degrees of freedom but maintaining them as functions
of two times. Because of this, the method yields eﬀective equations of motion for the hidden
variables treat these as decoupled from each other and from the observed variables but

30

Figure 7. (Left) Inference error in the vicinity of the critical region α → 0 and γ → γc,
as a function of α and for diﬀerent η. Solid lines are the numerics, dashed ones with circles
the analytic master curves. The dominant contribution to the inference error is given by
frequencies Ω ∼ Ω∗ for η = 0.1 while it is given by Ω ∼ O(1) for η = 1. The master curve in
this latter regime is the α-independent posterior variance of the case without observations,
i.e. a straight line, which C(0) approaches for α → 0 as it should; for larger α it exhibits
a smooth dependence on α unconnected to any critical behaviour. (Right) Inference error
for small p and δγ as a function of α. This connects the behaviours in the left plot and in
ﬁgure 8. The red master curve with circles is expected to give a good ﬁt only around α = 1,
as observed.

with nontrivial temporal self-correlations through memory integrals and coloured noises. In
mathematical terms, the Extended Plefka Expansion gives us a system of coupled equations
for the posterior physical correlations, responses and auxiliary correlations, the latter having
the role of implementing the constraint arising from knowledge of the dynamics of the
observed nodes. Evaluating the posterior covariances at equal times gives the posterior
error, hence the Plefka estimate of the inference error.

Having derived the Plefka equations in general form, we then focussed on systems with
weak, long-range, random interactions. Because of the mean ﬁeld character of the decoupling
we assumed in setting up the Plefka expansion, one would expect the approximation to
become exact for such a system, in the limit of a large network. We were able to verify this by
comparing to the results of a random matrix theory analysis of the exact inference approach
(Kalman ﬁlter) and to replica calculations, in the corresponding limit cases [17]. This
provides further evidence to indicate the usefulness of the Extended Plefka Expansion [12],
whose application we generalized here to inference problems for a linear stochastic dynamics
of continuous variables.

31

Inference error in the vicinity of the critical region α → 1 and p → 0, as a
Figure 8.
function of α, at ﬁxed γ = 0.1. The dashed line with circles shows the prediction from the
logarithmic divergence and ﬁrst subleading term (4.19), which is qualitatively remarkably
accurate even away from α = 1. The prediction behaves as ∼ (1− α) for α < 1 as the linear
scale inset shows.

Looking beyond the above exactness results for special cases,

it is worth stressing
that the approach presented here allows one to derive results for more general interaction
scenarios. Speciﬁcally it holds for any degree of symmetry of the interactions and is thus
applicable to non-equilibrium dynamics, for which the detailed balance condition is not
satisﬁed.

In a second part of the paper we studied the properties of the inference error as a function
of the relevant dimensionless parameters (α, γ, p, η). These parameters are assumed to be
known, either by direct measurement or by theoretical estimation, and our predictions for
the posterior statistics then quantify their interplay in determining the prediction error. As
the parameter space is relatively large, we organized the analysis around the critical regions
where the (suitably non-dimensionalized) prediction error diverges. We focussed attention
on the power spectrum of the posterior correlations, deploying critical scaling approaches to
identify the relevant variables and obtain scaling functions that serve as master curves for
appropriately scaled numerical data.

32

The ﬁrst critical region we analyzed concerns α (cid:28) 1, where there are many fewer
observed nodes than hidden ones. Here we found that the presence of interaction symmetry
(η = 1) leads to quite diﬀerent scaling behaviour than for the generic case −1 < η < 1,
indicating the importance of even small deviations from detailed balance for the dynamics.
This is in qualitative agreement with earlier studies on systems without observations, e.g. [28].
We were able to study the crossover from equilibrium to generic non-equilibrium dynamics
by including  = 1 − η as a small parameter in the scaling analysis.

The second critical region is 0 < α < 1 and p → 0, where some parts of the hidden
dynamics are strongly constrained but because α < 1 there are other parts that remain
unconstrained as there are still not enough observed nodes. We identiﬁed an analogue of
the resulting inference error divergence in studies on “underconstrained” learning in neural
networks, e.g. [10, 11]. There one ﬁnds a divergence when the number of patterns to be
learned equals the number of degrees of freedom. This happens when no weight decay is
imposed on the dynamics, which in our scenario corresponds to small λ and hence small p.
It will be interesting to understand better the comparison of our ﬁndings with equivalent
equilibrium problems where these exist.
In fact, for symmetric couplings, the stationary
regime of the hidden dynamics is eﬀectively at equilibrium. One could thus apply the static
Plefka expansion, using means and equal time correlations as order parameters, to the same
inference scenario. The main diﬀerence would be the fact that the statistics of the hidden
values would not be determined from observations over time as in our analysis, but from
snapshots of states of the observed nodes. As information on the temporal sequence of
observations is lost, this would be expected to lead to a larger inference error. A calculation
mirroring ours but in the equilbrium setup could therefore quantify the loss in prediction
performance from ignoring the temporal aspect of observational data. This could be an
important baseline study establishing for future work the beneﬁts of modelling data explicitly
as a trajectory over time.

There are many other directions of future and further investigation. A number of these
could start from data likelihood that the extended Plefka expansion predicts: this is an
approximate dynamical action for the observed nodes, having integrated out the hidden
node dynamics. As such it describes the observed subnetwork dynamics, and a comparison
with other approaches based on e.g. projection methods [29] should be revealing.

A further use of the Plefka data likelihood would be to learn unknown hidden-to-hidden
or observed-to-hidden couplings, which is an important statistical modelling problem for
dynamical data. Optimizing the data likelihood with respect to the couplings would by
deﬁnition give a maximum-likelihood estimate for these quantities. The relevant learning
rules could be developed starting from the Plefka equations for ﬁxed couplings. It would be
interesting then to compare with related studies for non-equilibrium Ising spins [15] and for
networks with binary visible units and continuous-valued hidden ones [16], both also relying
on mean-ﬁeld dynamical descriptions. One could investigate interesting questions such as the

33

accuracy of the inferred couplings and the computational eﬃciency of the iterative algorithms
for implementing the learning rules.

An application of the Extended Plefka Expansion to spin systems [30] might also be
worthwhile, both for inference of hidden states and learning of interactions. The analytically
tractable scenario of an inﬁnitely large network of spins with random asymmetric couplings
has already been studied by a replica approach [1]; intriguingly, there the error incurred in
predicting the states of hidden nodes does not exhibit a singularity structure like to the one
presented in this paper.

Exploiting further the analogy with dynamical learning in neural networks, one could
think of several additional studies, such as including noise in the observation process [31–34]
or investigating ﬁnite-size eﬀects [35].

Acknowledgement

This work was supported by the Marie Curie Training Network NETADIS (FP7, grant
290038). We are indebted to Manfred Opper for helpful suggestions and insightful
discussions.

34

[1] Bachschmid-Romano L and Opper M 2014 J. Stat. Mech. P06013
[2] Battistin C, Hertz J, Tyrcha J and Roudi Y 2015 J. Stat. Mech. P05021
[3] Mezard M and Sakellariou J 2011 J. Stat. Mech. L07001
[4] Trejo-Banos D, Millar A and Sanguinetti G 2015 Computational Methods in Systems Biology (CMSB

2015)

[5] Jones P J M, Sim A, Taylor H, Bugeon L, Dallman M, Pereira B, Stumpf M and Liepe J 2015 Phys.

Biol. 12 1–12

[6] Buettner F, Natarajan K N, Casale F P, Proserpio V, Scialdone A, Theis F J, Teichmann S A, Marioni

J C and Stegle O 2015 Nature Biotechnol. 33 155–160

[7] Bishop C 2006 Pattern Recognition and Machine Learning (Springer)
[8] Kalman R E 1960 J Basic Eng 82 35–45
[9] Elf J and Ehrenberg M 2003 Genome Res. 13 2475–2484
[10] Hertz J, Krogh A and Thorbergsson G 1989 J. Phys. A: Math. Gen. 22 2133–2150
[11] Opper M 1989 Europhys. Lett. 8 389–392
[12] Bravi B, Sollich P and Opper M 2016 J. Phys. A: Math. Gen.
[13] De Martino D, Capuani F, Mori M, De Martino A and Marinari E 2013 Metabolites 3 946–966
[14] Holme P 2006 Phys. Rev. E 74 036107
[15] Dunn B and Roudi Y 2013 Phys. Rev. E 87 022127
[16] Tyrcha J and Hertz J 2014 MBE 11 149–165
[17] Bravi B, Opper M and Sollich P 2016 in preparation
[18] Fruhwirth G O, Fernandes L P, Weitsman G, Patel G, Kelleher M, Lawler K, Brock A, Poland S P,
Matthews D R, Keri G, Barber P R, Vojnovic B J, Ameer-Beg S M, Coolen A C C, Fraternali F and
Ng T 2011 ChemPhysChem 12 442–461

[19] Okino M and Mavrovouniotis M 1998 Chem. Rev. 98 391–408
[20] Martin P C, Siggia E D and Rose H A 1973 Phys. Rev. A 8 423–436
[21] Janssen H 1976 Z. Phys. B: Cond. Mat. 23 377–380
[22] De Dominicis C 1978 Phys. Rev. B 18 4913–4919
[23] Van Kampen N G 1982 J. Stat. Phys. 24 175–187
[24] Stratonovich R 1960 Theor. Probab. Appl. 5 156–178
[25] Bravi B 2016 Path integral approaches to inference and dynamics Ph.D. thesis
[26] Girko V L 1986 Theory Probab. Appl. 30 677–690
[27] Cardy J 1996 Scaling and Renormalization in Statistical Physics (Cambridge University Press)
[28] Crisanti A and Sompolinsky H 1987 Phys. Rev. A 36 4922–4939
[29] Rubin K, Lawler K, Sollich P and Ng T 2014 J. Theor. Biol. 357 245–267
[30] Bachschmid-Romano L and Opper M 2016 In preparation
[31] Krogh A 1992 J. Phys. A. Math. Gen. 25 1119–1133
[32] Opper M and Sanguinetti G 2007 Adv. Neural Inf. Process. Syst. 20 1105–1112
[33] Opper M, Ruttor A and Sanguinetti G 2010 Adv. Neural Inf. Process. Syst. 23 1831–1839
[34] Cseke B, Opper M and Sanguinetti G 2013 Adv. Neural Inf. Process. Syst. 26 971–979
[35] Sollich P 1994 J. Phys. A: Math. Gen. 27 7771–7784

35

