6
1
0
2

 
r
a

 

M
7
1

 
 
]

.

C
O
h
t
a
m

[
 
 

1
v
2
1
4
5
0

.

3
0
6
1
:
v
i
X
r
a

Online semi-parametric learning for inverse dynamics modeling

D. Romeres, M. Zorzi and A. Chiuso †

Abstract— This paper presents a semi-parametric algorithm
for online learning of a robot inverse dynamics model. It
combines the strength of the parametric and non-parametric
modeling. The former exploits the rigid body dynamics equa-
tion, while the latter exploits a suitable kernel function. We
provide an extensive comparison with other methods from the
literature using real data from the iCub humanoid robot. In
doing so we also compare two different techniques, namely cross
validation and marginal likelihood optimization, for estimating
the hyperparameters of the kernel function.

I. INTRODUCTION

Inverse dynamics models are very useful in robotics be-
cause they can guarantee high accuracy and low gain control.
Building an inverse dynamics model from ﬁrst principles
may be very demanding and, in most cases, out of reach
and not suitable for online application. For this reason it
is of interest to build an inverse dynamics model directly
from data, possibly online to allow for real time updating
of the model, which is required for adaptation to changing
conditions.

Traditionally the inverse dynamics is described by a para-
metric model given by the rigid body dynamics (RBD),
[1]. Then,
inverse dynamics learning can be recasted as
a parametric estimation problem, [2]. The main advantage
of this approach is that it provides a global relationship
between the input (joint angles, velocities and accelerations)
and the output (torques). However, the linear model does not
capture nonlinearities in the data. To overcome this difﬁculty,
it is possible to describe the inverse dynamics using non-
parametric models; we do so costing the estimation problem
in the Gaussian regression framework, [3], or, equivalently, in
the regularization framework, [4]. The latter are characterized
by a suitable kernel function. However, the drawback of this
approach is that it is required a large amount of data to pro-
duce accurate predictions, as well as high computation load
to actually compute the estimated model. This approach is
not new and several contributions have recently appeared; for
instance in [5], [6] the inverse dynamics has been modeled
combining the strength of the parametric and of the non-
parametric approach. In the latter case two alternatives are
possible. The ﬁrst one is to embed the rigid body dynamics
as “mean” in the non-parametric part. The second one is to
incorporate the rigid body dynamics in the kernel function.
An important aspect in inverse dynamics learning is the
variation of the mechanical properties caused by changing

This work has been partially supported by the FIRB project “Learning
† Dept. of Information Engineering, University of Padova (e-mail:

meets time” (RBFR12M3AC) funded by MIUR.
{romeresd,zorzimat,chiuso}@dei.unipd.it)

of tasks. It is then necessary to update online the model. In
this framework it is important that the online algorithm is
able to take advantage of the knowledge already acquired
from previous available data, thus speeding up the learning
process. This concept is often called transfer learning [7],
[8].

Several online learning algorithms have been proposed
in the literature. We mention the non-parametric algorithm
selecting a sparse subset of training data points (i.e. dic-
tionary), [9], and the semi-parametric algorithms based on
the locally weighted projection approach, [10], and on the
local Gaussian process regression approach, [11]. In [12] a
non-parametric online algorithm has been proposed in which
the complexity is kept constant approximating the kernel
function using so called “random features”, [13], [14]. Fi-
nally, in [15] a semi-parametric online algorithm, exploiting
the above approximation, has been proposed. Here, the rigid
body dynamics, preliminarly estimated via least squares, has
been embedded as mean in the non-parametric part. The
ﬁrst contribution of the paper is a semi-parametric online
algorithm, exploiting the random features approximation,
where the rigid body dynamics is incorporated in the kernel
function. As we shall see, this approach includes as a special
case, also the previous in which the rigid body dynamics is
inserted as a mean term.

Another important aspect

is the estimation of the hy-
perparameters of the kernel function. The latter can be
estimated according to the maximum likelihood approach,
[3], or according to the validation set approach, [16].

The second contribution of this paper is to compare these
online algorithms based on the random features approxi-
mation, for estimating the inverse dynamic of right arm
of the iCub humanoid robot, [17], [18]. In doing that, we
also compare the two different approaches for estimating the
hyperparameters.

The paper is outlined as follows. In Section II we introduce
parametric, non-parametric and semi-parametric models. In
Section III the online algorithm to update the model. Section
IV deals with the hyperparameters estimation. In Section V
we test the different online algorithms for estimating the
inverse dynamics of the right arm of the iCub. Finally, in
Section VI we draw the conclusions.

II. INVERSE DYNAMICS LEARNING

Starting from the laws of physics it would in principle be
possible to write a (direct) dynamical model which, having
as inputs the torques acting on the robot’s joints, outputs the
(sampled) trajectory of the free coordinates (joint angles) qs,
s ∈ Z. This is the so called “direct dynamics”.

However, for the purpose of control design, it is of interest
to know which are the torques which should be applied in
order to obtained a certain trajectory qs. This is the purpose
of inverse dynamics modeling: ﬁnd a model which, having
joint trajectories as inputs, outputs the applied torques.

In order to simplify the modeling exercise, we shall
assume, as customary, that not only joint angles qs can
be measured, but also velocities ˙qs and accelerations ¨qs.
Of course this is a (crude) approximation, but we leave to
future work possible alternatives. This assumption simpliﬁes
considerably the modeling exercise because, given qs, ˙qs, ¨qs,
the inverse dynamics model is, in principle, linear (see (3)).
s ](cid:62) ∈
Rm, m = 3n, the vector “input locations” obtained by
stacking positions, velocities and accelerations of all the n
joints of the robot. Similarly, ys ∈ Rn are the applied torques
to the n joints of the robot at time s. The inverse dynamics
models we consider in this paper will be of the form

From now on we shall denote with xs = [q(cid:62)

˙q(cid:62)
s ¨q(cid:62)

s

M : ys = h(xs) + es

s ∈ Z

(1)

where h is a, possibly non-linear, function and es is a zero
mean white Gaussian noise with unknown variance σ2.
The problem of learning the inverse dynamics is that of
estimating the model M (i.e. the function h) starting from a
ﬁnite set of measured data ys, xs, s ∈ 1, . . . , t. This model
can then be used for robot motion control, see Figure 1.

Fig. 1: Schematic for robot motion control.

More precisely, it is exploited to determine the feedfor-
ward joint torques yd
t which should be applied to follow a
desired trajectory xd
t , while employing a feedback controller
in order to stabilize the system. Clearly, the more accurate the
inverse dynamics model M is the more accurate the motion
control is.
pending upon how the function h(·) in (1) is modelled.
A. Linear Parametric Model

In this paper we shall consider several approaches, de-

The rigid body dynamic (RBD) of a robot is described by

the equation

ys = M (qs)¨qs + C(qs, ˙qs)¨qs + G(qs)

(2)

where M (qs) are the inertia matrix of the robot, C(qs, ˙qs)
the Coriolis and centripetal forces and G(qs) the gravity
forces, [1]. The terms on the right hand side of (2) can be
rewritten as ψ(cid:62)(xs)π which is linear in the robot (base)
inertial parameters π ∈ Rp and where ψ(xs) ∈ Rp×n is the
known RBD regressor which is a combination of kinematic

parameters. In order to make the problem of determining π
from measured data ys well posed, we follow a Bayesian
approach modeling π as a zero mean Gaussian random
vector with covariance matrix γ2Ip. Therefore, we consider
h(x) = ψ(x)(cid:62)π in (1), so that

ys = ψ(cid:62)(xs)π + es

(3)

where es is a zero-mean Gaussian noise with covariance
matrix σ2In and it represents nonlinearities of the robot that
are not modeled in the rigid body dynamics (e.g. actuator
dynamics, friction, etc.).

B. Nonparametric Model

The robot inverse dynamics is modeled postulating

ys = g(xs) + es

(4)

(i.e. h(x) = g(x) in (1) ) where g(xs) is a zero mean
vector valued (taking values in Rn) Gaussian random process
indexed in Rm, with covariance function

E[g(xt)g(xs)(cid:62)] = ρ2K(xt, xs)In.

(5)

The parameter ρ2 plays the role of scaling factor and K
is a positive deﬁnite function, known also as (reproducing)
kernel, due to the link between Gaussian process regression
and inverse problems in Reproducing Kernel Hilbert Spaces
(RKHS) [19]. In robotics, a typical choice is the Gaussian
kernel, [12], [5], [6],

K(xt, xs) = e

− (cid:107)xt−xs(cid:107)2

2τ 2

(6)

where τ 2 is the kernel width1 and it represents the metric
to correlate the input locations xt and xs. The minimum
variance linear estimator of g at time t is given by the
solution of the regularization problem

ˆgt = argmin

g∈H

1
σ2

(cid:107)ys − g(xs)(cid:107)2 +

1

ρ2(cid:107)g(cid:107)2H

(7)

where H denotes the reproducing kernel Hilbert space
(RKHS) of deterministic functions from Rm to Rn associated
with KIn and with norm (cid:107) · (cid:107)H, [20]. By the representer
Theorem,

ˆgt(x) = ρ2

αsK(xs, x)

(8)

s=1

where αs ∈ Rn. Substituting (8) in (7) we obtain a Tikhonov
regularization problem. However, the number of parameters
is depending on the number of data t, making it hard
to obtain on-line (recursive) solutions. To overcome this
limitation, the kernel K can be approximated, e.g. using
so-called random features, [13]. This exploit the fact that
a positive deﬁnite real Kernel is the Fourier transform of a

1Therefore, to be precise the function K as well as its approximation
(11), depends on the parameter τ which will be estimated from data, see
Section IV. For simplicity of exposition this dependence is not made explicit
in the notation.

t(cid:88)

s=1

t(cid:88)

RobotControllerxdtydtxt+-Mnon-negative function, which can thus be interpreted as a
probability density [13]. For the Gaussian kernel that is:

(cid:90)

p(ω)ei ω(cid:62) (xt−xs )

τ

dω

K(xt, xs) =

where

Rm

p(ω) =

√
(

1
2π)m

e− (cid:107)ω(cid:107)2

2

.

(9)

(10)

Accordingly, we can approximate K(xt, xs) with the sample
, k = 1, .., d provided wk ∼ p(ω), that
mean of ei
is:

ω(cid:62)
k (xt−xs )

τ

K(xt, xs) =

1
d

ω(cid:62)
k (xt−xs)

τ

ei

= φ(xt)(cid:62)φ(xs)

(11)

d(cid:88)

k=1

where the ωk are drawn from (10) and the basis functions
φ(x) ∈ R2d are
1√
d

φ(x) =

. . . cos

(cid:17)

(cid:17)

1 x
τ

(cid:16) ω(cid:62)
(cid:17) (cid:105)(cid:62)

d x
τ

(cid:16) ω(cid:62)

d x
τ

(cid:104)
(cid:16) ω(cid:62)

cos

(cid:16) ω(cid:62)
(cid:17)

1 x
τ

sin

. . .

sin

.

(12)

This is equivalent to model g(x) in the form

g(x) = (φ(x)(cid:62) ⊗ In)α

(13)

where α is a zero mean Gaussian vector with variance
ρ2I2dn. Therefore, the nonparametric model of the robot
inverse dynamics (4) can be approximated by

ys = (φ(x)(cid:62) ⊗ In)α + es.

(14)

We underline that a peculiarity of model (14) is that the
regressor φ(x) is depending on the parameter to identify τ.
The advantage of reformulation (14) is that the dimension
of the parameters to estimate α ∈ R2dn is ﬁxed, which
allows a recursive identiﬁcation formulation, and arbitrary:
the number of basis functions d can be chosen in according
to a trade-off between model and computational complexity.

C. Semi-parametric model with RBD mean

This approach combines the parametric and nonparametric
models, embedding in the nonparametric model a mean term,
derived from the linear parametric model (3), of the form

ms := ψ(cid:62)(xs)π

(15)

where π is the vector of inertial parameters and ψ(xs) is
the RBD regressor. Therefore the inverse dynamics will be
modeled as in (4) with g(xs) a Gaussian process such that

E[g(xs)] = ms = ψ(cid:62)(xs)π
Var[g(xt)g(xs)(cid:62)] = ρ2K(xt, xs)In

(16)

where K is the Gaussian Kernel deﬁned in (6). Approximat-
ing, as above, the kernel K in (16) with the random features
(11), the semi-parametric model of the inverse dynamics
takes the form

ys = ψ(cid:62)(xs)π + (φ(cid:62)(xs) ⊗ In)α + es

(17)

where α is a random vector with zero mean and covariance
matrix ρ2I2dn. As before, es is white noise with covariance
matrix σ2In.

At this point two alternatives are possible. The ﬁrst and
most principled one is to treat π as an unknown parameter,
which is to be estimated along with ρ, σ and τ using e.g. the
marginal likelihood as described in Section IV. A suboptimal
alternative is to assume π to be known, possibly estimated
using some preliminary experiment as in [5]. In this latter
case it will be denoted by ˆπ, and therefore we are only left
with modeling the residual vector

˜ys := ys − ψ(cid:62)(xs)ˆπ = (φ(cid:62)(xs) ⊗ In)α + es

(18)

This latter strategy is followed, for instance, in [15], where
the vector ˆπ is obtained solving in the least squares sense
the regression model (3).

D. Semi-parametric model with RBD kernel

An alternative possibility for combining the parametric
and nonparametric models in model (4), is to incorporate
the RBD structure in the kernel, [5]. Therefore, g(xs) is a
random process with zero mean and covariance function

τ 2

− (cid:107)xt−xs(cid:107)2

E[g(xt)g(xs)(cid:62)] = γ2ψ(xt)(cid:62)ψ(xs) + ρ2e

In
(19)
where in the ﬁrst term ψ(xs) is the RBD regressor and
the second term is the Gaussian kernel. As before, es is
white noise with covariance matrix σ2In. Using the kernel
approximation (11), we have
E[g(xt)g(xs)(cid:62)] = γ2ψ(xt)(cid:62)ψ(xs) + ρ2(φ(xt)(cid:62)φ(xs) ⊗ In2).
(20)
Accordingly, the approximated semi-parametric model of the
inverse dynamics with RBD kernel is:

ys =(cid:2) ψ(cid:62)(xs) φ(cid:62)(xs) ⊗ In

(cid:3) θ + es

(21)

a

where θ = [ π(cid:62)
zero
mean Gaussian random vector with covariance matrix
blkdiag(γ2Ip, ρ2I2dn).

∈ Rp+2dn is

α(cid:62) ](cid:62)

The semiprametric model with RBD kernel described in
this Section is connected, under the Bayesian framework,
with the RBD mean model in Section (II-C). In fact, as
explained in the Appendix, model (17) is equivalent
to
model (21) when γ → ∞ and the parameters (ρ, σ, τ ) are
ﬁxed. This indeed is useful to the purpose of computing the
estimator for model (II-C).

III. ONLINE LEARNING

It is apparent that, using the random features approxima-
tion of the Gaussian kernel (11), all model classes described
in the previous Section, see equations (3), (14), (17) and (21),
can ultimately be written in the form :

M : ys = ϕ(xs)(cid:62)θ + es, s = 1 . . . t

(22)
for a suitable choice of the regressor vector ϕ(xs) ∈ Rn×p
and θ ∈ Rp is modeled as a zero mean random vector
with a suitable covariance matrix Σ0. es is white noise with

covariance matrix σ2In. In this Section we shall assume that
Σ0, τ and σ2 are known, how to estimate them is a crucial
point and will be explained in Section IV. Thus, the vector
θ completely speciﬁes the inverse dynamics model and, as
such our learning problem has been reduced to estimating
the vector θ in (22). At time t, the minimum variance linear
estimator (i.e. Bayes estimator) of θ is given by the solution
of the Tikhonov regularization problem:

ˆθt = argmin
θ∈Rp

1
σ2

(cid:107)ys − ϕ(xs)(cid:62)θ(cid:107)2 + (cid:107)θ(cid:107)2

−1
Σ
0

.

(23)

where

t(cid:88)

s=1

B. Maximum likelihood approach

Consider, without loss of generality, model (22) where
both θ and es are assumed to be Gaussian and uncorre-
lated. Accordingly, the negative marginal loglikelihood (or
given ξ ∈ Ξ takes the

evidence) of y =(cid:2) y(cid:62)

(cid:3)(cid:62)

y(cid:62)

. . .

1

t

form

Lξ(y) =

1
2

log det V +

y(cid:62)V −1y + c

1
2

(29)

Φ(cid:62) =(cid:2) ϕ(x1)

V = ΦΣ0ΦT + σ2Itn

. . . ϕ(xt) (cid:3)

and c is a term not depending on ξ. According to the
maximum likelihood approach, [3, Chapter 5], the optimal
hyperparameters are given by solving

ˆξ = argmin

ξ∈Ξ

Lξ(y).

(30)

V. INVERSE DYNAMICS LEARNING OF ICUB

iCub is a full-body humanoid robot with 53 degrees of
freedom, [17]. We aim to test the models of Section II
for learning online the inverse dynamics of its right arm.
We consider as inputs xs the angular positions, velocities
and accelerations of the 3 degree shoulder joints and of
the elbow joint. The outputs ys are the 3 forces and 3
torques components measured by the six-axes force/torque
(F/T) sensor embedded in the shoulder of the iCub arm, see
Figure 2.

The optimal solution admits the following recursive structure,
[21, Chapter 11]:

ˆθt = ˆθt−1 + Lt(yt − ϕ(xt)(cid:62) ˆθt−1)

Lt =

Pt−1ϕ(xt)

1 + ϕ(xt)(cid:62)Pt−1ϕ(xt)

Pt = Pt−1 − Pt−1ϕ(xt)ϕ(xt)(cid:62)Pt−1
1 + ϕ(xt)(cid:62)Pt−1ϕ(xt)

(24)

(25)

(26)

with initial conditions

1
σ2 Σ0.

ˆθ0 = 0, P0 =

(27)
Recursion (24)-(27) can be used to update the model M
when new data become available.

In practice the Recursive Least Squares algorithm can be
implemented taking advantage of square-root algorithms, e.g.
propagating Cholesky factors Lt of Pt rather than Pt as in
(26). We have actually used the Cholesky-based update, see
[21], which has better numerical properties.

IV. HYPERPARAMETERS ESTIMATION

All the models presented in Section II depend on one
or more parameter, called hyperparameters, which describe
the prior model. For instance, the hyperparameters in model
(17), used in semi-parametric learning with RBD mean,
are ξ := (π, ρ2, τ 2, σ2) while those in model (21), used
in semi-parametric learning with RBD kernel, are ξ :=
(γ2, ρ2, τ 2, σ2). These hyperparameters are not known and
need to be estimated from the data. In what follows we
consider two different approaches to address this problem.
A. Validation set approach

The batch of data used for the identiﬁcation is split in two
data sets: the training set and the hold-out set. We deﬁne a
set of candidate hyperparameters and we denote it as Ξ. For
each ξ ∈ Ξ we estimate the inverse dynamics model Mξ
using the training set. Then, for any Mξ the mean square
error MSE(ξ) is computed using the hold-out set. Hence,
the latter provides an estimate of the error rate. According
to the validation set approach, [16, Chapter 6], the optimal
hyperparameters are given by solving

ˆξ = argmin

ξ∈Ξ

MSE(ξ).

(28)

In practice this approach is limited to estimation of a
small number of hyperparameters since minimization (28)
is typically performed gridding the search space Ξ.

Fig. 2: iCub right arm.

It is worth observing that the measured forces/torques
are not the applied joint forces and torques and, as such,
the model we learn is not, strictly speaking, the inverse
dynamics model. Yet, as explained in [22], the feedforward
joint torques can be determined from components (forces
and torques) of ys. Indeed, such model has been used in the
literature as a benchmark for the inverse dynamics learning,
[12], [15] .

We consider the 2 datasets used in [15], corresponding
to different trajectories of the end-effector. In the ﬁrst one
the end-effector tracks circles in the XY plane of radius
10cm at an approximative speed of 6m/s; in the second one,
the end-effector tracks similar circles but in the XZ plane
(the Z axis corresponds to the vertical direction, parallel
to the gravity force). The two circles are tracked using the

to a different motion with respect to XZ-dataset. Our goal
is that the model estimated with the second dataset quickly
capture the new information gathered from the XZ-dataset,
adapting to the new task. For instance, in model predictive
control the quality of the control depends on the prediction
capability of the model over a prescribed horizon, [26]. In
order to measure this ability we consider the following index:

(cid:80)T

ε(k)
t =

s=1(y(k)

(cid:80)T

t+s − ˆy(k)
s=1(y(k)
t+s)2

t+s|t)2

(31)

t+s|t is the estimate of the output y(k)

where ˆy(k)
t+s at time t + s
using the model estimated with data up to time t. Therefore,
ε(k)
represents the relative squared prediction error over the
horizon [t + 1, t + T ] using model M computed at time t.
t
Let εF
for the 3 forces
and the 3 torques, respectively.

t be the average value of ε(k)

t and εT

t

Cartesian controller proposed in [23]. Each dataset contains
approximately 8 minutes of data collected at a sampling rate
of 20Hz, for a total of 10000 points per dataset. One single
circle is completed by the robot in about 1.25 seconds which
corresponds to 25 points.

We shall consider the models described in Section II,
endowed with the marginal likelihood approach (ML) for the
estimation of the hyperparameters, as well as the validation
based methods2 discussed in [15]. For ease of exposition we
will use the following shorthands:

• P: the parametric model.
• NP-ML: the nonparametric model; hyperparameters es-

timated with ML.

• SP-ML: the semi-parametric model with RBD mean;

hyperparameters estimated with ML.

• SP2-ML: the semi-parametric model with RBD mean,
in which the mean is computed via least squares as
in [15] and then the nonparametric model is applied
to the residuals (see Section (II-C)); hyperparameters
estimated with ML.

• SPK-ML: the semi-parametric model with RBD kernel;

hyperparameters estimated with ML.

• NP-VS: the nonparametric model with hyperparameters

estimated with VS.

• SP2-VS: the semi-parametric model with RBD mean,
in which the mean is computed via least squares;
hyperparameters estimated with VS.

The proposed algorithms have been implemented using
Matlab. The RBD regressor ψ for the right arm of iCub
has been computed using the library iDynTree, [24]. The
Marginal Likelihood has been optimized using the Matlab
fminsearch.m function. The recursive least square algo-
rithms have been implemented using library GURLS, [25].
The results of all validation based methods are obtained using
code which has been kindly provided by the authors of [15].
For each algorithm as above, we consider the following
online learning scenario (with reference to the general model
structure (22)):

Fig. 3: Average (over the 5 subsets of 100 seconds each) of
t , computed
the relative squared prediction errors εF
with T = 25 corresponding to an horizon of 1.25 seconds.

t and εT

• Initialization: The ﬁrst 1000 points in XY-dataset are
used to estimate the hyperparameters, as well as to
compute an initial estimate of parameter θ, say ˆθ0.

• Training XY: Use the remaining 9000 points of XY-
dataset to update online parameter θ using the recur-
thus obtaining ˆθt, t =
sive least-squares algorithm,
1, . . . , 9000.

• Training XZ: The XZ-dataset is split in 5 sequential
subsets of 2000 points (approximately 100 seconds)
each. For each subset we update online the parameter
θ independently always initializing the recursions with
ˆθ9000, computed from the training dataset XY .

In the last step of the procedure, the initial model has been
computed from the Training XY dataset, which corresponds

2As discussed in Section IV, using validation based methods is unfeasible
when the number of hyperparameters is large; therefore we have not applied
validation to the semi-parametric model with RBD mean when the mean
is to be considered as an hyperparameter nor to the semi-parametric model
with RBD kernel which has the extra parameter γ.

t and εT

In Figure 3 we show εF

t , averaged over the 5
subsets, with T = 25 (1.25 seconds), i.e. with the end-
effector completing one circle during the prediction horizon.
Clearly, the parametric algorithm P exhibits a poor perfor-
mance because it describes only crude idealizations of the
actual dynamics. The algorithms based on the VS approach
perform signiﬁcantly worse in the ﬁrst 60 seconds than those
based on the ML approach. This result is not unexpected
because the ML approach represents a robust way to estimate
hyperparamters, [27]. The models with the best performance
are SP-ML and SPK-ML because they combine the beneﬁt
of the parametric and the non-parametric approach. Although
also SP2-ML exploits this beneﬁt,
it provides a slightly
worse performance. This is probably due by the fact that the
ﬁrst (least squares) step, i.e. estimation of the linear model,
is subject to a strong bias deriving from the unmodeled
dynamics. Instead, a sound approach is followed by SP-ML
and SPK-ML in which the estimation of the hyperparameters

0.0511098.75Seconds100102εFtPNP-VSSP2-VSNP-MLSP2-MLSP-MLSPK-ML0.0511098.75Seconds100102εTtis performed jointly, avoiding such bias. In the steady state
all these methods, with the exception of P, provide similar
performance; yet the two semi-parametric models (SP-ML
and SPK-ML) perform better both in terms of average as well
as standard deviation, as clearly shown in Figure 4 which

Fig. 4: Boxplots of the steady state (i.e. after 30 seconds,
see Figure 3) relative squared prediction errors εF
t and εT
t ,
computed with T = 25 corresponding to an horizon of 1.25
seconds.

in “steady state”, i.e. after
reports the boxplots of εF
the ﬁrst 30 seconds which is considered to be transient (see
Figure 3).

t and εT
t

VI. CONCLUSIONS

In this paper we have placed several algorithms used
for online learning the robot inverse dynamics in a com-
mon framework. Such algorithms are classiﬁed according
to the considered model (parametric, non-parametric, semi-
parametric with RBD mean and semi-parametric with RBD
kernel) and according to the way the hyperparameters are
estimated (VS approach and ML approach). We applied those
algorithms for online leaning the inverse dynamics of the
right arm of the iCub. The results showed the superiority of
the ML approach to estimate the hyperparameters. Finally,
semi-parametric models outperform the others. The latter re-
sult conﬁrms the fact the advantage in combining parametric
and non-parametric approaches together.

VII. ACKNOWLEDGMENTS

The authors gratefully acknowledge the contribution of the
IIT and IIT@MIT research groups lead by Francesco Nori
and Lorenzo Rosasco.

APPENDIX

We shall now show that the semi-parametric model with
RBD mean (17) can be obtained as a limiting case (as γ →
∞) of model (21). To do so, let us assume that the vector
parameter π in (17) is a zero mean Gaussian random vector
with covariance γ2I, independent of α in (17). This implies

that, conditionally on xs, ψ(cid:62)(xs)π is zero mean Gaussian
with covariance matrix γ2ψ(cid:62)(xs)ψ(xs) and it is independent
of α.
Therefore gm(xs) := ψ(cid:62)(xs)π + (φ(cid:62)(xs) ⊗ In)α is zero

mean Gaussian with covariance
E[gm(xt)gm(xs)(cid:62)] = γ2ψ(xt)(cid:62)ψ(xs)+ρ2(φ(xt)(cid:62)φ(xs)⊗In2),
equal to (20).

A well known connection between Bayes and Fisher (i.e.
assuming the parameter π is an unknown but ﬁxed quantity)
estimators, is that the latter can be obtained as a limiting
case of the former when:

• the parameter π is modeled as a zero mean Gaussian

• the variance of the prior distribution of π is let go to

vector with variance γ2I
inﬁnity by letting γ2 → ∞

To make this formal, let us stack the available data ys,
s = 1, .., t in the vector y and stack correspondingly the
regressors ψ(cid:62)(xs) and φ(cid:62)(xs) ⊗ I in the matrices Ψ and Φ
respectively, so that models (17) and (21) can be written as

y = Ψπ + Φα + e

(32)

where e is deﬁned with the same rule as y. The minimum
variance estimators of π and α under (20) are thus given by:

ˆπ = cov(π, y)V ar−1{y}y

= γ2Ψ(cid:62)(cid:0)γ2ΨΨ(cid:62) + ρ2ΦΦ2 + σ2I(cid:1)−1
= ρ2Φ(cid:62)(cid:0)γ2ΨΨ(cid:62) + ρ2ΦΦ2 + σ2I(cid:1)−1

ˆα = cov(α, y)V ary−1y

(33)

y

y.

so that, from (33)

Deﬁning R := ρ2ΦΦ(cid:62) + σ2I and using the matrix inversion
lemma we have:

(cid:0)γ2ΨΨ(cid:62) + ρ2ΦΦ(cid:62) + σ2I(cid:1)−1
= R−1 − R−1Ψ(cid:0)Ψ(cid:62)R−1Ψ + γ−2I(cid:1)−1
ˆπ = γ2(cid:16)
= (cid:0)Ψ(cid:62)R−1Ψ + γ−2I(cid:1)−1
ˆα = ρ2Φ(cid:62)(cid:16)

=(cid:0)γ2ΨΨ(cid:62) + R(cid:1)−1
I − Ψ(cid:62)R−1Ψ(cid:0)Ψ(cid:62)R−1Ψ + γ−2I(cid:1)−1(cid:17)
Ψ(cid:62)(cid:17)
I − R−1Ψ(cid:0)Ψ(cid:62)R−1Ψ + γ−2I(cid:1)−1

and, similarly

Ψ(cid:62)R−1y

=

Ψ(cid:62)R−1

Ψ(cid:62)R−1y

R−1y

= ρ2Φ(cid:62)R−1[y − Ψˆπ]

Clearly, as γ → ∞, we have that ˆπ converges to the weighted
least squares estimate

ˆπW LS =(cid:0)Ψ(cid:62)R−1Ψ(cid:1)−1

Ψ(cid:62)R−1y

(34)

and ˆα converges to

ˆα = ρ2Φ(cid:62)R−1[y − ΨˆπW LS].

On the other hand the marginal likelihood function for model
(32), under (16) i.e. when π is considered as an unknown
parameter, has the form:

Lm(y) = −2log(p(y))

= log(det(R)) + (y − Ψπ)(cid:62)R−1(y − Ψπ).

NP-VSSP2-VSNP-MLSP2-MLSP-MLSPK-ML00.511.5ForcesNP-VSSP2-VSNP-MLSP2-MLSP-MLSPK-ML00.511.5TorquesWhen (ρ, σ2, τ ) are kept ﬁxed, minimization w.r.t. to π can
be performed in closed form, and yields exactly the weighted
least squares solution (34). Note however that, even for γ →
∞, the marginal likelihoods of y given the hyperparameters
(ρ, σ2, τ ) computed using models (16) and (20) are different.
In fact, if (16) is postulated, and π is solved for as above,
one obtains the proﬁle marginal log-likelihood ˆLm(y) :=
Lm(y)|π=ˆπW LS
ˆLm(y) = log(det(R)) + (y − ΨˆπW LS)(cid:62)R−1(y − ΨˆπW LS)
(35)
where the hyperparameters (ρ, σ2, τ ) which are hidden in the
deﬁnition of R = ρ2ΦΦ(cid:62) + σ2I.

Instead, if (20) is postulated, the marginal log-likelihood

takes the form
LK(y) = log(det(γ2ΨΨ(cid:62) + R)) + y(cid:62)(γ2ΨΨ(cid:62) + R)−1y.
Using, as above, the matrix inversion Lemma on (γ2ΨΨ(cid:62) +
R), Sylvester determinant
identity and deﬁning ˆπ :=
(Ψ(cid:62)R−1Ψ + γ−2I)−1ΨR−1y, we obtain
LK(y) = log(det(R)) + log(det(I + γ2Ψ(cid:62)R−1Ψ))

+(y − Ψˆπ)(cid:62)R−1(y − Ψˆπ)
+ˆπ(cid:62)Ψ(cid:62)R−1(y − Ψˆπ).

As γ → ∞ we have that ˆπ → ˆπW LS and ˆπ(cid:62)Ψ(cid:62)R−1(y −
Ψˆπ) → 0 so that
LK(y) (cid:39) log(det(R)) + log(det(I + γ2Ψ(cid:62)R−1Ψ))

+(y − ΨˆπW LS)(cid:62)R−1(y − ΨˆπW LS).

(36)
The second term log(det(I + γ2Ψ(cid:62)R−1Ψ)) can be manip-
ulated as follows:

log(det(I + γ2Ψ(cid:62)R−1Ψ)) =
log(det(γ2I)) + log(det(γ−2I + Ψ(cid:62)R−1Ψ))
(cid:39) log(det(γ2I)) + log(det(Ψ(cid:62)R−1Ψ))

where the last approximation hold when γ → ∞. Inserting
the last expression in LK(y) we obtain that, up to the
constant log(det(γ2I)) which is not a function of ρ, σ2, τ,

LK(y) (cid:39) log(det(R)) + log(det(Ψ(cid:62)R−1Ψ))
+(y − ΨˆπW LS)(cid:62)R−1(y − ΨˆπW LS)

(cid:39) Lm(y) + log(det(Ψ(cid:62)R−1Ψ))

(37)

where the last equation shows that the two log lokelihoods
differ for a nontrivial term which have an inﬂuence on the
location of their minima.

REFERENCES

[1] B. Siciliano, L. Sciavicco, L. Villani, and G. Oriolo, Robotics: mod-
Springer Science & Business Media,

elling, planning and control.
2010.

[2] J. Hollerbach, W. Khalil, and M. Gautier, “Model identiﬁcation,” in

Springer Handbook of Robotics. Springer, 2008, pp. 321–344.

[3] C. Rasmussen and C. Williams, Gaussian Processes for Machine

Learning. The MIT Press, 2006.

[4] R. Rifkin, G. Yeo, and T. Poggio, “Regularized least-squares classi-
ﬁcation,” Nato Science Series Sub Series III Computer and Systems
Sciences, vol. 190, pp. 131–154, 2003.

[5] D. Nguyen-Tuong and J. Peters, “Using model knowledge for learning
inverse dynamics,” in IEEE International Conference on Robotics and
Automation, 2010.

[6] T. Wu and J. Movellan, “Semi-parametric gaussian process for robot
system identiﬁcation,” in IEEE/RSJ International Conference on In-
telligent Robots and Systems (IROS), 2012, pp. 725–731.

[7] S. J. Pan and Q. Yang, “A survey on transfer learning,” IEEE
Transactions on Knowledge and Data Engineering, vol. 22, no. 10,
pp. 1345–1359, 2010.

[8] B. Bocsi, L. Csat´o, and J. Peters, “Alignment-based transfer learning
for robot models,” in The 2013 International Joint Conference on
Neural Networks (IJCNN), 2013, pp. 1–7.

[9] D. Nguyen-Tuong and J. Peters, “Incremental online sparsiﬁcation for
model learning in real-time robot control,” Neurocomputing, vol. 74,
no. 11, pp. 1859–1867, 2011.

[10] J. S. de la Cruz, D. Kulic, W. S. Owen, E. Calisgan, and E. A. Croft,
“On-line dynamic model learning for manipulator control.” in SyRoCo,
2012, pp. 869–874.

[11] D. Nguyen-Tuong, M. Seeger, and J. Peters, “Model learning with
local gaussian process regression,” Advanced Robotics, vol. 23, no. 15,
pp. 2015–2034, 2009.

[12] A. Gijsberts and G. Metta, “Incremental learning of robot dynamics
using random features,” in IEEE International Conference on Robotics
and Automation (ICRA), 2011, pp. 951–956.

[13] A. Rahimi and B. Recht, “Random features for large-scale kernel
machines,” in Advances in neural information processing systems,
2007, pp. 1177–1184.

[14] J. Quinonero-Candela and C. E. Rasmussen, “A unifying view of
sparse approximate gaussian process regression,” The Journal of
Machine Learning Research, vol. 6, pp. 1939–1959, 2005.

[15] R. Camoriano, S. Traversaro, L. Rosasco, G. Metta, and F. Nori, “In-
cremental semiparametric inverse dynamics learning,” arXiv preprint
arXiv:1601.04549, 2016.

[16] G. James, D. Witten, T. Hastie, and R. Tibshirani, An introduction to

statistical learning. Springer, 2013, vol. 112.

[17] G. Metta, L. Natale, F. Nori, G. Sandini, D. Vernon, L. Fadiga,
C. Von Hofsten, K. Rosander, M. Lopes, J. Santos-Victor et al.,
“The icub humanoid robot: An open-systems platform for research
in cognitive development,” Neural Networks, vol. 23, no. 8, pp. 1125–
1134, 2010.

[18] S. Traversaro, A. Del Prete, R. Muradore, L. Natale, and F. Nori,
“Inertial parameter identiﬁcation including friction and motor dynam-
ics,” in 13th IEEE-RAS International Conference on Humanoid Robots
(Humanoids), 2013, pp. 68–73.

[19] G. Wahba, Spline models for observational data. Siam, 1990, vol. 59.
[20] N. Aronszajn, “Theory of reproducing kernels,” Trans. of the American

Mathematical Society, vol. 68, pp. 337–404, 1950.

[21] L. Ljung, System Identiﬁcation, Theory for the User. Prentice Hall,

education, 2002.

[27] G. Pillonetto and A. Chiuso, “Tuning complexity in regularized kernel-
based regression and linear system identiﬁcation: The robustness of the
marginal likelihood estimator,” Automatica, vol. 58, pp. 106 – 117,
2015.

1997.

[22] S. Ivaldi, M. Fumagalli, M. Randazzo, F. Nori, G. Metta, and
G. Sandini, “Computing robot internal/external wrenches by means of
inertial, tactile and f/t sensors: theory and implementation on the icub,”
in Humanoid Robots (Humanoids), 2011 11th IEEE-RAS International
Conference on, 2011, pp. 521–528.

[23] U. Pattacini, F. Nori, L. Natale, G. Metta, and G. Sandini, “An exper-
imental evaluation of a novel minimum-jerk cartesian controller for
humanoid robots,” IEEE/RSJ International Conference on Intelligent
Robots and Systems, pp. 1668–1674, 2010.

[24] F. Nori, S. Traversaro, J. Eljaik, F. Romano, A. Del Prete, and
D. Pucci, “icub whole-body control through force regulation on rigid
non-coplanar contacts,” Frontiers in Robotics and AI, p. 18, 2015.

[25] A. Tacchetti, P. Mallapragada, M. Santoro, and R. Rosasco, “Gurls:
A least squares library for supervised learning,” Journal of Machine
Learning Research, vol. 14, pp. 3201–3205, 2013.

[26] J. M. Maciejowski, Predictive control: with constraints.

Pearson

