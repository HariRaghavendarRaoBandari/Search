6
1
0
2

 
r
a

 

M
5
1

 
 
]

V
C
.
s
c
[
 
 

1
v
0
3
5
4
0

.

3
0
6
1
:
v
i
X
r
a

Object Contour Detection with a Fully Convolutional Encoder-Decoder Network

Jimei Yang

Adobe Research
jimyang@adobe.com

Brian Price

Adobe Research
bprice@adobe.com

Scott Cohen

Adobe Research
scohen@adobe.com

Honglak Lee

University of Michigan, Ann Arbor

Ming-Hsuan Yang

UC Merced

honglak@umich.edu

mhyang@ucmerced.edu

Abstract

We develop a deep learning algorithm for contour de-
tection with a fully convolutional encoder-decoder network.
Different from previous low-level edge detection, our al-
gorithm focuses on detecting higher-level object contours.
Our network is trained end-to-end on PASCAL VOC with
reﬁned ground truth from inaccurate polygon annotations,
yielding much higher precision in object contour detection
than previous methods. We ﬁnd that the learned model gen-
eralizes well to unseen object classes from the same super-
categories on MS COCO and can match state-of-the-art
edge detection on BSDS500 with ﬁne-tuning. By combining
with the multiscale combinatorial grouping algorithm, our
method can generate high-quality segmented object propos-
als, which signiﬁcantly advance the state-of-the-art on PAS-
CAL VOC (improving average recall from 0.62 to 0.67) with
a relatively small amount of candidates (∼1660 per image).

1. Introduction

Object contour detection is fundamental for numerous
vision tasks. For example, it can be used for image seg-
mentation [41, 3], for object detection [15, 18], and for oc-
clusion and depth reasoning [20, 2]. Given its axiomatic
importance, however, we ﬁnd that object contour detection
is relatively under-explored in the literature. At the same
time, many works have been devoted to edge detection that
responds to both foreground objects and background bound-
aries (Figure 1 (b)). In this paper, we address “object-only”
contour detection that is expected to suppress background
boundaries (Figure 1(c)).

Edge detection has a long history. Early research fo-
cused on designing simple ﬁlters to detect pixels with high-
est gradients in their local neighborhood, e.g. Sobel [16]
and Canny [8]. The main problem with ﬁlter based methods

(a) Image

(b) [12]

Figure 1. Object contour detection. Given input images (a), our
model can effectively learn to detect contours of foreground ob-
jects (c) in contrast to traditional edge detection (b).

Ours

is that they only look at the color or brightness differences
between adjacent pixels but cannot tell the texture differ-
ences in a larger receptive ﬁeld. With the advance of texture
descriptors [35], Martin et al. [37] combined color, bright-
ness and texture gradients in their probabilistic boundary
detector. Arbelaez et al. [3] further improved upon this
by computing local cues from multiscale and spectral clus-
tering, known as gPb, which yields state-of-the-art accu-
racy. However, the globalization step of gPb signiﬁcantly
increases the computational load. Lim and Dollar [30, 12]
analyzed the clustering structure of local contour maps and
developed efﬁcient supervised learning algorithms for fast
edge detection [12]. These efforts lift edge detection to a
higher abstract level, but still fall below human perception
due to their lack of object-level knowledge.

Recently deep convolutional networks [29] have demon-
strated remarkable ability of learning high-level represen-
tations for object recognition [18, 10]. These learned fea-
tures have been adopted to detect natural image edges [25,
6, 43, 47] and yield a new state-of-the-art performance [47].
All these methods require training on ground truth contour
annotations. However, since it is very challenging to col-
lect high-quality contour annotations, the available datasets
for training contour detectors are actually very limited and

1

in small scale. For example, the standard benchmarks,
Berkeley segmentation (BSDS500) [36] and NYU depth v2
(NYUDv2) [44] datasets only include 200 and 381 training
images, respectively. Therefore, the representation power
of deep convolutional networks has not been entirely har-
nessed for contour detection.
In this paper, we scale up
the training set of deep learning based contour detection to
more than 10k images on PASCAL VOC [14]. To address
the quality issue of ground truth contour annotations, we
develop a dense CRF [26] based method to reﬁne the object
segmentation masks from polygons.

Given image-contour pairs, we formulate object contour
detection as an image labeling problem.
Inspired by the
success of fully convolutional networks [34] and deconvolu-
tional networks [38] on semantic segmentation, we develop
a fully convolutional encoder-decoder network (CEDN).
Being fully convolutional, our CEDN network can operate
on arbitrary image size and the encoder-decoder network
emphasizes its asymmetric structure that differs from de-
convolutional network [38]. We initialize our encoder with
VGG-16 net [45] (up to the “fc6” layer) and to achieve
dense prediction of image size our decoder is constructed
by alternating unpooling and convolution layers where un-
pooling layers re-use the switches from max-pooling layers
of encoder to upscale the feature maps. During training,
we ﬁx the encoder parameters (VGG-16) and only optimize
decoder parameters. This allows the encoder to maintain its
generalization ability so that the learned decoder network
can be easily combined with other tasks, such as bounding
box regression or semantic segmentation.

We evaluate the trained network on unseen object cat-
egories from BSDS500 and MS COCO datasets [31], and
ﬁnd the network generalizes well to objects in similar
“super-categories” to those in the training set, e.g. it gener-
alizes to objects like “bear” in the “animal” super-category
since “dog” and “cat” are in the training set. We also show
the trained network can be easily adapted to detect natural
image edges through a few iterations of ﬁne-tuning, which
produces comparable results with the state-of-the-art algo-
rithm [47].

An immediate application of contour detection is gen-
erating object proposals. Previous literature has investi-
gated various methods of generating bounding box or seg-
mented object proposals by scoring edge features [49, 11]
and combinatorial grouping [46, 9, 4] and etc. In this pa-
per, we use a multiscale combinatorial grouping (MCG) al-
gorithm [4] to generate segmented object proposals from
our contour detection. As a result, our method signiﬁcantly
improves the quality of segmented object proposals on the
PASCAL VOC 2012 validation set, achieving 0.67 average
recall from overlap 0.5 to 1.0 with only about 1660 can-
didates per image, compared to the state-of-the-art average
recall 0.62 by original gPb-based MCG algorithm with near

5140 candidates per image. We also evaluate object pro-
posals on the MS COCO dataset with 80 object classes and
analyze the average recalls from different object classes and
their super-categories.

The key contributions are summarized below:
• We develop a simple yet effective fully convolutional
encoder-decoder network for object contour prediction
and the trained model generalizes well to unseen object
classes from the same super-categories, yielding sig-
niﬁcantly higher precision in object contour detection
than previous methods.

• We show we can ﬁne tune our network for edge detec-
tion and match the state-of-the-art in terms of precision
and recall.

• We generate accurate object contours from imperfect
polygon based segmentation annotations, which makes
it possible to train an object contour detector at scale.
• Our method obtains state-of-the-art results on seg-
mented object proposals by integrating with combina-
torial grouping [4].

2. Related Work
Semantic contour detection. Hariharan et al. [19] study
top-down contour detection problem. Their semantic con-
tour detectors [19] are devoted to ﬁnd the semantic bound-
aries between different object classes. Although they con-
sider object instance contours while collecting annotations,
they choose to ignore the occlusion boundaries between ob-
ject instances from the same class. As combining bottom-
up edges with object detector output, their method can be
extended to object instance contours but might encounter
challenges of generalizing to unseen object classes.

Occlusion boundary detection. Hoiem et al. [20] study
the problem of recovering occlusion boundaries from a sin-
gle image.
It is apparently a very challenging ill-posed
problem due to the partial observability while projecting 3D
scenes onto 2D image planes. They formulate a CRF model
to integrate various cues: color, position, edges, surface ori-
entation and depth estimates. We believe our instance-level
object contours will provide another strong cue for address-
ing this problem that is worth investigating in the future.

Object proposal generation. There is a large body of
works on generating bounding box or segmented object pro-
posals. Hosang et al. [21] and Jordi et al. [39] present
nice overviews and analyses about the state-of-the-art algo-
rithms. Bounding box proposal generation [46, 49, 11, 1] is

motivated by efﬁcient object detection. One of their draw-
backs is that bounding boxes usually cannot provide accu-
rate object localization. More related to our work is gener-
ating segmented object proposals [4, 9, 13, 22, 24, 27, 40].
At the core of segmented object proposal algorithms is con-
tour detection and superpixel segmentation. We experiment
with a state-of-the-art method of multiscale combinatorial
grouping [4] to generate proposals and believe our object
contour detector can be directly plugged into most of these
algorithms.

3. Object Contour Detection

In this section, we introduce our object contour detec-
tion method with the proposed fully convolutional encoder-
decoder network.
3.1. Fully Convolutional Encoder-Decoder Network
We formulate contour detection as a binary image la-
beling problem where “1” and “0” indicates “contour” and
“non-contour”, respectively. Image labeling is a task that re-
quires both high-level knowledge and low-level cues. Given
the success of deep convolutional networks [29] for learn-
ing rich feature hierarchies, image labeling has been greatly
advanced, especially on the task of semantic segmenta-
tion [10, 34, 32, 48, 38, 33]. Among those end-to-end
methods, fully convolutional networks [34] scale well up
to the image size but cannot produce very accurate label-
ing boundaries; unpooling layers help deconvolutional net-
works [38] to generate better label localization but their
symmetric structure introduces a heavy decoder network
which is difﬁcult to train with limited samples.

We borrow the ideas of full convolution and unpooling
from above two works and develop a fully convolutional
encoder-decoder network for object contour detection. The
network architecture is demonstrated in Figure 2. We use
the layers up to “fc6” from VGG-16 net [45] as our encoder.
Since we convert the “fc6” to be convolutional, so we name
it “conv6” in our decoder.

Due to the asymmetric nature of image labeling prob-
lems (image input and mask output), we break the sym-
metric structure of deconvolutional networks and introduce
a light-weighted decoder. The ﬁrst layer of decoder “de-
conv6” is designed for dimension reduction that projects
4096-d “conv6” to 512-d with 1× 1 kernel so that we can
re-use the pooling switches from “conv5” to upscale the
feature maps by twice in the following “deconv5” layer.
The number of channels of every decoder layer is properly
designed to allow unpooling from its corresponding max-
pooling layer. All the decoder convolution layers except
“deconv6” use 5× 5 kernels. All the decoder convolution
layers except the one next to the output label are followed
by relu activation function. We believe the features chan-
nels of our decoder are still redundant for binary labeling

addressed here and thus also add a dropout layer after each
relu layer. A complete decoder network setup is listed in Ta-
ble 1 and the loss function is simply the pixel-wise logistic
loss.

name
setup
kernel
acti
name
setup
kernel

activation

Table 1. Decoder network setup.

deconv5

deconv6
deconv4
unpool-conv unpool-conv
1×1×512 5×5×512 5×5×256

conv

relu

relu

relu

deconv1
deconv3
pred
unpool-conv unpool-conv unpool-conv conv
5×5×32 5×5×1
5×5×128
sigmoid

deconv2
5×5×64

relu

relu

relu

3.2. Contour Ground Truth Reﬁnement

Drawing detailed and accurate contours of objects is a
challenging task for human beings. This is why many large
scale segmentation datasets [42, 14, 31] provide contour an-
notations with polygons as they are less expensive to col-
lect at scale. However, because of unpredictable behaviors
of human annotators and limitations of polygon representa-
tion, the annotated contours usually do not align well with
the true image boundaries and thus cannot be directly used
as ground truth for training. Among all, the PASCAL VOC
dataset is a widely-accepted benchmark with high-quality
annotation for object segmentation. VOC 2012 release in-
cludes 11540 images from 20 classes covering a majority of
common objects from categories such as “person”, “vehi-
cle”, “animal” and “household”, where 1464 and 1449 im-
ages are annotated with object instance contours for train-
ing and validation. Hariharan et al. [19] further contribute
more than 10000 high-quality annotations to the remaining
images. Together there are 10582 images for training and
1449 images for validation (the exact 2012 validation set).
We choose this dataset for training our object contour detec-
tor with the proposed fully convolutional encoder-decoder
network.

The original PASCAL VOC annotations leave a thin un-
labeled (or uncertain) area between occluded objects (Fig-
ure 3(b)). To ﬁnd the high-ﬁdelity contour ground truth for
training, we need to align the annotated contours with the
true image boundaries. We consider contour alignment as
a multi-class labeling problem and introduce a dense CRF
model [26] where every instance (or background) is as-
signed with one unique label. The dense CRF optimization
then ﬁlls the uncertain area with neighboring instance labels
so that we obtain reﬁned contours at the labeling boundaries
(Figure 3(d)). We also experimented with the Graph Cut
method [7] but ﬁnd it usually produces jaggy contours due
to its shortcutting bias (Figure 3(c)).

Figure 2. Architecture of the proposed fully convolutional encoder-decoder network.

(a) Image

(b) Annotation

(c) GraphCut reﬁnement

(d) DenseCRF reﬁnement

Figure 3. Contour reﬁnement. The polygon based annotations (a)
cannot be directly used for training due to its inaccurate boundaries
(thin white area reﬂects unlabeled pixels between objects). We
align them to image boundaries by re-labeling the uncertain areas
with dense CRF (d), compared to Graph Cut (c).

3.3. Training

We train the network using Caffe [23]. For each training
image, we randomly crop four 224×224×3 patches and to-
gether with their mirrored ones compose a 224×224×3×8
minibatch. The ground truth contour mask is processed in
the same way. We initialize the encoder with pre-trained
VGG-16 net and the decoder with random values. During
training, we ﬁx the encoder parameters and only optimize
the decoder parameters. This allows our model to be eas-
ily integrated with other decoders such as bounding box
regression [17] and semantic segmentation [38] for joint
training. As the “contour” and “non-contour” pixels are ex-
tremely imbalanced in each minibatch, the penalty for being
“contour” is set to be 10 times the penalty for being “non-
contour”. We use the Adam method [5] to optimize the net-
work parameters and ﬁnd it is more efﬁcient than standard
stochastic gradient descent. We set the learning rate to 10−4
and train the network with 30 epochs with all the training
images being processed each epoch. Note that we ﬁx the
training patch to 224× 224 for memory efﬁciency and the
learned parameters can be used on images of arbitrary size
because of its fully convolutional nature.

4. Results

In this section, we evaluate our method on contour detec-
tion and proposal generation using three datasets: PASCAL
VOC 2012, BSDS500 and MS COCO. We will explain the
details of generating object proposals using our method af-
ter the contour detection evaluation. More evaluation results
are in the supplementary materials.
4.1. Contour Detection

Given trained models, all the test images are fed-forward
through our CEDN network in their original sizes to pro-
duce contour detection maps. The detection accuracies are
evaluated by four measures: F-measure (F), ﬁxed contour
threshold (ODS), per-image best threshold (OIS) and av-
erage precision (AP). Note that a standard non-maximum
suppression is used to clean up the predicted contour maps
(thinning the contours) before evaluation.

Figure 4. PR curve for contour detection on the PASCAL VOC
2012 validation set.

PASCAL val2012. We ﬁrst present results on the PAS-
CAL VOC 2012 validation set, shortly “PASCAL val2012”,
with comparisons to three baselines,
structured edge
detection (SE) [12], singlescale combinatorial grouping
(SCG) and multiscale combinatorial grouping (MCG) [4].
Precision-recall curves are shown in Figure 4. Note that we

Max poolingMax poolingMax poolingMax poolingMax poolingUnpoolingUnpoolingUnpoolingUnpoolingUnpoolingConvolutional EncoderConv1Conv2Conv3Conv4Conv6Conv5Deconv1Deconv2Deconv3Deconv4Deconv5Deconv6Convolutional DecoderPred00.10.20.30.40.50.60.70.80.91Recall00.10.20.30.40.50.60.70.80.91PrecisionContour Detection on PASCAL val2012[F=.74] GT-DenseCRF[F=.57] CEDN[F=.37] SE[F=.37] MCG[F=.36] SCG(a)

(b)

(c)

Figure 5. Example results on PASCAL VOC val2012. In each row from left to right we present (a) input image, (b) ground truth annotation,
(c) edge detection [12], (d) our object contour detection and (e) our best object proposals.

(d)

(e)

use the originally annotated contours instead of our reﬁned
ones as ground truth for unbiased evaluation. Accordingly
we consider the reﬁned contours as the upper bound since
our network is learned from them. Its precision-recall value
is referred as “GT-DenseCRF” with a green spot in Figure 4.
Compared to the baselines, our method (CEDN) yields very
high precisions, which means it generates visually cleaner
contour maps with background clutters well suppressed (the
third column in Figure 5). Note that the occlusion bound-
aries between two instances from the same class are also
well recovered by our method (the second example in Fig-
ure 5). We also note that there is still a big performance gap
between our current method (F=0.57) and the upper bound
(F=0.74), which requires further research for improvement.

BSDS500 with ﬁne-tuning. BSDS500 [36] is a standard
benchmark for contour detection. Different from our object-
centric goal, this dataset is designed for evaluating natural
edge detection that includes not only object contours but
also object interior boundaries and background boundaries
(examples in Figure 6(b)). It includes 500 natural images
with carefully annotated boundaries collected from multi-
ple users. The dataset is divided into three parts: 200 for
training, 100 for validation and the rest 200 for test. We
ﬁrst examine how well our CEDN model trained on PAS-

(a)

(b)

(c)

(d)

Figure 6. Example results on BSDS500 test set. In each row from
left to right we present (a) input image, (b) ground truth contour,
(c) contour detection with pretrained CEDN and (d) contour de-
tection with ﬁne-tuned CEDN.

CAL VOC can generalize to unseen object categories in
this dataset.
Interestingly, as shown in the Figure 6(c),
most of wild animal contours, e.g. elephants and ﬁsh are
accurately detected and meanwhile the background bound-

aries, e.g. building and mountains are clearly suppressed.
We further ﬁne-tune our CEDN model on the 200 training
images from BSDS500 with a small learning rate (10−5)
for 100 epochs. As a result, the boundaries suppressed
by pretrained CEDN model (“CEDN-pretrain”) re-surface
from the scenes. Quantitatively, we evaluate both the pre-
trained and ﬁne-tuned models on the test set in compar-
isons with previous methods. Figure 7 shows that 1) the

Figure 7. PR curve for contour detection on the BSDS500 set set.
pretrained CEDN model yields a high precision but a low
recall due to its object-selective nature and 2) the ﬁne-tuned
CEDN model achieves comparable performance (F=0.79)
with the state-of-the-art method (HED) [47]. Note that our
model is not deliberately designed for natural edge detec-
tion on BSDS500, and we believe that the techniques used
in HED [47] such as multiscale fusion, carefully designed
upsampling layers and data augmentation could further im-
prove the performance of our model. A more detailed com-
parison is listed in Table 2.

Table 2. Contour detection results on BSDS500.

Human
SCG [4]
SE [12]

DeepEdge [6]

DeepContour [43]

HED [47]
HED-new 1

CEDN-pretrain

CEDN

ODS OIS
.80
.80
.758
.739
.767
.746
.772
.753
.756
.773
.804
.782
.808
.788
.635
.610
.788
.804

AP
-

.773
.803
.807
.797
.833
.840
.580
.821

4.2. Object Proposal Generation

Object proposals are important mid-level representations
in computer vision. Most of proposal generation methods

1This is the latest result from http://vcl.ucsd.edu/hed/

are built upon effective contour detection and superpixel
segmentation. Thus the improvements on contour detection
will immediately boost the performance of object proposals.
We choose the MCG algorithm to generate segmented ob-
ject proposals from our detected contours. The MCG algo-
rithm is based the classic gPb contour detector. It ﬁrst com-
putes ultrametric contour maps from multiscale and then
aligns them into a single hierarchical segmentation. To ob-
tain object proposals, a multi-objective optimization is de-
signed to reduce the redundancy of combinatorial grouping
of adjacent regions. The reduced set of grouping candidates
are then ranked according to their low-level features as the
ﬁnal segmented object proposals. Note that the hierarchi-
cal segmentation can be obtained directly from single scale,
which leads to a fast algorithm of singlescale combinatorial
grouping (SCG). Based on the procedure above, we simply
replace gPb with our CEDN contour detector to generate
proposals. The multiscale and singlescale versions are re-
ferred to as “CEDNMCG” and “CEDNSCG”, respectively.
We evaluate the quality of object proposals by two mea-
sures: Average Recall (AR) and Average Best Overlap
(ABO). Both measures are based on the overlap (Jaccard
index or Intersection-over-Union) between a proposal and
a ground truth mask. AR is measured by 1) counting the
percentage of objects with their best Jaccard above a cer-
tain threshold T and then 2) averaging them within a range
of thresholds T ∈ [0.5, 1.0]. It is established in [21, 39]
to benchmark the quality of bounding box and segmented
object proposals. ABO is measured by calculating the best
proposal’s Jaccard for every ground truth object and then 2)
averaging them over all the objects.

We compare with state-of-the-art algorithms: MCG,
SCG, Category Independent object proposals (CI) [13],
Constraint Parametric Min Cuts (CPMC) [9], Global and
Local Search (GLS) [40], Geodesic Object Proposals
(GOP) [27], Learning to Propose Objects (LPO) [28], Re-
cycling Inference in Graph Cuts (RIGOR) [22], Selective
Search (SeSe) [46] and Shape Sharing (ShSh) [24]. Note
that these abbreviated names are inherited from [4].

PASCAL val2012. Figure 8 shows that CEDNMCG
achieves 0.67 AR and 0.83 ABO with ∼ 1660 proposals per
image, which improves the second best MCG by 8% in AR
and by 3% in ABO with a third as many proposals. It takes
0.1 second to compute the CEDN contour map for a PAS-
CAL image on a high-end GPU and 18 seconds to generate
proposals with MCG on a standard CPU. We notice that the
CEDNSCG achieves similar accuracies with CEDNMCG,
but it only takes less than 3 seconds to run SCG. We also
plot the per-class ARs in Figure 10 and ﬁnd that CEDN-
MCG and CEDNSCG improves MCG and SCG for all of
the 20 classes. Notably, the bicycle class has the worst AR
and we guess it is likely because of its incomplete annota-

00.10.20.30.40.50.60.70.80.91Recall00.10.20.30.40.50.60.70.80.91PrecisionContour Detection on BSDS500[F=.80] Human[F=.79] CEDN[F=.79] HED[F=.61] CEDN-pretrain(a)

(b)

(c)

Figure 9. Example results on MS COCO val2014. In each row from left to right we present (a) input image, (b) ground truth annotation,
(c) edge detection [12], (d) our object contour detection and (e) our best object proposals.

(d)

(e)

tions. Some examples of object proposals are demonstrated
in Figure 5(d).

MS COCO val2014. We present results in the MS COCO
2014 validation set, shortly “COCO val2014” that
in-
cludes 40504 images annotated by polygons from 80 object
classes. This dataset is more challenging due to its large
variations of object categories, contexts and scales. Com-
pared to PASCAL VOC, there are 60 unseen object classes
for our CEDN contour detector. Note that we did not train
CEDN on MS COCO. We report the AR and ABO results
in Figure 11. It turns out that the CEDNMCG achieves a
competitive AR to MCG with a slightly lower recall from
fewer proposals, but a weaker ABO than LPO, MCG and
SeSe. Taking a closer look at the results, we ﬁnd that our
CEDNMCG algorithm can still perform well on known ob-
jects (ﬁrst and third examples in Figure 9) but less effec-
tively on certain unknown object classes, such as food (sec-
ond example in Figure 9). It is likely because those novel
classes, although seen in our training set (PASCAL VOC),
are actually annotated as background. For example, there is

a “dining table” class but no “food” class in the PASCAL
VOC dataset. Quantitatively, we present per-class ARs in
Figure 12 and have following observations:

• CEDN obtains good results on those classes that share
common super-categories with PASCAL classes, such
as “vehicle”, “animal” and “furniture”.

• CEDN fails to detect the objects labeled as “back-
ground” in the PASCAL VOC training set, such as
“food” and “applicance”.

• CEDN works well on unseen classes that are not preva-
lent in the PASCAL VOC training set, such as “sports”.

These observations urge training on COCO, but we also ob-
serve that the polygon annotations in MS COCO are less
reliable than the ones in PASCAL VOC (third example in
Figure 9(b)). We will need more sophisticated methods for
reﬁning the COCO annotations.

Figure 11. Average best overlap and average recall on the MS COCO 2014 validation set.

Figure 12. Average recall per class on the MS COCO 2014 validation set.

5. Conclusion and Future Work

We have developed an object-centric contour detection
method using a simple yet efﬁcient fully convolutional
encoder-decoder network. Concerned with the imperfect
contour annotations from polygons, we have developed a
reﬁnement method based on dense CRF so that the pro-
posed network has been trained in an end-to-end manner.
As a result, the trained model yielded high precision on
PASCAL VOC and BSDS500, and has achieved compara-
ble performance with the state-of-the-art on BSDS500 after
ﬁne-tuning. We have combined the proposed contour de-
tector with multiscale combinatorial grouping algorithm for
generating segmented object proposals, which signiﬁcantly
advances the state-of-the-art on PASCAL VOC. We also
found that the proposed model generalizes well to unseen
object classes from the known super-categories and demon-
strated competitive performance on MS COCO without re-
training the network.

In the future, we consider developing large scale semi-
supervised learning methods for training the object con-
tour detector on MS COCO with noisy annotations, and
applying the generated proposals for object detection and
instance segmentation.

References
[1] B. Alexe, T. Deselaers, and V. Ferrari. Measuring the object-

ness of image windows. PAMI, 34:2189–2202, 2012.

[2] M. R. Amer, S. Youseﬁ, R. Raich, and S. Todorovic. Monoc-
ular extraction of 2.1 D sketch using constrained convex op-
timization. IJCV, 112(1):23–42, 2015.

[3] P. Arbel´aez, M. Maire, C. Fowlkes, and J. Malik. Con-
tour detection and hierarchical image segmentation. PAMI,
33(5):898–916, 2011.

[4] P. Arbel´aez, J. Pont-Tuset, J. Barron, F. Marques, and J. Ma-

lik. Multiscale combinatorial grouping. In CVPR, 2014.

[5] J. Ba and D. Kingma. Adam: A method for stochastic opti-

mization. In ICLR, 2015.

[6] G. Bertasius, J. Shi, and L. Torresani. Deepedge: A multi-
scale bifurcated deep network for top-down contour detec-
tion. In CVPR, 2015.

[7] Y. Y. Boykov and M.-P. Jolly. Interactive graph cuts for op-
timal boundary & region segmentation of objects in n-d im-
ages. In ICCV, 2001.

[8] J. Canny. A computational approach to edge detection.

PAMI, (6):679–698, 1986.

[9] J. Carreira and C. Sminchisescu. Constrained parametric
min-cuts for automatic object segmentation. In CVPR, 2010.
[10] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and
A. L. Yuille. Semantic image segmentation with deep con-

101102103104Number of candidates0.10.150.20.250.30.350.4RecallAverage Recall on COCO val2014CEDNMCGCEDNSCGMCGSCGGLSGOPLPORIGORSeSe101102103104Number of candidates0.10.20.30.40.50.60.7Jaccard at instance level (Ji)Average Best Overlap on COCO val2014CEDNMCGCEDNSCGMCGSCGGLSGOPLPORIGORSeSepersonbicyclecarmotorcycleairplanebustraintruckboattraffic lightfire hydrantstop signparking meterbenchbirdcatdoghorsesheepcowelephantbearzebragiraffebackpackumbrellahandbagtiesuitcasefrisbeeskissnowboardsports ballkitebaseball batbaseball gloveskateboardsurfboardtennis racketbottlewine glasscupforkknifespoonbowlbananaapplesandwichorangebroccolicarrothot dogpizzadonutcakechaircouchpotted plantbeddining tabletoilettvlaptopmouseremotekeyboardcell phonemicrowaveoventoastersinkrefrigeratorbookclockvasescissorsteddy bearhair driertoothbrush00.20.40.60.81Average Recall per Class on COCO val2014Number of candidates = 1000vehicleoutdooranimalaccessorysportskitchenfoodfurnitureelectronicapplianceindoorCEDNSCGSCG[15] V. Ferrari, L. Fevrier, F. Jurie, and C. Schmid. Groups
of adjacent contour segments for object detection. PAMI,
30(1):36–51, 2008.

[16] D. A. Forsyth and J. Ponce. Computer Vision: A Modern

Approach. Pearson Education, 2003.

[17] R. Girshick. Fast R-CNN. In ICCV, 2015.
[18] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich fea-
ture hierarchies for accurate object detection and semantic
segmentation. In CVPR, 2014.

[19] B. Hariharan, P. Arbel´aez, L. Bourdev, S. Maji, and J. Malik.

Semantic contours from inverse detectors. In ICCV, 2011.

[20] D. Hoiem, A. N. Stein, A. Efros, and M. Hebert. Recovering

occlusion boundaries from a single image. In ICCV, 2007.

[21] J. Hosang, R. Benenson, P. Doll´ar, and B. Schiele. What

makes for effective detection proposals? PAMI, 2015.

[22] A. Humayun, F. Li, and J. M. Rehg. RIGOR: Reusing infer-
ence in graph cuts for generating object regions. In CVPR,
2014.

[23] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Gir-
shick, S. Guadarrama, and T. Darrell. Caffe: Convolu-
tional architecture for fast feature embedding. arXiv preprint
arXiv:1408.5093, 2014.

[24] J. Kim and K. Grauman. Shape sharing for object segmenta-

tion. In ECCV, 2012.

[25] J. J. Kivinen, C. K. Williams, and N. Heess. Visual bound-
ary prediction: A deep neural prediction network and quality
dissection. In AISTATS, 2014.

[26] P. Kr¨ahenb¨uhl and V. Koltun. Efﬁcient inference in fully con-
nected CRFs with gaussian edge potentials. In NIPS, 2011.
[27] P. Kr¨ahenb¨uhl and V. Koltun. Geodesic object proposals. In

ECCV, 2014.

[28] P. Kr¨ahenb¨uhl and V. Koltun. Learning to propose objects.

Figure 8. Average best overlap and average recall on the PASCAL
VOC 2012 validation set.

In CVPR, 2015.

[29] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

classiﬁcation with deep convolutional neural networks.
NIPS, 2012.

Imagenet
In

Figure 10. Per-class average recall on the PASCAL VOC 2012
validation set.

volutional nets and fully connected crfs.
arXiv:1412.7062, 2014.

arXiv preprint

[11] M.-M. Cheng, Z. Zhang, W.-Y. Lin, and P. Torr. BING: Bina-
rized normed gradients for objectness estimation at 300fps.
In CVPR, 2014.

[12] P. Doll´ar and C. Zitnick. Structured forests for fast edge de-

tection. In ICCV, 2013.

[13] I. Endres and D. Hoiem. Category independent object pro-

posals. In ECCV, 2010.

[14] M. Everingham, L. J. V. Gool, C. K. I. Williams, J. M. Winn,
and A. Zisserman. The Pascal visual object classes (VOC)
challenge. IJCV, 88(2):303–338, 2010.

[30] J. J. Lim, C. L. Zitnick, and P. Doll´ar. Sketch tokens: A
learned mid-level representation for contour and object de-
tection. In CVPR, 2013.

[31] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ra-
manan, P. Doll´ar, and C. L. Zitnick. Microsoft COCO: Com-
mon objects in context. In ECCV, 2014.

[32] S. Liu, J. Yang, C. Huang, and M.-H. Yang. Multi-objective

convolutional learning for face labeling. In CVPR, 2015.

[33] Z. Liu, X. Li, P. Luo, C. C. Loy, and X. Tang. Semantic im-
age segmentation via deep parsing network. In ICCV, 2015.
[34] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional

networks for semantic segmentation. In CVPR, 2015.

[35] J. Malik, S. Belongie, T. Leung, and J. Shi. Contour and

texture analysis for image segmentation. IJCV, 2001.

[36] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database
of human segmented natural images and its application to
evaluating segmentation algorithms and measuring ecologi-
cal statistics. In ICCV, 2001.

[37] D. R. Martin, C. C. Fowlkes, and J. Malik. Learning to detect
natural image boundaries using local brightness, color, and
texture cues. PAMI, 26(5):530–549, 2004.

101102103104Number of candidates0.10.20.30.40.50.60.7RecallAverage Recall on PASCAL val2012CNNMCGCNNSCGMCGSCGCICPMCGLSGOPLPORIGORSeSeShSh101102103104Number of candidates0.10.20.30.40.50.60.70.80.9Jaccard at instance level (Ji)Average Best Overlap on PASCAL val2012CNNMCGCNNSCGMCGSCGCICPMCGLSGOPLPORIGORSeSeShShbicyclechairpottedplantbottlecarpersonboatmotorbikesheepdiningtableaeroplanesofahorsetvmonitorbirdbustraincowdogcat00.10.20.30.40.50.60.70.80.91Average Recall per Class on pascal2012 val2012Number of candidates = 1000CNNMCGCNNSCGMCGSCG[38] H. Noh, S. Hong, and B. Han. Learning deconvolution net-

work for semantic segmentation. In ICCV, 2015.

[39] J. Pont-Tuset and L. J. V. Gool. Boosting object proposals:

From Pascal to COCO. In ICCV, 2015.

[40] P. Rantalankila, J. Kannala, and E. Rahtu. Generating ob-
ject segmentation proposals using global and local search. In
CVPR, 2014.

[41] C. Rother, V. Kolmogorov, and A. Blake. Grabcut -
interactive foreground extraction using iterated graph cuts.
ACM Transactions on Graphics (SIGGRAPH), 2004.

[42] B. C. Russell, A. Torralba, K. P. Murphy, and W. T. Free-
man. LabelMe: a database and web-based tool for image
annotation. IJCV, 2008.

[43] W. Shen, X. Wang, Y. Wang, X. Bai, and Z. Zhang. Deep-
contour: A deep convolutional feature learned by positive-
sharing loss for contour detection. In CVPR, 2015.
[44] N. Silberman, P. Kohli, D. Hoiem, and R. Fergus.

segmentation and support inference from rgbd images.
ECCV, 2012.

Indoor
In

[45] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. In ICLR, 2015.
[46] K. E. A. van de Sande, J. R. R. Uijlingsy, T. Gevers, and
A. W. M. Smeulders. Segmentation as selective search for
object recognition. In ICCV, 2011.

[47] S. Xie and Z. Tu. Holistically-nested edge detection.

In

ICCV, 2015.

[48] S. Zheng, S. Jayasumana, B. Romera-Paredes, V. Vineet,
Z. Su, D. Du, C. Huang, and P. Torr. Conditional random
ﬁelds as recurrent neural networks. In ICCV, 2015.

[49] C. L. Zitnick and P. Doll´ar. Edge boxes: Locating object

proposals from edge. In ECCV, 2014.

Appendix
Comparing with HED trained on BSD and PASCAL
VOC. We trained the HED model on PASCAL VOC us-
ing the same training data as our model with 30000 itera-
tions. Its contour prediction precision-recall curve is illus-
trated in Figure 13 with comparisons to our CEDN model,
the pre-trained HED model on BSDS (referred as HEDB)
and others. It can be seen that the F-score of HED is im-
proved (from 0.42 to 0.44) by training on PASCAL VOC
but still signiﬁcantly lower than CEDN (0.57). We will
present visual examples in the revised submission. We fur-
ther feed the HED edge maps into MCG for generating pro-
posals and compare their average recalls with others in Fig-
ure 13. We refer the results from PASCAL-trained HED
as HEDMCG and the ones from pre-trained HED on BSDS
as HEDBMCG. Unfortunately, the HEDMCG does not im-
prove upon HEDBMCG. With 1000 proposals, the aver-
age recalls of CEDNMCG, HEDMCG and HEDBMCG are
about 0.65, 0.57 and 0.55, respectively.

Figure 13. Full comparisons with HED on PASCAL val2012.

00.10.20.30.40.50.60.70.80.91Recall00.10.20.30.40.50.60.70.80.91PrecisionContour Detection on PASCAL val2012[F=.74] GT-DenseCRF[F=.57] CEDN[F=.44] HED[F=.42] HEDB[F=.37] SE[F=.37] MCG[F=.36] SCG101102103104Number of candidates0.10.20.30.40.50.60.7RecallAverage Recall on PASCAL val2012CEDNMCGCEDNSCGHEDBMCGHEDMCGMCGSCGCICPMCGLSGOPLPORIGORSeSeShSh