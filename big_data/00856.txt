6
1
0
2

 
r
a

M
2

 

 
 
]
L
M

.
t
a
t
s
[
 
 

1
v
6
5
8
0
0

.

3
0
6
1
:
v
i
X
r
a

Molecular Graph Convolutions: Moving Beyond Fingerprints

Steven Kearnes
Stanford University

Kevin McCloskey

Google Inc.

Marc Berndl
Google Inc.

kearnes@stanford.edu

mccloskey@google.com

marcberndl@google.com

Vijay Pande

Stanford University
pande@stanford.edu

Patrick Riley

Google Inc.

pfr@google.com

Abstract

Molecular “ﬁngerprints” encoding structural
information are the workhorse of cheminfor-
matics and machine learning in drug discovery
applications. However, ﬁngerprint representa-
tions necessarily emphasize particular aspects
of the molecular structure while ignoring oth-
ers, rather than allowing the model to make
data-driven decisions. We describe molecular
graph convolutions, a novel machine learning ar-
chitecture for learning from undirected graphs,
speciﬁcally small molecules. Graph convolu-
tions use a simple encoding of the molecular
graph (atoms, bonds, distances, etc.), allowing
the model to take greater advantage of informa-
tion in the graph structure.

1 Introduction
Computer-aided drug design requires representations
of molecules that can be related to biological activ-
ity or other experimental endpoints. These repre-
sentations encode structural features, physical prop-
erties, or activity in other assays (Todeschini and
Consonni, 2009; Petrone et al., 2012). The recent
advent of “deep learning” has enabled the use of
very raw representations that are less application-
speciﬁc when building machine learning models (Le-
Cun et al., 2015). For instance, image recognition
models that were once based on complex features ex-
tracted from images are now trained exclusively on
the pixels themselves—deep architectures can “learn”
appropriate representations for input data. Conse-
quently, deep learning systems for drug screening or
design should beneﬁt from molecular representations

that are as complete and general as possible rather
than relying on application-speciﬁc features or encod-
ings.

First-year chemistry students quickly become fa-
miliar with a common representation for small
molecules:
the molecular graph. Figure 1 gives
an example of the molecular graph for ibuprofen,
an over-the-counter non-steroidal anti-inﬂammatory
drug (NSAID). The atoms and bonds between atoms
form the nodes and edges, respectively, of the graph.
Both atoms and bonds have associated properties,
such as atom type and bond order. Although the
basic molecular graph representation does not cap-
ture the quantum mechanical structure of molecules
or necessarily express all of the information that it
might suggest to an expert medicinal chemist,
its
ubiquity in academia and industry makes it a desir-
able starting point for machine learning on chemical
information.

Figure 1: Molecular graph for ibuprofen. Unmarked ver-
tices represent carbon atoms, and bond order is indicated
by the number of lines used for each edge.

1

Here we describe molecular graph convolutions, a
deep learning system using a representation of small
molecules as undirected graphs of atoms. Graph con-
volutions extract meaningful features from simple de-
scriptions of the graph structure—atom and bond
properties, and graph distances—to form molecule-
level representations that can be used in place of ﬁn-
gerprint descriptors in conventional machine learning
applications.

2 Related Work
The history of molecular representation is extremely
diverse (Todeschini and Consonni, 2009) and a full
review is outside the scope of this report. Below we
describe examples from several major branches of the
ﬁeld to provide context for our work. Additionally,
we review several recent examples of graph-centric
approaches in cheminformatics.

Much of cheminformatics is based on so-called
“2D” molecular descriptors that attempt to capture
relevant structural features derived from the molecu-
lar graph. In general, 2D features are computation-
ally inexpensive and easy to interpret and visualize.
One of the most common representations in this class
is extended-connectivity ﬁngerprints (ECFP), also re-
ferred to as circular or Morgan ﬁngerprints (Rogers
and Hahn, 2010). Starting at each heavy atom, a
“bag of fragments” is constructed by iteratively ex-
panding outward along bonds (usually the algorithm
is terminated after 2–3 steps). Each unique frag-
ment is assigned an integer identiﬁer, which is often
hashed into a ﬁxed-length representation or “ﬁnger-
print”. Additional descriptors in this class include de-
compositions of the molecular graph into subtrees or
ﬁxed-length paths (GraphSim Toolkit, OpenEye Sci-
entiﬁc Software, Inc., Santa Fe, NM, USA), as well as
atom pair (AP) descriptors that encode atom types
and graph distances (number of intervening bonds)
for all pairs of atoms in a molecule (Carhart et al.,
1985).

Many representations encode 3D information, with
special emphasis on molecular shape and electrostat-
ics as primary drivers of biological interactions in
real-world systems. Pharmacophore models attempt
to identify particular interactions relative to a com-
mon alignment that are critical for activity (Horvath,
2011). Rapid overlay of chemical structures (ROCS)
aligns pairs of pre-generated conformers and calcu-
lates shape and electrostatic (“color”) similarity us-
ing Gaussian representations of atoms and color fea-

2

tures deﬁned by a simple force ﬁeld (Rush et al.,
2005). ROCS can also be used to generate alignments
for calculation of electrostatic ﬁeld similarity (Much-
more et al., 2006). Ultrafast shape recognition (USR)
calculates alignment-free 3D similarity by comparing
distributions of intramolecular distances (Ballester
and Richards, 2007).

Alternative approaches incorporate a broader bio-
logical context into molecular descriptions. For ex-
ample, aﬃnity ﬁngerprints report activity in a panel
of assays, allowing structurally distinct compounds
to be related by similar pharmacology (Briem and
Lessel, 2000; Petrone et al., 2012). The similarity en-
semble approach (SEA) builds upon traditional ﬁn-
gerprint similarity methods for protein target iden-
tiﬁcation, relating proteins by the similarity of their
ligands (Keiser et al., 2007).

Other approaches from both the cheminformatics
and the machine learning community directly oper-
ate on graphs in a way similar to how we do here.
The “molecular graph networks” of Merkwirth and
Lengauer (2005) iteratively update a state variable
on each atom with learned weights speciﬁc to each
atom type–bond type pair. Similarly, Micheli (2009)
presents a more general formulation of the same
concept of iterated local information transfer across
edges and applies this method to predicting the boil-
ing point of alkanes.

Scarselli et al. (2009) similarly deﬁnes a local op-
eration on the graph. Rather than iterating a ﬁxed
number of times, they demonstrate that a ﬁxed point
across all the local functions can be found and deﬁnes
that ﬁxed point as the computed value for each node
in the graph.
In another vein, Lusci et al. (2013)
convert undirected molecular graphs to a directed re-
cursive neural net and take an ensemble over multiple
conversions.

Recently, Duvenaud et al. (2015) presented an ar-
chitecture trying to accomplish many of the same
goals as this work. The architecture was based on
generalizing the ﬁngerprint computation such that it
can be learned via backpropagation. They demon-
strate that this architecture improves predictions of
solubility and photovoltaic eﬃciency but not binding
aﬃnity.

Bruna et al. (2013) introduce convolutional deep
networks on spectral representations of graphs. How-
ever, these methods apply when the graph struc-
ture is ﬁxed across examples and only the label-
ing/features on individual nodes varies.

Convolutional networks on non-Euclidean mani-

folds were described by Masci et al. (2015). The prob-
lem addressed was to describe the shape of the man-
ifold (such as the surface of a human being) in such
a way that the shape descriptor of a particular point
was invariant to perturbations such as movement and
deformation. Unlike this and other work on molec-
ular graphs, their goal was to have a descriptor for
every point and not a descriptor for the entire shape.
Therefore, the problem of how to combine all the lo-
cal shape descriptors into a single descriptor was not
addressed.

3 Methods
3.1 Desired invariants of a model
A primary goal of designing a deep learning archi-
tecture is to restrict the set of functions that can
be learned to ones that match the desired proper-
ties from the domain. For example, in image un-
derstanding, spatial convolutions force the model to
learn functions that are invariant to translation.

For a deep learning architecture taking a molecular
graph as input, some arbitrary choice must be made
for the order that the various atoms and bonds are
presented to the model. Since that choice is arbitrary,
we want:
Property 1 (Order invariance). The output of the
model should be invariant to the order that the atom
and bond information is encoded in the input.

Note that many current procedures for ﬁngerprint-
ing molecules achieve Property 1. We will now grad-
ually construct an architecture which achieves Prop-
erty 1 while making available a richer space of learn-
able parameters.

The ﬁrst basic unit of representation is an atom
layer which contains an n-dimensional vector associ-
ated with each atom. Therefore the atom layer is a
2 dimensional matrix indexed ﬁrst by atom. Part of
the original input will be encoded in such an atom
layer and the details of how we construct the original
input vector are discussed in Section 3.4. The next
basic unit of representation is a pair layer which con-
tains an n-dimensional vector associated with each
pair of atoms. Therefore, the pair layer is a 3 di-
mensional matrix where the ﬁrst two dimensions are
indexed by atom. Note that the pair input can con-
tain information not just about edges but about any
arbitrary pair. Notably, we will encode the graph dis-
tance (length of shortest path from one atom to the

3

other) in the input pair layer. The order of the atom
indexing for the atom and pair layer inputs must be
the same.

We will describe various operations to compute
new atom and pair layers with learnable parameters
at every step. Notationally, let Ax be the value of a
particular atom layer x and P y be the value of a par-
ticular pair layer y. The inputs that produce those
values should be clear from the context. Ax
a refers to
(a,b) refers
the value of atom a in atom layer x and P y
to the value of pair (a, b) in pair layer y.

In order to achieve Property 1 for the overall ar-
chitecture, we need a diﬀerent type of invariance for
each atom and pair layer.

Property 2 (Atom and pair permutation invari-
ance). The values of an atom layer and pair permute
with the original input layer order. More precisely, if
the inputs are permuted with a permutation operator
Q, then for all layers x, y, Ax and P y are permuted
with operator Q as well.

In other words, Property 2 means that from a single
atom’s (or pair’s) perspective, its value in every layer
is invariant to the order of the other atoms (or pairs).
Since molecules are undirected graphs, we will also

maintain the following:

Property 3 (Pair order invariance). For all pair lay-
ers y, P y

(a,b) = P y

(b,a)

Property 3 is easy to achieve at the input layer and

the operations below will maintain this.

Properties 2 and 3 make it easy to construct a
molecule-level representation from an atom or pair
such that the molecule-level representation achieves
Property 1 (see Section 3.3).

3.2 Invariant-preserving operations
We now deﬁne a series of operations that maintain
the above properties.

Throughout, f represents an arbitrary function
and g represents an arbitrary commutative function
(g returns the same result regardless of the order the
arguments are presented). In this work, f is a learned
linear operator with a rectiﬁed linear activation func-
tion and g is a sum.

The most trivial operation is to combine one or
more layers of the same type by applying the same op-
eration to every atom or pair. Precisely, this means if
you have layers x1, x2, . . . , xn and function f, you can

compute a new atom layer from the previous atom
layer (A → A) as

a , Ax2

a = f(Ax1
Ay

a )
a , . . . , Axn

(1)
or pair layer from the previous pair layer (P → P) as
(2)

a,b = f(P x1
P y

a,b, P x2

a,b)
a,b, . . . , P xn

Since we apply the same function for every atom/pair,
we refer to this as a convolution. All the trans-
formations we develop below will have this convolu-
tion nature of applying the same operation to every
atom/pair, maintaining Property 2.

When operating on pairs of atoms,

instead of
putting all pairs through this function, you could se-
lect a subset. In Section 4.3.3 we show experiments
for restricting the set of pairs to those that are less
than some graph distance away.

Figure 3: A → P operation.

a and Ax

Note that just applying g to Ax
b would main-
tain Properties 2 and 3 but we use this more com-
plex form. While commutative operators (such as
max pooling) are common in neural networks, com-
mutative operators with learnable parameters are not
common. Therefore, we use f to give learnable pa-
rameters while maintaining the desired properties.

Figure 2: P → A operation.

Next, consider an operation that takes a pair layer
x and constructs an atom layer y (P → A). The
operation is depicted in Figure 2. Formally:

a = g(f(P x
Ay

(a,b)), f(P x

(a,c)), f(P x

(a,d)), ...)

(3)

Figure 4: Weave module.

In other words, take all pairs of which a is a part,
run them through f, and combine them with g. Note
that Property 3 means we can choose an arbitrary
one of P x
The most interesting construction is making a pair
layer from an atom layer (A → P). The operation is
graphically depicted in Figure 3 and formally as

(a,b) or P x

(b,a).

ab = g(f(Ax
P y

a, Ax

b ), f(Ax

b , Ax

a))

(4)

4

Once we have all the primitive operations on atom
and pair layers (A → A, P → P, P → A, A → P),
we can combine these into one module. We call this
the Weave module (Figure 4) because the atoms and
pair layers cross back and forth to each other. The
module can be stacked to an arbitrary depth similar
the Inception module that inspired it (Szegedy et al.,
2015).

PxAyabacadv1f(Pxab)v2f(Pxac)v3f(Pxad)ag(v1,v2,v3,...)Py(a,b)g(v1,v2)v1f(Axa,Axb)v2f(Axb,Axa)abAxAk′′(A→A)0Pk′Ak′PkAkPk+1Ak+1Pk′′ (A→P)0(P→A)0 (P→P)0(A→A)1(P→P)13.3 Molecule-level features
The construction of the Weave module maintains
Properties 2 and 3. What about overall order invari-
ance (Property 1)? At the end of a stack of Weave
modules we are left with an n-dimensional vector as-
sociated with every atom and an m-dimensional vec-
tor associated with every pair. We need to turn this
into a molecule-level representation with some com-
mutative function of these vectors.

In related work (Merkwirth and Lengauer, 2005;
Duvenaud et al., 2015; Lusci et al., 2013), a sim-
ple unweighted sum is often used to combine order-
dependent atom features
into order-independent
molecule-level features. However, reduction to a sin-
gle value does not capture the distribution of learned
features. We experimented with an alternative ap-
proach and created “fuzzy” histograms for each di-
mension of the feature vector.

A fuzzy histogram is described by a set of mem-
bership functions that are functions with range [0, 1]
representing the membership of the point in the bin
(Zadeh, 1965). A standard histogram has member-
ship functions which are 1 in the bin and 0 everywhere
else. For each point, we normalize so that the total
contribution to all bins is 1. The value of a bin in
the histogram over all points is just the sum of the
normalized contributions for all the points.

Figure 5 gives an example of a fuzzy histogram
composed of three Gaussian bins. A histogram is
built for each dimension of the feature vectors and the
concatenation of those histograms is the molecule-
level representation.

In this work we used Gaussian membership func-
tions (which are unnormalized versions of the stan-
dard Gaussian PDF) with eleven bins spanning a
Gaussian distribution with mean of 0 and unit stan-
dard deviation, shown in Figure A10.

Throughout this paper, we construct the molecule-
level features only from the top-level atom features
and not the pair features. This is to restrict the
total number of feature vectors that must be sum-
marized while still providing information about the
entire molecule.

Before the molecule-level featurization, we do one
ﬁnal convolution on the atoms. Since molecule-level
featurization can be a major bottleneck in the model,
this convolution expands the depth so that each di-
mension of the atom feature vector contains less infor-
mation and therefore less information is lost during
the molecule-level featurization. On this convolution,
we do not use a ReLU activation function to avoid the
histogram having many points at zero.

Once you have a molecule-level representation, this
becomes a more standard multitask problem. We
follow the common approach (Ramsundar et al.,
2015; Ma et al., 2015; Unterthiner et al., 2015) of a
small number of fully connected layers on top of the
molecule-level features followed by standard softmax
classiﬁcation.

The overall architecture is depicted in Figure 6.
Table 1 lists hyperparameters and default values for
graph convolution models. For molecules that have
more than the maximum number of atoms, only a
subset of atoms (and therefore atom pairs) are repre-
sented in the input encoding. This subset depends on
the order in which the atoms are traversed by the fea-
turization code and should be considered arbitrary.
In models with multiple Weave modules it is possible
to vary the convolution depths in a module-speciﬁc
way. However, the models in this work used the same
settings for all Weave modules.

Figure 5: Fuzzy histogram with three Gaussian “bins”.
Each curve represents the membership function for a dif-
ferent bin, indicating the degree to which a point con-
tributes to that bin. The vertical blue line represents an
example point which contributes normalized densities of
< 0.01, ∼ 0.25, and ∼ 0.75 to the bins (from left to right).

3.4 Input featurization
The initial atom and pair features are summarized in
Table 2 and Table 3, respectively. The features are a
mix of ﬂoating point, integer, and binary values (all
encoded as ﬂoating point numbers in the network).
The feature set is intended to be broad, but not nec-
essarily exhaustive, and we recognize that some fea-
tures can potentially be derived from or correlated
to a subset of the others (e.g., atom hybridization
can be determined by inspecting the bonds that atom
makes). We performed experiments using a “simple”

5

42024Feature Value0.00.20.40.60.81.0Membership FractionTable 1: Graph convolution model hyperparameters.

Group

Input

Weave

Reduction

Hyperparameter
Maximum number of atoms per molecule
Maximum atom pair graph distance
Number of Weave modules
(A → A)0 convolution depth
(A → P)0 convolution depth
(P → P)0 convolution depth
(P → A)0 convolution depth
(A → A)1 convolution depth
(P → P)1 convolution depth
Final atom layer convolution depth
Reduction to molecule-level features

Post-reduction Fully-connected layers (number of units)

Training

Batch size
Learning rate
Optimization method

Table 2: Atom features.

Default Value
60
2
1
50
50
50
50
50
50
128
Gaussian histogram
2000, 100
96
0.003
Adagrad

Feature
Atom type∗
Chirality
Formal charge
Partial charge
Ring sizes
Hybridization
Hydrogen bonding Whether this atom is a hydrogen bond donor and/or acceptor (binary values).
Aromaticity

Description
H, C, N, O, F, P, S, Cl, Br, I, or metal (one-hot or null).
R or S (one-hot or null).
Integer electronic charge.
Calculated partial charge.
For each ring size (3–8), the number of rings that include this atom.
sp, sp2, or sp3 (one-hot or null).

Whether this atom is part of an aromatic system.

* Included in the “simple” featurization (see Section 4.2).

Table 3: Atom pair features.

Feature
Bond type∗
Graph distance∗ For each distance (1–7), whether the shortest path between the atoms in the

Description
Single, double, triple, or aromatic (one-hot or null).

Same ring

pair is less than or equal to that number of bonds (binary values).
Whether the atoms in the pair are in the same ring.

Size
11
2
1
1
6
3
2
1
27

Size
4
7

1
12

* Included in the “simple” featurization (see Section 4.2).

6

3.5 Datasets
We used a dataset collection nearly identical to the
one described by Ramsundar et al. (2015) except for
some changes to the data processing pipeline (in-
cluding the duplicate merging process for the Tox21
dataset) and diﬀerent cross-validation fold divisions.
Brieﬂy, there are 259 datasets divided into four
groups indicating their source: PubChem BioAs-
say (Wang et al., 2012) (PCBA, 128 datasets), the
“maximum unbiased validation” datasets constructed
by Rohrer and Baumann (Rohrer and Baumann,
2009) (MUV, 17 datasets), the enhanced directory
of useful decoys (Mysinger et al., 2012) (DUD-E, 102
datasets), and the training set for the Tox21 challenge
(Huang et al., 2015) (Tox21, 12 datasets). The com-
bined dataset contained over 38 million data points
and included targets from many diﬀerent biological
classes.

3.6 Model training and evaluation
Graph convolution and traditional neural network
models were implemented in TensorFlow (Abadi
et al., 2015), an open-source library for machine
learning. Some additional features such as replicated
training were not available outside of Google at the
time of this work. Models were evaluated by their
5-fold mean area under the receiver operating char-
acteristic curve (AUC).

Graph convolution models were trained for 3–8 mil-
lion steps using the Adagrad optimizer (Duchi et al.,
2011) with learning rate 0.003 and batch size 96, with
periodic checkpointing. All convolution and fully-
connected layer outputs were batch normalized (Ioﬀe
and Szegedy, 2015) prior to applying the ReLU non-
linearity.

To establish a baseline, we also trained pyrami-
dal multitask neural network (MTNN) (Ramsundar
et al., 2015), random forest (RF), and logistic regres-
sion (LR) models using Morgan ﬁngerprints with ra-
dius 2 (essentially equivalent to ECFP4) generated
with RDKit (Landrum). The pyramidal MTNN had
two hidden layers (2000 and 100 units, respectively)
with rectiﬁed linear activations, and was trained for
20 million steps using the SGD optimizer with batch
size 128 and a learning rate of 0.0003, with periodic
checkpointing. (Training was allowed to continue to
20 million steps after we observed undertraining on
the Tox21 datasets.) Additionally, this model used
0.25 dropout (Srivastava et al., 2014), initial weight
standard deviations of 0.01 and 0.04 and initial biases

Figure 6: Abstract graph convolution architecture.

subset of these features in an eﬀort to understand
their relative contributions to learning (Section 4.2),
but many other questions about speciﬁcs of the input
featurization are left to future work.

All

features were

generated with RDKit
(Landrum),
including Gasteiger atomic partial
charges (Gasteiger and Marsili, 1980). Although our
featurization includes space for hydrogen atoms, we
did not use explicit hydrogens in any of our experi-
ments in order to conserve memory and emphasize
contributions from heavy atoms.

Other deep learning applications with more “natu-
ral” inputs such as computer vision and speech recog-
nition still require some input engineering; for exam-
ple, adjusting images to a speciﬁc size or scale, or
transforming audio into the frequency domain. Like-
wise, the initial values for the atom and pair layers
describe these primitives in terms of properties that
are often considered by medicinal chemists and other
experts in the ﬁeld, allowing the network to use or ig-
nore them as needed for the task at hand. One of the
purposes of this work is to demonstrate that learning
can occur with as little preprocessing as possible. Ac-
cordingly, we favor simple descriptors that are more
or less “obvious”.

7

Molecule featuresFully connected layersAPFeature weave modulesAPAFinal convolutionSoftmaxTask-specificof 0.5 and 3.0 in the respective hidden layers. This
model did not use batch normalization.

In graph convolution and pyramidal MTNN mod-
els, active compounds were weighted in the cost func-
tion such that the total active weight equalled the
total inactive weight within each dataset. For eval-
uation, a target step (a multiple of 1000) was cho-
sen such that the mean over all datasets (excluding
DUD-E) of the 5-fold mean AUC—using the closest
checkpoint to the target step in each cross-validation
fold—was maximized.

Logistic regression models were trained with the
LogisticRegression class in scikit-learn (Pedregosa
et al., 2011), with uniform example weighting. Val-
ues for the penalty and C parameters were chosen
by grid search. Random forest models were trained
using the
scikit-learn RandomForestClassifier
with hyperparameters similar to those described
by Ma et al. (2015), speciﬁcally n_estimators=100,
max_features=1/3.,
and min_samples_split=6,
with uniform example weighting.

4 Results
4.1 Proof of concept
With so many hyperparameters to adjust, we sought
to establish a centerpoint from which to investigate
speciﬁc questions. After several experiments, we
settled on a simple model with two Weave mod-
ules, a maximum atom pair distance of 2, Gaussian
histogram molecule-level reductions, and two fully-
connected layers of size 2000 and 100, respectively.
This model is listed in Table 4 as W2N2. Each graph
convolution model in Table 4 is derived from this cen-
terpoint by varying a single hyperparameter.

To statistically compare the graph convolution
models to current state of the art, for each model
(and within each dataset group) we do a sign test
compared to the Pyramidal (2000, 100) MTNN or
the Random Forest model. The sign test calculates
the proportion of datasets that achieved a higher 5-
fold mean AUC than the baseline model. In Table 4
we report a Wilson score interval for this propor-
tion at 95% conﬁdence. Models with intervals that
do not include 0.5 are considered signiﬁcantly dif-
ferent in their performance. To calculate these in-
tervals, we used the proportion_confint function
in statsmodels (Seabold and Perktold, 2010) version
0.6.1 with method=‘wilson’ and alpha=0.05, count-
ing only non-zero diﬀerences in the sign test. We do

8

not report values for the DUD-E datasets since all
models achieved > 0.99 median 5-fold mean AUC for
that dataset group.

Graph convolution models achieve comparable per-
formance to the baseline pyramidal multitask neu-
ral network (MTNN) on the classiﬁcation tasks in
our dataset collection, which is a remarkable result
considering the simplicity of our input representa-
tion. Notably, many of the graph convolution mod-
els signiﬁcantly outperform the pyramidal MTNN
and random forest baselines on the Tox21 datasets.
Inspection of the training curves for the pyramidal
MTNN shows that the median 5-fold mean AUC for
the Tox21 datasets continues to increase through-
out training, but even after 20 million steps it has
not matched the graph convolution performance level
(data not shown; note that the pyramidal MTNN is
evaluated much earlier than 20 million steps, per the
procedure described in Section 3.6).

4.2 Input featurization
As a further proof of concept and to address the
importance of the initial featurization, we built a
model using a subset of features that match typical
2D structural diagrams seen in chemistry textbooks:
only atom type, bond type, and graph distance are
provided to the network. Figure 7 compares a model
built with this “simple” input featurization to a “full”
featurization containing all features from Table 2 and
Table 3. Both featurizations achieve similar median
5-fold mean AUC scores, although the “full” featur-
ization appears to have less variance on the MUV
datasets. This result suggests that the additional fea-
tures in the “full” representation are either mostly
ignored during training or can be derived from a sim-
pler representation of the molecular graph. Further
work is required to understand the importance of in-
dividual features, perhaps with datasets that are sen-
sitive to particular components of the input represen-
tation (such as hydrogen bonding or formal charge).
Figure 8 gives examples of how the initial atom fea-
tures for a single molecule (ibuprofen) evolve as they
progress through graph convolution Weave modules.
The initial atom and pair feature encodings for the
“full” featurization are depicted in panel (A). Com-
paring the initial atom features to their source molec-
ular graph, the aromatic carbons in the central ring
are clearly visible (and nearly identical in the featur-
ization). The pair features are more diﬃcult to inter-
pret visually, and mostly encode graph distance. As
the atom features are transformed by the Weave mod-

Table 4: Median 5-fold mean AUC values for reported models. Graph convolution models are labeled as WxNy,
where x and y denote the number of Weave modules and the maximum atom pair distance, respectively (see the
text for descriptions of the simple, sum, and RMS models). All graph convolution models feed into a Pyrami-
dal (2000, 100) MTNN after the molecule-level feature reduction step. Logistic Regression, Random Forest, and
Pyramidal (2000, 100) MTNN baselines were trained using Morgan ﬁngerprints (ECFP4) as input. For each model,
we report the 95% Wilson score interval for a sign test comparing that model to the best baseline model within each
dataset group (Pyramidal (2000, 100) MTNN for PCBA and MUV, Random Forest for Tox21), excluding DUD-E
(see Section 3.6 and Section 4).

Model
Logistic Regression
Random Forest
Pyramidal (2000, 100) MTNN
W2N2-simple
W2N2-sum
W2N2-RMS
W1N2
W2N1
W2N2
W2N3
W2N4
W2N∞
W3N2
W4N2

PCBA (n = 128)

MUV (n = 17)

Tox21 (n = 12)

Median
0.845
0.847
0.915
0.908
0.902
0.904
0.903
0.913
0.912
0.910
0.907
0.904
0.912
0.910

Sign Test CI Median
0.795
[0.05, 0.15]
0.764
[0.04, 0.14]
0.894
0.875
0.847
0.850
0.845
0.881
0.872
0.877
0.878
0.856
0.874
0.865

[0.20, 0.36]
[0.19, 0.34]
[0.13, 0.26]
[0.10, 0.23]
[0.25, 0.41]
[0.33, 0.50]
[0.34, 0.51]
[0.24, 0.40]
[0.12, 0.26]
[0.27, 0.44]
[0.33, 0.49]

Sign Test CI Median
0.801
[0.10, 0.47]
0.841
[0.06, 0.41]
0.808
0.866
0.854
0.863
0.856
0.870
0.871
0.864
0.861
0.853
0.859
0.866

[0.10, 0.47]
[0.13, 0.53]
[0.17, 0.59]
[0.06, 0.41]
[0.13, 0.53]
[0.31, 0.74]
[0.13, 0.53]
[0.10, 0.47]
[0.13, 0.53]
[0.22, 0.64]
[0.22, 0.64]

Sign Test CI
[0.01, 0.35]

[0.01, 0.35]
[0.65, 0.99]
[0.55, 0.95]
[0.65, 0.99]
[0.47, 0.91]
[0.65, 0.99]
[0.65, 0.99]
[0.65, 0.99]
[0.65, 0.99]
[0.55, 0.95]
[0.55, 0.95]
[0.65, 0.99]

ules (panel (B)), they become more heterogeneous
and reﬂective of their unique chemical environments.
“Simple” features behave similarly, beginning with
rather sterile initial values and quickly diverging as
neighborhood information is included by Weave mod-
ule operations (panel (C)). Comparison of the “full”
and “simple” atom features after the second Weave
module shows that both featurizations lead to quali-
tatively similar feature distributions. Figure A8 and
Figure A9 show similar behavior for pair features.

4.3 Hyperparameter sensitivity
4.3.1 Number of Weave modules
In relatively “local” models with limited atom pair
distance, successive Weave modules update atom fea-
tures with information from progressively larger re-
gions of the molecule. This suggests that the num-
ber of Weave modules is a critical hyperparameter to
optimize, analogous to the number of hidden layers
in traditional neural networks. Figure 9 compares
models with 2–4 Weave modules to a model with a
single Weave module. As expected, models with a
single Weave layer are outperformed by deeper ar-
chitectures. However, there does not appear to be

much beneﬁt to using more than two Weave modules
(at least for our datasets), especially when the added
cost of training a deeper model is considered.

4.3.2 Alternative feature reductions
The reduction of atom features from the ﬁnal Weave
module to an order-invariant, molecule-level repre-
sentation is a major information bottleneck in graph
convolution models.
In related work, a simple un-
weighted sum (Duvenaud et al., 2015; Merkwirth and
Lengauer, 2005; Lusci et al., 2013) or root-mean-
square (Dieleman, March 17, 2015) (RMS) reduction
is used. Using a consistent base architecture with
two Weave modules and a maximum atom pair dis-
tance of 2 (see Section 4.3.3), we compared these tra-
ditional reductions with our Gaussian histogram ap-
proach. Models using the RMS reduction were some-
what unstable in terms of mean non-DUDE 5-fold
mean AUC during training (data not shown), and it
was not clear that these models had fully converged
when they were evaluated (after ∼ 4.5 million steps).
Gaussian histogram models have higher median 5-
fold mean AUC values in each dataset group (Fig-
ure A6) and improved scores relative to both sum and

9

Figure 8: Graph convolution feature evolution. Atoms or pairs are displayed on the y-axis and the dimensions of
the feature vectors are on the x-axis. (A) Conversion of the molecular graph for ibuprofen into atom and (unique)
atom pair features. (B) Evolution of atom features after successive Weave modules in a graph convolution model
with a W3N2 architecture and depth 50 convolutions in Weave modules. (C) Evolution of “simple” atom features
(see Section 4.2) starting from initial encoding and progressing through the Weave modules of a W2N2 architecture.

RMS reductions on the PCBA datasets (Figure 10).
These results are expected, since histograms capture
the distribution of feature values rather than summa-
rizing them in a single value.

every pair of atoms in the molecule) is being com-
bined in a way that may prevent useful information
from being available in later stages of the network.

4.3.3 Distance-dependent pair features
In the Weave module, atoms are informed about their
chemical environment by mixing with pair features in
the P → A operation. Recall that during this oper-
ation, pair features are combined for all pairs that
contain a given atom, yielding a new representation
for that atom. A critical parameter for this operation
is the maximum distance (in bonds) allowed between
the atoms of the pairs that are combined. If only ad-
jacent atoms are combined, then the resulting atom
features will reﬂect the local chemical environment.
As an alternative to increasing the number of Weave
modules, longer-range interactions can be captured
by increasing the maximum atom pair distance. How-
ever, our implementation of the P → A operation
uses a simple sum to combine pair features, which
means that a lot of information (possibly including

Figure 11 and Figure A7 show the performance of
several models with diﬀerent maximum pair distances
relative to a model that used only adjacent atom pairs
(N1). There is a slight decrease in performance with
increasing pair distance, and N∞ is clearly worse on
the PCBA and Tox21 datasets. This trend suggests
that the graph convolution models do not eﬀectively
handle or preserve the graph distance features for the
initial atom pairs.

To further investigate the eﬀect of distance in-
formation in Weave modules, we have begun ex-
perimenting with models that use distance-speciﬁc
weights for operations involving pair features in order
to maintain distance information explicitly through-
out the network. However, these results are prelimi-
nary and were not included in this report.

10

BACfeaturization includes additional

Figure 7: Comparison of models with “simple” and
“full” input featurizations. The simple featurization only
encodes atom type, bond type, and graph distance. The
full
features such as
aromaticity and hydrogen bonding propensity (see Sec-
tion 3.3 for more details). Conﬁdence intervals on box
√
plot medians are computed as ±1.57 × IQR/
N (McGill
et al., 1978).

5 Discussion
Graph convolutions are a deep learning architecture
for learning directly from undirected graphs.
In
this work, we emphasize their application to small
molecules—undirected graphs of atoms connected by
bonds—for virtual screening.
Starting from sim-
ple descriptions of atoms, bonds between atoms,
and pairwise relationships in a molecular graph, we
have demonstrated performance that is comparable
to state of the art multitask models trained on tradi-
tional molecular ﬁngerprint representations.

Flexibility is a highlight of the graph convolution
architecture. Because we begin with a representation
that encodes the complete molecular graph, graph
convolution models are free to use any of the avail-
able information for the task at hand.
In a sense,
every possible molecular “ﬁngerprint” is available to
the model.

Our experiments with the adjustable parameters in
graph convolution models suggest a relatively minor
sensitivity to the number of Weave modules and the
maximum distance between atom pairs. However, the

Figure 9: Comparison of models with diﬀerent numbers
of Weave modules with a model containing a single Weave
module. All models used a maximum atom pair distance
of two. The y-axis is cropped to emphasize diﬀerences
near zero.

use of Gaussian histograms for reduction to molecule-
level features is well supported, and there are trends
suggesting that a model with two Weave modules and
a maximum atom pair distance of 1 or 2 is a good
starting point for further optimization. Remarkably,
graph convolution models perform well with a “sim-
ple” input featurization containing only atom type,
bond type, and graph distances—essentially the in-
formation available from looking at Figure 1.

Looking forward, graph convolutions present a
“new hill to climb” in computational drug design and
cheminformatics. To list just a few of the opportu-
nities for further improvement: (1) additional opti-
mization of model hyperparameters such as Weave
module convolution depths, (2) ﬁne-tuning of archi-
tectural decisions, such as the choice of reduction in
the P → A operation (currently a sum, but perhaps a
Gaussian histogram or distance-dependent function),
(3) improvements in memory usage and training per-
formance, such as not handling all pairs of atoms or
implementing more eﬃcient versions of Weave mod-
ule operations. With these and other optimizations,
we anticipate that graph convolutions will exceed the
performance of the best available ﬁngerprint repre-
sentations.

11

pcbamuvtoxDataset group0.50.60.70.80.91.05 fold mean AUCmodelSimpleFullpcbamuvtoxDataset group0.040.020.000.020.04∆ 5 fold mean AUC vs. W1modelW2W3W4Figure 10: Comparison of root-mean-square (RMS) and
Gaussian histogram reductions vs. sum reduction. The
y-axis reports diﬀerence in 5-fold mean AUC relative to
sum reduction. All models used two Weave modules and
a maximum atom pair distance of two. The y-axis is
cropped to emphasize diﬀerences near zero.

Figure 11: Comparison of models with diﬀerent max-
imum atom pair distances to a model with a maximum
pair distance of one (bonded atoms). All models have
two Weave modules. The y-axis is cropped to emphasize
diﬀerences near zero.

Finally, we note that much (or most) of the infor-
mation required to represent biological systems and
the interactions responsible for small molecule activ-
ity is not encapsulated in the molecular graph. Bi-
ology takes place in a three-dimensional world, and
can be more sensitive to shape, electrostatics, quan-
tum eﬀects, and other properties that emerge from
the molecular graph than the structure of the graph
itself (see, e.g., Nicholls and Grant (2005); Böhm
et al. (2004), and related literature on “scaﬀold hop-
ping”). Additionally, most small molecules exhibit
3D conformational ﬂexibility that our graph repre-
sentation does not even attempt to describe. The
extension of deep learning methods (including graph
convolutions) to three-dimensional biology is an ac-
tive area of research (e.g., Wallach et al. (2015)) that
requires special attention to the added complexities of
multiple-instance learning in a relatively small-data
regime.

Acknowledgments
We thank Bharath Ramsundar, Brian Goldman, and
Robert McGibbon for helpful discussion. We also ac-

knowledge Manjunath Kudlur, Derek Murray, and
Rajat Monga for assistance with TensorFlow. S.K.
was supported by internships at Google Inc. and Ver-
tex Pharmaceuticals Inc.

References
Martın Abadi, Ashish Agarwal, Paul Barham, Eu-
gene Brevdo, Zhifeng Chen, Craig Citro, Greg S
Corrado, Andy Davis, Jeﬀrey Dean, Matthieu
Devin, et al. TensorFlow: Large-scale machine
learning on heterogeneous systems. Software avail-
able from tensorﬂow.org, 2015.

Pedro J Ballester and W Graham Richards. Ultrafast
shape recognition to search compound databases
for similar molecular shapes. Journal of computa-
tional chemistry, 28(10):1711–1723, 2007.

Hans-Joachim Böhm, Alexander Flohr, and Martin
Stahl. Scaﬀold hopping. Drug Discovery Today:
Technologies, 1(3):217–224, 2004.

Hans Briem and Uta F Lessel. In vitro and in silico
aﬃnity ﬁngerprints: Finding similarities beyond

12

pcbamuvtoxDataset group0.040.020.000.020.04∆ 5 fold mean AUC vs. summodelRMSGaussianpcbamuvtoxDataset group0.040.020.000.020.04∆ 5 fold mean AUC vs. N1modelN2N3N4N∞structural classes. Perspectives in Drug Discovery
and Design, 20(1):231–244, 2000.

reducing internal covariate shift. arXiv preprint
arXiv:1502.03167, 2015.

Joan Bruna, Wojciech Zaremba, Arthur Szlam, and
Spectral networks and locally
arXiv preprint

Yann LeCun.
connected networks on graphs.
arXiv:1312.6203, 2013.

Raymond E Carhart, Dennis H Smith,

and
R Venkataraghavan. Atom pairs as molecular fea-
tures in structure-activity studies: deﬁnition and
applications. Journal of Chemical Information and
Computer Sciences, 25(2):64–73, 1985.

Sander Dieleman. Classifying plankton with deep
neural networks. http://benanne.github.io, March
17, 2015.

John Duchi, Elad Hazan, and Yoram Singer. Adap-
tive subgradient methods for online learning and
stochastic optimization. The Journal of Machine
Learning Research, 12:2121–2159, 2011.

David K. Duvenaud, Dougal Maclaurin, Jorge
Aguilera-Iparraguirre, Rafael Gómez-Bombarelli,
Timothy Hirzel, Alán Aspuru-Guzik, and Ryan P.
Adams.
Convolutional networks on graphs
for
CoRR,
abs/1509.09292, 2015. URL http://arxiv.org/
abs/1509.09292.

learning molecular ﬁngerprints.

Johann Gasteiger and Mario Marsili. Iterative partial
equalization of orbital electronegativity—a rapid
access to atomic charges. Tetrahedron, 36(22):
3219–3228, 1980.

Michael J Keiser, Bryan L Roth, Blaine N Arm-
bruster, Paul Ernsberger, John J Irwin, and
Brian K Shoichet. Relating protein pharmacology
by ligand chemistry. Nature biotechnology, 25(2):
197–206, 2007.

Greg Landrum. RDKit: Open-source cheminfor-
matics; http://www.rdkit.org. URL http://www.
rdkit.org.

Yann LeCun, Yoshua Bengio, and Geoﬀrey Hinton.

Deep learning. Nature, 521(7553):436–444, 2015.

Alessandro Lusci, Gianluca Pollastri, and Pierre
Baldi. Deep architectures and deep learning in
chemoinformatics: the prediction of aqueous sol-
ubility for drug-like molecules. Journal of chemical
information and modeling, 53(7):1563–1575, 2013.
Junshui Ma, Robert P. Sheridan, Andy Liaw,
George E. Dahl, and Vladimir Svetnik. Deep neural
nets as a method for quantitative structureâĂŞac-
tivity relationships. Journal of Chemical Infor-
mation and Modeling, 55(2):263–274, 2015. doi:
10.1021/ci500747n. URL http://dx.doi.org/10.
1021/ci500747n. PMID: 25635324.

Jonathan Masci, Davide Boscaini, Michael M. Bron-
stein, and Pierre Vandergheynst. Shapenet: Con-
volutional neural networks on non-euclidean man-
ifolds. CoRR, abs/1501.06297, 2015. URL http:
//arxiv.org/abs/1501.06297.

GraphSim Toolkit, OpenEye Scientiﬁc Software,
Version 2.2.2.

Inc., Santa Fe, NM, USA.
http://www.eyesopen.com.

Robert McGill, John W Tukey, and Wayne A Larsen.
Variations of box plots. The American Statistician,
32(1):12–16, 1978.

Dragos Horvath.

Pharmacophore-based virtual
screening. In Chemoinformatics and computational
chemical biology, pages 261–298. Springer, 2011.

Ruili Huang, Menghang Xia, Dac-Trung Nguyen,
Tongan Zhao, Srilatha Sakamuru, Jinghua Zhao,
Sampada A Shahane, Anna Rossoshek, and Anton
Simeonov. Tox21challenge to build predictive mod-
els of nuclear receptor and stress response pathways
as mediated by exposure to environmental chemi-
cals and drugs. Frontiers in Environmental Sci-
ence, 3:85, 2015.

Sergey Ioﬀe and Christian Szegedy. Batch normal-
ization: Accelerating deep network training by

Christian Merkwirth and Thomas Lengauer. Au-
tomatic generation of complementary descriptors
with molecular graph networks. Journal of chem-
ical information and modeling, 45(5):1159–1168,
2005.

Alessio Micheli. Neural network for graphs: A contex-
tual constructive approach. Trans. Neur. Netw., 20
(3):498–511, March 2009. ISSN 1045-9227. doi: 10.
1109/TNN.2008.2010350. URL http://dx.doi.
org/10.1109/TNN.2008.2010350.

Steven W Muchmore, Andrew J Souers, and Irini
Akritopoulou-Zanze. The use of three-dimensional
shape and electrostatic similarity searching in the

13

Skipper Seabold and Josef Perktold. Statsmodels:
Econometric and statistical modeling with python.
In Proceedings of the 9th Python in Science Con-
ference, pages 57–61, 2010.

Sutskever,

Nitish Srivastava, Geoﬀrey Hinton, Alex Krizhevsky,
Ilya
Salakhutdinov.
Dropout: A simple way to prevent neural net-
works from overﬁtting. The Journal of Machine
Learning Research, 15(1):1929–1958, 2014.

and Ruslan

Christian Szegedy, Wei Liu, Yangqing Jia, Pierre
Sermanet, Scott Reed, Dragomir Anguelov, Du-
mitru Erhan, Vincent Vanhoucke, and Andrew Ra-
binovich. Going deeper with convolutions.
In
CVPR 2015, 2015. URL http://arxiv.org/abs/
1409.4842.

Roberto Todeschini and Viviana Consonni. Molecular
Descriptors for Chemoinformatics, Volume 41 (2
Volume Set), volume 41. John Wiley & Sons, 2009.
Thomas Unterthiner, Andreas Mayr, GÃĳnter Klam-
Toxicity pre-
arXiv preprint

bauer, and Sepp Hochreiter.
diction using deep learning.
arXiv:1503.01445, 2015.

Izhar Wallach, Michael Dzamba, and Abraham
Heifets. Atomnet: A deep convolutional neural net-
work for bioactivity prediction in structure-based
drug discovery. arXiv preprint arXiv:1510.02855,
2015.

Yanli Wang, Jewen Xiao, Tugba O Suzek, Jian
Zhang, Jiyao Wang, Zhigang Zhou, Lianyi Han,
Karen Karapetyan, Svetlana Dracheva, Ben-
jamin A Shoemaker, et al. Pubchem’s bioassay
database. Nucleic acids research, 40(D1):D400–
D412, 2012.

Lotﬁ A Zadeh. Fuzzy sets. Information and control,

8(3):338–353, 1965.

identiﬁcation of a melanin-concentrating hormone
receptor 1 antagonist. Chemical biology & drug de-
sign, 67(2):174–176, 2006.

Michael M Mysinger, Michael Carchia, John J Irwin,
and Brian K Shoichet. Directory of useful decoys,
enhanced (dud-e): better ligands and decoys for
better benchmarking. Journal of medicinal chem-
istry, 55(14):6582–6594, 2012.

Anthony Nicholls and J Andrew Grant. Molecular
shape and electrostatics in the encoding of relevant
chemical information. Journal of computer-aided
molecular design, 19(9-10):661–686, 2005.

Fabian Pedregosa, Gaël Varoquaux, Alexandre
Gramfort, Vincent Michel, Bertrand Thirion,
Olivier Grisel, Mathieu Blondel, Peter Pretten-
hofer, Ron Weiss, Vincent Dubourg, et al. Scikit-
learn: Machine learning in python. The Journal of
Machine Learning Research, 12:2825–2830, 2011.
Paula M Petrone, Benjamin Simms, Florian Nigsch,
Eugen Lounkine, Peter Kutchukian, Allen Cornett,
Zhan Deng, John W Davies, Jeremy L Jenkins, and
Meir Glick. Rethinking molecular similarity: com-
paring compounds on the basis of biological activ-
ity. ACS chemical biology, 7(8):1399–1409, 2012.

Bharath Ramsundar, Steven Kearnes, Patrick Riley,
Dale Webster, David Konerding, and Vijay Pande.
Massively multitask networks for drug discovery.
arXiv preprint arXiv:1502.02072, 2015.

David Rogers and Mathew Hahn.

Extended-
connectivity ﬁngerprints. Journal of chemical in-
formation and modeling, 50(5):742–754, 2010.

Sebastian G Rohrer and Knut Baumann. Maxi-
mum unbiased validation (muv) data sets for vir-
tual screening based on pubchem bioactivity data.
Journal of chemical information and modeling, 49
(2):169–184, 2009.

Thomas S Rush, J Andrew Grant, Lidia Mosyak,
and Anthony Nicholls. A shape-based 3-d scaﬀold
hopping method and its application to a bacterial
protein-protein interaction. Journal of medicinal
chemistry, 48(5):1489–1495, 2005.

F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuch-
ner, and G. Monfardini. The graph neural network
model. IEEE Trans Neural Netw, 20(1):61–80, Jan
2009.

14

Appendix

A Model comparison
The following ﬁgures are box plot representations of the data summarized in Table 4, organized by dataset
group. We provide (a) box plots for absolute 5-fold mean AUC scores for each model and (b) diﬀerence box
plots showing diﬀerences in 5-fold mean AUC scores against the Pyramidal (2000, 100) MTNN (PMTNN) or
Random Forest (RF) baseline model. The diﬀerence box plots are visual analogs of the sign test conﬁdence
intervals reported in Table 4. Note, however, that the conﬁdence intervals on box plot medians (calculated
√
as ±1.57×IQR/
N (McGill et al., 1978)) do not necessarily correspond to the sign test conﬁdence intervals.

(a) Full box plot.

(b) Diﬀerence box plot vs. PMTNN.

Figure A1: Model performance on PCBA datasets.

15

LRRFPMTNNSimpleSumRMSW1N2W2N1W2N2W2N3W2N4W2N∞W3N2W4N2Model0.50.60.70.80.91.05 fold mean AUCLRRFSimpleSumRMSW1N2W2N1W2N2W2N3W2N4W2N∞W3N2W4N2Model0.300.250.200.150.100.050.000.050.10∆ 5 fold mean AUC vs. PMTNN(a) Full box plot.

(b) Diﬀerence box plot vs. PMTNN.

Figure A2: Model performance on MUV datasets.

(a) Full box plot.

(b) Diﬀerence box plot vs. RF.

Figure A3: Model performance on Tox21 datasets.

16

LRRFPMTNNSimpleSumRMSW1N2W2N1W2N2W2N3W2N4W2N∞W3N2W4N2Model0.50.60.70.80.91.05 fold mean AUCLRRFSimpleSumRMSW1N2W2N1W2N2W2N3W2N4W2N∞W3N2W4N2Model0.40.30.20.10.00.10.2∆ 5 fold mean AUC vs. PMTNNLRRFPMTNNSimpleSumRMSW1N2W2N1W2N2W2N3W2N4W2N∞W3N2W4N2Model0.700.750.800.850.900.955 fold mean AUCLRPMTNNSimpleSumRMSW1N2W2N1W2N2W2N3W2N4W2N∞W3N2W4N2Model0.100.080.060.040.020.000.020.040.06∆ 5 fold mean AUC vs. RFB Input featurization
For each of the experiments described in Section 4.2, we provide ﬁgures showing (a) box plots for absolute
5-fold mean AUC scores for each model and (b) diﬀerence box plots showing diﬀerences in 5-fold mean AUC
scores against a baseline model (without any y-axis cropping).

(a) Full box plot.

(b) Diﬀerence box plot vs. “simple” featurization.

Figure A4: Comparison of models with “simple” and “full” input featurizations.

17

pcbamuvtoxDataset group0.50.60.70.80.91.05 fold mean AUCmodelSimpleFullpcbamuvtoxDataset group0.060.040.020.000.020.040.06∆ 5 fold mean AUC vs. SimplemodelFullC Hyperparameter sensitivity
For each of the experiments described in Section 4.3, we provide ﬁgures showing (a) box plots for absolute
5-fold mean AUC scores for each model and (b) diﬀerence box plots showing diﬀerences in 5-fold mean AUC
scores against a baseline model (without any y-axis cropping).

C.1 Number of Weave modules

(a) Full box plot.
Figure A5: Comparison of models with diﬀerent numbers of Weave modules.

(b) Diﬀerence box plot vs. W1 model.

18

pcbamuvtoxDataset group0.50.60.70.80.91.05 fold mean AUCmodelW1W2W3W4pcbamuvtoxDataset group0.080.060.040.020.000.020.040.060.080.10∆ 5 fold mean AUC vs. W1modelW2W3W4C.2 Alternative feature reductions

(a) Full box plot.
Figure A6: Comparison of models with diﬀerent feature reduction methods.

(b) Diﬀerence box plot vs. sum reduction.

C.3 Distance-dependent pair features

(a) Full box plot.

(b) Diﬀerence box plot vs. N1 model.

Figure A7: Comparison of models with diﬀerent maximum atom pair distances.

19

pcbamuvtoxDataset group0.50.60.70.80.91.05 fold mean AUCmodelsumRMSGaussianpcbamuvtoxDataset group0.080.060.040.020.000.020.040.060.080.10∆ 5 fold mean AUC vs. summodelRMSGaussianpcbamuvtoxDataset group0.50.60.70.80.91.05 fold mean AUCmodelN1N2N3N4N∞pcbamuvtoxDataset group0.200.150.100.050.000.050.10∆ 5 fold mean AUC vs. N1modelN2N3N4N∞D Atom pair feature evolution
Figure 8 showed the evolution of atom features at diﬀerent stages of a graph convolution model (after
subsequent Weave modules). The following ﬁgures show the evolution of atom pair features from the same
models, using both the “full” and “simple” input featurization. As in Figure 8, the initial pair features
describe ibuprofen. Most of the initial featurization describes the graph distance between the atoms in the
pair (see Table 3). There are many blank rows since pairs separated by more than the maximum atom pair
distance are masked. Note that only unique pairs are represented (i.e., (a, b) but not (b, a)). As the pair
features move through the graph convolution network, it can be seen that identical initial featurizations
diverge due to mixing with atom features in Weave modules.

Figure A8: Graph convolution atom pair feature evolution using the “full” featurization in a W3N2 architecture.
Unique atom pairs are on the y-axis (one atom pair per row).
Initial pair features are shown on the left, with
whitespace separating subsequent Weave module outputs.

20

Figure A9: Graph convolution atom pair feature evolution using the “simple” featurization in a W2N2 architecture.
Unique atom pairs are on the y-axis (one atom pair per row).
Initial pair features are shown on the left, with
whitespace separating subsequent Weave module outputs.

21

E Gaussian histogram membership functions

Table A1: Gaussian membership functions.

Mean Variance
.080
-1.645
.029
-1.080
-.739
.018
.014
-.468
.013
-.228
.013
.000
.228
.013
.014
.468
.018
.739
.029
1.080
1.645
.080

Figure A10: Visualization of the Gaussian membership functions.

References
Robert McGill, John W Tukey, and Wayne A Larsen. Variations of box plots. The American Statistician,

32(1):12–16, 1978.

22

