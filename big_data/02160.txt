Bayesian Learning of Kernel Embeddings

Seth Flaxman

ﬂaxman@stats.ox.ac.uk
Department of Statistics

University of Oxford

Dino Sejdinovic

dino.sejdinovic@stats.ox.ac.uk

Department of Statistics

University of Oxford

John P. Cunningham
jpc2181@columbia.edu
Department of Statistics

Columbia University

Sarah Filippi

ﬁlippi@stats.ox.ac.uk
Department of Statistics

University of Oxford

6
1
0
2

 
r
a

M
7

 

 
 
]
L
M

.
t
a
t
s
[
 
 

1
v
0
6
1
2
0

.

3
0
6
1
:
v
i
X
r
a

Abstract

Kernel methods are one of the mainstays of machine learning, but the problem of kernel learning remains chal-
lenging, with only a few heuristics and very little theory. This is of particular importance in methods based on
estimation of kernel mean embeddings of probability measures. For characteristic kernels, which include most
commonly used ones, the kernel mean embedding uniquely determines its probability measure, so it can be used
to design a powerful statistical testing framework, which includes nonparametric two-sample and independence
tests. In practice, however, the performance of these tests can be very sensitive to the choice of kernel and its
lengthscale parameters. To address this central issue, we propose a new probabilistic model for kernel mean
embeddings, the Bayesian Kernel Embedding model, combining a Gaussian process prior over the Reproduc-
ing Kernel Hilbert Space containing the mean embedding with a conjugate likelihood function, thus yielding a
closed form posterior over the mean embedding. The posterior mean of our model is closely related to recently
proposed shrinkage estimators for kernel mean embeddings, while the posterior uncertainty is a new, interesting
feature with various possible applications. Critically for the purposes of effective and principled kernel learning,
our model gives a simple, closed form marginal likelihood of the observed data given the kernel hyperparam-
eters. This marginal likelihood can either be optimized to inform the hyperparameter choice or fully Bayesian
inference can be used.

1

INTRODUCTION

A large class of popular and successful machine learning methods rely on kernels (positive semideﬁnite functions), includ-
ing support vector machines, kernel ridge regression, kernel PCA (Sch¨olkopf and Smola, 2002), Gaussian proceses (Ras-
mussen and Williams, 2006), and a more recent development on kernel-based hypothesis testing (Gretton et al., 2005, 2008,
2012a). A key component for many of these methods is that of estimating kernel mean embeddings and covariance opera-
tors of probability measures based on data. The use of simple empirical estimators has been challenged recently (Muandet
et al., 2016) and alternative, better-behaved frequentist shrinkage strategies have been proposed. In this article, we de-
velop a Bayesian framework for estimation of kernel mean embeddings, recovering desirable shrinkage properties as well
as allowing quantiﬁcation of full posterior uncertainty. Moreover, the developed framework has an additional extremely
useful feature. Namely, a persistent problem in kernel methods is that of kernel choice and hyperparameter selection, for
which no general-purpose strategy exists. When a large dataset is available in a supervised setting, the standard approach
is to use cross-validation. However, in unsupervised learning and kernel-based hypothesis testing, cross-validation is not
straightforward to apply and yet the choice of kernel is critically important. Our framework gives a tractable closed-form
marginal likelihood of the data allowing direct hyperparameter optimization as well as fully Bayesian posterior inference
with integrating over kernel hyperparameters. We emphasise that this approach is fully unsupervised: it is based solely on
the modelling of kernel mean embeddings – going beyond marginal likelihood based approaches in, e.g., Gaussian process
regression – and is thus broadly applicable in situations, such as kernel-based hypothesis testing, where the hyperparameter
choice has thus far been mainly driven by heuristics.

In Section 2 we provide the necessary background on Reproducing Kernel Hilbert Spaces (RKHS) as well as describe
some related works.
In Section 3 we develop our Bayesian Kernel Embedding model, showing a rigorous Gaussian
process prior formulation for an RKHS. In Section 4 we show how to perform kernel learning and posterior inference with
our model. In Section 5 we empirically evaluate our model, arguing that our Bayesian Kernel Learning (BKL) objective
should be considered as a “drop-in” replacement for heuristic methods of choosing kernel hyperparameters currently in

use, especially in unsupervised settings such as kernel-based testing. We close in Section 6 with a discussion of various
applications of our approach and future work.

2 BACKGROUND AND RELATED WORK

2.1 KERNEL EMBEDDINGS OF PROBABILITY MEASURES
For any positive deﬁnite kernel function k : X × X → R, there exists a unique reproducing kernel Hilbert space (RKHS)
Hk. RKHS is an (often inﬁnite-dimensional) space of functions h : X → R where evaluation can be written as an inner
product, and in particular h(x) = (cid:104)h, k(·, x)(cid:105)Hk for all h ∈ Hk, x ∈ X . Given a probability measure P on X , its kernel
embedding into Hk is deﬁned as:

(cid:90)

k (·, x) P(dx).

µP =

(1)
Embedding µP is an element of Hk and serves as a representation of P akin to a characteristic function. It represents

expectations of RKHS functions in the form of an inner product (cid:82) h(x)P(dx) = (cid:104)h, µP(cid:105)Hk. For a broad family of

kernels termed characteristic (Sriperumbudur et al., 2011), every probability measure has a unique embedding – thus, such
embeddings completely determine their probability measures and capture all of the moment information. This yields a
framework for constructing nonparametric hypothesis tests for the two-sample problem and for independence, which are
consistent against all alternatives (Gretton et al., 2008, 2012a) – we review this framework in the next section.

2.2 KERNEL MEAN EMBEDDING AND HYPOTHESIS TESTING

Given a kernel k and probability measures P and Q, the maximum mean discrepancy (MMD) between P and Q (Gretton
et al., 2012a) is deﬁned as the squared RKHS distance (cid:107)µP − µQ(cid:107)2Hk
between their embeddings. A related quantity is the
Hilbert Schmidt Independence Criterion (HSIC) (Gretton et al., 2005, 2008), a nonparametric dependence measure between
random variables X and Y on domains X and Y respectively, deﬁned as the squared RKHS distance (cid:107)µPXY − µPX PY (cid:107)2Hκ
between the embeddings of the joint distribution PXY and of the product of the marginals PX PY with respect to a kernel
κ : (X × Y) × (X × Y) → R on the product space. Typically, κ factorises, i.e. κ ((x, y), (x(cid:48), y(cid:48))) = k(x, x(cid:48))l(y, y(cid:48)). The
empirical versions of MMD and HSIC are used as test statistics for the two-sample (H0 : P = Q vs. H1 : P (cid:54)= Q) and
independence (H0 : X ⊥⊥ Y vs. H1 : X (cid:54) ⊥⊥ Y ) tests, respectively. With the help of the approximations to the asymptotic
distribution under the null hypothesis, corresponding p-values can be computed (Gretton et al., 2012a). In addition, the
so-called “witness function” which is proportional to µP − µQ can be used to assess where the difference between the
distributions arises.

2.3 KERNEL MEAN EMBEDDING ESTIMATORS

For a set of i.i.d. samples x1, . . . , xn, the kernel mean embedding is typically estimated by its empirical version

(cid:99)µP = µ(cid:98)P =

n(cid:88)

i=1

1
n

k(·, xi),

(2)

from which various associated quantities, including the estimators of the squared RKHS distances between embeddings
needed for kernel-based hypothesis tests, follow. As an empirical mean in an inﬁnite-dimensional space, (2) is affected by
Stein’s phenomenon, as overviewed by Muandet et al. (2013) who also propose alternative shrinkage estimators similar
to the well known James-Stein estimator. Improvements of test power using such shrinkage estimators are reported by
Ramdas and Wehbe (2015). Connections between the James-Stein estimator and empirical Bayes procedures are classical
(Efron and Morris, 1973), and thus a natural question to consider is whether a Bayesian formulation of the problem of
kernel embedding estimation would yield similar shrinkage properties. In this paper, we will give a Bayesian perspective
of the problem of kernel embedding estimation. In particular, we will construct a ﬂexible model for underlying probability
measures based on Gaussian measures in RKHSs which allows derivation of a full posterior distribution of µP, recovering
similar shrinkage properties to Muandet et al. (2013), as discussed in Section 4.2. The model will give us a further
advantage, however – as the marginal likelihood of the data given the kernel parameter can be derived leading to an
informed choice of kernel parameters.

2.4 SELECTION OF KERNEL PARAMETERS

In supervised kernel methods like support vector machines, leave-one-out or k-fold crossvalidation is an effective and
widely used method for kernel selection, and the myriad papers on multiple kernel learning (e.g. Bach et al. (2004);
Sonnenburg et al. (2006); G¨onen and Alpaydın (2011)) assume that some loss function is available and thus focus on
effective ways of learning combinations of kernels.
In the parallel universe of an overlapping but distinct concept of
smoothing kernels and kernel density estimation, there are a variety of long-standing approaches to bandwidth selection,
again based on a loss function (in this case, mean integrated squared error is a popular choice (Bowman, 1985), and
there is even a formula giving the optimal smoothing parameter asymptotically, see Rosenblatt (1956); Parzen (1962))
but we are not aware of work linking this literature to methods based on positive deﬁnite/RKHS kernels we study here.
Separately, Gaussian process learning can be undertaken by maximizing the marginal likelihood, which has a convenient
closed form. This is noteworthy for its success and general applicability even for learning complicated combinations of
kernels (Duvenaud et al., 2013) or rich kernel families (Wilson and Adams, 2013). Our approach has the same basic design
as that of Gaussian process learning, yet it is applicable to learning kernel embeddings, which falls outside the realm of
supervised learning.
As noted in Gretton et al. (2012b), the choice of the kernel k is critically important for the power of the tests presented
in Section 2.2. However, no general, theoretically-grounded approaches for kernel selection in this context exist. The
difﬁculty is that, unlike in supervised kernel methods, a simple cross-validation approach for the kernel parameter selec-
tion is not possible. What would be an ideal objective function – asymptotic test power – cannot be computed due to a
complicated asymptotic null distribution. Moreover, even if we were able to estimate the power by performing tests on
“training data” for each of the individual candidate kernels, in order to account for multiple comparisons, this training data
would have to be disjoint from the one on which the hypothesis test is performed, which is clearly wasteful of power and
appropriate only in the type of large-scale settings discussed in Gretton et al. (2012b). For these reasons, most users of
kernel hypothesis tests in practice resort to using a parameterized kernel family such as squared exponential, and setting
the lengthscale parameter based on the “median heuristic.”
The exact origins of the median heuristic are unclear (interestingly, it does not appear in the book that is most commonly
cited as its source, Sch¨olkopf and Smola (2002)) but it may have been derived from Takeuchi et al. (2006) and has pre-
cursors in classical work on bandwidth selection for kernel density estimation (Bowman, 1985). Note that there are two
versions of the median heuristic in the literature: in both versions, given a set of observations x1, . . . , xn we calculate
(cid:96) = median((cid:107)xi − xj(cid:107)2) and then one version (e.g. Mooij et al. (2015)) uses the Gaussian RBF / squared exponential
kernel parameterized as k(x, x(cid:48)) = exp(−(cid:107)x−x(cid:48)(cid:107)2
) and the second version (e.g. Muandet et al. (2014)) uses the param-
eterization k(x, x(cid:48)) = exp(−(cid:107)x−x(cid:48)(cid:107)2
). Some recent work has highlighted the situations in which the median heuristic
can lead to poor performance (Gretton et al., 2012b). Cases in which the median heuristic performs quite well and also
cases in which it performs quite poorly are discussed in (Reddi et al., 2015; Ramdas et al., 2015). We note that the median
heuristic has also been used as a default value for supervised learning tasks (e.g. for the SVM implementation in R package
kernlab) or when cross-validation is simply too expensive.
Outside of kernel methods, the same basic conundrum arises in spectral clustering in the choice of the parameters for
the similarity graph (Von Luxburg, 2007, Section 8.1) and it is implicitly an issue in any unsupervised statistical method
based on distances or dissimilarities, like the distance covariance (which is in fact equivalent to HSIC with a certain family
of kernel functions (Sejdinovic et al., 2013)), or even the choice of the number of neighbors k in k-nearest neighbors
algorithms.

2(cid:96)2

(cid:96)2

i=1 ∼ P of observations in

3 OUR MODEL: BAYESIAN KERNEL EMBEDDING
Below, we will work with a parametric family of kernels {kθ(·,·)}θ∈Θ. Given a dataset {xi}n

RD for an unknown probability distribution P, we wish to infer the kernel embedding µP,θ =(cid:82) kθ (·, x) P(dx) for a given
writing µθ to emphasize the dependence on θ. Similarly, we will use(cid:99)µθ to denote the simple empirical estimator from
a likelihood function linking it to the observations through the empirical estimator(cid:99)µθ. This will then allow us to infer

kernel kθ in the parametric family. Moreover, we wish to construct a model that will lead to the principled inference of the
kernel hyperparameter θ as well. Note that the two goals are related, since θ determines the space in which the embedding
µP,θ lies. When it is obvious from context, we suppress the dependence of the embeddings on the underlying measure P,
Eq. (2), which depends on a ﬁxed sample {xi}n
Our Bayesian Kernel Embedding (BKE) approach consists in specifying a prior on the kernel mean embedding µθ and

i=1.

the posterior distribution of the kernel mean embedding. The hyperparameter θ can itself have a prior and the posterior
distribution over the hyperparameter space can then be derived.

3.1 PRIOR

A given hyperparameter θ (which can itself have a prior distribution), parameterizes a kernel kθ and a corresponding RKHS
Hkθ. While it is tempting to deﬁne a GP(0, kθ(·,·)) prior on µθ, this is problematic since draws from such prior would
almost surely fall outside Hk (Wahba, 1990). Therefore, we deﬁne a GP prior over µθ as follows:

µθ | θ ∼ GP(0, rθ(·,·)) ,

(cid:90)

(4)
where ν is any ﬁnite measure on X . This choice of rθ ensures that µθ ∈ Hkθ with probability 1 by the nuclear domi-
nance Luki´c and Beder (2001); Pillai et al. (2007) of kθ over rθ for any stationary kernel kθ and more broadly whenever

(cid:82) kθ(x, x)ν(dx) < ∞. For completeness, we provide details of this construction in the Appendix in Section A.2. Since

kθ(x, u)kθ(u, y)ν(du) .

rθ(x, y) :=

Eq. (4) is the convolution of a kernel with itself with respect to ν, for typical kernels kθ, the resulting kernel rθ can be
thought of as a smoother version of kθ. A particularly convenient choice for X = RD is to take ν to be proportional to
a Gaussian measure in which case rθ can be computed analytically for a squared exponential kernel kθ. The derivation is
given in the Appendix in Section A.3. When letting ν to be proportional to an isotropic Gaussian measure with a large
variance parameter, rθ becomes very similar to another squared exponential with lengthscale θ

√

2.

(3)

(6)

via the empirical mean embedding estimator of Eq. (2),(cid:99)µθ which depends on {xi}n

3.2 LIKELIHOOD
Second, we need a likelihood linking the kernel mean embedding µθ to the observations {xi}n
i=1. We deﬁne the likelihood
some x ∈ RD (which need not be one of our observations). The result is a real number giving an empirical estimate of
µθ(x) based on {xi}n
a Gaussian distribution with variance σ2:

i=1 and θ. Consider evaluating(cid:99)µθ at
i=1 and θ. We link the empirical estimate,(cid:99)µθ(x), to the corresponding modeled estimate, µθ(x) using
Our motivation for choosing this likelihood comes from the Central Limit Theorem. For a ﬁxed location x, (cid:99)µθ(x) =
(cid:80)n

p((cid:99)µθ(x)|µθ(x)) = N ((cid:99)µθ(x); µθ(x), σ2),

i=1 kθ(xi, x) is an average of i.i.d. random variables so it satisﬁes:

x ∈ X .

(5)

1
n

√

n((cid:99)µθ(x) − µθ(x)) D→ N (0, VarX∼P[kθ(X, x)])

We note that considering a heteroscedastic variance dependent on x in (5) would be a straightforward extension to our
model, but we do not pursue this idea further here.

JUSTIFICATION FOR THE MODEL

3.3
There are various ways to understand the construction of our hierarchical model. {xi}n
i=1 are drawn iid from P, which we
do not have access to. We could estimate P directly (e.g. with a Gaussian mixture model) obtaining ˆP, and then estimate
µθ,ˆP. But since density estimation is challenging in high dimensions, we posit a generative model for µθ directly.
Beginning at the top of the hierarchy, we have a ﬁxed or random hyperparameter θ, which immediately deﬁnes kθ and the
corresponding RKHS Hkθ. Then, we introduce a GP prior over µθ to ensure that µθ ∈ Hkθ. A few realizations of µθ
drawn from our prior are shown in Figure 1 (A), for an illustrative one-dimensional example where the prior is a Gaussian
process with squared exponential kernel with lengthscale θ = 0.25. Small values of θ yield rough functions and large
values of θ yield smooth functions. Next, we need to deﬁne the likelihood, which links these draws from the prior to
i=1 ∈ X we need to
the observations {xi}n
mean embedding(cid:99)µθ as our link function. Given a few observations,(cid:99)µθ is shown in Figure 1 (B). Our likelihood links(cid:99)µθ
transform the observations so that we can put a probability distribution over them. We use the empirical estimate of the
to µθ at the observation locations {xi}n
i=1 by assuming a squared loss function, i.e. Gaussian errors. As mentioned above,
the motivation is the Central Limit Theorem, but also the convenient conjugate form that a Gaussian process with Gaussian
likelihood yields. A plot of the posterior over the mean embedding is shown in Figure 1 (C). A few points are worth noting:

i=1. Since µθ is an inﬁnite dimensional element in a Hilbert space and {xi}n

Figure 1: An illustration of the Bayesian Kernel Embedding model, where kθ is a squared exponential kernel with length-

scale 0.1. Three draws of µθ from the prior are shown in (A). The empirical mean estimator(cid:99)µθ, which is the link function

for the likelihood, is shown in (B) with the observations shown as a rug plot. In (C), the posterior mean embedding (black
line) with uncertainty intervals (gray lines) is shown, as is the true mean embedding (blue line) based on the true data
generating process (a mixture of Gaussians) and the same kθ.

since the empirical estimator is already quite smooth (notice its similarity to a kernel density estimate), the posterior mean
embedding is only slightly smoother than the empirical mean embedding. Notice that unlike kernel density estimation,
there is no requirement that the kernel mean embedding be non-negative, thus explaining the posterior uncertainty intervals
which are below zero.

Our original motivation for considering a Bayesian model for kernel mean embeddings was to see whether there was a
coherent Bayesian formulation that corresponded to the shrinkage estimators in Muandet et al. (2013), while also enabling
us to learn the hyperparameters. The main difﬁculty was in deﬁning a valid prior over the RKHS and in deﬁning a useful
likelihood function. In our ﬁrst attempt, we considered the same prior as above, but with a likelihood function deﬁned such
that each observation xi was projected into feature space as φi := kθ(xi,·) and then a Gaussian measure, with mean µθ
was placed on φi. Theoretically, this model makes sense, but in practice we found that large values of θ would effectively
lead to all observations being mapped to the same point in the RKHS, thus allowing for a very parsimonious representation
of µθ as a constant vector, so the model favored very large values of θ. We did not ﬁnd this to be an issue with our model.
As discussed below, calculating the likelihood of our observations requires including a Jacobian correction for the link
function, and this Jacobian penalizes large θ effectively.

−3−2−10123−2012(A) Draws from the priorxm(x)^−3−2−10123−0.3−0.10.10.3(B) Empirical meanxm(x)^−3−2−10123−0.50.00.5(C) Posteriorxp(m(x))4 BAYESIAN KERNEL LEARNING

In this section we show how to perform learning and inference in the Bayesian Kernel Embedding model introduced in
the previous section. Our model inherits various attractive properties from the Gaussian process framework (Rasmussen
and Williams, 2006). First, we derive the posterior and posterior predictive distributions for the kernel mean embedding in
closed form due to the conjugacy of our model, and show the relationship with previously proposed shrinkage estimators.
We then derive the tractable marginal likelihood of the observations given the hyperparameters allowing for efﬁcient MAP
estimation or posterior inference for hyperparameters.

4.1 POSTERIOR AND POSTERIOR PREDICTIVE DISTRIBUTIONS

Similarly to GP models, the posterior mean of µθ is available in closed form due to the conjugacy of Gaussians. Perhaps
given our data we wish to infer µθ at a new location x∗ ∈ RD. Given a value of the hyperparameter θ we can calculate the

posterior distribution of µθ as well as the posterior predictive distribution p(µθ(x∗)|(cid:99)µθ, θ).
[µθ(x1), . . . , µθ(xn)](cid:62) | [(cid:99)µθ(x1), . . . ,(cid:99)µθ(xn)](cid:62), θ
∼ N (Rθ(Rθ + σ2In)−1[(cid:99)µθ(x1), . . . ,(cid:99)µθ(xn)](cid:62),

Standard GP results (Rasmussen and Williams, 2006) yield the posterior distribution as:

(7)
where Rθ is the n × n matrix such that its (i, j)-th element is rθ(xi, xj). The posterior predictive distribution at a new
location x∗ is:

Rθ − Rθ(Rθ + σ2In)−1Rθ),

µθ(x∗)(cid:62) | [(cid:99)µθ(x1), . . . ,(cid:99)µθ(xn)](cid:62), θ

θ (Rθ + σ2In)−1[(cid:99)µθ(x1), . . . ,(cid:99)µθ(xn)](cid:62),

∼ N (R∗(cid:62)

θ (Rθ + σ2In)−1R∗
θ)

θ − R∗(cid:62)
r∗∗
θ = rθ(x∗, x∗).

(8)

θ = [rθ(x∗, x1), . . . rθ(x∗, xn)]

where R∗
As in standard GP inference, the time complexity is O(n3) due to the matrix inverses and the storage is O(n2) to store the
n × n matrix Rθ.

(cid:62) and r∗∗

4.2 RELATION TO THE SHRINKAGE ESTIMATOR

The spectral kernel mean shrinkage estimator (S-KMSE) of Muandet et al. (2013) for a ﬁxed kernel k is deﬁned as:

(cid:80)n
(9)
i=1 k(·, xi) ⊗ k(·, xi) is the empirical covariance
operator on Hk, and λ is a regularization parameter. (Muandet et al., 2013, Proposition 12) shows that ˇµλ can be expressed

i=1 k(·, xi) is the empirical embedding, ˆΣXX = 1

where ˆµ = (cid:80)n
as a weighted kernel mean ˇµλ =(cid:80)n

ˇµλ = ˆΣXX ( ˆΣXX + λI)−1 ˆµ,

i=1 βik(·, xi), where

n

Now, evaluating S-KMSE at any point x∗ gives
ˇµλ(x∗) =

β =

1
n

(K + nλI)−1K1

= (K + nλI)−1[(cid:98)µ(x1), . . . ,(cid:98)µ(xn)](cid:62).

n(cid:88)
∗ (K + nλI)−1[(cid:98)µ(x1), . . . ,(cid:98)µ(xn)](cid:62),

βik(x∗, xi)

i=1

= K(cid:62)

(cid:62). Thus, the posterior mean in Eq. (7) recovers the S-KMSE estimator (Muandet
where K∗ = [k(x∗, x1), . . . , k(x∗, xn)]
et al., 2013), where the regularization parameter corresponds to the variance in the likelihood model (5), with a difference
that in our case the kernel kθ used to compute the empirical embedding is not the same as the kernel rθ used to compute
the kernel matrices. We note that our method has various advantages over the frequentist estimator ˇµλ: we have a closed-
form uncertainty estimate, while we are not aware of a principled way of calculating the standard error of the frequentist
estimators of embeddings. Our model also leads to a coherent framework for learning the hyperparameters, which we
discuss next.

4.3

INFERENCE OF THE KERNEL PARAMETERS

In this section we focus on hyperparameter learning in our model. For the purposes of hyperparameter learning, we
want to integrate out the kernel mean embedding µθ and consider the probability of our observations {xi}n
i=1 given the
hyperparameters θ. We refer to the quantity we wish to compute as the marginal likelihood because we marginalize out
µθ, i.e. we wish to compute:

p(x|θ) =

p(x|µθ, θ)p(µθ|θ)dµθ

for any x ∈ RD. Our likelihood was given in Eq. (5) as a Gaussian distribution with the empirical mean embedding(cid:99)µθ as
a link function. Thus we consider the transformation(cid:99)µθ : RD → R and denote by x(i) the i-th element of the vector x
such that x = (x(1), . . . x(D))T ∈ RD. The Jacobian matrix of(cid:99)µθ evaluated in x is:
(cid:19)(cid:62)

(cid:90)

∂(cid:99)µθ(x)

∂x

(cid:18) ∂(cid:99)µθ(x)

∂x(1)

∂(cid:99)µθ(x)

∂x(D)

=

, . . . ,

.

(10)

There is an underappreciated generalization of the change-of-variables formula to non-square Jacobian matrices (Ben-
Israel, 1999) as stated in the following theorem.
Theorem 1. Given U ⊂ RD and V ⊂ RD(cid:48)
and a smooth function φ : U → V with compact support, then, for any
integrable function h on V:

where vol(Jφ) is the volume of the Jacobian matrix. Note that the volume of a full rank matrix is:

Applying this theorem to the function φ =(cid:99)µθ, we obtain the distribution of x conditionally on θ and µθ:

vol(Jφ) =

det(J T

φ Jφ) .

h(v)dv =

(φ ◦ h)(u) vol(Jφ(u))du

(cid:90)

V

(cid:90)

U

(cid:113)

p(x|µθ, θ) = p((cid:99)µθ(x)|µθ(x), θ)γ(x, θ)

(cid:118)(cid:117)(cid:117)(cid:116) d(cid:88)

j=1

(cid:18) ∂(cid:99)µθ(x)

(cid:19)2

∂x(j)

.

γ(xi, θ)

.

(13)

i=1

The time and space complexity for calculating the marginal likelihood in Eq. (13) is dominated by the O(n2) storage
for Rθ and O(n3) time for the necessary matrix inverse (usually done with a Cholesky decomposition) in calculating the

multivariate normal distribution. Calculating(cid:99)µθ(xi) for all i takes O(n2) total. The complexity of calculating the Jacobian

correction depends on the parametric form of kθ, as shown in Section 4.4, where for the squared exponential kernel the
time is O(Dn2).

where

γ(x, θ) =

The notation γ(x, θ) highlights the dependence on both θ and x. An explicit calculation of γ(x, θ) for squared exponential
kernels is described in Section 4.4.
Our model is effectively a Gaussian process model with a change-of-variables correction, so it is straightforward to calcu-
late the marginal likelihood of the observations, integrating out µθ. We combine the likelihood from Eq. (11) with the prior
from Eq. (3)

(11)

(12)

(cid:90)

=

p(x1, . . . , xn|θ) =

p(x1, . . . , xn|µθ, θ)p(µθ|θ)dµθ

(cid:90) (cid:32) n(cid:89)
(cid:33)
p((cid:99)µθ(xi)|µθ(xi), θ)γ(xi, θ)
= N(cid:0)[(cid:99)µθ(x1), . . . ,(cid:99)µθ(xn)](cid:62); 0, Rθ + σ2In
(cid:1)
(cid:32) n(cid:89)
(cid:33)

i=1

p(µθ|θ)dµθ

×

Just as in GP modeling, the marginal likelihood can be maximized directly for maximum likelihood II (also known as
empirical Bayes) estimation, in which we look for a single best ˆθ, or it can be used to construct an efﬁcient MCMC
sampler. For efﬁcient learning in either case, it is useful to calculate the gradients of the marginal likelihood. We derive
the gradients for the squared exponential ARD kernel in the Appendix in Section A.5.1.

4.4 EXPLICIT CALCULATIONS FOR SQUARED EXPONENTIAL (RBF) KERNEL

Consider the isotropic squared exponential kernel with lengthscale matrix θ2Id deﬁned by

kθ(x, y) = exp(−.5(x − y)(cid:62)θ−2Id(x − y)).

In this case, we can analytically calculate rθ(x, y), exact form is given in the Appendix in Section A.3.
The Jacobian from Eq. (10) can be calculated using Eq. (2):

(14)

(15)

(16)

(17)

∂(cid:99)µθ(x)

∂x(j)

=

=

i=1

1
n

n(cid:88)
n(cid:88)
(cid:118)(cid:117)(cid:117)(cid:116) D(cid:88)

(cid:32)

1
n

i=1

n(cid:88)

1
n

(cid:18)−.5(cid:107)xi − x(cid:107)2

(cid:19)

∂
∂xj exp

kθ(xi, x)

θ2
i − x(j)
x(j)

θ2

(cid:33)2

i − x(d)
x(d)

Now to calculate γ(x, θ) we have by Eq. (12):

θ2
Since we must calculate γ(x, θ) for each xi, the time complexity is O(Dn2).

d=1

i=1

γ(x, θ) =

kθ(xi, x)

5 EXPERIMENTS

We demonstrate our approach on two synthetic datasets and one example on real data, focusing on two-sample testing
with the Maximum Mean Discrepancy statistic and independence testing with HSIC. First, we use our Bayesian Kernel
Embedding model and learn the kernel hyperparameters with maximum likelihood II, optimizing the marginal likelihood.
Second, we take a fully Bayesian approach to inference and learning with our model. Finally, we apply the PC algorithm
for causal structure discovery to a real dataset. The PC algorithm relies on a series of independence tests, and we carry
these out with HSIC with the lengthscales set with Bayesian Kernel Learning.
Choosing lengthscales with the median heuristic is often a very bad idea. In the case of two sample testing, Gretton et al.
(2012b) showed that MMD with the median heuristic failed to reject the null hypothesis when comparing samples from a
grid of isotropic Gaussians to samples from a grid of non-isotropic Gaussians. We repeated this experiment by considering
a distribution P of a mixture of bivariate Gaussians centered on a grid with diagonal covariance and unit variance and a
distribution Q of a mixture of bivariate Gaussians centered at the same locations but with rotated covariance matrices with
a ratio  of largest to smallest covariance eigenvalues.
As illustrated in Figures 2(A) and (B), for small values of  both distributions are very similar whereas the distinction
between P and Q becomes more apparent as  increases. For different values of , we sample 100 observations from each
mixture component, yielding 900 obesrvations from P and 900 observations from Q and then perform a two-sample test
(H0 : P = Q vs. H1 : P (cid:54)= Q) using the MMD empirical estimate with a squared exponential kernel. The type II
error (i.e. probability that the test fails to reject the null hypothesis that P = Q at α = 0.05) is shown in Figure 2(C) for
differently skewed covariances ( from 0.5 to 15) when the median heuristic is chosen to select the kernel lengthscale or
when using the Bayesian Kernel Learning. In this example, the median heuristic picks a kernel with a long lengthscale,
since the median distance between points is long. With this long, incorrect lengthscale MMD always fails to reject at
α = 0.05 even for simple cases where  is large. When we use Bayesian Kernel Learning and optimizing the marginal
likelihood of Eq. (13) for σ2 = 0.1 (in practice we did not ﬁnd any advantage to different choices of σ2) we found the
maximum marginal likelihood at a lengthscale of 1.5. With this choice of lengthscale, MMD correctly rejects the null
hypothesis at α = 0.05 even for very hard situations when  = 2. We observe that when  is smaller than 2, the type II
error of MMD is very high for both choices of lengthscale, because the two distributions P and Q are so similar that the test

always retains the null hypothesis. In Figure 2(D) we illustrate the BKL marginal likelihood across a range of lengthscales.
Interestingly, there is a local maximum near the value found by the median heuristic. This makes sense, as the true data
generating process is a mixture model. This insight could be incorporated into the Bayesian Kernel Embedding framework
by expanding our model, as discussed below. In Figure 2(E) we used the BKE posterior to estimate the witness function
µP,θ−µQ,θ. This function is large in magnitude in the locations where the two distributions differ. For ease of visualization
we do not try to include posterior uncertainty intervals, but these are readily available from our model, and we show them
for a 1-dimensional case below.

√

Our model does not just provide a better way of choosing lengthscales. We can also use it in a fully Bayesian context to
learn the posterior distribution over the mean embedding. This can be illuminating in the case of mixture models as in
the above. Switching to one dimension, we consider a distribution P of a mixture of Gaussians centered at -3, 0, and 3
with standard deviation 1 and a distribution Q of a mixture of Laplaces with the same centers and standard deviation 1
.5). The densities are shown in Figure 3. We sampled 100 observations from each
(corresponding to a scale parameter of
mixture component and then combined the data together into a sample of size 600, following the strategy in the previous
experiment to learn a single lengthscale and kernel mean embedding for the combined dataset. We ran a Hamiltonian
Monte Carlo sampler (HMC) with NUTS (Stan source code is in the Appendix in Section B) for the Bayesian Kernel
Embedding model with a squared exponential kernel, placing a Gamma(1, 1) prior on the lengthscale θ of the kernel. We
ran 8 chains for 400 iterations, discarding 200 iterations as warmup, with the chains starting at different random initial
values. The posterior over θ shown in the traceplot in Figure 4 indicates clear multimodality, with a short lengthscale for
within each of the mixture components and a long lengthscale across components. HMC is unable to move between these
two modes, suggesting various interesting extensions of our model: we could consider a mixture model over θ or a mixture
model over µθ or we could switch to a more richly parameterized kernel such as a Spectral Mixture kernel Wilson and
Adams (2013) with two components. We sample a random witness function µP,θ − µQ,θ corresponding to each draw of
our parameters from the posterior. This gives the full posterior distribution over the witness function, integrating over the
kernel hyperparameter θ. For a particular choice of a short lengthscale θ we show the posterior in Figure 5(A) and for a

Figure 2: Two sample testing on a challenging simulated data set: comparing samples from a grid of isotropic Gaussians
(black dots) to samples from a grid of non-isotropic Gaussians (red dots) with a ratio  of largest to smallest covariance
eigenvalues. Panels (A) and (B) illustrate such samples for two values of . (C) Type II error as a function of  for signiﬁcant
level α = 0.05 following the median heuristic or the BKL approach to choose the lengthscale. (D) BKL marginal log-
likelihood across a range of lengthscales. It is maximised for a lengthscale of 1.4 whereas the median heuristic suggests a
value of 20. (E) Witness function for the difﬁcult case where  = 2 using the BKL lengthscale.

Figure 3: Two densities for two-sample testing. P is a mixture
of three Gaussians (black solid line), and Q is a mixture of
three Laplace distributions (blue dashed line), each with the
same means and standard deviations.

Figure 4: Traceplot for the θ in the full Bayesian Kernel Em-
bedding ﬁt to the data in Figure 3. It is clear that multimodal-
ity is present.

particular choice of a long lengthscale θ we show the posterior in Figure 5(B). As shown in Figure 5(C), the full posterior
is a compromise, since for short θ, µP,θ and µQ,θ differ, but for long θ they do not.

Figure 5: Witness function for a short lengthscale θ = 0.49 (A) and a long lengthscale θ = 2.9 (B) for the data in Figure
3, with 80% uncertainty intervals. (C) Posterior mean and 80% UIs for the witness function when θ is integrated out. The
result is a compromise between an identically zero witness function corresponding to the long lengthscale mode and a
non-zero witness function corresponding to the short lengthscale model.

Finally, we consider the ozone dataset analyzed in Breiman and Friedman (1985), consisting of daily measurements of
ozone concentration and eight related meteorological variables. Following the approach in Flaxman et al. (2015), we ﬁrst
pre-whiten the data to control for underlying temporal autocorrelation, then we use a combination of Gaussian process
regression followed by HSIC to test for conditional independence. Each time we run HSIC, we set the kernel hyperparam-
eters using Bayesian Kernel Learning. The graphical model that we learn is shown in Figure 6. The directed edge from the
temperature variable to ozone is encouraging, as higher temperatures favor ozone formation through a variety of chemical
processes which are not represented by variables in this dataset (Bloomer et al., 2009; Sillman, 1999). Note that this edge
was not present in the graphical model in Flaxman et al. (2015) in which the median heuristic was used.

6 DISCUSSION

We developed a framework for Bayesian learning of kernel embeddings of probability measures. It is primarily designed for
unsupervised settings, and in particular for kernel-based hypothesis testing. In these settings, one relies critically on a good
choice of kernel and our framework yields a principled method, termed Bayesian Kernel Learning, to inform this choice.
We conceive of Bayesian Kernel Learning as a principled drop-in replacement for selecting the kernel hyperparameters
in settings where cross-validation is unavailable. A fully Bayesian approach is also demonstrated, allowing integrating

−4−20240.000.15Normal vs. Laplacexdensity12340100200300400lengthscale12345678−4−2024−0.04−0.020.000.020.04(A) Posterior conditional on theta=0.49witness−4−2024−0.04−0.020.000.020.04(B) Posterior conditional on  theta=2.9witness−4−2024−0.04−0.020.000.020.04(C) Posterior integrating out thetaxwitnessover kernel hyperparameters, and e.g., obtaining the full posterior distribution over the witness function in two-sample
testing.

While our method is designed for unsupervised settings, there are various reasons it might be helpful in supervised settings
or in applied Bayesian modelling more generally. With the rise of large-scale kernel methods, it has become possible to
apply, e.g. SVMs or GPs to very large datasets. But even with efﬁcient methods, it can be very costly to run cross-validation
over a large space of hyperparameters. In practice, when, e.g. large scale approximations based on random Fourier features
(Rahimi and Recht, 2007) are used, we have not seen much attention paid to kernel learning – the features are often just
one part of a complicated pipeline, so again the median heuristic is often employed. For these reasons, we think that the
developed method for Bayesian Kernel Learning would be a judicious alternative. Moreover, it would be straightforward
to develop scalable approximate versions of Bayesian Kernel Learning itself.

Figure 6: Graphical model representing an equivalence class of DAGs for the Ozone dataset from Breiman and Friedman
(1985), learned using the PC algorithm following the approach in Flaxman et al. (2015) with HSIC to test for indepen-
dence. We used BKL to set hyperparameters of HSIC. Singly directed edges represent causal links, while bidirected edges
represent edges that the algorithm failed to orient. The causal edge from temperature to ozone accords with scientiﬁc
understanding, and was not present in the graphical model learned in Flaxman et al. (2015) which employed the median
heuristic.

OzoneTempInvHtPresVisHgtHumInvTmpWind7 Acknowledgments

SRF was supported by EPSRC (EP/K009362/1). Thanks to Krikamol Muandet, Sayan Mukherjee, Jonas Peters, Aaditya
Ramdas, and Alex Smola for helpful discussions.

References
Francis R Bach, Gert RG Lanckriet, and Michael I Jordan. Multiple kernel learning, conic duality, and the smo algorithm. In Proceedings

of the twenty-ﬁrst international conference on Machine learning, page 6. ACM, 2004.

Adi Ben-Israel. The change-of-variables formula using matrix volume. SIAM Journal on Matrix Analysis and Applications, 21(1):

300–312, 1999.

Bryan J. Bloomer, Jeffrey W. Stehr, Charles A. Piety, Ross J. Salawitch, and Russell R. Dickerson. Observed relationships of ozone air

pollution with temperature and emissions. Geophysical Research Letters, 36(9), 2009. ISSN 1944-8007. L09803.

Adrian W Bowman. A comparative study of some kernel-based nonparametric density estimators. Journal of Statistical Computation

and Simulation, 21(3-4):313–327, 1985.

Leo Breiman and Jerome H Friedman. Estimating optimal transformations for multiple regression and correlation. Journal of the

American Statistical Association, 80(391):580–598, 1985.

David Duvenaud, James Lloyd, Roger Grosse, Joshua Tenenbaum, and Ghahramani Zoubin. Structure discovery in nonparametric
regression through compositional kernel search. In Proceedings of The 30th International Conference on Machine Learning, pages
1166–1174, 2013.

Bradley Efron and Carl Morris. Stein’s estimation rule and its competitors—an empirical bayes approach. Journal of the American

Statistical Association, 68(341):117–130, 1973.

Seth R Flaxman, Daniel B Neill, and Alexander J Smola. Gaussian processes for independence tests with non-iid data in causal inference.

ACM Transactions on Intelligent Systems and Technology (TIST), 2015.

Mehmet G¨onen and Ethem Alpaydın. Multiple kernel learning algorithms. The Journal of Machine Learning Research, 12:2211–2268,

2011.

A. Gretton, K. Fukumizu, C.H. Teo, L. Song, B. Schoelkopf, and A. Smola. A kernel statistical test of independence. In J.C. Platt,
D. Koller, Y. Singer, and S. Roweis, editors, Advances in Neural Information Processing Systems 20, Cambridge, MA, 2008. MIT
Press.

Arthur Gretton, Olivier Bousquet, Alex Smola, and Bernhard Sch¨olkopf. Measuring statistical dependence with hilbert-schmidt norms.

Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard Sch¨olkopf, and Alexander Smola. A kernel two-sample test. The

In Algorithmic learning theory, pages 63–77. Springer, 2005.

Journal of Machine Learning Research, 13:723–773, 2012a.

Arthur Gretton, Dino Sejdinovic, Heiko Strathmann, Sivaraman Balakrishnan, Massimiliano Pontil, Kenji Fukumizu, and Bharath K
Sriperumbudur. Optimal kernel choice for large-scale two-sample tests. In Advances in neural information processing systems, pages
1205–1213, 2012b.

Milan N. Luki´c and Jay H. Beder. Stochastic Processes with Sample Paths in Reproducing Kernel Hilbert Spaces. Transactions of the

American Mathematical Society, 353(10):3945–3969, 2001.

Joris M Mooij, Jonas Peters, Dominik Janzing, Jakob Zscheischler, and Bernhard Sch¨olkopf. Distinguishing cause from effect using

observational data: methods and benchmarks. The Journal of Machine Learning Research, pages 1–96, 2015.

K. Muandet, B. Sriperumbudur, K. Fukumizu, A. Gretton, and B. Sch¨olkopf. Kernel Mean Shrinkage Estimators. Journal of Machine

Krikamol Muandet, Kenji Fukumizu, Bharath Sriperumbudur, Arthur Gretton, and Bernhard Sch¨olkopf. Kernel mean estimation and

Krikamol Muandet, Bharath Sriperumbudur, and Bernhard Sch¨olkopf. Kernel mean estimation via spectral ﬁltering. In Advances in

Emanuel Parzen. On estimation of a probability density function and mode. The Annals of Mathematical Statistics, 33(3):1065–1076,

Learning Research (forthcoming), 2016.

Stein’s effect. arXiv preprint arXiv:1306.0842, 2013.

Neural Information Processing Systems, pages 1–9, 2014.

1962.

Natesh S Pillai, Qiang Wu, Feng Liang, Sayan Mukherjee, and Robert L Wolpert. Characterizing the function space for bayesian kernel

models. Journal of Machine Learning Research, 8(8), 2007.

A. Rahimi and B. Recht. Random features for large-scale kernel machines. In Advances in Neural Information Processing Systems

Aaditya Ramdas and Leila Wehbe. Nonparametric independence testing for small sample sizes. 24th International Joint Conference on

(NIPS), pages 1177–1184, 2007.

Artiﬁcial Intelligence (IJCAI), 2015.

Aaditya Ramdas, Sashank Jakkam Reddi, Barnab´as P´oczos, Aarti Singh, and Larry Wasserman. On the decreasing power of kernel and
distance based nonparametric hypothesis tests in high dimensions. In Twenty-Ninth AAAI Conference on Artiﬁcial Intelligence, 2015.

Carl Edward Rasmussen and Christopher KI Williams. Gaussian processes for machine learning. MIT Press, Cambridge, MA, 2006.
Sashank J Reddi, Aaditya Ramdas, Barnab´as P´oczos, Aarti Singh, and Larry A Wasserman. On the high dimensional power of a

linear-time two sample test under mean-shift alternatives. In AISTATS, 2015.

Murray Rosenblatt. Remarks on some nonparametric estimates of a density function. The Annals of Mathematical Statistics, 27(3):

Bernhard Sch¨olkopf and Alexander J Smola. Learning with kernels: support vector machines, regularization, optimization and beyond.

832–837, 1956.

the MIT Press, 2002.

Dino Sejdinovic, Bharath Sriperumbudur, Arthur Gretton, and Kenji Fukumizu. Equivalence of distance-based and rkhs-based statistics

in hypothesis testing. The Annals of Statistics, 41(5):2263–2291, 2013.

Sanford Sillman. The relation between ozone, no x and hydrocarbons in urban and polluted rural environments. Atmospheric Environ-

ment, 33(12):1821–1845, 1999.

R. Silverman. Locally stationary random processes. IRE Transactions on Information Theory, 3(3):182–187, September 1957.
S¨oren Sonnenburg, Gunnar R¨atsch, Christin Sch¨afer, and Bernhard Sch¨olkopf. Large scale multiple kernel learning. The Journal of

B. Sriperumbudur, K. Fukumizu, and G. Lanckriet. Universality, characteristic kernels and RKHS embedding of measures. Journal of

Machine Learning Research, 7:1531–1565, 2006.

Machine Learning Research, 12:2389–2410, 2011.

Ingo Steinwart and Andreas Christmann. Support Vector Machines. Springer, 2008.
Ichiro Takeuchi, Quoc V Le, Timothy D Sears, and Alexander J Smola. Nonparametric quantile estimation. The Journal of Machine

Learning Research, 7:1231–1264, 2006.

Ulrike Von Luxburg. A tutorial on spectral clustering. Statistics and computing, 17(4):395–416, 2007.
Grace Wahba. Spline Models for Observational Data. Society for Industrial and Applied Mathematics, 1990.
Andrew G Wilson and Ryan P Adams. Gaussian process kernels for pattern discovery and extrapolation. In Proceedings of the 30th

International Conference on Machine Learning (ICML-13), pages 1067–1075, 2013.

A Some derivations for Bayesian Kernel Embedding

A.1 Notation
Consider a dataset x1, . . . , xn ∈ RD and suppose that there exists some unknown probability distribution P for which the
xi are i.i.d.:

Denote by µθ the RKHS mean embedding element for a given kernel kθ(·,·) with hyperparameter θ ∈ RQ and by(cid:99)µθ(·)

xi ∼ P .

(18)

the empirical mean embedding

We posit as our model that µθ has a GP prior with covariance rθ, where

n(cid:88)

i=1

1
n

kθ(xi,·) .

(cid:99)µθ(·) :=
(cid:90)

rθ(x, y) =

kθ(x, u)kθ(u, y)ν(du) ,

(19)

(20)

(21)

where ν is a ﬁnite measure on RD thus ensuring that µθ ∈ Hkθ when drawn from the prior

µθ|θ ∼ GP(0, rθ(·,·)) .

In addition, we model the link between the population mean embedding and the empirical mean embedding functions at a
given location x as follows

p((cid:99)µθ(x)|µθ(x)) = N ((cid:99)µθ(x); µθ(x), σ2)

where σ2 is another hyperparameter.

A.2 Priors over RKHS

The results in this section have appeared in the literature before, but as they are not well known or collected in one place,
we have included them for completeness. A similar discussion appears in Pillai et al. (2007), but without the construction
of explicit GP priors over the RKHSs which we provide below.
It is well known that the sample paths of a GP with kernel k are almost surely outside RKHS Hk, e.g. (Wahba, 1990).
While this issue is often sidelined in the literature, cf. e.g. (Rasmussen and Williams, 2006, Section 6.1), since the object
of our interest, kernel embedding, is by construction an element of Hk - we opt for an approach where the prior is indeed
speciﬁed over the correct space. Fortunately, it is straightforward to construct a kernel r such that the realizations from a
GP with kernel r are almost surely inside RKHS Hk. For this, we will need notions of dominance and nuclear dominance
for kernel functions.
Deﬁnition 1. Kernel k is said to dominate kernel r (written k (cid:31) r) if Hr ⊆ Hk.
Luki´c and Beder (2001, Theorem 1.1) characterise dominance k (cid:31) r via the existence of a certain positive, continuous and
self-adjoint operator L : Hk → Hk for which

r(x, x(cid:48)) = (cid:104)L[k(·, x)], k(·, x(cid:48))(cid:105)Hk ,

∀x, x(cid:48) ∈ X .

(22)

(cid:90)

When L is also a trace class operator, dominance is termed nuclear, and denoted k (cid:31)(cid:31) r. The following theorem from
Luki´c and Beder (2001, Theorem 7.2) then fully characterises kernels that lead to valid GP priors over RKHS Hk.
Theorem 2. Let Hk be separable and let m ∈ Hk. Then GP(0, r(·,·)) has trajectories in Hk with probability 1 if and
only if k (cid:31)(cid:31) r.
Thus, we just need to specify a trace-class, positive, continuous and self-adjoint operator L : Hk → Hk and compute
(cid:104)L[k(·, x)], k(·, x(cid:48))(cid:105)Hk. A convenient choice for a given bounded continuous kernel k can be deﬁned as follows. Take the
convolution operator Sk : L2(X ; ν) → Hk with respect to a ﬁnite measure ν, deﬁned as

[Skf ](x) =

f (u)k(x, u)ν(du).

(23)

It is well known that the adjoint of Sk is the inclusion of Hk into L2 (Steinwart and Christmann, 2008, Section 4.3). Thus,
we let L = SkS∗
operator, L is then positive, continuous and self-adjoint. It is also trace-class in most cases of interest – and in particular

k, which is the (uncentred) covariance operator L = (cid:82) k(·, u) ⊗ k(·, u)ν(du) of ν. As a covariance
whenever(cid:82) k(u, u)ν(du) < ∞ (Steinwart and Christmann, 2008, Theorem 4.27), and thus for every stationary kernel

provided that ν is a ﬁnite measure. This leads to

r(x, x(cid:48)) = (cid:104)SkS∗

k[k(·, x)], k(·, x(cid:48))(cid:105)Hk

= (cid:104)S∗

(cid:90)

=

kk(·, x(cid:48))(cid:105)L2(X ;ν)

k[k(·, x)], S∗
k(x, u)k(u, x(cid:48))ν(du),

so r can be simply computed as a convolution of k with itself, and we can use GP(0, r(·,·)) as a prior over Hk.

A.3 Covariance function rθ

In this subsection, we derive the covariance function rθ for squared exponential kernels. Consider a squared exponential
kernel on X = RD with full covariance matrix Σθ deﬁned by

(cid:18)

(cid:19)

kθ(x, y) = exp

− 1
2

(x − y)T Σ−1

θ (x − y)

x, y ∈ RD.

,

(24)

While we have required in A.2 that ν is a ﬁnite measure for the covariance operator to be trace class when working with
stationary kernels, let us for simplicity ﬁrst consider the instructive case when ν is the Lebesgue measure. Then, we
have

rθ(x, y) =

=

Note that

(x − u)T Σ−1

θ (x − u) + (y − u)T Σ−1

Then

(cid:18)
(cid:18)

− 1
2
− 1
2

rθ(x, y) = exp

= exp

(cid:90)
(cid:90)

kθ(x, u)kθ(u, y)du

(cid:18)

exp

− 1
2

(cid:0)(x − u)T Σ−1
(cid:18)

θ (y − u) = 2
(cid:19)(cid:90)
(cid:19)

(cid:18)

− 1
2

(x − y)T (2Σθ)−1(x − y)

(x − y)T (2Σθ)−1(x − y)

× (2π)D/2|Σθ/2|1/2

(cid:19)

= πD/2 |Σθ|1/2 exp

(x − y)T (2Σθ)−1(x − y)

.

θ (y − u)(cid:1)(cid:19)
(cid:19)

θ (x − u) + (y − u)T Σ−1

du

(cid:18)

(cid:19)T

Σ−1

θ

u − x + y

2

u − x + y

2

+

1
2

(x − y)T Σ−1

θ (x − y) .

(cid:18)

(cid:32)

− 1
2

exp

(cid:19)T(cid:18) 1

2

Σθ

u − x + y

2

(cid:19)−1(cid:18)

u − x + y

2

(cid:19)(cid:33)

du

Thus rθ is proportional to another squared exponential kernel with covariance 2Σθ. For the special case where the covari-
ance matrix Σθ is diagonal – let Σθ = θID and θ = (θ(1), . . . , θ(D))T – we have

rθ(x, y) = πD/2

θ(d)

exp

(x − y)T (2θID)−1(x − y)

.

(25)

(cid:32) D(cid:89)

d=1

(cid:33)1/2

(cid:18)

− 1
2

(cid:19)

Now, take ν(du) = exp
case, we have

du, i.e., ν is a ﬁnite measure and is proportional to a Gaussian measure on Rd. In that

2
2γ2

(cid:16)−(cid:107)u(cid:107)2
(cid:90)
(cid:90)

=

exp

(cid:17)
− 1

2

rθ(x, y) =

kθ(x, u)kθ(u, y)ν(du)

(cid:0)(x − u)T Σ−1
(cid:124)

θ (x − u) + (y − u)T Σ−1

From standard Gaussian integration rules, it follows that

A =

1
2

(x − y)T Σ−1

θ (x − y) + (u − m)(cid:62)S−1(u − m) +
θ + γ−2ID)−1. Therefore

(x + y) and S = (2Σ−1

where m = S−1Σ−1

θ

(cid:32)

(x − y)T (2Σθ)−1(x − y) − 1
2

rθ(x, y) = (2π)D/2|S|1/2 exp

= (2π)D/2(cid:12)(cid:12)2Σ−1

− 1
2
θ + γ−2ID

(cid:12)(cid:12)−1/2

(cid:18)

(cid:19)(cid:62)(cid:18) 1
Thus, we see that rθ has a nonstationary component that penalises the norm of(cid:0) x+y

(cid:18) x + y

− 1
2
− 1
2

(x − y)T (2Σθ)−1(x − y)

× exp

exp

2

2

(cid:32)

A

 du.
θ (y − u) + γ−2u(cid:62)u(cid:1)
(cid:125)
(cid:123)(cid:122)
(cid:19)(cid:62)(cid:18) 1
(cid:18) x + y
(cid:19)
(cid:19)−1(cid:18) x + y
(cid:19)(cid:62)(cid:18) 1
(cid:19)−1(cid:18) x + y
(cid:18) x + y
(cid:19)
(cid:19)(cid:33)

(cid:19)−1(cid:18) x + y

Σθ + γ2ID

Σθ + γ2ID

2

2

2

2

2

2

(cid:19)(cid:33)

Σθ + γ2ID

(cid:1). This is reminiscent of the well known

2

.

2

locally stationary covariance functions (Silverman, 1957). However, for large values of γ, the nonstationary component
becomes negligible and rθ reverts to being proportional to a standard squared exponential kernel with covariance 2Σθ, just
like in the case of Lebesgue measure. We note that any choice of γ > 0 gives a valid prior over Hk. Treating γ as another
hyperparameter to be learned would be an interesting direction for future research.

A.4

Jacobian calculation for the squared exponential kernel

In practice, we work with the log-marginal likelihood of the Bayesian Kernel Embedding model, so we calculate:

2

(cid:89)

i

=

1
2

log

γ(xi, θ) =

n(cid:88)

i=1

log

1
n2

= −n log n +

1
2

 1

n

n(cid:88)

j=1

D(cid:88)

d=1

i=1

1
2

log

n(cid:88)
 n(cid:88)
D(cid:88)
D(cid:88)
n(cid:88)

d=1

j=1

log

k(xj, xi)

 n(cid:88)

i=1

d=1

j=1

k(xj, xi)

j − x(d)
x(d)

i

θ2

2

j − x(d)
x(d)

i

θ2

k(xj, xi)

j − x(d)
x(d)

i

θ2

2

(26)

(27)

(28)

Algorithmically, we have a for loop over d = 1, . . . , D wherein we calculate square matrices Ld with entries
. Then we calculate vectors (cid:96)1 = sum(L1, 1), . . . , (cid:96)D = sum(LD, 1) so as to calculate

Lli = k(xj, xi)

j −x(d)
x(d)

i

l=1 k(xj, xi)

j −x(d)
x(d)

i

θ2

= 1
2

d (cid:96)d.∧2 where .∧ indicates elementwise power.

(cid:80)
i log(cid:80)D

d=1

1
2

(cid:18)(cid:80)n

θ2

(cid:19)2

(cid:80)
i log(cid:80)

A.5 Marginal likelihood and gradient

The marginal likelihood conditionally on the hyperparameters θ and σ2 is

p(x1, . . . , xn|θ) = N(cid:0)[ˆµθ(x1), . . . , ˆµθ(xn)](cid:62); 0, Rθ + σ2In

(cid:1) ×

(cid:33)

γ(xi, θ)

(29)

(cid:32)(cid:89)

i

where Rθ is the n × n matrix such that its (i, j)-th element is rθ(xi, xj) and

(cid:18) ∂ ˆµθ(x)

(cid:19)2

∂x(d)

.

γ(x, θ) =

(cid:118)(cid:117)(cid:117)(cid:116) D(cid:88)
(cid:18) ∂p(x1, . . . , xn|θ)

d=1

∂θ(1)

,··· ,

∂p(x1, . . . , xn|θ)

∂θ(Q)

(cid:19)T

.

∇θp(x1, . . . , xn|θ) =

In order to maximise (29) with respect to θ = (θ(1), . . . θ(Q))T , it is useful to derive the gradient

A.5.1 Gradient computation for a squared exponential kernel
√

Consider a squared exponential kernel with length-scale equal to

(cid:18)

kθ(x, y) = exp

(x − y)T (θID)−1(x − y)

− 1
2

= exp

− 1
2

(cid:19)

θ(d) for component 1 ≤ d ≤ D deﬁned by

(cid:32)

(cid:33)

D(cid:88)

(x(d) − y(d))2

d=1

θ(d)

.

(30)

For any 1 ≤ d ≤ D, the partial derivative of ˆµθ with respect to x(d) is

and therefore

γ(x, θ) =

∂ ˆµθ(x)
∂x(d)

1
n

(cid:32)

=

(cid:118)(cid:117)(cid:117)(cid:116) D(cid:88)

(cid:88)
(cid:88)

i

1
n

kθ(xi, x)

i − x(d)
x(d)
θ(d)

kθ(xi, x)

i − x(d)
x(d)
θ(d)

d=1

i

(cid:33)2

.

We want to compute the partial derivative of p(x1, . . . , xn|θ) with respect to θ(q):

∂log p(x1, . . . , xn|θ)

∂θ(q)

=

∂

∂θ(q)

log N(cid:0)[ˆµθ(x1), . . . , ˆµθ(xn)](cid:62); 0, Rθ + σ2In

(cid:1) +

∂

∂θ(q)

log

(cid:89)

j

 .

γ(xj, θ)

We start focusing on the ﬁrst term. For simpliﬁcation, denote by ¯µθ = [ˆµθ(x1), . . . , ˆµθ(xn)](cid:62) and by ¯Rθ = Rθ + σ2In.
We have

(cid:19)

∂ log N(cid:0)¯µθ; 0, ¯Rθ

(cid:1)

∂θ(q)

=

=

2

∂

∂θ(q)
1
2

(cid:18) 1
(cid:18) ∂ ¯µT
(cid:18) ∂Rθ

θ
∂θ(q)

(cid:19)

(cid:18) ∂ ¯Rθ

(cid:19)

=

∂θ(q)

ij

∂θ(q)

ij

where the (i,j)-th component of the matrix

¯µT
θ

θ ¯µθ − 1
¯R−1
2
¯R−1

log | ¯Rθ| − n
2
¯R−1
θ ¯µθ + ¯µT

¯R−1
θ ¯µθ + ¯µT

log 2π

θ

θ

θ

∂ ¯Rθ
∂θ(q)

¯R−1

θ

∂ ¯µθ
∂θ(q)

(cid:19)

− 1
2

tr

=

∂rθ(xi, xj)

∂θ(q)

=

1
2

rθ(xi, xj)

(cid:32) (x(q)

i − x(q)
j )2
2θ(q)2

+

1
θ(q)

(cid:19)

∂ ¯Rθ
∂θ(q)

¯R−1

θ

(cid:18)
(cid:33)

and the i-th component of the vector ∂ ¯µθ

∂θ(q) is

∂ ˆµθ(xi)

We then, compute the partial derivative of log(cid:81)

∂θ(q)

(cid:88)

j

1
2n

kθ(xj, xi)

i − x(q)
(x(q)
j )2
θ(q)2

.

=

j γ(xj, θ) with respect to θ(q):

(cid:88)

j γ(xj, θ)

=

∂θ(q)

∂ log γ(xijθ)

=

1

∂γ(xj, θ)

∂θ(q)

j

γ(xj, θ)

∂θ(q)

j

∂

∂ log(cid:81)
(cid:32)
(cid:88)
(cid:32)(cid:88)
(cid:32)(cid:88)

∂θ(q)

1
n

d=1

D(cid:88)
(cid:32)
D(cid:88)
D(cid:88)
D(cid:88)

d=1

d=1

i

i

where

∂γ(x, θ)

∂θ(q)

=

=

=

=

1

2γ(x, θ)

1

γ(x, θ)

1

n2γ(x, θ)

1

(cid:88)

i

1
n

kθ(xi, x)

kθ(xi, x)

kθ(xi, x)

∂

i − x(d)
x(d)
(cid:33)
θ(d)
(cid:33)(cid:32)(cid:88)
(cid:33)(cid:32)(cid:88)

∂θ(q)

i

i − x(d)
x(d)
θ(d)
i − x(d)
x(d)
θ(d)
i − x(d)
x(d)
θ(d)

i

(cid:88)

(cid:33)2
(cid:32)

(cid:88)

i

1
n

(cid:33)

i − x(d)
x(d)
θ(d)

kθ(xi, x)

(cid:18) 1

θ(d)
i − x(d)
x(d)
θ(q)2

(cid:32)

i − x(d))
(x(d)

∂kθ(xi, x)

∂θ(q)

− kθ(xi, x)

θ(q)2

δd=q

i − x(q))2
(x(q)
2θ(d)

− δd=q

(cid:19)(cid:33)
(cid:33)(cid:33)

.

n2γ(x, θ)

d=1

i

kθ(xi, x)

kθ(xi, x)

B Source for Stan model

data {

int<lower=1> n;
vector[n] x;
real sigma2;

}

transformed data {

matrix[n,n] x_diff;
matrix[n,n] x_dist2;
vector[n] zeros;
vector[n] ones;

real nf;

nf <- n * 1.0;
for (i in 1:n) {
zeros[i] <- 0;
ones[i] <- 1;

for (j in 1:n) {

x_diff[i, j] <- (x[i] - x[j]);
x_dist2[i, j] <- square(x[i] - x[j]);

}

}

}

parameters {

real<lower=0> lengthscale;

}
transformed parameters {

matrix[n,n] L;
matrix[n,n] R;
matrix[n,n] J;
matrix[n,n] K;

vector[n] muhat;

R <- exp(- x_dist2/(4*lengthscaleˆ2));
K <- exp(- x_dist2/(2*lengthscaleˆ2));
L <- cholesky_decompose(R+diag_matrix(sigma2*ones));
J <- -K .* x_diff/(lengthscaleˆ2 * nf);
for(i in 1:n)

muhat[i] <- mean(K[i]);

}

model {

for(i in 1:n) { // Jacobian

increment_log_prob(log(fabs(sum(J[i]))));

}
muhat ˜ multi_normal_cholesky(zeros, L);
lengthscale ˜ gamma(.1,.1);

}

