6
1
0
2

 
r
a

 

M
3
2

 
 
]

.

A
N
h
t
a
m

[
 
 

3
v
3
3
1
5
0

.

3
0
6
1
:
v
i
X
r
a

CHARACTERIZATIONS OF VARIATIONAL SOURCE CONDITIONS,

CONVERSE RESULTS, AND MAXISETS OF SPECTRAL

REGULARIZATION METHODS

THORSTEN HOHAGE AND FREDERIC WEIDLING

Institute for Numerical and Applied Mathematics, University of Goettingen, Lotzestr.

16-18, 37083 Goettingen, Germany

Abstract. We describe a general strategy for the veriﬁcation of variational source con-
dition by formulating two suﬃcient criteria describing the smoothness of the solution and
the degree of ill-posedness of the forward operator in terms of a family of subspaces. For
linear deterministic inverse problems we show that variational source conditions are nec-
essary and suﬃcient for convergence rates of spectral regularization methods, which are
slower than the square root of the noise level. A similar result is shown for linear inverse
problems with white noise. In many cases variational source conditions can be character-
ized by Besov spaces. This is discussed for a number of prominent inverse problems.

1. Introduction

This paper is concerned with inverse problems described by ill-posed operator equations
in real Hilbert spaces X and Y. Let T : X → Y an injective, bounded, linear operator and
f† ∈ X the unknown solution to the inverse problem. We will study both a deterministic
and a white noise noise model. In the ﬁrst case measurement errors are described by a
vector ξ ∈ Y, and observed data are given by
gobs = T f† + ξ,
(1)

kξk ≤ δ

for some deterministic noise level δ > 0.
described by a white noise process W on Y, and observed data are given by

In the second case measurement errors are

(2)

gobs = T f† + εW

with a stochastic noise level ε > 0. Recall that a white noise process is characterized by
the relations E[hW, yi] = 0 and E [hW, y1ihW, y2i] = hy1, y2i for all y, y1, y2 ∈ Y.
Regularization theory is concerned with error estimates for approximate reconstruction
methods (regularization methods) for f† given data gobs described by (1) or (2).
It is
well-known that for ill-posed problems uniform error bounds necessarily require further
assumptions on the solution f† (see [5, Prop. 3.11]). Such conditions are usually called

E-mail address: t.hohage@math.uni-goettingen.de,f.weidling@math.uni-goettingen.de.

1

(3)

2Df†, f† − fEX ≤

1

2(cid:13)(cid:13)(cid:13)f − f†(cid:13)(cid:13)(cid:13)

Y(cid:19)
+ ψ(cid:18)(cid:13)(cid:13)(cid:13)T (f ) − T (f†)(cid:13)(cid:13)(cid:13)

2

2

X

for all f ∈ X

2

MAXISETS FOR SPECTRAL REGULARIZATION

source conditions. Over the last years, starting with [8] it has become increasingly popular
to formulate such conditions in the form of variational inequalities

with an index function ψ. (A function ψ : [0,∞) → [0,∞) is called an index function if it
is continuous, strictly monotonically increasing, and ψ(0) = 0.) Advantages of these varia-
tional source conditions (VSC) over classical source conditions of the form f† = ϕ(T ∗T )w
with an index function ϕ and w ∈ X include extensions to Banach spaces, general penalty
and data ﬁdelity functionals, treatment of nonlinear operators without the need of a de-
rivative of T and restrictive assumptions relating T ′ and T , as well as simpler proofs. As
a disadvantage we mention that (3) cannot be used to describe high order rates of conver-
gence since it is easy to see that it cannot hold true for f† 6= 0 if limx→0 ψ(x2)/x = 0. This
excludes in particular the case ψ(t) = tν with ν > 1/2.

In this paper we will address the following two related main questions:

the VSC (3) holds true?

of a given regularization method as the noise level δ or ε tends to 0?

• What are veriﬁable suﬃcient (and possibly even necessary) conditions such that
• What are necessary and suﬃcient conditions on f† for a given rate of convergence
Let us now discuss known results from the literature and the contributions of this paper
for both of these questions. Concerning the ﬁrst question, veriﬁable suﬃcient conditions
for (3) have mainly been given via spectral source conditions so far, see Appendix A for
more details. In [10, 22] we have recently derived suﬃcient conditions for (3) in the form
of bounds on Sobolev norms of f† for nonlinear inverse medium scattering problems. Here
we formulate in Theorem 2.1 two criteria which capture the main strategy of the proofs
in [10, 22]. In the following we will apply them to linear inverse problems. These criteria
describe the two main factors inﬂuencing rates of convergence: Smoothness of the solution
and ill-posedness of the inverse of the forward operator. Here both criteria are formulated
in terms of a sequence of approximating subspaces in X.
If these spaces are chosen as
eigenspaces of T ∗T , we obtain an equivalent characterization of the VSC (3) in terms of
the rate of decay of the spectral distribution function of f†. The latter criterion has been
introduced by Neubauer [17] and shown to be necessary and suﬃcient for H¨older rates of
convergence. A characterization of the VSC (3) for ψ(t) = tν, ν ∈ (0, 1/2] has previously
been shown by Andreev et al. [3] using a diﬀerent technique which seems to be limited to
this special case.

Let us now discuss the second question. In deterministic regularization theory theorems
on the necessity of conditions for a given rate of convergence (which have already shown to
be suﬃcient) are known as converse results. In statistics the maximal set of f† for which a
given estimator achieves a given desired rate of convergence is called a maxiset. Converse
results for H¨older rates of Tikhonov regularization have been established by Neubauer [17].
Andreev [2] has proven converse results for H¨older rates of (generalized) Tikhonov regular-
ization and Landweber iteration in terms of K-interpolation spaces with ﬁne index q = ∞
between X and (T ∗T )ν(X) equipped with the image space norm. Flemming, Hofmann &

MAXISETS FOR SPECTRAL REGULARIZATION

3

Math´e [7] have derived converse results for general convergence rates of the bias of gen-
eral spectral regularization methods in terms of approximate source conditions. Albani et
al. [1] proved converse results for general deterministic rates and spectral regularization
method, but additional assumptions had to be imposed, which are not always obvious to
interpret. Here we will prove converse results without such assumptions. As a byproduct
of our analysis we show the equivalence of weak and strong quasioptimality of a posteriori
parameter choice rules in many cases (see [19]). Together with our answer to the ﬁrst ques-
tion we also obtain converse results in terms of VSCs (3) for concave ψ. Moreover, we will
show for inverse problems for which the forward operator satisﬁes T ∗T = Λ(−∆) for some
Laplace-Beltrami operator ∆, that VSCs for certain index functions ψ are satisﬁed if and
only if f† belongs to a Besov space Bs
. This holds true in particular for the backward
and sideways heat equation, and the inverse gradiometry problem.

2,∞

In statistics maxisets of wavelet methods for the estimation of the density of i.i.d. ran-
dom variables have been characterized as Besov spaces by Kerkyacharian & Picard [11].
They consider not only L2, but also other Lp norms as loss functions. Maxisets of thresh-
olding and more general wavelet estimators have been characterized by the same authors
in [12, 13], and their results have been generalized by Rivoirand [20] to some linear esti-
mators in the sequence space model of inverse problems. These latter references show in
particular that under certain circumstances nonlinear thresholding methods have larger
maxisets than linear methods for given polynomial rates. Here we show under fairly gen-
eral assumptions that VSCs characterize maxisets of spectral regularization methods for
linear inverse problems with white noise.

The plan of this paper is as follows: In the following section we formulate and prove the
theorem describing our general strategy for the veriﬁcation of VSCs. In sections 3–5 we
derive converse results for the bias, rates of convergence with deterministic noise, and rates
of convergence with white noise, respectively. In Section 6 we introduce a class of inverse
problems for which maxisets of linear spectral regularization methods are given by Besov
spaces Bs
. Finally, in Section 7 we apply our theoretical results to a number of well-
known inverse problem before we end this paper with some conclusions. In an appendix we
show how the general strategy in Section 2 can be applied to verify a VSC given a spectral
source condition for linear problems.

2,∞

2. A general strategy for verifying variational source conditions

In this section we formulate suﬃcient conditions for VSCs in terms of arbitrary families
of subspaces. In the rest of this paper these will always be chosen as invariant subspaces
of T ∗T , but in principle the choice is arbitrary. To allow also polynomial, trigonometric,
wavelet and other subspaces, which may be relevant in more general situation (see e.g.
[10]), we will parametrize the spaces by a general index set J .
Theorem 2.1. Suppose there exists a family of orthogonal projections Pr ∈ L(X) indexed
by a parameter r in some index set J such that for some functions κ, σ : J → (0,∞) and
some C ≥ 0 the following conditions hold true for all r ∈ J :
(4)

kf† − Prf†kX ≤ κ(r)

4

(5)

MAXISETS FOR SPECTRAL REGULARIZATION

Df†, Pr(f† − f )EX ≤ σ(r)kT (f†) − T (f )k + Cκ(r)kf† − fk
for all f ∈ D(T ) with kf − f†k ≤ 4kf†k.
r∈Jh(C + 1)2κ(r)2 + σ(r)√ti .

ψ(t) := 2 inf

Then f† satisﬁes the VSC (3) with

Assumption (4) may be seen as an estimate of the approximation quality of the approx-
imating spaces PrX, and assumption (5) may be seen as a kind of stability estimate for T
on these spaces. If f† belongs to some smooth subspace of X, the stability estimate may
be taken with respect to the norm of the dual space. However, (5) is not exactly a stability
estimate for the restriction of T to PrX since by do not bound by kT (Prf )− T (Prf†)k, but
kT (f )− T (f†)k. On the other hand the additional term Cκ(r)kf − f†k may help. The case
C > 0 and the restriction to f ∈ X with kf − f†k ≤ 4kf†k are also crucial for nonlinear
inverse problems.
Proof of Theorem 2.1. If kf − f†k > 4kf†k we have
(6)

2Df†, f† − fE ≤ 2kf†k kf† − fk ≤

1
2kf† − fk2,

so (3) holds true. Otherwise we can apply (5) and (4) and the basic inequality 2ab ≤
2a2 + 1

2b2 to obtain

2Df†, (f† − f )E = 2Df†, Pr(f† − f )E + 2D(I − Pr)f†, f† − fE
≤ 2σ(r)kT (f ) − T (f†)k + 2(C + 1)κ(r)kf† − fk
≤ 2σ(r)kT (f ) − T (f†)k + 2(C + 1)2κ(r)2 +

1
2kf† − fk2

for all r ∈ J . Taking the inﬁmum over of the right hand side over r ∈ J with t =
kT (f ) − T (f†)k2 yields (3).

(cid:3)

3. Converse results for the bias

For λ ≥ 0 we deﬁne the spectral projections

(7)
The function λ 7→ kET ∗T
index function κ we deﬁne a subspace XT
spectral distribution function with weight 1/κ:

λ

λ

ET ∗T

:= χ[0,λ](T ∗T ).

fk is called the spectral distribution function of f ∈ X. For an
κ ⊂ X via a weighted supremum norm of the

(8)

XT

κ :=nf ∈ X : kfkXT

κ

< ∞o ,

kfkXT

κ

:= sup
λ>0

1

κ(λ)(cid:13)(cid:13)(cid:13)ET ∗T

λ

f(cid:13)(cid:13)(cid:13)X

It is not diﬃcult to see that XT
κ contains
all functions satisfying (4) with Pr = I − ET ∗T
. For the remainder of this and the following
two sections we will suppress the superscript T ∗T to simplify the notation. However, we

κ is a Banach space. Note that the unit ball in XT

r

MAXISETS FOR SPECTRAL REGULARIZATION

5

will need it in Section 6 to deal with spectral distribution functions w.r.t. several operators
simultaneously.
Theorem 3.1. Let κ : [0,∞) → [0,∞) be an index function such that t 7→ κ(t)2/t1−µ is
decreasing for some µ ∈ (0, 1) and κ · κ is concave. Moreover, we associate with each such
κ an index function ψκ by

(9)
Then the following statements for f† ∈ X are equivalent:

ψκ(t) := κ(cid:16)Θ−1

κ (√t)(cid:17)2

,

(i) f† satisﬁes a VSC with ψ(t) = Aψκ(t) for some A > 0.
(ii) kf†kXT

κ < ∞.

Θκ(λ) := √λκ(λ).

More precisely, (i) implies kEλf†k ≤ q 2A
3 κ(cid:16) 2A
if κ is normalized such that kf†kXT
2κ(kTk2) supt∈(0,4kTkkf †k] √t/ψκ(t), and A is ﬁnite.
Proof. (i) ⇒ (ii): First note that
(10)

3 )κ(λ). Vice versa,
κ = 1, then (i) holds true with A = 2(1 + µ−1) +

3 λ(cid:17) ≤ q 2A

3 max(1,q 2A

ψ−1
κ (ξ) = ξ · (κ · κ)−1(ξ)
κ (√t))2 · Θ−1

κ (√t) = Θκ(Θ−1

κ (√t))2 = √t

2

= t.

ψκ(t) · (κ · κ)−1(ψκ(t)) = κ(Θ−1
Choosing f = (I − Eλ)f† in (3) yields

since

As

1

2kEλf†k2 = 2Df†, Eλf†E ≤
kT Eλf†k2 =Z λ

λ dkEλf†k2 ≤ λZ λ

2kEλf†k2 + Aψκ(cid:16)kT Eλf†k2(cid:17) .
dkE˜λf†k2 = λkE˜λf†k2,

0

0

the spectral distribution function ˜κ(λ) := kEλf†k of f† satisﬁes the inequality 3
Aψκ (λ˜κ(λ)2) . Hence, setting eψκ(t) := ψκ(t)/t we have

ψκ (λ˜κ(λ)2)

2 ˜κ(λ)2 ≤

3
2Aλ ≤

λ˜κ(λ)2 = eψκ(λ˜κ(λ)2).

As κ· κ is assumed to be a concave index function, the inverse function (κ· κ)−1 is a convex
index function. This implies that ξ 7→ ξ · (κ · κ)−1(ξ) is a convex index function as well,
and from (10) we see that ψκ is concave. This in turn implies that eψκ is monotonically
decreasing, so we obtain

or with β = 2Aλ
3

(11)

κ (cid:18) 3
2Aλ(cid:19) ≥ λ˜κ(λ)2
eψ−1
3 vuut 1
β(cid:19) ≤s 2A
β!
κ   1
˜κ(cid:18) 3
β eψ−1

2A

6

MAXISETS FOR SPECTRAL REGULARIZATION

for all β > 0. Setting ξ = κ(β)2 in (10), we obtain ψ−1
ψκ(βκ(β)2). This is equivalent to 1

and to 1

into (11) shows that ˜κ(λ) ≤q 2A
3 λ(cid:17)2
function κ · κ we have κ(cid:16) 2A
(ii) ⇒ (i): Suppose that kf†kXT

show that

βκ(β)2

κ (κ(β)2) = βκ(β)2 or κ(β)2 =

β(cid:17) = κ2(β). Plugging this
β = ψκ(βκ(β)2)
3 λ(cid:17) for all λ > 0. Due to the concavity of the index
3 κ(cid:16) 2A
≤ κ(max(1, 2A
κ = 1, i.e. kEλf†k ≤ κ(λ) for all λ > 0. In a ﬁrst step we

κ (cid:16) 1
β eψ−1
3 )λ)2 ≤ max(1, 2A

3 )κ(λ)2.

(12)

k(T (I − Eλ))†f†k2 =Z kT ∗Tk

λ

1
t

dkEtf†k2 ≤

1
µ

κ(λ)2

λ

+ kf†k2

for all λ > 0. Here (T (I − Eλ))† denotes the Moore-Penrose inverse of T (I − Eλ). By
partial integration we have

1
t

dkEtf†k2 = kf†k2 − kEλf†k2 +Z kT ∗Tk

λ

kEtf†k2

t2

dt.

Now we use the assumption kEtf†k ≤ κ(t) and the monotonicity of κ(t)2/t1−µ to obtain

λ

Z kT ∗Tk
Z kT ∗Tk

λ

kEtf†k2

t2

dt ≤Z kT ∗Tk

κ(t)2
t1−µ

λ
κ(λ)2

µλ  1 −

=

1
t1+µ dt ≤
λµ

kT ∗Tkµ! ≤

κ(λ)2
µλ

.

κ(λ)2

λ1−µ Z kT ∗Tk

λ

1
t1+µ dt

In a second step we can now use (12) to verify assumption (5) in Theorem 2.1:

Df†, (I − Eλ)(f† − f )E =D(T (I − Eλ))†f†, T (I − Eλ)(f† − f )E
≤ kT (I − Eλ))†f†kkT (I − Eλ)(f† − f )k ≤  κ(λ)
√µλ
µλ + kf†k. Hence, by Theorem 2.1 (3) holds true with

i.e. σ(λ) = κ(λ)

+ kf†k!kT f† − T fk,

ψ(t) = 2 inf

λ>0"κ(λ)2 +  κ(λ)
√µλ
κ (√t), i.e. √λκ(λ) = √t. This implies κ(λ)√λ

+ kf†k!√t# ≤ 2 1 +

√µ! ψκ(t) + 2kf†k√t

√t = κ2(λ) =
where we have chosen λ = Θ−1
ψκ(t). It remains to bound √t in terms of ψκ(t). Note from (6) that we only need to show
the variational source condition for kf† − fk ≤ 4kf†k in order to prove it everywhere.
Hence it is enough to bound √t by ψκ(t) for √t = kT f† − T fk ≤ 4kTkkf†k. We have

1

kf†k√t ≤ κ(kTk2)ψκ(t)

√τ
ψκ(τ )

.

sup

τ∈(0,4kTkkf †k]

To see that this is ﬁnite and even limτ→0 √τ /ψκ(τ ) = 0, we substitute δ = Θ−1

κ (√τ ):

√τ
ψκ(τ )

lim
τ→0

√τ
κ (√τ ))2 = lim
κ(Θ−1
δ→0

= lim
τ→0

Θκ(δ)
κ(δ)2 ≤ lim
δ→0

δµ/2vuut δ1−µ

κ2(δ)

= 0.

(cid:3)

MAXISETS FOR SPECTRAL REGULARIZATION

7

We now consider spectral regularization methods of the form

(13)

with

Rα := qα(T ∗T )T ∗

bfα := Rαgobs

and impose the following assumptions:
Assumption 3.2. With rα(λ) := 1 − λqα(λ) assume that there are constants C1 > 0,
α ∈ (0,∞], and 0 < C2 ≤ C3 < 1 such that
α for all λ ∈ [0,kT ∗Tk] ,

(i) |qα(λ)| ≤ C1
(ii) λ 7→ rα(λ) is decreasing for all α > 0 and rα(λ) ≥ 0,
(iii) α 7→ rα(λ) is increasing for all λ ∈ [0,kT ∗Tk],
(iv) C2 ≤ rα(α) ≤ C3 for all 0 < α ≤ α.
As rα(0) = 1 − 0qα(0) = 1, assumption (ii) implies
0 ≤ rα(λ) ≤ 1

(14)
for all α > 0 and λ ≥ 0. Below we will use the following notations for x, y ∈ R:

x ∨ y := max(x, y),

x ∧ y := min(x, y)

Assumption 3.2 is satisﬁed in particular for the following methods. Unless stated otherwise
we choose α = ∞. For a more detailed discussion of these methods we refer to [5].

C1 = 1 and C2 = C3 = 1
2.

• Tikhonov regularization: Here qα(λ) = (α + λ)−1 and rα(λ) = α/(α + λ). We have
• Showalter’s method: Here rα(λ) = exp(−λ/α), C1 = 1 and C2 = C3 = exp(−1).
• Landweber iteration: For α > 0 let kα := min{n ∈ N0 : n + 1 > 1/α} be the
number of iterations. Then rα(λ) = (1 − µλ)kα and qα(λ) = Pkα−1
j=0 (1 − µλ)j
where 0 < µ ≤ kT ∗Tk−1 is the step length parameter. We have C1 = 1. For
α = 1/(n + ǫ) with ǫ ∈ [0, 1) we have kα = n, therefore rα(α) ≥ (1 − µ/n)n which
by the inequality of arithmetic and geometric means is monotonically increaing in
n = kα for kα > µ, hence we choose α < kT ∗Tk ∧ 1 and get C2 = (1− µ/kα)kα and
C3 = limn→∞(1 − µ/(n + 1))n = exp(−µ).
• k-times iterated Tikhonov regularization: This is described by rα(λ) = αk/(α+λ)k.
We have C1 = k and C2 = C3 = 2−k.
• Lardy’s method: Here rα(λ) = βkα/(β + λ)kα where β > 0 is ﬁxed and the iteration
number kα and C1 = 1. Choosing α := 1∧β we have C3 = exp(−1/2β)). Choosing
α as for Landweber we see that rα(α) ≥ (1 + 1/(βn))−n, therefore C2 = exp(−1/β)
since (1 + 1/(βn))n → exp(1/β) is monotonically increasing in n as argued above.
• modiﬁed spectral cutoﬀ: Here rα(λ) = (1 − λ/2α) ∨ 0, qα(λ) = 1/λ ∧ 1/2α, and
C1 = C2 = C3 = 1/2.

Note that Assumption 3.2(iv) is violated for standard spectral cutoﬀ (or truncated SVD),
i.e. rα(λ) = 1 if α ≤ λ and rα(λ) = 0 else.
Theorem 3.3 ([1, Prop. 2.3]). Assume that a spectral regularization method satisﬁes As-
sumption 3.2. Moreover, assume that for the index function κ there exists A > 0 and ν > 1

8

MAXISETS FOR SPECTRAL REGULARIZATION

such that

(15)

(i) kf†kXT
κ < ∞.
(ii) A := sup0<α≤α

More precisely,

for all α, λ > 0 (i.e. the qualiﬁcation of the regularization method covers κν in the termi-
nology of [16]). Then the following statements for f† ∈ X are equivalent:

rα(λ)κ(λ)ν ≤ Bκ(α)ν

1

κ(α)krα(T ∗T )f†k < ∞, i.e. the bias for f† is of order O(κ(α)).

kf†kXT

κ ≤

A

C2 ∨ kf†k

κ(α)

and A2 ≤

Bkf†k2
κ(kTk2)

+ kf†k2

XT

κ 1 +

B1/ννC (ν−1)/ν

3

ν − 1

 .

Proof. The more diﬃcult implication (i) ⇒ (ii) has been proved in [1, Prop. 2.3]. Since
we have slightly diﬀerent assumptions we give the proof of the implication (ii) ⇒ (i). For
0 < α ≤ α we have

1
rα(α)krα(T ∗T )f†k ≤
Otherwise, if α > α, we have kEαf†k/κ(α) ≤ kf†k/κ(α).

kEαf†k ≤

A
C2

κ(α).

(cid:3)

Recall that the largest number µ0 > 0 for which (15) holds true for κ(α)ν = αµ0 is
called the classical qualiﬁcation of the regularization method. We have µ0 = k for k-times
iterated Tikhonov regularization and µ = ∞ for Showalter’s method, Lardy’s methods,
Landweber iteration, and modiﬁed spectral cutoﬀ ([5]).

4. Converse results for deterministic noise

This section discusses regularization methods for the deterministic noise model (1).

Theorem 4.1. Assume that a spectral regularization method satisﬁes Assumption 3.2.
Moreover, let κ be an index function for which there exists p ≥ 1 such that
(16)
for all α > 0 and r ≥ 1 (i.e. κ does not grow faster than polynomially). Then the following
statements are equivalent for f† ∈ X:

κ(rα) ≤ rpκ(α)

(i) A := sup0<α≤α
(ii) B := sup0<δ≤Θκ(α)

More precisely,

1

κ(α)2krα(T ∗T )f†k2 < ∞.

B ≤ 2(A + C1)

A ≤ B  4B2
Proof. (i) ⇒ (ii): From the standard estimate
(17)

and

1

ψκ(δ2) inf 0<α≤α supkξk≤δ kRα(T f† + ξ) − f†k2 < ∞.
∨ kf†k2κ  α(1 − C3)2

(1 − C3)4 ∨ 1!p

2B

!−2

.

kRαk2 = kR∗αRαk = kqα(T ∗T )2T ∗Tk ≤ kqαk∞k1 − rαk∞ ≤

C1
α

MAXISETS FOR SPECTRAL REGULARIZATION

9

using Assumption 3.2(i) and (14). Hence we have

kRα(T f† + ξ) − f†k2 ≤(cid:16)krα(T ∗T )f†k + kRαkδ(cid:17)2

for all kξk ≤ δ and 0 < α ≤ α. Choosing α = Θ−1
Θκ(Θ−1

κ (δ) = ψκ(δ2), we obtain

κ (δ)) = δ, i.e. δ2/Θ−1

C1δ2

α

≤ 2Aκ(α)2 + 2
κ (δ) and using qΘ−1

κ (δ)κ(Θ−1

κ (δ)) =

sup

kξk≤δ kRα(T f† + ξ) − f†k2 ≤ (2A + 2C1)ψκ(δ2).

(ii) ⇒ (i): Expanding
kRα(T f†+ξ)−f†k2 = krα(T ∗T )f†+Rαξk2 = krα(T ∗T )f†k2+2Drα(T ∗T )f†, RαξE+kRαξk2,

we see that only the middle of the three terms on the right hand side is aﬀected by a sign
change of ξ. Therefore, to bound the supremum over ξ from below, we may assume that
the middle term is positive and neglect it to obtain

(18)

sup

kξk≤δ kRα(T f†+ξ)−f†k2 ≥ krα(T ∗T )f†k2 + sup

kξk≤δ kRαξk2 = krα(T ∗T )f†k2+kRαk2δ2.

From the equality in (17) and the isometry of the functional calculus together with the
last point in Assumption 3.2 we obtain

kRαk2 = sup
λ≥0

λ|qα(λ)|2 = sup
λ≥0

(1 − rα(λ))2

λ

(1 − rα(α))2

α

(1 − C3)2

α

≥

≥

if 0 < α ≤ α. By Assumption 3.2(iii) the ﬁrst term on the right hand side of (18)
is increasing in α whereas the second term is decreasing. Therefore, using the choice
α∗(δ) = Θ−1
κ (δ)(1 − C3)2/(2B) from the ﬁrst part of the proof, for which both terms are
of the same order, we obtain the lower bound

Bψκ(δ2) ≥ inf

α>0

sup

kξk≤δ kRα(T f† + ξ) − f†k2 ≥ krα∗(δ)(T ∗T )f†k2 ∧ kRα∗(δ)k2δ2
δ2 = krα∗(δ)(T ∗T )f†k2 ∧ 2Bψκ(δ2)

(1 − C3)2
α∗(δ)

≥ krα∗(δ)(T ∗T )f†k2 ∧

for α ≥ α∗(δ) > 0. As krα∗(δ)(T ∗T )f†k2 ≥ 2Bψκ(δ2) would lead to a contradiction, the
minimum is attained at the ﬁrst argument, and we have krα∗(δ)(T ∗T )f†k2 ≤ Bψκ(δ2). As
δ = Θκ(

2B

(1−C3)2 α∗(δ)) and ψκ(t) = (κ ◦ Θ−1
krα∗(T ∗T )f†k2 ≤ Bκ 
2B

κ (√t))2, we obtain
≤ B  4B2

(1 − C3)2 α∗!2

κ(α∗)2

(1 − C3)4 ∨ 1!p
!−2

kf†k2

for all 0 < α∗ ≤ α(1 − C3)2/(2B). If α(1 − C3)2/(2B) < α ≤ α we can bound

κ(α)−2krα(T ∗T )f†k2 ≤ κ  α(1 − C3)2

2B

ﬁnishing the proof.

(cid:3)

10

MAXISETS FOR SPECTRAL REGULARIZATION

We point out that in comparison to similar results by Neubauer [17, Thm. 2.6] and
Albani et al. [1, Prop. 3.3] we have interchanged the order of the supremum over the noise
vector ξ and the inﬁmum over the regularization parameter α. Since obviously

(19)

sup
kξk≤δ

α kRα(T f† + ξ) − f†k ≤ inf
inf

α

sup

kξk≤δ kRα(T f† + ξ) − f†k,

the more diﬃcult implication (ii) ⇒ (i) in Theorem 4.1 is weaker than in [1]. However,
we do not have to impose additional assumptions relating the regularization method and
the index function as required in [1]. Let us now state conditions under which a reverse
inequality to (19) holds true:

Lemma 4.2. Under Assumption 3.2 the estimate

(20)

inf

0<α≤α

holds true for all

(21)

sup

kξk≤δ(cid:13)(cid:13)(cid:13)Rα(T f† + ξ) − f†(cid:13)(cid:13)(cid:13) ≤ 4√2 sup
δ ∈ ∆(f†) :=(krα(T ∗T )f†k

kξk≤δ

inf

0<α≤α(cid:13)(cid:13)(cid:13)Rα(T f† + ξ) − f†(cid:13)(cid:13)(cid:13)
: 0 < α < α)

kRαk

This set has the following properties:

∆(f†) = (0,∞).

(i) If qα(λ) is continuous in α with α = ∞ for all λ ∈ σ(T ∗T ) and f† 6= 0, then
(ii) If Eαf† 6= 0 for all α > 0, then 0 is always a cluster point of ∆(f†).
(iii) For Landweber iteration with µkT ∗Tk < 1 and Lardy’s method and f† 6= 0 the size
of the gaps of ∆(f†) on a logarithmic scale is bounded by ln γ with

(22)

γ := sup( b

a

: a, b ∈ ∆(f†) ∧ 0 < a < b ∧ (a, b) ∩ ∆(f†) = ∅) < ∞

Proof. For δ ∈ ∆(f†) there exists α′ = α′(δ, f†) such that

(cid:13)(cid:13)(cid:13)rα′(T ∗T )f†(cid:13)(cid:13)(cid:13) = kRα′k δ.

For this α′ and 0 < ǫ < 1 we choose the noise vector ξ′ such that kξ′k ≤ δ and kRα′ξ′k ≥
(1 − ǫ)kRα′kδ. Then a case distinction α ≤ α′ and α ≥ α′ shows that

inf
α>0

sup

kξk≤δ(cid:13)(cid:13)(cid:13)Rα(T f† + ξ) − f†(cid:13)(cid:13)(cid:13) ≤ inf

α>0h(cid:13)(cid:13)(cid:13)rα(T ∗T )f†(cid:13)(cid:13)(cid:13) + kRαk δi
≤ 2(cid:13)(cid:13)(cid:13)rα′(T ∗T )f†(cid:13)(cid:13)(cid:13) + 2kRα′k δ ≤ 2(cid:13)(cid:13)(cid:13)rα′(T ∗T )f†(cid:13)(cid:13)(cid:13) +

2

1 − ǫ kRα′ξ′k

since by Assumption 3.2(iii) krα(T ∗T )f†k is increasing in α and kRαk is decreasing. Since
kRαξ′k is decreasing in α as well, a similar case distinction shows that

2(cid:13)(cid:13)(cid:13)rα′(T ∗T )f†(cid:13)(cid:13)(cid:13) +

2

1 − ǫ kRα′ξ′k ≤

4
1 − ǫ

inf

α≥β>0(cid:16)(cid:13)(cid:13)(cid:13)rβ(T ∗T )f†(cid:13)(cid:13)(cid:13) + kRβξ′k(cid:17) .

MAXISETS FOR SPECTRAL REGULARIZATION

11

Obviously all estimates so far hold true also for ξ′ replaced by −ξ′. We now choose for
each α ≥ β > 0 the sign sβ ∈ {−1, 1} in such a way that
Drβ(T ∗T )f†, Rβsβξ′E ≥ 0.
(23)
Since for hx, yi ≥ 0 we have the inequality (kxk + kyk)2 ≤ 2kx + yk2 we obtain via (23)
that

inf

α≥α>0

sup

kξk≤δ(cid:13)(cid:13)(cid:13)Rα(T f† + ξ) − f†(cid:13)(cid:13)(cid:13) ≤

4
1 − ǫ

inf

α≥β>0(cid:16)(cid:13)(cid:13)(cid:13)rβ(T ∗T )f†(cid:13)(cid:13)(cid:13) + kRβsβξ′k(cid:17)

4√2
1 − ǫ

≤

inf

α≥β>0(cid:13)(cid:13)(cid:13)Rβ(T f† + sβξ′) − f†(cid:13)(cid:13)(cid:13) ≤

4√2
1 − ǫ

sup
kξk≤δ

inf

α≥β>0(cid:13)(cid:13)(cid:13)Rβ(T f† + ξ) − f†(cid:13)(cid:13)(cid:13) .

As this inequality holds true for all 0 < ǫ < 1 we have proven (20). Let us now show the
properties of ∆(f†):

(i) If qα(λ) and rα(λ) are continuous in α, then krα(T ∗T )f†k is continuous in α by
Lebesgue’s Dominated Convergence Theorem, and so is α 7→ kRαk. Moreover,
limα→0 krα(T ∗T )f†k = 0 as T is assumed to be injective, α 7→ kRαk is decreasing,
by (17) we have kRαk → 0 as α → α, and by (14) we have krα(T ∗T )f†k ≤ kf†k
for all α. As f† 6= 0, the case rα(T ∗T )f† = 0 for all α > 0 can be excluded by
noting that

(24)

rα(λ) > 0

for α > C1λ

since qα(λ) ≤ C1/α < 1/λ. This shows that krα(T ∗T )f†k/kRαk tends to 0 as
α → 0 and to ∞ as α → α. Now ∆(f†) = (0,∞) follows from continuity and the
intermediate value theorem.
(ii) If qα(λ) is not continuous or α 6= ∞, we still have the same limiting behaviour of
krα(T ∗T )f†k/kRαk for α → 0. However, we have to exclude the case rα(T ∗T )f† =
0 for all α in some neighborhood of 0 to ensure that 0 is a cluster point of ∆(f†).
This is achieved by (24) and the assumption Eαf† 6= 0 for all α > 0.
∆(f†) =(δn(f†) := kr1/n(T ∗T )f†k

(iii) For Landweber iteration and Lardy’s method we have

: n ∈ N)

δn(f†)
δn+1(f†)

and

.

γ = sup
n∈N

kR1/nk

Using (17) we can bound quotients of the denominators of δn(f†) by

kR1/(n+1)k = sup

1 − r1/(n+1)(λ)

√λ
≤ sup
1 − r1/(n+1)(λ)
1 − r1/n(λ) kR1/nk.

λ∈σ(T ∗T )

λ∈σ(T ∗T )
≤ sup

λ∈[0,kT ∗Tk]

1 − r1/(n+1)(λ)
1 − r1/n(λ)

sup

λ∈σ(T ∗T )

1 − r1/n(λ)

√λ

Now setting x = (1 − µλ) and x = β/(β + λ) for Landweber iteration and Lardy’s
method respectively we obtain the bound

kR1/(n+1)k
kR1/nk ≤ sup

x∈[0,1]

1 − xn+1
1 − xn = sup

x∈[0,1](cid:18)x +

1 − x

1 − xn(cid:19) ≤ 2.

12

MAXISETS FOR SPECTRAL REGULARIZATION

For Landweber iteration quotients of enumerators of δn(f†) are bounded by

(1 − µλ)2n dkEλf†k2
(1 − µλ)2n+2 dkEλf†k2 ≤

1

(1 − µ kT ∗Tk)2

0

0

kr1/n(T ∗T )f†k2

kr1/(n+1)(T ∗T )f†k2 = R kT ∗Tk
R kT ∗Tk
β+λ(cid:17)2n
(cid:16) β
kr1/(n+1)(T ∗T )f†k2 = R kT ∗Tk
(cid:16) β
β+λ(cid:17)2n+2
R kT ∗Tk

kr1/n(T ∗T )f†k2

This shows that γ is ﬁnite in both cases.

0

0

since (1 − µλ) ≥ 1 − µ kT ∗Tk for all λ ≤ kT ∗Tk. Similar for Lardy’s method we
use β/(β + λ) ≥ β/(β + kT ∗Tk) for all λ ≤ kT ∗Tk to obtain

dkEλf†k2

β !2
dkEλf†k2 ≤ 1 + kT ∗Tk

.

(cid:3)

The diﬀerence between our Theorem 4.1 and the corresponding results in [1,17] is analo-
gous to the diﬀerence between the concepts of weakly and strongly quasioptimal parameter
choice rules as introduced by Raus & H¨amarik [19]. They called a parameter choice rule
α∗ : [0,∞) × Y → [0,∞) weakly quasioptimal (or simply quasioptimal) for the regulariza-
tion method {Rα} if there exists a constant C > 0 such that
(25)

sup

kRα∗(δ,gobs)gobs − f†k ≤ C inf

α>0

kξk≤δ kRα(T f† + ξ) − f†k + O(δ)

for all f† ∈ X and all gobs ∈ Y with kgobs − T f†k ≤ δ as δ → 0. (Our formulation of this
deﬁnition slightly diﬀers from that in [19], but it is equivalent due to the arguments at
the beginning of part 2 of the proof of Theorem 4.1.) A parameter choice rule α∗ is called
strongly quasioptimal if there exists C > 0 such that

(26)

kRα∗(δ,gobs)gobs − f†k ≤ C sup
kξk≤δ

α>0kRα(T f† + ξ) − f†k + O(δ)
inf

for all f† ∈ X and all gobs ∈ Y with kgobs − T f†k ≤ δ. Lemma 4.2 shows that the notions
In
of weak and strong quasioptimality coincide for continuous regularization methods.
[19] it is shown that the discrepancy principle is strongly quasioptimal for regularization
methods of inﬁnite qualiﬁcation such as Landweber iteration, Lardy’s method or spectral
cut-oﬀ (but it is not even weakly quasioptimal for Tikhonov regularization and iterated
Tikhonov regularization). For iterated Tikhonov regularization the Raus-Gfrerer rule is
weakly quasioptimal by results in [18], and hence by Lemma 4.2 also strongly quasioptimal.
Moreover, Lepski˘ı’s rule was shown to be weakly quasioptimal for all considered methods
([19]), and hence it is strongly quasioptimal for (iterated) Tikhonov regularization, Showal-
ter’s method, and modiﬁed spectral cut-oﬀ by Lemma 4.2. The Monotone Error Rule is
strongly quasioptimal for Landweber iteration and Lardy’s method ([19]). In most cases
the constant C in (25) or (26) can be given explicitly.

Theorem 4.3. Suppose Assumption 3.2 holds true, let α∗ be weakly quasioptimal param-
eter choice rule, let ψκ be concave, and assume that (16) holds true. Then the following
statements are equivalent for any f† ∈ X for which ∆(f†) satisﬁes (22):

(i) supα≥α>0

1

κ(α)2krα(T ∗T )f†k2 < ∞.

MAXISETS FOR SPECTRAL REGULARIZATION

13

(ii) For any ﬁnite δ0 > 0 we have

1

ψκ(δ2)

sup
δ∈(0,δ0]

sup

kξk≤δ kRα∗(δ,T f †+ξ)(T f† + ξ) − f†k2 < ∞.

Proof. (i) ⇒ (ii): Using Theorem 4.1 and the deﬁnition of weak quasioptimality (25) we
see that there exists a constant C such that for all δ > 0 the estimate

sup

kξk≤δ kRα∗(δ,T f †+ξ)(T f† + ξ) − f†k2 ≤ C(cid:16)ψκ(δ2) + δ2(cid:17)

holds true. Since ψκ is concave limt→0 t/ψκ(t) is bounded and hence we obtain (ii) for any
ﬁnite δ0 > 0.

(ii) ⇒ (i): By Lemma 4.2 we have

inf

α≥α>0
≤ 32

sup

kgobs−T f †k≤δ kRα(gobs) − f†k2 ≤ 32
kgobs−T f †k≤δ kRα∗(δ,gobs)(gobs) − f†k2 ≤ 32C(f†, δ0)ψκ(δ)

kgobs−T f †k≤δ

sup

sup

inf

α≥α>0kRα(gobs) − f†k2

for all δ ∈ ∆(f†)∩[0, δ0]. Now choose δ0 = Θκ(2Bβ/(1−C3)2) with β = α∧kT ∗Tk and ﬁrst
assume that ∆(f†) ∩ (0, δ0] = (0, δ0]. Then we obtain supα∈(0,β]
κ(α)2krα(T ∗T )f†k2 < ∞
following the proof of Theorem 4.1. As krα(T ∗T )f†k2 ≤ kf†k2 for all α > 0, this is
equivalent to (i).
If ∆(f†) has gaps, but satisﬁes (22), then for each δ ∈ (0, δ0] we can ﬁnd δ ∈ [δ/γ, δ]
with δ ∈ ∆(f†). By the concavity of ψκ we have ψκ(δ)/ψκ(δ) ≤ γ. Therefore, replacing
the supremum over δ ∈ (0, δ0]∩ ∆(f†) by the supremum over δ ∈ (0, δ0] increases the value
at most by a factor γ.

(cid:3)

1

5. Converse results for white noise

We now want to prove a theorem similar to Theorem 4.1 for the white noise error model
(2) using the expected square error as error measure. By the bias-variance decomposition
this equals

2

X

2

(27)

E(cid:20)(cid:13)(cid:13)(cid:13)bfα − f†(cid:13)(cid:13)(cid:13)

X(cid:21) =(cid:13)(cid:13)(cid:13)Ehbfαi − f†(cid:13)(cid:13)(cid:13)

+ ε2EhkRαWk2i .
By Theorem 3.3 the bias can be controlled by assuming that f† ∈ XT
κ . The variance is
given by ε2E [kRαWk]2 = ε2trace(R∗αRα), i.e. as opposed to the deterministic the eﬀect of
the noise is not described by the maximum, but by the sum of the eigenvalues of R∗αRα.
Often the sum grows faster than the maximum as α → 0, and the speciﬁc rate depends not
only on the regularization methods, but also on the eigenvalue distribution of the operator.
We will assume that there exists a constant D ≥ 1 and a monotonically decreasing function
v ∈ C((0,∞)) such that
(28a)

1
D

v(α)2 ≤ EhkRαWk2i ≤ Dv(α)2

∀α ≥ α > 0

14

MAXISETS FOR SPECTRAL REGULARIZATION

with limits limt→0 v(t) = ∞ and limt→∞ v(t) = 0. Note the in the deterministic case, i.e.
for EhkRαWk2i replaced by kRαk2, we could simply choose v(α) = c/√α. Moreover, we
will assume that v(α) does not grow faster than polynomially as α → 0, or equivalently,
that the inverse function v−1 : (0,∞) → (0,∞) does not decay faster than polynomially
at inﬁnity in the sense that there exists p ≥ 1 such that
v−1(rt) ≥ r−qv−1 (t)
(28b)
for all t > 0 and r ≥ 1.
It was shown in [4] that E [kRαWk2] ∼ Ehk(T (I − ET ∗T
and explicit expressions for v have been derived.

))†k2i under certain conditions,

α

Theorem 5.1. Let Assumption 3.2 and (28) hold true and deﬁne

ψκ,v(t) := κ(cid:16)Θ−1

κ,v(cid:16)√t(cid:17)(cid:17)2

with

Θκ,v(α) :=

κ(α)
v(α)

.

Moreover, assume that κ satisﬁes (16). Then for f† ∈ X the following statements are
equivalent:

(i) A := sup0<α≤α
(ii) B := sup0<ε≤Θκ,v(α)

1

κ(α)2krα(T ∗T )f†k2

X < ∞

1

ψκ,v(ε2) inf 0<α≤α E[kRα(T f† + εW ) − f†k2

X] < ∞.

More precisely,

B ≤ A + D

and

A ≤ B ∨ B(2BD)pq ∨

kf†k2

κ(cid:16)v−1(√2BDv(α))(cid:17) .

Proof. (i) ⇒ (ii): Set bfα := Rα(T f† + εW ). By (i) and Theorem 3.3 we can bound
kE[bfα] − f†k2
X ≤ Aκ(α)2, and by assumption (28a) we can estimate
X = krα(T ∗T )f†k2
E[kRαWk2] ≤ Dv2(α). Hence,

E(cid:20)(cid:13)(cid:13)(cid:13)bfα − f†(cid:13)(cid:13)(cid:13)

2

X(cid:21) ≤ Aκ2(α) + Dε2v2(α).

The minimum over the right hand side is approximately attained if κ(α) = εv(α) or
equivalently if α = Θ−1

κ,v(ε). The equality κ(α) = εv(α) implies in particular that

(29)

ψκ,v(ε2) = ε2v(Θ−1

κ,v(ε))2.

Therefore, for all ε > 0 we obtain

0<α≤α
and can choose B = A + D.

inf

E(cid:20)(cid:13)(cid:13)(cid:13)bfα − f†(cid:13)(cid:13)(cid:13)
X(cid:21) ≥(cid:13)(cid:13)(cid:13)Ehbfαi − f†(cid:13)(cid:13)(cid:13)

2

E(cid:20)(cid:13)(cid:13)(cid:13)bfα − f†(cid:13)(cid:13)(cid:13)

(ii) ⇒ (i): Using again (27) and the lower bound on the variance in (28a) we obtain

2

X(cid:21) ≤ [A + D] κ(cid:16)Θ−1

κ,v(ε)(cid:17)2

2

X

+

1
D

v(α) =(cid:13)(cid:13)(cid:13)rα(T ∗T )f†(cid:13)(cid:13)(cid:13)

2

X

+

ε2
D

v(α)2.

MAXISETS FOR SPECTRAL REGULARIZATION

15

Because the ﬁrst term is increasing and the second term is decreasing in α, we obtain

α>0"(cid:13)(cid:13)(cid:13)rα(T ∗T )f†(cid:13)(cid:13)(cid:13)

2

X

+

ε2
D

v(α)2# ≥(cid:13)(cid:13)(cid:13)rα∗(T ∗T )f†(cid:13)(cid:13)(cid:13)

2

X ∧

ε2
D

v(α∗)2.

(30)

Bψκ,v(ε2) ≥ inf

for any α∗ ∈ (0, α]. We will choose
(31)

α∗(ε) = v−1(cid:16)√2BD v(cid:16)Θ−1

κ,v (ε)(cid:17)(cid:17) .

Using (29) we see that the second term in the minimum in (30) equals twice the left hand
side:

ε2
v(α∗)2 = 2Bε2v(Θ−1
D
2
X ≥ ε2

κ,v(ε))2 = 2Bψκ,v(ε2),

2

attained at the ﬁrst argument. We obtain

D v(α∗)2 leads to a contradiction, i.e. the minimum in (30) is

Therefore, (cid:13)(cid:13)(cid:13)rα∗(T ∗T )f†(cid:13)(cid:13)(cid:13)
(cid:13)(cid:13)(cid:13)rα∗(T ∗T )f†(cid:13)(cid:13)(cid:13)
(cid:13)(cid:13)(cid:13)rα∗(T ∗T )f†(cid:13)(cid:13)(cid:13)
= Bκ (z)2 ≤ Bκ(cid:16)(cid:16)1 ∨ (2BD)q/2(cid:17) α∗(cid:17)2

√BD!!!2 ,
X ≤Bψκ,v(ε2) = Bψκ,v Θκ,v v−1  v(α∗)
X ≤ Bψκ,v ((Θκ,v (z)))2 = B (Θκ,v (z))2 v(cid:16)Θ−1

where we have solved (31) for ε in the second step. Abbreviating z := v−1(cid:16)v(α∗)/√2BD(cid:17)

and using (29) again, we ﬁnd
2

≤ (B ∨ B(2BD)pq) κ(α∗)2

κ,v (Θκ,v (z))(cid:17)2

for all α ∈ (0, α] deﬁned by (31) using (28b).
For α ∈ (0, α] not of the given form note that α ≥ v−1(√2BDv(α)) and using mononicity
we obtain for these α

1

κ(α)2(cid:13)(cid:13)(cid:13)rα(T ∗T )f†(cid:13)(cid:13)(cid:13)

2

X ≤

kf†k2

κ(cid:16)v−1(√2BDv(α))(cid:17)

(cid:3)

showing boundedness for all α ∈ (0, α].
Remark 5.2. If assumption (28a) is relaxed to

(32)
where possibly limα→0(v+/v−)(α) = ∞, and v− satisﬁes (28b), then it can be seen by
inspection of the proof that

v−(α)2 ≤ EhkRαWk2i ≤ v+(α)2

∀α ≥ α > 0

Theorem 5.1(i) ⇒ sup

0<ε≤Θκ,v+ (α)

Theorem 5.1(i) ⇐ sup

0<ε≤Θκ,v− (α)

1

ψκ,v+(ε2)

1

ψκ,v−(ε2)

inf

0<α≤α

inf

0<α≤α

EhkRα(T f† + εW ) − f†k2
EhkRα(T f† + εW ) − f†k2

Xi < ∞,
Xi < ∞.

This is relevant for operators T with exponentially decaying singular values. Whereas
for polynomial decay assumption (28a) can be veriﬁed using results from [4], for singular

16

MAXISETS FOR SPECTRAL REGULARIZATION

values with asympototic behaviour σj(T ) ∼ exp(−cjβ) with c, β > 0 one can only (easily)
verify the relaxed condition (32) with
v−(α) = c−α−1/2

v+(α) = c+α−1/2−τ

and

for any τ > 0 and some c−, c+ > 0. However, for such operators (i) is typically satisﬁed only
for logarithmic functions κ(α) = (− ln α)−p with some p > 0 for f† of ﬁnite smoothness.
In this case one has

ψκ,v+(t) = (− ln t)−2p(1 + o(1)),

t → 0

independent of the choice of τ ∈ [0,∞) (see [15]). Therefore, the equivalence in Theorem
5.1 still holds true with either v = v− or v = v+.

6. Besov spaces as maxisets

We have seen in the previous sections that convergence rates of ψκ to a true solution f†
for regularization methods are completely characterized by f† ∈ XT
κ . Andreev [2] showed
that these spaces coincide with K-interpolation spaces with equivalent norms. Recall that
for a Banach space Z ⊂ X, which is continuously embedded in X the K-functional is deﬁned
by

Kt(f ) := inf

g∈Z(cid:16)kf − gk2

X + t2kgk2

.

Z(cid:17)1/2
kf : (X, Z)ν,∞k := sup

t>0

For ν ∈ (0, 1) the K-interpolation space with ﬁne index ∞ is deﬁned by
(X, Z)ν,∞ := {f ∈ X : kf : (X, Z)ν,∞k < ∞} where
Here we temporarily switch to a diﬀerent norm notation because of the numereous indices.
It can be shown that (X, Z)ν,∞ with this norm is a Banach space. If Z = (S∗S)k(X) for
a bounded linear operator S : X → Y and some k ∈ N with norm kfkZ := k(S∗S)−kfkX,
Andreev [2] showed that

t−νKt(f ).

(33)
for ν ∈ (0, 1) with

XS
idkν = (X, (S∗S)k(X))ν,∞

√1 − ν(cid:13)(cid:13)(cid:13)f : (X, (S∗S)k(X)ν,∞(cid:13)(cid:13)(cid:13) ≤(cid:13)(cid:13)(cid:13)f : XS

idkν(cid:13)(cid:13)(cid:13) ≤ (1−ν)1−ν νν(cid:13)(cid:13)(cid:13)f : (X, (S∗S)k(X)ν,∞(cid:13)(cid:13)(cid:13) .

We further recall that the K-interpolation of certain Sobolev spaces yields Besov spaces.

In particular,

(L2(M), H k(M))ν,∞ = Bkν

(34)
if M is a smooth Riemannian manifold with Laplace-Beltrami operator ∆ satisfying As-
sumption 6.1 below and H k(M) := (I − ∆)−k/2(L2(M)) with norm kfkH k := k(I −
∆)k/2fkL2 (see [21, Chapter 7]).
Assumption 6.1. Let M be a connected smooth Riemanian manifold. Let M

2,∞(M)

• be complete,
• have an injectivity radius r > 0 and
• a bounded geometry.

MAXISETS FOR SPECTRAL REGULARIZATION

17

Here completeness means that all geodesics are inﬁnitely extendable, the injectivity
radius refers to the size of the domains in which the exponential map is bijective, and
bounded geometry means that the determinant of the Riemannian metric is bounded from
below by a positive constant and all its derivatives are bounded from above (see [21]
for further discussions). Important examples of such manifolds include Rn and compact
manifolds without boundaries.

In the following we will consider operators T : X = L2(M) → Y such that

(35)

T ∗T = Λ(−∆),

where Λ fulﬁlls the following conditions:

Assumption 6.2. Let Λ : [0,∞) → (0,∞) such that

• Λ is continuous,
• Λ|[t0,∞) is strictly decreasing for some t0 ≥ 0,
• Λ(µ) → 0 for µ → ∞.

Our aim of this section is to prove the following theorem:

Theorem 6.3. Let M fulﬁll Assumption 6.1, Λ fulﬁll Assumption 6.2 and s > 0. Let
T : L2(M) → Y be of the form (35) and deﬁne
(cid:16)Λ|[t0,∞)−1(α)(cid:17)−1/2
(M) with equivalent norms.

if α = 0
if α ∈ ((0, Λ(t0)]),
if α > Λ(t0).

κ(α) :=

κs = Bs

Then XT

t−1/2
0

2,∞

0,

,

,

Proof. We introduce the operator S := κ(T ∗T )1/2 : L2(M) → L2(M)) such that

S∗S = κ(T ∗T ) = (κ ◦ Λ)(−∆).

As (κ ◦ Λ)(t) = t−1/2 for t ≥ t0 and inf 0≤t≤t0(κ ◦ Λ)(t) > 0 by continuity, we have
(36)
with equivalent norms for all k ∈ N. Using the substitution t = κ(α) we obtain

(S∗S)k(L2(M)) = H k(M)

t

1

As ET ∗T

0<t≤1/t0
(f ) = ET ∗T

f(cid:13)(cid:13)(cid:13) = sup

f(cid:13)(cid:13)(cid:13) .
ids(cid:13)(cid:13)(cid:13) = sup
(cid:13)(cid:13)(cid:13)f : XS
ids(cid:13)(cid:13)(cid:13) and
(cid:13)(cid:13)(cid:13)f : XT
κs(cid:13)(cid:13)(cid:13) are equivalent. Choosing k ∈ N with k > s and using (33), (36) and (34) we

κ(α)s(cid:13)(cid:13)(cid:13)ET ∗T
t−s(cid:13)(cid:13)(cid:13)ES∗S
Λ(0)(f ) = f for α > Λ(0), this shows that the norms (cid:13)(cid:13)(cid:13)f : XS

κ(α)f(cid:13)(cid:13)(cid:13) = sup

κ(α)s(cid:13)(cid:13)(cid:13)ES∗S

α∈(0,Λ(t0)]

α∈(0,Λ(t0)]

obtain

1

α

α

with equivalent norms.

(cid:3)

XT

κs = XS

ids = (L2(M), (S∗S)k(M))s/k,∞ = Bs
2,∞

(M)

18

MAXISETS FOR SPECTRAL REGULARIZATION

7. Examples

In this section we want to apply our results to some examples. The examples are taken

from [9] and complement the results there.

7.1. Operators in Sobolev scales. In the following we describe a fairly general class
It contains convolution operators (if M = Rd or M = (S1)d), for which
of problems.
the convolution kernel has a certain type of singularity at 0, boundary integral operators,
injective elliptic pseudo-diﬀerential operators, and compositions of such operators.
Theorem 7.1. Let M be a d-dimensional manifold satisfying Assumption 6.1, and let T be
an operator which is a times smoothing (a > d/2) in the sense that T : H s(M) → H s+a(M)
is well-deﬁned, bounded and has a bounded inverse for all s ∈ R. We will consider T
as an operator from L2(M) into itself, i.e. X = Y = L2(M). We consider a spectral
regularization method with classical qualiﬁcation µ0 ≥ 1 satisfying Assumption 3.2. Then
the following statements are equivalent for all f† ∈ X \ {0} and u ∈ (0, a):

(i) f† satisﬁes a the VSC (3) with ψ(t) = Ct
(ii) f† ∈ Bu
2,∞
(iii) For a quasioptimal parameter choice rule α∗ and a regularization method for which

u+a for some C > 0.

(M).

u

∆(f†) satisﬁes (22) we have

sup{(cid:13)(cid:13)(cid:13)Rα∗(δ,gobs)gobs − f†(cid:13)(cid:13)(cid:13)L2 :(cid:13)(cid:13)(cid:13)gobs − T f†(cid:13)(cid:13)(cid:13)L2 ≤ δ} = O(cid:16)δ
(iv) (cid:18)inf α>0 E(cid:20)(cid:13)(cid:13)(cid:13)Rα(T f† + εW ) − f†(cid:13)(cid:13)(cid:13)
u+a+d/2(cid:17) ,

= O(cid:16)ε

L2(cid:21)(cid:19)1/2

2

u

u

u+a(cid:17) ,

(ii)–(iv) are equivalent for all u ∈ (0, 2aµ0), and the assumption a > d/2 can be relaxed to
a > 0 if (iv) is neglected.
Proof. (i) ⇔ f† ∈ XT

κ : Note that ψ = ψκ with

δ → 0.

ε → 0.

κ(t) = C′tu/2a

for some C′ > 0,

and that the assumption u ∈ (0, a) ensures that κ satisﬁes the conditions of Theorem 3.1.

f† ∈ XT
XT
f† ∈ XT

κ ⇔ (ii): It follows from (33) and (34) that
κ = (L2(M), (T ∗T )(L2(M)))u/2a,∞ = (L2(M), H 2a(M))u/2a,∞ = Bu
2,∞
κ ⇔ (37) below: For u/2a < µ0 Theorem 3.3 yields equivalence to

(M).

(37)

sup
α>0

α−u/2akrα(T ∗T )f†k < ∞.

(37) ⇔ (iii): This follows from Theorem 4.3.
(37) ⇔ (iv): It has been shown in [4, §5.3] that (28a) holds true with v(α) = α−(a+d/2)/(2a).

Hence, we can apply Theorem 5.1.
Example 7.2. We consider a circle M = rS1 ⊂ R2 with r > 0 and the single layer potential
πRM
operator (T f )(x) := − 1
ln |x−y|f (y) ds(y). Let fn(r cos t, r sin t) := (2πr)−1/2 exp(int),
n ∈ Z denote the trigonometric basis of L2(rS).
It is known (see [14, Sec. 3.3]) that
T fn = −1/|n|fn for n 6= 0, and T f0 = ln(r)f0. Let us choose r = exp(1) for simplicity.

(cid:3)

MAXISETS FOR SPECTRAL REGULARIZATION

19

H s = Pn∈Z(1 ∨ |n|)2shf, fni2
Recall that an (equivalent) norm on H s(M) is given by kfk2
for s ≥ 0. W.r.t. this norm T ν = (T ∗T )u/2 is isometric from H s(M) to H s+u(M) for
all u > 0, so the assumptions of Theorem 7.1 hold true with a = d = 1. Moreover, the
spectral source condition f† ∈ ran((T ∗T )u/2 is equivalent to f† ∈ H u(M) and yields the
convergence rate O(cid:16)δu/(u+1)(cid:17). The (equivalent) XT
(M) (with κ(t) = tu) is
= supm≥0(1 ∨ m)2uP|n|≥mhf†, fni2. This shows that Bu
given by kf†k2
2,∞(M) is the
set of f ∈ L2(M) for which the L2-orthogonal projections onto the space of trigonometric
polynomials of degree ≤ m converge with rate O(m−u) as m → ∞. Note that

κ -norm of Bu

2,∞

Bu

2,∞

f† = Xn∈Z

(1 ∨ |n|)−ufn ∈ Bu
2,∞

(M) \ H u(M)

for any u > 0, but f† ∈ H ν(M) for ν < u. Therefore, we obtain the convergence rate
O(δu/(u+1)) for f†, whereas an analysis via spectral source conditions only yields rates
O(δν/(ν+1)) for ν ∈ (0, u). Moreover, as limνրu kf†kH ν = ∞, constants explode as ν → u.
7.2. Backward heat equation. Let us consider the heat equation on a manifold M
satisfying Assumption 6.1:

∂tu = ∆u
u(·, 0) = f

in M × (0, t)
on M

The backward heat equation is the inverse problem to estimate the initial temperature f
from observations of the ﬁnal temperature g = u(·, t). This ﬁts into the framework (35)
with the function

ΛBH(µ) = exp(−2tµ).

We obtain the following equivalence result:
Theorem 7.3. Let M be a compact manifold satisfying Assumption 6.1. For spectral
regularization methods satisfying Assumption (3.2) and the forward operator T : L2(M) →
L2(M) with T ∗T = ΛBH(−∆) of the backward heat equation the following statements for
β > 0 and f† ∈ L2(M) \ {0} are equivalent:

(i) f† ∈ B2β
(ii) f† satisﬁes a VSC (3) with index function ψ(t) = C log(3+t−1)−2β for some C > 0.
(iii) For a quasioptimal parameter choice rule α∗ and a regularization method for which

2,∞(M).

∆(f†) satisﬁes (22) we have

sup{(cid:13)(cid:13)(cid:13)Rα∗(δ,gobs)gobs − f†(cid:13)(cid:13)(cid:13)L2 :(cid:13)(cid:13)(cid:13)gobs − T f†(cid:13)(cid:13)(cid:13)L2 ≤ δ} = O(cid:16)log(δ−1)−β(cid:17) ,
(iv) (cid:18)inf α>0 E(cid:20)(cid:13)(cid:13)(cid:13)Rα(T f† + εW ) − f†(cid:13)(cid:13)(cid:13)

= O(cid:16)log(ε−1)−β(cid:17) ,
Proof. (i) ⇔ f† ∈ XT
with κ(α) = ((1/2t) ln(α−1))−1/2 for 0 < α ≤ ΛBH(t0) and any t0 > 0.

κ2β : By Theorem 6.3 we have f† ∈ B2β

L2(cid:21)(cid:19)1/2

2

f† ∈ XT

κ2β ⇔ (ii): This follows from Theorem 3.1 since
ψκ2β (t) = C log(t−1)−2β (1 + o(1)) ,

(38)

as t → 0

δ → 0.

ε → 0.

2,∞(M) if and only if f† ∈ XT

κ2β

20

MAXISETS FOR SPECTRAL REGULARIZATION

as shown in [15]. The 3 is included in the deﬁnition of ψ to avoid a singularity at t = 1.

f† ∈ XT
f† ∈ XT

κ2β ⇔ (iii): Follows from Theorems 3.3 and 4.3.
κ2β ⇔ (iv): Use the results of [4, §5.1] to see that (32) is fulﬁlled for any τ > 0

and apply Remark 5.2 and Theorem 3.3.

(cid:3)

7.3. Sideways heat equation. We now consider the heat equation in the interval [0, 1].
We may think of [0, 1] as the wall of a furnace where the right boundary 1 is the inaccessible
interior side and 0 the accessible outer side. We assume the left boundary is insulated and
impose the no-ﬂux boundary condition ∂xu(0, t) = 0. The forward problem reads

ut = uxx
u(1, t) = f (t),
ux(0, t) = 0,

in [0, 1] × R,
t ∈ R,
t ∈ R.

We will consider the inverse problem to estimate the temperature f (t) = u(1, t) at the
inaccessible side from measurements of the temperature g(t) = u(0, t) at the accessible
side for all times t ∈ R. As shown in [9] this ﬁts into the framework (35) if we set

ΛSH(µ) =(cid:12)(cid:12)(cid:12)(cid:12)coshqi√µ(cid:12)(cid:12)(cid:12)(cid:12)

−2

, M = R.

Theorem 7.4. For spectral regularization methods satisfying Assumption (3.2) and the
forward operator T : L2(R) → L2(R) such that T ∗T = ΛSH(−∆) of the sideways heat
equation the following statements for β > 0 and f† ∈ L2(R) \ {0} are equivalent:

(i) f† ∈ Bβ/2
(ii) f† satisﬁes a VSC (3) with index function ψ(t) = C log(3+t−1)−2β for some C > 0.
(iii) For a quasioptimal parameter choice rule α∗ and a regularization method for which

2,∞(R).

∆(f†) satisﬁes (22) we have

sup{(cid:13)(cid:13)(cid:13)Rα∗(δ,gobs)gobs − f†(cid:13)(cid:13)(cid:13)L2 :(cid:13)(cid:13)(cid:13)gobs − T f†(cid:13)(cid:13)(cid:13)L2 ≤ δ} = O(cid:16)log(δ−1)−β(cid:17) ,
(iv) (cid:18)inf α>0 E(cid:20)(cid:13)(cid:13)(cid:13)Rα(T f† + εW ) − f†(cid:13)(cid:13)(cid:13)

= O(cid:16)log(ε−1)−β(cid:17) ,
κβ/2: As shown in [9] ΛSH(µ) = (1/4) exp(−√2µ1/4)(1 + o(µ)) as
Proof. (i) ⇔ f† ∈ XT
µ → ∞. Therefore we obtain κ(α) = 2 ln(α−1)−2(1 + o(α)) as α → 0.
κβ/2 ⇔ (ii) ⇔ (iii) ⇔ (iv): This follows as in proof of Theorem 7.3. Due to the dif-
ferent exponent in the asymptotic formula for κ we have ψκβ/2(t) = C log(t−1)−2β (1 + o(1))
here instead of (38).
(cid:3)

L2(cid:21)(cid:19)1/2

f† ∈ XT

δ → 0.

ε → 0.

2

7.4. Satellite gradiometry. Let us assume that the Earth is a perfect ball of radius 1.
The gravitational potential u of the Earth is determined by its values f at the surface by
the exterior boundary value problem

∆u = 0
|u| → 0,

in {x ∈ R3 : |x| > 1}
|x| → ∞

MAXISETS FOR SPECTRAL REGULARIZATION

21

u = f

on S2

In satellite gradiometry one studies the inverse problem to determine f from satellite
measurements of the rate of change of the gravitational force in radial direction at height
R > 0, i.e. the data are described by the function g = ∂2u
∂r2 |RS2. As shown in [9] this ﬁts
into the framework (35) if we set

ΛSG(µ) :=(cid:18)1

2

+ λ(cid:19)2(cid:18)3

2

+ λ(cid:19)2

R−2λ,

λ =s 1

2

+ µ, M = S2.

Note that ΛSG (unlike ΛBH and ΛSH) is not globally monotonically decreasing unless R is

large enough (one needs R ≥ exp((4√2 + 2)/(√2 + 5)) ≈ 3.3, which is not realistic).
Theorem 7.5. For spectral regularization methods satisfying Assumption 3.2 and the for-
ward operator T : L2(S2) → L2(S2) such that T ∗T = ΛSG(−∆) with R large enough such
that ΛSG fulﬁlls Assumption 6.2 of the satellite gradiometry problem the following state-
ments for β > 0 and f† ∈ L2(S2) \ {0} are equivalent:

(i) f† ∈ Bβ
(ii) f† satisﬁes a VSC (3) with index function ψ(t) = C log(3+t−1)−2β for some C > 0.
(iii) For a quasioptimal parameter choice rule α∗ and a regularization method for which

2,∞(S2).

∆(f†) satisﬁes (22) we have

sup{(cid:13)(cid:13)(cid:13)Rα∗(δ,gobs)gobs − f†(cid:13)(cid:13)(cid:13)L2 :(cid:13)(cid:13)(cid:13)gobs − T f†(cid:13)(cid:13)(cid:13)L2 ≤ δ} = O(cid:16)log(δ−1)−β(cid:17) ,
(iv) (cid:18)inf α>0 E(cid:20)(cid:13)(cid:13)(cid:13)Rα(T f† + εW ) − f†(cid:13)(cid:13)(cid:13)

= O(cid:16)log(ε−1)−β(cid:17) ,

L2(cid:21)(cid:19)1/2

2

κ : Theorem 6.3 shows that f† ∈ Bβ

Proof. (i) ⇔ f† ∈ XT
κ where
κ(α) = 2 ln(R)(ln(α−1))−1(1 + o(1)) as α → 0 since ΛSG(µ) = exp(−2 ln(R)µ1/2)(1 + o(1))
as µ → ∞.
f† ∈ XT
κ ⇔ (ii) ⇔ (iii) ⇔ (iv): This follows again along the line of the proof of Theorem
7.3. Here ψκβ (t) = C log(t−1)−2β (1 + o(1)) as t → 0.
8. Conclusions

2,∞(S2) if and only if f† ∈ XT

(cid:3)

δ → 0.

ε → 0.

We have described a general strategy for the veriﬁcation of VSCs. For linear operators in
Hilbert spaces we have shown via a series of equivalence theorems that VSCs are necessary
and suﬃcient for certain rates of convergence both for deterministic errors and for white
noise. For a number of relevant inverse problems VSCs with certain index functions are
satisﬁed if and only if the solution belongs to some Besov space.

For other forward operators the set of solutions which satisﬁes a VSC with a (multiple of
a) given index function will not be any known function space. Nevertheless it is interesting
to derive veriﬁable suﬃcient conditions for VSCs and rates of convergence also for such
operators, and we intend to explore the potential of our general strategy in such situations
in future research.

Furthermore, our strategy for the veriﬁcation of VSCs has straightforward extensions to
Banach spaces, general data ﬁdelity and penalty functionals, and it has already successfully

22

MAXISETS FOR SPECTRAL REGULARIZATION

been applied to nonlinear inverse scattering problems. These extensions will be an inter-
esting topic of future research. Although VSC are known to be suﬃcient for certain rates
of convergence in such general situations, little is known about necessity so far. However,
we expect that diﬀerent techniques than those applied in this paper will be required for
such converse results.

Acknowledgement

Financial support by the German Research Foundation DFG through CRC 755, project

C09 is gratefully acknowledged.

Appendix A. Spectral source conditions

In this appendix we will use the general strategy of §2 to derive variational source
conditions from spectral source conditions. Compared to the implication (ii) ⇒ (i) in
Theorem 3.1, we can relax the assumption that t 7→ κ(t)2/t1−µ is decreasing for some
µ ∈ (0, 1) by allowing also µ = 0. Moreover, the proof for spectral source conditions is
considerably simpler.
The result has been known in principle, but previous derivations have been indirect via
distance functions, did not yield explicit control over constants, and already for logarithmic
source conditions involved quite heavy computations (see [6]).
Proposition A.1. If T is linear, Y is a Hilbert space, and f† satisﬁes a spectral source
condition

with an index function ϕ such that ϕ2 is concave, then f† satisﬁes the variational source
condition (3) with

f† = ϕ(T ∗T )w,

kwk ≤ ρ

(39)

ψ(δ2) = 4ρ2ϕ Θ−1  δ

ρ!!2

,

Θ(t) := √tϕ(t).

Proof. Let Er = 1[0,r](T ∗T ) denote the spectral family generated by the operator T ∗T and
set Pr := I − Er for r > 0. Then

k(I − Pr)f†k2 = kE(r)ϕ(T ∗T )wk2 =Z r

0

ϕ(t)2 dkEtwk2 ≤ ϕ(r)2ρ2.

ψ(δ) = 2 inf

r>0"ρ2ϕ(r)2 +

ϕ(r)
√r

ρδ# = 2ρ2 inf

r>0"ϕ(r)2 +

ϕ(r)
√r

δ

ρ# .

Therefore, (4) holds true with κ(r) = ρϕ(r). Moreover,

Df†, Pr(f† − f )E =Dw, Prϕ(T ∗T )(f† − f )E ≤ ρ(cid:18)Z ∞

r

tdkEt(f† − f )k2!1/2
where supt≥r ϕ(t)2t = ϕ(r)2/r since ϕ2 is concave and ϕ(0) = 0. Hence, (5) holds true with
σ(r) = ρϕ(r)/√r and C = 0. Therefore, (3) holds true with

≤ ρ sup

Z ∞

ϕ(t)2

t≥r

t

ρ

ϕ(t)2 dkEt(f† − f )k2(cid:19)1/2
ϕ(r)
√r kT (f† − f )k

≤ ρ

MAXISETS FOR SPECTRAL REGULARIZATION

23

We choose r = Θ−1(δ/ρ), i.e. √rϕ(r) = δ/ρ. Then ϕ(r)√r
obtain (39).

δ

ρ = ϕ(r)2 = ϕ(Θ−1(δ/ρ))2, so we
(cid:3)

References

[1] Vinicius Albani, Peter Elbau, Maarten V. de Hoop, and Otmar Scherzer, Optimal convergence rates

results for linear inverse problems in Hilbert spaces (November 2015), available at 1511.02950.

[2] Roman Andreev, Tikhonov and Landweber convergence rates: characterization by interpolation spaces,

Inverse Problems 31 (2015Sep), no. 10, 105007.

[3] Roman Andreev, Peter Elbau, Maarten V. de Hoop, Lingyun Qiu, and Otmar Scherzer, General-
ized Convergence Rates Results for Linear Inverse Problems in Hilbert Spaces, Numerical Functional
Analysis and Optimization 36 (2015), no. 5, 549–566.

[4] Nicolai Bissantz, Thorsten Hohage, Axel Munk, and Frits Ruymgaart, Convergence rates of general
regularization methods for statistical inverse problems and applications, SIAM J. Numer. Anal. 45
(2007), 2610–2636.

[5] Heinz W. Engl, Martin Hanke, and Andreas Neubauer, Regularization of inverse problems, Mathe-

matics and its Applications, vol. 375, Kluwer Academic Publishers Group, Dordrecht, 1996.

[6] Jens Flemming, Generalized Tikhonov regularization and modern convergence rate theory in Banach

spaces, Shaker, 2012.

[7] Jens Flemming, Bernd Hofmann, and Peter Math´e, Sharp converse results for the regularization error

using distance functions, Inverse Problems 27 (2011), no. 2, 025006, 18.

[8] Bernd Hofmann, Barbara Kaltenbacher, Christiane P¨oschl, and Otmar Scherzer, A convergence rates
result for Tikhonov regularization in Banach spaces with non-smooth operators, Inverse Problems 23
(2007), no. 3, 987–1010.

[9] Thorsten Hohage, Regularization of exponentially ill-posed problems, Numer. Funct. Anal. Optim. 21

(2000), 439–464.

[10] Thorsten Hohage and Frederic Weidling, Veriﬁcation of a variational source condition for acoustic

inverse medium scattering problems, Inverse Problems 31 (2015), no. 7, 075006, 14.

[11] G´erard Kerkyacharian and Dominique Picard, Density estimation by kernel and wavelets methods:

optimality of Besov spaces, Statist. Probab. Lett. 18 (1993), no. 4, 327–336.

[12]
[13]
[14] Andreas Kirsch, An introduction to the mathematical theory of inverse problems, Springer, New York,

, Thresholding algorithms, maxisets and well-concentrated bases, Test 9 (2000), no. 2, 283–344.
, Minimax or maxisets?, Bernoulli 8 (2002), no. 2, 219–253.

Berlin, Heidelberg, 1996.

[15] Bernard A. Mair, Tikhonov regularization for ﬁnitely and inﬁnitely smoothing operators, SIAM J.

Math. Anal. 25 (1994), 135–147.

[16] Peter Math´e and Sergei V. Pereverzev, Geometry of ill-posed problems in variable Hilbert scales,

Inverse Problems 19 (2003), 789–803.

[17] Andreas Neubauer, On converse and saturation results for Tikhonov regularization of linear ill-posed

problems, SIAM journal on numerical analysis 34 (1997), no. 2, 517–527.

[18] Toomas Raus, The principle of the residual in the solution of ill-posed problems with nonselfadjoint

operator, Tartu Riikl. ¨Ul. Toimetised 715 (1985), 12–20.

[19] Toomas Raus and Uno H¨amarik, On the quasioptimal regularization parameter choices for solving

ill-posed problems, J. Inverse Ill-Posed Probl. 15 (2007), no. 4, 419–439.

[20] Vincent Rivoirard, Maxisets for linear procedures, Statist. Probab. Lett. 67 (2004), no. 3, 267–275.
[21] Hans Triebel, Theory of Function Spaces II, Birkh¨auser, Basel, 1992.
[22] Frederic Weidling and Thorsten Hohage, Variational source conditions and stability estimates for

inverse electromagnetic medium scattering problems (December 2015), available at 1512.06586.

