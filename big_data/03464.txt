6
1
0
2

 

b
e
F
 
7
2

 
 
]
T
I
.
s
c
[
 
 

1
v
4
6
4
3
0

.

3
0
6
1
:
v
i
X
r
a

Recovery of signals under the high order RIP condition via prior

support information

Wengu Chen and Yaling Li ∗†‡

March 14, 2016

Abstract

In this paper we study the recovery conditions of weighted l1 minimization for signal re-
construction from incomplete linear measurements when partial prior support information is
available. We obtain that a high order RIP condition can guarantee stable and robust recovery
of signals in bounded l2 and Dantzig selector noise settings. Meanwhile, we not only prove
that the suﬃcient recovery condition of weighted l1 minimization method is weaker than that
of standard l1 minimization method, but also prove that weighted l1 minimization method pro-
vides better upper bounds on the reconstruction error in terms of the measurement noise and
the compressibility of the signal, provided that the accuracy of prior support estimate is at least
50%. Furthermore, the condition is proved sharp.

Keywords: Compressed sensing, restricted isometry property, weighted l1 minimization.

1

Introduction

Compressed sensing is a new type of sampling theory that admits that high dimensional sparse
signals can be reconstructed through fewer measurements than their ambient dimension. The
central goal in compressed sensing is to recover a signal x ∈ RN based on A and y from the
following model:

y = Ax + z

(1)

∗W. Chen is with Institute of Applied Physics and Computational Mathematics, Beijing, 100088, China, e-mail:

chenwg@iapcm.ac.cn.

†Y. Li

is with Graduate School, China Academy of Engineering Physics, Beijing, 100088, China, e-

mail:leeyaling@126.com.

‡This work was supported by the NSF of China (Nos.11271050, 11371183) and Beijing Center for Mathematics

and Information Interdisciplinary Sciences (BCMIIS).

1

where sensing matrix A ∈ Rn×N with n ≪ N , i.e., using very few measurements, y ∈ Rn is
a vector of measurements, and z ∈ Rn is the measurement error.
In past decade, compressed
sensing has triggered considerable research in a number of ﬁelds including applied mathematics,
statistics, electrical engineering, seismology and signal processing. Compressed sensing is especially
promising in applications where taking measurements is costly, e.g., hyperspectral imaging [11], as
well as in applications where the ambient dimension of the signal is very large, e.g., medical image
reconstruction [18], DNA microarrays [20], radar system [1, 13, 23].

To reconstruct the signal x from (1), Cand`es and Tao [10] proposed the following constrained

l1 minimization method:

minimize

x∈RN

kxk1

subject

to ky − Axk2 ≤ ǫ.

(2)

It is well known that l1 minimization is a convex relaxation of l0 minimization and is polynomial-
time solvable. And it has been shown that l1 minimization is an eﬀective way to recover sparse
signals in many settings [3–9, 19]. Cai and Zhang [6, 7] established sharp restricted isometry
conditions to achieve the exact and stable recovery of signals in both noiseless and noisy cases via
l1 minimization method.

Note that compressed sensing is a nonadaptive data acquisition technique and l1 minimization
method (2) is itself nonadaptive because no prior information on the signal x is used. In practical
examples, however, the estimate of the support of the signal or of its largest coeﬃcients may
be possible to be drawn. For example, support estimation of the previous time instant may be
applied to recover time sequences of sparse signals iteratively. Incorporating prior information is
very useful for recovering signals from compressive measurements. Thus, the following weighted
l1 minimization method which incorporates partial support information of the signals has been
introduced to replace standard l1 minimization

minimize

x∈RN

kxk1,w subject

to

ky − Axk2 ≤ ǫ,

(3)

where w ∈ [0, 1]N and kxk1,w = Pi

wi|xi|. Reconstructing compressively sampled signals with
partially known support has been previously studied in the literature; see [2, 12, 14–17, 21]. Borries,
Miosso and Potes in [2], Khajehnejad et al. in [15], and Vaswani and Lu in [21] introduced the
problem of signal recovery with partially known support independently. The works by Borries
et al. in [2], Vaswani and Lu in [16, 21, 22] and Jacques in [14] incorporated known support
information using weighted l1 minimization approach with zero weights on the known support,

[12] extended weighted l1 minimization approach to nonzero weights. They allow the weights

namely, given a support estimate eT ⊂ {1, 2, . . . , N} of unknown signal x, setting wi = 0 whenever
i ∈ eT and wi = 1 otherwise, and derived suﬃcient recovery conditions. Friedlander et al. in
wi = ω ∈ [0, 1] if i ∈ eT . Since Friedlander et al. incorporated the prior support information

2

and consider the accuracy of the support estimate, they derived the stable and robust recovery
guarantees for weighted l1 minimization which generalize the results of Cand`es, Romberg and Tao in
[9] stated below. Friedlander et al. [12] pointed out that once at least 50% of the support information
is accurate, the weighted l1 minimization method (3) can stably and robustly recover any signals
under weaker suﬃcient conditions than the analogous conditions for standard l1 minimization
method (2). In addition, the weighted l1 minimization method (3) gives better upper bounds on
the reconstruction error. Furthermore, they also pointed out suﬃcient conditions are weaker than
those of [21] when ω = 0.

To recover sparse signals via constrained l1 minimization, Cand`es and Tao [10] introduced the
notion of Restricted Isometry Property (RIP), which is one of the most commonly used frameworks
for compressive sensing. The deﬁnition of RIP is as follows.

Deﬁnition 1.1. Let A ∈ Rn×N be a matrix and 1 ≤ k ≤ N is an integer. The restricted isometry
constant (RIC) δk of order k is deﬁned as the smallest nonnegative constant that satisﬁes

(1 − δk)kxk2

2 ≤ kAxk2

2 ≤ (1 + δk)kxk2
2,

for all k−sparse vectors x ∈ RN . A vector x ∈ RN is k−sparse if |supp(x)| ≤ k, where supp(x) =
{i : xi 6= 0} is the support of x. When k is not an integer, we deﬁne δk as δ⌈k⌉, where ⌈k⌉ denotes
the smallest integer strictly bigger than k.

Cand`es, Romberg and Tao [9] showed that the condition δak + aδ(a+1)k < a− 1 with a ∈ 1
δtk <q t−1
(2). Furthermore, Cai and Zhang proved that for any ǫ > 0, δtk <q t−1

Z and
a > 1 is suﬃcient for stable and robust recovery of all signals using l1 minimization method (2). Cai
and Zhang [6] improved the result of Cand`es, Romberg and Tao [9] and proved that the condition
t with t ≥ 4/3 can guarantee the exact recovery of all k−sparse signals in the noiseless
case and stable recovery of approximately sparse signals in the noise case by l1 minimization method
t +ǫ fails to ensure the exact
reconstruction of all k−sparse signals and stable reconstruction of approximately sparse signals for
large k.

k

In [6], Cai and Zhang use the following l1 minimization

minimize

x∈RN

kxk1

subject

to

ky − Axk2 ∈ B,

(4)

where B is a bounded set determined by the noise structure, and B is especially taken to be {0} in
the noiseless case. They consider two types of noise settings

and

Bl2(ε) = {z : kzk2 ≤ ε}

BDS(ε) = {z : kAT zk∞ ≤ ε}.

3

(5)

(6)

In this paper, we adopt the corresponding weighted l1 minimization method:

minimize

kxk1,w subject

x∈RN

with wi =( 1,

ω,

to y − Ax ∈ B

(7)

i ∈ eT c
i ∈ eT .

where 0 ≤ ω ≤ 1 and eT ⊂ {1, 2, . . . , N} a given support estimate of unknown signal x. B is also a

bounded set determined by the noise settings (5) and (6). Our goal is to generalize the results of
Cai and Zhang [6] via the weighted l1 minimization method (7). We establish the high order RIP
condition for the stable and robust recovery of signals with partially known support information
from (1). We also show that the recovery by weighted l1 minimization method (7) is stable and
robust under weaker suﬃcient conditions compared to the standard l1 minimization method (4)
when we have the partial support information with accuracy better than 50%.

The rest of the paper is organized as follows. In Section 2, we will introduce some notations
and some basic lemmas that will be used. The main results are given in Section 3, and the proofs
of our main results are presented in Section 4.

2 Preliminaries

Let us begin with basic notations. For arbitrary x ∈ RN , let xk be its best k−term approximation.
xmax(k) is deﬁned as x with all but the largest k entries in absolute value set to zero, and x− max(k) =
x − xmax(k). Let T0 be the support of xk, i.e., T0 = supp(xk), with T0 ⊆ {1, . . . , N} and |T0| ≤ k.

the size of the estimated support to the size of the actual support of xk (or the support of x if x is

Let eT ⊆ {1, . . . , N} be the support estimate of x with |eT| = ρk, where ρ ≥ 0 represents the ratio of
0 ∩ eT with |eTα| = α|eT| = αρk and |eTβ| = β|eT| = βρk,
k− sparse). Denote eTα = T0 ∩ eT and eTβ = T c
where α denotes the ratio of the number of indices in T0 that were accurately estimated in eT to
the size of eT and α + β = 1. For arbitrary nonnegative number ξ, we denote by [[ξ]] an integer

Cai and Zhang developed a new elementary technique which is a key technical tool for the proof
of the main result (see Theorem 3.1). It states that any point in a polytope can be represented as
a convex combination of sparse vectors ([6], Lemma 1.1). Another key technical tool for our proof
was Lemma 2.2 introduced by Cai and Zhang ([8], Lemma 5.3). The speciﬁc contents are presented
in Lemmas 2.1 and 2.2, respectively.

satisfying ξ ≤ [[ξ]] < ξ + 1.

Lemma 2.1 ([6], Lemma 1.1). For a positive number α and a positive integer k, deﬁne the polytope
T (α, k) ⊂ Rd by

T (α, k) = {v ∈ Rd : kvk∞ ≤ α,kvk1 ≤ kα}.

4

For any v ∈ Rd, deﬁne the set of sparse vectors U (α, k, v) ⊂ Rd by

U (α, k, v) = {u ∈ Rd : supp(u) ⊆ supp(v),kuk0 ≤ k,

kuk1 = kvk1,kuk∞ ≤ α},

where kuk0 = |supp(u)|. Then any v ∈ T (α, k) can be expressed as

where ui ∈ U (α, k, v) and 0 ≤ λi ≤ 1,

v =

NXi=1

λiui,

λi = 1.

NPi=1

Lemma 2.2 ([8], Lemma 5.3). Assume m ≥ k, a1 ≥ a2 ≥ ··· ≥ am ≥ 0,
all α ≥ 1,

mXj=k+1

aα
j ≤

aα
i .

kXi=1

ai ≥

kPi=1

mPi=k+1

ai, then for

More generally, assume a1 ≥ a2 ≥ ··· ≥ am ≥ 0, λ ≥ 0 and

ai + λ ≥

j ≤ k(cid:16) αsPk

aα

i=1 aα
i
k

mXj=k+1

kPi=1
k(cid:17)α

λ

.

+

mPi=k+1

ai, then for all α ≥ 1,

As we mentioned in the introduction, Cai and Zhang [6] provided the sharp suﬃcient condition
to recover sparse signals and approximately sparse signals via l1 minimization (4). Their main
result can be stated as below.

Theorem 2.1 ([6], Theorem 2.1). Let y = Ax + z with kzk2 ≤ ε and bxl2 is the minimizer of (4)

with B = Bl2(η) = {z : kzk2 ≤ η} for some η ≥ ε. If
δtk <r t − 1

t

(8)

(9)

,

for some t ≥ 4/3, then

where

2kx− max (k)k1

√k

kbxl2 − xk2 ≤ C0(ε + η) + C1
C0 = p2t(t − 1)(1 + δtk)
t(p(t − 1)/t − δtk)
√2δtk +qt(p(t − 1)/t − δtk)δtk

C1 =

t(p(t − 1)/t − δtk)

,

5

+ 1.

(10)

Let y = Ax + z with kAT zk∞ ≤ ε and bxDS is the minimizer of (4) with B = BDS(η) = {z :
kAT zk∞ ≤ η} for some η ≥ ε. If δtk <q t−1

t

for some t ≥ 4/3, then
2kx− max (k)k1

√k

,

(11)

(12)

, C′1 = C1.

kbxDS − xk2 ≤ C′0(ε + η) + C′1
C′0 = p2t2(t − 1)k
t(p(t − 1)/t − δtk)

where

sharp.

Note that Theorem 2.1 always holds for t > 1, and the condition t ≥ 4/3 ensures that (8) is

Friedlander et al. [12] used the prior support information to recover any signals by weighted l1

minimization (7). The following theorem was showed in [12].
Theorem 2.2 ([12], Theorem 3). Let x ∈ RN be an arbitrary signal and y = Ax + z with kzk2 ≤ ε.
Deﬁne xk be its best k−term approximation with supp{xk} = T0. Let eT ⊆ {1, . . . , N} be an
arbitrary set and deﬁne ρ ≥ 0 and 0 ≤ α ≤ 1 such that |eT| = ρk and |eT ∩ T0| = αρk. Suppose
that there exists an a ∈ 1
satisfying

Z with a ≥ (1 − α)ρ and a > 1. If the measurement matrix A has RIP

k

δak +

a
γ2 δ(a+1)k <

a
γ2 − 1,

where γ = ω + (1 − ω)√1 + ρ − 2αρ for some given 0 ≤ ω ≤ 1. Then the solution bx to (7) with (5)

obeys

0k1(cid:17)
2(cid:16)ωkx − xkk1 + (1 − ω)kxeT c∩T c

√k

,

(14)

where

kbx − xk2 ≤ C′′0 (2ε) + C′′1

where γ = ω + (1 − ω)√1 + ρ − 2αρ, then Theorem 2.2 holds with same constants. If

then weighted l1 minimization (7) with (5) can stably and robustly recover the original signal.

(13)

(15)

(16)

(17)

Remark 2.1 ([12], Remarks 3.3 and 3.4). If A satisﬁes

C′′0 =

C′′1 =

1 + γ√a

,

√1 + δak

p1 − δ(a+1)k − γ√a
a−1/2(cid:0)p1 − δ(a+1)k + √1 + δak(cid:1)
√1 + δak
p1 − δ(a+1)k − γ√a
a − γ2
a + γ2 ,

δ(a+1)k < δω

a :=

.

δ2k <(cid:16)√2γ + 1(cid:17)−1

,

6

3 Main results

Theorem 3.1. Suppose that x ∈ RN be an arbitrary signal and xk be its best k−term approximation
supported on T0 ⊆ {1, . . . , N} with |T0| ≤ k. Let eT ⊆ {1, . . . , N} be an arbitrary set and denote
ρ ≥ 0 and 0 ≤ α ≤ 1 such that |eT| = ρk and |eT ∩ T0| = αρk. Let y = Ax + z with kzk2 ≤ ε and bxl2

is the minimizer of (7) with (5). If the measurement matrix A satisﬁes RIP with

for t > d, where γ = ω + (1 − ω)√1 + ρ − 2αρ and

δtk < δω
t

:=s t − d

t − d + γ2

d =(

1,

ω = 1

1 − αρ + a, 0 ≤ ω < 1

(18)

,

(19)

with a = max{α, β}ρ. Then

where

2(cid:16)ωkxT c

0k1(cid:17)
0 k1 + (1 − ω)kxeT c∩T c

√k

kbxl2 − xk2 ≤ D0(2ε) + D1
D0 =p2(t − d)(t − d + γ2)(1 + δtk)
(t − d + γ2)(q t−d
√2δtkγ +r(t − d + γ2)(q t−d

t−d+γ2 − δtk)

D1 =

,

(t − d + γ2)(q t−d

t−d+γ2 − δtk)

t−d+γ2 − δtk)δtk

+

1
√d

.

(20)

matrix A satisﬁes RIP (18). Then

Let y = Ax + z with kAT zk∞ ≤ ε. Assume that bxDS is the minimizer of (7) with (6) and the

2(cid:16)ωkxT c

0k1(cid:17)
0k1 + (1 − ω)kxeT c∩T c

√k

,

(21)

kbxDS − xk2 ≤ D′0(2ε) + D′1

where

D′0 = p2(t − d)(t − d + γ2)[[tk]]

(t − d + γ2)(q t−d

t−d+γ2 − δtk)

, D′1 = D1.

(22)

Remark 3.1. In Theorem 3.1, every signal x ∈ RN can be stably and robustly recovered. And if
B = {0} and x is a k−sparse vector, then Theorem 3.1 ensures exact recovery of the signal x.

7

For Gaussian noise case, the above results on the bounded noise case can be directly applied
to yield the corresponding results by using the same argument as in [3, 5]. The concrete content is
stated as follows.

Remark 3.2. Let x ∈ RN be an arbitrary signal and xk be its best k−term approximation supported
on T0 ⊆ {1, . . . , N} with |T0| ≤ k. Let eT ⊆ {1, . . . , N} be an arbitrary set and deﬁne ρ ≥ 0 and 0 ≤
α ≤ 1 such that |eT| = ρk and |eT ∩T0| = αρk. Assume that z ∼ Nn(0, σ2I) in (1) and δtk <q t−d
for t > d. Let Bl2 = {z : kzk2 ≤ σpn + 2√n log n} and BDS = {z : kAT zk∞ ≤ σ√2 log N}. bxl2
and bxDS are the minimizer of (7) with Bl2 and BDS, respectively. Then, with probability at least

1 − 1/n,

t−d+γ2

2(cid:16)ωkxT c

0k1(cid:17)
0k1 + (1 − ω)kxeT c∩T c

,

and

√k

kbxl2 − xk2 ≤ D0(2σqn + 2pn log n) + D1
kbxDS − xk2 ≤ D′0(2σp2 log N ) + D′1
with probability at least 1−1/√π log N . Here d =(
and γ = ω + (1 − ω)√1 + ρ − 2αρ.
(cid:16)1−√1−γ2(cid:17)2
γ2+2(cid:16)1−√1−γ2(cid:17) . For any ε > 0 and k ≥ 6
Theorem 3.2. Let d = 1 and t ≥ 1 +

1 − αρ + a, 0 ≤ ω < 1

2(cid:16)ωkxT c

0k1(cid:17)
0k1 + (1 − ω)kxeT c∩T c

√k

1,

ω = 1

,

with a = max{α, β}ρ,

ε . Then there exists

a sensing matrix A ∈ Rn×N with δtk <q t−d

t−d+γ2 + ε and some k−sparse signal x0 such that

(1) In the noiseless case, i.e., y = Ax0, the weighted l1 minimization (7) can not exactly recover

(2) In the noise case, i.e., y = Ax0 + z, for any bounded noise setting B, the weighted l1 minimiza-

the k−sparse signal x0, i.e., bx 6= x0, where bx is the solution to (7).
tion (7) can not stably recover the k−sparse signal x0, i.e., bx 9 x0 as z → 0, where bx is the

solution to (7).

Proposition 3.1. (1) If ω = 1, then d = 1 and γ = 1. The suﬃcient condition (18) of Theorem
3.1 is identical to (8) in Theorem 2.1 and D0 = C0, D1 = C1, D′0 = C′0, D′1 = C′1. Moreover,
the condition is sharp if t ≥ 4
3 .

(2) If α = 1

2 , then d = 1, γ = 1. The suﬃcient condition (18) of Theorem 3.1 is identical to that
3 , the

of Theorem 2.1 with (8) and D0 = C0, D1 = C1, D′0 = C′0, D′1 = C′1. Moreover, if t ≥ 4
condition is sharp.

8

(3) Assume that 0 ≤ ω < 1. If α > 1

2 , then d = 1 and γ < 1. The suﬃcient condition (18)
in Theorem 3.1 is weaker than (8) in Theorem 2.1 and (16) in Remark 2.1. Then D0 <
C0, D1 < C1, D0 < C′′0 , D1 < C′′1 . When t = 2, the suﬃcient condition (18) in Theorem
3.1 is weaker than (17) in Remark 2.1.

t = 0.9330, however δ1

Fig. 1 illustrates how the suﬃcient conditions on the RIP constants given in (18) and the
stability constants given in (20) change with ω for diﬀerent values of α in the case of weighted
l1 when t = 4. Note that (18) reduces to (8), and (20) reduces to (10) if ω = 1 or α = 0.5. In
Fig. 1 (a), we plot δω
t versus ω with diﬀerent values of α when t = 4. We observe that the bound
on RIP constant gets larger as α increases. That is to say, the suﬃcient condition on the RIP
constant becomes weaker as α increases. For example, if 90% of the support estimate is accurate
and ω = 0.4, we have δω
t = 0.8660 of standard l1. Figs. 1(b) and 1(b′) show
that the constant D0 decreases as α increases with δtk = 0.1 and δtk = 0.6, respectively. But Fig.
1(c) demonstrates the constant D1 with α 6= 0.5 is smaller than that with α = 0.5 when δtk = 0.1.
Fig. 1(c′) illustrates that the constant D1 decreases as α increases with δtk = 0.6. From above
recovery results by standard l1 and weighted l1, we see that if the partial support estimate is more
than 50% accurate, i.e. α > 0.5, the measurement matrix A for signal recovery by weighted l1
satisﬁes weaker conditions than the analogous conditions for recovery by standard l1. Moreover,
we have better upper bounds when α > 0.5 than those of standard l1.
in (18) and δω

a as well as (20) and (15) versus ω with various α. Fig.2(d) illustrates δω
t

a in (16) as well as stability
constants in (20) and (15) with various α when t = 4, a = 3 and δtk = δ(a+1)k = 0.1. Here we
plot δω
t and δω
is larger
than δω
a under the same support estimate. Moreover, Figs. 2(e) and 2(f) describe that constants
D0 and D1 are always smaller than C′′0 and C′′1 , respectively. These results state that the suﬃcient
condition (18) is weaker than (16), and error bound constants (20) in Theorem 3.1 are better than
those (15) in Theorem 2.2.

Fig. 2 compares the suﬃcient recovery conditions δω
t

4 Proofs

Proof of Theorem 3.1. Firstly, we show the estimate (19). Let bxl2 = x + h, where x is the original
signal and bxl2 is the minimizer of (7) with (5). Now assume that tk is an integer. We use the

following inequality which has been shown by Friedlander et al. (see (21) in [12]).

khT c

0 k1 ≤ωkhT0k1 + (1 − ω)khT0∪eT\eTαk1 + 2(cid:16)ωkxT c

0 k1 + (1 − ω)kxeT c∩T c

0k1(cid:17) .

(23)

9

1

0.95

ω t
δ
 
)

C
R

I

(
 
t

n
a

t
s
n
o
C
 
y
r
t

e
m
o
s
I
 

d
e

t
c
i
r
t
s
e
R

 

α=0.1
α=0.3
α=0.5
α=0.7
α=0.9

0.9

0.85

0.8

0.75

0.7

 
0

0.2

0.4

Weights(ω)

0.6

0.8

1

(a) δω

t versus ω

α=0.1
α=0.3
α=0.5
α=0.7
α=0.9

 

1.25

)

1

D

(
 
t

n
a

t
s
n
o
c
 
y
t
i
l
i

i

b
s
s
e
r
p
m
o
c
 

d
n
u
o
b

 
r
o
r
r

E

α=0.1
α=0.3
α=0.5
α=0.7
α=0.9

1.2

1.15

1.1

1.05

0.2

0.4

Weights(ω)

0.6

0.8

(b) D0 versus ω

1

 

α=0.1
α=0.3
α=0.5
α=0.7
α=0.9

0.2

0.4

Weights(ω)

0.6

0.8

1

(b′) D0 versus ω

1

0

3.8

3.6

3.4

3.2

3

2.8

2.6

2.4

2.2

2

0

0.2

0.4

Weights(ω)

0.6

0.8

1

(c) D1 versus ω

α=0.1
α=0.3
α=0.5
α=0.7
α=0.9

0.2

0.4

Weights(ω)

0.6

0.8

1

(c′) D1 versus ω

)

1

D

(
 
t

n
a
t
s
n
o
c
 
y
t
i
l
i

i

b
s
s
e
r
p
m
o
c
 

d
n
u
o
b

 
r
o
r
r

E

)

0

D

(
 
t

n
a

t
s
n
o
c
 

i

 

e
s
o
n
d
n
u
o
b

 
r
o
r
r

E

1.73

1.72

1.71

1.7

1.69

1.68

1.67

1.66

1.65

 
0

)

0

D

(
 
t

n
a
t
s
n
o
c
 
e
s
o
n

i

 

d
n
u
o
b

 
r
o
r
r

E

9.5

9

8.5

8

7.5

7

6.5

6

5.5

5

4.5

 
0

Figure 1: Comparison of the suﬃcient conditions for recovery and stability constants for weighted
l1 reconstruction with various α. In all the ﬁgures, we set t = 4 and ρ = 1. In (b) and (c), we ﬁx
δtk = 0.1. In (b′) and (c′), we ﬁx δtk = 0.6.

10

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

ω a
δ
 
d
n
a

 

ω t
δ
 
)

C
R

I

(
 
t

n
a
t
s
n
o
C
 
y
r
t

e
m
o
s
I
 

d
e
t
c
i
r
t
s
e
R

 

0.2

0

0.2

0.4

0.6

Weights(ω)

(d) δω

t and δω

a versus ω

α=0.1
α=0.3
α=0.5
α=0.7
α=0.9
α=0.1
α=0.3
α=0.5
α=0.7
α=0.9

α=0.1
α=0.3
α=0.5
α=0.7
α=0.9
α=0.1
α=0.3
α=0.5
α=0.7
α=0.9

0.8

1

12

)

0"
C

10

(
 

d
n
a

0

 
)

D

(
 
t

n
a

t
s
n
o
c
 

i

 

e
s
o
n
d
n
u
o
b

 
r
o
r
r

E

8

6

4

2

0

0

α=0.1
α=0.3
α=0.5
α=0.7
α=0.9
α=0.1
α=0.3
α=0.5
α=0.7
α=0.9

0.2

0.4

Weights(ω)

0.6

0.8

1

(e) D0 and C′′0 versus ω

)

1"
C

(
 

d
n
a

 
)

1

D

(
 
t

n
a
t
s
n
o
c
 
y
t
i
l
i

i

b
s
s
e
r
p
m
o
c
 
d
n
u
o
b

 
r
o
r
r

E

8

7

6

5

4

3

2

1

0

0.2

0.4

Weights(ω)

0.6

0.8

1

(f) D1 and C′′1 versus ω

Figure 2: Comparison between the bounds of suﬃcient recovery conditions δω
a in (16)
t
as well as stability constants in (20) and (15) with various α. In all the ﬁgures, we set t = 4, a = 3
and ρ = 1. The solid describe our main results and the dotted describe the results of Friedlander
et al. [12]. In (e) and (f), we ﬁx δtk = δ(a+1)k = 0.1 and δak = 0.05.

in (18) and δω

11

Let eT0 = T0\eTα, and T1 indexes the ak largest in magnitude coeﬃcients of heT c

and a = max{α, β}ρ. Denote

0

, where |T1| = ak

Clearly, |T01| = dk where

ω = 1,

eT0 ∪ T1, 0 ≤ ω < 1.

T01 =( T0,
d =(

1,

ω = 1

.

1 − αρ + a, 0 ≤ ω < 1

From (23) and d ≥ 1, it is clear that

Let

kh− max(dk)k1 ≤ ωkhT0k1 + (1 − ω)khT0∪eT\eTαk1 + 2(cid:16)ωkxT c
khωkhT0k1 + (1 − ω)khT0∪eT\eTαk1 + 2(cid:16)ωkxT c

r =

0k1(cid:17) .
0 k1 + (1 − ω)kxeT c∩T c
0k1(cid:17)i.

0k1 + (1 − ω)kxeT c∩T c

1

We partition h− max(dk) into two parts,
h− max(dk)(i) if |h− max(dk)(i)| > r
and 0 else.

i.e., h− max(dk) = h(1) + h(2), where h(1)(i) equals to
t−d and 0 else, h(2)(i) equals to h− max(dk)(i) if |h− max(dk)(i)| ≤ r
t−d

(24)

In view of the above deﬁnitions and (24),

kh(1)k1 ≤ kh− max (dk)k1 ≤ kr.

Let

From the deﬁnition of h(1), it is clear that

kh(1)k0 = m.
|h(1)(i)| ≥ Xi∈supp(h(1))

r
t − d

=

mr
t − d

.

kr ≥ kh(1)k1 = Xi∈supp(h(1))

Namely m ≤ k(t − d). Moreover, khmax (dk) + h(1)k0 = dk + m ≤ dk + k(t − d) = tk, and

kh(2)k1 = kh− max(dk)k1 − kh(1)k1 ≤ kr −

mr
t − d

= (k(t − d) − m) ·

r
t − d
By the deﬁnition of δk and the fact that

kh(2)k∞ ≤

.

r
t − d

,

kAhk2 ≤ kAbxl2 − Axk2 ≤ ky − Abxl2k2 + kAx − yk2 ≤ 2ε,

12

(25)

(26)

we obtain

hA(hmax (dk) + h(1)), Ahi ≤kA(hmax (dk) + h(1))k2kAhk2

≤p1 + δtkkhmax (dk) + h(1)k2 · (2ε).

(27)

Thus, using Lemma 2.1 and (25), we have h(2) =
and kuik∞ ≤ r
Thus,

λiui, supp(ui) ⊆ supp(h(2)),kuik1 = kh(2)k1
t−d , where ui is (k(t − d) − m)−sparse, namely, |supp(ui)| = kuik0 ≤ k(t − d) − m.

NPi=1

(28)

λjβj −

NPj=1

kuik2 ≤pkuik0kuik∞ ≤pk(t − d) − mkuik∞

r

≤pk(t − d) ·

t − d ≤r k

t − d

r.

Take βi = hmax (dk) + h(1) + µui, where 0 ≤ µ ≤ 1. We observe that

NXj=1

1
2

βi = hmax (dk) + h(1) + µh(2) −

λjβj −
1
2 − µ)(hmax (dk) + h(1)) −

=(

1
2

µui + µh.

1
2

βi

Because hmax (dk) is dk−sparse, h(1) is m−sparse, and ui is k(t − d) − m−sparse, βi and
2 βi − µh are tk−sparse. Let

1

X = khmax (dk) + h(1)k2,
γ = ω + (1 − ω)p1 + ρ − 2αρ,
0k1(cid:17)
0 k1 + (1 − ω)kxeT c∩T c

2(cid:16)ωkxT c

P =

√kγ

.

13

r

1

0k1(cid:17)i

2(cid:16)ωkxT c

0k1 + (1 − ω)kxeT c∩T c

Due to |T0 ∪ eT\eTα| = (1 + ρ − 2αρ)k,
kuik2 ≤r k
t − d
=r k
t − d ·
ωkhT0k1 + (1 − ω)khT0∪eT\eTαk1
ω√kkhT0k2 + (1 − ω)p(1 + ρ − 2αρ)kkhT0∪eT\eTαk2
ωkhmax (dk)k2 + (1 − ω)√1 + ρ − 2αρkhmax (dk)k2
2(cid:16)ωkxT c
=(cid:0)ω + (1 − ω)√1 + ρ − 2αρ(cid:1)khmax (dk)k2
2(cid:16)ωkxT c
√t − dkhmax (dk) + h(1)k2 +
√t − d

khωkhT0k1 + (1 − ω)khT0∪eT\eTαk1 + 2(cid:16)ωkxT c
0k1(cid:17)
0k1 + (1 − ω)kxeT c∩T c
pk(t − d)
pk(t − d)
0k1(cid:17)
2(cid:16)ωkxT c
0 k1 + (1 − ω)kxeT c∩T c
pk(t − d)
0k1(cid:17)
2(cid:16)ωkxT c
0 k1 + (1 − ω)kxeT c∩T c
pk(t − d)
0k1(cid:17)
0k1 + (1 − ω)kxeT c∩T c
pk(t − d)
0k1(cid:17)
0k1 + (1 − ω)kxeT c∩T c
pk(t − d)

pk(t − d)
√t − d

√t − d

=

≤

≤

≤

=

+

+

γ

γ

(X + P ).

+

+

(29)

We use the following identity (see (25) in [6])

NXi=1

λi(cid:13)(cid:13)(cid:13)A(cid:16) NXj=1

λjβj −

1
2

2

2

βi(cid:17)(cid:13)(cid:13)(cid:13)

=

NXi=1

λi
4 kAβik2
2.

(30)

14

Combining (27) and (28), we can estimate the left hand side of (30)

NXi=1

2

2

=

=

=

1
2

λjβj −

βi(cid:17)(cid:13)(cid:13)(cid:13)

1
2 − µ)(hmax (dk) + h(1)) −

1
2 − µ)(hmax (dk) + h(1)) −

1
2 − µ)(hmax (dk) + h(1)) −

1
2 − µ)(hmax (dk) + h(1)) −
1
2

λi(cid:13)(cid:13)(cid:13)A(cid:16) NXj=1
λi(cid:13)(cid:13)(cid:13)Ah(
NXi=1
λi(cid:13)(cid:13)(cid:13)Ah(
NXi=1
+ 2(cid:28)A(cid:18)(
λi(cid:13)(cid:13)(cid:13)Ah(
NXi=1
+ µ(1 − µ)hA(hmax (dk) + h(1)), Ahi
NXi=1
≤ (1 + δtk)
NXi=1

1
2 − µ)(hmax (dk) + h(1)) −
+ µ(1 − µ)p1 + δtkkhmax (dk) + h(1)k2 · (2ε)
1
2 − µ)2khmax (dk) + h(1)k2
+ µ(1 − µ)p1 + δtkkhmax (dk) + h(1)k2 · (2ε).
NXi=1

λi(cid:13)(cid:13)(cid:13)(
λih(

λi
4 kAβik2

= (1 + δtk)

2 =

λi
4 kA(hmax (dk) + h(1) + µui)k2

2

2

2

2

2

1
2

1
2

µui + µhi(cid:13)(cid:13)(cid:13)
µuii(cid:13)(cid:13)(cid:13)
µh(2)(cid:19) , µAh(cid:29) + µ2kAhk2
µuii(cid:13)(cid:13)(cid:13)

1
2

2

2

2

1
2

2

2

µui(cid:13)(cid:13)(cid:13)

2 +

2i
µ2
4 kuik2

On the other hand, in view of the expression of βi,

NXi=1
NXi=1

≥

λi
4

(1 − δtk)khmax (dk) + h(1) + µuik2
NXi=1

4(cid:16)khmax (dk) + h(1)k2

λi

2

2(cid:17).
2 + µ2kuik2

= (1 − δtk)

15

It follows from the above two inequalities and (29) that

2 +

2i
µ2
4 kuik2

λi

=

1
4

1
4

δtkµ2γ2

2 +

1
2

0 =

NXi=1

≤(1 + δtk)

2

2 −

NXi=1

λjβj −

1
2

(1 − δtk) +

− (1 − δtk)

λi
4 kAβik2

2

βi(cid:17)(cid:13)(cid:13)(cid:13)

1
2 − µ)2 −

λi(cid:13)(cid:13)(cid:13)A(cid:16) NXj=1
λih(
NXi=1
1
2 − µ)2khmax (dk) + h(1)k2
+ µ(1 − µ)p1 + δtkkhmax (dk) + h(1)k2 · (2ε)
2(cid:17)
4(cid:16)khmax (dk) + h(1)k2
NXi=1
2 + µ2kuik2
λi(cid:26)(cid:18)(1 + δtk)(
(1 − δtk)(cid:19)
NXi=1
1
2 − µ)2 −
2(cid:27)
δtkµ2kuik2
· khmax (dk) + h(1)k2
+ µ(1 − µ)p1 + δtkkhmax (dk) + h(1)k2 · (2ε)
2(t − d)(cid:21) X 2
≤(cid:20)(1 + δtk)(
+(cid:20)µ(1 − µ)p1 + δtk · (2ε) +
(cid:21) X +
)µ2(cid:17)δtkiX 2
=h(µ2 − µ) +(cid:16) 1
+hµ(1 − µ)p1 + δtk · (2ε) +
iX +
√(t−d)(t−d+γ2 )−(t−d)
µ2 s t − d
t − d + γ2 − δtk! X 2
µ2s t − d
t − d + γ2p1 + δtk · (2ε) +
t − dh − (t − d + γ2)(cid:16)s t − d
i ≥ 0,

t − d + γ2 − δtk(cid:17)X 2
+(cid:16)p(t − d)(t − d + γ2)(1 + δtk) · (2ε)
+ δtkγ2P(cid:17)X +

δtkµ2γ2P
t − d

δtkµ2γ2P
t − d

δtkµ2γ2P

t − d (cid:19)X +

2 − µ + (1 +

δtkγ2P 2

2

γ2

2(t − d)

16

δtkµ2γ2P 2
2(t − d)

δtkµ2γ2P 2
2(t − d)

.

δtkµ2γ2P 2
2(t − d) ≥ 0.

Taking µ =

γ2

, we obtain

−

t − d

t − d + γ2
+(cid:18) t − d + γ2

t − d

Namely,

µ2

which is a second-order inequality for X. Hence, we have

X ≤(cid:26)(cid:16)2εp(t − d)(t − d + γ2)(1 + δtk) + δtkγ2P(cid:17)
+h(cid:16)2εp(t − d)(t − d + γ2)(1 + δtk) + δtkγ2P(cid:17)2
+ 2(t − d + γ2)(cid:16)s t − d
t − d + γ2 − δtk(cid:17)δtkγ2P 2i1/2(cid:27)
·(cid:16)2(t − d + γ2)s t − d
t − d + γ2 − δtk)(cid:17)−1
≤ p(t − d)(t − d + γ2)(1 + δtk)
(t − d + γ2)(q t−d
2δtkγ2 +r2(t − d + γ2)(q t−d
2(t − d + γ2)(q t−d
kh− max(dk)k1 ≤khmax(dk)k1 + 2(cid:16)ωkxT c
=khmax(dk)k1 + P√kγ.

t−d+γ2 − δtk)δtkγ2

t−d+γ2 − δtk)

(2ε)

t−d+γ2 − δtk)

0k1(cid:17)
0k1 + (1 − ω)kxeT c∩T c

+

P.

From (23) and the representation of P , it is clear that

It follows from Lemma 2.2 that

kh− max(dk)k2 ≤ khmax(dk)k2 +

P γ
√d

.

17

Thus, we have the estimate of khk2 by the above inequalities

2 + kh− max(dk)k2
P γ

2

√d(cid:19)2

≤

(2ε)

P γ
√d

P γ
√d

t−d+γ2 − δtk)

2 +(cid:18)khmax(dk)k2 +

khk2 =qkhmax(dk)k2
≤skhmax(dk)k2
√2khmax(dk)k2 +
P γ
√d
√2khmax(dk) + h(1)k2 +
≤
=√2X +
≤p2(t − d)(t − d + γ2)(1 + δtk)
(t − d + γ2)(q t−d
+ √2δtkγ2 +r(t − d + γ2)(q t−d
(t − d + γ2)(q t−d
=p2(t − d)(t − d + γ2)(1 + δtk)
(t − d + γ2)(q t−d
+ √2δtkγ2 +r(t − d + γ2)(q t−d
(t − d + γ2)(q t−d
=p2(t − d)(t − d + γ2)(1 + δtk)
(t − d + γ2)(q t−d
+ √2δtkγ +r(t − d + γ2)(q t−d
(t − d + γ2)(q t−d

t−d+γ2 − δtk)

t−d+γ2 − δtk)

(2ε)

(2ε)

t−d+γ2 − δtk)δtkγ2

t−d+γ2 − δtk)

t−d+γ2 − δtk)δtkγ2

t−d+γ2 − δtk)

γ

√d!P

+

γ

√d!2(cid:16)ωkxT c

0k1(cid:17)
0 k1 + (1 − ω)kxeT c∩T c

√kγ

+

t−d+γ2 − δtk)δtk

t−d+γ2 − δtk)

1

√d! 2(cid:16)ωkxT c

0k1(cid:17)
0 k1 + (1 − ω)kxeT c∩T c

√k

.

+

If tk is not an integer, taking t′ = ⌈tk⌉/k, then t′k is an integer and t < t′. Thus we have

δt′k = δtk <s t − d

t − d + γ2 <s t′ − d
t′ − d + γ2 .

Then we can prove the result the same as the proof above by working on δt′k. So, we obtain (19).
Next, we prove (21). The proof of (21) is similar to the proof of (19). We only need to replace

(26) and (27) with the following (31) and (32), respectively. We also can get (21).

18

kAT Ahk∞ = kAT A(bxDS − x)k∞

≤kAT (AbxDS − y)k∞ + kAT (y − Ax)k∞

≤2ε,

hA(hmax (dk) + h(1)), Ahi = hhmax (dk) + h(1), AT Ahi
≤khmax (dk) + h(1)k1kAT Ahk∞
√tkkhmax (dk) + h(1)k2 · (2ε).
≤

This completes the proof of Theorem 3.1.

(31)

(32)

(cid:3)

Proof of Theorem 3.2. For d = 1, we have γ = ω + (1 − ω)√1 + ρ − 2αρ ≤ 1. Moreover, for all
ε > 0 and k ≥ 6

ε , we deﬁne

m′ =

γ2

(cid:16)t − 1 +p(t − 1)(t − 1 + γ2)(cid:17) k

1 +p1 − γ2
(cid:16)1−√1−γ2(cid:17)2
γ2+2(cid:16)1−√1−γ2(cid:17) , we obtain m′ ≥ k. Let m be the largest integer

and N ≥ k + m′. Since t ≥ 1 +
strictly smaller than m′, then m < m′ and m′ − m ≤ 1. We take

x1 =

1

qk + mk2

m′2

(1, . . . , 1

k−αρk

| {z }

,−

|

k
m′

, . . . ,−

ρk

{z

k
m′

}

k
m′

, . . . ,−
m−ρk

{z

k
m′

, 0, . . . , 0) ∈ RN ,
}

,−

|

if m > ρk; or take

x1 =

1

qk + mk2

m′2

if m ≤ ρk. It is easy to know kx1k2 = 1. Deﬁne the linear map A : RN → RN by

, 0, . . . , 0

, 1, . . . , 1

, 0, . . . , 0) ∈ RN ,

αρk

| {z }

}

, 1, . . . , 1

αρk

| {z }
{

k
m′
ρk

m

}|

−

z
|

k
m′

(1, . . . , 1

,

, . . . ,−

k−αρk

| {z }
{z
Ax =vuut1 +s t − d
=s1 +r t − 1
2 =(cid:18)1 +r t − 1

kAxk2

t − d + γ2 (x − hx1, xix1)

t − 1 + γ2 (x − hx1, xix1),
t − 1 + γ2(cid:19)(cid:0)kxk2

2 − |hx1, xi|2(cid:1) .

19

for all x ∈ RN . Then for any ⌈tk⌉−sparse vector x, we get

Hence, using Cauchy-Schwarz inequality and the fact that m′ ≥ k, m′ − m ≤ 1 and

m′2 + k2(t − 1)

m′2 + m′k

=

we have

2√t − 1(pt − 1 + γ2 − √t − 1)

γ2

,

kxk2

2

0 ≤|hx1, xi|2 ≤ kxk2

|x1(i)|2

2 · Xi∈supp(x)
m′2 + k(⌈tk⌉ − k)

2

2 · kx1,max (⌈tk⌉)k2
≤kxk2
=kxk2
2 ·
m′2 + mk
m′2 + k2(t − 1) + k
≤
m′2 + k2(t − 1) + k
m′2 + k2(t − 1) + k

m′2 + m′k

m′2 + mk

=

=

·

=

≤

m′2 + m′k

m′2 + m′k
m′2 + k2(t − 1)
2√t − 1(pt − 1 + γ2 − √t − 1)
2√t − 1(pt − 1 + γ2 − √t − 1)
≤  2√t − 1(pt − 1 + γ2 − √t − 1)

≤

γ2

γ2

γ2

2

2

kxk2
m′2 + m′k
m′2 + mk kxk2
·
kxk2
m′2 + k2(t − 1) + k
m′2 + k2(t − 1)

1 − k(m′−m)

m′2+m′k

1

·

·

2

1

m′2+m′k

1 − k(m′−m)
) ·

1
1 − 1

1
tk

2k kxk2

2

· (1 +

· (1 +
3

+

3
k

2

)kxk2
k!kxk2

2.

20

2

2 ≥ kAxk2
2√t − 1(pt − 1 + γ2 − √t − 1)
2√t − 1(pt − 1 + γ2 − √t − 1)

γ2

γ2

−

2

3

k!kxk2
!

t − 1 + γ2(cid:19) 3

k(cid:21)kxk2

2

2

t − 1 + γ2 + ε(cid:19) kxk2
t − 1 + γ2(cid:19)kxk2
t − 1 + γ2(cid:19) · 1 −
t − 1 + γ2(cid:19) · 1 −
k#kxk2
t − 1 + γ2(cid:19) 3
t − 1 + γ2 −(cid:18)1 +r t − 1
t − 1 + γ2 − ε(cid:19) kxk2

2,

2

t−1+γ2 + ε. Next, we deﬁne

ρk

αρk

Consequently,

(cid:18)1 +r t − 1
≥(cid:18)1 +r t − 1
≥(cid:18)1 +r t − 1
="(cid:18)1 +r t − 1
−(cid:18)1 +r t − 1
=(cid:20)1 −r t − 1
≥(cid:18)1 −r t − 1
which deduces δtk ≤q t−1
z }| {
| {z }
| {z }

k−αρk
1, . . . , 1,

η0 = (0, . . . , 0

x0 = (

k−αρk

k−αρk

k
m′

k
m′

,

,

, . . . ,

, . . . ,

k
m′

0, . . . , 0,

, 0, . . . , 0

z }| {
{z
|
}|
z
|
where kx0k1,w = k, kη0k1,w ≤ m · k
kη0k1,w < kx0k1,w. In view of Ax1 = 0, we have Ax0 = Aη0.

z }| {
1, . . . , 1, 0, . . . , 0) ∈ RN ,
k
k
, 0, . . . , 0) ∈ RN , if m > ρk,
m′
m′
| {z }
}
}
{
, 0, . . . , 0) ∈ RN , if m ≤ ρk,
{z

m′ < k. Obviously, x0 is k−sparse, x1 =

{z
|
| {z }

or η0 = (0, . . . , 0

1qk+ mk2

k
m′
ρk

, 0, . . . , 0

, 0, . . . , 0

m−ρk

, . . . ,

}

ρk
m

αρk

αρk

m′2

,

(x0 − η0) and

Thus, in the noiseless case y = Ax0, suppose that the weighted l1 minimization method (7) can

In the noise case y = Ax0 + z, suppose that the weighted l1 minimization method (7) can

exactly recover x0, i.e., bx = x0. According to the deﬁnition of bx and y = Aη0, it contradicts that
kη0k1,w < kx0k1,w = kbxk1,w.
z→0bx = x0. We observe that y − A(bx − x0 + η0) = y − Abx ∈ B, thus
kbxk1,w ≤ kbx − x0 + η0k1,w. As z → 0, kx0k1,w ≤ kη0k1,w. It contradicts that kη0k1,w < kx0k1,w.

Hence, the weighted l1 minimization method (7) fails to exactly and stably recover x0 based on
(cid:3)

stably recover x0, i.e.,

lim

y and A.

21

References

[1] R. Baraniuk and P. Steeghs, Compressive radar imaging, in Proc. IEEE Radar Conf., 2007,

pp. 128-133.

[2] R. V. Borries, C. Miosso and C. Potes, Compressed sensing using prior information,

in
2nd IEEE Int. Workshop on Computational Advances in Multi-Sensor Adaptive Processing,
CAMPSAP 2077, 12-14, 2007, pp. 121-124.

[3] T. T. Cai, L. Wang and G. W. Xu, Shifting inequality and recovery of sparse signals, IEEE

Trans. Signal Process, 58(3), pp. 1300-1308, 2010.

[4] T. T. Cai, L. Wang and G. W. Xu, New bounds for restricted isometry constants, IEEE Trans.

Inform. Theory, 56(9), pp. 4388-4394, 2010.

[5] T. T. Cai, G. W. Xu, J. Zhang, On recovery of sparse signal via l1 minimization, IEEE Trans.

Inf. Theory, 55(7), pp. 3388-3397, 2009.

[6] T. T. Cai and A. Zhang, Spares representation of a polytope and recovery of sparse signals

and low-rank matrices, IEEE Trans. Inform. Theory, 60(1), pp. 122-132, 2014.

[7] T. T. Cai and A. Zhang, Compressed sensing and aﬃne rank minimization under restricted

isometry, IEEE Trans. Signal Process., 61(13), pp. 3279-3290, 2013.

[8] T. T. Cai and A. Zhang, Sharp RIP bound for sparse signal and low-rank matrix recovery,

Appl. Comput. Harmon. Anal., 35, pp. 74-93, 2013.

[9] E. J. Cand`e, J. Romberg and T. Tao, Stable signal recovery from incomplete and inaccurate

measurements, Commum. Pure Appl. Math., 59, pp. 1207-1223, 2006.

[10] E. Cand`es and T. Tao, Decoding by linear programming, IEEE Trans. Inf. Theory, 51(12), pp.

4203-4215, 2005.

[11] Q. Du and J. E. Fowler, Hyperspectral image compression using jpeg2000 and principal com-

ponent analysis, IEEE Geosci. Remote Sens. Lett., 4(4), pp. 201-205, 2007.

[12] M. P. Friedlander, H. Mansour, R. Saab and O. Yilmaz, Recoverying compressively sampled
signals using partial support information, IEEE Transactions Information Theory, 58(2), pp.
1122-1134, 2012.

[13] M. Herman and T. Strohmer, High-resoluton radar via compressed sensing, IEEE Signal Pro-

cess. Mag., 57(6), pp. 2275-2284, 2009.

22

[14] L. Jacques, A short note compressed sensing with partially known signal support, Signal

Process., 90, pp. 3308-3312, 2010.

[15] M. A. Khajehnejad, W. Xu, A. S. Avestimehr and B. Hassibi, Weighted l1 minimization for
sparse recovery with prior information, in IEEE Int. Symp. Information Theory, ISIT 2009,
2009, pp. 483-487.

[16] W. Lu and N. Vaswani, Exact reconstruction conditions and error bounds for regularized

modiﬁed basis pursuit, in Proc. Asilomar Conf. on Signals, Systems and Computers, 2010.

[17] W. Lu and N. Vaswani, Modiﬁed basis pursuit denoising (modiﬁedbpdn) for noisy compressive
sensing with partially known signal support, in IEEE Int. Conf. Acoustics Speech and Signal
Processing (ICASSP), 2010, 14-19, 2010, pp. 3926-3929.

[18] M. Lustig, J. M. Santos, J. H. Lee, D. L. Donoho and J. M. Pauly, Application of compressed

sensing for rapid MR inmaging, in SPARS, (Rennes, France), 2005.

[19] Q. Mo and S. Li, New bounds on the restricted isometry constant δ2k, Appl. Comput. Harmon.

Anal., 31(3), pp. 3335460-468, 2011.

[20] F. Parvaresh, H. Vikalo, S. Misra and B. Hassibi, Recovering sparse signals using sparse
measurement matrices in compressed DNA microarrays, IEEE J. Sel. Top. Signal Process.,
2(3), pp. 275-285, 2008.

[21] N. Vaswani and W. Lu, Modiﬁed-CS: Modifying compressive sensing for problems with par-

tially known support, IEEE Trans. Signal Process., 58(9), pp. 4595-4607, 2010.

[22] N. Vaswani and W. Lu, Modiﬁed-CS: Modifying compressive sensing for problems with par-

tially known support, IEEE Int. Symp. Information Theory, ISIT 2009, pp. 488-492, 2009.

[23] J. Zhang, D. Zhu and G. Zhang, Adaptive compressed sensing radar oriented toward cognitive
decetion in dynamic sparse target scene, IEEE Trans. Signal Process., 60(4), pp. 1718-1729,
2012.

23

