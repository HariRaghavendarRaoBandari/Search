6
1
0
2

 
r
a

 

M
3
1

 
 
]

V
C
.
s
c
[
 
 

1
v
4
8
9
3
0

.

3
0
6
1
:
v
i
X
r
a

An eﬃcient Exact-PGA algorithm for constant

curvature manifolds

Rudrasis Chakraborty1, Dohyung Seo2 and Baba C. Vemuri1

1Department of CISE, University of Florida
2U-Systems, A GE Healthcare Company

1{rudrasis, vemuri}@cise.ufl.edu

2dhseo.118@gmail.com

Abstract

Manifold-valued datasets are widely encountered in many computer vi-
sion tasks. A non-linear analog of the PCA, called the Principal Geodesic
Analysis (PGA) suited for data lying on Riemannian manifolds was re-
ported in literature a decade ago. Since the objective function in PGA
is highly non-linear and hard to solve eﬃciently in general, researchers
have proposed a linear approximation. Though this linear approximation
is easy to compute, it lacks accuracy especially when the data exhibits
a large variance. Recently, an alternative called exact PGA was pro-
posed which tries to solve the optimization without any linearization. For
general Riemannian manifolds, though it gives better accuracy than the
original (linearized) PGA, for data that exhibit large variance, the opti-
mization is not computationally eﬃcient. In this paper, we propose an
eﬃcient exact PGA for constant curvature Riemannian manifolds (CCM-
EPGA). CCM-EPGA diﬀers signiﬁcantly from existing PGA algorithms
in two aspects, (i) the distance between a given manifold-valued data point
and the principal submanifold is computed analytically and thus no opti-
mization is required as in existing methods. (ii) Unlike the existing PGA
algorithms, the descent into codimension-1 submanifolds does not require
any optimization but is accomplished through the use of the Rimean-
nian inverse Exponential map and the parallel transport operations. We
present theoretical and experimental results for constant curvature Rie-
mannian manifolds depicting favorable performance of CCM-EPGA com-
pared to existing PGA algorithms. We also present data reconstruction
from principal components and directions which hasnâĂŹt been presented
in literature in this setting.

Introduction

1
Principal Component Analysis (PCA) is a widely used dimensionality reduction
technique in Science and Engineering. PCA however requires the input data to
lie in a vector space. With the advent of new technologies and wide spread use
of sophisticated feature extraction methods, manifold-valued data have become

1

ubiquitous in many ﬁelds including Computer Vision, Medical Imaging and Ma-
chine Learning. A nonlinear version of PCA, called Principal Geodesic Analysis
(PGA), for data lying on Riemannian manifolds was introduced in [8].

Since the objective function of PGA is highly nonlinear and hard to solve
in general, researchers proposed a linearized version of PGA [8]. Though this
linearized PGA, hereafter referred to as PGA, is computationally eﬃcient, it
lacks accuracy for data with large spread/variance. In order to solve the objec-
tive function exactly, some researchers proposed to solve the original objective
function (not the approximation) and called it exact PGA [24]. While exact
PGA attempts to solve this complex nonlinear optimization problem, it is how-
ever computationally ineﬃcient. Though it is not possible to eﬃciently and
accurately solve this optimization problem for a general manifold, however, for
manifolds with constant sectional curvature, we formulate an eﬃcient and exact
PGA algorithm, dubbed CCM-EPGA. It is well known in geometry, by virtue
of Killing-Hopf theorem, [4] that any non-zero constant curvature manifold is
isomorphic to either the hypersphere (SN) or the hyperbolic space (HN), hence
in this work, we present the CCM-EPGA formulation for (SN) and (HN). Our
formulation has several applications to Computer Vision and Statistics includ-
ing directional data [20] and color spaces [18]. Several other applications of
hyperbolic geometry are, shape analysis [29], electrical Impedence Tomography,
Geoscience imaging [27], brain morphometry [28], catadiaoptric vision [3] etc.
In order to depict the eﬀectiveness of our proposed CCM-EPGA, we use the
average projection error as deﬁned in [24]. We also report the computational
time comparison of the CCM-EPGA with the PGA [8] and the exact PGA [24].
Several variants of PGA exist in literature and we brieﬂy mention a few here. In
[22], authors computed the principal geodesics without approximation only for
a special Lie group, SO(3). Geodesic PCA (GPCA) [13, 12] solves a diﬀerent
optimization function namely, optimizing the projection error along geodesics.
The authors in [12] minimize the projection error instead of maximizing variance
in geodesic subspaces (deﬁned later in the paper). GPCA does not use a linear
approximation, but it is restricted to manifolds where a closed form expression
for the geodesics exists. More recently, a probabilistic version of PGA called
PPGA was presented in [30], which is a nonlinear version of PPCA [26]. None
of these methods attempt to compute the solution to the exact PGA problem
deﬁned in [24].

The rest of the paper is organized as follows. In Section 2, we present the
formulation of PGA. We also discuss the details of the linearized version of PGA
[8] and exact PGA [24]. Our formulation of CCM-EPGA is presented in Sec-
tion 2. We present experimental results of CCM-EPGA algorithm along with
comparisons to exact PGA and PGA in Section 3. In addition to synthetic data
experiments, we present the comparative performance of CCM-EPGA on two
real data applications. In Section 4, we present the formulation for reconstruc-
tion of data from principal directions and components in this nonlinear setting.
Finally, in section 5, we draw conclusions.

2

2 Principal Geodesic Analysis
Principal Component Analysis (PCA) [16] is a well known and widely used sta-
tistical method for dimensionality reduction. Given a vector valued dataset, it
returns a sequence of linear subspaces that maximize the variance of the pro-
jected data. The kth subspace is spanned by the principal vectors {v1, v2,··· , vk}
which are mutually orthogonal. PCA is well suited for vector-valued data sets
but not for manifold-valued inputs. A decade ago, the nonlinear version called
the Principal Geodesic Analysis (PGA) was developed to cope with manifold-
valued inputs [8]. In this section, ﬁrst, we brieﬂy describe this PGA algorithm,
then, we show the key modiﬁcation performed in [24] to arrive at what they
termed as the exact PGA algorithm. We then motivate and present our ap-
proach which leads to an eﬃcient and novel algorithm for exact PGA on constant
curvature manifolds (CCM-EPGA).
Let M be a Riemannian manifold. Let’s suppose we are given a dataset, X =
{x1,··· , xn}, where xj ∈ M. Let us assume that the ﬁnite sample Fr´echet mean
[9] of the data set exists and be denoted by µ. Let Vk be the space spanned by
mutually orthogonal vectors (principal directions) {v1,··· , vk}, vj ∈ TµM,∀j.
Let Sk be the kth geodesic subspace of TµM, i.e., Sk = Expµ(Vk), where Exp
is the Riemannian exponential map (see [4] for deﬁnition). Then, the principal
directions, vi are deﬁned recursively by

vi =

arg max
kvk=1,v∈V ⊥
i−1

1
n

nX

j=1

d2(µ, ΠSi(xj))

(1)

Si = Expµ(spanVi−1, vi)

(2)
where d(x, y) is the geodesic distance between x ∈ M and y ∈ M, ΠS(x) is the
point in S closest to x ∈ M.

The PGA algorithm on M is summarized in Alg. 1.

Algorithm 1 The PGA algorithm on manifold M
1: Given dataset X = {x1,··· , xn} ∈ M, and 1 ≤ L ≤ dim(M)
2: Compute the FM, µ, of X [1]
3: Set k ← 1
4: Set {¯x0
5: while k ≤ L do
6:

n} ← {x1,··· , xn}

1,··· , ¯x0

nX

kvk=1,v∈TµM,v∈V ⊥
k−1

arg max
,··· , ¯xk−1

Solve vk =
Project {¯xk−1
Set the projected points to {¯xk1,··· , ¯xk
n}
k ← k + 1

j=1

1

n

which is orthogonal to the current geodesic subspace.

7:

8:
9:
10: end while

d2(µ, ΠSk(xj)) as in Eq. (1).

} to a k co-dimension one submanifold Z of M,

1
n

3

2.1 PGA and exact PGA
In Alg. 1 (lines 6 − 7), as the projection operator Π is hard to compute, hence
a common alternative is to locally linearize the manifold. This approach [8]
maps all data points on the tangent space at µ, and as tangent plane is a vector
space, one can use PCA to get the principal directions. This simple scheme is an
approximation of the PGA. This motivated the researchers to ask the following
question: Is it possible to do PGA, i.e., solve Eq. (1) without any linearization?
The answer is yes. But, computation of the projection operator, ΠS(x), i.e.,
the closest point of x in S is computationally expensive. In [24], Sommer et al.
give an alternative deﬁnition of PGA, i.e., they minimize the average squared
reconstruction error, i.e., d2(xj, ΠSi(xj)) instead of d2(µ, ΠSi(xj)) in eqns. (1).
They use an optimization scheme to compute this projection. Further, they
termed their algorithm, exact PGA, as it does not require any linearization.
However, their optimization scheme is in general computationally expensive and
for a data set with large variance, convergence is not guaranteed. Hence, for large
variance data, their exact PGA is still an approximation as it might not converge.
This motivates us to formulate an accurate and computationally eﬃcient exact
PGA.

2.2 Eﬃcient and accurate exact PGA
In this paper, we present an analytic expression for the projected point and
design an eﬀective way to project data points on to the co-dimension k sub-
manifold (as in 1, line 7). An analytic expression is in general not possible
to derive for arbitrary Riemannian manifolds. However, for constant curvature
Riemannian manifolds, i.e., SN (positive constant curvature) and HN (negative
constant curvature), we derive an analytic expression for the projected point and
devise an eﬃcient algorithm to project data points on to a co-dimension k sub-
manifold. Both these manifolds are quite commonly encountered in Computer
Vision [10, 19, 3, 28, 29] as well as many other ﬁelds of Science and Engineering.
The former more so than the latter. Eventhough, there are applications that
can be naturally posed in hyperbolic spaces (e.g., color spaces in Vision [18],
catadiaoptric images [3] etc.), their full potential has not yet been exploited in
Computer Vision research as much as in the former case.

We ﬁrst present some background material for the N-dimensional spheri-
cal and hyperbolic manifolds and then derive an analytical expression of the
projected point.

2.2.1 Basic Riemannian Geometry of SN

• Geodesic distance: The geodesic distance between ψ, ¯ψ ∈ SN is given

by, d(ψ, ¯ψ) = arccos(ψt ¯ψ).

• Exponential Map: Given a vector v ∈ TψSN, the Riemannian Expo-
nential map on SN is deﬁned as Expψ(v) = cos(|v|)ψ +sin(|v|)v/|v|. The

4

Exponential map gives the point which is located on the great circle along
the direction deﬁned by the tangent vector v at a distance |v| from ψ.

• Inverse Exponential Map: The tangent vector v ∈ TψSN directed
sin(θ)( ¯ψ − ψ cos(θ)) where, θ =
from ψ to ¯ψ is given by, Exp−1
d(ψ, ¯ψ). Note that, the inverse exponential map is deﬁned everywhere on
the hypersphere except at the antipodal points.

ψ ( ¯ψ) = θ

2.2.2 Basic Riemannian Geometry of HN
The hyperbolic N-dimensional manifold can be embedded in RN+1 using three
diﬀerent models. In this discussion, we have used the hyperboloid model [14]. In
this model, HN is deﬁned as HN = {x = (x1,··· , xN+1)t ∈ RN+1| < x, x >H=
−1, x1 > 0}, where the inner product on HN, denoted by < x, y >H is deﬁned

as < x, y >H= −x1 ∗ y1 +PN+1

i=2 (xi ∗ yi).

• Geodesic distance: The geodesic distance between ψ, ¯ψ ∈ HN is given

by, d(ψ, ¯ψ) = cosh−1(− < ψ, ¯ψ >H).

• Exponential Map: Given a vector v ∈ TψHN, the Riemannian Expo-
nential map on HN is deﬁned as, Expψ(v) = cosh(|v|)ψ + sinh(|v|)v/|v|.
• Inverse Exponential Map: The tangent vector v ∈ TψHN directed
sinh(θ)( ¯ψ − ψ cosh(θ)) where, θ =

ψ ( ¯ψ) = θ

from ψ to ¯ψ is given by Exp−1
d(ψ, ¯ψ).

For the rest of this paper, we consider the underlying manifold, M, as a
constant curvature Riemannian manifold, i.e., M is diﬀeomorphic to either SN
or HN, where N = dim(M) [4]. Let ψ, ¯ψ ∈ M, v ∈ T ¯ψM. Further, let y(v, ψ)
be deﬁned as the projection of ψ on the geodesic submanifold deﬁned by ¯ψ and
v. Now, we will derive a closed form expression for y(v, ψ) in the case of SN
and HN.

2.3 Analytic expression for y(v, ψ) on SN
Theorem 1. Let ψ ∈ SN and v ∈ T ¯ψSN. Then the projection of ψ on the
geodesic submanifold deﬁned by ¯ψ and v, i.e., y(v, ψ) is given by:

y(v, ψ) = cos(arctan

 (< v, ψ >)/(< ψ, ¯ψ >)
!
 (< v, ψ >)/(< ψ, ¯ψ >)

|v|

!

) ¯ψ

+ sin(arctan

|v|

)v/|v|

(3)

Proof. Consider the spherical triangle shown in Fig. 1, 4 ¯ψψyψ, where yψ =
y(v, ψ). Let, a = d( ¯ψ, yψ), b = d(yψ, ψ) and c = d(ψ, ¯ψ). Also, let A = ∠ ¯ψψyψ,

5

Figure 1: Analytic expression for the projected point on the Sphere.

B = ∠ψ ¯ψyψ, C = ∠ ¯ψyψψ. Clearly, since yψ is the projected point , C = π/2.
So,

sin c(ψ − ¯ψ cos c), v >
cos B = < c
sin c − cot c < v, ¯ψ >
<v,ψ>

c|v|

=

|v|

(4)

Here, < ., . > denotes the Euclidean inner product, where both ψ and v are
viewed as points in RN+1, i.e., the ambient space. Note that, < v, ¯ψ >= 0, as
v ∈ T ¯ψSN. From spherical trigonometry, we know that tan a = cos B tan c.

∴ cos B tan c =

<v,ψ>
cos c
|v|

= (< v, ψ >)/(< ψ, ¯ψ >)

|v|

6

 (< v, ψ >)/(< ψ, ¯ψ >)

|v|

!

∴ a = arctan

Hence, using the Exponential map, we can show that yψ is given by,

yψ = cos(a) ¯ψ + sin(a)v/|v|

(5)

(6)
(cid:4)

Analogously, we can derive the formula for y(v, ψ) on HN, v ∈ T ¯ψHN.

Theorem 2. Let ψ ∈ HN and v ∈ T ¯ψHN. Then the projection of ψ on the
geodesic submanifold deﬁned by ¯ψ and v, i.e., y(v, ψ) is given by:

where,

a = tanh−1

yψ = cosh(a) ¯ψ + sinh(a)v/|v|

 (< v, ψ >H)/(− < ψ, ¯ψ >H)

(7)

!

.
Proof. As before, consider the hyperbolic triangle shown in, 4 ¯ψψyψ, where
yψ = y(v, ψ). Let, a = d( ¯ψ, yψ), b = d(yψ, ψ) and c = d(ψ, ¯ψ). Also, let
A = ∠ ¯ψψyψ, B = ∠ψ ¯ψyψ, C = ∠ ¯ψyψψ. Clearly, since yψ is the projected point
, C = π/2. Then, B is the angle between Log ¯ψ(ψ) and v. Hence,

|v|

|v|

|v|

cosh B =

sinh c − coth c < v, ¯ψ >H
<v,ψ>H

Then, from hyperbolic trigonometry, as tanh a = cosh B tanh c, we get

 (< v, ψ >H)/(− < ψ, ¯ψ >H)

!

a = tanh−1

(8)

(9)

map, we can show that yψ = cosh(a) ¯ψ + sinh(a)v/|v|.

Note that, as the arc length between ¯ψ and yψ is a, hence, using Exponential
(cid:4)

Given the closed form expression of the projected point, now we are in a
position to devise an eﬃcient projection algorithm (for line 7 in Alg. 1), which is
given in Alg. 2. Note that, using Alg. 2, data points on the current submanifold
are projected to a submanifold of dimension one less, which is needed by the
PGA algorithm in line 6. Also note that, in order to ensure existence and
uniqueness of FM, we have restricted the data points to be within a geodesic ball
of convexity radius < π/2 [1].

Note that in order to descend to the codimension-1 submanifolds, we use
step-1 and step-2 instead of the optimization method used in the exact PGA
algorithm of [24].

7

Algorithm 2 Algorithm for projecting the data points to a co-dimension one
submanifold
1: Input: a data point xi ∈ SN(HN), a geodesic submanifold deﬁned at µ
and v ∈ TµSN(TµHN), and y(v, xi) which is the projection of ψ on to the
geodesic submanifold.
2: Output: ¯xi which is the projection of the data point xi to a subspace,
SN−1(HN−1), which is orthogonal to the current geodesic submanifold.
3: Step 1. Evaluate the tangent vector, vi ∈ Ty(v,xi)SN(Ty(v,xi)HN) directed
towards xi using the Inverse Exponential Map. It is clear that vi is orthog-
onal to v.

4: Step 2. Parallel transport vi to µ. Let vµ

vector. The geodesic submanifold deﬁned by µ and vµ
geodesic submanifolds obtained from the previous steps in Alg. 1.

i

i denote the parallel transported
is orthogonal to

5: Step 3. Set ¯xi ← y(vµ

i , xi)

3 Experimental Results
In this section, we present experiments demonstrating the performance of CCM-
EPGA compared to PGA [8] and exact PGA [24]. We used the average projec-
tion error, deﬁned in [24], as a measure of performance in our experiments. The
average projection error is deﬁned as follows. Let {xi}n
i=1 be data points on a
manifold M. Let µ be the mean of the data points. Let, v be the ﬁrst principal
direction and S = Expµ(v). Then the error (E) is deﬁned as follows:

nX

i=1

E = 1

n

d2(xi, ΠS(xi))

(10)

where d(., .) is the geodesic distance function on M. We also present the com-
putation time for each of the three algorithms. All the experimental results
reported here were obtained on a desktop with a single 3.33 GHz Intel-i7 CPU
with 24 GB RAM.

3.1 Comparative performance of CCM-EPGA on Synthetic

data

In this section, we present the comparitive performance of CCM-EPGA on sev-
eral synthetic datasets. For each of the synthetic data, we have reported the
average projection error and computation time for all three PGA algorithms in
Table 1. All the four datasets are on S2 and the Fr´echet mean is at the ”north
pole”. For all the datasets, samples are in the northern hemisphere to ensure
that the Fr´echet mean is unique. Data 1 and Data 2 are generated by tak-
ing samples along a geodesic with a slight perturbation. The last two datasets
are constructed by drawing random samples on the northern hemisphere. In
addition, data points from Data 1 are depicted in Fig. 2. The ﬁrst principal
direction is also shown (black for CCM-EPGA, blue for PGA and red for exact

8

Data

Var.

Data 1
Data 2
Data 3
Data 4

2.16
0.95
7.1e-03
5.9e-02

CCM-EPGA

avg. proj. err. Time(s)
1.13e − 04
5.87e − 02
2.33e − 03

0.70
0.27
0.19
0.33

0.27

PGA

avg. proj. err. Time(s)

0.174
0.59
0.55
0.37

0.46
0.12
0.05
0.14

exact PGA

2.54e-02

avg. proj. err. Time(s)
14853
84.38
16.87
71.84

0.59
0.55
0.37

Table 1: Comparison results on synthetic datasets

Figure 2: Synthetic data (Data 1) on S2

PGA). Further, we also report the data variance for these synthetic datasets.
By examining the results, it’s evident that for data with low variance, the sig-
niﬁcance of CCM-EPGA in terms of projection error is marginal, while for high
variance data, CCM-EPGA yields signiﬁcantly better accuracy. Also, CCM-
EPGA is computationally very fast in comparison to exact PGA. The results
in Table 1 indicate that CCM-EPGA outperforms exact PGA in terms of eﬃ-
ciency and accuracy. Although, the required time for PGA is less than that of
CCM-EPGA, in terms of accuracy, CCM-EPGA dominates PGA.

3.2 Comparative performance on point-set data (SN ex-

ample)

In this section, we depict the performance of the proposed CCM-EPGA on 2D
point-set data. The database is called GatorBait-100 dataset [21]. This dataset
consists of 100 images of shapes of diﬀerent ﬁshes. From each of these images of
size 20 × 200, we ﬁrst extract boundary points, then we apply the Schr¨odinger
distance transform [7] to map each of these point sets on a hypersphere (S3999).
Hence, this data consists of 100 point-sets each of which lie on S3999. As before,

9

Method
CCM-EPGA
PGA

avg. proj. error Time(s)

2.83e − 10
9.68e − 02

0.40
0.28

Table 2: Comparison results on Gator-Bait database

we have used the average projection error [24], to measure the performance of
algorithms in the comparisons. Additionally, we report the computation time
for each of these PGA algorithms. We used the code available online for exact
PGA [23]. This online implementation is not scalable to large (even moderate)
number of data points, and furhter requires the computation of the Hessian
matrix in the optimization step, which is computationally expensive. Hence, for
this real data application on the high dimensional hypersphere, we could not
report the results for the exact PGA algorithm. Though one can use a Sparse
matrix version of the exact PGA code, along with eﬃcient parallelization to
make the exact PGA algorithm suitable for moderately large data, we would
like to point out that since our algorithm does not need such modiﬁcations, it
clearly gives CCM-EPGA an advantage over exact PGA from a computational
eﬃciency perspective. In terms of accuracy, it can be clearly seen that CCM-
EPGA outperforms exact PGA from the results on synthetic datasets. Both
average projection error and computational time on GatorBait-100 dataset are
reported in Table 2. This result demonstrates accuracy of CCM-EPGA over
the PGA algorithm with a marginal sacriﬁce in eﬃciency but signiﬁcant gains
in accuracy.

3.3 PGA on population of gaussian distributions (HN ex-

ample)

In this section, we propose a novel scheme to compute principal geodesic sub-
manifolds for the manifold of gaussian densities. Here, we use concepts from
information geometry presented in [2], speciﬁcally, the Fisher information ma-
trix [17] to deﬁne a metric on this manifold [6]. Consider a normal density f(.|θ)
in an n−dimensional space, with parameters represented by θ. Then the ijth
entry of the n × n Fisher matrix, denoted by gij, is deﬁned as follows:

For example, for a univariate normal density f(.|µ, σ), the ﬁsher information
matrix is

dx

(11)

So, the metric is deﬁned as follows:

Z

R

gij(θ) =

∂lnf(x|θ)

∂θj

∂θi

f(x|θ) ∂lnf(x|θ)
(cid:18) 1

(gij(µ, σ)) =

σ2
0

(cid:19)

0
2
σ2

< u, v >= utGv

10

(12)

(13)

where G = (gij) is the Fisher information matrix. Now, consider the param-
eter space for the univariate normal distributions. The parameter space is
HF = (µ, σ) ∈ R2|σ > 0, i.e., positive half space, which is the Hyperbolic space,
modeled by the Poincar´e half-plane, denoted by P2. We can deﬁne a bijection
F1 : HF → P2 as F(µ, σ) = ( µ√
2 , σ). Hence, the univariate normal distributions
can be parametrized by the 2−dimensional hyperbolic space. Moreover, there
exists a diﬀeomorphsim between P2 and H2 (the mapping is analogous to stereo-
graphic projection for SN), thus, we can readily use the formulation in Section 2
to compute principal geodesic submanifold on the manifold of univariate normal
distributions.

Motivated by the above formulation, we ask the following question: Does
there exist a similar relation for multivariate normal distributions? The an-
swer is no in general. But if the multivariate distributions have diagonal co-
variance matrix, (i.e., independent uncorrelated variables in the mutivariate
case), the above relation between P2 and H2 can be generalized. Consider
an N−dimensional normal distribution parametrized by (µ, Σ) where µ =
(µ1,··· , µN)t and Σ is a diagonal positive deﬁnite matrix (i.e., Σij = σi, if
i = j, else Σij = 0). Then, analogous to the univariate normal distribution
case, we can deﬁne a bijection FN : H N
FN(µ, Σ) = ( µ1√

F → P2N as follows:
µN√
2 , σ1,··· ,
2 , σN)

(14)

Hence, we can use our formulation in Section 2 since there is a diﬀeomorphism
between P2N and H2N. But, for general non-diagonal N−dimensional covari-
ance matrix space, SP D(N), the above formulation does not hold. This mo-
tivated us to go one step further to search for a parametrization of SP D(N)
where we can use the above formulation. In [15], authors have used the Iwasawa
coordinates to parametrize SP D(N). Using the Iwasawa coordinates [25], we
can get a one-to-one mapping between SP D(N) and the product manifold of
P D(N) and U(N − 1), where P D(N) is manifold of N−dimensional diagonal
positive deﬁnite matrix and U(N − 1) is the space of (N − 1)−dimensional up-
per triangular matrices, which is isomorphic to RN(N+1)/2. We have used the
formulation in [25], as discussed below.
Let Y = VN ∈ SP D(N), then we can use Iwasawa decomposition to rep-
resent VN as a tuple (VN−1, xN−1, wN−1). And repeating the following partial
Iwasawa decomposition:

(cid:18)I xN−1

(cid:19)T(cid:18)VN−1

VN =

0

1

(cid:19)

0

wN−1

0

(15)

where wN−1 > 0 and xN−1 ∈ RN−1. We get the following vectorized expression:
VN 7→ (((w0, xt1, w1), xt2, w2),··· , xt
N−1, wN−1). Note that as each of wi is > 0,
we can construct a positive deﬁnite diagonal matrix with ith diagonal entry
being wi. And as each xi is in Ri, we will arrange them columnwise to form a
upper triangular matrix. Thus, for P D(N), we can use our formulation for the

11

Method
CCM-EPGA
PGA
exact PGA

avg. proj. error Time(s)

7.73e − 03

0.14
0.09

0.09
0.05
732

Table 3: Comparison results on Brodatz database

hyperboloid model of the hyperbolic space given in Section 2, and the standard
PCA can be applied for RN(N+1)/2.

We now use the above formulation to compute the principal geodesic sub-
manifolds for a covariance descriptor representation of Brodatz texture dataset
[5]. Similar to our previous experiment on point-set data, in this experiment, we
report the average projection error and computation time. We adopt a similar
procedure as in [11] to derive the covariance descriptors for the texture images in
the Brodatz database. Each 256 × 256 texture image is ﬁrst partitioned into 64
non-overlapping 8×8 blocks. Then, for each block, the covariance matrix (F F T )
is summed over the blocks. Here, the covariance matrix is computed from the
. We make the covariance matrix
feature vector F =
positive deﬁnite by adding a small positive diagonal matrix. Then, each im-
age is represented as a normal distribution with zero mean and this computed
covariance matrix. Then, we used the above formulation to map each normal
distribution on to H10. The comparative results of CCM-EPGA with PGA and
exact PGA are presented in Table 3. The results clearly indicate eﬃciency and
accuracy of CCM-EPGA compared to PGA and exact PGA algorithms.

(cid:19)t
∂y2|
∂x2|,| ∂2I

I,| ∂I

∂x|,| ∂I

∂y|,| ∂2I

(cid:18)

4 Data Reconstruction from principal directions

and coeﬃcients

In this section, we discuss a recursive scheme to approximate an original data
point with principal directions and coeﬃcients. We discuss the reconstruction
for data on SN, the reconstruction for data points on HN can be done in an
analogous manner. Let xj ∈ SN be the jth data point and vk the kth principal
j is the kth principal component of xj. Note that on SN, kth principal
vector. ¯xk
component of a data point is y(vk, xj). Let xk
j be the approximated xj from the
/kLogµxk−1
k. Let ¯wk−1
ﬁrst k principal components. Let wk−1
be the parallel transported vector wk−1
j . Let, ¯vk be the parallel
transported version of vk to xk−1
. We refer readers to Fig. 3 for a geometric
interpretation.
Now, we will formulate a recursive scheme to reconstruct xj. Let us re-
construct the data using the ﬁrst (k − 1) principal components. Then, the kth
, ¯vk
approximated point xk

j is the intersection of two geodesics deﬁned by xk−1

be Logµxk−1
from µ to ¯xk

j

j

j

j

j

j

j

12

Figure 3: Approximation of xj from the ﬁrst k principal components.

and ¯xk

j , ¯wk−1

j

. Let these two great circles be denoted by

G1(t) = cos(t)xk−1
G2(u) = cos(u)¯xk

j + sin(t) ¯vk
j + sin(u) ¯wk−1
At t = α1 and u = α2, let G1(α1) = G2(α2) = xk

j

mutually orthogonal, we get,

(16)
(17)

j . Since, ¯vk and ¯wk−1

j

are

tan(α1) tan(α2) =< xk−1

j >< ¯xk
Note that, as our goal is to solve for α1 or α2 to get xk
The second equation can be derived as follows:

, ¯wk−1

j

j , ¯vk >
j , we need two equations.

(18)

d(µ, G1(α1)) = d(µ, G2(α2))

This leads to,

cos(α2) =

cos(α1) < µ, xk−1

j >

< µ, ¯xk

j >

Then, by solving Eqs. (18) and (19) we get,

a cos4(α1) + b cos2(α1) + d = 0

(19)

(20)

where,

and

a =< µ, xk−1

j >2< xk−1

j

, ¯wk−1

j >2< ¯xk

j , ¯vk >2
j >2

− < µ, xk−1

b =< µ, ¯xk

j >2 + < µ, xk−1

j >2

d = − < µ, ¯xk

j >2

13

By solving the equation (20), we get

(cid:18)s

−b +p(b2 − 4ad)sgn(a)

(cid:19)

(21)

α1 = arccos

2a

where, sgn(.) is the signum function. Hence, xk
j = G1(α1). This completes
the reconstruction algorithm. Our future eﬀorts will be focused on using this
reconstruction algorithm in a variety of applications mentioned earlier.

5 Conclusions
In this paper, we presented an eﬃcient and accurate exact-PGA algorithm for
(non-zero) constant curvature manifolds, namely the hypersphere Sn and the
hyperbolic space H n. We presented analytic expressions for the shortest dis-
tance between the data point and the geodesic submanifolds, which is required
in the PGA algorithm and in general involves solving a diﬃcult optimization
problem. Using these analytic expressions, we achieve a much more accurate and
eﬃcient solution for PGA on constant curvature manifolds, that are frequently
encountered in Computer Vision, Medical Imaging and Machine Learning tasks.
We presented comparison results on synthetic and real data sets demonstrating
favorable performance of our algorithm in comparison to the state-of-the-art.

References
[1] B. Afsari. Riemannian Lˆ{p} center of mass: Existence, uniqueness, and convex-

ity. Proceedings of the American Mathematical Society, 139(2):655–673, 2011.

[2] S.-i. Amari and H. Nagaoka. Methods of information geometry, volume 191.

American Mathematical Soc., 2007.

[3] I. Bogdanova, X. Bresson, J.-P. Thiran, and P. Vandergheynst. Scale space anal-
Image Processing, IEEE

ysis and active contours for omnidirectional images.
Transactions on, 16(7):1888–1901, 2007.

[4] W. M. Boothby. An introduction to diﬀerentiable manifolds and Riemannian

geometry, volume 120. Academic press, 1986.

[5] P. Brodatz. Textures: a photographic album for artists and designers. Dover

Pubns, 1966.

[6] S. I. Costa, S. A. Santos, and J. E. Strapasson. Fisher information distance: a

geometrical reading. Discrete Applied Mathematics, 2014.

[7] Y. Deng, A. Rangarajan, S. Eisenschenk, and B. C. Vemuri. A riemannian frame-
work for matching point clouds represented by the schr¨odinger distance transform.
In Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on,
pages 3756–3761. IEEE, 2014.

[8] P. T. Fletcher, C. Lu, S. M. Pizer, and S. Joshi. Principal geodesic analysis for
the study of nonlinear statistics of shape. Medical Imaging, IEEE Transactions
on, 23(8):995–1005, 2004.

[9] M. Fr´echet. Les ´el´ements al´eatoires de nature quelconque dans un espace dis-
tanci´e. In Annales de l’institut Henri Poincar´e, volume 10, pages 215–310. Presses
universitaires de France, 1948.

14

[10] R. Hartley, J. Trumpf, Y. Dai, and H. Li. Rotation averaging.

International

journal of computer vision, 103(3):267–305, 2013.

[11] J. Ho, Y. Xie, and B. Vemuri. On a nonlinear generalization of sparse coding
and dictionary learning. In Proceedings of The 30th International Conference on
Machine Learning, pages 1480–1488, 2013.

[12] S. Huckemann, T. Hotz, and A. Munk. Intrinsic shape analysis: Geodesic pca for
riemannian manifolds modulo isometric lie group actions. Statistica Sinica, 2010.
[13] S. Huckemann and H. Ziezold. Principal component analysis for riemannian man-
ifolds, with an application to triangular shape spaces. Advances in Applied Prob-
ability, pages 299–319, 2006.

[14] B. Iversen. Hyperbolic geometry, volume 25. Cambridge University Press, 1992.
[15] B. Jian and B. C. Vemuri. Metric learning using iwasawa decomposition.
In
Computer Vision, 2007. ICCV 2007. IEEE 11th International Conference on,
pages 1–6. IEEE, 2007.

[16] I. Jolliﬀe. Principal component analysis. Wiley Online Library, 2002.
[17] E. L. Lehmann and G. Casella. Theory of point estimation, volume 31. Springer

Science & Business Media, 1998.

[18] R. Lenz, P. L. Carmona, and P. Meer. The hyperbolic geometry of illumination-
In Computer Vision and Pattern Recognition,

induced chromaticity changes.
2007. CVPR’07. IEEE Conference on, pages 1–6. IEEE, 2007.

[19] R. Lenz and G. Granlund. If i had a ﬁsheye i would not need so (1, n), or is
hyperbolic geometry useful in image processing. In Proc. of the SSAB Symposium,
Uppsala, Sweden, pages 49–52, 1998.

[20] K. Mardia and I. Dryden. Shape distributions for landmark data. Advances in

Applied Probability, pages 742–755, 1989.

[21] A. Rangarajan. https://www.cise.ufl.edu/˜anand/publications.html.
[22] S. Said, N. Courty, N. Le Bihan, and S. J. Sangwine. Exact principal geodesic
In 15th European Signal Processing Conference

analysis for data on so (3).
(EUSIPCO-2007), pages 1700–1705. EURASIP, 2007.

[23] S. Sommer. https://github.com/nefan/smanifold, 2014.
[24] S. Sommer, F. Lauze, S. Hauberg, and M. Nielsen. Manifold valued statistics,
exact principal geodesic analysis and the eﬀect of linear approximations. In Com-
puter Vision–ECCV 2010, pages 43–56. Springer, 2010.

[25] A. Terras. Harmonic analysis on symmetric spaces and applications. Springer,

1985.

[26] M. E. Tipping and C. M. Bishop. Probabilistic principal component analy-
sis. Journal of the Royal Statistical Society: Series B (Statistical Methodology),
61(3):611–622, 1999.

[27] G. Uhlmann. Inverse Problems and Applications: Inside Out II, volume 60. Cam-

bridge University Press, 2013.

[28] Y. Wang, W. Dai, X. Gu, T. F. Chan, S.-T. Yau, A. W. Toga, and P. M. Thomp-
son. Teichm¨uller shape space theory and its application to brain morphometry.
In Medical Image Computing and Computer-Assisted Intervention–MICCAI 2009,
pages 133–140. Springer, 2009.

[29] W. Zeng, D. Samaras, and X. D. Gu. Ricci ﬂow for 3d shape analysis. Pattern
Analysis and Machine Intelligence, IEEE Transactions on, 32(4):662–677, 2010.
[30] M. Zhang and P. T. Fletcher. Probabilistic principal geodesic analysis. In Ad-

vances in Neural Information Processing Systems, pages 1178–1186, 2013.

15

