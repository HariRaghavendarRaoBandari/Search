6
1
0
2

 
r
a

 

M
0
1

 
 
]
T
S
h
t
a
m

.

[
 
 

1
v
4
7
4
3
0

.

3
0
6
1
:
v
i
X
r
a

Submitted to the Annals of Statistics
arXiv: arXiv:0000.0000

ACCURACY ASSESSMENT FOR HIGH-DIMENSIONAL

LINEAR REGRESSION∗

By T. Tony Cai, and Zijian Guo

University of Pennsylvania

This paper considers point and interval estimation of the (cid:96)q loss
of an estimator in high-dimensional linear regression with random
design. Both the setting of known identity design covariance matrix
and known noise level and the setting of unknown design covariance
matrix and noise level are studied. We establish the minimax conver-
gence rate for estimating the (cid:96)q loss and the minimax expected length
of conﬁdence intervals for the (cid:96)q loss of a broad collection of estima-
tors of the regression vector. We also investigate the adaptivity of the
conﬁdence intervals for the (cid:96)q loss. The results reveal interesting and
signiﬁcant diﬀerences between estimating the (cid:96)2 loss and (cid:96)q loss with
1 ≤ q < 2 as well as the diﬀerences between the two settings.

A major step in our analysis is to establish rate sharp lower bounds
for the minimax estimation error and the expected length of minimax
and adaptive conﬁdence intervals for the (cid:96)q loss, which requires the
development of new technical tools. A signiﬁcant diﬀerence between
loss estimation and the traditional parameter estimation is that for
loss estimation the constraint is on the performance of the estimator
of the regression vector, but the lower bounds are on the diﬃculty
of estimating its (cid:96)q loss. The technical tools developed in this paper
can also be of independent interest.

1. Introduction.

In many applications, the goal of statistical inference
is not only to construct a good estimator, but also to provide a measure of
accuracy for this estimator. In classical statistics, when the parameter of
interest is one-dimensional, this is achieved in the form of a standard er-
ror or a conﬁdence interval. A prototypical example is the inference for a
binomial proportion, where often not only an estimate of the proportion
but also its margin of error are given. Accuracy measures of an estimation
procedure have also been used as a tool for the empirical selection of tun-
ing parameters. A well known example is Stein’s Unbiased Risk Estimate
(SURE), which has been an eﬀective tool for the construction of data-driven
∗The research was supported in part by NSF Grants DMS-1208982 and DMS-1403708,

and NIH Grant R01 CA127334.

MSC 2010 subject classiﬁcations: Primary 62G15; secondary 62C20, 62H35
Keywords and phrases: Accuracy assessment, adaptivity, conﬁdence interval, high-
dimensional linear regression, loss estimation, minimax lower bound, minimaxity, rate
of convergence, sparsity.

1

2

T. T. CAI AND Z. GUO

adaptive estimators in normal means estimation, nonparametric signal re-
covery, covariance matrix estimation, and other problems. See, for instance,
[15, 12, 9, 7, 21]. The commonly used cross-validation methods for choosing
tuning parameters can also be viewed as a useful tool based on the idea of
empirical assessment of accuracy.

In this paper, we consider the problem of estimating the loss of a given
estimator in the setting of high-dimensional linear regression, where one
observes (X, y) with X ∈ Rn×p and y ∈ Rn, and for 1 ≤ i ≤ n,

yi = Xi·β + i.

Here β ∈ Rp is the regression vector, Xi· iid∼ Np(0, Σ) are the rows of X,
iid∼ N (0, σ2) are independent of X. This high dimensional
and the errors i
linear model has been well studied in the literature, with the main focus
on estimation of β. Several penalized/constrained (cid:96)1 minimization methods,
including Lasso [18], Dantzig selector [8], scaled Lasso [16] and square-root
Lasso [2], have been proposed. These methods have been shown to work well
in applications and produce interpretable estimates of β when β is assumed
to be sparse. Theoretically, with a properly chosen tuning parameter, these
For a given estimator (cid:98)β, the (cid:96)q loss (cid:107)(cid:98)β − β(cid:107)2
estimators achieve the optimal rate of convergence over collections of sparse
parameter spaces. See, for example, [8, 16, 2, 14, 3, 4, 19].
used as a metric of accuracy for (cid:98)β. We consider in the present paper both
q with 1 ≤ q ≤ 2 is commonly
point and interval estimation of the (cid:96)q loss (cid:107)(cid:98)β − β(cid:107)2
q for a given (cid:98)β. Note
that the loss (cid:107)(cid:98)β − β(cid:107)2
mator (cid:98)β and the parameter β. Usually, prediction and prediction interval
loss (cid:107)(cid:98)β− β(cid:107)2
q. Since the (cid:96)q loss depends on the estimator (cid:98)β, it is necessary to
paper, we restrict our attention to a broad collection of estimators (cid:98)β that

are used for point and interval estimation of a random quantity. However,
we slightly abuse the terminologies in the present paper by using estimation
and conﬁdence interval to represent the point and interval estimator of the

q is a random quantity, depending on both the esti-

specify the estimator in the discussion of loss estimation. Throughout this

perform well at least at one interior point or a small subset of the parame-
ter space. This collection of estimators includes most state-of-art estimators
such as Lasso, Dantzig selector, scaled Lasso and square-root Lasso.

High-dimensional linear regression has been well studied in two settings.
One is the setting with known design covariance matrix Σ = I and known
noise level σ = σ0 and sparse β. See for example, [10, 1, 13, 19, 17, 11, 6].
Another commonly considered setting is sparse β with unknown Σ and σ. In
q in

this paper, we consider point and interval estimation of the (cid:96)q loss (cid:107)(cid:98)β−β(cid:107)2

HIGH-DIMENSIONAL ACCURACY ASSESSMENT

3

both settings. Speciﬁcally, we consider the parameter space Θ0(k) introduced
in (2.3) consisting of k-sparse signal β with known design covariance matrix
Σ = I and known noise level σ = σ0, and the parameter space Θ(k) deﬁned
in (2.4) which consists of k-sparse signals with unknown Σ and σ.

minimax and adaptive estimation of the loss (cid:107)(cid:98)β−β(cid:107)2

q for a given estimator (cid:98)β

1.1. Our contributions. The goals of the present paper are to study the

and the minimax expected length and the adaptivity of conﬁdence intervals
for the loss. A major step in our analysis is to establish rate sharp lower
bounds for the minimax estimation error and minimax expected length of
conﬁdence intervals for the (cid:96)q loss over Θ0(k) and Θ(k) for a broad collec-
tion of estimators of β. We then take the Lasso and scaled Lasso estimators
as speciﬁc examples and propose procedures for point estimation as well as
conﬁdence intervals for their (cid:96)q losses. It is shown that the proposed pro-
cedures achieve their corresponding lower bounds up to a constant factor.
These results together establish the minimax rates of estimating the (cid:96)q loss
over Θ0(k) and Θ(k). The analysis shows interesting and signiﬁcant diﬀer-
ences between estimating the (cid:96)2 loss and (cid:96)q loss with 1 ≤ q < 2 as well as
the diﬀerences between the two parameter spaces Θ(k) and Θ0(k).

• The minimax rates of estimating the (cid:96)2 loss, (cid:107)(cid:98)β − β(cid:107)2

and over Θ(k) is k log p

2, over Θ0(k) is
n . So loss estimation is much
√
log p (cid:28) k (cid:46)
• The minimax rate of estimating the (cid:96)q loss with 1 ≤ q < 2 over both

easier with the prior information Σ = I and σ = σ0 when
n
log p .

(cid:110) 1√

n , k log p

n

(cid:111)

min

n

Θ0(k) and Θ(k) is k

2
q log p
n .
√
log p (cid:28) k (cid:46) n

n

In the regime

log p , a practical loss estimator is proposed for
estimating the (cid:96)2 loss and shown to achieve the optimal convergence rate 1√
n
adaptively over Θ0(k). We say estimation of loss is impossible if the minimax
rate can be achieved by a trivial estimator, say, 0. In all other considered
cases, estimation of loss is shown to be impossible. These results indicate
that loss estimation is diﬃcult.

We then turn to construction of conﬁdence intervals for the (cid:96)q loss. A
conﬁdence interval for the loss is useful even when it is “impossible” to es-
timate the loss, as a conﬁdence interval can provide non-trivial upper and
lower bounds for the loss. In terms of convergence rate over Θ0(k) or Θ(k),

the minimax rate of the expected length of conﬁdence intervals for (cid:107)(cid:98)β − β(cid:107)2

q
coincides with the minimax estimation rate. We also consider the adaptivity
of conﬁdence intervals. (The framework for adaptive conﬁdence intervals is

4

T. T. CAI AND Z. GUO

n

n

discussed in detail in Section 3.1.) Regarding conﬁdence intervals for the
(cid:96)2 loss in the case of known Σ = I and σ = σ0, a procedure is proposed
and is shown to achieve the optimal length 1√
n adaptively over Θ0(k) for
√
log p (cid:28) k (cid:46) n
log p . Furthermore, it is shown that this is the only regime where
adaptive conﬁdence intervals exist, even over two given parameter spaces.
For example, when k1 (cid:28) √
log p and k1 (cid:28) k2, it is impossible to construct a
conﬁdence interval for the (cid:96)2 loss with guaranteed coverage probability over
Θ0(k2) (consequently also over Θ0(k1)) and with expected length automat-
ically adjusted to the sparsity. Similarly, for the (cid:96)q loss with 1 ≤ q < 2,
construction of adaptive conﬁdence intervals is impossible over Θ0(k1) and
Θ0(k2) for k1 (cid:28) k2 (cid:46) n
log p . Regarding conﬁdence intervals for the (cid:96)q loss with
1 ≤ q ≤ 2 in the case of unknown Σ and σ, the impossibility of adaptivity
also holds over Θ(k1) and Θ(k2) for k1 (cid:28) k2 (cid:46) n
log p .

Establishing rate-optimal lower bounds for the minimax estimation error
and the expected length of minimax and adaptive conﬁdence intervals for the
(cid:96)q loss requires the development of new technical tools. One main diﬀerence
between loss estimation and the traditional parameter estimation is that for

loss estimation the constraint is on the performance of the estimator (cid:98)β of
its loss (cid:107)(cid:98)β − β(cid:107)2
of adaptive conﬁdence intervals for the loss (cid:107)(cid:98)β− β(cid:107)2

the regression vector β, but the lower bound is on the diﬃculty of estimating
q in terms of the minimax estimation error and the expected
length of conﬁdence intervals. Conventional lower bound methods are either
not applicable or do not yield rate-optimal results. We introduce useful lower
bound techniques for the minimax estimation error and the expected length
q. In one interesting case,
it is necessary to test a composite null against a composite alternative in
order to establish rate-optimal lower bounds. The technical tools developed
in this paper can also be of independent interest.

1.2. Comparison with other works. Statistical inference on the loss of
speciﬁc estimators of β has been considered in the recent literature. In
particular, the limit of a normalized loss has been studied in [10, 1, 17].
More speciﬁcally, the papers [10, 1] established, in the setting Σ = I and
n

p → δ ∈ (0,∞), the limit of the normalized loss 1

p(cid:107)(cid:98)β(λ) − β(cid:107)2

2 where (cid:98)β(λ)

is the Lasso estimator with a pre-speciﬁed tuning parameter λ. Although
[10, 1] provided an exact asymptotic expression of the normalized loss, the
limit itself depends on the unknown β. In a similar setting, the paper [17]
established the limit of a normalized (cid:96)2 loss of the square-root Lasso estima-
tor in the case Σ = I. Such a limit also depends on the true signal β. These
limits of the normalized losses help understand the theoretical properties
of the corresponding estimators of β, but they do not lead to an estimate

HIGH-DIMENSIONAL ACCURACY ASSESSMENT

5

the (cid:96)2 loss (cid:107)(cid:98)β − β(cid:107)2

of the loss. Comparing the above line of work with the present paper, our
results imply that although these normalized losses have a limit under some
regularity conditions, such losses cannot be estimated well in most settings.
In a recent paper [11], the authors constructed a conﬁdence interval for
2 in the case of known design covariance matrix Σ =
I and unknown noise level σ. They considered the moderate-dimensional
setting where the dimension p is of the same order as the sample size n,
i.e., n/p → ξ ∈ (0, 1) and no sparsity is assumed on β. No optimality on
the expected length of conﬁdence intervals is discussed. By further assuming
that β is sparse, an application of the convex algorithm proposed in [11] can
produce a conﬁdence interval of length 1√
n . However, taking the scaled Lasso
2 , then the
minimax expected length of conﬁdence intervals over the parameter space
{(β, I, σ) : (cid:107)β(cid:107)0 ≤ k, σ ≤ M2} is of order k log p
n in
the regime n/p → ξ ∈ (0, 1) and k = cpγ with 0 ≤ γ < 1
2 , hence a simple
application of the conﬁdence interval constructed in [11] does not achieve
the optimal rate.

estimator (cid:98)β as an example, if k = cpγ for some constant 0 ≤ γ < 1

n . Note that k log p

n (cid:28) 1√

2 establishes the minimax convergence rate for estimating the loss (cid:107)(cid:98)β − β(cid:107)2
tion of (cid:107)(cid:98)β − β(cid:107)2
adaptivity of conﬁdence intervals for (cid:107)(cid:98)β − β(cid:107)2
lower bounds of estimating the loss (cid:107)(cid:98)β − β(cid:107)2

1.3. Organization. The rest of this paper is organized as follows. Section
with 1 ≤ q ≤ 2 over both Θ0(k) and Θ(k). We then turn to interval estima-
q. Sections 3 and 4 present the minimax expected length and
q over the parameter spaces
Θ0(k) and Θ(k), respectively. Section 5 presents general tool for minimax
q. Section 6 compares the mini-
maxity and adaptivity behaviors over Θ(k) and Θ0(k). Section 7 proves the
main lower bound results and more proofs of lower bound and upper bound
results are presented in the supplemental material [5].

q

1.4. Notation. For a matrix X ∈ Rn×p, Xi·, X·j, and Xi,j denote re-
spectively the i-th row, j-th column, and (i, j) entry of the matrix X. For
a subset J ⊂ {1, 2,··· , p}, |J| denotes the cardinality of J, J c denotes the
complement {1, 2,··· , p}\J, XJ denotes the submatrix of X consisting of
columns X·j with j ∈ J and for a vector x ∈ Rp, xJ is the subvector of
x with indices in J. For a vector x ∈ Rp, supp(x) denotes the support of
q for q ≥ 0 with
(cid:107)x(cid:107)0 = |supp(x)| and (cid:107)x(cid:107)∞ = max1≤j≤p |xj|. For a ∈ R, a+ = max{a, 0}.
We use max(cid:107)X·j(cid:107)2 as a shorthand for max1≤j≤p (cid:107)X·j(cid:107)2 and min(cid:107)X·j(cid:107)2 as a
shorthand for min1≤j≤p (cid:107)X·j(cid:107)2. For a matrix A, (cid:107)A(cid:107)2 = sup(cid:107)x(cid:107)2=1 (cid:107)Ax(cid:107)2 is
the spectral norm; For a symmetric matrix A, λmin (A) and λmax (A) denote

x and the (cid:96)q norm of x is deﬁned as (cid:107)x(cid:107)q = ((cid:80)q

i=1 |xi|q)

1

6

T. T. CAI AND Z. GUO

respectively the smallest and largest eigenvalue of A. We use c and C to
denote generic positive constants that may vary from place to place. For
two positive sequences an and bn, an (cid:46) bn means an ≤ Cbn for all n and
an (cid:38) bn if bn (cid:46) an and an (cid:16) bn if an (cid:46) bn and bn (cid:46) an, and an (cid:28) bn if
lim supn→∞ an
bn

= 0 and an (cid:29) bn if bn (cid:28) an.

2. Minimax estimation of the (cid:96)q loss.

presenting the minimax framework for estimating the (cid:96)q loss, (cid:107)(cid:98)β − β(cid:107)2
a given estimator (cid:98)β, and then establish the minimax lower bound for the
estimation error for a broad collection of estimators (cid:98)β. We also show that

In this section, we begin by
q, of

such minimax lower bound can be achieved for the Lasso and scaled Lasso
estimators.

2.1. Problem formulation. Recall the high-dimensional linear model,

(2.1)

yn×1 = Xn×pβp×1 + n×1,

 ∼ Nn(0, σ2I).
iid∼ N (0, Σ) and Xi· and  are

We focus on the random design with Xi·

independent. Let Z = (X, y) denote the observed data and (cid:98)β be a given
estimator of β. Denoting by (cid:98)Lq(Z) any estimator of the loss (cid:107)(cid:98)β − β(cid:107)2
minimax rate of convergence for estimating (cid:107)(cid:98)β− β(cid:107)2
Θ is deﬁned as the largest quantity γ(cid:98)β,(cid:96)q
(cid:16)|(cid:98)Lq(Z) − (cid:107)(cid:98)β − β(cid:107)2
q| ≥ γ(cid:98)β,(cid:96)q
for some positive constant δ not depending on n or p. We shall write (cid:98)Lq for
(cid:98)Lq(Z) when there is no confusion.

q, the
q over a parameter space

(cid:17) ≥ δ,

(Θ) such that

(2.2)

inf(cid:98)Lq

Pθ

sup
θ∈Θ

(Θ)

We denote the parameter by θ = (β, Σ, σ), which consists of the signal β,
the design covariance matrix Σ and the noise level σ. For a given parameter
θ = (β, Σ, σ), we use β(θ) to denote the corresponding β of this parameter.
The problem of loss estimation is studied in two settings: One is the setting
with known design covariance matrix Σ = I and known noise level σ = σ0
and the other is unknown Σ and σ. In the ﬁrst setting, we consider the
following parameter space that consists of k-sparse signals,

(2.3)

Θ0(k) = {(β, I, σ0) : (cid:107)β(cid:107)0 ≤ k} ,

and in the second setting, we consider
(2.4)

(cid:26)

Θ(k) =

(β, Σ, σ) : (cid:107)β(cid:107)0 ≤ k,

1
M1

≤ λmin (Σ) ≤ λmax (Σ) ≤ M1, 0 < σ ≤ M2

(cid:27)

,

HIGH-DIMENSIONAL ACCURACY ASSESSMENT

7

where M1 ≥ 1 and M2 > 0 are constants. The parameter space Θ0(k) is a
proper sub-space of Θ(k), which consists of k-sparse signals with unknown
≤ λmin (Σ) ≤ λmax (Σ) ≤ M1 and 0 < σ ≤ M2
Σ and σ. The conditions
are two regularity conditions on the covariance matrix and the noise level.

(Θ) of estimating (cid:107)(cid:98)β − β(cid:107)2
Note that the minimax convergence rate γ(cid:98)β,(cid:96)q
also depends on the particular estimator (cid:98)β. Diﬀerent estimators (cid:98)β could lead
to diﬀerent losses (cid:107)(cid:98)β − β(cid:107)2
loss (cid:107)(cid:98)β − β(cid:107)2

q varies with the estimator (cid:98)β.

q and in general the diﬃculty of estimating the

1
M1

q

We ﬁrst recall the properties of some state-of-art estimators in high-
dimensional linear regression and then specify the collection of estimators
that we focus on in this paper. As shown in [8, 3, 2, 16], Lasso, Dantzig
Selector, scaled Lasso and square-root Lasso satisfy the following property
if the tuning parameter is chosen properly,
q ≥ Ck

(cid:18)
(cid:107)(cid:98)β − β(cid:107)2

→ 0,

(cid:19)

(2.5)

log p

Pθ

2
q

n

sup
θ∈Θ(k)

n

2
q log p

where C > 0 is a constant. The minimax lower bounds established in [19,
14, 20] imply that k
is the minimax optimal rate of estimating β.
It should be stressed that all of these algorithms do not require a prior
knowledge of the sparsity k and is thus adaptive to the sparsity provided
k (cid:46) n
log p . Throughout the paper, we consider a broad collection of estimators

(cid:98)β satisfying one of the following two assumptions.
(A1) The estimator (cid:98)β satisﬁes the following property at a single point θ0 =

where 0 ≤ α0 < 1

(A2) The estimator(cid:98)β satisﬁes the following property on the set {θ = (β∗, I, σ) , σ ≤ 2σ0},

4 and C > 0 are constants and σ0 > 0 is given.

(2.7)

inf

{θ=(β∗,I,σ):σ≤2σ0}

q ≤ C(cid:107)β∗(cid:107) 2

q
0

log p

n

σ2

≥ 1 − α0,

where 0 ≤ α0 < 1

4 and C > 0 are constants and σ0 > 0 is given.

In view of the minimax rate given in (2.5), the assumption (A1) requires

(cid:98)β to be a good estimator of β at least at a single point {θ0}. The assumption
(A2) is slightly stronger than (A1) and requires (cid:98)β to estimate β well for a
any rate-optimal estimator (cid:98)β satisfying (2.5) satisﬁes both (A1) and (A2) .

single β∗ but over a range of noise levels σ ≤ 2σ0 while Σ = I. Of course,

(β∗, I, σ0),

(2.6)

Pθ0

(cid:18)
(cid:107)(cid:98)β − β∗(cid:107)2
q ≤ C(cid:107)β∗(cid:107) 2
(cid:18)
(cid:107)(cid:98)β − β∗(cid:107)2

Pθ

q
0

(cid:19)

log p

n

σ2
0

≥ 1 − α0,

(cid:19)

8

T. T. CAI AND Z. GUO

rem establishes the minimax lower bound for estimating the loss (cid:107)(cid:98)β − β(cid:107)2

2.2. Minimax estimation of the (cid:96)q loss over Θ0(k). The following theo-

q

over the parameter space Θ0 (k).

(cid:110)

(cid:111)

c min

Theorem 1. Suppose that 1 ≤ q ≤ 2, k0 (cid:28) min{k,

√
log p} and k ≤
(cid:98)β satisfying the assumption (A1) with (cid:107)β∗(cid:107)0 ≤ k0, then we have
2 . For any estimator
(cid:27)

for some constants c > 0 and 0 ≤ γ < 1

pγ, n
log p

(cid:26)

(cid:19)

n

(cid:18)
|(cid:98)L2 − (cid:107)(cid:98)β − β(cid:107)2

θ∈Θ0(k)

For any estimator (cid:98)β satisfying the assumption (A2) with (cid:107)β∗(cid:107)0 ≤ k0, then

n

log p

k

,

1√
n

2| ≥ c min

inf(cid:98)L2

sup

Pθ

≥ δ;

σ2
0

(2.8)

we have

(2.9)

inf(cid:98)Lq

Pθ

sup

θ∈Θ0(k)

(cid:18)
|(cid:98)Lq − (cid:107)(cid:98)β − β(cid:107)2

(cid:19)

q| ≥ ck

2
q

log p

n

σ2
0

≥ δ,

for 1 ≤ q < 2,

where δ > 0 and c > 0 are positive constants.

Theorem 1 establishes the minimax lower bounds for estimating the (cid:96)2

(A2). We will take the Lasso estimator as an example and demonstrate
the implications of the above theorem. We randomly split Z = (y, X) into

2 of any estimator (cid:98)β satisfying the assumption (A1) and the (cid:96)q
loss (cid:107)(cid:98)β − β(cid:107)2
q with 1 ≤ q < 2 of any estimator (cid:98)β satisfying the assumption
loss (cid:107)(cid:98)β − β(cid:107)2
subsamples Z(1) =(cid:0)y(1), X (1)(cid:1) and Z(2) =(cid:0)y(2), X (2)(cid:1) with sample sizes n1
and n2, respectively. The Lasso estimator (cid:98)βL based on the ﬁrst subsample
Z(1) =(cid:0)y(1), X (1)(cid:1) is deﬁned as
(cid:114) log p
(cid:98)βL = arg min

|βj| with λ = A

(cid:107)y(1) − X (1)β(cid:107)2

(cid:107)X (1)·j (cid:107)2√

p(cid:88)

(2.10)

+ λ

σ0,

2

n1

n1

j=1

n1

(cid:16)|0 − (cid:107)(cid:98)βL − β(cid:107)2

ply that the estimation of the (cid:96)q loss (cid:107)(cid:98)βL − β(cid:107)2
where A >
2 is a pre-speciﬁed constant. Without loss of generality, we
assume n1 (cid:16) n2. For the case 1 ≤ q < 2, (2.5) and (2.9) together im-
(cid:17) → 0.
q is impossible since the
lower bound can be achieved by the trivial estimator of the loss, 0. That is,
supθ∈Θ0(k)
For the case q = 2, in the regime k (cid:28) √
(cid:107)(cid:98)βL − β(cid:107)2

in
(2.8) can be achieved by the zero estimator and hence estimation of the loss

2 is impossible. However, the interesting case is when

log p , the lower bound k log p

√
log p (cid:28) k (cid:46)

q| ≥ Ck

2
q log p

Pθ

n

n

n

n

β∈Rp
√

9

n2

(2.11)

(cid:19)

(cid:18) 1

(cid:101)L2 =

n
bound 1√

HIGH-DIMENSIONAL ACCURACY ASSESSMENT

n in (2.8), which cannot be achieved by the zero estimator. We now

log p , the loss estimator (cid:101)L2 proposed in (2.11) achieves the minimax lower
detail the construction of the loss estimator (cid:101)L2. Based on the second half
sample Z(2) =(cid:0)y(2), X (2)(cid:1), we propose the following estimator,
(cid:13)(cid:13)(cid:13)y(2) − X (2)(cid:98)βL(cid:13)(cid:13)(cid:13)2
Note that the ﬁrst subsample Z(1) =(cid:0)y(1), X (1)(cid:1) is used to produce the Lasso
estimator (cid:98)βL deﬁned in (2.10) and the second subsample Z(2) =(cid:0)y(2), X (2)(cid:1)
is retained for evaluating the loss (cid:107)(cid:98)βL − β(cid:107)2
The following proposition establishes that the estimator (cid:101)L2 achieves the
log p and (cid:98)βL is the Lasso estima-

2. Such sample splitting tech-
nique is same with cross-validation in nature and has been used in [13] for
constructing conﬁdence sets for β and in [11] for constructing conﬁdence
intervals for the (cid:96)2 loss.

Proposition 1. Suppose that k (cid:46) n

minimax lower bound of (2.8) over the regime

√
log p (cid:28) k (cid:46) n
log p .

− σ2

0

.

+

n

2

√

tor deﬁned in (2.10) with A >
satisﬁes, for any sequence δn,p → ∞,

(2.12)

lim sup
n,p→∞ sup
θ∈Θ0(k)

Pθ

2, then the estimator proposed in (2.11)

(cid:18)(cid:12)(cid:12)(cid:12)(cid:101)L2 − (cid:107)(cid:98)βL − β(cid:107)2

2

(cid:12)(cid:12)(cid:12) ≥ δn,p

(cid:19)

1√
n

= 0.

2.3. Minimax estimation of the (cid:96)q loss over Θ(k). We now turn to the
case of unknown Σ and σ and establish the minimax lower bound for esti-
mating the (cid:96)q loss over the parameter space Θ(k).

(cid:111)
2 . For any estimator (cid:98)β satisfying the

pγ, n
log p

(cid:110)

for

Theorem 2. Suppose that 1 ≤ q ≤ 2, k0 (cid:28) k ≤ c min

some constants c > 0 and 0 ≤ γ < 1
assumption (A1) with (cid:107)β∗(cid:107)0 ≤ k0, then we have
q| ≥ ck

(cid:18)
|(cid:98)Lq − (cid:107)(cid:98)β − β(cid:107)2

(2.13)

Pθ

inf(cid:98)Lq

sup
θ∈Θ(k)

(cid:19)

2
q

log p

n

≥ δ,

where δ > 0 and c > 0 are positive constants.

any estimator (cid:98)β satisfying the assumption (A1), including the scaled Lasso

Theorem 2 provides a minimax lower bound for estimating the (cid:96)q loss of

10

T. T. CAI AND Z. GUO

estimator deﬁned as

(2.14)

{(cid:98)βSL, ˆσ} = arg min
(cid:113) log p

√

β∈Rp,σ∈R+

(cid:107)y − Xβ(cid:107)2

2

2nσ

+

σ
2

+ λ0

p(cid:88)

j=1

(cid:107)X·j(cid:107)2√

n

|βj|,

n with A >

where λ0 = A
2. Note that for the scaled Lasso estimator,
the lower bound in (2.13) can be achieved by the trivial loss estimate 0
in the sense, supθ∈Θ(k)
estimation of loss is impossible in this case.

(cid:17) → 0, and hence

(cid:16)|0 − (cid:107)(cid:98)βSL − β(cid:107)2

q| ≥ Ck

2
q log p

Pθ

n

intervals for (cid:107)(cid:98)β − β(cid:107)2

3. Minimaxity and adaptivity of conﬁdence intervals over Θ0(k).
We focused in the last section on point estimation of the (cid:96)q loss and showed
the impossibility of loss estimation except for one regime. The results natu-
rally lead to the next question: Is it possible to construct “useful” conﬁdence
q that can provide non-trivial upper and lower bounds
for the loss. In this section, after introducing the framework for minimaxity
and adaptivity of conﬁdence intervals, we consider the case of known Σ = I
and σ = σ0 and establish the minimax expected length and adaptivity re-
sults for conﬁdence intervals over the parameter space Θ0(k). Minimaxity
and adaptivity of conﬁdence intervals for the case of unknown Σ and σ will
be the focus of next section.

3.1. Framework for minimaxity and adaptivity of conﬁdence intervals.

In
this section, we introduce the following decision theoretical framework for
q. Given 0 < α < 1 and the parameter
q, denote by Iα
the set of

conﬁdence intervals of the loss (cid:107)(cid:98)β−β(cid:107)2
space Θ and the loss function (cid:107)(cid:98)β − β(cid:107)2
all (1 − α) level conﬁdence intervals for (cid:107)(cid:98)β − β(cid:107)2
(cid:16)
Θ,(cid:98)β, (cid:96)q

(cid:17)
(cid:16)
Θ,(cid:98)β, (cid:96)q
(cid:16)(cid:107)(cid:98)β − β(θ)(cid:107)2

(3.1)
Iα

q over Θ,

q ∈ CIα

(cid:26)

(cid:17)

Pθ

=

= [l (Z) , u (Z)] : inf
θ∈Θ

We will write CIα for CIα

when there is no confusion. For any

conﬁdence interval CIα

= [l (Z) , u (Z)], its length is denoted by
= u (Z) − l (Z) and the maximum expected length over

CIα

L
a parameter space Θ1 is deﬁned as

(cid:16)

(cid:16)(cid:98)β, (cid:96)q, Z
(cid:16)

(3.2)

L

CIα

(cid:16)

(cid:16)(cid:98)β, (cid:96)q, Z

(cid:17)(cid:17)

.

= sup
θ∈Θ1

EθL

CIα

(cid:17)

CIα

(cid:16)(cid:98)β, (cid:96)q, Z
(cid:17)
(cid:16)(cid:98)β, (cid:96)q, Z
(cid:17)
(cid:16)(cid:98)β, (cid:96)q, Z
(cid:17)(cid:17)
(cid:17)
(cid:16)(cid:98)β, (cid:96)q, Z

(cid:17)

, Θ1

(cid:16)(cid:98)β, (cid:96)q, Z

(cid:17)(cid:17) ≥ 1 − α

(cid:27)

.

HIGH-DIMENSIONAL ACCURACY ASSESSMENT

(cid:16)
(cid:17)(cid:17)

α

11

Θ1, Θ2,(cid:98)β, (cid:96)q

(cid:17)

,

.

α

α

α

=

inf

EθL

CIα

(cid:16)

(cid:16)

(cid:16)

sup
θ∈Θ1

for L∗

, which is the minimax

We will write L∗

(cid:16)(cid:98)β, (cid:96)q, Z

Θ1, Θ2,(cid:98)β, (cid:96)q
(cid:16)
Θ1, Θ2,(cid:98)β, (cid:96)q
(cid:17)

(cid:17)
CIα((cid:98)β,(cid:96)q,Z)∈Iα(Θ2,(cid:98)β,(cid:96)q)
(cid:17)
Θ1,(cid:98)β, (cid:96)q
(cid:17)

For two nested parameter spaces Θ1 ⊆ Θ2, we deﬁne the benchmark L∗
measuring the degree of adaptivity over the nested spaces Θ1 ⊂ Θ2,
(3.3)
L∗

(cid:16)
Θ1,(cid:98)β, (cid:96)q

(cid:17)
Θ1, Θ1,(cid:98)β, (cid:96)q
expected length of conﬁdence intervals of (cid:107)(cid:98)β − β(cid:107)2
(cid:16)

q over Θ1. The bench-
mark L∗
is the inﬁmum of the maximum expected length
over Θ1 among all (1 − α)-level conﬁdence intervals over Θ2. In contrast,
is considering all (1 − α)-level conﬁdence intervals over Θ1.
L∗
In words, if there is prior information that the parameter lies in the smaller
parameter space Θ1, L∗
measures the benchmark length of con-
ﬁdence intervals over the parameter space Θ1, which is illustrated in the left
of Figure 1; however, if there is only prior information that the parameter
lies in the larger parameter space Θ2, L∗
measures the bench-
mark length of conﬁdence intervals over the parameter space Θ1, which is
illustrated in the right of Figure 1.

Θ1, Θ2,(cid:98)β, (cid:96)q

Θ1,(cid:98)β, (cid:96)q

(cid:17)

(cid:16)

(cid:16)

(cid:17)

α

α

α

α

(cid:16)
Θ1,(cid:98)β, (cid:96)q

(cid:17)

(cid:16)

Θ1, Θ2,(cid:98)β, (cid:96)q

(cid:17)

.

and L∗

α

Fig 1. The plot demonstrates the deﬁnition of L∗

α

(cid:17)

(cid:16)
Θ1,(cid:98)β, (cid:96)q

(cid:16)

Θ2,(cid:98)β, (cid:96)q
(cid:17)

Rigorously, we deﬁne a conﬁdence interval CI∗ to be simultaneously adap-

tive over Θ1 and Θ2 if CI∗ ∈ Iα
(3.4) L (CI∗, Θ1) (cid:16) L∗
The condition (3.4) means that the conﬁdence interval CI∗ has coverage over
the larger parameter space Θ2 and achieves the minimax rate over both Θ1
and Θ2. Note that L (CI∗, Θ1) ≥ L∗

Θ2,(cid:98)β, (cid:96)q
Θ1, Θ2,(cid:98)β, (cid:96)q

, and L (CI∗, Θ2) (cid:16) L∗

Θ1, Θ2,(cid:98)β, (cid:96)q

(cid:17)
(cid:17) (cid:29)

. If L∗

(cid:16)

(cid:16)

(cid:17)

(cid:16)

α

α

,

.

α

α

ΩΘ(cid:2869)Θ(cid:2870)𝐋(cid:3080)∗(Θ(cid:2869),Θ(cid:2870),(cid:4632)𝛽,ℓ𝓁(cid:3044))Θ(cid:2869)𝐋(cid:3080)∗(Θ(cid:2869),(cid:4632)𝛽,ℓ𝓁(cid:3044))(cid:16)

(cid:17)

α

Θ1,(cid:98)β, (cid:96)q

T. T. CAI AND Z. GUO

12
L∗
, then the rate-optimal adaptation (3.4) is impossible to achieve
for Θ1 ⊂ Θ2. Otherwise, it is possible to construct conﬁdence intervals si-
multaneously adaptive over parameter spaces Θ1 and Θ2. The possibility of
adaptation over parameter spaces Θ1 and Θ2 can thus be answered by in-
vestigating the benchmark quantities L∗
.
Such framework has already been introduced in [6], which studies the mini-
maxity and adaptivity of conﬁdence intervals for linear functionals in high-
dimensional linear regression.

Θ1, Θ2,(cid:98)β, (cid:96)q

Θ1,(cid:98)β, (cid:96)q

and L∗

(cid:16)

(cid:17)

(cid:16)

(cid:17)

α

α

In the following sections, we will adopt the minimax and adaptation

framework discussed above and establish the minimax expected length L∗
and the adaptation benchmark L∗
. In terms of the
minimax expected length and the adaptivity behavior, there exists funda-
mental diﬀerence between the case q = 2 and 1 ≤ q < 2 . We will discuss
them separately in the following two sections.

Θ0(k1), Θ0(k2),(cid:98)β, (cid:96)q

α

α

(cid:17)

(cid:16)

(cid:16)

Θ0(k),(cid:98)β, (cid:96)q

(cid:17)

expected length of conﬁdence intervals of (cid:107)(cid:98)β − β(cid:107)2

3.2. Conﬁdence intervals for the (cid:96)2 loss over Θ0(k). We begin with the
(cid:96)2 loss. The following theorem establishes the minimax lower bound for the
2 over the parameter space

Θ0(k).

Theorem 3. Suppose 0 < α < 1

for some constants c > 0 and 0 ≤ γ < 1
assumption (A1) with (cid:107)β∗(cid:107)0 ≤ k0, then there is some constant c > 0 such
that

(cid:111)

pγ, n
log p

n

(cid:110)
2 . For any estimator (cid:98)β satisfying the
4 , k0 (cid:28) min{k,
(cid:26) k log p

√
log p} and k ≤ c min
(cid:27)

Θ0(k),(cid:98)β, (cid:96)2

(cid:17) ≥ c min

L∗

(3.5)

α

√
2,
then the minimax expected length for (1 − α) level conﬁdence intervals of

In particular, if (cid:98)βL is the Lasso estimator deﬁned in (2.10) with A >
(cid:107)(cid:98)βL − β(cid:107)2

2 over Θ0(k) is

n

,

1√
n

σ2
0.

Θ0(k),(cid:98)βL, (cid:96)2

(cid:17) (cid:16) min

(cid:26) k log p

,

1√
n

n

(cid:27)

(3.6)

L∗

α

(cid:16)

(cid:16)

We now consider adaptivity of conﬁdence intervals for the (cid:96)2 loss. The fol-

lowing theorem gives the lower bound for the benchmark L∗
We will then discuss Theorems 3 and 4 together.

α

σ2
0.

(cid:16)

Θ0(k1), Θ0(k2),(cid:98)β, (cid:96)2

(cid:17)

.

(cid:111)
(cid:16)

(cid:110)

HIGH-DIMENSIONAL ACCURACY ASSESSMENT

13

Theorem 4. Suppose 0 < α < 1

√
log p} and k1 ≤ k2 ≤
tor (cid:98)β satisfying the assumption (A1) with (cid:107)β∗(cid:107)0 ≤ k0, then there is some
2 . For any estima-

for some constants c > 0 and 0 ≤ γ < 1

4 , k0 (cid:28) min{k1,

pγ, n
log p

c min

n

constant c > 0 such that

Θ0(k1), Θ0(k2),(cid:98)β, (cid:96)2

L∗

(3.7)

(cid:17) ≥ c min

(cid:26) k2 log p

(cid:27)

,

1√
n

σ2
0.

α

In particular, if (cid:98)βL is the Lasso estimator deﬁned in (2.10) with A >

n

√

2,

the above lower bound can be achieved.

α

(cid:17)

Θ0(k1), Θ0(k2),(cid:98)β, (cid:96)2

. In the regime k2 (cid:28) √
(cid:46) k2 (cid:46) n

(cid:16)
0. For the Lasso estimator (cid:98)βL deﬁned in (2.10), the lower bound

The lower bound established in Theorem 4 implies that of Theorem 3
and both lower bounds hold for a general class of estimators satisfying the
assumption (A1). There exists a phase transition for the lower bound of the
benchmark L∗
log p , the lower
bound in (3.7) is k2 log p
log p , the lower bound in
(3.7) is 1√
k log p
n σ2
0 in (3.7) can be achieved by the conﬁdence
interval CI0
α (Z, k2, 2) deﬁned in (3.15), respectively. Such
an interval estimator is also used for the (cid:96)q loss with 1 ≤ q < 2. The mini-
max lower bound 1√
0 in (3.6) and (3.7) can be achieved by the following
constructed conﬁdence interval,

0 in (3.5) and k2 log p
n σ2
α (Z, k, 2) and CI0

0; when

√
n
log p

n σ2

n σ2

n σ2

n

(cid:32)

(cid:33)

(cid:32)

,

0

(n2)

− σ2

ψ (Z)
χ2
1− α
(n2) are the 1 − α

+

2

1
n2

α
2

(cid:26) 1

n2

(cid:13)(cid:13)(cid:13)y(2) − X (2)(cid:98)βL(cid:13)(cid:13)(cid:13)2

2

(3.8)

CI1

α (Z) =

where χ2
respectively, and

1− α

2

(n2) and χ2

(3.9)

ψ (Z) = min

(cid:33)

 ,

ψ (Z)
χ2

(n2)

α
2

1
n2

− σ2

0

+

2 and α

2 quantiles of χ2 (n2),

(cid:27)

, σ2

0 log p

.

Note that the two-sided conﬁdence interval (3.8) is simply based on the
observed data Z, not depending on any prior knowledge of the sparsity k.
Furthermore, it is a two-sided conﬁdence interval, which tells not only just
an upper bound, but also a lower bound for the loss. The coverage property
and expected length of CI1
α (Z) are established in the following proposition.

Proposition 2. Suppose k ≤ c n

√

T. T. CAI AND Z. GUO

log p and (cid:98)βL is the estimator deﬁned in

(2.10) with A >

2. Then CI1

α (Z) deﬁned in (3.8) satisﬁes,

lim inf
n,p→∞ inf

θ∈Θ0(k)

14

(3.10)

and

(3.11)

(cid:17) ≥ 1 − α,

α (Z)

P(cid:16)(cid:107)(cid:98)βL − β(cid:107)2
α (Z) , Θ0 (k)(cid:1) (cid:46) 1√
L(cid:0)CI1

2 ∈ CI1

σ2
0.

n

(cid:16)

(cid:17)

α

(cid:17)

on the

left

illustrates L∗

α

Θ0(k1),(cid:98)βL, (cid:96)2
log p (cid:28) k2 (cid:46) n

on the right column. The top, middle and bottom plots

(cid:16)
Θ0(k1), Θ0(k2),(cid:98)βL, (cid:96)2
Fig 2. The ﬁgure
L∗
represent regimes k1 ≤ k2 (cid:46) √
Regarding the Lasso estimator (cid:98)βL deﬁned in (2.10), we will discuss the
adaptivity of conﬁdence intervals of the loss (cid:107)(cid:98)βL − β(cid:107)2
behavior of conﬁdence intervals for (cid:107)(cid:98)βL − β(cid:107)2

√
log p (cid:28) k1 ≤ k2 (cid:46) n

log p , k1 (cid:46) √

column and

respectively.

n

n

log p and

n

log p ,

2. The adaptivity
2 is demonstrated in Figure
√
log p (cid:28) k1 ≤

n

2. As illustrated in the bottom plot of Figure 2, in the regime

ΩΘ(cid:2868)𝑘(cid:2869)Θ(cid:2868)𝑘(cid:2870)𝑘(cid:2869)log𝑝𝑛𝑘(cid:2870)log𝑝𝑛Θ(cid:2868)𝑘(cid:2869)ΩΩΘ(cid:2868)𝑘(cid:2869)Θ(cid:2868)𝑘(cid:2870)𝑘(cid:2869)log𝑝𝑛1𝑛Θ(cid:2868)𝑘(cid:2869)Θ(cid:2868)𝑘(cid:2869)Θ(cid:2868)𝑘(cid:2870)1𝑛1𝑛Θ(cid:2868)𝑘(cid:2869)n

and k1 (cid:28) k2.

HIGH-DIMENSIONAL ACCURACY ASSESSMENT

(cid:16)

α

Θ0(k2), Θ0(k2),(cid:98)βL, (cid:96)2

(cid:17) (cid:16) L∗

α

(cid:16)

Θ0(k1),(cid:98)βL, (cid:96)2

15

(cid:17) (cid:16)

(cid:17)

L∗

α

(cid:16)

α

n

log p since

(cid:17) (cid:29) L∗

Θ0(k1),(cid:98)βL, (cid:96)2

log p , we obtain L∗

k2 (cid:46) n
1√
n , which implies that adaptation is possible over this regime. As shown
in Proposition 2, the conﬁdence interval CI1
α (Z) deﬁned in (3.8) is fully
adaptive over the regime

√
log p (cid:28) k (cid:46) n

log p in the sense of (3.4).

Illustrated in the top and middle of Figure 2, it is impossible to construct

Θ0(k1), Θ0(k2),(cid:98)βL, (cid:96)2

an adaptive conﬁdence interval for the Lasso estimator (cid:98)βL over the regime
k1 (cid:28) √
(cid:16)
To sum up, the adaptation of conﬁdence intervals for (cid:107)(cid:98)βL − β(cid:107)2
vals for (cid:107)(cid:98)β − β(cid:107)2
(cid:110)
β : (cid:107)β −(cid:98)β(cid:107)2
conﬁdence interval for the loss (cid:107)(cid:98)β − β(cid:107)2
(cid:110)(cid:107)(cid:98)β − β(cid:107)2

balls constructed in [13] are of form
be the Lasso estimator and Un (Z) is a data dependent squared radius. See
[13] for further details. Such a conﬁdence ball directly leads to a one-sided

Comparison with conﬁdence balls. The construction of conﬁdence inter-
2 is related to that of conﬁdence balls for β. Conﬁdence

2 ≤ Un (Z)
(cid:111)
2 ≤ Un (Z)

(cid:111)
, where (cid:98)β can

2,

2 : (cid:107)(cid:98)β − β(cid:107)2

possible over the regime

(cid:46) k (cid:46) n

if k1 (cid:28)

CIinduced

α

2 is only

√

n
log p

√
n
log p

(Z) =

(3.12)

log p .

.

This induced one-sided conﬁdence interval for the loss is not as informative
as the two-sided conﬁdence interval constructed in (3.8) since the conﬁdence
interval (3.12) does not contain the information whether the loss is close to
zero or not. Furthermore, as shown in [13], the length of conﬁdence interval
CIinduced
n . The two-
sided conﬁdence interval CI1
α (Z) constructed in (3.8) is of expected length
n , which is much shorter than 1√
1√
log p . That
is, the two-sided conﬁdence interval (3.8) provides a more accurate interval
estimator of the (cid:96)2 loss. This is illustrated in Figure 3.

n + k log p
in the regime k (cid:29) √

(Z) over the parameter space Θ0(k) is of order 1√

n + k log p

α

n

n

length and adaptivity of conﬁdence intervals for (cid:107)(cid:98)β−β(cid:107)2

3.3. Conﬁdence intervals for the (cid:96)q loss with 1 ≤ q < 2 over Θ0(k).
We now consider the case 1 ≤ q < 2 and investigate the minimax expected
q over the parameter
space Θ0(k). The following theorem characterizes the minimax convergence
rate for the expected length of conﬁdence intervals.

16

T. T. CAI AND Z. GUO

Fig 3. Comparison of the two-sided conﬁdence interval CI1
dence interval CIinduced

(Z).

α

α (Z) with the one-sided conﬁ-

(cid:110)

(cid:111)

Theorem 5. Suppose 0 < α < 1

√
log p} and
estimator (cid:98)β satisfying the assumption (A2) with (cid:107)β∗(cid:107)0 ≤ k0, then there is
2 . For any

4 , 1 ≤ q < 2, k0 (cid:28) min{k,
for some constants c > 0 and 0 ≤ γ < 1

k ≤ c min

pγ, n
log p

n

some constant c > 0 such that

Θ0(k),(cid:98)β, (cid:96)q

(cid:17) ≥ ck

L∗

log p

2
q

σ2
0.

(3.13)

α

√
2,
then the minimax expected length for (1 − α) level conﬁdence intervals of

In particular, if (cid:98)βL is the Lasso estimator deﬁned in (2.10) with A > 4
(cid:107)(cid:98)βL − β(cid:107)2

q over Θ0(k) is

n

(cid:17) (cid:16) k

2
q

log p

n

σ2
0.

Θ0(k),(cid:98)βL, (cid:96)q
(cid:18)

(3.14)

L∗

α

gence rate of (3.14),

(3.15)

CI0

α (Z, k, q) =

(cid:16)

(cid:16)

We now construct the conﬁdence interval achieving the minimax conver-

(cid:40)

log p

2
q

n

0, C∗(A, k)k
(cid:17)4 ,

(cid:16) 3η0
(cid:16) 1
4−(9+11η0)

η0+1 Aσ0

(22Aσ0)2

(cid:113) 2k log p

(cid:16) 1
4−42

n1

(cid:41)

(cid:17)4

with η0 =

(cid:19)
(cid:17)2
(cid:113) 2k log p

,

n1

where C∗(A, k) = max

√
√

√
A−√

A+

2
2

1.01
the expected length of CI0

. The following proposition establishes the coverage property and

Proposition 3. Suppose k (cid:46) n

α (Z, k, q).

log p and (cid:98)βL is the estimator deﬁned in

√
2. For 1 ≤ q ≤ 2, the conﬁdence interval CI0

α (Z, k, q)

(2.10) with A > 4

0∥(cid:4632)𝛽−𝛽∥(cid:2870)(cid:2870)CI(cid:3080)(cid:2869)(Z)CI(cid:3080)(cid:2919)(cid:2924)(cid:2914)(cid:2931)(cid:2913)(cid:2915)(cid:2914)(Z)HIGH-DIMENSIONAL ACCURACY ASSESSMENT

17

(cid:16)(cid:107)(cid:98)β − β(cid:107)2

deﬁned in (3.15) satisﬁes

(3.16)

lim inf
n,p→∞ inf

θ∈Θ0(k)

Pθ

and

(3.17)

L(cid:0)CI0
(cid:98)βL deﬁned in (2.10) with A >

√

2.

(cid:17)

q ∈ CI0

α (Z, k, q)

= 1,

α (Z, k, q) , Θ0 (k)(cid:1) (cid:46) k

2
q

log p

n

σ2
0.

In particular, for the case q = 2, (3.16) and (3.17) also hold for the estimator

(cid:110)

(cid:111)

The above proposition shows that the conﬁdence interval CI0

α (Z, k, q)
achieves the minimax convergence rate given in (3.14). In contrast to the
(cid:96)2 loss where the two-sided conﬁdence interval (3.8) is signiﬁcantly shorter
than the one-sided interval and achieves the optimal convergence rate over
log p , for the (cid:96)q loss with 1 ≤ q < 2, the one-sided
the regime
conﬁdence interval is rate-optimal.

√
log p (cid:28) k (cid:46) n

n

We now consider the adaptivity of conﬁdence intervals. The following
with 1 ≤

theorem establishes the lower bound for L∗
q < 2.

α

Θ0(k1), Θ0(k2),(cid:98)β, (cid:96)q

(cid:16)

(cid:17)

n

c min

pγ, n
log p

4 , k0 (cid:28) min{k1,

Theorem 6. Suppose 0 < α < 1

constant c > 0 such that
(3.18)

for some constants c > 0 and 0 ≤ γ < 1

√
log p} and k1 ≤ k2 ≤
tor (cid:98)β satisfying the assumption (A2) with (cid:107)β∗(cid:107)0 ≤ k0, then there is some
2 . For any estima-
(cid:16)

√
log p (cid:28) k2 (cid:46) n
log p ;
√
log p (cid:28) k1 ≤ k2 (cid:46) n
In particular, if p ≥ n and (cid:98)βL is the Lasso estimator deﬁned in (2.10) with
log p .

Θ0(k1), Θ0(k2),(cid:98)β, (cid:96)q

if k1 ≤ k2 (cid:46)
if k1 (cid:46)

log p
n σ2
0
1√
n σ2



(cid:17) ≥

q −1
q −1

2
q
ck
2
2

ck
2
2

ck
2

k1

log p
n σ2
0

√
n
log p ;

L∗

α

if

n

n

0

√

A > 4

2, the above lower bounds can be achieved.

The lower bounds of Theorem 6 imply that of Theorem 5 and both lower
bounds hold for a general class of estimators satisfying the assumption (A2).
However, the lower bound (3.18) in Theorem 6 has a signiﬁcantly diﬀerent

18

T. T. CAI AND Z. GUO

meaning from (3.13) in Theorem 5 where (3.18) quantiﬁes the cost of adapta-

tion without knowing the sparsity level. For the Lasso estimator (cid:98)βL deﬁned

in (2.10), by comparing Theorem 5 and Theorem 6, we obtain

(cid:16)

L∗

α

Θ0(k1), Θ0(k2),(cid:98)βL, (cid:96)q

(cid:17) (cid:29) L∗

α

(cid:16)

Θ0(k1),(cid:98)βL, (cid:96)q

(cid:17)

if k1 (cid:28) k2,

n

that the conﬁdence interval CI0

√
log p (cid:28) k (cid:46) n
log p .

which implies the impossibility of constructing adaptive conﬁdence intervals
for the case 1 ≤ q < 2. There exists marked diﬀerence between the case
1 ≤ q < 2 and the case q = 2, where it is possible to construct adaptive
conﬁdence intervals over the regime

For the Lasso estimator (cid:98)βL deﬁned in (2.10), it is shown in Proposition 3
 ,

α (Z, k2, q) deﬁned in (3.15) achieves the lower
2
1√
n σ2
q
bound k
2
of (3.18) can be achieved by the following proposed conﬁdence interval,
(3.19)

0 of (3.18). The lower bounds k
2

(cid:32)

α (Z, k2, q) =

0 and k

log p
n σ2

log p
n σ2

, (16k2)

− σ2

− σ2

(cid:33)

(cid:32)

(cid:33)

2

q −1

2

q −1

CI2

2

q −1

2

k1

0

0

0

ψ (Z)
χ2
1− α

2

1
n2

(n2)

ψ (Z)
χ2

(n2)

α
2

1
n2

+

+

where ψ (Z) is deﬁned in (3.9). The following proposition veriﬁes the above
claim.

Proposition 4. Suppose p ≥ n, k1 ≤ k2 (cid:46) n

√

tor deﬁned in (2.10) with A > 4
satisﬁes,

n,p→∞ inf
lim inf

2. Then CI2

Pθ

θ∈Θ0(k2)

(cid:16)(cid:107)(cid:98)β − β(cid:107)2
α (Z, k2, q) , Θ0 (k1)(cid:1) (cid:46) k

q ∈ CI2
(cid:18)

q −1

2

2

L(cid:0)CI2

(3.20)

and

(3.21)

α (Z, k2, q)

log p and (cid:98)βL is the estima-

α (Z, k2, q) deﬁned in (3.19)

(cid:17) ≥ 1 − α,
(cid:19)

σ2
0.

k1

log p

n

+

1√
n

4. Minimaxity and adaptivity of conﬁdence intervals over Θ(k).
In this section, we focus on the case of unknown Σ and σ and establish the
rates of convergence for the minimax expected length of conﬁdence intervals
q with 1 ≤ q ≤ 2 over the parameter space Θ(k) deﬁned in
(cid:17)
q. The
following theorem establishes the lower bounds for the benchmark quantities
L∗

for (cid:107)(cid:98)β − β(cid:107)2
(2.4). We also study the adaptivity of conﬁdence intervals for (cid:107)(cid:98)β − β(cid:107)2
(cid:16)
Θ (ki) ,(cid:98)β, (cid:96)q

Θ (k1) , Θ (k2) ,(cid:98)β, (cid:96)q

with i = 1, 2 and L∗

(cid:16)

(cid:17)

.

α

α

2
q
2

log p

n

.

HIGH-DIMENSIONAL ACCURACY ASSESSMENT

19

(cid:110)

(cid:111)

(4.1)

pγ, n
log p

k2 ≤ c min

Theorem 7. Suppose that 0 < α < 1

4 , 1 ≤ q ≤ 2, k0 (cid:28) k1 and k1 ≤
estimator (cid:98)β satisfying the assumption (A1) at θ0 = (β∗, I, σ0) with (cid:107)β∗(cid:107)0 ≤
2 . For any

for some constants c > 0 and 0 ≤ γ < 1

k0, then there is some constant c > 0 such that

Θ (ki) ,(cid:98)β, (cid:96)q
(cid:17)

(cid:16)
(cid:17)(cid:17) ≥ ck
(cid:16){θ0} , Θ (k2) ,(cid:98)β, (cid:96)q
In particular, if (cid:98)βSL is the scaled Lasso estimator deﬁned in (2.14) with

CIα((cid:98)β,(cid:96)q,Z)∈Iα(Θ(k2),(cid:98)β,(cid:96)q)

(cid:16)(cid:98)β, (cid:96)q, Z

(cid:17) ≥ ck

and
(4.2)
L∗

i = 1, 2;

Eθ0L

(cid:16)

log p

L∗

CIα

for

inf

2
q
i

=

n

α

α

,

√

2, then the above lower bounds can be achieved,

A > 2

(4.3)

and

(4.4)

L∗

α

(cid:16)

(cid:17) (cid:16) k
Θ (ki) ,(cid:98)βSL, (cid:96)q
(cid:16){θ0} , Θ (k2) ,(cid:98)βSL, (cid:96)q

2
q
i

L∗

α

log p

n

(cid:17) (cid:16) ck

for

i = 1, 2;

2
q
2

log p

n

.

α

α

α

2
q
2

L∗

(cid:16)

log p
n .

The lower bounds (4.1) and (4.2) hold for any estimator satisfying the
assumption (A1) at an interior point θ0, including the scaled Lasso estimator
as a special case. We will demonstrate the impossibility of adaptivity of

conﬁdence intervals for the (cid:96)q loss of the scaled Lasso estimator (cid:98)βSL deﬁned

in (2.14). By Theorem 7, we have L∗
Since L∗

(cid:16)
Θ (k1) , Θ (k2) ,(cid:98)βSL, (cid:96)q
Θ (k1) , Θ (k2) ,(cid:98)βSL, (cid:96)q

(cid:17) ≥ ck
(cid:17)

(cid:16){θ0} , Θ (k2) ,(cid:98)βSL, (cid:96)q
(cid:16){θ0} , Θ (k2) ,(cid:98)βSL, (cid:96)q
(cid:17) ≥ L∗
(cid:16)
(cid:17) (cid:29) L∗
Θ (k1) ,(cid:98)βSL, (cid:96)q
(cid:16)
(cid:17)
Θ (k1) ,(cid:98)βSL, (cid:96)q

and L∗

(cid:17)
Θ (k1) , Θ (k2) ,(cid:98)βSL, (cid:96)q

if k1 (cid:28) k2.

The comparision of L∗
is illustrated in Figure 4. Referring to the adaptivity deﬁned in (3.4), it
is impossible to construct adaptive conﬁdence intervals for the loss of the

(cid:16)
scaled Lasso estimator (cid:98)βSL.
loss of any estimator (cid:98)β satisfying the assumption (A1), under the coverage
(cid:16)(cid:98)β, (cid:96)q, Z

Theorem 7 shows that for any conﬁdence interval CIα

Θ (k2) ,(cid:98)β, (cid:96)q

(cid:16)(cid:98)β, (cid:96)q, Z

(cid:17) ∈ Iα

, its expected length at

constraint that CIα

for the

, then

(cid:17)

(cid:17)

(cid:16)

(cid:17)

α

α

α

α

20

T. T. CAI AND Z. GUO

4. The

(cid:16)
Θ (k1) , Θ (k2) ,(cid:98)βSL, (cid:96)q

ﬁgure

Fig
L∗

α

(cid:17)

demonstrates L∗

α

on the right.

(cid:16)

Θ (k1) ,(cid:98)βSL, (cid:96)q

(cid:17)

on

the

left

and

any given θ0 = (β∗, I, σ) ∈ Θ (k0) must be of order k
log p
n . In contrast to
Theorem 4 and 6, Theorem 7 demonstrates that conﬁdence intervals must
be long at a large subset of points in the parameter space, not just at a small
number of “unlucky” points. Therefore, the lack of adaptivity for conﬁdence
intervals is not due to the conservativeness of the minimax framework.

2
q
2

In the following, we detail the construction of conﬁdence intervals for
q. The construction of conﬁdence intervals is based on the following

(cid:107)(cid:98)βSL−β(cid:107)2

deﬁnition of restricted eigenvalue, which is introduced in [3],
(cid:107)Xδ(cid:107)2
n(cid:107)δJ01(cid:107)2

κ(X, k, s, α0) = min
|J0|≤k

min
δ(cid:54)=0,
(cid:107)1≤α0(cid:107)δJ0(cid:107)1

J0⊂{1,··· ,p},

(4.5)

(cid:107)δJc

√

0

,

where J1 denotes the subset corresponding to the s largest in absolute value
coordinates of δ outside of J0 and J01 = J0 ∪ J1. Deﬁne the event B =

{ˆσ ≤ log p} . The conﬁdence interval for (cid:107)(cid:98)βSL − β(cid:107)2
(cid:26) [0, ϕ (Z, k, q)]

(4.6)

CIα (Z, k, q) =

q is deﬁned as
on B
on Bc,

{0}

where

ϕ (Z, k, q) = min




nκ2(cid:16)

2(cid:98)σ
(cid:16) max (cid:107)X·j(cid:107)2

16Amax(cid:107)X·j(cid:107)2
X, k, k, 3

min (cid:107)X·j(cid:107)2

2

(cid:17)(cid:17)

(cid:18)

,

k

2
q

k

log p

n

2
q

log p

n

log p

(cid:19)(cid:98)σ2

 .

Properties for the coverage probability and the expected length of CIα (Z, k, q)

are established as follows.

ΩΘ𝑘(cid:2869)Θ𝑘(cid:2870)𝑘(cid:2869)(cid:2870)(cid:3044)log𝑝𝑛𝑘(cid:2870)(cid:2870)(cid:3044)log𝑝𝑛Θ𝑘(cid:2869)HIGH-DIMENSIONAL ACCURACY ASSESSMENT

21

2. For 1 ≤ q ≤ 2, then CIα (Z, k, q) deﬁned in (4.6)

log p and (cid:98)βSL is the estimator deﬁned in
(cid:16)(cid:107)(cid:98)β − β(cid:107)2

q ∈ CIα (Z, k, q)

(cid:17)

= 1,

Proposition 5. Suppose k (cid:46) n

√
(2.14) with A > 2
satisﬁes the following properties,

lim inf
n,p→∞ inf
θ∈Θ(k)

Pθ

(4.7)

and

(4.8)

L (CIα (Z, k, q) , Θ (k)) (cid:46) k

2
q

log p

n

.

Proposition 5 shows that the conﬁdence interval CIα (Z, ki, q) deﬁned in
(4.6) achieves the minimax convergence rate in (4.3), for i = 1, 2, and the
conﬁdence interval CIα (Z, k2, q) deﬁned in (4.6) achieves the minimax con-
vergence rate in (4.4).

5. General tools for minimax lower bounds. A major step in our
analysis is to establish rate sharp lower bounds for the minimax estimation
error and the expected length of minimax and adaptive conﬁdence inter-
vals for the (cid:96)q loss. We introduce in this section new technical tools that are
needed to establish these lower bounds. A signiﬁcant distinction of the lower
bound results given in the previous sections from those for the traditional
parameter estimation problems is that the constraint is on the performance

of the estimator(cid:98)β of the regression vector β, but the lower bounds are on the
diﬃculty of estimating its loss (cid:107)(cid:98)β − β(cid:107)2
dence intervals for the loss (cid:107)(cid:98)β − β(cid:107)2

q. Conventional lower bound methods
are either not applicable or do not yield rate-optimal results. We introduce
useful lower bound techniques to establish rate-optimal lower bounds for
the minimax estimation error and the expected length of adaptive conﬁ-
q. These technical tools may also be of

independent interest.

Before stating the technical tools, we ﬁrst introduce the following no-
tations used in the later discussion. Let Z denote a random variable with
parameter θ ∈ Θ and let π denote a prior on the parameter space Θ. We will
use fθ(z) to denote the distribution of Z with the parameter θ and fπ (z) to
denote the marginal distribution of Z with a prior π on the parameter space
Θ. For a function g, we shall write Eπ (g(Z)) for the expectation with respect
to the marginal distribution of Z with a prior π on Θ. More speciﬁcally, we
write

(cid:90)

(cid:90)

(5.1)

fπ (z) =

fθ (z) π (θ) dθ

and Eπ (g(Z)) =

g (z) fπ (z) dz.

T. T. CAI AND Z. GUO

function. The L1 distance between two probability distributions is given by

22
We will use Pπ to denote the probability distribution of Z corresponding

to fπ (z), deﬁned as, Pπ (A) =(cid:82) 1z∈Afπ (z) dz, where 1z∈A is the indicator
L1(f1, f0) =(cid:82) |f1(z) − f0(z)| dz.
under the constraint that (cid:98)β is a good estimator at at least one interior point.
4 , 1 ≤ q ≤ 2, θ0 =
a prior over the parameter space F. Assume the estimator (cid:98)β satisﬁes
(β∗, I, σ0) ∈ Θ and F ⊂ Θ. Deﬁne d = minθ∈F (cid:107)β (θ) − β∗(cid:107)q. Let π denote

The following theorem establishes the minimax lower bounds for the esti-
mation error and the expected length of conﬁdence intervals for the (cid:96)q loss,

Theorem 8. Suppose 0 < α < 1

4 , 0 < α0 < 1

Pθ0

(cid:19)

(cid:18)
(cid:107)(cid:98)β − β∗(cid:107)2
q ≤ 1
16
(cid:18)
|(cid:98)Lq − (cid:107)(cid:98)β − β(cid:107)2

d2

≥ 1 − α0,

(5.2)

then

(5.3)

and
(5.4)
L∗

α

sup

Pθ

θ∈{θ0}∪F

inf(cid:98)Lq
(cid:17)
(cid:16){θ0} , Θ,(cid:98)β, (cid:96)q
CIα((cid:98)β,(cid:96)q,Z)∈Iα(Θ,(cid:98)β,(cid:96)q)
(cid:110) 1
10 ,(cid:0) 9
10 − α0 − L1 (fπ, fθ0)(cid:1)

inf

=

(cid:111)

+

d2

q| ≥ 1
4
(cid:16)

Eθ0L

CIα

where ¯c1 = min

≥ ¯c1,

(cid:19)
(cid:16)(cid:98)β, (cid:96)q, Z

(cid:17)(cid:17) ≥ c∗

2d2,

and c∗

2 = 1

2 (1 − 2α − α0 − 2L1 (fπ, fθ0))+ .

Remark 1. The minimax lower bound (5.3) for the estimation error
and (5.4) for the expected length of conﬁdence intervals hold as long as the

estimator (cid:98)β estimates β well at an interior point θ0. Besides the assumption
bound (5.3), constraining that (cid:107)(cid:98)β − β∗(cid:107)2
loss (cid:107)(cid:98)β − β(cid:107)2
we will show that (cid:107)(cid:98)β − β(cid:107)2

(5.2), another key ingredient for the lower bounds (5.3) and (5.4) is to con-
struct the least favorable space F with the prior π such that the marginal
distributions fπ and fθ0 are non-distinguishable. For the estimation lower
q can be well estimated at θ0, due
to the non-distinguishability between fπ and fθ0, we can establish that the
q cannot be estimated well over F. For the lower bound (5.4),
by the assumption (5.2) and the non-distinguishability between fπ and fθ0,
q and hence

q over F is much larger than (cid:107)(cid:98)β − β∗(cid:107)2

the honest conﬁdence intervals must be suﬃciently long.

Theorem 8 is used to establish the minimax lower bounds for both the
estimation error and the expected length of conﬁdence intervals of the (cid:96)q loss

HIGH-DIMENSIONAL ACCURACY ASSESSMENT

23

over Θ(k). By taking θ0 ∈ Θ(k0) and Θ = Θ(k), Theorem 2 follows from
(5.3) with a properly constructed subspace F ⊂ Θ(k). By taking θ0 ∈ Θ(k0)
and Θ = Θ(k2), Theorem 7 follows from (5.4) with a properly constructed
F ⊂ Θ(k2). In both cases, the assumption (A1) implies Condition (5.2).

Furthermore, several minimax lower bounds over the parameter space
Θ0(k) can also be implied by Theorem 8. For the estimation error, the
√
minimax lower bounds (2.8) and (2.9) over the regime k (cid:46)
log p in Theorem
1 follow from (5.3). For the expected length of conﬁdence intervals, the
minimax lower bounds (3.7) in Theorem 4 and (3.18) in the region k1 ≤
k2 (cid:46)
log p in Theorem 6 follow from (5.4). In
these cases, the assumption (A1) or (A2) can guarantee that Condition (5.2)
is satisﬁed.

√
log p (cid:28) k2 (cid:46) n

√
log p and k1 (cid:46)

n

n

n

n

gion

√
log p (cid:28) k (cid:46) n

However, the minimax lower bound for estimation error (2.9) in the re-
log p and for the expected length of conﬁdence intervals
√
log p (cid:28) k1 ≤ k2 (cid:46) n
(3.18) in the region
log p cannot be established using the
above theorem. The following theorem, which requires testing a composite
null against a composite alternative, establishes the reﬁned minimax lower
bounds over Θ0(k).

n

Theorem 9. Suppose that 0 < α < 1

4 , 1 ≤ q ≤ 2 and
θ0 = (β∗, I, σ0) ∈ Θ0 (k0). Assume that for given disti and di, there exists
Fi ⊂ Θ0 (ki) satisfying
(cid:107)β (θ) − β∗(cid:107)2 = disti
for θ ∈ Fi where i = 1, 2.
Let πi denote a prior over the parameter space Fi for i = 1, 2. Suppose that

(cid:107)β (θ)− β∗(cid:107)q = di

4 , 0 < α0 < 1

and

for θ1 =(cid:0)β∗, I, σ2

(5.5)

Pθi

0 + dist2
1

(cid:16)(cid:107)(cid:98)β − β∗(cid:107)2

(cid:1),

0 + dist2
2

q ≤ c2
i d2
i

(cid:1) and θ2 =(cid:0)β∗, I, σ2
(cid:17) ≥ 1 − α0,
(cid:16)|(cid:98)Lq − (cid:107)(cid:98)β − β(cid:107)2
(cid:16)
(cid:17) ≥ c∗
(1 − c2)2 d2
(cid:27)
(cid:17)
i=1 L1 (fπi , fθi ) − 2L1 (fπ2 , fπ1 )(cid:1)

4 − (1 + c1)2 d2

q| ≥ c∗
3d2
2

, c∗

1
d2
2

4

+

sup

Pθ

θ∈F1∪F2

inf(cid:98)Lq
(cid:16)
Θ0 (k1) , Θ0 (k2) ,(cid:98)β, (cid:96)q
(cid:26)
(cid:16)
(cid:110) 1
10 ,(cid:0) 9
10 − 2α0 −(cid:80)2

(1 − c2)2 − 1

1
4 ,

(5.6)

and

(5.7)

L∗

α

where c∗

3 = min

and ¯c3 = min

where c1 and c2 are positive constants. Then we have

for i = 1, 2,

(cid:17) ≥ ¯c3,

(cid:17)
2 − (1 + c1)2 d2
4 =(cid:0)1 − 2α0 − 2α −(cid:80)2
i=1 L1 (fπi , fθi ) − 2L1 (fπ2 , fπ1 )(cid:1)

+

1

,

+

(cid:111)

.

+

24

Remark 2. As long as the estimator (cid:98)β performs well at two points, θ1

T. T. CAI AND Z. GUO

and θ2, the minimax lower bounds (5.6) for the estimation error and (5.7)
for the expected length of conﬁdence intervals hold. Note that θi in the
above theorem does not belong to the parameter space Θ0 (ki), for i = 1, 2.
In contrast to Theorem 8, Theorem 9 compares composite hypothesises F1
and F2, which will lead to a sharper lower bound than comparing the sim-
ple null {θ0} with the composite alternative F. For simplicity, we construct
least favorable parameter spaces Fi such that the points in Fi is of ﬁxed
(cid:96)2 distance and ﬁxed (cid:96)q distance to β∗, for i = 1, 2, respectively. More im-
portantly, we construct F1 with the prior π1 and F2 with the prior π2 such
that fπ1 and fπ2 are not distinguishable, where θ1 and θ2 are introduced to
facilitate the comparison. By the assumption (5.5) and the construction of
F1 and F2, we establish that the (cid:96)q loss cannot be simultaneously estimated
well over F1 and F2. For the lower bound (5.7), under the same conditions,
it is shown that the (cid:96)q loss over F1 and F2 are far apart and any conﬁ-
dence interval with guaranteed coverage probability over F1 ∪ F2 must be
suﬃciently long. Due to the prior information Σ = I and σ = σ0, the lower
bound construction over Θ0(k) is more involved than that over Θ(k). We
shall stress that the construction of F1 and F2 and the comparison between
composite hypothesises are of independent interest.

The minimax lower bound (2.9) for the estimation error in the region
√
log p (cid:28) k (cid:46) n
n
log p follows from (5.6) and the minimax lower bound (3.18)
√
log p (cid:28) k1 ≤ k2 (cid:46) n
in the region
log p for the expected length of conﬁdence
intervals follows from (5.7). In these cases, the assumption (A2) implies
Condition (5.5).

n

6. Discussion.

for the loss (cid:107)(cid:98)β − β(cid:107)2

In this paper, we have investigated the minimax esti-
mation rate, minimax expected length and adaptivity of conﬁdence intervals
q with 1 ≤ q ≤ 2 over the parameter spaces Θ0(k) and
Θ(k). It is interesting to compare the minimaxity and adaptivity behav-
iors between loss estimation over the parameter spaces Θ(k) and Θ0(k).
The comparison shows signiﬁcant diﬀerences between estimating the (cid:96)2 loss
and the (cid:96)q loss with 1 ≤ q < 2 as well as the diﬀerences between the two
parameter spaces Θ(k) and Θ0(k).

(cid:110) k log p

(cid:111)

In terms of the minimax estimation rate and minimax expected length of
conﬁdence intervals, the prior information Σ = I and σ = σ0 reduces the
.
With this prior information, the adaptive estimation of (cid:96)2 loss is made pos-
log p . In contrast, even with such prior
sible over the regime

convergence rate for the (cid:96)2 loss (cid:107)(cid:98)β − β(cid:107)2
√
log p (cid:28) k (cid:46) n

2 from k log p

n , 1√

n

to min

n

n

HIGH-DIMENSIONAL ACCURACY ASSESSMENT

25

information, the minimax convergence rate remains unchanged for the case
1 ≤ q < 2.

n

n

log p

n

to 1√

√
log p (cid:28) k (cid:46) n

Regarding adaptivity of conﬁdence intervals, the prior knowledge Σ = I
and σ = σ0 is extremely useful for the construction of adaptive conﬁdence
intervals for the (cid:96)2 loss in the regime
log p . Though adaptivity is
still impossible outside this regime, we have seen that, with this prior knowl-
edge, the expected length of optimal conﬁdence intervals over the regime
k1 (cid:46)

is still impossible to construct adaptive conﬁdence intervals for (cid:107)(cid:98)β−β(cid:107)2

In contrast, for (cid:96)q loss with 1 ≤ q < 2, even with this prior information, it
q with
1 ≤ q < 2. However, a comparison of Theorem 6 with Theorem 7 reveals
that the expected length of conﬁdence intervals is reduced with such prior

√
log p (cid:28) k2 (cid:46) n

log p is reduced from k2

2
q
knowledge, from k
2

n in the regime k1 (cid:46)
1√

√
log p (cid:28) k2 (cid:46) n
assumption (A1) or (A2), which basically requires the estimator(cid:98)β to perform
interesting to investigate estimation of the loss for more general estimator (cid:98)β

well at least at a point or a small subset of k-sparse parameter spaces. It is

The focus of this paper is on the collection of estimators satisfying the

√
log p (cid:28) k1 ≤ k2 (cid:46) n
log p .

in the regime

and from k

2

q −1

n
2

q −1

to k

2

to k

2

log p

n

n .

k1

log p

n

log p

log p

2
q
2

n

n

that does not satisfy the assumption (A1) or (A2). We leave this for future
research.

7. Proofs.

In this section, we present the proofs of the lower bound re-
sults. In Section 7.1, we establish the general lower bound result, Theorem
8. By applying Theorem 8 and Theorem 9, we establish Theorem 4, 6 in Sec-
tion 7.2 and Theorem 3, 5 in Section 7.3. For reasons of space, the proofs of
Theorem 1, 2, 7 and Theorem 9, the upper bound results, including Propo-
sition 1, 2, 3, 4 and 5 and the proofs of technical lemmas are postponed to
the supplement [5].

We deﬁne the χ2 distance between two density functions f1 and f0 by

(7.2)
Let PZ,θ∼π denote the joint probability of Z and θ with the joint density
function f (θ, z) = fθ (z) π (θ) . We introduce the following lemma, which is
used in the proofs of Theorem 8 and Theorem 9. The proof of this lemma
can be found in the supplement [5].

χ2(f1, f0).

(7.1)

χ2(f1, f0) =

and it is well known that

(cid:90) f 2

dz =

dz − 1

1 (z)
f0(z)

(cid:90) (f1(z) − f0(z))2
L1(f1, f0) ≤(cid:112)

f0(z)

26

T. T. CAI AND Z. GUO

Lemma 1. For any event A, we have

(7.3)

Pπ (Z ∈ A) = PZ,θ∼π (Z ∈ A) ,

(cid:90)

and
(7.4)

(7.6)

|fπ1 (z) − fπ2 (z)| dz = L1 (fπ2, fπ1) .
|Pπ1 (Z ∈ A) − Pπ2 (Z ∈ A)| ≤
and PZ,θ∼π(Z ∈ A) respectively, if there is no confusion. Recall that (cid:98)Lq(Z)
In the following proofs, we will write Pπ(A) and PZ,θ∼π(A) for Pπ(Z ∈ A)

denotes the data-dependent loss estimator and β(θ) denotes the correspond-
ing β of the parameter θ.

7.1. Proof of Theorem 8. We will ﬁrst prove (5.3) and then prove (5.4).

4 and α1 = 1
10 .

We set c0 = 1
Proof of (5.3)
We assume

(7.5)

Pθ0

d2

4

(cid:19)

≥ 1 − α1.

Otherwise, we have

(cid:18)(cid:12)(cid:12)(cid:12)(cid:98)Lq(Z) − (cid:107)(cid:98)β(Z) − β∗(cid:107)2
(cid:12)(cid:12)(cid:12) ≤ 1
(cid:18)(cid:12)(cid:12)(cid:12)(cid:98)Lq(Z) − (cid:107)(cid:98)β(Z) − β∗(cid:107)2
(cid:12)(cid:12)(cid:12) ≥ 1
(cid:26)
(cid:12)(cid:12)(cid:12)(cid:98)Lq(z) − (cid:107)(cid:98)β(z) − β∗(cid:107)2
z : (cid:107)(cid:98)β(z) − β∗(cid:107)2
(cid:90)

(cid:27)
and hence (5.3) follows. Deﬁne the event
(7.7) A0 =
.
By (5.2) and (7.5), we have Pθ0 (A0) ≥ 1 − α0 − α1. By (7.4), we obtain

(cid:12)(cid:12)(cid:12) ≤ 1

q ≤ c2

≥ α1.

(cid:19)

d2

4

d2

4

0d2 ,

Pθ0

q

q

q

Pπ (A0) ≥ 1 − α0 − α1 −

|fθ0 (z) − fπ (z)| dz.

(7.8)
For z ∈ A0 and θ ∈ F, by triangle inequality,

(cid:107)(cid:98)β(z) − β(θ)(cid:107)q ≥(cid:12)(cid:12)(cid:12)(cid:107)β(θ) − β∗(cid:107)q − (cid:107)(cid:98)β(z) − β∗(cid:107)q
q − (cid:107)(cid:98)β(z) − β∗(cid:107)2

(cid:12)(cid:12)(cid:12) ≥(cid:12)(cid:12)(cid:12)(cid:107)(cid:98)β(z) − β (θ)(cid:107)2

(cid:12)(cid:12)(cid:12) ≥ (1 − c0) d.
(cid:12)(cid:12)(cid:12) −(cid:12)(cid:12)(cid:12)(cid:98)Lq (z) − (cid:107)(cid:98)β(z) − β∗(cid:107)2

(7.9)
For z ∈ A0 and θ ∈ F, we have
(7.10)

(cid:12)(cid:12)(cid:12)(cid:98)Lq (z) − (cid:107)(cid:98)β(z) − β (θ)(cid:107)2

q

q

q

(cid:12)(cid:12)(cid:12)

≥(1 − 2c0 − 1
4

)d2,

HIGH-DIMENSIONAL ACCURACY ASSESSMENT

27

where the ﬁrst inequality follows from triangle inequality and the last in-
equality follows from (7.7) and (7.9). Hence, for z ∈ A0, we obtain

(7.11)

Note that

sup
θ∈F
≥ sup
θ∈F

Pπ

inf
θ∈F

(7.12) sup
θ∈F

Pθ

(cid:16)(cid:98)β, (cid:96)q, Z

For CIα

(cid:19)

.

Pθ

Pθ

(cid:18)

inf
θ∈F

q

q

q

4

)d2

4

)d2.

inf
θ∈F

(cid:12)(cid:12)(cid:12) ≥ (1 − 2c0 − 1
(cid:19)
(cid:12)(cid:12)(cid:12) ≥ (1 − 2c0 − 1
(cid:12)(cid:12)(cid:12) ≥ (1 − 2c0 − 1
(cid:12)(cid:12)(cid:12) ≥ (1 − 2c0 − 1
(cid:12)(cid:12)(cid:12) ≥ (1 − 2c0 − 1

(cid:12)(cid:12)(cid:12)(cid:98)Lq (z) − (cid:107)(cid:98)β(z) − β (θ)(cid:107)2
(cid:18)(cid:12)(cid:12)(cid:12)(cid:98)Lq (Z) − (cid:107)(cid:98)β(Z) − β (θ)(cid:107)2
(cid:18)
(cid:12)(cid:12)(cid:12)(cid:98)Lq (Z) − (cid:107)(cid:98)β(Z) − β (θ)(cid:107)2
(cid:12)(cid:12)(cid:12)(cid:98)Lq (Z) − (cid:107)(cid:98)β(Z) − β (θ)(cid:107)2
(cid:18)(cid:12)(cid:12)(cid:12)(cid:98)Lq (Z) − (cid:107)(cid:98)β(Z) − β (θ)(cid:107)2
(cid:17) ∈ Iα
(cid:16)
(cid:17)
Θ,(cid:98)β, (cid:96)q
(cid:16)(cid:107)(cid:98)β(Z) − β (θ)(cid:107)2
(cid:17)(cid:17) ≥ 1 − α.
(cid:110)
z : (cid:107)(cid:98)β(z) − β∗(cid:107)q < c0d, (cid:107)(cid:98)β(z) − β∗(cid:107)2

(cid:16)(cid:98)β, (cid:96)q, Z

q ∈ CIα

, we have

(cid:19)

Pθ

)d2

4

q

q

)d2

4

(cid:19)

)d2

.

4

Combining (7.6), (7.8) and (7.12), we establish (5.3).
Proof of (5.4)

Since the max risk is lower bounded by the Bayesian risk, we can further
lower bound the right hand side of above equation by

Combined with (7.11), we establish

≥ Pπ(A0).

inf
θ∈Θ

(7.13)

(cid:17)(cid:111)
Deﬁne the event A =
By (5.2) and (7.13), we have Pθ0 (A) ≥ 1 − α − α0. By (7.3) and (7.4), we
have

(cid:16)(cid:98)β, L, z

q ∈ CIα

.

(cid:17)(cid:111)
PZ,θ∼π (A) = Pπ (A) ≥ 1 − α − α0 − L1 (fπ, fθ0) .

(cid:110)
z : (cid:107)(cid:98)β(z) − β (θ)(cid:107)2

(cid:16)(cid:98)β, (cid:96)q, z

(7.14)
Deﬁne the event Bθ =
∪θ∈FBθ. By (7.13), we have

(cid:19)

q ∈ CIα
(cid:90) (cid:18)(cid:90)

(cid:90) (cid:18)(cid:90)

and M =

(cid:19)

PZ,θ∼π (M) =

1z∈Mfθ(z)dz

π (θ) dθ ≥

1z∈Bθ fθ(z)dz

π (θ) dθ ≥ 1−α.

28

T. T. CAI AND Z. GUO

Combined with (7.14), we have

;

(7.15)

q ∈ CIα

q ∈ CIα

(cid:17)
(cid:16)(cid:98)β, (cid:96)q, z
PZ,θ∼π (A ∩ M) ≥ 1 − 2α − α0 − L1 (fπ, fθ0) .
For z ∈ M, there exists ¯θ ∈ F such that (cid:107)(cid:98)β(z) − β(¯θ)(cid:107)2
(cid:16)(cid:98)β, (cid:96)q, z
(cid:17)
For z ∈ A, we have (cid:107)(cid:98)β(z) − β∗(cid:107)2
and (cid:107)(cid:98)β(z) − β∗(cid:107)q < c0d.
(cid:16)(cid:98)β, (cid:96)q, z
(cid:17)
q,(cid:107)(cid:98)β(z)− β∗(cid:107)2
Hence, for z ∈ A∩M, we have (cid:107)(cid:98)β(z)− β(¯θ)(cid:107)2
and (cid:107)(cid:98)β(z) − β(¯θ)(cid:107)q ≥ (cid:107)β(¯θ) − β∗(cid:107)q − (cid:107)(cid:98)β(z) − β∗(cid:107)q ≥ (1 − c0) d and hence
(cid:16)(cid:98)β, (cid:96)q, z
(cid:17)(cid:17) ≥ (1 − 2c0) d2.
(cid:17)(cid:17) ≥ (1 − 2c0) d2(cid:111)
(cid:16)(cid:98)β, (cid:96)q, z
(cid:16)

Deﬁne the event C =
have
(7.17) Pπ (C) = PZ,θ∼π (C) ≥ PZ,θ∼π (A ∩ M) ≥ 1 − 2α − α0 − L1 (fπ, fθ0) .
By (7.4), we establish Pθ0 (C) ≥ 1 − 2α − α0 − 2L1 (fπ, fθ0) and hence (5.4).

. By (7.16), we

q ∈ CIα

(7.16)

(cid:110)

(cid:16)

z : L

CIα

CIα

L

7.2. Proof of Theorems 4 and 6. We ﬁrst prove a general theorem for

1 ≤ q ≤ 2, which implies Theorem 4 and 6.

Theorem 10. Suppose 0 < α < 1

4 , k0 (cid:28) min{k1,

(cid:110)

(cid:111)

n

√
2 . Suppose that (cid:98)β
log p} and k1 ≤ k2 ≤

pγ, n
log p

c min
satisﬁes the assumption (A2) with (cid:107)β∗(cid:107)0 ≤ k0.

for some constants c > 0 and 0 ≤ γ < 1

1. If k2 (cid:46)

√
log p , then there is some constant c > 0 such that

n

α

L∗

(cid:16)
(7.18)
√
log p (cid:28) k2 (cid:46) n
(cid:16)
Θ0 (k1) , Θ0 (k2) ,(cid:98)β, (cid:96)q

(cid:17) ≥ ck
Θ0 (k1) , Θ0 (k2) ,(cid:98)β, (cid:96)q

(cid:18)
(cid:17) ≥ c max

(7.19)
L∗

α

n

2
q
2

log p

n

σ2
0.

2. If

log p , then there is some constant c > 0 such that

(1 − δ∗(n, p))k

2

q −1

2

k1

log p

n

− k

2
q
1

log p

n

where lim sup δ∗(n, p) = 0.

q −1
2√
In particular, the minimax lower bound (7.18) and the term k
n σ2
can be established under the weaker assumption (A1) with (cid:107)β∗(cid:107)0 ≤ k0.

2

0 in (7.19)

(cid:19)

,

+

2

q −1
2√
k

n

 σ2

0,

HIGH-DIMENSIONAL ACCURACY ASSESSMENT

29

n

n

n

L∗

α

(7.20)

Θ0 (k∗

By Theorem 10, we establish (3.7) in Theorem 4 and (3.18) in Theorem
√
6. In the regime k2 (cid:46)
log p , the lower bound (3.7) for q = 2 and (3.18) for
√
log p (cid:28) k2 (cid:46)
1 ≤ q < 2 follow from (7.18). For the case q = 2, in the regime
n
log p , the ﬁrst term of the right hand side of (7.19) is 0 while the second
1 = min{k1, ζ0k2} for
term is
some constant 0 < ζ0 < 1, an application of (7.19) leads to

n , which leads to (3.7). For 1 ≤ q < 2, let k∗
1√
(cid:16)

k
(cid:17)
1) , Θ0 (k2) ,(cid:98)β, (cid:96)q
(cid:40)
(cid:17) ≥ c max

(cid:17) ≥ c max
1) , Θ0 (k2) ,(cid:98)β, (cid:96)q
If k1 ≤ ζ0k2, (7.20) leads to the lower bounds (3.18) in the regions k1 (cid:46)
(cid:17) ≥ L∗
(cid:16)
√
√
log p (cid:28) k1 ≤ k2 (cid:46) n
log p ; if ζ0k2 < k1 ≤ k2, by the fact
log p (cid:28) k2 (cid:46) n
that L∗
Θ0 (k∗
Θ0 (k1) , Θ0 (k2) ,(cid:98)β, (cid:96)q
1 =
ζ0k2 ≥ ζ0k1, we establish L∗
and hence the lower bounds (3.18) in the regions k1 (cid:46)
√
The following lemma shows that (3.7) holds for (cid:98)βL deﬁned in (2.10) with
log p (cid:28) k1 ≤ k2 (cid:46) n
2 by verifying the assumption (A1) and (3.18) holds for (cid:98)βL deﬁned

Θ0 (k1) , Θ0 (k2) ,(cid:98)β, (cid:96)q
(cid:16)

k1
√
log p (cid:28) k2 (cid:46) n

 σ2

and k∗
q −1

log p follow.

q −1
2√
k

log p and

log p and

(cid:16)

2

q −1

log p

k∗

1

√

2

k

2

0.

,

n

α

2

n

n

n

α

α

n

2

√
A >
in (2.10) with A > 4
lemma can be found in the supplement [5].

2 by verifying the assumption (A2). The proof of this
√

Lemma 2.

2, then we have

(cid:19)

≥ 1 − c exp(cid:0)−c(cid:48)n(cid:1) − p−c;

If A >

Pθ0
√
2, then we have

(cid:18)
(cid:107)(cid:98)βL − β∗(cid:107)2
2 ≤ C(cid:107)β∗(cid:107)0
(cid:18)
(cid:107)(cid:98)βL − β∗(cid:107)2
(cid:16)

Pθ

inf

If A > 4

{θ=(β∗,I,σ):σ≤2σ0}

into the following lower bounds,

(7.21)

log p

n

σ2
0

q ≤ C(cid:107)β∗(cid:107) 2

q
0

(cid:41)

σ2
0

log p

2

q −1
2√
n , k

n

(cid:19)

≥ 1−c exp(cid:0)−c(cid:48)n(cid:1)−p−c.

log p

n

σ2

Proof of Theorem 10. The lower bound (7.19) can be further decomposed

Θ0 (k1) , Θ0 (k2) ,(cid:98)β, (cid:96)q

L∗

α

(cid:17) ≥ ck

2

q −1

2

1√
n

σ2
0,

(cid:16)

and
(7.22)
L∗

α

Θ0 (k1) , Θ0 (k2) ,(cid:98)β, (cid:96)q

(cid:17) ≥ c

(cid:18)

(1 − δ∗(n, p))k

2

q −1

2

k1

log p

n

− k

2
1
1

log p

n

(cid:19)

+

σ2
0,

T. T. CAI AND Z. GUO

30
where lim sup δ∗(n, p) = 0. Hence, the proof of Theorem 10 is equivalent to
establishing (7.18), (7.21) and (7.22).
Proof of (7.18)
The proof of (7.18) is an application of (5.4) of Theorem 8. For θ0 =
(β∗, I, σ0) ∈ Θ0 (k0) , we construct
(7.23)

F = {(β∗ + δ, I, σ0) : δ ∈ (cid:96) (β∗, k2 − k0, ρ)} ⊂ Θ0 (k2) ,

where
(7.24)
(cid:96) (β∗, k2 − k0, ρ) = {δ : supp (δ) ⊂ supp (β∗)c ,(cid:107)δ(cid:107)0 = k2 − k0, δi ∈ {0, ρ}} .
Let S = supp (β∗). Without loss of generality, we assume S = {1, 2,··· , k0}.
Let p1 denote the size of Sc and hence p1 = p − k0. Let π denote the
uniform prior on the parameter space F, which is induced by the uniform
prior of δ on (cid:96) (β∗, k2 − k0, ρ). Under the Gaussian random design model,
Zi = (yi, Xi·) ∈ Rp+1 follows a joint Gaussian distribution with mean 0. Let
Σz denote the covariance matrix of Zi. For the indices of Σz, we use 0 as the
index of yi and {1,··· , p} as the indices for (Xi1,··· , Xip) ∈ Rp. Decompose
Σz into blocks
xy denote the variance

(cid:18)Σz

(cid:0)Σz

xx and Σz

, where Σz

yy, Σz

(cid:1)(cid:124)

(cid:19)

yy
Σz
xy

xy
Σz
xx

of y, the variance of X and the covariance of y and X, respectively. There
exists a bijective function h : Σz → (β, Σ, σ) and the inverse mapping h−1 :
(β, Σ, σ) → Σz, where h−1 ((β, Σ, σ)) =

Σβ + σ2 β

and

Σ

(cid:124)

(cid:124)

(cid:19)

Σ

(cid:18)β
yy −(cid:0)Σz

Σβ

xy

(cid:1)(cid:124)

(cid:17)

(cid:16)

 (cid:107)β∗(cid:107)2

2 + (cid:107)δ(cid:107)2

2 + σ2
0

 .

(cid:124)

(cid:124)

δ

(β∗
S)
Ik0×k0 0k0×p1
Ip1×p1
0p1×k0

(7.25)

h(Σz) =

(Σz

xx)

−1 Σz

xy, Σz

xx, Σz

(Σz

xx)

−1 Σz

xy

.

Based on the bijection, the control of χ2 (fπ, fθ0) is reduced to the control
of the χ2 distance between two multivariate Gaussian distributions.

The parameter spaces for Σz corresponding to {θ0} and F are
(β∗
01×p1
S)
Ik0×k0 0k0×p1
Ip1×p1
0p1×k0

0} , where Σz

2 + σ2
0
β∗
S
0p1×1

 (cid:107)β∗(cid:107)2

H1 = {Σz

0 =

(cid:124)

 ,

and

H2 = {Σz

δ : δ ∈ (cid:96) (β∗, k2 − k0, ρ)} , where Σz

Deﬁne θ1 = (cid:0)β∗, I, σ2
(k2 − k0)ρ2 and hence θ1 = (cid:0)β∗, I, σ2

(cid:1). For δ ∈ (cid:96) (β∗, k2 − k0, ρ), we have (cid:107)δ(cid:107)2
0 + (k2 − k0)ρ2(cid:1). By L1 (fπ, fθ0) ≤

0 + (cid:107)δ(cid:107)2

2 =

2

β∗
S
δ

δ =

HIGH-DIMENSIONAL ACCURACY ASSESSMENT

31

L1 (fπ, fθ1)+L1 (fθ1, fθ0), it is suﬃcient to control L1 (fπ, fθ1) and L1 (fθ1, fθ0) .
By (7.2), it is suﬃcient to establish χ2 (fπ, fθ1) ≤ 2
1 and χ2 (fθ1, fθ0) ≤ 2
. Let Eδ,(cid:101)δ denote the expectation with respect to the
1,
independent random variables δ,(cid:101)δ with a uniform prior over the parame-
where 1 = 1−2α−2α0
ter space (cid:96) (β∗, k2 − k0, ρ). The following two lemmas are useful to control
χ2 (fπ, fθ1) and χ2 (fθ1, fθ0). The proof of Lemma 3 is given in the supple-
ment [5].

12

Lemma 3.

(7.26) χ2 (fπ, fθ1) + 1 = Eδ,(cid:101)δ

(cid:32)

1 −

(7.27)

χ2 (fθ1, fθ0) + 1 = Eδ,(cid:101)δ

(cid:33)− n

2

.

2(cid:32)
(cid:33)− n

(cid:124)(cid:101)δ

δ
(cid:107)δ(cid:107)2
2 + σ2
0

(cid:32)
1 − (cid:107)δ(cid:107)2

2(cid:107)(cid:101)δ(cid:107)2

2

σ4
0

1 −

(cid:124)(cid:101)δ
(cid:107)(cid:101)δ(cid:107)2
(cid:33)− n

2

δ
2 + σ2
0

.

The following lemma (Lemma 3 in [6]) controls the right hand side of

(7.26).

Lemma 4. Suppose the random variable J follows Hypergeometric (p, k, k)

with P (J = j) =

, then we have

j)(p−k
(k
k−j)
(p
k)

(7.28)

E exp (tJ) ≤ e

k2
p−k

1 − k
p

+

k
p

(cid:18)

(cid:19)k

exp (t)

.

By the construction (7.23), we have (cid:107)(cid:101)δ(cid:107)2
1−x ≤ exp(2x) for x ∈ (cid:104)
(cid:124)(cid:101)δ ≤ (k2 − k0)ρ2. By the inequality 1
(cid:33)− n
(7.29)(cid:32)
(cid:124)(cid:101)δ

(cid:105)
2 = (k2 − k0)ρ2 and
, if
(cid:33)

(cid:33)− n
2(cid:32)

δ
(k2−k0)ρ2

2 = (cid:107)δ(cid:107)2

2 , we have

0, log 2
2

(cid:124)(cid:101)δ

< log 2

(cid:32)

σ2
0

δ

2 ≤ exp

(cid:124)(cid:101)δ
(cid:107)(cid:101)δ(cid:107)2

δ
2 + σ2
0

1 −

2n

(k2 − k0)ρ2 + σ2

0

1 −

δ
(cid:107)δ(cid:107)2
2 + σ2
0

(cid:32)

2n

(cid:33)

.

(cid:124)(cid:101)δ

δ
σ2
0

≤ exp

Let J denote the hypergeometric distribution with parameters (p1, k2 −

32

T. T. CAI AND Z. GUO

k0, k2 − k0). We further have
(7.30)

(cid:32)

(cid:18)

E exp

2n

= EJ exp

(cid:33)
(cid:124)(cid:101)δ
(cid:18)

δ
σ2
0

≤ e

(k2−k0)2
p1−(k2−k0)

1 − k2 − k0
(cid:114)

p1

+

p1

(cid:19)

(k2−k0)2
p1−(k2−k0)

Jρ2
2n
σ2
0
k2 − k0

≤ e

(cid:114) p1

p1

(k2 − k0)2

(cid:18) 1

(cid:19)(cid:19)(k2−k0)

k2 − k0

exp

(cid:18)

σ2
0

2nρ2

(cid:19)(k2−k0)

,

1 +

1√
p1

+

p1

(cid:18)
1 − k2 − k0
(cid:19)(k2−k0) ≤ e
(cid:110) n
log p , pγ(cid:111)

p1
(k2−k0)2
p1−(k2−k0)

where the ﬁrst inequality applies Lemma 4 and the second inequality follows

n

log

(k2−k0)2

σ0. If (k2 − k0) ≤ c min

by plugging ρ = 1
2
suﬃciently small positive constant c, we have (k2 − k0)ρ2 < log 2
2 σ2
(k2 − k0) ≤ cpγ with 0 < γ < 1
(cid:107)(cid:101)δ(cid:107)2
1. Since

2 , we have the following control of (7.27),

< log 2

2

for a

0. Since
(cid:107)δ(cid:107)2
=
σ2
0

2

σ2
0

= (k2−k0)ρ2

(cid:32)
1 − (cid:107)δ(cid:107)2

2(cid:107)(cid:101)δ(cid:107)2

2

σ2
0
(7.31)

Eδ,(cid:101)δ

σ4
0

(cid:33)− n

2 ≤ exp

where the inequality follows from 1

(cid:19)2(cid:33)

2 , we have χ2 (fπ, fθ1) ≤ 2
(cid:32)

(cid:18) (k2 − k0)ρ2
1−x ≤ exp(2x) for x ∈(cid:104)
(cid:114)

= exp

σ2
0

n

log

p1

(cid:16)



(k2 − k0)log
16n

(cid:105)

0, log 2
2

and the

(cid:17)2

 ,

p1

(k2−k0)2

n

n

(k2−k0)2
√
log p , we have χ2 (fθ1, fθ0) ≤ 2
(cid:18)
(cid:107)(cid:98)β − β∗(cid:107)2

equality follows by plugging in ρ = 1
σ0. Under the assump-
2
tion (k2 − k0) ≤ c
1 and hence L1 (fπ, fθ1) ≤
(cid:19)
(cid:17) 2
1, L1 (fθ1, fθ0) ≤ 1 and L1 (fπ, fθ0) ≤ 21. Note that d = (k2 − k0)
1
q ρ.
(cid:17) 1
By (2.6) and (cid:107)β∗(cid:107)0 ≤ k0, we establish Pθ0
≥
q d2
1 − α0. By (5.4) and the fact C
Proof of (7.21)
The proof of (7.21) is based on the exactly same argument with (7.18) by

k2−k0
q (cid:28) 1, we establish (7.18).

(cid:16) k0

(cid:16) k0

q ≤ C

k2−k0

σ0. Since k2 (cid:29) √

n

log p and k0 (cid:28) k2, then

taking ρ = (log(cid:0)1 + 2
(log(cid:0)1 + 2

(cid:1))

1
4

1

√

(cid:1))

1
4

1
4

n

σ0 ≤ 1

σ0 and hence (7.30) holds. It is

p1

(k2−k0)2

n
suﬃcient to control the following term

n

1
4

1

1
k2−k0

(cid:32)
1 − (cid:107)δ(cid:107)2

2(cid:107)(cid:101)δ(cid:107)2

2

σ4
0

(7.32) Eδ,(cid:101)δ

(cid:32)

(cid:18) (k2 − k0)ρ2

σ2
0

n

(cid:19)2(cid:33)

2 ≤ exp

≤ 1 + 2
1.

1
k2−k0
log

√

(cid:114)
(cid:33)− n

2

(cid:113)
log(cid:0)1 + 2

(cid:1)(k2−k0)

In this case, d2 =

1 − α0. Since k0 (cid:28) min{k1,

2

q −1 1√

0 and Pθ0

n σ2

1
√
log p} and k1 ≤ k2, we have C

n

HIGH-DIMENSIONAL ACCURACY ASSESSMENT

(cid:32)
(cid:107)(cid:98)β − β∗(cid:107)2

q ≤ C

33

2
q
k
0

(k2−k0)

log p

n
2

q −1 1√

n

2
q
k
0

log p

n
2

q −1 1√

n

(k2−k0)
(cid:28) 1

(cid:33)

d2

≥

(cid:32)

(cid:40)

F2 =

and the lower bound (7.21) follows from (5.4) of Theorem 8.
Proof of (7.22) The proof of (7.22) is an application of (5.7) of Theorem
9. The key is to construct parameter spaces F1, F2 and the points θ1 and θ2
and then control the distribution distances between the density functions.
For θ0 = (β∗, I, σ0), we construct
(cid:33)(cid:41)
(cid:114) k1 − k0
(7.33)F1 = {(β∗ + ν, I, σ0) : ν ∈ (cid:96) (β∗, k1 − k0, ρ)} ⊂ Θ0 (k1) ;
(cid:113) k1−k0
(cid:17)
 (cid:107)β∗(cid:107)2

where (cid:96) (β∗, k2 − k0, ρ) and (cid:96)
are deﬁned in (7.24).
Let πi denote the uniform prior on the parameter space Fi for i = 1, 2. The
corresponding parameter spaces for Σz corresponding to F1 and F2 are
(β∗
S)
Ik0×k0
0p1×k0

(β∗ + δ, I, σ0) : δ ∈ (cid:96)

β∗, k2 − k0,

β∗, k2 − k0,

⊂ Θ0 (k2) ,

H1 = {Σz

2 + (cid:107)ν(cid:107)2

k2 − k0

2 + σ2
0

k2−k0

β∗
S
ν

(cid:16)

ν =

ρ

ρ

(cid:124)

ν : ν ∈ (cid:96) (β∗, k1 − k0, ρ)} , where Σz
(cid:33)(cid:41)

(cid:114) k1 − k0

(cid:32)

δ : δ ∈ (cid:96)
Σz

β∗, k2 − k0,

(cid:40)

k2 − k0

and

H2 =

 (cid:107)β∗(cid:107)2

2 + (cid:107)δ(cid:107)2

2 + σ2
0

β∗
S
δ

ρ

, where Σz

δ =

0 + dist2

2). Since dist2

argument of (7.26) in Lemma 3, we have

Deﬁne θ1 = (β∗, I, σ2
(cid:107)δ(cid:107)2

(cid:0)β∗, I, σ2
0 + (k1 − k0)ρ2(cid:1). In this case, we have L1 (fθ2, fθ1) = 0. By the same
χ2 (fπ1, fθ1) + 1 = Eν,(cid:101)ν

1) and θ2 = (β∗, I, σ2
0 + dist2
1 =
2 = (cid:107)ν(cid:107)2
2 = (k1 − k0)ρ2 and dist2
2 = (k1 − k0)ρ2, we have θ1 = θ2 =
(cid:19)− n
2(cid:18)
(cid:18)
(cid:124)(cid:101)ν
(cid:33)− n
(cid:32)
2(cid:32)

(cid:19)− n
(cid:33)− n

ν
(cid:107)δ(cid:107)2
2 + σ2
0

ν
2 + σ2
0

1 −

1 −

and

,

2

2

χ2 (fπ2, fθ2) + 1 = Eδ,(cid:101)δ

1 −

δ
(cid:107)δ(cid:107)2
2 + σ2
0

1 −

(cid:124)(cid:101)ν
(cid:107)(cid:101)ν(cid:107)2
(cid:124)(cid:101)δ
(cid:107)(cid:101)δ(cid:107)2

δ
2 + σ2
0

(cid:124)(cid:101)δ

.

 .

(cid:124)

ν

0k0×p1
Ip1×p1

(cid:124)

(β∗
S)
Ik0×k0
0p1×k0

(cid:124)

δ

0k0×p1
Ip1×p1

 .

1
q
Ck
0
q − 1
1

2 (k1−k0)

(cid:28)

1
2

(k2−k0)

2

q −1(k1 − k0)ρ2 − (1 + c1)2 (k1 − k0)

q ρ2(cid:17)

2

.

+

34

(cid:114)

log

p1

(k2−k0)2

T. T. CAI AND Z. GUO

σ0, a similar argument to (7.29) and (7.30) leads
2 = (k2 −
q ≤ c2
i d2
i

(cid:16)(cid:107)(cid:98)β − β∗(cid:107)2

1 = (k1 − k0)

(cid:17) ≥

q ρ2, d2

2

n

q −1(k1 − k0)ρ2. The assumption (2.7) leads to Pθi
(cid:28) 1 and c2 =

Taking ρ = 1
2
to L1 (fπi, fθi) ≤ 1 for i = 1, 2. Note that d2
k0)
1− α0, for i = 1, 2, where c1 = Ck
(k1−k0)
1. By (5.7), we obtain
L∗

Θ0 (k1) , Θ0 (k2) ,(cid:98)β, (cid:96)q

(1 − c2)2 (k2 − k0)

(cid:17) ≥ c

(cid:16)

(cid:16)

1
q
0

1
q

2

α

Since k0 (cid:28) min{k1,

√
log p} and k1 ≤ k2, we establish (7.22).

n

7.3. Proof of Theorems 3 and 5. The minimax lower bound of Theorem
3 follows from Theorem 4. We take k1 = k2 = k and (3.5) follows from
(3.7). The minimax lower bound (3.6) follows from (3.5) and Lemma 2.
The minimax lower bound of Theorem 5 follows from Theorem 6. We take
k1 = k2 = k and (3.13) follows from (3.18). The minimax lower bound (3.14)
follows from (3.13) and Lemma 2.

References.
[1] Mohsen Bayati and Andrea Montanari. The lasso risk for gaussian matrices. Infor-

mation Theory, IEEE Transactions on, 58(4):1997–2017, 2012.

[2] Alexandre Belloni, Victor Chernozhukov, and Lie Wang. Square-root lasso: pivotal
recovery of sparse signals via conic programming. Biometrika, 98(4):791–806, 2011.
[3] Peter J Bickel, Ya’acov Ritov, and Alexandre B Tsybakov. Simultaneous analysis of

lasso and dantzig selector. The Annals of Statistics, 37(4):1705–1732, 2009.

[4] Peter B¨uhlmann and Sara Van De Geer. Statistics for high-dimensional data: meth-

ods, theory and applications. Springer Science & Business Media, 2011.

[5] T Tony Cai and Zijian Guo.

Supplement to “accuracy assessment for high-

dimensional linear regression”. 2016.

[6] T Tony Cai and Zijian Guo. Conﬁdence intervals for high-dimensional linear regres-

sion: Minimax rates and adaptivity. The Annals of Statistics, to appear.

[7] T. Tony Cai and Harrison H Zhou. A data-driven block thresholding approach to

wavelet estimation. The Annals of Statistics, 37(2):569–595, 2009.

[8] Emmanuel Cand`es and Terence Tao. The dantzig selector: statistical estimation when

p is much larger than n. The Annals of Statistics, 35(6):2313–2351, 2007.

[9] David L Donoho and Iain M Johnstone. Adapting to unknown smoothness via wavelet
shrinkage. Journal of the American Statistical Association, 90(432):1200–1224, 1995.
[10] David L Donoho, Arian Maleki, and Andrea Montanari. The noise-sensitivity
Information Theory, IEEE Transactions

phase transition in compressed sensing.
on, 57(10):6920–6941, 2011.

[11] Lucas Janson, Rina Foygel Barber, and Emmanuel Cand`es. Eigenprism: Inference

for high-dimensional signal-to-noise ratios. arXiv preprint arXiv:1505.02097, 2015.

[12] Ker-Chau Li. From stein’s unbiased risk estimates to the method of generalized cross

validation. The Annals of Statistics, 13(4):1352–1377, 1985.

HIGH-DIMENSIONAL ACCURACY ASSESSMENT

35

[13] Richard Nickl and Sara van de Geer. Conﬁdence sets in sparse regression. The Annals

of Statistics, 41(6):2852–2876, 2013.

[14] Garvesh Raskutti, Martin J Wainwright, and Bin Yu. Minimax rates of estimation
for high-dimensional linear regression over-balls. Information Theory, IEEE Trans-
actions on, 57(10):6976–6994, 2011.

[15] Charles M Stein. Estimation of the mean of a multivariate normal distribution. The

Annals of Statistics, 9(6):1135–1151, 1981.

[16] Tingni Sun and Cun-Hui Zhang.

Scaled sparse linear regression. Biometrika,

101(2):269–284, 2012.

[17] Christos Thrampoulidis, Ashkan Panahi, and Babak Hassibi. Asymptotically exact

error analysis for the generalized (cid:96)2

2-lasso. arXiv preprint arXiv:1502.06287, 2015.

[18] Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the

Royal Statistical Society. Series B (Methodological), 58(1):267–288, 1996.

[19] Nicolas Verzelen. Minimax risks for sparse regressions: Ultra-high dimensional phe-

nomenons. Electronic Journal of Statistics, 6:38–90, 2012.

[20] Fei Ye and Cun-Hui Zhang. Rate minimaxity of the lasso and dantzig selector for the
(cid:96)q loss in (cid:96)r balls. The Journal of Machine Learning Research, 11:3519–3540, 2010.
[21] Feng Yi and Hui Zou. SURE-tuned tapering estimation of large covariance matrices.

Computational Statistics & Data Analysis, 58:339–351, 2013.

DEPARTMENT OF STATISTICS
THE WHARTON SCHOOL
UNIVERSITY OF PENNSYLVANIA
PHILADELPHIA, PENNSYLVANIA 19104
USA
E-mail: tcai@wharton.upenn.edu
URL: http://www-stat.wharton.upenn.edu/∼tcai/

zijguo@wharton.upenn.edu

