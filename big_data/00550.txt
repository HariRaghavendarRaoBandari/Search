Synthesized Classiﬁers for Zero-Shot Learning

Soravit Changpinyo∗, Wei-Lun Chao∗
Department of Computer Science
University of Southern California

Los Angeles, CA

Boqing Gong

Center for Research in Computer Vision

University of Central Florida

Orlando, FL

6
1
0
2

 
r
a

M
2

 

 
 
]

V
C
.
s
c
[
 
 

1
v
0
5
5
0
0

.

3
0
6
1
:
v
i
X
r
a

schangpi, weilunc@usc.edu

bgong@crcv.ucf.edu

Fei Sha

Department of Computer Science

University of California

Los Angeles, CA
feisha@cs.ucla.edu

Abstract

Given semantic descriptions of object classes, zero-
shot learning aims to accurately recognize objects of the
unseen classes, from which no examples are available
at the training stage, by associating them to the seen
classes, from which labeled examples are provided. We
propose to tackle this problem from the perspective of
manifold learning. Our main idea is to align the seman-
tic space that is derived from external information to the
model space that concerns itself with recognizing visual
features. To this end, we introduce a set of “phantom”
object classes whose coordinates live in both the seman-
tic space and the model space. Serving as bases in a
dictionary, they can be optimized from labeled data such
that the synthesized real object classiﬁers achieve opti-
mal discriminative performance. We demonstrate supe-
rior accuracy of our approach over the state of the art
on four benchmark datasets for zero-shot learning, in-
cluding the full ImageNet Fall 2011 dataset with more
than 20,000 unseen classes.

1. Introduction

Visual recognition has made signiﬁcant progress due
to the widespread use of deep learning architectures [18,
36] that are optimized on large-scale datasets of human-
labeled images [32]. Despite the exciting advances, to
recognize objects “in the wild” remains a daunting chal-

∗ Equal contributions

lenge. Many objects follow a long-tailed distribution:
in contrast to common objects such as household items,
they do not occur frequently enough for us to collect and
label a large set of representative exemplar images.

For example, this challenge is especially crippling
for ﬁne-grained object recognition (classifying species
of birds, designer products, etc.). Suppose we want to
carry a visual search of “Chanel Tweed Fantasy Flap
Handbag”. Where handbag, ﬂap, tweed, and Chanel are
popular accessory, style, fabric and brand respectively,
the combination of them is rare — the term generates
about 55,000 results on Google search with a very few
number of images. The amount of labeled images is thus
far from enough for directly building a high-quality clas-
siﬁer, unless we treat this category as a composition of
attributes, for each of which more training data can be
easily acquired [20].

It is thus imperative to develop methods for zero-shot
learning, namely, to expand classiﬁers and the space of
possible labels beyond seen objects, of which we have
access to the labeled images for training, to unseen ones,
of which no labeled images are available [26, 20]. To
this end, we need to address two key interwoven chal-
lenges [26]: (1) how to relate unseen classes to the seen
ones and (2) how to attain optimal discriminative per-
formance on the unseen classes even though we do not
have their labeled data.

To address the ﬁrst challenge, researchers have been
using visual attributes [19, 9, 27] and word vectors [23,
10, 35] to associate seen and unseen classes. We call
them the semantic embeddings of objects. Some work

1

takes advantage of them directly as the middle layer be-
tween input images and output class labels [20, 11, 12,
10, 25], whereas others extract new representations from
the semantic embeddings by linear (or bilinear) transfor-
mation [1, 21, 31] and sparse coding [40, 17]. For the
second challenge, the hand-designed probabilistic mod-
els in [20] have been competitive baselines. Nonethe-
less, more recent studies show that the nearest neighbor
classiﬁer in the semantic space is very effective [11, 25].
This is further strengthened by learning a distance met-
ric [13]. Finally, classiﬁers for the unseen classes in the
input feature space can be constructed [1, 40, 21, 8, 22].
In this paper, we tackle these two challenges with
ideas from manifold learning [5, 14], converging to a
two-pronged approach. We view object classes in a se-
mantic space as a weighted graph where the nodes cor-
respond to object class names and the weights of the
edges represent how they are related. Various informa-
tion sources can be used to infer the weights — human-
deﬁned attributes or context-based word vectors learnt
from language corpora. On the other end, we view mod-
els for recognizing visual images of those classes as if
they live in a space of models. In particular, the param-
eters for each object model are nothing but coordinates
in this model space whose geometric conﬁguration also
reﬂects the relatedness among objects. Fig. 1 illustrates
this idea conceptually.

But how do we align the semantic space and the
model space? The semantic space coordinates of ob-
jects are designated or derived based on external infor-
mation (such as textual data) that do not directly ex-
amine visual appearances at the lowest-level, while the
model space concerns itself largely for recognizing low-
level visual features. To align them, we view the co-
ordinates in the model space are the projection of the
vertices on the graph from the semantic space – there
is a wealth of literature on manifold learning on com-
puting (low-dimensional) Euclidean space embeddings
from the weighted graph, for example, the well-known
algorithm of Laplacian eigenmaps [5].

To adapt the embeddings (or the coordinates in the
model space) to data, we introduce a set of phantom ob-
ject classes — the coordinates of these classes in both
the semantic space and the model space are adjustable
and optimized such that the resulting model for the real
object classes achieve the best performance in discrimi-
native tasks. However, as their names imply, those phan-
tom classes do not correspond to and are not optimized
to recognize any real classes directly. For mathemati-
cal convenience, however, we parameterize the weighted
graph in the semantic space with the phantom classes in

such a way that the model for any real class is a con-
vex combinations of the coordinates of those phantom
classes. In other words, the “models” for the phantom
classes can also be interpreted as base (classiﬁers) in a
dictionary from which a large number of classiﬁers for
real classes can be synthesized via convex combinations.
In particular, when we need to construct classiﬁers for an
unseen class, we will compute the convex combination
coefﬁcients from this class’s semantic space coordinates
and use them to form the corresponding classiﬁer.

To summarize, our main contribution is a novel ideal
to cast the challenging problem of recognizing unseen
classes as learning manifold embeddings from graphs
composed of object classes. As a concrete realization of
this idea, we show how to parameterize the graph with
the locations of the phantom classes, and how to derive
embeddings (ie, recognition models) as convex combi-
nations of base classiﬁers. Our empirical study exten-
sively test our synthesized classiﬁers on four benchmark
datasets for zero-shot learning, including the full Ima-
geNet Fall 2011 release [7] with 20,345 unseen classes.
The experimental results are very encouraging; the syn-
thesized classiﬁers outperform several state-of-the-art
methods, including attaining better or matching perfor-
mance as Google’s ConSE algorithm [25] on the large-
scale setting.

The rest of the paper is organized as follows. Section
2 gives an overview of relevant literature. We describe
our approach in more details in Section 3, and demon-
strate its effecetiveness in Section 4. We conclude with
discussions in Section 5.

2. Related Work

In order to transfer knowledge between classes, zero-
shot learning relies on semantic embeddings of class la-
bels, including attributes (both manually deﬁned [20,
38, 1] and discriminatively learned
[39, 3]), word
vectors [30, 35, 10, 25], knowledge mined from the
Web [22, 30, 29, 8], or a combination of several em-
beddings [11, 2, 13].

Given such semantic embeddings, the approaches to
zero-shot learning mostly fall into embedding-based and
similarity-based methods. In the embedding-based ap-
proaches, one ﬁrst maps the input image representations
to the semantic space, and then determines the class la-
bels in this space by various relatedness measures im-
plied by the class embeddings [20, 38, 1, 35, 10, 25, 11,
2, 13]. Our work as well as recent existing work com-
bines these two stages [38, 10, 2, 31], leading to a uniﬁed
framework empirically shown to have, in general, lower
computation cost and more accurate predictions. In ad-

Figure 1: Illustration of our method for zero-shot learning. Object classes live in two spaces. They are characterized in the
semantic space with semantic embeddings (as) such as attributes and word vectors of their names. They are also represented as
models for visual recognition (ws) in the model space. In both spaces, those classes form weighted graphs. The main idea behind
our approach is that these two spaces should be aligned. In particular, the coordinates in the model space should be the projection of
the graph vertices from the semantic space to the model space — preserving class relatedness encoded in the graph. We introduce
adaptable phantom classes (b and v) to connect seen and unseen classes — the classiﬁers for the phantom classes are the bases for
synthesizing classiﬁers for real classes. In particular, the synthesis takes the form of convex combination.

dition to directly using the ﬁxed semantic embeddings,
some work maps them into a different space through lin-
ear (or bilinear) transformation [1, 21, 31] and sparse
coding [40, 17].

In the similarity-based approaches, in contrast, one
builds the classiﬁers of unseen classes by relating them
to seen ones via class-wise similarities [30, 29, 8, 22].
Our approach shares similar spirit to but offers richer
modeling ﬂexibilities and cleaner formulations thanks to
the introduction of phantom classes.

Finally, our convex combination of base classiﬁers
for synthesizing real classiﬁers can also be motivated
from multi-task learning with shared representations [4].
While labeled examples of each task are required in [4],
our method has no access to data of the unseen classes.

3. Approach

We describe our methods for addressing zero-shot
learning where the task is to classify images from un-
seen classes into the label space of unseen classes.

Notations Suppose we have training data D =
{(xn ∈ RD, yn)}N
n=1 with the labels coming from the
label space of seen classes S = {1, 2,··· , S}. Denote
by U = {S + 1,··· , S + U} the label space of unseen
classes.

We focus on linear classiﬁers in the visual feature

space RD that assigns a label ˆy to a data point x by

ˆy = arg max

c

wT

c x,

(1)

where wc ∈ RD, although our approach can be readily
extended to nonlinear settings by the kernel trick [33].

3.1. Main idea
Manifold learning
The main idea behind our ap-
proach is shown by the conceptual diagram in Fig. 1.
Each class c has a coordinate ac and they live on a man-
ifold in the semantic embedding space.
In this paper,
we explore two types of such spaces: attributes [20, 37]
and class name embedding via word vectors [24]. We
use attributes in this text to illustrate the idea and in the
experiments we test our approach on both types.

Additionally, we introduce a set of phantom
classes associated with semantic embeddings br, r =
1, 2, . . . , R. We stress that they are phantom as they
themselves do not correspond to any real objects – they
are introduced to increase the modeling ﬂexibility, as
shown below.

The real and phantom classes form a weighted bipar-

tite graph, with the weights deﬁned as
(cid:80)R
exp{−d(ac, br)}
r=1 exp{−d(ac, br)}

scr =

(2)

to correlate a real class c and a phantom class r, where

d(ac, br) = (ac − br)T Σ−1(ac − br),

and Σ−1 is a parameter that can be learned from the
data, modeling the correlation among attributes. For
simplicity, we set Σ = σ2I and tune the scalar free
hyper-parameter σ by cross-validation. The more gen-
eral Mahalanobis metric can be used and we propose
one way of learning such metric as well as demonstrate
its effectiveness in the Suppl.

The speciﬁc form of deﬁning weights is motivated
by several manifold learning work such as SNE [14].

Semantic space Model space penguin cat dog We have the standard Crammer-Singer loss when the
structured loss ∆(c, yn) = 1, which, however,
ig-
nores the semantic relatedness between classes. We
additionally use the (cid:96)2 distance for the structured loss
∆(c, yn) = (cid:107)ac − ayn(cid:107)2
2 to exploit the class relatedness
in our experiments. These two learning settings have
separate strengths and weakness in empirical studies.

Learning semantic embeddings The weighted graph
eq. (2) is also parameterized by adaptable embeddings
of the phantom classes br. For this work, however, for
simplicity, we assume that each of them is a sparse linear
combination of the seen classes’ attribute vectors:

S(cid:88)

c=1

In particular, scr can be interpreted as the conditional
probability of observing class r in the neighborhood of
class c. However, other forms can be explored and are
left for future work.

In the model space, each real class is associated with
a classiﬁer wc and the phantom class r is associated with
a virtual classiﬁer vr. We align the semantic and the
model spaces by viewing wc (or vr) as the embedding
of the weighted graph. In particular, we appeal to the
idea behind Laplacian eigenmaps [5], which seeks the
embedding that maintains the graph structure as much
as possible, equally, the distortion error

(cid:107)wr − R(cid:88)

scrvr(cid:107)2

2

min
wr,vr

is minimized. This objective has an analytical solution

wc =

scrvr,

∀ c ∈ T = {1, 2,··· , S + U} (3)

R(cid:88)

r=1

In other words, the solution gives rise to the idea of syn-
thesizing classiﬁers from those virtual classiﬁers vr. For
conceptual clarity, we refer from now vr as base classi-
ﬁers in a dictionary from which new classiﬁers can be
synthesized. We identify several advantages. First, we
could construct an inﬁnite number of classiﬁers as long
as we know how to compute scr. Second, by making
R (cid:28) S, the formulation can signiﬁcantly reduce the
learning cost as we only need to learn R base classiﬁers.
3.2. Learning phantom classes
Learning base classiﬁers We learn the base classi-
ﬁers {vr}R
r=1 from the training data (of the seen classes
only). We experiment with two settings. To learn one-
versus-other classiﬁers, we optimize,

min
v1,··· ,vR

(cid:96)(xn, Iyn,c; wc) +

λ
2

(cid:107)wc(cid:107)2

2 , (4)

s.t. wc =

scrvr,

∀ c ∈ T = {1,··· , S + U}

r=1

where (cid:96)(x, y; w) = max(0, 1 − ywTx)2 is the squared
hinge loss. The indicator Iyn,c ∈ {−1, 1} denotes
whether or not yn = c.

Alternatively, we apply the Crammer-Singer multi-

class SVM loss [6], given by

(cid:96)cs({wc}S
= max(0, max

c=1, yn, xn)
c∈S−{yn} ∆(c, yn) + wc

Txn − wyn

Txn),

S(cid:88)

c=1

N(cid:88)
R(cid:88)

n=1

S(cid:88)

c=1

r=1

br =

βrcac,∀r ∈ {1,··· , R},

Thus, to optimize those embeddings, we solve the fol-
lowing optimization

S(cid:88)
N(cid:88)
R(cid:88)

c=1

n=1

γ
2

{vr}R

+ η

min

r,c=1

R,S(cid:88)

r=1,{βrc}R,S
|βrc| +

r,c=1

r=1

(cid:96)(xn, Iyn,c; wc) +

λ
2

(cid:107)wc(cid:107)2

2

S(cid:88)

c=1

((cid:107)br(cid:107)2

2 − h2)2,

r=1, we learn combination weights {βrc}R,S

where h is a predeﬁned scalar equal to the norm of real
attribute vectors (i.e., 1 in our experiments since we per-
form (cid:96)2 normalization). Note that in addition to learn-
ing {vr}R
r,c=1.
Clearly, the constraint together with the third term in
the objective encourages the sparse linear combination
of the seen classes’ attribute vectors. The last term in
the objective demands that the norm of br be not too far
from the norm of ac.
We perform alternating optimization on minimiz-
ing the objective function with respect to {vr}R
r=1 and
{βrc}R,S
r,c=1, alternatively. While this process is non-
convex, there are useful heuristics to initialize the op-
timization routine.
then
the simplest setting is to let br = ar for r =
1, . . . , R.
them be (ran-
domly) selected from the seen classes’ attribute vectors
{b1, b2,··· , bR} ⊆ {a1, a2,··· , aS}, or ﬁrst perform
clustering on {a1, a2,··· , aS} and then let each br be
a combination of the seen classes’ attribute vectors in
cluster r.
If R > S, we could use a combination of
the above two strategies. We describe in more detail
how to optimize and cross-validate hyperparameters in
the Suppl.

If R ≤ S, we can let

For example,

if R = S,

3.3. Comparison to several existing methods

Table 1: Key characteristics of studied datasets

We contrast our approach to some existing methods.
[22] combines pre-trained classiﬁers of seen classes
to construct new classiﬁers. To estimate the seman-
tic embedding (e.g., word vector) of a testing image,
[25] uses the decision values of pre-trained classiﬁers
of seen objects to weighted average the corresponding
semantic embeddings. Neither of them has the notion
of base classiﬁers, which we introduce for constructing
the classiﬁers and nothing else. We thus expect them
to be more effective in transferring knowledge between
seen and unseen classes than overloading the pretrained
and ﬁxed classiﬁers of the seen classes for dual duties.
We note that [1] can be considered as a special case of
our method. In [1], each attribute corresponds to a base
and the “real” classiﬁer corresponding to an actual ob-
ject is represented as the linear combination of those
bases where the weights are the real object’s “descrip-
tion” in attributes. This modeling is limiting as the num-
ber of bases is fundamentally limited by the number of
attributes. Moreover, the model is strictly a subset of our
model.1

4. Experiments

We evaluate our methods and compare to exist-
ing state-of-the-art ones on several benchmark datasets.
While there are a large degree of variations in the current
empirical studies in terms of datasets, evaluation proto-
cols, experimental settings and implementation details,
we strive to provide a comprehensive comparison to as
many methods as possible, not only citing the published
results but also reimplementing some of those methods
to exploit several crucial insights we have discovered in
studying our methods.

We summarize our main results in this section. More
extensive details are reported in the Suppl. We provide
not only comparison in recognition accuracy but also
analysis in an effort to understand the sources of better
performance.
4.1. Setup
Datasets We use four benchmark datasets in our ex-
the Animals with Attributes (AwA) [20],
periments:
CUB-200-2011 Birds (CUB) [37], SUN Attribute
(SUN)
(with full 21,841

[28] and the ImageNet

1For interested readers, if we set the number of attributes as the
number of phantom classes, and use Gaussian kernel with anisotrop-
ically diagonal covariance matrix in eq. (3.1) with properly set band-
widths (either very small or very large) for each attribute, we will
recover the formulation in [1] when the bandwidths tend to zero or
inﬁnity.

Dataset
name
AwA†
CUB‡
SUN‡

ImageNet§

# of seen
classes

# of unseen

classes

40
150

645/646

1000

10
50

72/71
20,842

Total #
of images
30,475
11,788
14,340

14,197,122

†: follow the prescribed split in [20].
‡: 4 (or 10, respectively) random splits, reporting average.
§: seen and unseen classes from ImageNet ILSVRC 2012
1K [32] and Fall 2011 release [7, 10, 25].

classes) [32]. Table 1 summarizes their key character-
istics. The Suppl. provides more details.

Semantic spaces For the classes in AwA, we use 85-
dimensional binary or continuous attributes [20], as well
as the 100- and 1,000-dimensional word vectors [23],
derived from their class names and extracted by Fu et
al. [11, 12]. For CUB and SUN, we use 312 and 102
dimensional continuous valued attributes, respectively.
We also thresh them at the global means to obtain binary
valued attributes, as suggested in [20]. Neither datasets
have word vectors for their class names. For ImageNet,
we train a skip-gram language model [23, 24] on the
latest Wikipedia dump corpus2 (with more than 3 bil-
lion words) to extract a 500-dimensional word vector for
each class. Details of this training are in the suppl. ma-
terial. We ignore those classes without word vectors in
the experiments, which result in 20,345 (out of 20,842)
unseen classes. For both the continuous attribute vec-
tors and the word vector embeddings of the class names,
we normalize them to have unit (cid:96)2 norms unless stated
otherwise.

Visual features Due to the large number of differ-
ent features being used in literature, it is impractical
to try all possible combinations of features and meth-
ods. Thus, we make a major distinction in using shallow
features (such as color histograms, SIFT, PHOG, Fisher
vectors) [20, 38, 30, 1, 2] and deep learning features in
several recent studies of zero-shot learning. Whenever
possible, we use (shallow) features provided by those
datasets or prior studies. For comparative studies, we
also extract the following deep features: AlexNet [18]
for AwA and CUB and GoogLeNet [36] for all datasets
(all extracted with the Caffe package [16]). For AlexNet

2http://dumps.wikimedia.org/enwiki/latest/

enwiki-latest-pages-articles.xml.bz2 on September
1, 2015

we use the 4,096-dimensional activations of the penul-
timate layer (fc7) as features, and for GoogLeNet we
extract features by the 1,024-dimensional activations of
the pooling units, as in [2]. Detailed procedures of com-
puting those features are in the Suppl.

Table 2: Comparison to the previously published results of
multi-way classiﬁcation accuracies (in %) on the task of zero-
shot learning. For each dataset, the best is in red and the 2nd
best in blue. (†Results reported by the authors on a particular
seen-unseen split that is not publicly available.)

Evaluation protocols For AwA, CUB, and SUN, we
use the (weighted, by class-size) multi-way classiﬁca-
tion accuracy, the same as in the previous work. Note
that the accuracy is always computed on images from
unseen classes.

Evaluating zero-shot learning on the large-scale Im-
ageNet requires substantial new components from eval-
uating on the other 3 datasets. First, two evaluation met-
rics are used, as in [10]: Flat hit@K (F@K) and Hierar-
chical precision@k (HP@K).

F@K is deﬁned as the percentage of test images for
which the model returns the true label in its top K pre-
dictions. Note that, F@1 is the multi-way classiﬁcation
accuracy. HP@K takes into account the hierarchical or-
ganization of object categories. For each true label, we
generate a ground-truth list of K closest categories in the
hierarchy and compute the degree of overlapping (i.e.,
precision) between the ground-truth and the model’s top
K predictions. For the detailed description of this met-
ric, please see the Appendix of [10] and the Suppl.

Secondly, following the procedure in [10, 25], we

evaluate on three scenarios of increasing difﬁculty:

• 2-hop contains 1,509 unseen classes that are within
two tree hops of the seen 1K classes according to
the ImageNet label hierarchy3.

• 3-hop contains 7,678 unseen classes that are within

three tree hops of seen classes.

• All contains all 20,345 unseen classes in the Ima-
geNet 2011 21K dataset that are not in the ILSVRC
2012 1K dataset.

The numbers of unseen classes are slightly different
from what are used in [10, 25] due to the missing se-
mantic embeddings (i.e., word vectors) for certain class
names.

In addition to reporting published results, we
have also reimplemented the state-of-the-art method
ConSE [25] on this dataset, introducing a few improve-
ments. Details are in the Suppl.

3http://www.image-net.org/api/xml/structure_

released.xml

SUN ImageNet
22.2
18.0

AwA CUB
41.4
42.2
43.4
37.4
66.7
49.3

18.0†
50.1†

-
-
-

-
-

Methods
DAP [20]
IAP [20]
BN [38]
ALE [1]
SJE [2]

ESZSL [31]
ConSE[25]
Ourso-vs-o
Ourscs
Oursstruct

-
-
-
-
-

-

69.7
68.4
72.9

53.4
51.6
54.7

62.8
52.9
62.7

-
-
-
-
-
-
1.4
1.4
-
1.5

Implementation details We cross-validate all hyper-
parameters – details are in the Suppl. We also experi-
ment setting different number of phantom classes (ie, R).
For convenience, we set it to be the same as the number
of seen classes S, each corresponding to ac. However,
our study (cf. Fig. 2) shows that when R is about 60% of
S, the performance saturates. We denote the 3 variants
of our methods in constructing classiﬁers (sec 3.2) by
Ourso-vs-o (one-versus-other), Ourscs (Crammer-Singer)
and Oursstruct (Crammer-Singer with structured loss).
4.2. Experimental results
4.2.1 Main results

Table 2 compares the proposed methods to the state-
of-the-art from the previously published results on the
benchmark datasets. While there is a large degree of
variation in implementation details, the main observa-
tion is that our methods attain the best performance in
most scenarios. In what follows, we analyze those re-
sults in details.

We also point out that the settings in several other
existing work are highly different from ours, we do not
include their results in the main text for fair compari-
son [11, 12, 13, 3, 39, 15] – but we include them in the
Suppl.
(In some cases, even with additional data and
attributes, those methods underperform ours.)

4.2.2 Large-scale zero-shot learning

One major limitation of many existing work on zero-
shot learning is that the number of unseen classes is of-
ten small, dwarfed by the number of seen classes. How-
ever, real-world computer vision systems need to face
a very large number of unseen objects. To this end,

Table 3: Zero-shot learning results on ImageNet. For both types of metrics, the higher the better.

Scenarios

Methods

Flat Hit@K

2-hop

3-hop

All

K=

ConSE[25]
ConSE by us

Ourso-vs-o
Oursstruct
ConSE [25]
ConSE by us

Ourso-vs-o
Oursstruct
ConSE [25]
ConSE by us

Ourso-vs-o
Oursstruct

1
9.4
8.3
10.5
9.8
2.7
2.6
2.9
2.9
1.4
1.3
1.4
1.5

2

15.1
12.9
16.7
15.3
4.4
4.1
4.9
4.7
2.2
2.1
2.4
2.4

5

24.7
21.8
28.6
25.8
7.8
7.3
9.2
8.7
3.9
3.8
4.5
4.4

10
32.7
30.9
40.1
35.8
11.5
11.1
14.2
13.0
5.8
5.8
7.1
6.7

20
41.8
41.7
52.0
46.5
16.1
16.4
20.9
18.6
8.3
8.7
10.9
10.0

5

Hierarchical precision@K
20
2
28.4
31.3
32.1
29.6
24.7
26.3
28.6
26.7
10.4
12.0
12.5
12.2

10
26.9
27.5
30.3
28.2
22.4
23.8
26.4
25.0
9.2
10.7
10.9
11.0

24.7
23.8
27.7
25.8
20.2
21.4
23.7
22.8
7.8
9.2
9.0
9.6

21.4
21.5
25.1
23.8
5.3
6.7
7.4
8.0
2.5
3.2
3.1
3.6

we evaluate our methods on the large-scale dataset Ima-
geNet.

Table 3 summarizes our results and compares to the
ConSE method [25], which is, to the best of our knowl-
edge, the state-of-the-art method on this dataset. Note
that in some cases, our own implementation (“ConSE
by us” in the table) performs slightly worse than the re-
ported results, possibly attributed to difference in visual
features, word vector embeddings and other implemen-
tation details. Nonetheless, the proposed methods (using
the same setting as “ConSE by us”) always outperform
both, especially in the very challenging scenario of All
where the number of unseen classes is 20,345, signiﬁ-
cantly more than the number of seen classes. Note also
that, for both types of metrics, when K is larger, the
improvement over the existing approaches is more pro-
nounced. It is also not surprising to notice that as the
number of unseen classes increases from the setting 2-
hop to All, the performance of both our methods and
ConSE degrade.

4.2.3 Detailed analysis

We experiment extensively to understand the beneﬁts of
many factors in our and other algorithms. While trying
all possible combinations is prohibitively exhaustive, we
have provided a comprehensive set of results for com-
parison and drawing conclusions.

Advantage of continuous attributes
It is clear from
Table 4 that, in general, continuous attributes as se-
mantic embeddings for classes attain better performance
than binary attributes. This is especially true where deep
learning features are used to construct classiﬁers. It is
somewhat expected that continuous attributes provide

Table 5: Compare attributes to word vectors on AwA

Semantic embedding

Dimensions Accuracy (%)

word vectors
word vectors

attributes

attributes + word vectors
attributes + word vectors

100
1000
85
185
1085

42.2
57.5
69.7
73.2
76.3

real-valued similarity measure among classes, presum-
ably exploited further by more powerful classiﬁers.

Advantage of deep features
It is also clear from Ta-
ble 4, across all methods, deep features signiﬁcantly
boost performance from shallow features. We use
GoogleLeNet and AlexNets (numbers in parenthesis)
and GoogleLeNet generally outperforms AlexNet.
In
more details, it is worthwhile to point out that the re-
ported results under deep feature columns are obtained
using linear classiﬁers, which outperform several non-
linear classiﬁers that use shallow features. This seems
to suggest that deep features, often thought to be specif-
ically adapted to seen training images, can still transfer
to unseen images [10].

Which type of semantic space? In Table 5, we con-
trast how effective our proposed method (Ourso-vs-o) ex-
ploit the two types of semantic spaces: (continuous) at-
tributes and word-vector embeddings on the AwA (this
is the only dataset which has both embeddings). We ﬁnd
that attributes yield better performance than the word-
vector embeddings. However, combining two gives the
best result. This suggests that these two semantic spaces
could be complementary and further investigation is en-
sured.

Table 6 takes a different view on identifying the best

Table 4: Detailed analysis of various methods, the effectiveness of feature and attribute types in multi-way classiﬁcation accuracies
(in %) . Within each column, the best is in red and the 2nd best in blue. We cite both previously published results (numbers in bold
italics) and results from our implementations of those competing methods (numbers in normal font) to enhance comparability and
to ease analysis (see texts for details).

Methods

Attribute

DAP [20]
IAP [20]
BN [38]
ALE‡ [1]

ALE

ConSE [25]

SJE [2]

SJE

ESZSL§ [31]

ESZSL

AMP (SR+SE)(cid:93) [13]

Ourso-vs-o
Ourscs
Oursstruct

type
binary
binary
binary
binary
binary

continuous
continuous
continuous
continuous
continuous
continuous
continuous
continuous
continuous

Shallow features

Deep features
CUB

AwA

60.5 (50.0)
57.2 (53.2)

39.1 (34.8)
36.7 (32.7)

-
-

49.7 (49.6)
63.3 (56.5)
66.7 (61.9)
66.3 (60.9)
59.6 (53.2)
64.5 (59.4)

66.0

69.7 (64.0)
68.4 (64.8)
72.9 (62.8)

-
-

35.8 (33.3)
36.2 (32.6)
50.1 (40.3)†
46.5 (41.1)
44.0 (37.2)
34.5 (28.0)

-

53.4 (46.6)
51.6 (45.7)
54.5 (47.1)

SUN
44.5
40.8

-
-

38.2
51.9

-

50.9
8.7
18.7

-

62.8
52.9
62.7

SUN
22.2
18.0
-
-
-
-
-
-
-
-
-
-
-
-

AwA CUB
41.4
28.3
42.2
24.4
43.4
18.0†
37.4
28.4
32.7
36.5
23.7

-

-

36.2
49.3
44.1

-

42.6
42.1
41.5

-

32.6
37.0
38.3

-

35.0
34.7
36.4

† Results reported by the authors on a particular seen-unseen split that is not publicly available.
‡ Based on Fisher vectors as shallow features.
§ On the attribute vectors without (cid:96)2 normalization, while our own implementation shows normalization helps.
(cid:93) Based on OverFeat [34] and using both attributes and 100-d word vectors.

Table 6: Effect of learning semantic embeddings

Dataset
AwA

CUB
SUN

type of embedding

w/o learning

w/ learning

attributes

100-d word vectors
1000-d word vectors

attributes
attributes

69.7%
42.2%
57.6%
53.4%
62.8%

71.1%
42.5%
56.6%
54.2%
63.3%

semantic space. There, we study whether we can learn
optimally the semantic embeddings for the phantom
classes who correspond to base classiﬁers. These pre-
liminary studies seem to suggest that learning attributes
could have a positive effect while it is difﬁcult to im-
prove over word-vector embeddings. We plan to study
this issue more thoroughly in the future.

How many base classiﬁers are necessary?
In Fig. 2,
we investigate how many base classiﬁers are needed —
so far, we have set that number to be the number of seen
classes out of convenience. The plot shows that in fact,
a smaller number (about 60% -70%) is enough for our
algorithm to reach the plateau of the performance curve.
Moreover, increasing the number of base classiﬁers does
not seem to have an overwhelming effect. Details are in
the Suppl.

Figure 2: We vary the number of phantom classes R as a
percentage of the number of seen classes S and investigate how
much that will affect recognition accuracy (vertical axis plots
the ratio with respect to the accuracy when R = S.)

5. Conclusion

We have developed a novel classiﬁer synthesis
mechanism for zero-shot learning by introducing the
notion of “phantom” classes. The phantom classes
connect the dots between the seen and unseen classes
— the classiﬁers of
the seen and unseen classes
are constructed from the same base classiﬁers for
the phantom classes and with the same coefﬁcient
functions. As a result, we can conveniently learn the
classiﬁer synthesis mechanism leveraging the labeled
data of the seen classes and then readily apply it to
the unseen classes. Our approach outperforms the
state-of-the-art methods on four benchmarks datasets.

20406080100120140160708090100110Ratio to the number of seen classes (%)Relative accuracy (%)  AwACUBReferences
[1] Z. Akata, F. Perronnin, Z. Harchaoui, and C. Schmid.
In

Label-embedding for attribute-based classiﬁcation.
CVPR, 2013. 2, 3, 5, 6, 8

[2] Z. Akata, S. Reed, D. Walter, H. Lee, and B. Schiele.
Evaluation of output embeddings for ﬁne-grained image
classiﬁcation. In CVPR, 2015. 2, 5, 6, 8

[3] Z. Al-Halah and R. Stiefelhagen. How to transfer? zero-
shot object recognition via hierarchical transfer of se-
mantic attributes. In WACV, 2015. 2, 6

[4] A. Argyriou, T. Evgeniou, and M. Pontil. Convex multi-
task feature learning. Machine Learning, 73(3):243–272,
2008. 3

[5] M. Belkin and P. Niyogi. Laplacian eigenmaps for di-
mensionality reduction and data representation. Neural
computation, 15(6):1373–1396, 2003. 2, 4

[6] K. Crammer and Y. Singer. On the algorithmic imple-
mentation of multiclass kernel-based vector machines.
JMLR, 2:265–292, 2002. 4

[7] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and
L. Fei-Fei. Imagenet: A large-scale hierarchical image
database. In CVPR, 2009. 2, 5

[8] M. Elhoseiny, B. Saleh, and A. Elgammal. Write a classi-
ﬁer: Zero-shot learning using purely textual descriptions.
In ICCV, 2013. 2, 3

[9] A. Farhadi, I. Endres, D. Hoiem, and D. Forsyth. De-

scribing objects by their attributes. In CVPR, 2009. 1

[10] A. Frome, G. S. Corrado, J. Shlens, S. Bengio, J. Dean,
M. Ranzato, and T. Mikolov. Devise: A deep visual-
semantic embedding model. In NIPS, 2013. 1, 2, 5, 6,
7

[11] Y. Fu, T. M. Hospedales, T. Xiang, Z. Fu, and S. Gong.
Transductive multi-view embedding for zero-shot recog-
nition and annotation. In ECCV, 2014. 2, 5, 6

[12] Y. Fu, T. M. Hospedales, T. Xiang, and S. Gong. Trans-
ductive multi-view zero-shot learning. TPAMI, 2015. 2,
5, 6

[13] Z. Fu, T. Xiang, E. Kodirov, and S. Gong. Zero-shot
In

object recognition by semantic manifold distance.
CVPR, 2015. 2, 6, 8

[14] G. E. Hinton and S. T. Roweis. Stochastic neighbor em-

bedding. In NIPS, 2002. 2, 3

[15] D. Jayaraman and K. Grauman. Zero-shot recognition

with unreliable attributes. In NIPS, 2014. 6

[16] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long,
R. Girshick, S. Guadarrama, and T. Darrell. Caffe: Con-
volutional architecture for fast feature embedding.
In
ACM Multimedia, 2014. 5

[17] E. Kodirov, T. Xiang, Z. Fu, and S. Gong. Unsupervised
domain adaptation for zero-shot learning. In ICCV, 2015.
2, 3

[18] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet
classiﬁcation with deep convolutional neural networks.
In NIPS, 2012. 1, 5

[19] C. H. Lampert, H. Nickisch, and S. Harmeling. Learning
to detect unseen object classes by between-class attribute
transfer. In CVPR, 2009. 1

[20] C. H. Lampert, H. Nickisch, and S. Harmeling. Attribute-
based classiﬁcation for zero-shot visual object catego-
rization. TPAMI, 36(3):453–465, 2014. 1, 2, 3, 5, 6,
8

[21] X. Li, Y. Guo, and D. Schuurmans. Semi-supervised
zero-shot classiﬁcation with label representation learn-
ing. In ICCV, 2015. 2, 3

[22] T. Mensink, E. Gavves, and C. G. Snoek.

Costa:
In

Co-occurrence statistics for zero-shot classiﬁcation.
CVPR, 2014. 2, 3, 5

[23] T. Mikolov, K. Chen, G. S. Corrado, and J. Dean. Efﬁ-
cient estimation of word representations in vector space.
In ICLR Workshops, 2013. 1, 5

[24] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and
J. Dean. Distributed representations of words and phrases
and their compositionality. In NIPS, 2013. 3, 5

[25] M. Norouzi, T. Mikolov, S. Bengio, Y. Singer, J. Shlens,
A. Frome, G. S. Corrado, and J. Dean. Zero-shot learn-
ing by convex combination of semantic embeddings. In
ICLR, 2014. 2, 5, 6, 7, 8

[26] M. Palatucci, D. Pomerleau, G. E. Hinton, and T. M.
Mitchell. Zero-shot learning with semantic output codes.
In NIPS, 2009. 1

[27] D. Parikh and K. Grauman. Relative attributes. In ICCV,

2011. 1

[28] G. Patterson, C. Xu, H. Su, and J. Hays. The sun at-
tribute database: Beyond categories for deeper scene un-
derstanding. IJCV, 108(1-2):59–81, 2014. 5

[29] M. Rohrbach, M. Stark, and B. Schiele. Evaluating
knowledge transfer and zero-shot learning in a large-
scale setting. In CVPR, 2011. 2, 3

[30] M. Rohrbach, M. Stark, G. Szarvas, I. Gurevych, and
B. Schiele. What helps where–and why? semantic re-
latedness for knowledge transfer. In CVPR, 2010. 2, 3,
5

[31] B. Romera-Paredes and P. H. S. Torr. An embarrassingly
simple approach to zero-shot learning. In ICML, 2015.
2, 3, 6, 8

[32] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,
S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,
A. C. Berg, and L. Fei-Fei. ImageNet Large Scale Visual
Recognition Challenge. IJCV, 2015. 1, 5

[33] B. Sch¨olkopf and A. J. Smola. Learning with kernels:
support vector machines, regularization, optimization,
and beyond. MIT press, 2002. 3

[34] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus,
and Y. LeCun. Overfeat: Integrated recognition, local-
ization and detection using convolutional networks.
In
ICLR, 2014. 8

[35] R. Socher, M. Ganjoo, C. D. Manning, and A. Ng. Zero-
In NIPS,

shot learning through cross-modal transfer.
2013. 1, 2

[36] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed,
D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabi-
novich. Going deeper with convolutions. CVPR, 2015.
1, 5

[37] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Be-
longie. The Caltech-UCSD Birds-200-2011 Dataset.
Technical Report CNS-TR-2011-001, California Insti-
tute of Technology, 2011. 3, 5

[38] X. Wang and Q. Ji. A uniﬁed probabilistic approach
modeling relationships between attributes and objects. In
ICCV, 2013. 2, 5, 6, 8

[39] F. X. Yu, L. Cao, R. S. Feris, J. R. Smith, and S.-F.
Chang. Designing category-level attributes for discrimi-
native visual recognition. In CVPR, 2013. 2, 6

[40] Z. Zhang and V. Saligrama. Zero-shot learning via se-

mantic similarity embedding. In ICCV, 2015. 2, 3

