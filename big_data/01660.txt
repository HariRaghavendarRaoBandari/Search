6
1
0
2

 
r
a

M
 
0
1

 
 
]

.

O
H
h
t
a
m

[
 
 

Introduction to Tensor Calculus

Taha Sochi∗

2
v
0
6
6
1
0

.

3
0
6
1
:
v
i
X
r
a

March 11, 2016

∗Department of Physics & Astronomy, University College London, Gower Street, London, WC1E 6BT.

Email: t.sochi@ucl.ac.uk.

1

2

Preface

These are general notes on tensor calculus originated from a collection of personal notes

which I prepared some time ago for my own use and reference when I was studying the

subject. I decided to put them in the public domain hoping they may be beneﬁcial to some

students in their eﬀort to learn this subject. Most of these notes were prepared in the

form of bullet points like tutorials and presentations and hence some of them may be more

concise than they should be. Moreover, some notes may not be suﬃciently thorough or

general. However this should be understandable considering the level and original purpose

of these notes and the desire for conciseness. There may also be some minor repetition

in some places for the purpose of gathering similar items together, or emphasizing key

points, or having self-contained sections and units.

These notes, in my view, can be used as a short reference for an introductory course on

tensor algebra and calculus. I assume a basic knowledge of calculus and linear algebra

with some commonly used mathematical terminology. I tried to be as clear as possible and

to highlight the key issues of the subject at an introductory level in a concise form. I hope

I have achieved some success in reaching these objectives at least for some of my target

audience. The present text is supposed to be the ﬁrst part of a series of documents about

tensor calculus for gradually increasing levels or tiers. I hope I will be able to ﬁnalize and

publicize the document for the next level in the near future.

CONTENTS

Contents

Preface

Contents

1 Notation, Nomenclature and Conventions

2 Preliminaries

2.1

Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.2 General Rules

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.3 Examples of Tensors of Diﬀerent Ranks . . . . . . . . . . . . . . . . . . . .

2.4 Applications of Tensors . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.5 Types of Tensors

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.5.1 Covariant and Contravariant Tensors . . . . . . . . . . . . . . . . .

2.5.2 True and Pseudo Tensors . . . . . . . . . . . . . . . . . . . . . . . .

2.5.3 Absolute and Relative Tensors . . . . . . . . . . . . . . . . . . . . .

2.5.4

Isotropic and Anisotropic Tensors . . . . . . . . . . . . . . . . . . .

2.5.5

Symmetric and Anti-symmetric Tensors . . . . . . . . . . . . . . . .

2.6 Tensor Operations

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.6.1 Addition and Subtraction . . . . . . . . . . . . . . . . . . . . . . .

2.6.2 Multiplication by Scalar

. . . . . . . . . . . . . . . . . . . . . . . .

2.6.3 Tensor Multiplication

. . . . . . . . . . . . . . . . . . . . . . . . .

2.6.4 Contraction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.6.5

Inner Product . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.6.6 Permutation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.7 Tensor Test: Quotient Rule

. . . . . . . . . . . . . . . . . . . . . . . . . .

3

2

3

5

10

10

12

15

16

17

17

22

24

25

25

27

28

28

29

30

31

33

33

CONTENTS

3 δ and  Tensors

3.1 Kronecker δ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.2 Permutation  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.3 Useful Identities Involving δ or/and  . . . . . . . . . . . . . . . . . . . . .

3.3.1

Identities Involving δ . . . . . . . . . . . . . . . . . . . . . . . . . .

3.3.2

Identities Involving  . . . . . . . . . . . . . . . . . . . . . . . . . .

3.3.3

Identities Involving δ and  . . . . . . . . . . . . . . . . . . . . . . .

3.4 Generalized Kronecker delta . . . . . . . . . . . . . . . . . . . . . . . . . .

4 Applications of Tensor Notation and Techniques

4.1 Common Deﬁnitions in Tensor Notation . . . . . . . . . . . . . . . . . . .

4.2 Scalar Invariants of Tensors

. . . . . . . . . . . . . . . . . . . . . . . . . .

4.3 Common Diﬀerential Operations in Tensor Notation . . . . . . . . . . . . .

4.3.1 Cartesian System . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4.3.2 Other Coordinate Systems . . . . . . . . . . . . . . . . . . . . . . .

4.4 Common Identities in Vector and Tensor Notation . . . . . . . . . . . . . .

4.5

Integral Theorems in Tensor Notation . . . . . . . . . . . . . . . . . . . . .

4.6 Examples of Using Tensor Techniques to Prove Identities . . . . . . . . . .

5 Metric Tensor

6 Covariant Diﬀerentiation

References

4

34

34

35

36

36

38

40

42

44

44

46

47

48

51

54

60

62

75

78

82

1 NOTATION, NOMENCLATURE AND CONVENTIONS

5

1 Notation, Nomenclature and Conventions

• In the present notes we largely follow certain conventions and general notations; most of

which are commonly used in the mathematical literature although they may not be univer-

sally adopted. In the following bullet points we outline these conventions and notations.

We also give initial deﬁnitions of the most basic terms and concepts in tensor calculus;

more thorough technical deﬁnitions will follow, if needed, in the forthcoming sections.
• Scalars are algebraic objects which are uniquely identiﬁed by their magnitude (abso-
lute value) and sign (±), while vectors are broadly geometric objects which are uniquely

identiﬁed by their magnitude (length) and direction in a presumed underlying space.
• At this early stage in these notes, we generically deﬁne “tensor” as an organized array

of mathematical objects such as numbers or functions.
• In generic terms, the rank of a tensor signiﬁes the complexity of its structure. Rank-0

tensors are called scalars while rank-1 tensors are called vectors. Rank-2 tensors may be

called dyads although this, in common use, may be restricted to the outer product of two

vectors and hence is a special case of rank-2 tensors assuming it meets the requirements

of a tensor and hence transforms as a tensor. Like rank-2 tensors, rank-3 tensors may

be called triads. Similar labels, which are much less common in use, may be attached to

higher rank tensors; however, none will be used in the present notes. More generic names

for higher rank tensors, such as polyad, are also in use.
• In these notes we may use “tensor” to mean tensors of all ranks including scalars (rank-0)

and vectors (rank-1). We may also use it as opposite to scalar and vector (i.e. tensor of

rank-n where n > 1). In almost all cases, the meaning should be obvious from the context.
• Non-indexed lower case light face Latin letters (e.g. f and h) are used for scalars.
• Non-indexed (lower or upper case) bold face Latin letters (e.g. a and A) are used for

vectors.
• Non-indexed upper case bold face Latin letters (e.g. A and B) are used for tensors (i.e.

1 NOTATION, NOMENCLATURE AND CONVENTIONS

6

of rank > 1).
• Indexed light face italic symbols (e.g. ai and Bjk

i ) are used to denote tensors of rank > 0

in their explicit tensor form (index notation). Such symbols may also be used to denote

the components of these tensors. The meaning is usually transparent and can be identiﬁed

from the context if not explicitly declared.
• Tensor indices in this document are lower case Latin letters usually taken from the

middle of the Latin alphabet like (i, j, k). We also use numbered indices like (i1, i2, . . . , ik)

when the number of tensor indices is variable.
• The present notes are largely based on assuming an underlying orthonormal Cartesian

coordinate system. However, parts of which are based on more general coordinate systems;

in these cases this is stated explicitly or made clear by the content and context.
• Mathematical identities and deﬁnitions may be denoted by using the symbol ‘≡’. How-

ever, for simplicity we will use in the present notes the equality sign “=” to mark identities

and mathematical deﬁnitions as well as normal equalities.
• We use 2D, 3D and nD for two-, three- and n-dimensional spaces. We also use Eq./Eqs.

to abbreviate Equation/Equations.
• Vertical bars are used to symbolize determinants while square brackets are used for

matrices.
• All tensors in the present notes are assumed to be real quantities (i.e. have real rather

than complex components).
• Partial derivative symbol with a subscript index (e.g. i) is frequently used to denote the
ith component of the Cartesian gradient operator ∇:

• A comma preceding a subscript index (e.g. , i) is also used to denote partial diﬀerentia-

∂i = ∇i =

∂
∂xi

(1)

1 NOTATION, NOMENCLATURE AND CONVENTIONS

tion with respect to the ith spatial coordinate in Cartesian systems, e.g.

A,i =

∂A
∂xi

7

(2)

• Partial derivative symbol with a spatial subscript, rather than an index, are used to

denote partial diﬀerentiation with respect to that spatial variable. For instance

∂r = ∇r =

∂
∂r

(3)

is used for the partial derivative with respect to the radial coordinate in spherical coordi-

nate systems identiﬁed by (r, θ, φ) spatial variables.
• Partial derivative symbol with repeated double index is used to denote the Laplacian

operator:

∂ii = ∂i∂i = ∇2 = ∆

(4)

The notation is not aﬀected by using repeated double index other than i (e.g. ∂jj or ∂kk).

The following notations:

∂2
ii

∂2

∂i∂i

(5)

are also used in the literature of tensor calculus to symbolize the Laplacian operator.

However, these notations will not be used in the present notes.
• We follow the common convention of using a subscript semicolon preceding a subscript

index (e.g. Akl;i) to symbolize covariant diﬀerentiation with respect to the ith coordinate
(see § 6). The semicolon notation may also be attached to the normal diﬀerential operators
to indicate covariant diﬀerentiation (e.g. ∇;i or ∂;i to indicate covariant diﬀerentiation with

respect to the index i).

1 NOTATION, NOMENCLATURE AND CONVENTIONS

8

• All transformation equations in these notes are assumed continuous and real, and all

derivatives are continuous in their domain of variables.
• Based on the continuity condition of the diﬀerentiable quantities, the individual diﬀer-

ential operators in the mixed partial derivatives are commutative, that is

∂i∂j = ∂j∂i

(6)

• A permutation of a set of objects, which are normally numbers like (1, 2, . . . , n) or

symbols like (i, j, k), is a particular ordering or arrangement of these objects. An even

permutation is a permutation resulting from an even number of single-step exchanges

(also known as transpositions) of neighboring objects starting from a presumed original

permutation of these objects. Similarly, an odd permutation is a permutation resulting

from an odd number of such exchanges. It has been shown that when a transformation

from one permutation to another can be done in diﬀerent ways, possibly with diﬀerent

number of exchanges, the parity of all these possible transformations is the same, i.e. all

even or all odd, and hence there is no ambiguity in characterizing the transformation from

one permutation to another by the parity alone.
• We normally use indexed square brackets (e.g. [A]i and [∇f ]i) to denote the ith compo-
nent of vectors, tensors and operators in their symbolic or vector notation.
• In general terms, a transformation from an nD space to another nD space is a corre-

lation that maps a point from the ﬁrst space (original) to a point in the second space

(transformed) where each point in the original and transformed spaces is identiﬁed by n

independent variables or coordinates. To distinguish between the two sets of coordinates

in the two spaces, the coordinates of the points in the transformed space may be notated

with barred symbols, e.g. (¯x1, ¯x2, . . . , ¯xn) or (¯x1, ¯x2, . . . , ¯xn) where the superscripts and

subscripts are indices, while the coordinates of the points in the original space are notated

with unbarred symbols, e.g. (x1, x2, . . . , xn) or (x1, x2, . . . , xn). Under certain conditions,

1 NOTATION, NOMENCLATURE AND CONVENTIONS

9

such a transformation is unique and hence an inverse transformation from the transformed

to the original space is also deﬁned. Mathematically, each one of the direct and inverse

transformation can be regarded as a mathematical correlation expressed by a set of equa-

tions in which each coordinate in one space is considered as a function of the coordinates

in the other space. Hence the transformations between the two sets of coordinates in

the two spaces can by expressed mathematically by the following two sets of independent

relations:

¯xi = ¯xi(x1, x2, . . . , xn)

&

xi = xi(¯x1, ¯x2, . . . , ¯xn)

(7)

where i = 1, 2, . . . , n. An alternative to viewing the transformation as a mapping between

two diﬀerent spaces is to view it as being correlating the same point in the same space but

observed from two diﬀerent coordinate frames of reference which are subject to a similar

transformation.
• Coordinate transformations are described as “proper” when they preserve the handedness

(right- or left-handed) of the coordinate system and “improper” when they reverse the

handedness. Improper transformations usually involve an odd number of coordinate axes

inversions through the origin.
• Inversion of axes may be called improper rotation while ordinary rotation is described

as proper rotation.
• Transformations can be active, when they change the state of the observed object (e.g.

translating the object in space), or passive when they are based on keeping the state of the

object and changing the state of the coordinate system from which the object is observed.

Such distinction is based on an implicit assumption of a more general frame of reference

in the background.
• Finally, tensor calculus is riddled with conﬂicting conventions and terminology. In this

text we will try to use what we believe to be the most common, clear or useful of all.

2 PRELIMINARIES

2 Preliminaries

2.1 Introduction

10

• A tensor is an array of mathematical objects (usually numbers or functions) which

transforms according to certain rules under coordinates change. In a d-dimensional space,

a tensor of rank-n has dn components which may be speciﬁed with reference to a given

coordinate system. Accordingly, a scalar, such as temperature, is a rank-0 tensor with

(assuming 3D space) 30 = 1 component, a vector, such as force, is a rank-1 tensor with

31 = 3 components, and stress is a rank-2 tensor with 32 = 9 components.
• The term “tensor” was originally derived from the Latin word “tensus” which means

tension or stress since one of the ﬁrst uses of tensors was related to the mathematical

description of mechanical stress.
• The dn components of a tensor are identiﬁed by n distinctive integer indices (e.g. i, j, k)

which are attached, according to the commonly-employed tensor notation, as superscripts

or subscripts or a mix of these to the right side of the symbol utilized to label the tensor

(e.g. Aijk, Aijk and Ajk

i ). Each tensor index takes all the values over a predeﬁned range

of dimensions such as 1 to d in the above example of a d-dimensional space. In general,

all tensor indices have the same range, i.e. they are uniformly dimensioned.
• When the range of tensor indices is not stated explicitly, it is usually assumed to have

the values (1, 2, 3). However, the range must be stated explicitly or implicitly to avoid

ambiguity.
• The characteristic property of tensors is that they satisfy the principle of invariance un-

der certain coordinate transformations. Therefore, formulating the fundamental physical

laws in a tensor form ensures that they are form-invariant; hence they are objectively-

representing the physical reality and do not depend on the observer. Having the same

form in diﬀerent coordinate systems may be labeled as being “covariant” but this word is

2.1 Introduction

11

also used for a diﬀerent meaning in tensor calculus as explained in § 2.5.1.
• “Tensor term” is a product of tensors including scalars and vectors.
• “Tensor expression” is an algebraic sum (or more generally a linear combination) of

tensor terms which may be a trivial sum in the case of a single term.
• “Tensor equality” (symbolized by ‘=’) is an equality of two tensor terms and/or expres-

sions. A special case of this is tensor identity which is an equality of general validity (the
symbol ‘≡’ may be used for identity as well as for deﬁnition).
• The order of a tensor is identiﬁed by the number of its indices (e.g. Ai

jk is a tensor of

order 3) which normally identiﬁes the tensor rank as well. However, when contraction (see
§ 2.6.4) takes place once or more, the order of the tensor is not aﬀected but its rank is

reduced by two for each contraction operation.1
• “Zero tensor” is a tensor whose all components are zero.
• “Unit tensor” or “unity tensor”, which is usually deﬁned for rank-2 tensors, is a tensor

whose all elements are zero except the ones with identical values of all indices which are

assigned the value 1.
• While tensors of rank-0 are generally represented in a common form of light face non-
indexed symbols, tensors of rank ≥ 1 are represented in several forms and notations,

the main ones are the index-free notation, which may also be called direct or symbolic or

Gibbs notation, and the indicial notation which is also called index or component or tensor

notation. The ﬁrst is a geometrically oriented notation with no reference to a particular

reference frame and hence it is intrinsically invariant to the choice of coordinate systems,

whereas the second takes an algebraic form based on components identiﬁed by indices

and hence the notation is suggestive of an underlying coordinate system, although being

a tensor makes it form-invariant under certain coordinate transformations and therefore

1In the literature of tensor calculus, rank and order of tensors are generally used interchangeably;
however some authors diﬀerentiate between the two as they assign order to the total number of indices,
including repetitive indices, while they keep rank to the number of free indices. We think the latter is
better and hence we follow this convention in the present text.

2.2 General Rules

12

it possesses certain invariant properties. The index-free notation is usually identiﬁed by

using bold face symbols, like a and B, while the indicial notation is identiﬁed by using

light face indexed symbols such as ai and Bij.

2.2 General Rules

• An index that occurs once in a tensor term is a “free index”.
• An index that occurs twice in a tensor term is a “dummy” or “bound” index.
• No index is allowed to occur more than twice in a legitimate tensor term.2
• A free index should be understood to vary over its range (e.g. 1, . . . , n) and hence it

can be interpreted as saying “for all components represented by the index”. Therefore a

free index represents a number of terms or expressions or equalities equal to the number

of allowed values of its range. For example, when i and j can vary over the range 1, . . . , n

the following expression

represents n separate expressions while the following equation

Ai + Bi

Aj

i = Bj

i

(8)

(9)

represents n × n separate equations.
• According to the “summation convention”, which is widely used in the literature of

tensor calculus including in the present notes, dummy indices imply summation over their

2We adopt this assertion, which is common in the literature of tensor calculus, as we think it is suitable
for this level. However, there are many instances in the literature of tensor calculus where indices are
repeated more than twice in a single term. The bottom line is that as long as the tensor expression makes
sense and the intention is clear, such repetitions should be allowed with no need in our view to take special
precaution like using parentheses. In particular, the summation convention will not apply automatically
in such cases although summation on such indices can be carried out explicitly, by using the summation

symbol(cid:80), or by special declaration of such intention similar to the summation convention. Anyway, in

the present text we will not use indices repeated more than twice in a single term.

2.2 General Rules

range, e.g. for nD

AiBi ≡ n(cid:88)

AiBi = A1B1 + A2B2 + . . . + AnBn

i=1

n(cid:88)
δijAij ≡ n(cid:88)
n(cid:88)
n(cid:88)
ijkAijBk ≡ n(cid:88)

j=1

δijAij

i=1

ijkAijBk

13

(10)

(11)

(12)

• When dummy indices do not imply summation, the situation must be clariﬁed by en-

i=1

j=1

k=1

closing such indices in parentheses or by underscoring or by using upper case letters (with

declaration of these conventions) or by adding a clarifying comment like “no summation

on repeated indices”.
• Tensors with subscript indices, like Aij, are called covariant, while tensors with super-
script indices, like Ak, are called contravariant. Tensors with both types of indices, like
lk , are called mixed type. More details about this will follow in § 2.5.1.
Almn
• Subscript indices, rather than subscripted tensors, are also dubbed “covariant” and

superscript indices are dubbed “contravariant”.
• Each tensor index should conform to one of the variance transformation rules as given

by Eqs. 20 and 21, i.e. it is either covariant or contravariant.
• For orthonormal Cartesian coordinate systems, the two variance types (i.e. covariant

and contravariant) do not diﬀer because the metric tensor is given by the Kronecker delta
(refer to § 5 and 3.1) and hence any index can be upper or lower although it is common

to use lower indices in such cases.
• For tensor invariance, a pair of dummy indices should in general be complementary

in their variance type, i.e. one covariant and the other contravariant. However, for or-

thonormal Cartesian systems the two are the same and hence when both dummy indices

2.2 General Rules

14

are covariant or both are contravariant it should be understood as an indication that the

underlying coordinate system is orthonormal Cartesian if the possibility of an error is

excluded.
• As indicated earlier, tensor order is equal to the number of its indices while tensor rank is

equal to the number of its free indices; hence vectors (terms, expressions and equalities) are

represented by a single free index and rank-2 tensors are represented by two free indices.

The dimension of a tensor is determined by the range taken by its indices.
• The rank of all terms in legitimate tensor expressions and equalities must be the same.
• Each term in valid tensor expressions and equalities must have the same set of free

indices (e.g. i, j, k).
• A free index should keep its variance type in every term in valid tensor expressions and

equations, i.e. it must be covariant in all terms or contravariant in all terms.
• While free indices should be named uniformly in all terms of tensor expressions and

equalities, dummy indices can be named in each term independently, e.g.

Ai

ik + Bj

jk + C lm

lmk

(13)

• A free index in an expression or equality can be renamed uniformly using a diﬀerent

symbol, as long as this symbol is not already in use, assuming that both symbols vary

over the same range, i.e. have the same dimension.
• Examples of legitimate tensor terms, expressions and equalities:

Aij
ij,

Aim

m + Bink
nk ,

Cij = Aij − Bij,

a = Bj
j

• Examples of illegitimate tensor terms, expressions and equalities:

Bii
i ,

Ai + Bij,

Ai + Bj,

Ai − Bi,

Ai

i = Bi,

(14)

(15)

2.3 Examples of Tensors of Diﬀerent Ranks

15

• Indexing is generally distributive over the terms of tensor expressions and equalities, e.g.

and

[A + B]i = [A]i + [B]i

[A = B]i ⇐⇒ [A]i = [B]i

(16)

(17)

• Unlike scalars and tensor components, which are essentially scalars in a generic sense,

operators cannot in general be freely reordered in tensor terms, therefore

but

f h = hf

&

AiBi = BiAi

∂iAi (cid:54)= Ai∂i

(18)

(19)

• Almost all the identities in the present notes which are given in a covariant or a con-

travariant or a mixed form are similarly valid for the other forms unless it is stated other-

wise. The objective of reporting in only one form is conciseness and to avoid unnecessary

repetition.

2.3 Examples of Tensors of Diﬀerent Ranks

• Examples of rank-0 tensors (scalars) are energy, mass, temperature, volume and density.

These are totally identiﬁed by a single number regardless of any coordinate system and

hence they are invariant under coordinate transformations.
• Examples of rank-1 tensors (vectors) are displacement, force, electric ﬁeld, velocity and

acceleration. These need for their complete identiﬁcation a number, representing their

magnitude, and a direction representing their geometric orientation within their space.

2.4 Applications of Tensors

16

Alternatively, they can be uniquely identiﬁed by a set of numbers, equal to the number

of dimensions of the underlying space, in reference to a particular coordinate system and

hence this identiﬁcation is system-dependent although they still have system-invariant

properties such as length.
• Examples of rank-2 tensors are Kronecker delta (see § 3.1), stress, strain, rate of strain

and inertia tensors. These require for their full identiﬁcation a set of numbers each of

which is associated with two directions.
• Examples of rank-3 tensors are the Levi-Civita tensor (see § 3.2) and the tensor of

piezoelectric moduli.
• Examples of rank-4 tensors are the elasticity or stiﬀness tensor, the compliance tensor

and the fourth-order moment of inertia tensor.
• Tensors of high ranks are rare in science and engineering.
• Although rank-0 and rank-1 tensors are, respectively, scalars and vectors, not all scalars

and vectors (in their generic sense) are tensors of these ranks. Similarly, rank-2 tensors

are normally represented by matrices but not all matrices represent tensors.

2.4 Applications of Tensors

• Tensor calculus is very powerful mathematical tool. Tensor notation and techniques

are used in many branches of science and engineering such as ﬂuid mechanics, contin-

uum mechanics, general relativity and structural engineering. Tensor calculus is used for

elegant and compact formulation and presentation of equations and identities in mathe-

matics, science and engineering. It is also used for algebraic manipulation of mathematical
expressions and proving identities in a neat and succinct way (refer to § 4.6).
• As indicated earlier, the invariance of tensor forms serves a theoretically and practically

important role by allowing the formulation of physical laws in coordinate-free forms.

2.5 Types of Tensors

17

2.5 Types of Tensors

• In the following subsections we introduce a number of tensor types and categories and

highlight their main characteristics and diﬀerences. These types and categories are not

mutually exclusive and hence they overlap in general; moreover they may not be exhaustive

in their classes as some tensors may not instantiate any one of a complementary set of

types such as being symmetric or anti-symmetric.

2.5.1 Covariant and Contravariant Tensors

• These are the main types of tensor with regard to the rules of their transformation

between diﬀerent coordinate systems.
• Covariant tensors are notated with subscript indices (e.g. Ai) while contravariant tensors
are notated with superscript indices (e.g. Aij).
• A covariant tensor is transformed according to the following rule

¯Ai =

∂xj
∂ ¯xi Aj

while a contravariant tensor is transformed according to the following rule

¯Ai =

∂ ¯xi
∂xj Aj

(20)

(21)

where the barred and unbarred symbols represent the same mathematical object (tensor

or coordinate) in the transformed and original coordinate systems respectively.
• An example of covariant tensors is the gradient of a scalar ﬁeld.
• An example of contravariant tensors is the displacement vector.
• Some tensors have mixed variance type, i.e.

they are covariant in some indices and

contravariant in others. In this case the covariant variables are indexed with subscripts

while the contravariant variables are indexed with superscripts, e.g. Aj

i which is covariant

2.5.1 Covariant and Contravariant Tensors

18

in i and contravariant in j.
• A mixed type tensor transforms covariantly in its covariant indices and contravariantly

in its contravariant indices, e.g.

¯Al n

m =

∂ ¯xl
∂xi

∂xj
∂ ¯xm

∂ ¯xn
∂xk Ai k

j

(22)

• To clarify the pattern of mathematical transformation of tensors, we explain step-by-

step the practical rules to follow in writing tensor transformation equations between two

coordinate systems, unbarred and barred, where for clarity we color the symbols of the

tensor and the coordinates belonging to the unbarred system with blue while we use

red to mark the symbols belonging to the barred system. Since there are three types

of tensors: covariant, contravariant and mixed, we use three equations in each step. In

this demonstration we use rank-4 tensors as examples since this is suﬃciently general

and hence adequate to elucidate the rules for tensors of any rank. The demonstration

is based on the assumption that the transformation is taking place from the unbarred

system to the barred system; the same rules should apply for the opposite transformation
from the barred system to the unbarred system. We use the sign ‘(cid:36)’ for the equality in

the transitional steps to indicate that the equalities are under construction and are not

complete.

We start by the very generic equations between the barred tensor ¯A and the unbarred

tensor A for the three types:

¯A (cid:36) A
¯A (cid:36) A
¯A (cid:36) A

(covariant)

(contravariant)

(mixed)

2.5.1 Covariant and Contravariant Tensors

19

We assume that the barred tensor and its coordinates are indexed with ijkl and the

unbarred are indexed with npqr, so we add these indices in their presumed order and

position (lower or upper) paying particular attention to the order in the mixed type:

¯Aijkl (cid:36) Anpqr
¯Aijkl (cid:36) Anpqr
(cid:36) Anp

¯Aij
kl

qr

Since the barred and unbarred tensors are of the same type, as they represent the same

tensor in two coordinate systems, the indices on the two sides of the equalities should match

in their position and order. We then insert a number of partial diﬀerential operators on

the right hand side of the equations equal to the rank of these tensors, which is 4 in our

example. These operators represent the transformation rules for each pair of corresponding

coordinates one from the barred and one from the unbarred:

∂

¯Aijkl (cid:36) ∂
¯Aijkl (cid:36) ∂
(cid:36) ∂

¯Aij
kl

∂

∂

∂
∂

∂
∂

∂
∂

∂
∂

∂
∂

∂
∂

∂
∂ Anpqr

∂

∂ Anpqr

∂

∂ Anp

qr

Now we insert the coordinates of the barred system into the partial diﬀerential operators

noting that (i) the positions of any index on the two sides should match, i.e. both upper

or both lower, since they are free indices in diﬀerent terms of tensor equalities, (ii) a

superscript index in the denominator of a partial derivative is in lieu of a covariant index

2.5.1 Covariant and Contravariant Tensors

20

in the numerator3, and (iii) the order of the coordinates should match the order of the

indices in the tensor:

∂xi

¯Aijkl (cid:36) ∂
¯Aijkl (cid:36) ∂xi
(cid:36) ∂xi

¯Aij
kl

∂

∂

∂
∂xj

∂

∂xk

∂
∂xl Anpqr

∂xj
∂

∂xk
∂

∂xl
∂ Anpqr

∂xj
∂

∂

∂xk

∂

∂xl Anp

qr

For consistency, these coordinates should be barred as they belong to the barred tensor;

hence we add bars:

∂ ¯xi

¯Aijkl (cid:36) ∂
¯Aijkl (cid:36) ∂ ¯xi
(cid:36) ∂ ¯xi

¯Aij
kl

∂

∂

∂
∂ ¯xj

∂

∂ ¯xk

∂
∂ ¯xl Anpqr

∂ ¯xj
∂

∂ ¯xk
∂

∂ ¯xl
∂ Anpqr

∂ ¯xj
∂

∂

∂ ¯xk

∂

∂ ¯xl Anp

qr

Finally, we insert the coordinates of the unbarred system into the partial diﬀerential

operators noting that (i) the positions of the repeated indices on the same side should

be opposite, i.e. one upper and one lower, since they are dummy indices and hence the

position of the index of the unbarred coordinate should be opposite to its position in the

unbarred tensor, (ii) an upper index in the denominator is in lieu of a lower index in the

numerator, and (iii) the order of the coordinates should match the order of the indices in

the tensor:

3The use of upper indices in the denominator of partial derivatives, which is common in this type of

equations, is to indicate the fact that the coordinates and their diﬀerentials transform contravariantly.

2.5.1 Covariant and Contravariant Tensors

21

¯Aijkl = ∂xn
∂ ¯xi

∂xp
∂ ¯xj

∂xq
∂ ¯xk

∂xr
∂ ¯xl Anpqr

¯Aijkl = ∂ ¯xi
∂xn

∂ ¯xj
∂xp

∂ ¯xk
∂xq

∂ ¯xl
∂xr Anpqr

¯Aij

kl = ∂ ¯xi
∂xn

∂ ¯xj
∂xp

∂xq
∂ ¯xk

∂xr
∂ ¯xl Anp

qr

We also replaced the ‘(cid:36)’ sign in the ﬁnal set of equations with the strict equality sign ‘=’

as the equations now are complete.
• A tensor of m contravariant indices and n covariant indices may be called type (m, n)

tensor, e.g. Ak

ij is a type (1, 2) tensor. When one or both variance types are absent, zero

is used to refer to the absent type in this notation, e.g. Bik is a type (2, 0) tensor.
• The covariant and contravariant types of a tensor are linked through the metric tensor
(refer to § 5).
• For orthonormal Cartesian systems there is no diﬀerence between covariant and con-

travariant tensors, and hence the indices can be upper or lower.
• The vectors providing the basis set (not necessarily of unit length or orthogonal) for

a coordinate system are of covariant type when they are tangent to the coordinate axes,

and they are of contravariant type when they are perpendicular to the local coordinate

surfaces. These two sets are identical for orthonormal Cartesian systems.
• Formally, the covariant and contravariant basis vectors are given respectively by:

Ei =

∂r
∂xi

&

Ei = ∇xi

(23)

where r is the position vector and xi is a generalized curvilinear coordinate. As indicated

already, a superscript in the denominator of partial derivatives is equivalent to a subscript

in the numerator.
• In general, the covariant and contravariant basis vectors are not orthogonal or of unit

2.5.2 True and Pseudo Tensors

22

length; however the two sets are reciprocal systems and hence they satisfy the following

reciprocity relation:

Ei · Ej = δj

i

(24)

i is the Kronecker delta (refer to § 3.1).

where δj
• A vector can be represented either by covariant components with contravariant coordi-

nate basis vectors or by contravariant components with covariant coordinate basis vectors.

For example, a vector A can be expressed as

A = AiEi

or

A = AiEi

(25)

where Ei and Ei are the contravariant and covariant basis vectors respectively. The use of

the covariant or contravariant form of the vector representation is a matter of choice and

convenience.
• More generally, a tensor of any rank (≥ 1) can be represented covariantly using con-

travariant basis tensors of that rank, or contravariantly using covariant basis tensors, or

in a mixed form using a mixed basis of opposite sense. For example, a rank-2 tensor A

can be written as:

A = AijEiEj = AijEiEj = A j

i EiEj

(26)

where EiEj, EiEj and EiEj are dyadic products (refer to § 2.6.3).

2.5.2 True and Pseudo Tensors

• These are also called polar and axial tensors respectively although it is more common

to use the latter terms for vectors. Pseudo tensors may also be called tensor densities.
• True tensors are proper (or ordinary) tensors and hence they are invariant under co-

2.5.2 True and Pseudo Tensors

23

ordinate transformations, while pseudo tensors are not proper tensors since they do not

transform invariantly as they acquire a minus sign under improper orthogonal transfor-

mations which involve inversion of coordinate axes through the origin with a change of

system handedness.
• Because true and pseudo tensors have diﬀerent mathematical properties and represent

diﬀerent types of physical entities, the terms of consistent tensor expressions and equations

should be uniform in their true and pseudo type, i.e. all terms are true or all are pseudo.
• The direct product (refer to § 2.6.3) of even number of pseudo tensors is a proper tensor,

while the direct product of odd number of pseudo tensors is a pseudo tensor.
• The direct product of a mix of true and pseudo tensors is a true or pseudo tensor

depending on the number of pseudo tensors involved in the product as being even or odd

respectively.
• Similar rules to those of direct product apply to cross products (including curl operations)

involving tensors (usually of rank-1) with the addition of a pseudo factor for each cross
product operation. This factor is contributed by the permutation tensor (see § 3.2) which

is implicit in the deﬁnition of the cross product (see Eqs. 109 and 134).
• In summary, what determines the tensor type (true or pseudo) of the tensor terms in-

volving direct4 and cross products is the parity of the multiplicative factors of pseudo type

plus the number of cross product operations involved since each cross product contributes

an  factor.
• Examples of true scalars are temperature, mass and the dot product of two polar or two

axial vectors, while examples of pseudo scalars are the dot product of an axial vector and

a polar vector and the scalar triple product of polar vectors.
• Examples of polar vectors are displacement and acceleration, while examples of axial

vectors are angular velocity and cross product of polar vectors in general (including curl
4Inner product (see § 2.6.5) is the result of a direct product operation followed by contraction, and

hence it is a direct product in this sense.

2.5.3 Absolute and Relative Tensors

24

operation on polar vectors) due to the involvement of the permutation symbol which is
a pseudo tensor (refer to § 3.2). The essence of this distinction is that the direction of a

pseudo vector depends on the observer choice of the handedness of the coordinate system

whereas the direction of a proper vector is independent of such choice.
• Examples of proper tensors of rank-2 are stress and rate of strain tensors, while examples

of pseudo tensors of rank-2 are direct products of two vectors: one polar and one axial.
• Examples of proper tensors of higher ranks are piezoelectric moduli tensor (rank-3)

and elasticity tensor (rank-4), while examples of pseudo tensors of higher ranks are the

permutation tensor of these ranks.

2.5.3 Absolute and Relative Tensors

• Considering an arbitrary transformation from a general coordinate system to another, a

relative tensor of weight w is deﬁned by the following tensor transformation:

(cid:12)(cid:12)(cid:12)(cid:12) ∂x

(cid:12)(cid:12)(cid:12)(cid:12)w ∂ ¯xi

∂xa

∂ ¯xj

∂xb ··· ∂ ¯xk

∂xc

∂xd
∂ ¯xl

∂xe

∂ ¯xm ··· ∂xf

where(cid:12)(cid:12) ∂x

∂ ¯x

¯Aij...k

lm...n =

(cid:12)(cid:12) is the Jacobian of the transformation between the two systems. When w = 0

∂ ¯xn Aab...c

(27)

∂ ¯x

de...f

the tensor is described as an absolute or true tensor, while when w = −1 the tensor is

described as a pseudo tensor. When w = 1 the tensor may be described as a tensor

density.5
• As indicated earlier, a tensor of m contravariant indices and n covariant indices may be

called type (m, n). This may be generalized to include the weight as a third entry and

hence the type of the tensor is identiﬁed by (m, n, w).

5Some of these labels are used diﬀerently by diﬀerent authors.

2.5.4 Isotropic and Anisotropic Tensors

25

2.5.4

Isotropic and Anisotropic Tensors

• Isotropic tensors are characterized by the property that the values of their components

are invariant under coordinate transformation by proper rotation of axes. In contrast, the

values of the components of anisotropic tensors are dependent on the orientation of the

coordinate axes. Notable examples of isotropic tensors are scalars (rank-0), the vector 0

(rank-1), Kronecker delta δij (rank-2) and Levi-Civita tensor ijk (rank-3). Most tensors

describing physical properties of materials, such as stress and magnetic susceptibility, are

anisotropic.
• Direct and inner products of isotropic tensors are isotropic tensors.
• The zero tensor of any rank is isotropic; therefore if the components of a tensor vanish

in a particular coordinate system they will vanish in all properly and improperly rotated

coordinate systems. Consequently, if the components of two tensors are identical in a

particular coordinate system they are identical in all transformed coordinate systems.

2.5.5 Symmetric and Anti-symmetric Tensors

• These types of tensor apply to high ranks only (rank ≥ 2). Moreover, these types are
not exhaustive, even for tensors of ranks ≥ 2, as there are high-rank tensors which are

neither symmetric nor anti-symmetric.
• A rank-2 tensor Aij is symmetric iﬀ for all i and j

Aji = Aij

and anti-symmetric or skew-symmetric iﬀ

Aji = −Aij

(28)

(29)

Similar conditions apply to contravariant type tensors (refer also to the following).

2.5.5 Symmetric and Anti-symmetric Tensors

• A rank-n tensor Ai1...in is symmetric in its two indices ij and il iﬀ

Ai1...il...ij ...in = Ai1...ij ...il...in

and anti-symmetric or skew-symmetric in its two indices ij and il iﬀ

Ai1...il...ij ...in = −Ai1...ij ...il...in

26

(30)

(31)

• Any rank-2 tensor Aij can be synthesized from (or decomposed into) a symmetric part

A(ij) (marked with round brackets enclosing the indices) and an anti-symmetric part A[ij]

(marked with square brackets) where

Aij = A(ij) + A[ij],

A(ij) =

1
2

(Aij + Aji)

& A[ij] =

(Aij − Aji)

1
2

(32)

• A rank-3 tensor Aijk can be symmetrized by

A(ijk) =

and anti-symmetrized by

A[ijk] =

1
3!

1
3!

(Aijk + Akij + Ajki + Aikj + Ajik + Akji)

(33)

(Aijk + Akij + Ajki − Aikj − Ajik − Akji)

(34)

• A rank-n tensor Ai1...in can be symmetrized by

A(i1...in) =

1
n!

(sum of all even & odd permutations of indices i’s)

(35)

2.6 Tensor Operations

and anti-symmetrized by

27

A[i1...in] =

1
n!

(sum of all even permutations minus sum of all odd permutations)

(36)

• For a symmetric tensor Aij and an anti-symmetric tensor Bij (or the other way around)

we have

AijBij = 0

(37)

• The indices whose exchange deﬁnes the symmetry and anti-symmetry relations should

be of the same variance type, i.e. both upper or both lower.
• The symmetry and anti-symmetry characteristic of a tensor is invariant under coordinate

transformation.
• A tensor of high rank (> 2) may be symmetrized or anti-symmetrized with respect to

only some of its indices instead of all of its indices, e.g.

A(ij)k =

1
2

(Aijk + Ajik)

&

A[ij]k =

(Aijk − Ajik)

1
2

(38)

2.6 Tensor Operations

• There are many operations that can be performed on tensors to produce other tensors

in general. Some examples of these operations are addition/subtraction, multiplication

by a scalar (rank-0 tensor), multiplication of tensors (each of rank > 0), contraction and

permutation. Some of these operations, such as addition and multiplication, involve more

than one tensor while others are performed on a single tensor, such as contraction and

permutation.
• In tensor algebra, division is allowed only for scalars, hence if the components of an

2.6.1 Addition and Subtraction

28

indexed tensor should appear in a denominator, the tensor should be redeﬁned to avoid

this, e.g. Bi = 1
Ai

.

2.6.1 Addition and Subtraction

• Tensors of the same rank and type (covariant/contravariant/mixed and true/pseudo)

can be added algebraically to produce a tensor of the same rank and type, e.g.

a = b + c

Ai = Bi − Ci

Ai

j = Bi

j + C i
j

(39)

(40)

(41)

• The added/subtracted terms should have the same indicial structure with regard to
their free indices, as explained in § 2.2, hence Ai

ik cannot be added or subtracted

jk and Bj

although they are of the same rank and type, but Ami

mjk and Bi

jk can be added and sub-

tracted.
• Addition of tensors is associative and commutative.

2.6.2 Multiplication by Scalar

• A tensor can be multiplied by a scalar, which generally should not be zero, to produce

a tensor of the same variance type and rank, e.g.

• As indicated above, multiplying a tensor by a scalar means multiplying each component

Aj

ik = aBj

ik

(42)

2.6.3 Tensor Multiplication

29

of the tensor by that scalar.
• Multiplication by a scalar is commutative, and associative when more than two factors

are involved.

2.6.3 Tensor Multiplication

• This may also be called outer or exterior or direct or dyadic multiplication, although

some of these names may be reserved for operations on vectors.
• On multiplying each component of a tensor of rank r by each component of a tensor of

rank k, both of dimension m, a tensor of rank (r + k) with mr+k components is obtained

where the variance type of each index (covariant or contravariant) is preserved, e.g.

AiBj = Cij

AijBkl = C ij
kl

(43)

(44)

• The outer product of a tensor of type (m, n) by a tensor of type (p, q) results in a tensor

of type (m + p, n + q).

• Direct multiplication of tensors may be marked by the symbol (cid:15), mostly when using
symbolic notation for tensors, e.g. A(cid:15) B. However, in the present text no symbol will be

used for the operation of direct multiplication.
• Direct multiplication of tensors is associative but not commutative.
• Multiplication of a tensor by a scalar (refer to § 2.6.2) may be regarded as a special case

of direct multiplication.
• The rank-2 tensor constructed as a result of the direct multiplication of two vectors is

commonly called dyad.
• Tensors may be expressed as an outer product of vectors where the rank of the resultant

2.6.4 Contraction

30

product is equal to the number of the vectors involved (e.g. 2 for dyads and 3 for triads).
• Not every tensor can be synthesized as a product of lower rank tensors.
• In the outer product, it is understood that all the indices of the involved tensors have

the same range.

2.6.4 Contraction

• Contraction of a tensor of rank > 1 is to make two free indices identical, by unifying

their symbols, and perform summation over these repeated indices, e.g.

Aj
i

−−−−−−−−→ Ai
contraction

i

Ajk
il

−−−−−−−−−−−−→ Amk
contraction on jl

im

(45)

(46)

• Contraction results in a reduction of the rank by 2 since it implies the annihilation of

two free indices. Therefore, the contraction of a rank-2 tensor is a scalar, the contraction

of a rank-3 tensor is a vector, the contraction of a rank-4 tensor is a rank-2 tensor, and so

on.
• For general non-Cartesian coordinate systems, the pair of contracted indices should be

diﬀerent in their variance type, i.e. one upper and one lower. Hence, contraction of a
mixed tensor of type (m, n) will, in general, produce a tensor of type (m − 1, n − 1).
• A common example of contraction is the dot product operation on vectors which can be
regarded as a direct multiplication (refer to § 2.6.3) of the two vectors, which results in a

rank-2 tensor, followed by a contraction.
• In matrix algebra, taking the trace (summing the diagonal elements) can also be consid-

ered as contraction of the matrix, which under certain conditions can represent a rank-2

tensor, and hence it yields the trace which is a scalar.

2.6.5 Inner Product

2.6.5

Inner Product

31

• On taking the outer product (refer to § 2.6.3) of two tensors of rank ≥ 1 followed by a

contraction on two indices of the product, an inner product of the two tensors is formed.

Hence if one of the original tensors is of rank-m and the other is of rank-n, the inner
product will be of rank-(m + n − 2).
• The inner product operation is usually symbolized by a single dot between the two
tensors, e.g. A · B, to indicate contraction following outer multiplication.
• In general, the inner product is not commutative. When one or both of the tensors

involved in the inner product are of rank > 1 the order of the multiplicands does matter.
• As indicated before (see § 2.6.4), the dot product of two vectors is an example of the

inner product of tensors, i.e.

it is an inner product of two rank-1 tensors to produce a

rank-0 tensor:

[ab] j

i = aibj

−−−−−−−−→ a · b = aibi

contraction

(47)

• Another common example (from linear algebra) of inner product is the multiplication of

a matrix (representing a rank-2 tensor assuming certain conditions) by a vector (rank-1

tensor) to produce a vector, e.g.

[Ab] k

ij = Aijbk

−−−−−−−−−−−−−→ [A · b]i = Aijbj

contraction on jk

(48)

• For tensors whose outer product produces a tensor of rank > 2, various contraction

operations between diﬀerent sets of indices can occur and hence more than one inner

product, which are diﬀerent in general, can be deﬁned. Moreover, when the outer product

produces a tensor of rank > 3 more than one contraction can take place simultaneously.
• There are more specialized types of inner product; some of which may be deﬁned dif-

ferently by diﬀerent authors. For example, a double inner product of two rank-2 tensors,

2.6.5 Inner Product

32

A and B, may be deﬁned and denoted by double vertically- or horizontally-aligned dots
(e.g. A : B or A · ·B) to indicate double contraction taking place between diﬀerent pairs

of indices. An instance of these types is the inner product with double contraction of two

dyads which is commonly deﬁned by6

ab : cd = (a · c) (b · d)

(49)

and the result is a scalar. The single dots in the right hand side of the last equation

symbolize the conventional dot product of two vectors. Some authors may deﬁne a diﬀerent

type of double-contraction inner product of two dyads, symbolized by two horizontally-

aligned dots, which may be called a “transposed contraction”, and is given by

ab · ·cd = ab : dc = (a · d) (b · c)

(50)

where the result is also a scalar. However, diﬀerent authors may have diﬀerent conventions

and hence one should be vigilant about such diﬀerences.
• For two rank-2 tensors, the aforementioned double-contraction inner products are simi-

larly deﬁned as in the case of two dyads:

A : B = AijBij

& A · ·B = AijBji

(51)

• Inner products with higher multiplicity of contractions are similarly deﬁned, and hence

can be regarded as trivial extensions of the inner products with lower contraction multi-

plicities.

6It is also deﬁned diﬀerently by some authors.

2.6.6 Permutation

2.6.6 Permutation

33

• A tensor may be obtained by exchanging the indices of another tensor, e.g. transposition

of rank-2 tensors.
• Tensor permutation applies only to tensors of rank ≥ 2.
• The collection of tensors obtained by permuting the indices of a basic tensor may be

called isomers.

2.7 Tensor Test: Quotient Rule

• Sometimes a tensor-like object may be suspected for being a tensor; in such cases a test

based on the “quotient rule” can be used to clarify the situation. According to this rule, if

the inner product of a suspected tensor with a known tensor is a tensor then the suspect

is a tensor. In more formal terms, if it is not known if A is a tensor but it is known that

B and C are tensors; moreover it is known that the following relation holds true in all

rotated (properly-transformed) coordinate frames:

Apq...k...mBij...k...n = Cpq...mij...n

(52)

then A is a tensor. Here, A, B and C are respectively of ranks m, n and (m + n− 2), due

to the contraction on k which can be any index of A and B independently.
• Testing for being a tensor can also be done by applying ﬁrst principles through direct

substitution in the transformation equations. However, using the quotient rule is generally

more convenient and requires less work.
• The quotient rule may be considered as a replacement for the division operation which

is not deﬁned for tensors.

3 δ AND  TENSORS

34

3

δ and  Tensors

• These tensors are of particular importance in tensor calculus due to their distinctive

properties and unique transformation attributes. They are numerical tensors with ﬁxed

components in all coordinate systems. The ﬁrst is called Kronecker delta or unit ten-

sor, while the second is called Levi-Civita7, permutation, anti-symmetric and alternating

tensor.
• The δ and  tensors are conserved under coordinate transformation and hence they are

the same for all systems of coordinate.

3.1 Kronecker δ

• This is a rank-2 symmetric tensor in all dimensions, i.e.

δij = δji

(i, j = 1, 2, . . . , n)

(53)

Similar identities apply to the contravariant and mixed types of this tensor.
• It is invariant in all coordinate systems, and hence it is an isotropic tensor.
• It is deﬁned as:

1

0

δij =

(i = j)
(i (cid:54)= j)

(54)

7This name is usually used for the rank-3 tensor. Also some authors distinguish between the permuta-
tion tensor and Levi-Civita tensor even for rank-3. Moreover, some of the common labels and descriptions
of  are more speciﬁc to rank-3.

3.2 Permutation 

and hence it can be considered as the identity matrix, e.g. for 3D



[δij] =

δ11

δ12

δ13

δ21

δ22

δ23

δ31

δ32

δ33

 =





1 0 0

0 1 0

0 0 1

• Covariant, contravariant and mixed type of this tensor are the same, that is

j = δj
δi

i = δij = δij

35

(55)

(56)

3.2 Permutation 

• This tensor has a rank equal to the number of dimensions. Hence, a rank-n permutation

tensor has nn components.
• It is totally anti-symmetric in each pair of its indices, i.e. it changes sign on swapping

any two of its indices, that is

i1...ik...il...in = −i1...il...ik...in

(57)

The reason is that any exchange of two indices requires an even/odd number of single-

step shifts to the right of the ﬁrst index plus an odd/even number of single-step shifts to

the left of the second index, so the total number of shifts is odd and hence it is an odd

permutation of the original arrangement.
• It is a pseudo tensor since it acquires a minus sign under improper orthogonal transfor-

mation of coordinates (inversion of axes with possible superposition of rotation).
• Deﬁnition of rank-2  (ij):

12 = 1,

21 = −1

&

11 = 22 = 0

(58)

3.3 Useful Identities Involving δ or/and 

36

• Deﬁnition of rank-3  (ijk):





ijk =

1
−1

0

(i, j, k is even permutation of 1,2,3)

(i, j, k is odd permutation of 1,2,3)

(59)

(repeated index)

• The deﬁnition of rank-n  (i1i2...in) is similar to the deﬁnition of rank-3  considering
index repetition and even or odd permutations of its indices (i1, i2,··· , in) corresponding
to (1, 2,··· , n), that is

i1i2...in =

1
−1

0

[(i1, i2, . . . , in) is even permutation of (1, 2, . . . , n)]

[(i1, i2, . . . , in) is odd permutation of (1, 2, . . . , n)]

(60)

[repeated index]

•  may be considered a contravariant relative tensor of weight +1 or a covariant relative
tensor of weight −1. Hence, in 2D, 3D and nD spaces respectively we have:

ij = ij

ijk = ijk

i1i2...in = i1i2...in

(61)

(62)

(63)

3.3 Useful Identities Involving δ or/and 

3.3.1

Identities Involving δ

• When an index of the Kronecker delta is involved in a summation operation by repeating

an index in another tensor in its own term, the eﬀect of this is to replace the shared index

3.3.1 Identities Involving δ

in the other tensor by the other index of the Kronecker delta, that is

δijAj = Ai

37

(64)

In such cases the Kronecker delta is described as the substitution or index replacement

operator. Hence,

Similarly,

δijδjk = δik

δijδjkδki = δikδki = δii = n

where n is the space dimension.
• Because the coordinates are independent of each other:

∂xi
∂xj

= ∂jxi = xi,j = δij

Hence, in nD we have

• For orthonormal Cartesian systems:

∂ixi = δii = n

∂xi
∂xj =

∂xj
∂xi = δij = δij

• For a set of orthonormal basis vectors in orthonormal Cartesian systems:

ei · ej = δij

(65)

(66)

(67)

(68)

(69)

(70)

3.3.2 Identities Involving 

38

• The double inner product of two dyads formed by orthonormal basis vectors of an

orthonormal Cartesian system is given by:

eiej : ekel = δikδjl

(71)

3.3.2

Identities Involving 

• For rank-3 :

ijk = kij = jki = −ikj = −jik = −kji

(sense of cyclic order)

(72)

These equations demonstrate the fact that rank-3  is totally anti-symmetric in all of its

indices since a shift of any two indices reverses the sign. This also reﬂects the fact that

the above tensor system has only one independent component.
• For rank-2 :

ij = (j − i)

ijk =

1
2

(j − i) (k − i) (k − j)

ijkl =

1
12

(j − i) (k − i) (l − i) (k − j) (l − j) (l − k)

(73)

(74)

(75)

• For rank-3 :

• For rank-4 :

• For rank-n :

(cid:34)

n−1(cid:89)

i=1

1
i!

n(cid:89)

j=i+1

(cid:35)

(aj − ai)

(cid:89)

1≤i<j≤n

=

1

S(n − 1)

(aj − ai)

(76)

a1a2···an =

a1a2···an =

where

σ (aj − ai) = σ

(aj − ai)

(cid:33)

(cid:89)

1≤i<j≤n



+1
−1

0

σ(k) =

(cid:32) (cid:89)

1≤i<j≤n

(k > 0)

(k < 0)

(k = 0)

39

(77)

(78)

(79)

(80)

3.3.2 Identities Involving 

where S(n − 1) is the super-factorial function of (n − 1) which is deﬁned as

k(cid:89)

S(k) =

i! = 1! · 2! · . . . · k!

A simpler formula for rank-n  can be obtained from the previous one by ignoring the

i=1

magnitude of the multiplication factors and taking only their signs, that is

• For rank-n :

i1i2···in i1i2···in = n!

because this is the sum of the squares of i1i2···in over all the permutations of n diﬀerent

indices which is equal to n! where the value of  of each one of these permutations is either
+1 or −1 and hence in both cases their square is 1.
• For a symmetric tensor Ajk:

ijkAjk = 0

(81)

because an exchange of the two indices of Ajk does not aﬀect its value due to the symmetry

whereas a similar exchange in these indices in ijk results in a sign change; hence each term

in the sum has its own negative and therefore the total sum will vanish.

3.3.3 Identities Involving δ and 

40

•

ijkAiAj = ijkAiAk = ijkAjAk = 0

(82)

because, due to the commutativity of multiplication, an exchange of the indices in A’s will

not aﬀect the value but a similar exchange in the corresponding indices of ijk will cause

a change in sign; hence each term in the sum has its own negative and therefore the total

sum will be zero.
• For a set of orthonormal basis vectors in 3D space with a right-handed orthonormal

Cartesian coordinate system:

ei × ej = ijkek

ei · (ej × ek) = ijk

3.3.3

Identities Involving δ and 

•

• For rank-2 :

ijkδ1iδ2jδ3k = 123 = 1

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) δik

δjk

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) = δikδjl − δilδjk

δil

δjl

ijkl =

ilkl = δik

(83)

(84)

(85)

(86)

(87)

3.3.3 Identities Involving δ and 

ijij = 2

41

(88)

• For rank-3 :

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

δil

δjl

δim δin

δjm δjn

δkl

δkm δkn

ijklmn =

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) = δilδjmδkn+δimδjnδkl+δinδjlδkm−δilδjnδkm−δimδjlδkn−δinδjmδkl
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) δil

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) = δilδjm − δimδjl

(90)

(89)

ijklmk =

δim

δjl

δjm

The last identity is very useful in manipulating and simplifying tensor expressions and

proving vector and tensor identities.

since the rank and dimension of  are the same, which is 3 in this case.
• For rank-n :

ijkljk = 2δil

ijkijk = 2δii = 6

i1i2···in j1j2···jn =

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

δi1j1

δi1j2

δi2j1
...

δi2j2
...

δinj1

δinj2

···
···
. . .
···

δi1jn

δi2jn
...

δinjn

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(91)

(92)

(93)

3.4 Generalized Kronecker delta

• According to Eqs. 59 and 64:

ijkδij = ijkδik = ijkδjk = 0

3.4 Generalized Kronecker delta

42

(94)

• The generalized Kronecker delta is deﬁned by:

δi1...in
j1...jn =

[(j1 . . . jn) is even permutation of (i1 . . . in)]

[(j1 . . . jn) is odd permutation of (i1 . . . in)]

(95)

0

[repeated j’s]

It can also be deﬁned by the following n × n determinant:



1
−1

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

δi1
j1
δi2
j1
...

δin
j1

δi1
j2
δi2
j2
...

δin
j2

···
···
. . .
···

δi1
jn

δi2
jn
...

δin
jn

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(96)

δi1...in
j1...jn =

where the δi

j entries in the determinant are the normal Kronecker delta as deﬁned by Eq.

54.
• Accordingly, the relation between the rank-n  and the generalized Kronecker delta in

nD is given by:

i1i2...in = δ1 2...n
i1i2...in

&

i1i2...in = δi1i2...in
1 2...n

(97)

Hence, the permutation tensor  may be considered as a special case of the generalized
Kronecker delta. Consequently the permutation symbol can be written as an n × n deter-

minant consisting of the normal Kronecker deltas.

3.4 Generalized Kronecker delta

• If we deﬁne

lm = δijk
δij

lmk

then Eq. 90 will take the following form:

δij
lm = δi

m − δi
l δj

mδj

l

43

(98)

(99)

Other identities involving δ and  can also be formulated in terms of the generalized

Kronecker delta.
• On comparing Eq. 93 with Eq. 96 we conclude

δi1...in
j1...jn = i1...in j1...jn

(100)

4 APPLICATIONS OF TENSOR NOTATION AND TECHNIQUES

44

4 Applications of Tensor Notation and Techniques

4.1 Common Deﬁnitions in Tensor Notation

• The trace of a matrix A representing a rank-2 tensor is:

tr (A) = Aii

(101)

• For a 3 × 3 matrix representing a rank-2 tensor in 3D, the determinant is:

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

det (A) =

A11 A12 A13

A21 A22 A23

A31 A32 A33

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) = ijkA1iA2jA3k = ijkAi1Aj2Ak3

(102)

where the last two equalities represent the expansion of the determinant by row and by

column. Alternatively

det (A) =

1
3!

ijklmnAilAjmAkn

(103)

• For an n × n matrix representing a rank-2 tensor in nD, the determinant is:

det (A) = i1···inA1i1 . . . Anin = i1···inAi11 . . . Ainn =

1
n!

i1···in j1···jnAi1j1 . . . Ainjn

(104)

• The inverse of a matrix A representing a rank-2 tensor is:

(cid:2)A−1(cid:3)

ij =

1

2 det (A)

jmn ipqAmpAnq

(105)

4.1 Common Deﬁnitions in Tensor Notation

• The multiplication of a matrix A by a vector b as deﬁned in linear algebra is:

[Ab]i = Aijbj

45

(106)

It should be noticed that we are here using matrix notation. The multiplication operation,

according to the symbolic notation of tensors, should be denoted by a dot between the
tensor and the vector, i.e. A·b.
• The multiplication of two matrices A and B as deﬁned in linear algebra is:

[AB]ik = AijBjk

(107)

Again, we are using here matrix notation; otherwise a dot should be inserted between the

two matrices.
• The dot product of two vectors is:

A · B =δijAiBj = AiBi

(108)

The readers are referred to § 2.6.5 for a more general deﬁnition of this type of product

that includes higher rank tensors.
• The cross product of two vectors is:

[A × B]i = ijkAjBk

• The scalar triple product of three vectors is:

A · (B × C) =

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

A1 A2 A3

B1 B2 B3

C1 C2 C3

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) = ijkAiBjCk

(109)

(110)

4.2 Scalar Invariants of Tensors

• The vector triple product of three vectors is:

[A × (B × C)]i = ijkklmAjBlCm

46

(111)

4.2 Scalar Invariants of Tensors

• In the following we list and write in tensor notation a number of invariants of low rank

tensors which have special importance due to their widespread applications in vector and

tensor calculus. All These invariants are scalars.
• The value of a scalar (rank-0 tensor), which consists of a magnitude and a sign, is

invariant under coordinate transformation.
• Invariant of a vector (rank-1 tensor) under coordinate transformation is its magnitude,

i.e. length (the direction is also invariant but it is not scalar!).
• The main three independent scalar invariants of a rank-2 tensor A under change of basis

are:

I = tr (A) = Aii

II = tr(cid:0)A2(cid:1) = AijAji
III = tr(cid:0)A3(cid:1) = AijAjkAki

(112)

(113)

(114)

• Diﬀerent forms of the three invariants of a rank-2 tensor A, which are also widely used,

are:

I1 = I = Aii

(115)

(AiiAjj − AijAji)

(cid:0)I 2 − II(cid:1) =
(cid:0)I 3 − 3I II + 2III(cid:1) =

1
2

47

(116)

4.3 Common Diﬀerential Operations in Tensor Notation

I2 =

I3 = det (A) =

1
2

1
3!

1
3!

ijkpqrAipAjqAkr

(117)

• The invariants I, II and III can similarly be deﬁned in terms of the invariants I1, I2

and I3 as follow:

I = I1

II = I 2

1 − 2I2

III = I 3

1 − 3I1I2 + 3I3

(118)

(119)

(120)

• Ten joint invariants between two rank-2 tensors, A and B, can be formed; these are:
tr (A), tr (B), tr (A2), tr (B2), tr (A3), tr (B3), tr (A · B), tr (A2 · B), tr (A · B2) and
tr (A2 · B2).

4.3 Common Diﬀerential Operations in Tensor Notation

• Here we present the most common diﬀerential operations as deﬁned by tensor notation.

These operations are mostly based on the various types of interaction between the vector
diﬀerential operator nabla ∇ with tensors of diﬀerent ranks as well as interaction with

other types of operation like dot and cross products.
• ∇ is essentially a spatial partial diﬀerential operator deﬁned in Cartesian coordinate

4.3.1 Cartesian System

systems by:

The deﬁnition of ∇ in some non-Cartesian systems will be given in § 4.3.2.

∇i =

∂
∂xi

4.3.1 Cartesian System

• The gradient of a diﬀerentiable scalar function of position f is a vector given by:

[∇f ]i = ∇if =

∂f
∂xi

= ∂if = f,i

48

(121)

(122)

• The gradient of a diﬀerentiable vector function of position A (which is the outer product,
as deﬁned in § 2.6.3, between the ∇ operator and the vector) is a rank-2 tensor deﬁned

by:

[∇A]ij = ∂iAj

• The gradient operation is distributive but not commutative or associative:

∇ (f + h) = ∇f + ∇h

∇f (cid:54)= f∇

(∇f ) h (cid:54)= ∇ (f h)

where f and h are diﬀerentiable scalar functions of position.

(123)

(124)

(125)

(126)

4.3.1 Cartesian System

• The divergence of a diﬀerentiable vector A is a scalar given by:

∇ · A = δij

∂Ai
∂xj

=

∂Ai
∂xi

= ∇iAi = ∂iAi = Ai,i

49

(127)

The divergence operation can also be viewed as taking the gradient of the vector followed

by a contraction. Hence, the divergence of a vector is invariant because it is the trace of

a rank-2 tensor.8
• The divergence of a diﬀerentiable rank-2 tensor A is a vector deﬁned in one of its forms

by:

and in another form by

[∇ · A]i = ∂jAji

[∇ · A]j = ∂iAji

These two diﬀerent forms can be given, respectively, in symbolic notation by:

∇ · A &

∇ · AT

(128)

(129)

(130)

where AT is the transpose of A. More generally, the divergence of a tensor of rank n ≥ 2,
which is a tensor of rank-(n − 1), can be deﬁned in several forms, which are diﬀerent in

general, depending on the combination of the contracted indices.
• The divergence operation is distributive but not commutative or associative:

∇ · (A + B) = ∇ · A + ∇ · B

(131)

8It may also be argued that the divergence of a vector is a scalar and hence it is invariant.

4.3.1 Cartesian System

∇ · A (cid:54)= A · ∇

∇ · (f A) (cid:54)= ∇f · A

50

(132)

(133)

where A and B are diﬀerentiable tensor functions of position.
• The curl of a diﬀerentiable vector A is a vector given by:

[∇ × A]i = ijk

∂Ak
∂xj

= ijk∇jAk = ijk∂jAk = ijkAk,j

(134)

• The curl operation may be generalized to tensors of rank > 1, and hence the curl of a

diﬀerentiable rank-2 tensor A can be deﬁned as a rank-2 tensor given by:

[∇ × A]ij = imn∂mAnj

• The curl operation is distributive but not commutative or associative:

∇ × (A + B) = ∇ × A + ∇ × B

∇ × A (cid:54)= A × ∇

∇ × (A × B) (cid:54)= (∇ × A) × B

• The Laplacian scalar operator acting on a diﬀerentiable scalar f is given by:

∆f = ∇2f = δij

∂2f

∂xi∂xj

=

∂2f

∂xi∂xi

= ∇iif = ∂iif = f,ii

(135)

(136)

(137)

(138)

(139)

4.3.2 Other Coordinate Systems

51

• The Laplacian operator acting on a diﬀerentiable vector A is deﬁned for each component

of the vector similar to the deﬁnition of the Laplacian acting on a scalar, that is

(cid:2)∇2A(cid:3)

i = ∂jjAi

(140)

• The following scalar diﬀerential operator is commonly used in science (e.g.

in ﬂuid

dynamics):

A · ∇ = Ai∇i = Ai

∂
∂xi

= Ai∂i

(141)

where A is a vector. As indicated earlier, the order of Ai and ∂i should be respected.
• The following vector diﬀerential operator also has common applications in science:

[A × ∇]i = ijkAj∂k

(142)

• The diﬀerentiation of a tensor increases its rank by one, by introducing an extra covariant

index, unless it implies a contraction in which case it reduces the rank by one. Therefore

the gradient of a scalar is a vector and the gradient of a vector is a rank-2 tensor (∂iAj),

while the divergence of a vector is a scalar and the divergence of a rank-2 tensor is a vector
(∂jAji or ∂iAji). This may be justiﬁed by the fact that ∇ is a vector operator. On the

other hand the Laplacian operator does not change the rank since it is a scalar operator;

hence the Laplacian of a scalar is a scalar and the Laplacian of a vector is a vector.

4.3.2 Other Coordinate Systems

• For completeness, we deﬁne here some diﬀerential operations in the most commonly

used non-Cartesian coordinate systems, namely cylindrical and spherical systems, as well

as general curvilinear coordinate systems.
• We can use indexed generalized coordinates like q1, q2 and q3 for the cylindrical coor-

4.3.2 Other Coordinate Systems

52

dinates (ρ, φ, z) and the spherical coordinates (r, θ, φ). However, for more clarity at this

level and to follow the more conventional practice, we use the coordinates of these systems

as suﬃxes in place of the indices used in the tensor notation.
• For the cylindrical system identiﬁed by the coordinates (ρ, φ, z) with an orthonormal

basis vectors eρ, eφ and ez:9
The ∇ operator is:

The Laplacian operator is:

∇ = eρ∂ρ + eφ

1
ρ

∂φ + ez∂z

∇2 = ∂ρρ +

1
ρ

∂ρ +

1
ρ2 ∂φφ + ∂zz

The gradient of a diﬀerentiable scalar f is:

∇f = eρ

∂f
∂ρ

+ eφ

1
ρ

∂f
∂φ

+ ez

∂f
∂z

The divergence of a diﬀerentiable vector A is:

(cid:20)∂ (ρAρ)

∂ρ

∇ · A =

1
ρ

+

∂Aφ
∂φ

+

∂ (ρAz)

∂z

The curl of a diﬀerentiable vector A is:

∇ × A =

1
ρ

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

eρ

∂
∂ρ

ρeφ

∂
∂φ

ez

∂
∂z

Aρ ρAφ Az

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(143)

(144)

(145)

(146)

(147)

(cid:21)

For plane polar coordinate systems, these operators and operations can be obtained by

9It should be obvious that since ρ, φ and z are speciﬁc coordinates and not variable indices, the

summation convention does not apply.

4.3.2 Other Coordinate Systems

53

dropping the z components or terms from the cylindrical form of the above operators and

operations.
• For the spherical system identiﬁed by the coordinates (r, θ, φ) with an orthonormal basis

vectors er, eθ and eφ:10
The ∇ operator is:

The Laplacian operator is:

∇ = er∂r + eθ

1
r

∂θ + eφ

1

r sin θ

∂φ

∇2 = ∂rr +

2
r

∂r +

1
r2 ∂θθ +

cos θ
r2 sin θ

∂θ +

1

r2 sin2 θ

∂φφ

The gradient of a diﬀerentiable scalar f is:

∇f = er

∂f
∂r

+ eθ

1
r

∂f
∂θ

+ eφ

1

r sin θ

∂f
∂φ

The divergence of a diﬀerentiable vector A is:

(cid:20)

∇ · A =

1

r2 sin θ

sin θ

∂ (r2Ar)

∂r

+ r

∂ (sin θAθ)

∂θ

+ r

∂Aφ
∂φ

The curl of a diﬀerentiable vector A is:

∇ × A =

1

r2 sin θ

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

er

∂
∂r

reθ

r sin θeφ

∂
∂θ

∂
∂φ

Ar

rAθ

r sin θAφ

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(148)

(149)

(150)

(151)

(152)

(cid:21)

(cid:12)(cid:12)(cid:12) ∂r

∂ui

(cid:12)(cid:12)(cid:12) and r is the

• For a general curvilinear system in 3D identiﬁed by the coordinates (u1, u2, u3) with unit

basis vectors u1, u2 and u3 and scale factors h1, h2 and h3 where hi =

position vector:

10Again, the summation convention does not apply to r, θ and φ.

4.4 Common Identities in Vector and Tensor Notation

54

The ∇ operator is:

∇ =

u1
h1

∂
∂u1

+

u2
h2

∂
∂u2

+

u3
h3

∂
∂u3

The Laplacian operator is:

(cid:20) ∂

(cid:18) h2h3

(cid:19)

∂
∂u1

+

∂
∂u2

(cid:19)

(cid:18) h1h3

h2

∂
∂u2

+

∂
∂u3

(cid:19)(cid:21)

(cid:18)h1h2

h3

∂
∂u3

h1h2h3

∂u1

h1

∇2 =

1

(153)

(154)

(155)

(cid:21)

(h1h2A3)

(156)

(157)

The gradient of a diﬀerentiable scalar f is:

∇f =

u1
h1

∂f
∂u1

+

u2
h2

∂f
∂u2

+

u3
h3

∂f
∂u3

The divergence of a diﬀerentiable vector A is:

(cid:20) ∂

∇ · A =

1

h1h2h3

∂u1

(h2h3A1) +

∂
∂u2

(h1h3A2) +

∂
∂u3

The curl of a diﬀerentiable vector A is:

∇ × A =

1

h1h2h3

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

h1u1 h2u2 h3u3

∂

∂u1

∂

∂u2

∂

∂u3

h1A1 h2A2 h3A3

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

4.4 Common Identities in Vector and Tensor Notation

• Here we present some of the widely used identities of vector calculus in the traditional

vector notation and in its equivalent tensor notation.

In the following bullet points, f

and h are diﬀerentiable scalar ﬁelds; A, B, C and D are diﬀerentiable vector ﬁelds; and

r = xiei is the position vector.
•

4.4 Common Identities in Vector and Tensor Notation

55

∇ · r = n

(cid:109)

∂ixi = n

∇ × r = 0

(cid:109)

ijk∂jxk = 0

∇ (a · r) = a

(cid:109)

∂i (ajxj) = ai

where n is the space dimension.
•

•

where a is a constant vector.
•

(158)

(159)

(160)

4.4 Common Identities in Vector and Tensor Notation

56

•

•

•

∇ · (∇f ) = ∇2f

(cid:109)

∂i (∂if ) = ∂iif

∇ · (∇ × A) = 0

(cid:109)

ijk∂i∂jAk = 0

∇ × (∇f ) = 0

(cid:109)

ijk∂j∂kf = 0

∇ (f h) = f∇h + h∇f

(cid:109)

∂i (f h) = f ∂ih + h∂if

(161)

(162)

(163)

(164)

4.4 Common Identities in Vector and Tensor Notation

57

•

•

•

•

(165)

(166)

∇ · (f A) = f∇ · A + A · ∇f

(cid:109)

∂i (f Ai) = f ∂iAi + Ai∂if

∇ × (f A) = f∇ × A + ∇f × A

(cid:109)

ijk∂j (f Ak) = f ijk∂jAk + ijk (∂jf ) Ak

A · (B × C) = C · (A × B) = B · (C × A)

(cid:109)

(cid:109)

ijkAiBjCk = kijCkAiBj = jkiBjCkAi

4.4 Common Identities in Vector and Tensor Notation

58

A × (B × C) = B (A · C) − C (A · B)

(cid:109)

ijkAjklmBlCm = Bi (AmCm) − Ci (AlBl)

A × (∇ × B) = (∇B) · A − A · ∇B

(cid:109)

ijkklmAj∂lBm = (∂iBm) Am − Al (∂lBi)

∇ × (∇ × A) = ∇ (∇ · A) − ∇2A

(cid:109)

ijkklm∂j∂lAm = ∂i (∂mAm) − ∂llAi

•

•

•

∇ (A · B) = A × (∇ × B) + B × (∇ × A) + (A · ∇) B + (B · ∇) A

(cid:109)

∂i (AmBm) = ijkAj (klm∂lBm) + ijkBj (klm∂lAm) + (Al∂l) Bi + (Bl∂l) Ai

(167)

(168)

(169)

(170)

4.4 Common Identities in Vector and Tensor Notation

59

•

•

•

•

∇ · (A × B) = B · (∇ × A) − A · (∇ × B)

(cid:109)

∂i (ijkAjBk) = Bk (kij∂iAj) − Aj (jik∂iBk)

∇ × (A × B) = (B · ∇) A + (∇ · B) A − (∇ · A) B − (A · ∇) B

(cid:109)

ijkklm∂j (AlBm) = (Bm∂m) Ai + (∂mBm) Ai − (∂jAj) Bi − (Aj∂j) Bi

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) A · C A · D

B · C B · D

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(A × B) · (C × D) =

(cid:109)

ijkAjBkilmClDm = (AlCl) (BmDm) − (AmDm) (BlCl)

(171)

(172)

(173)

4.5 Integral Theorems in Tensor Notation

60

(A × B) × (C × D) = [D · (A × B)] C − [C · (A × B)] D

(cid:109)

(174)

ijkjmnAmBnkpqCpDq = (qmnDqAmBn) Ci − (pmnCpAmBn) Di

• In vector and tensor notations, the condition for a vector ﬁeld A to be solenoidal is:

∇ · A = 0

(cid:109)

∂iAi = 0

(175)

• In vector and tensor notations, the condition for a vector ﬁeld A to be irrotational is:

∇ × A = 0

(cid:109)

ijk∂jAk = 0

(176)

4.5 Integral Theorems in Tensor Notation

• The divergence theorem for a diﬀerentiable vector ﬁeld A in vector and tensor notation

is:

4.5 Integral Theorems in Tensor Notation

61

(cid:90)(cid:90)(cid:90)

(cid:90)(cid:90)
(cid:90)

S

A · n dσ

S

Ainidσ

∇ · A dτ =
(cid:109)

∂iAidτ =

V

(cid:90)

V

(177)

where V is a bounded region in nD space enclosed by a generalized surface S, dτ and dσ

are generalized volume and surface elements respectively, n and ni are unit normal to the

surface and its ith component respectively, and the index i ranges over 1, . . . , n.
• The divergence theorem for a diﬀerentiable rank-2 tensor ﬁeld A in tensor notation for

the ﬁrst index is given by:

(cid:90)

V

(cid:90)

S

∂iAildτ =

Ailnidσ

(178)

• The divergence theorem for diﬀerentiable tensor ﬁelds of higher ranks A in tensor nota-

tion for the index k is:

(cid:90)

V

(cid:90)

S

∂kAij...k...mdτ =

Aij...k...mnkdσ

(179)

• Stokes theorem for a diﬀerentiable vector ﬁeld A in vector and tensor notation is:

(cid:90)(cid:90)

S

(cid:90)

(∇ × A) · n dσ =
(cid:109)

ijk∂jAknidσ =

S

(cid:90)
(cid:90)

C

C

A · dr

Aidxi

(180)

where C stands for the perimeter of the surface S and dr is the vector element tangent to

4.6 Examples of Using Tensor Techniques to Prove Identities

62

the perimeter.
• Stokes theorem for a diﬀerentiable rank-2 tensor ﬁeld A in tensor notation for the ﬁrst

index is:

(cid:90)

S

ijk∂jAklnidσ =

(cid:90)

C

Aildxi

(181)

• Stokes theorem for diﬀerentiable tensor ﬁelds of higher ranks A in tensor notation for

the index k is:

(cid:90)

S

ijk∂jAlm...k...nnidσ =

(cid:90)

C

Alm...k...ndxk

(182)

4.6 Examples of Using Tensor Techniques to Prove Identities

• ∇ · r = n:

• ∇ × r = 0:

∇ · r = ∂ixi

(Eq. 127)

= δii

(Eq. 68)

= n (Eq. 68)

[∇ × r]i = ijk∂jxk

(Eq. 134)

= ijkδkj

(Eq. 67)

= ijj

(Eq. 64)

= 0 (Eq. 59)

Since i is a free index the identity is proved for all components.

4.6 Examples of Using Tensor Techniques to Prove Identities

63

• ∇ (a · r) = a:

[∇ (a · r)]i = ∂i (ajxj)

(Eqs. 122 & 108)

= aj∂ixj + xj∂iaj

(product rule)

= aj∂ixj

(aj is constant)

= ajδji

(Eq. 67)

= ai

(Eq. 64)

= [a]i

(deﬁnition of index)

Since i is a free index the identity is proved for all components.
• ∇ · (∇f ) = ∇2f :

∇ · (∇f ) = ∂i [∇f ]i

(Eq. 127)

= ∂i (∂if )

(Eq. 122)

= ∂i∂if

(rules of diﬀerentiation)

= ∂iif
= ∇2f

(deﬁnition of 2nd derivative)

(Eq. 139)

• ∇ · (∇ × A) = 0:

4.6 Examples of Using Tensor Techniques to Prove Identities

64

∇ · (∇ × A) = ∂i [∇ × A]i

(Eq. 127)

= ∂i (ijk∂jAk)

(Eq. 134)

= ijk∂i∂jAk

(∂ not acting on )

= ijk∂j∂iAk
= −jik∂j∂iAk
= −ijk∂i∂jAk

(continuity condition)

(Eq. 72)

(relabeling dummy indices i and j)

= 0

(since ijk∂i∂jAk = −ijk∂i∂jAk)

This can also be concluded from line three by arguing that: since by the continuity con-

dition ∂i and ∂j can change their order with no change in the value of the term while a

corresponding change of the order of i and j in ijk results in a sign change, we see that

each term in the sum has its own negative and hence the terms add up to zero (see Eq.

82).
• ∇ × (∇f ) = 0:

[∇ × (∇f )]i = ijk∂j [∇f ]k

(Eq. 134)

= ijk∂j (∂kf )

(Eq. 122)

= ijk∂j∂kf

(rules of diﬀerentiation)

= ijk∂k∂jf
= −ikj∂k∂jf
= −ijk∂j∂kf

(continuity condition)

(Eq. 72)

(relabeling dummy indices j and k)

= 0

(since ijk∂j∂kf = −ijk∂j∂kf )

4.6 Examples of Using Tensor Techniques to Prove Identities

65

This can also be concluded from line three by a similar argument to the one given in the
previous point. Because [∇ × (∇f )]i is an arbitrary component, then each component is
zero.
• ∇ (f h) = f∇h + h∇f :

[∇ (f h)]i = ∂i (f h)

(Eq. 122)

= f ∂ih + h∂if
= [f∇h]i + [h∇f ]i
= [f∇h + h∇f ]i

(product rule)

(Eq. 122)

(Eq. 16)

Because i is a free index the identity is proved for all components.
• ∇ · (f A) = f∇ · A + A · ∇f :

∇ · (f A) = ∂i [f A]i

(Eq. 127)

= ∂i (f Ai)

(deﬁnition of index)

= f ∂iAi + Ai∂if
= f∇ · A + A · ∇f

(product rule)

(Eqs. 127 & 141)

• ∇ × (f A) = f∇ × A + ∇f × A:

4.6 Examples of Using Tensor Techniques to Prove Identities

66

[∇ × (f A)]i = ijk∂j [f A]k

(Eq. 134)

= ijk∂j (f Ak)

(deﬁnition of index)

= f ijk∂jAk + ijk (∂jf ) Ak
= f ijk∂jAk + ijk [∇f ]j Ak
= [f∇ × A]i + [∇f × A]i
= [f∇ × A + ∇f × A]i

(product rule & commutativity)

(Eq. 122)

(Eqs. 134 & 109)

(Eq. 16)

Because i is a free index the identity is proved for all components.
• A · (B × C) = C · (A × B) = B · (C × A):

A · (B × C) = ijkAiBjCk

(Eq. 110)

= kijAiBjCk

(Eq. 72)

= kijCkAiBj
= C · (A × B)

(commutativity)

(Eq. 110)

= jkiAiBjCk

(Eq. 72)

= jkiBjCkAi
= B · (C × A)

(commutativity)

(Eq. 110)

The negative permutations of these identities can be similarly obtained and proved by

changing the order of the vectors in the cross products which results in a sign change.
• A × (B × C) = B (A · C) − C (A · B):

4.6 Examples of Using Tensor Techniques to Prove Identities

67

[A × (B × C)]i = ijkAj [B × C]k

(Eq. 109)

= ijkAjklmBlCm (Eq. 109)

= ijkklmAjBlCm (commutativity)

= ijklmkAjBlCm (Eq. 72)
= (δilδjm − δimδjl) AjBlCm (Eq. 90)
= δilδjmAjBlCm − δimδjlAjBlCm (distributivity)
= (δilBl) (δjmAjCm) − (δimCm) (δjlAjBl)
= Bi (AmCm) − Ci (AlBl)
= Bi (A · C) − Ci (A · B)
= [B (A · C)]i − [C (A · B)]i
= [B (A · C) − C (A · B)]i

(Eq. 108)

(Eq. 64)

(commutativity and grouping)

(deﬁnition of index)

(Eq. 16)

Because i is a free index the identity is proved for all components. Other variants of this
identity [e.g. (A × B) × C] can be obtained and proved similarly by changing the order

of the factors in the external cross product with adding a minus sign.
• A × (∇ × B) = (∇B) · A − A · ∇B:

4.6 Examples of Using Tensor Techniques to Prove Identities

68

[A × (∇ × B)]i = ijkAj [∇ × B]k

(Eq. 109)

= ijkAjklm∂lBm

(Eq. 134)

= ijkklmAj∂lBm

(commutativity)

(Eq. 90)

(distributivity)

(Eq. 72)

= ijklmkAj∂lBm
= (δilδjm − δimδjl) Aj∂lBm
= δilδjmAj∂lBm − δimδjlAj∂lBm
= Am∂iBm − Al∂lBi
= (∂iBm) Am − Al (∂lBi)
= [(∇B) · A]i − [A · ∇B]i
= [(∇B) · A − A · ∇B]i

(Eq. 64)

(commutativity & grouping)
(Eq. 123 & § 2.6.5)

(Eq. 16)

Because i is a free index the identity is proved for all components.
• ∇ × (∇ × A) = ∇ (∇ · A) − ∇2A:

4.6 Examples of Using Tensor Techniques to Prove Identities

69

[∇ × (∇ × A)]i = ijk∂j [∇ × A]k

(Eq. 134)

= ijk∂j (klm∂lAm)

(Eq. 134)

= ijkklm∂j (∂lAm)

(∂ not acting on )

= ijklmk∂j∂lAm (Eq. 72 & deﬁnition of derivative)
= (δilδjm − δimδjl) ∂j∂lAm (Eq. 90)
= δilδjm∂j∂lAm − δimδjl∂j∂lAm (distributivity)
= ∂m∂iAm − ∂l∂lAi
= ∂i (∂mAm) − ∂llAi

(∂ shift, grouping & Eq. 4)

(Eq. 64)

= [∇ (∇ · A)]i −(cid:2)∇2A(cid:3)
= (cid:2)∇ (∇ · A) − ∇2A(cid:3)

i

(Eqs. 127, 122 & 140)

i

(Eqs. 16)

Because i is a free index the identity is proved for all components. This identity can also

be considered as an instance of the identity before the last one, observing that in the

second term on the right hand side the Laplacian should precede the vector, and hence no

independent proof is required.
• ∇ (A · B) = A × (∇ × B) + B × (∇ × A) + (A · ∇) B + (B · ∇) A:

We start from the right hand side and end with the left hand side

4.6 Examples of Using Tensor Techniques to Prove Identities

70

[A × (∇ × B) + B × (∇ × A) + (A · ∇) B + (B · ∇) A]i =

[A × (∇ × B)]i + [B × (∇ × A)]i + [(A · ∇) B]i + [(B · ∇) A]i = (Eq. 16)

ijkAj [∇ × B]k + ijkBj [∇ × A]k + (Al∂l) Bi + (Bl∂l) Ai = (Eqs. 109, 127 & indexing)

ijkAj (klm∂lBm) + ijkBj (klm∂lAm) + (Al∂l) Bi + (Bl∂l) Ai = (Eq. 134)

ijkklmAj∂lBm + ijkklmBj∂lAm + (Al∂l) Bi + (Bl∂l) Ai = (commutativity)

ijklmkAj∂lBm + ijklmkBj∂lAm + (Al∂l) Bi + (Bl∂l) Ai = (Eq. 72)
(δilδjm − δimδjl) Aj∂lBm + (δilδjm − δimδjl) Bj∂lAm + (Al∂l) Bi + (Bl∂l) Ai = (Eq. 90)

(δilδjmAj∂lBm − δimδjlAj∂lBm) + (δilδjmBj∂lAm − δimδjlBj∂lAm) + (Al∂l) Bi + (Bl∂l) Ai = (distributivity)

δilδjmAj∂lBm − Al∂lBi + δilδjmBj∂lAm − Bl∂lAi + (Al∂l) Bi + (Bl∂l) Ai = (Eq. 64)
δilδjmAj∂lBm − (Al∂l) Bi + δilδjmBj∂lAm − (Bl∂l) Ai + (Al∂l) Bi + (Bl∂l) Ai = (grouping)

δilδjmAj∂lBm + δilδjmBj∂lAm = (cancellation)

Am∂iBm + Bm∂iAm = (Eq. 64)

∂i (AmBm) = (product rule)

= [∇ (A · B)]i (Eqs. 122 & 127)

Because i is a free index the identity is proved for all components.
• ∇ · (A × B) = B · (∇ × A) − A · (∇ × B):

4.6 Examples of Using Tensor Techniques to Prove Identities

71

∇ · (A × B) = ∂i [A × B]i

(Eq. 127)

= ∂i (ijkAjBk)

(Eq. 109)

= ijk∂i (AjBk)

(∂ not acting on )

= ijk (Bk∂iAj + Aj∂iBk)

(product rule)

(distributivity)

(Eq. 72)

= ijkBk∂iAj + ijkAj∂iBk
= kijBk∂iAj − jikAj∂iBk
= Bk (kij∂iAj) − Aj (jik∂iBk)
= Bk [∇ × A]k − Aj [∇ × B]j
= B · (∇ × A) − A · (∇ × B)

(commutativity & grouping)

(Eq. 134)

(Eq. 108)

• ∇ × (A × B) = (B · ∇) A + (∇ · B) A − (∇ · A) B − (A · ∇) B:

4.6 Examples of Using Tensor Techniques to Prove Identities

72

[∇ × (A × B)]i = ijk∂j [A × B]k

(Eq. 134)

= ijk∂j (klmAlBm)

(Eq. 109)

= ijkklm∂j (AlBm)

(∂ not acting on )

= ijkklm (Bm∂jAl + Al∂jBm)

(product rule)

(Eq. 72)

(Eq. 90)

= ijklmk (Bm∂jAl + Al∂jBm)
= (δilδjm − δimδjl) (Bm∂jAl + Al∂jBm)
= δilδjmBm∂jAl + δilδjmAl∂jBm − δimδjlBm∂jAl − δimδjlAl∂jBm (distributivity)
= Bm∂mAi + Ai∂mBm − Bi∂jAj − Aj∂jBi
(Eq. 64)
= (Bm∂m) Ai + (∂mBm) Ai − (∂jAj) Bi − (Aj∂j) Bi
= [(B · ∇) A]i + [(∇ · B) A]i − [(∇ · A) B]i − [(A · ∇) B]i
= [(B · ∇) A + (∇ · B) A − (∇ · A) B − (A · ∇) B]i

(Eqs. 141 & 127)

(grouping)

(Eq. 16)

Because i is a free index the identity is proved for all components.

• (A × B) · (C × D) =

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) A · C A · D

B · C B · D

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12):

4.6 Examples of Using Tensor Techniques to Prove Identities

73

(A × B) · (C × D) = [A × B]i [C × D]i

(Eq. 108)

= ijkAjBkilmClDm

(Eq. 109)

(Eqs. 72 & 90)

(commutativity)

= ijkilmAjBkClDm
= (δjlδkm − δjmδkl) AjBkClDm
= δjlδkmAjBkClDm − δjmδklAjBkClDm
= (δjlAjCl) (δkmBkDm) − (δjmAjDm) (δklBkCl)
= (AlCl) (BmDm) − (AmDm) (BlCl)
= (A · C) (B · D) − (A · D) (B · C)

(Eq. 64)

(Eq. 108)

=

(deﬁnition of determinant)

(distributivity)

(commutativity & grouping)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) A · C A · D

B · C B · D

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

• (A × B) × (C × D) = [D · (A × B)] C − [C · (A × B)] D:

4.6 Examples of Using Tensor Techniques to Prove Identities

74

[(A × B) × (C × D)]i = ijk [A × B]j [C × D]k

(Eq. 109)

= ijkjmnAmBnkpqCpDq

(Eq. 109)

= ijkkpqjmnAmBnCpDq

(commutativity)

(distributivity)

(distributivity)

(Eq. 90)

= ijkpqkjmnAmBnCpDq
(Eq. 72)
= (δipδjq − δiqδjp) jmnAmBnCpDq
= (δipδjqjmn − δiqδjpjmn) AmBnCpDq
= (δipqmn − δiqpmn) AmBnCpDq
(Eq. 64)
= δipqmnAmBnCpDq − δiqpmnAmBnCpDq
= qmnAmBnCiDq − pmnAmBnCpDi
= qmnDqAmBnCi − pmnCpAmBnDi
= (qmnDqAmBn) Ci − (pmnCpAmBn) Di
= [D · (A × B)] Ci − [C · (A × B)] Di
= [[D · (A × B)] C]i − [[C · (A × B)] D]i
= [[D · (A × B)] C − [C · (A × B)] D]i

(Eq. 64)

(commutativity)

(grouping)

(Eq. 110)

(deﬁnition of index)

(Eq. 16)

Because i is a free index the identity is proved for all components.

5 METRIC TENSOR

75

5 Metric Tensor

• This is a rank-2 tensor which may also be called the fundamental tensor.
• The main purpose of the metric tensor is to generalize the concept of distance to gen-

eral curvilinear coordinate frames and maintain the invariance of distance in diﬀerent

coordinate systems.
• In orthonormal Cartesian coordinate systems the distance element squared, (ds)2, be-

tween two inﬁnitesimally neighboring points in space, one with coordinates xi and the

other with coordinates xi + dxi, is given by

(ds)2 = dxidxi = δijdxidxj

(183)

This deﬁnition of distance is the key to introducing a rank-2 tensor, gij, called the metric

tensor which, for a general coordinate system, is deﬁned by

(ds)2 = gijdxidxj = gijdxidxj

• The components of the metric tensor are given by:

gij = Ei · Ej

&

gij = Ei · Ej

• The mixed type metric tensor is given by:

j = Ei · Ej = δi
gi

j

(184)

(185)

(186)

and hence it is the same as the unity tensor.
• For a coordinate system in which the metric tensor can be cast in a diagonal form the

metric is called ﬂat.

5 METRIC TENSOR

76

• For Cartesian coordinate systems, which are orthonormal ﬂat-space systems, we have

gij = δij = gij = δij

• The metric tensor is symmetric, that is

gij = gji

&

gij = gji

(187)

(188)

• The contravariant metric tensor is used for raising indices of covariant tensors and the

covariant metric tensor is used for lowering indices of contravariant tensors, e.g.

Ai = gijAj

Ai = gijAj

(189)

where the metric tensor acts, like a Kronecker delta, as an index replacement operator.

Hence, any tensor can be cast into a covariant or a contravariant form, as well as a mixed

form. The order of the indices should be respected in this process, e.g.

j = gjkAik (cid:54)= A i

Ai

j = gjkAki

(190)

Some authors insert dots (e.g. A· i
• The covariant and contravariant metric tensors are inverse of each other, that is

j ) to remove any ambiguity about the order of the indices.

[gij] =(cid:2)gij(cid:3)−1

&

(cid:2)gij(cid:3) = [gij]

−1

Hence

gikgkj = δi
j

&

gikgkj = δ j
i

• It is common to reserve the “metric tensor” to the covariant form and call the con-

(191)

(192)

5 METRIC TENSOR

77

travariant form, which is its inverse, the “associate” or “conjugate” or “reciprocal” metric

tensor.
• As a tensor, the metric has a signiﬁcance regardless of any coordinate system although

it requires a coordinate system to be represented in a speciﬁc form.
• For orthogonal coordinate systems the metric tensor is diagonal, i.e. gij = gij = 0 for
i (cid:54)= j.
• For ﬂat-space orthonormal Cartesian coordinate systems with coordinates (x, y, z), the

• For cylindrical coordinate systems with coordinates (ρ, φ, z), the metric tensor is given

• For spherical coordinate systems with coordinates (r, θ, φ), the metric tensor is given by:

metric tensor is given by:

[gij] = [δij] =

by:



1

0

0

0 ρ2 0

0

0

1

[gij] =



[gij] =

1

0

0 r2

0

0

0

0

r2 sin2 θ





1 0 0

0 1 0

0 0 1

&

&

 =(cid:2)δij(cid:3) =(cid:2)gij(cid:3)



(cid:2)gij(cid:3) =

(cid:2)gij(cid:3) =

1
r2

0

0

1

0

0

1

0

0

0

(193)

(195)



(194)

0

1
ρ2 0

0

1



0

0

1

r2 sin2 θ

6 COVARIANT DIFFERENTIATION

78

6 Covariant Diﬀerentiation

• The ordinary derivative of a tensor is not a tensor in general. The objective of covariant

diﬀerentiation is to ensure the invariance of derivative (i.e. being a tensor) in general

coordinate systems, and this results in applying more sophisticated rules using Christof-

fel symbols where diﬀerent diﬀerentiation rules for covariant and contravariant indices

apply. The resulting covariant derivative is a tensor which is one rank higher than the

diﬀerentiated tensor.
• Christoﬀel symbol of the second kind is deﬁned by:

(cid:8)k

ij

(cid:9) =

gkl
2

(cid:18) ∂gil

∂xj +

∂gjl

∂xi − ∂gij

∂xl

(cid:19)

(196)

where the indexed g is the metric tensor in its contravariant and covariant forms with

implied summation over l. It is noteworthy that Christoﬀel symbols are not tensors.
• Christoﬀel symbol of the second kind is symmetric in its two lower indices:

(cid:8)k

ij

(cid:9) =(cid:8)k

ji

(cid:9)

(197)

• In Cartesian coordinate systems (x, y, z), Christoﬀel symbols are zero for all the values

of indices.
• For cylindrical coordinate systems (ρ, φ, z), Christoﬀel symbols are zero for all the values

of indices except:

(cid:8)1
(cid:8)2

22

12

(cid:9) = −ρ
(cid:9) = (cid:8)2

21

(cid:9) =

1
ρ

where (1, 2, 3) stand for (ρ, φ, z).

6 COVARIANT DIFFERENTIATION

79

• For spherical coordinate systems (r, θ, φ), Christoﬀel symbols are zero for all the values

of indices except:

22

33

12

33

(cid:8)1
(cid:8)1
(cid:8)2
(cid:8)2
(cid:8)3
(cid:8)3

13

23

(cid:9) = −r
(cid:9) = −r sin2 θ
(cid:9) = (cid:8)2
(cid:9)
(cid:9) = − sin θ cos θ
(cid:9) = (cid:8)3
(cid:9) =
(cid:9) = (cid:8)3
(cid:9) = cot θ

1
r

1
r

=

31

21

32

where (1, 2, 3) stand for (r, θ, φ).
• For a diﬀerentiable scalar f the covariant derivative is the same as the normal partial

derivative, that is:

f;i = f,i = ∂if

(198)

This is justiﬁed by the fact that the covariant derivative is diﬀerent from the normal partial

derivative because the basis vectors in general coordinate systems are dependent on their

spatial position, and since a scalar is independent of the basis vectors the covariant and

partial derivatives are identical.
• For a diﬀerentiable vector A the covariant derivative is:

(cid:9) Ak
Aj;i = ∂iAj −(cid:8)k
;i = ∂iAj +(cid:8)j
(cid:9) Ak

ji

ki

Aj

(covariant)

(199)

(contravariant)

(200)

6 COVARIANT DIFFERENTIATION

80

• For a diﬀerentiable rank-2 tensor A the covariant derivative is:

ji

ki

Ajk;i = ∂iAjk −(cid:8)l
(cid:9) Ajl
(cid:9) Alk −(cid:8)l
;i = ∂iAjk +(cid:8)j
(cid:9) Alk +(cid:8)k
(cid:9) Ajl
j +(cid:8)k
(cid:9) Al
j −(cid:8)l
(cid:9) Ak

j;i = ∂iAk

Ak

ji

l

li

li

li

Ajk

(covariant)

(201)

(contravariant)

(202)

(mixed)

(203)

• For a diﬀerentiable rank-n tensor A the covariant derivative is:

Aij...k

lm...p;q = ∂qAij...k

lm...p +(cid:8)i
−(cid:8)a

aq

lq

(cid:9) Aaj...k
lm...p +(cid:8)j
(cid:9) Aij...k
am...p −(cid:8)a

(cid:9) Aia...k
lm...p + ··· +(cid:8)k
(cid:9) Aij...k
la...p − ··· −(cid:8)a

aq

pq

(cid:9) Aij...a
(cid:9) Aij...k

lm...p

lm...a

aq

mq

• From the last three points a pattern for covariant diﬀerentiation emerges, that is it starts

with a partial derivative term then for each tensor index an extra Christoﬀel symbol term

is added, positive for superscripts and negative for subscripts, where the diﬀerentiation

index is the second of the lower indices in the Christoﬀel symbol.
• Since Christoﬀel symbols are identically zero in Cartesian coordinate systems, the co-

variant derivative is the same as the normal partial derivative for all tensor ranks.
• The covariant derivative of the metric tensor is zero in all coordinate systems.
• Several rules of normal diﬀerentiation similarly apply to covariant diﬀerentiation. For

example, covariant diﬀerentiation is a linear operation with respect to algebraic sums of

tensor terms:

∂;i (aA ± bB) = a∂;iA ± b∂;iB

(204)

6 COVARIANT DIFFERENTIATION

81

where a and b are scalar constants and A and B are diﬀerentiable tensor ﬁelds. The

product rule of normal diﬀerentiation also applies to covariant diﬀerentiation of tensor

multiplication:

∂;i (AB) = (∂;iA) B + A∂;iB

(205)

This rule is also valid for the inner product of tensors.
• The covariant derivative operator can bypass the raising/lowering index operator:

Ai = gijAj

=⇒

∂;mAi = gij∂;mAj

(206)

and hence the metric behaves like a constant with respect to the covariant operator.
• A principal diﬀerence between normal partial diﬀerentiation and covariant diﬀerentiation

is that for successive diﬀerential operations the partial derivative operators do commute

with each other (assuming certain continuity conditions) but the covariant operators do

not commute, that is

∂i∂j = ∂j∂i

but

∂;i∂;j (cid:54)= ∂;j∂;i

(207)

• Higher order covariant derivatives are similarly deﬁned as derivatives of derivatives;

however the order of diﬀerentiation should be respected (refer to the previous point).

REFERENCES

References

82

[1] G.B. Arfken; H.J. Weber; F.E. Harris. Mathematical Methods for Physicists A Com-

prehensive Guide. Elsevier Academic Press, seventh edition, 2013.

[2] R.B. Bird; R.C. Armstrong; O. Hassager. Dynamics of Polymeric Liquids, volume 1.

John Wiley & Sons, second edition, 1987.

[3] R.B. Bird; W.E. Stewart; E.N. Lightfoot. Transport Phenomena. John Wiley & Sons,

second edition, 2002.

[4] M.L. Boas. Mathematical Methods in the Physical Sciences. John Wiley & Sons Inc.,

third edition, 2006.

[5] C.F. Chan Man Fong; D. De Kee; P.N. Kaloni. Advanced Mathematics for Engineering

and Science. World Scientiﬁc Publishing Co. Pte. Ltd., ﬁrst edition, 2003.

[6] T.L. Chow. Mathematical Methods for Physicists: A concise introduction. Cambridge

University Press, ﬁrst edition, 2003.

[7] J.H. Heinbockel. Introduction to Tensor Calculus and Continuum Mechanics. 1996.

[8] D.C. Kay. Schaum’s Outline of Theory and Problems of Tensor Calculus. McGraw-

Hill, ﬁrst edition, 1988.

[9] K.F. Riley; M.P. Hobson; S.J. Bence. Mathematical Methods for Physics and Engi-

neering. Cambridge University Press, third edition, 2006.

[10] D. Zwillinger, editor. CRC Standard Mathematical Tables and Formulae. CRC Press,

32nd edition, 2012.

Note: As well as the references cited above, I beneﬁted during the writing of these notes

from many sources such as tutorials, presentations, and articles which I found on the

REFERENCES

83

Internet authored or composed by other people. As it is diﬃcult or impossible to retrace

and state all these sources, I make a general acknowledgment to all those who made their

documents available to the public.

