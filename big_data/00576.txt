Distributed Estimation of Dynamic Parameters : Regret Analysis

Shahin Shahrampour, Alexander Rakhlin and Ali Jadbabaie

6
1
0
2

 
r
a

M
2

 

 
 
]

.

C
O
h
t
a
m

[
 
 

1
v
6
7
5
0
0

.

3
0
6
1
:
v
i
X
r
a

Abstract— This paper addresses the estimation of a time-
varying parameter in a network. A group of agents sequentially
receive noisy signals about the parameter (or moving target),
which does not follow any particular dynamics. The parameter
is not observable to an individual agent, but it is globally
identiﬁable for the whole network. Viewing the problem with
an online optimization lens, we aim to provide the ﬁnite-time or
non-asymptotic analysis of the problem. To this end, we use a
notion of dynamic regret which suits the online, non-stationary
nature of the problem. In our setting, dynamic regret can be
recognized as a ﬁnite-time counterpart of stability in the mean-
square sense. We develop a distributed, online algorithm for
tracking the moving target. Deﬁning the path-length as the
consecutive differences between target locations, we express an
upper bound on regret in terms of the path-length of the target
and network errors. We further show the consistency of the
result with static setting and noiseless observations.

I. INTRODUCTION

Distributed detection, learning and estimation has been
a main topic of interest in the past three decades [1]–[4].
With wide-spread applications in sensor, robotic, economic
and social networks, distributed algorithms have received a
considerable attention in science and engineering [5], [6]. In
these scenarios, a group of agents aim to learn or estimate the
value of a parameter. Each individual agent receives partially
informative data about the parameter; however, the global
spread of information in the network allows agents to ac-
complish the task collaboratively. Many of these information
aggregation methods use consensus protocols as a crucial
component [7], [8]. Distributed algorithms are popular for
their ability to handle large data sets, low computation burden
on agents and robustness to node failures.

On the other hand, online learning and optimization has
been intensively studied in the literature of machine learning
[9], [10], proving to be a powerful tool to model sequential
decision problems. The problem can be modeled as a game
between a learner and an adversary. The learner sequentially
selects actions, and the adversary reveals the corresponding
losses to the learner. The term online refers to the fact that
the learner receives data in a sequential fashion. The popular
performance metric for online algorithms is called regret.

This work was supported by ONR BRC Program on Decentralized,

Online Optimization.

Shahin Shahrampour

is with the Department of Electrical Engi-
neering at Harvard University, Cambridge, MA 02138 USA. (e-mail:
shahin@seas.harvard.edu).

Alexander Rakhlin is with the Department of Statistics
University of Pennsylvania, Philadelphia, PA 19104 USA.
rakhlin@wharton.upenn.edu).

at
the
(e-mail:

Ali Jadbabaie is with the Department of Electrical and Systems Engi-
neering at the University of Pennsylvania, Philadelphia, PA 19104 USA.
(email: jadbabai@seas.upenn.edu).

Regret measures the performance of algorithm versus a pre-
deﬁned benchmark. For instance, the benchmark could be
the best ﬁxed action had the learner known all the losses in
advance. In a broad sense, when the benchmark is a ﬁxed
sequence, the regret is called static, whereas a time-varying
benchmark sequence brings forward the notion of dynamic
regret [10]–[12].

The goal of this paper is to develop a distributed, online
algorithm for tracking time-varying parameters. To this end,
a unifying observation is to view the distributed estimation
problem as an online optimization. A network of agents aim
to track a moving target which is only partially observable
to each agent. The parameter can represent a sequence of
high-dimensional data where agents only receive a low-
dimensional version of it. Therefore, they must communicate
with each other to track the target. Each time agents form
their estimates of the parameter, the whole network incurs a
loss (network loss). We formalize the problem as an instance
of distributed, online optimization whose dynamic regret can
be characterized in the general form of

Regret = Decentralized Loss - Centralized Loss.

In other words, the regret compares the performance of the
decentralized algorithm versus its centralized counterpart.

Our main contribution is to provide the ﬁnite-time or non-
asymptotic analysis of the problem using the notion of regret.
In this context, regret can also be interpreted as a ﬁnite-
time analog of stability in mean-square sense. In contrast
to existing approaches, we do not restrict
the dynamics
of the parameter, i.e., the target is allowed to move along
an arbitrary trajectory. The freedom given to the dynamics
broadens the application of the problem. For instance, the
parameter can represent the price of a product which does
not have a particular pattern, or the location of a vehicle
that moves along an unpredictable trajectory. In turn, our
estimation update is akin to consensus+innovation updates
in the literature with an important distinction that it does not
assume any dynamics on the parameter.

We then characterize the behavior of regret in ﬁnite-time
regime. To this end, we quantify the path-length in terms
of the consecutive differences between target locations, and
prove that the regret can be bounded in terms of the path-
length of the target. Interestingly, our regret bound interpo-
lates smoothly from static to dynamic setting. That is, when
the parameter is static, we can recover the corresponding
results with static regret (with appropriate tuning of problem
parameters). We further simplify the bound in two cases:
static target and noiseless observation.

Related Work

There exists a large body of literature on estimation
of dynamic parameters. These works mostly assume that
the parameter follows a known dynamics, say, the model
can be represented as an LTI system. Perhaps the most
classical example is the celebrated Kalman ﬁltering [13]. The
elegance of Kalman ﬁlter has motivated a lot of researchers
to investigate the problem in the context of networks. We
cannot hope to do the justice to the extensive literature on
distributed Kalman ﬁltering, and refer the avid reader to
a series of works on this topic [14]–[17]. The works of
[18]–[21] are also in the same spirit in which the parameter
follows linear dynamics. Acemoglu et al. [18] present a rule-
of-thumb learning rule, and provide the asymptotic behavior
of their update. In [19], Khan et al. investigate the trade-
off between stability of the linear dynamics and connectivity
of the network. They show that their update can potentially
track unstable linear models driven by noise. Similarly, an
algorithm consisting of pseudo-innovations is developed in
[20]. Moreover, the authors of [21] consider a scalar linear
model, and characterize an explicit expression for the mean-
square deviation. Restricting their attention to noiseless case,
De et al. pose an inverse problem in [22]. In their model, the
parameter can be observed whereas the dynamics is unknown
and must be estimated. On the other hand, Atanasov et
al. [23] propose a distributed, linear estimator in wireless
sensor networks which encompasses an auxiliary localization
procedure. The authors prove the mean-square consistency
of the joint localization and target estimation algorithm.
Moreover, Sayin et al. [24] present a distributed, online
algorithm for state estimation applicable to Big Data. The
authors of [25] develop an optimization-based, prediction-
correction method for tracking the trajectory of a moving
target. Finally, our work is also related to [26], where the
problem is considered in the context of social networks.
Organization

The rest of this paper is organized as follows. In the
next section we introduce the notation, and formalize the
estimation problem using the notion of regret. In Section
III, we present our technical results and their consequences.
Notably, we demonstrate a path-length bound on regret.
Section IV consists of concluding remarks, and in Section V
we provide the proof of our technical results.
II. PROBLEM FORMULATION

A. Nomenclature

We adhere to the following notation in the exposition of

our results:

[n]
x⊤
Id
k · k

1d
0d

λi(P )
ρ(P )

The set {1, 2, ..., n} for any integer n
Transpose of the vector x
Identity matrix of size d × d
The spectral norm operator
Vector of all ones with dimension d
Vector of all zeros with dimension d
The i-th largest eigenvalue of matrix P
The spectral radius of matrix P

Throughout, the variables in bold are random, and all the
vectors are in column form. When referring to vector of all
ones or zeros, sometimes the dimension is omitted and can
be inferred from the context.

B. Observation Model and Regret Deﬁnition

Our objective is to track a time-varying parameter that
is not conﬁned to a certain dynamics. The parameter could
represent
the location of a moving target following an
arbitrary trajectory. We denote the parameter by θt ∈ R
d
at each time t ∈ [T ]. A network of n agents collaborate with
each other to track the parameter which is only partially
observable to each agent. More formally, the observation
model for agent i ∈ [n] can be expressed in the following
form

yi,t = Hiθt + wi,t,

(1)

mi×d and wi,t ∈ R

mi denote the observation
where Hi ∈ R
matrix and noise, respectively. For instance, θt can be a
high-dimensional data where agents only observe a low-
dimensional version of that, i.e. mi might be much smaller
than d. We respect the standard assumption of zero-mean
and ﬁnite-covariance for the observation noise. That
is,

i,twi,t(cid:3) = Wi,

respectively. We further assume the independence over time

we assume that E [wi,t] = 0 and E(cid:2)w⊤
and space; that is, E(cid:2)w⊤

i,twj,s(cid:3) = 0 for i 6= j or s 6= t.

Note that when the parameter is static, i.e. when θt = θ,
we recover the classical distributed estimation problem (see
e.g. [2]). Though not locally observable to each agent, we
assume that the parameter is globally identiﬁable from the
standpoint of all agents in the network.

Assumption 1: The sequence {θt}T

t=1 is globally identiﬁ-

able, i.e., it holds that the matrix

H =

1
n

n

Xi=1

H ⊤

i Hi,

is invertible.

The estimation procedure follows an online protocol. At
time t, each agent i forms an estimate ˆθi,t of θt based
on observations {yi,τ }t−1
τ =1. After that, the new signal yi,t
becomes available to the agent. The online nature of our
estimation problem allows us to pose it as an instance of
online optimization. Therefore, before deriving the explicit
update for ˆθi,t, we need to introduce a few notions based
on the literature of online optimization. Let us deﬁne a local
square loss

ℓi,t(θ) := Ehkyi,t − Hiθk2i ,

for each agent i ∈ [n], followed by the network loss as

(2)

(3)

ℓt(θ) :=

n

Xi=1

Ehkyi,t − Hiθk2i .

Note that Assumption 1 guarantees that ℓt(θ) has a unique
minimizer at each time t. Agents aim to minimize the

accumulated network loss over time. Equivalently, the goal
of the network is to minimize the regret deﬁned as

RegT :=

1
T

T

Xt=1(cid:18) 1

n

n

Xj=1

ℓt(ˆθj,t) − ℓt(θt)(cid:19).

(4)

The regret quantiﬁes the difference between the realized
and ideal network loss: the estimation ˆθj,t of each agent
j is evaluated at the function ℓt(·), and the average (among
agents) is subtracted by the loss of exact, ideal estimate θt.
Evidently, the ideal estimate (solution to the network loss
(3)) is not available to an individual agent; hence, one can
also interpret the regret as the difference between an algo-
rithm that receives local loss (decentralized) versus another
algorithm that accesses the network loss (centralized).

C. Network Model and Estimation Update

The interactions of agents, which in turn deﬁnes the
network, is captured with the matrix P . Formally, we denote
by [P ]ij, the ij-th entry of the matrix P . When [P ]ij > 0,
agent i communicates with agent j. We assume that P is
symmetric, stochastic with positive diagonal elements. The
assumption simply guarantees the information ﬂow in the
network. Alternatively, from technical point of view, we
respect the following hypothesis.

Assumption 2: We assume the Markov chain P is irre-

ducible and aperiodic, and P ⊤ = P .
The assumption implies that the Markov chain P has a
⊤ is the unique
unique stationary distribution i.e., 1
(unnormalized) left eigenvector corresponding to λ1(P ) = 1.
It also entails that λ1(P ) is unique, and the other eigenvalues
of P are less than unit in magnitude [27].

⊤P = 1

To follow the trajectory of the parameter, we propose an
update rule which can be cast as a distributed variant of
Online Gradient Descent [10] with noisy feedback. It takes
the form

ˆθi,t =

n

Xj=1

[P ]ij ˆθj,t−1 + αH ⊤

i (cid:16)yi,t−1 − Hi ˆθi,t−1(cid:17) ,

(5)

and

becomes useful. As we shall explore in the next section, there
are connections between mean-square stability and regret;
hence, we close this section by deﬁning the mean-square
stability.

Deﬁnition 1: We say the process is stable in the mean-

square sense, whenever

1
n

n

Xi=1

Eh(cid:13)(cid:13)

ˆθi,t − θt(cid:13)(cid:13)

2i −→ σ,

where σ ≥ 0 is a ﬁnite constant.

III. MAIN RESULTS

In this section, we provide our technical results and
their consequences. More speciﬁcally, our objective is to
ﬁnd an anytime bound on the regret involving the path-
length of target. We further interpret the bound in several
circumstances.

A. Error Process and Stability

Stability (in mean or mean-square sense) is closely at-
tached to the behavior of estimation error. Let us deﬁne the
local error process as follows,

ei,t := ˆθi,t − θt.

(7)

Stacking the local errors in a vector, we represent the global
error by

et := [e1,t, . . . , en,t]⊤.

(8)

In the following lemma, we show that the error process can
be represented as an LTI system.

Lemma 1: The error process (8) can be characterized via

an LTI system as follows

et = Qet−1 + ut,

where

1 H1, . . . , H ⊤

Q := P ⊗ Id − α diag(cid:2)H ⊤
ut := 1n ⊗ (θt−1 − θt) + α 


n Hn(cid:3) ,



.

...

H ⊤

1 w1,t−1

H ⊤

n wn,t−1

We observe from Lemma 1 that to ensure mean stability (to
have E[et] → 0 as t → ∞), the conditions

ρ(Q) < 1

and

E[θt − θt−1] → 0,

need to be satisﬁed. To push the spectral radius of Q inside
the unit circle, we can simply tune the step size α. We
quantify this condition in Section III-C, and show the role
of global identiﬁability (Assumption 1) and connectivity of
network (Assumption 2) in existence of such α.

However, the second condition entirely depends on the
problem environment. For instance, the condition is evidently
satisﬁed in the static case (θt = θ). It also holds when θt is
generated randomly based on a stationary distribution, i.e.
whenever E[θt] = θ. Regardless of stability, we can always
characterize the regret behavior in terms of path-length of
the target.

where α ∈ R is the step size. The update is akin to
consensus+innovation updates in the literature (see e.g. [19],
[28]) with a distinction that it does not assume any dynamics
on the parameter θt. The consensus part forces agents to
keep their estimates close to each other, and the innovation
part takes into account the new observation. We can now
shed light on our previous interpretation of regret: had the
agents known the network loss ℓt(θ), they could have formed
estimates based on Online Gradient Descent (which is a
centralized algorithm). Since agents only know the local loss
(2), regret (4) is comparing the distributed algorithm versus
its centralized counterpart.

We further deﬁne the path-length of the moving target as

T

CT :=

kθt − θt−1k2,

(6)

which indicates how much the parameter drifts over time. In
particular, when bounding the regret (4), the path-length CT

Xt=1

B. A Path-Length Bound for Regret

In this section, we analyze the regret (4), which is a
measure of ﬁnite-time performance. The following lemma
exhibits the connection of mean-square stability (Deﬁnition
1) and regret (4). In particular, the lemma is used to bound
the regret in terms of path-length of the target (6).

Lemma 2: The regret deﬁned in (4) can be bounded as,

RegT ≤

1
nT

T

n

Xt=1

Xi=1

kHik2

Ehketk2i .

Lemma 2 proves that stability implies boundedness of
asymptotic regret since

Ehketk2i =

n

Xi=1

Eh(cid:13)(cid:13)

2i ,
ˆθi,t − θt(cid:13)(cid:13)

and once the limit exists, the Ces`aro mean preserves it.
However, we are interested in non-asymptotic analysis of
the problem. We present our ﬁnite-time statement in the
subsequent theorem.

Theorem 1: Let the sequence {θt}T

t=1 be globally identiﬁ-
able (Assumption 1), and the Markov chain P be irreducible
and aperiodic (Assumption 2). If each agent i ∈ [n] generates
the estimate sequence {ˆθi,t}T
t=1 according to the update rule
(5), the regret satisﬁes

RegT ≤

1
n

n

Xi=1

i=1 kHik2Wi
1 − kQk

kHik2 α2Pn
Xi=1

1
T

+

n

kHik2

CT

(1 − kQk)2 .

Theorem 1 provides a path-length bound for regret. The
underlying intuition behind the term CT is as follows: as
agents decide on the next value based on previous observa-
tions (online prediction), they are always one step behind
in estimation. Even when the observations are noiseless
(Wi = 0 for all i ∈ [n]), the second term still remains,
which is an artifact of agent i using yi,t−1 to predict θt.

We remark that path-length regret bounds were previously
studied in the context of online optimization for centralized
frameworks (see e.g. [10]–[12]). Here, we specialized to
quadratic losses, and proved a path-length bound in dis-
tributed setting. Therefore, our bound involves an additional
network penalty comparing to the centralized framework.

as H = 1
obtain

nPn

i=1 H ⊤

i Hi. Therefore, since 1

⊤

1 = nd, we

⊤Q1

1

⊤

= 1 − nα 1

⊤

d H 1d

⊤

= 1 − α 1

1

1

1

1

⊤

d H 1d
⊤
d 1d

1

.

In view of Assumption 2, λ1(P ) = 1 is unique, and no direc-
tion other than 1n can recover the trivial eigenvalue. Hence,

depending on the null space of diag(cid:2)H ⊤

can be set small enough such that the following upper bound
holds

n Hn(cid:3), α

1 H1, . . . , H ⊤

λ1(Q) ≤ 1 − αλn(H).

(9)

By the same token, Weyl’s eigenvalue inequality implies a
lower bound the smallest eigenvalue of Q as

1 H1, . . . , H ⊤

(10)

λn(Q) ≥ λn(P ) − αλ1(diag(cid:2)H ⊤

Once again choosing small enough α guarantees that the
RHS of (9) is larger than the absolute value of the RHS of
(10). Let us distinguish such regime as α ≤ αmax. Then,
symmetry of Q warrants that

n Hn(cid:3)).

kQk ≤ 1 − αλn(H),

∀α ≤ αmax.

(dependence on
i=1 and H) in Theorem 1, we can simplify the regret

Therefore, disregarding the constants
{Hi}n
bound using above as follows

RegT ≤ O α

Wi +

CT

T α2! .

n

Xi=1

The following comments are now in order:

• If the target is ﬁxed, i.e. θt = θ for every t ∈ [T ],
we have CT = 0, and the second term in the bound
vanishes. Then, we can set α = min{ 1
T , αmax} to
maintain the O( 1
T ) rate. This choice of step size and
the corresponding result is consistent with the results in
static setting.

• When the observations are noiseless (Wi = 0) the ﬁrst
term becomes zero, and α = min{1, αmax} recovers
the O( CT
T ) rate. In this regime, a sub-linear path-length
CT always guarantees a zero asymptotic regret.

• In the general case tuning α = min{C

1/3
T T −1/3, αmax}

yields a regret of O(cid:16)C

T .

1/3

T T −1/3(cid:17) which holds for any

C. Tuning the Step Size

We now discuss several aspects of the regret bound derived
in Theorem 1. To this end, we need to clarify the dependence
of kQk on step size α to analyze the bound. Recall the
closed-form of Q in Lemma 1 as well as stochasticity of P
which implies P 1n = 1n; pre/post multiply Q by a vector
of all ones with dimension nd (1 = 1n ⊗ 1d) to get

1 H1, . . . , H ⊤

1

⊤Q1 = 1
= 1

= 1

= 1

⊤(cid:0)P ⊗ Id − α diag(cid:2)H ⊤
⊤(cid:0)1 − α diag(cid:2)H ⊤
⊤diag(cid:2)H ⊤

1 − nα 1

1 − α 1

d H 1d,

⊤

⊤

⊤

1 H1, . . . , H ⊤

1 H1, . . . , H ⊤

n Hn(cid:3) 1(cid:1)
n Hn(cid:3) 1

n Hn(cid:3)(cid:1) 1

IV. CONCLUDING REMARKS

In this paper, we considered the distributed estimation
problem in dynamic environments. A network of agents
partially observe the parameter of interest which is time-
varying with no particular dynamics. However, the parameter
is globally observable from the standpoint of all agents
together. We pose the problem as an instance of distributed,
online optimization, where agents suffer some loss after
each round of estimation. In turn, they aim to minimize the
network loss deﬁned as the sum of individual losses. We
formulated the problem using the notion of dynamic regret
from online optimization literature. Our main contribution
was to provide the non-asymptotic analysis of the dynamic

where we used (7) in the last step. Given that noise realiza-
tions are independent over time, we can simplify above

ℓt(ˆθj,t) =

=

n

n

EhkHiej,t − wi,tk2i
Xi=1
Xi=1(cid:16)EhkHiej,tk2i + Wi(cid:17) .

Therefore, combining (12) and (13), we get

(13)

ℓt(ˆθj,t) − ℓt(θt) =

1
n

n

Xj=1

n

n

1
n

n

n

≤

1
n

EhkHiej,tk2i

Xj=1
Xi=1
Ehkej,tk2i
Xj=1
Xi=1
=(cid:16) 1
kHik2(cid:17)Ehketk2i ,
Xi=1

kHik2

n

n

where we used (8) in the last line. Summing over t ∈ [T ]
completes the proof.
(cid:4)

regret. We ﬁrst showed that the regret can be related to
the ﬁnite-time counterpart of stability. Next, we deﬁned the
path-length as the consecutive differences between target
locations, and derived an upper bound on regret in terms
of the path-length. We ﬁnally demonstrated that our bound
is consistent with special cases of static setting and noiseless
observations.

There are several directions in which our work can be
improved. Our setting does not include any prior assumption
on the dynamics of the parameter. While the generality of
this setup is appealing, it is also interesting to investigate
the problem for particular dynamics. More speciﬁcally, once
agents know the dynamics of the parameter (even in the
non-linear case) they can incorporate it into the algorithm,
and obtain more accurate estimates. Furthermore, the choice
of step size in the general case requires a prior knowledge
of the path-length. An alternative approach is to have an
online feedback of the trajectory as the algorithm moves for-
ward. Therefore, an adaptive, online algorithm for tracking
dynamic parameters is still an open problem.

Proof of Lemma 1. In view of equations (1) and (5), we
write

n

ˆθi,t − θt =

+ αH ⊤

=

n

[P ]ij ˆθj,t−1 − θt

Xj=1
i (cid:16)yi,t−1 − Hi ˆθi,t−1(cid:17)
Xj=1
i (cid:16)Hiθt−1 + wi,t−1 − Hi ˆθi,t−1(cid:17) ,

[P ]ij(ˆθj,t−1 − θt−1) + θt−1 − θt

+ αH ⊤

(11)

j=1[P ]ij = 1. Rewriting above based on the deﬁni-

tion of local error process (7), we obtain

since Pn

n

Xj=1

ei,t =

[P ]ij ej,t−1 − αH ⊤

i Hiei,t−1

+ θt−1 − θt + α H ⊤

i wi,t−1.

Representing above in the matrix form using the Kronecker
product completes the proof.
(cid:4)

V. APPENDIX : PROOFS

Proof of Theorem 1. In view of Lemma 1, we have

E(cid:2)ketk2(cid:3) ≤ kQk2
E(cid:2)ket−1k2(cid:3)
Xi=1

+ α2

n

kHik2Wi + nkθt−1 − θtk2

+ 2[1n ⊗ (θt−1 − θt)]⊤QE[et−1].

(14)

since E[wi,t−1] = 0. Using the simple fact that for any
β > 0

2ab ≤

1
β

a2 + βb2,

based on AM-GM inequality, we apply the Cauchy-Schwarz
inequality to bound

2[1 ⊗ (θt−1 − θt)]⊤QE[et−1] ≤

1
β

k1n ⊗ (θt−1 − θt)k2 + βkQk2E[ket−1k]2,

for any β > 0, and simplify (14) to

E(cid:2)ketk2(cid:3) ≤ (1 + β)kQk2

n

E(cid:2)ket−1k2(cid:3)

β + 1

kHik2Wi +

+ α2

nkθt−1 − θtk2,

β

Xi=1

Proof of Lemma 2. Note equations (1) and (3) to observe

n

n

which entails

and

ℓt(θt) =

n

Wi,

Xi=1
Xi=1
E(cid:2)kwi,tk2(cid:3) =
E(cid:20)(cid:13)(cid:13)(cid:13)
2(cid:21)
yi,t − Hiˆθj,t(cid:13)(cid:13)(cid:13)
2(cid:21)
E(cid:20)(cid:13)(cid:13)(cid:13)
Hiθt + wi,t − Hiˆθj,t(cid:13)(cid:13)(cid:13)
EhkHiej,t − wi,tk2i ,

n

Xi=1
Xi=1
Xi=1

n

(12)

(cid:16)1 − (1 + β)kQk2(cid:17)E(cid:2)ketk2(cid:3)

≤ (1 + β)kQk2(cid:16)E(cid:2)ket−1k2(cid:3) − E(cid:2)ketk2(cid:3)(cid:17)

β + 1

n

+ α2

kHik2Wi +

nkθt−1 − θtk2. (15)

β

Xi=1

Using the convention E(cid:2)ke0k2(cid:3) = 0, the following sum

telescopes

T

Xt=1

E(cid:2)ket−1k2(cid:3) − E(cid:2)ketk2(cid:3) ≤ E(cid:2)ke0k2(cid:3) = 0,

ℓt(ˆθj,t) =

=

=

[18] D. Acemoglu, A. Nedi´c, and A. Ozdaglar, “Convergence of rule-of-
thumb learning rules in social networks,” in IEEE Conference on
Decision and Control (CDC), 2008, pp. 1714–1720.

[19] U. Khan, S. Kar, A. Jadbabaie, J. M. Moura et al., “On connectivity,
observability, and stability in distributed estimation,” in IEEE Confer-
ence on Decision and Control (CDC), 2010, pp. 6639–6644.

[20] S. Das and J. M. Moura, “Distributed state estimation in multi-agent
networks,” in IEEE International Conference on Acoustics, Speech and
Signal Processing (ICASSP), 2013, pp. 4246–4250.

[21] S. Shahrampour, S. Rakhlin, and A. Jadbabaie, “Online learning
of dynamic parameters in social networks,” in Advances in Neural
Information Processing Systems, 2013.

[22] A. De, S. Bhattacharya, P. Bhattacharya, N. Ganguly,

and
S. Chakrabarti, “Learning a linear inﬂuence model from transient
opinion dynamics,” in Proceedings of the 23rd ACM International
Conference on Information and Knowledge Management.
ACM,
2014, pp. 401–410.

[23] N. Atanasov, R. Tron, V. M. Preciado, and G. J. Pappas, “Joint
estimation and localization in sensor networks,” in IEEE Conference
on Decision and Control (CDC), 2014, pp. 6875–6882.

[24] M. O. Sayin, N. D. Vanli, I. Delibalta, and S. S. Kozat, “Optimal and
efﬁcient distributed online learning for big data,” in IEEE International
Congress on Big Data (BigData Congress), 2015, pp. 126–133.

[25] A. Simonetto, A. Mokhtari, A. Koppel, G. Leus, and A. Ribeiro,
“A decentralized prediction-correction method for networked time-
varying convex optimization,” in Proceedings of the 6th IEEE Interna-
tional Workshop on Computational Advances in Multi-Sensor Adaptive
Processing,(Cancun, Mexico), 2015.

[26] R. van Oosten, “Learning from neighbors in a changing world,” 2016.
[27] J. S. Rosenthal, “Convergence rates for markov chains,” SIAM Review,

vol. 37, no. 3, pp. 387–405, 1995.

[28] S. Kar, J. M. Moura, and K. Ramanan, “Distributed parameter estima-
tion in sensor networks: Nonlinear observation models and imperfect
communication,” IEEE Transactions on Information Theory, vol. 58,
no. 6, pp. 3575–3605, 2012.

and summing (15) over t ∈ [T ], we derive

T

Xt=1

E(cid:2)ketk2(cid:3) ≤

α2T Pn

i=1 kHik2Wi

1 − (1 + β)kQk2

+

β + 1

β

nPT

t=1 kθt−1 − θtk2

1 − (1 + β)kQk2

.

Recall that 0 < β < kQk−2 − 1 is a free parameter in the
analysis. Letting β = kQk−1 − 1, and recalling Deﬁnition 6,
we calculate the bound above to get

T

Xt=1

E(cid:2)ketk2(cid:3) ≤

α2T Pn

i=1 kHik2Wi
1 − kQk

+

nCT

(1 − kQk)2 .

Appealing to Lemma 2 completes the proof.

(cid:4)

REFERENCES

[1] J. N. Tsitsiklis, “Decentralized detection by a large number of sensors,”
Mathematics of Control, Signals and Systems, vol. 1, no. 2, pp. 167–
182, 1988.

[2] S. S. Stankovi´c, M. S. Stankovi´c, and D. M. Stipanovi´c, “Decentralized
parameter estimation by consensus based stochastic approximation,”
IEEE Transactions on Automatic Control, vol. 56, no. 3, pp. 531–543,
2011.

[3] K. Drakopoulos, A. Ozdaglar, and J. N. Tsitsiklis, “On learning with
ﬁnite memory,” IEEE Transactions on Information Theory, vol. 59,
no. 10, pp. 6859–6872, 2013.

[4] S. Shahrampour, A. Rakhlin, and A. Jadbabaie, “Distributed detection
: Finite-time analysis and impact of network topology,” IEEE Trans-
actions on Automatic Control, vol. 61, 2016.

[5] F. Bullo, J. Cort´es, and S. Martinez, Distributed Control of Robotic
Networks: A Mathematical Approach to Motion Coordination Algo-
rithms: A Mathematical Approach to Motion Coordination Algorithms.
Princeton University Press, 2009.

[6] D. Acemoglu, M. A. Dahleh, I. Lobel, and A. Ozdaglar, “Bayesian
learning in social networks,” The Review of Economic Studies, vol. 78,
no. 4, pp. 1201–1236, 2011.

[7] A. Jadbabaie, J. Lin, and A. S. Morse, “Coordination of groups
of mobile autonomous agents using nearest neighbor rules,” IEEE
Transactions on Automatic Control, vol. 48, no. 6, pp. 988–1001, 2003.
[8] R. Olfati-Saber and R. M. Murray, “Consensus problems in networks
of agents with switching topology and time-delays,” IEEE Transac-
tions on Automatic Control, vol. 49, no. 9, pp. 1520–1533, 2004.

[9] N. Cesa-Bianchi and G. Lugosi, Prediction, learning, and games.

Cambridge University Press, 2006.

[10] M. Zinkevich, “Online convex programming and generalized in-
ﬁnitesimal gradient ascent,” in International Conference on Machine
Learning, 2003.

[11] E. Hall and R. Willett, “Online convex optimization in dynamic
environments,” IEEE Journal of Selected Topics in Signal Processing,
vol. 9, no. 4, pp. 647–662, June 2015.

[12] A. Jadbabaie, A. Rakhlin, S. Shahrampour, and K. Sridharan, “Online
optimization: Competing with dynamic comparators,” in Proceedings
of the Eighteenth International Conference on Artiﬁcial Intelligence
and Statistics, 2015, pp. 398–406.

[13] R. E. Kalman, “A new approach to linear ﬁltering and prediction
problems,” Journal of Fluids Engineering, vol. 82, no. 1, pp. 35–45,
1960.

[14] R. Olfati-Saber, “Distributed kalman ﬁlter with embedded consensus
ﬁlters,” in IEEE Conference on Decision and Control (CDC), Euro-
pean Control Conference (ECC), 2005, pp. 8179–8184.

[15] ——, “Distributed kalman ﬁltering for sensor networks,” in IEEE

Conference on Decision and Control (CDC), 2007, pp. 5492–5498.

[16] R. Carli, A. Chiuso, L. Schenato, and S. Zampieri, “Distributed kalman
ﬁltering based on consensus strategies,” IEEE Journal on Selected
Areas in Communications, vol. 26, no. 4, pp. 622–633, 2008.

[17] R. Olfati-Saber, “Kalman-consensus ﬁlter: Optimality, stability, and
performance,” in IEEE Conference on Decision and Control (CDC)
held jointly with Chinese Control Conference (CCC), 2009, pp. 7036–
7042.

