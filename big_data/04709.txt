6
1
0
2

 
r
a

 

M
5
1

 
 
]
S
D
h
t
a
m

.

[
 
 

1
v
9
0
7
4
0

.

3
0
6
1
:
v
i
X
r
a

Understanding the geometry of transport:

diﬀusion maps for Lagrangian trajectory data unravel

coherent sets

Ralf Banisch∗

P´eter Koltai†

Abstract

Dynamical systems often exhibit the emergence of long-lived coherent sets, which
are regions in state space that keep their geometric integrity to a high extent and thus
play an important role in transport. In this article, we provide a method for extracting
coherent sets from possibly sparse Lagrangian trajectory data. Our method can be
seen as an extension of diﬀusion maps to trajectory space, and it allows us to construct
“dynamical coordinates” which reveal the intrinsic low-dimensional organization of the
data. The only a priori knowledge about the dynamics that we require is a locally valid
notion of distance, which renders our method highly suitable for automated data anal-
ysis. We show convergence of our method to the analytic transfer operator framework
of coherence in the inﬁnite data limit, and illustrate its potential on several two- and
three-dimensional examples as well as real world data.

1

Introduction

The term coherent sets, as used here, was coined in recent studies [27, 28, 33]. They are
understood to be sets (one at each time point), in the state space of a ﬂow governed by a
possibly non-autonomous (time-variant) system, which keep their geometric integrity to a
high extent, and allow little transport in and out of themselves. In particular, if we were
to place a large number of passive tracers in a coherent set at some initial time, then most
of these tracers would end up in the corresponding coherent set at some later time point.
Natural examples are moving vortices in atmospheric [49, 33], oceanographic [57, 13, 24],
and plasma ﬂows [47].

Dynamical systems techniques have been developed for the qualitative and quantitative
study of transport problems. Most of these are either geometric or transfer operator based

∗School

of Mathematics, University

of Edinburgh, Edinburgh EH9

3FD, UK. E-mail:

ralf.banisch@ed.ac.uk

†Institute of Mathematics, Freie Universit¨at Berlin, 14195 Berlin, Germany. E-mail: peter.koltai@fu-

berlin.de

1

(probabilistic) methods, but topological [2] and ergodicity-based [8] methods appeared re-
cently as well. Geometric approaches are mainly aiming at detecting transport barriers
(Lagrangian coherent structures), and include studying invariant manifolds, lobe dynam-
ics [48], ﬁnite-time material lines [35], geodesics [37] and surfaces [36]. The notions of shape
coherence [44] and ﬂux optimizing curves [5] are also of geometric nature. Transfer operator
based methods aim at detecting sets (i.e. full-dimensional objects in contrast to codimension
one transport barriers), and consider almost-invariant [14, 23] and coherent sets [27, 28, 33].
Eﬀorts have been made to compare geometric and probabilistic methods and understand the
connection between them [29, 30, 22, 1].

Increasing computational and storage capacities, just as improving measurement tech-
niques supply us with large amounts of data. Even if a tractable computational model is
not available, analysis of this data can reveal much of the desired properties of the system
at hand. Recently, diﬀerent approaches emerged that compute coherent sets and coherent
structures based on Lagrangian trajectory data, such as GPS coordinates from ocean drifters:
Ser-Giacomi et al [54] use graph theoretical tools to perform a geometric analysis of trans-
port, the works [32, 34] introduce dynamical distances and clustering to extract coherent
sets as tight bundles of trajectories in space-time, while Williams et al [59] use a meshfree
collocation-based approach for a transfer operator based classiﬁcation.

Here, we introduce a method based on Lagrangian trajectory data, which (i) uses only
local distances between the data points, and which (ii) can be shown to “converge” to the
analytical transfer operator based framework of Froyland [21] in the inﬁnite-data limit; hence
it can be viewed as a natural extension of the functional analytic framework to the sparse
data case. Moreover, our approach provides dynamical coordinates which shed light on the
connectivity of coherent sets, and reveal how transport is occurring. One key ingredient
here is to use diﬀusion maps [12, 45, 40], which were successfully applied to extract intrinsic
geometrical properties from high-dimensional data. The basic idea there is to introduce a
diﬀusion operator on the data points, whose eigenvectors will give a good low-dimensional
parametrization of the data set, if this is possible.

This paper is organized as follows. In section 2 we introduce the analytic transfer operator
based framework of coherent sets. In section 3, we ﬁrst review the construction of diﬀusion
maps. This is followed by our main result: The extension of diﬀusion maps to trajectory
data and Theorem 4, which shows that our method coincides with the analytic transfer
operator approach in the rich data limit. We also discuss algorithmic aspects and show how
to extract coherent sets. In [22] Froyland draws a connection between the analytic transfer
operator approach and geometric properties of coherent sets.
In section 4 we seek direct
connections between our data-based framework and this geometry-oriented construction.
Finally, section 5 demonstrates our method for diverse numerical examples.

In this paper, we denote sets by double-stroke symbols (e.g. A), matrices whose size
is compatible with the data by upper case bold face symbols (e.g. P), and operators on
(weighted) L2-spaces by calligraphic symbols (e.g. P). (cid:107) · (cid:107) is always the Euclidean norm
on Rn, for some n ∈ N, unless stated otherwise.

2

Figure 1: Coherent pairs are robust under small perturbations. Top left: two sets at initial
time. Bottom left: the image of these two sets under the dynamics. Bottom right: the images
of 100 random test points (taken in the respective sets at initial time) under the dynamics,
perturbed by a small additive random noise. Top right: preimages of the perturbed image
points under the dynamics, which reveal that the black set and its image under the dynamics
form a coherent pair, whereas the grey set and its image do not.

2 The analytic framework for coherent sets

In this section we quantitatively formalize what we mean by a coherent set. To this end we
review Froyand’s analytic framework [21] for coherent pairs. In particular, we make a small
simplifying modiﬁcation to this, which we will comment on in (d) below.
Let a map Φ : X → Y be given, describing the evolution of states under the dynamics
from some initial to some ﬁnal time. We assume X, Y ⊂ Rd, d ∈ N, to be bounded sets.
Consider a pair of sets, A ⊂ X at initial and B ⊂ Y at ﬁnal time. In order for A and B to
form a coherent pair, we must have ΦA ≈ B. This is however not enough, as Figure 1 readily
suggests: If we are to distinguish between sets that keep their geometric integrity under the
dynamics to a high degree and sets which do not, then we additionally need a robustness
property under small perturbations.
Let Φε : X → Y denote a small random perturbation of Φ. The meaning of this is made
precise below, for now, one may think of Φεx as Φx plus some zero-mean noise with variance ε,
B ≈ A” in a
where ε is small. Now, a coherent pair has to satisfy “ΦεA ≈ B”, and “Φ−1
(ΦεA) ≈ A”.
suitable sense. In particular, A ⊂ X can be part of a coherent pair only if “Φ−1
Note that exactly this is depicted in Figure 1, if we start at the top left image and proceed
counterclockwise: applying forward dynamics, then diﬀusion, then the backward dynamics

ε

ε

3

In formalizing the expression Φ−1

to the points of a coherent set, most of these points should return to the set.
(ΦεA) ≈ A, randomness plays an important role. We
deﬁne a non-deterministic dynamical system Ψ : X → Y by its transition density function
k ∈ L2(X × Y, µ × (cid:96)). Here L2 denotes the usual space of square-integrable functions in the
Lebesgue sense, µ is a probability measure (some reference measure of interest), and (cid:96) is the
Lebesgue measure. We have for the probability that Ψx ∈ S for some Lebesgue-measurable
set S, that

ε

P[Ψx ∈ S] =

In particular, (i) k ≥ 0 almost everywhere; and (ii) (cid:82) k(·, y)dy = 1, the constant one
is a random variable with density h with respect to (cid:96), then k(x, y) = h(cid:0)ε−1/2 (Φx − y)(cid:1).

function.1 From (2.1) we can compute that if Ψx = Φεx := Φx +

We introduce the forward operator F : L2(X, µ) → L2(Y, (cid:96)) associated with Ψ,

εη, where ε > 0, and η

(cid:90)

S

k(x, y) dy .

(2.1)

√

(cid:90)

Ff (y) =

k(x, y)f (x)dµ(x) .

The operator F describes how an ensemble of states which has distribution f (x)dµ(x) is
mapped by the dynamics; i.e. Ff is the distribution (given as a density with respect to (cid:96))
of the ensemble after it has been mapped state-by-state by Ψ.
It is not necessary for the initial distribution to be stationary, thus we normalize our
transfer operator.2 Let qν := F1 be the image density of the initial distribution, deﬁning
a measure ν through dν(x) = qν(x)dx. The normalized forward operator T : L2(X, µ) →
L2(Y, ν) is then deﬁned as

T f = (F1)−1Ff =

f (x)dµ(x) ,

(2.2)

(cid:90) k(x,·)
(cid:90)

qν(·)

(cid:90) k(·, y)

qν(y)

and its adjoint T ∗ : L2(Y, ν) → L2(X, µ) turns out to be

T ∗g =

g(y) dν(y) =

k(·, y)g(y) dy ,

(2.3)

i.e. (cid:104)T f, g(cid:105)ν = (cid:104)f,T ∗g(cid:105)µ for every f ∈ L2(X, µ), g ∈ L2(Y, ν), where (cid:104)·,·(cid:105)µ and (cid:104)·,·(cid:105)ν are
the usual inner products in the respective spaces. Note that T 1 = 1, which just encodes
the fact that the initial reference distribution µ is mapped onto the ﬁnal distribution ν by
the dynamics. Now, if T is associated with Φε, then A and B being a coherent pair reads
as T 1A ≈ 1B. Note that this approximation can be made quantitative, since
µ(A)(cid:104)T 1A, 1B(cid:105)ν
is the probability that a µ-distributed initial state from A gets mapped by Ψ into B. Fur-
thermore, T ∗ is the forward operator of the time-reversed dynamics (see Appendix A.1 for
a short proof).

1

1If the range of an integral is not speciﬁed, then it is meant to be the whole domain of the integrand.
2We call every operator, transporting some object (a distribution, or an observable) by the dynamics, a

transfer operator.

4

Froyland [21] extracts coherent pairs from the left and right singular vectors of T for dom-
inant singular values. Right singular vectors of T are eigenvectors of T ∗T . Moreover, T ∗T
is the transfer operator of the “forward-backward system”. Here, the “backward system” de-
notes the time-reversed forward system, and the forward system is described by the forward
operator T . Coherent sets are those sets which are hard to exit under the forward-backward
µ(A), 1A(cid:105)µ ≈ 1, which is the probability that the forward-backward sys-
dynamics, i.e. (cid:104)T ∗T 1A
tem ends up in A, provided it started there.3 Thus, the method described in [21], is a kind
of a spectral clustering [58, 41] of the forward-backward system.

A few remarks are in order:

(a) We see from (2.2) and (2.3) that

T ∗T f (z) =

f (x)

(cid:90)

(cid:90) k(z, y)k(x, y)
(cid:124)

(cid:123)(cid:122)

qν(y)

dy

(cid:125)

(cid:90)

dµ(x) =

f (x)κ(x, z) dµ(x) .

(2.4)

The kernel κ is trivially symmetric, but also doubly stochastic: (cid:82) κ(x,·) dµ(x) =
(cid:82) κ(·, z) dµ(z) = 1. Symmetry of κ implies that the forward-backward process is re-

=:κ(x,z)

versible with respect to µ.

function, where δ is the Dirac distribution, satisfying δ(u) = 0 for u (cid:54)= 0, and(cid:82) δ(u)du =

(b) If Ψ = Φ is the deterministic dynamics, the forward operators F and T are often called
In this case we denote the normalized forward
the Perron–Frobenius operator [42].
operator by P. Note that here the kernel k(x, y) = δ(Φx − y) is only formally an L2
1. By (2.3) we have P∗g(y) = g(Φy), which is called the Koopman operator. We will
denote the formal Koopman operator by U , given by U g(x) = g(Φx), to decouple its
deﬁnition from the function spaces in consideration. However, it always holds true that
if U is considered as an operator from L2(Y, ν) to L2(X, µ), where µ, ν are arbitrary
measures such that U is well-deﬁned, then the adjoint of the Koopman operator, U∗,
is the Perron–Frobenius operator P describing the dynamical transport of µ-densities
to ν-densities. That is,

(cid:104)Pf, g(cid:105)ν = (cid:104)f, U g(cid:105)µ ∀f ∈ L2(X, µ), g ∈ L2(Y, ν) .

(2.5)

(c) Our standard case is going to be Ψ = Φε = Φ +

εη, where η is a standard normally

distributed4 random variable. This implies

√

exp(cid:0)−ε−1(cid:107)Φx − y(cid:107)2(cid:1) =:

k(x, y) =

1
Zε

1
Zε

kε(Φx, y) ,

3This statement is a quantitative version of “Φ−1
4The distribution of η is cut oﬀ at some speciﬁc distance from the mean, such that we can work on
bounded sets. This might necessitate the enlargement of Y, which we tacitly assume has been done, and
denote the result by Y again.

ε

(ΦεA) ≈ A”.

5

with Zε being a normalizing constant, independent on x, and (cid:107) · (cid:107) being the Euclidean
norm on Rd. The noisy dynamics Ψ is given by two steps (ﬁrst apply Φ, then add noise),
hence the associated forward operator is a concatenation of the forward operators of the
two components: T = DεP. Here, P : L2(X, µ) → L2(Y, νΦ) is the normalized Perron–
Frobenius operator, where νΦ is the image of the distribution µ under Φ. The diﬀusion
operator Dε : L2(Y, νΦ) → L2(Y, ν) is the normalized forward operator of the noise,
where ν is the image of νΦ under the noise. If we denote qν the density of ν with respect
to (cid:96), i.e. dν(x) = qν(x)dx, then
Dεf (x) =

kε(x, y)f (y)dνΦ(y) .

(cid:90)

1

Zεqν(x)

The evaluation chain can now be represented as
P−→ L2(Y, νΦ)

T : L2(X, µ)

Dε−→ L2(Y, ν) .

For later reference, we also deﬁne the formal diﬀusion operator Dε (i.e. without the
spaces it acts on) by

(cid:90)

1
Zε

Dεf (x) =

kε(x, y)f (y) dy ,

and note that formally, due to the symmetry of the kernel kε, we have (cid:104)Dεf, g(cid:105)ν =
(cid:104)f, Dεg(cid:105)νΦ. Hence, viewed as an operator between the right spaces, Dε = D∗
ε.

(d) In Froyland’s construction [21], T is the forward operator associated with a process
where a small diﬀusion is applied both before and after the deterministic dynamics takes
place.5 This assures that both the sets A and B are geometrically nice (cf. Figure 1).
This is, however, not necessary. In fact, if one would like to have coherence at several
time instances, it is natural to average the operators T ∗T for all the diﬀerent time
instances, and compute the dominant eigenfunctions of the resulting operator [31, 22].
To make this precise, let for arbitrary time instances s ≤ t, the forward operator Ts,t
correspond to the deterministic dynamics from s to t, plus random noise scaled by a
parameter ε (as above). Given a set of time instances, IT := {t0, . . . , tT−1}, at which
we would like to ﬁnd coherent sets (now tuples, instead of pairs), one can consider the
dominant eigenfunctions of

(2.6)

(cid:88)

t∈IT

1
T

T ∗
t0,tTt0,t .

Note, that all summands on the left-hand side of (2.6) are self-mappings of L2(X, µ),
hence we can sum them up. If we include the initial time instance into this averaging
(as we do in (2.6)), we automatically account for the geometrical smoothness of the sets
at that time too. Since we will adopt this construction in the current work, it suﬃces to
take T as the forward operator associated with the deterministic forward dynamics plus
5In our setting, this would mean Φεx = Φ(x +

εη2, where η1, η2 are independent random

εη1) +

√

√

variables.

6

some small diﬀusion (at ﬁnal time). If the ﬁnal and initial times are equal, T describes
only diﬀusion, cf. (2.7) below.

(e) So far, the choice of the small random perturbation (diﬀusion) which we apply to the
dynamics was arbitrary. In practice, choosing the size of this perturbation does not have
to be obvious. In [22], Froyland hence developed an “ε-free” version of the notion of
coherent pairs. In fact, he derives a ﬁrst order perturbation expansion for ε → 0 for the
construction from (d).
We summarize this for coherent pairs, i.e. where only two time instances are involved,
say s and t, s < t. Note that then Ts,s = DεPs,s = Dε corresponds to only diﬀusion,
since Ps,s = Id. Froyland shows that if the deterministic dynamics is volume-preserving
for every time, and the noise has zero mean and covariance equal to the identity matrix,
then6

T ∗
s,sTs,s + T ∗

s,tTs,t = Id +

(∆ + P∗∆P) + o(ε) ,

(2.7)
where Id is the identity and ∆ denotes the Laplace operator on X. He calls (∆ +P∗∆P)
the dynamic Laplacian, and its eigenfunctions yield coherent pairs. We give a data-based
version of (2.7) at the end of section 3.2. In section 4 we further elaborate on data-based
approximations of the dynamic Laplacian.

ε
2

3 Diﬀusion in trajectory space

3.1 Diﬀusion maps

To set the stage, we give a brief review of the method of diﬀusion maps. For details and the
proofs of the statements presented in this section, we refer to [12] and the references therein.
The goal of diﬀusion maps is to learn global geometric information from point-cloud data
by imposing local geometric information only. Suppose that we have m data points xi ∈
Rn which are i.i.d. realizations of random variables distributed according to an unknown
density q on a likewise unknown Riemannian submanifold M ⊂ Rn of dimension dim M = d.
The assumption made by diﬀusion maps is that the Euclidean distance in Rn is a good local
approximation for distances in M. Now, a Markov chain is constructed on the data points
by the following procedure: Fix a rotation-invariant kernel7

(cid:18)(cid:107)xi − xj(cid:107)2

(cid:19)

(3.1)
Here, ε > 0 is a scale parameter and we will always choose h(x) = cr exp(−x)1x≤r with some

cutoﬀ radius r and the constant cr chosen such that (cid:82) h((cid:107)x(cid:107)2)dx = 1. We will comment

kε(xi, xj) = h

ε

.

6Equation (2.7) follows from Froyland’s result if we take two diﬀerences into account: (i) we take ε as
the variance of the noise, and not the standard deviation, as he does, and (ii) we apply noise only after the
dynamic evolution, and not before and after.

7Due to the apparent connection between the kernels kε from section 2 (c) and from (3.1), we abuse

notation by denoting these objects by the same symbol. From now on, this latter deﬁnition applies.

7

second moment(cid:82) h((cid:107)x(cid:107))x2

more on the choice of ε and r below, but in practice we choose r large enough such that the

1dx ≈ 1/2 up to reasonable precision.8 Now, we let

m(cid:88)

j=1

kε(xi) =

kε(xi, xj)

kε(xi, xj)

kε(xi)αkε(xj)α

m(cid:88)

j=1

lim
m→∞

1
m

(cid:90)

M

m(cid:88)

j=1

and form the new kernel

ε (xi, xj) =
k(α)

for some α ∈ [0, 1]. Finally, the transition matrix Pε,α of the Markov chain is constructed
by row-normalising k(α)

:

ε

Pε,α(i, j) :=

k(α)
ε (xi, xj)
d(α)
ε (xi)

,

ε (xi) =
d(α)

ε (xi, xj).
k(α)

(3.2)

In the limit m → ∞ of inﬁnite data, the strong law of large numbers ensures that all discrete
sums converge almost surely to integrals over q. In particular, for f : M → R,

m(cid:88)

j=1

kε(x, xj)f (xj) =

kε(x, y)f (y)q(y)dy .

(3.3)

The matrix Pε,α only exists on the data points, but using the kernel function h it is straight-
ε (x) to all x ∈ Rd. We now deﬁne
forward to extend the kernel pε,α(x, xj) = k(α)
the operator Pε,α by

ε (x, xj)/d(α)

Pε,αf (x) := lim
m→∞

pε,α(x, xj)f (xj)

(3.4)

and let Lε,α = ε−1 (Pε,α − Id) be the corresponding generator. The error in (3.4) for ﬁnite m
is of order O(ε−d/4m−1/2) [38, 56]. In [12] Coifman et al showed that9

lim
ε→0

Lε,αf =

4q1−α − ∆(q1−α)
∆(f q1−α)
4q1−α f.

(3.5)

In particular, for α = 1 one has

lim
ε→0

Lε,1f =

∆f ,

1
4

where ∆ = div ◦ grad is the (negative semi-deﬁnite) Laplace-Beltrami operator on M . In
other words, the random walk generated by Pε,1 on the data points converges to Brownian
motion on M as m → ∞ and ε → 0. The method now proceeds by analysing the dominant
spectrum of Pε,1. The number Λ of leading non-trivial eigenvalues is an estimator of the

8This is in order to have an explicit factor 1

4 in (3.5). For our work here, the exact value of this factor

is irrelevant; we assume in our theoretical considerations that the second moment of h is 1
2 .

9Note that they choose h to have second moment equal to 2, we chose it to have 1
2 .

8

dimension of M, and the corresponding eigenfunctions ξi, which converge in probability to
those of ∆ for m → ∞ and ε → 0, are good global intrinsic coordinates on M. The ξi are
the so-called diﬀusion maps since they provide a map xi (cid:55)→ (ξ1(xi), . . . , ξΛ(xi)) from M to
the embedding space E = span{ξ1, . . . , ξΛ} [12]. See Figure 2 for an example.

We are going to need the following ﬁnite ε version of the result (3.5):

Lemma 1: Let us recast the diﬀusion operators from section 2 (c) to our current setting:

(cid:90)

Dεf (x) =

1
εd/2

M

kε(x, y)f (y)dy,

Dεf (x) =

Then

Pε,αf =

Dεq1−α + O(cid:0)ε2(cid:1) .

Dε (q1−αf )

Dε(qf )

Dεf

.

(3.6)

(3.7)

In particular, Pε,1f = Dεf + O (ε2) and Pε,0f = Dεf (this last equation is exact).
Proof. See Appendix A.2.

Figure 2: Diﬀusion map example. Left: m = 10.000 data points sampled from a rectangular
strip embedded in R3. The sampling density is non-uniform and decreases with distance
from the origin. Color according to ξ1. Middle: Diﬀusion map embedding for α = 0. Right:
Diﬀusion map embedding for α = 1.

Forward-backward diﬀusion maps.
In order to make contact with the forward-backward
dynamics developed in section 2, we will need the following forward-backward version of dif-
fusion maps:
i=1 on M ⊂ Rn and Pε,0 as in (3.2), let us
Lemma 2: For q-distributed data points {xi}m
deﬁne the forward-backward diﬀusion matrix10

i,j=1 =(cid:0)diag(PT

ε,01)(cid:1)−1

Bε = (bε(xi, xj))m

PT

ε,0Pε,0

(3.8)

10Here diag(v) denotes the diagonal matrix with entries given by vector v on the diagonal.

9

with corresponding (m-dependent) kernel

m(cid:88)

1

kε(x, xi)kε(xi, y)

bε(x, y) =

dε(x)

i=1

kε(xi)2

m(cid:88)

i=1

kε(x, xi)
kε(xi)

.

,

dε(x) :=

We further deﬁne the operator Bε : L2(Rn, µ) → L2(Rn, µ), where dµ(x) = q(x)dx, by

m(cid:88)

(3.9)

(3.10)

b∞
ε (x, y)q(y)f (y)dy ,

(3.11)

10

Bεf (x) := lim
m→∞

bε(x, xj)f (xj)

ε. Let f be bounded, let Bεf (xi) :=(cid:80)

j=1

and denote its adjoint in L2(Rn, µ) by B∗
and Bεf := (Bεf (x1), . . . , Bεf (xm)). Then we have the following properties:
(i) Bε1 = 1 and Bε1 = 1.
(ii) |Bεf (xi) − Bεf (xi)| = O(ε−d/4m−1/2).
(iii) For any x ∈ X, Bεf (x) = DεDεf (x)+O(ε2). As a consequence, Bε is almost self-adjoint:

j B(i, j)f (xj),

|Bεf (x) − B∗

ε f (x)| = O(ε2).

(iv) Bε is almost symmetric: (cid:107)Bε − BT

ε (cid:107) ≤ O(ε2) + O(ε−d/4m−1/2) for any compatible

matrix norm (cid:107) · (cid:107).

(v) If Dεq = q, then Bεf (x) = DεDεf (x) holds for all x ∈ X.
(vi) If, additionally, f ∈ C 3(M), then limε→0
ε (Bεf − f ) = 1

1

2q−1∇ · (q∇f ).

Proof. See Appendix A.3.

Property (vi) shows that in the small ε limit, Bεf approximates the action of the q-
weighted Laplace-Beltrami operator on M [38]. Note the following two special cases of
property (vi):
(a) If q ≡ const is the uniform distribution, then limε→0
2∆f .
(b) If q = e−V with some potential energy function V : X → R, then limε→0

ε (Bεf − f ) =
2 (∆f − ∇V · ∇f ). Up to a factor 1
2, this is the inﬁnitesimal generator of the diﬀusion
process given by the stochastic diﬀerential equation dxt = −∇V (xt)dt+dwt, where wt is
a standard Wiener process (Brownian motion). Note that q is the invariant distribution
of this process.

ε (Bεf − f ) = 1

1

1

1

Remark 3 (Limiting kernel): Note that (3.10) can be written as

(cid:90)

Bεf (x) =

with the m-independent limiting kernel

b∞
ε (x, y) =

1
d∞
ε (x)

1
εd

where we introduced the shorthands

qε(x) := Dεq(x),

(cid:90) kε(x, z)kε(z, y)
(cid:90) kε(x, y)

q2
ε (z)

d∞
ε (x) =

1
εd/2

qε(y)

q(z)dz,

(3.12)

q(y)dy.

(3.13)

ε as the best way to represent 1. If qε = q holds, then d∞

One can think of qε as the best way to represent q with the kernel functions kε(x, y), and
of d∞
ε (x, y) reduces to a
symmetric, doubly stochastic kernel quite similar to κ; cf. (2.4). Intuitively, this observation
will allow us to connect our diﬀusion maps construction below to the analytic framework of
coherence, introduced in section 2.

ε = 1, and b∞

3.2 Space-time Diﬀusion maps

General setting.
In this section, we combine the geometric ideas of diﬀusion maps with
dynamics. Let Φs,t : Xs → Xt for s, t ∈ R be the unknown, possibly non-autonomous ﬂow
map from Xs ⊂ Rd to Xt ⊂ Rd, s, t ∈ R. The data set we have at our disposal consists
of m trajectories evaluated at T ∈ N time slices It = {t0, . . . , tT−1}. To simplify notation,
we set Φt := Φt0,t. That is, we have access to the data set11,12

X = {xi

t := Φtxi : i = 1, . . . , m; t ∈ It}

with initial points xi ∈ X that we assume to be i.i.d. realizations of random variables dis-
tributed according to the distribution q0. We call qt the distribution of the points xi
t at
time t. At every timeslice t ∈ It, we can construct diﬀusion map matrices Pε,α,t and a
forward-backward diﬀusion matrix Bε,t via (3.8) by using the m data points {Φtxi}m
i=1.
Then Bε,t(i, j) = bε,t(Φtxi, Φtxj), where

1

kε(x, Φtxi)kε(Φtxi, y)

dε,t(x)

i=1

kε,t(Φtxi)2

,

dε,t(x) :=

kε(x, Φtxi)
kε,t(Φtxi)

,

(3.14)

by specifying the following Spacetime Diﬀusion Map transition matrix Qε ∈ Rm×m:

j kε(Φtxi, Φtxj). Now we construct a Markov chain on the trajectories

(cid:88)

t∈It

(cid:88)

t∈It

Qε(i, j) =

1
T

Bε,t(i, j) =

1
T

bε,t(Φtxi, Φtxj) .

(3.15)

11We assume access to the full state xi

only information about yi
this situation in a future publication.

t = Oxi

t = Φtxi of the dynamical system. Another typical situation is that
t is available, for some observation operator O : X → O. We will address

12In section 5, we also address the case of missing data.

11

m(cid:88)

bε,t(x, y) =

and kε,t(Φtxi) :=(cid:80)

m(cid:88)

i=1

We will show in Theorem 4 below, that (3.15) is a data-based version of the time-averaged
forward-backward transfer operators from (2.6). The transition matrix (3.15) describes
jumps between trajectories in the following manner: Starting at trajectory i, ﬁrst one of
the timeslices It is selected uniformly at random. Then, the forward-backward diﬀusion map
transition matrix (3.8) at the selected timeslice is used to jump to a new trajectory j.

Connection to coherence. The connection between the transition probabilities pre-
scribed by (3.15) and the notion of coherence is now intuitively clear: Coherent sets are
tight bundles of trajectories. That is, if there is a subset Ib = {i1, . . . , ib} of trajectories
such that (cid:107)Φtxi − Φtxj(cid:107) is small for all i, j ∈ Ib and all t ∈ It, we would like to see these
trajectories as a part of a coherent set. For such a tight bundle of trajectories, all transi-
tion probabilities assigned by Qε between i, j ∈ Ib will be large, and we should be able to
identify Ib by clustering Qε.

Our main result is the following theorem, which links the transition matrix Qε with the

analytical coherence framework.
Theorem 4: With Qε as in (3.15), we have for ﬁxed ε > 0

m(cid:88)

j=1

(cid:88)

t∈It

lim
m→∞

Qε(i, j)f (xj) =

1
T

t D∗
P∗

ε,tDε,tPtf (xi) + O(ε2),

(3.16)

where Dε,t is the diﬀusion operator Dε from (3.6) with respect to the distribution q = qt.
Convergence in (3.16) is a.s. as m → ∞. The pointwise error for ﬁnite m is O(ε−d/4m−1/2).

Proof. See Appendix A.4 for the full proof. The idea, though, can be sketched with Figure 3
as follows.

Let us think of the values of the function f in the data points xi as statistical weights. The
collection of these weighted point measures approximates the distribution f q0. Fixing a time
slice t, if we assign the weight f (xi) to the points xi
t, respectively, they will approximate the
distribution (Ptf )qt. Application of the matrix Bε,t to this latter data vector redistributes
the statistical weights like diﬀusion. Pulling back the new statistical weights to the data
t is assigned to xi) approximates
points at initial time (the new weight of each data point xi
the application of P∗
t .

Theorem 4 says that in the data-rich limit we are approximating the very analytical
object that was designed to identify coherent pairs (tuples).
In particular, the dominant
eigenfunctions {Ξ1, . . . , ΞΛ} of Qε approximate those of the operator 1
ε,tDε,tPt,
and have thus a signiﬁcance for the dynamical system Φt which is similar to the signiﬁcance
of the diﬀusion map eigenfunctions {ξ1, . . . , ξΛ} of Pε,α for the purely geometrical problem.
In fact {Ξ1, . . . , ΞΛ} encode both dynamical and geometrical properties, and for this reason
we call them spacetime diﬀusion maps.

P∗
t D∗

t∈It

T

(cid:80)

A few comments are in order:

(a) Although the operator T ∗T is symmetric (self-adjoint) and stochastic, Qε is merely
stochastic by construction. It is, however, by Lemma 2 (iv), O(ε2) close to a symmetric

12

Figure 3: To implement “forward-diﬀuse-backward”, we start at xj at time t0, then push
the statistical weight of this data point, f (xj), along the jth trajectory (shown in blue) to
the data point xj
t , then use Bε,t to redistribute the weights between the data points (this is
diﬀusion, shown in red) and ﬁnally transport the new weights along the ith trajectory back
to initial time, to arrive at xi.

matrix, and this estimate is getting better as m → ∞, cf. (3.16). In all our numerical
studies, the dominant spectrum of Qε was real-valued.

(b) The break of symmetry of Qε comes from the row-normalization by dε,t in (3.14). As
discussed in Remark 3, dε,t is our best approximation of the constant one function, hence
the more data is available, the closer we get to this normalization not having any eﬀect.
If we would omit it, we would get a symmetric matrix Qε, which is then only almost
stochastic.

(c) Formally, we can apply our method, and construct the space-time diﬀusion matrix Qε
also, if the trajectories are generated by non-deterministic dynamics. In this case, Φtx
is a random variable for any ﬁxed x, and the Koopman operator of this dynamics is
deﬁned by Utg(x) := E[f (Φtx)], where the expectation E[·] is taken with respect to
the law of Φtx. Nevertheless, coherent sets can still be extracted from the dominant
eigenmodes of T ∗
t Tt [33, 15], where Tt is the (normalized) forward operator associated
with the non-deterministic dynamics Φt. Note that due to the intrinsic noise of such
systems, no extra diﬀusion is needed between the forward and backward operators.
However, if we still introduce an instantaneous diﬀusion, which is a lot smaller than the
intrinsic noise, then T ∗
εDεTt will yield essentially the same coherent sets.
We study non-deterministic dynamics in a future publication.

t Tt and T ∗

t D∗

Remark 5 (A data-based dynamic Laplacian): Suppose that the dynamics Φ is a diﬀeo-
morphism13, e.g. the solution of an ODE. Then P∗
t Pt = Id,14 and using property (vi) of

13A diﬀeomorphism is a diﬀerentiable, everywhere invertible map, with a diﬀerentiable inverse.
14This is a consequence of [42, Corollary 3.2.1], by noting that the operator P therein plays the role of

our F.

13

xixjxjtxit t  1tB",tLemma 2 it can be readily seen that

m(cid:88)

j=1

lim
m→∞

Qε(i, j)f (xj) = Id +

ε
2

1
T

(cid:88)

t∈It

P∗
t ∆qtPtf (xi) + O(ε2)

(3.17)

with the q-weighted Laplacian ∆qf := q−1∇ · (q∇f ). In other words, the operator Lε =
ε−1(Qε − I), with I being the identity matrix matching the size of the data, is a data-based
t ∆qtPt, which can be seen as a generalization of the
P∗
approximation of the operator 1
2T
dynamic Laplacian introduced in [22] for multiple time slices and non-uniform densities q.

(cid:80)

t∈It

3.3 Clustering with Space-time diﬀusion maps
A consequence of Theorem 4 is that we may compute sets which are coherent for all times t ∈
It by searching for a subset Ib = {i1, . . . , ib} of trajectories which is metastable under Qε. This
reduces the problem of computing coherent sets, which involves both geometry and dynamics,
to the problem of clustering a graph. In this article, we use spectral clustering [58, 41] on Qε
to solve this problem, since

(i) spectral clustering can identify metastable sets [52, 16], and

(ii) eigenvectors of a diﬀusion maps transition matrix yield good coordinates to the intrinsic
geometry of the data set; cf Figure 2. We will see in section 5 that the eigenvectors
of Qε give natural “transport coordinates”.

Alternative clustering methods, e.g. to save computational time, are of course possible and
deserve further exploration, see for example [51]. Any spectral clustering algorithm proceeds
in the following three steps:

1. For some not too large N , compute the N largest eigenvalues λi of Qε. Identify Λ such

that λΛ − λΛ+1 is large (this is known as spectral gap).

2. Compute the Λ largest eigenfunctions Ξ1, . . . , ΞΛ.

3. Postprocessing: Extract Λ clusters C1, . . . , CΛ from15 Ξ2, . . . , ΞΛ.

The justiﬁcation for this approach is that a spectral gap after Λ dominant eigenvalues
indicates Λ metastable sets, and that the eigenfunctions Ξ2, . . . , ΞΛ are almost constant on
the metastable sets [52, 16]. A number of diﬀerent algorithms exist, depending on how the
postprocessing step is handled, and whether hard or soft clusters are being sought. For
example, the algorithms by Shi and Malik [55] and Ng et al [46] compute Λ hard clusters
which form a full partition of the state space V = {1, . . . , m} of Qε by performing k-means
on Ξ2, . . . , ΞΛ. We do the same in this paper16, mostly for reasons of simplicity and ease
of implementation. However, we note that enforcing a full partition into metastable sets
is often too strict. In some cases, it might be desirable to use fuzzy membership functions
instead. We refer the reader to [17, 6, 50] for more information.

15Ξ1 = 1 is the constant function.
16We normalize the Ξi such that (cid:107)Ξi(cid:107)2 = 1.

14

Remark 6 (Frame-independence): The eigenfunctions Ξ2, . . . , ΞΛ are functions of the Eu-
clidean distances (cid:107)·(cid:107) between Lagrangian observers17. As such, they are objective, i.e. inde-
pendent of the frame of reference.

3.4 Algorithmic aspects

We describe an algorithm for extracting coherent sets from data, which we assume to be
given as a d × m × T array of mT time-ordered data points in Rd. The algorithm has two
stages:

1. Compute Qε. The computational cost of this is dominated by the m2T distance
computations between the mT data points. In practice, for any given point xi only
the distances to points within the cutoﬀ radius r, that is, only distances that sat-
isfy (cid:107)xi − xj(cid:107)2 ≤ rε, need to be computed and stored. This is a typical nearest
neighbor search problem [4], and an eﬃcient implementation is readily available in
many software packages. To illustrate, we provide a Matlab pseudocode that uses the
rangesearch function, which solves this problem using k-d trees.

1 Q = s p a r s e (m,m) ;

r a n g e = s q r t ( r∗ e p s )

3 f o r

t =1:T

% r e t r i e v e data p o i n t s

i n t i m e s l i c e t

5

7

9

11

13

15

r a n g e s e a r c h i n t o s p a r s e matrix

data = p t s ( : , : , t ) ;
% compute a l l d i s t a n c e s w i t h i n r a n g e
[ idx , D] = r a n g e s e a r c h ( data ’ , data ’ , r a n g e ) ;
% r e s h a p e output o f
K = a s s e m b l e s i m m a t r i x ( idx , D, e p s ) ;
% compute d i f f u s i o n map matrix B
q = s p a r s e ( 1 . / sum (K, 2 ) ) ;
Peps = d i a g ( q )∗K;
deps = s p a r s e ( 1 . / sum ( Peps , 1 ) ) ;
B = d i a g ( deps )∗ ( t r a n s p o s e ( Peps ) )∗ Peps ;
% add up t o Q
Q = Q + B ;

17 end ;
% n o r m a l i z e
19 Q = 1/T∗Q;

:

Besides the distance computations, the only other computationally expensive task is
the T sparse matrix multiplications that are needed to compute the matrices Bε,t. The
cost for this can be estimated as [9] O(b2mT ) in the best and O(bm2T ) in the worst
case, where b is the typical number of nonzero elements in any row of Pε,0,t. In many

17Observers that move according to the dynamics Φt, here the particles themselves.

15

practical cases, one can avoid this cost by computing a simpliﬁed version of Qε, e.g.

(cid:88)

t∈It

˜Qε(i, j) =

1
T

Pε,α,t(i, j)

(3.18)

which requires no matrix multiplication. If α = 1/2 is chosen, then Qε and ˜Q2ε agree
up to ﬁrst order in ε for large m. This can be seen from comparing (3.5) for α = 1/2
with Lemma 2 (vi). If the densities qt are uniform for all t ∈ It then there will be
no diﬀerence at all between Qε and ˜Q2ε. In our numerical experiments, Qε and ˜Q2ε
always produced very similar results.

2. Run the spectral clustering algorithm. This requires the computation of the leading
eigenvectors of Qε, which is challenging for large m with a worst-case complexity
of O(m3) even for sparse matrices [11]. We do not discuss large-scale spectral clustering
in here, since we have shown that our method “converges” to the analytical method
in the data-rich limit.
In that case other, Galerkin projection-based methods, are
available, see [33, 31, 15, 59], and [39] for an overview of methods. For more information
about fast spectral clustering algorithms we refer to [10, 19, 43].

Choice of parameters. The cutoﬀ radius r is used to tune the shape of the kernel func-
tion h. For diﬀusion maps, r should be smaller then the scalar curvature of M, which
determines the length scale at which the manifold no longer looks locally ﬂat [38]. However,
the scalar curvature is typically not known. We will choose r = 2, which corresponds to a
cutoﬀ at exp(−r) ≈ 0.1.
How should one choose ε for a given amount of data, that is, for a given m? There are
two error terms present in (3.16), a variance term scaling as O(ε−d/4m−1/2) and a bias term
scaling as O(ε2). This represents a trade-oﬀ: If ε is reduced then the bias is decreased but
the variance is increased. Qε inherits this behavior from diﬀusion maps. The optimal choice
of ε for diﬀusion maps was investigated in [56] and found to be

ε =

C(M)
m1/(3+d/2) .

(3.19)

Here C(M) is an unknown constant which depends on the manifold M. Equation (3.19) tells
us that if twice the amount of data is available, then we can reduce ε by a factor of 2−1/(3+d/2).
In practice, there are two good indicators for choosing ε: (i) One could choose ε such that
the sparsity18 of Qε is between 1% and 5%, (ii) one may compute the dominant spectrum of
Lε = ε−1(Qε − I) (see Remark 5) for diﬀerent ε, and choose ε based on minimal sensitivity
of the eigenvalues, see section 5 for more details.

18The sparsity of a matrix A is the number of of nonzero entries of A devided by the total number of

entries.

16

Missing data. Many real-world data sets are incomplete. For example, not all of the
data points {Φtxi}t∈It
of any given trajectory might be available, but only some of them.
Our algorithm can handle this naturally. We adopt the following convention: Whenever the
distance (cid:107)Φtxi − Φtxj(cid:107) cannot be computed because e.g. Φtxi is missing, we set (cid:107)Φtxi −
Φtxj(cid:107) = ∞. This convention is easily implemented and leads to the i-th row of Bε,t being
equal to δij. Hence, from the point of view of the Markov chain induced by Qε, a missing
data point Φtxi means that at the time slice t, the Markov chain cannot leave or jump to
trajectory i.

4

Is there an ε-free construction?

As already noted in section 2 (e), the parameter ε is in general artiﬁcial, and it is not
immediate what are natural choices for it. Even more, one could apply noise that is not
Gaussian. We recall equation (2.7), which provides a perturbation expansion in ε [22] if the
dynamics Φ is a volume-preserving diﬀeomorphism:

T ∗T = Id +

ε
2

P∗∆P + o(ε) ,

(4.1)

1

Equation (4.1) also holds if the noise is not Gaussian, but has mean zero and covariance
matrix I. This result allows to extract coherent sets from the eigenfunctions of the dynamic
Laplacian, an ε-free operator.
In Remark 5, we extended this result to the non-volume-preserving case, where the
Laplace operator has to be replaced by the q-Laplacian ∆q = q−1∇· (q∇). Can one obtain a
purely data-based ε-free construction that mimics the dynamic Laplacian? Remark 5 readily
ε (Qε − I). Note, however, that ε (cid:55)→ kε(x, y) is for x (cid:54)= y an inﬁnitely
suggests to take limε→0
smooth function, with its derivatives of any order being zero at ε = 0. This renders the ε-
derivatives of Qε of any order also zero. This holds true if the kernel base function h in (3.1)
is replaced by any compactly supported function.
It seems like something went wrong here. It turns out, we cannot exchange the limits m →
∞ and ε → 0. Lemma 2 (ii) already indicates this: the O(ε−d/4m−1/2) estimate diverges
as ε → 0 for ﬁnite m. One can mimic (4.1) by ﬁnite data, but only in the sense of taking
the ε → 0 and m → ∞ limits simultaneously, such that ε yields a lower bound on m. In
particular, statements like Qε = I + εLε + O(ε2) only hold for ε > ε(m), where ε(m) → 0
as m → ∞.
So far, we chose the exponential kernel base function h(x) = cr exp(−x)1x≤r for our
computations, since it gives rise to explicit diﬀusion operators Pε,α via Lemma 1. It turns
out this choice is not necessary for (3.5), and thus (3.17), to hold. In [38], it was shown
that (3.5) can be established with virtually any kernel base function h : R+ → R, it only has
to satisfy the following mild conditions:
(i) h : R+ → R is measurable, non-negative and non-increasing,
(ii) h ∈ C 2(R+), with bounded zeroth and second derivatives,
(iii) h has compact support.

17

Observe now that the choice h(x) = x−a/2 with a > 0 would lead to kε(x, y) = εa/2(cid:107)x −
y(cid:107)−a, and when we compute Pε,α with this kernel the εa/2 factor cancels due to the row
normalization in (3.2), apparently leading to Pε,α being ε-independent. But h(x) = x−a/2 is
neither bounded nor compact, hence we must introduce a cutoﬀ19, i.e. via h(x) = x−a/21x≤r
(actually, a molliﬁed version of this, such that h ∈ C 2(R+)). This reintroduces the ε-
dependence of Pε,α via

Pε,α(i, j) (cid:54)= 0 ⇔ (cid:107)xi − xj(cid:107)2 ≤ εr .

which essentially means that we trade in the physical parameter ε for a “proximity” param-
eter εr. Due to computational reasons, one introduces such a cutoﬀ parameter in practice
anyway, since a large data set would render manipulation with fully occupied matrices impos-
sible. But the message here is that such a parameter is actually necessary for mathematical
reasons: in order for ε−1(Pε,α − I) to converge to the scaled Laplace operator, as in (3.5),
the proximity parameter must be scaled to 0 as m → ∞, such that mεd+4/ log m → ∞, [38,
Theorem 3].
Remark 7: In contrast to our approach (ﬁrst compute a graph Laplacian for every time
slice, then perform temporal averaging), the approach in [34] computes a dynamical dis-
tance rij by performing a temporal average of the Euclidean distance ﬁrst. Using the dy-
namical distance and the weights r−1
ij , they construct a single graph Laplacian and perform
spectral clustering with it. Their method uses a cutoﬀ parameter as well. The main diﬀerence
to our method is that they perform time-averaging before setting up the graph Laplacian,
and these two operations do not commute. Also, their weights correspond to a kernel base
function20 h(x) = x−1/2 (note that we plug the squared Euclidean distances into h). Froyland
and Padberg-Gehle [32] use a fuzzy c-means clustering method on the dynamical distances
directly. Hereby, their dynamical distances are squared Euclidean distances between the
trajectories embedded into the high-dimensional space RdT , where d is the data dimension
and T is the number of time slices.

5 Numerical examples

5.1 Double gyre

We consider the non-autonomous system [31]

˙x = −πA sin (πf (t, x)) cos(πy)
df
dx

˙y = πA cos (πf (t, x)) sin(πy)

(t, x),

(5.1)

19To achieve boundedness, one can introduce a saturation value, i.e. via h(x) = min{h0, x−a/21x≤r}. We
ii = K, where K (cid:29)
20To achieve boundedness at the origin, the authors in [34] set the diagonal terms to r−1

do not do this here to keep notation simple.

1 is some large constant.

18

where f (t, x) = α sin(ωt)x2 + (1 − 2α sin(ωt))x. We ﬁx the parameter values A = 0.25,
α = 0.25 and ω = 2π. The system preserves the Lebesgue measure on X = [0, 2] × [0, 1].
Equation (5.1) describes two counter-rotating gyres next to each other (the left one rotates
clockwise), with the vertical boundary between the gyres oscillating periodically. The period
of revolution of the gyres varies with the distance from the “center”, and is, on average, about
5 time units.
First, we consider a data-rich case. We simulate 20000 trajectories, with initial states
from a 200× 100 grid of X, with position information obtained every 0.1 time instances from
initial time 0 to ﬁnal time 20. Thus d = 2, m = 20000, and T = 201.
We construct the space-time diﬀusion matrix ˜Qε for various values of ε, and show the
dominant spectrum of Lε = ε−1( ˜Qε − I) in Figure 4. We can identify a gap after three

Figure 4: Scaled eigenvalues of the space-time diﬀusion matrix.

eigenvalues, and expect to ﬁnd Λ = 3 coherent sets. Extracting three clusters yields for
every ε the coherent sets shown in Figure 5.

Figure 5: Result of 3-clustering the double gyre trajectory data, shown at initial time (t = 0;
1st time slice), and half a period before ﬁnal time (t = 19.5; 196th time slice).

We observe an interesting “bifurcation” in the 2-clustering of the 2nd eigenvector Ξ2,
when decreasing ε. Figure 6 shows the eigenvectors and corresponding 2-clusterings for ε =

19

0.004 and ε = 0.0002. For the smaller diﬀusion value, one of the gyres gets separated

Figure 6: Top: second eigenfunction Ξ2 of ˜Qε for ε = 0.0002 (left) and ε = 0.004 (right) at
initial time t = 0. Bottom: corresponding 2-clusterings.

from the rest of phase space to yield the most coherent splitting. For the larger diﬀusion,
however, the separation is along the stable manifold of the hyperbolic periodic orbit on
the {y = 0} boundary of X. This latter case has been observed on diﬀerent occasions both
with transfer operator based methods and Lagrangian drifter-based techniques [31, 32]. The
transition between the clusters for changing ε, however, has not been yet reported, to the
best knowledge of the authors. We see the reason for this bifurcation in the following. If
no diﬀusion is present, the central parts of the two gyres (from now on “gyre cores”) are
regular regions of the ﬂow (invariant tori of the time-1 ﬂow map, cf [31, Figure 1]), hence
they are perfectly coherent. Meanwhile, convective transport between the regions {x ≤ 1}
and {x ≥ 1} occurs along the unstable manifold of the periodic orbit on {y = 1} (which is
the image of the stable manifold of the periodic point on {y = 0} under point reﬂection with
respect to the point (1, 0.5)), which, close to {y = 0}, meanders back and forth between
the two regions. Thus, if convective transport dominates diﬀusion, the gyre cores are the
most coherent sets. However, if we increase diﬀusion, trajectories can leave the gyre cores.
Note that there is also diﬀusive transport across the separatrix {x = 1}, but it is less than
transport across the gyre core boundaries, because these latter boundaries are longer than
the separatrix (this we can see with the naked eye). Hence, if ε is large enough that diﬀusive
transport dominates convective transport, the “left-right” separation of X reveals the most
coherent sets. Of course, if diﬀusion is that large, there is less determinism in the fate of the
single trajectories. A hard clustering of the complete state space might not be sensible, and

20

a soft clustering shall be used instead [32]. We remark, that for e.g. metastability analysis of
molecular dynamics, where diﬀusion plays a decisive role, the concept of “soft membership”
in dynamical coarse graining is well established [53].

We note that techniques based on Galerkin-type projections of transfer operators were
always reported to reveal the “left-right” separation as most coherent splitting [31, 59].
This is most likely due to the smoothing eﬀect of the projection onto basis functions: they
introduce numerical diﬀusion [20, 25], which seems to be over our “bifurcation threshold”
at ε ≈ 0.004.

While the computational cost of Galerkin projection methods decreases with decreasing
√
number of basis functions (which, in general leads to increased numerical diﬀusion), the
computational eﬀort of our method decreases with decreasing ε, since less points are O(
ε)-
close to each other, and this sparsiﬁes Qε. While for ε = 0.004 around 11% of the entries
of ˜Qε are nonzero, for ε = 0.0002 the fraction of nonzeros is 0.6%.

Looking for additional coherent sets, we cluster the eigenvector data into 4 clusters,
shown in Figure 7. In the large-diﬀusion case we ﬁnd the gyre cores along with the left-right
separation according to the stable manifold as coherent sets. In the small-diﬀusion case we
ﬁnd further subdivision of the regular region. The interested reader may compare this result
with that in [26].

Figure 7: Result of 4-clustering the double gyre trajectory data, shown at initial time (t = 0;
1st time slice). Left: ε = 0.0005, right: ε = 0.004.

We turn now to a sparse, incomplete data case. We take our previous data set, and
pick m = 500 trajectories randomly, and discard the rest. Then, we destroy 80% of the
remaining data, by setting randomly (both in time and space) entries to NaN (“Not a Number”
in Matlab). To balance the sparsiﬁed neighborhoods due to the loss of data, we set ε =
0.01. Then we assemble the space-time diﬀusion matrix, and carry out the clustering of its
eigenvectors for 2 and 3 clusters, respectively. The results are shown in Figure 8. There, the
original 2- and 3-clusterings in the large diﬀusion case are overlayed by the clustering of the
sparse incomplete data. We observe an excellent agreement; note that in some cases even
the ﬁlaments of the one cluster reaching well into the other are correctly identiﬁed.

21

Figure 8: Results for the sparse incomplete data set, compared with the results of the full data
case from before. In the sparse case, 97.5% of the previous trajectories is discarded, and 80%
of the remaining data is destroyed. The sparse incomplete data clusters are represented by the
colors cyan and magenta (2-clustering, left ﬁgure), and cyan, magenta, orange (3-clustering,
right ﬁgure), respectively.

5.2 Bickley jet

We consider a perturbed Bickley jet as described in [49]. This is an idealized zonal jet
approximation in a band around a ﬁxed latitude, assuming incompressibility, on which three
traveling Rossby waves are superimposed. The dynamics is given by ( ˙x, ˙y) = (− ∂Ψ
∂x ),
with stream function

∂y , ∂Ψ

Ψ(t, x, y) = −U0L tanh(cid:0) y

(cid:1) + U0L sech2(cid:0) y

(cid:1) 3(cid:88)

n=1

L

L

An cos (kn (x − cnt)) .

The constants are chosen as in [49, Section 4], the length unit is Mm (1 Mm = 106 m), the
time unit is days. In particular, we set kn = 2n/re with re = 6.371, U0 = 5.414, and L = 1.77.
The phase speeds cn of the Rossby waves are c1 = 0.1446U0, c2 = 0.2053U0, c3 = 0.4561U0,
their amplitudes A1 = 0.0075, A2 = 0.4, and A3 = 0.3. The system is usually considered on
a state space which is periodic in the x coordinate with period πre; we will, however, not
make any use of this knowledge in our computations.
We advect m = 12000 particles with initial conditions at t0 = 0 on a uniform grid inside
the domain [0, 20] × [−3, 3]. We save the positions of the particles at the T = 401 time
frames It = {0, 0.1, . . . , 40}, during which each particle traverses the cylinder ∼ 5 times.
With this data as input, we compute Qε according to (3.15). The dominant spectrum of
Lε = ε−1(Qε − I) for diﬀerent values of ε is shown in Figure 9 on the left. The λn for
n ≤ 9 are stable for 0.01 ≤ ε ≤ 0.05. We choose ε = 0.02, yielding a sparsity of 4.5%. The
unifying features of the spectra are large spectral gaps after the 2nd, 3rd and 9th eigenvalue,
which indicates that clusterings with Λ = 2, 3 or 9 are all possible. The eigenfunctions
Ψ2, Ψ3 and Ψ4 are shown in Figure 10 on the right at time t = 20. Clearly, Ψ2 and Ψ3
pick out the meandering jet stream region in the middle, which constitutes the strongest
dynamical boundary in this system, and the six vortices. Ψ4 distinguishes between two of
the six vortices, {Ξ5, . . . , Ξ9} distinguish between the other vortices.

22

Figure 9: Bickley jet, eigenvalues (left) and embedding using the eigenfunctions Ξ2, Ξ4 and
Ξ5 (right).

The clustering for Λ = 9 is shown in Figure 11 on the left at times t = 5, t = 20 and
t = 35. The long and narrow cluster in the jet stream region stays perfectly coherent for
the whole time interval, while the six clusters in the vortex region loose some mass. This is
in perfect agreement with the eigenvalue structure in Figure 9. For Λ = 3, the six clusters
in the vortex region merge with the corresponding background cluster, as shown in Figure
11 on the right. A movie showing the full time evolution can be found in the supporting
information. In Figure 9 on the right, the m = 12000 trajectories are embedded as points
in span{Ξ2, Ξ4, Ξ5} and coloured according to the clustering in Figure 11. This embedding
highlights the connectivity structure of the clusters.

Note that we only use the Euclidean metric as input; no information about the global
cylindrical geometry of the state space is given. The fact that our method picks up the jet
stream region clearly shows that it learns the cylindrical geometry and highlights dynamical
features that are encoded in the time-ordering of the data. A purely geometrical, heuristic
method based on the Euclidean metric alone will always struggle to identify the long, narrow
and meandering clusters that we ﬁnd in the data.

5.3 Ocean drifter data set

To test our method on real world data, we consider a dataset of ocean drifters from the
Global Ocean Drifter Program available at AOML/NOAA Drifter Data Assembly Center
(http://www.aoml.noaa.gov/envids/gld/). We focus on the years 2005-2009 and restrict
to those drifters that have a minimum lifespan of one year within this timespan. We record
the position of these 2267 drifters every month, i.e. our trajectories have 60 time frames.
This is the same dataset which has been studied in [32].

The drifter data is sparse: The average lifetime of a drifter is only 23 months, and there
are also gaps in observations where a drifter location failed to be recorded. On average,
only 38% of the drifters are available at any given time instant. The dataset is also extremely

23

05101520−7−6−5−4−3−2−10n(cid:104)n  (cid:161) = 0.5(cid:161) = 0.2(cid:161) = 0.1(cid:161) = 0.05(cid:161) = 0.02(cid:161) = 0.01−0.02−0.0100.010.02−0.2−0.100.10.2−0.1−0.0500.050.1(cid:85)2(cid:85)4(cid:85)5Figure 10: Top to bottom: Eigenfunctions Ξ2, Ξ3 and Ξ4 at t = 20.

sparse spatially, with only 2267 drifters covering the global ocean, it serves therefore as a
good test case for our method. Additionally, we do not use any metric that is adapted to
the sphere. We simply consider the drifters as data points21 in R3 and take the Euclidean
metric in R3.

To set ε, we compute Qε for a range of values for ε, the result is shown in Figure 12 on
the left. Because the data is so sparse, the spectra show some variation with changing ε.
For 0.05 ≤ ε ≤ 0.2 they are reasonably close, indicating an optimal balance between the
variance and bias terms in (3.17). We choose ε = 0.1, which leads to a sparsity of Qε of 18%.
There is no clear spectral gap in the data, so we choose Λ = 5, as did the authors in [32].
The resulting clusters are shown in Figure 13. To display as much information as possible,
we divide the full time span into the four time intervals January 2005 – March 2006, April
2006 – June 2007, July 2007 – September 2008 and October 2008 – December 2009. For
every time interval, we plot all drifter locations in a single plot and color-code time in each
of the plots by color saturation (the darker the color, the “later” the drifter location). A
movie showing all 60 frames can be found in the supplementary information.

The ﬁve clusters we ﬁnd may be described broadly as the Northern Paciﬁc, the Southern
Paciﬁc, the Northern Atlantic, the Southern Atlantic together with the Indian Ocean, and
the Arctic Ocean. Boundaries between clusters are in locations where continents and islands
form bottlenecks (for example, the boundary between the green and purple cluster is a line
between Great Britain and Iceland) and at the equator. In Figure 12 on the right, we show
the embedding of the 2267 drifters produced by Ξ2 and Ξ3. We see that Ξ2 separates the
Arctic and Northern Atlantic from the rest, while Ξ3 distinguishes between the Northern
Paciﬁc, the Southern Paciﬁc and the Southern Atlantic/Indian Ocean. We can also infer
connectivity patterns from this plot. Note that there is no connection between the red, the
yellow, and the purple clusters, showing that none of the drifters passed trough the Bering

21We scale all distances such that radius of the Earth is equal to one.

24

Figure 11: Top to bottom: Bickley jet, clusters at times t = 5, t = 20 and t = 35. Left:
Clustering for Λ = 9. Right: Clustering for Λ = 2.

Strait and the Indonesian Archipelago, respectively. A few isolated data points hint at a
possible connection between the blue and green clusters, this could be due to the vicinity of
drifters across the Panama Strait.

The main diﬀerence to the result of Froyland and Padberg-Gehle [32] is that we do not
separate the Indian Ocean from the Southern Atlantic, but instead separate the Arctic from
the Northern Atlantic. A possible explanation for this is that fuzzy clustering, used by them,
has a tendency to produce clusters of similar size, and although this is equally true for the k-
means algorithm we use, we measure size in terms of the geometry given by the diﬀusion
coordinates Ξ1, . . . , ΞΛ. As a result, we do produce clusters of diﬀerent sizes as long as their
dynamical separation is strong.

We note that the Southern Atlantic, the Southern Paciﬁc and the Indian Ocean are
dynamically well connected through the Antarctic Circumpolar Current, this can be seen
by the substantial overlap between the blue and yellow clusters close to the Antarctic. As a
result, drifters in this region are diﬃcult to classify. By contrast, the Arctic is well separated
from the Northern Atlantic, and the Arctic drifters are actually only available for the last
30 of the 60 months.

5.4 The ABC-ﬂow

As a last, three-dimensional example, we consider the steady Arnold–Beltrami–Childress
ﬂow (short: ABC ﬂow) [3], generated by the ODE

˙x = A sin(z) + C cos(y)
˙y = B sin(x) + A cos(z)
˙z = C sin(y) + B cos(x)

on X = [0, 2π]3 (with periodic boundary conditions), with the “usual” set of parameters, A =
√
√
2, and C = 1. This autonomous system with this set of parameters yields six

3, B =

25

Figure 12: Ocean drifter data. Left: Eigenvalues for diﬀerent ε. Right: Embedding using the
eigenfunctions Ξ2 and Ξ3 for ε = 0.1 with coloring according to the clusters Figure 13 (red:
Northern Paciﬁc, blue: Southern Paciﬁc, Yellow: Southern Atlantic/Indian Ocean, green:
Northern Atlantic, purple: Arctic).

three-dimensional vortices, which are invariant under the dynamics [18, 29, 7]. Thus, they
are also coherent sets.
The trajectory data we use consists of initial states building a 40 × 40 × 40 uniform grid
of X, integrated on a time window of length 40, and sampled uniformly in time every 0.2
time instances. Thus d = 3, m = 64000, and T = 201.

We build the space-time diﬀusion map transition matrix ˜Qε for ε = 0.02, and extract 7
clusters from its six subdominant eigenvectors. The spectrum of ˜Qε does not show a clear
spectral gap after six eigenvalues for any values of ε. This is because on the considered time
interval, parts of the respective vortices are also coherent. Figure 14 shows the clusters that
indicate the six invariant vortices. Note that the vortices in this autonomous system do not
move in space, hence the clusters look the same at every time slice. The right-hand side
of Figure 14 shows the boundaries of the clusters computed by using the data points from
all time slices. Figure 15 shows the embedding of the data by three diﬀerent eigenvectors,
respectively. Note that the star-shaped geometry indicates that transport between the vor-
tices can only occur through the “transition region” between the vortices. This was similar
in the Bickley jet example, but with more than one single transition region. However, for
the ocean drifters, the topology of continents and ocean basins resulted in a quite diﬀerent
dynamical connectivity pattern; cf Figure 12.

6 Conclusion

In this article, we provided a data-driven method for the detection of coherent sets. Our
main result is Theorem 4, which establishes a connection between our method and the
“forward-diﬀuse-backward” transfer operator T ∗T studied within the analytical framework
of coherence [21]. This allows us to give meaning to the dominant eigenfunctions Ξi of Qε,

26

05101520−1−0.8−0.6−0.4−0.20n(cid:104)n  (cid:161) = 0.5(cid:161) = 0.2(cid:161) = 0.1(cid:161) = 0.05(cid:161) = 0.02(cid:161) = 0.01−0.0200.020.04−0.04−0.0200.020.04(cid:85)2(cid:85)3Figure 13: Ocean drifter data, clusters. Top left: January 2005 – March 2006. Top right:
April 2006 – June 2007. Bottom left: July 2007 – September 2008. Bottom right: October
2008 – December 2009. Color saturation is proportional to time in the respective 15 month
time window.

which represent our main computational output: They are approximations of the respective
eigenfunctions of a time-averaged version of T ∗T . We use the Ξi in two ways: (i) To
detect coherent sets via spectral clustering, and (ii) as “dynamical coordinates” which can
be used to reveal the intrinsic low-dimensional organization of the trajectory data, such as
the connectivity structure between clusters.

We based our method on diﬀusion maps, but we expect that the techniques presented
herein can be used to analyze other methods based on the construction of similarity graphs
between trajectories in a similar manner. However, we found that using diﬀusion maps has
several important advantages:

• The diﬀusion kernel function h(x) = exp(−x)1x≤r is numerically very well behaved.
• We need very little a priori knowledge about the system at hand.

In fact, we only
need a distance function (cid:107) · (cid:107) which is a good approximation of the actual intrinsic
distance locally. No knowledge about invariant measures, the global geometry of the
state space, or a good set of basis functions is assumed. In all of our numerical examples
we just used the Euclidean distance, even though the state spaces considered included
spherical and toroidal geometries.

27

Figure 14: Left: the six coherent vortices extracted by a 7-clustering of the eigenvectors,
using the data points at ﬁnal time. The seventh cluster, the region between the vortices,
is not shown. Right: the boundary of the same six coherent data point sets, computed by
Matlab’s boundary function.

• Only a single scale parameter ε needs to be tuned, and we provided criteria for doing

so.

These properties indicate that space-time diﬀusion maps are well suited as an analysis tool for
trajectory data generated from a “black box” dynamical system. Our numerical experiments
suggest robust results even for very sparse and incomplete data.

There are two possible directions to extend this research. First, one could consider
stochastic dynamics. The transfer operator framework used here incorporates this case but
the pointwise assertions of Theorem 4 will have to be replaced by suitable local averages
due to the noise in the dynamics. Second, one could consider noisy, incomplete, and even
corrupted observations. For example, the data might be of the form yt = Oxt + ηt, where xt
is the true state of the system, O is some linear operator and ηt is additive noise. There is
evidence that diﬀusion maps is robust under additive noise [12], but if observations are in-
complete then Euclidean distances between observations will not represent distances between
the underlying states even locally, and one has to resort to other techniques.

Acknowledgments

RB was supported by EPSRC Grant No. EP/K039512/1. PK was supported by the Einstein
Center for Mathematics Berlin (ECMath), project grant CH7.

28

Figure 15: Eigenvector-embedding of the data into R3 (left: Ξ2, Ξ3, and Ξ4; right: Ξ2, Ξ5,
and Ξ6), with colors identical to those of the clusters in Figure 14.

A Proofs

A.1 Reversed dynamics and adjoint operator
To see that T ∗ is the forward operator of the backward dynamics, let x, y be random variables
with distributions x ∼ µ and y ∼ k(x,·)d(cid:96); that is, we think of y as the image of x under
the non-deterministic dynamics Ψ. Then, for any µ-measurable set B, and ν-measurable
set A, we have by Bayes’ law for the probability that the time-reversed dynamics ends up
in B after one step, provided it started in A, that

P[x ∈ B| y ∈ A] =

P[x ∈ B, y ∈ A]

P[y ∈ A]

=

(cid:90)

1BT ∗1A dµ =

=

1

ν(A)

µ(B)(cid:82)
(cid:90)

A T 1B
ν(A)
T ∗ 1A
ν(A)

B

dµ .

(cid:90)

µ(B) dν

=

1

ν(A)

1AT 1B dν

A.2 Proof of Lemma 1

On the one hand, from (3.5) we have

Pε,αf (x) = f (x) + ε

(cid:18)∆(f q1−α)
q1−α − ∆(q1−α)
q1−α f
(cid:18)∆(f q1−α)
q1−α − ∆(q1−α)
q1−α f

(cid:19)

(cid:19)

+ O(ε2).

+ O(ε2).

On the other hand, by using Dεf = f + ε∆f + O(ε2) and the product rule, we have

Dε(q1−αf )
Dεq1−α = f + ε

29

Thus the left-hand side and right-hand side of (3.7) are equal up to O(ε2). To show Pε,0f =
Dεf , note that almost surely

(cid:80)m
(cid:80)m

1

mεd/2
1

mεd/2

j=1 kε(x, xj)f (xj)

k=1 kε(x, xk)

=

Dε(f q)(x)

Dεq(x)

= Dεf (x).

(A.1)

Pε,0f (x) = lim
m→∞

This ﬁnishes the proof. (cid:4)

A.3 Proof of Lemma 2

(i) follows directly from (3.8).
(ii) Recall that by Lemma 1, Pε,0f = Dεf . Now note that we can write Bεf (x) =

ε (x) P ∗
1
d∞

ε,0Pε,0f (x) where

(cid:90)

P ∗
ε,0f (x) =

1
εd/2

kε(x, y)

q(y)
qε(y)

f (y)dy

is the adjoint of Pε,0. This follows by inspecting (3.11) and (3.12). Further note that
ε = P ∗
d∞

ε,01. Set h = Pε,0f and estimate

(cid:12)(cid:12)(cid:12)(cid:12)(cid:0)diag(PT
(cid:12)(cid:12)Bεf (xi) − Bεf (xi)(cid:12)(cid:12) =
ε,01)(cid:1)−1
≤(cid:12)(cid:12)(cid:12)(cid:0)PT
ε,01(xi)(cid:1)−1 − (P ∗
(cid:12)(cid:12)PT

+

1

P ∗
ε,01(xi)

ε,0Pε,0f (xi) − 1
PT
d∞
ε (xi)
ε,0Pε,0f (xi)

ε,01(xi))−1(cid:12)(cid:12)(cid:12) PT
ε,0h(xi)(cid:12)(cid:12) +

ε,0h(xi) − P ∗

1

P ∗
ε,01(xi)

P ∗
ε,0Pε,0f (xi)

(cid:12)(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)P ∗
ε,0(Pε,0f − Pε,0f )(xi)(cid:12)(cid:12)

ε,0f (xi) − P ∗

In [38], it was shown that |Pε,0f (xi) − Pε,0f (xi)| = O(ε−d/4m−1/2) holds uniformly.
ε,0f (xi)| = O(ε−d/4m−1/2) follows, and since Pε,0 and Pε,0 are bounded,
|PT
we have

(cid:12)(cid:12)Bεf (xi) − Bεf (xi)(cid:12)(cid:12) = O(ε−d/4m−1/2) +
(cid:17)(cid:17)
(cid:16) ∆q
(iii) As already noted in (ii), we can write Bεf = 1
ε (x) P ∗
d∞
q − ω

ε 1 = 1 + O(ε).
ε,0g(x) with g := Dεf . We use
+ O(ε2) where ω : Rn → R is
the Taylor expansion q−α
a potential term depending on the embedding of M, see [12]. With the shorthand
q − ω, (3.13) yields
hq := ∆q

(ii) follows by using the Taylor expansion qε = q + O(ε), which gives P ∗

ε = q−α(cid:16)

O(ε−d/4m−1/2).

P ∗
ε 1(xi)

1 + αε

1

ε = 1 + εDεhq + O(ε2).
d∞

30

Using the same expansion on P ∗

ε,0g gives P ∗

ε,0g = Dε [g + εhqg] + O(ε2), and ﬁnally

Bεf =

Dεg + εDε[hqg]

1 + εDεhq

+ O(ε2)

where the quotient is taken pointwise. Now we use the formula a+εc
O(ε2) on this quotient expression and obtain

b+εd = a

b + ε c

b − ε ad

b2 +

Bεf = Dεg + ε (Dε[hqg] − Dε[hq]Dε[g]) + O(ε2).

Now note that since Dε = I + O(ε), where O(ε) depends on the ﬁrst derivative of the
argument of Dε, the term Dε[hqg] − Dε[hq]Dε[g] is O(ε). This together with g = Dεf
shows (iii).

(iv) From (ii), we know |Bεf (xi) − Bεf (xi)| = O(ε−d/4m−1/2) and |BT

(cid:12)(cid:12)Bεf (xi) − BT

O(ε−d/4m−1/2). From (iii), |Bεf (xi) − B∗

ε f (xi)(cid:12)(cid:12) ≤(cid:12)(cid:12)Bεf (xi) − Bεf (xi)(cid:12)(cid:12) +(cid:12)(cid:12)Bεf (xi) − B∗

ε f (xi)(cid:107) = O(ε2). Then, for any bounded f ,
ε f (xi) − B∗

εf (xi)(cid:12)(cid:12) +(cid:12)(cid:12)BT

ε f (xi) − B∗

εf (xi)| =

εf (xi)(cid:12)(cid:12)

= O(ε−d/4m−1/2) + O(ε2).

Since this holds for any bounded f , the result follows for any compatible matrix norm
(cid:107) · (cid:107).

(v) If qε = q, then d∞

ε = 1, and P ∗

ε,0f = Dεf . Then Bεf (x) = 1
ε (x) P ∗
d∞

ε,0Pε,0f (x) =

DεDεf (x) follows.

(vi) By (iv), limε→0

and (3.5),

1

ε (Bεf − f ) = d

dε(DεDεf )|ε=0 = d
Dεf|ε=0 =

Pε,1f|ε=0 =

d
dε

d
dε

1
4

∆f

dε Dεf|ε=0 + d

dεDεf|ε=0. By Lemma 1

and

Thus

Dεf|ε=0 =

d
dε

d
dε

Pε,0f|ε=0 =

∆(f q)

4q

− ∆q
4q

f =

1
4

∆f +

∇q · ∇f.

1
2

lim
ε→0

1
ε

(Bεf − f ) =

1
2

∆f +

∇q · ∇f =

1
2

1
2

q−1∇ · (q∇f ).

A.4 Proof of Theorem 4

We have, by the deﬁnition of Qε,

m(cid:88)

j=1

Qε(i, j)f (xj) =

bε,t(Φtxi, Φtxj)f (xj).

(cid:88)

m(cid:88)

t∈It

j=1

1
T

31

(cid:80)m
(cid:80)m
j=1 δ(yk − Φtxj)f (xj)
j=1 δ(yk − Φtxj)
l(cid:88)

.

For one ﬁxed t ∈ It, we label the images {Φtx1, . . . , Φtxj} with {y1, . . . , yl}, taking into
account possible duplicates since Φt is not assumed to be injective (if it is, then l = m).
Deﬁne

ˆg(yk) :=

Then

bε,t(Φtxi, Φtxj)f (xj) =

m(cid:88)
where d(yk) := (cid:80)m
1. For any test function h, we have (cid:12)(cid:12)(cid:80)
m(cid:88)

g := Ptf . Now the proof has 3 steps:

l(cid:88)

l(cid:88)

follows since

bε,t(Φtxi, yk)d(yk)ˆg(yk).

(A.2)

j=1
j=1 δ(yk = Φtxj) counts the multiplicities of the yk’s. Deﬁne further

k=1

(cid:12)(cid:12) = O(m−1/2). This
k h(yk)d(yk)ˆg(yk) − (cid:104)h, g(cid:105)qt
m(cid:88)

h(yk)δ(yk − Φtxj)f (xj) =

j=1

j=1

k=1

k=1

h(Φtxj)f (xj),

h(yk)d(yk)ˆg(yk) =

(cid:104)Uth, f(cid:105)q0 = (cid:104)h,Ptf(cid:105)qt with variance O(m−1).

and the right-hand side is an unbiased Monte Carlo estimator of(cid:82) h(Φtx)f (x)q0(x)dx =
k h(yk)d(yk)g(yk)(cid:12)(cid:12) = O(m−1/2). This follows since the data points {yk}l
2. (cid:12)(cid:12)(cid:104)h, g(cid:105)qt −(cid:80)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:88)

3. Observe that (see (3.14)) bε,t(x, y) is exactly equal to bε(x, y) in (3.9), but with the q-
distributed samples xi replaced by qt-distributed samples Φtxi. By Lemma 2 (ii), we
thus have

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) = O(ε−d/4m−1/2)

taking into account their multiplicities d(yk), are i.i.d. qt-distributed.

bε,t(x, yk)d(yk)g(yk) − Bε,tg(x)

(A.3)

k=1,

k

where Bε,tf (x) is deﬁned as

b∞
ε,t(x, y)qt(y)f (y)dy,
ε,t is given by the same formula as b∞

Bε,tf (x) =

X

ε

and b∞
everywhere.

Now for the test function h(·) = Bε,t(x,·), the steps 1.-3. allow the estimate

in (3.12) and (3.13), with q replaced by qt

h(yk)d(yk)ˆg(yk) − bε,tg(x)

h(yk)d(yk)ˆg(yk) − (cid:104)h, g(cid:105)qt

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:88)

k

(cid:90)

32

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≤

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:88)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:88)

k

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:104)h, g(cid:105)qt −(cid:88)

k

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

h(yk)d(yk)g(yk)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) +
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

h(yk)d(yk)g(yk) − Bε,tg(x)

+
= O(m−1/2) + O(m−1/2) + O(ε−d/4m−1/2).

k

But in view of the deﬁnition of g, we have Bε,tg(Φtxi) = UtBε,tPtf (xi). By using (A.2), we
can rewrite the estimate above as

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) = O(m−1/2) + O(ε−d/4m−1/2)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) m(cid:88)

j=1

bε,t(Φtxi, Φtxj)f (xj) − UtBε,tPtf (xi)

Finally, Property (iii) of Lemma 2, Bε,tf = DεDε,tf + O(ε2), ﬁnishes the proof. (cid:4)

References

[1] Michael R. Allshouse and Thomas Peacock. Lagrangian based methods for coherent structure

detection. Chaos: An Interdisciplinary Journal of Nonlinear Science, 25(9):097617, 2015.

[2] Michael R Allshouse and Jean-Luc Thiﬀeault. Detecting coherent structures using braids.

Physica D: Nonlinear Phenomena, 241(2):95–105, 2012.

[3] Vladimir I Arnol’d. On the topology of three-dimensional steady ﬂows of an ideal ﬂuid. Journal

of Applied Mathematics and Mechanics, 30(1):223–226, 1966.

[4] Sunil Arya, David M Mount, N Netanyahu, Ruth Silverman, and Angela Y Wu. An optimal

algorithm for approximate nearest neighbor searching in ﬁxed dimensions. 1994.

[5] Sanjeeva Balasuriya, Gary Froyland, and Naratip Santitissadeekorn. Absolute ﬂux optimising
curves of ﬂows on a surface. Journal of Mathematical Analysis and Applications, 409(1):119–
139, 2014.

[6] James C Bezdek. Pattern recognition with fuzzy objective function algorithms. Springer Science

& Business Media, 2013.

[7] Daniel Blazevski and George Haller. Hyperbolic and elliptic transport barriers in three-

dimensional unsteady ﬂows. Physica D: Nonlinear Phenomena, 273:46–62, 2014.

[8] Marko Budiˇsi´c and Igor Mezi´c. Geometry of the ergodic quotient reveals coherent structures

in ﬂows. Physica D: Nonlinear Phenomena, 241(15):1255–1269, 2012.

[9] Aydin Bulu¸c and John R Gilbert. Parallel sparse matrix-matrix multiplication and indexing:
Implementation and experiments. SIAM Journal on Scientiﬁc Computing, 34(4):C170–C191,
2012.

[10] Bo Chen, Bin Gao, Tie-Yan Liu, Yu-Fu Chen, and Wei-Ying Ma. Fast spectral clustering of
data using sequential matrix compression. In Machine Learning: ECML 2006, pages 590–597.
Springer, 2006.

[11] Wen-Yen Chen, Yangqiu Song, Hongjie Bai, Chih-Jen Lin, and Edward Y Chang. Parallel
spectral clustering in distributed systems. Pattern Analysis and Machine Intelligence, IEEE
Transactions on, 33(3):568–586, 2011.

[12] Ronald R Coifman and St´ephane Lafon. Diﬀusion maps. Applied and computational harmonic

analysis, 21(1):5–30, 2006.

33

[13] Michael Dellnitz, Gary Froyland, Christian Horenkamp, Kathrin Padberg-Gehle, and Alex Sen
Gupta. Seasonal variability of the subpolar gyres in the Southern Ocean: a numerical investi-
gation based on transfer operators. Nonlinear Processes in Geophysics, 16:655–664, 2009.

[14] Michael Dellnitz and Oliver Junge. On the approximation of complicated dynamical behavior.

SIAM J. Numer. Anal., 36:491–515, 1999.

[15] Andreas Denner, Oliver Junge, and Daniel Matthes. Computing coherent sets using the

Fokker–Planck equation. Preprint, arXiv:1512.03761, 2015.

[16] P. Deuﬂhard, W. Huisinga, A. Fischer, and Ch. Sch¨utte. Identiﬁcation of almost invariant
aggregates in reversible nearly uncoupled Markov chains. Linear Algebra and its Applications,
315(13):39 – 59, 2000.

[17] Peter Deuﬂhard and Marcus Weber. Robust Perron cluster analysis in conformation dynamics.

Linear algebra and its applications, 398:161–184, 2005.

[18] T. Dombre, U. Frisch, J. M. Greene, M. H´enon, a. Mehr, and a. M. Soward. Chaotic streamlines

in the ABC ﬂows. Journal of Fluid Mechanics, 167:353, 1986.

[19] Charless Fowlkes, Serge Belongie, Fan Chung, and Jitendra Malik. Spectral grouping using
the Nystrom method. Pattern Analysis and Machine Intelligence, IEEE Transactions on,
26(2):214–225, 2004.

[20] Gary Froyland. Estimating Physical Invariant Measures and Space Averages of Dynamical

Systems Indicators. PhD thesis, University of Western Australia, 1996.

[21] Gary Froyland. An analytic framework for identifying ﬁnite-time coherent sets in time-

dependent dynamical systems. Physica D: Nonlinear Phenomena, 250:1–19, 2013.

[22] Gary Froyland. Dynamic isoperimetry and the geometry of Lagrangian coherent structures.

Nonlinearity, 28(10):3587–3622, 2015.

[23] Gary Froyland and Michael Dellnitz. Detecting and locating near-optimal almost-invariant

sets and cycles. SIAM J. Sci. Comput., 24(6):1839–1863, 2003.

[24] Gary Froyland, Christian Horenkamp, Vincent Rossi, and Erik van Sebille. Studying an Ag-
ulhas ring’s long-term pathway and decay with ﬁnite-time coherent sets. Chaos, 25(8):083119,
2015.

[25] Gary Froyland, Oliver Junge, and P´eter Koltai. Estimating long-term behavior of ﬂows without
trajectory integration: The inﬁnitesimal generator approach. SIAM Journal on Numerical
Analysis, 51(1):223–247, 2013.

[26] Gary Froyland and P´eter Koltai. Estimating long-term behavior of periodically driven ﬂows

without trajectory integration. 39 pp. Preprint: arXiv:1511.07272, 2015.

[27] Gary Froyland, Simon Lloyd, and Athony Quas. Coherent structures and isolated spectrum

for Perron–Frobenius cocycles. Ergodic Theory and Dynamical Systems, 30:729–756, 2010.

34

[28] Gary Froyland, Simon Lloyd, and Naratip Santitissadeekorn. Coherent sets for nonautonomous

dynamical systems. Physica D: Nonlinear Phenomena, 239(16):1527 – 1541, 2010.

[29] Gary Froyland and Kathrin Padberg. Almost-invariant sets and invariant manifolds - Con-
necting probabilistic and geometric descriptions of coherent structures in ﬂows. Physica D:
Nonlinear Phenomena, 238(16):1507–1523, 2009.

[30] Gary Froyland and Kathrin Padberg-Gehle. Finite-time entropy: A probabilistic approach for
measuring nonlinear stretching. Physica D: Nonlinear Phenomena, 241(19):1612 – 1628, 2012.

[31] Gary Froyland and Kathrin Padberg-Gehle. Almost-invariant and ﬁnite-time coherent sets:
In Ergodic Theory, Open Dynamics, and Coherent

directionality, duration, and diﬀusion.
Structures, pages 171–216. Springer, 2014.

[32] Gary Froyland and Kathrin Padberg-Gehle. A rough-and-ready cluster-based approach for
extracting ﬁnite-time coherent sets from sparse and incomplete trajectory data. Chaos, 25(8),
2015.

[33] Gary Froyland, Naratip Santitissadeekorn, and Adam Monahan. Transport in time-dependent
dynamical systems: Finite-time coherent sets. Chaos: An Interdisciplinary Journal of Non-
linear Science, 20(4):043116, 2010.

[34] Alireza Hadjighasem, Daniel Karrasch, Hiroshi Teramoto, and George Haller. A spectral

clustering approach to Lagrangian vortex detection. Preprint. arXiv:1506.02258, 2015.

[35] George Haller. Finding ﬁnite-time invariant manifolds in two-dimensional velocity ﬁelds.

Chaos: An Interdisciplinary Journal of Nonlinear Science, 10(1):99–108, 2000.

[36] George Haller. Distinguished material surfaces and coherent structures in three-dimensional

ﬂuid ﬂows. Physica D, 149(4):248–277, 2001.

[37] George Haller and F. J. Beron-Vera. Geodesic theory of transport barriers in two-dimensional

ﬂows. to appear in Physica D, 2012.

[38] Matthias Hein, Jean-Yves Audibert, and Ulrike von Luxburg. From graphs to manifolds –
weak and strong pointwise consistency of graph Laplacians. In Proceedings of the 18th Annual
Conference on Learning Theory, COLT’05, pages 470–485, Berlin, Heidelberg, 2005. Springer-
Verlag.

[39] Stefan Klus, P´eter Koltai, and Christof Sch¨utte. On the numerical approximation of the

Perron–Frobenius and Koopman operator. Preprint, arXiv:1512.05997, 2015.

[40] Stephane Lafon and Ann B Lee. Diﬀusion maps and coarse-graining: A uniﬁed framework for
dimensionality reduction, graph partitioning, and data set parameterization. Pattern Analysis
and Machine Intelligence, IEEE Transactions on, 28(9):1393–1403, 2006.

[41] R. Lambiotte, J. C. Delvenne, and M. Barahona. Laplacian dynamics and multiscale modular

structure in networks. ArXiv, 2009.

[42] Andrzej Lasota and Michael C Mackey. Chaos, fractals, and noise: stochastic aspects of

dynamics, volume 97. Springer, 1994.

35

[43] Tie-Yan Liu, Huai-Yuan Yang, Xin Zheng, Tao Qin, and Wei-Ying Ma. Fast large-scale spectral

clustering by sequential shrinkage optimization. Springer, 2007.

[44] Tian Ma and Erik M Bollt. Diﬀerential geometry perspective of shape coherence and curvature
evolution by ﬁnite-time nonhyperbolic splitting. SIAM Journal on Applied Dynamical Systems,
13(3):1106–1136, 2014.

[45] Boaz Nadler, St´ephane Lafon, Ronald R Coifman, and Ioannis G Kevrekidis. Diﬀusion maps,
spectral clustering and reaction coordinates of dynamical systems. Applied and Computational
Harmonic Analysis, 21(1):113–127, 2006.

[46] Andrew Y Ng, Michael I Jordan, Yair Weiss, et al. On spectral clustering: Analysis and an

algorithm. Advances in neural information processing systems, 2:849–856, 2002.

[47] Kathrin Padberg, Thilo Hauﬀ, Frank Jenko, and Oliver Junge. Lagrangian structures and

transport in turbulent magnetized plasmas. New Journal of Physics, 9:400, 2007.

[48] V. Rom-Kedar and S. Wiggins. Transport in two-dimensional maps. Archive for Rational

Mech. and Anal., 109:239–298, 1990.

[49] I. I. Rypina, M. G. Brown, F. J. Beron-Vera, H. Ko¸cak, M. J. Olascoaga, and I. A. Udovyd-
chenkov. On the Lagrangian Dynamics of Atmospheric Zonal Jets and the Permeability of the
Stratospheric Polar Vortex. Journal of the Atmospheric Sciences, 64(10):3595–3610, 2007.

[50] Marco Sarich, Natasa Djurdjevac, Sharon Bruckner, Tim OF Conrad, and Ch Sch¨utte. Modu-
larity revisited: A novel dynamics-based concept for decomposing complex networks. Journal
of Computational Dynamics, 1(1):191–212, 2014.

[51] Satu Elisa Schaeﬀer. Graph clustering. Computer Science Review, 1(1):27–64, 2007.

[52] Ch Sch¨utte, A Fischer, W Huisinga, and P Deuﬂhard. A direct approach to conformational
dynamics based on hybrid Monte Carlo. Journal of Computational Physics, 151(1):146 – 168,
1999.

[53] Christof Sch¨utte and Marco Sarich. Metastability and Markov State Models in Molecular

Dynamics. Courant Lecture Notes in Mathematics, 2013.

[54] Enrico Ser-Giacomi, Vincent Rossi, Crist´obal L´opez, and Emilio Hern´andez-Garc´ıa. Flow
networks: A characterization of geophysical ﬂuid transport. Chaos: An Interdisciplinary
Journal of Nonlinear Science, 25(3):036404, 2015.

[55] Jianbo Shi and Jitendra Malik. Normalized cuts and image segmentation. Pattern Analysis

and Machine Intelligence, IEEE Transactions on, 22(8):888–905, 2000.

[56] A. Singer. From graph to manifold Laplacian: The convergence rate. Applied and Computa-
tional Harmonic Analysis, 21(1):128 – 134, 2006. Special Issue: Diﬀusion Maps and Wavelets.

[57] Anne-Marie Treguier, O. Boebel, B. Barnier, and G. Madec. Agulhas eddy ﬂuxes in a 1/6

degrees atlantic model. Deep Sea Research Part II, 50(1):251–280, 2003.

36

[58] Ulrike Von Luxburg. A tutorial on spectral clustering. Statistics and computing, 17(4):395–416,

2007.

[59] Matthew O. Williams, Irina I. Rypina, and Clarence W. Rowley. Identifying ﬁnite-time co-

herent sets from limited quantities of Lagrangian data. Chaos, 25(8), 2015.

37

