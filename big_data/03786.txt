6
1
0
2

 
r
a

 

M
0
1

 
 
]
h
c
e
m

-
t
a
t
s
.
t
a
m
-
d
n
o
c
[
 
 

1
v
6
8
7
3
0

.

3
0
6
1
:
v
i
X
r
a

Large deviation theory to model systems under an external feedback

Alessio Gagliardi1, Alessandro Pecchia2, Aldo Di Carlo3

(1) Technische Universitaet Muenchen,

Arcisstrasse 21, 80333, Munich (Germany)

alessio.gagliardi@tum.de.

(2) CNR, Via Salaria Km 29,

600, 0017 Monte Rotondo (Italy)

(3) University of Rome ”Tor Vergata”,

Via del Politecnico 1, 00133, Rome (Italy)

(Dated: March 15, 2016)

In this paper we address the problem of systems under an external feedback. This is performed
using a large deviation approach and rate distortion from information theory.
In particular we
deﬁne a lower boundary for the maximum entropy reduction that can be obtained using a feedback
apparatus with a well deﬁned accuracy in terms of measurement of the state of the system. The
large deviation approach allows also to deﬁne a new set of potentials, including information, which
similarly to more conventional thermodynamic potentials can deﬁne the state with optimal use of
the information given the accuracy of the feedback apparatus.
PACS numbers:

I.

INTRODUCTION

The idea of a thermodynamic theory of systems under
an external feedback dates back to the origin of statisti-
cal physics when Maxwell made a gedanken experiment
about the work that could be extracted by a system con-
trolled by an external apparatus.
In principle the ex-
ternal feedback can probe the state of the system and
manipulates it in order to extract work. In his ﬁrst for-
mulation the feedback was treated like an oracle that can
make measures and manipulate the system without any
cost in terms of work and entropy production, for this
reason it was called a ”Demon” to stress its abstract
nature. However the idea was reconsidered along all
the past century1,2 with diﬀerent approaches and conclu-
sions. One of the central work on the topic was the paper
by Szilard and his famous Szilard engine3, a still very ide-
alized machine that anyway shows already some more re-
alistic characteristics in terms of its components and the
nature of the feedback controller. Starting from Szilard
engine, but generalizing the concept, several authors4–10
and in particular Sagawa and Ueda, have developed a
new branch of thermodynamics of systems under an ex-
ternal feedback11–13, i.e. information thermodynamics.

In particular in14 it was shown, using an ingenious ex-
tension of ﬂuctuation theorems including the information
gathered by the feedback, that the presence of the exter-
nal controller can increase the average work that can be
extracted from a system when it is driven from one equi-
librium state to another according to

hWi ≤ ∆F + kBT I,

(1)

with hWi the average work, ∆F free energy variation
between the ﬁnal and initial equilibrium states, kB is
the Boltzmann constant, T the bath temperature and I
the mutual information. For a cyclic transformation, for
which ∆F = 0 the maximum work reduces to kBT I.

The mutual information, for a classical system, is de-
scribed by the mutual information functional of informa-
tion theory. There is an understandable debate about
the validity of using information theoretical quantities
and their interpretations in thermodynamics15. However,
for many cases of interest16 Shannon entropy represents
a good choice for the entropy functional, especially in
equilibrium cases. In particular Shannon entropy gives
the right entropy functional but only physical and ex-
perimental evidences can determine which is the correct
probability density function (PDF) of the problem under
investigation, as information theory has nothing to say
about that17.

An interesting alternative approach to non equilibrium
thermodynamics is arising within the framework of a dif-
ferent branch of information theory, e.g. large deviation
theory (LDT). LDT is a mathematical framework used
to establish the probability of ﬂuctuations of statistical
quantities from their typical values. Typicality in infor-
mation theory leads to the deﬁnition of thermodynamic
averages and state variables18,19.

In particular, LDT demonstrates that for a broad class
of PDFs, the probability of ﬂuctuations from the aver-
age value drops exponentially with the ﬂuctuation mag-
nitude. The exponent is proportional to the number of
degrees of freedom of the system, explaining why, in the
thermodynamic limit, ﬂuctuations of state variables be-
come negligible. However in the emerging ﬁeld of stochas-
tic thermodynamics which also copes with small systems
in non equilibrium conditions, ﬂuctuations can be an im-
portant aspect of their behavior20–24.

Many systems of interest have the peculiarity of being
the interconnection between a physical system probed
and controlled by a feedback apparatus, for example bi-
ological processes in cells and other living organisms be-
long to this class. Recently, several studies have inves-
tigated the eﬀect of including information terms in the
analysis of biological processes within the framework of

information thermodynamics with great success, see for
example25–32. Thus the investigation of thermodynamics
under feedback is rapidly rising interest in many ﬁelds
beyond statistical physics.

In this paper we give a new insight to the problem of
a system under an external feedback using a special case
of LDT and the concept of typicality. The link between
systems under feedback and LDT is obtained using rate
distortion theory (RDT), a fundamental part of informa-
tion theory. The chain of relationships between LDT and
RDT that we outline, is particularly pleasing as it makes
a direct connection between the information the feedback
controller apparatus gathers about a system and the as-
sociated entropy reduction, also leading to an explicit
construction of a thermodynamic potential for a system
under feedback. The possibility to construct potentials
including the eﬀect of information opens the perspective
of a complete new analysis of those systems more similar
to conventional equilibrium thermodynamic approaches.
in the ﬁrst part a
brief summary of the main information theoretical con-
cepts, Shannon entropy, conditional entropy and mutual
information is given. Then the concept of typicality is
introduced. In the next section we present the large devi-
ation theory and the code large distortion problem. The
ﬁnal part is devoted to introducing rate distortion theory
and the ﬁnal link to thermodynamics under an external
feedback.

The paper is organized as follows:

II. A BRIEF EXCURSUS IN INFORMATION

THEORY

The central quantity of information theory is the Shan-

non entropy deﬁned as

S(P ) = −kBXi

pi ln pi,

(2)

where pi represents the probability of the ith event and
P the PDF. Shannon entropy is usually expressed in bits
and adimensional, we have chosen here the convention to
include the Boltzmann constant and express the entropy
using natural logarithm. However, this is totally imma-
terial for the present discussion. Shannon entropy plays
a central role in thermodynamics, even for a large class
of systems under non equilibrium conditions16.

More generally if we have a PDF p(ξ) which depends
on a set of degrees of freedom, ξ, we can deﬁne the proper
discrete (or continuous) Shannon entropy. The degrees
of freedom depend on the problem at hand, they could
be the set of positions and momenta of a collection of
particle in gas phase for example. From the Shannon
entropy it is possible to promptly derive other four con-
nected quantities. The ﬁrst is the joint (Shannon) en-
tropy which is just the entropy of two random vectors ξ
and π:

2

with p(ξ, π) the joint probability. If the two vectors are
independent the joint entropy reduces to the sum of the
individual entropies, in the other case we have:

S(Ξ, Π) = S(Ξ) + S(Π) − kBI(Ξ, Π),

(4)

with I(Ξ, Π) the mutual information functional deﬁned
as:

I(Ξ, Π) =X p(ξ, π) ln(cid:20) p(ξ, π)
p(ξ)p(π)(cid:21) .

(5)

The mutual information is a special case of the Kullback-
Leibler divergence (KLd) deﬁned as:

D(PkQ) =X p(ξ) ln(cid:18) p(ξ)
q(ξ)(cid:19) ,

(6)

where P and Q are two PDFs. The KLd is a pseudo
distance between PDFs and has an important role
in LDT due to the Chernoﬀ bound19,
in stochastic
thermodynamics33, within ﬂuctuation theorems34 and in
entropy production within the Boltzmann equation35. In
particular, we observe that the mutual information has
the form of a KLd between the joint PDF, p(ξ, π), and
the independent PDF, p(ξ, π) = p(ξ)p(π) ≡ q(ξ, π).
dom vectors are connected by the conditional entropy:

Finally, mutual information and entropy of two ran-

S(Ξ|Π) = S(Ξ) − kBI(Ξ, Π) =X p(ξ, π) ln p(ξ|π). (7)

Conditional entropy represents the entropy (uncertainty)
left in Ξ after the conditioning to Π.

These ﬁve are the most relevant functionals in infor-
mation theory as practically every important theorem is
related to one or another in some form.

III. TYPICAL SET IN THE PHASE SPACE

The Shannon entropy has a nice geometrical inter-
pretation in the typical set theorem consequence of the
asymptotic equipartition principle36. The theorem states
that given a PDF p(ξ), where ξ is the vector of degrees
of freedom of the problem, with entropy S(Ξ) then the
”typical set” within the phase space has a volume equal
to:

Ωtyp ∼ e

S(Ξ)

kB ,

(8)

where the meaning of ”typical” means that the probabil-
ity for the system to be found in a microstate within the
typical set converges to 1 in the thermodynamic limit,
namely,

p(ξ ∈ Ωtyp) → 1.

(9)

S(Ξ, Π) = −kBX p(ξ, π) ln p(ξ, π),

(3)

In other words the concept of typicality states that only
a portion of the entire phase space is really relevant to

3

ﬂuctuations with the macroscopic behavior of the sys-
tem described by thermodynamic potentials, this formal-
ism is large deviation theory (LDT). This connection is
a well established fact dating back to the ’70, several
authors37–42 used LDT to derive many results of equilib-
rium thermodynamics. In particular it was possible to
derive in a very elegant way the maximum entropy prin-
ciple/ minimum free energy for systems in the thermo-
dynamic limit in microcanonical or canonical ensemble.
The entire idea of LDT is to estimate the probability
of ﬂuctuations departing from the average in stochastic
problems. This can be directly applied to evaluate the
probability of observing a ﬂuctuation of a state variable
in a thermodynamic system.

Let us assume we have a quantity A with average value
A∗ and N degrees of freedom in the system. We deﬁne
the contribution to A per degrees of freedom a = A/N
and a∗ = A∗/N .
It is said that a stochastic problem
follows a Large deviation (LD) law if the probability that
a departs from a∗ follows an exponential law:

q(a 6= a∗) ∼ e−N K(a),

(12)

where K(a) is an exponent which is dependent on the
thermodynamic quantity, while q(a) is deﬁned as in eq. 11
from the PDF per microstate p(ξ).

A powerful theorem to check if a problem satisﬁes a
LD law is the G¨artner-Ellis theorem (GET)38. We ﬁrst
deﬁne the Scaled Cumulant Generating Function (SCGF)
as

with

1
N

λ(α) = lim
N→∞

ln(cid:2)heN αai(cid:3) ,
(cid:10)eN αa(cid:11) =Z p(ξ)eαN a(ξ)dξ,

(13)

(14)

with α a real number. The GET states that if the SCGF
exists and is diﬀerentiable everywhere in α, then the sys-
tem ﬂuctuations follow an exponential law.

We notice that the SCGF is very similar to a partition
function, but scaled with respect to N and also where
every exponential term is weighted by the probability per
microstate, p(ξ).
If we make a simple variable change
α = −β, the new parameter β plays the same role as
the inverse temperature, β = 1/(kBT ) and the SCGF
can be rewritten as −φ(β) = λ(α). The GET does not
only provide a condition for existence, but also gives an
operative way to evaluate the exponent K(a).
In fact
it demonstrates that K(a) is related to the Legendre-
Fenchel transform of the SCGF:

K(a) = − min

β ≥ 0

[βa − φ(β)].

(15)

FIG. 1. (color online) The phase space and the typical sub-
set. Usually, except for the uniform distribution, the typical
set is indeed a proper subset of the entire class of possible mi-
crostates in the phase space. Its volume grows exponentially
with the entropy of the problem.

compute ensemble averages of thermodynamic quanti-
ties, considering that the volume Ωtyp fundamentally col-
lects all the probability (see Fig. 1).

In particular for any quantity, A(ξ), deﬁned over the

phase space, the ensamble average is deﬁned as,

with

hAi =Z ˜Aq( ˜A)d ˜A,

q( ˜A) =Z p(ξ)δ( ˜A − A(ξ))dξ.

(10)

(11)

If it happens that for all microstates within the typical
set, A(ξ ∈ Ωtyp) = A∗ (constant), then A is a state
variable of the problem and hAi = A∗.
Several generalizations of the typical set concept ex-
ist for example for a joint PDF in the joint typical set
theorem36. The concept of typicality is extremely im-
portant also in thermodynamics. In the microcanonical
ensemble where the PDF over the phase space is a uni-
form PDF, the entropy reduces to the integration of the
density of accessible microstates.

However, as recent studies on stochastic thermody-
namics have shown15,33, it is possible to extend many
thermodynamic results also for small systems, i.e., sys-
tems where signiﬁcant ﬂuctuations of the state variables
are not only possible, but also probable. Such type of
systems are also those for which a practical implementa-
tion of a feedback controller is more feasible due to their
limited number of degrees of freedom.

IV. LARGE DEVIATION THEORY

There is an elegant formalism within information the-
ory connecting the statistical analysis of microscopic

It is possible to demonstrate43 that the previous equa-
tion can be rewritten in terms of the real partition func-
tion (without the PDF weighting the exponents as in the

SCGF), but adding a constant:

J(a) = − min

β ≥ 0

[βa − φ(β)] +

1
N

ln Λ = K(a) +

1
N

ln Λ.

(16)
The latter constant has the form of the entropy of a uni-
form PDF U (ξ) = 1/Λ, with Λ a particular volume within
the phase space (see43), which depends on the prior PDF
p(ξ). In the case of a microcanonical ensemble it reduces
to all the microstates with the same energy ¯E. The func-
tion φ(β) is related to the free energy potential. Speciﬁ-
cally, we have that the free energy per degrees of freedom
(f = F/N ) is equal to:

f =

φ(β)

β

,

(17)

thus the Legendre-Fenchel transform of the SCGF is
linked to the entropy of the system per degree of free-
dom, s(a), by

J(a) = −

s(a)
kB

+

1
N

ln Λ.

(18)

If we consider the constant term as the entropy associ-
ated to a uniform PDF we have that the exponent J(a)
can be treated as follows:
1
N

ln Λ

+

J(a) = −s(a)
kB
N 
Xξ

=

1

1
N

=

D(Ξ//U ),

p(ξ) ln p(ξ) +Xξ

p(ξ) ln Λ


(19)

where U = 1/Λ is the uniform PDF over the phase
space volume Λ, D is the KLd and N s(a) = Stot =

−P p(ξ) ln p(ξ) the total entropy. Thus the probability
q(a 6= a∗) goes like the following:

q(a 6= a∗) ≈ e−D(Ξ//U),

(20)

recovering the Chernoﬀ bound for large ﬂuctuations
within the typical set formalism36. The LDT provides a
clear understanding why entropy maximization is at the
essence of equilibrium thermodynamics. Similar results
are obtained for a canonical ensemble18. For a general
discussion about the GET and the LDT applied to ther-
modynamics we refer to38.

Notably, LDT has been applied to non equilibrium
systems37 substituting the PDF for a microstate with
the PDF of entire time dependent trajectories to take
into account the time evolution of the system. Even more
important, LDT can be used to derive ﬂuctuation theo-
rems. In fact if the exponent has the symmetry relation,
e.g. K(−a) − K(a) = γa (γ is a positive real number)
for a certain thermodynamic quantity, A = N a , we im-
mediately get the ﬂuctuation theorem18,33,44–46:

q(a)

q(−a) ≈ eN (K(−a)−K(a)) = eN γa.

(21)

A dual theorem of GET is the Varadhan theorem47
which allows to invert the Legendre-Fenchel transform.
This states that if

4

K(a) = − min

β ≥ 0

[βa − φ(β)],

is valid, then also the following relation holds:

φ(β) = min

a

[βa + K(a)].

(22)

(23)

The LD law in microcanonical or canonical ensemble ex-
plains, through the Varadhan theorem, why the minima
of the free energy potentials are associated to the state
variable values for the equilibrium state.

V. RATE DISTORTION THEORY AND LARGE

DEVIATION THEORY

In this paragraph we make explicit the connection be-
tween LDT and the information theory quantities pre-
sented in the previous sections and consider a system
under feedback. In order to do so we need to introduce
the rate distortion function (RDF), a central functional
in rate distortion theory (RDT). RDT copes with a fun-
damental problem in communication, namely estimating
the minimal information content that any message sent
over a communication channel must contain such that
the receiver can still have a good reconstruction of the
original signal. The error source can be either due to
distorting noise, a ﬁnite channel capacity or because of
a lossy compression operated by the sender. In mathe-
matical form, if we deﬁne a distance, d(π, ξ), between the
message sent, ξ, and the message received, π (eventually
after decompression, noise deconvolution, etc...), the tar-
get of RDT is to ﬁnd under which conditions the average
distance can be kept lower than a given threshold, Γ36.
Formally we ask,

hd(ξ, π)i ≤ Γ,

(24)

where the average is computed over the joint PDF p(ξ, π).
The distance, d, can be any functional with the properties
of a distance (symmetry, positive deﬁnite, d = 0 iif ξ =
π, and must satisfy the Schwartz inequality). The most
important theorem of RDT states that, given d and Γ,
there exists a function, R(Γ) (the rate distortion func-
tion), representing the minimum information required in
order to send messages with an average distortion not
greater than Γ. The function R(Γ) has some remarkable
properties: it is concave in the argument Γ, it converges
to the entropy of the source (for a discrete PDF) for
Γ = 0, while for Γ ≥ Γ∗ it is zero. In practice Γ∗ is the
limit distortion value after which the information content
is completely lost and the receiver has equal chance by
just guessing at random the most likely message, based
on the joint PDF36. For an example of a RDF for a
Gaussian PDF see Fig. 2.

5

Zπ has the form of a generalized partition function, linked
to the distribution of π, where the distance d has a role
similar to energy in the conventional partition function50.
An interesting aspect of this particular case of the LDT
is that minimizing R(Γ) w.r.t. λ, we get an important
relation,

d(ξ, π)

Γ =Xξ,π

p(ξ)q(π)e−λ∗ d(ξ,π)

Zπ(λ∗)

= hd(ξ, π)i.

(29)

The average is made with respect to the joint PDF,

˜p(ξ, π) =

p(ξ)q(π)e−λ∗d(ξ,π)

Zπ(λ∗)

,

(30)

which is the joint probability that fulﬁll the minimal rate
function for a deﬁned average error Γ and λ∗ is the value
minimizing the Legendre-Fenchel transform.

VI. LARGE DEVIATION, RATE DISTORTION

AND FEEDBACK CONTROL

We can now use the results of RDT applied to LDT to
obtain our most important result concerning the thermo-
dynamics of systems with feedback control. Let assume
that we have a feedback controller that performs mea-
surements of the state of a system and then afterwards
manipulates it. If we assume that the measurement, πk,
has some correlation with the state ξ, then the entropy of
the system after measurement will be given by the PDF,
p(ξ|π = πk), conditioned by the outcome πk. This is
equal to:

S(Ξ|π = πk) = −kBX p(ξ|π = πk) ln p(ξ|π = πk).

(31)
A typical set volume is always associated to this entropy,

Ωtyp(ξ|π = πk) = e

S(Ξ|π=πk )

kB

,

(32)

hence the average volume of the typical set after a mea-
surement is thus:

hΩtyp(ξ|π = πk)i =(cid:28)e

S(Ξ|π=πk)

kB (cid:29) ,

(33)

where the average is made w.r.t. q(π).

The lower boundary of this formula can be obtained
using the Jensen inequality, thanks to the concavity of
the exponential function, hexp(f )i ≥ exp(hfi), to obtain:

hΩ(ξ|π = πk)typi ≥ e

hS(Ξ|π=πk)i

kB

= e

S(Ξ|Π)

kB ,

(34)

exploiting the fact that hS(Ξ|π = πk)i = S(Ξ|Π).
Thus, we ﬁnd that the feedback can -at best- con-
strain the volume of phase-space by the conditional en-
tropy. Splitting the conditional entropy in the usual,
S(Ξ|Π) = S(Ξ) − kBI(Ξ, Π), we obtain that the eﬀect

FIG. 2. Typical shape of a RDF. In this case it is plot the
2 = 9. For Γ larger
RDF of a Gaussian PDF with variance σ
than σ

2 the RDF is 0.

The second important aspect of the rate distortion
function is that it can be computed as a constrained min-
imization of a mutual information functional,

R(Γ) =

min

p(π|ξ): hd(ξ,π)i≤Γ

I(Ξ, Π).

(25)

In the latter the minimization is with respect to the con-
ditional PDF, p(π|ξ), and the constrain is that the aver-
age distortion remains always smaller than Γ.
The connection between LDT and RDT is material-
ized by the distortion coding problem (DCP)48. DCP
can be formulated in the following way:
let assume we
have two random vectors ξ and π with PDFs p(ξ) and
q(π), respectively, and we want to know what is the prob-
ability that picking up at random two vectors, one from
each distribution, the distance is such that d(ξ, π) ≤ Γ.
The probability of such condition follows a LDT with an
exponent equal to the rate distortion function:

p(ξ, π : hd(ξ, π)i ≤ Γ) ∝ e−R(Γ).

(26)

The function R(Γ) monotonically decreases in the range
0 < Γ < Γ∗, with limiting values R(0) = S/kb, R(Γ >
Γ∗) = 0.

In the two limiting cases: when Γ = 0, R(0) =
S(Ξ)/kB. In the case Γ ≥ Γ∗ the probability converges
to 1 because R(Γ ≥ Γ∗) = 0.
Since the rate distortion function appears in the form
of a LD law, there is a second way to deﬁne the rate dis-
tortion function using the GET and the Legendre-Fenchel
transform of the SCGF48,49:

with

R(Γ) = − min

λ≥0hλΓ +X p(ξ) ln(Zπ(λ))i ,
Zπ(λ) =Xπ

q(π)e−λd(ξ,π).

(27)

(28)

of the feedback is to compress the volume of the typical
set with an exponent at best equal to the mutual infor-
mation:

Ωtyp(ξ|π) ≥ Ωtyp(ξ)e−I(Ξ,Π).

(35)

The fact that the mutual information is always non
negative ensures that the eﬀect of the feedback is always
to compress the original typical set.

If we want to connect the mutual information to the
eﬀective measurement operated by the feedback appa-
ratus (FA), then we can deﬁne a distance functional, d,
and an average distortion Γ between state and estimation
and use the rate distortion function to ﬁnd the minimal
mutual information required to have a certain average
distortion Γ. The ﬁnal result is the lower boundary,

hΩtyp(ξ|π = πk)i ≥ Ωtyp(ξ)e−R(Γ).

(36)

The appealing of equation (36) is that it gives a direct
connection between the eﬀect of feedback information
and the eﬀective operative measurement performed by
the FA.

A ﬁnal note regarding the eﬀect of the FA. The re-
duction in the volume of the typical set remains only
”virtual” until the FA does not operate and manipulate
the system. We can thus imagine this shrinking of the
typical set linked to the R(Γ) as the best average reduc-
tion in uncertainty about the state of the system from
the FA side once the measurements have been made. It
is anyway clear that the entropy and the physics of the
system are left unchanged until the FA does not directly
operate.

VII. TOWARDS A THERMODYNAMIC

POTENTIAL INCLUDING INFORMATION

A thermodynamic potential

is nothing else than a
quantity that when minimized/maximized allows to ﬁnd
the equilibrium or stationary states of a certain thermo-
dynamic system, subject to given constrains. We have
already discussed in section 4 and 5 how the LDT pro-
vides an elegant way to derive the maximal entropy and
minimal energy principles for the microcanonical and
canonical ensembles. In particular, we saw how the free
energy potential comes as part of the Legendre-Fenchel
transform of the large deviation exponent thanks to the
Gartner-Ellis theorem.
In this section we put together
this result and the results of section 6 in order to con-
struct an explicit thermodynamic potential for a system
under feedback control.

First of all we deﬁne a simpliﬁed feedback apparatus
(SFA). We assume a system in thermal equilibrium with
an external bath at temperature T0 and we assume that
the system can be coupled to the bath or disconnected
from that and coupled to the SFA. The SFA is a sys-
tem that can make a measurement and manipulate the

6

system accordingly in a time τ much smaller than any re-
laxation time of the system. Moreover, we assume a cer-
tain level of ideality in the feedback, i) during the mea-
surement no perturbation of the system occurs, ii) the
feedback uses the entire information during the manipu-
lation achieving the maximal eﬃciency. Once the system
has been manipulated by the feedback it is allowed to
relax and ﬁnally it is connected again to the external
bath until thermalize with it. This simple cycle assures
that every time the feedback performs its new measure-
ment/manipulation the system is at thermal equilibrium
with temperature T0.

With these approximations we can decouple the initial
state before the feedback operation from the feedback
action. A completely diﬀerent and more complex sce-
nario occurs in the case when the system is not allowed
to thermalize or the feedback acts continuously in such a
way that the system state depends on previous feedback
history.

We ﬁnally assume that the feedback can obtain an es-
timate, π, of the real state, ξ, of the system such that
hd(ξ, π)i ≤ Γ for a well-deﬁned distance functional.
We have shown in section 6 that the feedback action
entails to an entropy reduction after measurement by a
factor I(ξ, π) = R(Γ). Thus we can ﬁnally deﬁne the
maximum increase in free energy due to the presence of
the feedback:

∆F = F − Feq

= U − T0(S − kBR(Γ)) − U + T0S = kBT0R(Γ) = hWi.
(37)

with U the internal energy. It recovers the result found
by Sagawa with eq. 1, as the maximum work for a cyclic
transformation, but now with a direct connection be-
tween the type and accuracy of the measurement, d(ξ, π)
and Γ, and the average extracted work hWi.
The R(Γ) exponent of equation (36), following a LD
law, can be expanded in the Legendre-Fenchel transform
as in equation (15), leading to a joint probability between
the microstate and the guess equal to equation (29). Us-
ing the Varadhan theorem we can invert the Legendre-
Fenchel transform for the RDF and obtain something
equivalent to a thermodynamic potential for the infor-
mation:

[λΓ + R(Γ)] .

Γ

hln(Zπ(λ)i =X p(ξ) ln(Zπ(λ)) = min

(38)
The structure of this equation is similar to the one of the
free energy in standard thermodynamics with the rela-
tion between free energy and partition function, where
now the Chernoﬀ coeﬃcient λ plays the role of inverse
temperature:

φI (λ) ≡ −

1
λ hln(Zπ(λ)i .

(39)

What is the meaning of this functional? We can un-
derstand its role by calculating the distance, using the

KLd, between a generic joint probability p(ξ, π) and the
optimal ˜p(ξ, π) as deﬁned in eq. (29):

7

p(ξ, π)
˜p(ξ, π)

p(ξ, π) ln

D(P// ˜P ) =Xξπ
=Xξπ
= I(Ξ, Π) + λ∗hd(ξ, π)i − λ∗φI (λ∗).

p(ξ, π)
p(ξ)q(π)

p(ξ, π) log

+ λ∗hd(ξ, π)i + hln Zπ(λ∗)i
(40)

FIG. 3. (color online) Scheme of a channel between two joint
gaussian random variables.

The latter equation can be rewritten as:

Following36 the feedback has a PDF equals to:

λ∗φI (λ∗) = λ∗hd(ξ, π)i + I(Ξ, Π) − D(P// ˜P ).

(41)

Considering that the three terms in the rhs are all posi-
tive it is easy to demonstrate that the maximum of the
potential φI (λ∗) is obtained for D(P// ˜P ) = 0, that is
when the joint probability is the ideal one for which the
RDF is achieved.

VIII. A SIMPLE PHYSICAL MODEL

We apply the formalism to a simple model: a set of sin-
gle particles in a box in gas phase. We assume that every
particle is under a feedback and that it is in thermal equi-
librium with a bath at temperature T . The Hamiltonian
of the system is given by E = Ap2, where A = 1/2m,
being m the particle mass. The particle state, ξ = (r, p),
is characterized by an homogeneous spatial distribution
for the position r within the box, and a normal distribu-
tion of the momentum, p. Therefore, neglecting position,
we identify the particle state just with the momentum,
x = p. The equilibrium distribution is a normal distri-
bution with 0 mean value and a variance σ2
ξ = kBT /2A.
The model includes a feedback that can probe the mo-
mentum of the particle. The feedback uses a distance
d(ξ, π) = (ξ − π)2. This measurement is aﬀected by er-
ror, so the measurement π is a random variable, statis-
tically correlated with the dynamical state of the parti-
cle. The model assumes that between every probe and
manipulation the system is allowed to relax to thermal
equilibrium, that means that at every measurement the
system is found in the same equilibrium state. Finally,
we also assume that π is distributed like a normal ran-
dom variable. The situation can be formalized like in36
by assuming that the feedback and the source are con-
nected by a channel with Gaussian noise (see Fig. 3).

The Gaussian noise has distribution:

p(z) = N (0, Γ) =

1

√2πΓ

e− z2
2Γ .

(42)

For simplicity we assume that the source has mean value
equal to zero (µξ = 0), p(ξ) = N (0, σ2
ξ ). This assumption
is totally immaterial for the generality of the discussion.

p(π) = N (0, σ2

ξ − Γ) =

− π2
2(σ2
ξ

−Γ) .

e

(43)

1
ξ − Γ)

q2π(σ2

In the latter we have assumed that 0 ≤ Γ ≤ σ2
ξ . With the
previous deﬁned distance functional the average distance
Γ is equal to the mean square error h(ξ − π)2i.
function is well known:

For this simple model the form of the rate distortion

R(Γ) =

1
2

ξ

Γ ! ,
ln  σ2

(44)

with R(Γ) = 0 for Γ ≥ σ2
ξ .
It is a simple matter of calculation to demonstrate that
indeed eq. 27, using p(ξ), p(π) and d(ξ, π) = (ξ − π)2,
gives back the correct rate distortion function. The rela-
tion between λ and Γ is very simple:

λ =

1
2Γ

.

(45)

We can insert the PDF and the distance functional in
eq. 30 in order to get the optimal joint distribution for
such marginal PDFs. The solution is a joint Gaussian
distribution:

˜p(ξ, π) =

1

2ρξπ
σξσπ

+

π2
σ2

π#!

exp −

1

2(1 − ρ2)" ξ2
ξ −
σ2

2πσξσπp1 − ρ2
with correlation coeﬃcient equal to ρ =q1 − Γ/σ2

ξ . Us-
ing the result in eq. 45 we obtain the relation between λ
and the correlation coeﬃcient:

(46)

λ =

1

2σ2

ξ (1 − ρ2)

.

(47)

As expected if Γ is related to the distance between state
and estimation by the feedback, λ has a simple interpre-
tation in terms of correlation between the two random
variables. In case of random vectors (with N elements)

the total average distance Γ = P N Γi, with Γi the dis-

tance associated to the ith component. Clearly the value

Γ has an extensive characteristic being function of the
number of degree of freedom (N ) of the system. On the
contrary λ is the intensive counterpart being connected
to the correlation between state and estimation.

In this particular case we can also calculate the optimal
work that such a feedback apparatus can recover from
the system as a function of the average distance Γ, using
eq.37:

T kB

hWi = T kBR(Γ) = −

2

ln(1 − ρ2) = T kBI(Ξ, Π).

(48)
The latter shows that the maximum work extracted is
proportional as should be to the mutual information be-
tween two Gaussian random variables.

8

IX. CONCLUSIONS

In this work we have analyzed a system under feedback
using typicality and large deviation theory. In particular
this connection assures to make a simple and natural link
between the way the measurement is performed and its
accuracy and the maximum work that can be extracted in
terms of entropy reduction. Clearly in this discussion we
have not considered the eﬀect of the manipulation from
the feedback and its cost in terms of overall eﬃciency,
thus all our results must be considered as boundary lim-
its.

The main result of the paper is the possibility, us-
ing rate distortion theory, of developing the equivalent
of thermodynamic potentials even in the case of systems
under an external feedback.

This perspective to the problem is particular appealing
not only because it establishes a nice relation between the
measurement and the respective entropy and information
functional, but also because it can be generalized for a
large class of problems where large deviation applies.

1 H. S. Leﬀ, A. F. Rex, ”Maxwell Demon: Entropy, Informa-
tion, Computing, Princeton University Press”, Princeton,
NJ, (1990).

2 H. S. Leﬀ, A. F. Rex, ”Maxwell Demon 2: Entropy, Clas-
sical and Quantum Information, Computing”, Institute of
Physics, Bristol, (2003).

3 L. Szilard, On the Decrease of Entropy in a Thermody-
namic System by the Intervention of Intelligent Beings, Z.
Phys. 53:840 (1929): English translation reprinted Behav-
ioral Science 9:301 (1964).

4 H. Touchette and S. Lloyd,Phys. Rev. E, 84, 1156 (2000).
5 A. E. Allahverdyan, D. Janzing, and G. Mahler,J. Stat.

Mech., P09011 (2009).

18 H. Touchette, Phys. Rep., vol. 478, 1-69 (2009).
19 N.
Trans.
Available

Merhav,
Theory,

IEEE
(2007).

form.
http://www.ee.technion.ac.il/people/merhav/papers/p115.pdf.

In-
at

20 A. Kis Andras and A. Zettl, Philos. Trans. R. Soc. A, 366,

1591 (2008).

21 J. R. Gomez-Solano, L. Bellon, A. Petrosyan and S. Cilib-

erto, Europhys. Lett., 89, 60003 (2010).

22 L. Bellon, L. Buisson, S. Ciliberto and F. Vittoz, Rev. Sci.

Instrum., 73, 3286 (2002).

23 A. Berut, A. Arakelyan, A. Petrosyan, S. Ciliberto, R. Dil-

lenschneider and E Lutz, Nature, 187, 483 (2012).

24 J. V. Koski, V. F. Maisi, T. Sagawa and J. P. Pekola, Phys.

6 J. Horowitz, T. Sagawa, and J. M. R. Parrondo,Phys. Rev.

Rev. Lett., 113, 030601 (2014).

Lett., 111, 010602 (2013).

25 A. C. Barato, D. Hartich and U. Seifert, Phys. Rev.E, 87,

7 J. M. Horowitz and M. Esposito,Phys. Rev. X, 4, 031015

042104 (2013).

(2014).

8 J. M. Horowitz and H. Sandberg,New J. Phys., 16, 125007

(2014).

26 S. Ito, T. Sagawa, Nat. Comm., 6, 7498 (2015).
27 A. H. Lang, C. K. Fisher, T. Mora and P. Mehta, Phys.

Rev. Lett., 113, 148103 (2014).

9 N. Shiraishi, S. Ito, K. Kawaguchi, and T. Sagawa,New J.

28 P. Sartori, L. Granger, C. F. Lee and J. M. Horowitz, PLoS

Phys., 17, 045012 (2015).

Comput. Biol., 10, 1003974 (2014).

10 D. Hartich, A. C. Barato, and U. Seifert,J. Stat. Mech.,

29 R. G. Endres and N. S. Wingreen, Phys. Rev. Lett., 103,

P02016 (2014).

158101 (2009).

11 S. Toyabe, T. Sagawa, M. Ueda, E. Muneyuki and M.

30 H. Qian and T. C. Reluga, Phys. Rev. Lett., 94, 028101

Sano,Nature Physics, 6, 988 (2010).

(2005).

12 T. Sagawa and M. Ueda, Phys. Rev. Lett., 100, 080403

31 P. Mehta and D. Schwab, Proc. Natl Acad. Sci. USA, 109,

(2008).

13 T. Sagawa and M. Ueda, Phys. Rev. Lett., vol. 104, 090602

(2010).

14 T. Sagawa and M. Ueda, Chap. 6, ”Nonequilibrium Sta-
tistical Physics of Small systems”, (Wiley-VCH), (2013).
15 S. Hilbert, P. Haenggi and J. Dunkel, Phys. Rev. E, vol.

90, 062116 (2014).

16 J. M. R. Parrondo, J. M. Horowitz and T. Sagawa, Nature

Phys., vol. 11, 131 (2015).

17978 (2012).

32 Y. Tu,Proc. Natl Acad. Sci. USA, 105, 11737 (2008).
33 U. Seifert, Rep. Prog. Phys., 75, 126001 (2012).
34 C. Jarzynski, Non-equilibrium equality for free energy dif-

ferences, Phys. Rev. Lett., vol. 78, 2690 (1997).

35 F. Rezakhanlou and C. Villani, ”Entropy Methods for the
Boltzmann Equation”, Springer-Verlag Berlin Heidelberg
(2008).

36 T. M. Cover and J. A. Thomas, ”Elements of information

17 A. Gagliardi and A. Pecchia, arXiv:1503.02824v1 (2015).

theory”, Wyley (2006).

9

37 H. Touchette, R. J. Harris, Chap. 11, ”Nonequilibrium Sta-
tistical Physics of Small systems”, (Wiley-VCH), (2013).
38 R. S. Ellis, ”Entropy, Large Deviations and Statistical Me-

chanics”, Springer, New York (1985).

39 R. S. Ellis, Physica D, 133, 106-136 (1999).
40 Y. Oono, Progr. Theoret. Phys. Suppl. 99, 165-205 (1989).
41 R. S. Ellis, Scand. Actuar. J., 1, 97-142 (1995).
42 O. E. Lanford, ”Entropy and equilibrium states in classi-
cal statistical mechanics”, in: A. Lenard (Ed.), Statistical
Mechanics and Mathematical Problems, vol. 20, Springer,
Berlin, pp. 1-113 (1973).

43 N. Merhav, IEEE IST (2008) Toronto, 499 (2008).
44 D. J. Evans and D. J. Searles, Phys. Rev. E, 50, 1645

(1994).

45 G. Gallavotti and E. G. D. Cohen, Phys. Rev. Lett., 74,

2694 (1995).

46 J. L. Lebowitz and H. Spohn, J. Stat. Phys., 95, 333 (1999).
47 S. R. S. Varadhan, Comm. Pure App. Math., 19, 261

(1966).

48 N. Merhav, Statistical Physics and Information theory,
Foundation and Trends in Communication and Informa-
tion Theory, vol. 6, 1-212 (2009).

49 T. Berger, ”Rate distortion theory: a mathematical ba-
sis for data compression”, PrenticeHall, Inc., Engelwood
Cliﬀs, NJ, (1971).

50 R. M. Gray, ”Source Coding Theory”, Kluwer Academic

Publishers, (1990).

