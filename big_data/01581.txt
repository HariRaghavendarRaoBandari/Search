6
1
0
2

 
r
a

M
4

 

 
 
]
I

A
.
s
c
[
 
 

1
v
1
8
5
1
0

.

3
0
6
1
:
v
i
X
r
a

Causal models for debugging and control in cloud computing

Philipp Geiger

Max Planck Institute for

Intelligent Systems,
T¨ubingen, Germany

pgeiger@tuebingen.mpg.de

Lucian Carata

Univeristy of Cambridge,

Cambridge, United Kingdom

lc525@cam.ac.uk

Bernhard Sch¨olkopf
Max Planck Institute for

Intelligent Systems,
T¨ubingen, Germany
bs@tuebingen.mpg.de

Abstract

Two challenges in the theory and practice of
cloud computing are: (1) smart control (allo-
cation) of resources under exploration con-
straints, on time-varying systems, and (2)
understanding and debugging of the perfor-
mance of complex systems that involve e.g.
virtualization.
In this paper, we examine
how these challenges can be approached us-
ing causal models. For challenge (1) we in-
vestigate how to infer and use causal models
and selection diagrams to design and inte-
grate experiments in a principled way, as well
as to cope with partially varying systems.
For challenge (2) we examine how to for-
malize performance attribution and debug-
ging questions by counterfactual probabili-
ties, and how to approximately answer them
based on inferred (non-deterministic) graph-
ical causal models.

1 Introduction

In recent years, the paradigm and business model
of cloud computing [Armbrust et al., 2010] has be-
come increasingly popular. It allows to rent comput-
ing resources on-demand, and to use them eﬃciently
by sharing them in a smart way. However, it also
raises new challenges in terms of understanding, de-
bugging and managing the complex systems involved.
In the present paper, we investigate how causal mod-
els [Pearl, 2000, Spirtes et al., 2000] can help address
two of these challenges. Note that we do not aim to
present mature methodology here. We assay, for var-
ious aspects of cloud computing, how causal models
can be applied to them, and draw inspiration from
the kind of problems cloud computing brings about.
We ﬁnd that a number of concepts commonly used in

the ﬁeld of cloud computing directly map to concepts
from the theory of causal models (in particular coun-
terfactuals and selection diagrams), and explain these
connections in our paper.

Our main contributions are conceptual rather than
empirical:

• We translate the problem of inferring controllers
for cloud servers from experimental and time-
varying systems into causal models, and, based
on this, present an approach for designing exper-
iments and integrating their outcome as well as
past experience of partially time-varying systems
(Sections 4.2 and 4.3).

• We translate observation-level performance at-
tribution and debugging questions into queries
for counterfactual probabilities deﬁned in causal
models (Section 5.1), and present a new ap-
proach to approximately answer them from a
(non-deterministic) graphical causal model only
(Section 5.2).

• Furthermore, we investigate how causal models
for computer systems can be inferred in general
(Sections 4.1 and 6).

The remainder of this paper is structured as follows:
in Section 2 we introduce basic concepts; in Section 3
we formulate two problems from cloud computing and
outline our approaches; in Sections 4 and 5 we elab-
orate our approach for the ﬁrst and second problem,
respectively; in Section 6, we brieﬂy discuss some im-
plementation aspects; in Section 7, we discuss related
work; and we conclude the paper with Section 8.

2 Preliminaries

In what follows, we introduce concepts that are used
throughout the paper in a succinct way. More detailed
expositions of these concepts can be found in the ref-

erences provided.

Causal models and interventions. We ﬁrst deﬁne
causal models on a mathematical level, following Pearl
[2000], Spirtes et al. [2000]. We consider variables with
discrete as well as continuous domains, and by a (prob-
ability) density we either refer to a density w.r.t. the
Lebesgue measure, in the continuous case, or w.r.t. the
counting measure, in the discrete case, respectively. If
a formula that contains a sum over densities is not
explicitly restricted to discrete variables, then usually
the formula also holds with the sum replaced by the
Lebesgue integral.

Let V be a set of variables. A functional causal model
(FCM), or structural equation model (SEM), over V
(we call the elements of V the (regular) variables of
the model) consists of a background variable UX with
density pUX for each X ∈ V , and a structural equation
X := fX (PAX , UX) for each X ∈ V , where PAX ⊂ V
are called the parents of X. A graphical causal model
(GCM), or causal Bayesian network (CBN), over V
consists of a directed acyclic graph (DAG) G with V
as node set, called causal diagram or causal DAG, and
a conditional probability density pX|PAX =paX (for all
paX in the domain of PAX ) for each X ∈ V , where
PAX are the parents of X in G.1 Each FCM induces
a GCM in a natural way: the parents in the structural
equations deﬁne the parents in the causal diagram, and
pX|PAX =paX is deﬁned as the density of fX (paX , UX ),
for all variables X. By causal models we refer to FCMs
as well as GCMs.

Given a causal model M and a tuple of variables Z
of M , the post-interventional causal model Mdo Z=z
is deﬁned as follows: (1) if M is a FCM, drop the
structural equations for all variables in Z and replace,
in all remaining structural equations, variables of Z
by the corresponding constant entries of z; (2) if M
is a GCM, drop the variables in Z and all incoming
arrows from the causal diagram, and ﬁx the value of
variables in Z to the corresponding entry of z in all re-
maining conditional densities. Based on this, we deﬁne
the post-interventional density of Y after setting Z to
z, denoted by pY |do Z=z, by the the density of Y in
Mdo Z=z. For a succinct terminology, we will consider
the observational density pZ, for any set of variables Z
in M , as a special case of a post-interventional density.

On a non-mathematical level, we consider M to be a
correct causal model of some part of reality, if it cor-
rectly predicts the outcomes of interventions in that

part of reality (clearly there are other reasonable deﬁ-
nitions of causation). Note that we will use expressions
like p(x|y) as shorthand for pX|Y (x|y).

Selection diagrams. The idea behind selection di-
agrams [Bareinboim and Pearl, 2012] is to allow to
graphically encode how the causal structure of two
or more populations (or systems) relate to each other
by expressing which mechanisms vary and which are
invariant. Formally, a selection diagram is a DAG
(with additional two-sided arrows to encode hidden
confounders) which has two types of nodes: one for
regular variables (the elements of V in the deﬁnition
of causal models above) and one for so-called selec-
tion variables (we indicate the latter type of node by a
rectangle). The selection diagram induced by a family
(cid:0)M(s1,...,sn)(cid:1)(s1,...,sn)∈I of causal models over the same
set of variables V and with the same causal diagram
G is deﬁned as follows (slightly adapting the deﬁnition
to our needs): we start with the DAG G and for each
variable X and for each i = 1, . . . , n, for which the
generating mechanism of X depends on the parameter
si, we add a selection variable node Si and an arrow
from that node to X to the selection diagram.

Structural counterfactuals. Let M be a FCM over
a set V of variables, with a set U of background vari-
ables. Let E, X, Y be (sets of) variables in V . The
structural counterfactual probability of Y being y, had
X been x, given evidence E = e, can be deﬁned [Pearl,
2000] based on M as2

p(Ydo X=x = y|e) := X

p(y|do (x), u)p(u|e).

(1)

u

Reinforcement learning and control. Reinforce-
ment learning (RL) [Sutton and Barto, 1998] is con-
cerned with designing automated agents which per-
form actions (or decisions) that optimize some given
long-term reward or utility. It is usually assumed that
the environment of the agent, also called the dynam-
ics, is (partially) unknown and stochastic but time-
invariant. An agent whose actions only depend on
a ﬁxed (small) number of previous time steps is also
called a policy or controller.
In RL, one usually as-
sumes the environment to be unknown, and so an
agent ﬁrst has to explore it, and later has to trade oﬀ
exploration with exploitation of the gained experience
(based on the time-invariance and additional assump-
tion).
In principle, given a perfect model of the en-
vironment, including utility function, the RL problem
is reduced to ﬁnding an optimum w.r.t. the policy, of
the utility under the model. In this paper, for simplic-

1The conditionals induce a joint density pV . If pV has
support everywhere, then it, together with G, already de-
termines the GCM.

2Note that while [Pearl, 2000] in his deﬁnition uses func-
tions, we, for the sake of a concise deﬁnition, use (deter-
ministic) conditional distributions.

ity, we often neglect inter-temporal dependences, and
thus focus on what is usually referred to as contextual
bandits [Langford and Zhang, 2008].

Cloud computing. Traditionally, both businesses
and individuals have used dedicated local computers,
or computer networks, for storing, managing and pro-
cessing data. However, this can be ineﬃcient in sev-
eral ways: the overhead of maintaining such an in-
frastructure is high, and one needs to buy enough
computers to handle peak loads, while during nor-
mal times most will remain unutilised. Cloud com-
puting signiﬁcantly changes this, by allowing comput-
ing resources to be rented on demand. A company,
the “cloud provider”, is now responsible for running
all the hardware, keeping it upgraded and sharing it
amongst multiple clients. Such an infrastructure can
be run in a highly eﬃcient manner: tens or hundreds of
virtual machines (VMs), i.e., emulations of computer
systems, rented by diﬀerent clients, run on a single
physical server and share its resources such as central
processing units (CPUs), memory and network. Note
that by production, we will refer to the operation of the
system that does actual work for clients and visitors
and w.r.t. which contracts have to be met.

3 Two problems in cloud computing

and outline of our approach

In this section we introduce the two problems we are
addressing in this paper and outline our approaches,
which we then elaborate in the two subsequent sec-
tions. Besides addressing the two speciﬁc problems
described below, in this paper we also follow the more
general side goals of (1) understanding how causal and
probabilistic models and methods can help for com-
puter systems problems, and (2) better understanding
the concept of experimentation and causation.

3.1 Problem 1: inferring controllers from

experimental and time-varying systems

Problem formulation. During the operation of a
cloud server many “decisions” automatically have to
be made regarding how resources, such as CPU time or
bandwidth of input/output (IO) devices, are allocated
among the various applications or VMs running on
that sever. Here we consider the goal of optimizing this
automatic decision making, based on some given util-
ity function, encoding e.g. energy consumption, guar-
antees given to customers, or simply proﬁt. We regard
two speciﬁc scenarios: In the ﬁrst scenario (described
in more detail in Section 4.2), we assume that an ex-

VM1

Sec. 4.2

?

VM1

VM3

Sec. 4.3

VM1

VM2

experimental

production sys.,

production sys.,

sys.,to be designed

target setup

initial setup

Figure 1: Illustration of problem 1 and our approach:
transfer knowledge about VM1 between systems, in
spite of varying circumstances (concurrent VMs).

perimental system, separate from the system in pro-
duction, is available, and we can execute applications
and VMs there and transfer the experiences gained
there to the system in production. The motivation for
considering such a setup is the following: when explo-
ration (in the sense of RL) is performed during produc-
tion, then tentative decisions are exposed to the clients
and visitors. Several papers have examined such a sce-
nario, assuming that experiments are performed either
in advance [Chiang et al., 2014, Vasi´c et al., 2012] or
in parallel [Zheng et al., 2009] to production. In the
second scenario (described in more detail in Section
4.3), we assume that there is only a single system, the
production system, but this system is time-varying,
e.g. due to varying applications (or VMs) running on
it. A version of such a setup has been considered and
addressed e.g. in [Padala et al., 2009], using adaptive
control methods. A common limitation of the men-
tioned approaches (to both scenarios) can be seen in
the lack of a framework to treat things such as ex-
perimentation or knowledge transfer between diﬀerent
environments in a principled way, potentially leading
to all sorts of prediction errors and subsequent sub-
optimal decisions. For instance, decisions about which
properties to vary and to regress on during an experi-
ment are made ad-hoc. The structure of the problem
and our approach is summarized in Figure 1.

Outline of our approach. Generally, we propose
to use a selection diagram to encode the invariant
causal diagram of the system in production as well as
the varying mechanisms, and furthermore write down
what information becomes available at which point.
The inference of such a diagrams is discussed in Sec-
tion 4.1. Then, to address the ﬁrst scenario described
in the problem formulation above, we propose to use
the selection diagram to design the experiment and
integrate the outcome for the sake of decision mak-
ing on the production system (Section 4, based on a
“quintessential” scenario). For the second scenario,
we suggest to use the information on invariant mech-
anisms encoded in the selection diagram to harness
experience from an earlier stage of the production

system, which we refer to as “initial setup”, for a
later one, “target setup” (Section 4.3, again based on
a “quintessential” scenario). We argue that such a
principled approach, compared to the ones mentioned
above, is less prone to making errors, e.g. in the design
and knowledge transfer from experiments, and allows
to harness available information from “diﬀerent” sys-
tems (Section 4.4).

4 Addressing problem 1: inferring

controllers from experimental and
time-varying systems

Keep in mind the problem formulation in Section 3.1.
Here we elaborate the approach which we already out-
lined in that section, structuring it into “modules”,
which we then combine and discuss in Section 4.4.

3.2 Problem 2: observation-level

performance attribution and debugging

Problem formulation. Performance attribution in
computing systems means to understand which com-
ponent of a system contributes to what extent to the
measured performance. (Part of the problem is that
there are currently no rigorous and general deﬁnitions
of many concepts. We hope that with this paper we
also contribute a bit to making deﬁnitions more rigor-
ous, to the extent this is reasonable.) Such attribution
often forms an important step for debugging the per-
formance, i.e., modifying certain components of the
system, to achieve the goal of improving the perfor-
mance. In cloud computing, performance attribution
and debugging is, due to the complexity of the sys-
tem, non-trivial but crucial: for instance, in case of a
poor performance of some application running in a VM
on a cloud server, one wants to understand whether
this problem has to be attributed to the conﬁgura-
tion of the VM (which one could change directly) or
the conﬁguration of the server or other VMs on the
server (in which case one may have to buy a more ex-
pensive cloud product) [Snee et al., 2015]. Note that
we presently focus on attribution and debugging for
individual observations.
In previous approaches, the
lack of an appropriate formal framework leads to dif-
ﬁculties w.r.t., for instance, encoding valuable prior
knowledge of the system, or rigorously distinguishing
between correlation and causation.

4.1 Module 1A: inference of causal models

and selection diagram

Clearly, as with all models, inference and application
of causal models is a back-and-forth, trial-and-error
process. Based on their deﬁnition (Section 2), an im-
portant ingredient to the inference of causal models are
(randomized) interventions on variables and the subse-
quent observation of their outcome, to the extent such
interventions are feasible.3 Important additional input
to causal model inference often comes from domain
knowledge of experts [Pearl, 2009]. Additional insight
can come from observational causal discovery methods
[Spirtes et al., 2000, Mooij et al., 2014]. Generally, as
many sources as possible should be used, and poten-
tially redundant information can always be very valu-
able to make the inference more robust. We brieﬂy dis-
cuss some aspects of implementation of causal model
inference on real system in Section 6.

Regarding selection diagrams, we ﬁrst note that
their mathematical deﬁnition (Section 2, based on
[Bareinboim and Pearl, 2012]) takes complete causal
models of various domains as input. In practice how-
ever, it is rather the goal to “output” the complete
(or relevant part of the) causal model of some “target
domain”, based on (1) the joint causal diagram of all
domains, (2) knowledge about mechanisms in “source
domains”, and (3) knowledge on which mechanisms
vary between target and various source domains. The
role of the selection diagram is to allow to simultane-
ously encode (1) and (3). It seems that generally, for
this method to work, a lot of expert knowledge and a
good intuition for how to use causal modeling language
are necessary.

Outline of our approach. We ﬁrst show how to
translate performance attribution and debugging ques-
tions into queries for counterfactual distributions in
causal models (Section 5.1). Although generally a
FCM is necessary to answer such queries, we show
that they can still approximately be answered based
only on a GCM (Section 5.2). Last, we discuss how
we propose to combine the above modules and discuss
some advantages of our approach (Section 5.3).

We want to point out how the inference of causal
models and selection diagrams can be facilitated for
the case of computer systems, based on several ap-
pealing properties of this domain: First, many as-
pects of computer systems (hardware and software)

3Clearly, the theory of causal model inference faces the
classical problem of induction [Hume and Hendel, 1955].
We simply postulate here that humans can infer sensible
models from “ﬁnite” data or supervise machines to do so.

are - by design - modular, i.e., separable into indi-
vidually manipulable input-output mechanisms and
the same (or similar) mechanisms occur in diﬀerent
systems. And modularity is a central assumption in
causal models [Pearl, 2000]. Second, there is a poten-
tial source of information which is speciﬁc to computer
systems: a lot of knowledge about non-causal associ-
ations, such as which program calls which other pro-
gram during execution, is available, often times even in
a well-formatted way (e.g. program code or system ar-
chitecture speciﬁcations). Such information could be
translated into hypotheses on causal association (or be
used for measurement selection), in a (semi-)automatic
way. Generally, keep in mind that the inference of a
causal model for some system or component may be
non-trivial; however, a good causal model, which only
needs to be inferred once, can subsequently save a lot
of exploration (in the sense of RL) for all instances of
the system or component, and thus signiﬁcantly lower
the overall costs.

4.2 Module 1B: integration of experiments

Here we describe in more detail a quintessential ver-
sion of the ﬁrst scenario from the problem formulation
(Section 3.1) and show how to address it.

A quintessential scenario. We consider the follow-
ing scenario (which will be formalized by the selec-
tion diagram D in Figure 2). There is some target
stage, where some cloud server, which we also refer
to as “target platform”, will run a speciﬁc VM Vm
and some concurrent workload (e.g., concurrent VMs)
Con. We call the target platform, together with spe-
ciﬁc Vm and Con, “target setup”. 4 We assume that
we have an experimental cloud server, which we refer
to as “experimentation platform” (or “system identi-
ﬁcation” stage), equal but not identical to the target
platform. The approach we will describe below is ap-
plicable to any type of resource, but for the sake of
instructiveness we consider the speciﬁc resource CPU
time, and denote by Cpu the fraction of that resource
which is allocated for the execution of Vm at some
ﬁxed time point. The (subsequent) performance of
Vm is measured by some variable Perf . We assume
that we are given some utility function which depends
on the distribution of Perf , among others.

Based on this, we formulate the overall goal of this
scenario as follows: ﬁnd a policy Π, which, based on
some ﬁxed input Feat, makes “optimal” allocation de-

4Work such as [Chiang et al., 2014] do not assume to
know the exact VM in advance but only a “similar” one.
However, such details are beyond the scope of this paper.

In

Oth

Con

Vm

Perf

Π

Cpu

Feat

Figure 2: Selection diagram D.

cisions w.r.t. Cpu, under the given utility function, in
the target setup, by making use of the target setup as
well as the experimentation platform in a smart way.
Note that, ignoring the availability of the experimen-
tal platform, this could be seen as a contextual bandit
problem. Below, we will focus on an important ingre-
dient towards the overall goal, which we will simply
refer to as “goal”: predict (the distribution) of Perf
from Feat, for all possible decision Cpu, in the tar-
get setup, by making use of the target setup as well
as the experimentation platform in a smart way. The
goal relates to the overall goal as follows: given we
achieved the former, the latter is reduced to an opti-
mization problem (in theory, obviously), as mentioned
in Section 2.

We introduce the following additional variables: Oth
denotes the “state” of the execution of the concurrent
workload Con at some ﬁxed time point, and In de-
notes some factor (diﬀerent from Cpu) via which Perf
of Vm is inﬂuenced by concurrent activities, such as
the available bandwidth of some IO device, at a ﬁxed
time point. (One could also think of diﬀerent scenar-
ios where Oth is some observed or hidden confounder,
such as time of the day or some class of users that
interact with both the execution of Vm and the con-
current VMs.)

Selection diagram and revelation structure. We
assume that the target platform is correctly described
by the selection diagram D depicted in Figure 2.5
That is: there is a (unknown) family of causal mod-
els (Mvm,con,π)(vm,con,π)∈I =: F such that the target
platform under the conﬁguration Vm = vm, Con =
con, Π = π is correctly described by Mvm,con,π and D
is the selection diagram induced by this family. We
assume that the target setup under the policy Π = π
to be described by Mπ := Mv,c,π, with v, c denoting
constants which are not known from the beginning.

5Note that a more accurate model would need to be
temporal since clearly the execution of Vm also inﬂuences
the concurrent VMs but this would lead to cycles in our
scenario which we aim to keep simple for the sake of in-
structiveness. Furthermore, the “complete” policy would
also allocate the resources for the concurrent workload.

By revelation structure we mean the information about
which constants v, c, of Mπ = Mv,c,π become revealed
at which stage (note that we assume D to be known
from the beginning). For the scenario at hand, we
assume the following revelation structure: at the be-
ginning of the experimental stage we get to know v,
i.e., the Vm that will be executed in the target setup;
the concurrent workload Con in the target setup, i.e.
c, become visible only during the target stage.

Now we can formally express the above mentioned goal
as follows: infer p(perf |v, c, π) for all π in a smart way,
using the experimental platform as well as the target
setup.

Our approach. We assume the following as given:
the selection diagram D (e.g. based on Section 4.1) and
the revelation structure. Keep in mind that, based on
the selection diagram D, we have

setup” of a system and some diﬀerent “target setup”,
and the goal is to harness information from the former
for “optimal” control on the latter. Due to space con-
straints, here we only give a summary of our approach,
while a more detailed version can be found in Section
A. Again, we assume that the variation of the system is
encoded by the selection diagram D (Figure 2) and this
D is given (e.g. based on Section 4.1). The intuition
is that the concurrent workload Con changes between
these setups in an unpredictable way, while Vm is in-
variant. Let v be the invariant state of VM , and c be
the state of Con in the target setup. Then, similarly as
in Section 4.2, the idea is to infer p(perf |in, cpu, v) in
the initial setup, then infer only p(in, feat|c) in the tar-
get setup, and based on this, calculate p(perf |v, c, π)
based on equation (5).

4.4 Combination of modules and discussion

p(perf |v, c, π) =

(2)

X

in,cpu,feat

p(perf |in, cpu, v) p(cpu|feat, π) p(in, feat|c).

We now describe an approach for the quintessential
scenario described above, which we believe can be gen-
eralized to other relevant scenarios as well:

Combining the modules. One ﬁrst has to infer a se-
lection diagram based on Section 4.1. This then serves
as input for Section 4.2, if one wants to design and inte-
grate experiments; or as an input to Section 4.3, if one
wants to integrate past experience of a time-varying
system. In principle, for more complex structures, it
is also possible to combine both.

1. Design and conduct of experiment: We know Vm
of the target setup, i.e. v, when setting up the ex-
periment and, additionally looking at D or equa-
tion (5), we conclude that the information which
is relevant and can at most be inferred during the
experiment is exactly p(perf |in, cpu, v). There-
fore, we should conduct the experiment as follows:
we should randomly vary In, Cpu during the ex-
periment, record In, Cpu, Perf , which means sam-
pling from p(perf |in, cpu, v), and based on this es-
timate the latter quantity.6

2. Inference in the target setup: We know Con of the
target setup, i.e. c, only during the target stage
and based on the above, we conclude that we have
to estimate p(in, feat|c) on-line during that stage.
3. Calculating the desired quantity: p(perf |sIn, sCpu)

can now be calculated based on equation (5).

4.3 Module 1C: addressing time-varying

systems

Here we address a quintessential version of the second
scenario described in Section 3.1: one has some “initial

6More formally, one could also design the experiment
based on writing down requirements regarding the selec-
tion diagram induced by target together with experimental
system. We leave such investigations for future work.

Advantages of module 1B. First note that our ap-
proach in fact solves the problem of expensive or im-
possible exploration during production (mentioned as
motivation for the ﬁrst scenario in Section 3.1): the
performance properties of the VM do not have to be
explored during production. Second, without a prin-
cipled and explicit modeling approach e.g. based on
causal models, which provide terms such as “causal
suﬃciency” or “causes of a variable”,
it is neither
clear how to design the experiment (i.e., what vari-
ables to randomly intervened upon), nor what to mea-
sure and regress on. Therefore previous approaches
which are not based on causal models [Chiang et al.,
2014, Zheng et al., 2009] are prone to errors. For in-
stance one may forget to vary In during the experi-
ment, or forget to regress on it, which would lead to
wrong predictions for the target setup:
for instance
a high concurrent workload Con slows down the per-
formance of the VM, Perf , via In as well as Cpu, but
if we do not vary and regress on In in the experiment,
we will only predict that part of the slowdown that
is produced via Cpu. Also regressing on children of
Perf (w.r.t. D) can lead to wrong results. Note that
this complies with previous research [Sch¨olkopf et al.,
2012] that suggests that predictions from causes to ef-
fects are better transferable than vice versa. Keep in
mind that our approach is also applicable to scenarios
where Con would stand for a hidden confounder with

an unknown distribution parameter Oth. Generally,
one could try to learn (in the sense of machine learn-
ing [Bishop, 2006]) things such as how to perform and
integrate the experiment, but one would always have
to rely on prior assumptions.

Advantages of of module 1C. The main advan-
tage of our method, compared to e.g. [Padala et al.,
2009] which is based on adaptive control, is that one
can encode and harness prior knowledge about which
mechanisms vary and which stay invariant. A general
advantage of causal models seems to be that they allow
to express and reason about why distributions change,
and not only observe that they change. Note that there
might be other reasonable approaches to the scenario
described above, e.g. based on treating the evolution
of Con as a stochastic process.

Remark. Recall that approaches such as RL often
rely on the assumption of invariant dynamics (Sec-
tion 2). In principle, the one-step dynamics of a ﬁxed
“single” computer can in fact be seen as invariant (if
one observes input, memory state, etc.). For instance,
in the above scenario, p(perf |do (cpu), vm, con) as a
function of all variables cpu, vm, con is invariant. Ne-
glecting the diﬃculties this would bring along in terms
of sampling and interpolation, one could in principle
try to infer this invariant dynamics and then causal
models would be superﬂuous. However, the long-term
dynamics of a large-scale system, e.g. a complete data
center, is rarely invariant (unless one would include
time itself into the observation but this would make in-
terpolation diﬃcult), due to its interaction with vary-
ing components in the environment, e.g. the Internet,
and changing hardware components. And usually cost
functions depend on the long-term, large-scale dynam-
ics. It seems that in contrast to usual RL, causal mod-
els are applicable to this continuum between “invari-
ant” and “completely varying”.

5 Addressing problem 2:

observation-level performance
attribution and debugging

Keep in mind the problem formulation in Section 3.2.
Here we elaborate the approach which we already out-
lined in that section, structuring it into “modules”
which we then combine and discuss in Section 5.3.

5.1 Module 2A: formalizing attribution and

debugging statements

Here we examine how the concepts “performance at-
tribution”7 and “performance debugging” can be for-
malized based on causal modeling language. We deem
both concepts closely related: attribution can be a
crucial step towards debugging. Note that we do not
claim that our proposed formalization is the only sen-
sible one.

We assume the following form of a performance attri-
bution statement: “Performance Y being y instead of
y0 can be attributed to X having value x instead of
the base case x0.” We translate this into the follow-
ing counterfactual statement: “Performance Y would
have been y0 if X had been x0 given in fact we ob-
served, among others, X = x, Y = y.” This in turn
we translate into the structural counterfactual proba-
bility p(Ydo X=x0 = y0|x, y, f ), where F denotes some
potential additional information about the situation
in which the above statements are done. For a perfor-
mance debugging statement we assume the following
form: “It would improve performance Y from the cur-
rent y to y′, if instead of its current state x, we would
set X to x′.” Stated this way, it seems natural to trans-
late this statement into the structural counterfactual
probability p(Ydo X=x′ = y′|x, y, f ), where F is meant
as above.

Generally, attribution statements do not have to have
a counterfactual form (e.g., one can also attribute
hypothetical performance), and also debugging state-
ments may have diﬀerent forms. However, here we
focus on scenarios where one observes a speciﬁc perfor-
mance of a system at a speciﬁc time point, and would
like to understand the current situation and possibly
improve it soon. Also, keep in mind that we assume a
pragmatic interpretation of counterfactual statements:
an idea of what would have happened in some past
situation can also include predictions about what will
happen in similar situations in the future.

5.2 Module 2B: approximate structural

counterfactuals

Even though computing systems are “more determin-
istic” than many other systems, due to interactions
with the environment and missing information, one
usually can only infer a GCM, and not a FCM, of a
computer system. Without an FCM though, counter-

7Clearly, “attribution” may have various meanings and,
in particular, a correct causal model may help for attribu-
tion in various ways.

factual probabilities (equation (5.1)) are generally not
uniquely determined, i.e., they cannot be derived from
a GCM. Here we show that nonetheless counterfactual
probabilities can be calculated approximately, and one
can know, from only the GCM, how wrong the approx-
imation is at most (on average).

Let M be a GCM over variable set V , and W ⊂ V
its root nodes. We deﬁne the approximate structural
counterfactual or approximate counterfactual, i.e., the
approximate probability of Y being y, had X been x,
given evidence E = e, relative to W as

pW (Ydo X=x = y|e) := X

p(y|do (x), w)p(w|e).

(3)

w

Let D(·k·) denote
the Kullback-Leibler diver-
gence, and H(·|·) the conditional Shannon entropy
[Cover and Thomas, 1991].
Now we bound the
average deviation between the structural and the
approximate counterfactual by H(E|W ), which is
calculable from only the GCM M . In particular, we
show that our approximation gets arbitrarily close to
the structural counterfactual when H(E|W ) goes to 0.

Proposition 1. Let M0 be a FCM over a set of dis-
crete variables V , that induces a GCM M , and let
W ⊂ V denote the root nodes in M . For all (sets
of) variables E, X, Y we have

D(p(Ydo X=x|E)kpW (Ydo X=x|E)) ≤ H(E|W )

(4)

(where p(Ydo X=x|E)
pW (Ydo X=x|E) w.r.t. M ).

is deﬁned w.r.t. M0 and

To give some intuition about the approximate coun-
terfactual and the proposition, we consider two spe-
cial cases: If M is already a “FCM” in the sense that
all its variables are completely determined by the root
nodes, then we have H(E|W ) = 0, and thus, based on
equation (7), both quantities coincide, which seems
natural.
If the evidence comprises the root nodes,
W ⊂ E, then the approximation amounts to the sim-
ple conditional p(y|do (x), w) (where w is the part of e
the corresponds to W ), similar as if we had evidence
on all background variables in a FCM. We prove (us-
ing monotonicity of the Kullback-Leibler divergence) a
generalization of Proposition 1 in Section B. Note that
the idea of a counterfactual deﬁnition based on only
the GCM has been mentioned in [Pearl, 2000, Section
7.2.2], but not been further investigated. In principle,
it seems, one could also apply a similar approach as
in [Balke and Pearl, 1994], i.e., considering all possi-
ble FCMs that explain given facts and then looking
at their implications, to the problem of counterfactual
inference from only a GCM. Note that, depending on
the speciﬁc setting and the available information, there

may be more suitable quantities as the approximate
counterfactual to encode counterfactual-like probabil-
ities.

5.3 Combination of modules and discussion

Combining the modules. To summarize, we pro-
pose the following approach, given a performance at-
tribution or debugging question Q:

1. Infer a GCM M , based on Section 4.1. Let W

denote its root nodes.

2. Translate Q into a counterfactual probability
query p(Ydo X=x′ = y′|x, y, f ), as described in Sec-
tion 5.1.

3. Calculate the approximate answer pW (Ydo X=x′ =
y′|x, y, f ) from the GCM M , based on Section 5.2,
if H(E|W ) is small.

Advantages. Previous approaches [Ostrowski et al.,
2011, Snee et al., 2015] which do not use a rigorous
and explicit modeling and reasoning framework such
as causal models may be more prone to making errors,
such as not distinguishing between correlation and cau-
sation, in certain cases. Furthermore, formalization
can be an important step towards more automated
treatment of attribution and debugging. Note that
generally, if there is a known, invariant distribution
over the root nodes of a causal model, then one can
optimize the performance on the population level, sim-
ilarly as suggested in Section 4. However, if one is
uncertain about the distribution of root nodes, or this
distribution are time-varying in an unpredictable way,
but one still believes in the invariance of the other
mechanisms, then the proposed observation-level at-
tribution and debugging can be seen as a suitable tool,
since the distributions on the root nodes are either up-
dated or completely determined, by the observation.

6 Some implementation aspects

While the main focus of the paper is on the concep-
tual level, here we want to discuss two implementation-
related points: the general problem of measurements
on a computer system, and an example of how the
detailed structure and data of a real system looks like.

Notes on measurements. Measurements of com-
puter systems were discussed e.g. by Carata et al.
[2014], Snee et al. [2015], and we build on this work
for our example system below. Two additional points
need to be mentioned: First, care needs to be taken
that the measurement process itself does not signif-

Local

Con vm

Kso

Uso

Xb

Xy

Kc

Uc

Kxo

Uxo

execution was in user mode”), and plot a sample of it
against one of its parents, Xb (“number of times the
target VM has been scheduled out by Xen due to a
block”) in Figure 4. To show that, in principle, infer-
ence of these kind of mechanisms is possible, we also
plot the posterior mean of a Gaussian process (GP),
which was trained on a separate training set.

Lat

7 Related work

Figure 3: Preliminary causal diagram GE.

·108

3

2

1

0

0

50

100

150

200

250

300

350

Figure 4: X-axis: Xb; y-axis: Uxo. Test points in gray,
GP posterior mean in blue.

icantly incure the performance of the system. Sec-
ond, the process of deciding which variables to mea-
sure and which not is diﬃcult but central, in particular
for sccessful debugging. In the theory of causal models
[Pearl, 2000], this process is usually completely ignored
and it is simply assumed that a sensible set of variables
is given. In the practice of computer systems, this pro-
cess has been studied, but it has not been investigated
how causal considerations can guide measurement se-
lection (see Section 4.4).

system, diagram and data. To il-
Example:
lustrate the structure and data of a real system,
we concisder a physical server with a Xen hypervi-
sor [Barham et al., 2003]. We run a web server within
one speciﬁc VM and measure its latency, Lat, (an im-
portant performance measure) upon requests, while
also measuring the number of concurrent VMs run-
ning on the physical server, Con vm, workload within
the VM, parallel to the web server, Local, and several
other properties within the VM (Kso, Uso, Kc, Uc) as
well as properties of the Xen hypervisor (variables in
Figure 3 that contain an “x”). A preliminary causal
diagram GE of the experimental system is depicted
in Figure 3. For illustration purposes, we pick one
variable, Uxo (“cycles the VM is scheduled out while

To our knowledge, this is the ﬁrst work that uses causal
models as introduced by Pearl [2000], Spirtes et al.
[2000] to address control and performance debugging
problems in cloud computing, or in complex computer
systems in general. Ostrowski et al. [2011], Snee et al.
[2015] study the problem of performance attribution
and debugging of cloud systems, but without causal
models, which, as brieﬂy discussed in Section 5.3, can
lead to errors. Baah et al. [2010] apply causal mod-
els for debugging of programs, but not of entire com-
puter systems. Bottou et al. [2013], Bareinboim et al.
[2015] apply causal models to RL problems, as we do.
However, their models are not applicable to the sce-
nario considered in Section 4. For instance, they do
not address the problem of designing, and transfer-
ring knowledge from a separate experimental (com-
puter) system for shaping controllers. Previous work
that does consider this problem [Chiang et al., 2014,
Zheng et al., 2009], or the problem of time-dependence
[Padala et al., 2009], is not based on causal models and
we explained in Section 4.4 why this can be problem-
atic.

8 Conclusions

The present work translates several important prob-
lems from the ﬁeld of cloud computing into causal
inference terminology such as selection diagrams and
counterfactuals. Based on this, we propose meth-
ods for inferring controllers from experimental and
time-varying systems; and for observation-level per-
formance attribution and debugging. Our work forms
only a start of what we consider a fruitful direction,
and leaves us with several open problems as well as
suggestions for future work. It would be highly desir-
able to develop tests, based on causal and/or prob-
abilistic methods, to check how accurate a model
is, or whether and what kind of additional measure-
ments should be taken. Moreover, due to strong inter-
temporal inﬂuences in real systems, temporal models
should have the potential of being more accurate.

Appendix

A Elaboration of Section 4.3 (addressing time-varying systems)

This is an elaborated version of Section 3.1. Recall that we address a quintessential version of the second scenario
described in Section 3.1: one has some “initial setup” of a system and some diﬀerent “target setup”, and the goal
is to harness information from the former for “optimal” control on the latter. (Note that this can be extended
to more than two stages.) Recall the following equation:

p(perf |v, c, π) = X

in,cpu,feat

p(perf |in, cpu, v) p(cpu|feat, π) p(in, feat|c).

(5)

Scenario, selection diagram and revelation structure. We consider a similar scenario as in Section 4.2,
in particular with the same selection diagram D, except for the following diﬀerences: there is no experimental
platform, but there are two episodes during production, i.e., the target platform is run with two diﬀerent setups,
which we refer to as “initial setup” and “target setup”. The intuition is simply that the concurrent workload
(applications) change between these setups. The goal, semi-formally, is to predict (the distribution) of Perf from
Feat, for all possible decision Cpu, in the target setup, by making use of the target as well as the initial setup (in
a smart way). We assume that Vm is invariant between both setups and we denote its state by v. The concurrent
applications Con in the target setup we denote by c. That is, we assume the following revelation structure: v is
given from the beginning; c is revealed at the beginning of the target episode. Now, formally expressed, the goal
is to infer p(perf |v, c, π) (for all π).

Our approach. We assume the following as given: the selection diagram D and the revelation structure. We
now describe an approach for the scenario described above, which we belief can be generalized to other relevant
scenarios as well.

1. Design of, and inference in, initial setup: For the initial policy, chose a π such that p(cpu|feat, π) has
support everywhere, for any feat. Based on this, estimate p(perf |in, cpu, v). (Note that we assume here that
all other mechanisms have support every as well, to keep things simple, but the approach can be extended
to other cases.)

2. Inference in target setup: Infer p(in, feat|c).

3. Calculating the desired quantity: Now calculate p(perf |v, c, π) based on equation (5).

B Generalized version and proof of Proposition 1

Here we state and prove a generalization of Proposition 1.

Proposition 2 (Generalization of Proposition 1). Let M0 be a FCM over discrete variables that induces a GCM
M . Let the triple (X, Y, Z) of (sets of) variables in M be such that (Y ⊥⊥ An(Z)|Z)M (i.e., are d-separated
[Pearl, 2000]) and X does not inﬂuence W := Z \ X. Let E be an arbitrary set of variables in M . Let (slightly
generalizing the deﬁnition from the main text):

pW (Ydo X=x = y|e) := X

p(y|do X = x, w)p(w|e).

w

Then

(where p(Ydo X=x|E) is deﬁned w.r.t. M0 and pW (Ydo X=x|E) w.r.t. M ).

D(p(Ydo X=x|E)kpW (Ydo X=x|E)) ≤ H(E|W )

(6)

(7)

This is a generalization of Proposition 1 since, whenever Z is a set of root nodes in M , the conditions (Y ⊥⊥
An(Z)|Z)M and X does not inﬂuence W := Z \ X are necessarily met.

Proof. Let U1 be the set (tuple) of background variables that inﬂuence W and U0 = U \ U1. Then

pW (Ydo X=x = y|e)
= X

p(y|do X = x, w)p(w|e)

w

= X

w,u0

= X

w,u0

= X

w,u0

p(y|do X = x, w, u0)p(w|e)p(u0|do X = x, w)

p(y|do X = x, w, u0)p(w|e)p(u0|w)

p(y|do X = x, w, u0)p(w|e)p(u0),

where equation (11) is due to the fact that X neither inﬂuences U0 nor W .

On the other hand, we have

p(Ydo X=x = y|e)
= X

p(y|do X = x, u)p(u|e)

=

=

=

=

=

=

=

=

=

u:p(u,e)>0

X

u0,u1:p(u0,u1,e)>0

p(y|do X = x, u0, u1)p(u0, u1|e)

X

p(y, w|do X = x, u0, u1)p(u0, u1|e)

u0,u1,w:p(u0,u1,e)>0

X

p(y, w|do X = x, u0, u1)p(u0, u1|e)

u0,u1,w:p(u0,u1,e),p(u0,u1,w)>0

X

p(y|do X = x, u0, u1, w)p(w|do X = x, u1, u0)p(u0, u1|e)

u0,u1,w:p(u0,u1,e),p(u0,u1,w)>0

X

p(y|do X = x, u0, w)p(w|do X = x, u1, u0)p(u0, u1|e)

u0,u1,w:p(u0,u1,e),p(u0,u1,w)>0

X

p(y|do X = x, u0, w)p(w|u1, u0)p(u0, u1|e)

u0,u1,w:p(u0,u1,e),p(u0,u1,w)>0

X

p(y|do X = x, u0, w)p(w|u0, u1, e)p(u0, u1|e)

u0,u1,w:p(u0,u1,e),p(u0,u1,w)>0

X

p(y|do X = x, u0, w)p(w, u0, u1|e)

u0,u1,w:p(u0,u1,e),p(u0,u1,w)>0

X

p(y|do X = x, u0, w)p(w, u0, u1|e)

u0,u1,w:p(u0,u1,e,w)>0

= X

p(y|do X = x, u0, w) X

p(w, u0, u1|e)

u0,w:p(u0,e,w)>0

= X

u0,w:p(u0,e,w)>0

u1:p(u0,u1,e,w)>0

p(y|do X = x, u0, w)p(w, u0|e)

= X

u0,w

p(y|do X = x, u0, w)p(w, u0|e),

(8)

(9)

(10)

(11)

(12)

(13)

(14)

(15)

(16)

(17)

(18)

(19)

(20)

(21)

(22)

(23)

(24)

(25)

(26)

where equation (19) is due to Markovianity and (Y ⊥⊥ An(Z)|Z)M , which implies (Y ⊥⊥ U1|Z)M , and thus
(Y ⊥⊥ U1|W )Mdo X=x, equation (20) follows from the fact that X does not inﬂuence W , equation (21) follows
from the fact that U1 already determines W .

Note that p(w|e)p(u0) = 0 implies p(w, u0|e) = 0 and therefore D[p(Ydo X=x|E)kpW (Ydo X=x|D)] is deﬁned .

Now we can calculate

D[p(Ydo X=x|E)kpW (Ydo X=x|E)]
= X

p(e)D[p(Ydo X=x|e)kpW (Ydo X=x|e)]

e

≤ X

e

p(e)D[p(W, U0|e)kp(W |e)p(U0)]

= X

e,w,u0

= X

e,w,u0

p(w, u0, e) log

p(w, u0, e) log

p(w, u0|e)
p(w|e)p(u0)

p(w, u0, e)
p(w, e)p(u0)

= I(W, E : U0)
= I(W : U0) + I(E : U0|W )
= 0 + H(E|W ) − H(E|U0, W ),

(27)

(28)

(29)

(30)

(31)

(32)
(33)

(34)

where inequality (29) follows from the monotonicity (which follows from the chain rule) of the Kullback-Leibler
divergence [Cover and Thomas, 1991] together with equations (26) and (12), equation (33) is the chain rule for
mutual information, and I(W : U0) = 0 is due to U0 not inﬂuencing W and Markovianity.

References

M. Armbrust, A. Fox, R. Griﬃth, A. D. Joseph, R. Katz, A. Konwinski, G. Lee, D. Patterson, A. Rabkin,

I. Stoica, et al. A view of cloud computing. Communications of the ACM, 53(4):50–58, 2010.

G. K. Baah, A. Podgurski, and M. J. Harrold. Causal inference for statistical fault localization. In Proceedings

of the 19th international symposium on Software testing and analysis, pages 73–84. ACM, 2010.

A. Balke and J. Pearl. Counterfactual probabilities: Computational methods, bounds and applications.

In
Proceedings of the Tenth international conference on Uncertainty in artiﬁcial intelligence, pages 46–54. Morgan
Kaufmann Publishers Inc., 1994.

E. Bareinboim and J. Pearl. Transportability of causal eﬀects: Completeness results. In Proceedings of the 26th
National Conference on Artiﬁcial Intelligence (AAAI), pages 698–704. AAAI Press, Menlo Park, CA., 2012.

E. Bareinboim, A. Forney, and J. Pearl. Bandits with unobserved confounders: A causal approach. In Advances

in Neural Information Processing Systems, pages 1342–1350, 2015.

P. Barham, B. Dragovic, K. Fraser, S. Hand, T. Harris, A. Ho, R. Neugebauer, I. Pratt, and A. Warﬁeld. Xen and
the art of virtualization. In Proceedings of the Nineteenth ACM Symposium on Operating Systems Principles,
SOSP ’03, pages 164–177. ACM, 2003.

C. M. Bishop. Pattern Recognition and Machine Learning (Information Science and Statistics). Springer-Verlag

New York, Inc., Secaucus, NJ, USA, 2006.

L. Bottou, J. Peters, J. Quinonero-Candela, D. X. Charles, D. M. Chickering, E. Portugaly, D. Ray, P. Simard,
and E. Snelson. Counterfactual reasoning and learning systems: The example of computational advertising.
The Journal of Machine Learning Research, 14(1):3207–3260, 2013.

L. Carata, O. Chick, J. Snee, R. Sohan, A. Rice, and A. Hopper. Resourceful: ﬁne-grained resource accounting

for explaining service variability. Technical report, University of Cambridge, Computer Laboratory, 2014.

R. C. Chiang, J. Hwang, H. H. Huang, and T. Wood. Matrix: Achieving predictable virtual machine performance

in the clouds. In 11th International Conference on Autonomic Computing (ICAC 14), pages 45–56, 2014.

T. Cover and J. Thomas. Elements of Information Theory. Wileys Series in Telecommunications, New York,

1991.

D. Hume and C. W. Hendel. An inquiry concerning human understanding, volume 49. Bobbs-Merrill Indianapolis,

1955.

J. Langford and T. Zhang. The epoch-greedy algorithm for multi-armed bandits with side information.

In

Advances in neural information processing systems, pages 817–824, 2008.

J. M. Mooij, J. Peters, D. Janzing, J. Zscheischler, and B. Sch¨olkopf. Distinguishing cause from eﬀect using

observational data: methods and benchmarks. arXiv preprint arXiv:1412.3773, 2014.

K. Ostrowski, G. Mann, and M. Sandler. Diagnosing latency in multi-tier black-box services. In 5th Workshop

on Large Scale Distributed Systems and Middleware (LADIS 2011), volume 3, page 14, 2011.

P. Padala, K.-Y. Hou, K. G. Shin, X. Zhu, M. Uysal, Z. Wang, S. Singhal, and A. Merchant. Automated control
of multiple virtualized resources. In Proceedings of the 4th ACM European conference on Computer systems,
pages 13–26. ACM, 2009.

J. Pearl. Causality. Cambridge University Press, 2000.

J. Pearl. Causal inference in statistics: An overview. Statistics Surveys, 3:96–146, 2009.

B. Sch¨olkopf, D. Janzing, J. Peters, E. Sgouritsa, K. Zhang, and J. Mooij. On causal and anticausal learning.

arXiv preprint arXiv:1206.6471, 2012.

J. Snee, L. Carata, O. R. Chick, R. Sohan, R. M. Faragher, A. Rice, and A. Hopper. Soroban: attributing
latency in virtualized environments. In Proceedings of the 7th USENIX Conference on Hot Topics in Cloud
Computing, pages 11–11. USENIX Association, 2015.

P. Spirtes, C. Glymour, and R. Scheines. Causation, prediction, and search. MIT, Cambridge, MA, 2nd edition,

2000.

R. S. Sutton and A. G. Barto. Reinforcement learning: An introduction. MIT press, 1998.

N. Vasi´c, D. Novakovi´c, S. Miuˇcin, D. Kosti´c, and R. Bianchini. Dejavu: accelerating resource allocation in
virtualized environments. In ACM SIGARCH computer architecture news, volume 40, pages 423–436. ACM,
2012.

W. Zheng, R. Bianchini, G. J. Janakiraman, J. R. Santos, and Y. Turner. Justrunit: Experiment-based man-

agement of virtualized data centers. In Proc. USENIX Annual technical conference, 2009.

