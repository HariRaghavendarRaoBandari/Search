6
1
0
2

 
r
a

 

M
2
2

 
 
]
L
C
.
s
c
[
 
 

2
v
9
5
0
6
0

.

3
0
6
1
:
v
i
X
r
a

GeneratingNaturalQuestionsAboutanImageNasrinMostafazadeh1,IshanMisra2,JacobDevlin4,LarryZitnick3,MargaretMitchell4XiaodongHe4,LucyVanderwende41UniversityofRochester,2CarnegieMellonUniversity,3FacebookAIResearch,4MicrosoftResearchnasrinm@cs.rochester.edu,lucyv@microsoft.comAbstractTherehasbeenanexplosionofworkinthevision&languagecommunityduringthepastfewyearsfromimagecaption-ingtovideotranscription,andansweringquestionsaboutimages.Thesetasksfo-cusonliteraldescriptionsoftheimage.Tomovebeyondtheliteral,wechoosetoexplorehowquestionsaboutanim-ageoftenaddressabstracteventsthattheobjectsevoke.Inthispaper,weintro-ducethenoveltaskof‘VisualQuestionGeneration’(VQG),wherethesystemistaskedwithaskinganaturalanden-gagingquestionwhenshownanimage.Weprovidethreedatasetswhichcoveravarietyofimagesfromobject-centrictoevent-centric,providingdifferentandmoreabstracttrainingdatathanthestate-of-the-artcaptioningsystemshaveusedthusfar.Wetrainandtestseveralgen-erativeandretrievalmodelstotacklethetaskofVQG.Evaluationresultsshowthatwhilesuchmodelsaskreasonableques-tionsgivenvariousimages,thereisstillawidegapwithhumanperformance.Ourproposedtaskoffersanewchallengetothecommunitywhichwehopecanspurfur-therinterestinexploringdeeperconnec-tionsbetweenvision&language.1IntroductionWearewitnessingarenewedinterestininterdis-ciplinaryAIresearchinvision&language,fromdescriptionsofthevisualinputsuchasimagecap-tioning(Chenetal.,2015;Fangetal.,2014;Don-ahueetal.,2014;Chenetal.,2015)andvideotranscription(Rohrbachetal.,2012;Venugopalanetal.,2015),totestingcomputerunderstandingofNaturalQuestions:-Wasanyoneinjuredinthecrash?-Isthemotorcyclistalive?-Whatcausedthisaccident?GeneratedCaption:-Amanstandingnexttoamotorcycle.Figure1:Exampleimagealongwithitsnaturalquestionsandautomaticallygeneratedcaption.animagethroughquestionanswering(Antoletal.,2015;MalinowskiandFritz,2014).Themostes-tablishedworkinvision&languagecommunityis‘imagecaptioning’,wherethetaskistopro-ducealiteraldescriptionoftheimage.Thistaskhasgreatvalueforcommunitiesofpeoplewhoarelow-sightedorcannotseeinallorsomeenviron-ments,butforothers,thedescriptiondoesnotaddanythingtowhatapersonhasalreadyperceived.Thepopularityoftheimagesharingapplica-tionsinsocialmediaisevidenceforcommentingonpicturesasaverynaturaltask.Apersonmightrespondtoanimagewithashortcommentsuchas‘cool’or‘nicepic’,orapersonmightseeanimageandaskaquestion.ImaginesomeonehassharedtheimageinFigure1.Whatistheveryﬁrstquestionthatcomestomind?Yourquestionismostprobablyoneofthequestionslistednexttotheimage,expressingconcernaboutthemotorcy-clist(whoisnotevenpresentintheimage).Nat-uralquestionsarenotaboutwhatisseen,thepo-licemenorthemotorcycle,butratheraboutwhatisinferredgiventheseobjects,e.g.,anaccidentorinjury.Assuch,questionsareoftenaboutab-stractconcepts,i.e.,eventsorstates,incontrasttotheconcretetermsusedinimagecaptioning.Itisclearthatthecorrespondingautomaticallygener-atedcaption1presentsonlyaliteraldescriptionof1Throughoutthispaperweusethestate-of-the-artcap-objects.Tomovebeyondtheliteraldescriptionofim-agecontent,weintroducethenoveltaskof‘VisualQuestionGeneration(VQG)’,wheregivenanimage,thesystemshould‘askanatural2anden-gagingquestion’.Generatingquestionsisanim-portanttaskinNLPandismorethanasyntactictransformationonadeclarativesentence(Vander-wende,2008).Asystemthatasksrelevantandto-the-pointquestionscanbeusedasanintegralcomponentinanyconversationalagent,eithertosimplyengagetheuserortoelicittask-speciﬁcinformation.Furthermore,decidingwhattoaskaboutdemonstratesunderstanding;someeduca-tionalmethodsassessstudentunderstandingbytheirabilitytoaskrelevantquestions3.Interest-ingly,askingagoodquestionappearstobeacog-nitiveabilityuniquetohumansamongotherpri-mates(Jordania,2006).Thecontributionsofthispapercanbesum-marizedasfollows:(1)inordertoenabletheVQGresearch,wecarefullycreatedthreedatasetswithatotalof75,000questions,whichrangefromobject-toevent-centricimages,whereweshowthatVQGcoversawiderangeofabstracttermsincludingeventsandstates(Section3).(2)wecollected25,000goldcaptionsforourevent-centricdatasetandshowthatthisdatasetpresentschallengestothestate-of-the-artimagecaption-ingmodels(Section3.3).(3)weperformanalysisofvariousgenerativeandretrievalapproachesandshowthatend-to-enddeepneuralmodelsoutper-formothergenerativeapproaches(Section4).(4)weprovideasystematicevaluationmethodologyforthistask,whereweshowthattheautomaticmetric∆BLEUbestcorrelateswithhumanjudg-ments(Section5.3).Theresultsshowthatwhilethemodelslearntogeneratepromisingquestions,thereisstillalargegaptomatchhumanperfor-mance,makingthegenerationofrelevantandnat-uralquestionsaninterestingandpromisingnewchallengetothecommunity.2RelatedWorkForimagecaptioning,datasetshavefocusedonobjectsprimarily,e.g.PascalVOC(Everinghametal.,2010)andMicrosoftCOCO(MSCOCO)(Linetal.,2014).MSCOCO,e.g.,includescomplextioningsystem(Fangetal.,2014)togeneratecaptions.2Anaturalquestionisaquestionthatsomeonewouldaskinanaturalhuman-humanconversation.3http://rightquestion.org/everydaysceneswith91basicobjectsin328kim-ages,eachpairedwith5captions.Videoprocess-ingandactiondetectionareconcernedwitheventdetection,butdonotincludeatextualdescriptionoftheevent(Yaoetal.,2011b;Andrilukaetal.,2014;Chaoetal.,2015;Xiongetal.,2015)).Thenumberofactionsineachdatasetisstillrelativelysmall,rangingfrom40(Yaoetal.,2011a)to600(Chaoetal.,2015)andallinvolvehumanactivity(e.g.‘cooking’,‘gardening’,‘ridingabike’).Inourwork,wearefocusedongeneratingquestionsforstaticimagesofevents,suchas‘ﬁre’,‘explo-sion’or‘snowing’,whichhavenotyetbeeninves-tigatedinanyoftheabovedatasets.VisualQuestionAnsweringisarelativelynewtaskwherethesystemprovidesananswertoaquestionabouttheimagecontent.Themostno-table,VisualQuestionAnswering(VQA)(Antoletal.,2015),isanopen-ended(free-form)dataset,inwhichboththequestionsandtheanswersaremanuallycreatedbycrowd-sourcingonAmazonMechanicalTurk,withworkerspromptedtoaskavisuallyveriﬁablequestionwhichwill‘stumpasmartrobot’.Questionsrangefrom‘isthisavegetarianpizza?’to‘isitrainy?’.(Gaoetal.,2015)usedsimilarmethodologytocreateavisualquestionansweringdatasetinChinese.COCO-QA(CQA)(Renetal.,2015),incontrast,doesnotusehuman-authoredquestions,butgeneratesquestionsautomaticallyfromimagecaptionsofMSCOCOdatasetbyapplyingasetoftransfor-mationrulestogeneratethewh-question.Theex-pectedanswersinCQAarebydesignlimitedtoobjects,numbers,colors,orlocations.Amorein-depthanalysisofVQAandCQAdatasetswillbepresentedinSection3.1.Inthiswork,wefocusonquestionswhichareinterestingforapersontoanswer,notquestionsdesignedtotestcomputervision.Previousworkonquestiongenerationfromtex-tualinputhasfocusedontwoaspects:thegram-maticality(Wolfe,1976;MitkovandHa,2003;HeilmanandSmith,2010)andthecontentfocusofquestiongeneration,i.e.,“whattoaskabout”.Forthelatter,severalmethodshavebeenexplored:(Beckeretal.,2012)createﬁll-in-the-blankques-tions,(MazidiandNielsen,2014)and(Lindbergetal.,2013)usemanuallyconstructedquestiontemplates,while(Labutovetal.,2015)usecrowd-sourcingtocollectasetoftemplatesandthenrankthepotentiallyrelevanttemplatesfortheselectedcontent.Toourknowledge,neitheraretrievalFigure2:ExamplerightandwrongquestionsforthetaskofVQG.modelnoradeeprepresentationoftextualinput,presentedinourwork,haveyetbeenusedtogen-eratequestions.3DataCollectionMethodologyTaskDeﬁnition:Givenanimage,thetaskistogenerateanaturalquestionwhichcanpotentiallyengageahumaninstartingaconversation.Ques-tionsthatarevisuallyveriﬁable,i.e.,thatcanbeansweredbylookingatonlytheimage,areout-sidethescopeofthistask.Aquestion,e.g.,aboutthenumberofhorsesorthecoloroftheﬁeldinFigure2isnotofinterest.Althoughinthispaperwefocusonaskingaquestionaboutanimageinisolation,addingpriorcontextorhistoryofcon-versationisthenaturalnextstepinthisproject.WecollectedtheVQGquestionsbycrowd-sourcingthetaskonAmazonMechanicalTurk(AMT).Weprovidedetailsonthepromptandspeciﬁcinstructions(forallthecrowdsourcingtasksinthispaper)inthesupplementarymaterial.Ourpromptwasverysuccessfulatcapturingnon-literalquestions,asthegoodquestioninFigure2shows.InthefollowingSections,wedescribeourprocessforselectingtheimagestobeincludedintheVQGdataset.WestartwithimagesfromMSCOCO,whichallowscomparisonwithVQAandCQAquestions.Asitismorenaturalforpeo-pletoaskquestionsaboutevent-centricimages,however,wealsoexploresourcingimagesfromFlickrandfromqueryinganimagesearchengine.Eachdatasourceisrepresentedby5,000images,with5questionsperimage.3.1VQGcoco−5000andVQGFlickr−5000Asourﬁrstdataset,wecollectedVQGques-tionsforasampleofimagesfromtheMSCOCOdataset4.Inordertoenablecomparisonswithrelateddatasets,wesampled5,000imagesofMSCOCOwhichwerealsoannotatedbyCQAdataset(Renetal.,2015)andbyVQA(Antoletal.,2015).WenamethisdatasetVQGcoco−5000.Table1showsasampleMSCOCOimagealong4http://mscoco.org/withannotationsinthevariousdatasets.AstheCQAquestionsaregeneratedbyruleapplicationfromcaptions,theyarenotalwayscoherent.TheVQAquestionsarewrittentoevaluatethedetailedvisualunderstandingofarobot,sotheirquestionsaremainlyvisuallygroundedandliteral.DatasetAnnotationsCOCO-Amanholdingaboxwithalargechocolatecovereddonut.CQA-Whatisthemanholdingwithalargechocolate-covereddoughnutinit?VQA-Isthisalargedoughnut?VQG-Whyisthedonutsolarge?-Isthatforaspeciﬁccelebration?-Haveyouevereatenadonutthatlargebefore?-Isthatabigdonutoracake?-Wheredidyougetthat?Table1:Datasetannotationsontheaboveimage.InFigure3weprovidestatisticsforthevariousannotationsonthatportionoftheMSCOCOim-ageswhicharerepresentedintheVQGcoco−5000dataset.InFigure3(a)wecomparethepercent-ageofobject-mentionsineachoftheannota-tions.Object-mentionsarewordsassociatedwiththegold-annotatedobjectboundaryboxes5aspro-videdwithMSCOCOdataset.Naturally,COCOcaptions(greenbars)havethehighestpercentageoftheseliteralobjects.Sinceobject-mentionsaretheanswerofVQAandCQAquestions,thosequestionscontainnaturallycontainobjectslessfrequently.Hence,weseethatVQGquestionsin-cludethementionofmoreofthoseliteralobjects.Figure3(b)showsthatCOCOcaptionshavealargervocabularysize,whichreﬂectstheirlongerandmoredescriptivesentences.VQGshowsarelativelylargevocabularysizeaswell,indicat-inggreaterdiversityinquestionformulationthanVQAandCQA.Moreover,Figure3(c)showsthattheverbpartofspeechisrepresentedwithhighfrequencyinourdataset.Figure3(d)depictsthepercentageofabstracttermssuchas‘think’or‘win’inthevocabulary(Ferraroetal.,2015).ThisfurthersupportsthatVQGindeedcoversmoreabstractconcepts.Fur-thermore,Figure3(e)showsinter-annotationtex-5NotethatMSCOCOannotatesonly91objectcategories.tualsimilarityaccordingtotheBLEUmetric(Pa-pinenietal.,2002).Interestingly,VQGshowsthehighestinter-annotatortextualsimilarity,whichreﬂectsontheexistenceofconsensusamonghu-manforaskinganaturalquestion.(a)(b)(c)(d)(e)Figure3:ComparisonofvariousannotationsonMSCOCOdataset.(a)Goldobjectsusedinanno-tations.(b)Vocabularysize(c)VerbPOSpercent-age(d)Abstractvsconcretetokenpercentage(e)Inter-annotationtextualsimilarityMSCOCOdatasetislimitedintermsofthecon-ceptsitcovers,duetoitspre-speciﬁedsetofob-jectcategories.WordfrequencyinVQGcoco−5000dataset,asdemonstratedinFigure4,bearsthisout,withthewords‘cat’and‘dog’thefourthandﬁfthmostfrequentwordsinthedataset.Notshowninthefrequencygraphisthatwordssuchas‘wedding’,‘injured’,or‘accident’areattheverybottomoffrequencyrankinglist.ThisobservationmotivatedthecollectionoftheVQGFlickr−5000dataset,withimagesappearinginphotoalbumsonFlickr6.Thedetailsaboutthisdatasetcanbefoundinthesupplementarymaterial.3.2VQGBing−5000Toobtainamorerepresentativevisualizationofspeciﬁceventtypes,wequeriedasearchengine7with1,200event-centricquerytermswhichwereobtainedasfollows:weaggregatedall‘event’and‘process’hyponymsinWordNet(Miller,1995),1,000mostfrequentTimeBankevents(Puste-jovskyetal.,2003)andasetofmanuallycurated30stereotypicalevents,fromwhichweselected6http://www.flickr.com7https://datamarket.azure.com/dataset/bing/searchFigure4:Frequencygraphoftop40wordsinVQGcoco−5000dataset.thetop1,200queriesbasedonProjectGutenbergwordfrequencies.Foreachquery,wecollectedtheﬁrstfourtoﬁveimagesretrieved,foratotalof5,000images,havingﬁrstusedcrowdsourcingtoﬁlteroutimagesdepictinggraphicsandcar-toons.AsimilarwordfrequencyanalysisshowsthattheVQGBing−5000datasetindeedcontainsmorewordsaskingaboutevents:happen,work,causeappearintop40words,whichwasouraimincreatingtheBingdataset.Statistics:Ourthreedatasetstogethercoveragoodrangeofvisualconceptsandevents,totaling15,000imageswith75,000questions.Figure6drawsthehistogramofnumberoftokensinVQGquestions,wheretheaveragequestionlengthis6tokens.Figure5visualizesthen-gramdistribution(withnsetto6)ofquestionsinthethreeVQGdatasets.Table2showsthestatisticsofthecrowd-sourcingtask.TogetherwiththispaperwearereleasinganextendedsetofVQGdatasettothecommunity.Wehopethattheavailabilityofthisdatasetwillencouragetheresearchcommunitytotacklemoreend-goalorientedvision&languagetasks.#allimages15,000#questionsperimage5#allworkersparticipated308Max#questionswrittenbyoneworker6,368Averageworktimeamongworkers(sec)106.5Medianworktimeamongworkers(sec)23.0Averagepaymentperquestion(cents)6.0Table2:Statisticsofcrowdsourcingtask,aggre-gatingallthreedatasets.3.3CaptionsBing−5000ThewordfrequenciesofquestionsabouttheVQGBing−5000datasetindicatethatthisdatasetFigure5:VQGN-gramsequences.‘End’tokendistinguishesnaturalendingwithn-gramcut-off.Figure6:AverageannotationlengthofthethreeVQGdatasets.issubstantiallydifferentfromtheMSCOCOdataset.Tofurtherexplorethisdifference,wecrowdsourced5captionsforeachimageintheBingdatasetusingthesamepromptasusedtosourcetheMSCOCOcaptions.Table3showstheevaluationresultsofthesamecaptioningsystemonourBingdatasetascomparedtotheMSCOCOdataset,measuredbythestandardBLEU(Pap-inenietal.,2002)andMETEOR(DenkowskiandLavie,2014)metrics.ThewidegapintheresultsfurtherconﬁrmsthatindeedtheVQGBing−5000datasetcoversanewclassofimages;wehopetheavailabilityofthisnewdatasetwillencouragein-cludingmorediversedomainsforimagecaption-ing.BLEUMETEORBingMSCOCOBingMSCOCO0.1010.2910.1510.247Table3:Imagecaptioningresults4ModelsInthisSectionwepresentseveralgenerativeandretrievalmodelsfortacklingthetaskofVQG.ForalltheforthcomingmodelsweusetheVGG16(Si-monyanandZisserman,2014)networkarchitec-tureforcomputingdeepconvolutionalimagefea-tures.Weprimarilyusethe4096-dimensionalout-puttheﬁnalhiddenlayer(fc7)astheinputtothegenerationmodels.4.1GenerativeModelsFigure7representsanoverviewofourthreegen-erativemodels.TheMELMmodel(Fangetal.,2014)isapipelinestartingfromasetofcandi-datewordprobabilitieswhicharedirectlytrainedonimages,whichthengoesthroughamaximumentropy(ME)languagemodel.TheMTmodelisaSequence2Sequencetranslationmodel(Choetal.,2014)whichdirectlytranslatesadescriptionofanimageintoaquestion.Thesetwomodelstendedtogeneratelesscoherentsentences,detailsofwhichcanbefoundinthesupplementarymaterial.Weobtainedthebestresultsbyusinganend-to-endneuralmodel,GRNN,whichisasfollows.GatedRecurrentNeuralNetwork(GRNN):Thisgenerationmodelisbasedonthestate-of-the-artMultimodalRecurrentNeuralNetworksys-temsusedforimagecaptioning(Devlinetal.,2015;Vinyalsetal.,2015).First,wetrans-formthefc7vectortoa500-dimensionalvec-torwhichservesastheinitialrecurrentstatetoa500-dimensionalGatedRecurrentUnit(GRU).WeproducetheoutputquestiononewordatatimeusingtheGRU,untilwehittheend-of-sentencetoken.WetraintheGRUandthetransformationmatrixjointly,butwedonotback-propagatetheCNNduetothesizeofthetrainingdata.Theneu-ralnetworkistrainedusingStochasticGradientDescentwithearlystopping,anddecodedusingabeamsearchofsize8.Thevocabularyconsistsofallwordsseen3ormoretimesinthetraining,whichamountsto1942uniquetokensinthefulltrainingset.Unknownwordsaremappedtotoan<unk>tokenduringtraining,butwedonotallowthedecodertoproducethistokenattesttime.Figure7:ThreedifferentgenerativemodelsfortacklingthetaskofVQG.4.2RetrievalMethodsRetrievalmodelsusethecaptionofanearestneighbortrainingimagetolabelthetestimage(Hodoshetal.,2013;Devlinetal.,2015;Farhadietal.,2010;Ordonezetal.,2011).Forthetaskofimagecaptioning,ithasbeenshownthatupto80%ofthecaptionsgeneratedbyanearstate-of-the-artgenerationapproach(Vinyalsetal.,2015)wereexactlyidenticaltothetrainingcaptions,whichsuggeststhatreusingtrainingannotationscanachievegoodresults.Moreover,basicnear-estneighborapproachestoimagecaptioningonMSCOCOdatasetareshowntooutperformgener-ationmodelsaccordingtoautomaticmetrics(De-vlinetal.,2015).Theperformanceofretrievalmodelsofcoursedependsonthediversityofthedataset.Weimplementeddifferentretrievalmodelscus-tomizedforthetaskofVQG.Astheﬁrststep,wecomputeKnearestneighborsforeachtestim-ageusingthefc7featurestogetacandidatepool.Eachimageinthepoolhasﬁvequestionlabels.Wedeﬁnetheone-bestquestiontobethequestionwiththehighestsemanticsimilarity8totheotherfourquestions.ThisresultsinapoolofKcan-didates,eachhavingonequestion.WeobtainedthemostcompetitiveresultsbysettingKdynam-ically,asopposedtotheearlierworkswhichﬁxKthroughouttesting.Weobservedthatcandi-dateimagesbeyondacertaindistancemadethepoolnoisy,andsoweestablishaparametercalledmax−distance.Moreover,ifthereisacandidateinpoolwhichisverysimilartothetestimage,itiscloseenoughtobeusedasthehypothesisandtherestofthecandidatepoolcanbeignored.Wesetmin−distanceparametertoaddressthis9.Theseparametersweretunedonthecorrespond-ingvalsetsusingtheSmoothed-BLEU(LinandOch,2004)metricagainstthehumanreferencequestions.Thefollowingsettingswereusedfor8WeuseBLEUtocomputetextualsimilarity.Thispro-cesseliminatesoutlierquestionsperimage.9Attesttime,thefrequencyofﬁndingatrainsetimagewithdistance≤0.1is7.68%,8.4%and3.0%inCOCO,FlickrandBingdatasetsrespectively.ourﬁnalretrievalmodels:–1-NN:SetK=1,whichretrievestheclosestim-ageandemitsitsone-best.–K-NN+min:SetK=30withmax−distance=0.35,andmin−distance=0.1.Among30one-bestquestions,ﬁndthequestionwiththehighestsimilaritytotherestofthepoolandemitthat.Wecomputetheinter-poolsimilaritybytwometrics:Smoothed-BLEUandGenSim10.Table4showsafewexampleimagesalongwiththegenerationsofourbestperformingsystems.InthenextSectionwewilldiscussthedetailsofeval-uationmethodology.5EvaluationWhileinVQGthesetofpossiblequestionsisnotlimited,thereisconsensusamongthenaturalquestions(discussedinSection3.1)whichenablesmeaningfulevaluation.Althoughhumanevalua-tionistheidealformofevaluation,itisimpor-tanttoﬁndanautomaticmetricthatstronglycorre-lateswithhumanjudgmentinordertobenchmarkprogressonthetask.5.1HumanEvaluationThequalityoftheevaluationisinpartdeterminedbyhowtheevaluationispresented.Forinstance,itisimportantforthehumanjudgestoseevar-ioussystemhypothesesatthesametimeinor-dertogiveacalibratedrating.WecrowdsourcedourhumanevaluationonAMT,askingthreecrowdworkerstoeachratethequalityofcandidateques-tionsonathree-pointsemanticscale,rangingfrom‘disagree’to‘agree’.Weaskedthecrowdworkerstoconsiderthefollowingcriterion,theresultsofwhicharepresentedinpart1ofTable5:“Thisquestionisoneoftheﬁrstnaturalquestionsthatcomestomindifmyfriendsharesthispicturewithme.”10GenSimisaverageWord2Vec(Mikolovetal.,2013)similaritymetricwhichweuseforcomputingsentence-leveltextualsimilarity.Q.ExplosionHurricaneRainCloudCarAccidentHuman-Whatcausedthisexplosion?-Wasthisexplosionanaccident?-Whatcausedthedamagetothiscity?-Whathappenedtothisplace?-Arethoserainclouds?-Diditrain?-Didthedriversofthisaccidentlivethroughit?-Howfastweretheygoing?GRNN-Howmuchdidtheﬁrecost?-Whatisbeingburnedhere?-Whathappenedtothecity?-Whatcausedthefall?-Whatkindofcloudsarethese?-Wasthereabadstorm?-Howdidthecarcrash?-Whathappenedtothetrailer?KNN-Whatcausedthisﬁre?-Whatstatewasthisearthquakein?-Diditrain?-Wasanybodyhurtinthisaccident?Caption-Atrainwithsmokecomingfromit.-Apileofdirt.-Somecloudsinacloudyday.-Amanstandingnexttoamotorcycle.Table4:SamplegenerationsbydifferentsystemsonVQGbing−5000.Therowsareasfollows:theimage,thequeryterm,HumanconsensusandHumanrandom,GRNNbingandGRNNall,KNN+minbleu−all,Fang2014captioningsystem.5.2AutomaticEvaluationThegoalofautomaticevaluationistomeasurethesimilarityofsystem-generatedquestionhypothe-sesandthecrowdsourcedquestionreferences.Tocapturen-gramoverlapandtextualsimilaritybe-tweenhypothesesandreferences,weusestandardMachineTranslationmetrics,BLEU(Papinenietal.,2002)andMETEOR(DenkowskiandLavie,2014).WeuseBLEUwithequalweightsupto4-gramsanddefaultsettingofMETEORversion1.5.Additionallyweuse∆BLEU(Galleyetal.,2015)whichisspeciﬁcallytailoredtowardsgen-erationtaskswithdiversereferences,suchascon-versations.Theresultsoftheautomaticmetricsformodelscanbefoundinpart2ofTable5.Thepairwisecorrelationalanalysisofhumanandau-tomaticmetricsispresentedinTable6,wherewereportonPearson’sr,Spearman’sρandKendall’sτcorrelationcoefﬁcients.Asthistablereveals,∆BLEUstronglycorrelateswithhumanjudgmentandwesuggestitasthemainevaluationmetricforthetaskofVQG.5.3ResultsInthissectionwepresentthehumanandauto-maticmetricevaluationresultsofthemodelsin-troducedearlier.WerandomlydividedeachVQG-5000datasetintotrain(50%),val(25%)andtest(25%)sets.Inordertoshedsomelightondif-ferencesbetweenourthreedatasets,wepresenttheevaluationresultsseparatelyoneachdatasetintable5.Eachmodelisoncetrainedonalltrainsets,andoncetrainedonlyonitscorrespondingtrainset(representedasXintheresultstable).Forqualitycontrolandfurtherinsightonthetask,weincludetwohumanannotationsasmethods:‘Humanconsensus’(thesameasone-best)whichindicatestheconsensushumanannotationonthetestimageand‘Humanrandom’whichisrandomlychosenfromtheﬁvehumanannotations.Forde-scriptionofeachoftheothermodelspleaserefertoSection4.2.Astheresultsofhumanevaluationshow,GRNNallperformsthebestascomparedwithalltheothermodelsin2/3ofrunsandachievesthebestscoreonVQGCOCO−5000,whichwasex-pectedgiventhelessdiversesetofimages.Us-ingautomaticmetrics,theGRNNXmodelout-performsothermodelsaccordingtoallthreemetricsontheVQGBing−5000dataset.Amongretrievalmodels,themostcompetitiveisk-NN+minbleuall,whichperformsthebestonVQGCOCO−5000andVQGFlickr−5000datasetsaccordingtoBLEUand∆BLEUscore.Theboostfrom1-NNtoK-NNmodelsisconsiderableac-cordingtobothhumanandautomaticmetrics.HumanconsensusHumanrandomGRNNXGRNNall1-NNbleu−X1-NNgensim−XK-NN+minbleu−XK-NN+mingensim−X1-NNbleu−all1-NNgensim−allK-NN+minbleu−allK-NN+mingensim−allHumanEvaluationBing2.492.381.351.761.721.721.691.571.721.731.751.58COCO2.492.381.661.941.811.821.881.641.821.821.961.74Flickr2.342.261.241.571.441.441.541.281.461.461.521.30AutomaticEvaluationBLEUBing87.183.712.311.19.09.011.27.99.09.011.87.9COCO86.083.513.914.211.011.019.111.510.710.719.211.2Flickr84.483.69.99.97.47.410.95.97.67.611.75.8MET.Bing62.258.816.215.814.714.715.414.714.714.715.514.7COCO60.858.318.518.516.216.219.717,415.915.919.517.5Flickr59.958.614.314.912.312.313.612.612.612.614.613.0∆BLEUBing63.3857.2511.610.88.288.2810.247.118.438.4311.017.59COCO60.8156.7912.4512.469.859.8516.149.969.789.7816.299.96Flickr62.3757.349.369.556.476.479.495.376.736.739.85.26Table5:Resultsofevaluatingvariousmodels.Xrepresentstrainingonthecorrespondingdatasetintherow.Humanscorepermodeliscomputedbyaveraginghumanscoreacrossmultipleimages,wherehumanscoreperimageisthemedianratingacrossthethreeraters.METEORBLEU∆BLEUr0.916(4.8e-27)0.915(4.6e-27)0.915(5.8e-27)ρ0.628(1.5e-08)0.67(7.0e-10)0.702(5.0e-11)τ0.476(1.6e-08)0.51(7.9e-10)0.557(3.5e-11)Table6:Correlationsofautomaticmetricsagainsthumanjudgments,withp-valuesinparentheses.6DiscussionWeintroducedthenoveltaskof‘VisualQues-tionGeneration’,wheregivenanimage,thesys-temistaskedwithaskinganaturalquestion.Weprovidethreedistinctdatasets,eachcoveringavarietyofimages.ThemostchallengingistheBingdataset,requiringsystemstogenerateques-tionswithevent-centricconceptssuchas‘cause’,‘event’,‘happen’,etc.,fromthevisualinput.Fur-thermore,weshowthatourBingdatasetpresentschallengingimagestothestate-of-the-artcaption-ingmodels.WeencouragethecommunitytoreporttheirsystemresultsontheBingdatasetandaccordingtothe∆BLEUautomaticmetric.Wewillreleaseallthedatasetsandcorrespond-ingevaluationframeworkstothepublic.Ourhu-manevaluationmeasurednotonlyhownaturalasystem-generatedquestionWebelievethatformakingfurtherprogress,thistaskcannotbedependentonlyonincreasingthetrainingdata,butweexpecttodevelopmodelsthatHuman-Howlongdidittaketomakethaticesculpture?-HowfastaretheyfallingtotheEarth?GRNN-Howlonghashebeenhiking?-Isthisaprofessionalsurfer?KNN-Howdeepwasthesnow?-Amanholdingasnowboardintheair.Table7:Examplesoferrorsingeneration.TherowsareHumanconsensus,GRNNall,KNN+minbleu−allandtheCaptioningsystem.willlearntogeneralizetounseenconcepts.Con-sidertheexamplesofsystemerrorsinTable7,wherethevisualfeaturescandetectthespeciﬁcobjects,butthesystemcannotmakesenseofacombinationofpreviouslyunseenconcepts.Anaturalfutureextensionofthisworkistoin-cludequestiongenerationwithinaconversationalsystem,wherethecontextandconversationhis-torycanaffectthetypesofquestionsbeingasked.ReferencesMykhayloAndriluka,LeonidPishchulin,PeterGehler,andBerntSchiele.2014.2dhumanposeestima-tion:Newbenchmarkandstateoftheartanalysis.InIEEEConferenceonComputerVisionandPat-ternRecognition(CVPR),June.StanislawAntol,AishwaryaAgrawal,JiasenLu,Mar-garetMitchell,DhruvBatra,C.LawrenceZitnick,andDeviParikh.2015.Vqa:Visualquestionan-swering.InInternationalConferenceonComputerVision(ICCV).LeeBecker,SumitBasu,andLucyVanderwende.2012.Mindthegap:Learningtochoosegapsforquestiongeneration.InProceedingsofthe2012ConferenceoftheNorthAmericanChapteroftheAssociationforComputationalLinguistics:HumanLanguageTechnologies,pages742–751,Montr´eal,Canada,June.AssociationforComputationalLin-guistics.Yu-WeiChao,ZhanWang,YugengHe,JiaxuanWang,andJiaDeng.2015.Hico:Abenchmarkforrec-ognizinghuman-objectinteractionsinimages.InProceedingsoftheIEEEInternationalConferenceonComputerVision.JianfuChen,PolinaKuznetsova,DavidWarren,andYejinChoi.2015.D´ej`aimage-captions:Acor-pusofexpressivedescriptionsinrepetition.InPro-ceedingsofthe2015ConferenceoftheNorthAmer-icanChapteroftheAssociationforComputationalLinguistics:HumanLanguageTechnologies,pages504–514,Denver,Colorado,May–June.AssociationforComputationalLinguistics.KyunghyunCho,BartVanMerri¨enboer,CaglarGul-cehre,DzmitryBahdanau,FethiBougares,HolgerSchwenk,andYoshuaBengio.2014.Learningphraserepresentationsusingrnnencoder-decoderforstatisticalmachinetranslation.arXivpreprintarXiv:1406.1078.MichaelDenkowskiandAlonLavie.2014.Meteoruniversal:Languagespeciﬁctranslationevaluationforanytargetlanguage.InProceedingsoftheEACL2014WorkshoponStatisticalMachineTranslation.JacobDevlin,HaoCheng,HaoFang,SaurabhGupta,LiDeng,XiaodongHe,GeoffreyZweig,andMar-garetMitchell.2015.Languagemodelsforimagecaptioning:Thequirksandwhatworks.InProceed-ingsofthe53rdAnnualMeetingoftheAssociationforComputationalLinguisticsandthe7thInterna-tionalJointConferenceonNaturalLanguagePro-cessing(Volume2:ShortPapers),pages100–105,Beijing,China,July.AssociationforComputationalLinguistics.JeffDonahue,LisaAnneHendricks,SergioGuadar-rama,MarcusRohrbach,SubhashiniVenugopalan,KateSaenko,andTrevorDarrell.2014.Long-termrecurrentconvolutionalnetworksforvisualrecogni-tionanddescription.CoRR,abs/1411.4389.MarkEveringham,LucGool,ChristopherK.Williams,JohnWinn,andAndrewZisserman.2010.Thepascalvisualobjectclasses(voc)challenge.Int.J.Comput.Vision,88(2):303–338,June.HaoFang,SaurabhGupta,ForrestN.Iandola,Ru-peshSrivastava,LiDeng,PiotrDoll´ar,JianfengGao,XiaodongHe,MargaretMitchell,JohnC.Platt,C.LawrenceZitnick,andGeoffreyZweig.2014.Fromcaptionstovisualconceptsandback.CoRR,abs/1411.4952.AliFarhadi,MohsenHejrati,MohammadAminSadeghi,PeterYoung,CyrusRashtchian,JuliaHockenmaier,andDavidForsyth.2010.Everypic-turetellsastory:Generatingsentencesfromimages.InProceedingsofthe11thEuropeanConferenceonComputerVision:PartIV,ECCV’10,pages15–29,Berlin,Heidelberg.Springer-Verlag.FrancisFerraro,NasrinMostafazadeh,Ting-HaoHuang,LucyVanderwende,JacobDevlin,MichelGalley,andMargaretMitchell.2015.Asurveyofcurrentdatasetsforvisionandlanguageresearch.InProceedingsofthe2015ConferenceonEmpiri-calMethodsinNaturalLanguageProcessing,pages207–213,Lisbon,Portugal,September.AssociationforComputationalLinguistics.MichelGalley,ChrisBrockett,AlessandroSordoni,YangfengJi,MichaelAuli,ChrisQuirk,MargaretMitchell,JianfengGao,andBillDolan.2015.deltableu:Adiscriminativemetricforgenerationtaskswithintrinsicallydiversetargets.InProceed-ingsofthe53rdAnnualMeetingoftheAssociationforComputationalLinguisticsandthe7thInterna-tionalJointConferenceonNaturalLanguagePro-cessing(Volume2:ShortPapers),pages445–450,Beijing,China,July.AssociationforComputationalLinguistics.HaoyuanGao,JunhuaMao,JieZhou,ZhihengHuang,LeiWang,andWeiXu.2015.Areyoutalkingtoamachine?datasetandmethodsformultilingualim-agequestionanswering.CoRR,abs/1505.05612.MichaelHeilmanandNoahA.Smith.2010.Goodquestion!statisticalrankingforquestiongenera-tion.InHumanLanguageTechnologies:The2010AnnualConferenceoftheNorthAmericanChap-teroftheAssociationforComputationalLinguistics,pages609–617,LosAngeles,California,June.As-sociationforComputationalLinguistics.MicahHodosh,PeterYoung,andJuliaHockenmaier.2013.Framingimagedescriptionasarankingtask:Data,modelsandevaluationmetrics.J.Artif.Int.Res.,47(1):853–899,May.JosephJordania.2006.WhoAskedtheFirstQuestion?TheOriginsofHumanChoralSinging,Intelligence,LanguageandSpeech.Logos.IgorLabutov,SumitBasu,andLucyVanderwende.2015.Deepquestionswithoutdeepunderstanding.InProceedingsofthe53rdAnnualMeetingoftheAssociationforComputationalLinguisticsandthe7thInternationalJointConferenceonNaturalLan-guageProcessing(Volume1:LongPapers).Chin-YewLinandFranzJosefOch.2004.Auto-maticevaluationofmachinetranslationqualityus-inglongestcommonsubsequenceandskip-bigramstatistics.InProceedingsofthe42NdAnnualMeet-ingonAssociationforComputationalLinguistics,ACL’04,Stroudsburg,PA,USA.AssociationforComputationalLinguistics.Tsung-YiLin,MichaelMaire,SergeBelongie,JamesHays,PietroPerona,DevaRamanan,PiotrDollar,andC.LawrenceZitnick.2014.MicrosoftCOCO:commonobjectsincontext.CoRR,abs/1405.0312.DavidLindberg,FredPopowich,JohnNesbit,andPhilWinne.2013.Generatingnaturallanguageques-tionstosupportlearningon-line.InProceedingsofthe14thEuropeanWorkshoponNaturalLanguageGeneration,pages105–114,Soﬁa,Bulgaria,Au-gust.AssociationforComputationalLinguistics.MateuszMalinowskiandMarioFritz.2014.Amulti-worldapproachtoquestionansweringaboutreal-worldscenesbasedonuncertaininput.InAdvancesinNeuralInformationProcessingSystems27,pages1682–1690.KarenMazidiandRodneyD.Nielsen.2014.Linguis-ticconsiderationsinautomaticquestiongeneration.InProceedingsofthe52ndAnnualMeetingoftheAssociationforComputationalLinguistics(Volume2:ShortPapers),pages321–326,Baltimore,Mary-land,June.AssociationforComputationalLinguis-tics.TomasMikolov,IlyaSutskever,KaiChen,GregoryS.Corrado,andJeffreyDean.2013.Distributedrep-resentationsofwordsandphrasesandtheircom-positionality.InAdvancesinNeuralInformationProcessingSystems26:27thAnnualConferenceonNeuralInformationProcessingSystems2013.Pro-ceedingsofameetingheldDecember5-8,2013,LakeTahoe,Nevada,UnitedStates.,pages3111–3119.GeorgeA.Miller.1995.Wordnet:Alexicaldatabaseforenglish.Commun.ACM,38(11):39–41,Novem-ber.RuslanMitkovandLeAnHa.2003.Computer-aidedgenerationofmultiple-choicetests.InJillBursteinandClaudiaLeacock,editors,ProceedingsoftheHLT-NAACL03WorkshoponBuildingEducationalApplicationsUsingNaturalLanguageProcessing,pages17–22.VicenteOrdonez,GirishKulkarni,andTamaraL.Berg.2011.Im2text:Describingimagesusing1millioncaptionedphotographs.InNeuralInformationPro-cessingSystems(NIPS).KishorePapineni,SalimRoukos,ToddWard,andWei-JingZhu.2002.Bleu:Amethodforautomaticevaluationofmachinetranslation.InProceedingsofthe40thAnnualMeetingonAssociationforCom-putationalLinguistics,ACL’02,pages311–318,Stroudsburg,PA,USA.AssociationforComputa-tionalLinguistics.J.Pustejovsky,P.Hanks,R.Sauri,A.See,R.Gaizauskas,A.Setzer,D.Radev,B.Sund-heim,D.Day,L.Ferro,andM.Lazo.2003.TheTIMEBANKcorpus.InProceedingsofCorpusLinguistics2003,pages647–656,Lancaster,March.MengyeRen,RyanKiros,andRichardZemel.2015.Questionansweringaboutimagesusingvisualse-manticembeddings.InDeepLearningWorkshop,ICML2015.MarcusRohrbach,SikandarAmin,MykhayloAn-driluka,andBerntSchiele.2012.Adatabaseforﬁnegrainedactivitydetectionofcookingactivities.InIEEEConferenceonComputerVisionandPatternRecognition(CVPR).IEEE,IEEE,June.K.SimonyanandA.Zisserman.2014.Verydeepcon-volutionalnetworksforlarge-scaleimagerecogni-tion.CoRR,abs/1409.1556.LucyVanderwende.2008.Theimportanceofbeingimportant:Questiongeneration.InInWorkshopontheQuestionGenerationSharedTaskandEvalua-tionChallenge,Arlington,VA.SubhashiniVenugopalan,HuijuanXu,JeffDonahue,MarcusRohrbach,RaymondMooney,andKateSaenko.2015.Translatingvideostonaturallan-guageusingdeeprecurrentneuralnetworks.InProceedingsthe2015ConferenceoftheNorthAmericanChapteroftheAssociationforCompu-tationalLinguistics–HumanLanguageTechnolo-gies(NAACLHLT2015),pages1494–1504,Denver,Colorado,June.OriolVinyals,AlexanderToshev,SamyBengio,andDumitruErhan.2015.Showandtell:Aneuralim-agecaptiongenerator.InComputerVisionandPat-ternRecognition.JohnHWolfe.1976.Automaticquestiongener-ationfromtext-anaidtoindependentstudy.InACMSIGCUEOutlook,volume10,pages104–112.ACM.YuanjunXiong,KaiZhu,DahuaLin,andXiaoouTang.2015.Recognizecomplexeventsfromstaticim-agesbyfusingdeepchannels.InTheIEEEConfer-enceonComputerVisionandPatternRecognition(CVPR),June.BangpengYao,XiaoyeJiang,AdityaKhosla,AndyLaiLin,LeonidasJ.Guibas,andLiFei-Fei.2011a.Actionrecognitionbylearningbasesofactionat-tributesandparts.InInternationalConferenceonComputerVision(ICCV),Barcelona,Spain,Novem-ber.BangpengYao,XiaoyeJiang,AdityaKhosla,AndyLaiLin,LeonidasJ.Guibas,andLiFei-Fei.2011b.Hu-manactionrecognitionbylearningbasesofactionattributesandparts.InInternationalConferenceonComputerVision(ICCV),Barcelona,Spain,Novem-ber.