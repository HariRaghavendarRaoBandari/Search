Randomized gap and amplitude estimation

Quantum Architectures and Computation Group, Microsoft Research, Redmond, WA 98052, USA

Ilia Zintchenko and Nathan Wiebe

We provide a new method for estimating spectral gaps in low-dimensional systems. Unlike tra-
ditional phase estimation, our approach does not require ancillary qubits nor does it require well
characterised gates.
Instead, it only requires the ability to perform approximate Haar–random
unitary operations, applying the unitary whose eigenspectrum is sought out and performing mea-
surements in the computational basis. We discuss application of these ideas to in-place amplitude
estimation and quantum device calibration.

6
1
0
2

 
r
a

 

M
1
1

 
 
]
h
p
-
t
n
a
u
q
[
 
 

1
v
2
7
6
3
0

.

3
0
6
1
:
v
i
X
r
a

I.

INTRODUCTION

In recent years a host of methods have been developed for performing phase estimation in quantum systems [1–4].
These methods, driven by demand in quantum algorithms and metrology, have provided ever more eﬃcient means of
learning the eigenvalues of a quantum system. Through the use of sophisticated ideas from optimization and machine
learning, recent phase estimation methods come close to saturating the Heisenberg bound [3–6], which is the ultimate
performance limit for any phase estimation algorithm.

Despite the near-optimality of existing methods, there are a number of avenues of inquiry that still remain open
surrounding phase estimation.
Importantly, recent work has begun to look at operational restrictions on phase
estimation including limited visibility in measurements, de-coherence and time-dependent drift. Such generalisations
are especially important as we push towards building a scalable quantum computer and are faced with the challenge
of characterising the gates in a massive quantum system that is imperfectly calibrated.

One important restriction that has not received as much attention is the issue that traditional phase estimation
algorithms require entangling the quantum device with one (or more) ancilla qubits. This need to couple the system
with an external qubit precludes ideas from phase estimation to be applied to single qubit devices. Even if ancilla
qubits are available, such characterisation methods would entail the use of entangling gates which are very costly in
many quantum computing platforms.

Our work provides a way to circumvent this problem for small quantum devices. It allows an experimentalist to
learn the spectral gaps in an uncharacterised device with an amount of experimental time that is proportional to that
of phase estimation without requiring any ancillary qubits. The idea of our approach is reminiscent of randomised
benchmarking [7–10] in that we use random operations to extract information about the underlying dynamics of the
system.

We consider three applications for this method, which we call randomised gap estimation:

1. Eigenspectrum estimation for small quantum systems that lack well characterised gates.

2. Ancilla–free amplitude estimation.

3. Control map learning for small quantum devices.

This last application is interesting in the context of quantum bootstrapping [11] because it gives an inexpensive way
of calibrating a set of one and two qubit gates at the beginning of a bootstrapping protocol. This is signiﬁcant
because an important caveat in bootstrapping is that while a trusted simulator can be used to quickly learn how
to control an uncalibrated system, calibrating that trusted simulator requires exponentially more measurements
and polynomially more experimental time than using traditional methods. Thus our techniques can, under some
circumstances, dramatically reduce the total cost of a bootstrapping protocol.

Our paper is laid out as follows. We ﬁrst provide a review of Bayesian inference in Section II and discuss the
method for approximate Bayesian inference that we use in our numerical studies. We then introduce randomised gap
estimation in Section III and discuss the challenges faced when applying the method to high–dimensional systems.
We then show how the adiabatic theorem can be used to eliminate this curse of dimensionality for certain systems
in Section IV. We then apply randomised gap estimation to the problem of amplitude estimation in Section V and
also to learning a map between experimental controls and the system Hamiltonian for a quantum device Section VI
before concluding.

II. BAYESIAN INFERENCE

2

Bayesian inference is a widely used method to extract information from a system. The goal of Bayesian inference is
to compute the probability that a hypothesis is true given evidence E and a set of prior beliefs about the hypotheses.
These prior beliefs are represented as a probability distribution known as a prior. This prior, along with evidence E,
can be thought of as an input to the inference algorithm. The output of the algorithm is the posterior distribution
which is given by Bayes’ theorem as

P (x|E) =

P (E|x)P (x)

(cid:82) P (E|x)P (x)dx

(1)
where P (E|x) is known as the likelihood function. The posterior distribution output by this process is then used as a
prior distribution in online inference algorithms and this process of generating a new prior distribution using Bayes’
rule is known as a Bayesian update.

,

As an example of how Bayesian inference plays a role in eigenvalue estimation, consider the traditional approach
used for iterative phase estimation. In iterative phase estimation, one wishes to learn the eigenvalues of a unitary
operation U using the following circuit:

e−iM θZ

|0(cid:105)
|φ(cid:105)

H

/

•

U M

H

E

where M and θ are user–speciﬁable experimental parameters, H is the Hadamard matrix and Z is the Pauli–Z
operator. Let |φ(cid:105) be an eigenstate such that U |φ(cid:105) = eiλ |φ(cid:105). The likelihood function for this circuit is then

P (E = 0|λ; θ, M ) = cos2(M (θ − λ)).

(2)

The problem of phase estimation is thus the problem estimating λ given a sequence of diﬀerent experimental
outcomes. This can be done using (1) starting from an initial prior distribution for λ that is uniform over [0, 2π). A
further beneﬁt is that the posterior variance provides an estimate of the uncertainty in the inferred phase as well as
an estimate of the most likely value of λ. This approach is considered in [2, 3] wherein exact Bayesian inference is
found to perform extremely well both in settings where θ and M are chosen adaptively as well as non–adaptively.

If P (x) has support over an inﬁnite number of points then exact Bayesian inference is usually intractable and
discretisations are often employed to address this problem. Such discretisations include particle ﬁlter methods and
sequential Monte Carlo methods [12–14]. Here we employ a newly developed approach known as rejection ﬁlter
inference [3, 15].

The idea behind Rejection ﬁltering is to use rejection sampling to convert an ensemble of samples from the prior
distribution to a smaller set of samples from the posterior distribution. Speciﬁcally, if evidence E is observed and we
draw a sample from the prior distribution and accept it with probability equal to P (E|x) then the probability that
hypothesis x is accepted as a sample is from Bayes’ theorem

P (E|x)P (x) ∝ P (x|E)

(3)

Therefore, the samples that pass through the rejection ﬁlter are distributed according to the posterior distribution.

Although this process allows us to sample from the posterior distribution, it is not eﬃcient. This is because every
time we perform an update, we will, on average, reject a constant fraction of samples. This means that the number
of samples kept will shrink exponentially with the number of updates. We can, however, make this process eﬃcient
by ﬁtting these samples to a family of distributions and then draw a new set of samples from this model distribution
for the next update. This ability to regenerate samples allows rejection ﬁltering inference to avoid this catastrophic
loss of support for the approximation to the posterior.

There are a number of models for the prior and posterior that can be considered. Here we use a unimodal
Gaussian distribution. Gaussian models for the posterior distribution provide a number of advantages. First, they
are parametrised by the posterior mean and covariance matrix which give an estimate of the true hypothesis and the
uncertainty in it. Furthermore, these quantities are easy to estimate from the accepted samples and can be computed
incrementally, which allows our algorithm to be executed using near-constant memory.

There are several advantages to using rejection ﬁltering for approximate inference. Speciﬁcally, it is very fast,
easy to parallelise, can be implemented using far less memory than particle ﬁlter or sequential Monte-Carlo methods.
Perhaps most importantly, it is also substantially easier to implement than traditional particle ﬁlters. Rejection
ﬁltering has also been successfully used in phase estimation algorithms [3], where the use of rejection ﬁltering leads to
substantial reductions in the experimental time needed to perform the inference relative to Kitaev’s phase estimation
algorithm [1] and information theoretic phase estimation [2].

Algorithm 1 Bayesian inference algorithm for eigenphases

Input: Set of unitary operators {U : j = 1 : K}, evolution time t, prior over gaps P (∆).

Prepare state |0(cid:105) ∈ CN .
Randomly select U from set of unitary operations.
|0(cid:105) ← U†e−iHtU |0(cid:105).
Measure state and E ← 0 if the result is ’0’ otherwise E ← 1.
return P (∆|E).

Use approximate Bayesian inference to estimate P (∆|E) ∝ P (∆) ·(cid:82) P (E|∆; t, U )µ(U )dU .

III. RANDOMIZED GAP ESTIMATION

3

Traditional approaches for learning the eigenspectrum of a Hamiltonian require ancillary qubits and well charac-
terised gates. Here we present an approach, which we call randomised gap estimation, to eﬃciently estimate the
eigenspectrum of a small system with no ancillary qubits and potentially poorly characterised gates. The idea behind
this approach is to use Bayesian inference in concert with random evolutions to infer the gaps between the eigenphases
of a unitary operator.

In the ﬁrst step, the state

|Ψ(cid:105) := U |0(cid:105) =

(cid:88)

βk|vk(cid:105)

(4)

is prepared, where |vk(cid:105) is the eigenvector corresponding to eigenvalue λk of H in an arbitrary ordering. Here βk
are unknown parameters that depend not only on the random unitary chosen, but also the eigenbasis of H. For
low–dimensional systems exact methods for drawing U uniformly according to the Haar measure are known (more
generally it suﬃces to draw U from a unitary 2–design see Appendix A for more details). For example, a single qubit
system U has an Euler angle decomposition of

k

U = Rz(φ)Rx(θ)Rz(ψ),

(5)

up to an irrelevant global phase.

Next, we evolve the system according to e−iHt for a controlled time t. This results in the state

(6)
Finally, U† is applied and a measurement in the computational basis is performed, which returns ’0’ with probability

e−iHt |Ψ(cid:105) = e−iHtU |0(cid:105) .

P (0|H; t, U ) := |(cid:104)0| U†e−iHtU |0(cid:105)|2

(cid:32)(cid:88)
(cid:88)

k

=

=

(cid:33)2

(cid:32)(cid:88)

k

|βk|2 cos (λkt)

+

|βk|2 sin (λkt)

cos ((λi − λj)t)|βi|2|βj|2 := P (0|∆; t, U ).

(cid:33)2

(7)

(9)

Note that the gaps ∆ij = λi − λj cannot be easily learned from this expression because of the unknown βi and βj
terms. Haar averaging provides a solution to this problem.

ij

Given an unknown Haar–random unitary U , the likelihood of measuring ’0’ is given by the law of conditional

probability

P (0|∆; t) =

(cid:90)

P (0|∆; t, U )µ(U )dU,

(8)

where µ is the Haar–measure over U (N ). This probability is distinct from the likelihood that would be used if the
user knew, or was capable of computing, P (0|H; t, U ) for the particular U that was used in the experiment.

If we deﬁne ∆ to be a matrix such that ∆i,j := λi − λj, this Haar average evaluates to

P (0|∆; t) : = N(cid:104)|βi|4(cid:105) + (cid:104)|βi|2|βj|2(cid:105)i(cid:54)=j

cos (∆ijt)

(cid:88)

i(cid:54)=j

 .

cos (∆ijt)

1 +

(cid:88)

i>j

1
N

=

2

N + 1

4

FIG. 1: Median error and uncertainty in the eigenvalues computed using randomised gap estimation using rejection ﬁltering for
2-level (left), 3-level (middle) and 4-level (right) Hamiltonians with eigenspectra drawn from the Gaussian Unitary Ensemble.

Eq. (9) provides a likelihood function that can be used to perform Bayesian inference. In particular, Bayes’ rule states
that if a binary experiment is performed wherein the only two outcomes are |0(cid:105) and |v (cid:54)= 0(cid:105), the latter occurs with
probability 1 − P (0|H; t). If we deﬁne our prior distribution over eigenvalues to be P (λ), then given a measurement
of ’0’ is recorded, Bayes’ rule states that the posterior distribution is

(cid:16)

(cid:80)

(cid:17)

P (λ|0; t) =

2

1 + 1
N

i>j cos (∆ijt)

(N + 1)P (0)

P (λ)

,

(10)

where P (0) is a normalisation factor. We outline this approach in Algorithm 1. Thus Bayesian inference allows the
gaps to be learned from such experiments. Since the likelihood of observing ’0’ in (10) scales inversely with N , we
expect that the number of experiments required to learn the spectrum should grow at least linearly with N .

The Cram´er–Rao bound allows us to formalise this argument by bounding the minimum number of experiments
If we assume that R
and/or experimental time needed ensure that the variance of ∆ij is suﬃciently small [16].
experiments are performed, each with D ∈ O(1) outcomes and evolution time at least t, then the elements of the
Fisher matrix are

D−1(cid:88)

··· D−1(cid:88)

d1=0

dR=0

∂∆ij

(cid:81)R
q=1 P (dq|∆)∂∆kl

(cid:81)R
(cid:81)R
q=1 P (dq|∆)
q=1 P (dq|∆)

(cid:18) R2t2

(cid:19)

N 2

∈ O

Iij,kl =

,

(11)

which through the use of the Cram´er–Rao bound implies that the variance of any unbiased estimator of ∆ij after all
R experiments scales at least as Ω(N 2/R2t2).

While this shows that the number of experiments needed to learn the gaps grows at least linearly with N , the
uncertainty in the optimal unbiased estimator also shrinks as Ω(1/T ), where T is the total evolution time. The error
scaling is hence quadratically smaller than would be expected from statistical sampling. This opens up the possibility
of very high precision frequency estimates for small quantum devices.

A. Numerical tests

While the prior argument suggests that the error should scale as 1/T for randomised gap estimation, an important
question remains regarding how well it actually scales in practice. We assess this by using rejection ﬁltering to estimate
the gaps and use the particle guess heuristic, discussed in [11, 17, 18] and also the appendix. We use this approach
because it is fast, accurate and more importantly easier to implement than sequential Monte–Carlo methods [20, 30].
Although performing rejection ﬁltering to infer the gaps may seem straight forward, a complication emerges that
makes it conceptually more challenging to apply the technique directly. To see this, consider a Gaussian prior over
the eigenvalue gaps for an N –dimensional system. If the eigenvalue gaps are drawn independently, the result will with
high probability have inconsistent eigenvalue gaps. By inconsistent we mean that if ∆43 = λ4 − λ3 and ∆32 = λ3 − λ2
are eigenvalue gaps, then ∆42 = λ4 − λ2 = ∆43 + ∆32 must also be one of the eigenvalue gaps for the system. If ∆43
and ∆32 are chosen independently from a Gaussian prior, then it is very unlikely that their sum is also. Hence, gaps
chose independently from the prior distribution will not correspond to a feasible set of eigenvalues.

There are many ways that this self-consistency constraint could be imposed in the inference step, but perhaps the
simplest approach is to use Bayesian inference to learn a set of consistent eigenvalues for the system, rather than gaps

02004006008001000# iteration10-1310-1210-1110-1010-910-810-710-610-510-410-310-210-1100101∆Median ∆erroruncertainty0200040006000800010000# iteration10-1310-1210-1110-1010-910-810-710-610-510-410-310-210-1100101102∆Median ∆erroruncertainty050001000015000200002500030000# iteration10-1210-1110-1010-910-810-710-610-510-410-310-210-1100101102∆Median ∆erroruncertaintyAlgorithm 2 Bayesian inference algorithm for eigenphases

Input: Set of unitary operators {U : j = 1 : K} such that U = 11 ⊗ Vj where V acts on a M ≤ N dimensional subspace of
the computational basis, a diagonal Hamiltonian H0, annealing time T , an interpolation function f : [0, 1] (cid:55)→ [0, 1] such that
f (0) = 1 − f (1) = 0, evolution time t and prior over M smallest gaps gaps P (∆).
Prepare state |0(cid:105) ∈ CN .
Randomly select U from set of unitary operations.

0 (1−f (s))H0+f (s)HdsT(cid:17)†

|0(cid:105) ← U†(cid:16)T e−i(cid:82) 1
Use approximate Bayesian inference to estimate P (∆|E) ∝ P (∆) ·(cid:82) P (E|∆; t, U )µ(U )dU .

Measure state and E ← 0 if the result is ’0’ otherwise E ← 1.
return P (∆|E).

0 (1−f (s))H0+f (s)HdsT(cid:17)

e−iHt(cid:16)T e−i(cid:82) 1

U |0(cid:105).

5

directly. The solution is not unique [21] as we discuss further in Appendix C. However, any gap inferred from the
potentially erroneous spectrum will be automatically consistent. We enforce this by taking the lowest–eigenvalue to
be zero without loss of generality and then deﬁne the error in the inference, given that rejection ﬁltering reports a
mean of µ and covariance matrix of Σ over the eigenvalues, to be

(cid:40)(cid:88)

(cid:88)

(cid:41)

∆ = min

|λi − µi|,

|λi − (max{µi}i − µi)|

(12)

uncertainty is computed as(cid:112)Tr(Σ). We ﬁnd numerically that Bayesian inference can rapidly estimate the eigenvalues

Here the minimization is meant to remove a mirror symmettry in the gaps that naturally arises in this problem. The

i

i

of an unknown Hamiltonian, up to a degeneracy, in one and two qubit systems (see Figure 1). In particular, we see
that the error shrinks exponentially with the number of measurements before being ultimately limited by machine–.
This illustrates that randomised gap estimation can learn the eigenvalue gaps for low–dimensional systems using
exponentially less data than would ordinarily be required by statistical sampling.

For higher–dimensional systems achieving the same task is expected to be much more daunting and the error
metric in (12) should be the minimum over all consistent re–orderings of the spectrum. Such problems can, however,
be sidestepped in systems with a gapped adiabatic path connecting the Hamiltonian to a eﬃciently diagonalizable
Hamiltonian, as we show below.

IV. ADIABATIC ELIMINATION OF EIGENVALUES

Most eigenvalue estimation tasks in quantum computing focus on learning only a part of the spectrum rather than
all the eigenvalues. We could use randomised gap estimation to learn the eigenvalues in this part of the spectrum if
we could apply Haar–random unitary operations in only that subspace. Since the eigenvectors of the Hamiltonian are
generally not known, it is diﬃcult to do this directly. Despite this fact, the adiabatic theorem provides a means by
which these random unitaries can be applied to the appropriate subspace by employing adiabatic state preparation.
Let us assume we wish to learn the spectrum, up to an additive constant, for a given subspace of a Hamiltonian
Hp. In particular let S = span(|λj1(cid:105) , . . . ,|λjm(cid:105)) be a subspace of eigenvectors of Hp such that the eigenvalues obey
λ1 ≤ λ2 ≤ ··· and j is a monotonically increasing sequence on {1, . . . , 2n}. Then we deﬁne an adiabatic interpolation
to be a time–dependent Hamiltonian of the form H(s) such that H(0) = H0 and H(1) = Hp where H(s) is at least
three-times diﬀerentiable for all s ∈ (0, 1) (where s is the dimensionless evolution time) and furthermore has a gapped
spectrum for all times in this interval. A common example of such a Hamiltonian is

H(s) = (1 − s)H0 + sHp.
(13)
j(cid:105) is an eigenvector of H0 whose eigenvalue corresponds to that of |λj(cid:105)

The adiabatic theorem [22–24] shows that if |λ0
in the sorted list of eigenvalues,

T e−i(cid:82) 1
(14)
where T is the time-ordering operator. Hence, if we can perform a Haar–random unitary on the subspace of eigen-
vectors of H0, denoted by S0 = span(|λ0
(cid:105)), we can adiabatically transform the resultant state to a Haar–
random state in S, up to phases on each of the eigenvectors and error O(1/T ).

(cid:105) = eiωjk T |λjk(cid:105) + O(1/T ),

0 H(s)dsT |λ0

(cid:105) , . . . ,|λ0

jm

jk

j1

Now let U be a Haar–random unitary acting on S0. Then the new adiabatic protocol for estimating the eigenphase

is

P (0|H; t, U ) =

(cid:12)(cid:12)(cid:12)(cid:104)0| U†(cid:16)T ei(cid:82) 0

1 H(s)dsT(cid:17)

e−iHpt(cid:16)T e−i(cid:82) 1

0 H(s)dsT(cid:17)

U |0(cid:105)(cid:12)(cid:12)(cid:12)2

.

(15)

Given U |0(cid:105) =(cid:80)

j:|λj(cid:105)∈S0

αj |λ0

Then using the fact that Hp |λj(cid:105) = λj |λj(cid:105), we see from (15) and (16) that

j:|λj(cid:105)∈S αj |λj(cid:105), (14) shows that
U |0(cid:105) =

αjeiωj T |λj(cid:105) + O(1/T ).

j(cid:105) and ˜U |0(cid:105) :=(cid:80)
0 H(s)dsT(cid:17)
(cid:16)T e−i(cid:82) 1
 (cid:88)
(cid:12)(cid:12)(cid:12)(cid:104)0| ˜U†e−iHpt ˜U |0(cid:105)(cid:12)(cid:12)(cid:12)2

k:|λk(cid:105)∈S

=

j:|λj(cid:105)∈S0

(cid:88)
2

+

 (cid:88)

k:|λk(cid:105)∈S

+ O(1/T ).

6

(16)

(17)

+ O(1/T ).

2

P (0|H; t, U ) =

|αk|2 cos (λkt)

|αk|2 sin (λkt)

The adiabatic theorem can therefore be used to allow randomised gap estimation to be performed on a speciﬁed
subspace of eigenvalues if there exists a gapped adiabatic path between a Hamiltonian that is diagonal in the compu-
tational basis and the problem Hamiltonian (see Algorithm 2 for an outline of this approach). This shows that the
curse of dimensionality that aﬄicts this method can be excised in cases where such an adiabatic evolution is possi-
ble. Randomized gap estimation can thus, in principle, be used as a fundamental tool to characterise and calibrate
untrusted quantum systems. We will examine this further in subsequent sections.

As a ﬁnal note, the asymptotic scaling of the error that emerges because of diabatic leakage out of the eigenstates,
or more generally eigenspaces, can be exponentially improved using boundary cancellation methods [25–28], which
set one or more derivatives of the Hamiltonian to zero at the beginning and end of the evolution. However, while
this can substantially reduce the cost of the adiabatic transport, it requires ﬁne control of the system Hamiltonian
in order to realise the beneﬁts of the cancellation [27]. As a result, it is not necessarily clear when the higher-order
versions of these algorithms will ﬁnd use in applications outside of fault tolerant quantum computing.

V. APPLICATION TO AMPLITUDE ESTIMATION

Randomised gap estimation also provides an important simpliﬁcation for amplitude estimation, which is a quantum
algorithm in which the probability of an outcome occurring is estimated by combining ideas from amplitude ampliﬁ-
cation and phase estimation [29]. The algorithm quadratically reduces the number of queries needed to learn a given
probability. The main signiﬁcance of this is that it quadratically speeds up Monte–Carlo algorithms. Speciﬁcally,
imagine that you are given an unknown quantum state of the form

|ψ(cid:105) := A|0(cid:105) = a|φ(cid:105) +(cid:112)1 − |a|2 |φ⊥(cid:105) ,

(18)
where |φ(cid:105) ∈ CN , A is a unitary operator and |a| ∈ (0, 1). Furthermore, assume that you are given access to an oracle
such that χ|φ(cid:105) = −|φ(cid:105) and for all states |v(cid:105) orthogonal to |φ(cid:105), χ|v(cid:105) = |v(cid:105). Amplitude estimation then allows |a|2 to
be estimated to within error  using ˜O(1/) applications of χ and O(log(1/)) measurements.
In order to understand how this works, consider the Grover search operator Q = −Aχ0A†χ, where χ0 acts as χ

would for φ = 0. For the case of a single marked state we then have that

(19)
where θa := sin−1(a). It is then clear that Q enacts a rotation in this two–dimensional subspace and has eigenvectors

Qj |ψ(cid:105) = sin([2j + 1]θa)|φ(cid:105) + cos([2j + 1]θa)|φ⊥(cid:105) ,

(cid:0)|φ(cid:105) ± i|φ⊥(cid:105)(cid:1) ,

|ψ±(cid:105) =

1√
2

where

Q|ψ±(cid:105) = e±i2θa |ψ±(cid:105) .

All other eigenvectors in the space orthogonal to this have eigenvalue ±1.

We can learn these phases using randomised gap estimation. The most signiﬁcant change is that here t must be taken
to be an integer since fractional applications of the Grover oracle have not been assumed. In this case, the eigenvalues
are in the set {−2θa, 0, 2θa, π}, which implies that |∆i,j| takes the values {0,±2θa, 4θa, π±2θa, π}. This means that the
gap estimation process has very diﬀerent likelihoods if t is even or odd. Since cos([π±2θa](2p+1)) = − cos(2θa(2p+1)),

(20)

(21)

×
e−iH(G(c))t/r ×

|ψ(cid:105)

cest, t

cest, t

eiH(Gtrust(cest))t/r ×

× eiH(Gtrust(cest))t/r

× e−iH(G(c))t/r ×

···
···

7

D

c, t

c, t

FIG. 2: Quantum bootstrapping experiment to learn a control map using interactions between a trusted simulator and an
uncharacterised device. The value r can be increased to combat the spread of non–local correlations and also to reduce
ambiguities that can arise in the inference process when the untrusted and trusted Hamiltonians do not commute.

for integer p, many of the terms in (9) cancel if t is odd. As a result, it is better to choose t to be an even integer for
this application, in which case the likelihood function is

P (0|θa; t) =

2

N + 1

1 +

1
N

+ 2(N − 2) cos(2θat) + cos(4θat)

.

(22)

Such experiments cannot yield substantial information as N → ∞ according to (11) and thus diﬀerent ideas are
needed to make large N instances tractable unless we can sample according to the Haar measure only within the
relevant subspace using techniques similar to those in Section IV.

This method is, however, viable without modiﬁcation for small N . To illustrate this, consider the case where N = 2,

(cid:18)

(cid:18)(cid:18)N − 2
(cid:19)

2

(cid:19)(cid:19)

where the likelihood function reduces to

P (0|θa; t) =

2
3

(cid:18) 1

2

(cid:19)

+ cos2(2θat)

.

(23)

This likelihood function yields only slightly less information than the one for iterative phase estimation, P (0|θa; t) =
cos2(2θat), as their derivatives with respect to θa diﬀer by at most a constant factor. This observation, coupled with
the fact that an additional qubit is not needed, means that amplitude estimation can be realistically applied in-place
in single qubit devices using this method.

VI. APPLICATION TO CONTROL MAP LEARNING

Another important application of randomised gap estimation is calibration of quantum devices. In order to under-
stand why calibration can be a problem, imagine that you wish to calibrate a qubit so that you can perform an X
gate within error  = 10−5. In order to verify that the error is this small, roughly 1010 experiments are needed. This
process would also have to be repeated for every quantum bit in the system, which could easily result in terabytes of
data needed to calibrate a small fault tolerant quantum device. Our approach provides a method that can exponen-
tially reduce the number of measurements needed to control such a qubit under these circumstances and polynomially
reduce the experimental time needed.

vector c) and the system Hamiltonian. Speciﬁcally, if we deﬁne H(x) =(cid:80)N

In order to abstract the problem of calibrating an unknown quantum device, let us consider the problem of learning
a control map. We deﬁne the control map to be the mapping between a set of experimental controls (speciﬁed by the
i=1 xiHi for a set of Hamiltonians Hi, then

x is given by c via

x = Gc + x0,

(24)

where the matrix G is known as the control map. For simplicity we will only consider the linear case where x0 = 0
below. The general case is handled by ﬁrst learning x0 and is discussed in [11].

Recently, a method called quantum bootstrapping has been proposed to learn such maps eﬃciently, wherein a
trusted quantum simulator is used to infer the controls of an untrusted device (see Figure 2 for a circuit diagram of a
bootstrapping experiment). Although this can allow the control map to be learned using a poly-logarithmic number
of experiments, it necessitates the use of swap gates that couple the device to the trusted simulator that is used to
characterise it. This means that verifying that the trusted simulator is properly working can still require an exponential
number of measurements (even under assumptions of locality). This is signiﬁcant because the error in the trusted
simulator places a lower limit on the error that can be attained using the experiments proposed in [11, 17, 18, 30].
Figure 3 demonstrates the dependence of the error in the inference of a control map for a single qubit that arises from

8

FIG. 3: Median error for a quantum bootstrapping protocol that attempts to learn a control map for a universal qubit using a
miscalibrated second qubit to control the measurement basis with miscalibrations of size δ. These results are simulated in the
limit as r → ∞ (see Figure 2) as rejection sampling ﬁltering has poor success probability for non–commuting models because
of (intermediate multi-modality), unlike the sequential Monte-Carlo methods with r = 1 studied in [18].

the use of a miscalibrated trusted simulator that has G drawn from a zero–mean Gaussian with covariance matrix δ11,
where δ is chosen to take values within the interval [0, 10−2]. We see there that the error decays until it saturates at a
level dictated by the miscalibrations in the trusted Hamiltonian. This illustrates that errors in the trusted simulator
can prevent a bootstrapping protocol from perfectly controlling this idealised quantum system.

It is worth noting that although Bayesian inference does not correctly infer the true control map for the system
because of these errors in the trusted simulator, it does infer a set of controls that precisely mimics the trusted device.
In this sense, the learning task can be thought of as a machine learning task, wherein the untrusted device is trained
to maximise the ﬁdelity of its output with that of the “trusted” simulator, wherein the output state can be thought
of as the training data for the problem. It is therefore crucial to have a well calibrated trusted device if we wish to
leverage the full power of quantum bootstrapping.

To this end we provide a number of examples below that show how the control maps of single qubit Hamiltonians
can be learned using randomised gap estimation. We thereby show that our techniques may be of great value in
building a trusted simulator that can be used in bootstrapping protocols.

In the case for a two–level, single–qubit, system, the Hamiltonian can be written, up to an irrelevant shift in energy,

as

A. Two level example

Since these three operations anti–commute it is evident that the eigenvalues of H are

Thus we can infer information about α, β and γ, and in turn G, from the eigenspectra of diﬀerent experiments.
Speciﬁcally,

The simplest example of control map learning is the diagonal case, for which

H([α, β, γ]) = αX + βY + γZ.

α2 + β2 + γ2.

E = ±(cid:112)
G00 G01 G02
 =
α
E = ±(cid:113)

G10 G11 G12
G20 G21 G22

G2

00c2

1 + G2

β
γ

c1

 .

c2
c3

11c2

2 + G2

22c2
3.

(25)

(26)

(27)

(28)

02004006008001000# iteration10-1110-1010-910-810-710-610-510-410-310-210-1100101∆10−1010−810−610−410−20and the control map can be learned, up to signs of Gij, using a sequence of three randomised phase estimation
experiments with c = [1, 0, 0], [0, 1, 0] and [0, 0, 1]. The signs can be inexpensively learned using auxiliary experiments
because only a single bit of information is required. Thus empirically the number of experiments required to learn G
using randomised phase estimation is upper bounded by a constant times log(1/) (see Figure 1).

The next simplest case to consider is that of an upper triangular control map with a positive diagonal in the form

Clearly G00 can be learned directly with randomised gap estimation using the prior experiments. However, the
remaining elements must be inferred from diﬀerent diﬀerent experiments. If c = [1, 1, 0] then

G00 G01 G02

0 G11 G12
0
0 G22

c1

 .

c2
c3

β
γ

 =
α
E([1, 1, 0]) = ±(cid:113)
E([0, 1, 0]) = ±(cid:113)

(G00 + G01)2 + G2
11

G2

01 + G2

11.

E([0, 1, 1]) = ±(cid:113)
E([1, 0, 1]) = ±(cid:113)
E([0, 0, 1]) = ±(cid:113)

(G01 + G02)2 + (G11 + G12)2 + G2
22

(G00 + G02)2 + G2

12 + G2
22

G2

02 + G2

12 + G2
22

9

(29)

(30)

(31)

(32)

(33)

(34)

(35)

After squaring and subtracting both equations we ﬁnd

E2([1, 1, 0]) − E2([0, 1, 0]) = G2

00 + 2G00G01,

which can be solved uniquely for G01 since G00 is known. Once G01 is known, G11 can be learned unambiguously
from (31) using the fact that G11 ≥ 0.

After these steps we have inferred the ﬁrst two columns of G. The remaining column can be learned similarly by

performing three randomised gap estimation experiments with c = [0, 0, 1], [1, 0, 1] and [0, 1, 1] which yield

Then by subtracting the square of (35) from (34) we learn G02, from which we learn G12 from substituting the result
into (33). G22 can then be learned by subtracting the square of (34) from (33), substituting the value of G02 and
using G22 ≥ 0.

More generally, this approach provides information about the inner product between any two columns of G. No
more information about G can be extracted with such measurements. That is, GT G can be learned, but G itself is
only determined up to an orthogonal transformation G(cid:48) = QG which preserves the inner products of its columns. The
matrix Q can further be determined only if additional constraints are imposed on G. For example, if G is upper or
lower triangular and with a positive diagonal it is unique within such measurements and Q is the identity matrix as
discussed above.

B. Learning 2 × 2 single qubit control maps

As mentioned above, gap estimation experiments alone are in general not enough to uniquely specify the control
map. In fact, amplitudes of the states in the eigenbasis are required. This raises the question of whether amplitude
estimation may be used to glean the necessary information from these coeﬃcients using O(log(1/)) measurements.

For simplicity lets consider the Hamiltonian of a single qubit with only X and Z rotations

and

H([α, γ]) = αX + γZ.

(cid:21)

(cid:20)α

γ

=

(cid:20)G00 G01

(cid:21)(cid:20)c1

(cid:21)

G10 G11

c2

.

(36)

(37)

The case of a full 3 × 3 control map is similar, but we do not discuss it here because it is much more cumbersome
owing to the increased number of equations and unknowns.

Randomized gap estimation can provide

E([1, 0]) = ±(cid:113)
E([0, 1]) = ±(cid:113)
E([1, 1]) = ±(cid:112)(G00 + G01)2 + (G10 + G11)2,

00 + G2
10

01 + G2
11

G2

G2

10

(38)

Since there are four unknowns and three equations, the control map cannot be unambiguously learned from these
relations. However, it can be seen that any other such randomised gap estimation provides information that is
not linearly independent of these three equation. We need additional information that cannot directly come from
randomised gap estimation to solve the problem. This problem was avoided in the prior example by constraining G10
to be zero.

We can learn the required ﬁnal piece of information by measuring expectation values of the following three operators

These each correspond to free evolution of the system Hamiltonian under particular control settings for ﬁxed durations.
We can then use these operators (see eq (18)) to learn the following amplitudes while marking the |0(cid:105) state

(39)

A([1, 0]) = e−i(G00X+G10Z)π/(2E([1,0])),
A([0, 1]) = e−i(G01X+G11Z)π/(2E([0,1])),
A([1, 1]) = e−i((G00+G01)X+(G10+G11)Z)π/(2E([1,1])).

|a([1, 0])| = |(cid:104)0| A([1, 0])||0(cid:105)| =

|a([0, 1])| = |(cid:104)0| A([0, 1])|0(cid:105)| =

|a([1, 1])| = |(cid:104)0| A([1, 1])||0(cid:105)| =

E([1, 0])

(cid:12)(cid:12)(cid:12)(cid:12) ,
(cid:12)(cid:12)(cid:12)(cid:12) G10
(cid:12)(cid:12)(cid:12)(cid:12) ,
(cid:12)(cid:12)(cid:12)(cid:12) G11
(cid:12)(cid:12)(cid:12)(cid:12) G10 + G11

E([1, 1])

E([0, 1])

(cid:12)(cid:12)(cid:12)(cid:12) .

(40)

(41)

(42)

If the sign of G11 or G10 is known, these three quantities are enough to unambiguously solve for G11 and G10, and
can all be learned in-place using randomised amplitude estimation if an accurate Z–gate can be performed. The
remaining elements of the control map can then be found from (38). If the sign of G11 or G10 is not known, they can
still be learned with amplitude estimation if the states (|0(cid:105) ± i|1(cid:105))/
In the absence of a calibrated Z gate, the above strategy of amplitude estimation does not apply. The requisite
quantities |a([0, 1])|,|a([1, 0])| and |a([1, 1])| can nonetheless be learned by statistical sampling. Doing so requires
O(1/2) experiments, which dominates the cost of data acquisition. However, randomised gap estimation eﬃciently
provides E([0, 1]), E([1, 0]) and E([1, 1]) which allows us to optimally extract these expectation values (for the form
of the experiments considered). To see this consider the case of learning |a([0, 1])| for arbitrary t. The likelihood
function is

2 can be prepared on the quantum device.

√

P (0|G; t) = cos2(E([0, 1])t) + sin2(E([0, 1])t)|a([0, 1])|2.

(43)

It is then clear that the Fisher information is maximised at t = π/(2E([0, 1])). In order to compare, let us consider
the case where E([0, 1])t mod 2π is taken from a uniform prior over 0, 2π. Then the marginalised likelihood is

dx cos2(x) + sin2(x)|a([0, 1])|2 =

|a([1, 0])|2 + 1

2

.

(44)

(cid:90) 2π

0

1
2π

thus (11) shows that the Fisher information for such experiments is reduced by a factor of 2. This furthermore reduces
the inference problem to frequency estimation wherein the non–adaptive strategy of estimating |a([0, 1])|2 using the
sample frequency is optimal. Thus, even though randomised gap estimation does not lead to a quadratic reduction in
the experimental time needed to learn G here, it dramatically simpliﬁes the inference problem and removes the need
to use multiple experimental settings to learn the amplitudes.

Above we only discussed the case of a single qubit Hamiltonian with uncalibrated X and Z interactions. The
case of a general (traceless) single qubit Hamiltonian follows from exactly the same reasoning. First, one applies
randomised gap estimation to learn the relevant energies corresponding to the 6 linearly independent sets of controls.
The remaining 3 equations are found by measuring appropriate expectation values using amplitude estimation if a Z
gate is available, and statistical sampling if it is not.

Finally, higher dimensional control maps can also be learned using this approach. The analytic approach given
above no longer holds unless the Hamiltonian can be decomposed into a set of anti–commuting Pauli operators. The
same process can, however, be applied numerically in such cases. We leave a detailed discussion of the issues that
arise when doing so for future work.

VII. CONCLUSION

11

We introduced an alternative form of phase estimation that uses Bayesian inference on top of randomised exper-
iments to learn the spectral gaps of a unitary operation without needing external qubits or well calibrated gates.
In addition to calibration, this gap estimation procedure allows amplitude estimation to be performed in place for
quantum systems without needing auxiliary qubits to control the evolution. These randomised gap and amplitude
estimation algorithms could radically reduce the costs of characterising and calibrating small quantum devices. Our
work further suggests that they may ﬁnd use in building the initial trusted simulator that is required to start a
quantum bootstrapping protocol [11] as well as allowing amplitude estimation to be done in place in small quantum
systems.

An important caveat of our work is that the randomisation procedure that we use to extract information about the
gaps leads to an exponential decrease in the signal yielded by experiments on high–dimensional systems. We show
that this can be mitigated, for some systems, using adiabatic paths to allow the randomisation to be performed only
within a low–dimensional eigenspace of the system. Regardless, this caveat still holds in general and as such our
methods are complimentary to existing phase and amplitude estimation methods and do not replace them.

This suggests an important remaining question namely that of whether eﬃcient methods exist for estimating phase
and amplitude that do not require entangling the system with ancillary qubits. The development of such methods
would constitute a major step forward for quantum metrology. Similarly, proof that such methods are impossible
would give us new insights into the eﬃcacy of phase estimation and reveal fundamental trade oﬀs that exist between
quantum resources for metrological tasks.

Acknowledgments

We thank Matt Hastings and Shawn Cui for valuable comments and introducing us to the turnpike problem as well
as Michael Beverland, Alex Kubica, Christopher Granade and Paolo Zanardi for providing useful feedback on this
work.

Appendix A: Implementing random unitaries

In low dimensional systems there are a number of methods that can be used to perform random or pseudo-random
unitary operations. The most direct approach is to exploit an Euler angle decomposition of U (N ) to generate the
unitary. For example, for U ∈ SU (2), we can implement such a unitary via

(A1)
In order to pick such a unitary uniformly according to the Haar measure it suﬃces to pick θ := γ/2 ∈ [0, π/2],
φ := (β +δ)/2 mod 2π and ψ := (β−δ)/2 mod 2π chosen according to the following probability density function [31]

U = Rz(β)Ry(γ)Rz(δ).

dP (φ, θ, ψ) = sin(2θ)dφdθdψ.

(A2)

Such constructions also are known for higher-dimensional systems [31].

In practice, Haar-random unitaries are hard to exactly implement on a quantum computer. This task is even
more challenging in an uncalibrated device. Fortunately, eﬃcient methods for implementing pseudo–random unitaries
are well known. Here we use the deﬁnition of t–design given by Dankert et al [7], wherein a unitary t–design on
N dimensions is taken to be a ﬁnite set of unitary operations such that the average of any polynomial function of
degree at most t in the matrix elements and their complex conjugates agrees with the Haar average. In particular,
let P(t,t)(Uk) be such a polynomial function and assume that there are K elements in this design. Then

K(cid:88)

k=1

1
K

(cid:90)

P(t,t)(Uk) =

P(t,t)(U )µ(U )dU,

(A3)

where µ is the Haar–measure.

More generally, one can also consider the notion of an –approximate t–design. This notion diﬀers from a t–design in
that strict equality is not required and a discrepancy of at most  in the diamond–distance is permitted. Speciﬁcally,
†
i )⊗k be a super-operator corresponding to twirling the input operator over the elements of

let GW (ρ) =(cid:80)
the design and GH (ρ) =(cid:82) U⊗kρ(U†)⊗kµ(U )dU be the same quantity averaged over the Haar–measure. Then the set

i U⊗k

i ρ(U

12
of unitary operators forms a –approximate k–design if (cid:107)GW − GH(cid:107)(cid:5) ≤ , where the diamond norm is discussed in
detail in [32].

This deﬁnition implies that the Haar–expectations of the terms in (9) can be found using a 2-design since all such
terms are quadratic in the matrix elements of U and their complex conjugates. The remaining question is how to
form such a design.

Dankert et al. show that random Cliﬀord operations can be used to form exact and –approximate 2-designs using
O(n2) and O(n log(1/)) gates respectively [7]. These results are suﬃcient in settings where the device has access
to well characterised Cliﬀord gates. If the system does not have such gates, the result of Harrow and Low [33] can
be applied to show that –approximate t–designs can be formed out of random sequences of gates taken from any
universal gate set. The number of gates needed to generate such a pseudo–random unitary is O(n(n + log(1/))) [33]
in such cases, where the multiplicative constant depends on the details of the universal gate set used.

The result of Harrow and Low is especially signiﬁcant here because the gates do not need to be known for it to
hold. This means that even if we want to use randomised gap estimation to learn how to control an uncalibrated
quantum system, we do not need to explicitly know the gates that are actually applied to the system to suﬃciently
randomise the measurement results if the underlying gate set is universal and a suﬃciently long sequence of random
operations is used.

Appendix B: Numerics

Here we provide detailed information about the numerical experiments performed to generate Figure 1. As men-
tioned in the main body we use sets of eigenvalues as our hypotheses for Bayesian inference. Without loss of generality
we can shift the eigenspectrum such that λ1 = 0 and we hence have N − 1 variables, where N is the number of levels.
For the Bayesian inference step, we used rejection ﬁltering with a threshold of 10000 accepted samples. We found
that drawing parameters from the model until 10000 samples are accepted provides suﬃcient accuracy to represent
the likelihood function in these experiments. We use a standard unimodal multivariate Gaussian as a model for the
prior distribution on hypothesis space. While this unimodal distribution is incapable of representing a multi–modal
distribution (which may occur in cases where degenerate solutions exist), multimodal models can also be used to
improve learning. However, for our experiments the mean and covariance of all modes quickly converged to the same
value and the accuracy was hence no better than in the unimodal case.

The initial prior distribution is taken to be a Gaussian with mean and standard deviation set to be those appropriate
for Gaussian Unitary Ensemble (GUE) Hamiltonians. In particular, we ﬁnd these values using the gap distribution
of GUE Hamiltonians and set the mean eigenvalue to be that found by adding N − 1 random gaps drawn from the
distribution appropriate for GUE Hamiltonians to λ1 = 0. Similarly the covariance matrix for the N − 1 unknown
eigenvalues was set to be variances equivalent to those found for each of the eigenvalues of GUE Hamiltonians. While
this distribution does not accurately model the true prior distribution of the eigenvalues, it has suﬃcient overlap to be
able to zero in on the correct eigenvalues (up to degeneracies that cannot be resolved by randomised gap estimation
alone).

Evolution times are chosen to be t = 1/(2Γ), where Γ is the uncertainty which we deﬁne to be the trace of the
inferred covariance matrix. This heuristic was introduced in previous works [11, 17, 18] as the particle guess heuristic
and is shown to be nearly optimal for frequency estimation problems. The prefactor of 1/2 was found to work well
empirically, but more optimal choices may exist for this problem.

Since degeneracies in the eigenvalues can emerge in our problem that prevent correct inference of the gaps, we
resort to modifying the likelihood function in order to forbid particular Hamiltonian models. Speciﬁcally, we force
the eigenvalues to be sorted and positive by introducing an additional sign-term in the likelihood function

1 +

1
N

(cid:88)

i>j

(cid:104)L(cid:105) =

2

N + 1



sign(∆ij) cos (∆ijt)

(B1)

where the sign-function is +1 if its argument is positive and −1 otherwise and ∆ij = λi − λj. Hence, this likelihood
will align with the experimental likelihood at all time-steps t only if λi > λj. As this modiﬁed likelihood can for some
sets of parameters be larger than unity or smaller than zero, we do an additional truncation step such that 0 ≤ L ≤ 1.
The median error and uncertainty reported are computed across 1000 instances drawn from the Gaussian Unitary

Ensemble. This number was suﬃcient to make the sample error in the estimates graphically insigniﬁcant.

13

Appendix C: The turnpike problem for unique gaps

Eigenvalues are not directly provided in randomised phase estimation. Instead the eigenvalues for the system must
be inferred from the gaps. In the main body, as well as the numerical experiments, we use Bayesian inference to infer
the eigenvalues from the data. However, it remains an open question under what circumstances it can successfully
infer the eigenvalues, up to a constant shift, from the gaps. We show below that for typical spectra, wherein the
spectral gaps are unique, there are at most two degenerate choices of spectra that could equivalently describe the
system.
Theorem 1. Let {λ1 := 0 < λ2 < ··· < λN} be a set such that for any i (cid:54)= k (λi+1 − λi) (cid:54)= (λk+1 − λk). Given
∆ : ∆xy = λpx − λqy for unknown permutations of {1, . . . , N} p and q, there are only two possible solutions for the
values of λ.
Proof. The nearest neighbour gaps can be isolated from this set easily. Consider λi+2 − λi = λi+2 − λi+1 + λi+1 − λj.
Since λi+2 − λi+1 and λi+1 − λi are in ∆ it follows that such a gap can be decomposed into a sum of gaps in the
set. Similarly, by induction λi+x − λi can be decomposed into a sum of nearest neighbour gaps for all x ≥ 2. In
contrast, the nearest neighbour diﬀerence λi+2 − λi+1 cannot be decomposed into a sum of other gaps because of our
assumptions of uniqueness. This means that ∆1, the set of all nearest neighbour gaps, can be uniquely identiﬁed from
this data.

It remains to show that there are only two ways that these gaps can be laid out to form a spectrum that is consistent
with the data in ∆. The quantity ∆xy λN can be identiﬁed from the spectrum since it is simply the largest gap under
the assumption that λ1 = 0. The second largest gap can also be uniquely found from this data. There are two
possibilities. Either the second largest gap is λN−1 or it is λN − λ2. This can be seen by contradiction. Imagine
λN − λj is the second largest gap for j ≥ 3. Since λj is a monotonically increasing sequence, λN − λj ≤ λN − λ2 ≤ λN .
Therefore, it cannot be the second largest gap. Applying the same argument to λN−1 − λj for j ≥ 2 leads to the
conclusion that λN−1 is the only other candidate for the second largest gap. Similarly λx − λj ≤ λN−1 − λj for all
x ≤ N − 1 so these are the only two possible candidates.

Assume that the second largest gap is λN−1. Then

λN−2 = (λN−2 − λN−1) + λN−1.

Seeking a contradiction, imagine there is an alternative solution to this such that

λp − λN−1 = (λr−1 − λr).

If λp − λN−1 is not a nearest–neighbour gap, we have a contradiction, since the set of nearest–neighbour gaps is
known and each gap is unique. Hence, we can immediately deduce that λp is not a solution unless p ∈ {N − 2, N}.
Given λp − λN−1 is a nearest neighbour gap we must have that p = N − 2 or p = N . Since the nearest neighbour
gaps are unique, λN−2 − λN−1 (cid:54)= λN−1 − λN and since we already know λN − λN−1 this confusion is impossible.
Furthermore, the uniqueness of the nearest neighbour gaps imply that the only possible solution is r = N − 1. Thus
λN−2 is uniquely determined.

the spectrum can be uniquely determined under our assumptions.

This process can be repeated using λp and λp+1 for any P < N . Therefore, given the second largest gap is λN−1,
Now let us assume the second largest gap is λN − λ2. Then λ2 is known because λN is the largest gap, given λ1 = 0.
By repeating the exact same argument as above we see that λ3 is uniquely determined by these two values and the
uniqueness of the nearest neighbour gaps. This same argument can be extended leading to the conclusion that if λ2
is λN minus the second largest gap then the spectrum is also unique. There are hence at most two possible spectra
based on this data.

This approach shows that if the gaps are unique, a solution to the turnpike problem is also unique up to a reﬂection
of the spectrum about its midpoint. However, in the presence of degenerate gaps the number of possible solutions
could scale slightly faster than linear in N [21] and the eigenspectrum can in this case only narrowed down to be
one of these solutions. Moreover, while a backtracking algorithm can ﬁnd a solution for the typical case in time
O(N 2 log(N )), the worst case complexity is O(2N N log(N )) [21]. This suggests that Bayesian inference will not
always ﬁnd a solution in time that is polynomial in N , which itself is exponential in the number of qubits. However,
for the two level spectrum of a single qubit, the phase can be determined unambiguously.

[1] Alexei Yu Kitaev, Alexander Shen, and Mikhail N Vyalyi. Classical and quantum computation, volume 47. American

Mathematical Society Providence, 2002.

14

[2] K. M. Svore, M. B. Hastings, and M. Freedman. Faster Phase Estimation. ArXiv e-prints, April 2013.
[3] Nathan Wiebe and Christopher E Granade. Eﬃcient bayesian phase estimation. arXiv preprint arXiv:1508.00869, 2015.
[4] Cristian Bonato, Machiel S Blok, Hossein T Dinani, Dominic W Berry, Matthew L Markham, Daniel J Twitchen, and
Ronald Hanson. Optimized quantum sensing with a single electron spin using real-time adaptive measurements. Nature
nanotechnology, 2015.

[5] Brendon L Higgins, Dominic W Berry, Stephen D Bartlett, Howard M Wiseman, and Geoﬀ J Pryde. Entanglement-free

heisenberg-limited phase estimation. Nature, 450(7168):393–396, 2007.

[6] Alexander Hentschel and Barry C Sanders. Machine learning for precise quantum measurement. Physical review letters,

104(6):063603, 2010.

[7] Christoph Dankert, Richard Cleve, Joseph Emerson, and Etera Livine. Exact and approximate unitary 2-designs and their

application to ﬁdelity estimation. Physical Review A, 80(1):012304, 2009.

[8] Shelby Kimmel, Marcus P da Silva, Colm A Ryan, Blake R Johnson, and Thomas Ohki. Robust extraction of tomographic

information via randomized benchmarking. Physical Review X, 4(1):011050, 2014.

[9] Shelby Kimmel, Guang Hao Low, and Theodore J Yoder. Robust calibration of a universal single-qubit gate set via robust

phase estimation. Physical Review A, 92(6):062315, 2015.

[10] Easwar Magesan, Jay M Gambetta, and Joseph Emerson. Scalable and robust randomized benchmarking of quantum

processes. Physical review letters, 106(18):180504, 2011.

[11] Nathan Wiebe, Christopher Granade, and D G Cory. Quantum bootstrapping via compressed quantum hamiltonian

learning. New Journal of Physics, 17(2):022005, 2015.

[12] Xavier Boyen and Daphne Koller. Tractable inference for complex stochastic processes. In Proceedings of the Fourteenth

conference on Uncertainty in artiﬁcial intelligence, pages 33–42. Morgan Kaufmann Publishers Inc., 1998.

[13] Arnaud Doucet, Simon Godsill, and Christophe Andrieu. On sequential monte carlo sampling methods for bayesian

ﬁltering. Statistics and computing, 10(3):197–208, 2000.

[14] Jane Liu and Mike West. Combined parameter and state estimation in simulation-based ﬁltering. In Sequential Monte

Carlo methods in practice, pages 197–223. Springer, 2001.

[15] Nathan Wiebe, Christopher Granade, Ashish Kapoor, and Krysta M Svore. Bayesian inference via rejection ﬁltering.

arXiv preprint arXiv:1511.06458, 2015.

[16] Harald Cram´er. Mathematical methods of statistics, volume 9. Princeton university press, 1945.
[17] Nathan Wiebe, Christopher Granade, Christopher Ferrie, and D. G. Cory. Hamiltonian learning and certiﬁcation using

quantum resources. Phys. Rev. Lett., 112:190501, May 2014.

[18] Nathan Wiebe, Christopher Granade, Christopher Ferrie, and David Cory. Quantum hamiltonian learning using imperfect

quantum resources. Phys. Rev. A, 89:042314, Apr 2014.

[19] Christopher E Granade, Christopher Ferrie, Nathan Wiebe, and David G Cory. Robust online hamiltonian learning. New

Journal of Physics, 14(10):103013, 2012.

[20] Markku PV Stenberg, Yuval R Sanders, and Frank K Wilhelm. Eﬃcient estimation of resonant coupling between quantum

systems. Physical review letters, 113(21):210404, 2014.

[21] Steven S Skiena, Warren D Smith, and Paul Lemke. Reconstructing sets from interpoint distances. In Proceedings of the

sixth annual symposium on Computational geometry, pages 332–339. ACM, 1990.

[22] Tosio Kato. On the adiabatic theorem of quantum mechanics. Journal of the Physical Society of Japan, 5(6):435–439,

1950.

[23] Donny Cheung, Peter Høyer, and Nathan Wiebe. Improved error bounds for the adiabatic approximation. Journal of

Physics A: Mathematical and Theoretical, 44(41):415302, 2011.

[24] Alexander Elgart and George A Hagedorn. A note on the switching adiabatic theorem. Journal of Mathematical Physics,

53(10):102202, 2012.

[25] Daniel A Lidar, Ali T Rezakhani, and Alioscia Hamma. Adiabatic approximation with exponential accuracy for many-body

systems and quantum computation. Journal of Mathematical Physics, 50(10):102106, 2009.

[26] AT Rezakhani, AK Pimachev, and DA Lidar. Accuracy versus run time in an adiabatic quantum search. Physical Review

A, 82(5):052305, 2010.

[27] Nathan Wiebe and Nathan S Babcock. Improved error-scaling for adiabatic quantum evolutions. New Journal of Physics,

14(1):013024, 2012.

[28] M´aria Kieferov´a and Nathan Wiebe. On the power of coherently controlled quantum adiabatic evolutions. New Journal

of Physics, 16(12):123034, 2014.

[29] Gilles Brassard, Peter Hoyer, Michele Mosca, and Alain Tapp. Quantum amplitude ampliﬁcation and estimation. Con-

temporary Mathematics, 305:53–74, 2002.

[30] Christopher E Granade, Christopher Ferrie, Nathan Wiebe, and D G Cory. Robust online hamiltonian learning. New

Journal of Physics, 14(10):103013, 2012.

[31] Karol Zyczkowski and Marek Kus. Random unitary matrices. Journal of Physics A: Mathematical and General, 27(12):4235,

1994.

[32] John Watrous. Theory of quantum information. University of Waterloo Fall, 2011.
[33] Aram W Harrow and Richard A Low. Random quantum circuits are approximate 2-designs. Communications in Mathe-

matical Physics, 291(1):257–302, 2009.

